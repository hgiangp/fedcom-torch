v = 22.697160301531774
a_0 = 27.840057009285058	a_alpha = -0.3425458469693173
['f_00000', 'f_00001', 'f_00002', 'f_00003', 'f_00004', 'f_00005', 'f_00006', 'f_00007', 'f_00008', 'f_00009']
10
dict_keys(['x', 'y'])
id = f_00000, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 126
id = f_00001, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 265
id = f_00002, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 124
id = f_00003, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 43
id = f_00004, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 306
id = f_00005, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 146
id = f_00006, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 54
id = f_00007, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 179
id = f_00008, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 130
id = f_00009, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 118
BaseFederated generated!
num_samples = [126 265 124  43 306 146  54 179 130 118]
msize = 502400
xs = 6.094342 -20.799682 15.009024 18.811294 -39.020704 -26.043590 2.556808 -6.324852 -0.336023 -17.060879 
ys = 17.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 7.375016 -19.177652 17.569006 -0.998518 
xs mean: -6.711426168117427
ys mean: 5.394142535528706
dists_uav = 101.717635 103.318009 101.128704 104.221562 107.749902 104.755057 100.304178 102.018558 101.532177 101.449843 
uav_gains = -100.184927 -100.354426 -100.121880 -100.448968 -100.810468 -100.504406 -100.032992 -100.217001 -100.165112 -100.156304 
uav_gains_db_mean: -100.29964844724496
dists_bs = 239.945225 221.811133 257.425636 246.587507 214.313391 243.153064 244.181399 257.208604 235.142558 236.474617 
bs_gains = -106.210198 -105.254593 -107.065309 -106.542249 -104.836440 -106.371692 -106.423011 -107.055053 -105.964334 -106.033026 
bs_gains_db_mean: -106.1755905375805
SystemModel __init__!
t_co_uav = [0.06351163 0.06396501 0.06334461 0.06422068 0.06521719 0.06437154
 0.0631106  0.06359693 0.06345904 0.0634357 ]
t_co_bs = [0.08476871 0.08051968 0.08895491 0.08634835 0.0787878  0.08552995
 0.08577462 0.08890235 0.0836346  0.08394849]
difference = [-0.02125708 -0.01655467 -0.0256103  -0.02212767 -0.01357061 -0.0211584
 -0.02266402 -0.02530541 -0.02017555 -0.0205128 ]
decs_opt = [1 0 1 1 0 0 1 1 0 0]
af = 6.796163711028166	bf = 20.32894796997589	zeta = 7.475780082130983	eta = 0.9090909090909091
af = 6.796163711028166	bf = 20.32894796997589	zeta = 230.74360710778967	eta = 0.02945331312192499
af = 6.796163711028166	bf = 20.32894796997589	zeta = 45.629002909774904	eta = 0.1489439452461113
af = 6.796163711028166	bf = 20.32894796997589	zeta = 39.092727771299245	eta = 0.173847262610764
af = 6.796163711028166	bf = 20.32894796997589	zeta = 38.999286881910415	eta = 0.17426379440236703
af = 6.796163711028166	bf = 20.32894796997589	zeta = 38.99926333319505	eta = 0.17426389962713645
eta_opt = 0.17426389962713645
initialize_feasible_solution eta = 0.17426389962713645, tau = 10.000839233398438
ti_comp = [0.03604337 0.0758055  0.03547125 0.01230051 0.0875339  0.04176454
 0.01544716 0.05120447 0.0371876  0.0337549 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [3.35654071 5.27057237 3.33162034 2.5799458  5.60760928 4.29178968
 2.64860954 3.87057992 4.07357378 3.96842191]
system_model train() tau = 30	t0 = 0.050004196166992185	t_min = 10.000839233398438
Round 0
-------------------------------
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86489914 22.7015787  10.6948057   3.82750737 26.17399534 12.62305496
  4.75731633 15.36016845 11.26073498 10.24387519]
obj_prev = 128.5079361631131
eta_min = 2.1972428731623064e-09	eta_max = 0.9187482009227209
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 50.862560519193266	eta = 0.5344727942639534
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 40.950101804889684	eta = 0.6638482847646219
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.18020327767913	eta = 0.6938364931761265
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.094800455198204	eta = 0.6953521830931377
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.094588160812094	eta = 0.6953559590470941
eta = 0.6953559590470941
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.02998124 0.06305577 0.02950534 0.01023169 0.07281157 0.03474016
 0.0128491  0.04259239 0.03093302 0.02807767]
ene_total = [3.3202555  6.49821862 3.27523947 1.52191262 7.37393057 3.95636804
 1.75096148 4.4735121  3.5911297  3.33306006]
ti_comp = [0.26476791 0.24775986 0.26493494 0.26405887 0.24949175 0.2427496
 0.26516894 0.26468261 0.24464495 0.24433105]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.40269086e-05 2.55265831e-04 2.28719695e-05 9.60109636e-07
 3.87586715e-04 4.44691157e-05 1.88561461e-06 6.89326925e-05
 3.09082577e-05 2.31742490e-05]
ene_total = [0.58260011 0.75916097 0.58096823 0.58697181 0.75542628 0.78568377
 0.5769119  0.5874834  0.76712376 0.76928555]
optimize_network iter = 0 obj = 6.7516157792990965
eta = 0.6953559590470941
freqs = [5.66179550e+07 1.27251793e+08 5.56841306e+07 1.93738838e+07
 1.45919802e+08 7.15555497e+07 2.42281409e+07 8.04593666e+07
 6.32202327e+07 5.74582420e+07]
eta_min = 0.6595211103034527	eta_max = 0.6953559590470929
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 0.07249356690995133	eta = 0.909090909090909
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 22.43093651594189	eta = 0.002938051320263521
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.424233800512977	eta = 0.027185184296772417
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.333929777668074	eta = 0.028237028926918685
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.333881523150504	eta = 0.02823761274584649
eta = 0.02823761274584649
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.35890421e-04 2.50613865e-03 2.24551507e-04 9.42612594e-06
 3.80523333e-03 4.36587104e-04 1.85125116e-05 6.76764628e-04
 3.03449855e-04 2.27519213e-04]
ene_total = [0.18871274 0.3024796  0.18790939 0.18425611 0.33473573 0.25754286
 0.18133617 0.20158775 0.24829862 0.24702256]
ti_comp = [0.30338297 0.28637492 0.30355    0.30267393 0.28810681 0.28136466
 0.303784   0.30329767 0.28326001 0.28294611]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.75152158e-05 2.87283940e-04 2.61968029e-05 1.09875040e-06
 4.37019626e-04 4.97694879e-05 2.16020605e-06 7.89340962e-05
 3.46659600e-05 2.59825795e-05]
ene_total = [0.52156762 0.68187855 0.52009413 0.52520532 0.6799609  0.70342523
 0.51621531 0.52646951 0.68669249 0.68854913]
optimize_network iter = 1 obj = 6.050058200424808
eta = 0.6595211103034527
freqs = [5.66070564e+07 1.26125273e+08 5.56778794e+07 1.93635364e+07
 1.44763522e+08 7.07252580e+07 2.42281409e+07 8.04405789e+07
 6.25531698e+07 5.68420211e+07]
eta_min = 0.6595211103034622	eta_max = 0.6595211103034516
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 0.07141177968017555	eta = 0.909090909090909
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 22.429905462588092	eta = 0.0028943412096646293
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.4193533145705937	eta = 0.026833534117679286
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.3303118142037405	eta = 0.02785884674898469
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.3302654452731733	eta = 0.02785940109996355
eta = 0.02785940109996355
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.36690987e-04 2.47126971e-03 2.25349755e-04 9.45165461e-06
 3.75932384e-03 4.28126361e-04 1.85824929e-05 6.79005729e-04
 2.98203014e-04 2.23506966e-04]
ene_total = [0.18866225 0.30136334 0.18785915 0.18418516 0.33329075 0.25720037
 0.18126762 0.2015735  0.24805176 0.24681156]
ti_comp = [0.30338297 0.28637492 0.30355    0.30267393 0.28810681 0.28136466
 0.303784   0.30329767 0.28326001 0.28294611]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.75152158e-05 2.87283940e-04 2.61968029e-05 1.09875040e-06
 4.37019626e-04 4.97694879e-05 2.16020605e-06 7.89340962e-05
 3.46659600e-05 2.59825795e-05]
ene_total = [0.52156762 0.68187855 0.52009413 0.52520532 0.6799609  0.70342523
 0.51621531 0.52646951 0.68669249 0.68854913]
optimize_network iter = 2 obj = 6.0500582004249726
eta = 0.6595211103034622
freqs = [5.66070564e+07 1.26125273e+08 5.56778794e+07 1.93635364e+07
 1.44763522e+08 7.07252580e+07 2.42281409e+07 8.04405789e+07
 6.25531698e+07 5.68420211e+07]
Done!
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.62436988e-05 2.74008143e-04 2.49862116e-05 1.04797559e-06
 4.16824332e-04 4.74695695e-05 2.06037988e-06 7.52864401e-05
 3.30639970e-05 2.47818878e-05]
ene_total = [0.00637741 0.00832598 0.00635945 0.00642312 0.0082956  0.00860046
 0.00631312 0.00643498 0.00839652 0.00841963]
At round 0 energy consumption: 0.07394626955167252
At round 0 eta: 0.6595211103034622
At round 0 a_n: 27.840057009285058
At round 0 local rounds: 13.62985484868938
At round 0 global rounds: 81.76735137411414
gradient difference: 0.6684499979019165
train() client id: f_00000-0-0 loss: 0.900237  [   32/  126]
train() client id: f_00000-0-1 loss: 0.839195  [   64/  126]
train() client id: f_00000-0-2 loss: 1.132388  [   96/  126]
train() client id: f_00000-1-0 loss: 0.860148  [   32/  126]
train() client id: f_00000-1-1 loss: 0.976661  [   64/  126]
train() client id: f_00000-1-2 loss: 0.836685  [   96/  126]
train() client id: f_00000-2-0 loss: 1.016482  [   32/  126]
train() client id: f_00000-2-1 loss: 0.789526  [   64/  126]
train() client id: f_00000-2-2 loss: 0.904598  [   96/  126]
train() client id: f_00000-3-0 loss: 0.927547  [   32/  126]
train() client id: f_00000-3-1 loss: 1.104912  [   64/  126]
train() client id: f_00000-3-2 loss: 0.981365  [   96/  126]
train() client id: f_00000-4-0 loss: 0.941456  [   32/  126]
train() client id: f_00000-4-1 loss: 1.045976  [   64/  126]
train() client id: f_00000-4-2 loss: 1.037894  [   96/  126]
train() client id: f_00000-5-0 loss: 0.904993  [   32/  126]
train() client id: f_00000-5-1 loss: 1.167038  [   64/  126]
train() client id: f_00000-5-2 loss: 0.898205  [   96/  126]
train() client id: f_00000-6-0 loss: 0.909409  [   32/  126]
train() client id: f_00000-6-1 loss: 1.164461  [   64/  126]
train() client id: f_00000-6-2 loss: 0.987294  [   96/  126]
train() client id: f_00000-7-0 loss: 0.927393  [   32/  126]
train() client id: f_00000-7-1 loss: 0.988255  [   64/  126]
train() client id: f_00000-7-2 loss: 0.895351  [   96/  126]
train() client id: f_00000-8-0 loss: 0.924910  [   32/  126]
train() client id: f_00000-8-1 loss: 1.010216  [   64/  126]
train() client id: f_00000-8-2 loss: 0.848202  [   96/  126]
train() client id: f_00000-9-0 loss: 1.100134  [   32/  126]
train() client id: f_00000-9-1 loss: 0.930543  [   64/  126]
train() client id: f_00000-9-2 loss: 0.816172  [   96/  126]
train() client id: f_00000-10-0 loss: 0.970211  [   32/  126]
train() client id: f_00000-10-1 loss: 0.830195  [   64/  126]
train() client id: f_00000-10-2 loss: 1.105525  [   96/  126]
train() client id: f_00000-11-0 loss: 1.022089  [   32/  126]
train() client id: f_00000-11-1 loss: 0.712208  [   64/  126]
train() client id: f_00000-11-2 loss: 0.932569  [   96/  126]
train() client id: f_00000-12-0 loss: 0.935313  [   32/  126]
train() client id: f_00000-12-1 loss: 1.007156  [   64/  126]
train() client id: f_00000-12-2 loss: 0.954012  [   96/  126]
train() client id: f_00001-0-0 loss: 0.796803  [   32/  265]
train() client id: f_00001-0-1 loss: 0.809362  [   64/  265]
train() client id: f_00001-0-2 loss: 0.787435  [   96/  265]
train() client id: f_00001-0-3 loss: 0.770734  [  128/  265]
train() client id: f_00001-0-4 loss: 0.818169  [  160/  265]
train() client id: f_00001-0-5 loss: 0.779518  [  192/  265]
train() client id: f_00001-0-6 loss: 0.814169  [  224/  265]
train() client id: f_00001-0-7 loss: 0.881851  [  256/  265]
train() client id: f_00001-1-0 loss: 0.803796  [   32/  265]
train() client id: f_00001-1-1 loss: 0.716766  [   64/  265]
train() client id: f_00001-1-2 loss: 0.781486  [   96/  265]
train() client id: f_00001-1-3 loss: 0.799602  [  128/  265]
train() client id: f_00001-1-4 loss: 0.790151  [  160/  265]
train() client id: f_00001-1-5 loss: 0.773080  [  192/  265]
train() client id: f_00001-1-6 loss: 0.926504  [  224/  265]
train() client id: f_00001-1-7 loss: 0.899780  [  256/  265]
train() client id: f_00001-2-0 loss: 0.759554  [   32/  265]
train() client id: f_00001-2-1 loss: 0.719446  [   64/  265]
train() client id: f_00001-2-2 loss: 0.808710  [   96/  265]
train() client id: f_00001-2-3 loss: 0.883205  [  128/  265]
train() client id: f_00001-2-4 loss: 0.841089  [  160/  265]
train() client id: f_00001-2-5 loss: 0.962152  [  192/  265]
train() client id: f_00001-2-6 loss: 0.816930  [  224/  265]
train() client id: f_00001-2-7 loss: 0.815044  [  256/  265]
train() client id: f_00001-3-0 loss: 0.815372  [   32/  265]
train() client id: f_00001-3-1 loss: 0.836235  [   64/  265]
train() client id: f_00001-3-2 loss: 0.752442  [   96/  265]
train() client id: f_00001-3-3 loss: 0.851093  [  128/  265]
train() client id: f_00001-3-4 loss: 0.841592  [  160/  265]
train() client id: f_00001-3-5 loss: 0.808672  [  192/  265]
train() client id: f_00001-3-6 loss: 0.911750  [  224/  265]
train() client id: f_00001-3-7 loss: 0.846300  [  256/  265]
train() client id: f_00001-4-0 loss: 0.847838  [   32/  265]
train() client id: f_00001-4-1 loss: 0.845143  [   64/  265]
train() client id: f_00001-4-2 loss: 0.884544  [   96/  265]
train() client id: f_00001-4-3 loss: 0.902179  [  128/  265]
train() client id: f_00001-4-4 loss: 0.782437  [  160/  265]
train() client id: f_00001-4-5 loss: 0.810234  [  192/  265]
train() client id: f_00001-4-6 loss: 0.830886  [  224/  265]
train() client id: f_00001-4-7 loss: 0.862448  [  256/  265]
train() client id: f_00001-5-0 loss: 0.871434  [   32/  265]
train() client id: f_00001-5-1 loss: 0.902408  [   64/  265]
train() client id: f_00001-5-2 loss: 0.902230  [   96/  265]
train() client id: f_00001-5-3 loss: 0.801003  [  128/  265]
train() client id: f_00001-5-4 loss: 0.928302  [  160/  265]
train() client id: f_00001-5-5 loss: 0.803966  [  192/  265]
train() client id: f_00001-5-6 loss: 0.893604  [  224/  265]
train() client id: f_00001-5-7 loss: 0.779704  [  256/  265]
train() client id: f_00001-6-0 loss: 0.942864  [   32/  265]
train() client id: f_00001-6-1 loss: 0.841564  [   64/  265]
train() client id: f_00001-6-2 loss: 0.975256  [   96/  265]
train() client id: f_00001-6-3 loss: 0.887617  [  128/  265]
train() client id: f_00001-6-4 loss: 0.822029  [  160/  265]
train() client id: f_00001-6-5 loss: 0.832423  [  192/  265]
train() client id: f_00001-6-6 loss: 0.843207  [  224/  265]
train() client id: f_00001-6-7 loss: 0.818324  [  256/  265]
train() client id: f_00001-7-0 loss: 0.900668  [   32/  265]
train() client id: f_00001-7-1 loss: 0.771429  [   64/  265]
train() client id: f_00001-7-2 loss: 0.896048  [   96/  265]
train() client id: f_00001-7-3 loss: 0.876563  [  128/  265]
train() client id: f_00001-7-4 loss: 1.000851  [  160/  265]
train() client id: f_00001-7-5 loss: 0.975830  [  192/  265]
train() client id: f_00001-7-6 loss: 0.841911  [  224/  265]
train() client id: f_00001-7-7 loss: 0.826459  [  256/  265]
train() client id: f_00001-8-0 loss: 0.890794  [   32/  265]
train() client id: f_00001-8-1 loss: 0.906697  [   64/  265]
train() client id: f_00001-8-2 loss: 0.860502  [   96/  265]
train() client id: f_00001-8-3 loss: 0.936726  [  128/  265]
train() client id: f_00001-8-4 loss: 0.849270  [  160/  265]
train() client id: f_00001-8-5 loss: 0.917105  [  192/  265]
train() client id: f_00001-8-6 loss: 1.008483  [  224/  265]
train() client id: f_00001-8-7 loss: 0.820158  [  256/  265]
train() client id: f_00001-9-0 loss: 0.814290  [   32/  265]
train() client id: f_00001-9-1 loss: 1.044505  [   64/  265]
train() client id: f_00001-9-2 loss: 0.965565  [   96/  265]
train() client id: f_00001-9-3 loss: 0.916436  [  128/  265]
train() client id: f_00001-9-4 loss: 0.864756  [  160/  265]
train() client id: f_00001-9-5 loss: 0.892959  [  192/  265]
train() client id: f_00001-9-6 loss: 0.922773  [  224/  265]
train() client id: f_00001-9-7 loss: 0.893461  [  256/  265]
train() client id: f_00001-10-0 loss: 0.980932  [   32/  265]
train() client id: f_00001-10-1 loss: 0.884471  [   64/  265]
train() client id: f_00001-10-2 loss: 0.871858  [   96/  265]
train() client id: f_00001-10-3 loss: 0.911342  [  128/  265]
train() client id: f_00001-10-4 loss: 0.857029  [  160/  265]
train() client id: f_00001-10-5 loss: 0.976516  [  192/  265]
train() client id: f_00001-10-6 loss: 1.063675  [  224/  265]
train() client id: f_00001-10-7 loss: 0.888372  [  256/  265]
train() client id: f_00001-11-0 loss: 0.978311  [   32/  265]
train() client id: f_00001-11-1 loss: 0.858761  [   64/  265]
train() client id: f_00001-11-2 loss: 0.983988  [   96/  265]
train() client id: f_00001-11-3 loss: 0.907647  [  128/  265]
train() client id: f_00001-11-4 loss: 0.975896  [  160/  265]
train() client id: f_00001-11-5 loss: 1.000391  [  192/  265]
train() client id: f_00001-11-6 loss: 0.947641  [  224/  265]
train() client id: f_00001-11-7 loss: 0.897341  [  256/  265]
train() client id: f_00001-12-0 loss: 1.003340  [   32/  265]
train() client id: f_00001-12-1 loss: 0.868732  [   64/  265]
train() client id: f_00001-12-2 loss: 1.014078  [   96/  265]
train() client id: f_00001-12-3 loss: 0.888933  [  128/  265]
train() client id: f_00001-12-4 loss: 0.968847  [  160/  265]
train() client id: f_00001-12-5 loss: 1.026387  [  192/  265]
train() client id: f_00001-12-6 loss: 0.963534  [  224/  265]
train() client id: f_00001-12-7 loss: 0.918645  [  256/  265]
train() client id: f_00002-0-0 loss: 1.053867  [   32/  124]
train() client id: f_00002-0-1 loss: 1.130171  [   64/  124]
train() client id: f_00002-0-2 loss: 0.883132  [   96/  124]
train() client id: f_00002-1-0 loss: 1.084717  [   32/  124]
train() client id: f_00002-1-1 loss: 1.002121  [   64/  124]
train() client id: f_00002-1-2 loss: 1.188979  [   96/  124]
train() client id: f_00002-2-0 loss: 1.097235  [   32/  124]
train() client id: f_00002-2-1 loss: 1.135239  [   64/  124]
train() client id: f_00002-2-2 loss: 1.012941  [   96/  124]
train() client id: f_00002-3-0 loss: 1.158245  [   32/  124]
train() client id: f_00002-3-1 loss: 1.115002  [   64/  124]
train() client id: f_00002-3-2 loss: 0.943601  [   96/  124]
train() client id: f_00002-4-0 loss: 1.124910  [   32/  124]
train() client id: f_00002-4-1 loss: 1.219226  [   64/  124]
train() client id: f_00002-4-2 loss: 0.980235  [   96/  124]
train() client id: f_00002-5-0 loss: 1.159563  [   32/  124]
train() client id: f_00002-5-1 loss: 1.063952  [   64/  124]
train() client id: f_00002-5-2 loss: 1.057964  [   96/  124]
train() client id: f_00002-6-0 loss: 0.996903  [   32/  124]
train() client id: f_00002-6-1 loss: 1.149003  [   64/  124]
train() client id: f_00002-6-2 loss: 1.080541  [   96/  124]
train() client id: f_00002-7-0 loss: 1.187290  [   32/  124]
train() client id: f_00002-7-1 loss: 1.066983  [   64/  124]
train() client id: f_00002-7-2 loss: 0.996741  [   96/  124]
train() client id: f_00002-8-0 loss: 1.014462  [   32/  124]
train() client id: f_00002-8-1 loss: 1.131744  [   64/  124]
train() client id: f_00002-8-2 loss: 1.134588  [   96/  124]
train() client id: f_00002-9-0 loss: 1.032473  [   32/  124]
train() client id: f_00002-9-1 loss: 1.214320  [   64/  124]
train() client id: f_00002-9-2 loss: 1.136049  [   96/  124]
train() client id: f_00002-10-0 loss: 1.164134  [   32/  124]
train() client id: f_00002-10-1 loss: 1.094398  [   64/  124]
train() client id: f_00002-10-2 loss: 1.130771  [   96/  124]
train() client id: f_00002-11-0 loss: 1.124138  [   32/  124]
train() client id: f_00002-11-1 loss: 1.011402  [   64/  124]
train() client id: f_00002-11-2 loss: 1.284779  [   96/  124]
train() client id: f_00002-12-0 loss: 1.111051  [   32/  124]
train() client id: f_00002-12-1 loss: 1.198529  [   64/  124]
train() client id: f_00002-12-2 loss: 1.192599  [   96/  124]
train() client id: f_00003-0-0 loss: 0.986216  [   32/   43]
train() client id: f_00003-1-0 loss: 0.958226  [   32/   43]
train() client id: f_00003-2-0 loss: 0.967973  [   32/   43]
train() client id: f_00003-3-0 loss: 0.992290  [   32/   43]
train() client id: f_00003-4-0 loss: 0.971672  [   32/   43]
train() client id: f_00003-5-0 loss: 1.007722  [   32/   43]
train() client id: f_00003-6-0 loss: 1.050233  [   32/   43]
train() client id: f_00003-7-0 loss: 1.038528  [   32/   43]
train() client id: f_00003-8-0 loss: 0.906996  [   32/   43]
train() client id: f_00003-9-0 loss: 0.982034  [   32/   43]
train() client id: f_00003-10-0 loss: 1.013963  [   32/   43]
train() client id: f_00003-11-0 loss: 1.000319  [   32/   43]
train() client id: f_00003-12-0 loss: 0.992324  [   32/   43]
train() client id: f_00004-0-0 loss: 0.943807  [   32/  306]
train() client id: f_00004-0-1 loss: 0.998928  [   64/  306]
train() client id: f_00004-0-2 loss: 1.039952  [   96/  306]
train() client id: f_00004-0-3 loss: 0.968258  [  128/  306]
train() client id: f_00004-0-4 loss: 1.022255  [  160/  306]
train() client id: f_00004-0-5 loss: 0.991354  [  192/  306]
train() client id: f_00004-0-6 loss: 0.953090  [  224/  306]
train() client id: f_00004-0-7 loss: 1.019537  [  256/  306]
train() client id: f_00004-0-8 loss: 0.959220  [  288/  306]
train() client id: f_00004-1-0 loss: 1.008371  [   32/  306]
train() client id: f_00004-1-1 loss: 0.952671  [   64/  306]
train() client id: f_00004-1-2 loss: 0.954930  [   96/  306]
train() client id: f_00004-1-3 loss: 0.969050  [  128/  306]
train() client id: f_00004-1-4 loss: 0.953490  [  160/  306]
train() client id: f_00004-1-5 loss: 0.991465  [  192/  306]
train() client id: f_00004-1-6 loss: 1.006647  [  224/  306]
train() client id: f_00004-1-7 loss: 1.000954  [  256/  306]
train() client id: f_00004-1-8 loss: 1.092076  [  288/  306]
train() client id: f_00004-2-0 loss: 0.991057  [   32/  306]
train() client id: f_00004-2-1 loss: 0.958641  [   64/  306]
train() client id: f_00004-2-2 loss: 1.024188  [   96/  306]
train() client id: f_00004-2-3 loss: 1.026556  [  128/  306]
train() client id: f_00004-2-4 loss: 0.967774  [  160/  306]
train() client id: f_00004-2-5 loss: 0.966970  [  192/  306]
train() client id: f_00004-2-6 loss: 1.005947  [  224/  306]
train() client id: f_00004-2-7 loss: 1.077821  [  256/  306]
train() client id: f_00004-2-8 loss: 1.040526  [  288/  306]
train() client id: f_00004-3-0 loss: 0.989941  [   32/  306]
train() client id: f_00004-3-1 loss: 1.014126  [   64/  306]
train() client id: f_00004-3-2 loss: 1.079366  [   96/  306]
train() client id: f_00004-3-3 loss: 1.033532  [  128/  306]
train() client id: f_00004-3-4 loss: 1.036896  [  160/  306]
train() client id: f_00004-3-5 loss: 1.020546  [  192/  306]
train() client id: f_00004-3-6 loss: 0.996878  [  224/  306]
train() client id: f_00004-3-7 loss: 0.992545  [  256/  306]
train() client id: f_00004-3-8 loss: 0.984758  [  288/  306]
train() client id: f_00004-4-0 loss: 0.950893  [   32/  306]
train() client id: f_00004-4-1 loss: 1.071978  [   64/  306]
train() client id: f_00004-4-2 loss: 1.006861  [   96/  306]
train() client id: f_00004-4-3 loss: 0.961430  [  128/  306]
train() client id: f_00004-4-4 loss: 1.012715  [  160/  306]
train() client id: f_00004-4-5 loss: 1.082921  [  192/  306]
train() client id: f_00004-4-6 loss: 1.084401  [  224/  306]
train() client id: f_00004-4-7 loss: 1.057368  [  256/  306]
train() client id: f_00004-4-8 loss: 0.998761  [  288/  306]
train() client id: f_00004-5-0 loss: 1.047621  [   32/  306]
train() client id: f_00004-5-1 loss: 0.984135  [   64/  306]
train() client id: f_00004-5-2 loss: 1.050018  [   96/  306]
train() client id: f_00004-5-3 loss: 0.996756  [  128/  306]
train() client id: f_00004-5-4 loss: 1.048205  [  160/  306]
train() client id: f_00004-5-5 loss: 1.011189  [  192/  306]
train() client id: f_00004-5-6 loss: 0.984715  [  224/  306]
train() client id: f_00004-5-7 loss: 1.026427  [  256/  306]
train() client id: f_00004-5-8 loss: 1.121477  [  288/  306]
train() client id: f_00004-6-0 loss: 0.988186  [   32/  306]
train() client id: f_00004-6-1 loss: 1.004209  [   64/  306]
train() client id: f_00004-6-2 loss: 0.997344  [   96/  306]
train() client id: f_00004-6-3 loss: 1.077033  [  128/  306]
train() client id: f_00004-6-4 loss: 1.067784  [  160/  306]
train() client id: f_00004-6-5 loss: 1.094810  [  192/  306]
train() client id: f_00004-6-6 loss: 1.048043  [  224/  306]
train() client id: f_00004-6-7 loss: 1.059189  [  256/  306]
train() client id: f_00004-6-8 loss: 1.057360  [  288/  306]
train() client id: f_00004-7-0 loss: 1.068841  [   32/  306]
train() client id: f_00004-7-1 loss: 1.005640  [   64/  306]
train() client id: f_00004-7-2 loss: 1.100821  [   96/  306]
train() client id: f_00004-7-3 loss: 1.023695  [  128/  306]
train() client id: f_00004-7-4 loss: 1.053122  [  160/  306]
train() client id: f_00004-7-5 loss: 1.017070  [  192/  306]
train() client id: f_00004-7-6 loss: 1.004757  [  224/  306]
train() client id: f_00004-7-7 loss: 1.045458  [  256/  306]
train() client id: f_00004-7-8 loss: 1.123899  [  288/  306]
train() client id: f_00004-8-0 loss: 1.038502  [   32/  306]
train() client id: f_00004-8-1 loss: 1.027689  [   64/  306]
train() client id: f_00004-8-2 loss: 1.162375  [   96/  306]
train() client id: f_00004-8-3 loss: 1.035861  [  128/  306]
train() client id: f_00004-8-4 loss: 1.025831  [  160/  306]
train() client id: f_00004-8-5 loss: 1.093267  [  192/  306]
train() client id: f_00004-8-6 loss: 1.090829  [  224/  306]
train() client id: f_00004-8-7 loss: 1.061000  [  256/  306]
train() client id: f_00004-8-8 loss: 1.011964  [  288/  306]
train() client id: f_00004-9-0 loss: 1.122531  [   32/  306]
train() client id: f_00004-9-1 loss: 1.096138  [   64/  306]
train() client id: f_00004-9-2 loss: 1.075602  [   96/  306]
train() client id: f_00004-9-3 loss: 1.065167  [  128/  306]
train() client id: f_00004-9-4 loss: 1.104026  [  160/  306]
train() client id: f_00004-9-5 loss: 1.053075  [  192/  306]
train() client id: f_00004-9-6 loss: 1.018619  [  224/  306]
train() client id: f_00004-9-7 loss: 1.065599  [  256/  306]
train() client id: f_00004-9-8 loss: 1.011326  [  288/  306]
train() client id: f_00004-10-0 loss: 1.095968  [   32/  306]
train() client id: f_00004-10-1 loss: 1.031388  [   64/  306]
train() client id: f_00004-10-2 loss: 1.021067  [   96/  306]
train() client id: f_00004-10-3 loss: 1.044867  [  128/  306]
train() client id: f_00004-10-4 loss: 1.145755  [  160/  306]
train() client id: f_00004-10-5 loss: 1.124979  [  192/  306]
train() client id: f_00004-10-6 loss: 1.118953  [  224/  306]
train() client id: f_00004-10-7 loss: 1.058710  [  256/  306]
train() client id: f_00004-10-8 loss: 1.107721  [  288/  306]
train() client id: f_00004-11-0 loss: 1.091107  [   32/  306]
train() client id: f_00004-11-1 loss: 1.100622  [   64/  306]
train() client id: f_00004-11-2 loss: 1.076157  [   96/  306]
train() client id: f_00004-11-3 loss: 1.066040  [  128/  306]
train() client id: f_00004-11-4 loss: 1.089061  [  160/  306]
train() client id: f_00004-11-5 loss: 1.145621  [  192/  306]
train() client id: f_00004-11-6 loss: 1.052751  [  224/  306]
train() client id: f_00004-11-7 loss: 1.051234  [  256/  306]
train() client id: f_00004-11-8 loss: 1.183826  [  288/  306]
train() client id: f_00004-12-0 loss: 1.085682  [   32/  306]
train() client id: f_00004-12-1 loss: 1.083563  [   64/  306]
train() client id: f_00004-12-2 loss: 1.146705  [   96/  306]
train() client id: f_00004-12-3 loss: 1.105441  [  128/  306]
train() client id: f_00004-12-4 loss: 1.085572  [  160/  306]
train() client id: f_00004-12-5 loss: 1.096261  [  192/  306]
train() client id: f_00004-12-6 loss: 1.048964  [  224/  306]
train() client id: f_00004-12-7 loss: 1.117473  [  256/  306]
train() client id: f_00004-12-8 loss: 1.062592  [  288/  306]
train() client id: f_00005-0-0 loss: 0.816276  [   32/  146]
train() client id: f_00005-0-1 loss: 0.910984  [   64/  146]
train() client id: f_00005-0-2 loss: 1.029441  [   96/  146]
train() client id: f_00005-0-3 loss: 0.873939  [  128/  146]
train() client id: f_00005-1-0 loss: 0.824292  [   32/  146]
train() client id: f_00005-1-1 loss: 0.761674  [   64/  146]
train() client id: f_00005-1-2 loss: 0.984306  [   96/  146]
train() client id: f_00005-1-3 loss: 0.982209  [  128/  146]
train() client id: f_00005-2-0 loss: 0.761657  [   32/  146]
train() client id: f_00005-2-1 loss: 1.097769  [   64/  146]
train() client id: f_00005-2-2 loss: 0.870366  [   96/  146]
train() client id: f_00005-2-3 loss: 0.911376  [  128/  146]
train() client id: f_00005-3-0 loss: 0.810867  [   32/  146]
train() client id: f_00005-3-1 loss: 0.870573  [   64/  146]
train() client id: f_00005-3-2 loss: 0.892917  [   96/  146]
train() client id: f_00005-3-3 loss: 1.116071  [  128/  146]
train() client id: f_00005-4-0 loss: 0.724661  [   32/  146]
train() client id: f_00005-4-1 loss: 1.053444  [   64/  146]
train() client id: f_00005-4-2 loss: 1.048334  [   96/  146]
train() client id: f_00005-4-3 loss: 0.929083  [  128/  146]
train() client id: f_00005-5-0 loss: 0.884953  [   32/  146]
train() client id: f_00005-5-1 loss: 0.868453  [   64/  146]
train() client id: f_00005-5-2 loss: 0.885386  [   96/  146]
train() client id: f_00005-5-3 loss: 0.992524  [  128/  146]
train() client id: f_00005-6-0 loss: 1.215648  [   32/  146]
train() client id: f_00005-6-1 loss: 0.720952  [   64/  146]
train() client id: f_00005-6-2 loss: 0.941024  [   96/  146]
train() client id: f_00005-6-3 loss: 0.875038  [  128/  146]
train() client id: f_00005-7-0 loss: 1.039102  [   32/  146]
train() client id: f_00005-7-1 loss: 0.847351  [   64/  146]
train() client id: f_00005-7-2 loss: 0.831040  [   96/  146]
train() client id: f_00005-7-3 loss: 0.981445  [  128/  146]
train() client id: f_00005-8-0 loss: 0.707963  [   32/  146]
train() client id: f_00005-8-1 loss: 0.851739  [   64/  146]
train() client id: f_00005-8-2 loss: 1.024751  [   96/  146]
train() client id: f_00005-8-3 loss: 1.033959  [  128/  146]
train() client id: f_00005-9-0 loss: 0.890582  [   32/  146]
train() client id: f_00005-9-1 loss: 1.040520  [   64/  146]
train() client id: f_00005-9-2 loss: 0.884755  [   96/  146]
train() client id: f_00005-9-3 loss: 0.898051  [  128/  146]
train() client id: f_00005-10-0 loss: 0.810358  [   32/  146]
train() client id: f_00005-10-1 loss: 0.875021  [   64/  146]
train() client id: f_00005-10-2 loss: 0.890746  [   96/  146]
train() client id: f_00005-10-3 loss: 1.169653  [  128/  146]
train() client id: f_00005-11-0 loss: 0.942314  [   32/  146]
train() client id: f_00005-11-1 loss: 0.798153  [   64/  146]
train() client id: f_00005-11-2 loss: 0.995529  [   96/  146]
train() client id: f_00005-11-3 loss: 1.073846  [  128/  146]
train() client id: f_00005-12-0 loss: 0.863992  [   32/  146]
train() client id: f_00005-12-1 loss: 0.958195  [   64/  146]
train() client id: f_00005-12-2 loss: 0.891907  [   96/  146]
train() client id: f_00005-12-3 loss: 1.105464  [  128/  146]
train() client id: f_00006-0-0 loss: 0.940992  [   32/   54]
train() client id: f_00006-1-0 loss: 0.975522  [   32/   54]
train() client id: f_00006-2-0 loss: 0.940714  [   32/   54]
train() client id: f_00006-3-0 loss: 0.952093  [   32/   54]
train() client id: f_00006-4-0 loss: 0.979820  [   32/   54]
train() client id: f_00006-5-0 loss: 0.923045  [   32/   54]
train() client id: f_00006-6-0 loss: 0.920381  [   32/   54]
train() client id: f_00006-7-0 loss: 0.963306  [   32/   54]
train() client id: f_00006-8-0 loss: 0.946326  [   32/   54]
train() client id: f_00006-9-0 loss: 0.956752  [   32/   54]
train() client id: f_00006-10-0 loss: 0.944049  [   32/   54]
train() client id: f_00006-11-0 loss: 0.966588  [   32/   54]
train() client id: f_00006-12-0 loss: 0.946353  [   32/   54]
train() client id: f_00007-0-0 loss: 0.884594  [   32/  179]
train() client id: f_00007-0-1 loss: 0.918870  [   64/  179]
train() client id: f_00007-0-2 loss: 0.877111  [   96/  179]
train() client id: f_00007-0-3 loss: 0.984218  [  128/  179]
train() client id: f_00007-0-4 loss: 0.916902  [  160/  179]
train() client id: f_00007-1-0 loss: 0.872171  [   32/  179]
train() client id: f_00007-1-1 loss: 0.983171  [   64/  179]
train() client id: f_00007-1-2 loss: 0.833098  [   96/  179]
train() client id: f_00007-1-3 loss: 0.926748  [  128/  179]
train() client id: f_00007-1-4 loss: 0.972066  [  160/  179]
train() client id: f_00007-2-0 loss: 0.866191  [   32/  179]
train() client id: f_00007-2-1 loss: 1.007166  [   64/  179]
train() client id: f_00007-2-2 loss: 0.951856  [   96/  179]
train() client id: f_00007-2-3 loss: 0.897159  [  128/  179]
train() client id: f_00007-2-4 loss: 0.877923  [  160/  179]
train() client id: f_00007-3-0 loss: 0.899175  [   32/  179]
train() client id: f_00007-3-1 loss: 0.983795  [   64/  179]
train() client id: f_00007-3-2 loss: 0.891119  [   96/  179]
train() client id: f_00007-3-3 loss: 0.857155  [  128/  179]
train() client id: f_00007-3-4 loss: 0.863097  [  160/  179]
train() client id: f_00007-4-0 loss: 0.844676  [   32/  179]
train() client id: f_00007-4-1 loss: 0.937780  [   64/  179]
train() client id: f_00007-4-2 loss: 1.089265  [   96/  179]
train() client id: f_00007-4-3 loss: 0.856293  [  128/  179]
train() client id: f_00007-4-4 loss: 0.913374  [  160/  179]
train() client id: f_00007-5-0 loss: 1.091571  [   32/  179]
train() client id: f_00007-5-1 loss: 0.918608  [   64/  179]
train() client id: f_00007-5-2 loss: 0.889685  [   96/  179]
train() client id: f_00007-5-3 loss: 0.844354  [  128/  179]
train() client id: f_00007-5-4 loss: 0.891580  [  160/  179]
train() client id: f_00007-6-0 loss: 0.933077  [   32/  179]
train() client id: f_00007-6-1 loss: 1.008401  [   64/  179]
train() client id: f_00007-6-2 loss: 0.906787  [   96/  179]
train() client id: f_00007-6-3 loss: 0.835828  [  128/  179]
train() client id: f_00007-6-4 loss: 1.027876  [  160/  179]
train() client id: f_00007-7-0 loss: 0.837723  [   32/  179]
train() client id: f_00007-7-1 loss: 1.099054  [   64/  179]
train() client id: f_00007-7-2 loss: 0.880046  [   96/  179]
train() client id: f_00007-7-3 loss: 1.031381  [  128/  179]
train() client id: f_00007-7-4 loss: 0.993514  [  160/  179]
train() client id: f_00007-8-0 loss: 0.885209  [   32/  179]
train() client id: f_00007-8-1 loss: 1.120329  [   64/  179]
train() client id: f_00007-8-2 loss: 0.858228  [   96/  179]
train() client id: f_00007-8-3 loss: 0.986782  [  128/  179]
train() client id: f_00007-8-4 loss: 0.939850  [  160/  179]
train() client id: f_00007-9-0 loss: 1.020267  [   32/  179]
train() client id: f_00007-9-1 loss: 0.926549  [   64/  179]
train() client id: f_00007-9-2 loss: 1.072815  [   96/  179]
train() client id: f_00007-9-3 loss: 1.030916  [  128/  179]
train() client id: f_00007-9-4 loss: 0.907985  [  160/  179]
train() client id: f_00007-10-0 loss: 0.913942  [   32/  179]
train() client id: f_00007-10-1 loss: 1.068602  [   64/  179]
train() client id: f_00007-10-2 loss: 1.006780  [   96/  179]
train() client id: f_00007-10-3 loss: 1.063771  [  128/  179]
train() client id: f_00007-10-4 loss: 0.933207  [  160/  179]
train() client id: f_00007-11-0 loss: 0.978241  [   32/  179]
train() client id: f_00007-11-1 loss: 1.018507  [   64/  179]
train() client id: f_00007-11-2 loss: 1.132883  [   96/  179]
train() client id: f_00007-11-3 loss: 0.872628  [  128/  179]
train() client id: f_00007-11-4 loss: 1.030715  [  160/  179]
train() client id: f_00007-12-0 loss: 0.951957  [   32/  179]
train() client id: f_00007-12-1 loss: 0.977405  [   64/  179]
train() client id: f_00007-12-2 loss: 1.002658  [   96/  179]
train() client id: f_00007-12-3 loss: 1.010416  [  128/  179]
train() client id: f_00007-12-4 loss: 1.109866  [  160/  179]
train() client id: f_00008-0-0 loss: 0.995467  [   32/  130]
train() client id: f_00008-0-1 loss: 1.010231  [   64/  130]
train() client id: f_00008-0-2 loss: 1.070813  [   96/  130]
train() client id: f_00008-0-3 loss: 0.984441  [  128/  130]
train() client id: f_00008-1-0 loss: 1.061478  [   32/  130]
train() client id: f_00008-1-1 loss: 0.977193  [   64/  130]
train() client id: f_00008-1-2 loss: 1.023459  [   96/  130]
train() client id: f_00008-1-3 loss: 0.961325  [  128/  130]
train() client id: f_00008-2-0 loss: 0.935302  [   32/  130]
train() client id: f_00008-2-1 loss: 0.940568  [   64/  130]
train() client id: f_00008-2-2 loss: 1.000202  [   96/  130]
train() client id: f_00008-2-3 loss: 1.099582  [  128/  130]
train() client id: f_00008-3-0 loss: 1.016748  [   32/  130]
train() client id: f_00008-3-1 loss: 1.120151  [   64/  130]
train() client id: f_00008-3-2 loss: 0.912914  [   96/  130]
train() client id: f_00008-3-3 loss: 0.927722  [  128/  130]
train() client id: f_00008-4-0 loss: 0.969363  [   32/  130]
train() client id: f_00008-4-1 loss: 1.035610  [   64/  130]
train() client id: f_00008-4-2 loss: 0.986030  [   96/  130]
train() client id: f_00008-4-3 loss: 0.993547  [  128/  130]
train() client id: f_00008-5-0 loss: 0.998629  [   32/  130]
train() client id: f_00008-5-1 loss: 0.953011  [   64/  130]
train() client id: f_00008-5-2 loss: 1.010982  [   96/  130]
train() client id: f_00008-5-3 loss: 0.999871  [  128/  130]
train() client id: f_00008-6-0 loss: 0.948342  [   32/  130]
train() client id: f_00008-6-1 loss: 0.998787  [   64/  130]
train() client id: f_00008-6-2 loss: 1.015268  [   96/  130]
train() client id: f_00008-6-3 loss: 0.987311  [  128/  130]
train() client id: f_00008-7-0 loss: 0.957034  [   32/  130]
train() client id: f_00008-7-1 loss: 0.973408  [   64/  130]
train() client id: f_00008-7-2 loss: 0.997145  [   96/  130]
train() client id: f_00008-7-3 loss: 1.001592  [  128/  130]
train() client id: f_00008-8-0 loss: 0.977207  [   32/  130]
train() client id: f_00008-8-1 loss: 0.997829  [   64/  130]
train() client id: f_00008-8-2 loss: 0.966826  [   96/  130]
train() client id: f_00008-8-3 loss: 0.997222  [  128/  130]
train() client id: f_00008-9-0 loss: 1.038033  [   32/  130]
train() client id: f_00008-9-1 loss: 0.943230  [   64/  130]
train() client id: f_00008-9-2 loss: 0.987674  [   96/  130]
train() client id: f_00008-9-3 loss: 0.994660  [  128/  130]
train() client id: f_00008-10-0 loss: 0.982662  [   32/  130]
train() client id: f_00008-10-1 loss: 0.965220  [   64/  130]
train() client id: f_00008-10-2 loss: 1.065407  [   96/  130]
train() client id: f_00008-10-3 loss: 0.948841  [  128/  130]
train() client id: f_00008-11-0 loss: 1.066006  [   32/  130]
train() client id: f_00008-11-1 loss: 0.946052  [   64/  130]
train() client id: f_00008-11-2 loss: 1.081391  [   96/  130]
train() client id: f_00008-11-3 loss: 0.900555  [  128/  130]
train() client id: f_00008-12-0 loss: 0.928613  [   32/  130]
train() client id: f_00008-12-1 loss: 0.913097  [   64/  130]
train() client id: f_00008-12-2 loss: 1.131605  [   96/  130]
train() client id: f_00008-12-3 loss: 1.011790  [  128/  130]
train() client id: f_00009-0-0 loss: 0.893410  [   32/  118]
train() client id: f_00009-0-1 loss: 1.013510  [   64/  118]
train() client id: f_00009-0-2 loss: 0.814453  [   96/  118]
train() client id: f_00009-1-0 loss: 0.801796  [   32/  118]
train() client id: f_00009-1-1 loss: 0.890765  [   64/  118]
train() client id: f_00009-1-2 loss: 0.930962  [   96/  118]
train() client id: f_00009-2-0 loss: 0.968714  [   32/  118]
train() client id: f_00009-2-1 loss: 0.815089  [   64/  118]
train() client id: f_00009-2-2 loss: 0.869756  [   96/  118]
train() client id: f_00009-3-0 loss: 0.952323  [   32/  118]
train() client id: f_00009-3-1 loss: 0.835544  [   64/  118]
train() client id: f_00009-3-2 loss: 0.849889  [   96/  118]
train() client id: f_00009-4-0 loss: 0.891840  [   32/  118]
train() client id: f_00009-4-1 loss: 0.941680  [   64/  118]
train() client id: f_00009-4-2 loss: 0.792535  [   96/  118]
train() client id: f_00009-5-0 loss: 0.874558  [   32/  118]
train() client id: f_00009-5-1 loss: 0.820743  [   64/  118]
train() client id: f_00009-5-2 loss: 1.014776  [   96/  118]
train() client id: f_00009-6-0 loss: 0.869548  [   32/  118]
train() client id: f_00009-6-1 loss: 0.950356  [   64/  118]
train() client id: f_00009-6-2 loss: 0.853307  [   96/  118]
train() client id: f_00009-7-0 loss: 0.870074  [   32/  118]
train() client id: f_00009-7-1 loss: 0.936148  [   64/  118]
train() client id: f_00009-7-2 loss: 0.914590  [   96/  118]
train() client id: f_00009-8-0 loss: 1.022069  [   32/  118]
train() client id: f_00009-8-1 loss: 0.818968  [   64/  118]
train() client id: f_00009-8-2 loss: 0.820805  [   96/  118]
train() client id: f_00009-9-0 loss: 0.804636  [   32/  118]
train() client id: f_00009-9-1 loss: 1.102013  [   64/  118]
train() client id: f_00009-9-2 loss: 0.907622  [   96/  118]
train() client id: f_00009-10-0 loss: 0.798117  [   32/  118]
train() client id: f_00009-10-1 loss: 1.016720  [   64/  118]
train() client id: f_00009-10-2 loss: 0.859541  [   96/  118]
train() client id: f_00009-11-0 loss: 0.804329  [   32/  118]
train() client id: f_00009-11-1 loss: 0.875730  [   64/  118]
train() client id: f_00009-11-2 loss: 1.039438  [   96/  118]
train() client id: f_00009-12-0 loss: 0.817826  [   32/  118]
train() client id: f_00009-12-1 loss: 0.941912  [   64/  118]
train() client id: f_00009-12-2 loss: 0.769985  [   96/  118]
At round 0 accuracy: 0.4960212201591512
At round 0 training accuracy: 0.47283702213279677
At round 0 training loss: 0.9729029142440847
update_location
xs = 1.094342 -15.799682 20.009024 18.811294 -34.020704 -21.043590 2.556808 -6.324852 4.663977 -17.060879 
ys = 17.587959 15.555839 1.320614 17.544824 9.350187 -17.185849 2.375016 -14.177652 17.569006 4.001482 
xs mean: -4.711426168117426
ys mean: 5.394142535528706
dists_uav = 101.540799 102.428580 101.990711 103.255439 106.041663 103.625219 100.060871 101.197873 101.638687 101.523817 
uav_gains = -100.166034 -100.260551 -100.214037 -100.347849 -100.636950 -100.386663 -100.006623 -100.129304 -100.176496 -100.164218 
uav_gains_db_mean: -100.2488725258736
dists_bs = 236.194343 225.315738 261.137988 249.709732 217.520166 246.247796 247.640476 253.455104 238.880017 232.777274 
bs_gains = -106.018605 -105.445222 -107.239421 -106.695252 -105.017046 -106.525485 -106.594065 -106.876288 -106.156094 -105.841395 
bs_gains_db_mean: -106.24088731229632
Round 1
-------------------------------
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86475814 22.70386864 10.69549308  3.82673858 26.17607366 12.62512831
  4.75712203 15.35951406 11.26321529 10.24142851]
obj_prev = 128.51334030366516
eta_min = 2.311289869413348e-09	eta_max = 0.9179068358889176
af = 27.184654844112664	bf = 2.038244896144109	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.038244896144109	zeta = 50.92141160980498	eta = 0.5338550913006942
af = 27.184654844112664	bf = 2.038244896144109	zeta = 40.97475341706718	eta = 0.6634488941863763
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.19821510905128	eta = 0.693517670855259
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.11235513228073	eta = 0.6950400903288041
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.11214097403846	eta = 0.6950438960157429
eta = 0.6950438960157429
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [0.03001828 0.06313368 0.0295418  0.01024433 0.07290153 0.03478308
 0.01286498 0.04264501 0.03097124 0.02811235]
ene_total = [3.31978164 6.5061149  3.27744721 1.51901348 7.38134405 3.96296862
 1.74998819 4.47161385 3.59899574 3.32487331]
ti_comp = [0.26460659 0.24673409 0.26447904 0.26412078 0.24854123 0.24180084
 0.26502657 0.26470385 0.24355148 0.24498963]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.41454867e-05 2.58347910e-04 2.30360589e-05 9.63220790e-07
 3.92006301e-04 4.49850892e-05 1.89464606e-06 6.91773975e-05
 3.13021490e-05 2.31353594e-05]
ene_total = [0.58155701 0.76609949 0.58262017 0.58387567 0.76180371 0.79165776
 0.57569161 0.58478017 0.77442671 0.76055195]
optimize_network iter = 0 obj = 6.763064260913875
eta = 0.6950438960157429
freqs = [5.67224657e+07 1.27938699e+08 5.58490306e+07 1.93932720e+07
 1.46658825e+08 7.19250640e+07 2.42711055e+07 8.05523076e+07
 6.35825283e+07 5.73745792e+07]
eta_min = 0.6596845988039914	eta_max = 0.6950438960157337
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 0.07315436494122368	eta = 0.9090909090909091
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 22.49041741434538	eta = 0.0029569912778037635
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.432729834459089	eta = 0.02733717784291988
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.3416402766936963	eta = 0.028400591154114474
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.3415910621216036	eta = 0.028401188065745816
eta = 0.028401188065745816
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.36378699e-04 2.52916596e-03 2.25517659e-04 9.42970754e-06
 3.83765052e-03 4.40393562e-04 1.85481443e-05 6.77230634e-04
 3.06440760e-04 2.26489788e-04]
ene_total = [0.18861483 0.30552365 0.1886691  0.18350388 0.33783864 0.25980791
 0.18116974 0.20096826 0.2509534  0.24454163]
ti_comp = [0.30264569 0.28477319 0.30251814 0.30215989 0.28658033 0.27983995
 0.30306567 0.30274295 0.28159058 0.28302874]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.76000760e-05 2.90006048e-04 2.63287260e-05 1.10052366e-06
 4.40899051e-04 5.02234785e-05 2.16658346e-06 7.90821828e-05
 3.50155897e-05 2.59210505e-05]
ene_total = [0.52141495 0.68909041 0.52235439 0.52322131 0.68665088 0.70983183
 0.51589861 0.52483088 0.69426636 0.68175732]
optimize_network iter = 1 obj = 6.069316940771649
eta = 0.6596845988039914
freqs = [5.67111680e+07 1.26759149e+08 5.58345224e+07 1.93849279e+07
 1.45447947e+08 7.10682616e+07 2.42711055e+07 8.05399831e+07
 6.28865503e+07 5.67915891e+07]
eta_min = 0.659684598803993	eta_max = 0.6596845988039878
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 0.072022402521563	eta = 0.9090909090909091
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 22.489338538927885	eta = 0.002911375595591902
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.427629129214501	eta = 0.026970722420201213
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.3378597367303393	eta = 0.028006347153576604
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.337812514296872	eta = 0.028006912865265216
eta = 0.028006912865265216
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.37212058e-04 2.49249066e-03 2.26285293e-04 9.45857837e-06
 3.78935809e-03 4.31651518e-04 1.86209531e-05 6.79681002e-04
 3.00945552e-04 2.22781478e-04]
ene_total = [0.18856219 0.30434926 0.18861456 0.18343028 0.3363184  0.25945213
 0.18109834 0.20095692 0.25069421 0.24433622]
ti_comp = [0.30264569 0.28477319 0.30251814 0.30215989 0.28658033 0.27983995
 0.30306567 0.30274295 0.28159058 0.28302874]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.76000760e-05 2.90006048e-04 2.63287260e-05 1.10052366e-06
 4.40899051e-04 5.02234785e-05 2.16658346e-06 7.90821828e-05
 3.50155897e-05 2.59210505e-05]
ene_total = [0.52141495 0.68909041 0.52235439 0.52322131 0.68665088 0.70983183
 0.51589861 0.52483088 0.69426636 0.68175732]
optimize_network iter = 2 obj = 6.069316940771676
eta = 0.659684598803993
freqs = [5.67111680e+07 1.26759149e+08 5.58345224e+07 1.93849279e+07
 1.45447947e+08 7.10682616e+07 2.42711055e+07 8.05399831e+07
 6.28865503e+07 5.67915891e+07]
Done!
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.63403224e-05 2.76769267e-04 2.51270008e-05 1.05029232e-06
 4.20775042e-04 4.79311221e-05 2.06769383e-06 7.54726250e-05
 3.34173689e-05 2.47379328e-05]
ene_total = [0.00637249 0.00841017 0.00638403 0.00639578 0.00837346 0.00867465
 0.00630622 0.0064119  0.00848508 0.00833258]
At round 1 energy consumption: 0.07414635829145368
At round 1 eta: 0.659684598803993
At round 1 a_n: 27.840057009285058
At round 1 local rounds: 13.621738685881681
At round 1 global rounds: 81.80663264560978
gradient difference: 0.27550625801086426
train() client id: f_00000-0-0 loss: 1.006027  [   32/  126]
train() client id: f_00000-0-1 loss: 0.887076  [   64/  126]
train() client id: f_00000-0-2 loss: 1.174992  [   96/  126]
train() client id: f_00000-1-0 loss: 1.051220  [   32/  126]
train() client id: f_00000-1-1 loss: 1.088112  [   64/  126]
train() client id: f_00000-1-2 loss: 0.855575  [   96/  126]
train() client id: f_00000-2-0 loss: 0.967063  [   32/  126]
train() client id: f_00000-2-1 loss: 0.839599  [   64/  126]
train() client id: f_00000-2-2 loss: 0.979409  [   96/  126]
train() client id: f_00000-3-0 loss: 0.736189  [   32/  126]
train() client id: f_00000-3-1 loss: 0.841931  [   64/  126]
train() client id: f_00000-3-2 loss: 1.121157  [   96/  126]
train() client id: f_00000-4-0 loss: 0.840984  [   32/  126]
train() client id: f_00000-4-1 loss: 0.742452  [   64/  126]
train() client id: f_00000-4-2 loss: 0.976048  [   96/  126]
train() client id: f_00000-5-0 loss: 0.992333  [   32/  126]
train() client id: f_00000-5-1 loss: 0.764179  [   64/  126]
train() client id: f_00000-5-2 loss: 0.653840  [   96/  126]
train() client id: f_00000-6-0 loss: 0.685691  [   32/  126]
train() client id: f_00000-6-1 loss: 0.876021  [   64/  126]
train() client id: f_00000-6-2 loss: 0.879992  [   96/  126]
train() client id: f_00000-7-0 loss: 0.720104  [   32/  126]
train() client id: f_00000-7-1 loss: 0.723915  [   64/  126]
train() client id: f_00000-7-2 loss: 0.775272  [   96/  126]
train() client id: f_00000-8-0 loss: 0.863511  [   32/  126]
train() client id: f_00000-8-1 loss: 0.756987  [   64/  126]
train() client id: f_00000-8-2 loss: 0.795429  [   96/  126]
train() client id: f_00000-9-0 loss: 0.763434  [   32/  126]
train() client id: f_00000-9-1 loss: 0.699080  [   64/  126]
train() client id: f_00000-9-2 loss: 1.050948  [   96/  126]
train() client id: f_00000-10-0 loss: 0.807504  [   32/  126]
train() client id: f_00000-10-1 loss: 0.835150  [   64/  126]
train() client id: f_00000-10-2 loss: 0.803267  [   96/  126]
train() client id: f_00000-11-0 loss: 0.807012  [   32/  126]
train() client id: f_00000-11-1 loss: 0.672187  [   64/  126]
train() client id: f_00000-11-2 loss: 0.719227  [   96/  126]
train() client id: f_00000-12-0 loss: 0.858950  [   32/  126]
train() client id: f_00000-12-1 loss: 0.814558  [   64/  126]
train() client id: f_00000-12-2 loss: 0.758076  [   96/  126]
train() client id: f_00001-0-0 loss: 0.863345  [   32/  265]
train() client id: f_00001-0-1 loss: 0.801582  [   64/  265]
train() client id: f_00001-0-2 loss: 0.826920  [   96/  265]
train() client id: f_00001-0-3 loss: 0.889746  [  128/  265]
train() client id: f_00001-0-4 loss: 0.784433  [  160/  265]
train() client id: f_00001-0-5 loss: 0.745094  [  192/  265]
train() client id: f_00001-0-6 loss: 0.734976  [  224/  265]
train() client id: f_00001-0-7 loss: 0.743712  [  256/  265]
train() client id: f_00001-1-0 loss: 0.681305  [   32/  265]
train() client id: f_00001-1-1 loss: 0.764451  [   64/  265]
train() client id: f_00001-1-2 loss: 0.771588  [   96/  265]
train() client id: f_00001-1-3 loss: 0.692590  [  128/  265]
train() client id: f_00001-1-4 loss: 0.699078  [  160/  265]
train() client id: f_00001-1-5 loss: 0.720138  [  192/  265]
train() client id: f_00001-1-6 loss: 0.725663  [  224/  265]
train() client id: f_00001-1-7 loss: 0.770527  [  256/  265]
train() client id: f_00001-2-0 loss: 0.690370  [   32/  265]
train() client id: f_00001-2-1 loss: 0.687419  [   64/  265]
train() client id: f_00001-2-2 loss: 0.622692  [   96/  265]
train() client id: f_00001-2-3 loss: 0.774623  [  128/  265]
train() client id: f_00001-2-4 loss: 0.625803  [  160/  265]
train() client id: f_00001-2-5 loss: 0.676147  [  192/  265]
train() client id: f_00001-2-6 loss: 0.743878  [  224/  265]
train() client id: f_00001-2-7 loss: 0.605700  [  256/  265]
train() client id: f_00001-3-0 loss: 0.627421  [   32/  265]
train() client id: f_00001-3-1 loss: 0.687108  [   64/  265]
train() client id: f_00001-3-2 loss: 0.655133  [   96/  265]
train() client id: f_00001-3-3 loss: 0.612729  [  128/  265]
train() client id: f_00001-3-4 loss: 0.661985  [  160/  265]
train() client id: f_00001-3-5 loss: 0.616386  [  192/  265]
train() client id: f_00001-3-6 loss: 0.640506  [  224/  265]
train() client id: f_00001-3-7 loss: 0.595324  [  256/  265]
train() client id: f_00001-4-0 loss: 0.619549  [   32/  265]
train() client id: f_00001-4-1 loss: 0.630348  [   64/  265]
train() client id: f_00001-4-2 loss: 0.664355  [   96/  265]
train() client id: f_00001-4-3 loss: 0.525019  [  128/  265]
train() client id: f_00001-4-4 loss: 0.567656  [  160/  265]
train() client id: f_00001-4-5 loss: 0.628545  [  192/  265]
train() client id: f_00001-4-6 loss: 0.558523  [  224/  265]
train() client id: f_00001-4-7 loss: 0.664601  [  256/  265]
train() client id: f_00001-5-0 loss: 0.579035  [   32/  265]
train() client id: f_00001-5-1 loss: 0.573130  [   64/  265]
train() client id: f_00001-5-2 loss: 0.543918  [   96/  265]
train() client id: f_00001-5-3 loss: 0.692222  [  128/  265]
train() client id: f_00001-5-4 loss: 0.565609  [  160/  265]
train() client id: f_00001-5-5 loss: 0.679454  [  192/  265]
train() client id: f_00001-5-6 loss: 0.539758  [  224/  265]
train() client id: f_00001-5-7 loss: 0.573224  [  256/  265]
train() client id: f_00001-6-0 loss: 0.692200  [   32/  265]
train() client id: f_00001-6-1 loss: 0.554001  [   64/  265]
train() client id: f_00001-6-2 loss: 0.497419  [   96/  265]
train() client id: f_00001-6-3 loss: 0.568264  [  128/  265]
train() client id: f_00001-6-4 loss: 0.597990  [  160/  265]
train() client id: f_00001-6-5 loss: 0.548179  [  192/  265]
train() client id: f_00001-6-6 loss: 0.499983  [  224/  265]
train() client id: f_00001-6-7 loss: 0.643584  [  256/  265]
train() client id: f_00001-7-0 loss: 0.556950  [   32/  265]
train() client id: f_00001-7-1 loss: 0.464764  [   64/  265]
train() client id: f_00001-7-2 loss: 0.556608  [   96/  265]
train() client id: f_00001-7-3 loss: 0.564266  [  128/  265]
train() client id: f_00001-7-4 loss: 0.511536  [  160/  265]
train() client id: f_00001-7-5 loss: 0.571572  [  192/  265]
train() client id: f_00001-7-6 loss: 0.634209  [  224/  265]
train() client id: f_00001-7-7 loss: 0.657954  [  256/  265]
train() client id: f_00001-8-0 loss: 0.583429  [   32/  265]
train() client id: f_00001-8-1 loss: 0.644492  [   64/  265]
train() client id: f_00001-8-2 loss: 0.523662  [   96/  265]
train() client id: f_00001-8-3 loss: 0.551035  [  128/  265]
train() client id: f_00001-8-4 loss: 0.567262  [  160/  265]
train() client id: f_00001-8-5 loss: 0.482381  [  192/  265]
train() client id: f_00001-8-6 loss: 0.574673  [  224/  265]
train() client id: f_00001-8-7 loss: 0.560965  [  256/  265]
train() client id: f_00001-9-0 loss: 0.571645  [   32/  265]
train() client id: f_00001-9-1 loss: 0.634324  [   64/  265]
train() client id: f_00001-9-2 loss: 0.478187  [   96/  265]
train() client id: f_00001-9-3 loss: 0.528647  [  128/  265]
train() client id: f_00001-9-4 loss: 0.583440  [  160/  265]
train() client id: f_00001-9-5 loss: 0.468587  [  192/  265]
train() client id: f_00001-9-6 loss: 0.487208  [  224/  265]
train() client id: f_00001-9-7 loss: 0.618208  [  256/  265]
train() client id: f_00001-10-0 loss: 0.493461  [   32/  265]
train() client id: f_00001-10-1 loss: 0.553378  [   64/  265]
train() client id: f_00001-10-2 loss: 0.567542  [   96/  265]
train() client id: f_00001-10-3 loss: 0.487356  [  128/  265]
train() client id: f_00001-10-4 loss: 0.571280  [  160/  265]
train() client id: f_00001-10-5 loss: 0.606375  [  192/  265]
train() client id: f_00001-10-6 loss: 0.540156  [  224/  265]
train() client id: f_00001-10-7 loss: 0.593809  [  256/  265]
train() client id: f_00001-11-0 loss: 0.492819  [   32/  265]
train() client id: f_00001-11-1 loss: 0.532550  [   64/  265]
train() client id: f_00001-11-2 loss: 0.581268  [   96/  265]
train() client id: f_00001-11-3 loss: 0.534144  [  128/  265]
train() client id: f_00001-11-4 loss: 0.478661  [  160/  265]
train() client id: f_00001-11-5 loss: 0.648045  [  192/  265]
train() client id: f_00001-11-6 loss: 0.605030  [  224/  265]
train() client id: f_00001-11-7 loss: 0.496406  [  256/  265]
train() client id: f_00001-12-0 loss: 0.504417  [   32/  265]
train() client id: f_00001-12-1 loss: 0.556753  [   64/  265]
train() client id: f_00001-12-2 loss: 0.537653  [   96/  265]
train() client id: f_00001-12-3 loss: 0.578042  [  128/  265]
train() client id: f_00001-12-4 loss: 0.526966  [  160/  265]
train() client id: f_00001-12-5 loss: 0.520927  [  192/  265]
train() client id: f_00001-12-6 loss: 0.549631  [  224/  265]
train() client id: f_00001-12-7 loss: 0.541631  [  256/  265]
train() client id: f_00002-0-0 loss: 1.092393  [   32/  124]
train() client id: f_00002-0-1 loss: 1.133983  [   64/  124]
train() client id: f_00002-0-2 loss: 0.983951  [   96/  124]
train() client id: f_00002-1-0 loss: 1.030570  [   32/  124]
train() client id: f_00002-1-1 loss: 0.969961  [   64/  124]
train() client id: f_00002-1-2 loss: 1.026103  [   96/  124]
train() client id: f_00002-2-0 loss: 0.885715  [   32/  124]
train() client id: f_00002-2-1 loss: 1.008312  [   64/  124]
train() client id: f_00002-2-2 loss: 0.993305  [   96/  124]
train() client id: f_00002-3-0 loss: 0.870336  [   32/  124]
train() client id: f_00002-3-1 loss: 0.945830  [   64/  124]
train() client id: f_00002-3-2 loss: 0.841210  [   96/  124]
train() client id: f_00002-4-0 loss: 0.850628  [   32/  124]
train() client id: f_00002-4-1 loss: 0.899412  [   64/  124]
train() client id: f_00002-4-2 loss: 0.969325  [   96/  124]
train() client id: f_00002-5-0 loss: 0.965458  [   32/  124]
train() client id: f_00002-5-1 loss: 0.830664  [   64/  124]
train() client id: f_00002-5-2 loss: 0.847176  [   96/  124]
train() client id: f_00002-6-0 loss: 0.855314  [   32/  124]
train() client id: f_00002-6-1 loss: 0.864931  [   64/  124]
train() client id: f_00002-6-2 loss: 0.864400  [   96/  124]
train() client id: f_00002-7-0 loss: 0.908878  [   32/  124]
train() client id: f_00002-7-1 loss: 0.941635  [   64/  124]
train() client id: f_00002-7-2 loss: 0.786823  [   96/  124]
train() client id: f_00002-8-0 loss: 0.807164  [   32/  124]
train() client id: f_00002-8-1 loss: 0.802432  [   64/  124]
train() client id: f_00002-8-2 loss: 0.885865  [   96/  124]
train() client id: f_00002-9-0 loss: 0.974100  [   32/  124]
train() client id: f_00002-9-1 loss: 0.847258  [   64/  124]
train() client id: f_00002-9-2 loss: 0.781257  [   96/  124]
train() client id: f_00002-10-0 loss: 0.769850  [   32/  124]
train() client id: f_00002-10-1 loss: 0.990210  [   64/  124]
train() client id: f_00002-10-2 loss: 0.692003  [   96/  124]
train() client id: f_00002-11-0 loss: 0.754423  [   32/  124]
train() client id: f_00002-11-1 loss: 0.812125  [   64/  124]
train() client id: f_00002-11-2 loss: 0.791148  [   96/  124]
train() client id: f_00002-12-0 loss: 0.687997  [   32/  124]
train() client id: f_00002-12-1 loss: 0.749421  [   64/  124]
train() client id: f_00002-12-2 loss: 0.923633  [   96/  124]
train() client id: f_00003-0-0 loss: 1.009590  [   32/   43]
train() client id: f_00003-1-0 loss: 1.061373  [   32/   43]
train() client id: f_00003-2-0 loss: 1.083861  [   32/   43]
train() client id: f_00003-3-0 loss: 0.971801  [   32/   43]
train() client id: f_00003-4-0 loss: 0.985297  [   32/   43]
train() client id: f_00003-5-0 loss: 1.017780  [   32/   43]
train() client id: f_00003-6-0 loss: 1.014497  [   32/   43]
train() client id: f_00003-7-0 loss: 1.026525  [   32/   43]
train() client id: f_00003-8-0 loss: 0.970721  [   32/   43]
train() client id: f_00003-9-0 loss: 1.029934  [   32/   43]
train() client id: f_00003-10-0 loss: 1.006382  [   32/   43]
train() client id: f_00003-11-0 loss: 1.008539  [   32/   43]
train() client id: f_00003-12-0 loss: 1.012620  [   32/   43]
train() client id: f_00004-0-0 loss: 1.107792  [   32/  306]
train() client id: f_00004-0-1 loss: 1.048076  [   64/  306]
train() client id: f_00004-0-2 loss: 1.086842  [   96/  306]
train() client id: f_00004-0-3 loss: 1.123739  [  128/  306]
train() client id: f_00004-0-4 loss: 1.103216  [  160/  306]
train() client id: f_00004-0-5 loss: 1.041117  [  192/  306]
train() client id: f_00004-0-6 loss: 1.017202  [  224/  306]
train() client id: f_00004-0-7 loss: 1.093858  [  256/  306]
train() client id: f_00004-0-8 loss: 1.114931  [  288/  306]
train() client id: f_00004-1-0 loss: 1.056720  [   32/  306]
train() client id: f_00004-1-1 loss: 1.099130  [   64/  306]
train() client id: f_00004-1-2 loss: 1.015303  [   96/  306]
train() client id: f_00004-1-3 loss: 1.074621  [  128/  306]
train() client id: f_00004-1-4 loss: 1.071932  [  160/  306]
train() client id: f_00004-1-5 loss: 1.090597  [  192/  306]
train() client id: f_00004-1-6 loss: 1.047743  [  224/  306]
train() client id: f_00004-1-7 loss: 1.032859  [  256/  306]
train() client id: f_00004-1-8 loss: 1.065538  [  288/  306]
train() client id: f_00004-2-0 loss: 1.045537  [   32/  306]
train() client id: f_00004-2-1 loss: 1.167257  [   64/  306]
train() client id: f_00004-2-2 loss: 0.987802  [   96/  306]
train() client id: f_00004-2-3 loss: 1.031817  [  128/  306]
train() client id: f_00004-2-4 loss: 1.007071  [  160/  306]
train() client id: f_00004-2-5 loss: 1.082428  [  192/  306]
train() client id: f_00004-2-6 loss: 0.998134  [  224/  306]
train() client id: f_00004-2-7 loss: 1.002411  [  256/  306]
train() client id: f_00004-2-8 loss: 1.028345  [  288/  306]
train() client id: f_00004-3-0 loss: 0.970975  [   32/  306]
train() client id: f_00004-3-1 loss: 1.002861  [   64/  306]
train() client id: f_00004-3-2 loss: 1.039500  [   96/  306]
train() client id: f_00004-3-3 loss: 1.015755  [  128/  306]
train() client id: f_00004-3-4 loss: 1.036688  [  160/  306]
train() client id: f_00004-3-5 loss: 1.049661  [  192/  306]
train() client id: f_00004-3-6 loss: 1.062720  [  224/  306]
train() client id: f_00004-3-7 loss: 0.993894  [  256/  306]
train() client id: f_00004-3-8 loss: 1.019986  [  288/  306]
train() client id: f_00004-4-0 loss: 1.046173  [   32/  306]
train() client id: f_00004-4-1 loss: 0.946762  [   64/  306]
train() client id: f_00004-4-2 loss: 1.009472  [   96/  306]
train() client id: f_00004-4-3 loss: 0.982706  [  128/  306]
train() client id: f_00004-4-4 loss: 1.003326  [  160/  306]
train() client id: f_00004-4-5 loss: 1.072246  [  192/  306]
train() client id: f_00004-4-6 loss: 1.048692  [  224/  306]
train() client id: f_00004-4-7 loss: 0.964309  [  256/  306]
train() client id: f_00004-4-8 loss: 0.952250  [  288/  306]
train() client id: f_00004-5-0 loss: 0.910953  [   32/  306]
train() client id: f_00004-5-1 loss: 0.963677  [   64/  306]
train() client id: f_00004-5-2 loss: 1.021924  [   96/  306]
train() client id: f_00004-5-3 loss: 1.072519  [  128/  306]
train() client id: f_00004-5-4 loss: 1.023143  [  160/  306]
train() client id: f_00004-5-5 loss: 0.990313  [  192/  306]
train() client id: f_00004-5-6 loss: 1.060513  [  224/  306]
train() client id: f_00004-5-7 loss: 0.972773  [  256/  306]
train() client id: f_00004-5-8 loss: 0.984763  [  288/  306]
train() client id: f_00004-6-0 loss: 0.893537  [   32/  306]
train() client id: f_00004-6-1 loss: 1.044971  [   64/  306]
train() client id: f_00004-6-2 loss: 0.999640  [   96/  306]
train() client id: f_00004-6-3 loss: 1.052258  [  128/  306]
train() client id: f_00004-6-4 loss: 0.981754  [  160/  306]
train() client id: f_00004-6-5 loss: 1.004621  [  192/  306]
train() client id: f_00004-6-6 loss: 0.959244  [  224/  306]
train() client id: f_00004-6-7 loss: 0.972123  [  256/  306]
train() client id: f_00004-6-8 loss: 0.956605  [  288/  306]
train() client id: f_00004-7-0 loss: 1.004035  [   32/  306]
train() client id: f_00004-7-1 loss: 0.954978  [   64/  306]
train() client id: f_00004-7-2 loss: 0.994946  [   96/  306]
train() client id: f_00004-7-3 loss: 1.019887  [  128/  306]
train() client id: f_00004-7-4 loss: 1.009628  [  160/  306]
train() client id: f_00004-7-5 loss: 0.982060  [  192/  306]
train() client id: f_00004-7-6 loss: 1.009928  [  224/  306]
train() client id: f_00004-7-7 loss: 0.941540  [  256/  306]
train() client id: f_00004-7-8 loss: 0.953464  [  288/  306]
train() client id: f_00004-8-0 loss: 1.040873  [   32/  306]
train() client id: f_00004-8-1 loss: 0.965389  [   64/  306]
train() client id: f_00004-8-2 loss: 0.975639  [   96/  306]
train() client id: f_00004-8-3 loss: 0.959511  [  128/  306]
train() client id: f_00004-8-4 loss: 0.959375  [  160/  306]
train() client id: f_00004-8-5 loss: 1.003358  [  192/  306]
train() client id: f_00004-8-6 loss: 0.990397  [  224/  306]
train() client id: f_00004-8-7 loss: 0.938846  [  256/  306]
train() client id: f_00004-8-8 loss: 0.956750  [  288/  306]
train() client id: f_00004-9-0 loss: 1.024241  [   32/  306]
train() client id: f_00004-9-1 loss: 0.998829  [   64/  306]
train() client id: f_00004-9-2 loss: 0.992359  [   96/  306]
train() client id: f_00004-9-3 loss: 0.987735  [  128/  306]
train() client id: f_00004-9-4 loss: 0.961890  [  160/  306]
train() client id: f_00004-9-5 loss: 0.966587  [  192/  306]
train() client id: f_00004-9-6 loss: 0.924076  [  224/  306]
train() client id: f_00004-9-7 loss: 0.974764  [  256/  306]
train() client id: f_00004-9-8 loss: 0.938513  [  288/  306]
train() client id: f_00004-10-0 loss: 0.995030  [   32/  306]
train() client id: f_00004-10-1 loss: 0.958190  [   64/  306]
train() client id: f_00004-10-2 loss: 0.989713  [   96/  306]
train() client id: f_00004-10-3 loss: 0.946674  [  128/  306]
train() client id: f_00004-10-4 loss: 1.027422  [  160/  306]
train() client id: f_00004-10-5 loss: 0.982756  [  192/  306]
train() client id: f_00004-10-6 loss: 0.976459  [  224/  306]
train() client id: f_00004-10-7 loss: 0.953256  [  256/  306]
train() client id: f_00004-10-8 loss: 0.951145  [  288/  306]
train() client id: f_00004-11-0 loss: 0.920451  [   32/  306]
train() client id: f_00004-11-1 loss: 0.960500  [   64/  306]
train() client id: f_00004-11-2 loss: 1.046463  [   96/  306]
train() client id: f_00004-11-3 loss: 0.987724  [  128/  306]
train() client id: f_00004-11-4 loss: 1.018295  [  160/  306]
train() client id: f_00004-11-5 loss: 0.878613  [  192/  306]
train() client id: f_00004-11-6 loss: 0.996669  [  224/  306]
train() client id: f_00004-11-7 loss: 0.951011  [  256/  306]
train() client id: f_00004-11-8 loss: 0.996503  [  288/  306]
train() client id: f_00004-12-0 loss: 0.982243  [   32/  306]
train() client id: f_00004-12-1 loss: 0.903362  [   64/  306]
train() client id: f_00004-12-2 loss: 1.008130  [   96/  306]
train() client id: f_00004-12-3 loss: 0.934952  [  128/  306]
train() client id: f_00004-12-4 loss: 0.967014  [  160/  306]
train() client id: f_00004-12-5 loss: 0.989187  [  192/  306]
train() client id: f_00004-12-6 loss: 0.913286  [  224/  306]
train() client id: f_00004-12-7 loss: 1.013192  [  256/  306]
train() client id: f_00004-12-8 loss: 1.050398  [  288/  306]
train() client id: f_00005-0-0 loss: 0.933580  [   32/  146]
train() client id: f_00005-0-1 loss: 0.731015  [   64/  146]
train() client id: f_00005-0-2 loss: 0.741106  [   96/  146]
train() client id: f_00005-0-3 loss: 0.749128  [  128/  146]
train() client id: f_00005-1-0 loss: 0.932152  [   32/  146]
train() client id: f_00005-1-1 loss: 0.673092  [   64/  146]
train() client id: f_00005-1-2 loss: 0.726108  [   96/  146]
train() client id: f_00005-1-3 loss: 0.763130  [  128/  146]
train() client id: f_00005-2-0 loss: 0.707408  [   32/  146]
train() client id: f_00005-2-1 loss: 0.784241  [   64/  146]
train() client id: f_00005-2-2 loss: 0.751101  [   96/  146]
train() client id: f_00005-2-3 loss: 0.708192  [  128/  146]
train() client id: f_00005-3-0 loss: 0.686812  [   32/  146]
train() client id: f_00005-3-1 loss: 0.651172  [   64/  146]
train() client id: f_00005-3-2 loss: 0.741093  [   96/  146]
train() client id: f_00005-3-3 loss: 0.828919  [  128/  146]
train() client id: f_00005-4-0 loss: 0.792212  [   32/  146]
train() client id: f_00005-4-1 loss: 0.690407  [   64/  146]
train() client id: f_00005-4-2 loss: 0.735192  [   96/  146]
train() client id: f_00005-4-3 loss: 0.529408  [  128/  146]
train() client id: f_00005-5-0 loss: 0.694306  [   32/  146]
train() client id: f_00005-5-1 loss: 0.588126  [   64/  146]
train() client id: f_00005-5-2 loss: 0.562840  [   96/  146]
train() client id: f_00005-5-3 loss: 0.690757  [  128/  146]
train() client id: f_00005-6-0 loss: 0.605790  [   32/  146]
train() client id: f_00005-6-1 loss: 0.852181  [   64/  146]
train() client id: f_00005-6-2 loss: 0.596678  [   96/  146]
train() client id: f_00005-6-3 loss: 0.558454  [  128/  146]
train() client id: f_00005-7-0 loss: 0.648745  [   32/  146]
train() client id: f_00005-7-1 loss: 0.626451  [   64/  146]
train() client id: f_00005-7-2 loss: 0.760037  [   96/  146]
train() client id: f_00005-7-3 loss: 0.590297  [  128/  146]
train() client id: f_00005-8-0 loss: 0.616388  [   32/  146]
train() client id: f_00005-8-1 loss: 0.728817  [   64/  146]
train() client id: f_00005-8-2 loss: 0.475657  [   96/  146]
train() client id: f_00005-8-3 loss: 0.735876  [  128/  146]
train() client id: f_00005-9-0 loss: 0.704133  [   32/  146]
train() client id: f_00005-9-1 loss: 0.508391  [   64/  146]
train() client id: f_00005-9-2 loss: 0.729560  [   96/  146]
train() client id: f_00005-9-3 loss: 0.668715  [  128/  146]
train() client id: f_00005-10-0 loss: 0.768886  [   32/  146]
train() client id: f_00005-10-1 loss: 0.660079  [   64/  146]
train() client id: f_00005-10-2 loss: 0.516382  [   96/  146]
train() client id: f_00005-10-3 loss: 0.512806  [  128/  146]
train() client id: f_00005-11-0 loss: 0.605676  [   32/  146]
train() client id: f_00005-11-1 loss: 0.698535  [   64/  146]
train() client id: f_00005-11-2 loss: 0.637136  [   96/  146]
train() client id: f_00005-11-3 loss: 0.525642  [  128/  146]
train() client id: f_00005-12-0 loss: 0.478095  [   32/  146]
train() client id: f_00005-12-1 loss: 0.741873  [   64/  146]
train() client id: f_00005-12-2 loss: 0.699397  [   96/  146]
train() client id: f_00005-12-3 loss: 0.558361  [  128/  146]
train() client id: f_00006-0-0 loss: 0.942855  [   32/   54]
train() client id: f_00006-1-0 loss: 0.922484  [   32/   54]
train() client id: f_00006-2-0 loss: 0.925002  [   32/   54]
train() client id: f_00006-3-0 loss: 0.935037  [   32/   54]
train() client id: f_00006-4-0 loss: 0.936360  [   32/   54]
train() client id: f_00006-5-0 loss: 0.933434  [   32/   54]
train() client id: f_00006-6-0 loss: 0.929219  [   32/   54]
train() client id: f_00006-7-0 loss: 0.905812  [   32/   54]
train() client id: f_00006-8-0 loss: 0.902135  [   32/   54]
train() client id: f_00006-9-0 loss: 0.933388  [   32/   54]
train() client id: f_00006-10-0 loss: 0.959112  [   32/   54]
train() client id: f_00006-11-0 loss: 0.953157  [   32/   54]
train() client id: f_00006-12-0 loss: 0.936605  [   32/   54]
train() client id: f_00007-0-0 loss: 0.718469  [   32/  179]
train() client id: f_00007-0-1 loss: 0.846570  [   64/  179]
train() client id: f_00007-0-2 loss: 0.768879  [   96/  179]
train() client id: f_00007-0-3 loss: 0.842126  [  128/  179]
train() client id: f_00007-0-4 loss: 0.708510  [  160/  179]
train() client id: f_00007-1-0 loss: 0.737713  [   32/  179]
train() client id: f_00007-1-1 loss: 0.638258  [   64/  179]
train() client id: f_00007-1-2 loss: 0.752253  [   96/  179]
train() client id: f_00007-1-3 loss: 0.655280  [  128/  179]
train() client id: f_00007-1-4 loss: 0.876342  [  160/  179]
train() client id: f_00007-2-0 loss: 0.689319  [   32/  179]
train() client id: f_00007-2-1 loss: 0.602934  [   64/  179]
train() client id: f_00007-2-2 loss: 0.703745  [   96/  179]
train() client id: f_00007-2-3 loss: 0.685461  [  128/  179]
train() client id: f_00007-2-4 loss: 0.621872  [  160/  179]
train() client id: f_00007-3-0 loss: 0.619080  [   32/  179]
train() client id: f_00007-3-1 loss: 0.781633  [   64/  179]
train() client id: f_00007-3-2 loss: 0.548610  [   96/  179]
train() client id: f_00007-3-3 loss: 0.659172  [  128/  179]
train() client id: f_00007-3-4 loss: 0.634718  [  160/  179]
train() client id: f_00007-4-0 loss: 0.497557  [   32/  179]
train() client id: f_00007-4-1 loss: 0.661469  [   64/  179]
train() client id: f_00007-4-2 loss: 0.580974  [   96/  179]
train() client id: f_00007-4-3 loss: 0.646538  [  128/  179]
train() client id: f_00007-4-4 loss: 0.613427  [  160/  179]
train() client id: f_00007-5-0 loss: 0.643669  [   32/  179]
train() client id: f_00007-5-1 loss: 0.588475  [   64/  179]
train() client id: f_00007-5-2 loss: 0.546359  [   96/  179]
train() client id: f_00007-5-3 loss: 0.554965  [  128/  179]
train() client id: f_00007-5-4 loss: 0.597217  [  160/  179]
train() client id: f_00007-6-0 loss: 0.551467  [   32/  179]
train() client id: f_00007-6-1 loss: 0.555518  [   64/  179]
train() client id: f_00007-6-2 loss: 0.578372  [   96/  179]
train() client id: f_00007-6-3 loss: 0.652551  [  128/  179]
train() client id: f_00007-6-4 loss: 0.471159  [  160/  179]
train() client id: f_00007-7-0 loss: 0.652948  [   32/  179]
train() client id: f_00007-7-1 loss: 0.522346  [   64/  179]
train() client id: f_00007-7-2 loss: 0.677593  [   96/  179]
train() client id: f_00007-7-3 loss: 0.480410  [  128/  179]
train() client id: f_00007-7-4 loss: 0.505716  [  160/  179]
train() client id: f_00007-8-0 loss: 0.699393  [   32/  179]
train() client id: f_00007-8-1 loss: 0.503202  [   64/  179]
train() client id: f_00007-8-2 loss: 0.541431  [   96/  179]
train() client id: f_00007-8-3 loss: 0.507196  [  128/  179]
train() client id: f_00007-8-4 loss: 0.468800  [  160/  179]
train() client id: f_00007-9-0 loss: 0.504898  [   32/  179]
train() client id: f_00007-9-1 loss: 0.803303  [   64/  179]
train() client id: f_00007-9-2 loss: 0.444039  [   96/  179]
train() client id: f_00007-9-3 loss: 0.523627  [  128/  179]
train() client id: f_00007-9-4 loss: 0.471203  [  160/  179]
train() client id: f_00007-10-0 loss: 0.590432  [   32/  179]
train() client id: f_00007-10-1 loss: 0.410628  [   64/  179]
train() client id: f_00007-10-2 loss: 0.650374  [   96/  179]
train() client id: f_00007-10-3 loss: 0.507157  [  128/  179]
train() client id: f_00007-10-4 loss: 0.438859  [  160/  179]
train() client id: f_00007-11-0 loss: 0.518041  [   32/  179]
train() client id: f_00007-11-1 loss: 0.583362  [   64/  179]
train() client id: f_00007-11-2 loss: 0.462489  [   96/  179]
train() client id: f_00007-11-3 loss: 0.526119  [  128/  179]
train() client id: f_00007-11-4 loss: 0.597067  [  160/  179]
train() client id: f_00007-12-0 loss: 0.642396  [   32/  179]
train() client id: f_00007-12-1 loss: 0.489983  [   64/  179]
train() client id: f_00007-12-2 loss: 0.428730  [   96/  179]
train() client id: f_00007-12-3 loss: 0.444213  [  128/  179]
train() client id: f_00007-12-4 loss: 0.604736  [  160/  179]
train() client id: f_00008-0-0 loss: 0.956663  [   32/  130]
train() client id: f_00008-0-1 loss: 1.070837  [   64/  130]
train() client id: f_00008-0-2 loss: 0.956133  [   96/  130]
train() client id: f_00008-0-3 loss: 0.918709  [  128/  130]
train() client id: f_00008-1-0 loss: 0.940214  [   32/  130]
train() client id: f_00008-1-1 loss: 0.987139  [   64/  130]
train() client id: f_00008-1-2 loss: 0.923026  [   96/  130]
train() client id: f_00008-1-3 loss: 1.009609  [  128/  130]
train() client id: f_00008-2-0 loss: 0.959938  [   32/  130]
train() client id: f_00008-2-1 loss: 0.947171  [   64/  130]
train() client id: f_00008-2-2 loss: 0.891337  [   96/  130]
train() client id: f_00008-2-3 loss: 0.983850  [  128/  130]
train() client id: f_00008-3-0 loss: 0.928852  [   32/  130]
train() client id: f_00008-3-1 loss: 0.860661  [   64/  130]
train() client id: f_00008-3-2 loss: 1.043112  [   96/  130]
train() client id: f_00008-3-3 loss: 0.891479  [  128/  130]
train() client id: f_00008-4-0 loss: 0.923546  [   32/  130]
train() client id: f_00008-4-1 loss: 0.919385  [   64/  130]
train() client id: f_00008-4-2 loss: 0.890719  [   96/  130]
train() client id: f_00008-4-3 loss: 0.959557  [  128/  130]
train() client id: f_00008-5-0 loss: 0.902727  [   32/  130]
train() client id: f_00008-5-1 loss: 0.868006  [   64/  130]
train() client id: f_00008-5-2 loss: 0.943980  [   96/  130]
train() client id: f_00008-5-3 loss: 0.963806  [  128/  130]
train() client id: f_00008-6-0 loss: 0.957433  [   32/  130]
train() client id: f_00008-6-1 loss: 0.880206  [   64/  130]
train() client id: f_00008-6-2 loss: 0.864433  [   96/  130]
train() client id: f_00008-6-3 loss: 0.934793  [  128/  130]
train() client id: f_00008-7-0 loss: 0.872287  [   32/  130]
train() client id: f_00008-7-1 loss: 0.920599  [   64/  130]
train() client id: f_00008-7-2 loss: 0.888254  [   96/  130]
train() client id: f_00008-7-3 loss: 0.921935  [  128/  130]
train() client id: f_00008-8-0 loss: 0.911482  [   32/  130]
train() client id: f_00008-8-1 loss: 0.933130  [   64/  130]
train() client id: f_00008-8-2 loss: 0.854325  [   96/  130]
train() client id: f_00008-8-3 loss: 0.902304  [  128/  130]
train() client id: f_00008-9-0 loss: 0.860312  [   32/  130]
train() client id: f_00008-9-1 loss: 0.929747  [   64/  130]
train() client id: f_00008-9-2 loss: 0.993666  [   96/  130]
train() client id: f_00008-9-3 loss: 0.804400  [  128/  130]
train() client id: f_00008-10-0 loss: 0.866947  [   32/  130]
train() client id: f_00008-10-1 loss: 0.936873  [   64/  130]
train() client id: f_00008-10-2 loss: 0.864480  [   96/  130]
train() client id: f_00008-10-3 loss: 0.931441  [  128/  130]
train() client id: f_00008-11-0 loss: 0.857076  [   32/  130]
train() client id: f_00008-11-1 loss: 0.893000  [   64/  130]
train() client id: f_00008-11-2 loss: 0.947956  [   96/  130]
train() client id: f_00008-11-3 loss: 0.888330  [  128/  130]
train() client id: f_00008-12-0 loss: 0.850146  [   32/  130]
train() client id: f_00008-12-1 loss: 0.840873  [   64/  130]
train() client id: f_00008-12-2 loss: 0.952382  [   96/  130]
train() client id: f_00008-12-3 loss: 0.919147  [  128/  130]
train() client id: f_00009-0-0 loss: 1.107491  [   32/  118]
train() client id: f_00009-0-1 loss: 0.981396  [   64/  118]
train() client id: f_00009-0-2 loss: 1.027639  [   96/  118]
train() client id: f_00009-1-0 loss: 1.058638  [   32/  118]
train() client id: f_00009-1-1 loss: 0.915703  [   64/  118]
train() client id: f_00009-1-2 loss: 1.007760  [   96/  118]
train() client id: f_00009-2-0 loss: 0.955601  [   32/  118]
train() client id: f_00009-2-1 loss: 1.022036  [   64/  118]
train() client id: f_00009-2-2 loss: 0.952659  [   96/  118]
train() client id: f_00009-3-0 loss: 0.866761  [   32/  118]
train() client id: f_00009-3-1 loss: 1.006178  [   64/  118]
train() client id: f_00009-3-2 loss: 0.952114  [   96/  118]
train() client id: f_00009-4-0 loss: 0.946571  [   32/  118]
train() client id: f_00009-4-1 loss: 0.983713  [   64/  118]
train() client id: f_00009-4-2 loss: 0.956239  [   96/  118]
train() client id: f_00009-5-0 loss: 1.073502  [   32/  118]
train() client id: f_00009-5-1 loss: 0.925689  [   64/  118]
train() client id: f_00009-5-2 loss: 0.834523  [   96/  118]
train() client id: f_00009-6-0 loss: 0.871058  [   32/  118]
train() client id: f_00009-6-1 loss: 0.883430  [   64/  118]
train() client id: f_00009-6-2 loss: 1.008683  [   96/  118]
train() client id: f_00009-7-0 loss: 0.878386  [   32/  118]
train() client id: f_00009-7-1 loss: 1.013173  [   64/  118]
train() client id: f_00009-7-2 loss: 0.890746  [   96/  118]
train() client id: f_00009-8-0 loss: 0.885742  [   32/  118]
train() client id: f_00009-8-1 loss: 0.952502  [   64/  118]
train() client id: f_00009-8-2 loss: 0.811360  [   96/  118]
train() client id: f_00009-9-0 loss: 0.824296  [   32/  118]
train() client id: f_00009-9-1 loss: 0.897219  [   64/  118]
train() client id: f_00009-9-2 loss: 0.928540  [   96/  118]
train() client id: f_00009-10-0 loss: 0.764212  [   32/  118]
train() client id: f_00009-10-1 loss: 1.033896  [   64/  118]
train() client id: f_00009-10-2 loss: 0.932066  [   96/  118]
train() client id: f_00009-11-0 loss: 0.901299  [   32/  118]
train() client id: f_00009-11-1 loss: 0.793301  [   64/  118]
train() client id: f_00009-11-2 loss: 1.043162  [   96/  118]
train() client id: f_00009-12-0 loss: 0.980115  [   32/  118]
train() client id: f_00009-12-1 loss: 0.897972  [   64/  118]
train() client id: f_00009-12-2 loss: 0.867052  [   96/  118]
At round 1 accuracy: 0.6286472148541115
At round 1 training accuracy: 0.5526492287055668
At round 1 training loss: 0.917468927176359
update_location
xs = -3.905658 -10.799682 25.009024 18.811294 -29.020704 -16.043590 2.556808 -6.324852 9.663977 -12.060879 
ys = 17.587959 15.555839 1.320614 12.544824 9.350187 -17.185849 -2.624984 -9.177652 17.569006 4.001482 
xs mean: -2.211426168117426
ys mean: 4.894142535528705
dists_uav = 101.609992 101.777292 103.088289 102.524326 104.544858 102.726580 100.067117 100.619248 101.990502 100.804150 
uav_gains = -100.173430 -100.191293 -100.330258 -100.270696 -100.482597 -100.292094 -100.007301 -100.067044 -100.214014 -100.086978 
uav_gains_db_mean: -100.21157055839473
dists_bs = 232.490482 228.875916 264.892693 252.892273 220.793604 249.404372 251.151459 249.745296 242.662940 236.198329 
bs_gains = -105.826404 -105.635862 -107.413019 -106.849255 -105.198681 -106.680373 -106.765259 -106.696984 -106.347156 -106.018810 
bs_gains_db_mean: -106.34318019714516
Round 2
-------------------------------
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.73113195 22.42682549 10.56475866  3.77907907 25.85610411 12.47188502
  4.69859503 15.17007359 11.12712296 10.11765294]
obj_prev = 126.94322881666007
eta_min = 1.8621250529904804e-09	eta_max = 0.9180913765363099
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 29.535190418159765	eta = 0.9090909090909091
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 50.41602139462105	eta = 0.5325722332838158
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 40.52128855107027	eta = 0.6626189360582114
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.752941123532246	eta = 0.6928551054183996
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.66719520504412	eta = 0.6943915369355615
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.6669798224976	eta = 0.694395404830551
eta = 0.694395404830551
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [0.0300953  0.06329568 0.0296176  0.01027062 0.07308859 0.03487234
 0.01289799 0.04275444 0.03105071 0.02818449]
ene_total = [3.27908415 6.43447607 3.23987046 1.49764204 7.29869581 3.92072634
 1.72777469 4.41559147 3.56239059 3.2907282 ]
ti_comp = [0.26882441 0.25014105 0.26840555 0.26856527 0.25202169 0.24528326
 0.26926224 0.26910548 0.24689208 0.24842218]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.35742980e-05 2.53298284e-04 2.25396212e-05 9.38793768e-07
 3.84196142e-04 4.40541962e-05 1.84967284e-06 6.74494733e-05
 3.06958933e-05 2.26740888e-05]
ene_total = [0.57330776 0.76208585 0.57698348 0.57360274 0.75694218 0.78696767
 0.5674136  0.57472659 0.77128999 0.75680081]
optimize_network iter = 0 obj = 6.700120669142431
eta = 0.694395404830551
freqs = [5.59757626e+07 1.26519974e+08 5.51732252e+07 1.91212720e+07
 1.45004571e+08 7.10858434e+07 2.39506050e+07 7.94380680e+07
 6.28831617e+07 5.67270011e+07]
eta_min = 0.6637595643039359	eta_max = 0.6943954048305468
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 0.07059948298279546	eta = 0.909090909090909
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 22.33326679569504	eta = 0.0028738002708385358
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.406693151543204	eta = 0.026667856733220745
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.3186248233022884	eta = 0.02768078195366155
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.3185795201975767	eta = 0.02768132281299047
eta = 0.02768132281299047
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.31855663e-04 2.49121488e-03 2.21679510e-04 9.23313400e-06
 3.77860887e-03 4.33277584e-04 1.81917241e-05 6.63372559e-04
 3.01897292e-04 2.23002013e-04]
ene_total = [0.18608375 0.30281643 0.18698052 0.18052076 0.33390587 0.25835522
 0.17880307 0.19749234 0.25008995 0.2435316 ]
ti_comp = [0.30213693 0.28345356 0.30171807 0.30187779 0.28533421 0.27859578
 0.30257476 0.302418   0.2802046  0.2817347 ]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.64813595e-05 2.79904609e-04 2.53103811e-05 1.05433666e-06
 4.25297547e-04 4.84557995e-05 2.07851021e-06 7.57844495e-05
 3.38154992e-05 2.50150645e-05]
ene_total = [0.52130977 0.69482585 0.52463943 0.52134959 0.69133623 0.71562472
 0.51573361 0.52304321 0.70127064 0.68803794]
optimize_network iter = 1 obj = 6.097170992212053
eta = 0.6637595643039359
freqs = [5.59657273e+07 1.25464071e+08 5.51538439e+07 1.91158103e+07
 1.43920617e+08 7.03288863e+07 2.39506050e+07 7.94329736e+07
 6.22620637e+07 5.62078659e+07]
eta_min = 0.6637595643039447	eta_max = 0.6637595643039276
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 0.06961124159514927	eta = 0.909090909090909
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 22.33232490105157	eta = 0.0028336927384439636
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.4022158004882264	eta = 0.026343572834638552
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.3153036098319895	eta = 0.027332461555343614
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.315259971692997	eta = 0.02733297671898439
eta = 0.02733297671898439
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.32590817e-04 2.45845542e-03 2.22305891e-04 9.26043939e-06
 3.73546925e-03 4.25596504e-04 1.82559506e-05 6.65629235e-04
 2.97007961e-04 2.19712069e-04]
ene_total = [0.18603789 0.30178187 0.18693126 0.18045688 0.33256672 0.25804555
 0.17874085 0.19748541 0.24986216 0.24335138]
ti_comp = [0.30213693 0.28345356 0.30171807 0.30187779 0.28533421 0.27859578
 0.30257476 0.302418   0.2802046  0.2817347 ]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.64813595e-05 2.79904609e-04 2.53103811e-05 1.05433666e-06
 4.25297547e-04 4.84557995e-05 2.07851021e-06 7.57844495e-05
 3.38154992e-05 2.50150645e-05]
ene_total = [0.52130977 0.69482585 0.52463943 0.52134959 0.69133623 0.71562472
 0.51573361 0.52304321 0.70127064 0.68803794]
optimize_network iter = 2 obj = 6.09717099221221
eta = 0.6637595643039447
freqs = [5.59657273e+07 1.25464071e+08 5.51538439e+07 1.91158103e+07
 1.43920617e+08 7.03288863e+07 2.39506050e+07 7.94329736e+07
 6.22620637e+07 5.62078659e+07]
Done!
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.56524120e-05 2.71142739e-04 2.45180888e-05 1.02133270e-06
 4.11984434e-04 4.69389849e-05 2.01344649e-06 7.34121646e-05
 3.27569707e-05 2.42320165e-05]
ene_total = [0.00637376 0.00848759 0.00641452 0.00637505 0.00844037 0.00874916
 0.00630634 0.00639342 0.0085741  0.00841257]
At round 2 energy consumption: 0.07452687465895509
At round 2 eta: 0.6637595643039447
At round 2 a_n: 27.49751116231574
At round 2 local rounds: 13.420089837852764
At round 2 global rounds: 81.77931100223806
gradient difference: 0.3272774815559387
train() client id: f_00000-0-0 loss: 0.890634  [   32/  126]
train() client id: f_00000-0-1 loss: 0.941159  [   64/  126]
train() client id: f_00000-0-2 loss: 1.064759  [   96/  126]
train() client id: f_00000-1-0 loss: 0.831518  [   32/  126]
train() client id: f_00000-1-1 loss: 0.985250  [   64/  126]
train() client id: f_00000-1-2 loss: 0.852665  [   96/  126]
train() client id: f_00000-2-0 loss: 0.846995  [   32/  126]
train() client id: f_00000-2-1 loss: 1.119698  [   64/  126]
train() client id: f_00000-2-2 loss: 0.825982  [   96/  126]
train() client id: f_00000-3-0 loss: 0.709206  [   32/  126]
train() client id: f_00000-3-1 loss: 0.861658  [   64/  126]
train() client id: f_00000-3-2 loss: 0.901173  [   96/  126]
train() client id: f_00000-4-0 loss: 0.971115  [   32/  126]
train() client id: f_00000-4-1 loss: 0.695772  [   64/  126]
train() client id: f_00000-4-2 loss: 0.953486  [   96/  126]
train() client id: f_00000-5-0 loss: 0.956552  [   32/  126]
train() client id: f_00000-5-1 loss: 0.703769  [   64/  126]
train() client id: f_00000-5-2 loss: 0.811180  [   96/  126]
train() client id: f_00000-6-0 loss: 0.771991  [   32/  126]
train() client id: f_00000-6-1 loss: 0.736522  [   64/  126]
train() client id: f_00000-6-2 loss: 0.688456  [   96/  126]
train() client id: f_00000-7-0 loss: 0.757803  [   32/  126]
train() client id: f_00000-7-1 loss: 0.895663  [   64/  126]
train() client id: f_00000-7-2 loss: 0.658007  [   96/  126]
train() client id: f_00000-8-0 loss: 0.735953  [   32/  126]
train() client id: f_00000-8-1 loss: 0.884212  [   64/  126]
train() client id: f_00000-8-2 loss: 0.805524  [   96/  126]
train() client id: f_00000-9-0 loss: 0.666144  [   32/  126]
train() client id: f_00000-9-1 loss: 0.746309  [   64/  126]
train() client id: f_00000-9-2 loss: 0.742650  [   96/  126]
train() client id: f_00000-10-0 loss: 0.640660  [   32/  126]
train() client id: f_00000-10-1 loss: 0.763483  [   64/  126]
train() client id: f_00000-10-2 loss: 0.960266  [   96/  126]
train() client id: f_00000-11-0 loss: 0.786442  [   32/  126]
train() client id: f_00000-11-1 loss: 0.691224  [   64/  126]
train() client id: f_00000-11-2 loss: 0.829919  [   96/  126]
train() client id: f_00000-12-0 loss: 0.715967  [   32/  126]
train() client id: f_00000-12-1 loss: 0.979360  [   64/  126]
train() client id: f_00000-12-2 loss: 0.639829  [   96/  126]
train() client id: f_00001-0-0 loss: 0.853026  [   32/  265]
train() client id: f_00001-0-1 loss: 0.790103  [   64/  265]
train() client id: f_00001-0-2 loss: 0.755437  [   96/  265]
train() client id: f_00001-0-3 loss: 0.692001  [  128/  265]
train() client id: f_00001-0-4 loss: 0.750958  [  160/  265]
train() client id: f_00001-0-5 loss: 0.739900  [  192/  265]
train() client id: f_00001-0-6 loss: 0.737532  [  224/  265]
train() client id: f_00001-0-7 loss: 0.715377  [  256/  265]
train() client id: f_00001-1-0 loss: 0.795264  [   32/  265]
train() client id: f_00001-1-1 loss: 0.683641  [   64/  265]
train() client id: f_00001-1-2 loss: 0.706137  [   96/  265]
train() client id: f_00001-1-3 loss: 0.687133  [  128/  265]
train() client id: f_00001-1-4 loss: 0.720565  [  160/  265]
train() client id: f_00001-1-5 loss: 0.733921  [  192/  265]
train() client id: f_00001-1-6 loss: 0.630190  [  224/  265]
train() client id: f_00001-1-7 loss: 0.602229  [  256/  265]
train() client id: f_00001-2-0 loss: 0.649374  [   32/  265]
train() client id: f_00001-2-1 loss: 0.749661  [   64/  265]
train() client id: f_00001-2-2 loss: 0.613533  [   96/  265]
train() client id: f_00001-2-3 loss: 0.627505  [  128/  265]
train() client id: f_00001-2-4 loss: 0.728965  [  160/  265]
train() client id: f_00001-2-5 loss: 0.690662  [  192/  265]
train() client id: f_00001-2-6 loss: 0.644693  [  224/  265]
train() client id: f_00001-2-7 loss: 0.616865  [  256/  265]
train() client id: f_00001-3-0 loss: 0.644588  [   32/  265]
train() client id: f_00001-3-1 loss: 0.638542  [   64/  265]
train() client id: f_00001-3-2 loss: 0.672378  [   96/  265]
train() client id: f_00001-3-3 loss: 0.733489  [  128/  265]
train() client id: f_00001-3-4 loss: 0.600543  [  160/  265]
train() client id: f_00001-3-5 loss: 0.586302  [  192/  265]
train() client id: f_00001-3-6 loss: 0.591834  [  224/  265]
train() client id: f_00001-3-7 loss: 0.631629  [  256/  265]
train() client id: f_00001-4-0 loss: 0.591395  [   32/  265]
train() client id: f_00001-4-1 loss: 0.619152  [   64/  265]
train() client id: f_00001-4-2 loss: 0.606731  [   96/  265]
train() client id: f_00001-4-3 loss: 0.628175  [  128/  265]
train() client id: f_00001-4-4 loss: 0.697598  [  160/  265]
train() client id: f_00001-4-5 loss: 0.572357  [  192/  265]
train() client id: f_00001-4-6 loss: 0.594073  [  224/  265]
train() client id: f_00001-4-7 loss: 0.624931  [  256/  265]
train() client id: f_00001-5-0 loss: 0.661526  [   32/  265]
train() client id: f_00001-5-1 loss: 0.586932  [   64/  265]
train() client id: f_00001-5-2 loss: 0.606606  [   96/  265]
train() client id: f_00001-5-3 loss: 0.640014  [  128/  265]
train() client id: f_00001-5-4 loss: 0.601951  [  160/  265]
train() client id: f_00001-5-5 loss: 0.533151  [  192/  265]
train() client id: f_00001-5-6 loss: 0.574311  [  224/  265]
train() client id: f_00001-5-7 loss: 0.570526  [  256/  265]
train() client id: f_00001-6-0 loss: 0.565702  [   32/  265]
train() client id: f_00001-6-1 loss: 0.544545  [   64/  265]
train() client id: f_00001-6-2 loss: 0.657903  [   96/  265]
train() client id: f_00001-6-3 loss: 0.715780  [  128/  265]
train() client id: f_00001-6-4 loss: 0.535601  [  160/  265]
train() client id: f_00001-6-5 loss: 0.604281  [  192/  265]
train() client id: f_00001-6-6 loss: 0.551322  [  224/  265]
train() client id: f_00001-6-7 loss: 0.586602  [  256/  265]
train() client id: f_00001-7-0 loss: 0.594502  [   32/  265]
train() client id: f_00001-7-1 loss: 0.521263  [   64/  265]
train() client id: f_00001-7-2 loss: 0.581593  [   96/  265]
train() client id: f_00001-7-3 loss: 0.633859  [  128/  265]
train() client id: f_00001-7-4 loss: 0.540889  [  160/  265]
train() client id: f_00001-7-5 loss: 0.540009  [  192/  265]
train() client id: f_00001-7-6 loss: 0.559242  [  224/  265]
train() client id: f_00001-7-7 loss: 0.631183  [  256/  265]
train() client id: f_00001-8-0 loss: 0.582756  [   32/  265]
train() client id: f_00001-8-1 loss: 0.563872  [   64/  265]
train() client id: f_00001-8-2 loss: 0.485985  [   96/  265]
train() client id: f_00001-8-3 loss: 0.608351  [  128/  265]
train() client id: f_00001-8-4 loss: 0.708723  [  160/  265]
train() client id: f_00001-8-5 loss: 0.551338  [  192/  265]
train() client id: f_00001-8-6 loss: 0.581840  [  224/  265]
train() client id: f_00001-8-7 loss: 0.614504  [  256/  265]
train() client id: f_00001-9-0 loss: 0.579026  [   32/  265]
train() client id: f_00001-9-1 loss: 0.623627  [   64/  265]
train() client id: f_00001-9-2 loss: 0.614769  [   96/  265]
train() client id: f_00001-9-3 loss: 0.629084  [  128/  265]
train() client id: f_00001-9-4 loss: 0.521444  [  160/  265]
train() client id: f_00001-9-5 loss: 0.646058  [  192/  265]
train() client id: f_00001-9-6 loss: 0.472422  [  224/  265]
train() client id: f_00001-9-7 loss: 0.576490  [  256/  265]
train() client id: f_00001-10-0 loss: 0.659541  [   32/  265]
train() client id: f_00001-10-1 loss: 0.610707  [   64/  265]
train() client id: f_00001-10-2 loss: 0.515708  [   96/  265]
train() client id: f_00001-10-3 loss: 0.622999  [  128/  265]
train() client id: f_00001-10-4 loss: 0.518670  [  160/  265]
train() client id: f_00001-10-5 loss: 0.547177  [  192/  265]
train() client id: f_00001-10-6 loss: 0.568774  [  224/  265]
train() client id: f_00001-10-7 loss: 0.567316  [  256/  265]
train() client id: f_00001-11-0 loss: 0.635860  [   32/  265]
train() client id: f_00001-11-1 loss: 0.638499  [   64/  265]
train() client id: f_00001-11-2 loss: 0.759065  [   96/  265]
train() client id: f_00001-11-3 loss: 0.526395  [  128/  265]
train() client id: f_00001-11-4 loss: 0.486205  [  160/  265]
train() client id: f_00001-11-5 loss: 0.523271  [  192/  265]
train() client id: f_00001-11-6 loss: 0.531910  [  224/  265]
train() client id: f_00001-11-7 loss: 0.501490  [  256/  265]
train() client id: f_00001-12-0 loss: 0.567888  [   32/  265]
train() client id: f_00001-12-1 loss: 0.498689  [   64/  265]
train() client id: f_00001-12-2 loss: 0.599022  [   96/  265]
train() client id: f_00001-12-3 loss: 0.537911  [  128/  265]
train() client id: f_00001-12-4 loss: 0.538374  [  160/  265]
train() client id: f_00001-12-5 loss: 0.692525  [  192/  265]
train() client id: f_00001-12-6 loss: 0.598226  [  224/  265]
train() client id: f_00001-12-7 loss: 0.523340  [  256/  265]
train() client id: f_00002-0-0 loss: 0.961094  [   32/  124]
train() client id: f_00002-0-1 loss: 1.069003  [   64/  124]
train() client id: f_00002-0-2 loss: 0.963911  [   96/  124]
train() client id: f_00002-1-0 loss: 1.096648  [   32/  124]
train() client id: f_00002-1-1 loss: 0.977623  [   64/  124]
train() client id: f_00002-1-2 loss: 0.948173  [   96/  124]
train() client id: f_00002-2-0 loss: 1.012390  [   32/  124]
train() client id: f_00002-2-1 loss: 0.917678  [   64/  124]
train() client id: f_00002-2-2 loss: 0.953660  [   96/  124]
train() client id: f_00002-3-0 loss: 0.945984  [   32/  124]
train() client id: f_00002-3-1 loss: 1.026883  [   64/  124]
train() client id: f_00002-3-2 loss: 0.849769  [   96/  124]
train() client id: f_00002-4-0 loss: 0.924676  [   32/  124]
train() client id: f_00002-4-1 loss: 0.963969  [   64/  124]
train() client id: f_00002-4-2 loss: 0.901790  [   96/  124]
train() client id: f_00002-5-0 loss: 1.036454  [   32/  124]
train() client id: f_00002-5-1 loss: 0.918999  [   64/  124]
train() client id: f_00002-5-2 loss: 0.815492  [   96/  124]
train() client id: f_00002-6-0 loss: 0.867014  [   32/  124]
train() client id: f_00002-6-1 loss: 0.836562  [   64/  124]
train() client id: f_00002-6-2 loss: 0.848628  [   96/  124]
train() client id: f_00002-7-0 loss: 0.838170  [   32/  124]
train() client id: f_00002-7-1 loss: 0.808211  [   64/  124]
train() client id: f_00002-7-2 loss: 0.780516  [   96/  124]
train() client id: f_00002-8-0 loss: 0.824336  [   32/  124]
train() client id: f_00002-8-1 loss: 0.760478  [   64/  124]
train() client id: f_00002-8-2 loss: 0.785612  [   96/  124]
train() client id: f_00002-9-0 loss: 0.771655  [   32/  124]
train() client id: f_00002-9-1 loss: 0.842121  [   64/  124]
train() client id: f_00002-9-2 loss: 0.843915  [   96/  124]
train() client id: f_00002-10-0 loss: 0.801990  [   32/  124]
train() client id: f_00002-10-1 loss: 0.939021  [   64/  124]
train() client id: f_00002-10-2 loss: 0.719183  [   96/  124]
train() client id: f_00002-11-0 loss: 0.738542  [   32/  124]
train() client id: f_00002-11-1 loss: 0.937438  [   64/  124]
train() client id: f_00002-11-2 loss: 0.775068  [   96/  124]
train() client id: f_00002-12-0 loss: 0.734419  [   32/  124]
train() client id: f_00002-12-1 loss: 0.877500  [   64/  124]
train() client id: f_00002-12-2 loss: 0.832465  [   96/  124]
train() client id: f_00003-0-0 loss: 1.034148  [   32/   43]
train() client id: f_00003-1-0 loss: 1.049273  [   32/   43]
train() client id: f_00003-2-0 loss: 1.055494  [   32/   43]
train() client id: f_00003-3-0 loss: 1.095669  [   32/   43]
train() client id: f_00003-4-0 loss: 1.044757  [   32/   43]
train() client id: f_00003-5-0 loss: 1.029544  [   32/   43]
train() client id: f_00003-6-0 loss: 1.082404  [   32/   43]
train() client id: f_00003-7-0 loss: 1.053938  [   32/   43]
train() client id: f_00003-8-0 loss: 0.992379  [   32/   43]
train() client id: f_00003-9-0 loss: 0.975714  [   32/   43]
train() client id: f_00003-10-0 loss: 1.079347  [   32/   43]
train() client id: f_00003-11-0 loss: 1.096817  [   32/   43]
train() client id: f_00003-12-0 loss: 1.067287  [   32/   43]
train() client id: f_00004-0-0 loss: 1.032811  [   32/  306]
train() client id: f_00004-0-1 loss: 1.050811  [   64/  306]
train() client id: f_00004-0-2 loss: 0.993621  [   96/  306]
train() client id: f_00004-0-3 loss: 1.021896  [  128/  306]
train() client id: f_00004-0-4 loss: 1.027251  [  160/  306]
train() client id: f_00004-0-5 loss: 0.889141  [  192/  306]
train() client id: f_00004-0-6 loss: 1.054611  [  224/  306]
train() client id: f_00004-0-7 loss: 0.934960  [  256/  306]
train() client id: f_00004-0-8 loss: 0.992497  [  288/  306]
train() client id: f_00004-1-0 loss: 1.047791  [   32/  306]
train() client id: f_00004-1-1 loss: 0.947987  [   64/  306]
train() client id: f_00004-1-2 loss: 0.966863  [   96/  306]
train() client id: f_00004-1-3 loss: 1.019598  [  128/  306]
train() client id: f_00004-1-4 loss: 0.957425  [  160/  306]
train() client id: f_00004-1-5 loss: 0.971959  [  192/  306]
train() client id: f_00004-1-6 loss: 0.959724  [  224/  306]
train() client id: f_00004-1-7 loss: 0.924559  [  256/  306]
train() client id: f_00004-1-8 loss: 0.986433  [  288/  306]
train() client id: f_00004-2-0 loss: 0.980514  [   32/  306]
train() client id: f_00004-2-1 loss: 0.988734  [   64/  306]
train() client id: f_00004-2-2 loss: 1.017082  [   96/  306]
train() client id: f_00004-2-3 loss: 1.047381  [  128/  306]
train() client id: f_00004-2-4 loss: 0.908410  [  160/  306]
train() client id: f_00004-2-5 loss: 0.948102  [  192/  306]
train() client id: f_00004-2-6 loss: 0.909797  [  224/  306]
train() client id: f_00004-2-7 loss: 0.918324  [  256/  306]
train() client id: f_00004-2-8 loss: 0.969636  [  288/  306]
train() client id: f_00004-3-0 loss: 1.011048  [   32/  306]
train() client id: f_00004-3-1 loss: 0.960628  [   64/  306]
train() client id: f_00004-3-2 loss: 0.965496  [   96/  306]
train() client id: f_00004-3-3 loss: 0.962325  [  128/  306]
train() client id: f_00004-3-4 loss: 0.907571  [  160/  306]
train() client id: f_00004-3-5 loss: 0.939078  [  192/  306]
train() client id: f_00004-3-6 loss: 0.922749  [  224/  306]
train() client id: f_00004-3-7 loss: 0.935504  [  256/  306]
train() client id: f_00004-3-8 loss: 0.954451  [  288/  306]
train() client id: f_00004-4-0 loss: 0.964729  [   32/  306]
train() client id: f_00004-4-1 loss: 0.952131  [   64/  306]
train() client id: f_00004-4-2 loss: 0.948843  [   96/  306]
train() client id: f_00004-4-3 loss: 0.967602  [  128/  306]
train() client id: f_00004-4-4 loss: 0.986971  [  160/  306]
train() client id: f_00004-4-5 loss: 0.884958  [  192/  306]
train() client id: f_00004-4-6 loss: 0.892653  [  224/  306]
train() client id: f_00004-4-7 loss: 0.922778  [  256/  306]
train() client id: f_00004-4-8 loss: 1.003487  [  288/  306]
train() client id: f_00004-5-0 loss: 0.950961  [   32/  306]
train() client id: f_00004-5-1 loss: 0.898755  [   64/  306]
train() client id: f_00004-5-2 loss: 0.895024  [   96/  306]
train() client id: f_00004-5-3 loss: 0.916218  [  128/  306]
train() client id: f_00004-5-4 loss: 0.933569  [  160/  306]
train() client id: f_00004-5-5 loss: 0.925707  [  192/  306]
train() client id: f_00004-5-6 loss: 0.953637  [  224/  306]
train() client id: f_00004-5-7 loss: 0.956665  [  256/  306]
train() client id: f_00004-5-8 loss: 0.948664  [  288/  306]
train() client id: f_00004-6-0 loss: 1.007025  [   32/  306]
train() client id: f_00004-6-1 loss: 0.897244  [   64/  306]
train() client id: f_00004-6-2 loss: 0.862447  [   96/  306]
train() client id: f_00004-6-3 loss: 0.907127  [  128/  306]
train() client id: f_00004-6-4 loss: 0.958798  [  160/  306]
train() client id: f_00004-6-5 loss: 0.890003  [  192/  306]
train() client id: f_00004-6-6 loss: 0.861363  [  224/  306]
train() client id: f_00004-6-7 loss: 0.975366  [  256/  306]
train() client id: f_00004-6-8 loss: 0.966187  [  288/  306]
train() client id: f_00004-7-0 loss: 0.951748  [   32/  306]
train() client id: f_00004-7-1 loss: 0.943932  [   64/  306]
train() client id: f_00004-7-2 loss: 0.972007  [   96/  306]
train() client id: f_00004-7-3 loss: 0.924514  [  128/  306]
train() client id: f_00004-7-4 loss: 0.949266  [  160/  306]
train() client id: f_00004-7-5 loss: 0.901114  [  192/  306]
train() client id: f_00004-7-6 loss: 0.853094  [  224/  306]
train() client id: f_00004-7-7 loss: 0.897788  [  256/  306]
train() client id: f_00004-7-8 loss: 0.900568  [  288/  306]
train() client id: f_00004-8-0 loss: 0.939898  [   32/  306]
train() client id: f_00004-8-1 loss: 0.936424  [   64/  306]
train() client id: f_00004-8-2 loss: 1.002651  [   96/  306]
train() client id: f_00004-8-3 loss: 0.915780  [  128/  306]
train() client id: f_00004-8-4 loss: 0.879258  [  160/  306]
train() client id: f_00004-8-5 loss: 0.863555  [  192/  306]
train() client id: f_00004-8-6 loss: 0.884349  [  224/  306]
train() client id: f_00004-8-7 loss: 0.864015  [  256/  306]
train() client id: f_00004-8-8 loss: 0.911638  [  288/  306]
train() client id: f_00004-9-0 loss: 0.912338  [   32/  306]
train() client id: f_00004-9-1 loss: 0.981315  [   64/  306]
train() client id: f_00004-9-2 loss: 0.858860  [   96/  306]
train() client id: f_00004-9-3 loss: 0.933285  [  128/  306]
train() client id: f_00004-9-4 loss: 0.843707  [  160/  306]
train() client id: f_00004-9-5 loss: 0.880037  [  192/  306]
train() client id: f_00004-9-6 loss: 0.895972  [  224/  306]
train() client id: f_00004-9-7 loss: 0.927746  [  256/  306]
train() client id: f_00004-9-8 loss: 1.008771  [  288/  306]
train() client id: f_00004-10-0 loss: 0.947228  [   32/  306]
train() client id: f_00004-10-1 loss: 0.920021  [   64/  306]
train() client id: f_00004-10-2 loss: 0.913760  [   96/  306]
train() client id: f_00004-10-3 loss: 0.899980  [  128/  306]
train() client id: f_00004-10-4 loss: 0.929194  [  160/  306]
train() client id: f_00004-10-5 loss: 0.885480  [  192/  306]
train() client id: f_00004-10-6 loss: 0.881414  [  224/  306]
train() client id: f_00004-10-7 loss: 0.910281  [  256/  306]
train() client id: f_00004-10-8 loss: 0.911757  [  288/  306]
train() client id: f_00004-11-0 loss: 0.953546  [   32/  306]
train() client id: f_00004-11-1 loss: 0.877443  [   64/  306]
train() client id: f_00004-11-2 loss: 0.808323  [   96/  306]
train() client id: f_00004-11-3 loss: 0.901953  [  128/  306]
train() client id: f_00004-11-4 loss: 0.953985  [  160/  306]
train() client id: f_00004-11-5 loss: 0.883238  [  192/  306]
train() client id: f_00004-11-6 loss: 0.893213  [  224/  306]
train() client id: f_00004-11-7 loss: 0.956567  [  256/  306]
train() client id: f_00004-11-8 loss: 0.935358  [  288/  306]
train() client id: f_00004-12-0 loss: 0.980422  [   32/  306]
train() client id: f_00004-12-1 loss: 0.944070  [   64/  306]
train() client id: f_00004-12-2 loss: 0.901069  [   96/  306]
train() client id: f_00004-12-3 loss: 0.945152  [  128/  306]
train() client id: f_00004-12-4 loss: 0.884145  [  160/  306]
train() client id: f_00004-12-5 loss: 0.861552  [  192/  306]
train() client id: f_00004-12-6 loss: 0.810018  [  224/  306]
train() client id: f_00004-12-7 loss: 0.896301  [  256/  306]
train() client id: f_00004-12-8 loss: 0.936983  [  288/  306]
train() client id: f_00005-0-0 loss: 0.925605  [   32/  146]
train() client id: f_00005-0-1 loss: 1.094077  [   64/  146]
train() client id: f_00005-0-2 loss: 0.990639  [   96/  146]
train() client id: f_00005-0-3 loss: 0.817797  [  128/  146]
train() client id: f_00005-1-0 loss: 0.821679  [   32/  146]
train() client id: f_00005-1-1 loss: 0.800348  [   64/  146]
train() client id: f_00005-1-2 loss: 1.011166  [   96/  146]
train() client id: f_00005-1-3 loss: 1.009872  [  128/  146]
train() client id: f_00005-2-0 loss: 0.937437  [   32/  146]
train() client id: f_00005-2-1 loss: 0.804220  [   64/  146]
train() client id: f_00005-2-2 loss: 0.883608  [   96/  146]
train() client id: f_00005-2-3 loss: 1.013003  [  128/  146]
train() client id: f_00005-3-0 loss: 0.912724  [   32/  146]
train() client id: f_00005-3-1 loss: 0.904230  [   64/  146]
train() client id: f_00005-3-2 loss: 0.835318  [   96/  146]
train() client id: f_00005-3-3 loss: 0.955890  [  128/  146]
train() client id: f_00005-4-0 loss: 0.895350  [   32/  146]
train() client id: f_00005-4-1 loss: 0.899927  [   64/  146]
train() client id: f_00005-4-2 loss: 0.910219  [   96/  146]
train() client id: f_00005-4-3 loss: 0.840721  [  128/  146]
train() client id: f_00005-5-0 loss: 0.817045  [   32/  146]
train() client id: f_00005-5-1 loss: 0.939486  [   64/  146]
train() client id: f_00005-5-2 loss: 0.834751  [   96/  146]
train() client id: f_00005-5-3 loss: 0.995718  [  128/  146]
train() client id: f_00005-6-0 loss: 1.096399  [   32/  146]
train() client id: f_00005-6-1 loss: 0.803408  [   64/  146]
train() client id: f_00005-6-2 loss: 0.859181  [   96/  146]
train() client id: f_00005-6-3 loss: 0.699974  [  128/  146]
train() client id: f_00005-7-0 loss: 0.895582  [   32/  146]
train() client id: f_00005-7-1 loss: 1.009820  [   64/  146]
train() client id: f_00005-7-2 loss: 0.762102  [   96/  146]
train() client id: f_00005-7-3 loss: 0.794072  [  128/  146]
train() client id: f_00005-8-0 loss: 0.869349  [   32/  146]
train() client id: f_00005-8-1 loss: 1.029188  [   64/  146]
train() client id: f_00005-8-2 loss: 0.821019  [   96/  146]
train() client id: f_00005-8-3 loss: 0.765386  [  128/  146]
train() client id: f_00005-9-0 loss: 0.983560  [   32/  146]
train() client id: f_00005-9-1 loss: 0.900187  [   64/  146]
train() client id: f_00005-9-2 loss: 0.835385  [   96/  146]
train() client id: f_00005-9-3 loss: 0.756709  [  128/  146]
train() client id: f_00005-10-0 loss: 0.743335  [   32/  146]
train() client id: f_00005-10-1 loss: 0.944314  [   64/  146]
train() client id: f_00005-10-2 loss: 1.045463  [   96/  146]
train() client id: f_00005-10-3 loss: 0.881146  [  128/  146]
train() client id: f_00005-11-0 loss: 1.000103  [   32/  146]
train() client id: f_00005-11-1 loss: 0.954991  [   64/  146]
train() client id: f_00005-11-2 loss: 0.758906  [   96/  146]
train() client id: f_00005-11-3 loss: 0.777193  [  128/  146]
train() client id: f_00005-12-0 loss: 0.807568  [   32/  146]
train() client id: f_00005-12-1 loss: 0.836985  [   64/  146]
train() client id: f_00005-12-2 loss: 0.729507  [   96/  146]
train() client id: f_00005-12-3 loss: 1.177160  [  128/  146]
train() client id: f_00006-0-0 loss: 0.857040  [   32/   54]
train() client id: f_00006-1-0 loss: 0.828933  [   32/   54]
train() client id: f_00006-2-0 loss: 0.832563  [   32/   54]
train() client id: f_00006-3-0 loss: 0.818632  [   32/   54]
train() client id: f_00006-4-0 loss: 0.825544  [   32/   54]
train() client id: f_00006-5-0 loss: 0.852099  [   32/   54]
train() client id: f_00006-6-0 loss: 0.856435  [   32/   54]
train() client id: f_00006-7-0 loss: 0.833248  [   32/   54]
train() client id: f_00006-8-0 loss: 0.869695  [   32/   54]
train() client id: f_00006-9-0 loss: 0.866621  [   32/   54]
train() client id: f_00006-10-0 loss: 0.862887  [   32/   54]
train() client id: f_00006-11-0 loss: 0.857738  [   32/   54]
train() client id: f_00006-12-0 loss: 0.789998  [   32/   54]
train() client id: f_00007-0-0 loss: 0.692558  [   32/  179]
train() client id: f_00007-0-1 loss: 0.798136  [   64/  179]
train() client id: f_00007-0-2 loss: 0.711126  [   96/  179]
train() client id: f_00007-0-3 loss: 0.855518  [  128/  179]
train() client id: f_00007-0-4 loss: 0.638984  [  160/  179]
train() client id: f_00007-1-0 loss: 0.701088  [   32/  179]
train() client id: f_00007-1-1 loss: 0.715778  [   64/  179]
train() client id: f_00007-1-2 loss: 0.684885  [   96/  179]
train() client id: f_00007-1-3 loss: 0.652131  [  128/  179]
train() client id: f_00007-1-4 loss: 0.753802  [  160/  179]
train() client id: f_00007-2-0 loss: 0.641699  [   32/  179]
train() client id: f_00007-2-1 loss: 0.704628  [   64/  179]
train() client id: f_00007-2-2 loss: 0.654068  [   96/  179]
train() client id: f_00007-2-3 loss: 0.679401  [  128/  179]
train() client id: f_00007-2-4 loss: 0.649979  [  160/  179]
train() client id: f_00007-3-0 loss: 0.575535  [   32/  179]
train() client id: f_00007-3-1 loss: 0.560891  [   64/  179]
train() client id: f_00007-3-2 loss: 0.603507  [   96/  179]
train() client id: f_00007-3-3 loss: 0.569856  [  128/  179]
train() client id: f_00007-3-4 loss: 0.793365  [  160/  179]
train() client id: f_00007-4-0 loss: 0.591045  [   32/  179]
train() client id: f_00007-4-1 loss: 0.529203  [   64/  179]
train() client id: f_00007-4-2 loss: 0.781084  [   96/  179]
train() client id: f_00007-4-3 loss: 0.579491  [  128/  179]
train() client id: f_00007-4-4 loss: 0.573470  [  160/  179]
train() client id: f_00007-5-0 loss: 0.642269  [   32/  179]
train() client id: f_00007-5-1 loss: 0.663504  [   64/  179]
train() client id: f_00007-5-2 loss: 0.574442  [   96/  179]
train() client id: f_00007-5-3 loss: 0.555203  [  128/  179]
train() client id: f_00007-5-4 loss: 0.554479  [  160/  179]
train() client id: f_00007-6-0 loss: 0.608602  [   32/  179]
train() client id: f_00007-6-1 loss: 0.701836  [   64/  179]
train() client id: f_00007-6-2 loss: 0.490247  [   96/  179]
train() client id: f_00007-6-3 loss: 0.588501  [  128/  179]
train() client id: f_00007-6-4 loss: 0.482209  [  160/  179]
train() client id: f_00007-7-0 loss: 0.483020  [   32/  179]
train() client id: f_00007-7-1 loss: 0.797723  [   64/  179]
train() client id: f_00007-7-2 loss: 0.595936  [   96/  179]
train() client id: f_00007-7-3 loss: 0.512208  [  128/  179]
train() client id: f_00007-7-4 loss: 0.545237  [  160/  179]
train() client id: f_00007-8-0 loss: 0.706625  [   32/  179]
train() client id: f_00007-8-1 loss: 0.474612  [   64/  179]
train() client id: f_00007-8-2 loss: 0.580274  [   96/  179]
train() client id: f_00007-8-3 loss: 0.574497  [  128/  179]
train() client id: f_00007-8-4 loss: 0.568946  [  160/  179]
train() client id: f_00007-9-0 loss: 0.661044  [   32/  179]
train() client id: f_00007-9-1 loss: 0.515462  [   64/  179]
train() client id: f_00007-9-2 loss: 0.566649  [   96/  179]
train() client id: f_00007-9-3 loss: 0.560003  [  128/  179]
train() client id: f_00007-9-4 loss: 0.496967  [  160/  179]
train() client id: f_00007-10-0 loss: 0.590575  [   32/  179]
train() client id: f_00007-10-1 loss: 0.817096  [   64/  179]
train() client id: f_00007-10-2 loss: 0.458543  [   96/  179]
train() client id: f_00007-10-3 loss: 0.581334  [  128/  179]
train() client id: f_00007-10-4 loss: 0.475123  [  160/  179]
train() client id: f_00007-11-0 loss: 0.476178  [   32/  179]
train() client id: f_00007-11-1 loss: 0.723819  [   64/  179]
train() client id: f_00007-11-2 loss: 0.541216  [   96/  179]
train() client id: f_00007-11-3 loss: 0.437308  [  128/  179]
train() client id: f_00007-11-4 loss: 0.633027  [  160/  179]
train() client id: f_00007-12-0 loss: 0.713443  [   32/  179]
train() client id: f_00007-12-1 loss: 0.437880  [   64/  179]
train() client id: f_00007-12-2 loss: 0.518654  [   96/  179]
train() client id: f_00007-12-3 loss: 0.590610  [  128/  179]
train() client id: f_00007-12-4 loss: 0.547600  [  160/  179]
train() client id: f_00008-0-0 loss: 0.963037  [   32/  130]
train() client id: f_00008-0-1 loss: 0.958040  [   64/  130]
train() client id: f_00008-0-2 loss: 0.884385  [   96/  130]
train() client id: f_00008-0-3 loss: 0.894182  [  128/  130]
train() client id: f_00008-1-0 loss: 0.925255  [   32/  130]
train() client id: f_00008-1-1 loss: 0.914802  [   64/  130]
train() client id: f_00008-1-2 loss: 0.976081  [   96/  130]
train() client id: f_00008-1-3 loss: 0.840579  [  128/  130]
train() client id: f_00008-2-0 loss: 0.933469  [   32/  130]
train() client id: f_00008-2-1 loss: 0.955402  [   64/  130]
train() client id: f_00008-2-2 loss: 0.888031  [   96/  130]
train() client id: f_00008-2-3 loss: 0.864709  [  128/  130]
train() client id: f_00008-3-0 loss: 0.902283  [   32/  130]
train() client id: f_00008-3-1 loss: 0.859277  [   64/  130]
train() client id: f_00008-3-2 loss: 0.896057  [   96/  130]
train() client id: f_00008-3-3 loss: 0.943585  [  128/  130]
train() client id: f_00008-4-0 loss: 0.907429  [   32/  130]
train() client id: f_00008-4-1 loss: 0.844720  [   64/  130]
train() client id: f_00008-4-2 loss: 0.905918  [   96/  130]
train() client id: f_00008-4-3 loss: 0.940529  [  128/  130]
train() client id: f_00008-5-0 loss: 0.936819  [   32/  130]
train() client id: f_00008-5-1 loss: 0.915005  [   64/  130]
train() client id: f_00008-5-2 loss: 0.889202  [   96/  130]
train() client id: f_00008-5-3 loss: 0.835167  [  128/  130]
train() client id: f_00008-6-0 loss: 0.949307  [   32/  130]
train() client id: f_00008-6-1 loss: 0.896632  [   64/  130]
train() client id: f_00008-6-2 loss: 0.869208  [   96/  130]
train() client id: f_00008-6-3 loss: 0.864853  [  128/  130]
train() client id: f_00008-7-0 loss: 0.825813  [   32/  130]
train() client id: f_00008-7-1 loss: 0.820988  [   64/  130]
train() client id: f_00008-7-2 loss: 0.972292  [   96/  130]
train() client id: f_00008-7-3 loss: 0.938032  [  128/  130]
train() client id: f_00008-8-0 loss: 0.888133  [   32/  130]
train() client id: f_00008-8-1 loss: 0.980716  [   64/  130]
train() client id: f_00008-8-2 loss: 0.902139  [   96/  130]
train() client id: f_00008-8-3 loss: 0.781645  [  128/  130]
train() client id: f_00008-9-0 loss: 0.942355  [   32/  130]
train() client id: f_00008-9-1 loss: 0.860360  [   64/  130]
train() client id: f_00008-9-2 loss: 0.879624  [   96/  130]
train() client id: f_00008-9-3 loss: 0.857339  [  128/  130]
train() client id: f_00008-10-0 loss: 0.909974  [   32/  130]
train() client id: f_00008-10-1 loss: 0.854239  [   64/  130]
train() client id: f_00008-10-2 loss: 0.916201  [   96/  130]
train() client id: f_00008-10-3 loss: 0.848902  [  128/  130]
train() client id: f_00008-11-0 loss: 0.937464  [   32/  130]
train() client id: f_00008-11-1 loss: 0.778135  [   64/  130]
train() client id: f_00008-11-2 loss: 0.885076  [   96/  130]
train() client id: f_00008-11-3 loss: 0.955416  [  128/  130]
train() client id: f_00008-12-0 loss: 0.796983  [   32/  130]
train() client id: f_00008-12-1 loss: 1.000137  [   64/  130]
train() client id: f_00008-12-2 loss: 0.882544  [   96/  130]
train() client id: f_00008-12-3 loss: 0.871160  [  128/  130]
train() client id: f_00009-0-0 loss: 1.029108  [   32/  118]
train() client id: f_00009-0-1 loss: 1.052881  [   64/  118]
train() client id: f_00009-0-2 loss: 1.051149  [   96/  118]
train() client id: f_00009-1-0 loss: 1.084491  [   32/  118]
train() client id: f_00009-1-1 loss: 1.067888  [   64/  118]
train() client id: f_00009-1-2 loss: 0.932612  [   96/  118]
train() client id: f_00009-2-0 loss: 1.007396  [   32/  118]
train() client id: f_00009-2-1 loss: 0.876467  [   64/  118]
train() client id: f_00009-2-2 loss: 0.980614  [   96/  118]
train() client id: f_00009-3-0 loss: 0.983762  [   32/  118]
train() client id: f_00009-3-1 loss: 0.898955  [   64/  118]
train() client id: f_00009-3-2 loss: 1.005501  [   96/  118]
train() client id: f_00009-4-0 loss: 0.954695  [   32/  118]
train() client id: f_00009-4-1 loss: 0.920411  [   64/  118]
train() client id: f_00009-4-2 loss: 0.941066  [   96/  118]
train() client id: f_00009-5-0 loss: 0.848636  [   32/  118]
train() client id: f_00009-5-1 loss: 0.889439  [   64/  118]
train() client id: f_00009-5-2 loss: 0.921725  [   96/  118]
train() client id: f_00009-6-0 loss: 0.998251  [   32/  118]
train() client id: f_00009-6-1 loss: 0.882797  [   64/  118]
train() client id: f_00009-6-2 loss: 0.787802  [   96/  118]
train() client id: f_00009-7-0 loss: 0.834194  [   32/  118]
train() client id: f_00009-7-1 loss: 0.902000  [   64/  118]
train() client id: f_00009-7-2 loss: 0.909000  [   96/  118]
train() client id: f_00009-8-0 loss: 0.826106  [   32/  118]
train() client id: f_00009-8-1 loss: 0.881299  [   64/  118]
train() client id: f_00009-8-2 loss: 0.806395  [   96/  118]
train() client id: f_00009-9-0 loss: 0.856985  [   32/  118]
train() client id: f_00009-9-1 loss: 0.973383  [   64/  118]
train() client id: f_00009-9-2 loss: 0.822208  [   96/  118]
train() client id: f_00009-10-0 loss: 1.059157  [   32/  118]
train() client id: f_00009-10-1 loss: 0.789674  [   64/  118]
train() client id: f_00009-10-2 loss: 0.781591  [   96/  118]
train() client id: f_00009-11-0 loss: 0.926360  [   32/  118]
train() client id: f_00009-11-1 loss: 0.928832  [   64/  118]
train() client id: f_00009-11-2 loss: 0.828102  [   96/  118]
train() client id: f_00009-12-0 loss: 0.980735  [   32/  118]
train() client id: f_00009-12-1 loss: 0.762242  [   64/  118]
train() client id: f_00009-12-2 loss: 0.892455  [   96/  118]
At round 2 accuracy: 0.6339522546419099
At round 2 training accuracy: 0.5687458081824279
At round 2 training loss: 0.893173686044177
update_location
xs = -3.905658 -5.799682 30.009024 18.811294 -24.020704 -11.043590 2.556808 -6.324852 14.663977 -7.060879 
ys = 22.587959 15.555839 1.320614 7.544824 9.350187 -17.185849 2.375016 -4.177652 17.569006 4.001482 
xs mean: 0.7885738318825737
ys mean: 5.894142535528706
dists_uav = 102.593714 101.368735 104.414010 102.033275 103.268680 102.065245 100.060871 100.286871 102.585097 100.328799 
uav_gains = -100.278042 -100.147620 -100.468999 -100.218567 -100.349241 -100.221968 -100.006623 -100.031119 -100.277130 -100.035657 
uav_gains_db_mean: -100.2034965671306
dists_bs = 229.134685 232.489114 268.687977 256.132883 224.130784 252.620476 247.640476 246.081159 246.489233 239.674867 
bs_gains = -105.649603 -105.826333 -107.586010 -107.004089 -105.381101 -106.836179 -106.594065 -106.517253 -106.537402 -106.196489 
bs_gains_db_mean: -106.41285223025574
Round 3
-------------------------------
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.59821531 22.14976829 10.43417884  3.73162013 25.53613015 12.31863725
  4.64005817 14.98083601 10.99100861  9.99386679]
obj_prev = 125.37431955255585
eta_min = 1.4920442712219943e-09	eta_max = 0.9182779578592448
af = 26.51569137072327	bf = 2.011304995713791	zeta = 29.1672605077956	eta = 0.9090909090909091
af = 26.51569137072327	bf = 2.011304995713791	zeta = 49.923723386827525	eta = 0.531124074325744
af = 26.51569137072327	bf = 2.011304995713791	zeta = 40.07321095605609	eta = 0.6616812263883753
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.31157890911957	eta = 0.692106462999664
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.22584319738169	eta = 0.6936587699009733
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.22562613755516	eta = 0.6936627087626083
eta = 0.6936627087626083
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [0.03018242 0.0634789  0.02970333 0.01030035 0.07330016 0.03497328
 0.01293532 0.0428782  0.03114059 0.02826607]
ene_total = [3.24068386 6.36286468 3.20278272 1.47684991 7.21615316 3.87841894
 1.70546401 4.36028936 3.52564504 3.25647446]
ti_comp = [0.27298014 0.25372925 0.27246494 0.27313894 0.25568174 0.24894535
 0.27369854 0.27363436 0.25041517 0.25203535]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.30610989e-05 2.48328765e-04 2.20634935e-05 9.15521600e-07
 3.76526319e-04 4.31400861e-05 1.80578817e-06 6.58033963e-05
 3.00981129e-05 2.22204868e-05]
ene_total = [0.56723758 0.75785384 0.57171611 0.56386684 0.75191015 0.7820715
 0.55898529 0.56522719 0.76788634 0.75282607]
optimize_network iter = 0 obj = 6.639580919785389
eta = 0.6936627087626083
freqs = [5.52831753e+07 1.25091797e+08 5.45085400e+07 1.88555117e+07
 1.43342577e+08 7.02428831e+07 2.36306014e+07 7.83494392e+07
 6.21779230e+07 5.60756135e+07]
eta_min = 0.6679443667432773	eta_max = 0.6936627087626027
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 0.06810536589768883	eta = 0.9090909090909091
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 22.189266299545174	eta = 0.002790266616394072
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.3821060289713065	eta = 0.025991273371083312
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.2969923954010767	eta = 0.026954363942109684
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.2969507848071644	eta = 0.02695485223602507
eta = 0.02695485223602507
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.27830406e-04 2.45334550e-03 2.17974637e-04 9.04482732e-06
 3.71986367e-03 4.26199261e-04 1.78401495e-05 6.50099743e-04
 2.97352060e-04 2.19525641e-04]
ene_total = [0.18429415 0.3001259  0.18545689 0.17774529 0.33002202 0.25690445
 0.17642907 0.19425277 0.24920683 0.24251343]
ti_comp = [0.30125093 0.28200003 0.30073572 0.30140973 0.28395252 0.27721613
 0.30196932 0.30190514 0.27868595 0.28030614]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.54305861e-05 2.69985736e-04 2.43218458e-05 1.00970000e-06
 4.09991014e-04 4.67221780e-05 1.99231109e-06 7.25971523e-05
 3.26363375e-05 2.41258334e-05]
ene_total = [0.52349772 0.70092769 0.52762029 0.52020198 0.69640997 0.72179151
 0.51570607 0.52200485 0.70861965 0.69467407]
optimize_network iter = 1 obj = 6.13145379564688
eta = 0.6679443667432773
freqs = [5.52695580e+07 1.24176825e+08 5.44854450e+07 1.88518958e+07
 1.42403130e+08 6.95950244e+07 2.36306014e+07 7.83477189e+07
 6.16413456e+07 5.56279733e+07]
eta_min = 0.6679443667432772	eta_max = 0.6679443667432751
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 0.06726955400122223	eta = 0.9090909090909091
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 22.188469685723824	eta = 0.002756122475650405
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.378298126804526	eta = 0.025713403762074868
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.294165378733591	eta = 0.026656378205336263
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.2941251141107752	eta = 0.02665684605646063
eta = 0.02665684605646063
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.28418763e-04 2.42502502e-03 2.18460004e-04 9.06917459e-06
 3.68255925e-03 4.19660878e-04 1.78950352e-05 6.52071157e-04
 2.93141173e-04 2.16699410e-04]
ene_total = [0.18425414 0.2992439  0.18541365 0.17769155 0.32888024 0.25664338
 0.17637659 0.19424829 0.24901305 0.24236033]
ti_comp = [0.30125093 0.28200003 0.30073572 0.30140973 0.28395252 0.27721613
 0.30196932 0.30190514 0.27868595 0.28030614]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.54305861e-05 2.69985736e-04 2.43218458e-05 1.00970000e-06
 4.09991014e-04 4.67221780e-05 1.99231109e-06 7.25971523e-05
 3.26363375e-05 2.41258334e-05]
ene_total = [0.52349772 0.70092769 0.52762029 0.52020198 0.69640997 0.72179151
 0.51570607 0.52200485 0.70861965 0.69467407]
optimize_network iter = 2 obj = 6.1314537956468795
eta = 0.6679443667432772
freqs = [5.52695580e+07 1.24176825e+08 5.44854450e+07 1.88518958e+07
 1.42403130e+08 6.95950244e+07 2.36306014e+07 7.83477189e+07
 6.16413456e+07 5.56279733e+07]
Done!
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.50181899e-05 2.65607500e-04 2.39274295e-05 9.93326158e-07
 4.03342377e-04 4.59645058e-05 1.96000269e-06 7.14198774e-05
 3.21070889e-05 2.37345958e-05]
ene_total = [0.00640101 0.00856669 0.00645144 0.0063611  0.00850917 0.00882543
 0.00630611 0.00638199 0.00866459 0.0084942 ]
At round 3 energy consumption: 0.07496174437238437
At round 3 eta: 0.6679443667432772
At round 3 a_n: 27.154965315346423
At round 3 local rounds: 13.214290123550223
At round 3 global rounds: 81.77836059884595
gradient difference: 0.34608274698257446
train() client id: f_00000-0-0 loss: 0.889526  [   32/  126]
train() client id: f_00000-0-1 loss: 1.168592  [   64/  126]
train() client id: f_00000-0-2 loss: 1.079787  [   96/  126]
train() client id: f_00000-1-0 loss: 1.095838  [   32/  126]
train() client id: f_00000-1-1 loss: 0.918211  [   64/  126]
train() client id: f_00000-1-2 loss: 1.014008  [   96/  126]
train() client id: f_00000-2-0 loss: 0.863120  [   32/  126]
train() client id: f_00000-2-1 loss: 1.152524  [   64/  126]
train() client id: f_00000-2-2 loss: 0.954537  [   96/  126]
train() client id: f_00000-3-0 loss: 1.108982  [   32/  126]
train() client id: f_00000-3-1 loss: 1.006991  [   64/  126]
train() client id: f_00000-3-2 loss: 0.827665  [   96/  126]
train() client id: f_00000-4-0 loss: 0.756891  [   32/  126]
train() client id: f_00000-4-1 loss: 0.948910  [   64/  126]
train() client id: f_00000-4-2 loss: 1.073258  [   96/  126]
train() client id: f_00000-5-0 loss: 1.078790  [   32/  126]
train() client id: f_00000-5-1 loss: 0.916091  [   64/  126]
train() client id: f_00000-5-2 loss: 0.800607  [   96/  126]
train() client id: f_00000-6-0 loss: 0.817737  [   32/  126]
train() client id: f_00000-6-1 loss: 1.006409  [   64/  126]
train() client id: f_00000-6-2 loss: 0.799522  [   96/  126]
train() client id: f_00000-7-0 loss: 0.719462  [   32/  126]
train() client id: f_00000-7-1 loss: 0.892649  [   64/  126]
train() client id: f_00000-7-2 loss: 0.957546  [   96/  126]
train() client id: f_00000-8-0 loss: 0.863637  [   32/  126]
train() client id: f_00000-8-1 loss: 0.882081  [   64/  126]
train() client id: f_00000-8-2 loss: 0.873722  [   96/  126]
train() client id: f_00000-9-0 loss: 0.850468  [   32/  126]
train() client id: f_00000-9-1 loss: 1.029360  [   64/  126]
train() client id: f_00000-9-2 loss: 0.941232  [   96/  126]
train() client id: f_00000-10-0 loss: 0.917043  [   32/  126]
train() client id: f_00000-10-1 loss: 0.921480  [   64/  126]
train() client id: f_00000-10-2 loss: 0.870879  [   96/  126]
train() client id: f_00000-11-0 loss: 0.822366  [   32/  126]
train() client id: f_00000-11-1 loss: 1.100983  [   64/  126]
train() client id: f_00000-11-2 loss: 0.921120  [   96/  126]
train() client id: f_00000-12-0 loss: 1.032654  [   32/  126]
train() client id: f_00000-12-1 loss: 0.799456  [   64/  126]
train() client id: f_00000-12-2 loss: 1.010888  [   96/  126]
train() client id: f_00001-0-0 loss: 0.752020  [   32/  265]
train() client id: f_00001-0-1 loss: 0.651726  [   64/  265]
train() client id: f_00001-0-2 loss: 0.678269  [   96/  265]
train() client id: f_00001-0-3 loss: 0.703680  [  128/  265]
train() client id: f_00001-0-4 loss: 0.681469  [  160/  265]
train() client id: f_00001-0-5 loss: 0.725862  [  192/  265]
train() client id: f_00001-0-6 loss: 0.709966  [  224/  265]
train() client id: f_00001-0-7 loss: 0.689448  [  256/  265]
train() client id: f_00001-1-0 loss: 0.681890  [   32/  265]
train() client id: f_00001-1-1 loss: 0.628950  [   64/  265]
train() client id: f_00001-1-2 loss: 0.678231  [   96/  265]
train() client id: f_00001-1-3 loss: 0.718331  [  128/  265]
train() client id: f_00001-1-4 loss: 0.752913  [  160/  265]
train() client id: f_00001-1-5 loss: 0.583889  [  192/  265]
train() client id: f_00001-1-6 loss: 0.644134  [  224/  265]
train() client id: f_00001-1-7 loss: 0.640107  [  256/  265]
train() client id: f_00001-2-0 loss: 0.636373  [   32/  265]
train() client id: f_00001-2-1 loss: 0.600850  [   64/  265]
train() client id: f_00001-2-2 loss: 0.700393  [   96/  265]
train() client id: f_00001-2-3 loss: 0.685994  [  128/  265]
train() client id: f_00001-2-4 loss: 0.642252  [  160/  265]
train() client id: f_00001-2-5 loss: 0.656795  [  192/  265]
train() client id: f_00001-2-6 loss: 0.668244  [  224/  265]
train() client id: f_00001-2-7 loss: 0.563861  [  256/  265]
train() client id: f_00001-3-0 loss: 0.564797  [   32/  265]
train() client id: f_00001-3-1 loss: 0.598956  [   64/  265]
train() client id: f_00001-3-2 loss: 0.687059  [   96/  265]
train() client id: f_00001-3-3 loss: 0.618459  [  128/  265]
train() client id: f_00001-3-4 loss: 0.655742  [  160/  265]
train() client id: f_00001-3-5 loss: 0.683102  [  192/  265]
train() client id: f_00001-3-6 loss: 0.608873  [  224/  265]
train() client id: f_00001-3-7 loss: 0.576831  [  256/  265]
train() client id: f_00001-4-0 loss: 0.538392  [   32/  265]
train() client id: f_00001-4-1 loss: 0.605559  [   64/  265]
train() client id: f_00001-4-2 loss: 0.620116  [   96/  265]
train() client id: f_00001-4-3 loss: 0.607059  [  128/  265]
train() client id: f_00001-4-4 loss: 0.611329  [  160/  265]
train() client id: f_00001-4-5 loss: 0.641040  [  192/  265]
train() client id: f_00001-4-6 loss: 0.655662  [  224/  265]
train() client id: f_00001-4-7 loss: 0.594340  [  256/  265]
train() client id: f_00001-5-0 loss: 0.591865  [   32/  265]
train() client id: f_00001-5-1 loss: 0.611726  [   64/  265]
train() client id: f_00001-5-2 loss: 0.560203  [   96/  265]
train() client id: f_00001-5-3 loss: 0.639638  [  128/  265]
train() client id: f_00001-5-4 loss: 0.554776  [  160/  265]
train() client id: f_00001-5-5 loss: 0.690535  [  192/  265]
train() client id: f_00001-5-6 loss: 0.533021  [  224/  265]
train() client id: f_00001-5-7 loss: 0.652121  [  256/  265]
train() client id: f_00001-6-0 loss: 0.676913  [   32/  265]
train() client id: f_00001-6-1 loss: 0.636365  [   64/  265]
train() client id: f_00001-6-2 loss: 0.595835  [   96/  265]
train() client id: f_00001-6-3 loss: 0.605702  [  128/  265]
train() client id: f_00001-6-4 loss: 0.575108  [  160/  265]
train() client id: f_00001-6-5 loss: 0.554437  [  192/  265]
train() client id: f_00001-6-6 loss: 0.555657  [  224/  265]
train() client id: f_00001-6-7 loss: 0.640018  [  256/  265]
train() client id: f_00001-7-0 loss: 0.581837  [   32/  265]
train() client id: f_00001-7-1 loss: 0.646721  [   64/  265]
train() client id: f_00001-7-2 loss: 0.648648  [   96/  265]
train() client id: f_00001-7-3 loss: 0.584894  [  128/  265]
train() client id: f_00001-7-4 loss: 0.593082  [  160/  265]
train() client id: f_00001-7-5 loss: 0.560956  [  192/  265]
train() client id: f_00001-7-6 loss: 0.610292  [  224/  265]
train() client id: f_00001-7-7 loss: 0.598740  [  256/  265]
train() client id: f_00001-8-0 loss: 0.555033  [   32/  265]
train() client id: f_00001-8-1 loss: 0.595754  [   64/  265]
train() client id: f_00001-8-2 loss: 0.512805  [   96/  265]
train() client id: f_00001-8-3 loss: 0.636140  [  128/  265]
train() client id: f_00001-8-4 loss: 0.600104  [  160/  265]
train() client id: f_00001-8-5 loss: 0.572982  [  192/  265]
train() client id: f_00001-8-6 loss: 0.721841  [  224/  265]
train() client id: f_00001-8-7 loss: 0.551472  [  256/  265]
train() client id: f_00001-9-0 loss: 0.531859  [   32/  265]
train() client id: f_00001-9-1 loss: 0.694391  [   64/  265]
train() client id: f_00001-9-2 loss: 0.563686  [   96/  265]
train() client id: f_00001-9-3 loss: 0.680074  [  128/  265]
train() client id: f_00001-9-4 loss: 0.555571  [  160/  265]
train() client id: f_00001-9-5 loss: 0.541757  [  192/  265]
train() client id: f_00001-9-6 loss: 0.544109  [  224/  265]
train() client id: f_00001-9-7 loss: 0.701101  [  256/  265]
train() client id: f_00001-10-0 loss: 0.612216  [   32/  265]
train() client id: f_00001-10-1 loss: 0.650839  [   64/  265]
train() client id: f_00001-10-2 loss: 0.721243  [   96/  265]
train() client id: f_00001-10-3 loss: 0.625968  [  128/  265]
train() client id: f_00001-10-4 loss: 0.572265  [  160/  265]
train() client id: f_00001-10-5 loss: 0.562786  [  192/  265]
train() client id: f_00001-10-6 loss: 0.524866  [  224/  265]
train() client id: f_00001-10-7 loss: 0.525212  [  256/  265]
train() client id: f_00001-11-0 loss: 0.667417  [   32/  265]
train() client id: f_00001-11-1 loss: 0.685654  [   64/  265]
train() client id: f_00001-11-2 loss: 0.606923  [   96/  265]
train() client id: f_00001-11-3 loss: 0.586451  [  128/  265]
train() client id: f_00001-11-4 loss: 0.541175  [  160/  265]
train() client id: f_00001-11-5 loss: 0.543608  [  192/  265]
train() client id: f_00001-11-6 loss: 0.631733  [  224/  265]
train() client id: f_00001-11-7 loss: 0.531448  [  256/  265]
train() client id: f_00001-12-0 loss: 0.598267  [   32/  265]
train() client id: f_00001-12-1 loss: 0.682648  [   64/  265]
train() client id: f_00001-12-2 loss: 0.541244  [   96/  265]
train() client id: f_00001-12-3 loss: 0.642000  [  128/  265]
train() client id: f_00001-12-4 loss: 0.663439  [  160/  265]
train() client id: f_00001-12-5 loss: 0.598485  [  192/  265]
train() client id: f_00001-12-6 loss: 0.533403  [  224/  265]
train() client id: f_00001-12-7 loss: 0.534544  [  256/  265]
train() client id: f_00002-0-0 loss: 1.220347  [   32/  124]
train() client id: f_00002-0-1 loss: 1.164491  [   64/  124]
train() client id: f_00002-0-2 loss: 1.060498  [   96/  124]
train() client id: f_00002-1-0 loss: 1.178478  [   32/  124]
train() client id: f_00002-1-1 loss: 1.005316  [   64/  124]
train() client id: f_00002-1-2 loss: 1.071944  [   96/  124]
train() client id: f_00002-2-0 loss: 1.190389  [   32/  124]
train() client id: f_00002-2-1 loss: 0.976751  [   64/  124]
train() client id: f_00002-2-2 loss: 1.020629  [   96/  124]
train() client id: f_00002-3-0 loss: 1.079741  [   32/  124]
train() client id: f_00002-3-1 loss: 0.963479  [   64/  124]
train() client id: f_00002-3-2 loss: 0.991823  [   96/  124]
train() client id: f_00002-4-0 loss: 1.043185  [   32/  124]
train() client id: f_00002-4-1 loss: 1.089920  [   64/  124]
train() client id: f_00002-4-2 loss: 0.972786  [   96/  124]
train() client id: f_00002-5-0 loss: 1.244388  [   32/  124]
train() client id: f_00002-5-1 loss: 0.917846  [   64/  124]
train() client id: f_00002-5-2 loss: 0.933360  [   96/  124]
train() client id: f_00002-6-0 loss: 1.148934  [   32/  124]
train() client id: f_00002-6-1 loss: 1.022485  [   64/  124]
train() client id: f_00002-6-2 loss: 0.853238  [   96/  124]
train() client id: f_00002-7-0 loss: 0.959355  [   32/  124]
train() client id: f_00002-7-1 loss: 1.015806  [   64/  124]
train() client id: f_00002-7-2 loss: 1.010787  [   96/  124]
train() client id: f_00002-8-0 loss: 0.891866  [   32/  124]
train() client id: f_00002-8-1 loss: 0.848094  [   64/  124]
train() client id: f_00002-8-2 loss: 1.047254  [   96/  124]
train() client id: f_00002-9-0 loss: 1.010168  [   32/  124]
train() client id: f_00002-9-1 loss: 0.876003  [   64/  124]
train() client id: f_00002-9-2 loss: 0.894967  [   96/  124]
train() client id: f_00002-10-0 loss: 0.906069  [   32/  124]
train() client id: f_00002-10-1 loss: 0.965931  [   64/  124]
train() client id: f_00002-10-2 loss: 0.915722  [   96/  124]
train() client id: f_00002-11-0 loss: 1.022066  [   32/  124]
train() client id: f_00002-11-1 loss: 0.881329  [   64/  124]
train() client id: f_00002-11-2 loss: 0.775022  [   96/  124]
train() client id: f_00002-12-0 loss: 1.072567  [   32/  124]
train() client id: f_00002-12-1 loss: 0.792028  [   64/  124]
train() client id: f_00002-12-2 loss: 0.965833  [   96/  124]
train() client id: f_00003-0-0 loss: 1.030393  [   32/   43]
train() client id: f_00003-1-0 loss: 0.988725  [   32/   43]
train() client id: f_00003-2-0 loss: 1.005530  [   32/   43]
train() client id: f_00003-3-0 loss: 1.066108  [   32/   43]
train() client id: f_00003-4-0 loss: 1.099753  [   32/   43]
train() client id: f_00003-5-0 loss: 0.996306  [   32/   43]
train() client id: f_00003-6-0 loss: 0.986756  [   32/   43]
train() client id: f_00003-7-0 loss: 1.097031  [   32/   43]
train() client id: f_00003-8-0 loss: 1.017247  [   32/   43]
train() client id: f_00003-9-0 loss: 1.035639  [   32/   43]
train() client id: f_00003-10-0 loss: 0.998685  [   32/   43]
train() client id: f_00003-11-0 loss: 0.980275  [   32/   43]
train() client id: f_00003-12-0 loss: 1.051563  [   32/   43]
train() client id: f_00004-0-0 loss: 0.854481  [   32/  306]
train() client id: f_00004-0-1 loss: 0.964844  [   64/  306]
train() client id: f_00004-0-2 loss: 0.923587  [   96/  306]
train() client id: f_00004-0-3 loss: 0.996539  [  128/  306]
train() client id: f_00004-0-4 loss: 0.931696  [  160/  306]
train() client id: f_00004-0-5 loss: 1.014044  [  192/  306]
train() client id: f_00004-0-6 loss: 0.916183  [  224/  306]
train() client id: f_00004-0-7 loss: 0.930364  [  256/  306]
train() client id: f_00004-0-8 loss: 0.904039  [  288/  306]
train() client id: f_00004-1-0 loss: 0.964837  [   32/  306]
train() client id: f_00004-1-1 loss: 0.868031  [   64/  306]
train() client id: f_00004-1-2 loss: 0.979423  [   96/  306]
train() client id: f_00004-1-3 loss: 0.937190  [  128/  306]
train() client id: f_00004-1-4 loss: 0.948654  [  160/  306]
train() client id: f_00004-1-5 loss: 0.943780  [  192/  306]
train() client id: f_00004-1-6 loss: 0.906907  [  224/  306]
train() client id: f_00004-1-7 loss: 0.940278  [  256/  306]
train() client id: f_00004-1-8 loss: 0.931981  [  288/  306]
train() client id: f_00004-2-0 loss: 0.957007  [   32/  306]
train() client id: f_00004-2-1 loss: 0.884602  [   64/  306]
train() client id: f_00004-2-2 loss: 0.936874  [   96/  306]
train() client id: f_00004-2-3 loss: 0.851705  [  128/  306]
train() client id: f_00004-2-4 loss: 0.941078  [  160/  306]
train() client id: f_00004-2-5 loss: 0.926951  [  192/  306]
train() client id: f_00004-2-6 loss: 1.003464  [  224/  306]
train() client id: f_00004-2-7 loss: 0.901496  [  256/  306]
train() client id: f_00004-2-8 loss: 0.935017  [  288/  306]
train() client id: f_00004-3-0 loss: 0.904534  [   32/  306]
train() client id: f_00004-3-1 loss: 0.876129  [   64/  306]
train() client id: f_00004-3-2 loss: 0.956204  [   96/  306]
train() client id: f_00004-3-3 loss: 0.932315  [  128/  306]
train() client id: f_00004-3-4 loss: 0.917363  [  160/  306]
train() client id: f_00004-3-5 loss: 0.859750  [  192/  306]
train() client id: f_00004-3-6 loss: 0.912674  [  224/  306]
train() client id: f_00004-3-7 loss: 0.949115  [  256/  306]
train() client id: f_00004-3-8 loss: 0.959741  [  288/  306]
train() client id: f_00004-4-0 loss: 0.964152  [   32/  306]
train() client id: f_00004-4-1 loss: 0.877241  [   64/  306]
train() client id: f_00004-4-2 loss: 0.947748  [   96/  306]
train() client id: f_00004-4-3 loss: 0.941726  [  128/  306]
train() client id: f_00004-4-4 loss: 0.919002  [  160/  306]
train() client id: f_00004-4-5 loss: 0.930945  [  192/  306]
train() client id: f_00004-4-6 loss: 0.898706  [  224/  306]
train() client id: f_00004-4-7 loss: 0.852237  [  256/  306]
train() client id: f_00004-4-8 loss: 0.899438  [  288/  306]
train() client id: f_00004-5-0 loss: 0.894403  [   32/  306]
train() client id: f_00004-5-1 loss: 0.942017  [   64/  306]
train() client id: f_00004-5-2 loss: 1.044071  [   96/  306]
train() client id: f_00004-5-3 loss: 0.883529  [  128/  306]
train() client id: f_00004-5-4 loss: 0.877005  [  160/  306]
train() client id: f_00004-5-5 loss: 0.853605  [  192/  306]
train() client id: f_00004-5-6 loss: 0.934618  [  224/  306]
train() client id: f_00004-5-7 loss: 0.941200  [  256/  306]
train() client id: f_00004-5-8 loss: 0.864382  [  288/  306]
train() client id: f_00004-6-0 loss: 0.819129  [   32/  306]
train() client id: f_00004-6-1 loss: 0.868326  [   64/  306]
train() client id: f_00004-6-2 loss: 0.945951  [   96/  306]
train() client id: f_00004-6-3 loss: 0.930987  [  128/  306]
train() client id: f_00004-6-4 loss: 0.981950  [  160/  306]
train() client id: f_00004-6-5 loss: 0.903766  [  192/  306]
train() client id: f_00004-6-6 loss: 0.958883  [  224/  306]
train() client id: f_00004-6-7 loss: 0.962773  [  256/  306]
train() client id: f_00004-6-8 loss: 0.914780  [  288/  306]
train() client id: f_00004-7-0 loss: 0.921906  [   32/  306]
train() client id: f_00004-7-1 loss: 0.914271  [   64/  306]
train() client id: f_00004-7-2 loss: 0.894400  [   96/  306]
train() client id: f_00004-7-3 loss: 0.933227  [  128/  306]
train() client id: f_00004-7-4 loss: 0.846050  [  160/  306]
train() client id: f_00004-7-5 loss: 0.934827  [  192/  306]
train() client id: f_00004-7-6 loss: 0.958307  [  224/  306]
train() client id: f_00004-7-7 loss: 0.901109  [  256/  306]
train() client id: f_00004-7-8 loss: 0.912541  [  288/  306]
train() client id: f_00004-8-0 loss: 0.849890  [   32/  306]
train() client id: f_00004-8-1 loss: 0.890328  [   64/  306]
train() client id: f_00004-8-2 loss: 0.928009  [   96/  306]
train() client id: f_00004-8-3 loss: 0.943691  [  128/  306]
train() client id: f_00004-8-4 loss: 0.889637  [  160/  306]
train() client id: f_00004-8-5 loss: 0.926640  [  192/  306]
train() client id: f_00004-8-6 loss: 0.912256  [  224/  306]
train() client id: f_00004-8-7 loss: 0.918489  [  256/  306]
train() client id: f_00004-8-8 loss: 0.896937  [  288/  306]
train() client id: f_00004-9-0 loss: 0.801625  [   32/  306]
train() client id: f_00004-9-1 loss: 0.931734  [   64/  306]
train() client id: f_00004-9-2 loss: 1.017151  [   96/  306]
train() client id: f_00004-9-3 loss: 0.870680  [  128/  306]
train() client id: f_00004-9-4 loss: 0.967760  [  160/  306]
train() client id: f_00004-9-5 loss: 0.878713  [  192/  306]
train() client id: f_00004-9-6 loss: 0.962632  [  224/  306]
train() client id: f_00004-9-7 loss: 0.886087  [  256/  306]
train() client id: f_00004-9-8 loss: 0.847662  [  288/  306]
train() client id: f_00004-10-0 loss: 0.917977  [   32/  306]
train() client id: f_00004-10-1 loss: 0.841615  [   64/  306]
train() client id: f_00004-10-2 loss: 0.922398  [   96/  306]
train() client id: f_00004-10-3 loss: 0.932634  [  128/  306]
train() client id: f_00004-10-4 loss: 0.847265  [  160/  306]
train() client id: f_00004-10-5 loss: 0.891493  [  192/  306]
train() client id: f_00004-10-6 loss: 0.987347  [  224/  306]
train() client id: f_00004-10-7 loss: 0.896933  [  256/  306]
train() client id: f_00004-10-8 loss: 0.993176  [  288/  306]
train() client id: f_00004-11-0 loss: 0.874070  [   32/  306]
train() client id: f_00004-11-1 loss: 0.989673  [   64/  306]
train() client id: f_00004-11-2 loss: 0.944650  [   96/  306]
train() client id: f_00004-11-3 loss: 0.889896  [  128/  306]
train() client id: f_00004-11-4 loss: 0.894372  [  160/  306]
train() client id: f_00004-11-5 loss: 0.955058  [  192/  306]
train() client id: f_00004-11-6 loss: 0.869811  [  224/  306]
train() client id: f_00004-11-7 loss: 0.923430  [  256/  306]
train() client id: f_00004-11-8 loss: 0.908545  [  288/  306]
train() client id: f_00004-12-0 loss: 0.853567  [   32/  306]
train() client id: f_00004-12-1 loss: 0.999410  [   64/  306]
train() client id: f_00004-12-2 loss: 0.925882  [   96/  306]
train() client id: f_00004-12-3 loss: 0.846264  [  128/  306]
train() client id: f_00004-12-4 loss: 0.919549  [  160/  306]
train() client id: f_00004-12-5 loss: 0.959700  [  192/  306]
train() client id: f_00004-12-6 loss: 0.912683  [  224/  306]
train() client id: f_00004-12-7 loss: 0.904996  [  256/  306]
train() client id: f_00004-12-8 loss: 0.930694  [  288/  306]
train() client id: f_00005-0-0 loss: 0.589197  [   32/  146]
train() client id: f_00005-0-1 loss: 0.551234  [   64/  146]
train() client id: f_00005-0-2 loss: 0.677049  [   96/  146]
train() client id: f_00005-0-3 loss: 0.697108  [  128/  146]
train() client id: f_00005-1-0 loss: 0.663330  [   32/  146]
train() client id: f_00005-1-1 loss: 0.724978  [   64/  146]
train() client id: f_00005-1-2 loss: 0.612006  [   96/  146]
train() client id: f_00005-1-3 loss: 0.520322  [  128/  146]
train() client id: f_00005-2-0 loss: 0.466204  [   32/  146]
train() client id: f_00005-2-1 loss: 0.574848  [   64/  146]
train() client id: f_00005-2-2 loss: 0.763009  [   96/  146]
train() client id: f_00005-2-3 loss: 0.479034  [  128/  146]
train() client id: f_00005-3-0 loss: 0.640802  [   32/  146]
train() client id: f_00005-3-1 loss: 0.595251  [   64/  146]
train() client id: f_00005-3-2 loss: 0.435620  [   96/  146]
train() client id: f_00005-3-3 loss: 0.663282  [  128/  146]
train() client id: f_00005-4-0 loss: 0.617032  [   32/  146]
train() client id: f_00005-4-1 loss: 0.556149  [   64/  146]
train() client id: f_00005-4-2 loss: 0.498710  [   96/  146]
train() client id: f_00005-4-3 loss: 0.633181  [  128/  146]
train() client id: f_00005-5-0 loss: 0.617531  [   32/  146]
train() client id: f_00005-5-1 loss: 0.338692  [   64/  146]
train() client id: f_00005-5-2 loss: 0.618758  [   96/  146]
train() client id: f_00005-5-3 loss: 0.755070  [  128/  146]
train() client id: f_00005-6-0 loss: 0.447323  [   32/  146]
train() client id: f_00005-6-1 loss: 0.555242  [   64/  146]
train() client id: f_00005-6-2 loss: 0.482812  [   96/  146]
train() client id: f_00005-6-3 loss: 0.614289  [  128/  146]
train() client id: f_00005-7-0 loss: 0.570326  [   32/  146]
train() client id: f_00005-7-1 loss: 0.660125  [   64/  146]
train() client id: f_00005-7-2 loss: 0.374610  [   96/  146]
train() client id: f_00005-7-3 loss: 0.476112  [  128/  146]
train() client id: f_00005-8-0 loss: 0.498296  [   32/  146]
train() client id: f_00005-8-1 loss: 0.667187  [   64/  146]
train() client id: f_00005-8-2 loss: 0.443253  [   96/  146]
train() client id: f_00005-8-3 loss: 0.530635  [  128/  146]
train() client id: f_00005-9-0 loss: 0.674529  [   32/  146]
train() client id: f_00005-9-1 loss: 0.466074  [   64/  146]
train() client id: f_00005-9-2 loss: 0.522924  [   96/  146]
train() client id: f_00005-9-3 loss: 0.417971  [  128/  146]
train() client id: f_00005-10-0 loss: 0.501676  [   32/  146]
train() client id: f_00005-10-1 loss: 0.499952  [   64/  146]
train() client id: f_00005-10-2 loss: 0.572457  [   96/  146]
train() client id: f_00005-10-3 loss: 0.542339  [  128/  146]
train() client id: f_00005-11-0 loss: 0.414955  [   32/  146]
train() client id: f_00005-11-1 loss: 0.690697  [   64/  146]
train() client id: f_00005-11-2 loss: 0.357724  [   96/  146]
train() client id: f_00005-11-3 loss: 0.481003  [  128/  146]
train() client id: f_00005-12-0 loss: 0.609991  [   32/  146]
train() client id: f_00005-12-1 loss: 0.396213  [   64/  146]
train() client id: f_00005-12-2 loss: 0.582213  [   96/  146]
train() client id: f_00005-12-3 loss: 0.500276  [  128/  146]
train() client id: f_00006-0-0 loss: 0.835358  [   32/   54]
train() client id: f_00006-1-0 loss: 0.829545  [   32/   54]
train() client id: f_00006-2-0 loss: 0.766330  [   32/   54]
train() client id: f_00006-3-0 loss: 0.832394  [   32/   54]
train() client id: f_00006-4-0 loss: 0.836168  [   32/   54]
train() client id: f_00006-5-0 loss: 0.802410  [   32/   54]
train() client id: f_00006-6-0 loss: 0.854001  [   32/   54]
train() client id: f_00006-7-0 loss: 0.853508  [   32/   54]
train() client id: f_00006-8-0 loss: 0.793273  [   32/   54]
train() client id: f_00006-9-0 loss: 0.823504  [   32/   54]
train() client id: f_00006-10-0 loss: 0.817500  [   32/   54]
train() client id: f_00006-11-0 loss: 0.849617  [   32/   54]
train() client id: f_00006-12-0 loss: 0.814366  [   32/   54]
train() client id: f_00007-0-0 loss: 0.917878  [   32/  179]
train() client id: f_00007-0-1 loss: 0.739748  [   64/  179]
train() client id: f_00007-0-2 loss: 0.665687  [   96/  179]
train() client id: f_00007-0-3 loss: 0.713641  [  128/  179]
train() client id: f_00007-0-4 loss: 0.814642  [  160/  179]
train() client id: f_00007-1-0 loss: 0.803405  [   32/  179]
train() client id: f_00007-1-1 loss: 0.697915  [   64/  179]
train() client id: f_00007-1-2 loss: 0.946621  [   96/  179]
train() client id: f_00007-1-3 loss: 0.719696  [  128/  179]
train() client id: f_00007-1-4 loss: 0.630737  [  160/  179]
train() client id: f_00007-2-0 loss: 0.603037  [   32/  179]
train() client id: f_00007-2-1 loss: 0.750724  [   64/  179]
train() client id: f_00007-2-2 loss: 0.736274  [   96/  179]
train() client id: f_00007-2-3 loss: 0.785580  [  128/  179]
train() client id: f_00007-2-4 loss: 0.718894  [  160/  179]
train() client id: f_00007-3-0 loss: 0.592610  [   32/  179]
train() client id: f_00007-3-1 loss: 0.673160  [   64/  179]
train() client id: f_00007-3-2 loss: 0.652862  [   96/  179]
train() client id: f_00007-3-3 loss: 0.843536  [  128/  179]
train() client id: f_00007-3-4 loss: 0.699094  [  160/  179]
train() client id: f_00007-4-0 loss: 0.601828  [   32/  179]
train() client id: f_00007-4-1 loss: 0.610478  [   64/  179]
train() client id: f_00007-4-2 loss: 0.801301  [   96/  179]
train() client id: f_00007-4-3 loss: 0.815728  [  128/  179]
train() client id: f_00007-4-4 loss: 0.589039  [  160/  179]
train() client id: f_00007-5-0 loss: 0.577236  [   32/  179]
train() client id: f_00007-5-1 loss: 0.649196  [   64/  179]
train() client id: f_00007-5-2 loss: 0.708338  [   96/  179]
train() client id: f_00007-5-3 loss: 0.808051  [  128/  179]
train() client id: f_00007-5-4 loss: 0.766371  [  160/  179]
train() client id: f_00007-6-0 loss: 0.712788  [   32/  179]
train() client id: f_00007-6-1 loss: 0.574290  [   64/  179]
train() client id: f_00007-6-2 loss: 0.678184  [   96/  179]
train() client id: f_00007-6-3 loss: 0.656711  [  128/  179]
train() client id: f_00007-6-4 loss: 0.687036  [  160/  179]
train() client id: f_00007-7-0 loss: 0.727532  [   32/  179]
train() client id: f_00007-7-1 loss: 0.689714  [   64/  179]
train() client id: f_00007-7-2 loss: 0.566073  [   96/  179]
train() client id: f_00007-7-3 loss: 0.552442  [  128/  179]
train() client id: f_00007-7-4 loss: 0.818993  [  160/  179]
train() client id: f_00007-8-0 loss: 0.673480  [   32/  179]
train() client id: f_00007-8-1 loss: 0.594887  [   64/  179]
train() client id: f_00007-8-2 loss: 0.568524  [   96/  179]
train() client id: f_00007-8-3 loss: 0.671781  [  128/  179]
train() client id: f_00007-8-4 loss: 0.707284  [  160/  179]
train() client id: f_00007-9-0 loss: 0.507142  [   32/  179]
train() client id: f_00007-9-1 loss: 0.647053  [   64/  179]
train() client id: f_00007-9-2 loss: 0.717686  [   96/  179]
train() client id: f_00007-9-3 loss: 0.831754  [  128/  179]
train() client id: f_00007-9-4 loss: 0.560764  [  160/  179]
train() client id: f_00007-10-0 loss: 0.631623  [   32/  179]
train() client id: f_00007-10-1 loss: 0.610065  [   64/  179]
train() client id: f_00007-10-2 loss: 0.686283  [   96/  179]
train() client id: f_00007-10-3 loss: 0.545073  [  128/  179]
train() client id: f_00007-10-4 loss: 0.769697  [  160/  179]
train() client id: f_00007-11-0 loss: 0.776960  [   32/  179]
train() client id: f_00007-11-1 loss: 0.593989  [   64/  179]
train() client id: f_00007-11-2 loss: 0.803870  [   96/  179]
train() client id: f_00007-11-3 loss: 0.537146  [  128/  179]
train() client id: f_00007-11-4 loss: 0.688622  [  160/  179]
train() client id: f_00007-12-0 loss: 0.710527  [   32/  179]
train() client id: f_00007-12-1 loss: 0.552126  [   64/  179]
train() client id: f_00007-12-2 loss: 0.743066  [   96/  179]
train() client id: f_00007-12-3 loss: 0.819795  [  128/  179]
train() client id: f_00007-12-4 loss: 0.574757  [  160/  179]
train() client id: f_00008-0-0 loss: 0.956549  [   32/  130]
train() client id: f_00008-0-1 loss: 0.990414  [   64/  130]
train() client id: f_00008-0-2 loss: 0.958073  [   96/  130]
train() client id: f_00008-0-3 loss: 0.882024  [  128/  130]
train() client id: f_00008-1-0 loss: 0.907426  [   32/  130]
train() client id: f_00008-1-1 loss: 0.933629  [   64/  130]
train() client id: f_00008-1-2 loss: 0.949915  [   96/  130]
train() client id: f_00008-1-3 loss: 0.950618  [  128/  130]
train() client id: f_00008-2-0 loss: 0.824529  [   32/  130]
train() client id: f_00008-2-1 loss: 0.934696  [   64/  130]
train() client id: f_00008-2-2 loss: 0.957874  [   96/  130]
train() client id: f_00008-2-3 loss: 1.001022  [  128/  130]
train() client id: f_00008-3-0 loss: 0.849927  [   32/  130]
train() client id: f_00008-3-1 loss: 0.962707  [   64/  130]
train() client id: f_00008-3-2 loss: 0.915848  [   96/  130]
train() client id: f_00008-3-3 loss: 0.965634  [  128/  130]
train() client id: f_00008-4-0 loss: 0.945710  [   32/  130]
train() client id: f_00008-4-1 loss: 0.919879  [   64/  130]
train() client id: f_00008-4-2 loss: 0.881848  [   96/  130]
train() client id: f_00008-4-3 loss: 0.933918  [  128/  130]
train() client id: f_00008-5-0 loss: 0.957525  [   32/  130]
train() client id: f_00008-5-1 loss: 0.972332  [   64/  130]
train() client id: f_00008-5-2 loss: 0.930175  [   96/  130]
train() client id: f_00008-5-3 loss: 0.818741  [  128/  130]
train() client id: f_00008-6-0 loss: 0.866801  [   32/  130]
train() client id: f_00008-6-1 loss: 0.928944  [   64/  130]
train() client id: f_00008-6-2 loss: 1.013898  [   96/  130]
train() client id: f_00008-6-3 loss: 0.854595  [  128/  130]
train() client id: f_00008-7-0 loss: 0.874149  [   32/  130]
train() client id: f_00008-7-1 loss: 0.921742  [   64/  130]
train() client id: f_00008-7-2 loss: 0.918259  [   96/  130]
train() client id: f_00008-7-3 loss: 0.935499  [  128/  130]
train() client id: f_00008-8-0 loss: 0.928302  [   32/  130]
train() client id: f_00008-8-1 loss: 0.836532  [   64/  130]
train() client id: f_00008-8-2 loss: 0.934834  [   96/  130]
train() client id: f_00008-8-3 loss: 0.958820  [  128/  130]
train() client id: f_00008-9-0 loss: 0.933610  [   32/  130]
train() client id: f_00008-9-1 loss: 1.053844  [   64/  130]
train() client id: f_00008-9-2 loss: 0.852312  [   96/  130]
train() client id: f_00008-9-3 loss: 0.819107  [  128/  130]
train() client id: f_00008-10-0 loss: 0.893751  [   32/  130]
train() client id: f_00008-10-1 loss: 0.928260  [   64/  130]
train() client id: f_00008-10-2 loss: 0.917287  [   96/  130]
train() client id: f_00008-10-3 loss: 0.896481  [  128/  130]
train() client id: f_00008-11-0 loss: 0.838409  [   32/  130]
train() client id: f_00008-11-1 loss: 0.905596  [   64/  130]
train() client id: f_00008-11-2 loss: 0.969265  [   96/  130]
train() client id: f_00008-11-3 loss: 0.904793  [  128/  130]
train() client id: f_00008-12-0 loss: 1.034569  [   32/  130]
train() client id: f_00008-12-1 loss: 0.882813  [   64/  130]
train() client id: f_00008-12-2 loss: 0.794824  [   96/  130]
train() client id: f_00008-12-3 loss: 0.926125  [  128/  130]
train() client id: f_00009-0-0 loss: 1.144943  [   32/  118]
train() client id: f_00009-0-1 loss: 1.052304  [   64/  118]
train() client id: f_00009-0-2 loss: 1.004701  [   96/  118]
train() client id: f_00009-1-0 loss: 0.994382  [   32/  118]
train() client id: f_00009-1-1 loss: 0.967562  [   64/  118]
train() client id: f_00009-1-2 loss: 1.110298  [   96/  118]
train() client id: f_00009-2-0 loss: 0.930766  [   32/  118]
train() client id: f_00009-2-1 loss: 0.971099  [   64/  118]
train() client id: f_00009-2-2 loss: 1.045518  [   96/  118]
train() client id: f_00009-3-0 loss: 1.007488  [   32/  118]
train() client id: f_00009-3-1 loss: 0.943424  [   64/  118]
train() client id: f_00009-3-2 loss: 0.887422  [   96/  118]
train() client id: f_00009-4-0 loss: 0.899062  [   32/  118]
train() client id: f_00009-4-1 loss: 0.845000  [   64/  118]
train() client id: f_00009-4-2 loss: 0.828651  [   96/  118]
train() client id: f_00009-5-0 loss: 0.821425  [   32/  118]
train() client id: f_00009-5-1 loss: 1.028068  [   64/  118]
train() client id: f_00009-5-2 loss: 0.766029  [   96/  118]
train() client id: f_00009-6-0 loss: 0.901069  [   32/  118]
train() client id: f_00009-6-1 loss: 0.876330  [   64/  118]
train() client id: f_00009-6-2 loss: 0.882741  [   96/  118]
train() client id: f_00009-7-0 loss: 1.017365  [   32/  118]
train() client id: f_00009-7-1 loss: 0.708685  [   64/  118]
train() client id: f_00009-7-2 loss: 0.836327  [   96/  118]
train() client id: f_00009-8-0 loss: 0.889382  [   32/  118]
train() client id: f_00009-8-1 loss: 0.737996  [   64/  118]
train() client id: f_00009-8-2 loss: 0.888685  [   96/  118]
train() client id: f_00009-9-0 loss: 0.791455  [   32/  118]
train() client id: f_00009-9-1 loss: 0.860048  [   64/  118]
train() client id: f_00009-9-2 loss: 0.775726  [   96/  118]
train() client id: f_00009-10-0 loss: 0.848287  [   32/  118]
train() client id: f_00009-10-1 loss: 0.850801  [   64/  118]
train() client id: f_00009-10-2 loss: 0.817074  [   96/  118]
train() client id: f_00009-11-0 loss: 0.864835  [   32/  118]
train() client id: f_00009-11-1 loss: 0.747910  [   64/  118]
train() client id: f_00009-11-2 loss: 0.841585  [   96/  118]
train() client id: f_00009-12-0 loss: 0.839320  [   32/  118]
train() client id: f_00009-12-1 loss: 0.937484  [   64/  118]
train() client id: f_00009-12-2 loss: 0.764841  [   96/  118]
At round 3 accuracy: 0.6392572944297082
At round 3 training accuracy: 0.5761234071093226
At round 3 training loss: 0.8686853433582117
update_location
xs = -3.905658 -0.799682 35.009024 18.811294 -19.020704 -6.043590 2.556808 -6.324852 19.663977 -2.060879 
ys = 27.587959 15.555839 1.320614 2.544824 9.350187 -17.185849 -2.624984 0.822348 17.569006 4.001482 
xs mean: 3.7885738318825743
ys mean: 5.894142535528706
dists_uav = 103.809198 101.205848 105.959312 101.785760 102.221393 101.645848 100.067117 100.203194 103.418286 100.101244 
uav_gains = -100.405923 -100.130159 -100.628515 -100.192196 -100.238567 -100.177261 -100.007301 -100.022055 -100.364959 -100.011003 
uav_gains_db_mean: -100.21779395769899
dists_bs = 225.839729 236.152898 272.522145 259.429384 227.528902 255.893863 251.151459 242.464760 250.356909 243.204509 
bs_gains = -105.473469 -106.016471 -107.758309 -107.159596 -105.564083 -106.992736 -106.765259 -106.337221 -106.726727 -106.374265 
bs_gains_db_mean: -106.51681352751042
Round 4
-------------------------------
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.46545628 21.87269455 10.30373985  3.68435765 25.21614886 12.16538239
  4.58153103 14.79179615 10.85487015  9.87006751]
obj_prev = 123.8060444419656
eta_min = 1.1887003308068872e-09	eta_max = 0.9184674772800265
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 28.799330597431425	eta = 0.9090909090909091
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 49.438331310801296	eta = 0.5295730850913346
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 39.62791236585137	eta = 0.660675974861339
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.87221877758381	eta = 0.6913038232004766
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.78643716203915	eta = 0.6928732000256066
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.78621816141451	eta = 0.6928772157665561
eta = 0.6928772157665561
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [0.03027591 0.06367553 0.02979534 0.01033226 0.07352722 0.03508161
 0.01297539 0.04301102 0.03123705 0.02835363]
ene_total = [3.202787   6.29122282 3.16613857 1.4566637  7.13363507 3.83605127
 1.68315721 4.30566359 3.48877145 3.22212748]
ti_comp = [0.2772398  0.25747119 0.276632   0.27781288 0.25949396 0.25275957
 0.27830054 0.2782619  0.25409312 0.25580165]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.25663118e-05 2.43411145e-04 2.16033125e-05 8.93224701e-07
 3.68952017e-04 4.22380117e-05 1.76284161e-06 6.42261255e-05
 2.95055929e-05 2.17720570e-05]
ene_total = [0.56161072 0.75347478 0.56683292 0.55471554 0.74677556 0.77704532
 0.5505341  0.55632463 0.76429157 0.74870068]
optimize_network iter = 0 obj = 6.580305820795663
eta = 0.6928772157665561
freqs = [5.46023917e+07 1.23655643e+08 5.38537534e+07 1.85957107e+07
 1.41674235e+08 6.93972005e+07 2.33118325e+07 7.72851390e+07
 6.14677254e+07 5.54211297e+07]
eta_min = 0.6721669962857192	eta_max = 0.692877215766553
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 0.06566942832741904	eta = 0.9090909090909091
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 22.05222718616278	eta = 0.002707185981428379
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.358376629903991	eta = 0.02531380252868433
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.276155607764052	eta = 0.02622820693542063
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.276117460225635	eta = 0.02622864651797696
eta = 0.02622864651797696
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.23933498e-04 2.41545493e-03 2.14377315e-04 8.86378481e-06
 3.66124143e-03 4.19142738e-04 1.74933012e-05 6.37338573e-04
 2.92794440e-04 2.16051827e-04]
ene_total = [0.18267388 0.2974487  0.18408433 0.17517407 0.32618133 0.25545427
 0.17406893 0.19124253 0.24830343 0.241486  ]
ti_comp = [0.30025765 0.28048904 0.29964984 0.30083073 0.28251181 0.27577742
 0.30131839 0.30127974 0.27711097 0.27881949]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44184640e-05 2.60315910e-04 2.33685526e-05 9.66842755e-07
 3.95080819e-04 4.50335349e-05 1.90864377e-06 6.95367030e-05
 3.14859996e-05 2.32591919e-05]
ene_total = [0.52628353 0.70725804 0.53116872 0.51967847 0.70173644 0.72818562
 0.51576709 0.52161422 0.71617089 0.70152458]
optimize_network iter = 1 obj = 6.169387607456715
eta = 0.6721669962857192
freqs = [5.45864375e+07 1.22896148e+08 5.38289512e+07 1.85932175e+07
 1.40894199e+08 6.88656172e+07 2.33118325e+07 7.72843192e+07
 6.10236134e+07 5.50512470e+07]
eta_min = 0.6721669962857293	eta_max = 0.6721669962857161
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 0.06499207998561081	eta = 0.909090909090909
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 22.05158160424028	eta = 0.0026793411075088985
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.3552731564287246	eta = 0.02508571412048698
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.273849552272878	eta = 0.0259840010165886
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.2738124460945865	eta = 0.025984425047592612
eta = 0.025984425047592612
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.24377690e-04 2.39200478e-03 2.14730208e-04 8.88417650e-06
 3.63033979e-03 4.13806558e-04 1.75382481e-05 6.38962582e-04
 2.89320240e-04 2.13725309e-04]
ene_total = [0.1826403  0.29672859 0.18404788 0.17513071 0.32524889 0.25524333
 0.17402652 0.19123929 0.24814553 0.24136141]
ti_comp = [0.30025765 0.28048904 0.29964984 0.30083073 0.28251181 0.27577742
 0.30131839 0.30127974 0.27711097 0.27881949]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44184640e-05 2.60315910e-04 2.33685526e-05 9.66842755e-07
 3.95080819e-04 4.50335349e-05 1.90864377e-06 6.95367030e-05
 3.14859996e-05 2.32591919e-05]
ene_total = [0.52628353 0.70725804 0.53116872 0.51967847 0.70173644 0.72818562
 0.51576709 0.52161422 0.71617089 0.70152458]
optimize_network iter = 2 obj = 6.169387607456903
eta = 0.6721669962857293
freqs = [5.45864375e+07 1.22896148e+08 5.38289512e+07 1.85932175e+07
 1.40894199e+08 6.88656172e+07 2.33118325e+07 7.72843192e+07
 6.10236134e+07 5.50512470e+07]
Done!
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44035723e-05 2.60157155e-04 2.33543012e-05 9.66253122e-07
 3.94839877e-04 4.50060710e-05 1.90747978e-06 6.94942958e-05
 3.14667978e-05 2.32450072e-05]
ene_total = [0.00643481 0.00864742 0.00649454 0.00635406 0.00857983 0.00890343
 0.00630624 0.00637769 0.00875654 0.00857746]
At round 4 energy consumption: 0.07543200489545869
At round 4 eta: 0.6721669962857293
At round 4 a_n: 26.812419468377104
At round 4 local rounds: 13.007932941342311
At round 4 global rounds: 81.78682184099438
gradient difference: 0.33373650908470154
train() client id: f_00000-0-0 loss: 0.845984  [   32/  126]
train() client id: f_00000-0-1 loss: 1.064142  [   64/  126]
train() client id: f_00000-0-2 loss: 0.987454  [   96/  126]
train() client id: f_00000-1-0 loss: 0.892683  [   32/  126]
train() client id: f_00000-1-1 loss: 0.900258  [   64/  126]
train() client id: f_00000-1-2 loss: 0.981271  [   96/  126]
train() client id: f_00000-2-0 loss: 0.919266  [   32/  126]
train() client id: f_00000-2-1 loss: 0.844344  [   64/  126]
train() client id: f_00000-2-2 loss: 1.003532  [   96/  126]
train() client id: f_00000-3-0 loss: 0.844758  [   32/  126]
train() client id: f_00000-3-1 loss: 0.931462  [   64/  126]
train() client id: f_00000-3-2 loss: 0.863280  [   96/  126]
train() client id: f_00000-4-0 loss: 0.834793  [   32/  126]
train() client id: f_00000-4-1 loss: 0.740742  [   64/  126]
train() client id: f_00000-4-2 loss: 0.690150  [   96/  126]
train() client id: f_00000-5-0 loss: 0.725417  [   32/  126]
train() client id: f_00000-5-1 loss: 0.682438  [   64/  126]
train() client id: f_00000-5-2 loss: 0.998674  [   96/  126]
train() client id: f_00000-6-0 loss: 0.966771  [   32/  126]
train() client id: f_00000-6-1 loss: 0.793790  [   64/  126]
train() client id: f_00000-6-2 loss: 0.674928  [   96/  126]
train() client id: f_00000-7-0 loss: 0.678847  [   32/  126]
train() client id: f_00000-7-1 loss: 0.629128  [   64/  126]
train() client id: f_00000-7-2 loss: 1.005751  [   96/  126]
train() client id: f_00000-8-0 loss: 0.798701  [   32/  126]
train() client id: f_00000-8-1 loss: 0.632596  [   64/  126]
train() client id: f_00000-8-2 loss: 0.751141  [   96/  126]
train() client id: f_00000-9-0 loss: 0.776235  [   32/  126]
train() client id: f_00000-9-1 loss: 0.869840  [   64/  126]
train() client id: f_00000-9-2 loss: 0.587664  [   96/  126]
train() client id: f_00000-10-0 loss: 0.667988  [   32/  126]
train() client id: f_00000-10-1 loss: 0.582424  [   64/  126]
train() client id: f_00000-10-2 loss: 0.887313  [   96/  126]
train() client id: f_00000-11-0 loss: 0.627578  [   32/  126]
train() client id: f_00000-11-1 loss: 0.669126  [   64/  126]
train() client id: f_00000-11-2 loss: 0.766399  [   96/  126]
train() client id: f_00000-12-0 loss: 0.549865  [   32/  126]
train() client id: f_00000-12-1 loss: 0.858208  [   64/  126]
train() client id: f_00000-12-2 loss: 0.818835  [   96/  126]
train() client id: f_00001-0-0 loss: 0.670151  [   32/  265]
train() client id: f_00001-0-1 loss: 0.672348  [   64/  265]
train() client id: f_00001-0-2 loss: 0.703352  [   96/  265]
train() client id: f_00001-0-3 loss: 0.579519  [  128/  265]
train() client id: f_00001-0-4 loss: 0.613785  [  160/  265]
train() client id: f_00001-0-5 loss: 0.616681  [  192/  265]
train() client id: f_00001-0-6 loss: 0.651178  [  224/  265]
train() client id: f_00001-0-7 loss: 0.636167  [  256/  265]
train() client id: f_00001-1-0 loss: 0.591189  [   32/  265]
train() client id: f_00001-1-1 loss: 0.586354  [   64/  265]
train() client id: f_00001-1-2 loss: 0.583564  [   96/  265]
train() client id: f_00001-1-3 loss: 0.625049  [  128/  265]
train() client id: f_00001-1-4 loss: 0.656204  [  160/  265]
train() client id: f_00001-1-5 loss: 0.600121  [  192/  265]
train() client id: f_00001-1-6 loss: 0.617706  [  224/  265]
train() client id: f_00001-1-7 loss: 0.665730  [  256/  265]
train() client id: f_00001-2-0 loss: 0.607575  [   32/  265]
train() client id: f_00001-2-1 loss: 0.729553  [   64/  265]
train() client id: f_00001-2-2 loss: 0.628677  [   96/  265]
train() client id: f_00001-2-3 loss: 0.598290  [  128/  265]
train() client id: f_00001-2-4 loss: 0.528953  [  160/  265]
train() client id: f_00001-2-5 loss: 0.628979  [  192/  265]
train() client id: f_00001-2-6 loss: 0.527680  [  224/  265]
train() client id: f_00001-2-7 loss: 0.605463  [  256/  265]
train() client id: f_00001-3-0 loss: 0.745744  [   32/  265]
train() client id: f_00001-3-1 loss: 0.562586  [   64/  265]
train() client id: f_00001-3-2 loss: 0.532397  [   96/  265]
train() client id: f_00001-3-3 loss: 0.586239  [  128/  265]
train() client id: f_00001-3-4 loss: 0.531853  [  160/  265]
train() client id: f_00001-3-5 loss: 0.627540  [  192/  265]
train() client id: f_00001-3-6 loss: 0.611152  [  224/  265]
train() client id: f_00001-3-7 loss: 0.552945  [  256/  265]
train() client id: f_00001-4-0 loss: 0.532562  [   32/  265]
train() client id: f_00001-4-1 loss: 0.544704  [   64/  265]
train() client id: f_00001-4-2 loss: 0.614999  [   96/  265]
train() client id: f_00001-4-3 loss: 0.519921  [  128/  265]
train() client id: f_00001-4-4 loss: 0.619433  [  160/  265]
train() client id: f_00001-4-5 loss: 0.615898  [  192/  265]
train() client id: f_00001-4-6 loss: 0.672546  [  224/  265]
train() client id: f_00001-4-7 loss: 0.585568  [  256/  265]
train() client id: f_00001-5-0 loss: 0.636101  [   32/  265]
train() client id: f_00001-5-1 loss: 0.587590  [   64/  265]
train() client id: f_00001-5-2 loss: 0.554291  [   96/  265]
train() client id: f_00001-5-3 loss: 0.571695  [  128/  265]
train() client id: f_00001-5-4 loss: 0.646533  [  160/  265]
train() client id: f_00001-5-5 loss: 0.488994  [  192/  265]
train() client id: f_00001-5-6 loss: 0.600303  [  224/  265]
train() client id: f_00001-5-7 loss: 0.580155  [  256/  265]
train() client id: f_00001-6-0 loss: 0.618592  [   32/  265]
train() client id: f_00001-6-1 loss: 0.569403  [   64/  265]
train() client id: f_00001-6-2 loss: 0.584426  [   96/  265]
train() client id: f_00001-6-3 loss: 0.571567  [  128/  265]
train() client id: f_00001-6-4 loss: 0.677570  [  160/  265]
train() client id: f_00001-6-5 loss: 0.573650  [  192/  265]
train() client id: f_00001-6-6 loss: 0.503528  [  224/  265]
train() client id: f_00001-6-7 loss: 0.494603  [  256/  265]
train() client id: f_00001-7-0 loss: 0.642145  [   32/  265]
train() client id: f_00001-7-1 loss: 0.531921  [   64/  265]
train() client id: f_00001-7-2 loss: 0.637460  [   96/  265]
train() client id: f_00001-7-3 loss: 0.557632  [  128/  265]
train() client id: f_00001-7-4 loss: 0.545147  [  160/  265]
train() client id: f_00001-7-5 loss: 0.582485  [  192/  265]
train() client id: f_00001-7-6 loss: 0.561449  [  224/  265]
train() client id: f_00001-7-7 loss: 0.597391  [  256/  265]
train() client id: f_00001-8-0 loss: 0.585488  [   32/  265]
train() client id: f_00001-8-1 loss: 0.569798  [   64/  265]
train() client id: f_00001-8-2 loss: 0.547980  [   96/  265]
train() client id: f_00001-8-3 loss: 0.648339  [  128/  265]
train() client id: f_00001-8-4 loss: 0.668033  [  160/  265]
train() client id: f_00001-8-5 loss: 0.631077  [  192/  265]
train() client id: f_00001-8-6 loss: 0.508078  [  224/  265]
train() client id: f_00001-8-7 loss: 0.500989  [  256/  265]
train() client id: f_00001-9-0 loss: 0.541987  [   32/  265]
train() client id: f_00001-9-1 loss: 0.667611  [   64/  265]
train() client id: f_00001-9-2 loss: 0.539111  [   96/  265]
train() client id: f_00001-9-3 loss: 0.520521  [  128/  265]
train() client id: f_00001-9-4 loss: 0.615130  [  160/  265]
train() client id: f_00001-9-5 loss: 0.672231  [  192/  265]
train() client id: f_00001-9-6 loss: 0.517914  [  224/  265]
train() client id: f_00001-9-7 loss: 0.597571  [  256/  265]
train() client id: f_00001-10-0 loss: 0.511785  [   32/  265]
train() client id: f_00001-10-1 loss: 0.601767  [   64/  265]
train() client id: f_00001-10-2 loss: 0.568842  [   96/  265]
train() client id: f_00001-10-3 loss: 0.583365  [  128/  265]
train() client id: f_00001-10-4 loss: 0.582877  [  160/  265]
train() client id: f_00001-10-5 loss: 0.488729  [  192/  265]
train() client id: f_00001-10-6 loss: 0.643652  [  224/  265]
train() client id: f_00001-10-7 loss: 0.683977  [  256/  265]
train() client id: f_00001-11-0 loss: 0.540401  [   32/  265]
train() client id: f_00001-11-1 loss: 0.703416  [   64/  265]
train() client id: f_00001-11-2 loss: 0.586961  [   96/  265]
train() client id: f_00001-11-3 loss: 0.611788  [  128/  265]
train() client id: f_00001-11-4 loss: 0.560046  [  160/  265]
train() client id: f_00001-11-5 loss: 0.591048  [  192/  265]
train() client id: f_00001-11-6 loss: 0.573291  [  224/  265]
train() client id: f_00001-11-7 loss: 0.536014  [  256/  265]
train() client id: f_00001-12-0 loss: 0.619222  [   32/  265]
train() client id: f_00001-12-1 loss: 0.501603  [   64/  265]
train() client id: f_00001-12-2 loss: 0.642841  [   96/  265]
train() client id: f_00001-12-3 loss: 0.536226  [  128/  265]
train() client id: f_00001-12-4 loss: 0.562576  [  160/  265]
train() client id: f_00001-12-5 loss: 0.563849  [  192/  265]
train() client id: f_00001-12-6 loss: 0.690707  [  224/  265]
train() client id: f_00001-12-7 loss: 0.494755  [  256/  265]
train() client id: f_00002-0-0 loss: 1.173595  [   32/  124]
train() client id: f_00002-0-1 loss: 1.091264  [   64/  124]
train() client id: f_00002-0-2 loss: 1.128728  [   96/  124]
train() client id: f_00002-1-0 loss: 1.148042  [   32/  124]
train() client id: f_00002-1-1 loss: 1.085161  [   64/  124]
train() client id: f_00002-1-2 loss: 0.943453  [   96/  124]
train() client id: f_00002-2-0 loss: 1.124562  [   32/  124]
train() client id: f_00002-2-1 loss: 0.990753  [   64/  124]
train() client id: f_00002-2-2 loss: 0.912478  [   96/  124]
train() client id: f_00002-3-0 loss: 1.181126  [   32/  124]
train() client id: f_00002-3-1 loss: 0.930666  [   64/  124]
train() client id: f_00002-3-2 loss: 0.961054  [   96/  124]
train() client id: f_00002-4-0 loss: 0.879994  [   32/  124]
train() client id: f_00002-4-1 loss: 1.066741  [   64/  124]
train() client id: f_00002-4-2 loss: 1.000432  [   96/  124]
train() client id: f_00002-5-0 loss: 1.020609  [   32/  124]
train() client id: f_00002-5-1 loss: 0.959449  [   64/  124]
train() client id: f_00002-5-2 loss: 0.957425  [   96/  124]
train() client id: f_00002-6-0 loss: 0.819155  [   32/  124]
train() client id: f_00002-6-1 loss: 1.119113  [   64/  124]
train() client id: f_00002-6-2 loss: 0.966546  [   96/  124]
train() client id: f_00002-7-0 loss: 0.958139  [   32/  124]
train() client id: f_00002-7-1 loss: 0.914270  [   64/  124]
train() client id: f_00002-7-2 loss: 0.961860  [   96/  124]
train() client id: f_00002-8-0 loss: 0.935511  [   32/  124]
train() client id: f_00002-8-1 loss: 0.910009  [   64/  124]
train() client id: f_00002-8-2 loss: 0.975095  [   96/  124]
train() client id: f_00002-9-0 loss: 0.992676  [   32/  124]
train() client id: f_00002-9-1 loss: 0.790433  [   64/  124]
train() client id: f_00002-9-2 loss: 0.885738  [   96/  124]
train() client id: f_00002-10-0 loss: 1.042741  [   32/  124]
train() client id: f_00002-10-1 loss: 0.814647  [   64/  124]
train() client id: f_00002-10-2 loss: 0.944392  [   96/  124]
train() client id: f_00002-11-0 loss: 0.905298  [   32/  124]
train() client id: f_00002-11-1 loss: 0.815497  [   64/  124]
train() client id: f_00002-11-2 loss: 1.105184  [   96/  124]
train() client id: f_00002-12-0 loss: 1.112401  [   32/  124]
train() client id: f_00002-12-1 loss: 0.815706  [   64/  124]
train() client id: f_00002-12-2 loss: 0.796990  [   96/  124]
train() client id: f_00003-0-0 loss: 1.006743  [   32/   43]
train() client id: f_00003-1-0 loss: 1.050945  [   32/   43]
train() client id: f_00003-2-0 loss: 1.015954  [   32/   43]
train() client id: f_00003-3-0 loss: 1.023214  [   32/   43]
train() client id: f_00003-4-0 loss: 0.914520  [   32/   43]
train() client id: f_00003-5-0 loss: 1.024490  [   32/   43]
train() client id: f_00003-6-0 loss: 1.000658  [   32/   43]
train() client id: f_00003-7-0 loss: 1.102791  [   32/   43]
train() client id: f_00003-8-0 loss: 1.074527  [   32/   43]
train() client id: f_00003-9-0 loss: 1.035595  [   32/   43]
train() client id: f_00003-10-0 loss: 0.943267  [   32/   43]
train() client id: f_00003-11-0 loss: 0.981306  [   32/   43]
train() client id: f_00003-12-0 loss: 1.015005  [   32/   43]
train() client id: f_00004-0-0 loss: 1.000874  [   32/  306]
train() client id: f_00004-0-1 loss: 0.955500  [   64/  306]
train() client id: f_00004-0-2 loss: 0.904008  [   96/  306]
train() client id: f_00004-0-3 loss: 0.851454  [  128/  306]
train() client id: f_00004-0-4 loss: 0.862126  [  160/  306]
train() client id: f_00004-0-5 loss: 0.952380  [  192/  306]
train() client id: f_00004-0-6 loss: 0.959019  [  224/  306]
train() client id: f_00004-0-7 loss: 0.967274  [  256/  306]
train() client id: f_00004-0-8 loss: 0.949706  [  288/  306]
train() client id: f_00004-1-0 loss: 0.823866  [   32/  306]
train() client id: f_00004-1-1 loss: 0.875167  [   64/  306]
train() client id: f_00004-1-2 loss: 0.956385  [   96/  306]
train() client id: f_00004-1-3 loss: 0.896182  [  128/  306]
train() client id: f_00004-1-4 loss: 1.017719  [  160/  306]
train() client id: f_00004-1-5 loss: 0.983214  [  192/  306]
train() client id: f_00004-1-6 loss: 0.935574  [  224/  306]
train() client id: f_00004-1-7 loss: 0.891275  [  256/  306]
train() client id: f_00004-1-8 loss: 0.928840  [  288/  306]
train() client id: f_00004-2-0 loss: 0.849196  [   32/  306]
train() client id: f_00004-2-1 loss: 0.923997  [   64/  306]
train() client id: f_00004-2-2 loss: 0.878245  [   96/  306]
train() client id: f_00004-2-3 loss: 0.887599  [  128/  306]
train() client id: f_00004-2-4 loss: 0.904565  [  160/  306]
train() client id: f_00004-2-5 loss: 0.979247  [  192/  306]
train() client id: f_00004-2-6 loss: 0.811217  [  224/  306]
train() client id: f_00004-2-7 loss: 0.990007  [  256/  306]
train() client id: f_00004-2-8 loss: 0.983476  [  288/  306]
train() client id: f_00004-3-0 loss: 0.929083  [   32/  306]
train() client id: f_00004-3-1 loss: 0.916375  [   64/  306]
train() client id: f_00004-3-2 loss: 0.916151  [   96/  306]
train() client id: f_00004-3-3 loss: 0.911917  [  128/  306]
train() client id: f_00004-3-4 loss: 0.922147  [  160/  306]
train() client id: f_00004-3-5 loss: 0.820121  [  192/  306]
train() client id: f_00004-3-6 loss: 0.904254  [  224/  306]
train() client id: f_00004-3-7 loss: 0.955886  [  256/  306]
train() client id: f_00004-3-8 loss: 0.875719  [  288/  306]
train() client id: f_00004-4-0 loss: 0.857677  [   32/  306]
train() client id: f_00004-4-1 loss: 0.906768  [   64/  306]
train() client id: f_00004-4-2 loss: 0.972873  [   96/  306]
train() client id: f_00004-4-3 loss: 0.911870  [  128/  306]
train() client id: f_00004-4-4 loss: 0.883422  [  160/  306]
train() client id: f_00004-4-5 loss: 0.948707  [  192/  306]
train() client id: f_00004-4-6 loss: 0.908425  [  224/  306]
train() client id: f_00004-4-7 loss: 0.959746  [  256/  306]
train() client id: f_00004-4-8 loss: 0.815571  [  288/  306]
train() client id: f_00004-5-0 loss: 0.917301  [   32/  306]
train() client id: f_00004-5-1 loss: 0.946878  [   64/  306]
train() client id: f_00004-5-2 loss: 0.902394  [   96/  306]
train() client id: f_00004-5-3 loss: 0.813160  [  128/  306]
train() client id: f_00004-5-4 loss: 0.922430  [  160/  306]
train() client id: f_00004-5-5 loss: 0.871830  [  192/  306]
train() client id: f_00004-5-6 loss: 0.889446  [  224/  306]
train() client id: f_00004-5-7 loss: 0.839704  [  256/  306]
train() client id: f_00004-5-8 loss: 0.992558  [  288/  306]
train() client id: f_00004-6-0 loss: 0.895395  [   32/  306]
train() client id: f_00004-6-1 loss: 0.884801  [   64/  306]
train() client id: f_00004-6-2 loss: 0.912508  [   96/  306]
train() client id: f_00004-6-3 loss: 0.901517  [  128/  306]
train() client id: f_00004-6-4 loss: 0.889475  [  160/  306]
train() client id: f_00004-6-5 loss: 0.967493  [  192/  306]
train() client id: f_00004-6-6 loss: 0.879373  [  224/  306]
train() client id: f_00004-6-7 loss: 0.768394  [  256/  306]
train() client id: f_00004-6-8 loss: 0.952782  [  288/  306]
train() client id: f_00004-7-0 loss: 0.840807  [   32/  306]
train() client id: f_00004-7-1 loss: 0.875113  [   64/  306]
train() client id: f_00004-7-2 loss: 0.933368  [   96/  306]
train() client id: f_00004-7-3 loss: 0.839744  [  128/  306]
train() client id: f_00004-7-4 loss: 0.907298  [  160/  306]
train() client id: f_00004-7-5 loss: 0.865835  [  192/  306]
train() client id: f_00004-7-6 loss: 0.953218  [  224/  306]
train() client id: f_00004-7-7 loss: 0.873974  [  256/  306]
train() client id: f_00004-7-8 loss: 0.957748  [  288/  306]
train() client id: f_00004-8-0 loss: 0.875084  [   32/  306]
train() client id: f_00004-8-1 loss: 0.910213  [   64/  306]
train() client id: f_00004-8-2 loss: 0.798015  [   96/  306]
train() client id: f_00004-8-3 loss: 0.919923  [  128/  306]
train() client id: f_00004-8-4 loss: 0.904051  [  160/  306]
train() client id: f_00004-8-5 loss: 0.926440  [  192/  306]
train() client id: f_00004-8-6 loss: 0.830833  [  224/  306]
train() client id: f_00004-8-7 loss: 0.936188  [  256/  306]
train() client id: f_00004-8-8 loss: 0.894120  [  288/  306]
train() client id: f_00004-9-0 loss: 0.788930  [   32/  306]
train() client id: f_00004-9-1 loss: 0.952033  [   64/  306]
train() client id: f_00004-9-2 loss: 0.830689  [   96/  306]
train() client id: f_00004-9-3 loss: 0.931333  [  128/  306]
train() client id: f_00004-9-4 loss: 0.876226  [  160/  306]
train() client id: f_00004-9-5 loss: 0.864129  [  192/  306]
train() client id: f_00004-9-6 loss: 0.928324  [  224/  306]
train() client id: f_00004-9-7 loss: 0.908282  [  256/  306]
train() client id: f_00004-9-8 loss: 0.970264  [  288/  306]
train() client id: f_00004-10-0 loss: 0.934687  [   32/  306]
train() client id: f_00004-10-1 loss: 0.917072  [   64/  306]
train() client id: f_00004-10-2 loss: 0.836448  [   96/  306]
train() client id: f_00004-10-3 loss: 0.835078  [  128/  306]
train() client id: f_00004-10-4 loss: 0.919148  [  160/  306]
train() client id: f_00004-10-5 loss: 0.877510  [  192/  306]
train() client id: f_00004-10-6 loss: 0.866981  [  224/  306]
train() client id: f_00004-10-7 loss: 0.957386  [  256/  306]
train() client id: f_00004-10-8 loss: 0.818217  [  288/  306]
train() client id: f_00004-11-0 loss: 0.860862  [   32/  306]
train() client id: f_00004-11-1 loss: 0.845472  [   64/  306]
train() client id: f_00004-11-2 loss: 0.933458  [   96/  306]
train() client id: f_00004-11-3 loss: 0.905306  [  128/  306]
train() client id: f_00004-11-4 loss: 0.869208  [  160/  306]
train() client id: f_00004-11-5 loss: 0.891033  [  192/  306]
train() client id: f_00004-11-6 loss: 0.885467  [  224/  306]
train() client id: f_00004-11-7 loss: 0.961831  [  256/  306]
train() client id: f_00004-11-8 loss: 0.865026  [  288/  306]
train() client id: f_00004-12-0 loss: 0.891857  [   32/  306]
train() client id: f_00004-12-1 loss: 0.914525  [   64/  306]
train() client id: f_00004-12-2 loss: 0.899668  [   96/  306]
train() client id: f_00004-12-3 loss: 0.865542  [  128/  306]
train() client id: f_00004-12-4 loss: 0.843842  [  160/  306]
train() client id: f_00004-12-5 loss: 0.911155  [  192/  306]
train() client id: f_00004-12-6 loss: 0.960869  [  224/  306]
train() client id: f_00004-12-7 loss: 0.838436  [  256/  306]
train() client id: f_00004-12-8 loss: 0.882108  [  288/  306]
train() client id: f_00005-0-0 loss: 0.981892  [   32/  146]
train() client id: f_00005-0-1 loss: 0.721034  [   64/  146]
train() client id: f_00005-0-2 loss: 0.807187  [   96/  146]
train() client id: f_00005-0-3 loss: 0.836016  [  128/  146]
train() client id: f_00005-1-0 loss: 0.606912  [   32/  146]
train() client id: f_00005-1-1 loss: 0.882113  [   64/  146]
train() client id: f_00005-1-2 loss: 0.707621  [   96/  146]
train() client id: f_00005-1-3 loss: 0.812950  [  128/  146]
train() client id: f_00005-2-0 loss: 0.671364  [   32/  146]
train() client id: f_00005-2-1 loss: 1.116691  [   64/  146]
train() client id: f_00005-2-2 loss: 0.592890  [   96/  146]
train() client id: f_00005-2-3 loss: 0.704822  [  128/  146]
train() client id: f_00005-3-0 loss: 0.796081  [   32/  146]
train() client id: f_00005-3-1 loss: 0.611992  [   64/  146]
train() client id: f_00005-3-2 loss: 0.591460  [   96/  146]
train() client id: f_00005-3-3 loss: 0.975469  [  128/  146]
train() client id: f_00005-4-0 loss: 0.955455  [   32/  146]
train() client id: f_00005-4-1 loss: 0.741965  [   64/  146]
train() client id: f_00005-4-2 loss: 0.823813  [   96/  146]
train() client id: f_00005-4-3 loss: 0.616611  [  128/  146]
train() client id: f_00005-5-0 loss: 0.744276  [   32/  146]
train() client id: f_00005-5-1 loss: 0.751919  [   64/  146]
train() client id: f_00005-5-2 loss: 0.604986  [   96/  146]
train() client id: f_00005-5-3 loss: 0.855699  [  128/  146]
train() client id: f_00005-6-0 loss: 0.582070  [   32/  146]
train() client id: f_00005-6-1 loss: 0.821690  [   64/  146]
train() client id: f_00005-6-2 loss: 0.829117  [   96/  146]
train() client id: f_00005-6-3 loss: 0.599748  [  128/  146]
train() client id: f_00005-7-0 loss: 0.600155  [   32/  146]
train() client id: f_00005-7-1 loss: 0.919053  [   64/  146]
train() client id: f_00005-7-2 loss: 0.747218  [   96/  146]
train() client id: f_00005-7-3 loss: 0.683378  [  128/  146]
train() client id: f_00005-8-0 loss: 0.671508  [   32/  146]
train() client id: f_00005-8-1 loss: 0.857098  [   64/  146]
train() client id: f_00005-8-2 loss: 0.686164  [   96/  146]
train() client id: f_00005-8-3 loss: 0.709344  [  128/  146]
train() client id: f_00005-9-0 loss: 0.811121  [   32/  146]
train() client id: f_00005-9-1 loss: 0.895753  [   64/  146]
train() client id: f_00005-9-2 loss: 0.885017  [   96/  146]
train() client id: f_00005-9-3 loss: 0.554171  [  128/  146]
train() client id: f_00005-10-0 loss: 0.877801  [   32/  146]
train() client id: f_00005-10-1 loss: 0.800038  [   64/  146]
train() client id: f_00005-10-2 loss: 0.696421  [   96/  146]
train() client id: f_00005-10-3 loss: 0.693817  [  128/  146]
train() client id: f_00005-11-0 loss: 0.661482  [   32/  146]
train() client id: f_00005-11-1 loss: 0.685570  [   64/  146]
train() client id: f_00005-11-2 loss: 0.715431  [   96/  146]
train() client id: f_00005-11-3 loss: 0.976065  [  128/  146]
train() client id: f_00005-12-0 loss: 0.753264  [   32/  146]
train() client id: f_00005-12-1 loss: 0.840339  [   64/  146]
train() client id: f_00005-12-2 loss: 0.619777  [   96/  146]
train() client id: f_00005-12-3 loss: 0.761132  [  128/  146]
train() client id: f_00006-0-0 loss: 0.704713  [   32/   54]
train() client id: f_00006-1-0 loss: 0.754934  [   32/   54]
train() client id: f_00006-2-0 loss: 0.737165  [   32/   54]
train() client id: f_00006-3-0 loss: 0.780495  [   32/   54]
train() client id: f_00006-4-0 loss: 0.781423  [   32/   54]
train() client id: f_00006-5-0 loss: 0.766941  [   32/   54]
train() client id: f_00006-6-0 loss: 0.756656  [   32/   54]
train() client id: f_00006-7-0 loss: 0.743633  [   32/   54]
train() client id: f_00006-8-0 loss: 0.801797  [   32/   54]
train() client id: f_00006-9-0 loss: 0.772388  [   32/   54]
train() client id: f_00006-10-0 loss: 0.792337  [   32/   54]
train() client id: f_00006-11-0 loss: 0.763141  [   32/   54]
train() client id: f_00006-12-0 loss: 0.746637  [   32/   54]
train() client id: f_00007-0-0 loss: 0.763613  [   32/  179]
train() client id: f_00007-0-1 loss: 0.777694  [   64/  179]
train() client id: f_00007-0-2 loss: 0.848384  [   96/  179]
train() client id: f_00007-0-3 loss: 0.804468  [  128/  179]
train() client id: f_00007-0-4 loss: 0.720198  [  160/  179]
train() client id: f_00007-1-0 loss: 0.899392  [   32/  179]
train() client id: f_00007-1-1 loss: 0.752436  [   64/  179]
train() client id: f_00007-1-2 loss: 0.658706  [   96/  179]
train() client id: f_00007-1-3 loss: 0.731958  [  128/  179]
train() client id: f_00007-1-4 loss: 0.686344  [  160/  179]
train() client id: f_00007-2-0 loss: 0.647009  [   32/  179]
train() client id: f_00007-2-1 loss: 0.668566  [   64/  179]
train() client id: f_00007-2-2 loss: 0.776880  [   96/  179]
train() client id: f_00007-2-3 loss: 0.702956  [  128/  179]
train() client id: f_00007-2-4 loss: 0.643979  [  160/  179]
train() client id: f_00007-3-0 loss: 0.691892  [   32/  179]
train() client id: f_00007-3-1 loss: 0.692481  [   64/  179]
train() client id: f_00007-3-2 loss: 0.819097  [   96/  179]
train() client id: f_00007-3-3 loss: 0.682636  [  128/  179]
train() client id: f_00007-3-4 loss: 0.660704  [  160/  179]
train() client id: f_00007-4-0 loss: 0.791353  [   32/  179]
train() client id: f_00007-4-1 loss: 0.601709  [   64/  179]
train() client id: f_00007-4-2 loss: 0.778960  [   96/  179]
train() client id: f_00007-4-3 loss: 0.580067  [  128/  179]
train() client id: f_00007-4-4 loss: 0.695162  [  160/  179]
train() client id: f_00007-5-0 loss: 0.576165  [   32/  179]
train() client id: f_00007-5-1 loss: 0.641693  [   64/  179]
train() client id: f_00007-5-2 loss: 0.810243  [   96/  179]
train() client id: f_00007-5-3 loss: 0.670947  [  128/  179]
train() client id: f_00007-5-4 loss: 0.661207  [  160/  179]
train() client id: f_00007-6-0 loss: 0.681747  [   32/  179]
train() client id: f_00007-6-1 loss: 0.621186  [   64/  179]
train() client id: f_00007-6-2 loss: 0.540465  [   96/  179]
train() client id: f_00007-6-3 loss: 0.814853  [  128/  179]
train() client id: f_00007-6-4 loss: 0.730230  [  160/  179]
train() client id: f_00007-7-0 loss: 0.545969  [   32/  179]
train() client id: f_00007-7-1 loss: 0.648731  [   64/  179]
train() client id: f_00007-7-2 loss: 0.683364  [   96/  179]
train() client id: f_00007-7-3 loss: 0.697863  [  128/  179]
train() client id: f_00007-7-4 loss: 0.794416  [  160/  179]
train() client id: f_00007-8-0 loss: 0.753698  [   32/  179]
train() client id: f_00007-8-1 loss: 0.606700  [   64/  179]
train() client id: f_00007-8-2 loss: 0.684994  [   96/  179]
train() client id: f_00007-8-3 loss: 0.631768  [  128/  179]
train() client id: f_00007-8-4 loss: 0.602691  [  160/  179]
train() client id: f_00007-9-0 loss: 0.813002  [   32/  179]
train() client id: f_00007-9-1 loss: 0.616157  [   64/  179]
train() client id: f_00007-9-2 loss: 0.628135  [   96/  179]
train() client id: f_00007-9-3 loss: 0.556851  [  128/  179]
train() client id: f_00007-9-4 loss: 0.578371  [  160/  179]
train() client id: f_00007-10-0 loss: 0.874062  [   32/  179]
train() client id: f_00007-10-1 loss: 0.508751  [   64/  179]
train() client id: f_00007-10-2 loss: 0.612266  [   96/  179]
train() client id: f_00007-10-3 loss: 0.654551  [  128/  179]
train() client id: f_00007-10-4 loss: 0.595889  [  160/  179]
train() client id: f_00007-11-0 loss: 0.630036  [   32/  179]
train() client id: f_00007-11-1 loss: 0.585039  [   64/  179]
train() client id: f_00007-11-2 loss: 0.653973  [   96/  179]
train() client id: f_00007-11-3 loss: 0.764300  [  128/  179]
train() client id: f_00007-11-4 loss: 0.480495  [  160/  179]
train() client id: f_00007-12-0 loss: 0.498819  [   32/  179]
train() client id: f_00007-12-1 loss: 0.583689  [   64/  179]
train() client id: f_00007-12-2 loss: 0.603659  [   96/  179]
train() client id: f_00007-12-3 loss: 0.663585  [  128/  179]
train() client id: f_00007-12-4 loss: 0.637468  [  160/  179]
train() client id: f_00008-0-0 loss: 0.859367  [   32/  130]
train() client id: f_00008-0-1 loss: 0.963329  [   64/  130]
train() client id: f_00008-0-2 loss: 0.878951  [   96/  130]
train() client id: f_00008-0-3 loss: 0.832596  [  128/  130]
train() client id: f_00008-1-0 loss: 0.837900  [   32/  130]
train() client id: f_00008-1-1 loss: 0.816157  [   64/  130]
train() client id: f_00008-1-2 loss: 0.940568  [   96/  130]
train() client id: f_00008-1-3 loss: 0.944684  [  128/  130]
train() client id: f_00008-2-0 loss: 0.962460  [   32/  130]
train() client id: f_00008-2-1 loss: 0.844324  [   64/  130]
train() client id: f_00008-2-2 loss: 0.817515  [   96/  130]
train() client id: f_00008-2-3 loss: 0.903246  [  128/  130]
train() client id: f_00008-3-0 loss: 0.937231  [   32/  130]
train() client id: f_00008-3-1 loss: 0.836663  [   64/  130]
train() client id: f_00008-3-2 loss: 0.911760  [   96/  130]
train() client id: f_00008-3-3 loss: 0.830881  [  128/  130]
train() client id: f_00008-4-0 loss: 0.932967  [   32/  130]
train() client id: f_00008-4-1 loss: 0.848358  [   64/  130]
train() client id: f_00008-4-2 loss: 0.849388  [   96/  130]
train() client id: f_00008-4-3 loss: 0.886081  [  128/  130]
train() client id: f_00008-5-0 loss: 0.806507  [   32/  130]
train() client id: f_00008-5-1 loss: 0.898637  [   64/  130]
train() client id: f_00008-5-2 loss: 0.922928  [   96/  130]
train() client id: f_00008-5-3 loss: 0.868828  [  128/  130]
train() client id: f_00008-6-0 loss: 0.857305  [   32/  130]
train() client id: f_00008-6-1 loss: 0.907905  [   64/  130]
train() client id: f_00008-6-2 loss: 0.947976  [   96/  130]
train() client id: f_00008-6-3 loss: 0.764607  [  128/  130]
train() client id: f_00008-7-0 loss: 0.902683  [   32/  130]
train() client id: f_00008-7-1 loss: 0.877137  [   64/  130]
train() client id: f_00008-7-2 loss: 0.811590  [   96/  130]
train() client id: f_00008-7-3 loss: 0.915571  [  128/  130]
train() client id: f_00008-8-0 loss: 0.936417  [   32/  130]
train() client id: f_00008-8-1 loss: 0.919368  [   64/  130]
train() client id: f_00008-8-2 loss: 0.925033  [   96/  130]
train() client id: f_00008-8-3 loss: 0.724134  [  128/  130]
train() client id: f_00008-9-0 loss: 0.871744  [   32/  130]
train() client id: f_00008-9-1 loss: 0.878065  [   64/  130]
train() client id: f_00008-9-2 loss: 0.830693  [   96/  130]
train() client id: f_00008-9-3 loss: 0.874274  [  128/  130]
train() client id: f_00008-10-0 loss: 0.897756  [   32/  130]
train() client id: f_00008-10-1 loss: 0.903672  [   64/  130]
train() client id: f_00008-10-2 loss: 0.882320  [   96/  130]
train() client id: f_00008-10-3 loss: 0.834439  [  128/  130]
train() client id: f_00008-11-0 loss: 0.812236  [   32/  130]
train() client id: f_00008-11-1 loss: 0.891698  [   64/  130]
train() client id: f_00008-11-2 loss: 0.839192  [   96/  130]
train() client id: f_00008-11-3 loss: 0.937112  [  128/  130]
train() client id: f_00008-12-0 loss: 0.863744  [   32/  130]
train() client id: f_00008-12-1 loss: 0.778544  [   64/  130]
train() client id: f_00008-12-2 loss: 0.833331  [   96/  130]
train() client id: f_00008-12-3 loss: 1.004983  [  128/  130]
train() client id: f_00009-0-0 loss: 1.227791  [   32/  118]
train() client id: f_00009-0-1 loss: 1.208544  [   64/  118]
train() client id: f_00009-0-2 loss: 1.205924  [   96/  118]
train() client id: f_00009-1-0 loss: 1.182678  [   32/  118]
train() client id: f_00009-1-1 loss: 1.205346  [   64/  118]
train() client id: f_00009-1-2 loss: 1.105101  [   96/  118]
train() client id: f_00009-2-0 loss: 1.154286  [   32/  118]
train() client id: f_00009-2-1 loss: 1.082649  [   64/  118]
train() client id: f_00009-2-2 loss: 1.211957  [   96/  118]
train() client id: f_00009-3-0 loss: 1.014093  [   32/  118]
train() client id: f_00009-3-1 loss: 1.168494  [   64/  118]
train() client id: f_00009-3-2 loss: 1.149495  [   96/  118]
train() client id: f_00009-4-0 loss: 1.105074  [   32/  118]
train() client id: f_00009-4-1 loss: 1.137480  [   64/  118]
train() client id: f_00009-4-2 loss: 1.035083  [   96/  118]
train() client id: f_00009-5-0 loss: 1.071999  [   32/  118]
train() client id: f_00009-5-1 loss: 1.101270  [   64/  118]
train() client id: f_00009-5-2 loss: 1.064803  [   96/  118]
train() client id: f_00009-6-0 loss: 1.115091  [   32/  118]
train() client id: f_00009-6-1 loss: 1.012120  [   64/  118]
train() client id: f_00009-6-2 loss: 0.968051  [   96/  118]
train() client id: f_00009-7-0 loss: 0.997174  [   32/  118]
train() client id: f_00009-7-1 loss: 1.000166  [   64/  118]
train() client id: f_00009-7-2 loss: 1.063643  [   96/  118]
train() client id: f_00009-8-0 loss: 1.145320  [   32/  118]
train() client id: f_00009-8-1 loss: 0.974566  [   64/  118]
train() client id: f_00009-8-2 loss: 0.916474  [   96/  118]
train() client id: f_00009-9-0 loss: 0.990333  [   32/  118]
train() client id: f_00009-9-1 loss: 0.994846  [   64/  118]
train() client id: f_00009-9-2 loss: 0.901574  [   96/  118]
train() client id: f_00009-10-0 loss: 1.021873  [   32/  118]
train() client id: f_00009-10-1 loss: 1.065038  [   64/  118]
train() client id: f_00009-10-2 loss: 0.801222  [   96/  118]
train() client id: f_00009-11-0 loss: 0.901685  [   32/  118]
train() client id: f_00009-11-1 loss: 1.038601  [   64/  118]
train() client id: f_00009-11-2 loss: 0.996511  [   96/  118]
train() client id: f_00009-12-0 loss: 1.005702  [   32/  118]
train() client id: f_00009-12-1 loss: 0.827933  [   64/  118]
train() client id: f_00009-12-2 loss: 1.107614  [   96/  118]
At round 4 accuracy: 0.6339522546419099
At round 4 training accuracy: 0.5781354795439303
At round 4 training loss: 0.8559338294630906
update_location
xs = -3.905658 4.200318 40.009024 18.811294 -14.020704 -1.043590 -2.443192 -6.324852 24.663977 2.939121 
ys = 32.587959 15.555839 1.320614 -2.455176 9.350187 -17.185849 -2.624984 -4.177652 17.569006 4.001482 
xs mean: 6.288573831882573
ys mean: 5.394142535528706
dists_uav = 105.248417 101.289816 107.714744 101.783558 101.410089 101.471388 100.064278 100.286871 104.484361 100.123176 
uav_gains = -100.555422 -100.139164 -100.806925 -100.191961 -100.152049 -100.158610 -100.006993 -100.031119 -100.476312 -100.013382 
uav_gains_db_mean: -100.25319357791217
dists_bs = 222.608317 239.864951 276.393577 262.779674 230.985269 259.222362 247.641852 246.081159 254.264078 246.784976 
bs_gains = -105.298218 -106.206129 -107.929842 -107.315629 -105.747419 -107.149888 -106.594132 -106.517253 -106.915039 -106.551983 
bs_gains_db_mean: -106.62255331954495
Round 5
-------------------------------
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.33284149 21.59560189 10.17342765  3.63728595 24.89615745 12.01211791
  4.52299688 14.602885   10.71870557  9.74625265]
obj_prev = 122.23827245410807
eta_min = 9.414027209246992e-10	eta_max = 0.9186608067299383
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 28.431400687067253	eta = 0.909090909090909
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 48.95841824133416	eta = 0.5279322499743717
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 39.18478015448109	eta = 0.6596114051281231
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.434409794184106	eta = 0.6904537306569067
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.34853700150222	eta = 0.6920412410342677
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.348315836191034	eta = 0.6920453390909805
eta = 0.6920453390909805
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [0.03037504 0.06388402 0.0298929  0.01036609 0.07376796 0.03519648
 0.01301788 0.04315185 0.03133933 0.02844647]
ene_total = [3.16534696 6.21953031 3.1298897  1.43707177 7.05111483 3.79361781
 1.66080998 4.25148272 3.45176733 3.18768443]
ti_comp = [0.28160493 0.26136622 0.28090865 0.2825856  0.2634578  0.25672527
 0.28307344 0.28301023 0.25792521 0.25972041]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20876875e-05 2.38538142e-04 2.11569937e-05 8.71815183e-07
 3.61460861e-04 4.13466617e-05 1.72068806e-06 6.27009232e-05
 2.89175573e-05 2.13281397e-05]
ene_total = [0.55639501 0.74895917 0.56229982 0.54614218 0.74154691 0.77190053
 0.54202196 0.54780676 0.76051829 0.74443548]
optimize_network iter = 0 obj = 6.522026104246858
eta = 0.6920453390909805
freqs = [5.39320171e+07 1.22211702e+08 5.32075115e+07 1.83414978e+07
 1.39999582e+08 6.85489188e+07 2.29938135e+07 7.62372573e+07
 6.07527495e+07 5.47636387e+07]
eta_min = 0.6764484296472902	eta_max = 0.6920453390909633
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 0.06328991478194519	eta = 0.9090909090909091
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 21.92072085717903	eta = 0.0026247442563715508
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.335363763739976	eta = 0.024636969648472683
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.25597545648311	eta = 0.025503950408706712
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.255940548856704	eta = 0.025504345047817744
eta = 0.025504345047817744
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20148930e-04 2.37751991e-03 2.10872665e-04 8.68941936e-06
 3.60269593e-03 4.12103953e-04 1.71501718e-05 6.24942794e-04
 2.88222535e-04 2.12578485e-04]
ene_total = [0.1812087  0.29478227 0.18284802 0.17280114 0.32238    0.25400243
 0.17170585 0.1883868  0.24737814 0.2404472 ]
ti_comp = [0.29913458 0.27889587 0.2984383  0.30011526 0.28098745 0.27425492
 0.3006031  0.30053989 0.27545486 0.27725007]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.34412342e-05 2.50873933e-04 2.24469855e-05 9.25616981e-07
 3.80533165e-04 4.33862178e-05 1.82724368e-06 6.65820168e-05
 3.03620624e-05 2.24132480e-05]
ene_total = [0.52968454 0.71386454 0.53529951 0.51981962 0.70736071 0.73485767
 0.51590233 0.52171707 0.72397543 0.70863849]
optimize_network iter = 1 obj = 6.211119913967213
eta = 0.6764484296472902
freqs = [5.39156214e+07 1.21622656e+08 5.31836109e+07 1.83396515e+07
 1.39394357e+08 6.81410966e+07 2.29938135e+07 7.62362643e+07
 6.04092728e+07 5.44779861e+07]
eta_min = 0.6764484296473243	eta_max = 0.6764484296472864
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 0.06277694578067981	eta = 0.909090909090909
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 21.92023194550156	eta = 0.002603528596394288
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.332999905489733	eta = 0.024462045872963308
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.2542173952325193	eta = 0.02531696846560011
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.2541832396466193	eta = 0.02531735207056884
eta = 0.02531735207056884
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20456386e-04 2.35937920e-03 2.11105834e-04 8.70509514e-06
 3.57877769e-03 4.08031791e-04 1.71845703e-05 6.26179946e-04
 2.85544288e-04 2.10788545e-04]
ene_total = [0.18118229 0.29423306 0.18281927 0.17276841 0.32166859 0.25384311
 0.17167384 0.18838425 0.24725795 0.24035246]
ti_comp = [0.29913458 0.27889587 0.2984383  0.30011526 0.28098745 0.27425492
 0.3006031  0.30053989 0.27545486 0.27725007]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.34412342e-05 2.50873933e-04 2.24469855e-05 9.25616981e-07
 3.80533165e-04 4.33862178e-05 1.82724368e-06 6.65820168e-05
 3.03620624e-05 2.24132480e-05]
ene_total = [0.52968454 0.71386454 0.53529951 0.51981962 0.70736071 0.73485767
 0.51590233 0.52171707 0.72397543 0.70863849]
optimize_network iter = 2 obj = 6.2111199139678615
eta = 0.6764484296473243
freqs = [5.39156214e+07 1.21622656e+08 5.31836109e+07 1.83396515e+07
 1.39394357e+08 6.81410966e+07 2.29938135e+07 7.62362643e+07
 6.04092728e+07 5.44779861e+07]
Done!
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.19761204e-05 2.35193919e-04 2.10440137e-05 8.67764468e-07
 3.56749245e-04 4.06745112e-05 1.71303809e-06 6.24205363e-05
 2.84643859e-05 2.10123849e-05]
ene_total = [0.00647308 0.00871016 0.00654177 0.0063539  0.00862256 0.00897974
 0.00630596 0.00637299 0.00884754 0.00866056]
At round 5 energy consumption: 0.07586826097698268
At round 5 eta: 0.6764484296473243
At round 5 a_n: 26.46987362140779
At round 5 local rounds: 12.80002140651856
At round 5 global rounds: 81.81036980458869
gradient difference: 0.3305463194847107
train() client id: f_00000-0-0 loss: 1.019735  [   32/  126]
train() client id: f_00000-0-1 loss: 1.068946  [   64/  126]
train() client id: f_00000-0-2 loss: 0.771647  [   96/  126]
train() client id: f_00000-1-0 loss: 1.025514  [   32/  126]
train() client id: f_00000-1-1 loss: 0.805742  [   64/  126]
train() client id: f_00000-1-2 loss: 0.832719  [   96/  126]
train() client id: f_00000-2-0 loss: 0.954831  [   32/  126]
train() client id: f_00000-2-1 loss: 0.803723  [   64/  126]
train() client id: f_00000-2-2 loss: 0.871329  [   96/  126]
train() client id: f_00000-3-0 loss: 0.996437  [   32/  126]
train() client id: f_00000-3-1 loss: 0.760735  [   64/  126]
train() client id: f_00000-3-2 loss: 0.779636  [   96/  126]
train() client id: f_00000-4-0 loss: 0.807266  [   32/  126]
train() client id: f_00000-4-1 loss: 0.749377  [   64/  126]
train() client id: f_00000-4-2 loss: 0.841981  [   96/  126]
train() client id: f_00000-5-0 loss: 0.649138  [   32/  126]
train() client id: f_00000-5-1 loss: 0.691777  [   64/  126]
train() client id: f_00000-5-2 loss: 0.854363  [   96/  126]
train() client id: f_00000-6-0 loss: 0.672005  [   32/  126]
train() client id: f_00000-6-1 loss: 0.838799  [   64/  126]
train() client id: f_00000-6-2 loss: 0.755484  [   96/  126]
train() client id: f_00000-7-0 loss: 0.717187  [   32/  126]
train() client id: f_00000-7-1 loss: 0.723963  [   64/  126]
train() client id: f_00000-7-2 loss: 0.681448  [   96/  126]
train() client id: f_00000-8-0 loss: 0.772407  [   32/  126]
train() client id: f_00000-8-1 loss: 0.630852  [   64/  126]
train() client id: f_00000-8-2 loss: 0.744912  [   96/  126]
train() client id: f_00000-9-0 loss: 0.956527  [   32/  126]
train() client id: f_00000-9-1 loss: 0.586468  [   64/  126]
train() client id: f_00000-9-2 loss: 0.562687  [   96/  126]
train() client id: f_00000-10-0 loss: 0.686225  [   32/  126]
train() client id: f_00000-10-1 loss: 0.708833  [   64/  126]
train() client id: f_00000-10-2 loss: 0.720641  [   96/  126]
train() client id: f_00000-11-0 loss: 0.921506  [   32/  126]
train() client id: f_00000-11-1 loss: 0.586242  [   64/  126]
train() client id: f_00000-11-2 loss: 0.688023  [   96/  126]
train() client id: f_00001-0-0 loss: 0.567357  [   32/  265]
train() client id: f_00001-0-1 loss: 0.582463  [   64/  265]
train() client id: f_00001-0-2 loss: 0.561966  [   96/  265]
train() client id: f_00001-0-3 loss: 0.622281  [  128/  265]
train() client id: f_00001-0-4 loss: 0.629368  [  160/  265]
train() client id: f_00001-0-5 loss: 0.741177  [  192/  265]
train() client id: f_00001-0-6 loss: 0.587603  [  224/  265]
train() client id: f_00001-0-7 loss: 0.557848  [  256/  265]
train() client id: f_00001-1-0 loss: 0.533621  [   32/  265]
train() client id: f_00001-1-1 loss: 0.562977  [   64/  265]
train() client id: f_00001-1-2 loss: 0.628177  [   96/  265]
train() client id: f_00001-1-3 loss: 0.595712  [  128/  265]
train() client id: f_00001-1-4 loss: 0.589820  [  160/  265]
train() client id: f_00001-1-5 loss: 0.577032  [  192/  265]
train() client id: f_00001-1-6 loss: 0.655362  [  224/  265]
train() client id: f_00001-1-7 loss: 0.611349  [  256/  265]
train() client id: f_00001-2-0 loss: 0.573400  [   32/  265]
train() client id: f_00001-2-1 loss: 0.509691  [   64/  265]
train() client id: f_00001-2-2 loss: 0.625783  [   96/  265]
train() client id: f_00001-2-3 loss: 0.673479  [  128/  265]
train() client id: f_00001-2-4 loss: 0.551074  [  160/  265]
train() client id: f_00001-2-5 loss: 0.499498  [  192/  265]
train() client id: f_00001-2-6 loss: 0.615143  [  224/  265]
train() client id: f_00001-2-7 loss: 0.599512  [  256/  265]
train() client id: f_00001-3-0 loss: 0.500663  [   32/  265]
train() client id: f_00001-3-1 loss: 0.592318  [   64/  265]
train() client id: f_00001-3-2 loss: 0.528136  [   96/  265]
train() client id: f_00001-3-3 loss: 0.558245  [  128/  265]
train() client id: f_00001-3-4 loss: 0.528042  [  160/  265]
train() client id: f_00001-3-5 loss: 0.629159  [  192/  265]
train() client id: f_00001-3-6 loss: 0.504107  [  224/  265]
train() client id: f_00001-3-7 loss: 0.615320  [  256/  265]
train() client id: f_00001-4-0 loss: 0.622292  [   32/  265]
train() client id: f_00001-4-1 loss: 0.490854  [   64/  265]
train() client id: f_00001-4-2 loss: 0.527118  [   96/  265]
train() client id: f_00001-4-3 loss: 0.602711  [  128/  265]
train() client id: f_00001-4-4 loss: 0.581902  [  160/  265]
train() client id: f_00001-4-5 loss: 0.551003  [  192/  265]
train() client id: f_00001-4-6 loss: 0.512910  [  224/  265]
train() client id: f_00001-4-7 loss: 0.649515  [  256/  265]
train() client id: f_00001-5-0 loss: 0.504986  [   32/  265]
train() client id: f_00001-5-1 loss: 0.571914  [   64/  265]
train() client id: f_00001-5-2 loss: 0.591860  [   96/  265]
train() client id: f_00001-5-3 loss: 0.544908  [  128/  265]
train() client id: f_00001-5-4 loss: 0.597377  [  160/  265]
train() client id: f_00001-5-5 loss: 0.615964  [  192/  265]
train() client id: f_00001-5-6 loss: 0.480893  [  224/  265]
train() client id: f_00001-5-7 loss: 0.573404  [  256/  265]
train() client id: f_00001-6-0 loss: 0.480909  [   32/  265]
train() client id: f_00001-6-1 loss: 0.675197  [   64/  265]
train() client id: f_00001-6-2 loss: 0.577406  [   96/  265]
train() client id: f_00001-6-3 loss: 0.526752  [  128/  265]
train() client id: f_00001-6-4 loss: 0.501402  [  160/  265]
train() client id: f_00001-6-5 loss: 0.542536  [  192/  265]
train() client id: f_00001-6-6 loss: 0.537824  [  224/  265]
train() client id: f_00001-6-7 loss: 0.575871  [  256/  265]
train() client id: f_00001-7-0 loss: 0.459418  [   32/  265]
train() client id: f_00001-7-1 loss: 0.628679  [   64/  265]
train() client id: f_00001-7-2 loss: 0.531178  [   96/  265]
train() client id: f_00001-7-3 loss: 0.484217  [  128/  265]
train() client id: f_00001-7-4 loss: 0.528904  [  160/  265]
train() client id: f_00001-7-5 loss: 0.656462  [  192/  265]
train() client id: f_00001-7-6 loss: 0.510606  [  224/  265]
train() client id: f_00001-7-7 loss: 0.570467  [  256/  265]
train() client id: f_00001-8-0 loss: 0.548281  [   32/  265]
train() client id: f_00001-8-1 loss: 0.546428  [   64/  265]
train() client id: f_00001-8-2 loss: 0.445675  [   96/  265]
train() client id: f_00001-8-3 loss: 0.522613  [  128/  265]
train() client id: f_00001-8-4 loss: 0.640608  [  160/  265]
train() client id: f_00001-8-5 loss: 0.524447  [  192/  265]
train() client id: f_00001-8-6 loss: 0.507448  [  224/  265]
train() client id: f_00001-8-7 loss: 0.638395  [  256/  265]
train() client id: f_00001-9-0 loss: 0.502046  [   32/  265]
train() client id: f_00001-9-1 loss: 0.528553  [   64/  265]
train() client id: f_00001-9-2 loss: 0.553135  [   96/  265]
train() client id: f_00001-9-3 loss: 0.625906  [  128/  265]
train() client id: f_00001-9-4 loss: 0.634817  [  160/  265]
train() client id: f_00001-9-5 loss: 0.539329  [  192/  265]
train() client id: f_00001-9-6 loss: 0.532268  [  224/  265]
train() client id: f_00001-9-7 loss: 0.518904  [  256/  265]
train() client id: f_00001-10-0 loss: 0.518727  [   32/  265]
train() client id: f_00001-10-1 loss: 0.576932  [   64/  265]
train() client id: f_00001-10-2 loss: 0.475621  [   96/  265]
train() client id: f_00001-10-3 loss: 0.514642  [  128/  265]
train() client id: f_00001-10-4 loss: 0.600434  [  160/  265]
train() client id: f_00001-10-5 loss: 0.520080  [  192/  265]
train() client id: f_00001-10-6 loss: 0.629487  [  224/  265]
train() client id: f_00001-10-7 loss: 0.527560  [  256/  265]
train() client id: f_00001-11-0 loss: 0.611892  [   32/  265]
train() client id: f_00001-11-1 loss: 0.583314  [   64/  265]
train() client id: f_00001-11-2 loss: 0.493271  [   96/  265]
train() client id: f_00001-11-3 loss: 0.558718  [  128/  265]
train() client id: f_00001-11-4 loss: 0.481278  [  160/  265]
train() client id: f_00001-11-5 loss: 0.476126  [  192/  265]
train() client id: f_00001-11-6 loss: 0.557799  [  224/  265]
train() client id: f_00001-11-7 loss: 0.668233  [  256/  265]
train() client id: f_00002-0-0 loss: 1.252400  [   32/  124]
train() client id: f_00002-0-1 loss: 1.229419  [   64/  124]
train() client id: f_00002-0-2 loss: 1.332251  [   96/  124]
train() client id: f_00002-1-0 loss: 1.278356  [   32/  124]
train() client id: f_00002-1-1 loss: 1.169860  [   64/  124]
train() client id: f_00002-1-2 loss: 1.200421  [   96/  124]
train() client id: f_00002-2-0 loss: 1.204911  [   32/  124]
train() client id: f_00002-2-1 loss: 1.285649  [   64/  124]
train() client id: f_00002-2-2 loss: 1.166345  [   96/  124]
train() client id: f_00002-3-0 loss: 1.047283  [   32/  124]
train() client id: f_00002-3-1 loss: 1.180377  [   64/  124]
train() client id: f_00002-3-2 loss: 1.280881  [   96/  124]
train() client id: f_00002-4-0 loss: 1.119755  [   32/  124]
train() client id: f_00002-4-1 loss: 1.056022  [   64/  124]
train() client id: f_00002-4-2 loss: 1.151087  [   96/  124]
train() client id: f_00002-5-0 loss: 1.211103  [   32/  124]
train() client id: f_00002-5-1 loss: 0.999622  [   64/  124]
train() client id: f_00002-5-2 loss: 1.134043  [   96/  124]
train() client id: f_00002-6-0 loss: 1.203817  [   32/  124]
train() client id: f_00002-6-1 loss: 1.091189  [   64/  124]
train() client id: f_00002-6-2 loss: 1.047735  [   96/  124]
train() client id: f_00002-7-0 loss: 1.177937  [   32/  124]
train() client id: f_00002-7-1 loss: 1.016628  [   64/  124]
train() client id: f_00002-7-2 loss: 1.220120  [   96/  124]
train() client id: f_00002-8-0 loss: 1.150094  [   32/  124]
train() client id: f_00002-8-1 loss: 0.981641  [   64/  124]
train() client id: f_00002-8-2 loss: 1.096392  [   96/  124]
train() client id: f_00002-9-0 loss: 0.927669  [   32/  124]
train() client id: f_00002-9-1 loss: 1.216600  [   64/  124]
train() client id: f_00002-9-2 loss: 1.059271  [   96/  124]
train() client id: f_00002-10-0 loss: 1.047940  [   32/  124]
train() client id: f_00002-10-1 loss: 1.021074  [   64/  124]
train() client id: f_00002-10-2 loss: 1.220474  [   96/  124]
train() client id: f_00002-11-0 loss: 1.105054  [   32/  124]
train() client id: f_00002-11-1 loss: 1.076789  [   64/  124]
train() client id: f_00002-11-2 loss: 0.984331  [   96/  124]
train() client id: f_00003-0-0 loss: 0.984223  [   32/   43]
train() client id: f_00003-1-0 loss: 1.011667  [   32/   43]
train() client id: f_00003-2-0 loss: 0.922709  [   32/   43]
train() client id: f_00003-3-0 loss: 1.016513  [   32/   43]
train() client id: f_00003-4-0 loss: 0.942000  [   32/   43]
train() client id: f_00003-5-0 loss: 1.072839  [   32/   43]
train() client id: f_00003-6-0 loss: 1.022747  [   32/   43]
train() client id: f_00003-7-0 loss: 1.064999  [   32/   43]
train() client id: f_00003-8-0 loss: 0.999826  [   32/   43]
train() client id: f_00003-9-0 loss: 1.058710  [   32/   43]
train() client id: f_00003-10-0 loss: 1.087821  [   32/   43]
train() client id: f_00003-11-0 loss: 1.008508  [   32/   43]
train() client id: f_00004-0-0 loss: 0.939206  [   32/  306]
train() client id: f_00004-0-1 loss: 0.893611  [   64/  306]
train() client id: f_00004-0-2 loss: 0.916970  [   96/  306]
train() client id: f_00004-0-3 loss: 0.953088  [  128/  306]
train() client id: f_00004-0-4 loss: 0.837987  [  160/  306]
train() client id: f_00004-0-5 loss: 0.830295  [  192/  306]
train() client id: f_00004-0-6 loss: 0.851708  [  224/  306]
train() client id: f_00004-0-7 loss: 0.837862  [  256/  306]
train() client id: f_00004-0-8 loss: 0.959630  [  288/  306]
train() client id: f_00004-1-0 loss: 0.899624  [   32/  306]
train() client id: f_00004-1-1 loss: 0.854529  [   64/  306]
train() client id: f_00004-1-2 loss: 0.886198  [   96/  306]
train() client id: f_00004-1-3 loss: 0.889462  [  128/  306]
train() client id: f_00004-1-4 loss: 0.851862  [  160/  306]
train() client id: f_00004-1-5 loss: 0.938559  [  192/  306]
train() client id: f_00004-1-6 loss: 0.953216  [  224/  306]
train() client id: f_00004-1-7 loss: 0.868720  [  256/  306]
train() client id: f_00004-1-8 loss: 0.895539  [  288/  306]
train() client id: f_00004-2-0 loss: 0.826498  [   32/  306]
train() client id: f_00004-2-1 loss: 0.844820  [   64/  306]
train() client id: f_00004-2-2 loss: 0.892750  [   96/  306]
train() client id: f_00004-2-3 loss: 0.852968  [  128/  306]
train() client id: f_00004-2-4 loss: 0.899780  [  160/  306]
train() client id: f_00004-2-5 loss: 0.975365  [  192/  306]
train() client id: f_00004-2-6 loss: 0.832203  [  224/  306]
train() client id: f_00004-2-7 loss: 0.889884  [  256/  306]
train() client id: f_00004-2-8 loss: 0.919620  [  288/  306]
train() client id: f_00004-3-0 loss: 0.859086  [   32/  306]
train() client id: f_00004-3-1 loss: 0.845605  [   64/  306]
train() client id: f_00004-3-2 loss: 0.899482  [   96/  306]
train() client id: f_00004-3-3 loss: 0.930014  [  128/  306]
train() client id: f_00004-3-4 loss: 0.848157  [  160/  306]
train() client id: f_00004-3-5 loss: 0.851631  [  192/  306]
train() client id: f_00004-3-6 loss: 0.919535  [  224/  306]
train() client id: f_00004-3-7 loss: 0.903612  [  256/  306]
train() client id: f_00004-3-8 loss: 0.864406  [  288/  306]
train() client id: f_00004-4-0 loss: 0.883743  [   32/  306]
train() client id: f_00004-4-1 loss: 0.907749  [   64/  306]
train() client id: f_00004-4-2 loss: 0.918136  [   96/  306]
train() client id: f_00004-4-3 loss: 0.943674  [  128/  306]
train() client id: f_00004-4-4 loss: 0.855276  [  160/  306]
train() client id: f_00004-4-5 loss: 0.837810  [  192/  306]
train() client id: f_00004-4-6 loss: 0.831516  [  224/  306]
train() client id: f_00004-4-7 loss: 0.933071  [  256/  306]
train() client id: f_00004-4-8 loss: 0.847113  [  288/  306]
train() client id: f_00004-5-0 loss: 0.885543  [   32/  306]
train() client id: f_00004-5-1 loss: 0.881697  [   64/  306]
train() client id: f_00004-5-2 loss: 0.892107  [   96/  306]
train() client id: f_00004-5-3 loss: 0.850514  [  128/  306]
train() client id: f_00004-5-4 loss: 0.905704  [  160/  306]
train() client id: f_00004-5-5 loss: 0.843471  [  192/  306]
train() client id: f_00004-5-6 loss: 0.899999  [  224/  306]
train() client id: f_00004-5-7 loss: 0.918649  [  256/  306]
train() client id: f_00004-5-8 loss: 0.812096  [  288/  306]
train() client id: f_00004-6-0 loss: 0.857946  [   32/  306]
train() client id: f_00004-6-1 loss: 0.893247  [   64/  306]
train() client id: f_00004-6-2 loss: 0.895844  [   96/  306]
train() client id: f_00004-6-3 loss: 0.908893  [  128/  306]
train() client id: f_00004-6-4 loss: 0.913505  [  160/  306]
train() client id: f_00004-6-5 loss: 0.917557  [  192/  306]
train() client id: f_00004-6-6 loss: 0.845170  [  224/  306]
train() client id: f_00004-6-7 loss: 0.859349  [  256/  306]
train() client id: f_00004-6-8 loss: 0.873444  [  288/  306]
train() client id: f_00004-7-0 loss: 0.745194  [   32/  306]
train() client id: f_00004-7-1 loss: 1.015846  [   64/  306]
train() client id: f_00004-7-2 loss: 0.820255  [   96/  306]
train() client id: f_00004-7-3 loss: 0.850439  [  128/  306]
train() client id: f_00004-7-4 loss: 0.900688  [  160/  306]
train() client id: f_00004-7-5 loss: 0.958776  [  192/  306]
train() client id: f_00004-7-6 loss: 0.907636  [  224/  306]
train() client id: f_00004-7-7 loss: 0.944880  [  256/  306]
train() client id: f_00004-7-8 loss: 0.810866  [  288/  306]
train() client id: f_00004-8-0 loss: 0.872753  [   32/  306]
train() client id: f_00004-8-1 loss: 0.899759  [   64/  306]
train() client id: f_00004-8-2 loss: 0.877632  [   96/  306]
train() client id: f_00004-8-3 loss: 0.847676  [  128/  306]
train() client id: f_00004-8-4 loss: 0.918934  [  160/  306]
train() client id: f_00004-8-5 loss: 0.931463  [  192/  306]
train() client id: f_00004-8-6 loss: 0.854221  [  224/  306]
train() client id: f_00004-8-7 loss: 0.816158  [  256/  306]
train() client id: f_00004-8-8 loss: 0.893015  [  288/  306]
train() client id: f_00004-9-0 loss: 0.814529  [   32/  306]
train() client id: f_00004-9-1 loss: 0.868786  [   64/  306]
train() client id: f_00004-9-2 loss: 0.941309  [   96/  306]
train() client id: f_00004-9-3 loss: 0.829078  [  128/  306]
train() client id: f_00004-9-4 loss: 0.791536  [  160/  306]
train() client id: f_00004-9-5 loss: 0.926061  [  192/  306]
train() client id: f_00004-9-6 loss: 0.873562  [  224/  306]
train() client id: f_00004-9-7 loss: 1.035246  [  256/  306]
train() client id: f_00004-9-8 loss: 0.903691  [  288/  306]
train() client id: f_00004-10-0 loss: 0.919638  [   32/  306]
train() client id: f_00004-10-1 loss: 0.870466  [   64/  306]
train() client id: f_00004-10-2 loss: 0.853903  [   96/  306]
train() client id: f_00004-10-3 loss: 0.831758  [  128/  306]
train() client id: f_00004-10-4 loss: 0.870645  [  160/  306]
train() client id: f_00004-10-5 loss: 0.900062  [  192/  306]
train() client id: f_00004-10-6 loss: 0.846489  [  224/  306]
train() client id: f_00004-10-7 loss: 0.887848  [  256/  306]
train() client id: f_00004-10-8 loss: 0.917890  [  288/  306]
train() client id: f_00004-11-0 loss: 0.880331  [   32/  306]
train() client id: f_00004-11-1 loss: 0.927639  [   64/  306]
train() client id: f_00004-11-2 loss: 0.922610  [   96/  306]
train() client id: f_00004-11-3 loss: 0.938897  [  128/  306]
train() client id: f_00004-11-4 loss: 0.963107  [  160/  306]
train() client id: f_00004-11-5 loss: 0.812047  [  192/  306]
train() client id: f_00004-11-6 loss: 0.826835  [  224/  306]
train() client id: f_00004-11-7 loss: 0.815605  [  256/  306]
train() client id: f_00004-11-8 loss: 0.897983  [  288/  306]
train() client id: f_00005-0-0 loss: 0.904565  [   32/  146]
train() client id: f_00005-0-1 loss: 0.948525  [   64/  146]
train() client id: f_00005-0-2 loss: 0.765055  [   96/  146]
train() client id: f_00005-0-3 loss: 0.694411  [  128/  146]
train() client id: f_00005-1-0 loss: 0.628565  [   32/  146]
train() client id: f_00005-1-1 loss: 0.561476  [   64/  146]
train() client id: f_00005-1-2 loss: 0.796975  [   96/  146]
train() client id: f_00005-1-3 loss: 0.965953  [  128/  146]
train() client id: f_00005-2-0 loss: 0.778481  [   32/  146]
train() client id: f_00005-2-1 loss: 0.764302  [   64/  146]
train() client id: f_00005-2-2 loss: 0.763489  [   96/  146]
train() client id: f_00005-2-3 loss: 0.769757  [  128/  146]
train() client id: f_00005-3-0 loss: 0.698164  [   32/  146]
train() client id: f_00005-3-1 loss: 0.653657  [   64/  146]
train() client id: f_00005-3-2 loss: 0.697826  [   96/  146]
train() client id: f_00005-3-3 loss: 0.925588  [  128/  146]
train() client id: f_00005-4-0 loss: 0.616493  [   32/  146]
train() client id: f_00005-4-1 loss: 0.711810  [   64/  146]
train() client id: f_00005-4-2 loss: 0.796302  [   96/  146]
train() client id: f_00005-4-3 loss: 0.955747  [  128/  146]
train() client id: f_00005-5-0 loss: 0.701020  [   32/  146]
train() client id: f_00005-5-1 loss: 0.632538  [   64/  146]
train() client id: f_00005-5-2 loss: 0.946865  [   96/  146]
train() client id: f_00005-5-3 loss: 0.815953  [  128/  146]
train() client id: f_00005-6-0 loss: 0.608222  [   32/  146]
train() client id: f_00005-6-1 loss: 0.707942  [   64/  146]
train() client id: f_00005-6-2 loss: 0.621486  [   96/  146]
train() client id: f_00005-6-3 loss: 0.715205  [  128/  146]
train() client id: f_00005-7-0 loss: 0.681900  [   32/  146]
train() client id: f_00005-7-1 loss: 0.488570  [   64/  146]
train() client id: f_00005-7-2 loss: 0.911297  [   96/  146]
train() client id: f_00005-7-3 loss: 0.555981  [  128/  146]
train() client id: f_00005-8-0 loss: 0.588179  [   32/  146]
train() client id: f_00005-8-1 loss: 0.745685  [   64/  146]
train() client id: f_00005-8-2 loss: 0.617128  [   96/  146]
train() client id: f_00005-8-3 loss: 0.958895  [  128/  146]
train() client id: f_00005-9-0 loss: 0.695586  [   32/  146]
train() client id: f_00005-9-1 loss: 0.567527  [   64/  146]
train() client id: f_00005-9-2 loss: 0.523893  [   96/  146]
train() client id: f_00005-9-3 loss: 0.936584  [  128/  146]
train() client id: f_00005-10-0 loss: 0.725696  [   32/  146]
train() client id: f_00005-10-1 loss: 0.786696  [   64/  146]
train() client id: f_00005-10-2 loss: 0.611283  [   96/  146]
train() client id: f_00005-10-3 loss: 0.732190  [  128/  146]
train() client id: f_00005-11-0 loss: 0.604258  [   32/  146]
train() client id: f_00005-11-1 loss: 0.822335  [   64/  146]
train() client id: f_00005-11-2 loss: 0.589521  [   96/  146]
train() client id: f_00005-11-3 loss: 0.953190  [  128/  146]
train() client id: f_00006-0-0 loss: 0.713098  [   32/   54]
train() client id: f_00006-1-0 loss: 0.708458  [   32/   54]
train() client id: f_00006-2-0 loss: 0.717828  [   32/   54]
train() client id: f_00006-3-0 loss: 0.698972  [   32/   54]
train() client id: f_00006-4-0 loss: 0.666992  [   32/   54]
train() client id: f_00006-5-0 loss: 0.749662  [   32/   54]
train() client id: f_00006-6-0 loss: 0.711867  [   32/   54]
train() client id: f_00006-7-0 loss: 0.765541  [   32/   54]
train() client id: f_00006-8-0 loss: 0.723657  [   32/   54]
train() client id: f_00006-9-0 loss: 0.723533  [   32/   54]
train() client id: f_00006-10-0 loss: 0.733671  [   32/   54]
train() client id: f_00006-11-0 loss: 0.710664  [   32/   54]
train() client id: f_00007-0-0 loss: 0.740095  [   32/  179]
train() client id: f_00007-0-1 loss: 0.687602  [   64/  179]
train() client id: f_00007-0-2 loss: 0.622009  [   96/  179]
train() client id: f_00007-0-3 loss: 0.775616  [  128/  179]
train() client id: f_00007-0-4 loss: 0.752430  [  160/  179]
train() client id: f_00007-1-0 loss: 0.832056  [   32/  179]
train() client id: f_00007-1-1 loss: 0.662507  [   64/  179]
train() client id: f_00007-1-2 loss: 0.563879  [   96/  179]
train() client id: f_00007-1-3 loss: 0.568518  [  128/  179]
train() client id: f_00007-1-4 loss: 0.762931  [  160/  179]
train() client id: f_00007-2-0 loss: 0.755657  [   32/  179]
train() client id: f_00007-2-1 loss: 0.606597  [   64/  179]
train() client id: f_00007-2-2 loss: 0.695035  [   96/  179]
train() client id: f_00007-2-3 loss: 0.627388  [  128/  179]
train() client id: f_00007-2-4 loss: 0.727269  [  160/  179]
train() client id: f_00007-3-0 loss: 0.584173  [   32/  179]
train() client id: f_00007-3-1 loss: 0.653832  [   64/  179]
train() client id: f_00007-3-2 loss: 0.816980  [   96/  179]
train() client id: f_00007-3-3 loss: 0.618636  [  128/  179]
train() client id: f_00007-3-4 loss: 0.606797  [  160/  179]
train() client id: f_00007-4-0 loss: 0.527380  [   32/  179]
train() client id: f_00007-4-1 loss: 0.744322  [   64/  179]
train() client id: f_00007-4-2 loss: 0.665930  [   96/  179]
train() client id: f_00007-4-3 loss: 0.601500  [  128/  179]
train() client id: f_00007-4-4 loss: 0.715669  [  160/  179]
train() client id: f_00007-5-0 loss: 0.662319  [   32/  179]
train() client id: f_00007-5-1 loss: 0.542607  [   64/  179]
train() client id: f_00007-5-2 loss: 0.596100  [   96/  179]
train() client id: f_00007-5-3 loss: 0.773886  [  128/  179]
train() client id: f_00007-5-4 loss: 0.707489  [  160/  179]
train() client id: f_00007-6-0 loss: 0.568033  [   32/  179]
train() client id: f_00007-6-1 loss: 0.610085  [   64/  179]
train() client id: f_00007-6-2 loss: 0.749492  [   96/  179]
train() client id: f_00007-6-3 loss: 0.758496  [  128/  179]
train() client id: f_00007-6-4 loss: 0.556988  [  160/  179]
train() client id: f_00007-7-0 loss: 0.602054  [   32/  179]
train() client id: f_00007-7-1 loss: 0.607886  [   64/  179]
train() client id: f_00007-7-2 loss: 0.499604  [   96/  179]
train() client id: f_00007-7-3 loss: 0.798876  [  128/  179]
train() client id: f_00007-7-4 loss: 0.764909  [  160/  179]
train() client id: f_00007-8-0 loss: 0.723304  [   32/  179]
train() client id: f_00007-8-1 loss: 0.665678  [   64/  179]
train() client id: f_00007-8-2 loss: 0.654078  [   96/  179]
train() client id: f_00007-8-3 loss: 0.581441  [  128/  179]
train() client id: f_00007-8-4 loss: 0.652588  [  160/  179]
train() client id: f_00007-9-0 loss: 0.877453  [   32/  179]
train() client id: f_00007-9-1 loss: 0.531749  [   64/  179]
train() client id: f_00007-9-2 loss: 0.655269  [   96/  179]
train() client id: f_00007-9-3 loss: 0.606206  [  128/  179]
train() client id: f_00007-9-4 loss: 0.619801  [  160/  179]
train() client id: f_00007-10-0 loss: 0.521736  [   32/  179]
train() client id: f_00007-10-1 loss: 0.806131  [   64/  179]
train() client id: f_00007-10-2 loss: 0.702606  [   96/  179]
train() client id: f_00007-10-3 loss: 0.500074  [  128/  179]
train() client id: f_00007-10-4 loss: 0.671536  [  160/  179]
train() client id: f_00007-11-0 loss: 0.716491  [   32/  179]
train() client id: f_00007-11-1 loss: 0.570923  [   64/  179]
train() client id: f_00007-11-2 loss: 0.611280  [   96/  179]
train() client id: f_00007-11-3 loss: 0.524550  [  128/  179]
train() client id: f_00007-11-4 loss: 0.478711  [  160/  179]
train() client id: f_00008-0-0 loss: 0.875976  [   32/  130]
train() client id: f_00008-0-1 loss: 0.762906  [   64/  130]
train() client id: f_00008-0-2 loss: 0.793413  [   96/  130]
train() client id: f_00008-0-3 loss: 0.848725  [  128/  130]
train() client id: f_00008-1-0 loss: 0.922188  [   32/  130]
train() client id: f_00008-1-1 loss: 0.843168  [   64/  130]
train() client id: f_00008-1-2 loss: 0.783893  [   96/  130]
train() client id: f_00008-1-3 loss: 0.694981  [  128/  130]
train() client id: f_00008-2-0 loss: 0.768421  [   32/  130]
train() client id: f_00008-2-1 loss: 0.790462  [   64/  130]
train() client id: f_00008-2-2 loss: 0.819731  [   96/  130]
train() client id: f_00008-2-3 loss: 0.800648  [  128/  130]
train() client id: f_00008-3-0 loss: 0.771823  [   32/  130]
train() client id: f_00008-3-1 loss: 0.791359  [   64/  130]
train() client id: f_00008-3-2 loss: 0.863268  [   96/  130]
train() client id: f_00008-3-3 loss: 0.750957  [  128/  130]
train() client id: f_00008-4-0 loss: 0.778720  [   32/  130]
train() client id: f_00008-4-1 loss: 0.775346  [   64/  130]
train() client id: f_00008-4-2 loss: 0.759288  [   96/  130]
train() client id: f_00008-4-3 loss: 0.863599  [  128/  130]
train() client id: f_00008-5-0 loss: 0.761636  [   32/  130]
train() client id: f_00008-5-1 loss: 0.767666  [   64/  130]
train() client id: f_00008-5-2 loss: 0.846631  [   96/  130]
train() client id: f_00008-5-3 loss: 0.796435  [  128/  130]
train() client id: f_00008-6-0 loss: 0.771122  [   32/  130]
train() client id: f_00008-6-1 loss: 0.771779  [   64/  130]
train() client id: f_00008-6-2 loss: 0.903843  [   96/  130]
train() client id: f_00008-6-3 loss: 0.672204  [  128/  130]
train() client id: f_00008-7-0 loss: 0.780503  [   32/  130]
train() client id: f_00008-7-1 loss: 0.748845  [   64/  130]
train() client id: f_00008-7-2 loss: 0.729095  [   96/  130]
train() client id: f_00008-7-3 loss: 0.839975  [  128/  130]
train() client id: f_00008-8-0 loss: 0.753535  [   32/  130]
train() client id: f_00008-8-1 loss: 0.785152  [   64/  130]
train() client id: f_00008-8-2 loss: 0.846435  [   96/  130]
train() client id: f_00008-8-3 loss: 0.761770  [  128/  130]
train() client id: f_00008-9-0 loss: 0.851739  [   32/  130]
train() client id: f_00008-9-1 loss: 0.864484  [   64/  130]
train() client id: f_00008-9-2 loss: 0.643999  [   96/  130]
train() client id: f_00008-9-3 loss: 0.784100  [  128/  130]
train() client id: f_00008-10-0 loss: 0.754448  [   32/  130]
train() client id: f_00008-10-1 loss: 0.760423  [   64/  130]
train() client id: f_00008-10-2 loss: 0.851573  [   96/  130]
train() client id: f_00008-10-3 loss: 0.712997  [  128/  130]
train() client id: f_00008-11-0 loss: 0.860852  [   32/  130]
train() client id: f_00008-11-1 loss: 0.681487  [   64/  130]
train() client id: f_00008-11-2 loss: 0.821958  [   96/  130]
train() client id: f_00008-11-3 loss: 0.741125  [  128/  130]
train() client id: f_00009-0-0 loss: 1.058955  [   32/  118]
train() client id: f_00009-0-1 loss: 1.093124  [   64/  118]
train() client id: f_00009-0-2 loss: 0.959090  [   96/  118]
train() client id: f_00009-1-0 loss: 0.946348  [   32/  118]
train() client id: f_00009-1-1 loss: 1.015752  [   64/  118]
train() client id: f_00009-1-2 loss: 0.990672  [   96/  118]
train() client id: f_00009-2-0 loss: 0.918784  [   32/  118]
train() client id: f_00009-2-1 loss: 1.039382  [   64/  118]
train() client id: f_00009-2-2 loss: 0.864306  [   96/  118]
train() client id: f_00009-3-0 loss: 0.879689  [   32/  118]
train() client id: f_00009-3-1 loss: 1.034017  [   64/  118]
train() client id: f_00009-3-2 loss: 0.861144  [   96/  118]
train() client id: f_00009-4-0 loss: 0.885995  [   32/  118]
train() client id: f_00009-4-1 loss: 0.859842  [   64/  118]
train() client id: f_00009-4-2 loss: 0.860523  [   96/  118]
train() client id: f_00009-5-0 loss: 0.819206  [   32/  118]
train() client id: f_00009-5-1 loss: 0.856212  [   64/  118]
train() client id: f_00009-5-2 loss: 0.836337  [   96/  118]
train() client id: f_00009-6-0 loss: 0.706863  [   32/  118]
train() client id: f_00009-6-1 loss: 0.881562  [   64/  118]
train() client id: f_00009-6-2 loss: 0.862075  [   96/  118]
train() client id: f_00009-7-0 loss: 0.949846  [   32/  118]
train() client id: f_00009-7-1 loss: 0.788990  [   64/  118]
train() client id: f_00009-7-2 loss: 0.672539  [   96/  118]
train() client id: f_00009-8-0 loss: 0.704715  [   32/  118]
train() client id: f_00009-8-1 loss: 0.839398  [   64/  118]
train() client id: f_00009-8-2 loss: 0.840923  [   96/  118]
train() client id: f_00009-9-0 loss: 0.848211  [   32/  118]
train() client id: f_00009-9-1 loss: 0.770576  [   64/  118]
train() client id: f_00009-9-2 loss: 0.771820  [   96/  118]
train() client id: f_00009-10-0 loss: 0.843817  [   32/  118]
train() client id: f_00009-10-1 loss: 0.764511  [   64/  118]
train() client id: f_00009-10-2 loss: 0.710316  [   96/  118]
train() client id: f_00009-11-0 loss: 0.765297  [   32/  118]
train() client id: f_00009-11-1 loss: 0.708825  [   64/  118]
train() client id: f_00009-11-2 loss: 0.624987  [   96/  118]
At round 5 accuracy: 0.6339522546419099
At round 5 training accuracy: 0.5835010060362174
At round 5 training loss: 0.851023214832598
update_location
xs = -3.905658 4.200318 45.009024 18.811294 -9.020704 3.956410 -7.443192 -1.324852 29.663977 -2.060879 
ys = 37.587959 20.555839 1.320614 -7.455176 9.350187 -17.185849 -2.624984 -4.177652 17.569006 4.001482 
xs mean: 7.788573831882573
ys mean: 5.894142535528706
dists_uav = 106.902333 102.177224 109.670216 102.026685 100.840464 101.543127 100.310975 100.095994 105.776280 100.101244 
uav_gains = -100.724721 -100.233874 -101.002277 -100.217866 -100.090889 -100.166283 -100.033728 -100.010434 -100.609743 -100.011003 
uav_gains_db_mean: -100.31008175078856
dists_bs = 219.443256 236.570820 280.300731 266.181722 234.497308 262.603879 244.184191 249.534944 258.208949 243.204509 
bs_gains = -105.124082 -106.037972 -108.100538 -107.472050 -105.930919 -107.307491 -106.423150 -106.686738 -107.102255 -106.374265 
bs_gains_db_mean: -106.65594589792565
Round 6
-------------------------------
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.20035717 21.31407924 10.04322819  3.59039783 24.57615325 11.85884139
  4.4646498  14.41376646 10.58251293  9.61787463]
obj_prev = 120.66186087163511
eta_min = 7.409354424179371e-10	eta_max = 0.91885879376386
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 28.063470776703085	eta = 0.909090909090909
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 48.384419986505044	eta = 0.5272822567213747
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 38.70245321762515	eta = 0.6591893805073021
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.96801720495558	eta = 0.6901167032896544
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.88278882583632	eta = 0.6917114180576249
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.88256856080987	eta = 0.6917155490018552
eta = 0.6917155490018552
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [0.03041438 0.06396674 0.02993161 0.01037951 0.07386349 0.03524206
 0.01303473 0.04320773 0.03137991 0.0284833 ]
ene_total = [3.12833392 6.13288912 3.09403261 1.41867893 6.96750532 3.75135634
 1.63958615 4.19625148 3.41497814 3.13895655]
ti_comp = [0.28546116 0.26646804 0.2846808  0.28683999 0.2669565  0.2602256
 0.28732669 0.28738774 0.26129447 0.26489704]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.15785733e-05 2.30384101e-04 2.06801576e-05 8.49436889e-07
 3.53417801e-04 4.03983548e-05 1.67661934e-06 6.10418604e-05
 2.82862087e-05 2.05824434e-05]
ene_total = [0.55252256 0.73118701 0.55906002 0.53908008 0.73747448 0.76799073
 0.53502532 0.53953919 0.7579054  0.72672045]
optimize_network iter = 0 obj = 6.446505230599096
eta = 0.6917155490018552
freqs = [5.32723555e+07 1.20027048e+08 5.25704736e+07 1.80928566e+07
 1.38343675e+08 6.77144291e+07 2.26827740e+07 7.51732237e+07
 6.00470262e+07 5.37629723e+07]
eta_min = 0.6801612059033121	eta_max = 0.6917155490018475
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 0.06073437862890649	eta = 0.9090909090909091
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 21.69496157536947	eta = 0.002544972079761971
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.3026899172665374	eta = 0.023977640700475235
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.226369694175787	eta = 0.024799597131268097
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.2263378919337997	eta = 0.024799951382431865
eta = 0.024799951382431865
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.16436107e-04 2.31078474e-03 2.07424872e-04 8.51997075e-06
 3.54482996e-03 4.05201146e-04 1.68167264e-05 6.12258394e-04
 2.83714628e-04 2.06444785e-04]
ene_total = [0.17988639 0.28688359 0.18173569 0.17062183 0.31863717 0.25255426
 0.16954014 0.18532952 0.24643571 0.23471359]
ti_comp = [0.29859544 0.27960232 0.29781508 0.29997427 0.28009078 0.27335988
 0.30046097 0.30052202 0.27442876 0.27803133]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.25514581e-05 2.39268435e-04 2.16073258e-05 8.88109865e-07
 3.67109805e-04 4.18618440e-05 1.75321215e-06 6.38316142e-05
 2.93224432e-05 2.13642617e-05]
ene_total = [0.53264186 0.70549825 0.53893943 0.51960869 0.71195131 0.74036622
 0.5157035  0.52027595 0.73061032 0.70053117]
optimize_network iter = 1 obj = 6.216126697450631
eta = 0.6801612059033121
freqs = [5.32566467e+07 1.19616625e+08 5.25486356e+07 1.80913467e+07
 1.37882470e+08 6.74069266e+07 2.26825634e+07 7.51732237e+07
 5.97860936e+07 5.35642116e+07]
eta_min = 0.6801612059033234	eta_max = 0.680161205903311
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 0.06036643683170785	eta = 0.909090909090909
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 21.694610889380957	eta = 0.0025295949864110306
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.300984648578189	eta = 0.023850041316801688
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.2251002879015918	eta = 0.024663418200206223
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.225068999906738	eta = 0.02466376500693525
eta = 0.02466376500693525
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.16630671e-04 2.29842707e-03 2.07561279e-04 8.53123712e-06
 3.52647900e-03 4.02127407e-04 1.68414620e-05 6.13170349e-04
 2.81673164e-04 2.05226390e-04]
ene_total = [0.17986649 0.2865125  0.18171397 0.17059831 0.31810109 0.25243665
 0.16951713 0.18532807 0.24634661 0.23464818]
ti_comp = [0.29859544 0.27960232 0.29781508 0.29997427 0.28009078 0.27335988
 0.30046097 0.30052202 0.27442876 0.27803133]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.25514581e-05 2.39268435e-04 2.16073258e-05 8.88109865e-07
 3.67109805e-04 4.18618440e-05 1.75321215e-06 6.38316142e-05
 2.93224432e-05 2.13642617e-05]
ene_total = [0.53264186 0.70549825 0.53893943 0.51960869 0.71195131 0.74036622
 0.5157035  0.52027595 0.73061032 0.70053117]
optimize_network iter = 2 obj = 6.216126697450847
eta = 0.6801612059033234
freqs = [5.32566467e+07 1.19616625e+08 5.25486356e+07 1.80913467e+07
 1.37882470e+08 6.74069266e+07 2.26825634e+07 7.51732237e+07
 5.97860936e+07 5.35642116e+07]
Done!
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.14422044e-05 2.27499377e-04 2.05445117e-05 8.44425808e-07
 3.49052528e-04 3.98027573e-05 1.66697572e-06 6.06918856e-05
 2.78801405e-05 2.03134033e-05]
ene_total = [0.00651925 0.00862462 0.00659639 0.00636077 0.00869732 0.00906117
 0.00631292 0.00636584 0.00894236 0.00857453]
At round 6 energy consumption: 0.07605515902799734
At round 6 eta: 0.6801612059033234
At round 6 a_n: 26.12732777443847
At round 6 local rounds: 12.620787147614399
At round 6 global rounds: 81.68905166188517
gradient difference: 0.38900095224380493
train() client id: f_00000-0-0 loss: 1.227262  [   32/  126]
train() client id: f_00000-0-1 loss: 1.184853  [   64/  126]
train() client id: f_00000-0-2 loss: 1.139717  [   96/  126]
train() client id: f_00000-1-0 loss: 1.096727  [   32/  126]
train() client id: f_00000-1-1 loss: 0.998630  [   64/  126]
train() client id: f_00000-1-2 loss: 1.051880  [   96/  126]
train() client id: f_00000-2-0 loss: 1.036568  [   32/  126]
train() client id: f_00000-2-1 loss: 0.997734  [   64/  126]
train() client id: f_00000-2-2 loss: 0.977814  [   96/  126]
train() client id: f_00000-3-0 loss: 1.010821  [   32/  126]
train() client id: f_00000-3-1 loss: 0.939221  [   64/  126]
train() client id: f_00000-3-2 loss: 0.912820  [   96/  126]
train() client id: f_00000-4-0 loss: 0.997550  [   32/  126]
train() client id: f_00000-4-1 loss: 1.109234  [   64/  126]
train() client id: f_00000-4-2 loss: 0.914908  [   96/  126]
train() client id: f_00000-5-0 loss: 0.940203  [   32/  126]
train() client id: f_00000-5-1 loss: 0.887595  [   64/  126]
train() client id: f_00000-5-2 loss: 0.889491  [   96/  126]
train() client id: f_00000-6-0 loss: 0.933008  [   32/  126]
train() client id: f_00000-6-1 loss: 0.986608  [   64/  126]
train() client id: f_00000-6-2 loss: 0.916016  [   96/  126]
train() client id: f_00000-7-0 loss: 0.797109  [   32/  126]
train() client id: f_00000-7-1 loss: 0.911954  [   64/  126]
train() client id: f_00000-7-2 loss: 0.873198  [   96/  126]
train() client id: f_00000-8-0 loss: 0.808842  [   32/  126]
train() client id: f_00000-8-1 loss: 0.897286  [   64/  126]
train() client id: f_00000-8-2 loss: 0.967140  [   96/  126]
train() client id: f_00000-9-0 loss: 0.844459  [   32/  126]
train() client id: f_00000-9-1 loss: 0.890995  [   64/  126]
train() client id: f_00000-9-2 loss: 0.916447  [   96/  126]
train() client id: f_00000-10-0 loss: 0.981893  [   32/  126]
train() client id: f_00000-10-1 loss: 0.828050  [   64/  126]
train() client id: f_00000-10-2 loss: 0.873494  [   96/  126]
train() client id: f_00000-11-0 loss: 0.894510  [   32/  126]
train() client id: f_00000-11-1 loss: 0.872303  [   64/  126]
train() client id: f_00000-11-2 loss: 0.922748  [   96/  126]
train() client id: f_00001-0-0 loss: 0.456860  [   32/  265]
train() client id: f_00001-0-1 loss: 0.553293  [   64/  265]
train() client id: f_00001-0-2 loss: 0.516996  [   96/  265]
train() client id: f_00001-0-3 loss: 0.431750  [  128/  265]
train() client id: f_00001-0-4 loss: 0.619167  [  160/  265]
train() client id: f_00001-0-5 loss: 0.470362  [  192/  265]
train() client id: f_00001-0-6 loss: 0.597568  [  224/  265]
train() client id: f_00001-0-7 loss: 0.476727  [  256/  265]
train() client id: f_00001-1-0 loss: 0.655672  [   32/  265]
train() client id: f_00001-1-1 loss: 0.493133  [   64/  265]
train() client id: f_00001-1-2 loss: 0.424757  [   96/  265]
train() client id: f_00001-1-3 loss: 0.419456  [  128/  265]
train() client id: f_00001-1-4 loss: 0.477508  [  160/  265]
train() client id: f_00001-1-5 loss: 0.542556  [  192/  265]
train() client id: f_00001-1-6 loss: 0.492558  [  224/  265]
train() client id: f_00001-1-7 loss: 0.444939  [  256/  265]
train() client id: f_00001-2-0 loss: 0.496121  [   32/  265]
train() client id: f_00001-2-1 loss: 0.521839  [   64/  265]
train() client id: f_00001-2-2 loss: 0.452946  [   96/  265]
train() client id: f_00001-2-3 loss: 0.510101  [  128/  265]
train() client id: f_00001-2-4 loss: 0.494298  [  160/  265]
train() client id: f_00001-2-5 loss: 0.435660  [  192/  265]
train() client id: f_00001-2-6 loss: 0.468209  [  224/  265]
train() client id: f_00001-2-7 loss: 0.460623  [  256/  265]
train() client id: f_00001-3-0 loss: 0.592626  [   32/  265]
train() client id: f_00001-3-1 loss: 0.442926  [   64/  265]
train() client id: f_00001-3-2 loss: 0.552814  [   96/  265]
train() client id: f_00001-3-3 loss: 0.453840  [  128/  265]
train() client id: f_00001-3-4 loss: 0.435344  [  160/  265]
train() client id: f_00001-3-5 loss: 0.415742  [  192/  265]
train() client id: f_00001-3-6 loss: 0.462129  [  224/  265]
train() client id: f_00001-3-7 loss: 0.439128  [  256/  265]
train() client id: f_00001-4-0 loss: 0.552694  [   32/  265]
train() client id: f_00001-4-1 loss: 0.459857  [   64/  265]
train() client id: f_00001-4-2 loss: 0.494197  [   96/  265]
train() client id: f_00001-4-3 loss: 0.486067  [  128/  265]
train() client id: f_00001-4-4 loss: 0.399017  [  160/  265]
train() client id: f_00001-4-5 loss: 0.418621  [  192/  265]
train() client id: f_00001-4-6 loss: 0.444149  [  224/  265]
train() client id: f_00001-4-7 loss: 0.491402  [  256/  265]
train() client id: f_00001-5-0 loss: 0.368705  [   32/  265]
train() client id: f_00001-5-1 loss: 0.410297  [   64/  265]
train() client id: f_00001-5-2 loss: 0.419683  [   96/  265]
train() client id: f_00001-5-3 loss: 0.501185  [  128/  265]
train() client id: f_00001-5-4 loss: 0.514718  [  160/  265]
train() client id: f_00001-5-5 loss: 0.549894  [  192/  265]
train() client id: f_00001-5-6 loss: 0.419491  [  224/  265]
train() client id: f_00001-5-7 loss: 0.444273  [  256/  265]
train() client id: f_00001-6-0 loss: 0.494358  [   32/  265]
train() client id: f_00001-6-1 loss: 0.462907  [   64/  265]
train() client id: f_00001-6-2 loss: 0.358416  [   96/  265]
train() client id: f_00001-6-3 loss: 0.499113  [  128/  265]
train() client id: f_00001-6-4 loss: 0.465536  [  160/  265]
train() client id: f_00001-6-5 loss: 0.447066  [  192/  265]
train() client id: f_00001-6-6 loss: 0.516751  [  224/  265]
train() client id: f_00001-6-7 loss: 0.410653  [  256/  265]
train() client id: f_00001-7-0 loss: 0.425564  [   32/  265]
train() client id: f_00001-7-1 loss: 0.391429  [   64/  265]
train() client id: f_00001-7-2 loss: 0.418548  [   96/  265]
train() client id: f_00001-7-3 loss: 0.561916  [  128/  265]
train() client id: f_00001-7-4 loss: 0.470297  [  160/  265]
train() client id: f_00001-7-5 loss: 0.467067  [  192/  265]
train() client id: f_00001-7-6 loss: 0.373789  [  224/  265]
train() client id: f_00001-7-7 loss: 0.438099  [  256/  265]
train() client id: f_00001-8-0 loss: 0.422642  [   32/  265]
train() client id: f_00001-8-1 loss: 0.551518  [   64/  265]
train() client id: f_00001-8-2 loss: 0.432532  [   96/  265]
train() client id: f_00001-8-3 loss: 0.388724  [  128/  265]
train() client id: f_00001-8-4 loss: 0.447738  [  160/  265]
train() client id: f_00001-8-5 loss: 0.479403  [  192/  265]
train() client id: f_00001-8-6 loss: 0.456151  [  224/  265]
train() client id: f_00001-8-7 loss: 0.410077  [  256/  265]
train() client id: f_00001-9-0 loss: 0.535768  [   32/  265]
train() client id: f_00001-9-1 loss: 0.385047  [   64/  265]
train() client id: f_00001-9-2 loss: 0.452144  [   96/  265]
train() client id: f_00001-9-3 loss: 0.512294  [  128/  265]
train() client id: f_00001-9-4 loss: 0.467422  [  160/  265]
train() client id: f_00001-9-5 loss: 0.406074  [  192/  265]
train() client id: f_00001-9-6 loss: 0.476669  [  224/  265]
train() client id: f_00001-9-7 loss: 0.361883  [  256/  265]
train() client id: f_00001-10-0 loss: 0.521026  [   32/  265]
train() client id: f_00001-10-1 loss: 0.368088  [   64/  265]
train() client id: f_00001-10-2 loss: 0.509080  [   96/  265]
train() client id: f_00001-10-3 loss: 0.493959  [  128/  265]
train() client id: f_00001-10-4 loss: 0.395918  [  160/  265]
train() client id: f_00001-10-5 loss: 0.505680  [  192/  265]
train() client id: f_00001-10-6 loss: 0.370786  [  224/  265]
train() client id: f_00001-10-7 loss: 0.424054  [  256/  265]
train() client id: f_00001-11-0 loss: 0.604277  [   32/  265]
train() client id: f_00001-11-1 loss: 0.441210  [   64/  265]
train() client id: f_00001-11-2 loss: 0.472179  [   96/  265]
train() client id: f_00001-11-3 loss: 0.346153  [  128/  265]
train() client id: f_00001-11-4 loss: 0.470101  [  160/  265]
train() client id: f_00001-11-5 loss: 0.432701  [  192/  265]
train() client id: f_00001-11-6 loss: 0.330057  [  224/  265]
train() client id: f_00001-11-7 loss: 0.487165  [  256/  265]
train() client id: f_00002-0-0 loss: 1.172129  [   32/  124]
train() client id: f_00002-0-1 loss: 1.303503  [   64/  124]
train() client id: f_00002-0-2 loss: 1.214888  [   96/  124]
train() client id: f_00002-1-0 loss: 1.280470  [   32/  124]
train() client id: f_00002-1-1 loss: 1.161001  [   64/  124]
train() client id: f_00002-1-2 loss: 1.196323  [   96/  124]
train() client id: f_00002-2-0 loss: 1.160655  [   32/  124]
train() client id: f_00002-2-1 loss: 1.228069  [   64/  124]
train() client id: f_00002-2-2 loss: 1.153519  [   96/  124]
train() client id: f_00002-3-0 loss: 1.222365  [   32/  124]
train() client id: f_00002-3-1 loss: 1.071763  [   64/  124]
train() client id: f_00002-3-2 loss: 1.230719  [   96/  124]
train() client id: f_00002-4-0 loss: 1.056742  [   32/  124]
train() client id: f_00002-4-1 loss: 1.119053  [   64/  124]
train() client id: f_00002-4-2 loss: 1.222848  [   96/  124]
train() client id: f_00002-5-0 loss: 1.225809  [   32/  124]
train() client id: f_00002-5-1 loss: 1.065970  [   64/  124]
train() client id: f_00002-5-2 loss: 1.096651  [   96/  124]
train() client id: f_00002-6-0 loss: 1.168710  [   32/  124]
train() client id: f_00002-6-1 loss: 1.134803  [   64/  124]
train() client id: f_00002-6-2 loss: 0.872260  [   96/  124]
train() client id: f_00002-7-0 loss: 1.171170  [   32/  124]
train() client id: f_00002-7-1 loss: 0.933607  [   64/  124]
train() client id: f_00002-7-2 loss: 1.261999  [   96/  124]
train() client id: f_00002-8-0 loss: 0.999868  [   32/  124]
train() client id: f_00002-8-1 loss: 1.068636  [   64/  124]
train() client id: f_00002-8-2 loss: 1.205390  [   96/  124]
train() client id: f_00002-9-0 loss: 1.187472  [   32/  124]
train() client id: f_00002-9-1 loss: 0.902714  [   64/  124]
train() client id: f_00002-9-2 loss: 1.103692  [   96/  124]
train() client id: f_00002-10-0 loss: 1.079823  [   32/  124]
train() client id: f_00002-10-1 loss: 1.087911  [   64/  124]
train() client id: f_00002-10-2 loss: 1.140464  [   96/  124]
train() client id: f_00002-11-0 loss: 1.013816  [   32/  124]
train() client id: f_00002-11-1 loss: 1.223011  [   64/  124]
train() client id: f_00002-11-2 loss: 1.023655  [   96/  124]
train() client id: f_00003-0-0 loss: 0.945056  [   32/   43]
train() client id: f_00003-1-0 loss: 0.911126  [   32/   43]
train() client id: f_00003-2-0 loss: 0.956346  [   32/   43]
train() client id: f_00003-3-0 loss: 1.013281  [   32/   43]
train() client id: f_00003-4-0 loss: 0.990696  [   32/   43]
train() client id: f_00003-5-0 loss: 1.068341  [   32/   43]
train() client id: f_00003-6-0 loss: 0.846344  [   32/   43]
train() client id: f_00003-7-0 loss: 0.855535  [   32/   43]
train() client id: f_00003-8-0 loss: 0.971214  [   32/   43]
train() client id: f_00003-9-0 loss: 1.051123  [   32/   43]
train() client id: f_00003-10-0 loss: 0.916387  [   32/   43]
train() client id: f_00003-11-0 loss: 0.942748  [   32/   43]
train() client id: f_00004-0-0 loss: 0.835999  [   32/  306]
train() client id: f_00004-0-1 loss: 0.894986  [   64/  306]
train() client id: f_00004-0-2 loss: 0.944888  [   96/  306]
train() client id: f_00004-0-3 loss: 0.849010  [  128/  306]
train() client id: f_00004-0-4 loss: 0.899729  [  160/  306]
train() client id: f_00004-0-5 loss: 0.905421  [  192/  306]
train() client id: f_00004-0-6 loss: 0.810014  [  224/  306]
train() client id: f_00004-0-7 loss: 0.884895  [  256/  306]
train() client id: f_00004-0-8 loss: 0.859065  [  288/  306]
train() client id: f_00004-1-0 loss: 0.867534  [   32/  306]
train() client id: f_00004-1-1 loss: 0.873029  [   64/  306]
train() client id: f_00004-1-2 loss: 0.876208  [   96/  306]
train() client id: f_00004-1-3 loss: 0.897632  [  128/  306]
train() client id: f_00004-1-4 loss: 0.848758  [  160/  306]
train() client id: f_00004-1-5 loss: 0.816201  [  192/  306]
train() client id: f_00004-1-6 loss: 0.932797  [  224/  306]
train() client id: f_00004-1-7 loss: 0.920797  [  256/  306]
train() client id: f_00004-1-8 loss: 0.903413  [  288/  306]
train() client id: f_00004-2-0 loss: 0.843306  [   32/  306]
train() client id: f_00004-2-1 loss: 0.838015  [   64/  306]
train() client id: f_00004-2-2 loss: 0.920140  [   96/  306]
train() client id: f_00004-2-3 loss: 0.874174  [  128/  306]
train() client id: f_00004-2-4 loss: 0.942532  [  160/  306]
train() client id: f_00004-2-5 loss: 0.898698  [  192/  306]
train() client id: f_00004-2-6 loss: 0.928926  [  224/  306]
train() client id: f_00004-2-7 loss: 0.801550  [  256/  306]
train() client id: f_00004-2-8 loss: 0.817812  [  288/  306]
train() client id: f_00004-3-0 loss: 0.919015  [   32/  306]
train() client id: f_00004-3-1 loss: 0.851544  [   64/  306]
train() client id: f_00004-3-2 loss: 0.789501  [   96/  306]
train() client id: f_00004-3-3 loss: 0.873768  [  128/  306]
train() client id: f_00004-3-4 loss: 0.852082  [  160/  306]
train() client id: f_00004-3-5 loss: 0.916227  [  192/  306]
train() client id: f_00004-3-6 loss: 0.847025  [  224/  306]
train() client id: f_00004-3-7 loss: 0.855464  [  256/  306]
train() client id: f_00004-3-8 loss: 0.928901  [  288/  306]
train() client id: f_00004-4-0 loss: 0.870051  [   32/  306]
train() client id: f_00004-4-1 loss: 0.963692  [   64/  306]
train() client id: f_00004-4-2 loss: 0.842353  [   96/  306]
train() client id: f_00004-4-3 loss: 0.921115  [  128/  306]
train() client id: f_00004-4-4 loss: 0.971855  [  160/  306]
train() client id: f_00004-4-5 loss: 0.729008  [  192/  306]
train() client id: f_00004-4-6 loss: 0.904976  [  224/  306]
train() client id: f_00004-4-7 loss: 0.814890  [  256/  306]
train() client id: f_00004-4-8 loss: 0.839096  [  288/  306]
train() client id: f_00004-5-0 loss: 0.900324  [   32/  306]
train() client id: f_00004-5-1 loss: 0.863708  [   64/  306]
train() client id: f_00004-5-2 loss: 0.826399  [   96/  306]
train() client id: f_00004-5-3 loss: 0.887652  [  128/  306]
train() client id: f_00004-5-4 loss: 0.908575  [  160/  306]
train() client id: f_00004-5-5 loss: 0.914818  [  192/  306]
train() client id: f_00004-5-6 loss: 0.891949  [  224/  306]
train() client id: f_00004-5-7 loss: 0.847677  [  256/  306]
train() client id: f_00004-5-8 loss: 0.911315  [  288/  306]
train() client id: f_00004-6-0 loss: 0.946827  [   32/  306]
train() client id: f_00004-6-1 loss: 0.794886  [   64/  306]
train() client id: f_00004-6-2 loss: 0.902169  [   96/  306]
train() client id: f_00004-6-3 loss: 0.871754  [  128/  306]
train() client id: f_00004-6-4 loss: 0.910619  [  160/  306]
train() client id: f_00004-6-5 loss: 0.865600  [  192/  306]
train() client id: f_00004-6-6 loss: 0.898874  [  224/  306]
train() client id: f_00004-6-7 loss: 0.831604  [  256/  306]
train() client id: f_00004-6-8 loss: 0.853055  [  288/  306]
train() client id: f_00004-7-0 loss: 0.827204  [   32/  306]
train() client id: f_00004-7-1 loss: 0.880336  [   64/  306]
train() client id: f_00004-7-2 loss: 0.874285  [   96/  306]
train() client id: f_00004-7-3 loss: 0.923214  [  128/  306]
train() client id: f_00004-7-4 loss: 0.840958  [  160/  306]
train() client id: f_00004-7-5 loss: 0.863437  [  192/  306]
train() client id: f_00004-7-6 loss: 0.862097  [  224/  306]
train() client id: f_00004-7-7 loss: 0.830851  [  256/  306]
train() client id: f_00004-7-8 loss: 0.962075  [  288/  306]
train() client id: f_00004-8-0 loss: 0.846190  [   32/  306]
train() client id: f_00004-8-1 loss: 0.918600  [   64/  306]
train() client id: f_00004-8-2 loss: 0.913899  [   96/  306]
train() client id: f_00004-8-3 loss: 0.910146  [  128/  306]
train() client id: f_00004-8-4 loss: 0.843674  [  160/  306]
train() client id: f_00004-8-5 loss: 0.832144  [  192/  306]
train() client id: f_00004-8-6 loss: 0.984477  [  224/  306]
train() client id: f_00004-8-7 loss: 0.901443  [  256/  306]
train() client id: f_00004-8-8 loss: 0.800054  [  288/  306]
train() client id: f_00004-9-0 loss: 0.854518  [   32/  306]
train() client id: f_00004-9-1 loss: 0.913302  [   64/  306]
train() client id: f_00004-9-2 loss: 0.873140  [   96/  306]
train() client id: f_00004-9-3 loss: 0.920686  [  128/  306]
train() client id: f_00004-9-4 loss: 0.835612  [  160/  306]
train() client id: f_00004-9-5 loss: 0.760866  [  192/  306]
train() client id: f_00004-9-6 loss: 0.872418  [  224/  306]
train() client id: f_00004-9-7 loss: 0.876904  [  256/  306]
train() client id: f_00004-9-8 loss: 0.932400  [  288/  306]
train() client id: f_00004-10-0 loss: 0.876716  [   32/  306]
train() client id: f_00004-10-1 loss: 0.827050  [   64/  306]
train() client id: f_00004-10-2 loss: 0.843589  [   96/  306]
train() client id: f_00004-10-3 loss: 0.836487  [  128/  306]
train() client id: f_00004-10-4 loss: 0.938970  [  160/  306]
train() client id: f_00004-10-5 loss: 0.788279  [  192/  306]
train() client id: f_00004-10-6 loss: 0.862690  [  224/  306]
train() client id: f_00004-10-7 loss: 0.974751  [  256/  306]
train() client id: f_00004-10-8 loss: 1.000734  [  288/  306]
train() client id: f_00004-11-0 loss: 1.004089  [   32/  306]
train() client id: f_00004-11-1 loss: 0.944749  [   64/  306]
train() client id: f_00004-11-2 loss: 0.811992  [   96/  306]
train() client id: f_00004-11-3 loss: 0.891146  [  128/  306]
train() client id: f_00004-11-4 loss: 0.870981  [  160/  306]
train() client id: f_00004-11-5 loss: 0.753843  [  192/  306]
train() client id: f_00004-11-6 loss: 0.865957  [  224/  306]
train() client id: f_00004-11-7 loss: 0.959422  [  256/  306]
train() client id: f_00004-11-8 loss: 0.893328  [  288/  306]
train() client id: f_00005-0-0 loss: 0.637356  [   32/  146]
train() client id: f_00005-0-1 loss: 0.486470  [   64/  146]
train() client id: f_00005-0-2 loss: 0.364355  [   96/  146]
train() client id: f_00005-0-3 loss: 0.797434  [  128/  146]
train() client id: f_00005-1-0 loss: 0.578398  [   32/  146]
train() client id: f_00005-1-1 loss: 0.549702  [   64/  146]
train() client id: f_00005-1-2 loss: 0.355293  [   96/  146]
train() client id: f_00005-1-3 loss: 0.617080  [  128/  146]
train() client id: f_00005-2-0 loss: 0.410029  [   32/  146]
train() client id: f_00005-2-1 loss: 0.381540  [   64/  146]
train() client id: f_00005-2-2 loss: 0.451190  [   96/  146]
train() client id: f_00005-2-3 loss: 0.739907  [  128/  146]
train() client id: f_00005-3-0 loss: 0.734560  [   32/  146]
train() client id: f_00005-3-1 loss: 0.528731  [   64/  146]
train() client id: f_00005-3-2 loss: 0.508328  [   96/  146]
train() client id: f_00005-3-3 loss: 0.400864  [  128/  146]
train() client id: f_00005-4-0 loss: 0.773009  [   32/  146]
train() client id: f_00005-4-1 loss: 0.539073  [   64/  146]
train() client id: f_00005-4-2 loss: 0.381757  [   96/  146]
train() client id: f_00005-4-3 loss: 0.457141  [  128/  146]
train() client id: f_00005-5-0 loss: 0.662046  [   32/  146]
train() client id: f_00005-5-1 loss: 0.503067  [   64/  146]
train() client id: f_00005-5-2 loss: 0.324626  [   96/  146]
train() client id: f_00005-5-3 loss: 0.467655  [  128/  146]
train() client id: f_00005-6-0 loss: 0.527064  [   32/  146]
train() client id: f_00005-6-1 loss: 0.637124  [   64/  146]
train() client id: f_00005-6-2 loss: 0.264232  [   96/  146]
train() client id: f_00005-6-3 loss: 0.570300  [  128/  146]
train() client id: f_00005-7-0 loss: 0.247888  [   32/  146]
train() client id: f_00005-7-1 loss: 0.479973  [   64/  146]
train() client id: f_00005-7-2 loss: 0.560898  [   96/  146]
train() client id: f_00005-7-3 loss: 0.801913  [  128/  146]
train() client id: f_00005-8-0 loss: 0.743659  [   32/  146]
train() client id: f_00005-8-1 loss: 0.795759  [   64/  146]
train() client id: f_00005-8-2 loss: 0.167675  [   96/  146]
train() client id: f_00005-8-3 loss: 0.435049  [  128/  146]
train() client id: f_00005-9-0 loss: 0.539756  [   32/  146]
train() client id: f_00005-9-1 loss: 0.588750  [   64/  146]
train() client id: f_00005-9-2 loss: 0.375496  [   96/  146]
train() client id: f_00005-9-3 loss: 0.386519  [  128/  146]
train() client id: f_00005-10-0 loss: 0.566779  [   32/  146]
train() client id: f_00005-10-1 loss: 0.369235  [   64/  146]
train() client id: f_00005-10-2 loss: 0.627791  [   96/  146]
train() client id: f_00005-10-3 loss: 0.508995  [  128/  146]
train() client id: f_00005-11-0 loss: 0.557454  [   32/  146]
train() client id: f_00005-11-1 loss: 0.473054  [   64/  146]
train() client id: f_00005-11-2 loss: 0.451738  [   96/  146]
train() client id: f_00005-11-3 loss: 0.457945  [  128/  146]
train() client id: f_00006-0-0 loss: 0.652619  [   32/   54]
train() client id: f_00006-1-0 loss: 0.696123  [   32/   54]
train() client id: f_00006-2-0 loss: 0.651954  [   32/   54]
train() client id: f_00006-3-0 loss: 0.669388  [   32/   54]
train() client id: f_00006-4-0 loss: 0.698026  [   32/   54]
train() client id: f_00006-5-0 loss: 0.634097  [   32/   54]
train() client id: f_00006-6-0 loss: 0.641909  [   32/   54]
train() client id: f_00006-7-0 loss: 0.670876  [   32/   54]
train() client id: f_00006-8-0 loss: 0.683562  [   32/   54]
train() client id: f_00006-9-0 loss: 0.707155  [   32/   54]
train() client id: f_00006-10-0 loss: 0.626577  [   32/   54]
train() client id: f_00006-11-0 loss: 0.676975  [   32/   54]
train() client id: f_00007-0-0 loss: 0.893389  [   32/  179]
train() client id: f_00007-0-1 loss: 0.785126  [   64/  179]
train() client id: f_00007-0-2 loss: 0.709473  [   96/  179]
train() client id: f_00007-0-3 loss: 0.883212  [  128/  179]
train() client id: f_00007-0-4 loss: 0.785971  [  160/  179]
train() client id: f_00007-1-0 loss: 0.810991  [   32/  179]
train() client id: f_00007-1-1 loss: 0.789609  [   64/  179]
train() client id: f_00007-1-2 loss: 0.735485  [   96/  179]
train() client id: f_00007-1-3 loss: 0.760936  [  128/  179]
train() client id: f_00007-1-4 loss: 0.824988  [  160/  179]
train() client id: f_00007-2-0 loss: 0.827511  [   32/  179]
train() client id: f_00007-2-1 loss: 0.839492  [   64/  179]
train() client id: f_00007-2-2 loss: 0.712905  [   96/  179]
train() client id: f_00007-2-3 loss: 0.728873  [  128/  179]
train() client id: f_00007-2-4 loss: 0.723647  [  160/  179]
train() client id: f_00007-3-0 loss: 0.691097  [   32/  179]
train() client id: f_00007-3-1 loss: 0.825851  [   64/  179]
train() client id: f_00007-3-2 loss: 0.844365  [   96/  179]
train() client id: f_00007-3-3 loss: 0.737471  [  128/  179]
train() client id: f_00007-3-4 loss: 0.740222  [  160/  179]
train() client id: f_00007-4-0 loss: 0.797987  [   32/  179]
train() client id: f_00007-4-1 loss: 0.695711  [   64/  179]
train() client id: f_00007-4-2 loss: 0.739661  [   96/  179]
train() client id: f_00007-4-3 loss: 0.742622  [  128/  179]
train() client id: f_00007-4-4 loss: 0.833216  [  160/  179]
train() client id: f_00007-5-0 loss: 0.707378  [   32/  179]
train() client id: f_00007-5-1 loss: 0.648310  [   64/  179]
train() client id: f_00007-5-2 loss: 0.894698  [   96/  179]
train() client id: f_00007-5-3 loss: 0.604667  [  128/  179]
train() client id: f_00007-5-4 loss: 0.772898  [  160/  179]
train() client id: f_00007-6-0 loss: 0.778544  [   32/  179]
train() client id: f_00007-6-1 loss: 0.886172  [   64/  179]
train() client id: f_00007-6-2 loss: 0.831959  [   96/  179]
train() client id: f_00007-6-3 loss: 0.679600  [  128/  179]
train() client id: f_00007-6-4 loss: 0.593294  [  160/  179]
train() client id: f_00007-7-0 loss: 0.717676  [   32/  179]
train() client id: f_00007-7-1 loss: 0.674406  [   64/  179]
train() client id: f_00007-7-2 loss: 0.756610  [   96/  179]
train() client id: f_00007-7-3 loss: 0.705150  [  128/  179]
train() client id: f_00007-7-4 loss: 0.658589  [  160/  179]
train() client id: f_00007-8-0 loss: 0.597109  [   32/  179]
train() client id: f_00007-8-1 loss: 0.576953  [   64/  179]
train() client id: f_00007-8-2 loss: 0.795369  [   96/  179]
train() client id: f_00007-8-3 loss: 0.757677  [  128/  179]
train() client id: f_00007-8-4 loss: 0.998217  [  160/  179]
train() client id: f_00007-9-0 loss: 0.715294  [   32/  179]
train() client id: f_00007-9-1 loss: 0.588404  [   64/  179]
train() client id: f_00007-9-2 loss: 0.847369  [   96/  179]
train() client id: f_00007-9-3 loss: 0.719512  [  128/  179]
train() client id: f_00007-9-4 loss: 0.690676  [  160/  179]
train() client id: f_00007-10-0 loss: 0.765924  [   32/  179]
train() client id: f_00007-10-1 loss: 0.655102  [   64/  179]
train() client id: f_00007-10-2 loss: 0.766034  [   96/  179]
train() client id: f_00007-10-3 loss: 0.585925  [  128/  179]
train() client id: f_00007-10-4 loss: 0.936530  [  160/  179]
train() client id: f_00007-11-0 loss: 0.690807  [   32/  179]
train() client id: f_00007-11-1 loss: 0.591119  [   64/  179]
train() client id: f_00007-11-2 loss: 0.772142  [   96/  179]
train() client id: f_00007-11-3 loss: 0.705684  [  128/  179]
train() client id: f_00007-11-4 loss: 0.905140  [  160/  179]
train() client id: f_00008-0-0 loss: 0.939671  [   32/  130]
train() client id: f_00008-0-1 loss: 0.902603  [   64/  130]
train() client id: f_00008-0-2 loss: 0.849084  [   96/  130]
train() client id: f_00008-0-3 loss: 0.951776  [  128/  130]
train() client id: f_00008-1-0 loss: 0.903774  [   32/  130]
train() client id: f_00008-1-1 loss: 0.857460  [   64/  130]
train() client id: f_00008-1-2 loss: 0.857692  [   96/  130]
train() client id: f_00008-1-3 loss: 0.986249  [  128/  130]
train() client id: f_00008-2-0 loss: 0.779002  [   32/  130]
train() client id: f_00008-2-1 loss: 0.996699  [   64/  130]
train() client id: f_00008-2-2 loss: 0.914246  [   96/  130]
train() client id: f_00008-2-3 loss: 0.911066  [  128/  130]
train() client id: f_00008-3-0 loss: 0.880118  [   32/  130]
train() client id: f_00008-3-1 loss: 0.911343  [   64/  130]
train() client id: f_00008-3-2 loss: 0.952907  [   96/  130]
train() client id: f_00008-3-3 loss: 0.852085  [  128/  130]
train() client id: f_00008-4-0 loss: 0.950903  [   32/  130]
train() client id: f_00008-4-1 loss: 0.927035  [   64/  130]
train() client id: f_00008-4-2 loss: 0.834016  [   96/  130]
train() client id: f_00008-4-3 loss: 0.881935  [  128/  130]
train() client id: f_00008-5-0 loss: 0.934127  [   32/  130]
train() client id: f_00008-5-1 loss: 0.918418  [   64/  130]
train() client id: f_00008-5-2 loss: 0.838803  [   96/  130]
train() client id: f_00008-5-3 loss: 0.897161  [  128/  130]
train() client id: f_00008-6-0 loss: 0.872422  [   32/  130]
train() client id: f_00008-6-1 loss: 0.862798  [   64/  130]
train() client id: f_00008-6-2 loss: 0.951403  [   96/  130]
train() client id: f_00008-6-3 loss: 0.895091  [  128/  130]
train() client id: f_00008-7-0 loss: 0.841870  [   32/  130]
train() client id: f_00008-7-1 loss: 0.890567  [   64/  130]
train() client id: f_00008-7-2 loss: 0.877918  [   96/  130]
train() client id: f_00008-7-3 loss: 0.967630  [  128/  130]
train() client id: f_00008-8-0 loss: 0.854842  [   32/  130]
train() client id: f_00008-8-1 loss: 0.906119  [   64/  130]
train() client id: f_00008-8-2 loss: 0.907637  [   96/  130]
train() client id: f_00008-8-3 loss: 0.911076  [  128/  130]
train() client id: f_00008-9-0 loss: 0.949405  [   32/  130]
train() client id: f_00008-9-1 loss: 0.966779  [   64/  130]
train() client id: f_00008-9-2 loss: 0.800721  [   96/  130]
train() client id: f_00008-9-3 loss: 0.850805  [  128/  130]
train() client id: f_00008-10-0 loss: 0.956440  [   32/  130]
train() client id: f_00008-10-1 loss: 0.891800  [   64/  130]
train() client id: f_00008-10-2 loss: 0.865720  [   96/  130]
train() client id: f_00008-10-3 loss: 0.844824  [  128/  130]
train() client id: f_00008-11-0 loss: 0.822061  [   32/  130]
train() client id: f_00008-11-1 loss: 1.022381  [   64/  130]
train() client id: f_00008-11-2 loss: 0.941970  [   96/  130]
train() client id: f_00008-11-3 loss: 0.765330  [  128/  130]
train() client id: f_00009-0-0 loss: 1.240740  [   32/  118]
train() client id: f_00009-0-1 loss: 1.311748  [   64/  118]
train() client id: f_00009-0-2 loss: 1.111534  [   96/  118]
train() client id: f_00009-1-0 loss: 1.133129  [   32/  118]
train() client id: f_00009-1-1 loss: 1.075130  [   64/  118]
train() client id: f_00009-1-2 loss: 1.206956  [   96/  118]
train() client id: f_00009-2-0 loss: 1.067084  [   32/  118]
train() client id: f_00009-2-1 loss: 1.130198  [   64/  118]
train() client id: f_00009-2-2 loss: 1.076339  [   96/  118]
train() client id: f_00009-3-0 loss: 1.081857  [   32/  118]
train() client id: f_00009-3-1 loss: 1.142810  [   64/  118]
train() client id: f_00009-3-2 loss: 1.014478  [   96/  118]
train() client id: f_00009-4-0 loss: 1.128274  [   32/  118]
train() client id: f_00009-4-1 loss: 1.085090  [   64/  118]
train() client id: f_00009-4-2 loss: 1.003287  [   96/  118]
train() client id: f_00009-5-0 loss: 0.987616  [   32/  118]
train() client id: f_00009-5-1 loss: 0.997914  [   64/  118]
train() client id: f_00009-5-2 loss: 1.034844  [   96/  118]
train() client id: f_00009-6-0 loss: 1.032924  [   32/  118]
train() client id: f_00009-6-1 loss: 1.019442  [   64/  118]
train() client id: f_00009-6-2 loss: 0.935330  [   96/  118]
train() client id: f_00009-7-0 loss: 1.112128  [   32/  118]
train() client id: f_00009-7-1 loss: 0.962617  [   64/  118]
train() client id: f_00009-7-2 loss: 0.955291  [   96/  118]
train() client id: f_00009-8-0 loss: 1.153098  [   32/  118]
train() client id: f_00009-8-1 loss: 0.978850  [   64/  118]
train() client id: f_00009-8-2 loss: 0.860714  [   96/  118]
train() client id: f_00009-9-0 loss: 0.995791  [   32/  118]
train() client id: f_00009-9-1 loss: 0.974164  [   64/  118]
train() client id: f_00009-9-2 loss: 1.001926  [   96/  118]
train() client id: f_00009-10-0 loss: 0.998007  [   32/  118]
train() client id: f_00009-10-1 loss: 0.856831  [   64/  118]
train() client id: f_00009-10-2 loss: 1.115909  [   96/  118]
train() client id: f_00009-11-0 loss: 0.988226  [   32/  118]
train() client id: f_00009-11-1 loss: 0.988069  [   64/  118]
train() client id: f_00009-11-2 loss: 1.033087  [   96/  118]
At round 6 accuracy: 0.6312997347480106
At round 6 training accuracy: 0.579476861167002
At round 6 training loss: 0.8490750355201694
update_location
xs = -3.905658 4.200318 50.009024 18.811294 -4.020704 3.956410 -12.443192 3.675148 34.663977 2.939121 
ys = 42.587959 25.555839 1.320614 -12.455176 9.350187 -12.185849 -2.624984 -4.177652 17.569006 4.001482 
xs mean: 9.788573831882573
ys mean: 6.894142535528705
dists_uav = 108.761153 103.299291 111.815234 102.513395 100.516626 100.817400 100.805375 100.154678 107.285886 100.123176 
uav_gains = -100.911898 -100.352459 -101.212603 -100.269538 -100.055965 -100.088405 -100.087110 -100.016797 -100.763608 -100.013382 
uav_gains_db_mean: -100.37717655551506
dists_bs = 216.347457 233.337334 284.242133 269.633568 238.062556 258.967061 240.780711 253.040391 262.189819 246.784976 
bs_gains = -104.951309 -105.870618 -108.270336 -107.628730 -106.114410 -107.137906 -106.252467 -106.856375 -107.288302 -106.551983 
bs_gains_db_mean: -106.69224353651109
Round 7
-------------------------------
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.06798939 21.0326556   9.91312762  3.54368464 24.2561337  11.70105956
  4.40648092 14.22483624 10.44629039  9.49400071]
obj_prev = 119.08625877018483
eta_min = 5.793901722590025e-10	eta_max = 0.9200929959618319
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 27.695540866338916	eta = 0.909090909090909
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 47.81923698001299	eta = 0.5265195769323551
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 38.22376664223015	eta = 0.6586939654483899
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.504271658621605	eta = 0.6897210457833629
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.41961812780936	eta = 0.6913242290346582
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.41939846351146	eta = 0.6913283987699586
eta = 0.6913283987699586
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [0.03046057 0.06406391 0.02997707 0.01039528 0.07397568 0.03529559
 0.01305453 0.04327336 0.03142758 0.02852657]
ene_total = [3.09168549 6.04664955 3.05847947 1.40079217 6.88392674 3.69460946
 1.61888686 4.14166343 3.37804549 3.10465979]
ti_comp = [0.28943977 0.27173208 0.28857988 0.29120492 0.27061872 0.2656134
 0.2916892  0.29187392 0.26482942 0.26854656]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.10852063e-05 2.22555556e-04 2.02169483e-05 8.27924223e-07
 3.45486619e-04 3.89530298e-05 1.63426776e-06 5.94499024e-05
 2.76617870e-05 2.01182160e-05]
ene_total = [0.54893267 0.71368309 0.55604324 0.53249539 0.73325245 0.74945808
 0.52851731 0.53180388 0.75506382 0.72338265]
optimize_network iter = 0 obj = 6.3726325729552675
eta = 0.6913283987699586
freqs = [5.26198833e+07 1.17880646e+08 5.19389521e+07 1.78487289e+07
 1.36678791e+08 6.64416515e+07 2.23774684e+07 7.41302190e+07
 5.93355081e+07 5.31128927e+07]
eta_min = 0.6839075543866936	eta_max = 0.6913283987699287
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 0.058240867111970324	eta = 0.9090909090909091
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 21.478076657689165	eta = 0.0024651296143925966
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.271073836819586	eta = 0.023313307551114187
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.1977536965308997	eta = 0.02409107213089359
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.1977247993541873	eta = 0.0240913888966546
eta = 0.0240913888966546
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.12822560e-04 2.24635427e-03 2.04058838e-04 8.35661507e-06
 3.48715330e-03 3.93170615e-04 1.64954064e-05 6.00054856e-04
 2.79202975e-04 2.03062288e-04]
ene_total = [0.17868869 0.2792033  0.18072909 0.16862269 0.31492846 0.24640612
 0.16755818 0.18248854 0.24546632 0.23363341]
ti_comp = [0.29797302 0.28026532 0.29711312 0.29973816 0.27915197 0.27414664
 0.30022245 0.30040716 0.27336266 0.27707981]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.16912899e-05 2.28100645e-04 2.07945251e-05 8.52018260e-07
 3.54005908e-04 3.98676209e-05 1.68198679e-06 6.11879833e-05
 2.83060505e-05 2.06045860e-05]
ene_total = [0.53609492 0.69738041 0.54303623 0.51999606 0.71673297 0.7319378
 0.5161133  0.5194606  0.73738988 0.7064396 ]
optimize_network iter = 1 obj = 6.224581758881931
eta = 0.6839075543866936
freqs = [5.26073161e+07 1.17632966e+08 5.19221168e+07 1.78475642e+07
 1.36374531e+08 6.62555801e+07 2.23770658e+07 7.41302190e+07
 5.91638860e+07 5.29821614e+07]
eta_min = 0.6839075543866936	eta_max = 0.6839075543866819
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 0.05801261670554691	eta = 0.9090909090909091
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 21.4778591118164	eta = 0.0024554934542136467
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.270009752860956	eta = 0.023232826375798606
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.196960837665713	eta = 0.024005317507445147
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.196932244064364	eta = 0.024005629942423928
eta = 0.024005629942423928
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.12924512e-04 2.23906549e-03 2.04121753e-04 8.36352164e-06
 3.47496787e-03 3.91345733e-04 1.65106003e-05 6.00629173e-04
 2.77855860e-04 2.02257286e-04]
ene_total = [0.17867568 0.27898621 0.18071488 0.16860805 0.31457886 0.24633625
 0.16754386 0.18248768 0.24540916 0.23359161]
ti_comp = [0.29797302 0.28026532 0.29711312 0.29973816 0.27915197 0.27414664
 0.30022245 0.30040716 0.27336266 0.27707981]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.16912899e-05 2.28100645e-04 2.07945251e-05 8.52018260e-07
 3.54005908e-04 3.98676209e-05 1.68198679e-06 6.11879833e-05
 2.83060505e-05 2.06045860e-05]
ene_total = [0.53609492 0.69738041 0.54303623 0.51999606 0.71673297 0.7319378
 0.5161133  0.5194606  0.73738988 0.7064396 ]
optimize_network iter = 2 obj = 6.224581758881931
eta = 0.6839075543866936
freqs = [5.26073161e+07 1.17632966e+08 5.19221168e+07 1.78475642e+07
 1.36374531e+08 6.62555801e+07 2.23770658e+07 7.41302190e+07
 5.91638860e+07 5.29821614e+07]
Done!
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.09225246e-05 2.20016485e-04 2.00575422e-05 8.21821712e-07
 3.41459515e-04 3.84546646e-05 1.62237516e-06 5.90194079e-05
 2.73028502e-05 1.98743347e-05]
ene_total = [0.00657115 0.00854102 0.00665628 0.00637454 0.00877379 0.00897132
 0.00632691 0.00636583 0.00903857 0.00865943]
At round 7 energy consumption: 0.07627883514893437
At round 7 eta: 0.6839075543866936
At round 7 a_n: 25.784781927469155
At round 7 local rounds: 12.440921153911754
At round 7 global rounds: 81.57354686993413
gradient difference: 0.41480594873428345
train() client id: f_00000-0-0 loss: 1.061092  [   32/  126]
train() client id: f_00000-0-1 loss: 1.139290  [   64/  126]
train() client id: f_00000-0-2 loss: 1.072484  [   96/  126]
train() client id: f_00000-1-0 loss: 0.960206  [   32/  126]
train() client id: f_00000-1-1 loss: 1.010665  [   64/  126]
train() client id: f_00000-1-2 loss: 1.013611  [   96/  126]
train() client id: f_00000-2-0 loss: 0.923288  [   32/  126]
train() client id: f_00000-2-1 loss: 0.912373  [   64/  126]
train() client id: f_00000-2-2 loss: 1.069468  [   96/  126]
train() client id: f_00000-3-0 loss: 0.870576  [   32/  126]
train() client id: f_00000-3-1 loss: 0.906483  [   64/  126]
train() client id: f_00000-3-2 loss: 0.930612  [   96/  126]
train() client id: f_00000-4-0 loss: 0.902185  [   32/  126]
train() client id: f_00000-4-1 loss: 1.023182  [   64/  126]
train() client id: f_00000-4-2 loss: 0.674539  [   96/  126]
train() client id: f_00000-5-0 loss: 0.856701  [   32/  126]
train() client id: f_00000-5-1 loss: 0.839945  [   64/  126]
train() client id: f_00000-5-2 loss: 0.702118  [   96/  126]
train() client id: f_00000-6-0 loss: 0.902477  [   32/  126]
train() client id: f_00000-6-1 loss: 0.871105  [   64/  126]
train() client id: f_00000-6-2 loss: 0.787666  [   96/  126]
train() client id: f_00000-7-0 loss: 0.807334  [   32/  126]
train() client id: f_00000-7-1 loss: 0.707456  [   64/  126]
train() client id: f_00000-7-2 loss: 0.964700  [   96/  126]
train() client id: f_00000-8-0 loss: 0.764279  [   32/  126]
train() client id: f_00000-8-1 loss: 0.617136  [   64/  126]
train() client id: f_00000-8-2 loss: 0.698702  [   96/  126]
train() client id: f_00000-9-0 loss: 0.852573  [   32/  126]
train() client id: f_00000-9-1 loss: 0.816971  [   64/  126]
train() client id: f_00000-9-2 loss: 0.688339  [   96/  126]
train() client id: f_00000-10-0 loss: 0.650428  [   32/  126]
train() client id: f_00000-10-1 loss: 0.844689  [   64/  126]
train() client id: f_00000-10-2 loss: 0.798798  [   96/  126]
train() client id: f_00000-11-0 loss: 0.789072  [   32/  126]
train() client id: f_00000-11-1 loss: 0.772855  [   64/  126]
train() client id: f_00000-11-2 loss: 0.722837  [   96/  126]
train() client id: f_00001-0-0 loss: 0.600323  [   32/  265]
train() client id: f_00001-0-1 loss: 0.509045  [   64/  265]
train() client id: f_00001-0-2 loss: 0.551254  [   96/  265]
train() client id: f_00001-0-3 loss: 0.569900  [  128/  265]
train() client id: f_00001-0-4 loss: 0.590835  [  160/  265]
train() client id: f_00001-0-5 loss: 0.533163  [  192/  265]
train() client id: f_00001-0-6 loss: 0.607280  [  224/  265]
train() client id: f_00001-0-7 loss: 0.577455  [  256/  265]
train() client id: f_00001-1-0 loss: 0.564763  [   32/  265]
train() client id: f_00001-1-1 loss: 0.547423  [   64/  265]
train() client id: f_00001-1-2 loss: 0.583166  [   96/  265]
train() client id: f_00001-1-3 loss: 0.518024  [  128/  265]
train() client id: f_00001-1-4 loss: 0.485509  [  160/  265]
train() client id: f_00001-1-5 loss: 0.494646  [  192/  265]
train() client id: f_00001-1-6 loss: 0.568819  [  224/  265]
train() client id: f_00001-1-7 loss: 0.574437  [  256/  265]
train() client id: f_00001-2-0 loss: 0.546946  [   32/  265]
train() client id: f_00001-2-1 loss: 0.476647  [   64/  265]
train() client id: f_00001-2-2 loss: 0.548051  [   96/  265]
train() client id: f_00001-2-3 loss: 0.480863  [  128/  265]
train() client id: f_00001-2-4 loss: 0.533771  [  160/  265]
train() client id: f_00001-2-5 loss: 0.565121  [  192/  265]
train() client id: f_00001-2-6 loss: 0.523654  [  224/  265]
train() client id: f_00001-2-7 loss: 0.529628  [  256/  265]
train() client id: f_00001-3-0 loss: 0.517962  [   32/  265]
train() client id: f_00001-3-1 loss: 0.624085  [   64/  265]
train() client id: f_00001-3-2 loss: 0.572728  [   96/  265]
train() client id: f_00001-3-3 loss: 0.450614  [  128/  265]
train() client id: f_00001-3-4 loss: 0.488773  [  160/  265]
train() client id: f_00001-3-5 loss: 0.555402  [  192/  265]
train() client id: f_00001-3-6 loss: 0.518676  [  224/  265]
train() client id: f_00001-3-7 loss: 0.501443  [  256/  265]
train() client id: f_00001-4-0 loss: 0.517140  [   32/  265]
train() client id: f_00001-4-1 loss: 0.672174  [   64/  265]
train() client id: f_00001-4-2 loss: 0.510268  [   96/  265]
train() client id: f_00001-4-3 loss: 0.499977  [  128/  265]
train() client id: f_00001-4-4 loss: 0.496534  [  160/  265]
train() client id: f_00001-4-5 loss: 0.439370  [  192/  265]
train() client id: f_00001-4-6 loss: 0.516429  [  224/  265]
train() client id: f_00001-4-7 loss: 0.522943  [  256/  265]
train() client id: f_00001-5-0 loss: 0.480040  [   32/  265]
train() client id: f_00001-5-1 loss: 0.535795  [   64/  265]
train() client id: f_00001-5-2 loss: 0.660077  [   96/  265]
train() client id: f_00001-5-3 loss: 0.431828  [  128/  265]
train() client id: f_00001-5-4 loss: 0.566994  [  160/  265]
train() client id: f_00001-5-5 loss: 0.525921  [  192/  265]
train() client id: f_00001-5-6 loss: 0.457355  [  224/  265]
train() client id: f_00001-5-7 loss: 0.506236  [  256/  265]
train() client id: f_00001-6-0 loss: 0.597363  [   32/  265]
train() client id: f_00001-6-1 loss: 0.433586  [   64/  265]
train() client id: f_00001-6-2 loss: 0.478474  [   96/  265]
train() client id: f_00001-6-3 loss: 0.502464  [  128/  265]
train() client id: f_00001-6-4 loss: 0.571507  [  160/  265]
train() client id: f_00001-6-5 loss: 0.539752  [  192/  265]
train() client id: f_00001-6-6 loss: 0.521161  [  224/  265]
train() client id: f_00001-6-7 loss: 0.413909  [  256/  265]
train() client id: f_00001-7-0 loss: 0.488599  [   32/  265]
train() client id: f_00001-7-1 loss: 0.574220  [   64/  265]
train() client id: f_00001-7-2 loss: 0.511475  [   96/  265]
train() client id: f_00001-7-3 loss: 0.495047  [  128/  265]
train() client id: f_00001-7-4 loss: 0.525427  [  160/  265]
train() client id: f_00001-7-5 loss: 0.462486  [  192/  265]
train() client id: f_00001-7-6 loss: 0.527299  [  224/  265]
train() client id: f_00001-7-7 loss: 0.484549  [  256/  265]
train() client id: f_00001-8-0 loss: 0.590292  [   32/  265]
train() client id: f_00001-8-1 loss: 0.504291  [   64/  265]
train() client id: f_00001-8-2 loss: 0.419082  [   96/  265]
train() client id: f_00001-8-3 loss: 0.419021  [  128/  265]
train() client id: f_00001-8-4 loss: 0.564522  [  160/  265]
train() client id: f_00001-8-5 loss: 0.533215  [  192/  265]
train() client id: f_00001-8-6 loss: 0.475234  [  224/  265]
train() client id: f_00001-8-7 loss: 0.585426  [  256/  265]
train() client id: f_00001-9-0 loss: 0.469772  [   32/  265]
train() client id: f_00001-9-1 loss: 0.502230  [   64/  265]
train() client id: f_00001-9-2 loss: 0.513222  [   96/  265]
train() client id: f_00001-9-3 loss: 0.566133  [  128/  265]
train() client id: f_00001-9-4 loss: 0.479035  [  160/  265]
train() client id: f_00001-9-5 loss: 0.598442  [  192/  265]
train() client id: f_00001-9-6 loss: 0.529376  [  224/  265]
train() client id: f_00001-9-7 loss: 0.431102  [  256/  265]
train() client id: f_00001-10-0 loss: 0.659416  [   32/  265]
train() client id: f_00001-10-1 loss: 0.423938  [   64/  265]
train() client id: f_00001-10-2 loss: 0.497665  [   96/  265]
train() client id: f_00001-10-3 loss: 0.426471  [  128/  265]
train() client id: f_00001-10-4 loss: 0.522307  [  160/  265]
train() client id: f_00001-10-5 loss: 0.526741  [  192/  265]
train() client id: f_00001-10-6 loss: 0.533023  [  224/  265]
train() client id: f_00001-10-7 loss: 0.490161  [  256/  265]
train() client id: f_00001-11-0 loss: 0.502448  [   32/  265]
train() client id: f_00001-11-1 loss: 0.511894  [   64/  265]
train() client id: f_00001-11-2 loss: 0.431932  [   96/  265]
train() client id: f_00001-11-3 loss: 0.575197  [  128/  265]
train() client id: f_00001-11-4 loss: 0.549624  [  160/  265]
train() client id: f_00001-11-5 loss: 0.425093  [  192/  265]
train() client id: f_00001-11-6 loss: 0.594021  [  224/  265]
train() client id: f_00001-11-7 loss: 0.491362  [  256/  265]
train() client id: f_00002-0-0 loss: 1.059080  [   32/  124]
train() client id: f_00002-0-1 loss: 1.358889  [   64/  124]
train() client id: f_00002-0-2 loss: 1.116548  [   96/  124]
train() client id: f_00002-1-0 loss: 1.120788  [   32/  124]
train() client id: f_00002-1-1 loss: 1.213300  [   64/  124]
train() client id: f_00002-1-2 loss: 1.204165  [   96/  124]
train() client id: f_00002-2-0 loss: 1.101534  [   32/  124]
train() client id: f_00002-2-1 loss: 1.052954  [   64/  124]
train() client id: f_00002-2-2 loss: 1.200853  [   96/  124]
train() client id: f_00002-3-0 loss: 1.017134  [   32/  124]
train() client id: f_00002-3-1 loss: 1.209725  [   64/  124]
train() client id: f_00002-3-2 loss: 0.968785  [   96/  124]
train() client id: f_00002-4-0 loss: 1.075331  [   32/  124]
train() client id: f_00002-4-1 loss: 1.130635  [   64/  124]
train() client id: f_00002-4-2 loss: 0.988903  [   96/  124]
train() client id: f_00002-5-0 loss: 1.246108  [   32/  124]
train() client id: f_00002-5-1 loss: 0.971143  [   64/  124]
train() client id: f_00002-5-2 loss: 1.204042  [   96/  124]
train() client id: f_00002-6-0 loss: 1.036057  [   32/  124]
train() client id: f_00002-6-1 loss: 1.031534  [   64/  124]
train() client id: f_00002-6-2 loss: 1.026641  [   96/  124]
train() client id: f_00002-7-0 loss: 0.964137  [   32/  124]
train() client id: f_00002-7-1 loss: 1.054278  [   64/  124]
train() client id: f_00002-7-2 loss: 1.194789  [   96/  124]
train() client id: f_00002-8-0 loss: 0.980516  [   32/  124]
train() client id: f_00002-8-1 loss: 1.026761  [   64/  124]
train() client id: f_00002-8-2 loss: 1.230969  [   96/  124]
train() client id: f_00002-9-0 loss: 0.978651  [   32/  124]
train() client id: f_00002-9-1 loss: 0.877875  [   64/  124]
train() client id: f_00002-9-2 loss: 1.018097  [   96/  124]
train() client id: f_00002-10-0 loss: 0.821436  [   32/  124]
train() client id: f_00002-10-1 loss: 1.191214  [   64/  124]
train() client id: f_00002-10-2 loss: 0.983740  [   96/  124]
train() client id: f_00002-11-0 loss: 1.034507  [   32/  124]
train() client id: f_00002-11-1 loss: 0.979839  [   64/  124]
train() client id: f_00002-11-2 loss: 1.114828  [   96/  124]
train() client id: f_00003-0-0 loss: 0.930313  [   32/   43]
train() client id: f_00003-1-0 loss: 0.958414  [   32/   43]
train() client id: f_00003-2-0 loss: 0.926368  [   32/   43]
train() client id: f_00003-3-0 loss: 1.052292  [   32/   43]
train() client id: f_00003-4-0 loss: 1.025925  [   32/   43]
train() client id: f_00003-5-0 loss: 1.008257  [   32/   43]
train() client id: f_00003-6-0 loss: 0.975348  [   32/   43]
train() client id: f_00003-7-0 loss: 1.052167  [   32/   43]
train() client id: f_00003-8-0 loss: 0.990090  [   32/   43]
train() client id: f_00003-9-0 loss: 1.044715  [   32/   43]
train() client id: f_00003-10-0 loss: 0.865671  [   32/   43]
train() client id: f_00003-11-0 loss: 1.055850  [   32/   43]
train() client id: f_00004-0-0 loss: 0.922644  [   32/  306]
train() client id: f_00004-0-1 loss: 0.795669  [   64/  306]
train() client id: f_00004-0-2 loss: 0.960092  [   96/  306]
train() client id: f_00004-0-3 loss: 0.819797  [  128/  306]
train() client id: f_00004-0-4 loss: 0.941041  [  160/  306]
train() client id: f_00004-0-5 loss: 0.879645  [  192/  306]
train() client id: f_00004-0-6 loss: 0.956128  [  224/  306]
train() client id: f_00004-0-7 loss: 0.913631  [  256/  306]
train() client id: f_00004-0-8 loss: 0.926141  [  288/  306]
train() client id: f_00004-1-0 loss: 0.939586  [   32/  306]
train() client id: f_00004-1-1 loss: 0.925831  [   64/  306]
train() client id: f_00004-1-2 loss: 0.895345  [   96/  306]
train() client id: f_00004-1-3 loss: 0.892578  [  128/  306]
train() client id: f_00004-1-4 loss: 0.821540  [  160/  306]
train() client id: f_00004-1-5 loss: 0.865737  [  192/  306]
train() client id: f_00004-1-6 loss: 0.879058  [  224/  306]
train() client id: f_00004-1-7 loss: 0.915884  [  256/  306]
train() client id: f_00004-1-8 loss: 0.901730  [  288/  306]
train() client id: f_00004-2-0 loss: 0.917888  [   32/  306]
train() client id: f_00004-2-1 loss: 0.908098  [   64/  306]
train() client id: f_00004-2-2 loss: 0.819557  [   96/  306]
train() client id: f_00004-2-3 loss: 0.934363  [  128/  306]
train() client id: f_00004-2-4 loss: 0.928260  [  160/  306]
train() client id: f_00004-2-5 loss: 0.911138  [  192/  306]
train() client id: f_00004-2-6 loss: 0.951646  [  224/  306]
train() client id: f_00004-2-7 loss: 0.779252  [  256/  306]
train() client id: f_00004-2-8 loss: 0.872133  [  288/  306]
train() client id: f_00004-3-0 loss: 0.939607  [   32/  306]
train() client id: f_00004-3-1 loss: 0.796395  [   64/  306]
train() client id: f_00004-3-2 loss: 0.828580  [   96/  306]
train() client id: f_00004-3-3 loss: 0.928342  [  128/  306]
train() client id: f_00004-3-4 loss: 0.957749  [  160/  306]
train() client id: f_00004-3-5 loss: 0.865246  [  192/  306]
train() client id: f_00004-3-6 loss: 0.858437  [  224/  306]
train() client id: f_00004-3-7 loss: 0.879494  [  256/  306]
train() client id: f_00004-3-8 loss: 0.928643  [  288/  306]
train() client id: f_00004-4-0 loss: 0.859777  [   32/  306]
train() client id: f_00004-4-1 loss: 0.930067  [   64/  306]
train() client id: f_00004-4-2 loss: 0.945602  [   96/  306]
train() client id: f_00004-4-3 loss: 0.926968  [  128/  306]
train() client id: f_00004-4-4 loss: 0.919089  [  160/  306]
train() client id: f_00004-4-5 loss: 0.780962  [  192/  306]
train() client id: f_00004-4-6 loss: 0.869988  [  224/  306]
train() client id: f_00004-4-7 loss: 0.955496  [  256/  306]
train() client id: f_00004-4-8 loss: 0.846414  [  288/  306]
train() client id: f_00004-5-0 loss: 0.847324  [   32/  306]
train() client id: f_00004-5-1 loss: 0.862190  [   64/  306]
train() client id: f_00004-5-2 loss: 0.917935  [   96/  306]
train() client id: f_00004-5-3 loss: 0.849664  [  128/  306]
train() client id: f_00004-5-4 loss: 0.901861  [  160/  306]
train() client id: f_00004-5-5 loss: 0.862004  [  192/  306]
train() client id: f_00004-5-6 loss: 0.951386  [  224/  306]
train() client id: f_00004-5-7 loss: 0.937581  [  256/  306]
train() client id: f_00004-5-8 loss: 0.874858  [  288/  306]
train() client id: f_00004-6-0 loss: 0.892696  [   32/  306]
train() client id: f_00004-6-1 loss: 0.944752  [   64/  306]
train() client id: f_00004-6-2 loss: 0.922619  [   96/  306]
train() client id: f_00004-6-3 loss: 0.824707  [  128/  306]
train() client id: f_00004-6-4 loss: 0.858232  [  160/  306]
train() client id: f_00004-6-5 loss: 0.971746  [  192/  306]
train() client id: f_00004-6-6 loss: 0.838386  [  224/  306]
train() client id: f_00004-6-7 loss: 0.932506  [  256/  306]
train() client id: f_00004-6-8 loss: 0.795471  [  288/  306]
train() client id: f_00004-7-0 loss: 0.901382  [   32/  306]
train() client id: f_00004-7-1 loss: 0.932337  [   64/  306]
train() client id: f_00004-7-2 loss: 0.874570  [   96/  306]
train() client id: f_00004-7-3 loss: 0.806694  [  128/  306]
train() client id: f_00004-7-4 loss: 0.865873  [  160/  306]
train() client id: f_00004-7-5 loss: 0.775898  [  192/  306]
train() client id: f_00004-7-6 loss: 0.983258  [  224/  306]
train() client id: f_00004-7-7 loss: 0.950269  [  256/  306]
train() client id: f_00004-7-8 loss: 0.868751  [  288/  306]
train() client id: f_00004-8-0 loss: 0.794087  [   32/  306]
train() client id: f_00004-8-1 loss: 0.921524  [   64/  306]
train() client id: f_00004-8-2 loss: 0.805165  [   96/  306]
train() client id: f_00004-8-3 loss: 0.940451  [  128/  306]
train() client id: f_00004-8-4 loss: 0.943975  [  160/  306]
train() client id: f_00004-8-5 loss: 0.855603  [  192/  306]
train() client id: f_00004-8-6 loss: 0.722794  [  224/  306]
train() client id: f_00004-8-7 loss: 0.909465  [  256/  306]
train() client id: f_00004-8-8 loss: 0.920395  [  288/  306]
train() client id: f_00004-9-0 loss: 0.881011  [   32/  306]
train() client id: f_00004-9-1 loss: 0.859779  [   64/  306]
train() client id: f_00004-9-2 loss: 1.028485  [   96/  306]
train() client id: f_00004-9-3 loss: 0.827680  [  128/  306]
train() client id: f_00004-9-4 loss: 0.930241  [  160/  306]
train() client id: f_00004-9-5 loss: 0.968203  [  192/  306]
train() client id: f_00004-9-6 loss: 0.786007  [  224/  306]
train() client id: f_00004-9-7 loss: 0.885262  [  256/  306]
train() client id: f_00004-9-8 loss: 0.765758  [  288/  306]
train() client id: f_00004-10-0 loss: 0.826635  [   32/  306]
train() client id: f_00004-10-1 loss: 0.846266  [   64/  306]
train() client id: f_00004-10-2 loss: 0.900996  [   96/  306]
train() client id: f_00004-10-3 loss: 0.809826  [  128/  306]
train() client id: f_00004-10-4 loss: 0.967111  [  160/  306]
train() client id: f_00004-10-5 loss: 0.879256  [  192/  306]
train() client id: f_00004-10-6 loss: 0.845565  [  224/  306]
train() client id: f_00004-10-7 loss: 0.899572  [  256/  306]
train() client id: f_00004-10-8 loss: 0.906014  [  288/  306]
train() client id: f_00004-11-0 loss: 0.906362  [   32/  306]
train() client id: f_00004-11-1 loss: 0.923463  [   64/  306]
train() client id: f_00004-11-2 loss: 0.949786  [   96/  306]
train() client id: f_00004-11-3 loss: 0.978041  [  128/  306]
train() client id: f_00004-11-4 loss: 0.873029  [  160/  306]
train() client id: f_00004-11-5 loss: 0.802123  [  192/  306]
train() client id: f_00004-11-6 loss: 0.787392  [  224/  306]
train() client id: f_00004-11-7 loss: 0.897457  [  256/  306]
train() client id: f_00004-11-8 loss: 0.864656  [  288/  306]
train() client id: f_00005-0-0 loss: 0.782176  [   32/  146]
train() client id: f_00005-0-1 loss: 0.938095  [   64/  146]
train() client id: f_00005-0-2 loss: 0.764807  [   96/  146]
train() client id: f_00005-0-3 loss: 0.659954  [  128/  146]
train() client id: f_00005-1-0 loss: 0.854834  [   32/  146]
train() client id: f_00005-1-1 loss: 0.691841  [   64/  146]
train() client id: f_00005-1-2 loss: 0.736804  [   96/  146]
train() client id: f_00005-1-3 loss: 0.703175  [  128/  146]
train() client id: f_00005-2-0 loss: 0.900355  [   32/  146]
train() client id: f_00005-2-1 loss: 0.829462  [   64/  146]
train() client id: f_00005-2-2 loss: 0.572749  [   96/  146]
train() client id: f_00005-2-3 loss: 0.933782  [  128/  146]
train() client id: f_00005-3-0 loss: 0.723663  [   32/  146]
train() client id: f_00005-3-1 loss: 0.794247  [   64/  146]
train() client id: f_00005-3-2 loss: 0.826411  [   96/  146]
train() client id: f_00005-3-3 loss: 0.863005  [  128/  146]
train() client id: f_00005-4-0 loss: 0.633077  [   32/  146]
train() client id: f_00005-4-1 loss: 0.798491  [   64/  146]
train() client id: f_00005-4-2 loss: 0.759423  [   96/  146]
train() client id: f_00005-4-3 loss: 0.877149  [  128/  146]
train() client id: f_00005-5-0 loss: 0.702614  [   32/  146]
train() client id: f_00005-5-1 loss: 0.849091  [   64/  146]
train() client id: f_00005-5-2 loss: 0.909207  [   96/  146]
train() client id: f_00005-5-3 loss: 0.658075  [  128/  146]
train() client id: f_00005-6-0 loss: 0.589300  [   32/  146]
train() client id: f_00005-6-1 loss: 0.961953  [   64/  146]
train() client id: f_00005-6-2 loss: 0.894750  [   96/  146]
train() client id: f_00005-6-3 loss: 0.756525  [  128/  146]
train() client id: f_00005-7-0 loss: 0.584547  [   32/  146]
train() client id: f_00005-7-1 loss: 0.928455  [   64/  146]
train() client id: f_00005-7-2 loss: 0.853900  [   96/  146]
train() client id: f_00005-7-3 loss: 0.962577  [  128/  146]
train() client id: f_00005-8-0 loss: 0.799215  [   32/  146]
train() client id: f_00005-8-1 loss: 0.659341  [   64/  146]
train() client id: f_00005-8-2 loss: 1.002992  [   96/  146]
train() client id: f_00005-8-3 loss: 0.808157  [  128/  146]
train() client id: f_00005-9-0 loss: 0.628490  [   32/  146]
train() client id: f_00005-9-1 loss: 0.913056  [   64/  146]
train() client id: f_00005-9-2 loss: 0.744432  [   96/  146]
train() client id: f_00005-9-3 loss: 0.831495  [  128/  146]
train() client id: f_00005-10-0 loss: 0.873175  [   32/  146]
train() client id: f_00005-10-1 loss: 0.878566  [   64/  146]
train() client id: f_00005-10-2 loss: 0.623257  [   96/  146]
train() client id: f_00005-10-3 loss: 0.868353  [  128/  146]
train() client id: f_00005-11-0 loss: 0.645808  [   32/  146]
train() client id: f_00005-11-1 loss: 0.912686  [   64/  146]
train() client id: f_00005-11-2 loss: 0.556615  [   96/  146]
train() client id: f_00005-11-3 loss: 0.947359  [  128/  146]
train() client id: f_00006-0-0 loss: 0.624123  [   32/   54]
train() client id: f_00006-1-0 loss: 0.682061  [   32/   54]
train() client id: f_00006-2-0 loss: 0.666916  [   32/   54]
train() client id: f_00006-3-0 loss: 0.652746  [   32/   54]
train() client id: f_00006-4-0 loss: 0.602248  [   32/   54]
train() client id: f_00006-5-0 loss: 0.633678  [   32/   54]
train() client id: f_00006-6-0 loss: 0.597066  [   32/   54]
train() client id: f_00006-7-0 loss: 0.651156  [   32/   54]
train() client id: f_00006-8-0 loss: 0.603379  [   32/   54]
train() client id: f_00006-9-0 loss: 0.659708  [   32/   54]
train() client id: f_00006-10-0 loss: 0.599498  [   32/   54]
train() client id: f_00006-11-0 loss: 0.629341  [   32/   54]
train() client id: f_00007-0-0 loss: 0.540994  [   32/  179]
train() client id: f_00007-0-1 loss: 0.514042  [   64/  179]
train() client id: f_00007-0-2 loss: 0.406168  [   96/  179]
train() client id: f_00007-0-3 loss: 0.718037  [  128/  179]
train() client id: f_00007-0-4 loss: 0.390581  [  160/  179]
train() client id: f_00007-1-0 loss: 0.484856  [   32/  179]
train() client id: f_00007-1-1 loss: 0.588143  [   64/  179]
train() client id: f_00007-1-2 loss: 0.465791  [   96/  179]
train() client id: f_00007-1-3 loss: 0.563029  [  128/  179]
train() client id: f_00007-1-4 loss: 0.435105  [  160/  179]
train() client id: f_00007-2-0 loss: 0.473228  [   32/  179]
train() client id: f_00007-2-1 loss: 0.671863  [   64/  179]
train() client id: f_00007-2-2 loss: 0.458659  [   96/  179]
train() client id: f_00007-2-3 loss: 0.386670  [  128/  179]
train() client id: f_00007-2-4 loss: 0.454312  [  160/  179]
train() client id: f_00007-3-0 loss: 0.510417  [   32/  179]
train() client id: f_00007-3-1 loss: 0.455579  [   64/  179]
train() client id: f_00007-3-2 loss: 0.539616  [   96/  179]
train() client id: f_00007-3-3 loss: 0.354012  [  128/  179]
train() client id: f_00007-3-4 loss: 0.504723  [  160/  179]
train() client id: f_00007-4-0 loss: 0.444178  [   32/  179]
train() client id: f_00007-4-1 loss: 0.403651  [   64/  179]
train() client id: f_00007-4-2 loss: 0.414649  [   96/  179]
train() client id: f_00007-4-3 loss: 0.395988  [  128/  179]
train() client id: f_00007-4-4 loss: 0.523016  [  160/  179]
train() client id: f_00007-5-0 loss: 0.383497  [   32/  179]
train() client id: f_00007-5-1 loss: 0.548887  [   64/  179]
train() client id: f_00007-5-2 loss: 0.450267  [   96/  179]
train() client id: f_00007-5-3 loss: 0.561569  [  128/  179]
train() client id: f_00007-5-4 loss: 0.313975  [  160/  179]
train() client id: f_00007-6-0 loss: 0.365514  [   32/  179]
train() client id: f_00007-6-1 loss: 0.282937  [   64/  179]
train() client id: f_00007-6-2 loss: 0.463201  [   96/  179]
train() client id: f_00007-6-3 loss: 0.313153  [  128/  179]
train() client id: f_00007-6-4 loss: 0.804545  [  160/  179]
train() client id: f_00007-7-0 loss: 0.326540  [   32/  179]
train() client id: f_00007-7-1 loss: 0.526104  [   64/  179]
train() client id: f_00007-7-2 loss: 0.263080  [   96/  179]
train() client id: f_00007-7-3 loss: 0.640367  [  128/  179]
train() client id: f_00007-7-4 loss: 0.312725  [  160/  179]
train() client id: f_00007-8-0 loss: 0.365629  [   32/  179]
train() client id: f_00007-8-1 loss: 0.471782  [   64/  179]
train() client id: f_00007-8-2 loss: 0.404598  [   96/  179]
train() client id: f_00007-8-3 loss: 0.358406  [  128/  179]
train() client id: f_00007-8-4 loss: 0.499915  [  160/  179]
train() client id: f_00007-9-0 loss: 0.692817  [   32/  179]
train() client id: f_00007-9-1 loss: 0.435756  [   64/  179]
train() client id: f_00007-9-2 loss: 0.297085  [   96/  179]
train() client id: f_00007-9-3 loss: 0.285635  [  128/  179]
train() client id: f_00007-9-4 loss: 0.329735  [  160/  179]
train() client id: f_00007-10-0 loss: 0.305418  [   32/  179]
train() client id: f_00007-10-1 loss: 0.539500  [   64/  179]
train() client id: f_00007-10-2 loss: 0.416709  [   96/  179]
train() client id: f_00007-10-3 loss: 0.352549  [  128/  179]
train() client id: f_00007-10-4 loss: 0.458408  [  160/  179]
train() client id: f_00007-11-0 loss: 0.377510  [   32/  179]
train() client id: f_00007-11-1 loss: 0.416535  [   64/  179]
train() client id: f_00007-11-2 loss: 0.360222  [   96/  179]
train() client id: f_00007-11-3 loss: 0.270104  [  128/  179]
train() client id: f_00007-11-4 loss: 0.465715  [  160/  179]
train() client id: f_00008-0-0 loss: 0.990141  [   32/  130]
train() client id: f_00008-0-1 loss: 0.917060  [   64/  130]
train() client id: f_00008-0-2 loss: 0.970505  [   96/  130]
train() client id: f_00008-0-3 loss: 0.850070  [  128/  130]
train() client id: f_00008-1-0 loss: 0.982760  [   32/  130]
train() client id: f_00008-1-1 loss: 0.902867  [   64/  130]
train() client id: f_00008-1-2 loss: 0.979245  [   96/  130]
train() client id: f_00008-1-3 loss: 0.884690  [  128/  130]
train() client id: f_00008-2-0 loss: 0.958759  [   32/  130]
train() client id: f_00008-2-1 loss: 0.986239  [   64/  130]
train() client id: f_00008-2-2 loss: 0.904900  [   96/  130]
train() client id: f_00008-2-3 loss: 0.906484  [  128/  130]
train() client id: f_00008-3-0 loss: 0.951521  [   32/  130]
train() client id: f_00008-3-1 loss: 1.021209  [   64/  130]
train() client id: f_00008-3-2 loss: 0.946297  [   96/  130]
train() client id: f_00008-3-3 loss: 0.856742  [  128/  130]
train() client id: f_00008-4-0 loss: 0.992254  [   32/  130]
train() client id: f_00008-4-1 loss: 0.887586  [   64/  130]
train() client id: f_00008-4-2 loss: 0.923479  [   96/  130]
train() client id: f_00008-4-3 loss: 0.909491  [  128/  130]
train() client id: f_00008-5-0 loss: 0.913722  [   32/  130]
train() client id: f_00008-5-1 loss: 1.018126  [   64/  130]
train() client id: f_00008-5-2 loss: 0.913791  [   96/  130]
train() client id: f_00008-5-3 loss: 0.938295  [  128/  130]
train() client id: f_00008-6-0 loss: 0.970958  [   32/  130]
train() client id: f_00008-6-1 loss: 0.894926  [   64/  130]
train() client id: f_00008-6-2 loss: 0.970882  [   96/  130]
train() client id: f_00008-6-3 loss: 0.892074  [  128/  130]
train() client id: f_00008-7-0 loss: 0.925113  [   32/  130]
train() client id: f_00008-7-1 loss: 1.008657  [   64/  130]
train() client id: f_00008-7-2 loss: 0.980815  [   96/  130]
train() client id: f_00008-7-3 loss: 0.877018  [  128/  130]
train() client id: f_00008-8-0 loss: 1.010230  [   32/  130]
train() client id: f_00008-8-1 loss: 0.949716  [   64/  130]
train() client id: f_00008-8-2 loss: 0.905677  [   96/  130]
train() client id: f_00008-8-3 loss: 0.946069  [  128/  130]
train() client id: f_00008-9-0 loss: 0.957749  [   32/  130]
train() client id: f_00008-9-1 loss: 0.914998  [   64/  130]
train() client id: f_00008-9-2 loss: 0.879742  [   96/  130]
train() client id: f_00008-9-3 loss: 1.032898  [  128/  130]
train() client id: f_00008-10-0 loss: 0.884371  [   32/  130]
train() client id: f_00008-10-1 loss: 0.978717  [   64/  130]
train() client id: f_00008-10-2 loss: 0.991319  [   96/  130]
train() client id: f_00008-10-3 loss: 0.961737  [  128/  130]
train() client id: f_00008-11-0 loss: 0.882218  [   32/  130]
train() client id: f_00008-11-1 loss: 0.953427  [   64/  130]
train() client id: f_00008-11-2 loss: 0.987535  [   96/  130]
train() client id: f_00008-11-3 loss: 0.971908  [  128/  130]
train() client id: f_00009-0-0 loss: 0.954565  [   32/  118]
train() client id: f_00009-0-1 loss: 1.219120  [   64/  118]
train() client id: f_00009-0-2 loss: 1.007747  [   96/  118]
train() client id: f_00009-1-0 loss: 0.983917  [   32/  118]
train() client id: f_00009-1-1 loss: 0.919329  [   64/  118]
train() client id: f_00009-1-2 loss: 1.015377  [   96/  118]
train() client id: f_00009-2-0 loss: 1.033389  [   32/  118]
train() client id: f_00009-2-1 loss: 0.939764  [   64/  118]
train() client id: f_00009-2-2 loss: 0.920963  [   96/  118]
train() client id: f_00009-3-0 loss: 0.904771  [   32/  118]
train() client id: f_00009-3-1 loss: 0.928202  [   64/  118]
train() client id: f_00009-3-2 loss: 0.869363  [   96/  118]
train() client id: f_00009-4-0 loss: 0.874560  [   32/  118]
train() client id: f_00009-4-1 loss: 0.958841  [   64/  118]
train() client id: f_00009-4-2 loss: 0.890840  [   96/  118]
train() client id: f_00009-5-0 loss: 0.880166  [   32/  118]
train() client id: f_00009-5-1 loss: 0.946519  [   64/  118]
train() client id: f_00009-5-2 loss: 0.850907  [   96/  118]
train() client id: f_00009-6-0 loss: 0.906854  [   32/  118]
train() client id: f_00009-6-1 loss: 0.938774  [   64/  118]
train() client id: f_00009-6-2 loss: 0.721291  [   96/  118]
train() client id: f_00009-7-0 loss: 0.896408  [   32/  118]
train() client id: f_00009-7-1 loss: 0.781187  [   64/  118]
train() client id: f_00009-7-2 loss: 0.964250  [   96/  118]
train() client id: f_00009-8-0 loss: 0.768976  [   32/  118]
train() client id: f_00009-8-1 loss: 0.824955  [   64/  118]
train() client id: f_00009-8-2 loss: 0.854886  [   96/  118]
train() client id: f_00009-9-0 loss: 0.782091  [   32/  118]
train() client id: f_00009-9-1 loss: 0.762941  [   64/  118]
train() client id: f_00009-9-2 loss: 0.916973  [   96/  118]
train() client id: f_00009-10-0 loss: 0.758413  [   32/  118]
train() client id: f_00009-10-1 loss: 0.840937  [   64/  118]
train() client id: f_00009-10-2 loss: 0.770082  [   96/  118]
train() client id: f_00009-11-0 loss: 0.959281  [   32/  118]
train() client id: f_00009-11-1 loss: 0.875470  [   64/  118]
train() client id: f_00009-11-2 loss: 0.690796  [   96/  118]
At round 7 accuracy: 0.6339522546419099
At round 7 training accuracy: 0.5855130784708249
At round 7 training loss: 0.84269478676398
update_location
xs = -3.905658 4.200318 55.009024 18.811294 0.979296 3.956410 -17.443192 -1.324852 39.663977 -2.060879 
ys = 47.587959 30.555839 1.320614 -17.455176 9.350187 -7.185849 -2.624984 -4.177652 17.569006 4.001482 
xs mean: 9.788573831882573
ys mean: 7.894142535528705
dists_uav = 110.814566 104.648468 114.139111 103.240244 100.440953 100.335884 101.543860 100.095994 109.004133 100.101244 
uav_gains = -101.114990 -100.493352 -101.435968 -100.346251 -100.047788 -100.036424 -100.166362 -100.010434 -100.936129 -100.011003 
uav_gains_db_mean: -100.45986996231362
dists_bs = 213.323936 230.167047 288.216377 273.133324 241.678657 255.376350 237.433744 249.534944 266.205073 243.204509 
bs_gains = -104.780168 -105.704267 -108.439181 -107.785550 -106.297732 -106.968118 -106.082247 -106.686738 -107.473116 -106.374265 
bs_gains_db_mean: -106.65913823028175
Round 8
-------------------------------
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.93572436 20.7513301   9.78311252  3.49713647 23.93609638 11.54337877
  4.34848004 14.03581921 10.31003617  9.36568175]
obj_prev = 117.50679575560986
eta_min = 4.500137844098843e-10	eta_max = 0.9201793140385103
af = 24.843282687249765	bf = 1.924183044657018	zeta = 27.327610955974745	eta = 0.909090909090909
af = 24.843282687249765	bf = 1.924183044657018	zeta = 47.21200862959305	eta = 0.5262068572883742
af = 24.843282687249765	bf = 1.924183044657018	zeta = 37.72761010314322	eta = 0.6584907609925703
af = 24.843282687249765	bf = 1.924183044657018	zeta = 36.02779691629381	eta = 0.689558752231509
af = 24.843282687249765	bf = 1.924183044657018	zeta = 35.944047891502834	eta = 0.6911654124832922
af = 24.843282687249765	bf = 1.924183044657018	zeta = 35.94383021535565	eta = 0.6911695981870181
eta = 0.6911695981870181
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [0.03047953 0.06410378 0.02999573 0.01040174 0.07402172 0.03531755
 0.01306266 0.04330029 0.03144714 0.02854432]
ene_total = [3.05537056 5.9604125  3.0232152  1.38370094 6.79982988 3.63829942
 1.59895401 4.0866222  3.34115314 3.05627237]
ti_comp = [0.29321835 0.27683249 0.29228351 0.29535599 0.2741193  0.27083978
 0.29583662 0.2962475  0.2682047  0.2737568 ]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.05836579e-05 2.14830890e-04 1.97446596e-05 8.06319238e-07
 3.37348113e-04 3.75341750e-05 1.59173447e-06 5.78153199e-05
 2.70203220e-05 1.93958759e-05]
ene_total = [0.54608481 0.69707832 0.55371718 0.52684511 0.72952358 0.73184161
 0.52295022 0.52419719 0.75268396 0.70631616]
optimize_network iter = 0 obj = 6.291238143797367
eta = 0.6911695981870181
freqs = [5.19741187e+07 1.15780803e+08 5.13127289e+07 1.76088264e+07
 1.35017343e+08 6.52000814e+07 2.20774830e+07 7.30812690e+07
 5.86252507e+07 5.21344532e+07]
eta_min = 0.6875884070281042	eta_max = 0.6911695981870177
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 0.05581615321591653	eta = 0.909090909090909
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 21.219211967217113	eta = 0.002391321484860486
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.2358034144344674	eta = 0.022695178449689362
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.1654164527348314	eta = 0.02343288627225956
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.1653901507160183	eta = 0.02343317090097433
eta = 0.02343317090097433
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.09174714e-04 2.18314889e-03 2.00648668e-04 8.19395643e-06
 3.42819024e-03 3.81428819e-04 1.61754827e-05 5.87529343e-04
 2.74585215e-04 1.97104266e-04]
ene_total = [0.17760768 0.27172476 0.17982108 0.16680246 0.3112301  0.24039767
 0.16575821 0.17957311 0.24447922 0.22799587]
ti_comp = [0.29738478 0.28099891 0.29644993 0.29952241 0.27828572 0.27500621
 0.30000305 0.30041392 0.27237112 0.27792323]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.08671664e-05 2.17429093e-04 2.00148185e-05 8.17590855e-07
 3.41327869e-04 3.79632066e-05 1.61405844e-06 5.86284344e-05
 2.73210417e-05 1.96239144e-05]
ene_total = [0.5398481  0.68929927 0.54739189 0.52080677 0.72148512 0.72348741
 0.51695743 0.51825451 0.74408039 0.69823819]
optimize_network iter = 1 obj = 6.219849079279121
eta = 0.6875884070281042
freqs = [5.19666731e+07 1.15668296e+08 5.13030792e+07 1.76080893e+07
 1.34866351e+08 6.51153626e+07 2.20770578e+07 7.30812690e+07
 5.85403612e+07 5.20751181e+07]
eta_min = 0.6875884070281044	eta_max = 0.6875884070280862
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 0.055709725023689025	eta = 0.909090909090909
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 21.21911053031574	eta = 0.0023867732106222467
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.23530448131582	eta = 0.022656960154787298
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.1650443530624273	eta = 0.023392224965439173
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.1650181860209075	eta = 0.023392507690695646
eta = 0.023392507690695646
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.09211545e-04 2.17991631e-03 2.00666013e-04 8.19706147e-06
 3.42210961e-03 3.80614259e-04 1.61823437e-05 5.87801193e-04
 2.73917274e-04 1.96746859e-04]
ene_total = [0.17760125 0.27162923 0.17981404 0.16679559 0.31105873 0.24036644
 0.16575149 0.17957271 0.24445164 0.22797706]
ti_comp = [0.29738478 0.28099891 0.29644993 0.29952241 0.27828572 0.27500621
 0.30000305 0.30041392 0.27237112 0.27792323]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.08671664e-05 2.17429093e-04 2.00148185e-05 8.17590855e-07
 3.41327869e-04 3.79632066e-05 1.61405844e-06 5.86284344e-05
 2.73210417e-05 1.96239144e-05]
ene_total = [0.5398481  0.68929927 0.54739189 0.52080677 0.72148512 0.72348741
 0.51695743 0.51825451 0.74408039 0.69823819]
optimize_network iter = 2 obj = 6.219849079279126
eta = 0.6875884070281044
freqs = [5.19666731e+07 1.15668296e+08 5.13030792e+07 1.76080893e+07
 1.34866351e+08 6.51153626e+07 2.20770578e+07 7.30812690e+07
 5.85403612e+07 5.20751181e+07]
Done!
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.04160455e-05 2.12728559e-04 1.95821241e-05 7.99915605e-07
 3.33948805e-04 3.71424915e-05 1.57916459e-06 5.73609639e-05
 2.67303963e-05 1.91996709e-05]
ene_total = [0.00662848 0.00845938 0.00672113 0.0063951  0.00885192 0.00888306
 0.00634781 0.00636251 0.00913616 0.00857342]
At round 8 energy consumption: 0.07635896655888749
At round 8 eta: 0.6875884070281044
At round 8 a_n: 25.442236080499836
At round 8 local rounds: 12.265156719696218
At round 8 global rounds: 81.4381945256065
gradient difference: 0.35732847452163696
train() client id: f_00000-0-0 loss: 1.269342  [   32/  126]
train() client id: f_00000-0-1 loss: 1.069260  [   64/  126]
train() client id: f_00000-0-2 loss: 1.047896  [   96/  126]
train() client id: f_00000-1-0 loss: 1.070501  [   32/  126]
train() client id: f_00000-1-1 loss: 1.069874  [   64/  126]
train() client id: f_00000-1-2 loss: 1.039023  [   96/  126]
train() client id: f_00000-2-0 loss: 0.999373  [   32/  126]
train() client id: f_00000-2-1 loss: 0.983201  [   64/  126]
train() client id: f_00000-2-2 loss: 1.161078  [   96/  126]
train() client id: f_00000-3-0 loss: 1.020720  [   32/  126]
train() client id: f_00000-3-1 loss: 1.044896  [   64/  126]
train() client id: f_00000-3-2 loss: 0.831973  [   96/  126]
train() client id: f_00000-4-0 loss: 1.011107  [   32/  126]
train() client id: f_00000-4-1 loss: 0.875087  [   64/  126]
train() client id: f_00000-4-2 loss: 0.901771  [   96/  126]
train() client id: f_00000-5-0 loss: 0.963165  [   32/  126]
train() client id: f_00000-5-1 loss: 0.896051  [   64/  126]
train() client id: f_00000-5-2 loss: 0.999661  [   96/  126]
train() client id: f_00000-6-0 loss: 0.717381  [   32/  126]
train() client id: f_00000-6-1 loss: 0.906211  [   64/  126]
train() client id: f_00000-6-2 loss: 0.933358  [   96/  126]
train() client id: f_00000-7-0 loss: 0.782005  [   32/  126]
train() client id: f_00000-7-1 loss: 0.774967  [   64/  126]
train() client id: f_00000-7-2 loss: 0.920924  [   96/  126]
train() client id: f_00000-8-0 loss: 0.901038  [   32/  126]
train() client id: f_00000-8-1 loss: 0.789852  [   64/  126]
train() client id: f_00000-8-2 loss: 0.794499  [   96/  126]
train() client id: f_00000-9-0 loss: 0.985719  [   32/  126]
train() client id: f_00000-9-1 loss: 0.963792  [   64/  126]
train() client id: f_00000-9-2 loss: 0.751025  [   96/  126]
train() client id: f_00000-10-0 loss: 0.772400  [   32/  126]
train() client id: f_00000-10-1 loss: 0.920635  [   64/  126]
train() client id: f_00000-10-2 loss: 0.804587  [   96/  126]
train() client id: f_00000-11-0 loss: 0.877942  [   32/  126]
train() client id: f_00000-11-1 loss: 0.828213  [   64/  126]
train() client id: f_00000-11-2 loss: 0.891504  [   96/  126]
train() client id: f_00001-0-0 loss: 0.604390  [   32/  265]
train() client id: f_00001-0-1 loss: 0.485655  [   64/  265]
train() client id: f_00001-0-2 loss: 0.626318  [   96/  265]
train() client id: f_00001-0-3 loss: 0.483534  [  128/  265]
train() client id: f_00001-0-4 loss: 0.526202  [  160/  265]
train() client id: f_00001-0-5 loss: 0.515746  [  192/  265]
train() client id: f_00001-0-6 loss: 0.533192  [  224/  265]
train() client id: f_00001-0-7 loss: 0.458634  [  256/  265]
train() client id: f_00001-1-0 loss: 0.440946  [   32/  265]
train() client id: f_00001-1-1 loss: 0.484316  [   64/  265]
train() client id: f_00001-1-2 loss: 0.411985  [   96/  265]
train() client id: f_00001-1-3 loss: 0.569842  [  128/  265]
train() client id: f_00001-1-4 loss: 0.680090  [  160/  265]
train() client id: f_00001-1-5 loss: 0.619476  [  192/  265]
train() client id: f_00001-1-6 loss: 0.468398  [  224/  265]
train() client id: f_00001-1-7 loss: 0.474982  [  256/  265]
train() client id: f_00001-2-0 loss: 0.570480  [   32/  265]
train() client id: f_00001-2-1 loss: 0.528908  [   64/  265]
train() client id: f_00001-2-2 loss: 0.588090  [   96/  265]
train() client id: f_00001-2-3 loss: 0.540873  [  128/  265]
train() client id: f_00001-2-4 loss: 0.452583  [  160/  265]
train() client id: f_00001-2-5 loss: 0.470979  [  192/  265]
train() client id: f_00001-2-6 loss: 0.426598  [  224/  265]
train() client id: f_00001-2-7 loss: 0.445642  [  256/  265]
train() client id: f_00001-3-0 loss: 0.490473  [   32/  265]
train() client id: f_00001-3-1 loss: 0.510701  [   64/  265]
train() client id: f_00001-3-2 loss: 0.459664  [   96/  265]
train() client id: f_00001-3-3 loss: 0.421779  [  128/  265]
train() client id: f_00001-3-4 loss: 0.620440  [  160/  265]
train() client id: f_00001-3-5 loss: 0.519104  [  192/  265]
train() client id: f_00001-3-6 loss: 0.464735  [  224/  265]
train() client id: f_00001-3-7 loss: 0.491753  [  256/  265]
train() client id: f_00001-4-0 loss: 0.447445  [   32/  265]
train() client id: f_00001-4-1 loss: 0.515377  [   64/  265]
train() client id: f_00001-4-2 loss: 0.488485  [   96/  265]
train() client id: f_00001-4-3 loss: 0.522485  [  128/  265]
train() client id: f_00001-4-4 loss: 0.491918  [  160/  265]
train() client id: f_00001-4-5 loss: 0.421464  [  192/  265]
train() client id: f_00001-4-6 loss: 0.500770  [  224/  265]
train() client id: f_00001-4-7 loss: 0.554341  [  256/  265]
train() client id: f_00001-5-0 loss: 0.427825  [   32/  265]
train() client id: f_00001-5-1 loss: 0.505489  [   64/  265]
train() client id: f_00001-5-2 loss: 0.512498  [   96/  265]
train() client id: f_00001-5-3 loss: 0.594716  [  128/  265]
train() client id: f_00001-5-4 loss: 0.540655  [  160/  265]
train() client id: f_00001-5-5 loss: 0.426251  [  192/  265]
train() client id: f_00001-5-6 loss: 0.390049  [  224/  265]
train() client id: f_00001-5-7 loss: 0.520475  [  256/  265]
train() client id: f_00001-6-0 loss: 0.577323  [   32/  265]
train() client id: f_00001-6-1 loss: 0.476317  [   64/  265]
train() client id: f_00001-6-2 loss: 0.523430  [   96/  265]
train() client id: f_00001-6-3 loss: 0.403347  [  128/  265]
train() client id: f_00001-6-4 loss: 0.495998  [  160/  265]
train() client id: f_00001-6-5 loss: 0.450049  [  192/  265]
train() client id: f_00001-6-6 loss: 0.412780  [  224/  265]
train() client id: f_00001-6-7 loss: 0.542446  [  256/  265]
train() client id: f_00001-7-0 loss: 0.420375  [   32/  265]
train() client id: f_00001-7-1 loss: 0.538226  [   64/  265]
train() client id: f_00001-7-2 loss: 0.585306  [   96/  265]
train() client id: f_00001-7-3 loss: 0.515478  [  128/  265]
train() client id: f_00001-7-4 loss: 0.474658  [  160/  265]
train() client id: f_00001-7-5 loss: 0.410510  [  192/  265]
train() client id: f_00001-7-6 loss: 0.521316  [  224/  265]
train() client id: f_00001-7-7 loss: 0.411672  [  256/  265]
train() client id: f_00001-8-0 loss: 0.631714  [   32/  265]
train() client id: f_00001-8-1 loss: 0.480097  [   64/  265]
train() client id: f_00001-8-2 loss: 0.524971  [   96/  265]
train() client id: f_00001-8-3 loss: 0.397637  [  128/  265]
train() client id: f_00001-8-4 loss: 0.460824  [  160/  265]
train() client id: f_00001-8-5 loss: 0.430867  [  192/  265]
train() client id: f_00001-8-6 loss: 0.417108  [  224/  265]
train() client id: f_00001-8-7 loss: 0.519546  [  256/  265]
train() client id: f_00001-9-0 loss: 0.470641  [   32/  265]
train() client id: f_00001-9-1 loss: 0.403632  [   64/  265]
train() client id: f_00001-9-2 loss: 0.408630  [   96/  265]
train() client id: f_00001-9-3 loss: 0.588981  [  128/  265]
train() client id: f_00001-9-4 loss: 0.537199  [  160/  265]
train() client id: f_00001-9-5 loss: 0.521910  [  192/  265]
train() client id: f_00001-9-6 loss: 0.405217  [  224/  265]
train() client id: f_00001-9-7 loss: 0.503474  [  256/  265]
train() client id: f_00001-10-0 loss: 0.481677  [   32/  265]
train() client id: f_00001-10-1 loss: 0.494786  [   64/  265]
train() client id: f_00001-10-2 loss: 0.492159  [   96/  265]
train() client id: f_00001-10-3 loss: 0.442253  [  128/  265]
train() client id: f_00001-10-4 loss: 0.536714  [  160/  265]
train() client id: f_00001-10-5 loss: 0.381061  [  192/  265]
train() client id: f_00001-10-6 loss: 0.485441  [  224/  265]
train() client id: f_00001-10-7 loss: 0.457316  [  256/  265]
train() client id: f_00001-11-0 loss: 0.469362  [   32/  265]
train() client id: f_00001-11-1 loss: 0.472088  [   64/  265]
train() client id: f_00001-11-2 loss: 0.608217  [   96/  265]
train() client id: f_00001-11-3 loss: 0.571227  [  128/  265]
train() client id: f_00001-11-4 loss: 0.431158  [  160/  265]
train() client id: f_00001-11-5 loss: 0.398824  [  192/  265]
train() client id: f_00001-11-6 loss: 0.456065  [  224/  265]
train() client id: f_00001-11-7 loss: 0.411851  [  256/  265]
train() client id: f_00002-0-0 loss: 1.203335  [   32/  124]
train() client id: f_00002-0-1 loss: 1.069465  [   64/  124]
train() client id: f_00002-0-2 loss: 1.194685  [   96/  124]
train() client id: f_00002-1-0 loss: 1.192710  [   32/  124]
train() client id: f_00002-1-1 loss: 1.165232  [   64/  124]
train() client id: f_00002-1-2 loss: 1.045062  [   96/  124]
train() client id: f_00002-2-0 loss: 1.035650  [   32/  124]
train() client id: f_00002-2-1 loss: 1.072637  [   64/  124]
train() client id: f_00002-2-2 loss: 1.047035  [   96/  124]
train() client id: f_00002-3-0 loss: 0.972532  [   32/  124]
train() client id: f_00002-3-1 loss: 0.967863  [   64/  124]
train() client id: f_00002-3-2 loss: 0.993331  [   96/  124]
train() client id: f_00002-4-0 loss: 1.029685  [   32/  124]
train() client id: f_00002-4-1 loss: 0.918525  [   64/  124]
train() client id: f_00002-4-2 loss: 0.999768  [   96/  124]
train() client id: f_00002-5-0 loss: 1.028489  [   32/  124]
train() client id: f_00002-5-1 loss: 0.997210  [   64/  124]
train() client id: f_00002-5-2 loss: 0.896728  [   96/  124]
train() client id: f_00002-6-0 loss: 0.925710  [   32/  124]
train() client id: f_00002-6-1 loss: 1.009877  [   64/  124]
train() client id: f_00002-6-2 loss: 0.843752  [   96/  124]
train() client id: f_00002-7-0 loss: 0.905839  [   32/  124]
train() client id: f_00002-7-1 loss: 0.842099  [   64/  124]
train() client id: f_00002-7-2 loss: 0.958046  [   96/  124]
train() client id: f_00002-8-0 loss: 0.828623  [   32/  124]
train() client id: f_00002-8-1 loss: 0.954217  [   64/  124]
train() client id: f_00002-8-2 loss: 0.803858  [   96/  124]
train() client id: f_00002-9-0 loss: 0.851259  [   32/  124]
train() client id: f_00002-9-1 loss: 0.836061  [   64/  124]
train() client id: f_00002-9-2 loss: 0.841250  [   96/  124]
train() client id: f_00002-10-0 loss: 0.972080  [   32/  124]
train() client id: f_00002-10-1 loss: 0.791252  [   64/  124]
train() client id: f_00002-10-2 loss: 0.829415  [   96/  124]
train() client id: f_00002-11-0 loss: 0.908393  [   32/  124]
train() client id: f_00002-11-1 loss: 0.728595  [   64/  124]
train() client id: f_00002-11-2 loss: 0.884953  [   96/  124]
train() client id: f_00003-0-0 loss: 0.810211  [   32/   43]
train() client id: f_00003-1-0 loss: 0.806344  [   32/   43]
train() client id: f_00003-2-0 loss: 0.844783  [   32/   43]
train() client id: f_00003-3-0 loss: 0.977144  [   32/   43]
train() client id: f_00003-4-0 loss: 0.750839  [   32/   43]
train() client id: f_00003-5-0 loss: 0.831345  [   32/   43]
train() client id: f_00003-6-0 loss: 0.769991  [   32/   43]
train() client id: f_00003-7-0 loss: 0.914060  [   32/   43]
train() client id: f_00003-8-0 loss: 0.968401  [   32/   43]
train() client id: f_00003-9-0 loss: 0.805703  [   32/   43]
train() client id: f_00003-10-0 loss: 0.809682  [   32/   43]
train() client id: f_00003-11-0 loss: 0.857409  [   32/   43]
train() client id: f_00004-0-0 loss: 0.929401  [   32/  306]
train() client id: f_00004-0-1 loss: 1.016695  [   64/  306]
train() client id: f_00004-0-2 loss: 0.831654  [   96/  306]
train() client id: f_00004-0-3 loss: 0.852247  [  128/  306]
train() client id: f_00004-0-4 loss: 0.926676  [  160/  306]
train() client id: f_00004-0-5 loss: 1.022963  [  192/  306]
train() client id: f_00004-0-6 loss: 0.957464  [  224/  306]
train() client id: f_00004-0-7 loss: 0.858741  [  256/  306]
train() client id: f_00004-0-8 loss: 0.846910  [  288/  306]
train() client id: f_00004-1-0 loss: 1.056916  [   32/  306]
train() client id: f_00004-1-1 loss: 0.980982  [   64/  306]
train() client id: f_00004-1-2 loss: 0.903029  [   96/  306]
train() client id: f_00004-1-3 loss: 0.866861  [  128/  306]
train() client id: f_00004-1-4 loss: 0.822353  [  160/  306]
train() client id: f_00004-1-5 loss: 0.955790  [  192/  306]
train() client id: f_00004-1-6 loss: 0.941168  [  224/  306]
train() client id: f_00004-1-7 loss: 0.863000  [  256/  306]
train() client id: f_00004-1-8 loss: 0.787053  [  288/  306]
train() client id: f_00004-2-0 loss: 0.986177  [   32/  306]
train() client id: f_00004-2-1 loss: 0.869748  [   64/  306]
train() client id: f_00004-2-2 loss: 0.931499  [   96/  306]
train() client id: f_00004-2-3 loss: 0.906886  [  128/  306]
train() client id: f_00004-2-4 loss: 0.940570  [  160/  306]
train() client id: f_00004-2-5 loss: 0.839569  [  192/  306]
train() client id: f_00004-2-6 loss: 0.891045  [  224/  306]
train() client id: f_00004-2-7 loss: 0.814857  [  256/  306]
train() client id: f_00004-2-8 loss: 0.977225  [  288/  306]
train() client id: f_00004-3-0 loss: 0.959537  [   32/  306]
train() client id: f_00004-3-1 loss: 0.829034  [   64/  306]
train() client id: f_00004-3-2 loss: 0.917262  [   96/  306]
train() client id: f_00004-3-3 loss: 0.935229  [  128/  306]
train() client id: f_00004-3-4 loss: 0.938334  [  160/  306]
train() client id: f_00004-3-5 loss: 0.947777  [  192/  306]
train() client id: f_00004-3-6 loss: 0.869353  [  224/  306]
train() client id: f_00004-3-7 loss: 0.869859  [  256/  306]
train() client id: f_00004-3-8 loss: 0.822643  [  288/  306]
train() client id: f_00004-4-0 loss: 0.887468  [   32/  306]
train() client id: f_00004-4-1 loss: 0.899984  [   64/  306]
train() client id: f_00004-4-2 loss: 0.921733  [   96/  306]
train() client id: f_00004-4-3 loss: 0.988995  [  128/  306]
train() client id: f_00004-4-4 loss: 0.944560  [  160/  306]
train() client id: f_00004-4-5 loss: 0.884202  [  192/  306]
train() client id: f_00004-4-6 loss: 0.900202  [  224/  306]
train() client id: f_00004-4-7 loss: 0.819084  [  256/  306]
train() client id: f_00004-4-8 loss: 0.846316  [  288/  306]
train() client id: f_00004-5-0 loss: 0.816845  [   32/  306]
train() client id: f_00004-5-1 loss: 1.018984  [   64/  306]
train() client id: f_00004-5-2 loss: 0.919154  [   96/  306]
train() client id: f_00004-5-3 loss: 0.944982  [  128/  306]
train() client id: f_00004-5-4 loss: 0.934375  [  160/  306]
train() client id: f_00004-5-5 loss: 0.838296  [  192/  306]
train() client id: f_00004-5-6 loss: 0.842093  [  224/  306]
train() client id: f_00004-5-7 loss: 0.923231  [  256/  306]
train() client id: f_00004-5-8 loss: 0.935895  [  288/  306]
train() client id: f_00004-6-0 loss: 1.024946  [   32/  306]
train() client id: f_00004-6-1 loss: 0.943223  [   64/  306]
train() client id: f_00004-6-2 loss: 0.887831  [   96/  306]
train() client id: f_00004-6-3 loss: 0.833730  [  128/  306]
train() client id: f_00004-6-4 loss: 0.810495  [  160/  306]
train() client id: f_00004-6-5 loss: 0.887065  [  192/  306]
train() client id: f_00004-6-6 loss: 0.957513  [  224/  306]
train() client id: f_00004-6-7 loss: 0.872463  [  256/  306]
train() client id: f_00004-6-8 loss: 0.873343  [  288/  306]
train() client id: f_00004-7-0 loss: 0.907713  [   32/  306]
train() client id: f_00004-7-1 loss: 1.018172  [   64/  306]
train() client id: f_00004-7-2 loss: 0.811081  [   96/  306]
train() client id: f_00004-7-3 loss: 0.952087  [  128/  306]
train() client id: f_00004-7-4 loss: 0.923813  [  160/  306]
train() client id: f_00004-7-5 loss: 0.812658  [  192/  306]
train() client id: f_00004-7-6 loss: 0.957951  [  224/  306]
train() client id: f_00004-7-7 loss: 0.884156  [  256/  306]
train() client id: f_00004-7-8 loss: 0.927313  [  288/  306]
train() client id: f_00004-8-0 loss: 0.880161  [   32/  306]
train() client id: f_00004-8-1 loss: 0.861120  [   64/  306]
train() client id: f_00004-8-2 loss: 1.034751  [   96/  306]
train() client id: f_00004-8-3 loss: 1.014718  [  128/  306]
train() client id: f_00004-8-4 loss: 0.773551  [  160/  306]
train() client id: f_00004-8-5 loss: 0.965756  [  192/  306]
train() client id: f_00004-8-6 loss: 0.888352  [  224/  306]
train() client id: f_00004-8-7 loss: 0.839956  [  256/  306]
train() client id: f_00004-8-8 loss: 0.923931  [  288/  306]
train() client id: f_00004-9-0 loss: 0.947256  [   32/  306]
train() client id: f_00004-9-1 loss: 0.898717  [   64/  306]
train() client id: f_00004-9-2 loss: 0.914874  [   96/  306]
train() client id: f_00004-9-3 loss: 0.860816  [  128/  306]
train() client id: f_00004-9-4 loss: 0.936043  [  160/  306]
train() client id: f_00004-9-5 loss: 0.981787  [  192/  306]
train() client id: f_00004-9-6 loss: 0.916034  [  224/  306]
train() client id: f_00004-9-7 loss: 0.808939  [  256/  306]
train() client id: f_00004-9-8 loss: 0.930286  [  288/  306]
train() client id: f_00004-10-0 loss: 0.893015  [   32/  306]
train() client id: f_00004-10-1 loss: 0.851158  [   64/  306]
train() client id: f_00004-10-2 loss: 0.975057  [   96/  306]
train() client id: f_00004-10-3 loss: 0.915962  [  128/  306]
train() client id: f_00004-10-4 loss: 0.961327  [  160/  306]
train() client id: f_00004-10-5 loss: 0.835259  [  192/  306]
train() client id: f_00004-10-6 loss: 0.920522  [  224/  306]
train() client id: f_00004-10-7 loss: 0.877034  [  256/  306]
train() client id: f_00004-10-8 loss: 0.941184  [  288/  306]
train() client id: f_00004-11-0 loss: 1.039563  [   32/  306]
train() client id: f_00004-11-1 loss: 0.895603  [   64/  306]
train() client id: f_00004-11-2 loss: 1.006082  [   96/  306]
train() client id: f_00004-11-3 loss: 0.917604  [  128/  306]
train() client id: f_00004-11-4 loss: 0.886296  [  160/  306]
train() client id: f_00004-11-5 loss: 0.796371  [  192/  306]
train() client id: f_00004-11-6 loss: 0.884111  [  224/  306]
train() client id: f_00004-11-7 loss: 0.850665  [  256/  306]
train() client id: f_00004-11-8 loss: 0.904088  [  288/  306]
train() client id: f_00005-0-0 loss: 0.587109  [   32/  146]
train() client id: f_00005-0-1 loss: 0.659387  [   64/  146]
train() client id: f_00005-0-2 loss: 0.423399  [   96/  146]
train() client id: f_00005-0-3 loss: 0.741762  [  128/  146]
train() client id: f_00005-1-0 loss: 0.566965  [   32/  146]
train() client id: f_00005-1-1 loss: 0.865592  [   64/  146]
train() client id: f_00005-1-2 loss: 0.452541  [   96/  146]
train() client id: f_00005-1-3 loss: 0.570298  [  128/  146]
train() client id: f_00005-2-0 loss: 0.459427  [   32/  146]
train() client id: f_00005-2-1 loss: 0.544814  [   64/  146]
train() client id: f_00005-2-2 loss: 0.774543  [   96/  146]
train() client id: f_00005-2-3 loss: 0.518260  [  128/  146]
train() client id: f_00005-3-0 loss: 0.441532  [   32/  146]
train() client id: f_00005-3-1 loss: 0.535221  [   64/  146]
train() client id: f_00005-3-2 loss: 0.464148  [   96/  146]
train() client id: f_00005-3-3 loss: 0.989285  [  128/  146]
train() client id: f_00005-4-0 loss: 0.428567  [   32/  146]
train() client id: f_00005-4-1 loss: 0.654524  [   64/  146]
train() client id: f_00005-4-2 loss: 0.801127  [   96/  146]
train() client id: f_00005-4-3 loss: 0.425679  [  128/  146]
train() client id: f_00005-5-0 loss: 0.771420  [   32/  146]
train() client id: f_00005-5-1 loss: 0.329549  [   64/  146]
train() client id: f_00005-5-2 loss: 0.530599  [   96/  146]
train() client id: f_00005-5-3 loss: 0.659972  [  128/  146]
train() client id: f_00005-6-0 loss: 0.690499  [   32/  146]
train() client id: f_00005-6-1 loss: 0.573816  [   64/  146]
train() client id: f_00005-6-2 loss: 0.310462  [   96/  146]
train() client id: f_00005-6-3 loss: 0.697370  [  128/  146]
train() client id: f_00005-7-0 loss: 0.598728  [   32/  146]
train() client id: f_00005-7-1 loss: 0.656597  [   64/  146]
train() client id: f_00005-7-2 loss: 0.652690  [   96/  146]
train() client id: f_00005-7-3 loss: 0.469390  [  128/  146]
train() client id: f_00005-8-0 loss: 0.592833  [   32/  146]
train() client id: f_00005-8-1 loss: 0.650692  [   64/  146]
train() client id: f_00005-8-2 loss: 0.323605  [   96/  146]
train() client id: f_00005-8-3 loss: 0.779881  [  128/  146]
train() client id: f_00005-9-0 loss: 0.557374  [   32/  146]
train() client id: f_00005-9-1 loss: 0.569550  [   64/  146]
train() client id: f_00005-9-2 loss: 0.606362  [   96/  146]
train() client id: f_00005-9-3 loss: 0.414163  [  128/  146]
train() client id: f_00005-10-0 loss: 0.455410  [   32/  146]
train() client id: f_00005-10-1 loss: 0.525553  [   64/  146]
train() client id: f_00005-10-2 loss: 0.584739  [   96/  146]
train() client id: f_00005-10-3 loss: 0.638815  [  128/  146]
train() client id: f_00005-11-0 loss: 0.424250  [   32/  146]
train() client id: f_00005-11-1 loss: 0.797603  [   64/  146]
train() client id: f_00005-11-2 loss: 0.397462  [   96/  146]
train() client id: f_00005-11-3 loss: 0.530343  [  128/  146]
train() client id: f_00006-0-0 loss: 0.624777  [   32/   54]
train() client id: f_00006-1-0 loss: 0.577733  [   32/   54]
train() client id: f_00006-2-0 loss: 0.581764  [   32/   54]
train() client id: f_00006-3-0 loss: 0.579630  [   32/   54]
train() client id: f_00006-4-0 loss: 0.619136  [   32/   54]
train() client id: f_00006-5-0 loss: 0.656067  [   32/   54]
train() client id: f_00006-6-0 loss: 0.628613  [   32/   54]
train() client id: f_00006-7-0 loss: 0.618510  [   32/   54]
train() client id: f_00006-8-0 loss: 0.571142  [   32/   54]
train() client id: f_00006-9-0 loss: 0.622155  [   32/   54]
train() client id: f_00006-10-0 loss: 0.613520  [   32/   54]
train() client id: f_00006-11-0 loss: 0.599081  [   32/   54]
train() client id: f_00007-0-0 loss: 0.868491  [   32/  179]
train() client id: f_00007-0-1 loss: 0.647931  [   64/  179]
train() client id: f_00007-0-2 loss: 0.530156  [   96/  179]
train() client id: f_00007-0-3 loss: 0.474011  [  128/  179]
train() client id: f_00007-0-4 loss: 0.658519  [  160/  179]
train() client id: f_00007-1-0 loss: 0.548985  [   32/  179]
train() client id: f_00007-1-1 loss: 0.695018  [   64/  179]
train() client id: f_00007-1-2 loss: 0.629326  [   96/  179]
train() client id: f_00007-1-3 loss: 0.625051  [  128/  179]
train() client id: f_00007-1-4 loss: 0.517106  [  160/  179]
train() client id: f_00007-2-0 loss: 0.600755  [   32/  179]
train() client id: f_00007-2-1 loss: 0.487853  [   64/  179]
train() client id: f_00007-2-2 loss: 0.590203  [   96/  179]
train() client id: f_00007-2-3 loss: 0.659010  [  128/  179]
train() client id: f_00007-2-4 loss: 0.537306  [  160/  179]
train() client id: f_00007-3-0 loss: 0.605550  [   32/  179]
train() client id: f_00007-3-1 loss: 0.538588  [   64/  179]
train() client id: f_00007-3-2 loss: 0.599024  [   96/  179]
train() client id: f_00007-3-3 loss: 0.494398  [  128/  179]
train() client id: f_00007-3-4 loss: 0.624368  [  160/  179]
train() client id: f_00007-4-0 loss: 0.590864  [   32/  179]
train() client id: f_00007-4-1 loss: 0.650557  [   64/  179]
train() client id: f_00007-4-2 loss: 0.532205  [   96/  179]
train() client id: f_00007-4-3 loss: 0.568160  [  128/  179]
train() client id: f_00007-4-4 loss: 0.527420  [  160/  179]
train() client id: f_00007-5-0 loss: 0.439387  [   32/  179]
train() client id: f_00007-5-1 loss: 0.551830  [   64/  179]
train() client id: f_00007-5-2 loss: 0.508349  [   96/  179]
train() client id: f_00007-5-3 loss: 0.634543  [  128/  179]
train() client id: f_00007-5-4 loss: 0.724113  [  160/  179]
train() client id: f_00007-6-0 loss: 0.523878  [   32/  179]
train() client id: f_00007-6-1 loss: 0.499890  [   64/  179]
train() client id: f_00007-6-2 loss: 0.671399  [   96/  179]
train() client id: f_00007-6-3 loss: 0.561390  [  128/  179]
train() client id: f_00007-6-4 loss: 0.569152  [  160/  179]
train() client id: f_00007-7-0 loss: 0.429404  [   32/  179]
train() client id: f_00007-7-1 loss: 0.455075  [   64/  179]
train() client id: f_00007-7-2 loss: 0.713378  [   96/  179]
train() client id: f_00007-7-3 loss: 0.786203  [  128/  179]
train() client id: f_00007-7-4 loss: 0.411032  [  160/  179]
train() client id: f_00007-8-0 loss: 0.514919  [   32/  179]
train() client id: f_00007-8-1 loss: 0.611121  [   64/  179]
train() client id: f_00007-8-2 loss: 0.500873  [   96/  179]
train() client id: f_00007-8-3 loss: 0.639781  [  128/  179]
train() client id: f_00007-8-4 loss: 0.441981  [  160/  179]
train() client id: f_00007-9-0 loss: 0.595169  [   32/  179]
train() client id: f_00007-9-1 loss: 0.765865  [   64/  179]
train() client id: f_00007-9-2 loss: 0.435151  [   96/  179]
train() client id: f_00007-9-3 loss: 0.479671  [  128/  179]
train() client id: f_00007-9-4 loss: 0.472528  [  160/  179]
train() client id: f_00007-10-0 loss: 0.662702  [   32/  179]
train() client id: f_00007-10-1 loss: 0.564681  [   64/  179]
train() client id: f_00007-10-2 loss: 0.644675  [   96/  179]
train() client id: f_00007-10-3 loss: 0.425254  [  128/  179]
train() client id: f_00007-10-4 loss: 0.582228  [  160/  179]
train() client id: f_00007-11-0 loss: 0.671575  [   32/  179]
train() client id: f_00007-11-1 loss: 0.608670  [   64/  179]
train() client id: f_00007-11-2 loss: 0.518107  [   96/  179]
train() client id: f_00007-11-3 loss: 0.510206  [  128/  179]
train() client id: f_00007-11-4 loss: 0.551522  [  160/  179]
train() client id: f_00008-0-0 loss: 0.825605  [   32/  130]
train() client id: f_00008-0-1 loss: 0.744200  [   64/  130]
train() client id: f_00008-0-2 loss: 0.769459  [   96/  130]
train() client id: f_00008-0-3 loss: 0.902740  [  128/  130]
train() client id: f_00008-1-0 loss: 0.819937  [   32/  130]
train() client id: f_00008-1-1 loss: 0.924689  [   64/  130]
train() client id: f_00008-1-2 loss: 0.800522  [   96/  130]
train() client id: f_00008-1-3 loss: 0.701115  [  128/  130]
train() client id: f_00008-2-0 loss: 0.810853  [   32/  130]
train() client id: f_00008-2-1 loss: 0.797070  [   64/  130]
train() client id: f_00008-2-2 loss: 0.758261  [   96/  130]
train() client id: f_00008-2-3 loss: 0.849552  [  128/  130]
train() client id: f_00008-3-0 loss: 0.771385  [   32/  130]
train() client id: f_00008-3-1 loss: 0.836822  [   64/  130]
train() client id: f_00008-3-2 loss: 0.709440  [   96/  130]
train() client id: f_00008-3-3 loss: 0.839279  [  128/  130]
train() client id: f_00008-4-0 loss: 0.837658  [   32/  130]
train() client id: f_00008-4-1 loss: 0.823839  [   64/  130]
train() client id: f_00008-4-2 loss: 0.822401  [   96/  130]
train() client id: f_00008-4-3 loss: 0.708650  [  128/  130]
train() client id: f_00008-5-0 loss: 0.764591  [   32/  130]
train() client id: f_00008-5-1 loss: 0.829338  [   64/  130]
train() client id: f_00008-5-2 loss: 0.730067  [   96/  130]
train() client id: f_00008-5-3 loss: 0.870078  [  128/  130]
train() client id: f_00008-6-0 loss: 0.810819  [   32/  130]
train() client id: f_00008-6-1 loss: 0.821873  [   64/  130]
train() client id: f_00008-6-2 loss: 0.778611  [   96/  130]
train() client id: f_00008-6-3 loss: 0.782282  [  128/  130]
train() client id: f_00008-7-0 loss: 0.782476  [   32/  130]
train() client id: f_00008-7-1 loss: 0.820689  [   64/  130]
train() client id: f_00008-7-2 loss: 0.724091  [   96/  130]
train() client id: f_00008-7-3 loss: 0.845518  [  128/  130]
train() client id: f_00008-8-0 loss: 0.739738  [   32/  130]
train() client id: f_00008-8-1 loss: 0.803599  [   64/  130]
train() client id: f_00008-8-2 loss: 0.759893  [   96/  130]
train() client id: f_00008-8-3 loss: 0.866718  [  128/  130]
train() client id: f_00008-9-0 loss: 0.749011  [   32/  130]
train() client id: f_00008-9-1 loss: 0.810866  [   64/  130]
train() client id: f_00008-9-2 loss: 0.854760  [   96/  130]
train() client id: f_00008-9-3 loss: 0.755011  [  128/  130]
train() client id: f_00008-10-0 loss: 0.726651  [   32/  130]
train() client id: f_00008-10-1 loss: 0.807214  [   64/  130]
train() client id: f_00008-10-2 loss: 0.817963  [   96/  130]
train() client id: f_00008-10-3 loss: 0.804325  [  128/  130]
train() client id: f_00008-11-0 loss: 0.778703  [   32/  130]
train() client id: f_00008-11-1 loss: 0.818282  [   64/  130]
train() client id: f_00008-11-2 loss: 0.756270  [   96/  130]
train() client id: f_00008-11-3 loss: 0.799797  [  128/  130]
train() client id: f_00009-0-0 loss: 1.023031  [   32/  118]
train() client id: f_00009-0-1 loss: 1.365542  [   64/  118]
train() client id: f_00009-0-2 loss: 1.110062  [   96/  118]
train() client id: f_00009-1-0 loss: 1.108789  [   32/  118]
train() client id: f_00009-1-1 loss: 1.082293  [   64/  118]
train() client id: f_00009-1-2 loss: 1.029517  [   96/  118]
train() client id: f_00009-2-0 loss: 1.056424  [   32/  118]
train() client id: f_00009-2-1 loss: 0.996715  [   64/  118]
train() client id: f_00009-2-2 loss: 1.128869  [   96/  118]
train() client id: f_00009-3-0 loss: 1.055170  [   32/  118]
train() client id: f_00009-3-1 loss: 1.013502  [   64/  118]
train() client id: f_00009-3-2 loss: 1.105734  [   96/  118]
train() client id: f_00009-4-0 loss: 1.066262  [   32/  118]
train() client id: f_00009-4-1 loss: 0.960976  [   64/  118]
train() client id: f_00009-4-2 loss: 1.041896  [   96/  118]
train() client id: f_00009-5-0 loss: 0.862352  [   32/  118]
train() client id: f_00009-5-1 loss: 1.240143  [   64/  118]
train() client id: f_00009-5-2 loss: 0.870156  [   96/  118]
train() client id: f_00009-6-0 loss: 0.889283  [   32/  118]
train() client id: f_00009-6-1 loss: 1.053545  [   64/  118]
train() client id: f_00009-6-2 loss: 1.014582  [   96/  118]
train() client id: f_00009-7-0 loss: 1.038425  [   32/  118]
train() client id: f_00009-7-1 loss: 0.958787  [   64/  118]
train() client id: f_00009-7-2 loss: 1.079383  [   96/  118]
train() client id: f_00009-8-0 loss: 0.933724  [   32/  118]
train() client id: f_00009-8-1 loss: 0.918784  [   64/  118]
train() client id: f_00009-8-2 loss: 1.074158  [   96/  118]
train() client id: f_00009-9-0 loss: 0.947726  [   32/  118]
train() client id: f_00009-9-1 loss: 0.968089  [   64/  118]
train() client id: f_00009-9-2 loss: 1.030693  [   96/  118]
train() client id: f_00009-10-0 loss: 1.030979  [   32/  118]
train() client id: f_00009-10-1 loss: 0.976830  [   64/  118]
train() client id: f_00009-10-2 loss: 0.988927  [   96/  118]
train() client id: f_00009-11-0 loss: 0.970478  [   32/  118]
train() client id: f_00009-11-1 loss: 1.035084  [   64/  118]
train() client id: f_00009-11-2 loss: 1.001691  [   96/  118]
At round 8 accuracy: 0.6286472148541115
At round 8 training accuracy: 0.5835010060362174
At round 8 training loss: 0.8442705856234323
update_location
xs = -3.905658 4.200318 60.009024 18.811294 0.979296 3.956410 -22.443192 -1.324852 44.663977 2.939121 
ys = 52.587959 35.555839 1.320614 -22.455176 14.350187 -2.185849 -2.624984 0.822348 17.569006 4.001482 
xs mean: 10.788573831882573
ys mean: 9.894142535528704
dists_uav = 113.051969 106.216102 116.631158 104.202206 101.029139 100.102103 102.521156 100.012157 110.921327 100.123176 
uav_gains = -101.332045 -100.654797 -101.670508 -100.446952 -100.111185 -100.011096 -100.270360 -100.001336 -101.125447 -100.013382 
uav_gains_db_mean: -100.56371065622658
dists_bs = 210.375810 227.062608 292.222125 276.679172 238.279406 251.833719 234.145713 245.969331 270.253179 246.784976 
bs_gains = -104.610942 -105.539137 -108.607026 -107.942400 -106.125481 -106.798248 -105.912673 -106.511726 -107.656642 -106.551983 
bs_gains_db_mean: -106.62562573686071
Round 9
-------------------------------
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.80354861 20.47010187  9.65317002  3.45074232 23.61178753 11.38579797
  4.29063581 13.84678522 10.17374854  9.24174878]
obj_prev = 115.9280666591941
eta_min = 3.3564500217132474e-10	eta_max = 0.9202772988105504
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 26.95968104561057	eta = 0.909090909090909
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 46.612772647926946	eta = 0.5257958185768866
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 37.23476446800202	eta = 0.6582236063723965
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.55373220830825	eta = 0.6893453774967627
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.470824829161245	eta = 0.690956612049402
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.47060887359621	eta = 0.690960818797758
eta = 0.690960818797758
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [0.03050446 0.06415621 0.03002026 0.01041025 0.07408226 0.03534644
 0.01307334 0.0433357  0.03147286 0.02856767]
ene_total = [3.01933378 5.87455757 2.98817258 1.36705087 6.70212565 3.5823017
 1.57947799 4.03156066 3.30411446 3.0219136 ]
ti_comp = [0.29712306 0.28209195 0.2961178  0.29961782 0.27945843 0.27622759
 0.30009367 0.30080535 0.27174348 0.27743751]
ti_coms = [0.06670996 0.08174107 0.06771522 0.0642152  0.08437459 0.08760544
 0.06373935 0.06302768 0.09208954 0.08639551]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.00954316e-05 2.07402819e-04 1.92838817e-05 7.85468046e-07
 3.25378074e-04 3.61728644e-05 1.55069375e-06 5.62142497e-05
 2.63857967e-05 1.89310232e-05]
ene_total = [0.54344022 0.68073341 0.5515389  0.52160987 0.71170427 0.71445587
 0.51780721 0.51646678 0.75008018 0.70322868]
optimize_network iter = 0 obj = 6.211065406906146
eta = 0.690960818797758
freqs = [5.13330407e+07 1.13715060e+08 5.06897303e+07 1.73725523e+07
 1.32546121e+08 6.39806453e+07 2.17820992e+07 7.20327985e+07
 5.79091277e+07 5.14848716e+07]
eta_min = 0.6909608187976134	eta_max = 0.6909608187977386
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 0.05319201668668125	eta = 0.9090909090909091
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 20.968149575428694	eta = 0.002306182461743776
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.200288580777995	eta = 0.021977289355824242
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.1330781928113622	eta = 0.022669763803801746
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.1330546285065273	eta = 0.022670014241468774
eta = 0.022670014241468774
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.05846218e-04 2.12451699e-03 1.97533160e-04 8.04588971e-06
 3.33298867e-03 3.70534332e-04 1.58844283e-05 5.75826930e-04
 2.70281154e-04 1.93918678e-04]
ene_total = [0.17661037 0.26448822 0.17897858 0.16512347 0.30228746 0.23450342
 0.1641027  0.17665537 0.24344476 0.22686028]
ti_comp = [0.29712306 0.28209195 0.2961178  0.29961782 0.27945843 0.27622759
 0.30009367 0.30080535 0.27174348 0.27743751]
ti_coms = [0.06670996 0.08174107 0.06771522 0.0642152  0.08437459 0.08760544
 0.06373935 0.06302768 0.09208954 0.08639551]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.00954316e-05 2.07402819e-04 1.92838817e-05 7.85468046e-07
 3.25378074e-04 3.61728644e-05 1.55069375e-06 5.62142497e-05
 2.63857967e-05 1.89310232e-05]
ene_total = [0.54344022 0.68073341 0.5515389  0.52160987 0.71170427 0.71445587
 0.51780721 0.51646678 0.75008018 0.70322868]
optimize_network iter = 1 obj = 6.211065406903268
eta = 0.6909608187976134
freqs = [5.13330407e+07 1.13715060e+08 5.06897303e+07 1.73725523e+07
 1.32546121e+08 6.39806453e+07 2.17820992e+07 7.20327985e+07
 5.79091277e+07 5.14848716e+07]
Done!
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [1.99212129e-05 2.05604726e-04 1.91166988e-05 7.78658377e-07
 3.22557186e-04 3.58592613e-05 1.53724990e-06 5.57268965e-05
 2.61570432e-05 1.87668994e-05]
ene_total = [0.00669092 0.00837971 0.00679064 0.0064223  0.00876002 0.0087964
 0.00637547 0.00635849 0.00923511 0.00865832]
At round 9 energy consumption: 0.07646738157410429
At round 9 eta: 0.6909608187976134
At round 9 a_n: 25.09969023353052
At round 9 local rounds: 12.104944645341616
At round 9 global rounds: 81.21847247936174
gradient difference: 0.3574024736881256
train() client id: f_00000-0-0 loss: 0.957664  [   32/  126]
train() client id: f_00000-0-1 loss: 1.115707  [   64/  126]
train() client id: f_00000-0-2 loss: 1.167701  [   96/  126]
train() client id: f_00000-1-0 loss: 1.041701  [   32/  126]
train() client id: f_00000-1-1 loss: 1.042342  [   64/  126]
train() client id: f_00000-1-2 loss: 0.943032  [   96/  126]
train() client id: f_00000-2-0 loss: 0.993336  [   32/  126]
train() client id: f_00000-2-1 loss: 1.044507  [   64/  126]
train() client id: f_00000-2-2 loss: 0.945091  [   96/  126]
train() client id: f_00000-3-0 loss: 0.884359  [   32/  126]
train() client id: f_00000-3-1 loss: 0.879178  [   64/  126]
train() client id: f_00000-3-2 loss: 1.074636  [   96/  126]
train() client id: f_00000-4-0 loss: 0.849074  [   32/  126]
train() client id: f_00000-4-1 loss: 0.861200  [   64/  126]
train() client id: f_00000-4-2 loss: 0.938721  [   96/  126]
train() client id: f_00000-5-0 loss: 0.757099  [   32/  126]
train() client id: f_00000-5-1 loss: 0.863285  [   64/  126]
train() client id: f_00000-5-2 loss: 0.862227  [   96/  126]
train() client id: f_00000-6-0 loss: 0.760745  [   32/  126]
train() client id: f_00000-6-1 loss: 0.865866  [   64/  126]
train() client id: f_00000-6-2 loss: 0.898991  [   96/  126]
train() client id: f_00000-7-0 loss: 0.788380  [   32/  126]
train() client id: f_00000-7-1 loss: 0.831125  [   64/  126]
train() client id: f_00000-7-2 loss: 0.866857  [   96/  126]
train() client id: f_00000-8-0 loss: 0.809733  [   32/  126]
train() client id: f_00000-8-1 loss: 0.859997  [   64/  126]
train() client id: f_00000-8-2 loss: 0.755867  [   96/  126]
train() client id: f_00000-9-0 loss: 0.814619  [   32/  126]
train() client id: f_00000-9-1 loss: 0.724625  [   64/  126]
train() client id: f_00000-9-2 loss: 0.927866  [   96/  126]
train() client id: f_00000-10-0 loss: 0.884898  [   32/  126]
train() client id: f_00000-10-1 loss: 0.790485  [   64/  126]
train() client id: f_00000-10-2 loss: 0.745264  [   96/  126]
train() client id: f_00000-11-0 loss: 0.740720  [   32/  126]
train() client id: f_00000-11-1 loss: 0.773389  [   64/  126]
train() client id: f_00000-11-2 loss: 0.897117  [   96/  126]
train() client id: f_00001-0-0 loss: 0.518064  [   32/  265]
train() client id: f_00001-0-1 loss: 0.544235  [   64/  265]
train() client id: f_00001-0-2 loss: 0.475331  [   96/  265]
train() client id: f_00001-0-3 loss: 0.561623  [  128/  265]
train() client id: f_00001-0-4 loss: 0.533055  [  160/  265]
train() client id: f_00001-0-5 loss: 0.471188  [  192/  265]
train() client id: f_00001-0-6 loss: 0.527560  [  224/  265]
train() client id: f_00001-0-7 loss: 0.595853  [  256/  265]
train() client id: f_00001-1-0 loss: 0.507187  [   32/  265]
train() client id: f_00001-1-1 loss: 0.471070  [   64/  265]
train() client id: f_00001-1-2 loss: 0.478679  [   96/  265]
train() client id: f_00001-1-3 loss: 0.493901  [  128/  265]
train() client id: f_00001-1-4 loss: 0.655771  [  160/  265]
train() client id: f_00001-1-5 loss: 0.506356  [  192/  265]
train() client id: f_00001-1-6 loss: 0.455377  [  224/  265]
train() client id: f_00001-1-7 loss: 0.503913  [  256/  265]
train() client id: f_00001-2-0 loss: 0.535218  [   32/  265]
train() client id: f_00001-2-1 loss: 0.521310  [   64/  265]
train() client id: f_00001-2-2 loss: 0.584996  [   96/  265]
train() client id: f_00001-2-3 loss: 0.441562  [  128/  265]
train() client id: f_00001-2-4 loss: 0.474830  [  160/  265]
train() client id: f_00001-2-5 loss: 0.462914  [  192/  265]
train() client id: f_00001-2-6 loss: 0.510302  [  224/  265]
train() client id: f_00001-2-7 loss: 0.428896  [  256/  265]
train() client id: f_00001-3-0 loss: 0.497821  [   32/  265]
train() client id: f_00001-3-1 loss: 0.438105  [   64/  265]
train() client id: f_00001-3-2 loss: 0.531106  [   96/  265]
train() client id: f_00001-3-3 loss: 0.404779  [  128/  265]
train() client id: f_00001-3-4 loss: 0.417035  [  160/  265]
train() client id: f_00001-3-5 loss: 0.641643  [  192/  265]
train() client id: f_00001-3-6 loss: 0.481897  [  224/  265]
train() client id: f_00001-3-7 loss: 0.536987  [  256/  265]
train() client id: f_00001-4-0 loss: 0.591501  [   32/  265]
train() client id: f_00001-4-1 loss: 0.439680  [   64/  265]
train() client id: f_00001-4-2 loss: 0.479999  [   96/  265]
train() client id: f_00001-4-3 loss: 0.416460  [  128/  265]
train() client id: f_00001-4-4 loss: 0.585519  [  160/  265]
train() client id: f_00001-4-5 loss: 0.397410  [  192/  265]
train() client id: f_00001-4-6 loss: 0.508439  [  224/  265]
train() client id: f_00001-4-7 loss: 0.523992  [  256/  265]
train() client id: f_00001-5-0 loss: 0.440207  [   32/  265]
train() client id: f_00001-5-1 loss: 0.428528  [   64/  265]
train() client id: f_00001-5-2 loss: 0.606806  [   96/  265]
train() client id: f_00001-5-3 loss: 0.515230  [  128/  265]
train() client id: f_00001-5-4 loss: 0.386788  [  160/  265]
train() client id: f_00001-5-5 loss: 0.466704  [  192/  265]
train() client id: f_00001-5-6 loss: 0.456980  [  224/  265]
train() client id: f_00001-5-7 loss: 0.526917  [  256/  265]
train() client id: f_00001-6-0 loss: 0.465491  [   32/  265]
train() client id: f_00001-6-1 loss: 0.392852  [   64/  265]
train() client id: f_00001-6-2 loss: 0.554668  [   96/  265]
train() client id: f_00001-6-3 loss: 0.574717  [  128/  265]
train() client id: f_00001-6-4 loss: 0.488883  [  160/  265]
train() client id: f_00001-6-5 loss: 0.484714  [  192/  265]
train() client id: f_00001-6-6 loss: 0.394036  [  224/  265]
train() client id: f_00001-6-7 loss: 0.451232  [  256/  265]
train() client id: f_00001-7-0 loss: 0.443660  [   32/  265]
train() client id: f_00001-7-1 loss: 0.458157  [   64/  265]
train() client id: f_00001-7-2 loss: 0.429512  [   96/  265]
train() client id: f_00001-7-3 loss: 0.456256  [  128/  265]
train() client id: f_00001-7-4 loss: 0.412345  [  160/  265]
train() client id: f_00001-7-5 loss: 0.564574  [  192/  265]
train() client id: f_00001-7-6 loss: 0.545950  [  224/  265]
train() client id: f_00001-7-7 loss: 0.560966  [  256/  265]
train() client id: f_00001-8-0 loss: 0.578130  [   32/  265]
train() client id: f_00001-8-1 loss: 0.400314  [   64/  265]
train() client id: f_00001-8-2 loss: 0.474200  [   96/  265]
train() client id: f_00001-8-3 loss: 0.534343  [  128/  265]
train() client id: f_00001-8-4 loss: 0.512488  [  160/  265]
train() client id: f_00001-8-5 loss: 0.444768  [  192/  265]
train() client id: f_00001-8-6 loss: 0.445316  [  224/  265]
train() client id: f_00001-8-7 loss: 0.464784  [  256/  265]
train() client id: f_00001-9-0 loss: 0.653623  [   32/  265]
train() client id: f_00001-9-1 loss: 0.404567  [   64/  265]
train() client id: f_00001-9-2 loss: 0.509477  [   96/  265]
train() client id: f_00001-9-3 loss: 0.448771  [  128/  265]
train() client id: f_00001-9-4 loss: 0.482183  [  160/  265]
train() client id: f_00001-9-5 loss: 0.509726  [  192/  265]
train() client id: f_00001-9-6 loss: 0.393823  [  224/  265]
train() client id: f_00001-9-7 loss: 0.399345  [  256/  265]
train() client id: f_00001-10-0 loss: 0.483333  [   32/  265]
train() client id: f_00001-10-1 loss: 0.602808  [   64/  265]
train() client id: f_00001-10-2 loss: 0.442438  [   96/  265]
train() client id: f_00001-10-3 loss: 0.401331  [  128/  265]
train() client id: f_00001-10-4 loss: 0.509563  [  160/  265]
train() client id: f_00001-10-5 loss: 0.481187  [  192/  265]
train() client id: f_00001-10-6 loss: 0.473777  [  224/  265]
train() client id: f_00001-10-7 loss: 0.450120  [  256/  265]
train() client id: f_00001-11-0 loss: 0.446786  [   32/  265]
train() client id: f_00001-11-1 loss: 0.463163  [   64/  265]
train() client id: f_00001-11-2 loss: 0.508728  [   96/  265]
train() client id: f_00001-11-3 loss: 0.389858  [  128/  265]
train() client id: f_00001-11-4 loss: 0.427774  [  160/  265]
train() client id: f_00001-11-5 loss: 0.613641  [  192/  265]
train() client id: f_00001-11-6 loss: 0.542684  [  224/  265]
train() client id: f_00001-11-7 loss: 0.432386  [  256/  265]
train() client id: f_00002-0-0 loss: 1.127021  [   32/  124]
train() client id: f_00002-0-1 loss: 1.080332  [   64/  124]
train() client id: f_00002-0-2 loss: 1.095429  [   96/  124]
train() client id: f_00002-1-0 loss: 1.156760  [   32/  124]
train() client id: f_00002-1-1 loss: 1.207097  [   64/  124]
train() client id: f_00002-1-2 loss: 1.036262  [   96/  124]
train() client id: f_00002-2-0 loss: 0.972722  [   32/  124]
train() client id: f_00002-2-1 loss: 0.990282  [   64/  124]
train() client id: f_00002-2-2 loss: 1.181476  [   96/  124]
train() client id: f_00002-3-0 loss: 0.951822  [   32/  124]
train() client id: f_00002-3-1 loss: 1.194661  [   64/  124]
train() client id: f_00002-3-2 loss: 1.104619  [   96/  124]
train() client id: f_00002-4-0 loss: 1.062376  [   32/  124]
train() client id: f_00002-4-1 loss: 1.072796  [   64/  124]
train() client id: f_00002-4-2 loss: 0.938111  [   96/  124]
train() client id: f_00002-5-0 loss: 1.009340  [   32/  124]
train() client id: f_00002-5-1 loss: 1.113969  [   64/  124]
train() client id: f_00002-5-2 loss: 0.913715  [   96/  124]
train() client id: f_00002-6-0 loss: 1.122658  [   32/  124]
train() client id: f_00002-6-1 loss: 0.985252  [   64/  124]
train() client id: f_00002-6-2 loss: 0.899605  [   96/  124]
train() client id: f_00002-7-0 loss: 0.895101  [   32/  124]
train() client id: f_00002-7-1 loss: 1.132393  [   64/  124]
train() client id: f_00002-7-2 loss: 0.922090  [   96/  124]
train() client id: f_00002-8-0 loss: 1.015700  [   32/  124]
train() client id: f_00002-8-1 loss: 1.002231  [   64/  124]
train() client id: f_00002-8-2 loss: 0.940322  [   96/  124]
train() client id: f_00002-9-0 loss: 1.012649  [   32/  124]
train() client id: f_00002-9-1 loss: 0.951825  [   64/  124]
train() client id: f_00002-9-2 loss: 0.918153  [   96/  124]
train() client id: f_00002-10-0 loss: 0.986170  [   32/  124]
train() client id: f_00002-10-1 loss: 0.874867  [   64/  124]
train() client id: f_00002-10-2 loss: 1.029786  [   96/  124]
train() client id: f_00002-11-0 loss: 1.073542  [   32/  124]
train() client id: f_00002-11-1 loss: 0.830582  [   64/  124]
train() client id: f_00002-11-2 loss: 1.050408  [   96/  124]
train() client id: f_00003-0-0 loss: 0.960688  [   32/   43]
train() client id: f_00003-1-0 loss: 0.875477  [   32/   43]
train() client id: f_00003-2-0 loss: 0.863713  [   32/   43]
train() client id: f_00003-3-0 loss: 1.131663  [   32/   43]
train() client id: f_00003-4-0 loss: 0.937806  [   32/   43]
train() client id: f_00003-5-0 loss: 0.880753  [   32/   43]
train() client id: f_00003-6-0 loss: 0.872373  [   32/   43]
train() client id: f_00003-7-0 loss: 0.921018  [   32/   43]
train() client id: f_00003-8-0 loss: 0.918194  [   32/   43]
train() client id: f_00003-9-0 loss: 0.992769  [   32/   43]
train() client id: f_00003-10-0 loss: 0.905377  [   32/   43]
train() client id: f_00003-11-0 loss: 0.887671  [   32/   43]
train() client id: f_00004-0-0 loss: 0.989371  [   32/  306]
train() client id: f_00004-0-1 loss: 0.893245  [   64/  306]
train() client id: f_00004-0-2 loss: 0.898161  [   96/  306]
train() client id: f_00004-0-3 loss: 0.938833  [  128/  306]
train() client id: f_00004-0-4 loss: 0.836513  [  160/  306]
train() client id: f_00004-0-5 loss: 0.919773  [  192/  306]
train() client id: f_00004-0-6 loss: 0.954616  [  224/  306]
train() client id: f_00004-0-7 loss: 0.903833  [  256/  306]
train() client id: f_00004-0-8 loss: 0.957422  [  288/  306]
train() client id: f_00004-1-0 loss: 0.836554  [   32/  306]
train() client id: f_00004-1-1 loss: 0.838253  [   64/  306]
train() client id: f_00004-1-2 loss: 0.911458  [   96/  306]
train() client id: f_00004-1-3 loss: 0.913013  [  128/  306]
train() client id: f_00004-1-4 loss: 0.893575  [  160/  306]
train() client id: f_00004-1-5 loss: 0.948177  [  192/  306]
train() client id: f_00004-1-6 loss: 0.938932  [  224/  306]
train() client id: f_00004-1-7 loss: 1.001842  [  256/  306]
train() client id: f_00004-1-8 loss: 0.863746  [  288/  306]
train() client id: f_00004-2-0 loss: 0.940096  [   32/  306]
train() client id: f_00004-2-1 loss: 0.918659  [   64/  306]
train() client id: f_00004-2-2 loss: 1.083712  [   96/  306]
train() client id: f_00004-2-3 loss: 0.836015  [  128/  306]
train() client id: f_00004-2-4 loss: 0.863358  [  160/  306]
train() client id: f_00004-2-5 loss: 0.770563  [  192/  306]
train() client id: f_00004-2-6 loss: 0.941900  [  224/  306]
train() client id: f_00004-2-7 loss: 1.031060  [  256/  306]
train() client id: f_00004-2-8 loss: 0.900638  [  288/  306]
train() client id: f_00004-3-0 loss: 0.908982  [   32/  306]
train() client id: f_00004-3-1 loss: 0.980225  [   64/  306]
train() client id: f_00004-3-2 loss: 0.882288  [   96/  306]
train() client id: f_00004-3-3 loss: 0.941142  [  128/  306]
train() client id: f_00004-3-4 loss: 0.783473  [  160/  306]
train() client id: f_00004-3-5 loss: 0.927802  [  192/  306]
train() client id: f_00004-3-6 loss: 0.885988  [  224/  306]
train() client id: f_00004-3-7 loss: 0.907845  [  256/  306]
train() client id: f_00004-3-8 loss: 0.965959  [  288/  306]
train() client id: f_00004-4-0 loss: 0.998296  [   32/  306]
train() client id: f_00004-4-1 loss: 0.969172  [   64/  306]
train() client id: f_00004-4-2 loss: 0.790663  [   96/  306]
train() client id: f_00004-4-3 loss: 0.956680  [  128/  306]
train() client id: f_00004-4-4 loss: 0.878942  [  160/  306]
train() client id: f_00004-4-5 loss: 0.960350  [  192/  306]
train() client id: f_00004-4-6 loss: 0.875357  [  224/  306]
train() client id: f_00004-4-7 loss: 0.806519  [  256/  306]
train() client id: f_00004-4-8 loss: 0.935868  [  288/  306]
train() client id: f_00004-5-0 loss: 0.940145  [   32/  306]
train() client id: f_00004-5-1 loss: 0.930678  [   64/  306]
train() client id: f_00004-5-2 loss: 0.862455  [   96/  306]
train() client id: f_00004-5-3 loss: 0.894328  [  128/  306]
train() client id: f_00004-5-4 loss: 0.870618  [  160/  306]
train() client id: f_00004-5-5 loss: 1.069725  [  192/  306]
train() client id: f_00004-5-6 loss: 0.916795  [  224/  306]
train() client id: f_00004-5-7 loss: 0.893856  [  256/  306]
train() client id: f_00004-5-8 loss: 0.804396  [  288/  306]
train() client id: f_00004-6-0 loss: 0.778507  [   32/  306]
train() client id: f_00004-6-1 loss: 1.030632  [   64/  306]
train() client id: f_00004-6-2 loss: 0.982372  [   96/  306]
train() client id: f_00004-6-3 loss: 1.051641  [  128/  306]
train() client id: f_00004-6-4 loss: 0.807704  [  160/  306]
train() client id: f_00004-6-5 loss: 1.033995  [  192/  306]
train() client id: f_00004-6-6 loss: 0.805223  [  224/  306]
train() client id: f_00004-6-7 loss: 0.816149  [  256/  306]
train() client id: f_00004-6-8 loss: 0.777909  [  288/  306]
train() client id: f_00004-7-0 loss: 1.035738  [   32/  306]
train() client id: f_00004-7-1 loss: 0.875711  [   64/  306]
train() client id: f_00004-7-2 loss: 0.874227  [   96/  306]
train() client id: f_00004-7-3 loss: 0.841238  [  128/  306]
train() client id: f_00004-7-4 loss: 0.979278  [  160/  306]
train() client id: f_00004-7-5 loss: 0.938388  [  192/  306]
train() client id: f_00004-7-6 loss: 0.841601  [  224/  306]
train() client id: f_00004-7-7 loss: 0.946259  [  256/  306]
train() client id: f_00004-7-8 loss: 0.878363  [  288/  306]
train() client id: f_00004-8-0 loss: 0.879327  [   32/  306]
train() client id: f_00004-8-1 loss: 0.896640  [   64/  306]
train() client id: f_00004-8-2 loss: 0.876302  [   96/  306]
train() client id: f_00004-8-3 loss: 0.902448  [  128/  306]
train() client id: f_00004-8-4 loss: 0.918473  [  160/  306]
train() client id: f_00004-8-5 loss: 0.919079  [  192/  306]
train() client id: f_00004-8-6 loss: 0.936669  [  224/  306]
train() client id: f_00004-8-7 loss: 1.021291  [  256/  306]
train() client id: f_00004-8-8 loss: 0.832260  [  288/  306]
train() client id: f_00004-9-0 loss: 0.814402  [   32/  306]
train() client id: f_00004-9-1 loss: 0.957989  [   64/  306]
train() client id: f_00004-9-2 loss: 0.928754  [   96/  306]
train() client id: f_00004-9-3 loss: 0.892084  [  128/  306]
train() client id: f_00004-9-4 loss: 0.915314  [  160/  306]
train() client id: f_00004-9-5 loss: 0.944947  [  192/  306]
train() client id: f_00004-9-6 loss: 0.947667  [  224/  306]
train() client id: f_00004-9-7 loss: 0.895052  [  256/  306]
train() client id: f_00004-9-8 loss: 0.946439  [  288/  306]
train() client id: f_00004-10-0 loss: 0.905869  [   32/  306]
train() client id: f_00004-10-1 loss: 0.895058  [   64/  306]
train() client id: f_00004-10-2 loss: 0.955193  [   96/  306]
train() client id: f_00004-10-3 loss: 0.889383  [  128/  306]
train() client id: f_00004-10-4 loss: 0.829415  [  160/  306]
train() client id: f_00004-10-5 loss: 0.900128  [  192/  306]
train() client id: f_00004-10-6 loss: 0.846913  [  224/  306]
train() client id: f_00004-10-7 loss: 1.024586  [  256/  306]
train() client id: f_00004-10-8 loss: 0.940622  [  288/  306]
train() client id: f_00004-11-0 loss: 0.821331  [   32/  306]
train() client id: f_00004-11-1 loss: 0.906558  [   64/  306]
train() client id: f_00004-11-2 loss: 0.924387  [   96/  306]
train() client id: f_00004-11-3 loss: 0.914661  [  128/  306]
train() client id: f_00004-11-4 loss: 0.906949  [  160/  306]
train() client id: f_00004-11-5 loss: 0.931987  [  192/  306]
train() client id: f_00004-11-6 loss: 0.929872  [  224/  306]
train() client id: f_00004-11-7 loss: 0.952291  [  256/  306]
train() client id: f_00004-11-8 loss: 0.885940  [  288/  306]
train() client id: f_00005-0-0 loss: 0.703924  [   32/  146]
train() client id: f_00005-0-1 loss: 0.771692  [   64/  146]
train() client id: f_00005-0-2 loss: 0.767706  [   96/  146]
train() client id: f_00005-0-3 loss: 0.809841  [  128/  146]
train() client id: f_00005-1-0 loss: 0.645421  [   32/  146]
train() client id: f_00005-1-1 loss: 0.648666  [   64/  146]
train() client id: f_00005-1-2 loss: 0.602683  [   96/  146]
train() client id: f_00005-1-3 loss: 0.897740  [  128/  146]
train() client id: f_00005-2-0 loss: 0.643610  [   32/  146]
train() client id: f_00005-2-1 loss: 0.708451  [   64/  146]
train() client id: f_00005-2-2 loss: 0.826767  [   96/  146]
train() client id: f_00005-2-3 loss: 0.710021  [  128/  146]
train() client id: f_00005-3-0 loss: 0.846208  [   32/  146]
train() client id: f_00005-3-1 loss: 0.549924  [   64/  146]
train() client id: f_00005-3-2 loss: 0.742335  [   96/  146]
train() client id: f_00005-3-3 loss: 0.771303  [  128/  146]
train() client id: f_00005-4-0 loss: 0.713182  [   32/  146]
train() client id: f_00005-4-1 loss: 1.004976  [   64/  146]
train() client id: f_00005-4-2 loss: 0.595358  [   96/  146]
train() client id: f_00005-4-3 loss: 0.509775  [  128/  146]
train() client id: f_00005-5-0 loss: 0.804310  [   32/  146]
train() client id: f_00005-5-1 loss: 0.759167  [   64/  146]
train() client id: f_00005-5-2 loss: 0.491303  [   96/  146]
train() client id: f_00005-5-3 loss: 0.782746  [  128/  146]
train() client id: f_00005-6-0 loss: 0.733237  [   32/  146]
train() client id: f_00005-6-1 loss: 0.858995  [   64/  146]
train() client id: f_00005-6-2 loss: 0.594195  [   96/  146]
train() client id: f_00005-6-3 loss: 0.596711  [  128/  146]
train() client id: f_00005-7-0 loss: 0.690320  [   32/  146]
train() client id: f_00005-7-1 loss: 0.737146  [   64/  146]
train() client id: f_00005-7-2 loss: 0.715191  [   96/  146]
train() client id: f_00005-7-3 loss: 0.561614  [  128/  146]
train() client id: f_00005-8-0 loss: 0.730112  [   32/  146]
train() client id: f_00005-8-1 loss: 0.690988  [   64/  146]
train() client id: f_00005-8-2 loss: 0.762666  [   96/  146]
train() client id: f_00005-8-3 loss: 0.650160  [  128/  146]
train() client id: f_00005-9-0 loss: 0.626993  [   32/  146]
train() client id: f_00005-9-1 loss: 0.790273  [   64/  146]
train() client id: f_00005-9-2 loss: 0.535903  [   96/  146]
train() client id: f_00005-9-3 loss: 0.854777  [  128/  146]
train() client id: f_00005-10-0 loss: 0.905407  [   32/  146]
train() client id: f_00005-10-1 loss: 0.556363  [   64/  146]
train() client id: f_00005-10-2 loss: 0.730726  [   96/  146]
train() client id: f_00005-10-3 loss: 0.534238  [  128/  146]
train() client id: f_00005-11-0 loss: 0.499180  [   32/  146]
train() client id: f_00005-11-1 loss: 1.108282  [   64/  146]
train() client id: f_00005-11-2 loss: 0.541703  [   96/  146]
train() client id: f_00005-11-3 loss: 0.716673  [  128/  146]
train() client id: f_00006-0-0 loss: 0.588307  [   32/   54]
train() client id: f_00006-1-0 loss: 0.635937  [   32/   54]
train() client id: f_00006-2-0 loss: 0.593093  [   32/   54]
train() client id: f_00006-3-0 loss: 0.599483  [   32/   54]
train() client id: f_00006-4-0 loss: 0.567074  [   32/   54]
train() client id: f_00006-5-0 loss: 0.587976  [   32/   54]
train() client id: f_00006-6-0 loss: 0.625291  [   32/   54]
train() client id: f_00006-7-0 loss: 0.603600  [   32/   54]
train() client id: f_00006-8-0 loss: 0.590305  [   32/   54]
train() client id: f_00006-9-0 loss: 0.632521  [   32/   54]
train() client id: f_00006-10-0 loss: 0.581563  [   32/   54]
train() client id: f_00006-11-0 loss: 0.591367  [   32/   54]
train() client id: f_00007-0-0 loss: 0.534048  [   32/  179]
train() client id: f_00007-0-1 loss: 0.514062  [   64/  179]
train() client id: f_00007-0-2 loss: 0.574436  [   96/  179]
train() client id: f_00007-0-3 loss: 0.732455  [  128/  179]
train() client id: f_00007-0-4 loss: 0.710162  [  160/  179]
train() client id: f_00007-1-0 loss: 0.528284  [   32/  179]
train() client id: f_00007-1-1 loss: 0.708093  [   64/  179]
train() client id: f_00007-1-2 loss: 0.739460  [   96/  179]
train() client id: f_00007-1-3 loss: 0.599123  [  128/  179]
train() client id: f_00007-1-4 loss: 0.496354  [  160/  179]
train() client id: f_00007-2-0 loss: 0.574559  [   32/  179]
train() client id: f_00007-2-1 loss: 0.592102  [   64/  179]
train() client id: f_00007-2-2 loss: 0.470249  [   96/  179]
train() client id: f_00007-2-3 loss: 0.457210  [  128/  179]
train() client id: f_00007-2-4 loss: 0.783534  [  160/  179]
train() client id: f_00007-3-0 loss: 0.579123  [   32/  179]
train() client id: f_00007-3-1 loss: 0.569606  [   64/  179]
train() client id: f_00007-3-2 loss: 0.819772  [   96/  179]
train() client id: f_00007-3-3 loss: 0.563734  [  128/  179]
train() client id: f_00007-3-4 loss: 0.416957  [  160/  179]
train() client id: f_00007-4-0 loss: 0.606460  [   32/  179]
train() client id: f_00007-4-1 loss: 0.631397  [   64/  179]
train() client id: f_00007-4-2 loss: 0.478876  [   96/  179]
train() client id: f_00007-4-3 loss: 0.432760  [  128/  179]
train() client id: f_00007-4-4 loss: 0.534393  [  160/  179]
train() client id: f_00007-5-0 loss: 0.629582  [   32/  179]
train() client id: f_00007-5-1 loss: 0.577968  [   64/  179]
train() client id: f_00007-5-2 loss: 0.564457  [   96/  179]
train() client id: f_00007-5-3 loss: 0.565704  [  128/  179]
train() client id: f_00007-5-4 loss: 0.434988  [  160/  179]
train() client id: f_00007-6-0 loss: 0.426202  [   32/  179]
train() client id: f_00007-6-1 loss: 0.391595  [   64/  179]
train() client id: f_00007-6-2 loss: 0.633449  [   96/  179]
train() client id: f_00007-6-3 loss: 0.451832  [  128/  179]
train() client id: f_00007-6-4 loss: 0.906022  [  160/  179]
train() client id: f_00007-7-0 loss: 0.692532  [   32/  179]
train() client id: f_00007-7-1 loss: 0.414682  [   64/  179]
train() client id: f_00007-7-2 loss: 0.722714  [   96/  179]
train() client id: f_00007-7-3 loss: 0.422084  [  128/  179]
train() client id: f_00007-7-4 loss: 0.520336  [  160/  179]
train() client id: f_00007-8-0 loss: 0.558406  [   32/  179]
train() client id: f_00007-8-1 loss: 0.668211  [   64/  179]
train() client id: f_00007-8-2 loss: 0.595188  [   96/  179]
train() client id: f_00007-8-3 loss: 0.521352  [  128/  179]
train() client id: f_00007-8-4 loss: 0.390335  [  160/  179]
train() client id: f_00007-9-0 loss: 0.696353  [   32/  179]
train() client id: f_00007-9-1 loss: 0.579446  [   64/  179]
train() client id: f_00007-9-2 loss: 0.459375  [   96/  179]
train() client id: f_00007-9-3 loss: 0.526789  [  128/  179]
train() client id: f_00007-9-4 loss: 0.437988  [  160/  179]
train() client id: f_00007-10-0 loss: 0.672984  [   32/  179]
train() client id: f_00007-10-1 loss: 0.397186  [   64/  179]
train() client id: f_00007-10-2 loss: 0.628834  [   96/  179]
train() client id: f_00007-10-3 loss: 0.583120  [  128/  179]
train() client id: f_00007-10-4 loss: 0.534501  [  160/  179]
train() client id: f_00007-11-0 loss: 0.565889  [   32/  179]
train() client id: f_00007-11-1 loss: 0.587950  [   64/  179]
train() client id: f_00007-11-2 loss: 0.499161  [   96/  179]
train() client id: f_00007-11-3 loss: 0.393864  [  128/  179]
train() client id: f_00007-11-4 loss: 0.670882  [  160/  179]
train() client id: f_00008-0-0 loss: 0.769350  [   32/  130]
train() client id: f_00008-0-1 loss: 0.913663  [   64/  130]
train() client id: f_00008-0-2 loss: 0.816794  [   96/  130]
train() client id: f_00008-0-3 loss: 0.820135  [  128/  130]
train() client id: f_00008-1-0 loss: 0.812901  [   32/  130]
train() client id: f_00008-1-1 loss: 0.914048  [   64/  130]
train() client id: f_00008-1-2 loss: 0.750629  [   96/  130]
train() client id: f_00008-1-3 loss: 0.789269  [  128/  130]
train() client id: f_00008-2-0 loss: 0.846057  [   32/  130]
train() client id: f_00008-2-1 loss: 0.804370  [   64/  130]
train() client id: f_00008-2-2 loss: 0.667737  [   96/  130]
train() client id: f_00008-2-3 loss: 0.965425  [  128/  130]
train() client id: f_00008-3-0 loss: 0.788468  [   32/  130]
train() client id: f_00008-3-1 loss: 0.801852  [   64/  130]
train() client id: f_00008-3-2 loss: 0.857391  [   96/  130]
train() client id: f_00008-3-3 loss: 0.837887  [  128/  130]
train() client id: f_00008-4-0 loss: 0.814433  [   32/  130]
train() client id: f_00008-4-1 loss: 0.902742  [   64/  130]
train() client id: f_00008-4-2 loss: 0.862176  [   96/  130]
train() client id: f_00008-4-3 loss: 0.713872  [  128/  130]
train() client id: f_00008-5-0 loss: 0.757621  [   32/  130]
train() client id: f_00008-5-1 loss: 0.808647  [   64/  130]
train() client id: f_00008-5-2 loss: 0.777869  [   96/  130]
train() client id: f_00008-5-3 loss: 0.940710  [  128/  130]
train() client id: f_00008-6-0 loss: 0.834474  [   32/  130]
train() client id: f_00008-6-1 loss: 0.870482  [   64/  130]
train() client id: f_00008-6-2 loss: 0.846604  [   96/  130]
train() client id: f_00008-6-3 loss: 0.712496  [  128/  130]
train() client id: f_00008-7-0 loss: 0.725456  [   32/  130]
train() client id: f_00008-7-1 loss: 0.751765  [   64/  130]
train() client id: f_00008-7-2 loss: 1.026922  [   96/  130]
train() client id: f_00008-7-3 loss: 0.789907  [  128/  130]
train() client id: f_00008-8-0 loss: 0.810521  [   32/  130]
train() client id: f_00008-8-1 loss: 0.766764  [   64/  130]
train() client id: f_00008-8-2 loss: 0.845492  [   96/  130]
train() client id: f_00008-8-3 loss: 0.816893  [  128/  130]
train() client id: f_00008-9-0 loss: 0.868932  [   32/  130]
train() client id: f_00008-9-1 loss: 0.792391  [   64/  130]
train() client id: f_00008-9-2 loss: 0.760823  [   96/  130]
train() client id: f_00008-9-3 loss: 0.857899  [  128/  130]
train() client id: f_00008-10-0 loss: 0.829169  [   32/  130]
train() client id: f_00008-10-1 loss: 0.776218  [   64/  130]
train() client id: f_00008-10-2 loss: 0.780925  [   96/  130]
train() client id: f_00008-10-3 loss: 0.859587  [  128/  130]
train() client id: f_00008-11-0 loss: 0.944820  [   32/  130]
train() client id: f_00008-11-1 loss: 0.738352  [   64/  130]
train() client id: f_00008-11-2 loss: 0.843167  [   96/  130]
train() client id: f_00008-11-3 loss: 0.754809  [  128/  130]
train() client id: f_00009-0-0 loss: 1.051631  [   32/  118]
train() client id: f_00009-0-1 loss: 1.166212  [   64/  118]
train() client id: f_00009-0-2 loss: 1.138332  [   96/  118]
train() client id: f_00009-1-0 loss: 1.029840  [   32/  118]
train() client id: f_00009-1-1 loss: 1.048438  [   64/  118]
train() client id: f_00009-1-2 loss: 0.962105  [   96/  118]
train() client id: f_00009-2-0 loss: 1.014653  [   32/  118]
train() client id: f_00009-2-1 loss: 1.019817  [   64/  118]
train() client id: f_00009-2-2 loss: 0.980665  [   96/  118]
train() client id: f_00009-3-0 loss: 1.031423  [   32/  118]
train() client id: f_00009-3-1 loss: 0.954571  [   64/  118]
train() client id: f_00009-3-2 loss: 0.820444  [   96/  118]
train() client id: f_00009-4-0 loss: 1.034409  [   32/  118]
train() client id: f_00009-4-1 loss: 0.819382  [   64/  118]
train() client id: f_00009-4-2 loss: 0.948240  [   96/  118]
train() client id: f_00009-5-0 loss: 0.881655  [   32/  118]
train() client id: f_00009-5-1 loss: 0.897993  [   64/  118]
train() client id: f_00009-5-2 loss: 0.991510  [   96/  118]
train() client id: f_00009-6-0 loss: 0.831739  [   32/  118]
train() client id: f_00009-6-1 loss: 0.835357  [   64/  118]
train() client id: f_00009-6-2 loss: 0.900026  [   96/  118]
train() client id: f_00009-7-0 loss: 0.877506  [   32/  118]
train() client id: f_00009-7-1 loss: 0.888333  [   64/  118]
train() client id: f_00009-7-2 loss: 0.828337  [   96/  118]
train() client id: f_00009-8-0 loss: 0.832666  [   32/  118]
train() client id: f_00009-8-1 loss: 0.941269  [   64/  118]
train() client id: f_00009-8-2 loss: 0.829233  [   96/  118]
train() client id: f_00009-9-0 loss: 1.013941  [   32/  118]
train() client id: f_00009-9-1 loss: 0.879475  [   64/  118]
train() client id: f_00009-9-2 loss: 0.778803  [   96/  118]
train() client id: f_00009-10-0 loss: 0.818642  [   32/  118]
train() client id: f_00009-10-1 loss: 0.733842  [   64/  118]
train() client id: f_00009-10-2 loss: 0.971261  [   96/  118]
train() client id: f_00009-11-0 loss: 0.761116  [   32/  118]
train() client id: f_00009-11-1 loss: 0.924887  [   64/  118]
train() client id: f_00009-11-2 loss: 0.875298  [   96/  118]
At round 9 accuracy: 0.6286472148541115
At round 9 training accuracy: 0.5949027498323273
At round 9 training loss: 0.8265919666083539
update_location
xs = -3.905658 4.200318 65.009024 18.811294 0.979296 3.956410 -27.443192 -6.324852 49.663977 2.939121 
ys = 57.587959 40.555839 1.320614 -27.455176 19.350187 2.814151 -2.624984 0.822348 17.569006 -0.998518 
xs mean: 10.788573831882571
ys mean: 10.894142535528704
dists_uav = 115.462666 107.992679 119.280833 105.392844 101.859652 100.117794 103.730513 100.203194 113.027344 100.048166 
uav_gains = -101.561164 -100.834906 -101.914463 -100.570312 -100.200075 -100.012798 -100.397690 -100.022055 -101.329680 -100.005244 
uav_gains_db_mean: -100.68483877521386
dists_bs = 207.506291 224.026754 296.258098 280.269363 234.937390 248.341224 230.919134 242.464760 274.332682 250.275467 
bs_gains = -104.443935 -105.375456 -108.773826 -108.099177 -105.953719 -106.628426 -105.743937 -106.337221 -107.838831 -106.722771 
bs_gains_db_mean: -106.59172971561992
Round 10
-------------------------------
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.67144915 20.18897006  9.52328793  3.40449035 23.28757639 11.22831614
  4.23293599 13.65794809 10.03742584  9.11771254]
obj_prev = 114.35011248363482
eta_min = 2.485131023397881e-10	eta_max = 0.9203875705067922
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 26.5917511352464	eta = 0.909090909090909
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 46.02197555438805	eta = 0.5252777379209097
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 36.745409717816976	eta = 0.6578867782263103
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 35.08220734696584	eta = 0.6890763450194114
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 35.00007514490989	eta = 0.6906933517648766
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 34.99986062549486	eta = 0.690697585128414
eta = 0.690697585128414
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [0.0305359  0.06422234 0.03005121 0.01042098 0.07415862 0.03538287
 0.01308682 0.04338037 0.0315053  0.02859712]
ene_total = [2.98353072 5.7890883  2.95330941 1.35079969 6.60483527 3.5266156
 1.5604156  3.97716561 3.26691924 2.98718119]
ti_comp = [0.30116936 0.28752245 0.30009808 0.30400477 0.28497028 0.28178895
 0.30447482 0.30547464 0.27545826 0.28132541]
ti_coms = [0.06738721 0.08103412 0.06845849 0.0645518  0.08358629 0.08676762
 0.06408176 0.06308193 0.09309831 0.08723116]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.96196272e-05 2.00260269e-04 1.88338590e-05 7.65323874e-07
 3.13881583e-04 3.48667396e-05 1.51105132e-06 5.46776000e-05
 2.57584149e-05 1.84683789e-05]
ene_total = [0.54095026 0.66464151 0.5494621  0.51674598 0.69416402 0.6972951
 0.51304332 0.50929606 0.74723805 0.69969276]
optimize_network iter = 0 obj = 6.132529161591935
eta = 0.690697585128414
freqs = [5.06955673e+07 1.11682299e+08 5.00689748e+07 1.71395055e+07
 1.30116415e+08 6.27825752e+07 2.14908014e+07 7.10048625e+07
 5.71870607e+07 5.08256882e+07]
eta_min = 0.6906975851284028	eta_max = 0.6906975851284034
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 0.050678425323249086	eta = 0.9090909090909091
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 20.72563143256374	eta = 0.002222913974819694
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.1660187348301907	eta = 0.02127003566846808
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.101859368191834	eta = 0.021919304614581324
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.101838286505997	eta = 0.02191952446779107
eta = 0.02191952446779107
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [2.02550289e-04 2.06745903e-03 1.94438128e-04 7.90109671e-06
 3.24046959e-03 3.59959346e-04 1.55998826e-05 5.64483901e-04
 2.65926276e-04 1.90664962e-04]
ene_total = [0.17569726 0.2574448  0.17820355 0.16359333 0.29359605 0.22873723
 0.16259842 0.17396099 0.24238129 0.22562536]
ti_comp = [0.30116936 0.28752245 0.30009808 0.30400477 0.28497028 0.28178895
 0.30447482 0.30547464 0.27545826 0.28132541]
ti_coms = [0.06738721 0.08103412 0.06845849 0.0645518  0.08358629 0.08676762
 0.06408176 0.06308193 0.09309831 0.08723116]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.96196272e-05 2.00260269e-04 1.88338590e-05 7.65323874e-07
 3.13881583e-04 3.48667396e-05 1.51105132e-06 5.46776000e-05
 2.57584149e-05 1.84683789e-05]
ene_total = [0.54095026 0.66464151 0.5494621  0.51674598 0.69416402 0.6972951
 0.51304332 0.50929606 0.74723805 0.69969276]
optimize_network iter = 1 obj = 6.132529161591719
eta = 0.6906975851284028
freqs = [5.06955673e+07 1.11682299e+08 5.00689748e+07 1.71395055e+07
 1.30116415e+08 6.27825752e+07 2.14908014e+07 7.10048625e+07
 5.71870607e+07 5.08256882e+07]
Done!
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.94295065e-05 1.98319681e-04 1.86513526e-05 7.57907632e-07
 3.10839966e-04 3.45288693e-05 1.49640873e-06 5.41477560e-05
 2.55088073e-05 1.82894141e-05]
ene_total = [0.00675815 0.00830173 0.0068645  0.00645594 0.00866947 0.00871129
 0.00640967 0.00636234 0.00933534 0.00874141]
At round 10 energy consumption: 0.07660983999268792
At round 10 eta: 0.6906975851284028
At round 10 a_n: 24.7571443865612
At round 10 local rounds: 12.11742183622639
At round 10 global rounds: 80.0418722784263
gradient difference: 0.36123842000961304
train() client id: f_00000-0-0 loss: 0.986731  [   32/  126]
train() client id: f_00000-0-1 loss: 1.192775  [   64/  126]
train() client id: f_00000-0-2 loss: 1.070884  [   96/  126]
train() client id: f_00000-1-0 loss: 1.179106  [   32/  126]
train() client id: f_00000-1-1 loss: 0.877612  [   64/  126]
train() client id: f_00000-1-2 loss: 1.092474  [   96/  126]
train() client id: f_00000-2-0 loss: 0.902950  [   32/  126]
train() client id: f_00000-2-1 loss: 0.845016  [   64/  126]
train() client id: f_00000-2-2 loss: 1.035290  [   96/  126]
train() client id: f_00000-3-0 loss: 0.843568  [   32/  126]
train() client id: f_00000-3-1 loss: 0.910943  [   64/  126]
train() client id: f_00000-3-2 loss: 0.879679  [   96/  126]
train() client id: f_00000-4-0 loss: 0.907377  [   32/  126]
train() client id: f_00000-4-1 loss: 0.850288  [   64/  126]
train() client id: f_00000-4-2 loss: 0.826309  [   96/  126]
train() client id: f_00000-5-0 loss: 0.796694  [   32/  126]
train() client id: f_00000-5-1 loss: 0.834548  [   64/  126]
train() client id: f_00000-5-2 loss: 0.745691  [   96/  126]
train() client id: f_00000-6-0 loss: 0.714285  [   32/  126]
train() client id: f_00000-6-1 loss: 0.836464  [   64/  126]
train() client id: f_00000-6-2 loss: 0.788774  [   96/  126]
train() client id: f_00000-7-0 loss: 0.693982  [   32/  126]
train() client id: f_00000-7-1 loss: 0.855296  [   64/  126]
train() client id: f_00000-7-2 loss: 0.779927  [   96/  126]
train() client id: f_00000-8-0 loss: 0.873999  [   32/  126]
train() client id: f_00000-8-1 loss: 0.681171  [   64/  126]
train() client id: f_00000-8-2 loss: 0.756958  [   96/  126]
train() client id: f_00000-9-0 loss: 0.712368  [   32/  126]
train() client id: f_00000-9-1 loss: 0.724670  [   64/  126]
train() client id: f_00000-9-2 loss: 0.792450  [   96/  126]
train() client id: f_00000-10-0 loss: 0.642445  [   32/  126]
train() client id: f_00000-10-1 loss: 0.762279  [   64/  126]
train() client id: f_00000-10-2 loss: 0.822448  [   96/  126]
train() client id: f_00000-11-0 loss: 0.751523  [   32/  126]
train() client id: f_00000-11-1 loss: 0.552996  [   64/  126]
train() client id: f_00000-11-2 loss: 0.815530  [   96/  126]
train() client id: f_00001-0-0 loss: 0.481696  [   32/  265]
train() client id: f_00001-0-1 loss: 0.474695  [   64/  265]
train() client id: f_00001-0-2 loss: 0.582249  [   96/  265]
train() client id: f_00001-0-3 loss: 0.547714  [  128/  265]
train() client id: f_00001-0-4 loss: 0.567269  [  160/  265]
train() client id: f_00001-0-5 loss: 0.503694  [  192/  265]
train() client id: f_00001-0-6 loss: 0.521905  [  224/  265]
train() client id: f_00001-0-7 loss: 0.507678  [  256/  265]
train() client id: f_00001-1-0 loss: 0.577511  [   32/  265]
train() client id: f_00001-1-1 loss: 0.441284  [   64/  265]
train() client id: f_00001-1-2 loss: 0.496935  [   96/  265]
train() client id: f_00001-1-3 loss: 0.500974  [  128/  265]
train() client id: f_00001-1-4 loss: 0.591205  [  160/  265]
train() client id: f_00001-1-5 loss: 0.535000  [  192/  265]
train() client id: f_00001-1-6 loss: 0.472756  [  224/  265]
train() client id: f_00001-1-7 loss: 0.490706  [  256/  265]
train() client id: f_00001-2-0 loss: 0.536207  [   32/  265]
train() client id: f_00001-2-1 loss: 0.557874  [   64/  265]
train() client id: f_00001-2-2 loss: 0.519683  [   96/  265]
train() client id: f_00001-2-3 loss: 0.529718  [  128/  265]
train() client id: f_00001-2-4 loss: 0.454350  [  160/  265]
train() client id: f_00001-2-5 loss: 0.545893  [  192/  265]
train() client id: f_00001-2-6 loss: 0.422425  [  224/  265]
train() client id: f_00001-2-7 loss: 0.473269  [  256/  265]
train() client id: f_00001-3-0 loss: 0.561399  [   32/  265]
train() client id: f_00001-3-1 loss: 0.466823  [   64/  265]
train() client id: f_00001-3-2 loss: 0.525866  [   96/  265]
train() client id: f_00001-3-3 loss: 0.537358  [  128/  265]
train() client id: f_00001-3-4 loss: 0.479763  [  160/  265]
train() client id: f_00001-3-5 loss: 0.415954  [  192/  265]
train() client id: f_00001-3-6 loss: 0.509392  [  224/  265]
train() client id: f_00001-3-7 loss: 0.497865  [  256/  265]
train() client id: f_00001-4-0 loss: 0.505915  [   32/  265]
train() client id: f_00001-4-1 loss: 0.524645  [   64/  265]
train() client id: f_00001-4-2 loss: 0.423578  [   96/  265]
train() client id: f_00001-4-3 loss: 0.551446  [  128/  265]
train() client id: f_00001-4-4 loss: 0.520764  [  160/  265]
train() client id: f_00001-4-5 loss: 0.408162  [  192/  265]
train() client id: f_00001-4-6 loss: 0.473130  [  224/  265]
train() client id: f_00001-4-7 loss: 0.505337  [  256/  265]
train() client id: f_00001-5-0 loss: 0.553755  [   32/  265]
train() client id: f_00001-5-1 loss: 0.472739  [   64/  265]
train() client id: f_00001-5-2 loss: 0.419057  [   96/  265]
train() client id: f_00001-5-3 loss: 0.495505  [  128/  265]
train() client id: f_00001-5-4 loss: 0.498160  [  160/  265]
train() client id: f_00001-5-5 loss: 0.430312  [  192/  265]
train() client id: f_00001-5-6 loss: 0.510031  [  224/  265]
train() client id: f_00001-5-7 loss: 0.551488  [  256/  265]
train() client id: f_00001-6-0 loss: 0.424447  [   32/  265]
train() client id: f_00001-6-1 loss: 0.401597  [   64/  265]
train() client id: f_00001-6-2 loss: 0.523709  [   96/  265]
train() client id: f_00001-6-3 loss: 0.421573  [  128/  265]
train() client id: f_00001-6-4 loss: 0.463763  [  160/  265]
train() client id: f_00001-6-5 loss: 0.513753  [  192/  265]
train() client id: f_00001-6-6 loss: 0.517000  [  224/  265]
train() client id: f_00001-6-7 loss: 0.584653  [  256/  265]
train() client id: f_00001-7-0 loss: 0.531812  [   32/  265]
train() client id: f_00001-7-1 loss: 0.510349  [   64/  265]
train() client id: f_00001-7-2 loss: 0.483904  [   96/  265]
train() client id: f_00001-7-3 loss: 0.409046  [  128/  265]
train() client id: f_00001-7-4 loss: 0.669206  [  160/  265]
train() client id: f_00001-7-5 loss: 0.470989  [  192/  265]
train() client id: f_00001-7-6 loss: 0.403470  [  224/  265]
train() client id: f_00001-7-7 loss: 0.434930  [  256/  265]
train() client id: f_00001-8-0 loss: 0.473002  [   32/  265]
train() client id: f_00001-8-1 loss: 0.475086  [   64/  265]
train() client id: f_00001-8-2 loss: 0.448741  [   96/  265]
train() client id: f_00001-8-3 loss: 0.641241  [  128/  265]
train() client id: f_00001-8-4 loss: 0.367369  [  160/  265]
train() client id: f_00001-8-5 loss: 0.605566  [  192/  265]
train() client id: f_00001-8-6 loss: 0.476126  [  224/  265]
train() client id: f_00001-8-7 loss: 0.423801  [  256/  265]
train() client id: f_00001-9-0 loss: 0.387274  [   32/  265]
train() client id: f_00001-9-1 loss: 0.532998  [   64/  265]
train() client id: f_00001-9-2 loss: 0.405532  [   96/  265]
train() client id: f_00001-9-3 loss: 0.568973  [  128/  265]
train() client id: f_00001-9-4 loss: 0.494425  [  160/  265]
train() client id: f_00001-9-5 loss: 0.421467  [  192/  265]
train() client id: f_00001-9-6 loss: 0.550223  [  224/  265]
train() client id: f_00001-9-7 loss: 0.465919  [  256/  265]
train() client id: f_00001-10-0 loss: 0.507564  [   32/  265]
train() client id: f_00001-10-1 loss: 0.413414  [   64/  265]
train() client id: f_00001-10-2 loss: 0.476281  [   96/  265]
train() client id: f_00001-10-3 loss: 0.452857  [  128/  265]
train() client id: f_00001-10-4 loss: 0.402934  [  160/  265]
train() client id: f_00001-10-5 loss: 0.466546  [  192/  265]
train() client id: f_00001-10-6 loss: 0.595688  [  224/  265]
train() client id: f_00001-10-7 loss: 0.533247  [  256/  265]
train() client id: f_00001-11-0 loss: 0.376891  [   32/  265]
train() client id: f_00001-11-1 loss: 0.497468  [   64/  265]
train() client id: f_00001-11-2 loss: 0.502486  [   96/  265]
train() client id: f_00001-11-3 loss: 0.536850  [  128/  265]
train() client id: f_00001-11-4 loss: 0.442031  [  160/  265]
train() client id: f_00001-11-5 loss: 0.446877  [  192/  265]
train() client id: f_00001-11-6 loss: 0.632156  [  224/  265]
train() client id: f_00001-11-7 loss: 0.477247  [  256/  265]
train() client id: f_00002-0-0 loss: 1.114316  [   32/  124]
train() client id: f_00002-0-1 loss: 1.154354  [   64/  124]
train() client id: f_00002-0-2 loss: 0.979043  [   96/  124]
train() client id: f_00002-1-0 loss: 1.005429  [   32/  124]
train() client id: f_00002-1-1 loss: 1.061050  [   64/  124]
train() client id: f_00002-1-2 loss: 1.117214  [   96/  124]
train() client id: f_00002-2-0 loss: 0.929612  [   32/  124]
train() client id: f_00002-2-1 loss: 0.901854  [   64/  124]
train() client id: f_00002-2-2 loss: 1.194848  [   96/  124]
train() client id: f_00002-3-0 loss: 0.951836  [   32/  124]
train() client id: f_00002-3-1 loss: 1.118403  [   64/  124]
train() client id: f_00002-3-2 loss: 0.890313  [   96/  124]
train() client id: f_00002-4-0 loss: 0.956758  [   32/  124]
train() client id: f_00002-4-1 loss: 1.031306  [   64/  124]
train() client id: f_00002-4-2 loss: 0.941354  [   96/  124]
train() client id: f_00002-5-0 loss: 0.970632  [   32/  124]
train() client id: f_00002-5-1 loss: 1.068503  [   64/  124]
train() client id: f_00002-5-2 loss: 0.803027  [   96/  124]
train() client id: f_00002-6-0 loss: 0.989453  [   32/  124]
train() client id: f_00002-6-1 loss: 0.774475  [   64/  124]
train() client id: f_00002-6-2 loss: 0.901662  [   96/  124]
train() client id: f_00002-7-0 loss: 0.888176  [   32/  124]
train() client id: f_00002-7-1 loss: 0.906313  [   64/  124]
train() client id: f_00002-7-2 loss: 0.950733  [   96/  124]
train() client id: f_00002-8-0 loss: 0.924906  [   32/  124]
train() client id: f_00002-8-1 loss: 0.918764  [   64/  124]
train() client id: f_00002-8-2 loss: 0.803328  [   96/  124]
train() client id: f_00002-9-0 loss: 0.972202  [   32/  124]
train() client id: f_00002-9-1 loss: 0.762575  [   64/  124]
train() client id: f_00002-9-2 loss: 1.007874  [   96/  124]
train() client id: f_00002-10-0 loss: 0.819280  [   32/  124]
train() client id: f_00002-10-1 loss: 0.921459  [   64/  124]
train() client id: f_00002-10-2 loss: 0.911495  [   96/  124]
train() client id: f_00002-11-0 loss: 1.150132  [   32/  124]
train() client id: f_00002-11-1 loss: 0.772886  [   64/  124]
train() client id: f_00002-11-2 loss: 0.737839  [   96/  124]
train() client id: f_00003-0-0 loss: 0.931644  [   32/   43]
train() client id: f_00003-1-0 loss: 1.059350  [   32/   43]
train() client id: f_00003-2-0 loss: 0.993050  [   32/   43]
train() client id: f_00003-3-0 loss: 0.883964  [   32/   43]
train() client id: f_00003-4-0 loss: 0.884298  [   32/   43]
train() client id: f_00003-5-0 loss: 0.876013  [   32/   43]
train() client id: f_00003-6-0 loss: 1.036210  [   32/   43]
train() client id: f_00003-7-0 loss: 0.972681  [   32/   43]
train() client id: f_00003-8-0 loss: 0.889969  [   32/   43]
train() client id: f_00003-9-0 loss: 1.030347  [   32/   43]
train() client id: f_00003-10-0 loss: 0.869963  [   32/   43]
train() client id: f_00003-11-0 loss: 0.988942  [   32/   43]
train() client id: f_00004-0-0 loss: 0.910802  [   32/  306]
train() client id: f_00004-0-1 loss: 0.890497  [   64/  306]
train() client id: f_00004-0-2 loss: 0.926499  [   96/  306]
train() client id: f_00004-0-3 loss: 0.899319  [  128/  306]
train() client id: f_00004-0-4 loss: 0.805251  [  160/  306]
train() client id: f_00004-0-5 loss: 0.927228  [  192/  306]
train() client id: f_00004-0-6 loss: 0.941856  [  224/  306]
train() client id: f_00004-0-7 loss: 0.925814  [  256/  306]
train() client id: f_00004-0-8 loss: 0.880221  [  288/  306]
train() client id: f_00004-1-0 loss: 0.900733  [   32/  306]
train() client id: f_00004-1-1 loss: 0.887063  [   64/  306]
train() client id: f_00004-1-2 loss: 0.816525  [   96/  306]
train() client id: f_00004-1-3 loss: 0.945045  [  128/  306]
train() client id: f_00004-1-4 loss: 0.850038  [  160/  306]
train() client id: f_00004-1-5 loss: 0.853373  [  192/  306]
train() client id: f_00004-1-6 loss: 0.842889  [  224/  306]
train() client id: f_00004-1-7 loss: 0.934881  [  256/  306]
train() client id: f_00004-1-8 loss: 0.994678  [  288/  306]
train() client id: f_00004-2-0 loss: 0.943609  [   32/  306]
train() client id: f_00004-2-1 loss: 0.877164  [   64/  306]
train() client id: f_00004-2-2 loss: 0.986577  [   96/  306]
train() client id: f_00004-2-3 loss: 1.000675  [  128/  306]
train() client id: f_00004-2-4 loss: 0.762552  [  160/  306]
train() client id: f_00004-2-5 loss: 0.889999  [  192/  306]
train() client id: f_00004-2-6 loss: 0.779593  [  224/  306]
train() client id: f_00004-2-7 loss: 0.888528  [  256/  306]
train() client id: f_00004-2-8 loss: 0.974604  [  288/  306]
train() client id: f_00004-3-0 loss: 0.840917  [   32/  306]
train() client id: f_00004-3-1 loss: 0.925784  [   64/  306]
train() client id: f_00004-3-2 loss: 0.950398  [   96/  306]
train() client id: f_00004-3-3 loss: 0.876066  [  128/  306]
train() client id: f_00004-3-4 loss: 0.859979  [  160/  306]
train() client id: f_00004-3-5 loss: 0.868772  [  192/  306]
train() client id: f_00004-3-6 loss: 0.846936  [  224/  306]
train() client id: f_00004-3-7 loss: 0.960708  [  256/  306]
train() client id: f_00004-3-8 loss: 0.930092  [  288/  306]
train() client id: f_00004-4-0 loss: 0.859699  [   32/  306]
train() client id: f_00004-4-1 loss: 0.961239  [   64/  306]
train() client id: f_00004-4-2 loss: 0.907802  [   96/  306]
train() client id: f_00004-4-3 loss: 0.877457  [  128/  306]
train() client id: f_00004-4-4 loss: 0.815059  [  160/  306]
train() client id: f_00004-4-5 loss: 0.847117  [  192/  306]
train() client id: f_00004-4-6 loss: 0.899557  [  224/  306]
train() client id: f_00004-4-7 loss: 0.970095  [  256/  306]
train() client id: f_00004-4-8 loss: 0.918717  [  288/  306]
train() client id: f_00004-5-0 loss: 0.938405  [   32/  306]
train() client id: f_00004-5-1 loss: 0.898017  [   64/  306]
train() client id: f_00004-5-2 loss: 0.836116  [   96/  306]
train() client id: f_00004-5-3 loss: 0.823668  [  128/  306]
train() client id: f_00004-5-4 loss: 1.025862  [  160/  306]
train() client id: f_00004-5-5 loss: 0.872531  [  192/  306]
train() client id: f_00004-5-6 loss: 0.825918  [  224/  306]
train() client id: f_00004-5-7 loss: 0.944774  [  256/  306]
train() client id: f_00004-5-8 loss: 0.911184  [  288/  306]
train() client id: f_00004-6-0 loss: 1.025486  [   32/  306]
train() client id: f_00004-6-1 loss: 0.820522  [   64/  306]
train() client id: f_00004-6-2 loss: 0.948456  [   96/  306]
train() client id: f_00004-6-3 loss: 0.886637  [  128/  306]
train() client id: f_00004-6-4 loss: 0.785362  [  160/  306]
train() client id: f_00004-6-5 loss: 0.824752  [  192/  306]
train() client id: f_00004-6-6 loss: 0.861364  [  224/  306]
train() client id: f_00004-6-7 loss: 1.022040  [  256/  306]
train() client id: f_00004-6-8 loss: 0.909762  [  288/  306]
train() client id: f_00004-7-0 loss: 0.946268  [   32/  306]
train() client id: f_00004-7-1 loss: 0.920295  [   64/  306]
train() client id: f_00004-7-2 loss: 0.868101  [   96/  306]
train() client id: f_00004-7-3 loss: 0.885179  [  128/  306]
train() client id: f_00004-7-4 loss: 0.849503  [  160/  306]
train() client id: f_00004-7-5 loss: 0.866268  [  192/  306]
train() client id: f_00004-7-6 loss: 0.800970  [  224/  306]
train() client id: f_00004-7-7 loss: 1.019976  [  256/  306]
train() client id: f_00004-7-8 loss: 0.905104  [  288/  306]
train() client id: f_00004-8-0 loss: 0.874926  [   32/  306]
train() client id: f_00004-8-1 loss: 0.956201  [   64/  306]
train() client id: f_00004-8-2 loss: 0.782759  [   96/  306]
train() client id: f_00004-8-3 loss: 0.891347  [  128/  306]
train() client id: f_00004-8-4 loss: 0.922994  [  160/  306]
train() client id: f_00004-8-5 loss: 0.924319  [  192/  306]
train() client id: f_00004-8-6 loss: 1.003730  [  224/  306]
train() client id: f_00004-8-7 loss: 0.810753  [  256/  306]
train() client id: f_00004-8-8 loss: 0.891877  [  288/  306]
train() client id: f_00004-9-0 loss: 0.922150  [   32/  306]
train() client id: f_00004-9-1 loss: 0.887574  [   64/  306]
train() client id: f_00004-9-2 loss: 0.810057  [   96/  306]
train() client id: f_00004-9-3 loss: 0.979047  [  128/  306]
train() client id: f_00004-9-4 loss: 0.812791  [  160/  306]
train() client id: f_00004-9-5 loss: 0.892118  [  192/  306]
train() client id: f_00004-9-6 loss: 0.890392  [  224/  306]
train() client id: f_00004-9-7 loss: 0.968619  [  256/  306]
train() client id: f_00004-9-8 loss: 0.875142  [  288/  306]
train() client id: f_00004-10-0 loss: 0.978661  [   32/  306]
train() client id: f_00004-10-1 loss: 0.860316  [   64/  306]
train() client id: f_00004-10-2 loss: 0.792875  [   96/  306]
train() client id: f_00004-10-3 loss: 0.831813  [  128/  306]
train() client id: f_00004-10-4 loss: 0.896435  [  160/  306]
train() client id: f_00004-10-5 loss: 0.941414  [  192/  306]
train() client id: f_00004-10-6 loss: 0.879144  [  224/  306]
train() client id: f_00004-10-7 loss: 0.967252  [  256/  306]
train() client id: f_00004-10-8 loss: 0.920303  [  288/  306]
train() client id: f_00004-11-0 loss: 0.936767  [   32/  306]
train() client id: f_00004-11-1 loss: 0.984695  [   64/  306]
train() client id: f_00004-11-2 loss: 0.885999  [   96/  306]
train() client id: f_00004-11-3 loss: 1.021292  [  128/  306]
train() client id: f_00004-11-4 loss: 0.889776  [  160/  306]
train() client id: f_00004-11-5 loss: 0.885624  [  192/  306]
train() client id: f_00004-11-6 loss: 0.907060  [  224/  306]
train() client id: f_00004-11-7 loss: 0.856121  [  256/  306]
train() client id: f_00004-11-8 loss: 0.836829  [  288/  306]
train() client id: f_00005-0-0 loss: 0.604274  [   32/  146]
train() client id: f_00005-0-1 loss: 0.566460  [   64/  146]
train() client id: f_00005-0-2 loss: 0.753068  [   96/  146]
train() client id: f_00005-0-3 loss: 0.764310  [  128/  146]
train() client id: f_00005-1-0 loss: 0.839651  [   32/  146]
train() client id: f_00005-1-1 loss: 0.451114  [   64/  146]
train() client id: f_00005-1-2 loss: 0.715525  [   96/  146]
train() client id: f_00005-1-3 loss: 0.685648  [  128/  146]
train() client id: f_00005-2-0 loss: 0.620922  [   32/  146]
train() client id: f_00005-2-1 loss: 0.551853  [   64/  146]
train() client id: f_00005-2-2 loss: 0.549548  [   96/  146]
train() client id: f_00005-2-3 loss: 0.941398  [  128/  146]
train() client id: f_00005-3-0 loss: 0.867980  [   32/  146]
train() client id: f_00005-3-1 loss: 0.689224  [   64/  146]
train() client id: f_00005-3-2 loss: 0.697644  [   96/  146]
train() client id: f_00005-3-3 loss: 0.597966  [  128/  146]
train() client id: f_00005-4-0 loss: 0.656344  [   32/  146]
train() client id: f_00005-4-1 loss: 0.804723  [   64/  146]
train() client id: f_00005-4-2 loss: 0.760980  [   96/  146]
train() client id: f_00005-4-3 loss: 0.620503  [  128/  146]
train() client id: f_00005-5-0 loss: 0.631188  [   32/  146]
train() client id: f_00005-5-1 loss: 0.725176  [   64/  146]
train() client id: f_00005-5-2 loss: 0.745313  [   96/  146]
train() client id: f_00005-5-3 loss: 0.578096  [  128/  146]
train() client id: f_00005-6-0 loss: 0.678813  [   32/  146]
train() client id: f_00005-6-1 loss: 0.505060  [   64/  146]
train() client id: f_00005-6-2 loss: 0.845757  [   96/  146]
train() client id: f_00005-6-3 loss: 0.539558  [  128/  146]
train() client id: f_00005-7-0 loss: 0.917350  [   32/  146]
train() client id: f_00005-7-1 loss: 0.542048  [   64/  146]
train() client id: f_00005-7-2 loss: 0.512479  [   96/  146]
train() client id: f_00005-7-3 loss: 0.732639  [  128/  146]
train() client id: f_00005-8-0 loss: 0.669632  [   32/  146]
train() client id: f_00005-8-1 loss: 0.758615  [   64/  146]
train() client id: f_00005-8-2 loss: 0.518978  [   96/  146]
train() client id: f_00005-8-3 loss: 0.834054  [  128/  146]
train() client id: f_00005-9-0 loss: 0.586201  [   32/  146]
train() client id: f_00005-9-1 loss: 0.674992  [   64/  146]
train() client id: f_00005-9-2 loss: 0.558103  [   96/  146]
train() client id: f_00005-9-3 loss: 0.692342  [  128/  146]
train() client id: f_00005-10-0 loss: 0.655455  [   32/  146]
train() client id: f_00005-10-1 loss: 0.556829  [   64/  146]
train() client id: f_00005-10-2 loss: 0.662747  [   96/  146]
train() client id: f_00005-10-3 loss: 0.786389  [  128/  146]
train() client id: f_00005-11-0 loss: 0.595698  [   32/  146]
train() client id: f_00005-11-1 loss: 0.710398  [   64/  146]
train() client id: f_00005-11-2 loss: 0.579470  [   96/  146]
train() client id: f_00005-11-3 loss: 0.720195  [  128/  146]
train() client id: f_00006-0-0 loss: 0.641195  [   32/   54]
train() client id: f_00006-1-0 loss: 0.579792  [   32/   54]
train() client id: f_00006-2-0 loss: 0.623567  [   32/   54]
train() client id: f_00006-3-0 loss: 0.588443  [   32/   54]
train() client id: f_00006-4-0 loss: 0.588495  [   32/   54]
train() client id: f_00006-5-0 loss: 0.596175  [   32/   54]
train() client id: f_00006-6-0 loss: 0.582637  [   32/   54]
train() client id: f_00006-7-0 loss: 0.593874  [   32/   54]
train() client id: f_00006-8-0 loss: 0.632316  [   32/   54]
train() client id: f_00006-9-0 loss: 0.631642  [   32/   54]
train() client id: f_00006-10-0 loss: 0.645271  [   32/   54]
train() client id: f_00006-11-0 loss: 0.590140  [   32/   54]
train() client id: f_00007-0-0 loss: 0.411918  [   32/  179]
train() client id: f_00007-0-1 loss: 0.605815  [   64/  179]
train() client id: f_00007-0-2 loss: 0.418974  [   96/  179]
train() client id: f_00007-0-3 loss: 0.777609  [  128/  179]
train() client id: f_00007-0-4 loss: 0.400446  [  160/  179]
train() client id: f_00007-1-0 loss: 0.595917  [   32/  179]
train() client id: f_00007-1-1 loss: 0.435615  [   64/  179]
train() client id: f_00007-1-2 loss: 0.437384  [   96/  179]
train() client id: f_00007-1-3 loss: 0.434157  [  128/  179]
train() client id: f_00007-1-4 loss: 0.433652  [  160/  179]
train() client id: f_00007-2-0 loss: 0.360147  [   32/  179]
train() client id: f_00007-2-1 loss: 0.589489  [   64/  179]
train() client id: f_00007-2-2 loss: 0.440152  [   96/  179]
train() client id: f_00007-2-3 loss: 0.489434  [  128/  179]
train() client id: f_00007-2-4 loss: 0.608090  [  160/  179]
train() client id: f_00007-3-0 loss: 0.531653  [   32/  179]
train() client id: f_00007-3-1 loss: 0.436127  [   64/  179]
train() client id: f_00007-3-2 loss: 0.524065  [   96/  179]
train() client id: f_00007-3-3 loss: 0.528526  [  128/  179]
train() client id: f_00007-3-4 loss: 0.332955  [  160/  179]
train() client id: f_00007-4-0 loss: 0.516255  [   32/  179]
train() client id: f_00007-4-1 loss: 0.357110  [   64/  179]
train() client id: f_00007-4-2 loss: 0.519355  [   96/  179]
train() client id: f_00007-4-3 loss: 0.546732  [  128/  179]
train() client id: f_00007-4-4 loss: 0.351122  [  160/  179]
train() client id: f_00007-5-0 loss: 0.466805  [   32/  179]
train() client id: f_00007-5-1 loss: 0.457331  [   64/  179]
train() client id: f_00007-5-2 loss: 0.430498  [   96/  179]
train() client id: f_00007-5-3 loss: 0.401486  [  128/  179]
train() client id: f_00007-5-4 loss: 0.569189  [  160/  179]
train() client id: f_00007-6-0 loss: 0.451318  [   32/  179]
train() client id: f_00007-6-1 loss: 0.686459  [   64/  179]
train() client id: f_00007-6-2 loss: 0.339441  [   96/  179]
train() client id: f_00007-6-3 loss: 0.543050  [  128/  179]
train() client id: f_00007-6-4 loss: 0.296196  [  160/  179]
train() client id: f_00007-7-0 loss: 0.358850  [   32/  179]
train() client id: f_00007-7-1 loss: 0.364644  [   64/  179]
train() client id: f_00007-7-2 loss: 0.416806  [   96/  179]
train() client id: f_00007-7-3 loss: 0.378769  [  128/  179]
train() client id: f_00007-7-4 loss: 0.649575  [  160/  179]
train() client id: f_00007-8-0 loss: 0.449294  [   32/  179]
train() client id: f_00007-8-1 loss: 0.368199  [   64/  179]
train() client id: f_00007-8-2 loss: 0.295516  [   96/  179]
train() client id: f_00007-8-3 loss: 0.619075  [  128/  179]
train() client id: f_00007-8-4 loss: 0.371770  [  160/  179]
train() client id: f_00007-9-0 loss: 0.449630  [   32/  179]
train() client id: f_00007-9-1 loss: 0.636844  [   64/  179]
train() client id: f_00007-9-2 loss: 0.269304  [   96/  179]
train() client id: f_00007-9-3 loss: 0.406703  [  128/  179]
train() client id: f_00007-9-4 loss: 0.377665  [  160/  179]
train() client id: f_00007-10-0 loss: 0.278904  [   32/  179]
train() client id: f_00007-10-1 loss: 0.475848  [   64/  179]
train() client id: f_00007-10-2 loss: 0.617014  [   96/  179]
train() client id: f_00007-10-3 loss: 0.374425  [  128/  179]
train() client id: f_00007-10-4 loss: 0.468773  [  160/  179]
train() client id: f_00007-11-0 loss: 0.370921  [   32/  179]
train() client id: f_00007-11-1 loss: 0.472399  [   64/  179]
train() client id: f_00007-11-2 loss: 0.271210  [   96/  179]
train() client id: f_00007-11-3 loss: 0.450820  [  128/  179]
train() client id: f_00007-11-4 loss: 0.614928  [  160/  179]
train() client id: f_00008-0-0 loss: 0.879955  [   32/  130]
train() client id: f_00008-0-1 loss: 0.941809  [   64/  130]
train() client id: f_00008-0-2 loss: 0.815346  [   96/  130]
train() client id: f_00008-0-3 loss: 0.825816  [  128/  130]
train() client id: f_00008-1-0 loss: 0.792068  [   32/  130]
train() client id: f_00008-1-1 loss: 0.881161  [   64/  130]
train() client id: f_00008-1-2 loss: 0.853477  [   96/  130]
train() client id: f_00008-1-3 loss: 0.922624  [  128/  130]
train() client id: f_00008-2-0 loss: 0.860277  [   32/  130]
train() client id: f_00008-2-1 loss: 0.869692  [   64/  130]
train() client id: f_00008-2-2 loss: 0.855962  [   96/  130]
train() client id: f_00008-2-3 loss: 0.835706  [  128/  130]
train() client id: f_00008-3-0 loss: 0.939875  [   32/  130]
train() client id: f_00008-3-1 loss: 0.830947  [   64/  130]
train() client id: f_00008-3-2 loss: 0.833943  [   96/  130]
train() client id: f_00008-3-3 loss: 0.852883  [  128/  130]
train() client id: f_00008-4-0 loss: 0.921436  [   32/  130]
train() client id: f_00008-4-1 loss: 0.864595  [   64/  130]
train() client id: f_00008-4-2 loss: 0.853446  [   96/  130]
train() client id: f_00008-4-3 loss: 0.836027  [  128/  130]
train() client id: f_00008-5-0 loss: 0.873031  [   32/  130]
train() client id: f_00008-5-1 loss: 0.985322  [   64/  130]
train() client id: f_00008-5-2 loss: 0.824546  [   96/  130]
train() client id: f_00008-5-3 loss: 0.784248  [  128/  130]
train() client id: f_00008-6-0 loss: 0.927780  [   32/  130]
train() client id: f_00008-6-1 loss: 0.776890  [   64/  130]
train() client id: f_00008-6-2 loss: 0.899522  [   96/  130]
train() client id: f_00008-6-3 loss: 0.869534  [  128/  130]
train() client id: f_00008-7-0 loss: 0.868597  [   32/  130]
train() client id: f_00008-7-1 loss: 0.896785  [   64/  130]
train() client id: f_00008-7-2 loss: 0.743708  [   96/  130]
train() client id: f_00008-7-3 loss: 0.932088  [  128/  130]
train() client id: f_00008-8-0 loss: 0.908390  [   32/  130]
train() client id: f_00008-8-1 loss: 0.769523  [   64/  130]
train() client id: f_00008-8-2 loss: 0.879717  [   96/  130]
train() client id: f_00008-8-3 loss: 0.922798  [  128/  130]
train() client id: f_00008-9-0 loss: 0.934818  [   32/  130]
train() client id: f_00008-9-1 loss: 0.774420  [   64/  130]
train() client id: f_00008-9-2 loss: 0.793740  [   96/  130]
train() client id: f_00008-9-3 loss: 0.940396  [  128/  130]
train() client id: f_00008-10-0 loss: 0.831080  [   32/  130]
train() client id: f_00008-10-1 loss: 0.936617  [   64/  130]
train() client id: f_00008-10-2 loss: 0.842033  [   96/  130]
train() client id: f_00008-10-3 loss: 0.859633  [  128/  130]
train() client id: f_00008-11-0 loss: 0.846900  [   32/  130]
train() client id: f_00008-11-1 loss: 0.798560  [   64/  130]
train() client id: f_00008-11-2 loss: 0.934913  [   96/  130]
train() client id: f_00008-11-3 loss: 0.904077  [  128/  130]
train() client id: f_00009-0-0 loss: 1.062050  [   32/  118]
train() client id: f_00009-0-1 loss: 1.191857  [   64/  118]
train() client id: f_00009-0-2 loss: 1.074323  [   96/  118]
train() client id: f_00009-1-0 loss: 1.090758  [   32/  118]
train() client id: f_00009-1-1 loss: 1.100511  [   64/  118]
train() client id: f_00009-1-2 loss: 0.947693  [   96/  118]
train() client id: f_00009-2-0 loss: 1.047958  [   32/  118]
train() client id: f_00009-2-1 loss: 1.010269  [   64/  118]
train() client id: f_00009-2-2 loss: 0.915165  [   96/  118]
train() client id: f_00009-3-0 loss: 0.970312  [   32/  118]
train() client id: f_00009-3-1 loss: 1.005255  [   64/  118]
train() client id: f_00009-3-2 loss: 0.843802  [   96/  118]
train() client id: f_00009-4-0 loss: 0.954513  [   32/  118]
train() client id: f_00009-4-1 loss: 0.992453  [   64/  118]
train() client id: f_00009-4-2 loss: 0.908159  [   96/  118]
train() client id: f_00009-5-0 loss: 0.898930  [   32/  118]
train() client id: f_00009-5-1 loss: 0.947335  [   64/  118]
train() client id: f_00009-5-2 loss: 0.980178  [   96/  118]
train() client id: f_00009-6-0 loss: 1.032810  [   32/  118]
train() client id: f_00009-6-1 loss: 0.947852  [   64/  118]
train() client id: f_00009-6-2 loss: 0.829277  [   96/  118]
train() client id: f_00009-7-0 loss: 0.952291  [   32/  118]
train() client id: f_00009-7-1 loss: 0.843414  [   64/  118]
train() client id: f_00009-7-2 loss: 0.740850  [   96/  118]
train() client id: f_00009-8-0 loss: 0.800150  [   32/  118]
train() client id: f_00009-8-1 loss: 0.871542  [   64/  118]
train() client id: f_00009-8-2 loss: 0.799362  [   96/  118]
train() client id: f_00009-9-0 loss: 0.755521  [   32/  118]
train() client id: f_00009-9-1 loss: 0.813573  [   64/  118]
train() client id: f_00009-9-2 loss: 0.931711  [   96/  118]
train() client id: f_00009-10-0 loss: 0.846855  [   32/  118]
train() client id: f_00009-10-1 loss: 0.935330  [   64/  118]
train() client id: f_00009-10-2 loss: 0.773805  [   96/  118]
train() client id: f_00009-11-0 loss: 0.829740  [   32/  118]
train() client id: f_00009-11-1 loss: 0.763927  [   64/  118]
train() client id: f_00009-11-2 loss: 0.948388  [   96/  118]
At round 10 accuracy: 0.6312997347480106
At round 10 training accuracy: 0.5828303152246814
At round 10 training loss: 0.849508067745638
update_location
xs = -3.905658 4.200318 70.009024 18.811294 0.979296 3.956410 -32.443192 -11.324852 54.663977 -2.060879 
ys = 62.587959 45.555839 1.320614 -32.455176 24.350187 7.814151 -2.624984 0.822348 17.569006 -0.998518 
xs mean: 10.288573831882571
ys mean: 12.394142535528704
dists_uav = 118.036040 109.968073 122.077875 106.804509 102.926627 100.382838 105.163926 100.642578 115.311840 100.026218 
uav_gains = -101.800535 -101.031727 -102.166196 -100.714781 -100.313218 -100.041504 -100.546703 -100.069561 -101.546969 -100.002862 
uav_gains_db_mean: -100.82340565495677
dists_bs = 204.718686 221.062310 300.323078 283.902215 231.655086 244.901010 227.756621 239.013407 278.442202 246.745655 
bs_gains = -104.279469 -105.213471 -108.939543 -108.255785 -105.782631 -106.458795 -105.576247 -106.162882 -108.019641 -106.550046 
bs_gains_db_mean: -106.52385098105107
Round 11
-------------------------------
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.53941363 19.90793381  9.39345483  3.35836813 22.96346205 11.07093228
  4.17536772 13.46928103  9.90106646  8.98947369]
obj_prev = 112.76875362310263
eta_min = 1.8259794231621306e-10	eta_max = 0.9205107351620903
af = 23.839837477165666	bf = 1.854557298226717	zeta = 26.223821224882236	eta = 0.909090909090909
af = 23.839837477165666	bf = 1.854557298226717	zeta = 45.394101441493795	eta = 0.5251747852723034
af = 23.839837477165666	bf = 1.854557298226717	zeta = 36.24067924685511	eta = 0.6578198304391449
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.59948643670541	eta = 0.6890228709254712
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.51842070411875	eta = 0.6906410255994443
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.51820885585075	eta = 0.6906452642639038
eta = 0.6906452642639038
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [0.03054215 0.06423548 0.03005736 0.01042312 0.0741738  0.03539011
 0.01308949 0.04338925 0.03151175 0.02860297]
ene_total = [2.94794512 5.70363107 2.91862696 1.33519898 6.50745954 3.47131979
 1.5419756  3.92315854 3.2297488  2.93914446]
ti_comp = [0.30504778 0.29281109 0.30391469 0.30820672 0.29034205 0.28721115
 0.30867006 0.30995051 0.27903667 0.28677106]
ti_coms = [0.0681094  0.08034609 0.06924248 0.06495046 0.08281513 0.08594602
 0.06448711 0.06320667 0.09412051 0.08638612]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.91356519e-05 1.93210162e-04 1.83750713e-05 7.45055334e-07
 3.02560183e-04 3.35832973e-05 1.47115905e-06 5.31424719e-05
 2.51174115e-05 1.77845446e-05]
ene_total = [0.53903666 0.6493481  0.54791909 0.51265458 0.677464   0.68094541
 0.50905514 0.50302767 0.7447912  0.68317183]
optimize_network iter = 0 obj = 6.04741367166706
eta = 0.6906452642639038
freqs = [5.00612622e+07 1.09687587e+08 4.94503204e+07 1.69092939e+07
 1.27735205e+08 6.16099244e+07 2.12030516e+07 6.99938362e+07
 5.64652429e+07 4.98707394e+07]
eta_min = 0.6906452642638832	eta_max = 0.6906452642639023
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 0.04826057494319857	eta = 0.909090909090909
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 20.44612752124682	eta = 0.0021457975307437047
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.1286929945616637	eta = 0.020610416842846153
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.067483129885282	eta = 0.02122060843649869
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.0674642356817103	eta = 0.021220802368025433
eta = 0.021220802368025433
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.99188226e-04 2.01117734e-03 1.91271134e-04 7.75548441e-06
 3.14943156e-03 3.49577711e-04 1.53136962e-05 5.53174501e-04
 2.61453995e-04 1.85124181e-04]
ene_total = [0.17486013 0.25058139 0.17748901 0.16220541 0.28513266 0.22310299
 0.16123818 0.17146061 0.24129522 0.22009864]
ti_comp = [0.30504778 0.29281109 0.30391469 0.30820672 0.29034205 0.28721115
 0.30867006 0.30995051 0.27903667 0.28677106]
ti_coms = [0.0681094  0.08034609 0.06924248 0.06495046 0.08281513 0.08594602
 0.06448711 0.06320667 0.09412051 0.08638612]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.91356519e-05 1.93210162e-04 1.83750713e-05 7.45055334e-07
 3.02560183e-04 3.35832973e-05 1.47115905e-06 5.31424719e-05
 2.51174115e-05 1.77845446e-05]
ene_total = [0.53903666 0.6493481  0.54791909 0.51265458 0.677464   0.68094541
 0.50905514 0.50302767 0.7447912  0.68317183]
optimize_network iter = 1 obj = 6.047413671666661
eta = 0.6906452642638832
freqs = [5.00612622e+07 1.09687587e+08 4.94503204e+07 1.69092939e+07
 1.27735205e+08 6.16099244e+07 2.12030516e+07 6.99938362e+07
 5.64652429e+07 4.98707394e+07]
Done!
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.89463426e-05 1.91298731e-04 1.81932864e-05 7.37684489e-07
 2.99566950e-04 3.32510571e-05 1.45660485e-06 5.26167325e-05
 2.48689246e-05 1.76086018e-05]
ene_total = [0.00682989 0.00822591 0.00694244 0.00649578 0.00858108 0.00862785
 0.00645017 0.00637328 0.00943692 0.00865622]
At round 11 energy consumption: 0.0766195440375612
At round 11 eta: 0.6906452642638832
At round 11 a_n: 24.414598539591886
At round 11 local rounds: 12.119902394804324
At round 11 global rounds: 78.921043447086
gradient difference: 0.36330682039260864
train() client id: f_00000-0-0 loss: 1.055135  [   32/  126]
train() client id: f_00000-0-1 loss: 0.993811  [   64/  126]
train() client id: f_00000-0-2 loss: 1.043028  [   96/  126]
train() client id: f_00000-1-0 loss: 1.089219  [   32/  126]
train() client id: f_00000-1-1 loss: 0.975001  [   64/  126]
train() client id: f_00000-1-2 loss: 0.886016  [   96/  126]
train() client id: f_00000-2-0 loss: 0.992932  [   32/  126]
train() client id: f_00000-2-1 loss: 0.878812  [   64/  126]
train() client id: f_00000-2-2 loss: 0.910041  [   96/  126]
train() client id: f_00000-3-0 loss: 0.857608  [   32/  126]
train() client id: f_00000-3-1 loss: 0.968917  [   64/  126]
train() client id: f_00000-3-2 loss: 0.859341  [   96/  126]
train() client id: f_00000-4-0 loss: 0.903524  [   32/  126]
train() client id: f_00000-4-1 loss: 0.859176  [   64/  126]
train() client id: f_00000-4-2 loss: 0.818390  [   96/  126]
train() client id: f_00000-5-0 loss: 0.767050  [   32/  126]
train() client id: f_00000-5-1 loss: 0.903287  [   64/  126]
train() client id: f_00000-5-2 loss: 0.859052  [   96/  126]
train() client id: f_00000-6-0 loss: 0.780330  [   32/  126]
train() client id: f_00000-6-1 loss: 0.700504  [   64/  126]
train() client id: f_00000-6-2 loss: 0.856248  [   96/  126]
train() client id: f_00000-7-0 loss: 0.782288  [   32/  126]
train() client id: f_00000-7-1 loss: 0.688533  [   64/  126]
train() client id: f_00000-7-2 loss: 0.844565  [   96/  126]
train() client id: f_00000-8-0 loss: 0.612258  [   32/  126]
train() client id: f_00000-8-1 loss: 0.735221  [   64/  126]
train() client id: f_00000-8-2 loss: 0.828971  [   96/  126]
train() client id: f_00000-9-0 loss: 0.656965  [   32/  126]
train() client id: f_00000-9-1 loss: 0.668077  [   64/  126]
train() client id: f_00000-9-2 loss: 0.802647  [   96/  126]
train() client id: f_00000-10-0 loss: 0.745500  [   32/  126]
train() client id: f_00000-10-1 loss: 0.727138  [   64/  126]
train() client id: f_00000-10-2 loss: 0.785592  [   96/  126]
train() client id: f_00000-11-0 loss: 0.706994  [   32/  126]
train() client id: f_00000-11-1 loss: 0.818503  [   64/  126]
train() client id: f_00000-11-2 loss: 0.747090  [   96/  126]
train() client id: f_00001-0-0 loss: 0.590223  [   32/  265]
train() client id: f_00001-0-1 loss: 0.513824  [   64/  265]
train() client id: f_00001-0-2 loss: 0.505581  [   96/  265]
train() client id: f_00001-0-3 loss: 0.622306  [  128/  265]
train() client id: f_00001-0-4 loss: 0.453637  [  160/  265]
train() client id: f_00001-0-5 loss: 0.414166  [  192/  265]
train() client id: f_00001-0-6 loss: 0.492619  [  224/  265]
train() client id: f_00001-0-7 loss: 0.512568  [  256/  265]
train() client id: f_00001-1-0 loss: 0.468002  [   32/  265]
train() client id: f_00001-1-1 loss: 0.511334  [   64/  265]
train() client id: f_00001-1-2 loss: 0.542576  [   96/  265]
train() client id: f_00001-1-3 loss: 0.582487  [  128/  265]
train() client id: f_00001-1-4 loss: 0.454430  [  160/  265]
train() client id: f_00001-1-5 loss: 0.474699  [  192/  265]
train() client id: f_00001-1-6 loss: 0.504216  [  224/  265]
train() client id: f_00001-1-7 loss: 0.442439  [  256/  265]
train() client id: f_00001-2-0 loss: 0.710348  [   32/  265]
train() client id: f_00001-2-1 loss: 0.536351  [   64/  265]
train() client id: f_00001-2-2 loss: 0.402693  [   96/  265]
train() client id: f_00001-2-3 loss: 0.508980  [  128/  265]
train() client id: f_00001-2-4 loss: 0.477118  [  160/  265]
train() client id: f_00001-2-5 loss: 0.390461  [  192/  265]
train() client id: f_00001-2-6 loss: 0.444292  [  224/  265]
train() client id: f_00001-2-7 loss: 0.454161  [  256/  265]
train() client id: f_00001-3-0 loss: 0.465497  [   32/  265]
train() client id: f_00001-3-1 loss: 0.538363  [   64/  265]
train() client id: f_00001-3-2 loss: 0.499106  [   96/  265]
train() client id: f_00001-3-3 loss: 0.538775  [  128/  265]
train() client id: f_00001-3-4 loss: 0.407119  [  160/  265]
train() client id: f_00001-3-5 loss: 0.485182  [  192/  265]
train() client id: f_00001-3-6 loss: 0.477966  [  224/  265]
train() client id: f_00001-3-7 loss: 0.492165  [  256/  265]
train() client id: f_00001-4-0 loss: 0.471648  [   32/  265]
train() client id: f_00001-4-1 loss: 0.413854  [   64/  265]
train() client id: f_00001-4-2 loss: 0.484965  [   96/  265]
train() client id: f_00001-4-3 loss: 0.553760  [  128/  265]
train() client id: f_00001-4-4 loss: 0.474283  [  160/  265]
train() client id: f_00001-4-5 loss: 0.514536  [  192/  265]
train() client id: f_00001-4-6 loss: 0.402483  [  224/  265]
train() client id: f_00001-4-7 loss: 0.528985  [  256/  265]
train() client id: f_00001-5-0 loss: 0.491060  [   32/  265]
train() client id: f_00001-5-1 loss: 0.478185  [   64/  265]
train() client id: f_00001-5-2 loss: 0.399478  [   96/  265]
train() client id: f_00001-5-3 loss: 0.550563  [  128/  265]
train() client id: f_00001-5-4 loss: 0.416560  [  160/  265]
train() client id: f_00001-5-5 loss: 0.616960  [  192/  265]
train() client id: f_00001-5-6 loss: 0.391045  [  224/  265]
train() client id: f_00001-5-7 loss: 0.497859  [  256/  265]
train() client id: f_00001-6-0 loss: 0.615522  [   32/  265]
train() client id: f_00001-6-1 loss: 0.402663  [   64/  265]
train() client id: f_00001-6-2 loss: 0.471709  [   96/  265]
train() client id: f_00001-6-3 loss: 0.432540  [  128/  265]
train() client id: f_00001-6-4 loss: 0.477996  [  160/  265]
train() client id: f_00001-6-5 loss: 0.525574  [  192/  265]
train() client id: f_00001-6-6 loss: 0.409282  [  224/  265]
train() client id: f_00001-6-7 loss: 0.506303  [  256/  265]
train() client id: f_00001-7-0 loss: 0.519618  [   32/  265]
train() client id: f_00001-7-1 loss: 0.469982  [   64/  265]
train() client id: f_00001-7-2 loss: 0.495202  [   96/  265]
train() client id: f_00001-7-3 loss: 0.377323  [  128/  265]
train() client id: f_00001-7-4 loss: 0.424007  [  160/  265]
train() client id: f_00001-7-5 loss: 0.523472  [  192/  265]
train() client id: f_00001-7-6 loss: 0.498258  [  224/  265]
train() client id: f_00001-7-7 loss: 0.499932  [  256/  265]
train() client id: f_00001-8-0 loss: 0.518281  [   32/  265]
train() client id: f_00001-8-1 loss: 0.441004  [   64/  265]
train() client id: f_00001-8-2 loss: 0.544683  [   96/  265]
train() client id: f_00001-8-3 loss: 0.390406  [  128/  265]
train() client id: f_00001-8-4 loss: 0.379447  [  160/  265]
train() client id: f_00001-8-5 loss: 0.536764  [  192/  265]
train() client id: f_00001-8-6 loss: 0.494143  [  224/  265]
train() client id: f_00001-8-7 loss: 0.486430  [  256/  265]
train() client id: f_00001-9-0 loss: 0.413364  [   32/  265]
train() client id: f_00001-9-1 loss: 0.530024  [   64/  265]
train() client id: f_00001-9-2 loss: 0.425555  [   96/  265]
train() client id: f_00001-9-3 loss: 0.393706  [  128/  265]
train() client id: f_00001-9-4 loss: 0.495919  [  160/  265]
train() client id: f_00001-9-5 loss: 0.538313  [  192/  265]
train() client id: f_00001-9-6 loss: 0.553379  [  224/  265]
train() client id: f_00001-9-7 loss: 0.481515  [  256/  265]
train() client id: f_00001-10-0 loss: 0.460135  [   32/  265]
train() client id: f_00001-10-1 loss: 0.546406  [   64/  265]
train() client id: f_00001-10-2 loss: 0.533712  [   96/  265]
train() client id: f_00001-10-3 loss: 0.507526  [  128/  265]
train() client id: f_00001-10-4 loss: 0.478593  [  160/  265]
train() client id: f_00001-10-5 loss: 0.459470  [  192/  265]
train() client id: f_00001-10-6 loss: 0.401818  [  224/  265]
train() client id: f_00001-10-7 loss: 0.439814  [  256/  265]
train() client id: f_00001-11-0 loss: 0.501149  [   32/  265]
train() client id: f_00001-11-1 loss: 0.413029  [   64/  265]
train() client id: f_00001-11-2 loss: 0.478214  [   96/  265]
train() client id: f_00001-11-3 loss: 0.500063  [  128/  265]
train() client id: f_00001-11-4 loss: 0.477110  [  160/  265]
train() client id: f_00001-11-5 loss: 0.497666  [  192/  265]
train() client id: f_00001-11-6 loss: 0.455995  [  224/  265]
train() client id: f_00001-11-7 loss: 0.496106  [  256/  265]
train() client id: f_00002-0-0 loss: 1.202429  [   32/  124]
train() client id: f_00002-0-1 loss: 1.341038  [   64/  124]
train() client id: f_00002-0-2 loss: 1.255159  [   96/  124]
train() client id: f_00002-1-0 loss: 1.270439  [   32/  124]
train() client id: f_00002-1-1 loss: 1.392755  [   64/  124]
train() client id: f_00002-1-2 loss: 1.189058  [   96/  124]
train() client id: f_00002-2-0 loss: 1.304774  [   32/  124]
train() client id: f_00002-2-1 loss: 1.212917  [   64/  124]
train() client id: f_00002-2-2 loss: 1.218884  [   96/  124]
train() client id: f_00002-3-0 loss: 1.118405  [   32/  124]
train() client id: f_00002-3-1 loss: 1.141145  [   64/  124]
train() client id: f_00002-3-2 loss: 1.302372  [   96/  124]
train() client id: f_00002-4-0 loss: 1.333134  [   32/  124]
train() client id: f_00002-4-1 loss: 1.199296  [   64/  124]
train() client id: f_00002-4-2 loss: 1.069436  [   96/  124]
train() client id: f_00002-5-0 loss: 1.240858  [   32/  124]
train() client id: f_00002-5-1 loss: 1.167560  [   64/  124]
train() client id: f_00002-5-2 loss: 1.228901  [   96/  124]
train() client id: f_00002-6-0 loss: 1.190924  [   32/  124]
train() client id: f_00002-6-1 loss: 1.126695  [   64/  124]
train() client id: f_00002-6-2 loss: 1.142968  [   96/  124]
train() client id: f_00002-7-0 loss: 1.214540  [   32/  124]
train() client id: f_00002-7-1 loss: 1.141480  [   64/  124]
train() client id: f_00002-7-2 loss: 0.984745  [   96/  124]
train() client id: f_00002-8-0 loss: 1.000546  [   32/  124]
train() client id: f_00002-8-1 loss: 1.330578  [   64/  124]
train() client id: f_00002-8-2 loss: 1.185466  [   96/  124]
train() client id: f_00002-9-0 loss: 1.101245  [   32/  124]
train() client id: f_00002-9-1 loss: 1.109675  [   64/  124]
train() client id: f_00002-9-2 loss: 1.163217  [   96/  124]
train() client id: f_00002-10-0 loss: 1.279562  [   32/  124]
train() client id: f_00002-10-1 loss: 1.039282  [   64/  124]
train() client id: f_00002-10-2 loss: 1.187289  [   96/  124]
train() client id: f_00002-11-0 loss: 1.252868  [   32/  124]
train() client id: f_00002-11-1 loss: 0.964802  [   64/  124]
train() client id: f_00002-11-2 loss: 1.057611  [   96/  124]
train() client id: f_00003-0-0 loss: 0.924740  [   32/   43]
train() client id: f_00003-1-0 loss: 0.765285  [   32/   43]
train() client id: f_00003-2-0 loss: 0.837822  [   32/   43]
train() client id: f_00003-3-0 loss: 0.924528  [   32/   43]
train() client id: f_00003-4-0 loss: 0.768564  [   32/   43]
train() client id: f_00003-5-0 loss: 0.954822  [   32/   43]
train() client id: f_00003-6-0 loss: 0.791395  [   32/   43]
train() client id: f_00003-7-0 loss: 0.927416  [   32/   43]
train() client id: f_00003-8-0 loss: 0.893401  [   32/   43]
train() client id: f_00003-9-0 loss: 0.838121  [   32/   43]
train() client id: f_00003-10-0 loss: 0.890612  [   32/   43]
train() client id: f_00003-11-0 loss: 0.858717  [   32/   43]
train() client id: f_00004-0-0 loss: 0.887887  [   32/  306]
train() client id: f_00004-0-1 loss: 1.018923  [   64/  306]
train() client id: f_00004-0-2 loss: 0.889375  [   96/  306]
train() client id: f_00004-0-3 loss: 0.984824  [  128/  306]
train() client id: f_00004-0-4 loss: 1.006441  [  160/  306]
train() client id: f_00004-0-5 loss: 1.015386  [  192/  306]
train() client id: f_00004-0-6 loss: 0.811982  [  224/  306]
train() client id: f_00004-0-7 loss: 0.885066  [  256/  306]
train() client id: f_00004-0-8 loss: 0.822769  [  288/  306]
train() client id: f_00004-1-0 loss: 0.907548  [   32/  306]
train() client id: f_00004-1-1 loss: 1.029462  [   64/  306]
train() client id: f_00004-1-2 loss: 0.892109  [   96/  306]
train() client id: f_00004-1-3 loss: 0.919874  [  128/  306]
train() client id: f_00004-1-4 loss: 0.839025  [  160/  306]
train() client id: f_00004-1-5 loss: 0.922692  [  192/  306]
train() client id: f_00004-1-6 loss: 0.941760  [  224/  306]
train() client id: f_00004-1-7 loss: 0.939185  [  256/  306]
train() client id: f_00004-1-8 loss: 0.944785  [  288/  306]
train() client id: f_00004-2-0 loss: 0.887868  [   32/  306]
train() client id: f_00004-2-1 loss: 0.972192  [   64/  306]
train() client id: f_00004-2-2 loss: 1.014930  [   96/  306]
train() client id: f_00004-2-3 loss: 0.801919  [  128/  306]
train() client id: f_00004-2-4 loss: 0.959437  [  160/  306]
train() client id: f_00004-2-5 loss: 0.955393  [  192/  306]
train() client id: f_00004-2-6 loss: 0.835059  [  224/  306]
train() client id: f_00004-2-7 loss: 0.914386  [  256/  306]
train() client id: f_00004-2-8 loss: 0.839990  [  288/  306]
train() client id: f_00004-3-0 loss: 0.969525  [   32/  306]
train() client id: f_00004-3-1 loss: 0.933213  [   64/  306]
train() client id: f_00004-3-2 loss: 0.877849  [   96/  306]
train() client id: f_00004-3-3 loss: 0.858162  [  128/  306]
train() client id: f_00004-3-4 loss: 0.885516  [  160/  306]
train() client id: f_00004-3-5 loss: 0.911682  [  192/  306]
train() client id: f_00004-3-6 loss: 0.934023  [  224/  306]
train() client id: f_00004-3-7 loss: 0.919279  [  256/  306]
train() client id: f_00004-3-8 loss: 0.940839  [  288/  306]
train() client id: f_00004-4-0 loss: 1.025107  [   32/  306]
train() client id: f_00004-4-1 loss: 0.897549  [   64/  306]
train() client id: f_00004-4-2 loss: 0.990244  [   96/  306]
train() client id: f_00004-4-3 loss: 0.924751  [  128/  306]
train() client id: f_00004-4-4 loss: 0.852225  [  160/  306]
train() client id: f_00004-4-5 loss: 0.804730  [  192/  306]
train() client id: f_00004-4-6 loss: 0.884951  [  224/  306]
train() client id: f_00004-4-7 loss: 0.936937  [  256/  306]
train() client id: f_00004-4-8 loss: 0.928378  [  288/  306]
train() client id: f_00004-5-0 loss: 0.842085  [   32/  306]
train() client id: f_00004-5-1 loss: 0.890147  [   64/  306]
train() client id: f_00004-5-2 loss: 0.869004  [   96/  306]
train() client id: f_00004-5-3 loss: 0.910710  [  128/  306]
train() client id: f_00004-5-4 loss: 0.909875  [  160/  306]
train() client id: f_00004-5-5 loss: 0.925001  [  192/  306]
train() client id: f_00004-5-6 loss: 1.071090  [  224/  306]
train() client id: f_00004-5-7 loss: 0.917142  [  256/  306]
train() client id: f_00004-5-8 loss: 0.883021  [  288/  306]
train() client id: f_00004-6-0 loss: 0.819985  [   32/  306]
train() client id: f_00004-6-1 loss: 0.921938  [   64/  306]
train() client id: f_00004-6-2 loss: 0.915621  [   96/  306]
train() client id: f_00004-6-3 loss: 0.856074  [  128/  306]
train() client id: f_00004-6-4 loss: 1.042572  [  160/  306]
train() client id: f_00004-6-5 loss: 0.982096  [  192/  306]
train() client id: f_00004-6-6 loss: 0.965665  [  224/  306]
train() client id: f_00004-6-7 loss: 0.873293  [  256/  306]
train() client id: f_00004-6-8 loss: 0.947738  [  288/  306]
train() client id: f_00004-7-0 loss: 0.921428  [   32/  306]
train() client id: f_00004-7-1 loss: 0.904215  [   64/  306]
train() client id: f_00004-7-2 loss: 0.961113  [   96/  306]
train() client id: f_00004-7-3 loss: 0.948148  [  128/  306]
train() client id: f_00004-7-4 loss: 0.904026  [  160/  306]
train() client id: f_00004-7-5 loss: 0.823888  [  192/  306]
train() client id: f_00004-7-6 loss: 0.864692  [  224/  306]
train() client id: f_00004-7-7 loss: 0.928553  [  256/  306]
train() client id: f_00004-7-8 loss: 0.984060  [  288/  306]
train() client id: f_00004-8-0 loss: 0.900422  [   32/  306]
train() client id: f_00004-8-1 loss: 0.923925  [   64/  306]
train() client id: f_00004-8-2 loss: 0.871632  [   96/  306]
train() client id: f_00004-8-3 loss: 1.003564  [  128/  306]
train() client id: f_00004-8-4 loss: 0.958704  [  160/  306]
train() client id: f_00004-8-5 loss: 0.882238  [  192/  306]
train() client id: f_00004-8-6 loss: 0.971630  [  224/  306]
train() client id: f_00004-8-7 loss: 0.887873  [  256/  306]
train() client id: f_00004-8-8 loss: 0.897237  [  288/  306]
train() client id: f_00004-9-0 loss: 0.911084  [   32/  306]
train() client id: f_00004-9-1 loss: 0.898172  [   64/  306]
train() client id: f_00004-9-2 loss: 0.912284  [   96/  306]
train() client id: f_00004-9-3 loss: 0.874819  [  128/  306]
train() client id: f_00004-9-4 loss: 0.938902  [  160/  306]
train() client id: f_00004-9-5 loss: 0.904796  [  192/  306]
train() client id: f_00004-9-6 loss: 0.966825  [  224/  306]
train() client id: f_00004-9-7 loss: 0.908816  [  256/  306]
train() client id: f_00004-9-8 loss: 0.932015  [  288/  306]
train() client id: f_00004-10-0 loss: 0.925435  [   32/  306]
train() client id: f_00004-10-1 loss: 0.905761  [   64/  306]
train() client id: f_00004-10-2 loss: 0.897228  [   96/  306]
train() client id: f_00004-10-3 loss: 0.976010  [  128/  306]
train() client id: f_00004-10-4 loss: 0.920304  [  160/  306]
train() client id: f_00004-10-5 loss: 0.962143  [  192/  306]
train() client id: f_00004-10-6 loss: 0.830754  [  224/  306]
train() client id: f_00004-10-7 loss: 0.936802  [  256/  306]
train() client id: f_00004-10-8 loss: 0.946484  [  288/  306]
train() client id: f_00004-11-0 loss: 0.903166  [   32/  306]
train() client id: f_00004-11-1 loss: 0.925871  [   64/  306]
train() client id: f_00004-11-2 loss: 0.818915  [   96/  306]
train() client id: f_00004-11-3 loss: 0.874200  [  128/  306]
train() client id: f_00004-11-4 loss: 0.964445  [  160/  306]
train() client id: f_00004-11-5 loss: 1.025101  [  192/  306]
train() client id: f_00004-11-6 loss: 0.936553  [  224/  306]
train() client id: f_00004-11-7 loss: 0.908542  [  256/  306]
train() client id: f_00004-11-8 loss: 0.915856  [  288/  306]
train() client id: f_00005-0-0 loss: 0.683894  [   32/  146]
train() client id: f_00005-0-1 loss: 0.738698  [   64/  146]
train() client id: f_00005-0-2 loss: 0.716302  [   96/  146]
train() client id: f_00005-0-3 loss: 0.533646  [  128/  146]
train() client id: f_00005-1-0 loss: 0.653662  [   32/  146]
train() client id: f_00005-1-1 loss: 0.608941  [   64/  146]
train() client id: f_00005-1-2 loss: 0.826596  [   96/  146]
train() client id: f_00005-1-3 loss: 0.526398  [  128/  146]
train() client id: f_00005-2-0 loss: 0.636556  [   32/  146]
train() client id: f_00005-2-1 loss: 0.451883  [   64/  146]
train() client id: f_00005-2-2 loss: 0.666323  [   96/  146]
train() client id: f_00005-2-3 loss: 0.759735  [  128/  146]
train() client id: f_00005-3-0 loss: 0.835006  [   32/  146]
train() client id: f_00005-3-1 loss: 0.524988  [   64/  146]
train() client id: f_00005-3-2 loss: 0.637046  [   96/  146]
train() client id: f_00005-3-3 loss: 0.523901  [  128/  146]
train() client id: f_00005-4-0 loss: 0.407425  [   32/  146]
train() client id: f_00005-4-1 loss: 0.694930  [   64/  146]
train() client id: f_00005-4-2 loss: 0.475327  [   96/  146]
train() client id: f_00005-4-3 loss: 0.890717  [  128/  146]
train() client id: f_00005-5-0 loss: 0.852347  [   32/  146]
train() client id: f_00005-5-1 loss: 0.752610  [   64/  146]
train() client id: f_00005-5-2 loss: 0.478336  [   96/  146]
train() client id: f_00005-5-3 loss: 0.500393  [  128/  146]
train() client id: f_00005-6-0 loss: 0.611399  [   32/  146]
train() client id: f_00005-6-1 loss: 0.530822  [   64/  146]
train() client id: f_00005-6-2 loss: 0.463204  [   96/  146]
train() client id: f_00005-6-3 loss: 0.655956  [  128/  146]
train() client id: f_00005-7-0 loss: 0.470761  [   32/  146]
train() client id: f_00005-7-1 loss: 0.457565  [   64/  146]
train() client id: f_00005-7-2 loss: 0.741092  [   96/  146]
train() client id: f_00005-7-3 loss: 0.849877  [  128/  146]
train() client id: f_00005-8-0 loss: 0.749397  [   32/  146]
train() client id: f_00005-8-1 loss: 0.641922  [   64/  146]
train() client id: f_00005-8-2 loss: 0.513302  [   96/  146]
train() client id: f_00005-8-3 loss: 0.573056  [  128/  146]
train() client id: f_00005-9-0 loss: 0.573627  [   32/  146]
train() client id: f_00005-9-1 loss: 0.685327  [   64/  146]
train() client id: f_00005-9-2 loss: 0.553903  [   96/  146]
train() client id: f_00005-9-3 loss: 0.567848  [  128/  146]
train() client id: f_00005-10-0 loss: 0.501398  [   32/  146]
train() client id: f_00005-10-1 loss: 0.445160  [   64/  146]
train() client id: f_00005-10-2 loss: 0.729183  [   96/  146]
train() client id: f_00005-10-3 loss: 0.658224  [  128/  146]
train() client id: f_00005-11-0 loss: 0.637106  [   32/  146]
train() client id: f_00005-11-1 loss: 0.429243  [   64/  146]
train() client id: f_00005-11-2 loss: 0.769017  [   96/  146]
train() client id: f_00005-11-3 loss: 0.512845  [  128/  146]
train() client id: f_00006-0-0 loss: 0.582722  [   32/   54]
train() client id: f_00006-1-0 loss: 0.575315  [   32/   54]
train() client id: f_00006-2-0 loss: 0.614347  [   32/   54]
train() client id: f_00006-3-0 loss: 0.578589  [   32/   54]
train() client id: f_00006-4-0 loss: 0.618125  [   32/   54]
train() client id: f_00006-5-0 loss: 0.600512  [   32/   54]
train() client id: f_00006-6-0 loss: 0.627263  [   32/   54]
train() client id: f_00006-7-0 loss: 0.596287  [   32/   54]
train() client id: f_00006-8-0 loss: 0.562507  [   32/   54]
train() client id: f_00006-9-0 loss: 0.624334  [   32/   54]
train() client id: f_00006-10-0 loss: 0.630655  [   32/   54]
train() client id: f_00006-11-0 loss: 0.620174  [   32/   54]
train() client id: f_00007-0-0 loss: 0.563298  [   32/  179]
train() client id: f_00007-0-1 loss: 0.391006  [   64/  179]
train() client id: f_00007-0-2 loss: 0.590680  [   96/  179]
train() client id: f_00007-0-3 loss: 0.328929  [  128/  179]
train() client id: f_00007-0-4 loss: 0.494926  [  160/  179]
train() client id: f_00007-1-0 loss: 0.430151  [   32/  179]
train() client id: f_00007-1-1 loss: 0.431224  [   64/  179]
train() client id: f_00007-1-2 loss: 0.477433  [   96/  179]
train() client id: f_00007-1-3 loss: 0.403532  [  128/  179]
train() client id: f_00007-1-4 loss: 0.392583  [  160/  179]
train() client id: f_00007-2-0 loss: 0.412779  [   32/  179]
train() client id: f_00007-2-1 loss: 0.463127  [   64/  179]
train() client id: f_00007-2-2 loss: 0.373936  [   96/  179]
train() client id: f_00007-2-3 loss: 0.354275  [  128/  179]
train() client id: f_00007-2-4 loss: 0.426115  [  160/  179]
train() client id: f_00007-3-0 loss: 0.589733  [   32/  179]
train() client id: f_00007-3-1 loss: 0.347190  [   64/  179]
train() client id: f_00007-3-2 loss: 0.268191  [   96/  179]
train() client id: f_00007-3-3 loss: 0.509898  [  128/  179]
train() client id: f_00007-3-4 loss: 0.425511  [  160/  179]
train() client id: f_00007-4-0 loss: 0.366631  [   32/  179]
train() client id: f_00007-4-1 loss: 0.511778  [   64/  179]
train() client id: f_00007-4-2 loss: 0.417661  [   96/  179]
train() client id: f_00007-4-3 loss: 0.439226  [  128/  179]
train() client id: f_00007-4-4 loss: 0.259644  [  160/  179]
train() client id: f_00007-5-0 loss: 0.276621  [   32/  179]
train() client id: f_00007-5-1 loss: 0.529354  [   64/  179]
train() client id: f_00007-5-2 loss: 0.411468  [   96/  179]
train() client id: f_00007-5-3 loss: 0.229929  [  128/  179]
train() client id: f_00007-5-4 loss: 0.345607  [  160/  179]
train() client id: f_00007-6-0 loss: 0.511251  [   32/  179]
train() client id: f_00007-6-1 loss: 0.384717  [   64/  179]
train() client id: f_00007-6-2 loss: 0.539001  [   96/  179]
train() client id: f_00007-6-3 loss: 0.252003  [  128/  179]
train() client id: f_00007-6-4 loss: 0.352274  [  160/  179]
train() client id: f_00007-7-0 loss: 0.238661  [   32/  179]
train() client id: f_00007-7-1 loss: 0.339150  [   64/  179]
train() client id: f_00007-7-2 loss: 0.254950  [   96/  179]
train() client id: f_00007-7-3 loss: 0.513561  [  128/  179]
train() client id: f_00007-7-4 loss: 0.534977  [  160/  179]
train() client id: f_00007-8-0 loss: 0.397030  [   32/  179]
train() client id: f_00007-8-1 loss: 0.374221  [   64/  179]
train() client id: f_00007-8-2 loss: 0.352187  [   96/  179]
train() client id: f_00007-8-3 loss: 0.433645  [  128/  179]
train() client id: f_00007-8-4 loss: 0.444911  [  160/  179]
train() client id: f_00007-9-0 loss: 0.235759  [   32/  179]
train() client id: f_00007-9-1 loss: 0.478782  [   64/  179]
train() client id: f_00007-9-2 loss: 0.334666  [   96/  179]
train() client id: f_00007-9-3 loss: 0.447138  [  128/  179]
train() client id: f_00007-9-4 loss: 0.463493  [  160/  179]
train() client id: f_00007-10-0 loss: 0.355677  [   32/  179]
train() client id: f_00007-10-1 loss: 0.251294  [   64/  179]
train() client id: f_00007-10-2 loss: 0.397264  [   96/  179]
train() client id: f_00007-10-3 loss: 0.405029  [  128/  179]
train() client id: f_00007-10-4 loss: 0.570395  [  160/  179]
train() client id: f_00007-11-0 loss: 0.311098  [   32/  179]
train() client id: f_00007-11-1 loss: 0.240549  [   64/  179]
train() client id: f_00007-11-2 loss: 0.326037  [   96/  179]
train() client id: f_00007-11-3 loss: 0.670587  [  128/  179]
train() client id: f_00007-11-4 loss: 0.412504  [  160/  179]
train() client id: f_00008-0-0 loss: 0.703953  [   32/  130]
train() client id: f_00008-0-1 loss: 0.688214  [   64/  130]
train() client id: f_00008-0-2 loss: 0.870282  [   96/  130]
train() client id: f_00008-0-3 loss: 0.785217  [  128/  130]
train() client id: f_00008-1-0 loss: 0.735752  [   32/  130]
train() client id: f_00008-1-1 loss: 0.819323  [   64/  130]
train() client id: f_00008-1-2 loss: 0.779980  [   96/  130]
train() client id: f_00008-1-3 loss: 0.721121  [  128/  130]
train() client id: f_00008-2-0 loss: 0.670141  [   32/  130]
train() client id: f_00008-2-1 loss: 0.760039  [   64/  130]
train() client id: f_00008-2-2 loss: 0.826588  [   96/  130]
train() client id: f_00008-2-3 loss: 0.776928  [  128/  130]
train() client id: f_00008-3-0 loss: 0.741404  [   32/  130]
train() client id: f_00008-3-1 loss: 0.753139  [   64/  130]
train() client id: f_00008-3-2 loss: 0.753367  [   96/  130]
train() client id: f_00008-3-3 loss: 0.750480  [  128/  130]
train() client id: f_00008-4-0 loss: 0.738276  [   32/  130]
train() client id: f_00008-4-1 loss: 0.680733  [   64/  130]
train() client id: f_00008-4-2 loss: 0.781953  [   96/  130]
train() client id: f_00008-4-3 loss: 0.797957  [  128/  130]
train() client id: f_00008-5-0 loss: 0.729851  [   32/  130]
train() client id: f_00008-5-1 loss: 0.708367  [   64/  130]
train() client id: f_00008-5-2 loss: 0.847754  [   96/  130]
train() client id: f_00008-5-3 loss: 0.693288  [  128/  130]
train() client id: f_00008-6-0 loss: 0.741640  [   32/  130]
train() client id: f_00008-6-1 loss: 0.747096  [   64/  130]
train() client id: f_00008-6-2 loss: 0.714220  [   96/  130]
train() client id: f_00008-6-3 loss: 0.799569  [  128/  130]
train() client id: f_00008-7-0 loss: 0.997085  [   32/  130]
train() client id: f_00008-7-1 loss: 0.658759  [   64/  130]
train() client id: f_00008-7-2 loss: 0.609475  [   96/  130]
train() client id: f_00008-7-3 loss: 0.734691  [  128/  130]
train() client id: f_00008-8-0 loss: 0.893140  [   32/  130]
train() client id: f_00008-8-1 loss: 0.643975  [   64/  130]
train() client id: f_00008-8-2 loss: 0.771700  [   96/  130]
train() client id: f_00008-8-3 loss: 0.674000  [  128/  130]
train() client id: f_00008-9-0 loss: 0.670135  [   32/  130]
train() client id: f_00008-9-1 loss: 0.774917  [   64/  130]
train() client id: f_00008-9-2 loss: 0.773557  [   96/  130]
train() client id: f_00008-9-3 loss: 0.714843  [  128/  130]
train() client id: f_00008-10-0 loss: 0.767482  [   32/  130]
train() client id: f_00008-10-1 loss: 0.769807  [   64/  130]
train() client id: f_00008-10-2 loss: 0.605067  [   96/  130]
train() client id: f_00008-10-3 loss: 0.796246  [  128/  130]
train() client id: f_00008-11-0 loss: 0.754901  [   32/  130]
train() client id: f_00008-11-1 loss: 0.731807  [   64/  130]
train() client id: f_00008-11-2 loss: 0.766666  [   96/  130]
train() client id: f_00008-11-3 loss: 0.714512  [  128/  130]
train() client id: f_00009-0-0 loss: 1.125470  [   32/  118]
train() client id: f_00009-0-1 loss: 1.028234  [   64/  118]
train() client id: f_00009-0-2 loss: 0.970910  [   96/  118]
train() client id: f_00009-1-0 loss: 0.978172  [   32/  118]
train() client id: f_00009-1-1 loss: 0.961172  [   64/  118]
train() client id: f_00009-1-2 loss: 1.005169  [   96/  118]
train() client id: f_00009-2-0 loss: 0.863729  [   32/  118]
train() client id: f_00009-2-1 loss: 1.089037  [   64/  118]
train() client id: f_00009-2-2 loss: 0.879932  [   96/  118]
train() client id: f_00009-3-0 loss: 0.838466  [   32/  118]
train() client id: f_00009-3-1 loss: 1.018048  [   64/  118]
train() client id: f_00009-3-2 loss: 0.843391  [   96/  118]
train() client id: f_00009-4-0 loss: 0.870931  [   32/  118]
train() client id: f_00009-4-1 loss: 0.804188  [   64/  118]
train() client id: f_00009-4-2 loss: 0.792221  [   96/  118]
train() client id: f_00009-5-0 loss: 0.797413  [   32/  118]
train() client id: f_00009-5-1 loss: 0.792966  [   64/  118]
train() client id: f_00009-5-2 loss: 0.934989  [   96/  118]
train() client id: f_00009-6-0 loss: 0.820157  [   32/  118]
train() client id: f_00009-6-1 loss: 0.706209  [   64/  118]
train() client id: f_00009-6-2 loss: 0.864972  [   96/  118]
train() client id: f_00009-7-0 loss: 0.817495  [   32/  118]
train() client id: f_00009-7-1 loss: 0.738674  [   64/  118]
train() client id: f_00009-7-2 loss: 0.814485  [   96/  118]
train() client id: f_00009-8-0 loss: 0.806506  [   32/  118]
train() client id: f_00009-8-1 loss: 0.857342  [   64/  118]
train() client id: f_00009-8-2 loss: 0.756352  [   96/  118]
train() client id: f_00009-9-0 loss: 0.764394  [   32/  118]
train() client id: f_00009-9-1 loss: 0.836285  [   64/  118]
train() client id: f_00009-9-2 loss: 0.692826  [   96/  118]
train() client id: f_00009-10-0 loss: 0.703251  [   32/  118]
train() client id: f_00009-10-1 loss: 0.861705  [   64/  118]
train() client id: f_00009-10-2 loss: 0.721747  [   96/  118]
train() client id: f_00009-11-0 loss: 0.755015  [   32/  118]
train() client id: f_00009-11-1 loss: 0.815395  [   64/  118]
train() client id: f_00009-11-2 loss: 0.732999  [   96/  118]
At round 11 accuracy: 0.6339522546419099
At round 11 training accuracy: 0.5855130784708249
At round 11 training loss: 0.8409157512771545
update_location
xs = -3.905658 4.200318 75.009024 18.811294 0.979296 3.956410 -37.443192 -16.324852 59.663977 -2.060879 
ys = 67.587959 50.555839 1.320614 -37.455176 29.350187 12.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 10.288573831882571
ys mean: 14.394142535528704
dists_uav = 120.761693 112.131777 125.012390 108.428571 104.222802 100.895271 106.812374 101.327080 117.764426 100.101244 
uav_gains = -102.048464 -101.243300 -102.424207 -100.878644 -100.449097 -100.096788 -100.715580 -100.143157 -101.775517 -100.011003 
uav_gains_db_mean: -100.97857575153994
dists_bs = 202.016386 218.172187 304.415902 287.576111 228.435069 241.515313 224.660879 235.617608 282.580431 243.204509 
bs_gains = -104.117884 -105.053442 -109.104145 -108.412138 -105.612417 -106.289510 -105.409828 -105.988876 -108.199037 -106.374265 
bs_gains_db_mean: -106.45615406974218
Round 12
-------------------------------
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.40743039 19.62699225  9.26366007  3.31236285 22.63944362 10.91364539
  4.11791779 13.28077427  9.76466884  8.86129599]
obj_prev = 111.18819146740302
eta_min = 1.3309903798194387e-10	eta_max = 0.9206473861818304
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 25.855891314518058	eta = 0.9090909090909091
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 44.77490344422677	eta = 0.5249672010962586
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 35.73954375539702	eta = 0.6576848294802706
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.11938254858583	eta = 0.6889150384535667
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.039315064938776	eta = 0.6905355085914164
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.039105597683566	eta = 0.6905397579562299
eta = 0.6905397579562299
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [0.03055476 0.064262   0.03006976 0.01042742 0.07420442 0.03540472
 0.0130949  0.04340716 0.03152475 0.02861478]
ene_total = [2.91251884 5.6185508  2.88405621 1.31991488 6.41048895 3.41632264
 1.52386453 3.86970026 3.19241569 2.89127279]
ti_comp = [0.30907982 0.29827603 0.3078889  0.3125449  0.295892   0.29281253
 0.31300078 0.31455258 0.28279738 0.29241128]
ti_coms = [0.06887363 0.07967742 0.07006456 0.06540855 0.08206146 0.08514092
 0.06495268 0.06340088 0.09515608 0.08554218]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.86627385e-05 1.86425773e-04 1.79259442e-05 7.25413394e-07
 2.91677471e-04 3.23507450e-05 1.43250282e-06 5.16627645e-05
 2.44841111e-05 1.71262716e-05]
ene_total = [0.53720059 0.6342901  0.54640715 0.5088514  0.66102208 0.66480406
 0.50536026 0.49719653 0.74209725 0.66674105]
optimize_network iter = 0 obj = 5.96397047673148
eta = 0.6905397579562299
freqs = [4.94285918e+07 1.07722361e+08 4.88321676e+07 1.66814724e+07
 1.25391054e+08 6.04562965e+07 2.09183144e+07 6.89982586e+07
 5.57373503e+07 4.89289881e+07]
eta_min = 0.6905397579563017	eta_max = 0.6905397579562269
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 0.04594401078389732	eta = 0.909090909090909
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 20.175396261636216	eta = 0.002070208782478131
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.0926044394311156	eta = 0.019959473345172857
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.034227485351597	eta = 0.020532257494100595
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.0342105771628316	eta = 0.02053242815651352
eta = 0.02053242815651352
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.95847449e-04 1.95635877e-03 1.88115503e-04 7.61251424e-06
 3.06087387e-03 3.39489881e-04 1.50327361e-05 5.42150909e-04
 2.56937143e-04 1.79723710e-04]
ene_total = [0.17408175 0.24390138 0.17681862 0.16093954 0.27690584 0.21759158
 0.16000151 0.16914252 0.24017663 0.21465121]
ti_comp = [0.30907982 0.29827603 0.3078889  0.3125449  0.295892   0.29281253
 0.31300078 0.31455258 0.28279738 0.29241128]
ti_coms = [0.06887363 0.07967742 0.07006456 0.06540855 0.08206146 0.08514092
 0.06495268 0.06340088 0.09515608 0.08554218]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.86627385e-05 1.86425773e-04 1.79259442e-05 7.25413394e-07
 2.91677471e-04 3.23507450e-05 1.43250282e-06 5.16627645e-05
 2.44841111e-05 1.71262716e-05]
ene_total = [0.53720059 0.6342901  0.54640715 0.5088514  0.66102208 0.66480406
 0.50536026 0.49719653 0.74209725 0.66674105]
optimize_network iter = 1 obj = 5.963970476732851
eta = 0.6905397579563017
freqs = [4.94285918e+07 1.07722361e+08 4.88321676e+07 1.66814724e+07
 1.25391054e+08 6.04562965e+07 2.09183144e+07 6.89982586e+07
 5.57373503e+07 4.89289881e+07]
Done!
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.84704838e-05 1.84505302e-04 1.77412796e-05 7.17940527e-07
 2.88672747e-04 3.20174828e-05 1.41774584e-06 5.11305590e-05
 2.42318873e-05 1.69498448e-05]
ene_total = [0.00690583 0.00815225 0.0070242  0.00654157 0.00849482 0.00854611
 0.00649669 0.00639122 0.00953984 0.00857117]
At round 12 energy consumption: 0.07666369174488584
At round 12 eta: 0.6905397579563017
At round 12 a_n: 24.072052692622567
At round 12 local rounds: 12.124905073660063
At round 12 global rounds: 77.78722246725123
gradient difference: 0.40003490447998047
train() client id: f_00000-0-0 loss: 1.278932  [   32/  126]
train() client id: f_00000-0-1 loss: 1.025193  [   64/  126]
train() client id: f_00000-0-2 loss: 0.862327  [   96/  126]
train() client id: f_00000-1-0 loss: 1.091019  [   32/  126]
train() client id: f_00000-1-1 loss: 0.988266  [   64/  126]
train() client id: f_00000-1-2 loss: 0.936904  [   96/  126]
train() client id: f_00000-2-0 loss: 0.917240  [   32/  126]
train() client id: f_00000-2-1 loss: 0.995530  [   64/  126]
train() client id: f_00000-2-2 loss: 0.930072  [   96/  126]
train() client id: f_00000-3-0 loss: 0.923183  [   32/  126]
train() client id: f_00000-3-1 loss: 0.785659  [   64/  126]
train() client id: f_00000-3-2 loss: 0.854442  [   96/  126]
train() client id: f_00000-4-0 loss: 0.933285  [   32/  126]
train() client id: f_00000-4-1 loss: 0.910445  [   64/  126]
train() client id: f_00000-4-2 loss: 0.760268  [   96/  126]
train() client id: f_00000-5-0 loss: 0.805697  [   32/  126]
train() client id: f_00000-5-1 loss: 0.949416  [   64/  126]
train() client id: f_00000-5-2 loss: 0.795346  [   96/  126]
train() client id: f_00000-6-0 loss: 0.884306  [   32/  126]
train() client id: f_00000-6-1 loss: 0.780447  [   64/  126]
train() client id: f_00000-6-2 loss: 0.757629  [   96/  126]
train() client id: f_00000-7-0 loss: 0.756768  [   32/  126]
train() client id: f_00000-7-1 loss: 0.807949  [   64/  126]
train() client id: f_00000-7-2 loss: 0.805243  [   96/  126]
train() client id: f_00000-8-0 loss: 0.681382  [   32/  126]
train() client id: f_00000-8-1 loss: 0.817117  [   64/  126]
train() client id: f_00000-8-2 loss: 0.863585  [   96/  126]
train() client id: f_00000-9-0 loss: 0.863388  [   32/  126]
train() client id: f_00000-9-1 loss: 0.817341  [   64/  126]
train() client id: f_00000-9-2 loss: 0.839921  [   96/  126]
train() client id: f_00000-10-0 loss: 0.948157  [   32/  126]
train() client id: f_00000-10-1 loss: 0.883258  [   64/  126]
train() client id: f_00000-10-2 loss: 0.684629  [   96/  126]
train() client id: f_00000-11-0 loss: 0.689031  [   32/  126]
train() client id: f_00000-11-1 loss: 0.977067  [   64/  126]
train() client id: f_00000-11-2 loss: 0.830847  [   96/  126]
train() client id: f_00001-0-0 loss: 0.429839  [   32/  265]
train() client id: f_00001-0-1 loss: 0.403928  [   64/  265]
train() client id: f_00001-0-2 loss: 0.418669  [   96/  265]
train() client id: f_00001-0-3 loss: 0.436970  [  128/  265]
train() client id: f_00001-0-4 loss: 0.452338  [  160/  265]
train() client id: f_00001-0-5 loss: 0.565452  [  192/  265]
train() client id: f_00001-0-6 loss: 0.479823  [  224/  265]
train() client id: f_00001-0-7 loss: 0.469828  [  256/  265]
train() client id: f_00001-1-0 loss: 0.456901  [   32/  265]
train() client id: f_00001-1-1 loss: 0.424176  [   64/  265]
train() client id: f_00001-1-2 loss: 0.411625  [   96/  265]
train() client id: f_00001-1-3 loss: 0.383465  [  128/  265]
train() client id: f_00001-1-4 loss: 0.646218  [  160/  265]
train() client id: f_00001-1-5 loss: 0.421120  [  192/  265]
train() client id: f_00001-1-6 loss: 0.412094  [  224/  265]
train() client id: f_00001-1-7 loss: 0.422437  [  256/  265]
train() client id: f_00001-2-0 loss: 0.363486  [   32/  265]
train() client id: f_00001-2-1 loss: 0.373429  [   64/  265]
train() client id: f_00001-2-2 loss: 0.500123  [   96/  265]
train() client id: f_00001-2-3 loss: 0.491505  [  128/  265]
train() client id: f_00001-2-4 loss: 0.363329  [  160/  265]
train() client id: f_00001-2-5 loss: 0.371343  [  192/  265]
train() client id: f_00001-2-6 loss: 0.621143  [  224/  265]
train() client id: f_00001-2-7 loss: 0.425099  [  256/  265]
train() client id: f_00001-3-0 loss: 0.492703  [   32/  265]
train() client id: f_00001-3-1 loss: 0.471805  [   64/  265]
train() client id: f_00001-3-2 loss: 0.405406  [   96/  265]
train() client id: f_00001-3-3 loss: 0.410876  [  128/  265]
train() client id: f_00001-3-4 loss: 0.481987  [  160/  265]
train() client id: f_00001-3-5 loss: 0.400028  [  192/  265]
train() client id: f_00001-3-6 loss: 0.383313  [  224/  265]
train() client id: f_00001-3-7 loss: 0.415075  [  256/  265]
train() client id: f_00001-4-0 loss: 0.354669  [   32/  265]
train() client id: f_00001-4-1 loss: 0.455821  [   64/  265]
train() client id: f_00001-4-2 loss: 0.355737  [   96/  265]
train() client id: f_00001-4-3 loss: 0.330813  [  128/  265]
train() client id: f_00001-4-4 loss: 0.480527  [  160/  265]
train() client id: f_00001-4-5 loss: 0.463618  [  192/  265]
train() client id: f_00001-4-6 loss: 0.545160  [  224/  265]
train() client id: f_00001-4-7 loss: 0.382163  [  256/  265]
train() client id: f_00001-5-0 loss: 0.478288  [   32/  265]
train() client id: f_00001-5-1 loss: 0.470994  [   64/  265]
train() client id: f_00001-5-2 loss: 0.362544  [   96/  265]
train() client id: f_00001-5-3 loss: 0.328625  [  128/  265]
train() client id: f_00001-5-4 loss: 0.402093  [  160/  265]
train() client id: f_00001-5-5 loss: 0.376121  [  192/  265]
train() client id: f_00001-5-6 loss: 0.438922  [  224/  265]
train() client id: f_00001-5-7 loss: 0.470549  [  256/  265]
train() client id: f_00001-6-0 loss: 0.383148  [   32/  265]
train() client id: f_00001-6-1 loss: 0.381786  [   64/  265]
train() client id: f_00001-6-2 loss: 0.474395  [   96/  265]
train() client id: f_00001-6-3 loss: 0.335664  [  128/  265]
train() client id: f_00001-6-4 loss: 0.485549  [  160/  265]
train() client id: f_00001-6-5 loss: 0.483154  [  192/  265]
train() client id: f_00001-6-6 loss: 0.368384  [  224/  265]
train() client id: f_00001-6-7 loss: 0.462888  [  256/  265]
train() client id: f_00001-7-0 loss: 0.452055  [   32/  265]
train() client id: f_00001-7-1 loss: 0.395452  [   64/  265]
train() client id: f_00001-7-2 loss: 0.433206  [   96/  265]
train() client id: f_00001-7-3 loss: 0.434949  [  128/  265]
train() client id: f_00001-7-4 loss: 0.334043  [  160/  265]
train() client id: f_00001-7-5 loss: 0.425020  [  192/  265]
train() client id: f_00001-7-6 loss: 0.396015  [  224/  265]
train() client id: f_00001-7-7 loss: 0.490337  [  256/  265]
train() client id: f_00001-8-0 loss: 0.333360  [   32/  265]
train() client id: f_00001-8-1 loss: 0.370237  [   64/  265]
train() client id: f_00001-8-2 loss: 0.338575  [   96/  265]
train() client id: f_00001-8-3 loss: 0.560157  [  128/  265]
train() client id: f_00001-8-4 loss: 0.387669  [  160/  265]
train() client id: f_00001-8-5 loss: 0.477208  [  192/  265]
train() client id: f_00001-8-6 loss: 0.386858  [  224/  265]
train() client id: f_00001-8-7 loss: 0.377321  [  256/  265]
train() client id: f_00001-9-0 loss: 0.324294  [   32/  265]
train() client id: f_00001-9-1 loss: 0.433680  [   64/  265]
train() client id: f_00001-9-2 loss: 0.495876  [   96/  265]
train() client id: f_00001-9-3 loss: 0.392845  [  128/  265]
train() client id: f_00001-9-4 loss: 0.370541  [  160/  265]
train() client id: f_00001-9-5 loss: 0.487436  [  192/  265]
train() client id: f_00001-9-6 loss: 0.431427  [  224/  265]
train() client id: f_00001-9-7 loss: 0.377095  [  256/  265]
train() client id: f_00001-10-0 loss: 0.415065  [   32/  265]
train() client id: f_00001-10-1 loss: 0.378514  [   64/  265]
train() client id: f_00001-10-2 loss: 0.352942  [   96/  265]
train() client id: f_00001-10-3 loss: 0.499490  [  128/  265]
train() client id: f_00001-10-4 loss: 0.501103  [  160/  265]
train() client id: f_00001-10-5 loss: 0.470389  [  192/  265]
train() client id: f_00001-10-6 loss: 0.415985  [  224/  265]
train() client id: f_00001-10-7 loss: 0.330734  [  256/  265]
train() client id: f_00001-11-0 loss: 0.569852  [   32/  265]
train() client id: f_00001-11-1 loss: 0.513019  [   64/  265]
train() client id: f_00001-11-2 loss: 0.418149  [   96/  265]
train() client id: f_00001-11-3 loss: 0.314696  [  128/  265]
train() client id: f_00001-11-4 loss: 0.467043  [  160/  265]
train() client id: f_00001-11-5 loss: 0.332974  [  192/  265]
train() client id: f_00001-11-6 loss: 0.381027  [  224/  265]
train() client id: f_00001-11-7 loss: 0.371610  [  256/  265]
train() client id: f_00002-0-0 loss: 1.185065  [   32/  124]
train() client id: f_00002-0-1 loss: 1.129768  [   64/  124]
train() client id: f_00002-0-2 loss: 1.265484  [   96/  124]
train() client id: f_00002-1-0 loss: 1.066745  [   32/  124]
train() client id: f_00002-1-1 loss: 1.105100  [   64/  124]
train() client id: f_00002-1-2 loss: 1.334982  [   96/  124]
train() client id: f_00002-2-0 loss: 1.073958  [   32/  124]
train() client id: f_00002-2-1 loss: 1.313373  [   64/  124]
train() client id: f_00002-2-2 loss: 0.967685  [   96/  124]
train() client id: f_00002-3-0 loss: 0.893360  [   32/  124]
train() client id: f_00002-3-1 loss: 1.101386  [   64/  124]
train() client id: f_00002-3-2 loss: 1.280256  [   96/  124]
train() client id: f_00002-4-0 loss: 1.072018  [   32/  124]
train() client id: f_00002-4-1 loss: 1.035074  [   64/  124]
train() client id: f_00002-4-2 loss: 1.005021  [   96/  124]
train() client id: f_00002-5-0 loss: 1.179558  [   32/  124]
train() client id: f_00002-5-1 loss: 1.065552  [   64/  124]
train() client id: f_00002-5-2 loss: 0.997639  [   96/  124]
train() client id: f_00002-6-0 loss: 0.952894  [   32/  124]
train() client id: f_00002-6-1 loss: 0.926740  [   64/  124]
train() client id: f_00002-6-2 loss: 1.055532  [   96/  124]
train() client id: f_00002-7-0 loss: 0.900026  [   32/  124]
train() client id: f_00002-7-1 loss: 1.175174  [   64/  124]
train() client id: f_00002-7-2 loss: 0.979874  [   96/  124]
train() client id: f_00002-8-0 loss: 0.964870  [   32/  124]
train() client id: f_00002-8-1 loss: 0.979993  [   64/  124]
train() client id: f_00002-8-2 loss: 0.892672  [   96/  124]
train() client id: f_00002-9-0 loss: 0.872946  [   32/  124]
train() client id: f_00002-9-1 loss: 1.053341  [   64/  124]
train() client id: f_00002-9-2 loss: 1.176681  [   96/  124]
train() client id: f_00002-10-0 loss: 0.832947  [   32/  124]
train() client id: f_00002-10-1 loss: 1.002839  [   64/  124]
train() client id: f_00002-10-2 loss: 1.038416  [   96/  124]
train() client id: f_00002-11-0 loss: 0.896780  [   32/  124]
train() client id: f_00002-11-1 loss: 0.941671  [   64/  124]
train() client id: f_00002-11-2 loss: 1.110867  [   96/  124]
train() client id: f_00003-0-0 loss: 0.936418  [   32/   43]
train() client id: f_00003-1-0 loss: 0.877572  [   32/   43]
train() client id: f_00003-2-0 loss: 0.894589  [   32/   43]
train() client id: f_00003-3-0 loss: 0.840162  [   32/   43]
train() client id: f_00003-4-0 loss: 0.994538  [   32/   43]
train() client id: f_00003-5-0 loss: 1.099311  [   32/   43]
train() client id: f_00003-6-0 loss: 0.907614  [   32/   43]
train() client id: f_00003-7-0 loss: 0.927799  [   32/   43]
train() client id: f_00003-8-0 loss: 0.899951  [   32/   43]
train() client id: f_00003-9-0 loss: 0.848326  [   32/   43]
train() client id: f_00003-10-0 loss: 0.998058  [   32/   43]
train() client id: f_00003-11-0 loss: 0.877003  [   32/   43]
train() client id: f_00004-0-0 loss: 0.886394  [   32/  306]
train() client id: f_00004-0-1 loss: 0.908316  [   64/  306]
train() client id: f_00004-0-2 loss: 0.829716  [   96/  306]
train() client id: f_00004-0-3 loss: 0.870033  [  128/  306]
train() client id: f_00004-0-4 loss: 0.984908  [  160/  306]
train() client id: f_00004-0-5 loss: 0.986666  [  192/  306]
train() client id: f_00004-0-6 loss: 0.812599  [  224/  306]
train() client id: f_00004-0-7 loss: 0.874256  [  256/  306]
train() client id: f_00004-0-8 loss: 0.933620  [  288/  306]
train() client id: f_00004-1-0 loss: 0.908704  [   32/  306]
train() client id: f_00004-1-1 loss: 0.827083  [   64/  306]
train() client id: f_00004-1-2 loss: 0.924238  [   96/  306]
train() client id: f_00004-1-3 loss: 0.831044  [  128/  306]
train() client id: f_00004-1-4 loss: 0.919114  [  160/  306]
train() client id: f_00004-1-5 loss: 0.954446  [  192/  306]
train() client id: f_00004-1-6 loss: 0.821347  [  224/  306]
train() client id: f_00004-1-7 loss: 0.844995  [  256/  306]
train() client id: f_00004-1-8 loss: 0.850237  [  288/  306]
train() client id: f_00004-2-0 loss: 0.871246  [   32/  306]
train() client id: f_00004-2-1 loss: 0.882733  [   64/  306]
train() client id: f_00004-2-2 loss: 0.937899  [   96/  306]
train() client id: f_00004-2-3 loss: 0.870384  [  128/  306]
train() client id: f_00004-2-4 loss: 0.864087  [  160/  306]
train() client id: f_00004-2-5 loss: 0.962430  [  192/  306]
train() client id: f_00004-2-6 loss: 0.995582  [  224/  306]
train() client id: f_00004-2-7 loss: 0.840875  [  256/  306]
train() client id: f_00004-2-8 loss: 0.892170  [  288/  306]
train() client id: f_00004-3-0 loss: 0.868135  [   32/  306]
train() client id: f_00004-3-1 loss: 0.939147  [   64/  306]
train() client id: f_00004-3-2 loss: 0.915330  [   96/  306]
train() client id: f_00004-3-3 loss: 0.909797  [  128/  306]
train() client id: f_00004-3-4 loss: 0.899155  [  160/  306]
train() client id: f_00004-3-5 loss: 0.863116  [  192/  306]
train() client id: f_00004-3-6 loss: 0.852924  [  224/  306]
train() client id: f_00004-3-7 loss: 0.948190  [  256/  306]
train() client id: f_00004-3-8 loss: 0.892439  [  288/  306]
train() client id: f_00004-4-0 loss: 0.912272  [   32/  306]
train() client id: f_00004-4-1 loss: 0.892974  [   64/  306]
train() client id: f_00004-4-2 loss: 0.873145  [   96/  306]
train() client id: f_00004-4-3 loss: 0.783978  [  128/  306]
train() client id: f_00004-4-4 loss: 0.840162  [  160/  306]
train() client id: f_00004-4-5 loss: 0.933818  [  192/  306]
train() client id: f_00004-4-6 loss: 0.890735  [  224/  306]
train() client id: f_00004-4-7 loss: 0.869811  [  256/  306]
train() client id: f_00004-4-8 loss: 0.945349  [  288/  306]
train() client id: f_00004-5-0 loss: 0.892974  [   32/  306]
train() client id: f_00004-5-1 loss: 0.905388  [   64/  306]
train() client id: f_00004-5-2 loss: 0.863230  [   96/  306]
train() client id: f_00004-5-3 loss: 0.937363  [  128/  306]
train() client id: f_00004-5-4 loss: 0.915450  [  160/  306]
train() client id: f_00004-5-5 loss: 0.819450  [  192/  306]
train() client id: f_00004-5-6 loss: 0.765956  [  224/  306]
train() client id: f_00004-5-7 loss: 1.003142  [  256/  306]
train() client id: f_00004-5-8 loss: 0.922551  [  288/  306]
train() client id: f_00004-6-0 loss: 0.787580  [   32/  306]
train() client id: f_00004-6-1 loss: 0.888964  [   64/  306]
train() client id: f_00004-6-2 loss: 0.867156  [   96/  306]
train() client id: f_00004-6-3 loss: 0.896201  [  128/  306]
train() client id: f_00004-6-4 loss: 0.895881  [  160/  306]
train() client id: f_00004-6-5 loss: 0.958765  [  192/  306]
train() client id: f_00004-6-6 loss: 0.988485  [  224/  306]
train() client id: f_00004-6-7 loss: 0.811571  [  256/  306]
train() client id: f_00004-6-8 loss: 0.909953  [  288/  306]
train() client id: f_00004-7-0 loss: 0.935551  [   32/  306]
train() client id: f_00004-7-1 loss: 0.927637  [   64/  306]
train() client id: f_00004-7-2 loss: 0.803260  [   96/  306]
train() client id: f_00004-7-3 loss: 0.749289  [  128/  306]
train() client id: f_00004-7-4 loss: 0.986764  [  160/  306]
train() client id: f_00004-7-5 loss: 0.952316  [  192/  306]
train() client id: f_00004-7-6 loss: 0.824223  [  224/  306]
train() client id: f_00004-7-7 loss: 0.942148  [  256/  306]
train() client id: f_00004-7-8 loss: 0.853016  [  288/  306]
train() client id: f_00004-8-0 loss: 0.924517  [   32/  306]
train() client id: f_00004-8-1 loss: 0.829811  [   64/  306]
train() client id: f_00004-8-2 loss: 1.001255  [   96/  306]
train() client id: f_00004-8-3 loss: 0.898156  [  128/  306]
train() client id: f_00004-8-4 loss: 0.966806  [  160/  306]
train() client id: f_00004-8-5 loss: 0.800827  [  192/  306]
train() client id: f_00004-8-6 loss: 0.967588  [  224/  306]
train() client id: f_00004-8-7 loss: 0.842508  [  256/  306]
train() client id: f_00004-8-8 loss: 0.752857  [  288/  306]
train() client id: f_00004-9-0 loss: 0.892496  [   32/  306]
train() client id: f_00004-9-1 loss: 0.806835  [   64/  306]
train() client id: f_00004-9-2 loss: 0.884081  [   96/  306]
train() client id: f_00004-9-3 loss: 1.021066  [  128/  306]
train() client id: f_00004-9-4 loss: 0.856484  [  160/  306]
train() client id: f_00004-9-5 loss: 0.864259  [  192/  306]
train() client id: f_00004-9-6 loss: 0.876489  [  224/  306]
train() client id: f_00004-9-7 loss: 0.849370  [  256/  306]
train() client id: f_00004-9-8 loss: 0.896558  [  288/  306]
train() client id: f_00004-10-0 loss: 0.863189  [   32/  306]
train() client id: f_00004-10-1 loss: 0.995989  [   64/  306]
train() client id: f_00004-10-2 loss: 0.970093  [   96/  306]
train() client id: f_00004-10-3 loss: 0.958671  [  128/  306]
train() client id: f_00004-10-4 loss: 0.853361  [  160/  306]
train() client id: f_00004-10-5 loss: 0.770036  [  192/  306]
train() client id: f_00004-10-6 loss: 0.862341  [  224/  306]
train() client id: f_00004-10-7 loss: 0.831792  [  256/  306]
train() client id: f_00004-10-8 loss: 0.890840  [  288/  306]
train() client id: f_00004-11-0 loss: 0.905938  [   32/  306]
train() client id: f_00004-11-1 loss: 0.938514  [   64/  306]
train() client id: f_00004-11-2 loss: 0.933229  [   96/  306]
train() client id: f_00004-11-3 loss: 0.918261  [  128/  306]
train() client id: f_00004-11-4 loss: 0.843928  [  160/  306]
train() client id: f_00004-11-5 loss: 0.883585  [  192/  306]
train() client id: f_00004-11-6 loss: 0.953301  [  224/  306]
train() client id: f_00004-11-7 loss: 0.853227  [  256/  306]
train() client id: f_00004-11-8 loss: 0.851466  [  288/  306]
train() client id: f_00005-0-0 loss: 1.044640  [   32/  146]
train() client id: f_00005-0-1 loss: 0.721196  [   64/  146]
train() client id: f_00005-0-2 loss: 0.782362  [   96/  146]
train() client id: f_00005-0-3 loss: 0.754308  [  128/  146]
train() client id: f_00005-1-0 loss: 0.864511  [   32/  146]
train() client id: f_00005-1-1 loss: 0.808791  [   64/  146]
train() client id: f_00005-1-2 loss: 0.823936  [   96/  146]
train() client id: f_00005-1-3 loss: 0.738499  [  128/  146]
train() client id: f_00005-2-0 loss: 0.938700  [   32/  146]
train() client id: f_00005-2-1 loss: 0.756385  [   64/  146]
train() client id: f_00005-2-2 loss: 0.593336  [   96/  146]
train() client id: f_00005-2-3 loss: 0.896344  [  128/  146]
train() client id: f_00005-3-0 loss: 0.880031  [   32/  146]
train() client id: f_00005-3-1 loss: 0.743357  [   64/  146]
train() client id: f_00005-3-2 loss: 0.772929  [   96/  146]
train() client id: f_00005-3-3 loss: 0.742162  [  128/  146]
train() client id: f_00005-4-0 loss: 0.857759  [   32/  146]
train() client id: f_00005-4-1 loss: 0.759672  [   64/  146]
train() client id: f_00005-4-2 loss: 0.857152  [   96/  146]
train() client id: f_00005-4-3 loss: 0.742006  [  128/  146]
train() client id: f_00005-5-0 loss: 1.068041  [   32/  146]
train() client id: f_00005-5-1 loss: 0.718447  [   64/  146]
train() client id: f_00005-5-2 loss: 0.556964  [   96/  146]
train() client id: f_00005-5-3 loss: 0.734419  [  128/  146]
train() client id: f_00005-6-0 loss: 0.724058  [   32/  146]
train() client id: f_00005-6-1 loss: 0.783245  [   64/  146]
train() client id: f_00005-6-2 loss: 0.729515  [   96/  146]
train() client id: f_00005-6-3 loss: 0.684593  [  128/  146]
train() client id: f_00005-7-0 loss: 0.579379  [   32/  146]
train() client id: f_00005-7-1 loss: 0.838664  [   64/  146]
train() client id: f_00005-7-2 loss: 0.766006  [   96/  146]
train() client id: f_00005-7-3 loss: 0.797411  [  128/  146]
train() client id: f_00005-8-0 loss: 0.727509  [   32/  146]
train() client id: f_00005-8-1 loss: 0.746258  [   64/  146]
train() client id: f_00005-8-2 loss: 0.719805  [   96/  146]
train() client id: f_00005-8-3 loss: 0.933138  [  128/  146]
train() client id: f_00005-9-0 loss: 0.832351  [   32/  146]
train() client id: f_00005-9-1 loss: 0.823154  [   64/  146]
train() client id: f_00005-9-2 loss: 0.568764  [   96/  146]
train() client id: f_00005-9-3 loss: 0.829622  [  128/  146]
train() client id: f_00005-10-0 loss: 0.745681  [   32/  146]
train() client id: f_00005-10-1 loss: 0.631790  [   64/  146]
train() client id: f_00005-10-2 loss: 0.542685  [   96/  146]
train() client id: f_00005-10-3 loss: 1.080014  [  128/  146]
train() client id: f_00005-11-0 loss: 0.799934  [   32/  146]
train() client id: f_00005-11-1 loss: 0.618439  [   64/  146]
train() client id: f_00005-11-2 loss: 0.961361  [   96/  146]
train() client id: f_00005-11-3 loss: 0.760360  [  128/  146]
train() client id: f_00006-0-0 loss: 0.553527  [   32/   54]
train() client id: f_00006-1-0 loss: 0.486824  [   32/   54]
train() client id: f_00006-2-0 loss: 0.521665  [   32/   54]
train() client id: f_00006-3-0 loss: 0.563168  [   32/   54]
train() client id: f_00006-4-0 loss: 0.518310  [   32/   54]
train() client id: f_00006-5-0 loss: 0.525659  [   32/   54]
train() client id: f_00006-6-0 loss: 0.504416  [   32/   54]
train() client id: f_00006-7-0 loss: 0.473995  [   32/   54]
train() client id: f_00006-8-0 loss: 0.465984  [   32/   54]
train() client id: f_00006-9-0 loss: 0.472224  [   32/   54]
train() client id: f_00006-10-0 loss: 0.566365  [   32/   54]
train() client id: f_00006-11-0 loss: 0.483992  [   32/   54]
train() client id: f_00007-0-0 loss: 0.720827  [   32/  179]
train() client id: f_00007-0-1 loss: 0.523825  [   64/  179]
train() client id: f_00007-0-2 loss: 0.647059  [   96/  179]
train() client id: f_00007-0-3 loss: 0.623949  [  128/  179]
train() client id: f_00007-0-4 loss: 0.481393  [  160/  179]
train() client id: f_00007-1-0 loss: 0.473761  [   32/  179]
train() client id: f_00007-1-1 loss: 0.540367  [   64/  179]
train() client id: f_00007-1-2 loss: 0.514777  [   96/  179]
train() client id: f_00007-1-3 loss: 0.688171  [  128/  179]
train() client id: f_00007-1-4 loss: 0.589250  [  160/  179]
train() client id: f_00007-2-0 loss: 0.452567  [   32/  179]
train() client id: f_00007-2-1 loss: 0.615295  [   64/  179]
train() client id: f_00007-2-2 loss: 0.671662  [   96/  179]
train() client id: f_00007-2-3 loss: 0.490608  [  128/  179]
train() client id: f_00007-2-4 loss: 0.562868  [  160/  179]
train() client id: f_00007-3-0 loss: 0.484685  [   32/  179]
train() client id: f_00007-3-1 loss: 0.587358  [   64/  179]
train() client id: f_00007-3-2 loss: 0.502535  [   96/  179]
train() client id: f_00007-3-3 loss: 0.811167  [  128/  179]
train() client id: f_00007-3-4 loss: 0.406211  [  160/  179]
train() client id: f_00007-4-0 loss: 0.583643  [   32/  179]
train() client id: f_00007-4-1 loss: 0.474130  [   64/  179]
train() client id: f_00007-4-2 loss: 0.645914  [   96/  179]
train() client id: f_00007-4-3 loss: 0.591301  [  128/  179]
train() client id: f_00007-4-4 loss: 0.407098  [  160/  179]
train() client id: f_00007-5-0 loss: 0.534349  [   32/  179]
train() client id: f_00007-5-1 loss: 0.395168  [   64/  179]
train() client id: f_00007-5-2 loss: 0.646747  [   96/  179]
train() client id: f_00007-5-3 loss: 0.558938  [  128/  179]
train() client id: f_00007-5-4 loss: 0.645641  [  160/  179]
train() client id: f_00007-6-0 loss: 0.561303  [   32/  179]
train() client id: f_00007-6-1 loss: 0.407320  [   64/  179]
train() client id: f_00007-6-2 loss: 0.635823  [   96/  179]
train() client id: f_00007-6-3 loss: 0.530472  [  128/  179]
train() client id: f_00007-6-4 loss: 0.516668  [  160/  179]
train() client id: f_00007-7-0 loss: 0.459277  [   32/  179]
train() client id: f_00007-7-1 loss: 0.388915  [   64/  179]
train() client id: f_00007-7-2 loss: 0.546439  [   96/  179]
train() client id: f_00007-7-3 loss: 0.470713  [  128/  179]
train() client id: f_00007-7-4 loss: 0.508823  [  160/  179]
train() client id: f_00007-8-0 loss: 0.508459  [   32/  179]
train() client id: f_00007-8-1 loss: 0.431153  [   64/  179]
train() client id: f_00007-8-2 loss: 0.547276  [   96/  179]
train() client id: f_00007-8-3 loss: 0.556892  [  128/  179]
train() client id: f_00007-8-4 loss: 0.650020  [  160/  179]
train() client id: f_00007-9-0 loss: 0.401150  [   32/  179]
train() client id: f_00007-9-1 loss: 0.632208  [   64/  179]
train() client id: f_00007-9-2 loss: 0.474981  [   96/  179]
train() client id: f_00007-9-3 loss: 0.395971  [  128/  179]
train() client id: f_00007-9-4 loss: 0.699198  [  160/  179]
train() client id: f_00007-10-0 loss: 0.498034  [   32/  179]
train() client id: f_00007-10-1 loss: 0.473633  [   64/  179]
train() client id: f_00007-10-2 loss: 0.344945  [   96/  179]
train() client id: f_00007-10-3 loss: 0.702390  [  128/  179]
train() client id: f_00007-10-4 loss: 0.560672  [  160/  179]
train() client id: f_00007-11-0 loss: 0.479183  [   32/  179]
train() client id: f_00007-11-1 loss: 0.482315  [   64/  179]
train() client id: f_00007-11-2 loss: 0.447529  [   96/  179]
train() client id: f_00007-11-3 loss: 0.439184  [  128/  179]
train() client id: f_00007-11-4 loss: 0.737946  [  160/  179]
train() client id: f_00008-0-0 loss: 0.791296  [   32/  130]
train() client id: f_00008-0-1 loss: 0.843365  [   64/  130]
train() client id: f_00008-0-2 loss: 0.808359  [   96/  130]
train() client id: f_00008-0-3 loss: 0.871872  [  128/  130]
train() client id: f_00008-1-0 loss: 0.818496  [   32/  130]
train() client id: f_00008-1-1 loss: 0.875398  [   64/  130]
train() client id: f_00008-1-2 loss: 0.844540  [   96/  130]
train() client id: f_00008-1-3 loss: 0.783003  [  128/  130]
train() client id: f_00008-2-0 loss: 0.744521  [   32/  130]
train() client id: f_00008-2-1 loss: 0.918465  [   64/  130]
train() client id: f_00008-2-2 loss: 0.846660  [   96/  130]
train() client id: f_00008-2-3 loss: 0.793672  [  128/  130]
train() client id: f_00008-3-0 loss: 0.718917  [   32/  130]
train() client id: f_00008-3-1 loss: 0.740074  [   64/  130]
train() client id: f_00008-3-2 loss: 0.962400  [   96/  130]
train() client id: f_00008-3-3 loss: 0.868135  [  128/  130]
train() client id: f_00008-4-0 loss: 0.896459  [   32/  130]
train() client id: f_00008-4-1 loss: 0.836812  [   64/  130]
train() client id: f_00008-4-2 loss: 0.744599  [   96/  130]
train() client id: f_00008-4-3 loss: 0.793439  [  128/  130]
train() client id: f_00008-5-0 loss: 0.732905  [   32/  130]
train() client id: f_00008-5-1 loss: 0.881162  [   64/  130]
train() client id: f_00008-5-2 loss: 0.838828  [   96/  130]
train() client id: f_00008-5-3 loss: 0.849352  [  128/  130]
train() client id: f_00008-6-0 loss: 0.812451  [   32/  130]
train() client id: f_00008-6-1 loss: 0.890391  [   64/  130]
train() client id: f_00008-6-2 loss: 0.885137  [   96/  130]
train() client id: f_00008-6-3 loss: 0.725326  [  128/  130]
train() client id: f_00008-7-0 loss: 0.844357  [   32/  130]
train() client id: f_00008-7-1 loss: 0.867170  [   64/  130]
train() client id: f_00008-7-2 loss: 0.829180  [   96/  130]
train() client id: f_00008-7-3 loss: 0.770664  [  128/  130]
train() client id: f_00008-8-0 loss: 0.856055  [   32/  130]
train() client id: f_00008-8-1 loss: 0.821479  [   64/  130]
train() client id: f_00008-8-2 loss: 0.849239  [   96/  130]
train() client id: f_00008-8-3 loss: 0.789212  [  128/  130]
train() client id: f_00008-9-0 loss: 0.786465  [   32/  130]
train() client id: f_00008-9-1 loss: 0.817225  [   64/  130]
train() client id: f_00008-9-2 loss: 0.863253  [   96/  130]
train() client id: f_00008-9-3 loss: 0.836121  [  128/  130]
train() client id: f_00008-10-0 loss: 0.891137  [   32/  130]
train() client id: f_00008-10-1 loss: 0.746753  [   64/  130]
train() client id: f_00008-10-2 loss: 0.833133  [   96/  130]
train() client id: f_00008-10-3 loss: 0.824933  [  128/  130]
train() client id: f_00008-11-0 loss: 0.971079  [   32/  130]
train() client id: f_00008-11-1 loss: 0.811501  [   64/  130]
train() client id: f_00008-11-2 loss: 0.792843  [   96/  130]
train() client id: f_00008-11-3 loss: 0.730049  [  128/  130]
train() client id: f_00009-0-0 loss: 1.133676  [   32/  118]
train() client id: f_00009-0-1 loss: 1.030535  [   64/  118]
train() client id: f_00009-0-2 loss: 1.084699  [   96/  118]
train() client id: f_00009-1-0 loss: 1.011871  [   32/  118]
train() client id: f_00009-1-1 loss: 0.967729  [   64/  118]
train() client id: f_00009-1-2 loss: 1.118992  [   96/  118]
train() client id: f_00009-2-0 loss: 1.000026  [   32/  118]
train() client id: f_00009-2-1 loss: 0.912709  [   64/  118]
train() client id: f_00009-2-2 loss: 0.940093  [   96/  118]
train() client id: f_00009-3-0 loss: 0.827491  [   32/  118]
train() client id: f_00009-3-1 loss: 0.803635  [   64/  118]
train() client id: f_00009-3-2 loss: 1.124618  [   96/  118]
train() client id: f_00009-4-0 loss: 0.998584  [   32/  118]
train() client id: f_00009-4-1 loss: 0.924034  [   64/  118]
train() client id: f_00009-4-2 loss: 0.810075  [   96/  118]
train() client id: f_00009-5-0 loss: 0.982513  [   32/  118]
train() client id: f_00009-5-1 loss: 0.870247  [   64/  118]
train() client id: f_00009-5-2 loss: 0.796945  [   96/  118]
train() client id: f_00009-6-0 loss: 0.943786  [   32/  118]
train() client id: f_00009-6-1 loss: 0.915714  [   64/  118]
train() client id: f_00009-6-2 loss: 0.792499  [   96/  118]
train() client id: f_00009-7-0 loss: 0.912097  [   32/  118]
train() client id: f_00009-7-1 loss: 0.934330  [   64/  118]
train() client id: f_00009-7-2 loss: 0.842126  [   96/  118]
train() client id: f_00009-8-0 loss: 0.899248  [   32/  118]
train() client id: f_00009-8-1 loss: 0.761088  [   64/  118]
train() client id: f_00009-8-2 loss: 0.830485  [   96/  118]
train() client id: f_00009-9-0 loss: 0.989411  [   32/  118]
train() client id: f_00009-9-1 loss: 0.829574  [   64/  118]
train() client id: f_00009-9-2 loss: 0.769074  [   96/  118]
train() client id: f_00009-10-0 loss: 0.815618  [   32/  118]
train() client id: f_00009-10-1 loss: 0.720609  [   64/  118]
train() client id: f_00009-10-2 loss: 0.828262  [   96/  118]
train() client id: f_00009-11-0 loss: 1.009172  [   32/  118]
train() client id: f_00009-11-1 loss: 0.833120  [   64/  118]
train() client id: f_00009-11-2 loss: 0.746222  [   96/  118]
At round 12 accuracy: 0.6339522546419099
At round 12 training accuracy: 0.5868544600938967
At round 12 training loss: 0.8338486546193714
update_location
xs = -3.905658 4.200318 80.009024 18.811294 0.979296 3.956410 -42.443192 -21.324852 64.663977 -7.060879 
ys = 72.587959 55.555839 1.320614 -42.455176 34.350187 17.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 9.788573831882571
ys mean: 15.894142535528704
dists_uav = 123.629552 114.473114 128.074931 110.255643 105.739748 101.651351 108.666071 102.251775 120.374831 100.328799 
uav_gains = -102.303383 -101.467697 -102.687136 -101.060085 -100.605992 -100.177849 -100.902402 -100.241793 -102.013616 -100.035657 
uav_gains_db_mean: -101.14956098571665
dists_bs = 199.402858 215.359378 308.535462 291.289497 225.280009 238.186456 221.634705 232.279800 286.746124 239.674867 
bs_gains = -103.959538 -104.895645 -109.267602 -108.568154 -105.443294 -106.120737 -105.244917 -105.815380 -108.376991 -106.196489 
bs_gains_db_mean: -106.3888745130291
Round 13
-------------------------------
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.27548856 19.34614452  9.13389385  3.26646163 22.31552022 10.75645448
  4.0605729  13.0924169   9.62823145  8.73319217]
obj_prev = 109.60837667978073
eta_min = 9.621283683779976e-11	eta_max = 0.9207981058157783
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 25.487961404153893	eta = 0.9090909090909091
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 44.16384428532155	eta = 0.5246570894979226
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 35.24177796016038	eta = 0.6574831164866354
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.64173104871985	eta = 0.6887539160877387
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.56259776728117	eta = 0.6903778475206297
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.56239040785321	eta = 0.6903821129008305
eta = 0.6903821129008305
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [0.0305736  0.06430162 0.03008831 0.01043385 0.07425017 0.03542655
 0.01310297 0.04343393 0.03154419 0.02863242]
ene_total = [2.87721447 5.53384282 2.84956312 1.30490774 6.3139176  3.36162512
 1.50604088 3.81675253 3.15491369 2.84361242]
ti_comp = [0.31327504 0.30392366 0.31202998 0.31702896 0.30162658 0.29859963
 0.31747675 0.31928923 0.2867473  0.29824756]
ti_coms = [0.06967721 0.0790286  0.07092227 0.06592329 0.08132567 0.08435263
 0.0654755  0.06366303 0.09620495 0.08470469]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.81998670e-05 1.79894040e-04 1.74856014e-05 7.06343005e-07
 2.81211599e-04 3.11665129e-05 1.39497231e-06 5.02341124e-05
 2.38583025e-05 1.64930279e-05]
ene_total = [0.53540984 0.61947247 0.54489739 0.50529856 0.64484267 0.64887784
 0.50191943 0.49177151 0.73915545 0.65045154]
optimize_network iter = 0 obj = 5.882096701797337
eta = 0.6903821129008305
freqs = [4.87967396e+07 1.05785811e+08 4.82138059e+07 1.64556699e+07
 1.23082943e+08 5.93211620e+07 2.06361124e+07 6.80165848e+07
 5.50034670e+07 4.80010972e+07]
eta_min = 0.6903821129008529	eta_max = 0.6903821129008285
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 0.04372443773870311	eta = 0.909090909090909
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 19.912896282792712	eta = 0.0019961681258650003
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.05768598057864	eta = 0.019317568000433452
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.002030029818999	eta = 0.019854591720065342
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.0020149221690455	eta = 0.019854741547231138
eta = 0.019854741547231138
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.92521000e-04 1.90294690e-03 1.84965388e-04 7.47180525e-06
 2.97469966e-03 3.29684180e-04 1.47562323e-05 5.31384189e-04
 2.52376804e-04 1.74465793e-04]
ene_total = [0.17335085 0.23740048 0.17618224 0.15978245 0.26890909 0.21220106
 0.15887471 0.16699434 0.23902414 0.20929556]
ti_comp = [0.31327504 0.30392366 0.31202998 0.31702896 0.30162658 0.29859963
 0.31747675 0.31928923 0.2867473  0.29824756]
ti_coms = [0.06967721 0.0790286  0.07092227 0.06592329 0.08132567 0.08435263
 0.0654755  0.06366303 0.09620495 0.08470469]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.81998670e-05 1.79894040e-04 1.74856014e-05 7.06343005e-07
 2.81211599e-04 3.11665129e-05 1.39497231e-06 5.02341124e-05
 2.38583025e-05 1.64930279e-05]
ene_total = [0.53540984 0.61947247 0.54489739 0.50529856 0.64484267 0.64887784
 0.50191943 0.49177151 0.73915545 0.65045154]
optimize_network iter = 1 obj = 5.882096701797758
eta = 0.6903821129008529
freqs = [4.87967396e+07 1.05785811e+08 4.82138059e+07 1.64556699e+07
 1.23082943e+08 5.93211620e+07 2.06361124e+07 6.80165848e+07
 5.50034670e+07 4.80010972e+07]
Done!
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.80012808e-05 1.77931142e-04 1.72948088e-05 6.98635807e-07
 2.78143184e-04 3.08264423e-05 1.37975120e-06 4.96859874e-05
 2.35979748e-05 1.63130657e-05]
ene_total = [0.00698572 0.00808079 0.00710952 0.00659303 0.00841071 0.00846609
 0.00654893 0.00641599 0.00964409 0.00848678]
At round 13 energy consumption: 0.07674165610159829
At round 13 eta: 0.6903821129008529
At round 13 a_n: 23.72950684565325
At round 13 local rounds: 12.132381383768099
At round 13 global rounds: 76.641266006878
gradient difference: 0.4103463292121887
train() client id: f_00000-0-0 loss: 0.999921  [   32/  126]
train() client id: f_00000-0-1 loss: 1.183566  [   64/  126]
train() client id: f_00000-0-2 loss: 1.139878  [   96/  126]
train() client id: f_00000-1-0 loss: 1.038822  [   32/  126]
train() client id: f_00000-1-1 loss: 0.981815  [   64/  126]
train() client id: f_00000-1-2 loss: 1.138819  [   96/  126]
train() client id: f_00000-2-0 loss: 1.112268  [   32/  126]
train() client id: f_00000-2-1 loss: 0.788996  [   64/  126]
train() client id: f_00000-2-2 loss: 0.932368  [   96/  126]
train() client id: f_00000-3-0 loss: 0.847279  [   32/  126]
train() client id: f_00000-3-1 loss: 0.823222  [   64/  126]
train() client id: f_00000-3-2 loss: 0.916402  [   96/  126]
train() client id: f_00000-4-0 loss: 0.881661  [   32/  126]
train() client id: f_00000-4-1 loss: 1.035973  [   64/  126]
train() client id: f_00000-4-2 loss: 0.799926  [   96/  126]
train() client id: f_00000-5-0 loss: 0.930853  [   32/  126]
train() client id: f_00000-5-1 loss: 0.765782  [   64/  126]
train() client id: f_00000-5-2 loss: 0.961727  [   96/  126]
train() client id: f_00000-6-0 loss: 0.840288  [   32/  126]
train() client id: f_00000-6-1 loss: 0.776603  [   64/  126]
train() client id: f_00000-6-2 loss: 0.925417  [   96/  126]
train() client id: f_00000-7-0 loss: 1.016707  [   32/  126]
train() client id: f_00000-7-1 loss: 0.681925  [   64/  126]
train() client id: f_00000-7-2 loss: 0.870247  [   96/  126]
train() client id: f_00000-8-0 loss: 0.918267  [   32/  126]
train() client id: f_00000-8-1 loss: 0.959549  [   64/  126]
train() client id: f_00000-8-2 loss: 0.755039  [   96/  126]
train() client id: f_00000-9-0 loss: 0.718839  [   32/  126]
train() client id: f_00000-9-1 loss: 0.961209  [   64/  126]
train() client id: f_00000-9-2 loss: 0.885608  [   96/  126]
train() client id: f_00000-10-0 loss: 0.757272  [   32/  126]
train() client id: f_00000-10-1 loss: 0.878442  [   64/  126]
train() client id: f_00000-10-2 loss: 0.849581  [   96/  126]
train() client id: f_00000-11-0 loss: 0.844618  [   32/  126]
train() client id: f_00000-11-1 loss: 0.857397  [   64/  126]
train() client id: f_00000-11-2 loss: 0.871016  [   96/  126]
train() client id: f_00001-0-0 loss: 0.569905  [   32/  265]
train() client id: f_00001-0-1 loss: 0.421696  [   64/  265]
train() client id: f_00001-0-2 loss: 0.489905  [   96/  265]
train() client id: f_00001-0-3 loss: 0.595349  [  128/  265]
train() client id: f_00001-0-4 loss: 0.560092  [  160/  265]
train() client id: f_00001-0-5 loss: 0.475496  [  192/  265]
train() client id: f_00001-0-6 loss: 0.428462  [  224/  265]
train() client id: f_00001-0-7 loss: 0.491226  [  256/  265]
train() client id: f_00001-1-0 loss: 0.534669  [   32/  265]
train() client id: f_00001-1-1 loss: 0.469328  [   64/  265]
train() client id: f_00001-1-2 loss: 0.540490  [   96/  265]
train() client id: f_00001-1-3 loss: 0.584490  [  128/  265]
train() client id: f_00001-1-4 loss: 0.445047  [  160/  265]
train() client id: f_00001-1-5 loss: 0.482109  [  192/  265]
train() client id: f_00001-1-6 loss: 0.541124  [  224/  265]
train() client id: f_00001-1-7 loss: 0.410147  [  256/  265]
train() client id: f_00001-2-0 loss: 0.539815  [   32/  265]
train() client id: f_00001-2-1 loss: 0.486713  [   64/  265]
train() client id: f_00001-2-2 loss: 0.490370  [   96/  265]
train() client id: f_00001-2-3 loss: 0.561050  [  128/  265]
train() client id: f_00001-2-4 loss: 0.378757  [  160/  265]
train() client id: f_00001-2-5 loss: 0.574982  [  192/  265]
train() client id: f_00001-2-6 loss: 0.453155  [  224/  265]
train() client id: f_00001-2-7 loss: 0.452557  [  256/  265]
train() client id: f_00001-3-0 loss: 0.502608  [   32/  265]
train() client id: f_00001-3-1 loss: 0.431722  [   64/  265]
train() client id: f_00001-3-2 loss: 0.396573  [   96/  265]
train() client id: f_00001-3-3 loss: 0.396778  [  128/  265]
train() client id: f_00001-3-4 loss: 0.583800  [  160/  265]
train() client id: f_00001-3-5 loss: 0.620974  [  192/  265]
train() client id: f_00001-3-6 loss: 0.482627  [  224/  265]
train() client id: f_00001-3-7 loss: 0.476218  [  256/  265]
train() client id: f_00001-4-0 loss: 0.378776  [   32/  265]
train() client id: f_00001-4-1 loss: 0.528300  [   64/  265]
train() client id: f_00001-4-2 loss: 0.395674  [   96/  265]
train() client id: f_00001-4-3 loss: 0.401385  [  128/  265]
train() client id: f_00001-4-4 loss: 0.457795  [  160/  265]
train() client id: f_00001-4-5 loss: 0.634558  [  192/  265]
train() client id: f_00001-4-6 loss: 0.591205  [  224/  265]
train() client id: f_00001-4-7 loss: 0.471976  [  256/  265]
train() client id: f_00001-5-0 loss: 0.432920  [   32/  265]
train() client id: f_00001-5-1 loss: 0.426348  [   64/  265]
train() client id: f_00001-5-2 loss: 0.520329  [   96/  265]
train() client id: f_00001-5-3 loss: 0.391362  [  128/  265]
train() client id: f_00001-5-4 loss: 0.531627  [  160/  265]
train() client id: f_00001-5-5 loss: 0.517106  [  192/  265]
train() client id: f_00001-5-6 loss: 0.430715  [  224/  265]
train() client id: f_00001-5-7 loss: 0.564561  [  256/  265]
train() client id: f_00001-6-0 loss: 0.538723  [   32/  265]
train() client id: f_00001-6-1 loss: 0.444335  [   64/  265]
train() client id: f_00001-6-2 loss: 0.488732  [   96/  265]
train() client id: f_00001-6-3 loss: 0.435679  [  128/  265]
train() client id: f_00001-6-4 loss: 0.523659  [  160/  265]
train() client id: f_00001-6-5 loss: 0.454903  [  192/  265]
train() client id: f_00001-6-6 loss: 0.463410  [  224/  265]
train() client id: f_00001-6-7 loss: 0.437693  [  256/  265]
train() client id: f_00001-7-0 loss: 0.439288  [   32/  265]
train() client id: f_00001-7-1 loss: 0.458745  [   64/  265]
train() client id: f_00001-7-2 loss: 0.481505  [   96/  265]
train() client id: f_00001-7-3 loss: 0.437386  [  128/  265]
train() client id: f_00001-7-4 loss: 0.471185  [  160/  265]
train() client id: f_00001-7-5 loss: 0.528482  [  192/  265]
train() client id: f_00001-7-6 loss: 0.475795  [  224/  265]
train() client id: f_00001-7-7 loss: 0.500536  [  256/  265]
train() client id: f_00001-8-0 loss: 0.428240  [   32/  265]
train() client id: f_00001-8-1 loss: 0.478432  [   64/  265]
train() client id: f_00001-8-2 loss: 0.487976  [   96/  265]
train() client id: f_00001-8-3 loss: 0.567549  [  128/  265]
train() client id: f_00001-8-4 loss: 0.452176  [  160/  265]
train() client id: f_00001-8-5 loss: 0.374319  [  192/  265]
train() client id: f_00001-8-6 loss: 0.499496  [  224/  265]
train() client id: f_00001-8-7 loss: 0.394036  [  256/  265]
train() client id: f_00001-9-0 loss: 0.350022  [   32/  265]
train() client id: f_00001-9-1 loss: 0.446076  [   64/  265]
train() client id: f_00001-9-2 loss: 0.488538  [   96/  265]
train() client id: f_00001-9-3 loss: 0.630243  [  128/  265]
train() client id: f_00001-9-4 loss: 0.448262  [  160/  265]
train() client id: f_00001-9-5 loss: 0.529258  [  192/  265]
train() client id: f_00001-9-6 loss: 0.490638  [  224/  265]
train() client id: f_00001-9-7 loss: 0.385198  [  256/  265]
train() client id: f_00001-10-0 loss: 0.410372  [   32/  265]
train() client id: f_00001-10-1 loss: 0.380776  [   64/  265]
train() client id: f_00001-10-2 loss: 0.407023  [   96/  265]
train() client id: f_00001-10-3 loss: 0.535348  [  128/  265]
train() client id: f_00001-10-4 loss: 0.519527  [  160/  265]
train() client id: f_00001-10-5 loss: 0.601747  [  192/  265]
train() client id: f_00001-10-6 loss: 0.452577  [  224/  265]
train() client id: f_00001-10-7 loss: 0.441170  [  256/  265]
train() client id: f_00001-11-0 loss: 0.394105  [   32/  265]
train() client id: f_00001-11-1 loss: 0.413413  [   64/  265]
train() client id: f_00001-11-2 loss: 0.455320  [   96/  265]
train() client id: f_00001-11-3 loss: 0.524128  [  128/  265]
train() client id: f_00001-11-4 loss: 0.463080  [  160/  265]
train() client id: f_00001-11-5 loss: 0.392468  [  192/  265]
train() client id: f_00001-11-6 loss: 0.672682  [  224/  265]
train() client id: f_00001-11-7 loss: 0.425531  [  256/  265]
train() client id: f_00002-0-0 loss: 1.032997  [   32/  124]
train() client id: f_00002-0-1 loss: 0.931638  [   64/  124]
train() client id: f_00002-0-2 loss: 1.070517  [   96/  124]
train() client id: f_00002-1-0 loss: 0.965389  [   32/  124]
train() client id: f_00002-1-1 loss: 1.047829  [   64/  124]
train() client id: f_00002-1-2 loss: 0.904421  [   96/  124]
train() client id: f_00002-2-0 loss: 1.123969  [   32/  124]
train() client id: f_00002-2-1 loss: 0.989879  [   64/  124]
train() client id: f_00002-2-2 loss: 0.986441  [   96/  124]
train() client id: f_00002-3-0 loss: 0.983356  [   32/  124]
train() client id: f_00002-3-1 loss: 0.918844  [   64/  124]
train() client id: f_00002-3-2 loss: 0.984798  [   96/  124]
train() client id: f_00002-4-0 loss: 0.983993  [   32/  124]
train() client id: f_00002-4-1 loss: 0.918781  [   64/  124]
train() client id: f_00002-4-2 loss: 0.835969  [   96/  124]
train() client id: f_00002-5-0 loss: 0.844663  [   32/  124]
train() client id: f_00002-5-1 loss: 1.029389  [   64/  124]
train() client id: f_00002-5-2 loss: 0.981401  [   96/  124]
train() client id: f_00002-6-0 loss: 0.903515  [   32/  124]
train() client id: f_00002-6-1 loss: 1.168922  [   64/  124]
train() client id: f_00002-6-2 loss: 0.723895  [   96/  124]
train() client id: f_00002-7-0 loss: 0.874398  [   32/  124]
train() client id: f_00002-7-1 loss: 0.737995  [   64/  124]
train() client id: f_00002-7-2 loss: 0.978226  [   96/  124]
train() client id: f_00002-8-0 loss: 0.867788  [   32/  124]
train() client id: f_00002-8-1 loss: 0.862285  [   64/  124]
train() client id: f_00002-8-2 loss: 0.856671  [   96/  124]
train() client id: f_00002-9-0 loss: 1.058370  [   32/  124]
train() client id: f_00002-9-1 loss: 0.695115  [   64/  124]
train() client id: f_00002-9-2 loss: 0.731880  [   96/  124]
train() client id: f_00002-10-0 loss: 0.766841  [   32/  124]
train() client id: f_00002-10-1 loss: 0.911674  [   64/  124]
train() client id: f_00002-10-2 loss: 0.807308  [   96/  124]
train() client id: f_00002-11-0 loss: 0.791680  [   32/  124]
train() client id: f_00002-11-1 loss: 1.032804  [   64/  124]
train() client id: f_00002-11-2 loss: 0.880418  [   96/  124]
train() client id: f_00003-0-0 loss: 0.843316  [   32/   43]
train() client id: f_00003-1-0 loss: 0.988835  [   32/   43]
train() client id: f_00003-2-0 loss: 0.842135  [   32/   43]
train() client id: f_00003-3-0 loss: 0.872042  [   32/   43]
train() client id: f_00003-4-0 loss: 0.757248  [   32/   43]
train() client id: f_00003-5-0 loss: 0.743636  [   32/   43]
train() client id: f_00003-6-0 loss: 0.944355  [   32/   43]
train() client id: f_00003-7-0 loss: 0.863691  [   32/   43]
train() client id: f_00003-8-0 loss: 0.844381  [   32/   43]
train() client id: f_00003-9-0 loss: 0.953780  [   32/   43]
train() client id: f_00003-10-0 loss: 0.840425  [   32/   43]
train() client id: f_00003-11-0 loss: 0.780247  [   32/   43]
train() client id: f_00004-0-0 loss: 0.940694  [   32/  306]
train() client id: f_00004-0-1 loss: 0.870512  [   64/  306]
train() client id: f_00004-0-2 loss: 1.007587  [   96/  306]
train() client id: f_00004-0-3 loss: 0.944009  [  128/  306]
train() client id: f_00004-0-4 loss: 1.060205  [  160/  306]
train() client id: f_00004-0-5 loss: 0.874495  [  192/  306]
train() client id: f_00004-0-6 loss: 0.984346  [  224/  306]
train() client id: f_00004-0-7 loss: 0.848760  [  256/  306]
train() client id: f_00004-0-8 loss: 0.936060  [  288/  306]
train() client id: f_00004-1-0 loss: 0.850356  [   32/  306]
train() client id: f_00004-1-1 loss: 0.889086  [   64/  306]
train() client id: f_00004-1-2 loss: 0.849786  [   96/  306]
train() client id: f_00004-1-3 loss: 0.925283  [  128/  306]
train() client id: f_00004-1-4 loss: 0.966439  [  160/  306]
train() client id: f_00004-1-5 loss: 0.917923  [  192/  306]
train() client id: f_00004-1-6 loss: 1.013093  [  224/  306]
train() client id: f_00004-1-7 loss: 0.947899  [  256/  306]
train() client id: f_00004-1-8 loss: 0.995685  [  288/  306]
train() client id: f_00004-2-0 loss: 0.845453  [   32/  306]
train() client id: f_00004-2-1 loss: 1.002422  [   64/  306]
train() client id: f_00004-2-2 loss: 0.858219  [   96/  306]
train() client id: f_00004-2-3 loss: 0.909138  [  128/  306]
train() client id: f_00004-2-4 loss: 0.858516  [  160/  306]
train() client id: f_00004-2-5 loss: 0.887988  [  192/  306]
train() client id: f_00004-2-6 loss: 0.916972  [  224/  306]
train() client id: f_00004-2-7 loss: 0.978617  [  256/  306]
train() client id: f_00004-2-8 loss: 1.042564  [  288/  306]
train() client id: f_00004-3-0 loss: 1.072593  [   32/  306]
train() client id: f_00004-3-1 loss: 0.837192  [   64/  306]
train() client id: f_00004-3-2 loss: 0.935968  [   96/  306]
train() client id: f_00004-3-3 loss: 0.946671  [  128/  306]
train() client id: f_00004-3-4 loss: 0.901233  [  160/  306]
train() client id: f_00004-3-5 loss: 0.942823  [  192/  306]
train() client id: f_00004-3-6 loss: 0.795973  [  224/  306]
train() client id: f_00004-3-7 loss: 1.048720  [  256/  306]
train() client id: f_00004-3-8 loss: 0.919526  [  288/  306]
train() client id: f_00004-4-0 loss: 0.972845  [   32/  306]
train() client id: f_00004-4-1 loss: 0.936829  [   64/  306]
train() client id: f_00004-4-2 loss: 0.941970  [   96/  306]
train() client id: f_00004-4-3 loss: 1.017148  [  128/  306]
train() client id: f_00004-4-4 loss: 0.801062  [  160/  306]
train() client id: f_00004-4-5 loss: 0.960444  [  192/  306]
train() client id: f_00004-4-6 loss: 0.959106  [  224/  306]
train() client id: f_00004-4-7 loss: 0.922642  [  256/  306]
train() client id: f_00004-4-8 loss: 0.839184  [  288/  306]
train() client id: f_00004-5-0 loss: 0.895940  [   32/  306]
train() client id: f_00004-5-1 loss: 0.813852  [   64/  306]
train() client id: f_00004-5-2 loss: 1.007937  [   96/  306]
train() client id: f_00004-5-3 loss: 0.924757  [  128/  306]
train() client id: f_00004-5-4 loss: 1.034734  [  160/  306]
train() client id: f_00004-5-5 loss: 1.031013  [  192/  306]
train() client id: f_00004-5-6 loss: 0.931265  [  224/  306]
train() client id: f_00004-5-7 loss: 0.874233  [  256/  306]
train() client id: f_00004-5-8 loss: 0.887881  [  288/  306]
train() client id: f_00004-6-0 loss: 0.804231  [   32/  306]
train() client id: f_00004-6-1 loss: 0.997024  [   64/  306]
train() client id: f_00004-6-2 loss: 0.947912  [   96/  306]
train() client id: f_00004-6-3 loss: 0.955887  [  128/  306]
train() client id: f_00004-6-4 loss: 1.032787  [  160/  306]
train() client id: f_00004-6-5 loss: 0.858033  [  192/  306]
train() client id: f_00004-6-6 loss: 0.982497  [  224/  306]
train() client id: f_00004-6-7 loss: 0.836677  [  256/  306]
train() client id: f_00004-6-8 loss: 0.905466  [  288/  306]
train() client id: f_00004-7-0 loss: 0.914871  [   32/  306]
train() client id: f_00004-7-1 loss: 1.023997  [   64/  306]
train() client id: f_00004-7-2 loss: 0.830063  [   96/  306]
train() client id: f_00004-7-3 loss: 0.908105  [  128/  306]
train() client id: f_00004-7-4 loss: 0.954044  [  160/  306]
train() client id: f_00004-7-5 loss: 0.817718  [  192/  306]
train() client id: f_00004-7-6 loss: 0.925221  [  224/  306]
train() client id: f_00004-7-7 loss: 0.904411  [  256/  306]
train() client id: f_00004-7-8 loss: 0.993594  [  288/  306]
train() client id: f_00004-8-0 loss: 0.929607  [   32/  306]
train() client id: f_00004-8-1 loss: 0.855276  [   64/  306]
train() client id: f_00004-8-2 loss: 0.913766  [   96/  306]
train() client id: f_00004-8-3 loss: 0.888172  [  128/  306]
train() client id: f_00004-8-4 loss: 0.945424  [  160/  306]
train() client id: f_00004-8-5 loss: 0.915562  [  192/  306]
train() client id: f_00004-8-6 loss: 0.909539  [  224/  306]
train() client id: f_00004-8-7 loss: 0.969933  [  256/  306]
train() client id: f_00004-8-8 loss: 0.981984  [  288/  306]
train() client id: f_00004-9-0 loss: 0.959158  [   32/  306]
train() client id: f_00004-9-1 loss: 0.860708  [   64/  306]
train() client id: f_00004-9-2 loss: 0.899243  [   96/  306]
train() client id: f_00004-9-3 loss: 0.870178  [  128/  306]
train() client id: f_00004-9-4 loss: 0.919731  [  160/  306]
train() client id: f_00004-9-5 loss: 0.922957  [  192/  306]
train() client id: f_00004-9-6 loss: 0.859041  [  224/  306]
train() client id: f_00004-9-7 loss: 0.995201  [  256/  306]
train() client id: f_00004-9-8 loss: 1.014430  [  288/  306]
train() client id: f_00004-10-0 loss: 0.805299  [   32/  306]
train() client id: f_00004-10-1 loss: 1.081594  [   64/  306]
train() client id: f_00004-10-2 loss: 1.041781  [   96/  306]
train() client id: f_00004-10-3 loss: 0.941468  [  128/  306]
train() client id: f_00004-10-4 loss: 0.940897  [  160/  306]
train() client id: f_00004-10-5 loss: 0.787539  [  192/  306]
train() client id: f_00004-10-6 loss: 0.914017  [  224/  306]
train() client id: f_00004-10-7 loss: 0.886180  [  256/  306]
train() client id: f_00004-10-8 loss: 0.809685  [  288/  306]
train() client id: f_00004-11-0 loss: 0.798318  [   32/  306]
train() client id: f_00004-11-1 loss: 0.926329  [   64/  306]
train() client id: f_00004-11-2 loss: 0.981065  [   96/  306]
train() client id: f_00004-11-3 loss: 0.977670  [  128/  306]
train() client id: f_00004-11-4 loss: 0.906454  [  160/  306]
train() client id: f_00004-11-5 loss: 1.026864  [  192/  306]
train() client id: f_00004-11-6 loss: 0.941437  [  224/  306]
train() client id: f_00004-11-7 loss: 0.852652  [  256/  306]
train() client id: f_00004-11-8 loss: 0.919205  [  288/  306]
train() client id: f_00005-0-0 loss: 0.778336  [   32/  146]
train() client id: f_00005-0-1 loss: 0.663844  [   64/  146]
train() client id: f_00005-0-2 loss: 0.796444  [   96/  146]
train() client id: f_00005-0-3 loss: 0.628370  [  128/  146]
train() client id: f_00005-1-0 loss: 0.711290  [   32/  146]
train() client id: f_00005-1-1 loss: 0.765101  [   64/  146]
train() client id: f_00005-1-2 loss: 0.648285  [   96/  146]
train() client id: f_00005-1-3 loss: 0.767641  [  128/  146]
train() client id: f_00005-2-0 loss: 0.415309  [   32/  146]
train() client id: f_00005-2-1 loss: 0.791813  [   64/  146]
train() client id: f_00005-2-2 loss: 0.669996  [   96/  146]
train() client id: f_00005-2-3 loss: 0.927481  [  128/  146]
train() client id: f_00005-3-0 loss: 0.623650  [   32/  146]
train() client id: f_00005-3-1 loss: 0.755319  [   64/  146]
train() client id: f_00005-3-2 loss: 0.723510  [   96/  146]
train() client id: f_00005-3-3 loss: 0.649780  [  128/  146]
train() client id: f_00005-4-0 loss: 0.658824  [   32/  146]
train() client id: f_00005-4-1 loss: 0.840259  [   64/  146]
train() client id: f_00005-4-2 loss: 0.845368  [   96/  146]
train() client id: f_00005-4-3 loss: 0.504507  [  128/  146]
train() client id: f_00005-5-0 loss: 0.709239  [   32/  146]
train() client id: f_00005-5-1 loss: 0.693119  [   64/  146]
train() client id: f_00005-5-2 loss: 0.707997  [   96/  146]
train() client id: f_00005-5-3 loss: 0.465995  [  128/  146]
train() client id: f_00005-6-0 loss: 1.040771  [   32/  146]
train() client id: f_00005-6-1 loss: 0.664056  [   64/  146]
train() client id: f_00005-6-2 loss: 0.537975  [   96/  146]
train() client id: f_00005-6-3 loss: 0.627641  [  128/  146]
train() client id: f_00005-7-0 loss: 0.798207  [   32/  146]
train() client id: f_00005-7-1 loss: 0.377076  [   64/  146]
train() client id: f_00005-7-2 loss: 0.766304  [   96/  146]
train() client id: f_00005-7-3 loss: 0.872451  [  128/  146]
train() client id: f_00005-8-0 loss: 0.783029  [   32/  146]
train() client id: f_00005-8-1 loss: 0.447203  [   64/  146]
train() client id: f_00005-8-2 loss: 0.833140  [   96/  146]
train() client id: f_00005-8-3 loss: 0.594168  [  128/  146]
train() client id: f_00005-9-0 loss: 0.631481  [   32/  146]
train() client id: f_00005-9-1 loss: 0.600794  [   64/  146]
train() client id: f_00005-9-2 loss: 0.804526  [   96/  146]
train() client id: f_00005-9-3 loss: 0.826286  [  128/  146]
train() client id: f_00005-10-0 loss: 0.617125  [   32/  146]
train() client id: f_00005-10-1 loss: 0.651613  [   64/  146]
train() client id: f_00005-10-2 loss: 0.636772  [   96/  146]
train() client id: f_00005-10-3 loss: 0.855450  [  128/  146]
train() client id: f_00005-11-0 loss: 0.562537  [   32/  146]
train() client id: f_00005-11-1 loss: 0.648947  [   64/  146]
train() client id: f_00005-11-2 loss: 0.738145  [   96/  146]
train() client id: f_00005-11-3 loss: 0.822257  [  128/  146]
train() client id: f_00006-0-0 loss: 0.602798  [   32/   54]
train() client id: f_00006-1-0 loss: 0.593869  [   32/   54]
train() client id: f_00006-2-0 loss: 0.639277  [   32/   54]
train() client id: f_00006-3-0 loss: 0.645542  [   32/   54]
train() client id: f_00006-4-0 loss: 0.558747  [   32/   54]
train() client id: f_00006-5-0 loss: 0.593093  [   32/   54]
train() client id: f_00006-6-0 loss: 0.610596  [   32/   54]
train() client id: f_00006-7-0 loss: 0.607280  [   32/   54]
train() client id: f_00006-8-0 loss: 0.564526  [   32/   54]
train() client id: f_00006-9-0 loss: 0.604892  [   32/   54]
train() client id: f_00006-10-0 loss: 0.621993  [   32/   54]
train() client id: f_00006-11-0 loss: 0.549423  [   32/   54]
train() client id: f_00007-0-0 loss: 0.486171  [   32/  179]
train() client id: f_00007-0-1 loss: 0.425607  [   64/  179]
train() client id: f_00007-0-2 loss: 0.286830  [   96/  179]
train() client id: f_00007-0-3 loss: 0.463889  [  128/  179]
train() client id: f_00007-0-4 loss: 0.519901  [  160/  179]
train() client id: f_00007-1-0 loss: 0.298584  [   32/  179]
train() client id: f_00007-1-1 loss: 0.484413  [   64/  179]
train() client id: f_00007-1-2 loss: 0.519488  [   96/  179]
train() client id: f_00007-1-3 loss: 0.626183  [  128/  179]
train() client id: f_00007-1-4 loss: 0.298771  [  160/  179]
train() client id: f_00007-2-0 loss: 0.362164  [   32/  179]
train() client id: f_00007-2-1 loss: 0.374413  [   64/  179]
train() client id: f_00007-2-2 loss: 0.410244  [   96/  179]
train() client id: f_00007-2-3 loss: 0.369865  [  128/  179]
train() client id: f_00007-2-4 loss: 0.644683  [  160/  179]
train() client id: f_00007-3-0 loss: 0.504594  [   32/  179]
train() client id: f_00007-3-1 loss: 0.386189  [   64/  179]
train() client id: f_00007-3-2 loss: 0.432326  [   96/  179]
train() client id: f_00007-3-3 loss: 0.353702  [  128/  179]
train() client id: f_00007-3-4 loss: 0.434783  [  160/  179]
train() client id: f_00007-4-0 loss: 0.374703  [   32/  179]
train() client id: f_00007-4-1 loss: 0.378734  [   64/  179]
train() client id: f_00007-4-2 loss: 0.287600  [   96/  179]
train() client id: f_00007-4-3 loss: 0.423629  [  128/  179]
train() client id: f_00007-4-4 loss: 0.360214  [  160/  179]
train() client id: f_00007-5-0 loss: 0.569345  [   32/  179]
train() client id: f_00007-5-1 loss: 0.358158  [   64/  179]
train() client id: f_00007-5-2 loss: 0.335080  [   96/  179]
train() client id: f_00007-5-3 loss: 0.320874  [  128/  179]
train() client id: f_00007-5-4 loss: 0.399518  [  160/  179]
train() client id: f_00007-6-0 loss: 0.519840  [   32/  179]
train() client id: f_00007-6-1 loss: 0.597713  [   64/  179]
train() client id: f_00007-6-2 loss: 0.294583  [   96/  179]
train() client id: f_00007-6-3 loss: 0.315524  [  128/  179]
train() client id: f_00007-6-4 loss: 0.280678  [  160/  179]
train() client id: f_00007-7-0 loss: 0.314278  [   32/  179]
train() client id: f_00007-7-1 loss: 0.386506  [   64/  179]
train() client id: f_00007-7-2 loss: 0.221596  [   96/  179]
train() client id: f_00007-7-3 loss: 0.341825  [  128/  179]
train() client id: f_00007-7-4 loss: 0.423931  [  160/  179]
train() client id: f_00007-8-0 loss: 0.644570  [   32/  179]
train() client id: f_00007-8-1 loss: 0.324409  [   64/  179]
train() client id: f_00007-8-2 loss: 0.358270  [   96/  179]
train() client id: f_00007-8-3 loss: 0.309393  [  128/  179]
train() client id: f_00007-8-4 loss: 0.362820  [  160/  179]
train() client id: f_00007-9-0 loss: 0.481490  [   32/  179]
train() client id: f_00007-9-1 loss: 0.362029  [   64/  179]
train() client id: f_00007-9-2 loss: 0.402215  [   96/  179]
train() client id: f_00007-9-3 loss: 0.329090  [  128/  179]
train() client id: f_00007-9-4 loss: 0.411273  [  160/  179]
train() client id: f_00007-10-0 loss: 0.308381  [   32/  179]
train() client id: f_00007-10-1 loss: 0.504536  [   64/  179]
train() client id: f_00007-10-2 loss: 0.358901  [   96/  179]
train() client id: f_00007-10-3 loss: 0.500672  [  128/  179]
train() client id: f_00007-10-4 loss: 0.318483  [  160/  179]
train() client id: f_00007-11-0 loss: 0.412139  [   32/  179]
train() client id: f_00007-11-1 loss: 0.410950  [   64/  179]
train() client id: f_00007-11-2 loss: 0.502343  [   96/  179]
train() client id: f_00007-11-3 loss: 0.345537  [  128/  179]
train() client id: f_00007-11-4 loss: 0.307554  [  160/  179]
train() client id: f_00008-0-0 loss: 0.939419  [   32/  130]
train() client id: f_00008-0-1 loss: 0.920576  [   64/  130]
train() client id: f_00008-0-2 loss: 0.865999  [   96/  130]
train() client id: f_00008-0-3 loss: 0.829547  [  128/  130]
train() client id: f_00008-1-0 loss: 0.864404  [   32/  130]
train() client id: f_00008-1-1 loss: 0.868100  [   64/  130]
train() client id: f_00008-1-2 loss: 0.821674  [   96/  130]
train() client id: f_00008-1-3 loss: 0.947439  [  128/  130]
train() client id: f_00008-2-0 loss: 0.886047  [   32/  130]
train() client id: f_00008-2-1 loss: 0.943252  [   64/  130]
train() client id: f_00008-2-2 loss: 0.915989  [   96/  130]
train() client id: f_00008-2-3 loss: 0.819857  [  128/  130]
train() client id: f_00008-3-0 loss: 0.953669  [   32/  130]
train() client id: f_00008-3-1 loss: 0.825331  [   64/  130]
train() client id: f_00008-3-2 loss: 1.003542  [   96/  130]
train() client id: f_00008-3-3 loss: 0.788886  [  128/  130]
train() client id: f_00008-4-0 loss: 0.870406  [   32/  130]
train() client id: f_00008-4-1 loss: 0.991845  [   64/  130]
train() client id: f_00008-4-2 loss: 0.796586  [   96/  130]
train() client id: f_00008-4-3 loss: 0.874293  [  128/  130]
train() client id: f_00008-5-0 loss: 0.942095  [   32/  130]
train() client id: f_00008-5-1 loss: 0.850778  [   64/  130]
train() client id: f_00008-5-2 loss: 0.918313  [   96/  130]
train() client id: f_00008-5-3 loss: 0.856839  [  128/  130]
train() client id: f_00008-6-0 loss: 1.008820  [   32/  130]
train() client id: f_00008-6-1 loss: 0.818436  [   64/  130]
train() client id: f_00008-6-2 loss: 0.882377  [   96/  130]
train() client id: f_00008-6-3 loss: 0.863471  [  128/  130]
train() client id: f_00008-7-0 loss: 0.913410  [   32/  130]
train() client id: f_00008-7-1 loss: 0.904803  [   64/  130]
train() client id: f_00008-7-2 loss: 0.850113  [   96/  130]
train() client id: f_00008-7-3 loss: 0.885819  [  128/  130]
train() client id: f_00008-8-0 loss: 0.892139  [   32/  130]
train() client id: f_00008-8-1 loss: 0.902840  [   64/  130]
train() client id: f_00008-8-2 loss: 0.899446  [   96/  130]
train() client id: f_00008-8-3 loss: 0.881492  [  128/  130]
train() client id: f_00008-9-0 loss: 0.941623  [   32/  130]
train() client id: f_00008-9-1 loss: 0.883954  [   64/  130]
train() client id: f_00008-9-2 loss: 0.871156  [   96/  130]
train() client id: f_00008-9-3 loss: 0.878837  [  128/  130]
train() client id: f_00008-10-0 loss: 0.851647  [   32/  130]
train() client id: f_00008-10-1 loss: 0.938460  [   64/  130]
train() client id: f_00008-10-2 loss: 0.848546  [   96/  130]
train() client id: f_00008-10-3 loss: 0.952156  [  128/  130]
train() client id: f_00008-11-0 loss: 0.917420  [   32/  130]
train() client id: f_00008-11-1 loss: 0.885984  [   64/  130]
train() client id: f_00008-11-2 loss: 0.823551  [   96/  130]
train() client id: f_00008-11-3 loss: 0.962585  [  128/  130]
train() client id: f_00009-0-0 loss: 1.151697  [   32/  118]
train() client id: f_00009-0-1 loss: 1.196794  [   64/  118]
train() client id: f_00009-0-2 loss: 1.190451  [   96/  118]
train() client id: f_00009-1-0 loss: 1.076886  [   32/  118]
train() client id: f_00009-1-1 loss: 1.068641  [   64/  118]
train() client id: f_00009-1-2 loss: 1.281449  [   96/  118]
train() client id: f_00009-2-0 loss: 0.998397  [   32/  118]
train() client id: f_00009-2-1 loss: 1.043520  [   64/  118]
train() client id: f_00009-2-2 loss: 1.150583  [   96/  118]
train() client id: f_00009-3-0 loss: 0.968398  [   32/  118]
train() client id: f_00009-3-1 loss: 1.267961  [   64/  118]
train() client id: f_00009-3-2 loss: 0.968224  [   96/  118]
train() client id: f_00009-4-0 loss: 0.929622  [   32/  118]
train() client id: f_00009-4-1 loss: 1.201496  [   64/  118]
train() client id: f_00009-4-2 loss: 1.025224  [   96/  118]
train() client id: f_00009-5-0 loss: 1.075881  [   32/  118]
train() client id: f_00009-5-1 loss: 1.001722  [   64/  118]
train() client id: f_00009-5-2 loss: 0.985394  [   96/  118]
train() client id: f_00009-6-0 loss: 0.938342  [   32/  118]
train() client id: f_00009-6-1 loss: 1.061893  [   64/  118]
train() client id: f_00009-6-2 loss: 0.942049  [   96/  118]
train() client id: f_00009-7-0 loss: 1.001208  [   32/  118]
train() client id: f_00009-7-1 loss: 1.021732  [   64/  118]
train() client id: f_00009-7-2 loss: 0.930501  [   96/  118]
train() client id: f_00009-8-0 loss: 1.037101  [   32/  118]
train() client id: f_00009-8-1 loss: 0.837041  [   64/  118]
train() client id: f_00009-8-2 loss: 0.924651  [   96/  118]
train() client id: f_00009-9-0 loss: 1.018463  [   32/  118]
train() client id: f_00009-9-1 loss: 1.059752  [   64/  118]
train() client id: f_00009-9-2 loss: 0.846068  [   96/  118]
train() client id: f_00009-10-0 loss: 0.978722  [   32/  118]
train() client id: f_00009-10-1 loss: 0.911218  [   64/  118]
train() client id: f_00009-10-2 loss: 1.014137  [   96/  118]
train() client id: f_00009-11-0 loss: 0.876416  [   32/  118]
train() client id: f_00009-11-1 loss: 1.188230  [   64/  118]
train() client id: f_00009-11-2 loss: 0.860131  [   96/  118]
At round 13 accuracy: 0.6286472148541115
At round 13 training accuracy: 0.5848423876592891
At round 13 training loss: 0.835335100057899
update_location
xs = -3.905658 4.200318 85.009024 18.811294 0.979296 3.956410 -47.443192 -26.324852 69.663977 -12.060879 
ys = 77.587959 60.555839 1.320614 -47.455176 39.350187 22.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 9.288573831882571
ys mean: 17.394142535528704
dists_uav = 126.629955 116.981418 131.256536 112.275814 107.468117 102.645695 110.714710 103.410222 123.133016 100.804150 
uav_gains = -102.563866 -101.703071 -102.953772 -101.257239 -100.782036 -100.283542 -101.105202 -100.364113 -102.259670 -100.086978 
uav_gains_db_mean: -101.33594878705915
dists_bs = 196.881638 212.626950 312.680702 295.040884 222.192674 234.916856 218.680988 229.002520 290.938102 236.198329 
bs_gains = -103.804805 -104.740372 -109.429889 -108.723761 -105.275492 -105.952656 -105.081768 -105.642586 -108.553476 -106.018810 
bs_gains_db_mean: -106.32236155033128
Round 14
-------------------------------
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.14357806 19.06538973  9.00414718  3.22065168 21.99169094 10.5993586
  4.00331986 12.90419715  9.4917528   8.60518442]
obj_prev = 108.02927042739734
eta_min = 6.894580279323513e-11	eta_max = 0.920963466545226
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 25.120031493789718	eta = 0.9090909090909091
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 43.56050107516789	eta = 0.5242453990066629
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 34.74720289395531	eta = 0.6572152681404528
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.166400765192755	eta = 0.6885399603277947
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.08814089362885	eta = 0.6901684908951391
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.08793538155577	eta = 0.6901727775922599
eta = 0.6901727775922599
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [0.03059863 0.06435425 0.03011293 0.01044239 0.07431095 0.03545555
 0.0131137  0.04346948 0.03157001 0.02865586]
ene_total = [2.84199716 5.44950306 2.81511635 1.29013739 6.21774054 3.30722781
 1.48846279 3.76427446 3.1172366  2.79623923]
ti_comp = [0.31764397 0.30976147 0.31634818 0.32166983 0.30755338 0.30458011
 0.32210904 0.32417046 0.29089448 0.30427822]
ti_coms = [0.07051759 0.0784001  0.07181339 0.06649173 0.08060818 0.08358146
 0.06605252 0.06399111 0.09726708 0.08388335]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.77461660e-05 1.73602902e-04 1.70533033e-05 6.87794728e-07
 2.71142377e-04 3.00282236e-05 1.35846854e-06 4.88524522e-05
 2.32398452e-05 1.58846785e-05]
ene_total = [0.53363372 0.6048986  0.54336256 0.50195733 0.62892872 0.63317194
 0.49869262 0.4867173  0.73596393 0.63438309]
optimize_network iter = 0 obj = 5.8017097996113725
eta = 0.6901727775922599
freqs = [4.81649713e+07 1.03877111e+08 4.75946046e+07 1.62315316e+07
 1.20809836e+08 5.82039824e+07 2.03559896e+07 6.70472512e+07
 5.42636806e+07 4.70882466e+07]
eta_min = 0.6901727775922659	eta_max = 0.6901727775922564
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 0.041597769241550876	eta = 0.9090909090909091
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 19.658200800192578	eta = 0.0019236833645317572
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 2.023882036535733	eta = 0.01868495948542783
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 1.9708396260580614	eta = 0.01918783921124654
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 1.9708261485414031	eta = 0.01918797042749958
eta = 0.01918797042749958
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.89202933e-04 1.85088870e-03 1.81815891e-04 7.33300812e-06
 2.89081782e-03 3.20149601e-04 1.44834795e-05 5.20846430e-04
 2.47774470e-04 1.69356454e-04]
ene_total = [0.17265704 0.23107452 0.17557065 0.15872099 0.26113616 0.20692952
 0.15784421 0.16500284 0.23783644 0.20405377]
ti_comp = [0.31764397 0.30976147 0.31634818 0.32166983 0.30755338 0.30458011
 0.32210904 0.32417046 0.29089448 0.30427822]
ti_coms = [0.07051759 0.0784001  0.07181339 0.06649173 0.08060818 0.08358146
 0.06605252 0.06399111 0.09726708 0.08388335]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.77461660e-05 1.73602902e-04 1.70533033e-05 6.87794728e-07
 2.71142377e-04 3.00282236e-05 1.35846854e-06 4.88524522e-05
 2.32398452e-05 1.58846785e-05]
ene_total = [0.53363372 0.6048986  0.54336256 0.50195733 0.62892872 0.63317194
 0.49869262 0.4867173  0.73596393 0.63438309]
optimize_network iter = 1 obj = 5.801709799611484
eta = 0.6901727775922659
freqs = [4.81649713e+07 1.03877111e+08 4.75946046e+07 1.62315316e+07
 1.20809836e+08 5.82039824e+07 2.03559896e+07 6.70472512e+07
 5.42636806e+07 4.70882466e+07]
Done!
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.75381754e-05 1.71568221e-04 1.68534331e-05 6.79733555e-07
 2.67964503e-04 2.96762833e-05 1.34254685e-06 4.82798860e-05
 2.29674669e-05 1.56985050e-05]
ene_total = [0.0070693  0.00801158 0.00719819 0.00664985 0.00832878 0.00838782
 0.00660659 0.00644739 0.00974968 0.00840403]
At round 14 energy consumption: 0.0768532209744656
At round 14 eta: 0.6901727775922659
At round 14 a_n: 23.386960998683932
At round 14 local rounds: 12.142311740979952
At round 14 global rounds: 75.48388039288098
gradient difference: 0.4229205548763275
train() client id: f_00000-0-0 loss: 0.947201  [   32/  126]
train() client id: f_00000-0-1 loss: 1.245425  [   64/  126]
train() client id: f_00000-0-2 loss: 0.992978  [   96/  126]
train() client id: f_00000-1-0 loss: 1.148159  [   32/  126]
train() client id: f_00000-1-1 loss: 1.126785  [   64/  126]
train() client id: f_00000-1-2 loss: 1.007993  [   96/  126]
train() client id: f_00000-2-0 loss: 1.007054  [   32/  126]
train() client id: f_00000-2-1 loss: 1.074803  [   64/  126]
train() client id: f_00000-2-2 loss: 0.948366  [   96/  126]
train() client id: f_00000-3-0 loss: 0.949242  [   32/  126]
train() client id: f_00000-3-1 loss: 0.904905  [   64/  126]
train() client id: f_00000-3-2 loss: 0.901692  [   96/  126]
train() client id: f_00000-4-0 loss: 0.853935  [   32/  126]
train() client id: f_00000-4-1 loss: 0.997791  [   64/  126]
train() client id: f_00000-4-2 loss: 0.814605  [   96/  126]
train() client id: f_00000-5-0 loss: 0.910153  [   32/  126]
train() client id: f_00000-5-1 loss: 1.009651  [   64/  126]
train() client id: f_00000-5-2 loss: 0.772173  [   96/  126]
train() client id: f_00000-6-0 loss: 0.889478  [   32/  126]
train() client id: f_00000-6-1 loss: 0.844309  [   64/  126]
train() client id: f_00000-6-2 loss: 0.802358  [   96/  126]
train() client id: f_00000-7-0 loss: 0.980835  [   32/  126]
train() client id: f_00000-7-1 loss: 0.810366  [   64/  126]
train() client id: f_00000-7-2 loss: 0.761511  [   96/  126]
train() client id: f_00000-8-0 loss: 0.861813  [   32/  126]
train() client id: f_00000-8-1 loss: 0.801091  [   64/  126]
train() client id: f_00000-8-2 loss: 0.760453  [   96/  126]
train() client id: f_00000-9-0 loss: 0.778837  [   32/  126]
train() client id: f_00000-9-1 loss: 0.703053  [   64/  126]
train() client id: f_00000-9-2 loss: 0.812471  [   96/  126]
train() client id: f_00000-10-0 loss: 0.846038  [   32/  126]
train() client id: f_00000-10-1 loss: 0.803511  [   64/  126]
train() client id: f_00000-10-2 loss: 0.632035  [   96/  126]
train() client id: f_00000-11-0 loss: 0.821811  [   32/  126]
train() client id: f_00000-11-1 loss: 0.709848  [   64/  126]
train() client id: f_00000-11-2 loss: 0.842818  [   96/  126]
train() client id: f_00001-0-0 loss: 0.409209  [   32/  265]
train() client id: f_00001-0-1 loss: 0.527564  [   64/  265]
train() client id: f_00001-0-2 loss: 0.486445  [   96/  265]
train() client id: f_00001-0-3 loss: 0.511081  [  128/  265]
train() client id: f_00001-0-4 loss: 0.548870  [  160/  265]
train() client id: f_00001-0-5 loss: 0.495836  [  192/  265]
train() client id: f_00001-0-6 loss: 0.560580  [  224/  265]
train() client id: f_00001-0-7 loss: 0.494958  [  256/  265]
train() client id: f_00001-1-0 loss: 0.548411  [   32/  265]
train() client id: f_00001-1-1 loss: 0.439698  [   64/  265]
train() client id: f_00001-1-2 loss: 0.658956  [   96/  265]
train() client id: f_00001-1-3 loss: 0.668742  [  128/  265]
train() client id: f_00001-1-4 loss: 0.451004  [  160/  265]
train() client id: f_00001-1-5 loss: 0.425822  [  192/  265]
train() client id: f_00001-1-6 loss: 0.432183  [  224/  265]
train() client id: f_00001-1-7 loss: 0.411940  [  256/  265]
train() client id: f_00001-2-0 loss: 0.400549  [   32/  265]
train() client id: f_00001-2-1 loss: 0.497897  [   64/  265]
train() client id: f_00001-2-2 loss: 0.615434  [   96/  265]
train() client id: f_00001-2-3 loss: 0.469182  [  128/  265]
train() client id: f_00001-2-4 loss: 0.460869  [  160/  265]
train() client id: f_00001-2-5 loss: 0.495434  [  192/  265]
train() client id: f_00001-2-6 loss: 0.467293  [  224/  265]
train() client id: f_00001-2-7 loss: 0.500844  [  256/  265]
train() client id: f_00001-3-0 loss: 0.445761  [   32/  265]
train() client id: f_00001-3-1 loss: 0.482438  [   64/  265]
train() client id: f_00001-3-2 loss: 0.472952  [   96/  265]
train() client id: f_00001-3-3 loss: 0.582164  [  128/  265]
train() client id: f_00001-3-4 loss: 0.421277  [  160/  265]
train() client id: f_00001-3-5 loss: 0.585841  [  192/  265]
train() client id: f_00001-3-6 loss: 0.423486  [  224/  265]
train() client id: f_00001-3-7 loss: 0.453139  [  256/  265]
train() client id: f_00001-4-0 loss: 0.420489  [   32/  265]
train() client id: f_00001-4-1 loss: 0.486730  [   64/  265]
train() client id: f_00001-4-2 loss: 0.633412  [   96/  265]
train() client id: f_00001-4-3 loss: 0.394506  [  128/  265]
train() client id: f_00001-4-4 loss: 0.496911  [  160/  265]
train() client id: f_00001-4-5 loss: 0.521109  [  192/  265]
train() client id: f_00001-4-6 loss: 0.556710  [  224/  265]
train() client id: f_00001-4-7 loss: 0.388954  [  256/  265]
train() client id: f_00001-5-0 loss: 0.444756  [   32/  265]
train() client id: f_00001-5-1 loss: 0.462619  [   64/  265]
train() client id: f_00001-5-2 loss: 0.497477  [   96/  265]
train() client id: f_00001-5-3 loss: 0.438865  [  128/  265]
train() client id: f_00001-5-4 loss: 0.528307  [  160/  265]
train() client id: f_00001-5-5 loss: 0.442865  [  192/  265]
train() client id: f_00001-5-6 loss: 0.591330  [  224/  265]
train() client id: f_00001-5-7 loss: 0.437671  [  256/  265]
train() client id: f_00001-6-0 loss: 0.399088  [   32/  265]
train() client id: f_00001-6-1 loss: 0.402120  [   64/  265]
train() client id: f_00001-6-2 loss: 0.380748  [   96/  265]
train() client id: f_00001-6-3 loss: 0.534707  [  128/  265]
train() client id: f_00001-6-4 loss: 0.410962  [  160/  265]
train() client id: f_00001-6-5 loss: 0.525033  [  192/  265]
train() client id: f_00001-6-6 loss: 0.591239  [  224/  265]
train() client id: f_00001-6-7 loss: 0.549624  [  256/  265]
train() client id: f_00001-7-0 loss: 0.479028  [   32/  265]
train() client id: f_00001-7-1 loss: 0.438777  [   64/  265]
train() client id: f_00001-7-2 loss: 0.528809  [   96/  265]
train() client id: f_00001-7-3 loss: 0.403410  [  128/  265]
train() client id: f_00001-7-4 loss: 0.406985  [  160/  265]
train() client id: f_00001-7-5 loss: 0.554464  [  192/  265]
train() client id: f_00001-7-6 loss: 0.534644  [  224/  265]
train() client id: f_00001-7-7 loss: 0.437188  [  256/  265]
train() client id: f_00001-8-0 loss: 0.552120  [   32/  265]
train() client id: f_00001-8-1 loss: 0.459901  [   64/  265]
train() client id: f_00001-8-2 loss: 0.432533  [   96/  265]
train() client id: f_00001-8-3 loss: 0.571297  [  128/  265]
train() client id: f_00001-8-4 loss: 0.512365  [  160/  265]
train() client id: f_00001-8-5 loss: 0.404276  [  192/  265]
train() client id: f_00001-8-6 loss: 0.429804  [  224/  265]
train() client id: f_00001-8-7 loss: 0.390557  [  256/  265]
train() client id: f_00001-9-0 loss: 0.419947  [   32/  265]
train() client id: f_00001-9-1 loss: 0.399984  [   64/  265]
train() client id: f_00001-9-2 loss: 0.475047  [   96/  265]
train() client id: f_00001-9-3 loss: 0.486404  [  128/  265]
train() client id: f_00001-9-4 loss: 0.451934  [  160/  265]
train() client id: f_00001-9-5 loss: 0.574580  [  192/  265]
train() client id: f_00001-9-6 loss: 0.467014  [  224/  265]
train() client id: f_00001-9-7 loss: 0.487835  [  256/  265]
train() client id: f_00001-10-0 loss: 0.426635  [   32/  265]
train() client id: f_00001-10-1 loss: 0.489108  [   64/  265]
train() client id: f_00001-10-2 loss: 0.523483  [   96/  265]
train() client id: f_00001-10-3 loss: 0.431081  [  128/  265]
train() client id: f_00001-10-4 loss: 0.478155  [  160/  265]
train() client id: f_00001-10-5 loss: 0.468888  [  192/  265]
train() client id: f_00001-10-6 loss: 0.449004  [  224/  265]
train() client id: f_00001-10-7 loss: 0.428712  [  256/  265]
train() client id: f_00001-11-0 loss: 0.462192  [   32/  265]
train() client id: f_00001-11-1 loss: 0.484172  [   64/  265]
train() client id: f_00001-11-2 loss: 0.534975  [   96/  265]
train() client id: f_00001-11-3 loss: 0.545190  [  128/  265]
train() client id: f_00001-11-4 loss: 0.355970  [  160/  265]
train() client id: f_00001-11-5 loss: 0.476657  [  192/  265]
train() client id: f_00001-11-6 loss: 0.437532  [  224/  265]
train() client id: f_00001-11-7 loss: 0.492588  [  256/  265]
train() client id: f_00002-0-0 loss: 1.199356  [   32/  124]
train() client id: f_00002-0-1 loss: 1.064319  [   64/  124]
train() client id: f_00002-0-2 loss: 1.130265  [   96/  124]
train() client id: f_00002-1-0 loss: 0.952185  [   32/  124]
train() client id: f_00002-1-1 loss: 1.072798  [   64/  124]
train() client id: f_00002-1-2 loss: 1.237429  [   96/  124]
train() client id: f_00002-2-0 loss: 1.150725  [   32/  124]
train() client id: f_00002-2-1 loss: 0.971744  [   64/  124]
train() client id: f_00002-2-2 loss: 0.969258  [   96/  124]
train() client id: f_00002-3-0 loss: 0.823375  [   32/  124]
train() client id: f_00002-3-1 loss: 1.145063  [   64/  124]
train() client id: f_00002-3-2 loss: 1.061711  [   96/  124]
train() client id: f_00002-4-0 loss: 0.916131  [   32/  124]
train() client id: f_00002-4-1 loss: 1.073964  [   64/  124]
train() client id: f_00002-4-2 loss: 0.943398  [   96/  124]
train() client id: f_00002-5-0 loss: 0.988557  [   32/  124]
train() client id: f_00002-5-1 loss: 1.092035  [   64/  124]
train() client id: f_00002-5-2 loss: 0.794526  [   96/  124]
train() client id: f_00002-6-0 loss: 0.901327  [   32/  124]
train() client id: f_00002-6-1 loss: 1.077539  [   64/  124]
train() client id: f_00002-6-2 loss: 0.749014  [   96/  124]
train() client id: f_00002-7-0 loss: 0.978944  [   32/  124]
train() client id: f_00002-7-1 loss: 0.995948  [   64/  124]
train() client id: f_00002-7-2 loss: 0.924612  [   96/  124]
train() client id: f_00002-8-0 loss: 0.990305  [   32/  124]
train() client id: f_00002-8-1 loss: 0.866256  [   64/  124]
train() client id: f_00002-8-2 loss: 0.911559  [   96/  124]
train() client id: f_00002-9-0 loss: 1.065640  [   32/  124]
train() client id: f_00002-9-1 loss: 0.996823  [   64/  124]
train() client id: f_00002-9-2 loss: 0.902527  [   96/  124]
train() client id: f_00002-10-0 loss: 1.020903  [   32/  124]
train() client id: f_00002-10-1 loss: 0.975328  [   64/  124]
train() client id: f_00002-10-2 loss: 0.851128  [   96/  124]
train() client id: f_00002-11-0 loss: 1.009643  [   32/  124]
train() client id: f_00002-11-1 loss: 0.954588  [   64/  124]
train() client id: f_00002-11-2 loss: 0.923362  [   96/  124]
train() client id: f_00003-0-0 loss: 1.006827  [   32/   43]
train() client id: f_00003-1-0 loss: 1.023558  [   32/   43]
train() client id: f_00003-2-0 loss: 0.896925  [   32/   43]
train() client id: f_00003-3-0 loss: 1.025725  [   32/   43]
train() client id: f_00003-4-0 loss: 0.956664  [   32/   43]
train() client id: f_00003-5-0 loss: 0.952835  [   32/   43]
train() client id: f_00003-6-0 loss: 0.948461  [   32/   43]
train() client id: f_00003-7-0 loss: 0.897509  [   32/   43]
train() client id: f_00003-8-0 loss: 1.033647  [   32/   43]
train() client id: f_00003-9-0 loss: 0.858249  [   32/   43]
train() client id: f_00003-10-0 loss: 0.825524  [   32/   43]
train() client id: f_00003-11-0 loss: 0.983698  [   32/   43]
train() client id: f_00004-0-0 loss: 0.815841  [   32/  306]
train() client id: f_00004-0-1 loss: 0.895634  [   64/  306]
train() client id: f_00004-0-2 loss: 0.963130  [   96/  306]
train() client id: f_00004-0-3 loss: 0.834121  [  128/  306]
train() client id: f_00004-0-4 loss: 0.854301  [  160/  306]
train() client id: f_00004-0-5 loss: 0.777088  [  192/  306]
train() client id: f_00004-0-6 loss: 0.830870  [  224/  306]
train() client id: f_00004-0-7 loss: 0.902684  [  256/  306]
train() client id: f_00004-0-8 loss: 0.926143  [  288/  306]
train() client id: f_00004-1-0 loss: 0.934027  [   32/  306]
train() client id: f_00004-1-1 loss: 0.907353  [   64/  306]
train() client id: f_00004-1-2 loss: 0.916917  [   96/  306]
train() client id: f_00004-1-3 loss: 0.926952  [  128/  306]
train() client id: f_00004-1-4 loss: 0.748725  [  160/  306]
train() client id: f_00004-1-5 loss: 0.957789  [  192/  306]
train() client id: f_00004-1-6 loss: 0.946314  [  224/  306]
train() client id: f_00004-1-7 loss: 0.764201  [  256/  306]
train() client id: f_00004-1-8 loss: 0.844078  [  288/  306]
train() client id: f_00004-2-0 loss: 0.926849  [   32/  306]
train() client id: f_00004-2-1 loss: 0.838263  [   64/  306]
train() client id: f_00004-2-2 loss: 0.792613  [   96/  306]
train() client id: f_00004-2-3 loss: 0.819157  [  128/  306]
train() client id: f_00004-2-4 loss: 0.853233  [  160/  306]
train() client id: f_00004-2-5 loss: 0.926839  [  192/  306]
train() client id: f_00004-2-6 loss: 0.807729  [  224/  306]
train() client id: f_00004-2-7 loss: 0.974738  [  256/  306]
train() client id: f_00004-2-8 loss: 0.862668  [  288/  306]
train() client id: f_00004-3-0 loss: 0.832321  [   32/  306]
train() client id: f_00004-3-1 loss: 0.735103  [   64/  306]
train() client id: f_00004-3-2 loss: 0.785742  [   96/  306]
train() client id: f_00004-3-3 loss: 0.867912  [  128/  306]
train() client id: f_00004-3-4 loss: 0.837068  [  160/  306]
train() client id: f_00004-3-5 loss: 0.894611  [  192/  306]
train() client id: f_00004-3-6 loss: 0.966353  [  224/  306]
train() client id: f_00004-3-7 loss: 0.925729  [  256/  306]
train() client id: f_00004-3-8 loss: 0.954211  [  288/  306]
train() client id: f_00004-4-0 loss: 0.889491  [   32/  306]
train() client id: f_00004-4-1 loss: 0.845133  [   64/  306]
train() client id: f_00004-4-2 loss: 0.964420  [   96/  306]
train() client id: f_00004-4-3 loss: 0.805239  [  128/  306]
train() client id: f_00004-4-4 loss: 0.902785  [  160/  306]
train() client id: f_00004-4-5 loss: 0.962073  [  192/  306]
train() client id: f_00004-4-6 loss: 0.880046  [  224/  306]
train() client id: f_00004-4-7 loss: 0.717783  [  256/  306]
train() client id: f_00004-4-8 loss: 0.850421  [  288/  306]
train() client id: f_00004-5-0 loss: 0.847596  [   32/  306]
train() client id: f_00004-5-1 loss: 0.735757  [   64/  306]
train() client id: f_00004-5-2 loss: 0.910270  [   96/  306]
train() client id: f_00004-5-3 loss: 0.883196  [  128/  306]
train() client id: f_00004-5-4 loss: 0.849619  [  160/  306]
train() client id: f_00004-5-5 loss: 0.811590  [  192/  306]
train() client id: f_00004-5-6 loss: 0.981404  [  224/  306]
train() client id: f_00004-5-7 loss: 0.957959  [  256/  306]
train() client id: f_00004-5-8 loss: 0.852724  [  288/  306]
train() client id: f_00004-6-0 loss: 0.913631  [   32/  306]
train() client id: f_00004-6-1 loss: 0.815978  [   64/  306]
train() client id: f_00004-6-2 loss: 0.858292  [   96/  306]
train() client id: f_00004-6-3 loss: 0.840972  [  128/  306]
train() client id: f_00004-6-4 loss: 0.795582  [  160/  306]
train() client id: f_00004-6-5 loss: 0.890296  [  192/  306]
train() client id: f_00004-6-6 loss: 0.824053  [  224/  306]
train() client id: f_00004-6-7 loss: 0.864677  [  256/  306]
train() client id: f_00004-6-8 loss: 0.938094  [  288/  306]
train() client id: f_00004-7-0 loss: 0.891250  [   32/  306]
train() client id: f_00004-7-1 loss: 0.900449  [   64/  306]
train() client id: f_00004-7-2 loss: 0.829846  [   96/  306]
train() client id: f_00004-7-3 loss: 0.896272  [  128/  306]
train() client id: f_00004-7-4 loss: 0.825871  [  160/  306]
train() client id: f_00004-7-5 loss: 0.835636  [  192/  306]
train() client id: f_00004-7-6 loss: 0.913540  [  224/  306]
train() client id: f_00004-7-7 loss: 0.809369  [  256/  306]
train() client id: f_00004-7-8 loss: 0.833481  [  288/  306]
train() client id: f_00004-8-0 loss: 0.844307  [   32/  306]
train() client id: f_00004-8-1 loss: 0.899109  [   64/  306]
train() client id: f_00004-8-2 loss: 0.800569  [   96/  306]
train() client id: f_00004-8-3 loss: 0.920288  [  128/  306]
train() client id: f_00004-8-4 loss: 0.920941  [  160/  306]
train() client id: f_00004-8-5 loss: 0.774131  [  192/  306]
train() client id: f_00004-8-6 loss: 0.833442  [  224/  306]
train() client id: f_00004-8-7 loss: 0.872383  [  256/  306]
train() client id: f_00004-8-8 loss: 0.836442  [  288/  306]
train() client id: f_00004-9-0 loss: 0.780693  [   32/  306]
train() client id: f_00004-9-1 loss: 0.893986  [   64/  306]
train() client id: f_00004-9-2 loss: 0.918353  [   96/  306]
train() client id: f_00004-9-3 loss: 0.923280  [  128/  306]
train() client id: f_00004-9-4 loss: 0.848961  [  160/  306]
train() client id: f_00004-9-5 loss: 0.924302  [  192/  306]
train() client id: f_00004-9-6 loss: 0.835344  [  224/  306]
train() client id: f_00004-9-7 loss: 0.836498  [  256/  306]
train() client id: f_00004-9-8 loss: 0.836742  [  288/  306]
train() client id: f_00004-10-0 loss: 0.843982  [   32/  306]
train() client id: f_00004-10-1 loss: 0.856080  [   64/  306]
train() client id: f_00004-10-2 loss: 0.832545  [   96/  306]
train() client id: f_00004-10-3 loss: 0.885696  [  128/  306]
train() client id: f_00004-10-4 loss: 0.933757  [  160/  306]
train() client id: f_00004-10-5 loss: 1.005571  [  192/  306]
train() client id: f_00004-10-6 loss: 0.777519  [  224/  306]
train() client id: f_00004-10-7 loss: 0.798602  [  256/  306]
train() client id: f_00004-10-8 loss: 0.844427  [  288/  306]
train() client id: f_00004-11-0 loss: 0.889947  [   32/  306]
train() client id: f_00004-11-1 loss: 0.866677  [   64/  306]
train() client id: f_00004-11-2 loss: 0.836881  [   96/  306]
train() client id: f_00004-11-3 loss: 0.874346  [  128/  306]
train() client id: f_00004-11-4 loss: 0.887044  [  160/  306]
train() client id: f_00004-11-5 loss: 0.795329  [  192/  306]
train() client id: f_00004-11-6 loss: 0.876089  [  224/  306]
train() client id: f_00004-11-7 loss: 0.866842  [  256/  306]
train() client id: f_00004-11-8 loss: 0.900191  [  288/  306]
train() client id: f_00005-0-0 loss: 0.393407  [   32/  146]
train() client id: f_00005-0-1 loss: 0.596614  [   64/  146]
train() client id: f_00005-0-2 loss: 0.674827  [   96/  146]
train() client id: f_00005-0-3 loss: 0.510101  [  128/  146]
train() client id: f_00005-1-0 loss: 0.539674  [   32/  146]
train() client id: f_00005-1-1 loss: 0.701484  [   64/  146]
train() client id: f_00005-1-2 loss: 0.499567  [   96/  146]
train() client id: f_00005-1-3 loss: 0.301066  [  128/  146]
train() client id: f_00005-2-0 loss: 0.481579  [   32/  146]
train() client id: f_00005-2-1 loss: 0.652885  [   64/  146]
train() client id: f_00005-2-2 loss: 0.628021  [   96/  146]
train() client id: f_00005-2-3 loss: 0.473757  [  128/  146]
train() client id: f_00005-3-0 loss: 0.481034  [   32/  146]
train() client id: f_00005-3-1 loss: 0.497126  [   64/  146]
train() client id: f_00005-3-2 loss: 0.570119  [   96/  146]
train() client id: f_00005-3-3 loss: 0.655246  [  128/  146]
train() client id: f_00005-4-0 loss: 0.729492  [   32/  146]
train() client id: f_00005-4-1 loss: 0.615414  [   64/  146]
train() client id: f_00005-4-2 loss: 0.480187  [   96/  146]
train() client id: f_00005-4-3 loss: 0.374613  [  128/  146]
train() client id: f_00005-5-0 loss: 0.408587  [   32/  146]
train() client id: f_00005-5-1 loss: 0.481347  [   64/  146]
train() client id: f_00005-5-2 loss: 0.733827  [   96/  146]
train() client id: f_00005-5-3 loss: 0.287443  [  128/  146]
train() client id: f_00005-6-0 loss: 0.686151  [   32/  146]
train() client id: f_00005-6-1 loss: 0.334643  [   64/  146]
train() client id: f_00005-6-2 loss: 0.575343  [   96/  146]
train() client id: f_00005-6-3 loss: 0.632213  [  128/  146]
train() client id: f_00005-7-0 loss: 0.599174  [   32/  146]
train() client id: f_00005-7-1 loss: 0.421460  [   64/  146]
train() client id: f_00005-7-2 loss: 0.491308  [   96/  146]
train() client id: f_00005-7-3 loss: 0.557327  [  128/  146]
train() client id: f_00005-8-0 loss: 0.401966  [   32/  146]
train() client id: f_00005-8-1 loss: 0.268618  [   64/  146]
train() client id: f_00005-8-2 loss: 0.505962  [   96/  146]
train() client id: f_00005-8-3 loss: 0.733161  [  128/  146]
train() client id: f_00005-9-0 loss: 0.620993  [   32/  146]
train() client id: f_00005-9-1 loss: 0.403289  [   64/  146]
train() client id: f_00005-9-2 loss: 0.335061  [   96/  146]
train() client id: f_00005-9-3 loss: 0.584747  [  128/  146]
train() client id: f_00005-10-0 loss: 0.790882  [   32/  146]
train() client id: f_00005-10-1 loss: 0.362927  [   64/  146]
train() client id: f_00005-10-2 loss: 0.470598  [   96/  146]
train() client id: f_00005-10-3 loss: 0.464367  [  128/  146]
train() client id: f_00005-11-0 loss: 0.440811  [   32/  146]
train() client id: f_00005-11-1 loss: 0.570792  [   64/  146]
train() client id: f_00005-11-2 loss: 0.667512  [   96/  146]
train() client id: f_00005-11-3 loss: 0.449371  [  128/  146]
train() client id: f_00006-0-0 loss: 0.588934  [   32/   54]
train() client id: f_00006-1-0 loss: 0.536397  [   32/   54]
train() client id: f_00006-2-0 loss: 0.559787  [   32/   54]
train() client id: f_00006-3-0 loss: 0.579631  [   32/   54]
train() client id: f_00006-4-0 loss: 0.570118  [   32/   54]
train() client id: f_00006-5-0 loss: 0.573882  [   32/   54]
train() client id: f_00006-6-0 loss: 0.575934  [   32/   54]
train() client id: f_00006-7-0 loss: 0.585216  [   32/   54]
train() client id: f_00006-8-0 loss: 0.618567  [   32/   54]
train() client id: f_00006-9-0 loss: 0.526916  [   32/   54]
train() client id: f_00006-10-0 loss: 0.556492  [   32/   54]
train() client id: f_00006-11-0 loss: 0.579356  [   32/   54]
train() client id: f_00007-0-0 loss: 0.648684  [   32/  179]
train() client id: f_00007-0-1 loss: 0.685834  [   64/  179]
train() client id: f_00007-0-2 loss: 0.843533  [   96/  179]
train() client id: f_00007-0-3 loss: 0.820568  [  128/  179]
train() client id: f_00007-0-4 loss: 0.568003  [  160/  179]
train() client id: f_00007-1-0 loss: 0.722956  [   32/  179]
train() client id: f_00007-1-1 loss: 0.711670  [   64/  179]
train() client id: f_00007-1-2 loss: 0.757784  [   96/  179]
train() client id: f_00007-1-3 loss: 0.635081  [  128/  179]
train() client id: f_00007-1-4 loss: 0.674328  [  160/  179]
train() client id: f_00007-2-0 loss: 0.735034  [   32/  179]
train() client id: f_00007-2-1 loss: 0.515732  [   64/  179]
train() client id: f_00007-2-2 loss: 0.702968  [   96/  179]
train() client id: f_00007-2-3 loss: 0.832209  [  128/  179]
train() client id: f_00007-2-4 loss: 0.653031  [  160/  179]
train() client id: f_00007-3-0 loss: 0.766988  [   32/  179]
train() client id: f_00007-3-1 loss: 0.799573  [   64/  179]
train() client id: f_00007-3-2 loss: 0.513809  [   96/  179]
train() client id: f_00007-3-3 loss: 0.540164  [  128/  179]
train() client id: f_00007-3-4 loss: 0.790811  [  160/  179]
train() client id: f_00007-4-0 loss: 0.574769  [   32/  179]
train() client id: f_00007-4-1 loss: 0.594620  [   64/  179]
train() client id: f_00007-4-2 loss: 0.521077  [   96/  179]
train() client id: f_00007-4-3 loss: 0.964055  [  128/  179]
train() client id: f_00007-4-4 loss: 0.602849  [  160/  179]
train() client id: f_00007-5-0 loss: 0.748629  [   32/  179]
train() client id: f_00007-5-1 loss: 0.512884  [   64/  179]
train() client id: f_00007-5-2 loss: 0.615920  [   96/  179]
train() client id: f_00007-5-3 loss: 0.666070  [  128/  179]
train() client id: f_00007-5-4 loss: 0.686509  [  160/  179]
train() client id: f_00007-6-0 loss: 0.672950  [   32/  179]
train() client id: f_00007-6-1 loss: 0.510550  [   64/  179]
train() client id: f_00007-6-2 loss: 0.681009  [   96/  179]
train() client id: f_00007-6-3 loss: 0.591814  [  128/  179]
train() client id: f_00007-6-4 loss: 0.642832  [  160/  179]
train() client id: f_00007-7-0 loss: 0.695688  [   32/  179]
train() client id: f_00007-7-1 loss: 0.598384  [   64/  179]
train() client id: f_00007-7-2 loss: 0.626874  [   96/  179]
train() client id: f_00007-7-3 loss: 0.658711  [  128/  179]
train() client id: f_00007-7-4 loss: 0.633727  [  160/  179]
train() client id: f_00007-8-0 loss: 0.496941  [   32/  179]
train() client id: f_00007-8-1 loss: 0.864150  [   64/  179]
train() client id: f_00007-8-2 loss: 0.676451  [   96/  179]
train() client id: f_00007-8-3 loss: 0.677310  [  128/  179]
train() client id: f_00007-8-4 loss: 0.561845  [  160/  179]
train() client id: f_00007-9-0 loss: 0.696041  [   32/  179]
train() client id: f_00007-9-1 loss: 0.619947  [   64/  179]
train() client id: f_00007-9-2 loss: 0.757149  [   96/  179]
train() client id: f_00007-9-3 loss: 0.561662  [  128/  179]
train() client id: f_00007-9-4 loss: 0.664705  [  160/  179]
train() client id: f_00007-10-0 loss: 0.690043  [   32/  179]
train() client id: f_00007-10-1 loss: 0.610742  [   64/  179]
train() client id: f_00007-10-2 loss: 0.564017  [   96/  179]
train() client id: f_00007-10-3 loss: 0.500690  [  128/  179]
train() client id: f_00007-10-4 loss: 0.743041  [  160/  179]
train() client id: f_00007-11-0 loss: 0.581403  [   32/  179]
train() client id: f_00007-11-1 loss: 0.848583  [   64/  179]
train() client id: f_00007-11-2 loss: 0.475158  [   96/  179]
train() client id: f_00007-11-3 loss: 0.767454  [  128/  179]
train() client id: f_00007-11-4 loss: 0.614275  [  160/  179]
train() client id: f_00008-0-0 loss: 0.810394  [   32/  130]
train() client id: f_00008-0-1 loss: 0.693656  [   64/  130]
train() client id: f_00008-0-2 loss: 0.722316  [   96/  130]
train() client id: f_00008-0-3 loss: 0.731211  [  128/  130]
train() client id: f_00008-1-0 loss: 0.754509  [   32/  130]
train() client id: f_00008-1-1 loss: 0.758471  [   64/  130]
train() client id: f_00008-1-2 loss: 0.742583  [   96/  130]
train() client id: f_00008-1-3 loss: 0.717101  [  128/  130]
train() client id: f_00008-2-0 loss: 0.728823  [   32/  130]
train() client id: f_00008-2-1 loss: 0.678852  [   64/  130]
train() client id: f_00008-2-2 loss: 0.638598  [   96/  130]
train() client id: f_00008-2-3 loss: 0.892593  [  128/  130]
train() client id: f_00008-3-0 loss: 0.832518  [   32/  130]
train() client id: f_00008-3-1 loss: 0.729162  [   64/  130]
train() client id: f_00008-3-2 loss: 0.703928  [   96/  130]
train() client id: f_00008-3-3 loss: 0.657681  [  128/  130]
train() client id: f_00008-4-0 loss: 0.785145  [   32/  130]
train() client id: f_00008-4-1 loss: 0.793147  [   64/  130]
train() client id: f_00008-4-2 loss: 0.664245  [   96/  130]
train() client id: f_00008-4-3 loss: 0.725116  [  128/  130]
train() client id: f_00008-5-0 loss: 0.749661  [   32/  130]
train() client id: f_00008-5-1 loss: 0.685614  [   64/  130]
train() client id: f_00008-5-2 loss: 0.712197  [   96/  130]
train() client id: f_00008-5-3 loss: 0.794495  [  128/  130]
train() client id: f_00008-6-0 loss: 0.683351  [   32/  130]
train() client id: f_00008-6-1 loss: 0.762043  [   64/  130]
train() client id: f_00008-6-2 loss: 0.759748  [   96/  130]
train() client id: f_00008-6-3 loss: 0.712511  [  128/  130]
train() client id: f_00008-7-0 loss: 0.681417  [   32/  130]
train() client id: f_00008-7-1 loss: 0.694648  [   64/  130]
train() client id: f_00008-7-2 loss: 0.791703  [   96/  130]
train() client id: f_00008-7-3 loss: 0.793419  [  128/  130]
train() client id: f_00008-8-0 loss: 0.624008  [   32/  130]
train() client id: f_00008-8-1 loss: 0.674986  [   64/  130]
train() client id: f_00008-8-2 loss: 0.716973  [   96/  130]
train() client id: f_00008-8-3 loss: 0.933071  [  128/  130]
train() client id: f_00008-9-0 loss: 0.699721  [   32/  130]
train() client id: f_00008-9-1 loss: 0.807314  [   64/  130]
train() client id: f_00008-9-2 loss: 0.639950  [   96/  130]
train() client id: f_00008-9-3 loss: 0.815936  [  128/  130]
train() client id: f_00008-10-0 loss: 0.688301  [   32/  130]
train() client id: f_00008-10-1 loss: 0.704600  [   64/  130]
train() client id: f_00008-10-2 loss: 0.802845  [   96/  130]
train() client id: f_00008-10-3 loss: 0.771244  [  128/  130]
train() client id: f_00008-11-0 loss: 0.711389  [   32/  130]
train() client id: f_00008-11-1 loss: 0.698899  [   64/  130]
train() client id: f_00008-11-2 loss: 0.831881  [   96/  130]
train() client id: f_00008-11-3 loss: 0.718744  [  128/  130]
train() client id: f_00009-0-0 loss: 1.256241  [   32/  118]
train() client id: f_00009-0-1 loss: 0.910286  [   64/  118]
train() client id: f_00009-0-2 loss: 1.100581  [   96/  118]
train() client id: f_00009-1-0 loss: 1.025703  [   32/  118]
train() client id: f_00009-1-1 loss: 1.049436  [   64/  118]
train() client id: f_00009-1-2 loss: 1.148023  [   96/  118]
train() client id: f_00009-2-0 loss: 1.064521  [   32/  118]
train() client id: f_00009-2-1 loss: 0.961744  [   64/  118]
train() client id: f_00009-2-2 loss: 0.977492  [   96/  118]
train() client id: f_00009-3-0 loss: 0.898306  [   32/  118]
train() client id: f_00009-3-1 loss: 1.004153  [   64/  118]
train() client id: f_00009-3-2 loss: 1.037219  [   96/  118]
train() client id: f_00009-4-0 loss: 0.819746  [   32/  118]
train() client id: f_00009-4-1 loss: 0.931799  [   64/  118]
train() client id: f_00009-4-2 loss: 1.112150  [   96/  118]
train() client id: f_00009-5-0 loss: 0.875676  [   32/  118]
train() client id: f_00009-5-1 loss: 0.848505  [   64/  118]
train() client id: f_00009-5-2 loss: 1.006248  [   96/  118]
train() client id: f_00009-6-0 loss: 0.970742  [   32/  118]
train() client id: f_00009-6-1 loss: 0.894737  [   64/  118]
train() client id: f_00009-6-2 loss: 0.998338  [   96/  118]
train() client id: f_00009-7-0 loss: 0.973742  [   32/  118]
train() client id: f_00009-7-1 loss: 0.993892  [   64/  118]
train() client id: f_00009-7-2 loss: 0.822105  [   96/  118]
train() client id: f_00009-8-0 loss: 0.931482  [   32/  118]
train() client id: f_00009-8-1 loss: 0.899932  [   64/  118]
train() client id: f_00009-8-2 loss: 0.971419  [   96/  118]
train() client id: f_00009-9-0 loss: 1.095677  [   32/  118]
train() client id: f_00009-9-1 loss: 0.871224  [   64/  118]
train() client id: f_00009-9-2 loss: 0.927383  [   96/  118]
train() client id: f_00009-10-0 loss: 0.849528  [   32/  118]
train() client id: f_00009-10-1 loss: 0.964347  [   64/  118]
train() client id: f_00009-10-2 loss: 0.930392  [   96/  118]
train() client id: f_00009-11-0 loss: 0.936505  [   32/  118]
train() client id: f_00009-11-1 loss: 0.889264  [   64/  118]
train() client id: f_00009-11-2 loss: 0.902926  [   96/  118]
At round 14 accuracy: 0.6312997347480106
At round 14 training accuracy: 0.5841716968477532
At round 14 training loss: 0.8341721273237444
update_location
xs = -3.905658 4.200318 90.009024 18.811294 0.979296 3.956410 -52.443192 -31.324852 74.663977 -17.060879 
ys = 82.587959 65.555839 1.320614 -52.455176 44.350187 27.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 8.788573831882571
ys mean: 18.894142535528704
dists_uav = 129.753710 119.646189 134.548758 114.478864 109.397889 103.871460 112.947682 104.794669 126.029280 101.523817 
uav_gains = -102.828633 -101.947677 -103.223044 -101.468243 -100.975281 -100.412433 -101.322024 -100.508511 -102.512212 -100.164218 
uav_gains_db_mean: -101.53622762687334
dists_bs = 194.456316 209.978043 316.850614 298.828839 219.175926 231.709022 215.802702 225.788402 295.155246 232.777274 
bs_gains = -103.654077 -104.587928 -109.590987 -108.878889 -105.109259 -105.785462 -104.920652 -105.470705 -108.728473 -105.841395 
bs_gains_db_mean: -106.25678262560085
Round 15
-------------------------------
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.01168963 18.78472696  8.87441189  3.17492049 21.66795492 10.44235678
  3.94614583 12.71610262  9.35523145  8.4772718 ]
obj_prev = 106.45081237176021
eta_min = 4.89584141592709e-11	eta_max = 0.9211440323876784
af = 22.501910530386866	bf = 1.761176745733534	zeta = 24.752101583425553	eta = 0.9090909090909091
af = 22.501910530386866	bf = 1.761176745733534	zeta = 42.96421672758091	eta = 0.5237360818902523
af = 22.501910530386866	bf = 1.761176745733534	zeta = 34.25554170777676	eta = 0.6568838035709252
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.69318907763576	eta = 0.6882751779568552
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.6157469047972	eta = 0.6899094046832094
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.61554299983585	eta = 0.6899137178399916
eta = 0.6899137178399916
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [0.0306296  0.06441941 0.03014342 0.01045296 0.07438618 0.03549145
 0.01312697 0.04351349 0.03160197 0.02868487]
ene_total = [2.80683503 5.36552507 2.7806876  1.27556609 6.12194985 3.25313134
 1.47109077 3.71222193 3.07937974 2.74915557]
ti_comp = [0.32219551 0.3157955  0.32085207 0.32647704 0.31367854 0.31076018
 0.32690731 0.32920522 0.2952455  0.31050951]
ti_coms = [0.07139244 0.07779246 0.07273588 0.06711092 0.07990942 0.08282778
 0.06668064 0.06438274 0.09834245 0.08307845]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.73007617e-05 1.67539923e-04 1.66283024e-05 6.69719050e-07
 2.61449152e-04 2.89334621e-05 1.32289255e-06 4.75136331e-05
 2.26285138e-05 1.52999065e-05]
ene_total = [0.5318468  0.59057441 0.54178073 0.49879221 0.61328584 0.61769428
 0.4956431  0.48199869 0.73252479 0.61854397]
optimize_network iter = 0 obj = 5.7226848172315465
eta = 0.6899137178399916
freqs = [4.75326373e+07 1.01995448e+08 4.69740154e+07 1.60087223e+07
 1.18570726e+08 5.71042387e+07 2.00775158e+07 6.60886950e+07
 5.35181277e+07 4.61899994e+07]
eta_min = 0.6899137178400166	eta_max = 0.6899137178399891
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 0.03956004889123434	eta = 0.9090909090909091
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 19.410648956797782	eta = 0.0018527809600934662
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9911157923761436	eta = 0.018062074012930685
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9405838032428373	eta = 0.018532402852232132
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9405717992779243	eta = 0.01853251748973932
eta = 0.01853251748973932
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.85887615e-04 1.80012864e-03 1.78662392e-04 7.19578012e-06
 2.80913409e-03 3.10874884e-04 1.42137870e-05 5.10509081e-04
 2.43131519e-04 1.64389475e-04]
ene_total = [0.1719908  0.2249194  0.17497549 0.15774238 0.25358093 0.2017751
 0.15689688 0.1631543  0.23661227 0.19892425]
ti_comp = [0.32219551 0.3157955  0.32085207 0.32647704 0.31367854 0.31076018
 0.32690731 0.32920522 0.2952455  0.31050951]
ti_coms = [0.07139244 0.07779246 0.07273588 0.06711092 0.07990942 0.08282778
 0.06668064 0.06438274 0.09834245 0.08307845]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.73007617e-05 1.67539923e-04 1.66283024e-05 6.69719050e-07
 2.61449152e-04 2.89334621e-05 1.32289255e-06 4.75136331e-05
 2.26285138e-05 1.52999065e-05]
ene_total = [0.5318468  0.59057441 0.54178073 0.49879221 0.61328584 0.61769428
 0.4956431  0.48199869 0.73252479 0.61854397]
optimize_network iter = 1 obj = 5.722684817232005
eta = 0.6899137178400166
freqs = [4.75326373e+07 1.01995448e+08 4.69740154e+07 1.60087223e+07
 1.18570726e+08 5.71042387e+07 2.00775158e+07 6.60886950e+07
 5.35181277e+07 4.61899994e+07]
Done!
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.70806981e-05 1.65408836e-04 1.64167924e-05 6.61200306e-07
 2.58123551e-04 2.85654321e-05 1.30606552e-06 4.69092656e-05
 2.23406819e-05 1.51052936e-05]
ene_total = [0.00715633 0.00794465 0.00729    0.00671175 0.00824907 0.00831134
 0.00666937 0.00648518 0.00985659 0.00832295]
At round 15 energy consumption: 0.07699723536707746
At round 15 eta: 0.6899137178400166
At round 15 a_n: 23.044415151714617
At round 15 local rounds: 12.154605075633594
At round 15 global rounds: 74.31613869273092
gradient difference: 0.41439929604530334
train() client id: f_00000-0-0 loss: 1.261472  [   32/  126]
train() client id: f_00000-0-1 loss: 1.155400  [   64/  126]
train() client id: f_00000-0-2 loss: 0.887756  [   96/  126]
train() client id: f_00000-1-0 loss: 1.028112  [   32/  126]
train() client id: f_00000-1-1 loss: 1.173124  [   64/  126]
train() client id: f_00000-1-2 loss: 0.943139  [   96/  126]
train() client id: f_00000-2-0 loss: 0.945267  [   32/  126]
train() client id: f_00000-2-1 loss: 0.915602  [   64/  126]
train() client id: f_00000-2-2 loss: 1.005556  [   96/  126]
train() client id: f_00000-3-0 loss: 0.936027  [   32/  126]
train() client id: f_00000-3-1 loss: 0.995041  [   64/  126]
train() client id: f_00000-3-2 loss: 0.911278  [   96/  126]
train() client id: f_00000-4-0 loss: 0.968470  [   32/  126]
train() client id: f_00000-4-1 loss: 0.862365  [   64/  126]
train() client id: f_00000-4-2 loss: 1.004657  [   96/  126]
train() client id: f_00000-5-0 loss: 0.917932  [   32/  126]
train() client id: f_00000-5-1 loss: 0.905555  [   64/  126]
train() client id: f_00000-5-2 loss: 0.899828  [   96/  126]
train() client id: f_00000-6-0 loss: 0.870704  [   32/  126]
train() client id: f_00000-6-1 loss: 0.893885  [   64/  126]
train() client id: f_00000-6-2 loss: 0.941310  [   96/  126]
train() client id: f_00000-7-0 loss: 0.718214  [   32/  126]
train() client id: f_00000-7-1 loss: 0.996836  [   64/  126]
train() client id: f_00000-7-2 loss: 0.888237  [   96/  126]
train() client id: f_00000-8-0 loss: 0.832384  [   32/  126]
train() client id: f_00000-8-1 loss: 0.909017  [   64/  126]
train() client id: f_00000-8-2 loss: 0.803275  [   96/  126]
train() client id: f_00000-9-0 loss: 0.840248  [   32/  126]
train() client id: f_00000-9-1 loss: 0.845595  [   64/  126]
train() client id: f_00000-9-2 loss: 0.888939  [   96/  126]
train() client id: f_00000-10-0 loss: 0.822434  [   32/  126]
train() client id: f_00000-10-1 loss: 0.877091  [   64/  126]
train() client id: f_00000-10-2 loss: 0.831531  [   96/  126]
train() client id: f_00000-11-0 loss: 0.853338  [   32/  126]
train() client id: f_00000-11-1 loss: 0.933159  [   64/  126]
train() client id: f_00000-11-2 loss: 0.813318  [   96/  126]
train() client id: f_00001-0-0 loss: 0.587981  [   32/  265]
train() client id: f_00001-0-1 loss: 0.469421  [   64/  265]
train() client id: f_00001-0-2 loss: 0.448268  [   96/  265]
train() client id: f_00001-0-3 loss: 0.508035  [  128/  265]
train() client id: f_00001-0-4 loss: 0.560807  [  160/  265]
train() client id: f_00001-0-5 loss: 0.472146  [  192/  265]
train() client id: f_00001-0-6 loss: 0.462464  [  224/  265]
train() client id: f_00001-0-7 loss: 0.433710  [  256/  265]
train() client id: f_00001-1-0 loss: 0.581536  [   32/  265]
train() client id: f_00001-1-1 loss: 0.504814  [   64/  265]
train() client id: f_00001-1-2 loss: 0.483082  [   96/  265]
train() client id: f_00001-1-3 loss: 0.458801  [  128/  265]
train() client id: f_00001-1-4 loss: 0.463679  [  160/  265]
train() client id: f_00001-1-5 loss: 0.519477  [  192/  265]
train() client id: f_00001-1-6 loss: 0.558193  [  224/  265]
train() client id: f_00001-1-7 loss: 0.418336  [  256/  265]
train() client id: f_00001-2-0 loss: 0.553895  [   32/  265]
train() client id: f_00001-2-1 loss: 0.516064  [   64/  265]
train() client id: f_00001-2-2 loss: 0.525021  [   96/  265]
train() client id: f_00001-2-3 loss: 0.439549  [  128/  265]
train() client id: f_00001-2-4 loss: 0.540202  [  160/  265]
train() client id: f_00001-2-5 loss: 0.558435  [  192/  265]
train() client id: f_00001-2-6 loss: 0.418452  [  224/  265]
train() client id: f_00001-2-7 loss: 0.405679  [  256/  265]
train() client id: f_00001-3-0 loss: 0.491038  [   32/  265]
train() client id: f_00001-3-1 loss: 0.476488  [   64/  265]
train() client id: f_00001-3-2 loss: 0.395250  [   96/  265]
train() client id: f_00001-3-3 loss: 0.606719  [  128/  265]
train() client id: f_00001-3-4 loss: 0.387543  [  160/  265]
train() client id: f_00001-3-5 loss: 0.498356  [  192/  265]
train() client id: f_00001-3-6 loss: 0.552551  [  224/  265]
train() client id: f_00001-3-7 loss: 0.503020  [  256/  265]
train() client id: f_00001-4-0 loss: 0.578177  [   32/  265]
train() client id: f_00001-4-1 loss: 0.408729  [   64/  265]
train() client id: f_00001-4-2 loss: 0.367349  [   96/  265]
train() client id: f_00001-4-3 loss: 0.466315  [  128/  265]
train() client id: f_00001-4-4 loss: 0.501130  [  160/  265]
train() client id: f_00001-4-5 loss: 0.511164  [  192/  265]
train() client id: f_00001-4-6 loss: 0.509914  [  224/  265]
train() client id: f_00001-4-7 loss: 0.476126  [  256/  265]
train() client id: f_00001-5-0 loss: 0.544744  [   32/  265]
train() client id: f_00001-5-1 loss: 0.392967  [   64/  265]
train() client id: f_00001-5-2 loss: 0.603691  [   96/  265]
train() client id: f_00001-5-3 loss: 0.409430  [  128/  265]
train() client id: f_00001-5-4 loss: 0.417882  [  160/  265]
train() client id: f_00001-5-5 loss: 0.527231  [  192/  265]
train() client id: f_00001-5-6 loss: 0.570417  [  224/  265]
train() client id: f_00001-5-7 loss: 0.406752  [  256/  265]
train() client id: f_00001-6-0 loss: 0.393962  [   32/  265]
train() client id: f_00001-6-1 loss: 0.417459  [   64/  265]
train() client id: f_00001-6-2 loss: 0.395236  [   96/  265]
train() client id: f_00001-6-3 loss: 0.523795  [  128/  265]
train() client id: f_00001-6-4 loss: 0.441978  [  160/  265]
train() client id: f_00001-6-5 loss: 0.619991  [  192/  265]
train() client id: f_00001-6-6 loss: 0.519379  [  224/  265]
train() client id: f_00001-6-7 loss: 0.508222  [  256/  265]
train() client id: f_00001-7-0 loss: 0.566680  [   32/  265]
train() client id: f_00001-7-1 loss: 0.468837  [   64/  265]
train() client id: f_00001-7-2 loss: 0.468272  [   96/  265]
train() client id: f_00001-7-3 loss: 0.580278  [  128/  265]
train() client id: f_00001-7-4 loss: 0.400628  [  160/  265]
train() client id: f_00001-7-5 loss: 0.436943  [  192/  265]
train() client id: f_00001-7-6 loss: 0.435235  [  224/  265]
train() client id: f_00001-7-7 loss: 0.504404  [  256/  265]
train() client id: f_00001-8-0 loss: 0.408241  [   32/  265]
train() client id: f_00001-8-1 loss: 0.489955  [   64/  265]
train() client id: f_00001-8-2 loss: 0.467044  [   96/  265]
train() client id: f_00001-8-3 loss: 0.459409  [  128/  265]
train() client id: f_00001-8-4 loss: 0.627869  [  160/  265]
train() client id: f_00001-8-5 loss: 0.507642  [  192/  265]
train() client id: f_00001-8-6 loss: 0.525802  [  224/  265]
train() client id: f_00001-8-7 loss: 0.384172  [  256/  265]
train() client id: f_00001-9-0 loss: 0.466910  [   32/  265]
train() client id: f_00001-9-1 loss: 0.572313  [   64/  265]
train() client id: f_00001-9-2 loss: 0.462722  [   96/  265]
train() client id: f_00001-9-3 loss: 0.499736  [  128/  265]
train() client id: f_00001-9-4 loss: 0.468749  [  160/  265]
train() client id: f_00001-9-5 loss: 0.405171  [  192/  265]
train() client id: f_00001-9-6 loss: 0.551927  [  224/  265]
train() client id: f_00001-9-7 loss: 0.396932  [  256/  265]
train() client id: f_00001-10-0 loss: 0.414307  [   32/  265]
train() client id: f_00001-10-1 loss: 0.568232  [   64/  265]
train() client id: f_00001-10-2 loss: 0.387930  [   96/  265]
train() client id: f_00001-10-3 loss: 0.562127  [  128/  265]
train() client id: f_00001-10-4 loss: 0.487698  [  160/  265]
train() client id: f_00001-10-5 loss: 0.496685  [  192/  265]
train() client id: f_00001-10-6 loss: 0.428091  [  224/  265]
train() client id: f_00001-10-7 loss: 0.539782  [  256/  265]
train() client id: f_00001-11-0 loss: 0.514553  [   32/  265]
train() client id: f_00001-11-1 loss: 0.548020  [   64/  265]
train() client id: f_00001-11-2 loss: 0.402601  [   96/  265]
train() client id: f_00001-11-3 loss: 0.451424  [  128/  265]
train() client id: f_00001-11-4 loss: 0.462723  [  160/  265]
train() client id: f_00001-11-5 loss: 0.513200  [  192/  265]
train() client id: f_00001-11-6 loss: 0.380742  [  224/  265]
train() client id: f_00001-11-7 loss: 0.560978  [  256/  265]
train() client id: f_00002-0-0 loss: 1.235981  [   32/  124]
train() client id: f_00002-0-1 loss: 0.996522  [   64/  124]
train() client id: f_00002-0-2 loss: 1.397306  [   96/  124]
train() client id: f_00002-1-0 loss: 1.249803  [   32/  124]
train() client id: f_00002-1-1 loss: 1.014218  [   64/  124]
train() client id: f_00002-1-2 loss: 1.038564  [   96/  124]
train() client id: f_00002-2-0 loss: 1.127211  [   32/  124]
train() client id: f_00002-2-1 loss: 1.145448  [   64/  124]
train() client id: f_00002-2-2 loss: 1.194321  [   96/  124]
train() client id: f_00002-3-0 loss: 1.001393  [   32/  124]
train() client id: f_00002-3-1 loss: 1.227636  [   64/  124]
train() client id: f_00002-3-2 loss: 0.928573  [   96/  124]
train() client id: f_00002-4-0 loss: 1.008198  [   32/  124]
train() client id: f_00002-4-1 loss: 1.151158  [   64/  124]
train() client id: f_00002-4-2 loss: 0.994094  [   96/  124]
train() client id: f_00002-5-0 loss: 1.135728  [   32/  124]
train() client id: f_00002-5-1 loss: 0.964128  [   64/  124]
train() client id: f_00002-5-2 loss: 0.967437  [   96/  124]
train() client id: f_00002-6-0 loss: 0.943315  [   32/  124]
train() client id: f_00002-6-1 loss: 0.903290  [   64/  124]
train() client id: f_00002-6-2 loss: 1.020250  [   96/  124]
train() client id: f_00002-7-0 loss: 0.900329  [   32/  124]
train() client id: f_00002-7-1 loss: 1.047651  [   64/  124]
train() client id: f_00002-7-2 loss: 0.956007  [   96/  124]
train() client id: f_00002-8-0 loss: 0.933370  [   32/  124]
train() client id: f_00002-8-1 loss: 1.074761  [   64/  124]
train() client id: f_00002-8-2 loss: 0.821997  [   96/  124]
train() client id: f_00002-9-0 loss: 0.959940  [   32/  124]
train() client id: f_00002-9-1 loss: 0.967903  [   64/  124]
train() client id: f_00002-9-2 loss: 1.031176  [   96/  124]
train() client id: f_00002-10-0 loss: 1.032443  [   32/  124]
train() client id: f_00002-10-1 loss: 0.892859  [   64/  124]
train() client id: f_00002-10-2 loss: 0.934888  [   96/  124]
train() client id: f_00002-11-0 loss: 0.919339  [   32/  124]
train() client id: f_00002-11-1 loss: 0.892674  [   64/  124]
train() client id: f_00002-11-2 loss: 1.019218  [   96/  124]
train() client id: f_00003-0-0 loss: 0.901752  [   32/   43]
train() client id: f_00003-1-0 loss: 0.736104  [   32/   43]
train() client id: f_00003-2-0 loss: 0.789257  [   32/   43]
train() client id: f_00003-3-0 loss: 0.964605  [   32/   43]
train() client id: f_00003-4-0 loss: 0.749803  [   32/   43]
train() client id: f_00003-5-0 loss: 0.716001  [   32/   43]
train() client id: f_00003-6-0 loss: 0.611248  [   32/   43]
train() client id: f_00003-7-0 loss: 0.942210  [   32/   43]
train() client id: f_00003-8-0 loss: 0.854113  [   32/   43]
train() client id: f_00003-9-0 loss: 0.892166  [   32/   43]
train() client id: f_00003-10-0 loss: 0.901883  [   32/   43]
train() client id: f_00003-11-0 loss: 0.781222  [   32/   43]
train() client id: f_00004-0-0 loss: 0.879557  [   32/  306]
train() client id: f_00004-0-1 loss: 0.845272  [   64/  306]
train() client id: f_00004-0-2 loss: 0.748852  [   96/  306]
train() client id: f_00004-0-3 loss: 0.891161  [  128/  306]
train() client id: f_00004-0-4 loss: 0.747980  [  160/  306]
train() client id: f_00004-0-5 loss: 0.750150  [  192/  306]
train() client id: f_00004-0-6 loss: 0.796744  [  224/  306]
train() client id: f_00004-0-7 loss: 0.744342  [  256/  306]
train() client id: f_00004-0-8 loss: 0.862831  [  288/  306]
train() client id: f_00004-1-0 loss: 0.807761  [   32/  306]
train() client id: f_00004-1-1 loss: 0.765759  [   64/  306]
train() client id: f_00004-1-2 loss: 0.794755  [   96/  306]
train() client id: f_00004-1-3 loss: 0.916841  [  128/  306]
train() client id: f_00004-1-4 loss: 0.764168  [  160/  306]
train() client id: f_00004-1-5 loss: 0.785979  [  192/  306]
train() client id: f_00004-1-6 loss: 0.818436  [  224/  306]
train() client id: f_00004-1-7 loss: 0.763350  [  256/  306]
train() client id: f_00004-1-8 loss: 0.863987  [  288/  306]
train() client id: f_00004-2-0 loss: 0.820772  [   32/  306]
train() client id: f_00004-2-1 loss: 0.849419  [   64/  306]
train() client id: f_00004-2-2 loss: 0.675619  [   96/  306]
train() client id: f_00004-2-3 loss: 0.844624  [  128/  306]
train() client id: f_00004-2-4 loss: 0.683648  [  160/  306]
train() client id: f_00004-2-5 loss: 0.788188  [  192/  306]
train() client id: f_00004-2-6 loss: 0.875821  [  224/  306]
train() client id: f_00004-2-7 loss: 0.733424  [  256/  306]
train() client id: f_00004-2-8 loss: 0.952439  [  288/  306]
train() client id: f_00004-3-0 loss: 0.852374  [   32/  306]
train() client id: f_00004-3-1 loss: 0.711924  [   64/  306]
train() client id: f_00004-3-2 loss: 0.823762  [   96/  306]
train() client id: f_00004-3-3 loss: 0.809310  [  128/  306]
train() client id: f_00004-3-4 loss: 0.705456  [  160/  306]
train() client id: f_00004-3-5 loss: 0.785135  [  192/  306]
train() client id: f_00004-3-6 loss: 0.761560  [  224/  306]
train() client id: f_00004-3-7 loss: 0.771472  [  256/  306]
train() client id: f_00004-3-8 loss: 0.928396  [  288/  306]
train() client id: f_00004-4-0 loss: 0.649913  [   32/  306]
train() client id: f_00004-4-1 loss: 0.791146  [   64/  306]
train() client id: f_00004-4-2 loss: 0.888622  [   96/  306]
train() client id: f_00004-4-3 loss: 0.861563  [  128/  306]
train() client id: f_00004-4-4 loss: 0.795306  [  160/  306]
train() client id: f_00004-4-5 loss: 0.780365  [  192/  306]
train() client id: f_00004-4-6 loss: 0.788071  [  224/  306]
train() client id: f_00004-4-7 loss: 0.809726  [  256/  306]
train() client id: f_00004-4-8 loss: 0.868542  [  288/  306]
train() client id: f_00004-5-0 loss: 0.864980  [   32/  306]
train() client id: f_00004-5-1 loss: 0.695148  [   64/  306]
train() client id: f_00004-5-2 loss: 0.792140  [   96/  306]
train() client id: f_00004-5-3 loss: 0.939150  [  128/  306]
train() client id: f_00004-5-4 loss: 0.882392  [  160/  306]
train() client id: f_00004-5-5 loss: 0.824730  [  192/  306]
train() client id: f_00004-5-6 loss: 0.837738  [  224/  306]
train() client id: f_00004-5-7 loss: 0.846137  [  256/  306]
train() client id: f_00004-5-8 loss: 0.692466  [  288/  306]
train() client id: f_00004-6-0 loss: 0.932740  [   32/  306]
train() client id: f_00004-6-1 loss: 0.840330  [   64/  306]
train() client id: f_00004-6-2 loss: 0.679880  [   96/  306]
train() client id: f_00004-6-3 loss: 0.827625  [  128/  306]
train() client id: f_00004-6-4 loss: 0.823140  [  160/  306]
train() client id: f_00004-6-5 loss: 0.777171  [  192/  306]
train() client id: f_00004-6-6 loss: 0.793910  [  224/  306]
train() client id: f_00004-6-7 loss: 0.722953  [  256/  306]
train() client id: f_00004-6-8 loss: 0.846522  [  288/  306]
train() client id: f_00004-7-0 loss: 0.755850  [   32/  306]
train() client id: f_00004-7-1 loss: 0.865545  [   64/  306]
train() client id: f_00004-7-2 loss: 0.795488  [   96/  306]
train() client id: f_00004-7-3 loss: 0.704525  [  128/  306]
train() client id: f_00004-7-4 loss: 0.786058  [  160/  306]
train() client id: f_00004-7-5 loss: 0.959306  [  192/  306]
train() client id: f_00004-7-6 loss: 0.837783  [  224/  306]
train() client id: f_00004-7-7 loss: 0.833288  [  256/  306]
train() client id: f_00004-7-8 loss: 0.837610  [  288/  306]
train() client id: f_00004-8-0 loss: 0.798636  [   32/  306]
train() client id: f_00004-8-1 loss: 0.755874  [   64/  306]
train() client id: f_00004-8-2 loss: 0.866592  [   96/  306]
train() client id: f_00004-8-3 loss: 0.721200  [  128/  306]
train() client id: f_00004-8-4 loss: 0.912388  [  160/  306]
train() client id: f_00004-8-5 loss: 0.863335  [  192/  306]
train() client id: f_00004-8-6 loss: 0.801529  [  224/  306]
train() client id: f_00004-8-7 loss: 0.751287  [  256/  306]
train() client id: f_00004-8-8 loss: 0.809986  [  288/  306]
train() client id: f_00004-9-0 loss: 0.860747  [   32/  306]
train() client id: f_00004-9-1 loss: 0.900250  [   64/  306]
train() client id: f_00004-9-2 loss: 0.862377  [   96/  306]
train() client id: f_00004-9-3 loss: 0.764805  [  128/  306]
train() client id: f_00004-9-4 loss: 0.717608  [  160/  306]
train() client id: f_00004-9-5 loss: 0.745925  [  192/  306]
train() client id: f_00004-9-6 loss: 0.662500  [  224/  306]
train() client id: f_00004-9-7 loss: 0.941647  [  256/  306]
train() client id: f_00004-9-8 loss: 0.916384  [  288/  306]
train() client id: f_00004-10-0 loss: 0.796584  [   32/  306]
train() client id: f_00004-10-1 loss: 0.886307  [   64/  306]
train() client id: f_00004-10-2 loss: 0.901382  [   96/  306]
train() client id: f_00004-10-3 loss: 0.837914  [  128/  306]
train() client id: f_00004-10-4 loss: 0.780453  [  160/  306]
train() client id: f_00004-10-5 loss: 0.765344  [  192/  306]
train() client id: f_00004-10-6 loss: 0.853221  [  224/  306]
train() client id: f_00004-10-7 loss: 0.922982  [  256/  306]
train() client id: f_00004-10-8 loss: 0.694379  [  288/  306]
train() client id: f_00004-11-0 loss: 0.777054  [   32/  306]
train() client id: f_00004-11-1 loss: 0.790914  [   64/  306]
train() client id: f_00004-11-2 loss: 0.818596  [   96/  306]
train() client id: f_00004-11-3 loss: 0.905555  [  128/  306]
train() client id: f_00004-11-4 loss: 0.725453  [  160/  306]
train() client id: f_00004-11-5 loss: 0.819118  [  192/  306]
train() client id: f_00004-11-6 loss: 0.877696  [  224/  306]
train() client id: f_00004-11-7 loss: 0.874612  [  256/  306]
train() client id: f_00004-11-8 loss: 0.803251  [  288/  306]
train() client id: f_00005-0-0 loss: 0.940941  [   32/  146]
train() client id: f_00005-0-1 loss: 0.759110  [   64/  146]
train() client id: f_00005-0-2 loss: 0.529472  [   96/  146]
train() client id: f_00005-0-3 loss: 0.827996  [  128/  146]
train() client id: f_00005-1-0 loss: 0.646670  [   32/  146]
train() client id: f_00005-1-1 loss: 0.584594  [   64/  146]
train() client id: f_00005-1-2 loss: 0.847030  [   96/  146]
train() client id: f_00005-1-3 loss: 0.871244  [  128/  146]
train() client id: f_00005-2-0 loss: 0.745323  [   32/  146]
train() client id: f_00005-2-1 loss: 0.756441  [   64/  146]
train() client id: f_00005-2-2 loss: 0.831527  [   96/  146]
train() client id: f_00005-2-3 loss: 0.706922  [  128/  146]
train() client id: f_00005-3-0 loss: 0.760656  [   32/  146]
train() client id: f_00005-3-1 loss: 0.653877  [   64/  146]
train() client id: f_00005-3-2 loss: 0.690511  [   96/  146]
train() client id: f_00005-3-3 loss: 0.671312  [  128/  146]
train() client id: f_00005-4-0 loss: 0.548006  [   32/  146]
train() client id: f_00005-4-1 loss: 0.879061  [   64/  146]
train() client id: f_00005-4-2 loss: 0.802050  [   96/  146]
train() client id: f_00005-4-3 loss: 0.659829  [  128/  146]
train() client id: f_00005-5-0 loss: 0.751606  [   32/  146]
train() client id: f_00005-5-1 loss: 0.690353  [   64/  146]
train() client id: f_00005-5-2 loss: 0.768303  [   96/  146]
train() client id: f_00005-5-3 loss: 0.572551  [  128/  146]
train() client id: f_00005-6-0 loss: 0.731533  [   32/  146]
train() client id: f_00005-6-1 loss: 0.693236  [   64/  146]
train() client id: f_00005-6-2 loss: 0.733220  [   96/  146]
train() client id: f_00005-6-3 loss: 0.776266  [  128/  146]
train() client id: f_00005-7-0 loss: 0.623985  [   32/  146]
train() client id: f_00005-7-1 loss: 0.686232  [   64/  146]
train() client id: f_00005-7-2 loss: 0.709139  [   96/  146]
train() client id: f_00005-7-3 loss: 0.904443  [  128/  146]
train() client id: f_00005-8-0 loss: 0.774988  [   32/  146]
train() client id: f_00005-8-1 loss: 0.583469  [   64/  146]
train() client id: f_00005-8-2 loss: 0.721161  [   96/  146]
train() client id: f_00005-8-3 loss: 0.676297  [  128/  146]
train() client id: f_00005-9-0 loss: 0.766567  [   32/  146]
train() client id: f_00005-9-1 loss: 0.776914  [   64/  146]
train() client id: f_00005-9-2 loss: 0.924572  [   96/  146]
train() client id: f_00005-9-3 loss: 0.543887  [  128/  146]
train() client id: f_00005-10-0 loss: 0.769861  [   32/  146]
train() client id: f_00005-10-1 loss: 0.849020  [   64/  146]
train() client id: f_00005-10-2 loss: 0.697886  [   96/  146]
train() client id: f_00005-10-3 loss: 0.602791  [  128/  146]
train() client id: f_00005-11-0 loss: 0.960623  [   32/  146]
train() client id: f_00005-11-1 loss: 0.625015  [   64/  146]
train() client id: f_00005-11-2 loss: 0.654864  [   96/  146]
train() client id: f_00005-11-3 loss: 0.753147  [  128/  146]
train() client id: f_00006-0-0 loss: 0.555370  [   32/   54]
train() client id: f_00006-1-0 loss: 0.510755  [   32/   54]
train() client id: f_00006-2-0 loss: 0.496171  [   32/   54]
train() client id: f_00006-3-0 loss: 0.556554  [   32/   54]
train() client id: f_00006-4-0 loss: 0.576105  [   32/   54]
train() client id: f_00006-5-0 loss: 0.539356  [   32/   54]
train() client id: f_00006-6-0 loss: 0.545478  [   32/   54]
train() client id: f_00006-7-0 loss: 0.567176  [   32/   54]
train() client id: f_00006-8-0 loss: 0.577318  [   32/   54]
train() client id: f_00006-9-0 loss: 0.555105  [   32/   54]
train() client id: f_00006-10-0 loss: 0.606896  [   32/   54]
train() client id: f_00006-11-0 loss: 0.509002  [   32/   54]
train() client id: f_00007-0-0 loss: 0.660174  [   32/  179]
train() client id: f_00007-0-1 loss: 0.515703  [   64/  179]
train() client id: f_00007-0-2 loss: 0.547455  [   96/  179]
train() client id: f_00007-0-3 loss: 0.842519  [  128/  179]
train() client id: f_00007-0-4 loss: 0.645740  [  160/  179]
train() client id: f_00007-1-0 loss: 0.655659  [   32/  179]
train() client id: f_00007-1-1 loss: 0.699952  [   64/  179]
train() client id: f_00007-1-2 loss: 0.580957  [   96/  179]
train() client id: f_00007-1-3 loss: 0.595989  [  128/  179]
train() client id: f_00007-1-4 loss: 0.637433  [  160/  179]
train() client id: f_00007-2-0 loss: 0.707562  [   32/  179]
train() client id: f_00007-2-1 loss: 0.543194  [   64/  179]
train() client id: f_00007-2-2 loss: 0.695219  [   96/  179]
train() client id: f_00007-2-3 loss: 0.589384  [  128/  179]
train() client id: f_00007-2-4 loss: 0.626123  [  160/  179]
train() client id: f_00007-3-0 loss: 0.654870  [   32/  179]
train() client id: f_00007-3-1 loss: 0.679882  [   64/  179]
train() client id: f_00007-3-2 loss: 0.570096  [   96/  179]
train() client id: f_00007-3-3 loss: 0.629626  [  128/  179]
train() client id: f_00007-3-4 loss: 0.643909  [  160/  179]
train() client id: f_00007-4-0 loss: 0.523534  [   32/  179]
train() client id: f_00007-4-1 loss: 0.605436  [   64/  179]
train() client id: f_00007-4-2 loss: 0.630395  [   96/  179]
train() client id: f_00007-4-3 loss: 0.621714  [  128/  179]
train() client id: f_00007-4-4 loss: 0.759060  [  160/  179]
train() client id: f_00007-5-0 loss: 0.638018  [   32/  179]
train() client id: f_00007-5-1 loss: 0.470760  [   64/  179]
train() client id: f_00007-5-2 loss: 0.702033  [   96/  179]
train() client id: f_00007-5-3 loss: 0.778609  [  128/  179]
train() client id: f_00007-5-4 loss: 0.531632  [  160/  179]
train() client id: f_00007-6-0 loss: 0.543681  [   32/  179]
train() client id: f_00007-6-1 loss: 0.578889  [   64/  179]
train() client id: f_00007-6-2 loss: 0.679707  [   96/  179]
train() client id: f_00007-6-3 loss: 0.592396  [  128/  179]
train() client id: f_00007-6-4 loss: 0.686728  [  160/  179]
train() client id: f_00007-7-0 loss: 0.473244  [   32/  179]
train() client id: f_00007-7-1 loss: 0.656021  [   64/  179]
train() client id: f_00007-7-2 loss: 0.510902  [   96/  179]
train() client id: f_00007-7-3 loss: 0.575293  [  128/  179]
train() client id: f_00007-7-4 loss: 0.708623  [  160/  179]
train() client id: f_00007-8-0 loss: 0.642511  [   32/  179]
train() client id: f_00007-8-1 loss: 0.727236  [   64/  179]
train() client id: f_00007-8-2 loss: 0.433804  [   96/  179]
train() client id: f_00007-8-3 loss: 0.535637  [  128/  179]
train() client id: f_00007-8-4 loss: 0.644672  [  160/  179]
train() client id: f_00007-9-0 loss: 0.457256  [   32/  179]
train() client id: f_00007-9-1 loss: 0.623220  [   64/  179]
train() client id: f_00007-9-2 loss: 0.641057  [   96/  179]
train() client id: f_00007-9-3 loss: 0.624038  [  128/  179]
train() client id: f_00007-9-4 loss: 0.628101  [  160/  179]
train() client id: f_00007-10-0 loss: 0.474211  [   32/  179]
train() client id: f_00007-10-1 loss: 0.683805  [   64/  179]
train() client id: f_00007-10-2 loss: 0.637800  [   96/  179]
train() client id: f_00007-10-3 loss: 0.734199  [  128/  179]
train() client id: f_00007-10-4 loss: 0.540565  [  160/  179]
train() client id: f_00007-11-0 loss: 0.459226  [   32/  179]
train() client id: f_00007-11-1 loss: 0.542851  [   64/  179]
train() client id: f_00007-11-2 loss: 0.834230  [   96/  179]
train() client id: f_00007-11-3 loss: 0.569098  [  128/  179]
train() client id: f_00007-11-4 loss: 0.684298  [  160/  179]
train() client id: f_00008-0-0 loss: 0.774006  [   32/  130]
train() client id: f_00008-0-1 loss: 0.718847  [   64/  130]
train() client id: f_00008-0-2 loss: 0.719030  [   96/  130]
train() client id: f_00008-0-3 loss: 0.973127  [  128/  130]
train() client id: f_00008-1-0 loss: 0.714718  [   32/  130]
train() client id: f_00008-1-1 loss: 0.897540  [   64/  130]
train() client id: f_00008-1-2 loss: 0.838893  [   96/  130]
train() client id: f_00008-1-3 loss: 0.716755  [  128/  130]
train() client id: f_00008-2-0 loss: 0.828186  [   32/  130]
train() client id: f_00008-2-1 loss: 0.778993  [   64/  130]
train() client id: f_00008-2-2 loss: 0.752246  [   96/  130]
train() client id: f_00008-2-3 loss: 0.818536  [  128/  130]
train() client id: f_00008-3-0 loss: 0.807236  [   32/  130]
train() client id: f_00008-3-1 loss: 0.767757  [   64/  130]
train() client id: f_00008-3-2 loss: 0.931929  [   96/  130]
train() client id: f_00008-3-3 loss: 0.695499  [  128/  130]
train() client id: f_00008-4-0 loss: 0.636816  [   32/  130]
train() client id: f_00008-4-1 loss: 0.821669  [   64/  130]
train() client id: f_00008-4-2 loss: 0.863464  [   96/  130]
train() client id: f_00008-4-3 loss: 0.871425  [  128/  130]
train() client id: f_00008-5-0 loss: 0.782936  [   32/  130]
train() client id: f_00008-5-1 loss: 0.724174  [   64/  130]
train() client id: f_00008-5-2 loss: 0.834666  [   96/  130]
train() client id: f_00008-5-3 loss: 0.798942  [  128/  130]
train() client id: f_00008-6-0 loss: 0.781093  [   32/  130]
train() client id: f_00008-6-1 loss: 0.815611  [   64/  130]
train() client id: f_00008-6-2 loss: 0.919807  [   96/  130]
train() client id: f_00008-6-3 loss: 0.675621  [  128/  130]
train() client id: f_00008-7-0 loss: 0.858768  [   32/  130]
train() client id: f_00008-7-1 loss: 0.824240  [   64/  130]
train() client id: f_00008-7-2 loss: 0.654074  [   96/  130]
train() client id: f_00008-7-3 loss: 0.844867  [  128/  130]
train() client id: f_00008-8-0 loss: 0.782334  [   32/  130]
train() client id: f_00008-8-1 loss: 0.860840  [   64/  130]
train() client id: f_00008-8-2 loss: 0.794165  [   96/  130]
train() client id: f_00008-8-3 loss: 0.746169  [  128/  130]
train() client id: f_00008-9-0 loss: 0.896111  [   32/  130]
train() client id: f_00008-9-1 loss: 0.695207  [   64/  130]
train() client id: f_00008-9-2 loss: 0.834684  [   96/  130]
train() client id: f_00008-9-3 loss: 0.746847  [  128/  130]
train() client id: f_00008-10-0 loss: 0.813065  [   32/  130]
train() client id: f_00008-10-1 loss: 0.805542  [   64/  130]
train() client id: f_00008-10-2 loss: 0.723752  [   96/  130]
train() client id: f_00008-10-3 loss: 0.822798  [  128/  130]
train() client id: f_00008-11-0 loss: 0.871600  [   32/  130]
train() client id: f_00008-11-1 loss: 0.679173  [   64/  130]
train() client id: f_00008-11-2 loss: 0.775247  [   96/  130]
train() client id: f_00008-11-3 loss: 0.866863  [  128/  130]
train() client id: f_00009-0-0 loss: 1.079035  [   32/  118]
train() client id: f_00009-0-1 loss: 1.147389  [   64/  118]
train() client id: f_00009-0-2 loss: 1.154468  [   96/  118]
train() client id: f_00009-1-0 loss: 1.218123  [   32/  118]
train() client id: f_00009-1-1 loss: 0.995787  [   64/  118]
train() client id: f_00009-1-2 loss: 0.989163  [   96/  118]
train() client id: f_00009-2-0 loss: 1.090078  [   32/  118]
train() client id: f_00009-2-1 loss: 1.121646  [   64/  118]
train() client id: f_00009-2-2 loss: 0.894373  [   96/  118]
train() client id: f_00009-3-0 loss: 0.933754  [   32/  118]
train() client id: f_00009-3-1 loss: 0.983257  [   64/  118]
train() client id: f_00009-3-2 loss: 1.096945  [   96/  118]
train() client id: f_00009-4-0 loss: 0.903025  [   32/  118]
train() client id: f_00009-4-1 loss: 1.112428  [   64/  118]
train() client id: f_00009-4-2 loss: 0.956389  [   96/  118]
train() client id: f_00009-5-0 loss: 0.974203  [   32/  118]
train() client id: f_00009-5-1 loss: 0.964080  [   64/  118]
train() client id: f_00009-5-2 loss: 0.902055  [   96/  118]
train() client id: f_00009-6-0 loss: 1.076042  [   32/  118]
train() client id: f_00009-6-1 loss: 0.812631  [   64/  118]
train() client id: f_00009-6-2 loss: 0.858117  [   96/  118]
train() client id: f_00009-7-0 loss: 0.968626  [   32/  118]
train() client id: f_00009-7-1 loss: 0.913063  [   64/  118]
train() client id: f_00009-7-2 loss: 0.888807  [   96/  118]
train() client id: f_00009-8-0 loss: 0.797747  [   32/  118]
train() client id: f_00009-8-1 loss: 0.929850  [   64/  118]
train() client id: f_00009-8-2 loss: 0.989651  [   96/  118]
train() client id: f_00009-9-0 loss: 0.870511  [   32/  118]
train() client id: f_00009-9-1 loss: 0.949743  [   64/  118]
train() client id: f_00009-9-2 loss: 0.904548  [   96/  118]
train() client id: f_00009-10-0 loss: 1.088003  [   32/  118]
train() client id: f_00009-10-1 loss: 0.806330  [   64/  118]
train() client id: f_00009-10-2 loss: 0.801120  [   96/  118]
train() client id: f_00009-11-0 loss: 0.839434  [   32/  118]
train() client id: f_00009-11-1 loss: 0.780557  [   64/  118]
train() client id: f_00009-11-2 loss: 0.926135  [   96/  118]
At round 15 accuracy: 0.6312997347480106
At round 15 training accuracy: 0.5875251509054326
At round 15 training loss: 0.8301445211228271
update_location
xs = -3.905658 4.200318 95.009024 18.811294 0.979296 3.956410 -57.443192 -36.324852 79.663977 -22.060879 
ys = 87.587959 70.555839 1.320614 -57.455176 49.350187 32.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 8.288573831882571
ys mean: 20.394142535528704
dists_uav = 132.992123 122.457213 137.943679 116.854448 111.518608 105.320566 115.354284 106.396293 129.054326 102.482653 
uav_gains = -103.096545 -102.199894 -103.494024 -101.691278 -101.183759 -100.562863 -101.550966 -100.673201 -102.769906 -100.266282 
uav_gains_db_mean: -101.7488717637712
dists_bs = 192.130525 207.415855 321.044237 302.651989 216.232718 228.565554 213.002906 222.640183 299.396491 229.414185 
bs_gains = -103.507757 -104.438634 -109.750876 -109.033478 -104.944859 -105.619361 -104.761854 -105.299959 -108.901967 -105.664427 
bs_gains_db_mean: -106.19231706546174
Round 16
-------------------------------
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.87981483 18.50415528  8.7446806   3.12925602 21.34431125 10.28544805
  3.88903844 12.52812053  9.21866598  8.34945337]
obj_prev = 104.87294435893298
eta_min = 3.443571724151839e-11	eta_max = 0.9213403601232922
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 24.38417167306138	eta = 0.9090909090909091
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 42.374357946194124	eta = 0.5231330896350052
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 33.76652681787571	eta = 0.6564912320788918
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.22189998010197	eta = 0.6879615667412922
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.14522467977385	eta = 0.6896025463975112
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.14502216089842	eta = 0.6896068910058766
eta = 0.6896068910058766
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [0.03066631 0.06449661 0.03017954 0.01046549 0.07447533 0.03553398
 0.0131427  0.04356563 0.03163984 0.02871924]
ene_total = [2.771699   5.28190218 2.74625142 1.26115748 6.02653747 3.19933593
 1.45388682 3.66054961 3.04133893 2.70236334]
ti_comp = [0.32693877 0.3220322  0.32555044 0.33146051 0.32000855 0.31714645
 0.33188162 0.33440318 0.29980737 0.31694807]
ti_coms = [0.07229963 0.0772062  0.07368796 0.06777789 0.07922985 0.08209195
 0.06735678 0.06483522 0.09943103 0.08229033]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.68628817e-05 1.61693297e-04 1.62099429e-05 6.52070823e-07
 2.52112341e-04 2.78799409e-05 1.28815427e-06 4.62137600e-05
 2.20241139e-05 1.47374657e-05]
ene_total = [0.53002626 0.57650519 0.54013261 0.49576897 0.59791914 0.60245213
 0.49273557 0.47757895 0.72884031 0.60294189]
optimize_network iter = 0 obj = 5.644901027509804
eta = 0.6896068910058766
freqs = [4.68991664e+07 1.00139996e+08 4.63515646e+07 1.57869290e+07
 1.16364588e+08 5.60214049e+07 1.98002903e+07 6.51393823e+07
 5.27669565e+07 4.53059144e+07]
eta_min = 0.6896068910059152	eta_max = 0.6896068910058611
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 0.03760750116688597	eta = 0.909090909090909
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 19.169603857786115	eta = 0.0017834816868453646
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9593134418751286	eta = 0.01744929458133157
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9111928987478155	eta = 0.017888637743914634
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9111822247051171	eta = 0.01788873765279858
eta = 0.01788873765279858
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.82570215e-04 1.75061300e-03 1.75501009e-04 7.05980817e-06
 2.72955744e-03 3.01849167e-04 1.39465251e-05 5.00344854e-04
 2.38449589e-04 1.59558866e-04]
ene_total = [0.17134348 0.21893109 0.17438925 0.15683431 0.24623749 0.19673596
 0.1560201  0.1614347  0.23535039 0.19390545]
ti_comp = [0.32693877 0.3220322  0.32555044 0.33146051 0.32000855 0.31714645
 0.33188162 0.33440318 0.29980737 0.31694807]
ti_coms = [0.07229963 0.0772062  0.07368796 0.06777789 0.07922985 0.08209195
 0.06735678 0.06483522 0.09943103 0.08229033]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.68628817e-05 1.61693297e-04 1.62099429e-05 6.52070823e-07
 2.52112341e-04 2.78799409e-05 1.28815427e-06 4.62137600e-05
 2.20241139e-05 1.47374657e-05]
ene_total = [0.53002626 0.57650519 0.54013261 0.49576897 0.59791914 0.60245213
 0.49273557 0.47757895 0.72884031 0.60294189]
optimize_network iter = 1 obj = 5.644901027510502
eta = 0.6896068910059152
freqs = [4.68991664e+07 1.00139996e+08 4.63515646e+07 1.57869290e+07
 1.16364588e+08 5.60214049e+07 1.98002903e+07 6.51393823e+07
 5.27669565e+07 4.53059144e+07]
Done!
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.66284604e-05 1.59445500e-04 1.59845985e-05 6.43005986e-07
 2.48607572e-04 2.74923648e-05 1.27024685e-06 4.55713142e-05
 2.17179432e-05 1.45325912e-05]
ene_total = [0.00724659 0.00788007 0.00738478 0.00677843 0.00817159 0.00823669
 0.00673695 0.00652909 0.00996482 0.00824357]
At round 16 energy consumption: 0.0771725767342164
At round 16 eta: 0.6896068910059152
At round 16 a_n: 22.701869304745298
At round 16 local rounds: 12.169171106060379
At round 16 global rounds: 73.13908926108901
gradient difference: 0.38527098298072815
train() client id: f_00000-0-0 loss: 1.103620  [   32/  126]
train() client id: f_00000-0-1 loss: 1.129099  [   64/  126]
train() client id: f_00000-0-2 loss: 1.324733  [   96/  126]
train() client id: f_00000-1-0 loss: 1.218318  [   32/  126]
train() client id: f_00000-1-1 loss: 1.093841  [   64/  126]
train() client id: f_00000-1-2 loss: 0.911313  [   96/  126]
train() client id: f_00000-2-0 loss: 1.071550  [   32/  126]
train() client id: f_00000-2-1 loss: 0.949024  [   64/  126]
train() client id: f_00000-2-2 loss: 1.034639  [   96/  126]
train() client id: f_00000-3-0 loss: 0.927703  [   32/  126]
train() client id: f_00000-3-1 loss: 0.938723  [   64/  126]
train() client id: f_00000-3-2 loss: 0.867771  [   96/  126]
train() client id: f_00000-4-0 loss: 0.846391  [   32/  126]
train() client id: f_00000-4-1 loss: 0.912979  [   64/  126]
train() client id: f_00000-4-2 loss: 0.871435  [   96/  126]
train() client id: f_00000-5-0 loss: 0.985336  [   32/  126]
train() client id: f_00000-5-1 loss: 0.898259  [   64/  126]
train() client id: f_00000-5-2 loss: 0.899410  [   96/  126]
train() client id: f_00000-6-0 loss: 0.874528  [   32/  126]
train() client id: f_00000-6-1 loss: 1.001175  [   64/  126]
train() client id: f_00000-6-2 loss: 0.859675  [   96/  126]
train() client id: f_00000-7-0 loss: 0.913480  [   32/  126]
train() client id: f_00000-7-1 loss: 0.875019  [   64/  126]
train() client id: f_00000-7-2 loss: 0.821716  [   96/  126]
train() client id: f_00000-8-0 loss: 0.849603  [   32/  126]
train() client id: f_00000-8-1 loss: 0.827771  [   64/  126]
train() client id: f_00000-8-2 loss: 0.839657  [   96/  126]
train() client id: f_00000-9-0 loss: 0.903689  [   32/  126]
train() client id: f_00000-9-1 loss: 0.915939  [   64/  126]
train() client id: f_00000-9-2 loss: 0.844928  [   96/  126]
train() client id: f_00000-10-0 loss: 0.789516  [   32/  126]
train() client id: f_00000-10-1 loss: 0.857512  [   64/  126]
train() client id: f_00000-10-2 loss: 0.840384  [   96/  126]
train() client id: f_00000-11-0 loss: 0.794109  [   32/  126]
train() client id: f_00000-11-1 loss: 0.970538  [   64/  126]
train() client id: f_00000-11-2 loss: 0.913477  [   96/  126]
train() client id: f_00001-0-0 loss: 0.350802  [   32/  265]
train() client id: f_00001-0-1 loss: 0.403535  [   64/  265]
train() client id: f_00001-0-2 loss: 0.392691  [   96/  265]
train() client id: f_00001-0-3 loss: 0.302137  [  128/  265]
train() client id: f_00001-0-4 loss: 0.397125  [  160/  265]
train() client id: f_00001-0-5 loss: 0.416713  [  192/  265]
train() client id: f_00001-0-6 loss: 0.498400  [  224/  265]
train() client id: f_00001-0-7 loss: 0.293425  [  256/  265]
train() client id: f_00001-1-0 loss: 0.285637  [   32/  265]
train() client id: f_00001-1-1 loss: 0.345281  [   64/  265]
train() client id: f_00001-1-2 loss: 0.452115  [   96/  265]
train() client id: f_00001-1-3 loss: 0.400456  [  128/  265]
train() client id: f_00001-1-4 loss: 0.411080  [  160/  265]
train() client id: f_00001-1-5 loss: 0.328176  [  192/  265]
train() client id: f_00001-1-6 loss: 0.373073  [  224/  265]
train() client id: f_00001-1-7 loss: 0.411176  [  256/  265]
train() client id: f_00001-2-0 loss: 0.294365  [   32/  265]
train() client id: f_00001-2-1 loss: 0.358078  [   64/  265]
train() client id: f_00001-2-2 loss: 0.407513  [   96/  265]
train() client id: f_00001-2-3 loss: 0.405995  [  128/  265]
train() client id: f_00001-2-4 loss: 0.344262  [  160/  265]
train() client id: f_00001-2-5 loss: 0.297851  [  192/  265]
train() client id: f_00001-2-6 loss: 0.372928  [  224/  265]
train() client id: f_00001-2-7 loss: 0.417303  [  256/  265]
train() client id: f_00001-3-0 loss: 0.307338  [   32/  265]
train() client id: f_00001-3-1 loss: 0.435028  [   64/  265]
train() client id: f_00001-3-2 loss: 0.388413  [   96/  265]
train() client id: f_00001-3-3 loss: 0.278970  [  128/  265]
train() client id: f_00001-3-4 loss: 0.384265  [  160/  265]
train() client id: f_00001-3-5 loss: 0.383732  [  192/  265]
train() client id: f_00001-3-6 loss: 0.287214  [  224/  265]
train() client id: f_00001-3-7 loss: 0.411807  [  256/  265]
train() client id: f_00001-4-0 loss: 0.428570  [   32/  265]
train() client id: f_00001-4-1 loss: 0.336534  [   64/  265]
train() client id: f_00001-4-2 loss: 0.308677  [   96/  265]
train() client id: f_00001-4-3 loss: 0.275483  [  128/  265]
train() client id: f_00001-4-4 loss: 0.352998  [  160/  265]
train() client id: f_00001-4-5 loss: 0.284638  [  192/  265]
train() client id: f_00001-4-6 loss: 0.350814  [  224/  265]
train() client id: f_00001-4-7 loss: 0.494787  [  256/  265]
train() client id: f_00001-5-0 loss: 0.406592  [   32/  265]
train() client id: f_00001-5-1 loss: 0.366243  [   64/  265]
train() client id: f_00001-5-2 loss: 0.262106  [   96/  265]
train() client id: f_00001-5-3 loss: 0.399757  [  128/  265]
train() client id: f_00001-5-4 loss: 0.315373  [  160/  265]
train() client id: f_00001-5-5 loss: 0.389150  [  192/  265]
train() client id: f_00001-5-6 loss: 0.255587  [  224/  265]
train() client id: f_00001-5-7 loss: 0.399135  [  256/  265]
train() client id: f_00001-6-0 loss: 0.313921  [   32/  265]
train() client id: f_00001-6-1 loss: 0.334211  [   64/  265]
train() client id: f_00001-6-2 loss: 0.284274  [   96/  265]
train() client id: f_00001-6-3 loss: 0.402309  [  128/  265]
train() client id: f_00001-6-4 loss: 0.348500  [  160/  265]
train() client id: f_00001-6-5 loss: 0.248038  [  192/  265]
train() client id: f_00001-6-6 loss: 0.326791  [  224/  265]
train() client id: f_00001-6-7 loss: 0.506962  [  256/  265]
train() client id: f_00001-7-0 loss: 0.370055  [   32/  265]
train() client id: f_00001-7-1 loss: 0.264872  [   64/  265]
train() client id: f_00001-7-2 loss: 0.253259  [   96/  265]
train() client id: f_00001-7-3 loss: 0.465820  [  128/  265]
train() client id: f_00001-7-4 loss: 0.440112  [  160/  265]
train() client id: f_00001-7-5 loss: 0.228917  [  192/  265]
train() client id: f_00001-7-6 loss: 0.259447  [  224/  265]
train() client id: f_00001-7-7 loss: 0.375866  [  256/  265]
train() client id: f_00001-8-0 loss: 0.418839  [   32/  265]
train() client id: f_00001-8-1 loss: 0.333152  [   64/  265]
train() client id: f_00001-8-2 loss: 0.358734  [   96/  265]
train() client id: f_00001-8-3 loss: 0.313101  [  128/  265]
train() client id: f_00001-8-4 loss: 0.384245  [  160/  265]
train() client id: f_00001-8-5 loss: 0.346288  [  192/  265]
train() client id: f_00001-8-6 loss: 0.221810  [  224/  265]
train() client id: f_00001-8-7 loss: 0.251418  [  256/  265]
train() client id: f_00001-9-0 loss: 0.337383  [   32/  265]
train() client id: f_00001-9-1 loss: 0.358602  [   64/  265]
train() client id: f_00001-9-2 loss: 0.293989  [   96/  265]
train() client id: f_00001-9-3 loss: 0.281263  [  128/  265]
train() client id: f_00001-9-4 loss: 0.336750  [  160/  265]
train() client id: f_00001-9-5 loss: 0.550550  [  192/  265]
train() client id: f_00001-9-6 loss: 0.230847  [  224/  265]
train() client id: f_00001-9-7 loss: 0.297096  [  256/  265]
train() client id: f_00001-10-0 loss: 0.307322  [   32/  265]
train() client id: f_00001-10-1 loss: 0.453982  [   64/  265]
train() client id: f_00001-10-2 loss: 0.284924  [   96/  265]
train() client id: f_00001-10-3 loss: 0.411709  [  128/  265]
train() client id: f_00001-10-4 loss: 0.302644  [  160/  265]
train() client id: f_00001-10-5 loss: 0.283184  [  192/  265]
train() client id: f_00001-10-6 loss: 0.298374  [  224/  265]
train() client id: f_00001-10-7 loss: 0.301098  [  256/  265]
train() client id: f_00001-11-0 loss: 0.285276  [   32/  265]
train() client id: f_00001-11-1 loss: 0.325878  [   64/  265]
train() client id: f_00001-11-2 loss: 0.242006  [   96/  265]
train() client id: f_00001-11-3 loss: 0.469923  [  128/  265]
train() client id: f_00001-11-4 loss: 0.393607  [  160/  265]
train() client id: f_00001-11-5 loss: 0.297279  [  192/  265]
train() client id: f_00001-11-6 loss: 0.263188  [  224/  265]
train() client id: f_00001-11-7 loss: 0.325765  [  256/  265]
train() client id: f_00002-0-0 loss: 1.033607  [   32/  124]
train() client id: f_00002-0-1 loss: 1.276537  [   64/  124]
train() client id: f_00002-0-2 loss: 0.970377  [   96/  124]
train() client id: f_00002-1-0 loss: 1.071432  [   32/  124]
train() client id: f_00002-1-1 loss: 1.100158  [   64/  124]
train() client id: f_00002-1-2 loss: 1.237888  [   96/  124]
train() client id: f_00002-2-0 loss: 1.095227  [   32/  124]
train() client id: f_00002-2-1 loss: 1.062932  [   64/  124]
train() client id: f_00002-2-2 loss: 0.963571  [   96/  124]
train() client id: f_00002-3-0 loss: 1.097542  [   32/  124]
train() client id: f_00002-3-1 loss: 0.996078  [   64/  124]
train() client id: f_00002-3-2 loss: 1.018172  [   96/  124]
train() client id: f_00002-4-0 loss: 0.827261  [   32/  124]
train() client id: f_00002-4-1 loss: 1.152511  [   64/  124]
train() client id: f_00002-4-2 loss: 1.012898  [   96/  124]
train() client id: f_00002-5-0 loss: 1.168463  [   32/  124]
train() client id: f_00002-5-1 loss: 0.906988  [   64/  124]
train() client id: f_00002-5-2 loss: 0.980780  [   96/  124]
train() client id: f_00002-6-0 loss: 1.062514  [   32/  124]
train() client id: f_00002-6-1 loss: 0.905286  [   64/  124]
train() client id: f_00002-6-2 loss: 0.998582  [   96/  124]
train() client id: f_00002-7-0 loss: 1.023381  [   32/  124]
train() client id: f_00002-7-1 loss: 0.825844  [   64/  124]
train() client id: f_00002-7-2 loss: 1.004371  [   96/  124]
train() client id: f_00002-8-0 loss: 1.023270  [   32/  124]
train() client id: f_00002-8-1 loss: 0.858746  [   64/  124]
train() client id: f_00002-8-2 loss: 1.041088  [   96/  124]
train() client id: f_00002-9-0 loss: 0.953256  [   32/  124]
train() client id: f_00002-9-1 loss: 0.978649  [   64/  124]
train() client id: f_00002-9-2 loss: 0.839518  [   96/  124]
train() client id: f_00002-10-0 loss: 1.137003  [   32/  124]
train() client id: f_00002-10-1 loss: 0.845967  [   64/  124]
train() client id: f_00002-10-2 loss: 0.823892  [   96/  124]
train() client id: f_00002-11-0 loss: 0.837194  [   32/  124]
train() client id: f_00002-11-1 loss: 0.880532  [   64/  124]
train() client id: f_00002-11-2 loss: 0.937611  [   96/  124]
train() client id: f_00003-0-0 loss: 0.733109  [   32/   43]
train() client id: f_00003-1-0 loss: 0.935982  [   32/   43]
train() client id: f_00003-2-0 loss: 0.896526  [   32/   43]
train() client id: f_00003-3-0 loss: 0.832374  [   32/   43]
train() client id: f_00003-4-0 loss: 1.028446  [   32/   43]
train() client id: f_00003-5-0 loss: 0.792917  [   32/   43]
train() client id: f_00003-6-0 loss: 0.733379  [   32/   43]
train() client id: f_00003-7-0 loss: 0.905642  [   32/   43]
train() client id: f_00003-8-0 loss: 0.921204  [   32/   43]
train() client id: f_00003-9-0 loss: 0.930060  [   32/   43]
train() client id: f_00003-10-0 loss: 0.766910  [   32/   43]
train() client id: f_00003-11-0 loss: 0.885298  [   32/   43]
train() client id: f_00004-0-0 loss: 1.051340  [   32/  306]
train() client id: f_00004-0-1 loss: 1.025978  [   64/  306]
train() client id: f_00004-0-2 loss: 0.844512  [   96/  306]
train() client id: f_00004-0-3 loss: 0.885226  [  128/  306]
train() client id: f_00004-0-4 loss: 0.760191  [  160/  306]
train() client id: f_00004-0-5 loss: 0.975858  [  192/  306]
train() client id: f_00004-0-6 loss: 0.988808  [  224/  306]
train() client id: f_00004-0-7 loss: 0.860169  [  256/  306]
train() client id: f_00004-0-8 loss: 0.805205  [  288/  306]
train() client id: f_00004-1-0 loss: 0.964797  [   32/  306]
train() client id: f_00004-1-1 loss: 0.856462  [   64/  306]
train() client id: f_00004-1-2 loss: 0.890304  [   96/  306]
train() client id: f_00004-1-3 loss: 0.813035  [  128/  306]
train() client id: f_00004-1-4 loss: 0.988254  [  160/  306]
train() client id: f_00004-1-5 loss: 0.866181  [  192/  306]
train() client id: f_00004-1-6 loss: 0.800362  [  224/  306]
train() client id: f_00004-1-7 loss: 0.976909  [  256/  306]
train() client id: f_00004-1-8 loss: 1.004940  [  288/  306]
train() client id: f_00004-2-0 loss: 0.892037  [   32/  306]
train() client id: f_00004-2-1 loss: 0.761022  [   64/  306]
train() client id: f_00004-2-2 loss: 0.780204  [   96/  306]
train() client id: f_00004-2-3 loss: 0.933097  [  128/  306]
train() client id: f_00004-2-4 loss: 1.020979  [  160/  306]
train() client id: f_00004-2-5 loss: 0.983398  [  192/  306]
train() client id: f_00004-2-6 loss: 0.885444  [  224/  306]
train() client id: f_00004-2-7 loss: 0.890182  [  256/  306]
train() client id: f_00004-2-8 loss: 0.951630  [  288/  306]
train() client id: f_00004-3-0 loss: 0.908783  [   32/  306]
train() client id: f_00004-3-1 loss: 0.886700  [   64/  306]
train() client id: f_00004-3-2 loss: 0.882341  [   96/  306]
train() client id: f_00004-3-3 loss: 0.937039  [  128/  306]
train() client id: f_00004-3-4 loss: 0.818000  [  160/  306]
train() client id: f_00004-3-5 loss: 0.939184  [  192/  306]
train() client id: f_00004-3-6 loss: 0.934375  [  224/  306]
train() client id: f_00004-3-7 loss: 0.852221  [  256/  306]
train() client id: f_00004-3-8 loss: 0.883203  [  288/  306]
train() client id: f_00004-4-0 loss: 0.927983  [   32/  306]
train() client id: f_00004-4-1 loss: 0.863305  [   64/  306]
train() client id: f_00004-4-2 loss: 0.873036  [   96/  306]
train() client id: f_00004-4-3 loss: 0.826264  [  128/  306]
train() client id: f_00004-4-4 loss: 0.964767  [  160/  306]
train() client id: f_00004-4-5 loss: 0.909673  [  192/  306]
train() client id: f_00004-4-6 loss: 0.894193  [  224/  306]
train() client id: f_00004-4-7 loss: 0.856901  [  256/  306]
train() client id: f_00004-4-8 loss: 0.931100  [  288/  306]
train() client id: f_00004-5-0 loss: 0.852119  [   32/  306]
train() client id: f_00004-5-1 loss: 0.839563  [   64/  306]
train() client id: f_00004-5-2 loss: 0.959443  [   96/  306]
train() client id: f_00004-5-3 loss: 0.925479  [  128/  306]
train() client id: f_00004-5-4 loss: 1.000071  [  160/  306]
train() client id: f_00004-5-5 loss: 0.864968  [  192/  306]
train() client id: f_00004-5-6 loss: 0.925294  [  224/  306]
train() client id: f_00004-5-7 loss: 0.899268  [  256/  306]
train() client id: f_00004-5-8 loss: 0.858669  [  288/  306]
train() client id: f_00004-6-0 loss: 0.913307  [   32/  306]
train() client id: f_00004-6-1 loss: 0.853454  [   64/  306]
train() client id: f_00004-6-2 loss: 0.953789  [   96/  306]
train() client id: f_00004-6-3 loss: 0.927750  [  128/  306]
train() client id: f_00004-6-4 loss: 0.882556  [  160/  306]
train() client id: f_00004-6-5 loss: 0.865817  [  192/  306]
train() client id: f_00004-6-6 loss: 0.936595  [  224/  306]
train() client id: f_00004-6-7 loss: 0.817162  [  256/  306]
train() client id: f_00004-6-8 loss: 0.837568  [  288/  306]
train() client id: f_00004-7-0 loss: 0.855780  [   32/  306]
train() client id: f_00004-7-1 loss: 0.803744  [   64/  306]
train() client id: f_00004-7-2 loss: 0.930547  [   96/  306]
train() client id: f_00004-7-3 loss: 0.859343  [  128/  306]
train() client id: f_00004-7-4 loss: 0.897083  [  160/  306]
train() client id: f_00004-7-5 loss: 0.852832  [  192/  306]
train() client id: f_00004-7-6 loss: 0.873381  [  224/  306]
train() client id: f_00004-7-7 loss: 1.021662  [  256/  306]
train() client id: f_00004-7-8 loss: 0.851565  [  288/  306]
train() client id: f_00004-8-0 loss: 0.893711  [   32/  306]
train() client id: f_00004-8-1 loss: 0.936096  [   64/  306]
train() client id: f_00004-8-2 loss: 0.903100  [   96/  306]
train() client id: f_00004-8-3 loss: 0.852304  [  128/  306]
train() client id: f_00004-8-4 loss: 0.858143  [  160/  306]
train() client id: f_00004-8-5 loss: 0.896294  [  192/  306]
train() client id: f_00004-8-6 loss: 0.856783  [  224/  306]
train() client id: f_00004-8-7 loss: 0.970019  [  256/  306]
train() client id: f_00004-8-8 loss: 0.775184  [  288/  306]
train() client id: f_00004-9-0 loss: 0.939161  [   32/  306]
train() client id: f_00004-9-1 loss: 0.772100  [   64/  306]
train() client id: f_00004-9-2 loss: 0.772261  [   96/  306]
train() client id: f_00004-9-3 loss: 0.974394  [  128/  306]
train() client id: f_00004-9-4 loss: 0.922061  [  160/  306]
train() client id: f_00004-9-5 loss: 0.838531  [  192/  306]
train() client id: f_00004-9-6 loss: 0.881559  [  224/  306]
train() client id: f_00004-9-7 loss: 0.862641  [  256/  306]
train() client id: f_00004-9-8 loss: 0.955145  [  288/  306]
train() client id: f_00004-10-0 loss: 0.788333  [   32/  306]
train() client id: f_00004-10-1 loss: 0.875532  [   64/  306]
train() client id: f_00004-10-2 loss: 0.864214  [   96/  306]
train() client id: f_00004-10-3 loss: 0.925139  [  128/  306]
train() client id: f_00004-10-4 loss: 0.802696  [  160/  306]
train() client id: f_00004-10-5 loss: 0.827433  [  192/  306]
train() client id: f_00004-10-6 loss: 0.906079  [  224/  306]
train() client id: f_00004-10-7 loss: 0.989109  [  256/  306]
train() client id: f_00004-10-8 loss: 0.887993  [  288/  306]
train() client id: f_00004-11-0 loss: 0.842666  [   32/  306]
train() client id: f_00004-11-1 loss: 0.822528  [   64/  306]
train() client id: f_00004-11-2 loss: 0.885989  [   96/  306]
train() client id: f_00004-11-3 loss: 0.760757  [  128/  306]
train() client id: f_00004-11-4 loss: 0.918458  [  160/  306]
train() client id: f_00004-11-5 loss: 0.952332  [  192/  306]
train() client id: f_00004-11-6 loss: 0.877573  [  224/  306]
train() client id: f_00004-11-7 loss: 0.860595  [  256/  306]
train() client id: f_00004-11-8 loss: 0.895213  [  288/  306]
train() client id: f_00005-0-0 loss: 0.707551  [   32/  146]
train() client id: f_00005-0-1 loss: 0.658832  [   64/  146]
train() client id: f_00005-0-2 loss: 0.744684  [   96/  146]
train() client id: f_00005-0-3 loss: 0.530373  [  128/  146]
train() client id: f_00005-1-0 loss: 0.578474  [   32/  146]
train() client id: f_00005-1-1 loss: 0.671131  [   64/  146]
train() client id: f_00005-1-2 loss: 0.781648  [   96/  146]
train() client id: f_00005-1-3 loss: 0.670517  [  128/  146]
train() client id: f_00005-2-0 loss: 0.697231  [   32/  146]
train() client id: f_00005-2-1 loss: 0.712012  [   64/  146]
train() client id: f_00005-2-2 loss: 0.648358  [   96/  146]
train() client id: f_00005-2-3 loss: 0.341766  [  128/  146]
train() client id: f_00005-3-0 loss: 0.825319  [   32/  146]
train() client id: f_00005-3-1 loss: 0.532288  [   64/  146]
train() client id: f_00005-3-2 loss: 0.926884  [   96/  146]
train() client id: f_00005-3-3 loss: 0.395039  [  128/  146]
train() client id: f_00005-4-0 loss: 0.756826  [   32/  146]
train() client id: f_00005-4-1 loss: 0.543019  [   64/  146]
train() client id: f_00005-4-2 loss: 0.859262  [   96/  146]
train() client id: f_00005-4-3 loss: 0.594294  [  128/  146]
train() client id: f_00005-5-0 loss: 0.518275  [   32/  146]
train() client id: f_00005-5-1 loss: 0.880576  [   64/  146]
train() client id: f_00005-5-2 loss: 0.501168  [   96/  146]
train() client id: f_00005-5-3 loss: 0.684246  [  128/  146]
train() client id: f_00005-6-0 loss: 1.023729  [   32/  146]
train() client id: f_00005-6-1 loss: 0.658943  [   64/  146]
train() client id: f_00005-6-2 loss: 0.512829  [   96/  146]
train() client id: f_00005-6-3 loss: 0.527919  [  128/  146]
train() client id: f_00005-7-0 loss: 0.489161  [   32/  146]
train() client id: f_00005-7-1 loss: 0.574130  [   64/  146]
train() client id: f_00005-7-2 loss: 0.949286  [   96/  146]
train() client id: f_00005-7-3 loss: 0.550363  [  128/  146]
train() client id: f_00005-8-0 loss: 0.667215  [   32/  146]
train() client id: f_00005-8-1 loss: 0.944066  [   64/  146]
train() client id: f_00005-8-2 loss: 0.498164  [   96/  146]
train() client id: f_00005-8-3 loss: 0.442995  [  128/  146]
train() client id: f_00005-9-0 loss: 0.508037  [   32/  146]
train() client id: f_00005-9-1 loss: 0.499962  [   64/  146]
train() client id: f_00005-9-2 loss: 0.755682  [   96/  146]
train() client id: f_00005-9-3 loss: 0.866488  [  128/  146]
train() client id: f_00005-10-0 loss: 0.940859  [   32/  146]
train() client id: f_00005-10-1 loss: 0.602976  [   64/  146]
train() client id: f_00005-10-2 loss: 0.736694  [   96/  146]
train() client id: f_00005-10-3 loss: 0.489976  [  128/  146]
train() client id: f_00005-11-0 loss: 0.804736  [   32/  146]
train() client id: f_00005-11-1 loss: 0.616564  [   64/  146]
train() client id: f_00005-11-2 loss: 0.921922  [   96/  146]
train() client id: f_00005-11-3 loss: 0.288779  [  128/  146]
train() client id: f_00006-0-0 loss: 0.574257  [   32/   54]
train() client id: f_00006-1-0 loss: 0.586407  [   32/   54]
train() client id: f_00006-2-0 loss: 0.603333  [   32/   54]
train() client id: f_00006-3-0 loss: 0.573146  [   32/   54]
train() client id: f_00006-4-0 loss: 0.610275  [   32/   54]
train() client id: f_00006-5-0 loss: 0.579614  [   32/   54]
train() client id: f_00006-6-0 loss: 0.618949  [   32/   54]
train() client id: f_00006-7-0 loss: 0.543389  [   32/   54]
train() client id: f_00006-8-0 loss: 0.577296  [   32/   54]
train() client id: f_00006-9-0 loss: 0.532851  [   32/   54]
train() client id: f_00006-10-0 loss: 0.534759  [   32/   54]
train() client id: f_00006-11-0 loss: 0.547380  [   32/   54]
train() client id: f_00007-0-0 loss: 0.396593  [   32/  179]
train() client id: f_00007-0-1 loss: 0.533330  [   64/  179]
train() client id: f_00007-0-2 loss: 0.618986  [   96/  179]
train() client id: f_00007-0-3 loss: 0.415647  [  128/  179]
train() client id: f_00007-0-4 loss: 0.652479  [  160/  179]
train() client id: f_00007-1-0 loss: 0.570976  [   32/  179]
train() client id: f_00007-1-1 loss: 0.454681  [   64/  179]
train() client id: f_00007-1-2 loss: 0.740261  [   96/  179]
train() client id: f_00007-1-3 loss: 0.445972  [  128/  179]
train() client id: f_00007-1-4 loss: 0.408098  [  160/  179]
train() client id: f_00007-2-0 loss: 0.538380  [   32/  179]
train() client id: f_00007-2-1 loss: 0.434308  [   64/  179]
train() client id: f_00007-2-2 loss: 0.542090  [   96/  179]
train() client id: f_00007-2-3 loss: 0.460669  [  128/  179]
train() client id: f_00007-2-4 loss: 0.595078  [  160/  179]
train() client id: f_00007-3-0 loss: 0.477431  [   32/  179]
train() client id: f_00007-3-1 loss: 0.571406  [   64/  179]
train() client id: f_00007-3-2 loss: 0.501585  [   96/  179]
train() client id: f_00007-3-3 loss: 0.382088  [  128/  179]
train() client id: f_00007-3-4 loss: 0.584388  [  160/  179]
train() client id: f_00007-4-0 loss: 0.793489  [   32/  179]
train() client id: f_00007-4-1 loss: 0.334522  [   64/  179]
train() client id: f_00007-4-2 loss: 0.471117  [   96/  179]
train() client id: f_00007-4-3 loss: 0.415932  [  128/  179]
train() client id: f_00007-4-4 loss: 0.361500  [  160/  179]
train() client id: f_00007-5-0 loss: 0.614425  [   32/  179]
train() client id: f_00007-5-1 loss: 0.410652  [   64/  179]
train() client id: f_00007-5-2 loss: 0.462173  [   96/  179]
train() client id: f_00007-5-3 loss: 0.394146  [  128/  179]
train() client id: f_00007-5-4 loss: 0.578104  [  160/  179]
train() client id: f_00007-6-0 loss: 0.341191  [   32/  179]
train() client id: f_00007-6-1 loss: 0.491926  [   64/  179]
train() client id: f_00007-6-2 loss: 0.347083  [   96/  179]
train() client id: f_00007-6-3 loss: 0.628814  [  128/  179]
train() client id: f_00007-6-4 loss: 0.645212  [  160/  179]
train() client id: f_00007-7-0 loss: 0.456760  [   32/  179]
train() client id: f_00007-7-1 loss: 0.531327  [   64/  179]
train() client id: f_00007-7-2 loss: 0.441889  [   96/  179]
train() client id: f_00007-7-3 loss: 0.482010  [  128/  179]
train() client id: f_00007-7-4 loss: 0.438774  [  160/  179]
train() client id: f_00007-8-0 loss: 0.443995  [   32/  179]
train() client id: f_00007-8-1 loss: 0.392723  [   64/  179]
train() client id: f_00007-8-2 loss: 0.398318  [   96/  179]
train() client id: f_00007-8-3 loss: 0.504370  [  128/  179]
train() client id: f_00007-8-4 loss: 0.691202  [  160/  179]
train() client id: f_00007-9-0 loss: 0.575006  [   32/  179]
train() client id: f_00007-9-1 loss: 0.308222  [   64/  179]
train() client id: f_00007-9-2 loss: 0.310211  [   96/  179]
train() client id: f_00007-9-3 loss: 0.363973  [  128/  179]
train() client id: f_00007-9-4 loss: 0.774040  [  160/  179]
train() client id: f_00007-10-0 loss: 0.459495  [   32/  179]
train() client id: f_00007-10-1 loss: 0.330274  [   64/  179]
train() client id: f_00007-10-2 loss: 0.546727  [   96/  179]
train() client id: f_00007-10-3 loss: 0.408632  [  128/  179]
train() client id: f_00007-10-4 loss: 0.517528  [  160/  179]
train() client id: f_00007-11-0 loss: 0.422871  [   32/  179]
train() client id: f_00007-11-1 loss: 0.444036  [   64/  179]
train() client id: f_00007-11-2 loss: 0.395761  [   96/  179]
train() client id: f_00007-11-3 loss: 0.554910  [  128/  179]
train() client id: f_00007-11-4 loss: 0.478101  [  160/  179]
train() client id: f_00008-0-0 loss: 0.656033  [   32/  130]
train() client id: f_00008-0-1 loss: 0.688733  [   64/  130]
train() client id: f_00008-0-2 loss: 0.768828  [   96/  130]
train() client id: f_00008-0-3 loss: 0.755659  [  128/  130]
train() client id: f_00008-1-0 loss: 0.760019  [   32/  130]
train() client id: f_00008-1-1 loss: 0.704207  [   64/  130]
train() client id: f_00008-1-2 loss: 0.685673  [   96/  130]
train() client id: f_00008-1-3 loss: 0.719211  [  128/  130]
train() client id: f_00008-2-0 loss: 0.727333  [   32/  130]
train() client id: f_00008-2-1 loss: 0.666603  [   64/  130]
train() client id: f_00008-2-2 loss: 0.751854  [   96/  130]
train() client id: f_00008-2-3 loss: 0.725786  [  128/  130]
train() client id: f_00008-3-0 loss: 0.775580  [   32/  130]
train() client id: f_00008-3-1 loss: 0.737923  [   64/  130]
train() client id: f_00008-3-2 loss: 0.725305  [   96/  130]
train() client id: f_00008-3-3 loss: 0.631470  [  128/  130]
train() client id: f_00008-4-0 loss: 0.623976  [   32/  130]
train() client id: f_00008-4-1 loss: 0.666385  [   64/  130]
train() client id: f_00008-4-2 loss: 0.741390  [   96/  130]
train() client id: f_00008-4-3 loss: 0.819523  [  128/  130]
train() client id: f_00008-5-0 loss: 0.708318  [   32/  130]
train() client id: f_00008-5-1 loss: 0.703941  [   64/  130]
train() client id: f_00008-5-2 loss: 0.669769  [   96/  130]
train() client id: f_00008-5-3 loss: 0.758057  [  128/  130]
train() client id: f_00008-6-0 loss: 0.594561  [   32/  130]
train() client id: f_00008-6-1 loss: 0.547054  [   64/  130]
train() client id: f_00008-6-2 loss: 0.735532  [   96/  130]
train() client id: f_00008-6-3 loss: 0.940776  [  128/  130]
train() client id: f_00008-7-0 loss: 0.740346  [   32/  130]
train() client id: f_00008-7-1 loss: 0.659632  [   64/  130]
train() client id: f_00008-7-2 loss: 0.744845  [   96/  130]
train() client id: f_00008-7-3 loss: 0.694079  [  128/  130]
train() client id: f_00008-8-0 loss: 0.660962  [   32/  130]
train() client id: f_00008-8-1 loss: 0.686642  [   64/  130]
train() client id: f_00008-8-2 loss: 0.761899  [   96/  130]
train() client id: f_00008-8-3 loss: 0.724380  [  128/  130]
train() client id: f_00008-9-0 loss: 0.752105  [   32/  130]
train() client id: f_00008-9-1 loss: 0.668890  [   64/  130]
train() client id: f_00008-9-2 loss: 0.718975  [   96/  130]
train() client id: f_00008-9-3 loss: 0.689044  [  128/  130]
train() client id: f_00008-10-0 loss: 0.712501  [   32/  130]
train() client id: f_00008-10-1 loss: 0.681827  [   64/  130]
train() client id: f_00008-10-2 loss: 0.735293  [   96/  130]
train() client id: f_00008-10-3 loss: 0.706614  [  128/  130]
train() client id: f_00008-11-0 loss: 0.563989  [   32/  130]
train() client id: f_00008-11-1 loss: 0.745921  [   64/  130]
train() client id: f_00008-11-2 loss: 0.687035  [   96/  130]
train() client id: f_00008-11-3 loss: 0.818182  [  128/  130]
train() client id: f_00009-0-0 loss: 1.076505  [   32/  118]
train() client id: f_00009-0-1 loss: 1.128795  [   64/  118]
train() client id: f_00009-0-2 loss: 0.966642  [   96/  118]
train() client id: f_00009-1-0 loss: 0.963769  [   32/  118]
train() client id: f_00009-1-1 loss: 1.047680  [   64/  118]
train() client id: f_00009-1-2 loss: 1.021443  [   96/  118]
train() client id: f_00009-2-0 loss: 0.817990  [   32/  118]
train() client id: f_00009-2-1 loss: 0.884841  [   64/  118]
train() client id: f_00009-2-2 loss: 1.114105  [   96/  118]
train() client id: f_00009-3-0 loss: 0.832222  [   32/  118]
train() client id: f_00009-3-1 loss: 1.066650  [   64/  118]
train() client id: f_00009-3-2 loss: 0.914312  [   96/  118]
train() client id: f_00009-4-0 loss: 0.824886  [   32/  118]
train() client id: f_00009-4-1 loss: 0.837652  [   64/  118]
train() client id: f_00009-4-2 loss: 0.984214  [   96/  118]
train() client id: f_00009-5-0 loss: 0.979489  [   32/  118]
train() client id: f_00009-5-1 loss: 0.664560  [   64/  118]
train() client id: f_00009-5-2 loss: 0.948224  [   96/  118]
train() client id: f_00009-6-0 loss: 0.905936  [   32/  118]
train() client id: f_00009-6-1 loss: 1.069452  [   64/  118]
train() client id: f_00009-6-2 loss: 0.667373  [   96/  118]
train() client id: f_00009-7-0 loss: 0.933069  [   32/  118]
train() client id: f_00009-7-1 loss: 0.776721  [   64/  118]
train() client id: f_00009-7-2 loss: 0.827458  [   96/  118]
train() client id: f_00009-8-0 loss: 0.891664  [   32/  118]
train() client id: f_00009-8-1 loss: 0.708545  [   64/  118]
train() client id: f_00009-8-2 loss: 0.753812  [   96/  118]
train() client id: f_00009-9-0 loss: 0.771993  [   32/  118]
train() client id: f_00009-9-1 loss: 0.770390  [   64/  118]
train() client id: f_00009-9-2 loss: 0.895925  [   96/  118]
train() client id: f_00009-10-0 loss: 0.715189  [   32/  118]
train() client id: f_00009-10-1 loss: 0.673015  [   64/  118]
train() client id: f_00009-10-2 loss: 0.853943  [   96/  118]
train() client id: f_00009-11-0 loss: 0.973650  [   32/  118]
train() client id: f_00009-11-1 loss: 0.747455  [   64/  118]
train() client id: f_00009-11-2 loss: 0.814416  [   96/  118]
At round 16 accuracy: 0.6312997347480106
At round 16 training accuracy: 0.5781354795439303
At round 16 training loss: 0.8423588683042095
update_location
xs = -3.905658 4.200318 100.009024 18.811294 0.979296 3.956410 -62.443192 -41.324852 84.663977 -27.060879 
ys = 92.587959 75.555839 1.320614 -62.455176 54.350187 37.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 7.788573831882571
ys mean: 21.894142535528704
dists_uav = 136.337025 125.404655 141.433903 119.392269 113.819602 106.983939 117.923885 108.205451 132.199315 103.674023 
uav_gains = -103.366606 -102.458239 -103.765913 -101.924604 -101.405528 -100.733006 -101.790212 -100.856278 -103.031556 -100.391775 
uav_gains_db_mean: -101.97237168284224
dists_bs = 189.907920 204.943639 325.260653 306.509018 213.366094 225.489144 210.284736 219.560697 303.660828 226.111647 
bs_gains = -103.366265 -104.292824 -109.909542 -109.187470 -104.782571 -105.454577 -104.605676 -105.130588 -109.073945 -105.488101 
bs_gains_db_mean: -106.12915588822226
Round 17
-------------------------------
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.74794599 18.22367373  8.61494673  3.08364678 21.02075903 10.12863146
  3.83198593 12.340238    9.082055    8.2217282 ]
obj_prev = 103.29561085136496
eta_min = 2.3980574083554427e-11	eta_max = 0.9215530004474818
af = 21.83294705699746	bf = 1.718217883710131	zeta = 24.01624176269721	eta = 0.9090909090909091
af = 21.83294705699746	bf = 1.718217883710131	zeta = 41.790319927079516	eta = 0.5224402946685754
af = 21.83294705699746	bf = 1.718217883710131	zeta = 33.27990204277066	eta = 0.6560400036315671
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.752345685579005	eta = 0.6876010759391976
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.67639108760098	eta = 0.6892498263649575
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.676189751909728	eta = 0.6892542072766557
eta = 0.6892542072766557
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [0.03070852 0.06458539 0.03022109 0.01047989 0.07457784 0.03558289
 0.0131608  0.0436256  0.0316834  0.02875878]
ene_total = [2.73656278 5.19862747 2.71178509 1.24687687 5.93149526 3.1458414
 1.43681478 3.6092118  3.0031103  2.655864  ]
ti_comp = [0.33188313 0.32847851 0.33045231 0.33663064 0.32655044 0.32374602
 0.33704245 0.33977473 0.30458757 0.32360102]
ti_coms = [0.07323726 0.07664188 0.07466807 0.06848974 0.07856994 0.08137436
 0.06807793 0.06534565 0.10053281 0.08151936]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.64318511e-05 1.56051844e-04 1.57976562e-05 6.34809284e-07
 2.43113417e-04 2.68654979e-05 1.25417250e-06 4.49492172e-05
 2.14264829e-05 1.41961788e-05]
ene_total = [0.52815183 0.56269552 0.53840142 0.492855   0.58283307 0.5874521
 0.48993648 0.47342069 0.72491292 0.58758383]
optimize_network iter = 0 obj = 5.56824286220838
eta = 0.6892542072766557
freqs = [4.62640631e+07 9.83099137e+07 4.57268501e+07 1.55658631e+07
 1.14190388e+08 5.49549501e+07 1.95239441e+07 6.41978302e+07
 5.20103250e+07 4.44355465e+07]
eta_min = 0.6892542072766561	eta_max = 0.6892542072766527
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 0.03573652736322809	eta = 0.9090909090909091
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 18.93445726929715	eta = 0.0017158005474531947
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.9284046028512083	eta = 0.016846958413371853
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.882600477836571	eta = 0.017256848986739414
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.8825910021965795	eta = 0.01725693584558884
eta = 0.01725693584558884
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.79246673e-04 1.70228989e-03 1.72328565e-04 6.92481033e-06
 2.65200012e-03 2.93061998e-04 1.36811274e-05 4.90328058e-04
 2.33730561e-04 1.54858866e-04]
ene_total = [0.17070727 0.21310565 0.17380525 0.15598508 0.23910014 0.1918103
 0.15520187 0.15983003 0.23404959 0.18899581]
ti_comp = [0.33188313 0.32847851 0.33045231 0.33663064 0.32655044 0.32374602
 0.33704245 0.33977473 0.30458757 0.32360102]
ti_coms = [0.07323726 0.07664188 0.07466807 0.06848974 0.07856994 0.08137436
 0.06807793 0.06534565 0.10053281 0.08151936]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.64318511e-05 1.56051844e-04 1.57976562e-05 6.34809284e-07
 2.43113417e-04 2.68654979e-05 1.25417250e-06 4.49492172e-05
 2.14264829e-05 1.41961788e-05]
ene_total = [0.52815183 0.56269552 0.53840142 0.492855   0.58283307 0.5874521
 0.48993648 0.47342069 0.72491292 0.58758383]
optimize_network iter = 1 obj = 5.56824286220839
eta = 0.6892542072766561
freqs = [4.62640631e+07 9.83099137e+07 4.57268501e+07 1.55658631e+07
 1.14190388e+08 5.49549501e+07 1.95239441e+07 6.41978302e+07
 5.20103250e+07 4.44355465e+07]
Done!
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.61811483e-05 1.53670942e-04 1.55566295e-05 6.25123921e-07
 2.39404205e-04 2.64556077e-05 1.23503744e-06 4.42634215e-05
 2.10995764e-05 1.39795860e-05]
ene_total = [0.00733991 0.00781786 0.00748236 0.0068496  0.0080964  0.00816389
 0.00680903 0.00657883 0.01007438 0.00816592]
At round 17 energy consumption: 0.07737817222069016
At round 17 eta: 0.6892542072766561
At round 17 a_n: 22.359323457775982
At round 17 local rounds: 12.185922115151007
At round 17 global rounds: 71.9537447693859
gradient difference: 0.4398508071899414
train() client id: f_00000-0-0 loss: 1.332983  [   32/  126]
train() client id: f_00000-0-1 loss: 1.169531  [   64/  126]
train() client id: f_00000-0-2 loss: 1.088476  [   96/  126]
train() client id: f_00000-1-0 loss: 1.042463  [   32/  126]
train() client id: f_00000-1-1 loss: 1.130786  [   64/  126]
train() client id: f_00000-1-2 loss: 0.942590  [   96/  126]
train() client id: f_00000-2-0 loss: 0.972799  [   32/  126]
train() client id: f_00000-2-1 loss: 1.179279  [   64/  126]
train() client id: f_00000-2-2 loss: 0.822770  [   96/  126]
train() client id: f_00000-3-0 loss: 0.956090  [   32/  126]
train() client id: f_00000-3-1 loss: 1.095341  [   64/  126]
train() client id: f_00000-3-2 loss: 0.784167  [   96/  126]
train() client id: f_00000-4-0 loss: 1.002468  [   32/  126]
train() client id: f_00000-4-1 loss: 0.926372  [   64/  126]
train() client id: f_00000-4-2 loss: 0.939258  [   96/  126]
train() client id: f_00000-5-0 loss: 1.005082  [   32/  126]
train() client id: f_00000-5-1 loss: 0.863041  [   64/  126]
train() client id: f_00000-5-2 loss: 0.813003  [   96/  126]
train() client id: f_00000-6-0 loss: 0.879733  [   32/  126]
train() client id: f_00000-6-1 loss: 0.729226  [   64/  126]
train() client id: f_00000-6-2 loss: 0.773738  [   96/  126]
train() client id: f_00000-7-0 loss: 0.883126  [   32/  126]
train() client id: f_00000-7-1 loss: 0.831756  [   64/  126]
train() client id: f_00000-7-2 loss: 0.848623  [   96/  126]
train() client id: f_00000-8-0 loss: 0.924082  [   32/  126]
train() client id: f_00000-8-1 loss: 0.791536  [   64/  126]
train() client id: f_00000-8-2 loss: 0.750279  [   96/  126]
train() client id: f_00000-9-0 loss: 0.938131  [   32/  126]
train() client id: f_00000-9-1 loss: 0.826740  [   64/  126]
train() client id: f_00000-9-2 loss: 0.740216  [   96/  126]
train() client id: f_00000-10-0 loss: 0.719159  [   32/  126]
train() client id: f_00000-10-1 loss: 0.905070  [   64/  126]
train() client id: f_00000-10-2 loss: 0.955602  [   96/  126]
train() client id: f_00000-11-0 loss: 0.881257  [   32/  126]
train() client id: f_00000-11-1 loss: 0.856872  [   64/  126]
train() client id: f_00000-11-2 loss: 0.796481  [   96/  126]
train() client id: f_00001-0-0 loss: 0.472541  [   32/  265]
train() client id: f_00001-0-1 loss: 0.494850  [   64/  265]
train() client id: f_00001-0-2 loss: 0.507847  [   96/  265]
train() client id: f_00001-0-3 loss: 0.563702  [  128/  265]
train() client id: f_00001-0-4 loss: 0.468436  [  160/  265]
train() client id: f_00001-0-5 loss: 0.494991  [  192/  265]
train() client id: f_00001-0-6 loss: 0.405020  [  224/  265]
train() client id: f_00001-0-7 loss: 0.474893  [  256/  265]
train() client id: f_00001-1-0 loss: 0.476247  [   32/  265]
train() client id: f_00001-1-1 loss: 0.522896  [   64/  265]
train() client id: f_00001-1-2 loss: 0.509335  [   96/  265]
train() client id: f_00001-1-3 loss: 0.458694  [  128/  265]
train() client id: f_00001-1-4 loss: 0.443458  [  160/  265]
train() client id: f_00001-1-5 loss: 0.469386  [  192/  265]
train() client id: f_00001-1-6 loss: 0.450766  [  224/  265]
train() client id: f_00001-1-7 loss: 0.456843  [  256/  265]
train() client id: f_00001-2-0 loss: 0.386240  [   32/  265]
train() client id: f_00001-2-1 loss: 0.384815  [   64/  265]
train() client id: f_00001-2-2 loss: 0.382204  [   96/  265]
train() client id: f_00001-2-3 loss: 0.736416  [  128/  265]
train() client id: f_00001-2-4 loss: 0.494110  [  160/  265]
train() client id: f_00001-2-5 loss: 0.436898  [  192/  265]
train() client id: f_00001-2-6 loss: 0.506841  [  224/  265]
train() client id: f_00001-2-7 loss: 0.420208  [  256/  265]
train() client id: f_00001-3-0 loss: 0.512487  [   32/  265]
train() client id: f_00001-3-1 loss: 0.404358  [   64/  265]
train() client id: f_00001-3-2 loss: 0.394856  [   96/  265]
train() client id: f_00001-3-3 loss: 0.409698  [  128/  265]
train() client id: f_00001-3-4 loss: 0.436170  [  160/  265]
train() client id: f_00001-3-5 loss: 0.593917  [  192/  265]
train() client id: f_00001-3-6 loss: 0.433386  [  224/  265]
train() client id: f_00001-3-7 loss: 0.377968  [  256/  265]
train() client id: f_00001-4-0 loss: 0.461236  [   32/  265]
train() client id: f_00001-4-1 loss: 0.610703  [   64/  265]
train() client id: f_00001-4-2 loss: 0.459233  [   96/  265]
train() client id: f_00001-4-3 loss: 0.405233  [  128/  265]
train() client id: f_00001-4-4 loss: 0.482536  [  160/  265]
train() client id: f_00001-4-5 loss: 0.356765  [  192/  265]
train() client id: f_00001-4-6 loss: 0.365025  [  224/  265]
train() client id: f_00001-4-7 loss: 0.504209  [  256/  265]
train() client id: f_00001-5-0 loss: 0.486094  [   32/  265]
train() client id: f_00001-5-1 loss: 0.388402  [   64/  265]
train() client id: f_00001-5-2 loss: 0.558031  [   96/  265]
train() client id: f_00001-5-3 loss: 0.395525  [  128/  265]
train() client id: f_00001-5-4 loss: 0.463667  [  160/  265]
train() client id: f_00001-5-5 loss: 0.476399  [  192/  265]
train() client id: f_00001-5-6 loss: 0.404427  [  224/  265]
train() client id: f_00001-5-7 loss: 0.453501  [  256/  265]
train() client id: f_00001-6-0 loss: 0.513654  [   32/  265]
train() client id: f_00001-6-1 loss: 0.412498  [   64/  265]
train() client id: f_00001-6-2 loss: 0.467278  [   96/  265]
train() client id: f_00001-6-3 loss: 0.431285  [  128/  265]
train() client id: f_00001-6-4 loss: 0.472814  [  160/  265]
train() client id: f_00001-6-5 loss: 0.357649  [  192/  265]
train() client id: f_00001-6-6 loss: 0.368640  [  224/  265]
train() client id: f_00001-6-7 loss: 0.589592  [  256/  265]
train() client id: f_00001-7-0 loss: 0.482771  [   32/  265]
train() client id: f_00001-7-1 loss: 0.428346  [   64/  265]
train() client id: f_00001-7-2 loss: 0.361405  [   96/  265]
train() client id: f_00001-7-3 loss: 0.397636  [  128/  265]
train() client id: f_00001-7-4 loss: 0.366924  [  160/  265]
train() client id: f_00001-7-5 loss: 0.448420  [  192/  265]
train() client id: f_00001-7-6 loss: 0.388444  [  224/  265]
train() client id: f_00001-7-7 loss: 0.622304  [  256/  265]
train() client id: f_00001-8-0 loss: 0.367154  [   32/  265]
train() client id: f_00001-8-1 loss: 0.496535  [   64/  265]
train() client id: f_00001-8-2 loss: 0.504731  [   96/  265]
train() client id: f_00001-8-3 loss: 0.366815  [  128/  265]
train() client id: f_00001-8-4 loss: 0.443829  [  160/  265]
train() client id: f_00001-8-5 loss: 0.371340  [  192/  265]
train() client id: f_00001-8-6 loss: 0.604763  [  224/  265]
train() client id: f_00001-8-7 loss: 0.409086  [  256/  265]
train() client id: f_00001-9-0 loss: 0.474487  [   32/  265]
train() client id: f_00001-9-1 loss: 0.362816  [   64/  265]
train() client id: f_00001-9-2 loss: 0.393448  [   96/  265]
train() client id: f_00001-9-3 loss: 0.520511  [  128/  265]
train() client id: f_00001-9-4 loss: 0.335815  [  160/  265]
train() client id: f_00001-9-5 loss: 0.442385  [  192/  265]
train() client id: f_00001-9-6 loss: 0.516267  [  224/  265]
train() client id: f_00001-9-7 loss: 0.518314  [  256/  265]
train() client id: f_00001-10-0 loss: 0.503872  [   32/  265]
train() client id: f_00001-10-1 loss: 0.350269  [   64/  265]
train() client id: f_00001-10-2 loss: 0.365584  [   96/  265]
train() client id: f_00001-10-3 loss: 0.625441  [  128/  265]
train() client id: f_00001-10-4 loss: 0.436769  [  160/  265]
train() client id: f_00001-10-5 loss: 0.446489  [  192/  265]
train() client id: f_00001-10-6 loss: 0.492900  [  224/  265]
train() client id: f_00001-10-7 loss: 0.337611  [  256/  265]
train() client id: f_00001-11-0 loss: 0.484087  [   32/  265]
train() client id: f_00001-11-1 loss: 0.558887  [   64/  265]
train() client id: f_00001-11-2 loss: 0.593881  [   96/  265]
train() client id: f_00001-11-3 loss: 0.373200  [  128/  265]
train() client id: f_00001-11-4 loss: 0.356265  [  160/  265]
train() client id: f_00001-11-5 loss: 0.365987  [  192/  265]
train() client id: f_00001-11-6 loss: 0.458409  [  224/  265]
train() client id: f_00001-11-7 loss: 0.360557  [  256/  265]
train() client id: f_00002-0-0 loss: 1.229203  [   32/  124]
train() client id: f_00002-0-1 loss: 1.055306  [   64/  124]
train() client id: f_00002-0-2 loss: 1.145621  [   96/  124]
train() client id: f_00002-1-0 loss: 1.136976  [   32/  124]
train() client id: f_00002-1-1 loss: 1.087557  [   64/  124]
train() client id: f_00002-1-2 loss: 1.043055  [   96/  124]
train() client id: f_00002-2-0 loss: 0.920401  [   32/  124]
train() client id: f_00002-2-1 loss: 1.074326  [   64/  124]
train() client id: f_00002-2-2 loss: 1.062420  [   96/  124]
train() client id: f_00002-3-0 loss: 0.994632  [   32/  124]
train() client id: f_00002-3-1 loss: 0.885882  [   64/  124]
train() client id: f_00002-3-2 loss: 1.202442  [   96/  124]
train() client id: f_00002-4-0 loss: 0.966331  [   32/  124]
train() client id: f_00002-4-1 loss: 1.010798  [   64/  124]
train() client id: f_00002-4-2 loss: 1.129684  [   96/  124]
train() client id: f_00002-5-0 loss: 1.066729  [   32/  124]
train() client id: f_00002-5-1 loss: 0.953084  [   64/  124]
train() client id: f_00002-5-2 loss: 0.788054  [   96/  124]
train() client id: f_00002-6-0 loss: 0.893255  [   32/  124]
train() client id: f_00002-6-1 loss: 0.872702  [   64/  124]
train() client id: f_00002-6-2 loss: 1.043795  [   96/  124]
train() client id: f_00002-7-0 loss: 0.880939  [   32/  124]
train() client id: f_00002-7-1 loss: 0.918140  [   64/  124]
train() client id: f_00002-7-2 loss: 0.944489  [   96/  124]
train() client id: f_00002-8-0 loss: 1.033772  [   32/  124]
train() client id: f_00002-8-1 loss: 1.039251  [   64/  124]
train() client id: f_00002-8-2 loss: 0.854770  [   96/  124]
train() client id: f_00002-9-0 loss: 0.918895  [   32/  124]
train() client id: f_00002-9-1 loss: 0.825728  [   64/  124]
train() client id: f_00002-9-2 loss: 0.871610  [   96/  124]
train() client id: f_00002-10-0 loss: 0.853783  [   32/  124]
train() client id: f_00002-10-1 loss: 1.028106  [   64/  124]
train() client id: f_00002-10-2 loss: 0.864970  [   96/  124]
train() client id: f_00002-11-0 loss: 0.983755  [   32/  124]
train() client id: f_00002-11-1 loss: 0.927831  [   64/  124]
train() client id: f_00002-11-2 loss: 0.849892  [   96/  124]
train() client id: f_00003-0-0 loss: 0.770807  [   32/   43]
train() client id: f_00003-1-0 loss: 0.929600  [   32/   43]
train() client id: f_00003-2-0 loss: 0.814751  [   32/   43]
train() client id: f_00003-3-0 loss: 0.721449  [   32/   43]
train() client id: f_00003-4-0 loss: 0.768118  [   32/   43]
train() client id: f_00003-5-0 loss: 0.782071  [   32/   43]
train() client id: f_00003-6-0 loss: 0.715913  [   32/   43]
train() client id: f_00003-7-0 loss: 0.925180  [   32/   43]
train() client id: f_00003-8-0 loss: 0.840327  [   32/   43]
train() client id: f_00003-9-0 loss: 0.868733  [   32/   43]
train() client id: f_00003-10-0 loss: 0.756652  [   32/   43]
train() client id: f_00003-11-0 loss: 0.731708  [   32/   43]
train() client id: f_00004-0-0 loss: 0.866334  [   32/  306]
train() client id: f_00004-0-1 loss: 0.778316  [   64/  306]
train() client id: f_00004-0-2 loss: 0.770898  [   96/  306]
train() client id: f_00004-0-3 loss: 0.765895  [  128/  306]
train() client id: f_00004-0-4 loss: 0.797135  [  160/  306]
train() client id: f_00004-0-5 loss: 0.768405  [  192/  306]
train() client id: f_00004-0-6 loss: 0.823439  [  224/  306]
train() client id: f_00004-0-7 loss: 0.922276  [  256/  306]
train() client id: f_00004-0-8 loss: 0.809574  [  288/  306]
train() client id: f_00004-1-0 loss: 0.862246  [   32/  306]
train() client id: f_00004-1-1 loss: 0.834705  [   64/  306]
train() client id: f_00004-1-2 loss: 0.778262  [   96/  306]
train() client id: f_00004-1-3 loss: 0.784693  [  128/  306]
train() client id: f_00004-1-4 loss: 0.831425  [  160/  306]
train() client id: f_00004-1-5 loss: 0.953158  [  192/  306]
train() client id: f_00004-1-6 loss: 0.756490  [  224/  306]
train() client id: f_00004-1-7 loss: 0.820828  [  256/  306]
train() client id: f_00004-1-8 loss: 0.795563  [  288/  306]
train() client id: f_00004-2-0 loss: 0.807088  [   32/  306]
train() client id: f_00004-2-1 loss: 0.926622  [   64/  306]
train() client id: f_00004-2-2 loss: 0.834979  [   96/  306]
train() client id: f_00004-2-3 loss: 0.751547  [  128/  306]
train() client id: f_00004-2-4 loss: 0.848686  [  160/  306]
train() client id: f_00004-2-5 loss: 0.789156  [  192/  306]
train() client id: f_00004-2-6 loss: 0.814364  [  224/  306]
train() client id: f_00004-2-7 loss: 0.852701  [  256/  306]
train() client id: f_00004-2-8 loss: 0.764925  [  288/  306]
train() client id: f_00004-3-0 loss: 0.757808  [   32/  306]
train() client id: f_00004-3-1 loss: 0.760961  [   64/  306]
train() client id: f_00004-3-2 loss: 0.821157  [   96/  306]
train() client id: f_00004-3-3 loss: 0.869599  [  128/  306]
train() client id: f_00004-3-4 loss: 0.897746  [  160/  306]
train() client id: f_00004-3-5 loss: 0.821049  [  192/  306]
train() client id: f_00004-3-6 loss: 0.726956  [  224/  306]
train() client id: f_00004-3-7 loss: 0.937730  [  256/  306]
train() client id: f_00004-3-8 loss: 0.859750  [  288/  306]
train() client id: f_00004-4-0 loss: 0.821134  [   32/  306]
train() client id: f_00004-4-1 loss: 0.815704  [   64/  306]
train() client id: f_00004-4-2 loss: 0.883150  [   96/  306]
train() client id: f_00004-4-3 loss: 0.832769  [  128/  306]
train() client id: f_00004-4-4 loss: 0.733653  [  160/  306]
train() client id: f_00004-4-5 loss: 0.901635  [  192/  306]
train() client id: f_00004-4-6 loss: 0.851964  [  224/  306]
train() client id: f_00004-4-7 loss: 0.701719  [  256/  306]
train() client id: f_00004-4-8 loss: 0.825266  [  288/  306]
train() client id: f_00004-5-0 loss: 0.782765  [   32/  306]
train() client id: f_00004-5-1 loss: 0.835745  [   64/  306]
train() client id: f_00004-5-2 loss: 0.794845  [   96/  306]
train() client id: f_00004-5-3 loss: 0.786059  [  128/  306]
train() client id: f_00004-5-4 loss: 0.917861  [  160/  306]
train() client id: f_00004-5-5 loss: 0.876429  [  192/  306]
train() client id: f_00004-5-6 loss: 0.748306  [  224/  306]
train() client id: f_00004-5-7 loss: 0.834919  [  256/  306]
train() client id: f_00004-5-8 loss: 0.805286  [  288/  306]
train() client id: f_00004-6-0 loss: 0.827288  [   32/  306]
train() client id: f_00004-6-1 loss: 0.784674  [   64/  306]
train() client id: f_00004-6-2 loss: 0.957165  [   96/  306]
train() client id: f_00004-6-3 loss: 0.887452  [  128/  306]
train() client id: f_00004-6-4 loss: 0.744696  [  160/  306]
train() client id: f_00004-6-5 loss: 0.784884  [  192/  306]
train() client id: f_00004-6-6 loss: 0.718407  [  224/  306]
train() client id: f_00004-6-7 loss: 0.847930  [  256/  306]
train() client id: f_00004-6-8 loss: 0.830754  [  288/  306]
train() client id: f_00004-7-0 loss: 0.852393  [   32/  306]
train() client id: f_00004-7-1 loss: 0.835769  [   64/  306]
train() client id: f_00004-7-2 loss: 0.902098  [   96/  306]
train() client id: f_00004-7-3 loss: 0.882928  [  128/  306]
train() client id: f_00004-7-4 loss: 0.768663  [  160/  306]
train() client id: f_00004-7-5 loss: 0.813763  [  192/  306]
train() client id: f_00004-7-6 loss: 0.820120  [  224/  306]
train() client id: f_00004-7-7 loss: 0.754490  [  256/  306]
train() client id: f_00004-7-8 loss: 0.823078  [  288/  306]
train() client id: f_00004-8-0 loss: 0.863362  [   32/  306]
train() client id: f_00004-8-1 loss: 0.880046  [   64/  306]
train() client id: f_00004-8-2 loss: 0.907192  [   96/  306]
train() client id: f_00004-8-3 loss: 0.799073  [  128/  306]
train() client id: f_00004-8-4 loss: 0.955652  [  160/  306]
train() client id: f_00004-8-5 loss: 0.712216  [  192/  306]
train() client id: f_00004-8-6 loss: 0.746274  [  224/  306]
train() client id: f_00004-8-7 loss: 0.800560  [  256/  306]
train() client id: f_00004-8-8 loss: 0.785450  [  288/  306]
train() client id: f_00004-9-0 loss: 0.847696  [   32/  306]
train() client id: f_00004-9-1 loss: 0.705207  [   64/  306]
train() client id: f_00004-9-2 loss: 0.739566  [   96/  306]
train() client id: f_00004-9-3 loss: 0.910873  [  128/  306]
train() client id: f_00004-9-4 loss: 0.925182  [  160/  306]
train() client id: f_00004-9-5 loss: 0.805085  [  192/  306]
train() client id: f_00004-9-6 loss: 0.785424  [  224/  306]
train() client id: f_00004-9-7 loss: 0.861460  [  256/  306]
train() client id: f_00004-9-8 loss: 0.818103  [  288/  306]
train() client id: f_00004-10-0 loss: 0.852126  [   32/  306]
train() client id: f_00004-10-1 loss: 0.775331  [   64/  306]
train() client id: f_00004-10-2 loss: 0.773860  [   96/  306]
train() client id: f_00004-10-3 loss: 0.795355  [  128/  306]
train() client id: f_00004-10-4 loss: 0.821004  [  160/  306]
train() client id: f_00004-10-5 loss: 0.804175  [  192/  306]
train() client id: f_00004-10-6 loss: 0.865126  [  224/  306]
train() client id: f_00004-10-7 loss: 0.964606  [  256/  306]
train() client id: f_00004-10-8 loss: 0.727600  [  288/  306]
train() client id: f_00004-11-0 loss: 0.832725  [   32/  306]
train() client id: f_00004-11-1 loss: 0.884416  [   64/  306]
train() client id: f_00004-11-2 loss: 0.751373  [   96/  306]
train() client id: f_00004-11-3 loss: 0.904073  [  128/  306]
train() client id: f_00004-11-4 loss: 0.813684  [  160/  306]
train() client id: f_00004-11-5 loss: 0.850112  [  192/  306]
train() client id: f_00004-11-6 loss: 0.680943  [  224/  306]
train() client id: f_00004-11-7 loss: 0.811599  [  256/  306]
train() client id: f_00004-11-8 loss: 0.987391  [  288/  306]
train() client id: f_00005-0-0 loss: 1.137044  [   32/  146]
train() client id: f_00005-0-1 loss: 0.955528  [   64/  146]
train() client id: f_00005-0-2 loss: 0.900725  [   96/  146]
train() client id: f_00005-0-3 loss: 0.889351  [  128/  146]
train() client id: f_00005-1-0 loss: 0.905923  [   32/  146]
train() client id: f_00005-1-1 loss: 0.940264  [   64/  146]
train() client id: f_00005-1-2 loss: 0.856987  [   96/  146]
train() client id: f_00005-1-3 loss: 0.955609  [  128/  146]
train() client id: f_00005-2-0 loss: 1.180593  [   32/  146]
train() client id: f_00005-2-1 loss: 0.963364  [   64/  146]
train() client id: f_00005-2-2 loss: 0.865784  [   96/  146]
train() client id: f_00005-2-3 loss: 0.984821  [  128/  146]
train() client id: f_00005-3-0 loss: 0.989938  [   32/  146]
train() client id: f_00005-3-1 loss: 1.021621  [   64/  146]
train() client id: f_00005-3-2 loss: 0.939085  [   96/  146]
train() client id: f_00005-3-3 loss: 0.883485  [  128/  146]
train() client id: f_00005-4-0 loss: 1.092562  [   32/  146]
train() client id: f_00005-4-1 loss: 1.144692  [   64/  146]
train() client id: f_00005-4-2 loss: 0.918258  [   96/  146]
train() client id: f_00005-4-3 loss: 0.916412  [  128/  146]
train() client id: f_00005-5-0 loss: 0.968718  [   32/  146]
train() client id: f_00005-5-1 loss: 0.988444  [   64/  146]
train() client id: f_00005-5-2 loss: 0.889501  [   96/  146]
train() client id: f_00005-5-3 loss: 1.203566  [  128/  146]
train() client id: f_00005-6-0 loss: 1.029951  [   32/  146]
train() client id: f_00005-6-1 loss: 0.718207  [   64/  146]
train() client id: f_00005-6-2 loss: 1.004931  [   96/  146]
train() client id: f_00005-6-3 loss: 1.148222  [  128/  146]
train() client id: f_00005-7-0 loss: 0.957222  [   32/  146]
train() client id: f_00005-7-1 loss: 0.951428  [   64/  146]
train() client id: f_00005-7-2 loss: 0.931445  [   96/  146]
train() client id: f_00005-7-3 loss: 1.247769  [  128/  146]
train() client id: f_00005-8-0 loss: 0.910469  [   32/  146]
train() client id: f_00005-8-1 loss: 1.054160  [   64/  146]
train() client id: f_00005-8-2 loss: 0.807313  [   96/  146]
train() client id: f_00005-8-3 loss: 1.081636  [  128/  146]
train() client id: f_00005-9-0 loss: 0.831729  [   32/  146]
train() client id: f_00005-9-1 loss: 1.196615  [   64/  146]
train() client id: f_00005-9-2 loss: 1.189083  [   96/  146]
train() client id: f_00005-9-3 loss: 0.927665  [  128/  146]
train() client id: f_00005-10-0 loss: 1.001151  [   32/  146]
train() client id: f_00005-10-1 loss: 1.050275  [   64/  146]
train() client id: f_00005-10-2 loss: 0.925308  [   96/  146]
train() client id: f_00005-10-3 loss: 0.991567  [  128/  146]
train() client id: f_00005-11-0 loss: 1.019371  [   32/  146]
train() client id: f_00005-11-1 loss: 0.829817  [   64/  146]
train() client id: f_00005-11-2 loss: 1.320630  [   96/  146]
train() client id: f_00005-11-3 loss: 0.913240  [  128/  146]
train() client id: f_00006-0-0 loss: 0.578938  [   32/   54]
train() client id: f_00006-1-0 loss: 0.537098  [   32/   54]
train() client id: f_00006-2-0 loss: 0.590002  [   32/   54]
train() client id: f_00006-3-0 loss: 0.629920  [   32/   54]
train() client id: f_00006-4-0 loss: 0.610603  [   32/   54]
train() client id: f_00006-5-0 loss: 0.580563  [   32/   54]
train() client id: f_00006-6-0 loss: 0.543891  [   32/   54]
train() client id: f_00006-7-0 loss: 0.580210  [   32/   54]
train() client id: f_00006-8-0 loss: 0.540153  [   32/   54]
train() client id: f_00006-9-0 loss: 0.633220  [   32/   54]
train() client id: f_00006-10-0 loss: 0.633035  [   32/   54]
train() client id: f_00006-11-0 loss: 0.581631  [   32/   54]
train() client id: f_00007-0-0 loss: 0.601354  [   32/  179]
train() client id: f_00007-0-1 loss: 0.723066  [   64/  179]
train() client id: f_00007-0-2 loss: 0.606126  [   96/  179]
train() client id: f_00007-0-3 loss: 0.596088  [  128/  179]
train() client id: f_00007-0-4 loss: 0.498321  [  160/  179]
train() client id: f_00007-1-0 loss: 0.522888  [   32/  179]
train() client id: f_00007-1-1 loss: 0.702651  [   64/  179]
train() client id: f_00007-1-2 loss: 0.395374  [   96/  179]
train() client id: f_00007-1-3 loss: 0.774796  [  128/  179]
train() client id: f_00007-1-4 loss: 0.496474  [  160/  179]
train() client id: f_00007-2-0 loss: 0.610599  [   32/  179]
train() client id: f_00007-2-1 loss: 0.405521  [   64/  179]
train() client id: f_00007-2-2 loss: 0.424365  [   96/  179]
train() client id: f_00007-2-3 loss: 0.535617  [  128/  179]
train() client id: f_00007-2-4 loss: 0.739937  [  160/  179]
train() client id: f_00007-3-0 loss: 0.464056  [   32/  179]
train() client id: f_00007-3-1 loss: 0.549564  [   64/  179]
train() client id: f_00007-3-2 loss: 0.651752  [   96/  179]
train() client id: f_00007-3-3 loss: 0.674521  [  128/  179]
train() client id: f_00007-3-4 loss: 0.414692  [  160/  179]
train() client id: f_00007-4-0 loss: 0.605776  [   32/  179]
train() client id: f_00007-4-1 loss: 0.510026  [   64/  179]
train() client id: f_00007-4-2 loss: 0.517413  [   96/  179]
train() client id: f_00007-4-3 loss: 0.597634  [  128/  179]
train() client id: f_00007-4-4 loss: 0.393306  [  160/  179]
train() client id: f_00007-5-0 loss: 0.504622  [   32/  179]
train() client id: f_00007-5-1 loss: 0.545170  [   64/  179]
train() client id: f_00007-5-2 loss: 0.644893  [   96/  179]
train() client id: f_00007-5-3 loss: 0.474201  [  128/  179]
train() client id: f_00007-5-4 loss: 0.616274  [  160/  179]
train() client id: f_00007-6-0 loss: 0.422084  [   32/  179]
train() client id: f_00007-6-1 loss: 0.606067  [   64/  179]
train() client id: f_00007-6-2 loss: 0.479451  [   96/  179]
train() client id: f_00007-6-3 loss: 0.468209  [  128/  179]
train() client id: f_00007-6-4 loss: 0.478369  [  160/  179]
train() client id: f_00007-7-0 loss: 0.843003  [   32/  179]
train() client id: f_00007-7-1 loss: 0.359400  [   64/  179]
train() client id: f_00007-7-2 loss: 0.409140  [   96/  179]
train() client id: f_00007-7-3 loss: 0.612746  [  128/  179]
train() client id: f_00007-7-4 loss: 0.474417  [  160/  179]
train() client id: f_00007-8-0 loss: 0.457731  [   32/  179]
train() client id: f_00007-8-1 loss: 0.671724  [   64/  179]
train() client id: f_00007-8-2 loss: 0.484707  [   96/  179]
train() client id: f_00007-8-3 loss: 0.357470  [  128/  179]
train() client id: f_00007-8-4 loss: 0.627852  [  160/  179]
train() client id: f_00007-9-0 loss: 0.736628  [   32/  179]
train() client id: f_00007-9-1 loss: 0.557487  [   64/  179]
train() client id: f_00007-9-2 loss: 0.534927  [   96/  179]
train() client id: f_00007-9-3 loss: 0.464207  [  128/  179]
train() client id: f_00007-9-4 loss: 0.454441  [  160/  179]
train() client id: f_00007-10-0 loss: 0.462992  [   32/  179]
train() client id: f_00007-10-1 loss: 0.574951  [   64/  179]
train() client id: f_00007-10-2 loss: 0.565043  [   96/  179]
train() client id: f_00007-10-3 loss: 0.384112  [  128/  179]
train() client id: f_00007-10-4 loss: 0.718432  [  160/  179]
train() client id: f_00007-11-0 loss: 0.758249  [   32/  179]
train() client id: f_00007-11-1 loss: 0.427922  [   64/  179]
train() client id: f_00007-11-2 loss: 0.566229  [   96/  179]
train() client id: f_00007-11-3 loss: 0.503294  [  128/  179]
train() client id: f_00007-11-4 loss: 0.468722  [  160/  179]
train() client id: f_00008-0-0 loss: 0.664512  [   32/  130]
train() client id: f_00008-0-1 loss: 0.596651  [   64/  130]
train() client id: f_00008-0-2 loss: 0.691585  [   96/  130]
train() client id: f_00008-0-3 loss: 0.666825  [  128/  130]
train() client id: f_00008-1-0 loss: 0.678167  [   32/  130]
train() client id: f_00008-1-1 loss: 0.777254  [   64/  130]
train() client id: f_00008-1-2 loss: 0.541454  [   96/  130]
train() client id: f_00008-1-3 loss: 0.626848  [  128/  130]
train() client id: f_00008-2-0 loss: 0.605097  [   32/  130]
train() client id: f_00008-2-1 loss: 0.734336  [   64/  130]
train() client id: f_00008-2-2 loss: 0.676033  [   96/  130]
train() client id: f_00008-2-3 loss: 0.626309  [  128/  130]
train() client id: f_00008-3-0 loss: 0.677756  [   32/  130]
train() client id: f_00008-3-1 loss: 0.580609  [   64/  130]
train() client id: f_00008-3-2 loss: 0.661654  [   96/  130]
train() client id: f_00008-3-3 loss: 0.707391  [  128/  130]
train() client id: f_00008-4-0 loss: 0.650708  [   32/  130]
train() client id: f_00008-4-1 loss: 0.621066  [   64/  130]
train() client id: f_00008-4-2 loss: 0.594762  [   96/  130]
train() client id: f_00008-4-3 loss: 0.750265  [  128/  130]
train() client id: f_00008-5-0 loss: 0.641043  [   32/  130]
train() client id: f_00008-5-1 loss: 0.744008  [   64/  130]
train() client id: f_00008-5-2 loss: 0.596979  [   96/  130]
train() client id: f_00008-5-3 loss: 0.619031  [  128/  130]
train() client id: f_00008-6-0 loss: 0.720474  [   32/  130]
train() client id: f_00008-6-1 loss: 0.621877  [   64/  130]
train() client id: f_00008-6-2 loss: 0.577072  [   96/  130]
train() client id: f_00008-6-3 loss: 0.698957  [  128/  130]
train() client id: f_00008-7-0 loss: 0.705333  [   32/  130]
train() client id: f_00008-7-1 loss: 0.666142  [   64/  130]
train() client id: f_00008-7-2 loss: 0.704381  [   96/  130]
train() client id: f_00008-7-3 loss: 0.545933  [  128/  130]
train() client id: f_00008-8-0 loss: 0.668684  [   32/  130]
train() client id: f_00008-8-1 loss: 0.610766  [   64/  130]
train() client id: f_00008-8-2 loss: 0.747461  [   96/  130]
train() client id: f_00008-8-3 loss: 0.596846  [  128/  130]
train() client id: f_00008-9-0 loss: 0.714866  [   32/  130]
train() client id: f_00008-9-1 loss: 0.620787  [   64/  130]
train() client id: f_00008-9-2 loss: 0.645917  [   96/  130]
train() client id: f_00008-9-3 loss: 0.630380  [  128/  130]
train() client id: f_00008-10-0 loss: 0.695063  [   32/  130]
train() client id: f_00008-10-1 loss: 0.635571  [   64/  130]
train() client id: f_00008-10-2 loss: 0.678675  [   96/  130]
train() client id: f_00008-10-3 loss: 0.566345  [  128/  130]
train() client id: f_00008-11-0 loss: 0.620640  [   32/  130]
train() client id: f_00008-11-1 loss: 0.687059  [   64/  130]
train() client id: f_00008-11-2 loss: 0.593250  [   96/  130]
train() client id: f_00008-11-3 loss: 0.679172  [  128/  130]
train() client id: f_00009-0-0 loss: 1.130480  [   32/  118]
train() client id: f_00009-0-1 loss: 1.017002  [   64/  118]
train() client id: f_00009-0-2 loss: 0.975324  [   96/  118]
train() client id: f_00009-1-0 loss: 1.103420  [   32/  118]
train() client id: f_00009-1-1 loss: 0.974964  [   64/  118]
train() client id: f_00009-1-2 loss: 1.060712  [   96/  118]
train() client id: f_00009-2-0 loss: 0.762004  [   32/  118]
train() client id: f_00009-2-1 loss: 0.906379  [   64/  118]
train() client id: f_00009-2-2 loss: 1.147459  [   96/  118]
train() client id: f_00009-3-0 loss: 0.912873  [   32/  118]
train() client id: f_00009-3-1 loss: 0.963454  [   64/  118]
train() client id: f_00009-3-2 loss: 0.936196  [   96/  118]
train() client id: f_00009-4-0 loss: 1.024095  [   32/  118]
train() client id: f_00009-4-1 loss: 0.865993  [   64/  118]
train() client id: f_00009-4-2 loss: 0.863611  [   96/  118]
train() client id: f_00009-5-0 loss: 0.840432  [   32/  118]
train() client id: f_00009-5-1 loss: 1.059198  [   64/  118]
train() client id: f_00009-5-2 loss: 0.797526  [   96/  118]
train() client id: f_00009-6-0 loss: 0.936511  [   32/  118]
train() client id: f_00009-6-1 loss: 0.848952  [   64/  118]
train() client id: f_00009-6-2 loss: 0.853465  [   96/  118]
train() client id: f_00009-7-0 loss: 1.074104  [   32/  118]
train() client id: f_00009-7-1 loss: 0.747951  [   64/  118]
train() client id: f_00009-7-2 loss: 0.784298  [   96/  118]
train() client id: f_00009-8-0 loss: 1.015497  [   32/  118]
train() client id: f_00009-8-1 loss: 0.784989  [   64/  118]
train() client id: f_00009-8-2 loss: 0.757255  [   96/  118]
train() client id: f_00009-9-0 loss: 0.769623  [   32/  118]
train() client id: f_00009-9-1 loss: 0.730735  [   64/  118]
train() client id: f_00009-9-2 loss: 0.912129  [   96/  118]
train() client id: f_00009-10-0 loss: 0.950376  [   32/  118]
train() client id: f_00009-10-1 loss: 0.766989  [   64/  118]
train() client id: f_00009-10-2 loss: 0.786052  [   96/  118]
train() client id: f_00009-11-0 loss: 0.757508  [   32/  118]
train() client id: f_00009-11-1 loss: 0.846325  [   64/  118]
train() client id: f_00009-11-2 loss: 0.772272  [   96/  118]
At round 17 accuracy: 0.6339522546419099
At round 17 training accuracy: 0.5828303152246814
At round 17 training loss: 0.8327425717869422
update_location
xs = -3.905658 4.200318 105.009024 18.811294 0.979296 3.956410 -67.443192 -46.324852 89.663977 -32.060879 
ys = 97.587959 80.555839 1.320614 -67.455176 59.350187 42.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 7.288573831882571
ys mean: 23.394142535528704
dists_uav = 139.780771 128.479126 145.012548 122.082208 116.290170 108.851756 120.646072 110.211924 135.455892 105.090018 
uav_gains = -103.637956 -102.721371 -104.038043 -102.166582 -101.638713 -100.920940 -102.038061 -101.055779 -103.296101 -100.539069 
uav_gains_db_mean: -102.20526121057358
dists_bs = 187.792166 202.564690 329.498987 310.398663 210.579182 222.482573 207.651395 216.552876 307.947298 222.872353 
bs_gains = -103.230028 -104.150844 -110.066974 -109.340815 -104.622691 -105.291347 -104.452435 -104.962850 -109.244398 -105.312633 
bs_gains_db_mean: -106.06750149528617
Round 18
-------------------------------
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.61607623 17.94328127  8.48520447  3.03808191 20.69729735  9.97190604
  3.77497723 12.15244223  8.94539714  8.09409538]
obj_prev = 101.71875926654823
eta_min = 1.6526197130074664e-11	eta_max = 0.921782499053972
af = 21.498465320302763	bf = 1.69748013632556	zeta = 23.64831185233304	eta = 0.9090909090909091
af = 21.498465320302763	bf = 1.69748013632556	zeta = 41.211530046727255	eta = 0.5216614208675814
af = 21.498465320302763	bf = 1.69748013632556	zeta = 32.795424289084025	eta = 0.6555324648584754
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.284347894451706	eta = 0.6871955711793989
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.2090722305542	eta = 0.6888530732821788
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.208871892116978	eta = 0.6888574952218328
eta = 0.6888574952218328
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [0.03075603 0.06468531 0.03026784 0.01049611 0.07469322 0.03563794
 0.01318116 0.04369309 0.03173241 0.02880327]
ene_total = [2.70140281 5.11569384 2.67726863 1.23269158 5.83681499 3.09264715
 1.41984069 3.55816331 2.96469034 2.60965854]
ti_comp = [0.33703834 0.33514192 0.33556709 0.34199829 0.33331177 0.33056654
 0.34240076 0.34533101 0.3095942  0.33047606]
ti_coms = [0.07420365 0.07610006 0.0756749  0.0692437  0.07793021 0.08067545
 0.06884123 0.06591098 0.10164779 0.08076593]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.60070873e-05 1.50605001e-04 1.53909571e-05 6.17897996e-07
 2.34434896e-04 2.58880941e-05 1.22087479e-06 4.37166851e-05
 2.08354902e-05 1.36749361e-05]
ene_total = [0.52620567 0.54914919 0.53657281 0.49001957 0.56803143 0.57270002
 0.4872143  0.4694866  0.72074503 0.57247604]
optimize_network iter = 0 obj = 5.492600646974247
eta = 0.6888574952218328
freqs = [4.56269040e+07 9.65043471e+07 4.50995376e+07 1.53452611e+07
 1.12047077e+08 5.39043395e+07 1.92481414e+07 6.32626258e+07
 5.12483990e+07 4.35784487e+07]
eta_min = 0.6888574952218411	eta_max = 0.6888574952218237
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 0.033943701105718926	eta = 0.909090909090909
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 18.704633302137236	eta = 0.001649746862055949
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8983226351773008	eta = 0.016255355925430464
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8547436567121627	eta = 0.016637291080325793
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8547352592532886	eta = 0.016637366406961725
eta = 0.016637366406961725
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.75913676e-04 1.65510932e-03 1.69142568e-04 6.79053635e-06
 2.57637780e-03 2.84503340e-04 1.34170925e-05 4.80434863e-04
 2.28976553e-04 1.50283948e-04]
ene_total = [0.17007518 0.20743924 0.17321761 0.15518366 0.23216334 0.18699632
 0.15443092 0.1583265  0.23270867 0.18419382]
ti_comp = [0.33703834 0.33514192 0.33556709 0.34199829 0.33331177 0.33056654
 0.34240076 0.34533101 0.3095942  0.33047606]
ti_coms = [0.07420365 0.07610006 0.0756749  0.0692437  0.07793021 0.08067545
 0.06884123 0.06591098 0.10164779 0.08076593]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.60070873e-05 1.50605001e-04 1.53909571e-05 6.17897996e-07
 2.34434896e-04 2.58880941e-05 1.22087479e-06 4.37166851e-05
 2.08354902e-05 1.36749361e-05]
ene_total = [0.52620567 0.54914919 0.53657281 0.49001957 0.56803143 0.57270002
 0.4872143  0.4694866  0.72074503 0.57247604]
optimize_network iter = 1 obj = 5.492600646974393
eta = 0.6888574952218411
freqs = [4.56269040e+07 9.65043471e+07 4.50995376e+07 1.53452611e+07
 1.12047077e+08 5.39043395e+07 1.92481414e+07 6.32626258e+07
 5.12483990e+07 4.35784487e+07]
Done!
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.57385166e-05 1.48078115e-04 1.51327241e-05 6.07530761e-07
 2.30501493e-04 2.54537376e-05 1.20039067e-06 4.29831965e-05
 2.04859075e-05 1.34454948e-05]
ene_total = [0.0074361  0.00775808 0.00758262 0.00692498 0.00802352 0.008093
 0.00688532 0.00663408 0.01018527 0.00809004]
At round 18 energy consumption: 0.07761301768692713
At round 18 eta: 0.6888574952218411
At round 18 a_n: 22.016777610806663
At round 18 local rounds: 12.204774532627562
At round 18 global rounds: 70.7610733753795
gradient difference: 0.45237088203430176
train() client id: f_00000-0-0 loss: 1.223347  [   32/  126]
train() client id: f_00000-0-1 loss: 1.264658  [   64/  126]
train() client id: f_00000-0-2 loss: 1.163605  [   96/  126]
train() client id: f_00000-1-0 loss: 1.144113  [   32/  126]
train() client id: f_00000-1-1 loss: 1.106253  [   64/  126]
train() client id: f_00000-1-2 loss: 1.167181  [   96/  126]
train() client id: f_00000-2-0 loss: 1.010546  [   32/  126]
train() client id: f_00000-2-1 loss: 1.019820  [   64/  126]
train() client id: f_00000-2-2 loss: 1.031303  [   96/  126]
train() client id: f_00000-3-0 loss: 1.078101  [   32/  126]
train() client id: f_00000-3-1 loss: 1.078295  [   64/  126]
train() client id: f_00000-3-2 loss: 0.955143  [   96/  126]
train() client id: f_00000-4-0 loss: 1.006884  [   32/  126]
train() client id: f_00000-4-1 loss: 1.006660  [   64/  126]
train() client id: f_00000-4-2 loss: 0.898485  [   96/  126]
train() client id: f_00000-5-0 loss: 1.092178  [   32/  126]
train() client id: f_00000-5-1 loss: 0.929155  [   64/  126]
train() client id: f_00000-5-2 loss: 0.822848  [   96/  126]
train() client id: f_00000-6-0 loss: 0.943342  [   32/  126]
train() client id: f_00000-6-1 loss: 0.854067  [   64/  126]
train() client id: f_00000-6-2 loss: 1.074103  [   96/  126]
train() client id: f_00000-7-0 loss: 0.861737  [   32/  126]
train() client id: f_00000-7-1 loss: 0.954457  [   64/  126]
train() client id: f_00000-7-2 loss: 0.915251  [   96/  126]
train() client id: f_00000-8-0 loss: 0.903942  [   32/  126]
train() client id: f_00000-8-1 loss: 0.922573  [   64/  126]
train() client id: f_00000-8-2 loss: 0.968208  [   96/  126]
train() client id: f_00000-9-0 loss: 0.886866  [   32/  126]
train() client id: f_00000-9-1 loss: 0.987014  [   64/  126]
train() client id: f_00000-9-2 loss: 0.949810  [   96/  126]
train() client id: f_00000-10-0 loss: 0.967106  [   32/  126]
train() client id: f_00000-10-1 loss: 0.923631  [   64/  126]
train() client id: f_00000-10-2 loss: 0.793131  [   96/  126]
train() client id: f_00000-11-0 loss: 0.876813  [   32/  126]
train() client id: f_00000-11-1 loss: 0.820082  [   64/  126]
train() client id: f_00000-11-2 loss: 0.889789  [   96/  126]
train() client id: f_00001-0-0 loss: 0.488162  [   32/  265]
train() client id: f_00001-0-1 loss: 0.485551  [   64/  265]
train() client id: f_00001-0-2 loss: 0.490242  [   96/  265]
train() client id: f_00001-0-3 loss: 0.603348  [  128/  265]
train() client id: f_00001-0-4 loss: 0.670457  [  160/  265]
train() client id: f_00001-0-5 loss: 0.556393  [  192/  265]
train() client id: f_00001-0-6 loss: 0.484001  [  224/  265]
train() client id: f_00001-0-7 loss: 0.455826  [  256/  265]
train() client id: f_00001-1-0 loss: 0.480514  [   32/  265]
train() client id: f_00001-1-1 loss: 0.546465  [   64/  265]
train() client id: f_00001-1-2 loss: 0.516888  [   96/  265]
train() client id: f_00001-1-3 loss: 0.567880  [  128/  265]
train() client id: f_00001-1-4 loss: 0.698156  [  160/  265]
train() client id: f_00001-1-5 loss: 0.483828  [  192/  265]
train() client id: f_00001-1-6 loss: 0.430384  [  224/  265]
train() client id: f_00001-1-7 loss: 0.468899  [  256/  265]
train() client id: f_00001-2-0 loss: 0.493873  [   32/  265]
train() client id: f_00001-2-1 loss: 0.557416  [   64/  265]
train() client id: f_00001-2-2 loss: 0.461333  [   96/  265]
train() client id: f_00001-2-3 loss: 0.501173  [  128/  265]
train() client id: f_00001-2-4 loss: 0.456132  [  160/  265]
train() client id: f_00001-2-5 loss: 0.530354  [  192/  265]
train() client id: f_00001-2-6 loss: 0.522868  [  224/  265]
train() client id: f_00001-2-7 loss: 0.668403  [  256/  265]
train() client id: f_00001-3-0 loss: 0.444543  [   32/  265]
train() client id: f_00001-3-1 loss: 0.548648  [   64/  265]
train() client id: f_00001-3-2 loss: 0.471361  [   96/  265]
train() client id: f_00001-3-3 loss: 0.492804  [  128/  265]
train() client id: f_00001-3-4 loss: 0.611735  [  160/  265]
train() client id: f_00001-3-5 loss: 0.500553  [  192/  265]
train() client id: f_00001-3-6 loss: 0.516670  [  224/  265]
train() client id: f_00001-3-7 loss: 0.589394  [  256/  265]
train() client id: f_00001-4-0 loss: 0.626236  [   32/  265]
train() client id: f_00001-4-1 loss: 0.431982  [   64/  265]
train() client id: f_00001-4-2 loss: 0.519940  [   96/  265]
train() client id: f_00001-4-3 loss: 0.530663  [  128/  265]
train() client id: f_00001-4-4 loss: 0.477548  [  160/  265]
train() client id: f_00001-4-5 loss: 0.663516  [  192/  265]
train() client id: f_00001-4-6 loss: 0.406695  [  224/  265]
train() client id: f_00001-4-7 loss: 0.428581  [  256/  265]
train() client id: f_00001-5-0 loss: 0.521870  [   32/  265]
train() client id: f_00001-5-1 loss: 0.441388  [   64/  265]
train() client id: f_00001-5-2 loss: 0.547330  [   96/  265]
train() client id: f_00001-5-3 loss: 0.485762  [  128/  265]
train() client id: f_00001-5-4 loss: 0.419284  [  160/  265]
train() client id: f_00001-5-5 loss: 0.565748  [  192/  265]
train() client id: f_00001-5-6 loss: 0.505404  [  224/  265]
train() client id: f_00001-5-7 loss: 0.628617  [  256/  265]
train() client id: f_00001-6-0 loss: 0.492239  [   32/  265]
train() client id: f_00001-6-1 loss: 0.422439  [   64/  265]
train() client id: f_00001-6-2 loss: 0.554335  [   96/  265]
train() client id: f_00001-6-3 loss: 0.668012  [  128/  265]
train() client id: f_00001-6-4 loss: 0.493302  [  160/  265]
train() client id: f_00001-6-5 loss: 0.477248  [  192/  265]
train() client id: f_00001-6-6 loss: 0.560687  [  224/  265]
train() client id: f_00001-6-7 loss: 0.405655  [  256/  265]
train() client id: f_00001-7-0 loss: 0.550919  [   32/  265]
train() client id: f_00001-7-1 loss: 0.612280  [   64/  265]
train() client id: f_00001-7-2 loss: 0.598460  [   96/  265]
train() client id: f_00001-7-3 loss: 0.479597  [  128/  265]
train() client id: f_00001-7-4 loss: 0.486277  [  160/  265]
train() client id: f_00001-7-5 loss: 0.536649  [  192/  265]
train() client id: f_00001-7-6 loss: 0.432250  [  224/  265]
train() client id: f_00001-7-7 loss: 0.437967  [  256/  265]
train() client id: f_00001-8-0 loss: 0.559517  [   32/  265]
train() client id: f_00001-8-1 loss: 0.446371  [   64/  265]
train() client id: f_00001-8-2 loss: 0.536886  [   96/  265]
train() client id: f_00001-8-3 loss: 0.430996  [  128/  265]
train() client id: f_00001-8-4 loss: 0.576190  [  160/  265]
train() client id: f_00001-8-5 loss: 0.527215  [  192/  265]
train() client id: f_00001-8-6 loss: 0.550320  [  224/  265]
train() client id: f_00001-8-7 loss: 0.507675  [  256/  265]
train() client id: f_00001-9-0 loss: 0.655600  [   32/  265]
train() client id: f_00001-9-1 loss: 0.508335  [   64/  265]
train() client id: f_00001-9-2 loss: 0.487737  [   96/  265]
train() client id: f_00001-9-3 loss: 0.500178  [  128/  265]
train() client id: f_00001-9-4 loss: 0.478274  [  160/  265]
train() client id: f_00001-9-5 loss: 0.417644  [  192/  265]
train() client id: f_00001-9-6 loss: 0.565621  [  224/  265]
train() client id: f_00001-9-7 loss: 0.467996  [  256/  265]
train() client id: f_00001-10-0 loss: 0.515135  [   32/  265]
train() client id: f_00001-10-1 loss: 0.576028  [   64/  265]
train() client id: f_00001-10-2 loss: 0.509539  [   96/  265]
train() client id: f_00001-10-3 loss: 0.416481  [  128/  265]
train() client id: f_00001-10-4 loss: 0.479321  [  160/  265]
train() client id: f_00001-10-5 loss: 0.421588  [  192/  265]
train() client id: f_00001-10-6 loss: 0.528238  [  224/  265]
train() client id: f_00001-10-7 loss: 0.605267  [  256/  265]
train() client id: f_00001-11-0 loss: 0.520284  [   32/  265]
train() client id: f_00001-11-1 loss: 0.515185  [   64/  265]
train() client id: f_00001-11-2 loss: 0.565732  [   96/  265]
train() client id: f_00001-11-3 loss: 0.457322  [  128/  265]
train() client id: f_00001-11-4 loss: 0.450110  [  160/  265]
train() client id: f_00001-11-5 loss: 0.467638  [  192/  265]
train() client id: f_00001-11-6 loss: 0.553264  [  224/  265]
train() client id: f_00001-11-7 loss: 0.609485  [  256/  265]
train() client id: f_00002-0-0 loss: 1.249257  [   32/  124]
train() client id: f_00002-0-1 loss: 1.246030  [   64/  124]
train() client id: f_00002-0-2 loss: 1.043419  [   96/  124]
train() client id: f_00002-1-0 loss: 1.321954  [   32/  124]
train() client id: f_00002-1-1 loss: 1.116886  [   64/  124]
train() client id: f_00002-1-2 loss: 1.172672  [   96/  124]
train() client id: f_00002-2-0 loss: 1.230494  [   32/  124]
train() client id: f_00002-2-1 loss: 1.181339  [   64/  124]
train() client id: f_00002-2-2 loss: 1.086951  [   96/  124]
train() client id: f_00002-3-0 loss: 1.032905  [   32/  124]
train() client id: f_00002-3-1 loss: 1.303120  [   64/  124]
train() client id: f_00002-3-2 loss: 0.966261  [   96/  124]
train() client id: f_00002-4-0 loss: 1.313378  [   32/  124]
train() client id: f_00002-4-1 loss: 1.000903  [   64/  124]
train() client id: f_00002-4-2 loss: 0.932169  [   96/  124]
train() client id: f_00002-5-0 loss: 1.077514  [   32/  124]
train() client id: f_00002-5-1 loss: 0.966845  [   64/  124]
train() client id: f_00002-5-2 loss: 1.180683  [   96/  124]
train() client id: f_00002-6-0 loss: 0.964443  [   32/  124]
train() client id: f_00002-6-1 loss: 1.093749  [   64/  124]
train() client id: f_00002-6-2 loss: 0.911652  [   96/  124]
train() client id: f_00002-7-0 loss: 0.904279  [   32/  124]
train() client id: f_00002-7-1 loss: 0.936689  [   64/  124]
train() client id: f_00002-7-2 loss: 1.124340  [   96/  124]
train() client id: f_00002-8-0 loss: 1.150059  [   32/  124]
train() client id: f_00002-8-1 loss: 1.091359  [   64/  124]
train() client id: f_00002-8-2 loss: 0.945280  [   96/  124]
train() client id: f_00002-9-0 loss: 1.009129  [   32/  124]
train() client id: f_00002-9-1 loss: 1.056885  [   64/  124]
train() client id: f_00002-9-2 loss: 1.123538  [   96/  124]
train() client id: f_00002-10-0 loss: 1.005566  [   32/  124]
train() client id: f_00002-10-1 loss: 1.026302  [   64/  124]
train() client id: f_00002-10-2 loss: 1.132889  [   96/  124]
train() client id: f_00002-11-0 loss: 0.997491  [   32/  124]
train() client id: f_00002-11-1 loss: 0.898659  [   64/  124]
train() client id: f_00002-11-2 loss: 1.309314  [   96/  124]
train() client id: f_00003-0-0 loss: 0.888608  [   32/   43]
train() client id: f_00003-1-0 loss: 0.926208  [   32/   43]
train() client id: f_00003-2-0 loss: 1.018938  [   32/   43]
train() client id: f_00003-3-0 loss: 0.838791  [   32/   43]
train() client id: f_00003-4-0 loss: 1.051039  [   32/   43]
train() client id: f_00003-5-0 loss: 0.799332  [   32/   43]
train() client id: f_00003-6-0 loss: 1.029807  [   32/   43]
train() client id: f_00003-7-0 loss: 0.936358  [   32/   43]
train() client id: f_00003-8-0 loss: 0.800123  [   32/   43]
train() client id: f_00003-9-0 loss: 0.960164  [   32/   43]
train() client id: f_00003-10-0 loss: 0.728114  [   32/   43]
train() client id: f_00003-11-0 loss: 0.871625  [   32/   43]
train() client id: f_00004-0-0 loss: 0.688243  [   32/  306]
train() client id: f_00004-0-1 loss: 0.609983  [   64/  306]
train() client id: f_00004-0-2 loss: 0.752020  [   96/  306]
train() client id: f_00004-0-3 loss: 0.708403  [  128/  306]
train() client id: f_00004-0-4 loss: 0.669206  [  160/  306]
train() client id: f_00004-0-5 loss: 0.650531  [  192/  306]
train() client id: f_00004-0-6 loss: 0.720129  [  224/  306]
train() client id: f_00004-0-7 loss: 0.537372  [  256/  306]
train() client id: f_00004-0-8 loss: 0.757681  [  288/  306]
train() client id: f_00004-1-0 loss: 0.747326  [   32/  306]
train() client id: f_00004-1-1 loss: 0.731991  [   64/  306]
train() client id: f_00004-1-2 loss: 0.618993  [   96/  306]
train() client id: f_00004-1-3 loss: 0.762644  [  128/  306]
train() client id: f_00004-1-4 loss: 0.678829  [  160/  306]
train() client id: f_00004-1-5 loss: 0.645047  [  192/  306]
train() client id: f_00004-1-6 loss: 0.644541  [  224/  306]
train() client id: f_00004-1-7 loss: 0.601192  [  256/  306]
train() client id: f_00004-1-8 loss: 0.718749  [  288/  306]
train() client id: f_00004-2-0 loss: 0.598019  [   32/  306]
train() client id: f_00004-2-1 loss: 0.800599  [   64/  306]
train() client id: f_00004-2-2 loss: 0.657950  [   96/  306]
train() client id: f_00004-2-3 loss: 0.744925  [  128/  306]
train() client id: f_00004-2-4 loss: 0.609996  [  160/  306]
train() client id: f_00004-2-5 loss: 0.712301  [  192/  306]
train() client id: f_00004-2-6 loss: 0.643057  [  224/  306]
train() client id: f_00004-2-7 loss: 0.629656  [  256/  306]
train() client id: f_00004-2-8 loss: 0.701780  [  288/  306]
train() client id: f_00004-3-0 loss: 0.744518  [   32/  306]
train() client id: f_00004-3-1 loss: 0.666840  [   64/  306]
train() client id: f_00004-3-2 loss: 0.677852  [   96/  306]
train() client id: f_00004-3-3 loss: 0.587331  [  128/  306]
train() client id: f_00004-3-4 loss: 0.607110  [  160/  306]
train() client id: f_00004-3-5 loss: 0.758500  [  192/  306]
train() client id: f_00004-3-6 loss: 0.613990  [  224/  306]
train() client id: f_00004-3-7 loss: 0.745184  [  256/  306]
train() client id: f_00004-3-8 loss: 0.717864  [  288/  306]
train() client id: f_00004-4-0 loss: 0.736703  [   32/  306]
train() client id: f_00004-4-1 loss: 0.739485  [   64/  306]
train() client id: f_00004-4-2 loss: 0.738363  [   96/  306]
train() client id: f_00004-4-3 loss: 0.669985  [  128/  306]
train() client id: f_00004-4-4 loss: 0.656056  [  160/  306]
train() client id: f_00004-4-5 loss: 0.616427  [  192/  306]
train() client id: f_00004-4-6 loss: 0.780788  [  224/  306]
train() client id: f_00004-4-7 loss: 0.589968  [  256/  306]
train() client id: f_00004-4-8 loss: 0.655751  [  288/  306]
train() client id: f_00004-5-0 loss: 0.720463  [   32/  306]
train() client id: f_00004-5-1 loss: 0.636591  [   64/  306]
train() client id: f_00004-5-2 loss: 0.820754  [   96/  306]
train() client id: f_00004-5-3 loss: 0.567142  [  128/  306]
train() client id: f_00004-5-4 loss: 0.831717  [  160/  306]
train() client id: f_00004-5-5 loss: 0.620144  [  192/  306]
train() client id: f_00004-5-6 loss: 0.576012  [  224/  306]
train() client id: f_00004-5-7 loss: 0.711107  [  256/  306]
train() client id: f_00004-5-8 loss: 0.707447  [  288/  306]
train() client id: f_00004-6-0 loss: 0.648172  [   32/  306]
train() client id: f_00004-6-1 loss: 0.671000  [   64/  306]
train() client id: f_00004-6-2 loss: 0.682043  [   96/  306]
train() client id: f_00004-6-3 loss: 0.723319  [  128/  306]
train() client id: f_00004-6-4 loss: 0.744804  [  160/  306]
train() client id: f_00004-6-5 loss: 0.677449  [  192/  306]
train() client id: f_00004-6-6 loss: 0.702879  [  224/  306]
train() client id: f_00004-6-7 loss: 0.701592  [  256/  306]
train() client id: f_00004-6-8 loss: 0.687323  [  288/  306]
train() client id: f_00004-7-0 loss: 0.682505  [   32/  306]
train() client id: f_00004-7-1 loss: 0.711446  [   64/  306]
train() client id: f_00004-7-2 loss: 0.669050  [   96/  306]
train() client id: f_00004-7-3 loss: 0.642128  [  128/  306]
train() client id: f_00004-7-4 loss: 0.749790  [  160/  306]
train() client id: f_00004-7-5 loss: 0.755572  [  192/  306]
train() client id: f_00004-7-6 loss: 0.718833  [  224/  306]
train() client id: f_00004-7-7 loss: 0.711691  [  256/  306]
train() client id: f_00004-7-8 loss: 0.747628  [  288/  306]
train() client id: f_00004-8-0 loss: 0.667368  [   32/  306]
train() client id: f_00004-8-1 loss: 0.727346  [   64/  306]
train() client id: f_00004-8-2 loss: 0.663525  [   96/  306]
train() client id: f_00004-8-3 loss: 0.747229  [  128/  306]
train() client id: f_00004-8-4 loss: 0.673487  [  160/  306]
train() client id: f_00004-8-5 loss: 0.722979  [  192/  306]
train() client id: f_00004-8-6 loss: 0.617688  [  224/  306]
train() client id: f_00004-8-7 loss: 0.695305  [  256/  306]
train() client id: f_00004-8-8 loss: 0.714791  [  288/  306]
train() client id: f_00004-9-0 loss: 0.643512  [   32/  306]
train() client id: f_00004-9-1 loss: 0.687395  [   64/  306]
train() client id: f_00004-9-2 loss: 0.674285  [   96/  306]
train() client id: f_00004-9-3 loss: 0.772192  [  128/  306]
train() client id: f_00004-9-4 loss: 0.643702  [  160/  306]
train() client id: f_00004-9-5 loss: 0.657926  [  192/  306]
train() client id: f_00004-9-6 loss: 0.661612  [  224/  306]
train() client id: f_00004-9-7 loss: 0.752878  [  256/  306]
train() client id: f_00004-9-8 loss: 0.748494  [  288/  306]
train() client id: f_00004-10-0 loss: 0.707958  [   32/  306]
train() client id: f_00004-10-1 loss: 0.730556  [   64/  306]
train() client id: f_00004-10-2 loss: 0.800614  [   96/  306]
train() client id: f_00004-10-3 loss: 0.637919  [  128/  306]
train() client id: f_00004-10-4 loss: 0.704663  [  160/  306]
train() client id: f_00004-10-5 loss: 0.743355  [  192/  306]
train() client id: f_00004-10-6 loss: 0.671436  [  224/  306]
train() client id: f_00004-10-7 loss: 0.648030  [  256/  306]
train() client id: f_00004-10-8 loss: 0.794182  [  288/  306]
train() client id: f_00004-11-0 loss: 0.676073  [   32/  306]
train() client id: f_00004-11-1 loss: 0.781186  [   64/  306]
train() client id: f_00004-11-2 loss: 0.749319  [   96/  306]
train() client id: f_00004-11-3 loss: 0.716683  [  128/  306]
train() client id: f_00004-11-4 loss: 0.729687  [  160/  306]
train() client id: f_00004-11-5 loss: 0.688662  [  192/  306]
train() client id: f_00004-11-6 loss: 0.629443  [  224/  306]
train() client id: f_00004-11-7 loss: 0.692702  [  256/  306]
train() client id: f_00004-11-8 loss: 0.828968  [  288/  306]
train() client id: f_00005-0-0 loss: 0.696490  [   32/  146]
train() client id: f_00005-0-1 loss: 0.857706  [   64/  146]
train() client id: f_00005-0-2 loss: 0.660856  [   96/  146]
train() client id: f_00005-0-3 loss: 0.886699  [  128/  146]
train() client id: f_00005-1-0 loss: 0.682635  [   32/  146]
train() client id: f_00005-1-1 loss: 0.734671  [   64/  146]
train() client id: f_00005-1-2 loss: 0.761100  [   96/  146]
train() client id: f_00005-1-3 loss: 0.724312  [  128/  146]
train() client id: f_00005-2-0 loss: 0.872940  [   32/  146]
train() client id: f_00005-2-1 loss: 0.924787  [   64/  146]
train() client id: f_00005-2-2 loss: 0.666794  [   96/  146]
train() client id: f_00005-2-3 loss: 0.767725  [  128/  146]
train() client id: f_00005-3-0 loss: 0.881327  [   32/  146]
train() client id: f_00005-3-1 loss: 0.668603  [   64/  146]
train() client id: f_00005-3-2 loss: 0.911017  [   96/  146]
train() client id: f_00005-3-3 loss: 0.703919  [  128/  146]
train() client id: f_00005-4-0 loss: 0.891813  [   32/  146]
train() client id: f_00005-4-1 loss: 0.825658  [   64/  146]
train() client id: f_00005-4-2 loss: 0.552696  [   96/  146]
train() client id: f_00005-4-3 loss: 0.918689  [  128/  146]
train() client id: f_00005-5-0 loss: 0.876110  [   32/  146]
train() client id: f_00005-5-1 loss: 0.636844  [   64/  146]
train() client id: f_00005-5-2 loss: 0.616655  [   96/  146]
train() client id: f_00005-5-3 loss: 0.911169  [  128/  146]
train() client id: f_00005-6-0 loss: 0.717768  [   32/  146]
train() client id: f_00005-6-1 loss: 0.613457  [   64/  146]
train() client id: f_00005-6-2 loss: 1.009722  [   96/  146]
train() client id: f_00005-6-3 loss: 0.755405  [  128/  146]
train() client id: f_00005-7-0 loss: 0.580240  [   32/  146]
train() client id: f_00005-7-1 loss: 0.879691  [   64/  146]
train() client id: f_00005-7-2 loss: 0.922293  [   96/  146]
train() client id: f_00005-7-3 loss: 0.775122  [  128/  146]
train() client id: f_00005-8-0 loss: 0.815936  [   32/  146]
train() client id: f_00005-8-1 loss: 0.572831  [   64/  146]
train() client id: f_00005-8-2 loss: 0.600419  [   96/  146]
train() client id: f_00005-8-3 loss: 1.103709  [  128/  146]
train() client id: f_00005-9-0 loss: 0.582414  [   32/  146]
train() client id: f_00005-9-1 loss: 0.928972  [   64/  146]
train() client id: f_00005-9-2 loss: 0.809564  [   96/  146]
train() client id: f_00005-9-3 loss: 0.913667  [  128/  146]
train() client id: f_00005-10-0 loss: 0.834780  [   32/  146]
train() client id: f_00005-10-1 loss: 0.728389  [   64/  146]
train() client id: f_00005-10-2 loss: 0.724596  [   96/  146]
train() client id: f_00005-10-3 loss: 0.807126  [  128/  146]
train() client id: f_00005-11-0 loss: 0.759189  [   32/  146]
train() client id: f_00005-11-1 loss: 0.762818  [   64/  146]
train() client id: f_00005-11-2 loss: 0.625930  [   96/  146]
train() client id: f_00005-11-3 loss: 0.962429  [  128/  146]
train() client id: f_00006-0-0 loss: 0.575386  [   32/   54]
train() client id: f_00006-1-0 loss: 0.578212  [   32/   54]
train() client id: f_00006-2-0 loss: 0.528312  [   32/   54]
train() client id: f_00006-3-0 loss: 0.538980  [   32/   54]
train() client id: f_00006-4-0 loss: 0.576495  [   32/   54]
train() client id: f_00006-5-0 loss: 0.540643  [   32/   54]
train() client id: f_00006-6-0 loss: 0.541573  [   32/   54]
train() client id: f_00006-7-0 loss: 0.500675  [   32/   54]
train() client id: f_00006-8-0 loss: 0.502051  [   32/   54]
train() client id: f_00006-9-0 loss: 0.583339  [   32/   54]
train() client id: f_00006-10-0 loss: 0.540650  [   32/   54]
train() client id: f_00006-11-0 loss: 0.551078  [   32/   54]
train() client id: f_00007-0-0 loss: 0.644857  [   32/  179]
train() client id: f_00007-0-1 loss: 0.500390  [   64/  179]
train() client id: f_00007-0-2 loss: 0.577788  [   96/  179]
train() client id: f_00007-0-3 loss: 0.687136  [  128/  179]
train() client id: f_00007-0-4 loss: 0.768879  [  160/  179]
train() client id: f_00007-1-0 loss: 0.690916  [   32/  179]
train() client id: f_00007-1-1 loss: 0.601505  [   64/  179]
train() client id: f_00007-1-2 loss: 0.557168  [   96/  179]
train() client id: f_00007-1-3 loss: 0.676728  [  128/  179]
train() client id: f_00007-1-4 loss: 0.787302  [  160/  179]
train() client id: f_00007-2-0 loss: 0.549512  [   32/  179]
train() client id: f_00007-2-1 loss: 0.533294  [   64/  179]
train() client id: f_00007-2-2 loss: 0.816993  [   96/  179]
train() client id: f_00007-2-3 loss: 0.728262  [  128/  179]
train() client id: f_00007-2-4 loss: 0.568263  [  160/  179]
train() client id: f_00007-3-0 loss: 0.471874  [   32/  179]
train() client id: f_00007-3-1 loss: 0.742796  [   64/  179]
train() client id: f_00007-3-2 loss: 0.671984  [   96/  179]
train() client id: f_00007-3-3 loss: 0.589189  [  128/  179]
train() client id: f_00007-3-4 loss: 0.737049  [  160/  179]
train() client id: f_00007-4-0 loss: 0.535532  [   32/  179]
train() client id: f_00007-4-1 loss: 0.547873  [   64/  179]
train() client id: f_00007-4-2 loss: 0.784876  [   96/  179]
train() client id: f_00007-4-3 loss: 0.479483  [  128/  179]
train() client id: f_00007-4-4 loss: 0.859417  [  160/  179]
train() client id: f_00007-5-0 loss: 0.649629  [   32/  179]
train() client id: f_00007-5-1 loss: 0.443905  [   64/  179]
train() client id: f_00007-5-2 loss: 0.711650  [   96/  179]
train() client id: f_00007-5-3 loss: 0.796522  [  128/  179]
train() client id: f_00007-5-4 loss: 0.569019  [  160/  179]
train() client id: f_00007-6-0 loss: 0.601460  [   32/  179]
train() client id: f_00007-6-1 loss: 0.561021  [   64/  179]
train() client id: f_00007-6-2 loss: 0.534937  [   96/  179]
train() client id: f_00007-6-3 loss: 0.464410  [  128/  179]
train() client id: f_00007-6-4 loss: 0.714322  [  160/  179]
train() client id: f_00007-7-0 loss: 0.764681  [   32/  179]
train() client id: f_00007-7-1 loss: 0.498890  [   64/  179]
train() client id: f_00007-7-2 loss: 0.612474  [   96/  179]
train() client id: f_00007-7-3 loss: 0.561031  [  128/  179]
train() client id: f_00007-7-4 loss: 0.715551  [  160/  179]
train() client id: f_00007-8-0 loss: 0.574659  [   32/  179]
train() client id: f_00007-8-1 loss: 0.543076  [   64/  179]
train() client id: f_00007-8-2 loss: 0.716620  [   96/  179]
train() client id: f_00007-8-3 loss: 0.723021  [  128/  179]
train() client id: f_00007-8-4 loss: 0.565664  [  160/  179]
train() client id: f_00007-9-0 loss: 0.960867  [   32/  179]
train() client id: f_00007-9-1 loss: 0.632437  [   64/  179]
train() client id: f_00007-9-2 loss: 0.559608  [   96/  179]
train() client id: f_00007-9-3 loss: 0.465334  [  128/  179]
train() client id: f_00007-9-4 loss: 0.469251  [  160/  179]
train() client id: f_00007-10-0 loss: 0.713379  [   32/  179]
train() client id: f_00007-10-1 loss: 0.671310  [   64/  179]
train() client id: f_00007-10-2 loss: 0.658495  [   96/  179]
train() client id: f_00007-10-3 loss: 0.633447  [  128/  179]
train() client id: f_00007-10-4 loss: 0.423453  [  160/  179]
train() client id: f_00007-11-0 loss: 0.421985  [   32/  179]
train() client id: f_00007-11-1 loss: 0.469768  [   64/  179]
train() client id: f_00007-11-2 loss: 0.691968  [   96/  179]
train() client id: f_00007-11-3 loss: 0.742007  [  128/  179]
train() client id: f_00007-11-4 loss: 0.647823  [  160/  179]
train() client id: f_00008-0-0 loss: 0.723031  [   32/  130]
train() client id: f_00008-0-1 loss: 0.725538  [   64/  130]
train() client id: f_00008-0-2 loss: 0.793642  [   96/  130]
train() client id: f_00008-0-3 loss: 0.786990  [  128/  130]
train() client id: f_00008-1-0 loss: 0.820314  [   32/  130]
train() client id: f_00008-1-1 loss: 0.745075  [   64/  130]
train() client id: f_00008-1-2 loss: 0.787056  [   96/  130]
train() client id: f_00008-1-3 loss: 0.664178  [  128/  130]
train() client id: f_00008-2-0 loss: 0.710891  [   32/  130]
train() client id: f_00008-2-1 loss: 0.825795  [   64/  130]
train() client id: f_00008-2-2 loss: 0.680357  [   96/  130]
train() client id: f_00008-2-3 loss: 0.788920  [  128/  130]
train() client id: f_00008-3-0 loss: 0.829882  [   32/  130]
train() client id: f_00008-3-1 loss: 0.642097  [   64/  130]
train() client id: f_00008-3-2 loss: 0.786534  [   96/  130]
train() client id: f_00008-3-3 loss: 0.747658  [  128/  130]
train() client id: f_00008-4-0 loss: 0.771275  [   32/  130]
train() client id: f_00008-4-1 loss: 0.749247  [   64/  130]
train() client id: f_00008-4-2 loss: 0.683531  [   96/  130]
train() client id: f_00008-4-3 loss: 0.807412  [  128/  130]
train() client id: f_00008-5-0 loss: 0.739675  [   32/  130]
train() client id: f_00008-5-1 loss: 0.773285  [   64/  130]
train() client id: f_00008-5-2 loss: 0.754380  [   96/  130]
train() client id: f_00008-5-3 loss: 0.691074  [  128/  130]
train() client id: f_00008-6-0 loss: 0.747672  [   32/  130]
train() client id: f_00008-6-1 loss: 0.715963  [   64/  130]
train() client id: f_00008-6-2 loss: 0.764293  [   96/  130]
train() client id: f_00008-6-3 loss: 0.760038  [  128/  130]
train() client id: f_00008-7-0 loss: 0.753813  [   32/  130]
train() client id: f_00008-7-1 loss: 0.726925  [   64/  130]
train() client id: f_00008-7-2 loss: 0.772599  [   96/  130]
train() client id: f_00008-7-3 loss: 0.720634  [  128/  130]
train() client id: f_00008-8-0 loss: 0.655096  [   32/  130]
train() client id: f_00008-8-1 loss: 0.712443  [   64/  130]
train() client id: f_00008-8-2 loss: 0.774009  [   96/  130]
train() client id: f_00008-8-3 loss: 0.771401  [  128/  130]
train() client id: f_00008-9-0 loss: 0.802389  [   32/  130]
train() client id: f_00008-9-1 loss: 0.769049  [   64/  130]
train() client id: f_00008-9-2 loss: 0.705258  [   96/  130]
train() client id: f_00008-9-3 loss: 0.702694  [  128/  130]
train() client id: f_00008-10-0 loss: 0.653976  [   32/  130]
train() client id: f_00008-10-1 loss: 0.643091  [   64/  130]
train() client id: f_00008-10-2 loss: 0.887722  [   96/  130]
train() client id: f_00008-10-3 loss: 0.761405  [  128/  130]
train() client id: f_00008-11-0 loss: 0.661756  [   32/  130]
train() client id: f_00008-11-1 loss: 0.750042  [   64/  130]
train() client id: f_00008-11-2 loss: 0.725314  [   96/  130]
train() client id: f_00008-11-3 loss: 0.792773  [  128/  130]
train() client id: f_00009-0-0 loss: 1.071420  [   32/  118]
train() client id: f_00009-0-1 loss: 1.117556  [   64/  118]
train() client id: f_00009-0-2 loss: 1.199275  [   96/  118]
train() client id: f_00009-1-0 loss: 0.919191  [   32/  118]
train() client id: f_00009-1-1 loss: 0.971844  [   64/  118]
train() client id: f_00009-1-2 loss: 1.255635  [   96/  118]
train() client id: f_00009-2-0 loss: 1.195030  [   32/  118]
train() client id: f_00009-2-1 loss: 0.938498  [   64/  118]
train() client id: f_00009-2-2 loss: 1.043729  [   96/  118]
train() client id: f_00009-3-0 loss: 0.986812  [   32/  118]
train() client id: f_00009-3-1 loss: 0.931310  [   64/  118]
train() client id: f_00009-3-2 loss: 0.954846  [   96/  118]
train() client id: f_00009-4-0 loss: 0.969734  [   32/  118]
train() client id: f_00009-4-1 loss: 1.086559  [   64/  118]
train() client id: f_00009-4-2 loss: 0.936466  [   96/  118]
train() client id: f_00009-5-0 loss: 0.932998  [   32/  118]
train() client id: f_00009-5-1 loss: 0.896261  [   64/  118]
train() client id: f_00009-5-2 loss: 1.134579  [   96/  118]
train() client id: f_00009-6-0 loss: 0.895415  [   32/  118]
train() client id: f_00009-6-1 loss: 0.888259  [   64/  118]
train() client id: f_00009-6-2 loss: 1.042116  [   96/  118]
train() client id: f_00009-7-0 loss: 1.014032  [   32/  118]
train() client id: f_00009-7-1 loss: 0.803141  [   64/  118]
train() client id: f_00009-7-2 loss: 0.868842  [   96/  118]
train() client id: f_00009-8-0 loss: 0.841990  [   32/  118]
train() client id: f_00009-8-1 loss: 0.991982  [   64/  118]
train() client id: f_00009-8-2 loss: 0.983617  [   96/  118]
train() client id: f_00009-9-0 loss: 0.808750  [   32/  118]
train() client id: f_00009-9-1 loss: 0.898200  [   64/  118]
train() client id: f_00009-9-2 loss: 1.046723  [   96/  118]
train() client id: f_00009-10-0 loss: 0.833010  [   32/  118]
train() client id: f_00009-10-1 loss: 0.861038  [   64/  118]
train() client id: f_00009-10-2 loss: 1.039623  [   96/  118]
train() client id: f_00009-11-0 loss: 0.764340  [   32/  118]
train() client id: f_00009-11-1 loss: 1.071790  [   64/  118]
train() client id: f_00009-11-2 loss: 0.885770  [   96/  118]
At round 18 accuracy: 0.6339522546419099
At round 18 training accuracy: 0.5835010060362174
At round 18 training loss: 0.8311234406273244
update_location
xs = -3.905658 4.200318 110.009024 18.811294 0.979296 3.956410 -72.443192 -51.324852 94.663977 -37.060879 
ys = 102.587959 85.555839 1.320614 -72.455176 64.350187 47.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 6.78857383188257
ys mean: 24.894142535528704
dists_uav = 143.316236 131.671729 148.673230 124.914440 118.919744 110.913688 123.510755 112.405145 138.816204 106.721697 
uav_gains = -103.909864 -102.988095 -104.309866 -102.415692 -101.881538 -101.124699 -102.292940 -101.269740 -103.562615 -100.706358 
uav_gains_db_mean: -102.4461406712743
dists_bs = 185.786914 200.282331 333.758405 314.319713 207.875188 219.548712 205.106153 213.619748 312.254989 219.699100 
bs_gains = -103.099483 -104.013053 -110.223161 -109.493464 -104.465533 -105.129924 -104.302462 -104.797018 -109.413322 -105.138251 
bs_gains_db_mean: -106.00756722907128
Round 19
-------------------------------
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.48419946 17.66297686  8.35544877  2.99255128 20.37392527  9.81527082
  3.718002   11.96472075  8.80869105  7.96655398]
obj_prev = 100.14234023128081
eta_min = 1.126497835179761e-11	eta_max = 0.9220293976525566
af = 21.163983583608065	bf = 1.67717061299475	zeta = 23.280381941968873	eta = 0.9090909090909091
af = 21.163983583608065	bf = 1.67717061299475	zeta = 40.637450630966356	eta = 0.5207999826514902
af = 21.163983583608065	bf = 1.67717061299475	zeta = 32.312864825181585	eta = 0.6549708203871438
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.81773875501804	eta = 0.6867468035811661
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.743104385934746	eta = 0.6884140039315868
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.74290487459993	eta = 0.6884184715119078
eta = 0.6884184715119078
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [0.03080864 0.06479595 0.03031961 0.01051406 0.07482098 0.0356989
 0.0132037  0.04376783 0.03178669 0.02885253]
ene_total = [2.66619823 5.03309398 2.64268475 1.21857109 5.74248846 3.03975216
 1.40293291 3.50736005 2.92607578 2.56374747]
ti_comp = [0.34241459 0.34203063 0.34090455 0.34757484 0.34030078 0.33761632
 0.34796803 0.35108386 0.31483598 0.33758153]
ti_coms = [0.07519738 0.07558134 0.07670742 0.07003712 0.07731119 0.07999565
 0.06964393 0.0665281  0.10277599 0.08003044]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.55880946e-05 1.45342807e-04 1.49894390e-05 6.01304716e-07
 2.26060309e-04 2.49458102e-05 1.18819712e-06 4.25131485e-05
 2.02510366e-05 1.31726936e-05]
ene_total = [0.52417228 0.53586919 0.53463478 0.48723401 0.55351726 0.55820092
 0.48453973 0.46574004 0.71633895 0.557624  ]
optimize_network iter = 0 obj = 5.417871154985847
eta = 0.6884184715119078
freqs = [4.49873339e+07 9.47224336e+07 4.44693580e+07 1.51248853e+07
 1.09933602e+08 5.28690360e+07 1.89725795e+07 6.23324407e+07
 5.04813506e+07 4.27341730e+07]
eta_min = 0.6884184715119183	eta_max = 0.6884184715118975
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 0.03222576338096967	eta = 0.909090909090909
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 18.47959117596394	eta = 0.0015853244938805354
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.8690048705094464	eta = 0.015674730970695012
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.8275633379854408	eta = 0.016030168650925096
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.827555909002531	eta = 0.016030233813281204
eta = 0.016030233813281204
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.72568617e-04 1.60902329e-03 1.65941177e-04 6.65676762e-06
 2.50260959e-03 2.76163578e-04 1.31539832e-05 4.70643491e-04
 2.24189901e-04 1.45828825e-04]
ene_total = [0.16944101 0.20192804 0.17262124 0.15441975 0.22542177 0.18229224
 0.15369677 0.1569107  0.23132642 0.17949795]
ti_comp = [0.34241459 0.34203063 0.34090455 0.34757484 0.34030078 0.33761632
 0.34796803 0.35108386 0.31483598 0.33758153]
ti_coms = [0.07519738 0.07558134 0.07670742 0.07003712 0.07731119 0.07999565
 0.06964393 0.0665281  0.10277599 0.08003044]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.55880946e-05 1.45342807e-04 1.49894390e-05 6.01304716e-07
 2.26060309e-04 2.49458102e-05 1.18819712e-06 4.25131485e-05
 2.02510366e-05 1.31726936e-05]
ene_total = [0.52417228 0.53586919 0.53463478 0.48723401 0.55351726 0.55820092
 0.48453973 0.46574004 0.71633895 0.557624  ]
optimize_network iter = 1 obj = 5.4178711549860274
eta = 0.6884184715119183
freqs = [4.49873339e+07 9.47224336e+07 4.44693580e+07 1.51248853e+07
 1.09933602e+08 5.28690360e+07 1.89725795e+07 6.23324407e+07
 5.04813506e+07 4.27341730e+07]
Done!
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.53003832e-05 1.42660197e-04 1.47127771e-05 5.90206361e-07
 2.21887886e-04 2.44853823e-05 1.16626642e-06 4.17284781e-05
 1.98772608e-05 1.29295635e-05]
ene_total = [0.00753504 0.00770079 0.00768545 0.0070043  0.00795301 0.00802405
 0.00696556 0.00669454 0.01029748 0.00801597]
At round 19 energy consumption: 0.07787619467639152
At round 19 eta: 0.6884184715119183
At round 19 a_n: 21.674231763837348
At round 19 local rounds: 12.225650330368824
At round 19 global rounds: 69.56199190949924
gradient difference: 0.4202074408531189
train() client id: f_00000-0-0 loss: 1.173884  [   32/  126]
train() client id: f_00000-0-1 loss: 1.084014  [   64/  126]
train() client id: f_00000-0-2 loss: 1.090122  [   96/  126]
train() client id: f_00000-1-0 loss: 1.074259  [   32/  126]
train() client id: f_00000-1-1 loss: 1.011546  [   64/  126]
train() client id: f_00000-1-2 loss: 1.068829  [   96/  126]
train() client id: f_00000-2-0 loss: 0.902915  [   32/  126]
train() client id: f_00000-2-1 loss: 1.089947  [   64/  126]
train() client id: f_00000-2-2 loss: 0.978924  [   96/  126]
train() client id: f_00000-3-0 loss: 1.010235  [   32/  126]
train() client id: f_00000-3-1 loss: 0.947458  [   64/  126]
train() client id: f_00000-3-2 loss: 0.934273  [   96/  126]
train() client id: f_00000-4-0 loss: 0.875904  [   32/  126]
train() client id: f_00000-4-1 loss: 0.920673  [   64/  126]
train() client id: f_00000-4-2 loss: 0.919506  [   96/  126]
train() client id: f_00000-5-0 loss: 0.935429  [   32/  126]
train() client id: f_00000-5-1 loss: 0.903174  [   64/  126]
train() client id: f_00000-5-2 loss: 0.802492  [   96/  126]
train() client id: f_00000-6-0 loss: 0.789850  [   32/  126]
train() client id: f_00000-6-1 loss: 0.915679  [   64/  126]
train() client id: f_00000-6-2 loss: 0.949126  [   96/  126]
train() client id: f_00000-7-0 loss: 0.771243  [   32/  126]
train() client id: f_00000-7-1 loss: 0.854872  [   64/  126]
train() client id: f_00000-7-2 loss: 0.982067  [   96/  126]
train() client id: f_00000-8-0 loss: 0.976804  [   32/  126]
train() client id: f_00000-8-1 loss: 0.880837  [   64/  126]
train() client id: f_00000-8-2 loss: 0.842283  [   96/  126]
train() client id: f_00000-9-0 loss: 0.842123  [   32/  126]
train() client id: f_00000-9-1 loss: 0.803856  [   64/  126]
train() client id: f_00000-9-2 loss: 1.094837  [   96/  126]
train() client id: f_00000-10-0 loss: 0.789247  [   32/  126]
train() client id: f_00000-10-1 loss: 1.045225  [   64/  126]
train() client id: f_00000-10-2 loss: 0.801939  [   96/  126]
train() client id: f_00000-11-0 loss: 0.794119  [   32/  126]
train() client id: f_00000-11-1 loss: 0.953227  [   64/  126]
train() client id: f_00000-11-2 loss: 0.928380  [   96/  126]
train() client id: f_00001-0-0 loss: 0.437902  [   32/  265]
train() client id: f_00001-0-1 loss: 0.532238  [   64/  265]
train() client id: f_00001-0-2 loss: 0.369671  [   96/  265]
train() client id: f_00001-0-3 loss: 0.365891  [  128/  265]
train() client id: f_00001-0-4 loss: 0.332609  [  160/  265]
train() client id: f_00001-0-5 loss: 0.337341  [  192/  265]
train() client id: f_00001-0-6 loss: 0.343001  [  224/  265]
train() client id: f_00001-0-7 loss: 0.394606  [  256/  265]
train() client id: f_00001-1-0 loss: 0.461207  [   32/  265]
train() client id: f_00001-1-1 loss: 0.304575  [   64/  265]
train() client id: f_00001-1-2 loss: 0.418018  [   96/  265]
train() client id: f_00001-1-3 loss: 0.382806  [  128/  265]
train() client id: f_00001-1-4 loss: 0.407646  [  160/  265]
train() client id: f_00001-1-5 loss: 0.327879  [  192/  265]
train() client id: f_00001-1-6 loss: 0.373123  [  224/  265]
train() client id: f_00001-1-7 loss: 0.426546  [  256/  265]
train() client id: f_00001-2-0 loss: 0.292467  [   32/  265]
train() client id: f_00001-2-1 loss: 0.379419  [   64/  265]
train() client id: f_00001-2-2 loss: 0.496554  [   96/  265]
train() client id: f_00001-2-3 loss: 0.505190  [  128/  265]
train() client id: f_00001-2-4 loss: 0.291687  [  160/  265]
train() client id: f_00001-2-5 loss: 0.347409  [  192/  265]
train() client id: f_00001-2-6 loss: 0.302830  [  224/  265]
train() client id: f_00001-2-7 loss: 0.379922  [  256/  265]
train() client id: f_00001-3-0 loss: 0.327047  [   32/  265]
train() client id: f_00001-3-1 loss: 0.431454  [   64/  265]
train() client id: f_00001-3-2 loss: 0.390000  [   96/  265]
train() client id: f_00001-3-3 loss: 0.293888  [  128/  265]
train() client id: f_00001-3-4 loss: 0.359642  [  160/  265]
train() client id: f_00001-3-5 loss: 0.287991  [  192/  265]
train() client id: f_00001-3-6 loss: 0.442968  [  224/  265]
train() client id: f_00001-3-7 loss: 0.432699  [  256/  265]
train() client id: f_00001-4-0 loss: 0.352138  [   32/  265]
train() client id: f_00001-4-1 loss: 0.545926  [   64/  265]
train() client id: f_00001-4-2 loss: 0.298537  [   96/  265]
train() client id: f_00001-4-3 loss: 0.265919  [  128/  265]
train() client id: f_00001-4-4 loss: 0.464279  [  160/  265]
train() client id: f_00001-4-5 loss: 0.308554  [  192/  265]
train() client id: f_00001-4-6 loss: 0.425744  [  224/  265]
train() client id: f_00001-4-7 loss: 0.280477  [  256/  265]
train() client id: f_00001-5-0 loss: 0.351116  [   32/  265]
train() client id: f_00001-5-1 loss: 0.380062  [   64/  265]
train() client id: f_00001-5-2 loss: 0.483852  [   96/  265]
train() client id: f_00001-5-3 loss: 0.356454  [  128/  265]
train() client id: f_00001-5-4 loss: 0.441042  [  160/  265]
train() client id: f_00001-5-5 loss: 0.271227  [  192/  265]
train() client id: f_00001-5-6 loss: 0.299110  [  224/  265]
train() client id: f_00001-5-7 loss: 0.321873  [  256/  265]
train() client id: f_00001-6-0 loss: 0.261604  [   32/  265]
train() client id: f_00001-6-1 loss: 0.306292  [   64/  265]
train() client id: f_00001-6-2 loss: 0.279850  [   96/  265]
train() client id: f_00001-6-3 loss: 0.323480  [  128/  265]
train() client id: f_00001-6-4 loss: 0.432401  [  160/  265]
train() client id: f_00001-6-5 loss: 0.479348  [  192/  265]
train() client id: f_00001-6-6 loss: 0.324376  [  224/  265]
train() client id: f_00001-6-7 loss: 0.354507  [  256/  265]
train() client id: f_00001-7-0 loss: 0.324572  [   32/  265]
train() client id: f_00001-7-1 loss: 0.356807  [   64/  265]
train() client id: f_00001-7-2 loss: 0.316220  [   96/  265]
train() client id: f_00001-7-3 loss: 0.249419  [  128/  265]
train() client id: f_00001-7-4 loss: 0.393439  [  160/  265]
train() client id: f_00001-7-5 loss: 0.297359  [  192/  265]
train() client id: f_00001-7-6 loss: 0.468407  [  224/  265]
train() client id: f_00001-7-7 loss: 0.446321  [  256/  265]
train() client id: f_00001-8-0 loss: 0.449338  [   32/  265]
train() client id: f_00001-8-1 loss: 0.250168  [   64/  265]
train() client id: f_00001-8-2 loss: 0.267909  [   96/  265]
train() client id: f_00001-8-3 loss: 0.357930  [  128/  265]
train() client id: f_00001-8-4 loss: 0.386741  [  160/  265]
train() client id: f_00001-8-5 loss: 0.396796  [  192/  265]
train() client id: f_00001-8-6 loss: 0.299081  [  224/  265]
train() client id: f_00001-8-7 loss: 0.283656  [  256/  265]
train() client id: f_00001-9-0 loss: 0.266945  [   32/  265]
train() client id: f_00001-9-1 loss: 0.284259  [   64/  265]
train() client id: f_00001-9-2 loss: 0.367076  [   96/  265]
train() client id: f_00001-9-3 loss: 0.530626  [  128/  265]
train() client id: f_00001-9-4 loss: 0.359280  [  160/  265]
train() client id: f_00001-9-5 loss: 0.290178  [  192/  265]
train() client id: f_00001-9-6 loss: 0.356246  [  224/  265]
train() client id: f_00001-9-7 loss: 0.346082  [  256/  265]
train() client id: f_00001-10-0 loss: 0.341664  [   32/  265]
train() client id: f_00001-10-1 loss: 0.335052  [   64/  265]
train() client id: f_00001-10-2 loss: 0.359796  [   96/  265]
train() client id: f_00001-10-3 loss: 0.236090  [  128/  265]
train() client id: f_00001-10-4 loss: 0.485525  [  160/  265]
train() client id: f_00001-10-5 loss: 0.306788  [  192/  265]
train() client id: f_00001-10-6 loss: 0.294826  [  224/  265]
train() client id: f_00001-10-7 loss: 0.306142  [  256/  265]
train() client id: f_00001-11-0 loss: 0.376552  [   32/  265]
train() client id: f_00001-11-1 loss: 0.300020  [   64/  265]
train() client id: f_00001-11-2 loss: 0.319112  [   96/  265]
train() client id: f_00001-11-3 loss: 0.297290  [  128/  265]
train() client id: f_00001-11-4 loss: 0.424203  [  160/  265]
train() client id: f_00001-11-5 loss: 0.393019  [  192/  265]
train() client id: f_00001-11-6 loss: 0.421170  [  224/  265]
train() client id: f_00001-11-7 loss: 0.282729  [  256/  265]
train() client id: f_00002-0-0 loss: 0.855471  [   32/  124]
train() client id: f_00002-0-1 loss: 0.912212  [   64/  124]
train() client id: f_00002-0-2 loss: 1.041817  [   96/  124]
train() client id: f_00002-1-0 loss: 1.184542  [   32/  124]
train() client id: f_00002-1-1 loss: 0.838029  [   64/  124]
train() client id: f_00002-1-2 loss: 0.850996  [   96/  124]
train() client id: f_00002-2-0 loss: 0.770993  [   32/  124]
train() client id: f_00002-2-1 loss: 0.771649  [   64/  124]
train() client id: f_00002-2-2 loss: 0.960586  [   96/  124]
train() client id: f_00002-3-0 loss: 0.864152  [   32/  124]
train() client id: f_00002-3-1 loss: 0.748920  [   64/  124]
train() client id: f_00002-3-2 loss: 0.785204  [   96/  124]
train() client id: f_00002-4-0 loss: 0.895027  [   32/  124]
train() client id: f_00002-4-1 loss: 0.613941  [   64/  124]
train() client id: f_00002-4-2 loss: 0.867037  [   96/  124]
train() client id: f_00002-5-0 loss: 0.865289  [   32/  124]
train() client id: f_00002-5-1 loss: 0.755892  [   64/  124]
train() client id: f_00002-5-2 loss: 0.866778  [   96/  124]
train() client id: f_00002-6-0 loss: 0.725235  [   32/  124]
train() client id: f_00002-6-1 loss: 0.714637  [   64/  124]
train() client id: f_00002-6-2 loss: 0.690302  [   96/  124]
train() client id: f_00002-7-0 loss: 0.668751  [   32/  124]
train() client id: f_00002-7-1 loss: 0.857278  [   64/  124]
train() client id: f_00002-7-2 loss: 0.687960  [   96/  124]
train() client id: f_00002-8-0 loss: 1.033051  [   32/  124]
train() client id: f_00002-8-1 loss: 0.655575  [   64/  124]
train() client id: f_00002-8-2 loss: 0.565107  [   96/  124]
train() client id: f_00002-9-0 loss: 0.864482  [   32/  124]
train() client id: f_00002-9-1 loss: 0.666390  [   64/  124]
train() client id: f_00002-9-2 loss: 0.644724  [   96/  124]
train() client id: f_00002-10-0 loss: 0.712231  [   32/  124]
train() client id: f_00002-10-1 loss: 0.682881  [   64/  124]
train() client id: f_00002-10-2 loss: 0.750097  [   96/  124]
train() client id: f_00002-11-0 loss: 0.627903  [   32/  124]
train() client id: f_00002-11-1 loss: 0.584107  [   64/  124]
train() client id: f_00002-11-2 loss: 0.788615  [   96/  124]
train() client id: f_00003-0-0 loss: 0.673039  [   32/   43]
train() client id: f_00003-1-0 loss: 0.731266  [   32/   43]
train() client id: f_00003-2-0 loss: 0.795453  [   32/   43]
train() client id: f_00003-3-0 loss: 0.856755  [   32/   43]
train() client id: f_00003-4-0 loss: 0.873314  [   32/   43]
train() client id: f_00003-5-0 loss: 0.856971  [   32/   43]
train() client id: f_00003-6-0 loss: 0.902133  [   32/   43]
train() client id: f_00003-7-0 loss: 0.901835  [   32/   43]
train() client id: f_00003-8-0 loss: 0.640287  [   32/   43]
train() client id: f_00003-9-0 loss: 0.778073  [   32/   43]
train() client id: f_00003-10-0 loss: 0.900295  [   32/   43]
train() client id: f_00003-11-0 loss: 0.756668  [   32/   43]
train() client id: f_00004-0-0 loss: 0.776556  [   32/  306]
train() client id: f_00004-0-1 loss: 0.787978  [   64/  306]
train() client id: f_00004-0-2 loss: 0.717038  [   96/  306]
train() client id: f_00004-0-3 loss: 0.760693  [  128/  306]
train() client id: f_00004-0-4 loss: 0.864614  [  160/  306]
train() client id: f_00004-0-5 loss: 0.759355  [  192/  306]
train() client id: f_00004-0-6 loss: 0.923556  [  224/  306]
train() client id: f_00004-0-7 loss: 0.805607  [  256/  306]
train() client id: f_00004-0-8 loss: 0.683663  [  288/  306]
train() client id: f_00004-1-0 loss: 0.746513  [   32/  306]
train() client id: f_00004-1-1 loss: 0.786844  [   64/  306]
train() client id: f_00004-1-2 loss: 0.857069  [   96/  306]
train() client id: f_00004-1-3 loss: 0.893385  [  128/  306]
train() client id: f_00004-1-4 loss: 0.834619  [  160/  306]
train() client id: f_00004-1-5 loss: 0.609601  [  192/  306]
train() client id: f_00004-1-6 loss: 0.754265  [  224/  306]
train() client id: f_00004-1-7 loss: 0.822010  [  256/  306]
train() client id: f_00004-1-8 loss: 0.771656  [  288/  306]
train() client id: f_00004-2-0 loss: 0.825937  [   32/  306]
train() client id: f_00004-2-1 loss: 0.845761  [   64/  306]
train() client id: f_00004-2-2 loss: 0.747991  [   96/  306]
train() client id: f_00004-2-3 loss: 0.777153  [  128/  306]
train() client id: f_00004-2-4 loss: 0.859150  [  160/  306]
train() client id: f_00004-2-5 loss: 0.792844  [  192/  306]
train() client id: f_00004-2-6 loss: 0.763525  [  224/  306]
train() client id: f_00004-2-7 loss: 0.805203  [  256/  306]
train() client id: f_00004-2-8 loss: 0.699578  [  288/  306]
train() client id: f_00004-3-0 loss: 0.712352  [   32/  306]
train() client id: f_00004-3-1 loss: 0.695302  [   64/  306]
train() client id: f_00004-3-2 loss: 0.913568  [   96/  306]
train() client id: f_00004-3-3 loss: 0.796207  [  128/  306]
train() client id: f_00004-3-4 loss: 0.726449  [  160/  306]
train() client id: f_00004-3-5 loss: 0.791787  [  192/  306]
train() client id: f_00004-3-6 loss: 0.839117  [  224/  306]
train() client id: f_00004-3-7 loss: 0.765720  [  256/  306]
train() client id: f_00004-3-8 loss: 0.898970  [  288/  306]
train() client id: f_00004-4-0 loss: 0.684625  [   32/  306]
train() client id: f_00004-4-1 loss: 0.826409  [   64/  306]
train() client id: f_00004-4-2 loss: 0.837260  [   96/  306]
train() client id: f_00004-4-3 loss: 0.848051  [  128/  306]
train() client id: f_00004-4-4 loss: 0.695420  [  160/  306]
train() client id: f_00004-4-5 loss: 0.842182  [  192/  306]
train() client id: f_00004-4-6 loss: 0.873719  [  224/  306]
train() client id: f_00004-4-7 loss: 0.791431  [  256/  306]
train() client id: f_00004-4-8 loss: 0.698077  [  288/  306]
train() client id: f_00004-5-0 loss: 0.764625  [   32/  306]
train() client id: f_00004-5-1 loss: 0.736175  [   64/  306]
train() client id: f_00004-5-2 loss: 0.760718  [   96/  306]
train() client id: f_00004-5-3 loss: 0.746165  [  128/  306]
train() client id: f_00004-5-4 loss: 0.839958  [  160/  306]
train() client id: f_00004-5-5 loss: 0.823220  [  192/  306]
train() client id: f_00004-5-6 loss: 0.790687  [  224/  306]
train() client id: f_00004-5-7 loss: 0.822418  [  256/  306]
train() client id: f_00004-5-8 loss: 0.840481  [  288/  306]
train() client id: f_00004-6-0 loss: 0.777299  [   32/  306]
train() client id: f_00004-6-1 loss: 0.865268  [   64/  306]
train() client id: f_00004-6-2 loss: 0.739640  [   96/  306]
train() client id: f_00004-6-3 loss: 0.724668  [  128/  306]
train() client id: f_00004-6-4 loss: 0.754019  [  160/  306]
train() client id: f_00004-6-5 loss: 0.808908  [  192/  306]
train() client id: f_00004-6-6 loss: 0.839698  [  224/  306]
train() client id: f_00004-6-7 loss: 0.788910  [  256/  306]
train() client id: f_00004-6-8 loss: 0.787957  [  288/  306]
train() client id: f_00004-7-0 loss: 0.792801  [   32/  306]
train() client id: f_00004-7-1 loss: 0.745494  [   64/  306]
train() client id: f_00004-7-2 loss: 0.752890  [   96/  306]
train() client id: f_00004-7-3 loss: 0.717004  [  128/  306]
train() client id: f_00004-7-4 loss: 0.745368  [  160/  306]
train() client id: f_00004-7-5 loss: 0.749987  [  192/  306]
train() client id: f_00004-7-6 loss: 0.801188  [  224/  306]
train() client id: f_00004-7-7 loss: 0.890846  [  256/  306]
train() client id: f_00004-7-8 loss: 0.895290  [  288/  306]
train() client id: f_00004-8-0 loss: 0.704985  [   32/  306]
train() client id: f_00004-8-1 loss: 0.741500  [   64/  306]
train() client id: f_00004-8-2 loss: 0.738691  [   96/  306]
train() client id: f_00004-8-3 loss: 0.923790  [  128/  306]
train() client id: f_00004-8-4 loss: 0.844574  [  160/  306]
train() client id: f_00004-8-5 loss: 0.965306  [  192/  306]
train() client id: f_00004-8-6 loss: 0.759460  [  224/  306]
train() client id: f_00004-8-7 loss: 0.791212  [  256/  306]
train() client id: f_00004-8-8 loss: 0.706989  [  288/  306]
train() client id: f_00004-9-0 loss: 0.710720  [   32/  306]
train() client id: f_00004-9-1 loss: 0.947486  [   64/  306]
train() client id: f_00004-9-2 loss: 0.914895  [   96/  306]
train() client id: f_00004-9-3 loss: 0.696016  [  128/  306]
train() client id: f_00004-9-4 loss: 0.758399  [  160/  306]
train() client id: f_00004-9-5 loss: 0.696133  [  192/  306]
train() client id: f_00004-9-6 loss: 0.771710  [  224/  306]
train() client id: f_00004-9-7 loss: 0.864488  [  256/  306]
train() client id: f_00004-9-8 loss: 0.799308  [  288/  306]
train() client id: f_00004-10-0 loss: 0.692193  [   32/  306]
train() client id: f_00004-10-1 loss: 0.669233  [   64/  306]
train() client id: f_00004-10-2 loss: 0.860394  [   96/  306]
train() client id: f_00004-10-3 loss: 0.838198  [  128/  306]
train() client id: f_00004-10-4 loss: 0.761746  [  160/  306]
train() client id: f_00004-10-5 loss: 0.815438  [  192/  306]
train() client id: f_00004-10-6 loss: 0.917556  [  224/  306]
train() client id: f_00004-10-7 loss: 0.734037  [  256/  306]
train() client id: f_00004-10-8 loss: 0.839910  [  288/  306]
train() client id: f_00004-11-0 loss: 0.754005  [   32/  306]
train() client id: f_00004-11-1 loss: 0.813638  [   64/  306]
train() client id: f_00004-11-2 loss: 0.822769  [   96/  306]
train() client id: f_00004-11-3 loss: 0.822641  [  128/  306]
train() client id: f_00004-11-4 loss: 0.845334  [  160/  306]
train() client id: f_00004-11-5 loss: 0.734272  [  192/  306]
train() client id: f_00004-11-6 loss: 0.746903  [  224/  306]
train() client id: f_00004-11-7 loss: 0.786869  [  256/  306]
train() client id: f_00004-11-8 loss: 0.755755  [  288/  306]
train() client id: f_00005-0-0 loss: 0.471404  [   32/  146]
train() client id: f_00005-0-1 loss: 0.612868  [   64/  146]
train() client id: f_00005-0-2 loss: 0.621337  [   96/  146]
train() client id: f_00005-0-3 loss: 0.372577  [  128/  146]
train() client id: f_00005-1-0 loss: 0.355166  [   32/  146]
train() client id: f_00005-1-1 loss: 0.504379  [   64/  146]
train() client id: f_00005-1-2 loss: 0.669926  [   96/  146]
train() client id: f_00005-1-3 loss: 0.503166  [  128/  146]
train() client id: f_00005-2-0 loss: 0.557193  [   32/  146]
train() client id: f_00005-2-1 loss: 0.736316  [   64/  146]
train() client id: f_00005-2-2 loss: 0.271059  [   96/  146]
train() client id: f_00005-2-3 loss: 0.423189  [  128/  146]
train() client id: f_00005-3-0 loss: 0.949472  [   32/  146]
train() client id: f_00005-3-1 loss: 0.303638  [   64/  146]
train() client id: f_00005-3-2 loss: 0.412562  [   96/  146]
train() client id: f_00005-3-3 loss: 0.301223  [  128/  146]
train() client id: f_00005-4-0 loss: 0.492070  [   32/  146]
train() client id: f_00005-4-1 loss: 0.569874  [   64/  146]
train() client id: f_00005-4-2 loss: 0.622513  [   96/  146]
train() client id: f_00005-4-3 loss: 0.531373  [  128/  146]
train() client id: f_00005-5-0 loss: 0.461999  [   32/  146]
train() client id: f_00005-5-1 loss: 0.430739  [   64/  146]
train() client id: f_00005-5-2 loss: 0.604438  [   96/  146]
train() client id: f_00005-5-3 loss: 0.564019  [  128/  146]
train() client id: f_00005-6-0 loss: 0.236557  [   32/  146]
train() client id: f_00005-6-1 loss: 0.749423  [   64/  146]
train() client id: f_00005-6-2 loss: 0.604619  [   96/  146]
train() client id: f_00005-6-3 loss: 0.436654  [  128/  146]
train() client id: f_00005-7-0 loss: 0.632040  [   32/  146]
train() client id: f_00005-7-1 loss: 0.432300  [   64/  146]
train() client id: f_00005-7-2 loss: 0.290643  [   96/  146]
train() client id: f_00005-7-3 loss: 0.712795  [  128/  146]
train() client id: f_00005-8-0 loss: 0.371358  [   32/  146]
train() client id: f_00005-8-1 loss: 0.464881  [   64/  146]
train() client id: f_00005-8-2 loss: 0.514013  [   96/  146]
train() client id: f_00005-8-3 loss: 0.401355  [  128/  146]
train() client id: f_00005-9-0 loss: 0.477202  [   32/  146]
train() client id: f_00005-9-1 loss: 0.286115  [   64/  146]
train() client id: f_00005-9-2 loss: 0.820677  [   96/  146]
train() client id: f_00005-9-3 loss: 0.544025  [  128/  146]
train() client id: f_00005-10-0 loss: 0.321507  [   32/  146]
train() client id: f_00005-10-1 loss: 0.348184  [   64/  146]
train() client id: f_00005-10-2 loss: 0.456876  [   96/  146]
train() client id: f_00005-10-3 loss: 0.935878  [  128/  146]
train() client id: f_00005-11-0 loss: 0.352607  [   32/  146]
train() client id: f_00005-11-1 loss: 0.809846  [   64/  146]
train() client id: f_00005-11-2 loss: 0.580867  [   96/  146]
train() client id: f_00005-11-3 loss: 0.329516  [  128/  146]
train() client id: f_00006-0-0 loss: 0.547825  [   32/   54]
train() client id: f_00006-1-0 loss: 0.591191  [   32/   54]
train() client id: f_00006-2-0 loss: 0.590802  [   32/   54]
train() client id: f_00006-3-0 loss: 0.517884  [   32/   54]
train() client id: f_00006-4-0 loss: 0.549031  [   32/   54]
train() client id: f_00006-5-0 loss: 0.592789  [   32/   54]
train() client id: f_00006-6-0 loss: 0.501119  [   32/   54]
train() client id: f_00006-7-0 loss: 0.584125  [   32/   54]
train() client id: f_00006-8-0 loss: 0.549005  [   32/   54]
train() client id: f_00006-9-0 loss: 0.544406  [   32/   54]
train() client id: f_00006-10-0 loss: 0.503623  [   32/   54]
train() client id: f_00006-11-0 loss: 0.563048  [   32/   54]
train() client id: f_00007-0-0 loss: 0.415065  [   32/  179]
train() client id: f_00007-0-1 loss: 0.702245  [   64/  179]
train() client id: f_00007-0-2 loss: 0.470797  [   96/  179]
train() client id: f_00007-0-3 loss: 0.431244  [  128/  179]
train() client id: f_00007-0-4 loss: 0.828823  [  160/  179]
train() client id: f_00007-1-0 loss: 0.544184  [   32/  179]
train() client id: f_00007-1-1 loss: 0.448116  [   64/  179]
train() client id: f_00007-1-2 loss: 0.581170  [   96/  179]
train() client id: f_00007-1-3 loss: 0.581248  [  128/  179]
train() client id: f_00007-1-4 loss: 0.641132  [  160/  179]
train() client id: f_00007-2-0 loss: 0.512735  [   32/  179]
train() client id: f_00007-2-1 loss: 0.476076  [   64/  179]
train() client id: f_00007-2-2 loss: 0.499289  [   96/  179]
train() client id: f_00007-2-3 loss: 0.518136  [  128/  179]
train() client id: f_00007-2-4 loss: 0.552885  [  160/  179]
train() client id: f_00007-3-0 loss: 0.458109  [   32/  179]
train() client id: f_00007-3-1 loss: 0.504766  [   64/  179]
train() client id: f_00007-3-2 loss: 0.561069  [   96/  179]
train() client id: f_00007-3-3 loss: 0.477869  [  128/  179]
train() client id: f_00007-3-4 loss: 0.568454  [  160/  179]
train() client id: f_00007-4-0 loss: 0.403141  [   32/  179]
train() client id: f_00007-4-1 loss: 0.661447  [   64/  179]
train() client id: f_00007-4-2 loss: 0.534983  [   96/  179]
train() client id: f_00007-4-3 loss: 0.733005  [  128/  179]
train() client id: f_00007-4-4 loss: 0.470541  [  160/  179]
train() client id: f_00007-5-0 loss: 0.570857  [   32/  179]
train() client id: f_00007-5-1 loss: 0.680745  [   64/  179]
train() client id: f_00007-5-2 loss: 0.374514  [   96/  179]
train() client id: f_00007-5-3 loss: 0.643714  [  128/  179]
train() client id: f_00007-5-4 loss: 0.494234  [  160/  179]
train() client id: f_00007-6-0 loss: 0.699896  [   32/  179]
train() client id: f_00007-6-1 loss: 0.412093  [   64/  179]
train() client id: f_00007-6-2 loss: 0.738988  [   96/  179]
train() client id: f_00007-6-3 loss: 0.382652  [  128/  179]
train() client id: f_00007-6-4 loss: 0.480576  [  160/  179]
train() client id: f_00007-7-0 loss: 0.595408  [   32/  179]
train() client id: f_00007-7-1 loss: 0.373427  [   64/  179]
train() client id: f_00007-7-2 loss: 0.558519  [   96/  179]
train() client id: f_00007-7-3 loss: 0.634131  [  128/  179]
train() client id: f_00007-7-4 loss: 0.359881  [  160/  179]
train() client id: f_00007-8-0 loss: 0.393047  [   32/  179]
train() client id: f_00007-8-1 loss: 0.702254  [   64/  179]
train() client id: f_00007-8-2 loss: 0.574323  [   96/  179]
train() client id: f_00007-8-3 loss: 0.565418  [  128/  179]
train() client id: f_00007-8-4 loss: 0.395535  [  160/  179]
train() client id: f_00007-9-0 loss: 0.742493  [   32/  179]
train() client id: f_00007-9-1 loss: 0.437985  [   64/  179]
train() client id: f_00007-9-2 loss: 0.489746  [   96/  179]
train() client id: f_00007-9-3 loss: 0.486376  [  128/  179]
train() client id: f_00007-9-4 loss: 0.564856  [  160/  179]
train() client id: f_00007-10-0 loss: 0.463384  [   32/  179]
train() client id: f_00007-10-1 loss: 0.472691  [   64/  179]
train() client id: f_00007-10-2 loss: 0.723133  [   96/  179]
train() client id: f_00007-10-3 loss: 0.501843  [  128/  179]
train() client id: f_00007-10-4 loss: 0.557798  [  160/  179]
train() client id: f_00007-11-0 loss: 0.649387  [   32/  179]
train() client id: f_00007-11-1 loss: 0.551795  [   64/  179]
train() client id: f_00007-11-2 loss: 0.545872  [   96/  179]
train() client id: f_00007-11-3 loss: 0.364365  [  128/  179]
train() client id: f_00007-11-4 loss: 0.599626  [  160/  179]
train() client id: f_00008-0-0 loss: 0.637780  [   32/  130]
train() client id: f_00008-0-1 loss: 0.680383  [   64/  130]
train() client id: f_00008-0-2 loss: 0.750423  [   96/  130]
train() client id: f_00008-0-3 loss: 0.747695  [  128/  130]
train() client id: f_00008-1-0 loss: 0.702597  [   32/  130]
train() client id: f_00008-1-1 loss: 0.764522  [   64/  130]
train() client id: f_00008-1-2 loss: 0.637304  [   96/  130]
train() client id: f_00008-1-3 loss: 0.686383  [  128/  130]
train() client id: f_00008-2-0 loss: 0.637241  [   32/  130]
train() client id: f_00008-2-1 loss: 0.734048  [   64/  130]
train() client id: f_00008-2-2 loss: 0.648954  [   96/  130]
train() client id: f_00008-2-3 loss: 0.763620  [  128/  130]
train() client id: f_00008-3-0 loss: 0.696596  [   32/  130]
train() client id: f_00008-3-1 loss: 0.673834  [   64/  130]
train() client id: f_00008-3-2 loss: 0.711323  [   96/  130]
train() client id: f_00008-3-3 loss: 0.716829  [  128/  130]
train() client id: f_00008-4-0 loss: 0.670136  [   32/  130]
train() client id: f_00008-4-1 loss: 0.686672  [   64/  130]
train() client id: f_00008-4-2 loss: 0.712790  [   96/  130]
train() client id: f_00008-4-3 loss: 0.737941  [  128/  130]
train() client id: f_00008-5-0 loss: 0.657306  [   32/  130]
train() client id: f_00008-5-1 loss: 0.651205  [   64/  130]
train() client id: f_00008-5-2 loss: 0.808581  [   96/  130]
train() client id: f_00008-5-3 loss: 0.685321  [  128/  130]
train() client id: f_00008-6-0 loss: 0.677191  [   32/  130]
train() client id: f_00008-6-1 loss: 0.717171  [   64/  130]
train() client id: f_00008-6-2 loss: 0.710199  [   96/  130]
train() client id: f_00008-6-3 loss: 0.686754  [  128/  130]
train() client id: f_00008-7-0 loss: 0.642592  [   32/  130]
train() client id: f_00008-7-1 loss: 0.749267  [   64/  130]
train() client id: f_00008-7-2 loss: 0.776841  [   96/  130]
train() client id: f_00008-7-3 loss: 0.587066  [  128/  130]
train() client id: f_00008-8-0 loss: 0.737960  [   32/  130]
train() client id: f_00008-8-1 loss: 0.847750  [   64/  130]
train() client id: f_00008-8-2 loss: 0.527704  [   96/  130]
train() client id: f_00008-8-3 loss: 0.675961  [  128/  130]
train() client id: f_00008-9-0 loss: 0.625088  [   32/  130]
train() client id: f_00008-9-1 loss: 0.672032  [   64/  130]
train() client id: f_00008-9-2 loss: 0.751817  [   96/  130]
train() client id: f_00008-9-3 loss: 0.739683  [  128/  130]
train() client id: f_00008-10-0 loss: 0.705139  [   32/  130]
train() client id: f_00008-10-1 loss: 0.805680  [   64/  130]
train() client id: f_00008-10-2 loss: 0.646220  [   96/  130]
train() client id: f_00008-10-3 loss: 0.606223  [  128/  130]
train() client id: f_00008-11-0 loss: 0.735186  [   32/  130]
train() client id: f_00008-11-1 loss: 0.681006  [   64/  130]
train() client id: f_00008-11-2 loss: 0.756693  [   96/  130]
train() client id: f_00008-11-3 loss: 0.562327  [  128/  130]
train() client id: f_00009-0-0 loss: 1.054773  [   32/  118]
train() client id: f_00009-0-1 loss: 0.966777  [   64/  118]
train() client id: f_00009-0-2 loss: 1.044083  [   96/  118]
train() client id: f_00009-1-0 loss: 1.030958  [   32/  118]
train() client id: f_00009-1-1 loss: 0.980974  [   64/  118]
train() client id: f_00009-1-2 loss: 1.040854  [   96/  118]
train() client id: f_00009-2-0 loss: 0.991701  [   32/  118]
train() client id: f_00009-2-1 loss: 0.963070  [   64/  118]
train() client id: f_00009-2-2 loss: 0.993262  [   96/  118]
train() client id: f_00009-3-0 loss: 0.902933  [   32/  118]
train() client id: f_00009-3-1 loss: 0.911066  [   64/  118]
train() client id: f_00009-3-2 loss: 0.998639  [   96/  118]
train() client id: f_00009-4-0 loss: 0.816349  [   32/  118]
train() client id: f_00009-4-1 loss: 0.867382  [   64/  118]
train() client id: f_00009-4-2 loss: 0.928122  [   96/  118]
train() client id: f_00009-5-0 loss: 0.830347  [   32/  118]
train() client id: f_00009-5-1 loss: 0.837201  [   64/  118]
train() client id: f_00009-5-2 loss: 0.841003  [   96/  118]
train() client id: f_00009-6-0 loss: 0.762956  [   32/  118]
train() client id: f_00009-6-1 loss: 1.069016  [   64/  118]
train() client id: f_00009-6-2 loss: 0.826377  [   96/  118]
train() client id: f_00009-7-0 loss: 0.891991  [   32/  118]
train() client id: f_00009-7-1 loss: 0.805164  [   64/  118]
train() client id: f_00009-7-2 loss: 0.841417  [   96/  118]
train() client id: f_00009-8-0 loss: 0.698156  [   32/  118]
train() client id: f_00009-8-1 loss: 0.894741  [   64/  118]
train() client id: f_00009-8-2 loss: 0.884618  [   96/  118]
train() client id: f_00009-9-0 loss: 0.843854  [   32/  118]
train() client id: f_00009-9-1 loss: 0.888063  [   64/  118]
train() client id: f_00009-9-2 loss: 0.868585  [   96/  118]
train() client id: f_00009-10-0 loss: 0.833873  [   32/  118]
train() client id: f_00009-10-1 loss: 0.795051  [   64/  118]
train() client id: f_00009-10-2 loss: 0.901738  [   96/  118]
train() client id: f_00009-11-0 loss: 0.853353  [   32/  118]
train() client id: f_00009-11-1 loss: 0.949406  [   64/  118]
train() client id: f_00009-11-2 loss: 0.764614  [   96/  118]
At round 19 accuracy: 0.6339522546419099
At round 19 training accuracy: 0.5828303152246814
At round 19 training loss: 0.8298337929213965
update_location
xs = -3.905658 4.200318 115.009024 18.811294 0.979296 3.956410 -77.443192 -56.324852 99.663977 -42.060879 
ys = 107.587959 90.555839 1.320614 -77.455176 69.350187 52.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 6.288573831882571
ys mean: 26.394142535528708
dists_uav = 146.936800 134.974081 152.410038 127.879510 121.698017 113.159126 126.508255 114.774410 142.272901 108.559336 
uav_gains = -104.181726 -103.257358 -104.580954 -102.670545 -102.132348 -101.342333 -102.553420 -101.496241 -103.830302 -100.891731 
uav_gains_db_mean: -102.6936957982465
dists_bs = 183.895777 198.099900 338.038109 318.271007 205.257389 216.690513 202.652328 210.764431 316.583035 216.594790 
bs_gains = -102.975068 -103.879818 -110.378098 -109.645377 -104.311426 -104.970576 -104.156104 -104.633384 -109.580713 -104.965203 
bs_gains_db_mean: -105.94957678209816
Round 20
-------------------------------
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.3523103  17.38275937  8.22567535  2.94704545 20.05064182  9.65872482
  3.66105067 11.77706151  8.67193539  7.83910309]
obj_prev = 98.5663077651687
eta_min = 7.590997972408306e-12	eta_max = 0.9222942349256907
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 22.912452031604698	eta = 0.9090909090909091
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 40.06758095330687	eta = 0.519859231611392
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 31.832010204673995	eta = 0.654357098812908
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.352361561329435	eta = 0.6862563825495306
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.27833469058294	eta = 0.6879341965062457
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.27813585075753	eta = 0.6879387142452571
eta = 0.6879387142452571
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [0.03086617 0.06491693 0.03037623 0.01053369 0.07496069 0.03576556
 0.01322836 0.04384955 0.03184604 0.02890641]
ene_total = [2.63093078 4.95082043 2.60801881 1.20448716 5.64850744 2.98715498
 1.38606222 3.45675966 2.88726353 2.51813083]
ti_comp = [0.34802257 0.34915355 0.34647497 0.3533723  0.34752642 0.34490443
 0.35375633 0.35704591 0.32032243 0.34492651]
ti_coms = [0.07621728 0.0750863  0.07776488 0.07086754 0.07671342 0.07933541
 0.07048351 0.06719393 0.10391742 0.07931333]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.51744596e-05 1.40255892e-04 1.45927700e-05 5.85001217e-07
 2.17974172e-04 2.40368430e-05 1.15608361e-06 4.13358984e-05
 1.96730538e-05 1.26884708e-05]
ene_total = [0.52203848 0.52285762 0.53257769 0.48447181 0.53929285 0.54395903
 0.48188571 0.46214562 0.71169684 0.54303237]
optimize_network iter = 0 obj = 5.343958006285102
eta = 0.6879387142452571
freqs = [4.43450629e+07 9.29633038e+07 4.38361048e+07 1.49045232e+07
 1.07848904e+08 5.18485023e+07 1.86969890e+07 6.14060420e+07
 4.97093568e+07 4.19022717e+07]
eta_min = 0.6879387142452722	eta_max = 0.6879387142452453
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 0.030579617084646203	eta = 0.9090909090909091
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 18.258827212431594	eta = 0.001522532174257359
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.8403927678865823	eta = 0.015105282079029619
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.8010043726548246	eta = 0.015435638201229906
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.800997812209442	eta = 0.015435694428206238
eta = 0.015435694428206238
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.69209568e-04 1.56398576e-03 1.62723179e-04 6.52331649e-06
 2.43061804e-03 2.68033519e-04 1.28914250e-05 4.60934337e-04
 2.19373145e-04 1.41488442e-04]
ene_total = [0.16879936 0.19656835 0.17201187 0.15368379 0.21887026 0.17769628
 0.15298971 0.15556983 0.22990166 0.17490671]
ti_comp = [0.34802257 0.34915355 0.34647497 0.3533723  0.34752642 0.34490443
 0.35375633 0.35704591 0.32032243 0.34492651]
ti_coms = [0.07621728 0.0750863  0.07776488 0.07086754 0.07671342 0.07933541
 0.07048351 0.06719393 0.10391742 0.07931333]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.51744596e-05 1.40255892e-04 1.45927700e-05 5.85001217e-07
 2.17974172e-04 2.40368430e-05 1.15608361e-06 4.13358984e-05
 1.96730538e-05 1.26884708e-05]
ene_total = [0.52203848 0.52285762 0.53257769 0.48447181 0.53929285 0.54395903
 0.48188571 0.46214562 0.71169684 0.54303237]
optimize_network iter = 1 obj = 5.343958006285359
eta = 0.6879387142452722
freqs = [4.43450629e+07 9.29633038e+07 4.38361048e+07 1.49045232e+07
 1.07848904e+08 5.18485023e+07 1.86969890e+07 6.14060420e+07
 4.97093568e+07 4.19022717e+07]
Done!
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.48666236e-05 1.37410596e-04 1.42967344e-05 5.73133613e-07
 2.13552247e-04 2.35492206e-05 1.13263077e-06 4.04973394e-05
 1.92739572e-05 1.24310667e-05]
ene_total = [0.00763659 0.00764604 0.00779078 0.00708733 0.00788489 0.00795709
 0.00704948 0.00675989 0.01041102 0.00794376]
At round 20 energy consumption: 0.07816688572627806
At round 20 eta: 0.6879387142452722
At round 20 a_n: 21.33168591686803
At round 20 local rounds: 12.248478257840025
At round 20 global rounds: 68.35736084749131
gradient difference: 0.4155743718147278
train() client id: f_00000-0-0 loss: 1.055637  [   32/  126]
train() client id: f_00000-0-1 loss: 1.105651  [   64/  126]
train() client id: f_00000-0-2 loss: 1.142888  [   96/  126]
train() client id: f_00000-1-0 loss: 1.059897  [   32/  126]
train() client id: f_00000-1-1 loss: 0.972926  [   64/  126]
train() client id: f_00000-1-2 loss: 1.074313  [   96/  126]
train() client id: f_00000-2-0 loss: 0.989778  [   32/  126]
train() client id: f_00000-2-1 loss: 0.974432  [   64/  126]
train() client id: f_00000-2-2 loss: 0.908288  [   96/  126]
train() client id: f_00000-3-0 loss: 1.071071  [   32/  126]
train() client id: f_00000-3-1 loss: 0.882535  [   64/  126]
train() client id: f_00000-3-2 loss: 0.802907  [   96/  126]
train() client id: f_00000-4-0 loss: 1.007116  [   32/  126]
train() client id: f_00000-4-1 loss: 0.792631  [   64/  126]
train() client id: f_00000-4-2 loss: 0.887029  [   96/  126]
train() client id: f_00000-5-0 loss: 0.908493  [   32/  126]
train() client id: f_00000-5-1 loss: 0.759795  [   64/  126]
train() client id: f_00000-5-2 loss: 0.802454  [   96/  126]
train() client id: f_00000-6-0 loss: 0.740098  [   32/  126]
train() client id: f_00000-6-1 loss: 0.880992  [   64/  126]
train() client id: f_00000-6-2 loss: 0.852471  [   96/  126]
train() client id: f_00000-7-0 loss: 0.768841  [   32/  126]
train() client id: f_00000-7-1 loss: 0.750546  [   64/  126]
train() client id: f_00000-7-2 loss: 0.762926  [   96/  126]
train() client id: f_00000-8-0 loss: 0.695178  [   32/  126]
train() client id: f_00000-8-1 loss: 0.845782  [   64/  126]
train() client id: f_00000-8-2 loss: 0.742998  [   96/  126]
train() client id: f_00000-9-0 loss: 0.814894  [   32/  126]
train() client id: f_00000-9-1 loss: 0.752170  [   64/  126]
train() client id: f_00000-9-2 loss: 0.767622  [   96/  126]
train() client id: f_00000-10-0 loss: 0.842983  [   32/  126]
train() client id: f_00000-10-1 loss: 0.786133  [   64/  126]
train() client id: f_00000-10-2 loss: 0.754034  [   96/  126]
train() client id: f_00000-11-0 loss: 0.806451  [   32/  126]
train() client id: f_00000-11-1 loss: 0.715315  [   64/  126]
train() client id: f_00000-11-2 loss: 0.781575  [   96/  126]
train() client id: f_00001-0-0 loss: 0.593150  [   32/  265]
train() client id: f_00001-0-1 loss: 0.455474  [   64/  265]
train() client id: f_00001-0-2 loss: 0.479382  [   96/  265]
train() client id: f_00001-0-3 loss: 0.536052  [  128/  265]
train() client id: f_00001-0-4 loss: 0.416215  [  160/  265]
train() client id: f_00001-0-5 loss: 0.559104  [  192/  265]
train() client id: f_00001-0-6 loss: 0.626233  [  224/  265]
train() client id: f_00001-0-7 loss: 0.596860  [  256/  265]
train() client id: f_00001-1-0 loss: 0.563934  [   32/  265]
train() client id: f_00001-1-1 loss: 0.476112  [   64/  265]
train() client id: f_00001-1-2 loss: 0.483002  [   96/  265]
train() client id: f_00001-1-3 loss: 0.593312  [  128/  265]
train() client id: f_00001-1-4 loss: 0.474284  [  160/  265]
train() client id: f_00001-1-5 loss: 0.591449  [  192/  265]
train() client id: f_00001-1-6 loss: 0.590546  [  224/  265]
train() client id: f_00001-1-7 loss: 0.470761  [  256/  265]
train() client id: f_00001-2-0 loss: 0.509832  [   32/  265]
train() client id: f_00001-2-1 loss: 0.477283  [   64/  265]
train() client id: f_00001-2-2 loss: 0.508261  [   96/  265]
train() client id: f_00001-2-3 loss: 0.620263  [  128/  265]
train() client id: f_00001-2-4 loss: 0.579572  [  160/  265]
train() client id: f_00001-2-5 loss: 0.444664  [  192/  265]
train() client id: f_00001-2-6 loss: 0.447258  [  224/  265]
train() client id: f_00001-2-7 loss: 0.512005  [  256/  265]
train() client id: f_00001-3-0 loss: 0.539096  [   32/  265]
train() client id: f_00001-3-1 loss: 0.579427  [   64/  265]
train() client id: f_00001-3-2 loss: 0.525566  [   96/  265]
train() client id: f_00001-3-3 loss: 0.449930  [  128/  265]
train() client id: f_00001-3-4 loss: 0.466956  [  160/  265]
train() client id: f_00001-3-5 loss: 0.423535  [  192/  265]
train() client id: f_00001-3-6 loss: 0.501648  [  224/  265]
train() client id: f_00001-3-7 loss: 0.713813  [  256/  265]
train() client id: f_00001-4-0 loss: 0.491294  [   32/  265]
train() client id: f_00001-4-1 loss: 0.511947  [   64/  265]
train() client id: f_00001-4-2 loss: 0.560968  [   96/  265]
train() client id: f_00001-4-3 loss: 0.539527  [  128/  265]
train() client id: f_00001-4-4 loss: 0.513675  [  160/  265]
train() client id: f_00001-4-5 loss: 0.503383  [  192/  265]
train() client id: f_00001-4-6 loss: 0.448515  [  224/  265]
train() client id: f_00001-4-7 loss: 0.576002  [  256/  265]
train() client id: f_00001-5-0 loss: 0.508071  [   32/  265]
train() client id: f_00001-5-1 loss: 0.584517  [   64/  265]
train() client id: f_00001-5-2 loss: 0.572921  [   96/  265]
train() client id: f_00001-5-3 loss: 0.456425  [  128/  265]
train() client id: f_00001-5-4 loss: 0.428488  [  160/  265]
train() client id: f_00001-5-5 loss: 0.632921  [  192/  265]
train() client id: f_00001-5-6 loss: 0.447273  [  224/  265]
train() client id: f_00001-5-7 loss: 0.520025  [  256/  265]
train() client id: f_00001-6-0 loss: 0.492548  [   32/  265]
train() client id: f_00001-6-1 loss: 0.542870  [   64/  265]
train() client id: f_00001-6-2 loss: 0.594403  [   96/  265]
train() client id: f_00001-6-3 loss: 0.555772  [  128/  265]
train() client id: f_00001-6-4 loss: 0.517955  [  160/  265]
train() client id: f_00001-6-5 loss: 0.514959  [  192/  265]
train() client id: f_00001-6-6 loss: 0.449869  [  224/  265]
train() client id: f_00001-6-7 loss: 0.517049  [  256/  265]
train() client id: f_00001-7-0 loss: 0.686780  [   32/  265]
train() client id: f_00001-7-1 loss: 0.438631  [   64/  265]
train() client id: f_00001-7-2 loss: 0.545460  [   96/  265]
train() client id: f_00001-7-3 loss: 0.518851  [  128/  265]
train() client id: f_00001-7-4 loss: 0.525831  [  160/  265]
train() client id: f_00001-7-5 loss: 0.454367  [  192/  265]
train() client id: f_00001-7-6 loss: 0.426396  [  224/  265]
train() client id: f_00001-7-7 loss: 0.590975  [  256/  265]
train() client id: f_00001-8-0 loss: 0.641533  [   32/  265]
train() client id: f_00001-8-1 loss: 0.436529  [   64/  265]
train() client id: f_00001-8-2 loss: 0.587897  [   96/  265]
train() client id: f_00001-8-3 loss: 0.515179  [  128/  265]
train() client id: f_00001-8-4 loss: 0.539421  [  160/  265]
train() client id: f_00001-8-5 loss: 0.435466  [  192/  265]
train() client id: f_00001-8-6 loss: 0.519485  [  224/  265]
train() client id: f_00001-8-7 loss: 0.465700  [  256/  265]
train() client id: f_00001-9-0 loss: 0.513734  [   32/  265]
train() client id: f_00001-9-1 loss: 0.470291  [   64/  265]
train() client id: f_00001-9-2 loss: 0.523876  [   96/  265]
train() client id: f_00001-9-3 loss: 0.539991  [  128/  265]
train() client id: f_00001-9-4 loss: 0.585715  [  160/  265]
train() client id: f_00001-9-5 loss: 0.562667  [  192/  265]
train() client id: f_00001-9-6 loss: 0.553772  [  224/  265]
train() client id: f_00001-9-7 loss: 0.442195  [  256/  265]
train() client id: f_00001-10-0 loss: 0.555649  [   32/  265]
train() client id: f_00001-10-1 loss: 0.505754  [   64/  265]
train() client id: f_00001-10-2 loss: 0.438577  [   96/  265]
train() client id: f_00001-10-3 loss: 0.559890  [  128/  265]
train() client id: f_00001-10-4 loss: 0.556971  [  160/  265]
train() client id: f_00001-10-5 loss: 0.439802  [  192/  265]
train() client id: f_00001-10-6 loss: 0.602451  [  224/  265]
train() client id: f_00001-10-7 loss: 0.509084  [  256/  265]
train() client id: f_00001-11-0 loss: 0.424345  [   32/  265]
train() client id: f_00001-11-1 loss: 0.546423  [   64/  265]
train() client id: f_00001-11-2 loss: 0.449191  [   96/  265]
train() client id: f_00001-11-3 loss: 0.623371  [  128/  265]
train() client id: f_00001-11-4 loss: 0.552122  [  160/  265]
train() client id: f_00001-11-5 loss: 0.424265  [  192/  265]
train() client id: f_00001-11-6 loss: 0.514913  [  224/  265]
train() client id: f_00001-11-7 loss: 0.695768  [  256/  265]
train() client id: f_00002-0-0 loss: 1.218181  [   32/  124]
train() client id: f_00002-0-1 loss: 1.167769  [   64/  124]
train() client id: f_00002-0-2 loss: 0.942261  [   96/  124]
train() client id: f_00002-1-0 loss: 0.933296  [   32/  124]
train() client id: f_00002-1-1 loss: 0.950720  [   64/  124]
train() client id: f_00002-1-2 loss: 1.200380  [   96/  124]
train() client id: f_00002-2-0 loss: 0.876781  [   32/  124]
train() client id: f_00002-2-1 loss: 1.341344  [   64/  124]
train() client id: f_00002-2-2 loss: 0.864436  [   96/  124]
train() client id: f_00002-3-0 loss: 1.015755  [   32/  124]
train() client id: f_00002-3-1 loss: 1.037613  [   64/  124]
train() client id: f_00002-3-2 loss: 0.912346  [   96/  124]
train() client id: f_00002-4-0 loss: 0.739479  [   32/  124]
train() client id: f_00002-4-1 loss: 1.008456  [   64/  124]
train() client id: f_00002-4-2 loss: 1.028661  [   96/  124]
train() client id: f_00002-5-0 loss: 0.999538  [   32/  124]
train() client id: f_00002-5-1 loss: 0.947738  [   64/  124]
train() client id: f_00002-5-2 loss: 0.735059  [   96/  124]
train() client id: f_00002-6-0 loss: 1.001534  [   32/  124]
train() client id: f_00002-6-1 loss: 0.692901  [   64/  124]
train() client id: f_00002-6-2 loss: 0.969414  [   96/  124]
train() client id: f_00002-7-0 loss: 1.145123  [   32/  124]
train() client id: f_00002-7-1 loss: 0.751110  [   64/  124]
train() client id: f_00002-7-2 loss: 0.831683  [   96/  124]
train() client id: f_00002-8-0 loss: 0.846320  [   32/  124]
train() client id: f_00002-8-1 loss: 0.852081  [   64/  124]
train() client id: f_00002-8-2 loss: 0.953016  [   96/  124]
train() client id: f_00002-9-0 loss: 0.804030  [   32/  124]
train() client id: f_00002-9-1 loss: 0.817192  [   64/  124]
train() client id: f_00002-9-2 loss: 1.052947  [   96/  124]
train() client id: f_00002-10-0 loss: 0.742107  [   32/  124]
train() client id: f_00002-10-1 loss: 0.875094  [   64/  124]
train() client id: f_00002-10-2 loss: 0.705597  [   96/  124]
train() client id: f_00002-11-0 loss: 0.849628  [   32/  124]
train() client id: f_00002-11-1 loss: 0.725350  [   64/  124]
train() client id: f_00002-11-2 loss: 0.826360  [   96/  124]
train() client id: f_00003-0-0 loss: 0.780359  [   32/   43]
train() client id: f_00003-1-0 loss: 0.869628  [   32/   43]
train() client id: f_00003-2-0 loss: 0.944094  [   32/   43]
train() client id: f_00003-3-0 loss: 0.774626  [   32/   43]
train() client id: f_00003-4-0 loss: 0.902793  [   32/   43]
train() client id: f_00003-5-0 loss: 0.897038  [   32/   43]
train() client id: f_00003-6-0 loss: 0.859573  [   32/   43]
train() client id: f_00003-7-0 loss: 0.785141  [   32/   43]
train() client id: f_00003-8-0 loss: 0.894570  [   32/   43]
train() client id: f_00003-9-0 loss: 0.815023  [   32/   43]
train() client id: f_00003-10-0 loss: 0.774324  [   32/   43]
train() client id: f_00003-11-0 loss: 0.889835  [   32/   43]
train() client id: f_00004-0-0 loss: 0.958714  [   32/  306]
train() client id: f_00004-0-1 loss: 0.871015  [   64/  306]
train() client id: f_00004-0-2 loss: 0.772301  [   96/  306]
train() client id: f_00004-0-3 loss: 0.880030  [  128/  306]
train() client id: f_00004-0-4 loss: 0.944090  [  160/  306]
train() client id: f_00004-0-5 loss: 0.706568  [  192/  306]
train() client id: f_00004-0-6 loss: 0.837640  [  224/  306]
train() client id: f_00004-0-7 loss: 1.117807  [  256/  306]
train() client id: f_00004-0-8 loss: 0.886862  [  288/  306]
train() client id: f_00004-1-0 loss: 0.922406  [   32/  306]
train() client id: f_00004-1-1 loss: 0.911573  [   64/  306]
train() client id: f_00004-1-2 loss: 0.991199  [   96/  306]
train() client id: f_00004-1-3 loss: 0.912944  [  128/  306]
train() client id: f_00004-1-4 loss: 0.813218  [  160/  306]
train() client id: f_00004-1-5 loss: 0.869366  [  192/  306]
train() client id: f_00004-1-6 loss: 0.961749  [  224/  306]
train() client id: f_00004-1-7 loss: 0.785451  [  256/  306]
train() client id: f_00004-1-8 loss: 0.812892  [  288/  306]
train() client id: f_00004-2-0 loss: 0.829993  [   32/  306]
train() client id: f_00004-2-1 loss: 0.785929  [   64/  306]
train() client id: f_00004-2-2 loss: 0.839492  [   96/  306]
train() client id: f_00004-2-3 loss: 0.885898  [  128/  306]
train() client id: f_00004-2-4 loss: 0.896366  [  160/  306]
train() client id: f_00004-2-5 loss: 0.862702  [  192/  306]
train() client id: f_00004-2-6 loss: 0.956880  [  224/  306]
train() client id: f_00004-2-7 loss: 0.906624  [  256/  306]
train() client id: f_00004-2-8 loss: 0.927418  [  288/  306]
train() client id: f_00004-3-0 loss: 0.779450  [   32/  306]
train() client id: f_00004-3-1 loss: 0.866341  [   64/  306]
train() client id: f_00004-3-2 loss: 0.875828  [   96/  306]
train() client id: f_00004-3-3 loss: 0.923796  [  128/  306]
train() client id: f_00004-3-4 loss: 0.962872  [  160/  306]
train() client id: f_00004-3-5 loss: 0.859575  [  192/  306]
train() client id: f_00004-3-6 loss: 0.882977  [  224/  306]
train() client id: f_00004-3-7 loss: 0.822899  [  256/  306]
train() client id: f_00004-3-8 loss: 0.858948  [  288/  306]
train() client id: f_00004-4-0 loss: 0.927986  [   32/  306]
train() client id: f_00004-4-1 loss: 0.891500  [   64/  306]
train() client id: f_00004-4-2 loss: 1.041525  [   96/  306]
train() client id: f_00004-4-3 loss: 0.941611  [  128/  306]
train() client id: f_00004-4-4 loss: 0.762151  [  160/  306]
train() client id: f_00004-4-5 loss: 0.854992  [  192/  306]
train() client id: f_00004-4-6 loss: 0.823156  [  224/  306]
train() client id: f_00004-4-7 loss: 0.907765  [  256/  306]
train() client id: f_00004-4-8 loss: 0.803221  [  288/  306]
train() client id: f_00004-5-0 loss: 0.809187  [   32/  306]
train() client id: f_00004-5-1 loss: 1.031388  [   64/  306]
train() client id: f_00004-5-2 loss: 0.822080  [   96/  306]
train() client id: f_00004-5-3 loss: 0.867567  [  128/  306]
train() client id: f_00004-5-4 loss: 0.879923  [  160/  306]
train() client id: f_00004-5-5 loss: 0.924081  [  192/  306]
train() client id: f_00004-5-6 loss: 0.887895  [  224/  306]
train() client id: f_00004-5-7 loss: 0.952630  [  256/  306]
train() client id: f_00004-5-8 loss: 0.736370  [  288/  306]
train() client id: f_00004-6-0 loss: 0.975812  [   32/  306]
train() client id: f_00004-6-1 loss: 0.768192  [   64/  306]
train() client id: f_00004-6-2 loss: 0.861619  [   96/  306]
train() client id: f_00004-6-3 loss: 0.892088  [  128/  306]
train() client id: f_00004-6-4 loss: 0.802520  [  160/  306]
train() client id: f_00004-6-5 loss: 0.979461  [  192/  306]
train() client id: f_00004-6-6 loss: 0.829603  [  224/  306]
train() client id: f_00004-6-7 loss: 0.978454  [  256/  306]
train() client id: f_00004-6-8 loss: 0.893072  [  288/  306]
train() client id: f_00004-7-0 loss: 0.834523  [   32/  306]
train() client id: f_00004-7-1 loss: 0.941093  [   64/  306]
train() client id: f_00004-7-2 loss: 0.880551  [   96/  306]
train() client id: f_00004-7-3 loss: 1.008179  [  128/  306]
train() client id: f_00004-7-4 loss: 0.950672  [  160/  306]
train() client id: f_00004-7-5 loss: 0.793536  [  192/  306]
train() client id: f_00004-7-6 loss: 0.828424  [  224/  306]
train() client id: f_00004-7-7 loss: 0.783153  [  256/  306]
train() client id: f_00004-7-8 loss: 0.904134  [  288/  306]
train() client id: f_00004-8-0 loss: 0.737232  [   32/  306]
train() client id: f_00004-8-1 loss: 0.943061  [   64/  306]
train() client id: f_00004-8-2 loss: 0.861178  [   96/  306]
train() client id: f_00004-8-3 loss: 0.861067  [  128/  306]
train() client id: f_00004-8-4 loss: 0.844661  [  160/  306]
train() client id: f_00004-8-5 loss: 0.824603  [  192/  306]
train() client id: f_00004-8-6 loss: 0.903273  [  224/  306]
train() client id: f_00004-8-7 loss: 1.036231  [  256/  306]
train() client id: f_00004-8-8 loss: 0.810328  [  288/  306]
train() client id: f_00004-9-0 loss: 0.910568  [   32/  306]
train() client id: f_00004-9-1 loss: 0.804706  [   64/  306]
train() client id: f_00004-9-2 loss: 0.861925  [   96/  306]
train() client id: f_00004-9-3 loss: 0.861276  [  128/  306]
train() client id: f_00004-9-4 loss: 0.920839  [  160/  306]
train() client id: f_00004-9-5 loss: 0.885099  [  192/  306]
train() client id: f_00004-9-6 loss: 0.867855  [  224/  306]
train() client id: f_00004-9-7 loss: 0.907107  [  256/  306]
train() client id: f_00004-9-8 loss: 0.805839  [  288/  306]
train() client id: f_00004-10-0 loss: 0.958184  [   32/  306]
train() client id: f_00004-10-1 loss: 0.947776  [   64/  306]
train() client id: f_00004-10-2 loss: 0.908553  [   96/  306]
train() client id: f_00004-10-3 loss: 0.854242  [  128/  306]
train() client id: f_00004-10-4 loss: 0.840217  [  160/  306]
train() client id: f_00004-10-5 loss: 0.787368  [  192/  306]
train() client id: f_00004-10-6 loss: 0.843042  [  224/  306]
train() client id: f_00004-10-7 loss: 0.851266  [  256/  306]
train() client id: f_00004-10-8 loss: 0.794279  [  288/  306]
train() client id: f_00004-11-0 loss: 0.937395  [   32/  306]
train() client id: f_00004-11-1 loss: 0.823258  [   64/  306]
train() client id: f_00004-11-2 loss: 0.868305  [   96/  306]
train() client id: f_00004-11-3 loss: 0.805990  [  128/  306]
train() client id: f_00004-11-4 loss: 0.915693  [  160/  306]
train() client id: f_00004-11-5 loss: 0.991666  [  192/  306]
train() client id: f_00004-11-6 loss: 0.857601  [  224/  306]
train() client id: f_00004-11-7 loss: 0.823206  [  256/  306]
train() client id: f_00004-11-8 loss: 0.919559  [  288/  306]
train() client id: f_00005-0-0 loss: 0.619271  [   32/  146]
train() client id: f_00005-0-1 loss: 0.875777  [   64/  146]
train() client id: f_00005-0-2 loss: 0.606382  [   96/  146]
train() client id: f_00005-0-3 loss: 0.844756  [  128/  146]
train() client id: f_00005-1-0 loss: 0.969394  [   32/  146]
train() client id: f_00005-1-1 loss: 0.867881  [   64/  146]
train() client id: f_00005-1-2 loss: 0.726275  [   96/  146]
train() client id: f_00005-1-3 loss: 0.499143  [  128/  146]
train() client id: f_00005-2-0 loss: 0.937740  [   32/  146]
train() client id: f_00005-2-1 loss: 0.697691  [   64/  146]
train() client id: f_00005-2-2 loss: 0.850092  [   96/  146]
train() client id: f_00005-2-3 loss: 0.591776  [  128/  146]
train() client id: f_00005-3-0 loss: 0.506912  [   32/  146]
train() client id: f_00005-3-1 loss: 0.947594  [   64/  146]
train() client id: f_00005-3-2 loss: 0.818253  [   96/  146]
train() client id: f_00005-3-3 loss: 0.922852  [  128/  146]
train() client id: f_00005-4-0 loss: 0.808166  [   32/  146]
train() client id: f_00005-4-1 loss: 0.631878  [   64/  146]
train() client id: f_00005-4-2 loss: 0.888749  [   96/  146]
train() client id: f_00005-4-3 loss: 0.730907  [  128/  146]
train() client id: f_00005-5-0 loss: 0.737948  [   32/  146]
train() client id: f_00005-5-1 loss: 0.615786  [   64/  146]
train() client id: f_00005-5-2 loss: 1.160259  [   96/  146]
train() client id: f_00005-5-3 loss: 0.730432  [  128/  146]
train() client id: f_00005-6-0 loss: 0.852114  [   32/  146]
train() client id: f_00005-6-1 loss: 0.660088  [   64/  146]
train() client id: f_00005-6-2 loss: 0.841149  [   96/  146]
train() client id: f_00005-6-3 loss: 0.728945  [  128/  146]
train() client id: f_00005-7-0 loss: 0.868029  [   32/  146]
train() client id: f_00005-7-1 loss: 0.782267  [   64/  146]
train() client id: f_00005-7-2 loss: 0.880585  [   96/  146]
train() client id: f_00005-7-3 loss: 0.646239  [  128/  146]
train() client id: f_00005-8-0 loss: 0.920038  [   32/  146]
train() client id: f_00005-8-1 loss: 0.984760  [   64/  146]
train() client id: f_00005-8-2 loss: 0.517217  [   96/  146]
train() client id: f_00005-8-3 loss: 0.600900  [  128/  146]
train() client id: f_00005-9-0 loss: 0.841244  [   32/  146]
train() client id: f_00005-9-1 loss: 0.742168  [   64/  146]
train() client id: f_00005-9-2 loss: 0.835402  [   96/  146]
train() client id: f_00005-9-3 loss: 0.885041  [  128/  146]
train() client id: f_00005-10-0 loss: 0.733995  [   32/  146]
train() client id: f_00005-10-1 loss: 0.697486  [   64/  146]
train() client id: f_00005-10-2 loss: 0.799223  [   96/  146]
train() client id: f_00005-10-3 loss: 0.973429  [  128/  146]
train() client id: f_00005-11-0 loss: 0.734056  [   32/  146]
train() client id: f_00005-11-1 loss: 0.824270  [   64/  146]
train() client id: f_00005-11-2 loss: 0.984983  [   96/  146]
train() client id: f_00005-11-3 loss: 0.636909  [  128/  146]
train() client id: f_00006-0-0 loss: 0.558087  [   32/   54]
train() client id: f_00006-1-0 loss: 0.548922  [   32/   54]
train() client id: f_00006-2-0 loss: 0.541444  [   32/   54]
train() client id: f_00006-3-0 loss: 0.557875  [   32/   54]
train() client id: f_00006-4-0 loss: 0.517914  [   32/   54]
train() client id: f_00006-5-0 loss: 0.525865  [   32/   54]
train() client id: f_00006-6-0 loss: 0.523594  [   32/   54]
train() client id: f_00006-7-0 loss: 0.545406  [   32/   54]
train() client id: f_00006-8-0 loss: 0.561964  [   32/   54]
train() client id: f_00006-9-0 loss: 0.560829  [   32/   54]
train() client id: f_00006-10-0 loss: 0.517879  [   32/   54]
train() client id: f_00006-11-0 loss: 0.556931  [   32/   54]
train() client id: f_00007-0-0 loss: 0.550934  [   32/  179]
train() client id: f_00007-0-1 loss: 0.523630  [   64/  179]
train() client id: f_00007-0-2 loss: 0.479474  [   96/  179]
train() client id: f_00007-0-3 loss: 0.568342  [  128/  179]
train() client id: f_00007-0-4 loss: 0.503953  [  160/  179]
train() client id: f_00007-1-0 loss: 0.487663  [   32/  179]
train() client id: f_00007-1-1 loss: 0.646665  [   64/  179]
train() client id: f_00007-1-2 loss: 0.403866  [   96/  179]
train() client id: f_00007-1-3 loss: 0.789419  [  128/  179]
train() client id: f_00007-1-4 loss: 0.352085  [  160/  179]
train() client id: f_00007-2-0 loss: 0.390887  [   32/  179]
train() client id: f_00007-2-1 loss: 0.457067  [   64/  179]
train() client id: f_00007-2-2 loss: 0.382271  [   96/  179]
train() client id: f_00007-2-3 loss: 0.559324  [  128/  179]
train() client id: f_00007-2-4 loss: 0.617523  [  160/  179]
train() client id: f_00007-3-0 loss: 0.474585  [   32/  179]
train() client id: f_00007-3-1 loss: 0.550491  [   64/  179]
train() client id: f_00007-3-2 loss: 0.552500  [   96/  179]
train() client id: f_00007-3-3 loss: 0.536094  [  128/  179]
train() client id: f_00007-3-4 loss: 0.433107  [  160/  179]
train() client id: f_00007-4-0 loss: 0.467794  [   32/  179]
train() client id: f_00007-4-1 loss: 0.663097  [   64/  179]
train() client id: f_00007-4-2 loss: 0.323507  [   96/  179]
train() client id: f_00007-4-3 loss: 0.592495  [  128/  179]
train() client id: f_00007-4-4 loss: 0.390039  [  160/  179]
train() client id: f_00007-5-0 loss: 0.304998  [   32/  179]
train() client id: f_00007-5-1 loss: 0.627625  [   64/  179]
train() client id: f_00007-5-2 loss: 0.503059  [   96/  179]
train() client id: f_00007-5-3 loss: 0.629693  [  128/  179]
train() client id: f_00007-5-4 loss: 0.371888  [  160/  179]
train() client id: f_00007-6-0 loss: 0.449618  [   32/  179]
train() client id: f_00007-6-1 loss: 0.431594  [   64/  179]
train() client id: f_00007-6-2 loss: 0.541470  [   96/  179]
train() client id: f_00007-6-3 loss: 0.427500  [  128/  179]
train() client id: f_00007-6-4 loss: 0.459475  [  160/  179]
train() client id: f_00007-7-0 loss: 0.321302  [   32/  179]
train() client id: f_00007-7-1 loss: 0.331675  [   64/  179]
train() client id: f_00007-7-2 loss: 0.599560  [   96/  179]
train() client id: f_00007-7-3 loss: 0.553739  [  128/  179]
train() client id: f_00007-7-4 loss: 0.637259  [  160/  179]
train() client id: f_00007-8-0 loss: 0.398649  [   32/  179]
train() client id: f_00007-8-1 loss: 0.496624  [   64/  179]
train() client id: f_00007-8-2 loss: 0.393308  [   96/  179]
train() client id: f_00007-8-3 loss: 0.726476  [  128/  179]
train() client id: f_00007-8-4 loss: 0.347409  [  160/  179]
train() client id: f_00007-9-0 loss: 0.492777  [   32/  179]
train() client id: f_00007-9-1 loss: 0.602637  [   64/  179]
train() client id: f_00007-9-2 loss: 0.438557  [   96/  179]
train() client id: f_00007-9-3 loss: 0.318594  [  128/  179]
train() client id: f_00007-9-4 loss: 0.574791  [  160/  179]
train() client id: f_00007-10-0 loss: 0.406745  [   32/  179]
train() client id: f_00007-10-1 loss: 0.513111  [   64/  179]
train() client id: f_00007-10-2 loss: 0.698781  [   96/  179]
train() client id: f_00007-10-3 loss: 0.386995  [  128/  179]
train() client id: f_00007-10-4 loss: 0.302124  [  160/  179]
train() client id: f_00007-11-0 loss: 0.479051  [   32/  179]
train() client id: f_00007-11-1 loss: 0.468428  [   64/  179]
train() client id: f_00007-11-2 loss: 0.436864  [   96/  179]
train() client id: f_00007-11-3 loss: 0.462987  [  128/  179]
train() client id: f_00007-11-4 loss: 0.509413  [  160/  179]
train() client id: f_00008-0-0 loss: 0.872119  [   32/  130]
train() client id: f_00008-0-1 loss: 0.836236  [   64/  130]
train() client id: f_00008-0-2 loss: 0.871105  [   96/  130]
train() client id: f_00008-0-3 loss: 0.863521  [  128/  130]
train() client id: f_00008-1-0 loss: 0.817366  [   32/  130]
train() client id: f_00008-1-1 loss: 0.851000  [   64/  130]
train() client id: f_00008-1-2 loss: 0.963363  [   96/  130]
train() client id: f_00008-1-3 loss: 0.780703  [  128/  130]
train() client id: f_00008-2-0 loss: 0.805514  [   32/  130]
train() client id: f_00008-2-1 loss: 0.864944  [   64/  130]
train() client id: f_00008-2-2 loss: 0.908143  [   96/  130]
train() client id: f_00008-2-3 loss: 0.820060  [  128/  130]
train() client id: f_00008-3-0 loss: 0.758727  [   32/  130]
train() client id: f_00008-3-1 loss: 0.891252  [   64/  130]
train() client id: f_00008-3-2 loss: 0.822362  [   96/  130]
train() client id: f_00008-3-3 loss: 0.910653  [  128/  130]
train() client id: f_00008-4-0 loss: 0.886225  [   32/  130]
train() client id: f_00008-4-1 loss: 0.921957  [   64/  130]
train() client id: f_00008-4-2 loss: 0.718480  [   96/  130]
train() client id: f_00008-4-3 loss: 0.910751  [  128/  130]
train() client id: f_00008-5-0 loss: 0.830841  [   32/  130]
train() client id: f_00008-5-1 loss: 0.944764  [   64/  130]
train() client id: f_00008-5-2 loss: 0.751171  [   96/  130]
train() client id: f_00008-5-3 loss: 0.893401  [  128/  130]
train() client id: f_00008-6-0 loss: 0.838077  [   32/  130]
train() client id: f_00008-6-1 loss: 0.852197  [   64/  130]
train() client id: f_00008-6-2 loss: 0.820232  [   96/  130]
train() client id: f_00008-6-3 loss: 0.893200  [  128/  130]
train() client id: f_00008-7-0 loss: 0.871096  [   32/  130]
train() client id: f_00008-7-1 loss: 0.815088  [   64/  130]
train() client id: f_00008-7-2 loss: 0.892461  [   96/  130]
train() client id: f_00008-7-3 loss: 0.856496  [  128/  130]
train() client id: f_00008-8-0 loss: 0.824971  [   32/  130]
train() client id: f_00008-8-1 loss: 0.849633  [   64/  130]
train() client id: f_00008-8-2 loss: 0.894655  [   96/  130]
train() client id: f_00008-8-3 loss: 0.799918  [  128/  130]
train() client id: f_00008-9-0 loss: 0.875030  [   32/  130]
train() client id: f_00008-9-1 loss: 0.767996  [   64/  130]
train() client id: f_00008-9-2 loss: 0.885900  [   96/  130]
train() client id: f_00008-9-3 loss: 0.850863  [  128/  130]
train() client id: f_00008-10-0 loss: 0.914554  [   32/  130]
train() client id: f_00008-10-1 loss: 0.831447  [   64/  130]
train() client id: f_00008-10-2 loss: 0.883789  [   96/  130]
train() client id: f_00008-10-3 loss: 0.799714  [  128/  130]
train() client id: f_00008-11-0 loss: 0.886984  [   32/  130]
train() client id: f_00008-11-1 loss: 0.870414  [   64/  130]
train() client id: f_00008-11-2 loss: 0.843575  [   96/  130]
train() client id: f_00008-11-3 loss: 0.792044  [  128/  130]
train() client id: f_00009-0-0 loss: 0.978619  [   32/  118]
train() client id: f_00009-0-1 loss: 1.122138  [   64/  118]
train() client id: f_00009-0-2 loss: 1.047310  [   96/  118]
train() client id: f_00009-1-0 loss: 1.095245  [   32/  118]
train() client id: f_00009-1-1 loss: 0.970177  [   64/  118]
train() client id: f_00009-1-2 loss: 0.992560  [   96/  118]
train() client id: f_00009-2-0 loss: 1.003612  [   32/  118]
train() client id: f_00009-2-1 loss: 1.002128  [   64/  118]
train() client id: f_00009-2-2 loss: 0.884969  [   96/  118]
train() client id: f_00009-3-0 loss: 1.030364  [   32/  118]
train() client id: f_00009-3-1 loss: 0.842460  [   64/  118]
train() client id: f_00009-3-2 loss: 1.000551  [   96/  118]
train() client id: f_00009-4-0 loss: 1.007192  [   32/  118]
train() client id: f_00009-4-1 loss: 0.873282  [   64/  118]
train() client id: f_00009-4-2 loss: 0.899264  [   96/  118]
train() client id: f_00009-5-0 loss: 1.001479  [   32/  118]
train() client id: f_00009-5-1 loss: 1.047345  [   64/  118]
train() client id: f_00009-5-2 loss: 0.724162  [   96/  118]
train() client id: f_00009-6-0 loss: 0.917951  [   32/  118]
train() client id: f_00009-6-1 loss: 0.955409  [   64/  118]
train() client id: f_00009-6-2 loss: 0.864790  [   96/  118]
train() client id: f_00009-7-0 loss: 0.897963  [   32/  118]
train() client id: f_00009-7-1 loss: 1.044511  [   64/  118]
train() client id: f_00009-7-2 loss: 0.822073  [   96/  118]
train() client id: f_00009-8-0 loss: 0.841719  [   32/  118]
train() client id: f_00009-8-1 loss: 0.882901  [   64/  118]
train() client id: f_00009-8-2 loss: 0.952077  [   96/  118]
train() client id: f_00009-9-0 loss: 1.049376  [   32/  118]
train() client id: f_00009-9-1 loss: 0.942039  [   64/  118]
train() client id: f_00009-9-2 loss: 0.701984  [   96/  118]
train() client id: f_00009-10-0 loss: 0.760875  [   32/  118]
train() client id: f_00009-10-1 loss: 0.818807  [   64/  118]
train() client id: f_00009-10-2 loss: 1.213763  [   96/  118]
train() client id: f_00009-11-0 loss: 1.010641  [   32/  118]
train() client id: f_00009-11-1 loss: 0.848422  [   64/  118]
train() client id: f_00009-11-2 loss: 0.868212  [   96/  118]
At round 20 accuracy: 0.6339522546419099
At round 20 training accuracy: 0.5808182427900738
At round 20 training loss: 0.8326782236578227
update_location
xs = -3.905658 4.200318 120.009024 18.811294 0.979296 3.956410 -82.443192 -61.324852 104.663977 -47.060879 
ys = 112.587959 95.555839 1.320614 -82.455176 74.350187 57.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 5.788573831882571
ys mean: 27.894142535528708
dists_uav = 150.636326 138.378326 156.217508 130.968396 124.615044 115.577373 129.629358 117.309052 145.819128 110.592668 
uav_gains = -104.453056 -103.528245 -104.850992 -102.929888 -102.389625 -101.571946 -102.818214 -101.733444 -104.098489 -101.093226 
uav_gains_db_mean: -102.9467124412129
dists_bs = 182.122312 196.020735 342.337338 322.251432 202.729124 213.911009 200.293279 207.990129 320.930612 213.562431 
bs_gains = -102.857227 -103.751516 -110.531779 -109.796515 -104.160711 -104.813587 -104.013718 -104.472255 -109.746571 -104.793755 
bs_gains_db_mean: -105.89376344304972
Round 21
-------------------------------
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.22040415 17.10262761  8.09588073  2.90155574 19.72744601  9.50226703
  3.60411444 11.58945304  8.53512887  7.71174179]
obj_prev = 96.99061941020106
eta_min = 5.053940464590129e-12	eta_max = 0.9225775474279215
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 22.544522121240533	eta = 0.9090909090909091
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 39.50145864601099	eta = 0.5188421089429398
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 31.352662916909924	eta = 0.6536931221610769
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.88807124482688	eta = 0.6857257513318464
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.81462162317344	eta = 0.6874150666493414
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.81442331258453	eta = 0.6874196389895562
eta = 0.6874196389895562
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [0.03092845 0.06504793 0.03043752 0.01055495 0.07511195 0.03583773
 0.01325505 0.04393804 0.03191031 0.02896474]
ene_total = [2.59558483 4.86886559 2.57325892 1.19041384 5.55486377 2.93485376
 1.36920185 3.40632189 2.84825067 2.47280818]
ti_comp = [0.35387355 0.35652047 0.35228915 0.35940333 0.35499852 0.35244077
 0.35977839 0.36323055 0.32606389 0.35252092]
ti_coms = [0.07726245 0.07461553 0.07884685 0.07173267 0.07613748 0.07869523
 0.07135761 0.06790545 0.10507211 0.07861508]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.47658462e-05 1.35335454e-04 1.42006892e-05 5.68963075e-07
 2.10161950e-04 2.31595009e-05 1.12448598e-06 4.01825275e-05
 1.91015032e-05 1.22213485e-05]
ene_total = [0.51979327 0.51011572 0.53039422 0.48170862 0.52535973 0.52997772
 0.47922748 0.45866954 0.70682062 0.52870501]
optimize_network iter = 0 obj = 5.270771946473119
eta = 0.6874196389895562
freqs = [4.36998634e+07 9.12260848e+07 4.31996322e+07 1.46839871e+07
 1.05791923e+08 5.08422020e+07 1.84211324e+07 6.04822990e+07
 4.89325984e+07 4.10822992e+07]
eta_min = 0.6874196389895763	eta_max = 0.6874196389895456
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 0.029002321139164264	eta = 0.9090909090909091
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 18.041876240655995	eta = 0.0014613638924501724
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.8124320127004978	eta = 0.014547164420730322
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.775015665989668	eta = 0.01485381058620065
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.7750098831933823	eta = 0.01485385897835976
eta = 0.01485385897835976
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.65835243e-04 1.51995270e-03 1.59487963e-04 6.39002520e-06
 2.36032919e-03 2.60104392e-04 1.26291038e-05 4.51290031e-04
 2.14529013e-04 1.37257984e-04]
ene_total = [0.1681456  0.19135646 0.17138602 0.15296698 0.2125038  0.17320668
 0.15230083 0.15429171 0.22843322 0.17041859]
ti_comp = [0.35387355 0.35652047 0.35228915 0.35940333 0.35499852 0.35244077
 0.35977839 0.36323055 0.32606389 0.35252092]
ti_coms = [0.07726245 0.07461553 0.07884685 0.07173267 0.07613748 0.07869523
 0.07135761 0.06790545 0.10507211 0.07861508]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.47658462e-05 1.35335454e-04 1.42006892e-05 5.68963075e-07
 2.10161950e-04 2.31595009e-05 1.12448598e-06 4.01825275e-05
 1.91015032e-05 1.22213485e-05]
ene_total = [0.51979327 0.51011572 0.53039422 0.48170862 0.52535973 0.52997772
 0.47922748 0.45866954 0.70682062 0.52870501]
optimize_network iter = 1 obj = 5.270771946473455
eta = 0.6874196389895763
freqs = [4.36998634e+07 9.12260848e+07 4.31996322e+07 1.46839871e+07
 1.05791923e+08 5.08422020e+07 1.84211324e+07 6.04822990e+07
 4.89325984e+07 4.10822992e+07]
Done!
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.44371661e-05 1.32322957e-04 1.38845892e-05 5.56298252e-07
 2.05483854e-04 2.26439824e-05 1.09945550e-06 3.92880853e-05
 1.86763136e-05 1.19493076e-05]
ene_total = [0.00774068 0.00759388 0.00789857 0.00717382 0.00781923 0.00789217
 0.00713686 0.00682983 0.01052589 0.00787346]
At round 21 energy consumption: 0.07848438862700059
At round 21 eta: 0.6874196389895763
At round 21 a_n: 20.989140069898713
At round 21 local rounds: 12.273194960737252
At round 21 global rounds: 67.14798076901184
gradient difference: 0.40134841203689575
train() client id: f_00000-0-0 loss: 1.044092  [   32/  126]
train() client id: f_00000-0-1 loss: 0.995619  [   64/  126]
train() client id: f_00000-0-2 loss: 0.999063  [   96/  126]
train() client id: f_00000-1-0 loss: 1.064686  [   32/  126]
train() client id: f_00000-1-1 loss: 1.056948  [   64/  126]
train() client id: f_00000-1-2 loss: 1.024752  [   96/  126]
train() client id: f_00000-2-0 loss: 0.828766  [   32/  126]
train() client id: f_00000-2-1 loss: 1.135398  [   64/  126]
train() client id: f_00000-2-2 loss: 0.972670  [   96/  126]
train() client id: f_00000-3-0 loss: 0.892102  [   32/  126]
train() client id: f_00000-3-1 loss: 0.992176  [   64/  126]
train() client id: f_00000-3-2 loss: 0.993255  [   96/  126]
train() client id: f_00000-4-0 loss: 0.825978  [   32/  126]
train() client id: f_00000-4-1 loss: 0.999419  [   64/  126]
train() client id: f_00000-4-2 loss: 0.888922  [   96/  126]
train() client id: f_00000-5-0 loss: 0.881961  [   32/  126]
train() client id: f_00000-5-1 loss: 0.916484  [   64/  126]
train() client id: f_00000-5-2 loss: 0.840429  [   96/  126]
train() client id: f_00000-6-0 loss: 0.951541  [   32/  126]
train() client id: f_00000-6-1 loss: 0.794871  [   64/  126]
train() client id: f_00000-6-2 loss: 0.945755  [   96/  126]
train() client id: f_00000-7-0 loss: 0.906550  [   32/  126]
train() client id: f_00000-7-1 loss: 0.941392  [   64/  126]
train() client id: f_00000-7-2 loss: 0.769533  [   96/  126]
train() client id: f_00000-8-0 loss: 0.889034  [   32/  126]
train() client id: f_00000-8-1 loss: 0.817696  [   64/  126]
train() client id: f_00000-8-2 loss: 0.902561  [   96/  126]
train() client id: f_00000-9-0 loss: 0.866743  [   32/  126]
train() client id: f_00000-9-1 loss: 0.865137  [   64/  126]
train() client id: f_00000-9-2 loss: 0.887850  [   96/  126]
train() client id: f_00000-10-0 loss: 0.877831  [   32/  126]
train() client id: f_00000-10-1 loss: 0.775640  [   64/  126]
train() client id: f_00000-10-2 loss: 0.920491  [   96/  126]
train() client id: f_00000-11-0 loss: 0.924368  [   32/  126]
train() client id: f_00000-11-1 loss: 0.939966  [   64/  126]
train() client id: f_00000-11-2 loss: 0.820116  [   96/  126]
train() client id: f_00001-0-0 loss: 0.283456  [   32/  265]
train() client id: f_00001-0-1 loss: 0.356147  [   64/  265]
train() client id: f_00001-0-2 loss: 0.261231  [   96/  265]
train() client id: f_00001-0-3 loss: 0.278876  [  128/  265]
train() client id: f_00001-0-4 loss: 0.393133  [  160/  265]
train() client id: f_00001-0-5 loss: 0.339022  [  192/  265]
train() client id: f_00001-0-6 loss: 0.352082  [  224/  265]
train() client id: f_00001-0-7 loss: 0.428724  [  256/  265]
train() client id: f_00001-1-0 loss: 0.325051  [   32/  265]
train() client id: f_00001-1-1 loss: 0.273410  [   64/  265]
train() client id: f_00001-1-2 loss: 0.322502  [   96/  265]
train() client id: f_00001-1-3 loss: 0.220864  [  128/  265]
train() client id: f_00001-1-4 loss: 0.314488  [  160/  265]
train() client id: f_00001-1-5 loss: 0.385649  [  192/  265]
train() client id: f_00001-1-6 loss: 0.365415  [  224/  265]
train() client id: f_00001-1-7 loss: 0.343581  [  256/  265]
train() client id: f_00001-2-0 loss: 0.272076  [   32/  265]
train() client id: f_00001-2-1 loss: 0.303063  [   64/  265]
train() client id: f_00001-2-2 loss: 0.296219  [   96/  265]
train() client id: f_00001-2-3 loss: 0.392638  [  128/  265]
train() client id: f_00001-2-4 loss: 0.250118  [  160/  265]
train() client id: f_00001-2-5 loss: 0.364082  [  192/  265]
train() client id: f_00001-2-6 loss: 0.348759  [  224/  265]
train() client id: f_00001-2-7 loss: 0.294640  [  256/  265]
train() client id: f_00001-3-0 loss: 0.346019  [   32/  265]
train() client id: f_00001-3-1 loss: 0.347603  [   64/  265]
train() client id: f_00001-3-2 loss: 0.414432  [   96/  265]
train() client id: f_00001-3-3 loss: 0.249750  [  128/  265]
train() client id: f_00001-3-4 loss: 0.278247  [  160/  265]
train() client id: f_00001-3-5 loss: 0.329316  [  192/  265]
train() client id: f_00001-3-6 loss: 0.227145  [  224/  265]
train() client id: f_00001-3-7 loss: 0.263245  [  256/  265]
train() client id: f_00001-4-0 loss: 0.299064  [   32/  265]
train() client id: f_00001-4-1 loss: 0.275091  [   64/  265]
train() client id: f_00001-4-2 loss: 0.377452  [   96/  265]
train() client id: f_00001-4-3 loss: 0.269466  [  128/  265]
train() client id: f_00001-4-4 loss: 0.197548  [  160/  265]
train() client id: f_00001-4-5 loss: 0.307470  [  192/  265]
train() client id: f_00001-4-6 loss: 0.357719  [  224/  265]
train() client id: f_00001-4-7 loss: 0.316455  [  256/  265]
train() client id: f_00001-5-0 loss: 0.239806  [   32/  265]
train() client id: f_00001-5-1 loss: 0.325044  [   64/  265]
train() client id: f_00001-5-2 loss: 0.229359  [   96/  265]
train() client id: f_00001-5-3 loss: 0.195466  [  128/  265]
train() client id: f_00001-5-4 loss: 0.382011  [  160/  265]
train() client id: f_00001-5-5 loss: 0.263375  [  192/  265]
train() client id: f_00001-5-6 loss: 0.370149  [  224/  265]
train() client id: f_00001-5-7 loss: 0.277948  [  256/  265]
train() client id: f_00001-6-0 loss: 0.372435  [   32/  265]
train() client id: f_00001-6-1 loss: 0.311029  [   64/  265]
train() client id: f_00001-6-2 loss: 0.283544  [   96/  265]
train() client id: f_00001-6-3 loss: 0.286655  [  128/  265]
train() client id: f_00001-6-4 loss: 0.207254  [  160/  265]
train() client id: f_00001-6-5 loss: 0.366875  [  192/  265]
train() client id: f_00001-6-6 loss: 0.263499  [  224/  265]
train() client id: f_00001-6-7 loss: 0.236896  [  256/  265]
train() client id: f_00001-7-0 loss: 0.390667  [   32/  265]
train() client id: f_00001-7-1 loss: 0.318852  [   64/  265]
train() client id: f_00001-7-2 loss: 0.201263  [   96/  265]
train() client id: f_00001-7-3 loss: 0.346165  [  128/  265]
train() client id: f_00001-7-4 loss: 0.306639  [  160/  265]
train() client id: f_00001-7-5 loss: 0.225322  [  192/  265]
train() client id: f_00001-7-6 loss: 0.207758  [  224/  265]
train() client id: f_00001-7-7 loss: 0.297372  [  256/  265]
train() client id: f_00001-8-0 loss: 0.458765  [   32/  265]
train() client id: f_00001-8-1 loss: 0.185744  [   64/  265]
train() client id: f_00001-8-2 loss: 0.234907  [   96/  265]
train() client id: f_00001-8-3 loss: 0.197253  [  128/  265]
train() client id: f_00001-8-4 loss: 0.185179  [  160/  265]
train() client id: f_00001-8-5 loss: 0.202694  [  192/  265]
train() client id: f_00001-8-6 loss: 0.410474  [  224/  265]
train() client id: f_00001-8-7 loss: 0.393189  [  256/  265]
train() client id: f_00001-9-0 loss: 0.174559  [   32/  265]
train() client id: f_00001-9-1 loss: 0.306773  [   64/  265]
train() client id: f_00001-9-2 loss: 0.232002  [   96/  265]
train() client id: f_00001-9-3 loss: 0.408906  [  128/  265]
train() client id: f_00001-9-4 loss: 0.259379  [  160/  265]
train() client id: f_00001-9-5 loss: 0.260622  [  192/  265]
train() client id: f_00001-9-6 loss: 0.255170  [  224/  265]
train() client id: f_00001-9-7 loss: 0.274207  [  256/  265]
train() client id: f_00001-10-0 loss: 0.239443  [   32/  265]
train() client id: f_00001-10-1 loss: 0.224460  [   64/  265]
train() client id: f_00001-10-2 loss: 0.294419  [   96/  265]
train() client id: f_00001-10-3 loss: 0.321769  [  128/  265]
train() client id: f_00001-10-4 loss: 0.380215  [  160/  265]
train() client id: f_00001-10-5 loss: 0.180219  [  192/  265]
train() client id: f_00001-10-6 loss: 0.223747  [  224/  265]
train() client id: f_00001-10-7 loss: 0.367265  [  256/  265]
train() client id: f_00001-11-0 loss: 0.358464  [   32/  265]
train() client id: f_00001-11-1 loss: 0.296788  [   64/  265]
train() client id: f_00001-11-2 loss: 0.275837  [   96/  265]
train() client id: f_00001-11-3 loss: 0.170689  [  128/  265]
train() client id: f_00001-11-4 loss: 0.202915  [  160/  265]
train() client id: f_00001-11-5 loss: 0.282821  [  192/  265]
train() client id: f_00001-11-6 loss: 0.349186  [  224/  265]
train() client id: f_00001-11-7 loss: 0.250957  [  256/  265]
train() client id: f_00002-0-0 loss: 1.313207  [   32/  124]
train() client id: f_00002-0-1 loss: 1.081110  [   64/  124]
train() client id: f_00002-0-2 loss: 1.274639  [   96/  124]
train() client id: f_00002-1-0 loss: 1.040875  [   32/  124]
train() client id: f_00002-1-1 loss: 1.334516  [   64/  124]
train() client id: f_00002-1-2 loss: 1.074230  [   96/  124]
train() client id: f_00002-2-0 loss: 1.026791  [   32/  124]
train() client id: f_00002-2-1 loss: 1.083120  [   64/  124]
train() client id: f_00002-2-2 loss: 1.096354  [   96/  124]
train() client id: f_00002-3-0 loss: 1.192249  [   32/  124]
train() client id: f_00002-3-1 loss: 1.037116  [   64/  124]
train() client id: f_00002-3-2 loss: 1.068624  [   96/  124]
train() client id: f_00002-4-0 loss: 1.060997  [   32/  124]
train() client id: f_00002-4-1 loss: 1.213881  [   64/  124]
train() client id: f_00002-4-2 loss: 0.920603  [   96/  124]
train() client id: f_00002-5-0 loss: 1.123466  [   32/  124]
train() client id: f_00002-5-1 loss: 1.060480  [   64/  124]
train() client id: f_00002-5-2 loss: 0.784768  [   96/  124]
train() client id: f_00002-6-0 loss: 0.928513  [   32/  124]
train() client id: f_00002-6-1 loss: 0.959397  [   64/  124]
train() client id: f_00002-6-2 loss: 1.012913  [   96/  124]
train() client id: f_00002-7-0 loss: 0.825952  [   32/  124]
train() client id: f_00002-7-1 loss: 1.079972  [   64/  124]
train() client id: f_00002-7-2 loss: 1.051388  [   96/  124]
train() client id: f_00002-8-0 loss: 0.992555  [   32/  124]
train() client id: f_00002-8-1 loss: 0.845824  [   64/  124]
train() client id: f_00002-8-2 loss: 1.150669  [   96/  124]
train() client id: f_00002-9-0 loss: 0.842150  [   32/  124]
train() client id: f_00002-9-1 loss: 1.072029  [   64/  124]
train() client id: f_00002-9-2 loss: 0.853829  [   96/  124]
train() client id: f_00002-10-0 loss: 1.048362  [   32/  124]
train() client id: f_00002-10-1 loss: 0.998381  [   64/  124]
train() client id: f_00002-10-2 loss: 0.838367  [   96/  124]
train() client id: f_00002-11-0 loss: 0.937345  [   32/  124]
train() client id: f_00002-11-1 loss: 0.900428  [   64/  124]
train() client id: f_00002-11-2 loss: 0.996849  [   96/  124]
train() client id: f_00003-0-0 loss: 0.866630  [   32/   43]
train() client id: f_00003-1-0 loss: 0.711835  [   32/   43]
train() client id: f_00003-2-0 loss: 0.755031  [   32/   43]
train() client id: f_00003-3-0 loss: 0.726340  [   32/   43]
train() client id: f_00003-4-0 loss: 0.638422  [   32/   43]
train() client id: f_00003-5-0 loss: 0.693127  [   32/   43]
train() client id: f_00003-6-0 loss: 0.798986  [   32/   43]
train() client id: f_00003-7-0 loss: 0.873842  [   32/   43]
train() client id: f_00003-8-0 loss: 0.518481  [   32/   43]
train() client id: f_00003-9-0 loss: 0.882649  [   32/   43]
train() client id: f_00003-10-0 loss: 0.726412  [   32/   43]
train() client id: f_00003-11-0 loss: 0.670130  [   32/   43]
train() client id: f_00004-0-0 loss: 0.879722  [   32/  306]
train() client id: f_00004-0-1 loss: 0.868306  [   64/  306]
train() client id: f_00004-0-2 loss: 0.873718  [   96/  306]
train() client id: f_00004-0-3 loss: 0.752577  [  128/  306]
train() client id: f_00004-0-4 loss: 0.909329  [  160/  306]
train() client id: f_00004-0-5 loss: 0.818707  [  192/  306]
train() client id: f_00004-0-6 loss: 0.975587  [  224/  306]
train() client id: f_00004-0-7 loss: 0.738732  [  256/  306]
train() client id: f_00004-0-8 loss: 0.688853  [  288/  306]
train() client id: f_00004-1-0 loss: 0.796615  [   32/  306]
train() client id: f_00004-1-1 loss: 0.939176  [   64/  306]
train() client id: f_00004-1-2 loss: 0.842384  [   96/  306]
train() client id: f_00004-1-3 loss: 0.869460  [  128/  306]
train() client id: f_00004-1-4 loss: 0.837424  [  160/  306]
train() client id: f_00004-1-5 loss: 0.883224  [  192/  306]
train() client id: f_00004-1-6 loss: 0.842610  [  224/  306]
train() client id: f_00004-1-7 loss: 0.671961  [  256/  306]
train() client id: f_00004-1-8 loss: 0.813349  [  288/  306]
train() client id: f_00004-2-0 loss: 0.882838  [   32/  306]
train() client id: f_00004-2-1 loss: 0.824430  [   64/  306]
train() client id: f_00004-2-2 loss: 0.782520  [   96/  306]
train() client id: f_00004-2-3 loss: 0.759120  [  128/  306]
train() client id: f_00004-2-4 loss: 0.912349  [  160/  306]
train() client id: f_00004-2-5 loss: 0.861532  [  192/  306]
train() client id: f_00004-2-6 loss: 0.844404  [  224/  306]
train() client id: f_00004-2-7 loss: 0.795986  [  256/  306]
train() client id: f_00004-2-8 loss: 0.857032  [  288/  306]
train() client id: f_00004-3-0 loss: 0.825892  [   32/  306]
train() client id: f_00004-3-1 loss: 0.691317  [   64/  306]
train() client id: f_00004-3-2 loss: 0.848794  [   96/  306]
train() client id: f_00004-3-3 loss: 0.833247  [  128/  306]
train() client id: f_00004-3-4 loss: 0.793253  [  160/  306]
train() client id: f_00004-3-5 loss: 0.839212  [  192/  306]
train() client id: f_00004-3-6 loss: 0.975245  [  224/  306]
train() client id: f_00004-3-7 loss: 0.831472  [  256/  306]
train() client id: f_00004-3-8 loss: 0.859820  [  288/  306]
train() client id: f_00004-4-0 loss: 0.822744  [   32/  306]
train() client id: f_00004-4-1 loss: 0.813873  [   64/  306]
train() client id: f_00004-4-2 loss: 0.887605  [   96/  306]
train() client id: f_00004-4-3 loss: 0.782556  [  128/  306]
train() client id: f_00004-4-4 loss: 0.752773  [  160/  306]
train() client id: f_00004-4-5 loss: 0.844049  [  192/  306]
train() client id: f_00004-4-6 loss: 0.854765  [  224/  306]
train() client id: f_00004-4-7 loss: 0.822964  [  256/  306]
train() client id: f_00004-4-8 loss: 0.942803  [  288/  306]
train() client id: f_00004-5-0 loss: 0.733221  [   32/  306]
train() client id: f_00004-5-1 loss: 0.812138  [   64/  306]
train() client id: f_00004-5-2 loss: 0.876630  [   96/  306]
train() client id: f_00004-5-3 loss: 0.773708  [  128/  306]
train() client id: f_00004-5-4 loss: 0.865654  [  160/  306]
train() client id: f_00004-5-5 loss: 0.826237  [  192/  306]
train() client id: f_00004-5-6 loss: 0.879492  [  224/  306]
train() client id: f_00004-5-7 loss: 0.838306  [  256/  306]
train() client id: f_00004-5-8 loss: 0.836510  [  288/  306]
train() client id: f_00004-6-0 loss: 0.877276  [   32/  306]
train() client id: f_00004-6-1 loss: 0.710576  [   64/  306]
train() client id: f_00004-6-2 loss: 0.957396  [   96/  306]
train() client id: f_00004-6-3 loss: 0.863054  [  128/  306]
train() client id: f_00004-6-4 loss: 0.787341  [  160/  306]
train() client id: f_00004-6-5 loss: 0.747259  [  192/  306]
train() client id: f_00004-6-6 loss: 0.909378  [  224/  306]
train() client id: f_00004-6-7 loss: 0.761087  [  256/  306]
train() client id: f_00004-6-8 loss: 0.879439  [  288/  306]
train() client id: f_00004-7-0 loss: 0.666313  [   32/  306]
train() client id: f_00004-7-1 loss: 0.752169  [   64/  306]
train() client id: f_00004-7-2 loss: 0.859921  [   96/  306]
train() client id: f_00004-7-3 loss: 0.921254  [  128/  306]
train() client id: f_00004-7-4 loss: 0.904519  [  160/  306]
train() client id: f_00004-7-5 loss: 0.780706  [  192/  306]
train() client id: f_00004-7-6 loss: 0.853918  [  224/  306]
train() client id: f_00004-7-7 loss: 0.825031  [  256/  306]
train() client id: f_00004-7-8 loss: 0.940562  [  288/  306]
train() client id: f_00004-8-0 loss: 0.732896  [   32/  306]
train() client id: f_00004-8-1 loss: 0.935321  [   64/  306]
train() client id: f_00004-8-2 loss: 0.769021  [   96/  306]
train() client id: f_00004-8-3 loss: 0.854150  [  128/  306]
train() client id: f_00004-8-4 loss: 0.778592  [  160/  306]
train() client id: f_00004-8-5 loss: 0.858296  [  192/  306]
train() client id: f_00004-8-6 loss: 0.878577  [  224/  306]
train() client id: f_00004-8-7 loss: 0.720881  [  256/  306]
train() client id: f_00004-8-8 loss: 0.967214  [  288/  306]
train() client id: f_00004-9-0 loss: 0.817043  [   32/  306]
train() client id: f_00004-9-1 loss: 0.774131  [   64/  306]
train() client id: f_00004-9-2 loss: 0.733167  [   96/  306]
train() client id: f_00004-9-3 loss: 0.878748  [  128/  306]
train() client id: f_00004-9-4 loss: 0.860414  [  160/  306]
train() client id: f_00004-9-5 loss: 0.757646  [  192/  306]
train() client id: f_00004-9-6 loss: 0.886216  [  224/  306]
train() client id: f_00004-9-7 loss: 0.794032  [  256/  306]
train() client id: f_00004-9-8 loss: 0.942414  [  288/  306]
train() client id: f_00004-10-0 loss: 0.809820  [   32/  306]
train() client id: f_00004-10-1 loss: 0.825074  [   64/  306]
train() client id: f_00004-10-2 loss: 0.844464  [   96/  306]
train() client id: f_00004-10-3 loss: 0.778584  [  128/  306]
train() client id: f_00004-10-4 loss: 0.914251  [  160/  306]
train() client id: f_00004-10-5 loss: 0.764956  [  192/  306]
train() client id: f_00004-10-6 loss: 0.816090  [  224/  306]
train() client id: f_00004-10-7 loss: 0.876387  [  256/  306]
train() client id: f_00004-10-8 loss: 0.775147  [  288/  306]
train() client id: f_00004-11-0 loss: 0.980068  [   32/  306]
train() client id: f_00004-11-1 loss: 0.770968  [   64/  306]
train() client id: f_00004-11-2 loss: 0.761533  [   96/  306]
train() client id: f_00004-11-3 loss: 0.847853  [  128/  306]
train() client id: f_00004-11-4 loss: 0.878559  [  160/  306]
train() client id: f_00004-11-5 loss: 0.799241  [  192/  306]
train() client id: f_00004-11-6 loss: 0.819918  [  224/  306]
train() client id: f_00004-11-7 loss: 0.836441  [  256/  306]
train() client id: f_00004-11-8 loss: 0.760059  [  288/  306]
train() client id: f_00005-0-0 loss: 0.635998  [   32/  146]
train() client id: f_00005-0-1 loss: 1.046996  [   64/  146]
train() client id: f_00005-0-2 loss: 0.755490  [   96/  146]
train() client id: f_00005-0-3 loss: 0.790394  [  128/  146]
train() client id: f_00005-1-0 loss: 0.946088  [   32/  146]
train() client id: f_00005-1-1 loss: 0.909699  [   64/  146]
train() client id: f_00005-1-2 loss: 0.681317  [   96/  146]
train() client id: f_00005-1-3 loss: 0.611680  [  128/  146]
train() client id: f_00005-2-0 loss: 0.811191  [   32/  146]
train() client id: f_00005-2-1 loss: 0.866575  [   64/  146]
train() client id: f_00005-2-2 loss: 0.727048  [   96/  146]
train() client id: f_00005-2-3 loss: 1.010587  [  128/  146]
train() client id: f_00005-3-0 loss: 0.743054  [   32/  146]
train() client id: f_00005-3-1 loss: 0.897337  [   64/  146]
train() client id: f_00005-3-2 loss: 0.683657  [   96/  146]
train() client id: f_00005-3-3 loss: 0.844335  [  128/  146]
train() client id: f_00005-4-0 loss: 0.789676  [   32/  146]
train() client id: f_00005-4-1 loss: 0.892981  [   64/  146]
train() client id: f_00005-4-2 loss: 0.723868  [   96/  146]
train() client id: f_00005-4-3 loss: 0.948918  [  128/  146]
train() client id: f_00005-5-0 loss: 0.730330  [   32/  146]
train() client id: f_00005-5-1 loss: 0.829680  [   64/  146]
train() client id: f_00005-5-2 loss: 0.716996  [   96/  146]
train() client id: f_00005-5-3 loss: 0.839113  [  128/  146]
train() client id: f_00005-6-0 loss: 0.930968  [   32/  146]
train() client id: f_00005-6-1 loss: 0.722530  [   64/  146]
train() client id: f_00005-6-2 loss: 0.765724  [   96/  146]
train() client id: f_00005-6-3 loss: 0.807310  [  128/  146]
train() client id: f_00005-7-0 loss: 0.671518  [   32/  146]
train() client id: f_00005-7-1 loss: 0.799838  [   64/  146]
train() client id: f_00005-7-2 loss: 0.597909  [   96/  146]
train() client id: f_00005-7-3 loss: 0.900177  [  128/  146]
train() client id: f_00005-8-0 loss: 0.869917  [   32/  146]
train() client id: f_00005-8-1 loss: 0.737505  [   64/  146]
train() client id: f_00005-8-2 loss: 0.623539  [   96/  146]
train() client id: f_00005-8-3 loss: 0.763611  [  128/  146]
train() client id: f_00005-9-0 loss: 0.641834  [   32/  146]
train() client id: f_00005-9-1 loss: 0.950911  [   64/  146]
train() client id: f_00005-9-2 loss: 0.828843  [   96/  146]
train() client id: f_00005-9-3 loss: 0.837863  [  128/  146]
train() client id: f_00005-10-0 loss: 0.974835  [   32/  146]
train() client id: f_00005-10-1 loss: 0.761059  [   64/  146]
train() client id: f_00005-10-2 loss: 0.808545  [   96/  146]
train() client id: f_00005-10-3 loss: 0.761714  [  128/  146]
train() client id: f_00005-11-0 loss: 0.863116  [   32/  146]
train() client id: f_00005-11-1 loss: 0.680013  [   64/  146]
train() client id: f_00005-11-2 loss: 0.831269  [   96/  146]
train() client id: f_00005-11-3 loss: 0.888981  [  128/  146]
train() client id: f_00006-0-0 loss: 0.463588  [   32/   54]
train() client id: f_00006-1-0 loss: 0.474833  [   32/   54]
train() client id: f_00006-2-0 loss: 0.488086  [   32/   54]
train() client id: f_00006-3-0 loss: 0.507578  [   32/   54]
train() client id: f_00006-4-0 loss: 0.471757  [   32/   54]
train() client id: f_00006-5-0 loss: 0.472775  [   32/   54]
train() client id: f_00006-6-0 loss: 0.496727  [   32/   54]
train() client id: f_00006-7-0 loss: 0.478695  [   32/   54]
train() client id: f_00006-8-0 loss: 0.507872  [   32/   54]
train() client id: f_00006-9-0 loss: 0.488493  [   32/   54]
train() client id: f_00006-10-0 loss: 0.465837  [   32/   54]
train() client id: f_00006-11-0 loss: 0.435763  [   32/   54]
train() client id: f_00007-0-0 loss: 0.764700  [   32/  179]
train() client id: f_00007-0-1 loss: 0.602440  [   64/  179]
train() client id: f_00007-0-2 loss: 0.670101  [   96/  179]
train() client id: f_00007-0-3 loss: 0.361211  [  128/  179]
train() client id: f_00007-0-4 loss: 0.461477  [  160/  179]
train() client id: f_00007-1-0 loss: 0.451049  [   32/  179]
train() client id: f_00007-1-1 loss: 0.685837  [   64/  179]
train() client id: f_00007-1-2 loss: 0.469885  [   96/  179]
train() client id: f_00007-1-3 loss: 0.454313  [  128/  179]
train() client id: f_00007-1-4 loss: 0.643467  [  160/  179]
train() client id: f_00007-2-0 loss: 0.357769  [   32/  179]
train() client id: f_00007-2-1 loss: 0.562297  [   64/  179]
train() client id: f_00007-2-2 loss: 0.695763  [   96/  179]
train() client id: f_00007-2-3 loss: 0.443945  [  128/  179]
train() client id: f_00007-2-4 loss: 0.582433  [  160/  179]
train() client id: f_00007-3-0 loss: 0.442708  [   32/  179]
train() client id: f_00007-3-1 loss: 0.497037  [   64/  179]
train() client id: f_00007-3-2 loss: 0.534174  [   96/  179]
train() client id: f_00007-3-3 loss: 0.613259  [  128/  179]
train() client id: f_00007-3-4 loss: 0.493447  [  160/  179]
train() client id: f_00007-4-0 loss: 0.631917  [   32/  179]
train() client id: f_00007-4-1 loss: 0.354279  [   64/  179]
train() client id: f_00007-4-2 loss: 0.588223  [   96/  179]
train() client id: f_00007-4-3 loss: 0.622114  [  128/  179]
train() client id: f_00007-4-4 loss: 0.357088  [  160/  179]
train() client id: f_00007-5-0 loss: 0.386035  [   32/  179]
train() client id: f_00007-5-1 loss: 0.734559  [   64/  179]
train() client id: f_00007-5-2 loss: 0.442050  [   96/  179]
train() client id: f_00007-5-3 loss: 0.517282  [  128/  179]
train() client id: f_00007-5-4 loss: 0.462493  [  160/  179]
train() client id: f_00007-6-0 loss: 0.602591  [   32/  179]
train() client id: f_00007-6-1 loss: 0.386878  [   64/  179]
train() client id: f_00007-6-2 loss: 0.324474  [   96/  179]
train() client id: f_00007-6-3 loss: 0.546173  [  128/  179]
train() client id: f_00007-6-4 loss: 0.599105  [  160/  179]
train() client id: f_00007-7-0 loss: 0.622773  [   32/  179]
train() client id: f_00007-7-1 loss: 0.457458  [   64/  179]
train() client id: f_00007-7-2 loss: 0.441188  [   96/  179]
train() client id: f_00007-7-3 loss: 0.479203  [  128/  179]
train() client id: f_00007-7-4 loss: 0.472887  [  160/  179]
train() client id: f_00007-8-0 loss: 0.521737  [   32/  179]
train() client id: f_00007-8-1 loss: 0.531947  [   64/  179]
train() client id: f_00007-8-2 loss: 0.657810  [   96/  179]
train() client id: f_00007-8-3 loss: 0.447303  [  128/  179]
train() client id: f_00007-8-4 loss: 0.428762  [  160/  179]
train() client id: f_00007-9-0 loss: 0.678811  [   32/  179]
train() client id: f_00007-9-1 loss: 0.411879  [   64/  179]
train() client id: f_00007-9-2 loss: 0.362737  [   96/  179]
train() client id: f_00007-9-3 loss: 0.446260  [  128/  179]
train() client id: f_00007-9-4 loss: 0.362398  [  160/  179]
train() client id: f_00007-10-0 loss: 0.520437  [   32/  179]
train() client id: f_00007-10-1 loss: 0.619298  [   64/  179]
train() client id: f_00007-10-2 loss: 0.496535  [   96/  179]
train() client id: f_00007-10-3 loss: 0.503314  [  128/  179]
train() client id: f_00007-10-4 loss: 0.351785  [  160/  179]
train() client id: f_00007-11-0 loss: 0.548174  [   32/  179]
train() client id: f_00007-11-1 loss: 0.510625  [   64/  179]
train() client id: f_00007-11-2 loss: 0.427165  [   96/  179]
train() client id: f_00007-11-3 loss: 0.435093  [  128/  179]
train() client id: f_00007-11-4 loss: 0.521601  [  160/  179]
train() client id: f_00008-0-0 loss: 0.723938  [   32/  130]
train() client id: f_00008-0-1 loss: 0.862868  [   64/  130]
train() client id: f_00008-0-2 loss: 0.719943  [   96/  130]
train() client id: f_00008-0-3 loss: 0.824274  [  128/  130]
train() client id: f_00008-1-0 loss: 0.735138  [   32/  130]
train() client id: f_00008-1-1 loss: 0.771601  [   64/  130]
train() client id: f_00008-1-2 loss: 0.749291  [   96/  130]
train() client id: f_00008-1-3 loss: 0.815581  [  128/  130]
train() client id: f_00008-2-0 loss: 0.844115  [   32/  130]
train() client id: f_00008-2-1 loss: 0.831280  [   64/  130]
train() client id: f_00008-2-2 loss: 0.710272  [   96/  130]
train() client id: f_00008-2-3 loss: 0.713850  [  128/  130]
train() client id: f_00008-3-0 loss: 0.736500  [   32/  130]
train() client id: f_00008-3-1 loss: 0.799372  [   64/  130]
train() client id: f_00008-3-2 loss: 0.687708  [   96/  130]
train() client id: f_00008-3-3 loss: 0.836847  [  128/  130]
train() client id: f_00008-4-0 loss: 0.665712  [   32/  130]
train() client id: f_00008-4-1 loss: 0.952432  [   64/  130]
train() client id: f_00008-4-2 loss: 0.760369  [   96/  130]
train() client id: f_00008-4-3 loss: 0.732260  [  128/  130]
train() client id: f_00008-5-0 loss: 0.723145  [   32/  130]
train() client id: f_00008-5-1 loss: 0.792214  [   64/  130]
train() client id: f_00008-5-2 loss: 0.769016  [   96/  130]
train() client id: f_00008-5-3 loss: 0.825663  [  128/  130]
train() client id: f_00008-6-0 loss: 0.691676  [   32/  130]
train() client id: f_00008-6-1 loss: 0.770047  [   64/  130]
train() client id: f_00008-6-2 loss: 0.906980  [   96/  130]
train() client id: f_00008-6-3 loss: 0.740517  [  128/  130]
train() client id: f_00008-7-0 loss: 0.732988  [   32/  130]
train() client id: f_00008-7-1 loss: 0.761732  [   64/  130]
train() client id: f_00008-7-2 loss: 0.841454  [   96/  130]
train() client id: f_00008-7-3 loss: 0.768441  [  128/  130]
train() client id: f_00008-8-0 loss: 0.813535  [   32/  130]
train() client id: f_00008-8-1 loss: 0.673371  [   64/  130]
train() client id: f_00008-8-2 loss: 0.785756  [   96/  130]
train() client id: f_00008-8-3 loss: 0.794696  [  128/  130]
train() client id: f_00008-9-0 loss: 0.664123  [   32/  130]
train() client id: f_00008-9-1 loss: 0.781329  [   64/  130]
train() client id: f_00008-9-2 loss: 0.865205  [   96/  130]
train() client id: f_00008-9-3 loss: 0.753904  [  128/  130]
train() client id: f_00008-10-0 loss: 0.816945  [   32/  130]
train() client id: f_00008-10-1 loss: 0.733902  [   64/  130]
train() client id: f_00008-10-2 loss: 0.663337  [   96/  130]
train() client id: f_00008-10-3 loss: 0.845755  [  128/  130]
train() client id: f_00008-11-0 loss: 0.800867  [   32/  130]
train() client id: f_00008-11-1 loss: 0.742493  [   64/  130]
train() client id: f_00008-11-2 loss: 0.776246  [   96/  130]
train() client id: f_00008-11-3 loss: 0.765156  [  128/  130]
train() client id: f_00009-0-0 loss: 1.262205  [   32/  118]
train() client id: f_00009-0-1 loss: 1.102368  [   64/  118]
train() client id: f_00009-0-2 loss: 1.195900  [   96/  118]
train() client id: f_00009-1-0 loss: 1.091398  [   32/  118]
train() client id: f_00009-1-1 loss: 1.165171  [   64/  118]
train() client id: f_00009-1-2 loss: 1.177746  [   96/  118]
train() client id: f_00009-2-0 loss: 1.054167  [   32/  118]
train() client id: f_00009-2-1 loss: 1.047650  [   64/  118]
train() client id: f_00009-2-2 loss: 1.137045  [   96/  118]
train() client id: f_00009-3-0 loss: 1.043677  [   32/  118]
train() client id: f_00009-3-1 loss: 1.057964  [   64/  118]
train() client id: f_00009-3-2 loss: 1.128982  [   96/  118]
train() client id: f_00009-4-0 loss: 0.980627  [   32/  118]
train() client id: f_00009-4-1 loss: 1.001406  [   64/  118]
train() client id: f_00009-4-2 loss: 0.988227  [   96/  118]
train() client id: f_00009-5-0 loss: 0.915607  [   32/  118]
train() client id: f_00009-5-1 loss: 1.045979  [   64/  118]
train() client id: f_00009-5-2 loss: 1.040048  [   96/  118]
train() client id: f_00009-6-0 loss: 1.027148  [   32/  118]
train() client id: f_00009-6-1 loss: 0.997442  [   64/  118]
train() client id: f_00009-6-2 loss: 0.944279  [   96/  118]
train() client id: f_00009-7-0 loss: 0.875892  [   32/  118]
train() client id: f_00009-7-1 loss: 1.025090  [   64/  118]
train() client id: f_00009-7-2 loss: 0.846536  [   96/  118]
train() client id: f_00009-8-0 loss: 0.901619  [   32/  118]
train() client id: f_00009-8-1 loss: 0.907850  [   64/  118]
train() client id: f_00009-8-2 loss: 0.933299  [   96/  118]
train() client id: f_00009-9-0 loss: 1.010142  [   32/  118]
train() client id: f_00009-9-1 loss: 0.818816  [   64/  118]
train() client id: f_00009-9-2 loss: 0.970848  [   96/  118]
train() client id: f_00009-10-0 loss: 1.057806  [   32/  118]
train() client id: f_00009-10-1 loss: 0.781815  [   64/  118]
train() client id: f_00009-10-2 loss: 0.853660  [   96/  118]
train() client id: f_00009-11-0 loss: 0.974156  [   32/  118]
train() client id: f_00009-11-1 loss: 0.866283  [   64/  118]
train() client id: f_00009-11-2 loss: 0.972716  [   96/  118]
At round 21 accuracy: 0.6339522546419099
At round 21 training accuracy: 0.5855130784708249
At round 21 training loss: 0.8350889895506088
update_location
xs = -3.905658 4.200318 125.009024 18.811294 0.979296 3.956410 -87.443192 -66.324852 109.663977 -52.060879 
ys = 117.587959 100.555839 1.320614 -87.455176 79.350187 62.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 5.288573831882571
ys mean: 29.394142535528708
dists_uav = 154.409140 141.877128 160.090600 134.172548 127.661314 118.157821 132.865354 119.998593 149.448512 112.811112 
uav_gains = -104.723484 -103.799974 -105.119779 -103.192604 -102.651992 -101.811734 -103.086179 -101.979618 -104.366625 -101.308886 
uav_gains_db_mean: -103.20408738739403
dists_bs = 180.469987 194.048157 346.655367 326.259923 200.293783 211.213308 198.032396 205.300127 325.296938 210.605130 
bs_gains = -102.746399 -103.628526 -110.684201 -109.946844 -104.013748 -104.659255 -103.875674 -104.313957 -109.910898 -104.624190 
bs_gains_db_mean: -105.84036916739339
Round 22
-------------------------------
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.08847719 16.82258033  7.96606227  2.85607422 19.40433678  9.34589642
  3.54718528 11.40188453  8.39827017  7.58446914]
obj_prev = 95.4152363247567
eta_min = 3.3224549121166505e-12	eta_max = 0.9228798704318716
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 22.17659221087636	eta = 0.909090909090909
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 38.93866072382321	eta = 0.5177512014733847
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 30.874641848632802	eta = 0.6529804773886541
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.424734721470024	eta = 0.6851561641714188
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.351835344463147	eta = 0.6868578450692009
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.351637432902866	eta = 0.6868624763988199
eta = 0.6868624763988199
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [0.03099536 0.06518865 0.03050337 0.01057778 0.07527445 0.03591526
 0.01328373 0.04403309 0.03197934 0.0290274 ]
ene_total = [2.56014735 4.7872217  2.53839595 1.17632747 5.46154932 2.88284624
 1.35232746 3.3560089  2.80903437 2.42777865]
ti_comp = [0.35997945 0.36414214 0.35835849 0.36568135 0.36272784 0.36023616
 0.36604767 0.36965205 0.33207169 0.36037562]
ti_coms = [0.07833233 0.07416965 0.07995329 0.07263043 0.07558394 0.07807562
 0.07226411 0.06865973 0.1062401  0.07793616]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.43619908e-05 1.30573243e-04 1.38130026e-05 5.53169442e-07
 2.02610025e-04 2.23121998e-05 1.09336312e-06 3.90509212e-05
 1.85363747e-05 1.17704659e-05]
ene_total = [0.51742795 0.49764382 0.5280795  0.47892225 0.51171862 0.51625955
 0.47654255 0.45527992 0.70171191 0.51464497]
optimize_network iter = 0 obj = 5.19823104157566
eta = 0.6868624763988199
freqs = [4.30515682e+07 8.95099024e+07 4.25598540e+07 1.44631133e+07
 1.03761604e+08 4.98496017e+07 1.81448029e+07 5.95601869e+07
 4.81512593e+07 4.02738129e+07]
eta_min = 0.6868624763988167	eta_max = 0.6868624763987461
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 0.02749108426556509	eta = 0.9090909090909091
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 17.828312615008755	eta = 0.0014018093201842298
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.7850725781134285	eta = 0.014000492245133406
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.749550246261795	eta = 0.014284753947636938
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.7495451585936168	eta = 0.014284795487626767
eta = 0.014284795487626767
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.62444973e-04 1.47688208e-03 1.56235501e-04 6.25676453e-06
 2.29167254e-03 2.52367846e-04 1.23667633e-05 4.41695438e-04
 2.09660410e-04 1.33132867e-04]
ene_total = [0.16747587 0.18628874 0.17074105 0.15226131 0.20631753 0.16882169
 0.151622   0.15306496 0.22691989 0.16603211]
ti_comp = [0.35997945 0.36414214 0.35835849 0.36568135 0.36272784 0.36023616
 0.36604767 0.36965205 0.33207169 0.36037562]
ti_coms = [0.07833233 0.07416965 0.07995329 0.07263043 0.07558394 0.07807562
 0.07226411 0.06865973 0.1062401  0.07793616]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.43619908e-05 1.30573243e-04 1.38130026e-05 5.53169442e-07
 2.02610025e-04 2.23121998e-05 1.09336312e-06 3.90509212e-05
 1.85363747e-05 1.17704659e-05]
ene_total = [0.51742795 0.49764382 0.5280795  0.47892225 0.51171862 0.51625955
 0.47654255 0.45527992 0.70171191 0.51464497]
optimize_network iter = 1 obj = 5.198231041575606
eta = 0.6868624763988167
freqs = [4.30515682e+07 8.95099024e+07 4.25598540e+07 1.44631133e+07
 1.03761604e+08 4.98496017e+07 1.81448029e+07 5.95601869e+07
 4.81512593e+07 4.02738129e+07]
Done!
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.40119877e-05 1.27391160e-04 1.34763784e-05 5.39688647e-07
 1.97672399e-04 2.17684492e-05 1.06671775e-06 3.80992464e-05
 1.80846414e-05 1.14836185e-05]
ene_total = [0.00784724 0.00754436 0.00800881 0.00726358 0.00775607 0.00782933
 0.00722748 0.00690407 0.01064209 0.0078051 ]
At round 22 energy consumption: 0.0788281303900666
At round 22 eta: 0.6868624763988167
At round 22 a_n: 20.646594222929394
At round 22 local rounds: 12.299746037466248
At round 22 global rounds: 65.93458997020494
gradient difference: 0.43334540724754333
train() client id: f_00000-0-0 loss: 1.385188  [   32/  126]
train() client id: f_00000-0-1 loss: 1.039567  [   64/  126]
train() client id: f_00000-0-2 loss: 1.074708  [   96/  126]
train() client id: f_00000-1-0 loss: 1.006831  [   32/  126]
train() client id: f_00000-1-1 loss: 1.070146  [   64/  126]
train() client id: f_00000-1-2 loss: 1.083042  [   96/  126]
train() client id: f_00000-2-0 loss: 0.997401  [   32/  126]
train() client id: f_00000-2-1 loss: 0.837986  [   64/  126]
train() client id: f_00000-2-2 loss: 0.919873  [   96/  126]
train() client id: f_00000-3-0 loss: 0.976824  [   32/  126]
train() client id: f_00000-3-1 loss: 1.049106  [   64/  126]
train() client id: f_00000-3-2 loss: 0.789302  [   96/  126]
train() client id: f_00000-4-0 loss: 0.961906  [   32/  126]
train() client id: f_00000-4-1 loss: 0.881505  [   64/  126]
train() client id: f_00000-4-2 loss: 0.916523  [   96/  126]
train() client id: f_00000-5-0 loss: 0.849430  [   32/  126]
train() client id: f_00000-5-1 loss: 0.880612  [   64/  126]
train() client id: f_00000-5-2 loss: 1.021742  [   96/  126]
train() client id: f_00000-6-0 loss: 0.881317  [   32/  126]
train() client id: f_00000-6-1 loss: 0.893802  [   64/  126]
train() client id: f_00000-6-2 loss: 0.908355  [   96/  126]
train() client id: f_00000-7-0 loss: 0.916179  [   32/  126]
train() client id: f_00000-7-1 loss: 0.859280  [   64/  126]
train() client id: f_00000-7-2 loss: 0.869603  [   96/  126]
train() client id: f_00000-8-0 loss: 0.934470  [   32/  126]
train() client id: f_00000-8-1 loss: 0.792490  [   64/  126]
train() client id: f_00000-8-2 loss: 0.932315  [   96/  126]
train() client id: f_00000-9-0 loss: 0.823213  [   32/  126]
train() client id: f_00000-9-1 loss: 0.867627  [   64/  126]
train() client id: f_00000-9-2 loss: 0.873545  [   96/  126]
train() client id: f_00000-10-0 loss: 0.844061  [   32/  126]
train() client id: f_00000-10-1 loss: 0.925617  [   64/  126]
train() client id: f_00000-10-2 loss: 0.896685  [   96/  126]
train() client id: f_00000-11-0 loss: 0.849404  [   32/  126]
train() client id: f_00000-11-1 loss: 0.968266  [   64/  126]
train() client id: f_00000-11-2 loss: 0.948379  [   96/  126]
train() client id: f_00001-0-0 loss: 0.433484  [   32/  265]
train() client id: f_00001-0-1 loss: 0.480340  [   64/  265]
train() client id: f_00001-0-2 loss: 0.478809  [   96/  265]
train() client id: f_00001-0-3 loss: 0.548664  [  128/  265]
train() client id: f_00001-0-4 loss: 0.495087  [  160/  265]
train() client id: f_00001-0-5 loss: 0.491630  [  192/  265]
train() client id: f_00001-0-6 loss: 0.400919  [  224/  265]
train() client id: f_00001-0-7 loss: 0.499153  [  256/  265]
train() client id: f_00001-1-0 loss: 0.422261  [   32/  265]
train() client id: f_00001-1-1 loss: 0.473252  [   64/  265]
train() client id: f_00001-1-2 loss: 0.541455  [   96/  265]
train() client id: f_00001-1-3 loss: 0.448192  [  128/  265]
train() client id: f_00001-1-4 loss: 0.615707  [  160/  265]
train() client id: f_00001-1-5 loss: 0.452738  [  192/  265]
train() client id: f_00001-1-6 loss: 0.412721  [  224/  265]
train() client id: f_00001-1-7 loss: 0.384844  [  256/  265]
train() client id: f_00001-2-0 loss: 0.378289  [   32/  265]
train() client id: f_00001-2-1 loss: 0.524943  [   64/  265]
train() client id: f_00001-2-2 loss: 0.390519  [   96/  265]
train() client id: f_00001-2-3 loss: 0.381369  [  128/  265]
train() client id: f_00001-2-4 loss: 0.552706  [  160/  265]
train() client id: f_00001-2-5 loss: 0.522061  [  192/  265]
train() client id: f_00001-2-6 loss: 0.552826  [  224/  265]
train() client id: f_00001-2-7 loss: 0.439889  [  256/  265]
train() client id: f_00001-3-0 loss: 0.452274  [   32/  265]
train() client id: f_00001-3-1 loss: 0.476061  [   64/  265]
train() client id: f_00001-3-2 loss: 0.512244  [   96/  265]
train() client id: f_00001-3-3 loss: 0.405650  [  128/  265]
train() client id: f_00001-3-4 loss: 0.409828  [  160/  265]
train() client id: f_00001-3-5 loss: 0.474423  [  192/  265]
train() client id: f_00001-3-6 loss: 0.424952  [  224/  265]
train() client id: f_00001-3-7 loss: 0.560640  [  256/  265]
train() client id: f_00001-4-0 loss: 0.489397  [   32/  265]
train() client id: f_00001-4-1 loss: 0.581100  [   64/  265]
train() client id: f_00001-4-2 loss: 0.450837  [   96/  265]
train() client id: f_00001-4-3 loss: 0.480504  [  128/  265]
train() client id: f_00001-4-4 loss: 0.392273  [  160/  265]
train() client id: f_00001-4-5 loss: 0.456108  [  192/  265]
train() client id: f_00001-4-6 loss: 0.459977  [  224/  265]
train() client id: f_00001-4-7 loss: 0.397039  [  256/  265]
train() client id: f_00001-5-0 loss: 0.446064  [   32/  265]
train() client id: f_00001-5-1 loss: 0.392481  [   64/  265]
train() client id: f_00001-5-2 loss: 0.576622  [   96/  265]
train() client id: f_00001-5-3 loss: 0.396978  [  128/  265]
train() client id: f_00001-5-4 loss: 0.349747  [  160/  265]
train() client id: f_00001-5-5 loss: 0.441785  [  192/  265]
train() client id: f_00001-5-6 loss: 0.631053  [  224/  265]
train() client id: f_00001-5-7 loss: 0.423902  [  256/  265]
train() client id: f_00001-6-0 loss: 0.362720  [   32/  265]
train() client id: f_00001-6-1 loss: 0.515245  [   64/  265]
train() client id: f_00001-6-2 loss: 0.421467  [   96/  265]
train() client id: f_00001-6-3 loss: 0.405606  [  128/  265]
train() client id: f_00001-6-4 loss: 0.817557  [  160/  265]
train() client id: f_00001-6-5 loss: 0.431115  [  192/  265]
train() client id: f_00001-6-6 loss: 0.362970  [  224/  265]
train() client id: f_00001-6-7 loss: 0.373624  [  256/  265]
train() client id: f_00001-7-0 loss: 0.362989  [   32/  265]
train() client id: f_00001-7-1 loss: 0.443422  [   64/  265]
train() client id: f_00001-7-2 loss: 0.518121  [   96/  265]
train() client id: f_00001-7-3 loss: 0.338330  [  128/  265]
train() client id: f_00001-7-4 loss: 0.587739  [  160/  265]
train() client id: f_00001-7-5 loss: 0.462301  [  192/  265]
train() client id: f_00001-7-6 loss: 0.352566  [  224/  265]
train() client id: f_00001-7-7 loss: 0.629440  [  256/  265]
train() client id: f_00001-8-0 loss: 0.352899  [   32/  265]
train() client id: f_00001-8-1 loss: 0.484742  [   64/  265]
train() client id: f_00001-8-2 loss: 0.406009  [   96/  265]
train() client id: f_00001-8-3 loss: 0.387348  [  128/  265]
train() client id: f_00001-8-4 loss: 0.559898  [  160/  265]
train() client id: f_00001-8-5 loss: 0.460297  [  192/  265]
train() client id: f_00001-8-6 loss: 0.528113  [  224/  265]
train() client id: f_00001-8-7 loss: 0.380609  [  256/  265]
train() client id: f_00001-9-0 loss: 0.375600  [   32/  265]
train() client id: f_00001-9-1 loss: 0.442356  [   64/  265]
train() client id: f_00001-9-2 loss: 0.471536  [   96/  265]
train() client id: f_00001-9-3 loss: 0.546245  [  128/  265]
train() client id: f_00001-9-4 loss: 0.465231  [  160/  265]
train() client id: f_00001-9-5 loss: 0.537271  [  192/  265]
train() client id: f_00001-9-6 loss: 0.403771  [  224/  265]
train() client id: f_00001-9-7 loss: 0.447131  [  256/  265]
train() client id: f_00001-10-0 loss: 0.435110  [   32/  265]
train() client id: f_00001-10-1 loss: 0.508612  [   64/  265]
train() client id: f_00001-10-2 loss: 0.518231  [   96/  265]
train() client id: f_00001-10-3 loss: 0.457394  [  128/  265]
train() client id: f_00001-10-4 loss: 0.455512  [  160/  265]
train() client id: f_00001-10-5 loss: 0.436722  [  192/  265]
train() client id: f_00001-10-6 loss: 0.471692  [  224/  265]
train() client id: f_00001-10-7 loss: 0.409688  [  256/  265]
train() client id: f_00001-11-0 loss: 0.521719  [   32/  265]
train() client id: f_00001-11-1 loss: 0.483854  [   64/  265]
train() client id: f_00001-11-2 loss: 0.559627  [   96/  265]
train() client id: f_00001-11-3 loss: 0.361501  [  128/  265]
train() client id: f_00001-11-4 loss: 0.509539  [  160/  265]
train() client id: f_00001-11-5 loss: 0.467719  [  192/  265]
train() client id: f_00001-11-6 loss: 0.347594  [  224/  265]
train() client id: f_00001-11-7 loss: 0.431972  [  256/  265]
train() client id: f_00002-0-0 loss: 0.902768  [   32/  124]
train() client id: f_00002-0-1 loss: 1.303079  [   64/  124]
train() client id: f_00002-0-2 loss: 1.112063  [   96/  124]
train() client id: f_00002-1-0 loss: 1.189084  [   32/  124]
train() client id: f_00002-1-1 loss: 1.138221  [   64/  124]
train() client id: f_00002-1-2 loss: 1.134544  [   96/  124]
train() client id: f_00002-2-0 loss: 0.891084  [   32/  124]
train() client id: f_00002-2-1 loss: 1.297915  [   64/  124]
train() client id: f_00002-2-2 loss: 1.012120  [   96/  124]
train() client id: f_00002-3-0 loss: 0.982650  [   32/  124]
train() client id: f_00002-3-1 loss: 1.182784  [   64/  124]
train() client id: f_00002-3-2 loss: 0.948668  [   96/  124]
train() client id: f_00002-4-0 loss: 1.009493  [   32/  124]
train() client id: f_00002-4-1 loss: 0.927755  [   64/  124]
train() client id: f_00002-4-2 loss: 0.848997  [   96/  124]
train() client id: f_00002-5-0 loss: 0.904095  [   32/  124]
train() client id: f_00002-5-1 loss: 1.122047  [   64/  124]
train() client id: f_00002-5-2 loss: 0.988608  [   96/  124]
train() client id: f_00002-6-0 loss: 0.925444  [   32/  124]
train() client id: f_00002-6-1 loss: 0.896853  [   64/  124]
train() client id: f_00002-6-2 loss: 1.081249  [   96/  124]
train() client id: f_00002-7-0 loss: 1.039546  [   32/  124]
train() client id: f_00002-7-1 loss: 0.830799  [   64/  124]
train() client id: f_00002-7-2 loss: 1.016927  [   96/  124]
train() client id: f_00002-8-0 loss: 1.057009  [   32/  124]
train() client id: f_00002-8-1 loss: 0.938868  [   64/  124]
train() client id: f_00002-8-2 loss: 0.819732  [   96/  124]
train() client id: f_00002-9-0 loss: 0.984244  [   32/  124]
train() client id: f_00002-9-1 loss: 0.759469  [   64/  124]
train() client id: f_00002-9-2 loss: 0.902012  [   96/  124]
train() client id: f_00002-10-0 loss: 0.840782  [   32/  124]
train() client id: f_00002-10-1 loss: 0.871474  [   64/  124]
train() client id: f_00002-10-2 loss: 0.852845  [   96/  124]
train() client id: f_00002-11-0 loss: 0.822893  [   32/  124]
train() client id: f_00002-11-1 loss: 0.911345  [   64/  124]
train() client id: f_00002-11-2 loss: 0.923118  [   96/  124]
train() client id: f_00003-0-0 loss: 0.858873  [   32/   43]
train() client id: f_00003-1-0 loss: 1.098824  [   32/   43]
train() client id: f_00003-2-0 loss: 0.906108  [   32/   43]
train() client id: f_00003-3-0 loss: 0.725504  [   32/   43]
train() client id: f_00003-4-0 loss: 0.725481  [   32/   43]
train() client id: f_00003-5-0 loss: 0.771928  [   32/   43]
train() client id: f_00003-6-0 loss: 0.739298  [   32/   43]
train() client id: f_00003-7-0 loss: 0.679662  [   32/   43]
train() client id: f_00003-8-0 loss: 0.950621  [   32/   43]
train() client id: f_00003-9-0 loss: 0.868388  [   32/   43]
train() client id: f_00003-10-0 loss: 0.714788  [   32/   43]
train() client id: f_00003-11-0 loss: 0.909384  [   32/   43]
train() client id: f_00004-0-0 loss: 0.914809  [   32/  306]
train() client id: f_00004-0-1 loss: 0.717008  [   64/  306]
train() client id: f_00004-0-2 loss: 0.937379  [   96/  306]
train() client id: f_00004-0-3 loss: 1.007987  [  128/  306]
train() client id: f_00004-0-4 loss: 0.897128  [  160/  306]
train() client id: f_00004-0-5 loss: 0.900456  [  192/  306]
train() client id: f_00004-0-6 loss: 0.912329  [  224/  306]
train() client id: f_00004-0-7 loss: 0.983626  [  256/  306]
train() client id: f_00004-0-8 loss: 0.862098  [  288/  306]
train() client id: f_00004-1-0 loss: 0.926062  [   32/  306]
train() client id: f_00004-1-1 loss: 0.943139  [   64/  306]
train() client id: f_00004-1-2 loss: 0.880367  [   96/  306]
train() client id: f_00004-1-3 loss: 0.914250  [  128/  306]
train() client id: f_00004-1-4 loss: 0.973457  [  160/  306]
train() client id: f_00004-1-5 loss: 0.853513  [  192/  306]
train() client id: f_00004-1-6 loss: 0.987843  [  224/  306]
train() client id: f_00004-1-7 loss: 0.744632  [  256/  306]
train() client id: f_00004-1-8 loss: 0.877460  [  288/  306]
train() client id: f_00004-2-0 loss: 0.955210  [   32/  306]
train() client id: f_00004-2-1 loss: 0.943186  [   64/  306]
train() client id: f_00004-2-2 loss: 0.853023  [   96/  306]
train() client id: f_00004-2-3 loss: 0.928230  [  128/  306]
train() client id: f_00004-2-4 loss: 0.880641  [  160/  306]
train() client id: f_00004-2-5 loss: 0.851189  [  192/  306]
train() client id: f_00004-2-6 loss: 0.806137  [  224/  306]
train() client id: f_00004-2-7 loss: 0.896265  [  256/  306]
train() client id: f_00004-2-8 loss: 0.948716  [  288/  306]
train() client id: f_00004-3-0 loss: 0.895601  [   32/  306]
train() client id: f_00004-3-1 loss: 0.856573  [   64/  306]
train() client id: f_00004-3-2 loss: 0.850282  [   96/  306]
train() client id: f_00004-3-3 loss: 0.966697  [  128/  306]
train() client id: f_00004-3-4 loss: 0.994948  [  160/  306]
train() client id: f_00004-3-5 loss: 0.881026  [  192/  306]
train() client id: f_00004-3-6 loss: 0.916958  [  224/  306]
train() client id: f_00004-3-7 loss: 0.820121  [  256/  306]
train() client id: f_00004-3-8 loss: 0.897733  [  288/  306]
train() client id: f_00004-4-0 loss: 0.914790  [   32/  306]
train() client id: f_00004-4-1 loss: 0.851753  [   64/  306]
train() client id: f_00004-4-2 loss: 0.777984  [   96/  306]
train() client id: f_00004-4-3 loss: 0.916011  [  128/  306]
train() client id: f_00004-4-4 loss: 0.875746  [  160/  306]
train() client id: f_00004-4-5 loss: 0.874731  [  192/  306]
train() client id: f_00004-4-6 loss: 1.133925  [  224/  306]
train() client id: f_00004-4-7 loss: 0.808796  [  256/  306]
train() client id: f_00004-4-8 loss: 0.857727  [  288/  306]
train() client id: f_00004-5-0 loss: 0.957909  [   32/  306]
train() client id: f_00004-5-1 loss: 0.893305  [   64/  306]
train() client id: f_00004-5-2 loss: 0.927069  [   96/  306]
train() client id: f_00004-5-3 loss: 0.817449  [  128/  306]
train() client id: f_00004-5-4 loss: 0.881592  [  160/  306]
train() client id: f_00004-5-5 loss: 0.898678  [  192/  306]
train() client id: f_00004-5-6 loss: 0.835932  [  224/  306]
train() client id: f_00004-5-7 loss: 0.950674  [  256/  306]
train() client id: f_00004-5-8 loss: 0.845624  [  288/  306]
train() client id: f_00004-6-0 loss: 0.975045  [   32/  306]
train() client id: f_00004-6-1 loss: 0.951024  [   64/  306]
train() client id: f_00004-6-2 loss: 0.889757  [   96/  306]
train() client id: f_00004-6-3 loss: 0.789255  [  128/  306]
train() client id: f_00004-6-4 loss: 0.947956  [  160/  306]
train() client id: f_00004-6-5 loss: 0.784131  [  192/  306]
train() client id: f_00004-6-6 loss: 0.901366  [  224/  306]
train() client id: f_00004-6-7 loss: 0.897018  [  256/  306]
train() client id: f_00004-6-8 loss: 0.932273  [  288/  306]
train() client id: f_00004-7-0 loss: 0.816129  [   32/  306]
train() client id: f_00004-7-1 loss: 0.876885  [   64/  306]
train() client id: f_00004-7-2 loss: 0.877278  [   96/  306]
train() client id: f_00004-7-3 loss: 0.822306  [  128/  306]
train() client id: f_00004-7-4 loss: 0.824522  [  160/  306]
train() client id: f_00004-7-5 loss: 0.927147  [  192/  306]
train() client id: f_00004-7-6 loss: 0.972643  [  224/  306]
train() client id: f_00004-7-7 loss: 0.854300  [  256/  306]
train() client id: f_00004-7-8 loss: 0.961751  [  288/  306]
train() client id: f_00004-8-0 loss: 0.958435  [   32/  306]
train() client id: f_00004-8-1 loss: 0.972741  [   64/  306]
train() client id: f_00004-8-2 loss: 0.944049  [   96/  306]
train() client id: f_00004-8-3 loss: 0.816333  [  128/  306]
train() client id: f_00004-8-4 loss: 0.930650  [  160/  306]
train() client id: f_00004-8-5 loss: 0.890815  [  192/  306]
train() client id: f_00004-8-6 loss: 0.830895  [  224/  306]
train() client id: f_00004-8-7 loss: 0.896777  [  256/  306]
train() client id: f_00004-8-8 loss: 0.749327  [  288/  306]
train() client id: f_00004-9-0 loss: 0.924721  [   32/  306]
train() client id: f_00004-9-1 loss: 1.000621  [   64/  306]
train() client id: f_00004-9-2 loss: 0.948727  [   96/  306]
train() client id: f_00004-9-3 loss: 0.888052  [  128/  306]
train() client id: f_00004-9-4 loss: 0.810308  [  160/  306]
train() client id: f_00004-9-5 loss: 0.980663  [  192/  306]
train() client id: f_00004-9-6 loss: 0.772305  [  224/  306]
train() client id: f_00004-9-7 loss: 0.896223  [  256/  306]
train() client id: f_00004-9-8 loss: 0.797292  [  288/  306]
train() client id: f_00004-10-0 loss: 0.849255  [   32/  306]
train() client id: f_00004-10-1 loss: 0.802725  [   64/  306]
train() client id: f_00004-10-2 loss: 0.905276  [   96/  306]
train() client id: f_00004-10-3 loss: 0.936164  [  128/  306]
train() client id: f_00004-10-4 loss: 0.815236  [  160/  306]
train() client id: f_00004-10-5 loss: 0.889051  [  192/  306]
train() client id: f_00004-10-6 loss: 0.920526  [  224/  306]
train() client id: f_00004-10-7 loss: 0.889932  [  256/  306]
train() client id: f_00004-10-8 loss: 0.964910  [  288/  306]
train() client id: f_00004-11-0 loss: 0.912120  [   32/  306]
train() client id: f_00004-11-1 loss: 0.986341  [   64/  306]
train() client id: f_00004-11-2 loss: 0.809458  [   96/  306]
train() client id: f_00004-11-3 loss: 0.854824  [  128/  306]
train() client id: f_00004-11-4 loss: 0.862515  [  160/  306]
train() client id: f_00004-11-5 loss: 0.900988  [  192/  306]
train() client id: f_00004-11-6 loss: 0.941463  [  224/  306]
train() client id: f_00004-11-7 loss: 0.847608  [  256/  306]
train() client id: f_00004-11-8 loss: 0.764672  [  288/  306]
train() client id: f_00005-0-0 loss: 0.531928  [   32/  146]
train() client id: f_00005-0-1 loss: 0.522470  [   64/  146]
train() client id: f_00005-0-2 loss: 0.500861  [   96/  146]
train() client id: f_00005-0-3 loss: 0.554519  [  128/  146]
train() client id: f_00005-1-0 loss: 0.441305  [   32/  146]
train() client id: f_00005-1-1 loss: 0.541149  [   64/  146]
train() client id: f_00005-1-2 loss: 0.530053  [   96/  146]
train() client id: f_00005-1-3 loss: 0.523549  [  128/  146]
train() client id: f_00005-2-0 loss: 0.340144  [   32/  146]
train() client id: f_00005-2-1 loss: 0.530029  [   64/  146]
train() client id: f_00005-2-2 loss: 0.626014  [   96/  146]
train() client id: f_00005-2-3 loss: 0.826630  [  128/  146]
train() client id: f_00005-3-0 loss: 0.611056  [   32/  146]
train() client id: f_00005-3-1 loss: 0.575765  [   64/  146]
train() client id: f_00005-3-2 loss: 0.484825  [   96/  146]
train() client id: f_00005-3-3 loss: 0.572061  [  128/  146]
train() client id: f_00005-4-0 loss: 0.620217  [   32/  146]
train() client id: f_00005-4-1 loss: 0.297578  [   64/  146]
train() client id: f_00005-4-2 loss: 0.591216  [   96/  146]
train() client id: f_00005-4-3 loss: 0.541711  [  128/  146]
train() client id: f_00005-5-0 loss: 0.409036  [   32/  146]
train() client id: f_00005-5-1 loss: 0.475053  [   64/  146]
train() client id: f_00005-5-2 loss: 0.538197  [   96/  146]
train() client id: f_00005-5-3 loss: 0.754178  [  128/  146]
train() client id: f_00005-6-0 loss: 0.665462  [   32/  146]
train() client id: f_00005-6-1 loss: 0.398839  [   64/  146]
train() client id: f_00005-6-2 loss: 0.471098  [   96/  146]
train() client id: f_00005-6-3 loss: 0.468827  [  128/  146]
train() client id: f_00005-7-0 loss: 0.489266  [   32/  146]
train() client id: f_00005-7-1 loss: 0.518614  [   64/  146]
train() client id: f_00005-7-2 loss: 0.372703  [   96/  146]
train() client id: f_00005-7-3 loss: 0.656670  [  128/  146]
train() client id: f_00005-8-0 loss: 0.504860  [   32/  146]
train() client id: f_00005-8-1 loss: 0.464413  [   64/  146]
train() client id: f_00005-8-2 loss: 0.408810  [   96/  146]
train() client id: f_00005-8-3 loss: 0.580340  [  128/  146]
train() client id: f_00005-9-0 loss: 0.279488  [   32/  146]
train() client id: f_00005-9-1 loss: 0.409211  [   64/  146]
train() client id: f_00005-9-2 loss: 0.818271  [   96/  146]
train() client id: f_00005-9-3 loss: 0.594636  [  128/  146]
train() client id: f_00005-10-0 loss: 0.298372  [   32/  146]
train() client id: f_00005-10-1 loss: 0.628367  [   64/  146]
train() client id: f_00005-10-2 loss: 0.425762  [   96/  146]
train() client id: f_00005-10-3 loss: 0.631029  [  128/  146]
train() client id: f_00005-11-0 loss: 0.572052  [   32/  146]
train() client id: f_00005-11-1 loss: 0.333273  [   64/  146]
train() client id: f_00005-11-2 loss: 0.431585  [   96/  146]
train() client id: f_00005-11-3 loss: 0.607928  [  128/  146]
train() client id: f_00006-0-0 loss: 0.557323  [   32/   54]
train() client id: f_00006-1-0 loss: 0.620494  [   32/   54]
train() client id: f_00006-2-0 loss: 0.550326  [   32/   54]
train() client id: f_00006-3-0 loss: 0.554365  [   32/   54]
train() client id: f_00006-4-0 loss: 0.563207  [   32/   54]
train() client id: f_00006-5-0 loss: 0.613729  [   32/   54]
train() client id: f_00006-6-0 loss: 0.513180  [   32/   54]
train() client id: f_00006-7-0 loss: 0.543006  [   32/   54]
train() client id: f_00006-8-0 loss: 0.619855  [   32/   54]
train() client id: f_00006-9-0 loss: 0.620865  [   32/   54]
train() client id: f_00006-10-0 loss: 0.554531  [   32/   54]
train() client id: f_00006-11-0 loss: 0.556953  [   32/   54]
train() client id: f_00007-0-0 loss: 0.719501  [   32/  179]
train() client id: f_00007-0-1 loss: 0.590663  [   64/  179]
train() client id: f_00007-0-2 loss: 0.407032  [   96/  179]
train() client id: f_00007-0-3 loss: 0.562220  [  128/  179]
train() client id: f_00007-0-4 loss: 0.522564  [  160/  179]
train() client id: f_00007-1-0 loss: 0.483086  [   32/  179]
train() client id: f_00007-1-1 loss: 0.487271  [   64/  179]
train() client id: f_00007-1-2 loss: 0.625753  [   96/  179]
train() client id: f_00007-1-3 loss: 0.612289  [  128/  179]
train() client id: f_00007-1-4 loss: 0.655774  [  160/  179]
train() client id: f_00007-2-0 loss: 0.410923  [   32/  179]
train() client id: f_00007-2-1 loss: 0.377103  [   64/  179]
train() client id: f_00007-2-2 loss: 0.534505  [   96/  179]
train() client id: f_00007-2-3 loss: 0.588435  [  128/  179]
train() client id: f_00007-2-4 loss: 0.667087  [  160/  179]
train() client id: f_00007-3-0 loss: 0.460966  [   32/  179]
train() client id: f_00007-3-1 loss: 0.452607  [   64/  179]
train() client id: f_00007-3-2 loss: 0.670511  [   96/  179]
train() client id: f_00007-3-3 loss: 0.529385  [  128/  179]
train() client id: f_00007-3-4 loss: 0.606813  [  160/  179]
train() client id: f_00007-4-0 loss: 0.487731  [   32/  179]
train() client id: f_00007-4-1 loss: 0.557450  [   64/  179]
train() client id: f_00007-4-2 loss: 0.503586  [   96/  179]
train() client id: f_00007-4-3 loss: 0.532259  [  128/  179]
train() client id: f_00007-4-4 loss: 0.542332  [  160/  179]
train() client id: f_00007-5-0 loss: 0.551192  [   32/  179]
train() client id: f_00007-5-1 loss: 0.389201  [   64/  179]
train() client id: f_00007-5-2 loss: 0.544345  [   96/  179]
train() client id: f_00007-5-3 loss: 0.593570  [  128/  179]
train() client id: f_00007-5-4 loss: 0.428865  [  160/  179]
train() client id: f_00007-6-0 loss: 0.676985  [   32/  179]
train() client id: f_00007-6-1 loss: 0.467827  [   64/  179]
train() client id: f_00007-6-2 loss: 0.447588  [   96/  179]
train() client id: f_00007-6-3 loss: 0.378110  [  128/  179]
train() client id: f_00007-6-4 loss: 0.671677  [  160/  179]
train() client id: f_00007-7-0 loss: 0.362102  [   32/  179]
train() client id: f_00007-7-1 loss: 0.465999  [   64/  179]
train() client id: f_00007-7-2 loss: 0.616381  [   96/  179]
train() client id: f_00007-7-3 loss: 0.451269  [  128/  179]
train() client id: f_00007-7-4 loss: 0.730908  [  160/  179]
train() client id: f_00007-8-0 loss: 0.728538  [   32/  179]
train() client id: f_00007-8-1 loss: 0.506279  [   64/  179]
train() client id: f_00007-8-2 loss: 0.345213  [   96/  179]
train() client id: f_00007-8-3 loss: 0.480258  [  128/  179]
train() client id: f_00007-8-4 loss: 0.552696  [  160/  179]
train() client id: f_00007-9-0 loss: 0.471492  [   32/  179]
train() client id: f_00007-9-1 loss: 0.463127  [   64/  179]
train() client id: f_00007-9-2 loss: 0.502166  [   96/  179]
train() client id: f_00007-9-3 loss: 0.473214  [  128/  179]
train() client id: f_00007-9-4 loss: 0.720742  [  160/  179]
train() client id: f_00007-10-0 loss: 0.436357  [   32/  179]
train() client id: f_00007-10-1 loss: 0.556376  [   64/  179]
train() client id: f_00007-10-2 loss: 0.502480  [   96/  179]
train() client id: f_00007-10-3 loss: 0.458761  [  128/  179]
train() client id: f_00007-10-4 loss: 0.554430  [  160/  179]
train() client id: f_00007-11-0 loss: 0.531909  [   32/  179]
train() client id: f_00007-11-1 loss: 0.354935  [   64/  179]
train() client id: f_00007-11-2 loss: 0.349890  [   96/  179]
train() client id: f_00007-11-3 loss: 0.553313  [  128/  179]
train() client id: f_00007-11-4 loss: 0.518496  [  160/  179]
train() client id: f_00008-0-0 loss: 0.821203  [   32/  130]
train() client id: f_00008-0-1 loss: 0.792296  [   64/  130]
train() client id: f_00008-0-2 loss: 0.748440  [   96/  130]
train() client id: f_00008-0-3 loss: 0.719340  [  128/  130]
train() client id: f_00008-1-0 loss: 0.787075  [   32/  130]
train() client id: f_00008-1-1 loss: 0.766628  [   64/  130]
train() client id: f_00008-1-2 loss: 0.763121  [   96/  130]
train() client id: f_00008-1-3 loss: 0.778203  [  128/  130]
train() client id: f_00008-2-0 loss: 0.771007  [   32/  130]
train() client id: f_00008-2-1 loss: 0.845037  [   64/  130]
train() client id: f_00008-2-2 loss: 0.781448  [   96/  130]
train() client id: f_00008-2-3 loss: 0.659309  [  128/  130]
train() client id: f_00008-3-0 loss: 0.766229  [   32/  130]
train() client id: f_00008-3-1 loss: 0.905374  [   64/  130]
train() client id: f_00008-3-2 loss: 0.703577  [   96/  130]
train() client id: f_00008-3-3 loss: 0.690984  [  128/  130]
train() client id: f_00008-4-0 loss: 0.692716  [   32/  130]
train() client id: f_00008-4-1 loss: 0.828549  [   64/  130]
train() client id: f_00008-4-2 loss: 0.749633  [   96/  130]
train() client id: f_00008-4-3 loss: 0.804739  [  128/  130]
train() client id: f_00008-5-0 loss: 0.824673  [   32/  130]
train() client id: f_00008-5-1 loss: 0.743319  [   64/  130]
train() client id: f_00008-5-2 loss: 0.728647  [   96/  130]
train() client id: f_00008-5-3 loss: 0.805480  [  128/  130]
train() client id: f_00008-6-0 loss: 0.716077  [   32/  130]
train() client id: f_00008-6-1 loss: 0.814909  [   64/  130]
train() client id: f_00008-6-2 loss: 0.730892  [   96/  130]
train() client id: f_00008-6-3 loss: 0.841066  [  128/  130]
train() client id: f_00008-7-0 loss: 0.745779  [   32/  130]
train() client id: f_00008-7-1 loss: 0.738883  [   64/  130]
train() client id: f_00008-7-2 loss: 0.828954  [   96/  130]
train() client id: f_00008-7-3 loss: 0.776066  [  128/  130]
train() client id: f_00008-8-0 loss: 0.808435  [   32/  130]
train() client id: f_00008-8-1 loss: 0.692495  [   64/  130]
train() client id: f_00008-8-2 loss: 0.859752  [   96/  130]
train() client id: f_00008-8-3 loss: 0.745956  [  128/  130]
train() client id: f_00008-9-0 loss: 0.776069  [   32/  130]
train() client id: f_00008-9-1 loss: 0.791540  [   64/  130]
train() client id: f_00008-9-2 loss: 0.739077  [   96/  130]
train() client id: f_00008-9-3 loss: 0.792813  [  128/  130]
train() client id: f_00008-10-0 loss: 0.732639  [   32/  130]
train() client id: f_00008-10-1 loss: 0.778589  [   64/  130]
train() client id: f_00008-10-2 loss: 0.810382  [   96/  130]
train() client id: f_00008-10-3 loss: 0.740922  [  128/  130]
train() client id: f_00008-11-0 loss: 0.754569  [   32/  130]
train() client id: f_00008-11-1 loss: 0.725706  [   64/  130]
train() client id: f_00008-11-2 loss: 0.765394  [   96/  130]
train() client id: f_00008-11-3 loss: 0.809067  [  128/  130]
train() client id: f_00009-0-0 loss: 1.062633  [   32/  118]
train() client id: f_00009-0-1 loss: 0.763819  [   64/  118]
train() client id: f_00009-0-2 loss: 1.042803  [   96/  118]
train() client id: f_00009-1-0 loss: 0.785084  [   32/  118]
train() client id: f_00009-1-1 loss: 0.835449  [   64/  118]
train() client id: f_00009-1-2 loss: 1.053018  [   96/  118]
train() client id: f_00009-2-0 loss: 0.847101  [   32/  118]
train() client id: f_00009-2-1 loss: 0.903065  [   64/  118]
train() client id: f_00009-2-2 loss: 0.852919  [   96/  118]
train() client id: f_00009-3-0 loss: 0.754287  [   32/  118]
train() client id: f_00009-3-1 loss: 0.887483  [   64/  118]
train() client id: f_00009-3-2 loss: 0.836075  [   96/  118]
train() client id: f_00009-4-0 loss: 0.916566  [   32/  118]
train() client id: f_00009-4-1 loss: 0.866118  [   64/  118]
train() client id: f_00009-4-2 loss: 0.773952  [   96/  118]
train() client id: f_00009-5-0 loss: 0.855469  [   32/  118]
train() client id: f_00009-5-1 loss: 0.793969  [   64/  118]
train() client id: f_00009-5-2 loss: 0.697987  [   96/  118]
train() client id: f_00009-6-0 loss: 0.750755  [   32/  118]
train() client id: f_00009-6-1 loss: 0.807273  [   64/  118]
train() client id: f_00009-6-2 loss: 0.856405  [   96/  118]
train() client id: f_00009-7-0 loss: 1.036489  [   32/  118]
train() client id: f_00009-7-1 loss: 0.670298  [   64/  118]
train() client id: f_00009-7-2 loss: 0.699935  [   96/  118]
train() client id: f_00009-8-0 loss: 0.746936  [   32/  118]
train() client id: f_00009-8-1 loss: 0.862908  [   64/  118]
train() client id: f_00009-8-2 loss: 0.758787  [   96/  118]
train() client id: f_00009-9-0 loss: 0.876567  [   32/  118]
train() client id: f_00009-9-1 loss: 0.732885  [   64/  118]
train() client id: f_00009-9-2 loss: 0.788532  [   96/  118]
train() client id: f_00009-10-0 loss: 0.619688  [   32/  118]
train() client id: f_00009-10-1 loss: 0.833885  [   64/  118]
train() client id: f_00009-10-2 loss: 0.948396  [   96/  118]
train() client id: f_00009-11-0 loss: 0.693313  [   32/  118]
train() client id: f_00009-11-1 loss: 0.855717  [   64/  118]
train() client id: f_00009-11-2 loss: 0.755777  [   96/  118]
At round 22 accuracy: 0.6392572944297082
At round 22 training accuracy: 0.5868544600938967
At round 22 training loss: 0.8289636027025424
update_location
xs = -3.905658 4.200318 130.009024 18.811294 0.979296 3.956410 -92.443192 -71.324852 114.663977 -57.060879 
ys = 122.587959 105.555839 1.320614 -92.455176 84.350187 67.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 4.788573831882571
ys mean: 30.894142535528708
dists_uav = 158.249998 145.463665 164.024664 137.483906 130.827799 120.890083 136.208055 122.832857 153.155142 115.203974 
uav_gains = -104.992754 -104.071889 -105.387228 -103.457709 -102.918216 -102.060004 -103.356314 -102.233161 -104.634275 -101.536807 
uav_gains_db_mean: -103.46483581614922
dists_bs = 178.942157 192.185446 350.991501 330.295457 197.954796 208.600582 195.873075 202.697782 329.681266 207.726093 
bs_gains = -102.643014 -103.511233 -110.835364 -110.096332 -103.870908 -104.507894 -103.742352 -104.158831 -110.073699 -104.456809 
bs_gains_db_mean: -105.78964346265349
Round 23
-------------------------------
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.95652636 16.54261616  7.83621819  2.81059366 19.08131306  9.18961195
  3.4902559  11.21434588  8.261358    7.45728421]
obj_prev = 93.84012336038582
eta_min = 2.1552750513700054e-12	eta_max = 0.9232017387246043
af = 19.826056636829264	bf = 1.599357197144509	zeta = 21.808662300512193	eta = 0.909090909090909
af = 19.826056636829264	bf = 1.599357197144509	zeta = 38.37880442012573	eta = 0.5165886987983539
af = 19.826056636829264	bf = 1.599357197144509	zeta = 30.397782640654498	eta = 0.6522204882902731
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.962231155820284	eta = 0.6845486637463356
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.889857955433257	eta = 0.686263555446129
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.889660323483184	eta = 0.6862682501224668
eta = 0.6862682501224668
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [0.03106678 0.06533886 0.03057366 0.01060215 0.07544789 0.03599801
 0.01331433 0.04413455 0.03205303 0.02909429]
ene_total = [2.52460799 4.70588087 2.50342372 1.16220666 5.36855605 2.83112977
 1.33541705 3.30578544 2.76961187 2.3830409 ]
ti_comp = [0.36635294 0.37203036 0.36469503 0.37222065 0.37072619 0.36830251
 0.37257852 0.37632559 0.33835818 0.3685025 ]
ti_coms = [0.07942667 0.07374925 0.08108457 0.07355896 0.07505341 0.0774771
 0.07320109 0.06945402 0.10742142 0.07727711]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.39626984e-05 1.25961536e-04 1.34295805e-05 5.37602806e-07
 1.95305657e-04 2.14934581e-05 1.06268058e-06 3.79392448e-05
 1.79776854e-05 1.13350184e-05]
ene_total = [0.51493605 0.48544134 0.52563117 0.47609264 0.49836942 0.50280622
 0.47381058 0.45194692 0.69637198 0.5008545 ]
optimize_network iter = 0 obj = 5.126260825771548
eta = 0.6862682501224668
freqs = [4.24000684e+07 8.78138843e+07 4.19167432e+07 1.42417608e+07
 1.01756896e+08 4.88701724e+07 1.78678231e+07 5.86387868e+07
 4.73655258e+07 3.94763747e+07]
eta_min = 0.686268250122481	eta_max = 0.6862682501224266
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 0.026043258509324534	eta = 0.9090909090909091
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 17.617751045101738	eta = 0.0013438542463973541
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7582687679572628	eta = 0.01346534158224146
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7245653152193903	eta = 0.013728496882659091
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7245608478782077	eta = 0.013728532445265985
eta = 0.013728532445265985
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.59038678e-04 1.43473385e-03 1.52966329e-04 6.12343236e-06
 2.22458099e-03 2.44815942e-04 1.21042015e-05 4.32137624e-04
 2.04770398e-04 1.29108736e-04]
ene_total = [0.16678716 0.18136157 0.17007523 0.15155948 0.20030673 0.16453951
 0.15094588 0.151879   0.22536052 0.16174577]
ti_comp = [0.36635294 0.37203036 0.36469503 0.37222065 0.37072619 0.36830251
 0.37257852 0.37632559 0.33835818 0.3685025 ]
ti_coms = [0.07942667 0.07374925 0.08108457 0.07355896 0.07505341 0.0774771
 0.07320109 0.06945402 0.10742142 0.07727711]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.39626984e-05 1.25961536e-04 1.34295805e-05 5.37602806e-07
 1.95305657e-04 2.14934581e-05 1.06268058e-06 3.79392448e-05
 1.79776854e-05 1.13350184e-05]
ene_total = [0.51493605 0.48544134 0.52563117 0.47609264 0.49836942 0.50280622
 0.47381058 0.45194692 0.69637198 0.5008545 ]
optimize_network iter = 1 obj = 5.126260825771778
eta = 0.686268250122481
freqs = [4.24000684e+07 8.78138843e+07 4.19167432e+07 1.42417608e+07
 1.01756896e+08 4.88701724e+07 1.78678231e+07 5.86387868e+07
 4.73655258e+07 3.94763747e+07]
Done!
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.35911095e-05 1.22609325e-04 1.30721794e-05 5.23295595e-07
 1.90107992e-04 2.09214532e-05 1.03439949e-06 3.69295686e-05
 1.74992457e-05 1.10333598e-05]
ene_total = [0.00795626 0.00749753 0.00812153 0.00735642 0.00769545 0.00776863
 0.00732114 0.00698233 0.01075964 0.00773874]
At round 23 energy consumption: 0.07919768178596374
At round 23 eta: 0.686268250122481
At round 23 a_n: 20.30404837596008
At round 23 local rounds: 12.328087095105714
At round 23 global rounds: 64.71786290003097
gradient difference: 0.37736648321151733
train() client id: f_00000-0-0 loss: 0.852700  [   32/  126]
train() client id: f_00000-0-1 loss: 1.273443  [   64/  126]
train() client id: f_00000-0-2 loss: 1.277243  [   96/  126]
train() client id: f_00000-1-0 loss: 1.172974  [   32/  126]
train() client id: f_00000-1-1 loss: 1.005450  [   64/  126]
train() client id: f_00000-1-2 loss: 0.903306  [   96/  126]
train() client id: f_00000-2-0 loss: 0.901386  [   32/  126]
train() client id: f_00000-2-1 loss: 1.002012  [   64/  126]
train() client id: f_00000-2-2 loss: 0.951981  [   96/  126]
train() client id: f_00000-3-0 loss: 1.145577  [   32/  126]
train() client id: f_00000-3-1 loss: 0.942312  [   64/  126]
train() client id: f_00000-3-2 loss: 0.730886  [   96/  126]
train() client id: f_00000-4-0 loss: 0.889646  [   32/  126]
train() client id: f_00000-4-1 loss: 0.826565  [   64/  126]
train() client id: f_00000-4-2 loss: 0.887829  [   96/  126]
train() client id: f_00000-5-0 loss: 0.967790  [   32/  126]
train() client id: f_00000-5-1 loss: 0.863979  [   64/  126]
train() client id: f_00000-5-2 loss: 0.820736  [   96/  126]
train() client id: f_00000-6-0 loss: 0.688285  [   32/  126]
train() client id: f_00000-6-1 loss: 0.891250  [   64/  126]
train() client id: f_00000-6-2 loss: 0.877555  [   96/  126]
train() client id: f_00000-7-0 loss: 0.812304  [   32/  126]
train() client id: f_00000-7-1 loss: 0.824521  [   64/  126]
train() client id: f_00000-7-2 loss: 0.847564  [   96/  126]
train() client id: f_00000-8-0 loss: 0.845413  [   32/  126]
train() client id: f_00000-8-1 loss: 0.778315  [   64/  126]
train() client id: f_00000-8-2 loss: 0.833709  [   96/  126]
train() client id: f_00000-9-0 loss: 0.701714  [   32/  126]
train() client id: f_00000-9-1 loss: 0.880891  [   64/  126]
train() client id: f_00000-9-2 loss: 0.893678  [   96/  126]
train() client id: f_00000-10-0 loss: 0.869074  [   32/  126]
train() client id: f_00000-10-1 loss: 0.782143  [   64/  126]
train() client id: f_00000-10-2 loss: 0.700659  [   96/  126]
train() client id: f_00000-11-0 loss: 0.687076  [   32/  126]
train() client id: f_00000-11-1 loss: 0.763221  [   64/  126]
train() client id: f_00000-11-2 loss: 0.907418  [   96/  126]
train() client id: f_00001-0-0 loss: 0.515699  [   32/  265]
train() client id: f_00001-0-1 loss: 0.606148  [   64/  265]
train() client id: f_00001-0-2 loss: 0.483399  [   96/  265]
train() client id: f_00001-0-3 loss: 0.393043  [  128/  265]
train() client id: f_00001-0-4 loss: 0.510390  [  160/  265]
train() client id: f_00001-0-5 loss: 0.397876  [  192/  265]
train() client id: f_00001-0-6 loss: 0.441796  [  224/  265]
train() client id: f_00001-0-7 loss: 0.450516  [  256/  265]
train() client id: f_00001-1-0 loss: 0.444048  [   32/  265]
train() client id: f_00001-1-1 loss: 0.379284  [   64/  265]
train() client id: f_00001-1-2 loss: 0.467059  [   96/  265]
train() client id: f_00001-1-3 loss: 0.529189  [  128/  265]
train() client id: f_00001-1-4 loss: 0.429323  [  160/  265]
train() client id: f_00001-1-5 loss: 0.543443  [  192/  265]
train() client id: f_00001-1-6 loss: 0.375480  [  224/  265]
train() client id: f_00001-1-7 loss: 0.573553  [  256/  265]
train() client id: f_00001-2-0 loss: 0.424767  [   32/  265]
train() client id: f_00001-2-1 loss: 0.555373  [   64/  265]
train() client id: f_00001-2-2 loss: 0.372717  [   96/  265]
train() client id: f_00001-2-3 loss: 0.594633  [  128/  265]
train() client id: f_00001-2-4 loss: 0.430160  [  160/  265]
train() client id: f_00001-2-5 loss: 0.405687  [  192/  265]
train() client id: f_00001-2-6 loss: 0.505849  [  224/  265]
train() client id: f_00001-2-7 loss: 0.388022  [  256/  265]
train() client id: f_00001-3-0 loss: 0.475451  [   32/  265]
train() client id: f_00001-3-1 loss: 0.429307  [   64/  265]
train() client id: f_00001-3-2 loss: 0.466948  [   96/  265]
train() client id: f_00001-3-3 loss: 0.540092  [  128/  265]
train() client id: f_00001-3-4 loss: 0.387892  [  160/  265]
train() client id: f_00001-3-5 loss: 0.428208  [  192/  265]
train() client id: f_00001-3-6 loss: 0.573284  [  224/  265]
train() client id: f_00001-3-7 loss: 0.361726  [  256/  265]
train() client id: f_00001-4-0 loss: 0.428329  [   32/  265]
train() client id: f_00001-4-1 loss: 0.663348  [   64/  265]
train() client id: f_00001-4-2 loss: 0.515851  [   96/  265]
train() client id: f_00001-4-3 loss: 0.338828  [  128/  265]
train() client id: f_00001-4-4 loss: 0.434806  [  160/  265]
train() client id: f_00001-4-5 loss: 0.371480  [  192/  265]
train() client id: f_00001-4-6 loss: 0.388861  [  224/  265]
train() client id: f_00001-4-7 loss: 0.485679  [  256/  265]
train() client id: f_00001-5-0 loss: 0.346788  [   32/  265]
train() client id: f_00001-5-1 loss: 0.387862  [   64/  265]
train() client id: f_00001-5-2 loss: 0.499344  [   96/  265]
train() client id: f_00001-5-3 loss: 0.451884  [  128/  265]
train() client id: f_00001-5-4 loss: 0.624202  [  160/  265]
train() client id: f_00001-5-5 loss: 0.337633  [  192/  265]
train() client id: f_00001-5-6 loss: 0.503585  [  224/  265]
train() client id: f_00001-5-7 loss: 0.418619  [  256/  265]
train() client id: f_00001-6-0 loss: 0.500560  [   32/  265]
train() client id: f_00001-6-1 loss: 0.436951  [   64/  265]
train() client id: f_00001-6-2 loss: 0.437775  [   96/  265]
train() client id: f_00001-6-3 loss: 0.431555  [  128/  265]
train() client id: f_00001-6-4 loss: 0.347903  [  160/  265]
train() client id: f_00001-6-5 loss: 0.362041  [  192/  265]
train() client id: f_00001-6-6 loss: 0.483342  [  224/  265]
train() client id: f_00001-6-7 loss: 0.605784  [  256/  265]
train() client id: f_00001-7-0 loss: 0.415910  [   32/  265]
train() client id: f_00001-7-1 loss: 0.373903  [   64/  265]
train() client id: f_00001-7-2 loss: 0.443135  [   96/  265]
train() client id: f_00001-7-3 loss: 0.410420  [  128/  265]
train() client id: f_00001-7-4 loss: 0.409065  [  160/  265]
train() client id: f_00001-7-5 loss: 0.586972  [  192/  265]
train() client id: f_00001-7-6 loss: 0.478773  [  224/  265]
train() client id: f_00001-7-7 loss: 0.489153  [  256/  265]
train() client id: f_00001-8-0 loss: 0.354315  [   32/  265]
train() client id: f_00001-8-1 loss: 0.477861  [   64/  265]
train() client id: f_00001-8-2 loss: 0.439027  [   96/  265]
train() client id: f_00001-8-3 loss: 0.393966  [  128/  265]
train() client id: f_00001-8-4 loss: 0.466281  [  160/  265]
train() client id: f_00001-8-5 loss: 0.650952  [  192/  265]
train() client id: f_00001-8-6 loss: 0.396511  [  224/  265]
train() client id: f_00001-8-7 loss: 0.420215  [  256/  265]
train() client id: f_00001-9-0 loss: 0.366975  [   32/  265]
train() client id: f_00001-9-1 loss: 0.611870  [   64/  265]
train() client id: f_00001-9-2 loss: 0.480055  [   96/  265]
train() client id: f_00001-9-3 loss: 0.489858  [  128/  265]
train() client id: f_00001-9-4 loss: 0.398035  [  160/  265]
train() client id: f_00001-9-5 loss: 0.422394  [  192/  265]
train() client id: f_00001-9-6 loss: 0.440519  [  224/  265]
train() client id: f_00001-9-7 loss: 0.394308  [  256/  265]
train() client id: f_00001-10-0 loss: 0.444295  [   32/  265]
train() client id: f_00001-10-1 loss: 0.540846  [   64/  265]
train() client id: f_00001-10-2 loss: 0.351714  [   96/  265]
train() client id: f_00001-10-3 loss: 0.359141  [  128/  265]
train() client id: f_00001-10-4 loss: 0.450834  [  160/  265]
train() client id: f_00001-10-5 loss: 0.432387  [  192/  265]
train() client id: f_00001-10-6 loss: 0.599802  [  224/  265]
train() client id: f_00001-10-7 loss: 0.421737  [  256/  265]
train() client id: f_00001-11-0 loss: 0.449759  [   32/  265]
train() client id: f_00001-11-1 loss: 0.389098  [   64/  265]
train() client id: f_00001-11-2 loss: 0.517448  [   96/  265]
train() client id: f_00001-11-3 loss: 0.358563  [  128/  265]
train() client id: f_00001-11-4 loss: 0.538883  [  160/  265]
train() client id: f_00001-11-5 loss: 0.463101  [  192/  265]
train() client id: f_00001-11-6 loss: 0.383083  [  224/  265]
train() client id: f_00001-11-7 loss: 0.508670  [  256/  265]
train() client id: f_00002-0-0 loss: 1.401316  [   32/  124]
train() client id: f_00002-0-1 loss: 0.982692  [   64/  124]
train() client id: f_00002-0-2 loss: 1.067954  [   96/  124]
train() client id: f_00002-1-0 loss: 1.067586  [   32/  124]
train() client id: f_00002-1-1 loss: 1.141050  [   64/  124]
train() client id: f_00002-1-2 loss: 1.043143  [   96/  124]
train() client id: f_00002-2-0 loss: 1.121794  [   32/  124]
train() client id: f_00002-2-1 loss: 0.973811  [   64/  124]
train() client id: f_00002-2-2 loss: 1.212501  [   96/  124]
train() client id: f_00002-3-0 loss: 1.184502  [   32/  124]
train() client id: f_00002-3-1 loss: 0.898442  [   64/  124]
train() client id: f_00002-3-2 loss: 1.003518  [   96/  124]
train() client id: f_00002-4-0 loss: 1.182598  [   32/  124]
train() client id: f_00002-4-1 loss: 0.890176  [   64/  124]
train() client id: f_00002-4-2 loss: 0.958561  [   96/  124]
train() client id: f_00002-5-0 loss: 0.997131  [   32/  124]
train() client id: f_00002-5-1 loss: 0.984915  [   64/  124]
train() client id: f_00002-5-2 loss: 1.081264  [   96/  124]
train() client id: f_00002-6-0 loss: 0.805847  [   32/  124]
train() client id: f_00002-6-1 loss: 1.087523  [   64/  124]
train() client id: f_00002-6-2 loss: 0.810958  [   96/  124]
train() client id: f_00002-7-0 loss: 1.007069  [   32/  124]
train() client id: f_00002-7-1 loss: 0.814650  [   64/  124]
train() client id: f_00002-7-2 loss: 0.880034  [   96/  124]
train() client id: f_00002-8-0 loss: 0.782215  [   32/  124]
train() client id: f_00002-8-1 loss: 0.983447  [   64/  124]
train() client id: f_00002-8-2 loss: 1.005392  [   96/  124]
train() client id: f_00002-9-0 loss: 0.851611  [   32/  124]
train() client id: f_00002-9-1 loss: 0.902607  [   64/  124]
train() client id: f_00002-9-2 loss: 0.944663  [   96/  124]
train() client id: f_00002-10-0 loss: 0.808175  [   32/  124]
train() client id: f_00002-10-1 loss: 0.740145  [   64/  124]
train() client id: f_00002-10-2 loss: 0.993719  [   96/  124]
train() client id: f_00002-11-0 loss: 1.034096  [   32/  124]
train() client id: f_00002-11-1 loss: 0.898043  [   64/  124]
train() client id: f_00002-11-2 loss: 0.877381  [   96/  124]
train() client id: f_00003-0-0 loss: 0.862789  [   32/   43]
train() client id: f_00003-1-0 loss: 0.791673  [   32/   43]
train() client id: f_00003-2-0 loss: 0.624833  [   32/   43]
train() client id: f_00003-3-0 loss: 0.656614  [   32/   43]
train() client id: f_00003-4-0 loss: 0.773586  [   32/   43]
train() client id: f_00003-5-0 loss: 0.808627  [   32/   43]
train() client id: f_00003-6-0 loss: 0.724440  [   32/   43]
train() client id: f_00003-7-0 loss: 0.523312  [   32/   43]
train() client id: f_00003-8-0 loss: 0.676762  [   32/   43]
train() client id: f_00003-9-0 loss: 0.801605  [   32/   43]
train() client id: f_00003-10-0 loss: 0.779562  [   32/   43]
train() client id: f_00003-11-0 loss: 0.612587  [   32/   43]
train() client id: f_00004-0-0 loss: 0.848619  [   32/  306]
train() client id: f_00004-0-1 loss: 0.974904  [   64/  306]
train() client id: f_00004-0-2 loss: 0.976880  [   96/  306]
train() client id: f_00004-0-3 loss: 1.113275  [  128/  306]
train() client id: f_00004-0-4 loss: 0.876276  [  160/  306]
train() client id: f_00004-0-5 loss: 0.868611  [  192/  306]
train() client id: f_00004-0-6 loss: 0.811773  [  224/  306]
train() client id: f_00004-0-7 loss: 0.821500  [  256/  306]
train() client id: f_00004-0-8 loss: 0.897884  [  288/  306]
train() client id: f_00004-1-0 loss: 1.020881  [   32/  306]
train() client id: f_00004-1-1 loss: 0.831628  [   64/  306]
train() client id: f_00004-1-2 loss: 1.009917  [   96/  306]
train() client id: f_00004-1-3 loss: 0.927588  [  128/  306]
train() client id: f_00004-1-4 loss: 0.863326  [  160/  306]
train() client id: f_00004-1-5 loss: 0.906037  [  192/  306]
train() client id: f_00004-1-6 loss: 0.917497  [  224/  306]
train() client id: f_00004-1-7 loss: 0.859793  [  256/  306]
train() client id: f_00004-1-8 loss: 0.966621  [  288/  306]
train() client id: f_00004-2-0 loss: 1.058185  [   32/  306]
train() client id: f_00004-2-1 loss: 0.948129  [   64/  306]
train() client id: f_00004-2-2 loss: 0.884386  [   96/  306]
train() client id: f_00004-2-3 loss: 1.002704  [  128/  306]
train() client id: f_00004-2-4 loss: 0.901977  [  160/  306]
train() client id: f_00004-2-5 loss: 0.790134  [  192/  306]
train() client id: f_00004-2-6 loss: 0.865293  [  224/  306]
train() client id: f_00004-2-7 loss: 0.817310  [  256/  306]
train() client id: f_00004-2-8 loss: 0.947785  [  288/  306]
train() client id: f_00004-3-0 loss: 0.903490  [   32/  306]
train() client id: f_00004-3-1 loss: 0.911990  [   64/  306]
train() client id: f_00004-3-2 loss: 0.944939  [   96/  306]
train() client id: f_00004-3-3 loss: 0.881400  [  128/  306]
train() client id: f_00004-3-4 loss: 0.903697  [  160/  306]
train() client id: f_00004-3-5 loss: 1.012954  [  192/  306]
train() client id: f_00004-3-6 loss: 0.901900  [  224/  306]
train() client id: f_00004-3-7 loss: 0.888861  [  256/  306]
train() client id: f_00004-3-8 loss: 0.913928  [  288/  306]
train() client id: f_00004-4-0 loss: 0.931750  [   32/  306]
train() client id: f_00004-4-1 loss: 0.835225  [   64/  306]
train() client id: f_00004-4-2 loss: 0.899893  [   96/  306]
train() client id: f_00004-4-3 loss: 0.957059  [  128/  306]
train() client id: f_00004-4-4 loss: 1.124331  [  160/  306]
train() client id: f_00004-4-5 loss: 0.913515  [  192/  306]
train() client id: f_00004-4-6 loss: 0.913987  [  224/  306]
train() client id: f_00004-4-7 loss: 0.871445  [  256/  306]
train() client id: f_00004-4-8 loss: 0.786780  [  288/  306]
train() client id: f_00004-5-0 loss: 0.862012  [   32/  306]
train() client id: f_00004-5-1 loss: 1.012339  [   64/  306]
train() client id: f_00004-5-2 loss: 0.932117  [   96/  306]
train() client id: f_00004-5-3 loss: 1.018167  [  128/  306]
train() client id: f_00004-5-4 loss: 0.859971  [  160/  306]
train() client id: f_00004-5-5 loss: 0.875474  [  192/  306]
train() client id: f_00004-5-6 loss: 0.899819  [  224/  306]
train() client id: f_00004-5-7 loss: 0.788353  [  256/  306]
train() client id: f_00004-5-8 loss: 0.922817  [  288/  306]
train() client id: f_00004-6-0 loss: 0.943987  [   32/  306]
train() client id: f_00004-6-1 loss: 1.004336  [   64/  306]
train() client id: f_00004-6-2 loss: 0.879648  [   96/  306]
train() client id: f_00004-6-3 loss: 0.831614  [  128/  306]
train() client id: f_00004-6-4 loss: 0.886302  [  160/  306]
train() client id: f_00004-6-5 loss: 0.910587  [  192/  306]
train() client id: f_00004-6-6 loss: 0.887796  [  224/  306]
train() client id: f_00004-6-7 loss: 0.925977  [  256/  306]
train() client id: f_00004-6-8 loss: 0.916939  [  288/  306]
train() client id: f_00004-7-0 loss: 0.864699  [   32/  306]
train() client id: f_00004-7-1 loss: 0.927096  [   64/  306]
train() client id: f_00004-7-2 loss: 1.071717  [   96/  306]
train() client id: f_00004-7-3 loss: 0.915347  [  128/  306]
train() client id: f_00004-7-4 loss: 0.765153  [  160/  306]
train() client id: f_00004-7-5 loss: 1.003391  [  192/  306]
train() client id: f_00004-7-6 loss: 0.999320  [  224/  306]
train() client id: f_00004-7-7 loss: 0.857644  [  256/  306]
train() client id: f_00004-7-8 loss: 0.913710  [  288/  306]
train() client id: f_00004-8-0 loss: 0.863777  [   32/  306]
train() client id: f_00004-8-1 loss: 0.947778  [   64/  306]
train() client id: f_00004-8-2 loss: 0.821304  [   96/  306]
train() client id: f_00004-8-3 loss: 0.742041  [  128/  306]
train() client id: f_00004-8-4 loss: 0.949307  [  160/  306]
train() client id: f_00004-8-5 loss: 1.005570  [  192/  306]
train() client id: f_00004-8-6 loss: 0.902049  [  224/  306]
train() client id: f_00004-8-7 loss: 1.023392  [  256/  306]
train() client id: f_00004-8-8 loss: 0.879223  [  288/  306]
train() client id: f_00004-9-0 loss: 1.028822  [   32/  306]
train() client id: f_00004-9-1 loss: 0.848663  [   64/  306]
train() client id: f_00004-9-2 loss: 0.941314  [   96/  306]
train() client id: f_00004-9-3 loss: 0.826849  [  128/  306]
train() client id: f_00004-9-4 loss: 0.913082  [  160/  306]
train() client id: f_00004-9-5 loss: 0.928895  [  192/  306]
train() client id: f_00004-9-6 loss: 0.946233  [  224/  306]
train() client id: f_00004-9-7 loss: 0.901681  [  256/  306]
train() client id: f_00004-9-8 loss: 0.875588  [  288/  306]
train() client id: f_00004-10-0 loss: 0.828599  [   32/  306]
train() client id: f_00004-10-1 loss: 0.963608  [   64/  306]
train() client id: f_00004-10-2 loss: 0.956670  [   96/  306]
train() client id: f_00004-10-3 loss: 0.943468  [  128/  306]
train() client id: f_00004-10-4 loss: 0.788074  [  160/  306]
train() client id: f_00004-10-5 loss: 0.966922  [  192/  306]
train() client id: f_00004-10-6 loss: 0.973082  [  224/  306]
train() client id: f_00004-10-7 loss: 0.842453  [  256/  306]
train() client id: f_00004-10-8 loss: 0.845302  [  288/  306]
train() client id: f_00004-11-0 loss: 0.867622  [   32/  306]
train() client id: f_00004-11-1 loss: 0.919058  [   64/  306]
train() client id: f_00004-11-2 loss: 0.897412  [   96/  306]
train() client id: f_00004-11-3 loss: 1.028556  [  128/  306]
train() client id: f_00004-11-4 loss: 0.936976  [  160/  306]
train() client id: f_00004-11-5 loss: 0.806458  [  192/  306]
train() client id: f_00004-11-6 loss: 0.808297  [  224/  306]
train() client id: f_00004-11-7 loss: 0.890923  [  256/  306]
train() client id: f_00004-11-8 loss: 0.946465  [  288/  306]
train() client id: f_00005-0-0 loss: 0.408380  [   32/  146]
train() client id: f_00005-0-1 loss: 0.740697  [   64/  146]
train() client id: f_00005-0-2 loss: 0.766896  [   96/  146]
train() client id: f_00005-0-3 loss: 0.399850  [  128/  146]
train() client id: f_00005-1-0 loss: 0.353539  [   32/  146]
train() client id: f_00005-1-1 loss: 0.476628  [   64/  146]
train() client id: f_00005-1-2 loss: 0.899850  [   96/  146]
train() client id: f_00005-1-3 loss: 0.630389  [  128/  146]
train() client id: f_00005-2-0 loss: 0.664492  [   32/  146]
train() client id: f_00005-2-1 loss: 0.627063  [   64/  146]
train() client id: f_00005-2-2 loss: 0.493121  [   96/  146]
train() client id: f_00005-2-3 loss: 0.599907  [  128/  146]
train() client id: f_00005-3-0 loss: 0.681264  [   32/  146]
train() client id: f_00005-3-1 loss: 0.554731  [   64/  146]
train() client id: f_00005-3-2 loss: 0.594443  [   96/  146]
train() client id: f_00005-3-3 loss: 0.469779  [  128/  146]
train() client id: f_00005-4-0 loss: 0.585037  [   32/  146]
train() client id: f_00005-4-1 loss: 0.507389  [   64/  146]
train() client id: f_00005-4-2 loss: 0.520363  [   96/  146]
train() client id: f_00005-4-3 loss: 0.684096  [  128/  146]
train() client id: f_00005-5-0 loss: 0.496331  [   32/  146]
train() client id: f_00005-5-1 loss: 0.577691  [   64/  146]
train() client id: f_00005-5-2 loss: 0.666540  [   96/  146]
train() client id: f_00005-5-3 loss: 0.618814  [  128/  146]
train() client id: f_00005-6-0 loss: 0.411565  [   32/  146]
train() client id: f_00005-6-1 loss: 0.434091  [   64/  146]
train() client id: f_00005-6-2 loss: 0.547847  [   96/  146]
train() client id: f_00005-6-3 loss: 0.921848  [  128/  146]
train() client id: f_00005-7-0 loss: 0.528913  [   32/  146]
train() client id: f_00005-7-1 loss: 0.637496  [   64/  146]
train() client id: f_00005-7-2 loss: 0.529954  [   96/  146]
train() client id: f_00005-7-3 loss: 0.431060  [  128/  146]
train() client id: f_00005-8-0 loss: 0.651817  [   32/  146]
train() client id: f_00005-8-1 loss: 0.582053  [   64/  146]
train() client id: f_00005-8-2 loss: 0.625595  [   96/  146]
train() client id: f_00005-8-3 loss: 0.570702  [  128/  146]
train() client id: f_00005-9-0 loss: 0.354192  [   32/  146]
train() client id: f_00005-9-1 loss: 0.643505  [   64/  146]
train() client id: f_00005-9-2 loss: 0.476688  [   96/  146]
train() client id: f_00005-9-3 loss: 0.820326  [  128/  146]
train() client id: f_00005-10-0 loss: 0.343022  [   32/  146]
train() client id: f_00005-10-1 loss: 0.762470  [   64/  146]
train() client id: f_00005-10-2 loss: 0.823546  [   96/  146]
train() client id: f_00005-10-3 loss: 0.321184  [  128/  146]
train() client id: f_00005-11-0 loss: 0.577384  [   32/  146]
train() client id: f_00005-11-1 loss: 0.776006  [   64/  146]
train() client id: f_00005-11-2 loss: 0.229856  [   96/  146]
train() client id: f_00005-11-3 loss: 0.720236  [  128/  146]
train() client id: f_00006-0-0 loss: 0.549428  [   32/   54]
train() client id: f_00006-1-0 loss: 0.513181  [   32/   54]
train() client id: f_00006-2-0 loss: 0.528818  [   32/   54]
train() client id: f_00006-3-0 loss: 0.563981  [   32/   54]
train() client id: f_00006-4-0 loss: 0.522820  [   32/   54]
train() client id: f_00006-5-0 loss: 0.564435  [   32/   54]
train() client id: f_00006-6-0 loss: 0.573360  [   32/   54]
train() client id: f_00006-7-0 loss: 0.562675  [   32/   54]
train() client id: f_00006-8-0 loss: 0.564004  [   32/   54]
train() client id: f_00006-9-0 loss: 0.586802  [   32/   54]
train() client id: f_00006-10-0 loss: 0.564540  [   32/   54]
train() client id: f_00006-11-0 loss: 0.571392  [   32/   54]
train() client id: f_00007-0-0 loss: 0.660728  [   32/  179]
train() client id: f_00007-0-1 loss: 0.611524  [   64/  179]
train() client id: f_00007-0-2 loss: 0.608413  [   96/  179]
train() client id: f_00007-0-3 loss: 0.492559  [  128/  179]
train() client id: f_00007-0-4 loss: 0.497473  [  160/  179]
train() client id: f_00007-1-0 loss: 0.685818  [   32/  179]
train() client id: f_00007-1-1 loss: 0.595637  [   64/  179]
train() client id: f_00007-1-2 loss: 0.591395  [   96/  179]
train() client id: f_00007-1-3 loss: 0.459733  [  128/  179]
train() client id: f_00007-1-4 loss: 0.409365  [  160/  179]
train() client id: f_00007-2-0 loss: 0.387417  [   32/  179]
train() client id: f_00007-2-1 loss: 0.742458  [   64/  179]
train() client id: f_00007-2-2 loss: 0.735990  [   96/  179]
train() client id: f_00007-2-3 loss: 0.516087  [  128/  179]
train() client id: f_00007-2-4 loss: 0.419881  [  160/  179]
train() client id: f_00007-3-0 loss: 0.717422  [   32/  179]
train() client id: f_00007-3-1 loss: 0.438154  [   64/  179]
train() client id: f_00007-3-2 loss: 0.569496  [   96/  179]
train() client id: f_00007-3-3 loss: 0.400061  [  128/  179]
train() client id: f_00007-3-4 loss: 0.412490  [  160/  179]
train() client id: f_00007-4-0 loss: 0.720506  [   32/  179]
train() client id: f_00007-4-1 loss: 0.623187  [   64/  179]
train() client id: f_00007-4-2 loss: 0.497410  [   96/  179]
train() client id: f_00007-4-3 loss: 0.426246  [  128/  179]
train() client id: f_00007-4-4 loss: 0.505346  [  160/  179]
train() client id: f_00007-5-0 loss: 0.494196  [   32/  179]
train() client id: f_00007-5-1 loss: 0.771513  [   64/  179]
train() client id: f_00007-5-2 loss: 0.648322  [   96/  179]
train() client id: f_00007-5-3 loss: 0.436938  [  128/  179]
train() client id: f_00007-5-4 loss: 0.391389  [  160/  179]
train() client id: f_00007-6-0 loss: 0.510592  [   32/  179]
train() client id: f_00007-6-1 loss: 0.462123  [   64/  179]
train() client id: f_00007-6-2 loss: 0.411983  [   96/  179]
train() client id: f_00007-6-3 loss: 0.400180  [  128/  179]
train() client id: f_00007-6-4 loss: 0.740436  [  160/  179]
train() client id: f_00007-7-0 loss: 0.405377  [   32/  179]
train() client id: f_00007-7-1 loss: 0.716844  [   64/  179]
train() client id: f_00007-7-2 loss: 0.373269  [   96/  179]
train() client id: f_00007-7-3 loss: 0.626537  [  128/  179]
train() client id: f_00007-7-4 loss: 0.326193  [  160/  179]
train() client id: f_00007-8-0 loss: 0.465956  [   32/  179]
train() client id: f_00007-8-1 loss: 0.489441  [   64/  179]
train() client id: f_00007-8-2 loss: 0.607968  [   96/  179]
train() client id: f_00007-8-3 loss: 0.534811  [  128/  179]
train() client id: f_00007-8-4 loss: 0.509064  [  160/  179]
train() client id: f_00007-9-0 loss: 0.570720  [   32/  179]
train() client id: f_00007-9-1 loss: 0.698097  [   64/  179]
train() client id: f_00007-9-2 loss: 0.369339  [   96/  179]
train() client id: f_00007-9-3 loss: 0.463688  [  128/  179]
train() client id: f_00007-9-4 loss: 0.381312  [  160/  179]
train() client id: f_00007-10-0 loss: 0.455280  [   32/  179]
train() client id: f_00007-10-1 loss: 0.623025  [   64/  179]
train() client id: f_00007-10-2 loss: 0.640132  [   96/  179]
train() client id: f_00007-10-3 loss: 0.546530  [  128/  179]
train() client id: f_00007-10-4 loss: 0.351458  [  160/  179]
train() client id: f_00007-11-0 loss: 0.437631  [   32/  179]
train() client id: f_00007-11-1 loss: 0.602262  [   64/  179]
train() client id: f_00007-11-2 loss: 0.353870  [   96/  179]
train() client id: f_00007-11-3 loss: 0.589311  [  128/  179]
train() client id: f_00007-11-4 loss: 0.646875  [  160/  179]
train() client id: f_00008-0-0 loss: 0.606943  [   32/  130]
train() client id: f_00008-0-1 loss: 0.730891  [   64/  130]
train() client id: f_00008-0-2 loss: 0.661836  [   96/  130]
train() client id: f_00008-0-3 loss: 0.827279  [  128/  130]
train() client id: f_00008-1-0 loss: 0.806092  [   32/  130]
train() client id: f_00008-1-1 loss: 0.614163  [   64/  130]
train() client id: f_00008-1-2 loss: 0.681189  [   96/  130]
train() client id: f_00008-1-3 loss: 0.731416  [  128/  130]
train() client id: f_00008-2-0 loss: 0.732525  [   32/  130]
train() client id: f_00008-2-1 loss: 0.727873  [   64/  130]
train() client id: f_00008-2-2 loss: 0.710389  [   96/  130]
train() client id: f_00008-2-3 loss: 0.671692  [  128/  130]
train() client id: f_00008-3-0 loss: 0.748717  [   32/  130]
train() client id: f_00008-3-1 loss: 0.685482  [   64/  130]
train() client id: f_00008-3-2 loss: 0.712447  [   96/  130]
train() client id: f_00008-3-3 loss: 0.661571  [  128/  130]
train() client id: f_00008-4-0 loss: 0.669150  [   32/  130]
train() client id: f_00008-4-1 loss: 0.696211  [   64/  130]
train() client id: f_00008-4-2 loss: 0.747789  [   96/  130]
train() client id: f_00008-4-3 loss: 0.716605  [  128/  130]
train() client id: f_00008-5-0 loss: 0.749691  [   32/  130]
train() client id: f_00008-5-1 loss: 0.677717  [   64/  130]
train() client id: f_00008-5-2 loss: 0.702940  [   96/  130]
train() client id: f_00008-5-3 loss: 0.673296  [  128/  130]
train() client id: f_00008-6-0 loss: 0.718211  [   32/  130]
train() client id: f_00008-6-1 loss: 0.715091  [   64/  130]
train() client id: f_00008-6-2 loss: 0.725197  [   96/  130]
train() client id: f_00008-6-3 loss: 0.673029  [  128/  130]
train() client id: f_00008-7-0 loss: 0.629115  [   32/  130]
train() client id: f_00008-7-1 loss: 0.729672  [   64/  130]
train() client id: f_00008-7-2 loss: 0.778815  [   96/  130]
train() client id: f_00008-7-3 loss: 0.693085  [  128/  130]
train() client id: f_00008-8-0 loss: 0.755446  [   32/  130]
train() client id: f_00008-8-1 loss: 0.717636  [   64/  130]
train() client id: f_00008-8-2 loss: 0.726331  [   96/  130]
train() client id: f_00008-8-3 loss: 0.630596  [  128/  130]
train() client id: f_00008-9-0 loss: 0.784588  [   32/  130]
train() client id: f_00008-9-1 loss: 0.556558  [   64/  130]
train() client id: f_00008-9-2 loss: 0.693601  [   96/  130]
train() client id: f_00008-9-3 loss: 0.795239  [  128/  130]
train() client id: f_00008-10-0 loss: 0.711617  [   32/  130]
train() client id: f_00008-10-1 loss: 0.672641  [   64/  130]
train() client id: f_00008-10-2 loss: 0.647132  [   96/  130]
train() client id: f_00008-10-3 loss: 0.784036  [  128/  130]
train() client id: f_00008-11-0 loss: 0.659892  [   32/  130]
train() client id: f_00008-11-1 loss: 0.681310  [   64/  130]
train() client id: f_00008-11-2 loss: 0.683948  [   96/  130]
train() client id: f_00008-11-3 loss: 0.786479  [  128/  130]
train() client id: f_00009-0-0 loss: 1.249605  [   32/  118]
train() client id: f_00009-0-1 loss: 1.066237  [   64/  118]
train() client id: f_00009-0-2 loss: 1.118689  [   96/  118]
train() client id: f_00009-1-0 loss: 1.000391  [   32/  118]
train() client id: f_00009-1-1 loss: 1.080694  [   64/  118]
train() client id: f_00009-1-2 loss: 1.310614  [   96/  118]
train() client id: f_00009-2-0 loss: 1.233657  [   32/  118]
train() client id: f_00009-2-1 loss: 0.914320  [   64/  118]
train() client id: f_00009-2-2 loss: 1.096435  [   96/  118]
train() client id: f_00009-3-0 loss: 0.873684  [   32/  118]
train() client id: f_00009-3-1 loss: 1.091419  [   64/  118]
train() client id: f_00009-3-2 loss: 0.994110  [   96/  118]
train() client id: f_00009-4-0 loss: 1.060446  [   32/  118]
train() client id: f_00009-4-1 loss: 0.951768  [   64/  118]
train() client id: f_00009-4-2 loss: 0.822894  [   96/  118]
train() client id: f_00009-5-0 loss: 0.959995  [   32/  118]
train() client id: f_00009-5-1 loss: 0.928517  [   64/  118]
train() client id: f_00009-5-2 loss: 0.879611  [   96/  118]
train() client id: f_00009-6-0 loss: 0.746835  [   32/  118]
train() client id: f_00009-6-1 loss: 0.870277  [   64/  118]
train() client id: f_00009-6-2 loss: 1.015933  [   96/  118]
train() client id: f_00009-7-0 loss: 0.886973  [   32/  118]
train() client id: f_00009-7-1 loss: 0.899065  [   64/  118]
train() client id: f_00009-7-2 loss: 0.888598  [   96/  118]
train() client id: f_00009-8-0 loss: 0.900944  [   32/  118]
train() client id: f_00009-8-1 loss: 0.774596  [   64/  118]
train() client id: f_00009-8-2 loss: 0.950289  [   96/  118]
train() client id: f_00009-9-0 loss: 0.807675  [   32/  118]
train() client id: f_00009-9-1 loss: 0.879623  [   64/  118]
train() client id: f_00009-9-2 loss: 0.807719  [   96/  118]
train() client id: f_00009-10-0 loss: 0.782075  [   32/  118]
train() client id: f_00009-10-1 loss: 0.940245  [   64/  118]
train() client id: f_00009-10-2 loss: 0.929966  [   96/  118]
train() client id: f_00009-11-0 loss: 0.883817  [   32/  118]
train() client id: f_00009-11-1 loss: 0.761914  [   64/  118]
train() client id: f_00009-11-2 loss: 0.879865  [   96/  118]
At round 23 accuracy: 0.6419098143236074
At round 23 training accuracy: 0.5868544600938967
At round 23 training loss: 0.8386046385957627
update_location
xs = -3.905658 4.200318 135.009024 18.811294 0.979296 3.956410 -97.443192 -76.324852 119.663977 -62.060879 
ys = 127.587959 110.555839 1.320614 -97.455176 89.350187 72.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 4.288573831882571
ys mean: 32.39414253552871
dists_uav = 162.154067 149.131607 168.015418 140.894912 134.105984 123.764105 139.649798 125.802064 156.933544 117.760624 
uav_gains = -105.260725 -104.343457 -105.653369 -103.724353 -103.187209 -102.315198 -103.627755 -102.492609 -104.901117 -101.775167 
uav_gains_db_mean: -103.72809602815994
dists_bs = 177.542038 190.435826 355.345077 334.357056 195.715618 206.076065 193.818713 200.186511 334.082890 204.928618 
bs_gains = -102.547493 -103.400021 -110.985268 -110.244953 -103.732573 -104.359831 -103.614139 -104.007234 -110.234978 -104.291932 
bs_gains_db_mean: -105.74184208164894
Round 24
-------------------------------
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.82454946 16.26273367  7.70634767  2.76510758 18.75837368  9.03341251
  3.43331975 11.02682771  8.12439108  7.33018602]
obj_prev = 92.26524913835074
eta_min = 1.3786631317009973e-12	eta_max = 0.9235436873576162
af = 19.491574900134562	bf = 1.580577038913744	zeta = 21.44073239014802	eta = 0.909090909090909
af = 19.491574900134562	bf = 1.580577038913744	zeta = 37.821548020465336	eta = 0.5153563489677266
af = 19.491574900134562	bf = 1.580577038913744	zeta = 29.921938016706985	eta = 0.65141418611493
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.500452198585428	eta = 0.6839040575328834
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.428583728430333	eta = 0.6856329912995908
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.428386266155645	eta = 0.685637753675084
eta = 0.685637753675084
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [0.03114263 0.06549838 0.0306483  0.01062804 0.07563209 0.0360859
 0.01334684 0.0442423  0.03213128 0.02916532]
ene_total = [2.48895914 4.62483507 2.46833909 1.14803217 5.27587599 2.77970132
 1.31845091 3.25561899 2.72998043 2.33859317]
ti_comp = [0.37300745 0.38019815 0.37131151 0.37903648 0.37900657 0.37665288
 0.37938621 0.38326735 0.34493694 0.37691463]
ti_coms = [0.08054563 0.07335493 0.08224157 0.07451661 0.07454651 0.07690021
 0.07416687 0.07028573 0.10861614 0.07663845]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.35678390e-05 1.21493121e-04 1.30503540e-05 5.22248762e-07
 1.88236947e-04 2.07018929e-05 1.03241013e-06 3.68459284e-05
 1.74254783e-05 1.09142550e-05]
ene_total = [0.51231347 0.47350677 0.52304957 0.47320176 0.48531125 0.48961862
 0.47101339 0.44864286 0.69080172 0.48733502]
optimize_network iter = 0 obj = 5.054794435766291
eta = 0.685637753675084
freqs = [41745313.04826085 86137162.39129016 41270331.48315956 14019810.25468265
 99776755.16786237 47903390.74277508 17590043.05099849 57717284.54230393
 46575586.46414848 38689551.73454573]
eta_min = 0.6856377536750926	eta_max = 0.6856377536750637
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 0.024656332621380766	eta = 0.909090909090909
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 17.409847423005747	eta = 0.0012874810038828607
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.7319792587002336	eta = 0.012941753040645609
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.7000222977574324	eta = 0.013185031671165255
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.700018383049954	eta = 0.013185062032920516
eta = 0.013185062032920516
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.55616847e-04 1.39346998e-03 1.49681533e-04 5.98995210e-06
 2.15899085e-03 2.37441151e-04 1.18412674e-05 4.22605781e-04
 1.99862188e-04 1.25181464e-04]
ene_total = [0.16607727 0.17657133 0.16938779 0.15085494 0.1944668  0.16035839
 0.15026585 0.15072403 0.2237539  0.15755809]
ti_comp = [0.37300745 0.38019815 0.37131151 0.37903648 0.37900657 0.37665288
 0.37938621 0.38326735 0.34493694 0.37691463]
ti_coms = [0.08054563 0.07335493 0.08224157 0.07451661 0.07454651 0.07690021
 0.07416687 0.07028573 0.10861614 0.07663845]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.35678390e-05 1.21493121e-04 1.30503540e-05 5.22248762e-07
 1.88236947e-04 2.07018929e-05 1.03241013e-06 3.68459284e-05
 1.74254783e-05 1.09142550e-05]
ene_total = [0.51231347 0.47350677 0.52304957 0.47320176 0.48531125 0.48961862
 0.47101339 0.44864286 0.69080172 0.48733502]
optimize_network iter = 1 obj = 5.0547944357664285
eta = 0.6856377536750926
freqs = [41745313.04826085 86137162.3912901  41270331.48315956 14019810.25468264
 99776755.16786231 47903390.74277506 17590043.05099848 57717284.54230388
 46575586.46414861 38689551.73454572]
Done!
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.31745940e-05 1.17971811e-04 1.26721075e-05 5.07112105e-07
 1.82781160e-04 2.01018772e-05 1.00248715e-06 3.57780002e-05
 1.69204250e-05 1.05979204e-05]
ene_total = [0.00806774 0.00745347 0.00823683 0.00745217 0.00763743 0.00771012
 0.00741769 0.00706435 0.01087853 0.00767444]
At round 24 energy consumption: 0.07959277336451151
At round 24 eta: 0.6856377536750926
At round 24 a_n: 19.96150252899076
At round 24 local rounds: 12.358184869136853
At round 24 global rounds: 63.498409119903215
gradient difference: 0.3711216449737549
train() client id: f_00000-0-0 loss: 0.994561  [   32/  126]
train() client id: f_00000-0-1 loss: 1.178056  [   64/  126]
train() client id: f_00000-0-2 loss: 1.161881  [   96/  126]
train() client id: f_00000-1-0 loss: 0.984272  [   32/  126]
train() client id: f_00000-1-1 loss: 0.920612  [   64/  126]
train() client id: f_00000-1-2 loss: 1.130115  [   96/  126]
train() client id: f_00000-2-0 loss: 1.104003  [   32/  126]
train() client id: f_00000-2-1 loss: 0.860999  [   64/  126]
train() client id: f_00000-2-2 loss: 0.889006  [   96/  126]
train() client id: f_00000-3-0 loss: 0.889803  [   32/  126]
train() client id: f_00000-3-1 loss: 0.876380  [   64/  126]
train() client id: f_00000-3-2 loss: 0.969205  [   96/  126]
train() client id: f_00000-4-0 loss: 0.896523  [   32/  126]
train() client id: f_00000-4-1 loss: 0.712234  [   64/  126]
train() client id: f_00000-4-2 loss: 0.845837  [   96/  126]
train() client id: f_00000-5-0 loss: 0.802818  [   32/  126]
train() client id: f_00000-5-1 loss: 0.708474  [   64/  126]
train() client id: f_00000-5-2 loss: 0.783297  [   96/  126]
train() client id: f_00000-6-0 loss: 0.893723  [   32/  126]
train() client id: f_00000-6-1 loss: 0.770361  [   64/  126]
train() client id: f_00000-6-2 loss: 0.657516  [   96/  126]
train() client id: f_00000-7-0 loss: 0.718513  [   32/  126]
train() client id: f_00000-7-1 loss: 0.775601  [   64/  126]
train() client id: f_00000-7-2 loss: 0.702475  [   96/  126]
train() client id: f_00000-8-0 loss: 0.757157  [   32/  126]
train() client id: f_00000-8-1 loss: 0.666940  [   64/  126]
train() client id: f_00000-8-2 loss: 0.676188  [   96/  126]
train() client id: f_00000-9-0 loss: 0.652048  [   32/  126]
train() client id: f_00000-9-1 loss: 0.702069  [   64/  126]
train() client id: f_00000-9-2 loss: 0.635300  [   96/  126]
train() client id: f_00000-10-0 loss: 0.637733  [   32/  126]
train() client id: f_00000-10-1 loss: 0.689637  [   64/  126]
train() client id: f_00000-10-2 loss: 0.820721  [   96/  126]
train() client id: f_00000-11-0 loss: 0.669997  [   32/  126]
train() client id: f_00000-11-1 loss: 0.707796  [   64/  126]
train() client id: f_00000-11-2 loss: 0.731380  [   96/  126]
train() client id: f_00001-0-0 loss: 0.513956  [   32/  265]
train() client id: f_00001-0-1 loss: 0.522604  [   64/  265]
train() client id: f_00001-0-2 loss: 0.369726  [   96/  265]
train() client id: f_00001-0-3 loss: 0.413825  [  128/  265]
train() client id: f_00001-0-4 loss: 0.455968  [  160/  265]
train() client id: f_00001-0-5 loss: 0.480383  [  192/  265]
train() client id: f_00001-0-6 loss: 0.327432  [  224/  265]
train() client id: f_00001-0-7 loss: 0.351095  [  256/  265]
train() client id: f_00001-1-0 loss: 0.425461  [   32/  265]
train() client id: f_00001-1-1 loss: 0.427470  [   64/  265]
train() client id: f_00001-1-2 loss: 0.396247  [   96/  265]
train() client id: f_00001-1-3 loss: 0.429919  [  128/  265]
train() client id: f_00001-1-4 loss: 0.478961  [  160/  265]
train() client id: f_00001-1-5 loss: 0.346797  [  192/  265]
train() client id: f_00001-1-6 loss: 0.454656  [  224/  265]
train() client id: f_00001-1-7 loss: 0.383679  [  256/  265]
train() client id: f_00001-2-0 loss: 0.404477  [   32/  265]
train() client id: f_00001-2-1 loss: 0.322960  [   64/  265]
train() client id: f_00001-2-2 loss: 0.354197  [   96/  265]
train() client id: f_00001-2-3 loss: 0.453806  [  128/  265]
train() client id: f_00001-2-4 loss: 0.443656  [  160/  265]
train() client id: f_00001-2-5 loss: 0.366830  [  192/  265]
train() client id: f_00001-2-6 loss: 0.521202  [  224/  265]
train() client id: f_00001-2-7 loss: 0.407646  [  256/  265]
train() client id: f_00001-3-0 loss: 0.353454  [   32/  265]
train() client id: f_00001-3-1 loss: 0.432904  [   64/  265]
train() client id: f_00001-3-2 loss: 0.392130  [   96/  265]
train() client id: f_00001-3-3 loss: 0.525515  [  128/  265]
train() client id: f_00001-3-4 loss: 0.407231  [  160/  265]
train() client id: f_00001-3-5 loss: 0.300697  [  192/  265]
train() client id: f_00001-3-6 loss: 0.441056  [  224/  265]
train() client id: f_00001-3-7 loss: 0.287005  [  256/  265]
train() client id: f_00001-4-0 loss: 0.313390  [   32/  265]
train() client id: f_00001-4-1 loss: 0.319139  [   64/  265]
train() client id: f_00001-4-2 loss: 0.357652  [   96/  265]
train() client id: f_00001-4-3 loss: 0.469297  [  128/  265]
train() client id: f_00001-4-4 loss: 0.446594  [  160/  265]
train() client id: f_00001-4-5 loss: 0.585714  [  192/  265]
train() client id: f_00001-4-6 loss: 0.305075  [  224/  265]
train() client id: f_00001-4-7 loss: 0.334731  [  256/  265]
train() client id: f_00001-5-0 loss: 0.477394  [   32/  265]
train() client id: f_00001-5-1 loss: 0.463555  [   64/  265]
train() client id: f_00001-5-2 loss: 0.382605  [   96/  265]
train() client id: f_00001-5-3 loss: 0.285406  [  128/  265]
train() client id: f_00001-5-4 loss: 0.353249  [  160/  265]
train() client id: f_00001-5-5 loss: 0.328556  [  192/  265]
train() client id: f_00001-5-6 loss: 0.394860  [  224/  265]
train() client id: f_00001-5-7 loss: 0.458215  [  256/  265]
train() client id: f_00001-6-0 loss: 0.398146  [   32/  265]
train() client id: f_00001-6-1 loss: 0.284634  [   64/  265]
train() client id: f_00001-6-2 loss: 0.338706  [   96/  265]
train() client id: f_00001-6-3 loss: 0.325837  [  128/  265]
train() client id: f_00001-6-4 loss: 0.419698  [  160/  265]
train() client id: f_00001-6-5 loss: 0.407964  [  192/  265]
train() client id: f_00001-6-6 loss: 0.544623  [  224/  265]
train() client id: f_00001-6-7 loss: 0.374656  [  256/  265]
train() client id: f_00001-7-0 loss: 0.389237  [   32/  265]
train() client id: f_00001-7-1 loss: 0.402751  [   64/  265]
train() client id: f_00001-7-2 loss: 0.393528  [   96/  265]
train() client id: f_00001-7-3 loss: 0.312983  [  128/  265]
train() client id: f_00001-7-4 loss: 0.391009  [  160/  265]
train() client id: f_00001-7-5 loss: 0.331205  [  192/  265]
train() client id: f_00001-7-6 loss: 0.348876  [  224/  265]
train() client id: f_00001-7-7 loss: 0.436014  [  256/  265]
train() client id: f_00001-8-0 loss: 0.342868  [   32/  265]
train() client id: f_00001-8-1 loss: 0.378688  [   64/  265]
train() client id: f_00001-8-2 loss: 0.440216  [   96/  265]
train() client id: f_00001-8-3 loss: 0.493760  [  128/  265]
train() client id: f_00001-8-4 loss: 0.281789  [  160/  265]
train() client id: f_00001-8-5 loss: 0.380145  [  192/  265]
train() client id: f_00001-8-6 loss: 0.262556  [  224/  265]
train() client id: f_00001-8-7 loss: 0.494274  [  256/  265]
train() client id: f_00001-9-0 loss: 0.431573  [   32/  265]
train() client id: f_00001-9-1 loss: 0.378053  [   64/  265]
train() client id: f_00001-9-2 loss: 0.507516  [   96/  265]
train() client id: f_00001-9-3 loss: 0.267190  [  128/  265]
train() client id: f_00001-9-4 loss: 0.431868  [  160/  265]
train() client id: f_00001-9-5 loss: 0.472125  [  192/  265]
train() client id: f_00001-9-6 loss: 0.269747  [  224/  265]
train() client id: f_00001-9-7 loss: 0.290895  [  256/  265]
train() client id: f_00001-10-0 loss: 0.440705  [   32/  265]
train() client id: f_00001-10-1 loss: 0.404239  [   64/  265]
train() client id: f_00001-10-2 loss: 0.458163  [   96/  265]
train() client id: f_00001-10-3 loss: 0.271118  [  128/  265]
train() client id: f_00001-10-4 loss: 0.329401  [  160/  265]
train() client id: f_00001-10-5 loss: 0.428565  [  192/  265]
train() client id: f_00001-10-6 loss: 0.291312  [  224/  265]
train() client id: f_00001-10-7 loss: 0.419968  [  256/  265]
train() client id: f_00001-11-0 loss: 0.267989  [   32/  265]
train() client id: f_00001-11-1 loss: 0.320570  [   64/  265]
train() client id: f_00001-11-2 loss: 0.420915  [   96/  265]
train() client id: f_00001-11-3 loss: 0.369873  [  128/  265]
train() client id: f_00001-11-4 loss: 0.275232  [  160/  265]
train() client id: f_00001-11-5 loss: 0.450800  [  192/  265]
train() client id: f_00001-11-6 loss: 0.449332  [  224/  265]
train() client id: f_00001-11-7 loss: 0.479425  [  256/  265]
train() client id: f_00002-0-0 loss: 1.166749  [   32/  124]
train() client id: f_00002-0-1 loss: 1.283869  [   64/  124]
train() client id: f_00002-0-2 loss: 1.241161  [   96/  124]
train() client id: f_00002-1-0 loss: 1.291581  [   32/  124]
train() client id: f_00002-1-1 loss: 1.120949  [   64/  124]
train() client id: f_00002-1-2 loss: 1.181143  [   96/  124]
train() client id: f_00002-2-0 loss: 1.105639  [   32/  124]
train() client id: f_00002-2-1 loss: 1.050486  [   64/  124]
train() client id: f_00002-2-2 loss: 1.151099  [   96/  124]
train() client id: f_00002-3-0 loss: 1.098415  [   32/  124]
train() client id: f_00002-3-1 loss: 1.143756  [   64/  124]
train() client id: f_00002-3-2 loss: 0.963354  [   96/  124]
train() client id: f_00002-4-0 loss: 0.918380  [   32/  124]
train() client id: f_00002-4-1 loss: 1.151751  [   64/  124]
train() client id: f_00002-4-2 loss: 1.205117  [   96/  124]
train() client id: f_00002-5-0 loss: 0.994185  [   32/  124]
train() client id: f_00002-5-1 loss: 1.188573  [   64/  124]
train() client id: f_00002-5-2 loss: 1.106172  [   96/  124]
train() client id: f_00002-6-0 loss: 1.230135  [   32/  124]
train() client id: f_00002-6-1 loss: 0.951157  [   64/  124]
train() client id: f_00002-6-2 loss: 1.006149  [   96/  124]
train() client id: f_00002-7-0 loss: 1.006458  [   32/  124]
train() client id: f_00002-7-1 loss: 0.957174  [   64/  124]
train() client id: f_00002-7-2 loss: 1.071669  [   96/  124]
train() client id: f_00002-8-0 loss: 1.004004  [   32/  124]
train() client id: f_00002-8-1 loss: 1.089780  [   64/  124]
train() client id: f_00002-8-2 loss: 1.004159  [   96/  124]
train() client id: f_00002-9-0 loss: 1.072302  [   32/  124]
train() client id: f_00002-9-1 loss: 1.006018  [   64/  124]
train() client id: f_00002-9-2 loss: 1.029189  [   96/  124]
train() client id: f_00002-10-0 loss: 0.958806  [   32/  124]
train() client id: f_00002-10-1 loss: 1.116642  [   64/  124]
train() client id: f_00002-10-2 loss: 0.863767  [   96/  124]
train() client id: f_00002-11-0 loss: 1.046923  [   32/  124]
train() client id: f_00002-11-1 loss: 0.905437  [   64/  124]
train() client id: f_00002-11-2 loss: 1.050215  [   96/  124]
train() client id: f_00003-0-0 loss: 0.851194  [   32/   43]
train() client id: f_00003-1-0 loss: 0.845259  [   32/   43]
train() client id: f_00003-2-0 loss: 0.865317  [   32/   43]
train() client id: f_00003-3-0 loss: 0.810248  [   32/   43]
train() client id: f_00003-4-0 loss: 0.850397  [   32/   43]
train() client id: f_00003-5-0 loss: 0.870738  [   32/   43]
train() client id: f_00003-6-0 loss: 0.931159  [   32/   43]
train() client id: f_00003-7-0 loss: 0.903954  [   32/   43]
train() client id: f_00003-8-0 loss: 0.785409  [   32/   43]
train() client id: f_00003-9-0 loss: 0.886697  [   32/   43]
train() client id: f_00003-10-0 loss: 0.827026  [   32/   43]
train() client id: f_00003-11-0 loss: 0.841158  [   32/   43]
train() client id: f_00004-0-0 loss: 0.728476  [   32/  306]
train() client id: f_00004-0-1 loss: 0.869813  [   64/  306]
train() client id: f_00004-0-2 loss: 0.667843  [   96/  306]
train() client id: f_00004-0-3 loss: 0.920439  [  128/  306]
train() client id: f_00004-0-4 loss: 0.925885  [  160/  306]
train() client id: f_00004-0-5 loss: 0.825888  [  192/  306]
train() client id: f_00004-0-6 loss: 0.755260  [  224/  306]
train() client id: f_00004-0-7 loss: 0.809098  [  256/  306]
train() client id: f_00004-0-8 loss: 0.814387  [  288/  306]
train() client id: f_00004-1-0 loss: 1.011942  [   32/  306]
train() client id: f_00004-1-1 loss: 0.773008  [   64/  306]
train() client id: f_00004-1-2 loss: 0.727711  [   96/  306]
train() client id: f_00004-1-3 loss: 0.779747  [  128/  306]
train() client id: f_00004-1-4 loss: 0.792087  [  160/  306]
train() client id: f_00004-1-5 loss: 0.891686  [  192/  306]
train() client id: f_00004-1-6 loss: 0.875018  [  224/  306]
train() client id: f_00004-1-7 loss: 0.810722  [  256/  306]
train() client id: f_00004-1-8 loss: 0.694225  [  288/  306]
train() client id: f_00004-2-0 loss: 0.831802  [   32/  306]
train() client id: f_00004-2-1 loss: 0.839060  [   64/  306]
train() client id: f_00004-2-2 loss: 0.744335  [   96/  306]
train() client id: f_00004-2-3 loss: 0.896095  [  128/  306]
train() client id: f_00004-2-4 loss: 0.897508  [  160/  306]
train() client id: f_00004-2-5 loss: 0.776452  [  192/  306]
train() client id: f_00004-2-6 loss: 0.712853  [  224/  306]
train() client id: f_00004-2-7 loss: 0.848357  [  256/  306]
train() client id: f_00004-2-8 loss: 0.752602  [  288/  306]
train() client id: f_00004-3-0 loss: 0.857330  [   32/  306]
train() client id: f_00004-3-1 loss: 0.837108  [   64/  306]
train() client id: f_00004-3-2 loss: 0.796321  [   96/  306]
train() client id: f_00004-3-3 loss: 0.754467  [  128/  306]
train() client id: f_00004-3-4 loss: 0.805897  [  160/  306]
train() client id: f_00004-3-5 loss: 0.804985  [  192/  306]
train() client id: f_00004-3-6 loss: 0.861914  [  224/  306]
train() client id: f_00004-3-7 loss: 0.852585  [  256/  306]
train() client id: f_00004-3-8 loss: 0.776062  [  288/  306]
train() client id: f_00004-4-0 loss: 0.762705  [   32/  306]
train() client id: f_00004-4-1 loss: 0.908046  [   64/  306]
train() client id: f_00004-4-2 loss: 0.934995  [   96/  306]
train() client id: f_00004-4-3 loss: 0.823291  [  128/  306]
train() client id: f_00004-4-4 loss: 0.796376  [  160/  306]
train() client id: f_00004-4-5 loss: 0.729028  [  192/  306]
train() client id: f_00004-4-6 loss: 0.788313  [  224/  306]
train() client id: f_00004-4-7 loss: 0.777595  [  256/  306]
train() client id: f_00004-4-8 loss: 0.796603  [  288/  306]
train() client id: f_00004-5-0 loss: 0.783279  [   32/  306]
train() client id: f_00004-5-1 loss: 0.691181  [   64/  306]
train() client id: f_00004-5-2 loss: 0.730693  [   96/  306]
train() client id: f_00004-5-3 loss: 0.900331  [  128/  306]
train() client id: f_00004-5-4 loss: 0.901155  [  160/  306]
train() client id: f_00004-5-5 loss: 0.869480  [  192/  306]
train() client id: f_00004-5-6 loss: 0.816192  [  224/  306]
train() client id: f_00004-5-7 loss: 0.835207  [  256/  306]
train() client id: f_00004-5-8 loss: 0.791211  [  288/  306]
train() client id: f_00004-6-0 loss: 0.776117  [   32/  306]
train() client id: f_00004-6-1 loss: 0.804131  [   64/  306]
train() client id: f_00004-6-2 loss: 0.696055  [   96/  306]
train() client id: f_00004-6-3 loss: 0.727427  [  128/  306]
train() client id: f_00004-6-4 loss: 0.866040  [  160/  306]
train() client id: f_00004-6-5 loss: 0.873531  [  192/  306]
train() client id: f_00004-6-6 loss: 0.796861  [  224/  306]
train() client id: f_00004-6-7 loss: 0.875412  [  256/  306]
train() client id: f_00004-6-8 loss: 0.737804  [  288/  306]
train() client id: f_00004-7-0 loss: 0.743028  [   32/  306]
train() client id: f_00004-7-1 loss: 0.758190  [   64/  306]
train() client id: f_00004-7-2 loss: 0.800125  [   96/  306]
train() client id: f_00004-7-3 loss: 0.883682  [  128/  306]
train() client id: f_00004-7-4 loss: 0.857425  [  160/  306]
train() client id: f_00004-7-5 loss: 0.851222  [  192/  306]
train() client id: f_00004-7-6 loss: 0.778893  [  224/  306]
train() client id: f_00004-7-7 loss: 0.881888  [  256/  306]
train() client id: f_00004-7-8 loss: 0.714236  [  288/  306]
train() client id: f_00004-8-0 loss: 0.740101  [   32/  306]
train() client id: f_00004-8-1 loss: 0.825246  [   64/  306]
train() client id: f_00004-8-2 loss: 0.761356  [   96/  306]
train() client id: f_00004-8-3 loss: 0.893182  [  128/  306]
train() client id: f_00004-8-4 loss: 0.909328  [  160/  306]
train() client id: f_00004-8-5 loss: 0.732212  [  192/  306]
train() client id: f_00004-8-6 loss: 0.895348  [  224/  306]
train() client id: f_00004-8-7 loss: 0.689062  [  256/  306]
train() client id: f_00004-8-8 loss: 0.899333  [  288/  306]
train() client id: f_00004-9-0 loss: 0.803885  [   32/  306]
train() client id: f_00004-9-1 loss: 0.796531  [   64/  306]
train() client id: f_00004-9-2 loss: 0.839987  [   96/  306]
train() client id: f_00004-9-3 loss: 0.799892  [  128/  306]
train() client id: f_00004-9-4 loss: 0.862902  [  160/  306]
train() client id: f_00004-9-5 loss: 0.744353  [  192/  306]
train() client id: f_00004-9-6 loss: 0.863380  [  224/  306]
train() client id: f_00004-9-7 loss: 0.677554  [  256/  306]
train() client id: f_00004-9-8 loss: 0.882133  [  288/  306]
train() client id: f_00004-10-0 loss: 0.813138  [   32/  306]
train() client id: f_00004-10-1 loss: 0.755875  [   64/  306]
train() client id: f_00004-10-2 loss: 0.734222  [   96/  306]
train() client id: f_00004-10-3 loss: 0.906183  [  128/  306]
train() client id: f_00004-10-4 loss: 0.839285  [  160/  306]
train() client id: f_00004-10-5 loss: 0.809592  [  192/  306]
train() client id: f_00004-10-6 loss: 0.775119  [  224/  306]
train() client id: f_00004-10-7 loss: 0.942662  [  256/  306]
train() client id: f_00004-10-8 loss: 0.850288  [  288/  306]
train() client id: f_00004-11-0 loss: 0.852826  [   32/  306]
train() client id: f_00004-11-1 loss: 0.800801  [   64/  306]
train() client id: f_00004-11-2 loss: 0.817384  [   96/  306]
train() client id: f_00004-11-3 loss: 0.996363  [  128/  306]
train() client id: f_00004-11-4 loss: 0.811212  [  160/  306]
train() client id: f_00004-11-5 loss: 0.736632  [  192/  306]
train() client id: f_00004-11-6 loss: 0.842910  [  224/  306]
train() client id: f_00004-11-7 loss: 0.785445  [  256/  306]
train() client id: f_00004-11-8 loss: 0.634135  [  288/  306]
train() client id: f_00005-0-0 loss: 0.465972  [   32/  146]
train() client id: f_00005-0-1 loss: 0.438610  [   64/  146]
train() client id: f_00005-0-2 loss: 0.388741  [   96/  146]
train() client id: f_00005-0-3 loss: 0.784887  [  128/  146]
train() client id: f_00005-1-0 loss: 0.390278  [   32/  146]
train() client id: f_00005-1-1 loss: 0.460086  [   64/  146]
train() client id: f_00005-1-2 loss: 0.782333  [   96/  146]
train() client id: f_00005-1-3 loss: 0.495452  [  128/  146]
train() client id: f_00005-2-0 loss: 0.585015  [   32/  146]
train() client id: f_00005-2-1 loss: 0.555809  [   64/  146]
train() client id: f_00005-2-2 loss: 0.424483  [   96/  146]
train() client id: f_00005-2-3 loss: 0.436696  [  128/  146]
train() client id: f_00005-3-0 loss: 0.553642  [   32/  146]
train() client id: f_00005-3-1 loss: 0.548389  [   64/  146]
train() client id: f_00005-3-2 loss: 0.438515  [   96/  146]
train() client id: f_00005-3-3 loss: 0.663709  [  128/  146]
train() client id: f_00005-4-0 loss: 0.730357  [   32/  146]
train() client id: f_00005-4-1 loss: 0.293371  [   64/  146]
train() client id: f_00005-4-2 loss: 0.490279  [   96/  146]
train() client id: f_00005-4-3 loss: 0.470675  [  128/  146]
train() client id: f_00005-5-0 loss: 0.581822  [   32/  146]
train() client id: f_00005-5-1 loss: 0.278761  [   64/  146]
train() client id: f_00005-5-2 loss: 0.579467  [   96/  146]
train() client id: f_00005-5-3 loss: 0.471070  [  128/  146]
train() client id: f_00005-6-0 loss: 0.424371  [   32/  146]
train() client id: f_00005-6-1 loss: 0.963661  [   64/  146]
train() client id: f_00005-6-2 loss: 0.464832  [   96/  146]
train() client id: f_00005-6-3 loss: 0.337281  [  128/  146]
train() client id: f_00005-7-0 loss: 0.652051  [   32/  146]
train() client id: f_00005-7-1 loss: 0.532322  [   64/  146]
train() client id: f_00005-7-2 loss: 0.418118  [   96/  146]
train() client id: f_00005-7-3 loss: 0.505541  [  128/  146]
train() client id: f_00005-8-0 loss: 0.381275  [   32/  146]
train() client id: f_00005-8-1 loss: 0.548742  [   64/  146]
train() client id: f_00005-8-2 loss: 0.732875  [   96/  146]
train() client id: f_00005-8-3 loss: 0.389220  [  128/  146]
train() client id: f_00005-9-0 loss: 0.580904  [   32/  146]
train() client id: f_00005-9-1 loss: 0.359110  [   64/  146]
train() client id: f_00005-9-2 loss: 0.638230  [   96/  146]
train() client id: f_00005-9-3 loss: 0.627051  [  128/  146]
train() client id: f_00005-10-0 loss: 0.519334  [   32/  146]
train() client id: f_00005-10-1 loss: 0.459257  [   64/  146]
train() client id: f_00005-10-2 loss: 0.589789  [   96/  146]
train() client id: f_00005-10-3 loss: 0.621677  [  128/  146]
train() client id: f_00005-11-0 loss: 0.379355  [   32/  146]
train() client id: f_00005-11-1 loss: 0.308909  [   64/  146]
train() client id: f_00005-11-2 loss: 0.388945  [   96/  146]
train() client id: f_00005-11-3 loss: 0.753881  [  128/  146]
train() client id: f_00006-0-0 loss: 0.459121  [   32/   54]
train() client id: f_00006-1-0 loss: 0.425897  [   32/   54]
train() client id: f_00006-2-0 loss: 0.436414  [   32/   54]
train() client id: f_00006-3-0 loss: 0.445073  [   32/   54]
train() client id: f_00006-4-0 loss: 0.508883  [   32/   54]
train() client id: f_00006-5-0 loss: 0.466640  [   32/   54]
train() client id: f_00006-6-0 loss: 0.488974  [   32/   54]
train() client id: f_00006-7-0 loss: 0.481265  [   32/   54]
train() client id: f_00006-8-0 loss: 0.484380  [   32/   54]
train() client id: f_00006-9-0 loss: 0.522110  [   32/   54]
train() client id: f_00006-10-0 loss: 0.476394  [   32/   54]
train() client id: f_00006-11-0 loss: 0.475141  [   32/   54]
train() client id: f_00007-0-0 loss: 0.649818  [   32/  179]
train() client id: f_00007-0-1 loss: 0.535492  [   64/  179]
train() client id: f_00007-0-2 loss: 0.383020  [   96/  179]
train() client id: f_00007-0-3 loss: 0.446584  [  128/  179]
train() client id: f_00007-0-4 loss: 0.465981  [  160/  179]
train() client id: f_00007-1-0 loss: 0.475009  [   32/  179]
train() client id: f_00007-1-1 loss: 0.484317  [   64/  179]
train() client id: f_00007-1-2 loss: 0.573896  [   96/  179]
train() client id: f_00007-1-3 loss: 0.463860  [  128/  179]
train() client id: f_00007-1-4 loss: 0.453402  [  160/  179]
train() client id: f_00007-2-0 loss: 0.655581  [   32/  179]
train() client id: f_00007-2-1 loss: 0.372816  [   64/  179]
train() client id: f_00007-2-2 loss: 0.423895  [   96/  179]
train() client id: f_00007-2-3 loss: 0.449337  [  128/  179]
train() client id: f_00007-2-4 loss: 0.475747  [  160/  179]
train() client id: f_00007-3-0 loss: 0.316809  [   32/  179]
train() client id: f_00007-3-1 loss: 0.507447  [   64/  179]
train() client id: f_00007-3-2 loss: 0.410937  [   96/  179]
train() client id: f_00007-3-3 loss: 0.544592  [  128/  179]
train() client id: f_00007-3-4 loss: 0.321315  [  160/  179]
train() client id: f_00007-4-0 loss: 0.534071  [   32/  179]
train() client id: f_00007-4-1 loss: 0.591661  [   64/  179]
train() client id: f_00007-4-2 loss: 0.324055  [   96/  179]
train() client id: f_00007-4-3 loss: 0.453760  [  128/  179]
train() client id: f_00007-4-4 loss: 0.426403  [  160/  179]
train() client id: f_00007-5-0 loss: 0.420178  [   32/  179]
train() client id: f_00007-5-1 loss: 0.475432  [   64/  179]
train() client id: f_00007-5-2 loss: 0.525519  [   96/  179]
train() client id: f_00007-5-3 loss: 0.485631  [  128/  179]
train() client id: f_00007-5-4 loss: 0.275276  [  160/  179]
train() client id: f_00007-6-0 loss: 0.375149  [   32/  179]
train() client id: f_00007-6-1 loss: 0.571055  [   64/  179]
train() client id: f_00007-6-2 loss: 0.355141  [   96/  179]
train() client id: f_00007-6-3 loss: 0.382237  [  128/  179]
train() client id: f_00007-6-4 loss: 0.537602  [  160/  179]
train() client id: f_00007-7-0 loss: 0.272871  [   32/  179]
train() client id: f_00007-7-1 loss: 0.505636  [   64/  179]
train() client id: f_00007-7-2 loss: 0.494209  [   96/  179]
train() client id: f_00007-7-3 loss: 0.518320  [  128/  179]
train() client id: f_00007-7-4 loss: 0.277108  [  160/  179]
train() client id: f_00007-8-0 loss: 0.423870  [   32/  179]
train() client id: f_00007-8-1 loss: 0.421867  [   64/  179]
train() client id: f_00007-8-2 loss: 0.577291  [   96/  179]
train() client id: f_00007-8-3 loss: 0.436530  [  128/  179]
train() client id: f_00007-8-4 loss: 0.353707  [  160/  179]
train() client id: f_00007-9-0 loss: 0.314983  [   32/  179]
train() client id: f_00007-9-1 loss: 0.470626  [   64/  179]
train() client id: f_00007-9-2 loss: 0.771124  [   96/  179]
train() client id: f_00007-9-3 loss: 0.334980  [  128/  179]
train() client id: f_00007-9-4 loss: 0.283045  [  160/  179]
train() client id: f_00007-10-0 loss: 0.573334  [   32/  179]
train() client id: f_00007-10-1 loss: 0.245725  [   64/  179]
train() client id: f_00007-10-2 loss: 0.326415  [   96/  179]
train() client id: f_00007-10-3 loss: 0.385734  [  128/  179]
train() client id: f_00007-10-4 loss: 0.381934  [  160/  179]
train() client id: f_00007-11-0 loss: 0.483400  [   32/  179]
train() client id: f_00007-11-1 loss: 0.441541  [   64/  179]
train() client id: f_00007-11-2 loss: 0.241067  [   96/  179]
train() client id: f_00007-11-3 loss: 0.274854  [  128/  179]
train() client id: f_00007-11-4 loss: 0.539665  [  160/  179]
train() client id: f_00008-0-0 loss: 0.704625  [   32/  130]
train() client id: f_00008-0-1 loss: 0.688435  [   64/  130]
train() client id: f_00008-0-2 loss: 0.662890  [   96/  130]
train() client id: f_00008-0-3 loss: 0.701412  [  128/  130]
train() client id: f_00008-1-0 loss: 0.884242  [   32/  130]
train() client id: f_00008-1-1 loss: 0.672009  [   64/  130]
train() client id: f_00008-1-2 loss: 0.604291  [   96/  130]
train() client id: f_00008-1-3 loss: 0.611864  [  128/  130]
train() client id: f_00008-2-0 loss: 0.625776  [   32/  130]
train() client id: f_00008-2-1 loss: 0.697326  [   64/  130]
train() client id: f_00008-2-2 loss: 0.723295  [   96/  130]
train() client id: f_00008-2-3 loss: 0.710583  [  128/  130]
train() client id: f_00008-3-0 loss: 0.614447  [   32/  130]
train() client id: f_00008-3-1 loss: 0.740283  [   64/  130]
train() client id: f_00008-3-2 loss: 0.693454  [   96/  130]
train() client id: f_00008-3-3 loss: 0.658998  [  128/  130]
train() client id: f_00008-4-0 loss: 0.648642  [   32/  130]
train() client id: f_00008-4-1 loss: 0.718248  [   64/  130]
train() client id: f_00008-4-2 loss: 0.648591  [   96/  130]
train() client id: f_00008-4-3 loss: 0.709872  [  128/  130]
train() client id: f_00008-5-0 loss: 0.650212  [   32/  130]
train() client id: f_00008-5-1 loss: 0.660317  [   64/  130]
train() client id: f_00008-5-2 loss: 0.757048  [   96/  130]
train() client id: f_00008-5-3 loss: 0.659665  [  128/  130]
train() client id: f_00008-6-0 loss: 0.709876  [   32/  130]
train() client id: f_00008-6-1 loss: 0.613498  [   64/  130]
train() client id: f_00008-6-2 loss: 0.687947  [   96/  130]
train() client id: f_00008-6-3 loss: 0.755049  [  128/  130]
train() client id: f_00008-7-0 loss: 0.702255  [   32/  130]
train() client id: f_00008-7-1 loss: 0.645651  [   64/  130]
train() client id: f_00008-7-2 loss: 0.701834  [   96/  130]
train() client id: f_00008-7-3 loss: 0.654272  [  128/  130]
train() client id: f_00008-8-0 loss: 0.636548  [   32/  130]
train() client id: f_00008-8-1 loss: 0.725718  [   64/  130]
train() client id: f_00008-8-2 loss: 0.687590  [   96/  130]
train() client id: f_00008-8-3 loss: 0.705039  [  128/  130]
train() client id: f_00008-9-0 loss: 0.593605  [   32/  130]
train() client id: f_00008-9-1 loss: 0.729784  [   64/  130]
train() client id: f_00008-9-2 loss: 0.733322  [   96/  130]
train() client id: f_00008-9-3 loss: 0.663440  [  128/  130]
train() client id: f_00008-10-0 loss: 0.749815  [   32/  130]
train() client id: f_00008-10-1 loss: 0.611859  [   64/  130]
train() client id: f_00008-10-2 loss: 0.673791  [   96/  130]
train() client id: f_00008-10-3 loss: 0.692854  [  128/  130]
train() client id: f_00008-11-0 loss: 0.751947  [   32/  130]
train() client id: f_00008-11-1 loss: 0.627407  [   64/  130]
train() client id: f_00008-11-2 loss: 0.756554  [   96/  130]
train() client id: f_00008-11-3 loss: 0.615128  [  128/  130]
train() client id: f_00009-0-0 loss: 1.060816  [   32/  118]
train() client id: f_00009-0-1 loss: 1.036574  [   64/  118]
train() client id: f_00009-0-2 loss: 0.988599  [   96/  118]
train() client id: f_00009-1-0 loss: 1.015126  [   32/  118]
train() client id: f_00009-1-1 loss: 0.964924  [   64/  118]
train() client id: f_00009-1-2 loss: 0.939841  [   96/  118]
train() client id: f_00009-2-0 loss: 0.993793  [   32/  118]
train() client id: f_00009-2-1 loss: 0.967688  [   64/  118]
train() client id: f_00009-2-2 loss: 0.759576  [   96/  118]
train() client id: f_00009-3-0 loss: 0.816726  [   32/  118]
train() client id: f_00009-3-1 loss: 1.026438  [   64/  118]
train() client id: f_00009-3-2 loss: 0.858846  [   96/  118]
train() client id: f_00009-4-0 loss: 0.887515  [   32/  118]
train() client id: f_00009-4-1 loss: 0.814161  [   64/  118]
train() client id: f_00009-4-2 loss: 0.952085  [   96/  118]
train() client id: f_00009-5-0 loss: 0.775159  [   32/  118]
train() client id: f_00009-5-1 loss: 0.928329  [   64/  118]
train() client id: f_00009-5-2 loss: 0.847989  [   96/  118]
train() client id: f_00009-6-0 loss: 0.880138  [   32/  118]
train() client id: f_00009-6-1 loss: 0.802191  [   64/  118]
train() client id: f_00009-6-2 loss: 0.825923  [   96/  118]
train() client id: f_00009-7-0 loss: 0.863751  [   32/  118]
train() client id: f_00009-7-1 loss: 0.731640  [   64/  118]
train() client id: f_00009-7-2 loss: 0.812211  [   96/  118]
train() client id: f_00009-8-0 loss: 0.875613  [   32/  118]
train() client id: f_00009-8-1 loss: 0.837799  [   64/  118]
train() client id: f_00009-8-2 loss: 0.781339  [   96/  118]
train() client id: f_00009-9-0 loss: 0.792151  [   32/  118]
train() client id: f_00009-9-1 loss: 0.788580  [   64/  118]
train() client id: f_00009-9-2 loss: 0.720545  [   96/  118]
train() client id: f_00009-10-0 loss: 0.783288  [   32/  118]
train() client id: f_00009-10-1 loss: 0.890714  [   64/  118]
train() client id: f_00009-10-2 loss: 0.797015  [   96/  118]
train() client id: f_00009-11-0 loss: 0.628109  [   32/  118]
train() client id: f_00009-11-1 loss: 0.839549  [   64/  118]
train() client id: f_00009-11-2 loss: 0.781165  [   96/  118]
At round 24 accuracy: 0.6419098143236074
At round 24 training accuracy: 0.5848423876592891
At round 24 training loss: 0.8372565796408489
update_location
xs = -3.905658 4.200318 140.009024 18.811294 0.979296 3.956410 -102.443192 -81.324852 124.663977 -67.060879 
ys = 132.587959 115.555839 1.320614 -102.455176 94.350187 77.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 3.7885738318825717
ys mean: 33.89414253552871
dists_uav = 166.116890 152.875095 172.058917 144.398504 137.487879 126.770246 143.183442 128.896888 160.778659 120.470632 
uav_gains = -105.527368 -104.614262 -105.918350 -103.991810 -103.458024 -102.575895 -103.899768 -102.756643 -105.166945 -102.022256 
uav_gains_db_mean: -103.99313213234262
dists_bs = 176.272672 188.802442 359.715463 338.443780 193.579712 203.643036 191.872680 197.769785 338.501133 202.216090 
bs_gains = -102.460239 -103.295272 -111.133914 -110.392682 -103.599135 -104.215407 -103.491427 -103.859538 -110.394743 -104.129899 
bs_gains_db_mean: -105.69722552096675
Round 25
-------------------------------
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.69254516 15.98293133  7.57645092  2.71961017 18.43551744  8.87729699
  3.37637101 10.83932141  7.98736814  7.20317358]
obj_prev = 90.69058614039777
eta_min = 8.689629583387306e-13	eta_max = 0.9239062523538892
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 21.07280247978385	eta = 0.909090909090909
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 37.266591851058905	eta = 0.5140554102721235
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 29.4469781495679	eta = 0.65056227726107
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 28.03930224522773	eta = 0.683222891778642
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 27.967919357716607	eta = 0.6849666905290988
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 27.967721963314986	eta = 0.684971524980406
eta = 0.684971524980406
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [0.03122285 0.0656671  0.03072725 0.01065542 0.07582691 0.03617885
 0.01338122 0.04435627 0.03221405 0.02924044]
ene_total = [2.45319609 4.54407619 2.43314225 1.13378689 5.18350126 2.72855748
 1.30141153 3.20547972 2.69013729 2.29443326]
ti_comp = [0.37995732 0.38865987 0.37822142 0.38614518 0.38758329 0.38530164
 0.38648714 0.39049469 0.35182285 0.38562639]
ti_coms = [0.08168984 0.07298729 0.08342574 0.07550198 0.07406386 0.07634552
 0.07516002 0.07115247 0.10982431 0.07602077]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.31773438e-05 1.17161273e-04 1.26753124e-05 5.07095790e-07
 1.81392801e-04 1.99362151e-05 1.00252924e-06 3.57696506e-05
 1.68798211e-05 1.05074756e-05]
ene_total = [0.50955862 0.46183769 0.52033793 0.47023356 0.47254241 0.47669679
 0.46813477 0.44534224 0.68500157 0.47408718]
optimize_network iter = 0 obj = 4.983772760330714
eta = 0.684971524980406
freqs = [41087307.98029104 84478875.62799275 40620710.39935426 13797163.01214605
 97820151.49584162 46948740.71955757 17311339.16633044 56794967.35235866
 45781631.37350148 37912918.15352243]
eta_min = 0.6849715249804152	eta_max = 0.68497152498039
af = 0.02120720489719426	bf = 1.5620059925242489	zeta = 0.023327925386913688	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [3.31926004e-06 2.95119211e-05 3.19280262e-06 1.27733086e-07
 4.56912927e-05 5.02176180e-06 2.52528528e-07 9.01006858e-06
 4.25188234e-06 2.64674311e-06]
ene_total = [1.76365275 1.58149679 1.80108751 1.6294247  1.6082219  1.64868503
 1.62207175 1.53747541 2.37101979 1.64116416]
ti_comp = [0.37995732 0.38865987 0.37822142 0.38614518 0.38758329 0.38530164
 0.38648714 0.39049469 0.35182285 0.38562639]
ti_coms = [0.08168984 0.07298729 0.08342574 0.07550198 0.07406386 0.07634552
 0.07516002 0.07115247 0.10982431 0.07602077]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.31773438e-05 1.17161273e-04 1.26753124e-05 5.07095790e-07
 1.81392801e-04 1.99362151e-05 1.00252924e-06 3.57696506e-05
 1.68798211e-05 1.05074756e-05]
ene_total = [0.50955862 0.46183769 0.52033793 0.47023356 0.47254241 0.47669679
 0.46813477 0.44534224 0.68500157 0.47408718]
optimize_network iter = 1 obj = 4.983772760330463
eta = 0.68497152498039
freqs = [41087307.98029105 84478875.62799287 40620710.39935425 13797163.01214607
 97820151.49584176 46948740.71955761 17311339.16633046 56794967.35235876
 45781631.37350126 37912918.15352247]
Done!
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.27625416e-05 1.13473219e-04 1.22763133e-05 4.91133205e-07
 1.75682838e-04 1.93086541e-05 9.70971182e-07 3.46436777e-05
 1.63484706e-05 1.01767167e-05]
ene_total = [0.00818175 0.0074122  0.00835485 0.00755069 0.00758207 0.00765386
 0.00751697 0.00714989 0.01099878 0.00761225]
At round 25 energy consumption: 0.08001331387073954
At round 25 eta: 0.68497152498039
At round 25 a_n: 19.61895668202144
At round 25 local rounds: 12.390018469521971
At round 25 global rounds: 62.276772538737
gradient difference: 0.4029882550239563
train() client id: f_00000-0-0 loss: 1.138451  [   32/  126]
train() client id: f_00000-0-1 loss: 0.970065  [   64/  126]
train() client id: f_00000-0-2 loss: 1.035162  [   96/  126]
train() client id: f_00000-1-0 loss: 0.905234  [   32/  126]
train() client id: f_00000-1-1 loss: 0.859948  [   64/  126]
train() client id: f_00000-1-2 loss: 1.041933  [   96/  126]
train() client id: f_00000-2-0 loss: 0.692804  [   32/  126]
train() client id: f_00000-2-1 loss: 0.997423  [   64/  126]
train() client id: f_00000-2-2 loss: 0.983095  [   96/  126]
train() client id: f_00000-3-0 loss: 0.848134  [   32/  126]
train() client id: f_00000-3-1 loss: 0.943069  [   64/  126]
train() client id: f_00000-3-2 loss: 0.787645  [   96/  126]
train() client id: f_00000-4-0 loss: 0.862946  [   32/  126]
train() client id: f_00000-4-1 loss: 0.683354  [   64/  126]
train() client id: f_00000-4-2 loss: 0.871877  [   96/  126]
train() client id: f_00000-5-0 loss: 0.834085  [   32/  126]
train() client id: f_00000-5-1 loss: 0.753549  [   64/  126]
train() client id: f_00000-5-2 loss: 0.747945  [   96/  126]
train() client id: f_00000-6-0 loss: 1.026342  [   32/  126]
train() client id: f_00000-6-1 loss: 0.687435  [   64/  126]
train() client id: f_00000-6-2 loss: 0.788648  [   96/  126]
train() client id: f_00000-7-0 loss: 0.960740  [   32/  126]
train() client id: f_00000-7-1 loss: 0.705277  [   64/  126]
train() client id: f_00000-7-2 loss: 0.780665  [   96/  126]
train() client id: f_00000-8-0 loss: 0.850835  [   32/  126]
train() client id: f_00000-8-1 loss: 0.741915  [   64/  126]
train() client id: f_00000-8-2 loss: 0.677949  [   96/  126]
train() client id: f_00000-9-0 loss: 0.830665  [   32/  126]
train() client id: f_00000-9-1 loss: 0.807322  [   64/  126]
train() client id: f_00000-9-2 loss: 0.719354  [   96/  126]
train() client id: f_00000-10-0 loss: 0.763240  [   32/  126]
train() client id: f_00000-10-1 loss: 0.908548  [   64/  126]
train() client id: f_00000-10-2 loss: 0.844033  [   96/  126]
train() client id: f_00000-11-0 loss: 0.785728  [   32/  126]
train() client id: f_00000-11-1 loss: 0.698540  [   64/  126]
train() client id: f_00000-11-2 loss: 0.848954  [   96/  126]
train() client id: f_00001-0-0 loss: 0.367273  [   32/  265]
train() client id: f_00001-0-1 loss: 0.456231  [   64/  265]
train() client id: f_00001-0-2 loss: 0.395503  [   96/  265]
train() client id: f_00001-0-3 loss: 0.308772  [  128/  265]
train() client id: f_00001-0-4 loss: 0.430128  [  160/  265]
train() client id: f_00001-0-5 loss: 0.385461  [  192/  265]
train() client id: f_00001-0-6 loss: 0.302168  [  224/  265]
train() client id: f_00001-0-7 loss: 0.308588  [  256/  265]
train() client id: f_00001-1-0 loss: 0.337586  [   32/  265]
train() client id: f_00001-1-1 loss: 0.389096  [   64/  265]
train() client id: f_00001-1-2 loss: 0.271465  [   96/  265]
train() client id: f_00001-1-3 loss: 0.287966  [  128/  265]
train() client id: f_00001-1-4 loss: 0.370116  [  160/  265]
train() client id: f_00001-1-5 loss: 0.497354  [  192/  265]
train() client id: f_00001-1-6 loss: 0.382132  [  224/  265]
train() client id: f_00001-1-7 loss: 0.351614  [  256/  265]
train() client id: f_00001-2-0 loss: 0.318349  [   32/  265]
train() client id: f_00001-2-1 loss: 0.388883  [   64/  265]
train() client id: f_00001-2-2 loss: 0.349439  [   96/  265]
train() client id: f_00001-2-3 loss: 0.341655  [  128/  265]
train() client id: f_00001-2-4 loss: 0.271127  [  160/  265]
train() client id: f_00001-2-5 loss: 0.300649  [  192/  265]
train() client id: f_00001-2-6 loss: 0.321495  [  224/  265]
train() client id: f_00001-2-7 loss: 0.452189  [  256/  265]
train() client id: f_00001-3-0 loss: 0.323660  [   32/  265]
train() client id: f_00001-3-1 loss: 0.470550  [   64/  265]
train() client id: f_00001-3-2 loss: 0.259647  [   96/  265]
train() client id: f_00001-3-3 loss: 0.324578  [  128/  265]
train() client id: f_00001-3-4 loss: 0.252346  [  160/  265]
train() client id: f_00001-3-5 loss: 0.416002  [  192/  265]
train() client id: f_00001-3-6 loss: 0.318350  [  224/  265]
train() client id: f_00001-3-7 loss: 0.384302  [  256/  265]
train() client id: f_00001-4-0 loss: 0.245791  [   32/  265]
train() client id: f_00001-4-1 loss: 0.411319  [   64/  265]
train() client id: f_00001-4-2 loss: 0.252733  [   96/  265]
train() client id: f_00001-4-3 loss: 0.261128  [  128/  265]
train() client id: f_00001-4-4 loss: 0.403679  [  160/  265]
train() client id: f_00001-4-5 loss: 0.299935  [  192/  265]
train() client id: f_00001-4-6 loss: 0.413275  [  224/  265]
train() client id: f_00001-4-7 loss: 0.451462  [  256/  265]
train() client id: f_00001-5-0 loss: 0.356869  [   32/  265]
train() client id: f_00001-5-1 loss: 0.322608  [   64/  265]
train() client id: f_00001-5-2 loss: 0.345647  [   96/  265]
train() client id: f_00001-5-3 loss: 0.316374  [  128/  265]
train() client id: f_00001-5-4 loss: 0.298218  [  160/  265]
train() client id: f_00001-5-5 loss: 0.358309  [  192/  265]
train() client id: f_00001-5-6 loss: 0.345845  [  224/  265]
train() client id: f_00001-5-7 loss: 0.302799  [  256/  265]
train() client id: f_00001-6-0 loss: 0.292128  [   32/  265]
train() client id: f_00001-6-1 loss: 0.306233  [   64/  265]
train() client id: f_00001-6-2 loss: 0.298579  [   96/  265]
train() client id: f_00001-6-3 loss: 0.316263  [  128/  265]
train() client id: f_00001-6-4 loss: 0.382772  [  160/  265]
train() client id: f_00001-6-5 loss: 0.442545  [  192/  265]
train() client id: f_00001-6-6 loss: 0.239334  [  224/  265]
train() client id: f_00001-6-7 loss: 0.414862  [  256/  265]
train() client id: f_00001-7-0 loss: 0.313345  [   32/  265]
train() client id: f_00001-7-1 loss: 0.392344  [   64/  265]
train() client id: f_00001-7-2 loss: 0.250289  [   96/  265]
train() client id: f_00001-7-3 loss: 0.311079  [  128/  265]
train() client id: f_00001-7-4 loss: 0.316434  [  160/  265]
train() client id: f_00001-7-5 loss: 0.434093  [  192/  265]
train() client id: f_00001-7-6 loss: 0.261383  [  224/  265]
train() client id: f_00001-7-7 loss: 0.349250  [  256/  265]
train() client id: f_00001-8-0 loss: 0.318567  [   32/  265]
train() client id: f_00001-8-1 loss: 0.424163  [   64/  265]
train() client id: f_00001-8-2 loss: 0.230060  [   96/  265]
train() client id: f_00001-8-3 loss: 0.374193  [  128/  265]
train() client id: f_00001-8-4 loss: 0.429639  [  160/  265]
train() client id: f_00001-8-5 loss: 0.340107  [  192/  265]
train() client id: f_00001-8-6 loss: 0.301100  [  224/  265]
train() client id: f_00001-8-7 loss: 0.251564  [  256/  265]
train() client id: f_00001-9-0 loss: 0.358297  [   32/  265]
train() client id: f_00001-9-1 loss: 0.261151  [   64/  265]
train() client id: f_00001-9-2 loss: 0.363745  [   96/  265]
train() client id: f_00001-9-3 loss: 0.348044  [  128/  265]
train() client id: f_00001-9-4 loss: 0.386073  [  160/  265]
train() client id: f_00001-9-5 loss: 0.326303  [  192/  265]
train() client id: f_00001-9-6 loss: 0.249143  [  224/  265]
train() client id: f_00001-9-7 loss: 0.350348  [  256/  265]
train() client id: f_00001-10-0 loss: 0.246304  [   32/  265]
train() client id: f_00001-10-1 loss: 0.441558  [   64/  265]
train() client id: f_00001-10-2 loss: 0.435011  [   96/  265]
train() client id: f_00001-10-3 loss: 0.288654  [  128/  265]
train() client id: f_00001-10-4 loss: 0.231828  [  160/  265]
train() client id: f_00001-10-5 loss: 0.261823  [  192/  265]
train() client id: f_00001-10-6 loss: 0.398490  [  224/  265]
train() client id: f_00001-10-7 loss: 0.333298  [  256/  265]
train() client id: f_00001-11-0 loss: 0.439909  [   32/  265]
train() client id: f_00001-11-1 loss: 0.393991  [   64/  265]
train() client id: f_00001-11-2 loss: 0.356953  [   96/  265]
train() client id: f_00001-11-3 loss: 0.252477  [  128/  265]
train() client id: f_00001-11-4 loss: 0.224878  [  160/  265]
train() client id: f_00001-11-5 loss: 0.291293  [  192/  265]
train() client id: f_00001-11-6 loss: 0.312602  [  224/  265]
train() client id: f_00001-11-7 loss: 0.286233  [  256/  265]
train() client id: f_00002-0-0 loss: 1.163693  [   32/  124]
train() client id: f_00002-0-1 loss: 1.116429  [   64/  124]
train() client id: f_00002-0-2 loss: 1.057187  [   96/  124]
train() client id: f_00002-1-0 loss: 1.030194  [   32/  124]
train() client id: f_00002-1-1 loss: 1.207877  [   64/  124]
train() client id: f_00002-1-2 loss: 1.168519  [   96/  124]
train() client id: f_00002-2-0 loss: 1.191345  [   32/  124]
train() client id: f_00002-2-1 loss: 1.049650  [   64/  124]
train() client id: f_00002-2-2 loss: 1.091035  [   96/  124]
train() client id: f_00002-3-0 loss: 1.130524  [   32/  124]
train() client id: f_00002-3-1 loss: 1.017891  [   64/  124]
train() client id: f_00002-3-2 loss: 1.096181  [   96/  124]
train() client id: f_00002-4-0 loss: 0.872752  [   32/  124]
train() client id: f_00002-4-1 loss: 1.151770  [   64/  124]
train() client id: f_00002-4-2 loss: 1.109530  [   96/  124]
train() client id: f_00002-5-0 loss: 1.119622  [   32/  124]
train() client id: f_00002-5-1 loss: 1.096386  [   64/  124]
train() client id: f_00002-5-2 loss: 0.865552  [   96/  124]
train() client id: f_00002-6-0 loss: 0.894836  [   32/  124]
train() client id: f_00002-6-1 loss: 0.901928  [   64/  124]
train() client id: f_00002-6-2 loss: 1.127768  [   96/  124]
train() client id: f_00002-7-0 loss: 0.712772  [   32/  124]
train() client id: f_00002-7-1 loss: 1.051734  [   64/  124]
train() client id: f_00002-7-2 loss: 1.079372  [   96/  124]
train() client id: f_00002-8-0 loss: 0.846920  [   32/  124]
train() client id: f_00002-8-1 loss: 0.789745  [   64/  124]
train() client id: f_00002-8-2 loss: 1.031630  [   96/  124]
train() client id: f_00002-9-0 loss: 0.996430  [   32/  124]
train() client id: f_00002-9-1 loss: 0.747926  [   64/  124]
train() client id: f_00002-9-2 loss: 0.858018  [   96/  124]
train() client id: f_00002-10-0 loss: 0.818226  [   32/  124]
train() client id: f_00002-10-1 loss: 1.082574  [   64/  124]
train() client id: f_00002-10-2 loss: 0.818760  [   96/  124]
train() client id: f_00002-11-0 loss: 0.890716  [   32/  124]
train() client id: f_00002-11-1 loss: 0.884937  [   64/  124]
train() client id: f_00002-11-2 loss: 0.938244  [   96/  124]
train() client id: f_00003-0-0 loss: 0.652549  [   32/   43]
train() client id: f_00003-1-0 loss: 0.647219  [   32/   43]
train() client id: f_00003-2-0 loss: 0.840460  [   32/   43]
train() client id: f_00003-3-0 loss: 0.761222  [   32/   43]
train() client id: f_00003-4-0 loss: 0.654456  [   32/   43]
train() client id: f_00003-5-0 loss: 0.790747  [   32/   43]
train() client id: f_00003-6-0 loss: 0.598696  [   32/   43]
train() client id: f_00003-7-0 loss: 0.841444  [   32/   43]
train() client id: f_00003-8-0 loss: 0.693941  [   32/   43]
train() client id: f_00003-9-0 loss: 0.701716  [   32/   43]
train() client id: f_00003-10-0 loss: 0.661352  [   32/   43]
train() client id: f_00003-11-0 loss: 0.861636  [   32/   43]
train() client id: f_00004-0-0 loss: 0.876116  [   32/  306]
train() client id: f_00004-0-1 loss: 1.068946  [   64/  306]
train() client id: f_00004-0-2 loss: 0.868041  [   96/  306]
train() client id: f_00004-0-3 loss: 1.002368  [  128/  306]
train() client id: f_00004-0-4 loss: 0.936372  [  160/  306]
train() client id: f_00004-0-5 loss: 0.996342  [  192/  306]
train() client id: f_00004-0-6 loss: 1.060140  [  224/  306]
train() client id: f_00004-0-7 loss: 1.149953  [  256/  306]
train() client id: f_00004-0-8 loss: 1.060965  [  288/  306]
train() client id: f_00004-1-0 loss: 0.946669  [   32/  306]
train() client id: f_00004-1-1 loss: 0.800230  [   64/  306]
train() client id: f_00004-1-2 loss: 0.993073  [   96/  306]
train() client id: f_00004-1-3 loss: 1.043275  [  128/  306]
train() client id: f_00004-1-4 loss: 0.954931  [  160/  306]
train() client id: f_00004-1-5 loss: 0.956788  [  192/  306]
train() client id: f_00004-1-6 loss: 1.048283  [  224/  306]
train() client id: f_00004-1-7 loss: 1.056062  [  256/  306]
train() client id: f_00004-1-8 loss: 1.128399  [  288/  306]
train() client id: f_00004-2-0 loss: 0.918252  [   32/  306]
train() client id: f_00004-2-1 loss: 0.926041  [   64/  306]
train() client id: f_00004-2-2 loss: 0.947124  [   96/  306]
train() client id: f_00004-2-3 loss: 1.058897  [  128/  306]
train() client id: f_00004-2-4 loss: 0.949162  [  160/  306]
train() client id: f_00004-2-5 loss: 0.982409  [  192/  306]
train() client id: f_00004-2-6 loss: 0.931544  [  224/  306]
train() client id: f_00004-2-7 loss: 1.168173  [  256/  306]
train() client id: f_00004-2-8 loss: 0.989566  [  288/  306]
train() client id: f_00004-3-0 loss: 1.037117  [   32/  306]
train() client id: f_00004-3-1 loss: 0.957037  [   64/  306]
train() client id: f_00004-3-2 loss: 1.052259  [   96/  306]
train() client id: f_00004-3-3 loss: 1.034396  [  128/  306]
train() client id: f_00004-3-4 loss: 1.112209  [  160/  306]
train() client id: f_00004-3-5 loss: 0.983509  [  192/  306]
train() client id: f_00004-3-6 loss: 0.887531  [  224/  306]
train() client id: f_00004-3-7 loss: 0.911013  [  256/  306]
train() client id: f_00004-3-8 loss: 0.964724  [  288/  306]
train() client id: f_00004-4-0 loss: 1.086825  [   32/  306]
train() client id: f_00004-4-1 loss: 0.984841  [   64/  306]
train() client id: f_00004-4-2 loss: 0.870876  [   96/  306]
train() client id: f_00004-4-3 loss: 0.970533  [  128/  306]
train() client id: f_00004-4-4 loss: 1.044647  [  160/  306]
train() client id: f_00004-4-5 loss: 1.053213  [  192/  306]
train() client id: f_00004-4-6 loss: 0.894602  [  224/  306]
train() client id: f_00004-4-7 loss: 0.879966  [  256/  306]
train() client id: f_00004-4-8 loss: 1.009997  [  288/  306]
train() client id: f_00004-5-0 loss: 0.971826  [   32/  306]
train() client id: f_00004-5-1 loss: 0.984212  [   64/  306]
train() client id: f_00004-5-2 loss: 1.023620  [   96/  306]
train() client id: f_00004-5-3 loss: 0.970187  [  128/  306]
train() client id: f_00004-5-4 loss: 1.046133  [  160/  306]
train() client id: f_00004-5-5 loss: 0.992453  [  192/  306]
train() client id: f_00004-5-6 loss: 0.972802  [  224/  306]
train() client id: f_00004-5-7 loss: 0.927119  [  256/  306]
train() client id: f_00004-5-8 loss: 0.848610  [  288/  306]
train() client id: f_00004-6-0 loss: 0.943949  [   32/  306]
train() client id: f_00004-6-1 loss: 1.064890  [   64/  306]
train() client id: f_00004-6-2 loss: 0.893848  [   96/  306]
train() client id: f_00004-6-3 loss: 0.914995  [  128/  306]
train() client id: f_00004-6-4 loss: 0.976740  [  160/  306]
train() client id: f_00004-6-5 loss: 0.971851  [  192/  306]
train() client id: f_00004-6-6 loss: 0.966585  [  224/  306]
train() client id: f_00004-6-7 loss: 0.935721  [  256/  306]
train() client id: f_00004-6-8 loss: 1.047519  [  288/  306]
train() client id: f_00004-7-0 loss: 0.982256  [   32/  306]
train() client id: f_00004-7-1 loss: 1.032255  [   64/  306]
train() client id: f_00004-7-2 loss: 0.939492  [   96/  306]
train() client id: f_00004-7-3 loss: 0.975232  [  128/  306]
train() client id: f_00004-7-4 loss: 0.932272  [  160/  306]
train() client id: f_00004-7-5 loss: 0.945004  [  192/  306]
train() client id: f_00004-7-6 loss: 1.067452  [  224/  306]
train() client id: f_00004-7-7 loss: 0.888384  [  256/  306]
train() client id: f_00004-7-8 loss: 1.060166  [  288/  306]
train() client id: f_00004-8-0 loss: 0.902213  [   32/  306]
train() client id: f_00004-8-1 loss: 0.999840  [   64/  306]
train() client id: f_00004-8-2 loss: 0.903731  [   96/  306]
train() client id: f_00004-8-3 loss: 1.023853  [  128/  306]
train() client id: f_00004-8-4 loss: 1.043556  [  160/  306]
train() client id: f_00004-8-5 loss: 0.909442  [  192/  306]
train() client id: f_00004-8-6 loss: 1.074931  [  224/  306]
train() client id: f_00004-8-7 loss: 1.002200  [  256/  306]
train() client id: f_00004-8-8 loss: 0.921387  [  288/  306]
train() client id: f_00004-9-0 loss: 0.893289  [   32/  306]
train() client id: f_00004-9-1 loss: 0.947323  [   64/  306]
train() client id: f_00004-9-2 loss: 0.963893  [   96/  306]
train() client id: f_00004-9-3 loss: 0.867797  [  128/  306]
train() client id: f_00004-9-4 loss: 1.079539  [  160/  306]
train() client id: f_00004-9-5 loss: 0.951059  [  192/  306]
train() client id: f_00004-9-6 loss: 0.966535  [  224/  306]
train() client id: f_00004-9-7 loss: 0.960624  [  256/  306]
train() client id: f_00004-9-8 loss: 1.081537  [  288/  306]
train() client id: f_00004-10-0 loss: 0.960985  [   32/  306]
train() client id: f_00004-10-1 loss: 0.964866  [   64/  306]
train() client id: f_00004-10-2 loss: 1.091349  [   96/  306]
train() client id: f_00004-10-3 loss: 0.876498  [  128/  306]
train() client id: f_00004-10-4 loss: 1.038215  [  160/  306]
train() client id: f_00004-10-5 loss: 0.964523  [  192/  306]
train() client id: f_00004-10-6 loss: 1.004471  [  224/  306]
train() client id: f_00004-10-7 loss: 0.923261  [  256/  306]
train() client id: f_00004-10-8 loss: 0.964843  [  288/  306]
train() client id: f_00004-11-0 loss: 1.061463  [   32/  306]
train() client id: f_00004-11-1 loss: 0.923838  [   64/  306]
train() client id: f_00004-11-2 loss: 0.885419  [   96/  306]
train() client id: f_00004-11-3 loss: 0.906015  [  128/  306]
train() client id: f_00004-11-4 loss: 0.937719  [  160/  306]
train() client id: f_00004-11-5 loss: 0.945847  [  192/  306]
train() client id: f_00004-11-6 loss: 1.094225  [  224/  306]
train() client id: f_00004-11-7 loss: 0.961633  [  256/  306]
train() client id: f_00004-11-8 loss: 0.983934  [  288/  306]
train() client id: f_00005-0-0 loss: 0.580565  [   32/  146]
train() client id: f_00005-0-1 loss: 0.259394  [   64/  146]
train() client id: f_00005-0-2 loss: 0.522478  [   96/  146]
train() client id: f_00005-0-3 loss: 0.724534  [  128/  146]
train() client id: f_00005-1-0 loss: 0.322839  [   32/  146]
train() client id: f_00005-1-1 loss: 0.458151  [   64/  146]
train() client id: f_00005-1-2 loss: 0.624947  [   96/  146]
train() client id: f_00005-1-3 loss: 0.491379  [  128/  146]
train() client id: f_00005-2-0 loss: 0.678790  [   32/  146]
train() client id: f_00005-2-1 loss: 0.394086  [   64/  146]
train() client id: f_00005-2-2 loss: 0.392683  [   96/  146]
train() client id: f_00005-2-3 loss: 0.368805  [  128/  146]
train() client id: f_00005-3-0 loss: 0.526569  [   32/  146]
train() client id: f_00005-3-1 loss: 0.429477  [   64/  146]
train() client id: f_00005-3-2 loss: 0.480828  [   96/  146]
train() client id: f_00005-3-3 loss: 0.470348  [  128/  146]
train() client id: f_00005-4-0 loss: 0.399491  [   32/  146]
train() client id: f_00005-4-1 loss: 0.397547  [   64/  146]
train() client id: f_00005-4-2 loss: 0.393073  [   96/  146]
train() client id: f_00005-4-3 loss: 0.652936  [  128/  146]
train() client id: f_00005-5-0 loss: 0.460431  [   32/  146]
train() client id: f_00005-5-1 loss: 0.516434  [   64/  146]
train() client id: f_00005-5-2 loss: 0.579817  [   96/  146]
train() client id: f_00005-5-3 loss: 0.319199  [  128/  146]
train() client id: f_00005-6-0 loss: 0.472930  [   32/  146]
train() client id: f_00005-6-1 loss: 0.559681  [   64/  146]
train() client id: f_00005-6-2 loss: 0.442040  [   96/  146]
train() client id: f_00005-6-3 loss: 0.288618  [  128/  146]
train() client id: f_00005-7-0 loss: 0.463075  [   32/  146]
train() client id: f_00005-7-1 loss: 0.386504  [   64/  146]
train() client id: f_00005-7-2 loss: 0.327929  [   96/  146]
train() client id: f_00005-7-3 loss: 0.598937  [  128/  146]
train() client id: f_00005-8-0 loss: 0.302888  [   32/  146]
train() client id: f_00005-8-1 loss: 0.491130  [   64/  146]
train() client id: f_00005-8-2 loss: 0.487677  [   96/  146]
train() client id: f_00005-8-3 loss: 0.454783  [  128/  146]
train() client id: f_00005-9-0 loss: 0.425794  [   32/  146]
train() client id: f_00005-9-1 loss: 0.305183  [   64/  146]
train() client id: f_00005-9-2 loss: 0.221930  [   96/  146]
train() client id: f_00005-9-3 loss: 0.579502  [  128/  146]
train() client id: f_00005-10-0 loss: 0.286114  [   32/  146]
train() client id: f_00005-10-1 loss: 0.711910  [   64/  146]
train() client id: f_00005-10-2 loss: 0.550681  [   96/  146]
train() client id: f_00005-10-3 loss: 0.293357  [  128/  146]
train() client id: f_00005-11-0 loss: 0.619350  [   32/  146]
train() client id: f_00005-11-1 loss: 0.554730  [   64/  146]
train() client id: f_00005-11-2 loss: 0.392690  [   96/  146]
train() client id: f_00005-11-3 loss: 0.242108  [  128/  146]
train() client id: f_00006-0-0 loss: 0.559360  [   32/   54]
train() client id: f_00006-1-0 loss: 0.549077  [   32/   54]
train() client id: f_00006-2-0 loss: 0.495757  [   32/   54]
train() client id: f_00006-3-0 loss: 0.534205  [   32/   54]
train() client id: f_00006-4-0 loss: 0.456145  [   32/   54]
train() client id: f_00006-5-0 loss: 0.446718  [   32/   54]
train() client id: f_00006-6-0 loss: 0.513410  [   32/   54]
train() client id: f_00006-7-0 loss: 0.526459  [   32/   54]
train() client id: f_00006-8-0 loss: 0.488612  [   32/   54]
train() client id: f_00006-9-0 loss: 0.521308  [   32/   54]
train() client id: f_00006-10-0 loss: 0.515369  [   32/   54]
train() client id: f_00006-11-0 loss: 0.476880  [   32/   54]
train() client id: f_00007-0-0 loss: 0.486877  [   32/  179]
train() client id: f_00007-0-1 loss: 0.540864  [   64/  179]
train() client id: f_00007-0-2 loss: 0.360710  [   96/  179]
train() client id: f_00007-0-3 loss: 0.429520  [  128/  179]
train() client id: f_00007-0-4 loss: 0.433438  [  160/  179]
train() client id: f_00007-1-0 loss: 0.492784  [   32/  179]
train() client id: f_00007-1-1 loss: 0.485225  [   64/  179]
train() client id: f_00007-1-2 loss: 0.439280  [   96/  179]
train() client id: f_00007-1-3 loss: 0.392811  [  128/  179]
train() client id: f_00007-1-4 loss: 0.396534  [  160/  179]
train() client id: f_00007-2-0 loss: 0.360625  [   32/  179]
train() client id: f_00007-2-1 loss: 0.567131  [   64/  179]
train() client id: f_00007-2-2 loss: 0.293615  [   96/  179]
train() client id: f_00007-2-3 loss: 0.478017  [  128/  179]
train() client id: f_00007-2-4 loss: 0.544927  [  160/  179]
train() client id: f_00007-3-0 loss: 0.460953  [   32/  179]
train() client id: f_00007-3-1 loss: 0.414754  [   64/  179]
train() client id: f_00007-3-2 loss: 0.380332  [   96/  179]
train() client id: f_00007-3-3 loss: 0.248875  [  128/  179]
train() client id: f_00007-3-4 loss: 0.800115  [  160/  179]
train() client id: f_00007-4-0 loss: 0.558963  [   32/  179]
train() client id: f_00007-4-1 loss: 0.336791  [   64/  179]
train() client id: f_00007-4-2 loss: 0.387728  [   96/  179]
train() client id: f_00007-4-3 loss: 0.353185  [  128/  179]
train() client id: f_00007-4-4 loss: 0.575551  [  160/  179]
train() client id: f_00007-5-0 loss: 0.356714  [   32/  179]
train() client id: f_00007-5-1 loss: 0.324125  [   64/  179]
train() client id: f_00007-5-2 loss: 0.455000  [   96/  179]
train() client id: f_00007-5-3 loss: 0.652535  [  128/  179]
train() client id: f_00007-5-4 loss: 0.356643  [  160/  179]
train() client id: f_00007-6-0 loss: 0.442653  [   32/  179]
train() client id: f_00007-6-1 loss: 0.654313  [   64/  179]
train() client id: f_00007-6-2 loss: 0.368861  [   96/  179]
train() client id: f_00007-6-3 loss: 0.417239  [  128/  179]
train() client id: f_00007-6-4 loss: 0.277724  [  160/  179]
train() client id: f_00007-7-0 loss: 0.395263  [   32/  179]
train() client id: f_00007-7-1 loss: 0.280210  [   64/  179]
train() client id: f_00007-7-2 loss: 0.302172  [   96/  179]
train() client id: f_00007-7-3 loss: 0.508962  [  128/  179]
train() client id: f_00007-7-4 loss: 0.672379  [  160/  179]
train() client id: f_00007-8-0 loss: 0.651554  [   32/  179]
train() client id: f_00007-8-1 loss: 0.366362  [   64/  179]
train() client id: f_00007-8-2 loss: 0.551881  [   96/  179]
train() client id: f_00007-8-3 loss: 0.382425  [  128/  179]
train() client id: f_00007-8-4 loss: 0.241457  [  160/  179]
train() client id: f_00007-9-0 loss: 0.463119  [   32/  179]
train() client id: f_00007-9-1 loss: 0.386936  [   64/  179]
train() client id: f_00007-9-2 loss: 0.519359  [   96/  179]
train() client id: f_00007-9-3 loss: 0.452778  [  128/  179]
train() client id: f_00007-9-4 loss: 0.425690  [  160/  179]
train() client id: f_00007-10-0 loss: 0.437175  [   32/  179]
train() client id: f_00007-10-1 loss: 0.463093  [   64/  179]
train() client id: f_00007-10-2 loss: 0.370449  [   96/  179]
train() client id: f_00007-10-3 loss: 0.445456  [  128/  179]
train() client id: f_00007-10-4 loss: 0.509540  [  160/  179]
train() client id: f_00007-11-0 loss: 0.263299  [   32/  179]
train() client id: f_00007-11-1 loss: 0.279425  [   64/  179]
train() client id: f_00007-11-2 loss: 0.533368  [   96/  179]
train() client id: f_00007-11-3 loss: 0.667270  [  128/  179]
train() client id: f_00007-11-4 loss: 0.357851  [  160/  179]
train() client id: f_00008-0-0 loss: 0.772716  [   32/  130]
train() client id: f_00008-0-1 loss: 0.879489  [   64/  130]
train() client id: f_00008-0-2 loss: 0.783202  [   96/  130]
train() client id: f_00008-0-3 loss: 0.739973  [  128/  130]
train() client id: f_00008-1-0 loss: 0.725089  [   32/  130]
train() client id: f_00008-1-1 loss: 0.787521  [   64/  130]
train() client id: f_00008-1-2 loss: 0.863813  [   96/  130]
train() client id: f_00008-1-3 loss: 0.828020  [  128/  130]
train() client id: f_00008-2-0 loss: 0.849770  [   32/  130]
train() client id: f_00008-2-1 loss: 0.729033  [   64/  130]
train() client id: f_00008-2-2 loss: 0.702562  [   96/  130]
train() client id: f_00008-2-3 loss: 0.902533  [  128/  130]
train() client id: f_00008-3-0 loss: 0.895035  [   32/  130]
train() client id: f_00008-3-1 loss: 0.675595  [   64/  130]
train() client id: f_00008-3-2 loss: 0.710598  [   96/  130]
train() client id: f_00008-3-3 loss: 0.897340  [  128/  130]
train() client id: f_00008-4-0 loss: 0.771563  [   32/  130]
train() client id: f_00008-4-1 loss: 0.841324  [   64/  130]
train() client id: f_00008-4-2 loss: 0.713551  [   96/  130]
train() client id: f_00008-4-3 loss: 0.874855  [  128/  130]
train() client id: f_00008-5-0 loss: 0.675837  [   32/  130]
train() client id: f_00008-5-1 loss: 0.829174  [   64/  130]
train() client id: f_00008-5-2 loss: 0.818577  [   96/  130]
train() client id: f_00008-5-3 loss: 0.881777  [  128/  130]
train() client id: f_00008-6-0 loss: 0.799591  [   32/  130]
train() client id: f_00008-6-1 loss: 0.877482  [   64/  130]
train() client id: f_00008-6-2 loss: 0.724408  [   96/  130]
train() client id: f_00008-6-3 loss: 0.811216  [  128/  130]
train() client id: f_00008-7-0 loss: 0.779751  [   32/  130]
train() client id: f_00008-7-1 loss: 0.841966  [   64/  130]
train() client id: f_00008-7-2 loss: 0.701367  [   96/  130]
train() client id: f_00008-7-3 loss: 0.835606  [  128/  130]
train() client id: f_00008-8-0 loss: 0.885272  [   32/  130]
train() client id: f_00008-8-1 loss: 0.801816  [   64/  130]
train() client id: f_00008-8-2 loss: 0.826194  [   96/  130]
train() client id: f_00008-8-3 loss: 0.683304  [  128/  130]
train() client id: f_00008-9-0 loss: 0.753659  [   32/  130]
train() client id: f_00008-9-1 loss: 0.776271  [   64/  130]
train() client id: f_00008-9-2 loss: 0.950693  [   96/  130]
train() client id: f_00008-9-3 loss: 0.740749  [  128/  130]
train() client id: f_00008-10-0 loss: 0.856756  [   32/  130]
train() client id: f_00008-10-1 loss: 0.815886  [   64/  130]
train() client id: f_00008-10-2 loss: 0.799683  [   96/  130]
train() client id: f_00008-10-3 loss: 0.712352  [  128/  130]
train() client id: f_00008-11-0 loss: 0.874469  [   32/  130]
train() client id: f_00008-11-1 loss: 0.702054  [   64/  130]
train() client id: f_00008-11-2 loss: 0.857739  [   96/  130]
train() client id: f_00008-11-3 loss: 0.789009  [  128/  130]
train() client id: f_00009-0-0 loss: 0.951343  [   32/  118]
train() client id: f_00009-0-1 loss: 1.286077  [   64/  118]
train() client id: f_00009-0-2 loss: 0.857710  [   96/  118]
train() client id: f_00009-1-0 loss: 0.915515  [   32/  118]
train() client id: f_00009-1-1 loss: 1.117031  [   64/  118]
train() client id: f_00009-1-2 loss: 0.868534  [   96/  118]
train() client id: f_00009-2-0 loss: 0.798446  [   32/  118]
train() client id: f_00009-2-1 loss: 1.023602  [   64/  118]
train() client id: f_00009-2-2 loss: 0.987099  [   96/  118]
train() client id: f_00009-3-0 loss: 0.988170  [   32/  118]
train() client id: f_00009-3-1 loss: 0.959977  [   64/  118]
train() client id: f_00009-3-2 loss: 0.762824  [   96/  118]
train() client id: f_00009-4-0 loss: 0.827195  [   32/  118]
train() client id: f_00009-4-1 loss: 0.990575  [   64/  118]
train() client id: f_00009-4-2 loss: 0.898628  [   96/  118]
train() client id: f_00009-5-0 loss: 0.998722  [   32/  118]
train() client id: f_00009-5-1 loss: 0.893572  [   64/  118]
train() client id: f_00009-5-2 loss: 0.802466  [   96/  118]
train() client id: f_00009-6-0 loss: 0.956741  [   32/  118]
train() client id: f_00009-6-1 loss: 0.752647  [   64/  118]
train() client id: f_00009-6-2 loss: 0.793738  [   96/  118]
train() client id: f_00009-7-0 loss: 0.680471  [   32/  118]
train() client id: f_00009-7-1 loss: 0.746352  [   64/  118]
train() client id: f_00009-7-2 loss: 1.010247  [   96/  118]
train() client id: f_00009-8-0 loss: 0.813965  [   32/  118]
train() client id: f_00009-8-1 loss: 0.849215  [   64/  118]
train() client id: f_00009-8-2 loss: 0.797134  [   96/  118]
train() client id: f_00009-9-0 loss: 0.870007  [   32/  118]
train() client id: f_00009-9-1 loss: 0.838027  [   64/  118]
train() client id: f_00009-9-2 loss: 0.692803  [   96/  118]
train() client id: f_00009-10-0 loss: 0.742646  [   32/  118]
train() client id: f_00009-10-1 loss: 0.750432  [   64/  118]
train() client id: f_00009-10-2 loss: 0.796714  [   96/  118]
train() client id: f_00009-11-0 loss: 0.656043  [   32/  118]
train() client id: f_00009-11-1 loss: 0.822258  [   64/  118]
train() client id: f_00009-11-2 loss: 0.827093  [   96/  118]
At round 25 accuracy: 0.6392572944297082
At round 25 training accuracy: 0.5814889336016097
At round 25 training loss: 0.8413780736586418
update_location
xs = -3.905658 4.200318 145.009024 18.811294 0.979296 3.956410 -107.443192 -86.324852 129.663977 -72.060879 
ys = 137.587959 120.555839 1.320614 -107.455176 99.350187 82.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 3.2885738318825704
ys mean: 35.39414253552871
dists_uav = 170.134361 156.688713 176.151529 147.988106 140.966019 129.899333 146.802350 132.108502 164.685813 123.323891 
uav_gains = -105.792775 -104.884001 -106.182446 -104.259476 -103.729845 -102.840821 -104.171745 -103.024087 -105.431664 -102.276495 
uav_gains_db_mean: -104.25933534662786
dists_bs = 175.136902 187.288336 364.102052 342.554732 191.550534 201.304812 190.038305 195.451110 342.935353 199.591974 
bs_gains = -102.381634 -103.197360 -111.281307 -110.539498 -103.470993 -104.074976 -103.374611 -103.716127 -110.553002 -103.971065 
bs_gains_db_mean: -105.65605732657863
Round 26
-------------------------------
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.5605131  15.70320748  7.4465293   2.67409633 18.11274305  8.72126419
  3.31940453 10.65181908  7.8502879   7.07624586]
obj_prev = 89.11611082472227
eta_min = 5.392428774203226e-13	eta_max = 0.9242899713750001
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 20.70487256941968	eta = 0.909090909090909
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 36.71367954165409	eta = 0.5126865969778287
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 28.972791112910695	eta = 0.6496651065957375
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.57869875098564	eta = 0.6825054219090905
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.5077842638857	eta = 0.6842649064780151
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.50758684227239	eta = 0.6842698174388544
eta = 0.6842698174388544
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [0.03130742 0.06584498 0.03081048 0.01068428 0.07603231 0.03627686
 0.01341747 0.04447642 0.03230131 0.02931965]
ene_total = [2.4173172  4.46359599 2.39783696 1.11945579 5.09142412 2.67769452
 1.2842835  3.15534052 2.65007964 2.25055859]
ti_comp = [0.3872178  0.39743138 0.38543905 0.39356432 0.39647217 0.39426467
 0.39389891 0.39802621 0.35903228 0.39465362]
ti_coms = [0.08286047 0.07264689 0.08463921 0.07651394 0.0736061  0.07581359
 0.07617935 0.07205206 0.11104599 0.07542464]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.27912028e-05 1.12959740e-04 1.23045006e-05 4.92135047e-07
 1.74762896e-04 1.91952252e-05 9.73020721e-07 3.47093215e-05
 1.63408045e-05 1.01140293e-05]
ene_total = [0.50667253 0.45043075 0.51750266 0.46717392 0.46006034 0.46403994
 0.4651605  0.44202165 0.67897151 0.46111082]
optimize_network iter = 0 obj = 4.913144626268169
eta = 0.6842698174388544
freqs = [40426116.40412594 82838172.3252855  39968031.32913876 13573739.87970061
 95886067.29898961 46005714.64645749 17031612.51169543 55871219.40930291
 44983852.28187121 37146055.45423073]
eta_min = 0.6842698174388597	eta_max = 0.6842698174388548
af = 0.020050708162629324	bf = 1.5436207515894453	zeta = 0.022055778978892257	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [3.21329018e-06 2.83767235e-05 3.09102528e-06 1.23629712e-07
 4.39023530e-05 4.82205074e-06 2.44433458e-07 8.71936159e-06
 4.10498900e-06 2.54075489e-06]
ene_total = [1.757659   1.54642537 1.79534967 1.62243185 1.57005655 1.60857787
 1.61536282 1.52964443 2.35549529 1.59984681]
ti_comp = [0.3872178  0.39743138 0.38543905 0.39356432 0.39647217 0.39426467
 0.39389891 0.39802621 0.35903228 0.39465362]
ti_coms = [0.08286047 0.07264689 0.08463921 0.07651394 0.0736061  0.07581359
 0.07617935 0.07205206 0.11104599 0.07542464]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.27912028e-05 1.12959740e-04 1.23045006e-05 4.92135047e-07
 1.74762896e-04 1.91952252e-05 9.73020721e-07 3.47093215e-05
 1.63408045e-05 1.01140293e-05]
ene_total = [0.50667253 0.45043075 0.51750266 0.46717392 0.46006034 0.46403994
 0.4651605  0.44202165 0.67897151 0.46111082]
optimize_network iter = 1 obj = 4.913144626268177
eta = 0.6842698174388548
freqs = [40426116.40412594 82838172.32528551 39968031.32913877 13573739.87970061
 95886067.29898961 46005714.64645751 17031612.51169543 55871219.40930291
 44983852.28187121 37146055.45423073]
Done!
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.23550879e-05 1.09108388e-04 1.18849799e-05 4.75355749e-07
 1.68804372e-04 1.85407658e-05 9.39845672e-07 3.35259105e-05
 1.57836663e-05 9.76919241e-06]
ene_total = [0.0082984  0.0073738  0.00847581 0.00765187 0.00752941 0.0075999
 0.00761888 0.00723873 0.01112038 0.00755223]
At round 26 energy consumption: 0.08045941191860002
At round 26 eta: 0.6842698174388548
At round 26 a_n: 19.276410835052125
At round 26 local rounds: 12.423580809938418
At round 26 global rounds: 61.05343074483861
gradient difference: 0.4315962791442871
train() client id: f_00000-0-0 loss: 1.003634  [   32/  126]
train() client id: f_00000-0-1 loss: 1.186521  [   64/  126]
train() client id: f_00000-0-2 loss: 0.965191  [   96/  126]
train() client id: f_00000-1-0 loss: 1.103629  [   32/  126]
train() client id: f_00000-1-1 loss: 0.930368  [   64/  126]
train() client id: f_00000-1-2 loss: 1.004882  [   96/  126]
train() client id: f_00000-2-0 loss: 1.049664  [   32/  126]
train() client id: f_00000-2-1 loss: 0.879812  [   64/  126]
train() client id: f_00000-2-2 loss: 0.879028  [   96/  126]
train() client id: f_00000-3-0 loss: 0.928969  [   32/  126]
train() client id: f_00000-3-1 loss: 0.959463  [   64/  126]
train() client id: f_00000-3-2 loss: 0.883175  [   96/  126]
train() client id: f_00000-4-0 loss: 0.946975  [   32/  126]
train() client id: f_00000-4-1 loss: 0.790271  [   64/  126]
train() client id: f_00000-4-2 loss: 0.866588  [   96/  126]
train() client id: f_00000-5-0 loss: 0.881240  [   32/  126]
train() client id: f_00000-5-1 loss: 0.816589  [   64/  126]
train() client id: f_00000-5-2 loss: 0.894832  [   96/  126]
train() client id: f_00000-6-0 loss: 0.937834  [   32/  126]
train() client id: f_00000-6-1 loss: 0.744474  [   64/  126]
train() client id: f_00000-6-2 loss: 0.862415  [   96/  126]
train() client id: f_00000-7-0 loss: 0.861462  [   32/  126]
train() client id: f_00000-7-1 loss: 0.801178  [   64/  126]
train() client id: f_00000-7-2 loss: 0.747388  [   96/  126]
train() client id: f_00000-8-0 loss: 0.791244  [   32/  126]
train() client id: f_00000-8-1 loss: 0.833994  [   64/  126]
train() client id: f_00000-8-2 loss: 0.753565  [   96/  126]
train() client id: f_00000-9-0 loss: 0.830380  [   32/  126]
train() client id: f_00000-9-1 loss: 0.859086  [   64/  126]
train() client id: f_00000-9-2 loss: 0.632815  [   96/  126]
train() client id: f_00000-10-0 loss: 0.720617  [   32/  126]
train() client id: f_00000-10-1 loss: 0.810420  [   64/  126]
train() client id: f_00000-10-2 loss: 0.822510  [   96/  126]
train() client id: f_00000-11-0 loss: 0.756609  [   32/  126]
train() client id: f_00000-11-1 loss: 0.721167  [   64/  126]
train() client id: f_00000-11-2 loss: 0.807485  [   96/  126]
train() client id: f_00001-0-0 loss: 0.386170  [   32/  265]
train() client id: f_00001-0-1 loss: 0.451380  [   64/  265]
train() client id: f_00001-0-2 loss: 0.665995  [   96/  265]
train() client id: f_00001-0-3 loss: 0.545608  [  128/  265]
train() client id: f_00001-0-4 loss: 0.424336  [  160/  265]
train() client id: f_00001-0-5 loss: 0.444087  [  192/  265]
train() client id: f_00001-0-6 loss: 0.455766  [  224/  265]
train() client id: f_00001-0-7 loss: 0.423581  [  256/  265]
train() client id: f_00001-1-0 loss: 0.467837  [   32/  265]
train() client id: f_00001-1-1 loss: 0.451870  [   64/  265]
train() client id: f_00001-1-2 loss: 0.465240  [   96/  265]
train() client id: f_00001-1-3 loss: 0.443267  [  128/  265]
train() client id: f_00001-1-4 loss: 0.418018  [  160/  265]
train() client id: f_00001-1-5 loss: 0.474367  [  192/  265]
train() client id: f_00001-1-6 loss: 0.463251  [  224/  265]
train() client id: f_00001-1-7 loss: 0.486584  [  256/  265]
train() client id: f_00001-2-0 loss: 0.407109  [   32/  265]
train() client id: f_00001-2-1 loss: 0.459002  [   64/  265]
train() client id: f_00001-2-2 loss: 0.573622  [   96/  265]
train() client id: f_00001-2-3 loss: 0.355400  [  128/  265]
train() client id: f_00001-2-4 loss: 0.532115  [  160/  265]
train() client id: f_00001-2-5 loss: 0.467144  [  192/  265]
train() client id: f_00001-2-6 loss: 0.438227  [  224/  265]
train() client id: f_00001-2-7 loss: 0.475649  [  256/  265]
train() client id: f_00001-3-0 loss: 0.430474  [   32/  265]
train() client id: f_00001-3-1 loss: 0.487821  [   64/  265]
train() client id: f_00001-3-2 loss: 0.391002  [   96/  265]
train() client id: f_00001-3-3 loss: 0.576859  [  128/  265]
train() client id: f_00001-3-4 loss: 0.454874  [  160/  265]
train() client id: f_00001-3-5 loss: 0.528268  [  192/  265]
train() client id: f_00001-3-6 loss: 0.357703  [  224/  265]
train() client id: f_00001-3-7 loss: 0.424752  [  256/  265]
train() client id: f_00001-4-0 loss: 0.357994  [   32/  265]
train() client id: f_00001-4-1 loss: 0.510504  [   64/  265]
train() client id: f_00001-4-2 loss: 0.406872  [   96/  265]
train() client id: f_00001-4-3 loss: 0.529535  [  128/  265]
train() client id: f_00001-4-4 loss: 0.591084  [  160/  265]
train() client id: f_00001-4-5 loss: 0.361323  [  192/  265]
train() client id: f_00001-4-6 loss: 0.459644  [  224/  265]
train() client id: f_00001-4-7 loss: 0.433308  [  256/  265]
train() client id: f_00001-5-0 loss: 0.386320  [   32/  265]
train() client id: f_00001-5-1 loss: 0.507467  [   64/  265]
train() client id: f_00001-5-2 loss: 0.499573  [   96/  265]
train() client id: f_00001-5-3 loss: 0.396464  [  128/  265]
train() client id: f_00001-5-4 loss: 0.415029  [  160/  265]
train() client id: f_00001-5-5 loss: 0.475866  [  192/  265]
train() client id: f_00001-5-6 loss: 0.430385  [  224/  265]
train() client id: f_00001-5-7 loss: 0.480237  [  256/  265]
train() client id: f_00001-6-0 loss: 0.445862  [   32/  265]
train() client id: f_00001-6-1 loss: 0.550154  [   64/  265]
train() client id: f_00001-6-2 loss: 0.453718  [   96/  265]
train() client id: f_00001-6-3 loss: 0.401659  [  128/  265]
train() client id: f_00001-6-4 loss: 0.477007  [  160/  265]
train() client id: f_00001-6-5 loss: 0.391553  [  192/  265]
train() client id: f_00001-6-6 loss: 0.433876  [  224/  265]
train() client id: f_00001-6-7 loss: 0.454088  [  256/  265]
train() client id: f_00001-7-0 loss: 0.526713  [   32/  265]
train() client id: f_00001-7-1 loss: 0.516449  [   64/  265]
train() client id: f_00001-7-2 loss: 0.434794  [   96/  265]
train() client id: f_00001-7-3 loss: 0.396979  [  128/  265]
train() client id: f_00001-7-4 loss: 0.412979  [  160/  265]
train() client id: f_00001-7-5 loss: 0.486525  [  192/  265]
train() client id: f_00001-7-6 loss: 0.415003  [  224/  265]
train() client id: f_00001-7-7 loss: 0.409916  [  256/  265]
train() client id: f_00001-8-0 loss: 0.349555  [   32/  265]
train() client id: f_00001-8-1 loss: 0.460659  [   64/  265]
train() client id: f_00001-8-2 loss: 0.504132  [   96/  265]
train() client id: f_00001-8-3 loss: 0.631807  [  128/  265]
train() client id: f_00001-8-4 loss: 0.444549  [  160/  265]
train() client id: f_00001-8-5 loss: 0.406169  [  192/  265]
train() client id: f_00001-8-6 loss: 0.464494  [  224/  265]
train() client id: f_00001-8-7 loss: 0.335162  [  256/  265]
train() client id: f_00001-9-0 loss: 0.463523  [   32/  265]
train() client id: f_00001-9-1 loss: 0.385859  [   64/  265]
train() client id: f_00001-9-2 loss: 0.349882  [   96/  265]
train() client id: f_00001-9-3 loss: 0.375927  [  128/  265]
train() client id: f_00001-9-4 loss: 0.450563  [  160/  265]
train() client id: f_00001-9-5 loss: 0.461916  [  192/  265]
train() client id: f_00001-9-6 loss: 0.427032  [  224/  265]
train() client id: f_00001-9-7 loss: 0.671158  [  256/  265]
train() client id: f_00001-10-0 loss: 0.419772  [   32/  265]
train() client id: f_00001-10-1 loss: 0.545304  [   64/  265]
train() client id: f_00001-10-2 loss: 0.384295  [   96/  265]
train() client id: f_00001-10-3 loss: 0.462140  [  128/  265]
train() client id: f_00001-10-4 loss: 0.359334  [  160/  265]
train() client id: f_00001-10-5 loss: 0.455415  [  192/  265]
train() client id: f_00001-10-6 loss: 0.352929  [  224/  265]
train() client id: f_00001-10-7 loss: 0.559172  [  256/  265]
train() client id: f_00001-11-0 loss: 0.341743  [   32/  265]
train() client id: f_00001-11-1 loss: 0.487218  [   64/  265]
train() client id: f_00001-11-2 loss: 0.509252  [   96/  265]
train() client id: f_00001-11-3 loss: 0.364416  [  128/  265]
train() client id: f_00001-11-4 loss: 0.539583  [  160/  265]
train() client id: f_00001-11-5 loss: 0.517115  [  192/  265]
train() client id: f_00001-11-6 loss: 0.464563  [  224/  265]
train() client id: f_00001-11-7 loss: 0.364103  [  256/  265]
train() client id: f_00002-0-0 loss: 1.184415  [   32/  124]
train() client id: f_00002-0-1 loss: 1.294127  [   64/  124]
train() client id: f_00002-0-2 loss: 1.097786  [   96/  124]
train() client id: f_00002-1-0 loss: 1.307196  [   32/  124]
train() client id: f_00002-1-1 loss: 1.053408  [   64/  124]
train() client id: f_00002-1-2 loss: 1.056532  [   96/  124]
train() client id: f_00002-2-0 loss: 1.178799  [   32/  124]
train() client id: f_00002-2-1 loss: 1.082460  [   64/  124]
train() client id: f_00002-2-2 loss: 1.057197  [   96/  124]
train() client id: f_00002-3-0 loss: 0.899736  [   32/  124]
train() client id: f_00002-3-1 loss: 1.168581  [   64/  124]
train() client id: f_00002-3-2 loss: 1.155057  [   96/  124]
train() client id: f_00002-4-0 loss: 1.130607  [   32/  124]
train() client id: f_00002-4-1 loss: 0.989654  [   64/  124]
train() client id: f_00002-4-2 loss: 0.931868  [   96/  124]
train() client id: f_00002-5-0 loss: 0.823830  [   32/  124]
train() client id: f_00002-5-1 loss: 0.996500  [   64/  124]
train() client id: f_00002-5-2 loss: 1.060009  [   96/  124]
train() client id: f_00002-6-0 loss: 1.012375  [   32/  124]
train() client id: f_00002-6-1 loss: 0.960925  [   64/  124]
train() client id: f_00002-6-2 loss: 1.024800  [   96/  124]
train() client id: f_00002-7-0 loss: 0.949202  [   32/  124]
train() client id: f_00002-7-1 loss: 1.041986  [   64/  124]
train() client id: f_00002-7-2 loss: 0.899181  [   96/  124]
train() client id: f_00002-8-0 loss: 0.732205  [   32/  124]
train() client id: f_00002-8-1 loss: 0.902671  [   64/  124]
train() client id: f_00002-8-2 loss: 1.245149  [   96/  124]
train() client id: f_00002-9-0 loss: 0.832781  [   32/  124]
train() client id: f_00002-9-1 loss: 0.834653  [   64/  124]
train() client id: f_00002-9-2 loss: 1.044375  [   96/  124]
train() client id: f_00002-10-0 loss: 0.766292  [   32/  124]
train() client id: f_00002-10-1 loss: 0.896731  [   64/  124]
train() client id: f_00002-10-2 loss: 1.055089  [   96/  124]
train() client id: f_00002-11-0 loss: 0.756313  [   32/  124]
train() client id: f_00002-11-1 loss: 0.870000  [   64/  124]
train() client id: f_00002-11-2 loss: 0.874619  [   96/  124]
train() client id: f_00003-0-0 loss: 0.484597  [   32/   43]
train() client id: f_00003-1-0 loss: 0.536216  [   32/   43]
train() client id: f_00003-2-0 loss: 0.487248  [   32/   43]
train() client id: f_00003-3-0 loss: 0.609830  [   32/   43]
train() client id: f_00003-4-0 loss: 0.605284  [   32/   43]
train() client id: f_00003-5-0 loss: 0.626750  [   32/   43]
train() client id: f_00003-6-0 loss: 0.599401  [   32/   43]
train() client id: f_00003-7-0 loss: 0.644971  [   32/   43]
train() client id: f_00003-8-0 loss: 0.713099  [   32/   43]
train() client id: f_00003-9-0 loss: 0.586228  [   32/   43]
train() client id: f_00003-10-0 loss: 0.615684  [   32/   43]
train() client id: f_00003-11-0 loss: 0.702282  [   32/   43]
train() client id: f_00004-0-0 loss: 0.943697  [   32/  306]
train() client id: f_00004-0-1 loss: 0.757895  [   64/  306]
train() client id: f_00004-0-2 loss: 0.792588  [   96/  306]
train() client id: f_00004-0-3 loss: 0.714591  [  128/  306]
train() client id: f_00004-0-4 loss: 0.908203  [  160/  306]
train() client id: f_00004-0-5 loss: 0.819403  [  192/  306]
train() client id: f_00004-0-6 loss: 0.846616  [  224/  306]
train() client id: f_00004-0-7 loss: 0.792779  [  256/  306]
train() client id: f_00004-0-8 loss: 0.829485  [  288/  306]
train() client id: f_00004-1-0 loss: 0.968124  [   32/  306]
train() client id: f_00004-1-1 loss: 0.833990  [   64/  306]
train() client id: f_00004-1-2 loss: 0.735923  [   96/  306]
train() client id: f_00004-1-3 loss: 0.806029  [  128/  306]
train() client id: f_00004-1-4 loss: 0.789804  [  160/  306]
train() client id: f_00004-1-5 loss: 0.964832  [  192/  306]
train() client id: f_00004-1-6 loss: 0.698894  [  224/  306]
train() client id: f_00004-1-7 loss: 0.816434  [  256/  306]
train() client id: f_00004-1-8 loss: 0.752145  [  288/  306]
train() client id: f_00004-2-0 loss: 0.829696  [   32/  306]
train() client id: f_00004-2-1 loss: 0.776785  [   64/  306]
train() client id: f_00004-2-2 loss: 0.847482  [   96/  306]
train() client id: f_00004-2-3 loss: 0.799389  [  128/  306]
train() client id: f_00004-2-4 loss: 0.832307  [  160/  306]
train() client id: f_00004-2-5 loss: 0.727248  [  192/  306]
train() client id: f_00004-2-6 loss: 0.781380  [  224/  306]
train() client id: f_00004-2-7 loss: 0.965224  [  256/  306]
train() client id: f_00004-2-8 loss: 0.799528  [  288/  306]
train() client id: f_00004-3-0 loss: 0.861137  [   32/  306]
train() client id: f_00004-3-1 loss: 0.928360  [   64/  306]
train() client id: f_00004-3-2 loss: 0.796657  [   96/  306]
train() client id: f_00004-3-3 loss: 0.875298  [  128/  306]
train() client id: f_00004-3-4 loss: 0.840031  [  160/  306]
train() client id: f_00004-3-5 loss: 0.755526  [  192/  306]
train() client id: f_00004-3-6 loss: 0.854559  [  224/  306]
train() client id: f_00004-3-7 loss: 0.820452  [  256/  306]
train() client id: f_00004-3-8 loss: 0.746218  [  288/  306]
train() client id: f_00004-4-0 loss: 0.818806  [   32/  306]
train() client id: f_00004-4-1 loss: 0.882631  [   64/  306]
train() client id: f_00004-4-2 loss: 0.820223  [   96/  306]
train() client id: f_00004-4-3 loss: 0.753474  [  128/  306]
train() client id: f_00004-4-4 loss: 0.844946  [  160/  306]
train() client id: f_00004-4-5 loss: 0.842386  [  192/  306]
train() client id: f_00004-4-6 loss: 0.761084  [  224/  306]
train() client id: f_00004-4-7 loss: 0.802607  [  256/  306]
train() client id: f_00004-4-8 loss: 0.913873  [  288/  306]
train() client id: f_00004-5-0 loss: 0.639615  [   32/  306]
train() client id: f_00004-5-1 loss: 0.745843  [   64/  306]
train() client id: f_00004-5-2 loss: 0.804995  [   96/  306]
train() client id: f_00004-5-3 loss: 0.769046  [  128/  306]
train() client id: f_00004-5-4 loss: 0.961340  [  160/  306]
train() client id: f_00004-5-5 loss: 0.842806  [  192/  306]
train() client id: f_00004-5-6 loss: 0.838950  [  224/  306]
train() client id: f_00004-5-7 loss: 1.025661  [  256/  306]
train() client id: f_00004-5-8 loss: 0.769634  [  288/  306]
train() client id: f_00004-6-0 loss: 0.844681  [   32/  306]
train() client id: f_00004-6-1 loss: 0.781835  [   64/  306]
train() client id: f_00004-6-2 loss: 0.760216  [   96/  306]
train() client id: f_00004-6-3 loss: 0.765384  [  128/  306]
train() client id: f_00004-6-4 loss: 0.907715  [  160/  306]
train() client id: f_00004-6-5 loss: 0.788447  [  192/  306]
train() client id: f_00004-6-6 loss: 0.740561  [  224/  306]
train() client id: f_00004-6-7 loss: 0.887397  [  256/  306]
train() client id: f_00004-6-8 loss: 0.890071  [  288/  306]
train() client id: f_00004-7-0 loss: 0.731025  [   32/  306]
train() client id: f_00004-7-1 loss: 0.885538  [   64/  306]
train() client id: f_00004-7-2 loss: 0.831595  [   96/  306]
train() client id: f_00004-7-3 loss: 0.835205  [  128/  306]
train() client id: f_00004-7-4 loss: 0.920993  [  160/  306]
train() client id: f_00004-7-5 loss: 0.773237  [  192/  306]
train() client id: f_00004-7-6 loss: 0.753245  [  224/  306]
train() client id: f_00004-7-7 loss: 0.852720  [  256/  306]
train() client id: f_00004-7-8 loss: 0.773588  [  288/  306]
train() client id: f_00004-8-0 loss: 0.709161  [   32/  306]
train() client id: f_00004-8-1 loss: 0.789009  [   64/  306]
train() client id: f_00004-8-2 loss: 0.909313  [   96/  306]
train() client id: f_00004-8-3 loss: 0.824146  [  128/  306]
train() client id: f_00004-8-4 loss: 0.818055  [  160/  306]
train() client id: f_00004-8-5 loss: 0.806848  [  192/  306]
train() client id: f_00004-8-6 loss: 0.791643  [  224/  306]
train() client id: f_00004-8-7 loss: 0.749511  [  256/  306]
train() client id: f_00004-8-8 loss: 0.954661  [  288/  306]
train() client id: f_00004-9-0 loss: 0.867713  [   32/  306]
train() client id: f_00004-9-1 loss: 0.873152  [   64/  306]
train() client id: f_00004-9-2 loss: 0.778336  [   96/  306]
train() client id: f_00004-9-3 loss: 0.789389  [  128/  306]
train() client id: f_00004-9-4 loss: 0.770924  [  160/  306]
train() client id: f_00004-9-5 loss: 0.671659  [  192/  306]
train() client id: f_00004-9-6 loss: 0.856653  [  224/  306]
train() client id: f_00004-9-7 loss: 0.842345  [  256/  306]
train() client id: f_00004-9-8 loss: 0.942404  [  288/  306]
train() client id: f_00004-10-0 loss: 0.821510  [   32/  306]
train() client id: f_00004-10-1 loss: 0.808414  [   64/  306]
train() client id: f_00004-10-2 loss: 0.701123  [   96/  306]
train() client id: f_00004-10-3 loss: 0.821868  [  128/  306]
train() client id: f_00004-10-4 loss: 0.958663  [  160/  306]
train() client id: f_00004-10-5 loss: 0.745677  [  192/  306]
train() client id: f_00004-10-6 loss: 0.789396  [  224/  306]
train() client id: f_00004-10-7 loss: 1.075303  [  256/  306]
train() client id: f_00004-10-8 loss: 0.734503  [  288/  306]
train() client id: f_00004-11-0 loss: 0.832789  [   32/  306]
train() client id: f_00004-11-1 loss: 0.883782  [   64/  306]
train() client id: f_00004-11-2 loss: 0.846466  [   96/  306]
train() client id: f_00004-11-3 loss: 0.731589  [  128/  306]
train() client id: f_00004-11-4 loss: 0.952913  [  160/  306]
train() client id: f_00004-11-5 loss: 0.813201  [  192/  306]
train() client id: f_00004-11-6 loss: 0.836981  [  224/  306]
train() client id: f_00004-11-7 loss: 0.773208  [  256/  306]
train() client id: f_00004-11-8 loss: 0.827037  [  288/  306]
train() client id: f_00005-0-0 loss: 0.871683  [   32/  146]
train() client id: f_00005-0-1 loss: 0.781261  [   64/  146]
train() client id: f_00005-0-2 loss: 0.755043  [   96/  146]
train() client id: f_00005-0-3 loss: 0.761188  [  128/  146]
train() client id: f_00005-1-0 loss: 0.733715  [   32/  146]
train() client id: f_00005-1-1 loss: 0.923052  [   64/  146]
train() client id: f_00005-1-2 loss: 0.905856  [   96/  146]
train() client id: f_00005-1-3 loss: 0.627033  [  128/  146]
train() client id: f_00005-2-0 loss: 0.803842  [   32/  146]
train() client id: f_00005-2-1 loss: 0.644016  [   64/  146]
train() client id: f_00005-2-2 loss: 0.686970  [   96/  146]
train() client id: f_00005-2-3 loss: 1.065374  [  128/  146]
train() client id: f_00005-3-0 loss: 0.540305  [   32/  146]
train() client id: f_00005-3-1 loss: 1.038429  [   64/  146]
train() client id: f_00005-3-2 loss: 0.785029  [   96/  146]
train() client id: f_00005-3-3 loss: 0.913774  [  128/  146]
train() client id: f_00005-4-0 loss: 0.853984  [   32/  146]
train() client id: f_00005-4-1 loss: 1.027469  [   64/  146]
train() client id: f_00005-4-2 loss: 0.439353  [   96/  146]
train() client id: f_00005-4-3 loss: 0.878008  [  128/  146]
train() client id: f_00005-5-0 loss: 0.923336  [   32/  146]
train() client id: f_00005-5-1 loss: 0.513748  [   64/  146]
train() client id: f_00005-5-2 loss: 0.951850  [   96/  146]
train() client id: f_00005-5-3 loss: 0.861915  [  128/  146]
train() client id: f_00005-6-0 loss: 0.679713  [   32/  146]
train() client id: f_00005-6-1 loss: 0.746431  [   64/  146]
train() client id: f_00005-6-2 loss: 0.853830  [   96/  146]
train() client id: f_00005-6-3 loss: 0.762609  [  128/  146]
train() client id: f_00005-7-0 loss: 0.912046  [   32/  146]
train() client id: f_00005-7-1 loss: 0.670671  [   64/  146]
train() client id: f_00005-7-2 loss: 0.726325  [   96/  146]
train() client id: f_00005-7-3 loss: 0.818494  [  128/  146]
train() client id: f_00005-8-0 loss: 0.953624  [   32/  146]
train() client id: f_00005-8-1 loss: 0.752759  [   64/  146]
train() client id: f_00005-8-2 loss: 0.769837  [   96/  146]
train() client id: f_00005-8-3 loss: 0.696863  [  128/  146]
train() client id: f_00005-9-0 loss: 0.908360  [   32/  146]
train() client id: f_00005-9-1 loss: 0.856343  [   64/  146]
train() client id: f_00005-9-2 loss: 0.849990  [   96/  146]
train() client id: f_00005-9-3 loss: 0.587112  [  128/  146]
train() client id: f_00005-10-0 loss: 0.908826  [   32/  146]
train() client id: f_00005-10-1 loss: 0.930230  [   64/  146]
train() client id: f_00005-10-2 loss: 0.644403  [   96/  146]
train() client id: f_00005-10-3 loss: 0.673160  [  128/  146]
train() client id: f_00005-11-0 loss: 0.891864  [   32/  146]
train() client id: f_00005-11-1 loss: 0.777694  [   64/  146]
train() client id: f_00005-11-2 loss: 0.782309  [   96/  146]
train() client id: f_00005-11-3 loss: 0.838989  [  128/  146]
train() client id: f_00006-0-0 loss: 0.552424  [   32/   54]
train() client id: f_00006-1-0 loss: 0.563622  [   32/   54]
train() client id: f_00006-2-0 loss: 0.490535  [   32/   54]
train() client id: f_00006-3-0 loss: 0.505301  [   32/   54]
train() client id: f_00006-4-0 loss: 0.515440  [   32/   54]
train() client id: f_00006-5-0 loss: 0.451281  [   32/   54]
train() client id: f_00006-6-0 loss: 0.471019  [   32/   54]
train() client id: f_00006-7-0 loss: 0.560149  [   32/   54]
train() client id: f_00006-8-0 loss: 0.517426  [   32/   54]
train() client id: f_00006-9-0 loss: 0.556979  [   32/   54]
train() client id: f_00006-10-0 loss: 0.516243  [   32/   54]
train() client id: f_00006-11-0 loss: 0.465739  [   32/   54]
train() client id: f_00007-0-0 loss: 0.642447  [   32/  179]
train() client id: f_00007-0-1 loss: 0.648289  [   64/  179]
train() client id: f_00007-0-2 loss: 0.615076  [   96/  179]
train() client id: f_00007-0-3 loss: 0.744876  [  128/  179]
train() client id: f_00007-0-4 loss: 0.644681  [  160/  179]
train() client id: f_00007-1-0 loss: 0.641355  [   32/  179]
train() client id: f_00007-1-1 loss: 0.477784  [   64/  179]
train() client id: f_00007-1-2 loss: 0.579276  [   96/  179]
train() client id: f_00007-1-3 loss: 0.561175  [  128/  179]
train() client id: f_00007-1-4 loss: 0.794071  [  160/  179]
train() client id: f_00007-2-0 loss: 0.602180  [   32/  179]
train() client id: f_00007-2-1 loss: 0.624247  [   64/  179]
train() client id: f_00007-2-2 loss: 0.787331  [   96/  179]
train() client id: f_00007-2-3 loss: 0.483836  [  128/  179]
train() client id: f_00007-2-4 loss: 0.545785  [  160/  179]
train() client id: f_00007-3-0 loss: 0.586329  [   32/  179]
train() client id: f_00007-3-1 loss: 0.494307  [   64/  179]
train() client id: f_00007-3-2 loss: 0.721840  [   96/  179]
train() client id: f_00007-3-3 loss: 0.842700  [  128/  179]
train() client id: f_00007-3-4 loss: 0.472821  [  160/  179]
train() client id: f_00007-4-0 loss: 0.498923  [   32/  179]
train() client id: f_00007-4-1 loss: 0.652550  [   64/  179]
train() client id: f_00007-4-2 loss: 0.561764  [   96/  179]
train() client id: f_00007-4-3 loss: 0.568227  [  128/  179]
train() client id: f_00007-4-4 loss: 0.692293  [  160/  179]
train() client id: f_00007-5-0 loss: 0.530246  [   32/  179]
train() client id: f_00007-5-1 loss: 0.977708  [   64/  179]
train() client id: f_00007-5-2 loss: 0.671260  [   96/  179]
train() client id: f_00007-5-3 loss: 0.482305  [  128/  179]
train() client id: f_00007-5-4 loss: 0.536697  [  160/  179]
train() client id: f_00007-6-0 loss: 0.658748  [   32/  179]
train() client id: f_00007-6-1 loss: 0.549978  [   64/  179]
train() client id: f_00007-6-2 loss: 0.575940  [   96/  179]
train() client id: f_00007-6-3 loss: 0.466708  [  128/  179]
train() client id: f_00007-6-4 loss: 0.628489  [  160/  179]
train() client id: f_00007-7-0 loss: 0.581850  [   32/  179]
train() client id: f_00007-7-1 loss: 0.852332  [   64/  179]
train() client id: f_00007-7-2 loss: 0.521392  [   96/  179]
train() client id: f_00007-7-3 loss: 0.459919  [  128/  179]
train() client id: f_00007-7-4 loss: 0.662333  [  160/  179]
train() client id: f_00007-8-0 loss: 0.653138  [   32/  179]
train() client id: f_00007-8-1 loss: 0.753858  [   64/  179]
train() client id: f_00007-8-2 loss: 0.634297  [   96/  179]
train() client id: f_00007-8-3 loss: 0.566196  [  128/  179]
train() client id: f_00007-8-4 loss: 0.440574  [  160/  179]
train() client id: f_00007-9-0 loss: 0.769961  [   32/  179]
train() client id: f_00007-9-1 loss: 0.643115  [   64/  179]
train() client id: f_00007-9-2 loss: 0.796715  [   96/  179]
train() client id: f_00007-9-3 loss: 0.456618  [  128/  179]
train() client id: f_00007-9-4 loss: 0.465142  [  160/  179]
train() client id: f_00007-10-0 loss: 0.640465  [   32/  179]
train() client id: f_00007-10-1 loss: 0.608293  [   64/  179]
train() client id: f_00007-10-2 loss: 0.673367  [   96/  179]
train() client id: f_00007-10-3 loss: 0.688146  [  128/  179]
train() client id: f_00007-10-4 loss: 0.459310  [  160/  179]
train() client id: f_00007-11-0 loss: 0.622144  [   32/  179]
train() client id: f_00007-11-1 loss: 0.682863  [   64/  179]
train() client id: f_00007-11-2 loss: 0.592558  [   96/  179]
train() client id: f_00007-11-3 loss: 0.582900  [  128/  179]
train() client id: f_00007-11-4 loss: 0.543425  [  160/  179]
train() client id: f_00008-0-0 loss: 0.725752  [   32/  130]
train() client id: f_00008-0-1 loss: 0.794165  [   64/  130]
train() client id: f_00008-0-2 loss: 0.755346  [   96/  130]
train() client id: f_00008-0-3 loss: 0.776380  [  128/  130]
train() client id: f_00008-1-0 loss: 0.791957  [   32/  130]
train() client id: f_00008-1-1 loss: 0.821206  [   64/  130]
train() client id: f_00008-1-2 loss: 0.677728  [   96/  130]
train() client id: f_00008-1-3 loss: 0.762396  [  128/  130]
train() client id: f_00008-2-0 loss: 0.724971  [   32/  130]
train() client id: f_00008-2-1 loss: 0.805630  [   64/  130]
train() client id: f_00008-2-2 loss: 0.745541  [   96/  130]
train() client id: f_00008-2-3 loss: 0.747361  [  128/  130]
train() client id: f_00008-3-0 loss: 0.733239  [   32/  130]
train() client id: f_00008-3-1 loss: 0.800545  [   64/  130]
train() client id: f_00008-3-2 loss: 0.710269  [   96/  130]
train() client id: f_00008-3-3 loss: 0.753828  [  128/  130]
train() client id: f_00008-4-0 loss: 0.745886  [   32/  130]
train() client id: f_00008-4-1 loss: 0.739196  [   64/  130]
train() client id: f_00008-4-2 loss: 0.836045  [   96/  130]
train() client id: f_00008-4-3 loss: 0.730001  [  128/  130]
train() client id: f_00008-5-0 loss: 0.786759  [   32/  130]
train() client id: f_00008-5-1 loss: 0.808151  [   64/  130]
train() client id: f_00008-5-2 loss: 0.768713  [   96/  130]
train() client id: f_00008-5-3 loss: 0.674787  [  128/  130]
train() client id: f_00008-6-0 loss: 0.722682  [   32/  130]
train() client id: f_00008-6-1 loss: 0.774186  [   64/  130]
train() client id: f_00008-6-2 loss: 0.796639  [   96/  130]
train() client id: f_00008-6-3 loss: 0.745177  [  128/  130]
train() client id: f_00008-7-0 loss: 0.804251  [   32/  130]
train() client id: f_00008-7-1 loss: 0.715887  [   64/  130]
train() client id: f_00008-7-2 loss: 0.816574  [   96/  130]
train() client id: f_00008-7-3 loss: 0.712289  [  128/  130]
train() client id: f_00008-8-0 loss: 0.702413  [   32/  130]
train() client id: f_00008-8-1 loss: 0.804161  [   64/  130]
train() client id: f_00008-8-2 loss: 0.732174  [   96/  130]
train() client id: f_00008-8-3 loss: 0.816829  [  128/  130]
train() client id: f_00008-9-0 loss: 0.827515  [   32/  130]
train() client id: f_00008-9-1 loss: 0.617544  [   64/  130]
train() client id: f_00008-9-2 loss: 0.758145  [   96/  130]
train() client id: f_00008-9-3 loss: 0.818648  [  128/  130]
train() client id: f_00008-10-0 loss: 0.793733  [   32/  130]
train() client id: f_00008-10-1 loss: 0.825626  [   64/  130]
train() client id: f_00008-10-2 loss: 0.666367  [   96/  130]
train() client id: f_00008-10-3 loss: 0.755479  [  128/  130]
train() client id: f_00008-11-0 loss: 0.651434  [   32/  130]
train() client id: f_00008-11-1 loss: 0.748668  [   64/  130]
train() client id: f_00008-11-2 loss: 0.811924  [   96/  130]
train() client id: f_00008-11-3 loss: 0.804594  [  128/  130]
train() client id: f_00009-0-0 loss: 1.278239  [   32/  118]
train() client id: f_00009-0-1 loss: 0.824064  [   64/  118]
train() client id: f_00009-0-2 loss: 0.925041  [   96/  118]
train() client id: f_00009-1-0 loss: 0.915092  [   32/  118]
train() client id: f_00009-1-1 loss: 1.063478  [   64/  118]
train() client id: f_00009-1-2 loss: 0.811767  [   96/  118]
train() client id: f_00009-2-0 loss: 1.011291  [   32/  118]
train() client id: f_00009-2-1 loss: 0.835154  [   64/  118]
train() client id: f_00009-2-2 loss: 0.881275  [   96/  118]
train() client id: f_00009-3-0 loss: 0.994895  [   32/  118]
train() client id: f_00009-3-1 loss: 0.895744  [   64/  118]
train() client id: f_00009-3-2 loss: 0.848118  [   96/  118]
train() client id: f_00009-4-0 loss: 0.806909  [   32/  118]
train() client id: f_00009-4-1 loss: 0.890651  [   64/  118]
train() client id: f_00009-4-2 loss: 0.818014  [   96/  118]
train() client id: f_00009-5-0 loss: 0.732269  [   32/  118]
train() client id: f_00009-5-1 loss: 0.931930  [   64/  118]
train() client id: f_00009-5-2 loss: 0.868130  [   96/  118]
train() client id: f_00009-6-0 loss: 0.788653  [   32/  118]
train() client id: f_00009-6-1 loss: 0.854008  [   64/  118]
train() client id: f_00009-6-2 loss: 0.728385  [   96/  118]
train() client id: f_00009-7-0 loss: 0.936595  [   32/  118]
train() client id: f_00009-7-1 loss: 0.684545  [   64/  118]
train() client id: f_00009-7-2 loss: 0.817095  [   96/  118]
train() client id: f_00009-8-0 loss: 0.672795  [   32/  118]
train() client id: f_00009-8-1 loss: 0.754540  [   64/  118]
train() client id: f_00009-8-2 loss: 0.885471  [   96/  118]
train() client id: f_00009-9-0 loss: 0.879043  [   32/  118]
train() client id: f_00009-9-1 loss: 0.826024  [   64/  118]
train() client id: f_00009-9-2 loss: 0.723478  [   96/  118]
train() client id: f_00009-10-0 loss: 0.867428  [   32/  118]
train() client id: f_00009-10-1 loss: 0.586745  [   64/  118]
train() client id: f_00009-10-2 loss: 0.824303  [   96/  118]
train() client id: f_00009-11-0 loss: 0.769160  [   32/  118]
train() client id: f_00009-11-1 loss: 0.585402  [   64/  118]
train() client id: f_00009-11-2 loss: 0.878634  [   96/  118]
At round 26 accuracy: 0.6392572944297082
At round 26 training accuracy: 0.5888665325285044
At round 26 training loss: 0.8127626570030337
update_location
xs = -3.905658 4.200318 150.009024 18.811294 0.979296 3.956410 -112.443192 -91.324852 134.663977 -77.060879 
ys = 142.587959 125.555839 1.320614 -112.455176 104.350187 87.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 2.7885738318825704
ys mean: 36.89414253552871
dists_uav = 174.202699 160.567467 180.289909 151.657612 144.533458 133.142699 150.500372 135.428597 168.650694 126.310692 
uav_gains = -106.057157 -105.152486 -106.446062 -104.526866 -104.001987 -103.108845 -104.443195 -103.293910 -105.695296 -102.536442 
uav_gains_db_mean: -104.52622448514772
dists_bs = 174.137343 185.896420 368.504267 346.689048 189.631508 199.064736 188.318850 193.234016 347.384940 197.059800 
bs_gains = -102.312033 -103.106648 -111.427450 -110.685383 -103.348553 -103.938900 -103.264085 -103.577399 -110.709767 -103.815804 
bs_gains_db_mean: -105.61860221374613
Round 27
-------------------------------
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.428454   15.42356035  7.31658542  2.62856167 17.79004913  8.56531288
  3.26241587 10.46431357  7.71314909  6.94940181]
obj_prev = 87.54180377366474
eta_min = 3.2918080146347544e-13	eta_max = 0.9246953843511696
af = 18.488129690050464	bf = 1.525402092626451	zeta = 20.336942659055513	eta = 0.909090909090909
af = 18.488129690050464	bf = 1.525402092626451	zeta = 36.16259963393917	eta = 0.5112500173438599
af = 18.488129690050464	bf = 1.525402092626451	zeta = 28.49928344660974	eta = 0.6487226152435002
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.11857262246232	eta = 0.6817515784269833
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.048110971781785	eta = 0.6835275745998821
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.047913433089164	eta = 0.6835325665983885
eta = 0.6835325665983885
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [0.03139638 0.06603207 0.03089802 0.01071464 0.07624835 0.03637993
 0.01345559 0.04460279 0.03239309 0.02940296]
ene_total = [2.38132415 4.38338617 2.36243079 1.10502586 4.99963694 2.62710832
 1.26705354 3.1051769  2.60980459 2.20696617]
ti_comp = [0.39480521 0.40653023 0.39297956 0.40131289 0.40569068 0.40355948
 0.40164054 0.40588199 0.36658327 0.40401382]
ti_coms = [0.08405931 0.07233429 0.08588495 0.07755163 0.07317384 0.07530504
 0.07722398 0.07298253 0.11228124 0.07485069]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.24094611e-05 1.08882722e-04 1.19380162e-05 4.77360184e-07
 1.68337651e-04 1.84778100e-05 9.43872309e-07 3.36640665e-05
 1.58085418e-05 9.73331126e-06]
ene_total = [0.50365913 0.43928163 0.51455356 0.4640106  0.44786169 0.45164645
 0.46207824 0.43865975 0.67271096 0.448405  ]
optimize_network iter = 0 obj = 4.842867033053818
eta = 0.6835325665983885
freqs = [39761858.95611374 81214212.7965591  39312506.23491908 13349480.34302717
 93973501.61782414 45073814.47997239 16750787.69960297 54945517.04012107
 44182442.51361968 36388553.66776452]
eta_min = 0.6835325665983994	eta_max = 0.6835325665983844
af = 0.018943411267469068	bf = 1.525402092626451	zeta = 0.020837752394215977	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [3.10856006e-06 2.72750345e-05 2.99046350e-06 1.19578344e-07
 4.21684466e-05 4.62867660e-06 2.36439257e-07 8.43282168e-06
 3.96002704e-06 2.43818667e-06]
ene_total = [1.75137172 1.512205   1.78937036 1.61521218 1.53279247 1.56936094
 1.60841252 1.52178181 2.33933449 1.55944202]
ti_comp = [0.39480521 0.40653023 0.39297956 0.40131289 0.40569068 0.40355948
 0.40164054 0.40588199 0.36658327 0.40401382]
ti_coms = [0.08405931 0.07233429 0.08588495 0.07755163 0.07317384 0.07530504
 0.07722398 0.07298253 0.11228124 0.07485069]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.24094611e-05 1.08882722e-04 1.19380162e-05 4.77360184e-07
 1.68337651e-04 1.84778100e-05 9.43872309e-07 3.36640665e-05
 1.58085418e-05 9.73331126e-06]
ene_total = [0.50365913 0.43928163 0.51455356 0.4640106  0.44786169 0.45164645
 0.46207824 0.43865975 0.67271096 0.448405  ]
optimize_network iter = 1 obj = 4.842867033053756
eta = 0.6835325665983844
freqs = [39761858.95611373 81214212.79655911 39312506.23491906 13349480.34302717
 93973501.61782415 45073814.4799724  16750787.69960297 54945517.04012109
 44182442.51361962 36388553.66776453]
Done!
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.19524010e-05 1.04872399e-04 1.14983202e-05 4.59778254e-07
 1.62137509e-04 1.77972431e-05 9.09107999e-07 3.24241657e-05
 1.52262882e-05 9.37481801e-06]
ene_total = [0.00841788 0.0073383  0.00859999 0.00775562 0.00747952 0.0075483
 0.00772331 0.00733068 0.01124335 0.00749444]
At round 27 energy consumption: 0.08093140167794016
At round 27 eta: 0.6835325665983844
At round 27 a_n: 18.93386498808281
At round 27 local rounds: 12.458880266672946
At round 27 global rounds: 59.82879433933611
gradient difference: 0.4734843373298645
train() client id: f_00000-0-0 loss: 0.940920  [   32/  126]
train() client id: f_00000-0-1 loss: 0.869096  [   64/  126]
train() client id: f_00000-0-2 loss: 1.089715  [   96/  126]
train() client id: f_00000-1-0 loss: 1.177521  [   32/  126]
train() client id: f_00000-1-1 loss: 0.941337  [   64/  126]
train() client id: f_00000-1-2 loss: 0.727787  [   96/  126]
train() client id: f_00000-2-0 loss: 1.020462  [   32/  126]
train() client id: f_00000-2-1 loss: 0.825228  [   64/  126]
train() client id: f_00000-2-2 loss: 0.892971  [   96/  126]
train() client id: f_00000-3-0 loss: 0.765985  [   32/  126]
train() client id: f_00000-3-1 loss: 0.863490  [   64/  126]
train() client id: f_00000-3-2 loss: 0.899284  [   96/  126]
train() client id: f_00000-4-0 loss: 0.838541  [   32/  126]
train() client id: f_00000-4-1 loss: 0.769831  [   64/  126]
train() client id: f_00000-4-2 loss: 0.869934  [   96/  126]
train() client id: f_00000-5-0 loss: 0.780250  [   32/  126]
train() client id: f_00000-5-1 loss: 0.950012  [   64/  126]
train() client id: f_00000-5-2 loss: 0.771995  [   96/  126]
train() client id: f_00000-6-0 loss: 0.826445  [   32/  126]
train() client id: f_00000-6-1 loss: 0.765510  [   64/  126]
train() client id: f_00000-6-2 loss: 0.903762  [   96/  126]
train() client id: f_00000-7-0 loss: 0.737082  [   32/  126]
train() client id: f_00000-7-1 loss: 0.829705  [   64/  126]
train() client id: f_00000-7-2 loss: 0.888948  [   96/  126]
train() client id: f_00000-8-0 loss: 0.768949  [   32/  126]
train() client id: f_00000-8-1 loss: 0.875371  [   64/  126]
train() client id: f_00000-8-2 loss: 0.818031  [   96/  126]
train() client id: f_00000-9-0 loss: 0.783994  [   32/  126]
train() client id: f_00000-9-1 loss: 0.898902  [   64/  126]
train() client id: f_00000-9-2 loss: 0.896949  [   96/  126]
train() client id: f_00000-10-0 loss: 0.662462  [   32/  126]
train() client id: f_00000-10-1 loss: 0.862618  [   64/  126]
train() client id: f_00000-10-2 loss: 0.849591  [   96/  126]
train() client id: f_00000-11-0 loss: 0.837291  [   32/  126]
train() client id: f_00000-11-1 loss: 0.821798  [   64/  126]
train() client id: f_00000-11-2 loss: 0.926048  [   96/  126]
train() client id: f_00001-0-0 loss: 0.531354  [   32/  265]
train() client id: f_00001-0-1 loss: 0.526282  [   64/  265]
train() client id: f_00001-0-2 loss: 0.536216  [   96/  265]
train() client id: f_00001-0-3 loss: 0.433534  [  128/  265]
train() client id: f_00001-0-4 loss: 0.593814  [  160/  265]
train() client id: f_00001-0-5 loss: 0.515459  [  192/  265]
train() client id: f_00001-0-6 loss: 0.550426  [  224/  265]
train() client id: f_00001-0-7 loss: 0.538618  [  256/  265]
train() client id: f_00001-1-0 loss: 0.536284  [   32/  265]
train() client id: f_00001-1-1 loss: 0.457684  [   64/  265]
train() client id: f_00001-1-2 loss: 0.498178  [   96/  265]
train() client id: f_00001-1-3 loss: 0.602401  [  128/  265]
train() client id: f_00001-1-4 loss: 0.622037  [  160/  265]
train() client id: f_00001-1-5 loss: 0.428399  [  192/  265]
train() client id: f_00001-1-6 loss: 0.486037  [  224/  265]
train() client id: f_00001-1-7 loss: 0.566765  [  256/  265]
train() client id: f_00001-2-0 loss: 0.494661  [   32/  265]
train() client id: f_00001-2-1 loss: 0.525499  [   64/  265]
train() client id: f_00001-2-2 loss: 0.574414  [   96/  265]
train() client id: f_00001-2-3 loss: 0.470487  [  128/  265]
train() client id: f_00001-2-4 loss: 0.571346  [  160/  265]
train() client id: f_00001-2-5 loss: 0.445373  [  192/  265]
train() client id: f_00001-2-6 loss: 0.566579  [  224/  265]
train() client id: f_00001-2-7 loss: 0.504828  [  256/  265]
train() client id: f_00001-3-0 loss: 0.498210  [   32/  265]
train() client id: f_00001-3-1 loss: 0.558088  [   64/  265]
train() client id: f_00001-3-2 loss: 0.518789  [   96/  265]
train() client id: f_00001-3-3 loss: 0.542209  [  128/  265]
train() client id: f_00001-3-4 loss: 0.420675  [  160/  265]
train() client id: f_00001-3-5 loss: 0.517816  [  192/  265]
train() client id: f_00001-3-6 loss: 0.605971  [  224/  265]
train() client id: f_00001-3-7 loss: 0.461565  [  256/  265]
train() client id: f_00001-4-0 loss: 0.509905  [   32/  265]
train() client id: f_00001-4-1 loss: 0.506754  [   64/  265]
train() client id: f_00001-4-2 loss: 0.426343  [   96/  265]
train() client id: f_00001-4-3 loss: 0.427416  [  128/  265]
train() client id: f_00001-4-4 loss: 0.504852  [  160/  265]
train() client id: f_00001-4-5 loss: 0.607650  [  192/  265]
train() client id: f_00001-4-6 loss: 0.552274  [  224/  265]
train() client id: f_00001-4-7 loss: 0.494339  [  256/  265]
train() client id: f_00001-5-0 loss: 0.464578  [   32/  265]
train() client id: f_00001-5-1 loss: 0.511920  [   64/  265]
train() client id: f_00001-5-2 loss: 0.520013  [   96/  265]
train() client id: f_00001-5-3 loss: 0.554763  [  128/  265]
train() client id: f_00001-5-4 loss: 0.452111  [  160/  265]
train() client id: f_00001-5-5 loss: 0.468068  [  192/  265]
train() client id: f_00001-5-6 loss: 0.548277  [  224/  265]
train() client id: f_00001-5-7 loss: 0.511779  [  256/  265]
train() client id: f_00001-6-0 loss: 0.496627  [   32/  265]
train() client id: f_00001-6-1 loss: 0.498747  [   64/  265]
train() client id: f_00001-6-2 loss: 0.492227  [   96/  265]
train() client id: f_00001-6-3 loss: 0.422584  [  128/  265]
train() client id: f_00001-6-4 loss: 0.473088  [  160/  265]
train() client id: f_00001-6-5 loss: 0.600406  [  192/  265]
train() client id: f_00001-6-6 loss: 0.527302  [  224/  265]
train() client id: f_00001-6-7 loss: 0.567437  [  256/  265]
train() client id: f_00001-7-0 loss: 0.467121  [   32/  265]
train() client id: f_00001-7-1 loss: 0.540293  [   64/  265]
train() client id: f_00001-7-2 loss: 0.513064  [   96/  265]
train() client id: f_00001-7-3 loss: 0.445440  [  128/  265]
train() client id: f_00001-7-4 loss: 0.519716  [  160/  265]
train() client id: f_00001-7-5 loss: 0.496276  [  192/  265]
train() client id: f_00001-7-6 loss: 0.554826  [  224/  265]
train() client id: f_00001-7-7 loss: 0.465424  [  256/  265]
train() client id: f_00001-8-0 loss: 0.633652  [   32/  265]
train() client id: f_00001-8-1 loss: 0.398324  [   64/  265]
train() client id: f_00001-8-2 loss: 0.468188  [   96/  265]
train() client id: f_00001-8-3 loss: 0.457827  [  128/  265]
train() client id: f_00001-8-4 loss: 0.457458  [  160/  265]
train() client id: f_00001-8-5 loss: 0.525103  [  192/  265]
train() client id: f_00001-8-6 loss: 0.595600  [  224/  265]
train() client id: f_00001-8-7 loss: 0.445356  [  256/  265]
train() client id: f_00001-9-0 loss: 0.465138  [   32/  265]
train() client id: f_00001-9-1 loss: 0.431801  [   64/  265]
train() client id: f_00001-9-2 loss: 0.414854  [   96/  265]
train() client id: f_00001-9-3 loss: 0.560592  [  128/  265]
train() client id: f_00001-9-4 loss: 0.618888  [  160/  265]
train() client id: f_00001-9-5 loss: 0.407754  [  192/  265]
train() client id: f_00001-9-6 loss: 0.617200  [  224/  265]
train() client id: f_00001-9-7 loss: 0.557773  [  256/  265]
train() client id: f_00001-10-0 loss: 0.582589  [   32/  265]
train() client id: f_00001-10-1 loss: 0.473175  [   64/  265]
train() client id: f_00001-10-2 loss: 0.465764  [   96/  265]
train() client id: f_00001-10-3 loss: 0.413995  [  128/  265]
train() client id: f_00001-10-4 loss: 0.413178  [  160/  265]
train() client id: f_00001-10-5 loss: 0.415188  [  192/  265]
train() client id: f_00001-10-6 loss: 0.619522  [  224/  265]
train() client id: f_00001-10-7 loss: 0.689509  [  256/  265]
train() client id: f_00001-11-0 loss: 0.574744  [   32/  265]
train() client id: f_00001-11-1 loss: 0.564153  [   64/  265]
train() client id: f_00001-11-2 loss: 0.456935  [   96/  265]
train() client id: f_00001-11-3 loss: 0.498233  [  128/  265]
train() client id: f_00001-11-4 loss: 0.422047  [  160/  265]
train() client id: f_00001-11-5 loss: 0.444775  [  192/  265]
train() client id: f_00001-11-6 loss: 0.571134  [  224/  265]
train() client id: f_00001-11-7 loss: 0.534404  [  256/  265]
train() client id: f_00002-0-0 loss: 1.094854  [   32/  124]
train() client id: f_00002-0-1 loss: 1.062296  [   64/  124]
train() client id: f_00002-0-2 loss: 1.466658  [   96/  124]
train() client id: f_00002-1-0 loss: 1.329162  [   32/  124]
train() client id: f_00002-1-1 loss: 1.014786  [   64/  124]
train() client id: f_00002-1-2 loss: 1.217496  [   96/  124]
train() client id: f_00002-2-0 loss: 1.058042  [   32/  124]
train() client id: f_00002-2-1 loss: 1.104878  [   64/  124]
train() client id: f_00002-2-2 loss: 1.130574  [   96/  124]
train() client id: f_00002-3-0 loss: 1.066179  [   32/  124]
train() client id: f_00002-3-1 loss: 0.974258  [   64/  124]
train() client id: f_00002-3-2 loss: 1.162195  [   96/  124]
train() client id: f_00002-4-0 loss: 1.084865  [   32/  124]
train() client id: f_00002-4-1 loss: 1.151508  [   64/  124]
train() client id: f_00002-4-2 loss: 1.090523  [   96/  124]
train() client id: f_00002-5-0 loss: 1.003247  [   32/  124]
train() client id: f_00002-5-1 loss: 0.977811  [   64/  124]
train() client id: f_00002-5-2 loss: 1.214914  [   96/  124]
train() client id: f_00002-6-0 loss: 0.866829  [   32/  124]
train() client id: f_00002-6-1 loss: 1.203418  [   64/  124]
train() client id: f_00002-6-2 loss: 0.969405  [   96/  124]
train() client id: f_00002-7-0 loss: 0.938285  [   32/  124]
train() client id: f_00002-7-1 loss: 0.897443  [   64/  124]
train() client id: f_00002-7-2 loss: 1.047017  [   96/  124]
train() client id: f_00002-8-0 loss: 1.088075  [   32/  124]
train() client id: f_00002-8-1 loss: 1.048709  [   64/  124]
train() client id: f_00002-8-2 loss: 0.914361  [   96/  124]
train() client id: f_00002-9-0 loss: 1.129443  [   32/  124]
train() client id: f_00002-9-1 loss: 1.016122  [   64/  124]
train() client id: f_00002-9-2 loss: 0.804820  [   96/  124]
train() client id: f_00002-10-0 loss: 1.038379  [   32/  124]
train() client id: f_00002-10-1 loss: 1.046989  [   64/  124]
train() client id: f_00002-10-2 loss: 0.747338  [   96/  124]
train() client id: f_00002-11-0 loss: 0.909418  [   32/  124]
train() client id: f_00002-11-1 loss: 0.948827  [   64/  124]
train() client id: f_00002-11-2 loss: 1.064521  [   96/  124]
train() client id: f_00003-0-0 loss: 0.752504  [   32/   43]
train() client id: f_00003-1-0 loss: 0.656725  [   32/   43]
train() client id: f_00003-2-0 loss: 0.683652  [   32/   43]
train() client id: f_00003-3-0 loss: 0.611856  [   32/   43]
train() client id: f_00003-4-0 loss: 0.548814  [   32/   43]
train() client id: f_00003-5-0 loss: 0.524121  [   32/   43]
train() client id: f_00003-6-0 loss: 0.529832  [   32/   43]
train() client id: f_00003-7-0 loss: 0.604907  [   32/   43]
train() client id: f_00003-8-0 loss: 0.588858  [   32/   43]
train() client id: f_00003-9-0 loss: 0.739349  [   32/   43]
train() client id: f_00003-10-0 loss: 0.750923  [   32/   43]
train() client id: f_00003-11-0 loss: 0.722352  [   32/   43]
train() client id: f_00004-0-0 loss: 0.828373  [   32/  306]
train() client id: f_00004-0-1 loss: 0.966633  [   64/  306]
train() client id: f_00004-0-2 loss: 0.773123  [   96/  306]
train() client id: f_00004-0-3 loss: 0.887685  [  128/  306]
train() client id: f_00004-0-4 loss: 0.794975  [  160/  306]
train() client id: f_00004-0-5 loss: 0.909796  [  192/  306]
train() client id: f_00004-0-6 loss: 0.682397  [  224/  306]
train() client id: f_00004-0-7 loss: 0.891778  [  256/  306]
train() client id: f_00004-0-8 loss: 0.846941  [  288/  306]
train() client id: f_00004-1-0 loss: 0.977715  [   32/  306]
train() client id: f_00004-1-1 loss: 0.896058  [   64/  306]
train() client id: f_00004-1-2 loss: 0.821554  [   96/  306]
train() client id: f_00004-1-3 loss: 0.758196  [  128/  306]
train() client id: f_00004-1-4 loss: 0.853871  [  160/  306]
train() client id: f_00004-1-5 loss: 0.938911  [  192/  306]
train() client id: f_00004-1-6 loss: 0.794836  [  224/  306]
train() client id: f_00004-1-7 loss: 0.715583  [  256/  306]
train() client id: f_00004-1-8 loss: 0.865887  [  288/  306]
train() client id: f_00004-2-0 loss: 0.725891  [   32/  306]
train() client id: f_00004-2-1 loss: 0.861014  [   64/  306]
train() client id: f_00004-2-2 loss: 0.823994  [   96/  306]
train() client id: f_00004-2-3 loss: 0.995116  [  128/  306]
train() client id: f_00004-2-4 loss: 0.861774  [  160/  306]
train() client id: f_00004-2-5 loss: 0.849159  [  192/  306]
train() client id: f_00004-2-6 loss: 0.723763  [  224/  306]
train() client id: f_00004-2-7 loss: 0.835971  [  256/  306]
train() client id: f_00004-2-8 loss: 0.942113  [  288/  306]
train() client id: f_00004-3-0 loss: 0.893306  [   32/  306]
train() client id: f_00004-3-1 loss: 0.801939  [   64/  306]
train() client id: f_00004-3-2 loss: 0.796394  [   96/  306]
train() client id: f_00004-3-3 loss: 0.747371  [  128/  306]
train() client id: f_00004-3-4 loss: 0.883934  [  160/  306]
train() client id: f_00004-3-5 loss: 0.757724  [  192/  306]
train() client id: f_00004-3-6 loss: 0.943681  [  224/  306]
train() client id: f_00004-3-7 loss: 0.871744  [  256/  306]
train() client id: f_00004-3-8 loss: 0.857855  [  288/  306]
train() client id: f_00004-4-0 loss: 0.820161  [   32/  306]
train() client id: f_00004-4-1 loss: 0.772883  [   64/  306]
train() client id: f_00004-4-2 loss: 0.826508  [   96/  306]
train() client id: f_00004-4-3 loss: 0.979977  [  128/  306]
train() client id: f_00004-4-4 loss: 0.751943  [  160/  306]
train() client id: f_00004-4-5 loss: 0.901539  [  192/  306]
train() client id: f_00004-4-6 loss: 0.945210  [  224/  306]
train() client id: f_00004-4-7 loss: 0.750772  [  256/  306]
train() client id: f_00004-4-8 loss: 0.837974  [  288/  306]
train() client id: f_00004-5-0 loss: 1.020281  [   32/  306]
train() client id: f_00004-5-1 loss: 0.834562  [   64/  306]
train() client id: f_00004-5-2 loss: 0.851068  [   96/  306]
train() client id: f_00004-5-3 loss: 0.748439  [  128/  306]
train() client id: f_00004-5-4 loss: 0.838610  [  160/  306]
train() client id: f_00004-5-5 loss: 0.754683  [  192/  306]
train() client id: f_00004-5-6 loss: 0.813306  [  224/  306]
train() client id: f_00004-5-7 loss: 0.895338  [  256/  306]
train() client id: f_00004-5-8 loss: 0.918886  [  288/  306]
train() client id: f_00004-6-0 loss: 0.881634  [   32/  306]
train() client id: f_00004-6-1 loss: 0.831682  [   64/  306]
train() client id: f_00004-6-2 loss: 0.976592  [   96/  306]
train() client id: f_00004-6-3 loss: 0.866716  [  128/  306]
train() client id: f_00004-6-4 loss: 0.838970  [  160/  306]
train() client id: f_00004-6-5 loss: 0.786549  [  192/  306]
train() client id: f_00004-6-6 loss: 0.834576  [  224/  306]
train() client id: f_00004-6-7 loss: 0.764211  [  256/  306]
train() client id: f_00004-6-8 loss: 0.775914  [  288/  306]
train() client id: f_00004-7-0 loss: 0.794718  [   32/  306]
train() client id: f_00004-7-1 loss: 0.842426  [   64/  306]
train() client id: f_00004-7-2 loss: 0.865233  [   96/  306]
train() client id: f_00004-7-3 loss: 0.755304  [  128/  306]
train() client id: f_00004-7-4 loss: 0.649738  [  160/  306]
train() client id: f_00004-7-5 loss: 0.961938  [  192/  306]
train() client id: f_00004-7-6 loss: 0.850991  [  224/  306]
train() client id: f_00004-7-7 loss: 0.988790  [  256/  306]
train() client id: f_00004-7-8 loss: 0.820062  [  288/  306]
train() client id: f_00004-8-0 loss: 0.816105  [   32/  306]
train() client id: f_00004-8-1 loss: 0.877478  [   64/  306]
train() client id: f_00004-8-2 loss: 0.886696  [   96/  306]
train() client id: f_00004-8-3 loss: 0.783947  [  128/  306]
train() client id: f_00004-8-4 loss: 0.771729  [  160/  306]
train() client id: f_00004-8-5 loss: 0.821673  [  192/  306]
train() client id: f_00004-8-6 loss: 0.894358  [  224/  306]
train() client id: f_00004-8-7 loss: 0.942673  [  256/  306]
train() client id: f_00004-8-8 loss: 0.813303  [  288/  306]
train() client id: f_00004-9-0 loss: 0.890109  [   32/  306]
train() client id: f_00004-9-1 loss: 0.791898  [   64/  306]
train() client id: f_00004-9-2 loss: 0.852129  [   96/  306]
train() client id: f_00004-9-3 loss: 0.728402  [  128/  306]
train() client id: f_00004-9-4 loss: 0.814322  [  160/  306]
train() client id: f_00004-9-5 loss: 0.929053  [  192/  306]
train() client id: f_00004-9-6 loss: 0.961397  [  224/  306]
train() client id: f_00004-9-7 loss: 0.814425  [  256/  306]
train() client id: f_00004-9-8 loss: 0.822978  [  288/  306]
train() client id: f_00004-10-0 loss: 0.993926  [   32/  306]
train() client id: f_00004-10-1 loss: 0.831028  [   64/  306]
train() client id: f_00004-10-2 loss: 0.871590  [   96/  306]
train() client id: f_00004-10-3 loss: 0.737394  [  128/  306]
train() client id: f_00004-10-4 loss: 0.822413  [  160/  306]
train() client id: f_00004-10-5 loss: 0.884372  [  192/  306]
train() client id: f_00004-10-6 loss: 0.777382  [  224/  306]
train() client id: f_00004-10-7 loss: 0.870850  [  256/  306]
train() client id: f_00004-10-8 loss: 0.832439  [  288/  306]
train() client id: f_00004-11-0 loss: 0.810886  [   32/  306]
train() client id: f_00004-11-1 loss: 0.886627  [   64/  306]
train() client id: f_00004-11-2 loss: 0.928746  [   96/  306]
train() client id: f_00004-11-3 loss: 0.831143  [  128/  306]
train() client id: f_00004-11-4 loss: 0.777234  [  160/  306]
train() client id: f_00004-11-5 loss: 0.843362  [  192/  306]
train() client id: f_00004-11-6 loss: 0.888622  [  224/  306]
train() client id: f_00004-11-7 loss: 0.762699  [  256/  306]
train() client id: f_00004-11-8 loss: 0.827819  [  288/  306]
train() client id: f_00005-0-0 loss: 0.389907  [   32/  146]
train() client id: f_00005-0-1 loss: 0.759232  [   64/  146]
train() client id: f_00005-0-2 loss: 0.506777  [   96/  146]
train() client id: f_00005-0-3 loss: 0.729371  [  128/  146]
train() client id: f_00005-1-0 loss: 0.644827  [   32/  146]
train() client id: f_00005-1-1 loss: 0.580000  [   64/  146]
train() client id: f_00005-1-2 loss: 0.412549  [   96/  146]
train() client id: f_00005-1-3 loss: 0.802699  [  128/  146]
train() client id: f_00005-2-0 loss: 0.398517  [   32/  146]
train() client id: f_00005-2-1 loss: 0.612756  [   64/  146]
train() client id: f_00005-2-2 loss: 0.681073  [   96/  146]
train() client id: f_00005-2-3 loss: 0.633624  [  128/  146]
train() client id: f_00005-3-0 loss: 0.607257  [   32/  146]
train() client id: f_00005-3-1 loss: 0.482137  [   64/  146]
train() client id: f_00005-3-2 loss: 0.453741  [   96/  146]
train() client id: f_00005-3-3 loss: 0.665125  [  128/  146]
train() client id: f_00005-4-0 loss: 0.501032  [   32/  146]
train() client id: f_00005-4-1 loss: 0.663760  [   64/  146]
train() client id: f_00005-4-2 loss: 0.428309  [   96/  146]
train() client id: f_00005-4-3 loss: 0.851406  [  128/  146]
train() client id: f_00005-5-0 loss: 0.589557  [   32/  146]
train() client id: f_00005-5-1 loss: 0.509851  [   64/  146]
train() client id: f_00005-5-2 loss: 0.692522  [   96/  146]
train() client id: f_00005-5-3 loss: 0.637409  [  128/  146]
train() client id: f_00005-6-0 loss: 0.519580  [   32/  146]
train() client id: f_00005-6-1 loss: 0.556605  [   64/  146]
train() client id: f_00005-6-2 loss: 0.615994  [   96/  146]
train() client id: f_00005-6-3 loss: 0.651495  [  128/  146]
train() client id: f_00005-7-0 loss: 0.530361  [   32/  146]
train() client id: f_00005-7-1 loss: 0.589536  [   64/  146]
train() client id: f_00005-7-2 loss: 0.522226  [   96/  146]
train() client id: f_00005-7-3 loss: 0.807427  [  128/  146]
train() client id: f_00005-8-0 loss: 0.565403  [   32/  146]
train() client id: f_00005-8-1 loss: 0.903111  [   64/  146]
train() client id: f_00005-8-2 loss: 0.404793  [   96/  146]
train() client id: f_00005-8-3 loss: 0.418566  [  128/  146]
train() client id: f_00005-9-0 loss: 0.555791  [   32/  146]
train() client id: f_00005-9-1 loss: 0.330079  [   64/  146]
train() client id: f_00005-9-2 loss: 0.651922  [   96/  146]
train() client id: f_00005-9-3 loss: 0.793767  [  128/  146]
train() client id: f_00005-10-0 loss: 0.606942  [   32/  146]
train() client id: f_00005-10-1 loss: 0.632536  [   64/  146]
train() client id: f_00005-10-2 loss: 0.593943  [   96/  146]
train() client id: f_00005-10-3 loss: 0.626965  [  128/  146]
train() client id: f_00005-11-0 loss: 0.490568  [   32/  146]
train() client id: f_00005-11-1 loss: 0.754588  [   64/  146]
train() client id: f_00005-11-2 loss: 0.536295  [   96/  146]
train() client id: f_00005-11-3 loss: 0.555191  [  128/  146]
train() client id: f_00006-0-0 loss: 0.529314  [   32/   54]
train() client id: f_00006-1-0 loss: 0.599657  [   32/   54]
train() client id: f_00006-2-0 loss: 0.608043  [   32/   54]
train() client id: f_00006-3-0 loss: 0.543365  [   32/   54]
train() client id: f_00006-4-0 loss: 0.548431  [   32/   54]
train() client id: f_00006-5-0 loss: 0.586632  [   32/   54]
train() client id: f_00006-6-0 loss: 0.592312  [   32/   54]
train() client id: f_00006-7-0 loss: 0.604747  [   32/   54]
train() client id: f_00006-8-0 loss: 0.590410  [   32/   54]
train() client id: f_00006-9-0 loss: 0.584876  [   32/   54]
train() client id: f_00006-10-0 loss: 0.527458  [   32/   54]
train() client id: f_00006-11-0 loss: 0.498346  [   32/   54]
train() client id: f_00007-0-0 loss: 0.854789  [   32/  179]
train() client id: f_00007-0-1 loss: 0.590092  [   64/  179]
train() client id: f_00007-0-2 loss: 0.648889  [   96/  179]
train() client id: f_00007-0-3 loss: 0.474959  [  128/  179]
train() client id: f_00007-0-4 loss: 0.521692  [  160/  179]
train() client id: f_00007-1-0 loss: 0.647373  [   32/  179]
train() client id: f_00007-1-1 loss: 0.579551  [   64/  179]
train() client id: f_00007-1-2 loss: 0.534113  [   96/  179]
train() client id: f_00007-1-3 loss: 0.745498  [  128/  179]
train() client id: f_00007-1-4 loss: 0.659678  [  160/  179]
train() client id: f_00007-2-0 loss: 0.604216  [   32/  179]
train() client id: f_00007-2-1 loss: 0.712217  [   64/  179]
train() client id: f_00007-2-2 loss: 0.559112  [   96/  179]
train() client id: f_00007-2-3 loss: 0.555990  [  128/  179]
train() client id: f_00007-2-4 loss: 0.623814  [  160/  179]
train() client id: f_00007-3-0 loss: 0.585564  [   32/  179]
train() client id: f_00007-3-1 loss: 0.802973  [   64/  179]
train() client id: f_00007-3-2 loss: 0.472838  [   96/  179]
train() client id: f_00007-3-3 loss: 0.610803  [  128/  179]
train() client id: f_00007-3-4 loss: 0.544971  [  160/  179]
train() client id: f_00007-4-0 loss: 0.546982  [   32/  179]
train() client id: f_00007-4-1 loss: 0.448947  [   64/  179]
train() client id: f_00007-4-2 loss: 0.703225  [   96/  179]
train() client id: f_00007-4-3 loss: 0.635413  [  128/  179]
train() client id: f_00007-4-4 loss: 0.641577  [  160/  179]
train() client id: f_00007-5-0 loss: 0.838114  [   32/  179]
train() client id: f_00007-5-1 loss: 0.460179  [   64/  179]
train() client id: f_00007-5-2 loss: 0.545119  [   96/  179]
train() client id: f_00007-5-3 loss: 0.526973  [  128/  179]
train() client id: f_00007-5-4 loss: 0.467679  [  160/  179]
train() client id: f_00007-6-0 loss: 0.601566  [   32/  179]
train() client id: f_00007-6-1 loss: 0.583181  [   64/  179]
train() client id: f_00007-6-2 loss: 0.724219  [   96/  179]
train() client id: f_00007-6-3 loss: 0.540549  [  128/  179]
train() client id: f_00007-6-4 loss: 0.632765  [  160/  179]
train() client id: f_00007-7-0 loss: 0.526939  [   32/  179]
train() client id: f_00007-7-1 loss: 0.670918  [   64/  179]
train() client id: f_00007-7-2 loss: 0.431561  [   96/  179]
train() client id: f_00007-7-3 loss: 0.780537  [  128/  179]
train() client id: f_00007-7-4 loss: 0.523871  [  160/  179]
train() client id: f_00007-8-0 loss: 0.513760  [   32/  179]
train() client id: f_00007-8-1 loss: 0.439420  [   64/  179]
train() client id: f_00007-8-2 loss: 0.697227  [   96/  179]
train() client id: f_00007-8-3 loss: 0.470075  [  128/  179]
train() client id: f_00007-8-4 loss: 0.845316  [  160/  179]
train() client id: f_00007-9-0 loss: 0.536120  [   32/  179]
train() client id: f_00007-9-1 loss: 0.601651  [   64/  179]
train() client id: f_00007-9-2 loss: 0.459844  [   96/  179]
train() client id: f_00007-9-3 loss: 0.498336  [  128/  179]
train() client id: f_00007-9-4 loss: 0.670547  [  160/  179]
train() client id: f_00007-10-0 loss: 0.702215  [   32/  179]
train() client id: f_00007-10-1 loss: 0.543570  [   64/  179]
train() client id: f_00007-10-2 loss: 0.743532  [   96/  179]
train() client id: f_00007-10-3 loss: 0.612140  [  128/  179]
train() client id: f_00007-10-4 loss: 0.417438  [  160/  179]
train() client id: f_00007-11-0 loss: 0.704346  [   32/  179]
train() client id: f_00007-11-1 loss: 0.603738  [   64/  179]
train() client id: f_00007-11-2 loss: 0.427259  [   96/  179]
train() client id: f_00007-11-3 loss: 0.662053  [  128/  179]
train() client id: f_00007-11-4 loss: 0.543511  [  160/  179]
train() client id: f_00008-0-0 loss: 0.641440  [   32/  130]
train() client id: f_00008-0-1 loss: 0.719287  [   64/  130]
train() client id: f_00008-0-2 loss: 0.764085  [   96/  130]
train() client id: f_00008-0-3 loss: 0.787149  [  128/  130]
train() client id: f_00008-1-0 loss: 0.682252  [   32/  130]
train() client id: f_00008-1-1 loss: 0.645675  [   64/  130]
train() client id: f_00008-1-2 loss: 0.691384  [   96/  130]
train() client id: f_00008-1-3 loss: 0.874222  [  128/  130]
train() client id: f_00008-2-0 loss: 0.686918  [   32/  130]
train() client id: f_00008-2-1 loss: 0.769463  [   64/  130]
train() client id: f_00008-2-2 loss: 0.768057  [   96/  130]
train() client id: f_00008-2-3 loss: 0.703563  [  128/  130]
train() client id: f_00008-3-0 loss: 0.630438  [   32/  130]
train() client id: f_00008-3-1 loss: 0.752953  [   64/  130]
train() client id: f_00008-3-2 loss: 0.738519  [   96/  130]
train() client id: f_00008-3-3 loss: 0.749745  [  128/  130]
train() client id: f_00008-4-0 loss: 0.691434  [   32/  130]
train() client id: f_00008-4-1 loss: 0.749254  [   64/  130]
train() client id: f_00008-4-2 loss: 0.687963  [   96/  130]
train() client id: f_00008-4-3 loss: 0.780477  [  128/  130]
train() client id: f_00008-5-0 loss: 0.656456  [   32/  130]
train() client id: f_00008-5-1 loss: 0.892467  [   64/  130]
train() client id: f_00008-5-2 loss: 0.664502  [   96/  130]
train() client id: f_00008-5-3 loss: 0.709800  [  128/  130]
train() client id: f_00008-6-0 loss: 0.668975  [   32/  130]
train() client id: f_00008-6-1 loss: 0.791727  [   64/  130]
train() client id: f_00008-6-2 loss: 0.737965  [   96/  130]
train() client id: f_00008-6-3 loss: 0.668428  [  128/  130]
train() client id: f_00008-7-0 loss: 0.727487  [   32/  130]
train() client id: f_00008-7-1 loss: 0.586700  [   64/  130]
train() client id: f_00008-7-2 loss: 0.765824  [   96/  130]
train() client id: f_00008-7-3 loss: 0.793954  [  128/  130]
train() client id: f_00008-8-0 loss: 0.613592  [   32/  130]
train() client id: f_00008-8-1 loss: 0.766158  [   64/  130]
train() client id: f_00008-8-2 loss: 0.796314  [   96/  130]
train() client id: f_00008-8-3 loss: 0.681846  [  128/  130]
train() client id: f_00008-9-0 loss: 0.764546  [   32/  130]
train() client id: f_00008-9-1 loss: 0.751129  [   64/  130]
train() client id: f_00008-9-2 loss: 0.720228  [   96/  130]
train() client id: f_00008-9-3 loss: 0.611314  [  128/  130]
train() client id: f_00008-10-0 loss: 0.629097  [   32/  130]
train() client id: f_00008-10-1 loss: 0.801225  [   64/  130]
train() client id: f_00008-10-2 loss: 0.626088  [   96/  130]
train() client id: f_00008-10-3 loss: 0.832615  [  128/  130]
train() client id: f_00008-11-0 loss: 0.668955  [   32/  130]
train() client id: f_00008-11-1 loss: 0.864889  [   64/  130]
train() client id: f_00008-11-2 loss: 0.625349  [   96/  130]
train() client id: f_00008-11-3 loss: 0.741759  [  128/  130]
train() client id: f_00009-0-0 loss: 0.988466  [   32/  118]
train() client id: f_00009-0-1 loss: 0.941291  [   64/  118]
train() client id: f_00009-0-2 loss: 1.021895  [   96/  118]
train() client id: f_00009-1-0 loss: 0.928251  [   32/  118]
train() client id: f_00009-1-1 loss: 0.930922  [   64/  118]
train() client id: f_00009-1-2 loss: 0.852882  [   96/  118]
train() client id: f_00009-2-0 loss: 0.864121  [   32/  118]
train() client id: f_00009-2-1 loss: 0.816697  [   64/  118]
train() client id: f_00009-2-2 loss: 0.930700  [   96/  118]
train() client id: f_00009-3-0 loss: 0.939448  [   32/  118]
train() client id: f_00009-3-1 loss: 0.952116  [   64/  118]
train() client id: f_00009-3-2 loss: 0.841708  [   96/  118]
train() client id: f_00009-4-0 loss: 0.843611  [   32/  118]
train() client id: f_00009-4-1 loss: 0.670046  [   64/  118]
train() client id: f_00009-4-2 loss: 0.949075  [   96/  118]
train() client id: f_00009-5-0 loss: 0.785466  [   32/  118]
train() client id: f_00009-5-1 loss: 0.745247  [   64/  118]
train() client id: f_00009-5-2 loss: 0.751029  [   96/  118]
train() client id: f_00009-6-0 loss: 0.699555  [   32/  118]
train() client id: f_00009-6-1 loss: 0.590576  [   64/  118]
train() client id: f_00009-6-2 loss: 0.888389  [   96/  118]
train() client id: f_00009-7-0 loss: 0.811518  [   32/  118]
train() client id: f_00009-7-1 loss: 0.560867  [   64/  118]
train() client id: f_00009-7-2 loss: 0.877226  [   96/  118]
train() client id: f_00009-8-0 loss: 0.918311  [   32/  118]
train() client id: f_00009-8-1 loss: 0.618078  [   64/  118]
train() client id: f_00009-8-2 loss: 0.805149  [   96/  118]
train() client id: f_00009-9-0 loss: 0.832906  [   32/  118]
train() client id: f_00009-9-1 loss: 0.685767  [   64/  118]
train() client id: f_00009-9-2 loss: 0.655031  [   96/  118]
train() client id: f_00009-10-0 loss: 0.813009  [   32/  118]
train() client id: f_00009-10-1 loss: 0.662912  [   64/  118]
train() client id: f_00009-10-2 loss: 0.720307  [   96/  118]
train() client id: f_00009-11-0 loss: 0.876901  [   32/  118]
train() client id: f_00009-11-1 loss: 0.611362  [   64/  118]
train() client id: f_00009-11-2 loss: 0.592349  [   96/  118]
At round 27 accuracy: 0.6392572944297082
At round 27 training accuracy: 0.5861837692823608
At round 27 training loss: 0.8253124866318128
update_location
xs = -3.905658 4.200318 155.009024 18.811294 0.979296 3.956410 -117.443192 -96.324852 139.663977 -82.060879 
ys = 147.587959 130.555839 1.320614 -117.455176 109.350187 92.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 2.2885738318825704
ys mean: 38.39414253552871
dists_uav = 178.318423 164.506747 184.470977 155.401361 148.183745 136.492197 154.271818 138.849391 172.669327 129.421790 
uav_gains = -106.320857 -105.419643 -106.709742 -104.793609 -104.273887 -103.378976 -104.713747 -103.565216 -105.957984 -102.800801 
uav_gains_db_mean: -104.7934460769573
dists_bs = 173.276351 184.629460 372.921553 350.845903 187.826012 196.926155 186.717490 191.122038 351.849309 194.623158 
bs_gains = -102.251759 -103.023487 -111.572349 -110.830319 -103.232220 -103.807554 -103.160239 -103.443761 -110.865047 -103.664506 
bs_gains_db_mean: -105.58512401406836
Round 28
-------------------------------
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.2963697  15.14398805  7.18662324  2.58300247 17.46743421  8.40944175
  3.20540132 10.27679841  7.57595044  6.8226403 ]
obj_prev = 85.96764987436393
eta_min = 1.97492431522131e-13	eta_max = 0.9251230340770827
af = 18.15364795335576	bf = 1.507335053902618	zeta = 19.96901274869134	eta = 0.909090909090909
af = 18.15364795335576	bf = 1.507335053902618	zeta = 35.613187548855024	eta = 0.509745102946827
af = 18.15364795335576	bf = 1.507335053902618	zeta = 28.02638083867362	eta = 0.6477342921247089
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.65886868799346	eta = 0.6809609277055236
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.58884556407932	eta = 0.682754274141212
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.588647822049044	eta = 0.6827593518426902
eta = 0.6827593518426902
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [0.03148978 0.0662285  0.03098994 0.01074651 0.07647517 0.03648815
 0.01349562 0.04473547 0.03248945 0.02949042]
ene_total = [2.34522213 4.30343838 2.32693545 1.09048608 4.90813228 2.57679446
 1.24971036 3.05496696 2.56930912 2.1636526 ]
ti_comp = [0.402737   0.41597589 0.40085904 0.40941141 0.41525818 0.41320547
 0.40973259 0.41408375 0.37449574 0.41372635]
ti_coms = [0.08528891 0.07205001 0.08716686 0.07861449 0.07276772 0.07482043
 0.07829331 0.07394215 0.11353016 0.07429955]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.20322169e-05 1.04924856e-04 1.15760063e-05 4.62767175e-07
 1.62108192e-04 1.77829383e-05 9.15076330e-07 3.26332096e-05
 1.52831670e-05 9.36476148e-06]
ene_total = [0.50052546 0.42838512 0.51150413 0.46073322 0.43594225 0.43951388
 0.4588775  0.43523713 0.66621878 0.43596797]
optimize_network iter = 0 obj = 4.772905437718752
eta = 0.6827593518426902
freqs = [39094714.05548728 79606171.90046072 38654407.63725649 13124341.80140644
 92081472.39087442 44152552.9427729  16468811.85493763 54017423.64815775
 43377597.092848   35640012.17367139]
eta_min = 0.6827593518432681	eta_max = 0.6827593518426932
af = 0.017883468190837512	bf = 1.507335053902618	zeta = 0.019671815009921264	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [3.00512114e-06 2.62056364e-05 2.89117971e-06 1.15578985e-07
 4.04875310e-05 4.44139964e-06 2.28546015e-07 8.15034742e-06
 3.81706619e-06 2.33890753e-06]
ene_total = [1.74481114 1.47881446 1.78319283 1.60772537 1.49641265 1.53101993
 1.60118017 1.51381715 2.32252357 1.51993757]
ti_comp = [0.402737   0.41597589 0.40085904 0.40941141 0.41525818 0.41320547
 0.40973259 0.41408375 0.37449574 0.41372635]
ti_coms = [0.08528891 0.07205001 0.08716686 0.07861449 0.07276772 0.07482043
 0.07829331 0.07394215 0.11353016 0.07429955]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.20322169e-05 1.04924856e-04 1.15760063e-05 4.62767175e-07
 1.62108192e-04 1.77829383e-05 9.15076330e-07 3.26332096e-05
 1.52831670e-05 9.36476148e-06]
ene_total = [0.50052546 0.42838512 0.51150413 0.46073322 0.43594225 0.43951388
 0.4588775  0.43523713 0.66621878 0.43596797]
optimize_network iter = 1 obj = 4.772905437718796
eta = 0.6827593518426932
freqs = [39094714.05548728 79606171.9004607  38654407.6372565  13124341.80140644
 92081472.39087439 44152552.94277289 16468811.85493763 54017423.64815772
 43377597.09284804 35640012.17367138]
Done!
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.15546788e-05 1.00760567e-04 1.11165744e-05 4.44400737e-07
 1.55674395e-04 1.70771639e-05 8.78758515e-07 3.13380533e-05
 1.46766043e-05 8.99309011e-06]
ene_total = [0.00854045 0.00730576 0.0087278  0.00786189 0.00743245 0.00749912
 0.00783021 0.00742555 0.01136769 0.00743895]
At round 28 energy consumption: 0.0814298731583106
At round 28 eta: 0.6827593518426932
At round 28 a_n: 18.59131914111349
At round 28 local rounds: 12.495942598469853
At round 28 global rounds: 58.60320626975522
gradient difference: 0.46477994322776794
train() client id: f_00000-0-0 loss: 1.005434  [   32/  126]
train() client id: f_00000-0-1 loss: 1.084366  [   64/  126]
train() client id: f_00000-0-2 loss: 1.179454  [   96/  126]
train() client id: f_00000-1-0 loss: 1.002947  [   32/  126]
train() client id: f_00000-1-1 loss: 0.934746  [   64/  126]
train() client id: f_00000-1-2 loss: 0.851937  [   96/  126]
train() client id: f_00000-2-0 loss: 0.905098  [   32/  126]
train() client id: f_00000-2-1 loss: 0.917944  [   64/  126]
train() client id: f_00000-2-2 loss: 0.894716  [   96/  126]
train() client id: f_00000-3-0 loss: 0.720121  [   32/  126]
train() client id: f_00000-3-1 loss: 0.991196  [   64/  126]
train() client id: f_00000-3-2 loss: 0.974476  [   96/  126]
train() client id: f_00000-4-0 loss: 0.772509  [   32/  126]
train() client id: f_00000-4-1 loss: 0.755981  [   64/  126]
train() client id: f_00000-4-2 loss: 0.885877  [   96/  126]
train() client id: f_00000-5-0 loss: 0.815399  [   32/  126]
train() client id: f_00000-5-1 loss: 0.745279  [   64/  126]
train() client id: f_00000-5-2 loss: 0.766504  [   96/  126]
train() client id: f_00000-6-0 loss: 0.755927  [   32/  126]
train() client id: f_00000-6-1 loss: 0.824170  [   64/  126]
train() client id: f_00000-6-2 loss: 0.696809  [   96/  126]
train() client id: f_00000-7-0 loss: 0.826562  [   32/  126]
train() client id: f_00000-7-1 loss: 0.767149  [   64/  126]
train() client id: f_00000-7-2 loss: 0.748535  [   96/  126]
train() client id: f_00000-8-0 loss: 0.834131  [   32/  126]
train() client id: f_00000-8-1 loss: 0.726836  [   64/  126]
train() client id: f_00000-8-2 loss: 0.653240  [   96/  126]
train() client id: f_00000-9-0 loss: 0.612443  [   32/  126]
train() client id: f_00000-9-1 loss: 0.666370  [   64/  126]
train() client id: f_00000-9-2 loss: 0.737064  [   96/  126]
train() client id: f_00000-10-0 loss: 0.680239  [   32/  126]
train() client id: f_00000-10-1 loss: 0.695969  [   64/  126]
train() client id: f_00000-10-2 loss: 0.734652  [   96/  126]
train() client id: f_00000-11-0 loss: 0.581723  [   32/  126]
train() client id: f_00000-11-1 loss: 0.751701  [   64/  126]
train() client id: f_00000-11-2 loss: 0.772587  [   96/  126]
train() client id: f_00001-0-0 loss: 0.344334  [   32/  265]
train() client id: f_00001-0-1 loss: 0.505624  [   64/  265]
train() client id: f_00001-0-2 loss: 0.329507  [   96/  265]
train() client id: f_00001-0-3 loss: 0.525730  [  128/  265]
train() client id: f_00001-0-4 loss: 0.445498  [  160/  265]
train() client id: f_00001-0-5 loss: 0.480376  [  192/  265]
train() client id: f_00001-0-6 loss: 0.405611  [  224/  265]
train() client id: f_00001-0-7 loss: 0.344848  [  256/  265]
train() client id: f_00001-1-0 loss: 0.395250  [   32/  265]
train() client id: f_00001-1-1 loss: 0.479413  [   64/  265]
train() client id: f_00001-1-2 loss: 0.320123  [   96/  265]
train() client id: f_00001-1-3 loss: 0.504479  [  128/  265]
train() client id: f_00001-1-4 loss: 0.468741  [  160/  265]
train() client id: f_00001-1-5 loss: 0.313994  [  192/  265]
train() client id: f_00001-1-6 loss: 0.392254  [  224/  265]
train() client id: f_00001-1-7 loss: 0.399748  [  256/  265]
train() client id: f_00001-2-0 loss: 0.426816  [   32/  265]
train() client id: f_00001-2-1 loss: 0.363163  [   64/  265]
train() client id: f_00001-2-2 loss: 0.505433  [   96/  265]
train() client id: f_00001-2-3 loss: 0.323802  [  128/  265]
train() client id: f_00001-2-4 loss: 0.364986  [  160/  265]
train() client id: f_00001-2-5 loss: 0.418576  [  192/  265]
train() client id: f_00001-2-6 loss: 0.394778  [  224/  265]
train() client id: f_00001-2-7 loss: 0.467531  [  256/  265]
train() client id: f_00001-3-0 loss: 0.444289  [   32/  265]
train() client id: f_00001-3-1 loss: 0.415433  [   64/  265]
train() client id: f_00001-3-2 loss: 0.429163  [   96/  265]
train() client id: f_00001-3-3 loss: 0.396452  [  128/  265]
train() client id: f_00001-3-4 loss: 0.311304  [  160/  265]
train() client id: f_00001-3-5 loss: 0.417583  [  192/  265]
train() client id: f_00001-3-6 loss: 0.327358  [  224/  265]
train() client id: f_00001-3-7 loss: 0.402550  [  256/  265]
train() client id: f_00001-4-0 loss: 0.421980  [   32/  265]
train() client id: f_00001-4-1 loss: 0.321534  [   64/  265]
train() client id: f_00001-4-2 loss: 0.524628  [   96/  265]
train() client id: f_00001-4-3 loss: 0.373716  [  128/  265]
train() client id: f_00001-4-4 loss: 0.430782  [  160/  265]
train() client id: f_00001-4-5 loss: 0.342421  [  192/  265]
train() client id: f_00001-4-6 loss: 0.364334  [  224/  265]
train() client id: f_00001-4-7 loss: 0.397022  [  256/  265]
train() client id: f_00001-5-0 loss: 0.380495  [   32/  265]
train() client id: f_00001-5-1 loss: 0.440791  [   64/  265]
train() client id: f_00001-5-2 loss: 0.344523  [   96/  265]
train() client id: f_00001-5-3 loss: 0.463890  [  128/  265]
train() client id: f_00001-5-4 loss: 0.317500  [  160/  265]
train() client id: f_00001-5-5 loss: 0.325358  [  192/  265]
train() client id: f_00001-5-6 loss: 0.368904  [  224/  265]
train() client id: f_00001-5-7 loss: 0.500024  [  256/  265]
train() client id: f_00001-6-0 loss: 0.307424  [   32/  265]
train() client id: f_00001-6-1 loss: 0.372430  [   64/  265]
train() client id: f_00001-6-2 loss: 0.333306  [   96/  265]
train() client id: f_00001-6-3 loss: 0.444386  [  128/  265]
train() client id: f_00001-6-4 loss: 0.332962  [  160/  265]
train() client id: f_00001-6-5 loss: 0.425564  [  192/  265]
train() client id: f_00001-6-6 loss: 0.428424  [  224/  265]
train() client id: f_00001-6-7 loss: 0.471733  [  256/  265]
train() client id: f_00001-7-0 loss: 0.298815  [   32/  265]
train() client id: f_00001-7-1 loss: 0.281331  [   64/  265]
train() client id: f_00001-7-2 loss: 0.318634  [   96/  265]
train() client id: f_00001-7-3 loss: 0.455193  [  128/  265]
train() client id: f_00001-7-4 loss: 0.288867  [  160/  265]
train() client id: f_00001-7-5 loss: 0.389979  [  192/  265]
train() client id: f_00001-7-6 loss: 0.564398  [  224/  265]
train() client id: f_00001-7-7 loss: 0.434012  [  256/  265]
train() client id: f_00001-8-0 loss: 0.460825  [   32/  265]
train() client id: f_00001-8-1 loss: 0.432238  [   64/  265]
train() client id: f_00001-8-2 loss: 0.393263  [   96/  265]
train() client id: f_00001-8-3 loss: 0.316855  [  128/  265]
train() client id: f_00001-8-4 loss: 0.413309  [  160/  265]
train() client id: f_00001-8-5 loss: 0.357544  [  192/  265]
train() client id: f_00001-8-6 loss: 0.271232  [  224/  265]
train() client id: f_00001-8-7 loss: 0.438891  [  256/  265]
train() client id: f_00001-9-0 loss: 0.413008  [   32/  265]
train() client id: f_00001-9-1 loss: 0.337229  [   64/  265]
train() client id: f_00001-9-2 loss: 0.357036  [   96/  265]
train() client id: f_00001-9-3 loss: 0.473820  [  128/  265]
train() client id: f_00001-9-4 loss: 0.394989  [  160/  265]
train() client id: f_00001-9-5 loss: 0.389845  [  192/  265]
train() client id: f_00001-9-6 loss: 0.365239  [  224/  265]
train() client id: f_00001-9-7 loss: 0.345781  [  256/  265]
train() client id: f_00001-10-0 loss: 0.297293  [   32/  265]
train() client id: f_00001-10-1 loss: 0.482612  [   64/  265]
train() client id: f_00001-10-2 loss: 0.432428  [   96/  265]
train() client id: f_00001-10-3 loss: 0.366675  [  128/  265]
train() client id: f_00001-10-4 loss: 0.424248  [  160/  265]
train() client id: f_00001-10-5 loss: 0.267574  [  192/  265]
train() client id: f_00001-10-6 loss: 0.469394  [  224/  265]
train() client id: f_00001-10-7 loss: 0.325208  [  256/  265]
train() client id: f_00001-11-0 loss: 0.362646  [   32/  265]
train() client id: f_00001-11-1 loss: 0.538319  [   64/  265]
train() client id: f_00001-11-2 loss: 0.335108  [   96/  265]
train() client id: f_00001-11-3 loss: 0.350442  [  128/  265]
train() client id: f_00001-11-4 loss: 0.417498  [  160/  265]
train() client id: f_00001-11-5 loss: 0.389412  [  192/  265]
train() client id: f_00001-11-6 loss: 0.318353  [  224/  265]
train() client id: f_00001-11-7 loss: 0.335857  [  256/  265]
train() client id: f_00002-0-0 loss: 0.944374  [   32/  124]
train() client id: f_00002-0-1 loss: 1.002622  [   64/  124]
train() client id: f_00002-0-2 loss: 1.021237  [   96/  124]
train() client id: f_00002-1-0 loss: 0.944498  [   32/  124]
train() client id: f_00002-1-1 loss: 1.036765  [   64/  124]
train() client id: f_00002-1-2 loss: 0.861308  [   96/  124]
train() client id: f_00002-2-0 loss: 0.958240  [   32/  124]
train() client id: f_00002-2-1 loss: 0.983421  [   64/  124]
train() client id: f_00002-2-2 loss: 0.878069  [   96/  124]
train() client id: f_00002-3-0 loss: 0.676165  [   32/  124]
train() client id: f_00002-3-1 loss: 0.853575  [   64/  124]
train() client id: f_00002-3-2 loss: 0.979770  [   96/  124]
train() client id: f_00002-4-0 loss: 0.810589  [   32/  124]
train() client id: f_00002-4-1 loss: 0.975218  [   64/  124]
train() client id: f_00002-4-2 loss: 0.890883  [   96/  124]
train() client id: f_00002-5-0 loss: 0.805744  [   32/  124]
train() client id: f_00002-5-1 loss: 0.958688  [   64/  124]
train() client id: f_00002-5-2 loss: 0.797283  [   96/  124]
train() client id: f_00002-6-0 loss: 0.823323  [   32/  124]
train() client id: f_00002-6-1 loss: 0.995936  [   64/  124]
train() client id: f_00002-6-2 loss: 0.734517  [   96/  124]
train() client id: f_00002-7-0 loss: 0.773274  [   32/  124]
train() client id: f_00002-7-1 loss: 1.082665  [   64/  124]
train() client id: f_00002-7-2 loss: 0.757302  [   96/  124]
train() client id: f_00002-8-0 loss: 0.761774  [   32/  124]
train() client id: f_00002-8-1 loss: 1.042131  [   64/  124]
train() client id: f_00002-8-2 loss: 0.948099  [   96/  124]
train() client id: f_00002-9-0 loss: 0.774560  [   32/  124]
train() client id: f_00002-9-1 loss: 0.909165  [   64/  124]
train() client id: f_00002-9-2 loss: 0.701708  [   96/  124]
train() client id: f_00002-10-0 loss: 0.835984  [   32/  124]
train() client id: f_00002-10-1 loss: 0.914227  [   64/  124]
train() client id: f_00002-10-2 loss: 0.707344  [   96/  124]
train() client id: f_00002-11-0 loss: 0.737414  [   32/  124]
train() client id: f_00002-11-1 loss: 0.743647  [   64/  124]
train() client id: f_00002-11-2 loss: 0.816247  [   96/  124]
train() client id: f_00003-0-0 loss: 0.784411  [   32/   43]
train() client id: f_00003-1-0 loss: 0.708598  [   32/   43]
train() client id: f_00003-2-0 loss: 0.523206  [   32/   43]
train() client id: f_00003-3-0 loss: 0.689642  [   32/   43]
train() client id: f_00003-4-0 loss: 0.722189  [   32/   43]
train() client id: f_00003-5-0 loss: 0.651481  [   32/   43]
train() client id: f_00003-6-0 loss: 0.520705  [   32/   43]
train() client id: f_00003-7-0 loss: 0.451189  [   32/   43]
train() client id: f_00003-8-0 loss: 0.731296  [   32/   43]
train() client id: f_00003-9-0 loss: 0.585848  [   32/   43]
train() client id: f_00003-10-0 loss: 0.628685  [   32/   43]
train() client id: f_00003-11-0 loss: 0.569831  [   32/   43]
train() client id: f_00004-0-0 loss: 0.831440  [   32/  306]
train() client id: f_00004-0-1 loss: 0.811041  [   64/  306]
train() client id: f_00004-0-2 loss: 0.895432  [   96/  306]
train() client id: f_00004-0-3 loss: 0.839657  [  128/  306]
train() client id: f_00004-0-4 loss: 0.980072  [  160/  306]
train() client id: f_00004-0-5 loss: 0.859846  [  192/  306]
train() client id: f_00004-0-6 loss: 0.936201  [  224/  306]
train() client id: f_00004-0-7 loss: 0.947498  [  256/  306]
train() client id: f_00004-0-8 loss: 1.114504  [  288/  306]
train() client id: f_00004-1-0 loss: 0.896177  [   32/  306]
train() client id: f_00004-1-1 loss: 0.883835  [   64/  306]
train() client id: f_00004-1-2 loss: 0.789979  [   96/  306]
train() client id: f_00004-1-3 loss: 0.875205  [  128/  306]
train() client id: f_00004-1-4 loss: 0.833563  [  160/  306]
train() client id: f_00004-1-5 loss: 0.928542  [  192/  306]
train() client id: f_00004-1-6 loss: 0.902108  [  224/  306]
train() client id: f_00004-1-7 loss: 1.030086  [  256/  306]
train() client id: f_00004-1-8 loss: 0.999154  [  288/  306]
train() client id: f_00004-2-0 loss: 1.006652  [   32/  306]
train() client id: f_00004-2-1 loss: 1.031359  [   64/  306]
train() client id: f_00004-2-2 loss: 0.692097  [   96/  306]
train() client id: f_00004-2-3 loss: 0.848965  [  128/  306]
train() client id: f_00004-2-4 loss: 0.966026  [  160/  306]
train() client id: f_00004-2-5 loss: 0.913511  [  192/  306]
train() client id: f_00004-2-6 loss: 1.017630  [  224/  306]
train() client id: f_00004-2-7 loss: 0.803079  [  256/  306]
train() client id: f_00004-2-8 loss: 0.840366  [  288/  306]
train() client id: f_00004-3-0 loss: 0.959307  [   32/  306]
train() client id: f_00004-3-1 loss: 0.819818  [   64/  306]
train() client id: f_00004-3-2 loss: 1.013331  [   96/  306]
train() client id: f_00004-3-3 loss: 0.816782  [  128/  306]
train() client id: f_00004-3-4 loss: 0.880995  [  160/  306]
train() client id: f_00004-3-5 loss: 0.942831  [  192/  306]
train() client id: f_00004-3-6 loss: 0.770229  [  224/  306]
train() client id: f_00004-3-7 loss: 0.814928  [  256/  306]
train() client id: f_00004-3-8 loss: 0.966791  [  288/  306]
train() client id: f_00004-4-0 loss: 0.889729  [   32/  306]
train() client id: f_00004-4-1 loss: 0.967719  [   64/  306]
train() client id: f_00004-4-2 loss: 0.835098  [   96/  306]
train() client id: f_00004-4-3 loss: 1.058593  [  128/  306]
train() client id: f_00004-4-4 loss: 0.875264  [  160/  306]
train() client id: f_00004-4-5 loss: 0.802419  [  192/  306]
train() client id: f_00004-4-6 loss: 0.932947  [  224/  306]
train() client id: f_00004-4-7 loss: 0.926778  [  256/  306]
train() client id: f_00004-4-8 loss: 0.890415  [  288/  306]
train() client id: f_00004-5-0 loss: 0.894046  [   32/  306]
train() client id: f_00004-5-1 loss: 0.920838  [   64/  306]
train() client id: f_00004-5-2 loss: 0.884296  [   96/  306]
train() client id: f_00004-5-3 loss: 0.895371  [  128/  306]
train() client id: f_00004-5-4 loss: 0.953847  [  160/  306]
train() client id: f_00004-5-5 loss: 0.885854  [  192/  306]
train() client id: f_00004-5-6 loss: 0.942007  [  224/  306]
train() client id: f_00004-5-7 loss: 0.866355  [  256/  306]
train() client id: f_00004-5-8 loss: 0.875275  [  288/  306]
train() client id: f_00004-6-0 loss: 0.848681  [   32/  306]
train() client id: f_00004-6-1 loss: 0.998594  [   64/  306]
train() client id: f_00004-6-2 loss: 0.926979  [   96/  306]
train() client id: f_00004-6-3 loss: 0.922944  [  128/  306]
train() client id: f_00004-6-4 loss: 0.962965  [  160/  306]
train() client id: f_00004-6-5 loss: 0.864057  [  192/  306]
train() client id: f_00004-6-6 loss: 0.969470  [  224/  306]
train() client id: f_00004-6-7 loss: 0.858542  [  256/  306]
train() client id: f_00004-6-8 loss: 0.793848  [  288/  306]
train() client id: f_00004-7-0 loss: 0.759086  [   32/  306]
train() client id: f_00004-7-1 loss: 0.937219  [   64/  306]
train() client id: f_00004-7-2 loss: 0.857481  [   96/  306]
train() client id: f_00004-7-3 loss: 0.889993  [  128/  306]
train() client id: f_00004-7-4 loss: 0.899746  [  160/  306]
train() client id: f_00004-7-5 loss: 0.889533  [  192/  306]
train() client id: f_00004-7-6 loss: 0.955557  [  224/  306]
train() client id: f_00004-7-7 loss: 0.842889  [  256/  306]
train() client id: f_00004-7-8 loss: 0.996554  [  288/  306]
train() client id: f_00004-8-0 loss: 0.826384  [   32/  306]
train() client id: f_00004-8-1 loss: 0.939843  [   64/  306]
train() client id: f_00004-8-2 loss: 0.896582  [   96/  306]
train() client id: f_00004-8-3 loss: 0.964646  [  128/  306]
train() client id: f_00004-8-4 loss: 0.906597  [  160/  306]
train() client id: f_00004-8-5 loss: 0.842360  [  192/  306]
train() client id: f_00004-8-6 loss: 1.003553  [  224/  306]
train() client id: f_00004-8-7 loss: 0.952072  [  256/  306]
train() client id: f_00004-8-8 loss: 0.798729  [  288/  306]
train() client id: f_00004-9-0 loss: 0.801579  [   32/  306]
train() client id: f_00004-9-1 loss: 0.872087  [   64/  306]
train() client id: f_00004-9-2 loss: 1.019622  [   96/  306]
train() client id: f_00004-9-3 loss: 0.903284  [  128/  306]
train() client id: f_00004-9-4 loss: 0.871254  [  160/  306]
train() client id: f_00004-9-5 loss: 1.056094  [  192/  306]
train() client id: f_00004-9-6 loss: 0.769546  [  224/  306]
train() client id: f_00004-9-7 loss: 0.972910  [  256/  306]
train() client id: f_00004-9-8 loss: 0.821111  [  288/  306]
train() client id: f_00004-10-0 loss: 0.968858  [   32/  306]
train() client id: f_00004-10-1 loss: 0.826436  [   64/  306]
train() client id: f_00004-10-2 loss: 0.846524  [   96/  306]
train() client id: f_00004-10-3 loss: 1.026702  [  128/  306]
train() client id: f_00004-10-4 loss: 0.873777  [  160/  306]
train() client id: f_00004-10-5 loss: 0.882702  [  192/  306]
train() client id: f_00004-10-6 loss: 0.887564  [  224/  306]
train() client id: f_00004-10-7 loss: 0.891629  [  256/  306]
train() client id: f_00004-10-8 loss: 0.890021  [  288/  306]
train() client id: f_00004-11-0 loss: 0.865915  [   32/  306]
train() client id: f_00004-11-1 loss: 0.847403  [   64/  306]
train() client id: f_00004-11-2 loss: 0.807063  [   96/  306]
train() client id: f_00004-11-3 loss: 0.814934  [  128/  306]
train() client id: f_00004-11-4 loss: 0.905596  [  160/  306]
train() client id: f_00004-11-5 loss: 1.033582  [  192/  306]
train() client id: f_00004-11-6 loss: 0.965550  [  224/  306]
train() client id: f_00004-11-7 loss: 0.953643  [  256/  306]
train() client id: f_00004-11-8 loss: 0.849289  [  288/  306]
train() client id: f_00005-0-0 loss: 0.574442  [   32/  146]
train() client id: f_00005-0-1 loss: 0.648144  [   64/  146]
train() client id: f_00005-0-2 loss: 0.609765  [   96/  146]
train() client id: f_00005-0-3 loss: 1.019377  [  128/  146]
train() client id: f_00005-1-0 loss: 0.653581  [   32/  146]
train() client id: f_00005-1-1 loss: 0.698733  [   64/  146]
train() client id: f_00005-1-2 loss: 0.643899  [   96/  146]
train() client id: f_00005-1-3 loss: 0.856358  [  128/  146]
train() client id: f_00005-2-0 loss: 0.823587  [   32/  146]
train() client id: f_00005-2-1 loss: 0.892033  [   64/  146]
train() client id: f_00005-2-2 loss: 0.509485  [   96/  146]
train() client id: f_00005-2-3 loss: 0.344797  [  128/  146]
train() client id: f_00005-3-0 loss: 0.433529  [   32/  146]
train() client id: f_00005-3-1 loss: 0.902742  [   64/  146]
train() client id: f_00005-3-2 loss: 0.626403  [   96/  146]
train() client id: f_00005-3-3 loss: 0.728170  [  128/  146]
train() client id: f_00005-4-0 loss: 0.649037  [   32/  146]
train() client id: f_00005-4-1 loss: 0.654362  [   64/  146]
train() client id: f_00005-4-2 loss: 0.809531  [   96/  146]
train() client id: f_00005-4-3 loss: 0.502264  [  128/  146]
train() client id: f_00005-5-0 loss: 0.547858  [   32/  146]
train() client id: f_00005-5-1 loss: 0.896515  [   64/  146]
train() client id: f_00005-5-2 loss: 0.379069  [   96/  146]
train() client id: f_00005-5-3 loss: 0.653147  [  128/  146]
train() client id: f_00005-6-0 loss: 0.681829  [   32/  146]
train() client id: f_00005-6-1 loss: 0.716618  [   64/  146]
train() client id: f_00005-6-2 loss: 0.570247  [   96/  146]
train() client id: f_00005-6-3 loss: 0.596870  [  128/  146]
train() client id: f_00005-7-0 loss: 0.829637  [   32/  146]
train() client id: f_00005-7-1 loss: 0.615651  [   64/  146]
train() client id: f_00005-7-2 loss: 0.626091  [   96/  146]
train() client id: f_00005-7-3 loss: 0.645611  [  128/  146]
train() client id: f_00005-8-0 loss: 0.528954  [   32/  146]
train() client id: f_00005-8-1 loss: 0.905879  [   64/  146]
train() client id: f_00005-8-2 loss: 0.476890  [   96/  146]
train() client id: f_00005-8-3 loss: 0.530328  [  128/  146]
train() client id: f_00005-9-0 loss: 0.423826  [   32/  146]
train() client id: f_00005-9-1 loss: 0.721778  [   64/  146]
train() client id: f_00005-9-2 loss: 0.770550  [   96/  146]
train() client id: f_00005-9-3 loss: 0.775132  [  128/  146]
train() client id: f_00005-10-0 loss: 0.620901  [   32/  146]
train() client id: f_00005-10-1 loss: 0.789962  [   64/  146]
train() client id: f_00005-10-2 loss: 0.493387  [   96/  146]
train() client id: f_00005-10-3 loss: 0.804647  [  128/  146]
train() client id: f_00005-11-0 loss: 0.563757  [   32/  146]
train() client id: f_00005-11-1 loss: 0.633422  [   64/  146]
train() client id: f_00005-11-2 loss: 0.871472  [   96/  146]
train() client id: f_00005-11-3 loss: 0.722938  [  128/  146]
train() client id: f_00006-0-0 loss: 0.543881  [   32/   54]
train() client id: f_00006-1-0 loss: 0.537707  [   32/   54]
train() client id: f_00006-2-0 loss: 0.548600  [   32/   54]
train() client id: f_00006-3-0 loss: 0.603655  [   32/   54]
train() client id: f_00006-4-0 loss: 0.578383  [   32/   54]
train() client id: f_00006-5-0 loss: 0.518961  [   32/   54]
train() client id: f_00006-6-0 loss: 0.500126  [   32/   54]
train() client id: f_00006-7-0 loss: 0.506469  [   32/   54]
train() client id: f_00006-8-0 loss: 0.544777  [   32/   54]
train() client id: f_00006-9-0 loss: 0.551969  [   32/   54]
train() client id: f_00006-10-0 loss: 0.533613  [   32/   54]
train() client id: f_00006-11-0 loss: 0.529750  [   32/   54]
train() client id: f_00007-0-0 loss: 0.876722  [   32/  179]
train() client id: f_00007-0-1 loss: 0.639124  [   64/  179]
train() client id: f_00007-0-2 loss: 0.481567  [   96/  179]
train() client id: f_00007-0-3 loss: 0.757301  [  128/  179]
train() client id: f_00007-0-4 loss: 0.517327  [  160/  179]
train() client id: f_00007-1-0 loss: 0.625382  [   32/  179]
train() client id: f_00007-1-1 loss: 0.486639  [   64/  179]
train() client id: f_00007-1-2 loss: 0.677972  [   96/  179]
train() client id: f_00007-1-3 loss: 0.724319  [  128/  179]
train() client id: f_00007-1-4 loss: 0.635501  [  160/  179]
train() client id: f_00007-2-0 loss: 0.730948  [   32/  179]
train() client id: f_00007-2-1 loss: 0.474681  [   64/  179]
train() client id: f_00007-2-2 loss: 0.471003  [   96/  179]
train() client id: f_00007-2-3 loss: 0.656046  [  128/  179]
train() client id: f_00007-2-4 loss: 0.882592  [  160/  179]
train() client id: f_00007-3-0 loss: 0.934064  [   32/  179]
train() client id: f_00007-3-1 loss: 0.492888  [   64/  179]
train() client id: f_00007-3-2 loss: 0.688362  [   96/  179]
train() client id: f_00007-3-3 loss: 0.438739  [  128/  179]
train() client id: f_00007-3-4 loss: 0.478140  [  160/  179]
train() client id: f_00007-4-0 loss: 0.434106  [   32/  179]
train() client id: f_00007-4-1 loss: 0.660832  [   64/  179]
train() client id: f_00007-4-2 loss: 0.657169  [   96/  179]
train() client id: f_00007-4-3 loss: 0.497562  [  128/  179]
train() client id: f_00007-4-4 loss: 0.683182  [  160/  179]
train() client id: f_00007-5-0 loss: 0.739452  [   32/  179]
train() client id: f_00007-5-1 loss: 0.451738  [   64/  179]
train() client id: f_00007-5-2 loss: 0.467557  [   96/  179]
train() client id: f_00007-5-3 loss: 0.637500  [  128/  179]
train() client id: f_00007-5-4 loss: 0.576707  [  160/  179]
train() client id: f_00007-6-0 loss: 0.680723  [   32/  179]
train() client id: f_00007-6-1 loss: 0.713716  [   64/  179]
train() client id: f_00007-6-2 loss: 0.698130  [   96/  179]
train() client id: f_00007-6-3 loss: 0.434191  [  128/  179]
train() client id: f_00007-6-4 loss: 0.630780  [  160/  179]
train() client id: f_00007-7-0 loss: 0.539640  [   32/  179]
train() client id: f_00007-7-1 loss: 0.617680  [   64/  179]
train() client id: f_00007-7-2 loss: 0.460427  [   96/  179]
train() client id: f_00007-7-3 loss: 0.577693  [  128/  179]
train() client id: f_00007-7-4 loss: 0.689665  [  160/  179]
train() client id: f_00007-8-0 loss: 0.772527  [   32/  179]
train() client id: f_00007-8-1 loss: 0.826672  [   64/  179]
train() client id: f_00007-8-2 loss: 0.524319  [   96/  179]
train() client id: f_00007-8-3 loss: 0.510522  [  128/  179]
train() client id: f_00007-8-4 loss: 0.495271  [  160/  179]
train() client id: f_00007-9-0 loss: 0.455725  [   32/  179]
train() client id: f_00007-9-1 loss: 0.536125  [   64/  179]
train() client id: f_00007-9-2 loss: 0.543436  [   96/  179]
train() client id: f_00007-9-3 loss: 0.764072  [  128/  179]
train() client id: f_00007-9-4 loss: 0.647919  [  160/  179]
train() client id: f_00007-10-0 loss: 0.608048  [   32/  179]
train() client id: f_00007-10-1 loss: 0.652715  [   64/  179]
train() client id: f_00007-10-2 loss: 0.719802  [   96/  179]
train() client id: f_00007-10-3 loss: 0.585609  [  128/  179]
train() client id: f_00007-10-4 loss: 0.444462  [  160/  179]
train() client id: f_00007-11-0 loss: 0.474992  [   32/  179]
train() client id: f_00007-11-1 loss: 0.539519  [   64/  179]
train() client id: f_00007-11-2 loss: 0.924109  [   96/  179]
train() client id: f_00007-11-3 loss: 0.545839  [  128/  179]
train() client id: f_00007-11-4 loss: 0.561141  [  160/  179]
train() client id: f_00008-0-0 loss: 0.655206  [   32/  130]
train() client id: f_00008-0-1 loss: 0.686440  [   64/  130]
train() client id: f_00008-0-2 loss: 0.830968  [   96/  130]
train() client id: f_00008-0-3 loss: 0.590939  [  128/  130]
train() client id: f_00008-1-0 loss: 0.738829  [   32/  130]
train() client id: f_00008-1-1 loss: 0.660019  [   64/  130]
train() client id: f_00008-1-2 loss: 0.780224  [   96/  130]
train() client id: f_00008-1-3 loss: 0.601044  [  128/  130]
train() client id: f_00008-2-0 loss: 0.655059  [   32/  130]
train() client id: f_00008-2-1 loss: 0.699919  [   64/  130]
train() client id: f_00008-2-2 loss: 0.672004  [   96/  130]
train() client id: f_00008-2-3 loss: 0.736335  [  128/  130]
train() client id: f_00008-3-0 loss: 0.622743  [   32/  130]
train() client id: f_00008-3-1 loss: 0.674661  [   64/  130]
train() client id: f_00008-3-2 loss: 0.695031  [   96/  130]
train() client id: f_00008-3-3 loss: 0.789944  [  128/  130]
train() client id: f_00008-4-0 loss: 0.694441  [   32/  130]
train() client id: f_00008-4-1 loss: 0.674864  [   64/  130]
train() client id: f_00008-4-2 loss: 0.637620  [   96/  130]
train() client id: f_00008-4-3 loss: 0.751952  [  128/  130]
train() client id: f_00008-5-0 loss: 0.658124  [   32/  130]
train() client id: f_00008-5-1 loss: 0.821561  [   64/  130]
train() client id: f_00008-5-2 loss: 0.610539  [   96/  130]
train() client id: f_00008-5-3 loss: 0.695466  [  128/  130]
train() client id: f_00008-6-0 loss: 0.714270  [   32/  130]
train() client id: f_00008-6-1 loss: 0.668476  [   64/  130]
train() client id: f_00008-6-2 loss: 0.683951  [   96/  130]
train() client id: f_00008-6-3 loss: 0.705541  [  128/  130]
train() client id: f_00008-7-0 loss: 0.666505  [   32/  130]
train() client id: f_00008-7-1 loss: 0.713935  [   64/  130]
train() client id: f_00008-7-2 loss: 0.697590  [   96/  130]
train() client id: f_00008-7-3 loss: 0.703981  [  128/  130]
train() client id: f_00008-8-0 loss: 0.567818  [   32/  130]
train() client id: f_00008-8-1 loss: 0.695920  [   64/  130]
train() client id: f_00008-8-2 loss: 0.694216  [   96/  130]
train() client id: f_00008-8-3 loss: 0.812322  [  128/  130]
train() client id: f_00008-9-0 loss: 0.694763  [   32/  130]
train() client id: f_00008-9-1 loss: 0.760136  [   64/  130]
train() client id: f_00008-9-2 loss: 0.651279  [   96/  130]
train() client id: f_00008-9-3 loss: 0.670468  [  128/  130]
train() client id: f_00008-10-0 loss: 0.619808  [   32/  130]
train() client id: f_00008-10-1 loss: 0.576445  [   64/  130]
train() client id: f_00008-10-2 loss: 0.769846  [   96/  130]
train() client id: f_00008-10-3 loss: 0.732443  [  128/  130]
train() client id: f_00008-11-0 loss: 0.668140  [   32/  130]
train() client id: f_00008-11-1 loss: 0.666044  [   64/  130]
train() client id: f_00008-11-2 loss: 0.738971  [   96/  130]
train() client id: f_00008-11-3 loss: 0.706887  [  128/  130]
train() client id: f_00009-0-0 loss: 1.110635  [   32/  118]
train() client id: f_00009-0-1 loss: 1.289800  [   64/  118]
train() client id: f_00009-0-2 loss: 1.005359  [   96/  118]
train() client id: f_00009-1-0 loss: 1.257568  [   32/  118]
train() client id: f_00009-1-1 loss: 1.015106  [   64/  118]
train() client id: f_00009-1-2 loss: 1.073556  [   96/  118]
train() client id: f_00009-2-0 loss: 1.192871  [   32/  118]
train() client id: f_00009-2-1 loss: 1.172100  [   64/  118]
train() client id: f_00009-2-2 loss: 0.953265  [   96/  118]
train() client id: f_00009-3-0 loss: 1.099369  [   32/  118]
train() client id: f_00009-3-1 loss: 1.108271  [   64/  118]
train() client id: f_00009-3-2 loss: 0.957306  [   96/  118]
train() client id: f_00009-4-0 loss: 1.129061  [   32/  118]
train() client id: f_00009-4-1 loss: 1.014173  [   64/  118]
train() client id: f_00009-4-2 loss: 0.958088  [   96/  118]
train() client id: f_00009-5-0 loss: 1.050616  [   32/  118]
train() client id: f_00009-5-1 loss: 0.976810  [   64/  118]
train() client id: f_00009-5-2 loss: 0.901962  [   96/  118]
train() client id: f_00009-6-0 loss: 0.954598  [   32/  118]
train() client id: f_00009-6-1 loss: 0.987206  [   64/  118]
train() client id: f_00009-6-2 loss: 0.874236  [   96/  118]
train() client id: f_00009-7-0 loss: 0.936014  [   32/  118]
train() client id: f_00009-7-1 loss: 1.031175  [   64/  118]
train() client id: f_00009-7-2 loss: 0.937967  [   96/  118]
train() client id: f_00009-8-0 loss: 0.867374  [   32/  118]
train() client id: f_00009-8-1 loss: 0.946350  [   64/  118]
train() client id: f_00009-8-2 loss: 1.175424  [   96/  118]
train() client id: f_00009-9-0 loss: 0.905662  [   32/  118]
train() client id: f_00009-9-1 loss: 1.047744  [   64/  118]
train() client id: f_00009-9-2 loss: 0.886663  [   96/  118]
train() client id: f_00009-10-0 loss: 1.002940  [   32/  118]
train() client id: f_00009-10-1 loss: 0.924469  [   64/  118]
train() client id: f_00009-10-2 loss: 0.908857  [   96/  118]
train() client id: f_00009-11-0 loss: 0.791797  [   32/  118]
train() client id: f_00009-11-1 loss: 1.091790  [   64/  118]
train() client id: f_00009-11-2 loss: 0.920406  [   96/  118]
At round 28 accuracy: 0.6392572944297082
At round 28 training accuracy: 0.5814889336016097
At round 28 training loss: 0.8305925113312848
update_location
xs = -3.905658 4.200318 160.009024 18.811294 0.979296 3.956410 -122.443192 -101.324852 144.663977 -87.060879 
ys = 152.587959 135.555839 1.320614 -122.455176 114.350187 97.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 1.7885738318825701
ys mean: 39.89414253552871
dists_uav = 182.478326 168.502309 188.691896 159.214116 151.910909 139.940206 158.111435 142.363625 176.738044 132.648439 
uav_gains = -106.584351 -105.685513 -106.974172 -105.059447 -104.545102 -103.650360 -104.983140 -103.837242 -106.219998 -103.068419 
uav_gains_db_mean: -105.06077440118175
dists_bs = 172.556000 183.490043 377.353382 355.024505 186.137349 194.892411 185.237289 189.118698 356.327905 192.285679 
bs_gains = -102.201101 -102.948209 -111.716010 -110.974292 -103.122398 -103.681317 -103.063455 -103.315624 -111.018855 -103.517573 
bs_gains_db_mean: -105.55588346853698
Round 29
-------------------------------
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.16426333 14.86448857  7.05664821  2.53741575 17.14489673  8.25364943
  3.14835783 10.08926783  7.43869069  6.69596016]
obj_prev = 84.39363852739328
eta_min = 1.1633239161667213e-13	eta_max = 0.9255734667757686
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 19.601082838327166	eta = 0.9090909090909091
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 35.06532785864593	eta = 0.5081705292616409
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 27.55402889869821	eta = 0.6466991191078749
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.19954622843339	eta = 0.6801326275385099
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.129948192651668	eta = 0.6819441847065052
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.129750162898475	eta = 0.6819493529625255
eta = 0.6819493529625255
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [0.03158773 0.06643451 0.03108634 0.01077994 0.07671306 0.03660165
 0.0135376  0.04487463 0.03259051 0.02958216]
ene_total = [2.30902016 4.22374425 2.291367   1.07582749 4.81690287 2.52674818
 1.23224478 3.00469125 2.52859005 2.12061414]
ti_comp = [0.41103184 0.42578996 0.40909461 0.41788221 0.42519616 0.42322413
 0.41819745 0.42265509 0.38279172 0.42381267]
ti_coms = [0.08655269 0.07179457 0.08848992 0.07970232 0.07238837 0.0743604
 0.07938708 0.07492943 0.11479281 0.07377186]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.16596177e-05 1.01081200e-04 1.12186639e-05 4.48354160e-07
 1.56066329e-04 1.71096574e-05 8.86629391e-07 3.16162585e-05
 1.47648345e-05 9.00786229e-06]
ene_total = [0.49728187 0.41773503 0.50837179 0.45733327 0.42429695 0.4276389
 0.45554967 0.43173622 0.65949319 0.42379718]
optimize_network iter = 0 obj = 4.703234079287297
eta = 0.6819493529625255
freqs = [38424917.91998439 78013241.65854445 37994067.71862566 12898298.99218661
 90209018.95065838 43241454.64083756 16185653.82662158 53086584.80672698
 42569513.0112679  34900040.52352524]
eta_min = 0.6819493529625276	eta_max = 0.6819493529625246
af = 0.016869127524841664	bf = 1.4894091419856037	zeta = 0.018556040277325832	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [2.90303184e-06 2.51673727e-05 2.79324241e-06 1.11631997e-07
 3.88576653e-05 4.25999219e-06 2.20754524e-07 7.87187088e-06
 3.67617412e-06 2.24279318e-06]
ene_total = [1.73801127 1.44623117 1.77687644 1.59993898 1.46089904 1.49353974
 1.59363288 1.50568747 2.3050487  1.48132066]
ti_comp = [0.41103184 0.42578996 0.40909461 0.41788221 0.42519616 0.42322413
 0.41819745 0.42265509 0.38279172 0.42381267]
ti_coms = [0.08655269 0.07179457 0.08848992 0.07970232 0.07238837 0.0743604
 0.07938708 0.07492943 0.11479281 0.07377186]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.16596177e-05 1.01081200e-04 1.12186639e-05 4.48354160e-07
 1.56066329e-04 1.71096574e-05 8.86629391e-07 3.16162585e-05
 1.47648345e-05 9.00786229e-06]
ene_total = [0.49728187 0.41773503 0.50837179 0.45733327 0.42429695 0.4276389
 0.45554967 0.43173622 0.65949319 0.42379718]
optimize_network iter = 1 obj = 4.703234079287284
eta = 0.6819493529625246
freqs = [38424917.91998437 78013241.65854444 37994067.71862565 12898298.99218661
 90209018.95065837 43241454.64083755 16185653.82662157 53086584.80672697
 42569513.01126787 34900040.52352523]
Done!
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.11621458e-05 9.67684474e-05 1.07400059e-05 4.29224582e-07
 1.49407568e-04 1.63796530e-05 8.48800263e-07 3.02673121e-05
 1.41348748e-05 8.62353083e-06]
ene_total = [0.00866643 0.00727623 0.00885973 0.00797066 0.00738824 0.00745242
 0.00793956 0.00752321 0.01149342 0.00738581]
At round 29 energy consumption: 0.08195570742412175
At round 29 eta: 0.6819493529625246
At round 29 a_n: 18.248773294144172
At round 29 local rounds: 12.534813137868813
At round 29 global rounds: 57.376941264307334
gradient difference: 0.44853439927101135
train() client id: f_00000-0-0 loss: 0.837187  [   32/  126]
train() client id: f_00000-0-1 loss: 1.240156  [   64/  126]
train() client id: f_00000-0-2 loss: 0.877112  [   96/  126]
train() client id: f_00000-1-0 loss: 0.981036  [   32/  126]
train() client id: f_00000-1-1 loss: 0.880201  [   64/  126]
train() client id: f_00000-1-2 loss: 1.045353  [   96/  126]
train() client id: f_00000-2-0 loss: 0.909136  [   32/  126]
train() client id: f_00000-2-1 loss: 0.804193  [   64/  126]
train() client id: f_00000-2-2 loss: 0.914184  [   96/  126]
train() client id: f_00000-3-0 loss: 0.886945  [   32/  126]
train() client id: f_00000-3-1 loss: 0.916983  [   64/  126]
train() client id: f_00000-3-2 loss: 0.837441  [   96/  126]
train() client id: f_00000-4-0 loss: 0.817448  [   32/  126]
train() client id: f_00000-4-1 loss: 0.856390  [   64/  126]
train() client id: f_00000-4-2 loss: 0.803437  [   96/  126]
train() client id: f_00000-5-0 loss: 0.743028  [   32/  126]
train() client id: f_00000-5-1 loss: 0.746694  [   64/  126]
train() client id: f_00000-5-2 loss: 0.782326  [   96/  126]
train() client id: f_00000-6-0 loss: 0.893126  [   32/  126]
train() client id: f_00000-6-1 loss: 0.913045  [   64/  126]
train() client id: f_00000-6-2 loss: 0.696338  [   96/  126]
train() client id: f_00000-7-0 loss: 0.736962  [   32/  126]
train() client id: f_00000-7-1 loss: 0.835123  [   64/  126]
train() client id: f_00000-7-2 loss: 0.824124  [   96/  126]
train() client id: f_00000-8-0 loss: 0.739309  [   32/  126]
train() client id: f_00000-8-1 loss: 0.727154  [   64/  126]
train() client id: f_00000-8-2 loss: 0.895233  [   96/  126]
train() client id: f_00000-9-0 loss: 0.730146  [   32/  126]
train() client id: f_00000-9-1 loss: 0.737606  [   64/  126]
train() client id: f_00000-9-2 loss: 0.714038  [   96/  126]
train() client id: f_00000-10-0 loss: 0.694479  [   32/  126]
train() client id: f_00000-10-1 loss: 0.878431  [   64/  126]
train() client id: f_00000-10-2 loss: 0.786332  [   96/  126]
train() client id: f_00000-11-0 loss: 0.763369  [   32/  126]
train() client id: f_00000-11-1 loss: 0.785904  [   64/  126]
train() client id: f_00000-11-2 loss: 0.730326  [   96/  126]
train() client id: f_00001-0-0 loss: 0.492872  [   32/  265]
train() client id: f_00001-0-1 loss: 0.392170  [   64/  265]
train() client id: f_00001-0-2 loss: 0.490667  [   96/  265]
train() client id: f_00001-0-3 loss: 0.327427  [  128/  265]
train() client id: f_00001-0-4 loss: 0.428552  [  160/  265]
train() client id: f_00001-0-5 loss: 0.348916  [  192/  265]
train() client id: f_00001-0-6 loss: 0.511679  [  224/  265]
train() client id: f_00001-0-7 loss: 0.589286  [  256/  265]
train() client id: f_00001-1-0 loss: 0.434473  [   32/  265]
train() client id: f_00001-1-1 loss: 0.405806  [   64/  265]
train() client id: f_00001-1-2 loss: 0.352906  [   96/  265]
train() client id: f_00001-1-3 loss: 0.458888  [  128/  265]
train() client id: f_00001-1-4 loss: 0.460018  [  160/  265]
train() client id: f_00001-1-5 loss: 0.522811  [  192/  265]
train() client id: f_00001-1-6 loss: 0.462381  [  224/  265]
train() client id: f_00001-1-7 loss: 0.419156  [  256/  265]
train() client id: f_00001-2-0 loss: 0.474527  [   32/  265]
train() client id: f_00001-2-1 loss: 0.390732  [   64/  265]
train() client id: f_00001-2-2 loss: 0.462944  [   96/  265]
train() client id: f_00001-2-3 loss: 0.318724  [  128/  265]
train() client id: f_00001-2-4 loss: 0.588489  [  160/  265]
train() client id: f_00001-2-5 loss: 0.373083  [  192/  265]
train() client id: f_00001-2-6 loss: 0.423358  [  224/  265]
train() client id: f_00001-2-7 loss: 0.436782  [  256/  265]
train() client id: f_00001-3-0 loss: 0.406263  [   32/  265]
train() client id: f_00001-3-1 loss: 0.333489  [   64/  265]
train() client id: f_00001-3-2 loss: 0.461008  [   96/  265]
train() client id: f_00001-3-3 loss: 0.460943  [  128/  265]
train() client id: f_00001-3-4 loss: 0.336226  [  160/  265]
train() client id: f_00001-3-5 loss: 0.448331  [  192/  265]
train() client id: f_00001-3-6 loss: 0.445698  [  224/  265]
train() client id: f_00001-3-7 loss: 0.471547  [  256/  265]
train() client id: f_00001-4-0 loss: 0.446588  [   32/  265]
train() client id: f_00001-4-1 loss: 0.356886  [   64/  265]
train() client id: f_00001-4-2 loss: 0.427454  [   96/  265]
train() client id: f_00001-4-3 loss: 0.499056  [  128/  265]
train() client id: f_00001-4-4 loss: 0.320242  [  160/  265]
train() client id: f_00001-4-5 loss: 0.415130  [  192/  265]
train() client id: f_00001-4-6 loss: 0.364008  [  224/  265]
train() client id: f_00001-4-7 loss: 0.519424  [  256/  265]
train() client id: f_00001-5-0 loss: 0.461814  [   32/  265]
train() client id: f_00001-5-1 loss: 0.320351  [   64/  265]
train() client id: f_00001-5-2 loss: 0.388939  [   96/  265]
train() client id: f_00001-5-3 loss: 0.578989  [  128/  265]
train() client id: f_00001-5-4 loss: 0.330460  [  160/  265]
train() client id: f_00001-5-5 loss: 0.470704  [  192/  265]
train() client id: f_00001-5-6 loss: 0.359678  [  224/  265]
train() client id: f_00001-5-7 loss: 0.373641  [  256/  265]
train() client id: f_00001-6-0 loss: 0.428995  [   32/  265]
train() client id: f_00001-6-1 loss: 0.384673  [   64/  265]
train() client id: f_00001-6-2 loss: 0.422943  [   96/  265]
train() client id: f_00001-6-3 loss: 0.429833  [  128/  265]
train() client id: f_00001-6-4 loss: 0.375313  [  160/  265]
train() client id: f_00001-6-5 loss: 0.376112  [  192/  265]
train() client id: f_00001-6-6 loss: 0.361335  [  224/  265]
train() client id: f_00001-6-7 loss: 0.473973  [  256/  265]
train() client id: f_00001-7-0 loss: 0.424087  [   32/  265]
train() client id: f_00001-7-1 loss: 0.418971  [   64/  265]
train() client id: f_00001-7-2 loss: 0.616283  [   96/  265]
train() client id: f_00001-7-3 loss: 0.353453  [  128/  265]
train() client id: f_00001-7-4 loss: 0.444118  [  160/  265]
train() client id: f_00001-7-5 loss: 0.402346  [  192/  265]
train() client id: f_00001-7-6 loss: 0.313383  [  224/  265]
train() client id: f_00001-7-7 loss: 0.329751  [  256/  265]
train() client id: f_00001-8-0 loss: 0.367199  [   32/  265]
train() client id: f_00001-8-1 loss: 0.434289  [   64/  265]
train() client id: f_00001-8-2 loss: 0.312729  [   96/  265]
train() client id: f_00001-8-3 loss: 0.406763  [  128/  265]
train() client id: f_00001-8-4 loss: 0.329274  [  160/  265]
train() client id: f_00001-8-5 loss: 0.485850  [  192/  265]
train() client id: f_00001-8-6 loss: 0.470546  [  224/  265]
train() client id: f_00001-8-7 loss: 0.421490  [  256/  265]
train() client id: f_00001-9-0 loss: 0.457495  [   32/  265]
train() client id: f_00001-9-1 loss: 0.377620  [   64/  265]
train() client id: f_00001-9-2 loss: 0.446423  [   96/  265]
train() client id: f_00001-9-3 loss: 0.380322  [  128/  265]
train() client id: f_00001-9-4 loss: 0.456445  [  160/  265]
train() client id: f_00001-9-5 loss: 0.408000  [  192/  265]
train() client id: f_00001-9-6 loss: 0.364895  [  224/  265]
train() client id: f_00001-9-7 loss: 0.366247  [  256/  265]
train() client id: f_00001-10-0 loss: 0.403378  [   32/  265]
train() client id: f_00001-10-1 loss: 0.480994  [   64/  265]
train() client id: f_00001-10-2 loss: 0.403380  [   96/  265]
train() client id: f_00001-10-3 loss: 0.550130  [  128/  265]
train() client id: f_00001-10-4 loss: 0.415115  [  160/  265]
train() client id: f_00001-10-5 loss: 0.299190  [  192/  265]
train() client id: f_00001-10-6 loss: 0.366624  [  224/  265]
train() client id: f_00001-10-7 loss: 0.299154  [  256/  265]
train() client id: f_00001-11-0 loss: 0.292540  [   32/  265]
train() client id: f_00001-11-1 loss: 0.452417  [   64/  265]
train() client id: f_00001-11-2 loss: 0.526643  [   96/  265]
train() client id: f_00001-11-3 loss: 0.436255  [  128/  265]
train() client id: f_00001-11-4 loss: 0.380006  [  160/  265]
train() client id: f_00001-11-5 loss: 0.430337  [  192/  265]
train() client id: f_00001-11-6 loss: 0.394983  [  224/  265]
train() client id: f_00001-11-7 loss: 0.330433  [  256/  265]
train() client id: f_00002-0-0 loss: 1.367060  [   32/  124]
train() client id: f_00002-0-1 loss: 1.224934  [   64/  124]
train() client id: f_00002-0-2 loss: 1.149839  [   96/  124]
train() client id: f_00002-1-0 loss: 1.121191  [   32/  124]
train() client id: f_00002-1-1 loss: 1.209375  [   64/  124]
train() client id: f_00002-1-2 loss: 1.234734  [   96/  124]
train() client id: f_00002-2-0 loss: 1.135437  [   32/  124]
train() client id: f_00002-2-1 loss: 1.120107  [   64/  124]
train() client id: f_00002-2-2 loss: 1.260618  [   96/  124]
train() client id: f_00002-3-0 loss: 1.135456  [   32/  124]
train() client id: f_00002-3-1 loss: 1.064535  [   64/  124]
train() client id: f_00002-3-2 loss: 1.119714  [   96/  124]
train() client id: f_00002-4-0 loss: 1.106563  [   32/  124]
train() client id: f_00002-4-1 loss: 1.177888  [   64/  124]
train() client id: f_00002-4-2 loss: 1.124671  [   96/  124]
train() client id: f_00002-5-0 loss: 0.988158  [   32/  124]
train() client id: f_00002-5-1 loss: 1.073172  [   64/  124]
train() client id: f_00002-5-2 loss: 1.049945  [   96/  124]
train() client id: f_00002-6-0 loss: 1.072261  [   32/  124]
train() client id: f_00002-6-1 loss: 0.949287  [   64/  124]
train() client id: f_00002-6-2 loss: 1.028125  [   96/  124]
train() client id: f_00002-7-0 loss: 1.017970  [   32/  124]
train() client id: f_00002-7-1 loss: 1.134307  [   64/  124]
train() client id: f_00002-7-2 loss: 0.968859  [   96/  124]
train() client id: f_00002-8-0 loss: 1.041705  [   32/  124]
train() client id: f_00002-8-1 loss: 1.219860  [   64/  124]
train() client id: f_00002-8-2 loss: 0.842266  [   96/  124]
train() client id: f_00002-9-0 loss: 0.998733  [   32/  124]
train() client id: f_00002-9-1 loss: 0.861218  [   64/  124]
train() client id: f_00002-9-2 loss: 1.012163  [   96/  124]
train() client id: f_00002-10-0 loss: 0.996486  [   32/  124]
train() client id: f_00002-10-1 loss: 1.026982  [   64/  124]
train() client id: f_00002-10-2 loss: 1.002600  [   96/  124]
train() client id: f_00002-11-0 loss: 0.940688  [   32/  124]
train() client id: f_00002-11-1 loss: 1.118670  [   64/  124]
train() client id: f_00002-11-2 loss: 0.850119  [   96/  124]
train() client id: f_00003-0-0 loss: 0.662270  [   32/   43]
train() client id: f_00003-1-0 loss: 0.910265  [   32/   43]
train() client id: f_00003-2-0 loss: 1.003861  [   32/   43]
train() client id: f_00003-3-0 loss: 0.693749  [   32/   43]
train() client id: f_00003-4-0 loss: 0.721304  [   32/   43]
train() client id: f_00003-5-0 loss: 0.957682  [   32/   43]
train() client id: f_00003-6-0 loss: 0.894251  [   32/   43]
train() client id: f_00003-7-0 loss: 0.871788  [   32/   43]
train() client id: f_00003-8-0 loss: 0.753968  [   32/   43]
train() client id: f_00003-9-0 loss: 0.885146  [   32/   43]
train() client id: f_00003-10-0 loss: 0.833948  [   32/   43]
train() client id: f_00003-11-0 loss: 0.634545  [   32/   43]
train() client id: f_00004-0-0 loss: 0.803684  [   32/  306]
train() client id: f_00004-0-1 loss: 1.081349  [   64/  306]
train() client id: f_00004-0-2 loss: 0.704602  [   96/  306]
train() client id: f_00004-0-3 loss: 0.838661  [  128/  306]
train() client id: f_00004-0-4 loss: 0.776921  [  160/  306]
train() client id: f_00004-0-5 loss: 0.954469  [  192/  306]
train() client id: f_00004-0-6 loss: 0.905347  [  224/  306]
train() client id: f_00004-0-7 loss: 0.893910  [  256/  306]
train() client id: f_00004-0-8 loss: 0.786395  [  288/  306]
train() client id: f_00004-1-0 loss: 0.756500  [   32/  306]
train() client id: f_00004-1-1 loss: 0.853456  [   64/  306]
train() client id: f_00004-1-2 loss: 0.825571  [   96/  306]
train() client id: f_00004-1-3 loss: 0.799877  [  128/  306]
train() client id: f_00004-1-4 loss: 0.845783  [  160/  306]
train() client id: f_00004-1-5 loss: 0.777588  [  192/  306]
train() client id: f_00004-1-6 loss: 0.896297  [  224/  306]
train() client id: f_00004-1-7 loss: 1.035098  [  256/  306]
train() client id: f_00004-1-8 loss: 0.948843  [  288/  306]
train() client id: f_00004-2-0 loss: 0.945549  [   32/  306]
train() client id: f_00004-2-1 loss: 0.858235  [   64/  306]
train() client id: f_00004-2-2 loss: 0.975281  [   96/  306]
train() client id: f_00004-2-3 loss: 0.819385  [  128/  306]
train() client id: f_00004-2-4 loss: 0.801298  [  160/  306]
train() client id: f_00004-2-5 loss: 0.811084  [  192/  306]
train() client id: f_00004-2-6 loss: 0.774628  [  224/  306]
train() client id: f_00004-2-7 loss: 0.742339  [  256/  306]
train() client id: f_00004-2-8 loss: 1.018795  [  288/  306]
train() client id: f_00004-3-0 loss: 0.888891  [   32/  306]
train() client id: f_00004-3-1 loss: 0.929266  [   64/  306]
train() client id: f_00004-3-2 loss: 0.853872  [   96/  306]
train() client id: f_00004-3-3 loss: 0.924087  [  128/  306]
train() client id: f_00004-3-4 loss: 0.797927  [  160/  306]
train() client id: f_00004-3-5 loss: 0.848610  [  192/  306]
train() client id: f_00004-3-6 loss: 0.868846  [  224/  306]
train() client id: f_00004-3-7 loss: 0.922470  [  256/  306]
train() client id: f_00004-3-8 loss: 0.822108  [  288/  306]
train() client id: f_00004-4-0 loss: 0.741569  [   32/  306]
train() client id: f_00004-4-1 loss: 0.866967  [   64/  306]
train() client id: f_00004-4-2 loss: 0.858137  [   96/  306]
train() client id: f_00004-4-3 loss: 0.941546  [  128/  306]
train() client id: f_00004-4-4 loss: 0.848523  [  160/  306]
train() client id: f_00004-4-5 loss: 0.987276  [  192/  306]
train() client id: f_00004-4-6 loss: 0.869035  [  224/  306]
train() client id: f_00004-4-7 loss: 0.859915  [  256/  306]
train() client id: f_00004-4-8 loss: 0.881336  [  288/  306]
train() client id: f_00004-5-0 loss: 0.918775  [   32/  306]
train() client id: f_00004-5-1 loss: 0.885048  [   64/  306]
train() client id: f_00004-5-2 loss: 0.799910  [   96/  306]
train() client id: f_00004-5-3 loss: 0.717120  [  128/  306]
train() client id: f_00004-5-4 loss: 0.862298  [  160/  306]
train() client id: f_00004-5-5 loss: 0.913194  [  192/  306]
train() client id: f_00004-5-6 loss: 0.954502  [  224/  306]
train() client id: f_00004-5-7 loss: 0.921211  [  256/  306]
train() client id: f_00004-5-8 loss: 0.979954  [  288/  306]
train() client id: f_00004-6-0 loss: 0.848296  [   32/  306]
train() client id: f_00004-6-1 loss: 0.874393  [   64/  306]
train() client id: f_00004-6-2 loss: 1.017571  [   96/  306]
train() client id: f_00004-6-3 loss: 0.708990  [  128/  306]
train() client id: f_00004-6-4 loss: 0.948035  [  160/  306]
train() client id: f_00004-6-5 loss: 0.779783  [  192/  306]
train() client id: f_00004-6-6 loss: 0.822106  [  224/  306]
train() client id: f_00004-6-7 loss: 0.953894  [  256/  306]
train() client id: f_00004-6-8 loss: 0.908381  [  288/  306]
train() client id: f_00004-7-0 loss: 0.847691  [   32/  306]
train() client id: f_00004-7-1 loss: 0.806539  [   64/  306]
train() client id: f_00004-7-2 loss: 0.741756  [   96/  306]
train() client id: f_00004-7-3 loss: 1.006419  [  128/  306]
train() client id: f_00004-7-4 loss: 0.825342  [  160/  306]
train() client id: f_00004-7-5 loss: 0.933358  [  192/  306]
train() client id: f_00004-7-6 loss: 0.912479  [  224/  306]
train() client id: f_00004-7-7 loss: 0.802546  [  256/  306]
train() client id: f_00004-7-8 loss: 1.009080  [  288/  306]
train() client id: f_00004-8-0 loss: 0.861325  [   32/  306]
train() client id: f_00004-8-1 loss: 0.889699  [   64/  306]
train() client id: f_00004-8-2 loss: 0.878778  [   96/  306]
train() client id: f_00004-8-3 loss: 0.903075  [  128/  306]
train() client id: f_00004-8-4 loss: 0.837140  [  160/  306]
train() client id: f_00004-8-5 loss: 0.845717  [  192/  306]
train() client id: f_00004-8-6 loss: 0.820815  [  224/  306]
train() client id: f_00004-8-7 loss: 0.925229  [  256/  306]
train() client id: f_00004-8-8 loss: 0.895923  [  288/  306]
train() client id: f_00004-9-0 loss: 0.929627  [   32/  306]
train() client id: f_00004-9-1 loss: 0.876856  [   64/  306]
train() client id: f_00004-9-2 loss: 0.826851  [   96/  306]
train() client id: f_00004-9-3 loss: 1.053629  [  128/  306]
train() client id: f_00004-9-4 loss: 0.795706  [  160/  306]
train() client id: f_00004-9-5 loss: 0.878532  [  192/  306]
train() client id: f_00004-9-6 loss: 0.807130  [  224/  306]
train() client id: f_00004-9-7 loss: 0.773522  [  256/  306]
train() client id: f_00004-9-8 loss: 0.949142  [  288/  306]
train() client id: f_00004-10-0 loss: 0.796861  [   32/  306]
train() client id: f_00004-10-1 loss: 1.025318  [   64/  306]
train() client id: f_00004-10-2 loss: 0.972729  [   96/  306]
train() client id: f_00004-10-3 loss: 0.823712  [  128/  306]
train() client id: f_00004-10-4 loss: 0.851228  [  160/  306]
train() client id: f_00004-10-5 loss: 0.782343  [  192/  306]
train() client id: f_00004-10-6 loss: 0.841381  [  224/  306]
train() client id: f_00004-10-7 loss: 0.954886  [  256/  306]
train() client id: f_00004-10-8 loss: 0.874568  [  288/  306]
train() client id: f_00004-11-0 loss: 0.862875  [   32/  306]
train() client id: f_00004-11-1 loss: 0.863219  [   64/  306]
train() client id: f_00004-11-2 loss: 0.926151  [   96/  306]
train() client id: f_00004-11-3 loss: 0.950802  [  128/  306]
train() client id: f_00004-11-4 loss: 0.863592  [  160/  306]
train() client id: f_00004-11-5 loss: 0.838797  [  192/  306]
train() client id: f_00004-11-6 loss: 0.835003  [  224/  306]
train() client id: f_00004-11-7 loss: 0.872939  [  256/  306]
train() client id: f_00004-11-8 loss: 0.858017  [  288/  306]
train() client id: f_00005-0-0 loss: 0.815972  [   32/  146]
train() client id: f_00005-0-1 loss: 0.579418  [   64/  146]
train() client id: f_00005-0-2 loss: 0.756434  [   96/  146]
train() client id: f_00005-0-3 loss: 0.589068  [  128/  146]
train() client id: f_00005-1-0 loss: 0.524500  [   32/  146]
train() client id: f_00005-1-1 loss: 0.547270  [   64/  146]
train() client id: f_00005-1-2 loss: 0.661566  [   96/  146]
train() client id: f_00005-1-3 loss: 0.637092  [  128/  146]
train() client id: f_00005-2-0 loss: 0.482572  [   32/  146]
train() client id: f_00005-2-1 loss: 0.733546  [   64/  146]
train() client id: f_00005-2-2 loss: 0.599825  [   96/  146]
train() client id: f_00005-2-3 loss: 0.695151  [  128/  146]
train() client id: f_00005-3-0 loss: 0.692186  [   32/  146]
train() client id: f_00005-3-1 loss: 0.682493  [   64/  146]
train() client id: f_00005-3-2 loss: 0.398721  [   96/  146]
train() client id: f_00005-3-3 loss: 0.726924  [  128/  146]
train() client id: f_00005-4-0 loss: 0.510196  [   32/  146]
train() client id: f_00005-4-1 loss: 0.503769  [   64/  146]
train() client id: f_00005-4-2 loss: 0.552670  [   96/  146]
train() client id: f_00005-4-3 loss: 0.604716  [  128/  146]
train() client id: f_00005-5-0 loss: 0.721241  [   32/  146]
train() client id: f_00005-5-1 loss: 0.602686  [   64/  146]
train() client id: f_00005-5-2 loss: 0.794779  [   96/  146]
train() client id: f_00005-5-3 loss: 0.375652  [  128/  146]
train() client id: f_00005-6-0 loss: 0.858682  [   32/  146]
train() client id: f_00005-6-1 loss: 0.323895  [   64/  146]
train() client id: f_00005-6-2 loss: 0.412377  [   96/  146]
train() client id: f_00005-6-3 loss: 0.652509  [  128/  146]
train() client id: f_00005-7-0 loss: 0.537469  [   32/  146]
train() client id: f_00005-7-1 loss: 0.791090  [   64/  146]
train() client id: f_00005-7-2 loss: 0.723573  [   96/  146]
train() client id: f_00005-7-3 loss: 0.385389  [  128/  146]
train() client id: f_00005-8-0 loss: 0.801998  [   32/  146]
train() client id: f_00005-8-1 loss: 0.343083  [   64/  146]
train() client id: f_00005-8-2 loss: 0.655701  [   96/  146]
train() client id: f_00005-8-3 loss: 0.659839  [  128/  146]
train() client id: f_00005-9-0 loss: 0.614107  [   32/  146]
train() client id: f_00005-9-1 loss: 0.484833  [   64/  146]
train() client id: f_00005-9-2 loss: 0.675554  [   96/  146]
train() client id: f_00005-9-3 loss: 0.750084  [  128/  146]
train() client id: f_00005-10-0 loss: 0.527819  [   32/  146]
train() client id: f_00005-10-1 loss: 0.419021  [   64/  146]
train() client id: f_00005-10-2 loss: 0.988264  [   96/  146]
train() client id: f_00005-10-3 loss: 0.704226  [  128/  146]
train() client id: f_00005-11-0 loss: 0.452179  [   32/  146]
train() client id: f_00005-11-1 loss: 0.671359  [   64/  146]
train() client id: f_00005-11-2 loss: 0.539281  [   96/  146]
train() client id: f_00005-11-3 loss: 0.658160  [  128/  146]
train() client id: f_00006-0-0 loss: 0.521174  [   32/   54]
train() client id: f_00006-1-0 loss: 0.536454  [   32/   54]
train() client id: f_00006-2-0 loss: 0.535192  [   32/   54]
train() client id: f_00006-3-0 loss: 0.536244  [   32/   54]
train() client id: f_00006-4-0 loss: 0.570991  [   32/   54]
train() client id: f_00006-5-0 loss: 0.509938  [   32/   54]
train() client id: f_00006-6-0 loss: 0.581509  [   32/   54]
train() client id: f_00006-7-0 loss: 0.501034  [   32/   54]
train() client id: f_00006-8-0 loss: 0.520272  [   32/   54]
train() client id: f_00006-9-0 loss: 0.469131  [   32/   54]
train() client id: f_00006-10-0 loss: 0.510685  [   32/   54]
train() client id: f_00006-11-0 loss: 0.456173  [   32/   54]
train() client id: f_00007-0-0 loss: 0.549994  [   32/  179]
train() client id: f_00007-0-1 loss: 0.715169  [   64/  179]
train() client id: f_00007-0-2 loss: 0.639151  [   96/  179]
train() client id: f_00007-0-3 loss: 0.465113  [  128/  179]
train() client id: f_00007-0-4 loss: 0.444554  [  160/  179]
train() client id: f_00007-1-0 loss: 0.641158  [   32/  179]
train() client id: f_00007-1-1 loss: 0.639265  [   64/  179]
train() client id: f_00007-1-2 loss: 0.722320  [   96/  179]
train() client id: f_00007-1-3 loss: 0.604391  [  128/  179]
train() client id: f_00007-1-4 loss: 0.522401  [  160/  179]
train() client id: f_00007-2-0 loss: 0.715748  [   32/  179]
train() client id: f_00007-2-1 loss: 0.447507  [   64/  179]
train() client id: f_00007-2-2 loss: 0.613387  [   96/  179]
train() client id: f_00007-2-3 loss: 0.728347  [  128/  179]
train() client id: f_00007-2-4 loss: 0.562311  [  160/  179]
train() client id: f_00007-3-0 loss: 0.642498  [   32/  179]
train() client id: f_00007-3-1 loss: 0.619127  [   64/  179]
train() client id: f_00007-3-2 loss: 0.548045  [   96/  179]
train() client id: f_00007-3-3 loss: 0.842876  [  128/  179]
train() client id: f_00007-3-4 loss: 0.480277  [  160/  179]
train() client id: f_00007-4-0 loss: 0.658846  [   32/  179]
train() client id: f_00007-4-1 loss: 0.494696  [   64/  179]
train() client id: f_00007-4-2 loss: 0.475478  [   96/  179]
train() client id: f_00007-4-3 loss: 0.925215  [  128/  179]
train() client id: f_00007-4-4 loss: 0.407201  [  160/  179]
train() client id: f_00007-5-0 loss: 0.466496  [   32/  179]
train() client id: f_00007-5-1 loss: 0.916811  [   64/  179]
train() client id: f_00007-5-2 loss: 0.454855  [   96/  179]
train() client id: f_00007-5-3 loss: 0.665644  [  128/  179]
train() client id: f_00007-5-4 loss: 0.543199  [  160/  179]
train() client id: f_00007-6-0 loss: 0.629078  [   32/  179]
train() client id: f_00007-6-1 loss: 0.432125  [   64/  179]
train() client id: f_00007-6-2 loss: 0.588538  [   96/  179]
train() client id: f_00007-6-3 loss: 0.781809  [  128/  179]
train() client id: f_00007-6-4 loss: 0.485197  [  160/  179]
train() client id: f_00007-7-0 loss: 0.594120  [   32/  179]
train() client id: f_00007-7-1 loss: 0.708411  [   64/  179]
train() client id: f_00007-7-2 loss: 0.602793  [   96/  179]
train() client id: f_00007-7-3 loss: 0.644075  [  128/  179]
train() client id: f_00007-7-4 loss: 0.484542  [  160/  179]
train() client id: f_00007-8-0 loss: 0.747515  [   32/  179]
train() client id: f_00007-8-1 loss: 0.645937  [   64/  179]
train() client id: f_00007-8-2 loss: 0.574625  [   96/  179]
train() client id: f_00007-8-3 loss: 0.535906  [  128/  179]
train() client id: f_00007-8-4 loss: 0.500847  [  160/  179]
train() client id: f_00007-9-0 loss: 0.470951  [   32/  179]
train() client id: f_00007-9-1 loss: 0.800601  [   64/  179]
train() client id: f_00007-9-2 loss: 0.433368  [   96/  179]
train() client id: f_00007-9-3 loss: 0.723412  [  128/  179]
train() client id: f_00007-9-4 loss: 0.622717  [  160/  179]
train() client id: f_00007-10-0 loss: 0.693943  [   32/  179]
train() client id: f_00007-10-1 loss: 0.520347  [   64/  179]
train() client id: f_00007-10-2 loss: 0.651511  [   96/  179]
train() client id: f_00007-10-3 loss: 0.420854  [  128/  179]
train() client id: f_00007-10-4 loss: 0.665028  [  160/  179]
train() client id: f_00007-11-0 loss: 0.628281  [   32/  179]
train() client id: f_00007-11-1 loss: 0.425327  [   64/  179]
train() client id: f_00007-11-2 loss: 0.660344  [   96/  179]
train() client id: f_00007-11-3 loss: 0.563186  [  128/  179]
train() client id: f_00007-11-4 loss: 0.688120  [  160/  179]
train() client id: f_00008-0-0 loss: 0.805052  [   32/  130]
train() client id: f_00008-0-1 loss: 0.675121  [   64/  130]
train() client id: f_00008-0-2 loss: 0.793744  [   96/  130]
train() client id: f_00008-0-3 loss: 0.669245  [  128/  130]
train() client id: f_00008-1-0 loss: 0.731049  [   32/  130]
train() client id: f_00008-1-1 loss: 0.626735  [   64/  130]
train() client id: f_00008-1-2 loss: 0.800623  [   96/  130]
train() client id: f_00008-1-3 loss: 0.767948  [  128/  130]
train() client id: f_00008-2-0 loss: 0.726545  [   32/  130]
train() client id: f_00008-2-1 loss: 0.769748  [   64/  130]
train() client id: f_00008-2-2 loss: 0.687095  [   96/  130]
train() client id: f_00008-2-3 loss: 0.748368  [  128/  130]
train() client id: f_00008-3-0 loss: 0.667299  [   32/  130]
train() client id: f_00008-3-1 loss: 0.877840  [   64/  130]
train() client id: f_00008-3-2 loss: 0.621795  [   96/  130]
train() client id: f_00008-3-3 loss: 0.740101  [  128/  130]
train() client id: f_00008-4-0 loss: 0.749436  [   32/  130]
train() client id: f_00008-4-1 loss: 0.875428  [   64/  130]
train() client id: f_00008-4-2 loss: 0.658136  [   96/  130]
train() client id: f_00008-4-3 loss: 0.641767  [  128/  130]
train() client id: f_00008-5-0 loss: 0.654461  [   32/  130]
train() client id: f_00008-5-1 loss: 0.695553  [   64/  130]
train() client id: f_00008-5-2 loss: 0.722020  [   96/  130]
train() client id: f_00008-5-3 loss: 0.855681  [  128/  130]
train() client id: f_00008-6-0 loss: 0.614027  [   32/  130]
train() client id: f_00008-6-1 loss: 0.769804  [   64/  130]
train() client id: f_00008-6-2 loss: 0.845252  [   96/  130]
train() client id: f_00008-6-3 loss: 0.695944  [  128/  130]
train() client id: f_00008-7-0 loss: 0.796365  [   32/  130]
train() client id: f_00008-7-1 loss: 0.757707  [   64/  130]
train() client id: f_00008-7-2 loss: 0.697899  [   96/  130]
train() client id: f_00008-7-3 loss: 0.647660  [  128/  130]
train() client id: f_00008-8-0 loss: 0.729444  [   32/  130]
train() client id: f_00008-8-1 loss: 0.707653  [   64/  130]
train() client id: f_00008-8-2 loss: 0.750003  [   96/  130]
train() client id: f_00008-8-3 loss: 0.743723  [  128/  130]
train() client id: f_00008-9-0 loss: 0.770228  [   32/  130]
train() client id: f_00008-9-1 loss: 0.742587  [   64/  130]
train() client id: f_00008-9-2 loss: 0.715350  [   96/  130]
train() client id: f_00008-9-3 loss: 0.692464  [  128/  130]
train() client id: f_00008-10-0 loss: 0.690619  [   32/  130]
train() client id: f_00008-10-1 loss: 0.856324  [   64/  130]
train() client id: f_00008-10-2 loss: 0.647110  [   96/  130]
train() client id: f_00008-10-3 loss: 0.702506  [  128/  130]
train() client id: f_00008-11-0 loss: 0.597744  [   32/  130]
train() client id: f_00008-11-1 loss: 0.843846  [   64/  130]
train() client id: f_00008-11-2 loss: 0.715551  [   96/  130]
train() client id: f_00008-11-3 loss: 0.743464  [  128/  130]
train() client id: f_00009-0-0 loss: 1.024825  [   32/  118]
train() client id: f_00009-0-1 loss: 1.007852  [   64/  118]
train() client id: f_00009-0-2 loss: 0.917535  [   96/  118]
train() client id: f_00009-1-0 loss: 0.887854  [   32/  118]
train() client id: f_00009-1-1 loss: 0.892487  [   64/  118]
train() client id: f_00009-1-2 loss: 0.985297  [   96/  118]
train() client id: f_00009-2-0 loss: 0.743370  [   32/  118]
train() client id: f_00009-2-1 loss: 1.072747  [   64/  118]
train() client id: f_00009-2-2 loss: 0.831955  [   96/  118]
train() client id: f_00009-3-0 loss: 0.991329  [   32/  118]
train() client id: f_00009-3-1 loss: 1.099726  [   64/  118]
train() client id: f_00009-3-2 loss: 0.770556  [   96/  118]
train() client id: f_00009-4-0 loss: 0.766776  [   32/  118]
train() client id: f_00009-4-1 loss: 0.808111  [   64/  118]
train() client id: f_00009-4-2 loss: 0.983470  [   96/  118]
train() client id: f_00009-5-0 loss: 0.827038  [   32/  118]
train() client id: f_00009-5-1 loss: 0.893185  [   64/  118]
train() client id: f_00009-5-2 loss: 0.795969  [   96/  118]
train() client id: f_00009-6-0 loss: 0.842043  [   32/  118]
train() client id: f_00009-6-1 loss: 0.869475  [   64/  118]
train() client id: f_00009-6-2 loss: 0.935551  [   96/  118]
train() client id: f_00009-7-0 loss: 0.770560  [   32/  118]
train() client id: f_00009-7-1 loss: 0.869873  [   64/  118]
train() client id: f_00009-7-2 loss: 0.891682  [   96/  118]
train() client id: f_00009-8-0 loss: 0.853239  [   32/  118]
train() client id: f_00009-8-1 loss: 0.806777  [   64/  118]
train() client id: f_00009-8-2 loss: 0.885489  [   96/  118]
train() client id: f_00009-9-0 loss: 0.852159  [   32/  118]
train() client id: f_00009-9-1 loss: 0.730150  [   64/  118]
train() client id: f_00009-9-2 loss: 0.981243  [   96/  118]
train() client id: f_00009-10-0 loss: 0.943001  [   32/  118]
train() client id: f_00009-10-1 loss: 0.810342  [   64/  118]
train() client id: f_00009-10-2 loss: 0.935211  [   96/  118]
train() client id: f_00009-11-0 loss: 0.837609  [   32/  118]
train() client id: f_00009-11-1 loss: 0.935403  [   64/  118]
train() client id: f_00009-11-2 loss: 0.794525  [   96/  118]
At round 29 accuracy: 0.6392572944297082
At round 29 training accuracy: 0.5902079141515761
At round 29 training loss: 0.8205690088149068
update_location
xs = -3.905658 4.200318 165.009024 18.811294 0.979296 3.956410 -127.443192 -106.324852 149.663977 -92.060879 
ys = 157.587959 140.555839 1.320614 -127.455176 119.350187 102.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 1.2885738318825701
ys mean: 41.39414253552871
dists_uav = 186.679456 172.550243 192.950051 163.091038 155.709428 143.479625 162.014375 145.964552 180.853465 135.982415 
uav_gains = -106.848255 -105.950259 -107.240184 -105.324237 -104.815300 -103.922273 -105.251230 -104.109353 -106.481738 -103.338285 
uav_gains_db_mean: -105.32811153934014
dists_bs = 171.978059 182.480558 381.799248 359.224096 184.568725 192.966820 183.881171 187.227483 360.820199 190.051022 
bs_gains = -102.160304 -102.881124 -111.858441 -111.117292 -103.019486 -103.560573 -102.974102 -103.193408 -111.171203 -103.375425 
bs_gains_db_mean: -105.53113589157076
Round 30
-------------------------------
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.03213939 14.58505974  6.92666737  2.4917993  16.82243501  8.09793447
  3.09128315  9.90171672  7.30136856  6.56936016]
obj_prev = 82.8197638713389
eta_min = 6.720783557496832e-14	eta_max = 0.9260472326332857
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 19.233152927962998	eta = 0.9090909090909091
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 34.518956732515555	eta = 0.5065241286245609
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 27.08219396594083	eta = 0.6456155103960739
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.74057952697607	eta = 0.6792653779081567
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.671393607462342	eta = 0.6810960381552402
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.671195205593733	eta = 0.6811013020600015
eta = 0.6811013020600015
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [0.03169041 0.06665046 0.03118739 0.01081498 0.07696242 0.03672063
 0.0135816  0.0450205  0.03269645 0.02967832]
ene_total = [2.2727313  4.14429542 2.25574611 1.06104318 4.72594168 2.47696442
 1.21464967 2.95433273 2.48764402 2.07784665]
ti_comp = [0.41970979 0.43599649 0.41770452 0.42674957 0.43552852 0.43363939
 0.42705945 0.43162177 0.39149563 0.43429663]
ti_coms = [0.08785512 0.07156842 0.08986039 0.08081534 0.07203639 0.07392552
 0.08050546 0.07594314 0.11606928 0.07326828]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.12918568e-05 9.73472227e-05 1.08662233e-05 4.34121306e-07
 1.50204523e-04 1.64570901e-05 8.58532087e-07 3.06128901e-05
 1.42537176e-05 8.66213655e-06]
ene_total = [0.49394235 0.40732423 0.50517807 0.45380416 0.41291985 0.4160174
 0.45208802 0.42814121 0.65253176 0.41188928]
optimize_network iter = 0 obj = 4.633836318316938
eta = 0.6811013020600015
freqs = [37752763.88864101 76434633.93405241 37331875.99521733 12671343.57363147
 88355204.53366868 42340057.1595814  15901303.6163569  52152723.73147339
 41758389.65718008 34168259.23522189]
eta_min = 0.6811013020600248	eta_max = 0.6811013020600005
af = 0.015898726867838657	bf = 1.4716185540757498	zeta = 0.017488599554622525	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [2.80235665e-06 2.41591478e-05 2.69672506e-06 1.07738059e-07
 3.72770088e-05 4.08423848e-06 2.13066208e-07 7.59735426e-06
 3.53741648e-06 2.14972581e-06]
ene_total = [1.73102112 1.4144311  1.77049778 1.59182881 1.42623245 1.45690442
 1.58574596 1.49733708 2.28689603 1.44357775]
ti_comp = [0.41970979 0.43599649 0.41770452 0.42674957 0.43552852 0.43363939
 0.42705945 0.43162177 0.39149563 0.43429663]
ti_coms = [0.08785512 0.07156842 0.08986039 0.08081534 0.07203639 0.07392552
 0.08050546 0.07594314 0.11606928 0.07326828]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.12918568e-05 9.73472227e-05 1.08662233e-05 4.34121306e-07
 1.50204523e-04 1.64570901e-05 8.58532087e-07 3.06128901e-05
 1.42537176e-05 8.66213655e-06]
ene_total = [0.49394235 0.40732423 0.50517807 0.45380416 0.41291985 0.4160174
 0.45208802 0.42814121 0.65253176 0.41188928]
optimize_network iter = 1 obj = 4.633836318316922
eta = 0.6811013020600005
freqs = [37752763.88864101 76434633.93405241 37331875.99521733 12671343.57363147
 88355204.53366868 42340057.1595814  15901303.6163569  52152723.73147339
 41758389.65718006 34168259.23522189]
Done!
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.07750501e-05 9.28918269e-05 1.03688971e-05 4.14252406e-07
 1.43329950e-04 1.57038807e-05 8.19238720e-07 2.92117968e-05
 1.36013522e-05 8.26568717e-06]
ene_total = [0.00879629 0.00724973 0.00899641 0.00808195 0.00734697 0.00740826
 0.00805137 0.00762353 0.01162053 0.00733509]
At round 30 energy consumption: 0.08251011673575459
At round 30 eta: 0.6811013020600005
At round 30 a_n: 17.906227447174857
At round 30 local rounds: 12.575559237281574
At round 30 global rounds: 56.15020557576531
gradient difference: 0.511399507522583
train() client id: f_00000-0-0 loss: 0.870146  [   32/  126]
train() client id: f_00000-0-1 loss: 1.190128  [   64/  126]
train() client id: f_00000-0-2 loss: 0.920841  [   96/  126]
train() client id: f_00000-1-0 loss: 0.969828  [   32/  126]
train() client id: f_00000-1-1 loss: 0.789283  [   64/  126]
train() client id: f_00000-1-2 loss: 0.984383  [   96/  126]
train() client id: f_00000-2-0 loss: 0.997669  [   32/  126]
train() client id: f_00000-2-1 loss: 0.838038  [   64/  126]
train() client id: f_00000-2-2 loss: 0.872679  [   96/  126]
train() client id: f_00000-3-0 loss: 0.874002  [   32/  126]
train() client id: f_00000-3-1 loss: 0.961742  [   64/  126]
train() client id: f_00000-3-2 loss: 0.776123  [   96/  126]
train() client id: f_00000-4-0 loss: 0.819666  [   32/  126]
train() client id: f_00000-4-1 loss: 0.843519  [   64/  126]
train() client id: f_00000-4-2 loss: 0.913372  [   96/  126]
train() client id: f_00000-5-0 loss: 0.878552  [   32/  126]
train() client id: f_00000-5-1 loss: 0.824896  [   64/  126]
train() client id: f_00000-5-2 loss: 0.818225  [   96/  126]
train() client id: f_00000-6-0 loss: 0.923234  [   32/  126]
train() client id: f_00000-6-1 loss: 0.880976  [   64/  126]
train() client id: f_00000-6-2 loss: 0.828044  [   96/  126]
train() client id: f_00000-7-0 loss: 0.799047  [   32/  126]
train() client id: f_00000-7-1 loss: 0.843833  [   64/  126]
train() client id: f_00000-7-2 loss: 0.806081  [   96/  126]
train() client id: f_00000-8-0 loss: 0.852445  [   32/  126]
train() client id: f_00000-8-1 loss: 0.902997  [   64/  126]
train() client id: f_00000-8-2 loss: 0.766547  [   96/  126]
train() client id: f_00000-9-0 loss: 0.842392  [   32/  126]
train() client id: f_00000-9-1 loss: 0.882923  [   64/  126]
train() client id: f_00000-9-2 loss: 0.816536  [   96/  126]
train() client id: f_00000-10-0 loss: 0.877467  [   32/  126]
train() client id: f_00000-10-1 loss: 0.882725  [   64/  126]
train() client id: f_00000-10-2 loss: 0.903000  [   96/  126]
train() client id: f_00000-11-0 loss: 0.900454  [   32/  126]
train() client id: f_00000-11-1 loss: 0.929245  [   64/  126]
train() client id: f_00000-11-2 loss: 0.997560  [   96/  126]
train() client id: f_00001-0-0 loss: 0.510843  [   32/  265]
train() client id: f_00001-0-1 loss: 0.451616  [   64/  265]
train() client id: f_00001-0-2 loss: 0.512184  [   96/  265]
train() client id: f_00001-0-3 loss: 0.540939  [  128/  265]
train() client id: f_00001-0-4 loss: 0.651445  [  160/  265]
train() client id: f_00001-0-5 loss: 0.517301  [  192/  265]
train() client id: f_00001-0-6 loss: 0.454143  [  224/  265]
train() client id: f_00001-0-7 loss: 0.487845  [  256/  265]
train() client id: f_00001-1-0 loss: 0.484184  [   32/  265]
train() client id: f_00001-1-1 loss: 0.433840  [   64/  265]
train() client id: f_00001-1-2 loss: 0.439384  [   96/  265]
train() client id: f_00001-1-3 loss: 0.489004  [  128/  265]
train() client id: f_00001-1-4 loss: 0.525417  [  160/  265]
train() client id: f_00001-1-5 loss: 0.532717  [  192/  265]
train() client id: f_00001-1-6 loss: 0.517350  [  224/  265]
train() client id: f_00001-1-7 loss: 0.691973  [  256/  265]
train() client id: f_00001-2-0 loss: 0.429065  [   32/  265]
train() client id: f_00001-2-1 loss: 0.457135  [   64/  265]
train() client id: f_00001-2-2 loss: 0.531215  [   96/  265]
train() client id: f_00001-2-3 loss: 0.525906  [  128/  265]
train() client id: f_00001-2-4 loss: 0.468110  [  160/  265]
train() client id: f_00001-2-5 loss: 0.469944  [  192/  265]
train() client id: f_00001-2-6 loss: 0.591555  [  224/  265]
train() client id: f_00001-2-7 loss: 0.583817  [  256/  265]
train() client id: f_00001-3-0 loss: 0.467301  [   32/  265]
train() client id: f_00001-3-1 loss: 0.649745  [   64/  265]
train() client id: f_00001-3-2 loss: 0.404001  [   96/  265]
train() client id: f_00001-3-3 loss: 0.539671  [  128/  265]
train() client id: f_00001-3-4 loss: 0.430560  [  160/  265]
train() client id: f_00001-3-5 loss: 0.537202  [  192/  265]
train() client id: f_00001-3-6 loss: 0.617230  [  224/  265]
train() client id: f_00001-3-7 loss: 0.410892  [  256/  265]
train() client id: f_00001-4-0 loss: 0.422380  [   32/  265]
train() client id: f_00001-4-1 loss: 0.496351  [   64/  265]
train() client id: f_00001-4-2 loss: 0.561079  [   96/  265]
train() client id: f_00001-4-3 loss: 0.402489  [  128/  265]
train() client id: f_00001-4-4 loss: 0.474146  [  160/  265]
train() client id: f_00001-4-5 loss: 0.603267  [  192/  265]
train() client id: f_00001-4-6 loss: 0.566246  [  224/  265]
train() client id: f_00001-4-7 loss: 0.476150  [  256/  265]
train() client id: f_00001-5-0 loss: 0.432930  [   32/  265]
train() client id: f_00001-5-1 loss: 0.545535  [   64/  265]
train() client id: f_00001-5-2 loss: 0.472542  [   96/  265]
train() client id: f_00001-5-3 loss: 0.509924  [  128/  265]
train() client id: f_00001-5-4 loss: 0.461555  [  160/  265]
train() client id: f_00001-5-5 loss: 0.527875  [  192/  265]
train() client id: f_00001-5-6 loss: 0.528804  [  224/  265]
train() client id: f_00001-5-7 loss: 0.548354  [  256/  265]
train() client id: f_00001-6-0 loss: 0.472678  [   32/  265]
train() client id: f_00001-6-1 loss: 0.472423  [   64/  265]
train() client id: f_00001-6-2 loss: 0.477611  [   96/  265]
train() client id: f_00001-6-3 loss: 0.583166  [  128/  265]
train() client id: f_00001-6-4 loss: 0.672244  [  160/  265]
train() client id: f_00001-6-5 loss: 0.455468  [  192/  265]
train() client id: f_00001-6-6 loss: 0.409604  [  224/  265]
train() client id: f_00001-6-7 loss: 0.484070  [  256/  265]
train() client id: f_00001-7-0 loss: 0.482952  [   32/  265]
train() client id: f_00001-7-1 loss: 0.469154  [   64/  265]
train() client id: f_00001-7-2 loss: 0.482422  [   96/  265]
train() client id: f_00001-7-3 loss: 0.447232  [  128/  265]
train() client id: f_00001-7-4 loss: 0.409005  [  160/  265]
train() client id: f_00001-7-5 loss: 0.587911  [  192/  265]
train() client id: f_00001-7-6 loss: 0.670267  [  224/  265]
train() client id: f_00001-7-7 loss: 0.398296  [  256/  265]
train() client id: f_00001-8-0 loss: 0.529503  [   32/  265]
train() client id: f_00001-8-1 loss: 0.422380  [   64/  265]
train() client id: f_00001-8-2 loss: 0.533240  [   96/  265]
train() client id: f_00001-8-3 loss: 0.528769  [  128/  265]
train() client id: f_00001-8-4 loss: 0.423149  [  160/  265]
train() client id: f_00001-8-5 loss: 0.608364  [  192/  265]
train() client id: f_00001-8-6 loss: 0.408199  [  224/  265]
train() client id: f_00001-8-7 loss: 0.484614  [  256/  265]
train() client id: f_00001-9-0 loss: 0.657462  [   32/  265]
train() client id: f_00001-9-1 loss: 0.519122  [   64/  265]
train() client id: f_00001-9-2 loss: 0.533231  [   96/  265]
train() client id: f_00001-9-3 loss: 0.473144  [  128/  265]
train() client id: f_00001-9-4 loss: 0.520125  [  160/  265]
train() client id: f_00001-9-5 loss: 0.435433  [  192/  265]
train() client id: f_00001-9-6 loss: 0.492086  [  224/  265]
train() client id: f_00001-9-7 loss: 0.393936  [  256/  265]
train() client id: f_00001-10-0 loss: 0.452857  [   32/  265]
train() client id: f_00001-10-1 loss: 0.480989  [   64/  265]
train() client id: f_00001-10-2 loss: 0.468255  [   96/  265]
train() client id: f_00001-10-3 loss: 0.457188  [  128/  265]
train() client id: f_00001-10-4 loss: 0.463140  [  160/  265]
train() client id: f_00001-10-5 loss: 0.452087  [  192/  265]
train() client id: f_00001-10-6 loss: 0.688765  [  224/  265]
train() client id: f_00001-10-7 loss: 0.568649  [  256/  265]
train() client id: f_00001-11-0 loss: 0.440838  [   32/  265]
train() client id: f_00001-11-1 loss: 0.553169  [   64/  265]
train() client id: f_00001-11-2 loss: 0.666891  [   96/  265]
train() client id: f_00001-11-3 loss: 0.480199  [  128/  265]
train() client id: f_00001-11-4 loss: 0.461535  [  160/  265]
train() client id: f_00001-11-5 loss: 0.457396  [  192/  265]
train() client id: f_00001-11-6 loss: 0.412361  [  224/  265]
train() client id: f_00001-11-7 loss: 0.568818  [  256/  265]
train() client id: f_00002-0-0 loss: 1.074615  [   32/  124]
train() client id: f_00002-0-1 loss: 1.202876  [   64/  124]
train() client id: f_00002-0-2 loss: 1.160289  [   96/  124]
train() client id: f_00002-1-0 loss: 1.137036  [   32/  124]
train() client id: f_00002-1-1 loss: 1.244483  [   64/  124]
train() client id: f_00002-1-2 loss: 0.902225  [   96/  124]
train() client id: f_00002-2-0 loss: 1.076621  [   32/  124]
train() client id: f_00002-2-1 loss: 1.246487  [   64/  124]
train() client id: f_00002-2-2 loss: 0.956895  [   96/  124]
train() client id: f_00002-3-0 loss: 1.267737  [   32/  124]
train() client id: f_00002-3-1 loss: 0.889741  [   64/  124]
train() client id: f_00002-3-2 loss: 1.018990  [   96/  124]
train() client id: f_00002-4-0 loss: 1.200924  [   32/  124]
train() client id: f_00002-4-1 loss: 0.960833  [   64/  124]
train() client id: f_00002-4-2 loss: 1.062398  [   96/  124]
train() client id: f_00002-5-0 loss: 1.065027  [   32/  124]
train() client id: f_00002-5-1 loss: 1.227962  [   64/  124]
train() client id: f_00002-5-2 loss: 0.907180  [   96/  124]
train() client id: f_00002-6-0 loss: 0.902578  [   32/  124]
train() client id: f_00002-6-1 loss: 1.128814  [   64/  124]
train() client id: f_00002-6-2 loss: 1.288100  [   96/  124]
train() client id: f_00002-7-0 loss: 0.871650  [   32/  124]
train() client id: f_00002-7-1 loss: 1.354668  [   64/  124]
train() client id: f_00002-7-2 loss: 0.851754  [   96/  124]
train() client id: f_00002-8-0 loss: 0.993729  [   32/  124]
train() client id: f_00002-8-1 loss: 0.983264  [   64/  124]
train() client id: f_00002-8-2 loss: 1.137551  [   96/  124]
train() client id: f_00002-9-0 loss: 0.908910  [   32/  124]
train() client id: f_00002-9-1 loss: 1.024922  [   64/  124]
train() client id: f_00002-9-2 loss: 1.017513  [   96/  124]
train() client id: f_00002-10-0 loss: 0.923263  [   32/  124]
train() client id: f_00002-10-1 loss: 1.133596  [   64/  124]
train() client id: f_00002-10-2 loss: 1.092254  [   96/  124]
train() client id: f_00002-11-0 loss: 1.114777  [   32/  124]
train() client id: f_00002-11-1 loss: 0.980891  [   64/  124]
train() client id: f_00002-11-2 loss: 1.096358  [   96/  124]
train() client id: f_00003-0-0 loss: 0.897977  [   32/   43]
train() client id: f_00003-1-0 loss: 1.097902  [   32/   43]
train() client id: f_00003-2-0 loss: 1.065377  [   32/   43]
train() client id: f_00003-3-0 loss: 1.022076  [   32/   43]
train() client id: f_00003-4-0 loss: 1.024964  [   32/   43]
train() client id: f_00003-5-0 loss: 0.975181  [   32/   43]
train() client id: f_00003-6-0 loss: 0.926865  [   32/   43]
train() client id: f_00003-7-0 loss: 0.972497  [   32/   43]
train() client id: f_00003-8-0 loss: 1.002303  [   32/   43]
train() client id: f_00003-9-0 loss: 1.110919  [   32/   43]
train() client id: f_00003-10-0 loss: 0.975005  [   32/   43]
train() client id: f_00003-11-0 loss: 0.882044  [   32/   43]
train() client id: f_00004-0-0 loss: 0.946100  [   32/  306]
train() client id: f_00004-0-1 loss: 0.885511  [   64/  306]
train() client id: f_00004-0-2 loss: 0.980641  [   96/  306]
train() client id: f_00004-0-3 loss: 0.847787  [  128/  306]
train() client id: f_00004-0-4 loss: 0.923806  [  160/  306]
train() client id: f_00004-0-5 loss: 0.994622  [  192/  306]
train() client id: f_00004-0-6 loss: 0.787798  [  224/  306]
train() client id: f_00004-0-7 loss: 0.833185  [  256/  306]
train() client id: f_00004-0-8 loss: 0.835866  [  288/  306]
train() client id: f_00004-1-0 loss: 0.883812  [   32/  306]
train() client id: f_00004-1-1 loss: 0.901350  [   64/  306]
train() client id: f_00004-1-2 loss: 1.060364  [   96/  306]
train() client id: f_00004-1-3 loss: 0.888805  [  128/  306]
train() client id: f_00004-1-4 loss: 0.832794  [  160/  306]
train() client id: f_00004-1-5 loss: 0.826260  [  192/  306]
train() client id: f_00004-1-6 loss: 0.940544  [  224/  306]
train() client id: f_00004-1-7 loss: 0.916715  [  256/  306]
train() client id: f_00004-1-8 loss: 0.881854  [  288/  306]
train() client id: f_00004-2-0 loss: 0.912370  [   32/  306]
train() client id: f_00004-2-1 loss: 0.831366  [   64/  306]
train() client id: f_00004-2-2 loss: 1.012263  [   96/  306]
train() client id: f_00004-2-3 loss: 0.900432  [  128/  306]
train() client id: f_00004-2-4 loss: 0.761519  [  160/  306]
train() client id: f_00004-2-5 loss: 0.933649  [  192/  306]
train() client id: f_00004-2-6 loss: 0.812375  [  224/  306]
train() client id: f_00004-2-7 loss: 0.874242  [  256/  306]
train() client id: f_00004-2-8 loss: 0.980007  [  288/  306]
train() client id: f_00004-3-0 loss: 0.795075  [   32/  306]
train() client id: f_00004-3-1 loss: 0.880183  [   64/  306]
train() client id: f_00004-3-2 loss: 0.870560  [   96/  306]
train() client id: f_00004-3-3 loss: 0.971425  [  128/  306]
train() client id: f_00004-3-4 loss: 0.848136  [  160/  306]
train() client id: f_00004-3-5 loss: 0.935453  [  192/  306]
train() client id: f_00004-3-6 loss: 0.811898  [  224/  306]
train() client id: f_00004-3-7 loss: 1.052572  [  256/  306]
train() client id: f_00004-3-8 loss: 0.956254  [  288/  306]
train() client id: f_00004-4-0 loss: 0.883539  [   32/  306]
train() client id: f_00004-4-1 loss: 0.810374  [   64/  306]
train() client id: f_00004-4-2 loss: 1.021545  [   96/  306]
train() client id: f_00004-4-3 loss: 0.892465  [  128/  306]
train() client id: f_00004-4-4 loss: 0.928014  [  160/  306]
train() client id: f_00004-4-5 loss: 0.766289  [  192/  306]
train() client id: f_00004-4-6 loss: 0.906647  [  224/  306]
train() client id: f_00004-4-7 loss: 0.905375  [  256/  306]
train() client id: f_00004-4-8 loss: 0.874045  [  288/  306]
train() client id: f_00004-5-0 loss: 0.803305  [   32/  306]
train() client id: f_00004-5-1 loss: 0.922983  [   64/  306]
train() client id: f_00004-5-2 loss: 0.957154  [   96/  306]
train() client id: f_00004-5-3 loss: 0.875643  [  128/  306]
train() client id: f_00004-5-4 loss: 0.840210  [  160/  306]
train() client id: f_00004-5-5 loss: 0.900384  [  192/  306]
train() client id: f_00004-5-6 loss: 0.919207  [  224/  306]
train() client id: f_00004-5-7 loss: 0.894118  [  256/  306]
train() client id: f_00004-5-8 loss: 0.908380  [  288/  306]
train() client id: f_00004-6-0 loss: 0.845548  [   32/  306]
train() client id: f_00004-6-1 loss: 0.913019  [   64/  306]
train() client id: f_00004-6-2 loss: 0.945219  [   96/  306]
train() client id: f_00004-6-3 loss: 0.931966  [  128/  306]
train() client id: f_00004-6-4 loss: 0.810678  [  160/  306]
train() client id: f_00004-6-5 loss: 0.865235  [  192/  306]
train() client id: f_00004-6-6 loss: 0.892655  [  224/  306]
train() client id: f_00004-6-7 loss: 0.827281  [  256/  306]
train() client id: f_00004-6-8 loss: 0.811578  [  288/  306]
train() client id: f_00004-7-0 loss: 0.846813  [   32/  306]
train() client id: f_00004-7-1 loss: 0.906527  [   64/  306]
train() client id: f_00004-7-2 loss: 0.789664  [   96/  306]
train() client id: f_00004-7-3 loss: 0.920866  [  128/  306]
train() client id: f_00004-7-4 loss: 0.869087  [  160/  306]
train() client id: f_00004-7-5 loss: 0.855181  [  192/  306]
train() client id: f_00004-7-6 loss: 0.797195  [  224/  306]
train() client id: f_00004-7-7 loss: 1.016244  [  256/  306]
train() client id: f_00004-7-8 loss: 0.953923  [  288/  306]
train() client id: f_00004-8-0 loss: 0.733612  [   32/  306]
train() client id: f_00004-8-1 loss: 0.998773  [   64/  306]
train() client id: f_00004-8-2 loss: 0.943054  [   96/  306]
train() client id: f_00004-8-3 loss: 0.911159  [  128/  306]
train() client id: f_00004-8-4 loss: 0.881489  [  160/  306]
train() client id: f_00004-8-5 loss: 0.864651  [  192/  306]
train() client id: f_00004-8-6 loss: 0.823597  [  224/  306]
train() client id: f_00004-8-7 loss: 0.923501  [  256/  306]
train() client id: f_00004-8-8 loss: 0.806813  [  288/  306]
train() client id: f_00004-9-0 loss: 0.906732  [   32/  306]
train() client id: f_00004-9-1 loss: 0.941804  [   64/  306]
train() client id: f_00004-9-2 loss: 0.756730  [   96/  306]
train() client id: f_00004-9-3 loss: 0.934676  [  128/  306]
train() client id: f_00004-9-4 loss: 0.819182  [  160/  306]
train() client id: f_00004-9-5 loss: 0.832044  [  192/  306]
train() client id: f_00004-9-6 loss: 0.898421  [  224/  306]
train() client id: f_00004-9-7 loss: 0.992036  [  256/  306]
train() client id: f_00004-9-8 loss: 0.903182  [  288/  306]
train() client id: f_00004-10-0 loss: 0.882132  [   32/  306]
train() client id: f_00004-10-1 loss: 0.940127  [   64/  306]
train() client id: f_00004-10-2 loss: 1.053394  [   96/  306]
train() client id: f_00004-10-3 loss: 0.718876  [  128/  306]
train() client id: f_00004-10-4 loss: 0.850742  [  160/  306]
train() client id: f_00004-10-5 loss: 0.842740  [  192/  306]
train() client id: f_00004-10-6 loss: 0.853533  [  224/  306]
train() client id: f_00004-10-7 loss: 0.953825  [  256/  306]
train() client id: f_00004-10-8 loss: 0.811353  [  288/  306]
train() client id: f_00004-11-0 loss: 0.964049  [   32/  306]
train() client id: f_00004-11-1 loss: 0.898559  [   64/  306]
train() client id: f_00004-11-2 loss: 0.812632  [   96/  306]
train() client id: f_00004-11-3 loss: 0.899948  [  128/  306]
train() client id: f_00004-11-4 loss: 0.876026  [  160/  306]
train() client id: f_00004-11-5 loss: 0.919440  [  192/  306]
train() client id: f_00004-11-6 loss: 0.871646  [  224/  306]
train() client id: f_00004-11-7 loss: 0.824047  [  256/  306]
train() client id: f_00004-11-8 loss: 0.797851  [  288/  306]
train() client id: f_00005-0-0 loss: 0.746483  [   32/  146]
train() client id: f_00005-0-1 loss: 0.588124  [   64/  146]
train() client id: f_00005-0-2 loss: 0.596565  [   96/  146]
train() client id: f_00005-0-3 loss: 0.484793  [  128/  146]
train() client id: f_00005-1-0 loss: 0.759863  [   32/  146]
train() client id: f_00005-1-1 loss: 0.522512  [   64/  146]
train() client id: f_00005-1-2 loss: 0.778350  [   96/  146]
train() client id: f_00005-1-3 loss: 0.498671  [  128/  146]
train() client id: f_00005-2-0 loss: 0.646508  [   32/  146]
train() client id: f_00005-2-1 loss: 0.496898  [   64/  146]
train() client id: f_00005-2-2 loss: 0.865936  [   96/  146]
train() client id: f_00005-2-3 loss: 0.509441  [  128/  146]
train() client id: f_00005-3-0 loss: 0.665492  [   32/  146]
train() client id: f_00005-3-1 loss: 0.511625  [   64/  146]
train() client id: f_00005-3-2 loss: 0.662520  [   96/  146]
train() client id: f_00005-3-3 loss: 0.581653  [  128/  146]
train() client id: f_00005-4-0 loss: 0.407608  [   32/  146]
train() client id: f_00005-4-1 loss: 0.628713  [   64/  146]
train() client id: f_00005-4-2 loss: 1.043408  [   96/  146]
train() client id: f_00005-4-3 loss: 0.385468  [  128/  146]
train() client id: f_00005-5-0 loss: 0.481500  [   32/  146]
train() client id: f_00005-5-1 loss: 0.733586  [   64/  146]
train() client id: f_00005-5-2 loss: 0.372463  [   96/  146]
train() client id: f_00005-5-3 loss: 0.516711  [  128/  146]
train() client id: f_00005-6-0 loss: 0.467749  [   32/  146]
train() client id: f_00005-6-1 loss: 0.831128  [   64/  146]
train() client id: f_00005-6-2 loss: 0.553203  [   96/  146]
train() client id: f_00005-6-3 loss: 0.478602  [  128/  146]
train() client id: f_00005-7-0 loss: 0.863910  [   32/  146]
train() client id: f_00005-7-1 loss: 0.536571  [   64/  146]
train() client id: f_00005-7-2 loss: 0.382634  [   96/  146]
train() client id: f_00005-7-3 loss: 0.459154  [  128/  146]
train() client id: f_00005-8-0 loss: 0.578461  [   32/  146]
train() client id: f_00005-8-1 loss: 0.708490  [   64/  146]
train() client id: f_00005-8-2 loss: 0.717385  [   96/  146]
train() client id: f_00005-8-3 loss: 0.433347  [  128/  146]
train() client id: f_00005-9-0 loss: 0.495492  [   32/  146]
train() client id: f_00005-9-1 loss: 0.663474  [   64/  146]
train() client id: f_00005-9-2 loss: 0.515327  [   96/  146]
train() client id: f_00005-9-3 loss: 0.474875  [  128/  146]
train() client id: f_00005-10-0 loss: 0.492040  [   32/  146]
train() client id: f_00005-10-1 loss: 0.651025  [   64/  146]
train() client id: f_00005-10-2 loss: 0.576249  [   96/  146]
train() client id: f_00005-10-3 loss: 0.500470  [  128/  146]
train() client id: f_00005-11-0 loss: 0.690728  [   32/  146]
train() client id: f_00005-11-1 loss: 0.671677  [   64/  146]
train() client id: f_00005-11-2 loss: 0.294708  [   96/  146]
train() client id: f_00005-11-3 loss: 0.730663  [  128/  146]
train() client id: f_00006-0-0 loss: 0.596600  [   32/   54]
train() client id: f_00006-1-0 loss: 0.581031  [   32/   54]
train() client id: f_00006-2-0 loss: 0.585073  [   32/   54]
train() client id: f_00006-3-0 loss: 0.574699  [   32/   54]
train() client id: f_00006-4-0 loss: 0.541801  [   32/   54]
train() client id: f_00006-5-0 loss: 0.556914  [   32/   54]
train() client id: f_00006-6-0 loss: 0.591747  [   32/   54]
train() client id: f_00006-7-0 loss: 0.553020  [   32/   54]
train() client id: f_00006-8-0 loss: 0.540457  [   32/   54]
train() client id: f_00006-9-0 loss: 0.604838  [   32/   54]
train() client id: f_00006-10-0 loss: 0.556935  [   32/   54]
train() client id: f_00006-11-0 loss: 0.546519  [   32/   54]
train() client id: f_00007-0-0 loss: 0.660506  [   32/  179]
train() client id: f_00007-0-1 loss: 0.516422  [   64/  179]
train() client id: f_00007-0-2 loss: 0.507646  [   96/  179]
train() client id: f_00007-0-3 loss: 0.391863  [  128/  179]
train() client id: f_00007-0-4 loss: 0.479891  [  160/  179]
train() client id: f_00007-1-0 loss: 0.662460  [   32/  179]
train() client id: f_00007-1-1 loss: 0.435553  [   64/  179]
train() client id: f_00007-1-2 loss: 0.380437  [   96/  179]
train() client id: f_00007-1-3 loss: 0.434464  [  128/  179]
train() client id: f_00007-1-4 loss: 0.560449  [  160/  179]
train() client id: f_00007-2-0 loss: 0.340890  [   32/  179]
train() client id: f_00007-2-1 loss: 0.436323  [   64/  179]
train() client id: f_00007-2-2 loss: 0.561964  [   96/  179]
train() client id: f_00007-2-3 loss: 0.602440  [  128/  179]
train() client id: f_00007-2-4 loss: 0.473874  [  160/  179]
train() client id: f_00007-3-0 loss: 0.435267  [   32/  179]
train() client id: f_00007-3-1 loss: 0.293329  [   64/  179]
train() client id: f_00007-3-2 loss: 0.412205  [   96/  179]
train() client id: f_00007-3-3 loss: 0.659568  [  128/  179]
train() client id: f_00007-3-4 loss: 0.582277  [  160/  179]
train() client id: f_00007-4-0 loss: 0.322510  [   32/  179]
train() client id: f_00007-4-1 loss: 0.422624  [   64/  179]
train() client id: f_00007-4-2 loss: 0.409670  [   96/  179]
train() client id: f_00007-4-3 loss: 0.662844  [  128/  179]
train() client id: f_00007-4-4 loss: 0.366812  [  160/  179]
train() client id: f_00007-5-0 loss: 0.382480  [   32/  179]
train() client id: f_00007-5-1 loss: 0.416590  [   64/  179]
train() client id: f_00007-5-2 loss: 0.429528  [   96/  179]
train() client id: f_00007-5-3 loss: 0.477036  [  128/  179]
train() client id: f_00007-5-4 loss: 0.478989  [  160/  179]
train() client id: f_00007-6-0 loss: 0.570760  [   32/  179]
train() client id: f_00007-6-1 loss: 0.317267  [   64/  179]
train() client id: f_00007-6-2 loss: 0.284457  [   96/  179]
train() client id: f_00007-6-3 loss: 0.465511  [  128/  179]
train() client id: f_00007-6-4 loss: 0.383059  [  160/  179]
train() client id: f_00007-7-0 loss: 0.412638  [   32/  179]
train() client id: f_00007-7-1 loss: 0.567400  [   64/  179]
train() client id: f_00007-7-2 loss: 0.537354  [   96/  179]
train() client id: f_00007-7-3 loss: 0.468700  [  128/  179]
train() client id: f_00007-7-4 loss: 0.280769  [  160/  179]
train() client id: f_00007-8-0 loss: 0.602658  [   32/  179]
train() client id: f_00007-8-1 loss: 0.282893  [   64/  179]
train() client id: f_00007-8-2 loss: 0.518687  [   96/  179]
train() client id: f_00007-8-3 loss: 0.375456  [  128/  179]
train() client id: f_00007-8-4 loss: 0.427005  [  160/  179]
train() client id: f_00007-9-0 loss: 0.403443  [   32/  179]
train() client id: f_00007-9-1 loss: 0.323853  [   64/  179]
train() client id: f_00007-9-2 loss: 0.361561  [   96/  179]
train() client id: f_00007-9-3 loss: 0.377871  [  128/  179]
train() client id: f_00007-9-4 loss: 0.326489  [  160/  179]
train() client id: f_00007-10-0 loss: 0.488050  [   32/  179]
train() client id: f_00007-10-1 loss: 0.386187  [   64/  179]
train() client id: f_00007-10-2 loss: 0.351738  [   96/  179]
train() client id: f_00007-10-3 loss: 0.362989  [  128/  179]
train() client id: f_00007-10-4 loss: 0.472917  [  160/  179]
train() client id: f_00007-11-0 loss: 0.485684  [   32/  179]
train() client id: f_00007-11-1 loss: 0.363146  [   64/  179]
train() client id: f_00007-11-2 loss: 0.462242  [   96/  179]
train() client id: f_00007-11-3 loss: 0.542233  [  128/  179]
train() client id: f_00007-11-4 loss: 0.394151  [  160/  179]
train() client id: f_00008-0-0 loss: 0.722839  [   32/  130]
train() client id: f_00008-0-1 loss: 0.679808  [   64/  130]
train() client id: f_00008-0-2 loss: 0.806620  [   96/  130]
train() client id: f_00008-0-3 loss: 0.695630  [  128/  130]
train() client id: f_00008-1-0 loss: 0.802336  [   32/  130]
train() client id: f_00008-1-1 loss: 0.746469  [   64/  130]
train() client id: f_00008-1-2 loss: 0.669705  [   96/  130]
train() client id: f_00008-1-3 loss: 0.699192  [  128/  130]
train() client id: f_00008-2-0 loss: 0.752083  [   32/  130]
train() client id: f_00008-2-1 loss: 0.592142  [   64/  130]
train() client id: f_00008-2-2 loss: 0.809486  [   96/  130]
train() client id: f_00008-2-3 loss: 0.747054  [  128/  130]
train() client id: f_00008-3-0 loss: 0.777750  [   32/  130]
train() client id: f_00008-3-1 loss: 0.682062  [   64/  130]
train() client id: f_00008-3-2 loss: 0.737638  [   96/  130]
train() client id: f_00008-3-3 loss: 0.698784  [  128/  130]
train() client id: f_00008-4-0 loss: 0.615116  [   32/  130]
train() client id: f_00008-4-1 loss: 0.722023  [   64/  130]
train() client id: f_00008-4-2 loss: 0.843944  [   96/  130]
train() client id: f_00008-4-3 loss: 0.713768  [  128/  130]
train() client id: f_00008-5-0 loss: 0.737023  [   32/  130]
train() client id: f_00008-5-1 loss: 0.708786  [   64/  130]
train() client id: f_00008-5-2 loss: 0.772321  [   96/  130]
train() client id: f_00008-5-3 loss: 0.663277  [  128/  130]
train() client id: f_00008-6-0 loss: 0.735736  [   32/  130]
train() client id: f_00008-6-1 loss: 0.691301  [   64/  130]
train() client id: f_00008-6-2 loss: 0.752362  [   96/  130]
train() client id: f_00008-6-3 loss: 0.685041  [  128/  130]
train() client id: f_00008-7-0 loss: 0.718830  [   32/  130]
train() client id: f_00008-7-1 loss: 0.631822  [   64/  130]
train() client id: f_00008-7-2 loss: 0.846161  [   96/  130]
train() client id: f_00008-7-3 loss: 0.703546  [  128/  130]
train() client id: f_00008-8-0 loss: 0.815469  [   32/  130]
train() client id: f_00008-8-1 loss: 0.699853  [   64/  130]
train() client id: f_00008-8-2 loss: 0.693453  [   96/  130]
train() client id: f_00008-8-3 loss: 0.682970  [  128/  130]
train() client id: f_00008-9-0 loss: 0.734008  [   32/  130]
train() client id: f_00008-9-1 loss: 0.816409  [   64/  130]
train() client id: f_00008-9-2 loss: 0.603999  [   96/  130]
train() client id: f_00008-9-3 loss: 0.751800  [  128/  130]
train() client id: f_00008-10-0 loss: 0.794444  [   32/  130]
train() client id: f_00008-10-1 loss: 0.709629  [   64/  130]
train() client id: f_00008-10-2 loss: 0.668062  [   96/  130]
train() client id: f_00008-10-3 loss: 0.697538  [  128/  130]
train() client id: f_00008-11-0 loss: 0.668354  [   32/  130]
train() client id: f_00008-11-1 loss: 0.813279  [   64/  130]
train() client id: f_00008-11-2 loss: 0.781189  [   96/  130]
train() client id: f_00008-11-3 loss: 0.644060  [  128/  130]
train() client id: f_00009-0-0 loss: 1.137431  [   32/  118]
train() client id: f_00009-0-1 loss: 1.054668  [   64/  118]
train() client id: f_00009-0-2 loss: 1.083491  [   96/  118]
train() client id: f_00009-1-0 loss: 1.034460  [   32/  118]
train() client id: f_00009-1-1 loss: 1.203864  [   64/  118]
train() client id: f_00009-1-2 loss: 1.016931  [   96/  118]
train() client id: f_00009-2-0 loss: 1.006989  [   32/  118]
train() client id: f_00009-2-1 loss: 0.997622  [   64/  118]
train() client id: f_00009-2-2 loss: 0.996603  [   96/  118]
train() client id: f_00009-3-0 loss: 0.992913  [   32/  118]
train() client id: f_00009-3-1 loss: 0.999706  [   64/  118]
train() client id: f_00009-3-2 loss: 0.972975  [   96/  118]
train() client id: f_00009-4-0 loss: 0.925211  [   32/  118]
train() client id: f_00009-4-1 loss: 0.946539  [   64/  118]
train() client id: f_00009-4-2 loss: 1.066031  [   96/  118]
train() client id: f_00009-5-0 loss: 0.802132  [   32/  118]
train() client id: f_00009-5-1 loss: 0.921963  [   64/  118]
train() client id: f_00009-5-2 loss: 0.938494  [   96/  118]
train() client id: f_00009-6-0 loss: 0.855977  [   32/  118]
train() client id: f_00009-6-1 loss: 0.932138  [   64/  118]
train() client id: f_00009-6-2 loss: 0.843206  [   96/  118]
train() client id: f_00009-7-0 loss: 0.858133  [   32/  118]
train() client id: f_00009-7-1 loss: 0.985493  [   64/  118]
train() client id: f_00009-7-2 loss: 0.917525  [   96/  118]
train() client id: f_00009-8-0 loss: 0.941343  [   32/  118]
train() client id: f_00009-8-1 loss: 0.773694  [   64/  118]
train() client id: f_00009-8-2 loss: 0.931920  [   96/  118]
train() client id: f_00009-9-0 loss: 0.926896  [   32/  118]
train() client id: f_00009-9-1 loss: 0.938183  [   64/  118]
train() client id: f_00009-9-2 loss: 0.831767  [   96/  118]
train() client id: f_00009-10-0 loss: 0.900082  [   32/  118]
train() client id: f_00009-10-1 loss: 0.869374  [   64/  118]
train() client id: f_00009-10-2 loss: 0.880851  [   96/  118]
train() client id: f_00009-11-0 loss: 0.968897  [   32/  118]
train() client id: f_00009-11-1 loss: 0.925542  [   64/  118]
train() client id: f_00009-11-2 loss: 0.783151  [   96/  118]
At round 30 accuracy: 0.6392572944297082
At round 30 training accuracy: 0.5855130784708249
At round 30 training loss: 0.8350440297512309
update_location
xs = -3.905658 4.200318 170.009024 18.811294 0.979296 3.956410 -132.443192 -111.324852 154.663977 -97.060879 
ys = 162.587959 145.555839 1.320614 -132.455176 124.350187 107.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 0.7885738318825701
ys mean: 42.89414253552871
dists_uav = 190.919090 176.646950 197.243028 167.027658 159.574208 147.103855 165.976172 149.645912 185.012474 139.416018 
uav_gains = -107.113331 -106.214170 -107.508753 -105.587948 -105.084268 -104.194116 -105.517986 -104.381033 -106.743743 -103.609525 
uav_gains_db_mean: -105.5954873453281
dists_bs = 171.543967 181.603173 386.258664 363.443947 183.123227 191.152648 182.651901 185.451824 365.325684 187.922856 
bs_gains = -102.129572 -102.822515 -111.999650 -111.259307 -102.923875 -103.445708 -102.892537 -103.077530 -111.322105 -103.238488 
bs_gains_db_mean: -105.51112873697309
Round 31
-------------------------------
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.90000386 14.30569929  6.7966894   2.4461517  16.50004725  7.94229532
  3.0341758   9.71414062  7.16398278  6.44283898]
obj_prev = 81.24602500345188
eta_min = 3.803699202355281e-14	eta_max = 0.9265448863059464
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 18.86522301759883	eta = 0.9090909090909091
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 33.97406433952712	eta = 0.5048027981544223
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 26.610863860443008	eta = 0.6444812477044536
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.281958371636836	eta = 0.6783573681741374
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.213171638427752	eta = 0.6802080670062467
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.212972778010624	eta = 0.680213431961071
eta = 0.680213431961071
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [0.03179805 0.06687685 0.03129332 0.01085171 0.07722383 0.03684536
 0.01362773 0.04517342 0.03280751 0.02977912]
ene_total = [2.23637289 4.06508358 2.2200981  1.04612844 4.63524192 2.42743785
 1.19692013 2.90387674 2.44646747 2.03534566]
ti_comp = [0.42879245 0.44662227 0.42670833 0.43604005 0.44628188 0.44447784
 0.43634521 0.44101196 0.40063459 0.44520477]
ti_coms = [0.08920181 0.07137199 0.09128593 0.08195421 0.07171238 0.07351642
 0.08164905 0.0769823  0.11735967 0.07278949]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.09291688e-05 9.37187797e-05 1.05189542e-05 4.20070663e-07
 1.44515864e-04 1.58244307e-05 8.30788735e-07 2.96229369e-05
 1.37500070e-05 8.32714570e-06]
ene_total = [0.49052461 0.39714469 0.5019487  0.45014129 0.40180415 0.40464437
 0.44848781 0.42443794 0.64533131 0.40024009]
optimize_network iter = 0 obj = 4.564704952114099
eta = 0.680213431961071
freqs = [37078600.58661079 74869583.15751041 36668274.98538263 12443483.84142069
 86519118.7997421  41447912.13753933 15615771.9937943  51215637.28090309
 40944429.42168844 33444300.55685099]
eta_min = 0.6802134319610765	eta_max = 0.6802134319610728
af = 0.014970687331339992	bf = 1.4539623964515263	zeta = 0.016467756064473992	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [2.70316510e-06 2.31799269e-05 2.60170470e-06 1.03898144e-07
 3.57438195e-05 3.91393429e-06 2.05483065e-07 7.32678673e-06
 3.40085688e-06 2.05959391e-06]
ene_total = [1.72390573 1.38338877 1.76415135 1.58337944 1.39239245 1.42109708
 1.57750334 1.48871744 2.26805167 1.40669455]
ti_comp = [0.42879245 0.44662227 0.42670833 0.43604005 0.44628188 0.44447784
 0.43634521 0.44101196 0.40063459 0.44520477]
ti_coms = [0.08920181 0.07137199 0.09128593 0.08195421 0.07171238 0.07351642
 0.08164905 0.0769823  0.11735967 0.07278949]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.09291688e-05 9.37187797e-05 1.05189542e-05 4.20070663e-07
 1.44515864e-04 1.58244307e-05 8.30788735e-07 2.96229369e-05
 1.37500070e-05 8.32714570e-06]
ene_total = [0.49052461 0.39714469 0.5019487  0.45014129 0.40180415 0.40464437
 0.44848781 0.42443794 0.64533131 0.40024009]
optimize_network iter = 1 obj = 4.564704952114124
eta = 0.6802134319610728
freqs = [37078600.58661079 74869583.15751038 36668274.98538262 12443483.84142068
 86519118.79974204 41447912.1375393  15615771.9937943  51215637.28090306
 40944429.42168847 33444300.55685098]
Done!
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.03936590e-05 8.91267263e-05 1.00035442e-05 3.99487948e-07
 1.37434843e-04 1.50490617e-05 7.90081565e-07 2.81714657e-05
 1.30762811e-05 7.91913038e-06]
ene_total = [0.00893058 0.00722633 0.0091386  0.00819582 0.00730867 0.00736669
 0.00816569 0.0077264  0.01174904 0.00728687]
At round 31 energy consumption: 0.08309468916898873
At round 31 eta: 0.6802134319610728
At round 31 a_n: 17.56368160020554
At round 31 local rounds: 12.618272918924527
At round 31 global rounds: 54.923137353497395
gradient difference: 0.46751394867897034
train() client id: f_00000-0-0 loss: 1.119409  [   32/  126]
train() client id: f_00000-0-1 loss: 1.196043  [   64/  126]
train() client id: f_00000-0-2 loss: 1.222610  [   96/  126]
train() client id: f_00000-1-0 loss: 0.917020  [   32/  126]
train() client id: f_00000-1-1 loss: 0.969299  [   64/  126]
train() client id: f_00000-1-2 loss: 1.123744  [   96/  126]
train() client id: f_00000-2-0 loss: 1.032859  [   32/  126]
train() client id: f_00000-2-1 loss: 1.067206  [   64/  126]
train() client id: f_00000-2-2 loss: 0.868321  [   96/  126]
train() client id: f_00000-3-0 loss: 0.829590  [   32/  126]
train() client id: f_00000-3-1 loss: 0.988933  [   64/  126]
train() client id: f_00000-3-2 loss: 1.002545  [   96/  126]
train() client id: f_00000-4-0 loss: 0.803834  [   32/  126]
train() client id: f_00000-4-1 loss: 0.821693  [   64/  126]
train() client id: f_00000-4-2 loss: 0.942997  [   96/  126]
train() client id: f_00000-5-0 loss: 0.720665  [   32/  126]
train() client id: f_00000-5-1 loss: 0.837420  [   64/  126]
train() client id: f_00000-5-2 loss: 0.801440  [   96/  126]
train() client id: f_00000-6-0 loss: 0.759869  [   32/  126]
train() client id: f_00000-6-1 loss: 0.870769  [   64/  126]
train() client id: f_00000-6-2 loss: 0.836554  [   96/  126]
train() client id: f_00000-7-0 loss: 0.845266  [   32/  126]
train() client id: f_00000-7-1 loss: 0.662784  [   64/  126]
train() client id: f_00000-7-2 loss: 0.859044  [   96/  126]
train() client id: f_00000-8-0 loss: 0.813826  [   32/  126]
train() client id: f_00000-8-1 loss: 0.798233  [   64/  126]
train() client id: f_00000-8-2 loss: 0.651993  [   96/  126]
train() client id: f_00000-9-0 loss: 0.756473  [   32/  126]
train() client id: f_00000-9-1 loss: 0.780003  [   64/  126]
train() client id: f_00000-9-2 loss: 0.711383  [   96/  126]
train() client id: f_00000-10-0 loss: 0.722484  [   32/  126]
train() client id: f_00000-10-1 loss: 0.837688  [   64/  126]
train() client id: f_00000-10-2 loss: 0.717962  [   96/  126]
train() client id: f_00000-11-0 loss: 0.820838  [   32/  126]
train() client id: f_00000-11-1 loss: 0.717616  [   64/  126]
train() client id: f_00000-11-2 loss: 0.674380  [   96/  126]
train() client id: f_00001-0-0 loss: 0.464321  [   32/  265]
train() client id: f_00001-0-1 loss: 0.519875  [   64/  265]
train() client id: f_00001-0-2 loss: 0.341331  [   96/  265]
train() client id: f_00001-0-3 loss: 0.483622  [  128/  265]
train() client id: f_00001-0-4 loss: 0.311448  [  160/  265]
train() client id: f_00001-0-5 loss: 0.325248  [  192/  265]
train() client id: f_00001-0-6 loss: 0.392209  [  224/  265]
train() client id: f_00001-0-7 loss: 0.343836  [  256/  265]
train() client id: f_00001-1-0 loss: 0.402245  [   32/  265]
train() client id: f_00001-1-1 loss: 0.284162  [   64/  265]
train() client id: f_00001-1-2 loss: 0.395216  [   96/  265]
train() client id: f_00001-1-3 loss: 0.393179  [  128/  265]
train() client id: f_00001-1-4 loss: 0.517759  [  160/  265]
train() client id: f_00001-1-5 loss: 0.365925  [  192/  265]
train() client id: f_00001-1-6 loss: 0.353780  [  224/  265]
train() client id: f_00001-1-7 loss: 0.342364  [  256/  265]
train() client id: f_00001-2-0 loss: 0.392709  [   32/  265]
train() client id: f_00001-2-1 loss: 0.279635  [   64/  265]
train() client id: f_00001-2-2 loss: 0.313846  [   96/  265]
train() client id: f_00001-2-3 loss: 0.293975  [  128/  265]
train() client id: f_00001-2-4 loss: 0.398837  [  160/  265]
train() client id: f_00001-2-5 loss: 0.497115  [  192/  265]
train() client id: f_00001-2-6 loss: 0.422754  [  224/  265]
train() client id: f_00001-2-7 loss: 0.435898  [  256/  265]
train() client id: f_00001-3-0 loss: 0.411571  [   32/  265]
train() client id: f_00001-3-1 loss: 0.296767  [   64/  265]
train() client id: f_00001-3-2 loss: 0.302951  [   96/  265]
train() client id: f_00001-3-3 loss: 0.457267  [  128/  265]
train() client id: f_00001-3-4 loss: 0.370141  [  160/  265]
train() client id: f_00001-3-5 loss: 0.345718  [  192/  265]
train() client id: f_00001-3-6 loss: 0.308581  [  224/  265]
train() client id: f_00001-3-7 loss: 0.504908  [  256/  265]
train() client id: f_00001-4-0 loss: 0.394521  [   32/  265]
train() client id: f_00001-4-1 loss: 0.349998  [   64/  265]
train() client id: f_00001-4-2 loss: 0.328611  [   96/  265]
train() client id: f_00001-4-3 loss: 0.357368  [  128/  265]
train() client id: f_00001-4-4 loss: 0.327455  [  160/  265]
train() client id: f_00001-4-5 loss: 0.440476  [  192/  265]
train() client id: f_00001-4-6 loss: 0.449222  [  224/  265]
train() client id: f_00001-4-7 loss: 0.314461  [  256/  265]
train() client id: f_00001-5-0 loss: 0.342677  [   32/  265]
train() client id: f_00001-5-1 loss: 0.303287  [   64/  265]
train() client id: f_00001-5-2 loss: 0.356988  [   96/  265]
train() client id: f_00001-5-3 loss: 0.381648  [  128/  265]
train() client id: f_00001-5-4 loss: 0.279132  [  160/  265]
train() client id: f_00001-5-5 loss: 0.419070  [  192/  265]
train() client id: f_00001-5-6 loss: 0.443152  [  224/  265]
train() client id: f_00001-5-7 loss: 0.399294  [  256/  265]
train() client id: f_00001-6-0 loss: 0.426061  [   32/  265]
train() client id: f_00001-6-1 loss: 0.410589  [   64/  265]
train() client id: f_00001-6-2 loss: 0.265359  [   96/  265]
train() client id: f_00001-6-3 loss: 0.358678  [  128/  265]
train() client id: f_00001-6-4 loss: 0.392742  [  160/  265]
train() client id: f_00001-6-5 loss: 0.387662  [  192/  265]
train() client id: f_00001-6-6 loss: 0.370401  [  224/  265]
train() client id: f_00001-6-7 loss: 0.309734  [  256/  265]
train() client id: f_00001-7-0 loss: 0.370095  [   32/  265]
train() client id: f_00001-7-1 loss: 0.427838  [   64/  265]
train() client id: f_00001-7-2 loss: 0.263833  [   96/  265]
train() client id: f_00001-7-3 loss: 0.369295  [  128/  265]
train() client id: f_00001-7-4 loss: 0.294132  [  160/  265]
train() client id: f_00001-7-5 loss: 0.461749  [  192/  265]
train() client id: f_00001-7-6 loss: 0.254957  [  224/  265]
train() client id: f_00001-7-7 loss: 0.416726  [  256/  265]
train() client id: f_00001-8-0 loss: 0.322582  [   32/  265]
train() client id: f_00001-8-1 loss: 0.258015  [   64/  265]
train() client id: f_00001-8-2 loss: 0.454659  [   96/  265]
train() client id: f_00001-8-3 loss: 0.402667  [  128/  265]
train() client id: f_00001-8-4 loss: 0.393637  [  160/  265]
train() client id: f_00001-8-5 loss: 0.405678  [  192/  265]
train() client id: f_00001-8-6 loss: 0.402543  [  224/  265]
train() client id: f_00001-8-7 loss: 0.259459  [  256/  265]
train() client id: f_00001-9-0 loss: 0.268404  [   32/  265]
train() client id: f_00001-9-1 loss: 0.377108  [   64/  265]
train() client id: f_00001-9-2 loss: 0.363919  [   96/  265]
train() client id: f_00001-9-3 loss: 0.338224  [  128/  265]
train() client id: f_00001-9-4 loss: 0.392797  [  160/  265]
train() client id: f_00001-9-5 loss: 0.491516  [  192/  265]
train() client id: f_00001-9-6 loss: 0.320256  [  224/  265]
train() client id: f_00001-9-7 loss: 0.329836  [  256/  265]
train() client id: f_00001-10-0 loss: 0.307110  [   32/  265]
train() client id: f_00001-10-1 loss: 0.318232  [   64/  265]
train() client id: f_00001-10-2 loss: 0.384688  [   96/  265]
train() client id: f_00001-10-3 loss: 0.321470  [  128/  265]
train() client id: f_00001-10-4 loss: 0.313531  [  160/  265]
train() client id: f_00001-10-5 loss: 0.430529  [  192/  265]
train() client id: f_00001-10-6 loss: 0.489525  [  224/  265]
train() client id: f_00001-10-7 loss: 0.309682  [  256/  265]
train() client id: f_00001-11-0 loss: 0.271619  [   32/  265]
train() client id: f_00001-11-1 loss: 0.264395  [   64/  265]
train() client id: f_00001-11-2 loss: 0.436531  [   96/  265]
train() client id: f_00001-11-3 loss: 0.346798  [  128/  265]
train() client id: f_00001-11-4 loss: 0.302031  [  160/  265]
train() client id: f_00001-11-5 loss: 0.319571  [  192/  265]
train() client id: f_00001-11-6 loss: 0.353055  [  224/  265]
train() client id: f_00001-11-7 loss: 0.450370  [  256/  265]
train() client id: f_00002-0-0 loss: 0.916386  [   32/  124]
train() client id: f_00002-0-1 loss: 1.031129  [   64/  124]
train() client id: f_00002-0-2 loss: 0.848048  [   96/  124]
train() client id: f_00002-1-0 loss: 1.267967  [   32/  124]
train() client id: f_00002-1-1 loss: 0.935495  [   64/  124]
train() client id: f_00002-1-2 loss: 0.881128  [   96/  124]
train() client id: f_00002-2-0 loss: 0.904608  [   32/  124]
train() client id: f_00002-2-1 loss: 1.011261  [   64/  124]
train() client id: f_00002-2-2 loss: 0.833135  [   96/  124]
train() client id: f_00002-3-0 loss: 0.995201  [   32/  124]
train() client id: f_00002-3-1 loss: 0.904076  [   64/  124]
train() client id: f_00002-3-2 loss: 0.880563  [   96/  124]
train() client id: f_00002-4-0 loss: 0.909947  [   32/  124]
train() client id: f_00002-4-1 loss: 0.929881  [   64/  124]
train() client id: f_00002-4-2 loss: 0.881863  [   96/  124]
train() client id: f_00002-5-0 loss: 0.798629  [   32/  124]
train() client id: f_00002-5-1 loss: 1.075427  [   64/  124]
train() client id: f_00002-5-2 loss: 0.897284  [   96/  124]
train() client id: f_00002-6-0 loss: 0.717055  [   32/  124]
train() client id: f_00002-6-1 loss: 1.000433  [   64/  124]
train() client id: f_00002-6-2 loss: 0.798376  [   96/  124]
train() client id: f_00002-7-0 loss: 0.842518  [   32/  124]
train() client id: f_00002-7-1 loss: 0.915530  [   64/  124]
train() client id: f_00002-7-2 loss: 0.990654  [   96/  124]
train() client id: f_00002-8-0 loss: 1.003078  [   32/  124]
train() client id: f_00002-8-1 loss: 0.773941  [   64/  124]
train() client id: f_00002-8-2 loss: 0.868395  [   96/  124]
train() client id: f_00002-9-0 loss: 0.979949  [   32/  124]
train() client id: f_00002-9-1 loss: 0.739316  [   64/  124]
train() client id: f_00002-9-2 loss: 0.934211  [   96/  124]
train() client id: f_00002-10-0 loss: 0.907949  [   32/  124]
train() client id: f_00002-10-1 loss: 1.006328  [   64/  124]
train() client id: f_00002-10-2 loss: 0.772038  [   96/  124]
train() client id: f_00002-11-0 loss: 0.886655  [   32/  124]
train() client id: f_00002-11-1 loss: 0.716340  [   64/  124]
train() client id: f_00002-11-2 loss: 1.187694  [   96/  124]
train() client id: f_00003-0-0 loss: 0.794084  [   32/   43]
train() client id: f_00003-1-0 loss: 0.912443  [   32/   43]
train() client id: f_00003-2-0 loss: 0.794306  [   32/   43]
train() client id: f_00003-3-0 loss: 0.917699  [   32/   43]
train() client id: f_00003-4-0 loss: 0.698128  [   32/   43]
train() client id: f_00003-5-0 loss: 0.677170  [   32/   43]
train() client id: f_00003-6-0 loss: 0.879210  [   32/   43]
train() client id: f_00003-7-0 loss: 0.757089  [   32/   43]
train() client id: f_00003-8-0 loss: 0.980869  [   32/   43]
train() client id: f_00003-9-0 loss: 0.690572  [   32/   43]
train() client id: f_00003-10-0 loss: 0.707529  [   32/   43]
train() client id: f_00003-11-0 loss: 0.751374  [   32/   43]
train() client id: f_00004-0-0 loss: 0.932214  [   32/  306]
train() client id: f_00004-0-1 loss: 0.880261  [   64/  306]
train() client id: f_00004-0-2 loss: 0.891918  [   96/  306]
train() client id: f_00004-0-3 loss: 0.981437  [  128/  306]
train() client id: f_00004-0-4 loss: 0.978826  [  160/  306]
train() client id: f_00004-0-5 loss: 0.946037  [  192/  306]
train() client id: f_00004-0-6 loss: 0.884280  [  224/  306]
train() client id: f_00004-0-7 loss: 0.816730  [  256/  306]
train() client id: f_00004-0-8 loss: 0.841175  [  288/  306]
train() client id: f_00004-1-0 loss: 0.819467  [   32/  306]
train() client id: f_00004-1-1 loss: 0.971175  [   64/  306]
train() client id: f_00004-1-2 loss: 0.930252  [   96/  306]
train() client id: f_00004-1-3 loss: 0.968036  [  128/  306]
train() client id: f_00004-1-4 loss: 0.979167  [  160/  306]
train() client id: f_00004-1-5 loss: 0.711403  [  192/  306]
train() client id: f_00004-1-6 loss: 1.038683  [  224/  306]
train() client id: f_00004-1-7 loss: 0.914804  [  256/  306]
train() client id: f_00004-1-8 loss: 0.930495  [  288/  306]
train() client id: f_00004-2-0 loss: 1.010287  [   32/  306]
train() client id: f_00004-2-1 loss: 0.990388  [   64/  306]
train() client id: f_00004-2-2 loss: 0.743906  [   96/  306]
train() client id: f_00004-2-3 loss: 0.955701  [  128/  306]
train() client id: f_00004-2-4 loss: 0.842433  [  160/  306]
train() client id: f_00004-2-5 loss: 1.057294  [  192/  306]
train() client id: f_00004-2-6 loss: 0.911548  [  224/  306]
train() client id: f_00004-2-7 loss: 0.917580  [  256/  306]
train() client id: f_00004-2-8 loss: 0.807594  [  288/  306]
train() client id: f_00004-3-0 loss: 0.899566  [   32/  306]
train() client id: f_00004-3-1 loss: 0.878756  [   64/  306]
train() client id: f_00004-3-2 loss: 0.968732  [   96/  306]
train() client id: f_00004-3-3 loss: 0.959567  [  128/  306]
train() client id: f_00004-3-4 loss: 0.971326  [  160/  306]
train() client id: f_00004-3-5 loss: 0.814816  [  192/  306]
train() client id: f_00004-3-6 loss: 0.967989  [  224/  306]
train() client id: f_00004-3-7 loss: 0.751250  [  256/  306]
train() client id: f_00004-3-8 loss: 0.918704  [  288/  306]
train() client id: f_00004-4-0 loss: 0.832794  [   32/  306]
train() client id: f_00004-4-1 loss: 0.922479  [   64/  306]
train() client id: f_00004-4-2 loss: 0.829590  [   96/  306]
train() client id: f_00004-4-3 loss: 0.803294  [  128/  306]
train() client id: f_00004-4-4 loss: 1.009239  [  160/  306]
train() client id: f_00004-4-5 loss: 1.002243  [  192/  306]
train() client id: f_00004-4-6 loss: 0.978811  [  224/  306]
train() client id: f_00004-4-7 loss: 0.892799  [  256/  306]
train() client id: f_00004-4-8 loss: 0.870932  [  288/  306]
train() client id: f_00004-5-0 loss: 0.894085  [   32/  306]
train() client id: f_00004-5-1 loss: 0.875038  [   64/  306]
train() client id: f_00004-5-2 loss: 1.005695  [   96/  306]
train() client id: f_00004-5-3 loss: 0.867344  [  128/  306]
train() client id: f_00004-5-4 loss: 0.870119  [  160/  306]
train() client id: f_00004-5-5 loss: 0.820683  [  192/  306]
train() client id: f_00004-5-6 loss: 0.986379  [  224/  306]
train() client id: f_00004-5-7 loss: 1.073903  [  256/  306]
train() client id: f_00004-5-8 loss: 0.745730  [  288/  306]
train() client id: f_00004-6-0 loss: 0.956262  [   32/  306]
train() client id: f_00004-6-1 loss: 0.999417  [   64/  306]
train() client id: f_00004-6-2 loss: 0.814645  [   96/  306]
train() client id: f_00004-6-3 loss: 0.889098  [  128/  306]
train() client id: f_00004-6-4 loss: 0.944199  [  160/  306]
train() client id: f_00004-6-5 loss: 0.898148  [  192/  306]
train() client id: f_00004-6-6 loss: 0.850426  [  224/  306]
train() client id: f_00004-6-7 loss: 0.827752  [  256/  306]
train() client id: f_00004-6-8 loss: 0.873620  [  288/  306]
train() client id: f_00004-7-0 loss: 1.044452  [   32/  306]
train() client id: f_00004-7-1 loss: 0.874198  [   64/  306]
train() client id: f_00004-7-2 loss: 0.917370  [   96/  306]
train() client id: f_00004-7-3 loss: 0.804783  [  128/  306]
train() client id: f_00004-7-4 loss: 0.861731  [  160/  306]
train() client id: f_00004-7-5 loss: 0.923245  [  192/  306]
train() client id: f_00004-7-6 loss: 0.761614  [  224/  306]
train() client id: f_00004-7-7 loss: 0.942271  [  256/  306]
train() client id: f_00004-7-8 loss: 0.942994  [  288/  306]
train() client id: f_00004-8-0 loss: 0.794868  [   32/  306]
train() client id: f_00004-8-1 loss: 0.935062  [   64/  306]
train() client id: f_00004-8-2 loss: 1.038518  [   96/  306]
train() client id: f_00004-8-3 loss: 0.785998  [  128/  306]
train() client id: f_00004-8-4 loss: 0.875772  [  160/  306]
train() client id: f_00004-8-5 loss: 0.825356  [  192/  306]
train() client id: f_00004-8-6 loss: 0.903324  [  224/  306]
train() client id: f_00004-8-7 loss: 0.961120  [  256/  306]
train() client id: f_00004-8-8 loss: 0.978489  [  288/  306]
train() client id: f_00004-9-0 loss: 0.826387  [   32/  306]
train() client id: f_00004-9-1 loss: 0.918349  [   64/  306]
train() client id: f_00004-9-2 loss: 0.783857  [   96/  306]
train() client id: f_00004-9-3 loss: 0.857716  [  128/  306]
train() client id: f_00004-9-4 loss: 0.883949  [  160/  306]
train() client id: f_00004-9-5 loss: 1.007667  [  192/  306]
train() client id: f_00004-9-6 loss: 0.856659  [  224/  306]
train() client id: f_00004-9-7 loss: 1.042926  [  256/  306]
train() client id: f_00004-9-8 loss: 0.903750  [  288/  306]
train() client id: f_00004-10-0 loss: 0.978470  [   32/  306]
train() client id: f_00004-10-1 loss: 0.909156  [   64/  306]
train() client id: f_00004-10-2 loss: 0.875920  [   96/  306]
train() client id: f_00004-10-3 loss: 0.837622  [  128/  306]
train() client id: f_00004-10-4 loss: 0.930587  [  160/  306]
train() client id: f_00004-10-5 loss: 0.996578  [  192/  306]
train() client id: f_00004-10-6 loss: 0.782813  [  224/  306]
train() client id: f_00004-10-7 loss: 0.933271  [  256/  306]
train() client id: f_00004-10-8 loss: 0.866002  [  288/  306]
train() client id: f_00004-11-0 loss: 0.813853  [   32/  306]
train() client id: f_00004-11-1 loss: 0.829421  [   64/  306]
train() client id: f_00004-11-2 loss: 0.850179  [   96/  306]
train() client id: f_00004-11-3 loss: 0.920568  [  128/  306]
train() client id: f_00004-11-4 loss: 1.000448  [  160/  306]
train() client id: f_00004-11-5 loss: 1.024414  [  192/  306]
train() client id: f_00004-11-6 loss: 0.899126  [  224/  306]
train() client id: f_00004-11-7 loss: 0.862140  [  256/  306]
train() client id: f_00004-11-8 loss: 0.806293  [  288/  306]
train() client id: f_00005-0-0 loss: 0.312773  [   32/  146]
train() client id: f_00005-0-1 loss: 0.374487  [   64/  146]
train() client id: f_00005-0-2 loss: 0.573769  [   96/  146]
train() client id: f_00005-0-3 loss: 0.469232  [  128/  146]
train() client id: f_00005-1-0 loss: 0.405188  [   32/  146]
train() client id: f_00005-1-1 loss: 0.494765  [   64/  146]
train() client id: f_00005-1-2 loss: 0.691321  [   96/  146]
train() client id: f_00005-1-3 loss: 0.403867  [  128/  146]
train() client id: f_00005-2-0 loss: 0.761664  [   32/  146]
train() client id: f_00005-2-1 loss: 0.456778  [   64/  146]
train() client id: f_00005-2-2 loss: 0.454902  [   96/  146]
train() client id: f_00005-2-3 loss: 0.190887  [  128/  146]
train() client id: f_00005-3-0 loss: 0.648279  [   32/  146]
train() client id: f_00005-3-1 loss: 0.478232  [   64/  146]
train() client id: f_00005-3-2 loss: 0.406830  [   96/  146]
train() client id: f_00005-3-3 loss: 0.353076  [  128/  146]
train() client id: f_00005-4-0 loss: 0.523975  [   32/  146]
train() client id: f_00005-4-1 loss: 0.629692  [   64/  146]
train() client id: f_00005-4-2 loss: 0.322072  [   96/  146]
train() client id: f_00005-4-3 loss: 0.471411  [  128/  146]
train() client id: f_00005-5-0 loss: 0.337401  [   32/  146]
train() client id: f_00005-5-1 loss: 0.534750  [   64/  146]
train() client id: f_00005-5-2 loss: 0.352571  [   96/  146]
train() client id: f_00005-5-3 loss: 0.613816  [  128/  146]
train() client id: f_00005-6-0 loss: 0.304588  [   32/  146]
train() client id: f_00005-6-1 loss: 0.732960  [   64/  146]
train() client id: f_00005-6-2 loss: 0.291548  [   96/  146]
train() client id: f_00005-6-3 loss: 0.532814  [  128/  146]
train() client id: f_00005-7-0 loss: 0.449376  [   32/  146]
train() client id: f_00005-7-1 loss: 0.408793  [   64/  146]
train() client id: f_00005-7-2 loss: 0.551191  [   96/  146]
train() client id: f_00005-7-3 loss: 0.462641  [  128/  146]
train() client id: f_00005-8-0 loss: 0.340534  [   32/  146]
train() client id: f_00005-8-1 loss: 0.346161  [   64/  146]
train() client id: f_00005-8-2 loss: 0.664093  [   96/  146]
train() client id: f_00005-8-3 loss: 0.486814  [  128/  146]
train() client id: f_00005-9-0 loss: 0.297439  [   32/  146]
train() client id: f_00005-9-1 loss: 0.439941  [   64/  146]
train() client id: f_00005-9-2 loss: 0.607473  [   96/  146]
train() client id: f_00005-9-3 loss: 0.570314  [  128/  146]
train() client id: f_00005-10-0 loss: 0.504078  [   32/  146]
train() client id: f_00005-10-1 loss: 0.735161  [   64/  146]
train() client id: f_00005-10-2 loss: 0.347743  [   96/  146]
train() client id: f_00005-10-3 loss: 0.282116  [  128/  146]
train() client id: f_00005-11-0 loss: 0.499730  [   32/  146]
train() client id: f_00005-11-1 loss: 0.470850  [   64/  146]
train() client id: f_00005-11-2 loss: 0.304842  [   96/  146]
train() client id: f_00005-11-3 loss: 0.760963  [  128/  146]
train() client id: f_00006-0-0 loss: 0.442471  [   32/   54]
train() client id: f_00006-1-0 loss: 0.483876  [   32/   54]
train() client id: f_00006-2-0 loss: 0.543222  [   32/   54]
train() client id: f_00006-3-0 loss: 0.466001  [   32/   54]
train() client id: f_00006-4-0 loss: 0.460515  [   32/   54]
train() client id: f_00006-5-0 loss: 0.501625  [   32/   54]
train() client id: f_00006-6-0 loss: 0.486751  [   32/   54]
train() client id: f_00006-7-0 loss: 0.507499  [   32/   54]
train() client id: f_00006-8-0 loss: 0.539847  [   32/   54]
train() client id: f_00006-9-0 loss: 0.551190  [   32/   54]
train() client id: f_00006-10-0 loss: 0.541445  [   32/   54]
train() client id: f_00006-11-0 loss: 0.507422  [   32/   54]
train() client id: f_00007-0-0 loss: 0.654956  [   32/  179]
train() client id: f_00007-0-1 loss: 0.541551  [   64/  179]
train() client id: f_00007-0-2 loss: 0.752642  [   96/  179]
train() client id: f_00007-0-3 loss: 0.622745  [  128/  179]
train() client id: f_00007-0-4 loss: 0.694980  [  160/  179]
train() client id: f_00007-1-0 loss: 0.532218  [   32/  179]
train() client id: f_00007-1-1 loss: 0.745812  [   64/  179]
train() client id: f_00007-1-2 loss: 0.674115  [   96/  179]
train() client id: f_00007-1-3 loss: 0.591889  [  128/  179]
train() client id: f_00007-1-4 loss: 0.701662  [  160/  179]
train() client id: f_00007-2-0 loss: 0.639175  [   32/  179]
train() client id: f_00007-2-1 loss: 0.578742  [   64/  179]
train() client id: f_00007-2-2 loss: 0.706214  [   96/  179]
train() client id: f_00007-2-3 loss: 0.716058  [  128/  179]
train() client id: f_00007-2-4 loss: 0.496137  [  160/  179]
train() client id: f_00007-3-0 loss: 0.895908  [   32/  179]
train() client id: f_00007-3-1 loss: 0.521526  [   64/  179]
train() client id: f_00007-3-2 loss: 0.613868  [   96/  179]
train() client id: f_00007-3-3 loss: 0.628258  [  128/  179]
train() client id: f_00007-3-4 loss: 0.493934  [  160/  179]
train() client id: f_00007-4-0 loss: 0.480409  [   32/  179]
train() client id: f_00007-4-1 loss: 0.586467  [   64/  179]
train() client id: f_00007-4-2 loss: 0.745489  [   96/  179]
train() client id: f_00007-4-3 loss: 0.604949  [  128/  179]
train() client id: f_00007-4-4 loss: 0.759199  [  160/  179]
train() client id: f_00007-5-0 loss: 0.540243  [   32/  179]
train() client id: f_00007-5-1 loss: 0.706092  [   64/  179]
train() client id: f_00007-5-2 loss: 0.589740  [   96/  179]
train() client id: f_00007-5-3 loss: 0.561890  [  128/  179]
train() client id: f_00007-5-4 loss: 0.735152  [  160/  179]
train() client id: f_00007-6-0 loss: 0.545958  [   32/  179]
train() client id: f_00007-6-1 loss: 0.745702  [   64/  179]
train() client id: f_00007-6-2 loss: 0.524007  [   96/  179]
train() client id: f_00007-6-3 loss: 0.604470  [  128/  179]
train() client id: f_00007-6-4 loss: 0.708882  [  160/  179]
train() client id: f_00007-7-0 loss: 0.583153  [   32/  179]
train() client id: f_00007-7-1 loss: 0.656214  [   64/  179]
train() client id: f_00007-7-2 loss: 0.533715  [   96/  179]
train() client id: f_00007-7-3 loss: 0.569276  [  128/  179]
train() client id: f_00007-7-4 loss: 0.779534  [  160/  179]
train() client id: f_00007-8-0 loss: 0.431313  [   32/  179]
train() client id: f_00007-8-1 loss: 0.629903  [   64/  179]
train() client id: f_00007-8-2 loss: 0.487495  [   96/  179]
train() client id: f_00007-8-3 loss: 0.648934  [  128/  179]
train() client id: f_00007-8-4 loss: 0.746489  [  160/  179]
train() client id: f_00007-9-0 loss: 0.563391  [   32/  179]
train() client id: f_00007-9-1 loss: 0.853018  [   64/  179]
train() client id: f_00007-9-2 loss: 0.466896  [   96/  179]
train() client id: f_00007-9-3 loss: 0.605269  [  128/  179]
train() client id: f_00007-9-4 loss: 0.522065  [  160/  179]
train() client id: f_00007-10-0 loss: 0.602628  [   32/  179]
train() client id: f_00007-10-1 loss: 0.704951  [   64/  179]
train() client id: f_00007-10-2 loss: 0.647897  [   96/  179]
train() client id: f_00007-10-3 loss: 0.650634  [  128/  179]
train() client id: f_00007-10-4 loss: 0.517098  [  160/  179]
train() client id: f_00007-11-0 loss: 0.632353  [   32/  179]
train() client id: f_00007-11-1 loss: 0.444105  [   64/  179]
train() client id: f_00007-11-2 loss: 0.619271  [   96/  179]
train() client id: f_00007-11-3 loss: 0.645602  [  128/  179]
train() client id: f_00007-11-4 loss: 0.598054  [  160/  179]
train() client id: f_00008-0-0 loss: 1.019549  [   32/  130]
train() client id: f_00008-0-1 loss: 0.626400  [   64/  130]
train() client id: f_00008-0-2 loss: 0.753414  [   96/  130]
train() client id: f_00008-0-3 loss: 0.654873  [  128/  130]
train() client id: f_00008-1-0 loss: 0.807032  [   32/  130]
train() client id: f_00008-1-1 loss: 0.682685  [   64/  130]
train() client id: f_00008-1-2 loss: 0.820614  [   96/  130]
train() client id: f_00008-1-3 loss: 0.761223  [  128/  130]
train() client id: f_00008-2-0 loss: 0.856934  [   32/  130]
train() client id: f_00008-2-1 loss: 0.721709  [   64/  130]
train() client id: f_00008-2-2 loss: 0.692989  [   96/  130]
train() client id: f_00008-2-3 loss: 0.781536  [  128/  130]
train() client id: f_00008-3-0 loss: 0.678626  [   32/  130]
train() client id: f_00008-3-1 loss: 0.713610  [   64/  130]
train() client id: f_00008-3-2 loss: 0.889367  [   96/  130]
train() client id: f_00008-3-3 loss: 0.789612  [  128/  130]
train() client id: f_00008-4-0 loss: 0.827447  [   32/  130]
train() client id: f_00008-4-1 loss: 0.771112  [   64/  130]
train() client id: f_00008-4-2 loss: 0.808208  [   96/  130]
train() client id: f_00008-4-3 loss: 0.656474  [  128/  130]
train() client id: f_00008-5-0 loss: 0.805935  [   32/  130]
train() client id: f_00008-5-1 loss: 0.739114  [   64/  130]
train() client id: f_00008-5-2 loss: 0.751255  [   96/  130]
train() client id: f_00008-5-3 loss: 0.765635  [  128/  130]
train() client id: f_00008-6-0 loss: 0.805718  [   32/  130]
train() client id: f_00008-6-1 loss: 0.707503  [   64/  130]
train() client id: f_00008-6-2 loss: 0.844301  [   96/  130]
train() client id: f_00008-6-3 loss: 0.723525  [  128/  130]
train() client id: f_00008-7-0 loss: 0.695728  [   32/  130]
train() client id: f_00008-7-1 loss: 0.729926  [   64/  130]
train() client id: f_00008-7-2 loss: 0.760299  [   96/  130]
train() client id: f_00008-7-3 loss: 0.884353  [  128/  130]
train() client id: f_00008-8-0 loss: 0.753279  [   32/  130]
train() client id: f_00008-8-1 loss: 0.836907  [   64/  130]
train() client id: f_00008-8-2 loss: 0.707507  [   96/  130]
train() client id: f_00008-8-3 loss: 0.779145  [  128/  130]
train() client id: f_00008-9-0 loss: 0.761911  [   32/  130]
train() client id: f_00008-9-1 loss: 0.740242  [   64/  130]
train() client id: f_00008-9-2 loss: 0.833352  [   96/  130]
train() client id: f_00008-9-3 loss: 0.755693  [  128/  130]
train() client id: f_00008-10-0 loss: 0.741168  [   32/  130]
train() client id: f_00008-10-1 loss: 0.855604  [   64/  130]
train() client id: f_00008-10-2 loss: 0.660703  [   96/  130]
train() client id: f_00008-10-3 loss: 0.827814  [  128/  130]
train() client id: f_00008-11-0 loss: 0.790986  [   32/  130]
train() client id: f_00008-11-1 loss: 0.795860  [   64/  130]
train() client id: f_00008-11-2 loss: 0.767599  [   96/  130]
train() client id: f_00008-11-3 loss: 0.738549  [  128/  130]
train() client id: f_00009-0-0 loss: 1.310924  [   32/  118]
train() client id: f_00009-0-1 loss: 1.206615  [   64/  118]
train() client id: f_00009-0-2 loss: 1.025136  [   96/  118]
train() client id: f_00009-1-0 loss: 1.056697  [   32/  118]
train() client id: f_00009-1-1 loss: 1.049692  [   64/  118]
train() client id: f_00009-1-2 loss: 1.056990  [   96/  118]
train() client id: f_00009-2-0 loss: 1.083899  [   32/  118]
train() client id: f_00009-2-1 loss: 0.991827  [   64/  118]
train() client id: f_00009-2-2 loss: 1.051629  [   96/  118]
train() client id: f_00009-3-0 loss: 0.985280  [   32/  118]
train() client id: f_00009-3-1 loss: 0.955800  [   64/  118]
train() client id: f_00009-3-2 loss: 1.065111  [   96/  118]
train() client id: f_00009-4-0 loss: 0.912141  [   32/  118]
train() client id: f_00009-4-1 loss: 1.030290  [   64/  118]
train() client id: f_00009-4-2 loss: 0.907758  [   96/  118]
train() client id: f_00009-5-0 loss: 1.071084  [   32/  118]
train() client id: f_00009-5-1 loss: 0.903110  [   64/  118]
train() client id: f_00009-5-2 loss: 0.916924  [   96/  118]
train() client id: f_00009-6-0 loss: 0.783738  [   32/  118]
train() client id: f_00009-6-1 loss: 1.072228  [   64/  118]
train() client id: f_00009-6-2 loss: 1.084762  [   96/  118]
train() client id: f_00009-7-0 loss: 0.869562  [   32/  118]
train() client id: f_00009-7-1 loss: 0.922840  [   64/  118]
train() client id: f_00009-7-2 loss: 0.961419  [   96/  118]
train() client id: f_00009-8-0 loss: 0.898384  [   32/  118]
train() client id: f_00009-8-1 loss: 0.911605  [   64/  118]
train() client id: f_00009-8-2 loss: 0.941700  [   96/  118]
train() client id: f_00009-9-0 loss: 0.839163  [   32/  118]
train() client id: f_00009-9-1 loss: 0.861701  [   64/  118]
train() client id: f_00009-9-2 loss: 0.972988  [   96/  118]
train() client id: f_00009-10-0 loss: 0.945372  [   32/  118]
train() client id: f_00009-10-1 loss: 0.843611  [   64/  118]
train() client id: f_00009-10-2 loss: 0.909087  [   96/  118]
train() client id: f_00009-11-0 loss: 0.842210  [   32/  118]
train() client id: f_00009-11-1 loss: 0.849771  [   64/  118]
train() client id: f_00009-11-2 loss: 0.995751  [   96/  118]
At round 31 accuracy: 0.6419098143236074
At round 31 training accuracy: 0.5895372233400402
At round 31 training loss: 0.821626542635943
update_location
xs = -3.905658 4.200318 175.009024 18.811294 0.979296 3.956410 -137.443192 -116.324852 159.663977 -102.060879 
ys = 167.587959 150.555839 1.320614 -137.455176 129.350187 112.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: 0.2885738318825702
ys mean: 44.39414253552871
dists_uav = 195.194719 180.789113 201.568605 171.019853 163.500550 150.806783 169.992710 153.401915 189.212197 142.942068 
uav_gains = -107.380487 -106.477667 -107.780992 -105.850671 -105.351902 -104.465407 -105.783494 -104.651882 -107.006694 -103.881395 
uav_gains_db_mean: -105.86305902566426
dists_bs = 171.254816 180.859810 390.731168 367.683362 181.803790 189.453098 181.552056 183.795069 369.843880 185.904838 
bs_gains = -102.109057 -102.772637 -112.139645 -111.400331 -102.835941 -103.337107 -102.819092 -102.968407 -111.471575 -103.107199 
bs_gains_db_mean: -105.49609910233474
Round 32
-------------------------------
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.76786425 14.02640477  6.66672465  2.4004724  16.17773155  7.78673034
  2.97703515  9.52653575  7.02653207  6.31639524]
obj_prev = 79.67242616820144
eta_min = 2.1062834512478834e-14	eta_max = 0.9270669874026624
af = 16.815721006576958	bf = 1.436444871137545	zeta = 18.497293107234654	eta = 0.9090909090909091
af = 16.815721006576958	bf = 1.436444871137545	zeta = 33.430696901951336	eta = 0.5030024069164837
af = 16.815721006576958	bf = 1.436444871137545	zeta = 26.140048449463926	eta = 0.643293413900379
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.823688418099213	eta = 0.677406222772134
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.755287540536187	eta = 0.6792779513888344
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.755088130913464	eta = 0.6792834231754543
eta = 0.6792834231754543
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [0.03191095 0.06711429 0.03140442 0.01089024 0.07749801 0.03697618
 0.01367612 0.0453338  0.03292399 0.02988485]
ene_total = [2.19996663 3.98610054 2.18445289 1.03108082 4.54479709 2.37816282
 1.17905352 2.85331091 2.40505659 1.99310631]
ti_comp = [0.43830311 0.4576972  0.43612707 0.44578269 0.45748595 0.45576916
 0.44608386 0.45085659 0.41023879 0.45656667]
ti_coms = [0.09059974 0.07120565 0.09277578 0.08312016 0.07141689 0.07313368
 0.08281899 0.07804626 0.11866406 0.07233618]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.05718244e-05 9.01921040e-05 1.01771547e-05 4.06206033e-07
 1.38994033e-04 1.52109415e-05 8.03407087e-07 2.86463737e-05
 1.32539102e-05 8.00248765e-06]
ene_total = [0.48705029 0.38718743 0.49871352 0.44634221 0.39094219 0.39351399
 0.44474637 0.42061388 0.63788794 0.38884464]
optimize_network iter = 0 obj = 4.495842450611667
eta = 0.6792834231754543
freqs = [36402828.40748152 73317349.05671008 36003753.27798384 12214744.53533785
 84699880.32505599 40564586.30164858 15329090.24416601 50275192.55652302
 40127838.49551824 32727809.18735842]
eta_min = 0.6792834231754692	eta_max = 0.6792834231754555
af = 0.014083508139639458	bf = 1.436444871137545	zeta = 0.015491858953603405	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [2.60553049e-06 2.22287345e-05 2.50826025e-06 1.00113487e-07
 3.42564517e-05 3.74888670e-06 1.98007609e-07 7.06018162e-06
 3.26655702e-06 1.97229209e-06]
ene_total = [1.71674703 1.35307722 1.75794987 1.57458483 1.35935733 1.38609978
 1.56889819 1.47978715 2.24850168 1.37065582]
ti_comp = [0.43830311 0.4576972  0.43612707 0.44578269 0.45748595 0.45576916
 0.44608386 0.45085659 0.41023879 0.45656667]
ti_coms = [0.09059974 0.07120565 0.09277578 0.08312016 0.07141689 0.07313368
 0.08281899 0.07804626 0.11866406 0.07233618]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.05718244e-05 9.01921040e-05 1.01771547e-05 4.06206033e-07
 1.38994033e-04 1.52109415e-05 8.03407087e-07 2.86463737e-05
 1.32539102e-05 8.00248765e-06]
ene_total = [0.48705029 0.38718743 0.49871352 0.44634221 0.39094219 0.39351399
 0.44474637 0.42061388 0.63788794 0.38884464]
optimize_network iter = 1 obj = 4.495842450611684
eta = 0.6792834231754555
freqs = [36402828.40748152 73317349.05671008 36003753.27798384 12214744.53533785
 84699880.32505599 40564586.30164856 15329090.24416601 50275192.55652302
 40127838.49551825 32727809.18735842]
Done!
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.00182543e-05 8.54693954e-05 9.64425066e-06 3.84935959e-07
 1.31715920e-04 1.44144544e-05 7.61338465e-07 2.71463702e-05
 1.25598987e-05 7.58345522e-06]
ene_total = [0.00906999 0.00720603 0.00928722 0.0083124  0.00727341 0.00732778
 0.00828266 0.00783177 0.01187897 0.0072412 ]
At round 32 energy consumption: 0.08371143671629988
At round 32 eta: 0.6792834231754555
At round 32 a_n: 17.221135753236222
At round 32 local rounds: 12.663073636613152
At round 32 global rounds: 53.69580806749958
gradient difference: 0.4307192862033844
train() client id: f_00000-0-0 loss: 1.053437  [   32/  126]
train() client id: f_00000-0-1 loss: 1.152453  [   64/  126]
train() client id: f_00000-0-2 loss: 0.924629  [   96/  126]
train() client id: f_00000-1-0 loss: 1.183676  [   32/  126]
train() client id: f_00000-1-1 loss: 1.029769  [   64/  126]
train() client id: f_00000-1-2 loss: 0.903790  [   96/  126]
train() client id: f_00000-2-0 loss: 0.974619  [   32/  126]
train() client id: f_00000-2-1 loss: 1.046920  [   64/  126]
train() client id: f_00000-2-2 loss: 0.826510  [   96/  126]
train() client id: f_00000-3-0 loss: 1.139376  [   32/  126]
train() client id: f_00000-3-1 loss: 0.766398  [   64/  126]
train() client id: f_00000-3-2 loss: 0.844792  [   96/  126]
train() client id: f_00000-4-0 loss: 0.945594  [   32/  126]
train() client id: f_00000-4-1 loss: 0.896623  [   64/  126]
train() client id: f_00000-4-2 loss: 0.860742  [   96/  126]
train() client id: f_00000-5-0 loss: 1.039405  [   32/  126]
train() client id: f_00000-5-1 loss: 0.885667  [   64/  126]
train() client id: f_00000-5-2 loss: 0.901692  [   96/  126]
train() client id: f_00000-6-0 loss: 1.079769  [   32/  126]
train() client id: f_00000-6-1 loss: 0.853619  [   64/  126]
train() client id: f_00000-6-2 loss: 0.892417  [   96/  126]
train() client id: f_00000-7-0 loss: 0.989136  [   32/  126]
train() client id: f_00000-7-1 loss: 0.922346  [   64/  126]
train() client id: f_00000-7-2 loss: 0.961165  [   96/  126]
train() client id: f_00000-8-0 loss: 0.945190  [   32/  126]
train() client id: f_00000-8-1 loss: 0.836301  [   64/  126]
train() client id: f_00000-8-2 loss: 0.924351  [   96/  126]
train() client id: f_00000-9-0 loss: 0.912862  [   32/  126]
train() client id: f_00000-9-1 loss: 0.936328  [   64/  126]
train() client id: f_00000-9-2 loss: 0.887772  [   96/  126]
train() client id: f_00000-10-0 loss: 0.998456  [   32/  126]
train() client id: f_00000-10-1 loss: 0.884530  [   64/  126]
train() client id: f_00000-10-2 loss: 0.915734  [   96/  126]
train() client id: f_00000-11-0 loss: 0.856953  [   32/  126]
train() client id: f_00000-11-1 loss: 0.895286  [   64/  126]
train() client id: f_00000-11-2 loss: 0.977510  [   96/  126]
train() client id: f_00001-0-0 loss: 0.339529  [   32/  265]
train() client id: f_00001-0-1 loss: 0.407023  [   64/  265]
train() client id: f_00001-0-2 loss: 0.362841  [   96/  265]
train() client id: f_00001-0-3 loss: 0.365091  [  128/  265]
train() client id: f_00001-0-4 loss: 0.360639  [  160/  265]
train() client id: f_00001-0-5 loss: 0.403765  [  192/  265]
train() client id: f_00001-0-6 loss: 0.383803  [  224/  265]
train() client id: f_00001-0-7 loss: 0.373259  [  256/  265]
train() client id: f_00001-1-0 loss: 0.375173  [   32/  265]
train() client id: f_00001-1-1 loss: 0.417154  [   64/  265]
train() client id: f_00001-1-2 loss: 0.383272  [   96/  265]
train() client id: f_00001-1-3 loss: 0.329662  [  128/  265]
train() client id: f_00001-1-4 loss: 0.424682  [  160/  265]
train() client id: f_00001-1-5 loss: 0.361130  [  192/  265]
train() client id: f_00001-1-6 loss: 0.244568  [  224/  265]
train() client id: f_00001-1-7 loss: 0.358093  [  256/  265]
train() client id: f_00001-2-0 loss: 0.381747  [   32/  265]
train() client id: f_00001-2-1 loss: 0.335516  [   64/  265]
train() client id: f_00001-2-2 loss: 0.257779  [   96/  265]
train() client id: f_00001-2-3 loss: 0.269724  [  128/  265]
train() client id: f_00001-2-4 loss: 0.500476  [  160/  265]
train() client id: f_00001-2-5 loss: 0.296881  [  192/  265]
train() client id: f_00001-2-6 loss: 0.394833  [  224/  265]
train() client id: f_00001-2-7 loss: 0.389514  [  256/  265]
train() client id: f_00001-3-0 loss: 0.373351  [   32/  265]
train() client id: f_00001-3-1 loss: 0.316966  [   64/  265]
train() client id: f_00001-3-2 loss: 0.264153  [   96/  265]
train() client id: f_00001-3-3 loss: 0.317164  [  128/  265]
train() client id: f_00001-3-4 loss: 0.246019  [  160/  265]
train() client id: f_00001-3-5 loss: 0.477795  [  192/  265]
train() client id: f_00001-3-6 loss: 0.266007  [  224/  265]
train() client id: f_00001-3-7 loss: 0.321163  [  256/  265]
train() client id: f_00001-4-0 loss: 0.276764  [   32/  265]
train() client id: f_00001-4-1 loss: 0.336745  [   64/  265]
train() client id: f_00001-4-2 loss: 0.267448  [   96/  265]
train() client id: f_00001-4-3 loss: 0.365730  [  128/  265]
train() client id: f_00001-4-4 loss: 0.407076  [  160/  265]
train() client id: f_00001-4-5 loss: 0.332098  [  192/  265]
train() client id: f_00001-4-6 loss: 0.376282  [  224/  265]
train() client id: f_00001-4-7 loss: 0.356522  [  256/  265]
train() client id: f_00001-5-0 loss: 0.319770  [   32/  265]
train() client id: f_00001-5-1 loss: 0.392351  [   64/  265]
train() client id: f_00001-5-2 loss: 0.386715  [   96/  265]
train() client id: f_00001-5-3 loss: 0.403472  [  128/  265]
train() client id: f_00001-5-4 loss: 0.266295  [  160/  265]
train() client id: f_00001-5-5 loss: 0.436255  [  192/  265]
train() client id: f_00001-5-6 loss: 0.234617  [  224/  265]
train() client id: f_00001-5-7 loss: 0.254694  [  256/  265]
train() client id: f_00001-6-0 loss: 0.341964  [   32/  265]
train() client id: f_00001-6-1 loss: 0.245874  [   64/  265]
train() client id: f_00001-6-2 loss: 0.228560  [   96/  265]
train() client id: f_00001-6-3 loss: 0.441154  [  128/  265]
train() client id: f_00001-6-4 loss: 0.283614  [  160/  265]
train() client id: f_00001-6-5 loss: 0.303090  [  192/  265]
train() client id: f_00001-6-6 loss: 0.283682  [  224/  265]
train() client id: f_00001-6-7 loss: 0.445618  [  256/  265]
train() client id: f_00001-7-0 loss: 0.289420  [   32/  265]
train() client id: f_00001-7-1 loss: 0.306975  [   64/  265]
train() client id: f_00001-7-2 loss: 0.328726  [   96/  265]
train() client id: f_00001-7-3 loss: 0.372605  [  128/  265]
train() client id: f_00001-7-4 loss: 0.375414  [  160/  265]
train() client id: f_00001-7-5 loss: 0.345419  [  192/  265]
train() client id: f_00001-7-6 loss: 0.378498  [  224/  265]
train() client id: f_00001-7-7 loss: 0.224397  [  256/  265]
train() client id: f_00001-8-0 loss: 0.310265  [   32/  265]
train() client id: f_00001-8-1 loss: 0.373563  [   64/  265]
train() client id: f_00001-8-2 loss: 0.208034  [   96/  265]
train() client id: f_00001-8-3 loss: 0.360254  [  128/  265]
train() client id: f_00001-8-4 loss: 0.284597  [  160/  265]
train() client id: f_00001-8-5 loss: 0.220359  [  192/  265]
train() client id: f_00001-8-6 loss: 0.410408  [  224/  265]
train() client id: f_00001-8-7 loss: 0.386087  [  256/  265]
train() client id: f_00001-9-0 loss: 0.232366  [   32/  265]
train() client id: f_00001-9-1 loss: 0.432707  [   64/  265]
train() client id: f_00001-9-2 loss: 0.376159  [   96/  265]
train() client id: f_00001-9-3 loss: 0.244948  [  128/  265]
train() client id: f_00001-9-4 loss: 0.218388  [  160/  265]
train() client id: f_00001-9-5 loss: 0.321669  [  192/  265]
train() client id: f_00001-9-6 loss: 0.376543  [  224/  265]
train() client id: f_00001-9-7 loss: 0.260142  [  256/  265]
train() client id: f_00001-10-0 loss: 0.241336  [   32/  265]
train() client id: f_00001-10-1 loss: 0.361267  [   64/  265]
train() client id: f_00001-10-2 loss: 0.310715  [   96/  265]
train() client id: f_00001-10-3 loss: 0.249384  [  128/  265]
train() client id: f_00001-10-4 loss: 0.446930  [  160/  265]
train() client id: f_00001-10-5 loss: 0.278640  [  192/  265]
train() client id: f_00001-10-6 loss: 0.322676  [  224/  265]
train() client id: f_00001-10-7 loss: 0.321253  [  256/  265]
train() client id: f_00001-11-0 loss: 0.237411  [   32/  265]
train() client id: f_00001-11-1 loss: 0.225377  [   64/  265]
train() client id: f_00001-11-2 loss: 0.198783  [   96/  265]
train() client id: f_00001-11-3 loss: 0.262793  [  128/  265]
train() client id: f_00001-11-4 loss: 0.417333  [  160/  265]
train() client id: f_00001-11-5 loss: 0.422947  [  192/  265]
train() client id: f_00001-11-6 loss: 0.351424  [  224/  265]
train() client id: f_00001-11-7 loss: 0.382486  [  256/  265]
train() client id: f_00002-0-0 loss: 1.108730  [   32/  124]
train() client id: f_00002-0-1 loss: 0.899448  [   64/  124]
train() client id: f_00002-0-2 loss: 0.804993  [   96/  124]
train() client id: f_00002-1-0 loss: 0.893779  [   32/  124]
train() client id: f_00002-1-1 loss: 0.950457  [   64/  124]
train() client id: f_00002-1-2 loss: 0.817138  [   96/  124]
train() client id: f_00002-2-0 loss: 0.800721  [   32/  124]
train() client id: f_00002-2-1 loss: 0.809507  [   64/  124]
train() client id: f_00002-2-2 loss: 0.882541  [   96/  124]
train() client id: f_00002-3-0 loss: 0.796746  [   32/  124]
train() client id: f_00002-3-1 loss: 0.762614  [   64/  124]
train() client id: f_00002-3-2 loss: 0.849170  [   96/  124]
train() client id: f_00002-4-0 loss: 0.800564  [   32/  124]
train() client id: f_00002-4-1 loss: 0.781094  [   64/  124]
train() client id: f_00002-4-2 loss: 0.756964  [   96/  124]
train() client id: f_00002-5-0 loss: 0.919628  [   32/  124]
train() client id: f_00002-5-1 loss: 0.655585  [   64/  124]
train() client id: f_00002-5-2 loss: 0.636632  [   96/  124]
train() client id: f_00002-6-0 loss: 0.785263  [   32/  124]
train() client id: f_00002-6-1 loss: 0.736494  [   64/  124]
train() client id: f_00002-6-2 loss: 0.552158  [   96/  124]
train() client id: f_00002-7-0 loss: 0.625430  [   32/  124]
train() client id: f_00002-7-1 loss: 0.585862  [   64/  124]
train() client id: f_00002-7-2 loss: 0.883629  [   96/  124]
train() client id: f_00002-8-0 loss: 0.606237  [   32/  124]
train() client id: f_00002-8-1 loss: 0.669702  [   64/  124]
train() client id: f_00002-8-2 loss: 0.684425  [   96/  124]
train() client id: f_00002-9-0 loss: 0.738138  [   32/  124]
train() client id: f_00002-9-1 loss: 0.657935  [   64/  124]
train() client id: f_00002-9-2 loss: 0.492250  [   96/  124]
train() client id: f_00002-10-0 loss: 0.635954  [   32/  124]
train() client id: f_00002-10-1 loss: 0.670401  [   64/  124]
train() client id: f_00002-10-2 loss: 0.393955  [   96/  124]
train() client id: f_00002-11-0 loss: 0.698873  [   32/  124]
train() client id: f_00002-11-1 loss: 0.780432  [   64/  124]
train() client id: f_00002-11-2 loss: 0.555703  [   96/  124]
train() client id: f_00003-0-0 loss: 0.870195  [   32/   43]
train() client id: f_00003-1-0 loss: 0.722089  [   32/   43]
train() client id: f_00003-2-0 loss: 0.870971  [   32/   43]
train() client id: f_00003-3-0 loss: 0.737358  [   32/   43]
train() client id: f_00003-4-0 loss: 0.720272  [   32/   43]
train() client id: f_00003-5-0 loss: 0.602371  [   32/   43]
train() client id: f_00003-6-0 loss: 0.675860  [   32/   43]
train() client id: f_00003-7-0 loss: 0.651765  [   32/   43]
train() client id: f_00003-8-0 loss: 0.733153  [   32/   43]
train() client id: f_00003-9-0 loss: 0.809879  [   32/   43]
train() client id: f_00003-10-0 loss: 0.819380  [   32/   43]
train() client id: f_00003-11-0 loss: 0.787402  [   32/   43]
train() client id: f_00004-0-0 loss: 0.801856  [   32/  306]
train() client id: f_00004-0-1 loss: 0.897556  [   64/  306]
train() client id: f_00004-0-2 loss: 0.885749  [   96/  306]
train() client id: f_00004-0-3 loss: 1.084911  [  128/  306]
train() client id: f_00004-0-4 loss: 1.065379  [  160/  306]
train() client id: f_00004-0-5 loss: 0.820290  [  192/  306]
train() client id: f_00004-0-6 loss: 0.828402  [  224/  306]
train() client id: f_00004-0-7 loss: 0.957109  [  256/  306]
train() client id: f_00004-0-8 loss: 0.978812  [  288/  306]
train() client id: f_00004-1-0 loss: 0.872146  [   32/  306]
train() client id: f_00004-1-1 loss: 0.860196  [   64/  306]
train() client id: f_00004-1-2 loss: 0.901828  [   96/  306]
train() client id: f_00004-1-3 loss: 0.896305  [  128/  306]
train() client id: f_00004-1-4 loss: 1.039971  [  160/  306]
train() client id: f_00004-1-5 loss: 0.959424  [  192/  306]
train() client id: f_00004-1-6 loss: 1.068962  [  224/  306]
train() client id: f_00004-1-7 loss: 0.879847  [  256/  306]
train() client id: f_00004-1-8 loss: 0.864216  [  288/  306]
train() client id: f_00004-2-0 loss: 0.926658  [   32/  306]
train() client id: f_00004-2-1 loss: 0.948077  [   64/  306]
train() client id: f_00004-2-2 loss: 0.762288  [   96/  306]
train() client id: f_00004-2-3 loss: 0.985239  [  128/  306]
train() client id: f_00004-2-4 loss: 0.971663  [  160/  306]
train() client id: f_00004-2-5 loss: 0.835745  [  192/  306]
train() client id: f_00004-2-6 loss: 0.869983  [  224/  306]
train() client id: f_00004-2-7 loss: 1.121596  [  256/  306]
train() client id: f_00004-2-8 loss: 0.914855  [  288/  306]
train() client id: f_00004-3-0 loss: 0.960365  [   32/  306]
train() client id: f_00004-3-1 loss: 0.808941  [   64/  306]
train() client id: f_00004-3-2 loss: 0.936566  [   96/  306]
train() client id: f_00004-3-3 loss: 0.908456  [  128/  306]
train() client id: f_00004-3-4 loss: 0.999645  [  160/  306]
train() client id: f_00004-3-5 loss: 0.972152  [  192/  306]
train() client id: f_00004-3-6 loss: 0.886540  [  224/  306]
train() client id: f_00004-3-7 loss: 0.792401  [  256/  306]
train() client id: f_00004-3-8 loss: 0.962999  [  288/  306]
train() client id: f_00004-4-0 loss: 0.827889  [   32/  306]
train() client id: f_00004-4-1 loss: 1.067707  [   64/  306]
train() client id: f_00004-4-2 loss: 0.796086  [   96/  306]
train() client id: f_00004-4-3 loss: 0.945392  [  128/  306]
train() client id: f_00004-4-4 loss: 0.968737  [  160/  306]
train() client id: f_00004-4-5 loss: 0.797992  [  192/  306]
train() client id: f_00004-4-6 loss: 0.921924  [  224/  306]
train() client id: f_00004-4-7 loss: 0.905937  [  256/  306]
train() client id: f_00004-4-8 loss: 1.069167  [  288/  306]
train() client id: f_00004-5-0 loss: 0.874171  [   32/  306]
train() client id: f_00004-5-1 loss: 0.852382  [   64/  306]
train() client id: f_00004-5-2 loss: 0.976067  [   96/  306]
train() client id: f_00004-5-3 loss: 0.866140  [  128/  306]
train() client id: f_00004-5-4 loss: 0.943251  [  160/  306]
train() client id: f_00004-5-5 loss: 0.872830  [  192/  306]
train() client id: f_00004-5-6 loss: 0.844796  [  224/  306]
train() client id: f_00004-5-7 loss: 0.956867  [  256/  306]
train() client id: f_00004-5-8 loss: 0.920874  [  288/  306]
train() client id: f_00004-6-0 loss: 0.925752  [   32/  306]
train() client id: f_00004-6-1 loss: 0.881269  [   64/  306]
train() client id: f_00004-6-2 loss: 0.875319  [   96/  306]
train() client id: f_00004-6-3 loss: 0.875255  [  128/  306]
train() client id: f_00004-6-4 loss: 0.890860  [  160/  306]
train() client id: f_00004-6-5 loss: 0.933726  [  192/  306]
train() client id: f_00004-6-6 loss: 0.849419  [  224/  306]
train() client id: f_00004-6-7 loss: 0.872928  [  256/  306]
train() client id: f_00004-6-8 loss: 0.985198  [  288/  306]
train() client id: f_00004-7-0 loss: 0.909029  [   32/  306]
train() client id: f_00004-7-1 loss: 0.965208  [   64/  306]
train() client id: f_00004-7-2 loss: 0.849184  [   96/  306]
train() client id: f_00004-7-3 loss: 0.920578  [  128/  306]
train() client id: f_00004-7-4 loss: 0.894719  [  160/  306]
train() client id: f_00004-7-5 loss: 0.858036  [  192/  306]
train() client id: f_00004-7-6 loss: 0.977287  [  224/  306]
train() client id: f_00004-7-7 loss: 0.920168  [  256/  306]
train() client id: f_00004-7-8 loss: 0.867588  [  288/  306]
train() client id: f_00004-8-0 loss: 0.956691  [   32/  306]
train() client id: f_00004-8-1 loss: 0.907158  [   64/  306]
train() client id: f_00004-8-2 loss: 0.798519  [   96/  306]
train() client id: f_00004-8-3 loss: 0.821438  [  128/  306]
train() client id: f_00004-8-4 loss: 0.899268  [  160/  306]
train() client id: f_00004-8-5 loss: 0.962340  [  192/  306]
train() client id: f_00004-8-6 loss: 0.946580  [  224/  306]
train() client id: f_00004-8-7 loss: 0.800934  [  256/  306]
train() client id: f_00004-8-8 loss: 0.956015  [  288/  306]
train() client id: f_00004-9-0 loss: 0.905435  [   32/  306]
train() client id: f_00004-9-1 loss: 0.973711  [   64/  306]
train() client id: f_00004-9-2 loss: 0.758968  [   96/  306]
train() client id: f_00004-9-3 loss: 0.880927  [  128/  306]
train() client id: f_00004-9-4 loss: 0.835029  [  160/  306]
train() client id: f_00004-9-5 loss: 0.822703  [  192/  306]
train() client id: f_00004-9-6 loss: 0.878333  [  224/  306]
train() client id: f_00004-9-7 loss: 1.018666  [  256/  306]
train() client id: f_00004-9-8 loss: 0.908222  [  288/  306]
train() client id: f_00004-10-0 loss: 0.880195  [   32/  306]
train() client id: f_00004-10-1 loss: 0.965120  [   64/  306]
train() client id: f_00004-10-2 loss: 0.975372  [   96/  306]
train() client id: f_00004-10-3 loss: 1.017698  [  128/  306]
train() client id: f_00004-10-4 loss: 0.834136  [  160/  306]
train() client id: f_00004-10-5 loss: 0.911881  [  192/  306]
train() client id: f_00004-10-6 loss: 0.853530  [  224/  306]
train() client id: f_00004-10-7 loss: 0.938812  [  256/  306]
train() client id: f_00004-10-8 loss: 0.740605  [  288/  306]
train() client id: f_00004-11-0 loss: 0.989370  [   32/  306]
train() client id: f_00004-11-1 loss: 0.835842  [   64/  306]
train() client id: f_00004-11-2 loss: 0.955000  [   96/  306]
train() client id: f_00004-11-3 loss: 0.861924  [  128/  306]
train() client id: f_00004-11-4 loss: 0.855649  [  160/  306]
train() client id: f_00004-11-5 loss: 0.981640  [  192/  306]
train() client id: f_00004-11-6 loss: 0.873008  [  224/  306]
train() client id: f_00004-11-7 loss: 0.803907  [  256/  306]
train() client id: f_00004-11-8 loss: 0.907182  [  288/  306]
train() client id: f_00005-0-0 loss: 0.406049  [   32/  146]
train() client id: f_00005-0-1 loss: 0.421546  [   64/  146]
train() client id: f_00005-0-2 loss: 0.298282  [   96/  146]
train() client id: f_00005-0-3 loss: 0.433012  [  128/  146]
train() client id: f_00005-1-0 loss: 0.484869  [   32/  146]
train() client id: f_00005-1-1 loss: 0.417885  [   64/  146]
train() client id: f_00005-1-2 loss: 0.330569  [   96/  146]
train() client id: f_00005-1-3 loss: 0.373359  [  128/  146]
train() client id: f_00005-2-0 loss: 0.421180  [   32/  146]
train() client id: f_00005-2-1 loss: 0.455558  [   64/  146]
train() client id: f_00005-2-2 loss: 0.332416  [   96/  146]
train() client id: f_00005-2-3 loss: 0.368150  [  128/  146]
train() client id: f_00005-3-0 loss: 0.440412  [   32/  146]
train() client id: f_00005-3-1 loss: 0.393375  [   64/  146]
train() client id: f_00005-3-2 loss: 0.399555  [   96/  146]
train() client id: f_00005-3-3 loss: 0.422225  [  128/  146]
train() client id: f_00005-4-0 loss: 0.480771  [   32/  146]
train() client id: f_00005-4-1 loss: 0.198084  [   64/  146]
train() client id: f_00005-4-2 loss: 0.545363  [   96/  146]
train() client id: f_00005-4-3 loss: 0.558148  [  128/  146]
train() client id: f_00005-5-0 loss: 0.239279  [   32/  146]
train() client id: f_00005-5-1 loss: 0.347022  [   64/  146]
train() client id: f_00005-5-2 loss: 0.557327  [   96/  146]
train() client id: f_00005-5-3 loss: 0.533562  [  128/  146]
train() client id: f_00005-6-0 loss: 0.600465  [   32/  146]
train() client id: f_00005-6-1 loss: 0.310981  [   64/  146]
train() client id: f_00005-6-2 loss: 0.494765  [   96/  146]
train() client id: f_00005-6-3 loss: 0.237315  [  128/  146]
train() client id: f_00005-7-0 loss: 0.477665  [   32/  146]
train() client id: f_00005-7-1 loss: 0.277669  [   64/  146]
train() client id: f_00005-7-2 loss: 0.390692  [   96/  146]
train() client id: f_00005-7-3 loss: 0.357102  [  128/  146]
train() client id: f_00005-8-0 loss: 0.190971  [   32/  146]
train() client id: f_00005-8-1 loss: 0.316188  [   64/  146]
train() client id: f_00005-8-2 loss: 0.474281  [   96/  146]
train() client id: f_00005-8-3 loss: 0.708614  [  128/  146]
train() client id: f_00005-9-0 loss: 0.298120  [   32/  146]
train() client id: f_00005-9-1 loss: 0.817880  [   64/  146]
train() client id: f_00005-9-2 loss: 0.109580  [   96/  146]
train() client id: f_00005-9-3 loss: 0.291874  [  128/  146]
train() client id: f_00005-10-0 loss: 0.204831  [   32/  146]
train() client id: f_00005-10-1 loss: 0.122180  [   64/  146]
train() client id: f_00005-10-2 loss: 0.510718  [   96/  146]
train() client id: f_00005-10-3 loss: 0.610209  [  128/  146]
train() client id: f_00005-11-0 loss: 0.426622  [   32/  146]
train() client id: f_00005-11-1 loss: 0.258864  [   64/  146]
train() client id: f_00005-11-2 loss: 0.515852  [   96/  146]
train() client id: f_00005-11-3 loss: 0.444172  [  128/  146]
train() client id: f_00006-0-0 loss: 0.490547  [   32/   54]
train() client id: f_00006-1-0 loss: 0.434168  [   32/   54]
train() client id: f_00006-2-0 loss: 0.481972  [   32/   54]
train() client id: f_00006-3-0 loss: 0.551729  [   32/   54]
train() client id: f_00006-4-0 loss: 0.454675  [   32/   54]
train() client id: f_00006-5-0 loss: 0.501294  [   32/   54]
train() client id: f_00006-6-0 loss: 0.477715  [   32/   54]
train() client id: f_00006-7-0 loss: 0.436864  [   32/   54]
train() client id: f_00006-8-0 loss: 0.545082  [   32/   54]
train() client id: f_00006-9-0 loss: 0.492571  [   32/   54]
train() client id: f_00006-10-0 loss: 0.472497  [   32/   54]
train() client id: f_00006-11-0 loss: 0.497342  [   32/   54]
train() client id: f_00007-0-0 loss: 0.716569  [   32/  179]
train() client id: f_00007-0-1 loss: 0.591672  [   64/  179]
train() client id: f_00007-0-2 loss: 0.640773  [   96/  179]
train() client id: f_00007-0-3 loss: 0.496629  [  128/  179]
train() client id: f_00007-0-4 loss: 0.591905  [  160/  179]
train() client id: f_00007-1-0 loss: 0.529804  [   32/  179]
train() client id: f_00007-1-1 loss: 0.652274  [   64/  179]
train() client id: f_00007-1-2 loss: 0.572889  [   96/  179]
train() client id: f_00007-1-3 loss: 0.542709  [  128/  179]
train() client id: f_00007-1-4 loss: 0.676268  [  160/  179]
train() client id: f_00007-2-0 loss: 0.721190  [   32/  179]
train() client id: f_00007-2-1 loss: 0.668011  [   64/  179]
train() client id: f_00007-2-2 loss: 0.441573  [   96/  179]
train() client id: f_00007-2-3 loss: 0.530799  [  128/  179]
train() client id: f_00007-2-4 loss: 0.605816  [  160/  179]
train() client id: f_00007-3-0 loss: 0.500746  [   32/  179]
train() client id: f_00007-3-1 loss: 0.572022  [   64/  179]
train() client id: f_00007-3-2 loss: 0.482675  [   96/  179]
train() client id: f_00007-3-3 loss: 0.908501  [  128/  179]
train() client id: f_00007-3-4 loss: 0.578366  [  160/  179]
train() client id: f_00007-4-0 loss: 0.614598  [   32/  179]
train() client id: f_00007-4-1 loss: 0.426739  [   64/  179]
train() client id: f_00007-4-2 loss: 0.461155  [   96/  179]
train() client id: f_00007-4-3 loss: 0.466660  [  128/  179]
train() client id: f_00007-4-4 loss: 0.880501  [  160/  179]
train() client id: f_00007-5-0 loss: 0.588840  [   32/  179]
train() client id: f_00007-5-1 loss: 0.752016  [   64/  179]
train() client id: f_00007-5-2 loss: 0.440224  [   96/  179]
train() client id: f_00007-5-3 loss: 0.658215  [  128/  179]
train() client id: f_00007-5-4 loss: 0.563044  [  160/  179]
train() client id: f_00007-6-0 loss: 0.544780  [   32/  179]
train() client id: f_00007-6-1 loss: 0.519093  [   64/  179]
train() client id: f_00007-6-2 loss: 0.515021  [   96/  179]
train() client id: f_00007-6-3 loss: 0.755523  [  128/  179]
train() client id: f_00007-6-4 loss: 0.588377  [  160/  179]
train() client id: f_00007-7-0 loss: 0.451076  [   32/  179]
train() client id: f_00007-7-1 loss: 0.670739  [   64/  179]
train() client id: f_00007-7-2 loss: 0.618368  [   96/  179]
train() client id: f_00007-7-3 loss: 0.649099  [  128/  179]
train() client id: f_00007-7-4 loss: 0.514782  [  160/  179]
train() client id: f_00007-8-0 loss: 0.387399  [   32/  179]
train() client id: f_00007-8-1 loss: 0.452092  [   64/  179]
train() client id: f_00007-8-2 loss: 0.723332  [   96/  179]
train() client id: f_00007-8-3 loss: 0.439722  [  128/  179]
train() client id: f_00007-8-4 loss: 0.895761  [  160/  179]
train() client id: f_00007-9-0 loss: 0.715657  [   32/  179]
train() client id: f_00007-9-1 loss: 0.410612  [   64/  179]
train() client id: f_00007-9-2 loss: 0.671939  [   96/  179]
train() client id: f_00007-9-3 loss: 0.526015  [  128/  179]
train() client id: f_00007-9-4 loss: 0.537426  [  160/  179]
train() client id: f_00007-10-0 loss: 0.571824  [   32/  179]
train() client id: f_00007-10-1 loss: 0.590474  [   64/  179]
train() client id: f_00007-10-2 loss: 0.828821  [   96/  179]
train() client id: f_00007-10-3 loss: 0.528754  [  128/  179]
train() client id: f_00007-10-4 loss: 0.489229  [  160/  179]
train() client id: f_00007-11-0 loss: 0.441076  [   32/  179]
train() client id: f_00007-11-1 loss: 0.500406  [   64/  179]
train() client id: f_00007-11-2 loss: 0.530893  [   96/  179]
train() client id: f_00007-11-3 loss: 0.815863  [  128/  179]
train() client id: f_00007-11-4 loss: 0.423827  [  160/  179]
train() client id: f_00008-0-0 loss: 0.723191  [   32/  130]
train() client id: f_00008-0-1 loss: 0.839053  [   64/  130]
train() client id: f_00008-0-2 loss: 0.749576  [   96/  130]
train() client id: f_00008-0-3 loss: 0.740834  [  128/  130]
train() client id: f_00008-1-0 loss: 0.782433  [   32/  130]
train() client id: f_00008-1-1 loss: 0.826234  [   64/  130]
train() client id: f_00008-1-2 loss: 0.761706  [   96/  130]
train() client id: f_00008-1-3 loss: 0.670567  [  128/  130]
train() client id: f_00008-2-0 loss: 0.696689  [   32/  130]
train() client id: f_00008-2-1 loss: 0.775147  [   64/  130]
train() client id: f_00008-2-2 loss: 0.757198  [   96/  130]
train() client id: f_00008-2-3 loss: 0.801338  [  128/  130]
train() client id: f_00008-3-0 loss: 0.795445  [   32/  130]
train() client id: f_00008-3-1 loss: 0.804409  [   64/  130]
train() client id: f_00008-3-2 loss: 0.733882  [   96/  130]
train() client id: f_00008-3-3 loss: 0.683852  [  128/  130]
train() client id: f_00008-4-0 loss: 0.730098  [   32/  130]
train() client id: f_00008-4-1 loss: 0.731641  [   64/  130]
train() client id: f_00008-4-2 loss: 0.825866  [   96/  130]
train() client id: f_00008-4-3 loss: 0.759682  [  128/  130]
train() client id: f_00008-5-0 loss: 0.735116  [   32/  130]
train() client id: f_00008-5-1 loss: 0.777619  [   64/  130]
train() client id: f_00008-5-2 loss: 0.708219  [   96/  130]
train() client id: f_00008-5-3 loss: 0.827848  [  128/  130]
train() client id: f_00008-6-0 loss: 0.839717  [   32/  130]
train() client id: f_00008-6-1 loss: 0.700663  [   64/  130]
train() client id: f_00008-6-2 loss: 0.776618  [   96/  130]
train() client id: f_00008-6-3 loss: 0.727066  [  128/  130]
train() client id: f_00008-7-0 loss: 0.823886  [   32/  130]
train() client id: f_00008-7-1 loss: 0.739911  [   64/  130]
train() client id: f_00008-7-2 loss: 0.650375  [   96/  130]
train() client id: f_00008-7-3 loss: 0.823437  [  128/  130]
train() client id: f_00008-8-0 loss: 0.817924  [   32/  130]
train() client id: f_00008-8-1 loss: 0.749381  [   64/  130]
train() client id: f_00008-8-2 loss: 0.710565  [   96/  130]
train() client id: f_00008-8-3 loss: 0.707035  [  128/  130]
train() client id: f_00008-9-0 loss: 0.825838  [   32/  130]
train() client id: f_00008-9-1 loss: 0.752069  [   64/  130]
train() client id: f_00008-9-2 loss: 0.713768  [   96/  130]
train() client id: f_00008-9-3 loss: 0.707729  [  128/  130]
train() client id: f_00008-10-0 loss: 0.745398  [   32/  130]
train() client id: f_00008-10-1 loss: 0.723526  [   64/  130]
train() client id: f_00008-10-2 loss: 0.861852  [   96/  130]
train() client id: f_00008-10-3 loss: 0.714548  [  128/  130]
train() client id: f_00008-11-0 loss: 0.773869  [   32/  130]
train() client id: f_00008-11-1 loss: 0.710378  [   64/  130]
train() client id: f_00008-11-2 loss: 0.741522  [   96/  130]
train() client id: f_00008-11-3 loss: 0.817849  [  128/  130]
train() client id: f_00009-0-0 loss: 0.840950  [   32/  118]
train() client id: f_00009-0-1 loss: 0.976366  [   64/  118]
train() client id: f_00009-0-2 loss: 0.964629  [   96/  118]
train() client id: f_00009-1-0 loss: 1.120046  [   32/  118]
train() client id: f_00009-1-1 loss: 0.850163  [   64/  118]
train() client id: f_00009-1-2 loss: 0.819332  [   96/  118]
train() client id: f_00009-2-0 loss: 0.734710  [   32/  118]
train() client id: f_00009-2-1 loss: 0.876427  [   64/  118]
train() client id: f_00009-2-2 loss: 0.858423  [   96/  118]
train() client id: f_00009-3-0 loss: 1.077059  [   32/  118]
train() client id: f_00009-3-1 loss: 0.803929  [   64/  118]
train() client id: f_00009-3-2 loss: 0.705827  [   96/  118]
train() client id: f_00009-4-0 loss: 0.864889  [   32/  118]
train() client id: f_00009-4-1 loss: 0.771871  [   64/  118]
train() client id: f_00009-4-2 loss: 0.771348  [   96/  118]
train() client id: f_00009-5-0 loss: 0.800295  [   32/  118]
train() client id: f_00009-5-1 loss: 0.908533  [   64/  118]
train() client id: f_00009-5-2 loss: 0.644172  [   96/  118]
train() client id: f_00009-6-0 loss: 0.705865  [   32/  118]
train() client id: f_00009-6-1 loss: 0.650989  [   64/  118]
train() client id: f_00009-6-2 loss: 0.743051  [   96/  118]
train() client id: f_00009-7-0 loss: 0.897012  [   32/  118]
train() client id: f_00009-7-1 loss: 0.705061  [   64/  118]
train() client id: f_00009-7-2 loss: 0.688344  [   96/  118]
train() client id: f_00009-8-0 loss: 0.787682  [   32/  118]
train() client id: f_00009-8-1 loss: 0.671324  [   64/  118]
train() client id: f_00009-8-2 loss: 0.692907  [   96/  118]
train() client id: f_00009-9-0 loss: 0.823442  [   32/  118]
train() client id: f_00009-9-1 loss: 0.768851  [   64/  118]
train() client id: f_00009-9-2 loss: 0.621687  [   96/  118]
train() client id: f_00009-10-0 loss: 0.725804  [   32/  118]
train() client id: f_00009-10-1 loss: 0.654605  [   64/  118]
train() client id: f_00009-10-2 loss: 0.611082  [   96/  118]
train() client id: f_00009-11-0 loss: 0.584161  [   32/  118]
train() client id: f_00009-11-1 loss: 0.848969  [   64/  118]
train() client id: f_00009-11-2 loss: 0.669915  [   96/  118]
At round 32 accuracy: 0.6419098143236074
At round 32 training accuracy: 0.5875251509054326
At round 32 training loss: 0.823177952266415
update_location
xs = -3.905658 4.200318 180.009024 18.811294 0.979296 3.956410 -142.443192 -121.324852 164.663977 -107.060879 
ys = 172.587959 155.555839 1.320614 -142.455176 134.350187 117.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -0.2114261681174355
ys mean: 45.89414253552871
dists_uav = 199.504030 184.973678 205.924726 175.063822 167.484124 154.582752 174.060201 157.227211 193.449981 146.553893 
uav_gains = -107.650768 -106.741308 -108.058130 -106.112618 -105.618216 -104.735783 -106.047964 -104.921617 -107.271417 -104.153277 
uav_gains_db_mean: -106.13110981877784
dists_bs = 171.111343 180.252127 395.216316 371.941671 180.613178 187.871280 180.583999 182.260462 374.374325 184.000591 
bs_gains = -102.098866 -102.731710 -112.278435 -111.540355 -102.756043 -103.235150 -102.754079 -102.866448 -111.619629 -102.981997 
bs_gains_db_mean: -105.48627121333455
Round 33
-------------------------------
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.63572969 13.74717363  6.53678508  2.35476176 15.85548586  7.6312378
  2.9198615   9.33889895  6.88901514  6.19002745]
obj_prev = 78.09897687730752
eta_min = 1.1396317579661002e-14	eta_max = 0.9276141009437577
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 18.129363196870486	eta = 0.9090909090909091
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 32.88895800241827	eta = 0.5011177085230375
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 25.66977986634642	eta = 0.642048329034932
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.365791295624447	eta = 0.6764089485097873
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.297762088268055	eta = 0.678302767555703
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.29756203224391	eta = 0.678308352418689
eta = 0.678308352418689
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [0.03202948 0.06736359 0.03152108 0.0109307  0.07778588 0.03711352
 0.01372692 0.0455022  0.03304629 0.02999586]
ene_total = [2.16353862 3.9073382  2.14884473 1.01590035 4.454601   2.32913348
 1.16104971 2.80262517 2.36340733 1.95112344]
ti_comp = [0.44826702 0.46925465 0.4459835  0.45600932 0.4691739  0.46754648
 0.45630733 0.4611897  0.42034182 0.46841538]
ti_coms = [0.09205736 0.07106973 0.09434088 0.08431506 0.07115048 0.0727779
 0.08401706 0.07913468 0.11998256 0.07190901]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [1.02201224e-05 8.67637833e-05 9.84114204e-06 3.92532807e-07
 1.33633277e-04 1.46159488e-05 7.76398040e-07 2.76833041e-05
 1.27656487e-05 7.68779460e-06]
ene_total = [0.48354488 0.3774426  0.49550619 0.44240675 0.38032544 0.3826196
 0.44086332 0.41665811 0.63019699 0.37769716]
optimize_network iter = 0 obj = 4.427261042788227
eta = 0.678308352418689
freqs = [35725893.75226643 71777219.31597894 35338835.42638909 11985166.67285503
 82896638.99614602 39689662.43089405 15041309.96971984 49331324.0982344
 39308827.86172418 32018442.92741502]
eta_min = 0.6783083524187157	eta_max = 0.6783083524186895
af = 0.013235761297397841	bf = 1.4190753947365375	zeta = 0.014559337427137626	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [2.50952835e-06 2.13046542e-05 2.41647056e-06 9.63855586e-08
 3.28133544e-05 3.58891376e-06 1.90642814e-07 6.79757383e-06
 3.13457667e-06 1.88772088e-06]
ene_total = [1.70964427 1.323468   1.75202375 1.56544914 1.327104   1.35189348
 1.55993376 1.47051205 2.22823206 1.33544537]
ti_comp = [0.44826702 0.46925465 0.4459835  0.45600932 0.4691739  0.46754648
 0.45630733 0.4611897  0.42034182 0.46841538]
ti_coms = [0.09205736 0.07106973 0.09434088 0.08431506 0.07115048 0.0727779
 0.08401706 0.07913468 0.11998256 0.07190901]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [1.02201224e-05 8.67637833e-05 9.84114204e-06 3.92532807e-07
 1.33633277e-04 1.46159488e-05 7.76398040e-07 2.76833041e-05
 1.27656487e-05 7.68779460e-06]
ene_total = [0.48354488 0.3774426  0.49550619 0.44240675 0.38032544 0.3826196
 0.44086332 0.41665811 0.63019699 0.37769716]
optimize_network iter = 1 obj = 4.427261042788233
eta = 0.6783083524186895
freqs = [35725893.75226644 71777219.31597894 35338835.4263891  11985166.67285503
 82896638.99614602 39689662.43089406 15041309.96971984 49331324.09823441
 39308827.86172419 32018442.92741502]
Done!
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [9.64912650e-06 8.19163105e-05 9.29131967e-06 3.70602088e-07
 1.26167217e-04 1.37993591e-05 7.33020858e-07 2.61366442e-05
 1.20524348e-05 7.25827927e-06]
ene_total = [0.00921539 0.00718889 0.00944338 0.00843188 0.00724122 0.00729159
 0.00840244 0.0079396  0.01201031 0.00719816]
At round 33 energy consumption: 0.08436284522778167
At round 33 eta: 0.6783083524186895
At round 33 a_n: 16.878589906266903
At round 33 local rounds: 12.71011101057834
At round 33 global rounds: 52.468225498458686
gradient difference: 0.5750553607940674
train() client id: f_00000-0-0 loss: 0.893220  [   32/  126]
train() client id: f_00000-0-1 loss: 0.907497  [   64/  126]
train() client id: f_00000-0-2 loss: 1.032711  [   96/  126]
train() client id: f_00000-1-0 loss: 0.852101  [   32/  126]
train() client id: f_00000-1-1 loss: 0.906458  [   64/  126]
train() client id: f_00000-1-2 loss: 0.920364  [   96/  126]
train() client id: f_00000-2-0 loss: 0.921173  [   32/  126]
train() client id: f_00000-2-1 loss: 0.728745  [   64/  126]
train() client id: f_00000-2-2 loss: 0.793488  [   96/  126]
train() client id: f_00000-3-0 loss: 0.746635  [   32/  126]
train() client id: f_00000-3-1 loss: 0.878330  [   64/  126]
train() client id: f_00000-3-2 loss: 0.833414  [   96/  126]
train() client id: f_00000-4-0 loss: 0.754748  [   32/  126]
train() client id: f_00000-4-1 loss: 0.831660  [   64/  126]
train() client id: f_00000-4-2 loss: 0.841047  [   96/  126]
train() client id: f_00000-5-0 loss: 0.938848  [   32/  126]
train() client id: f_00000-5-1 loss: 0.729168  [   64/  126]
train() client id: f_00000-5-2 loss: 0.772058  [   96/  126]
train() client id: f_00000-6-0 loss: 0.791599  [   32/  126]
train() client id: f_00000-6-1 loss: 0.682637  [   64/  126]
train() client id: f_00000-6-2 loss: 0.810390  [   96/  126]
train() client id: f_00000-7-0 loss: 0.736993  [   32/  126]
train() client id: f_00000-7-1 loss: 0.759582  [   64/  126]
train() client id: f_00000-7-2 loss: 0.914017  [   96/  126]
train() client id: f_00000-8-0 loss: 0.884991  [   32/  126]
train() client id: f_00000-8-1 loss: 0.746733  [   64/  126]
train() client id: f_00000-8-2 loss: 0.835873  [   96/  126]
train() client id: f_00000-9-0 loss: 0.778696  [   32/  126]
train() client id: f_00000-9-1 loss: 0.804844  [   64/  126]
train() client id: f_00000-9-2 loss: 0.777907  [   96/  126]
train() client id: f_00000-10-0 loss: 0.788081  [   32/  126]
train() client id: f_00000-10-1 loss: 0.785105  [   64/  126]
train() client id: f_00000-10-2 loss: 0.872107  [   96/  126]
train() client id: f_00000-11-0 loss: 0.823608  [   32/  126]
train() client id: f_00000-11-1 loss: 0.699778  [   64/  126]
train() client id: f_00000-11-2 loss: 0.851017  [   96/  126]
train() client id: f_00001-0-0 loss: 0.444304  [   32/  265]
train() client id: f_00001-0-1 loss: 0.454150  [   64/  265]
train() client id: f_00001-0-2 loss: 0.336474  [   96/  265]
train() client id: f_00001-0-3 loss: 0.374836  [  128/  265]
train() client id: f_00001-0-4 loss: 0.501824  [  160/  265]
train() client id: f_00001-0-5 loss: 0.335074  [  192/  265]
train() client id: f_00001-0-6 loss: 0.344178  [  224/  265]
train() client id: f_00001-0-7 loss: 0.535799  [  256/  265]
train() client id: f_00001-1-0 loss: 0.422071  [   32/  265]
train() client id: f_00001-1-1 loss: 0.410864  [   64/  265]
train() client id: f_00001-1-2 loss: 0.535919  [   96/  265]
train() client id: f_00001-1-3 loss: 0.393809  [  128/  265]
train() client id: f_00001-1-4 loss: 0.366262  [  160/  265]
train() client id: f_00001-1-5 loss: 0.452265  [  192/  265]
train() client id: f_00001-1-6 loss: 0.309049  [  224/  265]
train() client id: f_00001-1-7 loss: 0.381748  [  256/  265]
train() client id: f_00001-2-0 loss: 0.425963  [   32/  265]
train() client id: f_00001-2-1 loss: 0.400585  [   64/  265]
train() client id: f_00001-2-2 loss: 0.353882  [   96/  265]
train() client id: f_00001-2-3 loss: 0.383224  [  128/  265]
train() client id: f_00001-2-4 loss: 0.504200  [  160/  265]
train() client id: f_00001-2-5 loss: 0.334701  [  192/  265]
train() client id: f_00001-2-6 loss: 0.417987  [  224/  265]
train() client id: f_00001-2-7 loss: 0.338159  [  256/  265]
train() client id: f_00001-3-0 loss: 0.386668  [   32/  265]
train() client id: f_00001-3-1 loss: 0.378492  [   64/  265]
train() client id: f_00001-3-2 loss: 0.401847  [   96/  265]
train() client id: f_00001-3-3 loss: 0.484322  [  128/  265]
train() client id: f_00001-3-4 loss: 0.388304  [  160/  265]
train() client id: f_00001-3-5 loss: 0.366101  [  192/  265]
train() client id: f_00001-3-6 loss: 0.365404  [  224/  265]
train() client id: f_00001-3-7 loss: 0.416667  [  256/  265]
train() client id: f_00001-4-0 loss: 0.407668  [   32/  265]
train() client id: f_00001-4-1 loss: 0.465671  [   64/  265]
train() client id: f_00001-4-2 loss: 0.474503  [   96/  265]
train() client id: f_00001-4-3 loss: 0.306483  [  128/  265]
train() client id: f_00001-4-4 loss: 0.342692  [  160/  265]
train() client id: f_00001-4-5 loss: 0.401192  [  192/  265]
train() client id: f_00001-4-6 loss: 0.390378  [  224/  265]
train() client id: f_00001-4-7 loss: 0.379816  [  256/  265]
train() client id: f_00001-5-0 loss: 0.369245  [   32/  265]
train() client id: f_00001-5-1 loss: 0.437190  [   64/  265]
train() client id: f_00001-5-2 loss: 0.532751  [   96/  265]
train() client id: f_00001-5-3 loss: 0.434362  [  128/  265]
train() client id: f_00001-5-4 loss: 0.387162  [  160/  265]
train() client id: f_00001-5-5 loss: 0.372790  [  192/  265]
train() client id: f_00001-5-6 loss: 0.278981  [  224/  265]
train() client id: f_00001-5-7 loss: 0.338667  [  256/  265]
train() client id: f_00001-6-0 loss: 0.343856  [   32/  265]
train() client id: f_00001-6-1 loss: 0.362546  [   64/  265]
train() client id: f_00001-6-2 loss: 0.415522  [   96/  265]
train() client id: f_00001-6-3 loss: 0.275814  [  128/  265]
train() client id: f_00001-6-4 loss: 0.341452  [  160/  265]
train() client id: f_00001-6-5 loss: 0.448623  [  192/  265]
train() client id: f_00001-6-6 loss: 0.371357  [  224/  265]
train() client id: f_00001-6-7 loss: 0.573823  [  256/  265]
train() client id: f_00001-7-0 loss: 0.356462  [   32/  265]
train() client id: f_00001-7-1 loss: 0.530257  [   64/  265]
train() client id: f_00001-7-2 loss: 0.341696  [   96/  265]
train() client id: f_00001-7-3 loss: 0.422150  [  128/  265]
train() client id: f_00001-7-4 loss: 0.413922  [  160/  265]
train() client id: f_00001-7-5 loss: 0.367217  [  192/  265]
train() client id: f_00001-7-6 loss: 0.365017  [  224/  265]
train() client id: f_00001-7-7 loss: 0.308267  [  256/  265]
train() client id: f_00001-8-0 loss: 0.314429  [   32/  265]
train() client id: f_00001-8-1 loss: 0.463866  [   64/  265]
train() client id: f_00001-8-2 loss: 0.323756  [   96/  265]
train() client id: f_00001-8-3 loss: 0.348671  [  128/  265]
train() client id: f_00001-8-4 loss: 0.435357  [  160/  265]
train() client id: f_00001-8-5 loss: 0.367638  [  192/  265]
train() client id: f_00001-8-6 loss: 0.386435  [  224/  265]
train() client id: f_00001-8-7 loss: 0.413666  [  256/  265]
train() client id: f_00001-9-0 loss: 0.294661  [   32/  265]
train() client id: f_00001-9-1 loss: 0.429834  [   64/  265]
train() client id: f_00001-9-2 loss: 0.361168  [   96/  265]
train() client id: f_00001-9-3 loss: 0.357419  [  128/  265]
train() client id: f_00001-9-4 loss: 0.606268  [  160/  265]
train() client id: f_00001-9-5 loss: 0.311659  [  192/  265]
train() client id: f_00001-9-6 loss: 0.388280  [  224/  265]
train() client id: f_00001-9-7 loss: 0.305301  [  256/  265]
train() client id: f_00001-10-0 loss: 0.465496  [   32/  265]
train() client id: f_00001-10-1 loss: 0.319561  [   64/  265]
train() client id: f_00001-10-2 loss: 0.408629  [   96/  265]
train() client id: f_00001-10-3 loss: 0.405805  [  128/  265]
train() client id: f_00001-10-4 loss: 0.348514  [  160/  265]
train() client id: f_00001-10-5 loss: 0.446938  [  192/  265]
train() client id: f_00001-10-6 loss: 0.392724  [  224/  265]
train() client id: f_00001-10-7 loss: 0.313860  [  256/  265]
train() client id: f_00001-11-0 loss: 0.424581  [   32/  265]
train() client id: f_00001-11-1 loss: 0.373427  [   64/  265]
train() client id: f_00001-11-2 loss: 0.362086  [   96/  265]
train() client id: f_00001-11-3 loss: 0.388649  [  128/  265]
train() client id: f_00001-11-4 loss: 0.356596  [  160/  265]
train() client id: f_00001-11-5 loss: 0.369789  [  192/  265]
train() client id: f_00001-11-6 loss: 0.298940  [  224/  265]
train() client id: f_00001-11-7 loss: 0.360704  [  256/  265]
train() client id: f_00002-0-0 loss: 1.362839  [   32/  124]
train() client id: f_00002-0-1 loss: 1.008128  [   64/  124]
train() client id: f_00002-0-2 loss: 0.963201  [   96/  124]
train() client id: f_00002-1-0 loss: 1.028016  [   32/  124]
train() client id: f_00002-1-1 loss: 1.165436  [   64/  124]
train() client id: f_00002-1-2 loss: 0.993151  [   96/  124]
train() client id: f_00002-2-0 loss: 1.059489  [   32/  124]
train() client id: f_00002-2-1 loss: 1.071605  [   64/  124]
train() client id: f_00002-2-2 loss: 1.012252  [   96/  124]
train() client id: f_00002-3-0 loss: 1.123716  [   32/  124]
train() client id: f_00002-3-1 loss: 0.861489  [   64/  124]
train() client id: f_00002-3-2 loss: 1.011020  [   96/  124]
train() client id: f_00002-4-0 loss: 0.818429  [   32/  124]
train() client id: f_00002-4-1 loss: 1.039074  [   64/  124]
train() client id: f_00002-4-2 loss: 1.072006  [   96/  124]
train() client id: f_00002-5-0 loss: 0.894266  [   32/  124]
train() client id: f_00002-5-1 loss: 0.910896  [   64/  124]
train() client id: f_00002-5-2 loss: 1.240553  [   96/  124]
train() client id: f_00002-6-0 loss: 0.863003  [   32/  124]
train() client id: f_00002-6-1 loss: 1.173124  [   64/  124]
train() client id: f_00002-6-2 loss: 1.066581  [   96/  124]
train() client id: f_00002-7-0 loss: 0.953417  [   32/  124]
train() client id: f_00002-7-1 loss: 1.065968  [   64/  124]
train() client id: f_00002-7-2 loss: 0.858665  [   96/  124]
train() client id: f_00002-8-0 loss: 1.049487  [   32/  124]
train() client id: f_00002-8-1 loss: 1.031235  [   64/  124]
train() client id: f_00002-8-2 loss: 1.139097  [   96/  124]
train() client id: f_00002-9-0 loss: 1.018823  [   32/  124]
train() client id: f_00002-9-1 loss: 1.032905  [   64/  124]
train() client id: f_00002-9-2 loss: 1.233675  [   96/  124]
train() client id: f_00002-10-0 loss: 1.114822  [   32/  124]
train() client id: f_00002-10-1 loss: 0.955066  [   64/  124]
train() client id: f_00002-10-2 loss: 1.243579  [   96/  124]
train() client id: f_00002-11-0 loss: 0.974032  [   32/  124]
train() client id: f_00002-11-1 loss: 0.990547  [   64/  124]
train() client id: f_00002-11-2 loss: 1.188348  [   96/  124]
train() client id: f_00003-0-0 loss: 0.610235  [   32/   43]
train() client id: f_00003-1-0 loss: 0.813194  [   32/   43]
train() client id: f_00003-2-0 loss: 0.687760  [   32/   43]
train() client id: f_00003-3-0 loss: 0.623455  [   32/   43]
train() client id: f_00003-4-0 loss: 0.562675  [   32/   43]
train() client id: f_00003-5-0 loss: 0.773186  [   32/   43]
train() client id: f_00003-6-0 loss: 0.642416  [   32/   43]
train() client id: f_00003-7-0 loss: 0.517255  [   32/   43]
train() client id: f_00003-8-0 loss: 0.706716  [   32/   43]
train() client id: f_00003-9-0 loss: 0.731100  [   32/   43]
train() client id: f_00003-10-0 loss: 0.682812  [   32/   43]
train() client id: f_00003-11-0 loss: 0.295044  [   32/   43]
train() client id: f_00004-0-0 loss: 0.691267  [   32/  306]
train() client id: f_00004-0-1 loss: 1.050165  [   64/  306]
train() client id: f_00004-0-2 loss: 0.698887  [   96/  306]
train() client id: f_00004-0-3 loss: 0.892568  [  128/  306]
train() client id: f_00004-0-4 loss: 0.840448  [  160/  306]
train() client id: f_00004-0-5 loss: 0.824660  [  192/  306]
train() client id: f_00004-0-6 loss: 0.826681  [  224/  306]
train() client id: f_00004-0-7 loss: 0.816655  [  256/  306]
train() client id: f_00004-0-8 loss: 1.024714  [  288/  306]
train() client id: f_00004-1-0 loss: 0.833107  [   32/  306]
train() client id: f_00004-1-1 loss: 0.800766  [   64/  306]
train() client id: f_00004-1-2 loss: 0.919971  [   96/  306]
train() client id: f_00004-1-3 loss: 0.823798  [  128/  306]
train() client id: f_00004-1-4 loss: 0.922511  [  160/  306]
train() client id: f_00004-1-5 loss: 0.849679  [  192/  306]
train() client id: f_00004-1-6 loss: 0.779028  [  224/  306]
train() client id: f_00004-1-7 loss: 0.788032  [  256/  306]
train() client id: f_00004-1-8 loss: 1.007765  [  288/  306]
train() client id: f_00004-2-0 loss: 0.800013  [   32/  306]
train() client id: f_00004-2-1 loss: 1.072989  [   64/  306]
train() client id: f_00004-2-2 loss: 0.865054  [   96/  306]
train() client id: f_00004-2-3 loss: 0.770355  [  128/  306]
train() client id: f_00004-2-4 loss: 0.858844  [  160/  306]
train() client id: f_00004-2-5 loss: 0.789103  [  192/  306]
train() client id: f_00004-2-6 loss: 0.715627  [  224/  306]
train() client id: f_00004-2-7 loss: 0.874382  [  256/  306]
train() client id: f_00004-2-8 loss: 0.875577  [  288/  306]
train() client id: f_00004-3-0 loss: 0.841987  [   32/  306]
train() client id: f_00004-3-1 loss: 0.869892  [   64/  306]
train() client id: f_00004-3-2 loss: 0.751143  [   96/  306]
train() client id: f_00004-3-3 loss: 0.861527  [  128/  306]
train() client id: f_00004-3-4 loss: 0.803261  [  160/  306]
train() client id: f_00004-3-5 loss: 0.886250  [  192/  306]
train() client id: f_00004-3-6 loss: 0.883471  [  224/  306]
train() client id: f_00004-3-7 loss: 0.860804  [  256/  306]
train() client id: f_00004-3-8 loss: 0.850173  [  288/  306]
train() client id: f_00004-4-0 loss: 0.782928  [   32/  306]
train() client id: f_00004-4-1 loss: 0.756383  [   64/  306]
train() client id: f_00004-4-2 loss: 0.931324  [   96/  306]
train() client id: f_00004-4-3 loss: 0.741162  [  128/  306]
train() client id: f_00004-4-4 loss: 0.860575  [  160/  306]
train() client id: f_00004-4-5 loss: 0.885907  [  192/  306]
train() client id: f_00004-4-6 loss: 0.836300  [  224/  306]
train() client id: f_00004-4-7 loss: 0.845839  [  256/  306]
train() client id: f_00004-4-8 loss: 0.974086  [  288/  306]
train() client id: f_00004-5-0 loss: 0.831586  [   32/  306]
train() client id: f_00004-5-1 loss: 0.795813  [   64/  306]
train() client id: f_00004-5-2 loss: 1.016502  [   96/  306]
train() client id: f_00004-5-3 loss: 0.790706  [  128/  306]
train() client id: f_00004-5-4 loss: 0.857484  [  160/  306]
train() client id: f_00004-5-5 loss: 0.770251  [  192/  306]
train() client id: f_00004-5-6 loss: 0.795511  [  224/  306]
train() client id: f_00004-5-7 loss: 0.931999  [  256/  306]
train() client id: f_00004-5-8 loss: 0.854366  [  288/  306]
train() client id: f_00004-6-0 loss: 0.913141  [   32/  306]
train() client id: f_00004-6-1 loss: 0.958236  [   64/  306]
train() client id: f_00004-6-2 loss: 0.786978  [   96/  306]
train() client id: f_00004-6-3 loss: 0.851373  [  128/  306]
train() client id: f_00004-6-4 loss: 0.787163  [  160/  306]
train() client id: f_00004-6-5 loss: 0.743991  [  192/  306]
train() client id: f_00004-6-6 loss: 0.841463  [  224/  306]
train() client id: f_00004-6-7 loss: 0.837684  [  256/  306]
train() client id: f_00004-6-8 loss: 0.833693  [  288/  306]
train() client id: f_00004-7-0 loss: 0.792123  [   32/  306]
train() client id: f_00004-7-1 loss: 0.875120  [   64/  306]
train() client id: f_00004-7-2 loss: 0.789771  [   96/  306]
train() client id: f_00004-7-3 loss: 0.846405  [  128/  306]
train() client id: f_00004-7-4 loss: 0.818923  [  160/  306]
train() client id: f_00004-7-5 loss: 0.877720  [  192/  306]
train() client id: f_00004-7-6 loss: 0.863697  [  224/  306]
train() client id: f_00004-7-7 loss: 0.917042  [  256/  306]
train() client id: f_00004-7-8 loss: 0.889270  [  288/  306]
train() client id: f_00004-8-0 loss: 0.945861  [   32/  306]
train() client id: f_00004-8-1 loss: 0.781755  [   64/  306]
train() client id: f_00004-8-2 loss: 0.735640  [   96/  306]
train() client id: f_00004-8-3 loss: 0.918800  [  128/  306]
train() client id: f_00004-8-4 loss: 0.809485  [  160/  306]
train() client id: f_00004-8-5 loss: 0.783433  [  192/  306]
train() client id: f_00004-8-6 loss: 0.920294  [  224/  306]
train() client id: f_00004-8-7 loss: 0.755735  [  256/  306]
train() client id: f_00004-8-8 loss: 0.894092  [  288/  306]
train() client id: f_00004-9-0 loss: 0.860251  [   32/  306]
train() client id: f_00004-9-1 loss: 0.768580  [   64/  306]
train() client id: f_00004-9-2 loss: 0.839920  [   96/  306]
train() client id: f_00004-9-3 loss: 0.869701  [  128/  306]
train() client id: f_00004-9-4 loss: 0.889668  [  160/  306]
train() client id: f_00004-9-5 loss: 0.880321  [  192/  306]
train() client id: f_00004-9-6 loss: 0.800275  [  224/  306]
train() client id: f_00004-9-7 loss: 0.780206  [  256/  306]
train() client id: f_00004-9-8 loss: 0.834473  [  288/  306]
train() client id: f_00004-10-0 loss: 0.905356  [   32/  306]
train() client id: f_00004-10-1 loss: 0.784838  [   64/  306]
train() client id: f_00004-10-2 loss: 0.862990  [   96/  306]
train() client id: f_00004-10-3 loss: 0.722659  [  128/  306]
train() client id: f_00004-10-4 loss: 0.811368  [  160/  306]
train() client id: f_00004-10-5 loss: 0.944707  [  192/  306]
train() client id: f_00004-10-6 loss: 0.901670  [  224/  306]
train() client id: f_00004-10-7 loss: 0.860218  [  256/  306]
train() client id: f_00004-10-8 loss: 0.803624  [  288/  306]
train() client id: f_00004-11-0 loss: 0.988620  [   32/  306]
train() client id: f_00004-11-1 loss: 0.686132  [   64/  306]
train() client id: f_00004-11-2 loss: 0.873329  [   96/  306]
train() client id: f_00004-11-3 loss: 0.817962  [  128/  306]
train() client id: f_00004-11-4 loss: 0.885070  [  160/  306]
train() client id: f_00004-11-5 loss: 0.832057  [  192/  306]
train() client id: f_00004-11-6 loss: 0.734760  [  224/  306]
train() client id: f_00004-11-7 loss: 0.891567  [  256/  306]
train() client id: f_00004-11-8 loss: 0.933780  [  288/  306]
train() client id: f_00005-0-0 loss: 0.480383  [   32/  146]
train() client id: f_00005-0-1 loss: 0.662442  [   64/  146]
train() client id: f_00005-0-2 loss: 0.610732  [   96/  146]
train() client id: f_00005-0-3 loss: 0.586733  [  128/  146]
train() client id: f_00005-1-0 loss: 0.744281  [   32/  146]
train() client id: f_00005-1-1 loss: 0.608523  [   64/  146]
train() client id: f_00005-1-2 loss: 0.689765  [   96/  146]
train() client id: f_00005-1-3 loss: 0.465354  [  128/  146]
train() client id: f_00005-2-0 loss: 0.531007  [   32/  146]
train() client id: f_00005-2-1 loss: 0.500213  [   64/  146]
train() client id: f_00005-2-2 loss: 0.655210  [   96/  146]
train() client id: f_00005-2-3 loss: 0.556239  [  128/  146]
train() client id: f_00005-3-0 loss: 0.834909  [   32/  146]
train() client id: f_00005-3-1 loss: 0.444554  [   64/  146]
train() client id: f_00005-3-2 loss: 0.506613  [   96/  146]
train() client id: f_00005-3-3 loss: 0.604036  [  128/  146]
train() client id: f_00005-4-0 loss: 0.474142  [   32/  146]
train() client id: f_00005-4-1 loss: 0.674576  [   64/  146]
train() client id: f_00005-4-2 loss: 0.652262  [   96/  146]
train() client id: f_00005-4-3 loss: 0.616579  [  128/  146]
train() client id: f_00005-5-0 loss: 0.562939  [   32/  146]
train() client id: f_00005-5-1 loss: 0.336741  [   64/  146]
train() client id: f_00005-5-2 loss: 0.662544  [   96/  146]
train() client id: f_00005-5-3 loss: 0.708150  [  128/  146]
train() client id: f_00005-6-0 loss: 0.866520  [   32/  146]
train() client id: f_00005-6-1 loss: 0.512784  [   64/  146]
train() client id: f_00005-6-2 loss: 0.562138  [   96/  146]
train() client id: f_00005-6-3 loss: 0.501415  [  128/  146]
train() client id: f_00005-7-0 loss: 0.235611  [   32/  146]
train() client id: f_00005-7-1 loss: 0.553209  [   64/  146]
train() client id: f_00005-7-2 loss: 0.744272  [   96/  146]
train() client id: f_00005-7-3 loss: 0.796880  [  128/  146]
train() client id: f_00005-8-0 loss: 0.480132  [   32/  146]
train() client id: f_00005-8-1 loss: 0.658121  [   64/  146]
train() client id: f_00005-8-2 loss: 0.460102  [   96/  146]
train() client id: f_00005-8-3 loss: 0.699697  [  128/  146]
train() client id: f_00005-9-0 loss: 0.503368  [   32/  146]
train() client id: f_00005-9-1 loss: 0.864886  [   64/  146]
train() client id: f_00005-9-2 loss: 0.417782  [   96/  146]
train() client id: f_00005-9-3 loss: 0.514672  [  128/  146]
train() client id: f_00005-10-0 loss: 0.562279  [   32/  146]
train() client id: f_00005-10-1 loss: 0.773876  [   64/  146]
train() client id: f_00005-10-2 loss: 0.782702  [   96/  146]
train() client id: f_00005-10-3 loss: 0.270938  [  128/  146]
train() client id: f_00005-11-0 loss: 0.225667  [   32/  146]
train() client id: f_00005-11-1 loss: 1.100617  [   64/  146]
train() client id: f_00005-11-2 loss: 0.686340  [   96/  146]
train() client id: f_00005-11-3 loss: 0.452267  [  128/  146]
train() client id: f_00006-0-0 loss: 0.507015  [   32/   54]
train() client id: f_00006-1-0 loss: 0.575683  [   32/   54]
train() client id: f_00006-2-0 loss: 0.529209  [   32/   54]
train() client id: f_00006-3-0 loss: 0.472942  [   32/   54]
train() client id: f_00006-4-0 loss: 0.521557  [   32/   54]
train() client id: f_00006-5-0 loss: 0.476857  [   32/   54]
train() client id: f_00006-6-0 loss: 0.467757  [   32/   54]
train() client id: f_00006-7-0 loss: 0.485726  [   32/   54]
train() client id: f_00006-8-0 loss: 0.523800  [   32/   54]
train() client id: f_00006-9-0 loss: 0.506075  [   32/   54]
train() client id: f_00006-10-0 loss: 0.535805  [   32/   54]
train() client id: f_00006-11-0 loss: 0.559470  [   32/   54]
train() client id: f_00007-0-0 loss: 0.762645  [   32/  179]
train() client id: f_00007-0-1 loss: 0.709685  [   64/  179]
train() client id: f_00007-0-2 loss: 0.876765  [   96/  179]
train() client id: f_00007-0-3 loss: 0.569354  [  128/  179]
train() client id: f_00007-0-4 loss: 0.649736  [  160/  179]
train() client id: f_00007-1-0 loss: 0.631108  [   32/  179]
train() client id: f_00007-1-1 loss: 0.745528  [   64/  179]
train() client id: f_00007-1-2 loss: 0.808016  [   96/  179]
train() client id: f_00007-1-3 loss: 0.542187  [  128/  179]
train() client id: f_00007-1-4 loss: 0.893059  [  160/  179]
train() client id: f_00007-2-0 loss: 0.814425  [   32/  179]
train() client id: f_00007-2-1 loss: 0.835874  [   64/  179]
train() client id: f_00007-2-2 loss: 0.515368  [   96/  179]
train() client id: f_00007-2-3 loss: 0.671435  [  128/  179]
train() client id: f_00007-2-4 loss: 0.568485  [  160/  179]
train() client id: f_00007-3-0 loss: 0.552113  [   32/  179]
train() client id: f_00007-3-1 loss: 0.746547  [   64/  179]
train() client id: f_00007-3-2 loss: 0.724852  [   96/  179]
train() client id: f_00007-3-3 loss: 0.692877  [  128/  179]
train() client id: f_00007-3-4 loss: 0.820899  [  160/  179]
train() client id: f_00007-4-0 loss: 0.608715  [   32/  179]
train() client id: f_00007-4-1 loss: 0.695269  [   64/  179]
train() client id: f_00007-4-2 loss: 0.642985  [   96/  179]
train() client id: f_00007-4-3 loss: 0.828127  [  128/  179]
train() client id: f_00007-4-4 loss: 0.538979  [  160/  179]
train() client id: f_00007-5-0 loss: 0.650572  [   32/  179]
train() client id: f_00007-5-1 loss: 0.598585  [   64/  179]
train() client id: f_00007-5-2 loss: 0.998974  [   96/  179]
train() client id: f_00007-5-3 loss: 0.506597  [  128/  179]
train() client id: f_00007-5-4 loss: 0.739531  [  160/  179]
train() client id: f_00007-6-0 loss: 0.729967  [   32/  179]
train() client id: f_00007-6-1 loss: 0.782124  [   64/  179]
train() client id: f_00007-6-2 loss: 0.607093  [   96/  179]
train() client id: f_00007-6-3 loss: 0.746768  [  128/  179]
train() client id: f_00007-6-4 loss: 0.550873  [  160/  179]
train() client id: f_00007-7-0 loss: 0.556598  [   32/  179]
train() client id: f_00007-7-1 loss: 0.773695  [   64/  179]
train() client id: f_00007-7-2 loss: 0.787049  [   96/  179]
train() client id: f_00007-7-3 loss: 0.707171  [  128/  179]
train() client id: f_00007-7-4 loss: 0.549979  [  160/  179]
train() client id: f_00007-8-0 loss: 0.690459  [   32/  179]
train() client id: f_00007-8-1 loss: 0.659493  [   64/  179]
train() client id: f_00007-8-2 loss: 0.518955  [   96/  179]
train() client id: f_00007-8-3 loss: 0.807880  [  128/  179]
train() client id: f_00007-8-4 loss: 0.646687  [  160/  179]
train() client id: f_00007-9-0 loss: 0.551189  [   32/  179]
train() client id: f_00007-9-1 loss: 0.889227  [   64/  179]
train() client id: f_00007-9-2 loss: 0.611121  [   96/  179]
train() client id: f_00007-9-3 loss: 0.675288  [  128/  179]
train() client id: f_00007-9-4 loss: 0.621473  [  160/  179]
train() client id: f_00007-10-0 loss: 0.616628  [   32/  179]
train() client id: f_00007-10-1 loss: 0.624804  [   64/  179]
train() client id: f_00007-10-2 loss: 0.637475  [   96/  179]
train() client id: f_00007-10-3 loss: 0.810442  [  128/  179]
train() client id: f_00007-10-4 loss: 0.707561  [  160/  179]
train() client id: f_00007-11-0 loss: 0.518290  [   32/  179]
train() client id: f_00007-11-1 loss: 0.695801  [   64/  179]
train() client id: f_00007-11-2 loss: 0.869955  [   96/  179]
train() client id: f_00007-11-3 loss: 0.596871  [  128/  179]
train() client id: f_00007-11-4 loss: 0.702534  [  160/  179]
train() client id: f_00008-0-0 loss: 0.574015  [   32/  130]
train() client id: f_00008-0-1 loss: 0.695987  [   64/  130]
train() client id: f_00008-0-2 loss: 0.653728  [   96/  130]
train() client id: f_00008-0-3 loss: 0.743696  [  128/  130]
train() client id: f_00008-1-0 loss: 0.634044  [   32/  130]
train() client id: f_00008-1-1 loss: 0.744767  [   64/  130]
train() client id: f_00008-1-2 loss: 0.672450  [   96/  130]
train() client id: f_00008-1-3 loss: 0.633369  [  128/  130]
train() client id: f_00008-2-0 loss: 0.823729  [   32/  130]
train() client id: f_00008-2-1 loss: 0.587452  [   64/  130]
train() client id: f_00008-2-2 loss: 0.626169  [   96/  130]
train() client id: f_00008-2-3 loss: 0.646304  [  128/  130]
train() client id: f_00008-3-0 loss: 0.633685  [   32/  130]
train() client id: f_00008-3-1 loss: 0.711332  [   64/  130]
train() client id: f_00008-3-2 loss: 0.677922  [   96/  130]
train() client id: f_00008-3-3 loss: 0.647162  [  128/  130]
train() client id: f_00008-4-0 loss: 0.696114  [   32/  130]
train() client id: f_00008-4-1 loss: 0.675859  [   64/  130]
train() client id: f_00008-4-2 loss: 0.612693  [   96/  130]
train() client id: f_00008-4-3 loss: 0.674010  [  128/  130]
train() client id: f_00008-5-0 loss: 0.649204  [   32/  130]
train() client id: f_00008-5-1 loss: 0.619741  [   64/  130]
train() client id: f_00008-5-2 loss: 0.707185  [   96/  130]
train() client id: f_00008-5-3 loss: 0.653370  [  128/  130]
train() client id: f_00008-6-0 loss: 0.687571  [   32/  130]
train() client id: f_00008-6-1 loss: 0.612880  [   64/  130]
train() client id: f_00008-6-2 loss: 0.624636  [   96/  130]
train() client id: f_00008-6-3 loss: 0.747776  [  128/  130]
train() client id: f_00008-7-0 loss: 0.706969  [   32/  130]
train() client id: f_00008-7-1 loss: 0.640254  [   64/  130]
train() client id: f_00008-7-2 loss: 0.723004  [   96/  130]
train() client id: f_00008-7-3 loss: 0.610158  [  128/  130]
train() client id: f_00008-8-0 loss: 0.687810  [   32/  130]
train() client id: f_00008-8-1 loss: 0.684494  [   64/  130]
train() client id: f_00008-8-2 loss: 0.600311  [   96/  130]
train() client id: f_00008-8-3 loss: 0.664932  [  128/  130]
train() client id: f_00008-9-0 loss: 0.746665  [   32/  130]
train() client id: f_00008-9-1 loss: 0.620555  [   64/  130]
train() client id: f_00008-9-2 loss: 0.748273  [   96/  130]
train() client id: f_00008-9-3 loss: 0.527570  [  128/  130]
train() client id: f_00008-10-0 loss: 0.621596  [   32/  130]
train() client id: f_00008-10-1 loss: 0.693420  [   64/  130]
train() client id: f_00008-10-2 loss: 0.672891  [   96/  130]
train() client id: f_00008-10-3 loss: 0.690748  [  128/  130]
train() client id: f_00008-11-0 loss: 0.689132  [   32/  130]
train() client id: f_00008-11-1 loss: 0.636571  [   64/  130]
train() client id: f_00008-11-2 loss: 0.560491  [   96/  130]
train() client id: f_00008-11-3 loss: 0.775957  [  128/  130]
train() client id: f_00009-0-0 loss: 1.110490  [   32/  118]
train() client id: f_00009-0-1 loss: 0.999491  [   64/  118]
train() client id: f_00009-0-2 loss: 1.067150  [   96/  118]
train() client id: f_00009-1-0 loss: 1.021357  [   32/  118]
train() client id: f_00009-1-1 loss: 0.837433  [   64/  118]
train() client id: f_00009-1-2 loss: 0.997314  [   96/  118]
train() client id: f_00009-2-0 loss: 0.935675  [   32/  118]
train() client id: f_00009-2-1 loss: 1.020687  [   64/  118]
train() client id: f_00009-2-2 loss: 0.829663  [   96/  118]
train() client id: f_00009-3-0 loss: 1.056463  [   32/  118]
train() client id: f_00009-3-1 loss: 0.855806  [   64/  118]
train() client id: f_00009-3-2 loss: 0.904042  [   96/  118]
train() client id: f_00009-4-0 loss: 0.930116  [   32/  118]
train() client id: f_00009-4-1 loss: 0.823762  [   64/  118]
train() client id: f_00009-4-2 loss: 0.942063  [   96/  118]
train() client id: f_00009-5-0 loss: 0.883410  [   32/  118]
train() client id: f_00009-5-1 loss: 0.719053  [   64/  118]
train() client id: f_00009-5-2 loss: 0.871522  [   96/  118]
train() client id: f_00009-6-0 loss: 0.845589  [   32/  118]
train() client id: f_00009-6-1 loss: 0.783095  [   64/  118]
train() client id: f_00009-6-2 loss: 0.991144  [   96/  118]
train() client id: f_00009-7-0 loss: 0.756950  [   32/  118]
train() client id: f_00009-7-1 loss: 1.092346  [   64/  118]
train() client id: f_00009-7-2 loss: 0.768447  [   96/  118]
train() client id: f_00009-8-0 loss: 1.000710  [   32/  118]
train() client id: f_00009-8-1 loss: 0.687555  [   64/  118]
train() client id: f_00009-8-2 loss: 0.688951  [   96/  118]
train() client id: f_00009-9-0 loss: 0.829518  [   32/  118]
train() client id: f_00009-9-1 loss: 0.813837  [   64/  118]
train() client id: f_00009-9-2 loss: 0.911324  [   96/  118]
train() client id: f_00009-10-0 loss: 0.739490  [   32/  118]
train() client id: f_00009-10-1 loss: 0.895704  [   64/  118]
train() client id: f_00009-10-2 loss: 0.711697  [   96/  118]
train() client id: f_00009-11-0 loss: 1.088650  [   32/  118]
train() client id: f_00009-11-1 loss: 0.674647  [   64/  118]
train() client id: f_00009-11-2 loss: 0.728813  [   96/  118]
At round 33 accuracy: 0.6419098143236074
At round 33 training accuracy: 0.5868544600938967
At round 33 training loss: 0.8313487789777902
update_location
xs = -3.905658 4.200318 185.009024 18.811294 0.979296 3.956410 -147.443192 -126.324852 169.663977 -112.060879 
ys = 177.587959 160.555839 1.320614 -147.455176 139.350187 122.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -0.7114261681174355
ys mean: 47.39414253552871
dists_uav = 203.844886 189.197833 210.309493 179.156060 171.520942 158.426541 178.175154 161.116866 197.723380 150.245307 
uav_gains = -107.925351 -107.005796 -108.341492 -106.374132 -105.883343 -105.004993 -106.311732 -105.190067 -107.538881 -104.424673 
uav_gains_db_mean: -106.40004607943783
dists_bs = 171.113913 179.781500 399.713681 376.218232 179.553953 186.410191 179.749862 180.851111 378.916580 182.213683 
bs_gains = -102.099048 -102.699919 -112.416032 -111.679375 -102.684518 -103.140209 -102.697779 -102.772052 -111.766280 -102.863327 
bs_gains_db_mean: -105.48185393341947
Round 34
-------------------------------
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.50361085 13.46800316  6.40688418  2.3090212  15.53330799  7.47581585
  2.86265615  9.15122779  6.75143071  6.06373403]
obj_prev = 76.52569191747648
eta_min = 6.016067092016877e-15	eta_max = 0.9281867977986356
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 17.761433286506318	eta = 0.9090909090909091
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 32.349008668159414	eta = 0.4991422673511644
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 25.200112188715238	eta = 0.6407414940167677
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.908304317469916	eta = 0.6753618876010812
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.840631284811383	eta = 0.6772789419999331
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.84043047624284	eta = 0.6772846467381501
eta = 0.6772846467381501
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [0.03215411 0.06762571 0.03164373 0.01097323 0.07808855 0.03725794
 0.01378033 0.04567925 0.03317488 0.03011258]
ene_total = [2.12711913 3.82878864 2.11331163 1.00058971 4.36464773 2.28034368
 1.14291118 2.75181183 2.3215154  1.90939155]
ti_comp = [0.45871166 0.48133195 0.45630249 0.4667549  0.48138282 0.47984682
 0.46705067 0.47204883 0.43098118 0.48078779]
ti_coms = [0.09358479 0.0709645  0.09599397 0.08554155 0.07091363 0.07244963
 0.08524578 0.08024762 0.12131527 0.07150866]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.87438211e-06 8.34307382e-05 9.51124209e-06 3.79057801e-07
 1.28428366e-04 1.40388386e-05 7.49775295e-07 2.67339474e-05
 1.22854563e-05 7.38273045e-06]
ene_total = [0.48003762 0.36789952 0.49236369 0.43833721 0.36994456 0.37195374
 0.43684068 0.41256138 0.62225307 0.3667911 ]
optimize_network iter = 0 obj = 4.3589825705706815
eta = 0.6772846467381501
freqs = [35048280.46691417 70248512.05463123 34674068.18486581 11754807.32564252
 81108578.19233961 38822740.19627412 14752502.84041418 48384031.59575192
 38487614.47521663 31315873.21898438]
eta_min = 0.677284646738166	eta_max = 0.6772846467381503
af = 0.012426086302136423	bf = 1.4018686060877306	zeta = 0.013668694932350066	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [2.41523466e-06 2.04068274e-05 2.32641206e-06 9.27160332e-08
 3.14130684e-05 3.43384418e-06 1.83392061e-07 6.53901742e-06
 3.00497385e-06 1.80578656e-06]
ene_total = [1.70271381 1.29453122 1.74651965 1.55598755 1.29560796 1.31845798
 1.55062412 1.46086543 2.20722871 1.30104589]
ti_comp = [0.45871166 0.48133195 0.45630249 0.4667549  0.48138282 0.47984682
 0.46705067 0.47204883 0.43098118 0.48078779]
ti_coms = [0.09358479 0.0709645  0.09599397 0.08554155 0.07091363 0.07244963
 0.08524578 0.08024762 0.12131527 0.07150866]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.87438211e-06 8.34307382e-05 9.51124209e-06 3.79057801e-07
 1.28428366e-04 1.40388386e-05 7.49775295e-07 2.67339474e-05
 1.22854563e-05 7.38273045e-06]
ene_total = [0.48003762 0.36789952 0.49236369 0.43833721 0.36994456 0.37195374
 0.43684068 0.41256138 0.62225307 0.3667911 ]
optimize_network iter = 1 obj = 4.358982570570683
eta = 0.6772846467381503
freqs = [35048280.46691417 70248512.05463123 34674068.18486582 11754807.32564252
 81108578.19233961 38822740.19627412 14752502.84041418 48384031.59575192
 38487614.47521663 31315873.21898438]
Done!
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.28656765e-06 7.84641698e-05 8.94504507e-06 3.56492778e-07
 1.20783123e-04 1.32031172e-05 7.05141742e-07 2.51424959e-05
 1.15541125e-05 6.94324212e-06]
ene_total = [0.00936777 0.00717491 0.00960834 0.00855451 0.00721215 0.00725817
 0.00852528 0.0080499  0.01214308 0.00715781]
At round 34 energy consumption: 0.0850519238371433
At round 34 eta: 0.6772846467381503
At round 34 a_n: 16.536044059297588
At round 34 local rounds: 12.759567346547202
At round 34 global rounds: 51.24033886878731
gradient difference: 0.37334492802619934
train() client id: f_00000-0-0 loss: 1.283703  [   32/  126]
train() client id: f_00000-0-1 loss: 1.094735  [   64/  126]
train() client id: f_00000-0-2 loss: 1.109131  [   96/  126]
train() client id: f_00000-1-0 loss: 1.168449  [   32/  126]
train() client id: f_00000-1-1 loss: 1.036912  [   64/  126]
train() client id: f_00000-1-2 loss: 1.003797  [   96/  126]
train() client id: f_00000-2-0 loss: 0.907200  [   32/  126]
train() client id: f_00000-2-1 loss: 1.109275  [   64/  126]
train() client id: f_00000-2-2 loss: 0.972195  [   96/  126]
train() client id: f_00000-3-0 loss: 0.922018  [   32/  126]
train() client id: f_00000-3-1 loss: 0.846894  [   64/  126]
train() client id: f_00000-3-2 loss: 0.879019  [   96/  126]
train() client id: f_00000-4-0 loss: 0.868730  [   32/  126]
train() client id: f_00000-4-1 loss: 0.874271  [   64/  126]
train() client id: f_00000-4-2 loss: 0.947081  [   96/  126]
train() client id: f_00000-5-0 loss: 0.807445  [   32/  126]
train() client id: f_00000-5-1 loss: 0.852571  [   64/  126]
train() client id: f_00000-5-2 loss: 0.846092  [   96/  126]
train() client id: f_00000-6-0 loss: 0.792168  [   32/  126]
train() client id: f_00000-6-1 loss: 0.765769  [   64/  126]
train() client id: f_00000-6-2 loss: 0.888184  [   96/  126]
train() client id: f_00000-7-0 loss: 0.792115  [   32/  126]
train() client id: f_00000-7-1 loss: 0.778297  [   64/  126]
train() client id: f_00000-7-2 loss: 0.878354  [   96/  126]
train() client id: f_00000-8-0 loss: 0.775142  [   32/  126]
train() client id: f_00000-8-1 loss: 0.788686  [   64/  126]
train() client id: f_00000-8-2 loss: 0.839847  [   96/  126]
train() client id: f_00000-9-0 loss: 0.758799  [   32/  126]
train() client id: f_00000-9-1 loss: 0.813835  [   64/  126]
train() client id: f_00000-9-2 loss: 0.837203  [   96/  126]
train() client id: f_00000-10-0 loss: 0.791929  [   32/  126]
train() client id: f_00000-10-1 loss: 0.747451  [   64/  126]
train() client id: f_00000-10-2 loss: 0.881161  [   96/  126]
train() client id: f_00000-11-0 loss: 0.756551  [   32/  126]
train() client id: f_00000-11-1 loss: 0.810936  [   64/  126]
train() client id: f_00000-11-2 loss: 0.809803  [   96/  126]
train() client id: f_00001-0-0 loss: 0.600783  [   32/  265]
train() client id: f_00001-0-1 loss: 0.422396  [   64/  265]
train() client id: f_00001-0-2 loss: 0.402974  [   96/  265]
train() client id: f_00001-0-3 loss: 0.409776  [  128/  265]
train() client id: f_00001-0-4 loss: 0.525877  [  160/  265]
train() client id: f_00001-0-5 loss: 0.470913  [  192/  265]
train() client id: f_00001-0-6 loss: 0.443601  [  224/  265]
train() client id: f_00001-0-7 loss: 0.448429  [  256/  265]
train() client id: f_00001-1-0 loss: 0.411112  [   32/  265]
train() client id: f_00001-1-1 loss: 0.556743  [   64/  265]
train() client id: f_00001-1-2 loss: 0.502931  [   96/  265]
train() client id: f_00001-1-3 loss: 0.421651  [  128/  265]
train() client id: f_00001-1-4 loss: 0.424209  [  160/  265]
train() client id: f_00001-1-5 loss: 0.449451  [  192/  265]
train() client id: f_00001-1-6 loss: 0.506335  [  224/  265]
train() client id: f_00001-1-7 loss: 0.460996  [  256/  265]
train() client id: f_00001-2-0 loss: 0.448785  [   32/  265]
train() client id: f_00001-2-1 loss: 0.445492  [   64/  265]
train() client id: f_00001-2-2 loss: 0.380219  [   96/  265]
train() client id: f_00001-2-3 loss: 0.506761  [  128/  265]
train() client id: f_00001-2-4 loss: 0.479364  [  160/  265]
train() client id: f_00001-2-5 loss: 0.517212  [  192/  265]
train() client id: f_00001-2-6 loss: 0.545327  [  224/  265]
train() client id: f_00001-2-7 loss: 0.380167  [  256/  265]
train() client id: f_00001-3-0 loss: 0.507424  [   32/  265]
train() client id: f_00001-3-1 loss: 0.446604  [   64/  265]
train() client id: f_00001-3-2 loss: 0.389867  [   96/  265]
train() client id: f_00001-3-3 loss: 0.440196  [  128/  265]
train() client id: f_00001-3-4 loss: 0.367917  [  160/  265]
train() client id: f_00001-3-5 loss: 0.474415  [  192/  265]
train() client id: f_00001-3-6 loss: 0.384906  [  224/  265]
train() client id: f_00001-3-7 loss: 0.679191  [  256/  265]
train() client id: f_00001-4-0 loss: 0.380072  [   32/  265]
train() client id: f_00001-4-1 loss: 0.433303  [   64/  265]
train() client id: f_00001-4-2 loss: 0.460618  [   96/  265]
train() client id: f_00001-4-3 loss: 0.543874  [  128/  265]
train() client id: f_00001-4-4 loss: 0.440089  [  160/  265]
train() client id: f_00001-4-5 loss: 0.378589  [  192/  265]
train() client id: f_00001-4-6 loss: 0.677167  [  224/  265]
train() client id: f_00001-4-7 loss: 0.369447  [  256/  265]
train() client id: f_00001-5-0 loss: 0.393611  [   32/  265]
train() client id: f_00001-5-1 loss: 0.377170  [   64/  265]
train() client id: f_00001-5-2 loss: 0.531166  [   96/  265]
train() client id: f_00001-5-3 loss: 0.511468  [  128/  265]
train() client id: f_00001-5-4 loss: 0.391534  [  160/  265]
train() client id: f_00001-5-5 loss: 0.373715  [  192/  265]
train() client id: f_00001-5-6 loss: 0.561319  [  224/  265]
train() client id: f_00001-5-7 loss: 0.412444  [  256/  265]
train() client id: f_00001-6-0 loss: 0.399368  [   32/  265]
train() client id: f_00001-6-1 loss: 0.387920  [   64/  265]
train() client id: f_00001-6-2 loss: 0.453745  [   96/  265]
train() client id: f_00001-6-3 loss: 0.531158  [  128/  265]
train() client id: f_00001-6-4 loss: 0.549644  [  160/  265]
train() client id: f_00001-6-5 loss: 0.390196  [  192/  265]
train() client id: f_00001-6-6 loss: 0.527757  [  224/  265]
train() client id: f_00001-6-7 loss: 0.435106  [  256/  265]
train() client id: f_00001-7-0 loss: 0.437951  [   32/  265]
train() client id: f_00001-7-1 loss: 0.409286  [   64/  265]
train() client id: f_00001-7-2 loss: 0.469627  [   96/  265]
train() client id: f_00001-7-3 loss: 0.506955  [  128/  265]
train() client id: f_00001-7-4 loss: 0.521104  [  160/  265]
train() client id: f_00001-7-5 loss: 0.409290  [  192/  265]
train() client id: f_00001-7-6 loss: 0.361596  [  224/  265]
train() client id: f_00001-7-7 loss: 0.562384  [  256/  265]
train() client id: f_00001-8-0 loss: 0.462439  [   32/  265]
train() client id: f_00001-8-1 loss: 0.351342  [   64/  265]
train() client id: f_00001-8-2 loss: 0.479460  [   96/  265]
train() client id: f_00001-8-3 loss: 0.475702  [  128/  265]
train() client id: f_00001-8-4 loss: 0.525085  [  160/  265]
train() client id: f_00001-8-5 loss: 0.465221  [  192/  265]
train() client id: f_00001-8-6 loss: 0.464927  [  224/  265]
train() client id: f_00001-8-7 loss: 0.448328  [  256/  265]
train() client id: f_00001-9-0 loss: 0.396554  [   32/  265]
train() client id: f_00001-9-1 loss: 0.623465  [   64/  265]
train() client id: f_00001-9-2 loss: 0.473234  [   96/  265]
train() client id: f_00001-9-3 loss: 0.410320  [  128/  265]
train() client id: f_00001-9-4 loss: 0.441970  [  160/  265]
train() client id: f_00001-9-5 loss: 0.494617  [  192/  265]
train() client id: f_00001-9-6 loss: 0.452233  [  224/  265]
train() client id: f_00001-9-7 loss: 0.386781  [  256/  265]
train() client id: f_00001-10-0 loss: 0.423959  [   32/  265]
train() client id: f_00001-10-1 loss: 0.413340  [   64/  265]
train() client id: f_00001-10-2 loss: 0.418618  [   96/  265]
train() client id: f_00001-10-3 loss: 0.424937  [  128/  265]
train() client id: f_00001-10-4 loss: 0.546727  [  160/  265]
train() client id: f_00001-10-5 loss: 0.506386  [  192/  265]
train() client id: f_00001-10-6 loss: 0.479523  [  224/  265]
train() client id: f_00001-10-7 loss: 0.467279  [  256/  265]
train() client id: f_00001-11-0 loss: 0.646075  [   32/  265]
train() client id: f_00001-11-1 loss: 0.452194  [   64/  265]
train() client id: f_00001-11-2 loss: 0.358673  [   96/  265]
train() client id: f_00001-11-3 loss: 0.457266  [  128/  265]
train() client id: f_00001-11-4 loss: 0.432545  [  160/  265]
train() client id: f_00001-11-5 loss: 0.450773  [  192/  265]
train() client id: f_00001-11-6 loss: 0.375320  [  224/  265]
train() client id: f_00001-11-7 loss: 0.438313  [  256/  265]
train() client id: f_00002-0-0 loss: 0.980858  [   32/  124]
train() client id: f_00002-0-1 loss: 1.085546  [   64/  124]
train() client id: f_00002-0-2 loss: 0.930851  [   96/  124]
train() client id: f_00002-1-0 loss: 1.052371  [   32/  124]
train() client id: f_00002-1-1 loss: 0.793797  [   64/  124]
train() client id: f_00002-1-2 loss: 0.886858  [   96/  124]
train() client id: f_00002-2-0 loss: 1.197387  [   32/  124]
train() client id: f_00002-2-1 loss: 0.838435  [   64/  124]
train() client id: f_00002-2-2 loss: 0.892226  [   96/  124]
train() client id: f_00002-3-0 loss: 1.065703  [   32/  124]
train() client id: f_00002-3-1 loss: 0.977452  [   64/  124]
train() client id: f_00002-3-2 loss: 0.902679  [   96/  124]
train() client id: f_00002-4-0 loss: 0.685217  [   32/  124]
train() client id: f_00002-4-1 loss: 0.905591  [   64/  124]
train() client id: f_00002-4-2 loss: 0.918711  [   96/  124]
train() client id: f_00002-5-0 loss: 0.767105  [   32/  124]
train() client id: f_00002-5-1 loss: 1.085920  [   64/  124]
train() client id: f_00002-5-2 loss: 0.659440  [   96/  124]
train() client id: f_00002-6-0 loss: 0.970199  [   32/  124]
train() client id: f_00002-6-1 loss: 0.876146  [   64/  124]
train() client id: f_00002-6-2 loss: 0.779503  [   96/  124]
train() client id: f_00002-7-0 loss: 0.921635  [   32/  124]
train() client id: f_00002-7-1 loss: 0.909012  [   64/  124]
train() client id: f_00002-7-2 loss: 0.842491  [   96/  124]
train() client id: f_00002-8-0 loss: 0.676591  [   32/  124]
train() client id: f_00002-8-1 loss: 0.750232  [   64/  124]
train() client id: f_00002-8-2 loss: 0.900801  [   96/  124]
train() client id: f_00002-9-0 loss: 0.842821  [   32/  124]
train() client id: f_00002-9-1 loss: 1.011796  [   64/  124]
train() client id: f_00002-9-2 loss: 0.721833  [   96/  124]
train() client id: f_00002-10-0 loss: 0.814513  [   32/  124]
train() client id: f_00002-10-1 loss: 0.807054  [   64/  124]
train() client id: f_00002-10-2 loss: 0.794567  [   96/  124]
train() client id: f_00002-11-0 loss: 0.742822  [   32/  124]
train() client id: f_00002-11-1 loss: 0.741869  [   64/  124]
train() client id: f_00002-11-2 loss: 0.909516  [   96/  124]
train() client id: f_00003-0-0 loss: 1.161448  [   32/   43]
train() client id: f_00003-1-0 loss: 1.146750  [   32/   43]
train() client id: f_00003-2-0 loss: 1.017695  [   32/   43]
train() client id: f_00003-3-0 loss: 1.112396  [   32/   43]
train() client id: f_00003-4-0 loss: 1.039969  [   32/   43]
train() client id: f_00003-5-0 loss: 1.207270  [   32/   43]
train() client id: f_00003-6-0 loss: 1.065532  [   32/   43]
train() client id: f_00003-7-0 loss: 1.007233  [   32/   43]
train() client id: f_00003-8-0 loss: 1.086106  [   32/   43]
train() client id: f_00003-9-0 loss: 1.241210  [   32/   43]
train() client id: f_00003-10-0 loss: 0.911013  [   32/   43]
train() client id: f_00003-11-0 loss: 1.292376  [   32/   43]
train() client id: f_00004-0-0 loss: 0.747060  [   32/  306]
train() client id: f_00004-0-1 loss: 0.767518  [   64/  306]
train() client id: f_00004-0-2 loss: 0.759788  [   96/  306]
train() client id: f_00004-0-3 loss: 0.806600  [  128/  306]
train() client id: f_00004-0-4 loss: 0.793479  [  160/  306]
train() client id: f_00004-0-5 loss: 0.804436  [  192/  306]
train() client id: f_00004-0-6 loss: 0.691729  [  224/  306]
train() client id: f_00004-0-7 loss: 0.903209  [  256/  306]
train() client id: f_00004-0-8 loss: 0.694791  [  288/  306]
train() client id: f_00004-1-0 loss: 0.756421  [   32/  306]
train() client id: f_00004-1-1 loss: 0.680139  [   64/  306]
train() client id: f_00004-1-2 loss: 0.773460  [   96/  306]
train() client id: f_00004-1-3 loss: 0.695118  [  128/  306]
train() client id: f_00004-1-4 loss: 0.760133  [  160/  306]
train() client id: f_00004-1-5 loss: 0.820089  [  192/  306]
train() client id: f_00004-1-6 loss: 0.752887  [  224/  306]
train() client id: f_00004-1-7 loss: 0.794115  [  256/  306]
train() client id: f_00004-1-8 loss: 0.946097  [  288/  306]
train() client id: f_00004-2-0 loss: 0.692489  [   32/  306]
train() client id: f_00004-2-1 loss: 0.847852  [   64/  306]
train() client id: f_00004-2-2 loss: 0.786841  [   96/  306]
train() client id: f_00004-2-3 loss: 0.693067  [  128/  306]
train() client id: f_00004-2-4 loss: 0.744753  [  160/  306]
train() client id: f_00004-2-5 loss: 0.776190  [  192/  306]
train() client id: f_00004-2-6 loss: 0.736932  [  224/  306]
train() client id: f_00004-2-7 loss: 0.782894  [  256/  306]
train() client id: f_00004-2-8 loss: 0.795878  [  288/  306]
train() client id: f_00004-3-0 loss: 0.804733  [   32/  306]
train() client id: f_00004-3-1 loss: 0.643012  [   64/  306]
train() client id: f_00004-3-2 loss: 0.783002  [   96/  306]
train() client id: f_00004-3-3 loss: 0.770201  [  128/  306]
train() client id: f_00004-3-4 loss: 0.782934  [  160/  306]
train() client id: f_00004-3-5 loss: 0.816867  [  192/  306]
train() client id: f_00004-3-6 loss: 0.664956  [  224/  306]
train() client id: f_00004-3-7 loss: 0.911257  [  256/  306]
train() client id: f_00004-3-8 loss: 0.752865  [  288/  306]
train() client id: f_00004-4-0 loss: 0.754034  [   32/  306]
train() client id: f_00004-4-1 loss: 0.633964  [   64/  306]
train() client id: f_00004-4-2 loss: 0.890237  [   96/  306]
train() client id: f_00004-4-3 loss: 0.816630  [  128/  306]
train() client id: f_00004-4-4 loss: 0.730268  [  160/  306]
train() client id: f_00004-4-5 loss: 0.800996  [  192/  306]
train() client id: f_00004-4-6 loss: 0.856426  [  224/  306]
train() client id: f_00004-4-7 loss: 0.762493  [  256/  306]
train() client id: f_00004-4-8 loss: 0.747451  [  288/  306]
train() client id: f_00004-5-0 loss: 0.682729  [   32/  306]
train() client id: f_00004-5-1 loss: 0.918674  [   64/  306]
train() client id: f_00004-5-2 loss: 0.792978  [   96/  306]
train() client id: f_00004-5-3 loss: 0.761207  [  128/  306]
train() client id: f_00004-5-4 loss: 0.675673  [  160/  306]
train() client id: f_00004-5-5 loss: 0.741674  [  192/  306]
train() client id: f_00004-5-6 loss: 0.905180  [  224/  306]
train() client id: f_00004-5-7 loss: 0.841466  [  256/  306]
train() client id: f_00004-5-8 loss: 0.640599  [  288/  306]
train() client id: f_00004-6-0 loss: 0.817691  [   32/  306]
train() client id: f_00004-6-1 loss: 0.823216  [   64/  306]
train() client id: f_00004-6-2 loss: 0.709447  [   96/  306]
train() client id: f_00004-6-3 loss: 0.798361  [  128/  306]
train() client id: f_00004-6-4 loss: 0.787576  [  160/  306]
train() client id: f_00004-6-5 loss: 0.864973  [  192/  306]
train() client id: f_00004-6-6 loss: 0.697557  [  224/  306]
train() client id: f_00004-6-7 loss: 0.760888  [  256/  306]
train() client id: f_00004-6-8 loss: 0.785707  [  288/  306]
train() client id: f_00004-7-0 loss: 0.839680  [   32/  306]
train() client id: f_00004-7-1 loss: 0.749351  [   64/  306]
train() client id: f_00004-7-2 loss: 0.926135  [   96/  306]
train() client id: f_00004-7-3 loss: 0.786370  [  128/  306]
train() client id: f_00004-7-4 loss: 0.658906  [  160/  306]
train() client id: f_00004-7-5 loss: 0.683257  [  192/  306]
train() client id: f_00004-7-6 loss: 0.826822  [  224/  306]
train() client id: f_00004-7-7 loss: 0.669193  [  256/  306]
train() client id: f_00004-7-8 loss: 0.857758  [  288/  306]
train() client id: f_00004-8-0 loss: 0.708105  [   32/  306]
train() client id: f_00004-8-1 loss: 0.881664  [   64/  306]
train() client id: f_00004-8-2 loss: 0.832191  [   96/  306]
train() client id: f_00004-8-3 loss: 0.828079  [  128/  306]
train() client id: f_00004-8-4 loss: 0.815129  [  160/  306]
train() client id: f_00004-8-5 loss: 0.859771  [  192/  306]
train() client id: f_00004-8-6 loss: 0.634168  [  224/  306]
train() client id: f_00004-8-7 loss: 0.723659  [  256/  306]
train() client id: f_00004-8-8 loss: 0.709174  [  288/  306]
train() client id: f_00004-9-0 loss: 0.752121  [   32/  306]
train() client id: f_00004-9-1 loss: 0.945812  [   64/  306]
train() client id: f_00004-9-2 loss: 0.695067  [   96/  306]
train() client id: f_00004-9-3 loss: 0.725901  [  128/  306]
train() client id: f_00004-9-4 loss: 0.771429  [  160/  306]
train() client id: f_00004-9-5 loss: 0.812920  [  192/  306]
train() client id: f_00004-9-6 loss: 0.772412  [  224/  306]
train() client id: f_00004-9-7 loss: 0.726215  [  256/  306]
train() client id: f_00004-9-8 loss: 0.819049  [  288/  306]
train() client id: f_00004-10-0 loss: 0.693133  [   32/  306]
train() client id: f_00004-10-1 loss: 0.768700  [   64/  306]
train() client id: f_00004-10-2 loss: 0.789551  [   96/  306]
train() client id: f_00004-10-3 loss: 0.764926  [  128/  306]
train() client id: f_00004-10-4 loss: 0.859788  [  160/  306]
train() client id: f_00004-10-5 loss: 0.835889  [  192/  306]
train() client id: f_00004-10-6 loss: 0.939879  [  224/  306]
train() client id: f_00004-10-7 loss: 0.554984  [  256/  306]
train() client id: f_00004-10-8 loss: 0.801834  [  288/  306]
train() client id: f_00004-11-0 loss: 0.793910  [   32/  306]
train() client id: f_00004-11-1 loss: 0.695857  [   64/  306]
train() client id: f_00004-11-2 loss: 0.842793  [   96/  306]
train() client id: f_00004-11-3 loss: 0.938697  [  128/  306]
train() client id: f_00004-11-4 loss: 0.768913  [  160/  306]
train() client id: f_00004-11-5 loss: 0.626832  [  192/  306]
train() client id: f_00004-11-6 loss: 0.732874  [  224/  306]
train() client id: f_00004-11-7 loss: 0.802027  [  256/  306]
train() client id: f_00004-11-8 loss: 0.775365  [  288/  306]
train() client id: f_00005-0-0 loss: 0.945862  [   32/  146]
train() client id: f_00005-0-1 loss: 0.820011  [   64/  146]
train() client id: f_00005-0-2 loss: 0.610175  [   96/  146]
train() client id: f_00005-0-3 loss: 0.664919  [  128/  146]
train() client id: f_00005-1-0 loss: 0.559301  [   32/  146]
train() client id: f_00005-1-1 loss: 0.796249  [   64/  146]
train() client id: f_00005-1-2 loss: 0.861803  [   96/  146]
train() client id: f_00005-1-3 loss: 0.806756  [  128/  146]
train() client id: f_00005-2-0 loss: 0.804293  [   32/  146]
train() client id: f_00005-2-1 loss: 0.778418  [   64/  146]
train() client id: f_00005-2-2 loss: 0.651589  [   96/  146]
train() client id: f_00005-2-3 loss: 0.567980  [  128/  146]
train() client id: f_00005-3-0 loss: 0.842286  [   32/  146]
train() client id: f_00005-3-1 loss: 0.581418  [   64/  146]
train() client id: f_00005-3-2 loss: 0.704949  [   96/  146]
train() client id: f_00005-3-3 loss: 0.758774  [  128/  146]
train() client id: f_00005-4-0 loss: 0.771189  [   32/  146]
train() client id: f_00005-4-1 loss: 0.641473  [   64/  146]
train() client id: f_00005-4-2 loss: 0.739924  [   96/  146]
train() client id: f_00005-4-3 loss: 0.570942  [  128/  146]
train() client id: f_00005-5-0 loss: 0.786765  [   32/  146]
train() client id: f_00005-5-1 loss: 0.694441  [   64/  146]
train() client id: f_00005-5-2 loss: 0.614307  [   96/  146]
train() client id: f_00005-5-3 loss: 0.813594  [  128/  146]
train() client id: f_00005-6-0 loss: 0.747401  [   32/  146]
train() client id: f_00005-6-1 loss: 0.828340  [   64/  146]
train() client id: f_00005-6-2 loss: 0.758348  [   96/  146]
train() client id: f_00005-6-3 loss: 0.572070  [  128/  146]
train() client id: f_00005-7-0 loss: 0.761939  [   32/  146]
train() client id: f_00005-7-1 loss: 0.620897  [   64/  146]
train() client id: f_00005-7-2 loss: 0.519713  [   96/  146]
train() client id: f_00005-7-3 loss: 0.806478  [  128/  146]
train() client id: f_00005-8-0 loss: 0.721861  [   32/  146]
train() client id: f_00005-8-1 loss: 0.801787  [   64/  146]
train() client id: f_00005-8-2 loss: 0.754194  [   96/  146]
train() client id: f_00005-8-3 loss: 0.622880  [  128/  146]
train() client id: f_00005-9-0 loss: 0.648783  [   32/  146]
train() client id: f_00005-9-1 loss: 0.815997  [   64/  146]
train() client id: f_00005-9-2 loss: 0.741960  [   96/  146]
train() client id: f_00005-9-3 loss: 0.658405  [  128/  146]
train() client id: f_00005-10-0 loss: 0.713309  [   32/  146]
train() client id: f_00005-10-1 loss: 0.897661  [   64/  146]
train() client id: f_00005-10-2 loss: 0.647718  [   96/  146]
train() client id: f_00005-10-3 loss: 0.465061  [  128/  146]
train() client id: f_00005-11-0 loss: 0.748442  [   32/  146]
train() client id: f_00005-11-1 loss: 0.735184  [   64/  146]
train() client id: f_00005-11-2 loss: 0.664094  [   96/  146]
train() client id: f_00005-11-3 loss: 0.720831  [  128/  146]
train() client id: f_00006-0-0 loss: 0.385492  [   32/   54]
train() client id: f_00006-1-0 loss: 0.392358  [   32/   54]
train() client id: f_00006-2-0 loss: 0.493191  [   32/   54]
train() client id: f_00006-3-0 loss: 0.447919  [   32/   54]
train() client id: f_00006-4-0 loss: 0.440832  [   32/   54]
train() client id: f_00006-5-0 loss: 0.452211  [   32/   54]
train() client id: f_00006-6-0 loss: 0.474273  [   32/   54]
train() client id: f_00006-7-0 loss: 0.443522  [   32/   54]
train() client id: f_00006-8-0 loss: 0.452569  [   32/   54]
train() client id: f_00006-9-0 loss: 0.485698  [   32/   54]
train() client id: f_00006-10-0 loss: 0.427085  [   32/   54]
train() client id: f_00006-11-0 loss: 0.448739  [   32/   54]
train() client id: f_00007-0-0 loss: 0.527710  [   32/  179]
train() client id: f_00007-0-1 loss: 0.379391  [   64/  179]
train() client id: f_00007-0-2 loss: 0.791522  [   96/  179]
train() client id: f_00007-0-3 loss: 0.574212  [  128/  179]
train() client id: f_00007-0-4 loss: 0.543076  [  160/  179]
train() client id: f_00007-1-0 loss: 0.594783  [   32/  179]
train() client id: f_00007-1-1 loss: 0.553234  [   64/  179]
train() client id: f_00007-1-2 loss: 0.526661  [   96/  179]
train() client id: f_00007-1-3 loss: 0.459831  [  128/  179]
train() client id: f_00007-1-4 loss: 0.480910  [  160/  179]
train() client id: f_00007-2-0 loss: 0.552905  [   32/  179]
train() client id: f_00007-2-1 loss: 0.604326  [   64/  179]
train() client id: f_00007-2-2 loss: 0.485815  [   96/  179]
train() client id: f_00007-2-3 loss: 0.435135  [  128/  179]
train() client id: f_00007-2-4 loss: 0.534099  [  160/  179]
train() client id: f_00007-3-0 loss: 0.416625  [   32/  179]
train() client id: f_00007-3-1 loss: 0.442465  [   64/  179]
train() client id: f_00007-3-2 loss: 0.584339  [   96/  179]
train() client id: f_00007-3-3 loss: 0.471393  [  128/  179]
train() client id: f_00007-3-4 loss: 0.710980  [  160/  179]
train() client id: f_00007-4-0 loss: 0.338793  [   32/  179]
train() client id: f_00007-4-1 loss: 0.566766  [   64/  179]
train() client id: f_00007-4-2 loss: 0.493822  [   96/  179]
train() client id: f_00007-4-3 loss: 0.691541  [  128/  179]
train() client id: f_00007-4-4 loss: 0.363957  [  160/  179]
train() client id: f_00007-5-0 loss: 0.685753  [   32/  179]
train() client id: f_00007-5-1 loss: 0.338251  [   64/  179]
train() client id: f_00007-5-2 loss: 0.385461  [   96/  179]
train() client id: f_00007-5-3 loss: 0.532620  [  128/  179]
train() client id: f_00007-5-4 loss: 0.449283  [  160/  179]
train() client id: f_00007-6-0 loss: 0.360247  [   32/  179]
train() client id: f_00007-6-1 loss: 0.346191  [   64/  179]
train() client id: f_00007-6-2 loss: 0.651781  [   96/  179]
train() client id: f_00007-6-3 loss: 0.659232  [  128/  179]
train() client id: f_00007-6-4 loss: 0.501296  [  160/  179]
train() client id: f_00007-7-0 loss: 0.438717  [   32/  179]
train() client id: f_00007-7-1 loss: 0.395574  [   64/  179]
train() client id: f_00007-7-2 loss: 0.529817  [   96/  179]
train() client id: f_00007-7-3 loss: 0.340843  [  128/  179]
train() client id: f_00007-7-4 loss: 0.659698  [  160/  179]
train() client id: f_00007-8-0 loss: 0.588187  [   32/  179]
train() client id: f_00007-8-1 loss: 0.329786  [   64/  179]
train() client id: f_00007-8-2 loss: 0.328219  [   96/  179]
train() client id: f_00007-8-3 loss: 0.598263  [  128/  179]
train() client id: f_00007-8-4 loss: 0.631957  [  160/  179]
train() client id: f_00007-9-0 loss: 0.350613  [   32/  179]
train() client id: f_00007-9-1 loss: 0.580266  [   64/  179]
train() client id: f_00007-9-2 loss: 0.404244  [   96/  179]
train() client id: f_00007-9-3 loss: 0.504010  [  128/  179]
train() client id: f_00007-9-4 loss: 0.546763  [  160/  179]
train() client id: f_00007-10-0 loss: 0.529859  [   32/  179]
train() client id: f_00007-10-1 loss: 0.556400  [   64/  179]
train() client id: f_00007-10-2 loss: 0.310613  [   96/  179]
train() client id: f_00007-10-3 loss: 0.337601  [  128/  179]
train() client id: f_00007-10-4 loss: 0.592890  [  160/  179]
train() client id: f_00007-11-0 loss: 0.302375  [   32/  179]
train() client id: f_00007-11-1 loss: 0.505222  [   64/  179]
train() client id: f_00007-11-2 loss: 0.568126  [   96/  179]
train() client id: f_00007-11-3 loss: 0.743283  [  128/  179]
train() client id: f_00007-11-4 loss: 0.297176  [  160/  179]
train() client id: f_00008-0-0 loss: 0.712721  [   32/  130]
train() client id: f_00008-0-1 loss: 0.722238  [   64/  130]
train() client id: f_00008-0-2 loss: 0.634809  [   96/  130]
train() client id: f_00008-0-3 loss: 0.684039  [  128/  130]
train() client id: f_00008-1-0 loss: 0.647995  [   32/  130]
train() client id: f_00008-1-1 loss: 0.699977  [   64/  130]
train() client id: f_00008-1-2 loss: 0.790534  [   96/  130]
train() client id: f_00008-1-3 loss: 0.617212  [  128/  130]
train() client id: f_00008-2-0 loss: 0.623611  [   32/  130]
train() client id: f_00008-2-1 loss: 0.653654  [   64/  130]
train() client id: f_00008-2-2 loss: 0.652729  [   96/  130]
train() client id: f_00008-2-3 loss: 0.786228  [  128/  130]
train() client id: f_00008-3-0 loss: 0.733953  [   32/  130]
train() client id: f_00008-3-1 loss: 0.541514  [   64/  130]
train() client id: f_00008-3-2 loss: 0.812361  [   96/  130]
train() client id: f_00008-3-3 loss: 0.659494  [  128/  130]
train() client id: f_00008-4-0 loss: 0.614528  [   32/  130]
train() client id: f_00008-4-1 loss: 0.646679  [   64/  130]
train() client id: f_00008-4-2 loss: 0.733459  [   96/  130]
train() client id: f_00008-4-3 loss: 0.727863  [  128/  130]
train() client id: f_00008-5-0 loss: 0.578119  [   32/  130]
train() client id: f_00008-5-1 loss: 0.659664  [   64/  130]
train() client id: f_00008-5-2 loss: 0.810883  [   96/  130]
train() client id: f_00008-5-3 loss: 0.676326  [  128/  130]
train() client id: f_00008-6-0 loss: 0.642815  [   32/  130]
train() client id: f_00008-6-1 loss: 0.702309  [   64/  130]
train() client id: f_00008-6-2 loss: 0.687883  [   96/  130]
train() client id: f_00008-6-3 loss: 0.704930  [  128/  130]
train() client id: f_00008-7-0 loss: 0.637798  [   32/  130]
train() client id: f_00008-7-1 loss: 0.740195  [   64/  130]
train() client id: f_00008-7-2 loss: 0.672183  [   96/  130]
train() client id: f_00008-7-3 loss: 0.687251  [  128/  130]
train() client id: f_00008-8-0 loss: 0.612724  [   32/  130]
train() client id: f_00008-8-1 loss: 0.667631  [   64/  130]
train() client id: f_00008-8-2 loss: 0.671628  [   96/  130]
train() client id: f_00008-8-3 loss: 0.767590  [  128/  130]
train() client id: f_00008-9-0 loss: 0.568372  [   32/  130]
train() client id: f_00008-9-1 loss: 0.719478  [   64/  130]
train() client id: f_00008-9-2 loss: 0.659018  [   96/  130]
train() client id: f_00008-9-3 loss: 0.745936  [  128/  130]
train() client id: f_00008-10-0 loss: 0.645750  [   32/  130]
train() client id: f_00008-10-1 loss: 0.672793  [   64/  130]
train() client id: f_00008-10-2 loss: 0.674288  [   96/  130]
train() client id: f_00008-10-3 loss: 0.722260  [  128/  130]
train() client id: f_00008-11-0 loss: 0.757606  [   32/  130]
train() client id: f_00008-11-1 loss: 0.686723  [   64/  130]
train() client id: f_00008-11-2 loss: 0.623438  [   96/  130]
train() client id: f_00008-11-3 loss: 0.664130  [  128/  130]
train() client id: f_00009-0-0 loss: 1.043831  [   32/  118]
train() client id: f_00009-0-1 loss: 1.130440  [   64/  118]
train() client id: f_00009-0-2 loss: 1.227601  [   96/  118]
train() client id: f_00009-1-0 loss: 1.136662  [   32/  118]
train() client id: f_00009-1-1 loss: 0.946773  [   64/  118]
train() client id: f_00009-1-2 loss: 1.101971  [   96/  118]
train() client id: f_00009-2-0 loss: 0.827875  [   32/  118]
train() client id: f_00009-2-1 loss: 1.260764  [   64/  118]
train() client id: f_00009-2-2 loss: 0.976297  [   96/  118]
train() client id: f_00009-3-0 loss: 1.036157  [   32/  118]
train() client id: f_00009-3-1 loss: 0.953441  [   64/  118]
train() client id: f_00009-3-2 loss: 0.890803  [   96/  118]
train() client id: f_00009-4-0 loss: 0.988439  [   32/  118]
train() client id: f_00009-4-1 loss: 1.092131  [   64/  118]
train() client id: f_00009-4-2 loss: 0.938674  [   96/  118]
train() client id: f_00009-5-0 loss: 1.061103  [   32/  118]
train() client id: f_00009-5-1 loss: 0.755432  [   64/  118]
train() client id: f_00009-5-2 loss: 0.930229  [   96/  118]
train() client id: f_00009-6-0 loss: 0.871389  [   32/  118]
train() client id: f_00009-6-1 loss: 0.943376  [   64/  118]
train() client id: f_00009-6-2 loss: 0.947639  [   96/  118]
train() client id: f_00009-7-0 loss: 0.920573  [   32/  118]
train() client id: f_00009-7-1 loss: 0.907018  [   64/  118]
train() client id: f_00009-7-2 loss: 0.989858  [   96/  118]
train() client id: f_00009-8-0 loss: 0.947138  [   32/  118]
train() client id: f_00009-8-1 loss: 0.827674  [   64/  118]
train() client id: f_00009-8-2 loss: 1.078284  [   96/  118]
train() client id: f_00009-9-0 loss: 0.881625  [   32/  118]
train() client id: f_00009-9-1 loss: 0.933383  [   64/  118]
train() client id: f_00009-9-2 loss: 0.982956  [   96/  118]
train() client id: f_00009-10-0 loss: 1.078730  [   32/  118]
train() client id: f_00009-10-1 loss: 0.781079  [   64/  118]
train() client id: f_00009-10-2 loss: 0.827474  [   96/  118]
train() client id: f_00009-11-0 loss: 0.980896  [   32/  118]
train() client id: f_00009-11-1 loss: 0.757708  [   64/  118]
train() client id: f_00009-11-2 loss: 0.912850  [   96/  118]
At round 34 accuracy: 0.6419098143236074
At round 34 training accuracy: 0.5888665325285044
At round 34 training loss: 0.8144423308586769
update_location
xs = -3.905658 4.200318 190.009024 18.811294 0.979296 3.956410 -152.443192 -131.324852 174.663977 -117.060879 
ys = 182.587959 165.555839 1.320614 -152.455176 144.350187 127.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -1.2114261681174354
ys mean: 48.89414253552871
dists_uav = 208.215314 193.458984 214.721152 183.293332 175.607333 162.333331 182.334356 165.066329 202.030133 154.010588 
uav_gains = -108.205518 -107.271979 -108.632453 -106.635691 -106.147541 -105.272899 -106.575273 -105.457174 -107.810189 -104.695202 
uav_gains_db_mean: -106.67039193676908
dists_bs = 171.262520 179.449007 404.222855 380.512431 178.628452 185.072691 179.051514 179.569967 383.470226 180.547598 
bs_gains = -102.109604 -102.677409 -112.552443 -111.817387 -102.621677 -103.052645 -102.650443 -102.685603 -111.911545 -102.751627 
bs_gains_db_mean: -105.48303834723907
Round 35
-------------------------------
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.3715199  13.18889052  6.27703667  2.2632532  15.21119563  7.32046252
  2.80542146  8.96352054  6.61377748  5.93751329]
obj_prev = 74.95259119677627
eta_min = 3.0936410554122904e-15	eta_max = 0.9287856551033156
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 17.39350337614215	eta = 0.9090909090909091
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 31.811065698035446	eta = 0.4970684084145405
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 24.73112036289277	eta = 0.6393675484357765
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.451279643601314	eta = 0.6742606815832007
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.38394553843602	eta = 0.6762022161958229
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.38374385979249	eta = 0.6762080482621735
eta = 0.6762080482621735
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [0.03228538 0.0679018  0.03177292 0.01101803 0.07840736 0.03741005
 0.01383659 0.04586574 0.03331032 0.03023552]
ene_total = [2.0907422  3.75044408 2.07789445 0.9851544  4.27493166 2.23178708
 1.1246433  2.70086556 2.2793763  1.86790483]
ti_comp = [0.46966707 0.49397079 0.46711134 0.47805782 0.49415416 0.49271153
 0.47835241 0.48347539 0.44219867 0.49372516]
ti_coms = [0.09519389 0.07089017 0.09774963 0.08680315 0.07070681 0.07214943
 0.08650855 0.08138558 0.1226623  0.07113581]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [9.53493184e-06 8.01901942e-05 9.18777644e-06 3.65789049e-07
 1.23374545e-04 1.34790504e-05 7.23554964e-07 2.57986237e-05
 1.18135760e-05 7.08698796e-06]
ene_total = [0.47656101 0.35854674 0.48932539 0.4341386  0.35978946 0.36150821
 0.43268317 0.40831618 0.61405008 0.35611918]
optimize_network iter = 0 obj = 4.291038018675944
eta = 0.6762080482621735
freqs = [34370497.98105174 68730577.98040815 34010002.78410535 11523739.23427672
 79334916.60468721 37963436.80656631 14462760.16316765 47433377.96218494
 37664422.59882781 30619785.51757285]
eta_min = 0.6762080482621772	eta_max = 0.6762080482621733
af = 0.011653184886415587	bf = 1.3848442141784587	zeta = 0.012818503375057147	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [2.32272370e-06 1.95344516e-05 2.23815612e-06 8.91067609e-08
 3.00542243e-05 3.28351691e-06 1.76259075e-07 6.28458343e-06
 2.87780483e-06 1.72640090e-06]
ene_total = [1.69608812 1.26623554 1.74159793 1.54622714 1.26484323 1.28577181
 1.54099513 1.45082842 2.18547748 1.2674389 ]
ti_comp = [0.46966707 0.49397079 0.46711134 0.47805782 0.49415416 0.49271153
 0.47835241 0.48347539 0.44219867 0.49372516]
ti_coms = [0.09519389 0.07089017 0.09774963 0.08680315 0.07070681 0.07214943
 0.08650855 0.08138558 0.1226623  0.07113581]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [9.53493184e-06 8.01901942e-05 9.18777644e-06 3.65789049e-07
 1.23374545e-04 1.34790504e-05 7.23554964e-07 2.57986237e-05
 1.18135760e-05 7.08698796e-06]
ene_total = [0.47656101 0.35854674 0.48932539 0.4341386  0.35978946 0.36150821
 0.43268317 0.40831618 0.61405008 0.35611918]
optimize_network iter = 1 obj = 4.291038018675939
eta = 0.6762080482621733
freqs = [34370497.98105174 68730577.98040818 34010002.78410535 11523739.23427672
 79334916.60468724 37963436.80656631 14462760.16316766 47433377.96218494
 37664422.59882782 30619785.51757286]
Done!
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [8.93086335e-06 7.51098884e-05 8.60570135e-06 3.42615140e-07
 1.15558372e-04 1.26251094e-05 6.77715438e-07 2.41641982e-05
 1.10651481e-05 6.63800456e-06]
ene_total = [0.00952832 0.00716413 0.00978357 0.00868066 0.00718624 0.00722757
 0.00865153 0.00816272 0.01227729 0.00712022]
At round 35 energy consumption: 0.0857822507966446
At round 35 eta: 0.6762080482621733
At round 35 a_n: 16.193498212328272
At round 35 local rounds: 12.811659702043181
At round 35 global rounds: 50.012046702878195
gradient difference: 0.4618118405342102
train() client id: f_00000-0-0 loss: 1.260619  [   32/  126]
train() client id: f_00000-0-1 loss: 1.346033  [   64/  126]
train() client id: f_00000-0-2 loss: 1.260863  [   96/  126]
train() client id: f_00000-1-0 loss: 1.296008  [   32/  126]
train() client id: f_00000-1-1 loss: 1.014663  [   64/  126]
train() client id: f_00000-1-2 loss: 1.094566  [   96/  126]
train() client id: f_00000-2-0 loss: 0.988126  [   32/  126]
train() client id: f_00000-2-1 loss: 1.142511  [   64/  126]
train() client id: f_00000-2-2 loss: 0.862166  [   96/  126]
train() client id: f_00000-3-0 loss: 0.995530  [   32/  126]
train() client id: f_00000-3-1 loss: 1.005018  [   64/  126]
train() client id: f_00000-3-2 loss: 0.968969  [   96/  126]
train() client id: f_00000-4-0 loss: 0.902209  [   32/  126]
train() client id: f_00000-4-1 loss: 0.927747  [   64/  126]
train() client id: f_00000-4-2 loss: 0.881294  [   96/  126]
train() client id: f_00000-5-0 loss: 0.813478  [   32/  126]
train() client id: f_00000-5-1 loss: 0.954196  [   64/  126]
train() client id: f_00000-5-2 loss: 0.846393  [   96/  126]
train() client id: f_00000-6-0 loss: 0.907572  [   32/  126]
train() client id: f_00000-6-1 loss: 0.761288  [   64/  126]
train() client id: f_00000-6-2 loss: 0.966062  [   96/  126]
train() client id: f_00000-7-0 loss: 0.906095  [   32/  126]
train() client id: f_00000-7-1 loss: 0.736379  [   64/  126]
train() client id: f_00000-7-2 loss: 0.902037  [   96/  126]
train() client id: f_00000-8-0 loss: 0.840150  [   32/  126]
train() client id: f_00000-8-1 loss: 0.746350  [   64/  126]
train() client id: f_00000-8-2 loss: 0.886147  [   96/  126]
train() client id: f_00000-9-0 loss: 0.796156  [   32/  126]
train() client id: f_00000-9-1 loss: 0.768172  [   64/  126]
train() client id: f_00000-9-2 loss: 0.844110  [   96/  126]
train() client id: f_00000-10-0 loss: 0.850172  [   32/  126]
train() client id: f_00000-10-1 loss: 0.684981  [   64/  126]
train() client id: f_00000-10-2 loss: 0.882793  [   96/  126]
train() client id: f_00000-11-0 loss: 0.847917  [   32/  126]
train() client id: f_00000-11-1 loss: 0.799897  [   64/  126]
train() client id: f_00000-11-2 loss: 0.732864  [   96/  126]
train() client id: f_00001-0-0 loss: 0.453465  [   32/  265]
train() client id: f_00001-0-1 loss: 0.423513  [   64/  265]
train() client id: f_00001-0-2 loss: 0.440304  [   96/  265]
train() client id: f_00001-0-3 loss: 0.438298  [  128/  265]
train() client id: f_00001-0-4 loss: 0.458825  [  160/  265]
train() client id: f_00001-0-5 loss: 0.394594  [  192/  265]
train() client id: f_00001-0-6 loss: 0.439004  [  224/  265]
train() client id: f_00001-0-7 loss: 0.392838  [  256/  265]
train() client id: f_00001-1-0 loss: 0.386387  [   32/  265]
train() client id: f_00001-1-1 loss: 0.437550  [   64/  265]
train() client id: f_00001-1-2 loss: 0.472950  [   96/  265]
train() client id: f_00001-1-3 loss: 0.445762  [  128/  265]
train() client id: f_00001-1-4 loss: 0.364489  [  160/  265]
train() client id: f_00001-1-5 loss: 0.465823  [  192/  265]
train() client id: f_00001-1-6 loss: 0.398057  [  224/  265]
train() client id: f_00001-1-7 loss: 0.401187  [  256/  265]
train() client id: f_00001-2-0 loss: 0.358536  [   32/  265]
train() client id: f_00001-2-1 loss: 0.383993  [   64/  265]
train() client id: f_00001-2-2 loss: 0.499911  [   96/  265]
train() client id: f_00001-2-3 loss: 0.376952  [  128/  265]
train() client id: f_00001-2-4 loss: 0.419359  [  160/  265]
train() client id: f_00001-2-5 loss: 0.319882  [  192/  265]
train() client id: f_00001-2-6 loss: 0.363362  [  224/  265]
train() client id: f_00001-2-7 loss: 0.561449  [  256/  265]
train() client id: f_00001-3-0 loss: 0.435227  [   32/  265]
train() client id: f_00001-3-1 loss: 0.402577  [   64/  265]
train() client id: f_00001-3-2 loss: 0.419712  [   96/  265]
train() client id: f_00001-3-3 loss: 0.361144  [  128/  265]
train() client id: f_00001-3-4 loss: 0.398891  [  160/  265]
train() client id: f_00001-3-5 loss: 0.380416  [  192/  265]
train() client id: f_00001-3-6 loss: 0.446790  [  224/  265]
train() client id: f_00001-3-7 loss: 0.451516  [  256/  265]
train() client id: f_00001-4-0 loss: 0.383546  [   32/  265]
train() client id: f_00001-4-1 loss: 0.299704  [   64/  265]
train() client id: f_00001-4-2 loss: 0.392906  [   96/  265]
train() client id: f_00001-4-3 loss: 0.499264  [  128/  265]
train() client id: f_00001-4-4 loss: 0.500093  [  160/  265]
train() client id: f_00001-4-5 loss: 0.387355  [  192/  265]
train() client id: f_00001-4-6 loss: 0.393499  [  224/  265]
train() client id: f_00001-4-7 loss: 0.410909  [  256/  265]
train() client id: f_00001-5-0 loss: 0.435177  [   32/  265]
train() client id: f_00001-5-1 loss: 0.547055  [   64/  265]
train() client id: f_00001-5-2 loss: 0.285277  [   96/  265]
train() client id: f_00001-5-3 loss: 0.442868  [  128/  265]
train() client id: f_00001-5-4 loss: 0.427449  [  160/  265]
train() client id: f_00001-5-5 loss: 0.303947  [  192/  265]
train() client id: f_00001-5-6 loss: 0.473512  [  224/  265]
train() client id: f_00001-5-7 loss: 0.329650  [  256/  265]
train() client id: f_00001-6-0 loss: 0.349364  [   32/  265]
train() client id: f_00001-6-1 loss: 0.455057  [   64/  265]
train() client id: f_00001-6-2 loss: 0.323231  [   96/  265]
train() client id: f_00001-6-3 loss: 0.354819  [  128/  265]
train() client id: f_00001-6-4 loss: 0.478582  [  160/  265]
train() client id: f_00001-6-5 loss: 0.356198  [  192/  265]
train() client id: f_00001-6-6 loss: 0.387836  [  224/  265]
train() client id: f_00001-6-7 loss: 0.467565  [  256/  265]
train() client id: f_00001-7-0 loss: 0.407642  [   32/  265]
train() client id: f_00001-7-1 loss: 0.467676  [   64/  265]
train() client id: f_00001-7-2 loss: 0.294270  [   96/  265]
train() client id: f_00001-7-3 loss: 0.452423  [  128/  265]
train() client id: f_00001-7-4 loss: 0.354476  [  160/  265]
train() client id: f_00001-7-5 loss: 0.358723  [  192/  265]
train() client id: f_00001-7-6 loss: 0.432866  [  224/  265]
train() client id: f_00001-7-7 loss: 0.360648  [  256/  265]
train() client id: f_00001-8-0 loss: 0.306142  [   32/  265]
train() client id: f_00001-8-1 loss: 0.441459  [   64/  265]
train() client id: f_00001-8-2 loss: 0.428975  [   96/  265]
train() client id: f_00001-8-3 loss: 0.280431  [  128/  265]
train() client id: f_00001-8-4 loss: 0.308352  [  160/  265]
train() client id: f_00001-8-5 loss: 0.509672  [  192/  265]
train() client id: f_00001-8-6 loss: 0.400825  [  224/  265]
train() client id: f_00001-8-7 loss: 0.476437  [  256/  265]
train() client id: f_00001-9-0 loss: 0.369185  [   32/  265]
train() client id: f_00001-9-1 loss: 0.376500  [   64/  265]
train() client id: f_00001-9-2 loss: 0.602282  [   96/  265]
train() client id: f_00001-9-3 loss: 0.321668  [  128/  265]
train() client id: f_00001-9-4 loss: 0.347175  [  160/  265]
train() client id: f_00001-9-5 loss: 0.345269  [  192/  265]
train() client id: f_00001-9-6 loss: 0.303330  [  224/  265]
train() client id: f_00001-9-7 loss: 0.465350  [  256/  265]
train() client id: f_00001-10-0 loss: 0.421896  [   32/  265]
train() client id: f_00001-10-1 loss: 0.408773  [   64/  265]
train() client id: f_00001-10-2 loss: 0.453355  [   96/  265]
train() client id: f_00001-10-3 loss: 0.396425  [  128/  265]
train() client id: f_00001-10-4 loss: 0.345617  [  160/  265]
train() client id: f_00001-10-5 loss: 0.487709  [  192/  265]
train() client id: f_00001-10-6 loss: 0.369790  [  224/  265]
train() client id: f_00001-10-7 loss: 0.319345  [  256/  265]
train() client id: f_00001-11-0 loss: 0.388851  [   32/  265]
train() client id: f_00001-11-1 loss: 0.299662  [   64/  265]
train() client id: f_00001-11-2 loss: 0.316582  [   96/  265]
train() client id: f_00001-11-3 loss: 0.322529  [  128/  265]
train() client id: f_00001-11-4 loss: 0.390039  [  160/  265]
train() client id: f_00001-11-5 loss: 0.625890  [  192/  265]
train() client id: f_00001-11-6 loss: 0.350934  [  224/  265]
train() client id: f_00001-11-7 loss: 0.447768  [  256/  265]
train() client id: f_00002-0-0 loss: 1.036116  [   32/  124]
train() client id: f_00002-0-1 loss: 1.081380  [   64/  124]
train() client id: f_00002-0-2 loss: 1.025762  [   96/  124]
train() client id: f_00002-1-0 loss: 0.917404  [   32/  124]
train() client id: f_00002-1-1 loss: 1.068463  [   64/  124]
train() client id: f_00002-1-2 loss: 0.891236  [   96/  124]
train() client id: f_00002-2-0 loss: 1.105739  [   32/  124]
train() client id: f_00002-2-1 loss: 0.936164  [   64/  124]
train() client id: f_00002-2-2 loss: 1.127440  [   96/  124]
train() client id: f_00002-3-0 loss: 0.838514  [   32/  124]
train() client id: f_00002-3-1 loss: 1.077176  [   64/  124]
train() client id: f_00002-3-2 loss: 0.782963  [   96/  124]
train() client id: f_00002-4-0 loss: 1.058812  [   32/  124]
train() client id: f_00002-4-1 loss: 0.966093  [   64/  124]
train() client id: f_00002-4-2 loss: 0.826896  [   96/  124]
train() client id: f_00002-5-0 loss: 1.051534  [   32/  124]
train() client id: f_00002-5-1 loss: 0.804222  [   64/  124]
train() client id: f_00002-5-2 loss: 0.903063  [   96/  124]
train() client id: f_00002-6-0 loss: 0.760625  [   32/  124]
train() client id: f_00002-6-1 loss: 1.065030  [   64/  124]
train() client id: f_00002-6-2 loss: 0.872751  [   96/  124]
train() client id: f_00002-7-0 loss: 0.832341  [   32/  124]
train() client id: f_00002-7-1 loss: 0.945621  [   64/  124]
train() client id: f_00002-7-2 loss: 1.031906  [   96/  124]
train() client id: f_00002-8-0 loss: 0.987675  [   32/  124]
train() client id: f_00002-8-1 loss: 0.825311  [   64/  124]
train() client id: f_00002-8-2 loss: 0.765287  [   96/  124]
train() client id: f_00002-9-0 loss: 0.901842  [   32/  124]
train() client id: f_00002-9-1 loss: 1.086194  [   64/  124]
train() client id: f_00002-9-2 loss: 0.742309  [   96/  124]
train() client id: f_00002-10-0 loss: 0.953737  [   32/  124]
train() client id: f_00002-10-1 loss: 0.623799  [   64/  124]
train() client id: f_00002-10-2 loss: 0.777915  [   96/  124]
train() client id: f_00002-11-0 loss: 0.784404  [   32/  124]
train() client id: f_00002-11-1 loss: 0.843386  [   64/  124]
train() client id: f_00002-11-2 loss: 0.944710  [   96/  124]
train() client id: f_00003-0-0 loss: 0.403419  [   32/   43]
train() client id: f_00003-1-0 loss: 0.369056  [   32/   43]
train() client id: f_00003-2-0 loss: 0.602316  [   32/   43]
train() client id: f_00003-3-0 loss: 0.414445  [   32/   43]
train() client id: f_00003-4-0 loss: 0.394443  [   32/   43]
train() client id: f_00003-5-0 loss: 0.581316  [   32/   43]
train() client id: f_00003-6-0 loss: 0.410626  [   32/   43]
train() client id: f_00003-7-0 loss: 0.508773  [   32/   43]
train() client id: f_00003-8-0 loss: 0.550956  [   32/   43]
train() client id: f_00003-9-0 loss: 0.501233  [   32/   43]
train() client id: f_00003-10-0 loss: 0.436029  [   32/   43]
train() client id: f_00003-11-0 loss: 0.448908  [   32/   43]
train() client id: f_00004-0-0 loss: 0.796659  [   32/  306]
train() client id: f_00004-0-1 loss: 0.924891  [   64/  306]
train() client id: f_00004-0-2 loss: 0.989324  [   96/  306]
train() client id: f_00004-0-3 loss: 0.702628  [  128/  306]
train() client id: f_00004-0-4 loss: 0.747336  [  160/  306]
train() client id: f_00004-0-5 loss: 0.797965  [  192/  306]
train() client id: f_00004-0-6 loss: 0.907941  [  224/  306]
train() client id: f_00004-0-7 loss: 0.788339  [  256/  306]
train() client id: f_00004-0-8 loss: 0.952570  [  288/  306]
train() client id: f_00004-1-0 loss: 0.921408  [   32/  306]
train() client id: f_00004-1-1 loss: 0.769458  [   64/  306]
train() client id: f_00004-1-2 loss: 0.805427  [   96/  306]
train() client id: f_00004-1-3 loss: 0.701425  [  128/  306]
train() client id: f_00004-1-4 loss: 0.938401  [  160/  306]
train() client id: f_00004-1-5 loss: 0.693914  [  192/  306]
train() client id: f_00004-1-6 loss: 0.936924  [  224/  306]
train() client id: f_00004-1-7 loss: 0.922501  [  256/  306]
train() client id: f_00004-1-8 loss: 0.948728  [  288/  306]
train() client id: f_00004-2-0 loss: 0.875667  [   32/  306]
train() client id: f_00004-2-1 loss: 0.939055  [   64/  306]
train() client id: f_00004-2-2 loss: 0.895910  [   96/  306]
train() client id: f_00004-2-3 loss: 0.779939  [  128/  306]
train() client id: f_00004-2-4 loss: 0.705980  [  160/  306]
train() client id: f_00004-2-5 loss: 0.803448  [  192/  306]
train() client id: f_00004-2-6 loss: 0.870983  [  224/  306]
train() client id: f_00004-2-7 loss: 0.903794  [  256/  306]
train() client id: f_00004-2-8 loss: 0.821079  [  288/  306]
train() client id: f_00004-3-0 loss: 0.979564  [   32/  306]
train() client id: f_00004-3-1 loss: 0.775222  [   64/  306]
train() client id: f_00004-3-2 loss: 0.826585  [   96/  306]
train() client id: f_00004-3-3 loss: 0.925372  [  128/  306]
train() client id: f_00004-3-4 loss: 0.938785  [  160/  306]
train() client id: f_00004-3-5 loss: 0.761461  [  192/  306]
train() client id: f_00004-3-6 loss: 0.751984  [  224/  306]
train() client id: f_00004-3-7 loss: 0.721047  [  256/  306]
train() client id: f_00004-3-8 loss: 0.780599  [  288/  306]
train() client id: f_00004-4-0 loss: 0.782335  [   32/  306]
train() client id: f_00004-4-1 loss: 0.893876  [   64/  306]
train() client id: f_00004-4-2 loss: 0.767553  [   96/  306]
train() client id: f_00004-4-3 loss: 0.924624  [  128/  306]
train() client id: f_00004-4-4 loss: 0.912633  [  160/  306]
train() client id: f_00004-4-5 loss: 0.893015  [  192/  306]
train() client id: f_00004-4-6 loss: 0.773908  [  224/  306]
train() client id: f_00004-4-7 loss: 0.737580  [  256/  306]
train() client id: f_00004-4-8 loss: 0.843582  [  288/  306]
train() client id: f_00004-5-0 loss: 0.825906  [   32/  306]
train() client id: f_00004-5-1 loss: 0.757997  [   64/  306]
train() client id: f_00004-5-2 loss: 0.831156  [   96/  306]
train() client id: f_00004-5-3 loss: 0.852387  [  128/  306]
train() client id: f_00004-5-4 loss: 0.782013  [  160/  306]
train() client id: f_00004-5-5 loss: 0.652651  [  192/  306]
train() client id: f_00004-5-6 loss: 0.886372  [  224/  306]
train() client id: f_00004-5-7 loss: 0.925357  [  256/  306]
train() client id: f_00004-5-8 loss: 1.027157  [  288/  306]
train() client id: f_00004-6-0 loss: 0.853202  [   32/  306]
train() client id: f_00004-6-1 loss: 0.784609  [   64/  306]
train() client id: f_00004-6-2 loss: 0.854041  [   96/  306]
train() client id: f_00004-6-3 loss: 0.786397  [  128/  306]
train() client id: f_00004-6-4 loss: 0.805426  [  160/  306]
train() client id: f_00004-6-5 loss: 0.840623  [  192/  306]
train() client id: f_00004-6-6 loss: 0.901606  [  224/  306]
train() client id: f_00004-6-7 loss: 0.822006  [  256/  306]
train() client id: f_00004-6-8 loss: 0.949222  [  288/  306]
train() client id: f_00004-7-0 loss: 0.910642  [   32/  306]
train() client id: f_00004-7-1 loss: 0.921102  [   64/  306]
train() client id: f_00004-7-2 loss: 0.806247  [   96/  306]
train() client id: f_00004-7-3 loss: 0.825409  [  128/  306]
train() client id: f_00004-7-4 loss: 0.975936  [  160/  306]
train() client id: f_00004-7-5 loss: 0.782364  [  192/  306]
train() client id: f_00004-7-6 loss: 0.768376  [  224/  306]
train() client id: f_00004-7-7 loss: 0.700639  [  256/  306]
train() client id: f_00004-7-8 loss: 0.849192  [  288/  306]
train() client id: f_00004-8-0 loss: 0.818531  [   32/  306]
train() client id: f_00004-8-1 loss: 0.990349  [   64/  306]
train() client id: f_00004-8-2 loss: 0.844332  [   96/  306]
train() client id: f_00004-8-3 loss: 0.890117  [  128/  306]
train() client id: f_00004-8-4 loss: 0.699315  [  160/  306]
train() client id: f_00004-8-5 loss: 0.757348  [  192/  306]
train() client id: f_00004-8-6 loss: 0.820234  [  224/  306]
train() client id: f_00004-8-7 loss: 0.839128  [  256/  306]
train() client id: f_00004-8-8 loss: 0.937596  [  288/  306]
train() client id: f_00004-9-0 loss: 0.850741  [   32/  306]
train() client id: f_00004-9-1 loss: 0.915129  [   64/  306]
train() client id: f_00004-9-2 loss: 0.861157  [   96/  306]
train() client id: f_00004-9-3 loss: 0.888575  [  128/  306]
train() client id: f_00004-9-4 loss: 0.881932  [  160/  306]
train() client id: f_00004-9-5 loss: 0.777911  [  192/  306]
train() client id: f_00004-9-6 loss: 0.780686  [  224/  306]
train() client id: f_00004-9-7 loss: 0.848661  [  256/  306]
train() client id: f_00004-9-8 loss: 0.767635  [  288/  306]
train() client id: f_00004-10-0 loss: 0.841767  [   32/  306]
train() client id: f_00004-10-1 loss: 0.767159  [   64/  306]
train() client id: f_00004-10-2 loss: 0.777998  [   96/  306]
train() client id: f_00004-10-3 loss: 0.852480  [  128/  306]
train() client id: f_00004-10-4 loss: 0.821764  [  160/  306]
train() client id: f_00004-10-5 loss: 0.835095  [  192/  306]
train() client id: f_00004-10-6 loss: 0.768851  [  224/  306]
train() client id: f_00004-10-7 loss: 0.833513  [  256/  306]
train() client id: f_00004-10-8 loss: 0.905289  [  288/  306]
train() client id: f_00004-11-0 loss: 0.841958  [   32/  306]
train() client id: f_00004-11-1 loss: 0.795820  [   64/  306]
train() client id: f_00004-11-2 loss: 0.712350  [   96/  306]
train() client id: f_00004-11-3 loss: 0.825325  [  128/  306]
train() client id: f_00004-11-4 loss: 0.887353  [  160/  306]
train() client id: f_00004-11-5 loss: 0.907349  [  192/  306]
train() client id: f_00004-11-6 loss: 0.760084  [  224/  306]
train() client id: f_00004-11-7 loss: 0.900817  [  256/  306]
train() client id: f_00004-11-8 loss: 0.868622  [  288/  306]
train() client id: f_00005-0-0 loss: 0.361382  [   32/  146]
train() client id: f_00005-0-1 loss: 0.471713  [   64/  146]
train() client id: f_00005-0-2 loss: 0.993525  [   96/  146]
train() client id: f_00005-0-3 loss: 0.623507  [  128/  146]
train() client id: f_00005-1-0 loss: 0.466482  [   32/  146]
train() client id: f_00005-1-1 loss: 0.462614  [   64/  146]
train() client id: f_00005-1-2 loss: 0.953256  [   96/  146]
train() client id: f_00005-1-3 loss: 0.417666  [  128/  146]
train() client id: f_00005-2-0 loss: 0.557398  [   32/  146]
train() client id: f_00005-2-1 loss: 0.763115  [   64/  146]
train() client id: f_00005-2-2 loss: 0.590120  [   96/  146]
train() client id: f_00005-2-3 loss: 0.399966  [  128/  146]
train() client id: f_00005-3-0 loss: 0.645777  [   32/  146]
train() client id: f_00005-3-1 loss: 0.650355  [   64/  146]
train() client id: f_00005-3-2 loss: 0.522733  [   96/  146]
train() client id: f_00005-3-3 loss: 0.716885  [  128/  146]
train() client id: f_00005-4-0 loss: 0.739776  [   32/  146]
train() client id: f_00005-4-1 loss: 0.620862  [   64/  146]
train() client id: f_00005-4-2 loss: 0.670016  [   96/  146]
train() client id: f_00005-4-3 loss: 0.358393  [  128/  146]
train() client id: f_00005-5-0 loss: 0.760535  [   32/  146]
train() client id: f_00005-5-1 loss: 0.640394  [   64/  146]
train() client id: f_00005-5-2 loss: 0.514512  [   96/  146]
train() client id: f_00005-5-3 loss: 0.425417  [  128/  146]
train() client id: f_00005-6-0 loss: 0.600918  [   32/  146]
train() client id: f_00005-6-1 loss: 0.565644  [   64/  146]
train() client id: f_00005-6-2 loss: 0.772661  [   96/  146]
train() client id: f_00005-6-3 loss: 0.403737  [  128/  146]
train() client id: f_00005-7-0 loss: 0.562546  [   32/  146]
train() client id: f_00005-7-1 loss: 0.764240  [   64/  146]
train() client id: f_00005-7-2 loss: 0.702295  [   96/  146]
train() client id: f_00005-7-3 loss: 0.438766  [  128/  146]
train() client id: f_00005-8-0 loss: 0.802456  [   32/  146]
train() client id: f_00005-8-1 loss: 0.417884  [   64/  146]
train() client id: f_00005-8-2 loss: 0.707472  [   96/  146]
train() client id: f_00005-8-3 loss: 0.612569  [  128/  146]
train() client id: f_00005-9-0 loss: 0.364235  [   32/  146]
train() client id: f_00005-9-1 loss: 0.700043  [   64/  146]
train() client id: f_00005-9-2 loss: 0.566278  [   96/  146]
train() client id: f_00005-9-3 loss: 0.589427  [  128/  146]
train() client id: f_00005-10-0 loss: 0.532420  [   32/  146]
train() client id: f_00005-10-1 loss: 0.524364  [   64/  146]
train() client id: f_00005-10-2 loss: 0.639298  [   96/  146]
train() client id: f_00005-10-3 loss: 0.485168  [  128/  146]
train() client id: f_00005-11-0 loss: 0.713056  [   32/  146]
train() client id: f_00005-11-1 loss: 0.777425  [   64/  146]
train() client id: f_00005-11-2 loss: 0.528776  [   96/  146]
train() client id: f_00005-11-3 loss: 0.520481  [  128/  146]
train() client id: f_00006-0-0 loss: 0.451923  [   32/   54]
train() client id: f_00006-1-0 loss: 0.520261  [   32/   54]
train() client id: f_00006-2-0 loss: 0.485424  [   32/   54]
train() client id: f_00006-3-0 loss: 0.478732  [   32/   54]
train() client id: f_00006-4-0 loss: 0.463926  [   32/   54]
train() client id: f_00006-5-0 loss: 0.519781  [   32/   54]
train() client id: f_00006-6-0 loss: 0.477442  [   32/   54]
train() client id: f_00006-7-0 loss: 0.474800  [   32/   54]
train() client id: f_00006-8-0 loss: 0.491078  [   32/   54]
train() client id: f_00006-9-0 loss: 0.527485  [   32/   54]
train() client id: f_00006-10-0 loss: 0.419504  [   32/   54]
train() client id: f_00006-11-0 loss: 0.489075  [   32/   54]
train() client id: f_00007-0-0 loss: 0.792764  [   32/  179]
train() client id: f_00007-0-1 loss: 0.838653  [   64/  179]
train() client id: f_00007-0-2 loss: 0.730837  [   96/  179]
train() client id: f_00007-0-3 loss: 0.673148  [  128/  179]
train() client id: f_00007-0-4 loss: 0.614278  [  160/  179]
train() client id: f_00007-1-0 loss: 0.592534  [   32/  179]
train() client id: f_00007-1-1 loss: 0.665819  [   64/  179]
train() client id: f_00007-1-2 loss: 0.736818  [   96/  179]
train() client id: f_00007-1-3 loss: 0.671869  [  128/  179]
train() client id: f_00007-1-4 loss: 0.812302  [  160/  179]
train() client id: f_00007-2-0 loss: 0.821619  [   32/  179]
train() client id: f_00007-2-1 loss: 0.796381  [   64/  179]
train() client id: f_00007-2-2 loss: 0.548001  [   96/  179]
train() client id: f_00007-2-3 loss: 0.763557  [  128/  179]
train() client id: f_00007-2-4 loss: 0.708904  [  160/  179]
train() client id: f_00007-3-0 loss: 0.677060  [   32/  179]
train() client id: f_00007-3-1 loss: 0.892228  [   64/  179]
train() client id: f_00007-3-2 loss: 0.502351  [   96/  179]
train() client id: f_00007-3-3 loss: 0.690518  [  128/  179]
train() client id: f_00007-3-4 loss: 0.847532  [  160/  179]
train() client id: f_00007-4-0 loss: 0.812215  [   32/  179]
train() client id: f_00007-4-1 loss: 0.764503  [   64/  179]
train() client id: f_00007-4-2 loss: 0.636701  [   96/  179]
train() client id: f_00007-4-3 loss: 0.599571  [  128/  179]
train() client id: f_00007-4-4 loss: 0.663267  [  160/  179]
train() client id: f_00007-5-0 loss: 0.628291  [   32/  179]
train() client id: f_00007-5-1 loss: 0.788044  [   64/  179]
train() client id: f_00007-5-2 loss: 0.646830  [   96/  179]
train() client id: f_00007-5-3 loss: 0.728102  [  128/  179]
train() client id: f_00007-5-4 loss: 0.582125  [  160/  179]
train() client id: f_00007-6-0 loss: 0.920593  [   32/  179]
train() client id: f_00007-6-1 loss: 0.659633  [   64/  179]
train() client id: f_00007-6-2 loss: 0.759907  [   96/  179]
train() client id: f_00007-6-3 loss: 0.551070  [  128/  179]
train() client id: f_00007-6-4 loss: 0.639707  [  160/  179]
train() client id: f_00007-7-0 loss: 0.612867  [   32/  179]
train() client id: f_00007-7-1 loss: 0.725070  [   64/  179]
train() client id: f_00007-7-2 loss: 0.621363  [   96/  179]
train() client id: f_00007-7-3 loss: 0.805166  [  128/  179]
train() client id: f_00007-7-4 loss: 0.640614  [  160/  179]
train() client id: f_00007-8-0 loss: 0.846821  [   32/  179]
train() client id: f_00007-8-1 loss: 0.528353  [   64/  179]
train() client id: f_00007-8-2 loss: 0.642402  [   96/  179]
train() client id: f_00007-8-3 loss: 0.658332  [  128/  179]
train() client id: f_00007-8-4 loss: 0.691695  [  160/  179]
train() client id: f_00007-9-0 loss: 0.811180  [   32/  179]
train() client id: f_00007-9-1 loss: 0.589746  [   64/  179]
train() client id: f_00007-9-2 loss: 0.764463  [   96/  179]
train() client id: f_00007-9-3 loss: 0.618613  [  128/  179]
train() client id: f_00007-9-4 loss: 0.747406  [  160/  179]
train() client id: f_00007-10-0 loss: 0.835084  [   32/  179]
train() client id: f_00007-10-1 loss: 0.558389  [   64/  179]
train() client id: f_00007-10-2 loss: 0.782450  [   96/  179]
train() client id: f_00007-10-3 loss: 0.506057  [  128/  179]
train() client id: f_00007-10-4 loss: 0.835391  [  160/  179]
train() client id: f_00007-11-0 loss: 0.797934  [   32/  179]
train() client id: f_00007-11-1 loss: 0.843654  [   64/  179]
train() client id: f_00007-11-2 loss: 0.512826  [   96/  179]
train() client id: f_00007-11-3 loss: 0.641137  [  128/  179]
train() client id: f_00007-11-4 loss: 0.747766  [  160/  179]
train() client id: f_00008-0-0 loss: 0.792057  [   32/  130]
train() client id: f_00008-0-1 loss: 0.717179  [   64/  130]
train() client id: f_00008-0-2 loss: 0.739423  [   96/  130]
train() client id: f_00008-0-3 loss: 0.762385  [  128/  130]
train() client id: f_00008-1-0 loss: 0.746426  [   32/  130]
train() client id: f_00008-1-1 loss: 0.786788  [   64/  130]
train() client id: f_00008-1-2 loss: 0.756861  [   96/  130]
train() client id: f_00008-1-3 loss: 0.762361  [  128/  130]
train() client id: f_00008-2-0 loss: 0.788368  [   32/  130]
train() client id: f_00008-2-1 loss: 0.797677  [   64/  130]
train() client id: f_00008-2-2 loss: 0.665178  [   96/  130]
train() client id: f_00008-2-3 loss: 0.760267  [  128/  130]
train() client id: f_00008-3-0 loss: 0.702740  [   32/  130]
train() client id: f_00008-3-1 loss: 0.823522  [   64/  130]
train() client id: f_00008-3-2 loss: 0.746860  [   96/  130]
train() client id: f_00008-3-3 loss: 0.780908  [  128/  130]
train() client id: f_00008-4-0 loss: 0.680726  [   32/  130]
train() client id: f_00008-4-1 loss: 0.754430  [   64/  130]
train() client id: f_00008-4-2 loss: 0.834335  [   96/  130]
train() client id: f_00008-4-3 loss: 0.711825  [  128/  130]
train() client id: f_00008-5-0 loss: 0.708398  [   32/  130]
train() client id: f_00008-5-1 loss: 0.874951  [   64/  130]
train() client id: f_00008-5-2 loss: 0.758276  [   96/  130]
train() client id: f_00008-5-3 loss: 0.705461  [  128/  130]
train() client id: f_00008-6-0 loss: 0.653599  [   32/  130]
train() client id: f_00008-6-1 loss: 0.845638  [   64/  130]
train() client id: f_00008-6-2 loss: 0.710491  [   96/  130]
train() client id: f_00008-6-3 loss: 0.801743  [  128/  130]
train() client id: f_00008-7-0 loss: 0.641615  [   32/  130]
train() client id: f_00008-7-1 loss: 0.846113  [   64/  130]
train() client id: f_00008-7-2 loss: 0.835772  [   96/  130]
train() client id: f_00008-7-3 loss: 0.687336  [  128/  130]
train() client id: f_00008-8-0 loss: 0.768784  [   32/  130]
train() client id: f_00008-8-1 loss: 0.775217  [   64/  130]
train() client id: f_00008-8-2 loss: 0.658308  [   96/  130]
train() client id: f_00008-8-3 loss: 0.819792  [  128/  130]
train() client id: f_00008-9-0 loss: 0.687165  [   32/  130]
train() client id: f_00008-9-1 loss: 0.882585  [   64/  130]
train() client id: f_00008-9-2 loss: 0.849947  [   96/  130]
train() client id: f_00008-9-3 loss: 0.634644  [  128/  130]
train() client id: f_00008-10-0 loss: 0.714148  [   32/  130]
train() client id: f_00008-10-1 loss: 0.743558  [   64/  130]
train() client id: f_00008-10-2 loss: 0.823367  [   96/  130]
train() client id: f_00008-10-3 loss: 0.756541  [  128/  130]
train() client id: f_00008-11-0 loss: 0.774449  [   32/  130]
train() client id: f_00008-11-1 loss: 0.905887  [   64/  130]
train() client id: f_00008-11-2 loss: 0.684207  [   96/  130]
train() client id: f_00008-11-3 loss: 0.672278  [  128/  130]
train() client id: f_00009-0-0 loss: 1.003796  [   32/  118]
train() client id: f_00009-0-1 loss: 0.961147  [   64/  118]
train() client id: f_00009-0-2 loss: 1.008181  [   96/  118]
train() client id: f_00009-1-0 loss: 1.209359  [   32/  118]
train() client id: f_00009-1-1 loss: 0.853118  [   64/  118]
train() client id: f_00009-1-2 loss: 0.883061  [   96/  118]
train() client id: f_00009-2-0 loss: 1.058706  [   32/  118]
train() client id: f_00009-2-1 loss: 0.743686  [   64/  118]
train() client id: f_00009-2-2 loss: 0.884048  [   96/  118]
train() client id: f_00009-3-0 loss: 0.981049  [   32/  118]
train() client id: f_00009-3-1 loss: 0.860487  [   64/  118]
train() client id: f_00009-3-2 loss: 0.959850  [   96/  118]
train() client id: f_00009-4-0 loss: 0.906359  [   32/  118]
train() client id: f_00009-4-1 loss: 0.893641  [   64/  118]
train() client id: f_00009-4-2 loss: 0.948534  [   96/  118]
train() client id: f_00009-5-0 loss: 0.815301  [   32/  118]
train() client id: f_00009-5-1 loss: 0.826029  [   64/  118]
train() client id: f_00009-5-2 loss: 0.950482  [   96/  118]
train() client id: f_00009-6-0 loss: 0.814074  [   32/  118]
train() client id: f_00009-6-1 loss: 0.991056  [   64/  118]
train() client id: f_00009-6-2 loss: 0.706736  [   96/  118]
train() client id: f_00009-7-0 loss: 0.622042  [   32/  118]
train() client id: f_00009-7-1 loss: 1.061016  [   64/  118]
train() client id: f_00009-7-2 loss: 0.887526  [   96/  118]
train() client id: f_00009-8-0 loss: 0.959463  [   32/  118]
train() client id: f_00009-8-1 loss: 0.896311  [   64/  118]
train() client id: f_00009-8-2 loss: 0.712009  [   96/  118]
train() client id: f_00009-9-0 loss: 0.874890  [   32/  118]
train() client id: f_00009-9-1 loss: 0.686150  [   64/  118]
train() client id: f_00009-9-2 loss: 0.868533  [   96/  118]
train() client id: f_00009-10-0 loss: 0.791173  [   32/  118]
train() client id: f_00009-10-1 loss: 0.879467  [   64/  118]
train() client id: f_00009-10-2 loss: 0.773767  [   96/  118]
train() client id: f_00009-11-0 loss: 0.888097  [   32/  118]
train() client id: f_00009-11-1 loss: 0.713850  [   64/  118]
train() client id: f_00009-11-2 loss: 0.899797  [   96/  118]
At round 35 accuracy: 0.6419098143236074
At round 35 training accuracy: 0.5915492957746479
At round 35 training loss: 0.8252460790309608
update_location
xs = -3.905658 4.200318 195.009024 18.811294 0.979296 3.956410 -157.443192 -136.324852 179.663977 -122.060879 
ys = 187.587959 170.555839 1.320614 -157.455176 149.350187 132.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -1.7114261681174354
ys mean: 50.39414253552871
dists_uav = 212.613491 197.754739 219.158079 187.472657 179.739916 166.298683 186.534847 169.071409 206.368153 157.844449 
uav_gains = -108.492625 -107.540848 -108.932389 -106.897913 -106.411197 -105.539480 -106.839197 -105.722999 -108.086562 -104.964595 
uav_gains_db_mean: -106.94278063161482
dists_bs = 171.556785 179.255417 408.743449 384.823676 177.838763 183.861476 178.490550 178.419790 388.034862 179.005709 
bs_gains = -102.130480 -102.664283 -112.687682 -111.954389 -102.567799 -102.972800 -102.612286 -102.607464 -112.055440 -102.647332 
bs_gains_db_mean: -105.48999546776679
Round 36
-------------------------------
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.2394703  12.90983275  6.14725815  2.21746141 14.88914632  7.16517573
  2.74816098  8.77577622  6.47605413  5.8113634 ]
obj_prev = 73.37969937876653
eta_min = 1.5469793420595954e-15	eta_max = 0.9294112566599255
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 17.025573465777978	eta = 0.9090909090909091
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 31.275397678211032	eta = 0.4948872023642162
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 24.26289815788516	eta = 0.6379202500492736
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.99478274194065	eta = 0.6731002520657826
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.927768157082628	eta = 0.6750676277672019
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.927565477060664	eta = 0.6750735953751348
eta = 0.6750735953751348
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [0.03242394 0.0681932  0.03190927 0.01106531 0.07874384 0.03757059
 0.01389597 0.04606258 0.03345327 0.03036527]
ene_total = [2.05444491 3.67229692 2.04263567 0.969603   4.18544742 2.18345709
 1.10625446 2.64978349 2.23698535 1.82665715]
ti_comp = [0.4811663  0.50721776 0.47844041 0.48996025 0.50753423 0.50618685
 0.49025487 0.4955151  0.45404092 0.50727357]
ti_coms = [0.09689836 0.07084691 0.09962426 0.08810442 0.07053044 0.07187782
 0.08780979 0.08254957 0.12402375 0.0707911 ]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [9.20209687e-06 7.70396484e-05 8.87104805e-06 3.52735562e-07
 1.18467487e-04 1.29360722e-05 6.97755096e-07 2.48777379e-05
 1.13502559e-05 6.80028536e-06]
ene_total = [0.47315021 0.34937218 0.48643187 0.42981882 0.34984932 0.35127413
 0.42839839 0.40391691 0.60558136 0.34567345]
optimize_network iter = 0 obj = 4.223466627519707
eta = 0.6750735953751348
freqs = [33693065.79010019 67222802.04491435 33347173.22625211 11292050.13707799
 77574909.50601687 37111387.37372465 14172192.11427163 46479487.54198894
 36839485.23777357 29929879.4291095 ]
eta_min = 0.6750735953751431	eta_max = 0.675073595375134
af = 0.01091581578850546	bf = 1.3680266359327833	zeta = 0.012007397367356006	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [2.23206564e-06 1.86867792e-05 2.15176626e-06 8.55597305e-08
 2.87355384e-05 3.13778074e-06 1.69247857e-07 6.03435770e-06
 2.75312427e-06 1.64948093e-06]
ene_total = [1.68991368 1.23854826 1.73742856 1.53620773 1.23478239 1.25381221
 1.53108527 1.44039041 2.16296411 1.23460465]
ti_comp = [0.4811663  0.50721776 0.47844041 0.48996025 0.50753423 0.50618685
 0.49025487 0.4955151  0.45404092 0.50727357]
ti_coms = [0.09689836 0.07084691 0.09962426 0.08810442 0.07053044 0.07187782
 0.08780979 0.08254957 0.12402375 0.0707911 ]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [9.20209687e-06 7.70396484e-05 8.87104805e-06 3.52735562e-07
 1.18467487e-04 1.29360722e-05 6.97755096e-07 2.48777379e-05
 1.13502559e-05 6.80028536e-06]
ene_total = [0.47315021 0.34937218 0.48643187 0.42981882 0.34984932 0.35127413
 0.42839839 0.40391691 0.60558136 0.34567345]
optimize_network iter = 1 obj = 4.223466627519696
eta = 0.675073595375134
freqs = [33693065.7901002  67222802.04491436 33347173.22625211 11292050.13707799
 77574909.50601688 37111387.37372466 14172192.11427163 46479487.54198895
 36839485.23777357 29929879.42910951]
Done!
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [8.58228348e-06 7.18505913e-05 8.27353268e-06 3.28976822e-07
 1.10488030e-04 1.20647544e-05 6.50757335e-07 2.32020813e-05
 1.05857518e-05 6.34224759e-06]
ene_total = [0.00969842 0.00715654 0.0099707  0.00881077 0.00716353 0.00719985
 0.00878163 0.00827816 0.01241296 0.00708545]
At round 36 energy consumption: 0.08655801199959033
At round 36 eta: 0.675073595375134
At round 36 a_n: 15.850952365358953
At round 36 local rounds: 12.866641222936334
At round 36 global rounds: 48.78320795030245
gradient difference: 0.3587716221809387
train() client id: f_00000-0-0 loss: 1.195981  [   32/  126]
train() client id: f_00000-0-1 loss: 1.327563  [   64/  126]
train() client id: f_00000-0-2 loss: 1.532029  [   96/  126]
train() client id: f_00000-1-0 loss: 1.067016  [   32/  126]
train() client id: f_00000-1-1 loss: 1.249008  [   64/  126]
train() client id: f_00000-1-2 loss: 1.265216  [   96/  126]
train() client id: f_00000-2-0 loss: 1.040499  [   32/  126]
train() client id: f_00000-2-1 loss: 1.312039  [   64/  126]
train() client id: f_00000-2-2 loss: 0.936875  [   96/  126]
train() client id: f_00000-3-0 loss: 1.131519  [   32/  126]
train() client id: f_00000-3-1 loss: 1.043180  [   64/  126]
train() client id: f_00000-3-2 loss: 1.078574  [   96/  126]
train() client id: f_00000-4-0 loss: 0.966260  [   32/  126]
train() client id: f_00000-4-1 loss: 1.081184  [   64/  126]
train() client id: f_00000-4-2 loss: 1.083678  [   96/  126]
train() client id: f_00000-5-0 loss: 0.957148  [   32/  126]
train() client id: f_00000-5-1 loss: 0.958684  [   64/  126]
train() client id: f_00000-5-2 loss: 0.966790  [   96/  126]
train() client id: f_00000-6-0 loss: 1.044838  [   32/  126]
train() client id: f_00000-6-1 loss: 1.072693  [   64/  126]
train() client id: f_00000-6-2 loss: 0.787655  [   96/  126]
train() client id: f_00000-7-0 loss: 0.921243  [   32/  126]
train() client id: f_00000-7-1 loss: 0.910252  [   64/  126]
train() client id: f_00000-7-2 loss: 0.929278  [   96/  126]
train() client id: f_00000-8-0 loss: 1.013333  [   32/  126]
train() client id: f_00000-8-1 loss: 0.820853  [   64/  126]
train() client id: f_00000-8-2 loss: 0.811394  [   96/  126]
train() client id: f_00000-9-0 loss: 0.864909  [   32/  126]
train() client id: f_00000-9-1 loss: 0.824805  [   64/  126]
train() client id: f_00000-9-2 loss: 0.921199  [   96/  126]
train() client id: f_00000-10-0 loss: 0.928480  [   32/  126]
train() client id: f_00000-10-1 loss: 0.815957  [   64/  126]
train() client id: f_00000-10-2 loss: 0.776481  [   96/  126]
train() client id: f_00000-11-0 loss: 0.847455  [   32/  126]
train() client id: f_00000-11-1 loss: 0.918866  [   64/  126]
train() client id: f_00000-11-2 loss: 0.883623  [   96/  126]
train() client id: f_00001-0-0 loss: 0.337290  [   32/  265]
train() client id: f_00001-0-1 loss: 0.382418  [   64/  265]
train() client id: f_00001-0-2 loss: 0.507003  [   96/  265]
train() client id: f_00001-0-3 loss: 0.426263  [  128/  265]
train() client id: f_00001-0-4 loss: 0.410783  [  160/  265]
train() client id: f_00001-0-5 loss: 0.392039  [  192/  265]
train() client id: f_00001-0-6 loss: 0.354797  [  224/  265]
train() client id: f_00001-0-7 loss: 0.448894  [  256/  265]
train() client id: f_00001-1-0 loss: 0.353712  [   32/  265]
train() client id: f_00001-1-1 loss: 0.353340  [   64/  265]
train() client id: f_00001-1-2 loss: 0.481150  [   96/  265]
train() client id: f_00001-1-3 loss: 0.383677  [  128/  265]
train() client id: f_00001-1-4 loss: 0.441985  [  160/  265]
train() client id: f_00001-1-5 loss: 0.382603  [  192/  265]
train() client id: f_00001-1-6 loss: 0.456713  [  224/  265]
train() client id: f_00001-1-7 loss: 0.327356  [  256/  265]
train() client id: f_00001-2-0 loss: 0.427730  [   32/  265]
train() client id: f_00001-2-1 loss: 0.348448  [   64/  265]
train() client id: f_00001-2-2 loss: 0.470749  [   96/  265]
train() client id: f_00001-2-3 loss: 0.345932  [  128/  265]
train() client id: f_00001-2-4 loss: 0.502336  [  160/  265]
train() client id: f_00001-2-5 loss: 0.374100  [  192/  265]
train() client id: f_00001-2-6 loss: 0.311416  [  224/  265]
train() client id: f_00001-2-7 loss: 0.323722  [  256/  265]
train() client id: f_00001-3-0 loss: 0.371711  [   32/  265]
train() client id: f_00001-3-1 loss: 0.282206  [   64/  265]
train() client id: f_00001-3-2 loss: 0.467952  [   96/  265]
train() client id: f_00001-3-3 loss: 0.317744  [  128/  265]
train() client id: f_00001-3-4 loss: 0.431427  [  160/  265]
train() client id: f_00001-3-5 loss: 0.386611  [  192/  265]
train() client id: f_00001-3-6 loss: 0.398087  [  224/  265]
train() client id: f_00001-3-7 loss: 0.364173  [  256/  265]
train() client id: f_00001-4-0 loss: 0.290760  [   32/  265]
train() client id: f_00001-4-1 loss: 0.301851  [   64/  265]
train() client id: f_00001-4-2 loss: 0.391658  [   96/  265]
train() client id: f_00001-4-3 loss: 0.310481  [  128/  265]
train() client id: f_00001-4-4 loss: 0.518718  [  160/  265]
train() client id: f_00001-4-5 loss: 0.357577  [  192/  265]
train() client id: f_00001-4-6 loss: 0.387647  [  224/  265]
train() client id: f_00001-4-7 loss: 0.393349  [  256/  265]
train() client id: f_00001-5-0 loss: 0.284326  [   32/  265]
train() client id: f_00001-5-1 loss: 0.375458  [   64/  265]
train() client id: f_00001-5-2 loss: 0.312544  [   96/  265]
train() client id: f_00001-5-3 loss: 0.331458  [  128/  265]
train() client id: f_00001-5-4 loss: 0.451397  [  160/  265]
train() client id: f_00001-5-5 loss: 0.285964  [  192/  265]
train() client id: f_00001-5-6 loss: 0.558372  [  224/  265]
train() client id: f_00001-5-7 loss: 0.299813  [  256/  265]
train() client id: f_00001-6-0 loss: 0.354899  [   32/  265]
train() client id: f_00001-6-1 loss: 0.367754  [   64/  265]
train() client id: f_00001-6-2 loss: 0.366679  [   96/  265]
train() client id: f_00001-6-3 loss: 0.382125  [  128/  265]
train() client id: f_00001-6-4 loss: 0.270119  [  160/  265]
train() client id: f_00001-6-5 loss: 0.458354  [  192/  265]
train() client id: f_00001-6-6 loss: 0.419810  [  224/  265]
train() client id: f_00001-6-7 loss: 0.324497  [  256/  265]
train() client id: f_00001-7-0 loss: 0.314995  [   32/  265]
train() client id: f_00001-7-1 loss: 0.492083  [   64/  265]
train() client id: f_00001-7-2 loss: 0.293310  [   96/  265]
train() client id: f_00001-7-3 loss: 0.307041  [  128/  265]
train() client id: f_00001-7-4 loss: 0.447908  [  160/  265]
train() client id: f_00001-7-5 loss: 0.425381  [  192/  265]
train() client id: f_00001-7-6 loss: 0.273133  [  224/  265]
train() client id: f_00001-7-7 loss: 0.329455  [  256/  265]
train() client id: f_00001-8-0 loss: 0.331870  [   32/  265]
train() client id: f_00001-8-1 loss: 0.353440  [   64/  265]
train() client id: f_00001-8-2 loss: 0.368565  [   96/  265]
train() client id: f_00001-8-3 loss: 0.272463  [  128/  265]
train() client id: f_00001-8-4 loss: 0.252324  [  160/  265]
train() client id: f_00001-8-5 loss: 0.396627  [  192/  265]
train() client id: f_00001-8-6 loss: 0.381697  [  224/  265]
train() client id: f_00001-8-7 loss: 0.555085  [  256/  265]
train() client id: f_00001-9-0 loss: 0.266151  [   32/  265]
train() client id: f_00001-9-1 loss: 0.337223  [   64/  265]
train() client id: f_00001-9-2 loss: 0.273087  [   96/  265]
train() client id: f_00001-9-3 loss: 0.425282  [  128/  265]
train() client id: f_00001-9-4 loss: 0.371616  [  160/  265]
train() client id: f_00001-9-5 loss: 0.421596  [  192/  265]
train() client id: f_00001-9-6 loss: 0.527470  [  224/  265]
train() client id: f_00001-9-7 loss: 0.287948  [  256/  265]
train() client id: f_00001-10-0 loss: 0.351076  [   32/  265]
train() client id: f_00001-10-1 loss: 0.399905  [   64/  265]
train() client id: f_00001-10-2 loss: 0.254406  [   96/  265]
train() client id: f_00001-10-3 loss: 0.352273  [  128/  265]
train() client id: f_00001-10-4 loss: 0.285361  [  160/  265]
train() client id: f_00001-10-5 loss: 0.421258  [  192/  265]
train() client id: f_00001-10-6 loss: 0.320506  [  224/  265]
train() client id: f_00001-10-7 loss: 0.464664  [  256/  265]
train() client id: f_00001-11-0 loss: 0.325120  [   32/  265]
train() client id: f_00001-11-1 loss: 0.360761  [   64/  265]
train() client id: f_00001-11-2 loss: 0.413144  [   96/  265]
train() client id: f_00001-11-3 loss: 0.246970  [  128/  265]
train() client id: f_00001-11-4 loss: 0.379962  [  160/  265]
train() client id: f_00001-11-5 loss: 0.447411  [  192/  265]
train() client id: f_00001-11-6 loss: 0.273366  [  224/  265]
train() client id: f_00001-11-7 loss: 0.349015  [  256/  265]
train() client id: f_00002-0-0 loss: 1.036321  [   32/  124]
train() client id: f_00002-0-1 loss: 1.172147  [   64/  124]
train() client id: f_00002-0-2 loss: 1.057395  [   96/  124]
train() client id: f_00002-1-0 loss: 1.099962  [   32/  124]
train() client id: f_00002-1-1 loss: 1.137249  [   64/  124]
train() client id: f_00002-1-2 loss: 0.921084  [   96/  124]
train() client id: f_00002-2-0 loss: 1.063424  [   32/  124]
train() client id: f_00002-2-1 loss: 0.944909  [   64/  124]
train() client id: f_00002-2-2 loss: 1.044711  [   96/  124]
train() client id: f_00002-3-0 loss: 0.960602  [   32/  124]
train() client id: f_00002-3-1 loss: 0.893324  [   64/  124]
train() client id: f_00002-3-2 loss: 1.033281  [   96/  124]
train() client id: f_00002-4-0 loss: 0.948317  [   32/  124]
train() client id: f_00002-4-1 loss: 0.938505  [   64/  124]
train() client id: f_00002-4-2 loss: 0.807742  [   96/  124]
train() client id: f_00002-5-0 loss: 1.019368  [   32/  124]
train() client id: f_00002-5-1 loss: 0.844243  [   64/  124]
train() client id: f_00002-5-2 loss: 0.911048  [   96/  124]
train() client id: f_00002-6-0 loss: 0.820044  [   32/  124]
train() client id: f_00002-6-1 loss: 0.825148  [   64/  124]
train() client id: f_00002-6-2 loss: 0.888591  [   96/  124]
train() client id: f_00002-7-0 loss: 0.984227  [   32/  124]
train() client id: f_00002-7-1 loss: 0.625119  [   64/  124]
train() client id: f_00002-7-2 loss: 1.021432  [   96/  124]
train() client id: f_00002-8-0 loss: 0.714181  [   32/  124]
train() client id: f_00002-8-1 loss: 0.879154  [   64/  124]
train() client id: f_00002-8-2 loss: 0.910909  [   96/  124]
train() client id: f_00002-9-0 loss: 1.079112  [   32/  124]
train() client id: f_00002-9-1 loss: 0.797102  [   64/  124]
train() client id: f_00002-9-2 loss: 0.797497  [   96/  124]
train() client id: f_00002-10-0 loss: 1.035246  [   32/  124]
train() client id: f_00002-10-1 loss: 0.828145  [   64/  124]
train() client id: f_00002-10-2 loss: 0.689626  [   96/  124]
train() client id: f_00002-11-0 loss: 1.100499  [   32/  124]
train() client id: f_00002-11-1 loss: 0.688456  [   64/  124]
train() client id: f_00002-11-2 loss: 0.732870  [   96/  124]
train() client id: f_00003-0-0 loss: 0.542972  [   32/   43]
train() client id: f_00003-1-0 loss: 0.777322  [   32/   43]
train() client id: f_00003-2-0 loss: 0.560088  [   32/   43]
train() client id: f_00003-3-0 loss: 0.628718  [   32/   43]
train() client id: f_00003-4-0 loss: 0.582227  [   32/   43]
train() client id: f_00003-5-0 loss: 0.726884  [   32/   43]
train() client id: f_00003-6-0 loss: 0.753483  [   32/   43]
train() client id: f_00003-7-0 loss: 0.822536  [   32/   43]
train() client id: f_00003-8-0 loss: 0.774632  [   32/   43]
train() client id: f_00003-9-0 loss: 0.857209  [   32/   43]
train() client id: f_00003-10-0 loss: 0.772037  [   32/   43]
train() client id: f_00003-11-0 loss: 0.754384  [   32/   43]
train() client id: f_00004-0-0 loss: 0.786404  [   32/  306]
train() client id: f_00004-0-1 loss: 0.769409  [   64/  306]
train() client id: f_00004-0-2 loss: 0.964747  [   96/  306]
train() client id: f_00004-0-3 loss: 0.969604  [  128/  306]
train() client id: f_00004-0-4 loss: 0.907142  [  160/  306]
train() client id: f_00004-0-5 loss: 0.829131  [  192/  306]
train() client id: f_00004-0-6 loss: 0.703617  [  224/  306]
train() client id: f_00004-0-7 loss: 1.033788  [  256/  306]
train() client id: f_00004-0-8 loss: 0.813258  [  288/  306]
train() client id: f_00004-1-0 loss: 0.788496  [   32/  306]
train() client id: f_00004-1-1 loss: 0.719366  [   64/  306]
train() client id: f_00004-1-2 loss: 0.821229  [   96/  306]
train() client id: f_00004-1-3 loss: 0.793026  [  128/  306]
train() client id: f_00004-1-4 loss: 0.968660  [  160/  306]
train() client id: f_00004-1-5 loss: 0.915253  [  192/  306]
train() client id: f_00004-1-6 loss: 0.848587  [  224/  306]
train() client id: f_00004-1-7 loss: 0.948015  [  256/  306]
train() client id: f_00004-1-8 loss: 0.885769  [  288/  306]
train() client id: f_00004-2-0 loss: 0.811789  [   32/  306]
train() client id: f_00004-2-1 loss: 0.779033  [   64/  306]
train() client id: f_00004-2-2 loss: 0.892702  [   96/  306]
train() client id: f_00004-2-3 loss: 0.745164  [  128/  306]
train() client id: f_00004-2-4 loss: 0.814862  [  160/  306]
train() client id: f_00004-2-5 loss: 0.849191  [  192/  306]
train() client id: f_00004-2-6 loss: 0.974351  [  224/  306]
train() client id: f_00004-2-7 loss: 0.895678  [  256/  306]
train() client id: f_00004-2-8 loss: 0.824800  [  288/  306]
train() client id: f_00004-3-0 loss: 0.781058  [   32/  306]
train() client id: f_00004-3-1 loss: 0.737843  [   64/  306]
train() client id: f_00004-3-2 loss: 0.686395  [   96/  306]
train() client id: f_00004-3-3 loss: 0.873617  [  128/  306]
train() client id: f_00004-3-4 loss: 0.990034  [  160/  306]
train() client id: f_00004-3-5 loss: 0.885346  [  192/  306]
train() client id: f_00004-3-6 loss: 0.886873  [  224/  306]
train() client id: f_00004-3-7 loss: 0.811158  [  256/  306]
train() client id: f_00004-3-8 loss: 1.031927  [  288/  306]
train() client id: f_00004-4-0 loss: 0.866913  [   32/  306]
train() client id: f_00004-4-1 loss: 0.746719  [   64/  306]
train() client id: f_00004-4-2 loss: 0.965354  [   96/  306]
train() client id: f_00004-4-3 loss: 0.759368  [  128/  306]
train() client id: f_00004-4-4 loss: 0.860858  [  160/  306]
train() client id: f_00004-4-5 loss: 0.752655  [  192/  306]
train() client id: f_00004-4-6 loss: 0.848851  [  224/  306]
train() client id: f_00004-4-7 loss: 0.869594  [  256/  306]
train() client id: f_00004-4-8 loss: 0.849664  [  288/  306]
train() client id: f_00004-5-0 loss: 0.745553  [   32/  306]
train() client id: f_00004-5-1 loss: 0.826463  [   64/  306]
train() client id: f_00004-5-2 loss: 0.918557  [   96/  306]
train() client id: f_00004-5-3 loss: 0.723143  [  128/  306]
train() client id: f_00004-5-4 loss: 0.817067  [  160/  306]
train() client id: f_00004-5-5 loss: 0.926463  [  192/  306]
train() client id: f_00004-5-6 loss: 0.849298  [  224/  306]
train() client id: f_00004-5-7 loss: 0.815810  [  256/  306]
train() client id: f_00004-5-8 loss: 0.907471  [  288/  306]
train() client id: f_00004-6-0 loss: 0.703050  [   32/  306]
train() client id: f_00004-6-1 loss: 0.863756  [   64/  306]
train() client id: f_00004-6-2 loss: 0.839880  [   96/  306]
train() client id: f_00004-6-3 loss: 0.816284  [  128/  306]
train() client id: f_00004-6-4 loss: 0.880158  [  160/  306]
train() client id: f_00004-6-5 loss: 0.783006  [  192/  306]
train() client id: f_00004-6-6 loss: 1.009679  [  224/  306]
train() client id: f_00004-6-7 loss: 0.732700  [  256/  306]
train() client id: f_00004-6-8 loss: 0.975937  [  288/  306]
train() client id: f_00004-7-0 loss: 0.866282  [   32/  306]
train() client id: f_00004-7-1 loss: 0.816296  [   64/  306]
train() client id: f_00004-7-2 loss: 0.845348  [   96/  306]
train() client id: f_00004-7-3 loss: 0.891721  [  128/  306]
train() client id: f_00004-7-4 loss: 0.706895  [  160/  306]
train() client id: f_00004-7-5 loss: 0.973281  [  192/  306]
train() client id: f_00004-7-6 loss: 0.778876  [  224/  306]
train() client id: f_00004-7-7 loss: 0.833309  [  256/  306]
train() client id: f_00004-7-8 loss: 0.848218  [  288/  306]
train() client id: f_00004-8-0 loss: 0.926007  [   32/  306]
train() client id: f_00004-8-1 loss: 0.657492  [   64/  306]
train() client id: f_00004-8-2 loss: 0.905999  [   96/  306]
train() client id: f_00004-8-3 loss: 0.847066  [  128/  306]
train() client id: f_00004-8-4 loss: 0.750182  [  160/  306]
train() client id: f_00004-8-5 loss: 0.791732  [  192/  306]
train() client id: f_00004-8-6 loss: 0.780030  [  224/  306]
train() client id: f_00004-8-7 loss: 0.902054  [  256/  306]
train() client id: f_00004-8-8 loss: 0.912217  [  288/  306]
train() client id: f_00004-9-0 loss: 0.803838  [   32/  306]
train() client id: f_00004-9-1 loss: 0.989147  [   64/  306]
train() client id: f_00004-9-2 loss: 0.817907  [   96/  306]
train() client id: f_00004-9-3 loss: 0.766924  [  128/  306]
train() client id: f_00004-9-4 loss: 0.819941  [  160/  306]
train() client id: f_00004-9-5 loss: 0.748950  [  192/  306]
train() client id: f_00004-9-6 loss: 0.792535  [  224/  306]
train() client id: f_00004-9-7 loss: 0.851587  [  256/  306]
train() client id: f_00004-9-8 loss: 0.838837  [  288/  306]
train() client id: f_00004-10-0 loss: 0.884060  [   32/  306]
train() client id: f_00004-10-1 loss: 0.863613  [   64/  306]
train() client id: f_00004-10-2 loss: 0.846728  [   96/  306]
train() client id: f_00004-10-3 loss: 0.788779  [  128/  306]
train() client id: f_00004-10-4 loss: 0.811959  [  160/  306]
train() client id: f_00004-10-5 loss: 0.810560  [  192/  306]
train() client id: f_00004-10-6 loss: 0.779299  [  224/  306]
train() client id: f_00004-10-7 loss: 0.841103  [  256/  306]
train() client id: f_00004-10-8 loss: 0.792323  [  288/  306]
train() client id: f_00004-11-0 loss: 0.706032  [   32/  306]
train() client id: f_00004-11-1 loss: 0.873748  [   64/  306]
train() client id: f_00004-11-2 loss: 0.788054  [   96/  306]
train() client id: f_00004-11-3 loss: 0.874383  [  128/  306]
train() client id: f_00004-11-4 loss: 0.980699  [  160/  306]
train() client id: f_00004-11-5 loss: 0.759673  [  192/  306]
train() client id: f_00004-11-6 loss: 0.760208  [  224/  306]
train() client id: f_00004-11-7 loss: 0.928172  [  256/  306]
train() client id: f_00004-11-8 loss: 0.829710  [  288/  306]
train() client id: f_00005-0-0 loss: 0.669343  [   32/  146]
train() client id: f_00005-0-1 loss: 0.823164  [   64/  146]
train() client id: f_00005-0-2 loss: 0.655145  [   96/  146]
train() client id: f_00005-0-3 loss: 0.614224  [  128/  146]
train() client id: f_00005-1-0 loss: 0.623060  [   32/  146]
train() client id: f_00005-1-1 loss: 0.744745  [   64/  146]
train() client id: f_00005-1-2 loss: 0.733063  [   96/  146]
train() client id: f_00005-1-3 loss: 0.671253  [  128/  146]
train() client id: f_00005-2-0 loss: 0.669654  [   32/  146]
train() client id: f_00005-2-1 loss: 0.801665  [   64/  146]
train() client id: f_00005-2-2 loss: 0.643105  [   96/  146]
train() client id: f_00005-2-3 loss: 0.543281  [  128/  146]
train() client id: f_00005-3-0 loss: 0.627034  [   32/  146]
train() client id: f_00005-3-1 loss: 0.693204  [   64/  146]
train() client id: f_00005-3-2 loss: 0.729941  [   96/  146]
train() client id: f_00005-3-3 loss: 0.727176  [  128/  146]
train() client id: f_00005-4-0 loss: 0.728916  [   32/  146]
train() client id: f_00005-4-1 loss: 0.586680  [   64/  146]
train() client id: f_00005-4-2 loss: 0.729805  [   96/  146]
train() client id: f_00005-4-3 loss: 0.812127  [  128/  146]
train() client id: f_00005-5-0 loss: 0.672035  [   32/  146]
train() client id: f_00005-5-1 loss: 0.615826  [   64/  146]
train() client id: f_00005-5-2 loss: 0.635944  [   96/  146]
train() client id: f_00005-5-3 loss: 0.696158  [  128/  146]
train() client id: f_00005-6-0 loss: 0.608495  [   32/  146]
train() client id: f_00005-6-1 loss: 0.698200  [   64/  146]
train() client id: f_00005-6-2 loss: 0.638705  [   96/  146]
train() client id: f_00005-6-3 loss: 0.788330  [  128/  146]
train() client id: f_00005-7-0 loss: 0.631735  [   32/  146]
train() client id: f_00005-7-1 loss: 0.692905  [   64/  146]
train() client id: f_00005-7-2 loss: 0.584757  [   96/  146]
train() client id: f_00005-7-3 loss: 0.758468  [  128/  146]
train() client id: f_00005-8-0 loss: 0.578875  [   32/  146]
train() client id: f_00005-8-1 loss: 0.815971  [   64/  146]
train() client id: f_00005-8-2 loss: 0.554000  [   96/  146]
train() client id: f_00005-8-3 loss: 0.533015  [  128/  146]
train() client id: f_00005-9-0 loss: 0.861558  [   32/  146]
train() client id: f_00005-9-1 loss: 0.526751  [   64/  146]
train() client id: f_00005-9-2 loss: 0.503223  [   96/  146]
train() client id: f_00005-9-3 loss: 0.805272  [  128/  146]
train() client id: f_00005-10-0 loss: 0.751793  [   32/  146]
train() client id: f_00005-10-1 loss: 0.632708  [   64/  146]
train() client id: f_00005-10-2 loss: 0.729610  [   96/  146]
train() client id: f_00005-10-3 loss: 0.493039  [  128/  146]
train() client id: f_00005-11-0 loss: 0.503924  [   32/  146]
train() client id: f_00005-11-1 loss: 0.946364  [   64/  146]
train() client id: f_00005-11-2 loss: 0.565660  [   96/  146]
train() client id: f_00005-11-3 loss: 0.534731  [  128/  146]
train() client id: f_00006-0-0 loss: 0.438743  [   32/   54]
train() client id: f_00006-1-0 loss: 0.479049  [   32/   54]
train() client id: f_00006-2-0 loss: 0.477521  [   32/   54]
train() client id: f_00006-3-0 loss: 0.467961  [   32/   54]
train() client id: f_00006-4-0 loss: 0.461102  [   32/   54]
train() client id: f_00006-5-0 loss: 0.481127  [   32/   54]
train() client id: f_00006-6-0 loss: 0.522879  [   32/   54]
train() client id: f_00006-7-0 loss: 0.482826  [   32/   54]
train() client id: f_00006-8-0 loss: 0.482003  [   32/   54]
train() client id: f_00006-9-0 loss: 0.539699  [   32/   54]
train() client id: f_00006-10-0 loss: 0.482127  [   32/   54]
train() client id: f_00006-11-0 loss: 0.470933  [   32/   54]
train() client id: f_00007-0-0 loss: 0.481897  [   32/  179]
train() client id: f_00007-0-1 loss: 0.684523  [   64/  179]
train() client id: f_00007-0-2 loss: 0.455993  [   96/  179]
train() client id: f_00007-0-3 loss: 0.545328  [  128/  179]
train() client id: f_00007-0-4 loss: 0.455156  [  160/  179]
train() client id: f_00007-1-0 loss: 0.507478  [   32/  179]
train() client id: f_00007-1-1 loss: 0.335215  [   64/  179]
train() client id: f_00007-1-2 loss: 0.394171  [   96/  179]
train() client id: f_00007-1-3 loss: 0.723180  [  128/  179]
train() client id: f_00007-1-4 loss: 0.558572  [  160/  179]
train() client id: f_00007-2-0 loss: 0.385424  [   32/  179]
train() client id: f_00007-2-1 loss: 0.391171  [   64/  179]
train() client id: f_00007-2-2 loss: 0.997484  [   96/  179]
train() client id: f_00007-2-3 loss: 0.373136  [  128/  179]
train() client id: f_00007-2-4 loss: 0.442467  [  160/  179]
train() client id: f_00007-3-0 loss: 0.450445  [   32/  179]
train() client id: f_00007-3-1 loss: 0.670294  [   64/  179]
train() client id: f_00007-3-2 loss: 0.339415  [   96/  179]
train() client id: f_00007-3-3 loss: 0.316666  [  128/  179]
train() client id: f_00007-3-4 loss: 0.701198  [  160/  179]
train() client id: f_00007-4-0 loss: 0.441343  [   32/  179]
train() client id: f_00007-4-1 loss: 0.524768  [   64/  179]
train() client id: f_00007-4-2 loss: 0.565911  [   96/  179]
train() client id: f_00007-4-3 loss: 0.328036  [  128/  179]
train() client id: f_00007-4-4 loss: 0.536778  [  160/  179]
train() client id: f_00007-5-0 loss: 0.424948  [   32/  179]
train() client id: f_00007-5-1 loss: 0.495058  [   64/  179]
train() client id: f_00007-5-2 loss: 0.494507  [   96/  179]
train() client id: f_00007-5-3 loss: 0.700458  [  128/  179]
train() client id: f_00007-5-4 loss: 0.343141  [  160/  179]
train() client id: f_00007-6-0 loss: 0.530830  [   32/  179]
train() client id: f_00007-6-1 loss: 0.494488  [   64/  179]
train() client id: f_00007-6-2 loss: 0.613774  [   96/  179]
train() client id: f_00007-6-3 loss: 0.526302  [  128/  179]
train() client id: f_00007-6-4 loss: 0.307326  [  160/  179]
train() client id: f_00007-7-0 loss: 0.491659  [   32/  179]
train() client id: f_00007-7-1 loss: 0.513222  [   64/  179]
train() client id: f_00007-7-2 loss: 0.309471  [   96/  179]
train() client id: f_00007-7-3 loss: 0.413679  [  128/  179]
train() client id: f_00007-7-4 loss: 0.524943  [  160/  179]
train() client id: f_00007-8-0 loss: 0.398247  [   32/  179]
train() client id: f_00007-8-1 loss: 0.321182  [   64/  179]
train() client id: f_00007-8-2 loss: 0.539481  [   96/  179]
train() client id: f_00007-8-3 loss: 0.671648  [  128/  179]
train() client id: f_00007-8-4 loss: 0.475122  [  160/  179]
train() client id: f_00007-9-0 loss: 0.537268  [   32/  179]
train() client id: f_00007-9-1 loss: 0.524833  [   64/  179]
train() client id: f_00007-9-2 loss: 0.500124  [   96/  179]
train() client id: f_00007-9-3 loss: 0.452528  [  128/  179]
train() client id: f_00007-9-4 loss: 0.407097  [  160/  179]
train() client id: f_00007-10-0 loss: 0.549441  [   32/  179]
train() client id: f_00007-10-1 loss: 0.531465  [   64/  179]
train() client id: f_00007-10-2 loss: 0.438425  [   96/  179]
train() client id: f_00007-10-3 loss: 0.435930  [  128/  179]
train() client id: f_00007-10-4 loss: 0.368040  [  160/  179]
train() client id: f_00007-11-0 loss: 0.643792  [   32/  179]
train() client id: f_00007-11-1 loss: 0.315765  [   64/  179]
train() client id: f_00007-11-2 loss: 0.321775  [   96/  179]
train() client id: f_00007-11-3 loss: 0.625327  [  128/  179]
train() client id: f_00007-11-4 loss: 0.414714  [  160/  179]
train() client id: f_00008-0-0 loss: 0.724771  [   32/  130]
train() client id: f_00008-0-1 loss: 0.927958  [   64/  130]
train() client id: f_00008-0-2 loss: 0.828349  [   96/  130]
train() client id: f_00008-0-3 loss: 0.788687  [  128/  130]
train() client id: f_00008-1-0 loss: 0.842553  [   32/  130]
train() client id: f_00008-1-1 loss: 0.829252  [   64/  130]
train() client id: f_00008-1-2 loss: 0.751675  [   96/  130]
train() client id: f_00008-1-3 loss: 0.841204  [  128/  130]
train() client id: f_00008-2-0 loss: 0.757521  [   32/  130]
train() client id: f_00008-2-1 loss: 0.797283  [   64/  130]
train() client id: f_00008-2-2 loss: 0.816650  [   96/  130]
train() client id: f_00008-2-3 loss: 0.896936  [  128/  130]
train() client id: f_00008-3-0 loss: 0.771001  [   32/  130]
train() client id: f_00008-3-1 loss: 0.875462  [   64/  130]
train() client id: f_00008-3-2 loss: 0.772815  [   96/  130]
train() client id: f_00008-3-3 loss: 0.832776  [  128/  130]
train() client id: f_00008-4-0 loss: 0.718577  [   32/  130]
train() client id: f_00008-4-1 loss: 0.842571  [   64/  130]
train() client id: f_00008-4-2 loss: 0.853550  [   96/  130]
train() client id: f_00008-4-3 loss: 0.855167  [  128/  130]
train() client id: f_00008-5-0 loss: 0.860848  [   32/  130]
train() client id: f_00008-5-1 loss: 0.780559  [   64/  130]
train() client id: f_00008-5-2 loss: 0.813793  [   96/  130]
train() client id: f_00008-5-3 loss: 0.759897  [  128/  130]
train() client id: f_00008-6-0 loss: 0.863483  [   32/  130]
train() client id: f_00008-6-1 loss: 0.821013  [   64/  130]
train() client id: f_00008-6-2 loss: 0.781872  [   96/  130]
train() client id: f_00008-6-3 loss: 0.797849  [  128/  130]
train() client id: f_00008-7-0 loss: 0.804736  [   32/  130]
train() client id: f_00008-7-1 loss: 0.795998  [   64/  130]
train() client id: f_00008-7-2 loss: 0.919975  [   96/  130]
train() client id: f_00008-7-3 loss: 0.738927  [  128/  130]
train() client id: f_00008-8-0 loss: 0.858001  [   32/  130]
train() client id: f_00008-8-1 loss: 0.888649  [   64/  130]
train() client id: f_00008-8-2 loss: 0.815320  [   96/  130]
train() client id: f_00008-8-3 loss: 0.702535  [  128/  130]
train() client id: f_00008-9-0 loss: 0.890102  [   32/  130]
train() client id: f_00008-9-1 loss: 0.826819  [   64/  130]
train() client id: f_00008-9-2 loss: 0.675062  [   96/  130]
train() client id: f_00008-9-3 loss: 0.843216  [  128/  130]
train() client id: f_00008-10-0 loss: 0.843783  [   32/  130]
train() client id: f_00008-10-1 loss: 0.712397  [   64/  130]
train() client id: f_00008-10-2 loss: 0.807951  [   96/  130]
train() client id: f_00008-10-3 loss: 0.861612  [  128/  130]
train() client id: f_00008-11-0 loss: 0.891259  [   32/  130]
train() client id: f_00008-11-1 loss: 0.721691  [   64/  130]
train() client id: f_00008-11-2 loss: 0.839745  [   96/  130]
train() client id: f_00008-11-3 loss: 0.811387  [  128/  130]
train() client id: f_00009-0-0 loss: 1.183320  [   32/  118]
train() client id: f_00009-0-1 loss: 1.016313  [   64/  118]
train() client id: f_00009-0-2 loss: 0.888393  [   96/  118]
train() client id: f_00009-1-0 loss: 1.021896  [   32/  118]
train() client id: f_00009-1-1 loss: 1.083559  [   64/  118]
train() client id: f_00009-1-2 loss: 0.923074  [   96/  118]
train() client id: f_00009-2-0 loss: 0.901813  [   32/  118]
train() client id: f_00009-2-1 loss: 0.910350  [   64/  118]
train() client id: f_00009-2-2 loss: 0.917076  [   96/  118]
train() client id: f_00009-3-0 loss: 0.961212  [   32/  118]
train() client id: f_00009-3-1 loss: 0.782600  [   64/  118]
train() client id: f_00009-3-2 loss: 0.874447  [   96/  118]
train() client id: f_00009-4-0 loss: 0.786609  [   32/  118]
train() client id: f_00009-4-1 loss: 0.774480  [   64/  118]
train() client id: f_00009-4-2 loss: 1.024668  [   96/  118]
train() client id: f_00009-5-0 loss: 0.946271  [   32/  118]
train() client id: f_00009-5-1 loss: 0.886781  [   64/  118]
train() client id: f_00009-5-2 loss: 0.678364  [   96/  118]
train() client id: f_00009-6-0 loss: 0.711055  [   32/  118]
train() client id: f_00009-6-1 loss: 0.705274  [   64/  118]
train() client id: f_00009-6-2 loss: 0.784146  [   96/  118]
train() client id: f_00009-7-0 loss: 0.759305  [   32/  118]
train() client id: f_00009-7-1 loss: 0.884988  [   64/  118]
train() client id: f_00009-7-2 loss: 0.626836  [   96/  118]
train() client id: f_00009-8-0 loss: 0.761504  [   32/  118]
train() client id: f_00009-8-1 loss: 0.809960  [   64/  118]
train() client id: f_00009-8-2 loss: 0.831459  [   96/  118]
train() client id: f_00009-9-0 loss: 0.807293  [   32/  118]
train() client id: f_00009-9-1 loss: 0.720424  [   64/  118]
train() client id: f_00009-9-2 loss: 0.771810  [   96/  118]
train() client id: f_00009-10-0 loss: 0.893870  [   32/  118]
train() client id: f_00009-10-1 loss: 0.733828  [   64/  118]
train() client id: f_00009-10-2 loss: 0.596999  [   96/  118]
train() client id: f_00009-11-0 loss: 0.759651  [   32/  118]
train() client id: f_00009-11-1 loss: 0.828314  [   64/  118]
train() client id: f_00009-11-2 loss: 0.572867  [   96/  118]
At round 36 accuracy: 0.6419098143236074
At round 36 training accuracy: 0.5895372233400402
At round 36 training loss: 0.8247938335244185
update_location
xs = -3.905658 4.200318 200.009024 18.811294 0.979296 3.956410 -162.443192 -141.324852 184.663977 -127.060879 
ys = 192.587959 175.555839 1.320614 -162.455176 154.350187 137.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -2.2114261681174354
ys mean: 51.8941425355287
dists_uav = 217.037730 202.082892 223.618769 191.691286 183.915576 170.318506 190.773901 173.128247 210.735508 161.742013 
uav_gains = -108.788055 -107.813529 -109.242609 -107.161560 -106.674836 -105.804832 -107.104263 -105.987723 -108.369310 -105.232699 
uav_gains_db_mean: -107.21794168348549
dists_bs = 171.995959 179.201180 413.275087 389.151402 177.186702 182.779058 178.068269 177.403129 392.610104 177.591251 
bs_gains = -102.161570 -102.660603 -112.821757 -112.090380 -102.523131 -102.900999 -102.583482 -102.537975 -112.197980 -102.550863 
bs_gains_db_mean: -105.50287411710389
Round 37
-------------------------------
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.10747648 12.63082676  6.01756463  2.17165072 14.56715747  7.00995328
  2.69087946  8.58799468  6.33825934  5.68528245]
obj_prev = 71.80704525657299
eta_min = 7.508228350489047e-16	eta_max = 0.9300641933191646
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 16.65764355541381	eta = 0.909090909090909
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 30.742318165825036	eta = 0.4925884977645458
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 23.795554951531557	eta = 0.636392483972255
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.538890009475697	eta = 0.6718748046925549
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.472173027112785	eta = 0.6738695143025546
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.47196919842037	eta = 0.6738756265369006
eta = 0.6738756265369006
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [0.0325705  0.06850145 0.03205351 0.01111533 0.07909978 0.03774042
 0.01395879 0.04627079 0.03360448 0.03050253]
ene_total = [2.01826629 3.59433964 2.00757769 0.95394728 4.09618977 2.13534693
 1.08775634 2.59856536 2.19433777 1.78564212]
ti_comp = [0.49324588 0.52112484 0.49032366 0.50250852 0.52157476 0.52032434
 0.50280454 0.5082184  0.46655987 0.52148445]
ti_coms = [0.09871374 0.07083478 0.10163597 0.08945111 0.07038486 0.07163528
 0.08915509 0.08374122 0.12539975 0.07047517]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.87618492e-06 7.39768305e-05 8.56132548e-06 3.39907040e-07
 1.13703226e-04 1.24094325e-05 6.72395117e-07 2.39717625e-05
 1.08957452e-05 6.52236260e-06]
ene_total = [0.46984203 0.34036317 0.48372332 0.42538889 0.34011276 0.34124206
 0.42399702 0.39936008 0.59683981 0.33544538]
optimize_network iter = 0 obj = 4.156314504851677
eta = 0.6738756265369006
freqs = [33016494.15795401 65724604.40972588 32686070.96869287 11059841.67344831
 75827849.2653193  36266244.90193002 13880926.45922257 45522544.15430705
 36013045.58118701 29245868.53629994]
eta_min = 0.673875626536905	eta_max = 0.6738756265369027
af = 0.01021278956996407	bf = 1.3514443765451492	zeta = 0.011234068526960479	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [2.14332394e-06 1.78631149e-05 2.06729513e-06 8.20770299e-08
 2.74558099e-05 2.99649386e-06 1.62362610e-07 5.78843872e-06
 2.63098523e-06 1.57494870e-06]
ene_total = [1.68434772 1.21143542 1.73418576 1.52598255 1.2053965  1.2225551
 1.52094637 1.42954967 2.1396742  1.20252205]
ti_comp = [0.49324588 0.52112484 0.49032366 0.50250852 0.52157476 0.52032434
 0.50280454 0.5082184  0.46655987 0.52148445]
ti_coms = [0.09871374 0.07083478 0.10163597 0.08945111 0.07038486 0.07163528
 0.08915509 0.08374122 0.12539975 0.07047517]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.87618492e-06 7.39768305e-05 8.56132548e-06 3.39907040e-07
 1.13703226e-04 1.24094325e-05 6.72395117e-07 2.39717625e-05
 1.08957452e-05 6.52236260e-06]
ene_total = [0.46984203 0.34036317 0.48372332 0.42538889 0.34011276 0.34124206
 0.42399702 0.39936008 0.59683981 0.33544538]
optimize_network iter = 1 obj = 4.156314504851704
eta = 0.6738756265369027
freqs = [33016494.15795401 65724604.40972584 32686070.96869286 11059841.67344831
 75827849.26531927 36266244.90193    13880926.45922257 45522544.15430704
 36013045.58118702 29245868.53629993]
Done!
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.24107198e-06 6.86836056e-05 7.94874151e-06 3.15585852e-07
 1.05567480e-04 1.15215069e-05 6.24283587e-07 2.22565238e-05
 1.01161277e-05 6.05567145e-06]
ene_total = [0.00987962 0.00715216 0.01017155 0.00894543 0.00714405 0.00717505
 0.00891613 0.00839638 0.01255009 0.00705357]
At round 37 energy consumption: 0.08738402800819062
At round 37 eta: 0.6738756265369027
At round 37 a_n: 15.508406518389636
At round 37 local rounds: 12.924801449949687
At round 37 global rounds: 47.55365676507614
gradient difference: 0.442664235830307
train() client id: f_00000-0-0 loss: 1.060869  [   32/  126]
train() client id: f_00000-0-1 loss: 1.064178  [   64/  126]
train() client id: f_00000-0-2 loss: 1.176359  [   96/  126]
train() client id: f_00000-1-0 loss: 1.107318  [   32/  126]
train() client id: f_00000-1-1 loss: 1.079467  [   64/  126]
train() client id: f_00000-1-2 loss: 1.109454  [   96/  126]
train() client id: f_00000-2-0 loss: 1.089945  [   32/  126]
train() client id: f_00000-2-1 loss: 0.915569  [   64/  126]
train() client id: f_00000-2-2 loss: 0.830319  [   96/  126]
train() client id: f_00000-3-0 loss: 0.986780  [   32/  126]
train() client id: f_00000-3-1 loss: 0.941669  [   64/  126]
train() client id: f_00000-3-2 loss: 0.926997  [   96/  126]
train() client id: f_00000-4-0 loss: 0.958688  [   32/  126]
train() client id: f_00000-4-1 loss: 0.907355  [   64/  126]
train() client id: f_00000-4-2 loss: 0.849261  [   96/  126]
train() client id: f_00000-5-0 loss: 0.743631  [   32/  126]
train() client id: f_00000-5-1 loss: 0.905485  [   64/  126]
train() client id: f_00000-5-2 loss: 0.861174  [   96/  126]
train() client id: f_00000-6-0 loss: 0.802129  [   32/  126]
train() client id: f_00000-6-1 loss: 0.944514  [   64/  126]
train() client id: f_00000-6-2 loss: 0.828729  [   96/  126]
train() client id: f_00000-7-0 loss: 0.803913  [   32/  126]
train() client id: f_00000-7-1 loss: 0.944852  [   64/  126]
train() client id: f_00000-7-2 loss: 0.893078  [   96/  126]
train() client id: f_00000-8-0 loss: 0.852103  [   32/  126]
train() client id: f_00000-8-1 loss: 0.869676  [   64/  126]
train() client id: f_00000-8-2 loss: 0.765651  [   96/  126]
train() client id: f_00000-9-0 loss: 0.849296  [   32/  126]
train() client id: f_00000-9-1 loss: 0.868688  [   64/  126]
train() client id: f_00000-9-2 loss: 0.773902  [   96/  126]
train() client id: f_00000-10-0 loss: 0.796173  [   32/  126]
train() client id: f_00000-10-1 loss: 0.738700  [   64/  126]
train() client id: f_00000-10-2 loss: 0.869856  [   96/  126]
train() client id: f_00000-11-0 loss: 0.768695  [   32/  126]
train() client id: f_00000-11-1 loss: 0.820859  [   64/  126]
train() client id: f_00000-11-2 loss: 0.813609  [   96/  126]
train() client id: f_00001-0-0 loss: 0.357537  [   32/  265]
train() client id: f_00001-0-1 loss: 0.398774  [   64/  265]
train() client id: f_00001-0-2 loss: 0.445730  [   96/  265]
train() client id: f_00001-0-3 loss: 0.571621  [  128/  265]
train() client id: f_00001-0-4 loss: 0.389417  [  160/  265]
train() client id: f_00001-0-5 loss: 0.495474  [  192/  265]
train() client id: f_00001-0-6 loss: 0.441460  [  224/  265]
train() client id: f_00001-0-7 loss: 0.448758  [  256/  265]
train() client id: f_00001-1-0 loss: 0.497456  [   32/  265]
train() client id: f_00001-1-1 loss: 0.448814  [   64/  265]
train() client id: f_00001-1-2 loss: 0.377150  [   96/  265]
train() client id: f_00001-1-3 loss: 0.478099  [  128/  265]
train() client id: f_00001-1-4 loss: 0.440908  [  160/  265]
train() client id: f_00001-1-5 loss: 0.563425  [  192/  265]
train() client id: f_00001-1-6 loss: 0.394179  [  224/  265]
train() client id: f_00001-1-7 loss: 0.348124  [  256/  265]
train() client id: f_00001-2-0 loss: 0.444565  [   32/  265]
train() client id: f_00001-2-1 loss: 0.487024  [   64/  265]
train() client id: f_00001-2-2 loss: 0.421255  [   96/  265]
train() client id: f_00001-2-3 loss: 0.376456  [  128/  265]
train() client id: f_00001-2-4 loss: 0.422646  [  160/  265]
train() client id: f_00001-2-5 loss: 0.408775  [  192/  265]
train() client id: f_00001-2-6 loss: 0.491033  [  224/  265]
train() client id: f_00001-2-7 loss: 0.450541  [  256/  265]
train() client id: f_00001-3-0 loss: 0.358668  [   32/  265]
train() client id: f_00001-3-1 loss: 0.449626  [   64/  265]
train() client id: f_00001-3-2 loss: 0.430856  [   96/  265]
train() client id: f_00001-3-3 loss: 0.396464  [  128/  265]
train() client id: f_00001-3-4 loss: 0.559740  [  160/  265]
train() client id: f_00001-3-5 loss: 0.429787  [  192/  265]
train() client id: f_00001-3-6 loss: 0.406783  [  224/  265]
train() client id: f_00001-3-7 loss: 0.409389  [  256/  265]
train() client id: f_00001-4-0 loss: 0.329800  [   32/  265]
train() client id: f_00001-4-1 loss: 0.361168  [   64/  265]
train() client id: f_00001-4-2 loss: 0.386212  [   96/  265]
train() client id: f_00001-4-3 loss: 0.380070  [  128/  265]
train() client id: f_00001-4-4 loss: 0.546006  [  160/  265]
train() client id: f_00001-4-5 loss: 0.503238  [  192/  265]
train() client id: f_00001-4-6 loss: 0.441621  [  224/  265]
train() client id: f_00001-4-7 loss: 0.460861  [  256/  265]
train() client id: f_00001-5-0 loss: 0.319648  [   32/  265]
train() client id: f_00001-5-1 loss: 0.429875  [   64/  265]
train() client id: f_00001-5-2 loss: 0.397246  [   96/  265]
train() client id: f_00001-5-3 loss: 0.462848  [  128/  265]
train() client id: f_00001-5-4 loss: 0.347345  [  160/  265]
train() client id: f_00001-5-5 loss: 0.389396  [  192/  265]
train() client id: f_00001-5-6 loss: 0.594169  [  224/  265]
train() client id: f_00001-5-7 loss: 0.384115  [  256/  265]
train() client id: f_00001-6-0 loss: 0.380925  [   32/  265]
train() client id: f_00001-6-1 loss: 0.335391  [   64/  265]
train() client id: f_00001-6-2 loss: 0.509485  [   96/  265]
train() client id: f_00001-6-3 loss: 0.508026  [  128/  265]
train() client id: f_00001-6-4 loss: 0.341435  [  160/  265]
train() client id: f_00001-6-5 loss: 0.478262  [  192/  265]
train() client id: f_00001-6-6 loss: 0.437765  [  224/  265]
train() client id: f_00001-6-7 loss: 0.393987  [  256/  265]
train() client id: f_00001-7-0 loss: 0.386675  [   32/  265]
train() client id: f_00001-7-1 loss: 0.362037  [   64/  265]
train() client id: f_00001-7-2 loss: 0.329747  [   96/  265]
train() client id: f_00001-7-3 loss: 0.515940  [  128/  265]
train() client id: f_00001-7-4 loss: 0.363717  [  160/  265]
train() client id: f_00001-7-5 loss: 0.453905  [  192/  265]
train() client id: f_00001-7-6 loss: 0.585489  [  224/  265]
train() client id: f_00001-7-7 loss: 0.361412  [  256/  265]
train() client id: f_00001-8-0 loss: 0.421500  [   32/  265]
train() client id: f_00001-8-1 loss: 0.417692  [   64/  265]
train() client id: f_00001-8-2 loss: 0.365706  [   96/  265]
train() client id: f_00001-8-3 loss: 0.397504  [  128/  265]
train() client id: f_00001-8-4 loss: 0.505957  [  160/  265]
train() client id: f_00001-8-5 loss: 0.494561  [  192/  265]
train() client id: f_00001-8-6 loss: 0.319991  [  224/  265]
train() client id: f_00001-8-7 loss: 0.421775  [  256/  265]
train() client id: f_00001-9-0 loss: 0.357840  [   32/  265]
train() client id: f_00001-9-1 loss: 0.472843  [   64/  265]
train() client id: f_00001-9-2 loss: 0.369964  [   96/  265]
train() client id: f_00001-9-3 loss: 0.397301  [  128/  265]
train() client id: f_00001-9-4 loss: 0.373711  [  160/  265]
train() client id: f_00001-9-5 loss: 0.381093  [  192/  265]
train() client id: f_00001-9-6 loss: 0.406035  [  224/  265]
train() client id: f_00001-9-7 loss: 0.583095  [  256/  265]
train() client id: f_00001-10-0 loss: 0.511686  [   32/  265]
train() client id: f_00001-10-1 loss: 0.473594  [   64/  265]
train() client id: f_00001-10-2 loss: 0.435677  [   96/  265]
train() client id: f_00001-10-3 loss: 0.389892  [  128/  265]
train() client id: f_00001-10-4 loss: 0.394833  [  160/  265]
train() client id: f_00001-10-5 loss: 0.326835  [  192/  265]
train() client id: f_00001-10-6 loss: 0.485839  [  224/  265]
train() client id: f_00001-10-7 loss: 0.326915  [  256/  265]
train() client id: f_00001-11-0 loss: 0.320141  [   32/  265]
train() client id: f_00001-11-1 loss: 0.391788  [   64/  265]
train() client id: f_00001-11-2 loss: 0.330130  [   96/  265]
train() client id: f_00001-11-3 loss: 0.474091  [  128/  265]
train() client id: f_00001-11-4 loss: 0.447848  [  160/  265]
train() client id: f_00001-11-5 loss: 0.503936  [  192/  265]
train() client id: f_00001-11-6 loss: 0.316498  [  224/  265]
train() client id: f_00001-11-7 loss: 0.553887  [  256/  265]
train() client id: f_00002-0-0 loss: 1.065506  [   32/  124]
train() client id: f_00002-0-1 loss: 1.327224  [   64/  124]
train() client id: f_00002-0-2 loss: 1.073426  [   96/  124]
train() client id: f_00002-1-0 loss: 1.218825  [   32/  124]
train() client id: f_00002-1-1 loss: 1.130932  [   64/  124]
train() client id: f_00002-1-2 loss: 1.138586  [   96/  124]
train() client id: f_00002-2-0 loss: 1.360874  [   32/  124]
train() client id: f_00002-2-1 loss: 1.118420  [   64/  124]
train() client id: f_00002-2-2 loss: 1.065546  [   96/  124]
train() client id: f_00002-3-0 loss: 1.015465  [   32/  124]
train() client id: f_00002-3-1 loss: 1.281899  [   64/  124]
train() client id: f_00002-3-2 loss: 1.104626  [   96/  124]
train() client id: f_00002-4-0 loss: 1.108089  [   32/  124]
train() client id: f_00002-4-1 loss: 0.990689  [   64/  124]
train() client id: f_00002-4-2 loss: 1.269728  [   96/  124]
train() client id: f_00002-5-0 loss: 1.026254  [   32/  124]
train() client id: f_00002-5-1 loss: 1.017842  [   64/  124]
train() client id: f_00002-5-2 loss: 1.093063  [   96/  124]
train() client id: f_00002-6-0 loss: 1.144237  [   32/  124]
train() client id: f_00002-6-1 loss: 1.171407  [   64/  124]
train() client id: f_00002-6-2 loss: 1.087224  [   96/  124]
train() client id: f_00002-7-0 loss: 1.131381  [   32/  124]
train() client id: f_00002-7-1 loss: 0.939738  [   64/  124]
train() client id: f_00002-7-2 loss: 0.951703  [   96/  124]
train() client id: f_00002-8-0 loss: 1.078087  [   32/  124]
train() client id: f_00002-8-1 loss: 0.935908  [   64/  124]
train() client id: f_00002-8-2 loss: 1.033654  [   96/  124]
train() client id: f_00002-9-0 loss: 1.176121  [   32/  124]
train() client id: f_00002-9-1 loss: 0.841077  [   64/  124]
train() client id: f_00002-9-2 loss: 1.100525  [   96/  124]
train() client id: f_00002-10-0 loss: 1.024468  [   32/  124]
train() client id: f_00002-10-1 loss: 0.955139  [   64/  124]
train() client id: f_00002-10-2 loss: 1.070920  [   96/  124]
train() client id: f_00002-11-0 loss: 1.106685  [   32/  124]
train() client id: f_00002-11-1 loss: 1.023083  [   64/  124]
train() client id: f_00002-11-2 loss: 1.091696  [   96/  124]
train() client id: f_00003-0-0 loss: 0.516354  [   32/   43]
train() client id: f_00003-1-0 loss: 0.522905  [   32/   43]
train() client id: f_00003-2-0 loss: 0.695437  [   32/   43]
train() client id: f_00003-3-0 loss: 0.569957  [   32/   43]
train() client id: f_00003-4-0 loss: 0.623085  [   32/   43]
train() client id: f_00003-5-0 loss: 0.696670  [   32/   43]
train() client id: f_00003-6-0 loss: 0.642160  [   32/   43]
train() client id: f_00003-7-0 loss: 0.572608  [   32/   43]
train() client id: f_00003-8-0 loss: 0.596116  [   32/   43]
train() client id: f_00003-9-0 loss: 0.639025  [   32/   43]
train() client id: f_00003-10-0 loss: 0.600781  [   32/   43]
train() client id: f_00003-11-0 loss: 0.490945  [   32/   43]
train() client id: f_00004-0-0 loss: 0.760422  [   32/  306]
train() client id: f_00004-0-1 loss: 0.957733  [   64/  306]
train() client id: f_00004-0-2 loss: 0.661553  [   96/  306]
train() client id: f_00004-0-3 loss: 0.830271  [  128/  306]
train() client id: f_00004-0-4 loss: 0.837453  [  160/  306]
train() client id: f_00004-0-5 loss: 0.789508  [  192/  306]
train() client id: f_00004-0-6 loss: 0.686488  [  224/  306]
train() client id: f_00004-0-7 loss: 0.952308  [  256/  306]
train() client id: f_00004-0-8 loss: 1.048226  [  288/  306]
train() client id: f_00004-1-0 loss: 0.840337  [   32/  306]
train() client id: f_00004-1-1 loss: 0.974882  [   64/  306]
train() client id: f_00004-1-2 loss: 0.789735  [   96/  306]
train() client id: f_00004-1-3 loss: 0.908457  [  128/  306]
train() client id: f_00004-1-4 loss: 0.795155  [  160/  306]
train() client id: f_00004-1-5 loss: 0.713120  [  192/  306]
train() client id: f_00004-1-6 loss: 0.870311  [  224/  306]
train() client id: f_00004-1-7 loss: 0.788172  [  256/  306]
train() client id: f_00004-1-8 loss: 0.960494  [  288/  306]
train() client id: f_00004-2-0 loss: 0.795224  [   32/  306]
train() client id: f_00004-2-1 loss: 0.771288  [   64/  306]
train() client id: f_00004-2-2 loss: 0.846710  [   96/  306]
train() client id: f_00004-2-3 loss: 0.759876  [  128/  306]
train() client id: f_00004-2-4 loss: 0.938219  [  160/  306]
train() client id: f_00004-2-5 loss: 0.892469  [  192/  306]
train() client id: f_00004-2-6 loss: 0.793199  [  224/  306]
train() client id: f_00004-2-7 loss: 0.913239  [  256/  306]
train() client id: f_00004-2-8 loss: 0.903857  [  288/  306]
train() client id: f_00004-3-0 loss: 0.980454  [   32/  306]
train() client id: f_00004-3-1 loss: 0.776065  [   64/  306]
train() client id: f_00004-3-2 loss: 0.935161  [   96/  306]
train() client id: f_00004-3-3 loss: 0.831959  [  128/  306]
train() client id: f_00004-3-4 loss: 0.952647  [  160/  306]
train() client id: f_00004-3-5 loss: 0.801990  [  192/  306]
train() client id: f_00004-3-6 loss: 0.709803  [  224/  306]
train() client id: f_00004-3-7 loss: 0.778820  [  256/  306]
train() client id: f_00004-3-8 loss: 0.859401  [  288/  306]
train() client id: f_00004-4-0 loss: 0.806533  [   32/  306]
train() client id: f_00004-4-1 loss: 0.905307  [   64/  306]
train() client id: f_00004-4-2 loss: 0.814420  [   96/  306]
train() client id: f_00004-4-3 loss: 0.871310  [  128/  306]
train() client id: f_00004-4-4 loss: 0.786391  [  160/  306]
train() client id: f_00004-4-5 loss: 0.869389  [  192/  306]
train() client id: f_00004-4-6 loss: 0.847993  [  224/  306]
train() client id: f_00004-4-7 loss: 0.989471  [  256/  306]
train() client id: f_00004-4-8 loss: 0.822309  [  288/  306]
train() client id: f_00004-5-0 loss: 0.999934  [   32/  306]
train() client id: f_00004-5-1 loss: 0.792775  [   64/  306]
train() client id: f_00004-5-2 loss: 1.048978  [   96/  306]
train() client id: f_00004-5-3 loss: 0.828227  [  128/  306]
train() client id: f_00004-5-4 loss: 0.753408  [  160/  306]
train() client id: f_00004-5-5 loss: 0.803459  [  192/  306]
train() client id: f_00004-5-6 loss: 0.937931  [  224/  306]
train() client id: f_00004-5-7 loss: 0.702961  [  256/  306]
train() client id: f_00004-5-8 loss: 0.888977  [  288/  306]
train() client id: f_00004-6-0 loss: 0.822768  [   32/  306]
train() client id: f_00004-6-1 loss: 0.764658  [   64/  306]
train() client id: f_00004-6-2 loss: 0.808726  [   96/  306]
train() client id: f_00004-6-3 loss: 0.873908  [  128/  306]
train() client id: f_00004-6-4 loss: 0.954422  [  160/  306]
train() client id: f_00004-6-5 loss: 0.901236  [  192/  306]
train() client id: f_00004-6-6 loss: 0.830749  [  224/  306]
train() client id: f_00004-6-7 loss: 0.929226  [  256/  306]
train() client id: f_00004-6-8 loss: 0.830994  [  288/  306]
train() client id: f_00004-7-0 loss: 0.932375  [   32/  306]
train() client id: f_00004-7-1 loss: 0.838632  [   64/  306]
train() client id: f_00004-7-2 loss: 0.931643  [   96/  306]
train() client id: f_00004-7-3 loss: 0.926971  [  128/  306]
train() client id: f_00004-7-4 loss: 0.847500  [  160/  306]
train() client id: f_00004-7-5 loss: 0.670283  [  192/  306]
train() client id: f_00004-7-6 loss: 0.878528  [  224/  306]
train() client id: f_00004-7-7 loss: 0.899646  [  256/  306]
train() client id: f_00004-7-8 loss: 0.685795  [  288/  306]
train() client id: f_00004-8-0 loss: 0.892970  [   32/  306]
train() client id: f_00004-8-1 loss: 0.886321  [   64/  306]
train() client id: f_00004-8-2 loss: 0.828530  [   96/  306]
train() client id: f_00004-8-3 loss: 0.781895  [  128/  306]
train() client id: f_00004-8-4 loss: 0.869980  [  160/  306]
train() client id: f_00004-8-5 loss: 0.766226  [  192/  306]
train() client id: f_00004-8-6 loss: 0.840073  [  224/  306]
train() client id: f_00004-8-7 loss: 0.906033  [  256/  306]
train() client id: f_00004-8-8 loss: 0.863742  [  288/  306]
train() client id: f_00004-9-0 loss: 0.834604  [   32/  306]
train() client id: f_00004-9-1 loss: 0.939881  [   64/  306]
train() client id: f_00004-9-2 loss: 0.908726  [   96/  306]
train() client id: f_00004-9-3 loss: 0.872599  [  128/  306]
train() client id: f_00004-9-4 loss: 0.815609  [  160/  306]
train() client id: f_00004-9-5 loss: 0.790504  [  192/  306]
train() client id: f_00004-9-6 loss: 0.812965  [  224/  306]
train() client id: f_00004-9-7 loss: 0.850169  [  256/  306]
train() client id: f_00004-9-8 loss: 0.910285  [  288/  306]
train() client id: f_00004-10-0 loss: 0.842653  [   32/  306]
train() client id: f_00004-10-1 loss: 0.884862  [   64/  306]
train() client id: f_00004-10-2 loss: 0.817554  [   96/  306]
train() client id: f_00004-10-3 loss: 0.813511  [  128/  306]
train() client id: f_00004-10-4 loss: 0.821457  [  160/  306]
train() client id: f_00004-10-5 loss: 0.844098  [  192/  306]
train() client id: f_00004-10-6 loss: 0.893302  [  224/  306]
train() client id: f_00004-10-7 loss: 0.850796  [  256/  306]
train() client id: f_00004-10-8 loss: 0.959762  [  288/  306]
train() client id: f_00004-11-0 loss: 0.827792  [   32/  306]
train() client id: f_00004-11-1 loss: 0.904846  [   64/  306]
train() client id: f_00004-11-2 loss: 0.841344  [   96/  306]
train() client id: f_00004-11-3 loss: 0.757768  [  128/  306]
train() client id: f_00004-11-4 loss: 0.899143  [  160/  306]
train() client id: f_00004-11-5 loss: 0.920138  [  192/  306]
train() client id: f_00004-11-6 loss: 0.908824  [  224/  306]
train() client id: f_00004-11-7 loss: 0.802337  [  256/  306]
train() client id: f_00004-11-8 loss: 0.853451  [  288/  306]
train() client id: f_00005-0-0 loss: 0.750644  [   32/  146]
train() client id: f_00005-0-1 loss: 1.026847  [   64/  146]
train() client id: f_00005-0-2 loss: 0.874640  [   96/  146]
train() client id: f_00005-0-3 loss: 0.493350  [  128/  146]
train() client id: f_00005-1-0 loss: 0.595900  [   32/  146]
train() client id: f_00005-1-1 loss: 0.745506  [   64/  146]
train() client id: f_00005-1-2 loss: 0.600217  [   96/  146]
train() client id: f_00005-1-3 loss: 0.978014  [  128/  146]
train() client id: f_00005-2-0 loss: 0.684373  [   32/  146]
train() client id: f_00005-2-1 loss: 0.997398  [   64/  146]
train() client id: f_00005-2-2 loss: 0.523539  [   96/  146]
train() client id: f_00005-2-3 loss: 0.650991  [  128/  146]
train() client id: f_00005-3-0 loss: 0.766246  [   32/  146]
train() client id: f_00005-3-1 loss: 0.412421  [   64/  146]
train() client id: f_00005-3-2 loss: 0.667435  [   96/  146]
train() client id: f_00005-3-3 loss: 1.122877  [  128/  146]
train() client id: f_00005-4-0 loss: 0.604090  [   32/  146]
train() client id: f_00005-4-1 loss: 0.788004  [   64/  146]
train() client id: f_00005-4-2 loss: 0.885250  [   96/  146]
train() client id: f_00005-4-3 loss: 0.777355  [  128/  146]
train() client id: f_00005-5-0 loss: 0.511854  [   32/  146]
train() client id: f_00005-5-1 loss: 0.578344  [   64/  146]
train() client id: f_00005-5-2 loss: 0.904819  [   96/  146]
train() client id: f_00005-5-3 loss: 0.937292  [  128/  146]
train() client id: f_00005-6-0 loss: 0.820943  [   32/  146]
train() client id: f_00005-6-1 loss: 1.017817  [   64/  146]
train() client id: f_00005-6-2 loss: 0.600365  [   96/  146]
train() client id: f_00005-6-3 loss: 0.637829  [  128/  146]
train() client id: f_00005-7-0 loss: 0.492403  [   32/  146]
train() client id: f_00005-7-1 loss: 0.691557  [   64/  146]
train() client id: f_00005-7-2 loss: 0.866238  [   96/  146]
train() client id: f_00005-7-3 loss: 0.844410  [  128/  146]
train() client id: f_00005-8-0 loss: 0.703413  [   32/  146]
train() client id: f_00005-8-1 loss: 0.888512  [   64/  146]
train() client id: f_00005-8-2 loss: 0.768826  [   96/  146]
train() client id: f_00005-8-3 loss: 0.769418  [  128/  146]
train() client id: f_00005-9-0 loss: 0.705147  [   32/  146]
train() client id: f_00005-9-1 loss: 0.814783  [   64/  146]
train() client id: f_00005-9-2 loss: 0.713600  [   96/  146]
train() client id: f_00005-9-3 loss: 0.804797  [  128/  146]
train() client id: f_00005-10-0 loss: 0.707298  [   32/  146]
train() client id: f_00005-10-1 loss: 0.893461  [   64/  146]
train() client id: f_00005-10-2 loss: 0.730256  [   96/  146]
train() client id: f_00005-10-3 loss: 0.648996  [  128/  146]
train() client id: f_00005-11-0 loss: 0.705968  [   32/  146]
train() client id: f_00005-11-1 loss: 0.776202  [   64/  146]
train() client id: f_00005-11-2 loss: 0.744607  [   96/  146]
train() client id: f_00005-11-3 loss: 0.593904  [  128/  146]
train() client id: f_00006-0-0 loss: 0.450023  [   32/   54]
train() client id: f_00006-1-0 loss: 0.461335  [   32/   54]
train() client id: f_00006-2-0 loss: 0.513460  [   32/   54]
train() client id: f_00006-3-0 loss: 0.481381  [   32/   54]
train() client id: f_00006-4-0 loss: 0.470791  [   32/   54]
train() client id: f_00006-5-0 loss: 0.543824  [   32/   54]
train() client id: f_00006-6-0 loss: 0.549882  [   32/   54]
train() client id: f_00006-7-0 loss: 0.528320  [   32/   54]
train() client id: f_00006-8-0 loss: 0.471886  [   32/   54]
train() client id: f_00006-9-0 loss: 0.509807  [   32/   54]
train() client id: f_00006-10-0 loss: 0.491036  [   32/   54]
train() client id: f_00006-11-0 loss: 0.436397  [   32/   54]
train() client id: f_00007-0-0 loss: 0.475249  [   32/  179]
train() client id: f_00007-0-1 loss: 0.786072  [   64/  179]
train() client id: f_00007-0-2 loss: 0.583788  [   96/  179]
train() client id: f_00007-0-3 loss: 0.779745  [  128/  179]
train() client id: f_00007-0-4 loss: 0.576066  [  160/  179]
train() client id: f_00007-1-0 loss: 0.545385  [   32/  179]
train() client id: f_00007-1-1 loss: 0.629498  [   64/  179]
train() client id: f_00007-1-2 loss: 0.475297  [   96/  179]
train() client id: f_00007-1-3 loss: 0.487688  [  128/  179]
train() client id: f_00007-1-4 loss: 0.822111  [  160/  179]
train() client id: f_00007-2-0 loss: 0.600248  [   32/  179]
train() client id: f_00007-2-1 loss: 0.498668  [   64/  179]
train() client id: f_00007-2-2 loss: 0.816574  [   96/  179]
train() client id: f_00007-2-3 loss: 0.572521  [  128/  179]
train() client id: f_00007-2-4 loss: 0.541223  [  160/  179]
train() client id: f_00007-3-0 loss: 0.490450  [   32/  179]
train() client id: f_00007-3-1 loss: 0.551366  [   64/  179]
train() client id: f_00007-3-2 loss: 0.580759  [   96/  179]
train() client id: f_00007-3-3 loss: 0.678516  [  128/  179]
train() client id: f_00007-3-4 loss: 0.608496  [  160/  179]
train() client id: f_00007-4-0 loss: 0.869622  [   32/  179]
train() client id: f_00007-4-1 loss: 0.486007  [   64/  179]
train() client id: f_00007-4-2 loss: 0.513528  [   96/  179]
train() client id: f_00007-4-3 loss: 0.601610  [  128/  179]
train() client id: f_00007-4-4 loss: 0.521344  [  160/  179]
train() client id: f_00007-5-0 loss: 0.786032  [   32/  179]
train() client id: f_00007-5-1 loss: 0.591065  [   64/  179]
train() client id: f_00007-5-2 loss: 0.448907  [   96/  179]
train() client id: f_00007-5-3 loss: 0.520458  [  128/  179]
train() client id: f_00007-5-4 loss: 0.569196  [  160/  179]
train() client id: f_00007-6-0 loss: 0.595507  [   32/  179]
train() client id: f_00007-6-1 loss: 0.526034  [   64/  179]
train() client id: f_00007-6-2 loss: 0.477479  [   96/  179]
train() client id: f_00007-6-3 loss: 0.670589  [  128/  179]
train() client id: f_00007-6-4 loss: 0.578444  [  160/  179]
train() client id: f_00007-7-0 loss: 0.475072  [   32/  179]
train() client id: f_00007-7-1 loss: 0.725124  [   64/  179]
train() client id: f_00007-7-2 loss: 0.789455  [   96/  179]
train() client id: f_00007-7-3 loss: 0.429774  [  128/  179]
train() client id: f_00007-7-4 loss: 0.518516  [  160/  179]
train() client id: f_00007-8-0 loss: 0.749333  [   32/  179]
train() client id: f_00007-8-1 loss: 0.392866  [   64/  179]
train() client id: f_00007-8-2 loss: 0.638688  [   96/  179]
train() client id: f_00007-8-3 loss: 0.428603  [  128/  179]
train() client id: f_00007-8-4 loss: 0.582811  [  160/  179]
train() client id: f_00007-9-0 loss: 0.453407  [   32/  179]
train() client id: f_00007-9-1 loss: 0.566472  [   64/  179]
train() client id: f_00007-9-2 loss: 0.737984  [   96/  179]
train() client id: f_00007-9-3 loss: 0.608882  [  128/  179]
train() client id: f_00007-9-4 loss: 0.631848  [  160/  179]
train() client id: f_00007-10-0 loss: 0.603732  [   32/  179]
train() client id: f_00007-10-1 loss: 0.602741  [   64/  179]
train() client id: f_00007-10-2 loss: 0.597347  [   96/  179]
train() client id: f_00007-10-3 loss: 0.512132  [  128/  179]
train() client id: f_00007-10-4 loss: 0.637506  [  160/  179]
train() client id: f_00007-11-0 loss: 0.636614  [   32/  179]
train() client id: f_00007-11-1 loss: 0.618551  [   64/  179]
train() client id: f_00007-11-2 loss: 0.542871  [   96/  179]
train() client id: f_00007-11-3 loss: 0.676485  [  128/  179]
train() client id: f_00007-11-4 loss: 0.526558  [  160/  179]
train() client id: f_00008-0-0 loss: 0.850977  [   32/  130]
train() client id: f_00008-0-1 loss: 0.784679  [   64/  130]
train() client id: f_00008-0-2 loss: 0.678993  [   96/  130]
train() client id: f_00008-0-3 loss: 0.668190  [  128/  130]
train() client id: f_00008-1-0 loss: 0.855718  [   32/  130]
train() client id: f_00008-1-1 loss: 0.699124  [   64/  130]
train() client id: f_00008-1-2 loss: 0.790187  [   96/  130]
train() client id: f_00008-1-3 loss: 0.630052  [  128/  130]
train() client id: f_00008-2-0 loss: 0.747801  [   32/  130]
train() client id: f_00008-2-1 loss: 0.751791  [   64/  130]
train() client id: f_00008-2-2 loss: 0.749906  [   96/  130]
train() client id: f_00008-2-3 loss: 0.704495  [  128/  130]
train() client id: f_00008-3-0 loss: 0.806273  [   32/  130]
train() client id: f_00008-3-1 loss: 0.795787  [   64/  130]
train() client id: f_00008-3-2 loss: 0.675061  [   96/  130]
train() client id: f_00008-3-3 loss: 0.694038  [  128/  130]
train() client id: f_00008-4-0 loss: 0.731702  [   32/  130]
train() client id: f_00008-4-1 loss: 0.774515  [   64/  130]
train() client id: f_00008-4-2 loss: 0.738200  [   96/  130]
train() client id: f_00008-4-3 loss: 0.755722  [  128/  130]
train() client id: f_00008-5-0 loss: 0.825783  [   32/  130]
train() client id: f_00008-5-1 loss: 0.776184  [   64/  130]
train() client id: f_00008-5-2 loss: 0.676337  [   96/  130]
train() client id: f_00008-5-3 loss: 0.698607  [  128/  130]
train() client id: f_00008-6-0 loss: 0.735165  [   32/  130]
train() client id: f_00008-6-1 loss: 0.738377  [   64/  130]
train() client id: f_00008-6-2 loss: 0.686106  [   96/  130]
train() client id: f_00008-6-3 loss: 0.843534  [  128/  130]
train() client id: f_00008-7-0 loss: 0.827823  [   32/  130]
train() client id: f_00008-7-1 loss: 0.671937  [   64/  130]
train() client id: f_00008-7-2 loss: 0.753115  [   96/  130]
train() client id: f_00008-7-3 loss: 0.748456  [  128/  130]
train() client id: f_00008-8-0 loss: 0.687431  [   32/  130]
train() client id: f_00008-8-1 loss: 0.713899  [   64/  130]
train() client id: f_00008-8-2 loss: 0.870495  [   96/  130]
train() client id: f_00008-8-3 loss: 0.726862  [  128/  130]
train() client id: f_00008-9-0 loss: 0.754564  [   32/  130]
train() client id: f_00008-9-1 loss: 0.711999  [   64/  130]
train() client id: f_00008-9-2 loss: 0.763951  [   96/  130]
train() client id: f_00008-9-3 loss: 0.762905  [  128/  130]
train() client id: f_00008-10-0 loss: 0.775794  [   32/  130]
train() client id: f_00008-10-1 loss: 0.737787  [   64/  130]
train() client id: f_00008-10-2 loss: 0.799239  [   96/  130]
train() client id: f_00008-10-3 loss: 0.668913  [  128/  130]
train() client id: f_00008-11-0 loss: 0.687997  [   32/  130]
train() client id: f_00008-11-1 loss: 0.801784  [   64/  130]
train() client id: f_00008-11-2 loss: 0.673088  [   96/  130]
train() client id: f_00008-11-3 loss: 0.832916  [  128/  130]
train() client id: f_00009-0-0 loss: 1.026569  [   32/  118]
train() client id: f_00009-0-1 loss: 1.119572  [   64/  118]
train() client id: f_00009-0-2 loss: 1.332918  [   96/  118]
train() client id: f_00009-1-0 loss: 1.193200  [   32/  118]
train() client id: f_00009-1-1 loss: 1.070921  [   64/  118]
train() client id: f_00009-1-2 loss: 1.038568  [   96/  118]
train() client id: f_00009-2-0 loss: 1.149361  [   32/  118]
train() client id: f_00009-2-1 loss: 0.979624  [   64/  118]
train() client id: f_00009-2-2 loss: 1.077264  [   96/  118]
train() client id: f_00009-3-0 loss: 1.051408  [   32/  118]
train() client id: f_00009-3-1 loss: 0.999282  [   64/  118]
train() client id: f_00009-3-2 loss: 1.069698  [   96/  118]
train() client id: f_00009-4-0 loss: 0.882501  [   32/  118]
train() client id: f_00009-4-1 loss: 1.113407  [   64/  118]
train() client id: f_00009-4-2 loss: 1.102733  [   96/  118]
train() client id: f_00009-5-0 loss: 0.972048  [   32/  118]
train() client id: f_00009-5-1 loss: 0.990951  [   64/  118]
train() client id: f_00009-5-2 loss: 0.994016  [   96/  118]
train() client id: f_00009-6-0 loss: 0.997308  [   32/  118]
train() client id: f_00009-6-1 loss: 0.987311  [   64/  118]
train() client id: f_00009-6-2 loss: 1.037903  [   96/  118]
train() client id: f_00009-7-0 loss: 0.903706  [   32/  118]
train() client id: f_00009-7-1 loss: 1.020374  [   64/  118]
train() client id: f_00009-7-2 loss: 1.080809  [   96/  118]
train() client id: f_00009-8-0 loss: 0.967371  [   32/  118]
train() client id: f_00009-8-1 loss: 1.018796  [   64/  118]
train() client id: f_00009-8-2 loss: 0.879265  [   96/  118]
train() client id: f_00009-9-0 loss: 0.814268  [   32/  118]
train() client id: f_00009-9-1 loss: 1.120895  [   64/  118]
train() client id: f_00009-9-2 loss: 0.893373  [   96/  118]
train() client id: f_00009-10-0 loss: 0.939884  [   32/  118]
train() client id: f_00009-10-1 loss: 0.854910  [   64/  118]
train() client id: f_00009-10-2 loss: 0.973352  [   96/  118]
train() client id: f_00009-11-0 loss: 1.102466  [   32/  118]
train() client id: f_00009-11-1 loss: 0.902618  [   64/  118]
train() client id: f_00009-11-2 loss: 0.787246  [   96/  118]
At round 37 accuracy: 0.6419098143236074
At round 37 training accuracy: 0.5888665325285044
At round 37 training loss: 0.8264630886844551
update_location
xs = -3.905658 4.200318 205.009024 18.811294 0.979296 3.956410 -167.443192 -146.324852 189.663977 -132.060879 
ys = 197.587959 180.555839 1.320614 -167.455176 159.350187 142.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -2.7114261681174354
ys mean: 53.3941425355287
dists_uav = 221.486469 206.441405 228.101828 195.946678 188.131446 174.389033 195.049002 177.233288 215.130412 165.698785 
uav_gains = -109.093156 -108.091264 -109.564280 -107.427541 -106.939127 -106.069172 -107.371372 -106.251655 -108.659792 -105.499473 
uav_gains_db_mean: -107.49668321596798
dists_bs = 172.578937 179.286422 417.817409 393.495064 176.673794 181.827736 177.785659 176.522289 397.195586 176.307292 
bs_gains = -102.202717 -102.666386 -112.954682 -112.225359 -102.487879 -102.837543 -102.564167 -102.477447 -112.339183 -102.462627 
bs_gains_db_mean: -105.52179902942211
Round 38
-------------------------------
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.97555343 12.35186937  5.88797184  2.12582729 14.24522634  6.85479284
  2.63358293  8.40017663  6.2003918   5.55926836]
obj_prev = 70.23466082817666
eta_min = 3.529677745883019e-16	eta_max = 0.9307450633473059
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 16.289713645049638	eta = 0.9090909090909091
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 30.212175618890367	eta = 0.49016101234197246
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 23.329211198156166	eta = 0.6347763094355793
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.083685449611664	eta = 0.6705778625672812
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.017241376491434	eta = 0.6726015459057763
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.017036233942203	eta = 0.6726078128344527
eta = 0.6726078128344527
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [0.03272589 0.06882827 0.03220643 0.01116836 0.07947717 0.03792048
 0.01402538 0.04649155 0.03376481 0.03064806]
ene_total = [1.98224589 3.51656478 1.97276077 0.93820235 4.0071535  2.08744961
 1.06916394 2.54721356 2.15142878 1.74485306]
ti_comp = [0.50594645 0.53574993 0.50279946 0.51575345 0.53633338 0.53518151
 0.51605241 0.52164089 0.47981336 0.53641515]
ti_coms = [0.10065732 0.07085384 0.10380431 0.09085032 0.07027039 0.07142226
 0.09055136 0.08496288 0.12679041 0.07018862]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [8.55747057e-06 7.09996571e-05 8.25882640e-06 3.27313532e-07
 1.09078086e-04 1.18986925e-05 6.47495169e-07 2.30812175e-05
 1.04502883e-05 6.25297698e-06]
ene_total = [0.46667354 0.33150672 0.48123759 0.42086309 0.33056794 0.33140208
 0.41949304 0.39464461 0.58781812 0.32542594]
optimize_network iter = 0 obj = 4.0896326721833045
eta = 0.6726078128344527
freqs = [32341261.25530905 64235440.52933019 32027116.84044752 10827227.71361229
 74093064.8966756  35427679.80345908 13589106.57226709 44562788.60807764
 35185358.32378028 28567479.83964483]
eta_min = 0.6726078128344555	eta_max = 0.6726078128344546
af = 0.00954296352207343	bf = 1.3351291140167194	zeta = 0.010497259874280773	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [2.05655253e-06 1.70628135e-05 1.98478162e-06 7.86607991e-08
 2.62139160e-05 2.85952327e-06 1.55607644e-07 5.54693540e-06
 2.51143918e-06 1.50273092e-06]
ene_total = [1.67955348 1.18486183 1.73204102 1.5156187  1.17665519 1.19197501
 1.51064412 1.41831402 2.11559328 1.17116856]
ti_comp = [0.50594645 0.53574993 0.50279946 0.51575345 0.53633338 0.53518151
 0.51605241 0.52164089 0.47981336 0.53641515]
ti_coms = [0.10065732 0.07085384 0.10380431 0.09085032 0.07027039 0.07142226
 0.09055136 0.08496288 0.12679041 0.07018862]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [8.55747057e-06 7.09996571e-05 8.25882640e-06 3.27313532e-07
 1.09078086e-04 1.18986925e-05 6.47495169e-07 2.30812175e-05
 1.04502883e-05 6.25297698e-06]
ene_total = [0.46667354 0.33150672 0.48123759 0.42086309 0.33056794 0.33140208
 0.41949304 0.39464461 0.58781812 0.32542594]
optimize_network iter = 1 obj = 4.0896326721833285
eta = 0.6726078128344546
freqs = [32341261.25530905 64235440.52933017 32027116.84044754 10827227.71361229
 74093064.89667559 35427679.80345907 13589106.57226709 44562788.60807764
 35185358.32378031 28567479.83964482]
Done!
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [7.90743628e-06 6.56064499e-05 7.63147743e-06 3.02450459e-07
 1.00792402e-04 1.09948555e-05 5.98310768e-07 2.13279445e-05
 9.65647363e-06 5.77799440e-06]
ene_total = [0.01007364 0.00715099 0.01038806 0.00908533 0.00712783 0.00715322
 0.00905573 0.00851762 0.0126887  0.00702464]
At round 38 energy consumption: 0.08826576524222389
At round 38 eta: 0.6726078128344546
At round 38 a_n: 15.165860671420319
At round 38 local rounds: 12.986465296587173
At round 38 global rounds: 46.323221096756726
gradient difference: 0.47131967544555664
train() client id: f_00000-0-0 loss: 1.056425  [   32/  126]
train() client id: f_00000-0-1 loss: 1.157530  [   64/  126]
train() client id: f_00000-0-2 loss: 1.023950  [   96/  126]
train() client id: f_00000-1-0 loss: 1.218495  [   32/  126]
train() client id: f_00000-1-1 loss: 0.752403  [   64/  126]
train() client id: f_00000-1-2 loss: 1.103403  [   96/  126]
train() client id: f_00000-2-0 loss: 0.949993  [   32/  126]
train() client id: f_00000-2-1 loss: 0.917826  [   64/  126]
train() client id: f_00000-2-2 loss: 1.059144  [   96/  126]
train() client id: f_00000-3-0 loss: 0.898974  [   32/  126]
train() client id: f_00000-3-1 loss: 1.028524  [   64/  126]
train() client id: f_00000-3-2 loss: 1.035643  [   96/  126]
train() client id: f_00000-4-0 loss: 0.857058  [   32/  126]
train() client id: f_00000-4-1 loss: 0.945908  [   64/  126]
train() client id: f_00000-4-2 loss: 0.947226  [   96/  126]
train() client id: f_00000-5-0 loss: 0.980160  [   32/  126]
train() client id: f_00000-5-1 loss: 0.794454  [   64/  126]
train() client id: f_00000-5-2 loss: 0.785789  [   96/  126]
train() client id: f_00000-6-0 loss: 0.906151  [   32/  126]
train() client id: f_00000-6-1 loss: 0.789007  [   64/  126]
train() client id: f_00000-6-2 loss: 0.845883  [   96/  126]
train() client id: f_00000-7-0 loss: 0.733632  [   32/  126]
train() client id: f_00000-7-1 loss: 0.842698  [   64/  126]
train() client id: f_00000-7-2 loss: 0.813806  [   96/  126]
train() client id: f_00000-8-0 loss: 0.918099  [   32/  126]
train() client id: f_00000-8-1 loss: 0.923099  [   64/  126]
train() client id: f_00000-8-2 loss: 0.617993  [   96/  126]
train() client id: f_00000-9-0 loss: 0.806235  [   32/  126]
train() client id: f_00000-9-1 loss: 0.849070  [   64/  126]
train() client id: f_00000-9-2 loss: 0.746430  [   96/  126]
train() client id: f_00000-10-0 loss: 0.792814  [   32/  126]
train() client id: f_00000-10-1 loss: 0.786268  [   64/  126]
train() client id: f_00000-10-2 loss: 0.928979  [   96/  126]
train() client id: f_00000-11-0 loss: 0.840463  [   32/  126]
train() client id: f_00000-11-1 loss: 0.734245  [   64/  126]
train() client id: f_00000-11-2 loss: 0.812209  [   96/  126]
train() client id: f_00001-0-0 loss: 0.378003  [   32/  265]
train() client id: f_00001-0-1 loss: 0.411703  [   64/  265]
train() client id: f_00001-0-2 loss: 0.308676  [   96/  265]
train() client id: f_00001-0-3 loss: 0.373034  [  128/  265]
train() client id: f_00001-0-4 loss: 0.333825  [  160/  265]
train() client id: f_00001-0-5 loss: 0.387378  [  192/  265]
train() client id: f_00001-0-6 loss: 0.350328  [  224/  265]
train() client id: f_00001-0-7 loss: 0.341573  [  256/  265]
train() client id: f_00001-1-0 loss: 0.309381  [   32/  265]
train() client id: f_00001-1-1 loss: 0.441141  [   64/  265]
train() client id: f_00001-1-2 loss: 0.515577  [   96/  265]
train() client id: f_00001-1-3 loss: 0.390804  [  128/  265]
train() client id: f_00001-1-4 loss: 0.272236  [  160/  265]
train() client id: f_00001-1-5 loss: 0.244601  [  192/  265]
train() client id: f_00001-1-6 loss: 0.266900  [  224/  265]
train() client id: f_00001-1-7 loss: 0.340577  [  256/  265]
train() client id: f_00001-2-0 loss: 0.373320  [   32/  265]
train() client id: f_00001-2-1 loss: 0.375644  [   64/  265]
train() client id: f_00001-2-2 loss: 0.280777  [   96/  265]
train() client id: f_00001-2-3 loss: 0.237077  [  128/  265]
train() client id: f_00001-2-4 loss: 0.503913  [  160/  265]
train() client id: f_00001-2-5 loss: 0.382654  [  192/  265]
train() client id: f_00001-2-6 loss: 0.257775  [  224/  265]
train() client id: f_00001-2-7 loss: 0.328920  [  256/  265]
train() client id: f_00001-3-0 loss: 0.354241  [   32/  265]
train() client id: f_00001-3-1 loss: 0.283075  [   64/  265]
train() client id: f_00001-3-2 loss: 0.252497  [   96/  265]
train() client id: f_00001-3-3 loss: 0.369500  [  128/  265]
train() client id: f_00001-3-4 loss: 0.243580  [  160/  265]
train() client id: f_00001-3-5 loss: 0.321615  [  192/  265]
train() client id: f_00001-3-6 loss: 0.465207  [  224/  265]
train() client id: f_00001-3-7 loss: 0.456578  [  256/  265]
train() client id: f_00001-4-0 loss: 0.379067  [   32/  265]
train() client id: f_00001-4-1 loss: 0.309601  [   64/  265]
train() client id: f_00001-4-2 loss: 0.291853  [   96/  265]
train() client id: f_00001-4-3 loss: 0.332084  [  128/  265]
train() client id: f_00001-4-4 loss: 0.524323  [  160/  265]
train() client id: f_00001-4-5 loss: 0.385403  [  192/  265]
train() client id: f_00001-4-6 loss: 0.244588  [  224/  265]
train() client id: f_00001-4-7 loss: 0.244160  [  256/  265]
train() client id: f_00001-5-0 loss: 0.336088  [   32/  265]
train() client id: f_00001-5-1 loss: 0.382297  [   64/  265]
train() client id: f_00001-5-2 loss: 0.401208  [   96/  265]
train() client id: f_00001-5-3 loss: 0.317644  [  128/  265]
train() client id: f_00001-5-4 loss: 0.266012  [  160/  265]
train() client id: f_00001-5-5 loss: 0.399648  [  192/  265]
train() client id: f_00001-5-6 loss: 0.277953  [  224/  265]
train() client id: f_00001-5-7 loss: 0.234906  [  256/  265]
train() client id: f_00001-6-0 loss: 0.327571  [   32/  265]
train() client id: f_00001-6-1 loss: 0.401761  [   64/  265]
train() client id: f_00001-6-2 loss: 0.287848  [   96/  265]
train() client id: f_00001-6-3 loss: 0.305386  [  128/  265]
train() client id: f_00001-6-4 loss: 0.230312  [  160/  265]
train() client id: f_00001-6-5 loss: 0.409716  [  192/  265]
train() client id: f_00001-6-6 loss: 0.422679  [  224/  265]
train() client id: f_00001-6-7 loss: 0.285123  [  256/  265]
train() client id: f_00001-7-0 loss: 0.342473  [   32/  265]
train() client id: f_00001-7-1 loss: 0.364083  [   64/  265]
train() client id: f_00001-7-2 loss: 0.406361  [   96/  265]
train() client id: f_00001-7-3 loss: 0.220721  [  128/  265]
train() client id: f_00001-7-4 loss: 0.249733  [  160/  265]
train() client id: f_00001-7-5 loss: 0.295322  [  192/  265]
train() client id: f_00001-7-6 loss: 0.304396  [  224/  265]
train() client id: f_00001-7-7 loss: 0.463630  [  256/  265]
train() client id: f_00001-8-0 loss: 0.317885  [   32/  265]
train() client id: f_00001-8-1 loss: 0.308353  [   64/  265]
train() client id: f_00001-8-2 loss: 0.315754  [   96/  265]
train() client id: f_00001-8-3 loss: 0.329297  [  128/  265]
train() client id: f_00001-8-4 loss: 0.403080  [  160/  265]
train() client id: f_00001-8-5 loss: 0.230158  [  192/  265]
train() client id: f_00001-8-6 loss: 0.225656  [  224/  265]
train() client id: f_00001-8-7 loss: 0.342779  [  256/  265]
train() client id: f_00001-9-0 loss: 0.316447  [   32/  265]
train() client id: f_00001-9-1 loss: 0.300278  [   64/  265]
train() client id: f_00001-9-2 loss: 0.483950  [   96/  265]
train() client id: f_00001-9-3 loss: 0.254251  [  128/  265]
train() client id: f_00001-9-4 loss: 0.373483  [  160/  265]
train() client id: f_00001-9-5 loss: 0.244847  [  192/  265]
train() client id: f_00001-9-6 loss: 0.340829  [  224/  265]
train() client id: f_00001-9-7 loss: 0.232981  [  256/  265]
train() client id: f_00001-10-0 loss: 0.400430  [   32/  265]
train() client id: f_00001-10-1 loss: 0.358374  [   64/  265]
train() client id: f_00001-10-2 loss: 0.333175  [   96/  265]
train() client id: f_00001-10-3 loss: 0.262322  [  128/  265]
train() client id: f_00001-10-4 loss: 0.402234  [  160/  265]
train() client id: f_00001-10-5 loss: 0.308819  [  192/  265]
train() client id: f_00001-10-6 loss: 0.232852  [  224/  265]
train() client id: f_00001-10-7 loss: 0.340257  [  256/  265]
train() client id: f_00001-11-0 loss: 0.242447  [   32/  265]
train() client id: f_00001-11-1 loss: 0.398853  [   64/  265]
train() client id: f_00001-11-2 loss: 0.320166  [   96/  265]
train() client id: f_00001-11-3 loss: 0.316060  [  128/  265]
train() client id: f_00001-11-4 loss: 0.468506  [  160/  265]
train() client id: f_00001-11-5 loss: 0.268771  [  192/  265]
train() client id: f_00001-11-6 loss: 0.237509  [  224/  265]
train() client id: f_00001-11-7 loss: 0.296317  [  256/  265]
train() client id: f_00002-0-0 loss: 1.029517  [   32/  124]
train() client id: f_00002-0-1 loss: 0.884180  [   64/  124]
train() client id: f_00002-0-2 loss: 1.136661  [   96/  124]
train() client id: f_00002-1-0 loss: 1.121964  [   32/  124]
train() client id: f_00002-1-1 loss: 0.875908  [   64/  124]
train() client id: f_00002-1-2 loss: 0.888251  [   96/  124]
train() client id: f_00002-2-0 loss: 1.037819  [   32/  124]
train() client id: f_00002-2-1 loss: 1.029407  [   64/  124]
train() client id: f_00002-2-2 loss: 0.934196  [   96/  124]
train() client id: f_00002-3-0 loss: 1.168708  [   32/  124]
train() client id: f_00002-3-1 loss: 0.928487  [   64/  124]
train() client id: f_00002-3-2 loss: 0.910933  [   96/  124]
train() client id: f_00002-4-0 loss: 0.844122  [   32/  124]
train() client id: f_00002-4-1 loss: 0.724548  [   64/  124]
train() client id: f_00002-4-2 loss: 1.112873  [   96/  124]
train() client id: f_00002-5-0 loss: 0.917593  [   32/  124]
train() client id: f_00002-5-1 loss: 0.832721  [   64/  124]
train() client id: f_00002-5-2 loss: 0.990415  [   96/  124]
train() client id: f_00002-6-0 loss: 0.875952  [   32/  124]
train() client id: f_00002-6-1 loss: 0.933328  [   64/  124]
train() client id: f_00002-6-2 loss: 0.843290  [   96/  124]
train() client id: f_00002-7-0 loss: 0.792307  [   32/  124]
train() client id: f_00002-7-1 loss: 0.929973  [   64/  124]
train() client id: f_00002-7-2 loss: 0.948021  [   96/  124]
train() client id: f_00002-8-0 loss: 1.031112  [   32/  124]
train() client id: f_00002-8-1 loss: 1.042237  [   64/  124]
train() client id: f_00002-8-2 loss: 0.772978  [   96/  124]
train() client id: f_00002-9-0 loss: 0.934036  [   32/  124]
train() client id: f_00002-9-1 loss: 0.934432  [   64/  124]
train() client id: f_00002-9-2 loss: 0.990662  [   96/  124]
train() client id: f_00002-10-0 loss: 0.793414  [   32/  124]
train() client id: f_00002-10-1 loss: 1.004467  [   64/  124]
train() client id: f_00002-10-2 loss: 0.929234  [   96/  124]
train() client id: f_00002-11-0 loss: 0.829246  [   32/  124]
train() client id: f_00002-11-1 loss: 0.692817  [   64/  124]
train() client id: f_00002-11-2 loss: 1.365386  [   96/  124]
train() client id: f_00003-0-0 loss: 0.882093  [   32/   43]
train() client id: f_00003-1-0 loss: 0.891296  [   32/   43]
train() client id: f_00003-2-0 loss: 0.935415  [   32/   43]
train() client id: f_00003-3-0 loss: 0.804119  [   32/   43]
train() client id: f_00003-4-0 loss: 0.813369  [   32/   43]
train() client id: f_00003-5-0 loss: 0.831728  [   32/   43]
train() client id: f_00003-6-0 loss: 0.870024  [   32/   43]
train() client id: f_00003-7-0 loss: 0.698460  [   32/   43]
train() client id: f_00003-8-0 loss: 0.929007  [   32/   43]
train() client id: f_00003-9-0 loss: 0.877715  [   32/   43]
train() client id: f_00003-10-0 loss: 0.765845  [   32/   43]
train() client id: f_00003-11-0 loss: 0.825580  [   32/   43]
train() client id: f_00004-0-0 loss: 0.703355  [   32/  306]
train() client id: f_00004-0-1 loss: 0.722269  [   64/  306]
train() client id: f_00004-0-2 loss: 0.704203  [   96/  306]
train() client id: f_00004-0-3 loss: 0.851390  [  128/  306]
train() client id: f_00004-0-4 loss: 1.004484  [  160/  306]
train() client id: f_00004-0-5 loss: 0.944240  [  192/  306]
train() client id: f_00004-0-6 loss: 0.766923  [  224/  306]
train() client id: f_00004-0-7 loss: 0.787076  [  256/  306]
train() client id: f_00004-0-8 loss: 0.959177  [  288/  306]
train() client id: f_00004-1-0 loss: 1.080072  [   32/  306]
train() client id: f_00004-1-1 loss: 0.838909  [   64/  306]
train() client id: f_00004-1-2 loss: 0.747144  [   96/  306]
train() client id: f_00004-1-3 loss: 0.835389  [  128/  306]
train() client id: f_00004-1-4 loss: 0.853106  [  160/  306]
train() client id: f_00004-1-5 loss: 0.754500  [  192/  306]
train() client id: f_00004-1-6 loss: 0.773834  [  224/  306]
train() client id: f_00004-1-7 loss: 0.873727  [  256/  306]
train() client id: f_00004-1-8 loss: 0.742853  [  288/  306]
train() client id: f_00004-2-0 loss: 0.794623  [   32/  306]
train() client id: f_00004-2-1 loss: 0.839154  [   64/  306]
train() client id: f_00004-2-2 loss: 0.805537  [   96/  306]
train() client id: f_00004-2-3 loss: 0.801581  [  128/  306]
train() client id: f_00004-2-4 loss: 0.835160  [  160/  306]
train() client id: f_00004-2-5 loss: 0.831038  [  192/  306]
train() client id: f_00004-2-6 loss: 0.779735  [  224/  306]
train() client id: f_00004-2-7 loss: 0.902616  [  256/  306]
train() client id: f_00004-2-8 loss: 0.934739  [  288/  306]
train() client id: f_00004-3-0 loss: 0.914085  [   32/  306]
train() client id: f_00004-3-1 loss: 0.796013  [   64/  306]
train() client id: f_00004-3-2 loss: 0.804191  [   96/  306]
train() client id: f_00004-3-3 loss: 0.841049  [  128/  306]
train() client id: f_00004-3-4 loss: 0.813028  [  160/  306]
train() client id: f_00004-3-5 loss: 0.737468  [  192/  306]
train() client id: f_00004-3-6 loss: 0.871463  [  224/  306]
train() client id: f_00004-3-7 loss: 0.880041  [  256/  306]
train() client id: f_00004-3-8 loss: 0.911517  [  288/  306]
train() client id: f_00004-4-0 loss: 0.770110  [   32/  306]
train() client id: f_00004-4-1 loss: 0.758753  [   64/  306]
train() client id: f_00004-4-2 loss: 0.708839  [   96/  306]
train() client id: f_00004-4-3 loss: 0.918558  [  128/  306]
train() client id: f_00004-4-4 loss: 0.962929  [  160/  306]
train() client id: f_00004-4-5 loss: 0.938648  [  192/  306]
train() client id: f_00004-4-6 loss: 0.775575  [  224/  306]
train() client id: f_00004-4-7 loss: 0.923029  [  256/  306]
train() client id: f_00004-4-8 loss: 0.822711  [  288/  306]
train() client id: f_00004-5-0 loss: 0.828275  [   32/  306]
train() client id: f_00004-5-1 loss: 0.869845  [   64/  306]
train() client id: f_00004-5-2 loss: 0.803468  [   96/  306]
train() client id: f_00004-5-3 loss: 0.873881  [  128/  306]
train() client id: f_00004-5-4 loss: 0.879435  [  160/  306]
train() client id: f_00004-5-5 loss: 0.821858  [  192/  306]
train() client id: f_00004-5-6 loss: 0.728310  [  224/  306]
train() client id: f_00004-5-7 loss: 0.877687  [  256/  306]
train() client id: f_00004-5-8 loss: 0.828217  [  288/  306]
train() client id: f_00004-6-0 loss: 0.914810  [   32/  306]
train() client id: f_00004-6-1 loss: 0.756490  [   64/  306]
train() client id: f_00004-6-2 loss: 0.937990  [   96/  306]
train() client id: f_00004-6-3 loss: 0.961897  [  128/  306]
train() client id: f_00004-6-4 loss: 0.802855  [  160/  306]
train() client id: f_00004-6-5 loss: 0.767018  [  192/  306]
train() client id: f_00004-6-6 loss: 0.764903  [  224/  306]
train() client id: f_00004-6-7 loss: 0.766395  [  256/  306]
train() client id: f_00004-6-8 loss: 0.870694  [  288/  306]
train() client id: f_00004-7-0 loss: 0.733569  [   32/  306]
train() client id: f_00004-7-1 loss: 0.834627  [   64/  306]
train() client id: f_00004-7-2 loss: 0.967091  [   96/  306]
train() client id: f_00004-7-3 loss: 0.827522  [  128/  306]
train() client id: f_00004-7-4 loss: 0.891411  [  160/  306]
train() client id: f_00004-7-5 loss: 0.774214  [  192/  306]
train() client id: f_00004-7-6 loss: 0.872430  [  224/  306]
train() client id: f_00004-7-7 loss: 0.880777  [  256/  306]
train() client id: f_00004-7-8 loss: 0.766935  [  288/  306]
train() client id: f_00004-8-0 loss: 0.968571  [   32/  306]
train() client id: f_00004-8-1 loss: 0.874995  [   64/  306]
train() client id: f_00004-8-2 loss: 0.834570  [   96/  306]
train() client id: f_00004-8-3 loss: 0.812400  [  128/  306]
train() client id: f_00004-8-4 loss: 0.738557  [  160/  306]
train() client id: f_00004-8-5 loss: 0.763292  [  192/  306]
train() client id: f_00004-8-6 loss: 0.730094  [  224/  306]
train() client id: f_00004-8-7 loss: 0.887982  [  256/  306]
train() client id: f_00004-8-8 loss: 0.863651  [  288/  306]
train() client id: f_00004-9-0 loss: 0.890928  [   32/  306]
train() client id: f_00004-9-1 loss: 0.815247  [   64/  306]
train() client id: f_00004-9-2 loss: 0.709630  [   96/  306]
train() client id: f_00004-9-3 loss: 0.938666  [  128/  306]
train() client id: f_00004-9-4 loss: 0.889690  [  160/  306]
train() client id: f_00004-9-5 loss: 0.842512  [  192/  306]
train() client id: f_00004-9-6 loss: 0.768958  [  224/  306]
train() client id: f_00004-9-7 loss: 0.743410  [  256/  306]
train() client id: f_00004-9-8 loss: 0.949722  [  288/  306]
train() client id: f_00004-10-0 loss: 0.791175  [   32/  306]
train() client id: f_00004-10-1 loss: 0.851546  [   64/  306]
train() client id: f_00004-10-2 loss: 0.862792  [   96/  306]
train() client id: f_00004-10-3 loss: 0.781303  [  128/  306]
train() client id: f_00004-10-4 loss: 0.746948  [  160/  306]
train() client id: f_00004-10-5 loss: 1.027886  [  192/  306]
train() client id: f_00004-10-6 loss: 0.932395  [  224/  306]
train() client id: f_00004-10-7 loss: 0.820028  [  256/  306]
train() client id: f_00004-10-8 loss: 0.798616  [  288/  306]
train() client id: f_00004-11-0 loss: 0.824418  [   32/  306]
train() client id: f_00004-11-1 loss: 0.784967  [   64/  306]
train() client id: f_00004-11-2 loss: 0.788582  [   96/  306]
train() client id: f_00004-11-3 loss: 0.872948  [  128/  306]
train() client id: f_00004-11-4 loss: 0.940101  [  160/  306]
train() client id: f_00004-11-5 loss: 0.816574  [  192/  306]
train() client id: f_00004-11-6 loss: 0.868535  [  224/  306]
train() client id: f_00004-11-7 loss: 0.874242  [  256/  306]
train() client id: f_00004-11-8 loss: 0.728546  [  288/  306]
train() client id: f_00005-0-0 loss: 0.614956  [   32/  146]
train() client id: f_00005-0-1 loss: 0.998614  [   64/  146]
train() client id: f_00005-0-2 loss: 0.775705  [   96/  146]
train() client id: f_00005-0-3 loss: 0.674364  [  128/  146]
train() client id: f_00005-1-0 loss: 0.634867  [   32/  146]
train() client id: f_00005-1-1 loss: 0.589000  [   64/  146]
train() client id: f_00005-1-2 loss: 0.840707  [   96/  146]
train() client id: f_00005-1-3 loss: 0.853017  [  128/  146]
train() client id: f_00005-2-0 loss: 0.763742  [   32/  146]
train() client id: f_00005-2-1 loss: 0.690934  [   64/  146]
train() client id: f_00005-2-2 loss: 0.826069  [   96/  146]
train() client id: f_00005-2-3 loss: 0.822077  [  128/  146]
train() client id: f_00005-3-0 loss: 0.780028  [   32/  146]
train() client id: f_00005-3-1 loss: 0.965004  [   64/  146]
train() client id: f_00005-3-2 loss: 0.842924  [   96/  146]
train() client id: f_00005-3-3 loss: 0.631315  [  128/  146]
train() client id: f_00005-4-0 loss: 0.804898  [   32/  146]
train() client id: f_00005-4-1 loss: 0.916777  [   64/  146]
train() client id: f_00005-4-2 loss: 0.659146  [   96/  146]
train() client id: f_00005-4-3 loss: 0.652876  [  128/  146]
train() client id: f_00005-5-0 loss: 1.061945  [   32/  146]
train() client id: f_00005-5-1 loss: 0.631654  [   64/  146]
train() client id: f_00005-5-2 loss: 0.731026  [   96/  146]
train() client id: f_00005-5-3 loss: 0.750276  [  128/  146]
train() client id: f_00005-6-0 loss: 0.647853  [   32/  146]
train() client id: f_00005-6-1 loss: 0.742603  [   64/  146]
train() client id: f_00005-6-2 loss: 0.814862  [   96/  146]
train() client id: f_00005-6-3 loss: 0.627587  [  128/  146]
train() client id: f_00005-7-0 loss: 0.574432  [   32/  146]
train() client id: f_00005-7-1 loss: 0.684704  [   64/  146]
train() client id: f_00005-7-2 loss: 0.803565  [   96/  146]
train() client id: f_00005-7-3 loss: 1.070228  [  128/  146]
train() client id: f_00005-8-0 loss: 0.736065  [   32/  146]
train() client id: f_00005-8-1 loss: 0.770794  [   64/  146]
train() client id: f_00005-8-2 loss: 0.901784  [   96/  146]
train() client id: f_00005-8-3 loss: 0.796689  [  128/  146]
train() client id: f_00005-9-0 loss: 0.867593  [   32/  146]
train() client id: f_00005-9-1 loss: 0.794179  [   64/  146]
train() client id: f_00005-9-2 loss: 0.767958  [   96/  146]
train() client id: f_00005-9-3 loss: 0.892006  [  128/  146]
train() client id: f_00005-10-0 loss: 0.880390  [   32/  146]
train() client id: f_00005-10-1 loss: 0.794339  [   64/  146]
train() client id: f_00005-10-2 loss: 0.712057  [   96/  146]
train() client id: f_00005-10-3 loss: 0.731283  [  128/  146]
train() client id: f_00005-11-0 loss: 0.987162  [   32/  146]
train() client id: f_00005-11-1 loss: 0.707438  [   64/  146]
train() client id: f_00005-11-2 loss: 0.820498  [   96/  146]
train() client id: f_00005-11-3 loss: 0.673241  [  128/  146]
train() client id: f_00006-0-0 loss: 0.594356  [   32/   54]
train() client id: f_00006-1-0 loss: 0.531584  [   32/   54]
train() client id: f_00006-2-0 loss: 0.475038  [   32/   54]
train() client id: f_00006-3-0 loss: 0.556472  [   32/   54]
train() client id: f_00006-4-0 loss: 0.546476  [   32/   54]
train() client id: f_00006-5-0 loss: 0.545618  [   32/   54]
train() client id: f_00006-6-0 loss: 0.583517  [   32/   54]
train() client id: f_00006-7-0 loss: 0.479284  [   32/   54]
train() client id: f_00006-8-0 loss: 0.573347  [   32/   54]
train() client id: f_00006-9-0 loss: 0.498208  [   32/   54]
train() client id: f_00006-10-0 loss: 0.482100  [   32/   54]
train() client id: f_00006-11-0 loss: 0.532013  [   32/   54]
train() client id: f_00007-0-0 loss: 0.603547  [   32/  179]
train() client id: f_00007-0-1 loss: 0.698309  [   64/  179]
train() client id: f_00007-0-2 loss: 0.643621  [   96/  179]
train() client id: f_00007-0-3 loss: 0.543468  [  128/  179]
train() client id: f_00007-0-4 loss: 0.622570  [  160/  179]
train() client id: f_00007-1-0 loss: 0.544797  [   32/  179]
train() client id: f_00007-1-1 loss: 0.548367  [   64/  179]
train() client id: f_00007-1-2 loss: 0.581757  [   96/  179]
train() client id: f_00007-1-3 loss: 0.589046  [  128/  179]
train() client id: f_00007-1-4 loss: 0.547938  [  160/  179]
train() client id: f_00007-2-0 loss: 0.656525  [   32/  179]
train() client id: f_00007-2-1 loss: 0.679707  [   64/  179]
train() client id: f_00007-2-2 loss: 0.560694  [   96/  179]
train() client id: f_00007-2-3 loss: 0.464632  [  128/  179]
train() client id: f_00007-2-4 loss: 0.668972  [  160/  179]
train() client id: f_00007-3-0 loss: 0.454726  [   32/  179]
train() client id: f_00007-3-1 loss: 0.679481  [   64/  179]
train() client id: f_00007-3-2 loss: 0.555136  [   96/  179]
train() client id: f_00007-3-3 loss: 0.600211  [  128/  179]
train() client id: f_00007-3-4 loss: 0.533146  [  160/  179]
train() client id: f_00007-4-0 loss: 0.781944  [   32/  179]
train() client id: f_00007-4-1 loss: 0.444950  [   64/  179]
train() client id: f_00007-4-2 loss: 0.752102  [   96/  179]
train() client id: f_00007-4-3 loss: 0.444117  [  128/  179]
train() client id: f_00007-4-4 loss: 0.476202  [  160/  179]
train() client id: f_00007-5-0 loss: 0.441252  [   32/  179]
train() client id: f_00007-5-1 loss: 0.623379  [   64/  179]
train() client id: f_00007-5-2 loss: 0.798121  [   96/  179]
train() client id: f_00007-5-3 loss: 0.499288  [  128/  179]
train() client id: f_00007-5-4 loss: 0.522087  [  160/  179]
train() client id: f_00007-6-0 loss: 0.435192  [   32/  179]
train() client id: f_00007-6-1 loss: 0.573669  [   64/  179]
train() client id: f_00007-6-2 loss: 0.488654  [   96/  179]
train() client id: f_00007-6-3 loss: 0.431808  [  128/  179]
train() client id: f_00007-6-4 loss: 0.761029  [  160/  179]
train() client id: f_00007-7-0 loss: 0.533831  [   32/  179]
train() client id: f_00007-7-1 loss: 0.658500  [   64/  179]
train() client id: f_00007-7-2 loss: 0.522041  [   96/  179]
train() client id: f_00007-7-3 loss: 0.673211  [  128/  179]
train() client id: f_00007-7-4 loss: 0.516692  [  160/  179]
train() client id: f_00007-8-0 loss: 0.459917  [   32/  179]
train() client id: f_00007-8-1 loss: 0.522788  [   64/  179]
train() client id: f_00007-8-2 loss: 0.528345  [   96/  179]
train() client id: f_00007-8-3 loss: 0.753312  [  128/  179]
train() client id: f_00007-8-4 loss: 0.477899  [  160/  179]
train() client id: f_00007-9-0 loss: 0.374014  [   32/  179]
train() client id: f_00007-9-1 loss: 0.749661  [   64/  179]
train() client id: f_00007-9-2 loss: 0.776223  [   96/  179]
train() client id: f_00007-9-3 loss: 0.599995  [  128/  179]
train() client id: f_00007-9-4 loss: 0.391534  [  160/  179]
train() client id: f_00007-10-0 loss: 0.573118  [   32/  179]
train() client id: f_00007-10-1 loss: 0.396594  [   64/  179]
train() client id: f_00007-10-2 loss: 0.597911  [   96/  179]
train() client id: f_00007-10-3 loss: 0.464469  [  128/  179]
train() client id: f_00007-10-4 loss: 0.695990  [  160/  179]
train() client id: f_00007-11-0 loss: 0.490827  [   32/  179]
train() client id: f_00007-11-1 loss: 0.402957  [   64/  179]
train() client id: f_00007-11-2 loss: 0.599203  [   96/  179]
train() client id: f_00007-11-3 loss: 0.633931  [  128/  179]
train() client id: f_00007-11-4 loss: 0.492276  [  160/  179]
train() client id: f_00008-0-0 loss: 0.627029  [   32/  130]
train() client id: f_00008-0-1 loss: 0.670961  [   64/  130]
train() client id: f_00008-0-2 loss: 0.688189  [   96/  130]
train() client id: f_00008-0-3 loss: 0.710091  [  128/  130]
train() client id: f_00008-1-0 loss: 0.586002  [   32/  130]
train() client id: f_00008-1-1 loss: 0.691211  [   64/  130]
train() client id: f_00008-1-2 loss: 0.739154  [   96/  130]
train() client id: f_00008-1-3 loss: 0.645333  [  128/  130]
train() client id: f_00008-2-0 loss: 0.836067  [   32/  130]
train() client id: f_00008-2-1 loss: 0.633270  [   64/  130]
train() client id: f_00008-2-2 loss: 0.634383  [   96/  130]
train() client id: f_00008-2-3 loss: 0.594856  [  128/  130]
train() client id: f_00008-3-0 loss: 0.606722  [   32/  130]
train() client id: f_00008-3-1 loss: 0.750214  [   64/  130]
train() client id: f_00008-3-2 loss: 0.718716  [   96/  130]
train() client id: f_00008-3-3 loss: 0.605661  [  128/  130]
train() client id: f_00008-4-0 loss: 0.687658  [   32/  130]
train() client id: f_00008-4-1 loss: 0.658747  [   64/  130]
train() client id: f_00008-4-2 loss: 0.622779  [   96/  130]
train() client id: f_00008-4-3 loss: 0.723038  [  128/  130]
train() client id: f_00008-5-0 loss: 0.708251  [   32/  130]
train() client id: f_00008-5-1 loss: 0.640103  [   64/  130]
train() client id: f_00008-5-2 loss: 0.742376  [   96/  130]
train() client id: f_00008-5-3 loss: 0.602454  [  128/  130]
train() client id: f_00008-6-0 loss: 0.637136  [   32/  130]
train() client id: f_00008-6-1 loss: 0.608538  [   64/  130]
train() client id: f_00008-6-2 loss: 0.785865  [   96/  130]
train() client id: f_00008-6-3 loss: 0.655815  [  128/  130]
train() client id: f_00008-7-0 loss: 0.602946  [   32/  130]
train() client id: f_00008-7-1 loss: 0.727809  [   64/  130]
train() client id: f_00008-7-2 loss: 0.650407  [   96/  130]
train() client id: f_00008-7-3 loss: 0.646481  [  128/  130]
train() client id: f_00008-8-0 loss: 0.758897  [   32/  130]
train() client id: f_00008-8-1 loss: 0.680167  [   64/  130]
train() client id: f_00008-8-2 loss: 0.596502  [   96/  130]
train() client id: f_00008-8-3 loss: 0.628646  [  128/  130]
train() client id: f_00008-9-0 loss: 0.663005  [   32/  130]
train() client id: f_00008-9-1 loss: 0.763121  [   64/  130]
train() client id: f_00008-9-2 loss: 0.673509  [   96/  130]
train() client id: f_00008-9-3 loss: 0.559655  [  128/  130]
train() client id: f_00008-10-0 loss: 0.722093  [   32/  130]
train() client id: f_00008-10-1 loss: 0.724259  [   64/  130]
train() client id: f_00008-10-2 loss: 0.603430  [   96/  130]
train() client id: f_00008-10-3 loss: 0.641263  [  128/  130]
train() client id: f_00008-11-0 loss: 0.629349  [   32/  130]
train() client id: f_00008-11-1 loss: 0.655316  [   64/  130]
train() client id: f_00008-11-2 loss: 0.643554  [   96/  130]
train() client id: f_00008-11-3 loss: 0.728161  [  128/  130]
train() client id: f_00009-0-0 loss: 1.113332  [   32/  118]
train() client id: f_00009-0-1 loss: 1.226467  [   64/  118]
train() client id: f_00009-0-2 loss: 1.142732  [   96/  118]
train() client id: f_00009-1-0 loss: 1.179867  [   32/  118]
train() client id: f_00009-1-1 loss: 1.053226  [   64/  118]
train() client id: f_00009-1-2 loss: 1.038460  [   96/  118]
train() client id: f_00009-2-0 loss: 1.109987  [   32/  118]
train() client id: f_00009-2-1 loss: 1.027575  [   64/  118]
train() client id: f_00009-2-2 loss: 0.967704  [   96/  118]
train() client id: f_00009-3-0 loss: 1.032635  [   32/  118]
train() client id: f_00009-3-1 loss: 1.083275  [   64/  118]
train() client id: f_00009-3-2 loss: 0.908418  [   96/  118]
train() client id: f_00009-4-0 loss: 0.934035  [   32/  118]
train() client id: f_00009-4-1 loss: 1.195234  [   64/  118]
train() client id: f_00009-4-2 loss: 0.962732  [   96/  118]
train() client id: f_00009-5-0 loss: 1.096323  [   32/  118]
train() client id: f_00009-5-1 loss: 1.160857  [   64/  118]
train() client id: f_00009-5-2 loss: 0.842897  [   96/  118]
train() client id: f_00009-6-0 loss: 0.924255  [   32/  118]
train() client id: f_00009-6-1 loss: 0.799426  [   64/  118]
train() client id: f_00009-6-2 loss: 1.074319  [   96/  118]
train() client id: f_00009-7-0 loss: 1.020475  [   32/  118]
train() client id: f_00009-7-1 loss: 0.814015  [   64/  118]
train() client id: f_00009-7-2 loss: 1.019209  [   96/  118]
train() client id: f_00009-8-0 loss: 0.908232  [   32/  118]
train() client id: f_00009-8-1 loss: 1.098259  [   64/  118]
train() client id: f_00009-8-2 loss: 0.928719  [   96/  118]
train() client id: f_00009-9-0 loss: 0.969814  [   32/  118]
train() client id: f_00009-9-1 loss: 0.912844  [   64/  118]
train() client id: f_00009-9-2 loss: 1.065073  [   96/  118]
train() client id: f_00009-10-0 loss: 0.979712  [   32/  118]
train() client id: f_00009-10-1 loss: 1.005496  [   64/  118]
train() client id: f_00009-10-2 loss: 0.864998  [   96/  118]
train() client id: f_00009-11-0 loss: 0.993825  [   32/  118]
train() client id: f_00009-11-1 loss: 1.050455  [   64/  118]
train() client id: f_00009-11-2 loss: 0.899618  [   96/  118]
At round 38 accuracy: 0.6419098143236074
At round 38 training accuracy: 0.5861837692823608
At round 38 training loss: 0.8378634615700711
update_location
xs = -3.905658 4.200318 210.009024 18.811294 0.979296 3.956410 -172.443192 -151.324852 194.663977 -137.060879 
ys = 202.587959 185.555839 1.320614 -172.455176 164.350187 147.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -3.2114261681174354
ys mean: 54.8941425355287
dists_uav = 225.958261 210.828395 232.605963 200.236491 192.384882 178.506796 199.357832 181.383260 219.551210 169.710625 
uav_gains = -109.409174 -108.375383 -109.898343 -107.696901 -107.204881 -106.332848 -107.641568 -106.515236 -108.959362 -105.764996 
uav_gains_db_mean: -107.77986909361044
dists_bs = 173.304268 179.510946 422.370072 397.854140 176.301251 181.009577 177.643385 175.779314 401.790958 175.156702 
bs_gains = -102.253718 -102.681605 -113.086467 -112.359328 -102.462210 -102.782703 -102.554432 -102.426157 -112.479064 -102.383008 
bs_gains_db_mean: -105.5468692213133
Round 39
-------------------------------
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.84371614 12.07295727  5.75849455  2.07999855 13.92335007  6.69969194
  2.5762787   8.21232374  6.06245015  5.43331894]
obj_prev = 68.66258004963703
eta_min = 1.603615121775097e-16	eta_max = 0.9314544727787696
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 15.921783734685466	eta = 0.909090909090909
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 29.685339818895304	eta = 0.4875924930628838
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 22.863992502600777	eta = 0.6330630509114977
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.629256357976853	eta = 0.6692023345673616
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.563057577188566	eta = 0.6712567917560257
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.562850936268624	eta = 0.6712632245380996
eta = 0.6712632245380996
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [0.03289102 0.06917555 0.03236894 0.01122471 0.07987818 0.03811181
 0.01409615 0.04672613 0.03393518 0.0308027 ]
ene_total = [1.94642197 3.43896482 1.93822068 0.92238666 3.91833323 2.03975789
 1.05049566 2.49573328 2.10825374 1.704283  ]
ti_comp = [0.51931345 0.55115743 0.51591145 0.5297508  0.55187418 0.5508223
 0.53005444 0.53584373 0.4938656  0.55212944]
ti_coms = [0.102748   0.07090402 0.10615    0.09231065 0.07018727 0.07123916
 0.09200701 0.08621772 0.12819586 0.06993201]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.24617860e-06 6.81061808e-05 7.96370126e-06 3.14965047e-07
 1.04588604e-04 1.14034376e-05 6.23075349e-07 2.22066489e-05
 1.00141191e-05 5.99189852e-06]
ene_total = [0.4636805  0.32278962 0.47900796 0.41625902 0.32120273 0.32174399
 0.41490377 0.38977208 0.57850904 0.31560581]
optimize_network iter = 0 obj = 4.023474514862622
eta = 0.6712632245380996
freqs = [31667787.3789384  62754800.17478311 31370631.5489173  10594331.96949695
 72369920.453618   34595378.85521331 13296888.56956221 43600515.27273497
 34356690.70712458 27894452.74815087]
eta_min = 0.6712632245381114	eta_max = 0.6712632245380962
af = 0.008905236727720691	bf = 1.3191144648464428	zeta = 0.009795760400492761	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [1.97179310e-06 1.62852764e-05 1.90424825e-06 7.53131768e-08
 2.50088069e-05 2.72674418e-06 1.48987275e-07 5.30996467e-06
 2.39453594e-06 1.43275870e-06]
ene_total = [1.67569409 1.15879127 1.73115483 1.50519706 1.14852664 1.1620451
 1.50025809 1.40670146 2.09070673 1.14052021]
ti_comp = [0.51931345 0.55115743 0.51591145 0.5297508  0.55187418 0.5508223
 0.53005444 0.53584373 0.4938656  0.55212944]
ti_coms = [0.102748   0.07090402 0.10615    0.09231065 0.07018727 0.07123916
 0.09200701 0.08621772 0.12819586 0.06993201]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.24617860e-06 6.81061808e-05 7.96370126e-06 3.14965047e-07
 1.04588604e-04 1.14034376e-05 6.23075349e-07 2.22066489e-05
 1.00141191e-05 5.99189852e-06]
ene_total = [0.4636805  0.32278962 0.47900796 0.41625902 0.32120273 0.32174399
 0.41490377 0.38977208 0.57850904 0.31560581]
optimize_network iter = 1 obj = 4.0234745148625795
eta = 0.6712632245380962
freqs = [31667787.37893841 62754800.17478317 31370631.54891731 10594331.96949696
 72369920.4536181  34595378.85521334 13296888.56956222 43600515.27273501
 34356690.70712457 27894452.7481509 ]
Done!
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.21333132e-06 6.78348912e-05 7.93197918e-06 3.13710436e-07
 1.04171993e-04 1.13580139e-05 6.20593432e-07 2.21181924e-05
 9.97422956e-06 5.96803079e-06]
ene_total = [0.01028301 0.00715824 0.01062293 0.00923138 0.0071229  0.00713527
 0.00920132 0.00864389 0.01282956 0.00699917]
At round 39 energy consumption: 0.08922767374894407
At round 39 eta: 0.6712632245380962
At round 39 a_n: 14.823314824451002
At round 39 local rounds: 13.051990433476096
At round 39 global rounds: 45.091744918477566
gradient difference: 0.42710453271865845
train() client id: f_00000-0-0 loss: 1.187493  [   32/  126]
train() client id: f_00000-0-1 loss: 1.312939  [   64/  126]
train() client id: f_00000-0-2 loss: 0.983251  [   96/  126]
train() client id: f_00000-1-0 loss: 1.184787  [   32/  126]
train() client id: f_00000-1-1 loss: 1.029486  [   64/  126]
train() client id: f_00000-1-2 loss: 0.980725  [   96/  126]
train() client id: f_00000-2-0 loss: 1.095669  [   32/  126]
train() client id: f_00000-2-1 loss: 0.998792  [   64/  126]
train() client id: f_00000-2-2 loss: 0.963807  [   96/  126]
train() client id: f_00000-3-0 loss: 0.994780  [   32/  126]
train() client id: f_00000-3-1 loss: 0.957023  [   64/  126]
train() client id: f_00000-3-2 loss: 1.000166  [   96/  126]
train() client id: f_00000-4-0 loss: 0.840094  [   32/  126]
train() client id: f_00000-4-1 loss: 0.896292  [   64/  126]
train() client id: f_00000-4-2 loss: 0.942368  [   96/  126]
train() client id: f_00000-5-0 loss: 1.087256  [   32/  126]
train() client id: f_00000-5-1 loss: 0.826705  [   64/  126]
train() client id: f_00000-5-2 loss: 0.884811  [   96/  126]
train() client id: f_00000-6-0 loss: 0.923213  [   32/  126]
train() client id: f_00000-6-1 loss: 0.968458  [   64/  126]
train() client id: f_00000-6-2 loss: 0.831154  [   96/  126]
train() client id: f_00000-7-0 loss: 0.889152  [   32/  126]
train() client id: f_00000-7-1 loss: 0.770670  [   64/  126]
train() client id: f_00000-7-2 loss: 0.930824  [   96/  126]
train() client id: f_00000-8-0 loss: 0.700423  [   32/  126]
train() client id: f_00000-8-1 loss: 0.979532  [   64/  126]
train() client id: f_00000-8-2 loss: 1.046481  [   96/  126]
train() client id: f_00000-9-0 loss: 0.770981  [   32/  126]
train() client id: f_00000-9-1 loss: 0.970197  [   64/  126]
train() client id: f_00000-9-2 loss: 0.858880  [   96/  126]
train() client id: f_00000-10-0 loss: 0.888721  [   32/  126]
train() client id: f_00000-10-1 loss: 0.905020  [   64/  126]
train() client id: f_00000-10-2 loss: 0.776307  [   96/  126]
train() client id: f_00000-11-0 loss: 0.809098  [   32/  126]
train() client id: f_00000-11-1 loss: 0.950116  [   64/  126]
train() client id: f_00000-11-2 loss: 0.862906  [   96/  126]
train() client id: f_00000-12-0 loss: 0.866820  [   32/  126]
train() client id: f_00000-12-1 loss: 0.821943  [   64/  126]
train() client id: f_00000-12-2 loss: 0.852578  [   96/  126]
train() client id: f_00001-0-0 loss: 0.405890  [   32/  265]
train() client id: f_00001-0-1 loss: 0.432676  [   64/  265]
train() client id: f_00001-0-2 loss: 0.517051  [   96/  265]
train() client id: f_00001-0-3 loss: 0.483667  [  128/  265]
train() client id: f_00001-0-4 loss: 0.487159  [  160/  265]
train() client id: f_00001-0-5 loss: 0.575302  [  192/  265]
train() client id: f_00001-0-6 loss: 0.392989  [  224/  265]
train() client id: f_00001-0-7 loss: 0.442323  [  256/  265]
train() client id: f_00001-1-0 loss: 0.534445  [   32/  265]
train() client id: f_00001-1-1 loss: 0.523025  [   64/  265]
train() client id: f_00001-1-2 loss: 0.511949  [   96/  265]
train() client id: f_00001-1-3 loss: 0.371103  [  128/  265]
train() client id: f_00001-1-4 loss: 0.423102  [  160/  265]
train() client id: f_00001-1-5 loss: 0.441082  [  192/  265]
train() client id: f_00001-1-6 loss: 0.472989  [  224/  265]
train() client id: f_00001-1-7 loss: 0.401780  [  256/  265]
train() client id: f_00001-2-0 loss: 0.530318  [   32/  265]
train() client id: f_00001-2-1 loss: 0.414634  [   64/  265]
train() client id: f_00001-2-2 loss: 0.459558  [   96/  265]
train() client id: f_00001-2-3 loss: 0.403788  [  128/  265]
train() client id: f_00001-2-4 loss: 0.489362  [  160/  265]
train() client id: f_00001-2-5 loss: 0.379015  [  192/  265]
train() client id: f_00001-2-6 loss: 0.492410  [  224/  265]
train() client id: f_00001-2-7 loss: 0.375752  [  256/  265]
train() client id: f_00001-3-0 loss: 0.432532  [   32/  265]
train() client id: f_00001-3-1 loss: 0.517129  [   64/  265]
train() client id: f_00001-3-2 loss: 0.385630  [   96/  265]
train() client id: f_00001-3-3 loss: 0.391445  [  128/  265]
train() client id: f_00001-3-4 loss: 0.446404  [  160/  265]
train() client id: f_00001-3-5 loss: 0.372506  [  192/  265]
train() client id: f_00001-3-6 loss: 0.539623  [  224/  265]
train() client id: f_00001-3-7 loss: 0.465166  [  256/  265]
train() client id: f_00001-4-0 loss: 0.481270  [   32/  265]
train() client id: f_00001-4-1 loss: 0.463206  [   64/  265]
train() client id: f_00001-4-2 loss: 0.630966  [   96/  265]
train() client id: f_00001-4-3 loss: 0.412870  [  128/  265]
train() client id: f_00001-4-4 loss: 0.372558  [  160/  265]
train() client id: f_00001-4-5 loss: 0.371362  [  192/  265]
train() client id: f_00001-4-6 loss: 0.383256  [  224/  265]
train() client id: f_00001-4-7 loss: 0.459986  [  256/  265]
train() client id: f_00001-5-0 loss: 0.385424  [   32/  265]
train() client id: f_00001-5-1 loss: 0.459411  [   64/  265]
train() client id: f_00001-5-2 loss: 0.475171  [   96/  265]
train() client id: f_00001-5-3 loss: 0.509402  [  128/  265]
train() client id: f_00001-5-4 loss: 0.487586  [  160/  265]
train() client id: f_00001-5-5 loss: 0.441136  [  192/  265]
train() client id: f_00001-5-6 loss: 0.391432  [  224/  265]
train() client id: f_00001-5-7 loss: 0.416693  [  256/  265]
train() client id: f_00001-6-0 loss: 0.472434  [   32/  265]
train() client id: f_00001-6-1 loss: 0.344870  [   64/  265]
train() client id: f_00001-6-2 loss: 0.486304  [   96/  265]
train() client id: f_00001-6-3 loss: 0.455632  [  128/  265]
train() client id: f_00001-6-4 loss: 0.445550  [  160/  265]
train() client id: f_00001-6-5 loss: 0.455114  [  192/  265]
train() client id: f_00001-6-6 loss: 0.458407  [  224/  265]
train() client id: f_00001-6-7 loss: 0.361182  [  256/  265]
train() client id: f_00001-7-0 loss: 0.413390  [   32/  265]
train() client id: f_00001-7-1 loss: 0.483812  [   64/  265]
train() client id: f_00001-7-2 loss: 0.507098  [   96/  265]
train() client id: f_00001-7-3 loss: 0.406995  [  128/  265]
train() client id: f_00001-7-4 loss: 0.500516  [  160/  265]
train() client id: f_00001-7-5 loss: 0.378820  [  192/  265]
train() client id: f_00001-7-6 loss: 0.414724  [  224/  265]
train() client id: f_00001-7-7 loss: 0.442960  [  256/  265]
train() client id: f_00001-8-0 loss: 0.453058  [   32/  265]
train() client id: f_00001-8-1 loss: 0.408228  [   64/  265]
train() client id: f_00001-8-2 loss: 0.541748  [   96/  265]
train() client id: f_00001-8-3 loss: 0.377403  [  128/  265]
train() client id: f_00001-8-4 loss: 0.435897  [  160/  265]
train() client id: f_00001-8-5 loss: 0.469820  [  192/  265]
train() client id: f_00001-8-6 loss: 0.396957  [  224/  265]
train() client id: f_00001-8-7 loss: 0.354136  [  256/  265]
train() client id: f_00001-9-0 loss: 0.510137  [   32/  265]
train() client id: f_00001-9-1 loss: 0.552003  [   64/  265]
train() client id: f_00001-9-2 loss: 0.344212  [   96/  265]
train() client id: f_00001-9-3 loss: 0.393140  [  128/  265]
train() client id: f_00001-9-4 loss: 0.527771  [  160/  265]
train() client id: f_00001-9-5 loss: 0.355494  [  192/  265]
train() client id: f_00001-9-6 loss: 0.388622  [  224/  265]
train() client id: f_00001-9-7 loss: 0.483254  [  256/  265]
train() client id: f_00001-10-0 loss: 0.351627  [   32/  265]
train() client id: f_00001-10-1 loss: 0.455810  [   64/  265]
train() client id: f_00001-10-2 loss: 0.445625  [   96/  265]
train() client id: f_00001-10-3 loss: 0.344807  [  128/  265]
train() client id: f_00001-10-4 loss: 0.570897  [  160/  265]
train() client id: f_00001-10-5 loss: 0.350891  [  192/  265]
train() client id: f_00001-10-6 loss: 0.516640  [  224/  265]
train() client id: f_00001-10-7 loss: 0.505431  [  256/  265]
train() client id: f_00001-11-0 loss: 0.506931  [   32/  265]
train() client id: f_00001-11-1 loss: 0.495077  [   64/  265]
train() client id: f_00001-11-2 loss: 0.349604  [   96/  265]
train() client id: f_00001-11-3 loss: 0.497258  [  128/  265]
train() client id: f_00001-11-4 loss: 0.441503  [  160/  265]
train() client id: f_00001-11-5 loss: 0.360312  [  192/  265]
train() client id: f_00001-11-6 loss: 0.525846  [  224/  265]
train() client id: f_00001-11-7 loss: 0.366826  [  256/  265]
train() client id: f_00001-12-0 loss: 0.435489  [   32/  265]
train() client id: f_00001-12-1 loss: 0.523023  [   64/  265]
train() client id: f_00001-12-2 loss: 0.549882  [   96/  265]
train() client id: f_00001-12-3 loss: 0.406884  [  128/  265]
train() client id: f_00001-12-4 loss: 0.416213  [  160/  265]
train() client id: f_00001-12-5 loss: 0.355495  [  192/  265]
train() client id: f_00001-12-6 loss: 0.353018  [  224/  265]
train() client id: f_00001-12-7 loss: 0.434401  [  256/  265]
train() client id: f_00002-0-0 loss: 0.808564  [   32/  124]
train() client id: f_00002-0-1 loss: 0.723582  [   64/  124]
train() client id: f_00002-0-2 loss: 0.690742  [   96/  124]
train() client id: f_00002-1-0 loss: 0.732113  [   32/  124]
train() client id: f_00002-1-1 loss: 0.654530  [   64/  124]
train() client id: f_00002-1-2 loss: 0.633817  [   96/  124]
train() client id: f_00002-2-0 loss: 0.752048  [   32/  124]
train() client id: f_00002-2-1 loss: 0.810093  [   64/  124]
train() client id: f_00002-2-2 loss: 0.440433  [   96/  124]
train() client id: f_00002-3-0 loss: 0.623343  [   32/  124]
train() client id: f_00002-3-1 loss: 0.489421  [   64/  124]
train() client id: f_00002-3-2 loss: 0.508389  [   96/  124]
train() client id: f_00002-4-0 loss: 0.884778  [   32/  124]
train() client id: f_00002-4-1 loss: 0.533821  [   64/  124]
train() client id: f_00002-4-2 loss: 0.429758  [   96/  124]
train() client id: f_00002-5-0 loss: 0.367970  [   32/  124]
train() client id: f_00002-5-1 loss: 0.526235  [   64/  124]
train() client id: f_00002-5-2 loss: 0.627433  [   96/  124]
train() client id: f_00002-6-0 loss: 0.523014  [   32/  124]
train() client id: f_00002-6-1 loss: 0.642716  [   64/  124]
train() client id: f_00002-6-2 loss: 0.300288  [   96/  124]
train() client id: f_00002-7-0 loss: 0.428598  [   32/  124]
train() client id: f_00002-7-1 loss: 0.461498  [   64/  124]
train() client id: f_00002-7-2 loss: 0.538691  [   96/  124]
train() client id: f_00002-8-0 loss: 0.607628  [   32/  124]
train() client id: f_00002-8-1 loss: 0.549951  [   64/  124]
train() client id: f_00002-8-2 loss: 0.330719  [   96/  124]
train() client id: f_00002-9-0 loss: 0.649603  [   32/  124]
train() client id: f_00002-9-1 loss: 0.685054  [   64/  124]
train() client id: f_00002-9-2 loss: 0.343874  [   96/  124]
train() client id: f_00002-10-0 loss: 0.488108  [   32/  124]
train() client id: f_00002-10-1 loss: 0.723710  [   64/  124]
train() client id: f_00002-10-2 loss: 0.379462  [   96/  124]
train() client id: f_00002-11-0 loss: 0.424440  [   32/  124]
train() client id: f_00002-11-1 loss: 0.349613  [   64/  124]
train() client id: f_00002-11-2 loss: 0.483058  [   96/  124]
train() client id: f_00002-12-0 loss: 0.341740  [   32/  124]
train() client id: f_00002-12-1 loss: 0.473345  [   64/  124]
train() client id: f_00002-12-2 loss: 0.421277  [   96/  124]
train() client id: f_00003-0-0 loss: 0.761697  [   32/   43]
train() client id: f_00003-1-0 loss: 0.882881  [   32/   43]
train() client id: f_00003-2-0 loss: 0.553573  [   32/   43]
train() client id: f_00003-3-0 loss: 0.701576  [   32/   43]
train() client id: f_00003-4-0 loss: 0.607129  [   32/   43]
train() client id: f_00003-5-0 loss: 0.744371  [   32/   43]
train() client id: f_00003-6-0 loss: 0.770099  [   32/   43]
train() client id: f_00003-7-0 loss: 0.673438  [   32/   43]
train() client id: f_00003-8-0 loss: 0.692998  [   32/   43]
train() client id: f_00003-9-0 loss: 0.496091  [   32/   43]
train() client id: f_00003-10-0 loss: 0.745451  [   32/   43]
train() client id: f_00003-11-0 loss: 0.702295  [   32/   43]
train() client id: f_00003-12-0 loss: 0.770369  [   32/   43]
train() client id: f_00004-0-0 loss: 1.032027  [   32/  306]
train() client id: f_00004-0-1 loss: 1.073921  [   64/  306]
train() client id: f_00004-0-2 loss: 0.772969  [   96/  306]
train() client id: f_00004-0-3 loss: 1.008585  [  128/  306]
train() client id: f_00004-0-4 loss: 1.084071  [  160/  306]
train() client id: f_00004-0-5 loss: 0.954916  [  192/  306]
train() client id: f_00004-0-6 loss: 0.924235  [  224/  306]
train() client id: f_00004-0-7 loss: 0.917055  [  256/  306]
train() client id: f_00004-0-8 loss: 0.942791  [  288/  306]
train() client id: f_00004-1-0 loss: 1.061991  [   32/  306]
train() client id: f_00004-1-1 loss: 0.896728  [   64/  306]
train() client id: f_00004-1-2 loss: 0.900365  [   96/  306]
train() client id: f_00004-1-3 loss: 0.945476  [  128/  306]
train() client id: f_00004-1-4 loss: 1.034577  [  160/  306]
train() client id: f_00004-1-5 loss: 0.980898  [  192/  306]
train() client id: f_00004-1-6 loss: 0.960824  [  224/  306]
train() client id: f_00004-1-7 loss: 0.956393  [  256/  306]
train() client id: f_00004-1-8 loss: 0.953674  [  288/  306]
train() client id: f_00004-2-0 loss: 0.807883  [   32/  306]
train() client id: f_00004-2-1 loss: 0.979353  [   64/  306]
train() client id: f_00004-2-2 loss: 1.038817  [   96/  306]
train() client id: f_00004-2-3 loss: 0.900111  [  128/  306]
train() client id: f_00004-2-4 loss: 0.926736  [  160/  306]
train() client id: f_00004-2-5 loss: 1.148107  [  192/  306]
train() client id: f_00004-2-6 loss: 0.886321  [  224/  306]
train() client id: f_00004-2-7 loss: 0.918413  [  256/  306]
train() client id: f_00004-2-8 loss: 1.057591  [  288/  306]
train() client id: f_00004-3-0 loss: 1.056403  [   32/  306]
train() client id: f_00004-3-1 loss: 0.936281  [   64/  306]
train() client id: f_00004-3-2 loss: 0.897182  [   96/  306]
train() client id: f_00004-3-3 loss: 1.096915  [  128/  306]
train() client id: f_00004-3-4 loss: 1.038651  [  160/  306]
train() client id: f_00004-3-5 loss: 0.952671  [  192/  306]
train() client id: f_00004-3-6 loss: 0.964937  [  224/  306]
train() client id: f_00004-3-7 loss: 0.870112  [  256/  306]
train() client id: f_00004-3-8 loss: 0.867329  [  288/  306]
train() client id: f_00004-4-0 loss: 1.091220  [   32/  306]
train() client id: f_00004-4-1 loss: 0.892768  [   64/  306]
train() client id: f_00004-4-2 loss: 1.067987  [   96/  306]
train() client id: f_00004-4-3 loss: 0.941545  [  128/  306]
train() client id: f_00004-4-4 loss: 0.913430  [  160/  306]
train() client id: f_00004-4-5 loss: 0.931735  [  192/  306]
train() client id: f_00004-4-6 loss: 0.954392  [  224/  306]
train() client id: f_00004-4-7 loss: 0.837046  [  256/  306]
train() client id: f_00004-4-8 loss: 1.016020  [  288/  306]
train() client id: f_00004-5-0 loss: 0.959016  [   32/  306]
train() client id: f_00004-5-1 loss: 0.936186  [   64/  306]
train() client id: f_00004-5-2 loss: 0.952480  [   96/  306]
train() client id: f_00004-5-3 loss: 0.986389  [  128/  306]
train() client id: f_00004-5-4 loss: 0.904395  [  160/  306]
train() client id: f_00004-5-5 loss: 1.111592  [  192/  306]
train() client id: f_00004-5-6 loss: 0.977016  [  224/  306]
train() client id: f_00004-5-7 loss: 0.917012  [  256/  306]
train() client id: f_00004-5-8 loss: 0.881438  [  288/  306]
train() client id: f_00004-6-0 loss: 1.019663  [   32/  306]
train() client id: f_00004-6-1 loss: 1.000679  [   64/  306]
train() client id: f_00004-6-2 loss: 0.954209  [   96/  306]
train() client id: f_00004-6-3 loss: 0.948181  [  128/  306]
train() client id: f_00004-6-4 loss: 0.853211  [  160/  306]
train() client id: f_00004-6-5 loss: 0.891126  [  192/  306]
train() client id: f_00004-6-6 loss: 0.940993  [  224/  306]
train() client id: f_00004-6-7 loss: 0.884228  [  256/  306]
train() client id: f_00004-6-8 loss: 0.992637  [  288/  306]
train() client id: f_00004-7-0 loss: 0.951790  [   32/  306]
train() client id: f_00004-7-1 loss: 0.966373  [   64/  306]
train() client id: f_00004-7-2 loss: 0.817195  [   96/  306]
train() client id: f_00004-7-3 loss: 0.927430  [  128/  306]
train() client id: f_00004-7-4 loss: 0.936091  [  160/  306]
train() client id: f_00004-7-5 loss: 0.900713  [  192/  306]
train() client id: f_00004-7-6 loss: 1.114641  [  224/  306]
train() client id: f_00004-7-7 loss: 1.011315  [  256/  306]
train() client id: f_00004-7-8 loss: 0.950409  [  288/  306]
train() client id: f_00004-8-0 loss: 1.071085  [   32/  306]
train() client id: f_00004-8-1 loss: 0.865869  [   64/  306]
train() client id: f_00004-8-2 loss: 0.958848  [   96/  306]
train() client id: f_00004-8-3 loss: 0.898601  [  128/  306]
train() client id: f_00004-8-4 loss: 0.932630  [  160/  306]
train() client id: f_00004-8-5 loss: 0.908550  [  192/  306]
train() client id: f_00004-8-6 loss: 0.961965  [  224/  306]
train() client id: f_00004-8-7 loss: 0.979402  [  256/  306]
train() client id: f_00004-8-8 loss: 0.920265  [  288/  306]
train() client id: f_00004-9-0 loss: 0.890214  [   32/  306]
train() client id: f_00004-9-1 loss: 1.015494  [   64/  306]
train() client id: f_00004-9-2 loss: 0.922427  [   96/  306]
train() client id: f_00004-9-3 loss: 0.974593  [  128/  306]
train() client id: f_00004-9-4 loss: 1.069170  [  160/  306]
train() client id: f_00004-9-5 loss: 0.869394  [  192/  306]
train() client id: f_00004-9-6 loss: 0.912180  [  224/  306]
train() client id: f_00004-9-7 loss: 0.953029  [  256/  306]
train() client id: f_00004-9-8 loss: 0.822797  [  288/  306]
train() client id: f_00004-10-0 loss: 0.833154  [   32/  306]
train() client id: f_00004-10-1 loss: 1.099452  [   64/  306]
train() client id: f_00004-10-2 loss: 0.871513  [   96/  306]
train() client id: f_00004-10-3 loss: 0.810882  [  128/  306]
train() client id: f_00004-10-4 loss: 0.902337  [  160/  306]
train() client id: f_00004-10-5 loss: 1.036285  [  192/  306]
train() client id: f_00004-10-6 loss: 0.987784  [  224/  306]
train() client id: f_00004-10-7 loss: 1.041876  [  256/  306]
train() client id: f_00004-10-8 loss: 0.868950  [  288/  306]
train() client id: f_00004-11-0 loss: 0.856614  [   32/  306]
train() client id: f_00004-11-1 loss: 0.942882  [   64/  306]
train() client id: f_00004-11-2 loss: 1.091071  [   96/  306]
train() client id: f_00004-11-3 loss: 0.791783  [  128/  306]
train() client id: f_00004-11-4 loss: 0.827983  [  160/  306]
train() client id: f_00004-11-5 loss: 0.992861  [  192/  306]
train() client id: f_00004-11-6 loss: 0.981421  [  224/  306]
train() client id: f_00004-11-7 loss: 0.824520  [  256/  306]
train() client id: f_00004-11-8 loss: 1.103163  [  288/  306]
train() client id: f_00004-12-0 loss: 1.064175  [   32/  306]
train() client id: f_00004-12-1 loss: 0.794708  [   64/  306]
train() client id: f_00004-12-2 loss: 0.985445  [   96/  306]
train() client id: f_00004-12-3 loss: 0.901060  [  128/  306]
train() client id: f_00004-12-4 loss: 1.012561  [  160/  306]
train() client id: f_00004-12-5 loss: 0.971480  [  192/  306]
train() client id: f_00004-12-6 loss: 0.908444  [  224/  306]
train() client id: f_00004-12-7 loss: 0.917793  [  256/  306]
train() client id: f_00004-12-8 loss: 0.872711  [  288/  306]
train() client id: f_00005-0-0 loss: 0.963738  [   32/  146]
train() client id: f_00005-0-1 loss: 0.796479  [   64/  146]
train() client id: f_00005-0-2 loss: 0.689668  [   96/  146]
train() client id: f_00005-0-3 loss: 0.592834  [  128/  146]
train() client id: f_00005-1-0 loss: 0.726927  [   32/  146]
train() client id: f_00005-1-1 loss: 0.710119  [   64/  146]
train() client id: f_00005-1-2 loss: 0.799525  [   96/  146]
train() client id: f_00005-1-3 loss: 0.582659  [  128/  146]
train() client id: f_00005-2-0 loss: 0.917745  [   32/  146]
train() client id: f_00005-2-1 loss: 0.717352  [   64/  146]
train() client id: f_00005-2-2 loss: 0.643267  [   96/  146]
train() client id: f_00005-2-3 loss: 0.709945  [  128/  146]
train() client id: f_00005-3-0 loss: 0.626368  [   32/  146]
train() client id: f_00005-3-1 loss: 0.707901  [   64/  146]
train() client id: f_00005-3-2 loss: 0.696474  [   96/  146]
train() client id: f_00005-3-3 loss: 0.855898  [  128/  146]
train() client id: f_00005-4-0 loss: 0.710482  [   32/  146]
train() client id: f_00005-4-1 loss: 0.697176  [   64/  146]
train() client id: f_00005-4-2 loss: 0.596771  [   96/  146]
train() client id: f_00005-4-3 loss: 0.941270  [  128/  146]
train() client id: f_00005-5-0 loss: 0.519346  [   32/  146]
train() client id: f_00005-5-1 loss: 0.776133  [   64/  146]
train() client id: f_00005-5-2 loss: 0.548486  [   96/  146]
train() client id: f_00005-5-3 loss: 0.758925  [  128/  146]
train() client id: f_00005-6-0 loss: 0.564923  [   32/  146]
train() client id: f_00005-6-1 loss: 0.716141  [   64/  146]
train() client id: f_00005-6-2 loss: 0.876224  [   96/  146]
train() client id: f_00005-6-3 loss: 0.688645  [  128/  146]
train() client id: f_00005-7-0 loss: 0.583129  [   32/  146]
train() client id: f_00005-7-1 loss: 0.549514  [   64/  146]
train() client id: f_00005-7-2 loss: 0.746193  [   96/  146]
train() client id: f_00005-7-3 loss: 0.853170  [  128/  146]
train() client id: f_00005-8-0 loss: 0.878710  [   32/  146]
train() client id: f_00005-8-1 loss: 0.644534  [   64/  146]
train() client id: f_00005-8-2 loss: 0.662055  [   96/  146]
train() client id: f_00005-8-3 loss: 0.695678  [  128/  146]
train() client id: f_00005-9-0 loss: 0.879015  [   32/  146]
train() client id: f_00005-9-1 loss: 0.445726  [   64/  146]
train() client id: f_00005-9-2 loss: 0.617738  [   96/  146]
train() client id: f_00005-9-3 loss: 0.772807  [  128/  146]
train() client id: f_00005-10-0 loss: 0.590715  [   32/  146]
train() client id: f_00005-10-1 loss: 0.785740  [   64/  146]
train() client id: f_00005-10-2 loss: 0.895982  [   96/  146]
train() client id: f_00005-10-3 loss: 0.688254  [  128/  146]
train() client id: f_00005-11-0 loss: 0.763592  [   32/  146]
train() client id: f_00005-11-1 loss: 0.672908  [   64/  146]
train() client id: f_00005-11-2 loss: 0.697163  [   96/  146]
train() client id: f_00005-11-3 loss: 0.814144  [  128/  146]
train() client id: f_00005-12-0 loss: 0.770244  [   32/  146]
train() client id: f_00005-12-1 loss: 0.631815  [   64/  146]
train() client id: f_00005-12-2 loss: 0.666568  [   96/  146]
train() client id: f_00005-12-3 loss: 0.967165  [  128/  146]
train() client id: f_00006-0-0 loss: 0.497926  [   32/   54]
train() client id: f_00006-1-0 loss: 0.451595  [   32/   54]
train() client id: f_00006-2-0 loss: 0.532734  [   32/   54]
train() client id: f_00006-3-0 loss: 0.515001  [   32/   54]
train() client id: f_00006-4-0 loss: 0.556511  [   32/   54]
train() client id: f_00006-5-0 loss: 0.561837  [   32/   54]
train() client id: f_00006-6-0 loss: 0.483815  [   32/   54]
train() client id: f_00006-7-0 loss: 0.543054  [   32/   54]
train() client id: f_00006-8-0 loss: 0.488271  [   32/   54]
train() client id: f_00006-9-0 loss: 0.541836  [   32/   54]
train() client id: f_00006-10-0 loss: 0.448826  [   32/   54]
train() client id: f_00006-11-0 loss: 0.503799  [   32/   54]
train() client id: f_00006-12-0 loss: 0.498781  [   32/   54]
train() client id: f_00007-0-0 loss: 0.653800  [   32/  179]
train() client id: f_00007-0-1 loss: 0.604201  [   64/  179]
train() client id: f_00007-0-2 loss: 0.671005  [   96/  179]
train() client id: f_00007-0-3 loss: 0.724760  [  128/  179]
train() client id: f_00007-0-4 loss: 0.792994  [  160/  179]
train() client id: f_00007-1-0 loss: 0.532994  [   32/  179]
train() client id: f_00007-1-1 loss: 0.615008  [   64/  179]
train() client id: f_00007-1-2 loss: 0.661406  [   96/  179]
train() client id: f_00007-1-3 loss: 0.621478  [  128/  179]
train() client id: f_00007-1-4 loss: 0.789458  [  160/  179]
train() client id: f_00007-2-0 loss: 0.755390  [   32/  179]
train() client id: f_00007-2-1 loss: 0.667988  [   64/  179]
train() client id: f_00007-2-2 loss: 0.686568  [   96/  179]
train() client id: f_00007-2-3 loss: 0.509953  [  128/  179]
train() client id: f_00007-2-4 loss: 0.664685  [  160/  179]
train() client id: f_00007-3-0 loss: 0.818129  [   32/  179]
train() client id: f_00007-3-1 loss: 0.670154  [   64/  179]
train() client id: f_00007-3-2 loss: 0.573837  [   96/  179]
train() client id: f_00007-3-3 loss: 0.646899  [  128/  179]
train() client id: f_00007-3-4 loss: 0.585493  [  160/  179]
train() client id: f_00007-4-0 loss: 0.591841  [   32/  179]
train() client id: f_00007-4-1 loss: 0.896401  [   64/  179]
train() client id: f_00007-4-2 loss: 0.586067  [   96/  179]
train() client id: f_00007-4-3 loss: 0.525284  [  128/  179]
train() client id: f_00007-4-4 loss: 0.496859  [  160/  179]
train() client id: f_00007-5-0 loss: 0.658385  [   32/  179]
train() client id: f_00007-5-1 loss: 0.655054  [   64/  179]
train() client id: f_00007-5-2 loss: 0.595743  [   96/  179]
train() client id: f_00007-5-3 loss: 0.725613  [  128/  179]
train() client id: f_00007-5-4 loss: 0.567143  [  160/  179]
train() client id: f_00007-6-0 loss: 0.746629  [   32/  179]
train() client id: f_00007-6-1 loss: 0.663855  [   64/  179]
train() client id: f_00007-6-2 loss: 0.524818  [   96/  179]
train() client id: f_00007-6-3 loss: 0.679930  [  128/  179]
train() client id: f_00007-6-4 loss: 0.552977  [  160/  179]
train() client id: f_00007-7-0 loss: 0.540247  [   32/  179]
train() client id: f_00007-7-1 loss: 0.806974  [   64/  179]
train() client id: f_00007-7-2 loss: 0.568229  [   96/  179]
train() client id: f_00007-7-3 loss: 0.669688  [  128/  179]
train() client id: f_00007-7-4 loss: 0.533273  [  160/  179]
train() client id: f_00007-8-0 loss: 0.531280  [   32/  179]
train() client id: f_00007-8-1 loss: 0.708434  [   64/  179]
train() client id: f_00007-8-2 loss: 0.493946  [   96/  179]
train() client id: f_00007-8-3 loss: 0.815382  [  128/  179]
train() client id: f_00007-8-4 loss: 0.468795  [  160/  179]
train() client id: f_00007-9-0 loss: 0.455096  [   32/  179]
train() client id: f_00007-9-1 loss: 0.521459  [   64/  179]
train() client id: f_00007-9-2 loss: 0.566681  [   96/  179]
train() client id: f_00007-9-3 loss: 0.588858  [  128/  179]
train() client id: f_00007-9-4 loss: 0.908829  [  160/  179]
train() client id: f_00007-10-0 loss: 0.441491  [   32/  179]
train() client id: f_00007-10-1 loss: 0.794118  [   64/  179]
train() client id: f_00007-10-2 loss: 0.636230  [   96/  179]
train() client id: f_00007-10-3 loss: 0.568440  [  128/  179]
train() client id: f_00007-10-4 loss: 0.570413  [  160/  179]
train() client id: f_00007-11-0 loss: 0.778536  [   32/  179]
train() client id: f_00007-11-1 loss: 0.582098  [   64/  179]
train() client id: f_00007-11-2 loss: 0.506132  [   96/  179]
train() client id: f_00007-11-3 loss: 0.620597  [  128/  179]
train() client id: f_00007-11-4 loss: 0.562581  [  160/  179]
train() client id: f_00007-12-0 loss: 0.974397  [   32/  179]
train() client id: f_00007-12-1 loss: 0.453824  [   64/  179]
train() client id: f_00007-12-2 loss: 0.459201  [   96/  179]
train() client id: f_00007-12-3 loss: 0.672192  [  128/  179]
train() client id: f_00007-12-4 loss: 0.557598  [  160/  179]
train() client id: f_00008-0-0 loss: 0.706394  [   32/  130]
train() client id: f_00008-0-1 loss: 0.634221  [   64/  130]
train() client id: f_00008-0-2 loss: 0.766375  [   96/  130]
train() client id: f_00008-0-3 loss: 0.567417  [  128/  130]
train() client id: f_00008-1-0 loss: 0.661611  [   32/  130]
train() client id: f_00008-1-1 loss: 0.677488  [   64/  130]
train() client id: f_00008-1-2 loss: 0.673754  [   96/  130]
train() client id: f_00008-1-3 loss: 0.659293  [  128/  130]
train() client id: f_00008-2-0 loss: 0.601294  [   32/  130]
train() client id: f_00008-2-1 loss: 0.754549  [   64/  130]
train() client id: f_00008-2-2 loss: 0.681073  [   96/  130]
train() client id: f_00008-2-3 loss: 0.667477  [  128/  130]
train() client id: f_00008-3-0 loss: 0.596078  [   32/  130]
train() client id: f_00008-3-1 loss: 0.730148  [   64/  130]
train() client id: f_00008-3-2 loss: 0.714542  [   96/  130]
train() client id: f_00008-3-3 loss: 0.646411  [  128/  130]
train() client id: f_00008-4-0 loss: 0.542026  [   32/  130]
train() client id: f_00008-4-1 loss: 0.745009  [   64/  130]
train() client id: f_00008-4-2 loss: 0.662426  [   96/  130]
train() client id: f_00008-4-3 loss: 0.747876  [  128/  130]
train() client id: f_00008-5-0 loss: 0.610011  [   32/  130]
train() client id: f_00008-5-1 loss: 0.710934  [   64/  130]
train() client id: f_00008-5-2 loss: 0.732777  [   96/  130]
train() client id: f_00008-5-3 loss: 0.611044  [  128/  130]
train() client id: f_00008-6-0 loss: 0.593527  [   32/  130]
train() client id: f_00008-6-1 loss: 0.679101  [   64/  130]
train() client id: f_00008-6-2 loss: 0.758370  [   96/  130]
train() client id: f_00008-6-3 loss: 0.668035  [  128/  130]
train() client id: f_00008-7-0 loss: 0.681316  [   32/  130]
train() client id: f_00008-7-1 loss: 0.757245  [   64/  130]
train() client id: f_00008-7-2 loss: 0.605796  [   96/  130]
train() client id: f_00008-7-3 loss: 0.612484  [  128/  130]
train() client id: f_00008-8-0 loss: 0.545727  [   32/  130]
train() client id: f_00008-8-1 loss: 0.691205  [   64/  130]
train() client id: f_00008-8-2 loss: 0.714180  [   96/  130]
train() client id: f_00008-8-3 loss: 0.716298  [  128/  130]
train() client id: f_00008-9-0 loss: 0.723937  [   32/  130]
train() client id: f_00008-9-1 loss: 0.559337  [   64/  130]
train() client id: f_00008-9-2 loss: 0.702397  [   96/  130]
train() client id: f_00008-9-3 loss: 0.707885  [  128/  130]
train() client id: f_00008-10-0 loss: 0.760763  [   32/  130]
train() client id: f_00008-10-1 loss: 0.668285  [   64/  130]
train() client id: f_00008-10-2 loss: 0.579509  [   96/  130]
train() client id: f_00008-10-3 loss: 0.669230  [  128/  130]
train() client id: f_00008-11-0 loss: 0.671342  [   32/  130]
train() client id: f_00008-11-1 loss: 0.566019  [   64/  130]
train() client id: f_00008-11-2 loss: 0.709088  [   96/  130]
train() client id: f_00008-11-3 loss: 0.706117  [  128/  130]
train() client id: f_00008-12-0 loss: 0.740155  [   32/  130]
train() client id: f_00008-12-1 loss: 0.560245  [   64/  130]
train() client id: f_00008-12-2 loss: 0.585701  [   96/  130]
train() client id: f_00008-12-3 loss: 0.753527  [  128/  130]
train() client id: f_00009-0-0 loss: 1.170168  [   32/  118]
train() client id: f_00009-0-1 loss: 1.028177  [   64/  118]
train() client id: f_00009-0-2 loss: 1.068392  [   96/  118]
train() client id: f_00009-1-0 loss: 0.900305  [   32/  118]
train() client id: f_00009-1-1 loss: 0.950279  [   64/  118]
train() client id: f_00009-1-2 loss: 1.162454  [   96/  118]
train() client id: f_00009-2-0 loss: 1.053361  [   32/  118]
train() client id: f_00009-2-1 loss: 0.971586  [   64/  118]
train() client id: f_00009-2-2 loss: 0.967394  [   96/  118]
train() client id: f_00009-3-0 loss: 0.851030  [   32/  118]
train() client id: f_00009-3-1 loss: 1.088212  [   64/  118]
train() client id: f_00009-3-2 loss: 0.889406  [   96/  118]
train() client id: f_00009-4-0 loss: 0.791603  [   32/  118]
train() client id: f_00009-4-1 loss: 0.973202  [   64/  118]
train() client id: f_00009-4-2 loss: 1.099806  [   96/  118]
train() client id: f_00009-5-0 loss: 0.816833  [   32/  118]
train() client id: f_00009-5-1 loss: 0.929867  [   64/  118]
train() client id: f_00009-5-2 loss: 0.900368  [   96/  118]
train() client id: f_00009-6-0 loss: 0.968720  [   32/  118]
train() client id: f_00009-6-1 loss: 0.901465  [   64/  118]
train() client id: f_00009-6-2 loss: 0.788206  [   96/  118]
train() client id: f_00009-7-0 loss: 0.974776  [   32/  118]
train() client id: f_00009-7-1 loss: 0.835396  [   64/  118]
train() client id: f_00009-7-2 loss: 0.712817  [   96/  118]
train() client id: f_00009-8-0 loss: 0.718641  [   32/  118]
train() client id: f_00009-8-1 loss: 0.997854  [   64/  118]
train() client id: f_00009-8-2 loss: 0.799246  [   96/  118]
train() client id: f_00009-9-0 loss: 0.801326  [   32/  118]
train() client id: f_00009-9-1 loss: 0.963637  [   64/  118]
train() client id: f_00009-9-2 loss: 0.816307  [   96/  118]
train() client id: f_00009-10-0 loss: 0.692078  [   32/  118]
train() client id: f_00009-10-1 loss: 0.889107  [   64/  118]
train() client id: f_00009-10-2 loss: 1.002432  [   96/  118]
train() client id: f_00009-11-0 loss: 0.718471  [   32/  118]
train() client id: f_00009-11-1 loss: 0.987550  [   64/  118]
train() client id: f_00009-11-2 loss: 0.794093  [   96/  118]
train() client id: f_00009-12-0 loss: 0.686279  [   32/  118]
train() client id: f_00009-12-1 loss: 0.828486  [   64/  118]
train() client id: f_00009-12-2 loss: 0.883113  [   96/  118]
At round 39 accuracy: 0.6419098143236074
At round 39 training accuracy: 0.5861837692823608
At round 39 training loss: 0.8351706793507208
update_location
xs = -3.905658 4.200318 215.009024 18.811294 0.979296 3.956410 -177.443192 -156.324852 199.663977 -142.060879 
ys = 207.587959 190.555839 1.320614 -177.455176 169.350187 152.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -3.7114261681174354
ys mean: 56.3941425355287
dists_uav = 230.451763 215.242120 237.129974 204.558559 196.673447 182.668601 203.698250 185.575148 223.996370 173.773718 
uav_gains = -109.737166 -108.667265 -110.245428 -107.970810 -107.473055 -106.596344 -107.916023 -106.779046 -109.269306 -106.029467 
uav_gains_db_mean: -108.06839097802565
dists_bs = 174.170172 179.874228 426.932744 402.228130 176.069966 180.326394 177.641786 175.175956 406.395883 174.142123 
bs_gains = -102.314325 -102.706190 -113.217124 -112.492287 -102.446247 -102.736720 -102.554323 -102.384345 -112.617640 -102.312367 
bs_gains_db_mean: -105.57815666999366
Round 40
-------------------------------
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.71197888 11.79408709  5.62914561  2.03417315 13.60152567  6.544648
  2.51897531  8.02443866  5.92443305  5.30743185]
obj_prev = 67.09083726402451
eta_min = 7.023642441479729e-17	eta_max = 0.9321930357555286
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 15.5538538243213	eta = 0.909090909090909
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 29.16218476187658	eta = 0.4848699515649548
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 22.40002232600407	eta = 0.6312434383873119
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.175688042561685	eta = 0.667740622387296
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.10970401340027	eta = 0.6698278243997874
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.109495669486368	eta = 0.6698344353843775
eta = 0.6698344353843775
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [0.03306684 0.06954534 0.03254197 0.01128472 0.08030519 0.03831555
 0.0141715  0.04697591 0.03411658 0.03096736]
ene_total = [1.91082934 3.36153203 1.90398611 0.90652186 3.82972324 1.99226433
 1.03177315 2.44413259 2.06480829 1.66392474]
ti_comp = [0.53339798 0.56741874 0.52970956 0.54456162 0.56826829 0.56731763
 0.54487186 0.55089405 0.50878776 0.5686981 ]
ti_coms = [0.10500598 0.07098523 0.10869441 0.09384235 0.07013567 0.07108634
 0.09353211 0.08750992 0.12961621 0.06970587]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.94246766e-06 6.52945351e-05 7.67601831e-06 3.02871121e-07
 1.00231444e-04 1.09232674e-05 5.99154859e-07 2.13486048e-05
 9.58745418e-06 5.73890489e-06]
ene_total = [0.46089529 0.31419869 0.47706073 0.4115975  0.31200491 0.31225746
 0.41024983 0.3847471  0.56890568 0.30597546]
optimize_network iter = 0 obj = 3.9578926480979075
eta = 0.6698344353843775
freqs = [30996407.38459397 61282205.26212957 30716806.61521623 10361284.76037134
 70657812.12672363 33769043.53321395 13004437.39099001 42636067.25492053
 33527323.09434476 27226537.57500104]
eta_min = 0.6698344353843826	eta_max = 0.6698344353843708
af = 0.008298545367209708	bf = 1.3034344286740114	zeta = 0.00912839990393068	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [1.88907249e-06 1.55299480e-05 1.82569896e-06 7.20362393e-08
 2.38395007e-05 2.59803940e-06 1.42505706e-07 5.07764887e-06
 2.28032353e-06 1.36496714e-06]
ene_total = [1.67292505 1.13318653 1.73166738 1.49481175 1.1209777  1.13273715
 1.48988129 1.3947409  2.06499979 1.11055146]
ti_comp = [0.53339798 0.56741874 0.52970956 0.54456162 0.56826829 0.56731763
 0.54487186 0.55089405 0.50878776 0.5686981 ]
ti_coms = [0.10500598 0.07098523 0.10869441 0.09384235 0.07013567 0.07108634
 0.09353211 0.08750992 0.12961621 0.06970587]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.94246766e-06 6.52945351e-05 7.67601831e-06 3.02871121e-07
 1.00231444e-04 1.09232674e-05 5.99154859e-07 2.13486048e-05
 9.58745418e-06 5.73890489e-06]
ene_total = [0.46089529 0.31419869 0.47706073 0.4115975  0.31200491 0.31225746
 0.41024983 0.3847471  0.56890568 0.30597546]
optimize_network iter = 1 obj = 3.9578926480978276
eta = 0.6698344353843708
freqs = [30996407.384594   61282205.26212971 30716806.61521625 10361284.76037136
 70657812.12672378 33769043.53321403 13004437.39099002 42636067.2549206
 33527323.09434475 27226537.5750011 ]
Done!
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.86876585e-06 6.46886370e-05 7.60478900e-06 3.00060640e-07
 9.93013504e-05 1.08219054e-05 5.93595025e-07 2.11505013e-05
 9.49848778e-06 5.68565095e-06]
ene_total = [0.01050847 0.00716321 0.01087705 0.00938453 0.00711287 0.00711946
 0.0093538  0.00877214 0.01297112 0.00697627]
At round 40 energy consumption: 0.09023892340678759
At round 40 eta: 0.6698344353843708
At round 40 a_n: 14.480768977481684
At round 40 local rounds: 13.121762881988987
At round 40 global rounds: 43.859113515789716
gradient difference: 0.42262014746665955
train() client id: f_00000-0-0 loss: 1.133652  [   32/  126]
train() client id: f_00000-0-1 loss: 1.248917  [   64/  126]
train() client id: f_00000-0-2 loss: 1.027715  [   96/  126]
train() client id: f_00000-1-0 loss: 0.981348  [   32/  126]
train() client id: f_00000-1-1 loss: 1.243259  [   64/  126]
train() client id: f_00000-1-2 loss: 0.912408  [   96/  126]
train() client id: f_00000-2-0 loss: 0.914097  [   32/  126]
train() client id: f_00000-2-1 loss: 0.999671  [   64/  126]
train() client id: f_00000-2-2 loss: 1.025487  [   96/  126]
train() client id: f_00000-3-0 loss: 0.986036  [   32/  126]
train() client id: f_00000-3-1 loss: 0.906976  [   64/  126]
train() client id: f_00000-3-2 loss: 0.898517  [   96/  126]
train() client id: f_00000-4-0 loss: 0.902350  [   32/  126]
train() client id: f_00000-4-1 loss: 0.870894  [   64/  126]
train() client id: f_00000-4-2 loss: 0.949329  [   96/  126]
train() client id: f_00000-5-0 loss: 0.971027  [   32/  126]
train() client id: f_00000-5-1 loss: 0.861267  [   64/  126]
train() client id: f_00000-5-2 loss: 0.868597  [   96/  126]
train() client id: f_00000-6-0 loss: 0.982060  [   32/  126]
train() client id: f_00000-6-1 loss: 0.853075  [   64/  126]
train() client id: f_00000-6-2 loss: 0.809278  [   96/  126]
train() client id: f_00000-7-0 loss: 0.851517  [   32/  126]
train() client id: f_00000-7-1 loss: 0.729803  [   64/  126]
train() client id: f_00000-7-2 loss: 0.958244  [   96/  126]
train() client id: f_00000-8-0 loss: 0.749707  [   32/  126]
train() client id: f_00000-8-1 loss: 0.750953  [   64/  126]
train() client id: f_00000-8-2 loss: 0.899699  [   96/  126]
train() client id: f_00000-9-0 loss: 0.796038  [   32/  126]
train() client id: f_00000-9-1 loss: 0.778247  [   64/  126]
train() client id: f_00000-9-2 loss: 0.746560  [   96/  126]
train() client id: f_00000-10-0 loss: 0.744564  [   32/  126]
train() client id: f_00000-10-1 loss: 0.759267  [   64/  126]
train() client id: f_00000-10-2 loss: 0.870324  [   96/  126]
train() client id: f_00000-11-0 loss: 0.871202  [   32/  126]
train() client id: f_00000-11-1 loss: 0.882871  [   64/  126]
train() client id: f_00000-11-2 loss: 0.772645  [   96/  126]
train() client id: f_00000-12-0 loss: 0.688016  [   32/  126]
train() client id: f_00000-12-1 loss: 0.796433  [   64/  126]
train() client id: f_00000-12-2 loss: 1.011153  [   96/  126]
train() client id: f_00001-0-0 loss: 0.521181  [   32/  265]
train() client id: f_00001-0-1 loss: 0.478453  [   64/  265]
train() client id: f_00001-0-2 loss: 0.423214  [   96/  265]
train() client id: f_00001-0-3 loss: 0.481471  [  128/  265]
train() client id: f_00001-0-4 loss: 0.448419  [  160/  265]
train() client id: f_00001-0-5 loss: 0.514967  [  192/  265]
train() client id: f_00001-0-6 loss: 0.349277  [  224/  265]
train() client id: f_00001-0-7 loss: 0.436612  [  256/  265]
train() client id: f_00001-1-0 loss: 0.359906  [   32/  265]
train() client id: f_00001-1-1 loss: 0.474878  [   64/  265]
train() client id: f_00001-1-2 loss: 0.469753  [   96/  265]
train() client id: f_00001-1-3 loss: 0.512233  [  128/  265]
train() client id: f_00001-1-4 loss: 0.397676  [  160/  265]
train() client id: f_00001-1-5 loss: 0.542950  [  192/  265]
train() client id: f_00001-1-6 loss: 0.378196  [  224/  265]
train() client id: f_00001-1-7 loss: 0.429704  [  256/  265]
train() client id: f_00001-2-0 loss: 0.536078  [   32/  265]
train() client id: f_00001-2-1 loss: 0.399608  [   64/  265]
train() client id: f_00001-2-2 loss: 0.429991  [   96/  265]
train() client id: f_00001-2-3 loss: 0.391485  [  128/  265]
train() client id: f_00001-2-4 loss: 0.495400  [  160/  265]
train() client id: f_00001-2-5 loss: 0.380418  [  192/  265]
train() client id: f_00001-2-6 loss: 0.378950  [  224/  265]
train() client id: f_00001-2-7 loss: 0.510229  [  256/  265]
train() client id: f_00001-3-0 loss: 0.469327  [   32/  265]
train() client id: f_00001-3-1 loss: 0.441487  [   64/  265]
train() client id: f_00001-3-2 loss: 0.528539  [   96/  265]
train() client id: f_00001-3-3 loss: 0.398604  [  128/  265]
train() client id: f_00001-3-4 loss: 0.406635  [  160/  265]
train() client id: f_00001-3-5 loss: 0.353178  [  192/  265]
train() client id: f_00001-3-6 loss: 0.475889  [  224/  265]
train() client id: f_00001-3-7 loss: 0.389462  [  256/  265]
train() client id: f_00001-4-0 loss: 0.437588  [   32/  265]
train() client id: f_00001-4-1 loss: 0.430774  [   64/  265]
train() client id: f_00001-4-2 loss: 0.437225  [   96/  265]
train() client id: f_00001-4-3 loss: 0.422528  [  128/  265]
train() client id: f_00001-4-4 loss: 0.393089  [  160/  265]
train() client id: f_00001-4-5 loss: 0.437490  [  192/  265]
train() client id: f_00001-4-6 loss: 0.450324  [  224/  265]
train() client id: f_00001-4-7 loss: 0.415720  [  256/  265]
train() client id: f_00001-5-0 loss: 0.559343  [   32/  265]
train() client id: f_00001-5-1 loss: 0.443745  [   64/  265]
train() client id: f_00001-5-2 loss: 0.478459  [   96/  265]
train() client id: f_00001-5-3 loss: 0.436353  [  128/  265]
train() client id: f_00001-5-4 loss: 0.435333  [  160/  265]
train() client id: f_00001-5-5 loss: 0.358007  [  192/  265]
train() client id: f_00001-5-6 loss: 0.348469  [  224/  265]
train() client id: f_00001-5-7 loss: 0.336017  [  256/  265]
train() client id: f_00001-6-0 loss: 0.498517  [   32/  265]
train() client id: f_00001-6-1 loss: 0.399139  [   64/  265]
train() client id: f_00001-6-2 loss: 0.378453  [   96/  265]
train() client id: f_00001-6-3 loss: 0.401990  [  128/  265]
train() client id: f_00001-6-4 loss: 0.418735  [  160/  265]
train() client id: f_00001-6-5 loss: 0.348743  [  192/  265]
train() client id: f_00001-6-6 loss: 0.430117  [  224/  265]
train() client id: f_00001-6-7 loss: 0.456833  [  256/  265]
train() client id: f_00001-7-0 loss: 0.382135  [   32/  265]
train() client id: f_00001-7-1 loss: 0.341445  [   64/  265]
train() client id: f_00001-7-2 loss: 0.560134  [   96/  265]
train() client id: f_00001-7-3 loss: 0.364582  [  128/  265]
train() client id: f_00001-7-4 loss: 0.369951  [  160/  265]
train() client id: f_00001-7-5 loss: 0.605781  [  192/  265]
train() client id: f_00001-7-6 loss: 0.387321  [  224/  265]
train() client id: f_00001-7-7 loss: 0.344763  [  256/  265]
train() client id: f_00001-8-0 loss: 0.417277  [   32/  265]
train() client id: f_00001-8-1 loss: 0.410308  [   64/  265]
train() client id: f_00001-8-2 loss: 0.395742  [   96/  265]
train() client id: f_00001-8-3 loss: 0.563324  [  128/  265]
train() client id: f_00001-8-4 loss: 0.441768  [  160/  265]
train() client id: f_00001-8-5 loss: 0.414265  [  192/  265]
train() client id: f_00001-8-6 loss: 0.368605  [  224/  265]
train() client id: f_00001-8-7 loss: 0.324747  [  256/  265]
train() client id: f_00001-9-0 loss: 0.378370  [   32/  265]
train() client id: f_00001-9-1 loss: 0.557014  [   64/  265]
train() client id: f_00001-9-2 loss: 0.402693  [   96/  265]
train() client id: f_00001-9-3 loss: 0.442942  [  128/  265]
train() client id: f_00001-9-4 loss: 0.467277  [  160/  265]
train() client id: f_00001-9-5 loss: 0.412964  [  192/  265]
train() client id: f_00001-9-6 loss: 0.335615  [  224/  265]
train() client id: f_00001-9-7 loss: 0.315908  [  256/  265]
train() client id: f_00001-10-0 loss: 0.545226  [   32/  265]
train() client id: f_00001-10-1 loss: 0.447084  [   64/  265]
train() client id: f_00001-10-2 loss: 0.347831  [   96/  265]
train() client id: f_00001-10-3 loss: 0.390388  [  128/  265]
train() client id: f_00001-10-4 loss: 0.469785  [  160/  265]
train() client id: f_00001-10-5 loss: 0.317694  [  192/  265]
train() client id: f_00001-10-6 loss: 0.379099  [  224/  265]
train() client id: f_00001-10-7 loss: 0.349974  [  256/  265]
train() client id: f_00001-11-0 loss: 0.354783  [   32/  265]
train() client id: f_00001-11-1 loss: 0.493857  [   64/  265]
train() client id: f_00001-11-2 loss: 0.380238  [   96/  265]
train() client id: f_00001-11-3 loss: 0.397500  [  128/  265]
train() client id: f_00001-11-4 loss: 0.489118  [  160/  265]
train() client id: f_00001-11-5 loss: 0.508897  [  192/  265]
train() client id: f_00001-11-6 loss: 0.351150  [  224/  265]
train() client id: f_00001-11-7 loss: 0.309382  [  256/  265]
train() client id: f_00001-12-0 loss: 0.589112  [   32/  265]
train() client id: f_00001-12-1 loss: 0.326219  [   64/  265]
train() client id: f_00001-12-2 loss: 0.464885  [   96/  265]
train() client id: f_00001-12-3 loss: 0.403569  [  128/  265]
train() client id: f_00001-12-4 loss: 0.306568  [  160/  265]
train() client id: f_00001-12-5 loss: 0.423807  [  192/  265]
train() client id: f_00001-12-6 loss: 0.397677  [  224/  265]
train() client id: f_00001-12-7 loss: 0.374797  [  256/  265]
train() client id: f_00002-0-0 loss: 1.183305  [   32/  124]
train() client id: f_00002-0-1 loss: 1.207261  [   64/  124]
train() client id: f_00002-0-2 loss: 0.980400  [   96/  124]
train() client id: f_00002-1-0 loss: 1.087224  [   32/  124]
train() client id: f_00002-1-1 loss: 1.248511  [   64/  124]
train() client id: f_00002-1-2 loss: 1.106861  [   96/  124]
train() client id: f_00002-2-0 loss: 0.979436  [   32/  124]
train() client id: f_00002-2-1 loss: 1.078973  [   64/  124]
train() client id: f_00002-2-2 loss: 1.137901  [   96/  124]
train() client id: f_00002-3-0 loss: 1.150228  [   32/  124]
train() client id: f_00002-3-1 loss: 1.107178  [   64/  124]
train() client id: f_00002-3-2 loss: 0.966427  [   96/  124]
train() client id: f_00002-4-0 loss: 1.103617  [   32/  124]
train() client id: f_00002-4-1 loss: 1.026871  [   64/  124]
train() client id: f_00002-4-2 loss: 1.088413  [   96/  124]
train() client id: f_00002-5-0 loss: 0.968477  [   32/  124]
train() client id: f_00002-5-1 loss: 0.990875  [   64/  124]
train() client id: f_00002-5-2 loss: 0.960887  [   96/  124]
train() client id: f_00002-6-0 loss: 1.075960  [   32/  124]
train() client id: f_00002-6-1 loss: 0.973585  [   64/  124]
train() client id: f_00002-6-2 loss: 1.034710  [   96/  124]
train() client id: f_00002-7-0 loss: 1.011741  [   32/  124]
train() client id: f_00002-7-1 loss: 0.985799  [   64/  124]
train() client id: f_00002-7-2 loss: 1.141518  [   96/  124]
train() client id: f_00002-8-0 loss: 1.158296  [   32/  124]
train() client id: f_00002-8-1 loss: 1.172258  [   64/  124]
train() client id: f_00002-8-2 loss: 0.790333  [   96/  124]
train() client id: f_00002-9-0 loss: 0.918291  [   32/  124]
train() client id: f_00002-9-1 loss: 0.973115  [   64/  124]
train() client id: f_00002-9-2 loss: 1.173325  [   96/  124]
train() client id: f_00002-10-0 loss: 0.937525  [   32/  124]
train() client id: f_00002-10-1 loss: 1.105468  [   64/  124]
train() client id: f_00002-10-2 loss: 1.001227  [   96/  124]
train() client id: f_00002-11-0 loss: 1.012007  [   32/  124]
train() client id: f_00002-11-1 loss: 1.002979  [   64/  124]
train() client id: f_00002-11-2 loss: 0.970057  [   96/  124]
train() client id: f_00002-12-0 loss: 0.976253  [   32/  124]
train() client id: f_00002-12-1 loss: 1.071333  [   64/  124]
train() client id: f_00002-12-2 loss: 0.943125  [   96/  124]
train() client id: f_00003-0-0 loss: 0.602876  [   32/   43]
train() client id: f_00003-1-0 loss: 0.666900  [   32/   43]
train() client id: f_00003-2-0 loss: 0.781414  [   32/   43]
train() client id: f_00003-3-0 loss: 0.697811  [   32/   43]
train() client id: f_00003-4-0 loss: 0.546941  [   32/   43]
train() client id: f_00003-5-0 loss: 0.448937  [   32/   43]
train() client id: f_00003-6-0 loss: 0.495807  [   32/   43]
train() client id: f_00003-7-0 loss: 0.515300  [   32/   43]
train() client id: f_00003-8-0 loss: 0.606011  [   32/   43]
train() client id: f_00003-9-0 loss: 0.457383  [   32/   43]
train() client id: f_00003-10-0 loss: 0.498516  [   32/   43]
train() client id: f_00003-11-0 loss: 0.693678  [   32/   43]
train() client id: f_00003-12-0 loss: 0.622434  [   32/   43]
train() client id: f_00004-0-0 loss: 0.969703  [   32/  306]
train() client id: f_00004-0-1 loss: 0.999978  [   64/  306]
train() client id: f_00004-0-2 loss: 0.827559  [   96/  306]
train() client id: f_00004-0-3 loss: 0.864149  [  128/  306]
train() client id: f_00004-0-4 loss: 1.149265  [  160/  306]
train() client id: f_00004-0-5 loss: 0.823204  [  192/  306]
train() client id: f_00004-0-6 loss: 0.931716  [  224/  306]
train() client id: f_00004-0-7 loss: 0.930101  [  256/  306]
train() client id: f_00004-0-8 loss: 0.830203  [  288/  306]
train() client id: f_00004-1-0 loss: 0.859529  [   32/  306]
train() client id: f_00004-1-1 loss: 0.893132  [   64/  306]
train() client id: f_00004-1-2 loss: 0.888956  [   96/  306]
train() client id: f_00004-1-3 loss: 0.927296  [  128/  306]
train() client id: f_00004-1-4 loss: 1.097747  [  160/  306]
train() client id: f_00004-1-5 loss: 0.974909  [  192/  306]
train() client id: f_00004-1-6 loss: 0.884664  [  224/  306]
train() client id: f_00004-1-7 loss: 0.889211  [  256/  306]
train() client id: f_00004-1-8 loss: 0.924948  [  288/  306]
train() client id: f_00004-2-0 loss: 1.001166  [   32/  306]
train() client id: f_00004-2-1 loss: 0.874901  [   64/  306]
train() client id: f_00004-2-2 loss: 1.000676  [   96/  306]
train() client id: f_00004-2-3 loss: 0.827950  [  128/  306]
train() client id: f_00004-2-4 loss: 0.987571  [  160/  306]
train() client id: f_00004-2-5 loss: 0.979792  [  192/  306]
train() client id: f_00004-2-6 loss: 0.870158  [  224/  306]
train() client id: f_00004-2-7 loss: 0.848055  [  256/  306]
train() client id: f_00004-2-8 loss: 0.868848  [  288/  306]
train() client id: f_00004-3-0 loss: 0.842043  [   32/  306]
train() client id: f_00004-3-1 loss: 0.884020  [   64/  306]
train() client id: f_00004-3-2 loss: 0.946402  [   96/  306]
train() client id: f_00004-3-3 loss: 0.905053  [  128/  306]
train() client id: f_00004-3-4 loss: 0.956558  [  160/  306]
train() client id: f_00004-3-5 loss: 0.978426  [  192/  306]
train() client id: f_00004-3-6 loss: 0.840726  [  224/  306]
train() client id: f_00004-3-7 loss: 1.013983  [  256/  306]
train() client id: f_00004-3-8 loss: 0.846312  [  288/  306]
train() client id: f_00004-4-0 loss: 0.857669  [   32/  306]
train() client id: f_00004-4-1 loss: 0.922169  [   64/  306]
train() client id: f_00004-4-2 loss: 0.931671  [   96/  306]
train() client id: f_00004-4-3 loss: 0.914945  [  128/  306]
train() client id: f_00004-4-4 loss: 0.893021  [  160/  306]
train() client id: f_00004-4-5 loss: 1.023568  [  192/  306]
train() client id: f_00004-4-6 loss: 0.836181  [  224/  306]
train() client id: f_00004-4-7 loss: 0.930691  [  256/  306]
train() client id: f_00004-4-8 loss: 0.889148  [  288/  306]
train() client id: f_00004-5-0 loss: 0.929892  [   32/  306]
train() client id: f_00004-5-1 loss: 0.849343  [   64/  306]
train() client id: f_00004-5-2 loss: 0.992453  [   96/  306]
train() client id: f_00004-5-3 loss: 1.018030  [  128/  306]
train() client id: f_00004-5-4 loss: 0.891364  [  160/  306]
train() client id: f_00004-5-5 loss: 0.930017  [  192/  306]
train() client id: f_00004-5-6 loss: 0.893665  [  224/  306]
train() client id: f_00004-5-7 loss: 0.709219  [  256/  306]
train() client id: f_00004-5-8 loss: 0.988844  [  288/  306]
train() client id: f_00004-6-0 loss: 0.844291  [   32/  306]
train() client id: f_00004-6-1 loss: 0.947699  [   64/  306]
train() client id: f_00004-6-2 loss: 0.908537  [   96/  306]
train() client id: f_00004-6-3 loss: 0.927104  [  128/  306]
train() client id: f_00004-6-4 loss: 1.002654  [  160/  306]
train() client id: f_00004-6-5 loss: 0.845390  [  192/  306]
train() client id: f_00004-6-6 loss: 0.941509  [  224/  306]
train() client id: f_00004-6-7 loss: 0.845913  [  256/  306]
train() client id: f_00004-6-8 loss: 0.936637  [  288/  306]
train() client id: f_00004-7-0 loss: 1.029790  [   32/  306]
train() client id: f_00004-7-1 loss: 0.910728  [   64/  306]
train() client id: f_00004-7-2 loss: 0.880375  [   96/  306]
train() client id: f_00004-7-3 loss: 0.887301  [  128/  306]
train() client id: f_00004-7-4 loss: 0.958044  [  160/  306]
train() client id: f_00004-7-5 loss: 0.873699  [  192/  306]
train() client id: f_00004-7-6 loss: 0.769734  [  224/  306]
train() client id: f_00004-7-7 loss: 0.994092  [  256/  306]
train() client id: f_00004-7-8 loss: 0.850936  [  288/  306]
train() client id: f_00004-8-0 loss: 0.854773  [   32/  306]
train() client id: f_00004-8-1 loss: 0.879045  [   64/  306]
train() client id: f_00004-8-2 loss: 0.974828  [   96/  306]
train() client id: f_00004-8-3 loss: 0.916120  [  128/  306]
train() client id: f_00004-8-4 loss: 0.934688  [  160/  306]
train() client id: f_00004-8-5 loss: 0.794939  [  192/  306]
train() client id: f_00004-8-6 loss: 0.887116  [  224/  306]
train() client id: f_00004-8-7 loss: 0.850810  [  256/  306]
train() client id: f_00004-8-8 loss: 0.906202  [  288/  306]
train() client id: f_00004-9-0 loss: 0.855002  [   32/  306]
train() client id: f_00004-9-1 loss: 1.013949  [   64/  306]
train() client id: f_00004-9-2 loss: 0.892009  [   96/  306]
train() client id: f_00004-9-3 loss: 0.809491  [  128/  306]
train() client id: f_00004-9-4 loss: 0.786020  [  160/  306]
train() client id: f_00004-9-5 loss: 0.901557  [  192/  306]
train() client id: f_00004-9-6 loss: 0.925911  [  224/  306]
train() client id: f_00004-9-7 loss: 1.003979  [  256/  306]
train() client id: f_00004-9-8 loss: 0.889456  [  288/  306]
train() client id: f_00004-10-0 loss: 0.879794  [   32/  306]
train() client id: f_00004-10-1 loss: 0.837709  [   64/  306]
train() client id: f_00004-10-2 loss: 0.927530  [   96/  306]
train() client id: f_00004-10-3 loss: 0.938487  [  128/  306]
train() client id: f_00004-10-4 loss: 0.914788  [  160/  306]
train() client id: f_00004-10-5 loss: 0.875282  [  192/  306]
train() client id: f_00004-10-6 loss: 0.893014  [  224/  306]
train() client id: f_00004-10-7 loss: 0.942743  [  256/  306]
train() client id: f_00004-10-8 loss: 0.826551  [  288/  306]
train() client id: f_00004-11-0 loss: 0.872785  [   32/  306]
train() client id: f_00004-11-1 loss: 0.853204  [   64/  306]
train() client id: f_00004-11-2 loss: 0.989142  [   96/  306]
train() client id: f_00004-11-3 loss: 0.868781  [  128/  306]
train() client id: f_00004-11-4 loss: 0.817221  [  160/  306]
train() client id: f_00004-11-5 loss: 0.818403  [  192/  306]
train() client id: f_00004-11-6 loss: 0.932238  [  224/  306]
train() client id: f_00004-11-7 loss: 0.839543  [  256/  306]
train() client id: f_00004-11-8 loss: 0.946831  [  288/  306]
train() client id: f_00004-12-0 loss: 0.934746  [   32/  306]
train() client id: f_00004-12-1 loss: 0.872789  [   64/  306]
train() client id: f_00004-12-2 loss: 0.913716  [   96/  306]
train() client id: f_00004-12-3 loss: 0.824187  [  128/  306]
train() client id: f_00004-12-4 loss: 0.887212  [  160/  306]
train() client id: f_00004-12-5 loss: 0.858093  [  192/  306]
train() client id: f_00004-12-6 loss: 0.863093  [  224/  306]
train() client id: f_00004-12-7 loss: 0.911887  [  256/  306]
train() client id: f_00004-12-8 loss: 0.854996  [  288/  306]
train() client id: f_00005-0-0 loss: 0.332451  [   32/  146]
train() client id: f_00005-0-1 loss: 0.556120  [   64/  146]
train() client id: f_00005-0-2 loss: 0.360482  [   96/  146]
train() client id: f_00005-0-3 loss: 0.499629  [  128/  146]
train() client id: f_00005-1-0 loss: 0.498160  [   32/  146]
train() client id: f_00005-1-1 loss: 0.281282  [   64/  146]
train() client id: f_00005-1-2 loss: 0.645720  [   96/  146]
train() client id: f_00005-1-3 loss: 0.274712  [  128/  146]
train() client id: f_00005-2-0 loss: 0.205350  [   32/  146]
train() client id: f_00005-2-1 loss: 0.292095  [   64/  146]
train() client id: f_00005-2-2 loss: 0.774934  [   96/  146]
train() client id: f_00005-2-3 loss: 0.499321  [  128/  146]
train() client id: f_00005-3-0 loss: 0.219658  [   32/  146]
train() client id: f_00005-3-1 loss: 0.509737  [   64/  146]
train() client id: f_00005-3-2 loss: 0.361299  [   96/  146]
train() client id: f_00005-3-3 loss: 0.480671  [  128/  146]
train() client id: f_00005-4-0 loss: 0.520379  [   32/  146]
train() client id: f_00005-4-1 loss: 0.233286  [   64/  146]
train() client id: f_00005-4-2 loss: 0.443446  [   96/  146]
train() client id: f_00005-4-3 loss: 0.397827  [  128/  146]
train() client id: f_00005-5-0 loss: 0.448626  [   32/  146]
train() client id: f_00005-5-1 loss: 0.668752  [   64/  146]
train() client id: f_00005-5-2 loss: 0.173704  [   96/  146]
train() client id: f_00005-5-3 loss: 0.353201  [  128/  146]
train() client id: f_00005-6-0 loss: 0.555115  [   32/  146]
train() client id: f_00005-6-1 loss: 0.242155  [   64/  146]
train() client id: f_00005-6-2 loss: 0.281310  [   96/  146]
train() client id: f_00005-6-3 loss: 0.639342  [  128/  146]
train() client id: f_00005-7-0 loss: 0.464312  [   32/  146]
train() client id: f_00005-7-1 loss: 0.192450  [   64/  146]
train() client id: f_00005-7-2 loss: 0.601940  [   96/  146]
train() client id: f_00005-7-3 loss: 0.288062  [  128/  146]
train() client id: f_00005-8-0 loss: 0.496318  [   32/  146]
train() client id: f_00005-8-1 loss: 0.409310  [   64/  146]
train() client id: f_00005-8-2 loss: 0.217484  [   96/  146]
train() client id: f_00005-8-3 loss: 0.160373  [  128/  146]
train() client id: f_00005-9-0 loss: 0.575183  [   32/  146]
train() client id: f_00005-9-1 loss: 0.260569  [   64/  146]
train() client id: f_00005-9-2 loss: 0.225616  [   96/  146]
train() client id: f_00005-9-3 loss: 0.508662  [  128/  146]
train() client id: f_00005-10-0 loss: 0.298349  [   32/  146]
train() client id: f_00005-10-1 loss: 0.368559  [   64/  146]
train() client id: f_00005-10-2 loss: 0.386893  [   96/  146]
train() client id: f_00005-10-3 loss: 0.364531  [  128/  146]
train() client id: f_00005-11-0 loss: 0.399754  [   32/  146]
train() client id: f_00005-11-1 loss: 0.265410  [   64/  146]
train() client id: f_00005-11-2 loss: 0.274354  [   96/  146]
train() client id: f_00005-11-3 loss: 0.370818  [  128/  146]
train() client id: f_00005-12-0 loss: 0.341648  [   32/  146]
train() client id: f_00005-12-1 loss: 0.326637  [   64/  146]
train() client id: f_00005-12-2 loss: 0.384267  [   96/  146]
train() client id: f_00005-12-3 loss: 0.339242  [  128/  146]
train() client id: f_00006-0-0 loss: 0.469468  [   32/   54]
train() client id: f_00006-1-0 loss: 0.477454  [   32/   54]
train() client id: f_00006-2-0 loss: 0.553388  [   32/   54]
train() client id: f_00006-3-0 loss: 0.509817  [   32/   54]
train() client id: f_00006-4-0 loss: 0.542443  [   32/   54]
train() client id: f_00006-5-0 loss: 0.514295  [   32/   54]
train() client id: f_00006-6-0 loss: 0.519648  [   32/   54]
train() client id: f_00006-7-0 loss: 0.550607  [   32/   54]
train() client id: f_00006-8-0 loss: 0.529492  [   32/   54]
train() client id: f_00006-9-0 loss: 0.502593  [   32/   54]
train() client id: f_00006-10-0 loss: 0.537826  [   32/   54]
train() client id: f_00006-11-0 loss: 0.493835  [   32/   54]
train() client id: f_00006-12-0 loss: 0.544992  [   32/   54]
train() client id: f_00007-0-0 loss: 0.481189  [   32/  179]
train() client id: f_00007-0-1 loss: 0.682129  [   64/  179]
train() client id: f_00007-0-2 loss: 0.517458  [   96/  179]
train() client id: f_00007-0-3 loss: 0.752460  [  128/  179]
train() client id: f_00007-0-4 loss: 0.704800  [  160/  179]
train() client id: f_00007-1-0 loss: 0.470991  [   32/  179]
train() client id: f_00007-1-1 loss: 0.502405  [   64/  179]
train() client id: f_00007-1-2 loss: 0.692128  [   96/  179]
train() client id: f_00007-1-3 loss: 0.816765  [  128/  179]
train() client id: f_00007-1-4 loss: 0.675978  [  160/  179]
train() client id: f_00007-2-0 loss: 0.470803  [   32/  179]
train() client id: f_00007-2-1 loss: 0.904934  [   64/  179]
train() client id: f_00007-2-2 loss: 0.554218  [   96/  179]
train() client id: f_00007-2-3 loss: 0.649258  [  128/  179]
train() client id: f_00007-2-4 loss: 0.441460  [  160/  179]
train() client id: f_00007-3-0 loss: 0.533137  [   32/  179]
train() client id: f_00007-3-1 loss: 0.669117  [   64/  179]
train() client id: f_00007-3-2 loss: 0.438245  [   96/  179]
train() client id: f_00007-3-3 loss: 0.566754  [  128/  179]
train() client id: f_00007-3-4 loss: 0.755023  [  160/  179]
train() client id: f_00007-4-0 loss: 0.540226  [   32/  179]
train() client id: f_00007-4-1 loss: 0.622385  [   64/  179]
train() client id: f_00007-4-2 loss: 0.414354  [   96/  179]
train() client id: f_00007-4-3 loss: 0.624098  [  128/  179]
train() client id: f_00007-4-4 loss: 0.772756  [  160/  179]
train() client id: f_00007-5-0 loss: 0.659883  [   32/  179]
train() client id: f_00007-5-1 loss: 0.607791  [   64/  179]
train() client id: f_00007-5-2 loss: 0.457938  [   96/  179]
train() client id: f_00007-5-3 loss: 0.653753  [  128/  179]
train() client id: f_00007-5-4 loss: 0.424417  [  160/  179]
train() client id: f_00007-6-0 loss: 0.546904  [   32/  179]
train() client id: f_00007-6-1 loss: 0.567111  [   64/  179]
train() client id: f_00007-6-2 loss: 0.474022  [   96/  179]
train() client id: f_00007-6-3 loss: 0.684125  [  128/  179]
train() client id: f_00007-6-4 loss: 0.482120  [  160/  179]
train() client id: f_00007-7-0 loss: 0.511728  [   32/  179]
train() client id: f_00007-7-1 loss: 0.436946  [   64/  179]
train() client id: f_00007-7-2 loss: 0.713572  [   96/  179]
train() client id: f_00007-7-3 loss: 0.408549  [  128/  179]
train() client id: f_00007-7-4 loss: 0.697236  [  160/  179]
train() client id: f_00007-8-0 loss: 0.521145  [   32/  179]
train() client id: f_00007-8-1 loss: 0.584136  [   64/  179]
train() client id: f_00007-8-2 loss: 0.542275  [   96/  179]
train() client id: f_00007-8-3 loss: 0.604284  [  128/  179]
train() client id: f_00007-8-4 loss: 0.665426  [  160/  179]
train() client id: f_00007-9-0 loss: 0.544933  [   32/  179]
train() client id: f_00007-9-1 loss: 0.497336  [   64/  179]
train() client id: f_00007-9-2 loss: 0.776462  [   96/  179]
train() client id: f_00007-9-3 loss: 0.623605  [  128/  179]
train() client id: f_00007-9-4 loss: 0.456437  [  160/  179]
train() client id: f_00007-10-0 loss: 0.558822  [   32/  179]
train() client id: f_00007-10-1 loss: 0.601849  [   64/  179]
train() client id: f_00007-10-2 loss: 0.474815  [   96/  179]
train() client id: f_00007-10-3 loss: 0.507088  [  128/  179]
train() client id: f_00007-10-4 loss: 0.513887  [  160/  179]
train() client id: f_00007-11-0 loss: 0.465580  [   32/  179]
train() client id: f_00007-11-1 loss: 0.410244  [   64/  179]
train() client id: f_00007-11-2 loss: 0.535415  [   96/  179]
train() client id: f_00007-11-3 loss: 0.691795  [  128/  179]
train() client id: f_00007-11-4 loss: 0.691296  [  160/  179]
train() client id: f_00007-12-0 loss: 0.463470  [   32/  179]
train() client id: f_00007-12-1 loss: 0.591659  [   64/  179]
train() client id: f_00007-12-2 loss: 0.723067  [   96/  179]
train() client id: f_00007-12-3 loss: 0.377399  [  128/  179]
train() client id: f_00007-12-4 loss: 0.673686  [  160/  179]
train() client id: f_00008-0-0 loss: 0.727473  [   32/  130]
train() client id: f_00008-0-1 loss: 0.559544  [   64/  130]
train() client id: f_00008-0-2 loss: 0.633878  [   96/  130]
train() client id: f_00008-0-3 loss: 0.611683  [  128/  130]
train() client id: f_00008-1-0 loss: 0.673382  [   32/  130]
train() client id: f_00008-1-1 loss: 0.622287  [   64/  130]
train() client id: f_00008-1-2 loss: 0.609993  [   96/  130]
train() client id: f_00008-1-3 loss: 0.647924  [  128/  130]
train() client id: f_00008-2-0 loss: 0.657038  [   32/  130]
train() client id: f_00008-2-1 loss: 0.584051  [   64/  130]
train() client id: f_00008-2-2 loss: 0.720574  [   96/  130]
train() client id: f_00008-2-3 loss: 0.571868  [  128/  130]
train() client id: f_00008-3-0 loss: 0.640128  [   32/  130]
train() client id: f_00008-3-1 loss: 0.723296  [   64/  130]
train() client id: f_00008-3-2 loss: 0.660633  [   96/  130]
train() client id: f_00008-3-3 loss: 0.546138  [  128/  130]
train() client id: f_00008-4-0 loss: 0.606491  [   32/  130]
train() client id: f_00008-4-1 loss: 0.649775  [   64/  130]
train() client id: f_00008-4-2 loss: 0.671293  [   96/  130]
train() client id: f_00008-4-3 loss: 0.639590  [  128/  130]
train() client id: f_00008-5-0 loss: 0.551379  [   32/  130]
train() client id: f_00008-5-1 loss: 0.678975  [   64/  130]
train() client id: f_00008-5-2 loss: 0.729000  [   96/  130]
train() client id: f_00008-5-3 loss: 0.607011  [  128/  130]
train() client id: f_00008-6-0 loss: 0.547980  [   32/  130]
train() client id: f_00008-6-1 loss: 0.608246  [   64/  130]
train() client id: f_00008-6-2 loss: 0.652270  [   96/  130]
train() client id: f_00008-6-3 loss: 0.716956  [  128/  130]
train() client id: f_00008-7-0 loss: 0.660996  [   32/  130]
train() client id: f_00008-7-1 loss: 0.547754  [   64/  130]
train() client id: f_00008-7-2 loss: 0.715865  [   96/  130]
train() client id: f_00008-7-3 loss: 0.618663  [  128/  130]
train() client id: f_00008-8-0 loss: 0.685553  [   32/  130]
train() client id: f_00008-8-1 loss: 0.671372  [   64/  130]
train() client id: f_00008-8-2 loss: 0.580490  [   96/  130]
train() client id: f_00008-8-3 loss: 0.617820  [  128/  130]
train() client id: f_00008-9-0 loss: 0.686841  [   32/  130]
train() client id: f_00008-9-1 loss: 0.611781  [   64/  130]
train() client id: f_00008-9-2 loss: 0.615943  [   96/  130]
train() client id: f_00008-9-3 loss: 0.607531  [  128/  130]
train() client id: f_00008-10-0 loss: 0.526752  [   32/  130]
train() client id: f_00008-10-1 loss: 0.626778  [   64/  130]
train() client id: f_00008-10-2 loss: 0.598979  [   96/  130]
train() client id: f_00008-10-3 loss: 0.757324  [  128/  130]
train() client id: f_00008-11-0 loss: 0.592007  [   32/  130]
train() client id: f_00008-11-1 loss: 0.665100  [   64/  130]
train() client id: f_00008-11-2 loss: 0.598955  [   96/  130]
train() client id: f_00008-11-3 loss: 0.655435  [  128/  130]
train() client id: f_00008-12-0 loss: 0.664394  [   32/  130]
train() client id: f_00008-12-1 loss: 0.614172  [   64/  130]
train() client id: f_00008-12-2 loss: 0.635335  [   96/  130]
train() client id: f_00008-12-3 loss: 0.633250  [  128/  130]
train() client id: f_00009-0-0 loss: 0.837690  [   32/  118]
train() client id: f_00009-0-1 loss: 1.046518  [   64/  118]
train() client id: f_00009-0-2 loss: 1.127248  [   96/  118]
train() client id: f_00009-1-0 loss: 1.152239  [   32/  118]
train() client id: f_00009-1-1 loss: 0.915721  [   64/  118]
train() client id: f_00009-1-2 loss: 0.930687  [   96/  118]
train() client id: f_00009-2-0 loss: 1.209358  [   32/  118]
train() client id: f_00009-2-1 loss: 0.865425  [   64/  118]
train() client id: f_00009-2-2 loss: 0.873707  [   96/  118]
train() client id: f_00009-3-0 loss: 1.007519  [   32/  118]
train() client id: f_00009-3-1 loss: 0.930634  [   64/  118]
train() client id: f_00009-3-2 loss: 0.929690  [   96/  118]
train() client id: f_00009-4-0 loss: 0.909553  [   32/  118]
train() client id: f_00009-4-1 loss: 0.968450  [   64/  118]
train() client id: f_00009-4-2 loss: 0.824969  [   96/  118]
train() client id: f_00009-5-0 loss: 0.887259  [   32/  118]
train() client id: f_00009-5-1 loss: 0.762057  [   64/  118]
train() client id: f_00009-5-2 loss: 0.798225  [   96/  118]
train() client id: f_00009-6-0 loss: 0.792863  [   32/  118]
train() client id: f_00009-6-1 loss: 0.804413  [   64/  118]
train() client id: f_00009-6-2 loss: 0.819718  [   96/  118]
train() client id: f_00009-7-0 loss: 0.940201  [   32/  118]
train() client id: f_00009-7-1 loss: 0.722236  [   64/  118]
train() client id: f_00009-7-2 loss: 0.783036  [   96/  118]
train() client id: f_00009-8-0 loss: 0.709992  [   32/  118]
train() client id: f_00009-8-1 loss: 0.721693  [   64/  118]
train() client id: f_00009-8-2 loss: 0.851376  [   96/  118]
train() client id: f_00009-9-0 loss: 0.713833  [   32/  118]
train() client id: f_00009-9-1 loss: 0.786445  [   64/  118]
train() client id: f_00009-9-2 loss: 0.751580  [   96/  118]
train() client id: f_00009-10-0 loss: 0.851186  [   32/  118]
train() client id: f_00009-10-1 loss: 0.670586  [   64/  118]
train() client id: f_00009-10-2 loss: 0.723298  [   96/  118]
train() client id: f_00009-11-0 loss: 0.753747  [   32/  118]
train() client id: f_00009-11-1 loss: 0.820045  [   64/  118]
train() client id: f_00009-11-2 loss: 0.620987  [   96/  118]
train() client id: f_00009-12-0 loss: 0.746487  [   32/  118]
train() client id: f_00009-12-1 loss: 0.791627  [   64/  118]
train() client id: f_00009-12-2 loss: 0.706716  [   96/  118]
At round 40 accuracy: 0.6392572944297082
At round 40 training accuracy: 0.590878604963112
At round 40 training loss: 0.8290661053407647
update_location
xs = -3.905658 4.200318 220.009024 18.811294 0.979296 3.956410 -182.443192 -161.324852 204.663977 -147.060879 
ys = 212.587959 195.555839 1.320614 -182.455176 174.350187 157.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -4.211426168117436
ys mean: 57.8941425355287
dists_uav = 234.965731 219.680970 241.672743 208.910881 200.994892 186.871505 208.068279 189.806175 228.464469 177.884552 
uav_gains = -110.077920 -108.968280 -110.605786 -108.250543 -107.744744 -106.860282 -108.196017 -107.043810 -109.590763 -106.293214 
uav_gains_db_mean: -108.36313594867394
dists_bs = 175.174566 180.375432 431.505108 406.616552 175.980496 179.779726 177.780866 174.713663 411.010040 173.265945 
bs_gains = -102.384249 -102.740026 -113.346665 -112.624241 -102.440066 -102.699799 -102.563840 -102.352212 -112.754927 -102.251029 
bs_gains_db_mean: -105.61570533344862
Round 41
-------------------------------
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.58035445 11.51525536  5.49993513  1.98836081 13.27975002  6.38965832
  2.46168242  7.8365251   5.78633913  5.1816046 ]
obj_prev = 65.51946532847218
eta_min = 2.9576516171485466e-17	eta_max = 0.9329613748543103
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 15.185923913957124	eta = 0.9090909090909091
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 28.643068262013102	eta = 0.4819799767971639
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 21.937413464592197	eta = 0.6293077986886223
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.723057688485657	eta = 0.6661847678972268
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.65725512436301	eta = 0.668306863289048
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.65704485277882	eta = 0.6683136660986402
eta = 0.6683136660986402
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [0.0332544  0.06993981 0.03272655 0.01134872 0.08076069 0.03853288
 0.01425189 0.04724236 0.0343101  0.03114301]
ene_total = [1.87549698 3.28425833 1.87007596 0.89063245 3.74131718 1.94496118
 1.01302102 2.39242243 2.02108855 1.62377077]
ti_comp = [0.54825773 0.58461281 0.54425104 0.56025271 0.5855944  0.58474602
 0.56057165 0.56686532 0.52465853 0.58619944]
ti_coms = [0.10745239 0.0710973  0.11145908 0.09545741 0.07011572 0.0709641
 0.09513846 0.0888448  0.13105159 0.06951068]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.64641487e-06 6.25628775e-05 7.39575090e-06 2.91040345e-07
 9.60033089e-05 1.04577867e-05 5.75751087e-07 2.05076110e-05
 9.17048565e-06 5.49377638e-06]
ene_total = [0.45834474 0.30572102 0.4754127  0.40690231 0.30296238 0.30293225
 0.40555492 0.37957759 0.55900197 0.29652539]
optimize_network iter = 0 obj = 3.892935261137077
eta = 0.6683136660986402
freqs = [30327342.94858874 59817206.41115162 30065677.93401769 10128218.84644976
 68956163.97393337 32948387.69791197 12711921.71227483 41669829.72547625
 32697548.88052001 26563493.52335148]
eta_min = 0.6683136660986426	eta_max = 0.6683136660986362
af = 0.007721858371378171	bf = 1.2881215340611454	zeta = 0.008494044208515988	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [1.80840054e-06 1.47963121e-05 1.74911774e-06 6.88319331e-08
 2.27050766e-05 2.47329858e-06 1.36166896e-07 4.85011281e-06
 2.16884794e-06 1.29929494e-06]
ene_total = [1.67138562 1.10800963 1.73368868 1.48456874 1.09397392 1.1040216
 1.47961889 1.38247263 2.03845756 1.08123529]
ti_comp = [0.54825773 0.58461281 0.54425104 0.56025271 0.5855944  0.58474602
 0.56057165 0.56686532 0.52465853 0.58619944]
ti_coms = [0.10745239 0.0710973  0.11145908 0.09545741 0.07011572 0.0709641
 0.09513846 0.0888448  0.13105159 0.06951068]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.64641487e-06 6.25628775e-05 7.39575090e-06 2.91040345e-07
 9.60033089e-05 1.04577867e-05 5.75751087e-07 2.05076110e-05
 9.17048565e-06 5.49377638e-06]
ene_total = [0.45834474 0.30572102 0.4754127  0.40690231 0.30296238 0.30293225
 0.40555492 0.37957759 0.55900197 0.29652539]
optimize_network iter = 1 obj = 3.8929352611370294
eta = 0.6683136660986362
freqs = [30327342.94858875 59817206.41115168 30065677.9340177  10128218.84644976
 68956163.97393346 32948387.697912   12711921.71227484 41669829.72547627
 32697548.88052    26563493.52335151]
Done!
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.53273391e-06 6.16327412e-05 7.28579662e-06 2.86713384e-07
 9.45760062e-05 1.03023084e-05 5.67191267e-07 2.02027197e-05
 9.03414599e-06 5.41209917e-06]
ene_total = [0.01075277 0.00717136 0.01115319 0.00954603 0.00710615 0.00710671
 0.00951441 0.00890468 0.01311419 0.00695648]
At round 41 energy consumption: 0.09132598543578574
At round 41 eta: 0.6683136660986362
At round 41 a_n: 14.138223130512367
At round 41 local rounds: 13.19619071883817
At round 41 global rounds: 42.625280831488105
gradient difference: 0.37253957986831665
train() client id: f_00000-0-0 loss: 1.278771  [   32/  126]
train() client id: f_00000-0-1 loss: 0.846135  [   64/  126]
train() client id: f_00000-0-2 loss: 1.185882  [   96/  126]
train() client id: f_00000-1-0 loss: 0.883657  [   32/  126]
train() client id: f_00000-1-1 loss: 1.114068  [   64/  126]
train() client id: f_00000-1-2 loss: 0.747911  [   96/  126]
train() client id: f_00000-2-0 loss: 0.918401  [   32/  126]
train() client id: f_00000-2-1 loss: 0.781811  [   64/  126]
train() client id: f_00000-2-2 loss: 0.848590  [   96/  126]
train() client id: f_00000-3-0 loss: 0.663190  [   32/  126]
train() client id: f_00000-3-1 loss: 0.824951  [   64/  126]
train() client id: f_00000-3-2 loss: 0.747566  [   96/  126]
train() client id: f_00000-4-0 loss: 0.702238  [   32/  126]
train() client id: f_00000-4-1 loss: 0.743502  [   64/  126]
train() client id: f_00000-4-2 loss: 0.747681  [   96/  126]
train() client id: f_00000-5-0 loss: 0.717264  [   32/  126]
train() client id: f_00000-5-1 loss: 0.573810  [   64/  126]
train() client id: f_00000-5-2 loss: 0.697226  [   96/  126]
train() client id: f_00000-6-0 loss: 0.697684  [   32/  126]
train() client id: f_00000-6-1 loss: 0.614405  [   64/  126]
train() client id: f_00000-6-2 loss: 0.712877  [   96/  126]
train() client id: f_00000-7-0 loss: 0.716589  [   32/  126]
train() client id: f_00000-7-1 loss: 0.578371  [   64/  126]
train() client id: f_00000-7-2 loss: 0.594518  [   96/  126]
train() client id: f_00000-8-0 loss: 0.692758  [   32/  126]
train() client id: f_00000-8-1 loss: 0.635440  [   64/  126]
train() client id: f_00000-8-2 loss: 0.606429  [   96/  126]
train() client id: f_00000-9-0 loss: 0.569543  [   32/  126]
train() client id: f_00000-9-1 loss: 0.606854  [   64/  126]
train() client id: f_00000-9-2 loss: 0.719352  [   96/  126]
train() client id: f_00000-10-0 loss: 0.619234  [   32/  126]
train() client id: f_00000-10-1 loss: 0.706558  [   64/  126]
train() client id: f_00000-10-2 loss: 0.647037  [   96/  126]
train() client id: f_00000-11-0 loss: 0.645434  [   32/  126]
train() client id: f_00000-11-1 loss: 0.513553  [   64/  126]
train() client id: f_00000-11-2 loss: 0.688576  [   96/  126]
train() client id: f_00000-12-0 loss: 0.613418  [   32/  126]
train() client id: f_00000-12-1 loss: 0.682427  [   64/  126]
train() client id: f_00000-12-2 loss: 0.604916  [   96/  126]
train() client id: f_00001-0-0 loss: 0.457068  [   32/  265]
train() client id: f_00001-0-1 loss: 0.467390  [   64/  265]
train() client id: f_00001-0-2 loss: 0.561871  [   96/  265]
train() client id: f_00001-0-3 loss: 0.466713  [  128/  265]
train() client id: f_00001-0-4 loss: 0.512267  [  160/  265]
train() client id: f_00001-0-5 loss: 0.553803  [  192/  265]
train() client id: f_00001-0-6 loss: 0.517366  [  224/  265]
train() client id: f_00001-0-7 loss: 0.499997  [  256/  265]
train() client id: f_00001-1-0 loss: 0.597665  [   32/  265]
train() client id: f_00001-1-1 loss: 0.422222  [   64/  265]
train() client id: f_00001-1-2 loss: 0.500970  [   96/  265]
train() client id: f_00001-1-3 loss: 0.437452  [  128/  265]
train() client id: f_00001-1-4 loss: 0.415339  [  160/  265]
train() client id: f_00001-1-5 loss: 0.542845  [  192/  265]
train() client id: f_00001-1-6 loss: 0.617660  [  224/  265]
train() client id: f_00001-1-7 loss: 0.439193  [  256/  265]
train() client id: f_00001-2-0 loss: 0.438675  [   32/  265]
train() client id: f_00001-2-1 loss: 0.493211  [   64/  265]
train() client id: f_00001-2-2 loss: 0.657220  [   96/  265]
train() client id: f_00001-2-3 loss: 0.476370  [  128/  265]
train() client id: f_00001-2-4 loss: 0.441860  [  160/  265]
train() client id: f_00001-2-5 loss: 0.533851  [  192/  265]
train() client id: f_00001-2-6 loss: 0.448657  [  224/  265]
train() client id: f_00001-2-7 loss: 0.439086  [  256/  265]
train() client id: f_00001-3-0 loss: 0.420850  [   32/  265]
train() client id: f_00001-3-1 loss: 0.506964  [   64/  265]
train() client id: f_00001-3-2 loss: 0.446441  [   96/  265]
train() client id: f_00001-3-3 loss: 0.400960  [  128/  265]
train() client id: f_00001-3-4 loss: 0.557391  [  160/  265]
train() client id: f_00001-3-5 loss: 0.410382  [  192/  265]
train() client id: f_00001-3-6 loss: 0.483487  [  224/  265]
train() client id: f_00001-3-7 loss: 0.557223  [  256/  265]
train() client id: f_00001-4-0 loss: 0.541258  [   32/  265]
train() client id: f_00001-4-1 loss: 0.490943  [   64/  265]
train() client id: f_00001-4-2 loss: 0.389419  [   96/  265]
train() client id: f_00001-4-3 loss: 0.463707  [  128/  265]
train() client id: f_00001-4-4 loss: 0.475876  [  160/  265]
train() client id: f_00001-4-5 loss: 0.607297  [  192/  265]
train() client id: f_00001-4-6 loss: 0.444684  [  224/  265]
train() client id: f_00001-4-7 loss: 0.415457  [  256/  265]
train() client id: f_00001-5-0 loss: 0.402330  [   32/  265]
train() client id: f_00001-5-1 loss: 0.504939  [   64/  265]
train() client id: f_00001-5-2 loss: 0.405172  [   96/  265]
train() client id: f_00001-5-3 loss: 0.578327  [  128/  265]
train() client id: f_00001-5-4 loss: 0.482553  [  160/  265]
train() client id: f_00001-5-5 loss: 0.468431  [  192/  265]
train() client id: f_00001-5-6 loss: 0.526626  [  224/  265]
train() client id: f_00001-5-7 loss: 0.485728  [  256/  265]
train() client id: f_00001-6-0 loss: 0.499636  [   32/  265]
train() client id: f_00001-6-1 loss: 0.443443  [   64/  265]
train() client id: f_00001-6-2 loss: 0.437030  [   96/  265]
train() client id: f_00001-6-3 loss: 0.402045  [  128/  265]
train() client id: f_00001-6-4 loss: 0.558268  [  160/  265]
train() client id: f_00001-6-5 loss: 0.539657  [  192/  265]
train() client id: f_00001-6-6 loss: 0.567849  [  224/  265]
train() client id: f_00001-6-7 loss: 0.402496  [  256/  265]
train() client id: f_00001-7-0 loss: 0.452680  [   32/  265]
train() client id: f_00001-7-1 loss: 0.378690  [   64/  265]
train() client id: f_00001-7-2 loss: 0.418643  [   96/  265]
train() client id: f_00001-7-3 loss: 0.471602  [  128/  265]
train() client id: f_00001-7-4 loss: 0.595502  [  160/  265]
train() client id: f_00001-7-5 loss: 0.588162  [  192/  265]
train() client id: f_00001-7-6 loss: 0.379821  [  224/  265]
train() client id: f_00001-7-7 loss: 0.550355  [  256/  265]
train() client id: f_00001-8-0 loss: 0.513698  [   32/  265]
train() client id: f_00001-8-1 loss: 0.482487  [   64/  265]
train() client id: f_00001-8-2 loss: 0.511238  [   96/  265]
train() client id: f_00001-8-3 loss: 0.462184  [  128/  265]
train() client id: f_00001-8-4 loss: 0.545198  [  160/  265]
train() client id: f_00001-8-5 loss: 0.411355  [  192/  265]
train() client id: f_00001-8-6 loss: 0.477437  [  224/  265]
train() client id: f_00001-8-7 loss: 0.381438  [  256/  265]
train() client id: f_00001-9-0 loss: 0.445809  [   32/  265]
train() client id: f_00001-9-1 loss: 0.380218  [   64/  265]
train() client id: f_00001-9-2 loss: 0.470486  [   96/  265]
train() client id: f_00001-9-3 loss: 0.401106  [  128/  265]
train() client id: f_00001-9-4 loss: 0.593950  [  160/  265]
train() client id: f_00001-9-5 loss: 0.414262  [  192/  265]
train() client id: f_00001-9-6 loss: 0.476005  [  224/  265]
train() client id: f_00001-9-7 loss: 0.636434  [  256/  265]
train() client id: f_00001-10-0 loss: 0.371443  [   32/  265]
train() client id: f_00001-10-1 loss: 0.460837  [   64/  265]
train() client id: f_00001-10-2 loss: 0.497661  [   96/  265]
train() client id: f_00001-10-3 loss: 0.578883  [  128/  265]
train() client id: f_00001-10-4 loss: 0.380806  [  160/  265]
train() client id: f_00001-10-5 loss: 0.565861  [  192/  265]
train() client id: f_00001-10-6 loss: 0.540336  [  224/  265]
train() client id: f_00001-10-7 loss: 0.407231  [  256/  265]
train() client id: f_00001-11-0 loss: 0.498949  [   32/  265]
train() client id: f_00001-11-1 loss: 0.551848  [   64/  265]
train() client id: f_00001-11-2 loss: 0.550743  [   96/  265]
train() client id: f_00001-11-3 loss: 0.373601  [  128/  265]
train() client id: f_00001-11-4 loss: 0.449848  [  160/  265]
train() client id: f_00001-11-5 loss: 0.521007  [  192/  265]
train() client id: f_00001-11-6 loss: 0.433848  [  224/  265]
train() client id: f_00001-11-7 loss: 0.459920  [  256/  265]
train() client id: f_00001-12-0 loss: 0.393526  [   32/  265]
train() client id: f_00001-12-1 loss: 0.425638  [   64/  265]
train() client id: f_00001-12-2 loss: 0.507209  [   96/  265]
train() client id: f_00001-12-3 loss: 0.371502  [  128/  265]
train() client id: f_00001-12-4 loss: 0.622690  [  160/  265]
train() client id: f_00001-12-5 loss: 0.477178  [  192/  265]
train() client id: f_00001-12-6 loss: 0.527538  [  224/  265]
train() client id: f_00001-12-7 loss: 0.523334  [  256/  265]
train() client id: f_00002-0-0 loss: 1.461535  [   32/  124]
train() client id: f_00002-0-1 loss: 1.125019  [   64/  124]
train() client id: f_00002-0-2 loss: 1.294085  [   96/  124]
train() client id: f_00002-1-0 loss: 1.200033  [   32/  124]
train() client id: f_00002-1-1 loss: 1.368669  [   64/  124]
train() client id: f_00002-1-2 loss: 1.167841  [   96/  124]
train() client id: f_00002-2-0 loss: 1.229843  [   32/  124]
train() client id: f_00002-2-1 loss: 1.311177  [   64/  124]
train() client id: f_00002-2-2 loss: 1.090296  [   96/  124]
train() client id: f_00002-3-0 loss: 1.256300  [   32/  124]
train() client id: f_00002-3-1 loss: 1.065546  [   64/  124]
train() client id: f_00002-3-2 loss: 1.070885  [   96/  124]
train() client id: f_00002-4-0 loss: 1.390508  [   32/  124]
train() client id: f_00002-4-1 loss: 0.806980  [   64/  124]
train() client id: f_00002-4-2 loss: 1.070343  [   96/  124]
train() client id: f_00002-5-0 loss: 1.002276  [   32/  124]
train() client id: f_00002-5-1 loss: 1.139912  [   64/  124]
train() client id: f_00002-5-2 loss: 1.019566  [   96/  124]
train() client id: f_00002-6-0 loss: 1.013170  [   32/  124]
train() client id: f_00002-6-1 loss: 1.090572  [   64/  124]
train() client id: f_00002-6-2 loss: 1.129090  [   96/  124]
train() client id: f_00002-7-0 loss: 0.984678  [   32/  124]
train() client id: f_00002-7-1 loss: 0.929908  [   64/  124]
train() client id: f_00002-7-2 loss: 0.937843  [   96/  124]
train() client id: f_00002-8-0 loss: 0.967982  [   32/  124]
train() client id: f_00002-8-1 loss: 0.921906  [   64/  124]
train() client id: f_00002-8-2 loss: 0.907241  [   96/  124]
train() client id: f_00002-9-0 loss: 0.967548  [   32/  124]
train() client id: f_00002-9-1 loss: 0.896894  [   64/  124]
train() client id: f_00002-9-2 loss: 1.132778  [   96/  124]
train() client id: f_00002-10-0 loss: 0.858273  [   32/  124]
train() client id: f_00002-10-1 loss: 0.899893  [   64/  124]
train() client id: f_00002-10-2 loss: 1.122565  [   96/  124]
train() client id: f_00002-11-0 loss: 0.929517  [   32/  124]
train() client id: f_00002-11-1 loss: 0.960363  [   64/  124]
train() client id: f_00002-11-2 loss: 0.968259  [   96/  124]
train() client id: f_00002-12-0 loss: 0.915303  [   32/  124]
train() client id: f_00002-12-1 loss: 0.992966  [   64/  124]
train() client id: f_00002-12-2 loss: 0.874451  [   96/  124]
train() client id: f_00003-0-0 loss: 0.675702  [   32/   43]
train() client id: f_00003-1-0 loss: 0.714267  [   32/   43]
train() client id: f_00003-2-0 loss: 0.558823  [   32/   43]
train() client id: f_00003-3-0 loss: 0.668405  [   32/   43]
train() client id: f_00003-4-0 loss: 0.629324  [   32/   43]
train() client id: f_00003-5-0 loss: 0.561194  [   32/   43]
train() client id: f_00003-6-0 loss: 0.663000  [   32/   43]
train() client id: f_00003-7-0 loss: 0.633025  [   32/   43]
train() client id: f_00003-8-0 loss: 0.412427  [   32/   43]
train() client id: f_00003-9-0 loss: 0.724506  [   32/   43]
train() client id: f_00003-10-0 loss: 0.575498  [   32/   43]
train() client id: f_00003-11-0 loss: 0.580740  [   32/   43]
train() client id: f_00003-12-0 loss: 0.532592  [   32/   43]
train() client id: f_00004-0-0 loss: 0.883394  [   32/  306]
train() client id: f_00004-0-1 loss: 1.009763  [   64/  306]
train() client id: f_00004-0-2 loss: 0.859611  [   96/  306]
train() client id: f_00004-0-3 loss: 0.960409  [  128/  306]
train() client id: f_00004-0-4 loss: 0.793057  [  160/  306]
train() client id: f_00004-0-5 loss: 0.806276  [  192/  306]
train() client id: f_00004-0-6 loss: 0.845410  [  224/  306]
train() client id: f_00004-0-7 loss: 0.910803  [  256/  306]
train() client id: f_00004-0-8 loss: 0.945557  [  288/  306]
train() client id: f_00004-1-0 loss: 0.827819  [   32/  306]
train() client id: f_00004-1-1 loss: 0.952060  [   64/  306]
train() client id: f_00004-1-2 loss: 0.807068  [   96/  306]
train() client id: f_00004-1-3 loss: 0.811913  [  128/  306]
train() client id: f_00004-1-4 loss: 0.723009  [  160/  306]
train() client id: f_00004-1-5 loss: 0.950038  [  192/  306]
train() client id: f_00004-1-6 loss: 0.952651  [  224/  306]
train() client id: f_00004-1-7 loss: 0.935747  [  256/  306]
train() client id: f_00004-1-8 loss: 0.944965  [  288/  306]
train() client id: f_00004-2-0 loss: 0.736742  [   32/  306]
train() client id: f_00004-2-1 loss: 0.845438  [   64/  306]
train() client id: f_00004-2-2 loss: 0.914758  [   96/  306]
train() client id: f_00004-2-3 loss: 0.925926  [  128/  306]
train() client id: f_00004-2-4 loss: 0.914303  [  160/  306]
train() client id: f_00004-2-5 loss: 0.777900  [  192/  306]
train() client id: f_00004-2-6 loss: 1.039234  [  224/  306]
train() client id: f_00004-2-7 loss: 0.886046  [  256/  306]
train() client id: f_00004-2-8 loss: 0.845195  [  288/  306]
train() client id: f_00004-3-0 loss: 0.906597  [   32/  306]
train() client id: f_00004-3-1 loss: 0.955784  [   64/  306]
train() client id: f_00004-3-2 loss: 0.848266  [   96/  306]
train() client id: f_00004-3-3 loss: 0.899039  [  128/  306]
train() client id: f_00004-3-4 loss: 0.938427  [  160/  306]
train() client id: f_00004-3-5 loss: 0.825582  [  192/  306]
train() client id: f_00004-3-6 loss: 0.840989  [  224/  306]
train() client id: f_00004-3-7 loss: 0.956895  [  256/  306]
train() client id: f_00004-3-8 loss: 0.783603  [  288/  306]
train() client id: f_00004-4-0 loss: 1.049004  [   32/  306]
train() client id: f_00004-4-1 loss: 0.801872  [   64/  306]
train() client id: f_00004-4-2 loss: 0.852647  [   96/  306]
train() client id: f_00004-4-3 loss: 0.791142  [  128/  306]
train() client id: f_00004-4-4 loss: 0.891670  [  160/  306]
train() client id: f_00004-4-5 loss: 0.836007  [  192/  306]
train() client id: f_00004-4-6 loss: 0.833733  [  224/  306]
train() client id: f_00004-4-7 loss: 0.945126  [  256/  306]
train() client id: f_00004-4-8 loss: 0.880468  [  288/  306]
train() client id: f_00004-5-0 loss: 0.984367  [   32/  306]
train() client id: f_00004-5-1 loss: 0.994823  [   64/  306]
train() client id: f_00004-5-2 loss: 0.954737  [   96/  306]
train() client id: f_00004-5-3 loss: 0.840807  [  128/  306]
train() client id: f_00004-5-4 loss: 0.947789  [  160/  306]
train() client id: f_00004-5-5 loss: 0.823264  [  192/  306]
train() client id: f_00004-5-6 loss: 0.857979  [  224/  306]
train() client id: f_00004-5-7 loss: 0.816684  [  256/  306]
train() client id: f_00004-5-8 loss: 0.679031  [  288/  306]
train() client id: f_00004-6-0 loss: 0.801629  [   32/  306]
train() client id: f_00004-6-1 loss: 0.834236  [   64/  306]
train() client id: f_00004-6-2 loss: 0.936911  [   96/  306]
train() client id: f_00004-6-3 loss: 0.872835  [  128/  306]
train() client id: f_00004-6-4 loss: 0.944045  [  160/  306]
train() client id: f_00004-6-5 loss: 0.889625  [  192/  306]
train() client id: f_00004-6-6 loss: 0.841222  [  224/  306]
train() client id: f_00004-6-7 loss: 0.924535  [  256/  306]
train() client id: f_00004-6-8 loss: 0.875393  [  288/  306]
train() client id: f_00004-7-0 loss: 0.893490  [   32/  306]
train() client id: f_00004-7-1 loss: 0.908321  [   64/  306]
train() client id: f_00004-7-2 loss: 0.870186  [   96/  306]
train() client id: f_00004-7-3 loss: 0.890120  [  128/  306]
train() client id: f_00004-7-4 loss: 0.791469  [  160/  306]
train() client id: f_00004-7-5 loss: 0.988720  [  192/  306]
train() client id: f_00004-7-6 loss: 0.803567  [  224/  306]
train() client id: f_00004-7-7 loss: 0.993028  [  256/  306]
train() client id: f_00004-7-8 loss: 0.848296  [  288/  306]
train() client id: f_00004-8-0 loss: 0.923769  [   32/  306]
train() client id: f_00004-8-1 loss: 0.889570  [   64/  306]
train() client id: f_00004-8-2 loss: 0.795406  [   96/  306]
train() client id: f_00004-8-3 loss: 0.798172  [  128/  306]
train() client id: f_00004-8-4 loss: 0.912258  [  160/  306]
train() client id: f_00004-8-5 loss: 0.869861  [  192/  306]
train() client id: f_00004-8-6 loss: 0.858277  [  224/  306]
train() client id: f_00004-8-7 loss: 0.865538  [  256/  306]
train() client id: f_00004-8-8 loss: 0.920291  [  288/  306]
train() client id: f_00004-9-0 loss: 0.831953  [   32/  306]
train() client id: f_00004-9-1 loss: 0.888401  [   64/  306]
train() client id: f_00004-9-2 loss: 0.925094  [   96/  306]
train() client id: f_00004-9-3 loss: 0.893296  [  128/  306]
train() client id: f_00004-9-4 loss: 0.833668  [  160/  306]
train() client id: f_00004-9-5 loss: 0.970325  [  192/  306]
train() client id: f_00004-9-6 loss: 0.931240  [  224/  306]
train() client id: f_00004-9-7 loss: 0.805573  [  256/  306]
train() client id: f_00004-9-8 loss: 0.882016  [  288/  306]
train() client id: f_00004-10-0 loss: 0.880364  [   32/  306]
train() client id: f_00004-10-1 loss: 0.881723  [   64/  306]
train() client id: f_00004-10-2 loss: 0.908187  [   96/  306]
train() client id: f_00004-10-3 loss: 0.875645  [  128/  306]
train() client id: f_00004-10-4 loss: 0.841608  [  160/  306]
train() client id: f_00004-10-5 loss: 0.929622  [  192/  306]
train() client id: f_00004-10-6 loss: 0.988982  [  224/  306]
train() client id: f_00004-10-7 loss: 0.737572  [  256/  306]
train() client id: f_00004-10-8 loss: 0.812866  [  288/  306]
train() client id: f_00004-11-0 loss: 0.975406  [   32/  306]
train() client id: f_00004-11-1 loss: 0.883209  [   64/  306]
train() client id: f_00004-11-2 loss: 0.897147  [   96/  306]
train() client id: f_00004-11-3 loss: 0.863985  [  128/  306]
train() client id: f_00004-11-4 loss: 0.838590  [  160/  306]
train() client id: f_00004-11-5 loss: 0.977663  [  192/  306]
train() client id: f_00004-11-6 loss: 0.879632  [  224/  306]
train() client id: f_00004-11-7 loss: 0.843149  [  256/  306]
train() client id: f_00004-11-8 loss: 0.777890  [  288/  306]
train() client id: f_00004-12-0 loss: 0.806496  [   32/  306]
train() client id: f_00004-12-1 loss: 0.816309  [   64/  306]
train() client id: f_00004-12-2 loss: 0.859603  [   96/  306]
train() client id: f_00004-12-3 loss: 0.906173  [  128/  306]
train() client id: f_00004-12-4 loss: 0.940941  [  160/  306]
train() client id: f_00004-12-5 loss: 0.878054  [  192/  306]
train() client id: f_00004-12-6 loss: 0.888225  [  224/  306]
train() client id: f_00004-12-7 loss: 0.879540  [  256/  306]
train() client id: f_00004-12-8 loss: 0.917488  [  288/  306]
train() client id: f_00005-0-0 loss: 0.733337  [   32/  146]
train() client id: f_00005-0-1 loss: 0.538789  [   64/  146]
train() client id: f_00005-0-2 loss: 0.827731  [   96/  146]
train() client id: f_00005-0-3 loss: 0.416577  [  128/  146]
train() client id: f_00005-1-0 loss: 0.520569  [   32/  146]
train() client id: f_00005-1-1 loss: 0.692289  [   64/  146]
train() client id: f_00005-1-2 loss: 0.689470  [   96/  146]
train() client id: f_00005-1-3 loss: 0.672853  [  128/  146]
train() client id: f_00005-2-0 loss: 0.479463  [   32/  146]
train() client id: f_00005-2-1 loss: 0.587981  [   64/  146]
train() client id: f_00005-2-2 loss: 1.084215  [   96/  146]
train() client id: f_00005-2-3 loss: 0.550753  [  128/  146]
train() client id: f_00005-3-0 loss: 0.941729  [   32/  146]
train() client id: f_00005-3-1 loss: 0.581250  [   64/  146]
train() client id: f_00005-3-2 loss: 0.718773  [   96/  146]
train() client id: f_00005-3-3 loss: 0.601379  [  128/  146]
train() client id: f_00005-4-0 loss: 0.542914  [   32/  146]
train() client id: f_00005-4-1 loss: 0.472007  [   64/  146]
train() client id: f_00005-4-2 loss: 0.576932  [   96/  146]
train() client id: f_00005-4-3 loss: 0.852065  [  128/  146]
train() client id: f_00005-5-0 loss: 0.562193  [   32/  146]
train() client id: f_00005-5-1 loss: 0.601965  [   64/  146]
train() client id: f_00005-5-2 loss: 0.819401  [   96/  146]
train() client id: f_00005-5-3 loss: 0.594386  [  128/  146]
train() client id: f_00005-6-0 loss: 0.725830  [   32/  146]
train() client id: f_00005-6-1 loss: 0.782538  [   64/  146]
train() client id: f_00005-6-2 loss: 0.603878  [   96/  146]
train() client id: f_00005-6-3 loss: 0.611955  [  128/  146]
train() client id: f_00005-7-0 loss: 0.580202  [   32/  146]
train() client id: f_00005-7-1 loss: 0.669445  [   64/  146]
train() client id: f_00005-7-2 loss: 1.016426  [   96/  146]
train() client id: f_00005-7-3 loss: 0.489962  [  128/  146]
train() client id: f_00005-8-0 loss: 0.826075  [   32/  146]
train() client id: f_00005-8-1 loss: 0.459665  [   64/  146]
train() client id: f_00005-8-2 loss: 0.657373  [   96/  146]
train() client id: f_00005-8-3 loss: 0.739691  [  128/  146]
train() client id: f_00005-9-0 loss: 0.400671  [   32/  146]
train() client id: f_00005-9-1 loss: 0.608552  [   64/  146]
train() client id: f_00005-9-2 loss: 0.717877  [   96/  146]
train() client id: f_00005-9-3 loss: 0.699395  [  128/  146]
train() client id: f_00005-10-0 loss: 0.679028  [   32/  146]
train() client id: f_00005-10-1 loss: 0.983003  [   64/  146]
train() client id: f_00005-10-2 loss: 0.423962  [   96/  146]
train() client id: f_00005-10-3 loss: 0.398505  [  128/  146]
train() client id: f_00005-11-0 loss: 0.850147  [   32/  146]
train() client id: f_00005-11-1 loss: 0.797103  [   64/  146]
train() client id: f_00005-11-2 loss: 0.553358  [   96/  146]
train() client id: f_00005-11-3 loss: 0.562379  [  128/  146]
train() client id: f_00005-12-0 loss: 0.850904  [   32/  146]
train() client id: f_00005-12-1 loss: 0.602961  [   64/  146]
train() client id: f_00005-12-2 loss: 0.512748  [   96/  146]
train() client id: f_00005-12-3 loss: 0.606332  [  128/  146]
train() client id: f_00006-0-0 loss: 0.434729  [   32/   54]
train() client id: f_00006-1-0 loss: 0.505076  [   32/   54]
train() client id: f_00006-2-0 loss: 0.423922  [   32/   54]
train() client id: f_00006-3-0 loss: 0.451369  [   32/   54]
train() client id: f_00006-4-0 loss: 0.457318  [   32/   54]
train() client id: f_00006-5-0 loss: 0.385430  [   32/   54]
train() client id: f_00006-6-0 loss: 0.432495  [   32/   54]
train() client id: f_00006-7-0 loss: 0.388719  [   32/   54]
train() client id: f_00006-8-0 loss: 0.396427  [   32/   54]
train() client id: f_00006-9-0 loss: 0.451658  [   32/   54]
train() client id: f_00006-10-0 loss: 0.386708  [   32/   54]
train() client id: f_00006-11-0 loss: 0.426768  [   32/   54]
train() client id: f_00006-12-0 loss: 0.434907  [   32/   54]
train() client id: f_00007-0-0 loss: 0.477233  [   32/  179]
train() client id: f_00007-0-1 loss: 0.295844  [   64/  179]
train() client id: f_00007-0-2 loss: 0.701224  [   96/  179]
train() client id: f_00007-0-3 loss: 0.365156  [  128/  179]
train() client id: f_00007-0-4 loss: 0.521996  [  160/  179]
train() client id: f_00007-1-0 loss: 0.505036  [   32/  179]
train() client id: f_00007-1-1 loss: 0.332795  [   64/  179]
train() client id: f_00007-1-2 loss: 0.381102  [   96/  179]
train() client id: f_00007-1-3 loss: 0.359186  [  128/  179]
train() client id: f_00007-1-4 loss: 0.676014  [  160/  179]
train() client id: f_00007-2-0 loss: 0.305925  [   32/  179]
train() client id: f_00007-2-1 loss: 0.624382  [   64/  179]
train() client id: f_00007-2-2 loss: 0.281442  [   96/  179]
train() client id: f_00007-2-3 loss: 0.516376  [  128/  179]
train() client id: f_00007-2-4 loss: 0.576720  [  160/  179]
train() client id: f_00007-3-0 loss: 0.389156  [   32/  179]
train() client id: f_00007-3-1 loss: 0.479078  [   64/  179]
train() client id: f_00007-3-2 loss: 0.506349  [   96/  179]
train() client id: f_00007-3-3 loss: 0.528322  [  128/  179]
train() client id: f_00007-3-4 loss: 0.363127  [  160/  179]
train() client id: f_00007-4-0 loss: 0.332942  [   32/  179]
train() client id: f_00007-4-1 loss: 0.267890  [   64/  179]
train() client id: f_00007-4-2 loss: 0.427291  [   96/  179]
train() client id: f_00007-4-3 loss: 0.839549  [  128/  179]
train() client id: f_00007-4-4 loss: 0.255855  [  160/  179]
train() client id: f_00007-5-0 loss: 0.534220  [   32/  179]
train() client id: f_00007-5-1 loss: 0.391490  [   64/  179]
train() client id: f_00007-5-2 loss: 0.327265  [   96/  179]
train() client id: f_00007-5-3 loss: 0.377729  [  128/  179]
train() client id: f_00007-5-4 loss: 0.560370  [  160/  179]
train() client id: f_00007-6-0 loss: 0.379066  [   32/  179]
train() client id: f_00007-6-1 loss: 0.288324  [   64/  179]
train() client id: f_00007-6-2 loss: 0.288979  [   96/  179]
train() client id: f_00007-6-3 loss: 0.339878  [  128/  179]
train() client id: f_00007-6-4 loss: 0.681186  [  160/  179]
train() client id: f_00007-7-0 loss: 0.328674  [   32/  179]
train() client id: f_00007-7-1 loss: 0.421512  [   64/  179]
train() client id: f_00007-7-2 loss: 0.494592  [   96/  179]
train() client id: f_00007-7-3 loss: 0.444198  [  128/  179]
train() client id: f_00007-7-4 loss: 0.257775  [  160/  179]
train() client id: f_00007-8-0 loss: 0.286608  [   32/  179]
train() client id: f_00007-8-1 loss: 0.486743  [   64/  179]
train() client id: f_00007-8-2 loss: 0.590085  [   96/  179]
train() client id: f_00007-8-3 loss: 0.311283  [  128/  179]
train() client id: f_00007-8-4 loss: 0.329559  [  160/  179]
train() client id: f_00007-9-0 loss: 0.353792  [   32/  179]
train() client id: f_00007-9-1 loss: 0.331822  [   64/  179]
train() client id: f_00007-9-2 loss: 0.525817  [   96/  179]
train() client id: f_00007-9-3 loss: 0.445027  [  128/  179]
train() client id: f_00007-9-4 loss: 0.213041  [  160/  179]
train() client id: f_00007-10-0 loss: 0.342814  [   32/  179]
train() client id: f_00007-10-1 loss: 0.748758  [   64/  179]
train() client id: f_00007-10-2 loss: 0.375913  [   96/  179]
train() client id: f_00007-10-3 loss: 0.314732  [  128/  179]
train() client id: f_00007-10-4 loss: 0.344890  [  160/  179]
train() client id: f_00007-11-0 loss: 0.533370  [   32/  179]
train() client id: f_00007-11-1 loss: 0.260814  [   64/  179]
train() client id: f_00007-11-2 loss: 0.532004  [   96/  179]
train() client id: f_00007-11-3 loss: 0.341491  [  128/  179]
train() client id: f_00007-11-4 loss: 0.247223  [  160/  179]
train() client id: f_00007-12-0 loss: 0.330794  [   32/  179]
train() client id: f_00007-12-1 loss: 0.350875  [   64/  179]
train() client id: f_00007-12-2 loss: 0.442339  [   96/  179]
train() client id: f_00007-12-3 loss: 0.517620  [  128/  179]
train() client id: f_00007-12-4 loss: 0.334482  [  160/  179]
train() client id: f_00008-0-0 loss: 0.718871  [   32/  130]
train() client id: f_00008-0-1 loss: 0.715162  [   64/  130]
train() client id: f_00008-0-2 loss: 0.811417  [   96/  130]
train() client id: f_00008-0-3 loss: 0.661314  [  128/  130]
train() client id: f_00008-1-0 loss: 0.812521  [   32/  130]
train() client id: f_00008-1-1 loss: 0.668153  [   64/  130]
train() client id: f_00008-1-2 loss: 0.718635  [   96/  130]
train() client id: f_00008-1-3 loss: 0.709902  [  128/  130]
train() client id: f_00008-2-0 loss: 0.650389  [   32/  130]
train() client id: f_00008-2-1 loss: 0.760026  [   64/  130]
train() client id: f_00008-2-2 loss: 0.785191  [   96/  130]
train() client id: f_00008-2-3 loss: 0.683078  [  128/  130]
train() client id: f_00008-3-0 loss: 0.777325  [   32/  130]
train() client id: f_00008-3-1 loss: 0.655182  [   64/  130]
train() client id: f_00008-3-2 loss: 0.731180  [   96/  130]
train() client id: f_00008-3-3 loss: 0.705008  [  128/  130]
train() client id: f_00008-4-0 loss: 0.694263  [   32/  130]
train() client id: f_00008-4-1 loss: 0.732238  [   64/  130]
train() client id: f_00008-4-2 loss: 0.686016  [   96/  130]
train() client id: f_00008-4-3 loss: 0.758048  [  128/  130]
train() client id: f_00008-5-0 loss: 0.669717  [   32/  130]
train() client id: f_00008-5-1 loss: 0.680691  [   64/  130]
train() client id: f_00008-5-2 loss: 0.763437  [   96/  130]
train() client id: f_00008-5-3 loss: 0.798617  [  128/  130]
train() client id: f_00008-6-0 loss: 0.708688  [   32/  130]
train() client id: f_00008-6-1 loss: 0.829490  [   64/  130]
train() client id: f_00008-6-2 loss: 0.687675  [   96/  130]
train() client id: f_00008-6-3 loss: 0.670118  [  128/  130]
train() client id: f_00008-7-0 loss: 0.715267  [   32/  130]
train() client id: f_00008-7-1 loss: 0.766813  [   64/  130]
train() client id: f_00008-7-2 loss: 0.787816  [   96/  130]
train() client id: f_00008-7-3 loss: 0.638413  [  128/  130]
train() client id: f_00008-8-0 loss: 0.671741  [   32/  130]
train() client id: f_00008-8-1 loss: 0.602870  [   64/  130]
train() client id: f_00008-8-2 loss: 0.841090  [   96/  130]
train() client id: f_00008-8-3 loss: 0.763876  [  128/  130]
train() client id: f_00008-9-0 loss: 0.767423  [   32/  130]
train() client id: f_00008-9-1 loss: 0.715589  [   64/  130]
train() client id: f_00008-9-2 loss: 0.714651  [   96/  130]
train() client id: f_00008-9-3 loss: 0.706854  [  128/  130]
train() client id: f_00008-10-0 loss: 0.680731  [   32/  130]
train() client id: f_00008-10-1 loss: 0.643156  [   64/  130]
train() client id: f_00008-10-2 loss: 0.781167  [   96/  130]
train() client id: f_00008-10-3 loss: 0.808137  [  128/  130]
train() client id: f_00008-11-0 loss: 0.809289  [   32/  130]
train() client id: f_00008-11-1 loss: 0.794024  [   64/  130]
train() client id: f_00008-11-2 loss: 0.714636  [   96/  130]
train() client id: f_00008-11-3 loss: 0.585163  [  128/  130]
train() client id: f_00008-12-0 loss: 0.792630  [   32/  130]
train() client id: f_00008-12-1 loss: 0.794359  [   64/  130]
train() client id: f_00008-12-2 loss: 0.582652  [   96/  130]
train() client id: f_00008-12-3 loss: 0.735960  [  128/  130]
train() client id: f_00009-0-0 loss: 1.017030  [   32/  118]
train() client id: f_00009-0-1 loss: 1.062459  [   64/  118]
train() client id: f_00009-0-2 loss: 1.121118  [   96/  118]
train() client id: f_00009-1-0 loss: 0.855984  [   32/  118]
train() client id: f_00009-1-1 loss: 1.210497  [   64/  118]
train() client id: f_00009-1-2 loss: 1.016709  [   96/  118]
train() client id: f_00009-2-0 loss: 0.972106  [   32/  118]
train() client id: f_00009-2-1 loss: 1.072186  [   64/  118]
train() client id: f_00009-2-2 loss: 0.899273  [   96/  118]
train() client id: f_00009-3-0 loss: 0.975648  [   32/  118]
train() client id: f_00009-3-1 loss: 0.939389  [   64/  118]
train() client id: f_00009-3-2 loss: 0.955383  [   96/  118]
train() client id: f_00009-4-0 loss: 0.886358  [   32/  118]
train() client id: f_00009-4-1 loss: 0.748242  [   64/  118]
train() client id: f_00009-4-2 loss: 1.003480  [   96/  118]
train() client id: f_00009-5-0 loss: 1.021640  [   32/  118]
train() client id: f_00009-5-1 loss: 0.741467  [   64/  118]
train() client id: f_00009-5-2 loss: 0.952891  [   96/  118]
train() client id: f_00009-6-0 loss: 0.856654  [   32/  118]
train() client id: f_00009-6-1 loss: 0.996783  [   64/  118]
train() client id: f_00009-6-2 loss: 0.755796  [   96/  118]
train() client id: f_00009-7-0 loss: 0.852133  [   32/  118]
train() client id: f_00009-7-1 loss: 0.834087  [   64/  118]
train() client id: f_00009-7-2 loss: 0.961095  [   96/  118]
train() client id: f_00009-8-0 loss: 0.695175  [   32/  118]
train() client id: f_00009-8-1 loss: 0.732448  [   64/  118]
train() client id: f_00009-8-2 loss: 0.937915  [   96/  118]
train() client id: f_00009-9-0 loss: 0.932789  [   32/  118]
train() client id: f_00009-9-1 loss: 0.726646  [   64/  118]
train() client id: f_00009-9-2 loss: 0.813078  [   96/  118]
train() client id: f_00009-10-0 loss: 0.776273  [   32/  118]
train() client id: f_00009-10-1 loss: 0.796770  [   64/  118]
train() client id: f_00009-10-2 loss: 0.790094  [   96/  118]
train() client id: f_00009-11-0 loss: 0.887187  [   32/  118]
train() client id: f_00009-11-1 loss: 0.868006  [   64/  118]
train() client id: f_00009-11-2 loss: 0.816362  [   96/  118]
train() client id: f_00009-12-0 loss: 0.747958  [   32/  118]
train() client id: f_00009-12-1 loss: 0.766408  [   64/  118]
train() client id: f_00009-12-2 loss: 0.803626  [   96/  118]
At round 41 accuracy: 0.6419098143236074
At round 41 training accuracy: 0.5902079141515761
At round 41 training loss: 0.827198578004213
update_location
xs = -3.905658 4.200318 225.009024 18.811294 0.979296 3.956410 -187.443192 -166.324852 209.663977 -152.060879 
ys = 217.587959 200.555839 1.320614 -187.455176 179.350187 162.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -4.711426168117436
ys mean: 59.3941425355287
dists_uav = 239.499007 224.143452 246.233233 213.291603 205.347142 191.112796 212.466093 194.073781 232.954187 182.039893 
uav_gains = -110.431879 -109.279725 -110.979226 -108.537441 -108.021163 -107.125430 -108.482905 -107.310399 -109.924643 -106.556698 
uav_gains_db_mean: -108.66495087706255
dists_bs = 176.315082 181.013411 436.086859 411.018944 176.033056 179.370821 178.060293 174.393557 415.633123 172.530277 
bs_gains = -102.463164 -102.782960 -113.475103 -112.755191 -102.443698 -102.672109 -102.582938 -102.329911 -112.890943 -102.199288 
bs_gains_db_mean: -105.65953053886098
Round 42
-------------------------------
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.44885328 11.23645857  5.37086954  1.94257213 12.9580199   6.23472006
  2.4044106   7.64858783  5.64816701  5.05583458]
obj_prev = 63.948493486950056
eta_min = 1.1938832184881957e-17	eta_max = 0.9337601214025776
af = 13.47090363962996	bf = 1.273204732538192	zeta = 14.817994003592956	eta = 0.9090909090909091
af = 13.47090363962996	bf = 1.273204732538192	zeta = 28.128308786138636	eta = 0.47890912112953954
af = 13.47090363962996	bf = 1.273204732538192	zeta = 21.476258560043068	eta = 0.6272462962749386
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.271427562519737	eta = 0.6645266396796146
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.2057708129807	eta = 0.666685956418743
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.20555837007598	eta = 0.6666929660097932
eta = 0.6666929660097932
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [0.03345475 0.07036119 0.03292373 0.0114171  0.08124726 0.03876503
 0.01433775 0.04752699 0.03451681 0.03133064]
ene_total = [1.84044551 3.20713505 1.83649691 0.87474524 3.65310778 1.89784042
 0.99426624 2.34061661 1.97709129 1.58381332]
ti_comp = [0.56395803 0.60282675 0.55960175 0.5768971  0.60393932 0.60319406
 0.57722705 0.58383781 0.54156463 0.6047199 ]
ti_coms = [0.11010873 0.07124001 0.11446502 0.09716967 0.07012744 0.0708727
 0.09683971 0.09022895 0.13250214 0.06934686]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.35800290e-06 5.99093333e-05 7.12276845e-06 2.79479884e-07
 9.19008568e-05 1.00065955e-05 5.52878649e-07 1.96841446e-05
 8.76337451e-06 5.25629088e-06]
ene_total = [0.45604785 0.2973442  0.47406892 0.4021997  0.29406337 0.29375838
 0.40084532 0.37427503 0.548793   0.28724627]
optimize_network iter = 0 obj = 3.828642052039432
eta = 0.6666929660097932
freqs = [29660676.66717937 58359378.23819214 29417104.30254062  9895264.30741952
 67264422.3037599  32133134.64789411 12419507.6458075  40702220.97084436
 31867673.54934066 25905086.18443851]
eta_min = 0.6666929660097993	eta_max = 0.6666929660097998
af = 0.007174173529547853	bf = 1.273204732538192	zeta = 0.007891590882502639	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [1.72976861e-06 1.40838874e-05 1.67446812e-06 6.57020031e-08
 2.16046690e-05 2.35241749e-06 1.29974416e-07 4.62748057e-06
 2.06015278e-06 1.23568407e-06]
ene_total = [1.67118944 1.08322193 1.73728876 1.47458345 1.06747966 1.07586755
 1.46958603 1.36994863 2.01106501 1.0525431 ]
ti_comp = [0.56395803 0.60282675 0.55960175 0.5768971  0.60393932 0.60319406
 0.57722705 0.58383781 0.54156463 0.6047199 ]
ti_coms = [0.11010873 0.07124001 0.11446502 0.09716967 0.07012744 0.0708727
 0.09683971 0.09022895 0.13250214 0.06934686]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.35800290e-06 5.99093333e-05 7.12276845e-06 2.79479884e-07
 9.19008568e-05 1.00065955e-05 5.52878649e-07 1.96841446e-05
 8.76337451e-06 5.25629088e-06]
ene_total = [0.45604785 0.2973442  0.47406892 0.4021997  0.29406337 0.29375838
 0.40084532 0.37427503 0.548793   0.28724627]
optimize_network iter = 1 obj = 3.8286420520395077
eta = 0.6666929660097998
freqs = [29660676.66717935 58359378.23819202 29417104.30254061  9895264.30741951
 67264422.30375975 32133134.64789404 12419507.64580748 40702220.97084429
 31867673.54934067 25905086.18443845]
Done!
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.20519951e-06 5.86651983e-05 6.97485017e-06 2.73675935e-07
 8.99923549e-05 9.79878887e-06 5.41397037e-07 1.92753647e-05
 8.58138582e-06 5.14713367e-06]
ene_total = [0.01101808 0.00718267 0.01145348 0.00971724 0.00710274 0.00709707
 0.00968451 0.00904217 0.01325879 0.00693983]
At round 42 energy consumption: 0.09249657683909618
At round 42 eta: 0.6666929660097998
At round 42 a_n: 13.79567728354305
At round 42 local rounds: 13.275695913565364
At round 42 global rounds: 41.390297463535276
gradient difference: 0.3811056911945343
train() client id: f_00000-0-0 loss: 1.263392  [   32/  126]
train() client id: f_00000-0-1 loss: 0.939916  [   64/  126]
train() client id: f_00000-0-2 loss: 1.095664  [   96/  126]
train() client id: f_00000-1-0 loss: 1.138817  [   32/  126]
train() client id: f_00000-1-1 loss: 1.058201  [   64/  126]
train() client id: f_00000-1-2 loss: 0.966458  [   96/  126]
train() client id: f_00000-2-0 loss: 1.020504  [   32/  126]
train() client id: f_00000-2-1 loss: 0.862556  [   64/  126]
train() client id: f_00000-2-2 loss: 0.861899  [   96/  126]
train() client id: f_00000-3-0 loss: 1.132207  [   32/  126]
train() client id: f_00000-3-1 loss: 0.890558  [   64/  126]
train() client id: f_00000-3-2 loss: 0.876045  [   96/  126]
train() client id: f_00000-4-0 loss: 0.955157  [   32/  126]
train() client id: f_00000-4-1 loss: 0.837366  [   64/  126]
train() client id: f_00000-4-2 loss: 0.907760  [   96/  126]
train() client id: f_00000-5-0 loss: 0.841067  [   32/  126]
train() client id: f_00000-5-1 loss: 0.884915  [   64/  126]
train() client id: f_00000-5-2 loss: 0.814154  [   96/  126]
train() client id: f_00000-6-0 loss: 0.888301  [   32/  126]
train() client id: f_00000-6-1 loss: 0.698127  [   64/  126]
train() client id: f_00000-6-2 loss: 0.826759  [   96/  126]
train() client id: f_00000-7-0 loss: 0.866674  [   32/  126]
train() client id: f_00000-7-1 loss: 0.796607  [   64/  126]
train() client id: f_00000-7-2 loss: 0.864113  [   96/  126]
train() client id: f_00000-8-0 loss: 0.987394  [   32/  126]
train() client id: f_00000-8-1 loss: 0.742618  [   64/  126]
train() client id: f_00000-8-2 loss: 0.881841  [   96/  126]
train() client id: f_00000-9-0 loss: 1.021460  [   32/  126]
train() client id: f_00000-9-1 loss: 0.800093  [   64/  126]
train() client id: f_00000-9-2 loss: 0.782408  [   96/  126]
train() client id: f_00000-10-0 loss: 0.790988  [   32/  126]
train() client id: f_00000-10-1 loss: 0.741789  [   64/  126]
train() client id: f_00000-10-2 loss: 0.925493  [   96/  126]
train() client id: f_00000-11-0 loss: 0.899977  [   32/  126]
train() client id: f_00000-11-1 loss: 0.626203  [   64/  126]
train() client id: f_00000-11-2 loss: 0.924424  [   96/  126]
train() client id: f_00000-12-0 loss: 0.800183  [   32/  126]
train() client id: f_00000-12-1 loss: 0.814931  [   64/  126]
train() client id: f_00000-12-2 loss: 0.784017  [   96/  126]
train() client id: f_00001-0-0 loss: 0.288389  [   32/  265]
train() client id: f_00001-0-1 loss: 0.449371  [   64/  265]
train() client id: f_00001-0-2 loss: 0.411094  [   96/  265]
train() client id: f_00001-0-3 loss: 0.443130  [  128/  265]
train() client id: f_00001-0-4 loss: 0.330352  [  160/  265]
train() client id: f_00001-0-5 loss: 0.448229  [  192/  265]
train() client id: f_00001-0-6 loss: 0.308190  [  224/  265]
train() client id: f_00001-0-7 loss: 0.461367  [  256/  265]
train() client id: f_00001-1-0 loss: 0.303443  [   32/  265]
train() client id: f_00001-1-1 loss: 0.413351  [   64/  265]
train() client id: f_00001-1-2 loss: 0.321006  [   96/  265]
train() client id: f_00001-1-3 loss: 0.498346  [  128/  265]
train() client id: f_00001-1-4 loss: 0.303517  [  160/  265]
train() client id: f_00001-1-5 loss: 0.462138  [  192/  265]
train() client id: f_00001-1-6 loss: 0.388293  [  224/  265]
train() client id: f_00001-1-7 loss: 0.381180  [  256/  265]
train() client id: f_00001-2-0 loss: 0.423031  [   32/  265]
train() client id: f_00001-2-1 loss: 0.313632  [   64/  265]
train() client id: f_00001-2-2 loss: 0.346075  [   96/  265]
train() client id: f_00001-2-3 loss: 0.309683  [  128/  265]
train() client id: f_00001-2-4 loss: 0.371085  [  160/  265]
train() client id: f_00001-2-5 loss: 0.440753  [  192/  265]
train() client id: f_00001-2-6 loss: 0.388016  [  224/  265]
train() client id: f_00001-2-7 loss: 0.417205  [  256/  265]
train() client id: f_00001-3-0 loss: 0.357254  [   32/  265]
train() client id: f_00001-3-1 loss: 0.318643  [   64/  265]
train() client id: f_00001-3-2 loss: 0.283558  [   96/  265]
train() client id: f_00001-3-3 loss: 0.416688  [  128/  265]
train() client id: f_00001-3-4 loss: 0.342576  [  160/  265]
train() client id: f_00001-3-5 loss: 0.344833  [  192/  265]
train() client id: f_00001-3-6 loss: 0.423556  [  224/  265]
train() client id: f_00001-3-7 loss: 0.433738  [  256/  265]
train() client id: f_00001-4-0 loss: 0.315130  [   32/  265]
train() client id: f_00001-4-1 loss: 0.305425  [   64/  265]
train() client id: f_00001-4-2 loss: 0.338182  [   96/  265]
train() client id: f_00001-4-3 loss: 0.307855  [  128/  265]
train() client id: f_00001-4-4 loss: 0.368597  [  160/  265]
train() client id: f_00001-4-5 loss: 0.412209  [  192/  265]
train() client id: f_00001-4-6 loss: 0.485950  [  224/  265]
train() client id: f_00001-4-7 loss: 0.286335  [  256/  265]
train() client id: f_00001-5-0 loss: 0.375751  [   32/  265]
train() client id: f_00001-5-1 loss: 0.425460  [   64/  265]
train() client id: f_00001-5-2 loss: 0.263926  [   96/  265]
train() client id: f_00001-5-3 loss: 0.458530  [  128/  265]
train() client id: f_00001-5-4 loss: 0.401453  [  160/  265]
train() client id: f_00001-5-5 loss: 0.361019  [  192/  265]
train() client id: f_00001-5-6 loss: 0.292336  [  224/  265]
train() client id: f_00001-5-7 loss: 0.259957  [  256/  265]
train() client id: f_00001-6-0 loss: 0.393504  [   32/  265]
train() client id: f_00001-6-1 loss: 0.321227  [   64/  265]
train() client id: f_00001-6-2 loss: 0.394412  [   96/  265]
train() client id: f_00001-6-3 loss: 0.265664  [  128/  265]
train() client id: f_00001-6-4 loss: 0.254572  [  160/  265]
train() client id: f_00001-6-5 loss: 0.294266  [  192/  265]
train() client id: f_00001-6-6 loss: 0.479342  [  224/  265]
train() client id: f_00001-6-7 loss: 0.307714  [  256/  265]
train() client id: f_00001-7-0 loss: 0.370531  [   32/  265]
train() client id: f_00001-7-1 loss: 0.408434  [   64/  265]
train() client id: f_00001-7-2 loss: 0.252983  [   96/  265]
train() client id: f_00001-7-3 loss: 0.356016  [  128/  265]
train() client id: f_00001-7-4 loss: 0.320596  [  160/  265]
train() client id: f_00001-7-5 loss: 0.519264  [  192/  265]
train() client id: f_00001-7-6 loss: 0.347011  [  224/  265]
train() client id: f_00001-7-7 loss: 0.300876  [  256/  265]
train() client id: f_00001-8-0 loss: 0.333879  [   32/  265]
train() client id: f_00001-8-1 loss: 0.380502  [   64/  265]
train() client id: f_00001-8-2 loss: 0.263317  [   96/  265]
train() client id: f_00001-8-3 loss: 0.304327  [  128/  265]
train() client id: f_00001-8-4 loss: 0.400793  [  160/  265]
train() client id: f_00001-8-5 loss: 0.352794  [  192/  265]
train() client id: f_00001-8-6 loss: 0.365078  [  224/  265]
train() client id: f_00001-8-7 loss: 0.473272  [  256/  265]
train() client id: f_00001-9-0 loss: 0.335018  [   32/  265]
train() client id: f_00001-9-1 loss: 0.484159  [   64/  265]
train() client id: f_00001-9-2 loss: 0.337820  [   96/  265]
train() client id: f_00001-9-3 loss: 0.347763  [  128/  265]
train() client id: f_00001-9-4 loss: 0.413729  [  160/  265]
train() client id: f_00001-9-5 loss: 0.295652  [  192/  265]
train() client id: f_00001-9-6 loss: 0.239467  [  224/  265]
train() client id: f_00001-9-7 loss: 0.314864  [  256/  265]
train() client id: f_00001-10-0 loss: 0.317946  [   32/  265]
train() client id: f_00001-10-1 loss: 0.309960  [   64/  265]
train() client id: f_00001-10-2 loss: 0.486801  [   96/  265]
train() client id: f_00001-10-3 loss: 0.330455  [  128/  265]
train() client id: f_00001-10-4 loss: 0.382120  [  160/  265]
train() client id: f_00001-10-5 loss: 0.375215  [  192/  265]
train() client id: f_00001-10-6 loss: 0.412290  [  224/  265]
train() client id: f_00001-10-7 loss: 0.240535  [  256/  265]
train() client id: f_00001-11-0 loss: 0.518511  [   32/  265]
train() client id: f_00001-11-1 loss: 0.335662  [   64/  265]
train() client id: f_00001-11-2 loss: 0.335745  [   96/  265]
train() client id: f_00001-11-3 loss: 0.255477  [  128/  265]
train() client id: f_00001-11-4 loss: 0.259620  [  160/  265]
train() client id: f_00001-11-5 loss: 0.428947  [  192/  265]
train() client id: f_00001-11-6 loss: 0.304944  [  224/  265]
train() client id: f_00001-11-7 loss: 0.363337  [  256/  265]
train() client id: f_00001-12-0 loss: 0.389093  [   32/  265]
train() client id: f_00001-12-1 loss: 0.333728  [   64/  265]
train() client id: f_00001-12-2 loss: 0.259946  [   96/  265]
train() client id: f_00001-12-3 loss: 0.325846  [  128/  265]
train() client id: f_00001-12-4 loss: 0.428611  [  160/  265]
train() client id: f_00001-12-5 loss: 0.344345  [  192/  265]
train() client id: f_00001-12-6 loss: 0.524381  [  224/  265]
train() client id: f_00001-12-7 loss: 0.261023  [  256/  265]
train() client id: f_00002-0-0 loss: 1.295495  [   32/  124]
train() client id: f_00002-0-1 loss: 1.251779  [   64/  124]
train() client id: f_00002-0-2 loss: 1.355713  [   96/  124]
train() client id: f_00002-1-0 loss: 1.280647  [   32/  124]
train() client id: f_00002-1-1 loss: 1.373774  [   64/  124]
train() client id: f_00002-1-2 loss: 1.107276  [   96/  124]
train() client id: f_00002-2-0 loss: 1.299297  [   32/  124]
train() client id: f_00002-2-1 loss: 1.197175  [   64/  124]
train() client id: f_00002-2-2 loss: 1.081323  [   96/  124]
train() client id: f_00002-3-0 loss: 1.156690  [   32/  124]
train() client id: f_00002-3-1 loss: 1.154915  [   64/  124]
train() client id: f_00002-3-2 loss: 1.273847  [   96/  124]
train() client id: f_00002-4-0 loss: 0.969567  [   32/  124]
train() client id: f_00002-4-1 loss: 1.111852  [   64/  124]
train() client id: f_00002-4-2 loss: 1.402479  [   96/  124]
train() client id: f_00002-5-0 loss: 1.161924  [   32/  124]
train() client id: f_00002-5-1 loss: 1.038322  [   64/  124]
train() client id: f_00002-5-2 loss: 1.144092  [   96/  124]
train() client id: f_00002-6-0 loss: 1.241391  [   32/  124]
train() client id: f_00002-6-1 loss: 1.016152  [   64/  124]
train() client id: f_00002-6-2 loss: 1.007565  [   96/  124]
train() client id: f_00002-7-0 loss: 1.035224  [   32/  124]
train() client id: f_00002-7-1 loss: 1.270846  [   64/  124]
train() client id: f_00002-7-2 loss: 1.077725  [   96/  124]
train() client id: f_00002-8-0 loss: 1.166725  [   32/  124]
train() client id: f_00002-8-1 loss: 1.039625  [   64/  124]
train() client id: f_00002-8-2 loss: 0.934404  [   96/  124]
train() client id: f_00002-9-0 loss: 1.322293  [   32/  124]
train() client id: f_00002-9-1 loss: 0.995563  [   64/  124]
train() client id: f_00002-9-2 loss: 0.895682  [   96/  124]
train() client id: f_00002-10-0 loss: 1.058413  [   32/  124]
train() client id: f_00002-10-1 loss: 1.082383  [   64/  124]
train() client id: f_00002-10-2 loss: 1.035328  [   96/  124]
train() client id: f_00002-11-0 loss: 1.022138  [   32/  124]
train() client id: f_00002-11-1 loss: 1.030297  [   64/  124]
train() client id: f_00002-11-2 loss: 1.012652  [   96/  124]
train() client id: f_00002-12-0 loss: 1.011204  [   32/  124]
train() client id: f_00002-12-1 loss: 0.949686  [   64/  124]
train() client id: f_00002-12-2 loss: 1.311477  [   96/  124]
train() client id: f_00003-0-0 loss: 0.930035  [   32/   43]
train() client id: f_00003-1-0 loss: 0.929105  [   32/   43]
train() client id: f_00003-2-0 loss: 0.794993  [   32/   43]
train() client id: f_00003-3-0 loss: 0.949073  [   32/   43]
train() client id: f_00003-4-0 loss: 0.825564  [   32/   43]
train() client id: f_00003-5-0 loss: 0.876514  [   32/   43]
train() client id: f_00003-6-0 loss: 0.817214  [   32/   43]
train() client id: f_00003-7-0 loss: 0.966867  [   32/   43]
train() client id: f_00003-8-0 loss: 0.836712  [   32/   43]
train() client id: f_00003-9-0 loss: 0.811375  [   32/   43]
train() client id: f_00003-10-0 loss: 0.836671  [   32/   43]
train() client id: f_00003-11-0 loss: 0.669012  [   32/   43]
train() client id: f_00003-12-0 loss: 0.737258  [   32/   43]
train() client id: f_00004-0-0 loss: 0.758042  [   32/  306]
train() client id: f_00004-0-1 loss: 0.716692  [   64/  306]
train() client id: f_00004-0-2 loss: 0.717371  [   96/  306]
train() client id: f_00004-0-3 loss: 0.865543  [  128/  306]
train() client id: f_00004-0-4 loss: 0.867085  [  160/  306]
train() client id: f_00004-0-5 loss: 0.788189  [  192/  306]
train() client id: f_00004-0-6 loss: 0.788764  [  224/  306]
train() client id: f_00004-0-7 loss: 0.796735  [  256/  306]
train() client id: f_00004-0-8 loss: 0.701293  [  288/  306]
train() client id: f_00004-1-0 loss: 0.790328  [   32/  306]
train() client id: f_00004-1-1 loss: 0.749406  [   64/  306]
train() client id: f_00004-1-2 loss: 0.758916  [   96/  306]
train() client id: f_00004-1-3 loss: 0.877019  [  128/  306]
train() client id: f_00004-1-4 loss: 0.794854  [  160/  306]
train() client id: f_00004-1-5 loss: 0.677345  [  192/  306]
train() client id: f_00004-1-6 loss: 0.818264  [  224/  306]
train() client id: f_00004-1-7 loss: 0.788817  [  256/  306]
train() client id: f_00004-1-8 loss: 0.757177  [  288/  306]
train() client id: f_00004-2-0 loss: 0.696085  [   32/  306]
train() client id: f_00004-2-1 loss: 0.667823  [   64/  306]
train() client id: f_00004-2-2 loss: 0.840016  [   96/  306]
train() client id: f_00004-2-3 loss: 0.880208  [  128/  306]
train() client id: f_00004-2-4 loss: 0.931555  [  160/  306]
train() client id: f_00004-2-5 loss: 0.813247  [  192/  306]
train() client id: f_00004-2-6 loss: 0.770817  [  224/  306]
train() client id: f_00004-2-7 loss: 0.703507  [  256/  306]
train() client id: f_00004-2-8 loss: 0.737842  [  288/  306]
train() client id: f_00004-3-0 loss: 0.746376  [   32/  306]
train() client id: f_00004-3-1 loss: 0.755531  [   64/  306]
train() client id: f_00004-3-2 loss: 0.821627  [   96/  306]
train() client id: f_00004-3-3 loss: 0.795868  [  128/  306]
train() client id: f_00004-3-4 loss: 0.812033  [  160/  306]
train() client id: f_00004-3-5 loss: 0.705422  [  192/  306]
train() client id: f_00004-3-6 loss: 0.888285  [  224/  306]
train() client id: f_00004-3-7 loss: 0.775566  [  256/  306]
train() client id: f_00004-3-8 loss: 0.598133  [  288/  306]
train() client id: f_00004-4-0 loss: 0.756784  [   32/  306]
train() client id: f_00004-4-1 loss: 0.771088  [   64/  306]
train() client id: f_00004-4-2 loss: 0.880621  [   96/  306]
train() client id: f_00004-4-3 loss: 0.857816  [  128/  306]
train() client id: f_00004-4-4 loss: 0.687640  [  160/  306]
train() client id: f_00004-4-5 loss: 0.723090  [  192/  306]
train() client id: f_00004-4-6 loss: 0.811625  [  224/  306]
train() client id: f_00004-4-7 loss: 0.781723  [  256/  306]
train() client id: f_00004-4-8 loss: 0.822946  [  288/  306]
train() client id: f_00004-5-0 loss: 0.690661  [   32/  306]
train() client id: f_00004-5-1 loss: 0.842690  [   64/  306]
train() client id: f_00004-5-2 loss: 0.847709  [   96/  306]
train() client id: f_00004-5-3 loss: 0.792332  [  128/  306]
train() client id: f_00004-5-4 loss: 0.694516  [  160/  306]
train() client id: f_00004-5-5 loss: 0.834861  [  192/  306]
train() client id: f_00004-5-6 loss: 0.807458  [  224/  306]
train() client id: f_00004-5-7 loss: 0.739956  [  256/  306]
train() client id: f_00004-5-8 loss: 0.755354  [  288/  306]
train() client id: f_00004-6-0 loss: 0.743363  [   32/  306]
train() client id: f_00004-6-1 loss: 0.726044  [   64/  306]
train() client id: f_00004-6-2 loss: 0.722778  [   96/  306]
train() client id: f_00004-6-3 loss: 0.881132  [  128/  306]
train() client id: f_00004-6-4 loss: 0.811713  [  160/  306]
train() client id: f_00004-6-5 loss: 0.874088  [  192/  306]
train() client id: f_00004-6-6 loss: 0.786678  [  224/  306]
train() client id: f_00004-6-7 loss: 0.613945  [  256/  306]
train() client id: f_00004-6-8 loss: 0.810647  [  288/  306]
train() client id: f_00004-7-0 loss: 0.815712  [   32/  306]
train() client id: f_00004-7-1 loss: 0.585920  [   64/  306]
train() client id: f_00004-7-2 loss: 0.758054  [   96/  306]
train() client id: f_00004-7-3 loss: 0.727814  [  128/  306]
train() client id: f_00004-7-4 loss: 0.811044  [  160/  306]
train() client id: f_00004-7-5 loss: 0.693633  [  192/  306]
train() client id: f_00004-7-6 loss: 0.760897  [  224/  306]
train() client id: f_00004-7-7 loss: 0.981314  [  256/  306]
train() client id: f_00004-7-8 loss: 0.850857  [  288/  306]
train() client id: f_00004-8-0 loss: 0.682633  [   32/  306]
train() client id: f_00004-8-1 loss: 0.837587  [   64/  306]
train() client id: f_00004-8-2 loss: 0.701960  [   96/  306]
train() client id: f_00004-8-3 loss: 0.765471  [  128/  306]
train() client id: f_00004-8-4 loss: 0.706325  [  160/  306]
train() client id: f_00004-8-5 loss: 0.788864  [  192/  306]
train() client id: f_00004-8-6 loss: 0.931758  [  224/  306]
train() client id: f_00004-8-7 loss: 0.817182  [  256/  306]
train() client id: f_00004-8-8 loss: 0.812369  [  288/  306]
train() client id: f_00004-9-0 loss: 0.833151  [   32/  306]
train() client id: f_00004-9-1 loss: 0.808203  [   64/  306]
train() client id: f_00004-9-2 loss: 0.812825  [   96/  306]
train() client id: f_00004-9-3 loss: 0.738457  [  128/  306]
train() client id: f_00004-9-4 loss: 0.716161  [  160/  306]
train() client id: f_00004-9-5 loss: 0.652348  [  192/  306]
train() client id: f_00004-9-6 loss: 0.777285  [  224/  306]
train() client id: f_00004-9-7 loss: 0.653780  [  256/  306]
train() client id: f_00004-9-8 loss: 0.963877  [  288/  306]
train() client id: f_00004-10-0 loss: 0.759209  [   32/  306]
train() client id: f_00004-10-1 loss: 0.858320  [   64/  306]
train() client id: f_00004-10-2 loss: 0.569774  [   96/  306]
train() client id: f_00004-10-3 loss: 0.768493  [  128/  306]
train() client id: f_00004-10-4 loss: 0.960941  [  160/  306]
train() client id: f_00004-10-5 loss: 0.770653  [  192/  306]
train() client id: f_00004-10-6 loss: 0.877713  [  224/  306]
train() client id: f_00004-10-7 loss: 0.705427  [  256/  306]
train() client id: f_00004-10-8 loss: 0.662694  [  288/  306]
train() client id: f_00004-11-0 loss: 0.834937  [   32/  306]
train() client id: f_00004-11-1 loss: 0.638256  [   64/  306]
train() client id: f_00004-11-2 loss: 0.847019  [   96/  306]
train() client id: f_00004-11-3 loss: 0.881455  [  128/  306]
train() client id: f_00004-11-4 loss: 0.794609  [  160/  306]
train() client id: f_00004-11-5 loss: 0.767735  [  192/  306]
train() client id: f_00004-11-6 loss: 0.726943  [  224/  306]
train() client id: f_00004-11-7 loss: 0.739256  [  256/  306]
train() client id: f_00004-11-8 loss: 0.783918  [  288/  306]
train() client id: f_00004-12-0 loss: 0.728056  [   32/  306]
train() client id: f_00004-12-1 loss: 0.824346  [   64/  306]
train() client id: f_00004-12-2 loss: 0.660110  [   96/  306]
train() client id: f_00004-12-3 loss: 0.771927  [  128/  306]
train() client id: f_00004-12-4 loss: 0.855781  [  160/  306]
train() client id: f_00004-12-5 loss: 0.823454  [  192/  306]
train() client id: f_00004-12-6 loss: 0.769064  [  224/  306]
train() client id: f_00004-12-7 loss: 0.741788  [  256/  306]
train() client id: f_00004-12-8 loss: 0.814489  [  288/  306]
train() client id: f_00005-0-0 loss: 0.544672  [   32/  146]
train() client id: f_00005-0-1 loss: 0.321221  [   64/  146]
train() client id: f_00005-0-2 loss: 0.962373  [   96/  146]
train() client id: f_00005-0-3 loss: 0.861208  [  128/  146]
train() client id: f_00005-1-0 loss: 0.662234  [   32/  146]
train() client id: f_00005-1-1 loss: 0.712029  [   64/  146]
train() client id: f_00005-1-2 loss: 0.672447  [   96/  146]
train() client id: f_00005-1-3 loss: 0.543107  [  128/  146]
train() client id: f_00005-2-0 loss: 0.809490  [   32/  146]
train() client id: f_00005-2-1 loss: 0.547047  [   64/  146]
train() client id: f_00005-2-2 loss: 0.755294  [   96/  146]
train() client id: f_00005-2-3 loss: 0.811754  [  128/  146]
train() client id: f_00005-3-0 loss: 0.503212  [   32/  146]
train() client id: f_00005-3-1 loss: 0.820104  [   64/  146]
train() client id: f_00005-3-2 loss: 0.969126  [   96/  146]
train() client id: f_00005-3-3 loss: 0.499705  [  128/  146]
train() client id: f_00005-4-0 loss: 0.535556  [   32/  146]
train() client id: f_00005-4-1 loss: 0.469955  [   64/  146]
train() client id: f_00005-4-2 loss: 0.969346  [   96/  146]
train() client id: f_00005-4-3 loss: 0.559641  [  128/  146]
train() client id: f_00005-5-0 loss: 0.431104  [   32/  146]
train() client id: f_00005-5-1 loss: 0.837331  [   64/  146]
train() client id: f_00005-5-2 loss: 0.747718  [   96/  146]
train() client id: f_00005-5-3 loss: 0.703104  [  128/  146]
train() client id: f_00005-6-0 loss: 0.869691  [   32/  146]
train() client id: f_00005-6-1 loss: 0.396283  [   64/  146]
train() client id: f_00005-6-2 loss: 0.930379  [   96/  146]
train() client id: f_00005-6-3 loss: 0.559557  [  128/  146]
train() client id: f_00005-7-0 loss: 0.572379  [   32/  146]
train() client id: f_00005-7-1 loss: 0.671084  [   64/  146]
train() client id: f_00005-7-2 loss: 0.581477  [   96/  146]
train() client id: f_00005-7-3 loss: 0.925992  [  128/  146]
train() client id: f_00005-8-0 loss: 0.515150  [   32/  146]
train() client id: f_00005-8-1 loss: 0.809486  [   64/  146]
train() client id: f_00005-8-2 loss: 0.563746  [   96/  146]
train() client id: f_00005-8-3 loss: 0.752086  [  128/  146]
train() client id: f_00005-9-0 loss: 0.648737  [   32/  146]
train() client id: f_00005-9-1 loss: 0.835279  [   64/  146]
train() client id: f_00005-9-2 loss: 0.702059  [   96/  146]
train() client id: f_00005-9-3 loss: 0.650962  [  128/  146]
train() client id: f_00005-10-0 loss: 0.646906  [   32/  146]
train() client id: f_00005-10-1 loss: 0.932983  [   64/  146]
train() client id: f_00005-10-2 loss: 0.629776  [   96/  146]
train() client id: f_00005-10-3 loss: 0.586011  [  128/  146]
train() client id: f_00005-11-0 loss: 0.705265  [   32/  146]
train() client id: f_00005-11-1 loss: 0.661472  [   64/  146]
train() client id: f_00005-11-2 loss: 0.831037  [   96/  146]
train() client id: f_00005-11-3 loss: 0.636082  [  128/  146]
train() client id: f_00005-12-0 loss: 0.564026  [   32/  146]
train() client id: f_00005-12-1 loss: 0.661844  [   64/  146]
train() client id: f_00005-12-2 loss: 0.922271  [   96/  146]
train() client id: f_00005-12-3 loss: 0.715597  [  128/  146]
train() client id: f_00006-0-0 loss: 0.441716  [   32/   54]
train() client id: f_00006-1-0 loss: 0.519471  [   32/   54]
train() client id: f_00006-2-0 loss: 0.483108  [   32/   54]
train() client id: f_00006-3-0 loss: 0.519112  [   32/   54]
train() client id: f_00006-4-0 loss: 0.522542  [   32/   54]
train() client id: f_00006-5-0 loss: 0.538609  [   32/   54]
train() client id: f_00006-6-0 loss: 0.512203  [   32/   54]
train() client id: f_00006-7-0 loss: 0.479947  [   32/   54]
train() client id: f_00006-8-0 loss: 0.500512  [   32/   54]
train() client id: f_00006-9-0 loss: 0.551799  [   32/   54]
train() client id: f_00006-10-0 loss: 0.501959  [   32/   54]
train() client id: f_00006-11-0 loss: 0.512246  [   32/   54]
train() client id: f_00006-12-0 loss: 0.558558  [   32/   54]
train() client id: f_00007-0-0 loss: 0.734444  [   32/  179]
train() client id: f_00007-0-1 loss: 0.552976  [   64/  179]
train() client id: f_00007-0-2 loss: 0.636316  [   96/  179]
train() client id: f_00007-0-3 loss: 0.672675  [  128/  179]
train() client id: f_00007-0-4 loss: 0.617176  [  160/  179]
train() client id: f_00007-1-0 loss: 0.743749  [   32/  179]
train() client id: f_00007-1-1 loss: 0.541525  [   64/  179]
train() client id: f_00007-1-2 loss: 0.622568  [   96/  179]
train() client id: f_00007-1-3 loss: 0.898063  [  128/  179]
train() client id: f_00007-1-4 loss: 0.536420  [  160/  179]
train() client id: f_00007-2-0 loss: 0.630326  [   32/  179]
train() client id: f_00007-2-1 loss: 0.600061  [   64/  179]
train() client id: f_00007-2-2 loss: 0.603443  [   96/  179]
train() client id: f_00007-2-3 loss: 0.628703  [  128/  179]
train() client id: f_00007-2-4 loss: 0.711907  [  160/  179]
train() client id: f_00007-3-0 loss: 0.786947  [   32/  179]
train() client id: f_00007-3-1 loss: 0.533165  [   64/  179]
train() client id: f_00007-3-2 loss: 0.647258  [   96/  179]
train() client id: f_00007-3-3 loss: 0.626890  [  128/  179]
train() client id: f_00007-3-4 loss: 0.450566  [  160/  179]
train() client id: f_00007-4-0 loss: 0.589062  [   32/  179]
train() client id: f_00007-4-1 loss: 0.659571  [   64/  179]
train() client id: f_00007-4-2 loss: 0.726101  [   96/  179]
train() client id: f_00007-4-3 loss: 0.439294  [  128/  179]
train() client id: f_00007-4-4 loss: 0.754492  [  160/  179]
train() client id: f_00007-5-0 loss: 0.658346  [   32/  179]
train() client id: f_00007-5-1 loss: 0.720280  [   64/  179]
train() client id: f_00007-5-2 loss: 0.519982  [   96/  179]
train() client id: f_00007-5-3 loss: 0.662362  [  128/  179]
train() client id: f_00007-5-4 loss: 0.558195  [  160/  179]
train() client id: f_00007-6-0 loss: 0.529221  [   32/  179]
train() client id: f_00007-6-1 loss: 0.417030  [   64/  179]
train() client id: f_00007-6-2 loss: 0.650456  [   96/  179]
train() client id: f_00007-6-3 loss: 0.830600  [  128/  179]
train() client id: f_00007-6-4 loss: 0.614493  [  160/  179]
train() client id: f_00007-7-0 loss: 0.647764  [   32/  179]
train() client id: f_00007-7-1 loss: 0.571165  [   64/  179]
train() client id: f_00007-7-2 loss: 0.822843  [   96/  179]
train() client id: f_00007-7-3 loss: 0.459418  [  128/  179]
train() client id: f_00007-7-4 loss: 0.647049  [  160/  179]
train() client id: f_00007-8-0 loss: 0.553665  [   32/  179]
train() client id: f_00007-8-1 loss: 0.601119  [   64/  179]
train() client id: f_00007-8-2 loss: 0.768162  [   96/  179]
train() client id: f_00007-8-3 loss: 0.621776  [  128/  179]
train() client id: f_00007-8-4 loss: 0.528006  [  160/  179]
train() client id: f_00007-9-0 loss: 0.525411  [   32/  179]
train() client id: f_00007-9-1 loss: 0.543842  [   64/  179]
train() client id: f_00007-9-2 loss: 0.413028  [   96/  179]
train() client id: f_00007-9-3 loss: 0.766754  [  128/  179]
train() client id: f_00007-9-4 loss: 0.552728  [  160/  179]
train() client id: f_00007-10-0 loss: 0.545669  [   32/  179]
train() client id: f_00007-10-1 loss: 0.750689  [   64/  179]
train() client id: f_00007-10-2 loss: 0.723164  [   96/  179]
train() client id: f_00007-10-3 loss: 0.626236  [  128/  179]
train() client id: f_00007-10-4 loss: 0.430922  [  160/  179]
train() client id: f_00007-11-0 loss: 0.616185  [   32/  179]
train() client id: f_00007-11-1 loss: 0.640942  [   64/  179]
train() client id: f_00007-11-2 loss: 0.545529  [   96/  179]
train() client id: f_00007-11-3 loss: 0.540498  [  128/  179]
train() client id: f_00007-11-4 loss: 0.551783  [  160/  179]
train() client id: f_00007-12-0 loss: 0.522787  [   32/  179]
train() client id: f_00007-12-1 loss: 0.512368  [   64/  179]
train() client id: f_00007-12-2 loss: 0.714939  [   96/  179]
train() client id: f_00007-12-3 loss: 0.556359  [  128/  179]
train() client id: f_00007-12-4 loss: 0.461336  [  160/  179]
train() client id: f_00008-0-0 loss: 0.698669  [   32/  130]
train() client id: f_00008-0-1 loss: 0.741351  [   64/  130]
train() client id: f_00008-0-2 loss: 0.688573  [   96/  130]
train() client id: f_00008-0-3 loss: 0.754806  [  128/  130]
train() client id: f_00008-1-0 loss: 0.718026  [   32/  130]
train() client id: f_00008-1-1 loss: 0.768834  [   64/  130]
train() client id: f_00008-1-2 loss: 0.727404  [   96/  130]
train() client id: f_00008-1-3 loss: 0.713631  [  128/  130]
train() client id: f_00008-2-0 loss: 0.781065  [   32/  130]
train() client id: f_00008-2-1 loss: 0.656144  [   64/  130]
train() client id: f_00008-2-2 loss: 0.705549  [   96/  130]
train() client id: f_00008-2-3 loss: 0.760836  [  128/  130]
train() client id: f_00008-3-0 loss: 0.743306  [   32/  130]
train() client id: f_00008-3-1 loss: 0.713042  [   64/  130]
train() client id: f_00008-3-2 loss: 0.696003  [   96/  130]
train() client id: f_00008-3-3 loss: 0.741407  [  128/  130]
train() client id: f_00008-4-0 loss: 0.813686  [   32/  130]
train() client id: f_00008-4-1 loss: 0.613827  [   64/  130]
train() client id: f_00008-4-2 loss: 0.659649  [   96/  130]
train() client id: f_00008-4-3 loss: 0.848270  [  128/  130]
train() client id: f_00008-5-0 loss: 0.772675  [   32/  130]
train() client id: f_00008-5-1 loss: 0.715730  [   64/  130]
train() client id: f_00008-5-2 loss: 0.743004  [   96/  130]
train() client id: f_00008-5-3 loss: 0.672657  [  128/  130]
train() client id: f_00008-6-0 loss: 0.752652  [   32/  130]
train() client id: f_00008-6-1 loss: 0.653774  [   64/  130]
train() client id: f_00008-6-2 loss: 0.745412  [   96/  130]
train() client id: f_00008-6-3 loss: 0.789255  [  128/  130]
train() client id: f_00008-7-0 loss: 0.721727  [   32/  130]
train() client id: f_00008-7-1 loss: 0.724802  [   64/  130]
train() client id: f_00008-7-2 loss: 0.790741  [   96/  130]
train() client id: f_00008-7-3 loss: 0.696267  [  128/  130]
train() client id: f_00008-8-0 loss: 0.891207  [   32/  130]
train() client id: f_00008-8-1 loss: 0.791867  [   64/  130]
train() client id: f_00008-8-2 loss: 0.649765  [   96/  130]
train() client id: f_00008-8-3 loss: 0.606910  [  128/  130]
train() client id: f_00008-9-0 loss: 0.697492  [   32/  130]
train() client id: f_00008-9-1 loss: 0.715485  [   64/  130]
train() client id: f_00008-9-2 loss: 0.757819  [   96/  130]
train() client id: f_00008-9-3 loss: 0.777771  [  128/  130]
train() client id: f_00008-10-0 loss: 0.793905  [   32/  130]
train() client id: f_00008-10-1 loss: 0.687371  [   64/  130]
train() client id: f_00008-10-2 loss: 0.840828  [   96/  130]
train() client id: f_00008-10-3 loss: 0.621610  [  128/  130]
train() client id: f_00008-11-0 loss: 0.710294  [   32/  130]
train() client id: f_00008-11-1 loss: 0.754409  [   64/  130]
train() client id: f_00008-11-2 loss: 0.706623  [   96/  130]
train() client id: f_00008-11-3 loss: 0.751896  [  128/  130]
train() client id: f_00008-12-0 loss: 0.718560  [   32/  130]
train() client id: f_00008-12-1 loss: 0.773246  [   64/  130]
train() client id: f_00008-12-2 loss: 0.730572  [   96/  130]
train() client id: f_00008-12-3 loss: 0.730641  [  128/  130]
train() client id: f_00009-0-0 loss: 1.093716  [   32/  118]
train() client id: f_00009-0-1 loss: 0.966294  [   64/  118]
train() client id: f_00009-0-2 loss: 1.091483  [   96/  118]
train() client id: f_00009-1-0 loss: 1.039251  [   32/  118]
train() client id: f_00009-1-1 loss: 1.036410  [   64/  118]
train() client id: f_00009-1-2 loss: 0.858908  [   96/  118]
train() client id: f_00009-2-0 loss: 0.964413  [   32/  118]
train() client id: f_00009-2-1 loss: 0.956430  [   64/  118]
train() client id: f_00009-2-2 loss: 0.985377  [   96/  118]
train() client id: f_00009-3-0 loss: 0.959356  [   32/  118]
train() client id: f_00009-3-1 loss: 0.968314  [   64/  118]
train() client id: f_00009-3-2 loss: 1.031801  [   96/  118]
train() client id: f_00009-4-0 loss: 0.881572  [   32/  118]
train() client id: f_00009-4-1 loss: 1.041157  [   64/  118]
train() client id: f_00009-4-2 loss: 0.816965  [   96/  118]
train() client id: f_00009-5-0 loss: 0.906439  [   32/  118]
train() client id: f_00009-5-1 loss: 0.889564  [   64/  118]
train() client id: f_00009-5-2 loss: 0.802090  [   96/  118]
train() client id: f_00009-6-0 loss: 0.907806  [   32/  118]
train() client id: f_00009-6-1 loss: 0.777936  [   64/  118]
train() client id: f_00009-6-2 loss: 0.906406  [   96/  118]
train() client id: f_00009-7-0 loss: 0.796751  [   32/  118]
train() client id: f_00009-7-1 loss: 0.776579  [   64/  118]
train() client id: f_00009-7-2 loss: 0.908394  [   96/  118]
train() client id: f_00009-8-0 loss: 0.934482  [   32/  118]
train() client id: f_00009-8-1 loss: 0.906579  [   64/  118]
train() client id: f_00009-8-2 loss: 0.765907  [   96/  118]
train() client id: f_00009-9-0 loss: 0.905135  [   32/  118]
train() client id: f_00009-9-1 loss: 0.717793  [   64/  118]
train() client id: f_00009-9-2 loss: 0.674201  [   96/  118]
train() client id: f_00009-10-0 loss: 0.714403  [   32/  118]
train() client id: f_00009-10-1 loss: 0.836353  [   64/  118]
train() client id: f_00009-10-2 loss: 0.823564  [   96/  118]
train() client id: f_00009-11-0 loss: 0.988399  [   32/  118]
train() client id: f_00009-11-1 loss: 0.752004  [   64/  118]
train() client id: f_00009-11-2 loss: 0.712182  [   96/  118]
train() client id: f_00009-12-0 loss: 0.920842  [   32/  118]
train() client id: f_00009-12-1 loss: 0.824518  [   64/  118]
train() client id: f_00009-12-2 loss: 0.708641  [   96/  118]
At round 42 accuracy: 0.6419098143236074
At round 42 training accuracy: 0.5875251509054326
At round 42 training loss: 0.8199151077566565
update_location
xs = -3.905658 4.200318 230.009024 18.811294 0.979296 3.956410 -192.443192 -171.324852 214.663977 -157.060879 
ys = 222.587959 205.555839 1.320614 -192.455176 184.350187 167.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -5.211426168117436
ys mean: 60.8941425355287
dists_uav = 244.050515 228.628182 250.810476 217.699011 209.728278 195.389975 216.890001 198.375606 237.464298 186.236762 
uav_gains = -110.799066 -109.602742 -111.365089 -108.832866 -108.303628 -107.392702 -108.778069 -107.579824 -110.271545 -106.820522 
uav_gains_db_mean: -108.97460519942481
dists_bs = 177.589098 181.786725 440.677704 415.434861 176.227520 179.100623 178.479411 174.216421 420.264836 171.936922 
bs_gains = -102.550716 -102.834800 -113.602449 -112.885141 -102.457124 -102.653778 -102.611527 -102.317554 -113.025705 -102.157395 
bs_gains_db_mean: -105.70961875677675
Round 43
-------------------------------
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.31748257 10.95769314  5.24195078  1.89681827 12.63633198  6.07983027
  2.34717104  7.46063269  5.5099153   4.93011902]
obj_prev = 62.377945058813665
eta_min = 4.6045000649351174e-18	eta_max = 0.9345899157842099
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 14.450064093228786	eta = 0.9090909090909091
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 27.618160282034836	eta = 0.47564435026761315
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 21.01662000340652	eta = 0.6250492182285267
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.820837826510346	eta = 0.6627581547216592
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.755289483233774	eta = 0.6649571961010281
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.755074608659463	eta = 0.6649644287942615
eta = 0.6649644287942615
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [0.03366898 0.07081173 0.03313455 0.01149021 0.08176751 0.03901326
 0.01442956 0.04783132 0.03473783 0.03153126]
ene_total = [1.80568477 3.13015273 1.80324111 0.85888841 3.56508653 1.85089364
 0.97553732 2.28873162 1.93281417 1.54404431]
ti_comp = [0.58057304 0.62215632 0.57583732 0.59457466 0.62339857 0.62275708
 0.59491807 0.60189896 0.55960141 0.62435462]
ti_coms = [0.11299635 0.07141307 0.11773207 0.09899474 0.07017082 0.07081231
 0.09865133 0.09167044 0.13396798 0.06921478]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [7.07711056e-06 5.73319437e-05 6.85683199e-06 2.68194986e-07
 8.79206213e-05 9.56928073e-06 5.30548435e-07 1.88786089e-05
 8.36624401e-06 5.02621941e-06]
ene_total = [0.45401354 0.2890566  0.47302065 0.39751757 0.28529668 0.2847264
 0.39614917 0.36885463 0.53827549 0.27812918]
optimize_network iter = 0 obj = 3.765039909361509
eta = 0.6649644287942615
freqs = [28996330.2200803  56908313.47423983 28770753.14460626  9662542.53169571
 65582048.83208953 31323013.60893005 12127351.29718995 39733680.81777115
 31038012.71809572 25251084.61040274]
eta_min = 0.6649644287942639	eta_max = 0.6649644287943239
af = 0.006654514151181828	bf = 1.2587071102671177	zeta = 0.007319965566300011	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [1.65314891e-06 1.33922227e-05 1.60169383e-06 6.26479190e-08
 2.05374606e-05 2.23529729e-06 1.23931308e-07 4.40987202e-06
 1.95427881e-06 1.17407932e-06]
ene_total = [1.6724149  1.05878427 1.74248852 1.46497738 1.04145823 1.04824288
 1.45990451 1.35723249 1.98280693 1.02444478]
ti_comp = [0.58057304 0.62215632 0.57583732 0.59457466 0.62339857 0.62275708
 0.59491807 0.60189896 0.55960141 0.62435462]
ti_coms = [0.11299635 0.07141307 0.11773207 0.09899474 0.07017082 0.07081231
 0.09865133 0.09167044 0.13396798 0.06921478]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [7.07711056e-06 5.73319437e-05 6.85683199e-06 2.68194986e-07
 8.79206213e-05 9.56928073e-06 5.30548435e-07 1.88786089e-05
 8.36624401e-06 5.02621941e-06]
ene_total = [0.45401354 0.2890566  0.47302065 0.39751757 0.28529668 0.2847264
 0.39614917 0.36885463 0.53827549 0.27812918]
optimize_network iter = 1 obj = 3.765039909362207
eta = 0.6649644287943239
freqs = [28996330.22008007 56908313.47423852 28770753.14460608  9662542.53169558
 65582048.832088   31323013.60892932 12127351.29718979 39733680.81777052
 31038012.71809573 25251084.61040214]
Done!
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [6.88604689e-06 5.57841296e-05 6.67171527e-06 2.60954415e-07
 8.55469920e-05 9.31093491e-06 5.16225000e-07 1.83689353e-05
 8.14037707e-06 4.89052449e-06]
ene_total = [0.01130652 0.00719709 0.01177988 0.00989973 0.00710263 0.00709054
 0.00986565 0.00918541 0.01340494 0.00692637]
At round 43 energy consumption: 0.09375876535404022
At round 43 eta: 0.6649644287943239
At round 43 a_n: 13.453131436573733
At round 43 local rounds: 13.360704448743727
At round 43 global rounds: 40.15433760708037
gradient difference: 0.38302499055862427
train() client id: f_00000-0-0 loss: 0.945635  [   32/  126]
train() client id: f_00000-0-1 loss: 1.180471  [   64/  126]
train() client id: f_00000-0-2 loss: 1.148414  [   96/  126]
train() client id: f_00000-1-0 loss: 1.054694  [   32/  126]
train() client id: f_00000-1-1 loss: 1.237003  [   64/  126]
train() client id: f_00000-1-2 loss: 0.942058  [   96/  126]
train() client id: f_00000-2-0 loss: 1.147211  [   32/  126]
train() client id: f_00000-2-1 loss: 0.902665  [   64/  126]
train() client id: f_00000-2-2 loss: 0.938405  [   96/  126]
train() client id: f_00000-3-0 loss: 0.822334  [   32/  126]
train() client id: f_00000-3-1 loss: 0.964762  [   64/  126]
train() client id: f_00000-3-2 loss: 0.922181  [   96/  126]
train() client id: f_00000-4-0 loss: 0.792158  [   32/  126]
train() client id: f_00000-4-1 loss: 0.874214  [   64/  126]
train() client id: f_00000-4-2 loss: 0.923570  [   96/  126]
train() client id: f_00000-5-0 loss: 0.914889  [   32/  126]
train() client id: f_00000-5-1 loss: 0.895182  [   64/  126]
train() client id: f_00000-5-2 loss: 0.988868  [   96/  126]
train() client id: f_00000-6-0 loss: 0.897839  [   32/  126]
train() client id: f_00000-6-1 loss: 0.861515  [   64/  126]
train() client id: f_00000-6-2 loss: 0.845514  [   96/  126]
train() client id: f_00000-7-0 loss: 0.849433  [   32/  126]
train() client id: f_00000-7-1 loss: 0.794728  [   64/  126]
train() client id: f_00000-7-2 loss: 1.041226  [   96/  126]
train() client id: f_00000-8-0 loss: 0.776842  [   32/  126]
train() client id: f_00000-8-1 loss: 0.873726  [   64/  126]
train() client id: f_00000-8-2 loss: 0.907213  [   96/  126]
train() client id: f_00000-9-0 loss: 0.738308  [   32/  126]
train() client id: f_00000-9-1 loss: 1.081283  [   64/  126]
train() client id: f_00000-9-2 loss: 0.841932  [   96/  126]
train() client id: f_00000-10-0 loss: 0.928664  [   32/  126]
train() client id: f_00000-10-1 loss: 0.795273  [   64/  126]
train() client id: f_00000-10-2 loss: 0.932746  [   96/  126]
train() client id: f_00000-11-0 loss: 0.846368  [   32/  126]
train() client id: f_00000-11-1 loss: 0.985090  [   64/  126]
train() client id: f_00000-11-2 loss: 0.878044  [   96/  126]
train() client id: f_00000-12-0 loss: 0.972778  [   32/  126]
train() client id: f_00000-12-1 loss: 0.867786  [   64/  126]
train() client id: f_00000-12-2 loss: 0.796577  [   96/  126]
train() client id: f_00001-0-0 loss: 0.526134  [   32/  265]
train() client id: f_00001-0-1 loss: 0.453182  [   64/  265]
train() client id: f_00001-0-2 loss: 0.592000  [   96/  265]
train() client id: f_00001-0-3 loss: 0.468760  [  128/  265]
train() client id: f_00001-0-4 loss: 0.424796  [  160/  265]
train() client id: f_00001-0-5 loss: 0.423801  [  192/  265]
train() client id: f_00001-0-6 loss: 0.524771  [  224/  265]
train() client id: f_00001-0-7 loss: 0.548916  [  256/  265]
train() client id: f_00001-1-0 loss: 0.576460  [   32/  265]
train() client id: f_00001-1-1 loss: 0.460437  [   64/  265]
train() client id: f_00001-1-2 loss: 0.611798  [   96/  265]
train() client id: f_00001-1-3 loss: 0.398408  [  128/  265]
train() client id: f_00001-1-4 loss: 0.487150  [  160/  265]
train() client id: f_00001-1-5 loss: 0.532684  [  192/  265]
train() client id: f_00001-1-6 loss: 0.403170  [  224/  265]
train() client id: f_00001-1-7 loss: 0.489557  [  256/  265]
train() client id: f_00001-2-0 loss: 0.523114  [   32/  265]
train() client id: f_00001-2-1 loss: 0.601064  [   64/  265]
train() client id: f_00001-2-2 loss: 0.375651  [   96/  265]
train() client id: f_00001-2-3 loss: 0.407372  [  128/  265]
train() client id: f_00001-2-4 loss: 0.402738  [  160/  265]
train() client id: f_00001-2-5 loss: 0.435654  [  192/  265]
train() client id: f_00001-2-6 loss: 0.510779  [  224/  265]
train() client id: f_00001-2-7 loss: 0.576265  [  256/  265]
train() client id: f_00001-3-0 loss: 0.508875  [   32/  265]
train() client id: f_00001-3-1 loss: 0.443117  [   64/  265]
train() client id: f_00001-3-2 loss: 0.540624  [   96/  265]
train() client id: f_00001-3-3 loss: 0.518455  [  128/  265]
train() client id: f_00001-3-4 loss: 0.485690  [  160/  265]
train() client id: f_00001-3-5 loss: 0.381409  [  192/  265]
train() client id: f_00001-3-6 loss: 0.381885  [  224/  265]
train() client id: f_00001-3-7 loss: 0.615431  [  256/  265]
train() client id: f_00001-4-0 loss: 0.487376  [   32/  265]
train() client id: f_00001-4-1 loss: 0.409325  [   64/  265]
train() client id: f_00001-4-2 loss: 0.581439  [   96/  265]
train() client id: f_00001-4-3 loss: 0.381662  [  128/  265]
train() client id: f_00001-4-4 loss: 0.637852  [  160/  265]
train() client id: f_00001-4-5 loss: 0.425377  [  192/  265]
train() client id: f_00001-4-6 loss: 0.373164  [  224/  265]
train() client id: f_00001-4-7 loss: 0.561393  [  256/  265]
train() client id: f_00001-5-0 loss: 0.457639  [   32/  265]
train() client id: f_00001-5-1 loss: 0.459866  [   64/  265]
train() client id: f_00001-5-2 loss: 0.468037  [   96/  265]
train() client id: f_00001-5-3 loss: 0.550950  [  128/  265]
train() client id: f_00001-5-4 loss: 0.511397  [  160/  265]
train() client id: f_00001-5-5 loss: 0.462783  [  192/  265]
train() client id: f_00001-5-6 loss: 0.424113  [  224/  265]
train() client id: f_00001-5-7 loss: 0.506166  [  256/  265]
train() client id: f_00001-6-0 loss: 0.466009  [   32/  265]
train() client id: f_00001-6-1 loss: 0.596727  [   64/  265]
train() client id: f_00001-6-2 loss: 0.548500  [   96/  265]
train() client id: f_00001-6-3 loss: 0.520456  [  128/  265]
train() client id: f_00001-6-4 loss: 0.493769  [  160/  265]
train() client id: f_00001-6-5 loss: 0.401916  [  192/  265]
train() client id: f_00001-6-6 loss: 0.364777  [  224/  265]
train() client id: f_00001-6-7 loss: 0.446138  [  256/  265]
train() client id: f_00001-7-0 loss: 0.490132  [   32/  265]
train() client id: f_00001-7-1 loss: 0.465143  [   64/  265]
train() client id: f_00001-7-2 loss: 0.377497  [   96/  265]
train() client id: f_00001-7-3 loss: 0.481545  [  128/  265]
train() client id: f_00001-7-4 loss: 0.545744  [  160/  265]
train() client id: f_00001-7-5 loss: 0.549526  [  192/  265]
train() client id: f_00001-7-6 loss: 0.405818  [  224/  265]
train() client id: f_00001-7-7 loss: 0.433731  [  256/  265]
train() client id: f_00001-8-0 loss: 0.500574  [   32/  265]
train() client id: f_00001-8-1 loss: 0.451635  [   64/  265]
train() client id: f_00001-8-2 loss: 0.529467  [   96/  265]
train() client id: f_00001-8-3 loss: 0.473740  [  128/  265]
train() client id: f_00001-8-4 loss: 0.448894  [  160/  265]
train() client id: f_00001-8-5 loss: 0.443354  [  192/  265]
train() client id: f_00001-8-6 loss: 0.474233  [  224/  265]
train() client id: f_00001-8-7 loss: 0.516739  [  256/  265]
train() client id: f_00001-9-0 loss: 0.430992  [   32/  265]
train() client id: f_00001-9-1 loss: 0.439794  [   64/  265]
train() client id: f_00001-9-2 loss: 0.516706  [   96/  265]
train() client id: f_00001-9-3 loss: 0.480450  [  128/  265]
train() client id: f_00001-9-4 loss: 0.554301  [  160/  265]
train() client id: f_00001-9-5 loss: 0.502180  [  192/  265]
train() client id: f_00001-9-6 loss: 0.446889  [  224/  265]
train() client id: f_00001-9-7 loss: 0.448083  [  256/  265]
train() client id: f_00001-10-0 loss: 0.654211  [   32/  265]
train() client id: f_00001-10-1 loss: 0.447774  [   64/  265]
train() client id: f_00001-10-2 loss: 0.474801  [   96/  265]
train() client id: f_00001-10-3 loss: 0.542638  [  128/  265]
train() client id: f_00001-10-4 loss: 0.410043  [  160/  265]
train() client id: f_00001-10-5 loss: 0.381302  [  192/  265]
train() client id: f_00001-10-6 loss: 0.441147  [  224/  265]
train() client id: f_00001-10-7 loss: 0.467441  [  256/  265]
train() client id: f_00001-11-0 loss: 0.420199  [   32/  265]
train() client id: f_00001-11-1 loss: 0.497307  [   64/  265]
train() client id: f_00001-11-2 loss: 0.442767  [   96/  265]
train() client id: f_00001-11-3 loss: 0.489781  [  128/  265]
train() client id: f_00001-11-4 loss: 0.550811  [  160/  265]
train() client id: f_00001-11-5 loss: 0.464911  [  192/  265]
train() client id: f_00001-11-6 loss: 0.513274  [  224/  265]
train() client id: f_00001-11-7 loss: 0.462739  [  256/  265]
train() client id: f_00001-12-0 loss: 0.445577  [   32/  265]
train() client id: f_00001-12-1 loss: 0.619334  [   64/  265]
train() client id: f_00001-12-2 loss: 0.417337  [   96/  265]
train() client id: f_00001-12-3 loss: 0.463688  [  128/  265]
train() client id: f_00001-12-4 loss: 0.510230  [  160/  265]
train() client id: f_00001-12-5 loss: 0.388564  [  192/  265]
train() client id: f_00001-12-6 loss: 0.563226  [  224/  265]
train() client id: f_00001-12-7 loss: 0.404681  [  256/  265]
train() client id: f_00002-0-0 loss: 0.961075  [   32/  124]
train() client id: f_00002-0-1 loss: 0.735884  [   64/  124]
train() client id: f_00002-0-2 loss: 0.779634  [   96/  124]
train() client id: f_00002-1-0 loss: 0.855488  [   32/  124]
train() client id: f_00002-1-1 loss: 0.749807  [   64/  124]
train() client id: f_00002-1-2 loss: 0.860055  [   96/  124]
train() client id: f_00002-2-0 loss: 0.828227  [   32/  124]
train() client id: f_00002-2-1 loss: 0.905463  [   64/  124]
train() client id: f_00002-2-2 loss: 0.713457  [   96/  124]
train() client id: f_00002-3-0 loss: 0.574242  [   32/  124]
train() client id: f_00002-3-1 loss: 0.931593  [   64/  124]
train() client id: f_00002-3-2 loss: 0.740728  [   96/  124]
train() client id: f_00002-4-0 loss: 0.748456  [   32/  124]
train() client id: f_00002-4-1 loss: 0.681563  [   64/  124]
train() client id: f_00002-4-2 loss: 0.656933  [   96/  124]
train() client id: f_00002-5-0 loss: 0.707274  [   32/  124]
train() client id: f_00002-5-1 loss: 0.884011  [   64/  124]
train() client id: f_00002-5-2 loss: 0.703733  [   96/  124]
train() client id: f_00002-6-0 loss: 0.773865  [   32/  124]
train() client id: f_00002-6-1 loss: 0.657987  [   64/  124]
train() client id: f_00002-6-2 loss: 0.693990  [   96/  124]
train() client id: f_00002-7-0 loss: 0.662832  [   32/  124]
train() client id: f_00002-7-1 loss: 0.690975  [   64/  124]
train() client id: f_00002-7-2 loss: 0.632729  [   96/  124]
train() client id: f_00002-8-0 loss: 0.622554  [   32/  124]
train() client id: f_00002-8-1 loss: 0.663342  [   64/  124]
train() client id: f_00002-8-2 loss: 0.677660  [   96/  124]
train() client id: f_00002-9-0 loss: 0.584495  [   32/  124]
train() client id: f_00002-9-1 loss: 0.879243  [   64/  124]
train() client id: f_00002-9-2 loss: 0.496956  [   96/  124]
train() client id: f_00002-10-0 loss: 0.510479  [   32/  124]
train() client id: f_00002-10-1 loss: 0.718465  [   64/  124]
train() client id: f_00002-10-2 loss: 0.765070  [   96/  124]
train() client id: f_00002-11-0 loss: 0.564878  [   32/  124]
train() client id: f_00002-11-1 loss: 0.634198  [   64/  124]
train() client id: f_00002-11-2 loss: 0.658319  [   96/  124]
train() client id: f_00002-12-0 loss: 0.590300  [   32/  124]
train() client id: f_00002-12-1 loss: 0.477619  [   64/  124]
train() client id: f_00002-12-2 loss: 0.791125  [   96/  124]
train() client id: f_00003-0-0 loss: 0.805098  [   32/   43]
train() client id: f_00003-1-0 loss: 0.763472  [   32/   43]
train() client id: f_00003-2-0 loss: 0.761066  [   32/   43]
train() client id: f_00003-3-0 loss: 0.791597  [   32/   43]
train() client id: f_00003-4-0 loss: 0.751133  [   32/   43]
train() client id: f_00003-5-0 loss: 0.536974  [   32/   43]
train() client id: f_00003-6-0 loss: 0.829974  [   32/   43]
train() client id: f_00003-7-0 loss: 0.705593  [   32/   43]
train() client id: f_00003-8-0 loss: 0.767742  [   32/   43]
train() client id: f_00003-9-0 loss: 0.604679  [   32/   43]
train() client id: f_00003-10-0 loss: 0.903192  [   32/   43]
train() client id: f_00003-11-0 loss: 0.734769  [   32/   43]
train() client id: f_00003-12-0 loss: 0.805385  [   32/   43]
train() client id: f_00004-0-0 loss: 0.754368  [   32/  306]
train() client id: f_00004-0-1 loss: 0.887751  [   64/  306]
train() client id: f_00004-0-2 loss: 0.643416  [   96/  306]
train() client id: f_00004-0-3 loss: 0.802788  [  128/  306]
train() client id: f_00004-0-4 loss: 0.794948  [  160/  306]
train() client id: f_00004-0-5 loss: 0.878312  [  192/  306]
train() client id: f_00004-0-6 loss: 0.870942  [  224/  306]
train() client id: f_00004-0-7 loss: 0.929725  [  256/  306]
train() client id: f_00004-0-8 loss: 0.828748  [  288/  306]
train() client id: f_00004-1-0 loss: 0.785646  [   32/  306]
train() client id: f_00004-1-1 loss: 0.754785  [   64/  306]
train() client id: f_00004-1-2 loss: 0.735819  [   96/  306]
train() client id: f_00004-1-3 loss: 0.757583  [  128/  306]
train() client id: f_00004-1-4 loss: 0.768777  [  160/  306]
train() client id: f_00004-1-5 loss: 0.925916  [  192/  306]
train() client id: f_00004-1-6 loss: 0.777957  [  224/  306]
train() client id: f_00004-1-7 loss: 0.830882  [  256/  306]
train() client id: f_00004-1-8 loss: 0.890730  [  288/  306]
train() client id: f_00004-2-0 loss: 0.928004  [   32/  306]
train() client id: f_00004-2-1 loss: 0.736484  [   64/  306]
train() client id: f_00004-2-2 loss: 0.774235  [   96/  306]
train() client id: f_00004-2-3 loss: 0.675954  [  128/  306]
train() client id: f_00004-2-4 loss: 0.996617  [  160/  306]
train() client id: f_00004-2-5 loss: 0.646158  [  192/  306]
train() client id: f_00004-2-6 loss: 0.891429  [  224/  306]
train() client id: f_00004-2-7 loss: 0.730898  [  256/  306]
train() client id: f_00004-2-8 loss: 0.986845  [  288/  306]
train() client id: f_00004-3-0 loss: 0.725778  [   32/  306]
train() client id: f_00004-3-1 loss: 0.786014  [   64/  306]
train() client id: f_00004-3-2 loss: 0.761858  [   96/  306]
train() client id: f_00004-3-3 loss: 0.898051  [  128/  306]
train() client id: f_00004-3-4 loss: 0.815028  [  160/  306]
train() client id: f_00004-3-5 loss: 0.919939  [  192/  306]
train() client id: f_00004-3-6 loss: 0.862909  [  224/  306]
train() client id: f_00004-3-7 loss: 0.783962  [  256/  306]
train() client id: f_00004-3-8 loss: 0.692654  [  288/  306]
train() client id: f_00004-4-0 loss: 0.760880  [   32/  306]
train() client id: f_00004-4-1 loss: 0.848461  [   64/  306]
train() client id: f_00004-4-2 loss: 0.810475  [   96/  306]
train() client id: f_00004-4-3 loss: 0.789689  [  128/  306]
train() client id: f_00004-4-4 loss: 0.815754  [  160/  306]
train() client id: f_00004-4-5 loss: 0.832353  [  192/  306]
train() client id: f_00004-4-6 loss: 0.733453  [  224/  306]
train() client id: f_00004-4-7 loss: 0.803565  [  256/  306]
train() client id: f_00004-4-8 loss: 0.853256  [  288/  306]
train() client id: f_00004-5-0 loss: 0.865626  [   32/  306]
train() client id: f_00004-5-1 loss: 0.825349  [   64/  306]
train() client id: f_00004-5-2 loss: 0.861355  [   96/  306]
train() client id: f_00004-5-3 loss: 0.744667  [  128/  306]
train() client id: f_00004-5-4 loss: 0.754169  [  160/  306]
train() client id: f_00004-5-5 loss: 0.805346  [  192/  306]
train() client id: f_00004-5-6 loss: 0.797853  [  224/  306]
train() client id: f_00004-5-7 loss: 0.855238  [  256/  306]
train() client id: f_00004-5-8 loss: 0.816016  [  288/  306]
train() client id: f_00004-6-0 loss: 0.750154  [   32/  306]
train() client id: f_00004-6-1 loss: 0.748578  [   64/  306]
train() client id: f_00004-6-2 loss: 1.016138  [   96/  306]
train() client id: f_00004-6-3 loss: 0.772890  [  128/  306]
train() client id: f_00004-6-4 loss: 0.808405  [  160/  306]
train() client id: f_00004-6-5 loss: 0.774294  [  192/  306]
train() client id: f_00004-6-6 loss: 0.774981  [  224/  306]
train() client id: f_00004-6-7 loss: 0.853061  [  256/  306]
train() client id: f_00004-6-8 loss: 0.760288  [  288/  306]
train() client id: f_00004-7-0 loss: 0.893756  [   32/  306]
train() client id: f_00004-7-1 loss: 0.746293  [   64/  306]
train() client id: f_00004-7-2 loss: 0.722866  [   96/  306]
train() client id: f_00004-7-3 loss: 0.804867  [  128/  306]
train() client id: f_00004-7-4 loss: 0.742130  [  160/  306]
train() client id: f_00004-7-5 loss: 0.936357  [  192/  306]
train() client id: f_00004-7-6 loss: 0.869581  [  224/  306]
train() client id: f_00004-7-7 loss: 0.734771  [  256/  306]
train() client id: f_00004-7-8 loss: 0.741311  [  288/  306]
train() client id: f_00004-8-0 loss: 0.797725  [   32/  306]
train() client id: f_00004-8-1 loss: 0.776659  [   64/  306]
train() client id: f_00004-8-2 loss: 0.681911  [   96/  306]
train() client id: f_00004-8-3 loss: 0.773865  [  128/  306]
train() client id: f_00004-8-4 loss: 0.856883  [  160/  306]
train() client id: f_00004-8-5 loss: 0.924652  [  192/  306]
train() client id: f_00004-8-6 loss: 0.682790  [  224/  306]
train() client id: f_00004-8-7 loss: 0.856967  [  256/  306]
train() client id: f_00004-8-8 loss: 0.831585  [  288/  306]
train() client id: f_00004-9-0 loss: 0.619920  [   32/  306]
train() client id: f_00004-9-1 loss: 0.767582  [   64/  306]
train() client id: f_00004-9-2 loss: 0.719291  [   96/  306]
train() client id: f_00004-9-3 loss: 0.834383  [  128/  306]
train() client id: f_00004-9-4 loss: 0.895614  [  160/  306]
train() client id: f_00004-9-5 loss: 0.851798  [  192/  306]
train() client id: f_00004-9-6 loss: 0.819099  [  224/  306]
train() client id: f_00004-9-7 loss: 0.838912  [  256/  306]
train() client id: f_00004-9-8 loss: 0.766757  [  288/  306]
train() client id: f_00004-10-0 loss: 0.808912  [   32/  306]
train() client id: f_00004-10-1 loss: 0.741905  [   64/  306]
train() client id: f_00004-10-2 loss: 0.762497  [   96/  306]
train() client id: f_00004-10-3 loss: 0.815529  [  128/  306]
train() client id: f_00004-10-4 loss: 0.872665  [  160/  306]
train() client id: f_00004-10-5 loss: 0.747580  [  192/  306]
train() client id: f_00004-10-6 loss: 0.745965  [  224/  306]
train() client id: f_00004-10-7 loss: 0.930902  [  256/  306]
train() client id: f_00004-10-8 loss: 0.865643  [  288/  306]
train() client id: f_00004-11-0 loss: 0.623881  [   32/  306]
train() client id: f_00004-11-1 loss: 0.786678  [   64/  306]
train() client id: f_00004-11-2 loss: 0.883626  [   96/  306]
train() client id: f_00004-11-3 loss: 0.823638  [  128/  306]
train() client id: f_00004-11-4 loss: 0.806886  [  160/  306]
train() client id: f_00004-11-5 loss: 0.838302  [  192/  306]
train() client id: f_00004-11-6 loss: 0.868098  [  224/  306]
train() client id: f_00004-11-7 loss: 0.731002  [  256/  306]
train() client id: f_00004-11-8 loss: 0.801597  [  288/  306]
train() client id: f_00004-12-0 loss: 0.897040  [   32/  306]
train() client id: f_00004-12-1 loss: 0.870932  [   64/  306]
train() client id: f_00004-12-2 loss: 0.768834  [   96/  306]
train() client id: f_00004-12-3 loss: 0.732719  [  128/  306]
train() client id: f_00004-12-4 loss: 0.748839  [  160/  306]
train() client id: f_00004-12-5 loss: 0.760674  [  192/  306]
train() client id: f_00004-12-6 loss: 0.878205  [  224/  306]
train() client id: f_00004-12-7 loss: 0.790435  [  256/  306]
train() client id: f_00004-12-8 loss: 0.763506  [  288/  306]
train() client id: f_00005-0-0 loss: 0.579610  [   32/  146]
train() client id: f_00005-0-1 loss: 0.593194  [   64/  146]
train() client id: f_00005-0-2 loss: 0.628711  [   96/  146]
train() client id: f_00005-0-3 loss: 0.569158  [  128/  146]
train() client id: f_00005-1-0 loss: 0.786304  [   32/  146]
train() client id: f_00005-1-1 loss: 0.345996  [   64/  146]
train() client id: f_00005-1-2 loss: 0.766148  [   96/  146]
train() client id: f_00005-1-3 loss: 0.356832  [  128/  146]
train() client id: f_00005-2-0 loss: 0.592814  [   32/  146]
train() client id: f_00005-2-1 loss: 0.715599  [   64/  146]
train() client id: f_00005-2-2 loss: 0.567946  [   96/  146]
train() client id: f_00005-2-3 loss: 0.500089  [  128/  146]
train() client id: f_00005-3-0 loss: 0.464615  [   32/  146]
train() client id: f_00005-3-1 loss: 0.513737  [   64/  146]
train() client id: f_00005-3-2 loss: 0.787834  [   96/  146]
train() client id: f_00005-3-3 loss: 0.531463  [  128/  146]
train() client id: f_00005-4-0 loss: 0.428495  [   32/  146]
train() client id: f_00005-4-1 loss: 0.696857  [   64/  146]
train() client id: f_00005-4-2 loss: 0.381934  [   96/  146]
train() client id: f_00005-4-3 loss: 0.697665  [  128/  146]
train() client id: f_00005-5-0 loss: 0.556574  [   32/  146]
train() client id: f_00005-5-1 loss: 0.617899  [   64/  146]
train() client id: f_00005-5-2 loss: 0.348443  [   96/  146]
train() client id: f_00005-5-3 loss: 0.747880  [  128/  146]
train() client id: f_00005-6-0 loss: 0.764478  [   32/  146]
train() client id: f_00005-6-1 loss: 0.287206  [   64/  146]
train() client id: f_00005-6-2 loss: 0.508103  [   96/  146]
train() client id: f_00005-6-3 loss: 0.595607  [  128/  146]
train() client id: f_00005-7-0 loss: 0.663965  [   32/  146]
train() client id: f_00005-7-1 loss: 0.730071  [   64/  146]
train() client id: f_00005-7-2 loss: 0.480771  [   96/  146]
train() client id: f_00005-7-3 loss: 0.582232  [  128/  146]
train() client id: f_00005-8-0 loss: 0.493664  [   32/  146]
train() client id: f_00005-8-1 loss: 0.484041  [   64/  146]
train() client id: f_00005-8-2 loss: 0.544586  [   96/  146]
train() client id: f_00005-8-3 loss: 0.814658  [  128/  146]
train() client id: f_00005-9-0 loss: 0.578948  [   32/  146]
train() client id: f_00005-9-1 loss: 0.813516  [   64/  146]
train() client id: f_00005-9-2 loss: 0.488230  [   96/  146]
train() client id: f_00005-9-3 loss: 0.243381  [  128/  146]
train() client id: f_00005-10-0 loss: 0.469195  [   32/  146]
train() client id: f_00005-10-1 loss: 0.572981  [   64/  146]
train() client id: f_00005-10-2 loss: 0.774269  [   96/  146]
train() client id: f_00005-10-3 loss: 0.547684  [  128/  146]
train() client id: f_00005-11-0 loss: 0.729765  [   32/  146]
train() client id: f_00005-11-1 loss: 0.413799  [   64/  146]
train() client id: f_00005-11-2 loss: 0.603950  [   96/  146]
train() client id: f_00005-11-3 loss: 0.689305  [  128/  146]
train() client id: f_00005-12-0 loss: 0.463932  [   32/  146]
train() client id: f_00005-12-1 loss: 0.811341  [   64/  146]
train() client id: f_00005-12-2 loss: 0.703741  [   96/  146]
train() client id: f_00005-12-3 loss: 0.478462  [  128/  146]
train() client id: f_00006-0-0 loss: 0.485269  [   32/   54]
train() client id: f_00006-1-0 loss: 0.412153  [   32/   54]
train() client id: f_00006-2-0 loss: 0.503500  [   32/   54]
train() client id: f_00006-3-0 loss: 0.421714  [   32/   54]
train() client id: f_00006-4-0 loss: 0.441345  [   32/   54]
train() client id: f_00006-5-0 loss: 0.374706  [   32/   54]
train() client id: f_00006-6-0 loss: 0.486414  [   32/   54]
train() client id: f_00006-7-0 loss: 0.498427  [   32/   54]
train() client id: f_00006-8-0 loss: 0.451600  [   32/   54]
train() client id: f_00006-9-0 loss: 0.466093  [   32/   54]
train() client id: f_00006-10-0 loss: 0.498400  [   32/   54]
train() client id: f_00006-11-0 loss: 0.440580  [   32/   54]
train() client id: f_00006-12-0 loss: 0.484570  [   32/   54]
train() client id: f_00007-0-0 loss: 0.642446  [   32/  179]
train() client id: f_00007-0-1 loss: 0.725122  [   64/  179]
train() client id: f_00007-0-2 loss: 0.555549  [   96/  179]
train() client id: f_00007-0-3 loss: 0.771598  [  128/  179]
train() client id: f_00007-0-4 loss: 0.930532  [  160/  179]
train() client id: f_00007-1-0 loss: 0.606264  [   32/  179]
train() client id: f_00007-1-1 loss: 0.529813  [   64/  179]
train() client id: f_00007-1-2 loss: 0.694175  [   96/  179]
train() client id: f_00007-1-3 loss: 0.928426  [  128/  179]
train() client id: f_00007-1-4 loss: 0.845806  [  160/  179]
train() client id: f_00007-2-0 loss: 0.637843  [   32/  179]
train() client id: f_00007-2-1 loss: 0.752021  [   64/  179]
train() client id: f_00007-2-2 loss: 0.717461  [   96/  179]
train() client id: f_00007-2-3 loss: 0.723133  [  128/  179]
train() client id: f_00007-2-4 loss: 0.662788  [  160/  179]
train() client id: f_00007-3-0 loss: 0.632689  [   32/  179]
train() client id: f_00007-3-1 loss: 0.962176  [   64/  179]
train() client id: f_00007-3-2 loss: 0.550640  [   96/  179]
train() client id: f_00007-3-3 loss: 0.686101  [  128/  179]
train() client id: f_00007-3-4 loss: 0.529321  [  160/  179]
train() client id: f_00007-4-0 loss: 0.828831  [   32/  179]
train() client id: f_00007-4-1 loss: 0.912031  [   64/  179]
train() client id: f_00007-4-2 loss: 0.524617  [   96/  179]
train() client id: f_00007-4-3 loss: 0.598371  [  128/  179]
train() client id: f_00007-4-4 loss: 0.557630  [  160/  179]
train() client id: f_00007-5-0 loss: 0.685166  [   32/  179]
train() client id: f_00007-5-1 loss: 0.585732  [   64/  179]
train() client id: f_00007-5-2 loss: 0.707807  [   96/  179]
train() client id: f_00007-5-3 loss: 0.849651  [  128/  179]
train() client id: f_00007-5-4 loss: 0.662185  [  160/  179]
train() client id: f_00007-6-0 loss: 0.588471  [   32/  179]
train() client id: f_00007-6-1 loss: 0.645254  [   64/  179]
train() client id: f_00007-6-2 loss: 0.818781  [   96/  179]
train() client id: f_00007-6-3 loss: 0.643036  [  128/  179]
train() client id: f_00007-6-4 loss: 0.661462  [  160/  179]
train() client id: f_00007-7-0 loss: 0.539432  [   32/  179]
train() client id: f_00007-7-1 loss: 0.584103  [   64/  179]
train() client id: f_00007-7-2 loss: 0.631147  [   96/  179]
train() client id: f_00007-7-3 loss: 0.641455  [  128/  179]
train() client id: f_00007-7-4 loss: 0.997387  [  160/  179]
train() client id: f_00007-8-0 loss: 0.551341  [   32/  179]
train() client id: f_00007-8-1 loss: 0.563889  [   64/  179]
train() client id: f_00007-8-2 loss: 0.513336  [   96/  179]
train() client id: f_00007-8-3 loss: 0.812458  [  128/  179]
train() client id: f_00007-8-4 loss: 0.948692  [  160/  179]
train() client id: f_00007-9-0 loss: 0.517115  [   32/  179]
train() client id: f_00007-9-1 loss: 0.608225  [   64/  179]
train() client id: f_00007-9-2 loss: 1.117609  [   96/  179]
train() client id: f_00007-9-3 loss: 0.517983  [  128/  179]
train() client id: f_00007-9-4 loss: 0.704996  [  160/  179]
train() client id: f_00007-10-0 loss: 0.503290  [   32/  179]
train() client id: f_00007-10-1 loss: 0.694922  [   64/  179]
train() client id: f_00007-10-2 loss: 0.676982  [   96/  179]
train() client id: f_00007-10-3 loss: 0.775986  [  128/  179]
train() client id: f_00007-10-4 loss: 0.722293  [  160/  179]
train() client id: f_00007-11-0 loss: 0.854784  [   32/  179]
train() client id: f_00007-11-1 loss: 0.588290  [   64/  179]
train() client id: f_00007-11-2 loss: 0.664249  [   96/  179]
train() client id: f_00007-11-3 loss: 0.694366  [  128/  179]
train() client id: f_00007-11-4 loss: 0.654368  [  160/  179]
train() client id: f_00007-12-0 loss: 0.707523  [   32/  179]
train() client id: f_00007-12-1 loss: 0.813325  [   64/  179]
train() client id: f_00007-12-2 loss: 0.697699  [   96/  179]
train() client id: f_00007-12-3 loss: 0.497728  [  128/  179]
train() client id: f_00007-12-4 loss: 0.596014  [  160/  179]
train() client id: f_00008-0-0 loss: 0.726314  [   32/  130]
train() client id: f_00008-0-1 loss: 0.663184  [   64/  130]
train() client id: f_00008-0-2 loss: 0.710228  [   96/  130]
train() client id: f_00008-0-3 loss: 0.648163  [  128/  130]
train() client id: f_00008-1-0 loss: 0.752350  [   32/  130]
train() client id: f_00008-1-1 loss: 0.663591  [   64/  130]
train() client id: f_00008-1-2 loss: 0.728466  [   96/  130]
train() client id: f_00008-1-3 loss: 0.617121  [  128/  130]
train() client id: f_00008-2-0 loss: 0.737085  [   32/  130]
train() client id: f_00008-2-1 loss: 0.668609  [   64/  130]
train() client id: f_00008-2-2 loss: 0.680589  [   96/  130]
train() client id: f_00008-2-3 loss: 0.653213  [  128/  130]
train() client id: f_00008-3-0 loss: 0.672911  [   32/  130]
train() client id: f_00008-3-1 loss: 0.629691  [   64/  130]
train() client id: f_00008-3-2 loss: 0.608750  [   96/  130]
train() client id: f_00008-3-3 loss: 0.806514  [  128/  130]
train() client id: f_00008-4-0 loss: 0.626773  [   32/  130]
train() client id: f_00008-4-1 loss: 0.780962  [   64/  130]
train() client id: f_00008-4-2 loss: 0.718986  [   96/  130]
train() client id: f_00008-4-3 loss: 0.603226  [  128/  130]
train() client id: f_00008-5-0 loss: 0.736265  [   32/  130]
train() client id: f_00008-5-1 loss: 0.738742  [   64/  130]
train() client id: f_00008-5-2 loss: 0.678416  [   96/  130]
train() client id: f_00008-5-3 loss: 0.602994  [  128/  130]
train() client id: f_00008-6-0 loss: 0.643613  [   32/  130]
train() client id: f_00008-6-1 loss: 0.649389  [   64/  130]
train() client id: f_00008-6-2 loss: 0.677329  [   96/  130]
train() client id: f_00008-6-3 loss: 0.787488  [  128/  130]
train() client id: f_00008-7-0 loss: 0.730495  [   32/  130]
train() client id: f_00008-7-1 loss: 0.697146  [   64/  130]
train() client id: f_00008-7-2 loss: 0.724902  [   96/  130]
train() client id: f_00008-7-3 loss: 0.620641  [  128/  130]
train() client id: f_00008-8-0 loss: 0.636483  [   32/  130]
train() client id: f_00008-8-1 loss: 0.729556  [   64/  130]
train() client id: f_00008-8-2 loss: 0.640335  [   96/  130]
train() client id: f_00008-8-3 loss: 0.721119  [  128/  130]
train() client id: f_00008-9-0 loss: 0.657337  [   32/  130]
train() client id: f_00008-9-1 loss: 0.720855  [   64/  130]
train() client id: f_00008-9-2 loss: 0.767239  [   96/  130]
train() client id: f_00008-9-3 loss: 0.623549  [  128/  130]
train() client id: f_00008-10-0 loss: 0.705422  [   32/  130]
train() client id: f_00008-10-1 loss: 0.642604  [   64/  130]
train() client id: f_00008-10-2 loss: 0.642123  [   96/  130]
train() client id: f_00008-10-3 loss: 0.764397  [  128/  130]
train() client id: f_00008-11-0 loss: 0.717533  [   32/  130]
train() client id: f_00008-11-1 loss: 0.736632  [   64/  130]
train() client id: f_00008-11-2 loss: 0.578696  [   96/  130]
train() client id: f_00008-11-3 loss: 0.716701  [  128/  130]
train() client id: f_00008-12-0 loss: 0.659869  [   32/  130]
train() client id: f_00008-12-1 loss: 0.735466  [   64/  130]
train() client id: f_00008-12-2 loss: 0.654927  [   96/  130]
train() client id: f_00008-12-3 loss: 0.725507  [  128/  130]
train() client id: f_00009-0-0 loss: 1.198665  [   32/  118]
train() client id: f_00009-0-1 loss: 0.985664  [   64/  118]
train() client id: f_00009-0-2 loss: 1.342983  [   96/  118]
train() client id: f_00009-1-0 loss: 1.270581  [   32/  118]
train() client id: f_00009-1-1 loss: 1.038574  [   64/  118]
train() client id: f_00009-1-2 loss: 1.122093  [   96/  118]
train() client id: f_00009-2-0 loss: 1.285835  [   32/  118]
train() client id: f_00009-2-1 loss: 0.951731  [   64/  118]
train() client id: f_00009-2-2 loss: 1.219660  [   96/  118]
train() client id: f_00009-3-0 loss: 1.021938  [   32/  118]
train() client id: f_00009-3-1 loss: 0.957750  [   64/  118]
train() client id: f_00009-3-2 loss: 1.104423  [   96/  118]
train() client id: f_00009-4-0 loss: 1.045806  [   32/  118]
train() client id: f_00009-4-1 loss: 0.985210  [   64/  118]
train() client id: f_00009-4-2 loss: 1.038094  [   96/  118]
train() client id: f_00009-5-0 loss: 0.918501  [   32/  118]
train() client id: f_00009-5-1 loss: 1.104551  [   64/  118]
train() client id: f_00009-5-2 loss: 0.910934  [   96/  118]
train() client id: f_00009-6-0 loss: 0.800767  [   32/  118]
train() client id: f_00009-6-1 loss: 1.038990  [   64/  118]
train() client id: f_00009-6-2 loss: 0.950659  [   96/  118]
train() client id: f_00009-7-0 loss: 0.937467  [   32/  118]
train() client id: f_00009-7-1 loss: 1.015401  [   64/  118]
train() client id: f_00009-7-2 loss: 0.875845  [   96/  118]
train() client id: f_00009-8-0 loss: 0.911918  [   32/  118]
train() client id: f_00009-8-1 loss: 0.998623  [   64/  118]
train() client id: f_00009-8-2 loss: 0.958820  [   96/  118]
train() client id: f_00009-9-0 loss: 1.128003  [   32/  118]
train() client id: f_00009-9-1 loss: 0.894469  [   64/  118]
train() client id: f_00009-9-2 loss: 0.834240  [   96/  118]
train() client id: f_00009-10-0 loss: 1.115759  [   32/  118]
train() client id: f_00009-10-1 loss: 0.699980  [   64/  118]
train() client id: f_00009-10-2 loss: 0.917034  [   96/  118]
train() client id: f_00009-11-0 loss: 0.802530  [   32/  118]
train() client id: f_00009-11-1 loss: 0.988780  [   64/  118]
train() client id: f_00009-11-2 loss: 0.890579  [   96/  118]
train() client id: f_00009-12-0 loss: 0.798207  [   32/  118]
train() client id: f_00009-12-1 loss: 0.803174  [   64/  118]
train() client id: f_00009-12-2 loss: 1.153170  [   96/  118]
At round 43 accuracy: 0.6419098143236074
At round 43 training accuracy: 0.5814889336016097
At round 43 training loss: 0.8411819806028048
update_location
xs = -3.905658 4.200318 235.009024 18.811294 0.979296 3.956410 -197.443192 -176.324852 219.663977 -162.060879 
ys = 227.587959 210.555839 1.320614 -197.455176 189.350187 172.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -5.711426168117436
ys mean: 62.3941425355287
dists_uav = 248.619254 233.133875 255.403573 222.131518 214.136527 199.700736 221.338439 202.709471 241.993663 190.472413 
uav_gains = -111.179050 -109.938241 -111.762242 -109.138137 -108.593508 -107.663150 -109.082861 -107.853230 -110.631688 -107.085431 
uav_gains_db_mean: -109.29275401911372
dists_bs = 178.993763 182.693655 445.277362 419.863878 176.563418 178.969759 179.037236 174.182691 424.904898 171.487358 
bs_gains = -102.646520 -102.895316 -113.728716 -113.014097 -102.480280 -102.644889 -102.649473 -102.315199 -113.159228 -102.125558 
bs_gains_db_mean: -105.76592776872943
Round 44
-------------------------------
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.18624546 10.67895549  5.11317553  1.85111052 12.31468286  5.9249859
  2.28997512  7.27266655  5.37158258  4.80445503]
obj_prev = 60.80783503027712
eta_min = 1.6905775058448652e-18	eta_max = 0.9354514077356543
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 14.082134182864616	eta = 0.9090909090909091
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 27.112785949573404	eta = 0.4721735416659381
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 20.558519671827195	eta = 0.622707294620243
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.371299283019138	eta = 0.6608715285021035
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.305821021572196	eta = 0.6631129622477986
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.305603441902093	eta = 0.6631204357204616
eta = 0.6631204357204616
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [0.03389812 0.07129367 0.03336005 0.01156841 0.08232401 0.03927877
 0.01452777 0.04815685 0.03497425 0.03174586]
ene_total = [1.77121153 3.05330088 1.77028457 0.8430904  3.47724339 1.80411205
 0.95686313 2.2367863  1.88825588 1.50445532]
ti_comp = [0.59818702 0.64270661 0.59304455 0.61337277 0.644077   0.64353969
 0.6137322  0.62114382 0.5788735  0.64520803]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.80350798e-06 5.48286200e-05 6.59759497e-06 2.57188538e-07
 8.40589437e-05 9.14540875e-06 5.08766715e-07 1.80913087e-05
 7.97917407e-06 4.80332220e-06]
ene_total = [0.45223865 0.28084755 0.47224396 0.39288439 0.2766519  0.27582751
 0.39149537 0.36333529 0.52744819 0.26916572]
optimize_network iter = 0 obj = 3.7021385292782734
eta = 0.6631204357204616
freqs = [28334048.79448884 55463616.087887   28126095.25989743  9430159.49291534
 63908512.8336163  30517755.77420575 11835590.3817379  38764656.20847187
 30208889.06833503 24601258.06337142]
eta_min = 0.6631204357204632	eta_max = 0.6660036163441293
af = 0.006161926359054621	bf = 1.2446435035998953	zeta = 0.006778118994960084	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [1.57849488e-06 1.27208928e-05 1.53072061e-06 5.96708039e-08
 1.95026760e-05 2.12184375e-06 1.18039937e-07 4.19739907e-06
 1.85126341e-06 1.11442796e-06]
ene_total = [1.67509603 1.03465718 1.74925201 1.45587345 1.015872   1.02111426
 1.45069832 1.34439885 1.95366795 0.99690874]
ti_comp = [0.59207349 0.63659309 0.58693102 0.60725925 0.63796347 0.63742617
 0.60761867 0.6150303  0.57275998 0.63909451]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.72701829e-06 5.41347322e-05 6.52458862e-06 2.54167082e-07
 8.29917378e-05 9.02944383e-06 5.02783784e-07 1.78742687e-05
 7.89490544e-06 4.74218027e-06]
ene_total = [0.45613954 0.28324469 0.47631768 0.39627579 0.27899817 0.278204
 0.39487466 0.36646322 0.53199801 0.27148686]
optimize_network iter = 1 obj = 3.734002631064773
eta = 0.6660036163441293
freqs = [28324287.6160973  55404880.64581718 28118923.39438503  9424501.49550652
 63839529.8841224  30485058.77113441 11828419.4116198  38736518.87654436
 30208889.06833503 24574290.61426852]
eta_min = 0.6660036163441323	eta_max = 0.6660036163441293
af = 0.006150348470560822	bf = 1.2446435035998953	zeta = 0.006765383317616905	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [1.57740747e-06 1.26939645e-05 1.52994007e-06 5.95992217e-08
 1.94605963e-05 2.11729946e-06 1.17896944e-07 4.19130791e-06
 1.85126341e-06 1.11198606e-06]
ene_total = [1.67509587 1.0346533  1.74925189 1.45587344 1.01586593 1.0211136
 1.4506983  1.34439798 1.95366795 0.99690839]
ti_comp = [0.59207349 0.63659309 0.58693102 0.60725925 0.63796347 0.63742617
 0.60761867 0.6150303  0.57275998 0.63909451]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.72701829e-06 5.41347322e-05 6.52458862e-06 2.54167082e-07
 8.29917378e-05 9.02944383e-06 5.02783784e-07 1.78742687e-05
 7.89490544e-06 4.74218027e-06]
ene_total = [0.45613954 0.28324469 0.47631768 0.39627579 0.27899817 0.278204
 0.39487466 0.36646322 0.53199801 0.27148686]
optimize_network iter = 2 obj = 3.734002631064773
eta = 0.6660036163441293
freqs = [28324287.6160973  55404880.64581718 28118923.39438503  9424501.49550652
 63839529.8841224  30485058.77113441 11828419.4116198  38736518.87654436
 30208889.06833503 24574290.61426852]
Done!
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.57055255e-06 5.28755963e-05 6.37283125e-06 2.48255333e-07
 8.10614081e-05 8.81942529e-06 4.91089385e-07 1.74585257e-05
 7.71127547e-06 4.63188047e-06]
ene_total = [0.01162015 0.00721449 0.01213419 0.01009525 0.00710564 0.00708713
 0.01005955 0.00933535 0.01355264 0.0069161 ]
At round 44 energy consumption: 0.09512048272349369
At round 44 eta: 0.6660036163441293
At round 44 a_n: 13.110585589604415
At round 44 local rounds: 13.309571309784454
At round 44 global rounds: 39.25367528264245
gradient difference: 0.4179946482181549
train() client id: f_00000-0-0 loss: 1.016563  [   32/  126]
train() client id: f_00000-0-1 loss: 1.144386  [   64/  126]
train() client id: f_00000-0-2 loss: 0.966489  [   96/  126]
train() client id: f_00000-1-0 loss: 0.842190  [   32/  126]
train() client id: f_00000-1-1 loss: 1.239032  [   64/  126]
train() client id: f_00000-1-2 loss: 1.093661  [   96/  126]
train() client id: f_00000-2-0 loss: 0.974846  [   32/  126]
train() client id: f_00000-2-1 loss: 1.006276  [   64/  126]
train() client id: f_00000-2-2 loss: 0.986538  [   96/  126]
train() client id: f_00000-3-0 loss: 1.047035  [   32/  126]
train() client id: f_00000-3-1 loss: 1.027976  [   64/  126]
train() client id: f_00000-3-2 loss: 0.846686  [   96/  126]
train() client id: f_00000-4-0 loss: 0.951450  [   32/  126]
train() client id: f_00000-4-1 loss: 0.860660  [   64/  126]
train() client id: f_00000-4-2 loss: 0.890855  [   96/  126]
train() client id: f_00000-5-0 loss: 1.027666  [   32/  126]
train() client id: f_00000-5-1 loss: 0.825381  [   64/  126]
train() client id: f_00000-5-2 loss: 0.865840  [   96/  126]
train() client id: f_00000-6-0 loss: 0.974041  [   32/  126]
train() client id: f_00000-6-1 loss: 0.685799  [   64/  126]
train() client id: f_00000-6-2 loss: 1.013349  [   96/  126]
train() client id: f_00000-7-0 loss: 0.921265  [   32/  126]
train() client id: f_00000-7-1 loss: 0.875687  [   64/  126]
train() client id: f_00000-7-2 loss: 0.981275  [   96/  126]
train() client id: f_00000-8-0 loss: 0.928298  [   32/  126]
train() client id: f_00000-8-1 loss: 0.882357  [   64/  126]
train() client id: f_00000-8-2 loss: 0.800181  [   96/  126]
train() client id: f_00000-9-0 loss: 0.987150  [   32/  126]
train() client id: f_00000-9-1 loss: 0.917953  [   64/  126]
train() client id: f_00000-9-2 loss: 0.853435  [   96/  126]
train() client id: f_00000-10-0 loss: 0.851251  [   32/  126]
train() client id: f_00000-10-1 loss: 0.883387  [   64/  126]
train() client id: f_00000-10-2 loss: 1.003119  [   96/  126]
train() client id: f_00000-11-0 loss: 0.840420  [   32/  126]
train() client id: f_00000-11-1 loss: 0.799597  [   64/  126]
train() client id: f_00000-11-2 loss: 0.979524  [   96/  126]
train() client id: f_00000-12-0 loss: 0.903865  [   32/  126]
train() client id: f_00000-12-1 loss: 0.748552  [   64/  126]
train() client id: f_00000-12-2 loss: 0.949892  [   96/  126]
train() client id: f_00001-0-0 loss: 0.467229  [   32/  265]
train() client id: f_00001-0-1 loss: 0.528128  [   64/  265]
train() client id: f_00001-0-2 loss: 0.591507  [   96/  265]
train() client id: f_00001-0-3 loss: 0.397159  [  128/  265]
train() client id: f_00001-0-4 loss: 0.531927  [  160/  265]
train() client id: f_00001-0-5 loss: 0.540156  [  192/  265]
train() client id: f_00001-0-6 loss: 0.548939  [  224/  265]
train() client id: f_00001-0-7 loss: 0.538173  [  256/  265]
train() client id: f_00001-1-0 loss: 0.450147  [   32/  265]
train() client id: f_00001-1-1 loss: 0.558756  [   64/  265]
train() client id: f_00001-1-2 loss: 0.451043  [   96/  265]
train() client id: f_00001-1-3 loss: 0.506196  [  128/  265]
train() client id: f_00001-1-4 loss: 0.577790  [  160/  265]
train() client id: f_00001-1-5 loss: 0.475105  [  192/  265]
train() client id: f_00001-1-6 loss: 0.392661  [  224/  265]
train() client id: f_00001-1-7 loss: 0.699319  [  256/  265]
train() client id: f_00001-2-0 loss: 0.524844  [   32/  265]
train() client id: f_00001-2-1 loss: 0.448647  [   64/  265]
train() client id: f_00001-2-2 loss: 0.582423  [   96/  265]
train() client id: f_00001-2-3 loss: 0.423177  [  128/  265]
train() client id: f_00001-2-4 loss: 0.503371  [  160/  265]
train() client id: f_00001-2-5 loss: 0.411415  [  192/  265]
train() client id: f_00001-2-6 loss: 0.495396  [  224/  265]
train() client id: f_00001-2-7 loss: 0.675221  [  256/  265]
train() client id: f_00001-3-0 loss: 0.524347  [   32/  265]
train() client id: f_00001-3-1 loss: 0.785561  [   64/  265]
train() client id: f_00001-3-2 loss: 0.441990  [   96/  265]
train() client id: f_00001-3-3 loss: 0.414179  [  128/  265]
train() client id: f_00001-3-4 loss: 0.407379  [  160/  265]
train() client id: f_00001-3-5 loss: 0.478677  [  192/  265]
train() client id: f_00001-3-6 loss: 0.601307  [  224/  265]
train() client id: f_00001-3-7 loss: 0.392558  [  256/  265]
train() client id: f_00001-4-0 loss: 0.527795  [   32/  265]
train() client id: f_00001-4-1 loss: 0.523175  [   64/  265]
train() client id: f_00001-4-2 loss: 0.589282  [   96/  265]
train() client id: f_00001-4-3 loss: 0.410076  [  128/  265]
train() client id: f_00001-4-4 loss: 0.519859  [  160/  265]
train() client id: f_00001-4-5 loss: 0.460527  [  192/  265]
train() client id: f_00001-4-6 loss: 0.499339  [  224/  265]
train() client id: f_00001-4-7 loss: 0.484568  [  256/  265]
train() client id: f_00001-5-0 loss: 0.401536  [   32/  265]
train() client id: f_00001-5-1 loss: 0.467097  [   64/  265]
train() client id: f_00001-5-2 loss: 0.506122  [   96/  265]
train() client id: f_00001-5-3 loss: 0.496754  [  128/  265]
train() client id: f_00001-5-4 loss: 0.556985  [  160/  265]
train() client id: f_00001-5-5 loss: 0.437401  [  192/  265]
train() client id: f_00001-5-6 loss: 0.417274  [  224/  265]
train() client id: f_00001-5-7 loss: 0.541894  [  256/  265]
train() client id: f_00001-6-0 loss: 0.529296  [   32/  265]
train() client id: f_00001-6-1 loss: 0.611935  [   64/  265]
train() client id: f_00001-6-2 loss: 0.500571  [   96/  265]
train() client id: f_00001-6-3 loss: 0.493027  [  128/  265]
train() client id: f_00001-6-4 loss: 0.501773  [  160/  265]
train() client id: f_00001-6-5 loss: 0.390978  [  192/  265]
train() client id: f_00001-6-6 loss: 0.465022  [  224/  265]
train() client id: f_00001-6-7 loss: 0.507909  [  256/  265]
train() client id: f_00001-7-0 loss: 0.499329  [   32/  265]
train() client id: f_00001-7-1 loss: 0.562022  [   64/  265]
train() client id: f_00001-7-2 loss: 0.457378  [   96/  265]
train() client id: f_00001-7-3 loss: 0.598364  [  128/  265]
train() client id: f_00001-7-4 loss: 0.452154  [  160/  265]
train() client id: f_00001-7-5 loss: 0.497304  [  192/  265]
train() client id: f_00001-7-6 loss: 0.428100  [  224/  265]
train() client id: f_00001-7-7 loss: 0.423579  [  256/  265]
train() client id: f_00001-8-0 loss: 0.435294  [   32/  265]
train() client id: f_00001-8-1 loss: 0.410444  [   64/  265]
train() client id: f_00001-8-2 loss: 0.459490  [   96/  265]
train() client id: f_00001-8-3 loss: 0.667228  [  128/  265]
train() client id: f_00001-8-4 loss: 0.627377  [  160/  265]
train() client id: f_00001-8-5 loss: 0.511761  [  192/  265]
train() client id: f_00001-8-6 loss: 0.435227  [  224/  265]
train() client id: f_00001-8-7 loss: 0.449839  [  256/  265]
train() client id: f_00001-9-0 loss: 0.387648  [   32/  265]
train() client id: f_00001-9-1 loss: 0.600822  [   64/  265]
train() client id: f_00001-9-2 loss: 0.413749  [   96/  265]
train() client id: f_00001-9-3 loss: 0.597292  [  128/  265]
train() client id: f_00001-9-4 loss: 0.476026  [  160/  265]
train() client id: f_00001-9-5 loss: 0.512166  [  192/  265]
train() client id: f_00001-9-6 loss: 0.425281  [  224/  265]
train() client id: f_00001-9-7 loss: 0.583441  [  256/  265]
train() client id: f_00001-10-0 loss: 0.404513  [   32/  265]
train() client id: f_00001-10-1 loss: 0.399786  [   64/  265]
train() client id: f_00001-10-2 loss: 0.474116  [   96/  265]
train() client id: f_00001-10-3 loss: 0.414089  [  128/  265]
train() client id: f_00001-10-4 loss: 0.497882  [  160/  265]
train() client id: f_00001-10-5 loss: 0.559265  [  192/  265]
train() client id: f_00001-10-6 loss: 0.475183  [  224/  265]
train() client id: f_00001-10-7 loss: 0.666949  [  256/  265]
train() client id: f_00001-11-0 loss: 0.490182  [   32/  265]
train() client id: f_00001-11-1 loss: 0.571379  [   64/  265]
train() client id: f_00001-11-2 loss: 0.505936  [   96/  265]
train() client id: f_00001-11-3 loss: 0.493048  [  128/  265]
train() client id: f_00001-11-4 loss: 0.478643  [  160/  265]
train() client id: f_00001-11-5 loss: 0.444825  [  192/  265]
train() client id: f_00001-11-6 loss: 0.497378  [  224/  265]
train() client id: f_00001-11-7 loss: 0.445653  [  256/  265]
train() client id: f_00001-12-0 loss: 0.393482  [   32/  265]
train() client id: f_00001-12-1 loss: 0.521736  [   64/  265]
train() client id: f_00001-12-2 loss: 0.520831  [   96/  265]
train() client id: f_00001-12-3 loss: 0.621269  [  128/  265]
train() client id: f_00001-12-4 loss: 0.548117  [  160/  265]
train() client id: f_00001-12-5 loss: 0.552055  [  192/  265]
train() client id: f_00001-12-6 loss: 0.413827  [  224/  265]
train() client id: f_00001-12-7 loss: 0.380155  [  256/  265]
train() client id: f_00002-0-0 loss: 1.009896  [   32/  124]
train() client id: f_00002-0-1 loss: 0.984240  [   64/  124]
train() client id: f_00002-0-2 loss: 0.772239  [   96/  124]
train() client id: f_00002-1-0 loss: 0.882500  [   32/  124]
train() client id: f_00002-1-1 loss: 0.815760  [   64/  124]
train() client id: f_00002-1-2 loss: 0.984512  [   96/  124]
train() client id: f_00002-2-0 loss: 0.754095  [   32/  124]
train() client id: f_00002-2-1 loss: 0.986122  [   64/  124]
train() client id: f_00002-2-2 loss: 0.955141  [   96/  124]
train() client id: f_00002-3-0 loss: 1.048548  [   32/  124]
train() client id: f_00002-3-1 loss: 0.793607  [   64/  124]
train() client id: f_00002-3-2 loss: 0.850587  [   96/  124]
train() client id: f_00002-4-0 loss: 0.762740  [   32/  124]
train() client id: f_00002-4-1 loss: 0.781786  [   64/  124]
train() client id: f_00002-4-2 loss: 0.823448  [   96/  124]
train() client id: f_00002-5-0 loss: 0.884460  [   32/  124]
train() client id: f_00002-5-1 loss: 0.897514  [   64/  124]
train() client id: f_00002-5-2 loss: 0.673827  [   96/  124]
train() client id: f_00002-6-0 loss: 0.882433  [   32/  124]
train() client id: f_00002-6-1 loss: 0.637150  [   64/  124]
train() client id: f_00002-6-2 loss: 0.749448  [   96/  124]
train() client id: f_00002-7-0 loss: 0.737121  [   32/  124]
train() client id: f_00002-7-1 loss: 0.857576  [   64/  124]
train() client id: f_00002-7-2 loss: 0.707525  [   96/  124]
train() client id: f_00002-8-0 loss: 0.613002  [   32/  124]
train() client id: f_00002-8-1 loss: 0.782651  [   64/  124]
train() client id: f_00002-8-2 loss: 0.797742  [   96/  124]
train() client id: f_00002-9-0 loss: 0.667236  [   32/  124]
train() client id: f_00002-9-1 loss: 0.674587  [   64/  124]
train() client id: f_00002-9-2 loss: 0.752981  [   96/  124]
train() client id: f_00002-10-0 loss: 0.681109  [   32/  124]
train() client id: f_00002-10-1 loss: 0.704759  [   64/  124]
train() client id: f_00002-10-2 loss: 0.523236  [   96/  124]
train() client id: f_00002-11-0 loss: 0.970455  [   32/  124]
train() client id: f_00002-11-1 loss: 0.487139  [   64/  124]
train() client id: f_00002-11-2 loss: 0.695493  [   96/  124]
train() client id: f_00002-12-0 loss: 0.854853  [   32/  124]
train() client id: f_00002-12-1 loss: 0.648691  [   64/  124]
train() client id: f_00002-12-2 loss: 0.659661  [   96/  124]
train() client id: f_00003-0-0 loss: 0.437673  [   32/   43]
train() client id: f_00003-1-0 loss: 0.629400  [   32/   43]
train() client id: f_00003-2-0 loss: 0.773787  [   32/   43]
train() client id: f_00003-3-0 loss: 0.579379  [   32/   43]
train() client id: f_00003-4-0 loss: 0.596967  [   32/   43]
train() client id: f_00003-5-0 loss: 0.426762  [   32/   43]
train() client id: f_00003-6-0 loss: 0.542388  [   32/   43]
train() client id: f_00003-7-0 loss: 0.718264  [   32/   43]
train() client id: f_00003-8-0 loss: 0.670324  [   32/   43]
train() client id: f_00003-9-0 loss: 0.786561  [   32/   43]
train() client id: f_00003-10-0 loss: 0.522747  [   32/   43]
train() client id: f_00003-11-0 loss: 0.747985  [   32/   43]
train() client id: f_00003-12-0 loss: 0.604942  [   32/   43]
train() client id: f_00004-0-0 loss: 0.872002  [   32/  306]
train() client id: f_00004-0-1 loss: 0.837035  [   64/  306]
train() client id: f_00004-0-2 loss: 0.824687  [   96/  306]
train() client id: f_00004-0-3 loss: 0.786507  [  128/  306]
train() client id: f_00004-0-4 loss: 0.893130  [  160/  306]
train() client id: f_00004-0-5 loss: 0.988442  [  192/  306]
train() client id: f_00004-0-6 loss: 0.838366  [  224/  306]
train() client id: f_00004-0-7 loss: 0.718077  [  256/  306]
train() client id: f_00004-0-8 loss: 0.824247  [  288/  306]
train() client id: f_00004-1-0 loss: 0.897334  [   32/  306]
train() client id: f_00004-1-1 loss: 0.776026  [   64/  306]
train() client id: f_00004-1-2 loss: 0.775803  [   96/  306]
train() client id: f_00004-1-3 loss: 0.801554  [  128/  306]
train() client id: f_00004-1-4 loss: 0.815611  [  160/  306]
train() client id: f_00004-1-5 loss: 0.839049  [  192/  306]
train() client id: f_00004-1-6 loss: 0.776370  [  224/  306]
train() client id: f_00004-1-7 loss: 1.008711  [  256/  306]
train() client id: f_00004-1-8 loss: 0.891201  [  288/  306]
train() client id: f_00004-2-0 loss: 0.896420  [   32/  306]
train() client id: f_00004-2-1 loss: 0.897429  [   64/  306]
train() client id: f_00004-2-2 loss: 0.886655  [   96/  306]
train() client id: f_00004-2-3 loss: 0.880174  [  128/  306]
train() client id: f_00004-2-4 loss: 0.826713  [  160/  306]
train() client id: f_00004-2-5 loss: 0.754107  [  192/  306]
train() client id: f_00004-2-6 loss: 0.825549  [  224/  306]
train() client id: f_00004-2-7 loss: 0.801575  [  256/  306]
train() client id: f_00004-2-8 loss: 0.801836  [  288/  306]
train() client id: f_00004-3-0 loss: 0.808728  [   32/  306]
train() client id: f_00004-3-1 loss: 0.829079  [   64/  306]
train() client id: f_00004-3-2 loss: 0.783607  [   96/  306]
train() client id: f_00004-3-3 loss: 0.768919  [  128/  306]
train() client id: f_00004-3-4 loss: 0.772112  [  160/  306]
train() client id: f_00004-3-5 loss: 0.844121  [  192/  306]
train() client id: f_00004-3-6 loss: 0.929518  [  224/  306]
train() client id: f_00004-3-7 loss: 0.947769  [  256/  306]
train() client id: f_00004-3-8 loss: 0.924215  [  288/  306]
train() client id: f_00004-4-0 loss: 0.881348  [   32/  306]
train() client id: f_00004-4-1 loss: 0.771868  [   64/  306]
train() client id: f_00004-4-2 loss: 0.814101  [   96/  306]
train() client id: f_00004-4-3 loss: 0.776940  [  128/  306]
train() client id: f_00004-4-4 loss: 0.801826  [  160/  306]
train() client id: f_00004-4-5 loss: 0.800637  [  192/  306]
train() client id: f_00004-4-6 loss: 0.897517  [  224/  306]
train() client id: f_00004-4-7 loss: 0.786206  [  256/  306]
train() client id: f_00004-4-8 loss: 0.978702  [  288/  306]
train() client id: f_00004-5-0 loss: 0.773014  [   32/  306]
train() client id: f_00004-5-1 loss: 0.852671  [   64/  306]
train() client id: f_00004-5-2 loss: 0.786699  [   96/  306]
train() client id: f_00004-5-3 loss: 0.871080  [  128/  306]
train() client id: f_00004-5-4 loss: 0.871945  [  160/  306]
train() client id: f_00004-5-5 loss: 0.867617  [  192/  306]
train() client id: f_00004-5-6 loss: 0.816728  [  224/  306]
train() client id: f_00004-5-7 loss: 0.872209  [  256/  306]
train() client id: f_00004-5-8 loss: 0.842232  [  288/  306]
train() client id: f_00004-6-0 loss: 0.928951  [   32/  306]
train() client id: f_00004-6-1 loss: 0.941975  [   64/  306]
train() client id: f_00004-6-2 loss: 0.734326  [   96/  306]
train() client id: f_00004-6-3 loss: 0.734379  [  128/  306]
train() client id: f_00004-6-4 loss: 1.049068  [  160/  306]
train() client id: f_00004-6-5 loss: 0.756741  [  192/  306]
train() client id: f_00004-6-6 loss: 0.796184  [  224/  306]
train() client id: f_00004-6-7 loss: 0.836367  [  256/  306]
train() client id: f_00004-6-8 loss: 0.866223  [  288/  306]
train() client id: f_00004-7-0 loss: 0.882750  [   32/  306]
train() client id: f_00004-7-1 loss: 0.946496  [   64/  306]
train() client id: f_00004-7-2 loss: 0.848701  [   96/  306]
train() client id: f_00004-7-3 loss: 0.801737  [  128/  306]
train() client id: f_00004-7-4 loss: 0.943580  [  160/  306]
train() client id: f_00004-7-5 loss: 0.774386  [  192/  306]
train() client id: f_00004-7-6 loss: 0.816915  [  224/  306]
train() client id: f_00004-7-7 loss: 0.716397  [  256/  306]
train() client id: f_00004-7-8 loss: 0.847157  [  288/  306]
train() client id: f_00004-8-0 loss: 0.778223  [   32/  306]
train() client id: f_00004-8-1 loss: 0.802629  [   64/  306]
train() client id: f_00004-8-2 loss: 0.892846  [   96/  306]
train() client id: f_00004-8-3 loss: 0.865236  [  128/  306]
train() client id: f_00004-8-4 loss: 0.869350  [  160/  306]
train() client id: f_00004-8-5 loss: 0.677893  [  192/  306]
train() client id: f_00004-8-6 loss: 0.925336  [  224/  306]
train() client id: f_00004-8-7 loss: 0.715041  [  256/  306]
train() client id: f_00004-8-8 loss: 0.975765  [  288/  306]
train() client id: f_00004-9-0 loss: 0.806782  [   32/  306]
train() client id: f_00004-9-1 loss: 0.925897  [   64/  306]
train() client id: f_00004-9-2 loss: 0.767634  [   96/  306]
train() client id: f_00004-9-3 loss: 0.833090  [  128/  306]
train() client id: f_00004-9-4 loss: 0.812161  [  160/  306]
train() client id: f_00004-9-5 loss: 0.876161  [  192/  306]
train() client id: f_00004-9-6 loss: 1.058921  [  224/  306]
train() client id: f_00004-9-7 loss: 0.854070  [  256/  306]
train() client id: f_00004-9-8 loss: 0.720342  [  288/  306]
train() client id: f_00004-10-0 loss: 0.899038  [   32/  306]
train() client id: f_00004-10-1 loss: 0.720142  [   64/  306]
train() client id: f_00004-10-2 loss: 0.912116  [   96/  306]
train() client id: f_00004-10-3 loss: 0.960727  [  128/  306]
train() client id: f_00004-10-4 loss: 0.796743  [  160/  306]
train() client id: f_00004-10-5 loss: 0.757917  [  192/  306]
train() client id: f_00004-10-6 loss: 0.870720  [  224/  306]
train() client id: f_00004-10-7 loss: 0.923734  [  256/  306]
train() client id: f_00004-10-8 loss: 0.825962  [  288/  306]
train() client id: f_00004-11-0 loss: 0.892782  [   32/  306]
train() client id: f_00004-11-1 loss: 0.839334  [   64/  306]
train() client id: f_00004-11-2 loss: 0.924111  [   96/  306]
train() client id: f_00004-11-3 loss: 0.848150  [  128/  306]
train() client id: f_00004-11-4 loss: 0.792933  [  160/  306]
train() client id: f_00004-11-5 loss: 0.886610  [  192/  306]
train() client id: f_00004-11-6 loss: 0.879362  [  224/  306]
train() client id: f_00004-11-7 loss: 0.814286  [  256/  306]
train() client id: f_00004-11-8 loss: 0.795576  [  288/  306]
train() client id: f_00004-12-0 loss: 0.771718  [   32/  306]
train() client id: f_00004-12-1 loss: 0.943276  [   64/  306]
train() client id: f_00004-12-2 loss: 0.961220  [   96/  306]
train() client id: f_00004-12-3 loss: 0.773885  [  128/  306]
train() client id: f_00004-12-4 loss: 0.931117  [  160/  306]
train() client id: f_00004-12-5 loss: 0.840795  [  192/  306]
train() client id: f_00004-12-6 loss: 0.774930  [  224/  306]
train() client id: f_00004-12-7 loss: 0.777143  [  256/  306]
train() client id: f_00004-12-8 loss: 0.931589  [  288/  306]
train() client id: f_00005-0-0 loss: 0.701812  [   32/  146]
train() client id: f_00005-0-1 loss: 0.811077  [   64/  146]
train() client id: f_00005-0-2 loss: 0.690294  [   96/  146]
train() client id: f_00005-0-3 loss: 0.541613  [  128/  146]
train() client id: f_00005-1-0 loss: 0.687421  [   32/  146]
train() client id: f_00005-1-1 loss: 0.697106  [   64/  146]
train() client id: f_00005-1-2 loss: 0.647896  [   96/  146]
train() client id: f_00005-1-3 loss: 0.801659  [  128/  146]
train() client id: f_00005-2-0 loss: 0.608739  [   32/  146]
train() client id: f_00005-2-1 loss: 0.778205  [   64/  146]
train() client id: f_00005-2-2 loss: 0.527909  [   96/  146]
train() client id: f_00005-2-3 loss: 0.763163  [  128/  146]
train() client id: f_00005-3-0 loss: 0.762375  [   32/  146]
train() client id: f_00005-3-1 loss: 0.633170  [   64/  146]
train() client id: f_00005-3-2 loss: 0.788290  [   96/  146]
train() client id: f_00005-3-3 loss: 0.624963  [  128/  146]
train() client id: f_00005-4-0 loss: 0.604735  [   32/  146]
train() client id: f_00005-4-1 loss: 0.524220  [   64/  146]
train() client id: f_00005-4-2 loss: 0.955428  [   96/  146]
train() client id: f_00005-4-3 loss: 0.845595  [  128/  146]
train() client id: f_00005-5-0 loss: 0.970997  [   32/  146]
train() client id: f_00005-5-1 loss: 0.770705  [   64/  146]
train() client id: f_00005-5-2 loss: 0.701153  [   96/  146]
train() client id: f_00005-5-3 loss: 0.401959  [  128/  146]
train() client id: f_00005-6-0 loss: 0.673343  [   32/  146]
train() client id: f_00005-6-1 loss: 0.572858  [   64/  146]
train() client id: f_00005-6-2 loss: 0.572658  [   96/  146]
train() client id: f_00005-6-3 loss: 1.070801  [  128/  146]
train() client id: f_00005-7-0 loss: 0.646741  [   32/  146]
train() client id: f_00005-7-1 loss: 0.637779  [   64/  146]
train() client id: f_00005-7-2 loss: 0.561036  [   96/  146]
train() client id: f_00005-7-3 loss: 0.879119  [  128/  146]
train() client id: f_00005-8-0 loss: 0.582799  [   32/  146]
train() client id: f_00005-8-1 loss: 0.943444  [   64/  146]
train() client id: f_00005-8-2 loss: 0.568009  [   96/  146]
train() client id: f_00005-8-3 loss: 0.457780  [  128/  146]
train() client id: f_00005-9-0 loss: 0.736218  [   32/  146]
train() client id: f_00005-9-1 loss: 0.680947  [   64/  146]
train() client id: f_00005-9-2 loss: 0.504160  [   96/  146]
train() client id: f_00005-9-3 loss: 0.814018  [  128/  146]
train() client id: f_00005-10-0 loss: 0.614845  [   32/  146]
train() client id: f_00005-10-1 loss: 0.717074  [   64/  146]
train() client id: f_00005-10-2 loss: 0.416661  [   96/  146]
train() client id: f_00005-10-3 loss: 0.987708  [  128/  146]
train() client id: f_00005-11-0 loss: 0.833509  [   32/  146]
train() client id: f_00005-11-1 loss: 0.729815  [   64/  146]
train() client id: f_00005-11-2 loss: 0.720831  [   96/  146]
train() client id: f_00005-11-3 loss: 0.362816  [  128/  146]
train() client id: f_00005-12-0 loss: 0.742331  [   32/  146]
train() client id: f_00005-12-1 loss: 0.723159  [   64/  146]
train() client id: f_00005-12-2 loss: 0.810964  [   96/  146]
train() client id: f_00005-12-3 loss: 0.559256  [  128/  146]
train() client id: f_00006-0-0 loss: 0.492006  [   32/   54]
train() client id: f_00006-1-0 loss: 0.485588  [   32/   54]
train() client id: f_00006-2-0 loss: 0.440701  [   32/   54]
train() client id: f_00006-3-0 loss: 0.448203  [   32/   54]
train() client id: f_00006-4-0 loss: 0.446750  [   32/   54]
train() client id: f_00006-5-0 loss: 0.487301  [   32/   54]
train() client id: f_00006-6-0 loss: 0.434624  [   32/   54]
train() client id: f_00006-7-0 loss: 0.389498  [   32/   54]
train() client id: f_00006-8-0 loss: 0.488277  [   32/   54]
train() client id: f_00006-9-0 loss: 0.376564  [   32/   54]
train() client id: f_00006-10-0 loss: 0.458416  [   32/   54]
train() client id: f_00006-11-0 loss: 0.501739  [   32/   54]
train() client id: f_00006-12-0 loss: 0.463298  [   32/   54]
train() client id: f_00007-0-0 loss: 0.552067  [   32/  179]
train() client id: f_00007-0-1 loss: 0.748131  [   64/  179]
train() client id: f_00007-0-2 loss: 0.845060  [   96/  179]
train() client id: f_00007-0-3 loss: 0.691063  [  128/  179]
train() client id: f_00007-0-4 loss: 0.530954  [  160/  179]
train() client id: f_00007-1-0 loss: 0.553545  [   32/  179]
train() client id: f_00007-1-1 loss: 0.602801  [   64/  179]
train() client id: f_00007-1-2 loss: 0.585007  [   96/  179]
train() client id: f_00007-1-3 loss: 0.870510  [  128/  179]
train() client id: f_00007-1-4 loss: 0.627108  [  160/  179]
train() client id: f_00007-2-0 loss: 0.822520  [   32/  179]
train() client id: f_00007-2-1 loss: 0.561209  [   64/  179]
train() client id: f_00007-2-2 loss: 0.603057  [   96/  179]
train() client id: f_00007-2-3 loss: 0.640980  [  128/  179]
train() client id: f_00007-2-4 loss: 0.614211  [  160/  179]
train() client id: f_00007-3-0 loss: 0.710976  [   32/  179]
train() client id: f_00007-3-1 loss: 0.592222  [   64/  179]
train() client id: f_00007-3-2 loss: 0.559397  [   96/  179]
train() client id: f_00007-3-3 loss: 0.525005  [  128/  179]
train() client id: f_00007-3-4 loss: 0.766479  [  160/  179]
train() client id: f_00007-4-0 loss: 0.652965  [   32/  179]
train() client id: f_00007-4-1 loss: 0.457254  [   64/  179]
train() client id: f_00007-4-2 loss: 0.622856  [   96/  179]
train() client id: f_00007-4-3 loss: 0.479224  [  128/  179]
train() client id: f_00007-4-4 loss: 0.717266  [  160/  179]
train() client id: f_00007-5-0 loss: 0.758638  [   32/  179]
train() client id: f_00007-5-1 loss: 0.566809  [   64/  179]
train() client id: f_00007-5-2 loss: 0.547332  [   96/  179]
train() client id: f_00007-5-3 loss: 0.698863  [  128/  179]
train() client id: f_00007-5-4 loss: 0.474929  [  160/  179]
train() client id: f_00007-6-0 loss: 1.139230  [   32/  179]
train() client id: f_00007-6-1 loss: 0.465788  [   64/  179]
train() client id: f_00007-6-2 loss: 0.468962  [   96/  179]
train() client id: f_00007-6-3 loss: 0.560802  [  128/  179]
train() client id: f_00007-6-4 loss: 0.519226  [  160/  179]
train() client id: f_00007-7-0 loss: 0.746626  [   32/  179]
train() client id: f_00007-7-1 loss: 0.544497  [   64/  179]
train() client id: f_00007-7-2 loss: 0.723396  [   96/  179]
train() client id: f_00007-7-3 loss: 0.453521  [  128/  179]
train() client id: f_00007-7-4 loss: 0.698308  [  160/  179]
train() client id: f_00007-8-0 loss: 0.407360  [   32/  179]
train() client id: f_00007-8-1 loss: 0.592622  [   64/  179]
train() client id: f_00007-8-2 loss: 0.549667  [   96/  179]
train() client id: f_00007-8-3 loss: 0.688735  [  128/  179]
train() client id: f_00007-8-4 loss: 0.683767  [  160/  179]
train() client id: f_00007-9-0 loss: 0.413984  [   32/  179]
train() client id: f_00007-9-1 loss: 0.602971  [   64/  179]
train() client id: f_00007-9-2 loss: 0.715843  [   96/  179]
train() client id: f_00007-9-3 loss: 0.749146  [  128/  179]
train() client id: f_00007-9-4 loss: 0.659717  [  160/  179]
train() client id: f_00007-10-0 loss: 0.467293  [   32/  179]
train() client id: f_00007-10-1 loss: 0.516226  [   64/  179]
train() client id: f_00007-10-2 loss: 0.652296  [   96/  179]
train() client id: f_00007-10-3 loss: 0.821831  [  128/  179]
train() client id: f_00007-10-4 loss: 0.679784  [  160/  179]
train() client id: f_00007-11-0 loss: 0.558151  [   32/  179]
train() client id: f_00007-11-1 loss: 0.426152  [   64/  179]
train() client id: f_00007-11-2 loss: 0.633580  [   96/  179]
train() client id: f_00007-11-3 loss: 0.528527  [  128/  179]
train() client id: f_00007-11-4 loss: 0.728737  [  160/  179]
train() client id: f_00007-12-0 loss: 0.739888  [   32/  179]
train() client id: f_00007-12-1 loss: 0.632233  [   64/  179]
train() client id: f_00007-12-2 loss: 0.531461  [   96/  179]
train() client id: f_00007-12-3 loss: 0.437970  [  128/  179]
train() client id: f_00007-12-4 loss: 0.686593  [  160/  179]
train() client id: f_00008-0-0 loss: 0.704500  [   32/  130]
train() client id: f_00008-0-1 loss: 0.757494  [   64/  130]
train() client id: f_00008-0-2 loss: 0.589394  [   96/  130]
train() client id: f_00008-0-3 loss: 0.803658  [  128/  130]
train() client id: f_00008-1-0 loss: 0.631277  [   32/  130]
train() client id: f_00008-1-1 loss: 0.754993  [   64/  130]
train() client id: f_00008-1-2 loss: 0.692140  [   96/  130]
train() client id: f_00008-1-3 loss: 0.719723  [  128/  130]
train() client id: f_00008-2-0 loss: 0.703285  [   32/  130]
train() client id: f_00008-2-1 loss: 0.676487  [   64/  130]
train() client id: f_00008-2-2 loss: 0.717828  [   96/  130]
train() client id: f_00008-2-3 loss: 0.768464  [  128/  130]
train() client id: f_00008-3-0 loss: 0.781114  [   32/  130]
train() client id: f_00008-3-1 loss: 0.666632  [   64/  130]
train() client id: f_00008-3-2 loss: 0.678161  [   96/  130]
train() client id: f_00008-3-3 loss: 0.750908  [  128/  130]
train() client id: f_00008-4-0 loss: 0.737698  [   32/  130]
train() client id: f_00008-4-1 loss: 0.788651  [   64/  130]
train() client id: f_00008-4-2 loss: 0.673562  [   96/  130]
train() client id: f_00008-4-3 loss: 0.676245  [  128/  130]
train() client id: f_00008-5-0 loss: 0.740630  [   32/  130]
train() client id: f_00008-5-1 loss: 0.701064  [   64/  130]
train() client id: f_00008-5-2 loss: 0.745260  [   96/  130]
train() client id: f_00008-5-3 loss: 0.689644  [  128/  130]
train() client id: f_00008-6-0 loss: 0.576185  [   32/  130]
train() client id: f_00008-6-1 loss: 0.716404  [   64/  130]
train() client id: f_00008-6-2 loss: 0.794075  [   96/  130]
train() client id: f_00008-6-3 loss: 0.782285  [  128/  130]
train() client id: f_00008-7-0 loss: 0.771801  [   32/  130]
train() client id: f_00008-7-1 loss: 0.635281  [   64/  130]
train() client id: f_00008-7-2 loss: 0.722679  [   96/  130]
train() client id: f_00008-7-3 loss: 0.759324  [  128/  130]
train() client id: f_00008-8-0 loss: 0.694398  [   32/  130]
train() client id: f_00008-8-1 loss: 0.620700  [   64/  130]
train() client id: f_00008-8-2 loss: 0.853544  [   96/  130]
train() client id: f_00008-8-3 loss: 0.704632  [  128/  130]
train() client id: f_00008-9-0 loss: 0.721992  [   32/  130]
train() client id: f_00008-9-1 loss: 0.729804  [   64/  130]
train() client id: f_00008-9-2 loss: 0.636871  [   96/  130]
train() client id: f_00008-9-3 loss: 0.780733  [  128/  130]
train() client id: f_00008-10-0 loss: 0.751538  [   32/  130]
train() client id: f_00008-10-1 loss: 0.726198  [   64/  130]
train() client id: f_00008-10-2 loss: 0.678903  [   96/  130]
train() client id: f_00008-10-3 loss: 0.707726  [  128/  130]
train() client id: f_00008-11-0 loss: 0.578712  [   32/  130]
train() client id: f_00008-11-1 loss: 0.855233  [   64/  130]
train() client id: f_00008-11-2 loss: 0.799730  [   96/  130]
train() client id: f_00008-11-3 loss: 0.654095  [  128/  130]
train() client id: f_00008-12-0 loss: 0.721180  [   32/  130]
train() client id: f_00008-12-1 loss: 0.707228  [   64/  130]
train() client id: f_00008-12-2 loss: 0.666686  [   96/  130]
train() client id: f_00008-12-3 loss: 0.724655  [  128/  130]
train() client id: f_00009-0-0 loss: 0.711747  [   32/  118]
train() client id: f_00009-0-1 loss: 0.746949  [   64/  118]
train() client id: f_00009-0-2 loss: 1.002152  [   96/  118]
train() client id: f_00009-1-0 loss: 0.742489  [   32/  118]
train() client id: f_00009-1-1 loss: 0.859577  [   64/  118]
train() client id: f_00009-1-2 loss: 0.774478  [   96/  118]
train() client id: f_00009-2-0 loss: 0.742090  [   32/  118]
train() client id: f_00009-2-1 loss: 0.696090  [   64/  118]
train() client id: f_00009-2-2 loss: 0.716949  [   96/  118]
train() client id: f_00009-3-0 loss: 0.776089  [   32/  118]
train() client id: f_00009-3-1 loss: 0.755819  [   64/  118]
train() client id: f_00009-3-2 loss: 0.705921  [   96/  118]
train() client id: f_00009-4-0 loss: 0.751829  [   32/  118]
train() client id: f_00009-4-1 loss: 0.808827  [   64/  118]
train() client id: f_00009-4-2 loss: 0.567966  [   96/  118]
train() client id: f_00009-5-0 loss: 0.638724  [   32/  118]
train() client id: f_00009-5-1 loss: 0.656043  [   64/  118]
train() client id: f_00009-5-2 loss: 0.604261  [   96/  118]
train() client id: f_00009-6-0 loss: 0.647763  [   32/  118]
train() client id: f_00009-6-1 loss: 0.567879  [   64/  118]
train() client id: f_00009-6-2 loss: 0.634215  [   96/  118]
train() client id: f_00009-7-0 loss: 0.647514  [   32/  118]
train() client id: f_00009-7-1 loss: 0.556256  [   64/  118]
train() client id: f_00009-7-2 loss: 0.537055  [   96/  118]
train() client id: f_00009-8-0 loss: 0.467299  [   32/  118]
train() client id: f_00009-8-1 loss: 0.612339  [   64/  118]
train() client id: f_00009-8-2 loss: 0.599332  [   96/  118]
train() client id: f_00009-9-0 loss: 0.728747  [   32/  118]
train() client id: f_00009-9-1 loss: 0.391676  [   64/  118]
train() client id: f_00009-9-2 loss: 0.561993  [   96/  118]
train() client id: f_00009-10-0 loss: 0.429467  [   32/  118]
train() client id: f_00009-10-1 loss: 0.556446  [   64/  118]
train() client id: f_00009-10-2 loss: 0.782616  [   96/  118]
train() client id: f_00009-11-0 loss: 0.559246  [   32/  118]
train() client id: f_00009-11-1 loss: 0.419187  [   64/  118]
train() client id: f_00009-11-2 loss: 0.743260  [   96/  118]
train() client id: f_00009-12-0 loss: 0.486600  [   32/  118]
train() client id: f_00009-12-1 loss: 0.613707  [   64/  118]
train() client id: f_00009-12-2 loss: 0.550287  [   96/  118]
At round 44 accuracy: 0.6419098143236074
At round 44 training accuracy: 0.5861837692823608
At round 44 training loss: 0.8370278869438044
update_location
xs = -3.905658 4.200318 240.009024 18.811294 0.979296 3.956410 -202.443192 -181.324852 224.663977 -167.060879 
ys = 232.587959 215.555839 1.320614 -202.455176 194.350187 177.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -6.211426168117436
ys mean: 63.8941425355287
dists_uav = 253.204291 237.659341 260.011684 226.587650 218.570250 204.042950 225.809957 207.073364 246.541219 194.744317 
uav_gains = -111.570923 -110.286811 -112.169118 -109.454464 -108.892183 -107.937957 -109.398532 -108.131876 -111.004853 -107.352318 
uav_gains_db_mean: -109.61990353784395
dists_bs = 180.526028 183.732224 449.885562 424.305583 177.039946 178.978535 179.732479 174.292450 429.553038 171.182718 
bs_gains = -102.750174 -102.964248 -113.853917 -113.142064 -102.513055 -102.645486 -102.696603 -102.322859 -113.291529 -102.103937 
bs_gains_db_mean: -105.82838722582848
Round 45
-------------------------------
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.05514024 10.40024199  4.98453475  1.80545975 11.99306906  5.77018381
  2.2328339   7.08469715  5.23316744  4.67883957]
obj_prev = 59.2381676466099
eta_min = 5.885406011502381e-19	eta_max = 0.9363452566333951
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 13.714204272500444	eta = 0.9090909090909091
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 26.6122320197388	eta = 0.4684860112559707
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 20.101928982645475	eta = 0.6202120423522212
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.922786406509566	eta = 0.6588595443458035
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.857340066002127	eta = 0.6611461842396013
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.857119499819667	eta = 0.6611539174721296
eta = 0.6611539174721296
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [0.03414319 0.0718091  0.03360124 0.01165204 0.08291919 0.03956275
 0.0146328  0.04850501 0.0352271  0.03197537]
ene_total = [1.73700778 2.97656775 1.73758614 0.82737842 3.38956642 1.75748632
 0.93827146 2.18480132 1.84341633 1.46503756]
ti_comp = [0.61689568 0.66459269 0.61132264 0.63338719 0.66608943 0.66565651
 0.63376527 0.64167568 0.59949544 0.66739459]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.53685718e-06 5.23971089e-05 6.34460979e-06 2.46460673e-07
 8.03119208e-05 8.73451965e-06 4.87534362e-07 1.73224277e-05
 7.60219691e-06 4.58734577e-06]
ene_total = [0.45070637 0.27270761 0.47169887 0.38832777 0.26811959 0.26705379
 0.38691223 0.35773942 0.51631227 0.26034825]
optimize_network iter = 0 obj = 3.639926174690373
eta = 0.6611539174721296
freqs = [27673393.660334   54024893.67528107 27482409.80128485  9198198.6140707
 62243282.60071961 29717090.05574555 11544335.26103987 37795583.8923452
 29380628.13581736 23955372.57683051]
eta_min = 0.6611539174721324	eta_max = 0.6721599416563945
af = 0.005695477061159853	bf = 1.2310181153532935	zeta = 0.006265024767275839	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [1.50574262e-06 1.20694943e-05 1.46145910e-06 5.67713705e-08
 1.84995754e-05 2.01196663e-06 1.12301867e-07 3.99016180e-06
 1.75113998e-06 1.05667936e-06]
ene_total = [1.67921435 1.010801   1.75748063 1.44739012 0.99068256 0.99444728
 1.44208793 1.33153211 1.92363253 0.96990197]
ti_comp = [0.5929754  0.64067241 0.58740236 0.60946691 0.64216915 0.64173622
 0.60984499 0.6177554  0.57557516 0.64347431]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.26134418e-06 4.98993487e-05 6.08167179e-06 2.35577818e-07
 7.64706350e-05 8.31715417e-06 4.65984701e-07 1.65407536e-05
 7.29886402e-06 4.36729864e-06]
ene_total = [0.46582645 0.28176549 0.48752418 0.40136402 0.27697112 0.27600289
 0.39990055 0.36971876 0.53363374 0.26907992]
optimize_network iter = 1 obj = 3.761787113624325
eta = 0.6721599416563945
freqs = [27640992.43136498 53805865.01782712 27460330.69138759  9177789.36033805
 61985734.40435877 29594844.6264841  11518450.63917791 37692613.12413883
 29380628.13581735 23854513.15446249]
eta_min = 0.6721599416563961	eta_max = 0.6721599416563635
af = 0.0056547282477792775	bf = 1.2310181153532935	zeta = 0.006220201072557206	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [1.50221871e-06 1.19718279e-05 1.45911179e-06 5.65197178e-08
 1.83467983e-05 1.99544766e-06 1.11798827e-07 3.96844972e-06
 1.75113998e-06 1.04780021e-06]
ene_total = [1.67921385 1.01078729 1.7574803  1.44739009 0.9906611  0.99444496
 1.44208786 1.33152906 1.92363253 0.96990072]
ti_comp = [0.5929754  0.64067241 0.58740236 0.60946691 0.64216915 0.64173622
 0.60984499 0.6177554  0.57557516 0.64347431]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.26134418e-06 4.98993487e-05 6.08167179e-06 2.35577818e-07
 7.64706350e-05 8.31715417e-06 4.65984701e-07 1.65407536e-05
 7.29886402e-06 4.36729864e-06]
ene_total = [0.46582645 0.28176549 0.48752418 0.40136402 0.27697112 0.27600289
 0.39990055 0.36971876 0.53363374 0.26907992]
optimize_network iter = 2 obj = 3.761787113623971
eta = 0.6721599416563635
freqs = [27640992.43136505 53805865.01782771 27460330.69138763  9177789.3603381
 61985734.40435947 29594844.62648444 11518450.63917798 37692613.1241391
 29380628.13581733 23854513.15446277]
Done!
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.25736035e-06 4.98675999e-05 6.07780228e-06 2.35427930e-07
 7.64219800e-05 8.31186232e-06 4.65688215e-07 1.65302294e-05
 7.29422007e-06 4.36451991e-06]
ene_total = [0.01196084 0.00723475 0.01251797 0.01030567 0.00711163 0.00708682
 0.01026809 0.00949312 0.0137019  0.00690906]
At round 45 energy consumption: 0.09658985266196866
At round 45 eta: 0.6721599416563635
At round 45 a_n: 12.768039742635098
At round 45 local rounds: 13.008276614299735
At round 45 global rounds: 38.94594152753551
gradient difference: 0.4245976507663727
train() client id: f_00000-0-0 loss: 1.386142  [   32/  126]
train() client id: f_00000-0-1 loss: 1.196685  [   64/  126]
train() client id: f_00000-0-2 loss: 1.216054  [   96/  126]
train() client id: f_00000-1-0 loss: 1.328906  [   32/  126]
train() client id: f_00000-1-1 loss: 1.037853  [   64/  126]
train() client id: f_00000-1-2 loss: 1.116134  [   96/  126]
train() client id: f_00000-2-0 loss: 1.002845  [   32/  126]
train() client id: f_00000-2-1 loss: 1.139003  [   64/  126]
train() client id: f_00000-2-2 loss: 1.196474  [   96/  126]
train() client id: f_00000-3-0 loss: 1.059178  [   32/  126]
train() client id: f_00000-3-1 loss: 0.876041  [   64/  126]
train() client id: f_00000-3-2 loss: 0.988910  [   96/  126]
train() client id: f_00000-4-0 loss: 0.925358  [   32/  126]
train() client id: f_00000-4-1 loss: 0.884026  [   64/  126]
train() client id: f_00000-4-2 loss: 0.920788  [   96/  126]
train() client id: f_00000-5-0 loss: 0.813692  [   32/  126]
train() client id: f_00000-5-1 loss: 0.939356  [   64/  126]
train() client id: f_00000-5-2 loss: 0.799791  [   96/  126]
train() client id: f_00000-6-0 loss: 0.774610  [   32/  126]
train() client id: f_00000-6-1 loss: 0.762092  [   64/  126]
train() client id: f_00000-6-2 loss: 0.938182  [   96/  126]
train() client id: f_00000-7-0 loss: 0.948618  [   32/  126]
train() client id: f_00000-7-1 loss: 0.755006  [   64/  126]
train() client id: f_00000-7-2 loss: 0.873190  [   96/  126]
train() client id: f_00000-8-0 loss: 0.878919  [   32/  126]
train() client id: f_00000-8-1 loss: 0.884180  [   64/  126]
train() client id: f_00000-8-2 loss: 0.763517  [   96/  126]
train() client id: f_00000-9-0 loss: 0.888501  [   32/  126]
train() client id: f_00000-9-1 loss: 0.790785  [   64/  126]
train() client id: f_00000-9-2 loss: 0.796686  [   96/  126]
train() client id: f_00000-10-0 loss: 0.798428  [   32/  126]
train() client id: f_00000-10-1 loss: 0.888221  [   64/  126]
train() client id: f_00000-10-2 loss: 0.659881  [   96/  126]
train() client id: f_00000-11-0 loss: 0.928787  [   32/  126]
train() client id: f_00000-11-1 loss: 0.602069  [   64/  126]
train() client id: f_00000-11-2 loss: 0.805573  [   96/  126]
train() client id: f_00000-12-0 loss: 0.827211  [   32/  126]
train() client id: f_00000-12-1 loss: 0.711971  [   64/  126]
train() client id: f_00000-12-2 loss: 0.799888  [   96/  126]
train() client id: f_00001-0-0 loss: 0.591578  [   32/  265]
train() client id: f_00001-0-1 loss: 0.555093  [   64/  265]
train() client id: f_00001-0-2 loss: 0.520220  [   96/  265]
train() client id: f_00001-0-3 loss: 0.431365  [  128/  265]
train() client id: f_00001-0-4 loss: 0.474159  [  160/  265]
train() client id: f_00001-0-5 loss: 0.421522  [  192/  265]
train() client id: f_00001-0-6 loss: 0.393057  [  224/  265]
train() client id: f_00001-0-7 loss: 0.537849  [  256/  265]
train() client id: f_00001-1-0 loss: 0.420460  [   32/  265]
train() client id: f_00001-1-1 loss: 0.402836  [   64/  265]
train() client id: f_00001-1-2 loss: 0.460138  [   96/  265]
train() client id: f_00001-1-3 loss: 0.550227  [  128/  265]
train() client id: f_00001-1-4 loss: 0.381025  [  160/  265]
train() client id: f_00001-1-5 loss: 0.635729  [  192/  265]
train() client id: f_00001-1-6 loss: 0.586075  [  224/  265]
train() client id: f_00001-1-7 loss: 0.403525  [  256/  265]
train() client id: f_00001-2-0 loss: 0.431977  [   32/  265]
train() client id: f_00001-2-1 loss: 0.519034  [   64/  265]
train() client id: f_00001-2-2 loss: 0.506653  [   96/  265]
train() client id: f_00001-2-3 loss: 0.436382  [  128/  265]
train() client id: f_00001-2-4 loss: 0.462229  [  160/  265]
train() client id: f_00001-2-5 loss: 0.399006  [  192/  265]
train() client id: f_00001-2-6 loss: 0.465096  [  224/  265]
train() client id: f_00001-2-7 loss: 0.529415  [  256/  265]
train() client id: f_00001-3-0 loss: 0.663907  [   32/  265]
train() client id: f_00001-3-1 loss: 0.431233  [   64/  265]
train() client id: f_00001-3-2 loss: 0.421451  [   96/  265]
train() client id: f_00001-3-3 loss: 0.372147  [  128/  265]
train() client id: f_00001-3-4 loss: 0.372644  [  160/  265]
train() client id: f_00001-3-5 loss: 0.450704  [  192/  265]
train() client id: f_00001-3-6 loss: 0.477245  [  224/  265]
train() client id: f_00001-3-7 loss: 0.561890  [  256/  265]
train() client id: f_00001-4-0 loss: 0.480241  [   32/  265]
train() client id: f_00001-4-1 loss: 0.477432  [   64/  265]
train() client id: f_00001-4-2 loss: 0.492909  [   96/  265]
train() client id: f_00001-4-3 loss: 0.561157  [  128/  265]
train() client id: f_00001-4-4 loss: 0.428556  [  160/  265]
train() client id: f_00001-4-5 loss: 0.465559  [  192/  265]
train() client id: f_00001-4-6 loss: 0.431899  [  224/  265]
train() client id: f_00001-4-7 loss: 0.372369  [  256/  265]
train() client id: f_00001-5-0 loss: 0.430686  [   32/  265]
train() client id: f_00001-5-1 loss: 0.454463  [   64/  265]
train() client id: f_00001-5-2 loss: 0.546417  [   96/  265]
train() client id: f_00001-5-3 loss: 0.424487  [  128/  265]
train() client id: f_00001-5-4 loss: 0.371490  [  160/  265]
train() client id: f_00001-5-5 loss: 0.482654  [  192/  265]
train() client id: f_00001-5-6 loss: 0.384622  [  224/  265]
train() client id: f_00001-5-7 loss: 0.501240  [  256/  265]
train() client id: f_00001-6-0 loss: 0.399833  [   32/  265]
train() client id: f_00001-6-1 loss: 0.567788  [   64/  265]
train() client id: f_00001-6-2 loss: 0.446914  [   96/  265]
train() client id: f_00001-6-3 loss: 0.645213  [  128/  265]
train() client id: f_00001-6-4 loss: 0.382255  [  160/  265]
train() client id: f_00001-6-5 loss: 0.439638  [  192/  265]
train() client id: f_00001-6-6 loss: 0.381639  [  224/  265]
train() client id: f_00001-6-7 loss: 0.389203  [  256/  265]
train() client id: f_00001-7-0 loss: 0.419269  [   32/  265]
train() client id: f_00001-7-1 loss: 0.544018  [   64/  265]
train() client id: f_00001-7-2 loss: 0.374040  [   96/  265]
train() client id: f_00001-7-3 loss: 0.341197  [  128/  265]
train() client id: f_00001-7-4 loss: 0.595511  [  160/  265]
train() client id: f_00001-7-5 loss: 0.419746  [  192/  265]
train() client id: f_00001-7-6 loss: 0.435197  [  224/  265]
train() client id: f_00001-7-7 loss: 0.478200  [  256/  265]
train() client id: f_00001-8-0 loss: 0.486147  [   32/  265]
train() client id: f_00001-8-1 loss: 0.347303  [   64/  265]
train() client id: f_00001-8-2 loss: 0.446816  [   96/  265]
train() client id: f_00001-8-3 loss: 0.457350  [  128/  265]
train() client id: f_00001-8-4 loss: 0.415612  [  160/  265]
train() client id: f_00001-8-5 loss: 0.428375  [  192/  265]
train() client id: f_00001-8-6 loss: 0.545187  [  224/  265]
train() client id: f_00001-8-7 loss: 0.513460  [  256/  265]
train() client id: f_00001-9-0 loss: 0.567567  [   32/  265]
train() client id: f_00001-9-1 loss: 0.362793  [   64/  265]
train() client id: f_00001-9-2 loss: 0.496774  [   96/  265]
train() client id: f_00001-9-3 loss: 0.564941  [  128/  265]
train() client id: f_00001-9-4 loss: 0.348212  [  160/  265]
train() client id: f_00001-9-5 loss: 0.342227  [  192/  265]
train() client id: f_00001-9-6 loss: 0.418024  [  224/  265]
train() client id: f_00001-9-7 loss: 0.523485  [  256/  265]
train() client id: f_00001-10-0 loss: 0.433842  [   32/  265]
train() client id: f_00001-10-1 loss: 0.350040  [   64/  265]
train() client id: f_00001-10-2 loss: 0.495709  [   96/  265]
train() client id: f_00001-10-3 loss: 0.463099  [  128/  265]
train() client id: f_00001-10-4 loss: 0.478622  [  160/  265]
train() client id: f_00001-10-5 loss: 0.348577  [  192/  265]
train() client id: f_00001-10-6 loss: 0.495859  [  224/  265]
train() client id: f_00001-10-7 loss: 0.541131  [  256/  265]
train() client id: f_00001-11-0 loss: 0.431232  [   32/  265]
train() client id: f_00001-11-1 loss: 0.349365  [   64/  265]
train() client id: f_00001-11-2 loss: 0.455479  [   96/  265]
train() client id: f_00001-11-3 loss: 0.424284  [  128/  265]
train() client id: f_00001-11-4 loss: 0.409270  [  160/  265]
train() client id: f_00001-11-5 loss: 0.468541  [  192/  265]
train() client id: f_00001-11-6 loss: 0.541363  [  224/  265]
train() client id: f_00001-11-7 loss: 0.525555  [  256/  265]
train() client id: f_00001-12-0 loss: 0.548210  [   32/  265]
train() client id: f_00001-12-1 loss: 0.613845  [   64/  265]
train() client id: f_00001-12-2 loss: 0.414301  [   96/  265]
train() client id: f_00001-12-3 loss: 0.408513  [  128/  265]
train() client id: f_00001-12-4 loss: 0.353524  [  160/  265]
train() client id: f_00001-12-5 loss: 0.373231  [  192/  265]
train() client id: f_00001-12-6 loss: 0.382231  [  224/  265]
train() client id: f_00001-12-7 loss: 0.501062  [  256/  265]
train() client id: f_00002-0-0 loss: 1.286086  [   32/  124]
train() client id: f_00002-0-1 loss: 1.215311  [   64/  124]
train() client id: f_00002-0-2 loss: 1.188764  [   96/  124]
train() client id: f_00002-1-0 loss: 1.058506  [   32/  124]
train() client id: f_00002-1-1 loss: 1.182891  [   64/  124]
train() client id: f_00002-1-2 loss: 1.048854  [   96/  124]
train() client id: f_00002-2-0 loss: 1.279645  [   32/  124]
train() client id: f_00002-2-1 loss: 0.827209  [   64/  124]
train() client id: f_00002-2-2 loss: 1.072920  [   96/  124]
train() client id: f_00002-3-0 loss: 1.069605  [   32/  124]
train() client id: f_00002-3-1 loss: 1.181679  [   64/  124]
train() client id: f_00002-3-2 loss: 1.185841  [   96/  124]
train() client id: f_00002-4-0 loss: 1.112298  [   32/  124]
train() client id: f_00002-4-1 loss: 0.903317  [   64/  124]
train() client id: f_00002-4-2 loss: 1.107932  [   96/  124]
train() client id: f_00002-5-0 loss: 1.019566  [   32/  124]
train() client id: f_00002-5-1 loss: 1.123279  [   64/  124]
train() client id: f_00002-5-2 loss: 1.062123  [   96/  124]
train() client id: f_00002-6-0 loss: 0.750986  [   32/  124]
train() client id: f_00002-6-1 loss: 1.327749  [   64/  124]
train() client id: f_00002-6-2 loss: 1.117205  [   96/  124]
train() client id: f_00002-7-0 loss: 1.032878  [   32/  124]
train() client id: f_00002-7-1 loss: 1.127961  [   64/  124]
train() client id: f_00002-7-2 loss: 1.034059  [   96/  124]
train() client id: f_00002-8-0 loss: 1.096005  [   32/  124]
train() client id: f_00002-8-1 loss: 0.938947  [   64/  124]
train() client id: f_00002-8-2 loss: 1.118265  [   96/  124]
train() client id: f_00002-9-0 loss: 0.981295  [   32/  124]
train() client id: f_00002-9-1 loss: 0.989438  [   64/  124]
train() client id: f_00002-9-2 loss: 0.953076  [   96/  124]
train() client id: f_00002-10-0 loss: 1.009618  [   32/  124]
train() client id: f_00002-10-1 loss: 0.981039  [   64/  124]
train() client id: f_00002-10-2 loss: 1.087815  [   96/  124]
train() client id: f_00002-11-0 loss: 0.902412  [   32/  124]
train() client id: f_00002-11-1 loss: 1.098563  [   64/  124]
train() client id: f_00002-11-2 loss: 0.969177  [   96/  124]
train() client id: f_00002-12-0 loss: 0.878891  [   32/  124]
train() client id: f_00002-12-1 loss: 1.026614  [   64/  124]
train() client id: f_00002-12-2 loss: 1.250073  [   96/  124]
train() client id: f_00003-0-0 loss: 0.580530  [   32/   43]
train() client id: f_00003-1-0 loss: 0.675747  [   32/   43]
train() client id: f_00003-2-0 loss: 0.905940  [   32/   43]
train() client id: f_00003-3-0 loss: 0.856749  [   32/   43]
train() client id: f_00003-4-0 loss: 0.769657  [   32/   43]
train() client id: f_00003-5-0 loss: 0.604175  [   32/   43]
train() client id: f_00003-6-0 loss: 0.703257  [   32/   43]
train() client id: f_00003-7-0 loss: 0.660706  [   32/   43]
train() client id: f_00003-8-0 loss: 0.553006  [   32/   43]
train() client id: f_00003-9-0 loss: 0.760383  [   32/   43]
train() client id: f_00003-10-0 loss: 0.741593  [   32/   43]
train() client id: f_00003-11-0 loss: 0.848386  [   32/   43]
train() client id: f_00003-12-0 loss: 0.663004  [   32/   43]
train() client id: f_00004-0-0 loss: 0.687107  [   32/  306]
train() client id: f_00004-0-1 loss: 0.764120  [   64/  306]
train() client id: f_00004-0-2 loss: 0.713922  [   96/  306]
train() client id: f_00004-0-3 loss: 0.814363  [  128/  306]
train() client id: f_00004-0-4 loss: 0.647836  [  160/  306]
train() client id: f_00004-0-5 loss: 0.824891  [  192/  306]
train() client id: f_00004-0-6 loss: 0.665206  [  224/  306]
train() client id: f_00004-0-7 loss: 0.551383  [  256/  306]
train() client id: f_00004-0-8 loss: 0.697012  [  288/  306]
train() client id: f_00004-1-0 loss: 0.628693  [   32/  306]
train() client id: f_00004-1-1 loss: 0.778720  [   64/  306]
train() client id: f_00004-1-2 loss: 0.894458  [   96/  306]
train() client id: f_00004-1-3 loss: 0.723094  [  128/  306]
train() client id: f_00004-1-4 loss: 0.632415  [  160/  306]
train() client id: f_00004-1-5 loss: 0.805047  [  192/  306]
train() client id: f_00004-1-6 loss: 0.672402  [  224/  306]
train() client id: f_00004-1-7 loss: 0.725021  [  256/  306]
train() client id: f_00004-1-8 loss: 0.592174  [  288/  306]
train() client id: f_00004-2-0 loss: 0.777091  [   32/  306]
train() client id: f_00004-2-1 loss: 0.659024  [   64/  306]
train() client id: f_00004-2-2 loss: 0.661928  [   96/  306]
train() client id: f_00004-2-3 loss: 0.733535  [  128/  306]
train() client id: f_00004-2-4 loss: 0.861544  [  160/  306]
train() client id: f_00004-2-5 loss: 0.716607  [  192/  306]
train() client id: f_00004-2-6 loss: 0.717593  [  224/  306]
train() client id: f_00004-2-7 loss: 0.736866  [  256/  306]
train() client id: f_00004-2-8 loss: 0.591167  [  288/  306]
train() client id: f_00004-3-0 loss: 0.678297  [   32/  306]
train() client id: f_00004-3-1 loss: 0.685859  [   64/  306]
train() client id: f_00004-3-2 loss: 0.775525  [   96/  306]
train() client id: f_00004-3-3 loss: 0.771433  [  128/  306]
train() client id: f_00004-3-4 loss: 0.763846  [  160/  306]
train() client id: f_00004-3-5 loss: 0.668004  [  192/  306]
train() client id: f_00004-3-6 loss: 0.818769  [  224/  306]
train() client id: f_00004-3-7 loss: 0.682430  [  256/  306]
train() client id: f_00004-3-8 loss: 0.722380  [  288/  306]
train() client id: f_00004-4-0 loss: 0.685107  [   32/  306]
train() client id: f_00004-4-1 loss: 0.653932  [   64/  306]
train() client id: f_00004-4-2 loss: 0.723798  [   96/  306]
train() client id: f_00004-4-3 loss: 0.700162  [  128/  306]
train() client id: f_00004-4-4 loss: 0.810136  [  160/  306]
train() client id: f_00004-4-5 loss: 0.749378  [  192/  306]
train() client id: f_00004-4-6 loss: 0.722176  [  224/  306]
train() client id: f_00004-4-7 loss: 0.782688  [  256/  306]
train() client id: f_00004-4-8 loss: 0.672467  [  288/  306]
train() client id: f_00004-5-0 loss: 0.609264  [   32/  306]
train() client id: f_00004-5-1 loss: 0.644717  [   64/  306]
train() client id: f_00004-5-2 loss: 0.734342  [   96/  306]
train() client id: f_00004-5-3 loss: 0.665642  [  128/  306]
train() client id: f_00004-5-4 loss: 0.794789  [  160/  306]
train() client id: f_00004-5-5 loss: 0.692980  [  192/  306]
train() client id: f_00004-5-6 loss: 0.887109  [  224/  306]
train() client id: f_00004-5-7 loss: 0.669900  [  256/  306]
train() client id: f_00004-5-8 loss: 0.850418  [  288/  306]
train() client id: f_00004-6-0 loss: 0.791705  [   32/  306]
train() client id: f_00004-6-1 loss: 0.750824  [   64/  306]
train() client id: f_00004-6-2 loss: 0.791707  [   96/  306]
train() client id: f_00004-6-3 loss: 0.821611  [  128/  306]
train() client id: f_00004-6-4 loss: 0.641143  [  160/  306]
train() client id: f_00004-6-5 loss: 0.540202  [  192/  306]
train() client id: f_00004-6-6 loss: 0.714192  [  224/  306]
train() client id: f_00004-6-7 loss: 0.757131  [  256/  306]
train() client id: f_00004-6-8 loss: 0.724336  [  288/  306]
train() client id: f_00004-7-0 loss: 0.696099  [   32/  306]
train() client id: f_00004-7-1 loss: 0.742067  [   64/  306]
train() client id: f_00004-7-2 loss: 0.682903  [   96/  306]
train() client id: f_00004-7-3 loss: 0.795653  [  128/  306]
train() client id: f_00004-7-4 loss: 0.622607  [  160/  306]
train() client id: f_00004-7-5 loss: 0.709343  [  192/  306]
train() client id: f_00004-7-6 loss: 0.733038  [  224/  306]
train() client id: f_00004-7-7 loss: 0.821379  [  256/  306]
train() client id: f_00004-7-8 loss: 0.688926  [  288/  306]
train() client id: f_00004-8-0 loss: 0.676942  [   32/  306]
train() client id: f_00004-8-1 loss: 0.673781  [   64/  306]
train() client id: f_00004-8-2 loss: 0.660210  [   96/  306]
train() client id: f_00004-8-3 loss: 0.765699  [  128/  306]
train() client id: f_00004-8-4 loss: 0.844592  [  160/  306]
train() client id: f_00004-8-5 loss: 0.817005  [  192/  306]
train() client id: f_00004-8-6 loss: 0.814365  [  224/  306]
train() client id: f_00004-8-7 loss: 0.721856  [  256/  306]
train() client id: f_00004-8-8 loss: 0.684742  [  288/  306]
train() client id: f_00004-9-0 loss: 0.745077  [   32/  306]
train() client id: f_00004-9-1 loss: 0.819881  [   64/  306]
train() client id: f_00004-9-2 loss: 0.826715  [   96/  306]
train() client id: f_00004-9-3 loss: 0.751514  [  128/  306]
train() client id: f_00004-9-4 loss: 0.721609  [  160/  306]
train() client id: f_00004-9-5 loss: 0.737949  [  192/  306]
train() client id: f_00004-9-6 loss: 0.585730  [  224/  306]
train() client id: f_00004-9-7 loss: 0.618757  [  256/  306]
train() client id: f_00004-9-8 loss: 0.789316  [  288/  306]
train() client id: f_00004-10-0 loss: 0.719571  [   32/  306]
train() client id: f_00004-10-1 loss: 0.796207  [   64/  306]
train() client id: f_00004-10-2 loss: 0.685246  [   96/  306]
train() client id: f_00004-10-3 loss: 0.677782  [  128/  306]
train() client id: f_00004-10-4 loss: 0.774624  [  160/  306]
train() client id: f_00004-10-5 loss: 0.800840  [  192/  306]
train() client id: f_00004-10-6 loss: 0.832456  [  224/  306]
train() client id: f_00004-10-7 loss: 0.644911  [  256/  306]
train() client id: f_00004-10-8 loss: 0.781089  [  288/  306]
train() client id: f_00004-11-0 loss: 0.722248  [   32/  306]
train() client id: f_00004-11-1 loss: 0.804410  [   64/  306]
train() client id: f_00004-11-2 loss: 0.745477  [   96/  306]
train() client id: f_00004-11-3 loss: 0.676139  [  128/  306]
train() client id: f_00004-11-4 loss: 0.656856  [  160/  306]
train() client id: f_00004-11-5 loss: 0.681290  [  192/  306]
train() client id: f_00004-11-6 loss: 0.663047  [  224/  306]
train() client id: f_00004-11-7 loss: 0.824628  [  256/  306]
train() client id: f_00004-11-8 loss: 0.761307  [  288/  306]
train() client id: f_00004-12-0 loss: 0.771966  [   32/  306]
train() client id: f_00004-12-1 loss: 0.721319  [   64/  306]
train() client id: f_00004-12-2 loss: 0.750866  [   96/  306]
train() client id: f_00004-12-3 loss: 0.782950  [  128/  306]
train() client id: f_00004-12-4 loss: 0.704412  [  160/  306]
train() client id: f_00004-12-5 loss: 0.825053  [  192/  306]
train() client id: f_00004-12-6 loss: 0.691536  [  224/  306]
train() client id: f_00004-12-7 loss: 0.672680  [  256/  306]
train() client id: f_00004-12-8 loss: 0.818245  [  288/  306]
train() client id: f_00005-0-0 loss: 0.824678  [   32/  146]
train() client id: f_00005-0-1 loss: 1.016242  [   64/  146]
train() client id: f_00005-0-2 loss: 0.823618  [   96/  146]
train() client id: f_00005-0-3 loss: 0.485769  [  128/  146]
train() client id: f_00005-1-0 loss: 0.904758  [   32/  146]
train() client id: f_00005-1-1 loss: 0.527233  [   64/  146]
train() client id: f_00005-1-2 loss: 0.483337  [   96/  146]
train() client id: f_00005-1-3 loss: 0.993353  [  128/  146]
train() client id: f_00005-2-0 loss: 0.962199  [   32/  146]
train() client id: f_00005-2-1 loss: 0.635202  [   64/  146]
train() client id: f_00005-2-2 loss: 0.659432  [   96/  146]
train() client id: f_00005-2-3 loss: 0.797288  [  128/  146]
train() client id: f_00005-3-0 loss: 0.707224  [   32/  146]
train() client id: f_00005-3-1 loss: 0.888930  [   64/  146]
train() client id: f_00005-3-2 loss: 0.881733  [   96/  146]
train() client id: f_00005-3-3 loss: 0.672327  [  128/  146]
train() client id: f_00005-4-0 loss: 0.744517  [   32/  146]
train() client id: f_00005-4-1 loss: 0.603553  [   64/  146]
train() client id: f_00005-4-2 loss: 0.936330  [   96/  146]
train() client id: f_00005-4-3 loss: 0.701136  [  128/  146]
train() client id: f_00005-5-0 loss: 0.748657  [   32/  146]
train() client id: f_00005-5-1 loss: 0.851622  [   64/  146]
train() client id: f_00005-5-2 loss: 0.802622  [   96/  146]
train() client id: f_00005-5-3 loss: 0.719249  [  128/  146]
train() client id: f_00005-6-0 loss: 0.719907  [   32/  146]
train() client id: f_00005-6-1 loss: 0.914835  [   64/  146]
train() client id: f_00005-6-2 loss: 0.777479  [   96/  146]
train() client id: f_00005-6-3 loss: 0.705851  [  128/  146]
train() client id: f_00005-7-0 loss: 0.765533  [   32/  146]
train() client id: f_00005-7-1 loss: 0.693802  [   64/  146]
train() client id: f_00005-7-2 loss: 0.680905  [   96/  146]
train() client id: f_00005-7-3 loss: 0.845038  [  128/  146]
train() client id: f_00005-8-0 loss: 0.740389  [   32/  146]
train() client id: f_00005-8-1 loss: 0.868743  [   64/  146]
train() client id: f_00005-8-2 loss: 0.779274  [   96/  146]
train() client id: f_00005-8-3 loss: 0.714053  [  128/  146]
train() client id: f_00005-9-0 loss: 0.763057  [   32/  146]
train() client id: f_00005-9-1 loss: 0.890964  [   64/  146]
train() client id: f_00005-9-2 loss: 0.761697  [   96/  146]
train() client id: f_00005-9-3 loss: 0.529378  [  128/  146]
train() client id: f_00005-10-0 loss: 0.434650  [   32/  146]
train() client id: f_00005-10-1 loss: 0.866898  [   64/  146]
train() client id: f_00005-10-2 loss: 0.643465  [   96/  146]
train() client id: f_00005-10-3 loss: 0.846855  [  128/  146]
train() client id: f_00005-11-0 loss: 0.883255  [   32/  146]
train() client id: f_00005-11-1 loss: 0.603071  [   64/  146]
train() client id: f_00005-11-2 loss: 0.908940  [   96/  146]
train() client id: f_00005-11-3 loss: 0.821992  [  128/  146]
train() client id: f_00005-12-0 loss: 0.997803  [   32/  146]
train() client id: f_00005-12-1 loss: 0.495631  [   64/  146]
train() client id: f_00005-12-2 loss: 0.721600  [   96/  146]
train() client id: f_00005-12-3 loss: 0.740681  [  128/  146]
train() client id: f_00006-0-0 loss: 0.485779  [   32/   54]
train() client id: f_00006-1-0 loss: 0.427471  [   32/   54]
train() client id: f_00006-2-0 loss: 0.520586  [   32/   54]
train() client id: f_00006-3-0 loss: 0.483225  [   32/   54]
train() client id: f_00006-4-0 loss: 0.442385  [   32/   54]
train() client id: f_00006-5-0 loss: 0.494274  [   32/   54]
train() client id: f_00006-6-0 loss: 0.449804  [   32/   54]
train() client id: f_00006-7-0 loss: 0.441976  [   32/   54]
train() client id: f_00006-8-0 loss: 0.468433  [   32/   54]
train() client id: f_00006-9-0 loss: 0.418806  [   32/   54]
train() client id: f_00006-10-0 loss: 0.525534  [   32/   54]
train() client id: f_00006-11-0 loss: 0.469589  [   32/   54]
train() client id: f_00006-12-0 loss: 0.468117  [   32/   54]
train() client id: f_00007-0-0 loss: 0.580375  [   32/  179]
train() client id: f_00007-0-1 loss: 0.458872  [   64/  179]
train() client id: f_00007-0-2 loss: 0.637162  [   96/  179]
train() client id: f_00007-0-3 loss: 0.687201  [  128/  179]
train() client id: f_00007-0-4 loss: 0.458917  [  160/  179]
train() client id: f_00007-1-0 loss: 0.423717  [   32/  179]
train() client id: f_00007-1-1 loss: 0.600886  [   64/  179]
train() client id: f_00007-1-2 loss: 0.489555  [   96/  179]
train() client id: f_00007-1-3 loss: 0.351288  [  128/  179]
train() client id: f_00007-1-4 loss: 0.550338  [  160/  179]
train() client id: f_00007-2-0 loss: 0.580668  [   32/  179]
train() client id: f_00007-2-1 loss: 0.513304  [   64/  179]
train() client id: f_00007-2-2 loss: 0.514658  [   96/  179]
train() client id: f_00007-2-3 loss: 0.469368  [  128/  179]
train() client id: f_00007-2-4 loss: 0.541046  [  160/  179]
train() client id: f_00007-3-0 loss: 0.488131  [   32/  179]
train() client id: f_00007-3-1 loss: 0.458588  [   64/  179]
train() client id: f_00007-3-2 loss: 0.669136  [   96/  179]
train() client id: f_00007-3-3 loss: 0.496159  [  128/  179]
train() client id: f_00007-3-4 loss: 0.471191  [  160/  179]
train() client id: f_00007-4-0 loss: 0.450363  [   32/  179]
train() client id: f_00007-4-1 loss: 0.444581  [   64/  179]
train() client id: f_00007-4-2 loss: 0.609266  [   96/  179]
train() client id: f_00007-4-3 loss: 0.382399  [  128/  179]
train() client id: f_00007-4-4 loss: 0.609071  [  160/  179]
train() client id: f_00007-5-0 loss: 0.388549  [   32/  179]
train() client id: f_00007-5-1 loss: 0.671929  [   64/  179]
train() client id: f_00007-5-2 loss: 0.595559  [   96/  179]
train() client id: f_00007-5-3 loss: 0.561594  [  128/  179]
train() client id: f_00007-5-4 loss: 0.432335  [  160/  179]
train() client id: f_00007-6-0 loss: 0.356447  [   32/  179]
train() client id: f_00007-6-1 loss: 0.526781  [   64/  179]
train() client id: f_00007-6-2 loss: 0.578760  [   96/  179]
train() client id: f_00007-6-3 loss: 0.463377  [  128/  179]
train() client id: f_00007-6-4 loss: 0.590491  [  160/  179]
train() client id: f_00007-7-0 loss: 0.551845  [   32/  179]
train() client id: f_00007-7-1 loss: 0.520990  [   64/  179]
train() client id: f_00007-7-2 loss: 0.463277  [   96/  179]
train() client id: f_00007-7-3 loss: 0.394586  [  128/  179]
train() client id: f_00007-7-4 loss: 0.549853  [  160/  179]
train() client id: f_00007-8-0 loss: 0.349228  [   32/  179]
train() client id: f_00007-8-1 loss: 0.554474  [   64/  179]
train() client id: f_00007-8-2 loss: 0.461220  [   96/  179]
train() client id: f_00007-8-3 loss: 0.856677  [  128/  179]
train() client id: f_00007-8-4 loss: 0.370740  [  160/  179]
train() client id: f_00007-9-0 loss: 0.504680  [   32/  179]
train() client id: f_00007-9-1 loss: 0.466058  [   64/  179]
train() client id: f_00007-9-2 loss: 0.344438  [   96/  179]
train() client id: f_00007-9-3 loss: 0.463340  [  128/  179]
train() client id: f_00007-9-4 loss: 0.646439  [  160/  179]
train() client id: f_00007-10-0 loss: 0.438432  [   32/  179]
train() client id: f_00007-10-1 loss: 0.432201  [   64/  179]
train() client id: f_00007-10-2 loss: 0.663598  [   96/  179]
train() client id: f_00007-10-3 loss: 0.534391  [  128/  179]
train() client id: f_00007-10-4 loss: 0.501359  [  160/  179]
train() client id: f_00007-11-0 loss: 0.458407  [   32/  179]
train() client id: f_00007-11-1 loss: 0.324215  [   64/  179]
train() client id: f_00007-11-2 loss: 0.324868  [   96/  179]
train() client id: f_00007-11-3 loss: 0.623359  [  128/  179]
train() client id: f_00007-11-4 loss: 0.667015  [  160/  179]
train() client id: f_00007-12-0 loss: 0.541938  [   32/  179]
train() client id: f_00007-12-1 loss: 0.581118  [   64/  179]
train() client id: f_00007-12-2 loss: 0.609295  [   96/  179]
train() client id: f_00007-12-3 loss: 0.506413  [  128/  179]
train() client id: f_00007-12-4 loss: 0.328638  [  160/  179]
train() client id: f_00008-0-0 loss: 0.742153  [   32/  130]
train() client id: f_00008-0-1 loss: 0.640770  [   64/  130]
train() client id: f_00008-0-2 loss: 0.720026  [   96/  130]
train() client id: f_00008-0-3 loss: 0.676207  [  128/  130]
train() client id: f_00008-1-0 loss: 0.810335  [   32/  130]
train() client id: f_00008-1-1 loss: 0.783612  [   64/  130]
train() client id: f_00008-1-2 loss: 0.687703  [   96/  130]
train() client id: f_00008-1-3 loss: 0.533627  [  128/  130]
train() client id: f_00008-2-0 loss: 0.778275  [   32/  130]
train() client id: f_00008-2-1 loss: 0.673883  [   64/  130]
train() client id: f_00008-2-2 loss: 0.678042  [   96/  130]
train() client id: f_00008-2-3 loss: 0.661369  [  128/  130]
train() client id: f_00008-3-0 loss: 0.656519  [   32/  130]
train() client id: f_00008-3-1 loss: 0.664975  [   64/  130]
train() client id: f_00008-3-2 loss: 0.754577  [   96/  130]
train() client id: f_00008-3-3 loss: 0.708882  [  128/  130]
train() client id: f_00008-4-0 loss: 0.787564  [   32/  130]
train() client id: f_00008-4-1 loss: 0.639489  [   64/  130]
train() client id: f_00008-4-2 loss: 0.682337  [   96/  130]
train() client id: f_00008-4-3 loss: 0.719759  [  128/  130]
train() client id: f_00008-5-0 loss: 0.776634  [   32/  130]
train() client id: f_00008-5-1 loss: 0.561608  [   64/  130]
train() client id: f_00008-5-2 loss: 0.693714  [   96/  130]
train() client id: f_00008-5-3 loss: 0.747287  [  128/  130]
train() client id: f_00008-6-0 loss: 0.657708  [   32/  130]
train() client id: f_00008-6-1 loss: 0.723740  [   64/  130]
train() client id: f_00008-6-2 loss: 0.735537  [   96/  130]
train() client id: f_00008-6-3 loss: 0.707910  [  128/  130]
train() client id: f_00008-7-0 loss: 0.643135  [   32/  130]
train() client id: f_00008-7-1 loss: 0.643025  [   64/  130]
train() client id: f_00008-7-2 loss: 0.823419  [   96/  130]
train() client id: f_00008-7-3 loss: 0.736129  [  128/  130]
train() client id: f_00008-8-0 loss: 0.613912  [   32/  130]
train() client id: f_00008-8-1 loss: 0.797726  [   64/  130]
train() client id: f_00008-8-2 loss: 0.663977  [   96/  130]
train() client id: f_00008-8-3 loss: 0.754950  [  128/  130]
train() client id: f_00008-9-0 loss: 0.763345  [   32/  130]
train() client id: f_00008-9-1 loss: 0.702609  [   64/  130]
train() client id: f_00008-9-2 loss: 0.632485  [   96/  130]
train() client id: f_00008-9-3 loss: 0.740790  [  128/  130]
train() client id: f_00008-10-0 loss: 0.617559  [   32/  130]
train() client id: f_00008-10-1 loss: 0.712772  [   64/  130]
train() client id: f_00008-10-2 loss: 0.729079  [   96/  130]
train() client id: f_00008-10-3 loss: 0.742382  [  128/  130]
train() client id: f_00008-11-0 loss: 0.668770  [   32/  130]
train() client id: f_00008-11-1 loss: 0.653368  [   64/  130]
train() client id: f_00008-11-2 loss: 0.808070  [   96/  130]
train() client id: f_00008-11-3 loss: 0.683446  [  128/  130]
train() client id: f_00008-12-0 loss: 0.618324  [   32/  130]
train() client id: f_00008-12-1 loss: 0.744154  [   64/  130]
train() client id: f_00008-12-2 loss: 0.735417  [   96/  130]
train() client id: f_00008-12-3 loss: 0.744153  [  128/  130]
train() client id: f_00009-0-0 loss: 1.071880  [   32/  118]
train() client id: f_00009-0-1 loss: 1.031468  [   64/  118]
train() client id: f_00009-0-2 loss: 1.100608  [   96/  118]
train() client id: f_00009-1-0 loss: 1.085142  [   32/  118]
train() client id: f_00009-1-1 loss: 1.303647  [   64/  118]
train() client id: f_00009-1-2 loss: 0.687209  [   96/  118]
train() client id: f_00009-2-0 loss: 0.862461  [   32/  118]
train() client id: f_00009-2-1 loss: 1.018234  [   64/  118]
train() client id: f_00009-2-2 loss: 0.927163  [   96/  118]
train() client id: f_00009-3-0 loss: 0.900095  [   32/  118]
train() client id: f_00009-3-1 loss: 1.055242  [   64/  118]
train() client id: f_00009-3-2 loss: 0.807441  [   96/  118]
train() client id: f_00009-4-0 loss: 0.897603  [   32/  118]
train() client id: f_00009-4-1 loss: 0.958925  [   64/  118]
train() client id: f_00009-4-2 loss: 0.810025  [   96/  118]
train() client id: f_00009-5-0 loss: 0.793258  [   32/  118]
train() client id: f_00009-5-1 loss: 0.792775  [   64/  118]
train() client id: f_00009-5-2 loss: 0.939744  [   96/  118]
train() client id: f_00009-6-0 loss: 0.875154  [   32/  118]
train() client id: f_00009-6-1 loss: 0.900190  [   64/  118]
train() client id: f_00009-6-2 loss: 0.876485  [   96/  118]
train() client id: f_00009-7-0 loss: 0.810735  [   32/  118]
train() client id: f_00009-7-1 loss: 0.843828  [   64/  118]
train() client id: f_00009-7-2 loss: 0.762505  [   96/  118]
train() client id: f_00009-8-0 loss: 0.690959  [   32/  118]
train() client id: f_00009-8-1 loss: 0.909744  [   64/  118]
train() client id: f_00009-8-2 loss: 0.843696  [   96/  118]
train() client id: f_00009-9-0 loss: 0.914407  [   32/  118]
train() client id: f_00009-9-1 loss: 0.836372  [   64/  118]
train() client id: f_00009-9-2 loss: 0.710132  [   96/  118]
train() client id: f_00009-10-0 loss: 0.671253  [   32/  118]
train() client id: f_00009-10-1 loss: 0.860971  [   64/  118]
train() client id: f_00009-10-2 loss: 0.916395  [   96/  118]
train() client id: f_00009-11-0 loss: 0.820730  [   32/  118]
train() client id: f_00009-11-1 loss: 0.705841  [   64/  118]
train() client id: f_00009-11-2 loss: 0.998024  [   96/  118]
train() client id: f_00009-12-0 loss: 0.925650  [   32/  118]
train() client id: f_00009-12-1 loss: 0.678548  [   64/  118]
train() client id: f_00009-12-2 loss: 0.911083  [   96/  118]
At round 45 accuracy: 0.6419098143236074
At round 45 training accuracy: 0.5888665325285044
At round 45 training loss: 0.8274285950388768
update_location
xs = -3.905658 4.200318 245.009024 18.811294 0.979296 3.956410 -207.443192 -186.324852 229.663977 -172.060879 
ys = 237.587959 220.555839 1.320614 -207.455176 199.350187 182.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -6.711426168117436
ys mean: 65.3941425355287
dists_uav = 257.804757 242.203470 264.634022 231.066040 223.027927 208.414651 230.303210 211.465427 251.105978 199.050139 
uav_gains = -111.973319 -110.648653 -112.583770 -109.782860 -109.200977 -108.218408 -109.726146 -108.417104 -111.390355 -107.622214 
uav_gains_db_mean: -109.95638066754711
dists_bs = 182.182673 184.900212 454.502045 428.759582 177.655972 179.126931 180.563550 174.545429 434.208996 171.023775 
bs_gains = -102.861257 -103.041307 -113.978063 -113.269047 -102.555294 -102.655564 -102.752702 -102.340497 -113.422626 -102.092641 
bs_gains_db_mean: -105.89689958552424
Round 46
-------------------------------
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.92415982 10.12154902  4.85601333  1.75987582 11.67148702  5.61542073
  2.1757575   6.89673295  5.09466844  4.55326949]
obj_prev = 57.668934107649186
eta_min = 1.934063127362615e-19	eta_max = 0.9372721317732582
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 13.346274362136274	eta = 0.9090909090909091
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 26.11640265875819	eta = 0.46457304443429304
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 19.646759764324543	eta = 0.617556118077178
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.475231020287687	eta = 0.6567158310241378
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.40977991233988	eta = 0.6590506106332402
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.40955607686498	eta = 0.6590586238034545
eta = 0.6590586238034545
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [0.03440512 0.07235997 0.03385901 0.01174143 0.08355529 0.03986625
 0.01474505 0.04887711 0.03549735 0.03222067]
ene_total = [1.70303953 2.89994011 1.70508731 0.81177669 3.30204155 1.7110066
 0.91978726 2.13279846 1.79829675 1.42578181]
ti_comp = [0.63680768 0.68794036 0.63078466 0.65472313 0.68956148 0.68923291
 0.65512248 0.66360668 0.62159244 0.69103952]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [6.27671845e-06 5.00349702e-05 6.09733991e-06 2.36008472e-07
 7.66753725e-05 8.33612381e-06 4.66846244e-07 1.65720103e-05
 7.23529432e-06 4.37802116e-06]
ene_total = [0.44938525 0.26462869 0.47132941 0.38387283 0.25969147 0.25839833
 0.38242583 0.35209241 0.50487161 0.25166996]
optimize_network iter = 0 obj = 3.578365787451716
eta = 0.6590586238034545
freqs = [27013744.23254671 52591749.45149608 26838799.91285678  8966713.64201406
 60585816.6007772  28920738.74380525 11253659.91650407 36826870.45355151
 28553553.02519245 23313187.49305714]
eta_min = 0.6590586238034565	eta_max = 0.6785433277513145
af = 0.005254252612989919	bf = 1.2178222333661453	zeta = 0.005779677874288912	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [1.43481355e-06 1.14376411e-05 1.39380888e-06 5.39498714e-08
 1.75274491e-05 1.90557908e-06 1.06717757e-07 3.78824463e-06
 1.65393723e-06 1.00078475e-06]
ene_total = [1.6846925  0.98717606 1.76700979 1.43963457 0.96585086 0.96820657
 1.43418346 1.3187243  1.89268496 0.94339011]
ti_comp = [0.59337097 0.64450365 0.58734795 0.61128643 0.64612477 0.6457962
 0.61168578 0.62016997 0.57815574 0.64760281]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.81719976e-06 4.58713790e-05 5.65886160e-06 2.17856639e-07
 7.02726667e-05 7.64051325e-06 4.30902936e-07 1.52683660e-05
 6.72969396e-06 4.01128493e-06]
ene_total = [0.47660643 0.28050789 0.49988152 0.40714012 0.27518486 0.27403394
 0.40560472 0.37338368 0.53545424 0.26691045]
optimize_network iter = 1 obj = 3.7947078371699785
eta = 0.6785433277513145
freqs = [26965339.44106933 52213419.54907353 26809447.82144539  8932753.44599161
 60140450.82785589 28709062.73531702 11210552.63537973 36652528.58326961
 28553553.02519245 23138485.42973953]
eta_min = 0.6785433277513172	eta_max = 0.6785433277513145
af = 0.005187942505206725	bf = 1.2178222333661453	zeta = 0.005706736755727398	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [1.42967619e-06 1.12736748e-05 1.39076189e-06 5.35419898e-08
 1.72707080e-05 1.87778663e-06 1.05901756e-07 3.75246172e-06
 1.65393723e-06 9.85841782e-07]
ene_total = [1.6846918  0.98715364 1.76700937 1.43963452 0.96581577 0.96820277
 1.43418335 1.31871941 1.89268496 0.94338807]
ti_comp = [0.59337097 0.64450365 0.58734795 0.61128643 0.64612477 0.6457962
 0.61168578 0.62016997 0.57815574 0.64760281]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.81719976e-06 4.58713790e-05 5.65886160e-06 2.17856639e-07
 7.02726667e-05 7.64051325e-06 4.30902936e-07 1.52683660e-05
 6.72969396e-06 4.01128493e-06]
ene_total = [0.47660643 0.28050789 0.49988152 0.40714012 0.27518486 0.27403394
 0.40560472 0.37338368 0.53545424 0.26691045]
optimize_network iter = 2 obj = 3.7947078371699785
eta = 0.6785433277513145
freqs = [26965339.44106933 52213419.54907353 26809447.82144539  8932753.44599161
 60140450.82785589 28709062.73531702 11210552.63537973 36652528.58326961
 28553553.02519245 23138485.42973953]
Done!
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.49709926e-06 4.33472348e-05 5.34747390e-06 2.05868737e-07
 6.64058034e-05 7.22008208e-06 4.07191829e-07 1.44282003e-05
 6.35938204e-06 3.79055772e-06]
ene_total = [0.01232984 0.00725442 0.01293199 0.010533   0.00711537 0.00708904
 0.01049327 0.00965887 0.01385222 0.00690495]
At round 46 energy consumption: 0.0981629748231883
At round 46 eta: 0.6785433277513145
At round 46 a_n: 12.425493895665781
At round 46 local rounds: 12.69876962905909
At round 46 global rounds: 38.653712827752926
gradient difference: 0.46845531463623047
train() client id: f_00000-0-0 loss: 1.269859  [   32/  126]
train() client id: f_00000-0-1 loss: 1.112483  [   64/  126]
train() client id: f_00000-0-2 loss: 1.172312  [   96/  126]
train() client id: f_00000-1-0 loss: 1.419554  [   32/  126]
train() client id: f_00000-1-1 loss: 0.805803  [   64/  126]
train() client id: f_00000-1-2 loss: 0.921263  [   96/  126]
train() client id: f_00000-2-0 loss: 1.220340  [   32/  126]
train() client id: f_00000-2-1 loss: 0.920322  [   64/  126]
train() client id: f_00000-2-2 loss: 0.936697  [   96/  126]
train() client id: f_00000-3-0 loss: 1.029385  [   32/  126]
train() client id: f_00000-3-1 loss: 0.812723  [   64/  126]
train() client id: f_00000-3-2 loss: 0.960893  [   96/  126]
train() client id: f_00000-4-0 loss: 0.653448  [   32/  126]
train() client id: f_00000-4-1 loss: 0.987519  [   64/  126]
train() client id: f_00000-4-2 loss: 0.876130  [   96/  126]
train() client id: f_00000-5-0 loss: 0.856814  [   32/  126]
train() client id: f_00000-5-1 loss: 0.931949  [   64/  126]
train() client id: f_00000-5-2 loss: 0.837777  [   96/  126]
train() client id: f_00000-6-0 loss: 0.827873  [   32/  126]
train() client id: f_00000-6-1 loss: 0.782127  [   64/  126]
train() client id: f_00000-6-2 loss: 0.827793  [   96/  126]
train() client id: f_00000-7-0 loss: 0.827754  [   32/  126]
train() client id: f_00000-7-1 loss: 0.848665  [   64/  126]
train() client id: f_00000-7-2 loss: 0.693297  [   96/  126]
train() client id: f_00000-8-0 loss: 0.691450  [   32/  126]
train() client id: f_00000-8-1 loss: 0.926463  [   64/  126]
train() client id: f_00000-8-2 loss: 0.809151  [   96/  126]
train() client id: f_00000-9-0 loss: 0.754812  [   32/  126]
train() client id: f_00000-9-1 loss: 0.758923  [   64/  126]
train() client id: f_00000-9-2 loss: 0.834712  [   96/  126]
train() client id: f_00000-10-0 loss: 0.873166  [   32/  126]
train() client id: f_00000-10-1 loss: 0.887120  [   64/  126]
train() client id: f_00000-10-2 loss: 0.653385  [   96/  126]
train() client id: f_00000-11-0 loss: 0.706927  [   32/  126]
train() client id: f_00000-11-1 loss: 0.773848  [   64/  126]
train() client id: f_00000-11-2 loss: 0.917316  [   96/  126]
train() client id: f_00001-0-0 loss: 0.481729  [   32/  265]
train() client id: f_00001-0-1 loss: 0.401839  [   64/  265]
train() client id: f_00001-0-2 loss: 0.550889  [   96/  265]
train() client id: f_00001-0-3 loss: 0.294959  [  128/  265]
train() client id: f_00001-0-4 loss: 0.290953  [  160/  265]
train() client id: f_00001-0-5 loss: 0.406189  [  192/  265]
train() client id: f_00001-0-6 loss: 0.334098  [  224/  265]
train() client id: f_00001-0-7 loss: 0.300349  [  256/  265]
train() client id: f_00001-1-0 loss: 0.344208  [   32/  265]
train() client id: f_00001-1-1 loss: 0.356479  [   64/  265]
train() client id: f_00001-1-2 loss: 0.404903  [   96/  265]
train() client id: f_00001-1-3 loss: 0.325331  [  128/  265]
train() client id: f_00001-1-4 loss: 0.439838  [  160/  265]
train() client id: f_00001-1-5 loss: 0.470850  [  192/  265]
train() client id: f_00001-1-6 loss: 0.382701  [  224/  265]
train() client id: f_00001-1-7 loss: 0.294283  [  256/  265]
train() client id: f_00001-2-0 loss: 0.376484  [   32/  265]
train() client id: f_00001-2-1 loss: 0.339499  [   64/  265]
train() client id: f_00001-2-2 loss: 0.374279  [   96/  265]
train() client id: f_00001-2-3 loss: 0.264208  [  128/  265]
train() client id: f_00001-2-4 loss: 0.352710  [  160/  265]
train() client id: f_00001-2-5 loss: 0.505562  [  192/  265]
train() client id: f_00001-2-6 loss: 0.284608  [  224/  265]
train() client id: f_00001-2-7 loss: 0.396730  [  256/  265]
train() client id: f_00001-3-0 loss: 0.285379  [   32/  265]
train() client id: f_00001-3-1 loss: 0.360439  [   64/  265]
train() client id: f_00001-3-2 loss: 0.427813  [   96/  265]
train() client id: f_00001-3-3 loss: 0.357745  [  128/  265]
train() client id: f_00001-3-4 loss: 0.329363  [  160/  265]
train() client id: f_00001-3-5 loss: 0.510643  [  192/  265]
train() client id: f_00001-3-6 loss: 0.307000  [  224/  265]
train() client id: f_00001-3-7 loss: 0.294587  [  256/  265]
train() client id: f_00001-4-0 loss: 0.393343  [   32/  265]
train() client id: f_00001-4-1 loss: 0.407675  [   64/  265]
train() client id: f_00001-4-2 loss: 0.309381  [   96/  265]
train() client id: f_00001-4-3 loss: 0.264943  [  128/  265]
train() client id: f_00001-4-4 loss: 0.436027  [  160/  265]
train() client id: f_00001-4-5 loss: 0.375893  [  192/  265]
train() client id: f_00001-4-6 loss: 0.397408  [  224/  265]
train() client id: f_00001-4-7 loss: 0.297182  [  256/  265]
train() client id: f_00001-5-0 loss: 0.485731  [   32/  265]
train() client id: f_00001-5-1 loss: 0.349983  [   64/  265]
train() client id: f_00001-5-2 loss: 0.312585  [   96/  265]
train() client id: f_00001-5-3 loss: 0.288133  [  128/  265]
train() client id: f_00001-5-4 loss: 0.325260  [  160/  265]
train() client id: f_00001-5-5 loss: 0.347762  [  192/  265]
train() client id: f_00001-5-6 loss: 0.368602  [  224/  265]
train() client id: f_00001-5-7 loss: 0.328571  [  256/  265]
train() client id: f_00001-6-0 loss: 0.251636  [   32/  265]
train() client id: f_00001-6-1 loss: 0.257884  [   64/  265]
train() client id: f_00001-6-2 loss: 0.277115  [   96/  265]
train() client id: f_00001-6-3 loss: 0.475055  [  128/  265]
train() client id: f_00001-6-4 loss: 0.325409  [  160/  265]
train() client id: f_00001-6-5 loss: 0.340877  [  192/  265]
train() client id: f_00001-6-6 loss: 0.480244  [  224/  265]
train() client id: f_00001-6-7 loss: 0.312429  [  256/  265]
train() client id: f_00001-7-0 loss: 0.252316  [   32/  265]
train() client id: f_00001-7-1 loss: 0.339918  [   64/  265]
train() client id: f_00001-7-2 loss: 0.310366  [   96/  265]
train() client id: f_00001-7-3 loss: 0.262824  [  128/  265]
train() client id: f_00001-7-4 loss: 0.363772  [  160/  265]
train() client id: f_00001-7-5 loss: 0.335666  [  192/  265]
train() client id: f_00001-7-6 loss: 0.435357  [  224/  265]
train() client id: f_00001-7-7 loss: 0.421367  [  256/  265]
train() client id: f_00001-8-0 loss: 0.319902  [   32/  265]
train() client id: f_00001-8-1 loss: 0.328953  [   64/  265]
train() client id: f_00001-8-2 loss: 0.417554  [   96/  265]
train() client id: f_00001-8-3 loss: 0.351994  [  128/  265]
train() client id: f_00001-8-4 loss: 0.300717  [  160/  265]
train() client id: f_00001-8-5 loss: 0.322661  [  192/  265]
train() client id: f_00001-8-6 loss: 0.414476  [  224/  265]
train() client id: f_00001-8-7 loss: 0.336035  [  256/  265]
train() client id: f_00001-9-0 loss: 0.265143  [   32/  265]
train() client id: f_00001-9-1 loss: 0.512137  [   64/  265]
train() client id: f_00001-9-2 loss: 0.281359  [   96/  265]
train() client id: f_00001-9-3 loss: 0.250422  [  128/  265]
train() client id: f_00001-9-4 loss: 0.351306  [  160/  265]
train() client id: f_00001-9-5 loss: 0.329980  [  192/  265]
train() client id: f_00001-9-6 loss: 0.251433  [  224/  265]
train() client id: f_00001-9-7 loss: 0.390573  [  256/  265]
train() client id: f_00001-10-0 loss: 0.341108  [   32/  265]
train() client id: f_00001-10-1 loss: 0.332013  [   64/  265]
train() client id: f_00001-10-2 loss: 0.424930  [   96/  265]
train() client id: f_00001-10-3 loss: 0.294463  [  128/  265]
train() client id: f_00001-10-4 loss: 0.453821  [  160/  265]
train() client id: f_00001-10-5 loss: 0.340928  [  192/  265]
train() client id: f_00001-10-6 loss: 0.258177  [  224/  265]
train() client id: f_00001-10-7 loss: 0.301184  [  256/  265]
train() client id: f_00001-11-0 loss: 0.424138  [   32/  265]
train() client id: f_00001-11-1 loss: 0.266054  [   64/  265]
train() client id: f_00001-11-2 loss: 0.342530  [   96/  265]
train() client id: f_00001-11-3 loss: 0.306043  [  128/  265]
train() client id: f_00001-11-4 loss: 0.352479  [  160/  265]
train() client id: f_00001-11-5 loss: 0.366955  [  192/  265]
train() client id: f_00001-11-6 loss: 0.380753  [  224/  265]
train() client id: f_00001-11-7 loss: 0.298823  [  256/  265]
train() client id: f_00002-0-0 loss: 0.801800  [   32/  124]
train() client id: f_00002-0-1 loss: 0.816468  [   64/  124]
train() client id: f_00002-0-2 loss: 0.865982  [   96/  124]
train() client id: f_00002-1-0 loss: 0.686185  [   32/  124]
train() client id: f_00002-1-1 loss: 0.700026  [   64/  124]
train() client id: f_00002-1-2 loss: 0.689307  [   96/  124]
train() client id: f_00002-2-0 loss: 0.794391  [   32/  124]
train() client id: f_00002-2-1 loss: 0.719663  [   64/  124]
train() client id: f_00002-2-2 loss: 0.656708  [   96/  124]
train() client id: f_00002-3-0 loss: 0.735479  [   32/  124]
train() client id: f_00002-3-1 loss: 0.645834  [   64/  124]
train() client id: f_00002-3-2 loss: 0.604288  [   96/  124]
train() client id: f_00002-4-0 loss: 0.482723  [   32/  124]
train() client id: f_00002-4-1 loss: 0.946284  [   64/  124]
train() client id: f_00002-4-2 loss: 0.560843  [   96/  124]
train() client id: f_00002-5-0 loss: 0.714397  [   32/  124]
train() client id: f_00002-5-1 loss: 0.835318  [   64/  124]
train() client id: f_00002-5-2 loss: 0.664775  [   96/  124]
train() client id: f_00002-6-0 loss: 0.683247  [   32/  124]
train() client id: f_00002-6-1 loss: 0.576745  [   64/  124]
train() client id: f_00002-6-2 loss: 0.721573  [   96/  124]
train() client id: f_00002-7-0 loss: 0.764257  [   32/  124]
train() client id: f_00002-7-1 loss: 0.587934  [   64/  124]
train() client id: f_00002-7-2 loss: 0.710035  [   96/  124]
train() client id: f_00002-8-0 loss: 0.588477  [   32/  124]
train() client id: f_00002-8-1 loss: 0.669618  [   64/  124]
train() client id: f_00002-8-2 loss: 0.838622  [   96/  124]
train() client id: f_00002-9-0 loss: 0.631330  [   32/  124]
train() client id: f_00002-9-1 loss: 0.735834  [   64/  124]
train() client id: f_00002-9-2 loss: 0.594554  [   96/  124]
train() client id: f_00002-10-0 loss: 0.709130  [   32/  124]
train() client id: f_00002-10-1 loss: 0.583236  [   64/  124]
train() client id: f_00002-10-2 loss: 0.811739  [   96/  124]
train() client id: f_00002-11-0 loss: 0.632755  [   32/  124]
train() client id: f_00002-11-1 loss: 0.670470  [   64/  124]
train() client id: f_00002-11-2 loss: 0.753771  [   96/  124]
train() client id: f_00003-0-0 loss: 0.650284  [   32/   43]
train() client id: f_00003-1-0 loss: 0.521016  [   32/   43]
train() client id: f_00003-2-0 loss: 0.566583  [   32/   43]
train() client id: f_00003-3-0 loss: 0.710825  [   32/   43]
train() client id: f_00003-4-0 loss: 0.760751  [   32/   43]
train() client id: f_00003-5-0 loss: 0.702951  [   32/   43]
train() client id: f_00003-6-0 loss: 0.555151  [   32/   43]
train() client id: f_00003-7-0 loss: 0.775809  [   32/   43]
train() client id: f_00003-8-0 loss: 0.737086  [   32/   43]
train() client id: f_00003-9-0 loss: 0.668991  [   32/   43]
train() client id: f_00003-10-0 loss: 0.760591  [   32/   43]
train() client id: f_00003-11-0 loss: 0.914645  [   32/   43]
train() client id: f_00004-0-0 loss: 0.625930  [   32/  306]
train() client id: f_00004-0-1 loss: 0.671437  [   64/  306]
train() client id: f_00004-0-2 loss: 0.846679  [   96/  306]
train() client id: f_00004-0-3 loss: 0.789873  [  128/  306]
train() client id: f_00004-0-4 loss: 0.696050  [  160/  306]
train() client id: f_00004-0-5 loss: 0.699675  [  192/  306]
train() client id: f_00004-0-6 loss: 0.808629  [  224/  306]
train() client id: f_00004-0-7 loss: 0.755168  [  256/  306]
train() client id: f_00004-0-8 loss: 0.673704  [  288/  306]
train() client id: f_00004-1-0 loss: 0.705407  [   32/  306]
train() client id: f_00004-1-1 loss: 0.769411  [   64/  306]
train() client id: f_00004-1-2 loss: 0.656279  [   96/  306]
train() client id: f_00004-1-3 loss: 0.566009  [  128/  306]
train() client id: f_00004-1-4 loss: 0.707108  [  160/  306]
train() client id: f_00004-1-5 loss: 0.909324  [  192/  306]
train() client id: f_00004-1-6 loss: 0.863043  [  224/  306]
train() client id: f_00004-1-7 loss: 0.706202  [  256/  306]
train() client id: f_00004-1-8 loss: 0.715453  [  288/  306]
train() client id: f_00004-2-0 loss: 0.662218  [   32/  306]
train() client id: f_00004-2-1 loss: 0.857228  [   64/  306]
train() client id: f_00004-2-2 loss: 0.768823  [   96/  306]
train() client id: f_00004-2-3 loss: 0.719686  [  128/  306]
train() client id: f_00004-2-4 loss: 0.777129  [  160/  306]
train() client id: f_00004-2-5 loss: 0.700852  [  192/  306]
train() client id: f_00004-2-6 loss: 0.802125  [  224/  306]
train() client id: f_00004-2-7 loss: 0.723481  [  256/  306]
train() client id: f_00004-2-8 loss: 0.669371  [  288/  306]
train() client id: f_00004-3-0 loss: 0.649851  [   32/  306]
train() client id: f_00004-3-1 loss: 0.808635  [   64/  306]
train() client id: f_00004-3-2 loss: 0.614017  [   96/  306]
train() client id: f_00004-3-3 loss: 0.684634  [  128/  306]
train() client id: f_00004-3-4 loss: 0.644588  [  160/  306]
train() client id: f_00004-3-5 loss: 0.772719  [  192/  306]
train() client id: f_00004-3-6 loss: 0.961107  [  224/  306]
train() client id: f_00004-3-7 loss: 0.732747  [  256/  306]
train() client id: f_00004-3-8 loss: 0.679620  [  288/  306]
train() client id: f_00004-4-0 loss: 0.701935  [   32/  306]
train() client id: f_00004-4-1 loss: 0.722555  [   64/  306]
train() client id: f_00004-4-2 loss: 0.838839  [   96/  306]
train() client id: f_00004-4-3 loss: 0.693791  [  128/  306]
train() client id: f_00004-4-4 loss: 0.696030  [  160/  306]
train() client id: f_00004-4-5 loss: 0.801902  [  192/  306]
train() client id: f_00004-4-6 loss: 0.764866  [  224/  306]
train() client id: f_00004-4-7 loss: 0.712580  [  256/  306]
train() client id: f_00004-4-8 loss: 0.724823  [  288/  306]
train() client id: f_00004-5-0 loss: 0.684282  [   32/  306]
train() client id: f_00004-5-1 loss: 0.809982  [   64/  306]
train() client id: f_00004-5-2 loss: 0.795946  [   96/  306]
train() client id: f_00004-5-3 loss: 0.809877  [  128/  306]
train() client id: f_00004-5-4 loss: 0.774710  [  160/  306]
train() client id: f_00004-5-5 loss: 0.671392  [  192/  306]
train() client id: f_00004-5-6 loss: 0.686858  [  224/  306]
train() client id: f_00004-5-7 loss: 0.795955  [  256/  306]
train() client id: f_00004-5-8 loss: 0.664544  [  288/  306]
train() client id: f_00004-6-0 loss: 0.774727  [   32/  306]
train() client id: f_00004-6-1 loss: 0.717150  [   64/  306]
train() client id: f_00004-6-2 loss: 0.675746  [   96/  306]
train() client id: f_00004-6-3 loss: 0.663846  [  128/  306]
train() client id: f_00004-6-4 loss: 0.759081  [  160/  306]
train() client id: f_00004-6-5 loss: 0.748012  [  192/  306]
train() client id: f_00004-6-6 loss: 0.832454  [  224/  306]
train() client id: f_00004-6-7 loss: 0.773416  [  256/  306]
train() client id: f_00004-6-8 loss: 0.831476  [  288/  306]
train() client id: f_00004-7-0 loss: 0.674588  [   32/  306]
train() client id: f_00004-7-1 loss: 0.799616  [   64/  306]
train() client id: f_00004-7-2 loss: 0.732896  [   96/  306]
train() client id: f_00004-7-3 loss: 0.688970  [  128/  306]
train() client id: f_00004-7-4 loss: 0.766221  [  160/  306]
train() client id: f_00004-7-5 loss: 0.767719  [  192/  306]
train() client id: f_00004-7-6 loss: 0.766989  [  224/  306]
train() client id: f_00004-7-7 loss: 0.799740  [  256/  306]
train() client id: f_00004-7-8 loss: 0.776629  [  288/  306]
train() client id: f_00004-8-0 loss: 0.834619  [   32/  306]
train() client id: f_00004-8-1 loss: 0.821875  [   64/  306]
train() client id: f_00004-8-2 loss: 0.731642  [   96/  306]
train() client id: f_00004-8-3 loss: 0.773705  [  128/  306]
train() client id: f_00004-8-4 loss: 0.664045  [  160/  306]
train() client id: f_00004-8-5 loss: 0.663792  [  192/  306]
train() client id: f_00004-8-6 loss: 0.831374  [  224/  306]
train() client id: f_00004-8-7 loss: 0.798809  [  256/  306]
train() client id: f_00004-8-8 loss: 0.739842  [  288/  306]
train() client id: f_00004-9-0 loss: 0.725412  [   32/  306]
train() client id: f_00004-9-1 loss: 0.798574  [   64/  306]
train() client id: f_00004-9-2 loss: 0.746027  [   96/  306]
train() client id: f_00004-9-3 loss: 0.855331  [  128/  306]
train() client id: f_00004-9-4 loss: 0.774880  [  160/  306]
train() client id: f_00004-9-5 loss: 0.691172  [  192/  306]
train() client id: f_00004-9-6 loss: 0.707192  [  224/  306]
train() client id: f_00004-9-7 loss: 0.680709  [  256/  306]
train() client id: f_00004-9-8 loss: 0.833586  [  288/  306]
train() client id: f_00004-10-0 loss: 0.746495  [   32/  306]
train() client id: f_00004-10-1 loss: 0.721973  [   64/  306]
train() client id: f_00004-10-2 loss: 0.691137  [   96/  306]
train() client id: f_00004-10-3 loss: 0.899566  [  128/  306]
train() client id: f_00004-10-4 loss: 0.815898  [  160/  306]
train() client id: f_00004-10-5 loss: 0.664728  [  192/  306]
train() client id: f_00004-10-6 loss: 0.657951  [  224/  306]
train() client id: f_00004-10-7 loss: 0.754578  [  256/  306]
train() client id: f_00004-10-8 loss: 0.891109  [  288/  306]
train() client id: f_00004-11-0 loss: 0.812302  [   32/  306]
train() client id: f_00004-11-1 loss: 0.659641  [   64/  306]
train() client id: f_00004-11-2 loss: 0.847300  [   96/  306]
train() client id: f_00004-11-3 loss: 0.641329  [  128/  306]
train() client id: f_00004-11-4 loss: 0.798647  [  160/  306]
train() client id: f_00004-11-5 loss: 0.796072  [  192/  306]
train() client id: f_00004-11-6 loss: 0.771345  [  224/  306]
train() client id: f_00004-11-7 loss: 0.785999  [  256/  306]
train() client id: f_00004-11-8 loss: 0.763774  [  288/  306]
train() client id: f_00005-0-0 loss: 0.668056  [   32/  146]
train() client id: f_00005-0-1 loss: 0.767864  [   64/  146]
train() client id: f_00005-0-2 loss: 0.593108  [   96/  146]
train() client id: f_00005-0-3 loss: 0.699299  [  128/  146]
train() client id: f_00005-1-0 loss: 0.668556  [   32/  146]
train() client id: f_00005-1-1 loss: 0.521438  [   64/  146]
train() client id: f_00005-1-2 loss: 0.534064  [   96/  146]
train() client id: f_00005-1-3 loss: 0.725295  [  128/  146]
train() client id: f_00005-2-0 loss: 0.650805  [   32/  146]
train() client id: f_00005-2-1 loss: 0.615541  [   64/  146]
train() client id: f_00005-2-2 loss: 0.732947  [   96/  146]
train() client id: f_00005-2-3 loss: 0.737793  [  128/  146]
train() client id: f_00005-3-0 loss: 0.660496  [   32/  146]
train() client id: f_00005-3-1 loss: 0.672452  [   64/  146]
train() client id: f_00005-3-2 loss: 0.506299  [   96/  146]
train() client id: f_00005-3-3 loss: 0.796846  [  128/  146]
train() client id: f_00005-4-0 loss: 0.556831  [   32/  146]
train() client id: f_00005-4-1 loss: 0.760628  [   64/  146]
train() client id: f_00005-4-2 loss: 0.952701  [   96/  146]
train() client id: f_00005-4-3 loss: 0.479519  [  128/  146]
train() client id: f_00005-5-0 loss: 0.813616  [   32/  146]
train() client id: f_00005-5-1 loss: 0.692257  [   64/  146]
train() client id: f_00005-5-2 loss: 0.446465  [   96/  146]
train() client id: f_00005-5-3 loss: 0.837817  [  128/  146]
train() client id: f_00005-6-0 loss: 0.570182  [   32/  146]
train() client id: f_00005-6-1 loss: 0.662985  [   64/  146]
train() client id: f_00005-6-2 loss: 0.757028  [   96/  146]
train() client id: f_00005-6-3 loss: 0.713151  [  128/  146]
train() client id: f_00005-7-0 loss: 0.697899  [   32/  146]
train() client id: f_00005-7-1 loss: 0.626035  [   64/  146]
train() client id: f_00005-7-2 loss: 0.572419  [   96/  146]
train() client id: f_00005-7-3 loss: 0.857660  [  128/  146]
train() client id: f_00005-8-0 loss: 1.008333  [   32/  146]
train() client id: f_00005-8-1 loss: 0.613064  [   64/  146]
train() client id: f_00005-8-2 loss: 0.690065  [   96/  146]
train() client id: f_00005-8-3 loss: 0.455267  [  128/  146]
train() client id: f_00005-9-0 loss: 0.613048  [   32/  146]
train() client id: f_00005-9-1 loss: 0.786471  [   64/  146]
train() client id: f_00005-9-2 loss: 0.603433  [   96/  146]
train() client id: f_00005-9-3 loss: 0.637092  [  128/  146]
train() client id: f_00005-10-0 loss: 0.758820  [   32/  146]
train() client id: f_00005-10-1 loss: 0.777449  [   64/  146]
train() client id: f_00005-10-2 loss: 0.823660  [   96/  146]
train() client id: f_00005-10-3 loss: 0.425519  [  128/  146]
train() client id: f_00005-11-0 loss: 0.648674  [   32/  146]
train() client id: f_00005-11-1 loss: 0.420967  [   64/  146]
train() client id: f_00005-11-2 loss: 0.951149  [   96/  146]
train() client id: f_00005-11-3 loss: 0.552012  [  128/  146]
train() client id: f_00006-0-0 loss: 0.507643  [   32/   54]
train() client id: f_00006-1-0 loss: 0.521271  [   32/   54]
train() client id: f_00006-2-0 loss: 0.544295  [   32/   54]
train() client id: f_00006-3-0 loss: 0.494780  [   32/   54]
train() client id: f_00006-4-0 loss: 0.537767  [   32/   54]
train() client id: f_00006-5-0 loss: 0.556669  [   32/   54]
train() client id: f_00006-6-0 loss: 0.449774  [   32/   54]
train() client id: f_00006-7-0 loss: 0.535838  [   32/   54]
train() client id: f_00006-8-0 loss: 0.502329  [   32/   54]
train() client id: f_00006-9-0 loss: 0.557060  [   32/   54]
train() client id: f_00006-10-0 loss: 0.537251  [   32/   54]
train() client id: f_00006-11-0 loss: 0.480347  [   32/   54]
train() client id: f_00007-0-0 loss: 0.537881  [   32/  179]
train() client id: f_00007-0-1 loss: 0.767067  [   64/  179]
train() client id: f_00007-0-2 loss: 0.830780  [   96/  179]
train() client id: f_00007-0-3 loss: 0.711846  [  128/  179]
train() client id: f_00007-0-4 loss: 0.747842  [  160/  179]
train() client id: f_00007-1-0 loss: 0.631305  [   32/  179]
train() client id: f_00007-1-1 loss: 1.144022  [   64/  179]
train() client id: f_00007-1-2 loss: 0.576609  [   96/  179]
train() client id: f_00007-1-3 loss: 0.668653  [  128/  179]
train() client id: f_00007-1-4 loss: 0.600538  [  160/  179]
train() client id: f_00007-2-0 loss: 0.659155  [   32/  179]
train() client id: f_00007-2-1 loss: 0.816934  [   64/  179]
train() client id: f_00007-2-2 loss: 0.799211  [   96/  179]
train() client id: f_00007-2-3 loss: 0.594925  [  128/  179]
train() client id: f_00007-2-4 loss: 0.625491  [  160/  179]
train() client id: f_00007-3-0 loss: 0.774197  [   32/  179]
train() client id: f_00007-3-1 loss: 0.791162  [   64/  179]
train() client id: f_00007-3-2 loss: 0.706946  [   96/  179]
train() client id: f_00007-3-3 loss: 0.537518  [  128/  179]
train() client id: f_00007-3-4 loss: 0.785807  [  160/  179]
train() client id: f_00007-4-0 loss: 0.906780  [   32/  179]
train() client id: f_00007-4-1 loss: 0.892388  [   64/  179]
train() client id: f_00007-4-2 loss: 0.687472  [   96/  179]
train() client id: f_00007-4-3 loss: 0.591610  [  128/  179]
train() client id: f_00007-4-4 loss: 0.545212  [  160/  179]
train() client id: f_00007-5-0 loss: 0.778606  [   32/  179]
train() client id: f_00007-5-1 loss: 0.576645  [   64/  179]
train() client id: f_00007-5-2 loss: 0.739286  [   96/  179]
train() client id: f_00007-5-3 loss: 0.643682  [  128/  179]
train() client id: f_00007-5-4 loss: 0.675823  [  160/  179]
train() client id: f_00007-6-0 loss: 0.547124  [   32/  179]
train() client id: f_00007-6-1 loss: 0.794158  [   64/  179]
train() client id: f_00007-6-2 loss: 0.702465  [   96/  179]
train() client id: f_00007-6-3 loss: 0.691102  [  128/  179]
train() client id: f_00007-6-4 loss: 0.809528  [  160/  179]
train() client id: f_00007-7-0 loss: 0.518745  [   32/  179]
train() client id: f_00007-7-1 loss: 0.604741  [   64/  179]
train() client id: f_00007-7-2 loss: 0.783399  [   96/  179]
train() client id: f_00007-7-3 loss: 0.665241  [  128/  179]
train() client id: f_00007-7-4 loss: 0.987353  [  160/  179]
train() client id: f_00007-8-0 loss: 0.796786  [   32/  179]
train() client id: f_00007-8-1 loss: 0.696442  [   64/  179]
train() client id: f_00007-8-2 loss: 0.637342  [   96/  179]
train() client id: f_00007-8-3 loss: 0.582222  [  128/  179]
train() client id: f_00007-8-4 loss: 0.852035  [  160/  179]
train() client id: f_00007-9-0 loss: 0.590004  [   32/  179]
train() client id: f_00007-9-1 loss: 0.901676  [   64/  179]
train() client id: f_00007-9-2 loss: 0.821578  [   96/  179]
train() client id: f_00007-9-3 loss: 0.515272  [  128/  179]
train() client id: f_00007-9-4 loss: 0.542362  [  160/  179]
train() client id: f_00007-10-0 loss: 0.783203  [   32/  179]
train() client id: f_00007-10-1 loss: 0.523785  [   64/  179]
train() client id: f_00007-10-2 loss: 0.816857  [   96/  179]
train() client id: f_00007-10-3 loss: 0.640816  [  128/  179]
train() client id: f_00007-10-4 loss: 0.640521  [  160/  179]
train() client id: f_00007-11-0 loss: 0.497912  [   32/  179]
train() client id: f_00007-11-1 loss: 0.642287  [   64/  179]
train() client id: f_00007-11-2 loss: 0.813007  [   96/  179]
train() client id: f_00007-11-3 loss: 0.836747  [  128/  179]
train() client id: f_00007-11-4 loss: 0.723570  [  160/  179]
train() client id: f_00008-0-0 loss: 0.658256  [   32/  130]
train() client id: f_00008-0-1 loss: 0.716425  [   64/  130]
train() client id: f_00008-0-2 loss: 0.618366  [   96/  130]
train() client id: f_00008-0-3 loss: 0.759169  [  128/  130]
train() client id: f_00008-1-0 loss: 0.665844  [   32/  130]
train() client id: f_00008-1-1 loss: 0.659705  [   64/  130]
train() client id: f_00008-1-2 loss: 0.721922  [   96/  130]
train() client id: f_00008-1-3 loss: 0.705248  [  128/  130]
train() client id: f_00008-2-0 loss: 0.621205  [   32/  130]
train() client id: f_00008-2-1 loss: 0.666614  [   64/  130]
train() client id: f_00008-2-2 loss: 0.783362  [   96/  130]
train() client id: f_00008-2-3 loss: 0.673018  [  128/  130]
train() client id: f_00008-3-0 loss: 0.691919  [   32/  130]
train() client id: f_00008-3-1 loss: 0.558859  [   64/  130]
train() client id: f_00008-3-2 loss: 0.637757  [   96/  130]
train() client id: f_00008-3-3 loss: 0.861980  [  128/  130]
train() client id: f_00008-4-0 loss: 0.635198  [   32/  130]
train() client id: f_00008-4-1 loss: 0.626958  [   64/  130]
train() client id: f_00008-4-2 loss: 0.654881  [   96/  130]
train() client id: f_00008-4-3 loss: 0.830453  [  128/  130]
train() client id: f_00008-5-0 loss: 0.758627  [   32/  130]
train() client id: f_00008-5-1 loss: 0.747334  [   64/  130]
train() client id: f_00008-5-2 loss: 0.554866  [   96/  130]
train() client id: f_00008-5-3 loss: 0.681831  [  128/  130]
train() client id: f_00008-6-0 loss: 0.676757  [   32/  130]
train() client id: f_00008-6-1 loss: 0.640324  [   64/  130]
train() client id: f_00008-6-2 loss: 0.725758  [   96/  130]
train() client id: f_00008-6-3 loss: 0.661826  [  128/  130]
train() client id: f_00008-7-0 loss: 0.742043  [   32/  130]
train() client id: f_00008-7-1 loss: 0.584801  [   64/  130]
train() client id: f_00008-7-2 loss: 0.728323  [   96/  130]
train() client id: f_00008-7-3 loss: 0.645943  [  128/  130]
train() client id: f_00008-8-0 loss: 0.626454  [   32/  130]
train() client id: f_00008-8-1 loss: 0.809709  [   64/  130]
train() client id: f_00008-8-2 loss: 0.649588  [   96/  130]
train() client id: f_00008-8-3 loss: 0.646416  [  128/  130]
train() client id: f_00008-9-0 loss: 0.735350  [   32/  130]
train() client id: f_00008-9-1 loss: 0.690166  [   64/  130]
train() client id: f_00008-9-2 loss: 0.610226  [   96/  130]
train() client id: f_00008-9-3 loss: 0.692957  [  128/  130]
train() client id: f_00008-10-0 loss: 0.578285  [   32/  130]
train() client id: f_00008-10-1 loss: 0.613712  [   64/  130]
train() client id: f_00008-10-2 loss: 0.742594  [   96/  130]
train() client id: f_00008-10-3 loss: 0.796427  [  128/  130]
train() client id: f_00008-11-0 loss: 0.681799  [   32/  130]
train() client id: f_00008-11-1 loss: 0.644559  [   64/  130]
train() client id: f_00008-11-2 loss: 0.727463  [   96/  130]
train() client id: f_00008-11-3 loss: 0.651945  [  128/  130]
train() client id: f_00009-0-0 loss: 0.818699  [   32/  118]
train() client id: f_00009-0-1 loss: 0.932513  [   64/  118]
train() client id: f_00009-0-2 loss: 0.879624  [   96/  118]
train() client id: f_00009-1-0 loss: 0.862545  [   32/  118]
train() client id: f_00009-1-1 loss: 0.809160  [   64/  118]
train() client id: f_00009-1-2 loss: 0.918672  [   96/  118]
train() client id: f_00009-2-0 loss: 0.928784  [   32/  118]
train() client id: f_00009-2-1 loss: 0.771318  [   64/  118]
train() client id: f_00009-2-2 loss: 0.748021  [   96/  118]
train() client id: f_00009-3-0 loss: 0.732840  [   32/  118]
train() client id: f_00009-3-1 loss: 0.792101  [   64/  118]
train() client id: f_00009-3-2 loss: 0.769221  [   96/  118]
train() client id: f_00009-4-0 loss: 0.746508  [   32/  118]
train() client id: f_00009-4-1 loss: 0.712335  [   64/  118]
train() client id: f_00009-4-2 loss: 0.781843  [   96/  118]
train() client id: f_00009-5-0 loss: 0.613130  [   32/  118]
train() client id: f_00009-5-1 loss: 0.915321  [   64/  118]
train() client id: f_00009-5-2 loss: 0.798098  [   96/  118]
train() client id: f_00009-6-0 loss: 0.681982  [   32/  118]
train() client id: f_00009-6-1 loss: 0.781856  [   64/  118]
train() client id: f_00009-6-2 loss: 0.884786  [   96/  118]
train() client id: f_00009-7-0 loss: 0.655795  [   32/  118]
train() client id: f_00009-7-1 loss: 0.608874  [   64/  118]
train() client id: f_00009-7-2 loss: 0.802453  [   96/  118]
train() client id: f_00009-8-0 loss: 0.733565  [   32/  118]
train() client id: f_00009-8-1 loss: 0.763216  [   64/  118]
train() client id: f_00009-8-2 loss: 0.631443  [   96/  118]
train() client id: f_00009-9-0 loss: 0.693237  [   32/  118]
train() client id: f_00009-9-1 loss: 0.711970  [   64/  118]
train() client id: f_00009-9-2 loss: 0.677445  [   96/  118]
train() client id: f_00009-10-0 loss: 0.848979  [   32/  118]
train() client id: f_00009-10-1 loss: 0.754817  [   64/  118]
train() client id: f_00009-10-2 loss: 0.597910  [   96/  118]
train() client id: f_00009-11-0 loss: 0.977523  [   32/  118]
train() client id: f_00009-11-1 loss: 0.598249  [   64/  118]
train() client id: f_00009-11-2 loss: 0.597628  [   96/  118]
At round 46 accuracy: 0.6419098143236074
At round 46 training accuracy: 0.5922199865861838
At round 46 training loss: 0.8249896668663913
update_location
xs = -3.905658 4.200318 250.009024 18.811294 0.979296 3.956410 -212.443192 -191.324852 234.663977 -177.060879 
ys = 242.587959 225.555839 1.320614 -212.455176 204.350187 187.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -7.211426168117436
ys mean: 66.8941425355287
dists_uav = 262.419840 246.765231 269.269857 235.565419 227.508149 212.814023 234.816951 215.883939 255.687020 203.387725 
uav_gains = -112.384465 -111.023523 -113.003964 -110.124065 -109.521078 -108.505862 -110.066504 -108.710294 -111.787044 -107.896284 
uav_gains_db_mean: -110.30230836391787
dists_bs = 183.960338 186.195185 459.126562 433.225497 178.410051 179.414601 181.528586 174.941005 438.872523 171.010937 
bs_gains = -102.979337 -103.126176 -114.101167 -113.395051 -102.606800 -102.675077 -102.817520 -102.368024 -113.552534 -102.091728 
bs_gains_db_mean: -105.97134140382042
Round 47
-------------------------------
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.7932913   9.84287299  4.72759004  1.71436679 11.34993316  5.46069336
  2.11875437  6.70878283  4.95608411  4.42774153]
obj_prev = 56.1001104698108
eta_min = 5.969830954362688e-20	eta_max = 0.9382327126425405
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 12.978344451772108	eta = 0.9090909090909091
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 25.625037120999544	eta = 0.4604284044719558
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 19.192856434060143	eta = 0.6147336638864522
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 18.0285169675076	eta = 0.6544351361468406
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 17.963027394397052	eta = 0.6568210745943969
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 17.962800013672382	eta = 0.6568293889135345
eta = 0.6568293889135345
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [0.0346847  0.07294799 0.03413415 0.01183684 0.08423428 0.04019021
 0.01486487 0.0492743  0.03578581 0.0324825 ]
ene_total = [1.66925633 2.82340307 1.67271292 0.79630461 3.21465228 1.66466235
 0.90143074 2.08079959 1.75289976 1.38667837]
ti_comp = [0.65804615 0.71288711 0.65155892 0.67749658 0.71463044 0.71440598
 0.67791976 0.68705877 0.6453014  0.71627974]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [6.02256253e-06 4.77395682e-05 5.85517725e-06 2.25825789e-07
 7.31448314e-05 7.94970091e-06 4.46690869e-07 1.58399461e-05
 6.87839679e-06 4.17506343e-06]
ene_total = [0.44822894 0.2566042  0.4710644  0.37954037 0.25136048 0.24985529
 0.37805813 0.34642191 0.493133   0.24312494]
optimize_network iter = 0 obj = 3.5173916587449052
eta = 0.6568293889135345
freqs = [26354310.22875877 51163774.23769676 26194218.65211249  8735722.05600123
 58935554.78523942 28128413.29923534 10963593.51009882 35858870.20763274
 27727978.21502854 22674452.1605431 ]
eta_min = 0.6568293889135383	eta_max = 0.6851522482532448
af = 0.0048373581457900995	bf = 1.2050321534900845	zeta = 0.00532109396036911	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [1.36561793e-06 1.08249620e-05 1.32766327e-06 5.12060679e-08
 1.65856133e-05 1.80259716e-06 1.01287294e-07 3.59171272e-06
 1.55967862e-06 9.46696933e-07]
ene_total = [1.69139003 0.96374279 1.77760827 1.4326949  0.9413374  0.94235589
 1.42707694 1.30607207 1.86080934 0.91733759]
ti_comp = [0.59323403 0.64807499 0.5867468  0.61268446 0.64981832 0.64959386
 0.61310764 0.62224665 0.58048928 0.65146762]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.39431995e-06 4.20498333e-05 5.25582343e-06 2.01005902e-07
 6.43958017e-05 6.99925668e-06 3.97543809e-07 1.40575984e-05
 6.18755144e-06 3.67398631e-06]
ene_total = [0.4885263  0.27946926 0.51341709 0.41368185 0.27363643 0.27229513
 0.41206534 0.3775167  0.53746741 0.26497658]
optimize_network iter = 1 obj = 3.8330520913049613
eta = 0.6851522482532448
freqs = [26297444.52410097 50627871.73999517 26166161.04026021  8689617.22720056
 58304024.4554098  27827872.27421467 10905010.46336445 35617179.03212754
 27727978.21502853 22426331.23656411]
eta_min = 0.6851522482532453	eta_max = 0.6851522482532441
af = 0.0047490407946122245	bf = 1.2050321534900845	zeta = 0.005223944874073448	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [1.35973100e-06 1.05993827e-05 1.32482057e-06 5.06669901e-08
 1.62320679e-05 1.76428286e-06 1.00207745e-07 3.54345912e-06
 1.55967862e-06 9.26091353e-07]
ene_total = [1.69138925 0.96371281 1.77760789 1.43269482 0.94129041 0.9423508
 1.4270768  1.30606566 1.86080934 0.91733486]
ti_comp = [0.59323403 0.64807499 0.5867468  0.61268446 0.64981832 0.64959386
 0.61310764 0.62224665 0.58048928 0.65146762]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.39431995e-06 4.20498333e-05 5.25582343e-06 2.01005902e-07
 6.43958017e-05 6.99925668e-06 3.97543809e-07 1.40575984e-05
 6.18755144e-06 3.67398631e-06]
ene_total = [0.4885263  0.27946926 0.51341709 0.41368185 0.27363643 0.27229513
 0.41206534 0.3775167  0.53746741 0.26497658]
optimize_network iter = 2 obj = 3.8330520913049524
eta = 0.6851522482532441
freqs = [26297444.52410097 50627871.73999519 26166161.04026021  8689617.22720056
 58304024.45540981 27827872.27421467 10905010.46336445 35617179.03212754
 27727978.21502853 22426331.23656412]
Done!
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.22816025e-06 4.07545842e-05 5.09392980e-06 1.94814375e-07
 6.24122362e-05 6.78366056e-06 3.85298380e-07 1.36245862e-05
 5.99695805e-06 3.56081756e-06]
ene_total = [0.01272946 0.00728089 0.01337805 0.01077938 0.00712821 0.00709503
 0.01073726 0.00983659 0.0140047  0.00690443]
At round 47 energy consumption: 0.09987401390775219
At round 47 eta: 0.6851522482532441
At round 47 a_n: 12.082948048696464
At round 47 local rounds: 12.381380128537735
At round 47 global rounds: 38.37711395955986
gradient difference: 0.501997709274292
train() client id: f_00000-0-0 loss: 1.237485  [   32/  126]
train() client id: f_00000-0-1 loss: 0.865362  [   64/  126]
train() client id: f_00000-0-2 loss: 1.084996  [   96/  126]
train() client id: f_00000-1-0 loss: 0.759794  [   32/  126]
train() client id: f_00000-1-1 loss: 1.048926  [   64/  126]
train() client id: f_00000-1-2 loss: 0.891623  [   96/  126]
train() client id: f_00000-2-0 loss: 0.879767  [   32/  126]
train() client id: f_00000-2-1 loss: 0.816556  [   64/  126]
train() client id: f_00000-2-2 loss: 0.869495  [   96/  126]
train() client id: f_00000-3-0 loss: 0.762410  [   32/  126]
train() client id: f_00000-3-1 loss: 1.031602  [   64/  126]
train() client id: f_00000-3-2 loss: 0.865789  [   96/  126]
train() client id: f_00000-4-0 loss: 0.698037  [   32/  126]
train() client id: f_00000-4-1 loss: 0.731821  [   64/  126]
train() client id: f_00000-4-2 loss: 0.833758  [   96/  126]
train() client id: f_00000-5-0 loss: 0.910496  [   32/  126]
train() client id: f_00000-5-1 loss: 0.641049  [   64/  126]
train() client id: f_00000-5-2 loss: 0.771245  [   96/  126]
train() client id: f_00000-6-0 loss: 0.796048  [   32/  126]
train() client id: f_00000-6-1 loss: 0.717525  [   64/  126]
train() client id: f_00000-6-2 loss: 0.741605  [   96/  126]
train() client id: f_00000-7-0 loss: 0.782688  [   32/  126]
train() client id: f_00000-7-1 loss: 0.613673  [   64/  126]
train() client id: f_00000-7-2 loss: 0.717821  [   96/  126]
train() client id: f_00000-8-0 loss: 0.681244  [   32/  126]
train() client id: f_00000-8-1 loss: 0.640626  [   64/  126]
train() client id: f_00000-8-2 loss: 0.724879  [   96/  126]
train() client id: f_00000-9-0 loss: 0.733648  [   32/  126]
train() client id: f_00000-9-1 loss: 0.590047  [   64/  126]
train() client id: f_00000-9-2 loss: 0.625032  [   96/  126]
train() client id: f_00000-10-0 loss: 0.791585  [   32/  126]
train() client id: f_00000-10-1 loss: 0.693966  [   64/  126]
train() client id: f_00000-10-2 loss: 0.569284  [   96/  126]
train() client id: f_00000-11-0 loss: 0.697453  [   32/  126]
train() client id: f_00000-11-1 loss: 0.684848  [   64/  126]
train() client id: f_00000-11-2 loss: 0.660670  [   96/  126]
train() client id: f_00001-0-0 loss: 0.539571  [   32/  265]
train() client id: f_00001-0-1 loss: 0.470268  [   64/  265]
train() client id: f_00001-0-2 loss: 0.563527  [   96/  265]
train() client id: f_00001-0-3 loss: 0.513617  [  128/  265]
train() client id: f_00001-0-4 loss: 0.393479  [  160/  265]
train() client id: f_00001-0-5 loss: 0.438514  [  192/  265]
train() client id: f_00001-0-6 loss: 0.409592  [  224/  265]
train() client id: f_00001-0-7 loss: 0.518697  [  256/  265]
train() client id: f_00001-1-0 loss: 0.423449  [   32/  265]
train() client id: f_00001-1-1 loss: 0.560287  [   64/  265]
train() client id: f_00001-1-2 loss: 0.428140  [   96/  265]
train() client id: f_00001-1-3 loss: 0.473129  [  128/  265]
train() client id: f_00001-1-4 loss: 0.463615  [  160/  265]
train() client id: f_00001-1-5 loss: 0.527403  [  192/  265]
train() client id: f_00001-1-6 loss: 0.402029  [  224/  265]
train() client id: f_00001-1-7 loss: 0.424723  [  256/  265]
train() client id: f_00001-2-0 loss: 0.506963  [   32/  265]
train() client id: f_00001-2-1 loss: 0.453064  [   64/  265]
train() client id: f_00001-2-2 loss: 0.469712  [   96/  265]
train() client id: f_00001-2-3 loss: 0.395215  [  128/  265]
train() client id: f_00001-2-4 loss: 0.434616  [  160/  265]
train() client id: f_00001-2-5 loss: 0.472146  [  192/  265]
train() client id: f_00001-2-6 loss: 0.518851  [  224/  265]
train() client id: f_00001-2-7 loss: 0.447511  [  256/  265]
train() client id: f_00001-3-0 loss: 0.401775  [   32/  265]
train() client id: f_00001-3-1 loss: 0.364759  [   64/  265]
train() client id: f_00001-3-2 loss: 0.395197  [   96/  265]
train() client id: f_00001-3-3 loss: 0.440572  [  128/  265]
train() client id: f_00001-3-4 loss: 0.630046  [  160/  265]
train() client id: f_00001-3-5 loss: 0.459523  [  192/  265]
train() client id: f_00001-3-6 loss: 0.557251  [  224/  265]
train() client id: f_00001-3-7 loss: 0.425823  [  256/  265]
train() client id: f_00001-4-0 loss: 0.371208  [   32/  265]
train() client id: f_00001-4-1 loss: 0.361321  [   64/  265]
train() client id: f_00001-4-2 loss: 0.381336  [   96/  265]
train() client id: f_00001-4-3 loss: 0.402802  [  128/  265]
train() client id: f_00001-4-4 loss: 0.462239  [  160/  265]
train() client id: f_00001-4-5 loss: 0.453159  [  192/  265]
train() client id: f_00001-4-6 loss: 0.653062  [  224/  265]
train() client id: f_00001-4-7 loss: 0.580523  [  256/  265]
train() client id: f_00001-5-0 loss: 0.454132  [   32/  265]
train() client id: f_00001-5-1 loss: 0.501512  [   64/  265]
train() client id: f_00001-5-2 loss: 0.365222  [   96/  265]
train() client id: f_00001-5-3 loss: 0.413431  [  128/  265]
train() client id: f_00001-5-4 loss: 0.460941  [  160/  265]
train() client id: f_00001-5-5 loss: 0.523281  [  192/  265]
train() client id: f_00001-5-6 loss: 0.367920  [  224/  265]
train() client id: f_00001-5-7 loss: 0.529662  [  256/  265]
train() client id: f_00001-6-0 loss: 0.352466  [   32/  265]
train() client id: f_00001-6-1 loss: 0.605493  [   64/  265]
train() client id: f_00001-6-2 loss: 0.414432  [   96/  265]
train() client id: f_00001-6-3 loss: 0.458975  [  128/  265]
train() client id: f_00001-6-4 loss: 0.458807  [  160/  265]
train() client id: f_00001-6-5 loss: 0.409550  [  192/  265]
train() client id: f_00001-6-6 loss: 0.533373  [  224/  265]
train() client id: f_00001-6-7 loss: 0.434534  [  256/  265]
train() client id: f_00001-7-0 loss: 0.424855  [   32/  265]
train() client id: f_00001-7-1 loss: 0.448156  [   64/  265]
train() client id: f_00001-7-2 loss: 0.602221  [   96/  265]
train() client id: f_00001-7-3 loss: 0.479893  [  128/  265]
train() client id: f_00001-7-4 loss: 0.409424  [  160/  265]
train() client id: f_00001-7-5 loss: 0.451774  [  192/  265]
train() client id: f_00001-7-6 loss: 0.366182  [  224/  265]
train() client id: f_00001-7-7 loss: 0.481452  [  256/  265]
train() client id: f_00001-8-0 loss: 0.465051  [   32/  265]
train() client id: f_00001-8-1 loss: 0.370763  [   64/  265]
train() client id: f_00001-8-2 loss: 0.665730  [   96/  265]
train() client id: f_00001-8-3 loss: 0.357664  [  128/  265]
train() client id: f_00001-8-4 loss: 0.504677  [  160/  265]
train() client id: f_00001-8-5 loss: 0.361454  [  192/  265]
train() client id: f_00001-8-6 loss: 0.481201  [  224/  265]
train() client id: f_00001-8-7 loss: 0.421807  [  256/  265]
train() client id: f_00001-9-0 loss: 0.415579  [   32/  265]
train() client id: f_00001-9-1 loss: 0.548428  [   64/  265]
train() client id: f_00001-9-2 loss: 0.467118  [   96/  265]
train() client id: f_00001-9-3 loss: 0.365501  [  128/  265]
train() client id: f_00001-9-4 loss: 0.388184  [  160/  265]
train() client id: f_00001-9-5 loss: 0.385671  [  192/  265]
train() client id: f_00001-9-6 loss: 0.546085  [  224/  265]
train() client id: f_00001-9-7 loss: 0.557302  [  256/  265]
train() client id: f_00001-10-0 loss: 0.441366  [   32/  265]
train() client id: f_00001-10-1 loss: 0.470003  [   64/  265]
train() client id: f_00001-10-2 loss: 0.553152  [   96/  265]
train() client id: f_00001-10-3 loss: 0.450098  [  128/  265]
train() client id: f_00001-10-4 loss: 0.376099  [  160/  265]
train() client id: f_00001-10-5 loss: 0.474827  [  192/  265]
train() client id: f_00001-10-6 loss: 0.394666  [  224/  265]
train() client id: f_00001-10-7 loss: 0.518836  [  256/  265]
train() client id: f_00001-11-0 loss: 0.361411  [   32/  265]
train() client id: f_00001-11-1 loss: 0.475153  [   64/  265]
train() client id: f_00001-11-2 loss: 0.455665  [   96/  265]
train() client id: f_00001-11-3 loss: 0.481412  [  128/  265]
train() client id: f_00001-11-4 loss: 0.519594  [  160/  265]
train() client id: f_00001-11-5 loss: 0.452368  [  192/  265]
train() client id: f_00001-11-6 loss: 0.472495  [  224/  265]
train() client id: f_00001-11-7 loss: 0.459067  [  256/  265]
train() client id: f_00002-0-0 loss: 1.198328  [   32/  124]
train() client id: f_00002-0-1 loss: 0.915413  [   64/  124]
train() client id: f_00002-0-2 loss: 1.104606  [   96/  124]
train() client id: f_00002-1-0 loss: 1.093914  [   32/  124]
train() client id: f_00002-1-1 loss: 1.181519  [   64/  124]
train() client id: f_00002-1-2 loss: 0.972167  [   96/  124]
train() client id: f_00002-2-0 loss: 1.182138  [   32/  124]
train() client id: f_00002-2-1 loss: 1.066473  [   64/  124]
train() client id: f_00002-2-2 loss: 0.913850  [   96/  124]
train() client id: f_00002-3-0 loss: 1.014630  [   32/  124]
train() client id: f_00002-3-1 loss: 0.910449  [   64/  124]
train() client id: f_00002-3-2 loss: 1.113187  [   96/  124]
train() client id: f_00002-4-0 loss: 0.908287  [   32/  124]
train() client id: f_00002-4-1 loss: 0.940110  [   64/  124]
train() client id: f_00002-4-2 loss: 1.019331  [   96/  124]
train() client id: f_00002-5-0 loss: 0.977186  [   32/  124]
train() client id: f_00002-5-1 loss: 1.024704  [   64/  124]
train() client id: f_00002-5-2 loss: 0.824894  [   96/  124]
train() client id: f_00002-6-0 loss: 0.902825  [   32/  124]
train() client id: f_00002-6-1 loss: 0.970558  [   64/  124]
train() client id: f_00002-6-2 loss: 0.935679  [   96/  124]
train() client id: f_00002-7-0 loss: 0.945110  [   32/  124]
train() client id: f_00002-7-1 loss: 0.891140  [   64/  124]
train() client id: f_00002-7-2 loss: 0.872689  [   96/  124]
train() client id: f_00002-8-0 loss: 0.710340  [   32/  124]
train() client id: f_00002-8-1 loss: 1.010662  [   64/  124]
train() client id: f_00002-8-2 loss: 0.781569  [   96/  124]
train() client id: f_00002-9-0 loss: 0.867594  [   32/  124]
train() client id: f_00002-9-1 loss: 0.805593  [   64/  124]
train() client id: f_00002-9-2 loss: 1.013629  [   96/  124]
train() client id: f_00002-10-0 loss: 0.816281  [   32/  124]
train() client id: f_00002-10-1 loss: 0.927889  [   64/  124]
train() client id: f_00002-10-2 loss: 0.878581  [   96/  124]
train() client id: f_00002-11-0 loss: 0.750080  [   32/  124]
train() client id: f_00002-11-1 loss: 0.813967  [   64/  124]
train() client id: f_00002-11-2 loss: 0.910263  [   96/  124]
train() client id: f_00003-0-0 loss: 0.343816  [   32/   43]
train() client id: f_00003-1-0 loss: 0.373513  [   32/   43]
train() client id: f_00003-2-0 loss: 0.796331  [   32/   43]
train() client id: f_00003-3-0 loss: 0.595661  [   32/   43]
train() client id: f_00003-4-0 loss: 0.671791  [   32/   43]
train() client id: f_00003-5-0 loss: 0.445510  [   32/   43]
train() client id: f_00003-6-0 loss: 0.458883  [   32/   43]
train() client id: f_00003-7-0 loss: 0.533769  [   32/   43]
train() client id: f_00003-8-0 loss: 0.385168  [   32/   43]
train() client id: f_00003-9-0 loss: 0.563783  [   32/   43]
train() client id: f_00003-10-0 loss: 0.450154  [   32/   43]
train() client id: f_00003-11-0 loss: 0.271407  [   32/   43]
train() client id: f_00004-0-0 loss: 0.862503  [   32/  306]
train() client id: f_00004-0-1 loss: 0.789401  [   64/  306]
train() client id: f_00004-0-2 loss: 0.860678  [   96/  306]
train() client id: f_00004-0-3 loss: 0.950097  [  128/  306]
train() client id: f_00004-0-4 loss: 0.887468  [  160/  306]
train() client id: f_00004-0-5 loss: 0.717828  [  192/  306]
train() client id: f_00004-0-6 loss: 0.824025  [  224/  306]
train() client id: f_00004-0-7 loss: 0.887856  [  256/  306]
train() client id: f_00004-0-8 loss: 0.818802  [  288/  306]
train() client id: f_00004-1-0 loss: 0.835782  [   32/  306]
train() client id: f_00004-1-1 loss: 0.734934  [   64/  306]
train() client id: f_00004-1-2 loss: 0.896892  [   96/  306]
train() client id: f_00004-1-3 loss: 0.937860  [  128/  306]
train() client id: f_00004-1-4 loss: 0.783911  [  160/  306]
train() client id: f_00004-1-5 loss: 0.919294  [  192/  306]
train() client id: f_00004-1-6 loss: 0.814150  [  224/  306]
train() client id: f_00004-1-7 loss: 0.719334  [  256/  306]
train() client id: f_00004-1-8 loss: 0.784579  [  288/  306]
train() client id: f_00004-2-0 loss: 0.960804  [   32/  306]
train() client id: f_00004-2-1 loss: 0.895074  [   64/  306]
train() client id: f_00004-2-2 loss: 0.843110  [   96/  306]
train() client id: f_00004-2-3 loss: 0.818788  [  128/  306]
train() client id: f_00004-2-4 loss: 0.908829  [  160/  306]
train() client id: f_00004-2-5 loss: 0.733186  [  192/  306]
train() client id: f_00004-2-6 loss: 0.796288  [  224/  306]
train() client id: f_00004-2-7 loss: 0.777926  [  256/  306]
train() client id: f_00004-2-8 loss: 0.734972  [  288/  306]
train() client id: f_00004-3-0 loss: 0.820899  [   32/  306]
train() client id: f_00004-3-1 loss: 0.863340  [   64/  306]
train() client id: f_00004-3-2 loss: 0.914262  [   96/  306]
train() client id: f_00004-3-3 loss: 0.861442  [  128/  306]
train() client id: f_00004-3-4 loss: 0.935113  [  160/  306]
train() client id: f_00004-3-5 loss: 0.738970  [  192/  306]
train() client id: f_00004-3-6 loss: 0.933501  [  224/  306]
train() client id: f_00004-3-7 loss: 0.777426  [  256/  306]
train() client id: f_00004-3-8 loss: 0.656187  [  288/  306]
train() client id: f_00004-4-0 loss: 0.706948  [   32/  306]
train() client id: f_00004-4-1 loss: 0.829023  [   64/  306]
train() client id: f_00004-4-2 loss: 0.998083  [   96/  306]
train() client id: f_00004-4-3 loss: 0.799517  [  128/  306]
train() client id: f_00004-4-4 loss: 0.772067  [  160/  306]
train() client id: f_00004-4-5 loss: 0.904243  [  192/  306]
train() client id: f_00004-4-6 loss: 0.686254  [  224/  306]
train() client id: f_00004-4-7 loss: 1.056270  [  256/  306]
train() client id: f_00004-4-8 loss: 0.722534  [  288/  306]
train() client id: f_00004-5-0 loss: 0.868377  [   32/  306]
train() client id: f_00004-5-1 loss: 0.686981  [   64/  306]
train() client id: f_00004-5-2 loss: 0.835778  [   96/  306]
train() client id: f_00004-5-3 loss: 0.902854  [  128/  306]
train() client id: f_00004-5-4 loss: 0.843078  [  160/  306]
train() client id: f_00004-5-5 loss: 0.713037  [  192/  306]
train() client id: f_00004-5-6 loss: 0.911358  [  224/  306]
train() client id: f_00004-5-7 loss: 0.768072  [  256/  306]
train() client id: f_00004-5-8 loss: 0.817401  [  288/  306]
train() client id: f_00004-6-0 loss: 0.868325  [   32/  306]
train() client id: f_00004-6-1 loss: 0.704241  [   64/  306]
train() client id: f_00004-6-2 loss: 0.810621  [   96/  306]
train() client id: f_00004-6-3 loss: 0.808086  [  128/  306]
train() client id: f_00004-6-4 loss: 0.840288  [  160/  306]
train() client id: f_00004-6-5 loss: 0.877264  [  192/  306]
train() client id: f_00004-6-6 loss: 0.779003  [  224/  306]
train() client id: f_00004-6-7 loss: 0.867965  [  256/  306]
train() client id: f_00004-6-8 loss: 0.925590  [  288/  306]
train() client id: f_00004-7-0 loss: 0.719896  [   32/  306]
train() client id: f_00004-7-1 loss: 0.894918  [   64/  306]
train() client id: f_00004-7-2 loss: 0.899424  [   96/  306]
train() client id: f_00004-7-3 loss: 0.855714  [  128/  306]
train() client id: f_00004-7-4 loss: 0.874784  [  160/  306]
train() client id: f_00004-7-5 loss: 0.770602  [  192/  306]
train() client id: f_00004-7-6 loss: 0.865548  [  224/  306]
train() client id: f_00004-7-7 loss: 0.854895  [  256/  306]
train() client id: f_00004-7-8 loss: 0.738654  [  288/  306]
train() client id: f_00004-8-0 loss: 0.823811  [   32/  306]
train() client id: f_00004-8-1 loss: 0.647275  [   64/  306]
train() client id: f_00004-8-2 loss: 0.919641  [   96/  306]
train() client id: f_00004-8-3 loss: 0.969012  [  128/  306]
train() client id: f_00004-8-4 loss: 0.869831  [  160/  306]
train() client id: f_00004-8-5 loss: 0.818218  [  192/  306]
train() client id: f_00004-8-6 loss: 0.799811  [  224/  306]
train() client id: f_00004-8-7 loss: 0.885596  [  256/  306]
train() client id: f_00004-8-8 loss: 0.765045  [  288/  306]
train() client id: f_00004-9-0 loss: 0.810256  [   32/  306]
train() client id: f_00004-9-1 loss: 0.676546  [   64/  306]
train() client id: f_00004-9-2 loss: 0.901274  [   96/  306]
train() client id: f_00004-9-3 loss: 0.823508  [  128/  306]
train() client id: f_00004-9-4 loss: 0.676765  [  160/  306]
train() client id: f_00004-9-5 loss: 0.870838  [  192/  306]
train() client id: f_00004-9-6 loss: 0.860071  [  224/  306]
train() client id: f_00004-9-7 loss: 0.854730  [  256/  306]
train() client id: f_00004-9-8 loss: 0.858351  [  288/  306]
train() client id: f_00004-10-0 loss: 0.946551  [   32/  306]
train() client id: f_00004-10-1 loss: 0.737715  [   64/  306]
train() client id: f_00004-10-2 loss: 0.821027  [   96/  306]
train() client id: f_00004-10-3 loss: 0.837056  [  128/  306]
train() client id: f_00004-10-4 loss: 0.811690  [  160/  306]
train() client id: f_00004-10-5 loss: 0.858225  [  192/  306]
train() client id: f_00004-10-6 loss: 0.837299  [  224/  306]
train() client id: f_00004-10-7 loss: 0.746293  [  256/  306]
train() client id: f_00004-10-8 loss: 0.789739  [  288/  306]
train() client id: f_00004-11-0 loss: 0.860827  [   32/  306]
train() client id: f_00004-11-1 loss: 0.848210  [   64/  306]
train() client id: f_00004-11-2 loss: 0.849259  [   96/  306]
train() client id: f_00004-11-3 loss: 0.817597  [  128/  306]
train() client id: f_00004-11-4 loss: 0.832025  [  160/  306]
train() client id: f_00004-11-5 loss: 0.833829  [  192/  306]
train() client id: f_00004-11-6 loss: 0.828907  [  224/  306]
train() client id: f_00004-11-7 loss: 0.802475  [  256/  306]
train() client id: f_00004-11-8 loss: 0.766720  [  288/  306]
train() client id: f_00005-0-0 loss: 0.758090  [   32/  146]
train() client id: f_00005-0-1 loss: 0.856079  [   64/  146]
train() client id: f_00005-0-2 loss: 0.566072  [   96/  146]
train() client id: f_00005-0-3 loss: 0.647393  [  128/  146]
train() client id: f_00005-1-0 loss: 0.582933  [   32/  146]
train() client id: f_00005-1-1 loss: 0.726880  [   64/  146]
train() client id: f_00005-1-2 loss: 0.571636  [   96/  146]
train() client id: f_00005-1-3 loss: 0.812138  [  128/  146]
train() client id: f_00005-2-0 loss: 0.742381  [   32/  146]
train() client id: f_00005-2-1 loss: 1.054773  [   64/  146]
train() client id: f_00005-2-2 loss: 0.460917  [   96/  146]
train() client id: f_00005-2-3 loss: 0.686132  [  128/  146]
train() client id: f_00005-3-0 loss: 0.450264  [   32/  146]
train() client id: f_00005-3-1 loss: 0.576356  [   64/  146]
train() client id: f_00005-3-2 loss: 0.615431  [   96/  146]
train() client id: f_00005-3-3 loss: 1.121326  [  128/  146]
train() client id: f_00005-4-0 loss: 0.784006  [   32/  146]
train() client id: f_00005-4-1 loss: 0.545406  [   64/  146]
train() client id: f_00005-4-2 loss: 1.077662  [   96/  146]
train() client id: f_00005-4-3 loss: 0.573577  [  128/  146]
train() client id: f_00005-5-0 loss: 0.624603  [   32/  146]
train() client id: f_00005-5-1 loss: 0.903406  [   64/  146]
train() client id: f_00005-5-2 loss: 0.909428  [   96/  146]
train() client id: f_00005-5-3 loss: 0.440512  [  128/  146]
train() client id: f_00005-6-0 loss: 0.755181  [   32/  146]
train() client id: f_00005-6-1 loss: 0.725880  [   64/  146]
train() client id: f_00005-6-2 loss: 0.825755  [   96/  146]
train() client id: f_00005-6-3 loss: 0.647675  [  128/  146]
train() client id: f_00005-7-0 loss: 0.468644  [   32/  146]
train() client id: f_00005-7-1 loss: 1.041547  [   64/  146]
train() client id: f_00005-7-2 loss: 0.747277  [   96/  146]
train() client id: f_00005-7-3 loss: 0.674468  [  128/  146]
train() client id: f_00005-8-0 loss: 0.864073  [   32/  146]
train() client id: f_00005-8-1 loss: 0.651282  [   64/  146]
train() client id: f_00005-8-2 loss: 0.719569  [   96/  146]
train() client id: f_00005-8-3 loss: 0.556491  [  128/  146]
train() client id: f_00005-9-0 loss: 0.770325  [   32/  146]
train() client id: f_00005-9-1 loss: 0.614327  [   64/  146]
train() client id: f_00005-9-2 loss: 0.482267  [   96/  146]
train() client id: f_00005-9-3 loss: 0.753365  [  128/  146]
train() client id: f_00005-10-0 loss: 0.583814  [   32/  146]
train() client id: f_00005-10-1 loss: 1.061406  [   64/  146]
train() client id: f_00005-10-2 loss: 0.750167  [   96/  146]
train() client id: f_00005-10-3 loss: 0.543471  [  128/  146]
train() client id: f_00005-11-0 loss: 0.647140  [   32/  146]
train() client id: f_00005-11-1 loss: 0.871562  [   64/  146]
train() client id: f_00005-11-2 loss: 0.505142  [   96/  146]
train() client id: f_00005-11-3 loss: 0.669264  [  128/  146]
train() client id: f_00006-0-0 loss: 0.552070  [   32/   54]
train() client id: f_00006-1-0 loss: 0.477869  [   32/   54]
train() client id: f_00006-2-0 loss: 0.497051  [   32/   54]
train() client id: f_00006-3-0 loss: 0.465157  [   32/   54]
train() client id: f_00006-4-0 loss: 0.551940  [   32/   54]
train() client id: f_00006-5-0 loss: 0.522007  [   32/   54]
train() client id: f_00006-6-0 loss: 0.522320  [   32/   54]
train() client id: f_00006-7-0 loss: 0.505160  [   32/   54]
train() client id: f_00006-8-0 loss: 0.458592  [   32/   54]
train() client id: f_00006-9-0 loss: 0.500637  [   32/   54]
train() client id: f_00006-10-0 loss: 0.562246  [   32/   54]
train() client id: f_00006-11-0 loss: 0.582933  [   32/   54]
train() client id: f_00007-0-0 loss: 0.418473  [   32/  179]
train() client id: f_00007-0-1 loss: 0.578546  [   64/  179]
train() client id: f_00007-0-2 loss: 0.465196  [   96/  179]
train() client id: f_00007-0-3 loss: 0.767344  [  128/  179]
train() client id: f_00007-0-4 loss: 0.590910  [  160/  179]
train() client id: f_00007-1-0 loss: 0.473271  [   32/  179]
train() client id: f_00007-1-1 loss: 0.445626  [   64/  179]
train() client id: f_00007-1-2 loss: 0.473054  [   96/  179]
train() client id: f_00007-1-3 loss: 0.456481  [  128/  179]
train() client id: f_00007-1-4 loss: 0.598930  [  160/  179]
train() client id: f_00007-2-0 loss: 0.389967  [   32/  179]
train() client id: f_00007-2-1 loss: 0.434934  [   64/  179]
train() client id: f_00007-2-2 loss: 0.464066  [   96/  179]
train() client id: f_00007-2-3 loss: 0.682078  [  128/  179]
train() client id: f_00007-2-4 loss: 0.565124  [  160/  179]
train() client id: f_00007-3-0 loss: 0.473260  [   32/  179]
train() client id: f_00007-3-1 loss: 0.543491  [   64/  179]
train() client id: f_00007-3-2 loss: 0.472833  [   96/  179]
train() client id: f_00007-3-3 loss: 0.660596  [  128/  179]
train() client id: f_00007-3-4 loss: 0.447926  [  160/  179]
train() client id: f_00007-4-0 loss: 0.570577  [   32/  179]
train() client id: f_00007-4-1 loss: 0.477182  [   64/  179]
train() client id: f_00007-4-2 loss: 0.389866  [   96/  179]
train() client id: f_00007-4-3 loss: 0.624795  [  128/  179]
train() client id: f_00007-4-4 loss: 0.491125  [  160/  179]
train() client id: f_00007-5-0 loss: 0.702292  [   32/  179]
train() client id: f_00007-5-1 loss: 0.556058  [   64/  179]
train() client id: f_00007-5-2 loss: 0.368777  [   96/  179]
train() client id: f_00007-5-3 loss: 0.565965  [  128/  179]
train() client id: f_00007-5-4 loss: 0.339950  [  160/  179]
train() client id: f_00007-6-0 loss: 0.559625  [   32/  179]
train() client id: f_00007-6-1 loss: 0.536988  [   64/  179]
train() client id: f_00007-6-2 loss: 0.345470  [   96/  179]
train() client id: f_00007-6-3 loss: 0.596452  [  128/  179]
train() client id: f_00007-6-4 loss: 0.483495  [  160/  179]
train() client id: f_00007-7-0 loss: 0.521875  [   32/  179]
train() client id: f_00007-7-1 loss: 0.445432  [   64/  179]
train() client id: f_00007-7-2 loss: 0.418350  [   96/  179]
train() client id: f_00007-7-3 loss: 0.618257  [  128/  179]
train() client id: f_00007-7-4 loss: 0.377328  [  160/  179]
train() client id: f_00007-8-0 loss: 0.736194  [   32/  179]
train() client id: f_00007-8-1 loss: 0.323376  [   64/  179]
train() client id: f_00007-8-2 loss: 0.424338  [   96/  179]
train() client id: f_00007-8-3 loss: 0.534587  [  128/  179]
train() client id: f_00007-8-4 loss: 0.396308  [  160/  179]
train() client id: f_00007-9-0 loss: 0.436497  [   32/  179]
train() client id: f_00007-9-1 loss: 0.658035  [   64/  179]
train() client id: f_00007-9-2 loss: 0.458207  [   96/  179]
train() client id: f_00007-9-3 loss: 0.313129  [  128/  179]
train() client id: f_00007-9-4 loss: 0.499525  [  160/  179]
train() client id: f_00007-10-0 loss: 0.421781  [   32/  179]
train() client id: f_00007-10-1 loss: 0.491141  [   64/  179]
train() client id: f_00007-10-2 loss: 0.475869  [   96/  179]
train() client id: f_00007-10-3 loss: 0.569721  [  128/  179]
train() client id: f_00007-10-4 loss: 0.421599  [  160/  179]
train() client id: f_00007-11-0 loss: 0.602691  [   32/  179]
train() client id: f_00007-11-1 loss: 0.544495  [   64/  179]
train() client id: f_00007-11-2 loss: 0.418407  [   96/  179]
train() client id: f_00007-11-3 loss: 0.503472  [  128/  179]
train() client id: f_00007-11-4 loss: 0.419563  [  160/  179]
train() client id: f_00008-0-0 loss: 0.783405  [   32/  130]
train() client id: f_00008-0-1 loss: 0.830006  [   64/  130]
train() client id: f_00008-0-2 loss: 0.691776  [   96/  130]
train() client id: f_00008-0-3 loss: 0.685798  [  128/  130]
train() client id: f_00008-1-0 loss: 0.763928  [   32/  130]
train() client id: f_00008-1-1 loss: 0.659447  [   64/  130]
train() client id: f_00008-1-2 loss: 0.664802  [   96/  130]
train() client id: f_00008-1-3 loss: 0.898005  [  128/  130]
train() client id: f_00008-2-0 loss: 0.777965  [   32/  130]
train() client id: f_00008-2-1 loss: 0.688468  [   64/  130]
train() client id: f_00008-2-2 loss: 0.697264  [   96/  130]
train() client id: f_00008-2-3 loss: 0.816349  [  128/  130]
train() client id: f_00008-3-0 loss: 0.648272  [   32/  130]
train() client id: f_00008-3-1 loss: 0.767927  [   64/  130]
train() client id: f_00008-3-2 loss: 0.763719  [   96/  130]
train() client id: f_00008-3-3 loss: 0.800069  [  128/  130]
train() client id: f_00008-4-0 loss: 0.736553  [   32/  130]
train() client id: f_00008-4-1 loss: 0.655007  [   64/  130]
train() client id: f_00008-4-2 loss: 0.746174  [   96/  130]
train() client id: f_00008-4-3 loss: 0.838598  [  128/  130]
train() client id: f_00008-5-0 loss: 0.878081  [   32/  130]
train() client id: f_00008-5-1 loss: 0.701028  [   64/  130]
train() client id: f_00008-5-2 loss: 0.664589  [   96/  130]
train() client id: f_00008-5-3 loss: 0.715254  [  128/  130]
train() client id: f_00008-6-0 loss: 0.761688  [   32/  130]
train() client id: f_00008-6-1 loss: 0.826987  [   64/  130]
train() client id: f_00008-6-2 loss: 0.606250  [   96/  130]
train() client id: f_00008-6-3 loss: 0.753006  [  128/  130]
train() client id: f_00008-7-0 loss: 0.737511  [   32/  130]
train() client id: f_00008-7-1 loss: 0.678140  [   64/  130]
train() client id: f_00008-7-2 loss: 0.649335  [   96/  130]
train() client id: f_00008-7-3 loss: 0.903729  [  128/  130]
train() client id: f_00008-8-0 loss: 0.809673  [   32/  130]
train() client id: f_00008-8-1 loss: 0.751074  [   64/  130]
train() client id: f_00008-8-2 loss: 0.746414  [   96/  130]
train() client id: f_00008-8-3 loss: 0.659407  [  128/  130]
train() client id: f_00008-9-0 loss: 0.737116  [   32/  130]
train() client id: f_00008-9-1 loss: 0.787013  [   64/  130]
train() client id: f_00008-9-2 loss: 0.683637  [   96/  130]
train() client id: f_00008-9-3 loss: 0.772787  [  128/  130]
train() client id: f_00008-10-0 loss: 0.718708  [   32/  130]
train() client id: f_00008-10-1 loss: 0.753036  [   64/  130]
train() client id: f_00008-10-2 loss: 0.773052  [   96/  130]
train() client id: f_00008-10-3 loss: 0.714706  [  128/  130]
train() client id: f_00008-11-0 loss: 0.827604  [   32/  130]
train() client id: f_00008-11-1 loss: 0.731330  [   64/  130]
train() client id: f_00008-11-2 loss: 0.772178  [   96/  130]
train() client id: f_00008-11-3 loss: 0.645813  [  128/  130]
train() client id: f_00009-0-0 loss: 0.922599  [   32/  118]
train() client id: f_00009-0-1 loss: 0.863737  [   64/  118]
train() client id: f_00009-0-2 loss: 0.944431  [   96/  118]
train() client id: f_00009-1-0 loss: 0.948165  [   32/  118]
train() client id: f_00009-1-1 loss: 0.752674  [   64/  118]
train() client id: f_00009-1-2 loss: 0.922304  [   96/  118]
train() client id: f_00009-2-0 loss: 1.061268  [   32/  118]
train() client id: f_00009-2-1 loss: 0.869273  [   64/  118]
train() client id: f_00009-2-2 loss: 0.786663  [   96/  118]
train() client id: f_00009-3-0 loss: 0.903313  [   32/  118]
train() client id: f_00009-3-1 loss: 0.860535  [   64/  118]
train() client id: f_00009-3-2 loss: 0.946054  [   96/  118]
train() client id: f_00009-4-0 loss: 0.640667  [   32/  118]
train() client id: f_00009-4-1 loss: 0.947007  [   64/  118]
train() client id: f_00009-4-2 loss: 0.872356  [   96/  118]
train() client id: f_00009-5-0 loss: 0.771764  [   32/  118]
train() client id: f_00009-5-1 loss: 0.935727  [   64/  118]
train() client id: f_00009-5-2 loss: 0.927528  [   96/  118]
train() client id: f_00009-6-0 loss: 0.897623  [   32/  118]
train() client id: f_00009-6-1 loss: 0.925326  [   64/  118]
train() client id: f_00009-6-2 loss: 0.839175  [   96/  118]
train() client id: f_00009-7-0 loss: 0.775908  [   32/  118]
train() client id: f_00009-7-1 loss: 0.874683  [   64/  118]
train() client id: f_00009-7-2 loss: 0.677411  [   96/  118]
train() client id: f_00009-8-0 loss: 0.747298  [   32/  118]
train() client id: f_00009-8-1 loss: 0.879381  [   64/  118]
train() client id: f_00009-8-2 loss: 0.840414  [   96/  118]
train() client id: f_00009-9-0 loss: 0.831436  [   32/  118]
train() client id: f_00009-9-1 loss: 1.055742  [   64/  118]
train() client id: f_00009-9-2 loss: 0.848531  [   96/  118]
train() client id: f_00009-10-0 loss: 0.854448  [   32/  118]
train() client id: f_00009-10-1 loss: 0.807087  [   64/  118]
train() client id: f_00009-10-2 loss: 0.909202  [   96/  118]
train() client id: f_00009-11-0 loss: 0.913670  [   32/  118]
train() client id: f_00009-11-1 loss: 0.841208  [   64/  118]
train() client id: f_00009-11-2 loss: 1.012734  [   96/  118]
At round 47 accuracy: 0.6445623342175066
At round 47 training accuracy: 0.5975855130784709
At round 47 training loss: 0.8204426312062487
update_location
xs = -3.905658 4.200318 255.009024 18.811294 0.979296 3.956410 -217.443192 -196.324852 239.663977 -182.060879 
ys = 247.587959 230.555839 1.320614 -217.455176 209.350187 192.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -7.711426168117436
ys mean: 68.3941425355287
dists_uav = 267.048782 251.343664 273.918503 240.084607 232.009611 217.239384 239.350020 220.327310 260.283483 207.755085 
uav_gains = -112.802252 -111.410707 -113.427282 -110.478464 -109.853462 -108.801701 -110.420058 -109.012813 -112.193340 -108.175798 
uav_gains_db_mean: -110.65758782902392
dists_bs = 185.855550 187.614512 463.758870 437.702962 179.300441 179.840876 182.625463 175.478215 443.543382 171.144235 
bs_gains = -103.103974 -103.218519 -114.223242 -113.520085 -102.667337 -102.703935 -102.890776 -102.405309 -113.681270 -102.101203 
bs_gains_db_mean: -106.05156495104242
Round 48
-------------------------------
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.66251584  9.56421031  4.59923771  1.66893827 11.02840386  5.30599829
  2.06183055  6.52085571  4.81741298  4.30225233]
obj_prev = 54.53165585566928
eta_min = 1.7212465954185627e-20	eta_max = 0.9392276891852349
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 12.610414541407936	eta = 0.909090909090909
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 25.13769025130001	eta = 0.4560487898791294
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 18.739989943316214	eta = 0.6117406281506839
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.582476098023903	eta = 0.6520135819067145
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.516919047777254	eta = 0.6544537420190021
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.516687862363067	eta = 0.6544623795057578
eta = 0.6544623795057578
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [0.03498261 0.07357454 0.03442733 0.01193851 0.08495777 0.0405354
 0.01499255 0.04969752 0.03609317 0.03276149]
ene_total = [1.63559168 2.74693988 1.64037262 0.78097472 3.12737952 1.61844234
 0.88321539 2.02882541 1.7072293  1.347717  ]
ti_comp = [0.68075038 0.73958322 0.67379048 0.70183595 0.74144644 0.74132564
 0.70228536 0.71216487 0.67077194 0.74326503]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [5.77378821e-06 4.55080801e-05 5.61746411e-06 2.15903231e-07
 6.97155574e-05 7.57470152e-06 4.27050309e-07 1.51259614e-05
 6.53138483e-06 3.97817259e-06]
ene_total = [0.4471766  0.2486291  0.47081906 0.37534488 0.24312091 0.24142
 0.37382507 0.34075668 0.48110627 0.23470829]
optimize_network iter = 0 obj = 3.4569068484449996
eta = 0.6544623795057578
freqs = [25694153.73552614 49740538.88381484 25547504.06148466  8505199.59149158
 57291910.54818947 27339810.52250793 10674113.26419133 34891861.85896511
 26904202.71882671 22038902.98735746]
eta_min = 0.6544623795057585	eta_max = 0.691983191142256
af = 0.004443917503062845	bf = 1.1926074070739463	zeta = 0.00488830925336913	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [1.29805932e-06 1.02310970e-05 1.26291463e-06 4.85392243e-08
 1.56734064e-05 1.70293948e-06 9.60091734e-08 3.40060882e-06
 1.46838170e-06 8.94370180e-07]
ene_total = [1.6991017  0.94046194 1.78898032 1.42663193 0.91710237 0.91685829
 1.42083414 1.2936725  1.8279896  0.89170773]
ti_comp = [0.59254498 0.65137783 0.58558508 0.61363055 0.65324104 0.65312024
 0.61407996 0.62395948 0.58256655 0.65505963]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.99236055e-06 3.84333847e-05 4.87216508e-06 1.85024130e-07
 5.88375125e-05 6.39306671e-06 3.65905026e-07 1.29087227e-05
 5.67251856e-06 3.35522234e-06]
ene_total = [0.50161926 0.27864598 0.52814309 0.42106603 0.27232185 0.27078333
 0.41935993 0.38218116 0.53967909 0.26327534]
optimize_network iter = 1 obj = 3.877075061802886
eta = 0.691983191142256
freqs = [25637255.917499   49049571.69376329 25530186.95114224  8448581.49608057
 56476825.65917366 26951443.47001215 10602081.72934842 34587483.18659778
 26904202.71882672 21718183.03696004]
eta_min = 0.6919831911422635	eta_max = 0.6919831911422551
af = 0.004337069151020109	bf = 1.1926074070739463	zeta = 0.004770776066122121	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [1.29231677e-06 9.94882221e-06 1.26120311e-06 4.78951357e-08
 1.52306115e-05 1.65490197e-06 9.47177586e-08 3.34153728e-06
 1.46838170e-06 8.68529037e-07]
ene_total = [1.69910096 0.94042549 1.7889801  1.42663185 0.91704519 0.91685209
 1.42083397 1.29366487 1.8279896  0.8917044 ]
ti_comp = [0.59254498 0.65137783 0.58558508 0.61363055 0.65324104 0.65312024
 0.61407996 0.62395948 0.58256655 0.65505963]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.99236055e-06 3.84333847e-05 4.87216508e-06 1.85024130e-07
 5.88375125e-05 6.39306671e-06 3.65905026e-07 1.29087227e-05
 5.67251856e-06 3.35522234e-06]
ene_total = [0.50161926 0.27864598 0.52814309 0.42106603 0.27232185 0.27078333
 0.41935993 0.38218116 0.53967909 0.26327534]
optimize_network iter = 2 obj = 3.8770750618028753
eta = 0.6919831911422551
freqs = [25637255.91749901 49049571.69376332 25530186.95114225  8448581.49608057
 56476825.6591737  26951443.47001216 10602081.72934843 34587483.18659779
 26904202.71882672 21718183.03696005]
Done!
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.96895282e-06 3.82531817e-05 4.84932092e-06 1.84156606e-07
 5.85616405e-05 6.36309147e-06 3.64189404e-07 1.28481975e-05
 5.64592177e-06 3.33949068e-06]
ene_total = [0.01316027 0.00731027 0.01385614 0.01104693 0.00714426 0.00710414
 0.01100217 0.0100267  0.01415879 0.00690718]
At round 48 energy consumption: 0.10171685606737808
At round 48 eta: 0.6919831911422551
At round 48 a_n: 11.740402201727147
At round 48 local rounds: 12.056529567513225
At round 48 global rounds: 38.11610880998822
gradient difference: 0.4510883390903473
train() client id: f_00000-0-0 loss: 1.032274  [   32/  126]
train() client id: f_00000-0-1 loss: 1.032517  [   64/  126]
train() client id: f_00000-0-2 loss: 0.981050  [   96/  126]
train() client id: f_00000-1-0 loss: 0.950885  [   32/  126]
train() client id: f_00000-1-1 loss: 0.869636  [   64/  126]
train() client id: f_00000-1-2 loss: 0.901625  [   96/  126]
train() client id: f_00000-2-0 loss: 0.725622  [   32/  126]
train() client id: f_00000-2-1 loss: 0.948806  [   64/  126]
train() client id: f_00000-2-2 loss: 0.852212  [   96/  126]
train() client id: f_00000-3-0 loss: 0.920081  [   32/  126]
train() client id: f_00000-3-1 loss: 0.819666  [   64/  126]
train() client id: f_00000-3-2 loss: 0.803123  [   96/  126]
train() client id: f_00000-4-0 loss: 0.788167  [   32/  126]
train() client id: f_00000-4-1 loss: 0.871860  [   64/  126]
train() client id: f_00000-4-2 loss: 0.828442  [   96/  126]
train() client id: f_00000-5-0 loss: 0.736578  [   32/  126]
train() client id: f_00000-5-1 loss: 0.890599  [   64/  126]
train() client id: f_00000-5-2 loss: 0.897615  [   96/  126]
train() client id: f_00000-6-0 loss: 0.830360  [   32/  126]
train() client id: f_00000-6-1 loss: 0.750183  [   64/  126]
train() client id: f_00000-6-2 loss: 0.816460  [   96/  126]
train() client id: f_00000-7-0 loss: 0.805024  [   32/  126]
train() client id: f_00000-7-1 loss: 0.994084  [   64/  126]
train() client id: f_00000-7-2 loss: 0.767908  [   96/  126]
train() client id: f_00000-8-0 loss: 0.876620  [   32/  126]
train() client id: f_00000-8-1 loss: 0.872824  [   64/  126]
train() client id: f_00000-8-2 loss: 0.788315  [   96/  126]
train() client id: f_00000-9-0 loss: 0.871165  [   32/  126]
train() client id: f_00000-9-1 loss: 0.799730  [   64/  126]
train() client id: f_00000-9-2 loss: 0.839980  [   96/  126]
train() client id: f_00000-10-0 loss: 0.688355  [   32/  126]
train() client id: f_00000-10-1 loss: 0.798988  [   64/  126]
train() client id: f_00000-10-2 loss: 0.986163  [   96/  126]
train() client id: f_00000-11-0 loss: 0.755838  [   32/  126]
train() client id: f_00000-11-1 loss: 0.835304  [   64/  126]
train() client id: f_00000-11-2 loss: 0.771576  [   96/  126]
train() client id: f_00001-0-0 loss: 0.381334  [   32/  265]
train() client id: f_00001-0-1 loss: 0.353045  [   64/  265]
train() client id: f_00001-0-2 loss: 0.465593  [   96/  265]
train() client id: f_00001-0-3 loss: 0.390290  [  128/  265]
train() client id: f_00001-0-4 loss: 0.424663  [  160/  265]
train() client id: f_00001-0-5 loss: 0.610805  [  192/  265]
train() client id: f_00001-0-6 loss: 0.532483  [  224/  265]
train() client id: f_00001-0-7 loss: 0.385121  [  256/  265]
train() client id: f_00001-1-0 loss: 0.444769  [   32/  265]
train() client id: f_00001-1-1 loss: 0.514766  [   64/  265]
train() client id: f_00001-1-2 loss: 0.408938  [   96/  265]
train() client id: f_00001-1-3 loss: 0.443410  [  128/  265]
train() client id: f_00001-1-4 loss: 0.396171  [  160/  265]
train() client id: f_00001-1-5 loss: 0.503272  [  192/  265]
train() client id: f_00001-1-6 loss: 0.425753  [  224/  265]
train() client id: f_00001-1-7 loss: 0.347586  [  256/  265]
train() client id: f_00001-2-0 loss: 0.384215  [   32/  265]
train() client id: f_00001-2-1 loss: 0.331075  [   64/  265]
train() client id: f_00001-2-2 loss: 0.557816  [   96/  265]
train() client id: f_00001-2-3 loss: 0.502045  [  128/  265]
train() client id: f_00001-2-4 loss: 0.393297  [  160/  265]
train() client id: f_00001-2-5 loss: 0.417274  [  192/  265]
train() client id: f_00001-2-6 loss: 0.464044  [  224/  265]
train() client id: f_00001-2-7 loss: 0.400823  [  256/  265]
train() client id: f_00001-3-0 loss: 0.349602  [   32/  265]
train() client id: f_00001-3-1 loss: 0.379788  [   64/  265]
train() client id: f_00001-3-2 loss: 0.493142  [   96/  265]
train() client id: f_00001-3-3 loss: 0.480940  [  128/  265]
train() client id: f_00001-3-4 loss: 0.344754  [  160/  265]
train() client id: f_00001-3-5 loss: 0.580647  [  192/  265]
train() client id: f_00001-3-6 loss: 0.319323  [  224/  265]
train() client id: f_00001-3-7 loss: 0.440048  [  256/  265]
train() client id: f_00001-4-0 loss: 0.406150  [   32/  265]
train() client id: f_00001-4-1 loss: 0.400747  [   64/  265]
train() client id: f_00001-4-2 loss: 0.370839  [   96/  265]
train() client id: f_00001-4-3 loss: 0.415790  [  128/  265]
train() client id: f_00001-4-4 loss: 0.477306  [  160/  265]
train() client id: f_00001-4-5 loss: 0.427041  [  192/  265]
train() client id: f_00001-4-6 loss: 0.457117  [  224/  265]
train() client id: f_00001-4-7 loss: 0.418136  [  256/  265]
train() client id: f_00001-5-0 loss: 0.360047  [   32/  265]
train() client id: f_00001-5-1 loss: 0.421250  [   64/  265]
train() client id: f_00001-5-2 loss: 0.365142  [   96/  265]
train() client id: f_00001-5-3 loss: 0.497027  [  128/  265]
train() client id: f_00001-5-4 loss: 0.401619  [  160/  265]
train() client id: f_00001-5-5 loss: 0.378793  [  192/  265]
train() client id: f_00001-5-6 loss: 0.338558  [  224/  265]
train() client id: f_00001-5-7 loss: 0.543810  [  256/  265]
train() client id: f_00001-6-0 loss: 0.324409  [   32/  265]
train() client id: f_00001-6-1 loss: 0.383737  [   64/  265]
train() client id: f_00001-6-2 loss: 0.332332  [   96/  265]
train() client id: f_00001-6-3 loss: 0.519655  [  128/  265]
train() client id: f_00001-6-4 loss: 0.451445  [  160/  265]
train() client id: f_00001-6-5 loss: 0.465790  [  192/  265]
train() client id: f_00001-6-6 loss: 0.466070  [  224/  265]
train() client id: f_00001-6-7 loss: 0.400351  [  256/  265]
train() client id: f_00001-7-0 loss: 0.324712  [   32/  265]
train() client id: f_00001-7-1 loss: 0.345174  [   64/  265]
train() client id: f_00001-7-2 loss: 0.511121  [   96/  265]
train() client id: f_00001-7-3 loss: 0.442426  [  128/  265]
train() client id: f_00001-7-4 loss: 0.425258  [  160/  265]
train() client id: f_00001-7-5 loss: 0.402989  [  192/  265]
train() client id: f_00001-7-6 loss: 0.419509  [  224/  265]
train() client id: f_00001-7-7 loss: 0.452181  [  256/  265]
train() client id: f_00001-8-0 loss: 0.319765  [   32/  265]
train() client id: f_00001-8-1 loss: 0.458556  [   64/  265]
train() client id: f_00001-8-2 loss: 0.417525  [   96/  265]
train() client id: f_00001-8-3 loss: 0.464872  [  128/  265]
train() client id: f_00001-8-4 loss: 0.340144  [  160/  265]
train() client id: f_00001-8-5 loss: 0.387027  [  192/  265]
train() client id: f_00001-8-6 loss: 0.389080  [  224/  265]
train() client id: f_00001-8-7 loss: 0.496225  [  256/  265]
train() client id: f_00001-9-0 loss: 0.392072  [   32/  265]
train() client id: f_00001-9-1 loss: 0.439905  [   64/  265]
train() client id: f_00001-9-2 loss: 0.400144  [   96/  265]
train() client id: f_00001-9-3 loss: 0.511327  [  128/  265]
train() client id: f_00001-9-4 loss: 0.331033  [  160/  265]
train() client id: f_00001-9-5 loss: 0.447382  [  192/  265]
train() client id: f_00001-9-6 loss: 0.340564  [  224/  265]
train() client id: f_00001-9-7 loss: 0.437860  [  256/  265]
train() client id: f_00001-10-0 loss: 0.361715  [   32/  265]
train() client id: f_00001-10-1 loss: 0.526868  [   64/  265]
train() client id: f_00001-10-2 loss: 0.314462  [   96/  265]
train() client id: f_00001-10-3 loss: 0.310835  [  128/  265]
train() client id: f_00001-10-4 loss: 0.600482  [  160/  265]
train() client id: f_00001-10-5 loss: 0.397438  [  192/  265]
train() client id: f_00001-10-6 loss: 0.378433  [  224/  265]
train() client id: f_00001-10-7 loss: 0.380610  [  256/  265]
train() client id: f_00001-11-0 loss: 0.399960  [   32/  265]
train() client id: f_00001-11-1 loss: 0.498834  [   64/  265]
train() client id: f_00001-11-2 loss: 0.370979  [   96/  265]
train() client id: f_00001-11-3 loss: 0.333106  [  128/  265]
train() client id: f_00001-11-4 loss: 0.443163  [  160/  265]
train() client id: f_00001-11-5 loss: 0.470025  [  192/  265]
train() client id: f_00001-11-6 loss: 0.333888  [  224/  265]
train() client id: f_00001-11-7 loss: 0.448179  [  256/  265]
train() client id: f_00002-0-0 loss: 1.212369  [   32/  124]
train() client id: f_00002-0-1 loss: 1.061930  [   64/  124]
train() client id: f_00002-0-2 loss: 1.151785  [   96/  124]
train() client id: f_00002-1-0 loss: 1.067710  [   32/  124]
train() client id: f_00002-1-1 loss: 1.154294  [   64/  124]
train() client id: f_00002-1-2 loss: 1.137318  [   96/  124]
train() client id: f_00002-2-0 loss: 1.043278  [   32/  124]
train() client id: f_00002-2-1 loss: 1.056699  [   64/  124]
train() client id: f_00002-2-2 loss: 1.167880  [   96/  124]
train() client id: f_00002-3-0 loss: 0.998998  [   32/  124]
train() client id: f_00002-3-1 loss: 1.062258  [   64/  124]
train() client id: f_00002-3-2 loss: 1.072095  [   96/  124]
train() client id: f_00002-4-0 loss: 1.003271  [   32/  124]
train() client id: f_00002-4-1 loss: 0.911780  [   64/  124]
train() client id: f_00002-4-2 loss: 1.024961  [   96/  124]
train() client id: f_00002-5-0 loss: 1.093938  [   32/  124]
train() client id: f_00002-5-1 loss: 0.940854  [   64/  124]
train() client id: f_00002-5-2 loss: 0.849171  [   96/  124]
train() client id: f_00002-6-0 loss: 0.859601  [   32/  124]
train() client id: f_00002-6-1 loss: 0.893166  [   64/  124]
train() client id: f_00002-6-2 loss: 1.034036  [   96/  124]
train() client id: f_00002-7-0 loss: 0.959777  [   32/  124]
train() client id: f_00002-7-1 loss: 0.926736  [   64/  124]
train() client id: f_00002-7-2 loss: 0.959643  [   96/  124]
train() client id: f_00002-8-0 loss: 0.946300  [   32/  124]
train() client id: f_00002-8-1 loss: 0.794180  [   64/  124]
train() client id: f_00002-8-2 loss: 1.144654  [   96/  124]
train() client id: f_00002-9-0 loss: 1.039480  [   32/  124]
train() client id: f_00002-9-1 loss: 0.877402  [   64/  124]
train() client id: f_00002-9-2 loss: 0.855635  [   96/  124]
train() client id: f_00002-10-0 loss: 1.203381  [   32/  124]
train() client id: f_00002-10-1 loss: 0.892583  [   64/  124]
train() client id: f_00002-10-2 loss: 0.735407  [   96/  124]
train() client id: f_00002-11-0 loss: 0.937077  [   32/  124]
train() client id: f_00002-11-1 loss: 0.928460  [   64/  124]
train() client id: f_00002-11-2 loss: 0.982303  [   96/  124]
train() client id: f_00003-0-0 loss: 0.816507  [   32/   43]
train() client id: f_00003-1-0 loss: 0.854932  [   32/   43]
train() client id: f_00003-2-0 loss: 0.919355  [   32/   43]
train() client id: f_00003-3-0 loss: 0.680838  [   32/   43]
train() client id: f_00003-4-0 loss: 1.089911  [   32/   43]
train() client id: f_00003-5-0 loss: 1.102379  [   32/   43]
train() client id: f_00003-6-0 loss: 0.768379  [   32/   43]
train() client id: f_00003-7-0 loss: 0.948219  [   32/   43]
train() client id: f_00003-8-0 loss: 0.829958  [   32/   43]
train() client id: f_00003-9-0 loss: 0.833319  [   32/   43]
train() client id: f_00003-10-0 loss: 0.752691  [   32/   43]
train() client id: f_00003-11-0 loss: 0.798877  [   32/   43]
train() client id: f_00004-0-0 loss: 0.885955  [   32/  306]
train() client id: f_00004-0-1 loss: 0.950422  [   64/  306]
train() client id: f_00004-0-2 loss: 0.942596  [   96/  306]
train() client id: f_00004-0-3 loss: 0.922717  [  128/  306]
train() client id: f_00004-0-4 loss: 0.940177  [  160/  306]
train() client id: f_00004-0-5 loss: 0.805421  [  192/  306]
train() client id: f_00004-0-6 loss: 1.035270  [  224/  306]
train() client id: f_00004-0-7 loss: 0.848012  [  256/  306]
train() client id: f_00004-0-8 loss: 0.876861  [  288/  306]
train() client id: f_00004-1-0 loss: 0.907904  [   32/  306]
train() client id: f_00004-1-1 loss: 0.873749  [   64/  306]
train() client id: f_00004-1-2 loss: 1.015471  [   96/  306]
train() client id: f_00004-1-3 loss: 0.830271  [  128/  306]
train() client id: f_00004-1-4 loss: 0.887300  [  160/  306]
train() client id: f_00004-1-5 loss: 0.945805  [  192/  306]
train() client id: f_00004-1-6 loss: 0.811331  [  224/  306]
train() client id: f_00004-1-7 loss: 1.120131  [  256/  306]
train() client id: f_00004-1-8 loss: 0.925446  [  288/  306]
train() client id: f_00004-2-0 loss: 0.874646  [   32/  306]
train() client id: f_00004-2-1 loss: 0.902454  [   64/  306]
train() client id: f_00004-2-2 loss: 1.081629  [   96/  306]
train() client id: f_00004-2-3 loss: 0.893695  [  128/  306]
train() client id: f_00004-2-4 loss: 0.968499  [  160/  306]
train() client id: f_00004-2-5 loss: 0.877192  [  192/  306]
train() client id: f_00004-2-6 loss: 0.870914  [  224/  306]
train() client id: f_00004-2-7 loss: 0.850193  [  256/  306]
train() client id: f_00004-2-8 loss: 0.880792  [  288/  306]
train() client id: f_00004-3-0 loss: 0.810441  [   32/  306]
train() client id: f_00004-3-1 loss: 0.962402  [   64/  306]
train() client id: f_00004-3-2 loss: 0.951281  [   96/  306]
train() client id: f_00004-3-3 loss: 0.861848  [  128/  306]
train() client id: f_00004-3-4 loss: 0.943951  [  160/  306]
train() client id: f_00004-3-5 loss: 0.966464  [  192/  306]
train() client id: f_00004-3-6 loss: 0.898083  [  224/  306]
train() client id: f_00004-3-7 loss: 0.826288  [  256/  306]
train() client id: f_00004-3-8 loss: 0.887515  [  288/  306]
train() client id: f_00004-4-0 loss: 0.836364  [   32/  306]
train() client id: f_00004-4-1 loss: 0.835923  [   64/  306]
train() client id: f_00004-4-2 loss: 1.014087  [   96/  306]
train() client id: f_00004-4-3 loss: 0.861906  [  128/  306]
train() client id: f_00004-4-4 loss: 0.889605  [  160/  306]
train() client id: f_00004-4-5 loss: 0.947100  [  192/  306]
train() client id: f_00004-4-6 loss: 1.014946  [  224/  306]
train() client id: f_00004-4-7 loss: 0.933018  [  256/  306]
train() client id: f_00004-4-8 loss: 0.755162  [  288/  306]
train() client id: f_00004-5-0 loss: 0.898789  [   32/  306]
train() client id: f_00004-5-1 loss: 0.924763  [   64/  306]
train() client id: f_00004-5-2 loss: 0.988938  [   96/  306]
train() client id: f_00004-5-3 loss: 0.829050  [  128/  306]
train() client id: f_00004-5-4 loss: 0.943065  [  160/  306]
train() client id: f_00004-5-5 loss: 0.789833  [  192/  306]
train() client id: f_00004-5-6 loss: 0.937730  [  224/  306]
train() client id: f_00004-5-7 loss: 0.923866  [  256/  306]
train() client id: f_00004-5-8 loss: 0.871777  [  288/  306]
train() client id: f_00004-6-0 loss: 0.748790  [   32/  306]
train() client id: f_00004-6-1 loss: 1.061302  [   64/  306]
train() client id: f_00004-6-2 loss: 0.920300  [   96/  306]
train() client id: f_00004-6-3 loss: 0.873186  [  128/  306]
train() client id: f_00004-6-4 loss: 0.876141  [  160/  306]
train() client id: f_00004-6-5 loss: 0.966660  [  192/  306]
train() client id: f_00004-6-6 loss: 0.806657  [  224/  306]
train() client id: f_00004-6-7 loss: 0.831867  [  256/  306]
train() client id: f_00004-6-8 loss: 0.954623  [  288/  306]
train() client id: f_00004-7-0 loss: 0.799574  [   32/  306]
train() client id: f_00004-7-1 loss: 0.829781  [   64/  306]
train() client id: f_00004-7-2 loss: 0.985111  [   96/  306]
train() client id: f_00004-7-3 loss: 0.741161  [  128/  306]
train() client id: f_00004-7-4 loss: 1.038103  [  160/  306]
train() client id: f_00004-7-5 loss: 0.936666  [  192/  306]
train() client id: f_00004-7-6 loss: 0.808126  [  224/  306]
train() client id: f_00004-7-7 loss: 0.922611  [  256/  306]
train() client id: f_00004-7-8 loss: 0.935533  [  288/  306]
train() client id: f_00004-8-0 loss: 0.962932  [   32/  306]
train() client id: f_00004-8-1 loss: 0.849287  [   64/  306]
train() client id: f_00004-8-2 loss: 1.000435  [   96/  306]
train() client id: f_00004-8-3 loss: 0.874741  [  128/  306]
train() client id: f_00004-8-4 loss: 0.762222  [  160/  306]
train() client id: f_00004-8-5 loss: 0.896519  [  192/  306]
train() client id: f_00004-8-6 loss: 0.794981  [  224/  306]
train() client id: f_00004-8-7 loss: 0.825629  [  256/  306]
train() client id: f_00004-8-8 loss: 0.934860  [  288/  306]
train() client id: f_00004-9-0 loss: 0.770150  [   32/  306]
train() client id: f_00004-9-1 loss: 0.982635  [   64/  306]
train() client id: f_00004-9-2 loss: 0.898784  [   96/  306]
train() client id: f_00004-9-3 loss: 0.894478  [  128/  306]
train() client id: f_00004-9-4 loss: 0.953020  [  160/  306]
train() client id: f_00004-9-5 loss: 0.862051  [  192/  306]
train() client id: f_00004-9-6 loss: 0.810899  [  224/  306]
train() client id: f_00004-9-7 loss: 0.886380  [  256/  306]
train() client id: f_00004-9-8 loss: 0.843058  [  288/  306]
train() client id: f_00004-10-0 loss: 0.842917  [   32/  306]
train() client id: f_00004-10-1 loss: 0.825116  [   64/  306]
train() client id: f_00004-10-2 loss: 0.957762  [   96/  306]
train() client id: f_00004-10-3 loss: 0.987080  [  128/  306]
train() client id: f_00004-10-4 loss: 0.892160  [  160/  306]
train() client id: f_00004-10-5 loss: 0.821837  [  192/  306]
train() client id: f_00004-10-6 loss: 0.761420  [  224/  306]
train() client id: f_00004-10-7 loss: 0.898271  [  256/  306]
train() client id: f_00004-10-8 loss: 0.996182  [  288/  306]
train() client id: f_00004-11-0 loss: 0.860076  [   32/  306]
train() client id: f_00004-11-1 loss: 0.711804  [   64/  306]
train() client id: f_00004-11-2 loss: 0.914806  [   96/  306]
train() client id: f_00004-11-3 loss: 0.876648  [  128/  306]
train() client id: f_00004-11-4 loss: 0.906912  [  160/  306]
train() client id: f_00004-11-5 loss: 0.960472  [  192/  306]
train() client id: f_00004-11-6 loss: 0.941749  [  224/  306]
train() client id: f_00004-11-7 loss: 0.797908  [  256/  306]
train() client id: f_00004-11-8 loss: 0.854921  [  288/  306]
train() client id: f_00005-0-0 loss: 0.528767  [   32/  146]
train() client id: f_00005-0-1 loss: 0.604215  [   64/  146]
train() client id: f_00005-0-2 loss: 0.802554  [   96/  146]
train() client id: f_00005-0-3 loss: 0.896866  [  128/  146]
train() client id: f_00005-1-0 loss: 0.902585  [   32/  146]
train() client id: f_00005-1-1 loss: 0.516000  [   64/  146]
train() client id: f_00005-1-2 loss: 0.851267  [   96/  146]
train() client id: f_00005-1-3 loss: 0.453647  [  128/  146]
train() client id: f_00005-2-0 loss: 0.641278  [   32/  146]
train() client id: f_00005-2-1 loss: 0.760456  [   64/  146]
train() client id: f_00005-2-2 loss: 0.608140  [   96/  146]
train() client id: f_00005-2-3 loss: 0.920722  [  128/  146]
train() client id: f_00005-3-0 loss: 0.745888  [   32/  146]
train() client id: f_00005-3-1 loss: 0.762811  [   64/  146]
train() client id: f_00005-3-2 loss: 0.653033  [   96/  146]
train() client id: f_00005-3-3 loss: 0.676406  [  128/  146]
train() client id: f_00005-4-0 loss: 0.729327  [   32/  146]
train() client id: f_00005-4-1 loss: 0.904133  [   64/  146]
train() client id: f_00005-4-2 loss: 0.742326  [   96/  146]
train() client id: f_00005-4-3 loss: 0.629041  [  128/  146]
train() client id: f_00005-5-0 loss: 0.762310  [   32/  146]
train() client id: f_00005-5-1 loss: 0.924018  [   64/  146]
train() client id: f_00005-5-2 loss: 0.770103  [   96/  146]
train() client id: f_00005-5-3 loss: 0.564592  [  128/  146]
train() client id: f_00005-6-0 loss: 0.510779  [   32/  146]
train() client id: f_00005-6-1 loss: 0.702033  [   64/  146]
train() client id: f_00005-6-2 loss: 0.666553  [   96/  146]
train() client id: f_00005-6-3 loss: 0.780248  [  128/  146]
train() client id: f_00005-7-0 loss: 0.783424  [   32/  146]
train() client id: f_00005-7-1 loss: 0.643223  [   64/  146]
train() client id: f_00005-7-2 loss: 0.475892  [   96/  146]
train() client id: f_00005-7-3 loss: 0.901820  [  128/  146]
train() client id: f_00005-8-0 loss: 0.784455  [   32/  146]
train() client id: f_00005-8-1 loss: 0.514260  [   64/  146]
train() client id: f_00005-8-2 loss: 0.835167  [   96/  146]
train() client id: f_00005-8-3 loss: 0.634054  [  128/  146]
train() client id: f_00005-9-0 loss: 0.621168  [   32/  146]
train() client id: f_00005-9-1 loss: 0.578319  [   64/  146]
train() client id: f_00005-9-2 loss: 0.876682  [   96/  146]
train() client id: f_00005-9-3 loss: 0.917029  [  128/  146]
train() client id: f_00005-10-0 loss: 0.627357  [   32/  146]
train() client id: f_00005-10-1 loss: 0.530926  [   64/  146]
train() client id: f_00005-10-2 loss: 0.637670  [   96/  146]
train() client id: f_00005-10-3 loss: 1.161356  [  128/  146]
train() client id: f_00005-11-0 loss: 0.606528  [   32/  146]
train() client id: f_00005-11-1 loss: 0.901429  [   64/  146]
train() client id: f_00005-11-2 loss: 0.482232  [   96/  146]
train() client id: f_00005-11-3 loss: 0.845465  [  128/  146]
train() client id: f_00006-0-0 loss: 0.447149  [   32/   54]
train() client id: f_00006-1-0 loss: 0.491409  [   32/   54]
train() client id: f_00006-2-0 loss: 0.433426  [   32/   54]
train() client id: f_00006-3-0 loss: 0.402756  [   32/   54]
train() client id: f_00006-4-0 loss: 0.433516  [   32/   54]
train() client id: f_00006-5-0 loss: 0.460365  [   32/   54]
train() client id: f_00006-6-0 loss: 0.385685  [   32/   54]
train() client id: f_00006-7-0 loss: 0.484804  [   32/   54]
train() client id: f_00006-8-0 loss: 0.501401  [   32/   54]
train() client id: f_00006-9-0 loss: 0.388071  [   32/   54]
train() client id: f_00006-10-0 loss: 0.485896  [   32/   54]
train() client id: f_00006-11-0 loss: 0.498687  [   32/   54]
train() client id: f_00007-0-0 loss: 0.522296  [   32/  179]
train() client id: f_00007-0-1 loss: 0.850935  [   64/  179]
train() client id: f_00007-0-2 loss: 0.912112  [   96/  179]
train() client id: f_00007-0-3 loss: 0.494586  [  128/  179]
train() client id: f_00007-0-4 loss: 0.541542  [  160/  179]
train() client id: f_00007-1-0 loss: 0.641432  [   32/  179]
train() client id: f_00007-1-1 loss: 0.639370  [   64/  179]
train() client id: f_00007-1-2 loss: 0.523261  [   96/  179]
train() client id: f_00007-1-3 loss: 0.718867  [  128/  179]
train() client id: f_00007-1-4 loss: 0.576465  [  160/  179]
train() client id: f_00007-2-0 loss: 0.496679  [   32/  179]
train() client id: f_00007-2-1 loss: 0.578377  [   64/  179]
train() client id: f_00007-2-2 loss: 0.629158  [   96/  179]
train() client id: f_00007-2-3 loss: 0.538921  [  128/  179]
train() client id: f_00007-2-4 loss: 0.651323  [  160/  179]
train() client id: f_00007-3-0 loss: 0.916261  [   32/  179]
train() client id: f_00007-3-1 loss: 0.474899  [   64/  179]
train() client id: f_00007-3-2 loss: 0.727815  [   96/  179]
train() client id: f_00007-3-3 loss: 0.525416  [  128/  179]
train() client id: f_00007-3-4 loss: 0.545248  [  160/  179]
train() client id: f_00007-4-0 loss: 0.569267  [   32/  179]
train() client id: f_00007-4-1 loss: 0.755229  [   64/  179]
train() client id: f_00007-4-2 loss: 0.447265  [   96/  179]
train() client id: f_00007-4-3 loss: 0.784656  [  128/  179]
train() client id: f_00007-4-4 loss: 0.654635  [  160/  179]
train() client id: f_00007-5-0 loss: 0.509399  [   32/  179]
train() client id: f_00007-5-1 loss: 0.519167  [   64/  179]
train() client id: f_00007-5-2 loss: 0.574561  [   96/  179]
train() client id: f_00007-5-3 loss: 0.641339  [  128/  179]
train() client id: f_00007-5-4 loss: 0.824216  [  160/  179]
train() client id: f_00007-6-0 loss: 0.620220  [   32/  179]
train() client id: f_00007-6-1 loss: 0.471512  [   64/  179]
train() client id: f_00007-6-2 loss: 0.451108  [   96/  179]
train() client id: f_00007-6-3 loss: 1.007183  [  128/  179]
train() client id: f_00007-6-4 loss: 0.608904  [  160/  179]
train() client id: f_00007-7-0 loss: 0.497456  [   32/  179]
train() client id: f_00007-7-1 loss: 0.655558  [   64/  179]
train() client id: f_00007-7-2 loss: 0.510768  [   96/  179]
train() client id: f_00007-7-3 loss: 0.839685  [  128/  179]
train() client id: f_00007-7-4 loss: 0.443405  [  160/  179]
train() client id: f_00007-8-0 loss: 0.610540  [   32/  179]
train() client id: f_00007-8-1 loss: 0.484498  [   64/  179]
train() client id: f_00007-8-2 loss: 0.733758  [   96/  179]
train() client id: f_00007-8-3 loss: 0.468714  [  128/  179]
train() client id: f_00007-8-4 loss: 0.659156  [  160/  179]
train() client id: f_00007-9-0 loss: 0.563704  [   32/  179]
train() client id: f_00007-9-1 loss: 0.899728  [   64/  179]
train() client id: f_00007-9-2 loss: 0.556863  [   96/  179]
train() client id: f_00007-9-3 loss: 0.479679  [  128/  179]
train() client id: f_00007-9-4 loss: 0.522282  [  160/  179]
train() client id: f_00007-10-0 loss: 0.470823  [   32/  179]
train() client id: f_00007-10-1 loss: 0.804122  [   64/  179]
train() client id: f_00007-10-2 loss: 0.686409  [   96/  179]
train() client id: f_00007-10-3 loss: 0.626058  [  128/  179]
train() client id: f_00007-10-4 loss: 0.560808  [  160/  179]
train() client id: f_00007-11-0 loss: 0.633721  [   32/  179]
train() client id: f_00007-11-1 loss: 0.524163  [   64/  179]
train() client id: f_00007-11-2 loss: 0.723089  [   96/  179]
train() client id: f_00007-11-3 loss: 0.562848  [  128/  179]
train() client id: f_00007-11-4 loss: 0.580988  [  160/  179]
train() client id: f_00008-0-0 loss: 0.746402  [   32/  130]
train() client id: f_00008-0-1 loss: 0.781531  [   64/  130]
train() client id: f_00008-0-2 loss: 0.876139  [   96/  130]
train() client id: f_00008-0-3 loss: 0.803751  [  128/  130]
train() client id: f_00008-1-0 loss: 0.926905  [   32/  130]
train() client id: f_00008-1-1 loss: 0.654719  [   64/  130]
train() client id: f_00008-1-2 loss: 0.797157  [   96/  130]
train() client id: f_00008-1-3 loss: 0.802802  [  128/  130]
train() client id: f_00008-2-0 loss: 0.791421  [   32/  130]
train() client id: f_00008-2-1 loss: 0.719120  [   64/  130]
train() client id: f_00008-2-2 loss: 0.881332  [   96/  130]
train() client id: f_00008-2-3 loss: 0.794085  [  128/  130]
train() client id: f_00008-3-0 loss: 0.909956  [   32/  130]
train() client id: f_00008-3-1 loss: 0.812883  [   64/  130]
train() client id: f_00008-3-2 loss: 0.727522  [   96/  130]
train() client id: f_00008-3-3 loss: 0.740499  [  128/  130]
train() client id: f_00008-4-0 loss: 0.742343  [   32/  130]
train() client id: f_00008-4-1 loss: 0.934807  [   64/  130]
train() client id: f_00008-4-2 loss: 0.779998  [   96/  130]
train() client id: f_00008-4-3 loss: 0.717408  [  128/  130]
train() client id: f_00008-5-0 loss: 0.730316  [   32/  130]
train() client id: f_00008-5-1 loss: 0.783754  [   64/  130]
train() client id: f_00008-5-2 loss: 0.839497  [   96/  130]
train() client id: f_00008-5-3 loss: 0.832920  [  128/  130]
train() client id: f_00008-6-0 loss: 0.784173  [   32/  130]
train() client id: f_00008-6-1 loss: 0.791739  [   64/  130]
train() client id: f_00008-6-2 loss: 0.863339  [   96/  130]
train() client id: f_00008-6-3 loss: 0.750532  [  128/  130]
train() client id: f_00008-7-0 loss: 0.902374  [   32/  130]
train() client id: f_00008-7-1 loss: 0.782371  [   64/  130]
train() client id: f_00008-7-2 loss: 0.819457  [   96/  130]
train() client id: f_00008-7-3 loss: 0.670041  [  128/  130]
train() client id: f_00008-8-0 loss: 0.774892  [   32/  130]
train() client id: f_00008-8-1 loss: 0.785459  [   64/  130]
train() client id: f_00008-8-2 loss: 0.795414  [   96/  130]
train() client id: f_00008-8-3 loss: 0.832848  [  128/  130]
train() client id: f_00008-9-0 loss: 0.765622  [   32/  130]
train() client id: f_00008-9-1 loss: 0.810768  [   64/  130]
train() client id: f_00008-9-2 loss: 0.750793  [   96/  130]
train() client id: f_00008-9-3 loss: 0.865886  [  128/  130]
train() client id: f_00008-10-0 loss: 0.717116  [   32/  130]
train() client id: f_00008-10-1 loss: 0.739394  [   64/  130]
train() client id: f_00008-10-2 loss: 0.907082  [   96/  130]
train() client id: f_00008-10-3 loss: 0.831868  [  128/  130]
train() client id: f_00008-11-0 loss: 0.811405  [   32/  130]
train() client id: f_00008-11-1 loss: 0.765373  [   64/  130]
train() client id: f_00008-11-2 loss: 0.818685  [   96/  130]
train() client id: f_00008-11-3 loss: 0.764045  [  128/  130]
train() client id: f_00009-0-0 loss: 0.981155  [   32/  118]
train() client id: f_00009-0-1 loss: 0.796165  [   64/  118]
train() client id: f_00009-0-2 loss: 0.943038  [   96/  118]
train() client id: f_00009-1-0 loss: 0.833782  [   32/  118]
train() client id: f_00009-1-1 loss: 0.884632  [   64/  118]
train() client id: f_00009-1-2 loss: 0.819815  [   96/  118]
train() client id: f_00009-2-0 loss: 1.132231  [   32/  118]
train() client id: f_00009-2-1 loss: 0.708254  [   64/  118]
train() client id: f_00009-2-2 loss: 0.655538  [   96/  118]
train() client id: f_00009-3-0 loss: 0.717954  [   32/  118]
train() client id: f_00009-3-1 loss: 0.914650  [   64/  118]
train() client id: f_00009-3-2 loss: 0.734713  [   96/  118]
train() client id: f_00009-4-0 loss: 0.811189  [   32/  118]
train() client id: f_00009-4-1 loss: 0.678347  [   64/  118]
train() client id: f_00009-4-2 loss: 0.783997  [   96/  118]
train() client id: f_00009-5-0 loss: 0.683746  [   32/  118]
train() client id: f_00009-5-1 loss: 0.805928  [   64/  118]
train() client id: f_00009-5-2 loss: 0.653639  [   96/  118]
train() client id: f_00009-6-0 loss: 0.772881  [   32/  118]
train() client id: f_00009-6-1 loss: 0.658776  [   64/  118]
train() client id: f_00009-6-2 loss: 0.834120  [   96/  118]
train() client id: f_00009-7-0 loss: 0.770535  [   32/  118]
train() client id: f_00009-7-1 loss: 0.682240  [   64/  118]
train() client id: f_00009-7-2 loss: 0.728056  [   96/  118]
train() client id: f_00009-8-0 loss: 0.697054  [   32/  118]
train() client id: f_00009-8-1 loss: 0.738020  [   64/  118]
train() client id: f_00009-8-2 loss: 0.631007  [   96/  118]
train() client id: f_00009-9-0 loss: 0.654860  [   32/  118]
train() client id: f_00009-9-1 loss: 0.826961  [   64/  118]
train() client id: f_00009-9-2 loss: 0.727219  [   96/  118]
train() client id: f_00009-10-0 loss: 0.660773  [   32/  118]
train() client id: f_00009-10-1 loss: 0.665138  [   64/  118]
train() client id: f_00009-10-2 loss: 0.837258  [   96/  118]
train() client id: f_00009-11-0 loss: 0.629542  [   32/  118]
train() client id: f_00009-11-1 loss: 0.837329  [   64/  118]
train() client id: f_00009-11-2 loss: 0.660641  [   96/  118]
At round 48 accuracy: 0.6445623342175066
At round 48 training accuracy: 0.5888665325285044
At round 48 training loss: 0.8294919634538124
update_location
xs = -3.905658 4.200318 260.009024 18.811294 0.979296 3.956410 -222.443192 -201.324852 244.663977 -187.060879 
ys = 252.587959 235.555839 1.320614 -222.455176 214.350187 197.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -8.211426168117436
ys mean: 69.8941425355287
dists_uav = 271.690875 255.937875 278.579318 244.622505 236.531101 221.689177 243.901341 224.794066 264.894567 212.150381 
uav_gains = -113.224338 -111.809021 -113.851229 -110.846028 -110.198807 -109.107271 -110.786848 -109.325940 -112.607292 -108.462109 
uav_gains_db_mean: -111.02188833243835
dists_bs = 187.864752 189.155395 468.398740 442.191627 180.325123 180.404773 183.851819 176.155761 448.221342 171.423330 
bs_gains = -103.234728 -103.317984 -114.344299 -113.644154 -102.736634 -102.742004 -102.972161 -102.452171 -113.808850 -102.121017 
bs_gains_db_mean: -106.13740011128567
Round 49
-------------------------------
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.53180875  9.28555743  4.47092374  1.6235926  10.7068955   5.1513321
  2.0049889   6.33296008  4.67865356  4.17679842]
obj_prev = 52.96351106757935
eta_min = 4.606985121778528e-21	eta_max = 0.9400638118405741
af = 11.129531482767057	bf = 1.180489388448874	zeta = 12.242484631043764	eta = 0.909090909090909
af = 11.129531482767057	bf = 1.180489388448874	zeta = 24.65371738730223	eta = 0.45143421204703454
af = 11.129531482767057	bf = 1.180489388448874	zeta = 18.287853905358034	eta = 0.608575043324591
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.136885852504452	eta = 0.6494488892881634
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.071238826404898	eta = 0.6519463289068677
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.07100360444631	eta = 0.6519553120982448
eta = 0.6519553120982448
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [0.03529932 0.07424063 0.03473901 0.01204659 0.08572692 0.04090239
 0.01512828 0.05014745 0.03641993 0.03305809]
ene_total = [1.60196419 2.67053184 1.60796312 0.76579092 3.04020143 1.57233453
 0.86514606 1.976894   1.66129063 1.30888688]
ti_comp = [0.70507767 0.76819322 0.69764277 0.72788409 0.77017386 0.77015604
 0.7283619  0.73907045 0.6981679  0.77215943]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [5.52974468e-06 4.33375200e-05 5.38351868e-06 2.06228308e-07
 6.63825764e-05 7.21055148e-06 4.07900477e-07 1.44296163e-05
 6.19409234e-06 3.78703601e-06]
ene_total = [0.44615416 0.2406999  0.47049736 0.37129277 0.23496834 0.23308889
 0.36973465 0.33512519 0.46880422 0.22641604]
optimize_network iter = 0 obj = 3.396781522582919
eta = 0.6519553120982448
freqs = [25032220.23451398 48321587.59240132 24897421.60469212  8275076.45541563
 55654263.85367705 26554609.35010472 10385139.39706882 33926025.18152012
 26082502.95796476 21406261.0487775 ]
eta_min = 0.6519553120982468	eta_max = 0.6979914102198819
af = 0.004073073698070901	bf = 1.180489388448874	zeta = 0.004480381067877992	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [1.23203954e-06 9.65569680e-06 1.19946005e-06 4.59481302e-08
 1.47901871e-05 1.60652707e-06 9.08811424e-08 3.21495093e-06
 1.38005769e-06 8.43760128e-07]
ene_total = [1.70755859 0.91729463 1.80077061 1.42147114 0.89310582 0.89167624
 1.41548626 1.28161791 1.79420944 0.86646288]
ti_comp = [0.59380367 0.65691922 0.58636878 0.6166101  0.65889986 0.65888204
 0.6170879  0.62779645 0.5868939  0.66088543]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.62920539e-06 3.51879741e-05 4.52484764e-06 1.70633650e-07
 5.38525197e-05 5.84956806e-06 3.37417651e-07 1.18741342e-05
 5.20464897e-06 3.06954669e-06]
ene_total = [0.51412882 0.27708296 0.54218432 0.42788873 0.2703124  0.26856798
 0.42609178 0.38611291 0.54022815 0.26090226]
optimize_network iter = 1 obj = 3.9135003002771818
eta = 0.6979914102198819
freqs = [24982229.19460944 47493855.941672   24897421.60469214  8210343.90468077
 54677108.13463603 26088475.84559726 10302681.03588401 33568945.36426646
 26078780.65421337 21021289.55092173]
eta_min = 0.697991410219886	eta_max = 0.6979914102198779
af = 0.003953064498406836	bf = 1.180489388448874	zeta = 0.004348370948247519	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [1.22712352e-06 9.32773270e-06 1.19946005e-06 4.52320749e-08
 1.42753859e-05 1.55062088e-06 8.94436732e-08 3.14763077e-06
 1.37966381e-06 8.13684556e-07]
ene_total = [1.70755797 0.91725351 1.80077061 1.42147105 0.89304128 0.89166923
 1.41548608 1.28160947 1.79420939 0.86645911]
ti_comp = [0.59380367 0.65691922 0.58636878 0.6166101  0.65889986 0.65888204
 0.6170879  0.62779645 0.5868939  0.66088543]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.62920539e-06 3.51879741e-05 4.52484764e-06 1.70633650e-07
 5.38525197e-05 5.84956806e-06 3.37417651e-07 1.18741342e-05
 5.20464897e-06 3.06954669e-06]
ene_total = [0.51412882 0.27708296 0.54218432 0.42788873 0.2703124  0.26856798
 0.42609178 0.38611291 0.54022815 0.26090226]
optimize_network iter = 2 obj = 3.9135003002771303
eta = 0.6979914102198779
freqs = [24982229.19460944 47493855.94167206 24897421.60469212  8210343.90468077
 54677108.13463611 26088475.8455973  10302681.03588402 33568945.36426648
 26078780.65421337 21021289.55092177]
Done!
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.32509460e-06 3.28763371e-05 4.22759253e-06 1.59424052e-07
 5.03147350e-05 5.46528683e-06 3.15251352e-07 1.10940754e-05
 4.86273502e-06 2.86789605e-06]
ene_total = [0.01362255 0.00733955 0.01436594 0.01133774 0.00715892 0.00711585
 0.01129012 0.01023004 0.01431406 0.00691292]
At round 49 energy consumption: 0.10368768459529902
At round 49 eta: 0.6979914102198779
At round 49 a_n: 11.397856354757828
At round 49 local rounds: 11.773444044723618
At round 49 global rounds: 37.74017276480797
gradient difference: 0.4286249577999115
train() client id: f_00000-0-0 loss: 1.111978  [   32/  126]
train() client id: f_00000-0-1 loss: 1.195832  [   64/  126]
train() client id: f_00000-0-2 loss: 1.126519  [   96/  126]
train() client id: f_00000-1-0 loss: 1.121446  [   32/  126]
train() client id: f_00000-1-1 loss: 0.864083  [   64/  126]
train() client id: f_00000-1-2 loss: 1.135541  [   96/  126]
train() client id: f_00000-2-0 loss: 0.978965  [   32/  126]
train() client id: f_00000-2-1 loss: 1.012768  [   64/  126]
train() client id: f_00000-2-2 loss: 0.993653  [   96/  126]
train() client id: f_00000-3-0 loss: 0.930878  [   32/  126]
train() client id: f_00000-3-1 loss: 0.983814  [   64/  126]
train() client id: f_00000-3-2 loss: 0.989068  [   96/  126]
train() client id: f_00000-4-0 loss: 1.091149  [   32/  126]
train() client id: f_00000-4-1 loss: 0.932460  [   64/  126]
train() client id: f_00000-4-2 loss: 0.799515  [   96/  126]
train() client id: f_00000-5-0 loss: 0.821536  [   32/  126]
train() client id: f_00000-5-1 loss: 0.832723  [   64/  126]
train() client id: f_00000-5-2 loss: 0.857262  [   96/  126]
train() client id: f_00000-6-0 loss: 0.957128  [   32/  126]
train() client id: f_00000-6-1 loss: 0.818055  [   64/  126]
train() client id: f_00000-6-2 loss: 0.860784  [   96/  126]
train() client id: f_00000-7-0 loss: 0.783607  [   32/  126]
train() client id: f_00000-7-1 loss: 0.818905  [   64/  126]
train() client id: f_00000-7-2 loss: 1.100770  [   96/  126]
train() client id: f_00000-8-0 loss: 0.942455  [   32/  126]
train() client id: f_00000-8-1 loss: 0.765624  [   64/  126]
train() client id: f_00000-8-2 loss: 0.885356  [   96/  126]
train() client id: f_00000-9-0 loss: 0.890085  [   32/  126]
train() client id: f_00000-9-1 loss: 0.958688  [   64/  126]
train() client id: f_00000-9-2 loss: 0.839322  [   96/  126]
train() client id: f_00000-10-0 loss: 0.929919  [   32/  126]
train() client id: f_00000-10-1 loss: 0.766208  [   64/  126]
train() client id: f_00000-10-2 loss: 0.819124  [   96/  126]
train() client id: f_00001-0-0 loss: 0.356285  [   32/  265]
train() client id: f_00001-0-1 loss: 0.360175  [   64/  265]
train() client id: f_00001-0-2 loss: 0.439312  [   96/  265]
train() client id: f_00001-0-3 loss: 0.486570  [  128/  265]
train() client id: f_00001-0-4 loss: 0.512313  [  160/  265]
train() client id: f_00001-0-5 loss: 0.403989  [  192/  265]
train() client id: f_00001-0-6 loss: 0.406837  [  224/  265]
train() client id: f_00001-0-7 loss: 0.506356  [  256/  265]
train() client id: f_00001-1-0 loss: 0.422024  [   32/  265]
train() client id: f_00001-1-1 loss: 0.530443  [   64/  265]
train() client id: f_00001-1-2 loss: 0.423896  [   96/  265]
train() client id: f_00001-1-3 loss: 0.359984  [  128/  265]
train() client id: f_00001-1-4 loss: 0.455252  [  160/  265]
train() client id: f_00001-1-5 loss: 0.346437  [  192/  265]
train() client id: f_00001-1-6 loss: 0.442675  [  224/  265]
train() client id: f_00001-1-7 loss: 0.445779  [  256/  265]
train() client id: f_00001-2-0 loss: 0.386769  [   32/  265]
train() client id: f_00001-2-1 loss: 0.486455  [   64/  265]
train() client id: f_00001-2-2 loss: 0.369067  [   96/  265]
train() client id: f_00001-2-3 loss: 0.463959  [  128/  265]
train() client id: f_00001-2-4 loss: 0.340377  [  160/  265]
train() client id: f_00001-2-5 loss: 0.419929  [  192/  265]
train() client id: f_00001-2-6 loss: 0.328806  [  224/  265]
train() client id: f_00001-2-7 loss: 0.441007  [  256/  265]
train() client id: f_00001-3-0 loss: 0.423185  [   32/  265]
train() client id: f_00001-3-1 loss: 0.345128  [   64/  265]
train() client id: f_00001-3-2 loss: 0.315051  [   96/  265]
train() client id: f_00001-3-3 loss: 0.310061  [  128/  265]
train() client id: f_00001-3-4 loss: 0.522658  [  160/  265]
train() client id: f_00001-3-5 loss: 0.414518  [  192/  265]
train() client id: f_00001-3-6 loss: 0.433748  [  224/  265]
train() client id: f_00001-3-7 loss: 0.402339  [  256/  265]
train() client id: f_00001-4-0 loss: 0.388354  [   32/  265]
train() client id: f_00001-4-1 loss: 0.372095  [   64/  265]
train() client id: f_00001-4-2 loss: 0.526907  [   96/  265]
train() client id: f_00001-4-3 loss: 0.445362  [  128/  265]
train() client id: f_00001-4-4 loss: 0.511313  [  160/  265]
train() client id: f_00001-4-5 loss: 0.347376  [  192/  265]
train() client id: f_00001-4-6 loss: 0.336430  [  224/  265]
train() client id: f_00001-4-7 loss: 0.377201  [  256/  265]
train() client id: f_00001-5-0 loss: 0.507585  [   32/  265]
train() client id: f_00001-5-1 loss: 0.425334  [   64/  265]
train() client id: f_00001-5-2 loss: 0.317714  [   96/  265]
train() client id: f_00001-5-3 loss: 0.386277  [  128/  265]
train() client id: f_00001-5-4 loss: 0.405590  [  160/  265]
train() client id: f_00001-5-5 loss: 0.369988  [  192/  265]
train() client id: f_00001-5-6 loss: 0.354416  [  224/  265]
train() client id: f_00001-5-7 loss: 0.482856  [  256/  265]
train() client id: f_00001-6-0 loss: 0.388146  [   32/  265]
train() client id: f_00001-6-1 loss: 0.477943  [   64/  265]
train() client id: f_00001-6-2 loss: 0.394991  [   96/  265]
train() client id: f_00001-6-3 loss: 0.383111  [  128/  265]
train() client id: f_00001-6-4 loss: 0.449607  [  160/  265]
train() client id: f_00001-6-5 loss: 0.394459  [  192/  265]
train() client id: f_00001-6-6 loss: 0.378047  [  224/  265]
train() client id: f_00001-6-7 loss: 0.399606  [  256/  265]
train() client id: f_00001-7-0 loss: 0.461860  [   32/  265]
train() client id: f_00001-7-1 loss: 0.306679  [   64/  265]
train() client id: f_00001-7-2 loss: 0.302960  [   96/  265]
train() client id: f_00001-7-3 loss: 0.394080  [  128/  265]
train() client id: f_00001-7-4 loss: 0.448050  [  160/  265]
train() client id: f_00001-7-5 loss: 0.441765  [  192/  265]
train() client id: f_00001-7-6 loss: 0.530315  [  224/  265]
train() client id: f_00001-7-7 loss: 0.338366  [  256/  265]
train() client id: f_00001-8-0 loss: 0.369541  [   32/  265]
train() client id: f_00001-8-1 loss: 0.302743  [   64/  265]
train() client id: f_00001-8-2 loss: 0.557107  [   96/  265]
train() client id: f_00001-8-3 loss: 0.335206  [  128/  265]
train() client id: f_00001-8-4 loss: 0.365626  [  160/  265]
train() client id: f_00001-8-5 loss: 0.523405  [  192/  265]
train() client id: f_00001-8-6 loss: 0.326272  [  224/  265]
train() client id: f_00001-8-7 loss: 0.382520  [  256/  265]
train() client id: f_00001-9-0 loss: 0.383994  [   32/  265]
train() client id: f_00001-9-1 loss: 0.422492  [   64/  265]
train() client id: f_00001-9-2 loss: 0.399453  [   96/  265]
train() client id: f_00001-9-3 loss: 0.484221  [  128/  265]
train() client id: f_00001-9-4 loss: 0.312349  [  160/  265]
train() client id: f_00001-9-5 loss: 0.328106  [  192/  265]
train() client id: f_00001-9-6 loss: 0.380306  [  224/  265]
train() client id: f_00001-9-7 loss: 0.482006  [  256/  265]
train() client id: f_00001-10-0 loss: 0.416687  [   32/  265]
train() client id: f_00001-10-1 loss: 0.365796  [   64/  265]
train() client id: f_00001-10-2 loss: 0.362686  [   96/  265]
train() client id: f_00001-10-3 loss: 0.440175  [  128/  265]
train() client id: f_00001-10-4 loss: 0.548517  [  160/  265]
train() client id: f_00001-10-5 loss: 0.394863  [  192/  265]
train() client id: f_00001-10-6 loss: 0.343019  [  224/  265]
train() client id: f_00001-10-7 loss: 0.380591  [  256/  265]
train() client id: f_00002-0-0 loss: 1.326601  [   32/  124]
train() client id: f_00002-0-1 loss: 1.104899  [   64/  124]
train() client id: f_00002-0-2 loss: 1.057237  [   96/  124]
train() client id: f_00002-1-0 loss: 1.017746  [   32/  124]
train() client id: f_00002-1-1 loss: 1.228242  [   64/  124]
train() client id: f_00002-1-2 loss: 0.946785  [   96/  124]
train() client id: f_00002-2-0 loss: 1.159610  [   32/  124]
train() client id: f_00002-2-1 loss: 1.044811  [   64/  124]
train() client id: f_00002-2-2 loss: 0.904903  [   96/  124]
train() client id: f_00002-3-0 loss: 0.948180  [   32/  124]
train() client id: f_00002-3-1 loss: 1.081912  [   64/  124]
train() client id: f_00002-3-2 loss: 0.912263  [   96/  124]
train() client id: f_00002-4-0 loss: 0.976692  [   32/  124]
train() client id: f_00002-4-1 loss: 1.016888  [   64/  124]
train() client id: f_00002-4-2 loss: 0.750931  [   96/  124]
train() client id: f_00002-5-0 loss: 0.791357  [   32/  124]
train() client id: f_00002-5-1 loss: 0.963834  [   64/  124]
train() client id: f_00002-5-2 loss: 0.890828  [   96/  124]
train() client id: f_00002-6-0 loss: 1.076716  [   32/  124]
train() client id: f_00002-6-1 loss: 0.820595  [   64/  124]
train() client id: f_00002-6-2 loss: 0.850930  [   96/  124]
train() client id: f_00002-7-0 loss: 0.816748  [   32/  124]
train() client id: f_00002-7-1 loss: 0.874385  [   64/  124]
train() client id: f_00002-7-2 loss: 0.804800  [   96/  124]
train() client id: f_00002-8-0 loss: 0.927710  [   32/  124]
train() client id: f_00002-8-1 loss: 0.687426  [   64/  124]
train() client id: f_00002-8-2 loss: 0.911637  [   96/  124]
train() client id: f_00002-9-0 loss: 0.997790  [   32/  124]
train() client id: f_00002-9-1 loss: 0.615942  [   64/  124]
train() client id: f_00002-9-2 loss: 0.932523  [   96/  124]
train() client id: f_00002-10-0 loss: 0.794371  [   32/  124]
train() client id: f_00002-10-1 loss: 0.684043  [   64/  124]
train() client id: f_00002-10-2 loss: 0.910484  [   96/  124]
train() client id: f_00003-0-0 loss: 0.818664  [   32/   43]
train() client id: f_00003-1-0 loss: 0.839435  [   32/   43]
train() client id: f_00003-2-0 loss: 0.709473  [   32/   43]
train() client id: f_00003-3-0 loss: 0.892862  [   32/   43]
train() client id: f_00003-4-0 loss: 0.605005  [   32/   43]
train() client id: f_00003-5-0 loss: 0.839179  [   32/   43]
train() client id: f_00003-6-0 loss: 0.845976  [   32/   43]
train() client id: f_00003-7-0 loss: 0.606187  [   32/   43]
train() client id: f_00003-8-0 loss: 0.718196  [   32/   43]
train() client id: f_00003-9-0 loss: 0.787962  [   32/   43]
train() client id: f_00003-10-0 loss: 0.796340  [   32/   43]
train() client id: f_00004-0-0 loss: 0.888152  [   32/  306]
train() client id: f_00004-0-1 loss: 1.011626  [   64/  306]
train() client id: f_00004-0-2 loss: 0.856855  [   96/  306]
train() client id: f_00004-0-3 loss: 0.821598  [  128/  306]
train() client id: f_00004-0-4 loss: 0.727845  [  160/  306]
train() client id: f_00004-0-5 loss: 0.827185  [  192/  306]
train() client id: f_00004-0-6 loss: 0.885120  [  224/  306]
train() client id: f_00004-0-7 loss: 0.909545  [  256/  306]
train() client id: f_00004-0-8 loss: 0.802283  [  288/  306]
train() client id: f_00004-1-0 loss: 0.836828  [   32/  306]
train() client id: f_00004-1-1 loss: 0.799229  [   64/  306]
train() client id: f_00004-1-2 loss: 1.090550  [   96/  306]
train() client id: f_00004-1-3 loss: 0.919640  [  128/  306]
train() client id: f_00004-1-4 loss: 0.682662  [  160/  306]
train() client id: f_00004-1-5 loss: 0.973191  [  192/  306]
train() client id: f_00004-1-6 loss: 0.749547  [  224/  306]
train() client id: f_00004-1-7 loss: 0.768668  [  256/  306]
train() client id: f_00004-1-8 loss: 0.831720  [  288/  306]
train() client id: f_00004-2-0 loss: 0.941573  [   32/  306]
train() client id: f_00004-2-1 loss: 0.708709  [   64/  306]
train() client id: f_00004-2-2 loss: 1.136502  [   96/  306]
train() client id: f_00004-2-3 loss: 0.796711  [  128/  306]
train() client id: f_00004-2-4 loss: 0.844683  [  160/  306]
train() client id: f_00004-2-5 loss: 0.860464  [  192/  306]
train() client id: f_00004-2-6 loss: 0.788156  [  224/  306]
train() client id: f_00004-2-7 loss: 0.820054  [  256/  306]
train() client id: f_00004-2-8 loss: 0.832014  [  288/  306]
train() client id: f_00004-3-0 loss: 0.790393  [   32/  306]
train() client id: f_00004-3-1 loss: 0.967261  [   64/  306]
train() client id: f_00004-3-2 loss: 0.858870  [   96/  306]
train() client id: f_00004-3-3 loss: 0.885906  [  128/  306]
train() client id: f_00004-3-4 loss: 0.985722  [  160/  306]
train() client id: f_00004-3-5 loss: 0.793631  [  192/  306]
train() client id: f_00004-3-6 loss: 0.729075  [  224/  306]
train() client id: f_00004-3-7 loss: 0.826594  [  256/  306]
train() client id: f_00004-3-8 loss: 0.865374  [  288/  306]
train() client id: f_00004-4-0 loss: 0.890908  [   32/  306]
train() client id: f_00004-4-1 loss: 0.887632  [   64/  306]
train() client id: f_00004-4-2 loss: 0.856045  [   96/  306]
train() client id: f_00004-4-3 loss: 0.860765  [  128/  306]
train() client id: f_00004-4-4 loss: 0.770189  [  160/  306]
train() client id: f_00004-4-5 loss: 0.702518  [  192/  306]
train() client id: f_00004-4-6 loss: 0.777575  [  224/  306]
train() client id: f_00004-4-7 loss: 0.929787  [  256/  306]
train() client id: f_00004-4-8 loss: 0.863790  [  288/  306]
train() client id: f_00004-5-0 loss: 0.859829  [   32/  306]
train() client id: f_00004-5-1 loss: 0.797768  [   64/  306]
train() client id: f_00004-5-2 loss: 0.712429  [   96/  306]
train() client id: f_00004-5-3 loss: 0.954626  [  128/  306]
train() client id: f_00004-5-4 loss: 0.888033  [  160/  306]
train() client id: f_00004-5-5 loss: 0.822765  [  192/  306]
train() client id: f_00004-5-6 loss: 0.976368  [  224/  306]
train() client id: f_00004-5-7 loss: 0.777380  [  256/  306]
train() client id: f_00004-5-8 loss: 0.796227  [  288/  306]
train() client id: f_00004-6-0 loss: 0.764225  [   32/  306]
train() client id: f_00004-6-1 loss: 0.871618  [   64/  306]
train() client id: f_00004-6-2 loss: 0.917480  [   96/  306]
train() client id: f_00004-6-3 loss: 0.849018  [  128/  306]
train() client id: f_00004-6-4 loss: 0.907130  [  160/  306]
train() client id: f_00004-6-5 loss: 0.807968  [  192/  306]
train() client id: f_00004-6-6 loss: 0.825102  [  224/  306]
train() client id: f_00004-6-7 loss: 0.810106  [  256/  306]
train() client id: f_00004-6-8 loss: 0.878307  [  288/  306]
train() client id: f_00004-7-0 loss: 0.864306  [   32/  306]
train() client id: f_00004-7-1 loss: 0.824750  [   64/  306]
train() client id: f_00004-7-2 loss: 0.829294  [   96/  306]
train() client id: f_00004-7-3 loss: 0.830839  [  128/  306]
train() client id: f_00004-7-4 loss: 0.841886  [  160/  306]
train() client id: f_00004-7-5 loss: 0.740206  [  192/  306]
train() client id: f_00004-7-6 loss: 0.973421  [  224/  306]
train() client id: f_00004-7-7 loss: 0.796250  [  256/  306]
train() client id: f_00004-7-8 loss: 0.944667  [  288/  306]
train() client id: f_00004-8-0 loss: 0.796812  [   32/  306]
train() client id: f_00004-8-1 loss: 0.831332  [   64/  306]
train() client id: f_00004-8-2 loss: 0.887925  [   96/  306]
train() client id: f_00004-8-3 loss: 0.818954  [  128/  306]
train() client id: f_00004-8-4 loss: 0.900643  [  160/  306]
train() client id: f_00004-8-5 loss: 0.694298  [  192/  306]
train() client id: f_00004-8-6 loss: 0.826504  [  224/  306]
train() client id: f_00004-8-7 loss: 0.853295  [  256/  306]
train() client id: f_00004-8-8 loss: 1.003769  [  288/  306]
train() client id: f_00004-9-0 loss: 0.902870  [   32/  306]
train() client id: f_00004-9-1 loss: 0.743733  [   64/  306]
train() client id: f_00004-9-2 loss: 0.871563  [   96/  306]
train() client id: f_00004-9-3 loss: 0.763994  [  128/  306]
train() client id: f_00004-9-4 loss: 0.899994  [  160/  306]
train() client id: f_00004-9-5 loss: 0.773823  [  192/  306]
train() client id: f_00004-9-6 loss: 0.967340  [  224/  306]
train() client id: f_00004-9-7 loss: 0.904918  [  256/  306]
train() client id: f_00004-9-8 loss: 0.797713  [  288/  306]
train() client id: f_00004-10-0 loss: 0.822386  [   32/  306]
train() client id: f_00004-10-1 loss: 0.841599  [   64/  306]
train() client id: f_00004-10-2 loss: 0.705838  [   96/  306]
train() client id: f_00004-10-3 loss: 0.743577  [  128/  306]
train() client id: f_00004-10-4 loss: 0.975049  [  160/  306]
train() client id: f_00004-10-5 loss: 0.801537  [  192/  306]
train() client id: f_00004-10-6 loss: 0.734195  [  224/  306]
train() client id: f_00004-10-7 loss: 0.888382  [  256/  306]
train() client id: f_00004-10-8 loss: 1.093003  [  288/  306]
train() client id: f_00005-0-0 loss: 0.892422  [   32/  146]
train() client id: f_00005-0-1 loss: 0.235047  [   64/  146]
train() client id: f_00005-0-2 loss: 0.520020  [   96/  146]
train() client id: f_00005-0-3 loss: 0.561217  [  128/  146]
train() client id: f_00005-1-0 loss: 0.500208  [   32/  146]
train() client id: f_00005-1-1 loss: 0.575218  [   64/  146]
train() client id: f_00005-1-2 loss: 0.720910  [   96/  146]
train() client id: f_00005-1-3 loss: 0.374686  [  128/  146]
train() client id: f_00005-2-0 loss: 0.354176  [   32/  146]
train() client id: f_00005-2-1 loss: 0.411456  [   64/  146]
train() client id: f_00005-2-2 loss: 0.410586  [   96/  146]
train() client id: f_00005-2-3 loss: 0.880547  [  128/  146]
train() client id: f_00005-3-0 loss: 0.506497  [   32/  146]
train() client id: f_00005-3-1 loss: 0.525362  [   64/  146]
train() client id: f_00005-3-2 loss: 0.306192  [   96/  146]
train() client id: f_00005-3-3 loss: 0.595440  [  128/  146]
train() client id: f_00005-4-0 loss: 0.739721  [   32/  146]
train() client id: f_00005-4-1 loss: 0.446488  [   64/  146]
train() client id: f_00005-4-2 loss: 0.599714  [   96/  146]
train() client id: f_00005-4-3 loss: 0.578260  [  128/  146]
train() client id: f_00005-5-0 loss: 0.600887  [   32/  146]
train() client id: f_00005-5-1 loss: 0.534096  [   64/  146]
train() client id: f_00005-5-2 loss: 0.419959  [   96/  146]
train() client id: f_00005-5-3 loss: 0.483867  [  128/  146]
train() client id: f_00005-6-0 loss: 0.397398  [   32/  146]
train() client id: f_00005-6-1 loss: 0.709556  [   64/  146]
train() client id: f_00005-6-2 loss: 0.600096  [   96/  146]
train() client id: f_00005-6-3 loss: 0.559291  [  128/  146]
train() client id: f_00005-7-0 loss: 0.417038  [   32/  146]
train() client id: f_00005-7-1 loss: 0.347510  [   64/  146]
train() client id: f_00005-7-2 loss: 0.579609  [   96/  146]
train() client id: f_00005-7-3 loss: 0.630299  [  128/  146]
train() client id: f_00005-8-0 loss: 0.373403  [   32/  146]
train() client id: f_00005-8-1 loss: 0.477838  [   64/  146]
train() client id: f_00005-8-2 loss: 0.475997  [   96/  146]
train() client id: f_00005-8-3 loss: 0.675523  [  128/  146]
train() client id: f_00005-9-0 loss: 0.422833  [   32/  146]
train() client id: f_00005-9-1 loss: 0.865109  [   64/  146]
train() client id: f_00005-9-2 loss: 0.365521  [   96/  146]
train() client id: f_00005-9-3 loss: 0.536189  [  128/  146]
train() client id: f_00005-10-0 loss: 0.594404  [   32/  146]
train() client id: f_00005-10-1 loss: 0.418459  [   64/  146]
train() client id: f_00005-10-2 loss: 0.602826  [   96/  146]
train() client id: f_00005-10-3 loss: 0.382459  [  128/  146]
train() client id: f_00006-0-0 loss: 0.462792  [   32/   54]
train() client id: f_00006-1-0 loss: 0.505744  [   32/   54]
train() client id: f_00006-2-0 loss: 0.435026  [   32/   54]
train() client id: f_00006-3-0 loss: 0.491888  [   32/   54]
train() client id: f_00006-4-0 loss: 0.424781  [   32/   54]
train() client id: f_00006-5-0 loss: 0.440826  [   32/   54]
train() client id: f_00006-6-0 loss: 0.398141  [   32/   54]
train() client id: f_00006-7-0 loss: 0.474743  [   32/   54]
train() client id: f_00006-8-0 loss: 0.506643  [   32/   54]
train() client id: f_00006-9-0 loss: 0.448126  [   32/   54]
train() client id: f_00006-10-0 loss: 0.426901  [   32/   54]
train() client id: f_00007-0-0 loss: 0.489449  [   32/  179]
train() client id: f_00007-0-1 loss: 0.585553  [   64/  179]
train() client id: f_00007-0-2 loss: 0.691750  [   96/  179]
train() client id: f_00007-0-3 loss: 0.779848  [  128/  179]
train() client id: f_00007-0-4 loss: 0.648674  [  160/  179]
train() client id: f_00007-1-0 loss: 0.686360  [   32/  179]
train() client id: f_00007-1-1 loss: 0.636279  [   64/  179]
train() client id: f_00007-1-2 loss: 0.586460  [   96/  179]
train() client id: f_00007-1-3 loss: 0.610871  [  128/  179]
train() client id: f_00007-1-4 loss: 0.692253  [  160/  179]
train() client id: f_00007-2-0 loss: 0.678100  [   32/  179]
train() client id: f_00007-2-1 loss: 0.482836  [   64/  179]
train() client id: f_00007-2-2 loss: 0.684716  [   96/  179]
train() client id: f_00007-2-3 loss: 0.605167  [  128/  179]
train() client id: f_00007-2-4 loss: 0.723390  [  160/  179]
train() client id: f_00007-3-0 loss: 0.786052  [   32/  179]
train() client id: f_00007-3-1 loss: 0.547716  [   64/  179]
train() client id: f_00007-3-2 loss: 0.648662  [   96/  179]
train() client id: f_00007-3-3 loss: 0.580858  [  128/  179]
train() client id: f_00007-3-4 loss: 0.495788  [  160/  179]
train() client id: f_00007-4-0 loss: 0.477146  [   32/  179]
train() client id: f_00007-4-1 loss: 0.582596  [   64/  179]
train() client id: f_00007-4-2 loss: 0.557196  [   96/  179]
train() client id: f_00007-4-3 loss: 0.671952  [  128/  179]
train() client id: f_00007-4-4 loss: 0.725214  [  160/  179]
train() client id: f_00007-5-0 loss: 0.590292  [   32/  179]
train() client id: f_00007-5-1 loss: 0.582537  [   64/  179]
train() client id: f_00007-5-2 loss: 0.583451  [   96/  179]
train() client id: f_00007-5-3 loss: 0.656980  [  128/  179]
train() client id: f_00007-5-4 loss: 0.646311  [  160/  179]
train() client id: f_00007-6-0 loss: 0.633034  [   32/  179]
train() client id: f_00007-6-1 loss: 0.499453  [   64/  179]
train() client id: f_00007-6-2 loss: 0.696645  [   96/  179]
train() client id: f_00007-6-3 loss: 0.800620  [  128/  179]
train() client id: f_00007-6-4 loss: 0.436059  [  160/  179]
train() client id: f_00007-7-0 loss: 0.685320  [   32/  179]
train() client id: f_00007-7-1 loss: 0.476987  [   64/  179]
train() client id: f_00007-7-2 loss: 0.553151  [   96/  179]
train() client id: f_00007-7-3 loss: 0.645722  [  128/  179]
train() client id: f_00007-7-4 loss: 0.614809  [  160/  179]
train() client id: f_00007-8-0 loss: 0.964397  [   32/  179]
train() client id: f_00007-8-1 loss: 0.455372  [   64/  179]
train() client id: f_00007-8-2 loss: 0.487232  [   96/  179]
train() client id: f_00007-8-3 loss: 0.533590  [  128/  179]
train() client id: f_00007-8-4 loss: 0.470592  [  160/  179]
train() client id: f_00007-9-0 loss: 0.508638  [   32/  179]
train() client id: f_00007-9-1 loss: 0.761920  [   64/  179]
train() client id: f_00007-9-2 loss: 0.649875  [   96/  179]
train() client id: f_00007-9-3 loss: 0.542538  [  128/  179]
train() client id: f_00007-9-4 loss: 0.591024  [  160/  179]
train() client id: f_00007-10-0 loss: 0.609978  [   32/  179]
train() client id: f_00007-10-1 loss: 0.542022  [   64/  179]
train() client id: f_00007-10-2 loss: 0.552985  [   96/  179]
train() client id: f_00007-10-3 loss: 0.631680  [  128/  179]
train() client id: f_00007-10-4 loss: 0.631853  [  160/  179]
train() client id: f_00008-0-0 loss: 0.861583  [   32/  130]
train() client id: f_00008-0-1 loss: 0.725486  [   64/  130]
train() client id: f_00008-0-2 loss: 0.776023  [   96/  130]
train() client id: f_00008-0-3 loss: 0.702536  [  128/  130]
train() client id: f_00008-1-0 loss: 0.770528  [   32/  130]
train() client id: f_00008-1-1 loss: 0.845182  [   64/  130]
train() client id: f_00008-1-2 loss: 0.715492  [   96/  130]
train() client id: f_00008-1-3 loss: 0.772867  [  128/  130]
train() client id: f_00008-2-0 loss: 0.632896  [   32/  130]
train() client id: f_00008-2-1 loss: 0.999489  [   64/  130]
train() client id: f_00008-2-2 loss: 0.793736  [   96/  130]
train() client id: f_00008-2-3 loss: 0.677394  [  128/  130]
train() client id: f_00008-3-0 loss: 0.806199  [   32/  130]
train() client id: f_00008-3-1 loss: 0.734037  [   64/  130]
train() client id: f_00008-3-2 loss: 0.869930  [   96/  130]
train() client id: f_00008-3-3 loss: 0.682859  [  128/  130]
train() client id: f_00008-4-0 loss: 0.838867  [   32/  130]
train() client id: f_00008-4-1 loss: 0.729472  [   64/  130]
train() client id: f_00008-4-2 loss: 0.805208  [   96/  130]
train() client id: f_00008-4-3 loss: 0.713922  [  128/  130]
train() client id: f_00008-5-0 loss: 0.705034  [   32/  130]
train() client id: f_00008-5-1 loss: 0.777939  [   64/  130]
train() client id: f_00008-5-2 loss: 0.720890  [   96/  130]
train() client id: f_00008-5-3 loss: 0.818013  [  128/  130]
train() client id: f_00008-6-0 loss: 0.756856  [   32/  130]
train() client id: f_00008-6-1 loss: 0.724756  [   64/  130]
train() client id: f_00008-6-2 loss: 0.864429  [   96/  130]
train() client id: f_00008-6-3 loss: 0.746310  [  128/  130]
train() client id: f_00008-7-0 loss: 0.700717  [   32/  130]
train() client id: f_00008-7-1 loss: 0.863846  [   64/  130]
train() client id: f_00008-7-2 loss: 0.840062  [   96/  130]
train() client id: f_00008-7-3 loss: 0.673530  [  128/  130]
train() client id: f_00008-8-0 loss: 0.777435  [   32/  130]
train() client id: f_00008-8-1 loss: 0.769577  [   64/  130]
train() client id: f_00008-8-2 loss: 0.738745  [   96/  130]
train() client id: f_00008-8-3 loss: 0.726753  [  128/  130]
train() client id: f_00008-9-0 loss: 0.761199  [   32/  130]
train() client id: f_00008-9-1 loss: 0.741532  [   64/  130]
train() client id: f_00008-9-2 loss: 0.646512  [   96/  130]
train() client id: f_00008-9-3 loss: 0.930772  [  128/  130]
train() client id: f_00008-10-0 loss: 0.787215  [   32/  130]
train() client id: f_00008-10-1 loss: 0.817353  [   64/  130]
train() client id: f_00008-10-2 loss: 0.693241  [   96/  130]
train() client id: f_00008-10-3 loss: 0.768577  [  128/  130]
train() client id: f_00009-0-0 loss: 0.936975  [   32/  118]
train() client id: f_00009-0-1 loss: 0.991870  [   64/  118]
train() client id: f_00009-0-2 loss: 0.964289  [   96/  118]
train() client id: f_00009-1-0 loss: 1.047112  [   32/  118]
train() client id: f_00009-1-1 loss: 0.852835  [   64/  118]
train() client id: f_00009-1-2 loss: 1.019392  [   96/  118]
train() client id: f_00009-2-0 loss: 1.025090  [   32/  118]
train() client id: f_00009-2-1 loss: 0.867536  [   64/  118]
train() client id: f_00009-2-2 loss: 0.783496  [   96/  118]
train() client id: f_00009-3-0 loss: 1.032041  [   32/  118]
train() client id: f_00009-3-1 loss: 1.037333  [   64/  118]
train() client id: f_00009-3-2 loss: 0.771790  [   96/  118]
train() client id: f_00009-4-0 loss: 0.928065  [   32/  118]
train() client id: f_00009-4-1 loss: 0.778004  [   64/  118]
train() client id: f_00009-4-2 loss: 0.939699  [   96/  118]
train() client id: f_00009-5-0 loss: 0.891760  [   32/  118]
train() client id: f_00009-5-1 loss: 0.895330  [   64/  118]
train() client id: f_00009-5-2 loss: 0.848983  [   96/  118]
train() client id: f_00009-6-0 loss: 0.783043  [   32/  118]
train() client id: f_00009-6-1 loss: 0.871535  [   64/  118]
train() client id: f_00009-6-2 loss: 0.813272  [   96/  118]
train() client id: f_00009-7-0 loss: 0.691884  [   32/  118]
train() client id: f_00009-7-1 loss: 0.788932  [   64/  118]
train() client id: f_00009-7-2 loss: 0.748049  [   96/  118]
train() client id: f_00009-8-0 loss: 0.852658  [   32/  118]
train() client id: f_00009-8-1 loss: 0.989715  [   64/  118]
train() client id: f_00009-8-2 loss: 0.598986  [   96/  118]
train() client id: f_00009-9-0 loss: 0.860027  [   32/  118]
train() client id: f_00009-9-1 loss: 0.824377  [   64/  118]
train() client id: f_00009-9-2 loss: 0.800407  [   96/  118]
train() client id: f_00009-10-0 loss: 0.792445  [   32/  118]
train() client id: f_00009-10-1 loss: 0.813492  [   64/  118]
train() client id: f_00009-10-2 loss: 0.914224  [   96/  118]
At round 49 accuracy: 0.6445623342175066
At round 49 training accuracy: 0.590878604963112
At round 49 training loss: 0.8300237108470875
update_location
xs = -3.905658 4.200318 265.009024 18.811294 0.979296 3.956410 -227.443192 -206.324852 249.663977 -192.060879 
ys = 257.587959 240.555839 1.320614 -227.455176 219.350187 202.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -8.711426168117436
ys mean: 71.3941425355287
dists_uav = 276.345456 260.547029 283.251702 249.178093 241.071490 226.161962 248.469910 229.282840 269.519519 216.571912 
uav_gains = -113.648252 -112.216848 -114.273340 -111.226272 -110.557415 -109.423809 -111.166457 -109.650790 -113.026671 -108.756597 
uav_gains_db_mean: -111.39464505017165
dists_bs = 189.984328 190.814889 473.045949 446.691153 181.481822 181.105007 185.205085 176.972034 452.906184 171.847510 
bs_gains = -103.371157 -103.424202 -114.464352 -113.767265 -102.814387 -102.789112 -103.061341 -102.508389 -113.935290 -102.151070 
bs_gains_db_mean: -106.22865651920647
Round 50
-------------------------------
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.40113982  9.00691085  4.34261068  1.57832816 10.38540444  4.99669129
  1.94822841  6.14510341  4.53980435  4.05137628]
obj_prev = 51.395597693307295
eta_min = 1.1367158248091635e-21	eta_max = 0.9395757044516898
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 11.874554720679596	eta = 0.909090909090909
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 24.172264619780965	eta = 0.44658826617504477
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 17.836063247717608	eta = 0.6052372430028105
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.691468667850707	eta = 0.6467405571604739
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.62571758399257	eta = 0.6492982748886553
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.62547813342434	eta = 0.6493076264898318
eta = 0.6493076264898318
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [0.03563512 0.07494687 0.03506948 0.01216119 0.08654243 0.04129149
 0.01527219 0.05062449 0.03676639 0.03337257]
ene_total = [1.56827942 2.59415826 1.57537116 0.75074681 2.95309342 1.52632602
 0.84721729 1.92501916 1.61509008 1.2701765 ]
ti_comp = [0.73120545 0.79889761 0.72329946 0.75580078 0.8009931  0.80107742
 0.7563088  0.76793556 0.72766908 0.80314305]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [5.28975756e-06 4.12247795e-05 5.15266296e-06 1.96785757e-07
 6.31407441e-05 6.85665898e-06 3.89211744e-07 1.37503103e-05
 5.86631230e-06 3.60133211e-06]
ene_total = [0.44507619 0.2328146  0.46999487 0.36738071 0.22689962 0.22485949
 0.36578528 0.32955399 0.45624248 0.21824512]
optimize_network iter = 0 obj = 3.3368523467960083
eta = 0.6493076264898318
freqs = [24367376.98591928 46906432.60757383 24242711.66907599  8045235.73109414
 54021956.04098655 25772468.52031963 10096532.76307652 32961419.31716986
 25263125.77228831 20776230.43734453]
eta_min = 0.6493076264898333	eta_max = 0.6979381775060964
af = 0.0037239897789525765	bf = 1.1686004695034842	zeta = 0.004096388756847835	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [1.16746392e-06 9.09842125e-06 1.13720677e-06 4.34311531e-08
 1.39353344e-05 1.51328333e-06 8.59000931e-08 3.03473097e-06
 1.29471112e-06 7.94823816e-07]
ene_total = [1.71643177 0.89420252 1.81257148 1.41719511 0.86930783 0.86677177
 1.41102229 1.26998969 1.7594524  0.84156459]
ti_comp = [0.61023825 0.67793041 0.60233226 0.63483358 0.6800259  0.68011022
 0.6353416  0.64696837 0.60670188 0.68217585]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.38601091e-06 3.30616333e-05 4.29091311e-06 1.61080327e-07
 5.05906534e-05 5.49359804e-06 3.18510394e-07 1.11879346e-05
 4.87344749e-06 2.88277172e-06]
ene_total = [0.51669831 0.2699979  0.54563033 0.42652599 0.26297008 0.26101093
 0.42467242 0.38251687 0.52965907 0.25335527]
optimize_network iter = 1 obj = 3.8730371713241745
eta = 0.6979381775060964
freqs = [24314579.32952386 46031634.30360291 24242711.66907599  7976349.99245549
 52989718.17503818 25279541.36753655 10008802.12815238 32581091.06223162
 25232695.58126215 20369543.75424322]
eta_min = 0.6979381775061025	eta_max = 0.697938177506091
af = 0.0036043043465234475	bf = 1.1686004695034842	zeta = 0.003964734781175793	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [1.16241023e-06 8.76221731e-06 1.13720677e-06 4.26905959e-08
 1.34078766e-05 1.45595045e-06 8.44137753e-08 2.96510198e-06
 1.29159396e-06 7.64011626e-07]
ene_total = [1.71643115 0.89416163 1.81257148 1.41719502 0.86924368 0.8667648
 1.41102211 1.26998122 1.75945202 0.84156084]
ti_comp = [0.61023825 0.67793041 0.60233226 0.63483358 0.6800259  0.68011022
 0.6353416  0.64696837 0.60670188 0.68217585]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.38601091e-06 3.30616333e-05 4.29091311e-06 1.61080327e-07
 5.05906534e-05 5.49359804e-06 3.18510394e-07 1.11879346e-05
 4.87344749e-06 2.88277172e-06]
ene_total = [0.51669831 0.2699979  0.54563033 0.42652599 0.26297008 0.26101093
 0.42467242 0.38251687 0.52965907 0.25335527]
optimize_network iter = 2 obj = 3.873037171324106
eta = 0.697938177506091
freqs = [24314579.32952385 46031634.30360299 24242711.66907597  7976349.99245549
 52989718.17503828 25279541.3675366  10008802.12815239 32581091.06223165
 25232695.58126213 20369543.75424326]
Done!
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.09700746e-06 3.08831330e-05 4.00817585e-06 1.50466407e-07
 4.72571292e-05 5.13161335e-06 2.97523077e-07 1.04507382e-05
 4.55232582e-06 2.69281985e-06]
ene_total = [0.01411735 0.00737491 0.01490785 0.01165387 0.00718174 0.00713118
 0.01160321 0.01045069 0.01447144 0.00692218]
At round 50 energy consumption: 0.10581441892494492
At round 50 eta: 0.697938177506091
At round 50 a_n: 11.055310507788512
At round 50 local rounds: 11.775941462244061
At round 50 global rounds: 36.59949614457299
gradient difference: 0.42866578698158264
train() client id: f_00000-0-0 loss: 1.003337  [   32/  126]
train() client id: f_00000-0-1 loss: 1.123390  [   64/  126]
train() client id: f_00000-0-2 loss: 1.161696  [   96/  126]
train() client id: f_00000-1-0 loss: 1.109485  [   32/  126]
train() client id: f_00000-1-1 loss: 1.001267  [   64/  126]
train() client id: f_00000-1-2 loss: 1.184392  [   96/  126]
train() client id: f_00000-2-0 loss: 1.064273  [   32/  126]
train() client id: f_00000-2-1 loss: 0.888199  [   64/  126]
train() client id: f_00000-2-2 loss: 0.929203  [   96/  126]
train() client id: f_00000-3-0 loss: 1.044523  [   32/  126]
train() client id: f_00000-3-1 loss: 0.794195  [   64/  126]
train() client id: f_00000-3-2 loss: 0.982439  [   96/  126]
train() client id: f_00000-4-0 loss: 0.986433  [   32/  126]
train() client id: f_00000-4-1 loss: 0.878270  [   64/  126]
train() client id: f_00000-4-2 loss: 0.841275  [   96/  126]
train() client id: f_00000-5-0 loss: 0.880648  [   32/  126]
train() client id: f_00000-5-1 loss: 0.873009  [   64/  126]
train() client id: f_00000-5-2 loss: 1.030468  [   96/  126]
train() client id: f_00000-6-0 loss: 0.718703  [   32/  126]
train() client id: f_00000-6-1 loss: 0.956238  [   64/  126]
train() client id: f_00000-6-2 loss: 0.839860  [   96/  126]
train() client id: f_00000-7-0 loss: 0.822097  [   32/  126]
train() client id: f_00000-7-1 loss: 0.971071  [   64/  126]
train() client id: f_00000-7-2 loss: 0.863318  [   96/  126]
train() client id: f_00000-8-0 loss: 0.880518  [   32/  126]
train() client id: f_00000-8-1 loss: 0.803687  [   64/  126]
train() client id: f_00000-8-2 loss: 0.853828  [   96/  126]
train() client id: f_00000-9-0 loss: 0.898342  [   32/  126]
train() client id: f_00000-9-1 loss: 0.877392  [   64/  126]
train() client id: f_00000-9-2 loss: 0.813541  [   96/  126]
train() client id: f_00000-10-0 loss: 0.748608  [   32/  126]
train() client id: f_00000-10-1 loss: 0.868037  [   64/  126]
train() client id: f_00000-10-2 loss: 0.867399  [   96/  126]
train() client id: f_00001-0-0 loss: 0.397796  [   32/  265]
train() client id: f_00001-0-1 loss: 0.399511  [   64/  265]
train() client id: f_00001-0-2 loss: 0.391990  [   96/  265]
train() client id: f_00001-0-3 loss: 0.644396  [  128/  265]
train() client id: f_00001-0-4 loss: 0.590136  [  160/  265]
train() client id: f_00001-0-5 loss: 0.515236  [  192/  265]
train() client id: f_00001-0-6 loss: 0.457590  [  224/  265]
train() client id: f_00001-0-7 loss: 0.443959  [  256/  265]
train() client id: f_00001-1-0 loss: 0.637577  [   32/  265]
train() client id: f_00001-1-1 loss: 0.369719  [   64/  265]
train() client id: f_00001-1-2 loss: 0.412627  [   96/  265]
train() client id: f_00001-1-3 loss: 0.504313  [  128/  265]
train() client id: f_00001-1-4 loss: 0.525992  [  160/  265]
train() client id: f_00001-1-5 loss: 0.477667  [  192/  265]
train() client id: f_00001-1-6 loss: 0.488970  [  224/  265]
train() client id: f_00001-1-7 loss: 0.411627  [  256/  265]
train() client id: f_00001-2-0 loss: 0.404367  [   32/  265]
train() client id: f_00001-2-1 loss: 0.433969  [   64/  265]
train() client id: f_00001-2-2 loss: 0.456953  [   96/  265]
train() client id: f_00001-2-3 loss: 0.493734  [  128/  265]
train() client id: f_00001-2-4 loss: 0.504486  [  160/  265]
train() client id: f_00001-2-5 loss: 0.545621  [  192/  265]
train() client id: f_00001-2-6 loss: 0.482563  [  224/  265]
train() client id: f_00001-2-7 loss: 0.528616  [  256/  265]
train() client id: f_00001-3-0 loss: 0.442336  [   32/  265]
train() client id: f_00001-3-1 loss: 0.473806  [   64/  265]
train() client id: f_00001-3-2 loss: 0.390185  [   96/  265]
train() client id: f_00001-3-3 loss: 0.535041  [  128/  265]
train() client id: f_00001-3-4 loss: 0.416658  [  160/  265]
train() client id: f_00001-3-5 loss: 0.377996  [  192/  265]
train() client id: f_00001-3-6 loss: 0.712038  [  224/  265]
train() client id: f_00001-3-7 loss: 0.477912  [  256/  265]
train() client id: f_00001-4-0 loss: 0.371395  [   32/  265]
train() client id: f_00001-4-1 loss: 0.499594  [   64/  265]
train() client id: f_00001-4-2 loss: 0.431453  [   96/  265]
train() client id: f_00001-4-3 loss: 0.486408  [  128/  265]
train() client id: f_00001-4-4 loss: 0.589221  [  160/  265]
train() client id: f_00001-4-5 loss: 0.537588  [  192/  265]
train() client id: f_00001-4-6 loss: 0.397455  [  224/  265]
train() client id: f_00001-4-7 loss: 0.510054  [  256/  265]
train() client id: f_00001-5-0 loss: 0.508888  [   32/  265]
train() client id: f_00001-5-1 loss: 0.427431  [   64/  265]
train() client id: f_00001-5-2 loss: 0.616728  [   96/  265]
train() client id: f_00001-5-3 loss: 0.384043  [  128/  265]
train() client id: f_00001-5-4 loss: 0.445853  [  160/  265]
train() client id: f_00001-5-5 loss: 0.416193  [  192/  265]
train() client id: f_00001-5-6 loss: 0.520608  [  224/  265]
train() client id: f_00001-5-7 loss: 0.433214  [  256/  265]
train() client id: f_00001-6-0 loss: 0.367495  [   32/  265]
train() client id: f_00001-6-1 loss: 0.537985  [   64/  265]
train() client id: f_00001-6-2 loss: 0.517950  [   96/  265]
train() client id: f_00001-6-3 loss: 0.454674  [  128/  265]
train() client id: f_00001-6-4 loss: 0.422620  [  160/  265]
train() client id: f_00001-6-5 loss: 0.421070  [  192/  265]
train() client id: f_00001-6-6 loss: 0.424726  [  224/  265]
train() client id: f_00001-6-7 loss: 0.588998  [  256/  265]
train() client id: f_00001-7-0 loss: 0.437508  [   32/  265]
train() client id: f_00001-7-1 loss: 0.367578  [   64/  265]
train() client id: f_00001-7-2 loss: 0.439963  [   96/  265]
train() client id: f_00001-7-3 loss: 0.532131  [  128/  265]
train() client id: f_00001-7-4 loss: 0.618332  [  160/  265]
train() client id: f_00001-7-5 loss: 0.538862  [  192/  265]
train() client id: f_00001-7-6 loss: 0.461873  [  224/  265]
train() client id: f_00001-7-7 loss: 0.385959  [  256/  265]
train() client id: f_00001-8-0 loss: 0.523453  [   32/  265]
train() client id: f_00001-8-1 loss: 0.507876  [   64/  265]
train() client id: f_00001-8-2 loss: 0.428251  [   96/  265]
train() client id: f_00001-8-3 loss: 0.429502  [  128/  265]
train() client id: f_00001-8-4 loss: 0.417682  [  160/  265]
train() client id: f_00001-8-5 loss: 0.685599  [  192/  265]
train() client id: f_00001-8-6 loss: 0.389364  [  224/  265]
train() client id: f_00001-8-7 loss: 0.422187  [  256/  265]
train() client id: f_00001-9-0 loss: 0.485847  [   32/  265]
train() client id: f_00001-9-1 loss: 0.450250  [   64/  265]
train() client id: f_00001-9-2 loss: 0.547354  [   96/  265]
train() client id: f_00001-9-3 loss: 0.554472  [  128/  265]
train() client id: f_00001-9-4 loss: 0.541894  [  160/  265]
train() client id: f_00001-9-5 loss: 0.426309  [  192/  265]
train() client id: f_00001-9-6 loss: 0.426019  [  224/  265]
train() client id: f_00001-9-7 loss: 0.372894  [  256/  265]
train() client id: f_00001-10-0 loss: 0.394417  [   32/  265]
train() client id: f_00001-10-1 loss: 0.619101  [   64/  265]
train() client id: f_00001-10-2 loss: 0.468232  [   96/  265]
train() client id: f_00001-10-3 loss: 0.454992  [  128/  265]
train() client id: f_00001-10-4 loss: 0.485970  [  160/  265]
train() client id: f_00001-10-5 loss: 0.412133  [  192/  265]
train() client id: f_00001-10-6 loss: 0.517637  [  224/  265]
train() client id: f_00001-10-7 loss: 0.466455  [  256/  265]
train() client id: f_00002-0-0 loss: 1.147872  [   32/  124]
train() client id: f_00002-0-1 loss: 0.941427  [   64/  124]
train() client id: f_00002-0-2 loss: 0.968707  [   96/  124]
train() client id: f_00002-1-0 loss: 1.065799  [   32/  124]
train() client id: f_00002-1-1 loss: 1.070717  [   64/  124]
train() client id: f_00002-1-2 loss: 1.066411  [   96/  124]
train() client id: f_00002-2-0 loss: 0.962489  [   32/  124]
train() client id: f_00002-2-1 loss: 0.985155  [   64/  124]
train() client id: f_00002-2-2 loss: 1.212381  [   96/  124]
train() client id: f_00002-3-0 loss: 1.105588  [   32/  124]
train() client id: f_00002-3-1 loss: 1.000787  [   64/  124]
train() client id: f_00002-3-2 loss: 1.112490  [   96/  124]
train() client id: f_00002-4-0 loss: 0.890721  [   32/  124]
train() client id: f_00002-4-1 loss: 1.090338  [   64/  124]
train() client id: f_00002-4-2 loss: 0.886679  [   96/  124]
train() client id: f_00002-5-0 loss: 0.815724  [   32/  124]
train() client id: f_00002-5-1 loss: 0.868616  [   64/  124]
train() client id: f_00002-5-2 loss: 1.059005  [   96/  124]
train() client id: f_00002-6-0 loss: 0.720242  [   32/  124]
train() client id: f_00002-6-1 loss: 1.281851  [   64/  124]
train() client id: f_00002-6-2 loss: 0.917077  [   96/  124]
train() client id: f_00002-7-0 loss: 0.719224  [   32/  124]
train() client id: f_00002-7-1 loss: 1.052456  [   64/  124]
train() client id: f_00002-7-2 loss: 0.881592  [   96/  124]
train() client id: f_00002-8-0 loss: 0.845341  [   32/  124]
train() client id: f_00002-8-1 loss: 1.075277  [   64/  124]
train() client id: f_00002-8-2 loss: 0.919111  [   96/  124]
train() client id: f_00002-9-0 loss: 0.904223  [   32/  124]
train() client id: f_00002-9-1 loss: 0.813815  [   64/  124]
train() client id: f_00002-9-2 loss: 0.910253  [   96/  124]
train() client id: f_00002-10-0 loss: 0.826662  [   32/  124]
train() client id: f_00002-10-1 loss: 0.928552  [   64/  124]
train() client id: f_00002-10-2 loss: 0.876815  [   96/  124]
train() client id: f_00003-0-0 loss: 0.456030  [   32/   43]
train() client id: f_00003-1-0 loss: 0.719330  [   32/   43]
train() client id: f_00003-2-0 loss: 0.759624  [   32/   43]
train() client id: f_00003-3-0 loss: 0.564346  [   32/   43]
train() client id: f_00003-4-0 loss: 0.798004  [   32/   43]
train() client id: f_00003-5-0 loss: 0.725337  [   32/   43]
train() client id: f_00003-6-0 loss: 0.557111  [   32/   43]
train() client id: f_00003-7-0 loss: 0.809324  [   32/   43]
train() client id: f_00003-8-0 loss: 0.568627  [   32/   43]
train() client id: f_00003-9-0 loss: 0.665140  [   32/   43]
train() client id: f_00003-10-0 loss: 0.510702  [   32/   43]
train() client id: f_00004-0-0 loss: 0.539407  [   32/  306]
train() client id: f_00004-0-1 loss: 0.501733  [   64/  306]
train() client id: f_00004-0-2 loss: 0.620658  [   96/  306]
train() client id: f_00004-0-3 loss: 0.491648  [  128/  306]
train() client id: f_00004-0-4 loss: 0.620516  [  160/  306]
train() client id: f_00004-0-5 loss: 0.581171  [  192/  306]
train() client id: f_00004-0-6 loss: 0.520465  [  224/  306]
train() client id: f_00004-0-7 loss: 0.456945  [  256/  306]
train() client id: f_00004-0-8 loss: 0.840283  [  288/  306]
train() client id: f_00004-1-0 loss: 0.522254  [   32/  306]
train() client id: f_00004-1-1 loss: 0.540005  [   64/  306]
train() client id: f_00004-1-2 loss: 0.536614  [   96/  306]
train() client id: f_00004-1-3 loss: 0.490804  [  128/  306]
train() client id: f_00004-1-4 loss: 0.565423  [  160/  306]
train() client id: f_00004-1-5 loss: 0.549553  [  192/  306]
train() client id: f_00004-1-6 loss: 0.671993  [  224/  306]
train() client id: f_00004-1-7 loss: 0.611237  [  256/  306]
train() client id: f_00004-1-8 loss: 0.617797  [  288/  306]
train() client id: f_00004-2-0 loss: 0.478797  [   32/  306]
train() client id: f_00004-2-1 loss: 0.693120  [   64/  306]
train() client id: f_00004-2-2 loss: 0.513784  [   96/  306]
train() client id: f_00004-2-3 loss: 0.453704  [  128/  306]
train() client id: f_00004-2-4 loss: 0.605703  [  160/  306]
train() client id: f_00004-2-5 loss: 0.728083  [  192/  306]
train() client id: f_00004-2-6 loss: 0.472752  [  224/  306]
train() client id: f_00004-2-7 loss: 0.492168  [  256/  306]
train() client id: f_00004-2-8 loss: 0.671698  [  288/  306]
train() client id: f_00004-3-0 loss: 0.559897  [   32/  306]
train() client id: f_00004-3-1 loss: 0.511498  [   64/  306]
train() client id: f_00004-3-2 loss: 0.545800  [   96/  306]
train() client id: f_00004-3-3 loss: 0.616284  [  128/  306]
train() client id: f_00004-3-4 loss: 0.621337  [  160/  306]
train() client id: f_00004-3-5 loss: 0.575855  [  192/  306]
train() client id: f_00004-3-6 loss: 0.676526  [  224/  306]
train() client id: f_00004-3-7 loss: 0.564628  [  256/  306]
train() client id: f_00004-3-8 loss: 0.494399  [  288/  306]
train() client id: f_00004-4-0 loss: 0.596334  [   32/  306]
train() client id: f_00004-4-1 loss: 0.557547  [   64/  306]
train() client id: f_00004-4-2 loss: 0.485137  [   96/  306]
train() client id: f_00004-4-3 loss: 0.625147  [  128/  306]
train() client id: f_00004-4-4 loss: 0.499979  [  160/  306]
train() client id: f_00004-4-5 loss: 0.727633  [  192/  306]
train() client id: f_00004-4-6 loss: 0.620351  [  224/  306]
train() client id: f_00004-4-7 loss: 0.593777  [  256/  306]
train() client id: f_00004-4-8 loss: 0.498006  [  288/  306]
train() client id: f_00004-5-0 loss: 0.608081  [   32/  306]
train() client id: f_00004-5-1 loss: 0.474749  [   64/  306]
train() client id: f_00004-5-2 loss: 0.553146  [   96/  306]
train() client id: f_00004-5-3 loss: 0.602525  [  128/  306]
train() client id: f_00004-5-4 loss: 0.647545  [  160/  306]
train() client id: f_00004-5-5 loss: 0.549103  [  192/  306]
train() client id: f_00004-5-6 loss: 0.582449  [  224/  306]
train() client id: f_00004-5-7 loss: 0.579037  [  256/  306]
train() client id: f_00004-5-8 loss: 0.636105  [  288/  306]
train() client id: f_00004-6-0 loss: 0.580154  [   32/  306]
train() client id: f_00004-6-1 loss: 0.491662  [   64/  306]
train() client id: f_00004-6-2 loss: 0.544263  [   96/  306]
train() client id: f_00004-6-3 loss: 0.628496  [  128/  306]
train() client id: f_00004-6-4 loss: 0.743296  [  160/  306]
train() client id: f_00004-6-5 loss: 0.669075  [  192/  306]
train() client id: f_00004-6-6 loss: 0.539678  [  224/  306]
train() client id: f_00004-6-7 loss: 0.551859  [  256/  306]
train() client id: f_00004-6-8 loss: 0.611472  [  288/  306]
train() client id: f_00004-7-0 loss: 0.591123  [   32/  306]
train() client id: f_00004-7-1 loss: 0.647083  [   64/  306]
train() client id: f_00004-7-2 loss: 0.660571  [   96/  306]
train() client id: f_00004-7-3 loss: 0.564761  [  128/  306]
train() client id: f_00004-7-4 loss: 0.511826  [  160/  306]
train() client id: f_00004-7-5 loss: 0.691109  [  192/  306]
train() client id: f_00004-7-6 loss: 0.532069  [  224/  306]
train() client id: f_00004-7-7 loss: 0.485353  [  256/  306]
train() client id: f_00004-7-8 loss: 0.687135  [  288/  306]
train() client id: f_00004-8-0 loss: 0.580823  [   32/  306]
train() client id: f_00004-8-1 loss: 0.522738  [   64/  306]
train() client id: f_00004-8-2 loss: 0.722668  [   96/  306]
train() client id: f_00004-8-3 loss: 0.555363  [  128/  306]
train() client id: f_00004-8-4 loss: 0.541818  [  160/  306]
train() client id: f_00004-8-5 loss: 0.567229  [  192/  306]
train() client id: f_00004-8-6 loss: 0.657807  [  224/  306]
train() client id: f_00004-8-7 loss: 0.574956  [  256/  306]
train() client id: f_00004-8-8 loss: 0.601104  [  288/  306]
train() client id: f_00004-9-0 loss: 0.547519  [   32/  306]
train() client id: f_00004-9-1 loss: 0.577937  [   64/  306]
train() client id: f_00004-9-2 loss: 0.594648  [   96/  306]
train() client id: f_00004-9-3 loss: 0.564010  [  128/  306]
train() client id: f_00004-9-4 loss: 0.586643  [  160/  306]
train() client id: f_00004-9-5 loss: 0.650645  [  192/  306]
train() client id: f_00004-9-6 loss: 0.572571  [  224/  306]
train() client id: f_00004-9-7 loss: 0.684586  [  256/  306]
train() client id: f_00004-9-8 loss: 0.576496  [  288/  306]
train() client id: f_00004-10-0 loss: 0.492483  [   32/  306]
train() client id: f_00004-10-1 loss: 0.592823  [   64/  306]
train() client id: f_00004-10-2 loss: 0.529105  [   96/  306]
train() client id: f_00004-10-3 loss: 0.613261  [  128/  306]
train() client id: f_00004-10-4 loss: 0.610656  [  160/  306]
train() client id: f_00004-10-5 loss: 0.582945  [  192/  306]
train() client id: f_00004-10-6 loss: 0.583702  [  224/  306]
train() client id: f_00004-10-7 loss: 0.548930  [  256/  306]
train() client id: f_00004-10-8 loss: 0.767894  [  288/  306]
train() client id: f_00005-0-0 loss: 0.684185  [   32/  146]
train() client id: f_00005-0-1 loss: 0.780692  [   64/  146]
train() client id: f_00005-0-2 loss: 0.923709  [   96/  146]
train() client id: f_00005-0-3 loss: 0.655597  [  128/  146]
train() client id: f_00005-1-0 loss: 1.099118  [   32/  146]
train() client id: f_00005-1-1 loss: 0.563906  [   64/  146]
train() client id: f_00005-1-2 loss: 0.634712  [   96/  146]
train() client id: f_00005-1-3 loss: 0.763603  [  128/  146]
train() client id: f_00005-2-0 loss: 0.635636  [   32/  146]
train() client id: f_00005-2-1 loss: 0.790916  [   64/  146]
train() client id: f_00005-2-2 loss: 0.833716  [   96/  146]
train() client id: f_00005-2-3 loss: 0.900797  [  128/  146]
train() client id: f_00005-3-0 loss: 0.563025  [   32/  146]
train() client id: f_00005-3-1 loss: 0.831080  [   64/  146]
train() client id: f_00005-3-2 loss: 1.131666  [   96/  146]
train() client id: f_00005-3-3 loss: 0.646367  [  128/  146]
train() client id: f_00005-4-0 loss: 0.858491  [   32/  146]
train() client id: f_00005-4-1 loss: 0.617855  [   64/  146]
train() client id: f_00005-4-2 loss: 0.726659  [   96/  146]
train() client id: f_00005-4-3 loss: 0.826730  [  128/  146]
train() client id: f_00005-5-0 loss: 0.941806  [   32/  146]
train() client id: f_00005-5-1 loss: 0.791388  [   64/  146]
train() client id: f_00005-5-2 loss: 0.717076  [   96/  146]
train() client id: f_00005-5-3 loss: 0.783966  [  128/  146]
train() client id: f_00005-6-0 loss: 0.774649  [   32/  146]
train() client id: f_00005-6-1 loss: 0.743176  [   64/  146]
train() client id: f_00005-6-2 loss: 0.894919  [   96/  146]
train() client id: f_00005-6-3 loss: 0.801283  [  128/  146]
train() client id: f_00005-7-0 loss: 0.870799  [   32/  146]
train() client id: f_00005-7-1 loss: 0.748932  [   64/  146]
train() client id: f_00005-7-2 loss: 0.529374  [   96/  146]
train() client id: f_00005-7-3 loss: 0.943139  [  128/  146]
train() client id: f_00005-8-0 loss: 0.590783  [   32/  146]
train() client id: f_00005-8-1 loss: 0.802582  [   64/  146]
train() client id: f_00005-8-2 loss: 0.844759  [   96/  146]
train() client id: f_00005-8-3 loss: 0.660269  [  128/  146]
train() client id: f_00005-9-0 loss: 0.499777  [   32/  146]
train() client id: f_00005-9-1 loss: 0.697602  [   64/  146]
train() client id: f_00005-9-2 loss: 0.999782  [   96/  146]
train() client id: f_00005-9-3 loss: 0.679564  [  128/  146]
train() client id: f_00005-10-0 loss: 0.711861  [   32/  146]
train() client id: f_00005-10-1 loss: 0.802526  [   64/  146]
train() client id: f_00005-10-2 loss: 0.908882  [   96/  146]
train() client id: f_00005-10-3 loss: 0.806056  [  128/  146]
train() client id: f_00006-0-0 loss: 0.514385  [   32/   54]
train() client id: f_00006-1-0 loss: 0.500560  [   32/   54]
train() client id: f_00006-2-0 loss: 0.529490  [   32/   54]
train() client id: f_00006-3-0 loss: 0.461207  [   32/   54]
train() client id: f_00006-4-0 loss: 0.527866  [   32/   54]
train() client id: f_00006-5-0 loss: 0.513806  [   32/   54]
train() client id: f_00006-6-0 loss: 0.498950  [   32/   54]
train() client id: f_00006-7-0 loss: 0.560464  [   32/   54]
train() client id: f_00006-8-0 loss: 0.491319  [   32/   54]
train() client id: f_00006-9-0 loss: 0.497056  [   32/   54]
train() client id: f_00006-10-0 loss: 0.450963  [   32/   54]
train() client id: f_00007-0-0 loss: 0.799952  [   32/  179]
train() client id: f_00007-0-1 loss: 0.460835  [   64/  179]
train() client id: f_00007-0-2 loss: 0.450447  [   96/  179]
train() client id: f_00007-0-3 loss: 0.613724  [  128/  179]
train() client id: f_00007-0-4 loss: 0.351707  [  160/  179]
train() client id: f_00007-1-0 loss: 0.459185  [   32/  179]
train() client id: f_00007-1-1 loss: 0.756120  [   64/  179]
train() client id: f_00007-1-2 loss: 0.518893  [   96/  179]
train() client id: f_00007-1-3 loss: 0.455312  [  128/  179]
train() client id: f_00007-1-4 loss: 0.440830  [  160/  179]
train() client id: f_00007-2-0 loss: 0.528289  [   32/  179]
train() client id: f_00007-2-1 loss: 0.472725  [   64/  179]
train() client id: f_00007-2-2 loss: 0.691330  [   96/  179]
train() client id: f_00007-2-3 loss: 0.731527  [  128/  179]
train() client id: f_00007-2-4 loss: 0.344554  [  160/  179]
train() client id: f_00007-3-0 loss: 0.617813  [   32/  179]
train() client id: f_00007-3-1 loss: 0.622392  [   64/  179]
train() client id: f_00007-3-2 loss: 0.391006  [   96/  179]
train() client id: f_00007-3-3 loss: 0.546735  [  128/  179]
train() client id: f_00007-3-4 loss: 0.536206  [  160/  179]
train() client id: f_00007-4-0 loss: 0.353826  [   32/  179]
train() client id: f_00007-4-1 loss: 0.485269  [   64/  179]
train() client id: f_00007-4-2 loss: 0.416035  [   96/  179]
train() client id: f_00007-4-3 loss: 0.464570  [  128/  179]
train() client id: f_00007-4-4 loss: 0.623387  [  160/  179]
train() client id: f_00007-5-0 loss: 0.703262  [   32/  179]
train() client id: f_00007-5-1 loss: 0.444557  [   64/  179]
train() client id: f_00007-5-2 loss: 0.307948  [   96/  179]
train() client id: f_00007-5-3 loss: 0.552310  [  128/  179]
train() client id: f_00007-5-4 loss: 0.576568  [  160/  179]
train() client id: f_00007-6-0 loss: 0.690940  [   32/  179]
train() client id: f_00007-6-1 loss: 0.544979  [   64/  179]
train() client id: f_00007-6-2 loss: 0.415736  [   96/  179]
train() client id: f_00007-6-3 loss: 0.579904  [  128/  179]
train() client id: f_00007-6-4 loss: 0.372773  [  160/  179]
train() client id: f_00007-7-0 loss: 0.646376  [   32/  179]
train() client id: f_00007-7-1 loss: 0.606717  [   64/  179]
train() client id: f_00007-7-2 loss: 0.464810  [   96/  179]
train() client id: f_00007-7-3 loss: 0.457634  [  128/  179]
train() client id: f_00007-7-4 loss: 0.337453  [  160/  179]
train() client id: f_00007-8-0 loss: 0.622919  [   32/  179]
train() client id: f_00007-8-1 loss: 0.536238  [   64/  179]
train() client id: f_00007-8-2 loss: 0.364290  [   96/  179]
train() client id: f_00007-8-3 loss: 0.401141  [  128/  179]
train() client id: f_00007-8-4 loss: 0.557863  [  160/  179]
train() client id: f_00007-9-0 loss: 0.626275  [   32/  179]
train() client id: f_00007-9-1 loss: 0.327652  [   64/  179]
train() client id: f_00007-9-2 loss: 0.449451  [   96/  179]
train() client id: f_00007-9-3 loss: 0.429656  [  128/  179]
train() client id: f_00007-9-4 loss: 0.727218  [  160/  179]
train() client id: f_00007-10-0 loss: 0.441678  [   32/  179]
train() client id: f_00007-10-1 loss: 0.349760  [   64/  179]
train() client id: f_00007-10-2 loss: 0.693447  [   96/  179]
train() client id: f_00007-10-3 loss: 0.471759  [  128/  179]
train() client id: f_00007-10-4 loss: 0.421835  [  160/  179]
train() client id: f_00008-0-0 loss: 0.726646  [   32/  130]
train() client id: f_00008-0-1 loss: 0.727737  [   64/  130]
train() client id: f_00008-0-2 loss: 0.713531  [   96/  130]
train() client id: f_00008-0-3 loss: 0.802687  [  128/  130]
train() client id: f_00008-1-0 loss: 0.792766  [   32/  130]
train() client id: f_00008-1-1 loss: 0.818722  [   64/  130]
train() client id: f_00008-1-2 loss: 0.705636  [   96/  130]
train() client id: f_00008-1-3 loss: 0.602345  [  128/  130]
train() client id: f_00008-2-0 loss: 0.680092  [   32/  130]
train() client id: f_00008-2-1 loss: 0.800176  [   64/  130]
train() client id: f_00008-2-2 loss: 0.798936  [   96/  130]
train() client id: f_00008-2-3 loss: 0.689626  [  128/  130]
train() client id: f_00008-3-0 loss: 0.736098  [   32/  130]
train() client id: f_00008-3-1 loss: 0.785543  [   64/  130]
train() client id: f_00008-3-2 loss: 0.707238  [   96/  130]
train() client id: f_00008-3-3 loss: 0.741290  [  128/  130]
train() client id: f_00008-4-0 loss: 0.733573  [   32/  130]
train() client id: f_00008-4-1 loss: 0.848075  [   64/  130]
train() client id: f_00008-4-2 loss: 0.607418  [   96/  130]
train() client id: f_00008-4-3 loss: 0.778239  [  128/  130]
train() client id: f_00008-5-0 loss: 0.789112  [   32/  130]
train() client id: f_00008-5-1 loss: 0.657028  [   64/  130]
train() client id: f_00008-5-2 loss: 0.640731  [   96/  130]
train() client id: f_00008-5-3 loss: 0.868449  [  128/  130]
train() client id: f_00008-6-0 loss: 0.681366  [   32/  130]
train() client id: f_00008-6-1 loss: 0.768100  [   64/  130]
train() client id: f_00008-6-2 loss: 0.736283  [   96/  130]
train() client id: f_00008-6-3 loss: 0.744198  [  128/  130]
train() client id: f_00008-7-0 loss: 0.673901  [   32/  130]
train() client id: f_00008-7-1 loss: 0.662989  [   64/  130]
train() client id: f_00008-7-2 loss: 0.877907  [   96/  130]
train() client id: f_00008-7-3 loss: 0.717783  [  128/  130]
train() client id: f_00008-8-0 loss: 0.738587  [   32/  130]
train() client id: f_00008-8-1 loss: 0.621317  [   64/  130]
train() client id: f_00008-8-2 loss: 0.795851  [   96/  130]
train() client id: f_00008-8-3 loss: 0.812920  [  128/  130]
train() client id: f_00008-9-0 loss: 0.643650  [   32/  130]
train() client id: f_00008-9-1 loss: 0.771301  [   64/  130]
train() client id: f_00008-9-2 loss: 0.795269  [   96/  130]
train() client id: f_00008-9-3 loss: 0.765393  [  128/  130]
train() client id: f_00008-10-0 loss: 0.735188  [   32/  130]
train() client id: f_00008-10-1 loss: 0.665875  [   64/  130]
train() client id: f_00008-10-2 loss: 0.878641  [   96/  130]
train() client id: f_00008-10-3 loss: 0.687943  [  128/  130]
train() client id: f_00009-0-0 loss: 0.945067  [   32/  118]
train() client id: f_00009-0-1 loss: 0.977297  [   64/  118]
train() client id: f_00009-0-2 loss: 0.971868  [   96/  118]
train() client id: f_00009-1-0 loss: 1.047809  [   32/  118]
train() client id: f_00009-1-1 loss: 0.951206  [   64/  118]
train() client id: f_00009-1-2 loss: 0.805901  [   96/  118]
train() client id: f_00009-2-0 loss: 1.011704  [   32/  118]
train() client id: f_00009-2-1 loss: 0.869963  [   64/  118]
train() client id: f_00009-2-2 loss: 0.815824  [   96/  118]
train() client id: f_00009-3-0 loss: 0.877846  [   32/  118]
train() client id: f_00009-3-1 loss: 0.792844  [   64/  118]
train() client id: f_00009-3-2 loss: 0.823865  [   96/  118]
train() client id: f_00009-4-0 loss: 0.776442  [   32/  118]
train() client id: f_00009-4-1 loss: 0.814294  [   64/  118]
train() client id: f_00009-4-2 loss: 0.824180  [   96/  118]
train() client id: f_00009-5-0 loss: 0.662358  [   32/  118]
train() client id: f_00009-5-1 loss: 0.863101  [   64/  118]
train() client id: f_00009-5-2 loss: 0.730138  [   96/  118]
train() client id: f_00009-6-0 loss: 0.827574  [   32/  118]
train() client id: f_00009-6-1 loss: 0.651355  [   64/  118]
train() client id: f_00009-6-2 loss: 0.874751  [   96/  118]
train() client id: f_00009-7-0 loss: 0.775652  [   32/  118]
train() client id: f_00009-7-1 loss: 0.656348  [   64/  118]
train() client id: f_00009-7-2 loss: 0.799684  [   96/  118]
train() client id: f_00009-8-0 loss: 0.636147  [   32/  118]
train() client id: f_00009-8-1 loss: 0.658313  [   64/  118]
train() client id: f_00009-8-2 loss: 1.034879  [   96/  118]
train() client id: f_00009-9-0 loss: 0.782832  [   32/  118]
train() client id: f_00009-9-1 loss: 0.730112  [   64/  118]
train() client id: f_00009-9-2 loss: 0.713766  [   96/  118]
train() client id: f_00009-10-0 loss: 0.629884  [   32/  118]
train() client id: f_00009-10-1 loss: 0.752209  [   64/  118]
train() client id: f_00009-10-2 loss: 0.792735  [   96/  118]
At round 50 accuracy: 0.6445623342175066
At round 50 training accuracy: 0.5902079141515761
At round 50 training loss: 0.8232966968954184
update_location
xs = -3.905658 4.200318 270.009024 18.811294 0.979296 3.956410 -232.443192 -211.324852 254.663977 -197.060879 
ys = 262.587959 245.555839 1.320614 -232.455176 224.350187 207.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -9.211426168117436
ys mean: 72.8941425355287
dists_uav = 281.011905 265.170346 287.935092 253.750416 245.629732 230.656399 253.054793 233.792364 274.157639 221.018103 
uav_gains = -114.071502 -112.632205 -114.691274 -111.618241 -110.929157 -109.752365 -111.557993 -109.988230 -113.449067 -109.060618 
uav_gains_db_mean: -111.77506503291278
dists_bs = 192.210625 192.589928 477.700283 451.201217 182.768032 181.940004 186.682499 177.925123 457.597695 172.415706 
bs_gains = -103.512826 -103.536799 -114.583413 -113.889426 -102.900266 -102.845049 -103.157960 -102.573703 -114.060606 -102.191210 
bs_gains_db_mean: -106.32512588313745
Round 51
-------------------------------
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.27047392  8.72826714  4.21425716  1.53313884 10.06392708  4.84207237
  1.89154359  5.95729153  4.4008638   3.92598233]
obj_prev = 49.827817776908454
eta_min = 2.5652490042097954e-22	eta_max = 0.9391007800776031
af = 10.460568009377656	bf = 1.156843673852587	zeta = 11.506624810315422	eta = 0.9090909090909091
af = 10.460568009377656	bf = 1.156843673852587	zeta = 23.692265208499137	eta = 0.44151827262279386
af = 10.460568009377656	bf = 1.156843673852587	zeta = 17.38415562977596	eta = 0.6017300024316722
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.245893349082294	eta = 0.6438899840474798
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.180034455220028	eta = 0.6465108611683364
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.179790636766835	eta = 0.6465206036478086
eta = 0.6465206036478086
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [0.03599007 0.0756934  0.0354188  0.01228233 0.08740446 0.04170278
 0.01542432 0.05112875 0.03713261 0.03370499]
ene_total = [1.53443253 2.51779645 1.54247681 0.73582445 2.86602817 1.48040303
 0.82941199 1.87320864 1.56863488 1.23157367]
ti_comp = [0.75933363 0.83189525 0.75096662 0.78576566 0.83410293 0.83428835
 0.78630533 0.79893754 0.75947361 0.83641438]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [5.05315743e-06 3.91666828e-05 4.92425175e-06 1.87558063e-07
 5.99848321e-05 6.51242403e-06 3.70949938e-07 1.30872950e-05
 5.54780444e-06 3.42073551e-06]
ene_total = [0.44384841 0.22497254 0.46920207 0.3635944  0.21891275 0.21673023
 0.36196441 0.32406582 0.44343917 0.21019324]
optimize_network iter = 0 obj = 3.2769230299910435
eta = 0.6465206036478086
freqs = [23698456.65782105 45494550.69374827 23582139.4675787   7815514.32290755
 52394286.76461308 24993025.32151997  9808095.66767819 31997964.51374336
 24446282.03194968 20148497.51725332]
eta_min = 0.6465206036478099	eta_max = 0.6980699554597802
af = 0.0033958499648260683	bf = 1.156843673852587	zeta = 0.003735434961308675	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [1.10424650e-06 8.55894019e-06 1.07607725e-06 4.09863212e-08
 1.31082480e-05 1.42313425e-06 8.10622219e-08 2.85991478e-06
 1.21233975e-06 7.47519793e-07]
ene_total = [1.7253386  0.8711479  1.82393243 1.41373717 0.84566865 0.84210666
 1.4073824  1.2588513  1.7237018  0.81697375]
ti_comp = [0.62724647 0.69980809 0.61887946 0.6536785  0.70201577 0.70220119
 0.65421817 0.66685038 0.62738645 0.70432722]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [4.14484175e-06 3.09779353e-05 4.05812745e-06 1.51687479e-07
 4.73961769e-05 5.14527051e-06 2.99922360e-07 1.05141713e-05
 4.55022169e-06 2.70003775e-06]
ene_total = [0.51959565 0.26309218 0.54927951 0.42567061 0.25584167 0.25368468
 0.42376107 0.37930318 0.51911337 0.24605454]
optimize_network iter = 1 obj = 3.8353964400634717
eta = 0.6980699554597802
freqs = [23642854.46224663 44569165.40272366 23582139.46757871  7742332.9461439
 51302927.41865983 24471403.78525164  9714909.2256883  31593097.86188618
 24387978.69085684 19718556.51146085]
eta_min = 0.698069955459783	eta_max = 0.6980699554597762
af = 0.0032766452472554383	bf = 1.156843673852587	zeta = 0.003604309771980982	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [1.09907093e-06 8.21429384e-06 1.07607725e-06 4.02223555e-08
 1.25678526e-05 1.36435058e-06 7.95291995e-08 2.78800028e-06
 1.20656389e-06 7.15958094e-07]
ene_total = [1.72533799 0.87110728 1.82393243 1.41373708 0.84560497 0.84209973
 1.40738222 1.25884283 1.72370112 0.81697003]
ti_comp = [0.62724647 0.69980809 0.61887946 0.6536785  0.70201577 0.70220119
 0.65421817 0.66685038 0.62738645 0.70432722]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [4.14484175e-06 3.09779353e-05 4.05812745e-06 1.51687479e-07
 4.73961769e-05 5.14527051e-06 2.99922360e-07 1.05141713e-05
 4.55022169e-06 2.70003775e-06]
ene_total = [0.51959565 0.26309218 0.54927951 0.42567061 0.25584167 0.25368468
 0.42376107 0.37930318 0.51911337 0.24605454]
optimize_network iter = 2 obj = 3.8353964400634206
eta = 0.6980699554597762
freqs = [23642854.46224662 44569165.40272372 23582139.46757869  7742332.9461439
 51302927.41865991 24471403.78525168  9714909.2256883  31593097.8618862
 24387978.69085682 19718556.51146088]
Done!
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [3.87376305e-06 2.89519331e-05 3.79272000e-06 1.41766896e-07
 4.42963978e-05 4.80876231e-06 2.80307000e-07 9.82652913e-06
 4.25263056e-06 2.52345135e-06]
ene_total = [0.01464408 0.007413   0.0154807  0.01199715 0.00720758 0.00714955
 0.01194332 0.01068965 0.01463047 0.00693466]
At round 51 energy consumption: 0.10809015491855846
At round 51 eta: 0.6980699554597762
At round 51 a_n: 10.712764660819193
At round 51 local rounds: 11.76975943561131
At round 51 global rounds: 35.48094949322612
gradient difference: 0.4298001825809479
train() client id: f_00000-0-0 loss: 1.391206  [   32/  126]
train() client id: f_00000-0-1 loss: 0.880777  [   64/  126]
train() client id: f_00000-0-2 loss: 1.004768  [   96/  126]
train() client id: f_00000-1-0 loss: 1.257400  [   32/  126]
train() client id: f_00000-1-1 loss: 0.716250  [   64/  126]
train() client id: f_00000-1-2 loss: 1.012561  [   96/  126]
train() client id: f_00000-2-0 loss: 0.798152  [   32/  126]
train() client id: f_00000-2-1 loss: 1.034376  [   64/  126]
train() client id: f_00000-2-2 loss: 0.871937  [   96/  126]
train() client id: f_00000-3-0 loss: 0.933943  [   32/  126]
train() client id: f_00000-3-1 loss: 0.974695  [   64/  126]
train() client id: f_00000-3-2 loss: 0.716802  [   96/  126]
train() client id: f_00000-4-0 loss: 0.823116  [   32/  126]
train() client id: f_00000-4-1 loss: 0.922908  [   64/  126]
train() client id: f_00000-4-2 loss: 0.905709  [   96/  126]
train() client id: f_00000-5-0 loss: 0.739541  [   32/  126]
train() client id: f_00000-5-1 loss: 0.860665  [   64/  126]
train() client id: f_00000-5-2 loss: 0.802942  [   96/  126]
train() client id: f_00000-6-0 loss: 0.725802  [   32/  126]
train() client id: f_00000-6-1 loss: 1.017087  [   64/  126]
train() client id: f_00000-6-2 loss: 0.881771  [   96/  126]
train() client id: f_00000-7-0 loss: 0.829979  [   32/  126]
train() client id: f_00000-7-1 loss: 0.802147  [   64/  126]
train() client id: f_00000-7-2 loss: 0.717588  [   96/  126]
train() client id: f_00000-8-0 loss: 0.774521  [   32/  126]
train() client id: f_00000-8-1 loss: 0.824070  [   64/  126]
train() client id: f_00000-8-2 loss: 0.869483  [   96/  126]
train() client id: f_00000-9-0 loss: 0.820392  [   32/  126]
train() client id: f_00000-9-1 loss: 0.827348  [   64/  126]
train() client id: f_00000-9-2 loss: 0.662571  [   96/  126]
train() client id: f_00000-10-0 loss: 0.825143  [   32/  126]
train() client id: f_00000-10-1 loss: 0.789534  [   64/  126]
train() client id: f_00000-10-2 loss: 0.708825  [   96/  126]
train() client id: f_00001-0-0 loss: 0.443150  [   32/  265]
train() client id: f_00001-0-1 loss: 0.367845  [   64/  265]
train() client id: f_00001-0-2 loss: 0.471826  [   96/  265]
train() client id: f_00001-0-3 loss: 0.652226  [  128/  265]
train() client id: f_00001-0-4 loss: 0.406172  [  160/  265]
train() client id: f_00001-0-5 loss: 0.470174  [  192/  265]
train() client id: f_00001-0-6 loss: 0.448993  [  224/  265]
train() client id: f_00001-0-7 loss: 0.442636  [  256/  265]
train() client id: f_00001-1-0 loss: 0.549561  [   32/  265]
train() client id: f_00001-1-1 loss: 0.438561  [   64/  265]
train() client id: f_00001-1-2 loss: 0.411209  [   96/  265]
train() client id: f_00001-1-3 loss: 0.452516  [  128/  265]
train() client id: f_00001-1-4 loss: 0.400815  [  160/  265]
train() client id: f_00001-1-5 loss: 0.460646  [  192/  265]
train() client id: f_00001-1-6 loss: 0.433600  [  224/  265]
train() client id: f_00001-1-7 loss: 0.464908  [  256/  265]
train() client id: f_00001-2-0 loss: 0.420568  [   32/  265]
train() client id: f_00001-2-1 loss: 0.575490  [   64/  265]
train() client id: f_00001-2-2 loss: 0.483566  [   96/  265]
train() client id: f_00001-2-3 loss: 0.455560  [  128/  265]
train() client id: f_00001-2-4 loss: 0.490445  [  160/  265]
train() client id: f_00001-2-5 loss: 0.362840  [  192/  265]
train() client id: f_00001-2-6 loss: 0.398854  [  224/  265]
train() client id: f_00001-2-7 loss: 0.357396  [  256/  265]
train() client id: f_00001-3-0 loss: 0.379796  [   32/  265]
train() client id: f_00001-3-1 loss: 0.339725  [   64/  265]
train() client id: f_00001-3-2 loss: 0.533456  [   96/  265]
train() client id: f_00001-3-3 loss: 0.529742  [  128/  265]
train() client id: f_00001-3-4 loss: 0.460467  [  160/  265]
train() client id: f_00001-3-5 loss: 0.339333  [  192/  265]
train() client id: f_00001-3-6 loss: 0.505722  [  224/  265]
train() client id: f_00001-3-7 loss: 0.428437  [  256/  265]
train() client id: f_00001-4-0 loss: 0.397060  [   32/  265]
train() client id: f_00001-4-1 loss: 0.590455  [   64/  265]
train() client id: f_00001-4-2 loss: 0.343858  [   96/  265]
train() client id: f_00001-4-3 loss: 0.420341  [  128/  265]
train() client id: f_00001-4-4 loss: 0.333434  [  160/  265]
train() client id: f_00001-4-5 loss: 0.433846  [  192/  265]
train() client id: f_00001-4-6 loss: 0.438624  [  224/  265]
train() client id: f_00001-4-7 loss: 0.505979  [  256/  265]
train() client id: f_00001-5-0 loss: 0.489886  [   32/  265]
train() client id: f_00001-5-1 loss: 0.398010  [   64/  265]
train() client id: f_00001-5-2 loss: 0.429260  [   96/  265]
train() client id: f_00001-5-3 loss: 0.433559  [  128/  265]
train() client id: f_00001-5-4 loss: 0.335099  [  160/  265]
train() client id: f_00001-5-5 loss: 0.388262  [  192/  265]
train() client id: f_00001-5-6 loss: 0.391990  [  224/  265]
train() client id: f_00001-5-7 loss: 0.537685  [  256/  265]
train() client id: f_00001-6-0 loss: 0.431202  [   32/  265]
train() client id: f_00001-6-1 loss: 0.334112  [   64/  265]
train() client id: f_00001-6-2 loss: 0.393533  [   96/  265]
train() client id: f_00001-6-3 loss: 0.396728  [  128/  265]
train() client id: f_00001-6-4 loss: 0.436469  [  160/  265]
train() client id: f_00001-6-5 loss: 0.431509  [  192/  265]
train() client id: f_00001-6-6 loss: 0.584524  [  224/  265]
train() client id: f_00001-6-7 loss: 0.404972  [  256/  265]
train() client id: f_00001-7-0 loss: 0.443756  [   32/  265]
train() client id: f_00001-7-1 loss: 0.437911  [   64/  265]
train() client id: f_00001-7-2 loss: 0.359859  [   96/  265]
train() client id: f_00001-7-3 loss: 0.333024  [  128/  265]
train() client id: f_00001-7-4 loss: 0.370241  [  160/  265]
train() client id: f_00001-7-5 loss: 0.412950  [  192/  265]
train() client id: f_00001-7-6 loss: 0.496730  [  224/  265]
train() client id: f_00001-7-7 loss: 0.499024  [  256/  265]
train() client id: f_00001-8-0 loss: 0.347027  [   32/  265]
train() client id: f_00001-8-1 loss: 0.434492  [   64/  265]
train() client id: f_00001-8-2 loss: 0.424125  [   96/  265]
train() client id: f_00001-8-3 loss: 0.554202  [  128/  265]
train() client id: f_00001-8-4 loss: 0.331027  [  160/  265]
train() client id: f_00001-8-5 loss: 0.360128  [  192/  265]
train() client id: f_00001-8-6 loss: 0.460209  [  224/  265]
train() client id: f_00001-8-7 loss: 0.412795  [  256/  265]
train() client id: f_00001-9-0 loss: 0.360201  [   32/  265]
train() client id: f_00001-9-1 loss: 0.370438  [   64/  265]
train() client id: f_00001-9-2 loss: 0.376898  [   96/  265]
train() client id: f_00001-9-3 loss: 0.376156  [  128/  265]
train() client id: f_00001-9-4 loss: 0.518021  [  160/  265]
train() client id: f_00001-9-5 loss: 0.490400  [  192/  265]
train() client id: f_00001-9-6 loss: 0.348764  [  224/  265]
train() client id: f_00001-9-7 loss: 0.489889  [  256/  265]
train() client id: f_00001-10-0 loss: 0.418745  [   32/  265]
train() client id: f_00001-10-1 loss: 0.519731  [   64/  265]
train() client id: f_00001-10-2 loss: 0.388029  [   96/  265]
train() client id: f_00001-10-3 loss: 0.401741  [  128/  265]
train() client id: f_00001-10-4 loss: 0.485582  [  160/  265]
train() client id: f_00001-10-5 loss: 0.397877  [  192/  265]
train() client id: f_00001-10-6 loss: 0.370094  [  224/  265]
train() client id: f_00001-10-7 loss: 0.381263  [  256/  265]
train() client id: f_00002-0-0 loss: 0.902491  [   32/  124]
train() client id: f_00002-0-1 loss: 1.304280  [   64/  124]
train() client id: f_00002-0-2 loss: 1.085753  [   96/  124]
train() client id: f_00002-1-0 loss: 1.076297  [   32/  124]
train() client id: f_00002-1-1 loss: 1.070260  [   64/  124]
train() client id: f_00002-1-2 loss: 1.147379  [   96/  124]
train() client id: f_00002-2-0 loss: 0.986332  [   32/  124]
train() client id: f_00002-2-1 loss: 1.021146  [   64/  124]
train() client id: f_00002-2-2 loss: 1.175989  [   96/  124]
train() client id: f_00002-3-0 loss: 0.936878  [   32/  124]
train() client id: f_00002-3-1 loss: 1.198235  [   64/  124]
train() client id: f_00002-3-2 loss: 1.046547  [   96/  124]
train() client id: f_00002-4-0 loss: 0.935612  [   32/  124]
train() client id: f_00002-4-1 loss: 1.090271  [   64/  124]
train() client id: f_00002-4-2 loss: 0.972640  [   96/  124]
train() client id: f_00002-5-0 loss: 1.016320  [   32/  124]
train() client id: f_00002-5-1 loss: 1.281948  [   64/  124]
train() client id: f_00002-5-2 loss: 0.781352  [   96/  124]
train() client id: f_00002-6-0 loss: 0.805340  [   32/  124]
train() client id: f_00002-6-1 loss: 1.047943  [   64/  124]
train() client id: f_00002-6-2 loss: 0.980891  [   96/  124]
train() client id: f_00002-7-0 loss: 0.883935  [   32/  124]
train() client id: f_00002-7-1 loss: 1.034458  [   64/  124]
train() client id: f_00002-7-2 loss: 0.906949  [   96/  124]
train() client id: f_00002-8-0 loss: 1.196462  [   32/  124]
train() client id: f_00002-8-1 loss: 0.910351  [   64/  124]
train() client id: f_00002-8-2 loss: 0.835153  [   96/  124]
train() client id: f_00002-9-0 loss: 0.775867  [   32/  124]
train() client id: f_00002-9-1 loss: 0.991874  [   64/  124]
train() client id: f_00002-9-2 loss: 0.982366  [   96/  124]
train() client id: f_00002-10-0 loss: 1.113632  [   32/  124]
train() client id: f_00002-10-1 loss: 0.935906  [   64/  124]
train() client id: f_00002-10-2 loss: 0.765787  [   96/  124]
train() client id: f_00003-0-0 loss: 0.786629  [   32/   43]
train() client id: f_00003-1-0 loss: 0.688668  [   32/   43]
train() client id: f_00003-2-0 loss: 0.800641  [   32/   43]
train() client id: f_00003-3-0 loss: 0.838449  [   32/   43]
train() client id: f_00003-4-0 loss: 0.791653  [   32/   43]
train() client id: f_00003-5-0 loss: 0.574760  [   32/   43]
train() client id: f_00003-6-0 loss: 0.867135  [   32/   43]
train() client id: f_00003-7-0 loss: 0.729574  [   32/   43]
train() client id: f_00003-8-0 loss: 0.637904  [   32/   43]
train() client id: f_00003-9-0 loss: 0.820368  [   32/   43]
train() client id: f_00003-10-0 loss: 0.973163  [   32/   43]
train() client id: f_00004-0-0 loss: 0.631387  [   32/  306]
train() client id: f_00004-0-1 loss: 0.647446  [   64/  306]
train() client id: f_00004-0-2 loss: 0.653131  [   96/  306]
train() client id: f_00004-0-3 loss: 0.625207  [  128/  306]
train() client id: f_00004-0-4 loss: 0.709550  [  160/  306]
train() client id: f_00004-0-5 loss: 0.670914  [  192/  306]
train() client id: f_00004-0-6 loss: 0.557174  [  224/  306]
train() client id: f_00004-0-7 loss: 0.566338  [  256/  306]
train() client id: f_00004-0-8 loss: 0.537799  [  288/  306]
train() client id: f_00004-1-0 loss: 0.592853  [   32/  306]
train() client id: f_00004-1-1 loss: 0.581234  [   64/  306]
train() client id: f_00004-1-2 loss: 0.528247  [   96/  306]
train() client id: f_00004-1-3 loss: 0.708717  [  128/  306]
train() client id: f_00004-1-4 loss: 0.667184  [  160/  306]
train() client id: f_00004-1-5 loss: 0.629705  [  192/  306]
train() client id: f_00004-1-6 loss: 0.783445  [  224/  306]
train() client id: f_00004-1-7 loss: 0.692284  [  256/  306]
train() client id: f_00004-1-8 loss: 0.542165  [  288/  306]
train() client id: f_00004-2-0 loss: 0.658820  [   32/  306]
train() client id: f_00004-2-1 loss: 0.608670  [   64/  306]
train() client id: f_00004-2-2 loss: 0.756563  [   96/  306]
train() client id: f_00004-2-3 loss: 0.574355  [  128/  306]
train() client id: f_00004-2-4 loss: 0.544851  [  160/  306]
train() client id: f_00004-2-5 loss: 0.728567  [  192/  306]
train() client id: f_00004-2-6 loss: 0.668970  [  224/  306]
train() client id: f_00004-2-7 loss: 0.547594  [  256/  306]
train() client id: f_00004-2-8 loss: 0.680138  [  288/  306]
train() client id: f_00004-3-0 loss: 0.608354  [   32/  306]
train() client id: f_00004-3-1 loss: 0.539746  [   64/  306]
train() client id: f_00004-3-2 loss: 0.549851  [   96/  306]
train() client id: f_00004-3-3 loss: 0.612993  [  128/  306]
train() client id: f_00004-3-4 loss: 0.767297  [  160/  306]
train() client id: f_00004-3-5 loss: 0.630287  [  192/  306]
train() client id: f_00004-3-6 loss: 0.623321  [  224/  306]
train() client id: f_00004-3-7 loss: 0.648148  [  256/  306]
train() client id: f_00004-3-8 loss: 0.774627  [  288/  306]
train() client id: f_00004-4-0 loss: 0.500087  [   32/  306]
train() client id: f_00004-4-1 loss: 0.778357  [   64/  306]
train() client id: f_00004-4-2 loss: 0.691197  [   96/  306]
train() client id: f_00004-4-3 loss: 0.688042  [  128/  306]
train() client id: f_00004-4-4 loss: 0.749256  [  160/  306]
train() client id: f_00004-4-5 loss: 0.552434  [  192/  306]
train() client id: f_00004-4-6 loss: 0.608705  [  224/  306]
train() client id: f_00004-4-7 loss: 0.658553  [  256/  306]
train() client id: f_00004-4-8 loss: 0.599847  [  288/  306]
train() client id: f_00004-5-0 loss: 0.814306  [   32/  306]
train() client id: f_00004-5-1 loss: 0.545208  [   64/  306]
train() client id: f_00004-5-2 loss: 0.625784  [   96/  306]
train() client id: f_00004-5-3 loss: 0.655777  [  128/  306]
train() client id: f_00004-5-4 loss: 0.593809  [  160/  306]
train() client id: f_00004-5-5 loss: 0.691483  [  192/  306]
train() client id: f_00004-5-6 loss: 0.664291  [  224/  306]
train() client id: f_00004-5-7 loss: 0.683832  [  256/  306]
train() client id: f_00004-5-8 loss: 0.592337  [  288/  306]
train() client id: f_00004-6-0 loss: 0.572293  [   32/  306]
train() client id: f_00004-6-1 loss: 0.625049  [   64/  306]
train() client id: f_00004-6-2 loss: 0.714579  [   96/  306]
train() client id: f_00004-6-3 loss: 0.570707  [  128/  306]
train() client id: f_00004-6-4 loss: 0.706899  [  160/  306]
train() client id: f_00004-6-5 loss: 0.608061  [  192/  306]
train() client id: f_00004-6-6 loss: 0.567691  [  224/  306]
train() client id: f_00004-6-7 loss: 0.714074  [  256/  306]
train() client id: f_00004-6-8 loss: 0.703241  [  288/  306]
train() client id: f_00004-7-0 loss: 0.646929  [   32/  306]
train() client id: f_00004-7-1 loss: 0.655306  [   64/  306]
train() client id: f_00004-7-2 loss: 0.713276  [   96/  306]
train() client id: f_00004-7-3 loss: 0.638099  [  128/  306]
train() client id: f_00004-7-4 loss: 0.612293  [  160/  306]
train() client id: f_00004-7-5 loss: 0.650931  [  192/  306]
train() client id: f_00004-7-6 loss: 0.637318  [  224/  306]
train() client id: f_00004-7-7 loss: 0.585818  [  256/  306]
train() client id: f_00004-7-8 loss: 0.728374  [  288/  306]
train() client id: f_00004-8-0 loss: 0.524037  [   32/  306]
train() client id: f_00004-8-1 loss: 0.638752  [   64/  306]
train() client id: f_00004-8-2 loss: 0.740060  [   96/  306]
train() client id: f_00004-8-3 loss: 0.802830  [  128/  306]
train() client id: f_00004-8-4 loss: 0.678969  [  160/  306]
train() client id: f_00004-8-5 loss: 0.616635  [  192/  306]
train() client id: f_00004-8-6 loss: 0.758185  [  224/  306]
train() client id: f_00004-8-7 loss: 0.671832  [  256/  306]
train() client id: f_00004-8-8 loss: 0.573231  [  288/  306]
train() client id: f_00004-9-0 loss: 0.684064  [   32/  306]
train() client id: f_00004-9-1 loss: 0.588792  [   64/  306]
train() client id: f_00004-9-2 loss: 0.512959  [   96/  306]
train() client id: f_00004-9-3 loss: 0.720600  [  128/  306]
train() client id: f_00004-9-4 loss: 0.671133  [  160/  306]
train() client id: f_00004-9-5 loss: 0.548474  [  192/  306]
train() client id: f_00004-9-6 loss: 0.766795  [  224/  306]
train() client id: f_00004-9-7 loss: 0.692110  [  256/  306]
train() client id: f_00004-9-8 loss: 0.717188  [  288/  306]
train() client id: f_00004-10-0 loss: 0.713719  [   32/  306]
train() client id: f_00004-10-1 loss: 0.655783  [   64/  306]
train() client id: f_00004-10-2 loss: 0.659310  [   96/  306]
train() client id: f_00004-10-3 loss: 0.661784  [  128/  306]
train() client id: f_00004-10-4 loss: 0.576802  [  160/  306]
train() client id: f_00004-10-5 loss: 0.634174  [  192/  306]
train() client id: f_00004-10-6 loss: 0.711583  [  224/  306]
train() client id: f_00004-10-7 loss: 0.712680  [  256/  306]
train() client id: f_00004-10-8 loss: 0.700589  [  288/  306]
train() client id: f_00005-0-0 loss: 0.313970  [   32/  146]
train() client id: f_00005-0-1 loss: 0.513294  [   64/  146]
train() client id: f_00005-0-2 loss: 0.591258  [   96/  146]
train() client id: f_00005-0-3 loss: 0.798852  [  128/  146]
train() client id: f_00005-1-0 loss: 0.717704  [   32/  146]
train() client id: f_00005-1-1 loss: 0.342672  [   64/  146]
train() client id: f_00005-1-2 loss: 0.465155  [   96/  146]
train() client id: f_00005-1-3 loss: 0.949186  [  128/  146]
train() client id: f_00005-2-0 loss: 0.628746  [   32/  146]
train() client id: f_00005-2-1 loss: 0.733416  [   64/  146]
train() client id: f_00005-2-2 loss: 0.439865  [   96/  146]
train() client id: f_00005-2-3 loss: 0.673742  [  128/  146]
train() client id: f_00005-3-0 loss: 0.725055  [   32/  146]
train() client id: f_00005-3-1 loss: 0.647206  [   64/  146]
train() client id: f_00005-3-2 loss: 0.402780  [   96/  146]
train() client id: f_00005-3-3 loss: 0.525047  [  128/  146]
train() client id: f_00005-4-0 loss: 0.815501  [   32/  146]
train() client id: f_00005-4-1 loss: 0.563517  [   64/  146]
train() client id: f_00005-4-2 loss: 0.514709  [   96/  146]
train() client id: f_00005-4-3 loss: 0.803144  [  128/  146]
train() client id: f_00005-5-0 loss: 0.525905  [   32/  146]
train() client id: f_00005-5-1 loss: 0.582932  [   64/  146]
train() client id: f_00005-5-2 loss: 0.540522  [   96/  146]
train() client id: f_00005-5-3 loss: 0.743963  [  128/  146]
train() client id: f_00005-6-0 loss: 0.874520  [   32/  146]
train() client id: f_00005-6-1 loss: 0.607369  [   64/  146]
train() client id: f_00005-6-2 loss: 0.398739  [   96/  146]
train() client id: f_00005-6-3 loss: 0.545731  [  128/  146]
train() client id: f_00005-7-0 loss: 0.325578  [   32/  146]
train() client id: f_00005-7-1 loss: 0.390212  [   64/  146]
train() client id: f_00005-7-2 loss: 0.756310  [   96/  146]
train() client id: f_00005-7-3 loss: 0.848764  [  128/  146]
train() client id: f_00005-8-0 loss: 0.782135  [   32/  146]
train() client id: f_00005-8-1 loss: 0.419317  [   64/  146]
train() client id: f_00005-8-2 loss: 0.702630  [   96/  146]
train() client id: f_00005-8-3 loss: 0.553304  [  128/  146]
train() client id: f_00005-9-0 loss: 0.626573  [   32/  146]
train() client id: f_00005-9-1 loss: 0.635813  [   64/  146]
train() client id: f_00005-9-2 loss: 0.638005  [   96/  146]
train() client id: f_00005-9-3 loss: 0.693503  [  128/  146]
train() client id: f_00005-10-0 loss: 0.355752  [   32/  146]
train() client id: f_00005-10-1 loss: 0.598998  [   64/  146]
train() client id: f_00005-10-2 loss: 0.673927  [   96/  146]
train() client id: f_00005-10-3 loss: 0.725607  [  128/  146]
train() client id: f_00006-0-0 loss: 0.542339  [   32/   54]
train() client id: f_00006-1-0 loss: 0.552618  [   32/   54]
train() client id: f_00006-2-0 loss: 0.521770  [   32/   54]
train() client id: f_00006-3-0 loss: 0.523724  [   32/   54]
train() client id: f_00006-4-0 loss: 0.589027  [   32/   54]
train() client id: f_00006-5-0 loss: 0.539195  [   32/   54]
train() client id: f_00006-6-0 loss: 0.589097  [   32/   54]
train() client id: f_00006-7-0 loss: 0.544124  [   32/   54]
train() client id: f_00006-8-0 loss: 0.530542  [   32/   54]
train() client id: f_00006-9-0 loss: 0.538993  [   32/   54]
train() client id: f_00006-10-0 loss: 0.590333  [   32/   54]
train() client id: f_00007-0-0 loss: 0.866917  [   32/  179]
train() client id: f_00007-0-1 loss: 0.661147  [   64/  179]
train() client id: f_00007-0-2 loss: 0.672374  [   96/  179]
train() client id: f_00007-0-3 loss: 0.638898  [  128/  179]
train() client id: f_00007-0-4 loss: 0.555992  [  160/  179]
train() client id: f_00007-1-0 loss: 0.679566  [   32/  179]
train() client id: f_00007-1-1 loss: 0.543002  [   64/  179]
train() client id: f_00007-1-2 loss: 0.810637  [   96/  179]
train() client id: f_00007-1-3 loss: 0.646455  [  128/  179]
train() client id: f_00007-1-4 loss: 0.527666  [  160/  179]
train() client id: f_00007-2-0 loss: 0.608255  [   32/  179]
train() client id: f_00007-2-1 loss: 0.788856  [   64/  179]
train() client id: f_00007-2-2 loss: 0.693555  [   96/  179]
train() client id: f_00007-2-3 loss: 0.580516  [  128/  179]
train() client id: f_00007-2-4 loss: 0.536634  [  160/  179]
train() client id: f_00007-3-0 loss: 0.649046  [   32/  179]
train() client id: f_00007-3-1 loss: 0.633332  [   64/  179]
train() client id: f_00007-3-2 loss: 0.546303  [   96/  179]
train() client id: f_00007-3-3 loss: 0.667016  [  128/  179]
train() client id: f_00007-3-4 loss: 0.762737  [  160/  179]
train() client id: f_00007-4-0 loss: 0.453036  [   32/  179]
train() client id: f_00007-4-1 loss: 0.816227  [   64/  179]
train() client id: f_00007-4-2 loss: 0.640958  [   96/  179]
train() client id: f_00007-4-3 loss: 0.446904  [  128/  179]
train() client id: f_00007-4-4 loss: 0.759077  [  160/  179]
train() client id: f_00007-5-0 loss: 0.602487  [   32/  179]
train() client id: f_00007-5-1 loss: 0.489441  [   64/  179]
train() client id: f_00007-5-2 loss: 0.815380  [   96/  179]
train() client id: f_00007-5-3 loss: 0.423048  [  128/  179]
train() client id: f_00007-5-4 loss: 0.869120  [  160/  179]
train() client id: f_00007-6-0 loss: 0.561342  [   32/  179]
train() client id: f_00007-6-1 loss: 0.565892  [   64/  179]
train() client id: f_00007-6-2 loss: 0.662702  [   96/  179]
train() client id: f_00007-6-3 loss: 0.761671  [  128/  179]
train() client id: f_00007-6-4 loss: 0.659696  [  160/  179]
train() client id: f_00007-7-0 loss: 0.678968  [   32/  179]
train() client id: f_00007-7-1 loss: 0.583608  [   64/  179]
train() client id: f_00007-7-2 loss: 0.728297  [   96/  179]
train() client id: f_00007-7-3 loss: 0.569710  [  128/  179]
train() client id: f_00007-7-4 loss: 0.437809  [  160/  179]
train() client id: f_00007-8-0 loss: 0.579872  [   32/  179]
train() client id: f_00007-8-1 loss: 0.563173  [   64/  179]
train() client id: f_00007-8-2 loss: 0.517932  [   96/  179]
train() client id: f_00007-8-3 loss: 0.697613  [  128/  179]
train() client id: f_00007-8-4 loss: 0.683452  [  160/  179]
train() client id: f_00007-9-0 loss: 0.843644  [   32/  179]
train() client id: f_00007-9-1 loss: 0.556851  [   64/  179]
train() client id: f_00007-9-2 loss: 0.569596  [   96/  179]
train() client id: f_00007-9-3 loss: 0.626555  [  128/  179]
train() client id: f_00007-9-4 loss: 0.445398  [  160/  179]
train() client id: f_00007-10-0 loss: 0.822951  [   32/  179]
train() client id: f_00007-10-1 loss: 0.548900  [   64/  179]
train() client id: f_00007-10-2 loss: 0.578678  [   96/  179]
train() client id: f_00007-10-3 loss: 0.760405  [  128/  179]
train() client id: f_00007-10-4 loss: 0.442384  [  160/  179]
train() client id: f_00008-0-0 loss: 0.667077  [   32/  130]
train() client id: f_00008-0-1 loss: 0.718408  [   64/  130]
train() client id: f_00008-0-2 loss: 0.634515  [   96/  130]
train() client id: f_00008-0-3 loss: 0.892456  [  128/  130]
train() client id: f_00008-1-0 loss: 0.720565  [   32/  130]
train() client id: f_00008-1-1 loss: 0.816508  [   64/  130]
train() client id: f_00008-1-2 loss: 0.702256  [   96/  130]
train() client id: f_00008-1-3 loss: 0.697847  [  128/  130]
train() client id: f_00008-2-0 loss: 0.660019  [   32/  130]
train() client id: f_00008-2-1 loss: 0.778470  [   64/  130]
train() client id: f_00008-2-2 loss: 0.784096  [   96/  130]
train() client id: f_00008-2-3 loss: 0.719819  [  128/  130]
train() client id: f_00008-3-0 loss: 0.858686  [   32/  130]
train() client id: f_00008-3-1 loss: 0.697407  [   64/  130]
train() client id: f_00008-3-2 loss: 0.735815  [   96/  130]
train() client id: f_00008-3-3 loss: 0.635646  [  128/  130]
train() client id: f_00008-4-0 loss: 0.655759  [   32/  130]
train() client id: f_00008-4-1 loss: 0.708615  [   64/  130]
train() client id: f_00008-4-2 loss: 0.733823  [   96/  130]
train() client id: f_00008-4-3 loss: 0.800027  [  128/  130]
train() client id: f_00008-5-0 loss: 0.676365  [   32/  130]
train() client id: f_00008-5-1 loss: 0.848811  [   64/  130]
train() client id: f_00008-5-2 loss: 0.796300  [   96/  130]
train() client id: f_00008-5-3 loss: 0.621053  [  128/  130]
train() client id: f_00008-6-0 loss: 0.723011  [   32/  130]
train() client id: f_00008-6-1 loss: 0.718723  [   64/  130]
train() client id: f_00008-6-2 loss: 0.684262  [   96/  130]
train() client id: f_00008-6-3 loss: 0.786795  [  128/  130]
train() client id: f_00008-7-0 loss: 0.745502  [   32/  130]
train() client id: f_00008-7-1 loss: 0.739014  [   64/  130]
train() client id: f_00008-7-2 loss: 0.734195  [   96/  130]
train() client id: f_00008-7-3 loss: 0.703250  [  128/  130]
train() client id: f_00008-8-0 loss: 0.698973  [   32/  130]
train() client id: f_00008-8-1 loss: 0.819517  [   64/  130]
train() client id: f_00008-8-2 loss: 0.765077  [   96/  130]
train() client id: f_00008-8-3 loss: 0.656459  [  128/  130]
train() client id: f_00008-9-0 loss: 0.691656  [   32/  130]
train() client id: f_00008-9-1 loss: 0.698033  [   64/  130]
train() client id: f_00008-9-2 loss: 0.770723  [   96/  130]
train() client id: f_00008-9-3 loss: 0.776695  [  128/  130]
train() client id: f_00008-10-0 loss: 0.692722  [   32/  130]
train() client id: f_00008-10-1 loss: 0.756278  [   64/  130]
train() client id: f_00008-10-2 loss: 0.673308  [   96/  130]
train() client id: f_00008-10-3 loss: 0.827050  [  128/  130]
train() client id: f_00009-0-0 loss: 1.096674  [   32/  118]
train() client id: f_00009-0-1 loss: 0.841336  [   64/  118]
train() client id: f_00009-0-2 loss: 0.862146  [   96/  118]
train() client id: f_00009-1-0 loss: 0.808160  [   32/  118]
train() client id: f_00009-1-1 loss: 0.825161  [   64/  118]
train() client id: f_00009-1-2 loss: 1.055020  [   96/  118]
train() client id: f_00009-2-0 loss: 0.877495  [   32/  118]
train() client id: f_00009-2-1 loss: 0.949937  [   64/  118]
train() client id: f_00009-2-2 loss: 0.942329  [   96/  118]
train() client id: f_00009-3-0 loss: 0.867555  [   32/  118]
train() client id: f_00009-3-1 loss: 0.908235  [   64/  118]
train() client id: f_00009-3-2 loss: 0.884188  [   96/  118]
train() client id: f_00009-4-0 loss: 0.766158  [   32/  118]
train() client id: f_00009-4-1 loss: 0.962019  [   64/  118]
train() client id: f_00009-4-2 loss: 0.887781  [   96/  118]
train() client id: f_00009-5-0 loss: 0.784615  [   32/  118]
train() client id: f_00009-5-1 loss: 0.727366  [   64/  118]
train() client id: f_00009-5-2 loss: 0.915772  [   96/  118]
train() client id: f_00009-6-0 loss: 0.693052  [   32/  118]
train() client id: f_00009-6-1 loss: 0.995007  [   64/  118]
train() client id: f_00009-6-2 loss: 0.717981  [   96/  118]
train() client id: f_00009-7-0 loss: 0.804734  [   32/  118]
train() client id: f_00009-7-1 loss: 0.725641  [   64/  118]
train() client id: f_00009-7-2 loss: 0.737557  [   96/  118]
train() client id: f_00009-8-0 loss: 0.908943  [   32/  118]
train() client id: f_00009-8-1 loss: 0.787162  [   64/  118]
train() client id: f_00009-8-2 loss: 0.742335  [   96/  118]
train() client id: f_00009-9-0 loss: 0.867075  [   32/  118]
train() client id: f_00009-9-1 loss: 0.686553  [   64/  118]
train() client id: f_00009-9-2 loss: 0.746925  [   96/  118]
train() client id: f_00009-10-0 loss: 1.015409  [   32/  118]
train() client id: f_00009-10-1 loss: 0.695084  [   64/  118]
train() client id: f_00009-10-2 loss: 0.635690  [   96/  118]
At round 51 accuracy: 0.6445623342175066
At round 51 training accuracy: 0.5895372233400402
At round 51 training loss: 0.8321963364671081
update_location
xs = -3.905658 4.200318 275.009024 18.811294 0.979296 3.956410 -237.443192 -216.324852 259.663977 -202.060879 
ys = 267.587959 250.555839 1.320614 -237.455176 229.350187 212.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -9.711426168117436
ys mean: 74.3941425355287
dists_uav = 285.689640 269.807100 292.628958 258.338587 250.204851 235.171248 257.655118 238.321459 278.808269 225.487495 
uav_gains = -114.491677 -113.052833 -115.102889 -112.020532 -111.313436 -110.093711 -111.960103 -110.338798 -113.872000 -109.375433 
uav_gains_db_mean: -112.16214117323392
dists_bs = 194.539980 194.477347 482.361535 455.721505 184.181040 182.907918 188.281139 179.012843 462.295674 173.126498 
bs_gains = -103.659307 -103.655392 -114.701494 -114.010646 -102.993917 -102.909569 -103.261650 -102.647816 -114.184814 -102.241238 
bs_gains_db_mean: -106.42658444077685
Round 52
-------------------------------
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.13977177 8.44962292 4.08581891 1.48801359 9.74245982 4.68747181
 1.834924   5.76952797 4.26183039 3.80061293]
obj_prev = 48.26005410532363
eta_min = 5.2478248998997527e-23	eta_max = 0.9386552346452276
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 11.138694899951254	eta = 0.9090909090909091
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 23.212442703241933	eta = 0.43623527270004686
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 16.931595727685714	eta = 0.598058590314986
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.799778450901695	eta = 0.6409005230136667
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.733820172197483	eta = 0.6435872637324471
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.733571912722388	eta = 0.6435974188731335
eta = 0.6435974188731335
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [0.03636401 0.07647987 0.03578681 0.01240994 0.0883126  0.04213608
 0.01558458 0.05165999 0.03751843 0.03405519]
ene_total = [1.50031121 2.44142183 1.50915709 0.72099371 2.77897583 1.43455086
 0.8117007  1.8214624  1.52193281 1.19306548]
ti_comp = [0.78968755 0.8674062  0.78087534 0.81798185 0.86972334 0.87000865
 0.81855413 0.83227438 0.79380079 0.87219318]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [4.81930945e-06 3.71600544e-05 4.69770128e-06 1.78526131e-07
 5.69096341e-05 6.17725006e-06 3.53077655e-07 1.24396969e-05
 5.23830472e-06 3.24492313e-06]
ene_total = [0.44237055 0.21717421 0.46800782 0.35990781 0.2110067  0.20870035
 0.35824773 0.31867769 0.43041446 0.20225874]
optimize_network iter = 0 obj = 3.2167660550334363
eta = 0.6435974188731335
freqs = [23024303.73895735 44085381.74515043 22914544.46834145  7585706.59027375
 50770513.42376783 24215895.58340987  9519576.07846752 31035429.18235327
 23632141.3100814  19522731.20042242]
eta_min = 0.6435974188731354	eta_max = 0.698454705717541
af = 0.003087860896766924	bf = 1.1451029605675662	zeta = 0.003396646986443617	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [1.04231482e-06 8.03693475e-06 1.01601355e-06 3.86114307e-08
 1.23083516e-05 1.33600869e-06 7.63632379e-08 2.69044364e-06
 1.13293465e-06 7.01808322e-07]
ene_total = [1.7338512  0.84809379 1.83437118 1.41097682 0.82214886 0.81764254
 1.40445301 1.24824095 1.68694076 0.79265082]
ti_comp = [0.6447458  0.72246444 0.63593359 0.6730401  0.72478158 0.7250669
 0.67361237 0.68733263 0.64885904 0.72725142]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.90488746e-06 2.89320090e-05 3.82574001e-06 1.42428669e-07
 4.42612736e-05 4.80370504e-06 2.81601583e-07 9.85142590e-06
 4.23451096e-06 2.52087196e-06]
ene_total = [0.52281539 0.2563997  0.55311808 0.42538119 0.24895817 0.24662
 0.4234179  0.3765627  0.50868118 0.23902886]
optimize_network iter = 1 obj = 3.8009831688814426
eta = 0.698454705717541
freqs = [22965892.7521965  43105303.11724808 22914544.46834147  7508078.8429111
 49615295.72405244 23663342.1290412   9420739.8305015  30604647.12122058
 23544761.66323882 19067718.62692055]
eta_min = 0.698454705717542	eta_max = 0.6984547057175405
af = 0.002969273754740996	bf = 1.1451029605675662	zeta = 0.003266201130215096	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [1.03703298e-06 7.68356267e-06 1.01601355e-06 3.78252199e-08
 1.17546026e-05 1.27573473e-06 7.47857989e-08 2.61627349e-06
 1.12457210e-06 6.69475724e-07]
ene_total = [1.73385059 0.84805348 1.83437118 1.41097673 0.8220857  0.81763566
 1.40445283 1.24823249 1.68693981 0.79264713]
ti_comp = [0.6447458  0.72246444 0.63593359 0.6730401  0.72478158 0.7250669
 0.67361237 0.68733263 0.64885904 0.72725142]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.90488746e-06 2.89320090e-05 3.82574001e-06 1.42428669e-07
 4.42612736e-05 4.80370504e-06 2.81601583e-07 9.85142590e-06
 4.23451096e-06 2.52087196e-06]
ene_total = [0.52281539 0.2563997  0.55311808 0.42538119 0.24895817 0.24662
 0.4234179  0.3765627  0.50868118 0.23902886]
optimize_network iter = 2 obj = 3.800983168881437
eta = 0.6984547057175405
freqs = [22965892.7521965  43105303.11724809 22914544.46834146  7508078.8429111
 49615295.72405244 23663342.12904121  9420739.8305015  30604647.12122059
 23544761.66323882 19067718.62692055]
Done!
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.65510535e-06 2.70813288e-05 3.58102069e-06 1.33318001e-07
 4.14300335e-05 4.49642870e-06 2.63588507e-07 9.22126437e-06
 3.96364398e-06 2.35962053e-06]
ene_total = [0.01520218 0.00745374 0.01608333 0.01236923 0.00723638 0.00717091
 0.01231213 0.01094906 0.01479116 0.00695032]
At round 52 energy consumption: 0.11051844173128197
At round 52 eta: 0.6984547057175405
At round 52 a_n: 10.370218813849878
At round 52 local rounds: 11.751716534008594
At round 52 global rounds: 34.39025251090811
gradient difference: 0.5485731363296509
train() client id: f_00000-0-0 loss: 0.999011  [   32/  126]
train() client id: f_00000-0-1 loss: 1.171206  [   64/  126]
train() client id: f_00000-0-2 loss: 1.129348  [   96/  126]
train() client id: f_00000-1-0 loss: 0.809595  [   32/  126]
train() client id: f_00000-1-1 loss: 1.021625  [   64/  126]
train() client id: f_00000-1-2 loss: 0.932889  [   96/  126]
train() client id: f_00000-2-0 loss: 1.109495  [   32/  126]
train() client id: f_00000-2-1 loss: 0.949133  [   64/  126]
train() client id: f_00000-2-2 loss: 0.930118  [   96/  126]
train() client id: f_00000-3-0 loss: 0.832069  [   32/  126]
train() client id: f_00000-3-1 loss: 1.070917  [   64/  126]
train() client id: f_00000-3-2 loss: 0.912998  [   96/  126]
train() client id: f_00000-4-0 loss: 0.885025  [   32/  126]
train() client id: f_00000-4-1 loss: 1.114141  [   64/  126]
train() client id: f_00000-4-2 loss: 0.836782  [   96/  126]
train() client id: f_00000-5-0 loss: 0.891485  [   32/  126]
train() client id: f_00000-5-1 loss: 0.980992  [   64/  126]
train() client id: f_00000-5-2 loss: 0.862111  [   96/  126]
train() client id: f_00000-6-0 loss: 0.940246  [   32/  126]
train() client id: f_00000-6-1 loss: 0.907049  [   64/  126]
train() client id: f_00000-6-2 loss: 0.881876  [   96/  126]
train() client id: f_00000-7-0 loss: 0.813646  [   32/  126]
train() client id: f_00000-7-1 loss: 1.015047  [   64/  126]
train() client id: f_00000-7-2 loss: 0.891833  [   96/  126]
train() client id: f_00000-8-0 loss: 0.848619  [   32/  126]
train() client id: f_00000-8-1 loss: 0.984116  [   64/  126]
train() client id: f_00000-8-2 loss: 0.814299  [   96/  126]
train() client id: f_00000-9-0 loss: 1.074988  [   32/  126]
train() client id: f_00000-9-1 loss: 0.826398  [   64/  126]
train() client id: f_00000-9-2 loss: 0.873727  [   96/  126]
train() client id: f_00000-10-0 loss: 0.947127  [   32/  126]
train() client id: f_00000-10-1 loss: 0.837445  [   64/  126]
train() client id: f_00000-10-2 loss: 1.038616  [   96/  126]
train() client id: f_00001-0-0 loss: 0.507773  [   32/  265]
train() client id: f_00001-0-1 loss: 0.459855  [   64/  265]
train() client id: f_00001-0-2 loss: 0.475751  [   96/  265]
train() client id: f_00001-0-3 loss: 0.464810  [  128/  265]
train() client id: f_00001-0-4 loss: 0.516589  [  160/  265]
train() client id: f_00001-0-5 loss: 0.428720  [  192/  265]
train() client id: f_00001-0-6 loss: 0.437819  [  224/  265]
train() client id: f_00001-0-7 loss: 0.587466  [  256/  265]
train() client id: f_00001-1-0 loss: 0.386166  [   32/  265]
train() client id: f_00001-1-1 loss: 0.515341  [   64/  265]
train() client id: f_00001-1-2 loss: 0.441527  [   96/  265]
train() client id: f_00001-1-3 loss: 0.390182  [  128/  265]
train() client id: f_00001-1-4 loss: 0.593229  [  160/  265]
train() client id: f_00001-1-5 loss: 0.474358  [  192/  265]
train() client id: f_00001-1-6 loss: 0.439264  [  224/  265]
train() client id: f_00001-1-7 loss: 0.643630  [  256/  265]
train() client id: f_00001-2-0 loss: 0.519293  [   32/  265]
train() client id: f_00001-2-1 loss: 0.483428  [   64/  265]
train() client id: f_00001-2-2 loss: 0.555660  [   96/  265]
train() client id: f_00001-2-3 loss: 0.561010  [  128/  265]
train() client id: f_00001-2-4 loss: 0.389844  [  160/  265]
train() client id: f_00001-2-5 loss: 0.423248  [  192/  265]
train() client id: f_00001-2-6 loss: 0.447552  [  224/  265]
train() client id: f_00001-2-7 loss: 0.466870  [  256/  265]
train() client id: f_00001-3-0 loss: 0.361584  [   32/  265]
train() client id: f_00001-3-1 loss: 0.461983  [   64/  265]
train() client id: f_00001-3-2 loss: 0.441941  [   96/  265]
train() client id: f_00001-3-3 loss: 0.499940  [  128/  265]
train() client id: f_00001-3-4 loss: 0.429492  [  160/  265]
train() client id: f_00001-3-5 loss: 0.507833  [  192/  265]
train() client id: f_00001-3-6 loss: 0.613456  [  224/  265]
train() client id: f_00001-3-7 loss: 0.501911  [  256/  265]
train() client id: f_00001-4-0 loss: 0.479147  [   32/  265]
train() client id: f_00001-4-1 loss: 0.474785  [   64/  265]
train() client id: f_00001-4-2 loss: 0.525286  [   96/  265]
train() client id: f_00001-4-3 loss: 0.577316  [  128/  265]
train() client id: f_00001-4-4 loss: 0.474960  [  160/  265]
train() client id: f_00001-4-5 loss: 0.398457  [  192/  265]
train() client id: f_00001-4-6 loss: 0.378707  [  224/  265]
train() client id: f_00001-4-7 loss: 0.456934  [  256/  265]
train() client id: f_00001-5-0 loss: 0.426761  [   32/  265]
train() client id: f_00001-5-1 loss: 0.535236  [   64/  265]
train() client id: f_00001-5-2 loss: 0.455189  [   96/  265]
train() client id: f_00001-5-3 loss: 0.474680  [  128/  265]
train() client id: f_00001-5-4 loss: 0.487917  [  160/  265]
train() client id: f_00001-5-5 loss: 0.499558  [  192/  265]
train() client id: f_00001-5-6 loss: 0.432078  [  224/  265]
train() client id: f_00001-5-7 loss: 0.423570  [  256/  265]
train() client id: f_00001-6-0 loss: 0.515307  [   32/  265]
train() client id: f_00001-6-1 loss: 0.412310  [   64/  265]
train() client id: f_00001-6-2 loss: 0.378286  [   96/  265]
train() client id: f_00001-6-3 loss: 0.614664  [  128/  265]
train() client id: f_00001-6-4 loss: 0.493846  [  160/  265]
train() client id: f_00001-6-5 loss: 0.474755  [  192/  265]
train() client id: f_00001-6-6 loss: 0.382104  [  224/  265]
train() client id: f_00001-6-7 loss: 0.529471  [  256/  265]
train() client id: f_00001-7-0 loss: 0.510782  [   32/  265]
train() client id: f_00001-7-1 loss: 0.461692  [   64/  265]
train() client id: f_00001-7-2 loss: 0.493344  [   96/  265]
train() client id: f_00001-7-3 loss: 0.537899  [  128/  265]
train() client id: f_00001-7-4 loss: 0.461793  [  160/  265]
train() client id: f_00001-7-5 loss: 0.441814  [  192/  265]
train() client id: f_00001-7-6 loss: 0.400573  [  224/  265]
train() client id: f_00001-7-7 loss: 0.488302  [  256/  265]
train() client id: f_00001-8-0 loss: 0.537813  [   32/  265]
train() client id: f_00001-8-1 loss: 0.476607  [   64/  265]
train() client id: f_00001-8-2 loss: 0.382221  [   96/  265]
train() client id: f_00001-8-3 loss: 0.470704  [  128/  265]
train() client id: f_00001-8-4 loss: 0.662803  [  160/  265]
train() client id: f_00001-8-5 loss: 0.415467  [  192/  265]
train() client id: f_00001-8-6 loss: 0.377678  [  224/  265]
train() client id: f_00001-8-7 loss: 0.468127  [  256/  265]
train() client id: f_00001-9-0 loss: 0.605308  [   32/  265]
train() client id: f_00001-9-1 loss: 0.452527  [   64/  265]
train() client id: f_00001-9-2 loss: 0.405561  [   96/  265]
train() client id: f_00001-9-3 loss: 0.557316  [  128/  265]
train() client id: f_00001-9-4 loss: 0.518601  [  160/  265]
train() client id: f_00001-9-5 loss: 0.373717  [  192/  265]
train() client id: f_00001-9-6 loss: 0.485186  [  224/  265]
train() client id: f_00001-9-7 loss: 0.391038  [  256/  265]
train() client id: f_00001-10-0 loss: 0.575302  [   32/  265]
train() client id: f_00001-10-1 loss: 0.385454  [   64/  265]
train() client id: f_00001-10-2 loss: 0.378584  [   96/  265]
train() client id: f_00001-10-3 loss: 0.506951  [  128/  265]
train() client id: f_00001-10-4 loss: 0.467348  [  160/  265]
train() client id: f_00001-10-5 loss: 0.489961  [  192/  265]
train() client id: f_00001-10-6 loss: 0.504441  [  224/  265]
train() client id: f_00001-10-7 loss: 0.495364  [  256/  265]
train() client id: f_00002-0-0 loss: 1.321192  [   32/  124]
train() client id: f_00002-0-1 loss: 1.354881  [   64/  124]
train() client id: f_00002-0-2 loss: 1.106902  [   96/  124]
train() client id: f_00002-1-0 loss: 1.236299  [   32/  124]
train() client id: f_00002-1-1 loss: 1.300272  [   64/  124]
train() client id: f_00002-1-2 loss: 1.197692  [   96/  124]
train() client id: f_00002-2-0 loss: 1.214150  [   32/  124]
train() client id: f_00002-2-1 loss: 1.127952  [   64/  124]
train() client id: f_00002-2-2 loss: 1.199344  [   96/  124]
train() client id: f_00002-3-0 loss: 1.154186  [   32/  124]
train() client id: f_00002-3-1 loss: 1.343543  [   64/  124]
train() client id: f_00002-3-2 loss: 1.009487  [   96/  124]
train() client id: f_00002-4-0 loss: 0.984658  [   32/  124]
train() client id: f_00002-4-1 loss: 1.405874  [   64/  124]
train() client id: f_00002-4-2 loss: 1.139216  [   96/  124]
train() client id: f_00002-5-0 loss: 0.983221  [   32/  124]
train() client id: f_00002-5-1 loss: 1.269288  [   64/  124]
train() client id: f_00002-5-2 loss: 0.908290  [   96/  124]
train() client id: f_00002-6-0 loss: 1.203527  [   32/  124]
train() client id: f_00002-6-1 loss: 1.052748  [   64/  124]
train() client id: f_00002-6-2 loss: 1.108836  [   96/  124]
train() client id: f_00002-7-0 loss: 1.222546  [   32/  124]
train() client id: f_00002-7-1 loss: 1.126716  [   64/  124]
train() client id: f_00002-7-2 loss: 0.904789  [   96/  124]
train() client id: f_00002-8-0 loss: 0.986840  [   32/  124]
train() client id: f_00002-8-1 loss: 1.011897  [   64/  124]
train() client id: f_00002-8-2 loss: 1.236319  [   96/  124]
train() client id: f_00002-9-0 loss: 1.079409  [   32/  124]
train() client id: f_00002-9-1 loss: 1.016870  [   64/  124]
train() client id: f_00002-9-2 loss: 1.032707  [   96/  124]
train() client id: f_00002-10-0 loss: 1.060292  [   32/  124]
train() client id: f_00002-10-1 loss: 1.228576  [   64/  124]
train() client id: f_00002-10-2 loss: 1.049721  [   96/  124]
train() client id: f_00003-0-0 loss: 0.674909  [   32/   43]
train() client id: f_00003-1-0 loss: 0.366195  [   32/   43]
train() client id: f_00003-2-0 loss: 0.440810  [   32/   43]
train() client id: f_00003-3-0 loss: 0.469748  [   32/   43]
train() client id: f_00003-4-0 loss: 0.582766  [   32/   43]
train() client id: f_00003-5-0 loss: 0.422353  [   32/   43]
train() client id: f_00003-6-0 loss: 0.417125  [   32/   43]
train() client id: f_00003-7-0 loss: 0.410933  [   32/   43]
train() client id: f_00003-8-0 loss: 0.500794  [   32/   43]
train() client id: f_00003-9-0 loss: 0.729783  [   32/   43]
train() client id: f_00003-10-0 loss: 0.498547  [   32/   43]
train() client id: f_00004-0-0 loss: 0.836993  [   32/  306]
train() client id: f_00004-0-1 loss: 0.753258  [   64/  306]
train() client id: f_00004-0-2 loss: 0.784826  [   96/  306]
train() client id: f_00004-0-3 loss: 0.729663  [  128/  306]
train() client id: f_00004-0-4 loss: 0.715064  [  160/  306]
train() client id: f_00004-0-5 loss: 0.717482  [  192/  306]
train() client id: f_00004-0-6 loss: 0.914367  [  224/  306]
train() client id: f_00004-0-7 loss: 0.793435  [  256/  306]
train() client id: f_00004-0-8 loss: 0.727716  [  288/  306]
train() client id: f_00004-1-0 loss: 0.612725  [   32/  306]
train() client id: f_00004-1-1 loss: 0.957248  [   64/  306]
train() client id: f_00004-1-2 loss: 0.800247  [   96/  306]
train() client id: f_00004-1-3 loss: 0.869835  [  128/  306]
train() client id: f_00004-1-4 loss: 0.608231  [  160/  306]
train() client id: f_00004-1-5 loss: 0.695186  [  192/  306]
train() client id: f_00004-1-6 loss: 0.952411  [  224/  306]
train() client id: f_00004-1-7 loss: 0.730027  [  256/  306]
train() client id: f_00004-1-8 loss: 0.860137  [  288/  306]
train() client id: f_00004-2-0 loss: 0.843560  [   32/  306]
train() client id: f_00004-2-1 loss: 0.814517  [   64/  306]
train() client id: f_00004-2-2 loss: 0.851158  [   96/  306]
train() client id: f_00004-2-3 loss: 0.739261  [  128/  306]
train() client id: f_00004-2-4 loss: 0.739655  [  160/  306]
train() client id: f_00004-2-5 loss: 0.807657  [  192/  306]
train() client id: f_00004-2-6 loss: 0.706642  [  224/  306]
train() client id: f_00004-2-7 loss: 0.764708  [  256/  306]
train() client id: f_00004-2-8 loss: 0.603198  [  288/  306]
train() client id: f_00004-3-0 loss: 0.796723  [   32/  306]
train() client id: f_00004-3-1 loss: 0.817706  [   64/  306]
train() client id: f_00004-3-2 loss: 0.726198  [   96/  306]
train() client id: f_00004-3-3 loss: 0.697184  [  128/  306]
train() client id: f_00004-3-4 loss: 0.811434  [  160/  306]
train() client id: f_00004-3-5 loss: 0.994568  [  192/  306]
train() client id: f_00004-3-6 loss: 0.588531  [  224/  306]
train() client id: f_00004-3-7 loss: 0.785753  [  256/  306]
train() client id: f_00004-3-8 loss: 0.728869  [  288/  306]
train() client id: f_00004-4-0 loss: 0.756041  [   32/  306]
train() client id: f_00004-4-1 loss: 0.662024  [   64/  306]
train() client id: f_00004-4-2 loss: 0.806463  [   96/  306]
train() client id: f_00004-4-3 loss: 0.914335  [  128/  306]
train() client id: f_00004-4-4 loss: 0.749872  [  160/  306]
train() client id: f_00004-4-5 loss: 0.725480  [  192/  306]
train() client id: f_00004-4-6 loss: 0.740727  [  224/  306]
train() client id: f_00004-4-7 loss: 0.765676  [  256/  306]
train() client id: f_00004-4-8 loss: 0.879552  [  288/  306]
train() client id: f_00004-5-0 loss: 0.767935  [   32/  306]
train() client id: f_00004-5-1 loss: 0.827422  [   64/  306]
train() client id: f_00004-5-2 loss: 0.745032  [   96/  306]
train() client id: f_00004-5-3 loss: 0.802765  [  128/  306]
train() client id: f_00004-5-4 loss: 0.883072  [  160/  306]
train() client id: f_00004-5-5 loss: 0.711426  [  192/  306]
train() client id: f_00004-5-6 loss: 0.706130  [  224/  306]
train() client id: f_00004-5-7 loss: 0.788106  [  256/  306]
train() client id: f_00004-5-8 loss: 0.743760  [  288/  306]
train() client id: f_00004-6-0 loss: 0.687752  [   32/  306]
train() client id: f_00004-6-1 loss: 0.815468  [   64/  306]
train() client id: f_00004-6-2 loss: 0.925290  [   96/  306]
train() client id: f_00004-6-3 loss: 0.709661  [  128/  306]
train() client id: f_00004-6-4 loss: 0.702472  [  160/  306]
train() client id: f_00004-6-5 loss: 0.714996  [  192/  306]
train() client id: f_00004-6-6 loss: 0.934498  [  224/  306]
train() client id: f_00004-6-7 loss: 0.766831  [  256/  306]
train() client id: f_00004-6-8 loss: 0.686893  [  288/  306]
train() client id: f_00004-7-0 loss: 0.679191  [   32/  306]
train() client id: f_00004-7-1 loss: 0.738602  [   64/  306]
train() client id: f_00004-7-2 loss: 0.807836  [   96/  306]
train() client id: f_00004-7-3 loss: 0.724347  [  128/  306]
train() client id: f_00004-7-4 loss: 0.700803  [  160/  306]
train() client id: f_00004-7-5 loss: 0.789902  [  192/  306]
train() client id: f_00004-7-6 loss: 0.784463  [  224/  306]
train() client id: f_00004-7-7 loss: 0.785432  [  256/  306]
train() client id: f_00004-7-8 loss: 0.785131  [  288/  306]
train() client id: f_00004-8-0 loss: 0.789016  [   32/  306]
train() client id: f_00004-8-1 loss: 0.707299  [   64/  306]
train() client id: f_00004-8-2 loss: 0.763690  [   96/  306]
train() client id: f_00004-8-3 loss: 0.721410  [  128/  306]
train() client id: f_00004-8-4 loss: 0.750149  [  160/  306]
train() client id: f_00004-8-5 loss: 0.711288  [  192/  306]
train() client id: f_00004-8-6 loss: 0.844555  [  224/  306]
train() client id: f_00004-8-7 loss: 0.644123  [  256/  306]
train() client id: f_00004-8-8 loss: 0.842214  [  288/  306]
train() client id: f_00004-9-0 loss: 0.793692  [   32/  306]
train() client id: f_00004-9-1 loss: 0.778354  [   64/  306]
train() client id: f_00004-9-2 loss: 0.763241  [   96/  306]
train() client id: f_00004-9-3 loss: 0.691189  [  128/  306]
train() client id: f_00004-9-4 loss: 0.712280  [  160/  306]
train() client id: f_00004-9-5 loss: 0.738567  [  192/  306]
train() client id: f_00004-9-6 loss: 0.727761  [  224/  306]
train() client id: f_00004-9-7 loss: 0.875248  [  256/  306]
train() client id: f_00004-9-8 loss: 0.807334  [  288/  306]
train() client id: f_00004-10-0 loss: 0.780579  [   32/  306]
train() client id: f_00004-10-1 loss: 0.774397  [   64/  306]
train() client id: f_00004-10-2 loss: 0.860384  [   96/  306]
train() client id: f_00004-10-3 loss: 0.769445  [  128/  306]
train() client id: f_00004-10-4 loss: 0.759863  [  160/  306]
train() client id: f_00004-10-5 loss: 0.774745  [  192/  306]
train() client id: f_00004-10-6 loss: 0.706267  [  224/  306]
train() client id: f_00004-10-7 loss: 0.710167  [  256/  306]
train() client id: f_00004-10-8 loss: 0.812701  [  288/  306]
train() client id: f_00005-0-0 loss: 0.613318  [   32/  146]
train() client id: f_00005-0-1 loss: 0.391896  [   64/  146]
train() client id: f_00005-0-2 loss: 0.596610  [   96/  146]
train() client id: f_00005-0-3 loss: 0.436531  [  128/  146]
train() client id: f_00005-1-0 loss: 0.370743  [   32/  146]
train() client id: f_00005-1-1 loss: 0.433343  [   64/  146]
train() client id: f_00005-1-2 loss: 0.569494  [   96/  146]
train() client id: f_00005-1-3 loss: 0.759250  [  128/  146]
train() client id: f_00005-2-0 loss: 0.437249  [   32/  146]
train() client id: f_00005-2-1 loss: 0.273000  [   64/  146]
train() client id: f_00005-2-2 loss: 0.703743  [   96/  146]
train() client id: f_00005-2-3 loss: 0.745142  [  128/  146]
train() client id: f_00005-3-0 loss: 0.618956  [   32/  146]
train() client id: f_00005-3-1 loss: 0.366498  [   64/  146]
train() client id: f_00005-3-2 loss: 0.487330  [   96/  146]
train() client id: f_00005-3-3 loss: 0.457946  [  128/  146]
train() client id: f_00005-4-0 loss: 0.547809  [   32/  146]
train() client id: f_00005-4-1 loss: 0.372844  [   64/  146]
train() client id: f_00005-4-2 loss: 0.521309  [   96/  146]
train() client id: f_00005-4-3 loss: 0.628891  [  128/  146]
train() client id: f_00005-5-0 loss: 0.859526  [   32/  146]
train() client id: f_00005-5-1 loss: 0.421731  [   64/  146]
train() client id: f_00005-5-2 loss: 0.332042  [   96/  146]
train() client id: f_00005-5-3 loss: 0.301566  [  128/  146]
train() client id: f_00005-6-0 loss: 0.348363  [   32/  146]
train() client id: f_00005-6-1 loss: 0.481825  [   64/  146]
train() client id: f_00005-6-2 loss: 0.443212  [   96/  146]
train() client id: f_00005-6-3 loss: 0.581499  [  128/  146]
train() client id: f_00005-7-0 loss: 0.597135  [   32/  146]
train() client id: f_00005-7-1 loss: 0.278041  [   64/  146]
train() client id: f_00005-7-2 loss: 0.476208  [   96/  146]
train() client id: f_00005-7-3 loss: 0.434390  [  128/  146]
train() client id: f_00005-8-0 loss: 0.640419  [   32/  146]
train() client id: f_00005-8-1 loss: 0.360387  [   64/  146]
train() client id: f_00005-8-2 loss: 0.433110  [   96/  146]
train() client id: f_00005-8-3 loss: 0.454574  [  128/  146]
train() client id: f_00005-9-0 loss: 0.344604  [   32/  146]
train() client id: f_00005-9-1 loss: 0.482637  [   64/  146]
train() client id: f_00005-9-2 loss: 0.380027  [   96/  146]
train() client id: f_00005-9-3 loss: 0.636016  [  128/  146]
train() client id: f_00005-10-0 loss: 0.252682  [   32/  146]
train() client id: f_00005-10-1 loss: 0.577945  [   64/  146]
train() client id: f_00005-10-2 loss: 0.541089  [   96/  146]
train() client id: f_00005-10-3 loss: 0.299774  [  128/  146]
train() client id: f_00006-0-0 loss: 0.501517  [   32/   54]
train() client id: f_00006-1-0 loss: 0.460651  [   32/   54]
train() client id: f_00006-2-0 loss: 0.408022  [   32/   54]
train() client id: f_00006-3-0 loss: 0.487514  [   32/   54]
train() client id: f_00006-4-0 loss: 0.490658  [   32/   54]
train() client id: f_00006-5-0 loss: 0.441361  [   32/   54]
train() client id: f_00006-6-0 loss: 0.482222  [   32/   54]
train() client id: f_00006-7-0 loss: 0.453626  [   32/   54]
train() client id: f_00006-8-0 loss: 0.453605  [   32/   54]
train() client id: f_00006-9-0 loss: 0.486812  [   32/   54]
train() client id: f_00006-10-0 loss: 0.482680  [   32/   54]
train() client id: f_00007-0-0 loss: 0.627613  [   32/  179]
train() client id: f_00007-0-1 loss: 0.939356  [   64/  179]
train() client id: f_00007-0-2 loss: 0.708800  [   96/  179]
train() client id: f_00007-0-3 loss: 0.610772  [  128/  179]
train() client id: f_00007-0-4 loss: 0.828333  [  160/  179]
train() client id: f_00007-1-0 loss: 0.757178  [   32/  179]
train() client id: f_00007-1-1 loss: 0.767049  [   64/  179]
train() client id: f_00007-1-2 loss: 0.711767  [   96/  179]
train() client id: f_00007-1-3 loss: 0.660414  [  128/  179]
train() client id: f_00007-1-4 loss: 0.695106  [  160/  179]
train() client id: f_00007-2-0 loss: 0.950318  [   32/  179]
train() client id: f_00007-2-1 loss: 0.704955  [   64/  179]
train() client id: f_00007-2-2 loss: 0.678132  [   96/  179]
train() client id: f_00007-2-3 loss: 0.669570  [  128/  179]
train() client id: f_00007-2-4 loss: 0.605682  [  160/  179]
train() client id: f_00007-3-0 loss: 0.674032  [   32/  179]
train() client id: f_00007-3-1 loss: 0.633241  [   64/  179]
train() client id: f_00007-3-2 loss: 0.611220  [   96/  179]
train() client id: f_00007-3-3 loss: 0.569909  [  128/  179]
train() client id: f_00007-3-4 loss: 1.063624  [  160/  179]
train() client id: f_00007-4-0 loss: 0.751733  [   32/  179]
train() client id: f_00007-4-1 loss: 0.861775  [   64/  179]
train() client id: f_00007-4-2 loss: 0.752194  [   96/  179]
train() client id: f_00007-4-3 loss: 0.560302  [  128/  179]
train() client id: f_00007-4-4 loss: 0.649140  [  160/  179]
train() client id: f_00007-5-0 loss: 0.716906  [   32/  179]
train() client id: f_00007-5-1 loss: 0.733213  [   64/  179]
train() client id: f_00007-5-2 loss: 0.697788  [   96/  179]
train() client id: f_00007-5-3 loss: 0.589430  [  128/  179]
train() client id: f_00007-5-4 loss: 0.594027  [  160/  179]
train() client id: f_00007-6-0 loss: 0.731934  [   32/  179]
train() client id: f_00007-6-1 loss: 0.568010  [   64/  179]
train() client id: f_00007-6-2 loss: 0.737285  [   96/  179]
train() client id: f_00007-6-3 loss: 0.643488  [  128/  179]
train() client id: f_00007-6-4 loss: 0.548739  [  160/  179]
train() client id: f_00007-7-0 loss: 0.771850  [   32/  179]
train() client id: f_00007-7-1 loss: 0.856600  [   64/  179]
train() client id: f_00007-7-2 loss: 0.557145  [   96/  179]
train() client id: f_00007-7-3 loss: 0.621049  [  128/  179]
train() client id: f_00007-7-4 loss: 0.663938  [  160/  179]
train() client id: f_00007-8-0 loss: 0.650699  [   32/  179]
train() client id: f_00007-8-1 loss: 0.725606  [   64/  179]
train() client id: f_00007-8-2 loss: 0.655018  [   96/  179]
train() client id: f_00007-8-3 loss: 0.754130  [  128/  179]
train() client id: f_00007-8-4 loss: 0.569963  [  160/  179]
train() client id: f_00007-9-0 loss: 0.696465  [   32/  179]
train() client id: f_00007-9-1 loss: 0.620282  [   64/  179]
train() client id: f_00007-9-2 loss: 0.739403  [   96/  179]
train() client id: f_00007-9-3 loss: 0.741421  [  128/  179]
train() client id: f_00007-9-4 loss: 0.666664  [  160/  179]
train() client id: f_00007-10-0 loss: 0.587059  [   32/  179]
train() client id: f_00007-10-1 loss: 0.999109  [   64/  179]
train() client id: f_00007-10-2 loss: 0.531671  [   96/  179]
train() client id: f_00007-10-3 loss: 0.719220  [  128/  179]
train() client id: f_00007-10-4 loss: 0.604351  [  160/  179]
train() client id: f_00008-0-0 loss: 0.550123  [   32/  130]
train() client id: f_00008-0-1 loss: 0.651568  [   64/  130]
train() client id: f_00008-0-2 loss: 0.678433  [   96/  130]
train() client id: f_00008-0-3 loss: 0.781453  [  128/  130]
train() client id: f_00008-1-0 loss: 0.614739  [   32/  130]
train() client id: f_00008-1-1 loss: 0.761737  [   64/  130]
train() client id: f_00008-1-2 loss: 0.684608  [   96/  130]
train() client id: f_00008-1-3 loss: 0.615159  [  128/  130]
train() client id: f_00008-2-0 loss: 0.626324  [   32/  130]
train() client id: f_00008-2-1 loss: 0.673720  [   64/  130]
train() client id: f_00008-2-2 loss: 0.637700  [   96/  130]
train() client id: f_00008-2-3 loss: 0.676860  [  128/  130]
train() client id: f_00008-3-0 loss: 0.720328  [   32/  130]
train() client id: f_00008-3-1 loss: 0.767740  [   64/  130]
train() client id: f_00008-3-2 loss: 0.588164  [   96/  130]
train() client id: f_00008-3-3 loss: 0.584134  [  128/  130]
train() client id: f_00008-4-0 loss: 0.527563  [   32/  130]
train() client id: f_00008-4-1 loss: 0.652368  [   64/  130]
train() client id: f_00008-4-2 loss: 0.825908  [   96/  130]
train() client id: f_00008-4-3 loss: 0.676594  [  128/  130]
train() client id: f_00008-5-0 loss: 0.604009  [   32/  130]
train() client id: f_00008-5-1 loss: 0.650201  [   64/  130]
train() client id: f_00008-5-2 loss: 0.554269  [   96/  130]
train() client id: f_00008-5-3 loss: 0.838405  [  128/  130]
train() client id: f_00008-6-0 loss: 0.716458  [   32/  130]
train() client id: f_00008-6-1 loss: 0.680555  [   64/  130]
train() client id: f_00008-6-2 loss: 0.636505  [   96/  130]
train() client id: f_00008-6-3 loss: 0.645842  [  128/  130]
train() client id: f_00008-7-0 loss: 0.582280  [   32/  130]
train() client id: f_00008-7-1 loss: 0.661697  [   64/  130]
train() client id: f_00008-7-2 loss: 0.803400  [   96/  130]
train() client id: f_00008-7-3 loss: 0.631042  [  128/  130]
train() client id: f_00008-8-0 loss: 0.662180  [   32/  130]
train() client id: f_00008-8-1 loss: 0.582292  [   64/  130]
train() client id: f_00008-8-2 loss: 0.623302  [   96/  130]
train() client id: f_00008-8-3 loss: 0.822173  [  128/  130]
train() client id: f_00008-9-0 loss: 0.580881  [   32/  130]
train() client id: f_00008-9-1 loss: 0.715889  [   64/  130]
train() client id: f_00008-9-2 loss: 0.665631  [   96/  130]
train() client id: f_00008-9-3 loss: 0.716825  [  128/  130]
train() client id: f_00008-10-0 loss: 0.546592  [   32/  130]
train() client id: f_00008-10-1 loss: 0.651312  [   64/  130]
train() client id: f_00008-10-2 loss: 0.668306  [   96/  130]
train() client id: f_00008-10-3 loss: 0.815257  [  128/  130]
train() client id: f_00009-0-0 loss: 1.170087  [   32/  118]
train() client id: f_00009-0-1 loss: 1.279364  [   64/  118]
train() client id: f_00009-0-2 loss: 1.053218  [   96/  118]
train() client id: f_00009-1-0 loss: 1.034143  [   32/  118]
train() client id: f_00009-1-1 loss: 1.047558  [   64/  118]
train() client id: f_00009-1-2 loss: 1.164671  [   96/  118]
train() client id: f_00009-2-0 loss: 1.171150  [   32/  118]
train() client id: f_00009-2-1 loss: 1.152107  [   64/  118]
train() client id: f_00009-2-2 loss: 0.913118  [   96/  118]
train() client id: f_00009-3-0 loss: 1.126027  [   32/  118]
train() client id: f_00009-3-1 loss: 0.929599  [   64/  118]
train() client id: f_00009-3-2 loss: 1.060697  [   96/  118]
train() client id: f_00009-4-0 loss: 1.019967  [   32/  118]
train() client id: f_00009-4-1 loss: 0.889036  [   64/  118]
train() client id: f_00009-4-2 loss: 1.011854  [   96/  118]
train() client id: f_00009-5-0 loss: 0.980736  [   32/  118]
train() client id: f_00009-5-1 loss: 1.074631  [   64/  118]
train() client id: f_00009-5-2 loss: 0.898724  [   96/  118]
train() client id: f_00009-6-0 loss: 1.069366  [   32/  118]
train() client id: f_00009-6-1 loss: 0.923760  [   64/  118]
train() client id: f_00009-6-2 loss: 0.886667  [   96/  118]
train() client id: f_00009-7-0 loss: 0.886943  [   32/  118]
train() client id: f_00009-7-1 loss: 1.058730  [   64/  118]
train() client id: f_00009-7-2 loss: 1.013604  [   96/  118]
train() client id: f_00009-8-0 loss: 1.165902  [   32/  118]
train() client id: f_00009-8-1 loss: 0.860161  [   64/  118]
train() client id: f_00009-8-2 loss: 0.890144  [   96/  118]
train() client id: f_00009-9-0 loss: 0.996183  [   32/  118]
train() client id: f_00009-9-1 loss: 0.965061  [   64/  118]
train() client id: f_00009-9-2 loss: 0.953437  [   96/  118]
train() client id: f_00009-10-0 loss: 0.927355  [   32/  118]
train() client id: f_00009-10-1 loss: 0.872863  [   64/  118]
train() client id: f_00009-10-2 loss: 0.980373  [   96/  118]
At round 52 accuracy: 0.6445623342175066
At round 52 training accuracy: 0.5942320590207915
At round 52 training loss: 0.8190562565419336
update_location
xs = -3.905658 4.200318 280.009024 18.811294 0.979296 3.956410 -242.443192 -221.324852 264.663977 -207.060879 
ys = 272.587959 255.555839 1.320614 -242.455176 234.350187 217.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -10.211426168117436
ys mean: 75.8941425355287
dists_uav = 290.378115 274.456607 297.332806 262.941775 254.795936 239.705355 262.270074 242.869031 283.470793 229.978737 
uav_gains = -114.906533 -113.476299 -115.506304 -112.431343 -111.709182 -110.448272 -112.371026 -110.702634 -114.293024 -109.702121 
uav_gains_db_mean: -112.55467381347913
dists_bs = 196.968738 196.473909 487.029507 460.251716 185.717951 184.006652 189.997945 180.232756 466.999925 173.978140 
bs_gains = -103.810184 -103.779596 -114.818607 -114.130931 -103.094968 -102.982398 -103.372028 -102.730403 -114.307930 -102.300910 
bs_gains_db_mean: -106.53279549296072
Round 53
-------------------------------
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.00899084 8.17097489 3.95724982 1.44293614 9.42099912 4.53288609
 1.77835404 5.58181324 4.12270253 3.67526442]
obj_prev = 46.692171130454035
eta_min = 9.633908147429921e-24	eta_max = 0.9382560279743308
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 10.770764989587082	eta = 0.9090909090909091
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 22.73132098521351	eta = 0.4307538722609915
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 16.477782321424517	eta = 0.5942307250446651
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.352697592199675	eta = 0.6377774640049659
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.286662235887265	eta = 0.6405325364618376
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.286409541549396	eta = 0.6405431248831895
eta = 0.6405431248831895
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [0.03675654 0.07730543 0.03617311 0.0125439  0.08926589 0.04259092
 0.0157528  0.05221763 0.03792342 0.0344228 ]
ene_total = [1.46579906 2.36500809 1.47528965 0.70621205 2.69190422 1.38875393
 0.79404133 1.76977105 1.47499189 1.15463827]
ti_comp = [0.82252146 0.90567545 0.81328504 0.85268029 0.90809929 0.90848315
 0.85328562 0.86816903 0.8308949  0.91072419]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [4.58764259e-06 3.52017978e-05 4.47251598e-06 1.69670107e-07
 5.39100853e-05 5.85055712e-06 3.35555868e-07 1.18065477e-05
 4.93753617e-06 3.07358121e-06]
ene_total = [0.44053948 0.20942103 0.46630285 0.35628294 0.2031812  0.20076965
 0.35459888 0.31339904 0.41719008 0.19444042]
optimize_network iter = 0 obj = 3.1561255646288915
eta = 0.6405431248831895
freqs = [22343821.0822694  42678329.73940749 22238886.46292163  7355570.59611931
 49149853.28596357 23440674.99681275  9230675.16871088 30073423.9891545
 22820828.02431027 18898584.29975834]
eta_min = 0.6405431248831939	eta_max = 0.6991595698398908
af = 0.002799252834533405	bf = 1.1332441370306174	zeta = 0.0030791781179867455	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [9.81614110e-07 7.53209970e-06 9.56980564e-07 3.63041731e-08
 1.15350966e-05 1.25183889e-06 7.17986128e-08 2.52623730e-06
 1.05648055e-06 6.57651642e-07]
ene_total = [1.74150662 0.82500401 1.84338565 1.40873715 0.79870952 0.79334107
 1.40206398 1.23816416 1.64915218 0.76855593]
ti_comp = [0.6626492  0.74580319 0.65341278 0.69280803 0.74822703 0.7486109
 0.69341336 0.70829677 0.67102264 0.75085193]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.66565189e-06 2.69213035e-05 3.59332702e-06 1.33286597e-07
 4.11817149e-05 4.46841209e-06 2.63514067e-07 9.19889168e-06
 3.92611172e-06 2.34500816e-06]
ene_total = [0.52634448 0.2499491  0.55712907 0.42570072 0.24234523 0.239842
 0.42368735 0.37437551 0.49844265 0.23230136]
optimize_network iter = 1 obj = 3.770117456851034
eta = 0.6991595698398908
freqs = [22282599.06518497 41639030.99946182 22238886.46292161  7273350.56555898
 47925539.38367743 22854708.69293805  9126001.3904321  29615341.56378339
 22703099.87437893 18416482.47146711]
eta_min = 0.6991595698399091	eta_max = 0.6991595698398846
af = 0.002681430697799304	bf = 1.1332441370306174	zeta = 0.0029495737675792346	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [9.76242239e-07 7.16972436e-06 9.56980564e-07 3.54970983e-08
 1.09675798e-05 1.19003461e-06 7.01794852e-08 2.44986346e-06
 1.04560832e-06 6.24526300e-07]
ene_total = [1.74150603 0.82496404 1.84338565 1.40873707 0.79864692 0.79333425
 1.4020638  1.23815573 1.64915098 0.76855228]
ti_comp = [0.6626492  0.74580319 0.65341278 0.69280803 0.74822703 0.7486109
 0.69341336 0.70829677 0.67102264 0.75085193]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.66565189e-06 2.69213035e-05 3.59332702e-06 1.33286597e-07
 4.11817149e-05 4.46841209e-06 2.63514067e-07 9.19889168e-06
 3.92611172e-06 2.34500816e-06]
ene_total = [0.52634448 0.2499491  0.55712907 0.42570072 0.24234523 0.239842
 0.42368735 0.37437551 0.49844265 0.23230136]
optimize_network iter = 2 obj = 3.770117456850956
eta = 0.6991595698398846
freqs = [22282599.06518495 41639030.99946193 22238886.46292159  7273350.56555898
 47925539.38367755 22854708.6929381   9126001.39043211 29615341.56378343
 22703099.87437893 18416482.47146716]
Done!
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.44084355e-06 2.52702648e-05 3.37295424e-06 1.25112351e-07
 3.86561088e-05 4.19437181e-06 2.47353187e-07 8.63473895e-06
 3.68532982e-06 2.20119270e-06]
ene_total = [0.01579065 0.00749708 0.01671422 0.01277145 0.00726808 0.00719523
 0.01271104 0.01123108 0.01495355 0.00696913]
At round 53 energy consumption: 0.11310150513411767
At round 53 eta: 0.6991595698398846
At round 53 a_n: 10.027672966880559
At round 53 local rounds: 11.718687629159392
At round 53 global rounds: 33.33219860622975
gradient difference: 0.49112367630004883
train() client id: f_00000-0-0 loss: 1.149451  [   32/  126]
train() client id: f_00000-0-1 loss: 0.875241  [   64/  126]
train() client id: f_00000-0-2 loss: 0.940711  [   96/  126]
train() client id: f_00000-1-0 loss: 0.976562  [   32/  126]
train() client id: f_00000-1-1 loss: 0.896221  [   64/  126]
train() client id: f_00000-1-2 loss: 0.854973  [   96/  126]
train() client id: f_00000-2-0 loss: 0.784890  [   32/  126]
train() client id: f_00000-2-1 loss: 0.789174  [   64/  126]
train() client id: f_00000-2-2 loss: 0.910319  [   96/  126]
train() client id: f_00000-3-0 loss: 0.965280  [   32/  126]
train() client id: f_00000-3-1 loss: 0.746798  [   64/  126]
train() client id: f_00000-3-2 loss: 0.827329  [   96/  126]
train() client id: f_00000-4-0 loss: 0.803426  [   32/  126]
train() client id: f_00000-4-1 loss: 0.852652  [   64/  126]
train() client id: f_00000-4-2 loss: 0.656694  [   96/  126]
train() client id: f_00000-5-0 loss: 0.748648  [   32/  126]
train() client id: f_00000-5-1 loss: 0.746089  [   64/  126]
train() client id: f_00000-5-2 loss: 0.799057  [   96/  126]
train() client id: f_00000-6-0 loss: 0.734919  [   32/  126]
train() client id: f_00000-6-1 loss: 0.654704  [   64/  126]
train() client id: f_00000-6-2 loss: 0.850737  [   96/  126]
train() client id: f_00000-7-0 loss: 0.689486  [   32/  126]
train() client id: f_00000-7-1 loss: 0.697935  [   64/  126]
train() client id: f_00000-7-2 loss: 0.833976  [   96/  126]
train() client id: f_00000-8-0 loss: 0.762718  [   32/  126]
train() client id: f_00000-8-1 loss: 0.614673  [   64/  126]
train() client id: f_00000-8-2 loss: 0.661988  [   96/  126]
train() client id: f_00000-9-0 loss: 0.662103  [   32/  126]
train() client id: f_00000-9-1 loss: 0.621274  [   64/  126]
train() client id: f_00000-9-2 loss: 0.767531  [   96/  126]
train() client id: f_00000-10-0 loss: 0.690287  [   32/  126]
train() client id: f_00000-10-1 loss: 0.807717  [   64/  126]
train() client id: f_00000-10-2 loss: 0.658503  [   96/  126]
train() client id: f_00001-0-0 loss: 0.414479  [   32/  265]
train() client id: f_00001-0-1 loss: 0.477455  [   64/  265]
train() client id: f_00001-0-2 loss: 0.473461  [   96/  265]
train() client id: f_00001-0-3 loss: 0.448405  [  128/  265]
train() client id: f_00001-0-4 loss: 0.313026  [  160/  265]
train() client id: f_00001-0-5 loss: 0.463846  [  192/  265]
train() client id: f_00001-0-6 loss: 0.384465  [  224/  265]
train() client id: f_00001-0-7 loss: 0.481462  [  256/  265]
train() client id: f_00001-1-0 loss: 0.376296  [   32/  265]
train() client id: f_00001-1-1 loss: 0.383253  [   64/  265]
train() client id: f_00001-1-2 loss: 0.426454  [   96/  265]
train() client id: f_00001-1-3 loss: 0.407575  [  128/  265]
train() client id: f_00001-1-4 loss: 0.564218  [  160/  265]
train() client id: f_00001-1-5 loss: 0.397998  [  192/  265]
train() client id: f_00001-1-6 loss: 0.362312  [  224/  265]
train() client id: f_00001-1-7 loss: 0.450020  [  256/  265]
train() client id: f_00001-2-0 loss: 0.403351  [   32/  265]
train() client id: f_00001-2-1 loss: 0.397337  [   64/  265]
train() client id: f_00001-2-2 loss: 0.473898  [   96/  265]
train() client id: f_00001-2-3 loss: 0.362422  [  128/  265]
train() client id: f_00001-2-4 loss: 0.330908  [  160/  265]
train() client id: f_00001-2-5 loss: 0.486921  [  192/  265]
train() client id: f_00001-2-6 loss: 0.450789  [  224/  265]
train() client id: f_00001-2-7 loss: 0.452643  [  256/  265]
train() client id: f_00001-3-0 loss: 0.315755  [   32/  265]
train() client id: f_00001-3-1 loss: 0.389287  [   64/  265]
train() client id: f_00001-3-2 loss: 0.398076  [   96/  265]
train() client id: f_00001-3-3 loss: 0.428718  [  128/  265]
train() client id: f_00001-3-4 loss: 0.289620  [  160/  265]
train() client id: f_00001-3-5 loss: 0.482959  [  192/  265]
train() client id: f_00001-3-6 loss: 0.472876  [  224/  265]
train() client id: f_00001-3-7 loss: 0.549432  [  256/  265]
train() client id: f_00001-4-0 loss: 0.342866  [   32/  265]
train() client id: f_00001-4-1 loss: 0.531332  [   64/  265]
train() client id: f_00001-4-2 loss: 0.300186  [   96/  265]
train() client id: f_00001-4-3 loss: 0.386808  [  128/  265]
train() client id: f_00001-4-4 loss: 0.501850  [  160/  265]
train() client id: f_00001-4-5 loss: 0.368563  [  192/  265]
train() client id: f_00001-4-6 loss: 0.445803  [  224/  265]
train() client id: f_00001-4-7 loss: 0.366935  [  256/  265]
train() client id: f_00001-5-0 loss: 0.364183  [   32/  265]
train() client id: f_00001-5-1 loss: 0.386046  [   64/  265]
train() client id: f_00001-5-2 loss: 0.339281  [   96/  265]
train() client id: f_00001-5-3 loss: 0.483242  [  128/  265]
train() client id: f_00001-5-4 loss: 0.380003  [  160/  265]
train() client id: f_00001-5-5 loss: 0.552594  [  192/  265]
train() client id: f_00001-5-6 loss: 0.427936  [  224/  265]
train() client id: f_00001-5-7 loss: 0.310498  [  256/  265]
train() client id: f_00001-6-0 loss: 0.491767  [   32/  265]
train() client id: f_00001-6-1 loss: 0.293481  [   64/  265]
train() client id: f_00001-6-2 loss: 0.402841  [   96/  265]
train() client id: f_00001-6-3 loss: 0.455526  [  128/  265]
train() client id: f_00001-6-4 loss: 0.373290  [  160/  265]
train() client id: f_00001-6-5 loss: 0.477467  [  192/  265]
train() client id: f_00001-6-6 loss: 0.299675  [  224/  265]
train() client id: f_00001-6-7 loss: 0.472962  [  256/  265]
train() client id: f_00001-7-0 loss: 0.497348  [   32/  265]
train() client id: f_00001-7-1 loss: 0.297722  [   64/  265]
train() client id: f_00001-7-2 loss: 0.440142  [   96/  265]
train() client id: f_00001-7-3 loss: 0.384988  [  128/  265]
train() client id: f_00001-7-4 loss: 0.391075  [  160/  265]
train() client id: f_00001-7-5 loss: 0.444806  [  192/  265]
train() client id: f_00001-7-6 loss: 0.450581  [  224/  265]
train() client id: f_00001-7-7 loss: 0.376852  [  256/  265]
train() client id: f_00001-8-0 loss: 0.378969  [   32/  265]
train() client id: f_00001-8-1 loss: 0.400933  [   64/  265]
train() client id: f_00001-8-2 loss: 0.383334  [   96/  265]
train() client id: f_00001-8-3 loss: 0.417551  [  128/  265]
train() client id: f_00001-8-4 loss: 0.469904  [  160/  265]
train() client id: f_00001-8-5 loss: 0.441471  [  192/  265]
train() client id: f_00001-8-6 loss: 0.386919  [  224/  265]
train() client id: f_00001-8-7 loss: 0.323465  [  256/  265]
train() client id: f_00001-9-0 loss: 0.385419  [   32/  265]
train() client id: f_00001-9-1 loss: 0.306910  [   64/  265]
train() client id: f_00001-9-2 loss: 0.579881  [   96/  265]
train() client id: f_00001-9-3 loss: 0.409677  [  128/  265]
train() client id: f_00001-9-4 loss: 0.317499  [  160/  265]
train() client id: f_00001-9-5 loss: 0.314754  [  192/  265]
train() client id: f_00001-9-6 loss: 0.458700  [  224/  265]
train() client id: f_00001-9-7 loss: 0.415580  [  256/  265]
train() client id: f_00001-10-0 loss: 0.365791  [   32/  265]
train() client id: f_00001-10-1 loss: 0.319771  [   64/  265]
train() client id: f_00001-10-2 loss: 0.399428  [   96/  265]
train() client id: f_00001-10-3 loss: 0.408338  [  128/  265]
train() client id: f_00001-10-4 loss: 0.428528  [  160/  265]
train() client id: f_00001-10-5 loss: 0.453045  [  192/  265]
train() client id: f_00001-10-6 loss: 0.510551  [  224/  265]
train() client id: f_00001-10-7 loss: 0.408279  [  256/  265]
train() client id: f_00002-0-0 loss: 1.142067  [   32/  124]
train() client id: f_00002-0-1 loss: 1.143751  [   64/  124]
train() client id: f_00002-0-2 loss: 1.179197  [   96/  124]
train() client id: f_00002-1-0 loss: 1.066295  [   32/  124]
train() client id: f_00002-1-1 loss: 1.199709  [   64/  124]
train() client id: f_00002-1-2 loss: 1.298434  [   96/  124]
train() client id: f_00002-2-0 loss: 1.095555  [   32/  124]
train() client id: f_00002-2-1 loss: 1.142434  [   64/  124]
train() client id: f_00002-2-2 loss: 1.187348  [   96/  124]
train() client id: f_00002-3-0 loss: 1.152172  [   32/  124]
train() client id: f_00002-3-1 loss: 1.164050  [   64/  124]
train() client id: f_00002-3-2 loss: 1.054830  [   96/  124]
train() client id: f_00002-4-0 loss: 1.027510  [   32/  124]
train() client id: f_00002-4-1 loss: 1.001225  [   64/  124]
train() client id: f_00002-4-2 loss: 1.141656  [   96/  124]
train() client id: f_00002-5-0 loss: 0.885390  [   32/  124]
train() client id: f_00002-5-1 loss: 1.054505  [   64/  124]
train() client id: f_00002-5-2 loss: 1.050447  [   96/  124]
train() client id: f_00002-6-0 loss: 0.966524  [   32/  124]
train() client id: f_00002-6-1 loss: 1.043134  [   64/  124]
train() client id: f_00002-6-2 loss: 1.078341  [   96/  124]
train() client id: f_00002-7-0 loss: 1.131742  [   32/  124]
train() client id: f_00002-7-1 loss: 1.007519  [   64/  124]
train() client id: f_00002-7-2 loss: 0.847558  [   96/  124]
train() client id: f_00002-8-0 loss: 1.067406  [   32/  124]
train() client id: f_00002-8-1 loss: 0.944008  [   64/  124]
train() client id: f_00002-8-2 loss: 1.033523  [   96/  124]
train() client id: f_00002-9-0 loss: 0.937680  [   32/  124]
train() client id: f_00002-9-1 loss: 1.145454  [   64/  124]
train() client id: f_00002-9-2 loss: 1.065704  [   96/  124]
train() client id: f_00002-10-0 loss: 1.220633  [   32/  124]
train() client id: f_00002-10-1 loss: 1.036582  [   64/  124]
train() client id: f_00002-10-2 loss: 0.945185  [   96/  124]
train() client id: f_00003-0-0 loss: 0.696299  [   32/   43]
train() client id: f_00003-1-0 loss: 0.831893  [   32/   43]
train() client id: f_00003-2-0 loss: 0.810062  [   32/   43]
train() client id: f_00003-3-0 loss: 0.727764  [   32/   43]
train() client id: f_00003-4-0 loss: 0.634304  [   32/   43]
train() client id: f_00003-5-0 loss: 0.632285  [   32/   43]
train() client id: f_00003-6-0 loss: 0.667488  [   32/   43]
train() client id: f_00003-7-0 loss: 0.543179  [   32/   43]
train() client id: f_00003-8-0 loss: 0.563323  [   32/   43]
train() client id: f_00003-9-0 loss: 0.633392  [   32/   43]
train() client id: f_00003-10-0 loss: 0.835863  [   32/   43]
train() client id: f_00004-0-0 loss: 0.698113  [   32/  306]
train() client id: f_00004-0-1 loss: 0.714015  [   64/  306]
train() client id: f_00004-0-2 loss: 0.882808  [   96/  306]
train() client id: f_00004-0-3 loss: 0.737868  [  128/  306]
train() client id: f_00004-0-4 loss: 0.829798  [  160/  306]
train() client id: f_00004-0-5 loss: 0.786071  [  192/  306]
train() client id: f_00004-0-6 loss: 0.799547  [  224/  306]
train() client id: f_00004-0-7 loss: 0.764904  [  256/  306]
train() client id: f_00004-0-8 loss: 0.781156  [  288/  306]
train() client id: f_00004-1-0 loss: 0.726957  [   32/  306]
train() client id: f_00004-1-1 loss: 0.780228  [   64/  306]
train() client id: f_00004-1-2 loss: 0.867076  [   96/  306]
train() client id: f_00004-1-3 loss: 0.696407  [  128/  306]
train() client id: f_00004-1-4 loss: 0.597625  [  160/  306]
train() client id: f_00004-1-5 loss: 0.795682  [  192/  306]
train() client id: f_00004-1-6 loss: 0.928798  [  224/  306]
train() client id: f_00004-1-7 loss: 0.767423  [  256/  306]
train() client id: f_00004-1-8 loss: 0.872823  [  288/  306]
train() client id: f_00004-2-0 loss: 0.888341  [   32/  306]
train() client id: f_00004-2-1 loss: 0.816504  [   64/  306]
train() client id: f_00004-2-2 loss: 0.779346  [   96/  306]
train() client id: f_00004-2-3 loss: 0.831528  [  128/  306]
train() client id: f_00004-2-4 loss: 0.785066  [  160/  306]
train() client id: f_00004-2-5 loss: 0.861943  [  192/  306]
train() client id: f_00004-2-6 loss: 0.587937  [  224/  306]
train() client id: f_00004-2-7 loss: 0.720996  [  256/  306]
train() client id: f_00004-2-8 loss: 0.757030  [  288/  306]
train() client id: f_00004-3-0 loss: 0.820717  [   32/  306]
train() client id: f_00004-3-1 loss: 0.731970  [   64/  306]
train() client id: f_00004-3-2 loss: 0.716975  [   96/  306]
train() client id: f_00004-3-3 loss: 0.797220  [  128/  306]
train() client id: f_00004-3-4 loss: 0.814009  [  160/  306]
train() client id: f_00004-3-5 loss: 0.795404  [  192/  306]
train() client id: f_00004-3-6 loss: 0.671943  [  224/  306]
train() client id: f_00004-3-7 loss: 0.887189  [  256/  306]
train() client id: f_00004-3-8 loss: 0.798764  [  288/  306]
train() client id: f_00004-4-0 loss: 0.849155  [   32/  306]
train() client id: f_00004-4-1 loss: 0.731650  [   64/  306]
train() client id: f_00004-4-2 loss: 0.805386  [   96/  306]
train() client id: f_00004-4-3 loss: 0.794777  [  128/  306]
train() client id: f_00004-4-4 loss: 0.762108  [  160/  306]
train() client id: f_00004-4-5 loss: 0.691099  [  192/  306]
train() client id: f_00004-4-6 loss: 0.746668  [  224/  306]
train() client id: f_00004-4-7 loss: 0.700609  [  256/  306]
train() client id: f_00004-4-8 loss: 0.839970  [  288/  306]
train() client id: f_00004-5-0 loss: 0.717489  [   32/  306]
train() client id: f_00004-5-1 loss: 0.714964  [   64/  306]
train() client id: f_00004-5-2 loss: 0.703756  [   96/  306]
train() client id: f_00004-5-3 loss: 0.902981  [  128/  306]
train() client id: f_00004-5-4 loss: 0.719097  [  160/  306]
train() client id: f_00004-5-5 loss: 0.857209  [  192/  306]
train() client id: f_00004-5-6 loss: 0.829477  [  224/  306]
train() client id: f_00004-5-7 loss: 0.788133  [  256/  306]
train() client id: f_00004-5-8 loss: 0.788572  [  288/  306]
train() client id: f_00004-6-0 loss: 0.813795  [   32/  306]
train() client id: f_00004-6-1 loss: 0.746148  [   64/  306]
train() client id: f_00004-6-2 loss: 0.888856  [   96/  306]
train() client id: f_00004-6-3 loss: 0.665385  [  128/  306]
train() client id: f_00004-6-4 loss: 0.746632  [  160/  306]
train() client id: f_00004-6-5 loss: 0.801056  [  192/  306]
train() client id: f_00004-6-6 loss: 0.858185  [  224/  306]
train() client id: f_00004-6-7 loss: 0.710993  [  256/  306]
train() client id: f_00004-6-8 loss: 0.729715  [  288/  306]
train() client id: f_00004-7-0 loss: 0.734185  [   32/  306]
train() client id: f_00004-7-1 loss: 0.777203  [   64/  306]
train() client id: f_00004-7-2 loss: 0.964203  [   96/  306]
train() client id: f_00004-7-3 loss: 0.626297  [  128/  306]
train() client id: f_00004-7-4 loss: 0.842221  [  160/  306]
train() client id: f_00004-7-5 loss: 0.773827  [  192/  306]
train() client id: f_00004-7-6 loss: 0.804998  [  224/  306]
train() client id: f_00004-7-7 loss: 0.928219  [  256/  306]
train() client id: f_00004-7-8 loss: 0.630835  [  288/  306]
train() client id: f_00004-8-0 loss: 0.898130  [   32/  306]
train() client id: f_00004-8-1 loss: 0.692987  [   64/  306]
train() client id: f_00004-8-2 loss: 0.752192  [   96/  306]
train() client id: f_00004-8-3 loss: 0.852944  [  128/  306]
train() client id: f_00004-8-4 loss: 0.808947  [  160/  306]
train() client id: f_00004-8-5 loss: 0.694665  [  192/  306]
train() client id: f_00004-8-6 loss: 0.758140  [  224/  306]
train() client id: f_00004-8-7 loss: 0.778451  [  256/  306]
train() client id: f_00004-8-8 loss: 0.770270  [  288/  306]
train() client id: f_00004-9-0 loss: 0.745091  [   32/  306]
train() client id: f_00004-9-1 loss: 0.764545  [   64/  306]
train() client id: f_00004-9-2 loss: 0.800931  [   96/  306]
train() client id: f_00004-9-3 loss: 0.744759  [  128/  306]
train() client id: f_00004-9-4 loss: 0.660243  [  160/  306]
train() client id: f_00004-9-5 loss: 0.748989  [  192/  306]
train() client id: f_00004-9-6 loss: 0.930020  [  224/  306]
train() client id: f_00004-9-7 loss: 0.766489  [  256/  306]
train() client id: f_00004-9-8 loss: 0.809835  [  288/  306]
train() client id: f_00004-10-0 loss: 0.809058  [   32/  306]
train() client id: f_00004-10-1 loss: 0.801592  [   64/  306]
train() client id: f_00004-10-2 loss: 0.806693  [   96/  306]
train() client id: f_00004-10-3 loss: 0.772612  [  128/  306]
train() client id: f_00004-10-4 loss: 0.698824  [  160/  306]
train() client id: f_00004-10-5 loss: 0.778513  [  192/  306]
train() client id: f_00004-10-6 loss: 0.817699  [  224/  306]
train() client id: f_00004-10-7 loss: 0.768894  [  256/  306]
train() client id: f_00004-10-8 loss: 0.723391  [  288/  306]
train() client id: f_00005-0-0 loss: 0.504218  [   32/  146]
train() client id: f_00005-0-1 loss: 0.820822  [   64/  146]
train() client id: f_00005-0-2 loss: 0.769020  [   96/  146]
train() client id: f_00005-0-3 loss: 0.912716  [  128/  146]
train() client id: f_00005-1-0 loss: 0.839436  [   32/  146]
train() client id: f_00005-1-1 loss: 0.803493  [   64/  146]
train() client id: f_00005-1-2 loss: 0.788430  [   96/  146]
train() client id: f_00005-1-3 loss: 0.625241  [  128/  146]
train() client id: f_00005-2-0 loss: 0.785385  [   32/  146]
train() client id: f_00005-2-1 loss: 0.979620  [   64/  146]
train() client id: f_00005-2-2 loss: 0.735273  [   96/  146]
train() client id: f_00005-2-3 loss: 0.619641  [  128/  146]
train() client id: f_00005-3-0 loss: 0.598056  [   32/  146]
train() client id: f_00005-3-1 loss: 0.758773  [   64/  146]
train() client id: f_00005-3-2 loss: 0.509781  [   96/  146]
train() client id: f_00005-3-3 loss: 1.060872  [  128/  146]
train() client id: f_00005-4-0 loss: 0.806690  [   32/  146]
train() client id: f_00005-4-1 loss: 0.814761  [   64/  146]
train() client id: f_00005-4-2 loss: 0.739672  [   96/  146]
train() client id: f_00005-4-3 loss: 0.697416  [  128/  146]
train() client id: f_00005-5-0 loss: 0.560878  [   32/  146]
train() client id: f_00005-5-1 loss: 1.029779  [   64/  146]
train() client id: f_00005-5-2 loss: 0.793248  [   96/  146]
train() client id: f_00005-5-3 loss: 0.673542  [  128/  146]
train() client id: f_00005-6-0 loss: 0.615115  [   32/  146]
train() client id: f_00005-6-1 loss: 0.687820  [   64/  146]
train() client id: f_00005-6-2 loss: 0.905103  [   96/  146]
train() client id: f_00005-6-3 loss: 0.833446  [  128/  146]
train() client id: f_00005-7-0 loss: 0.925666  [   32/  146]
train() client id: f_00005-7-1 loss: 0.582150  [   64/  146]
train() client id: f_00005-7-2 loss: 0.776675  [   96/  146]
train() client id: f_00005-7-3 loss: 0.833998  [  128/  146]
train() client id: f_00005-8-0 loss: 0.846532  [   32/  146]
train() client id: f_00005-8-1 loss: 0.609238  [   64/  146]
train() client id: f_00005-8-2 loss: 0.846102  [   96/  146]
train() client id: f_00005-8-3 loss: 0.810976  [  128/  146]
train() client id: f_00005-9-0 loss: 1.168862  [   32/  146]
train() client id: f_00005-9-1 loss: 0.839997  [   64/  146]
train() client id: f_00005-9-2 loss: 0.708694  [   96/  146]
train() client id: f_00005-9-3 loss: 0.564661  [  128/  146]
train() client id: f_00005-10-0 loss: 0.687567  [   32/  146]
train() client id: f_00005-10-1 loss: 0.627083  [   64/  146]
train() client id: f_00005-10-2 loss: 0.774171  [   96/  146]
train() client id: f_00005-10-3 loss: 1.089418  [  128/  146]
train() client id: f_00006-0-0 loss: 0.487915  [   32/   54]
train() client id: f_00006-1-0 loss: 0.428754  [   32/   54]
train() client id: f_00006-2-0 loss: 0.449446  [   32/   54]
train() client id: f_00006-3-0 loss: 0.478079  [   32/   54]
train() client id: f_00006-4-0 loss: 0.485003  [   32/   54]
train() client id: f_00006-5-0 loss: 0.482273  [   32/   54]
train() client id: f_00006-6-0 loss: 0.442489  [   32/   54]
train() client id: f_00006-7-0 loss: 0.365420  [   32/   54]
train() client id: f_00006-8-0 loss: 0.416724  [   32/   54]
train() client id: f_00006-9-0 loss: 0.478303  [   32/   54]
train() client id: f_00006-10-0 loss: 0.473494  [   32/   54]
train() client id: f_00007-0-0 loss: 0.644027  [   32/  179]
train() client id: f_00007-0-1 loss: 0.765237  [   64/  179]
train() client id: f_00007-0-2 loss: 0.545936  [   96/  179]
train() client id: f_00007-0-3 loss: 0.562766  [  128/  179]
train() client id: f_00007-0-4 loss: 0.822455  [  160/  179]
train() client id: f_00007-1-0 loss: 0.617791  [   32/  179]
train() client id: f_00007-1-1 loss: 0.773306  [   64/  179]
train() client id: f_00007-1-2 loss: 0.484968  [   96/  179]
train() client id: f_00007-1-3 loss: 0.583586  [  128/  179]
train() client id: f_00007-1-4 loss: 0.537316  [  160/  179]
train() client id: f_00007-2-0 loss: 0.535580  [   32/  179]
train() client id: f_00007-2-1 loss: 0.493978  [   64/  179]
train() client id: f_00007-2-2 loss: 0.743412  [   96/  179]
train() client id: f_00007-2-3 loss: 0.576486  [  128/  179]
train() client id: f_00007-2-4 loss: 0.614986  [  160/  179]
train() client id: f_00007-3-0 loss: 0.507705  [   32/  179]
train() client id: f_00007-3-1 loss: 0.841115  [   64/  179]
train() client id: f_00007-3-2 loss: 0.404274  [   96/  179]
train() client id: f_00007-3-3 loss: 0.573770  [  128/  179]
train() client id: f_00007-3-4 loss: 0.791296  [  160/  179]
train() client id: f_00007-4-0 loss: 0.583106  [   32/  179]
train() client id: f_00007-4-1 loss: 0.547217  [   64/  179]
train() client id: f_00007-4-2 loss: 0.845124  [   96/  179]
train() client id: f_00007-4-3 loss: 0.587894  [  128/  179]
train() client id: f_00007-4-4 loss: 0.441719  [  160/  179]
train() client id: f_00007-5-0 loss: 0.617545  [   32/  179]
train() client id: f_00007-5-1 loss: 0.449584  [   64/  179]
train() client id: f_00007-5-2 loss: 0.661217  [   96/  179]
train() client id: f_00007-5-3 loss: 0.558408  [  128/  179]
train() client id: f_00007-5-4 loss: 0.609636  [  160/  179]
train() client id: f_00007-6-0 loss: 0.531235  [   32/  179]
train() client id: f_00007-6-1 loss: 0.534539  [   64/  179]
train() client id: f_00007-6-2 loss: 0.584873  [   96/  179]
train() client id: f_00007-6-3 loss: 0.591355  [  128/  179]
train() client id: f_00007-6-4 loss: 0.560714  [  160/  179]
train() client id: f_00007-7-0 loss: 0.689331  [   32/  179]
train() client id: f_00007-7-1 loss: 0.448015  [   64/  179]
train() client id: f_00007-7-2 loss: 0.498810  [   96/  179]
train() client id: f_00007-7-3 loss: 0.761426  [  128/  179]
train() client id: f_00007-7-4 loss: 0.626104  [  160/  179]
train() client id: f_00007-8-0 loss: 0.609678  [   32/  179]
train() client id: f_00007-8-1 loss: 0.511635  [   64/  179]
train() client id: f_00007-8-2 loss: 0.865540  [   96/  179]
train() client id: f_00007-8-3 loss: 0.423550  [  128/  179]
train() client id: f_00007-8-4 loss: 0.610387  [  160/  179]
train() client id: f_00007-9-0 loss: 0.745077  [   32/  179]
train() client id: f_00007-9-1 loss: 0.539138  [   64/  179]
train() client id: f_00007-9-2 loss: 0.498463  [   96/  179]
train() client id: f_00007-9-3 loss: 0.675178  [  128/  179]
train() client id: f_00007-9-4 loss: 0.450685  [  160/  179]
train() client id: f_00007-10-0 loss: 0.397366  [   32/  179]
train() client id: f_00007-10-1 loss: 0.629091  [   64/  179]
train() client id: f_00007-10-2 loss: 0.523789  [   96/  179]
train() client id: f_00007-10-3 loss: 0.625076  [  128/  179]
train() client id: f_00007-10-4 loss: 0.821967  [  160/  179]
train() client id: f_00008-0-0 loss: 0.712577  [   32/  130]
train() client id: f_00008-0-1 loss: 0.565670  [   64/  130]
train() client id: f_00008-0-2 loss: 0.641513  [   96/  130]
train() client id: f_00008-0-3 loss: 0.569491  [  128/  130]
train() client id: f_00008-1-0 loss: 0.639041  [   32/  130]
train() client id: f_00008-1-1 loss: 0.545212  [   64/  130]
train() client id: f_00008-1-2 loss: 0.597298  [   96/  130]
train() client id: f_00008-1-3 loss: 0.701079  [  128/  130]
train() client id: f_00008-2-0 loss: 0.693076  [   32/  130]
train() client id: f_00008-2-1 loss: 0.659258  [   64/  130]
train() client id: f_00008-2-2 loss: 0.524374  [   96/  130]
train() client id: f_00008-2-3 loss: 0.621614  [  128/  130]
train() client id: f_00008-3-0 loss: 0.633611  [   32/  130]
train() client id: f_00008-3-1 loss: 0.604758  [   64/  130]
train() client id: f_00008-3-2 loss: 0.618200  [   96/  130]
train() client id: f_00008-3-3 loss: 0.621226  [  128/  130]
train() client id: f_00008-4-0 loss: 0.606357  [   32/  130]
train() client id: f_00008-4-1 loss: 0.597932  [   64/  130]
train() client id: f_00008-4-2 loss: 0.593820  [   96/  130]
train() client id: f_00008-4-3 loss: 0.693635  [  128/  130]
train() client id: f_00008-5-0 loss: 0.681249  [   32/  130]
train() client id: f_00008-5-1 loss: 0.500284  [   64/  130]
train() client id: f_00008-5-2 loss: 0.715360  [   96/  130]
train() client id: f_00008-5-3 loss: 0.596798  [  128/  130]
train() client id: f_00008-6-0 loss: 0.604888  [   32/  130]
train() client id: f_00008-6-1 loss: 0.592571  [   64/  130]
train() client id: f_00008-6-2 loss: 0.593424  [   96/  130]
train() client id: f_00008-6-3 loss: 0.691596  [  128/  130]
train() client id: f_00008-7-0 loss: 0.513188  [   32/  130]
train() client id: f_00008-7-1 loss: 0.563683  [   64/  130]
train() client id: f_00008-7-2 loss: 0.705285  [   96/  130]
train() client id: f_00008-7-3 loss: 0.691876  [  128/  130]
train() client id: f_00008-8-0 loss: 0.484233  [   32/  130]
train() client id: f_00008-8-1 loss: 0.672452  [   64/  130]
train() client id: f_00008-8-2 loss: 0.701511  [   96/  130]
train() client id: f_00008-8-3 loss: 0.615291  [  128/  130]
train() client id: f_00008-9-0 loss: 0.618891  [   32/  130]
train() client id: f_00008-9-1 loss: 0.700546  [   64/  130]
train() client id: f_00008-9-2 loss: 0.507434  [   96/  130]
train() client id: f_00008-9-3 loss: 0.656433  [  128/  130]
train() client id: f_00008-10-0 loss: 0.644834  [   32/  130]
train() client id: f_00008-10-1 loss: 0.591110  [   64/  130]
train() client id: f_00008-10-2 loss: 0.582749  [   96/  130]
train() client id: f_00008-10-3 loss: 0.672637  [  128/  130]
train() client id: f_00009-0-0 loss: 0.985066  [   32/  118]
train() client id: f_00009-0-1 loss: 0.905480  [   64/  118]
train() client id: f_00009-0-2 loss: 1.073467  [   96/  118]
train() client id: f_00009-1-0 loss: 1.104969  [   32/  118]
train() client id: f_00009-1-1 loss: 0.994372  [   64/  118]
train() client id: f_00009-1-2 loss: 0.768163  [   96/  118]
train() client id: f_00009-2-0 loss: 0.844942  [   32/  118]
train() client id: f_00009-2-1 loss: 0.903439  [   64/  118]
train() client id: f_00009-2-2 loss: 0.949850  [   96/  118]
train() client id: f_00009-3-0 loss: 0.945617  [   32/  118]
train() client id: f_00009-3-1 loss: 0.717606  [   64/  118]
train() client id: f_00009-3-2 loss: 0.917167  [   96/  118]
train() client id: f_00009-4-0 loss: 0.939477  [   32/  118]
train() client id: f_00009-4-1 loss: 0.694744  [   64/  118]
train() client id: f_00009-4-2 loss: 0.878414  [   96/  118]
train() client id: f_00009-5-0 loss: 0.692009  [   32/  118]
train() client id: f_00009-5-1 loss: 0.992389  [   64/  118]
train() client id: f_00009-5-2 loss: 0.816727  [   96/  118]
train() client id: f_00009-6-0 loss: 0.997720  [   32/  118]
train() client id: f_00009-6-1 loss: 0.691322  [   64/  118]
train() client id: f_00009-6-2 loss: 0.755013  [   96/  118]
train() client id: f_00009-7-0 loss: 0.738624  [   32/  118]
train() client id: f_00009-7-1 loss: 0.701423  [   64/  118]
train() client id: f_00009-7-2 loss: 0.894645  [   96/  118]
train() client id: f_00009-8-0 loss: 0.837021  [   32/  118]
train() client id: f_00009-8-1 loss: 0.776695  [   64/  118]
train() client id: f_00009-8-2 loss: 0.782367  [   96/  118]
train() client id: f_00009-9-0 loss: 0.835539  [   32/  118]
train() client id: f_00009-9-1 loss: 0.798418  [   64/  118]
train() client id: f_00009-9-2 loss: 0.847550  [   96/  118]
train() client id: f_00009-10-0 loss: 0.691559  [   32/  118]
train() client id: f_00009-10-1 loss: 0.710258  [   64/  118]
train() client id: f_00009-10-2 loss: 0.907001  [   96/  118]
At round 53 accuracy: 0.6445623342175066
At round 53 training accuracy: 0.5888665325285044
At round 53 training loss: 0.8257656366343422
update_location
xs = -3.905658 4.200318 285.009024 18.811294 0.979296 3.956410 -247.443192 -226.324852 269.663977 -212.060879 
ys = 277.587959 260.555839 1.320614 -247.455176 239.350187 222.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -10.711426168117436
ys mean: 77.3941425355287
dists_uav = 295.076820 279.118233 302.046168 267.559206 259.402141 244.257649 266.898902 247.434062 288.144634 234.490571 
uav_gains = -115.314060 -113.900108 -115.899930 -112.848554 -112.114880 -110.816052 -112.788663 -111.079425 -114.709818 -110.041505 
uav_gains_db_mean: -112.95129939058637
dists_bs = 199.493266 198.576322 491.704008 464.791559 187.375717 185.233878 191.829745 181.582199 471.710261 174.968574 
bs_gains = -103.965050 -103.909028 -114.934764 -114.250290 -103.203032 -103.063231 -103.488706 -102.821111 -114.429968 -102.369941 
bs_gains_db_mean: -106.6435119622231
Round 54
-------------------------------
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.87808648 7.89231983 3.82850312 1.39788506 9.0995415  4.37831172
 1.72181282 5.39414421 3.98347864 3.54993313]
obj_prev = 45.124016510283404
eta_min = 1.5688388831448793e-24	eta_max = 0.9379204733020944
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 10.402835079222914	eta = 0.9090909090909091
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 22.247241049856704	eta = 0.4250919373822522
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 16.022057938092566	eta = 0.5902564349620266
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.904186500929747	eta = 0.6345279427832983
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.838111741409746	eta = 0.6373535234204303
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.83785471002873	eta = 0.6373645640903601
eta = 0.6373645640903601
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [0.03716704 0.07816878 0.03657709 0.01268399 0.09026281 0.04306657
 0.01592873 0.0528008  0.03834695 0.03480723]
ene_total = [1.43077897 2.28852743 1.44075621 0.69142509 2.60477925 1.34299583
 0.77637967 1.71811452 1.42782004 1.11627769]
ti_comp = [0.85812284 0.94697766 0.84848758 0.89012496 0.94950542 0.94998635
 0.89076325 0.90687475 0.8710298  0.95228188]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [4.35767693e-06 3.32889784e-05 4.24831188e-06 1.60970289e-07
 5.09813926e-05 5.53179598e-06 3.18345717e-07 1.11868234e-05
 4.64522066e-06 2.90641279e-06]
ene_total = [0.4382524  0.20171506 0.46398296 0.35267029 0.19543653 0.19293825
 0.35096977 0.30823006 0.40378868 0.18673731]
optimize_network iter = 0 obj = 3.094721297443309
eta = 0.6373645640903601
freqs = [21656013.90690634 41272766.08711902 21554285.58755618  7124836.67893045
 47531488.32308537 22666941.75621849  8941057.85172046 29111404.29436724
 22012419.35744325 18275695.9412735 ]
eta_min = 0.6373645640903616	eta_max = 0.700248974656232
af = 0.0025292806273427074	bf = 1.1211163846456331	zeta = 0.002782208690076978	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [9.22110454e-07 7.04414656e-06 8.98968156e-07 3.40622741e-08
 1.07879670e-05 1.17056105e-06 6.73638541e-08 2.36719862e-06
 9.82956424e-07 6.15014298e-07]
ene_total = [1.74781835 0.80184327 1.85046646 1.40678466 0.77531226 0.76916407
 1.39998808 1.22858694 1.61031875 0.74464912]
ti_comp = [0.68086857 0.76972339 0.67123331 0.71287069 0.77225115 0.77273208
 0.71350898 0.72962048 0.69377552 0.77502761]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.42697873e-06 2.49456369e-05 3.36081341e-06 1.24253947e-07
 3.81569359e-05 4.13930121e-06 2.45645457e-07 8.55640634e-06
 3.62508715e-06 2.17239112e-06]
ene_total = [0.53016278 0.24376304 0.56129271 0.42665538 0.23602256 0.23336954
 0.42459695 0.37280828 0.48846606 0.22588899]
optimize_network iter = 1 obj = 3.7430263059334705
eta = 0.700248974656232
freqs = [21591991.81873228 40169519.43771622 21554285.58755618  7037900.85535273
 46232598.93038776 22044962.3818897   8830387.52189105 28624734.01565526
 21863003.51871486 17764389.24875431]
eta_min = 0.7002489746562424	eta_max = 0.7002489746562317
af = 0.0024124103758265034	bf = 1.1211163846456331	zeta = 0.002653651413409154	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [9.16666407e-07 6.67259097e-06 8.98968156e-07 3.32361034e-08
 1.02064191e-05 1.10720219e-06 6.57065467e-08 2.28871285e-06
 9.69657495e-07 5.81082673e-07]
ene_total = [1.74781777 0.80180369 1.85046646 1.40678457 0.7752503  0.76915732
 1.3999879  1.22857858 1.61031734 0.7446455 ]
ti_comp = [0.68086857 0.76972339 0.67123331 0.71287069 0.77225115 0.77273208
 0.71350898 0.72962048 0.69377552 0.77502761]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.42697873e-06 2.49456369e-05 3.36081341e-06 1.24253947e-07
 3.81569359e-05 4.13930121e-06 2.45645457e-07 8.55640634e-06
 3.62508715e-06 2.17239112e-06]
ene_total = [0.53016278 0.24376304 0.56129271 0.42665538 0.23602256 0.23336954
 0.42459695 0.37280828 0.48846606 0.22588899]
optimize_network iter = 2 obj = 3.743026305933466
eta = 0.7002489746562317
freqs = [21591991.81873227 40169519.4377162  21554285.58755618  7037900.85535273
 46232598.93038776 22044962.38188971  8830387.52189105 28624734.01565526
 21863003.51871486 17764389.24875431]
Done!
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.23086379e-06 2.35180785e-05 3.16848489e-06 1.17143295e-07
 3.59733374e-05 3.90242234e-06 2.31587959e-07 8.06675079e-06
 3.41763510e-06 2.04807218e-06]
ene_total = [0.01640814 0.00754295 0.01737161 0.01320482 0.00730263 0.00722246
 0.0131411  0.01153779 0.01511763 0.00699105]
At round 54 energy consumption: 0.11584017098855656
At round 54 eta: 0.7002489746562317
At round 54 a_n: 9.685127119911243
At round 54 local rounds: 11.66770515392171
At round 54 global rounds: 32.310572111651304
gradient difference: 0.4623446464538574
train() client id: f_00000-0-0 loss: 1.134229  [   32/  126]
train() client id: f_00000-0-1 loss: 0.966546  [   64/  126]
train() client id: f_00000-0-2 loss: 1.281119  [   96/  126]
train() client id: f_00000-1-0 loss: 0.931709  [   32/  126]
train() client id: f_00000-1-1 loss: 1.019111  [   64/  126]
train() client id: f_00000-1-2 loss: 1.216308  [   96/  126]
train() client id: f_00000-2-0 loss: 0.903058  [   32/  126]
train() client id: f_00000-2-1 loss: 0.999172  [   64/  126]
train() client id: f_00000-2-2 loss: 1.088362  [   96/  126]
train() client id: f_00000-3-0 loss: 0.934448  [   32/  126]
train() client id: f_00000-3-1 loss: 0.796817  [   64/  126]
train() client id: f_00000-3-2 loss: 0.995604  [   96/  126]
train() client id: f_00000-4-0 loss: 1.024032  [   32/  126]
train() client id: f_00000-4-1 loss: 0.869589  [   64/  126]
train() client id: f_00000-4-2 loss: 0.920238  [   96/  126]
train() client id: f_00000-5-0 loss: 0.993915  [   32/  126]
train() client id: f_00000-5-1 loss: 0.767526  [   64/  126]
train() client id: f_00000-5-2 loss: 0.902914  [   96/  126]
train() client id: f_00000-6-0 loss: 0.763828  [   32/  126]
train() client id: f_00000-6-1 loss: 0.886917  [   64/  126]
train() client id: f_00000-6-2 loss: 0.841207  [   96/  126]
train() client id: f_00000-7-0 loss: 0.685612  [   32/  126]
train() client id: f_00000-7-1 loss: 0.950958  [   64/  126]
train() client id: f_00000-7-2 loss: 0.899398  [   96/  126]
train() client id: f_00000-8-0 loss: 0.717095  [   32/  126]
train() client id: f_00000-8-1 loss: 0.873886  [   64/  126]
train() client id: f_00000-8-2 loss: 0.926378  [   96/  126]
train() client id: f_00000-9-0 loss: 0.929669  [   32/  126]
train() client id: f_00000-9-1 loss: 0.783714  [   64/  126]
train() client id: f_00000-9-2 loss: 0.851736  [   96/  126]
train() client id: f_00000-10-0 loss: 0.806211  [   32/  126]
train() client id: f_00000-10-1 loss: 0.979157  [   64/  126]
train() client id: f_00000-10-2 loss: 0.810651  [   96/  126]
train() client id: f_00001-0-0 loss: 0.536335  [   32/  265]
train() client id: f_00001-0-1 loss: 0.369937  [   64/  265]
train() client id: f_00001-0-2 loss: 0.457250  [   96/  265]
train() client id: f_00001-0-3 loss: 0.444173  [  128/  265]
train() client id: f_00001-0-4 loss: 0.419875  [  160/  265]
train() client id: f_00001-0-5 loss: 0.381077  [  192/  265]
train() client id: f_00001-0-6 loss: 0.434197  [  224/  265]
train() client id: f_00001-0-7 loss: 0.422228  [  256/  265]
train() client id: f_00001-1-0 loss: 0.442522  [   32/  265]
train() client id: f_00001-1-1 loss: 0.403895  [   64/  265]
train() client id: f_00001-1-2 loss: 0.448766  [   96/  265]
train() client id: f_00001-1-3 loss: 0.537113  [  128/  265]
train() client id: f_00001-1-4 loss: 0.361040  [  160/  265]
train() client id: f_00001-1-5 loss: 0.440620  [  192/  265]
train() client id: f_00001-1-6 loss: 0.495469  [  224/  265]
train() client id: f_00001-1-7 loss: 0.423502  [  256/  265]
train() client id: f_00001-2-0 loss: 0.483028  [   32/  265]
train() client id: f_00001-2-1 loss: 0.361586  [   64/  265]
train() client id: f_00001-2-2 loss: 0.490376  [   96/  265]
train() client id: f_00001-2-3 loss: 0.448115  [  128/  265]
train() client id: f_00001-2-4 loss: 0.429798  [  160/  265]
train() client id: f_00001-2-5 loss: 0.429044  [  192/  265]
train() client id: f_00001-2-6 loss: 0.482024  [  224/  265]
train() client id: f_00001-2-7 loss: 0.381955  [  256/  265]
train() client id: f_00001-3-0 loss: 0.368751  [   32/  265]
train() client id: f_00001-3-1 loss: 0.487646  [   64/  265]
train() client id: f_00001-3-2 loss: 0.483368  [   96/  265]
train() client id: f_00001-3-3 loss: 0.431615  [  128/  265]
train() client id: f_00001-3-4 loss: 0.426753  [  160/  265]
train() client id: f_00001-3-5 loss: 0.423548  [  192/  265]
train() client id: f_00001-3-6 loss: 0.441880  [  224/  265]
train() client id: f_00001-3-7 loss: 0.411288  [  256/  265]
train() client id: f_00001-4-0 loss: 0.476142  [   32/  265]
train() client id: f_00001-4-1 loss: 0.435901  [   64/  265]
train() client id: f_00001-4-2 loss: 0.457305  [   96/  265]
train() client id: f_00001-4-3 loss: 0.339523  [  128/  265]
train() client id: f_00001-4-4 loss: 0.460829  [  160/  265]
train() client id: f_00001-4-5 loss: 0.362544  [  192/  265]
train() client id: f_00001-4-6 loss: 0.419249  [  224/  265]
train() client id: f_00001-4-7 loss: 0.457204  [  256/  265]
train() client id: f_00001-5-0 loss: 0.361372  [   32/  265]
train() client id: f_00001-5-1 loss: 0.348926  [   64/  265]
train() client id: f_00001-5-2 loss: 0.478425  [   96/  265]
train() client id: f_00001-5-3 loss: 0.444921  [  128/  265]
train() client id: f_00001-5-4 loss: 0.410797  [  160/  265]
train() client id: f_00001-5-5 loss: 0.485296  [  192/  265]
train() client id: f_00001-5-6 loss: 0.437176  [  224/  265]
train() client id: f_00001-5-7 loss: 0.472045  [  256/  265]
train() client id: f_00001-6-0 loss: 0.324732  [   32/  265]
train() client id: f_00001-6-1 loss: 0.397747  [   64/  265]
train() client id: f_00001-6-2 loss: 0.434795  [   96/  265]
train() client id: f_00001-6-3 loss: 0.541131  [  128/  265]
train() client id: f_00001-6-4 loss: 0.456235  [  160/  265]
train() client id: f_00001-6-5 loss: 0.367770  [  192/  265]
train() client id: f_00001-6-6 loss: 0.400508  [  224/  265]
train() client id: f_00001-6-7 loss: 0.527606  [  256/  265]
train() client id: f_00001-7-0 loss: 0.447090  [   32/  265]
train() client id: f_00001-7-1 loss: 0.432518  [   64/  265]
train() client id: f_00001-7-2 loss: 0.389139  [   96/  265]
train() client id: f_00001-7-3 loss: 0.404133  [  128/  265]
train() client id: f_00001-7-4 loss: 0.401498  [  160/  265]
train() client id: f_00001-7-5 loss: 0.454290  [  192/  265]
train() client id: f_00001-7-6 loss: 0.379351  [  224/  265]
train() client id: f_00001-7-7 loss: 0.530470  [  256/  265]
train() client id: f_00001-8-0 loss: 0.335100  [   32/  265]
train() client id: f_00001-8-1 loss: 0.357272  [   64/  265]
train() client id: f_00001-8-2 loss: 0.552972  [   96/  265]
train() client id: f_00001-8-3 loss: 0.427662  [  128/  265]
train() client id: f_00001-8-4 loss: 0.480491  [  160/  265]
train() client id: f_00001-8-5 loss: 0.403529  [  192/  265]
train() client id: f_00001-8-6 loss: 0.395828  [  224/  265]
train() client id: f_00001-8-7 loss: 0.463154  [  256/  265]
train() client id: f_00001-9-0 loss: 0.483520  [   32/  265]
train() client id: f_00001-9-1 loss: 0.415827  [   64/  265]
train() client id: f_00001-9-2 loss: 0.619051  [   96/  265]
train() client id: f_00001-9-3 loss: 0.362727  [  128/  265]
train() client id: f_00001-9-4 loss: 0.356634  [  160/  265]
train() client id: f_00001-9-5 loss: 0.383826  [  192/  265]
train() client id: f_00001-9-6 loss: 0.410931  [  224/  265]
train() client id: f_00001-9-7 loss: 0.411669  [  256/  265]
train() client id: f_00001-10-0 loss: 0.468355  [   32/  265]
train() client id: f_00001-10-1 loss: 0.371464  [   64/  265]
train() client id: f_00001-10-2 loss: 0.404913  [   96/  265]
train() client id: f_00001-10-3 loss: 0.637824  [  128/  265]
train() client id: f_00001-10-4 loss: 0.324799  [  160/  265]
train() client id: f_00001-10-5 loss: 0.456621  [  192/  265]
train() client id: f_00001-10-6 loss: 0.438611  [  224/  265]
train() client id: f_00001-10-7 loss: 0.345721  [  256/  265]
train() client id: f_00002-0-0 loss: 1.136072  [   32/  124]
train() client id: f_00002-0-1 loss: 1.149094  [   64/  124]
train() client id: f_00002-0-2 loss: 1.286162  [   96/  124]
train() client id: f_00002-1-0 loss: 1.322094  [   32/  124]
train() client id: f_00002-1-1 loss: 0.923322  [   64/  124]
train() client id: f_00002-1-2 loss: 1.217877  [   96/  124]
train() client id: f_00002-2-0 loss: 1.149419  [   32/  124]
train() client id: f_00002-2-1 loss: 1.084605  [   64/  124]
train() client id: f_00002-2-2 loss: 1.106237  [   96/  124]
train() client id: f_00002-3-0 loss: 1.147156  [   32/  124]
train() client id: f_00002-3-1 loss: 1.151924  [   64/  124]
train() client id: f_00002-3-2 loss: 1.026152  [   96/  124]
train() client id: f_00002-4-0 loss: 1.047301  [   32/  124]
train() client id: f_00002-4-1 loss: 1.128990  [   64/  124]
train() client id: f_00002-4-2 loss: 1.053959  [   96/  124]
train() client id: f_00002-5-0 loss: 0.962623  [   32/  124]
train() client id: f_00002-5-1 loss: 1.126000  [   64/  124]
train() client id: f_00002-5-2 loss: 1.017596  [   96/  124]
train() client id: f_00002-6-0 loss: 0.928449  [   32/  124]
train() client id: f_00002-6-1 loss: 1.214785  [   64/  124]
train() client id: f_00002-6-2 loss: 0.975235  [   96/  124]
train() client id: f_00002-7-0 loss: 1.025674  [   32/  124]
train() client id: f_00002-7-1 loss: 1.148611  [   64/  124]
train() client id: f_00002-7-2 loss: 1.030310  [   96/  124]
train() client id: f_00002-8-0 loss: 1.012311  [   32/  124]
train() client id: f_00002-8-1 loss: 0.967400  [   64/  124]
train() client id: f_00002-8-2 loss: 1.153532  [   96/  124]
train() client id: f_00002-9-0 loss: 1.252398  [   32/  124]
train() client id: f_00002-9-1 loss: 1.009135  [   64/  124]
train() client id: f_00002-9-2 loss: 0.984317  [   96/  124]
train() client id: f_00002-10-0 loss: 1.045388  [   32/  124]
train() client id: f_00002-10-1 loss: 0.945868  [   64/  124]
train() client id: f_00002-10-2 loss: 0.844015  [   96/  124]
train() client id: f_00003-0-0 loss: 0.319125  [   32/   43]
train() client id: f_00003-1-0 loss: 0.273686  [   32/   43]
train() client id: f_00003-2-0 loss: 0.199496  [   32/   43]
train() client id: f_00003-3-0 loss: 0.395250  [   32/   43]
train() client id: f_00003-4-0 loss: 0.569929  [   32/   43]
train() client id: f_00003-5-0 loss: 0.469625  [   32/   43]
train() client id: f_00003-6-0 loss: 0.389559  [   32/   43]
train() client id: f_00003-7-0 loss: 0.472650  [   32/   43]
train() client id: f_00003-8-0 loss: 0.324310  [   32/   43]
train() client id: f_00003-9-0 loss: 0.347746  [   32/   43]
train() client id: f_00003-10-0 loss: 0.430685  [   32/   43]
train() client id: f_00004-0-0 loss: 0.839130  [   32/  306]
train() client id: f_00004-0-1 loss: 0.836798  [   64/  306]
train() client id: f_00004-0-2 loss: 0.890650  [   96/  306]
train() client id: f_00004-0-3 loss: 0.778938  [  128/  306]
train() client id: f_00004-0-4 loss: 0.999798  [  160/  306]
train() client id: f_00004-0-5 loss: 0.961145  [  192/  306]
train() client id: f_00004-0-6 loss: 0.862752  [  224/  306]
train() client id: f_00004-0-7 loss: 0.859133  [  256/  306]
train() client id: f_00004-0-8 loss: 0.906021  [  288/  306]
train() client id: f_00004-1-0 loss: 0.888976  [   32/  306]
train() client id: f_00004-1-1 loss: 0.880885  [   64/  306]
train() client id: f_00004-1-2 loss: 0.830516  [   96/  306]
train() client id: f_00004-1-3 loss: 0.779058  [  128/  306]
train() client id: f_00004-1-4 loss: 1.033819  [  160/  306]
train() client id: f_00004-1-5 loss: 0.886620  [  192/  306]
train() client id: f_00004-1-6 loss: 0.894732  [  224/  306]
train() client id: f_00004-1-7 loss: 0.887969  [  256/  306]
train() client id: f_00004-1-8 loss: 0.763122  [  288/  306]
train() client id: f_00004-2-0 loss: 0.852556  [   32/  306]
train() client id: f_00004-2-1 loss: 0.857221  [   64/  306]
train() client id: f_00004-2-2 loss: 0.839974  [   96/  306]
train() client id: f_00004-2-3 loss: 0.939276  [  128/  306]
train() client id: f_00004-2-4 loss: 0.801791  [  160/  306]
train() client id: f_00004-2-5 loss: 0.748454  [  192/  306]
train() client id: f_00004-2-6 loss: 0.850578  [  224/  306]
train() client id: f_00004-2-7 loss: 0.941310  [  256/  306]
train() client id: f_00004-2-8 loss: 0.937875  [  288/  306]
train() client id: f_00004-3-0 loss: 0.861207  [   32/  306]
train() client id: f_00004-3-1 loss: 0.863012  [   64/  306]
train() client id: f_00004-3-2 loss: 0.961470  [   96/  306]
train() client id: f_00004-3-3 loss: 0.927928  [  128/  306]
train() client id: f_00004-3-4 loss: 0.797444  [  160/  306]
train() client id: f_00004-3-5 loss: 0.820441  [  192/  306]
train() client id: f_00004-3-6 loss: 0.737175  [  224/  306]
train() client id: f_00004-3-7 loss: 0.795403  [  256/  306]
train() client id: f_00004-3-8 loss: 0.956795  [  288/  306]
train() client id: f_00004-4-0 loss: 0.844948  [   32/  306]
train() client id: f_00004-4-1 loss: 0.677529  [   64/  306]
train() client id: f_00004-4-2 loss: 0.987403  [   96/  306]
train() client id: f_00004-4-3 loss: 0.957354  [  128/  306]
train() client id: f_00004-4-4 loss: 0.956492  [  160/  306]
train() client id: f_00004-4-5 loss: 0.848006  [  192/  306]
train() client id: f_00004-4-6 loss: 0.742881  [  224/  306]
train() client id: f_00004-4-7 loss: 0.899013  [  256/  306]
train() client id: f_00004-4-8 loss: 0.768032  [  288/  306]
train() client id: f_00004-5-0 loss: 0.860678  [   32/  306]
train() client id: f_00004-5-1 loss: 0.851098  [   64/  306]
train() client id: f_00004-5-2 loss: 0.891881  [   96/  306]
train() client id: f_00004-5-3 loss: 0.838074  [  128/  306]
train() client id: f_00004-5-4 loss: 0.808776  [  160/  306]
train() client id: f_00004-5-5 loss: 0.957987  [  192/  306]
train() client id: f_00004-5-6 loss: 0.822135  [  224/  306]
train() client id: f_00004-5-7 loss: 0.778871  [  256/  306]
train() client id: f_00004-5-8 loss: 0.904908  [  288/  306]
train() client id: f_00004-6-0 loss: 0.758346  [   32/  306]
train() client id: f_00004-6-1 loss: 0.923330  [   64/  306]
train() client id: f_00004-6-2 loss: 0.850408  [   96/  306]
train() client id: f_00004-6-3 loss: 0.839587  [  128/  306]
train() client id: f_00004-6-4 loss: 0.817178  [  160/  306]
train() client id: f_00004-6-5 loss: 0.860016  [  192/  306]
train() client id: f_00004-6-6 loss: 0.908790  [  224/  306]
train() client id: f_00004-6-7 loss: 0.906773  [  256/  306]
train() client id: f_00004-6-8 loss: 0.805328  [  288/  306]
train() client id: f_00004-7-0 loss: 0.959795  [   32/  306]
train() client id: f_00004-7-1 loss: 0.818462  [   64/  306]
train() client id: f_00004-7-2 loss: 0.738243  [   96/  306]
train() client id: f_00004-7-3 loss: 0.919208  [  128/  306]
train() client id: f_00004-7-4 loss: 0.921259  [  160/  306]
train() client id: f_00004-7-5 loss: 0.829424  [  192/  306]
train() client id: f_00004-7-6 loss: 0.757336  [  224/  306]
train() client id: f_00004-7-7 loss: 0.956251  [  256/  306]
train() client id: f_00004-7-8 loss: 0.913674  [  288/  306]
train() client id: f_00004-8-0 loss: 0.936310  [   32/  306]
train() client id: f_00004-8-1 loss: 0.869607  [   64/  306]
train() client id: f_00004-8-2 loss: 0.759920  [   96/  306]
train() client id: f_00004-8-3 loss: 0.910295  [  128/  306]
train() client id: f_00004-8-4 loss: 1.048803  [  160/  306]
train() client id: f_00004-8-5 loss: 0.843380  [  192/  306]
train() client id: f_00004-8-6 loss: 0.800030  [  224/  306]
train() client id: f_00004-8-7 loss: 0.781452  [  256/  306]
train() client id: f_00004-8-8 loss: 0.844644  [  288/  306]
train() client id: f_00004-9-0 loss: 0.852654  [   32/  306]
train() client id: f_00004-9-1 loss: 0.842538  [   64/  306]
train() client id: f_00004-9-2 loss: 0.835292  [   96/  306]
train() client id: f_00004-9-3 loss: 0.966202  [  128/  306]
train() client id: f_00004-9-4 loss: 0.881478  [  160/  306]
train() client id: f_00004-9-5 loss: 0.763405  [  192/  306]
train() client id: f_00004-9-6 loss: 0.878028  [  224/  306]
train() client id: f_00004-9-7 loss: 0.770210  [  256/  306]
train() client id: f_00004-9-8 loss: 0.860683  [  288/  306]
train() client id: f_00004-10-0 loss: 0.741504  [   32/  306]
train() client id: f_00004-10-1 loss: 0.873992  [   64/  306]
train() client id: f_00004-10-2 loss: 0.939795  [   96/  306]
train() client id: f_00004-10-3 loss: 0.765469  [  128/  306]
train() client id: f_00004-10-4 loss: 0.945380  [  160/  306]
train() client id: f_00004-10-5 loss: 0.776366  [  192/  306]
train() client id: f_00004-10-6 loss: 0.758257  [  224/  306]
train() client id: f_00004-10-7 loss: 0.911749  [  256/  306]
train() client id: f_00004-10-8 loss: 0.895887  [  288/  306]
train() client id: f_00005-0-0 loss: 0.449181  [   32/  146]
train() client id: f_00005-0-1 loss: 0.415748  [   64/  146]
train() client id: f_00005-0-2 loss: 0.148776  [   96/  146]
train() client id: f_00005-0-3 loss: 0.117046  [  128/  146]
train() client id: f_00005-1-0 loss: 0.464484  [   32/  146]
train() client id: f_00005-1-1 loss: 0.340630  [   64/  146]
train() client id: f_00005-1-2 loss: 0.060404  [   96/  146]
train() client id: f_00005-1-3 loss: 0.388626  [  128/  146]
train() client id: f_00005-2-0 loss: 0.350446  [   32/  146]
train() client id: f_00005-2-1 loss: 0.370125  [   64/  146]
train() client id: f_00005-2-2 loss: 0.254236  [   96/  146]
train() client id: f_00005-2-3 loss: 0.384105  [  128/  146]
train() client id: f_00005-3-0 loss: 0.189424  [   32/  146]
train() client id: f_00005-3-1 loss: 0.081000  [   64/  146]
train() client id: f_00005-3-2 loss: 0.374766  [   96/  146]
train() client id: f_00005-3-3 loss: 0.637237  [  128/  146]
train() client id: f_00005-4-0 loss: 0.218414  [   32/  146]
train() client id: f_00005-4-1 loss: 0.385498  [   64/  146]
train() client id: f_00005-4-2 loss: 0.377088  [   96/  146]
train() client id: f_00005-4-3 loss: 0.332902  [  128/  146]
train() client id: f_00005-5-0 loss: 0.495327  [   32/  146]
train() client id: f_00005-5-1 loss: 0.026601  [   64/  146]
train() client id: f_00005-5-2 loss: 0.239760  [   96/  146]
train() client id: f_00005-5-3 loss: 0.276604  [  128/  146]
train() client id: f_00005-6-0 loss: 0.382775  [   32/  146]
train() client id: f_00005-6-1 loss: 0.348698  [   64/  146]
train() client id: f_00005-6-2 loss: 0.184511  [   96/  146]
train() client id: f_00005-6-3 loss: 0.252426  [  128/  146]
train() client id: f_00005-7-0 loss: 0.429241  [   32/  146]
train() client id: f_00005-7-1 loss: 0.358601  [   64/  146]
train() client id: f_00005-7-2 loss: 0.315478  [   96/  146]
train() client id: f_00005-7-3 loss: 0.284208  [  128/  146]
train() client id: f_00005-8-0 loss: 0.597192  [   32/  146]
train() client id: f_00005-8-1 loss: 0.268938  [   64/  146]
train() client id: f_00005-8-2 loss: 0.040916  [   96/  146]
train() client id: f_00005-8-3 loss: 0.393058  [  128/  146]
train() client id: f_00005-9-0 loss: 0.517035  [   32/  146]
train() client id: f_00005-9-1 loss: 0.517942  [   64/  146]
train() client id: f_00005-9-2 loss: 0.136097  [   96/  146]
train() client id: f_00005-9-3 loss: 0.196262  [  128/  146]
train() client id: f_00005-10-0 loss: 0.410131  [   32/  146]
train() client id: f_00005-10-1 loss: 0.487947  [   64/  146]
train() client id: f_00005-10-2 loss: 0.440988  [   96/  146]
train() client id: f_00005-10-3 loss: 0.083578  [  128/  146]
train() client id: f_00006-0-0 loss: 0.512613  [   32/   54]
train() client id: f_00006-1-0 loss: 0.432398  [   32/   54]
train() client id: f_00006-2-0 loss: 0.499099  [   32/   54]
train() client id: f_00006-3-0 loss: 0.429683  [   32/   54]
train() client id: f_00006-4-0 loss: 0.446271  [   32/   54]
train() client id: f_00006-5-0 loss: 0.460196  [   32/   54]
train() client id: f_00006-6-0 loss: 0.515937  [   32/   54]
train() client id: f_00006-7-0 loss: 0.469936  [   32/   54]
train() client id: f_00006-8-0 loss: 0.486060  [   32/   54]
train() client id: f_00006-9-0 loss: 0.532521  [   32/   54]
train() client id: f_00006-10-0 loss: 0.512275  [   32/   54]
train() client id: f_00007-0-0 loss: 0.348999  [   32/  179]
train() client id: f_00007-0-1 loss: 0.431617  [   64/  179]
train() client id: f_00007-0-2 loss: 0.260069  [   96/  179]
train() client id: f_00007-0-3 loss: 0.486926  [  128/  179]
train() client id: f_00007-0-4 loss: 0.371644  [  160/  179]
train() client id: f_00007-1-0 loss: 0.367202  [   32/  179]
train() client id: f_00007-1-1 loss: 0.280126  [   64/  179]
train() client id: f_00007-1-2 loss: 0.436022  [   96/  179]
train() client id: f_00007-1-3 loss: 0.290721  [  128/  179]
train() client id: f_00007-1-4 loss: 0.313714  [  160/  179]
train() client id: f_00007-2-0 loss: 0.281603  [   32/  179]
train() client id: f_00007-2-1 loss: 0.244580  [   64/  179]
train() client id: f_00007-2-2 loss: 0.230401  [   96/  179]
train() client id: f_00007-2-3 loss: 0.566157  [  128/  179]
train() client id: f_00007-2-4 loss: 0.334224  [  160/  179]
train() client id: f_00007-3-0 loss: 0.665827  [   32/  179]
train() client id: f_00007-3-1 loss: 0.201729  [   64/  179]
train() client id: f_00007-3-2 loss: 0.205305  [   96/  179]
train() client id: f_00007-3-3 loss: 0.292767  [  128/  179]
train() client id: f_00007-3-4 loss: 0.290229  [  160/  179]
train() client id: f_00007-4-0 loss: 0.199024  [   32/  179]
train() client id: f_00007-4-1 loss: 0.233531  [   64/  179]
train() client id: f_00007-4-2 loss: 0.380123  [   96/  179]
train() client id: f_00007-4-3 loss: 0.446018  [  128/  179]
train() client id: f_00007-4-4 loss: 0.334541  [  160/  179]
train() client id: f_00007-5-0 loss: 0.341460  [   32/  179]
train() client id: f_00007-5-1 loss: 0.383583  [   64/  179]
train() client id: f_00007-5-2 loss: 0.199540  [   96/  179]
train() client id: f_00007-5-3 loss: 0.234063  [  128/  179]
train() client id: f_00007-5-4 loss: 0.216500  [  160/  179]
train() client id: f_00007-6-0 loss: 0.145739  [   32/  179]
train() client id: f_00007-6-1 loss: 0.348299  [   64/  179]
train() client id: f_00007-6-2 loss: 0.407042  [   96/  179]
train() client id: f_00007-6-3 loss: 0.359417  [  128/  179]
train() client id: f_00007-6-4 loss: 0.317759  [  160/  179]
train() client id: f_00007-7-0 loss: 0.175031  [   32/  179]
train() client id: f_00007-7-1 loss: 0.296899  [   64/  179]
train() client id: f_00007-7-2 loss: 0.186240  [   96/  179]
train() client id: f_00007-7-3 loss: 0.283192  [  128/  179]
train() client id: f_00007-7-4 loss: 0.528299  [  160/  179]
train() client id: f_00007-8-0 loss: 0.323728  [   32/  179]
train() client id: f_00007-8-1 loss: 0.280119  [   64/  179]
train() client id: f_00007-8-2 loss: 0.272659  [   96/  179]
train() client id: f_00007-8-3 loss: 0.300504  [  128/  179]
train() client id: f_00007-8-4 loss: 0.209378  [  160/  179]
train() client id: f_00007-9-0 loss: 0.294312  [   32/  179]
train() client id: f_00007-9-1 loss: 0.636971  [   64/  179]
train() client id: f_00007-9-2 loss: 0.165834  [   96/  179]
train() client id: f_00007-9-3 loss: 0.239269  [  128/  179]
train() client id: f_00007-9-4 loss: 0.151431  [  160/  179]
train() client id: f_00007-10-0 loss: 0.393190  [   32/  179]
train() client id: f_00007-10-1 loss: 0.291875  [   64/  179]
train() client id: f_00007-10-2 loss: 0.284215  [   96/  179]
train() client id: f_00007-10-3 loss: 0.265574  [  128/  179]
train() client id: f_00007-10-4 loss: 0.187844  [  160/  179]
train() client id: f_00008-0-0 loss: 0.624502  [   32/  130]
train() client id: f_00008-0-1 loss: 0.725263  [   64/  130]
train() client id: f_00008-0-2 loss: 0.571355  [   96/  130]
train() client id: f_00008-0-3 loss: 0.745731  [  128/  130]
train() client id: f_00008-1-0 loss: 0.623976  [   32/  130]
train() client id: f_00008-1-1 loss: 0.734399  [   64/  130]
train() client id: f_00008-1-2 loss: 0.598030  [   96/  130]
train() client id: f_00008-1-3 loss: 0.676625  [  128/  130]
train() client id: f_00008-2-0 loss: 0.669906  [   32/  130]
train() client id: f_00008-2-1 loss: 0.704947  [   64/  130]
train() client id: f_00008-2-2 loss: 0.691009  [   96/  130]
train() client id: f_00008-2-3 loss: 0.620774  [  128/  130]
train() client id: f_00008-3-0 loss: 0.608131  [   32/  130]
train() client id: f_00008-3-1 loss: 0.676055  [   64/  130]
train() client id: f_00008-3-2 loss: 0.723117  [   96/  130]
train() client id: f_00008-3-3 loss: 0.653854  [  128/  130]
train() client id: f_00008-4-0 loss: 0.750650  [   32/  130]
train() client id: f_00008-4-1 loss: 0.529916  [   64/  130]
train() client id: f_00008-4-2 loss: 0.692623  [   96/  130]
train() client id: f_00008-4-3 loss: 0.708929  [  128/  130]
train() client id: f_00008-5-0 loss: 0.772793  [   32/  130]
train() client id: f_00008-5-1 loss: 0.669968  [   64/  130]
train() client id: f_00008-5-2 loss: 0.645829  [   96/  130]
train() client id: f_00008-5-3 loss: 0.597854  [  128/  130]
train() client id: f_00008-6-0 loss: 0.686004  [   32/  130]
train() client id: f_00008-6-1 loss: 0.705570  [   64/  130]
train() client id: f_00008-6-2 loss: 0.555137  [   96/  130]
train() client id: f_00008-6-3 loss: 0.708159  [  128/  130]
train() client id: f_00008-7-0 loss: 0.580659  [   32/  130]
train() client id: f_00008-7-1 loss: 0.641403  [   64/  130]
train() client id: f_00008-7-2 loss: 0.855068  [   96/  130]
train() client id: f_00008-7-3 loss: 0.592227  [  128/  130]
train() client id: f_00008-8-0 loss: 0.631371  [   32/  130]
train() client id: f_00008-8-1 loss: 0.610925  [   64/  130]
train() client id: f_00008-8-2 loss: 0.717627  [   96/  130]
train() client id: f_00008-8-3 loss: 0.707255  [  128/  130]
train() client id: f_00008-9-0 loss: 0.633168  [   32/  130]
train() client id: f_00008-9-1 loss: 0.657956  [   64/  130]
train() client id: f_00008-9-2 loss: 0.659642  [   96/  130]
train() client id: f_00008-9-3 loss: 0.701838  [  128/  130]
train() client id: f_00008-10-0 loss: 0.619717  [   32/  130]
train() client id: f_00008-10-1 loss: 0.719629  [   64/  130]
train() client id: f_00008-10-2 loss: 0.695880  [   96/  130]
train() client id: f_00008-10-3 loss: 0.641806  [  128/  130]
train() client id: f_00009-0-0 loss: 1.115716  [   32/  118]
train() client id: f_00009-0-1 loss: 1.023921  [   64/  118]
train() client id: f_00009-0-2 loss: 1.023211  [   96/  118]
train() client id: f_00009-1-0 loss: 0.913570  [   32/  118]
train() client id: f_00009-1-1 loss: 1.036868  [   64/  118]
train() client id: f_00009-1-2 loss: 0.918443  [   96/  118]
train() client id: f_00009-2-0 loss: 0.870396  [   32/  118]
train() client id: f_00009-2-1 loss: 0.940954  [   64/  118]
train() client id: f_00009-2-2 loss: 0.845319  [   96/  118]
train() client id: f_00009-3-0 loss: 0.986069  [   32/  118]
train() client id: f_00009-3-1 loss: 0.762805  [   64/  118]
train() client id: f_00009-3-2 loss: 0.872221  [   96/  118]
train() client id: f_00009-4-0 loss: 0.790360  [   32/  118]
train() client id: f_00009-4-1 loss: 0.861162  [   64/  118]
train() client id: f_00009-4-2 loss: 0.766579  [   96/  118]
train() client id: f_00009-5-0 loss: 0.900406  [   32/  118]
train() client id: f_00009-5-1 loss: 1.032948  [   64/  118]
train() client id: f_00009-5-2 loss: 0.591851  [   96/  118]
train() client id: f_00009-6-0 loss: 0.805714  [   32/  118]
train() client id: f_00009-6-1 loss: 0.794111  [   64/  118]
train() client id: f_00009-6-2 loss: 0.567705  [   96/  118]
train() client id: f_00009-7-0 loss: 0.780297  [   32/  118]
train() client id: f_00009-7-1 loss: 0.761529  [   64/  118]
train() client id: f_00009-7-2 loss: 0.864571  [   96/  118]
train() client id: f_00009-8-0 loss: 0.848907  [   32/  118]
train() client id: f_00009-8-1 loss: 0.641144  [   64/  118]
train() client id: f_00009-8-2 loss: 0.898079  [   96/  118]
train() client id: f_00009-9-0 loss: 0.774063  [   32/  118]
train() client id: f_00009-9-1 loss: 0.639008  [   64/  118]
train() client id: f_00009-9-2 loss: 0.955848  [   96/  118]
train() client id: f_00009-10-0 loss: 0.815216  [   32/  118]
train() client id: f_00009-10-1 loss: 0.733807  [   64/  118]
train() client id: f_00009-10-2 loss: 0.721792  [   96/  118]
At round 54 accuracy: 0.6445623342175066
At round 54 training accuracy: 0.5895372233400402
At round 54 training loss: 0.827251348087992
update_location
xs = -3.905658 4.200318 290.009024 18.811294 0.979296 3.956410 -252.443192 -231.324852 274.663977 -217.060879 
ys = 282.587959 265.555839 1.320614 -252.455176 244.350187 227.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -11.211426168117436
ys mean: 78.8941425355287
dists_uav = 299.785272 283.791378 306.768607 272.190155 264.022675 248.827130 271.540891 252.015601 292.829251 239.021833 
uav_gains = -115.712523 -114.321804 -116.282487 -113.269820 -112.528631 -111.196598 -113.210677 -111.468386 -115.120264 -110.394064 
uav_gains_db_mean: -113.35052523119906
dists_bs = 202.109977 200.781259 496.384852 469.340756 189.151159 186.587061 193.773277 183.058306 476.426500 176.095459 
bs_gains = -104.123516 -104.043308 -115.049978 -114.368731 -103.317711 -103.151742 -103.611288 -102.919563 -114.550945 -102.448008 
bs_gains_db_mean: -106.75847892580879
Round 55
-------------------------------
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.74701293 7.6136546  3.69953246 1.35283391 8.77808353 4.22374522
 1.66527442 5.20651362 3.84415712 3.42461541]
obj_prev = 43.55542321522954
eta_min = 2.2364048538474024e-25	eta_max = 0.9376658335407816
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 10.034905168858742	eta = 0.909090909090909
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 21.758383945021716	eta = 0.4192701574551497
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 15.56372063459429	eta = 0.5861478290943805
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.453751471416155	eta = 0.6311607806900414
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.387691546337155	eta = 0.6340587044987988
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.387430378338086	eta = 0.6340702142568856
eta = 0.6340702142568856
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [0.03759466 0.07906812 0.03699791 0.01282992 0.09130131 0.04356206
 0.016112   0.05340828 0.03878814 0.03520769]
ene_total = [1.39513647 2.21195089 1.40544566 0.67656761 2.51756531 1.29725943
 0.75865034 1.66646122 1.38042471 1.07796874]
ti_comp = [0.89681784 0.99162304 0.88681254 0.93061922 0.99425194 0.99482833
 0.93128983 0.94868174 0.91451495 0.99717632]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [4.12904738e-06 3.14189100e-05 4.02483533e-06 1.52408097e-07
 4.81191659e-05 5.22046259e-06 3.01410426e-07 1.05794898e-05
 4.36109102e-06 2.74314532e-06]
ene_total = [0.43540987 0.19405873 0.46095183 0.34900978 0.18777323 0.1852064
 0.34730146 0.30316045 0.39023318 0.17914848]
optimize_network iter = 0 obj = 3.032253401302107
eta = 0.6340702142568856
freqs = [20960028.75390409 39868035.25301737 20860054.05324307  6893217.87674232
 45914572.57827595 21894260.42256957  8650365.72831919 28148681.6418485
 21206945.13364052 17653694.93957705]
eta_min = 0.6340702142568873	eta_max = 0.7017829950267004
af = 0.0022772242984259367	bf = 1.1085543441262717	zeta = 0.0025049467282685306	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [8.63792940e-07 6.57280727e-06 8.41991876e-07 3.18836346e-08
 1.00664856e-05 1.09211601e-06 6.30547859e-08 2.21321959e-06
 9.12336258e-07 5.73863496e-07]
ene_total = [1.75228827 0.77857718 1.85510894 1.40483142 0.75191939 0.74507368
 1.39794292 1.21943002 1.57042295 0.72089048]
ti_comp = [0.69931807 0.79412327 0.68931277 0.73311945 0.79675218 0.79732856
 0.73379006 0.75118197 0.71701518 0.79967655]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.18905741e-06 2.30071054e-05 3.12847447e-06 1.15333663e-07
 3.51898948e-05 3.81666577e-06 2.28001571e-07 7.92444745e-06
 3.33175889e-06 2.00316859e-06]
ene_total = [0.53424404 0.23785794 0.56558689 0.42825434 0.23000374 0.22721516
 0.42615699 0.3719125  0.47880679 0.21980253]
optimize_network iter = 1 obj = 3.71984092060831
eta = 0.7017829950267004
freqs = [20893243.25277815 38696173.21796749 20860054.05324307  6801485.54855279
 44535695.14880739 21233696.28865842  8533594.53110815 27632357.80406622
 21024470.67520079 17111091.4088647 ]
eta_min = 0.7017829950267082	eta_max = 0.7017829950267004
af = 0.0021615562993590274	bf = 1.1085543441262717	zeta = 0.0023777119292949303	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [8.58297057e-07 6.19209011e-06 8.41991876e-07 3.10406904e-08
 9.47094369e-06 1.02721042e-06 6.13639241e-08 2.13277124e-06
 8.96703470e-07 5.39129116e-07]
ene_total = [1.75228771 0.77853806 1.85510894 1.40483134 0.75185818 0.74506701
 1.39794275 1.21942176 1.57042134 0.72088691]
ti_comp = [0.69931807 0.79412327 0.68931277 0.73311945 0.79675218 0.79732856
 0.73379006 0.75118197 0.71701518 0.79967655]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.18905741e-06 2.30071054e-05 3.12847447e-06 1.15333663e-07
 3.51898948e-05 3.81666577e-06 2.28001571e-07 7.92444745e-06
 3.33175889e-06 2.00316859e-06]
ene_total = [0.53424404 0.23785794 0.56558689 0.42825434 0.23000374 0.22721516
 0.42615699 0.3719125  0.47880679 0.21980253]
optimize_network iter = 2 obj = 3.71984092060831
eta = 0.7017829950267004
freqs = [20893243.25277815 38696173.21796749 20860054.05324307  6801485.54855279
 44535695.14880739 21233696.28865842  8533594.53110815 27632357.80406622
 21024470.67520079 17111091.4088647 ]
Done!
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.02513636e-06 2.18245150e-05 2.96766745e-06 1.09405386e-07
 3.33810956e-05 3.62048496e-06 2.16282040e-07 7.51712216e-06
 3.16050283e-06 1.90020353e-06]
ene_total = [0.01705301 0.00759129 0.01805348 0.01366996 0.00733996 0.00725256
 0.013603   0.01187111 0.01528343 0.00701604]
At round 55 energy consumption: 0.11873383165749558
At round 55 eta: 0.7017829950267004
At round 55 a_n: 9.342581272941924
At round 55 local rounds: 11.596049668428781
At round 55 global rounds: 31.328130579872187
gradient difference: 0.49091148376464844
train() client id: f_00000-0-0 loss: 1.326147  [   32/  126]
train() client id: f_00000-0-1 loss: 0.997855  [   64/  126]
train() client id: f_00000-0-2 loss: 1.206093  [   96/  126]
train() client id: f_00000-1-0 loss: 0.953375  [   32/  126]
train() client id: f_00000-1-1 loss: 1.252320  [   64/  126]
train() client id: f_00000-1-2 loss: 1.135023  [   96/  126]
train() client id: f_00000-2-0 loss: 0.885756  [   32/  126]
train() client id: f_00000-2-1 loss: 0.943322  [   64/  126]
train() client id: f_00000-2-2 loss: 1.115581  [   96/  126]
train() client id: f_00000-3-0 loss: 0.969884  [   32/  126]
train() client id: f_00000-3-1 loss: 1.038172  [   64/  126]
train() client id: f_00000-3-2 loss: 0.881372  [   96/  126]
train() client id: f_00000-4-0 loss: 0.790030  [   32/  126]
train() client id: f_00000-4-1 loss: 1.028759  [   64/  126]
train() client id: f_00000-4-2 loss: 0.874290  [   96/  126]
train() client id: f_00000-5-0 loss: 0.949291  [   32/  126]
train() client id: f_00000-5-1 loss: 0.890157  [   64/  126]
train() client id: f_00000-5-2 loss: 0.869705  [   96/  126]
train() client id: f_00000-6-0 loss: 1.033983  [   32/  126]
train() client id: f_00000-6-1 loss: 0.825269  [   64/  126]
train() client id: f_00000-6-2 loss: 0.885421  [   96/  126]
train() client id: f_00000-7-0 loss: 0.828204  [   32/  126]
train() client id: f_00000-7-1 loss: 0.949569  [   64/  126]
train() client id: f_00000-7-2 loss: 0.791033  [   96/  126]
train() client id: f_00000-8-0 loss: 0.872913  [   32/  126]
train() client id: f_00000-8-1 loss: 0.815147  [   64/  126]
train() client id: f_00000-8-2 loss: 0.983874  [   96/  126]
train() client id: f_00000-9-0 loss: 0.832236  [   32/  126]
train() client id: f_00000-9-1 loss: 0.824162  [   64/  126]
train() client id: f_00000-9-2 loss: 0.811300  [   96/  126]
train() client id: f_00000-10-0 loss: 0.685926  [   32/  126]
train() client id: f_00000-10-1 loss: 0.849541  [   64/  126]
train() client id: f_00000-10-2 loss: 0.851822  [   96/  126]
train() client id: f_00001-0-0 loss: 0.537971  [   32/  265]
train() client id: f_00001-0-1 loss: 0.581333  [   64/  265]
train() client id: f_00001-0-2 loss: 0.538356  [   96/  265]
train() client id: f_00001-0-3 loss: 0.405899  [  128/  265]
train() client id: f_00001-0-4 loss: 0.506997  [  160/  265]
train() client id: f_00001-0-5 loss: 0.433985  [  192/  265]
train() client id: f_00001-0-6 loss: 0.711546  [  224/  265]
train() client id: f_00001-0-7 loss: 0.456572  [  256/  265]
train() client id: f_00001-1-0 loss: 0.524093  [   32/  265]
train() client id: f_00001-1-1 loss: 0.584325  [   64/  265]
train() client id: f_00001-1-2 loss: 0.462557  [   96/  265]
train() client id: f_00001-1-3 loss: 0.505993  [  128/  265]
train() client id: f_00001-1-4 loss: 0.488906  [  160/  265]
train() client id: f_00001-1-5 loss: 0.490565  [  192/  265]
train() client id: f_00001-1-6 loss: 0.437819  [  224/  265]
train() client id: f_00001-1-7 loss: 0.587827  [  256/  265]
train() client id: f_00001-2-0 loss: 0.604007  [   32/  265]
train() client id: f_00001-2-1 loss: 0.506750  [   64/  265]
train() client id: f_00001-2-2 loss: 0.469856  [   96/  265]
train() client id: f_00001-2-3 loss: 0.556247  [  128/  265]
train() client id: f_00001-2-4 loss: 0.499417  [  160/  265]
train() client id: f_00001-2-5 loss: 0.564411  [  192/  265]
train() client id: f_00001-2-6 loss: 0.466515  [  224/  265]
train() client id: f_00001-2-7 loss: 0.422561  [  256/  265]
train() client id: f_00001-3-0 loss: 0.501506  [   32/  265]
train() client id: f_00001-3-1 loss: 0.402726  [   64/  265]
train() client id: f_00001-3-2 loss: 0.481027  [   96/  265]
train() client id: f_00001-3-3 loss: 0.535396  [  128/  265]
train() client id: f_00001-3-4 loss: 0.527759  [  160/  265]
train() client id: f_00001-3-5 loss: 0.486265  [  192/  265]
train() client id: f_00001-3-6 loss: 0.410532  [  224/  265]
train() client id: f_00001-3-7 loss: 0.683731  [  256/  265]
train() client id: f_00001-4-0 loss: 0.438240  [   32/  265]
train() client id: f_00001-4-1 loss: 0.459010  [   64/  265]
train() client id: f_00001-4-2 loss: 0.476087  [   96/  265]
train() client id: f_00001-4-3 loss: 0.516527  [  128/  265]
train() client id: f_00001-4-4 loss: 0.583063  [  160/  265]
train() client id: f_00001-4-5 loss: 0.527201  [  192/  265]
train() client id: f_00001-4-6 loss: 0.490783  [  224/  265]
train() client id: f_00001-4-7 loss: 0.520937  [  256/  265]
train() client id: f_00001-5-0 loss: 0.534862  [   32/  265]
train() client id: f_00001-5-1 loss: 0.472048  [   64/  265]
train() client id: f_00001-5-2 loss: 0.535588  [   96/  265]
train() client id: f_00001-5-3 loss: 0.609814  [  128/  265]
train() client id: f_00001-5-4 loss: 0.413301  [  160/  265]
train() client id: f_00001-5-5 loss: 0.532261  [  192/  265]
train() client id: f_00001-5-6 loss: 0.446942  [  224/  265]
train() client id: f_00001-5-7 loss: 0.531411  [  256/  265]
train() client id: f_00001-6-0 loss: 0.592158  [   32/  265]
train() client id: f_00001-6-1 loss: 0.486935  [   64/  265]
train() client id: f_00001-6-2 loss: 0.595144  [   96/  265]
train() client id: f_00001-6-3 loss: 0.455126  [  128/  265]
train() client id: f_00001-6-4 loss: 0.415354  [  160/  265]
train() client id: f_00001-6-5 loss: 0.468508  [  192/  265]
train() client id: f_00001-6-6 loss: 0.399047  [  224/  265]
train() client id: f_00001-6-7 loss: 0.660612  [  256/  265]
train() client id: f_00001-7-0 loss: 0.525516  [   32/  265]
train() client id: f_00001-7-1 loss: 0.580590  [   64/  265]
train() client id: f_00001-7-2 loss: 0.551845  [   96/  265]
train() client id: f_00001-7-3 loss: 0.473042  [  128/  265]
train() client id: f_00001-7-4 loss: 0.460899  [  160/  265]
train() client id: f_00001-7-5 loss: 0.590149  [  192/  265]
train() client id: f_00001-7-6 loss: 0.416251  [  224/  265]
train() client id: f_00001-7-7 loss: 0.469821  [  256/  265]
train() client id: f_00001-8-0 loss: 0.399837  [   32/  265]
train() client id: f_00001-8-1 loss: 0.550096  [   64/  265]
train() client id: f_00001-8-2 loss: 0.461631  [   96/  265]
train() client id: f_00001-8-3 loss: 0.580766  [  128/  265]
train() client id: f_00001-8-4 loss: 0.508398  [  160/  265]
train() client id: f_00001-8-5 loss: 0.431028  [  192/  265]
train() client id: f_00001-8-6 loss: 0.469402  [  224/  265]
train() client id: f_00001-8-7 loss: 0.468653  [  256/  265]
train() client id: f_00001-9-0 loss: 0.615410  [   32/  265]
train() client id: f_00001-9-1 loss: 0.423014  [   64/  265]
train() client id: f_00001-9-2 loss: 0.476813  [   96/  265]
train() client id: f_00001-9-3 loss: 0.515869  [  128/  265]
train() client id: f_00001-9-4 loss: 0.481378  [  160/  265]
train() client id: f_00001-9-5 loss: 0.407283  [  192/  265]
train() client id: f_00001-9-6 loss: 0.590659  [  224/  265]
train() client id: f_00001-9-7 loss: 0.587804  [  256/  265]
train() client id: f_00001-10-0 loss: 0.560208  [   32/  265]
train() client id: f_00001-10-1 loss: 0.583333  [   64/  265]
train() client id: f_00001-10-2 loss: 0.431997  [   96/  265]
train() client id: f_00001-10-3 loss: 0.417453  [  128/  265]
train() client id: f_00001-10-4 loss: 0.523625  [  160/  265]
train() client id: f_00001-10-5 loss: 0.532666  [  192/  265]
train() client id: f_00001-10-6 loss: 0.590060  [  224/  265]
train() client id: f_00001-10-7 loss: 0.461361  [  256/  265]
train() client id: f_00002-0-0 loss: 1.298376  [   32/  124]
train() client id: f_00002-0-1 loss: 1.191250  [   64/  124]
train() client id: f_00002-0-2 loss: 1.197690  [   96/  124]
train() client id: f_00002-1-0 loss: 1.162316  [   32/  124]
train() client id: f_00002-1-1 loss: 1.440616  [   64/  124]
train() client id: f_00002-1-2 loss: 1.103321  [   96/  124]
train() client id: f_00002-2-0 loss: 1.102009  [   32/  124]
train() client id: f_00002-2-1 loss: 1.224985  [   64/  124]
train() client id: f_00002-2-2 loss: 1.134209  [   96/  124]
train() client id: f_00002-3-0 loss: 1.136779  [   32/  124]
train() client id: f_00002-3-1 loss: 1.037604  [   64/  124]
train() client id: f_00002-3-2 loss: 1.101648  [   96/  124]
train() client id: f_00002-4-0 loss: 1.116346  [   32/  124]
train() client id: f_00002-4-1 loss: 1.205840  [   64/  124]
train() client id: f_00002-4-2 loss: 1.110375  [   96/  124]
train() client id: f_00002-5-0 loss: 1.381660  [   32/  124]
train() client id: f_00002-5-1 loss: 1.105677  [   64/  124]
train() client id: f_00002-5-2 loss: 0.940744  [   96/  124]
train() client id: f_00002-6-0 loss: 1.284463  [   32/  124]
train() client id: f_00002-6-1 loss: 0.916405  [   64/  124]
train() client id: f_00002-6-2 loss: 1.129866  [   96/  124]
train() client id: f_00002-7-0 loss: 1.040758  [   32/  124]
train() client id: f_00002-7-1 loss: 1.050810  [   64/  124]
train() client id: f_00002-7-2 loss: 1.051415  [   96/  124]
train() client id: f_00002-8-0 loss: 1.099574  [   32/  124]
train() client id: f_00002-8-1 loss: 1.022836  [   64/  124]
train() client id: f_00002-8-2 loss: 1.018932  [   96/  124]
train() client id: f_00002-9-0 loss: 1.059378  [   32/  124]
train() client id: f_00002-9-1 loss: 1.232130  [   64/  124]
train() client id: f_00002-9-2 loss: 1.140721  [   96/  124]
train() client id: f_00002-10-0 loss: 1.360938  [   32/  124]
train() client id: f_00002-10-1 loss: 0.949926  [   64/  124]
train() client id: f_00002-10-2 loss: 1.027113  [   96/  124]
train() client id: f_00003-0-0 loss: 0.474433  [   32/   43]
train() client id: f_00003-1-0 loss: 0.364203  [   32/   43]
train() client id: f_00003-2-0 loss: 0.576700  [   32/   43]
train() client id: f_00003-3-0 loss: 0.581005  [   32/   43]
train() client id: f_00003-4-0 loss: 0.643138  [   32/   43]
train() client id: f_00003-5-0 loss: 0.456235  [   32/   43]
train() client id: f_00003-6-0 loss: 0.764188  [   32/   43]
train() client id: f_00003-7-0 loss: 0.550560  [   32/   43]
train() client id: f_00003-8-0 loss: 0.468817  [   32/   43]
train() client id: f_00003-9-0 loss: 0.524701  [   32/   43]
train() client id: f_00003-10-0 loss: 0.489004  [   32/   43]
train() client id: f_00004-0-0 loss: 0.799933  [   32/  306]
train() client id: f_00004-0-1 loss: 0.930438  [   64/  306]
train() client id: f_00004-0-2 loss: 0.847011  [   96/  306]
train() client id: f_00004-0-3 loss: 0.756455  [  128/  306]
train() client id: f_00004-0-4 loss: 0.799945  [  160/  306]
train() client id: f_00004-0-5 loss: 0.847369  [  192/  306]
train() client id: f_00004-0-6 loss: 0.884131  [  224/  306]
train() client id: f_00004-0-7 loss: 0.743389  [  256/  306]
train() client id: f_00004-0-8 loss: 0.783157  [  288/  306]
train() client id: f_00004-1-0 loss: 0.912616  [   32/  306]
train() client id: f_00004-1-1 loss: 0.886859  [   64/  306]
train() client id: f_00004-1-2 loss: 0.845280  [   96/  306]
train() client id: f_00004-1-3 loss: 0.809637  [  128/  306]
train() client id: f_00004-1-4 loss: 0.729792  [  160/  306]
train() client id: f_00004-1-5 loss: 0.704884  [  192/  306]
train() client id: f_00004-1-6 loss: 0.631290  [  224/  306]
train() client id: f_00004-1-7 loss: 0.803414  [  256/  306]
train() client id: f_00004-1-8 loss: 1.050308  [  288/  306]
train() client id: f_00004-2-0 loss: 0.867615  [   32/  306]
train() client id: f_00004-2-1 loss: 0.707790  [   64/  306]
train() client id: f_00004-2-2 loss: 0.790663  [   96/  306]
train() client id: f_00004-2-3 loss: 0.848659  [  128/  306]
train() client id: f_00004-2-4 loss: 0.831688  [  160/  306]
train() client id: f_00004-2-5 loss: 0.856429  [  192/  306]
train() client id: f_00004-2-6 loss: 0.868045  [  224/  306]
train() client id: f_00004-2-7 loss: 0.751380  [  256/  306]
train() client id: f_00004-2-8 loss: 0.788228  [  288/  306]
train() client id: f_00004-3-0 loss: 0.791105  [   32/  306]
train() client id: f_00004-3-1 loss: 0.885076  [   64/  306]
train() client id: f_00004-3-2 loss: 0.735857  [   96/  306]
train() client id: f_00004-3-3 loss: 0.809601  [  128/  306]
train() client id: f_00004-3-4 loss: 0.782811  [  160/  306]
train() client id: f_00004-3-5 loss: 0.751682  [  192/  306]
train() client id: f_00004-3-6 loss: 0.868393  [  224/  306]
train() client id: f_00004-3-7 loss: 0.793230  [  256/  306]
train() client id: f_00004-3-8 loss: 0.893990  [  288/  306]
train() client id: f_00004-4-0 loss: 0.850331  [   32/  306]
train() client id: f_00004-4-1 loss: 0.841935  [   64/  306]
train() client id: f_00004-4-2 loss: 0.883382  [   96/  306]
train() client id: f_00004-4-3 loss: 0.785426  [  128/  306]
train() client id: f_00004-4-4 loss: 0.787024  [  160/  306]
train() client id: f_00004-4-5 loss: 0.832656  [  192/  306]
train() client id: f_00004-4-6 loss: 0.728145  [  224/  306]
train() client id: f_00004-4-7 loss: 0.816453  [  256/  306]
train() client id: f_00004-4-8 loss: 0.815056  [  288/  306]
train() client id: f_00004-5-0 loss: 0.677651  [   32/  306]
train() client id: f_00004-5-1 loss: 0.793051  [   64/  306]
train() client id: f_00004-5-2 loss: 0.846547  [   96/  306]
train() client id: f_00004-5-3 loss: 0.752869  [  128/  306]
train() client id: f_00004-5-4 loss: 0.815552  [  160/  306]
train() client id: f_00004-5-5 loss: 0.818055  [  192/  306]
train() client id: f_00004-5-6 loss: 0.824308  [  224/  306]
train() client id: f_00004-5-7 loss: 0.832765  [  256/  306]
train() client id: f_00004-5-8 loss: 0.783870  [  288/  306]
train() client id: f_00004-6-0 loss: 0.748337  [   32/  306]
train() client id: f_00004-6-1 loss: 0.913340  [   64/  306]
train() client id: f_00004-6-2 loss: 0.779213  [   96/  306]
train() client id: f_00004-6-3 loss: 0.823807  [  128/  306]
train() client id: f_00004-6-4 loss: 0.789472  [  160/  306]
train() client id: f_00004-6-5 loss: 0.837587  [  192/  306]
train() client id: f_00004-6-6 loss: 0.872524  [  224/  306]
train() client id: f_00004-6-7 loss: 0.795068  [  256/  306]
train() client id: f_00004-6-8 loss: 0.691049  [  288/  306]
train() client id: f_00004-7-0 loss: 0.843279  [   32/  306]
train() client id: f_00004-7-1 loss: 0.863769  [   64/  306]
train() client id: f_00004-7-2 loss: 0.822513  [   96/  306]
train() client id: f_00004-7-3 loss: 0.832005  [  128/  306]
train() client id: f_00004-7-4 loss: 0.830293  [  160/  306]
train() client id: f_00004-7-5 loss: 0.726599  [  192/  306]
train() client id: f_00004-7-6 loss: 0.833998  [  224/  306]
train() client id: f_00004-7-7 loss: 0.697857  [  256/  306]
train() client id: f_00004-7-8 loss: 0.819373  [  288/  306]
train() client id: f_00004-8-0 loss: 0.763610  [   32/  306]
train() client id: f_00004-8-1 loss: 0.826681  [   64/  306]
train() client id: f_00004-8-2 loss: 0.726613  [   96/  306]
train() client id: f_00004-8-3 loss: 0.962943  [  128/  306]
train() client id: f_00004-8-4 loss: 0.745758  [  160/  306]
train() client id: f_00004-8-5 loss: 0.728328  [  192/  306]
train() client id: f_00004-8-6 loss: 0.741608  [  224/  306]
train() client id: f_00004-8-7 loss: 0.969349  [  256/  306]
train() client id: f_00004-8-8 loss: 0.882034  [  288/  306]
train() client id: f_00004-9-0 loss: 0.899272  [   32/  306]
train() client id: f_00004-9-1 loss: 0.956769  [   64/  306]
train() client id: f_00004-9-2 loss: 0.717464  [   96/  306]
train() client id: f_00004-9-3 loss: 0.887008  [  128/  306]
train() client id: f_00004-9-4 loss: 0.893645  [  160/  306]
train() client id: f_00004-9-5 loss: 0.653671  [  192/  306]
train() client id: f_00004-9-6 loss: 0.841510  [  224/  306]
train() client id: f_00004-9-7 loss: 0.696426  [  256/  306]
train() client id: f_00004-9-8 loss: 0.727666  [  288/  306]
train() client id: f_00004-10-0 loss: 0.752007  [   32/  306]
train() client id: f_00004-10-1 loss: 0.793202  [   64/  306]
train() client id: f_00004-10-2 loss: 0.900893  [   96/  306]
train() client id: f_00004-10-3 loss: 0.793979  [  128/  306]
train() client id: f_00004-10-4 loss: 0.820095  [  160/  306]
train() client id: f_00004-10-5 loss: 0.743777  [  192/  306]
train() client id: f_00004-10-6 loss: 0.813364  [  224/  306]
train() client id: f_00004-10-7 loss: 0.722436  [  256/  306]
train() client id: f_00004-10-8 loss: 0.835789  [  288/  306]
train() client id: f_00005-0-0 loss: 0.775582  [   32/  146]
train() client id: f_00005-0-1 loss: 0.812767  [   64/  146]
train() client id: f_00005-0-2 loss: 0.948417  [   96/  146]
train() client id: f_00005-0-3 loss: 0.795408  [  128/  146]
train() client id: f_00005-1-0 loss: 1.010857  [   32/  146]
train() client id: f_00005-1-1 loss: 0.851379  [   64/  146]
train() client id: f_00005-1-2 loss: 0.942765  [   96/  146]
train() client id: f_00005-1-3 loss: 0.668861  [  128/  146]
train() client id: f_00005-2-0 loss: 0.742830  [   32/  146]
train() client id: f_00005-2-1 loss: 0.904133  [   64/  146]
train() client id: f_00005-2-2 loss: 0.752335  [   96/  146]
train() client id: f_00005-2-3 loss: 0.854053  [  128/  146]
train() client id: f_00005-3-0 loss: 0.697416  [   32/  146]
train() client id: f_00005-3-1 loss: 0.763486  [   64/  146]
train() client id: f_00005-3-2 loss: 1.025702  [   96/  146]
train() client id: f_00005-3-3 loss: 0.888755  [  128/  146]
train() client id: f_00005-4-0 loss: 0.798155  [   32/  146]
train() client id: f_00005-4-1 loss: 0.694380  [   64/  146]
train() client id: f_00005-4-2 loss: 1.051880  [   96/  146]
train() client id: f_00005-4-3 loss: 0.795695  [  128/  146]
train() client id: f_00005-5-0 loss: 0.769213  [   32/  146]
train() client id: f_00005-5-1 loss: 0.832500  [   64/  146]
train() client id: f_00005-5-2 loss: 0.918860  [   96/  146]
train() client id: f_00005-5-3 loss: 0.729293  [  128/  146]
train() client id: f_00005-6-0 loss: 0.802778  [   32/  146]
train() client id: f_00005-6-1 loss: 0.801829  [   64/  146]
train() client id: f_00005-6-2 loss: 0.795112  [   96/  146]
train() client id: f_00005-6-3 loss: 1.001282  [  128/  146]
train() client id: f_00005-7-0 loss: 0.877493  [   32/  146]
train() client id: f_00005-7-1 loss: 0.697379  [   64/  146]
train() client id: f_00005-7-2 loss: 1.005176  [   96/  146]
train() client id: f_00005-7-3 loss: 0.804258  [  128/  146]
train() client id: f_00005-8-0 loss: 0.848766  [   32/  146]
train() client id: f_00005-8-1 loss: 0.904792  [   64/  146]
train() client id: f_00005-8-2 loss: 0.808971  [   96/  146]
train() client id: f_00005-8-3 loss: 0.895357  [  128/  146]
train() client id: f_00005-9-0 loss: 0.905942  [   32/  146]
train() client id: f_00005-9-1 loss: 0.993091  [   64/  146]
train() client id: f_00005-9-2 loss: 0.746004  [   96/  146]
train() client id: f_00005-9-3 loss: 1.064618  [  128/  146]
train() client id: f_00005-10-0 loss: 1.152799  [   32/  146]
train() client id: f_00005-10-1 loss: 0.849607  [   64/  146]
train() client id: f_00005-10-2 loss: 0.876189  [   96/  146]
train() client id: f_00005-10-3 loss: 0.709614  [  128/  146]
train() client id: f_00006-0-0 loss: 0.477804  [   32/   54]
train() client id: f_00006-1-0 loss: 0.412591  [   32/   54]
train() client id: f_00006-2-0 loss: 0.469949  [   32/   54]
train() client id: f_00006-3-0 loss: 0.464653  [   32/   54]
train() client id: f_00006-4-0 loss: 0.532455  [   32/   54]
train() client id: f_00006-5-0 loss: 0.532321  [   32/   54]
train() client id: f_00006-6-0 loss: 0.522501  [   32/   54]
train() client id: f_00006-7-0 loss: 0.508862  [   32/   54]
train() client id: f_00006-8-0 loss: 0.527370  [   32/   54]
train() client id: f_00006-9-0 loss: 0.519534  [   32/   54]
train() client id: f_00006-10-0 loss: 0.414327  [   32/   54]
train() client id: f_00007-0-0 loss: 0.577732  [   32/  179]
train() client id: f_00007-0-1 loss: 0.815066  [   64/  179]
train() client id: f_00007-0-2 loss: 0.737629  [   96/  179]
train() client id: f_00007-0-3 loss: 0.574816  [  128/  179]
train() client id: f_00007-0-4 loss: 0.687652  [  160/  179]
train() client id: f_00007-1-0 loss: 0.650168  [   32/  179]
train() client id: f_00007-1-1 loss: 0.752121  [   64/  179]
train() client id: f_00007-1-2 loss: 0.475477  [   96/  179]
train() client id: f_00007-1-3 loss: 0.833768  [  128/  179]
train() client id: f_00007-1-4 loss: 0.665009  [  160/  179]
train() client id: f_00007-2-0 loss: 0.520689  [   32/  179]
train() client id: f_00007-2-1 loss: 0.926002  [   64/  179]
train() client id: f_00007-2-2 loss: 0.560988  [   96/  179]
train() client id: f_00007-2-3 loss: 0.693133  [  128/  179]
train() client id: f_00007-2-4 loss: 0.639464  [  160/  179]
train() client id: f_00007-3-0 loss: 0.620656  [   32/  179]
train() client id: f_00007-3-1 loss: 0.724253  [   64/  179]
train() client id: f_00007-3-2 loss: 0.668835  [   96/  179]
train() client id: f_00007-3-3 loss: 0.631219  [  128/  179]
train() client id: f_00007-3-4 loss: 0.665330  [  160/  179]
train() client id: f_00007-4-0 loss: 0.588941  [   32/  179]
train() client id: f_00007-4-1 loss: 0.731274  [   64/  179]
train() client id: f_00007-4-2 loss: 0.574296  [   96/  179]
train() client id: f_00007-4-3 loss: 0.557426  [  128/  179]
train() client id: f_00007-4-4 loss: 0.806865  [  160/  179]
train() client id: f_00007-5-0 loss: 0.683186  [   32/  179]
train() client id: f_00007-5-1 loss: 0.681502  [   64/  179]
train() client id: f_00007-5-2 loss: 0.747446  [   96/  179]
train() client id: f_00007-5-3 loss: 0.526689  [  128/  179]
train() client id: f_00007-5-4 loss: 0.492841  [  160/  179]
train() client id: f_00007-6-0 loss: 0.542571  [   32/  179]
train() client id: f_00007-6-1 loss: 0.462816  [   64/  179]
train() client id: f_00007-6-2 loss: 0.689056  [   96/  179]
train() client id: f_00007-6-3 loss: 0.845214  [  128/  179]
train() client id: f_00007-6-4 loss: 0.571219  [  160/  179]
train() client id: f_00007-7-0 loss: 0.710991  [   32/  179]
train() client id: f_00007-7-1 loss: 0.830607  [   64/  179]
train() client id: f_00007-7-2 loss: 0.705249  [   96/  179]
train() client id: f_00007-7-3 loss: 0.547041  [  128/  179]
train() client id: f_00007-7-4 loss: 0.443603  [  160/  179]
train() client id: f_00007-8-0 loss: 0.512276  [   32/  179]
train() client id: f_00007-8-1 loss: 0.504598  [   64/  179]
train() client id: f_00007-8-2 loss: 0.628655  [   96/  179]
train() client id: f_00007-8-3 loss: 0.731521  [  128/  179]
train() client id: f_00007-8-4 loss: 0.763180  [  160/  179]
train() client id: f_00007-9-0 loss: 0.695273  [   32/  179]
train() client id: f_00007-9-1 loss: 0.439494  [   64/  179]
train() client id: f_00007-9-2 loss: 0.920907  [   96/  179]
train() client id: f_00007-9-3 loss: 0.490562  [  128/  179]
train() client id: f_00007-9-4 loss: 0.663213  [  160/  179]
train() client id: f_00007-10-0 loss: 0.570489  [   32/  179]
train() client id: f_00007-10-1 loss: 0.669300  [   64/  179]
train() client id: f_00007-10-2 loss: 0.563234  [   96/  179]
train() client id: f_00007-10-3 loss: 0.729087  [  128/  179]
train() client id: f_00007-10-4 loss: 0.607304  [  160/  179]
train() client id: f_00008-0-0 loss: 0.633035  [   32/  130]
train() client id: f_00008-0-1 loss: 0.716921  [   64/  130]
train() client id: f_00008-0-2 loss: 0.652495  [   96/  130]
train() client id: f_00008-0-3 loss: 0.779994  [  128/  130]
train() client id: f_00008-1-0 loss: 0.687803  [   32/  130]
train() client id: f_00008-1-1 loss: 0.779925  [   64/  130]
train() client id: f_00008-1-2 loss: 0.568576  [   96/  130]
train() client id: f_00008-1-3 loss: 0.656589  [  128/  130]
train() client id: f_00008-2-0 loss: 0.696062  [   32/  130]
train() client id: f_00008-2-1 loss: 0.716452  [   64/  130]
train() client id: f_00008-2-2 loss: 0.642519  [   96/  130]
train() client id: f_00008-2-3 loss: 0.725246  [  128/  130]
train() client id: f_00008-3-0 loss: 0.742436  [   32/  130]
train() client id: f_00008-3-1 loss: 0.708071  [   64/  130]
train() client id: f_00008-3-2 loss: 0.684861  [   96/  130]
train() client id: f_00008-3-3 loss: 0.640814  [  128/  130]
train() client id: f_00008-4-0 loss: 0.676425  [   32/  130]
train() client id: f_00008-4-1 loss: 0.534005  [   64/  130]
train() client id: f_00008-4-2 loss: 0.725763  [   96/  130]
train() client id: f_00008-4-3 loss: 0.829342  [  128/  130]
train() client id: f_00008-5-0 loss: 0.596043  [   32/  130]
train() client id: f_00008-5-1 loss: 0.667305  [   64/  130]
train() client id: f_00008-5-2 loss: 0.735109  [   96/  130]
train() client id: f_00008-5-3 loss: 0.774139  [  128/  130]
train() client id: f_00008-6-0 loss: 0.780941  [   32/  130]
train() client id: f_00008-6-1 loss: 0.691867  [   64/  130]
train() client id: f_00008-6-2 loss: 0.597390  [   96/  130]
train() client id: f_00008-6-3 loss: 0.704921  [  128/  130]
train() client id: f_00008-7-0 loss: 0.614521  [   32/  130]
train() client id: f_00008-7-1 loss: 0.773184  [   64/  130]
train() client id: f_00008-7-2 loss: 0.677643  [   96/  130]
train() client id: f_00008-7-3 loss: 0.675457  [  128/  130]
train() client id: f_00008-8-0 loss: 0.672964  [   32/  130]
train() client id: f_00008-8-1 loss: 0.624256  [   64/  130]
train() client id: f_00008-8-2 loss: 0.789478  [   96/  130]
train() client id: f_00008-8-3 loss: 0.650216  [  128/  130]
train() client id: f_00008-9-0 loss: 0.668226  [   32/  130]
train() client id: f_00008-9-1 loss: 0.660488  [   64/  130]
train() client id: f_00008-9-2 loss: 0.629088  [   96/  130]
train() client id: f_00008-9-3 loss: 0.797582  [  128/  130]
train() client id: f_00008-10-0 loss: 0.819203  [   32/  130]
train() client id: f_00008-10-1 loss: 0.649880  [   64/  130]
train() client id: f_00008-10-2 loss: 0.641436  [   96/  130]
train() client id: f_00008-10-3 loss: 0.650292  [  128/  130]
train() client id: f_00009-0-0 loss: 1.040999  [   32/  118]
train() client id: f_00009-0-1 loss: 1.193267  [   64/  118]
train() client id: f_00009-0-2 loss: 0.994189  [   96/  118]
train() client id: f_00009-1-0 loss: 1.039320  [   32/  118]
train() client id: f_00009-1-1 loss: 1.112271  [   64/  118]
train() client id: f_00009-1-2 loss: 0.919422  [   96/  118]
train() client id: f_00009-2-0 loss: 0.979108  [   32/  118]
train() client id: f_00009-2-1 loss: 0.879162  [   64/  118]
train() client id: f_00009-2-2 loss: 1.060022  [   96/  118]
train() client id: f_00009-3-0 loss: 0.843973  [   32/  118]
train() client id: f_00009-3-1 loss: 0.966785  [   64/  118]
train() client id: f_00009-3-2 loss: 0.916333  [   96/  118]
train() client id: f_00009-4-0 loss: 0.913904  [   32/  118]
train() client id: f_00009-4-1 loss: 0.955934  [   64/  118]
train() client id: f_00009-4-2 loss: 0.870655  [   96/  118]
train() client id: f_00009-5-0 loss: 0.959876  [   32/  118]
train() client id: f_00009-5-1 loss: 0.860635  [   64/  118]
train() client id: f_00009-5-2 loss: 0.809032  [   96/  118]
train() client id: f_00009-6-0 loss: 0.865383  [   32/  118]
train() client id: f_00009-6-1 loss: 0.925838  [   64/  118]
train() client id: f_00009-6-2 loss: 0.904288  [   96/  118]
train() client id: f_00009-7-0 loss: 0.862433  [   32/  118]
train() client id: f_00009-7-1 loss: 0.936396  [   64/  118]
train() client id: f_00009-7-2 loss: 0.826126  [   96/  118]
train() client id: f_00009-8-0 loss: 0.999868  [   32/  118]
train() client id: f_00009-8-1 loss: 0.815418  [   64/  118]
train() client id: f_00009-8-2 loss: 0.634257  [   96/  118]
train() client id: f_00009-9-0 loss: 0.817412  [   32/  118]
train() client id: f_00009-9-1 loss: 0.908009  [   64/  118]
train() client id: f_00009-9-2 loss: 0.733637  [   96/  118]
train() client id: f_00009-10-0 loss: 0.826566  [   32/  118]
train() client id: f_00009-10-1 loss: 0.698581  [   64/  118]
train() client id: f_00009-10-2 loss: 1.003220  [   96/  118]
At round 55 accuracy: 0.6472148541114059
At round 55 training accuracy: 0.5895372233400402
At round 55 training loss: 0.8220929992833068
update_location
xs = -3.905658 4.200318 295.009024 18.811294 0.979296 3.956410 -257.443192 -236.324852 279.663977 -222.060879 
ys = 287.587959 270.555839 1.320614 -257.455176 249.350187 232.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -11.711426168117436
ys mean: 80.39414253552872
dists_uav = 304.503019 288.475483 311.499708 276.833944 268.656797 253.412869 276.195379 256.612766 297.524133 243.571438 
uav_gains = -116.100494 -114.739068 -116.653005 -113.692681 -112.948232 -111.588980 -113.634598 -111.868266 -115.522498 -110.759871 
uav_gains_db_mean: -113.7507692882309
dists_bs = 204.815337 203.085382 501.071863 473.899037 191.040998 188.063480 195.825215 184.658040 481.148470 177.356194 
bs_gains = -104.285208 -104.182062 -115.164260 -114.486262 -103.438603 -103.247584 -103.739380 -103.025369 -114.670874 -102.534757 
bs_gains_db_mean: -106.87743607727668
Round 56
-------------------------------
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.61572451 7.33497615 3.57029297 1.30775167 8.45662183 4.06918315
 1.60870824 5.01890964 3.70473634 3.29930761]
obj_prev = 41.986212112103765
eta_min = 2.748442212366637e-26	eta_max = 0.9375089458427088
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 9.666975258494574	eta = 0.909090909090909
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 21.262798910283387	eta = 0.41331150066296846
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 15.102037364580543	eta = 0.5819187910708925
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 14.000878826985186	eta = 0.6276862641626415
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 13.93490538719279	eta = 0.6306579830804683
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 13.934640394529897	eta = 0.6306699761950073
eta = 0.6306699761950073
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [0.03803835 0.0800013  0.03743457 0.01298134 0.09237885 0.04407619
 0.01630215 0.05403861 0.03924592 0.03562322]
ene_total = [1.3587627  2.13524871 1.36925661 0.66156516 2.4302258  1.25152696
 0.7407784  1.61476761 1.33281256 1.03969587]
ti_comp = [0.93897812 1.03996482 0.92863396 0.97451355 1.04269211 1.04336226
 0.97521529 0.99392524 0.96170282 1.04576067]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [3.90152269e-06 2.95892377e-05 3.80197612e-06 1.43967031e-07
 4.53195465e-05 4.91611198e-06 2.84717213e-07 9.98355239e-06
 4.08490276e-06 2.58353803e-06]
ene_total = [0.43191848 0.18645457 0.45712329 0.34523224 0.18019189 0.17757424
 0.34352564 0.29816856 0.37654616 0.17167282]
optimize_network iter = 0 obj = 2.96840789621886
eta = 0.6306699761950073
freqs = [20255185.25779217 38463462.35602392 20155718.00296714  6660421.60236042
 44298241.69414774 21122186.81412296  8358231.69852659 27184444.26259059
 20404389.67271167 17032203.97092095]
eta_min = 0.6306699761950091	eta_max = 0.7038160577019408
af = 0.0020423891099591697	bf = 1.0953806736156944	zeta = 0.0022466280209550867	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [8.06674524e-07 6.11783813e-06 7.86092385e-07 2.97664645e-08
 9.37021941e-06 1.01644989e-06 5.88678167e-08 2.06418827e-06
 8.44589985e-07 5.34169471e-07]
ene_total = [1.75441871 0.75517231 1.85682433 1.40253966 0.72849401 0.72103243
 1.3955953  1.21056458 1.52944703 0.69724031]
ti_comp = [0.71791719 0.81890388 0.70757302 0.75345262 0.82163117 0.82230133
 0.75415436 0.77286431 0.74064189 0.82469974]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.95240886e-06 2.11098588e-05 2.89691726e-06 1.06538614e-07
 3.22867276e-05 3.50114543e-06 2.10607755e-07 7.30409071e-06
 3.04668167e-06 1.83767148e-06]
ene_total = [0.53855695 0.23224405 0.56998772 0.43049024 0.22429635 0.22138528
 0.42836106 0.37172334 0.46950708 0.21404676]
optimize_network iter = 1 obj = 3.70059882926673
eta = 0.7038160577019408
freqs = [20185710.85684811 37218665.59464449 20155718.00296712  6563875.93252434
 42834368.86978544 20420657.36097779  8235336.89300485 26637755.85065275
 20187518.35748624 16456368.51125106]
eta_min = 0.7038160577019519	eta_max = 0.703816057701941
af = 0.0019282538903956608	bf = 1.0953806736156944	zeta = 0.0021210792794352272	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [8.01150298e-07 5.72826139e-06 7.86092385e-07 2.89097636e-08
 8.76115832e-06 9.50052597e-07 5.71494239e-08 1.98200003e-06
 8.26731677e-07 4.98660964e-07]
ene_total = [1.75441816 0.75513375 1.85682433 1.40253957 0.72843371 0.72102585
 1.39559513 1.21055645 1.52944526 0.6972368 ]
ti_comp = [0.71791719 0.81890388 0.70757302 0.75345262 0.82163117 0.82230133
 0.75415436 0.77286431 0.74064189 0.82469974]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.95240886e-06 2.11098588e-05 2.89691726e-06 1.06538614e-07
 3.22867276e-05 3.50114543e-06 2.10607755e-07 7.30409071e-06
 3.04668167e-06 1.83767148e-06]
ene_total = [0.53855695 0.23224405 0.56998772 0.43049024 0.22429635 0.22138528
 0.42836106 0.37172334 0.46950708 0.21404676]
optimize_network iter = 2 obj = 3.7005988292667333
eta = 0.703816057701941
freqs = [20185710.85684812 37218665.5946445  20155718.00296713  6563875.93252435
 42834368.86978545 20420657.36097779  8235336.89300485 26637755.85065275
 20187518.35748625 16456368.51125106]
Done!
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.82371805e-06 2.01897137e-05 2.77064524e-06 1.01894765e-07
 3.08794005e-05 3.34853607e-06 2.01427698e-07 6.98571701e-06
 2.91388167e-06 1.75757030e-06]
ene_total = [0.01772335 0.00764205 0.01875771 0.01416708 0.00738001 0.00728546
 0.01409701 0.0122328  0.01545097 0.00704403]
At round 56 energy consumption: 0.12178045716881733
At round 56 eta: 0.703816057701941
At round 56 a_n: 9.000035425972609
At round 56 local rounds: 11.50132443724633
At round 56 global rounds: 30.386642017599986
gradient difference: 0.537227988243103
train() client id: f_00000-0-0 loss: 0.839613  [   32/  126]
train() client id: f_00000-0-1 loss: 1.298650  [   64/  126]
train() client id: f_00000-0-2 loss: 0.901521  [   96/  126]
train() client id: f_00000-1-0 loss: 1.074251  [   32/  126]
train() client id: f_00000-1-1 loss: 0.945738  [   64/  126]
train() client id: f_00000-1-2 loss: 0.980589  [   96/  126]
train() client id: f_00000-2-0 loss: 0.899122  [   32/  126]
train() client id: f_00000-2-1 loss: 0.995848  [   64/  126]
train() client id: f_00000-2-2 loss: 1.015524  [   96/  126]
train() client id: f_00000-3-0 loss: 0.724384  [   32/  126]
train() client id: f_00000-3-1 loss: 0.846185  [   64/  126]
train() client id: f_00000-3-2 loss: 1.063267  [   96/  126]
train() client id: f_00000-4-0 loss: 1.105198  [   32/  126]
train() client id: f_00000-4-1 loss: 0.893244  [   64/  126]
train() client id: f_00000-4-2 loss: 0.876136  [   96/  126]
train() client id: f_00000-5-0 loss: 0.926425  [   32/  126]
train() client id: f_00000-5-1 loss: 0.897455  [   64/  126]
train() client id: f_00000-5-2 loss: 0.797481  [   96/  126]
train() client id: f_00000-6-0 loss: 0.881463  [   32/  126]
train() client id: f_00000-6-1 loss: 0.881926  [   64/  126]
train() client id: f_00000-6-2 loss: 0.912400  [   96/  126]
train() client id: f_00000-7-0 loss: 0.890081  [   32/  126]
train() client id: f_00000-7-1 loss: 0.877628  [   64/  126]
train() client id: f_00000-7-2 loss: 0.845841  [   96/  126]
train() client id: f_00000-8-0 loss: 0.842136  [   32/  126]
train() client id: f_00000-8-1 loss: 0.878850  [   64/  126]
train() client id: f_00000-8-2 loss: 0.950477  [   96/  126]
train() client id: f_00000-9-0 loss: 0.816476  [   32/  126]
train() client id: f_00000-9-1 loss: 0.950974  [   64/  126]
train() client id: f_00000-9-2 loss: 0.896260  [   96/  126]
train() client id: f_00000-10-0 loss: 0.822873  [   32/  126]
train() client id: f_00000-10-1 loss: 0.890052  [   64/  126]
train() client id: f_00000-10-2 loss: 0.906040  [   96/  126]
train() client id: f_00001-0-0 loss: 0.422355  [   32/  265]
train() client id: f_00001-0-1 loss: 0.633534  [   64/  265]
train() client id: f_00001-0-2 loss: 0.491726  [   96/  265]
train() client id: f_00001-0-3 loss: 0.450532  [  128/  265]
train() client id: f_00001-0-4 loss: 0.422156  [  160/  265]
train() client id: f_00001-0-5 loss: 0.413024  [  192/  265]
train() client id: f_00001-0-6 loss: 0.531198  [  224/  265]
train() client id: f_00001-0-7 loss: 0.409825  [  256/  265]
train() client id: f_00001-1-0 loss: 0.456134  [   32/  265]
train() client id: f_00001-1-1 loss: 0.495012  [   64/  265]
train() client id: f_00001-1-2 loss: 0.454696  [   96/  265]
train() client id: f_00001-1-3 loss: 0.508638  [  128/  265]
train() client id: f_00001-1-4 loss: 0.462421  [  160/  265]
train() client id: f_00001-1-5 loss: 0.412688  [  192/  265]
train() client id: f_00001-1-6 loss: 0.592833  [  224/  265]
train() client id: f_00001-1-7 loss: 0.358321  [  256/  265]
train() client id: f_00001-2-0 loss: 0.435465  [   32/  265]
train() client id: f_00001-2-1 loss: 0.344603  [   64/  265]
train() client id: f_00001-2-2 loss: 0.536936  [   96/  265]
train() client id: f_00001-2-3 loss: 0.361023  [  128/  265]
train() client id: f_00001-2-4 loss: 0.479026  [  160/  265]
train() client id: f_00001-2-5 loss: 0.539640  [  192/  265]
train() client id: f_00001-2-6 loss: 0.446511  [  224/  265]
train() client id: f_00001-2-7 loss: 0.575177  [  256/  265]
train() client id: f_00001-3-0 loss: 0.459641  [   32/  265]
train() client id: f_00001-3-1 loss: 0.478582  [   64/  265]
train() client id: f_00001-3-2 loss: 0.519041  [   96/  265]
train() client id: f_00001-3-3 loss: 0.449143  [  128/  265]
train() client id: f_00001-3-4 loss: 0.372537  [  160/  265]
train() client id: f_00001-3-5 loss: 0.412770  [  192/  265]
train() client id: f_00001-3-6 loss: 0.436151  [  224/  265]
train() client id: f_00001-3-7 loss: 0.554093  [  256/  265]
train() client id: f_00001-4-0 loss: 0.539508  [   32/  265]
train() client id: f_00001-4-1 loss: 0.421407  [   64/  265]
train() client id: f_00001-4-2 loss: 0.565515  [   96/  265]
train() client id: f_00001-4-3 loss: 0.410344  [  128/  265]
train() client id: f_00001-4-4 loss: 0.385272  [  160/  265]
train() client id: f_00001-4-5 loss: 0.584821  [  192/  265]
train() client id: f_00001-4-6 loss: 0.388409  [  224/  265]
train() client id: f_00001-4-7 loss: 0.339275  [  256/  265]
train() client id: f_00001-5-0 loss: 0.388359  [   32/  265]
train() client id: f_00001-5-1 loss: 0.403576  [   64/  265]
train() client id: f_00001-5-2 loss: 0.477547  [   96/  265]
train() client id: f_00001-5-3 loss: 0.459864  [  128/  265]
train() client id: f_00001-5-4 loss: 0.480304  [  160/  265]
train() client id: f_00001-5-5 loss: 0.530828  [  192/  265]
train() client id: f_00001-5-6 loss: 0.470165  [  224/  265]
train() client id: f_00001-5-7 loss: 0.417837  [  256/  265]
train() client id: f_00001-6-0 loss: 0.478238  [   32/  265]
train() client id: f_00001-6-1 loss: 0.563995  [   64/  265]
train() client id: f_00001-6-2 loss: 0.425428  [   96/  265]
train() client id: f_00001-6-3 loss: 0.465094  [  128/  265]
train() client id: f_00001-6-4 loss: 0.400135  [  160/  265]
train() client id: f_00001-6-5 loss: 0.357158  [  192/  265]
train() client id: f_00001-6-6 loss: 0.521100  [  224/  265]
train() client id: f_00001-6-7 loss: 0.401245  [  256/  265]
train() client id: f_00001-7-0 loss: 0.523187  [   32/  265]
train() client id: f_00001-7-1 loss: 0.361533  [   64/  265]
train() client id: f_00001-7-2 loss: 0.340277  [   96/  265]
train() client id: f_00001-7-3 loss: 0.484157  [  128/  265]
train() client id: f_00001-7-4 loss: 0.403233  [  160/  265]
train() client id: f_00001-7-5 loss: 0.682156  [  192/  265]
train() client id: f_00001-7-6 loss: 0.350676  [  224/  265]
train() client id: f_00001-7-7 loss: 0.456222  [  256/  265]
train() client id: f_00001-8-0 loss: 0.352492  [   32/  265]
train() client id: f_00001-8-1 loss: 0.822387  [   64/  265]
train() client id: f_00001-8-2 loss: 0.450616  [   96/  265]
train() client id: f_00001-8-3 loss: 0.348498  [  128/  265]
train() client id: f_00001-8-4 loss: 0.398873  [  160/  265]
train() client id: f_00001-8-5 loss: 0.429756  [  192/  265]
train() client id: f_00001-8-6 loss: 0.412121  [  224/  265]
train() client id: f_00001-8-7 loss: 0.394900  [  256/  265]
train() client id: f_00001-9-0 loss: 0.463782  [   32/  265]
train() client id: f_00001-9-1 loss: 0.399596  [   64/  265]
train() client id: f_00001-9-2 loss: 0.660578  [   96/  265]
train() client id: f_00001-9-3 loss: 0.382419  [  128/  265]
train() client id: f_00001-9-4 loss: 0.330694  [  160/  265]
train() client id: f_00001-9-5 loss: 0.602566  [  192/  265]
train() client id: f_00001-9-6 loss: 0.355829  [  224/  265]
train() client id: f_00001-9-7 loss: 0.406383  [  256/  265]
train() client id: f_00001-10-0 loss: 0.413779  [   32/  265]
train() client id: f_00001-10-1 loss: 0.380399  [   64/  265]
train() client id: f_00001-10-2 loss: 0.578165  [   96/  265]
train() client id: f_00001-10-3 loss: 0.523418  [  128/  265]
train() client id: f_00001-10-4 loss: 0.471759  [  160/  265]
train() client id: f_00001-10-5 loss: 0.347246  [  192/  265]
train() client id: f_00001-10-6 loss: 0.470437  [  224/  265]
train() client id: f_00001-10-7 loss: 0.423461  [  256/  265]
train() client id: f_00002-0-0 loss: 1.026937  [   32/  124]
train() client id: f_00002-0-1 loss: 1.045332  [   64/  124]
train() client id: f_00002-0-2 loss: 1.018382  [   96/  124]
train() client id: f_00002-1-0 loss: 1.243322  [   32/  124]
train() client id: f_00002-1-1 loss: 0.929465  [   64/  124]
train() client id: f_00002-1-2 loss: 1.161079  [   96/  124]
train() client id: f_00002-2-0 loss: 0.924800  [   32/  124]
train() client id: f_00002-2-1 loss: 1.140696  [   64/  124]
train() client id: f_00002-2-2 loss: 1.073384  [   96/  124]
train() client id: f_00002-3-0 loss: 1.019117  [   32/  124]
train() client id: f_00002-3-1 loss: 1.148909  [   64/  124]
train() client id: f_00002-3-2 loss: 1.117479  [   96/  124]
train() client id: f_00002-4-0 loss: 1.133461  [   32/  124]
train() client id: f_00002-4-1 loss: 0.966225  [   64/  124]
train() client id: f_00002-4-2 loss: 1.113220  [   96/  124]
train() client id: f_00002-5-0 loss: 0.934767  [   32/  124]
train() client id: f_00002-5-1 loss: 1.153902  [   64/  124]
train() client id: f_00002-5-2 loss: 0.897906  [   96/  124]
train() client id: f_00002-6-0 loss: 1.010158  [   32/  124]
train() client id: f_00002-6-1 loss: 1.020524  [   64/  124]
train() client id: f_00002-6-2 loss: 1.092286  [   96/  124]
train() client id: f_00002-7-0 loss: 0.913895  [   32/  124]
train() client id: f_00002-7-1 loss: 0.978380  [   64/  124]
train() client id: f_00002-7-2 loss: 1.051216  [   96/  124]
train() client id: f_00002-8-0 loss: 0.827622  [   32/  124]
train() client id: f_00002-8-1 loss: 1.060631  [   64/  124]
train() client id: f_00002-8-2 loss: 1.087463  [   96/  124]
train() client id: f_00002-9-0 loss: 0.931643  [   32/  124]
train() client id: f_00002-9-1 loss: 1.085691  [   64/  124]
train() client id: f_00002-9-2 loss: 1.024908  [   96/  124]
train() client id: f_00002-10-0 loss: 1.180060  [   32/  124]
train() client id: f_00002-10-1 loss: 1.055413  [   64/  124]
train() client id: f_00002-10-2 loss: 0.918097  [   96/  124]
train() client id: f_00003-0-0 loss: 0.692750  [   32/   43]
train() client id: f_00003-1-0 loss: 0.682498  [   32/   43]
train() client id: f_00003-2-0 loss: 0.858844  [   32/   43]
train() client id: f_00003-3-0 loss: 0.712961  [   32/   43]
train() client id: f_00003-4-0 loss: 0.904934  [   32/   43]
train() client id: f_00003-5-0 loss: 0.693541  [   32/   43]
train() client id: f_00003-6-0 loss: 0.950548  [   32/   43]
train() client id: f_00003-7-0 loss: 0.813664  [   32/   43]
train() client id: f_00003-8-0 loss: 0.963148  [   32/   43]
train() client id: f_00003-9-0 loss: 0.796375  [   32/   43]
train() client id: f_00003-10-0 loss: 0.916503  [   32/   43]
train() client id: f_00004-0-0 loss: 0.773151  [   32/  306]
train() client id: f_00004-0-1 loss: 0.713027  [   64/  306]
train() client id: f_00004-0-2 loss: 0.919562  [   96/  306]
train() client id: f_00004-0-3 loss: 0.944799  [  128/  306]
train() client id: f_00004-0-4 loss: 0.751245  [  160/  306]
train() client id: f_00004-0-5 loss: 0.727820  [  192/  306]
train() client id: f_00004-0-6 loss: 0.799145  [  224/  306]
train() client id: f_00004-0-7 loss: 0.966974  [  256/  306]
train() client id: f_00004-0-8 loss: 0.985367  [  288/  306]
train() client id: f_00004-1-0 loss: 0.788052  [   32/  306]
train() client id: f_00004-1-1 loss: 0.831843  [   64/  306]
train() client id: f_00004-1-2 loss: 0.866213  [   96/  306]
train() client id: f_00004-1-3 loss: 1.047743  [  128/  306]
train() client id: f_00004-1-4 loss: 0.864812  [  160/  306]
train() client id: f_00004-1-5 loss: 0.828604  [  192/  306]
train() client id: f_00004-1-6 loss: 0.769927  [  224/  306]
train() client id: f_00004-1-7 loss: 0.820805  [  256/  306]
train() client id: f_00004-1-8 loss: 0.677312  [  288/  306]
train() client id: f_00004-2-0 loss: 0.822152  [   32/  306]
train() client id: f_00004-2-1 loss: 0.827778  [   64/  306]
train() client id: f_00004-2-2 loss: 0.929779  [   96/  306]
train() client id: f_00004-2-3 loss: 0.921739  [  128/  306]
train() client id: f_00004-2-4 loss: 0.829556  [  160/  306]
train() client id: f_00004-2-5 loss: 0.895972  [  192/  306]
train() client id: f_00004-2-6 loss: 0.675148  [  224/  306]
train() client id: f_00004-2-7 loss: 0.836316  [  256/  306]
train() client id: f_00004-2-8 loss: 0.782207  [  288/  306]
train() client id: f_00004-3-0 loss: 0.885079  [   32/  306]
train() client id: f_00004-3-1 loss: 0.868364  [   64/  306]
train() client id: f_00004-3-2 loss: 0.939223  [   96/  306]
train() client id: f_00004-3-3 loss: 0.702141  [  128/  306]
train() client id: f_00004-3-4 loss: 0.730720  [  160/  306]
train() client id: f_00004-3-5 loss: 0.907976  [  192/  306]
train() client id: f_00004-3-6 loss: 0.864199  [  224/  306]
train() client id: f_00004-3-7 loss: 0.859991  [  256/  306]
train() client id: f_00004-3-8 loss: 0.729490  [  288/  306]
train() client id: f_00004-4-0 loss: 0.765016  [   32/  306]
train() client id: f_00004-4-1 loss: 0.887220  [   64/  306]
train() client id: f_00004-4-2 loss: 0.803800  [   96/  306]
train() client id: f_00004-4-3 loss: 0.780126  [  128/  306]
train() client id: f_00004-4-4 loss: 0.859935  [  160/  306]
train() client id: f_00004-4-5 loss: 0.904474  [  192/  306]
train() client id: f_00004-4-6 loss: 0.920789  [  224/  306]
train() client id: f_00004-4-7 loss: 0.733669  [  256/  306]
train() client id: f_00004-4-8 loss: 0.896626  [  288/  306]
train() client id: f_00004-5-0 loss: 0.695073  [   32/  306]
train() client id: f_00004-5-1 loss: 0.690724  [   64/  306]
train() client id: f_00004-5-2 loss: 0.928117  [   96/  306]
train() client id: f_00004-5-3 loss: 0.939283  [  128/  306]
train() client id: f_00004-5-4 loss: 0.937351  [  160/  306]
train() client id: f_00004-5-5 loss: 0.793975  [  192/  306]
train() client id: f_00004-5-6 loss: 0.781427  [  224/  306]
train() client id: f_00004-5-7 loss: 0.805977  [  256/  306]
train() client id: f_00004-5-8 loss: 0.917643  [  288/  306]
train() client id: f_00004-6-0 loss: 0.871530  [   32/  306]
train() client id: f_00004-6-1 loss: 0.882130  [   64/  306]
train() client id: f_00004-6-2 loss: 0.823232  [   96/  306]
train() client id: f_00004-6-3 loss: 0.961088  [  128/  306]
train() client id: f_00004-6-4 loss: 0.784045  [  160/  306]
train() client id: f_00004-6-5 loss: 0.916670  [  192/  306]
train() client id: f_00004-6-6 loss: 0.741410  [  224/  306]
train() client id: f_00004-6-7 loss: 0.810961  [  256/  306]
train() client id: f_00004-6-8 loss: 0.811055  [  288/  306]
train() client id: f_00004-7-0 loss: 0.770762  [   32/  306]
train() client id: f_00004-7-1 loss: 0.700222  [   64/  306]
train() client id: f_00004-7-2 loss: 0.946843  [   96/  306]
train() client id: f_00004-7-3 loss: 0.853895  [  128/  306]
train() client id: f_00004-7-4 loss: 0.823196  [  160/  306]
train() client id: f_00004-7-5 loss: 0.989757  [  192/  306]
train() client id: f_00004-7-6 loss: 0.797786  [  224/  306]
train() client id: f_00004-7-7 loss: 0.864555  [  256/  306]
train() client id: f_00004-7-8 loss: 0.780893  [  288/  306]
train() client id: f_00004-8-0 loss: 0.862863  [   32/  306]
train() client id: f_00004-8-1 loss: 0.768014  [   64/  306]
train() client id: f_00004-8-2 loss: 0.877807  [   96/  306]
train() client id: f_00004-8-3 loss: 0.832470  [  128/  306]
train() client id: f_00004-8-4 loss: 0.840206  [  160/  306]
train() client id: f_00004-8-5 loss: 0.759032  [  192/  306]
train() client id: f_00004-8-6 loss: 0.784005  [  224/  306]
train() client id: f_00004-8-7 loss: 0.841603  [  256/  306]
train() client id: f_00004-8-8 loss: 0.939537  [  288/  306]
train() client id: f_00004-9-0 loss: 0.711034  [   32/  306]
train() client id: f_00004-9-1 loss: 0.974074  [   64/  306]
train() client id: f_00004-9-2 loss: 0.717297  [   96/  306]
train() client id: f_00004-9-3 loss: 0.820238  [  128/  306]
train() client id: f_00004-9-4 loss: 0.837913  [  160/  306]
train() client id: f_00004-9-5 loss: 0.920288  [  192/  306]
train() client id: f_00004-9-6 loss: 0.764069  [  224/  306]
train() client id: f_00004-9-7 loss: 0.882319  [  256/  306]
train() client id: f_00004-9-8 loss: 0.834510  [  288/  306]
train() client id: f_00004-10-0 loss: 0.825003  [   32/  306]
train() client id: f_00004-10-1 loss: 0.892767  [   64/  306]
train() client id: f_00004-10-2 loss: 0.871105  [   96/  306]
train() client id: f_00004-10-3 loss: 0.770035  [  128/  306]
train() client id: f_00004-10-4 loss: 0.904223  [  160/  306]
train() client id: f_00004-10-5 loss: 0.753524  [  192/  306]
train() client id: f_00004-10-6 loss: 0.836872  [  224/  306]
train() client id: f_00004-10-7 loss: 0.946680  [  256/  306]
train() client id: f_00004-10-8 loss: 0.682691  [  288/  306]
train() client id: f_00005-0-0 loss: 0.922185  [   32/  146]
train() client id: f_00005-0-1 loss: 0.853959  [   64/  146]
train() client id: f_00005-0-2 loss: 0.686621  [   96/  146]
train() client id: f_00005-0-3 loss: 0.560477  [  128/  146]
train() client id: f_00005-1-0 loss: 0.843878  [   32/  146]
train() client id: f_00005-1-1 loss: 0.811219  [   64/  146]
train() client id: f_00005-1-2 loss: 0.807009  [   96/  146]
train() client id: f_00005-1-3 loss: 0.659626  [  128/  146]
train() client id: f_00005-2-0 loss: 0.718996  [   32/  146]
train() client id: f_00005-2-1 loss: 0.743642  [   64/  146]
train() client id: f_00005-2-2 loss: 0.619528  [   96/  146]
train() client id: f_00005-2-3 loss: 0.993289  [  128/  146]
train() client id: f_00005-3-0 loss: 0.956004  [   32/  146]
train() client id: f_00005-3-1 loss: 0.668008  [   64/  146]
train() client id: f_00005-3-2 loss: 0.656693  [   96/  146]
train() client id: f_00005-3-3 loss: 0.900498  [  128/  146]
train() client id: f_00005-4-0 loss: 0.732102  [   32/  146]
train() client id: f_00005-4-1 loss: 0.922118  [   64/  146]
train() client id: f_00005-4-2 loss: 0.693540  [   96/  146]
train() client id: f_00005-4-3 loss: 0.701455  [  128/  146]
train() client id: f_00005-5-0 loss: 0.927978  [   32/  146]
train() client id: f_00005-5-1 loss: 0.643988  [   64/  146]
train() client id: f_00005-5-2 loss: 0.849678  [   96/  146]
train() client id: f_00005-5-3 loss: 0.495325  [  128/  146]
train() client id: f_00005-6-0 loss: 0.801826  [   32/  146]
train() client id: f_00005-6-1 loss: 0.745986  [   64/  146]
train() client id: f_00005-6-2 loss: 0.514811  [   96/  146]
train() client id: f_00005-6-3 loss: 0.976576  [  128/  146]
train() client id: f_00005-7-0 loss: 0.787776  [   32/  146]
train() client id: f_00005-7-1 loss: 0.646439  [   64/  146]
train() client id: f_00005-7-2 loss: 0.616150  [   96/  146]
train() client id: f_00005-7-3 loss: 1.096827  [  128/  146]
train() client id: f_00005-8-0 loss: 0.705005  [   32/  146]
train() client id: f_00005-8-1 loss: 0.907750  [   64/  146]
train() client id: f_00005-8-2 loss: 0.636973  [   96/  146]
train() client id: f_00005-8-3 loss: 0.869770  [  128/  146]
train() client id: f_00005-9-0 loss: 0.750293  [   32/  146]
train() client id: f_00005-9-1 loss: 0.596070  [   64/  146]
train() client id: f_00005-9-2 loss: 0.980161  [   96/  146]
train() client id: f_00005-9-3 loss: 0.826320  [  128/  146]
train() client id: f_00005-10-0 loss: 0.642380  [   32/  146]
train() client id: f_00005-10-1 loss: 0.636270  [   64/  146]
train() client id: f_00005-10-2 loss: 0.921985  [   96/  146]
train() client id: f_00005-10-3 loss: 0.729825  [  128/  146]
train() client id: f_00006-0-0 loss: 0.606045  [   32/   54]
train() client id: f_00006-1-0 loss: 0.518055  [   32/   54]
train() client id: f_00006-2-0 loss: 0.514649  [   32/   54]
train() client id: f_00006-3-0 loss: 0.577443  [   32/   54]
train() client id: f_00006-4-0 loss: 0.495706  [   32/   54]
train() client id: f_00006-5-0 loss: 0.564160  [   32/   54]
train() client id: f_00006-6-0 loss: 0.565662  [   32/   54]
train() client id: f_00006-7-0 loss: 0.541134  [   32/   54]
train() client id: f_00006-8-0 loss: 0.464188  [   32/   54]
train() client id: f_00006-9-0 loss: 0.517524  [   32/   54]
train() client id: f_00006-10-0 loss: 0.586219  [   32/   54]
train() client id: f_00007-0-0 loss: 0.523833  [   32/  179]
train() client id: f_00007-0-1 loss: 0.658622  [   64/  179]
train() client id: f_00007-0-2 loss: 0.671160  [   96/  179]
train() client id: f_00007-0-3 loss: 0.579651  [  128/  179]
train() client id: f_00007-0-4 loss: 0.684480  [  160/  179]
train() client id: f_00007-1-0 loss: 0.508087  [   32/  179]
train() client id: f_00007-1-1 loss: 0.873080  [   64/  179]
train() client id: f_00007-1-2 loss: 0.592969  [   96/  179]
train() client id: f_00007-1-3 loss: 0.525376  [  128/  179]
train() client id: f_00007-1-4 loss: 0.450381  [  160/  179]
train() client id: f_00007-2-0 loss: 0.569701  [   32/  179]
train() client id: f_00007-2-1 loss: 0.736180  [   64/  179]
train() client id: f_00007-2-2 loss: 0.561459  [   96/  179]
train() client id: f_00007-2-3 loss: 0.694023  [  128/  179]
train() client id: f_00007-2-4 loss: 0.471372  [  160/  179]
train() client id: f_00007-3-0 loss: 0.447919  [   32/  179]
train() client id: f_00007-3-1 loss: 0.524553  [   64/  179]
train() client id: f_00007-3-2 loss: 0.455980  [   96/  179]
train() client id: f_00007-3-3 loss: 0.670543  [  128/  179]
train() client id: f_00007-3-4 loss: 0.829151  [  160/  179]
train() client id: f_00007-4-0 loss: 0.709881  [   32/  179]
train() client id: f_00007-4-1 loss: 0.533724  [   64/  179]
train() client id: f_00007-4-2 loss: 0.533425  [   96/  179]
train() client id: f_00007-4-3 loss: 0.489948  [  128/  179]
train() client id: f_00007-4-4 loss: 0.712987  [  160/  179]
train() client id: f_00007-5-0 loss: 0.693151  [   32/  179]
train() client id: f_00007-5-1 loss: 0.562361  [   64/  179]
train() client id: f_00007-5-2 loss: 0.507369  [   96/  179]
train() client id: f_00007-5-3 loss: 0.598605  [  128/  179]
train() client id: f_00007-5-4 loss: 0.515080  [  160/  179]
train() client id: f_00007-6-0 loss: 0.497849  [   32/  179]
train() client id: f_00007-6-1 loss: 0.380740  [   64/  179]
train() client id: f_00007-6-2 loss: 0.755799  [   96/  179]
train() client id: f_00007-6-3 loss: 0.639534  [  128/  179]
train() client id: f_00007-6-4 loss: 0.713800  [  160/  179]
train() client id: f_00007-7-0 loss: 0.602804  [   32/  179]
train() client id: f_00007-7-1 loss: 0.363163  [   64/  179]
train() client id: f_00007-7-2 loss: 0.489209  [   96/  179]
train() client id: f_00007-7-3 loss: 0.497397  [  128/  179]
train() client id: f_00007-7-4 loss: 0.760231  [  160/  179]
train() client id: f_00007-8-0 loss: 0.689063  [   32/  179]
train() client id: f_00007-8-1 loss: 0.551486  [   64/  179]
train() client id: f_00007-8-2 loss: 0.482833  [   96/  179]
train() client id: f_00007-8-3 loss: 0.629323  [  128/  179]
train() client id: f_00007-8-4 loss: 0.594724  [  160/  179]
train() client id: f_00007-9-0 loss: 0.425452  [   32/  179]
train() client id: f_00007-9-1 loss: 0.520239  [   64/  179]
train() client id: f_00007-9-2 loss: 0.671788  [   96/  179]
train() client id: f_00007-9-3 loss: 0.588430  [  128/  179]
train() client id: f_00007-9-4 loss: 0.645275  [  160/  179]
train() client id: f_00007-10-0 loss: 0.474228  [   32/  179]
train() client id: f_00007-10-1 loss: 0.713152  [   64/  179]
train() client id: f_00007-10-2 loss: 0.641619  [   96/  179]
train() client id: f_00007-10-3 loss: 0.502364  [  128/  179]
train() client id: f_00007-10-4 loss: 0.629247  [  160/  179]
train() client id: f_00008-0-0 loss: 0.684960  [   32/  130]
train() client id: f_00008-0-1 loss: 0.836501  [   64/  130]
train() client id: f_00008-0-2 loss: 0.673987  [   96/  130]
train() client id: f_00008-0-3 loss: 0.784785  [  128/  130]
train() client id: f_00008-1-0 loss: 0.624627  [   32/  130]
train() client id: f_00008-1-1 loss: 0.726179  [   64/  130]
train() client id: f_00008-1-2 loss: 0.792291  [   96/  130]
train() client id: f_00008-1-3 loss: 0.813522  [  128/  130]
train() client id: f_00008-2-0 loss: 0.664603  [   32/  130]
train() client id: f_00008-2-1 loss: 0.862852  [   64/  130]
train() client id: f_00008-2-2 loss: 0.721906  [   96/  130]
train() client id: f_00008-2-3 loss: 0.743346  [  128/  130]
train() client id: f_00008-3-0 loss: 0.798559  [   32/  130]
train() client id: f_00008-3-1 loss: 0.735506  [   64/  130]
train() client id: f_00008-3-2 loss: 0.754177  [   96/  130]
train() client id: f_00008-3-3 loss: 0.708691  [  128/  130]
train() client id: f_00008-4-0 loss: 0.695964  [   32/  130]
train() client id: f_00008-4-1 loss: 0.757507  [   64/  130]
train() client id: f_00008-4-2 loss: 0.871249  [   96/  130]
train() client id: f_00008-4-3 loss: 0.678984  [  128/  130]
train() client id: f_00008-5-0 loss: 0.706832  [   32/  130]
train() client id: f_00008-5-1 loss: 0.780486  [   64/  130]
train() client id: f_00008-5-2 loss: 0.767425  [   96/  130]
train() client id: f_00008-5-3 loss: 0.708331  [  128/  130]
train() client id: f_00008-6-0 loss: 0.646305  [   32/  130]
train() client id: f_00008-6-1 loss: 0.774592  [   64/  130]
train() client id: f_00008-6-2 loss: 0.724854  [   96/  130]
train() client id: f_00008-6-3 loss: 0.849439  [  128/  130]
train() client id: f_00008-7-0 loss: 0.636359  [   32/  130]
train() client id: f_00008-7-1 loss: 0.754291  [   64/  130]
train() client id: f_00008-7-2 loss: 0.807772  [   96/  130]
train() client id: f_00008-7-3 loss: 0.790762  [  128/  130]
train() client id: f_00008-8-0 loss: 0.796137  [   32/  130]
train() client id: f_00008-8-1 loss: 0.684218  [   64/  130]
train() client id: f_00008-8-2 loss: 0.773301  [   96/  130]
train() client id: f_00008-8-3 loss: 0.711916  [  128/  130]
train() client id: f_00008-9-0 loss: 0.686738  [   32/  130]
train() client id: f_00008-9-1 loss: 0.781390  [   64/  130]
train() client id: f_00008-9-2 loss: 0.762291  [   96/  130]
train() client id: f_00008-9-3 loss: 0.767344  [  128/  130]
train() client id: f_00008-10-0 loss: 0.639431  [   32/  130]
train() client id: f_00008-10-1 loss: 0.713118  [   64/  130]
train() client id: f_00008-10-2 loss: 0.905903  [   96/  130]
train() client id: f_00008-10-3 loss: 0.724903  [  128/  130]
train() client id: f_00009-0-0 loss: 0.940016  [   32/  118]
train() client id: f_00009-0-1 loss: 0.832036  [   64/  118]
train() client id: f_00009-0-2 loss: 0.968266  [   96/  118]
train() client id: f_00009-1-0 loss: 0.897348  [   32/  118]
train() client id: f_00009-1-1 loss: 0.815899  [   64/  118]
train() client id: f_00009-1-2 loss: 0.723445  [   96/  118]
train() client id: f_00009-2-0 loss: 0.798727  [   32/  118]
train() client id: f_00009-2-1 loss: 0.829851  [   64/  118]
train() client id: f_00009-2-2 loss: 0.742058  [   96/  118]
train() client id: f_00009-3-0 loss: 0.666492  [   32/  118]
train() client id: f_00009-3-1 loss: 0.791971  [   64/  118]
train() client id: f_00009-3-2 loss: 0.810019  [   96/  118]
train() client id: f_00009-4-0 loss: 0.762109  [   32/  118]
train() client id: f_00009-4-1 loss: 0.857853  [   64/  118]
train() client id: f_00009-4-2 loss: 0.710187  [   96/  118]
train() client id: f_00009-5-0 loss: 0.799675  [   32/  118]
train() client id: f_00009-5-1 loss: 0.586509  [   64/  118]
train() client id: f_00009-5-2 loss: 0.810072  [   96/  118]
train() client id: f_00009-6-0 loss: 0.730522  [   32/  118]
train() client id: f_00009-6-1 loss: 0.641328  [   64/  118]
train() client id: f_00009-6-2 loss: 0.713122  [   96/  118]
train() client id: f_00009-7-0 loss: 0.646487  [   32/  118]
train() client id: f_00009-7-1 loss: 0.743720  [   64/  118]
train() client id: f_00009-7-2 loss: 0.667616  [   96/  118]
train() client id: f_00009-8-0 loss: 0.616731  [   32/  118]
train() client id: f_00009-8-1 loss: 0.725475  [   64/  118]
train() client id: f_00009-8-2 loss: 0.738433  [   96/  118]
train() client id: f_00009-9-0 loss: 0.729102  [   32/  118]
train() client id: f_00009-9-1 loss: 0.611780  [   64/  118]
train() client id: f_00009-9-2 loss: 0.820289  [   96/  118]
train() client id: f_00009-10-0 loss: 0.820295  [   32/  118]
train() client id: f_00009-10-1 loss: 0.494911  [   64/  118]
train() client id: f_00009-10-2 loss: 0.634896  [   96/  118]
At round 56 accuracy: 0.6472148541114059
At round 56 training accuracy: 0.5888665325285044
At round 56 training loss: 0.8324695919676799
update_location
xs = -3.905658 4.200318 300.009024 18.811294 0.979296 3.956410 -262.443192 -241.324852 284.663977 -227.060879 
ys = 292.587959 275.555839 1.320614 -262.455176 254.350187 237.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -12.211426168117436
ys mean: 81.89414253552872
dists_uav = 309.229637 293.170024 316.239084 281.489936 273.303817 258.013999 280.861745 261.224732 302.228804 248.138378 
uav_gains = -116.476858 -115.149786 -117.010810 -114.114667 -113.371282 -111.991813 -114.057933 -112.277390 -115.914951 -111.138542 
uav_gains_db_mean: -114.1504032687378
dists_bs = 207.605881 205.485354 505.764868 478.466142 193.041873 189.660260 197.982188 186.378219 485.876002 178.747946 
bs_gains = -104.449769 -104.324924 -115.277622 -114.602893 -103.565302 -103.350397 -103.872591 -103.138124 -114.789772 -102.629809 
bs_gains_db_mean: -107.00012007649647
Round 57
-------------------------------
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.48417658 7.05628152 3.44074209 1.26260335 8.13515315 3.91462212
 1.55207962 4.8313157  3.56521466 3.17400612]
obj_prev = 40.416194911724524
eta_min = 2.86092118758865e-27	eta_max = 0.9374658932840896
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 9.299045348130402	eta = 0.909090909090909
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 20.758435476455084	eta = 0.4072405937720066
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 14.636258285641127	eta = 0.5775846137877274
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.545044928479468	eta = 0.6241158766062833
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.479247501397975	eta = 0.6271624278976032
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.478979113930322	eta = 0.6271749156783474
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.478979109435532	eta = 0.6271749158874893
eta = 0.6271749158874893
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [0.03849692 0.08096575 0.03788586 0.01313784 0.09349252 0.04460754
 0.01649868 0.05469007 0.03971905 0.03605267]
ene_total = [1.32155705 2.05839072 1.33209932 0.6463361  2.34272368 1.20578025
 0.72268136 1.56297838 1.28498919 1.00144307]
ti_comp = [0.9850295  1.0924086  0.97437897 1.022215   1.09523156 1.09599371
 1.02294619 1.0429956  1.01299828 1.09844051]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [3.67501858e-06 2.77980148e-05 3.57977419e-06 1.35633586e-07
 4.25793239e-05 4.61837109e-06 2.68239096e-07 9.39810980e-06
 3.81644508e-06 2.42738867e-06]
ene_total = [0.42769316 0.17890492 0.45242289 0.34126134 0.17269288 0.17004157
 0.3395665  0.29322113 0.36274925 0.16430889]
optimize_network iter = 0 obj = 2.90286252803159
eta = 0.6271749158874893
freqs = [19540999.15308214 37058362.31216387 19441028.73626838  6426161.89579366
 42681624.07611285 20350273.65967661  8064295.3890079  26217785.78934446
 19604695.49055661 16410844.32139915]
eta_min = 0.6271749158874926	eta_max = 0.7063960382409014
af = 0.0018241050152146168	bf = 1.0814089668242106	zeta = 0.002006515516736079	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [7.50791650e-07 5.67902365e-06 7.31333600e-07 2.77094010e-08
 8.69878619e-06 9.43514808e-07 5.48001783e-08 1.91999638e-06
 7.79684519e-07 4.95905832e-07]
ene_total = [1.75372365 0.73159623 1.85514936 1.39952837 0.7050001  0.6970034
 1.39256766 1.20180996 1.48737301 0.67365931]
ti_comp = [0.73659322 0.84397231 0.72594269 0.77377872 0.84679527 0.84755742
 0.77450991 0.79455932 0.764562   0.85000422]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.71785248e-06 1.92597612e-05 2.66704329e-06 9.78907100e-08
 2.94562262e-05 3.19366957e-06 1.93507141e-07 6.69693547e-06
 2.77060309e-06 1.67638409e-06]
ene_total = [0.54306642 0.22692583 0.57447011 0.4333404  0.21890245 0.21588069
 0.43118717 0.37225931 0.46059653 0.20862108]
optimize_network iter = 1 obj = 3.685249982672195
eta = 0.7063960382409014
freqs = [19468958.52365561 35736956.5886699  19441028.73626838  6324869.42089178
 41128502.02279741 19605756.94700696  7935360.67409761 25640506.16051596
 19352208.45479425 15800135.59527822]
eta_min = 0.7063960382409034	eta_max = 0.7063960382409006
af = 0.0017119208968572406	bf = 1.0814089668242106	zeta = 0.0018831129865429649	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [7.45266057e-07 5.28124555e-06 7.31333600e-07 2.68427459e-08
 8.07723219e-06 8.75740514e-07 5.30618587e-08 1.83637587e-06
 7.59730875e-07 4.59683582e-07]
ene_total = [1.75372312 0.73155835 1.85514936 1.39952829 0.7049409  0.69699695
 1.39256749 1.20180199 1.48737111 0.67365586]
ti_comp = [0.73659322 0.84397231 0.72594269 0.77377872 0.84679527 0.84755742
 0.77450991 0.79455932 0.764562   0.85000422]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.71785248e-06 1.92597612e-05 2.66704329e-06 9.78907100e-08
 2.94562262e-05 3.19366957e-06 1.93507141e-07 6.69693547e-06
 2.77060309e-06 1.67638409e-06]
ene_total = [0.54306642 0.22692583 0.57447011 0.4333404  0.21890245 0.21588069
 0.43118717 0.37225931 0.46059653 0.20862108]
optimize_network iter = 2 obj = 3.6852499826721847
eta = 0.7063960382409006
freqs = [19468958.52365561 35736956.5886699  19441028.73626836  6324869.42089178
 41128502.02279742 19605756.94700696  7935360.67409761 25640506.16051596
 19352208.45479425 15800135.59527823]
Done!
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.62674960e-06 1.86141707e-05 2.57764354e-06 9.46093966e-08
 2.84688484e-05 3.08661721e-06 1.87020749e-07 6.47245305e-06
 2.67773200e-06 1.62019141e-06]
ene_total = [0.01841708 0.00769516 0.01948208 0.014696   0.00742271 0.00732112
 0.01462297 0.01262431 0.01562025 0.00707497]
At round 57 energy consumption: 0.12497665052505343
At round 57 eta: 0.7063960382409006
At round 57 a_n: 8.65748957900329
At round 57 local rounds: 11.38151016928333
At round 57 global rounds: 29.486964437171718
gradient difference: 0.4392286241054535
train() client id: f_00000-0-0 loss: 1.101318  [   32/  126]
train() client id: f_00000-0-1 loss: 0.950849  [   64/  126]
train() client id: f_00000-0-2 loss: 1.091541  [   96/  126]
train() client id: f_00000-1-0 loss: 0.910572  [   32/  126]
train() client id: f_00000-1-1 loss: 1.033866  [   64/  126]
train() client id: f_00000-1-2 loss: 0.918600  [   96/  126]
train() client id: f_00000-2-0 loss: 0.953224  [   32/  126]
train() client id: f_00000-2-1 loss: 0.889586  [   64/  126]
train() client id: f_00000-2-2 loss: 0.893437  [   96/  126]
train() client id: f_00000-3-0 loss: 0.950725  [   32/  126]
train() client id: f_00000-3-1 loss: 0.713567  [   64/  126]
train() client id: f_00000-3-2 loss: 0.956002  [   96/  126]
train() client id: f_00000-4-0 loss: 0.913774  [   32/  126]
train() client id: f_00000-4-1 loss: 0.735215  [   64/  126]
train() client id: f_00000-4-2 loss: 0.822207  [   96/  126]
train() client id: f_00000-5-0 loss: 0.822225  [   32/  126]
train() client id: f_00000-5-1 loss: 0.801769  [   64/  126]
train() client id: f_00000-5-2 loss: 0.846155  [   96/  126]
train() client id: f_00000-6-0 loss: 0.830693  [   32/  126]
train() client id: f_00000-6-1 loss: 0.710831  [   64/  126]
train() client id: f_00000-6-2 loss: 0.813942  [   96/  126]
train() client id: f_00000-7-0 loss: 0.673503  [   32/  126]
train() client id: f_00000-7-1 loss: 0.721144  [   64/  126]
train() client id: f_00000-7-2 loss: 0.860649  [   96/  126]
train() client id: f_00000-8-0 loss: 0.872748  [   32/  126]
train() client id: f_00000-8-1 loss: 0.776834  [   64/  126]
train() client id: f_00000-8-2 loss: 0.725281  [   96/  126]
train() client id: f_00000-9-0 loss: 0.655944  [   32/  126]
train() client id: f_00000-9-1 loss: 0.865927  [   64/  126]
train() client id: f_00000-9-2 loss: 0.840447  [   96/  126]
train() client id: f_00000-10-0 loss: 0.875645  [   32/  126]
train() client id: f_00000-10-1 loss: 0.654944  [   64/  126]
train() client id: f_00000-10-2 loss: 0.784880  [   96/  126]
train() client id: f_00001-0-0 loss: 0.607850  [   32/  265]
train() client id: f_00001-0-1 loss: 0.378840  [   64/  265]
train() client id: f_00001-0-2 loss: 0.459188  [   96/  265]
train() client id: f_00001-0-3 loss: 0.421887  [  128/  265]
train() client id: f_00001-0-4 loss: 0.394039  [  160/  265]
train() client id: f_00001-0-5 loss: 0.434777  [  192/  265]
train() client id: f_00001-0-6 loss: 0.323455  [  224/  265]
train() client id: f_00001-0-7 loss: 0.310818  [  256/  265]
train() client id: f_00001-1-0 loss: 0.362119  [   32/  265]
train() client id: f_00001-1-1 loss: 0.455985  [   64/  265]
train() client id: f_00001-1-2 loss: 0.314466  [   96/  265]
train() client id: f_00001-1-3 loss: 0.538118  [  128/  265]
train() client id: f_00001-1-4 loss: 0.420909  [  160/  265]
train() client id: f_00001-1-5 loss: 0.472770  [  192/  265]
train() client id: f_00001-1-6 loss: 0.334372  [  224/  265]
train() client id: f_00001-1-7 loss: 0.401300  [  256/  265]
train() client id: f_00001-2-0 loss: 0.418229  [   32/  265]
train() client id: f_00001-2-1 loss: 0.545219  [   64/  265]
train() client id: f_00001-2-2 loss: 0.391069  [   96/  265]
train() client id: f_00001-2-3 loss: 0.397336  [  128/  265]
train() client id: f_00001-2-4 loss: 0.395536  [  160/  265]
train() client id: f_00001-2-5 loss: 0.365440  [  192/  265]
train() client id: f_00001-2-6 loss: 0.332811  [  224/  265]
train() client id: f_00001-2-7 loss: 0.325149  [  256/  265]
train() client id: f_00001-3-0 loss: 0.284794  [   32/  265]
train() client id: f_00001-3-1 loss: 0.456688  [   64/  265]
train() client id: f_00001-3-2 loss: 0.359580  [   96/  265]
train() client id: f_00001-3-3 loss: 0.433546  [  128/  265]
train() client id: f_00001-3-4 loss: 0.376115  [  160/  265]
train() client id: f_00001-3-5 loss: 0.532051  [  192/  265]
train() client id: f_00001-3-6 loss: 0.410126  [  224/  265]
train() client id: f_00001-3-7 loss: 0.375867  [  256/  265]
train() client id: f_00001-4-0 loss: 0.469498  [   32/  265]
train() client id: f_00001-4-1 loss: 0.334899  [   64/  265]
train() client id: f_00001-4-2 loss: 0.426276  [   96/  265]
train() client id: f_00001-4-3 loss: 0.449071  [  128/  265]
train() client id: f_00001-4-4 loss: 0.304129  [  160/  265]
train() client id: f_00001-4-5 loss: 0.426387  [  192/  265]
train() client id: f_00001-4-6 loss: 0.451995  [  224/  265]
train() client id: f_00001-4-7 loss: 0.308986  [  256/  265]
train() client id: f_00001-5-0 loss: 0.317914  [   32/  265]
train() client id: f_00001-5-1 loss: 0.476192  [   64/  265]
train() client id: f_00001-5-2 loss: 0.449662  [   96/  265]
train() client id: f_00001-5-3 loss: 0.304005  [  128/  265]
train() client id: f_00001-5-4 loss: 0.344827  [  160/  265]
train() client id: f_00001-5-5 loss: 0.490864  [  192/  265]
train() client id: f_00001-5-6 loss: 0.297384  [  224/  265]
train() client id: f_00001-5-7 loss: 0.519799  [  256/  265]
train() client id: f_00001-6-0 loss: 0.363416  [   32/  265]
train() client id: f_00001-6-1 loss: 0.495917  [   64/  265]
train() client id: f_00001-6-2 loss: 0.340080  [   96/  265]
train() client id: f_00001-6-3 loss: 0.466631  [  128/  265]
train() client id: f_00001-6-4 loss: 0.466977  [  160/  265]
train() client id: f_00001-6-5 loss: 0.405303  [  192/  265]
train() client id: f_00001-6-6 loss: 0.356005  [  224/  265]
train() client id: f_00001-6-7 loss: 0.292593  [  256/  265]
train() client id: f_00001-7-0 loss: 0.466046  [   32/  265]
train() client id: f_00001-7-1 loss: 0.416560  [   64/  265]
train() client id: f_00001-7-2 loss: 0.426122  [   96/  265]
train() client id: f_00001-7-3 loss: 0.491972  [  128/  265]
train() client id: f_00001-7-4 loss: 0.302652  [  160/  265]
train() client id: f_00001-7-5 loss: 0.298304  [  192/  265]
train() client id: f_00001-7-6 loss: 0.360899  [  224/  265]
train() client id: f_00001-7-7 loss: 0.397040  [  256/  265]
train() client id: f_00001-8-0 loss: 0.427792  [   32/  265]
train() client id: f_00001-8-1 loss: 0.349731  [   64/  265]
train() client id: f_00001-8-2 loss: 0.407114  [   96/  265]
train() client id: f_00001-8-3 loss: 0.296240  [  128/  265]
train() client id: f_00001-8-4 loss: 0.384578  [  160/  265]
train() client id: f_00001-8-5 loss: 0.442971  [  192/  265]
train() client id: f_00001-8-6 loss: 0.479088  [  224/  265]
train() client id: f_00001-8-7 loss: 0.375614  [  256/  265]
train() client id: f_00001-9-0 loss: 0.310565  [   32/  265]
train() client id: f_00001-9-1 loss: 0.300359  [   64/  265]
train() client id: f_00001-9-2 loss: 0.354309  [   96/  265]
train() client id: f_00001-9-3 loss: 0.380266  [  128/  265]
train() client id: f_00001-9-4 loss: 0.509122  [  160/  265]
train() client id: f_00001-9-5 loss: 0.530080  [  192/  265]
train() client id: f_00001-9-6 loss: 0.346028  [  224/  265]
train() client id: f_00001-9-7 loss: 0.356849  [  256/  265]
train() client id: f_00001-10-0 loss: 0.361357  [   32/  265]
train() client id: f_00001-10-1 loss: 0.426324  [   64/  265]
train() client id: f_00001-10-2 loss: 0.506914  [   96/  265]
train() client id: f_00001-10-3 loss: 0.401958  [  128/  265]
train() client id: f_00001-10-4 loss: 0.395428  [  160/  265]
train() client id: f_00001-10-5 loss: 0.307212  [  192/  265]
train() client id: f_00001-10-6 loss: 0.365016  [  224/  265]
train() client id: f_00001-10-7 loss: 0.369931  [  256/  265]
train() client id: f_00002-0-0 loss: 1.123732  [   32/  124]
train() client id: f_00002-0-1 loss: 1.387337  [   64/  124]
train() client id: f_00002-0-2 loss: 1.358145  [   96/  124]
train() client id: f_00002-1-0 loss: 1.070192  [   32/  124]
train() client id: f_00002-1-1 loss: 1.137866  [   64/  124]
train() client id: f_00002-1-2 loss: 1.334818  [   96/  124]
train() client id: f_00002-2-0 loss: 1.160869  [   32/  124]
train() client id: f_00002-2-1 loss: 1.261937  [   64/  124]
train() client id: f_00002-2-2 loss: 1.122486  [   96/  124]
train() client id: f_00002-3-0 loss: 1.127889  [   32/  124]
train() client id: f_00002-3-1 loss: 0.993131  [   64/  124]
train() client id: f_00002-3-2 loss: 1.242086  [   96/  124]
train() client id: f_00002-4-0 loss: 1.131975  [   32/  124]
train() client id: f_00002-4-1 loss: 1.105268  [   64/  124]
train() client id: f_00002-4-2 loss: 1.123698  [   96/  124]
train() client id: f_00002-5-0 loss: 1.104046  [   32/  124]
train() client id: f_00002-5-1 loss: 0.897916  [   64/  124]
train() client id: f_00002-5-2 loss: 1.346649  [   96/  124]
train() client id: f_00002-6-0 loss: 1.272690  [   32/  124]
train() client id: f_00002-6-1 loss: 0.871845  [   64/  124]
train() client id: f_00002-6-2 loss: 1.175975  [   96/  124]
train() client id: f_00002-7-0 loss: 1.034805  [   32/  124]
train() client id: f_00002-7-1 loss: 1.118096  [   64/  124]
train() client id: f_00002-7-2 loss: 1.022316  [   96/  124]
train() client id: f_00002-8-0 loss: 1.157032  [   32/  124]
train() client id: f_00002-8-1 loss: 1.008091  [   64/  124]
train() client id: f_00002-8-2 loss: 1.066469  [   96/  124]
train() client id: f_00002-9-0 loss: 0.824394  [   32/  124]
train() client id: f_00002-9-1 loss: 1.273798  [   64/  124]
train() client id: f_00002-9-2 loss: 0.897496  [   96/  124]
train() client id: f_00002-10-0 loss: 0.933720  [   32/  124]
train() client id: f_00002-10-1 loss: 1.198682  [   64/  124]
train() client id: f_00002-10-2 loss: 1.155190  [   96/  124]
train() client id: f_00003-0-0 loss: 0.371733  [   32/   43]
train() client id: f_00003-1-0 loss: 0.574153  [   32/   43]
train() client id: f_00003-2-0 loss: 0.603109  [   32/   43]
train() client id: f_00003-3-0 loss: 0.642264  [   32/   43]
train() client id: f_00003-4-0 loss: 0.730990  [   32/   43]
train() client id: f_00003-5-0 loss: 0.768401  [   32/   43]
train() client id: f_00003-6-0 loss: 0.412137  [   32/   43]
train() client id: f_00003-7-0 loss: 0.532492  [   32/   43]
train() client id: f_00003-8-0 loss: 0.575067  [   32/   43]
train() client id: f_00003-9-0 loss: 0.541060  [   32/   43]
train() client id: f_00003-10-0 loss: 0.623370  [   32/   43]
train() client id: f_00004-0-0 loss: 0.997373  [   32/  306]
train() client id: f_00004-0-1 loss: 0.921128  [   64/  306]
train() client id: f_00004-0-2 loss: 0.951902  [   96/  306]
train() client id: f_00004-0-3 loss: 1.045216  [  128/  306]
train() client id: f_00004-0-4 loss: 0.925876  [  160/  306]
train() client id: f_00004-0-5 loss: 0.922900  [  192/  306]
train() client id: f_00004-0-6 loss: 0.949737  [  224/  306]
train() client id: f_00004-0-7 loss: 0.977170  [  256/  306]
train() client id: f_00004-0-8 loss: 1.073895  [  288/  306]
train() client id: f_00004-1-0 loss: 0.943727  [   32/  306]
train() client id: f_00004-1-1 loss: 0.813709  [   64/  306]
train() client id: f_00004-1-2 loss: 1.099247  [   96/  306]
train() client id: f_00004-1-3 loss: 1.084823  [  128/  306]
train() client id: f_00004-1-4 loss: 0.806456  [  160/  306]
train() client id: f_00004-1-5 loss: 1.082559  [  192/  306]
train() client id: f_00004-1-6 loss: 0.901822  [  224/  306]
train() client id: f_00004-1-7 loss: 1.025123  [  256/  306]
train() client id: f_00004-1-8 loss: 1.047477  [  288/  306]
train() client id: f_00004-2-0 loss: 0.927295  [   32/  306]
train() client id: f_00004-2-1 loss: 0.947947  [   64/  306]
train() client id: f_00004-2-2 loss: 0.929207  [   96/  306]
train() client id: f_00004-2-3 loss: 0.900708  [  128/  306]
train() client id: f_00004-2-4 loss: 1.015192  [  160/  306]
train() client id: f_00004-2-5 loss: 1.014215  [  192/  306]
train() client id: f_00004-2-6 loss: 0.982342  [  224/  306]
train() client id: f_00004-2-7 loss: 1.037393  [  256/  306]
train() client id: f_00004-2-8 loss: 0.961277  [  288/  306]
train() client id: f_00004-3-0 loss: 0.917636  [   32/  306]
train() client id: f_00004-3-1 loss: 0.940175  [   64/  306]
train() client id: f_00004-3-2 loss: 1.109696  [   96/  306]
train() client id: f_00004-3-3 loss: 0.939765  [  128/  306]
train() client id: f_00004-3-4 loss: 0.890347  [  160/  306]
train() client id: f_00004-3-5 loss: 0.887479  [  192/  306]
train() client id: f_00004-3-6 loss: 0.986475  [  224/  306]
train() client id: f_00004-3-7 loss: 1.006859  [  256/  306]
train() client id: f_00004-3-8 loss: 0.914965  [  288/  306]
train() client id: f_00004-4-0 loss: 0.981277  [   32/  306]
train() client id: f_00004-4-1 loss: 0.979952  [   64/  306]
train() client id: f_00004-4-2 loss: 0.936097  [   96/  306]
train() client id: f_00004-4-3 loss: 1.117730  [  128/  306]
train() client id: f_00004-4-4 loss: 0.952857  [  160/  306]
train() client id: f_00004-4-5 loss: 0.999674  [  192/  306]
train() client id: f_00004-4-6 loss: 0.898327  [  224/  306]
train() client id: f_00004-4-7 loss: 0.830777  [  256/  306]
train() client id: f_00004-4-8 loss: 0.860687  [  288/  306]
train() client id: f_00004-5-0 loss: 0.963309  [   32/  306]
train() client id: f_00004-5-1 loss: 0.881446  [   64/  306]
train() client id: f_00004-5-2 loss: 1.030241  [   96/  306]
train() client id: f_00004-5-3 loss: 0.977263  [  128/  306]
train() client id: f_00004-5-4 loss: 0.945825  [  160/  306]
train() client id: f_00004-5-5 loss: 1.102874  [  192/  306]
train() client id: f_00004-5-6 loss: 0.796064  [  224/  306]
train() client id: f_00004-5-7 loss: 0.945516  [  256/  306]
train() client id: f_00004-5-8 loss: 1.042748  [  288/  306]
train() client id: f_00004-6-0 loss: 0.890953  [   32/  306]
train() client id: f_00004-6-1 loss: 0.964337  [   64/  306]
train() client id: f_00004-6-2 loss: 0.803060  [   96/  306]
train() client id: f_00004-6-3 loss: 0.968499  [  128/  306]
train() client id: f_00004-6-4 loss: 0.889188  [  160/  306]
train() client id: f_00004-6-5 loss: 1.100891  [  192/  306]
train() client id: f_00004-6-6 loss: 0.937436  [  224/  306]
train() client id: f_00004-6-7 loss: 1.084636  [  256/  306]
train() client id: f_00004-6-8 loss: 0.914311  [  288/  306]
train() client id: f_00004-7-0 loss: 0.966014  [   32/  306]
train() client id: f_00004-7-1 loss: 1.049085  [   64/  306]
train() client id: f_00004-7-2 loss: 0.843930  [   96/  306]
train() client id: f_00004-7-3 loss: 0.861591  [  128/  306]
train() client id: f_00004-7-4 loss: 0.922877  [  160/  306]
train() client id: f_00004-7-5 loss: 1.021901  [  192/  306]
train() client id: f_00004-7-6 loss: 1.030457  [  224/  306]
train() client id: f_00004-7-7 loss: 0.905870  [  256/  306]
train() client id: f_00004-7-8 loss: 0.924218  [  288/  306]
train() client id: f_00004-8-0 loss: 0.884090  [   32/  306]
train() client id: f_00004-8-1 loss: 0.899218  [   64/  306]
train() client id: f_00004-8-2 loss: 1.091619  [   96/  306]
train() client id: f_00004-8-3 loss: 0.903656  [  128/  306]
train() client id: f_00004-8-4 loss: 0.884651  [  160/  306]
train() client id: f_00004-8-5 loss: 0.880620  [  192/  306]
train() client id: f_00004-8-6 loss: 0.963265  [  224/  306]
train() client id: f_00004-8-7 loss: 0.975813  [  256/  306]
train() client id: f_00004-8-8 loss: 0.977026  [  288/  306]
train() client id: f_00004-9-0 loss: 0.975675  [   32/  306]
train() client id: f_00004-9-1 loss: 1.074642  [   64/  306]
train() client id: f_00004-9-2 loss: 0.928858  [   96/  306]
train() client id: f_00004-9-3 loss: 0.848522  [  128/  306]
train() client id: f_00004-9-4 loss: 0.978215  [  160/  306]
train() client id: f_00004-9-5 loss: 0.847670  [  192/  306]
train() client id: f_00004-9-6 loss: 0.998888  [  224/  306]
train() client id: f_00004-9-7 loss: 0.909150  [  256/  306]
train() client id: f_00004-9-8 loss: 0.866161  [  288/  306]
train() client id: f_00004-10-0 loss: 0.802000  [   32/  306]
train() client id: f_00004-10-1 loss: 0.923326  [   64/  306]
train() client id: f_00004-10-2 loss: 0.984563  [   96/  306]
train() client id: f_00004-10-3 loss: 0.884397  [  128/  306]
train() client id: f_00004-10-4 loss: 0.971745  [  160/  306]
train() client id: f_00004-10-5 loss: 0.931647  [  192/  306]
train() client id: f_00004-10-6 loss: 1.013269  [  224/  306]
train() client id: f_00004-10-7 loss: 0.895441  [  256/  306]
train() client id: f_00004-10-8 loss: 0.964864  [  288/  306]
train() client id: f_00005-0-0 loss: 0.785685  [   32/  146]
train() client id: f_00005-0-1 loss: 0.388518  [   64/  146]
train() client id: f_00005-0-2 loss: 0.552025  [   96/  146]
train() client id: f_00005-0-3 loss: 0.755196  [  128/  146]
train() client id: f_00005-1-0 loss: 0.614549  [   32/  146]
train() client id: f_00005-1-1 loss: 0.444984  [   64/  146]
train() client id: f_00005-1-2 loss: 0.962367  [   96/  146]
train() client id: f_00005-1-3 loss: 0.486115  [  128/  146]
train() client id: f_00005-2-0 loss: 0.639178  [   32/  146]
train() client id: f_00005-2-1 loss: 0.350549  [   64/  146]
train() client id: f_00005-2-2 loss: 0.798804  [   96/  146]
train() client id: f_00005-2-3 loss: 0.885752  [  128/  146]
train() client id: f_00005-3-0 loss: 0.574722  [   32/  146]
train() client id: f_00005-3-1 loss: 0.770263  [   64/  146]
train() client id: f_00005-3-2 loss: 0.579933  [   96/  146]
train() client id: f_00005-3-3 loss: 0.643530  [  128/  146]
train() client id: f_00005-4-0 loss: 0.810329  [   32/  146]
train() client id: f_00005-4-1 loss: 0.351000  [   64/  146]
train() client id: f_00005-4-2 loss: 0.468156  [   96/  146]
train() client id: f_00005-4-3 loss: 0.714863  [  128/  146]
train() client id: f_00005-5-0 loss: 0.643329  [   32/  146]
train() client id: f_00005-5-1 loss: 0.373805  [   64/  146]
train() client id: f_00005-5-2 loss: 0.890258  [   96/  146]
train() client id: f_00005-5-3 loss: 0.609066  [  128/  146]
train() client id: f_00005-6-0 loss: 0.594341  [   32/  146]
train() client id: f_00005-6-1 loss: 0.605150  [   64/  146]
train() client id: f_00005-6-2 loss: 0.810080  [   96/  146]
train() client id: f_00005-6-3 loss: 0.635879  [  128/  146]
train() client id: f_00005-7-0 loss: 0.604925  [   32/  146]
train() client id: f_00005-7-1 loss: 0.981247  [   64/  146]
train() client id: f_00005-7-2 loss: 0.656634  [   96/  146]
train() client id: f_00005-7-3 loss: 0.352309  [  128/  146]
train() client id: f_00005-8-0 loss: 0.798208  [   32/  146]
train() client id: f_00005-8-1 loss: 0.400890  [   64/  146]
train() client id: f_00005-8-2 loss: 0.695709  [   96/  146]
train() client id: f_00005-8-3 loss: 0.494787  [  128/  146]
train() client id: f_00005-9-0 loss: 0.797302  [   32/  146]
train() client id: f_00005-9-1 loss: 0.429589  [   64/  146]
train() client id: f_00005-9-2 loss: 0.569078  [   96/  146]
train() client id: f_00005-9-3 loss: 0.540636  [  128/  146]
train() client id: f_00005-10-0 loss: 0.837896  [   32/  146]
train() client id: f_00005-10-1 loss: 0.287774  [   64/  146]
train() client id: f_00005-10-2 loss: 0.760354  [   96/  146]
train() client id: f_00005-10-3 loss: 0.626608  [  128/  146]
train() client id: f_00006-0-0 loss: 0.428024  [   32/   54]
train() client id: f_00006-1-0 loss: 0.431991  [   32/   54]
train() client id: f_00006-2-0 loss: 0.414357  [   32/   54]
train() client id: f_00006-3-0 loss: 0.438133  [   32/   54]
train() client id: f_00006-4-0 loss: 0.432077  [   32/   54]
train() client id: f_00006-5-0 loss: 0.480770  [   32/   54]
train() client id: f_00006-6-0 loss: 0.445768  [   32/   54]
train() client id: f_00006-7-0 loss: 0.485892  [   32/   54]
train() client id: f_00006-8-0 loss: 0.439851  [   32/   54]
train() client id: f_00006-9-0 loss: 0.479313  [   32/   54]
train() client id: f_00006-10-0 loss: 0.451422  [   32/   54]
train() client id: f_00007-0-0 loss: 0.539118  [   32/  179]
train() client id: f_00007-0-1 loss: 0.401546  [   64/  179]
train() client id: f_00007-0-2 loss: 0.471887  [   96/  179]
train() client id: f_00007-0-3 loss: 0.522816  [  128/  179]
train() client id: f_00007-0-4 loss: 0.819501  [  160/  179]
train() client id: f_00007-1-0 loss: 0.391451  [   32/  179]
train() client id: f_00007-1-1 loss: 0.608716  [   64/  179]
train() client id: f_00007-1-2 loss: 0.445075  [   96/  179]
train() client id: f_00007-1-3 loss: 0.687819  [  128/  179]
train() client id: f_00007-1-4 loss: 0.437184  [  160/  179]
train() client id: f_00007-2-0 loss: 0.358596  [   32/  179]
train() client id: f_00007-2-1 loss: 0.517856  [   64/  179]
train() client id: f_00007-2-2 loss: 0.442810  [   96/  179]
train() client id: f_00007-2-3 loss: 0.513450  [  128/  179]
train() client id: f_00007-2-4 loss: 0.627637  [  160/  179]
train() client id: f_00007-3-0 loss: 0.696558  [   32/  179]
train() client id: f_00007-3-1 loss: 0.359960  [   64/  179]
train() client id: f_00007-3-2 loss: 0.588548  [   96/  179]
train() client id: f_00007-3-3 loss: 0.338001  [  128/  179]
train() client id: f_00007-3-4 loss: 0.531078  [  160/  179]
train() client id: f_00007-4-0 loss: 0.483309  [   32/  179]
train() client id: f_00007-4-1 loss: 0.616865  [   64/  179]
train() client id: f_00007-4-2 loss: 0.315273  [   96/  179]
train() client id: f_00007-4-3 loss: 0.657016  [  128/  179]
train() client id: f_00007-4-4 loss: 0.302328  [  160/  179]
train() client id: f_00007-5-0 loss: 0.360646  [   32/  179]
train() client id: f_00007-5-1 loss: 0.470993  [   64/  179]
train() client id: f_00007-5-2 loss: 0.341557  [   96/  179]
train() client id: f_00007-5-3 loss: 0.444074  [  128/  179]
train() client id: f_00007-5-4 loss: 0.573475  [  160/  179]
train() client id: f_00007-6-0 loss: 0.629201  [   32/  179]
train() client id: f_00007-6-1 loss: 0.536911  [   64/  179]
train() client id: f_00007-6-2 loss: 0.390852  [   96/  179]
train() client id: f_00007-6-3 loss: 0.375702  [  128/  179]
train() client id: f_00007-6-4 loss: 0.516476  [  160/  179]
train() client id: f_00007-7-0 loss: 0.605545  [   32/  179]
train() client id: f_00007-7-1 loss: 0.497853  [   64/  179]
train() client id: f_00007-7-2 loss: 0.400144  [   96/  179]
train() client id: f_00007-7-3 loss: 0.331981  [  128/  179]
train() client id: f_00007-7-4 loss: 0.521323  [  160/  179]
train() client id: f_00007-8-0 loss: 0.502009  [   32/  179]
train() client id: f_00007-8-1 loss: 0.374110  [   64/  179]
train() client id: f_00007-8-2 loss: 0.570245  [   96/  179]
train() client id: f_00007-8-3 loss: 0.652695  [  128/  179]
train() client id: f_00007-8-4 loss: 0.346220  [  160/  179]
train() client id: f_00007-9-0 loss: 0.410133  [   32/  179]
train() client id: f_00007-9-1 loss: 0.391220  [   64/  179]
train() client id: f_00007-9-2 loss: 0.427142  [   96/  179]
train() client id: f_00007-9-3 loss: 0.473811  [  128/  179]
train() client id: f_00007-9-4 loss: 0.596643  [  160/  179]
train() client id: f_00007-10-0 loss: 0.403450  [   32/  179]
train() client id: f_00007-10-1 loss: 0.403359  [   64/  179]
train() client id: f_00007-10-2 loss: 0.425870  [   96/  179]
train() client id: f_00007-10-3 loss: 0.612282  [  128/  179]
train() client id: f_00007-10-4 loss: 0.479932  [  160/  179]
train() client id: f_00008-0-0 loss: 0.770113  [   32/  130]
train() client id: f_00008-0-1 loss: 0.645122  [   64/  130]
train() client id: f_00008-0-2 loss: 0.711224  [   96/  130]
train() client id: f_00008-0-3 loss: 0.788718  [  128/  130]
train() client id: f_00008-1-0 loss: 0.717729  [   32/  130]
train() client id: f_00008-1-1 loss: 0.672690  [   64/  130]
train() client id: f_00008-1-2 loss: 0.815588  [   96/  130]
train() client id: f_00008-1-3 loss: 0.680678  [  128/  130]
train() client id: f_00008-2-0 loss: 0.676230  [   32/  130]
train() client id: f_00008-2-1 loss: 0.713558  [   64/  130]
train() client id: f_00008-2-2 loss: 0.790109  [   96/  130]
train() client id: f_00008-2-3 loss: 0.737401  [  128/  130]
train() client id: f_00008-3-0 loss: 0.843378  [   32/  130]
train() client id: f_00008-3-1 loss: 0.566932  [   64/  130]
train() client id: f_00008-3-2 loss: 0.744065  [   96/  130]
train() client id: f_00008-3-3 loss: 0.728763  [  128/  130]
train() client id: f_00008-4-0 loss: 0.723851  [   32/  130]
train() client id: f_00008-4-1 loss: 0.618145  [   64/  130]
train() client id: f_00008-4-2 loss: 0.821330  [   96/  130]
train() client id: f_00008-4-3 loss: 0.736523  [  128/  130]
train() client id: f_00008-5-0 loss: 0.719194  [   32/  130]
train() client id: f_00008-5-1 loss: 0.770555  [   64/  130]
train() client id: f_00008-5-2 loss: 0.743591  [   96/  130]
train() client id: f_00008-5-3 loss: 0.673648  [  128/  130]
train() client id: f_00008-6-0 loss: 0.602822  [   32/  130]
train() client id: f_00008-6-1 loss: 0.756055  [   64/  130]
train() client id: f_00008-6-2 loss: 0.749334  [   96/  130]
train() client id: f_00008-6-3 loss: 0.810919  [  128/  130]
train() client id: f_00008-7-0 loss: 0.672033  [   32/  130]
train() client id: f_00008-7-1 loss: 0.723105  [   64/  130]
train() client id: f_00008-7-2 loss: 0.799521  [   96/  130]
train() client id: f_00008-7-3 loss: 0.684340  [  128/  130]
train() client id: f_00008-8-0 loss: 0.780404  [   32/  130]
train() client id: f_00008-8-1 loss: 0.784817  [   64/  130]
train() client id: f_00008-8-2 loss: 0.582531  [   96/  130]
train() client id: f_00008-8-3 loss: 0.732829  [  128/  130]
train() client id: f_00008-9-0 loss: 0.762579  [   32/  130]
train() client id: f_00008-9-1 loss: 0.840373  [   64/  130]
train() client id: f_00008-9-2 loss: 0.612652  [   96/  130]
train() client id: f_00008-9-3 loss: 0.698280  [  128/  130]
train() client id: f_00008-10-0 loss: 0.730996  [   32/  130]
train() client id: f_00008-10-1 loss: 0.673999  [   64/  130]
train() client id: f_00008-10-2 loss: 0.738799  [   96/  130]
train() client id: f_00008-10-3 loss: 0.769945  [  128/  130]
train() client id: f_00009-0-0 loss: 1.063417  [   32/  118]
train() client id: f_00009-0-1 loss: 1.141469  [   64/  118]
train() client id: f_00009-0-2 loss: 0.846281  [   96/  118]
train() client id: f_00009-1-0 loss: 1.058694  [   32/  118]
train() client id: f_00009-1-1 loss: 0.900557  [   64/  118]
train() client id: f_00009-1-2 loss: 0.945627  [   96/  118]
train() client id: f_00009-2-0 loss: 1.094840  [   32/  118]
train() client id: f_00009-2-1 loss: 0.983573  [   64/  118]
train() client id: f_00009-2-2 loss: 0.807748  [   96/  118]
train() client id: f_00009-3-0 loss: 1.024893  [   32/  118]
train() client id: f_00009-3-1 loss: 1.008356  [   64/  118]
train() client id: f_00009-3-2 loss: 0.871516  [   96/  118]
train() client id: f_00009-4-0 loss: 1.082227  [   32/  118]
train() client id: f_00009-4-1 loss: 0.946059  [   64/  118]
train() client id: f_00009-4-2 loss: 0.866579  [   96/  118]
train() client id: f_00009-5-0 loss: 1.037614  [   32/  118]
train() client id: f_00009-5-1 loss: 0.863226  [   64/  118]
train() client id: f_00009-5-2 loss: 0.736651  [   96/  118]
train() client id: f_00009-6-0 loss: 0.981099  [   32/  118]
train() client id: f_00009-6-1 loss: 0.787789  [   64/  118]
train() client id: f_00009-6-2 loss: 0.812606  [   96/  118]
train() client id: f_00009-7-0 loss: 0.935642  [   32/  118]
train() client id: f_00009-7-1 loss: 0.780625  [   64/  118]
train() client id: f_00009-7-2 loss: 0.634698  [   96/  118]
train() client id: f_00009-8-0 loss: 0.732251  [   32/  118]
train() client id: f_00009-8-1 loss: 0.943754  [   64/  118]
train() client id: f_00009-8-2 loss: 0.768910  [   96/  118]
train() client id: f_00009-9-0 loss: 0.742055  [   32/  118]
train() client id: f_00009-9-1 loss: 0.897488  [   64/  118]
train() client id: f_00009-9-2 loss: 0.785010  [   96/  118]
train() client id: f_00009-10-0 loss: 0.967623  [   32/  118]
train() client id: f_00009-10-1 loss: 0.650524  [   64/  118]
train() client id: f_00009-10-2 loss: 0.906787  [   96/  118]
At round 57 accuracy: 0.6472148541114059
At round 57 training accuracy: 0.5902079141515761
At round 57 training loss: 0.8265012650458461
update_location
xs = -3.905658 4.200318 305.009024 18.811294 0.979296 3.956410 -267.443192 -246.324852 289.663977 -232.060879 
ys = 297.587959 280.555839 1.320614 -267.455176 259.350187 242.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -12.711426168117436
ys mean: 83.39414253552872
dists_uav = 313.964724 297.874506 320.986368 286.157537 277.963088 262.629711 285.539404 265.850727 306.942811 252.721711 
uav_gains = -116.840799 -115.552108 -117.355498 -114.533395 -113.795292 -112.403306 -114.478269 -112.693728 -116.296360 -111.529219 
uav_gains_db_mean: -114.54779728602006
dists_bs = 210.478221 207.977857 510.463703 483.041821 195.150370 191.374386 200.240802 188.215539 490.608937 180.267682 
bs_gains = -104.616860 -104.471538 -115.390075 -114.718632 -103.697402 -103.459806 -104.010531 -103.257412 -114.907652 -102.732760 
bs_gains_db_mean: -107.12626675434352
Round 58
-------------------------------
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.3523265  6.77756785 3.31084036 1.21735079 7.81367428 3.76005882
 1.49535056 4.64371042 3.4255904  3.04870737]
obj_prev = 38.84517734952248
eta_min = 2.470941254922374e-28	eta_max = 0.9375517380484754
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 8.931115437766236	eta = 0.909090909090909
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 20.2431781023763	eta = 0.40108306173335817
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 14.165631330058979	eta = 0.5731615953668162
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.085726257080514	eta = 0.6204619975235665
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.020212302687447	eta = 0.623583983407007
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.019941071601606	eta = 0.6235969738929089
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.01994106692125	eta = 0.623596974117077
eta = 0.623596974117077
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [0.03896902 0.08195865 0.03835046 0.01329895 0.09463905 0.04515458
 0.01670101 0.05536075 0.04020613 0.0364948 ]
ene_total = [1.28342906 1.9813468  1.29389683 0.63079383 2.25502199 1.16000083
 0.7042715  1.51102723 1.23695895 0.96319404]
ti_comp = [1.03546301 1.14942413 1.02453888 1.07419893 1.1523401  1.15319241
 1.07495744 1.09635059 1.06887034 1.15568559]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [3.44960441e-06 2.60437692e-05 3.35842002e-06 1.27398049e-07
 3.98960382e-05 4.32694980e-06 2.51956506e-07 8.82240566e-06
 3.55555046e-06 2.27453939e-06]
ene_total = [0.42265881 0.17141173 0.4467888  0.33701571 0.16527621 0.16260767
 0.33534287 0.28827359 0.34886259 0.15705475]
optimize_network iter = 0 obj = 2.83529273119976
eta = 0.623596974117077
freqs = [18817195.6470524  35652050.01875965 18715963.43024579  6190171.56284024
 41063853.09945352 19578076.71969618  7768218.52091367 25247740.71552675
 18807768.58899782 15789240.96805261]
eta_min = 0.6235969741170779	eta_max = 0.7095637697055857
af = 0.0016217254572042322	bf = 1.0664469018281375	zeta = 0.0017838980029246557	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [6.96202700e-07 5.25618019e-06 6.77799772e-07 2.57116048e-08
 8.05185933e-06 8.73269445e-07 5.08501203e-08 1.78054696e-06
 7.17584839e-07 4.59049871e-07]
ene_total = [1.74973877 0.70781749 1.84965402 1.39538178 0.68140253 0.67295033
 1.38844638 1.19293349 1.4441827  0.65010868]
ti_comp = [0.75528321 0.86924434 0.74435908 0.79401913 0.8721603  0.87301261
 0.79477765 0.81617079 0.78869054 0.8755058 ]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.48645699e-06 1.74639655e-05 2.43999568e-06 8.94195368e-08
 2.67091848e-05 2.89538630e-06 1.76757958e-07 6.10500434e-06
 2.50441200e-06 1.51990678e-06]
ene_total = [0.54773486 0.22190259 0.5790084  0.43676858 0.21381912 0.21069725
 0.43459951 0.37352246 0.45209302 0.2035201 ]
optimize_network iter = 1 obj = 3.6736658995998672
eta = 0.7095637697055857
freqs = [18742766.932176   34251295.00082695 18715963.43024578  6084298.06393133
 39418319.48246514 18789072.00779956  7633454.28294466 24640242.31966294
 18518666.61499257 15142443.926058  ]
eta_min = 0.7095637697056919	eta_max = 0.7095637697055788
af = 0.0015119964848909963	bf = 1.0664469018281375	zeta = 0.001663196133380096	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [6.90706132e-07 4.85126751e-06 6.77799772e-07 2.48396102e-08
 7.41947189e-06 8.04301492e-07 4.91011128e-08 1.69589257e-06
 6.95693805e-07 4.22210775e-07]
ene_total = [1.74973827 0.70778046 1.84965402 1.3953817  0.68134469 0.67294402
 1.38844622 1.19292575 1.44418069 0.65010531]
ti_comp = [0.75528321 0.86924434 0.74435908 0.79401913 0.8721603  0.87301261
 0.79477765 0.81617079 0.78869054 0.8755058 ]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.48645699e-06 1.74639655e-05 2.43999568e-06 8.94195368e-08
 2.67091848e-05 2.89538630e-06 1.76757958e-07 6.10500434e-06
 2.50441200e-06 1.51990678e-06]
ene_total = [0.54773486 0.22190259 0.5790084  0.43676858 0.21381912 0.21069725
 0.43459951 0.37352246 0.45209302 0.2035201 ]
optimize_network iter = 2 obj = 3.67366589959978
eta = 0.7095637697055788
freqs = [18742766.93217598 34251295.00082704 18715963.43024575  6084298.06393133
 39418319.48246525 18789072.00779961  7633454.28294467 24640242.31966297
 18518666.61499257 15142443.92605804]
Done!
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.43444879e-06 1.70986789e-05 2.38895930e-06 8.75491851e-08
 2.61505198e-05 2.83482469e-06 1.73060784e-07 5.97730848e-06
 2.45202824e-06 1.48811552e-06]
ene_total = [0.01913201 0.00775057 0.02022438 0.01525607 0.00746802 0.00735947
 0.01518031 0.0130468  0.0157913  0.00710881]
At round 58 energy consumption: 0.12831774437155147
At round 58 eta: 0.7095637697055788
At round 58 a_n: 8.314943732033974
At round 58 local rounds: 11.234997825322132
At round 58 global rounds: 28.629154577598474
gradient difference: 0.4529997706413269
train() client id: f_00000-0-0 loss: 1.121093  [   32/  126]
train() client id: f_00000-0-1 loss: 0.990572  [   64/  126]
train() client id: f_00000-0-2 loss: 1.088523  [   96/  126]
train() client id: f_00000-1-0 loss: 0.895413  [   32/  126]
train() client id: f_00000-1-1 loss: 1.107107  [   64/  126]
train() client id: f_00000-1-2 loss: 0.983569  [   96/  126]
train() client id: f_00000-2-0 loss: 1.001106  [   32/  126]
train() client id: f_00000-2-1 loss: 0.930040  [   64/  126]
train() client id: f_00000-2-2 loss: 0.953695  [   96/  126]
train() client id: f_00000-3-0 loss: 0.906862  [   32/  126]
train() client id: f_00000-3-1 loss: 0.828205  [   64/  126]
train() client id: f_00000-3-2 loss: 0.921313  [   96/  126]
train() client id: f_00000-4-0 loss: 0.901791  [   32/  126]
train() client id: f_00000-4-1 loss: 0.840137  [   64/  126]
train() client id: f_00000-4-2 loss: 0.980083  [   96/  126]
train() client id: f_00000-5-0 loss: 0.788714  [   32/  126]
train() client id: f_00000-5-1 loss: 0.860589  [   64/  126]
train() client id: f_00000-5-2 loss: 1.006845  [   96/  126]
train() client id: f_00000-6-0 loss: 0.818745  [   32/  126]
train() client id: f_00000-6-1 loss: 0.899283  [   64/  126]
train() client id: f_00000-6-2 loss: 0.779655  [   96/  126]
train() client id: f_00000-7-0 loss: 0.798662  [   32/  126]
train() client id: f_00000-7-1 loss: 0.835614  [   64/  126]
train() client id: f_00000-7-2 loss: 0.733204  [   96/  126]
train() client id: f_00000-8-0 loss: 0.920678  [   32/  126]
train() client id: f_00000-8-1 loss: 0.873947  [   64/  126]
train() client id: f_00000-8-2 loss: 0.813513  [   96/  126]
train() client id: f_00000-9-0 loss: 0.735672  [   32/  126]
train() client id: f_00000-9-1 loss: 0.831893  [   64/  126]
train() client id: f_00000-9-2 loss: 0.838094  [   96/  126]
train() client id: f_00000-10-0 loss: 0.724438  [   32/  126]
train() client id: f_00000-10-1 loss: 0.830424  [   64/  126]
train() client id: f_00000-10-2 loss: 1.014103  [   96/  126]
train() client id: f_00001-0-0 loss: 0.497116  [   32/  265]
train() client id: f_00001-0-1 loss: 0.597650  [   64/  265]
train() client id: f_00001-0-2 loss: 0.423659  [   96/  265]
train() client id: f_00001-0-3 loss: 0.589720  [  128/  265]
train() client id: f_00001-0-4 loss: 0.375750  [  160/  265]
train() client id: f_00001-0-5 loss: 0.434728  [  192/  265]
train() client id: f_00001-0-6 loss: 0.485529  [  224/  265]
train() client id: f_00001-0-7 loss: 0.422776  [  256/  265]
train() client id: f_00001-1-0 loss: 0.414658  [   32/  265]
train() client id: f_00001-1-1 loss: 0.505276  [   64/  265]
train() client id: f_00001-1-2 loss: 0.418909  [   96/  265]
train() client id: f_00001-1-3 loss: 0.462691  [  128/  265]
train() client id: f_00001-1-4 loss: 0.519101  [  160/  265]
train() client id: f_00001-1-5 loss: 0.532154  [  192/  265]
train() client id: f_00001-1-6 loss: 0.419307  [  224/  265]
train() client id: f_00001-1-7 loss: 0.449163  [  256/  265]
train() client id: f_00001-2-0 loss: 0.527309  [   32/  265]
train() client id: f_00001-2-1 loss: 0.519863  [   64/  265]
train() client id: f_00001-2-2 loss: 0.383083  [   96/  265]
train() client id: f_00001-2-3 loss: 0.534871  [  128/  265]
train() client id: f_00001-2-4 loss: 0.407093  [  160/  265]
train() client id: f_00001-2-5 loss: 0.418521  [  192/  265]
train() client id: f_00001-2-6 loss: 0.394138  [  224/  265]
train() client id: f_00001-2-7 loss: 0.511704  [  256/  265]
train() client id: f_00001-3-0 loss: 0.587192  [   32/  265]
train() client id: f_00001-3-1 loss: 0.431198  [   64/  265]
train() client id: f_00001-3-2 loss: 0.459604  [   96/  265]
train() client id: f_00001-3-3 loss: 0.433487  [  128/  265]
train() client id: f_00001-3-4 loss: 0.392412  [  160/  265]
train() client id: f_00001-3-5 loss: 0.431036  [  192/  265]
train() client id: f_00001-3-6 loss: 0.408826  [  224/  265]
train() client id: f_00001-3-7 loss: 0.474009  [  256/  265]
train() client id: f_00001-4-0 loss: 0.469241  [   32/  265]
train() client id: f_00001-4-1 loss: 0.475757  [   64/  265]
train() client id: f_00001-4-2 loss: 0.403238  [   96/  265]
train() client id: f_00001-4-3 loss: 0.343589  [  128/  265]
train() client id: f_00001-4-4 loss: 0.390177  [  160/  265]
train() client id: f_00001-4-5 loss: 0.630856  [  192/  265]
train() client id: f_00001-4-6 loss: 0.462504  [  224/  265]
train() client id: f_00001-4-7 loss: 0.501551  [  256/  265]
train() client id: f_00001-5-0 loss: 0.385507  [   32/  265]
train() client id: f_00001-5-1 loss: 0.437413  [   64/  265]
train() client id: f_00001-5-2 loss: 0.626254  [   96/  265]
train() client id: f_00001-5-3 loss: 0.459921  [  128/  265]
train() client id: f_00001-5-4 loss: 0.453242  [  160/  265]
train() client id: f_00001-5-5 loss: 0.420847  [  192/  265]
train() client id: f_00001-5-6 loss: 0.514173  [  224/  265]
train() client id: f_00001-5-7 loss: 0.369147  [  256/  265]
train() client id: f_00001-6-0 loss: 0.371340  [   32/  265]
train() client id: f_00001-6-1 loss: 0.452885  [   64/  265]
train() client id: f_00001-6-2 loss: 0.384904  [   96/  265]
train() client id: f_00001-6-3 loss: 0.442454  [  128/  265]
train() client id: f_00001-6-4 loss: 0.570086  [  160/  265]
train() client id: f_00001-6-5 loss: 0.434881  [  192/  265]
train() client id: f_00001-6-6 loss: 0.536439  [  224/  265]
train() client id: f_00001-6-7 loss: 0.455764  [  256/  265]
train() client id: f_00001-7-0 loss: 0.498704  [   32/  265]
train() client id: f_00001-7-1 loss: 0.564652  [   64/  265]
train() client id: f_00001-7-2 loss: 0.357592  [   96/  265]
train() client id: f_00001-7-3 loss: 0.411641  [  128/  265]
train() client id: f_00001-7-4 loss: 0.380038  [  160/  265]
train() client id: f_00001-7-5 loss: 0.500234  [  192/  265]
train() client id: f_00001-7-6 loss: 0.353279  [  224/  265]
train() client id: f_00001-7-7 loss: 0.578152  [  256/  265]
train() client id: f_00001-8-0 loss: 0.494407  [   32/  265]
train() client id: f_00001-8-1 loss: 0.717835  [   64/  265]
train() client id: f_00001-8-2 loss: 0.362887  [   96/  265]
train() client id: f_00001-8-3 loss: 0.418061  [  128/  265]
train() client id: f_00001-8-4 loss: 0.387801  [  160/  265]
train() client id: f_00001-8-5 loss: 0.447468  [  192/  265]
train() client id: f_00001-8-6 loss: 0.450444  [  224/  265]
train() client id: f_00001-8-7 loss: 0.339341  [  256/  265]
train() client id: f_00001-9-0 loss: 0.402494  [   32/  265]
train() client id: f_00001-9-1 loss: 0.545999  [   64/  265]
train() client id: f_00001-9-2 loss: 0.537103  [   96/  265]
train() client id: f_00001-9-3 loss: 0.349603  [  128/  265]
train() client id: f_00001-9-4 loss: 0.354804  [  160/  265]
train() client id: f_00001-9-5 loss: 0.352958  [  192/  265]
train() client id: f_00001-9-6 loss: 0.625669  [  224/  265]
train() client id: f_00001-9-7 loss: 0.403445  [  256/  265]
train() client id: f_00001-10-0 loss: 0.524882  [   32/  265]
train() client id: f_00001-10-1 loss: 0.479656  [   64/  265]
train() client id: f_00001-10-2 loss: 0.431401  [   96/  265]
train() client id: f_00001-10-3 loss: 0.383069  [  128/  265]
train() client id: f_00001-10-4 loss: 0.485820  [  160/  265]
train() client id: f_00001-10-5 loss: 0.393102  [  192/  265]
train() client id: f_00001-10-6 loss: 0.408248  [  224/  265]
train() client id: f_00001-10-7 loss: 0.526676  [  256/  265]
train() client id: f_00002-0-0 loss: 0.979335  [   32/  124]
train() client id: f_00002-0-1 loss: 0.929802  [   64/  124]
train() client id: f_00002-0-2 loss: 1.043886  [   96/  124]
train() client id: f_00002-1-0 loss: 0.917085  [   32/  124]
train() client id: f_00002-1-1 loss: 0.870331  [   64/  124]
train() client id: f_00002-1-2 loss: 0.984417  [   96/  124]
train() client id: f_00002-2-0 loss: 0.798556  [   32/  124]
train() client id: f_00002-2-1 loss: 0.915081  [   64/  124]
train() client id: f_00002-2-2 loss: 1.088864  [   96/  124]
train() client id: f_00002-3-0 loss: 1.089414  [   32/  124]
train() client id: f_00002-3-1 loss: 0.804777  [   64/  124]
train() client id: f_00002-3-2 loss: 0.843034  [   96/  124]
train() client id: f_00002-4-0 loss: 0.784437  [   32/  124]
train() client id: f_00002-4-1 loss: 0.841381  [   64/  124]
train() client id: f_00002-4-2 loss: 0.846598  [   96/  124]
train() client id: f_00002-5-0 loss: 0.896376  [   32/  124]
train() client id: f_00002-5-1 loss: 0.927487  [   64/  124]
train() client id: f_00002-5-2 loss: 0.809683  [   96/  124]
train() client id: f_00002-6-0 loss: 0.761455  [   32/  124]
train() client id: f_00002-6-1 loss: 0.873304  [   64/  124]
train() client id: f_00002-6-2 loss: 0.809126  [   96/  124]
train() client id: f_00002-7-0 loss: 0.837010  [   32/  124]
train() client id: f_00002-7-1 loss: 0.680525  [   64/  124]
train() client id: f_00002-7-2 loss: 0.955125  [   96/  124]
train() client id: f_00002-8-0 loss: 0.771456  [   32/  124]
train() client id: f_00002-8-1 loss: 0.682076  [   64/  124]
train() client id: f_00002-8-2 loss: 0.967352  [   96/  124]
train() client id: f_00002-9-0 loss: 0.789662  [   32/  124]
train() client id: f_00002-9-1 loss: 0.798279  [   64/  124]
train() client id: f_00002-9-2 loss: 0.945166  [   96/  124]
train() client id: f_00002-10-0 loss: 0.844363  [   32/  124]
train() client id: f_00002-10-1 loss: 0.948368  [   64/  124]
train() client id: f_00002-10-2 loss: 0.731665  [   96/  124]
train() client id: f_00003-0-0 loss: 0.803599  [   32/   43]
train() client id: f_00003-1-0 loss: 0.435629  [   32/   43]
train() client id: f_00003-2-0 loss: 0.774178  [   32/   43]
train() client id: f_00003-3-0 loss: 0.652559  [   32/   43]
train() client id: f_00003-4-0 loss: 0.534542  [   32/   43]
train() client id: f_00003-5-0 loss: 0.527992  [   32/   43]
train() client id: f_00003-6-0 loss: 0.567042  [   32/   43]
train() client id: f_00003-7-0 loss: 0.649358  [   32/   43]
train() client id: f_00003-8-0 loss: 0.510016  [   32/   43]
train() client id: f_00003-9-0 loss: 0.593285  [   32/   43]
train() client id: f_00003-10-0 loss: 0.459699  [   32/   43]
train() client id: f_00004-0-0 loss: 0.677484  [   32/  306]
train() client id: f_00004-0-1 loss: 0.883374  [   64/  306]
train() client id: f_00004-0-2 loss: 0.843688  [   96/  306]
train() client id: f_00004-0-3 loss: 0.759687  [  128/  306]
train() client id: f_00004-0-4 loss: 0.656540  [  160/  306]
train() client id: f_00004-0-5 loss: 0.905886  [  192/  306]
train() client id: f_00004-0-6 loss: 0.733888  [  224/  306]
train() client id: f_00004-0-7 loss: 0.882112  [  256/  306]
train() client id: f_00004-0-8 loss: 0.820600  [  288/  306]
train() client id: f_00004-1-0 loss: 0.884447  [   32/  306]
train() client id: f_00004-1-1 loss: 0.765057  [   64/  306]
train() client id: f_00004-1-2 loss: 0.644048  [   96/  306]
train() client id: f_00004-1-3 loss: 0.824441  [  128/  306]
train() client id: f_00004-1-4 loss: 0.801868  [  160/  306]
train() client id: f_00004-1-5 loss: 0.813776  [  192/  306]
train() client id: f_00004-1-6 loss: 0.873203  [  224/  306]
train() client id: f_00004-1-7 loss: 0.699817  [  256/  306]
train() client id: f_00004-1-8 loss: 0.880939  [  288/  306]
train() client id: f_00004-2-0 loss: 0.882892  [   32/  306]
train() client id: f_00004-2-1 loss: 0.821479  [   64/  306]
train() client id: f_00004-2-2 loss: 0.616573  [   96/  306]
train() client id: f_00004-2-3 loss: 0.756506  [  128/  306]
train() client id: f_00004-2-4 loss: 0.859384  [  160/  306]
train() client id: f_00004-2-5 loss: 0.703276  [  192/  306]
train() client id: f_00004-2-6 loss: 0.869441  [  224/  306]
train() client id: f_00004-2-7 loss: 0.860586  [  256/  306]
train() client id: f_00004-2-8 loss: 0.719110  [  288/  306]
train() client id: f_00004-3-0 loss: 0.642328  [   32/  306]
train() client id: f_00004-3-1 loss: 0.847513  [   64/  306]
train() client id: f_00004-3-2 loss: 0.729887  [   96/  306]
train() client id: f_00004-3-3 loss: 0.844318  [  128/  306]
train() client id: f_00004-3-4 loss: 0.796610  [  160/  306]
train() client id: f_00004-3-5 loss: 0.802566  [  192/  306]
train() client id: f_00004-3-6 loss: 0.744772  [  224/  306]
train() client id: f_00004-3-7 loss: 1.067873  [  256/  306]
train() client id: f_00004-3-8 loss: 0.744214  [  288/  306]
train() client id: f_00004-4-0 loss: 0.864824  [   32/  306]
train() client id: f_00004-4-1 loss: 0.707121  [   64/  306]
train() client id: f_00004-4-2 loss: 0.843181  [   96/  306]
train() client id: f_00004-4-3 loss: 0.771247  [  128/  306]
train() client id: f_00004-4-4 loss: 0.894442  [  160/  306]
train() client id: f_00004-4-5 loss: 0.770264  [  192/  306]
train() client id: f_00004-4-6 loss: 0.675235  [  224/  306]
train() client id: f_00004-4-7 loss: 0.789770  [  256/  306]
train() client id: f_00004-4-8 loss: 0.786660  [  288/  306]
train() client id: f_00004-5-0 loss: 0.751574  [   32/  306]
train() client id: f_00004-5-1 loss: 0.753193  [   64/  306]
train() client id: f_00004-5-2 loss: 0.850829  [   96/  306]
train() client id: f_00004-5-3 loss: 0.752042  [  128/  306]
train() client id: f_00004-5-4 loss: 0.859386  [  160/  306]
train() client id: f_00004-5-5 loss: 0.668925  [  192/  306]
train() client id: f_00004-5-6 loss: 0.807169  [  224/  306]
train() client id: f_00004-5-7 loss: 0.864964  [  256/  306]
train() client id: f_00004-5-8 loss: 0.730318  [  288/  306]
train() client id: f_00004-6-0 loss: 0.737438  [   32/  306]
train() client id: f_00004-6-1 loss: 0.791044  [   64/  306]
train() client id: f_00004-6-2 loss: 0.771860  [   96/  306]
train() client id: f_00004-6-3 loss: 0.858877  [  128/  306]
train() client id: f_00004-6-4 loss: 0.676382  [  160/  306]
train() client id: f_00004-6-5 loss: 0.735069  [  192/  306]
train() client id: f_00004-6-6 loss: 0.819285  [  224/  306]
train() client id: f_00004-6-7 loss: 0.738067  [  256/  306]
train() client id: f_00004-6-8 loss: 0.908627  [  288/  306]
train() client id: f_00004-7-0 loss: 0.669653  [   32/  306]
train() client id: f_00004-7-1 loss: 0.890282  [   64/  306]
train() client id: f_00004-7-2 loss: 0.773421  [   96/  306]
train() client id: f_00004-7-3 loss: 0.909250  [  128/  306]
train() client id: f_00004-7-4 loss: 0.915804  [  160/  306]
train() client id: f_00004-7-5 loss: 0.783034  [  192/  306]
train() client id: f_00004-7-6 loss: 0.763045  [  224/  306]
train() client id: f_00004-7-7 loss: 0.656366  [  256/  306]
train() client id: f_00004-7-8 loss: 0.816651  [  288/  306]
train() client id: f_00004-8-0 loss: 0.635314  [   32/  306]
train() client id: f_00004-8-1 loss: 0.789893  [   64/  306]
train() client id: f_00004-8-2 loss: 0.821868  [   96/  306]
train() client id: f_00004-8-3 loss: 0.891927  [  128/  306]
train() client id: f_00004-8-4 loss: 0.740597  [  160/  306]
train() client id: f_00004-8-5 loss: 0.819344  [  192/  306]
train() client id: f_00004-8-6 loss: 0.945220  [  224/  306]
train() client id: f_00004-8-7 loss: 0.707701  [  256/  306]
train() client id: f_00004-8-8 loss: 0.888519  [  288/  306]
train() client id: f_00004-9-0 loss: 0.834620  [   32/  306]
train() client id: f_00004-9-1 loss: 0.669801  [   64/  306]
train() client id: f_00004-9-2 loss: 0.839888  [   96/  306]
train() client id: f_00004-9-3 loss: 0.749686  [  128/  306]
train() client id: f_00004-9-4 loss: 0.693114  [  160/  306]
train() client id: f_00004-9-5 loss: 0.821724  [  192/  306]
train() client id: f_00004-9-6 loss: 0.887771  [  224/  306]
train() client id: f_00004-9-7 loss: 0.829998  [  256/  306]
train() client id: f_00004-9-8 loss: 0.841300  [  288/  306]
train() client id: f_00004-10-0 loss: 0.734071  [   32/  306]
train() client id: f_00004-10-1 loss: 0.739795  [   64/  306]
train() client id: f_00004-10-2 loss: 0.834276  [   96/  306]
train() client id: f_00004-10-3 loss: 0.857454  [  128/  306]
train() client id: f_00004-10-4 loss: 0.631339  [  160/  306]
train() client id: f_00004-10-5 loss: 0.921095  [  192/  306]
train() client id: f_00004-10-6 loss: 0.721528  [  224/  306]
train() client id: f_00004-10-7 loss: 0.795441  [  256/  306]
train() client id: f_00004-10-8 loss: 0.840165  [  288/  306]
train() client id: f_00005-0-0 loss: 0.524425  [   32/  146]
train() client id: f_00005-0-1 loss: 0.566064  [   64/  146]
train() client id: f_00005-0-2 loss: 0.682254  [   96/  146]
train() client id: f_00005-0-3 loss: 0.603257  [  128/  146]
train() client id: f_00005-1-0 loss: 0.571076  [   32/  146]
train() client id: f_00005-1-1 loss: 0.638444  [   64/  146]
train() client id: f_00005-1-2 loss: 0.642053  [   96/  146]
train() client id: f_00005-1-3 loss: 0.576629  [  128/  146]
train() client id: f_00005-2-0 loss: 0.608063  [   32/  146]
train() client id: f_00005-2-1 loss: 0.471741  [   64/  146]
train() client id: f_00005-2-2 loss: 0.410205  [   96/  146]
train() client id: f_00005-2-3 loss: 0.788617  [  128/  146]
train() client id: f_00005-3-0 loss: 0.649703  [   32/  146]
train() client id: f_00005-3-1 loss: 0.497476  [   64/  146]
train() client id: f_00005-3-2 loss: 0.549768  [   96/  146]
train() client id: f_00005-3-3 loss: 0.772951  [  128/  146]
train() client id: f_00005-4-0 loss: 0.484612  [   32/  146]
train() client id: f_00005-4-1 loss: 0.517543  [   64/  146]
train() client id: f_00005-4-2 loss: 0.449279  [   96/  146]
train() client id: f_00005-4-3 loss: 0.556322  [  128/  146]
train() client id: f_00005-5-0 loss: 0.627503  [   32/  146]
train() client id: f_00005-5-1 loss: 0.389017  [   64/  146]
train() client id: f_00005-5-2 loss: 0.715565  [   96/  146]
train() client id: f_00005-5-3 loss: 0.694425  [  128/  146]
train() client id: f_00005-6-0 loss: 0.507078  [   32/  146]
train() client id: f_00005-6-1 loss: 0.756344  [   64/  146]
train() client id: f_00005-6-2 loss: 0.508974  [   96/  146]
train() client id: f_00005-6-3 loss: 0.684712  [  128/  146]
train() client id: f_00005-7-0 loss: 0.781466  [   32/  146]
train() client id: f_00005-7-1 loss: 0.496601  [   64/  146]
train() client id: f_00005-7-2 loss: 0.381272  [   96/  146]
train() client id: f_00005-7-3 loss: 0.745793  [  128/  146]
train() client id: f_00005-8-0 loss: 0.280638  [   32/  146]
train() client id: f_00005-8-1 loss: 0.443542  [   64/  146]
train() client id: f_00005-8-2 loss: 0.723707  [   96/  146]
train() client id: f_00005-8-3 loss: 0.794686  [  128/  146]
train() client id: f_00005-9-0 loss: 0.397414  [   32/  146]
train() client id: f_00005-9-1 loss: 0.566469  [   64/  146]
train() client id: f_00005-9-2 loss: 0.855580  [   96/  146]
train() client id: f_00005-9-3 loss: 0.561484  [  128/  146]
train() client id: f_00005-10-0 loss: 0.843267  [   32/  146]
train() client id: f_00005-10-1 loss: 0.481572  [   64/  146]
train() client id: f_00005-10-2 loss: 0.638988  [   96/  146]
train() client id: f_00005-10-3 loss: 0.602090  [  128/  146]
train() client id: f_00006-0-0 loss: 0.505420  [   32/   54]
train() client id: f_00006-1-0 loss: 0.448202  [   32/   54]
train() client id: f_00006-2-0 loss: 0.461760  [   32/   54]
train() client id: f_00006-3-0 loss: 0.476099  [   32/   54]
train() client id: f_00006-4-0 loss: 0.399109  [   32/   54]
train() client id: f_00006-5-0 loss: 0.481928  [   32/   54]
train() client id: f_00006-6-0 loss: 0.460075  [   32/   54]
train() client id: f_00006-7-0 loss: 0.481913  [   32/   54]
train() client id: f_00006-8-0 loss: 0.513793  [   32/   54]
train() client id: f_00006-9-0 loss: 0.490905  [   32/   54]
train() client id: f_00006-10-0 loss: 0.497936  [   32/   54]
train() client id: f_00007-0-0 loss: 0.653492  [   32/  179]
train() client id: f_00007-0-1 loss: 0.697866  [   64/  179]
train() client id: f_00007-0-2 loss: 0.603901  [   96/  179]
train() client id: f_00007-0-3 loss: 0.578222  [  128/  179]
train() client id: f_00007-0-4 loss: 0.517452  [  160/  179]
train() client id: f_00007-1-0 loss: 0.675772  [   32/  179]
train() client id: f_00007-1-1 loss: 0.462038  [   64/  179]
train() client id: f_00007-1-2 loss: 0.700945  [   96/  179]
train() client id: f_00007-1-3 loss: 0.435460  [  128/  179]
train() client id: f_00007-1-4 loss: 0.418208  [  160/  179]
train() client id: f_00007-2-0 loss: 0.697539  [   32/  179]
train() client id: f_00007-2-1 loss: 0.650940  [   64/  179]
train() client id: f_00007-2-2 loss: 0.459732  [   96/  179]
train() client id: f_00007-2-3 loss: 0.542821  [  128/  179]
train() client id: f_00007-2-4 loss: 0.499617  [  160/  179]
train() client id: f_00007-3-0 loss: 0.581176  [   32/  179]
train() client id: f_00007-3-1 loss: 0.491785  [   64/  179]
train() client id: f_00007-3-2 loss: 0.615484  [   96/  179]
train() client id: f_00007-3-3 loss: 0.518664  [  128/  179]
train() client id: f_00007-3-4 loss: 0.573974  [  160/  179]
train() client id: f_00007-4-0 loss: 0.447481  [   32/  179]
train() client id: f_00007-4-1 loss: 0.551543  [   64/  179]
train() client id: f_00007-4-2 loss: 0.575852  [   96/  179]
train() client id: f_00007-4-3 loss: 0.524775  [  128/  179]
train() client id: f_00007-4-4 loss: 0.686325  [  160/  179]
train() client id: f_00007-5-0 loss: 0.461009  [   32/  179]
train() client id: f_00007-5-1 loss: 0.479499  [   64/  179]
train() client id: f_00007-5-2 loss: 0.444597  [   96/  179]
train() client id: f_00007-5-3 loss: 0.876372  [  128/  179]
train() client id: f_00007-5-4 loss: 0.417522  [  160/  179]
train() client id: f_00007-6-0 loss: 0.343433  [   32/  179]
train() client id: f_00007-6-1 loss: 0.520094  [   64/  179]
train() client id: f_00007-6-2 loss: 0.655169  [   96/  179]
train() client id: f_00007-6-3 loss: 0.649240  [  128/  179]
train() client id: f_00007-6-4 loss: 0.482382  [  160/  179]
train() client id: f_00007-7-0 loss: 0.619386  [   32/  179]
train() client id: f_00007-7-1 loss: 0.504052  [   64/  179]
train() client id: f_00007-7-2 loss: 0.466569  [   96/  179]
train() client id: f_00007-7-3 loss: 0.673653  [  128/  179]
train() client id: f_00007-7-4 loss: 0.555290  [  160/  179]
train() client id: f_00007-8-0 loss: 0.490034  [   32/  179]
train() client id: f_00007-8-1 loss: 0.579283  [   64/  179]
train() client id: f_00007-8-2 loss: 0.584012  [   96/  179]
train() client id: f_00007-8-3 loss: 0.588182  [  128/  179]
train() client id: f_00007-8-4 loss: 0.389225  [  160/  179]
train() client id: f_00007-9-0 loss: 0.477453  [   32/  179]
train() client id: f_00007-9-1 loss: 0.477930  [   64/  179]
train() client id: f_00007-9-2 loss: 0.585531  [   96/  179]
train() client id: f_00007-9-3 loss: 0.482275  [  128/  179]
train() client id: f_00007-9-4 loss: 0.679094  [  160/  179]
train() client id: f_00007-10-0 loss: 0.493802  [   32/  179]
train() client id: f_00007-10-1 loss: 0.527249  [   64/  179]
train() client id: f_00007-10-2 loss: 0.572734  [   96/  179]
train() client id: f_00007-10-3 loss: 0.607427  [  128/  179]
train() client id: f_00007-10-4 loss: 0.536152  [  160/  179]
train() client id: f_00008-0-0 loss: 0.637124  [   32/  130]
train() client id: f_00008-0-1 loss: 0.750042  [   64/  130]
train() client id: f_00008-0-2 loss: 0.630538  [   96/  130]
train() client id: f_00008-0-3 loss: 0.773536  [  128/  130]
train() client id: f_00008-1-0 loss: 0.589169  [   32/  130]
train() client id: f_00008-1-1 loss: 0.820784  [   64/  130]
train() client id: f_00008-1-2 loss: 0.702462  [   96/  130]
train() client id: f_00008-1-3 loss: 0.671957  [  128/  130]
train() client id: f_00008-2-0 loss: 0.759786  [   32/  130]
train() client id: f_00008-2-1 loss: 0.689401  [   64/  130]
train() client id: f_00008-2-2 loss: 0.648720  [   96/  130]
train() client id: f_00008-2-3 loss: 0.649874  [  128/  130]
train() client id: f_00008-3-0 loss: 0.562928  [   32/  130]
train() client id: f_00008-3-1 loss: 0.763884  [   64/  130]
train() client id: f_00008-3-2 loss: 0.738766  [   96/  130]
train() client id: f_00008-3-3 loss: 0.716035  [  128/  130]
train() client id: f_00008-4-0 loss: 0.689522  [   32/  130]
train() client id: f_00008-4-1 loss: 0.755613  [   64/  130]
train() client id: f_00008-4-2 loss: 0.745011  [   96/  130]
train() client id: f_00008-4-3 loss: 0.592593  [  128/  130]
train() client id: f_00008-5-0 loss: 0.718812  [   32/  130]
train() client id: f_00008-5-1 loss: 0.609742  [   64/  130]
train() client id: f_00008-5-2 loss: 0.649737  [   96/  130]
train() client id: f_00008-5-3 loss: 0.798322  [  128/  130]
train() client id: f_00008-6-0 loss: 0.658673  [   32/  130]
train() client id: f_00008-6-1 loss: 0.684100  [   64/  130]
train() client id: f_00008-6-2 loss: 0.722972  [   96/  130]
train() client id: f_00008-6-3 loss: 0.727030  [  128/  130]
train() client id: f_00008-7-0 loss: 0.672755  [   32/  130]
train() client id: f_00008-7-1 loss: 0.717412  [   64/  130]
train() client id: f_00008-7-2 loss: 0.687871  [   96/  130]
train() client id: f_00008-7-3 loss: 0.696897  [  128/  130]
train() client id: f_00008-8-0 loss: 0.666502  [   32/  130]
train() client id: f_00008-8-1 loss: 0.670718  [   64/  130]
train() client id: f_00008-8-2 loss: 0.757981  [   96/  130]
train() client id: f_00008-8-3 loss: 0.662491  [  128/  130]
train() client id: f_00008-9-0 loss: 0.727037  [   32/  130]
train() client id: f_00008-9-1 loss: 0.663570  [   64/  130]
train() client id: f_00008-9-2 loss: 0.750163  [   96/  130]
train() client id: f_00008-9-3 loss: 0.612472  [  128/  130]
train() client id: f_00008-10-0 loss: 0.783086  [   32/  130]
train() client id: f_00008-10-1 loss: 0.619439  [   64/  130]
train() client id: f_00008-10-2 loss: 0.748676  [   96/  130]
train() client id: f_00008-10-3 loss: 0.627293  [  128/  130]
train() client id: f_00009-0-0 loss: 0.975118  [   32/  118]
train() client id: f_00009-0-1 loss: 1.025842  [   64/  118]
train() client id: f_00009-0-2 loss: 0.913379  [   96/  118]
train() client id: f_00009-1-0 loss: 0.905086  [   32/  118]
train() client id: f_00009-1-1 loss: 0.829380  [   64/  118]
train() client id: f_00009-1-2 loss: 0.949217  [   96/  118]
train() client id: f_00009-2-0 loss: 0.771705  [   32/  118]
train() client id: f_00009-2-1 loss: 0.933708  [   64/  118]
train() client id: f_00009-2-2 loss: 0.910744  [   96/  118]
train() client id: f_00009-3-0 loss: 0.892684  [   32/  118]
train() client id: f_00009-3-1 loss: 0.829904  [   64/  118]
train() client id: f_00009-3-2 loss: 0.848784  [   96/  118]
train() client id: f_00009-4-0 loss: 0.640947  [   32/  118]
train() client id: f_00009-4-1 loss: 0.790322  [   64/  118]
train() client id: f_00009-4-2 loss: 0.980353  [   96/  118]
train() client id: f_00009-5-0 loss: 0.852966  [   32/  118]
train() client id: f_00009-5-1 loss: 0.691803  [   64/  118]
train() client id: f_00009-5-2 loss: 0.810926  [   96/  118]
train() client id: f_00009-6-0 loss: 0.912883  [   32/  118]
train() client id: f_00009-6-1 loss: 0.661248  [   64/  118]
train() client id: f_00009-6-2 loss: 0.699829  [   96/  118]
train() client id: f_00009-7-0 loss: 0.786086  [   32/  118]
train() client id: f_00009-7-1 loss: 0.770522  [   64/  118]
train() client id: f_00009-7-2 loss: 0.753009  [   96/  118]
train() client id: f_00009-8-0 loss: 0.805692  [   32/  118]
train() client id: f_00009-8-1 loss: 0.842407  [   64/  118]
train() client id: f_00009-8-2 loss: 0.624194  [   96/  118]
train() client id: f_00009-9-0 loss: 0.773316  [   32/  118]
train() client id: f_00009-9-1 loss: 0.818976  [   64/  118]
train() client id: f_00009-9-2 loss: 0.708897  [   96/  118]
train() client id: f_00009-10-0 loss: 0.562930  [   32/  118]
train() client id: f_00009-10-1 loss: 0.884548  [   64/  118]
train() client id: f_00009-10-2 loss: 0.853054  [   96/  118]
At round 58 accuracy: 0.6472148541114059
At round 58 training accuracy: 0.5868544600938967
At round 58 training loss: 0.8358564617193481
update_location
xs = -3.905658 4.200318 310.009024 18.811294 0.979296 3.956410 -272.443192 -251.324852 294.663977 -237.060879 
ys = 302.587959 285.555839 1.320614 -272.455176 264.350187 247.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -13.21142616811743
ys mean: 84.89414253552872
dists_uav = 318.707903 302.588466 325.741215 290.836187 282.634004 267.259250 290.227813 270.490032 311.665733 257.320563 
uav_gains = -117.191787 -115.944481 -117.686903 -114.946654 -114.217785 -112.821340 -114.893352 -113.114986 -116.665768 -111.930584 
uav_gains_db_mean: -114.94136393793016
dists_bs = 213.429054 210.559605 515.168208 487.625832 197.363038 193.202736 202.597657 190.166605 495.347120 181.912193 
bs_gains = -104.786158 -104.621561 -115.501633 -114.833487 -103.834502 -103.575431 -104.152823 -103.382818 -115.024529 -102.843190 
bs_gains_db_mean: -107.2556131453675
Round 59
-------------------------------
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.22013439 6.49883235 3.18055184 1.17195349 7.49218211 3.60548997
 1.43848068 4.45606788 3.28586187 2.92340788]
obj_prev = 37.272962461679306
eta_min = 1.7285175423879787e-29	eta_max = 0.9377803249311158
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 8.56318552740206	eta = 0.909090909090909
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 19.714881852065698	eta = 0.3948648627079844
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 13.689416383079331	eta = 0.5686666179167633
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.622409124377576	eta = 0.6167375846489944
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557303683729339	eta = 0.6199351637809642
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557030281762911	eta = 0.6199486615180114
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557030276907934	eta = 0.6199486617577047
eta = 0.6199486617577047
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [0.0394532  0.08297697 0.03882696 0.01346419 0.09581492 0.04571561
 0.01690851 0.0560486  0.04070568 0.03694824]
ene_total = [1.24429988 1.90408729 1.25458552 0.61484934 2.16708445 1.11417022
 0.68545833 1.4588381  1.18872479 0.92493235]
ti_comp = [1.09084903 1.21156017 1.07968336 1.13102359 1.21456653 1.21550713
 1.13180693 1.15453068 1.12986699 1.21804473]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [3.22550318e-06 2.43255565e-05 3.13824884e-06 1.19255159e-07
 3.72680608e-05 4.04164983e-06 2.35858602e-07 8.25587810e-06
 3.30210249e-06 2.12488134e-06]
ene_total = [0.4167514  0.16397633 0.44017198 0.33241142 0.15794132 0.15527123
 0.33077065 0.2832709  0.3349044  0.14990784]
optimize_network iter = 0 obj = 2.7653774631169483
eta = 0.6199486617577047
freqs = [18083713.04857494 34243850.94632931 17980716.29606591  5952213.52469169
 39444079.62545728 18805161.01539245  7469699.35866398 24273324.57197437
 18013484.95120145 15167027.70060071]
eta_min = 0.6199486617577048	eta_max = 0.7133529451356779
af = 0.0014346255173217418	bf = 1.0502994844473539	zeta = 0.001578088069053916	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [6.42985407e-07 4.84915903e-06 6.25591760e-07 2.37728263e-08
 7.42917241e-06 8.05679522e-07 4.70170484e-08 1.64576156e-06
 6.58255038e-07 4.23582807e-07]
ene_total = [1.74202974 0.68380564 1.83994709 1.38965906 0.65766716 0.64883763
 1.38279153 1.18365265 1.39985765 0.62655025]
ti_comp = [0.77393525 0.89464639 0.76276958 0.81410981 0.89765275 0.89859335
 0.81489315 0.8376169  0.81295321 0.90113095]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.25947941e-06 1.57304375e-05 2.21709516e-06 8.11606270e-08
 2.40576692e-05 2.60758319e-06 1.60430136e-07 5.53062528e-06
 2.24908028e-06 1.36891415e-06]
ene_total = [0.55252348 0.21716916 0.58357692 0.44072701 0.20903928 0.20582665
 0.43855056 0.37549926 0.44400411 0.19873447]
optimize_network iter = 1 obj = 3.6656508882711805
eta = 0.7133529451356779
freqs = [18007133.55773868 32762205.35417381 17980716.2960659   5842034.71544074
 37704373.23913116 17970837.74832523  7329456.31607533 23636668.595279
 17687093.31024747 14483474.75860257]
eta_min = 0.7133529451356788	eta_max = 0.7133529451356644
af = 0.0013279300383138625	bf = 1.0502994844473539	zeta = 0.0014607230421452488	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [6.37551209e-07 4.43861511e-06 6.25591760e-07 2.29008752e-08
 6.78828762e-06 7.35774715e-07 4.52681388e-08 1.56056161e-06
 6.34616915e-07 3.86262812e-07]
ene_total = [1.74202926 0.68376964 1.83994709 1.38965899 0.65761096 0.6488315
 1.38279138 1.18364518 1.39985558 0.62654698]
ti_comp = [0.77393525 0.89464639 0.76276958 0.81410981 0.89765275 0.89859335
 0.81489315 0.8376169  0.81295321 0.90113095]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.25947941e-06 1.57304375e-05 2.21709516e-06 8.11606270e-08
 2.40576692e-05 2.60758319e-06 1.60430136e-07 5.53062528e-06
 2.24908028e-06 1.36891415e-06]
ene_total = [0.55252348 0.21716916 0.58357692 0.44072701 0.20903928 0.20582665
 0.43855056 0.37549926 0.44400411 0.19873447]
optimize_network iter = 2 obj = 3.6656508882710073
eta = 0.7133529451356644
freqs = [18007133.55773862 32762205.35417396 17980716.29606583  5842034.71544073
 37704373.23913135 17970837.74832532  7329456.31607533 23636668.59527903
 17687093.31024747 14483474.75860265]
Done!
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.24710003e-06 1.56442526e-05 2.20494800e-06 8.07159591e-08
 2.39258606e-05 2.59329661e-06 1.59551162e-07 5.50032375e-06
 2.23675788e-06 1.36141406e-06]
ene_total = [0.01986593 0.00780822 0.02098246 0.01584631 0.00751586 0.00740047
 0.01576806 0.01350102 0.01596413 0.00714548]
At round 59 energy consumption: 0.131797934436514
At round 59 eta: 0.7133529451356644
At round 59 a_n: 7.972397885064655
At round 59 local rounds: 11.060599497015481
At round 59 global rounds: 27.812593047006303
gradient difference: 0.5026129484176636
train() client id: f_00000-0-0 loss: 0.788557  [   32/  126]
train() client id: f_00000-0-1 loss: 0.911824  [   64/  126]
train() client id: f_00000-0-2 loss: 0.935751  [   96/  126]
train() client id: f_00000-1-0 loss: 0.825397  [   32/  126]
train() client id: f_00000-1-1 loss: 0.884454  [   64/  126]
train() client id: f_00000-1-2 loss: 1.003194  [   96/  126]
train() client id: f_00000-2-0 loss: 0.898151  [   32/  126]
train() client id: f_00000-2-1 loss: 0.824852  [   64/  126]
train() client id: f_00000-2-2 loss: 0.731961  [   96/  126]
train() client id: f_00000-3-0 loss: 0.642853  [   32/  126]
train() client id: f_00000-3-1 loss: 0.805710  [   64/  126]
train() client id: f_00000-3-2 loss: 0.787621  [   96/  126]
train() client id: f_00000-4-0 loss: 0.635151  [   32/  126]
train() client id: f_00000-4-1 loss: 0.860376  [   64/  126]
train() client id: f_00000-4-2 loss: 0.594716  [   96/  126]
train() client id: f_00000-5-0 loss: 0.680795  [   32/  126]
train() client id: f_00000-5-1 loss: 0.654445  [   64/  126]
train() client id: f_00000-5-2 loss: 0.701802  [   96/  126]
train() client id: f_00000-6-0 loss: 0.707725  [   32/  126]
train() client id: f_00000-6-1 loss: 0.693402  [   64/  126]
train() client id: f_00000-6-2 loss: 0.650688  [   96/  126]
train() client id: f_00000-7-0 loss: 0.701018  [   32/  126]
train() client id: f_00000-7-1 loss: 0.694603  [   64/  126]
train() client id: f_00000-7-2 loss: 0.668771  [   96/  126]
train() client id: f_00000-8-0 loss: 0.635039  [   32/  126]
train() client id: f_00000-8-1 loss: 0.656125  [   64/  126]
train() client id: f_00000-8-2 loss: 0.713431  [   96/  126]
train() client id: f_00000-9-0 loss: 0.729358  [   32/  126]
train() client id: f_00000-9-1 loss: 0.691522  [   64/  126]
train() client id: f_00000-9-2 loss: 0.519266  [   96/  126]
train() client id: f_00000-10-0 loss: 0.616228  [   32/  126]
train() client id: f_00000-10-1 loss: 0.780583  [   64/  126]
train() client id: f_00000-10-2 loss: 0.621536  [   96/  126]
train() client id: f_00001-0-0 loss: 0.479492  [   32/  265]
train() client id: f_00001-0-1 loss: 0.293320  [   64/  265]
train() client id: f_00001-0-2 loss: 0.508954  [   96/  265]
train() client id: f_00001-0-3 loss: 0.392767  [  128/  265]
train() client id: f_00001-0-4 loss: 0.312817  [  160/  265]
train() client id: f_00001-0-5 loss: 0.362547  [  192/  265]
train() client id: f_00001-0-6 loss: 0.361591  [  224/  265]
train() client id: f_00001-0-7 loss: 0.400882  [  256/  265]
train() client id: f_00001-1-0 loss: 0.269340  [   32/  265]
train() client id: f_00001-1-1 loss: 0.582829  [   64/  265]
train() client id: f_00001-1-2 loss: 0.341140  [   96/  265]
train() client id: f_00001-1-3 loss: 0.359708  [  128/  265]
train() client id: f_00001-1-4 loss: 0.344025  [  160/  265]
train() client id: f_00001-1-5 loss: 0.337081  [  192/  265]
train() client id: f_00001-1-6 loss: 0.393529  [  224/  265]
train() client id: f_00001-1-7 loss: 0.402342  [  256/  265]
train() client id: f_00001-2-0 loss: 0.414050  [   32/  265]
train() client id: f_00001-2-1 loss: 0.284408  [   64/  265]
train() client id: f_00001-2-2 loss: 0.472774  [   96/  265]
train() client id: f_00001-2-3 loss: 0.299613  [  128/  265]
train() client id: f_00001-2-4 loss: 0.259751  [  160/  265]
train() client id: f_00001-2-5 loss: 0.389610  [  192/  265]
train() client id: f_00001-2-6 loss: 0.299239  [  224/  265]
train() client id: f_00001-2-7 loss: 0.439702  [  256/  265]
train() client id: f_00001-3-0 loss: 0.278217  [   32/  265]
train() client id: f_00001-3-1 loss: 0.464772  [   64/  265]
train() client id: f_00001-3-2 loss: 0.279951  [   96/  265]
train() client id: f_00001-3-3 loss: 0.531151  [  128/  265]
train() client id: f_00001-3-4 loss: 0.346640  [  160/  265]
train() client id: f_00001-3-5 loss: 0.265227  [  192/  265]
train() client id: f_00001-3-6 loss: 0.365898  [  224/  265]
train() client id: f_00001-3-7 loss: 0.271711  [  256/  265]
train() client id: f_00001-4-0 loss: 0.363289  [   32/  265]
train() client id: f_00001-4-1 loss: 0.508658  [   64/  265]
train() client id: f_00001-4-2 loss: 0.304615  [   96/  265]
train() client id: f_00001-4-3 loss: 0.314897  [  128/  265]
train() client id: f_00001-4-4 loss: 0.431077  [  160/  265]
train() client id: f_00001-4-5 loss: 0.372073  [  192/  265]
train() client id: f_00001-4-6 loss: 0.242829  [  224/  265]
train() client id: f_00001-4-7 loss: 0.315646  [  256/  265]
train() client id: f_00001-5-0 loss: 0.316402  [   32/  265]
train() client id: f_00001-5-1 loss: 0.248530  [   64/  265]
train() client id: f_00001-5-2 loss: 0.364802  [   96/  265]
train() client id: f_00001-5-3 loss: 0.386710  [  128/  265]
train() client id: f_00001-5-4 loss: 0.397626  [  160/  265]
train() client id: f_00001-5-5 loss: 0.394984  [  192/  265]
train() client id: f_00001-5-6 loss: 0.321228  [  224/  265]
train() client id: f_00001-5-7 loss: 0.391924  [  256/  265]
train() client id: f_00001-6-0 loss: 0.297460  [   32/  265]
train() client id: f_00001-6-1 loss: 0.393364  [   64/  265]
train() client id: f_00001-6-2 loss: 0.349929  [   96/  265]
train() client id: f_00001-6-3 loss: 0.250323  [  128/  265]
train() client id: f_00001-6-4 loss: 0.334313  [  160/  265]
train() client id: f_00001-6-5 loss: 0.356368  [  192/  265]
train() client id: f_00001-6-6 loss: 0.426922  [  224/  265]
train() client id: f_00001-6-7 loss: 0.383322  [  256/  265]
train() client id: f_00001-7-0 loss: 0.340820  [   32/  265]
train() client id: f_00001-7-1 loss: 0.416752  [   64/  265]
train() client id: f_00001-7-2 loss: 0.313927  [   96/  265]
train() client id: f_00001-7-3 loss: 0.450169  [  128/  265]
train() client id: f_00001-7-4 loss: 0.272247  [  160/  265]
train() client id: f_00001-7-5 loss: 0.258198  [  192/  265]
train() client id: f_00001-7-6 loss: 0.323471  [  224/  265]
train() client id: f_00001-7-7 loss: 0.275963  [  256/  265]
train() client id: f_00001-8-0 loss: 0.295099  [   32/  265]
train() client id: f_00001-8-1 loss: 0.295962  [   64/  265]
train() client id: f_00001-8-2 loss: 0.372638  [   96/  265]
train() client id: f_00001-8-3 loss: 0.434431  [  128/  265]
train() client id: f_00001-8-4 loss: 0.367576  [  160/  265]
train() client id: f_00001-8-5 loss: 0.308507  [  192/  265]
train() client id: f_00001-8-6 loss: 0.420625  [  224/  265]
train() client id: f_00001-8-7 loss: 0.253584  [  256/  265]
train() client id: f_00001-9-0 loss: 0.346057  [   32/  265]
train() client id: f_00001-9-1 loss: 0.401023  [   64/  265]
train() client id: f_00001-9-2 loss: 0.347984  [   96/  265]
train() client id: f_00001-9-3 loss: 0.357853  [  128/  265]
train() client id: f_00001-9-4 loss: 0.242417  [  160/  265]
train() client id: f_00001-9-5 loss: 0.253767  [  192/  265]
train() client id: f_00001-9-6 loss: 0.275959  [  224/  265]
train() client id: f_00001-9-7 loss: 0.488448  [  256/  265]
train() client id: f_00001-10-0 loss: 0.383361  [   32/  265]
train() client id: f_00001-10-1 loss: 0.312411  [   64/  265]
train() client id: f_00001-10-2 loss: 0.286741  [   96/  265]
train() client id: f_00001-10-3 loss: 0.330611  [  128/  265]
train() client id: f_00001-10-4 loss: 0.312510  [  160/  265]
train() client id: f_00001-10-5 loss: 0.247235  [  192/  265]
train() client id: f_00001-10-6 loss: 0.558804  [  224/  265]
train() client id: f_00001-10-7 loss: 0.229769  [  256/  265]
train() client id: f_00002-0-0 loss: 1.088600  [   32/  124]
train() client id: f_00002-0-1 loss: 0.855970  [   64/  124]
train() client id: f_00002-0-2 loss: 1.264714  [   96/  124]
train() client id: f_00002-1-0 loss: 1.329753  [   32/  124]
train() client id: f_00002-1-1 loss: 0.839979  [   64/  124]
train() client id: f_00002-1-2 loss: 0.978289  [   96/  124]
train() client id: f_00002-2-0 loss: 1.084784  [   32/  124]
train() client id: f_00002-2-1 loss: 1.178109  [   64/  124]
train() client id: f_00002-2-2 loss: 1.050205  [   96/  124]
train() client id: f_00002-3-0 loss: 1.184242  [   32/  124]
train() client id: f_00002-3-1 loss: 0.970310  [   64/  124]
train() client id: f_00002-3-2 loss: 0.990367  [   96/  124]
train() client id: f_00002-4-0 loss: 1.026514  [   32/  124]
train() client id: f_00002-4-1 loss: 0.903882  [   64/  124]
train() client id: f_00002-4-2 loss: 1.052177  [   96/  124]
train() client id: f_00002-5-0 loss: 1.306920  [   32/  124]
train() client id: f_00002-5-1 loss: 1.156495  [   64/  124]
train() client id: f_00002-5-2 loss: 0.784840  [   96/  124]
train() client id: f_00002-6-0 loss: 1.178629  [   32/  124]
train() client id: f_00002-6-1 loss: 0.936485  [   64/  124]
train() client id: f_00002-6-2 loss: 0.993239  [   96/  124]
train() client id: f_00002-7-0 loss: 0.990343  [   32/  124]
train() client id: f_00002-7-1 loss: 1.094292  [   64/  124]
train() client id: f_00002-7-2 loss: 0.915424  [   96/  124]
train() client id: f_00002-8-0 loss: 1.102314  [   32/  124]
train() client id: f_00002-8-1 loss: 1.005829  [   64/  124]
train() client id: f_00002-8-2 loss: 1.041894  [   96/  124]
train() client id: f_00002-9-0 loss: 1.264631  [   32/  124]
train() client id: f_00002-9-1 loss: 0.955193  [   64/  124]
train() client id: f_00002-9-2 loss: 0.991059  [   96/  124]
train() client id: f_00002-10-0 loss: 1.076292  [   32/  124]
train() client id: f_00002-10-1 loss: 1.063737  [   64/  124]
train() client id: f_00002-10-2 loss: 0.953235  [   96/  124]
train() client id: f_00003-0-0 loss: 0.840757  [   32/   43]
train() client id: f_00003-1-0 loss: 0.896850  [   32/   43]
train() client id: f_00003-2-0 loss: 0.807060  [   32/   43]
train() client id: f_00003-3-0 loss: 0.758910  [   32/   43]
train() client id: f_00003-4-0 loss: 0.709093  [   32/   43]
train() client id: f_00003-5-0 loss: 0.975598  [   32/   43]
train() client id: f_00003-6-0 loss: 0.727380  [   32/   43]
train() client id: f_00003-7-0 loss: 0.594145  [   32/   43]
train() client id: f_00003-8-0 loss: 0.720515  [   32/   43]
train() client id: f_00003-9-0 loss: 0.949611  [   32/   43]
train() client id: f_00003-10-0 loss: 0.753684  [   32/   43]
train() client id: f_00004-0-0 loss: 0.802380  [   32/  306]
train() client id: f_00004-0-1 loss: 0.837796  [   64/  306]
train() client id: f_00004-0-2 loss: 0.602477  [   96/  306]
train() client id: f_00004-0-3 loss: 0.805664  [  128/  306]
train() client id: f_00004-0-4 loss: 0.709821  [  160/  306]
train() client id: f_00004-0-5 loss: 0.649149  [  192/  306]
train() client id: f_00004-0-6 loss: 0.816414  [  224/  306]
train() client id: f_00004-0-7 loss: 0.775227  [  256/  306]
train() client id: f_00004-0-8 loss: 0.762884  [  288/  306]
train() client id: f_00004-1-0 loss: 0.679078  [   32/  306]
train() client id: f_00004-1-1 loss: 0.807520  [   64/  306]
train() client id: f_00004-1-2 loss: 0.564406  [   96/  306]
train() client id: f_00004-1-3 loss: 0.742838  [  128/  306]
train() client id: f_00004-1-4 loss: 0.792523  [  160/  306]
train() client id: f_00004-1-5 loss: 0.785076  [  192/  306]
train() client id: f_00004-1-6 loss: 0.767199  [  224/  306]
train() client id: f_00004-1-7 loss: 0.721241  [  256/  306]
train() client id: f_00004-1-8 loss: 0.875394  [  288/  306]
train() client id: f_00004-2-0 loss: 0.609614  [   32/  306]
train() client id: f_00004-2-1 loss: 0.757598  [   64/  306]
train() client id: f_00004-2-2 loss: 0.664238  [   96/  306]
train() client id: f_00004-2-3 loss: 0.747332  [  128/  306]
train() client id: f_00004-2-4 loss: 0.776940  [  160/  306]
train() client id: f_00004-2-5 loss: 0.718039  [  192/  306]
train() client id: f_00004-2-6 loss: 0.805079  [  224/  306]
train() client id: f_00004-2-7 loss: 0.824563  [  256/  306]
train() client id: f_00004-2-8 loss: 0.872356  [  288/  306]
train() client id: f_00004-3-0 loss: 0.818925  [   32/  306]
train() client id: f_00004-3-1 loss: 0.766788  [   64/  306]
train() client id: f_00004-3-2 loss: 0.836753  [   96/  306]
train() client id: f_00004-3-3 loss: 0.699622  [  128/  306]
train() client id: f_00004-3-4 loss: 0.604449  [  160/  306]
train() client id: f_00004-3-5 loss: 0.679422  [  192/  306]
train() client id: f_00004-3-6 loss: 0.760834  [  224/  306]
train() client id: f_00004-3-7 loss: 0.714846  [  256/  306]
train() client id: f_00004-3-8 loss: 0.879140  [  288/  306]
train() client id: f_00004-4-0 loss: 0.740905  [   32/  306]
train() client id: f_00004-4-1 loss: 0.729128  [   64/  306]
train() client id: f_00004-4-2 loss: 0.687992  [   96/  306]
train() client id: f_00004-4-3 loss: 0.732460  [  128/  306]
train() client id: f_00004-4-4 loss: 0.835912  [  160/  306]
train() client id: f_00004-4-5 loss: 0.751829  [  192/  306]
train() client id: f_00004-4-6 loss: 0.739824  [  224/  306]
train() client id: f_00004-4-7 loss: 0.846947  [  256/  306]
train() client id: f_00004-4-8 loss: 0.787713  [  288/  306]
train() client id: f_00004-5-0 loss: 0.653636  [   32/  306]
train() client id: f_00004-5-1 loss: 0.764189  [   64/  306]
train() client id: f_00004-5-2 loss: 0.693608  [   96/  306]
train() client id: f_00004-5-3 loss: 0.800022  [  128/  306]
train() client id: f_00004-5-4 loss: 0.767984  [  160/  306]
train() client id: f_00004-5-5 loss: 0.744216  [  192/  306]
train() client id: f_00004-5-6 loss: 0.685129  [  224/  306]
train() client id: f_00004-5-7 loss: 0.860373  [  256/  306]
train() client id: f_00004-5-8 loss: 0.888128  [  288/  306]
train() client id: f_00004-6-0 loss: 0.718736  [   32/  306]
train() client id: f_00004-6-1 loss: 0.642120  [   64/  306]
train() client id: f_00004-6-2 loss: 0.674124  [   96/  306]
train() client id: f_00004-6-3 loss: 0.900798  [  128/  306]
train() client id: f_00004-6-4 loss: 0.660906  [  160/  306]
train() client id: f_00004-6-5 loss: 0.815950  [  192/  306]
train() client id: f_00004-6-6 loss: 0.857198  [  224/  306]
train() client id: f_00004-6-7 loss: 0.767740  [  256/  306]
train() client id: f_00004-6-8 loss: 0.775030  [  288/  306]
train() client id: f_00004-7-0 loss: 0.727608  [   32/  306]
train() client id: f_00004-7-1 loss: 0.829358  [   64/  306]
train() client id: f_00004-7-2 loss: 0.671823  [   96/  306]
train() client id: f_00004-7-3 loss: 0.648314  [  128/  306]
train() client id: f_00004-7-4 loss: 0.786768  [  160/  306]
train() client id: f_00004-7-5 loss: 0.792442  [  192/  306]
train() client id: f_00004-7-6 loss: 0.841795  [  224/  306]
train() client id: f_00004-7-7 loss: 0.847290  [  256/  306]
train() client id: f_00004-7-8 loss: 0.753389  [  288/  306]
train() client id: f_00004-8-0 loss: 0.677549  [   32/  306]
train() client id: f_00004-8-1 loss: 0.575486  [   64/  306]
train() client id: f_00004-8-2 loss: 0.823765  [   96/  306]
train() client id: f_00004-8-3 loss: 0.760645  [  128/  306]
train() client id: f_00004-8-4 loss: 0.813561  [  160/  306]
train() client id: f_00004-8-5 loss: 0.780811  [  192/  306]
train() client id: f_00004-8-6 loss: 0.818807  [  224/  306]
train() client id: f_00004-8-7 loss: 0.908459  [  256/  306]
train() client id: f_00004-8-8 loss: 0.735476  [  288/  306]
train() client id: f_00004-9-0 loss: 0.744490  [   32/  306]
train() client id: f_00004-9-1 loss: 0.830405  [   64/  306]
train() client id: f_00004-9-2 loss: 0.629360  [   96/  306]
train() client id: f_00004-9-3 loss: 0.731206  [  128/  306]
train() client id: f_00004-9-4 loss: 0.838534  [  160/  306]
train() client id: f_00004-9-5 loss: 0.782802  [  192/  306]
train() client id: f_00004-9-6 loss: 0.771432  [  224/  306]
train() client id: f_00004-9-7 loss: 0.744793  [  256/  306]
train() client id: f_00004-9-8 loss: 0.861795  [  288/  306]
train() client id: f_00004-10-0 loss: 0.677324  [   32/  306]
train() client id: f_00004-10-1 loss: 0.737627  [   64/  306]
train() client id: f_00004-10-2 loss: 1.018412  [   96/  306]
train() client id: f_00004-10-3 loss: 0.952096  [  128/  306]
train() client id: f_00004-10-4 loss: 0.641525  [  160/  306]
train() client id: f_00004-10-5 loss: 0.663006  [  192/  306]
train() client id: f_00004-10-6 loss: 0.913789  [  224/  306]
train() client id: f_00004-10-7 loss: 0.731489  [  256/  306]
train() client id: f_00004-10-8 loss: 0.710447  [  288/  306]
train() client id: f_00005-0-0 loss: 0.585229  [   32/  146]
train() client id: f_00005-0-1 loss: 0.373726  [   64/  146]
train() client id: f_00005-0-2 loss: 0.765437  [   96/  146]
train() client id: f_00005-0-3 loss: 0.984728  [  128/  146]
train() client id: f_00005-1-0 loss: 0.519461  [   32/  146]
train() client id: f_00005-1-1 loss: 0.877859  [   64/  146]
train() client id: f_00005-1-2 loss: 0.595083  [   96/  146]
train() client id: f_00005-1-3 loss: 0.574815  [  128/  146]
train() client id: f_00005-2-0 loss: 0.691786  [   32/  146]
train() client id: f_00005-2-1 loss: 0.808148  [   64/  146]
train() client id: f_00005-2-2 loss: 0.728818  [   96/  146]
train() client id: f_00005-2-3 loss: 0.409707  [  128/  146]
train() client id: f_00005-3-0 loss: 0.924278  [   32/  146]
train() client id: f_00005-3-1 loss: 0.467846  [   64/  146]
train() client id: f_00005-3-2 loss: 0.701889  [   96/  146]
train() client id: f_00005-3-3 loss: 0.657320  [  128/  146]
train() client id: f_00005-4-0 loss: 0.431365  [   32/  146]
train() client id: f_00005-4-1 loss: 0.921693  [   64/  146]
train() client id: f_00005-4-2 loss: 0.843483  [   96/  146]
train() client id: f_00005-4-3 loss: 0.429690  [  128/  146]
train() client id: f_00005-5-0 loss: 0.628282  [   32/  146]
train() client id: f_00005-5-1 loss: 0.994544  [   64/  146]
train() client id: f_00005-5-2 loss: 0.541285  [   96/  146]
train() client id: f_00005-5-3 loss: 0.375387  [  128/  146]
train() client id: f_00005-6-0 loss: 0.546034  [   32/  146]
train() client id: f_00005-6-1 loss: 0.680531  [   64/  146]
train() client id: f_00005-6-2 loss: 0.719544  [   96/  146]
train() client id: f_00005-6-3 loss: 0.598197  [  128/  146]
train() client id: f_00005-7-0 loss: 0.504503  [   32/  146]
train() client id: f_00005-7-1 loss: 0.603870  [   64/  146]
train() client id: f_00005-7-2 loss: 0.877890  [   96/  146]
train() client id: f_00005-7-3 loss: 0.657346  [  128/  146]
train() client id: f_00005-8-0 loss: 0.679748  [   32/  146]
train() client id: f_00005-8-1 loss: 0.829966  [   64/  146]
train() client id: f_00005-8-2 loss: 0.462485  [   96/  146]
train() client id: f_00005-8-3 loss: 0.601511  [  128/  146]
train() client id: f_00005-9-0 loss: 0.805087  [   32/  146]
train() client id: f_00005-9-1 loss: 0.595497  [   64/  146]
train() client id: f_00005-9-2 loss: 0.635800  [   96/  146]
train() client id: f_00005-9-3 loss: 0.626947  [  128/  146]
train() client id: f_00005-10-0 loss: 0.496831  [   32/  146]
train() client id: f_00005-10-1 loss: 0.672074  [   64/  146]
train() client id: f_00005-10-2 loss: 0.616464  [   96/  146]
train() client id: f_00005-10-3 loss: 0.858120  [  128/  146]
train() client id: f_00006-0-0 loss: 0.370005  [   32/   54]
train() client id: f_00006-1-0 loss: 0.472350  [   32/   54]
train() client id: f_00006-2-0 loss: 0.415355  [   32/   54]
train() client id: f_00006-3-0 loss: 0.434347  [   32/   54]
train() client id: f_00006-4-0 loss: 0.422548  [   32/   54]
train() client id: f_00006-5-0 loss: 0.445000  [   32/   54]
train() client id: f_00006-6-0 loss: 0.383785  [   32/   54]
train() client id: f_00006-7-0 loss: 0.375285  [   32/   54]
train() client id: f_00006-8-0 loss: 0.473900  [   32/   54]
train() client id: f_00006-9-0 loss: 0.381181  [   32/   54]
train() client id: f_00006-10-0 loss: 0.466129  [   32/   54]
train() client id: f_00007-0-0 loss: 0.468237  [   32/  179]
train() client id: f_00007-0-1 loss: 0.496505  [   64/  179]
train() client id: f_00007-0-2 loss: 0.607413  [   96/  179]
train() client id: f_00007-0-3 loss: 0.708491  [  128/  179]
train() client id: f_00007-0-4 loss: 0.522302  [  160/  179]
train() client id: f_00007-1-0 loss: 0.905614  [   32/  179]
train() client id: f_00007-1-1 loss: 0.633585  [   64/  179]
train() client id: f_00007-1-2 loss: 0.509399  [   96/  179]
train() client id: f_00007-1-3 loss: 0.496401  [  128/  179]
train() client id: f_00007-1-4 loss: 0.467559  [  160/  179]
train() client id: f_00007-2-0 loss: 0.519472  [   32/  179]
train() client id: f_00007-2-1 loss: 0.708700  [   64/  179]
train() client id: f_00007-2-2 loss: 0.473146  [   96/  179]
train() client id: f_00007-2-3 loss: 0.641689  [  128/  179]
train() client id: f_00007-2-4 loss: 0.408626  [  160/  179]
train() client id: f_00007-3-0 loss: 0.634938  [   32/  179]
train() client id: f_00007-3-1 loss: 0.692723  [   64/  179]
train() client id: f_00007-3-2 loss: 0.433226  [   96/  179]
train() client id: f_00007-3-3 loss: 0.572100  [  128/  179]
train() client id: f_00007-3-4 loss: 0.456975  [  160/  179]
train() client id: f_00007-4-0 loss: 0.489163  [   32/  179]
train() client id: f_00007-4-1 loss: 0.394634  [   64/  179]
train() client id: f_00007-4-2 loss: 0.599387  [   96/  179]
train() client id: f_00007-4-3 loss: 0.901806  [  128/  179]
train() client id: f_00007-4-4 loss: 0.426311  [  160/  179]
train() client id: f_00007-5-0 loss: 0.482777  [   32/  179]
train() client id: f_00007-5-1 loss: 0.585716  [   64/  179]
train() client id: f_00007-5-2 loss: 0.577689  [   96/  179]
train() client id: f_00007-5-3 loss: 0.388989  [  128/  179]
train() client id: f_00007-5-4 loss: 0.441851  [  160/  179]
train() client id: f_00007-6-0 loss: 0.385314  [   32/  179]
train() client id: f_00007-6-1 loss: 0.393816  [   64/  179]
train() client id: f_00007-6-2 loss: 0.619153  [   96/  179]
train() client id: f_00007-6-3 loss: 0.475946  [  128/  179]
train() client id: f_00007-6-4 loss: 0.607238  [  160/  179]
train() client id: f_00007-7-0 loss: 0.594313  [   32/  179]
train() client id: f_00007-7-1 loss: 0.363162  [   64/  179]
train() client id: f_00007-7-2 loss: 0.515123  [   96/  179]
train() client id: f_00007-7-3 loss: 0.462490  [  128/  179]
train() client id: f_00007-7-4 loss: 0.517927  [  160/  179]
train() client id: f_00007-8-0 loss: 0.409594  [   32/  179]
train() client id: f_00007-8-1 loss: 0.697016  [   64/  179]
train() client id: f_00007-8-2 loss: 0.469372  [   96/  179]
train() client id: f_00007-8-3 loss: 0.455151  [  128/  179]
train() client id: f_00007-8-4 loss: 0.559304  [  160/  179]
train() client id: f_00007-9-0 loss: 0.566831  [   32/  179]
train() client id: f_00007-9-1 loss: 0.367144  [   64/  179]
train() client id: f_00007-9-2 loss: 0.554257  [   96/  179]
train() client id: f_00007-9-3 loss: 0.665147  [  128/  179]
train() client id: f_00007-9-4 loss: 0.592327  [  160/  179]
train() client id: f_00007-10-0 loss: 0.576811  [   32/  179]
train() client id: f_00007-10-1 loss: 0.381958  [   64/  179]
train() client id: f_00007-10-2 loss: 0.567125  [   96/  179]
train() client id: f_00007-10-3 loss: 0.588896  [  128/  179]
train() client id: f_00007-10-4 loss: 0.425662  [  160/  179]
train() client id: f_00008-0-0 loss: 0.747549  [   32/  130]
train() client id: f_00008-0-1 loss: 0.726195  [   64/  130]
train() client id: f_00008-0-2 loss: 0.712055  [   96/  130]
train() client id: f_00008-0-3 loss: 0.887494  [  128/  130]
train() client id: f_00008-1-0 loss: 0.742879  [   32/  130]
train() client id: f_00008-1-1 loss: 0.794112  [   64/  130]
train() client id: f_00008-1-2 loss: 0.766038  [   96/  130]
train() client id: f_00008-1-3 loss: 0.741113  [  128/  130]
train() client id: f_00008-2-0 loss: 0.704487  [   32/  130]
train() client id: f_00008-2-1 loss: 0.836068  [   64/  130]
train() client id: f_00008-2-2 loss: 0.783188  [   96/  130]
train() client id: f_00008-2-3 loss: 0.763144  [  128/  130]
train() client id: f_00008-3-0 loss: 0.694477  [   32/  130]
train() client id: f_00008-3-1 loss: 0.702497  [   64/  130]
train() client id: f_00008-3-2 loss: 0.858336  [   96/  130]
train() client id: f_00008-3-3 loss: 0.826765  [  128/  130]
train() client id: f_00008-4-0 loss: 0.772863  [   32/  130]
train() client id: f_00008-4-1 loss: 0.821421  [   64/  130]
train() client id: f_00008-4-2 loss: 0.744483  [   96/  130]
train() client id: f_00008-4-3 loss: 0.743465  [  128/  130]
train() client id: f_00008-5-0 loss: 0.745822  [   32/  130]
train() client id: f_00008-5-1 loss: 0.712949  [   64/  130]
train() client id: f_00008-5-2 loss: 0.757037  [   96/  130]
train() client id: f_00008-5-3 loss: 0.846018  [  128/  130]
train() client id: f_00008-6-0 loss: 0.756294  [   32/  130]
train() client id: f_00008-6-1 loss: 0.781867  [   64/  130]
train() client id: f_00008-6-2 loss: 0.769420  [   96/  130]
train() client id: f_00008-6-3 loss: 0.756013  [  128/  130]
train() client id: f_00008-7-0 loss: 0.678484  [   32/  130]
train() client id: f_00008-7-1 loss: 0.787052  [   64/  130]
train() client id: f_00008-7-2 loss: 0.834085  [   96/  130]
train() client id: f_00008-7-3 loss: 0.775816  [  128/  130]
train() client id: f_00008-8-0 loss: 0.828112  [   32/  130]
train() client id: f_00008-8-1 loss: 0.760223  [   64/  130]
train() client id: f_00008-8-2 loss: 0.673366  [   96/  130]
train() client id: f_00008-8-3 loss: 0.799555  [  128/  130]
train() client id: f_00008-9-0 loss: 0.787998  [   32/  130]
train() client id: f_00008-9-1 loss: 0.737118  [   64/  130]
train() client id: f_00008-9-2 loss: 0.760234  [   96/  130]
train() client id: f_00008-9-3 loss: 0.749160  [  128/  130]
train() client id: f_00008-10-0 loss: 0.754209  [   32/  130]
train() client id: f_00008-10-1 loss: 0.893256  [   64/  130]
train() client id: f_00008-10-2 loss: 0.665562  [   96/  130]
train() client id: f_00008-10-3 loss: 0.767679  [  128/  130]
train() client id: f_00009-0-0 loss: 0.812462  [   32/  118]
train() client id: f_00009-0-1 loss: 0.976203  [   64/  118]
train() client id: f_00009-0-2 loss: 0.881707  [   96/  118]
train() client id: f_00009-1-0 loss: 0.740622  [   32/  118]
train() client id: f_00009-1-1 loss: 0.855406  [   64/  118]
train() client id: f_00009-1-2 loss: 0.976890  [   96/  118]
train() client id: f_00009-2-0 loss: 0.836604  [   32/  118]
train() client id: f_00009-2-1 loss: 0.849869  [   64/  118]
train() client id: f_00009-2-2 loss: 0.879984  [   96/  118]
train() client id: f_00009-3-0 loss: 0.951155  [   32/  118]
train() client id: f_00009-3-1 loss: 0.873507  [   64/  118]
train() client id: f_00009-3-2 loss: 0.759463  [   96/  118]
train() client id: f_00009-4-0 loss: 0.784632  [   32/  118]
train() client id: f_00009-4-1 loss: 0.936536  [   64/  118]
train() client id: f_00009-4-2 loss: 0.781120  [   96/  118]
train() client id: f_00009-5-0 loss: 0.728568  [   32/  118]
train() client id: f_00009-5-1 loss: 0.835706  [   64/  118]
train() client id: f_00009-5-2 loss: 0.784438  [   96/  118]
train() client id: f_00009-6-0 loss: 0.717035  [   32/  118]
train() client id: f_00009-6-1 loss: 0.762048  [   64/  118]
train() client id: f_00009-6-2 loss: 0.883921  [   96/  118]
train() client id: f_00009-7-0 loss: 0.807836  [   32/  118]
train() client id: f_00009-7-1 loss: 0.790825  [   64/  118]
train() client id: f_00009-7-2 loss: 0.742218  [   96/  118]
train() client id: f_00009-8-0 loss: 0.626855  [   32/  118]
train() client id: f_00009-8-1 loss: 0.700745  [   64/  118]
train() client id: f_00009-8-2 loss: 0.852327  [   96/  118]
train() client id: f_00009-9-0 loss: 0.753858  [   32/  118]
train() client id: f_00009-9-1 loss: 0.732054  [   64/  118]
train() client id: f_00009-9-2 loss: 0.692561  [   96/  118]
train() client id: f_00009-10-0 loss: 0.758954  [   32/  118]
train() client id: f_00009-10-1 loss: 0.751453  [   64/  118]
train() client id: f_00009-10-2 loss: 0.716642  [   96/  118]
At round 59 accuracy: 0.6472148541114059
At round 59 training accuracy: 0.5875251509054326
At round 59 training loss: 0.8301549731934701
update_location
xs = -3.905658 4.200318 315.009024 18.811294 0.979296 3.956410 -277.443192 -256.324852 299.663977 -242.060879 
ys = 307.587959 290.555839 1.320614 -277.455176 269.350187 252.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -13.71142616811743
ys mean: 86.39414253552872
dists_uav = 323.458818 307.311468 330.503297 295.525362 287.315997 271.901909 294.926457 275.141974 316.397170 261.934115 
uav_gains = -117.529547 -116.325662 -118.005067 -115.352469 -114.636400 -113.243561 -115.301164 -113.538715 -117.022510 -112.340901 
uav_gains_db_mean: -115.32959963376159
dists_bs = 216.455170 213.227357 519.878229 492.217944 199.676414 195.142099 205.049366 192.227953 500.090401 183.678128 
bs_gains = -104.957362 -104.774661 -115.612305 -114.947468 -103.976209 -103.696886 -104.299095 -103.513922 -115.140418 -102.960668 
bs_gains_db_mean: -107.38789932880141
Round 60
-------------------------------
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.0875637  6.22007234 3.04984448 1.12636964 7.17067363 3.45091238
 1.38142813 4.26835796 3.14602737 2.7981042 ]
obj_prev = 35.69935382138897
eta_min = 9.518668131305417e-31	eta_max = 0.9381641580347129
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 8.195255617037894	eta = 0.909090909090909
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 19.17140763760524	eta = 0.38861165126506014
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 13.206898476870052	eta = 0.5641167297661407
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.154598615650826	eta = 0.6129558543819038
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.090043572188366	eta = 0.6162287451356823
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.089768790589424	eta = 0.6162427510544748
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.089768785575135	eta = 0.6162427513100645
eta = 0.6162427513100645
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [0.03994795 0.08401752 0.03931386 0.01363303 0.09701646 0.0462889
 0.01712055 0.05675146 0.04121614 0.03741158]
ene_total = [1.20410292 1.8265834  1.21411492 0.59841362 2.07887597 1.06827008
 0.66615118 1.4063269  1.14028819 0.8866416 ]
ti_comp = [1.15185544 1.27946326 1.14047878 1.19334864 1.28255751 1.28358448
 1.19415401 1.21817826 1.19663411 1.28616459]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [3.00308521e-06 2.26429994e-05 2.91972948e-06 1.11204574e-07
 3.46946537e-05 3.76237107e-06 2.19944230e-07 7.69820333e-06
 3.05604171e-06 1.97835807e-06]
ene_total = [0.40991842 0.15659935 0.43253582 0.32736424 0.15068698 0.14803017
 0.32576518 0.27814891 0.32089064 0.14286493]
optimize_network iter = 0 obj = 2.6928046428038024
eta = 0.6162427513100645
freqs = [17340697.3279414  32833111.65400368 17235681.79847541  5712090.78528441
 37821484.27558679 18031106.89702021  7168485.47995044 23293576.56497024
 17221697.8440477  14543852.06878811]
eta_min = 0.6162427513100649	eta_max = 0.7177903732762834
af = 0.0012621994794922648	bf = 1.0327722521438556	zeta = 0.0013884194274414914	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [5.91233479e-07 4.45784864e-06 5.74822789e-07 2.18934405e-08
 6.83052241e-06 7.40718154e-07 4.33015993e-08 1.51558655e-06
 6.01659308e-07 3.89489955e-07]
ene_total = [1.73019841 0.65953123 1.82567951 1.3819049  0.63376085 0.62463054
 1.37514743 1.17363935 1.35437921 0.60294663]
ti_comp = [0.79250899 0.92011681 0.78113233 0.8340022  0.92321106 0.92423803
 0.83480756 0.85883181 0.83728766 0.92681814]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [2.03829682e-06 1.40674660e-05 1.99976986e-06 7.31535092e-08
 2.15142627e-05 2.33160556e-06 1.44601475e-07 4.97630551e-06
 2.00560251e-06 1.22411200e-06]
ene_total = [0.55739348 0.21271672 0.58815051 0.44515884 0.2045524  0.20125725
 0.44298339 0.37816177 0.43632847 0.19425169]
optimize_network iter = 1 obj = 3.6609545155387906
eta = 0.7177903732762834
freqs = [17262263.50383285 31270462.24446457 17235681.79847541  5597996.92086191
 35987511.86898271 17151433.11384352  7023260.57451295 22629570.05842017
 16857767.37652736 13823527.31845897]
eta_min = 0.7177903732762844	eta_max = 0.7177903732762764
af = 0.001159170611255296	bf = 1.0327722521438556	zeta = 0.0012750876723808255	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [5.85897150e-07 4.04361532e-06 5.74822789e-07 2.10275717e-08
 6.18415585e-06 6.70207126e-07 4.15648943e-08 1.43041150e-06
 5.76499351e-07 3.51864226e-07]
ene_total = [1.73019796 0.65949647 1.82567951 1.38190483 0.6337066  0.62462463
 1.37514729 1.1736322  1.3543771  0.60294347]
ti_comp = [0.79250899 0.92011681 0.78113233 0.8340022  0.92321106 0.92423803
 0.83480756 0.85883181 0.83728766 0.92681814]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [2.03829682e-06 1.40674660e-05 1.99976986e-06 7.31535092e-08
 2.15142627e-05 2.33160556e-06 1.44601475e-07 4.97630551e-06
 2.00560251e-06 1.22411200e-06]
ene_total = [0.55739348 0.21271672 0.58815051 0.44515884 0.2045524  0.20125725
 0.44298339 0.37816177 0.43632847 0.19425169]
optimize_network iter = 2 obj = 3.6609545155387
eta = 0.7177903732762764
freqs = [17262263.50383282 31270462.24446465 17235681.79847537  5597996.92086191
 35987511.8689828  17151433.11384356  7023260.57451294 22629570.05842018
 16857767.37652736 13823527.31845901]
Done!
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [1.87731017e-06 1.29564040e-05 1.84182611e-06 6.73757745e-08
 1.98150455e-05 2.14745310e-06 1.33180711e-07 4.58327210e-06
 1.84719809e-06 1.12743045e-06]
ene_total = [0.02061646 0.00786676 0.0217541  0.01646533 0.0075642  0.00744383
 0.01638486 0.01398689 0.01613857 0.0071848 ]
At round 60 energy consumption: 0.135405804121047
At round 60 eta: 0.7177903732762764
At round 60 a_n: 7.62985203809534
At round 60 local rounds: 10.857538900724403
At round 60 global rounds: 27.03611541063686
gradient difference: 0.5311607122421265
train() client id: f_00000-0-0 loss: 1.128832  [   32/  126]
train() client id: f_00000-0-1 loss: 1.263136  [   64/  126]
train() client id: f_00000-0-2 loss: 1.213636  [   96/  126]
train() client id: f_00000-1-0 loss: 1.294647  [   32/  126]
train() client id: f_00000-1-1 loss: 0.802261  [   64/  126]
train() client id: f_00000-1-2 loss: 1.339914  [   96/  126]
train() client id: f_00000-2-0 loss: 1.164493  [   32/  126]
train() client id: f_00000-2-1 loss: 1.073953  [   64/  126]
train() client id: f_00000-2-2 loss: 0.892133  [   96/  126]
train() client id: f_00000-3-0 loss: 0.942711  [   32/  126]
train() client id: f_00000-3-1 loss: 1.116378  [   64/  126]
train() client id: f_00000-3-2 loss: 0.993052  [   96/  126]
train() client id: f_00000-4-0 loss: 0.857810  [   32/  126]
train() client id: f_00000-4-1 loss: 0.966086  [   64/  126]
train() client id: f_00000-4-2 loss: 1.026448  [   96/  126]
train() client id: f_00000-5-0 loss: 1.011125  [   32/  126]
train() client id: f_00000-5-1 loss: 0.852352  [   64/  126]
train() client id: f_00000-5-2 loss: 0.948080  [   96/  126]
train() client id: f_00000-6-0 loss: 0.918696  [   32/  126]
train() client id: f_00000-6-1 loss: 0.935341  [   64/  126]
train() client id: f_00000-6-2 loss: 0.779881  [   96/  126]
train() client id: f_00000-7-0 loss: 0.974794  [   32/  126]
train() client id: f_00000-7-1 loss: 0.867346  [   64/  126]
train() client id: f_00000-7-2 loss: 0.824559  [   96/  126]
train() client id: f_00000-8-0 loss: 0.813320  [   32/  126]
train() client id: f_00000-8-1 loss: 0.917452  [   64/  126]
train() client id: f_00000-8-2 loss: 0.843436  [   96/  126]
train() client id: f_00000-9-0 loss: 1.004681  [   32/  126]
train() client id: f_00000-9-1 loss: 0.814914  [   64/  126]
train() client id: f_00000-9-2 loss: 0.896372  [   96/  126]
train() client id: f_00001-0-0 loss: 0.465775  [   32/  265]
train() client id: f_00001-0-1 loss: 0.462415  [   64/  265]
train() client id: f_00001-0-2 loss: 0.525752  [   96/  265]
train() client id: f_00001-0-3 loss: 0.672664  [  128/  265]
train() client id: f_00001-0-4 loss: 0.434113  [  160/  265]
train() client id: f_00001-0-5 loss: 0.449126  [  192/  265]
train() client id: f_00001-0-6 loss: 0.446082  [  224/  265]
train() client id: f_00001-0-7 loss: 0.528946  [  256/  265]
train() client id: f_00001-1-0 loss: 0.524566  [   32/  265]
train() client id: f_00001-1-1 loss: 0.517427  [   64/  265]
train() client id: f_00001-1-2 loss: 0.432631  [   96/  265]
train() client id: f_00001-1-3 loss: 0.474726  [  128/  265]
train() client id: f_00001-1-4 loss: 0.468342  [  160/  265]
train() client id: f_00001-1-5 loss: 0.465190  [  192/  265]
train() client id: f_00001-1-6 loss: 0.389012  [  224/  265]
train() client id: f_00001-1-7 loss: 0.660850  [  256/  265]
train() client id: f_00001-2-0 loss: 0.447154  [   32/  265]
train() client id: f_00001-2-1 loss: 0.453075  [   64/  265]
train() client id: f_00001-2-2 loss: 0.527792  [   96/  265]
train() client id: f_00001-2-3 loss: 0.539958  [  128/  265]
train() client id: f_00001-2-4 loss: 0.566938  [  160/  265]
train() client id: f_00001-2-5 loss: 0.449554  [  192/  265]
train() client id: f_00001-2-6 loss: 0.447172  [  224/  265]
train() client id: f_00001-2-7 loss: 0.403665  [  256/  265]
train() client id: f_00001-3-0 loss: 0.440348  [   32/  265]
train() client id: f_00001-3-1 loss: 0.401013  [   64/  265]
train() client id: f_00001-3-2 loss: 0.467137  [   96/  265]
train() client id: f_00001-3-3 loss: 0.467023  [  128/  265]
train() client id: f_00001-3-4 loss: 0.644967  [  160/  265]
train() client id: f_00001-3-5 loss: 0.501040  [  192/  265]
train() client id: f_00001-3-6 loss: 0.520500  [  224/  265]
train() client id: f_00001-3-7 loss: 0.423402  [  256/  265]
train() client id: f_00001-4-0 loss: 0.439158  [   32/  265]
train() client id: f_00001-4-1 loss: 0.485874  [   64/  265]
train() client id: f_00001-4-2 loss: 0.514282  [   96/  265]
train() client id: f_00001-4-3 loss: 0.524994  [  128/  265]
train() client id: f_00001-4-4 loss: 0.490982  [  160/  265]
train() client id: f_00001-4-5 loss: 0.459904  [  192/  265]
train() client id: f_00001-4-6 loss: 0.402719  [  224/  265]
train() client id: f_00001-4-7 loss: 0.459467  [  256/  265]
train() client id: f_00001-5-0 loss: 0.403672  [   32/  265]
train() client id: f_00001-5-1 loss: 0.548465  [   64/  265]
train() client id: f_00001-5-2 loss: 0.423333  [   96/  265]
train() client id: f_00001-5-3 loss: 0.477026  [  128/  265]
train() client id: f_00001-5-4 loss: 0.385296  [  160/  265]
train() client id: f_00001-5-5 loss: 0.460891  [  192/  265]
train() client id: f_00001-5-6 loss: 0.511788  [  224/  265]
train() client id: f_00001-5-7 loss: 0.542035  [  256/  265]
train() client id: f_00001-6-0 loss: 0.389968  [   32/  265]
train() client id: f_00001-6-1 loss: 0.469927  [   64/  265]
train() client id: f_00001-6-2 loss: 0.524148  [   96/  265]
train() client id: f_00001-6-3 loss: 0.542628  [  128/  265]
train() client id: f_00001-6-4 loss: 0.522318  [  160/  265]
train() client id: f_00001-6-5 loss: 0.543680  [  192/  265]
train() client id: f_00001-6-6 loss: 0.445898  [  224/  265]
train() client id: f_00001-6-7 loss: 0.387134  [  256/  265]
train() client id: f_00001-7-0 loss: 0.414667  [   32/  265]
train() client id: f_00001-7-1 loss: 0.450301  [   64/  265]
train() client id: f_00001-7-2 loss: 0.458679  [   96/  265]
train() client id: f_00001-7-3 loss: 0.546809  [  128/  265]
train() client id: f_00001-7-4 loss: 0.384353  [  160/  265]
train() client id: f_00001-7-5 loss: 0.384787  [  192/  265]
train() client id: f_00001-7-6 loss: 0.507376  [  224/  265]
train() client id: f_00001-7-7 loss: 0.620965  [  256/  265]
train() client id: f_00001-8-0 loss: 0.400373  [   32/  265]
train() client id: f_00001-8-1 loss: 0.349808  [   64/  265]
train() client id: f_00001-8-2 loss: 0.530741  [   96/  265]
train() client id: f_00001-8-3 loss: 0.522034  [  128/  265]
train() client id: f_00001-8-4 loss: 0.436691  [  160/  265]
train() client id: f_00001-8-5 loss: 0.473271  [  192/  265]
train() client id: f_00001-8-6 loss: 0.601749  [  224/  265]
train() client id: f_00001-8-7 loss: 0.483947  [  256/  265]
train() client id: f_00001-9-0 loss: 0.530900  [   32/  265]
train() client id: f_00001-9-1 loss: 0.583161  [   64/  265]
train() client id: f_00001-9-2 loss: 0.441705  [   96/  265]
train() client id: f_00001-9-3 loss: 0.469692  [  128/  265]
train() client id: f_00001-9-4 loss: 0.428448  [  160/  265]
train() client id: f_00001-9-5 loss: 0.366159  [  192/  265]
train() client id: f_00001-9-6 loss: 0.506786  [  224/  265]
train() client id: f_00001-9-7 loss: 0.476164  [  256/  265]
train() client id: f_00002-0-0 loss: 1.075103  [   32/  124]
train() client id: f_00002-0-1 loss: 1.175174  [   64/  124]
train() client id: f_00002-0-2 loss: 1.009020  [   96/  124]
train() client id: f_00002-1-0 loss: 1.071558  [   32/  124]
train() client id: f_00002-1-1 loss: 1.149063  [   64/  124]
train() client id: f_00002-1-2 loss: 0.952713  [   96/  124]
train() client id: f_00002-2-0 loss: 0.893272  [   32/  124]
train() client id: f_00002-2-1 loss: 0.872076  [   64/  124]
train() client id: f_00002-2-2 loss: 1.332330  [   96/  124]
train() client id: f_00002-3-0 loss: 1.143665  [   32/  124]
train() client id: f_00002-3-1 loss: 0.979591  [   64/  124]
train() client id: f_00002-3-2 loss: 1.005143  [   96/  124]
train() client id: f_00002-4-0 loss: 0.940436  [   32/  124]
train() client id: f_00002-4-1 loss: 1.167752  [   64/  124]
train() client id: f_00002-4-2 loss: 0.917415  [   96/  124]
train() client id: f_00002-5-0 loss: 0.947992  [   32/  124]
train() client id: f_00002-5-1 loss: 1.298067  [   64/  124]
train() client id: f_00002-5-2 loss: 0.943860  [   96/  124]
train() client id: f_00002-6-0 loss: 0.996020  [   32/  124]
train() client id: f_00002-6-1 loss: 0.944790  [   64/  124]
train() client id: f_00002-6-2 loss: 1.042811  [   96/  124]
train() client id: f_00002-7-0 loss: 1.014107  [   32/  124]
train() client id: f_00002-7-1 loss: 0.963448  [   64/  124]
train() client id: f_00002-7-2 loss: 0.969625  [   96/  124]
train() client id: f_00002-8-0 loss: 0.993321  [   32/  124]
train() client id: f_00002-8-1 loss: 0.954671  [   64/  124]
train() client id: f_00002-8-2 loss: 0.937196  [   96/  124]
train() client id: f_00002-9-0 loss: 0.807166  [   32/  124]
train() client id: f_00002-9-1 loss: 1.144573  [   64/  124]
train() client id: f_00002-9-2 loss: 1.062658  [   96/  124]
train() client id: f_00003-0-0 loss: 0.901256  [   32/   43]
train() client id: f_00003-1-0 loss: 0.513093  [   32/   43]
train() client id: f_00003-2-0 loss: 0.746437  [   32/   43]
train() client id: f_00003-3-0 loss: 0.710474  [   32/   43]
train() client id: f_00003-4-0 loss: 0.626495  [   32/   43]
train() client id: f_00003-5-0 loss: 0.679771  [   32/   43]
train() client id: f_00003-6-0 loss: 0.618102  [   32/   43]
train() client id: f_00003-7-0 loss: 0.735466  [   32/   43]
train() client id: f_00003-8-0 loss: 0.803314  [   32/   43]
train() client id: f_00003-9-0 loss: 0.834995  [   32/   43]
train() client id: f_00004-0-0 loss: 0.815923  [   32/  306]
train() client id: f_00004-0-1 loss: 1.053158  [   64/  306]
train() client id: f_00004-0-2 loss: 1.081119  [   96/  306]
train() client id: f_00004-0-3 loss: 0.962634  [  128/  306]
train() client id: f_00004-0-4 loss: 0.832335  [  160/  306]
train() client id: f_00004-0-5 loss: 0.857843  [  192/  306]
train() client id: f_00004-0-6 loss: 1.045004  [  224/  306]
train() client id: f_00004-0-7 loss: 0.959802  [  256/  306]
train() client id: f_00004-0-8 loss: 0.784178  [  288/  306]
train() client id: f_00004-1-0 loss: 0.994406  [   32/  306]
train() client id: f_00004-1-1 loss: 0.955758  [   64/  306]
train() client id: f_00004-1-2 loss: 0.991853  [   96/  306]
train() client id: f_00004-1-3 loss: 1.103465  [  128/  306]
train() client id: f_00004-1-4 loss: 0.796129  [  160/  306]
train() client id: f_00004-1-5 loss: 0.896069  [  192/  306]
train() client id: f_00004-1-6 loss: 0.862487  [  224/  306]
train() client id: f_00004-1-7 loss: 0.844238  [  256/  306]
train() client id: f_00004-1-8 loss: 0.870395  [  288/  306]
train() client id: f_00004-2-0 loss: 0.990526  [   32/  306]
train() client id: f_00004-2-1 loss: 0.913590  [   64/  306]
train() client id: f_00004-2-2 loss: 0.938976  [   96/  306]
train() client id: f_00004-2-3 loss: 0.919309  [  128/  306]
train() client id: f_00004-2-4 loss: 0.997203  [  160/  306]
train() client id: f_00004-2-5 loss: 0.962392  [  192/  306]
train() client id: f_00004-2-6 loss: 0.883925  [  224/  306]
train() client id: f_00004-2-7 loss: 0.938241  [  256/  306]
train() client id: f_00004-2-8 loss: 0.803893  [  288/  306]
train() client id: f_00004-3-0 loss: 0.825505  [   32/  306]
train() client id: f_00004-3-1 loss: 0.921210  [   64/  306]
train() client id: f_00004-3-2 loss: 1.042814  [   96/  306]
train() client id: f_00004-3-3 loss: 0.888951  [  128/  306]
train() client id: f_00004-3-4 loss: 0.862668  [  160/  306]
train() client id: f_00004-3-5 loss: 0.803752  [  192/  306]
train() client id: f_00004-3-6 loss: 0.835809  [  224/  306]
train() client id: f_00004-3-7 loss: 1.035064  [  256/  306]
train() client id: f_00004-3-8 loss: 1.032760  [  288/  306]
train() client id: f_00004-4-0 loss: 0.965331  [   32/  306]
train() client id: f_00004-4-1 loss: 0.865055  [   64/  306]
train() client id: f_00004-4-2 loss: 0.981599  [   96/  306]
train() client id: f_00004-4-3 loss: 0.917484  [  128/  306]
train() client id: f_00004-4-4 loss: 0.948254  [  160/  306]
train() client id: f_00004-4-5 loss: 0.947347  [  192/  306]
train() client id: f_00004-4-6 loss: 0.877752  [  224/  306]
train() client id: f_00004-4-7 loss: 0.888886  [  256/  306]
train() client id: f_00004-4-8 loss: 0.914249  [  288/  306]
train() client id: f_00004-5-0 loss: 0.930704  [   32/  306]
train() client id: f_00004-5-1 loss: 0.863344  [   64/  306]
train() client id: f_00004-5-2 loss: 0.872148  [   96/  306]
train() client id: f_00004-5-3 loss: 0.832268  [  128/  306]
train() client id: f_00004-5-4 loss: 0.954478  [  160/  306]
train() client id: f_00004-5-5 loss: 1.017948  [  192/  306]
train() client id: f_00004-5-6 loss: 0.921059  [  224/  306]
train() client id: f_00004-5-7 loss: 0.848717  [  256/  306]
train() client id: f_00004-5-8 loss: 0.969850  [  288/  306]
train() client id: f_00004-6-0 loss: 0.963893  [   32/  306]
train() client id: f_00004-6-1 loss: 0.921147  [   64/  306]
train() client id: f_00004-6-2 loss: 0.901796  [   96/  306]
train() client id: f_00004-6-3 loss: 0.811689  [  128/  306]
train() client id: f_00004-6-4 loss: 0.975937  [  160/  306]
train() client id: f_00004-6-5 loss: 0.923110  [  192/  306]
train() client id: f_00004-6-6 loss: 0.818122  [  224/  306]
train() client id: f_00004-6-7 loss: 0.943503  [  256/  306]
train() client id: f_00004-6-8 loss: 0.980400  [  288/  306]
train() client id: f_00004-7-0 loss: 0.925476  [   32/  306]
train() client id: f_00004-7-1 loss: 0.817598  [   64/  306]
train() client id: f_00004-7-2 loss: 0.817195  [   96/  306]
train() client id: f_00004-7-3 loss: 0.878291  [  128/  306]
train() client id: f_00004-7-4 loss: 0.834236  [  160/  306]
train() client id: f_00004-7-5 loss: 0.945661  [  192/  306]
train() client id: f_00004-7-6 loss: 0.944919  [  224/  306]
train() client id: f_00004-7-7 loss: 0.848597  [  256/  306]
train() client id: f_00004-7-8 loss: 1.112851  [  288/  306]
train() client id: f_00004-8-0 loss: 0.936747  [   32/  306]
train() client id: f_00004-8-1 loss: 0.923814  [   64/  306]
train() client id: f_00004-8-2 loss: 0.936884  [   96/  306]
train() client id: f_00004-8-3 loss: 0.832006  [  128/  306]
train() client id: f_00004-8-4 loss: 0.939766  [  160/  306]
train() client id: f_00004-8-5 loss: 0.887147  [  192/  306]
train() client id: f_00004-8-6 loss: 0.920084  [  224/  306]
train() client id: f_00004-8-7 loss: 1.005860  [  256/  306]
train() client id: f_00004-8-8 loss: 0.877202  [  288/  306]
train() client id: f_00004-9-0 loss: 0.904576  [   32/  306]
train() client id: f_00004-9-1 loss: 0.857794  [   64/  306]
train() client id: f_00004-9-2 loss: 0.844962  [   96/  306]
train() client id: f_00004-9-3 loss: 0.851919  [  128/  306]
train() client id: f_00004-9-4 loss: 0.938774  [  160/  306]
train() client id: f_00004-9-5 loss: 0.945647  [  192/  306]
train() client id: f_00004-9-6 loss: 0.723502  [  224/  306]
train() client id: f_00004-9-7 loss: 0.959613  [  256/  306]
train() client id: f_00004-9-8 loss: 1.066544  [  288/  306]
train() client id: f_00005-0-0 loss: 0.592250  [   32/  146]
train() client id: f_00005-0-1 loss: 0.613591  [   64/  146]
train() client id: f_00005-0-2 loss: 0.309170  [   96/  146]
train() client id: f_00005-0-3 loss: 0.363442  [  128/  146]
train() client id: f_00005-1-0 loss: 0.318741  [   32/  146]
train() client id: f_00005-1-1 loss: 0.306578  [   64/  146]
train() client id: f_00005-1-2 loss: 0.683183  [   96/  146]
train() client id: f_00005-1-3 loss: 0.582844  [  128/  146]
train() client id: f_00005-2-0 loss: 0.346970  [   32/  146]
train() client id: f_00005-2-1 loss: 0.383154  [   64/  146]
train() client id: f_00005-2-2 loss: 0.326475  [   96/  146]
train() client id: f_00005-2-3 loss: 0.616910  [  128/  146]
train() client id: f_00005-3-0 loss: 0.331643  [   32/  146]
train() client id: f_00005-3-1 loss: 0.680381  [   64/  146]
train() client id: f_00005-3-2 loss: 0.285219  [   96/  146]
train() client id: f_00005-3-3 loss: 0.270000  [  128/  146]
train() client id: f_00005-4-0 loss: 0.553658  [   32/  146]
train() client id: f_00005-4-1 loss: 0.240846  [   64/  146]
train() client id: f_00005-4-2 loss: 0.472832  [   96/  146]
train() client id: f_00005-4-3 loss: 0.506286  [  128/  146]
train() client id: f_00005-5-0 loss: 0.397989  [   32/  146]
train() client id: f_00005-5-1 loss: 0.558000  [   64/  146]
train() client id: f_00005-5-2 loss: 0.458878  [   96/  146]
train() client id: f_00005-5-3 loss: 0.296962  [  128/  146]
train() client id: f_00005-6-0 loss: 0.519372  [   32/  146]
train() client id: f_00005-6-1 loss: 0.316131  [   64/  146]
train() client id: f_00005-6-2 loss: 0.464765  [   96/  146]
train() client id: f_00005-6-3 loss: 0.322265  [  128/  146]
train() client id: f_00005-7-0 loss: 0.359205  [   32/  146]
train() client id: f_00005-7-1 loss: 0.524058  [   64/  146]
train() client id: f_00005-7-2 loss: 0.115970  [   96/  146]
train() client id: f_00005-7-3 loss: 0.434920  [  128/  146]
train() client id: f_00005-8-0 loss: 0.370665  [   32/  146]
train() client id: f_00005-8-1 loss: 0.304249  [   64/  146]
train() client id: f_00005-8-2 loss: 0.743327  [   96/  146]
train() client id: f_00005-8-3 loss: 0.264464  [  128/  146]
train() client id: f_00005-9-0 loss: 0.639363  [   32/  146]
train() client id: f_00005-9-1 loss: 0.173007  [   64/  146]
train() client id: f_00005-9-2 loss: 0.616264  [   96/  146]
train() client id: f_00005-9-3 loss: 0.276439  [  128/  146]
train() client id: f_00006-0-0 loss: 0.464122  [   32/   54]
train() client id: f_00006-1-0 loss: 0.437670  [   32/   54]
train() client id: f_00006-2-0 loss: 0.482471  [   32/   54]
train() client id: f_00006-3-0 loss: 0.467301  [   32/   54]
train() client id: f_00006-4-0 loss: 0.435598  [   32/   54]
train() client id: f_00006-5-0 loss: 0.483043  [   32/   54]
train() client id: f_00006-6-0 loss: 0.439735  [   32/   54]
train() client id: f_00006-7-0 loss: 0.417996  [   32/   54]
train() client id: f_00006-8-0 loss: 0.443466  [   32/   54]
train() client id: f_00006-9-0 loss: 0.439280  [   32/   54]
train() client id: f_00007-0-0 loss: 0.847173  [   32/  179]
train() client id: f_00007-0-1 loss: 0.553086  [   64/  179]
train() client id: f_00007-0-2 loss: 0.773189  [   96/  179]
train() client id: f_00007-0-3 loss: 0.646496  [  128/  179]
train() client id: f_00007-0-4 loss: 0.522899  [  160/  179]
train() client id: f_00007-1-0 loss: 0.688814  [   32/  179]
train() client id: f_00007-1-1 loss: 0.651634  [   64/  179]
train() client id: f_00007-1-2 loss: 0.635502  [   96/  179]
train() client id: f_00007-1-3 loss: 0.741127  [  128/  179]
train() client id: f_00007-1-4 loss: 0.610131  [  160/  179]
train() client id: f_00007-2-0 loss: 0.609167  [   32/  179]
train() client id: f_00007-2-1 loss: 0.541236  [   64/  179]
train() client id: f_00007-2-2 loss: 0.808546  [   96/  179]
train() client id: f_00007-2-3 loss: 0.507961  [  128/  179]
train() client id: f_00007-2-4 loss: 0.730301  [  160/  179]
train() client id: f_00007-3-0 loss: 0.890756  [   32/  179]
train() client id: f_00007-3-1 loss: 0.578683  [   64/  179]
train() client id: f_00007-3-2 loss: 0.808852  [   96/  179]
train() client id: f_00007-3-3 loss: 0.528885  [  128/  179]
train() client id: f_00007-3-4 loss: 0.461991  [  160/  179]
train() client id: f_00007-4-0 loss: 0.629212  [   32/  179]
train() client id: f_00007-4-1 loss: 0.598080  [   64/  179]
train() client id: f_00007-4-2 loss: 0.495548  [   96/  179]
train() client id: f_00007-4-3 loss: 0.751959  [  128/  179]
train() client id: f_00007-4-4 loss: 0.659505  [  160/  179]
train() client id: f_00007-5-0 loss: 0.663132  [   32/  179]
train() client id: f_00007-5-1 loss: 0.508319  [   64/  179]
train() client id: f_00007-5-2 loss: 0.554641  [   96/  179]
train() client id: f_00007-5-3 loss: 0.766273  [  128/  179]
train() client id: f_00007-5-4 loss: 0.703570  [  160/  179]
train() client id: f_00007-6-0 loss: 0.582818  [   32/  179]
train() client id: f_00007-6-1 loss: 0.530414  [   64/  179]
train() client id: f_00007-6-2 loss: 0.887956  [   96/  179]
train() client id: f_00007-6-3 loss: 0.554367  [  128/  179]
train() client id: f_00007-6-4 loss: 0.621494  [  160/  179]
train() client id: f_00007-7-0 loss: 0.569468  [   32/  179]
train() client id: f_00007-7-1 loss: 0.685751  [   64/  179]
train() client id: f_00007-7-2 loss: 0.540069  [   96/  179]
train() client id: f_00007-7-3 loss: 0.667067  [  128/  179]
train() client id: f_00007-7-4 loss: 0.669958  [  160/  179]
train() client id: f_00007-8-0 loss: 0.500109  [   32/  179]
train() client id: f_00007-8-1 loss: 0.767153  [   64/  179]
train() client id: f_00007-8-2 loss: 0.896962  [   96/  179]
train() client id: f_00007-8-3 loss: 0.568258  [  128/  179]
train() client id: f_00007-8-4 loss: 0.484951  [  160/  179]
train() client id: f_00007-9-0 loss: 0.848781  [   32/  179]
train() client id: f_00007-9-1 loss: 0.463391  [   64/  179]
train() client id: f_00007-9-2 loss: 0.538421  [   96/  179]
train() client id: f_00007-9-3 loss: 0.679762  [  128/  179]
train() client id: f_00007-9-4 loss: 0.684090  [  160/  179]
train() client id: f_00008-0-0 loss: 0.698637  [   32/  130]
train() client id: f_00008-0-1 loss: 0.787989  [   64/  130]
train() client id: f_00008-0-2 loss: 0.711621  [   96/  130]
train() client id: f_00008-0-3 loss: 0.787172  [  128/  130]
train() client id: f_00008-1-0 loss: 0.741133  [   32/  130]
train() client id: f_00008-1-1 loss: 0.708088  [   64/  130]
train() client id: f_00008-1-2 loss: 0.821031  [   96/  130]
train() client id: f_00008-1-3 loss: 0.714649  [  128/  130]
train() client id: f_00008-2-0 loss: 0.721732  [   32/  130]
train() client id: f_00008-2-1 loss: 0.629227  [   64/  130]
train() client id: f_00008-2-2 loss: 0.780411  [   96/  130]
train() client id: f_00008-2-3 loss: 0.813744  [  128/  130]
train() client id: f_00008-3-0 loss: 0.701306  [   32/  130]
train() client id: f_00008-3-1 loss: 0.643881  [   64/  130]
train() client id: f_00008-3-2 loss: 0.828927  [   96/  130]
train() client id: f_00008-3-3 loss: 0.752740  [  128/  130]
train() client id: f_00008-4-0 loss: 0.677917  [   32/  130]
train() client id: f_00008-4-1 loss: 0.595083  [   64/  130]
train() client id: f_00008-4-2 loss: 0.692012  [   96/  130]
train() client id: f_00008-4-3 loss: 0.991405  [  128/  130]
train() client id: f_00008-5-0 loss: 0.650451  [   32/  130]
train() client id: f_00008-5-1 loss: 0.861203  [   64/  130]
train() client id: f_00008-5-2 loss: 0.758278  [   96/  130]
train() client id: f_00008-5-3 loss: 0.695418  [  128/  130]
train() client id: f_00008-6-0 loss: 0.819866  [   32/  130]
train() client id: f_00008-6-1 loss: 0.754421  [   64/  130]
train() client id: f_00008-6-2 loss: 0.645354  [   96/  130]
train() client id: f_00008-6-3 loss: 0.708078  [  128/  130]
train() client id: f_00008-7-0 loss: 0.652225  [   32/  130]
train() client id: f_00008-7-1 loss: 0.642269  [   64/  130]
train() client id: f_00008-7-2 loss: 0.880007  [   96/  130]
train() client id: f_00008-7-3 loss: 0.790036  [  128/  130]
train() client id: f_00008-8-0 loss: 0.748224  [   32/  130]
train() client id: f_00008-8-1 loss: 0.711188  [   64/  130]
train() client id: f_00008-8-2 loss: 0.776865  [   96/  130]
train() client id: f_00008-8-3 loss: 0.715715  [  128/  130]
train() client id: f_00008-9-0 loss: 0.711521  [   32/  130]
train() client id: f_00008-9-1 loss: 0.724448  [   64/  130]
train() client id: f_00008-9-2 loss: 0.797968  [   96/  130]
train() client id: f_00008-9-3 loss: 0.698046  [  128/  130]
train() client id: f_00009-0-0 loss: 0.817463  [   32/  118]
train() client id: f_00009-0-1 loss: 0.898013  [   64/  118]
train() client id: f_00009-0-2 loss: 1.004556  [   96/  118]
train() client id: f_00009-1-0 loss: 0.821982  [   32/  118]
train() client id: f_00009-1-1 loss: 0.973299  [   64/  118]
train() client id: f_00009-1-2 loss: 0.903642  [   96/  118]
train() client id: f_00009-2-0 loss: 0.784039  [   32/  118]
train() client id: f_00009-2-1 loss: 0.799703  [   64/  118]
train() client id: f_00009-2-2 loss: 0.931499  [   96/  118]
train() client id: f_00009-3-0 loss: 0.898020  [   32/  118]
train() client id: f_00009-3-1 loss: 0.788478  [   64/  118]
train() client id: f_00009-3-2 loss: 0.820491  [   96/  118]
train() client id: f_00009-4-0 loss: 0.948336  [   32/  118]
train() client id: f_00009-4-1 loss: 0.640907  [   64/  118]
train() client id: f_00009-4-2 loss: 0.694526  [   96/  118]
train() client id: f_00009-5-0 loss: 1.041220  [   32/  118]
train() client id: f_00009-5-1 loss: 0.803545  [   64/  118]
train() client id: f_00009-5-2 loss: 0.687751  [   96/  118]
train() client id: f_00009-6-0 loss: 0.814745  [   32/  118]
train() client id: f_00009-6-1 loss: 0.765204  [   64/  118]
train() client id: f_00009-6-2 loss: 0.860128  [   96/  118]
train() client id: f_00009-7-0 loss: 0.769194  [   32/  118]
train() client id: f_00009-7-1 loss: 0.823627  [   64/  118]
train() client id: f_00009-7-2 loss: 0.758675  [   96/  118]
train() client id: f_00009-8-0 loss: 0.757393  [   32/  118]
train() client id: f_00009-8-1 loss: 0.728624  [   64/  118]
train() client id: f_00009-8-2 loss: 0.857832  [   96/  118]
train() client id: f_00009-9-0 loss: 0.894707  [   32/  118]
train() client id: f_00009-9-1 loss: 0.582409  [   64/  118]
train() client id: f_00009-9-2 loss: 0.805379  [   96/  118]
At round 60 accuracy: 0.6472148541114059
At round 60 training accuracy: 0.5895372233400402
At round 60 training loss: 0.8169115502589666
update_location
xs = -3.905658 4.200318 320.009024 18.811294 0.979296 3.956410 -282.443192 -261.324852 304.663977 -247.060879 
ys = 312.587959 295.555839 1.320614 -282.455176 274.350187 257.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -14.21142616811743
ys mean: 87.89414253552872
dists_uav = 328.217133 312.043100 335.272306 300.224568 292.008534 276.557028 299.634856 279.805923 321.136745 266.561606 
uav_gains = -117.854026 -116.694716 -118.310199 -115.749144 -115.048966 -113.667497 -115.699961 -113.962418 -117.366191 -112.758094 
uav_gains_db_mean: -115.71112111214316
dists_bs = 219.553457 215.977925 524.593617 496.817930 202.087041 197.189199 207.592569 194.396076 504.838636 185.562020 
bs_gains = -105.130187 -104.930521 -115.722103 -115.060582 -104.122136 -103.823787 -104.448990 -103.650309 -115.255332 -103.084754 
bs_gains_db_mean: -107.52287006517335
Round 61
-------------------------------
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.95458156 5.94128521 2.91869021 1.0805571  6.84914591 3.29632295
 1.32415061 4.08054691 3.00608516 2.67279298]
obj_prev = 34.12415860899734
eta_min = 3.98916673942854e-32	eta_max = 0.9387143479457988
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 7.8273257066737205	eta = 0.9090909090909091
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 18.61065565296208	eta = 0.3823481974584871
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 12.717399503431304	eta = 0.5595287496088128
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.681826444107466	eta = 0.60912997436458
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617979436161402	eta = 0.6124774692131586
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617704178505617	eta = 0.6124919806097139
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617704173351733	eta = 0.6124919808814296
eta = 0.6124919808814296
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [0.04045173 0.08507706 0.03980964 0.01380496 0.09823992 0.04687264
 0.01733646 0.05746714 0.04173591 0.03788337]
ene_total = [1.16278399 1.74880761 1.17244705 0.58140006 1.9903631  1.02228243
 0.6462616  1.35340359 1.09164915 0.84830559]
ti_comp = [1.21927116 1.35390191 1.20771185 1.26195871 1.35708161 1.35819303
 1.2627831  1.28806187 1.26993947 1.36081379]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [2.78285633e-06 2.09963099e-05 2.70344885e-06 1.03251135e-07
 3.21760040e-05 3.48911535e-06 2.04222466e-07 7.14933098e-06
 2.81736924e-06 1.83496748e-06]
ene_total = [0.40211875 0.14928058 0.4238552  0.32179204 0.14351122 0.14088165
 0.32024363 0.27283609 0.3068348  0.13592205]
optimize_network iter = 0 obj = 2.617276020137961
eta = 0.6124919808814296
freqs = [16588488.88317586 31419209.64704492 16481431.96089219  5469654.50966602
 36195288.80600585 17255515.63488949  6864384.21294392 22307602.23984509
 16432245.4685148  13919379.90342778]
eta_min = 0.6124919808814304	eta_max = 0.7228965206792303
af = 0.0011038579011319113	bf = 1.0136743134601105	zeta = 0.0012142436912451024	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [5.41052682e-07 4.08217617e-06 5.25614002e-07 2.00744476e-08
 6.25577148e-06 6.78366036e-07 3.97056477e-08 1.38999799e-06
 5.47762803e-07 3.56760810e-07]
ene_total = [1.71388696 0.63496578 1.80654562 1.37166028 0.60965148 0.60029516
 1.36505352 1.16252622 1.30772847 0.57926126]
ti_comp = [0.8109757  0.94560644 0.79941638 0.85366324 0.94878614 0.94989756
 0.85448763 0.8797664  0.861644   0.95251833]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.82433614e-06 1.24831928e-05 1.78948422e-06 6.54396715e-08
 1.90913448e-05 2.06877816e-06 1.29353631e-07 4.44460588e-06
 1.77493813e-06 1.08619612e-06]
ene_total = [0.56230711 0.20853359 0.59270505 0.45000056 0.20034537 0.19697486
 0.44783425 0.38146927 0.42905758 0.19005691]
optimize_network iter = 1 obj = 3.659284534384905
eta = 0.7228965206792303
freqs = [16508552.89543737 29777055.50547944 16481431.96089219  5352148.75441332
 34268839.33358462 16331361.10328626  6714818.53144301 21618818.75231183
 16031043.09450184 13163002.59895364]
eta_min = 0.7228965206792295	eta_max = 0.7228965206792275
af = 0.0010051578008740583	bf = 1.0136743134601105	zeta = 0.0011056735809614642	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [5.35850837e-07 3.66661011e-06 5.25614002e-07 1.92211852e-08
 5.60758124e-06 6.07649264e-07 3.79942326e-08 1.30548627e-06
 5.21341470e-07 3.19041588e-07]
ene_total = [1.71388654 0.63493247 1.80654562 1.37166021 0.60959952 0.60028949
 1.36505338 1.16251944 1.30772635 0.57925823]
ti_comp = [0.8109757  0.94560644 0.79941638 0.85366324 0.94878614 0.94989756
 0.85448763 0.8797664  0.861644   0.95251833]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.82433614e-06 1.24831928e-05 1.78948422e-06 6.54396715e-08
 1.90913448e-05 2.06877816e-06 1.29353631e-07 4.44460588e-06
 1.77493813e-06 1.08619612e-06]
ene_total = [0.56230711 0.20853359 0.59270505 0.45000056 0.20034537 0.19697486
 0.44783425 0.38146927 0.42905758 0.19005691]
optimize_network iter = 2 obj = 3.6592845343848683
eta = 0.7228965206792275
freqs = [16508552.89543735 29777055.50547947 16481431.96089218  5352148.75441332
 34268839.33358465 16331361.10328627  6714818.531443   21618818.75231183
 16031043.09450183 13163002.59895365]
Done!
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.71695361e-06 1.17484177e-05 1.68415312e-06 6.15878170e-08
 1.79676062e-05 1.94700750e-06 1.21739727e-07 4.18299125e-06
 1.67046323e-06 1.02226136e-06]
ene_total = [0.0213819  0.00792886 0.0225378  0.01711149 0.00761711 0.00748995
 0.01702911 0.0145053  0.01631503 0.00722695]
At round 61 energy consumption: 0.13914349848735766
At round 61 eta: 0.7228965206792275
At round 61 a_n: 7.287306191126021
At round 61 local rounds: 10.625424748609518
At round 61 global rounds: 26.298140351714242
gradient difference: 0.5391515493392944
train() client id: f_00000-0-0 loss: 0.997681  [   32/  126]
train() client id: f_00000-0-1 loss: 0.976208  [   64/  126]
train() client id: f_00000-0-2 loss: 0.980410  [   96/  126]
train() client id: f_00000-1-0 loss: 0.875490  [   32/  126]
train() client id: f_00000-1-1 loss: 1.169511  [   64/  126]
train() client id: f_00000-1-2 loss: 0.749842  [   96/  126]
train() client id: f_00000-2-0 loss: 0.758015  [   32/  126]
train() client id: f_00000-2-1 loss: 0.881807  [   64/  126]
train() client id: f_00000-2-2 loss: 0.972622  [   96/  126]
train() client id: f_00000-3-0 loss: 0.878026  [   32/  126]
train() client id: f_00000-3-1 loss: 1.092445  [   64/  126]
train() client id: f_00000-3-2 loss: 0.757773  [   96/  126]
train() client id: f_00000-4-0 loss: 0.919571  [   32/  126]
train() client id: f_00000-4-1 loss: 0.886586  [   64/  126]
train() client id: f_00000-4-2 loss: 0.833601  [   96/  126]
train() client id: f_00000-5-0 loss: 0.752923  [   32/  126]
train() client id: f_00000-5-1 loss: 0.774220  [   64/  126]
train() client id: f_00000-5-2 loss: 1.039547  [   96/  126]
train() client id: f_00000-6-0 loss: 0.952499  [   32/  126]
train() client id: f_00000-6-1 loss: 0.789232  [   64/  126]
train() client id: f_00000-6-2 loss: 0.850239  [   96/  126]
train() client id: f_00000-7-0 loss: 0.794586  [   32/  126]
train() client id: f_00000-7-1 loss: 0.815054  [   64/  126]
train() client id: f_00000-7-2 loss: 0.831874  [   96/  126]
train() client id: f_00000-8-0 loss: 0.730714  [   32/  126]
train() client id: f_00000-8-1 loss: 0.746532  [   64/  126]
train() client id: f_00000-8-2 loss: 0.855573  [   96/  126]
train() client id: f_00000-9-0 loss: 0.756030  [   32/  126]
train() client id: f_00000-9-1 loss: 0.877041  [   64/  126]
train() client id: f_00000-9-2 loss: 0.770963  [   96/  126]
train() client id: f_00001-0-0 loss: 0.616257  [   32/  265]
train() client id: f_00001-0-1 loss: 0.467232  [   64/  265]
train() client id: f_00001-0-2 loss: 0.463977  [   96/  265]
train() client id: f_00001-0-3 loss: 0.413779  [  128/  265]
train() client id: f_00001-0-4 loss: 0.523625  [  160/  265]
train() client id: f_00001-0-5 loss: 0.521677  [  192/  265]
train() client id: f_00001-0-6 loss: 0.405367  [  224/  265]
train() client id: f_00001-0-7 loss: 0.393771  [  256/  265]
train() client id: f_00001-1-0 loss: 0.504189  [   32/  265]
train() client id: f_00001-1-1 loss: 0.446480  [   64/  265]
train() client id: f_00001-1-2 loss: 0.500542  [   96/  265]
train() client id: f_00001-1-3 loss: 0.426402  [  128/  265]
train() client id: f_00001-1-4 loss: 0.408178  [  160/  265]
train() client id: f_00001-1-5 loss: 0.391633  [  192/  265]
train() client id: f_00001-1-6 loss: 0.545493  [  224/  265]
train() client id: f_00001-1-7 loss: 0.528922  [  256/  265]
train() client id: f_00001-2-0 loss: 0.474867  [   32/  265]
train() client id: f_00001-2-1 loss: 0.448147  [   64/  265]
train() client id: f_00001-2-2 loss: 0.433848  [   96/  265]
train() client id: f_00001-2-3 loss: 0.433611  [  128/  265]
train() client id: f_00001-2-4 loss: 0.586726  [  160/  265]
train() client id: f_00001-2-5 loss: 0.487283  [  192/  265]
train() client id: f_00001-2-6 loss: 0.391211  [  224/  265]
train() client id: f_00001-2-7 loss: 0.384460  [  256/  265]
train() client id: f_00001-3-0 loss: 0.346545  [   32/  265]
train() client id: f_00001-3-1 loss: 0.393260  [   64/  265]
train() client id: f_00001-3-2 loss: 0.410847  [   96/  265]
train() client id: f_00001-3-3 loss: 0.596120  [  128/  265]
train() client id: f_00001-3-4 loss: 0.356467  [  160/  265]
train() client id: f_00001-3-5 loss: 0.522456  [  192/  265]
train() client id: f_00001-3-6 loss: 0.368066  [  224/  265]
train() client id: f_00001-3-7 loss: 0.668778  [  256/  265]
train() client id: f_00001-4-0 loss: 0.374436  [   32/  265]
train() client id: f_00001-4-1 loss: 0.427505  [   64/  265]
train() client id: f_00001-4-2 loss: 0.505080  [   96/  265]
train() client id: f_00001-4-3 loss: 0.367888  [  128/  265]
train() client id: f_00001-4-4 loss: 0.483284  [  160/  265]
train() client id: f_00001-4-5 loss: 0.564940  [  192/  265]
train() client id: f_00001-4-6 loss: 0.497551  [  224/  265]
train() client id: f_00001-4-7 loss: 0.339412  [  256/  265]
train() client id: f_00001-5-0 loss: 0.378370  [   32/  265]
train() client id: f_00001-5-1 loss: 0.351576  [   64/  265]
train() client id: f_00001-5-2 loss: 0.649444  [   96/  265]
train() client id: f_00001-5-3 loss: 0.351871  [  128/  265]
train() client id: f_00001-5-4 loss: 0.364599  [  160/  265]
train() client id: f_00001-5-5 loss: 0.474539  [  192/  265]
train() client id: f_00001-5-6 loss: 0.544439  [  224/  265]
train() client id: f_00001-5-7 loss: 0.479480  [  256/  265]
train() client id: f_00001-6-0 loss: 0.589209  [   32/  265]
train() client id: f_00001-6-1 loss: 0.410968  [   64/  265]
train() client id: f_00001-6-2 loss: 0.433013  [   96/  265]
train() client id: f_00001-6-3 loss: 0.547001  [  128/  265]
train() client id: f_00001-6-4 loss: 0.348069  [  160/  265]
train() client id: f_00001-6-5 loss: 0.429321  [  192/  265]
train() client id: f_00001-6-6 loss: 0.447542  [  224/  265]
train() client id: f_00001-6-7 loss: 0.405638  [  256/  265]
train() client id: f_00001-7-0 loss: 0.433798  [   32/  265]
train() client id: f_00001-7-1 loss: 0.545992  [   64/  265]
train() client id: f_00001-7-2 loss: 0.425427  [   96/  265]
train() client id: f_00001-7-3 loss: 0.349443  [  128/  265]
train() client id: f_00001-7-4 loss: 0.493622  [  160/  265]
train() client id: f_00001-7-5 loss: 0.402096  [  192/  265]
train() client id: f_00001-7-6 loss: 0.434901  [  224/  265]
train() client id: f_00001-7-7 loss: 0.416729  [  256/  265]
train() client id: f_00001-8-0 loss: 0.658558  [   32/  265]
train() client id: f_00001-8-1 loss: 0.413083  [   64/  265]
train() client id: f_00001-8-2 loss: 0.386793  [   96/  265]
train() client id: f_00001-8-3 loss: 0.345255  [  128/  265]
train() client id: f_00001-8-4 loss: 0.561288  [  160/  265]
train() client id: f_00001-8-5 loss: 0.475651  [  192/  265]
train() client id: f_00001-8-6 loss: 0.382668  [  224/  265]
train() client id: f_00001-8-7 loss: 0.350466  [  256/  265]
train() client id: f_00001-9-0 loss: 0.338203  [   32/  265]
train() client id: f_00001-9-1 loss: 0.481561  [   64/  265]
train() client id: f_00001-9-2 loss: 0.489959  [   96/  265]
train() client id: f_00001-9-3 loss: 0.476025  [  128/  265]
train() client id: f_00001-9-4 loss: 0.370733  [  160/  265]
train() client id: f_00001-9-5 loss: 0.437729  [  192/  265]
train() client id: f_00001-9-6 loss: 0.507423  [  224/  265]
train() client id: f_00001-9-7 loss: 0.475275  [  256/  265]
train() client id: f_00002-0-0 loss: 1.333714  [   32/  124]
train() client id: f_00002-0-1 loss: 1.076730  [   64/  124]
train() client id: f_00002-0-2 loss: 0.958067  [   96/  124]
train() client id: f_00002-1-0 loss: 0.929254  [   32/  124]
train() client id: f_00002-1-1 loss: 1.102119  [   64/  124]
train() client id: f_00002-1-2 loss: 1.097943  [   96/  124]
train() client id: f_00002-2-0 loss: 0.979320  [   32/  124]
train() client id: f_00002-2-1 loss: 1.221911  [   64/  124]
train() client id: f_00002-2-2 loss: 0.866679  [   96/  124]
train() client id: f_00002-3-0 loss: 0.972229  [   32/  124]
train() client id: f_00002-3-1 loss: 0.937123  [   64/  124]
train() client id: f_00002-3-2 loss: 0.914951  [   96/  124]
train() client id: f_00002-4-0 loss: 1.020822  [   32/  124]
train() client id: f_00002-4-1 loss: 1.000084  [   64/  124]
train() client id: f_00002-4-2 loss: 0.767939  [   96/  124]
train() client id: f_00002-5-0 loss: 1.085414  [   32/  124]
train() client id: f_00002-5-1 loss: 0.816950  [   64/  124]
train() client id: f_00002-5-2 loss: 0.882665  [   96/  124]
train() client id: f_00002-6-0 loss: 0.987321  [   32/  124]
train() client id: f_00002-6-1 loss: 0.829187  [   64/  124]
train() client id: f_00002-6-2 loss: 1.001552  [   96/  124]
train() client id: f_00002-7-0 loss: 0.874294  [   32/  124]
train() client id: f_00002-7-1 loss: 0.929806  [   64/  124]
train() client id: f_00002-7-2 loss: 0.997176  [   96/  124]
train() client id: f_00002-8-0 loss: 0.926915  [   32/  124]
train() client id: f_00002-8-1 loss: 0.853203  [   64/  124]
train() client id: f_00002-8-2 loss: 0.857041  [   96/  124]
train() client id: f_00002-9-0 loss: 0.818790  [   32/  124]
train() client id: f_00002-9-1 loss: 0.978514  [   64/  124]
train() client id: f_00002-9-2 loss: 0.796793  [   96/  124]
train() client id: f_00003-0-0 loss: 0.305413  [   32/   43]
train() client id: f_00003-1-0 loss: 0.151638  [   32/   43]
train() client id: f_00003-2-0 loss: 0.559935  [   32/   43]
train() client id: f_00003-3-0 loss: 0.594108  [   32/   43]
train() client id: f_00003-4-0 loss: 0.561889  [   32/   43]
train() client id: f_00003-5-0 loss: 0.501191  [   32/   43]
train() client id: f_00003-6-0 loss: 0.272333  [   32/   43]
train() client id: f_00003-7-0 loss: 0.640035  [   32/   43]
train() client id: f_00003-8-0 loss: 0.535623  [   32/   43]
train() client id: f_00003-9-0 loss: 0.489474  [   32/   43]
train() client id: f_00004-0-0 loss: 0.690249  [   32/  306]
train() client id: f_00004-0-1 loss: 0.438832  [   64/  306]
train() client id: f_00004-0-2 loss: 0.515880  [   96/  306]
train() client id: f_00004-0-3 loss: 0.625109  [  128/  306]
train() client id: f_00004-0-4 loss: 0.654132  [  160/  306]
train() client id: f_00004-0-5 loss: 0.695952  [  192/  306]
train() client id: f_00004-0-6 loss: 0.606602  [  224/  306]
train() client id: f_00004-0-7 loss: 0.601220  [  256/  306]
train() client id: f_00004-0-8 loss: 0.684635  [  288/  306]
train() client id: f_00004-1-0 loss: 0.645023  [   32/  306]
train() client id: f_00004-1-1 loss: 0.481290  [   64/  306]
train() client id: f_00004-1-2 loss: 0.430551  [   96/  306]
train() client id: f_00004-1-3 loss: 0.834937  [  128/  306]
train() client id: f_00004-1-4 loss: 0.579967  [  160/  306]
train() client id: f_00004-1-5 loss: 0.601641  [  192/  306]
train() client id: f_00004-1-6 loss: 0.543653  [  224/  306]
train() client id: f_00004-1-7 loss: 0.589910  [  256/  306]
train() client id: f_00004-1-8 loss: 0.816666  [  288/  306]
train() client id: f_00004-2-0 loss: 0.672255  [   32/  306]
train() client id: f_00004-2-1 loss: 0.660163  [   64/  306]
train() client id: f_00004-2-2 loss: 0.591354  [   96/  306]
train() client id: f_00004-2-3 loss: 0.660451  [  128/  306]
train() client id: f_00004-2-4 loss: 0.653525  [  160/  306]
train() client id: f_00004-2-5 loss: 0.683415  [  192/  306]
train() client id: f_00004-2-6 loss: 0.626539  [  224/  306]
train() client id: f_00004-2-7 loss: 0.505423  [  256/  306]
train() client id: f_00004-2-8 loss: 0.632014  [  288/  306]
train() client id: f_00004-3-0 loss: 0.530189  [   32/  306]
train() client id: f_00004-3-1 loss: 0.673503  [   64/  306]
train() client id: f_00004-3-2 loss: 0.681296  [   96/  306]
train() client id: f_00004-3-3 loss: 0.446906  [  128/  306]
train() client id: f_00004-3-4 loss: 0.771056  [  160/  306]
train() client id: f_00004-3-5 loss: 0.623201  [  192/  306]
train() client id: f_00004-3-6 loss: 0.756063  [  224/  306]
train() client id: f_00004-3-7 loss: 0.527526  [  256/  306]
train() client id: f_00004-3-8 loss: 0.641603  [  288/  306]
train() client id: f_00004-4-0 loss: 0.498145  [   32/  306]
train() client id: f_00004-4-1 loss: 0.572313  [   64/  306]
train() client id: f_00004-4-2 loss: 0.673801  [   96/  306]
train() client id: f_00004-4-3 loss: 0.702721  [  128/  306]
train() client id: f_00004-4-4 loss: 0.629398  [  160/  306]
train() client id: f_00004-4-5 loss: 0.524864  [  192/  306]
train() client id: f_00004-4-6 loss: 0.711759  [  224/  306]
train() client id: f_00004-4-7 loss: 0.606467  [  256/  306]
train() client id: f_00004-4-8 loss: 0.658863  [  288/  306]
train() client id: f_00004-5-0 loss: 0.546394  [   32/  306]
train() client id: f_00004-5-1 loss: 0.687944  [   64/  306]
train() client id: f_00004-5-2 loss: 0.576776  [   96/  306]
train() client id: f_00004-5-3 loss: 0.468653  [  128/  306]
train() client id: f_00004-5-4 loss: 0.886839  [  160/  306]
train() client id: f_00004-5-5 loss: 0.666732  [  192/  306]
train() client id: f_00004-5-6 loss: 0.736263  [  224/  306]
train() client id: f_00004-5-7 loss: 0.564932  [  256/  306]
train() client id: f_00004-5-8 loss: 0.571098  [  288/  306]
train() client id: f_00004-6-0 loss: 0.545067  [   32/  306]
train() client id: f_00004-6-1 loss: 0.625847  [   64/  306]
train() client id: f_00004-6-2 loss: 0.676046  [   96/  306]
train() client id: f_00004-6-3 loss: 0.792205  [  128/  306]
train() client id: f_00004-6-4 loss: 0.593587  [  160/  306]
train() client id: f_00004-6-5 loss: 0.707043  [  192/  306]
train() client id: f_00004-6-6 loss: 0.560861  [  224/  306]
train() client id: f_00004-6-7 loss: 0.659269  [  256/  306]
train() client id: f_00004-6-8 loss: 0.625134  [  288/  306]
train() client id: f_00004-7-0 loss: 0.616140  [   32/  306]
train() client id: f_00004-7-1 loss: 0.734205  [   64/  306]
train() client id: f_00004-7-2 loss: 0.525259  [   96/  306]
train() client id: f_00004-7-3 loss: 0.622164  [  128/  306]
train() client id: f_00004-7-4 loss: 0.702442  [  160/  306]
train() client id: f_00004-7-5 loss: 0.645175  [  192/  306]
train() client id: f_00004-7-6 loss: 0.672593  [  224/  306]
train() client id: f_00004-7-7 loss: 0.623037  [  256/  306]
train() client id: f_00004-7-8 loss: 0.618064  [  288/  306]
train() client id: f_00004-8-0 loss: 0.759742  [   32/  306]
train() client id: f_00004-8-1 loss: 0.646096  [   64/  306]
train() client id: f_00004-8-2 loss: 0.597749  [   96/  306]
train() client id: f_00004-8-3 loss: 0.615087  [  128/  306]
train() client id: f_00004-8-4 loss: 0.638516  [  160/  306]
train() client id: f_00004-8-5 loss: 0.507280  [  192/  306]
train() client id: f_00004-8-6 loss: 0.623514  [  224/  306]
train() client id: f_00004-8-7 loss: 0.581308  [  256/  306]
train() client id: f_00004-8-8 loss: 0.707613  [  288/  306]
train() client id: f_00004-9-0 loss: 0.632856  [   32/  306]
train() client id: f_00004-9-1 loss: 0.763302  [   64/  306]
train() client id: f_00004-9-2 loss: 0.644240  [   96/  306]
train() client id: f_00004-9-3 loss: 0.667328  [  128/  306]
train() client id: f_00004-9-4 loss: 0.620912  [  160/  306]
train() client id: f_00004-9-5 loss: 0.636164  [  192/  306]
train() client id: f_00004-9-6 loss: 0.661215  [  224/  306]
train() client id: f_00004-9-7 loss: 0.559699  [  256/  306]
train() client id: f_00004-9-8 loss: 0.647447  [  288/  306]
train() client id: f_00005-0-0 loss: 0.516560  [   32/  146]
train() client id: f_00005-0-1 loss: 0.679988  [   64/  146]
train() client id: f_00005-0-2 loss: 0.705126  [   96/  146]
train() client id: f_00005-0-3 loss: 0.850083  [  128/  146]
train() client id: f_00005-1-0 loss: 0.760268  [   32/  146]
train() client id: f_00005-1-1 loss: 0.666066  [   64/  146]
train() client id: f_00005-1-2 loss: 0.956492  [   96/  146]
train() client id: f_00005-1-3 loss: 0.669488  [  128/  146]
train() client id: f_00005-2-0 loss: 0.613535  [   32/  146]
train() client id: f_00005-2-1 loss: 0.755323  [   64/  146]
train() client id: f_00005-2-2 loss: 0.821600  [   96/  146]
train() client id: f_00005-2-3 loss: 0.611836  [  128/  146]
train() client id: f_00005-3-0 loss: 0.999426  [   32/  146]
train() client id: f_00005-3-1 loss: 0.682772  [   64/  146]
train() client id: f_00005-3-2 loss: 0.441409  [   96/  146]
train() client id: f_00005-3-3 loss: 0.985659  [  128/  146]
train() client id: f_00005-4-0 loss: 0.521429  [   32/  146]
train() client id: f_00005-4-1 loss: 0.750335  [   64/  146]
train() client id: f_00005-4-2 loss: 0.610246  [   96/  146]
train() client id: f_00005-4-3 loss: 1.061865  [  128/  146]
train() client id: f_00005-5-0 loss: 0.857777  [   32/  146]
train() client id: f_00005-5-1 loss: 0.830087  [   64/  146]
train() client id: f_00005-5-2 loss: 0.517390  [   96/  146]
train() client id: f_00005-5-3 loss: 0.638663  [  128/  146]
train() client id: f_00005-6-0 loss: 0.565145  [   32/  146]
train() client id: f_00005-6-1 loss: 0.666598  [   64/  146]
train() client id: f_00005-6-2 loss: 0.901444  [   96/  146]
train() client id: f_00005-6-3 loss: 1.003821  [  128/  146]
train() client id: f_00005-7-0 loss: 0.367283  [   32/  146]
train() client id: f_00005-7-1 loss: 0.859876  [   64/  146]
train() client id: f_00005-7-2 loss: 0.718897  [   96/  146]
train() client id: f_00005-7-3 loss: 0.954822  [  128/  146]
train() client id: f_00005-8-0 loss: 0.926843  [   32/  146]
train() client id: f_00005-8-1 loss: 0.832364  [   64/  146]
train() client id: f_00005-8-2 loss: 0.838208  [   96/  146]
train() client id: f_00005-8-3 loss: 0.388403  [  128/  146]
train() client id: f_00005-9-0 loss: 0.956880  [   32/  146]
train() client id: f_00005-9-1 loss: 0.715175  [   64/  146]
train() client id: f_00005-9-2 loss: 0.423097  [   96/  146]
train() client id: f_00005-9-3 loss: 0.677817  [  128/  146]
train() client id: f_00006-0-0 loss: 0.483444  [   32/   54]
train() client id: f_00006-1-0 loss: 0.483730  [   32/   54]
train() client id: f_00006-2-0 loss: 0.522007  [   32/   54]
train() client id: f_00006-3-0 loss: 0.544862  [   32/   54]
train() client id: f_00006-4-0 loss: 0.518422  [   32/   54]
train() client id: f_00006-5-0 loss: 0.524287  [   32/   54]
train() client id: f_00006-6-0 loss: 0.480591  [   32/   54]
train() client id: f_00006-7-0 loss: 0.452893  [   32/   54]
train() client id: f_00006-8-0 loss: 0.438266  [   32/   54]
train() client id: f_00006-9-0 loss: 0.415918  [   32/   54]
train() client id: f_00007-0-0 loss: 0.717562  [   32/  179]
train() client id: f_00007-0-1 loss: 0.352527  [   64/  179]
train() client id: f_00007-0-2 loss: 0.440524  [   96/  179]
train() client id: f_00007-0-3 loss: 0.526454  [  128/  179]
train() client id: f_00007-0-4 loss: 0.619683  [  160/  179]
train() client id: f_00007-1-0 loss: 0.665100  [   32/  179]
train() client id: f_00007-1-1 loss: 0.544059  [   64/  179]
train() client id: f_00007-1-2 loss: 0.445904  [   96/  179]
train() client id: f_00007-1-3 loss: 0.556701  [  128/  179]
train() client id: f_00007-1-4 loss: 0.580932  [  160/  179]
train() client id: f_00007-2-0 loss: 0.502536  [   32/  179]
train() client id: f_00007-2-1 loss: 0.444819  [   64/  179]
train() client id: f_00007-2-2 loss: 0.843450  [   96/  179]
train() client id: f_00007-2-3 loss: 0.353030  [  128/  179]
train() client id: f_00007-2-4 loss: 0.572773  [  160/  179]
train() client id: f_00007-3-0 loss: 0.727535  [   32/  179]
train() client id: f_00007-3-1 loss: 0.609305  [   64/  179]
train() client id: f_00007-3-2 loss: 0.470701  [   96/  179]
train() client id: f_00007-3-3 loss: 0.470427  [  128/  179]
train() client id: f_00007-3-4 loss: 0.385740  [  160/  179]
train() client id: f_00007-4-0 loss: 0.434648  [   32/  179]
train() client id: f_00007-4-1 loss: 0.416390  [   64/  179]
train() client id: f_00007-4-2 loss: 0.410842  [   96/  179]
train() client id: f_00007-4-3 loss: 0.981840  [  128/  179]
train() client id: f_00007-4-4 loss: 0.460353  [  160/  179]
train() client id: f_00007-5-0 loss: 0.469941  [   32/  179]
train() client id: f_00007-5-1 loss: 0.376724  [   64/  179]
train() client id: f_00007-5-2 loss: 0.467783  [   96/  179]
train() client id: f_00007-5-3 loss: 0.601648  [  128/  179]
train() client id: f_00007-5-4 loss: 0.660183  [  160/  179]
train() client id: f_00007-6-0 loss: 0.509089  [   32/  179]
train() client id: f_00007-6-1 loss: 0.559507  [   64/  179]
train() client id: f_00007-6-2 loss: 0.544216  [   96/  179]
train() client id: f_00007-6-3 loss: 0.339861  [  128/  179]
train() client id: f_00007-6-4 loss: 0.520190  [  160/  179]
train() client id: f_00007-7-0 loss: 0.560143  [   32/  179]
train() client id: f_00007-7-1 loss: 0.427527  [   64/  179]
train() client id: f_00007-7-2 loss: 0.391989  [   96/  179]
train() client id: f_00007-7-3 loss: 0.694141  [  128/  179]
train() client id: f_00007-7-4 loss: 0.472543  [  160/  179]
train() client id: f_00007-8-0 loss: 0.631889  [   32/  179]
train() client id: f_00007-8-1 loss: 0.744736  [   64/  179]
train() client id: f_00007-8-2 loss: 0.499176  [   96/  179]
train() client id: f_00007-8-3 loss: 0.385335  [  128/  179]
train() client id: f_00007-8-4 loss: 0.373165  [  160/  179]
train() client id: f_00007-9-0 loss: 0.428871  [   32/  179]
train() client id: f_00007-9-1 loss: 0.769974  [   64/  179]
train() client id: f_00007-9-2 loss: 0.461990  [   96/  179]
train() client id: f_00007-9-3 loss: 0.529653  [  128/  179]
train() client id: f_00007-9-4 loss: 0.444955  [  160/  179]
train() client id: f_00008-0-0 loss: 0.781314  [   32/  130]
train() client id: f_00008-0-1 loss: 0.823428  [   64/  130]
train() client id: f_00008-0-2 loss: 0.811550  [   96/  130]
train() client id: f_00008-0-3 loss: 0.779096  [  128/  130]
train() client id: f_00008-1-0 loss: 0.777523  [   32/  130]
train() client id: f_00008-1-1 loss: 0.775814  [   64/  130]
train() client id: f_00008-1-2 loss: 0.792049  [   96/  130]
train() client id: f_00008-1-3 loss: 0.850496  [  128/  130]
train() client id: f_00008-2-0 loss: 0.890419  [   32/  130]
train() client id: f_00008-2-1 loss: 0.776857  [   64/  130]
train() client id: f_00008-2-2 loss: 0.708060  [   96/  130]
train() client id: f_00008-2-3 loss: 0.814209  [  128/  130]
train() client id: f_00008-3-0 loss: 0.744877  [   32/  130]
train() client id: f_00008-3-1 loss: 0.838202  [   64/  130]
train() client id: f_00008-3-2 loss: 0.711525  [   96/  130]
train() client id: f_00008-3-3 loss: 0.886136  [  128/  130]
train() client id: f_00008-4-0 loss: 0.801514  [   32/  130]
train() client id: f_00008-4-1 loss: 0.746102  [   64/  130]
train() client id: f_00008-4-2 loss: 0.850434  [   96/  130]
train() client id: f_00008-4-3 loss: 0.727027  [  128/  130]
train() client id: f_00008-5-0 loss: 0.815257  [   32/  130]
train() client id: f_00008-5-1 loss: 0.749922  [   64/  130]
train() client id: f_00008-5-2 loss: 0.803261  [   96/  130]
train() client id: f_00008-5-3 loss: 0.797832  [  128/  130]
train() client id: f_00008-6-0 loss: 0.957072  [   32/  130]
train() client id: f_00008-6-1 loss: 0.788846  [   64/  130]
train() client id: f_00008-6-2 loss: 0.674370  [   96/  130]
train() client id: f_00008-6-3 loss: 0.759863  [  128/  130]
train() client id: f_00008-7-0 loss: 0.651737  [   32/  130]
train() client id: f_00008-7-1 loss: 0.950536  [   64/  130]
train() client id: f_00008-7-2 loss: 0.780306  [   96/  130]
train() client id: f_00008-7-3 loss: 0.763694  [  128/  130]
train() client id: f_00008-8-0 loss: 0.819909  [   32/  130]
train() client id: f_00008-8-1 loss: 0.792854  [   64/  130]
train() client id: f_00008-8-2 loss: 0.827505  [   96/  130]
train() client id: f_00008-8-3 loss: 0.730397  [  128/  130]
train() client id: f_00008-9-0 loss: 0.741511  [   32/  130]
train() client id: f_00008-9-1 loss: 0.937273  [   64/  130]
train() client id: f_00008-9-2 loss: 0.636855  [   96/  130]
train() client id: f_00008-9-3 loss: 0.851287  [  128/  130]
train() client id: f_00009-0-0 loss: 1.060355  [   32/  118]
train() client id: f_00009-0-1 loss: 0.941678  [   64/  118]
train() client id: f_00009-0-2 loss: 0.909466  [   96/  118]
train() client id: f_00009-1-0 loss: 1.042186  [   32/  118]
train() client id: f_00009-1-1 loss: 0.953078  [   64/  118]
train() client id: f_00009-1-2 loss: 0.794111  [   96/  118]
train() client id: f_00009-2-0 loss: 0.995681  [   32/  118]
train() client id: f_00009-2-1 loss: 0.922224  [   64/  118]
train() client id: f_00009-2-2 loss: 0.940283  [   96/  118]
train() client id: f_00009-3-0 loss: 0.891610  [   32/  118]
train() client id: f_00009-3-1 loss: 0.895311  [   64/  118]
train() client id: f_00009-3-2 loss: 0.955588  [   96/  118]
train() client id: f_00009-4-0 loss: 0.957277  [   32/  118]
train() client id: f_00009-4-1 loss: 0.859578  [   64/  118]
train() client id: f_00009-4-2 loss: 0.808626  [   96/  118]
train() client id: f_00009-5-0 loss: 0.883208  [   32/  118]
train() client id: f_00009-5-1 loss: 0.829293  [   64/  118]
train() client id: f_00009-5-2 loss: 0.892453  [   96/  118]
train() client id: f_00009-6-0 loss: 0.721842  [   32/  118]
train() client id: f_00009-6-1 loss: 0.941926  [   64/  118]
train() client id: f_00009-6-2 loss: 0.822612  [   96/  118]
train() client id: f_00009-7-0 loss: 0.926670  [   32/  118]
train() client id: f_00009-7-1 loss: 0.800903  [   64/  118]
train() client id: f_00009-7-2 loss: 0.682072  [   96/  118]
train() client id: f_00009-8-0 loss: 0.678877  [   32/  118]
train() client id: f_00009-8-1 loss: 0.924393  [   64/  118]
train() client id: f_00009-8-2 loss: 0.796832  [   96/  118]
train() client id: f_00009-9-0 loss: 0.933130  [   32/  118]
train() client id: f_00009-9-1 loss: 0.821946  [   64/  118]
train() client id: f_00009-9-2 loss: 0.800079  [   96/  118]
At round 61 accuracy: 0.6472148541114059
At round 61 training accuracy: 0.5848423876592891
At round 61 training loss: 0.8350097161294422
update_location
xs = -3.905658 4.200318 325.009024 18.811294 0.979296 3.956410 -287.443192 -266.324852 309.663977 -252.060879 
ys = 317.587959 300.555839 1.320614 -287.455176 279.350187 262.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -14.71142616811743
ys mean: 89.39414253552872
dists_uav = 332.982531 316.782978 340.047952 304.933342 296.711115 281.223987 304.352557 284.481288 325.884103 271.202320 
uav_gains = -118.165355 -117.051000 -118.602644 -116.135283 -115.453564 -114.090658 -116.088306 -114.383654 -117.696649 -113.179841 
uav_gains_db_mean: -116.08469542422571
dists_bs = 222.720901 218.808187 529.314229 501.425575 204.591481 199.340718 210.223944 196.667443 509.591688 187.560316 
bs_gains = -105.304366 -105.088839 -115.831039 -115.172841 -104.271910 -103.955748 -104.602160 -103.791568 -115.369285 -103.215006 
bs_gains_db_mean: -107.66027622222484
Round 62
-------------------------------
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.82115899 5.66246847 2.78706489 1.03447434 6.52759612 3.14171865
 1.26660636 3.89259813 2.86603349 2.54747095]
obj_prev = 32.54719040436046
eta_min = 1.2218214947720352e-33	eta_max = 0.9394406220974982
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 7.459395796309552	eta = 0.9090909090909091
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 18.0305957827678	eta = 0.37609788314465736
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 12.220288063290287	eta = 0.5549189078534793
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.203657475325056	eta = 0.605272779953424
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.140690513111194	eta = 0.6086937697223753
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.140415786273087	eta = 0.6087087803393881
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.14041578100363	eta = 0.6087087806273096
eta = 0.6087087806273096
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [0.040963   0.08615235 0.0403128  0.01397944 0.09948158 0.04746507
 0.01755557 0.05819347 0.04226342 0.03836218]
ene_total = [1.12030085 1.67073395 1.1295553  0.56372656 1.90151444 0.97618988
 0.62570552 1.29997444 1.04280632 0.80990852]
ti_comp = [1.2940368  1.43579767 1.28232047 1.33779388 1.43906047 1.44025443
 1.33863419 1.36510734 1.35070396 1.44291406]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [2.56544140e-06 1.93862966e-05 2.49009332e-06 9.54048993e-08
 2.97132334e-05 3.22198749e-06 1.88712718e-07 6.60950921e-06
 2.58614832e-06 1.69476237e-06]
ene_total = [0.39332206 0.14201894 0.41411524 0.31561678 0.13641131 0.13382205
 0.31412702 0.26725561 0.29274776 0.12907455]
optimize_network iter = 0 obj = 2.5385113123879783
eta = 0.6087087806273096
freqs = [15827603.24540961 30001562.15823117 15718689.94891933  5224809.85098799
 34564766.11963843 16478014.31267369  6557270.26566169 21314613.85120587
 15644958.54527248 13293299.23888385]
eta_min = 0.6087087806273109	eta_max = 0.7286862650795968
af = 0.0009590243179496368	bf = 0.9928211124535382	zeta = 0.0010549267497446007	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [4.92556679e-07 3.72210798e-06 4.78090085e-07 1.83174406e-08
 5.70484736e-06 6.18611464e-07 3.62322484e-08 1.26900498e-06
 4.96532343e-07 3.25389044e-07]
ene_total = [1.6927798  0.61008177 1.78228252 1.35847294 0.58530795 0.57579846
 1.35205494 1.14991453 1.25988629 0.5554585 ]
ti_comp = [0.82931765 0.97107852 0.81760132 0.87307473 0.97434132 0.97553528
 0.87391504 0.90038819 0.88598481 0.97819491]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.61900639e-06 1.09851926e-05 1.58767144e-06 5.80605679e-08
 1.68004452e-05 1.82033517e-06 1.14768207e-07 3.93802316e-06
 1.55795981e-06 9.55815292e-07]
ene_total = [0.56722855 0.204606   0.59721779 0.45518453 0.19640314 0.19296354
 0.45303505 0.38537017 0.42217726 0.18613361]
optimize_network iter = 1 obj = 3.660319648720293
eta = 0.7286862650795968
freqs = [15746566.89412167 28283149.87566576 15718689.94891934  5104500.91249742
 32549667.43739824 15511225.99197442  6404139.63058966 20604377.17176979
 15207342.34336579 12502384.68143362]
eta_min = 0.7286862650796062	eta_max = 0.7286862650795745
af = 0.0008653145558439159	bf = 0.9928211124535382	zeta = 0.0009518460114283075	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [4.87525871e-07 3.30793358e-06 4.78090085e-07 1.74835808e-08
 5.05906076e-06 5.48151319e-07 3.45597587e-08 1.18584348e-06
 4.69143123e-07 2.87821399e-07]
ene_total = [1.69277942 0.61005013 1.78228252 1.35847287 0.58525862 0.57579307
 1.35205481 1.14990818 1.25988419 0.55545563]
ti_comp = [0.82931765 0.97107852 0.81760132 0.87307473 0.97434132 0.97553528
 0.87391504 0.90038819 0.88598481 0.97819491]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.61900639e-06 1.09851926e-05 1.58767144e-06 5.80605679e-08
 1.68004452e-05 1.82033517e-06 1.14768207e-07 3.93802316e-06
 1.55795981e-06 9.55815292e-07]
ene_total = [0.56722855 0.204606   0.59721779 0.45518453 0.19640314 0.19296354
 0.45303505 0.38537017 0.42217726 0.18613361]
optimize_network iter = 2 obj = 3.6603196487199923
eta = 0.7286862650795745
freqs = [15746566.89412155 28283149.87566597 15718689.9489192   5104500.91249741
 32549667.4373985  15511225.99197455  6404139.63058965 20604377.17176981
 15207342.34336578 12502384.68143372]
Done!
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.56211252e-06 1.05991595e-05 1.53187873e-06 5.60202486e-08
 1.62100570e-05 1.75636636e-06 1.10735112e-07 3.79963621e-06
 1.50321120e-06 9.22226774e-07]
ene_total = [0.02216009 0.00799304 0.02333169 0.01778287 0.00767237 0.00753852
 0.0176989  0.01505527 0.01649331 0.00727172]
At round 62 energy consumption: 0.1429977884216537
At round 62 eta: 0.7286862650795745
At round 62 a_n: 6.944760344156705
At round 62 local rounds: 10.364211052331338
At round 62 global rounds: 25.59678869996595
gradient difference: 0.5579785108566284
train() client id: f_00000-0-0 loss: 1.369790  [   32/  126]
train() client id: f_00000-0-1 loss: 0.760647  [   64/  126]
train() client id: f_00000-0-2 loss: 0.895538  [   96/  126]
train() client id: f_00000-1-0 loss: 0.936931  [   32/  126]
train() client id: f_00000-1-1 loss: 0.819906  [   64/  126]
train() client id: f_00000-1-2 loss: 1.077972  [   96/  126]
train() client id: f_00000-2-0 loss: 0.870669  [   32/  126]
train() client id: f_00000-2-1 loss: 0.940769  [   64/  126]
train() client id: f_00000-2-2 loss: 0.967497  [   96/  126]
train() client id: f_00000-3-0 loss: 0.842736  [   32/  126]
train() client id: f_00000-3-1 loss: 0.884595  [   64/  126]
train() client id: f_00000-3-2 loss: 0.981350  [   96/  126]
train() client id: f_00000-4-0 loss: 0.782500  [   32/  126]
train() client id: f_00000-4-1 loss: 1.066103  [   64/  126]
train() client id: f_00000-4-2 loss: 0.883099  [   96/  126]
train() client id: f_00000-5-0 loss: 0.844398  [   32/  126]
train() client id: f_00000-5-1 loss: 0.748670  [   64/  126]
train() client id: f_00000-5-2 loss: 0.987698  [   96/  126]
train() client id: f_00000-6-0 loss: 0.763764  [   32/  126]
train() client id: f_00000-6-1 loss: 0.766445  [   64/  126]
train() client id: f_00000-6-2 loss: 0.934946  [   96/  126]
train() client id: f_00000-7-0 loss: 0.835335  [   32/  126]
train() client id: f_00000-7-1 loss: 0.968896  [   64/  126]
train() client id: f_00000-7-2 loss: 0.808408  [   96/  126]
train() client id: f_00000-8-0 loss: 0.847302  [   32/  126]
train() client id: f_00000-8-1 loss: 0.958928  [   64/  126]
train() client id: f_00000-8-2 loss: 0.812095  [   96/  126]
train() client id: f_00000-9-0 loss: 0.844611  [   32/  126]
train() client id: f_00000-9-1 loss: 0.950800  [   64/  126]
train() client id: f_00000-9-2 loss: 0.776345  [   96/  126]
train() client id: f_00001-0-0 loss: 0.368716  [   32/  265]
train() client id: f_00001-0-1 loss: 0.485851  [   64/  265]
train() client id: f_00001-0-2 loss: 0.425328  [   96/  265]
train() client id: f_00001-0-3 loss: 0.353918  [  128/  265]
train() client id: f_00001-0-4 loss: 0.491898  [  160/  265]
train() client id: f_00001-0-5 loss: 0.435338  [  192/  265]
train() client id: f_00001-0-6 loss: 0.400539  [  224/  265]
train() client id: f_00001-0-7 loss: 0.349091  [  256/  265]
train() client id: f_00001-1-0 loss: 0.373506  [   32/  265]
train() client id: f_00001-1-1 loss: 0.494156  [   64/  265]
train() client id: f_00001-1-2 loss: 0.339664  [   96/  265]
train() client id: f_00001-1-3 loss: 0.365375  [  128/  265]
train() client id: f_00001-1-4 loss: 0.532123  [  160/  265]
train() client id: f_00001-1-5 loss: 0.487860  [  192/  265]
train() client id: f_00001-1-6 loss: 0.371648  [  224/  265]
train() client id: f_00001-1-7 loss: 0.307718  [  256/  265]
train() client id: f_00001-2-0 loss: 0.310171  [   32/  265]
train() client id: f_00001-2-1 loss: 0.454686  [   64/  265]
train() client id: f_00001-2-2 loss: 0.454160  [   96/  265]
train() client id: f_00001-2-3 loss: 0.410213  [  128/  265]
train() client id: f_00001-2-4 loss: 0.325128  [  160/  265]
train() client id: f_00001-2-5 loss: 0.380698  [  192/  265]
train() client id: f_00001-2-6 loss: 0.485388  [  224/  265]
train() client id: f_00001-2-7 loss: 0.386980  [  256/  265]
train() client id: f_00001-3-0 loss: 0.317292  [   32/  265]
train() client id: f_00001-3-1 loss: 0.387672  [   64/  265]
train() client id: f_00001-3-2 loss: 0.395093  [   96/  265]
train() client id: f_00001-3-3 loss: 0.426003  [  128/  265]
train() client id: f_00001-3-4 loss: 0.453546  [  160/  265]
train() client id: f_00001-3-5 loss: 0.418108  [  192/  265]
train() client id: f_00001-3-6 loss: 0.382357  [  224/  265]
train() client id: f_00001-3-7 loss: 0.332355  [  256/  265]
train() client id: f_00001-4-0 loss: 0.459168  [   32/  265]
train() client id: f_00001-4-1 loss: 0.295640  [   64/  265]
train() client id: f_00001-4-2 loss: 0.367660  [   96/  265]
train() client id: f_00001-4-3 loss: 0.460719  [  128/  265]
train() client id: f_00001-4-4 loss: 0.305112  [  160/  265]
train() client id: f_00001-4-5 loss: 0.372315  [  192/  265]
train() client id: f_00001-4-6 loss: 0.442927  [  224/  265]
train() client id: f_00001-4-7 loss: 0.316130  [  256/  265]
train() client id: f_00001-5-0 loss: 0.411105  [   32/  265]
train() client id: f_00001-5-1 loss: 0.368848  [   64/  265]
train() client id: f_00001-5-2 loss: 0.429723  [   96/  265]
train() client id: f_00001-5-3 loss: 0.503574  [  128/  265]
train() client id: f_00001-5-4 loss: 0.384985  [  160/  265]
train() client id: f_00001-5-5 loss: 0.294980  [  192/  265]
train() client id: f_00001-5-6 loss: 0.317532  [  224/  265]
train() client id: f_00001-5-7 loss: 0.391004  [  256/  265]
train() client id: f_00001-6-0 loss: 0.396013  [   32/  265]
train() client id: f_00001-6-1 loss: 0.375938  [   64/  265]
train() client id: f_00001-6-2 loss: 0.425047  [   96/  265]
train() client id: f_00001-6-3 loss: 0.416823  [  128/  265]
train() client id: f_00001-6-4 loss: 0.312574  [  160/  265]
train() client id: f_00001-6-5 loss: 0.278057  [  192/  265]
train() client id: f_00001-6-6 loss: 0.469653  [  224/  265]
train() client id: f_00001-6-7 loss: 0.294361  [  256/  265]
train() client id: f_00001-7-0 loss: 0.449002  [   32/  265]
train() client id: f_00001-7-1 loss: 0.346589  [   64/  265]
train() client id: f_00001-7-2 loss: 0.366939  [   96/  265]
train() client id: f_00001-7-3 loss: 0.289883  [  128/  265]
train() client id: f_00001-7-4 loss: 0.587307  [  160/  265]
train() client id: f_00001-7-5 loss: 0.410332  [  192/  265]
train() client id: f_00001-7-6 loss: 0.288637  [  224/  265]
train() client id: f_00001-7-7 loss: 0.367269  [  256/  265]
train() client id: f_00001-8-0 loss: 0.307948  [   32/  265]
train() client id: f_00001-8-1 loss: 0.298124  [   64/  265]
train() client id: f_00001-8-2 loss: 0.452048  [   96/  265]
train() client id: f_00001-8-3 loss: 0.404819  [  128/  265]
train() client id: f_00001-8-4 loss: 0.313958  [  160/  265]
train() client id: f_00001-8-5 loss: 0.413349  [  192/  265]
train() client id: f_00001-8-6 loss: 0.448523  [  224/  265]
train() client id: f_00001-8-7 loss: 0.382870  [  256/  265]
train() client id: f_00001-9-0 loss: 0.366946  [   32/  265]
train() client id: f_00001-9-1 loss: 0.309063  [   64/  265]
train() client id: f_00001-9-2 loss: 0.511318  [   96/  265]
train() client id: f_00001-9-3 loss: 0.499051  [  128/  265]
train() client id: f_00001-9-4 loss: 0.467018  [  160/  265]
train() client id: f_00001-9-5 loss: 0.280634  [  192/  265]
train() client id: f_00001-9-6 loss: 0.302250  [  224/  265]
train() client id: f_00001-9-7 loss: 0.332686  [  256/  265]
train() client id: f_00002-0-0 loss: 1.348276  [   32/  124]
train() client id: f_00002-0-1 loss: 1.170622  [   64/  124]
train() client id: f_00002-0-2 loss: 1.147486  [   96/  124]
train() client id: f_00002-1-0 loss: 1.412062  [   32/  124]
train() client id: f_00002-1-1 loss: 0.987760  [   64/  124]
train() client id: f_00002-1-2 loss: 1.052720  [   96/  124]
train() client id: f_00002-2-0 loss: 1.037425  [   32/  124]
train() client id: f_00002-2-1 loss: 1.065132  [   64/  124]
train() client id: f_00002-2-2 loss: 1.184631  [   96/  124]
train() client id: f_00002-3-0 loss: 0.993565  [   32/  124]
train() client id: f_00002-3-1 loss: 1.058404  [   64/  124]
train() client id: f_00002-3-2 loss: 1.119186  [   96/  124]
train() client id: f_00002-4-0 loss: 0.932710  [   32/  124]
train() client id: f_00002-4-1 loss: 1.036321  [   64/  124]
train() client id: f_00002-4-2 loss: 1.050030  [   96/  124]
train() client id: f_00002-5-0 loss: 1.049695  [   32/  124]
train() client id: f_00002-5-1 loss: 0.930631  [   64/  124]
train() client id: f_00002-5-2 loss: 1.191725  [   96/  124]
train() client id: f_00002-6-0 loss: 1.005016  [   32/  124]
train() client id: f_00002-6-1 loss: 1.093525  [   64/  124]
train() client id: f_00002-6-2 loss: 1.070058  [   96/  124]
train() client id: f_00002-7-0 loss: 1.078298  [   32/  124]
train() client id: f_00002-7-1 loss: 1.069424  [   64/  124]
train() client id: f_00002-7-2 loss: 1.004686  [   96/  124]
train() client id: f_00002-8-0 loss: 0.971201  [   32/  124]
train() client id: f_00002-8-1 loss: 1.218402  [   64/  124]
train() client id: f_00002-8-2 loss: 0.863805  [   96/  124]
train() client id: f_00002-9-0 loss: 1.414858  [   32/  124]
train() client id: f_00002-9-1 loss: 0.806643  [   64/  124]
train() client id: f_00002-9-2 loss: 1.158901  [   96/  124]
train() client id: f_00003-0-0 loss: 0.707258  [   32/   43]
train() client id: f_00003-1-0 loss: 0.650982  [   32/   43]
train() client id: f_00003-2-0 loss: 0.430058  [   32/   43]
train() client id: f_00003-3-0 loss: 0.807805  [   32/   43]
train() client id: f_00003-4-0 loss: 0.590408  [   32/   43]
train() client id: f_00003-5-0 loss: 0.551907  [   32/   43]
train() client id: f_00003-6-0 loss: 0.717795  [   32/   43]
train() client id: f_00003-7-0 loss: 0.594505  [   32/   43]
train() client id: f_00003-8-0 loss: 0.800405  [   32/   43]
train() client id: f_00003-9-0 loss: 0.572379  [   32/   43]
train() client id: f_00004-0-0 loss: 0.761928  [   32/  306]
train() client id: f_00004-0-1 loss: 0.831555  [   64/  306]
train() client id: f_00004-0-2 loss: 0.933767  [   96/  306]
train() client id: f_00004-0-3 loss: 1.110527  [  128/  306]
train() client id: f_00004-0-4 loss: 0.853896  [  160/  306]
train() client id: f_00004-0-5 loss: 0.753679  [  192/  306]
train() client id: f_00004-0-6 loss: 0.893006  [  224/  306]
train() client id: f_00004-0-7 loss: 0.940793  [  256/  306]
train() client id: f_00004-0-8 loss: 0.718375  [  288/  306]
train() client id: f_00004-1-0 loss: 0.907278  [   32/  306]
train() client id: f_00004-1-1 loss: 0.770352  [   64/  306]
train() client id: f_00004-1-2 loss: 0.780979  [   96/  306]
train() client id: f_00004-1-3 loss: 0.959583  [  128/  306]
train() client id: f_00004-1-4 loss: 0.888089  [  160/  306]
train() client id: f_00004-1-5 loss: 0.919403  [  192/  306]
train() client id: f_00004-1-6 loss: 0.759887  [  224/  306]
train() client id: f_00004-1-7 loss: 0.817338  [  256/  306]
train() client id: f_00004-1-8 loss: 0.849894  [  288/  306]
train() client id: f_00004-2-0 loss: 0.850647  [   32/  306]
train() client id: f_00004-2-1 loss: 0.890354  [   64/  306]
train() client id: f_00004-2-2 loss: 0.873244  [   96/  306]
train() client id: f_00004-2-3 loss: 0.783028  [  128/  306]
train() client id: f_00004-2-4 loss: 0.823001  [  160/  306]
train() client id: f_00004-2-5 loss: 0.964484  [  192/  306]
train() client id: f_00004-2-6 loss: 0.850156  [  224/  306]
train() client id: f_00004-2-7 loss: 0.786925  [  256/  306]
train() client id: f_00004-2-8 loss: 0.900816  [  288/  306]
train() client id: f_00004-3-0 loss: 0.959289  [   32/  306]
train() client id: f_00004-3-1 loss: 0.814310  [   64/  306]
train() client id: f_00004-3-2 loss: 0.924171  [   96/  306]
train() client id: f_00004-3-3 loss: 0.747346  [  128/  306]
train() client id: f_00004-3-4 loss: 0.913663  [  160/  306]
train() client id: f_00004-3-5 loss: 0.798584  [  192/  306]
train() client id: f_00004-3-6 loss: 0.929949  [  224/  306]
train() client id: f_00004-3-7 loss: 0.832864  [  256/  306]
train() client id: f_00004-3-8 loss: 0.710204  [  288/  306]
train() client id: f_00004-4-0 loss: 0.758460  [   32/  306]
train() client id: f_00004-4-1 loss: 0.818513  [   64/  306]
train() client id: f_00004-4-2 loss: 0.898794  [   96/  306]
train() client id: f_00004-4-3 loss: 0.818765  [  128/  306]
train() client id: f_00004-4-4 loss: 0.895339  [  160/  306]
train() client id: f_00004-4-5 loss: 0.804251  [  192/  306]
train() client id: f_00004-4-6 loss: 0.770637  [  224/  306]
train() client id: f_00004-4-7 loss: 1.054513  [  256/  306]
train() client id: f_00004-4-8 loss: 0.818289  [  288/  306]
train() client id: f_00004-5-0 loss: 0.832198  [   32/  306]
train() client id: f_00004-5-1 loss: 0.806238  [   64/  306]
train() client id: f_00004-5-2 loss: 0.915912  [   96/  306]
train() client id: f_00004-5-3 loss: 0.874013  [  128/  306]
train() client id: f_00004-5-4 loss: 0.724633  [  160/  306]
train() client id: f_00004-5-5 loss: 0.982393  [  192/  306]
train() client id: f_00004-5-6 loss: 0.767117  [  224/  306]
train() client id: f_00004-5-7 loss: 0.788177  [  256/  306]
train() client id: f_00004-5-8 loss: 0.889353  [  288/  306]
train() client id: f_00004-6-0 loss: 0.809044  [   32/  306]
train() client id: f_00004-6-1 loss: 0.811728  [   64/  306]
train() client id: f_00004-6-2 loss: 0.837280  [   96/  306]
train() client id: f_00004-6-3 loss: 0.905294  [  128/  306]
train() client id: f_00004-6-4 loss: 0.857377  [  160/  306]
train() client id: f_00004-6-5 loss: 0.786623  [  192/  306]
train() client id: f_00004-6-6 loss: 0.903834  [  224/  306]
train() client id: f_00004-6-7 loss: 0.855476  [  256/  306]
train() client id: f_00004-6-8 loss: 0.874455  [  288/  306]
train() client id: f_00004-7-0 loss: 0.783428  [   32/  306]
train() client id: f_00004-7-1 loss: 0.918436  [   64/  306]
train() client id: f_00004-7-2 loss: 0.682831  [   96/  306]
train() client id: f_00004-7-3 loss: 0.912115  [  128/  306]
train() client id: f_00004-7-4 loss: 0.785448  [  160/  306]
train() client id: f_00004-7-5 loss: 0.839526  [  192/  306]
train() client id: f_00004-7-6 loss: 0.902400  [  224/  306]
train() client id: f_00004-7-7 loss: 0.801489  [  256/  306]
train() client id: f_00004-7-8 loss: 0.953958  [  288/  306]
train() client id: f_00004-8-0 loss: 0.921204  [   32/  306]
train() client id: f_00004-8-1 loss: 0.829156  [   64/  306]
train() client id: f_00004-8-2 loss: 0.764642  [   96/  306]
train() client id: f_00004-8-3 loss: 0.879919  [  128/  306]
train() client id: f_00004-8-4 loss: 0.946735  [  160/  306]
train() client id: f_00004-8-5 loss: 0.832362  [  192/  306]
train() client id: f_00004-8-6 loss: 0.827689  [  224/  306]
train() client id: f_00004-8-7 loss: 0.864486  [  256/  306]
train() client id: f_00004-8-8 loss: 0.682319  [  288/  306]
train() client id: f_00004-9-0 loss: 1.016030  [   32/  306]
train() client id: f_00004-9-1 loss: 0.793399  [   64/  306]
train() client id: f_00004-9-2 loss: 0.881496  [   96/  306]
train() client id: f_00004-9-3 loss: 0.921743  [  128/  306]
train() client id: f_00004-9-4 loss: 0.772824  [  160/  306]
train() client id: f_00004-9-5 loss: 0.817802  [  192/  306]
train() client id: f_00004-9-6 loss: 0.685377  [  224/  306]
train() client id: f_00004-9-7 loss: 0.887961  [  256/  306]
train() client id: f_00004-9-8 loss: 0.757181  [  288/  306]
train() client id: f_00005-0-0 loss: 0.854622  [   32/  146]
train() client id: f_00005-0-1 loss: 0.545907  [   64/  146]
train() client id: f_00005-0-2 loss: 0.711591  [   96/  146]
train() client id: f_00005-0-3 loss: 0.732572  [  128/  146]
train() client id: f_00005-1-0 loss: 0.915475  [   32/  146]
train() client id: f_00005-1-1 loss: 0.736190  [   64/  146]
train() client id: f_00005-1-2 loss: 0.673851  [   96/  146]
train() client id: f_00005-1-3 loss: 0.656046  [  128/  146]
train() client id: f_00005-2-0 loss: 0.802250  [   32/  146]
train() client id: f_00005-2-1 loss: 0.816867  [   64/  146]
train() client id: f_00005-2-2 loss: 0.661594  [   96/  146]
train() client id: f_00005-2-3 loss: 0.572951  [  128/  146]
train() client id: f_00005-3-0 loss: 0.663416  [   32/  146]
train() client id: f_00005-3-1 loss: 0.733397  [   64/  146]
train() client id: f_00005-3-2 loss: 0.643815  [   96/  146]
train() client id: f_00005-3-3 loss: 0.789269  [  128/  146]
train() client id: f_00005-4-0 loss: 0.733088  [   32/  146]
train() client id: f_00005-4-1 loss: 0.768100  [   64/  146]
train() client id: f_00005-4-2 loss: 0.472990  [   96/  146]
train() client id: f_00005-4-3 loss: 0.950735  [  128/  146]
train() client id: f_00005-5-0 loss: 0.823185  [   32/  146]
train() client id: f_00005-5-1 loss: 0.596980  [   64/  146]
train() client id: f_00005-5-2 loss: 0.903684  [   96/  146]
train() client id: f_00005-5-3 loss: 0.685547  [  128/  146]
train() client id: f_00005-6-0 loss: 0.791198  [   32/  146]
train() client id: f_00005-6-1 loss: 0.802818  [   64/  146]
train() client id: f_00005-6-2 loss: 0.497528  [   96/  146]
train() client id: f_00005-6-3 loss: 0.909186  [  128/  146]
train() client id: f_00005-7-0 loss: 0.712842  [   32/  146]
train() client id: f_00005-7-1 loss: 0.879218  [   64/  146]
train() client id: f_00005-7-2 loss: 0.812153  [   96/  146]
train() client id: f_00005-7-3 loss: 0.600841  [  128/  146]
train() client id: f_00005-8-0 loss: 0.744223  [   32/  146]
train() client id: f_00005-8-1 loss: 0.718146  [   64/  146]
train() client id: f_00005-8-2 loss: 0.611818  [   96/  146]
train() client id: f_00005-8-3 loss: 0.735652  [  128/  146]
train() client id: f_00005-9-0 loss: 0.945546  [   32/  146]
train() client id: f_00005-9-1 loss: 0.646396  [   64/  146]
train() client id: f_00005-9-2 loss: 0.625929  [   96/  146]
train() client id: f_00005-9-3 loss: 0.673414  [  128/  146]
train() client id: f_00006-0-0 loss: 0.556264  [   32/   54]
train() client id: f_00006-1-0 loss: 0.585842  [   32/   54]
train() client id: f_00006-2-0 loss: 0.534657  [   32/   54]
train() client id: f_00006-3-0 loss: 0.521522  [   32/   54]
train() client id: f_00006-4-0 loss: 0.586152  [   32/   54]
train() client id: f_00006-5-0 loss: 0.557061  [   32/   54]
train() client id: f_00006-6-0 loss: 0.538763  [   32/   54]
train() client id: f_00006-7-0 loss: 0.578245  [   32/   54]
train() client id: f_00006-8-0 loss: 0.506427  [   32/   54]
train() client id: f_00006-9-0 loss: 0.548858  [   32/   54]
train() client id: f_00007-0-0 loss: 0.630908  [   32/  179]
train() client id: f_00007-0-1 loss: 0.617043  [   64/  179]
train() client id: f_00007-0-2 loss: 0.603994  [   96/  179]
train() client id: f_00007-0-3 loss: 0.648508  [  128/  179]
train() client id: f_00007-0-4 loss: 0.883926  [  160/  179]
train() client id: f_00007-1-0 loss: 0.638959  [   32/  179]
train() client id: f_00007-1-1 loss: 0.662202  [   64/  179]
train() client id: f_00007-1-2 loss: 0.657645  [   96/  179]
train() client id: f_00007-1-3 loss: 0.517253  [  128/  179]
train() client id: f_00007-1-4 loss: 0.825042  [  160/  179]
train() client id: f_00007-2-0 loss: 0.610076  [   32/  179]
train() client id: f_00007-2-1 loss: 0.580804  [   64/  179]
train() client id: f_00007-2-2 loss: 0.690171  [   96/  179]
train() client id: f_00007-2-3 loss: 0.500413  [  128/  179]
train() client id: f_00007-2-4 loss: 0.559995  [  160/  179]
train() client id: f_00007-3-0 loss: 0.476244  [   32/  179]
train() client id: f_00007-3-1 loss: 0.662051  [   64/  179]
train() client id: f_00007-3-2 loss: 0.727541  [   96/  179]
train() client id: f_00007-3-3 loss: 0.667753  [  128/  179]
train() client id: f_00007-3-4 loss: 0.563694  [  160/  179]
train() client id: f_00007-4-0 loss: 0.766792  [   32/  179]
train() client id: f_00007-4-1 loss: 0.674142  [   64/  179]
train() client id: f_00007-4-2 loss: 0.481334  [   96/  179]
train() client id: f_00007-4-3 loss: 0.678791  [  128/  179]
train() client id: f_00007-4-4 loss: 0.574476  [  160/  179]
train() client id: f_00007-5-0 loss: 0.661139  [   32/  179]
train() client id: f_00007-5-1 loss: 0.607324  [   64/  179]
train() client id: f_00007-5-2 loss: 0.611276  [   96/  179]
train() client id: f_00007-5-3 loss: 0.645678  [  128/  179]
train() client id: f_00007-5-4 loss: 0.510623  [  160/  179]
train() client id: f_00007-6-0 loss: 0.775315  [   32/  179]
train() client id: f_00007-6-1 loss: 0.639979  [   64/  179]
train() client id: f_00007-6-2 loss: 0.645506  [   96/  179]
train() client id: f_00007-6-3 loss: 0.559133  [  128/  179]
train() client id: f_00007-6-4 loss: 0.434037  [  160/  179]
train() client id: f_00007-7-0 loss: 0.650631  [   32/  179]
train() client id: f_00007-7-1 loss: 0.650012  [   64/  179]
train() client id: f_00007-7-2 loss: 0.651052  [   96/  179]
train() client id: f_00007-7-3 loss: 0.528763  [  128/  179]
train() client id: f_00007-7-4 loss: 0.537178  [  160/  179]
train() client id: f_00007-8-0 loss: 0.517530  [   32/  179]
train() client id: f_00007-8-1 loss: 0.526921  [   64/  179]
train() client id: f_00007-8-2 loss: 0.975001  [   96/  179]
train() client id: f_00007-8-3 loss: 0.562390  [  128/  179]
train() client id: f_00007-8-4 loss: 0.426908  [  160/  179]
train() client id: f_00007-9-0 loss: 0.479273  [   32/  179]
train() client id: f_00007-9-1 loss: 0.662226  [   64/  179]
train() client id: f_00007-9-2 loss: 0.460363  [   96/  179]
train() client id: f_00007-9-3 loss: 0.725516  [  128/  179]
train() client id: f_00007-9-4 loss: 0.561140  [  160/  179]
train() client id: f_00008-0-0 loss: 0.658993  [   32/  130]
train() client id: f_00008-0-1 loss: 0.633049  [   64/  130]
train() client id: f_00008-0-2 loss: 0.610380  [   96/  130]
train() client id: f_00008-0-3 loss: 0.707244  [  128/  130]
train() client id: f_00008-1-0 loss: 0.604199  [   32/  130]
train() client id: f_00008-1-1 loss: 0.568026  [   64/  130]
train() client id: f_00008-1-2 loss: 0.752498  [   96/  130]
train() client id: f_00008-1-3 loss: 0.690623  [  128/  130]
train() client id: f_00008-2-0 loss: 0.655872  [   32/  130]
train() client id: f_00008-2-1 loss: 0.718815  [   64/  130]
train() client id: f_00008-2-2 loss: 0.568916  [   96/  130]
train() client id: f_00008-2-3 loss: 0.670119  [  128/  130]
train() client id: f_00008-3-0 loss: 0.694539  [   32/  130]
train() client id: f_00008-3-1 loss: 0.563873  [   64/  130]
train() client id: f_00008-3-2 loss: 0.692094  [   96/  130]
train() client id: f_00008-3-3 loss: 0.674854  [  128/  130]
train() client id: f_00008-4-0 loss: 0.669693  [   32/  130]
train() client id: f_00008-4-1 loss: 0.610348  [   64/  130]
train() client id: f_00008-4-2 loss: 0.641744  [   96/  130]
train() client id: f_00008-4-3 loss: 0.687149  [  128/  130]
train() client id: f_00008-5-0 loss: 0.601582  [   32/  130]
train() client id: f_00008-5-1 loss: 0.548537  [   64/  130]
train() client id: f_00008-5-2 loss: 0.719943  [   96/  130]
train() client id: f_00008-5-3 loss: 0.668448  [  128/  130]
train() client id: f_00008-6-0 loss: 0.648845  [   32/  130]
train() client id: f_00008-6-1 loss: 0.670842  [   64/  130]
train() client id: f_00008-6-2 loss: 0.704013  [   96/  130]
train() client id: f_00008-6-3 loss: 0.598785  [  128/  130]
train() client id: f_00008-7-0 loss: 0.778851  [   32/  130]
train() client id: f_00008-7-1 loss: 0.593475  [   64/  130]
train() client id: f_00008-7-2 loss: 0.545771  [   96/  130]
train() client id: f_00008-7-3 loss: 0.697404  [  128/  130]
train() client id: f_00008-8-0 loss: 0.648564  [   32/  130]
train() client id: f_00008-8-1 loss: 0.650110  [   64/  130]
train() client id: f_00008-8-2 loss: 0.596655  [   96/  130]
train() client id: f_00008-8-3 loss: 0.713623  [  128/  130]
train() client id: f_00008-9-0 loss: 0.589069  [   32/  130]
train() client id: f_00008-9-1 loss: 0.653196  [   64/  130]
train() client id: f_00008-9-2 loss: 0.614630  [   96/  130]
train() client id: f_00008-9-3 loss: 0.728539  [  128/  130]
train() client id: f_00009-0-0 loss: 1.256798  [   32/  118]
train() client id: f_00009-0-1 loss: 0.904227  [   64/  118]
train() client id: f_00009-0-2 loss: 0.856155  [   96/  118]
train() client id: f_00009-1-0 loss: 0.931152  [   32/  118]
train() client id: f_00009-1-1 loss: 0.984568  [   64/  118]
train() client id: f_00009-1-2 loss: 0.959824  [   96/  118]
train() client id: f_00009-2-0 loss: 0.924115  [   32/  118]
train() client id: f_00009-2-1 loss: 0.819436  [   64/  118]
train() client id: f_00009-2-2 loss: 0.852820  [   96/  118]
train() client id: f_00009-3-0 loss: 0.895908  [   32/  118]
train() client id: f_00009-3-1 loss: 0.892682  [   64/  118]
train() client id: f_00009-3-2 loss: 0.979085  [   96/  118]
train() client id: f_00009-4-0 loss: 0.864437  [   32/  118]
train() client id: f_00009-4-1 loss: 0.788114  [   64/  118]
train() client id: f_00009-4-2 loss: 0.880798  [   96/  118]
train() client id: f_00009-5-0 loss: 0.720727  [   32/  118]
train() client id: f_00009-5-1 loss: 0.911829  [   64/  118]
train() client id: f_00009-5-2 loss: 0.890477  [   96/  118]
train() client id: f_00009-6-0 loss: 0.963618  [   32/  118]
train() client id: f_00009-6-1 loss: 0.972119  [   64/  118]
train() client id: f_00009-6-2 loss: 0.754595  [   96/  118]
train() client id: f_00009-7-0 loss: 0.827290  [   32/  118]
train() client id: f_00009-7-1 loss: 0.813787  [   64/  118]
train() client id: f_00009-7-2 loss: 0.741482  [   96/  118]
train() client id: f_00009-8-0 loss: 0.797804  [   32/  118]
train() client id: f_00009-8-1 loss: 0.793115  [   64/  118]
train() client id: f_00009-8-2 loss: 0.829287  [   96/  118]
train() client id: f_00009-9-0 loss: 0.863584  [   32/  118]
train() client id: f_00009-9-1 loss: 0.744534  [   64/  118]
train() client id: f_00009-9-2 loss: 0.841597  [   96/  118]
At round 62 accuracy: 0.6472148541114059
At round 62 training accuracy: 0.5855130784708249
At round 62 training loss: 0.829338970544929
update_location
xs = -3.905658 4.200318 330.009024 18.811294 0.979296 3.956410 -292.443192 -271.324852 314.663977 -257.060879 
ys = 322.587959 305.555839 1.320614 -292.455176 284.350187 267.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -15.21142616811743
ys mean: 90.89414253552872
dists_uav = 337.754712 321.530735 344.829958 309.651247 301.423270 285.902208 309.079134 289.167515 330.638909 275.855591 
uav_gains = -118.463820 -117.394140 -118.882851 -116.509803 -115.848564 -114.510637 -116.465072 -114.800128 -118.013928 -113.603678 
uav_gains_db_mean: -116.44926216776996
dists_bs = 225.954596 221.715090 534.039927 506.040670 207.186332 201.593311 212.940223 199.038518 514.349422 189.669399 
bs_gains = -105.479652 -105.249326 -115.939124 -115.284251 -104.425170 -104.092390 -104.758275 -103.937299 -115.482290 -103.350983 
bs_gains_db_mean: -107.79987598962339
Round 63
-------------------------------
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.68727091 5.38361967 2.65494812 0.98808132 6.20602153 2.98709652
 1.20875503 3.70447299 2.72587058 2.42213493]
obj_prev = 30.968271608412685
eta_min = 2.604478585360512e-35	eta_max = 0.9403513878285256
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 7.091465885945381	eta = 0.9090909090909091
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 17.42929397319813	eta = 0.36988229006606876
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 11.714987188776163	eta = 0.5503025368408222
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.719694765497792	eta = 0.601396523881516
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657792616593055	eta = 0.6048895302207599
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657519519314839	eta = 0.6049050304207853
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657519513957864	eta = 0.6049050307248393
eta = 0.6049050307248393
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [0.04148026 0.08724024 0.04082185 0.01415596 0.10073778 0.04806443
 0.01777726 0.05892831 0.0427971  0.0388466 ]
ene_total = [1.07662235 1.59233833 1.08542302 0.54531731 1.81230098 0.92997575
 0.60440513 1.2459445  0.99375704 0.77143509]
ti_comp = [1.37728495 1.52626582 1.36543421 1.42198973 1.52960948 1.53088406
 1.42284286 1.45043831 1.44004221 1.53358086]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [2.35156460e-06 1.78143569e-05 2.28042807e-06 8.76809665e-08
 2.73083858e-05 2.96119390e-06 1.73444402e-07 6.07929826e-06
 2.36250370e-06 1.55784967e-06]
ene_total = [0.38350781 0.13481247 0.40330974 0.30876628 0.12938372 0.12684698
 0.3073421  0.26132755 0.2786378  0.12231706]
optimize_network iter = 0 obj = 2.4562515021459133
eta = 0.6049050307248393
freqs = [15058707.65852771 28579633.50514324 14948302.02718542  4977519.32111269
 32929248.53672925 15698259.84497274  6247090.26876788 20313966.31628257
 14859667.46433555 12665323.49976712]
eta_min = 0.6049050307248401	eta_max = 0.735169781083726
af = 0.0008271317221307269	bf = 0.9700368260492027	zeta = 0.0009098448943437997	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [4.45862869e-07 3.37764920e-06 4.32375195e-07 1.66245432e-08
 5.17774221e-06 5.61450199e-07 3.28855174e-08 1.15265103e-06
 4.47936952e-07 2.95372418e-07]
ene_total = [1.66660366 0.58485258 1.75266791 1.34190688 0.5607002  0.55110835
 1.3357123  1.1353834  1.21083327 0.53150371]
ti_comp = [0.84752723 0.9965081  0.83567649 0.89223201 0.99985176 1.00112634
 0.89308514 0.92068059 0.91028449 1.00382314]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.42363747e-06 9.58012233e-06 1.39567334e-06 5.10557738e-08
 1.46517044e-05 1.58736166e-06 1.00923117e-07 3.45888573e-06
 1.35541058e-06 8.33540515e-07]
ene_total = [0.57212459 0.20091879 0.60166777 0.4606414  0.19270948 0.18920625
 0.4585158  0.38980408 0.41566912 0.18246434]
optimize_network iter = 1 obj = 3.6637216068534983
eta = 0.735169781083726
freqs = [14977014.44528698 26790042.71364152 14948302.02718542  4855109.38680831
 30831466.09028835 14691709.46586062  6091289.81349057 19586300.25463695
 14387143.5153923  11842221.20609098]
eta_min = 0.7351697810837274	eta_max = 0.735169781083714
af = 0.0007390421746511915	bf = 0.9700368260492027	zeta = 0.0008129463921163107	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [4.41038396e-07 2.96789166e-06 4.32375195e-07 1.58169176e-08
 4.53905178e-06 4.91759633e-07 3.12656630e-08 1.07155188e-06
 4.19901921e-07 2.58228221e-07]
ene_total = [1.66660331 0.58482282 1.75266791 1.34190682 0.56065381 0.55110329
 1.33571218 1.13537751 1.21083123 0.53150101]
ti_comp = [0.84752723 0.9965081  0.83567649 0.89223201 0.99985176 1.00112634
 0.89308514 0.92068059 0.91028449 1.00382314]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.42363747e-06 9.58012233e-06 1.39567334e-06 5.10557738e-08
 1.46517044e-05 1.58736166e-06 1.00923117e-07 3.45888573e-06
 1.35541058e-06 8.33540515e-07]
ene_total = [0.57212459 0.20091879 0.60166777 0.4606414  0.19270948 0.18920625
 0.4585158  0.38980408 0.41566912 0.18246434]
optimize_network iter = 2 obj = 3.6637216068533323
eta = 0.735169781083714
freqs = [14977014.44528692 26790042.71364162 14948302.02718534  4855109.38680831
 30831466.09028849 14691709.46586069  6091289.81349056 19586300.25463695
 14387143.51539229 11842221.20609103]
Done!
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.41315906e-06 9.50960965e-06 1.38540075e-06 5.06799874e-08
 1.45438633e-05 1.57567819e-06 1.00180291e-07 3.43342726e-06
 1.34543434e-06 8.27405398e-07]
ene_total = [0.02294924 0.00805925 0.02413429 0.0184774  0.00772992 0.00758949
 0.01839214 0.01563593 0.01667345 0.00731906]
At round 63 energy consumption: 0.14696016345683638
At round 63 eta: 0.735169781083714
At round 63 a_n: 6.602214497187386
At round 63 local rounds: 10.074148866251232
At round 63 global rounds: 24.929989199134315
gradient difference: 0.5616282224655151
train() client id: f_00000-0-0 loss: 0.960191  [   32/  126]
train() client id: f_00000-0-1 loss: 0.763939  [   64/  126]
train() client id: f_00000-0-2 loss: 1.398636  [   96/  126]
train() client id: f_00000-1-0 loss: 0.993540  [   32/  126]
train() client id: f_00000-1-1 loss: 1.119321  [   64/  126]
train() client id: f_00000-1-2 loss: 0.872527  [   96/  126]
train() client id: f_00000-2-0 loss: 0.929057  [   32/  126]
train() client id: f_00000-2-1 loss: 0.966461  [   64/  126]
train() client id: f_00000-2-2 loss: 0.862768  [   96/  126]
train() client id: f_00000-3-0 loss: 0.821662  [   32/  126]
train() client id: f_00000-3-1 loss: 1.012087  [   64/  126]
train() client id: f_00000-3-2 loss: 0.912489  [   96/  126]
train() client id: f_00000-4-0 loss: 0.859523  [   32/  126]
train() client id: f_00000-4-1 loss: 0.878384  [   64/  126]
train() client id: f_00000-4-2 loss: 1.160070  [   96/  126]
train() client id: f_00000-5-0 loss: 1.031948  [   32/  126]
train() client id: f_00000-5-1 loss: 0.862955  [   64/  126]
train() client id: f_00000-5-2 loss: 0.904327  [   96/  126]
train() client id: f_00000-6-0 loss: 1.037269  [   32/  126]
train() client id: f_00000-6-1 loss: 0.820830  [   64/  126]
train() client id: f_00000-6-2 loss: 0.880895  [   96/  126]
train() client id: f_00000-7-0 loss: 0.911394  [   32/  126]
train() client id: f_00000-7-1 loss: 0.904061  [   64/  126]
train() client id: f_00000-7-2 loss: 0.892539  [   96/  126]
train() client id: f_00000-8-0 loss: 0.932970  [   32/  126]
train() client id: f_00000-8-1 loss: 0.865435  [   64/  126]
train() client id: f_00000-8-2 loss: 0.881522  [   96/  126]
train() client id: f_00000-9-0 loss: 0.990016  [   32/  126]
train() client id: f_00000-9-1 loss: 0.838891  [   64/  126]
train() client id: f_00000-9-2 loss: 0.903323  [   96/  126]
train() client id: f_00001-0-0 loss: 0.494717  [   32/  265]
train() client id: f_00001-0-1 loss: 0.475804  [   64/  265]
train() client id: f_00001-0-2 loss: 0.503115  [   96/  265]
train() client id: f_00001-0-3 loss: 0.523029  [  128/  265]
train() client id: f_00001-0-4 loss: 0.467413  [  160/  265]
train() client id: f_00001-0-5 loss: 0.357685  [  192/  265]
train() client id: f_00001-0-6 loss: 0.411124  [  224/  265]
train() client id: f_00001-0-7 loss: 0.504973  [  256/  265]
train() client id: f_00001-1-0 loss: 0.374798  [   32/  265]
train() client id: f_00001-1-1 loss: 0.472111  [   64/  265]
train() client id: f_00001-1-2 loss: 0.425659  [   96/  265]
train() client id: f_00001-1-3 loss: 0.387516  [  128/  265]
train() client id: f_00001-1-4 loss: 0.511560  [  160/  265]
train() client id: f_00001-1-5 loss: 0.457528  [  192/  265]
train() client id: f_00001-1-6 loss: 0.659201  [  224/  265]
train() client id: f_00001-1-7 loss: 0.338398  [  256/  265]
train() client id: f_00001-2-0 loss: 0.476206  [   32/  265]
train() client id: f_00001-2-1 loss: 0.421253  [   64/  265]
train() client id: f_00001-2-2 loss: 0.508436  [   96/  265]
train() client id: f_00001-2-3 loss: 0.429396  [  128/  265]
train() client id: f_00001-2-4 loss: 0.441116  [  160/  265]
train() client id: f_00001-2-5 loss: 0.443096  [  192/  265]
train() client id: f_00001-2-6 loss: 0.426544  [  224/  265]
train() client id: f_00001-2-7 loss: 0.505565  [  256/  265]
train() client id: f_00001-3-0 loss: 0.504779  [   32/  265]
train() client id: f_00001-3-1 loss: 0.418057  [   64/  265]
train() client id: f_00001-3-2 loss: 0.375027  [   96/  265]
train() client id: f_00001-3-3 loss: 0.504829  [  128/  265]
train() client id: f_00001-3-4 loss: 0.506650  [  160/  265]
train() client id: f_00001-3-5 loss: 0.486716  [  192/  265]
train() client id: f_00001-3-6 loss: 0.404667  [  224/  265]
train() client id: f_00001-3-7 loss: 0.383999  [  256/  265]
train() client id: f_00001-4-0 loss: 0.414404  [   32/  265]
train() client id: f_00001-4-1 loss: 0.488198  [   64/  265]
train() client id: f_00001-4-2 loss: 0.424211  [   96/  265]
train() client id: f_00001-4-3 loss: 0.465625  [  128/  265]
train() client id: f_00001-4-4 loss: 0.434793  [  160/  265]
train() client id: f_00001-4-5 loss: 0.432541  [  192/  265]
train() client id: f_00001-4-6 loss: 0.430483  [  224/  265]
train() client id: f_00001-4-7 loss: 0.496290  [  256/  265]
train() client id: f_00001-5-0 loss: 0.524203  [   32/  265]
train() client id: f_00001-5-1 loss: 0.421355  [   64/  265]
train() client id: f_00001-5-2 loss: 0.420286  [   96/  265]
train() client id: f_00001-5-3 loss: 0.395147  [  128/  265]
train() client id: f_00001-5-4 loss: 0.457707  [  160/  265]
train() client id: f_00001-5-5 loss: 0.424544  [  192/  265]
train() client id: f_00001-5-6 loss: 0.586804  [  224/  265]
train() client id: f_00001-5-7 loss: 0.364438  [  256/  265]
train() client id: f_00001-6-0 loss: 0.350469  [   32/  265]
train() client id: f_00001-6-1 loss: 0.341237  [   64/  265]
train() client id: f_00001-6-2 loss: 0.621726  [   96/  265]
train() client id: f_00001-6-3 loss: 0.547212  [  128/  265]
train() client id: f_00001-6-4 loss: 0.436436  [  160/  265]
train() client id: f_00001-6-5 loss: 0.364692  [  192/  265]
train() client id: f_00001-6-6 loss: 0.509283  [  224/  265]
train() client id: f_00001-6-7 loss: 0.411508  [  256/  265]
train() client id: f_00001-7-0 loss: 0.425427  [   32/  265]
train() client id: f_00001-7-1 loss: 0.443506  [   64/  265]
train() client id: f_00001-7-2 loss: 0.395241  [   96/  265]
train() client id: f_00001-7-3 loss: 0.534001  [  128/  265]
train() client id: f_00001-7-4 loss: 0.438881  [  160/  265]
train() client id: f_00001-7-5 loss: 0.336050  [  192/  265]
train() client id: f_00001-7-6 loss: 0.551939  [  224/  265]
train() client id: f_00001-7-7 loss: 0.453573  [  256/  265]
train() client id: f_00001-8-0 loss: 0.423987  [   32/  265]
train() client id: f_00001-8-1 loss: 0.379105  [   64/  265]
train() client id: f_00001-8-2 loss: 0.514144  [   96/  265]
train() client id: f_00001-8-3 loss: 0.377946  [  128/  265]
train() client id: f_00001-8-4 loss: 0.504241  [  160/  265]
train() client id: f_00001-8-5 loss: 0.448576  [  192/  265]
train() client id: f_00001-8-6 loss: 0.509750  [  224/  265]
train() client id: f_00001-8-7 loss: 0.422723  [  256/  265]
train() client id: f_00001-9-0 loss: 0.474916  [   32/  265]
train() client id: f_00001-9-1 loss: 0.384008  [   64/  265]
train() client id: f_00001-9-2 loss: 0.468169  [   96/  265]
train() client id: f_00001-9-3 loss: 0.356090  [  128/  265]
train() client id: f_00001-9-4 loss: 0.430144  [  160/  265]
train() client id: f_00001-9-5 loss: 0.532191  [  192/  265]
train() client id: f_00001-9-6 loss: 0.345555  [  224/  265]
train() client id: f_00001-9-7 loss: 0.520879  [  256/  265]
train() client id: f_00002-0-0 loss: 0.891231  [   32/  124]
train() client id: f_00002-0-1 loss: 0.903197  [   64/  124]
train() client id: f_00002-0-2 loss: 1.058625  [   96/  124]
train() client id: f_00002-1-0 loss: 0.893078  [   32/  124]
train() client id: f_00002-1-1 loss: 0.978726  [   64/  124]
train() client id: f_00002-1-2 loss: 1.010308  [   96/  124]
train() client id: f_00002-2-0 loss: 1.067503  [   32/  124]
train() client id: f_00002-2-1 loss: 1.010482  [   64/  124]
train() client id: f_00002-2-2 loss: 0.744024  [   96/  124]
train() client id: f_00002-3-0 loss: 0.699869  [   32/  124]
train() client id: f_00002-3-1 loss: 0.996227  [   64/  124]
train() client id: f_00002-3-2 loss: 0.905814  [   96/  124]
train() client id: f_00002-4-0 loss: 0.769382  [   32/  124]
train() client id: f_00002-4-1 loss: 0.833845  [   64/  124]
train() client id: f_00002-4-2 loss: 1.138281  [   96/  124]
train() client id: f_00002-5-0 loss: 1.114377  [   32/  124]
train() client id: f_00002-5-1 loss: 0.624603  [   64/  124]
train() client id: f_00002-5-2 loss: 0.913455  [   96/  124]
train() client id: f_00002-6-0 loss: 0.785078  [   32/  124]
train() client id: f_00002-6-1 loss: 0.724051  [   64/  124]
train() client id: f_00002-6-2 loss: 0.874586  [   96/  124]
train() client id: f_00002-7-0 loss: 0.862814  [   32/  124]
train() client id: f_00002-7-1 loss: 0.901823  [   64/  124]
train() client id: f_00002-7-2 loss: 0.899896  [   96/  124]
train() client id: f_00002-8-0 loss: 0.963031  [   32/  124]
train() client id: f_00002-8-1 loss: 0.985338  [   64/  124]
train() client id: f_00002-8-2 loss: 0.685765  [   96/  124]
train() client id: f_00002-9-0 loss: 0.932022  [   32/  124]
train() client id: f_00002-9-1 loss: 0.735377  [   64/  124]
train() client id: f_00002-9-2 loss: 0.727705  [   96/  124]
train() client id: f_00003-0-0 loss: 0.822329  [   32/   43]
train() client id: f_00003-1-0 loss: 0.706791  [   32/   43]
train() client id: f_00003-2-0 loss: 0.705890  [   32/   43]
train() client id: f_00003-3-0 loss: 0.551757  [   32/   43]
train() client id: f_00003-4-0 loss: 0.397441  [   32/   43]
train() client id: f_00003-5-0 loss: 0.409299  [   32/   43]
train() client id: f_00003-6-0 loss: 0.684342  [   32/   43]
train() client id: f_00003-7-0 loss: 0.651456  [   32/   43]
train() client id: f_00003-8-0 loss: 0.833565  [   32/   43]
train() client id: f_00003-9-0 loss: 0.613389  [   32/   43]
train() client id: f_00004-0-0 loss: 0.909067  [   32/  306]
train() client id: f_00004-0-1 loss: 0.906398  [   64/  306]
train() client id: f_00004-0-2 loss: 0.805020  [   96/  306]
train() client id: f_00004-0-3 loss: 0.828168  [  128/  306]
train() client id: f_00004-0-4 loss: 0.962061  [  160/  306]
train() client id: f_00004-0-5 loss: 0.987577  [  192/  306]
train() client id: f_00004-0-6 loss: 0.825754  [  224/  306]
train() client id: f_00004-0-7 loss: 0.717206  [  256/  306]
train() client id: f_00004-0-8 loss: 0.788275  [  288/  306]
train() client id: f_00004-1-0 loss: 0.826856  [   32/  306]
train() client id: f_00004-1-1 loss: 1.069322  [   64/  306]
train() client id: f_00004-1-2 loss: 0.669478  [   96/  306]
train() client id: f_00004-1-3 loss: 0.904503  [  128/  306]
train() client id: f_00004-1-4 loss: 0.914712  [  160/  306]
train() client id: f_00004-1-5 loss: 0.871891  [  192/  306]
train() client id: f_00004-1-6 loss: 0.823148  [  224/  306]
train() client id: f_00004-1-7 loss: 0.947478  [  256/  306]
train() client id: f_00004-1-8 loss: 0.788599  [  288/  306]
train() client id: f_00004-2-0 loss: 0.987820  [   32/  306]
train() client id: f_00004-2-1 loss: 0.773032  [   64/  306]
train() client id: f_00004-2-2 loss: 0.874377  [   96/  306]
train() client id: f_00004-2-3 loss: 0.880442  [  128/  306]
train() client id: f_00004-2-4 loss: 0.948436  [  160/  306]
train() client id: f_00004-2-5 loss: 0.704538  [  192/  306]
train() client id: f_00004-2-6 loss: 0.990206  [  224/  306]
train() client id: f_00004-2-7 loss: 0.811972  [  256/  306]
train() client id: f_00004-2-8 loss: 0.823796  [  288/  306]
train() client id: f_00004-3-0 loss: 0.920803  [   32/  306]
train() client id: f_00004-3-1 loss: 0.910874  [   64/  306]
train() client id: f_00004-3-2 loss: 0.996174  [   96/  306]
train() client id: f_00004-3-3 loss: 0.846672  [  128/  306]
train() client id: f_00004-3-4 loss: 0.859849  [  160/  306]
train() client id: f_00004-3-5 loss: 0.919601  [  192/  306]
train() client id: f_00004-3-6 loss: 0.848642  [  224/  306]
train() client id: f_00004-3-7 loss: 0.744948  [  256/  306]
train() client id: f_00004-3-8 loss: 0.854322  [  288/  306]
train() client id: f_00004-4-0 loss: 0.860198  [   32/  306]
train() client id: f_00004-4-1 loss: 0.970869  [   64/  306]
train() client id: f_00004-4-2 loss: 0.795508  [   96/  306]
train() client id: f_00004-4-3 loss: 0.873292  [  128/  306]
train() client id: f_00004-4-4 loss: 0.851048  [  160/  306]
train() client id: f_00004-4-5 loss: 0.829507  [  192/  306]
train() client id: f_00004-4-6 loss: 0.990951  [  224/  306]
train() client id: f_00004-4-7 loss: 0.844734  [  256/  306]
train() client id: f_00004-4-8 loss: 0.818451  [  288/  306]
train() client id: f_00004-5-0 loss: 0.867534  [   32/  306]
train() client id: f_00004-5-1 loss: 0.830914  [   64/  306]
train() client id: f_00004-5-2 loss: 1.023440  [   96/  306]
train() client id: f_00004-5-3 loss: 0.784864  [  128/  306]
train() client id: f_00004-5-4 loss: 0.799539  [  160/  306]
train() client id: f_00004-5-5 loss: 0.878436  [  192/  306]
train() client id: f_00004-5-6 loss: 0.822370  [  224/  306]
train() client id: f_00004-5-7 loss: 0.798545  [  256/  306]
train() client id: f_00004-5-8 loss: 0.967659  [  288/  306]
train() client id: f_00004-6-0 loss: 0.965661  [   32/  306]
train() client id: f_00004-6-1 loss: 0.838733  [   64/  306]
train() client id: f_00004-6-2 loss: 0.762242  [   96/  306]
train() client id: f_00004-6-3 loss: 0.814239  [  128/  306]
train() client id: f_00004-6-4 loss: 0.821182  [  160/  306]
train() client id: f_00004-6-5 loss: 0.834964  [  192/  306]
train() client id: f_00004-6-6 loss: 0.892660  [  224/  306]
train() client id: f_00004-6-7 loss: 0.977276  [  256/  306]
train() client id: f_00004-6-8 loss: 0.893155  [  288/  306]
train() client id: f_00004-7-0 loss: 0.882746  [   32/  306]
train() client id: f_00004-7-1 loss: 0.965199  [   64/  306]
train() client id: f_00004-7-2 loss: 0.934176  [   96/  306]
train() client id: f_00004-7-3 loss: 0.834200  [  128/  306]
train() client id: f_00004-7-4 loss: 0.853556  [  160/  306]
train() client id: f_00004-7-5 loss: 0.799012  [  192/  306]
train() client id: f_00004-7-6 loss: 0.911106  [  224/  306]
train() client id: f_00004-7-7 loss: 0.839058  [  256/  306]
train() client id: f_00004-7-8 loss: 0.829751  [  288/  306]
train() client id: f_00004-8-0 loss: 0.792081  [   32/  306]
train() client id: f_00004-8-1 loss: 0.720232  [   64/  306]
train() client id: f_00004-8-2 loss: 0.939025  [   96/  306]
train() client id: f_00004-8-3 loss: 0.832423  [  128/  306]
train() client id: f_00004-8-4 loss: 1.042985  [  160/  306]
train() client id: f_00004-8-5 loss: 0.827749  [  192/  306]
train() client id: f_00004-8-6 loss: 0.897660  [  224/  306]
train() client id: f_00004-8-7 loss: 0.960468  [  256/  306]
train() client id: f_00004-8-8 loss: 0.896646  [  288/  306]
train() client id: f_00004-9-0 loss: 0.891496  [   32/  306]
train() client id: f_00004-9-1 loss: 0.856915  [   64/  306]
train() client id: f_00004-9-2 loss: 0.835393  [   96/  306]
train() client id: f_00004-9-3 loss: 0.866523  [  128/  306]
train() client id: f_00004-9-4 loss: 0.886966  [  160/  306]
train() client id: f_00004-9-5 loss: 0.826417  [  192/  306]
train() client id: f_00004-9-6 loss: 0.890673  [  224/  306]
train() client id: f_00004-9-7 loss: 0.935799  [  256/  306]
train() client id: f_00004-9-8 loss: 0.853215  [  288/  306]
train() client id: f_00005-0-0 loss: 0.736651  [   32/  146]
train() client id: f_00005-0-1 loss: 0.575002  [   64/  146]
train() client id: f_00005-0-2 loss: 0.539575  [   96/  146]
train() client id: f_00005-0-3 loss: 0.748497  [  128/  146]
train() client id: f_00005-1-0 loss: 0.421679  [   32/  146]
train() client id: f_00005-1-1 loss: 0.440247  [   64/  146]
train() client id: f_00005-1-2 loss: 1.050819  [   96/  146]
train() client id: f_00005-1-3 loss: 0.811693  [  128/  146]
train() client id: f_00005-2-0 loss: 0.673513  [   32/  146]
train() client id: f_00005-2-1 loss: 0.477417  [   64/  146]
train() client id: f_00005-2-2 loss: 0.590762  [   96/  146]
train() client id: f_00005-2-3 loss: 0.809530  [  128/  146]
train() client id: f_00005-3-0 loss: 0.580863  [   32/  146]
train() client id: f_00005-3-1 loss: 0.491381  [   64/  146]
train() client id: f_00005-3-2 loss: 0.991120  [   96/  146]
train() client id: f_00005-3-3 loss: 0.575991  [  128/  146]
train() client id: f_00005-4-0 loss: 0.548401  [   32/  146]
train() client id: f_00005-4-1 loss: 0.988800  [   64/  146]
train() client id: f_00005-4-2 loss: 0.622440  [   96/  146]
train() client id: f_00005-4-3 loss: 0.451667  [  128/  146]
train() client id: f_00005-5-0 loss: 0.830232  [   32/  146]
train() client id: f_00005-5-1 loss: 0.729356  [   64/  146]
train() client id: f_00005-5-2 loss: 0.615240  [   96/  146]
train() client id: f_00005-5-3 loss: 0.598317  [  128/  146]
train() client id: f_00005-6-0 loss: 0.645526  [   32/  146]
train() client id: f_00005-6-1 loss: 0.599312  [   64/  146]
train() client id: f_00005-6-2 loss: 0.488289  [   96/  146]
train() client id: f_00005-6-3 loss: 0.616818  [  128/  146]
train() client id: f_00005-7-0 loss: 0.895365  [   32/  146]
train() client id: f_00005-7-1 loss: 0.625806  [   64/  146]
train() client id: f_00005-7-2 loss: 0.623621  [   96/  146]
train() client id: f_00005-7-3 loss: 0.628626  [  128/  146]
train() client id: f_00005-8-0 loss: 0.519976  [   32/  146]
train() client id: f_00005-8-1 loss: 0.568230  [   64/  146]
train() client id: f_00005-8-2 loss: 0.646582  [   96/  146]
train() client id: f_00005-8-3 loss: 0.859789  [  128/  146]
train() client id: f_00005-9-0 loss: 0.704660  [   32/  146]
train() client id: f_00005-9-1 loss: 0.743666  [   64/  146]
train() client id: f_00005-9-2 loss: 0.454315  [   96/  146]
train() client id: f_00005-9-3 loss: 0.612007  [  128/  146]
train() client id: f_00006-0-0 loss: 0.498044  [   32/   54]
train() client id: f_00006-1-0 loss: 0.410632  [   32/   54]
train() client id: f_00006-2-0 loss: 0.501642  [   32/   54]
train() client id: f_00006-3-0 loss: 0.456022  [   32/   54]
train() client id: f_00006-4-0 loss: 0.457653  [   32/   54]
train() client id: f_00006-5-0 loss: 0.474832  [   32/   54]
train() client id: f_00006-6-0 loss: 0.484198  [   32/   54]
train() client id: f_00006-7-0 loss: 0.383872  [   32/   54]
train() client id: f_00006-8-0 loss: 0.506337  [   32/   54]
train() client id: f_00006-9-0 loss: 0.448088  [   32/   54]
train() client id: f_00007-0-0 loss: 0.638902  [   32/  179]
train() client id: f_00007-0-1 loss: 0.469057  [   64/  179]
train() client id: f_00007-0-2 loss: 0.856175  [   96/  179]
train() client id: f_00007-0-3 loss: 0.748283  [  128/  179]
train() client id: f_00007-0-4 loss: 0.474992  [  160/  179]
train() client id: f_00007-1-0 loss: 0.571368  [   32/  179]
train() client id: f_00007-1-1 loss: 0.688036  [   64/  179]
train() client id: f_00007-1-2 loss: 0.661351  [   96/  179]
train() client id: f_00007-1-3 loss: 0.544539  [  128/  179]
train() client id: f_00007-1-4 loss: 0.639539  [  160/  179]
train() client id: f_00007-2-0 loss: 0.779239  [   32/  179]
train() client id: f_00007-2-1 loss: 0.478299  [   64/  179]
train() client id: f_00007-2-2 loss: 0.606146  [   96/  179]
train() client id: f_00007-2-3 loss: 0.478024  [  128/  179]
train() client id: f_00007-2-4 loss: 0.766461  [  160/  179]
train() client id: f_00007-3-0 loss: 0.780615  [   32/  179]
train() client id: f_00007-3-1 loss: 0.520749  [   64/  179]
train() client id: f_00007-3-2 loss: 0.432991  [   96/  179]
train() client id: f_00007-3-3 loss: 0.659540  [  128/  179]
train() client id: f_00007-3-4 loss: 0.444862  [  160/  179]
train() client id: f_00007-4-0 loss: 0.759058  [   32/  179]
train() client id: f_00007-4-1 loss: 0.596937  [   64/  179]
train() client id: f_00007-4-2 loss: 0.714564  [   96/  179]
train() client id: f_00007-4-3 loss: 0.560103  [  128/  179]
train() client id: f_00007-4-4 loss: 0.487250  [  160/  179]
train() client id: f_00007-5-0 loss: 0.615150  [   32/  179]
train() client id: f_00007-5-1 loss: 0.682568  [   64/  179]
train() client id: f_00007-5-2 loss: 0.675918  [   96/  179]
train() client id: f_00007-5-3 loss: 0.450461  [  128/  179]
train() client id: f_00007-5-4 loss: 0.492414  [  160/  179]
train() client id: f_00007-6-0 loss: 0.476730  [   32/  179]
train() client id: f_00007-6-1 loss: 0.833345  [   64/  179]
train() client id: f_00007-6-2 loss: 0.532200  [   96/  179]
train() client id: f_00007-6-3 loss: 0.683978  [  128/  179]
train() client id: f_00007-6-4 loss: 0.520151  [  160/  179]
train() client id: f_00007-7-0 loss: 0.521428  [   32/  179]
train() client id: f_00007-7-1 loss: 0.902569  [   64/  179]
train() client id: f_00007-7-2 loss: 0.480870  [   96/  179]
train() client id: f_00007-7-3 loss: 0.541663  [  128/  179]
train() client id: f_00007-7-4 loss: 0.599425  [  160/  179]
train() client id: f_00007-8-0 loss: 0.541893  [   32/  179]
train() client id: f_00007-8-1 loss: 0.588419  [   64/  179]
train() client id: f_00007-8-2 loss: 0.657981  [   96/  179]
train() client id: f_00007-8-3 loss: 0.609428  [  128/  179]
train() client id: f_00007-8-4 loss: 0.601664  [  160/  179]
train() client id: f_00007-9-0 loss: 0.437240  [   32/  179]
train() client id: f_00007-9-1 loss: 0.423398  [   64/  179]
train() client id: f_00007-9-2 loss: 0.482954  [   96/  179]
train() client id: f_00007-9-3 loss: 0.617494  [  128/  179]
train() client id: f_00007-9-4 loss: 0.650489  [  160/  179]
train() client id: f_00008-0-0 loss: 0.530687  [   32/  130]
train() client id: f_00008-0-1 loss: 0.743021  [   64/  130]
train() client id: f_00008-0-2 loss: 0.533831  [   96/  130]
train() client id: f_00008-0-3 loss: 0.763498  [  128/  130]
train() client id: f_00008-1-0 loss: 0.663435  [   32/  130]
train() client id: f_00008-1-1 loss: 0.589584  [   64/  130]
train() client id: f_00008-1-2 loss: 0.679799  [   96/  130]
train() client id: f_00008-1-3 loss: 0.633677  [  128/  130]
train() client id: f_00008-2-0 loss: 0.700871  [   32/  130]
train() client id: f_00008-2-1 loss: 0.691961  [   64/  130]
train() client id: f_00008-2-2 loss: 0.526558  [   96/  130]
train() client id: f_00008-2-3 loss: 0.647847  [  128/  130]
train() client id: f_00008-3-0 loss: 0.706306  [   32/  130]
train() client id: f_00008-3-1 loss: 0.596330  [   64/  130]
train() client id: f_00008-3-2 loss: 0.585628  [   96/  130]
train() client id: f_00008-3-3 loss: 0.697174  [  128/  130]
train() client id: f_00008-4-0 loss: 0.618738  [   32/  130]
train() client id: f_00008-4-1 loss: 0.581862  [   64/  130]
train() client id: f_00008-4-2 loss: 0.702225  [   96/  130]
train() client id: f_00008-4-3 loss: 0.694943  [  128/  130]
train() client id: f_00008-5-0 loss: 0.700353  [   32/  130]
train() client id: f_00008-5-1 loss: 0.613917  [   64/  130]
train() client id: f_00008-5-2 loss: 0.726869  [   96/  130]
train() client id: f_00008-5-3 loss: 0.557830  [  128/  130]
train() client id: f_00008-6-0 loss: 0.611208  [   32/  130]
train() client id: f_00008-6-1 loss: 0.650857  [   64/  130]
train() client id: f_00008-6-2 loss: 0.692208  [   96/  130]
train() client id: f_00008-6-3 loss: 0.638880  [  128/  130]
train() client id: f_00008-7-0 loss: 0.570351  [   32/  130]
train() client id: f_00008-7-1 loss: 0.759545  [   64/  130]
train() client id: f_00008-7-2 loss: 0.596884  [   96/  130]
train() client id: f_00008-7-3 loss: 0.664690  [  128/  130]
train() client id: f_00008-8-0 loss: 0.556722  [   32/  130]
train() client id: f_00008-8-1 loss: 0.753232  [   64/  130]
train() client id: f_00008-8-2 loss: 0.716282  [   96/  130]
train() client id: f_00008-8-3 loss: 0.554804  [  128/  130]
train() client id: f_00008-9-0 loss: 0.560918  [   32/  130]
train() client id: f_00008-9-1 loss: 0.622154  [   64/  130]
train() client id: f_00008-9-2 loss: 0.674098  [   96/  130]
train() client id: f_00008-9-3 loss: 0.733600  [  128/  130]
train() client id: f_00009-0-0 loss: 1.042448  [   32/  118]
train() client id: f_00009-0-1 loss: 0.932099  [   64/  118]
train() client id: f_00009-0-2 loss: 0.919229  [   96/  118]
train() client id: f_00009-1-0 loss: 0.882181  [   32/  118]
train() client id: f_00009-1-1 loss: 0.912811  [   64/  118]
train() client id: f_00009-1-2 loss: 0.905927  [   96/  118]
train() client id: f_00009-2-0 loss: 1.048160  [   32/  118]
train() client id: f_00009-2-1 loss: 0.867813  [   64/  118]
train() client id: f_00009-2-2 loss: 0.970399  [   96/  118]
train() client id: f_00009-3-0 loss: 1.019397  [   32/  118]
train() client id: f_00009-3-1 loss: 0.775072  [   64/  118]
train() client id: f_00009-3-2 loss: 0.819144  [   96/  118]
train() client id: f_00009-4-0 loss: 0.760509  [   32/  118]
train() client id: f_00009-4-1 loss: 0.810989  [   64/  118]
train() client id: f_00009-4-2 loss: 0.954568  [   96/  118]
train() client id: f_00009-5-0 loss: 1.007767  [   32/  118]
train() client id: f_00009-5-1 loss: 0.766991  [   64/  118]
train() client id: f_00009-5-2 loss: 0.779725  [   96/  118]
train() client id: f_00009-6-0 loss: 0.830146  [   32/  118]
train() client id: f_00009-6-1 loss: 0.778865  [   64/  118]
train() client id: f_00009-6-2 loss: 0.935967  [   96/  118]
train() client id: f_00009-7-0 loss: 0.680141  [   32/  118]
train() client id: f_00009-7-1 loss: 0.972621  [   64/  118]
train() client id: f_00009-7-2 loss: 0.816174  [   96/  118]
train() client id: f_00009-8-0 loss: 0.799613  [   32/  118]
train() client id: f_00009-8-1 loss: 0.806123  [   64/  118]
train() client id: f_00009-8-2 loss: 0.810084  [   96/  118]
train() client id: f_00009-9-0 loss: 0.838873  [   32/  118]
train() client id: f_00009-9-1 loss: 0.801363  [   64/  118]
train() client id: f_00009-9-2 loss: 0.903599  [   96/  118]
At round 63 accuracy: 0.6472148541114059
At round 63 training accuracy: 0.5922199865861838
At round 63 training loss: 0.8168544628114182
update_location
xs = -3.905658 4.200318 335.009024 18.811294 0.979296 3.956410 -297.443192 -276.324852 319.663977 -262.060879 
ys = 327.587959 310.555839 1.320614 -297.455176 289.350187 272.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -15.71142616811743
ys mean: 92.39414253552872
dists_uav = 342.533393 326.286027 349.618063 314.377872 306.144557 290.591146 313.814185 293.864084 335.400847 280.520794 
uav_gains = -118.749823 -117.723996 -119.151344 -116.871916 -116.232646 -114.925202 -116.829435 -115.209763 -118.318240 -114.027110 
uav_gains_db_mean: -116.80394737835158
dists_bs = 229.251737 224.695659 538.770576 510.663011 209.868239 203.943631 215.738199 201.505782 519.111710 191.885616 
bs_gains = -105.655813 -105.411710 -116.046367 -115.394822 -104.581567 -104.233343 -104.917016 -104.087109 -115.594362 -103.492247 
bs_gains_db_mean: -107.94143588695076
Round 64
-------------------------------
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.55289596 5.10473648 2.52232289 0.94134025 5.8844195  2.83245373
 1.15055847 3.5161317  2.58559463 2.29678183]
obj_prev = 29.387235422694697
eta_min = 3.6403482068472025e-37	eta_max = 0.941453835955468
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 6.723535975581212	eta = 0.909090909090909
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 16.80493378903063	eta = 0.36372088751318643
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 11.200979803524296	eta = 0.5456938178232738
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.229583039555175	eta = 0.5975126658351408
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168941452928603	eta = 0.601075879986136
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168671162537121	eta = 0.6010918570034193
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168671157124352	eta = 0.6010918573233796
eta = 0.6010918573233796
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [0.04200208 0.08833771 0.04133538 0.01433404 0.10200505 0.04866908
 0.01800089 0.05966962 0.04333548 0.03933528]
ene_total = [1.03172726 1.51359867 1.04004204 0.52610415 1.72269629 0.88362426
 0.58229031 1.19121994 0.94449752 0.7328707 ]
ti_comp = [1.47039382 1.62666926 1.45842809 1.51593046 1.63009164 1.63144497
 1.51679335 1.54542978 1.53931646 1.63417731]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [2.14202768e-06 1.62824548e-05 2.07527568e-06 8.00990907e-08
 2.49643934e-05 2.70703886e-06 1.58456196e-07 5.55957146e-06
 2.14661920e-06 1.42438847e-06]
ene_total = [0.37266401 0.1276584  0.3914396  0.30117553 0.12242423 0.11995133
 0.29982271 0.25497116 0.26451064 0.11564362]
optimize_network iter = 0 obj = 2.3702612355707346
eta = 0.6010918573233796
freqs = [14282595.45433634 27152940.78877741 14171209.70092077  4727803.67422734
 31288134.07679844 14915942.00582812  5933864.17259814 19305186.95139418
 14076208.69920771 12035193.86722137]
eta_min = 0.6010918573233806	eta_max = 0.7423534842131878
af = 0.0007076189579367038	bf = 0.9451563237723394	zeta = 0.0007783808537303743	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [4.01088465e-07 3.04884240e-06 3.88589348e-07 1.49983222e-08
 4.67451020e-06 5.06885170e-07 2.96704627e-08 1.04101362e-06
 4.01948215e-07 2.66712607e-07]
ene_total = [1.63512592 0.5592525  1.71751674 1.3215506  0.53579916 0.52619371
 1.31561006 1.11849968 1.16054979 0.50736327]
ti_comp = [0.86560563 1.02188107 0.8536399  0.91114227 1.02530345 1.02665678
 0.91200517 0.9406416  0.93452827 1.02938912]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.23942807e-06 8.27345194e-06 1.21469007e-06 4.44613668e-08
 1.26534594e-05 1.37074871e-06 8.78894039e-08 3.00926595e-06
 1.16787135e-06 7.19841221e-07]
ene_total = [0.57696514 0.19745596 0.60603597 0.46630224 0.18924751 0.18568541
 0.46420685 0.394704   0.40951193 0.17903122]
optimize_network iter = 1 obj = 3.6691462360745932
eta = 0.7423534842131878
freqs = [14200721.70636506 25299122.79358197 14171209.70092077  4604073.01677737
 29115814.90637482 13873547.37129865  5776388.64214124 18564736.76689159
 13570968.78990086 11183104.37494726]
eta_min = 0.7423534842131893	eta_max = 0.7423534842131851
af = 0.0006257175094953287	bf = 0.9451563237723394	zeta = 0.0006882892604448616	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [3.96503235e-07 2.64674534e-06 3.88589348e-07 1.42235570e-08
 4.04794575e-06 4.38513790e-07 2.81165434e-08 9.62688933e-07
 3.73611652e-07 2.30283129e-07]
ene_total = [1.63512561 0.55922482 1.71751674 1.32155055 0.53575602 0.526189
 1.31560995 1.11849429 1.16054784 0.50736076]
ti_comp = [0.86560563 1.02188107 0.8536399  0.91114227 1.02530345 1.02665678
 0.91200517 0.9406416  0.93452827 1.02938912]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.23942807e-06 8.27345194e-06 1.21469007e-06 4.44613668e-08
 1.26534594e-05 1.37074871e-06 8.78894039e-08 3.00926595e-06
 1.16787135e-06 7.19841221e-07]
ene_total = [0.57696514 0.19745596 0.60603597 0.46630224 0.18924751 0.18568541
 0.46420685 0.394704   0.40951193 0.17903122]
optimize_network iter = 2 obj = 3.669146236074556
eta = 0.7423534842131851
freqs = [14200721.70636504 25299122.79358199 14171209.70092075  4604073.01677736
 29115814.90637484 13873547.37129866  5776388.64214123 18564736.76689158
 13570968.78990086 11183104.37494727]
Done!
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.14341502e-06 7.63254395e-06 1.12059337e-06 4.10171399e-08
 1.16732514e-05 1.26456283e-06 8.10809978e-08 2.77615133e-06
 1.07740148e-06 6.64078283e-07]
ene_total = [0.02374766 0.0081266  0.02494421 0.01919289 0.0077884  0.00764266
 0.01910664 0.01624569 0.01685533 0.00736883]
At round 64 energy consumption: 0.15101890456706119
At round 64 eta: 0.7423534842131851
At round 64 a_n: 6.259668650218071
At round 64 local rounds: 9.755733857653984
At round 64 global rounds: 24.295568799375985
gradient difference: 0.5164176225662231
train() client id: f_00000-0-0 loss: 1.058321  [   32/  126]
train() client id: f_00000-0-1 loss: 1.154392  [   64/  126]
train() client id: f_00000-0-2 loss: 0.895436  [   96/  126]
train() client id: f_00000-1-0 loss: 0.883751  [   32/  126]
train() client id: f_00000-1-1 loss: 0.899315  [   64/  126]
train() client id: f_00000-1-2 loss: 0.973930  [   96/  126]
train() client id: f_00000-2-0 loss: 0.860653  [   32/  126]
train() client id: f_00000-2-1 loss: 0.866793  [   64/  126]
train() client id: f_00000-2-2 loss: 1.069968  [   96/  126]
train() client id: f_00000-3-0 loss: 1.040143  [   32/  126]
train() client id: f_00000-3-1 loss: 0.867417  [   64/  126]
train() client id: f_00000-3-2 loss: 0.996006  [   96/  126]
train() client id: f_00000-4-0 loss: 0.778959  [   32/  126]
train() client id: f_00000-4-1 loss: 0.913021  [   64/  126]
train() client id: f_00000-4-2 loss: 1.025354  [   96/  126]
train() client id: f_00000-5-0 loss: 0.886341  [   32/  126]
train() client id: f_00000-5-1 loss: 1.004433  [   64/  126]
train() client id: f_00000-5-2 loss: 0.881803  [   96/  126]
train() client id: f_00000-6-0 loss: 0.873129  [   32/  126]
train() client id: f_00000-6-1 loss: 0.808639  [   64/  126]
train() client id: f_00000-6-2 loss: 0.826184  [   96/  126]
train() client id: f_00000-7-0 loss: 0.877802  [   32/  126]
train() client id: f_00000-7-1 loss: 0.850572  [   64/  126]
train() client id: f_00000-7-2 loss: 0.881652  [   96/  126]
train() client id: f_00000-8-0 loss: 0.931728  [   32/  126]
train() client id: f_00000-8-1 loss: 0.845057  [   64/  126]
train() client id: f_00000-8-2 loss: 0.809043  [   96/  126]
train() client id: f_00001-0-0 loss: 0.490865  [   32/  265]
train() client id: f_00001-0-1 loss: 0.401970  [   64/  265]
train() client id: f_00001-0-2 loss: 0.434456  [   96/  265]
train() client id: f_00001-0-3 loss: 0.469252  [  128/  265]
train() client id: f_00001-0-4 loss: 0.396818  [  160/  265]
train() client id: f_00001-0-5 loss: 0.411316  [  192/  265]
train() client id: f_00001-0-6 loss: 0.370523  [  224/  265]
train() client id: f_00001-0-7 loss: 0.322250  [  256/  265]
train() client id: f_00001-1-0 loss: 0.531099  [   32/  265]
train() client id: f_00001-1-1 loss: 0.360107  [   64/  265]
train() client id: f_00001-1-2 loss: 0.313229  [   96/  265]
train() client id: f_00001-1-3 loss: 0.451181  [  128/  265]
train() client id: f_00001-1-4 loss: 0.351436  [  160/  265]
train() client id: f_00001-1-5 loss: 0.328254  [  192/  265]
train() client id: f_00001-1-6 loss: 0.575542  [  224/  265]
train() client id: f_00001-1-7 loss: 0.327462  [  256/  265]
train() client id: f_00001-2-0 loss: 0.418858  [   32/  265]
train() client id: f_00001-2-1 loss: 0.295206  [   64/  265]
train() client id: f_00001-2-2 loss: 0.309091  [   96/  265]
train() client id: f_00001-2-3 loss: 0.413899  [  128/  265]
train() client id: f_00001-2-4 loss: 0.355519  [  160/  265]
train() client id: f_00001-2-5 loss: 0.508147  [  192/  265]
train() client id: f_00001-2-6 loss: 0.373181  [  224/  265]
train() client id: f_00001-2-7 loss: 0.496947  [  256/  265]
train() client id: f_00001-3-0 loss: 0.422677  [   32/  265]
train() client id: f_00001-3-1 loss: 0.293662  [   64/  265]
train() client id: f_00001-3-2 loss: 0.513398  [   96/  265]
train() client id: f_00001-3-3 loss: 0.292589  [  128/  265]
train() client id: f_00001-3-4 loss: 0.439390  [  160/  265]
train() client id: f_00001-3-5 loss: 0.397938  [  192/  265]
train() client id: f_00001-3-6 loss: 0.394745  [  224/  265]
train() client id: f_00001-3-7 loss: 0.321045  [  256/  265]
train() client id: f_00001-4-0 loss: 0.399743  [   32/  265]
train() client id: f_00001-4-1 loss: 0.472409  [   64/  265]
train() client id: f_00001-4-2 loss: 0.315800  [   96/  265]
train() client id: f_00001-4-3 loss: 0.443734  [  128/  265]
train() client id: f_00001-4-4 loss: 0.466759  [  160/  265]
train() client id: f_00001-4-5 loss: 0.357662  [  192/  265]
train() client id: f_00001-4-6 loss: 0.388505  [  224/  265]
train() client id: f_00001-4-7 loss: 0.281180  [  256/  265]
train() client id: f_00001-5-0 loss: 0.378561  [   32/  265]
train() client id: f_00001-5-1 loss: 0.489638  [   64/  265]
train() client id: f_00001-5-2 loss: 0.277242  [   96/  265]
train() client id: f_00001-5-3 loss: 0.393574  [  128/  265]
train() client id: f_00001-5-4 loss: 0.389040  [  160/  265]
train() client id: f_00001-5-5 loss: 0.272909  [  192/  265]
train() client id: f_00001-5-6 loss: 0.389699  [  224/  265]
train() client id: f_00001-5-7 loss: 0.526373  [  256/  265]
train() client id: f_00001-6-0 loss: 0.633460  [   32/  265]
train() client id: f_00001-6-1 loss: 0.367212  [   64/  265]
train() client id: f_00001-6-2 loss: 0.316167  [   96/  265]
train() client id: f_00001-6-3 loss: 0.326430  [  128/  265]
train() client id: f_00001-6-4 loss: 0.382863  [  160/  265]
train() client id: f_00001-6-5 loss: 0.382954  [  192/  265]
train() client id: f_00001-6-6 loss: 0.351846  [  224/  265]
train() client id: f_00001-6-7 loss: 0.343182  [  256/  265]
train() client id: f_00001-7-0 loss: 0.338823  [   32/  265]
train() client id: f_00001-7-1 loss: 0.283860  [   64/  265]
train() client id: f_00001-7-2 loss: 0.343997  [   96/  265]
train() client id: f_00001-7-3 loss: 0.288167  [  128/  265]
train() client id: f_00001-7-4 loss: 0.388553  [  160/  265]
train() client id: f_00001-7-5 loss: 0.376923  [  192/  265]
train() client id: f_00001-7-6 loss: 0.487524  [  224/  265]
train() client id: f_00001-7-7 loss: 0.582057  [  256/  265]
train() client id: f_00001-8-0 loss: 0.379884  [   32/  265]
train() client id: f_00001-8-1 loss: 0.414727  [   64/  265]
train() client id: f_00001-8-2 loss: 0.357123  [   96/  265]
train() client id: f_00001-8-3 loss: 0.376508  [  128/  265]
train() client id: f_00001-8-4 loss: 0.391838  [  160/  265]
train() client id: f_00001-8-5 loss: 0.314809  [  192/  265]
train() client id: f_00001-8-6 loss: 0.315927  [  224/  265]
train() client id: f_00001-8-7 loss: 0.388245  [  256/  265]
train() client id: f_00002-0-0 loss: 1.190987  [   32/  124]
train() client id: f_00002-0-1 loss: 1.145022  [   64/  124]
train() client id: f_00002-0-2 loss: 1.374614  [   96/  124]
train() client id: f_00002-1-0 loss: 1.133985  [   32/  124]
train() client id: f_00002-1-1 loss: 1.142714  [   64/  124]
train() client id: f_00002-1-2 loss: 0.978360  [   96/  124]
train() client id: f_00002-2-0 loss: 0.970472  [   32/  124]
train() client id: f_00002-2-1 loss: 1.370311  [   64/  124]
train() client id: f_00002-2-2 loss: 1.114849  [   96/  124]
train() client id: f_00002-3-0 loss: 1.113365  [   32/  124]
train() client id: f_00002-3-1 loss: 1.258016  [   64/  124]
train() client id: f_00002-3-2 loss: 1.183306  [   96/  124]
train() client id: f_00002-4-0 loss: 1.149737  [   32/  124]
train() client id: f_00002-4-1 loss: 1.029849  [   64/  124]
train() client id: f_00002-4-2 loss: 1.213929  [   96/  124]
train() client id: f_00002-5-0 loss: 1.060623  [   32/  124]
train() client id: f_00002-5-1 loss: 1.070719  [   64/  124]
train() client id: f_00002-5-2 loss: 1.192926  [   96/  124]
train() client id: f_00002-6-0 loss: 1.137354  [   32/  124]
train() client id: f_00002-6-1 loss: 1.323165  [   64/  124]
train() client id: f_00002-6-2 loss: 0.981716  [   96/  124]
train() client id: f_00002-7-0 loss: 1.047046  [   32/  124]
train() client id: f_00002-7-1 loss: 0.955509  [   64/  124]
train() client id: f_00002-7-2 loss: 1.135585  [   96/  124]
train() client id: f_00002-8-0 loss: 1.013201  [   32/  124]
train() client id: f_00002-8-1 loss: 0.939969  [   64/  124]
train() client id: f_00002-8-2 loss: 1.108854  [   96/  124]
train() client id: f_00003-0-0 loss: 0.636565  [   32/   43]
train() client id: f_00003-1-0 loss: 0.791901  [   32/   43]
train() client id: f_00003-2-0 loss: 0.874421  [   32/   43]
train() client id: f_00003-3-0 loss: 0.779306  [   32/   43]
train() client id: f_00003-4-0 loss: 0.626748  [   32/   43]
train() client id: f_00003-5-0 loss: 0.751692  [   32/   43]
train() client id: f_00003-6-0 loss: 0.917476  [   32/   43]
train() client id: f_00003-7-0 loss: 0.747643  [   32/   43]
train() client id: f_00003-8-0 loss: 0.654951  [   32/   43]
train() client id: f_00004-0-0 loss: 0.744165  [   32/  306]
train() client id: f_00004-0-1 loss: 0.873271  [   64/  306]
train() client id: f_00004-0-2 loss: 0.915173  [   96/  306]
train() client id: f_00004-0-3 loss: 0.991365  [  128/  306]
train() client id: f_00004-0-4 loss: 0.822762  [  160/  306]
train() client id: f_00004-0-5 loss: 0.711723  [  192/  306]
train() client id: f_00004-0-6 loss: 0.844427  [  224/  306]
train() client id: f_00004-0-7 loss: 0.734328  [  256/  306]
train() client id: f_00004-0-8 loss: 0.770848  [  288/  306]
train() client id: f_00004-1-0 loss: 0.753056  [   32/  306]
train() client id: f_00004-1-1 loss: 0.739608  [   64/  306]
train() client id: f_00004-1-2 loss: 0.781637  [   96/  306]
train() client id: f_00004-1-3 loss: 0.815864  [  128/  306]
train() client id: f_00004-1-4 loss: 0.852800  [  160/  306]
train() client id: f_00004-1-5 loss: 0.768423  [  192/  306]
train() client id: f_00004-1-6 loss: 0.839178  [  224/  306]
train() client id: f_00004-1-7 loss: 0.874835  [  256/  306]
train() client id: f_00004-1-8 loss: 0.889587  [  288/  306]
train() client id: f_00004-2-0 loss: 0.781418  [   32/  306]
train() client id: f_00004-2-1 loss: 0.907964  [   64/  306]
train() client id: f_00004-2-2 loss: 0.765017  [   96/  306]
train() client id: f_00004-2-3 loss: 0.910267  [  128/  306]
train() client id: f_00004-2-4 loss: 0.854362  [  160/  306]
train() client id: f_00004-2-5 loss: 0.851440  [  192/  306]
train() client id: f_00004-2-6 loss: 0.724104  [  224/  306]
train() client id: f_00004-2-7 loss: 0.869103  [  256/  306]
train() client id: f_00004-2-8 loss: 0.736023  [  288/  306]
train() client id: f_00004-3-0 loss: 0.971493  [   32/  306]
train() client id: f_00004-3-1 loss: 0.756496  [   64/  306]
train() client id: f_00004-3-2 loss: 0.960425  [   96/  306]
train() client id: f_00004-3-3 loss: 0.919865  [  128/  306]
train() client id: f_00004-3-4 loss: 0.745235  [  160/  306]
train() client id: f_00004-3-5 loss: 0.734891  [  192/  306]
train() client id: f_00004-3-6 loss: 0.767902  [  224/  306]
train() client id: f_00004-3-7 loss: 0.856839  [  256/  306]
train() client id: f_00004-3-8 loss: 0.777972  [  288/  306]
train() client id: f_00004-4-0 loss: 0.772992  [   32/  306]
train() client id: f_00004-4-1 loss: 0.694090  [   64/  306]
train() client id: f_00004-4-2 loss: 0.884959  [   96/  306]
train() client id: f_00004-4-3 loss: 0.863186  [  128/  306]
train() client id: f_00004-4-4 loss: 0.726023  [  160/  306]
train() client id: f_00004-4-5 loss: 0.768523  [  192/  306]
train() client id: f_00004-4-6 loss: 0.851657  [  224/  306]
train() client id: f_00004-4-7 loss: 0.746482  [  256/  306]
train() client id: f_00004-4-8 loss: 1.087693  [  288/  306]
train() client id: f_00004-5-0 loss: 0.697873  [   32/  306]
train() client id: f_00004-5-1 loss: 0.745686  [   64/  306]
train() client id: f_00004-5-2 loss: 0.805951  [   96/  306]
train() client id: f_00004-5-3 loss: 0.771422  [  128/  306]
train() client id: f_00004-5-4 loss: 0.869298  [  160/  306]
train() client id: f_00004-5-5 loss: 0.975554  [  192/  306]
train() client id: f_00004-5-6 loss: 0.873115  [  224/  306]
train() client id: f_00004-5-7 loss: 0.781234  [  256/  306]
train() client id: f_00004-5-8 loss: 0.916254  [  288/  306]
train() client id: f_00004-6-0 loss: 0.752622  [   32/  306]
train() client id: f_00004-6-1 loss: 0.759763  [   64/  306]
train() client id: f_00004-6-2 loss: 0.799635  [   96/  306]
train() client id: f_00004-6-3 loss: 0.905038  [  128/  306]
train() client id: f_00004-6-4 loss: 0.868328  [  160/  306]
train() client id: f_00004-6-5 loss: 1.016380  [  192/  306]
train() client id: f_00004-6-6 loss: 0.718795  [  224/  306]
train() client id: f_00004-6-7 loss: 0.829610  [  256/  306]
train() client id: f_00004-6-8 loss: 0.775579  [  288/  306]
train() client id: f_00004-7-0 loss: 0.749637  [   32/  306]
train() client id: f_00004-7-1 loss: 0.853578  [   64/  306]
train() client id: f_00004-7-2 loss: 0.902500  [   96/  306]
train() client id: f_00004-7-3 loss: 0.893157  [  128/  306]
train() client id: f_00004-7-4 loss: 0.788697  [  160/  306]
train() client id: f_00004-7-5 loss: 0.736681  [  192/  306]
train() client id: f_00004-7-6 loss: 0.781378  [  224/  306]
train() client id: f_00004-7-7 loss: 0.833185  [  256/  306]
train() client id: f_00004-7-8 loss: 0.845364  [  288/  306]
train() client id: f_00004-8-0 loss: 0.860689  [   32/  306]
train() client id: f_00004-8-1 loss: 0.855173  [   64/  306]
train() client id: f_00004-8-2 loss: 0.771143  [   96/  306]
train() client id: f_00004-8-3 loss: 0.915914  [  128/  306]
train() client id: f_00004-8-4 loss: 0.819060  [  160/  306]
train() client id: f_00004-8-5 loss: 0.774845  [  192/  306]
train() client id: f_00004-8-6 loss: 0.851473  [  224/  306]
train() client id: f_00004-8-7 loss: 0.778541  [  256/  306]
train() client id: f_00004-8-8 loss: 0.828821  [  288/  306]
train() client id: f_00005-0-0 loss: 0.514200  [   32/  146]
train() client id: f_00005-0-1 loss: 0.642110  [   64/  146]
train() client id: f_00005-0-2 loss: 0.658097  [   96/  146]
train() client id: f_00005-0-3 loss: 0.694379  [  128/  146]
train() client id: f_00005-1-0 loss: 0.641552  [   32/  146]
train() client id: f_00005-1-1 loss: 0.807411  [   64/  146]
train() client id: f_00005-1-2 loss: 0.621472  [   96/  146]
train() client id: f_00005-1-3 loss: 0.661050  [  128/  146]
train() client id: f_00005-2-0 loss: 0.540864  [   32/  146]
train() client id: f_00005-2-1 loss: 0.669367  [   64/  146]
train() client id: f_00005-2-2 loss: 0.746302  [   96/  146]
train() client id: f_00005-2-3 loss: 0.828809  [  128/  146]
train() client id: f_00005-3-0 loss: 0.596846  [   32/  146]
train() client id: f_00005-3-1 loss: 0.570592  [   64/  146]
train() client id: f_00005-3-2 loss: 0.754534  [   96/  146]
train() client id: f_00005-3-3 loss: 0.876914  [  128/  146]
train() client id: f_00005-4-0 loss: 0.518889  [   32/  146]
train() client id: f_00005-4-1 loss: 0.644285  [   64/  146]
train() client id: f_00005-4-2 loss: 0.355977  [   96/  146]
train() client id: f_00005-4-3 loss: 0.823283  [  128/  146]
train() client id: f_00005-5-0 loss: 0.778280  [   32/  146]
train() client id: f_00005-5-1 loss: 0.353122  [   64/  146]
train() client id: f_00005-5-2 loss: 0.966940  [   96/  146]
train() client id: f_00005-5-3 loss: 0.581424  [  128/  146]
train() client id: f_00005-6-0 loss: 0.804651  [   32/  146]
train() client id: f_00005-6-1 loss: 0.435982  [   64/  146]
train() client id: f_00005-6-2 loss: 0.819924  [   96/  146]
train() client id: f_00005-6-3 loss: 0.666755  [  128/  146]
train() client id: f_00005-7-0 loss: 0.637559  [   32/  146]
train() client id: f_00005-7-1 loss: 0.655904  [   64/  146]
train() client id: f_00005-7-2 loss: 0.942041  [   96/  146]
train() client id: f_00005-7-3 loss: 0.694631  [  128/  146]
train() client id: f_00005-8-0 loss: 0.740783  [   32/  146]
train() client id: f_00005-8-1 loss: 0.465575  [   64/  146]
train() client id: f_00005-8-2 loss: 0.595337  [   96/  146]
train() client id: f_00005-8-3 loss: 0.822866  [  128/  146]
train() client id: f_00006-0-0 loss: 0.523311  [   32/   54]
train() client id: f_00006-1-0 loss: 0.445147  [   32/   54]
train() client id: f_00006-2-0 loss: 0.495964  [   32/   54]
train() client id: f_00006-3-0 loss: 0.538990  [   32/   54]
train() client id: f_00006-4-0 loss: 0.506081  [   32/   54]
train() client id: f_00006-5-0 loss: 0.508176  [   32/   54]
train() client id: f_00006-6-0 loss: 0.546996  [   32/   54]
train() client id: f_00006-7-0 loss: 0.553703  [   32/   54]
train() client id: f_00006-8-0 loss: 0.512113  [   32/   54]
train() client id: f_00007-0-0 loss: 0.670682  [   32/  179]
train() client id: f_00007-0-1 loss: 0.667618  [   64/  179]
train() client id: f_00007-0-2 loss: 0.370044  [   96/  179]
train() client id: f_00007-0-3 loss: 0.533754  [  128/  179]
train() client id: f_00007-0-4 loss: 0.464963  [  160/  179]
train() client id: f_00007-1-0 loss: 0.670970  [   32/  179]
train() client id: f_00007-1-1 loss: 0.433971  [   64/  179]
train() client id: f_00007-1-2 loss: 0.537512  [   96/  179]
train() client id: f_00007-1-3 loss: 0.570942  [  128/  179]
train() client id: f_00007-1-4 loss: 0.415012  [  160/  179]
train() client id: f_00007-2-0 loss: 0.546231  [   32/  179]
train() client id: f_00007-2-1 loss: 0.448706  [   64/  179]
train() client id: f_00007-2-2 loss: 0.473191  [   96/  179]
train() client id: f_00007-2-3 loss: 0.750608  [  128/  179]
train() client id: f_00007-2-4 loss: 0.348473  [  160/  179]
train() client id: f_00007-3-0 loss: 0.512351  [   32/  179]
train() client id: f_00007-3-1 loss: 0.725072  [   64/  179]
train() client id: f_00007-3-2 loss: 0.426310  [   96/  179]
train() client id: f_00007-3-3 loss: 0.481957  [  128/  179]
train() client id: f_00007-3-4 loss: 0.537467  [  160/  179]
train() client id: f_00007-4-0 loss: 0.534735  [   32/  179]
train() client id: f_00007-4-1 loss: 0.436086  [   64/  179]
train() client id: f_00007-4-2 loss: 0.412134  [   96/  179]
train() client id: f_00007-4-3 loss: 0.807949  [  128/  179]
train() client id: f_00007-4-4 loss: 0.472437  [  160/  179]
train() client id: f_00007-5-0 loss: 0.445457  [   32/  179]
train() client id: f_00007-5-1 loss: 0.694714  [   64/  179]
train() client id: f_00007-5-2 loss: 0.540618  [   96/  179]
train() client id: f_00007-5-3 loss: 0.541726  [  128/  179]
train() client id: f_00007-5-4 loss: 0.431495  [  160/  179]
train() client id: f_00007-6-0 loss: 0.509692  [   32/  179]
train() client id: f_00007-6-1 loss: 0.580777  [   64/  179]
train() client id: f_00007-6-2 loss: 0.537555  [   96/  179]
train() client id: f_00007-6-3 loss: 0.630962  [  128/  179]
train() client id: f_00007-6-4 loss: 0.329794  [  160/  179]
train() client id: f_00007-7-0 loss: 0.445174  [   32/  179]
train() client id: f_00007-7-1 loss: 0.532823  [   64/  179]
train() client id: f_00007-7-2 loss: 0.684014  [   96/  179]
train() client id: f_00007-7-3 loss: 0.432596  [  128/  179]
train() client id: f_00007-7-4 loss: 0.511379  [  160/  179]
train() client id: f_00007-8-0 loss: 0.386789  [   32/  179]
train() client id: f_00007-8-1 loss: 0.447568  [   64/  179]
train() client id: f_00007-8-2 loss: 0.515875  [   96/  179]
train() client id: f_00007-8-3 loss: 0.580397  [  128/  179]
train() client id: f_00007-8-4 loss: 0.644447  [  160/  179]
train() client id: f_00008-0-0 loss: 0.756079  [   32/  130]
train() client id: f_00008-0-1 loss: 0.658838  [   64/  130]
train() client id: f_00008-0-2 loss: 0.639764  [   96/  130]
train() client id: f_00008-0-3 loss: 0.534784  [  128/  130]
train() client id: f_00008-1-0 loss: 0.659940  [   32/  130]
train() client id: f_00008-1-1 loss: 0.689236  [   64/  130]
train() client id: f_00008-1-2 loss: 0.613155  [   96/  130]
train() client id: f_00008-1-3 loss: 0.651354  [  128/  130]
train() client id: f_00008-2-0 loss: 0.627867  [   32/  130]
train() client id: f_00008-2-1 loss: 0.702623  [   64/  130]
train() client id: f_00008-2-2 loss: 0.641772  [   96/  130]
train() client id: f_00008-2-3 loss: 0.641632  [  128/  130]
train() client id: f_00008-3-0 loss: 0.492851  [   32/  130]
train() client id: f_00008-3-1 loss: 0.654269  [   64/  130]
train() client id: f_00008-3-2 loss: 0.601270  [   96/  130]
train() client id: f_00008-3-3 loss: 0.827857  [  128/  130]
train() client id: f_00008-4-0 loss: 0.721658  [   32/  130]
train() client id: f_00008-4-1 loss: 0.709624  [   64/  130]
train() client id: f_00008-4-2 loss: 0.642010  [   96/  130]
train() client id: f_00008-4-3 loss: 0.545682  [  128/  130]
train() client id: f_00008-5-0 loss: 0.654608  [   32/  130]
train() client id: f_00008-5-1 loss: 0.603345  [   64/  130]
train() client id: f_00008-5-2 loss: 0.582153  [   96/  130]
train() client id: f_00008-5-3 loss: 0.738521  [  128/  130]
train() client id: f_00008-6-0 loss: 0.701624  [   32/  130]
train() client id: f_00008-6-1 loss: 0.615565  [   64/  130]
train() client id: f_00008-6-2 loss: 0.574369  [   96/  130]
train() client id: f_00008-6-3 loss: 0.726239  [  128/  130]
train() client id: f_00008-7-0 loss: 0.722496  [   32/  130]
train() client id: f_00008-7-1 loss: 0.665085  [   64/  130]
train() client id: f_00008-7-2 loss: 0.611341  [   96/  130]
train() client id: f_00008-7-3 loss: 0.625772  [  128/  130]
train() client id: f_00008-8-0 loss: 0.589399  [   32/  130]
train() client id: f_00008-8-1 loss: 0.729236  [   64/  130]
train() client id: f_00008-8-2 loss: 0.660653  [   96/  130]
train() client id: f_00008-8-3 loss: 0.594927  [  128/  130]
train() client id: f_00009-0-0 loss: 0.875949  [   32/  118]
train() client id: f_00009-0-1 loss: 0.936294  [   64/  118]
train() client id: f_00009-0-2 loss: 0.913281  [   96/  118]
train() client id: f_00009-1-0 loss: 0.706328  [   32/  118]
train() client id: f_00009-1-1 loss: 0.943116  [   64/  118]
train() client id: f_00009-1-2 loss: 0.956088  [   96/  118]
train() client id: f_00009-2-0 loss: 0.924596  [   32/  118]
train() client id: f_00009-2-1 loss: 0.743989  [   64/  118]
train() client id: f_00009-2-2 loss: 0.841079  [   96/  118]
train() client id: f_00009-3-0 loss: 0.725524  [   32/  118]
train() client id: f_00009-3-1 loss: 0.832767  [   64/  118]
train() client id: f_00009-3-2 loss: 0.933837  [   96/  118]
train() client id: f_00009-4-0 loss: 0.607900  [   32/  118]
train() client id: f_00009-4-1 loss: 0.819867  [   64/  118]
train() client id: f_00009-4-2 loss: 0.848285  [   96/  118]
train() client id: f_00009-5-0 loss: 0.746479  [   32/  118]
train() client id: f_00009-5-1 loss: 0.713692  [   64/  118]
train() client id: f_00009-5-2 loss: 0.718304  [   96/  118]
train() client id: f_00009-6-0 loss: 0.762002  [   32/  118]
train() client id: f_00009-6-1 loss: 0.834308  [   64/  118]
train() client id: f_00009-6-2 loss: 0.675705  [   96/  118]
train() client id: f_00009-7-0 loss: 0.685165  [   32/  118]
train() client id: f_00009-7-1 loss: 0.903764  [   64/  118]
train() client id: f_00009-7-2 loss: 0.727093  [   96/  118]
train() client id: f_00009-8-0 loss: 0.756572  [   32/  118]
train() client id: f_00009-8-1 loss: 0.729766  [   64/  118]
train() client id: f_00009-8-2 loss: 0.652594  [   96/  118]
At round 64 accuracy: 0.6472148541114059
At round 64 training accuracy: 0.5881958417169685
At round 64 training loss: 0.8291796714136584
update_location
xs = -3.905658 4.200318 340.009024 18.811294 0.979296 3.956410 -302.443192 -281.324852 324.663977 -267.060879 
ys = 332.587959 315.555839 1.320614 -302.455176 294.350187 277.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -16.21142616811743
ys mean: 93.89414253552872
dists_uav = 347.318305 331.048531 354.412021 319.112830 310.874559 295.290290 318.557334 298.570508 340.169616 285.197343 
uav_gains = -119.023854 -118.040629 -119.408694 -117.221114 -116.604801 -115.332351 -117.180856 -115.610749 -118.609927 -114.447716 
uav_gains_db_mean: -117.14806912447918
dists_bs = 232.609627 227.747004 543.506048 515.292405 212.633910 206.388338 218.614735 204.065746 523.878428 194.205300 
bs_gains = -105.832634 -105.575734 -116.152782 -115.504564 -104.740770 -104.378243 -105.078083 -104.240622 -115.705514 -103.638369 
bs_gains_db_mean: -108.08473157357538
Round 65
-------------------------------
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.41801623 4.82581662 2.38917526 0.89421619 5.56278746 2.67778749
 1.09198131 3.3275343  2.44520381 2.17140866]
obj_prev = 27.80392733965525
eta_min = 3.100306852884529e-39	eta_max = 0.942754072368237
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 6.3556060652170405	eta = 0.9090909090909091
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 16.15583264303265	eta = 0.3576308212219315
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 10.677811898063974	eta = 0.5411055889361986
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.733010612728588	eta = 0.5936317061131898
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673834452832171	eta = 0.597263031926322
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673568210715082	eta = 0.5972794701806058
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673568205281397	eta = 0.5972794705161003
eta = 0.5972794705161003
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [0.04252711 0.08944194 0.04185208 0.01451322 0.10328012 0.04927744
 0.0182259  0.0604155  0.04387718 0.03982698]
ene_total = [0.98560292 1.43449508 0.99341101 0.50602748 1.63267669 0.83712064
 0.55929955 1.13571028 0.89502304 0.69420151]
ti_comp = [1.57505947 1.73869087 1.56299493 1.62132055 1.74218994 1.74362015
 1.62219032 1.65177999 1.65020898 1.7463865 ]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.93768734e-06 1.47930885e-05 1.87549493e-06 7.26831216e-08
 2.26850246e-05 2.45991889e-06 1.43794955e-07 5.05150367e-06
 1.93873362e-06 1.29458708e-06]
ene_total = [0.36078585 0.12055314 0.37851121 0.29278761 0.11552795 0.1131294
 0.29151072 0.24810709 0.25036961 0.10904772]
optimize_network iter = 0 obj = 2.280330304694935
eta = 0.5972794705161003
freqs = [13500159.94622198 25721057.81080511 13388423.45249819  4475740.44373556
 29640890.63331935 14130785.41729783  5617683.65648641 18287997.60708139
 13294430.26761132 11402680.79036755]
eta_min = 0.5972794705161009	eta_max = 0.7502409697515174
af = 0.0005999271736932453	bf = 0.9180266431472464	zeta = 0.0006599198910625699	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [3.58346965e-07 2.73576559e-06 3.46845386e-07 1.34416814e-08
 4.19526387e-06 4.54926060e-07 2.65927760e-08 9.34201802e-07
 3.58540459e-07 2.39414968e-07]
ene_total = [1.59815174 0.53325669 1.67667707 1.29702355 0.51057679 0.50102436
 1.29136322 1.09882803 1.10901597 0.48300462]
ti_comp = [0.8835615  1.0471929  0.87149697 0.92982258 1.05069198 1.05212218
 0.93069235 0.96028202 0.95871102 1.05488853]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.06740436e-06 7.06927946e-06 1.04574117e-06 3.83085836e-08
 1.08119582e-05 1.17116240e-06 7.57285947e-08 2.59091048e-06
 9.95739029e-07 6.15068927e-07]
ene_total = [0.58172358 0.19420128 0.61030556 0.47210056 0.18600018 0.18238342
 0.47004085 0.39999859 0.40368272 0.1758164 ]
optimize_network iter = 1 obj = 3.676253139763781
eta = 0.7502409697515174
freqs = [13418605.79030925 23811832.48313907 13388423.4524982   4351530.1809025
 27404358.81401067 13057508.37108007  5459605.33203652 17539930.50999551
 12759371.1105045  10525653.53083322]
eta_min = 0.7502409697515208	eta_max = 0.7502409697515149
af = 0.00052469220506583	bf = 0.9180266431472464	zeta = 0.000577161425572413	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [3.54030510e-07 2.34469776e-06 3.46845386e-07 1.27059696e-08
 3.58604780e-06 3.88444377e-07 2.51172226e-08 8.59338214e-07
 3.30260967e-07 2.04002507e-07]
ene_total = [1.59815146 0.53323124 1.67667707 1.2970235  0.51053714 0.50102003
 1.29136313 1.09882316 1.10901413 0.48300231]
ti_comp = [0.8835615  1.0471929  0.87149697 0.92982258 1.05069198 1.05212218
 0.93069235 0.96028202 0.95871102 1.05488853]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.06740436e-06 7.06927946e-06 1.04574117e-06 3.83085836e-08
 1.08119582e-05 1.17116240e-06 7.57285947e-08 2.59091048e-06
 9.95739029e-07 6.15068927e-07]
ene_total = [0.58172358 0.19420128 0.61030556 0.47210056 0.18600018 0.18238342
 0.47004085 0.39999859 0.40368272 0.1758164 ]
optimize_network iter = 2 obj = 3.6762531397637437
eta = 0.7502409697515149
freqs = [13418605.79030923 23811832.48313908 13388423.45249818  4351530.1809025
 27404358.81401069 13057508.37108008  5459605.33203651 17539930.5099955
 12759371.11050449 10525653.53083322]
Done!
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.02093442e-06 6.76151514e-06 1.00021434e-06 3.66408019e-08
 1.03412546e-05 1.12017531e-06 7.24317156e-08 2.47811400e-06
 9.52389074e-07 5.88291620e-07]
ene_total = [0.02455424 0.00819684 0.02576067 0.01992715 0.00785051 0.00769827
 0.0198402  0.01688364 0.01703922 0.0074211 ]
At round 65 energy consumption: 0.15517184374296747
At round 65 eta: 0.7502409697515149
At round 65 a_n: 5.917122803248752
At round 65 local rounds: 9.409653576790653
At round 65 global rounds: 23.691326785509258
gradient difference: 0.6591960787773132
train() client id: f_00000-0-0 loss: 1.021950  [   32/  126]
train() client id: f_00000-0-1 loss: 1.037195  [   64/  126]
train() client id: f_00000-0-2 loss: 1.200451  [   96/  126]
train() client id: f_00000-1-0 loss: 1.217903  [   32/  126]
train() client id: f_00000-1-1 loss: 1.053628  [   64/  126]
train() client id: f_00000-1-2 loss: 0.940986  [   96/  126]
train() client id: f_00000-2-0 loss: 1.163815  [   32/  126]
train() client id: f_00000-2-1 loss: 0.982353  [   64/  126]
train() client id: f_00000-2-2 loss: 0.938534  [   96/  126]
train() client id: f_00000-3-0 loss: 1.101524  [   32/  126]
train() client id: f_00000-3-1 loss: 0.782706  [   64/  126]
train() client id: f_00000-3-2 loss: 1.052117  [   96/  126]
train() client id: f_00000-4-0 loss: 0.925288  [   32/  126]
train() client id: f_00000-4-1 loss: 0.925785  [   64/  126]
train() client id: f_00000-4-2 loss: 0.999874  [   96/  126]
train() client id: f_00000-5-0 loss: 0.970512  [   32/  126]
train() client id: f_00000-5-1 loss: 0.767801  [   64/  126]
train() client id: f_00000-5-2 loss: 1.046484  [   96/  126]
train() client id: f_00000-6-0 loss: 0.913316  [   32/  126]
train() client id: f_00000-6-1 loss: 0.979761  [   64/  126]
train() client id: f_00000-6-2 loss: 0.853119  [   96/  126]
train() client id: f_00000-7-0 loss: 0.849375  [   32/  126]
train() client id: f_00000-7-1 loss: 0.880177  [   64/  126]
train() client id: f_00000-7-2 loss: 0.966411  [   96/  126]
train() client id: f_00000-8-0 loss: 0.994326  [   32/  126]
train() client id: f_00000-8-1 loss: 0.897013  [   64/  126]
train() client id: f_00000-8-2 loss: 0.938129  [   96/  126]
train() client id: f_00001-0-0 loss: 0.410918  [   32/  265]
train() client id: f_00001-0-1 loss: 0.501962  [   64/  265]
train() client id: f_00001-0-2 loss: 0.424996  [   96/  265]
train() client id: f_00001-0-3 loss: 0.434448  [  128/  265]
train() client id: f_00001-0-4 loss: 0.470326  [  160/  265]
train() client id: f_00001-0-5 loss: 0.481327  [  192/  265]
train() client id: f_00001-0-6 loss: 0.510686  [  224/  265]
train() client id: f_00001-0-7 loss: 0.390142  [  256/  265]
train() client id: f_00001-1-0 loss: 0.412460  [   32/  265]
train() client id: f_00001-1-1 loss: 0.479377  [   64/  265]
train() client id: f_00001-1-2 loss: 0.461556  [   96/  265]
train() client id: f_00001-1-3 loss: 0.429869  [  128/  265]
train() client id: f_00001-1-4 loss: 0.439662  [  160/  265]
train() client id: f_00001-1-5 loss: 0.462845  [  192/  265]
train() client id: f_00001-1-6 loss: 0.353473  [  224/  265]
train() client id: f_00001-1-7 loss: 0.587022  [  256/  265]
train() client id: f_00001-2-0 loss: 0.383257  [   32/  265]
train() client id: f_00001-2-1 loss: 0.491492  [   64/  265]
train() client id: f_00001-2-2 loss: 0.373781  [   96/  265]
train() client id: f_00001-2-3 loss: 0.412663  [  128/  265]
train() client id: f_00001-2-4 loss: 0.437108  [  160/  265]
train() client id: f_00001-2-5 loss: 0.503104  [  192/  265]
train() client id: f_00001-2-6 loss: 0.544733  [  224/  265]
train() client id: f_00001-2-7 loss: 0.369556  [  256/  265]
train() client id: f_00001-3-0 loss: 0.461907  [   32/  265]
train() client id: f_00001-3-1 loss: 0.429284  [   64/  265]
train() client id: f_00001-3-2 loss: 0.529635  [   96/  265]
train() client id: f_00001-3-3 loss: 0.377437  [  128/  265]
train() client id: f_00001-3-4 loss: 0.419971  [  160/  265]
train() client id: f_00001-3-5 loss: 0.397162  [  192/  265]
train() client id: f_00001-3-6 loss: 0.342284  [  224/  265]
train() client id: f_00001-3-7 loss: 0.487127  [  256/  265]
train() client id: f_00001-4-0 loss: 0.385272  [   32/  265]
train() client id: f_00001-4-1 loss: 0.593742  [   64/  265]
train() client id: f_00001-4-2 loss: 0.341887  [   96/  265]
train() client id: f_00001-4-3 loss: 0.579791  [  128/  265]
train() client id: f_00001-4-4 loss: 0.470049  [  160/  265]
train() client id: f_00001-4-5 loss: 0.453353  [  192/  265]
train() client id: f_00001-4-6 loss: 0.385935  [  224/  265]
train() client id: f_00001-4-7 loss: 0.340441  [  256/  265]
train() client id: f_00001-5-0 loss: 0.400618  [   32/  265]
train() client id: f_00001-5-1 loss: 0.505533  [   64/  265]
train() client id: f_00001-5-2 loss: 0.350974  [   96/  265]
train() client id: f_00001-5-3 loss: 0.493273  [  128/  265]
train() client id: f_00001-5-4 loss: 0.421521  [  160/  265]
train() client id: f_00001-5-5 loss: 0.360583  [  192/  265]
train() client id: f_00001-5-6 loss: 0.594207  [  224/  265]
train() client id: f_00001-5-7 loss: 0.429336  [  256/  265]
train() client id: f_00001-6-0 loss: 0.436313  [   32/  265]
train() client id: f_00001-6-1 loss: 0.435349  [   64/  265]
train() client id: f_00001-6-2 loss: 0.371694  [   96/  265]
train() client id: f_00001-6-3 loss: 0.631031  [  128/  265]
train() client id: f_00001-6-4 loss: 0.361929  [  160/  265]
train() client id: f_00001-6-5 loss: 0.450317  [  192/  265]
train() client id: f_00001-6-6 loss: 0.376574  [  224/  265]
train() client id: f_00001-6-7 loss: 0.494544  [  256/  265]
train() client id: f_00001-7-0 loss: 0.599706  [   32/  265]
train() client id: f_00001-7-1 loss: 0.382714  [   64/  265]
train() client id: f_00001-7-2 loss: 0.329620  [   96/  265]
train() client id: f_00001-7-3 loss: 0.587148  [  128/  265]
train() client id: f_00001-7-4 loss: 0.482818  [  160/  265]
train() client id: f_00001-7-5 loss: 0.368197  [  192/  265]
train() client id: f_00001-7-6 loss: 0.404768  [  224/  265]
train() client id: f_00001-7-7 loss: 0.381045  [  256/  265]
train() client id: f_00001-8-0 loss: 0.737831  [   32/  265]
train() client id: f_00001-8-1 loss: 0.445201  [   64/  265]
train() client id: f_00001-8-2 loss: 0.370403  [   96/  265]
train() client id: f_00001-8-3 loss: 0.414462  [  128/  265]
train() client id: f_00001-8-4 loss: 0.346656  [  160/  265]
train() client id: f_00001-8-5 loss: 0.411988  [  192/  265]
train() client id: f_00001-8-6 loss: 0.399618  [  224/  265]
train() client id: f_00001-8-7 loss: 0.426061  [  256/  265]
train() client id: f_00002-0-0 loss: 1.017112  [   32/  124]
train() client id: f_00002-0-1 loss: 0.945043  [   64/  124]
train() client id: f_00002-0-2 loss: 0.821803  [   96/  124]
train() client id: f_00002-1-0 loss: 0.802313  [   32/  124]
train() client id: f_00002-1-1 loss: 1.043157  [   64/  124]
train() client id: f_00002-1-2 loss: 0.739522  [   96/  124]
train() client id: f_00002-2-0 loss: 0.924262  [   32/  124]
train() client id: f_00002-2-1 loss: 0.973556  [   64/  124]
train() client id: f_00002-2-2 loss: 1.002727  [   96/  124]
train() client id: f_00002-3-0 loss: 0.797439  [   32/  124]
train() client id: f_00002-3-1 loss: 0.961544  [   64/  124]
train() client id: f_00002-3-2 loss: 0.779034  [   96/  124]
train() client id: f_00002-4-0 loss: 0.901882  [   32/  124]
train() client id: f_00002-4-1 loss: 0.800923  [   64/  124]
train() client id: f_00002-4-2 loss: 1.031402  [   96/  124]
train() client id: f_00002-5-0 loss: 0.793986  [   32/  124]
train() client id: f_00002-5-1 loss: 0.923328  [   64/  124]
train() client id: f_00002-5-2 loss: 0.939600  [   96/  124]
train() client id: f_00002-6-0 loss: 0.975993  [   32/  124]
train() client id: f_00002-6-1 loss: 0.751561  [   64/  124]
train() client id: f_00002-6-2 loss: 0.871597  [   96/  124]
train() client id: f_00002-7-0 loss: 0.986530  [   32/  124]
train() client id: f_00002-7-1 loss: 0.659426  [   64/  124]
train() client id: f_00002-7-2 loss: 0.817965  [   96/  124]
train() client id: f_00002-8-0 loss: 0.730759  [   32/  124]
train() client id: f_00002-8-1 loss: 1.148153  [   64/  124]
train() client id: f_00002-8-2 loss: 0.912995  [   96/  124]
train() client id: f_00003-0-0 loss: 0.692593  [   32/   43]
train() client id: f_00003-1-0 loss: 0.749152  [   32/   43]
train() client id: f_00003-2-0 loss: 0.637052  [   32/   43]
train() client id: f_00003-3-0 loss: 0.901734  [   32/   43]
train() client id: f_00003-4-0 loss: 0.532207  [   32/   43]
train() client id: f_00003-5-0 loss: 0.952637  [   32/   43]
train() client id: f_00003-6-0 loss: 0.655005  [   32/   43]
train() client id: f_00003-7-0 loss: 0.695711  [   32/   43]
train() client id: f_00003-8-0 loss: 0.638664  [   32/   43]
train() client id: f_00004-0-0 loss: 0.786327  [   32/  306]
train() client id: f_00004-0-1 loss: 0.919581  [   64/  306]
train() client id: f_00004-0-2 loss: 0.750219  [   96/  306]
train() client id: f_00004-0-3 loss: 0.761945  [  128/  306]
train() client id: f_00004-0-4 loss: 0.840487  [  160/  306]
train() client id: f_00004-0-5 loss: 0.775965  [  192/  306]
train() client id: f_00004-0-6 loss: 0.946595  [  224/  306]
train() client id: f_00004-0-7 loss: 0.853497  [  256/  306]
train() client id: f_00004-0-8 loss: 0.677669  [  288/  306]
train() client id: f_00004-1-0 loss: 0.826615  [   32/  306]
train() client id: f_00004-1-1 loss: 0.813559  [   64/  306]
train() client id: f_00004-1-2 loss: 0.776826  [   96/  306]
train() client id: f_00004-1-3 loss: 0.908760  [  128/  306]
train() client id: f_00004-1-4 loss: 0.741960  [  160/  306]
train() client id: f_00004-1-5 loss: 0.955895  [  192/  306]
train() client id: f_00004-1-6 loss: 0.788664  [  224/  306]
train() client id: f_00004-1-7 loss: 0.618090  [  256/  306]
train() client id: f_00004-1-8 loss: 0.723080  [  288/  306]
train() client id: f_00004-2-0 loss: 0.775552  [   32/  306]
train() client id: f_00004-2-1 loss: 0.771176  [   64/  306]
train() client id: f_00004-2-2 loss: 0.713892  [   96/  306]
train() client id: f_00004-2-3 loss: 0.967131  [  128/  306]
train() client id: f_00004-2-4 loss: 0.733997  [  160/  306]
train() client id: f_00004-2-5 loss: 0.931729  [  192/  306]
train() client id: f_00004-2-6 loss: 0.718330  [  224/  306]
train() client id: f_00004-2-7 loss: 0.928328  [  256/  306]
train() client id: f_00004-2-8 loss: 0.706411  [  288/  306]
train() client id: f_00004-3-0 loss: 0.915376  [   32/  306]
train() client id: f_00004-3-1 loss: 0.721289  [   64/  306]
train() client id: f_00004-3-2 loss: 0.750933  [   96/  306]
train() client id: f_00004-3-3 loss: 0.807170  [  128/  306]
train() client id: f_00004-3-4 loss: 0.921890  [  160/  306]
train() client id: f_00004-3-5 loss: 0.795888  [  192/  306]
train() client id: f_00004-3-6 loss: 0.733931  [  224/  306]
train() client id: f_00004-3-7 loss: 0.823248  [  256/  306]
train() client id: f_00004-3-8 loss: 0.741395  [  288/  306]
train() client id: f_00004-4-0 loss: 0.881770  [   32/  306]
train() client id: f_00004-4-1 loss: 0.876335  [   64/  306]
train() client id: f_00004-4-2 loss: 0.707126  [   96/  306]
train() client id: f_00004-4-3 loss: 0.856407  [  128/  306]
train() client id: f_00004-4-4 loss: 0.834104  [  160/  306]
train() client id: f_00004-4-5 loss: 0.761693  [  192/  306]
train() client id: f_00004-4-6 loss: 0.751370  [  224/  306]
train() client id: f_00004-4-7 loss: 0.751584  [  256/  306]
train() client id: f_00004-4-8 loss: 0.837100  [  288/  306]
train() client id: f_00004-5-0 loss: 0.702476  [   32/  306]
train() client id: f_00004-5-1 loss: 0.824001  [   64/  306]
train() client id: f_00004-5-2 loss: 0.876409  [   96/  306]
train() client id: f_00004-5-3 loss: 0.738802  [  128/  306]
train() client id: f_00004-5-4 loss: 0.848619  [  160/  306]
train() client id: f_00004-5-5 loss: 0.730215  [  192/  306]
train() client id: f_00004-5-6 loss: 0.716312  [  224/  306]
train() client id: f_00004-5-7 loss: 0.887391  [  256/  306]
train() client id: f_00004-5-8 loss: 0.897299  [  288/  306]
train() client id: f_00004-6-0 loss: 0.800346  [   32/  306]
train() client id: f_00004-6-1 loss: 0.744737  [   64/  306]
train() client id: f_00004-6-2 loss: 0.793320  [   96/  306]
train() client id: f_00004-6-3 loss: 0.878574  [  128/  306]
train() client id: f_00004-6-4 loss: 0.743862  [  160/  306]
train() client id: f_00004-6-5 loss: 0.777544  [  192/  306]
train() client id: f_00004-6-6 loss: 0.803518  [  224/  306]
train() client id: f_00004-6-7 loss: 0.813994  [  256/  306]
train() client id: f_00004-6-8 loss: 0.800475  [  288/  306]
train() client id: f_00004-7-0 loss: 0.681848  [   32/  306]
train() client id: f_00004-7-1 loss: 0.724930  [   64/  306]
train() client id: f_00004-7-2 loss: 0.843103  [   96/  306]
train() client id: f_00004-7-3 loss: 0.898484  [  128/  306]
train() client id: f_00004-7-4 loss: 0.856300  [  160/  306]
train() client id: f_00004-7-5 loss: 0.823071  [  192/  306]
train() client id: f_00004-7-6 loss: 0.779597  [  224/  306]
train() client id: f_00004-7-7 loss: 0.810740  [  256/  306]
train() client id: f_00004-7-8 loss: 0.761622  [  288/  306]
train() client id: f_00004-8-0 loss: 0.754704  [   32/  306]
train() client id: f_00004-8-1 loss: 0.837759  [   64/  306]
train() client id: f_00004-8-2 loss: 0.887502  [   96/  306]
train() client id: f_00004-8-3 loss: 0.762584  [  128/  306]
train() client id: f_00004-8-4 loss: 0.766091  [  160/  306]
train() client id: f_00004-8-5 loss: 0.826655  [  192/  306]
train() client id: f_00004-8-6 loss: 0.833938  [  224/  306]
train() client id: f_00004-8-7 loss: 0.828436  [  256/  306]
train() client id: f_00004-8-8 loss: 0.731501  [  288/  306]
train() client id: f_00005-0-0 loss: 0.504230  [   32/  146]
train() client id: f_00005-0-1 loss: 0.740433  [   64/  146]
train() client id: f_00005-0-2 loss: 0.534698  [   96/  146]
train() client id: f_00005-0-3 loss: 0.420138  [  128/  146]
train() client id: f_00005-1-0 loss: 0.357145  [   32/  146]
train() client id: f_00005-1-1 loss: 0.816234  [   64/  146]
train() client id: f_00005-1-2 loss: 0.528698  [   96/  146]
train() client id: f_00005-1-3 loss: 0.440861  [  128/  146]
train() client id: f_00005-2-0 loss: 0.352395  [   32/  146]
train() client id: f_00005-2-1 loss: 0.564340  [   64/  146]
train() client id: f_00005-2-2 loss: 0.717320  [   96/  146]
train() client id: f_00005-2-3 loss: 0.513977  [  128/  146]
train() client id: f_00005-3-0 loss: 0.510926  [   32/  146]
train() client id: f_00005-3-1 loss: 0.554671  [   64/  146]
train() client id: f_00005-3-2 loss: 0.572739  [   96/  146]
train() client id: f_00005-3-3 loss: 0.339521  [  128/  146]
train() client id: f_00005-4-0 loss: 0.589586  [   32/  146]
train() client id: f_00005-4-1 loss: 0.623183  [   64/  146]
train() client id: f_00005-4-2 loss: 0.362353  [   96/  146]
train() client id: f_00005-4-3 loss: 0.589887  [  128/  146]
train() client id: f_00005-5-0 loss: 0.567408  [   32/  146]
train() client id: f_00005-5-1 loss: 0.414304  [   64/  146]
train() client id: f_00005-5-2 loss: 0.710002  [   96/  146]
train() client id: f_00005-5-3 loss: 0.224705  [  128/  146]
train() client id: f_00005-6-0 loss: 0.469606  [   32/  146]
train() client id: f_00005-6-1 loss: 0.679955  [   64/  146]
train() client id: f_00005-6-2 loss: 0.429484  [   96/  146]
train() client id: f_00005-6-3 loss: 0.502662  [  128/  146]
train() client id: f_00005-7-0 loss: 0.605109  [   32/  146]
train() client id: f_00005-7-1 loss: 0.474135  [   64/  146]
train() client id: f_00005-7-2 loss: 0.372975  [   96/  146]
train() client id: f_00005-7-3 loss: 0.321515  [  128/  146]
train() client id: f_00005-8-0 loss: 0.804420  [   32/  146]
train() client id: f_00005-8-1 loss: 0.592852  [   64/  146]
train() client id: f_00005-8-2 loss: 0.286330  [   96/  146]
train() client id: f_00005-8-3 loss: 0.402753  [  128/  146]
train() client id: f_00006-0-0 loss: 0.486140  [   32/   54]
train() client id: f_00006-1-0 loss: 0.416582  [   32/   54]
train() client id: f_00006-2-0 loss: 0.541797  [   32/   54]
train() client id: f_00006-3-0 loss: 0.486505  [   32/   54]
train() client id: f_00006-4-0 loss: 0.502633  [   32/   54]
train() client id: f_00006-5-0 loss: 0.545780  [   32/   54]
train() client id: f_00006-6-0 loss: 0.535479  [   32/   54]
train() client id: f_00006-7-0 loss: 0.479104  [   32/   54]
train() client id: f_00006-8-0 loss: 0.486646  [   32/   54]
train() client id: f_00007-0-0 loss: 0.562285  [   32/  179]
train() client id: f_00007-0-1 loss: 0.559983  [   64/  179]
train() client id: f_00007-0-2 loss: 0.304664  [   96/  179]
train() client id: f_00007-0-3 loss: 0.601481  [  128/  179]
train() client id: f_00007-0-4 loss: 0.455701  [  160/  179]
train() client id: f_00007-1-0 loss: 0.465325  [   32/  179]
train() client id: f_00007-1-1 loss: 0.551360  [   64/  179]
train() client id: f_00007-1-2 loss: 0.314228  [   96/  179]
train() client id: f_00007-1-3 loss: 0.404602  [  128/  179]
train() client id: f_00007-1-4 loss: 0.589840  [  160/  179]
train() client id: f_00007-2-0 loss: 0.414469  [   32/  179]
train() client id: f_00007-2-1 loss: 0.519657  [   64/  179]
train() client id: f_00007-2-2 loss: 0.397751  [   96/  179]
train() client id: f_00007-2-3 loss: 0.567435  [  128/  179]
train() client id: f_00007-2-4 loss: 0.413580  [  160/  179]
train() client id: f_00007-3-0 loss: 0.478588  [   32/  179]
train() client id: f_00007-3-1 loss: 0.473353  [   64/  179]
train() client id: f_00007-3-2 loss: 0.303669  [   96/  179]
train() client id: f_00007-3-3 loss: 0.532487  [  128/  179]
train() client id: f_00007-3-4 loss: 0.315083  [  160/  179]
train() client id: f_00007-4-0 loss: 0.623498  [   32/  179]
train() client id: f_00007-4-1 loss: 0.396513  [   64/  179]
train() client id: f_00007-4-2 loss: 0.345868  [   96/  179]
train() client id: f_00007-4-3 loss: 0.302543  [  128/  179]
train() client id: f_00007-4-4 loss: 0.357432  [  160/  179]
train() client id: f_00007-5-0 loss: 0.571339  [   32/  179]
train() client id: f_00007-5-1 loss: 0.473340  [   64/  179]
train() client id: f_00007-5-2 loss: 0.393705  [   96/  179]
train() client id: f_00007-5-3 loss: 0.563608  [  128/  179]
train() client id: f_00007-5-4 loss: 0.344911  [  160/  179]
train() client id: f_00007-6-0 loss: 0.728702  [   32/  179]
train() client id: f_00007-6-1 loss: 0.456306  [   64/  179]
train() client id: f_00007-6-2 loss: 0.424136  [   96/  179]
train() client id: f_00007-6-3 loss: 0.450173  [  128/  179]
train() client id: f_00007-6-4 loss: 0.274406  [  160/  179]
train() client id: f_00007-7-0 loss: 0.411187  [   32/  179]
train() client id: f_00007-7-1 loss: 0.361699  [   64/  179]
train() client id: f_00007-7-2 loss: 0.379460  [   96/  179]
train() client id: f_00007-7-3 loss: 0.646981  [  128/  179]
train() client id: f_00007-7-4 loss: 0.467694  [  160/  179]
train() client id: f_00007-8-0 loss: 0.351102  [   32/  179]
train() client id: f_00007-8-1 loss: 0.505739  [   64/  179]
train() client id: f_00007-8-2 loss: 0.292084  [   96/  179]
train() client id: f_00007-8-3 loss: 0.579749  [  128/  179]
train() client id: f_00007-8-4 loss: 0.509988  [  160/  179]
train() client id: f_00008-0-0 loss: 0.822012  [   32/  130]
train() client id: f_00008-0-1 loss: 0.708218  [   64/  130]
train() client id: f_00008-0-2 loss: 0.876659  [   96/  130]
train() client id: f_00008-0-3 loss: 0.599238  [  128/  130]
train() client id: f_00008-1-0 loss: 0.703317  [   32/  130]
train() client id: f_00008-1-1 loss: 0.892376  [   64/  130]
train() client id: f_00008-1-2 loss: 0.732957  [   96/  130]
train() client id: f_00008-1-3 loss: 0.671277  [  128/  130]
train() client id: f_00008-2-0 loss: 0.750580  [   32/  130]
train() client id: f_00008-2-1 loss: 0.687163  [   64/  130]
train() client id: f_00008-2-2 loss: 0.767701  [   96/  130]
train() client id: f_00008-2-3 loss: 0.790727  [  128/  130]
train() client id: f_00008-3-0 loss: 0.699503  [   32/  130]
train() client id: f_00008-3-1 loss: 0.744043  [   64/  130]
train() client id: f_00008-3-2 loss: 0.758963  [   96/  130]
train() client id: f_00008-3-3 loss: 0.793180  [  128/  130]
train() client id: f_00008-4-0 loss: 0.681277  [   32/  130]
train() client id: f_00008-4-1 loss: 0.746283  [   64/  130]
train() client id: f_00008-4-2 loss: 0.677972  [   96/  130]
train() client id: f_00008-4-3 loss: 0.826546  [  128/  130]
train() client id: f_00008-5-0 loss: 0.720294  [   32/  130]
train() client id: f_00008-5-1 loss: 0.765910  [   64/  130]
train() client id: f_00008-5-2 loss: 0.688012  [   96/  130]
train() client id: f_00008-5-3 loss: 0.815755  [  128/  130]
train() client id: f_00008-6-0 loss: 0.615510  [   32/  130]
train() client id: f_00008-6-1 loss: 0.851174  [   64/  130]
train() client id: f_00008-6-2 loss: 0.711452  [   96/  130]
train() client id: f_00008-6-3 loss: 0.771962  [  128/  130]
train() client id: f_00008-7-0 loss: 0.802725  [   32/  130]
train() client id: f_00008-7-1 loss: 0.710759  [   64/  130]
train() client id: f_00008-7-2 loss: 0.718794  [   96/  130]
train() client id: f_00008-7-3 loss: 0.743091  [  128/  130]
train() client id: f_00008-8-0 loss: 0.696230  [   32/  130]
train() client id: f_00008-8-1 loss: 0.756048  [   64/  130]
train() client id: f_00008-8-2 loss: 0.772082  [   96/  130]
train() client id: f_00008-8-3 loss: 0.751056  [  128/  130]
train() client id: f_00009-0-0 loss: 1.014140  [   32/  118]
train() client id: f_00009-0-1 loss: 0.936561  [   64/  118]
train() client id: f_00009-0-2 loss: 0.826741  [   96/  118]
train() client id: f_00009-1-0 loss: 0.732055  [   32/  118]
train() client id: f_00009-1-1 loss: 1.113703  [   64/  118]
train() client id: f_00009-1-2 loss: 0.885087  [   96/  118]
train() client id: f_00009-2-0 loss: 1.000099  [   32/  118]
train() client id: f_00009-2-1 loss: 0.879122  [   64/  118]
train() client id: f_00009-2-2 loss: 0.824771  [   96/  118]
train() client id: f_00009-3-0 loss: 1.038330  [   32/  118]
train() client id: f_00009-3-1 loss: 0.837049  [   64/  118]
train() client id: f_00009-3-2 loss: 0.727519  [   96/  118]
train() client id: f_00009-4-0 loss: 0.996114  [   32/  118]
train() client id: f_00009-4-1 loss: 0.778271  [   64/  118]
train() client id: f_00009-4-2 loss: 0.734821  [   96/  118]
train() client id: f_00009-5-0 loss: 1.023214  [   32/  118]
train() client id: f_00009-5-1 loss: 0.685232  [   64/  118]
train() client id: f_00009-5-2 loss: 0.971198  [   96/  118]
train() client id: f_00009-6-0 loss: 0.872026  [   32/  118]
train() client id: f_00009-6-1 loss: 0.820635  [   64/  118]
train() client id: f_00009-6-2 loss: 0.801378  [   96/  118]
train() client id: f_00009-7-0 loss: 1.054107  [   32/  118]
train() client id: f_00009-7-1 loss: 0.839296  [   64/  118]
train() client id: f_00009-7-2 loss: 0.723414  [   96/  118]
train() client id: f_00009-8-0 loss: 0.906306  [   32/  118]
train() client id: f_00009-8-1 loss: 0.780579  [   64/  118]
train() client id: f_00009-8-2 loss: 0.875548  [   96/  118]
At round 65 accuracy: 0.6472148541114059
At round 65 training accuracy: 0.5935613682092555
At round 65 training loss: 0.8189923817652891
update_location
xs = -3.905658 4.200318 345.009024 18.811294 0.979296 3.956410 -307.443192 -286.324852 329.663977 -272.060879 
ys = 337.587959 320.555839 1.320614 -307.455176 299.350187 282.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -16.71142616811743
ys mean: 95.39414253552872
dists_uav = 352.109194 335.817939 359.211596 323.855755 315.612885 299.999162 323.308223 303.286328 344.944934 289.884690 
uav_gains = -119.286465 -118.344266 -119.655502 -117.557142 -116.964315 -115.730366 -117.519053 -116.001580 -118.889438 -114.863229 
uav_gains_db_mean: -117.4811354283453
dists_bs = 236.025673 230.866317 548.246217 519.928663 215.480119 208.924120 221.566771 206.714966 528.649456 196.624788 
bs_gains = -106.009918 -105.741155 -116.258377 -115.613484 -104.902461 -104.526739 -105.241189 -104.397473 -115.815757 -103.788931 
bs_gains_db_mean: -108.22954847225984
Round 66
-------------------------------
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.28261697 4.54685792 2.2554939  0.84667744 5.24112296 2.52309512
 1.03299146 3.13864151 2.30469629 2.04601254]
obj_prev = 26.218206122007363
eta_min = 1.468265776943342e-41	eta_max = 0.944257265961695
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 5.987676154852869	eta = 0.9090909090909091
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 15.480452461148737	eta = 0.3516267998379439
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 10.14509351030873	eta = 0.5365492149900848
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.229709829328627	eta = 0.5897630651031099
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.172211188857027	eta = 0.5934601642807859
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.17195028532138	eta = 0.5934770457345999
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.17195027990423	eta = 0.5934770460851201
eta = 0.5934770460851201
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [0.04305412 0.09055032 0.04237072 0.01469307 0.10456    0.0498881
 0.01845176 0.06116418 0.04442091 0.04032052]
ene_total = [0.9382438  1.35500986 0.94553393 0.48503672 1.54222126 0.79045116
 0.53538055 1.0793304  0.84532808 0.65541452]
ti_comp = [1.69339463 1.86443235 1.68124436 1.74028295 1.86800619 1.86951147
 1.74115692 1.77160865 1.77482088 1.87231038]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [1.73943290e-06 1.33492469e-05 1.68196045e-06 6.54603068e-08
 2.04748185e-05 2.22031557e-06 1.29514340e-07 4.55654761e-06
 1.73913543e-06 1.16869917e-06]
ene_total = [0.34787422 0.11349245 0.36453489 0.28355414 0.10868941 0.10637493
 0.28235657 0.24065936 0.23621585 0.10252241]
optimize_network iter = 0 obj = 2.186274239551174
eta = 0.5934770460851201
freqs = [12712369.24332196 24283617.19415748 12600999.04046834  4221460.42224169
 27987058.04437446 13342550.5059045   5298707.90102926 17262328.31372014
 12514196.1057277  10767584.65528894]
eta_min = 0.5934770460851205	eta_max = 0.7588338965075888
af = 0.0005034964568992424	bf = 0.8885079592597985	zeta = 0.0005538461025891666	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [3.17745128e-07 2.43852935e-06 3.07246539e-07 1.19577441e-08
 3.74016947e-06 4.05588772e-07 2.36586018e-08 8.32352201e-07
 3.17690788e-07 2.13488239e-07]
ene_total = [1.55552023 0.5068411  1.6300255  1.26798066 0.48500598 0.47557109
 1.26262219 1.07594073 1.05621167 0.45839627]
ti_comp = [0.90140949 1.07244721 0.88925922 0.94829781 1.07602105 1.07752633
 0.94917178 0.97962351 0.98283574 1.08032524]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.08390094e-07 5.97022622e-06 8.89637979e-07 3.26227732e-08
 9.13119788e-06 9.89026201e-07 6.44906348e-08 2.20518843e-06
 8.39214901e-07 5.19447888e-07]
ene_total = [0.58637686 0.1911386  0.6144619  0.47797382 0.18295069 0.17928302
 0.47595438 0.40561441 0.39815774 0.17280245]
optimize_network iter = 1 obj = 3.6847138773635306
eta = 0.7588338965075888
freqs = [12631650.06371988 22329634.8077465  12600999.04046835  4097654.84679903
 25698769.41757561 12244375.3475877   5141153.96101874 16512221.31849963
 11952921.87158274  9870499.99002256]
eta_min = 0.7588338965075906	eta_max = 0.7588338965075868
af = 0.00043529367837054833	bf = 0.8885079592597985	zeta = 0.0004788230462076032	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [3.13722794e-07 2.06188515e-06 3.07246539e-07 1.12666437e-08
 3.15356247e-06 3.41571386e-07 2.22725702e-08 7.61586768e-07
 2.89832359e-07 1.79397204e-07]
ene_total = [1.55551998 0.506818   1.6300255  1.26798062 0.48497001 0.47556716
 1.2626221  1.0759364  1.05620996 0.45839418]
ti_comp = [0.90140949 1.07244721 0.88925922 0.94829781 1.07602105 1.07752633
 0.94917178 0.97962351 0.98283574 1.08032524]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.08390094e-07 5.97022622e-06 8.89637979e-07 3.26227732e-08
 9.13119788e-06 9.89026201e-07 6.44906348e-08 2.20518843e-06
 8.39214901e-07 5.19447888e-07]
ene_total = [0.58637686 0.1911386  0.6144619  0.47797382 0.18295069 0.17928302
 0.47595438 0.40561441 0.39815774 0.17280245]
optimize_network iter = 2 obj = 3.6847138773635004
eta = 0.7588338965075868
freqs = [12631650.06371987 22329634.80774652 12600999.04046834  4097654.84679903
 25698769.41757564 12244375.34758771  5141153.96101873 16512221.31849963
 11952921.87158274  9870499.99002257]
Done!
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.04697167e-07 5.94595514e-06 8.86021287e-07 3.24901501e-08
 9.09407634e-06 9.85005461e-07 6.42284576e-08 2.19622356e-06
 8.35803196e-07 5.17336149e-07]
ene_total = [0.0253677  0.00826897 0.02658271 0.020678   0.00791473 0.0077561
 0.02059063 0.01754759 0.01722501 0.00747574]
At round 66 energy consumption: 0.15940718136832308
At round 66 eta: 0.7588338965075868
At round 66 a_n: 5.574576956279433
At round 66 local rounds: 9.036737525084137
At round 66 global rounds: 23.115093189100694
gradient difference: 0.5695992708206177
train() client id: f_00000-0-0 loss: 1.015919  [   32/  126]
train() client id: f_00000-0-1 loss: 1.009913  [   64/  126]
train() client id: f_00000-0-2 loss: 1.000517  [   96/  126]
train() client id: f_00000-1-0 loss: 0.952467  [   32/  126]
train() client id: f_00000-1-1 loss: 0.761802  [   64/  126]
train() client id: f_00000-1-2 loss: 1.111683  [   96/  126]
train() client id: f_00000-2-0 loss: 0.947080  [   32/  126]
train() client id: f_00000-2-1 loss: 0.952455  [   64/  126]
train() client id: f_00000-2-2 loss: 0.909498  [   96/  126]
train() client id: f_00000-3-0 loss: 1.002934  [   32/  126]
train() client id: f_00000-3-1 loss: 0.812622  [   64/  126]
train() client id: f_00000-3-2 loss: 0.700003  [   96/  126]
train() client id: f_00000-4-0 loss: 0.829998  [   32/  126]
train() client id: f_00000-4-1 loss: 0.890857  [   64/  126]
train() client id: f_00000-4-2 loss: 0.757728  [   96/  126]
train() client id: f_00000-5-0 loss: 0.843839  [   32/  126]
train() client id: f_00000-5-1 loss: 0.885700  [   64/  126]
train() client id: f_00000-5-2 loss: 0.672794  [   96/  126]
train() client id: f_00000-6-0 loss: 0.750874  [   32/  126]
train() client id: f_00000-6-1 loss: 0.792553  [   64/  126]
train() client id: f_00000-6-2 loss: 0.734431  [   96/  126]
train() client id: f_00000-7-0 loss: 0.828798  [   32/  126]
train() client id: f_00000-7-1 loss: 0.868492  [   64/  126]
train() client id: f_00000-7-2 loss: 0.807495  [   96/  126]
train() client id: f_00000-8-0 loss: 0.761304  [   32/  126]
train() client id: f_00000-8-1 loss: 0.738161  [   64/  126]
train() client id: f_00000-8-2 loss: 0.792626  [   96/  126]
train() client id: f_00001-0-0 loss: 0.417103  [   32/  265]
train() client id: f_00001-0-1 loss: 0.430077  [   64/  265]
train() client id: f_00001-0-2 loss: 0.559719  [   96/  265]
train() client id: f_00001-0-3 loss: 0.553584  [  128/  265]
train() client id: f_00001-0-4 loss: 0.483335  [  160/  265]
train() client id: f_00001-0-5 loss: 0.449169  [  192/  265]
train() client id: f_00001-0-6 loss: 0.450030  [  224/  265]
train() client id: f_00001-0-7 loss: 0.634097  [  256/  265]
train() client id: f_00001-1-0 loss: 0.473281  [   32/  265]
train() client id: f_00001-1-1 loss: 0.527779  [   64/  265]
train() client id: f_00001-1-2 loss: 0.461325  [   96/  265]
train() client id: f_00001-1-3 loss: 0.398759  [  128/  265]
train() client id: f_00001-1-4 loss: 0.480061  [  160/  265]
train() client id: f_00001-1-5 loss: 0.524608  [  192/  265]
train() client id: f_00001-1-6 loss: 0.589330  [  224/  265]
train() client id: f_00001-1-7 loss: 0.508873  [  256/  265]
train() client id: f_00001-2-0 loss: 0.374506  [   32/  265]
train() client id: f_00001-2-1 loss: 0.409519  [   64/  265]
train() client id: f_00001-2-2 loss: 0.387344  [   96/  265]
train() client id: f_00001-2-3 loss: 0.522397  [  128/  265]
train() client id: f_00001-2-4 loss: 0.406580  [  160/  265]
train() client id: f_00001-2-5 loss: 0.523251  [  192/  265]
train() client id: f_00001-2-6 loss: 0.672053  [  224/  265]
train() client id: f_00001-2-7 loss: 0.623155  [  256/  265]
train() client id: f_00001-3-0 loss: 0.476206  [   32/  265]
train() client id: f_00001-3-1 loss: 0.485733  [   64/  265]
train() client id: f_00001-3-2 loss: 0.503958  [   96/  265]
train() client id: f_00001-3-3 loss: 0.451070  [  128/  265]
train() client id: f_00001-3-4 loss: 0.480974  [  160/  265]
train() client id: f_00001-3-5 loss: 0.454369  [  192/  265]
train() client id: f_00001-3-6 loss: 0.497697  [  224/  265]
train() client id: f_00001-3-7 loss: 0.530856  [  256/  265]
train() client id: f_00001-4-0 loss: 0.399887  [   32/  265]
train() client id: f_00001-4-1 loss: 0.593578  [   64/  265]
train() client id: f_00001-4-2 loss: 0.512149  [   96/  265]
train() client id: f_00001-4-3 loss: 0.633739  [  128/  265]
train() client id: f_00001-4-4 loss: 0.419880  [  160/  265]
train() client id: f_00001-4-5 loss: 0.400789  [  192/  265]
train() client id: f_00001-4-6 loss: 0.380499  [  224/  265]
train() client id: f_00001-4-7 loss: 0.447103  [  256/  265]
train() client id: f_00001-5-0 loss: 0.468313  [   32/  265]
train() client id: f_00001-5-1 loss: 0.533535  [   64/  265]
train() client id: f_00001-5-2 loss: 0.433345  [   96/  265]
train() client id: f_00001-5-3 loss: 0.394319  [  128/  265]
train() client id: f_00001-5-4 loss: 0.480277  [  160/  265]
train() client id: f_00001-5-5 loss: 0.491016  [  192/  265]
train() client id: f_00001-5-6 loss: 0.517649  [  224/  265]
train() client id: f_00001-5-7 loss: 0.551763  [  256/  265]
train() client id: f_00001-6-0 loss: 0.519266  [   32/  265]
train() client id: f_00001-6-1 loss: 0.419044  [   64/  265]
train() client id: f_00001-6-2 loss: 0.438037  [   96/  265]
train() client id: f_00001-6-3 loss: 0.536813  [  128/  265]
train() client id: f_00001-6-4 loss: 0.402985  [  160/  265]
train() client id: f_00001-6-5 loss: 0.574174  [  192/  265]
train() client id: f_00001-6-6 loss: 0.486735  [  224/  265]
train() client id: f_00001-6-7 loss: 0.480663  [  256/  265]
train() client id: f_00001-7-0 loss: 0.478676  [   32/  265]
train() client id: f_00001-7-1 loss: 0.473338  [   64/  265]
train() client id: f_00001-7-2 loss: 0.385057  [   96/  265]
train() client id: f_00001-7-3 loss: 0.423337  [  128/  265]
train() client id: f_00001-7-4 loss: 0.547271  [  160/  265]
train() client id: f_00001-7-5 loss: 0.548530  [  192/  265]
train() client id: f_00001-7-6 loss: 0.485621  [  224/  265]
train() client id: f_00001-7-7 loss: 0.490565  [  256/  265]
train() client id: f_00001-8-0 loss: 0.402180  [   32/  265]
train() client id: f_00001-8-1 loss: 0.528542  [   64/  265]
train() client id: f_00001-8-2 loss: 0.458213  [   96/  265]
train() client id: f_00001-8-3 loss: 0.443071  [  128/  265]
train() client id: f_00001-8-4 loss: 0.588550  [  160/  265]
train() client id: f_00001-8-5 loss: 0.413589  [  192/  265]
train() client id: f_00001-8-6 loss: 0.442983  [  224/  265]
train() client id: f_00001-8-7 loss: 0.550617  [  256/  265]
train() client id: f_00002-0-0 loss: 1.068138  [   32/  124]
train() client id: f_00002-0-1 loss: 1.019516  [   64/  124]
train() client id: f_00002-0-2 loss: 0.958962  [   96/  124]
train() client id: f_00002-1-0 loss: 1.082329  [   32/  124]
train() client id: f_00002-1-1 loss: 0.965505  [   64/  124]
train() client id: f_00002-1-2 loss: 0.904921  [   96/  124]
train() client id: f_00002-2-0 loss: 1.117298  [   32/  124]
train() client id: f_00002-2-1 loss: 0.958472  [   64/  124]
train() client id: f_00002-2-2 loss: 1.039338  [   96/  124]
train() client id: f_00002-3-0 loss: 0.927726  [   32/  124]
train() client id: f_00002-3-1 loss: 0.849576  [   64/  124]
train() client id: f_00002-3-2 loss: 1.285105  [   96/  124]
train() client id: f_00002-4-0 loss: 0.857469  [   32/  124]
train() client id: f_00002-4-1 loss: 1.118958  [   64/  124]
train() client id: f_00002-4-2 loss: 0.982893  [   96/  124]
train() client id: f_00002-5-0 loss: 0.986965  [   32/  124]
train() client id: f_00002-5-1 loss: 0.884483  [   64/  124]
train() client id: f_00002-5-2 loss: 1.047445  [   96/  124]
train() client id: f_00002-6-0 loss: 1.160586  [   32/  124]
train() client id: f_00002-6-1 loss: 1.022053  [   64/  124]
train() client id: f_00002-6-2 loss: 0.859953  [   96/  124]
train() client id: f_00002-7-0 loss: 0.939777  [   32/  124]
train() client id: f_00002-7-1 loss: 0.964398  [   64/  124]
train() client id: f_00002-7-2 loss: 1.144932  [   96/  124]
train() client id: f_00002-8-0 loss: 0.988479  [   32/  124]
train() client id: f_00002-8-1 loss: 0.857475  [   64/  124]
train() client id: f_00002-8-2 loss: 1.074775  [   96/  124]
train() client id: f_00003-0-0 loss: 0.844003  [   32/   43]
train() client id: f_00003-1-0 loss: 0.622361  [   32/   43]
train() client id: f_00003-2-0 loss: 0.768211  [   32/   43]
train() client id: f_00003-3-0 loss: 0.701576  [   32/   43]
train() client id: f_00003-4-0 loss: 0.666521  [   32/   43]
train() client id: f_00003-5-0 loss: 0.743596  [   32/   43]
train() client id: f_00003-6-0 loss: 0.688471  [   32/   43]
train() client id: f_00003-7-0 loss: 0.814086  [   32/   43]
train() client id: f_00003-8-0 loss: 0.736924  [   32/   43]
train() client id: f_00004-0-0 loss: 0.871005  [   32/  306]
train() client id: f_00004-0-1 loss: 0.827071  [   64/  306]
train() client id: f_00004-0-2 loss: 0.957220  [   96/  306]
train() client id: f_00004-0-3 loss: 0.862368  [  128/  306]
train() client id: f_00004-0-4 loss: 0.942344  [  160/  306]
train() client id: f_00004-0-5 loss: 1.084267  [  192/  306]
train() client id: f_00004-0-6 loss: 0.851470  [  224/  306]
train() client id: f_00004-0-7 loss: 0.845506  [  256/  306]
train() client id: f_00004-0-8 loss: 0.891506  [  288/  306]
train() client id: f_00004-1-0 loss: 0.864223  [   32/  306]
train() client id: f_00004-1-1 loss: 0.950255  [   64/  306]
train() client id: f_00004-1-2 loss: 0.824450  [   96/  306]
train() client id: f_00004-1-3 loss: 0.874165  [  128/  306]
train() client id: f_00004-1-4 loss: 0.850931  [  160/  306]
train() client id: f_00004-1-5 loss: 0.941463  [  192/  306]
train() client id: f_00004-1-6 loss: 0.973354  [  224/  306]
train() client id: f_00004-1-7 loss: 0.808331  [  256/  306]
train() client id: f_00004-1-8 loss: 0.886716  [  288/  306]
train() client id: f_00004-2-0 loss: 0.931689  [   32/  306]
train() client id: f_00004-2-1 loss: 0.844267  [   64/  306]
train() client id: f_00004-2-2 loss: 0.822656  [   96/  306]
train() client id: f_00004-2-3 loss: 0.909138  [  128/  306]
train() client id: f_00004-2-4 loss: 0.863060  [  160/  306]
train() client id: f_00004-2-5 loss: 0.889992  [  192/  306]
train() client id: f_00004-2-6 loss: 0.963733  [  224/  306]
train() client id: f_00004-2-7 loss: 0.900652  [  256/  306]
train() client id: f_00004-2-8 loss: 0.908236  [  288/  306]
train() client id: f_00004-3-0 loss: 0.957344  [   32/  306]
train() client id: f_00004-3-1 loss: 0.895197  [   64/  306]
train() client id: f_00004-3-2 loss: 1.028118  [   96/  306]
train() client id: f_00004-3-3 loss: 0.779140  [  128/  306]
train() client id: f_00004-3-4 loss: 0.870259  [  160/  306]
train() client id: f_00004-3-5 loss: 0.921670  [  192/  306]
train() client id: f_00004-3-6 loss: 0.738784  [  224/  306]
train() client id: f_00004-3-7 loss: 1.048014  [  256/  306]
train() client id: f_00004-3-8 loss: 0.835126  [  288/  306]
train() client id: f_00004-4-0 loss: 0.790925  [   32/  306]
train() client id: f_00004-4-1 loss: 0.829937  [   64/  306]
train() client id: f_00004-4-2 loss: 0.875430  [   96/  306]
train() client id: f_00004-4-3 loss: 0.757411  [  128/  306]
train() client id: f_00004-4-4 loss: 0.923193  [  160/  306]
train() client id: f_00004-4-5 loss: 0.963769  [  192/  306]
train() client id: f_00004-4-6 loss: 0.986153  [  224/  306]
train() client id: f_00004-4-7 loss: 0.832344  [  256/  306]
train() client id: f_00004-4-8 loss: 0.918323  [  288/  306]
train() client id: f_00004-5-0 loss: 0.892372  [   32/  306]
train() client id: f_00004-5-1 loss: 0.810265  [   64/  306]
train() client id: f_00004-5-2 loss: 0.889935  [   96/  306]
train() client id: f_00004-5-3 loss: 0.818120  [  128/  306]
train() client id: f_00004-5-4 loss: 0.934404  [  160/  306]
train() client id: f_00004-5-5 loss: 0.852118  [  192/  306]
train() client id: f_00004-5-6 loss: 0.881557  [  224/  306]
train() client id: f_00004-5-7 loss: 1.009399  [  256/  306]
train() client id: f_00004-5-8 loss: 0.850834  [  288/  306]
train() client id: f_00004-6-0 loss: 0.884651  [   32/  306]
train() client id: f_00004-6-1 loss: 0.923092  [   64/  306]
train() client id: f_00004-6-2 loss: 0.713241  [   96/  306]
train() client id: f_00004-6-3 loss: 1.071240  [  128/  306]
train() client id: f_00004-6-4 loss: 0.841784  [  160/  306]
train() client id: f_00004-6-5 loss: 0.893696  [  192/  306]
train() client id: f_00004-6-6 loss: 0.856377  [  224/  306]
train() client id: f_00004-6-7 loss: 0.906003  [  256/  306]
train() client id: f_00004-6-8 loss: 0.848145  [  288/  306]
train() client id: f_00004-7-0 loss: 0.926713  [   32/  306]
train() client id: f_00004-7-1 loss: 0.864838  [   64/  306]
train() client id: f_00004-7-2 loss: 0.815222  [   96/  306]
train() client id: f_00004-7-3 loss: 0.838275  [  128/  306]
train() client id: f_00004-7-4 loss: 0.737231  [  160/  306]
train() client id: f_00004-7-5 loss: 1.026649  [  192/  306]
train() client id: f_00004-7-6 loss: 0.848497  [  224/  306]
train() client id: f_00004-7-7 loss: 0.826765  [  256/  306]
train() client id: f_00004-7-8 loss: 0.921420  [  288/  306]
train() client id: f_00004-8-0 loss: 0.893711  [   32/  306]
train() client id: f_00004-8-1 loss: 0.952850  [   64/  306]
train() client id: f_00004-8-2 loss: 0.875257  [   96/  306]
train() client id: f_00004-8-3 loss: 0.991194  [  128/  306]
train() client id: f_00004-8-4 loss: 0.827004  [  160/  306]
train() client id: f_00004-8-5 loss: 0.822315  [  192/  306]
train() client id: f_00004-8-6 loss: 0.805641  [  224/  306]
train() client id: f_00004-8-7 loss: 0.842634  [  256/  306]
train() client id: f_00004-8-8 loss: 0.834382  [  288/  306]
train() client id: f_00005-0-0 loss: 0.925422  [   32/  146]
train() client id: f_00005-0-1 loss: 0.626514  [   64/  146]
train() client id: f_00005-0-2 loss: 0.580560  [   96/  146]
train() client id: f_00005-0-3 loss: 0.689132  [  128/  146]
train() client id: f_00005-1-0 loss: 0.929078  [   32/  146]
train() client id: f_00005-1-1 loss: 0.696926  [   64/  146]
train() client id: f_00005-1-2 loss: 0.645382  [   96/  146]
train() client id: f_00005-1-3 loss: 0.690768  [  128/  146]
train() client id: f_00005-2-0 loss: 0.607433  [   32/  146]
train() client id: f_00005-2-1 loss: 0.722779  [   64/  146]
train() client id: f_00005-2-2 loss: 0.640199  [   96/  146]
train() client id: f_00005-2-3 loss: 0.807049  [  128/  146]
train() client id: f_00005-3-0 loss: 0.494090  [   32/  146]
train() client id: f_00005-3-1 loss: 0.852899  [   64/  146]
train() client id: f_00005-3-2 loss: 0.770707  [   96/  146]
train() client id: f_00005-3-3 loss: 0.634764  [  128/  146]
train() client id: f_00005-4-0 loss: 0.376615  [   32/  146]
train() client id: f_00005-4-1 loss: 0.807109  [   64/  146]
train() client id: f_00005-4-2 loss: 0.809409  [   96/  146]
train() client id: f_00005-4-3 loss: 0.980754  [  128/  146]
train() client id: f_00005-5-0 loss: 0.689115  [   32/  146]
train() client id: f_00005-5-1 loss: 0.736658  [   64/  146]
train() client id: f_00005-5-2 loss: 0.697202  [   96/  146]
train() client id: f_00005-5-3 loss: 0.587299  [  128/  146]
train() client id: f_00005-6-0 loss: 0.629349  [   32/  146]
train() client id: f_00005-6-1 loss: 0.636735  [   64/  146]
train() client id: f_00005-6-2 loss: 0.738238  [   96/  146]
train() client id: f_00005-6-3 loss: 0.727084  [  128/  146]
train() client id: f_00005-7-0 loss: 0.642940  [   32/  146]
train() client id: f_00005-7-1 loss: 0.840309  [   64/  146]
train() client id: f_00005-7-2 loss: 0.568586  [   96/  146]
train() client id: f_00005-7-3 loss: 0.570625  [  128/  146]
train() client id: f_00005-8-0 loss: 0.504930  [   32/  146]
train() client id: f_00005-8-1 loss: 0.820487  [   64/  146]
train() client id: f_00005-8-2 loss: 0.649271  [   96/  146]
train() client id: f_00005-8-3 loss: 0.961768  [  128/  146]
train() client id: f_00006-0-0 loss: 0.453856  [   32/   54]
train() client id: f_00006-1-0 loss: 0.552343  [   32/   54]
train() client id: f_00006-2-0 loss: 0.564305  [   32/   54]
train() client id: f_00006-3-0 loss: 0.499294  [   32/   54]
train() client id: f_00006-4-0 loss: 0.499955  [   32/   54]
train() client id: f_00006-5-0 loss: 0.561429  [   32/   54]
train() client id: f_00006-6-0 loss: 0.536004  [   32/   54]
train() client id: f_00006-7-0 loss: 0.476582  [   32/   54]
train() client id: f_00006-8-0 loss: 0.515935  [   32/   54]
train() client id: f_00007-0-0 loss: 0.623709  [   32/  179]
train() client id: f_00007-0-1 loss: 0.687016  [   64/  179]
train() client id: f_00007-0-2 loss: 0.374722  [   96/  179]
train() client id: f_00007-0-3 loss: 0.706223  [  128/  179]
train() client id: f_00007-0-4 loss: 0.498114  [  160/  179]
train() client id: f_00007-1-0 loss: 0.545100  [   32/  179]
train() client id: f_00007-1-1 loss: 0.482796  [   64/  179]
train() client id: f_00007-1-2 loss: 0.539743  [   96/  179]
train() client id: f_00007-1-3 loss: 0.755410  [  128/  179]
train() client id: f_00007-1-4 loss: 0.439376  [  160/  179]
train() client id: f_00007-2-0 loss: 0.580932  [   32/  179]
train() client id: f_00007-2-1 loss: 0.560850  [   64/  179]
train() client id: f_00007-2-2 loss: 0.483583  [   96/  179]
train() client id: f_00007-2-3 loss: 0.585097  [  128/  179]
train() client id: f_00007-2-4 loss: 0.408389  [  160/  179]
train() client id: f_00007-3-0 loss: 0.388366  [   32/  179]
train() client id: f_00007-3-1 loss: 0.553003  [   64/  179]
train() client id: f_00007-3-2 loss: 0.576958  [   96/  179]
train() client id: f_00007-3-3 loss: 0.464194  [  128/  179]
train() client id: f_00007-3-4 loss: 0.673358  [  160/  179]
train() client id: f_00007-4-0 loss: 0.559698  [   32/  179]
train() client id: f_00007-4-1 loss: 0.619328  [   64/  179]
train() client id: f_00007-4-2 loss: 0.558153  [   96/  179]
train() client id: f_00007-4-3 loss: 0.443233  [  128/  179]
train() client id: f_00007-4-4 loss: 0.539792  [  160/  179]
train() client id: f_00007-5-0 loss: 0.509746  [   32/  179]
train() client id: f_00007-5-1 loss: 0.472226  [   64/  179]
train() client id: f_00007-5-2 loss: 0.521920  [   96/  179]
train() client id: f_00007-5-3 loss: 0.559026  [  128/  179]
train() client id: f_00007-5-4 loss: 0.602819  [  160/  179]
train() client id: f_00007-6-0 loss: 0.598106  [   32/  179]
train() client id: f_00007-6-1 loss: 0.622086  [   64/  179]
train() client id: f_00007-6-2 loss: 0.465028  [   96/  179]
train() client id: f_00007-6-3 loss: 0.469459  [  128/  179]
train() client id: f_00007-6-4 loss: 0.451527  [  160/  179]
train() client id: f_00007-7-0 loss: 0.426742  [   32/  179]
train() client id: f_00007-7-1 loss: 0.397069  [   64/  179]
train() client id: f_00007-7-2 loss: 0.585349  [   96/  179]
train() client id: f_00007-7-3 loss: 0.494731  [  128/  179]
train() client id: f_00007-7-4 loss: 0.729622  [  160/  179]
train() client id: f_00007-8-0 loss: 0.714871  [   32/  179]
train() client id: f_00007-8-1 loss: 0.357095  [   64/  179]
train() client id: f_00007-8-2 loss: 0.455467  [   96/  179]
train() client id: f_00007-8-3 loss: 0.546334  [  128/  179]
train() client id: f_00007-8-4 loss: 0.463019  [  160/  179]
train() client id: f_00008-0-0 loss: 0.673348  [   32/  130]
train() client id: f_00008-0-1 loss: 0.739345  [   64/  130]
train() client id: f_00008-0-2 loss: 0.659423  [   96/  130]
train() client id: f_00008-0-3 loss: 0.823504  [  128/  130]
train() client id: f_00008-1-0 loss: 0.685718  [   32/  130]
train() client id: f_00008-1-1 loss: 0.674529  [   64/  130]
train() client id: f_00008-1-2 loss: 0.742456  [   96/  130]
train() client id: f_00008-1-3 loss: 0.783530  [  128/  130]
train() client id: f_00008-2-0 loss: 0.667284  [   32/  130]
train() client id: f_00008-2-1 loss: 0.705796  [   64/  130]
train() client id: f_00008-2-2 loss: 0.710425  [   96/  130]
train() client id: f_00008-2-3 loss: 0.735125  [  128/  130]
train() client id: f_00008-3-0 loss: 0.661288  [   32/  130]
train() client id: f_00008-3-1 loss: 0.650447  [   64/  130]
train() client id: f_00008-3-2 loss: 0.865741  [   96/  130]
train() client id: f_00008-3-3 loss: 0.681282  [  128/  130]
train() client id: f_00008-4-0 loss: 0.666499  [   32/  130]
train() client id: f_00008-4-1 loss: 0.699611  [   64/  130]
train() client id: f_00008-4-2 loss: 0.746131  [   96/  130]
train() client id: f_00008-4-3 loss: 0.773728  [  128/  130]
train() client id: f_00008-5-0 loss: 0.694312  [   32/  130]
train() client id: f_00008-5-1 loss: 0.793823  [   64/  130]
train() client id: f_00008-5-2 loss: 0.695926  [   96/  130]
train() client id: f_00008-5-3 loss: 0.669598  [  128/  130]
train() client id: f_00008-6-0 loss: 0.720822  [   32/  130]
train() client id: f_00008-6-1 loss: 0.656472  [   64/  130]
train() client id: f_00008-6-2 loss: 0.729408  [   96/  130]
train() client id: f_00008-6-3 loss: 0.748790  [  128/  130]
train() client id: f_00008-7-0 loss: 0.754259  [   32/  130]
train() client id: f_00008-7-1 loss: 0.774280  [   64/  130]
train() client id: f_00008-7-2 loss: 0.676590  [   96/  130]
train() client id: f_00008-7-3 loss: 0.657805  [  128/  130]
train() client id: f_00008-8-0 loss: 0.698017  [   32/  130]
train() client id: f_00008-8-1 loss: 0.673821  [   64/  130]
train() client id: f_00008-8-2 loss: 0.809895  [   96/  130]
train() client id: f_00008-8-3 loss: 0.716617  [  128/  130]
train() client id: f_00009-0-0 loss: 0.886224  [   32/  118]
train() client id: f_00009-0-1 loss: 0.866147  [   64/  118]
train() client id: f_00009-0-2 loss: 0.965420  [   96/  118]
train() client id: f_00009-1-0 loss: 0.907173  [   32/  118]
train() client id: f_00009-1-1 loss: 0.853018  [   64/  118]
train() client id: f_00009-1-2 loss: 0.769155  [   96/  118]
train() client id: f_00009-2-0 loss: 0.657544  [   32/  118]
train() client id: f_00009-2-1 loss: 0.840837  [   64/  118]
train() client id: f_00009-2-2 loss: 0.945213  [   96/  118]
train() client id: f_00009-3-0 loss: 0.863273  [   32/  118]
train() client id: f_00009-3-1 loss: 0.917732  [   64/  118]
train() client id: f_00009-3-2 loss: 0.659177  [   96/  118]
train() client id: f_00009-4-0 loss: 0.851817  [   32/  118]
train() client id: f_00009-4-1 loss: 0.746401  [   64/  118]
train() client id: f_00009-4-2 loss: 0.966256  [   96/  118]
train() client id: f_00009-5-0 loss: 0.769936  [   32/  118]
train() client id: f_00009-5-1 loss: 0.719529  [   64/  118]
train() client id: f_00009-5-2 loss: 0.811510  [   96/  118]
train() client id: f_00009-6-0 loss: 0.727148  [   32/  118]
train() client id: f_00009-6-1 loss: 0.840861  [   64/  118]
train() client id: f_00009-6-2 loss: 0.696408  [   96/  118]
train() client id: f_00009-7-0 loss: 0.771016  [   32/  118]
train() client id: f_00009-7-1 loss: 0.694347  [   64/  118]
train() client id: f_00009-7-2 loss: 0.794063  [   96/  118]
train() client id: f_00009-8-0 loss: 0.764017  [   32/  118]
train() client id: f_00009-8-1 loss: 0.765726  [   64/  118]
train() client id: f_00009-8-2 loss: 0.776015  [   96/  118]
At round 66 accuracy: 0.649867374005305
At round 66 training accuracy: 0.5881958417169685
At round 66 training loss: 0.8212255905564936
update_location
xs = -3.905658 4.200318 350.009024 18.811294 0.979296 3.956410 -312.443192 -291.324852 334.663977 -277.060879 
ys = 342.587959 325.555839 1.320614 -312.455176 304.350187 287.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -17.21142616811743
ys mean: 96.89414253552872
dists_uav = 356.905820 340.593962 364.016567 328.606302 320.359166 304.717309 328.066516 308.011113 349.726532 294.582318 
uav_gains = -119.538245 -118.635267 -119.892382 -117.879958 -117.310750 -116.117831 -117.843965 -116.381057 -119.157291 -115.271610 
uav_gains_db_mean: -117.80283557964958
dists_bs = 239.497386 234.050880 552.990962 524.571603 218.403717 211.547700 224.591331 209.450055 533.424678 199.140443 
bs_gains = -106.187481 -105.907747 -116.363164 -115.721593 -105.066340 -104.678492 -105.406063 -104.557313 -115.925106 -103.943524 
bs_gains_db_mean: -108.37568222073605
Round 67
-------------------------------
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.14668611 4.26785824 2.12126966 0.79869584 4.91942362 2.36837405
 0.97356032 2.94941557 2.16407019 1.92059068]
obj_prev = 24.629944275071566
eta_min = 3.445448803028318e-44	eta_max = 0.9459678027745403
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 5.6197462444887005	eta = 0.9090909090909091
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 14.777404826319295	eta = 0.34572107093955495
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 9.602497695143875	eta = 0.5320345168992939
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.719456153128892	eta = 0.585915008062652
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.663852506476184	eta = 0.5896753457475884
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.663598265580141	eta = 0.5896926502882287
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.66359826021894	eta = 0.5896926506531418
eta = 0.5896926506531418
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [0.04358199 0.09166053 0.04289021 0.01487322 0.10584197 0.05049976
 0.01867799 0.06191409 0.04496554 0.04081488]
ene_total = [0.88965002 1.27512756 0.89641868 0.46309031 1.45131187 0.74360323
 0.51049018 1.02200221 0.79540655 0.61649765]
ti_comp = [1.82806619 2.00655166 1.81584028 1.87549598 2.01019846 2.01177704
 1.87637173 1.90759375 1.91580954 2.01460717]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [1.54816501e-06 1.19543613e-05 1.49554405e-06 5.84605015e-08
 1.83390085e-05 1.98878720e-06 1.15673257e-07 4.07639963e-06
 1.54815642e-06 1.04701937e-06]
ene_total = [0.33393439 0.10647147 0.34952357 0.27343533 0.10190267 0.09968125
 0.27231935 0.23255711 0.22204852 0.09606041]
optimize_network iter = 0 obj = 2.0879340736516
eta = 0.5896926506531418
freqs = [11920243.00940963 22840310.78845544 11810016.9072595   3965142.48672103
 26326248.17150466 12551033.48678927  4977157.22105609 16228322.10506473
 11735389.30408466 10129735.66511689]
eta_min = 0.589692650653142	eta_max = 0.7681327803953306
af = 0.00041776276143721885	bf = 0.8564740523773019	zeta = 0.00045953903758094077	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [2.79380519e-07 2.15727370e-06 2.69884586e-07 1.05497316e-08
 3.30944163e-06 3.58894819e-07 2.08742959e-08 7.35623554e-07
 2.79378970e-07 1.88944211e-07]
ene_total = [1.50710005 0.47998245 1.57746246 1.23411502 0.45906057 0.44980564
 1.22907549 1.04942654 1.00211652 0.43350782]
ti_comp = [0.91916882 1.09765429 0.90694292 0.96659861 1.10130109 1.10287968
 0.96747436 0.99869639 1.00691217 1.1057098 ]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.62987513e-07 4.97740325e-06 7.46967171e-07 2.74226484e-08
 7.61287159e-06 8.24515199e-07 5.42124102e-08 1.85305581e-06
 6.98301926e-07 4.33071962e-07]
ene_total = [0.59090555 0.18825224 0.61849267 0.4838647  0.18008278 0.17636757
 0.4818892  0.411478   0.39291321 0.16997262]
optimize_network iter = 1 obj = 3.694218537284266
eta = 0.7681327803953306
freqs = [11840881.82931954 20853986.15252445 11810016.9072595   3842652.168879
 24000712.97449805 11434929.97175277  4821288.08222465 15482045.45566182
 11152199.96866208  9218274.46340382]
eta_min = 0.7681327803953335	eta_max = 0.7681327803953281
af = 0.0003568274848523184	bf = 0.8564740523773019	zeta = 0.00039251023333755023	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [2.75672850e-07 1.79837142e-06 2.69884586e-07 9.90799918e-09
 2.75058500e-06 2.97903243e-07 1.95873319e-08 6.69522327e-07
 2.52301484e-07 1.56472000e-07]
ene_total = [1.50709984 0.47996179 1.57746246 1.23411499 0.4590284  0.44980213
 1.22907542 1.04942273 1.00211496 0.43350595]
ti_comp = [0.91916882 1.09765429 0.90694292 0.96659861 1.10130109 1.10287968
 0.96747436 0.99869639 1.00691217 1.1057098 ]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.62987513e-07 4.97740325e-06 7.46967171e-07 2.74226484e-08
 7.61287159e-06 8.24515199e-07 5.42124102e-08 1.85305581e-06
 6.98301926e-07 4.33071962e-07]
ene_total = [0.59090555 0.18825224 0.61849267 0.4838647  0.18008278 0.17636757
 0.4818892  0.411478   0.39291321 0.16997262]
optimize_network iter = 2 obj = 3.6942185372842253
eta = 0.7681327803953281
freqs = [11840881.82931953 20853986.15252446 11810016.90725948  3842652.168879
 24000712.97449807 11434929.97175278  4821288.08222465 15482045.45566182
 11152199.96866208  9218274.46340384]
Done!
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.06640672e-07 4.60982063e-06 6.91803437e-07 2.53974782e-08
 7.05065889e-06 7.63624521e-07 5.02088085e-08 1.71620712e-06
 6.46732134e-07 4.01089477e-07]
ene_total = [0.02618703 0.00834238 0.0274096  0.02144336 0.00798014 0.007816
 0.02135582 0.01823528 0.01741263 0.00753262]
At round 67 energy consumption: 0.16371485787111706
At round 67 eta: 0.7681327803953281
At round 67 a_n: 5.2320311093101175
At round 67 local rounds: 8.637912231455728
At round 67 global rounds: 22.564772710133866
gradient difference: 0.5302717089653015
train() client id: f_00000-0-0 loss: 1.087654  [   32/  126]
train() client id: f_00000-0-1 loss: 1.047550  [   64/  126]
train() client id: f_00000-0-2 loss: 1.208606  [   96/  126]
train() client id: f_00000-1-0 loss: 1.019111  [   32/  126]
train() client id: f_00000-1-1 loss: 1.110072  [   64/  126]
train() client id: f_00000-1-2 loss: 0.867214  [   96/  126]
train() client id: f_00000-2-0 loss: 0.821604  [   32/  126]
train() client id: f_00000-2-1 loss: 0.910025  [   64/  126]
train() client id: f_00000-2-2 loss: 1.039921  [   96/  126]
train() client id: f_00000-3-0 loss: 0.915441  [   32/  126]
train() client id: f_00000-3-1 loss: 0.798124  [   64/  126]
train() client id: f_00000-3-2 loss: 1.040622  [   96/  126]
train() client id: f_00000-4-0 loss: 0.686332  [   32/  126]
train() client id: f_00000-4-1 loss: 0.915697  [   64/  126]
train() client id: f_00000-4-2 loss: 1.082428  [   96/  126]
train() client id: f_00000-5-0 loss: 0.962404  [   32/  126]
train() client id: f_00000-5-1 loss: 0.789187  [   64/  126]
train() client id: f_00000-5-2 loss: 0.765090  [   96/  126]
train() client id: f_00000-6-0 loss: 0.898920  [   32/  126]
train() client id: f_00000-6-1 loss: 0.829245  [   64/  126]
train() client id: f_00000-6-2 loss: 0.869568  [   96/  126]
train() client id: f_00000-7-0 loss: 0.806651  [   32/  126]
train() client id: f_00000-7-1 loss: 0.824588  [   64/  126]
train() client id: f_00000-7-2 loss: 0.836759  [   96/  126]
train() client id: f_00001-0-0 loss: 0.382937  [   32/  265]
train() client id: f_00001-0-1 loss: 0.374643  [   64/  265]
train() client id: f_00001-0-2 loss: 0.441869  [   96/  265]
train() client id: f_00001-0-3 loss: 0.353471  [  128/  265]
train() client id: f_00001-0-4 loss: 0.594968  [  160/  265]
train() client id: f_00001-0-5 loss: 0.410925  [  192/  265]
train() client id: f_00001-0-6 loss: 0.496292  [  224/  265]
train() client id: f_00001-0-7 loss: 0.519531  [  256/  265]
train() client id: f_00001-1-0 loss: 0.534824  [   32/  265]
train() client id: f_00001-1-1 loss: 0.399330  [   64/  265]
train() client id: f_00001-1-2 loss: 0.524210  [   96/  265]
train() client id: f_00001-1-3 loss: 0.331993  [  128/  265]
train() client id: f_00001-1-4 loss: 0.501133  [  160/  265]
train() client id: f_00001-1-5 loss: 0.434596  [  192/  265]
train() client id: f_00001-1-6 loss: 0.335838  [  224/  265]
train() client id: f_00001-1-7 loss: 0.447227  [  256/  265]
train() client id: f_00001-2-0 loss: 0.435613  [   32/  265]
train() client id: f_00001-2-1 loss: 0.382969  [   64/  265]
train() client id: f_00001-2-2 loss: 0.342521  [   96/  265]
train() client id: f_00001-2-3 loss: 0.417888  [  128/  265]
train() client id: f_00001-2-4 loss: 0.498111  [  160/  265]
train() client id: f_00001-2-5 loss: 0.436270  [  192/  265]
train() client id: f_00001-2-6 loss: 0.598398  [  224/  265]
train() client id: f_00001-2-7 loss: 0.365057  [  256/  265]
train() client id: f_00001-3-0 loss: 0.422861  [   32/  265]
train() client id: f_00001-3-1 loss: 0.337078  [   64/  265]
train() client id: f_00001-3-2 loss: 0.400235  [   96/  265]
train() client id: f_00001-3-3 loss: 0.381768  [  128/  265]
train() client id: f_00001-3-4 loss: 0.515689  [  160/  265]
train() client id: f_00001-3-5 loss: 0.332651  [  192/  265]
train() client id: f_00001-3-6 loss: 0.611759  [  224/  265]
train() client id: f_00001-3-7 loss: 0.336762  [  256/  265]
train() client id: f_00001-4-0 loss: 0.342390  [   32/  265]
train() client id: f_00001-4-1 loss: 0.461639  [   64/  265]
train() client id: f_00001-4-2 loss: 0.491553  [   96/  265]
train() client id: f_00001-4-3 loss: 0.478741  [  128/  265]
train() client id: f_00001-4-4 loss: 0.495508  [  160/  265]
train() client id: f_00001-4-5 loss: 0.384264  [  192/  265]
train() client id: f_00001-4-6 loss: 0.379402  [  224/  265]
train() client id: f_00001-4-7 loss: 0.371626  [  256/  265]
train() client id: f_00001-5-0 loss: 0.389198  [   32/  265]
train() client id: f_00001-5-1 loss: 0.336434  [   64/  265]
train() client id: f_00001-5-2 loss: 0.390621  [   96/  265]
train() client id: f_00001-5-3 loss: 0.402312  [  128/  265]
train() client id: f_00001-5-4 loss: 0.440186  [  160/  265]
train() client id: f_00001-5-5 loss: 0.470540  [  192/  265]
train() client id: f_00001-5-6 loss: 0.385840  [  224/  265]
train() client id: f_00001-5-7 loss: 0.537125  [  256/  265]
train() client id: f_00001-6-0 loss: 0.421966  [   32/  265]
train() client id: f_00001-6-1 loss: 0.324328  [   64/  265]
train() client id: f_00001-6-2 loss: 0.403434  [   96/  265]
train() client id: f_00001-6-3 loss: 0.403450  [  128/  265]
train() client id: f_00001-6-4 loss: 0.322571  [  160/  265]
train() client id: f_00001-6-5 loss: 0.553305  [  192/  265]
train() client id: f_00001-6-6 loss: 0.435955  [  224/  265]
train() client id: f_00001-6-7 loss: 0.510193  [  256/  265]
train() client id: f_00001-7-0 loss: 0.317756  [   32/  265]
train() client id: f_00001-7-1 loss: 0.476702  [   64/  265]
train() client id: f_00001-7-2 loss: 0.309849  [   96/  265]
train() client id: f_00001-7-3 loss: 0.396644  [  128/  265]
train() client id: f_00001-7-4 loss: 0.402583  [  160/  265]
train() client id: f_00001-7-5 loss: 0.331588  [  192/  265]
train() client id: f_00001-7-6 loss: 0.479795  [  224/  265]
train() client id: f_00001-7-7 loss: 0.611226  [  256/  265]
train() client id: f_00002-0-0 loss: 1.020409  [   32/  124]
train() client id: f_00002-0-1 loss: 0.765865  [   64/  124]
train() client id: f_00002-0-2 loss: 0.907818  [   96/  124]
train() client id: f_00002-1-0 loss: 0.907259  [   32/  124]
train() client id: f_00002-1-1 loss: 1.145656  [   64/  124]
train() client id: f_00002-1-2 loss: 0.790239  [   96/  124]
train() client id: f_00002-2-0 loss: 0.763670  [   32/  124]
train() client id: f_00002-2-1 loss: 1.128750  [   64/  124]
train() client id: f_00002-2-2 loss: 0.962534  [   96/  124]
train() client id: f_00002-3-0 loss: 0.679868  [   32/  124]
train() client id: f_00002-3-1 loss: 0.919311  [   64/  124]
train() client id: f_00002-3-2 loss: 0.857805  [   96/  124]
train() client id: f_00002-4-0 loss: 0.939979  [   32/  124]
train() client id: f_00002-4-1 loss: 1.001287  [   64/  124]
train() client id: f_00002-4-2 loss: 0.853337  [   96/  124]
train() client id: f_00002-5-0 loss: 0.843514  [   32/  124]
train() client id: f_00002-5-1 loss: 0.861817  [   64/  124]
train() client id: f_00002-5-2 loss: 0.861540  [   96/  124]
train() client id: f_00002-6-0 loss: 0.993155  [   32/  124]
train() client id: f_00002-6-1 loss: 0.858575  [   64/  124]
train() client id: f_00002-6-2 loss: 0.736482  [   96/  124]
train() client id: f_00002-7-0 loss: 0.786994  [   32/  124]
train() client id: f_00002-7-1 loss: 0.791155  [   64/  124]
train() client id: f_00002-7-2 loss: 0.888617  [   96/  124]
train() client id: f_00003-0-0 loss: 0.535631  [   32/   43]
train() client id: f_00003-1-0 loss: 0.613099  [   32/   43]
train() client id: f_00003-2-0 loss: 0.637860  [   32/   43]
train() client id: f_00003-3-0 loss: 0.509326  [   32/   43]
train() client id: f_00003-4-0 loss: 0.857591  [   32/   43]
train() client id: f_00003-5-0 loss: 0.422296  [   32/   43]
train() client id: f_00003-6-0 loss: 0.473621  [   32/   43]
train() client id: f_00003-7-0 loss: 0.682605  [   32/   43]
train() client id: f_00004-0-0 loss: 1.014244  [   32/  306]
train() client id: f_00004-0-1 loss: 0.947646  [   64/  306]
train() client id: f_00004-0-2 loss: 0.783463  [   96/  306]
train() client id: f_00004-0-3 loss: 0.833695  [  128/  306]
train() client id: f_00004-0-4 loss: 0.866711  [  160/  306]
train() client id: f_00004-0-5 loss: 0.878179  [  192/  306]
train() client id: f_00004-0-6 loss: 0.954085  [  224/  306]
train() client id: f_00004-0-7 loss: 0.935641  [  256/  306]
train() client id: f_00004-0-8 loss: 0.914205  [  288/  306]
train() client id: f_00004-1-0 loss: 0.885219  [   32/  306]
train() client id: f_00004-1-1 loss: 0.974601  [   64/  306]
train() client id: f_00004-1-2 loss: 0.922500  [   96/  306]
train() client id: f_00004-1-3 loss: 0.928065  [  128/  306]
train() client id: f_00004-1-4 loss: 0.906186  [  160/  306]
train() client id: f_00004-1-5 loss: 0.980524  [  192/  306]
train() client id: f_00004-1-6 loss: 0.907928  [  224/  306]
train() client id: f_00004-1-7 loss: 0.927613  [  256/  306]
train() client id: f_00004-1-8 loss: 0.872737  [  288/  306]
train() client id: f_00004-2-0 loss: 0.793665  [   32/  306]
train() client id: f_00004-2-1 loss: 0.915020  [   64/  306]
train() client id: f_00004-2-2 loss: 0.905349  [   96/  306]
train() client id: f_00004-2-3 loss: 0.925222  [  128/  306]
train() client id: f_00004-2-4 loss: 0.974986  [  160/  306]
train() client id: f_00004-2-5 loss: 0.870887  [  192/  306]
train() client id: f_00004-2-6 loss: 0.942092  [  224/  306]
train() client id: f_00004-2-7 loss: 0.993222  [  256/  306]
train() client id: f_00004-2-8 loss: 0.825397  [  288/  306]
train() client id: f_00004-3-0 loss: 1.027764  [   32/  306]
train() client id: f_00004-3-1 loss: 0.937805  [   64/  306]
train() client id: f_00004-3-2 loss: 0.857882  [   96/  306]
train() client id: f_00004-3-3 loss: 0.930288  [  128/  306]
train() client id: f_00004-3-4 loss: 0.880113  [  160/  306]
train() client id: f_00004-3-5 loss: 0.881426  [  192/  306]
train() client id: f_00004-3-6 loss: 0.850952  [  224/  306]
train() client id: f_00004-3-7 loss: 0.939205  [  256/  306]
train() client id: f_00004-3-8 loss: 0.890420  [  288/  306]
train() client id: f_00004-4-0 loss: 0.821153  [   32/  306]
train() client id: f_00004-4-1 loss: 0.772099  [   64/  306]
train() client id: f_00004-4-2 loss: 0.967115  [   96/  306]
train() client id: f_00004-4-3 loss: 0.952244  [  128/  306]
train() client id: f_00004-4-4 loss: 0.860738  [  160/  306]
train() client id: f_00004-4-5 loss: 0.896238  [  192/  306]
train() client id: f_00004-4-6 loss: 1.006766  [  224/  306]
train() client id: f_00004-4-7 loss: 0.835607  [  256/  306]
train() client id: f_00004-4-8 loss: 0.960514  [  288/  306]
train() client id: f_00004-5-0 loss: 0.939735  [   32/  306]
train() client id: f_00004-5-1 loss: 0.783515  [   64/  306]
train() client id: f_00004-5-2 loss: 0.957318  [   96/  306]
train() client id: f_00004-5-3 loss: 0.803793  [  128/  306]
train() client id: f_00004-5-4 loss: 0.884380  [  160/  306]
train() client id: f_00004-5-5 loss: 1.062728  [  192/  306]
train() client id: f_00004-5-6 loss: 0.821448  [  224/  306]
train() client id: f_00004-5-7 loss: 0.840610  [  256/  306]
train() client id: f_00004-5-8 loss: 0.930014  [  288/  306]
train() client id: f_00004-6-0 loss: 0.995571  [   32/  306]
train() client id: f_00004-6-1 loss: 0.888929  [   64/  306]
train() client id: f_00004-6-2 loss: 0.841046  [   96/  306]
train() client id: f_00004-6-3 loss: 0.764900  [  128/  306]
train() client id: f_00004-6-4 loss: 1.037972  [  160/  306]
train() client id: f_00004-6-5 loss: 0.931632  [  192/  306]
train() client id: f_00004-6-6 loss: 0.943254  [  224/  306]
train() client id: f_00004-6-7 loss: 0.833599  [  256/  306]
train() client id: f_00004-6-8 loss: 0.865383  [  288/  306]
train() client id: f_00004-7-0 loss: 0.752567  [   32/  306]
train() client id: f_00004-7-1 loss: 0.945595  [   64/  306]
train() client id: f_00004-7-2 loss: 0.908783  [   96/  306]
train() client id: f_00004-7-3 loss: 0.911005  [  128/  306]
train() client id: f_00004-7-4 loss: 0.841339  [  160/  306]
train() client id: f_00004-7-5 loss: 0.891944  [  192/  306]
train() client id: f_00004-7-6 loss: 0.878843  [  224/  306]
train() client id: f_00004-7-7 loss: 0.883641  [  256/  306]
train() client id: f_00004-7-8 loss: 0.891512  [  288/  306]
train() client id: f_00005-0-0 loss: 0.230017  [   32/  146]
train() client id: f_00005-0-1 loss: 0.542342  [   64/  146]
train() client id: f_00005-0-2 loss: 0.428075  [   96/  146]
train() client id: f_00005-0-3 loss: 0.226490  [  128/  146]
train() client id: f_00005-1-0 loss: 0.670898  [   32/  146]
train() client id: f_00005-1-1 loss: 0.125723  [   64/  146]
train() client id: f_00005-1-2 loss: 0.296499  [   96/  146]
train() client id: f_00005-1-3 loss: 0.429316  [  128/  146]
train() client id: f_00005-2-0 loss: 0.236415  [   32/  146]
train() client id: f_00005-2-1 loss: 0.494565  [   64/  146]
train() client id: f_00005-2-2 loss: 0.376756  [   96/  146]
train() client id: f_00005-2-3 loss: 0.291205  [  128/  146]
train() client id: f_00005-3-0 loss: 0.161002  [   32/  146]
train() client id: f_00005-3-1 loss: 0.414242  [   64/  146]
train() client id: f_00005-3-2 loss: 0.442614  [   96/  146]
train() client id: f_00005-3-3 loss: 0.295181  [  128/  146]
train() client id: f_00005-4-0 loss: 0.252606  [   32/  146]
train() client id: f_00005-4-1 loss: 0.487132  [   64/  146]
train() client id: f_00005-4-2 loss: 0.305477  [   96/  146]
train() client id: f_00005-4-3 loss: 0.295448  [  128/  146]
train() client id: f_00005-5-0 loss: 0.254372  [   32/  146]
train() client id: f_00005-5-1 loss: 0.301718  [   64/  146]
train() client id: f_00005-5-2 loss: 0.534980  [   96/  146]
train() client id: f_00005-5-3 loss: 0.461580  [  128/  146]
train() client id: f_00005-6-0 loss: 0.229342  [   32/  146]
train() client id: f_00005-6-1 loss: 0.618160  [   64/  146]
train() client id: f_00005-6-2 loss: 0.465054  [   96/  146]
train() client id: f_00005-6-3 loss: 0.217251  [  128/  146]
train() client id: f_00005-7-0 loss: 0.842170  [   32/  146]
train() client id: f_00005-7-1 loss: 0.330554  [   64/  146]
train() client id: f_00005-7-2 loss: 0.150455  [   96/  146]
train() client id: f_00005-7-3 loss: 0.346027  [  128/  146]
train() client id: f_00006-0-0 loss: 0.481023  [   32/   54]
train() client id: f_00006-1-0 loss: 0.543386  [   32/   54]
train() client id: f_00006-2-0 loss: 0.447414  [   32/   54]
train() client id: f_00006-3-0 loss: 0.468700  [   32/   54]
train() client id: f_00006-4-0 loss: 0.559999  [   32/   54]
train() client id: f_00006-5-0 loss: 0.506878  [   32/   54]
train() client id: f_00006-6-0 loss: 0.511867  [   32/   54]
train() client id: f_00006-7-0 loss: 0.428278  [   32/   54]
train() client id: f_00007-0-0 loss: 0.565192  [   32/  179]
train() client id: f_00007-0-1 loss: 0.674419  [   64/  179]
train() client id: f_00007-0-2 loss: 0.629300  [   96/  179]
train() client id: f_00007-0-3 loss: 0.680806  [  128/  179]
train() client id: f_00007-0-4 loss: 0.682554  [  160/  179]
train() client id: f_00007-1-0 loss: 0.496698  [   32/  179]
train() client id: f_00007-1-1 loss: 0.523495  [   64/  179]
train() client id: f_00007-1-2 loss: 0.671466  [   96/  179]
train() client id: f_00007-1-3 loss: 0.653284  [  128/  179]
train() client id: f_00007-1-4 loss: 0.976023  [  160/  179]
train() client id: f_00007-2-0 loss: 0.753442  [   32/  179]
train() client id: f_00007-2-1 loss: 0.558086  [   64/  179]
train() client id: f_00007-2-2 loss: 0.584577  [   96/  179]
train() client id: f_00007-2-3 loss: 0.494326  [  128/  179]
train() client id: f_00007-2-4 loss: 0.727814  [  160/  179]
train() client id: f_00007-3-0 loss: 0.627487  [   32/  179]
train() client id: f_00007-3-1 loss: 0.506840  [   64/  179]
train() client id: f_00007-3-2 loss: 0.586528  [   96/  179]
train() client id: f_00007-3-3 loss: 0.716631  [  128/  179]
train() client id: f_00007-3-4 loss: 0.726487  [  160/  179]
train() client id: f_00007-4-0 loss: 0.644485  [   32/  179]
train() client id: f_00007-4-1 loss: 0.480042  [   64/  179]
train() client id: f_00007-4-2 loss: 0.740089  [   96/  179]
train() client id: f_00007-4-3 loss: 0.446626  [  128/  179]
train() client id: f_00007-4-4 loss: 0.739680  [  160/  179]
train() client id: f_00007-5-0 loss: 1.080137  [   32/  179]
train() client id: f_00007-5-1 loss: 0.491232  [   64/  179]
train() client id: f_00007-5-2 loss: 0.608518  [   96/  179]
train() client id: f_00007-5-3 loss: 0.509836  [  128/  179]
train() client id: f_00007-5-4 loss: 0.530991  [  160/  179]
train() client id: f_00007-6-0 loss: 0.741868  [   32/  179]
train() client id: f_00007-6-1 loss: 0.641084  [   64/  179]
train() client id: f_00007-6-2 loss: 0.523305  [   96/  179]
train() client id: f_00007-6-3 loss: 0.477148  [  128/  179]
train() client id: f_00007-6-4 loss: 0.704343  [  160/  179]
train() client id: f_00007-7-0 loss: 0.835492  [   32/  179]
train() client id: f_00007-7-1 loss: 0.476302  [   64/  179]
train() client id: f_00007-7-2 loss: 0.434823  [   96/  179]
train() client id: f_00007-7-3 loss: 0.736363  [  128/  179]
train() client id: f_00007-7-4 loss: 0.655512  [  160/  179]
train() client id: f_00008-0-0 loss: 0.761964  [   32/  130]
train() client id: f_00008-0-1 loss: 0.671012  [   64/  130]
train() client id: f_00008-0-2 loss: 0.600206  [   96/  130]
train() client id: f_00008-0-3 loss: 0.674418  [  128/  130]
train() client id: f_00008-1-0 loss: 0.735483  [   32/  130]
train() client id: f_00008-1-1 loss: 0.654682  [   64/  130]
train() client id: f_00008-1-2 loss: 0.721819  [   96/  130]
train() client id: f_00008-1-3 loss: 0.629563  [  128/  130]
train() client id: f_00008-2-0 loss: 0.775690  [   32/  130]
train() client id: f_00008-2-1 loss: 0.673263  [   64/  130]
train() client id: f_00008-2-2 loss: 0.682433  [   96/  130]
train() client id: f_00008-2-3 loss: 0.612048  [  128/  130]
train() client id: f_00008-3-0 loss: 0.716073  [   32/  130]
train() client id: f_00008-3-1 loss: 0.522916  [   64/  130]
train() client id: f_00008-3-2 loss: 0.809931  [   96/  130]
train() client id: f_00008-3-3 loss: 0.645930  [  128/  130]
train() client id: f_00008-4-0 loss: 0.643827  [   32/  130]
train() client id: f_00008-4-1 loss: 0.713960  [   64/  130]
train() client id: f_00008-4-2 loss: 0.631214  [   96/  130]
train() client id: f_00008-4-3 loss: 0.754117  [  128/  130]
train() client id: f_00008-5-0 loss: 0.610456  [   32/  130]
train() client id: f_00008-5-1 loss: 0.738930  [   64/  130]
train() client id: f_00008-5-2 loss: 0.740111  [   96/  130]
train() client id: f_00008-5-3 loss: 0.663709  [  128/  130]
train() client id: f_00008-6-0 loss: 0.694464  [   32/  130]
train() client id: f_00008-6-1 loss: 0.549203  [   64/  130]
train() client id: f_00008-6-2 loss: 0.747737  [   96/  130]
train() client id: f_00008-6-3 loss: 0.756279  [  128/  130]
train() client id: f_00008-7-0 loss: 0.634943  [   32/  130]
train() client id: f_00008-7-1 loss: 0.663745  [   64/  130]
train() client id: f_00008-7-2 loss: 0.744739  [   96/  130]
train() client id: f_00008-7-3 loss: 0.688008  [  128/  130]
train() client id: f_00009-0-0 loss: 0.850902  [   32/  118]
train() client id: f_00009-0-1 loss: 1.031003  [   64/  118]
train() client id: f_00009-0-2 loss: 1.140723  [   96/  118]
train() client id: f_00009-1-0 loss: 0.841391  [   32/  118]
train() client id: f_00009-1-1 loss: 1.078261  [   64/  118]
train() client id: f_00009-1-2 loss: 1.003331  [   96/  118]
train() client id: f_00009-2-0 loss: 0.999623  [   32/  118]
train() client id: f_00009-2-1 loss: 1.001445  [   64/  118]
train() client id: f_00009-2-2 loss: 0.878594  [   96/  118]
train() client id: f_00009-3-0 loss: 0.799820  [   32/  118]
train() client id: f_00009-3-1 loss: 0.960282  [   64/  118]
train() client id: f_00009-3-2 loss: 0.963230  [   96/  118]
train() client id: f_00009-4-0 loss: 0.889089  [   32/  118]
train() client id: f_00009-4-1 loss: 0.852799  [   64/  118]
train() client id: f_00009-4-2 loss: 1.001426  [   96/  118]
train() client id: f_00009-5-0 loss: 0.779793  [   32/  118]
train() client id: f_00009-5-1 loss: 0.932752  [   64/  118]
train() client id: f_00009-5-2 loss: 0.836101  [   96/  118]
train() client id: f_00009-6-0 loss: 0.823063  [   32/  118]
train() client id: f_00009-6-1 loss: 0.817489  [   64/  118]
train() client id: f_00009-6-2 loss: 0.893120  [   96/  118]
train() client id: f_00009-7-0 loss: 1.015333  [   32/  118]
train() client id: f_00009-7-1 loss: 0.748187  [   64/  118]
train() client id: f_00009-7-2 loss: 0.835656  [   96/  118]
At round 67 accuracy: 0.649867374005305
At round 67 training accuracy: 0.5881958417169685
At round 67 training loss: 0.8308562402470004
update_location
xs = -3.905658 4.200318 355.009024 18.811294 0.979296 3.956410 -317.443192 -296.324852 339.663977 -282.060879 
ys = 347.587959 330.555839 1.320614 -317.455176 309.350187 292.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -17.71142616811743
ys mean: 98.39414253552872
dists_uav = 361.707954 345.376324 368.826722 333.364145 325.113053 309.444308 332.831895 312.744455 354.514156 299.289744 
uav_gains = -119.779803 -118.914088 -120.119944 -118.189706 -117.643912 -116.493645 -118.155719 -116.748291 -119.414056 -115.671097 
uav_gains_db_mean: -118.11302619398094
dists_bs = 243.022381 237.298068 557.740168 529.221048 221.401638 214.255853 227.685524 212.267695 538.203983 201.748668 
bs_gains = -106.365155 -106.075297 -116.467153 -115.828898 -105.232122 -104.833175 -105.572451 -104.719808 -116.033573 -104.101758 
bs_gains_db_mean: -108.52293896707079
Round 68
-------------------------------
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.01021389 3.98881553 1.98649518 0.75024676 4.59768714 2.21362175
 0.91366287 2.75982094 2.02332361 1.79514037]
obj_prev = 23.039028040351884
eta_min = nan	eta_max = nan
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 5.251816334124527	eta = 0.9090909090909091
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 14.045450908523552	eta = 0.3399234753417846
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 9.049757744866326	eta = 0.5275697560275713
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.202066092537866	eta = 0.5820946127112314
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148578542585753	eta = 0.5859155017793716
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148332307464996	eta = 0.5859332076078638
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148332302200442	eta = 0.5859332079864292
eta = 0.5859332079864292
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [0.04410974 0.09277049 0.04340959 0.01505332 0.10712366 0.05111129
 0.01890417 0.06266384 0.04551005 0.04130912]
ene_total = [0.83982608 1.19483484 0.84607581 0.44015542 1.35993302 0.69656538
 0.48459427 0.96365584 0.74525191 0.57743974]
ti_comp = [1.98249026 2.16845803 1.97019612 2.03038779 2.17217608 2.17382626
 2.03126319 2.06316582 2.07658364 2.17668634]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [1.36477616e-06 1.06122506e-05 1.31709789e-06 5.17153298e-08
 1.62834394e-05 1.76595985e-06 1.02334201e-07 3.61295660e-06
 1.36616489e-06 9.29878567e-07]
ene_total = [0.31897471 0.09948487 0.33349161 0.26239966 0.0951614  0.09304137
 0.26136655 0.22373596 0.20786507 0.08965418]
optimize_network iter = 0 obj = 1.9851753783046104
eta = 0.5859332079864292
freqs = [11124831.81396149 21390888.52316821 11016564.88463251  3707007.23591558
 24658143.18962516 11756065.47830876  4653305.14511856 15186331.26526243
 10957914.22550441  9488993.01846851]
eta_min = 0.5859332079864303	eta_max = 0.7781376768963073
af = 0.0003421552139497174	bf = 0.8218123015887785	zeta = 0.00037637073534468917	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [2.43339611e-07 1.89216446e-06 2.34838575e-07 9.22084412e-09
 2.90333750e-06 3.14870668e-07 1.82461896e-08 6.44190219e-07
 2.43587222e-07 1.65797363e-07]
ene_total = [1.45278485 0.4526582  1.5189076  1.19515856 0.43271528 0.42370069
 1.19045077 1.01889828 0.94670988 0.40830993]
ti_comp = [0.93686199 1.12282976 0.92456785 0.98475952 1.12654781 1.12819799
 0.98563492 1.01753754 1.03095536 1.13105807]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.31568265e-07 4.09043688e-06 6.18083830e-07 2.27198204e-08
 6.25640601e-06 6.77560105e-07 4.49168218e-08 1.53503439e-06
 5.72809530e-07 3.55906665e-07]
ene_total = [0.59529372 0.18552719 0.62238771 0.48972193 0.17738094 0.17362127
 0.48779318 0.41751781 0.38792591 0.16731103]
optimize_network iter = 1 obj = 3.7044806936267416
eta = 0.7781376768963073
freqs = [11047352.84864209 19386314.72442644 11016564.8846325   3586753.80166813
 22311825.119887   10629940.50483284  4500294.94608312 14449934.82166095
 10357782.55225607  8569597.11605592]
eta_min = 0.7781376768963095	eta_max = 0.7781376768963063
af = 0.0002885807068502459	bf = 0.8218123015887785	zeta = 0.0003174387775352705	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [2.39961934e-07 1.55414576e-06 2.34838575e-07 8.63230839e-09
 2.37709741e-06 2.57436357e-07 1.70659737e-08 5.83230415e-07
 2.17636778e-07 1.35225369e-07]
ene_total = [1.45278467 0.45264002 1.5189076  1.19515853 0.43268697 0.4236976
 1.19045071 1.018895   0.94670848 0.40830829]
ti_comp = [0.93686199 1.12282976 0.92456785 0.98475952 1.12654781 1.12819799
 0.98563492 1.01753754 1.03095536 1.13105807]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.31568265e-07 4.09043688e-06 6.18083830e-07 2.27198204e-08
 6.25640601e-06 6.77560105e-07 4.49168218e-08 1.53503439e-06
 5.72809530e-07 3.55906665e-07]
ene_total = [0.59529372 0.18552719 0.62238771 0.48972193 0.17738094 0.17362127
 0.48779318 0.41751781 0.38792591 0.16731103]
optimize_network iter = 2 obj = 3.7044806936267247
eta = 0.7781376768963063
freqs = [11047352.84864208 19386314.72442644 11016564.88463249  3586753.80166813
 22311825.119887   10629940.50483284  4500294.94608312 14449934.82166094
 10357782.55225607  8569597.11605592]
Done!
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.15101785e-07 3.98378951e-06 6.01968921e-07 2.21274609e-08
 6.09328669e-06 6.59894509e-07 4.37457339e-08 1.49501241e-06
 5.57875029e-07 3.46627335e-07]
ene_total = [0.02701167 0.00841826 0.02824107 0.02222133 0.00804857 0.00787812
 0.02213381 0.018945   0.01760228 0.00759179]
At round 68 energy consumption: 0.16809188719852586
At round 68 eta: 0.7781376768963063
At round 68 a_n: 4.8894852623407985
At round 68 local rounds: 8.214162669625793
At round 68 global rounds: 22.038375844715002
gradient difference: 0.5793399214744568
train() client id: f_00000-0-0 loss: 1.189642  [   32/  126]
train() client id: f_00000-0-1 loss: 1.112073  [   64/  126]
train() client id: f_00000-0-2 loss: 1.134124  [   96/  126]
train() client id: f_00000-1-0 loss: 1.046861  [   32/  126]
train() client id: f_00000-1-1 loss: 1.195767  [   64/  126]
train() client id: f_00000-1-2 loss: 1.184404  [   96/  126]
train() client id: f_00000-2-0 loss: 1.220425  [   32/  126]
train() client id: f_00000-2-1 loss: 1.199235  [   64/  126]
train() client id: f_00000-2-2 loss: 0.853431  [   96/  126]
train() client id: f_00000-3-0 loss: 1.076611  [   32/  126]
train() client id: f_00000-3-1 loss: 0.911554  [   64/  126]
train() client id: f_00000-3-2 loss: 1.030662  [   96/  126]
train() client id: f_00000-4-0 loss: 1.014149  [   32/  126]
train() client id: f_00000-4-1 loss: 0.980233  [   64/  126]
train() client id: f_00000-4-2 loss: 0.974467  [   96/  126]
train() client id: f_00000-5-0 loss: 0.933833  [   32/  126]
train() client id: f_00000-5-1 loss: 0.985767  [   64/  126]
train() client id: f_00000-5-2 loss: 1.009275  [   96/  126]
train() client id: f_00000-6-0 loss: 0.813982  [   32/  126]
train() client id: f_00000-6-1 loss: 0.895923  [   64/  126]
train() client id: f_00000-6-2 loss: 0.909510  [   96/  126]
train() client id: f_00000-7-0 loss: 0.866082  [   32/  126]
train() client id: f_00000-7-1 loss: 0.867446  [   64/  126]
train() client id: f_00000-7-2 loss: 1.093259  [   96/  126]
train() client id: f_00001-0-0 loss: 0.332920  [   32/  265]
train() client id: f_00001-0-1 loss: 0.270231  [   64/  265]
train() client id: f_00001-0-2 loss: 0.288920  [   96/  265]
train() client id: f_00001-0-3 loss: 0.388731  [  128/  265]
train() client id: f_00001-0-4 loss: 0.406907  [  160/  265]
train() client id: f_00001-0-5 loss: 0.315800  [  192/  265]
train() client id: f_00001-0-6 loss: 0.374044  [  224/  265]
train() client id: f_00001-0-7 loss: 0.349754  [  256/  265]
train() client id: f_00001-1-0 loss: 0.462369  [   32/  265]
train() client id: f_00001-1-1 loss: 0.275674  [   64/  265]
train() client id: f_00001-1-2 loss: 0.301120  [   96/  265]
train() client id: f_00001-1-3 loss: 0.475316  [  128/  265]
train() client id: f_00001-1-4 loss: 0.250423  [  160/  265]
train() client id: f_00001-1-5 loss: 0.261723  [  192/  265]
train() client id: f_00001-1-6 loss: 0.267657  [  224/  265]
train() client id: f_00001-1-7 loss: 0.384396  [  256/  265]
train() client id: f_00001-2-0 loss: 0.337196  [   32/  265]
train() client id: f_00001-2-1 loss: 0.481210  [   64/  265]
train() client id: f_00001-2-2 loss: 0.515432  [   96/  265]
train() client id: f_00001-2-3 loss: 0.259944  [  128/  265]
train() client id: f_00001-2-4 loss: 0.231947  [  160/  265]
train() client id: f_00001-2-5 loss: 0.296183  [  192/  265]
train() client id: f_00001-2-6 loss: 0.242676  [  224/  265]
train() client id: f_00001-2-7 loss: 0.293499  [  256/  265]
train() client id: f_00001-3-0 loss: 0.264626  [   32/  265]
train() client id: f_00001-3-1 loss: 0.456059  [   64/  265]
train() client id: f_00001-3-2 loss: 0.302035  [   96/  265]
train() client id: f_00001-3-3 loss: 0.262245  [  128/  265]
train() client id: f_00001-3-4 loss: 0.275886  [  160/  265]
train() client id: f_00001-3-5 loss: 0.350858  [  192/  265]
train() client id: f_00001-3-6 loss: 0.466741  [  224/  265]
train() client id: f_00001-3-7 loss: 0.245229  [  256/  265]
train() client id: f_00001-4-0 loss: 0.330771  [   32/  265]
train() client id: f_00001-4-1 loss: 0.296364  [   64/  265]
train() client id: f_00001-4-2 loss: 0.416770  [   96/  265]
train() client id: f_00001-4-3 loss: 0.465740  [  128/  265]
train() client id: f_00001-4-4 loss: 0.299694  [  160/  265]
train() client id: f_00001-4-5 loss: 0.272089  [  192/  265]
train() client id: f_00001-4-6 loss: 0.257160  [  224/  265]
train() client id: f_00001-4-7 loss: 0.254435  [  256/  265]
train() client id: f_00001-5-0 loss: 0.433367  [   32/  265]
train() client id: f_00001-5-1 loss: 0.284661  [   64/  265]
train() client id: f_00001-5-2 loss: 0.349909  [   96/  265]
train() client id: f_00001-5-3 loss: 0.357631  [  128/  265]
train() client id: f_00001-5-4 loss: 0.221735  [  160/  265]
train() client id: f_00001-5-5 loss: 0.364362  [  192/  265]
train() client id: f_00001-5-6 loss: 0.265395  [  224/  265]
train() client id: f_00001-5-7 loss: 0.230733  [  256/  265]
train() client id: f_00001-6-0 loss: 0.317805  [   32/  265]
train() client id: f_00001-6-1 loss: 0.237914  [   64/  265]
train() client id: f_00001-6-2 loss: 0.360803  [   96/  265]
train() client id: f_00001-6-3 loss: 0.200718  [  128/  265]
train() client id: f_00001-6-4 loss: 0.332178  [  160/  265]
train() client id: f_00001-6-5 loss: 0.375872  [  192/  265]
train() client id: f_00001-6-6 loss: 0.348584  [  224/  265]
train() client id: f_00001-6-7 loss: 0.253888  [  256/  265]
train() client id: f_00001-7-0 loss: 0.322060  [   32/  265]
train() client id: f_00001-7-1 loss: 0.216694  [   64/  265]
train() client id: f_00001-7-2 loss: 0.218284  [   96/  265]
train() client id: f_00001-7-3 loss: 0.302743  [  128/  265]
train() client id: f_00001-7-4 loss: 0.356062  [  160/  265]
train() client id: f_00001-7-5 loss: 0.382239  [  192/  265]
train() client id: f_00001-7-6 loss: 0.302797  [  224/  265]
train() client id: f_00001-7-7 loss: 0.318759  [  256/  265]
train() client id: f_00002-0-0 loss: 0.953299  [   32/  124]
train() client id: f_00002-0-1 loss: 1.143590  [   64/  124]
train() client id: f_00002-0-2 loss: 0.906363  [   96/  124]
train() client id: f_00002-1-0 loss: 0.775204  [   32/  124]
train() client id: f_00002-1-1 loss: 1.092173  [   64/  124]
train() client id: f_00002-1-2 loss: 0.719302  [   96/  124]
train() client id: f_00002-2-0 loss: 1.025254  [   32/  124]
train() client id: f_00002-2-1 loss: 0.918934  [   64/  124]
train() client id: f_00002-2-2 loss: 0.711990  [   96/  124]
train() client id: f_00002-3-0 loss: 0.954361  [   32/  124]
train() client id: f_00002-3-1 loss: 0.911168  [   64/  124]
train() client id: f_00002-3-2 loss: 0.706913  [   96/  124]
train() client id: f_00002-4-0 loss: 0.917688  [   32/  124]
train() client id: f_00002-4-1 loss: 0.959792  [   64/  124]
train() client id: f_00002-4-2 loss: 0.969596  [   96/  124]
train() client id: f_00002-5-0 loss: 0.760916  [   32/  124]
train() client id: f_00002-5-1 loss: 1.091112  [   64/  124]
train() client id: f_00002-5-2 loss: 0.767697  [   96/  124]
train() client id: f_00002-6-0 loss: 1.077211  [   32/  124]
train() client id: f_00002-6-1 loss: 1.007971  [   64/  124]
train() client id: f_00002-6-2 loss: 0.677345  [   96/  124]
train() client id: f_00002-7-0 loss: 1.032489  [   32/  124]
train() client id: f_00002-7-1 loss: 0.870514  [   64/  124]
train() client id: f_00002-7-2 loss: 0.901896  [   96/  124]
train() client id: f_00003-0-0 loss: 0.511935  [   32/   43]
train() client id: f_00003-1-0 loss: 0.504327  [   32/   43]
train() client id: f_00003-2-0 loss: 0.454244  [   32/   43]
train() client id: f_00003-3-0 loss: 0.551386  [   32/   43]
train() client id: f_00003-4-0 loss: 0.666976  [   32/   43]
train() client id: f_00003-5-0 loss: 0.516665  [   32/   43]
train() client id: f_00003-6-0 loss: 0.591308  [   32/   43]
train() client id: f_00003-7-0 loss: 0.587955  [   32/   43]
train() client id: f_00004-0-0 loss: 0.748311  [   32/  306]
train() client id: f_00004-0-1 loss: 0.704300  [   64/  306]
train() client id: f_00004-0-2 loss: 0.843535  [   96/  306]
train() client id: f_00004-0-3 loss: 0.757390  [  128/  306]
train() client id: f_00004-0-4 loss: 0.935140  [  160/  306]
train() client id: f_00004-0-5 loss: 0.748254  [  192/  306]
train() client id: f_00004-0-6 loss: 0.805426  [  224/  306]
train() client id: f_00004-0-7 loss: 0.862662  [  256/  306]
train() client id: f_00004-0-8 loss: 0.719532  [  288/  306]
train() client id: f_00004-1-0 loss: 0.807952  [   32/  306]
train() client id: f_00004-1-1 loss: 0.747617  [   64/  306]
train() client id: f_00004-1-2 loss: 0.710710  [   96/  306]
train() client id: f_00004-1-3 loss: 0.829966  [  128/  306]
train() client id: f_00004-1-4 loss: 0.752799  [  160/  306]
train() client id: f_00004-1-5 loss: 0.906761  [  192/  306]
train() client id: f_00004-1-6 loss: 0.673072  [  224/  306]
train() client id: f_00004-1-7 loss: 0.880494  [  256/  306]
train() client id: f_00004-1-8 loss: 0.829257  [  288/  306]
train() client id: f_00004-2-0 loss: 0.698695  [   32/  306]
train() client id: f_00004-2-1 loss: 0.704782  [   64/  306]
train() client id: f_00004-2-2 loss: 0.701021  [   96/  306]
train() client id: f_00004-2-3 loss: 0.753914  [  128/  306]
train() client id: f_00004-2-4 loss: 0.881837  [  160/  306]
train() client id: f_00004-2-5 loss: 0.841925  [  192/  306]
train() client id: f_00004-2-6 loss: 0.842349  [  224/  306]
train() client id: f_00004-2-7 loss: 0.927817  [  256/  306]
train() client id: f_00004-2-8 loss: 0.968681  [  288/  306]
train() client id: f_00004-3-0 loss: 0.857129  [   32/  306]
train() client id: f_00004-3-1 loss: 0.879332  [   64/  306]
train() client id: f_00004-3-2 loss: 0.754745  [   96/  306]
train() client id: f_00004-3-3 loss: 0.679160  [  128/  306]
train() client id: f_00004-3-4 loss: 0.902014  [  160/  306]
train() client id: f_00004-3-5 loss: 0.897491  [  192/  306]
train() client id: f_00004-3-6 loss: 0.812186  [  224/  306]
train() client id: f_00004-3-7 loss: 0.804216  [  256/  306]
train() client id: f_00004-3-8 loss: 0.734281  [  288/  306]
train() client id: f_00004-4-0 loss: 0.755118  [   32/  306]
train() client id: f_00004-4-1 loss: 0.841945  [   64/  306]
train() client id: f_00004-4-2 loss: 0.945927  [   96/  306]
train() client id: f_00004-4-3 loss: 0.839712  [  128/  306]
train() client id: f_00004-4-4 loss: 0.821980  [  160/  306]
train() client id: f_00004-4-5 loss: 0.857843  [  192/  306]
train() client id: f_00004-4-6 loss: 0.951165  [  224/  306]
train() client id: f_00004-4-7 loss: 0.724706  [  256/  306]
train() client id: f_00004-4-8 loss: 0.619192  [  288/  306]
train() client id: f_00004-5-0 loss: 0.898649  [   32/  306]
train() client id: f_00004-5-1 loss: 0.751946  [   64/  306]
train() client id: f_00004-5-2 loss: 0.686217  [   96/  306]
train() client id: f_00004-5-3 loss: 0.811418  [  128/  306]
train() client id: f_00004-5-4 loss: 0.889993  [  160/  306]
train() client id: f_00004-5-5 loss: 0.784018  [  192/  306]
train() client id: f_00004-5-6 loss: 0.810406  [  224/  306]
train() client id: f_00004-5-7 loss: 0.831897  [  256/  306]
train() client id: f_00004-5-8 loss: 0.727917  [  288/  306]
train() client id: f_00004-6-0 loss: 0.819318  [   32/  306]
train() client id: f_00004-6-1 loss: 0.838975  [   64/  306]
train() client id: f_00004-6-2 loss: 0.816613  [   96/  306]
train() client id: f_00004-6-3 loss: 0.598111  [  128/  306]
train() client id: f_00004-6-4 loss: 0.878178  [  160/  306]
train() client id: f_00004-6-5 loss: 0.846431  [  192/  306]
train() client id: f_00004-6-6 loss: 0.714440  [  224/  306]
train() client id: f_00004-6-7 loss: 0.794216  [  256/  306]
train() client id: f_00004-6-8 loss: 0.807918  [  288/  306]
train() client id: f_00004-7-0 loss: 0.753366  [   32/  306]
train() client id: f_00004-7-1 loss: 0.833556  [   64/  306]
train() client id: f_00004-7-2 loss: 0.833453  [   96/  306]
train() client id: f_00004-7-3 loss: 0.774881  [  128/  306]
train() client id: f_00004-7-4 loss: 0.750381  [  160/  306]
train() client id: f_00004-7-5 loss: 0.746291  [  192/  306]
train() client id: f_00004-7-6 loss: 0.873513  [  224/  306]
train() client id: f_00004-7-7 loss: 0.752051  [  256/  306]
train() client id: f_00004-7-8 loss: 0.803745  [  288/  306]
train() client id: f_00005-0-0 loss: 0.466491  [   32/  146]
train() client id: f_00005-0-1 loss: 0.746811  [   64/  146]
train() client id: f_00005-0-2 loss: 0.888808  [   96/  146]
train() client id: f_00005-0-3 loss: 0.720883  [  128/  146]
train() client id: f_00005-1-0 loss: 0.629016  [   32/  146]
train() client id: f_00005-1-1 loss: 0.516732  [   64/  146]
train() client id: f_00005-1-2 loss: 0.821432  [   96/  146]
train() client id: f_00005-1-3 loss: 0.734777  [  128/  146]
train() client id: f_00005-2-0 loss: 0.676274  [   32/  146]
train() client id: f_00005-2-1 loss: 0.675342  [   64/  146]
train() client id: f_00005-2-2 loss: 0.512069  [   96/  146]
train() client id: f_00005-2-3 loss: 0.821782  [  128/  146]
train() client id: f_00005-3-0 loss: 0.499663  [   32/  146]
train() client id: f_00005-3-1 loss: 1.004686  [   64/  146]
train() client id: f_00005-3-2 loss: 0.394513  [   96/  146]
train() client id: f_00005-3-3 loss: 0.616556  [  128/  146]
train() client id: f_00005-4-0 loss: 0.777298  [   32/  146]
train() client id: f_00005-4-1 loss: 0.811630  [   64/  146]
train() client id: f_00005-4-2 loss: 0.287883  [   96/  146]
train() client id: f_00005-4-3 loss: 0.481997  [  128/  146]
train() client id: f_00005-5-0 loss: 0.800934  [   32/  146]
train() client id: f_00005-5-1 loss: 0.707972  [   64/  146]
train() client id: f_00005-5-2 loss: 0.692208  [   96/  146]
train() client id: f_00005-5-3 loss: 0.469614  [  128/  146]
train() client id: f_00005-6-0 loss: 0.601610  [   32/  146]
train() client id: f_00005-6-1 loss: 0.599929  [   64/  146]
train() client id: f_00005-6-2 loss: 0.951775  [   96/  146]
train() client id: f_00005-6-3 loss: 0.609013  [  128/  146]
train() client id: f_00005-7-0 loss: 0.623922  [   32/  146]
train() client id: f_00005-7-1 loss: 0.660598  [   64/  146]
train() client id: f_00005-7-2 loss: 0.582563  [   96/  146]
train() client id: f_00005-7-3 loss: 0.714717  [  128/  146]
train() client id: f_00006-0-0 loss: 0.593025  [   32/   54]
train() client id: f_00006-1-0 loss: 0.595333  [   32/   54]
train() client id: f_00006-2-0 loss: 0.510135  [   32/   54]
train() client id: f_00006-3-0 loss: 0.591203  [   32/   54]
train() client id: f_00006-4-0 loss: 0.488513  [   32/   54]
train() client id: f_00006-5-0 loss: 0.527668  [   32/   54]
train() client id: f_00006-6-0 loss: 0.581605  [   32/   54]
train() client id: f_00006-7-0 loss: 0.517004  [   32/   54]
train() client id: f_00007-0-0 loss: 0.657950  [   32/  179]
train() client id: f_00007-0-1 loss: 0.880557  [   64/  179]
train() client id: f_00007-0-2 loss: 0.406122  [   96/  179]
train() client id: f_00007-0-3 loss: 0.459944  [  128/  179]
train() client id: f_00007-0-4 loss: 0.576948  [  160/  179]
train() client id: f_00007-1-0 loss: 0.632401  [   32/  179]
train() client id: f_00007-1-1 loss: 0.445495  [   64/  179]
train() client id: f_00007-1-2 loss: 0.610300  [   96/  179]
train() client id: f_00007-1-3 loss: 0.589478  [  128/  179]
train() client id: f_00007-1-4 loss: 0.655431  [  160/  179]
train() client id: f_00007-2-0 loss: 0.581019  [   32/  179]
train() client id: f_00007-2-1 loss: 0.422408  [   64/  179]
train() client id: f_00007-2-2 loss: 0.521344  [   96/  179]
train() client id: f_00007-2-3 loss: 0.537576  [  128/  179]
train() client id: f_00007-2-4 loss: 0.744771  [  160/  179]
train() client id: f_00007-3-0 loss: 0.337055  [   32/  179]
train() client id: f_00007-3-1 loss: 0.607494  [   64/  179]
train() client id: f_00007-3-2 loss: 0.519797  [   96/  179]
train() client id: f_00007-3-3 loss: 0.639472  [  128/  179]
train() client id: f_00007-3-4 loss: 0.491898  [  160/  179]
train() client id: f_00007-4-0 loss: 0.514315  [   32/  179]
train() client id: f_00007-4-1 loss: 0.402567  [   64/  179]
train() client id: f_00007-4-2 loss: 0.455425  [   96/  179]
train() client id: f_00007-4-3 loss: 0.911209  [  128/  179]
train() client id: f_00007-4-4 loss: 0.550977  [  160/  179]
train() client id: f_00007-5-0 loss: 0.539116  [   32/  179]
train() client id: f_00007-5-1 loss: 0.613067  [   64/  179]
train() client id: f_00007-5-2 loss: 0.678075  [   96/  179]
train() client id: f_00007-5-3 loss: 0.577399  [  128/  179]
train() client id: f_00007-5-4 loss: 0.413736  [  160/  179]
train() client id: f_00007-6-0 loss: 0.442572  [   32/  179]
train() client id: f_00007-6-1 loss: 0.650795  [   64/  179]
train() client id: f_00007-6-2 loss: 0.367556  [   96/  179]
train() client id: f_00007-6-3 loss: 0.542708  [  128/  179]
train() client id: f_00007-6-4 loss: 0.744773  [  160/  179]
train() client id: f_00007-7-0 loss: 0.520132  [   32/  179]
train() client id: f_00007-7-1 loss: 0.497798  [   64/  179]
train() client id: f_00007-7-2 loss: 0.684328  [   96/  179]
train() client id: f_00007-7-3 loss: 0.552476  [  128/  179]
train() client id: f_00007-7-4 loss: 0.493377  [  160/  179]
train() client id: f_00008-0-0 loss: 0.729303  [   32/  130]
train() client id: f_00008-0-1 loss: 0.811147  [   64/  130]
train() client id: f_00008-0-2 loss: 0.686913  [   96/  130]
train() client id: f_00008-0-3 loss: 0.622809  [  128/  130]
train() client id: f_00008-1-0 loss: 0.736774  [   32/  130]
train() client id: f_00008-1-1 loss: 0.751684  [   64/  130]
train() client id: f_00008-1-2 loss: 0.544045  [   96/  130]
train() client id: f_00008-1-3 loss: 0.817067  [  128/  130]
train() client id: f_00008-2-0 loss: 0.701346  [   32/  130]
train() client id: f_00008-2-1 loss: 0.620269  [   64/  130]
train() client id: f_00008-2-2 loss: 0.798033  [   96/  130]
train() client id: f_00008-2-3 loss: 0.735255  [  128/  130]
train() client id: f_00008-3-0 loss: 0.765058  [   32/  130]
train() client id: f_00008-3-1 loss: 0.721466  [   64/  130]
train() client id: f_00008-3-2 loss: 0.617213  [   96/  130]
train() client id: f_00008-3-3 loss: 0.720050  [  128/  130]
train() client id: f_00008-4-0 loss: 0.713607  [   32/  130]
train() client id: f_00008-4-1 loss: 0.723872  [   64/  130]
train() client id: f_00008-4-2 loss: 0.624200  [   96/  130]
train() client id: f_00008-4-3 loss: 0.793543  [  128/  130]
train() client id: f_00008-5-0 loss: 0.724948  [   32/  130]
train() client id: f_00008-5-1 loss: 0.696160  [   64/  130]
train() client id: f_00008-5-2 loss: 0.746285  [   96/  130]
train() client id: f_00008-5-3 loss: 0.683697  [  128/  130]
train() client id: f_00008-6-0 loss: 0.902869  [   32/  130]
train() client id: f_00008-6-1 loss: 0.675896  [   64/  130]
train() client id: f_00008-6-2 loss: 0.587048  [   96/  130]
train() client id: f_00008-6-3 loss: 0.697024  [  128/  130]
train() client id: f_00008-7-0 loss: 0.690117  [   32/  130]
train() client id: f_00008-7-1 loss: 0.776506  [   64/  130]
train() client id: f_00008-7-2 loss: 0.662743  [   96/  130]
train() client id: f_00008-7-3 loss: 0.741265  [  128/  130]
train() client id: f_00009-0-0 loss: 1.040335  [   32/  118]
train() client id: f_00009-0-1 loss: 0.841165  [   64/  118]
train() client id: f_00009-0-2 loss: 1.041126  [   96/  118]
train() client id: f_00009-1-0 loss: 0.992034  [   32/  118]
train() client id: f_00009-1-1 loss: 0.853684  [   64/  118]
train() client id: f_00009-1-2 loss: 0.928351  [   96/  118]
train() client id: f_00009-2-0 loss: 1.077493  [   32/  118]
train() client id: f_00009-2-1 loss: 0.824125  [   64/  118]
train() client id: f_00009-2-2 loss: 0.809107  [   96/  118]
train() client id: f_00009-3-0 loss: 1.151854  [   32/  118]
train() client id: f_00009-3-1 loss: 0.778270  [   64/  118]
train() client id: f_00009-3-2 loss: 0.770843  [   96/  118]
train() client id: f_00009-4-0 loss: 0.833311  [   32/  118]
train() client id: f_00009-4-1 loss: 0.775170  [   64/  118]
train() client id: f_00009-4-2 loss: 0.927898  [   96/  118]
train() client id: f_00009-5-0 loss: 0.854763  [   32/  118]
train() client id: f_00009-5-1 loss: 0.807790  [   64/  118]
train() client id: f_00009-5-2 loss: 0.913943  [   96/  118]
train() client id: f_00009-6-0 loss: 0.805333  [   32/  118]
train() client id: f_00009-6-1 loss: 0.712500  [   64/  118]
train() client id: f_00009-6-2 loss: 0.939084  [   96/  118]
train() client id: f_00009-7-0 loss: 0.878676  [   32/  118]
train() client id: f_00009-7-1 loss: 0.843888  [   64/  118]
train() client id: f_00009-7-2 loss: 0.697184  [   96/  118]
At round 68 accuracy: 0.649867374005305
At round 68 training accuracy: 0.5902079141515761
At round 68 training loss: 0.8237356016017194
update_location
xs = -3.905658 4.200318 360.009024 18.811294 0.979296 3.956410 -322.443192 -301.324852 344.663977 -287.060879 
ys = 352.587959 335.555839 1.320614 -322.455176 314.350187 297.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -18.21142616811743
ys mean: 99.89414253552872
dists_uav = 366.515379 350.164766 373.641862 338.128977 329.874217 314.179760 337.604062 317.485972 359.307566 304.006513 
uav_gains = -120.011752 -119.181260 -120.338787 -118.486677 -117.963816 -116.857007 -118.454597 -117.102679 -119.660331 -116.060228 
uav_gains_db_mean: -118.41171348752778
dists_bs = 246.598372 240.605343 562.493720 533.876830 224.470905 217.045415 230.846551 215.164641 542.987262 204.445919 
bs_gains = -106.542785 -106.243607 -116.570354 -115.935409 -105.399541 -104.990476 -105.740114 -104.884644 -116.141169 -104.263256 
bs_gains_db_mean: -108.67113552551646
Round 69
-------------------------------
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.8731924  3.7097278  1.85116443 0.70130907 4.2759113  2.0588358
 0.85327763 2.56982483 1.88245466 1.66965903]
obj_prev = 21.445356959883735
eta_min = 1.2228941532952692e-50	eta_max = 0.9500254409851863
af = 4.439896748873053	bf = 0.784423253509249	zeta = 4.883886423760359	eta = 0.9090909090909091
af = 4.439896748873053	bf = 0.784423253509249	zeta = 13.283496720526747	eta = 0.3342415662294824
af = 4.439896748873053	bf = 0.784423253509249	zeta = 8.486662978303045	eta = 0.5231616667498248
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.677394178566005	eta = 0.5783077754778437
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.626245838238497	eta = 0.5821864181995182
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.626008957353865	eta = 0.5822045022110287
eta = 0.5822045022110287
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [0.04463654 0.09387843 0.04392802 0.0152331  0.10840302 0.0517217
 0.01912994 0.06341222 0.04605357 0.04180247]
ene_total = [0.78877956 1.11412037 0.79451743 0.41620728 1.26807169 0.64932725
 0.45766691 0.90423052 0.69485736 0.53823058]
ti_comp = [2.16111529 2.35459485 2.14875789 2.20941898 2.35838257 2.36010268
 2.21029221 2.24279005 2.26158603 2.36299155]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [1.19013336e-06 9.32706721e-06 1.14743992e-06 4.52573450e-08
 1.43144819e-05 1.55251797e-06 8.95615827e-08 3.16826659e-06
 1.19355847e-06 8.17638912e-07]
ene_total = [0.30300549 0.09252695 0.3164538  0.25042331 0.088459   0.08644808
 0.24947344 0.214139   0.19366152 0.08329608]
optimize_network iter = 0 obj = 1.8778866839980972
eta = 0.5822045022110287
freqs = [10327199.39136077 19935155.93888518 10221724.11669465  3447309.92299876
 22982492.36545908 10957510.88478662  4327469.55320673 14136905.79686903
 10181697.58740329  8845243.50076053]
eta_min = 0.5822045022110292	eta_max = 0.7888487440198617
af = 0.000276093861788328	bf = 0.784423253509249	zeta = 0.00030370324796716086	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [2.09696425e-07 1.64338949e-06 2.02174023e-07 7.97415130e-09
 2.52215070e-06 2.73547049e-07 1.57803692e-08 5.58235069e-07
 2.10299915e-07 1.44064491e-07]
ene_total = [1.3924886  0.42484648 1.45429544 1.1508812  0.40594569 0.39722982
 1.14651401 0.98399884 0.88997085 0.38277432]
ti_comp = [0.9545136  1.14799316 0.9421562  1.00281729 1.15178087 1.15350099
 1.00369052 1.03618836 1.05498434 1.15638985]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [5.14272772e-07 3.30754040e-06 5.03112813e-07 1.85185886e-08
 5.05907005e-06 5.47858951e-07 3.66123645e-08 1.25120311e-06
 4.62364171e-07 2.87795246e-07]
ene_total = [0.59952879 0.1829493  0.62613904 0.49550078 0.17483054 0.17102929
 0.49362075 0.42366592 0.3831736  0.1648028 ]
optimize_network iter = 1 obj = 3.715240812383447
eta = 0.7888487440198617
freqs = [10252122.85547994 17928004.42999087 10221724.11669466  3330213.08308708
 20633691.92866442  9830152.63900685  4178489.52262742 13416514.38117568
  9570237.57402436  7925070.10389261]
eta_min = 0.7888487440198628	eta_max = 0.7888487440198605
af = 0.00022982602808494754	bf = 0.784423253509249	zeta = 0.0002528086308934423	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [2.06658611e-07 1.32912288e-06 2.02174023e-07 7.44162632e-09
 2.03296858e-06 2.20155092e-07 1.47125432e-08 5.02791340e-07
 1.85799332e-07 1.15649455e-07]
ene_total = [1.39248845 0.42483077 1.45429544 1.15088117 0.40592122 0.39722715
 1.14651395 0.98399607 0.88996963 0.38277289]
ti_comp = [0.9545136  1.14799316 0.9421562  1.00281729 1.15178087 1.15350099
 1.00369052 1.03618836 1.05498434 1.15638985]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [5.14272772e-07 3.30754040e-06 5.03112813e-07 1.85185886e-08
 5.05907005e-06 5.47858951e-07 3.66123645e-08 1.25120311e-06
 4.62364171e-07 2.87795246e-07]
ene_total = [0.59952879 0.1829493  0.62613904 0.49550078 0.17483054 0.17102929
 0.49362075 0.42366592 0.3831736  0.1648028 ]
optimize_network iter = 2 obj = 3.715240812383425
eta = 0.7888487440198605
freqs = [10252122.85547993 17928004.42999088 10221724.11669465  3330213.08308708
 20633691.92866442  9830152.63900685  4178489.52262742 13416514.38117567
  9570237.57402436  7925070.10389262]
Done!
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [4.63517562e-07 2.98110875e-06 4.53459015e-07 1.66909304e-08
 4.55977439e-06 4.93789014e-07 3.29989742e-08 1.12771791e-06
 4.16731985e-07 2.59391820e-07]
ene_total = [0.02784093 0.00849549 0.02907666 0.02301012 0.0081183  0.00794222
 0.02292281 0.01967412 0.01779381 0.0076531 ]
At round 69 energy consumption: 0.17252756466419225
At round 69 eta: 0.7888487440198605
At round 69 a_n: 4.546939415371483
At round 69 local rounds: 7.766500565656417
At round 69 global rounds: 21.53403916195109
gradient difference: 0.593393087387085
train() client id: f_00000-0-0 loss: 0.871028  [   32/  126]
train() client id: f_00000-0-1 loss: 1.099770  [   64/  126]
train() client id: f_00000-0-2 loss: 0.868329  [   96/  126]
train() client id: f_00000-1-0 loss: 0.713367  [   32/  126]
train() client id: f_00000-1-1 loss: 1.051226  [   64/  126]
train() client id: f_00000-1-2 loss: 0.958635  [   96/  126]
train() client id: f_00000-2-0 loss: 0.819325  [   32/  126]
train() client id: f_00000-2-1 loss: 0.666199  [   64/  126]
train() client id: f_00000-2-2 loss: 0.936873  [   96/  126]
train() client id: f_00000-3-0 loss: 0.617224  [   32/  126]
train() client id: f_00000-3-1 loss: 0.888002  [   64/  126]
train() client id: f_00000-3-2 loss: 0.775090  [   96/  126]
train() client id: f_00000-4-0 loss: 0.800668  [   32/  126]
train() client id: f_00000-4-1 loss: 0.783370  [   64/  126]
train() client id: f_00000-4-2 loss: 0.717607  [   96/  126]
train() client id: f_00000-5-0 loss: 0.856317  [   32/  126]
train() client id: f_00000-5-1 loss: 0.720730  [   64/  126]
train() client id: f_00000-5-2 loss: 0.649766  [   96/  126]
train() client id: f_00000-6-0 loss: 0.746780  [   32/  126]
train() client id: f_00000-6-1 loss: 0.845822  [   64/  126]
train() client id: f_00000-6-2 loss: 0.639188  [   96/  126]
train() client id: f_00001-0-0 loss: 0.352163  [   32/  265]
train() client id: f_00001-0-1 loss: 0.537970  [   64/  265]
train() client id: f_00001-0-2 loss: 0.476091  [   96/  265]
train() client id: f_00001-0-3 loss: 0.495058  [  128/  265]
train() client id: f_00001-0-4 loss: 0.425200  [  160/  265]
train() client id: f_00001-0-5 loss: 0.428374  [  192/  265]
train() client id: f_00001-0-6 loss: 0.386421  [  224/  265]
train() client id: f_00001-0-7 loss: 0.444347  [  256/  265]
train() client id: f_00001-1-0 loss: 0.486912  [   32/  265]
train() client id: f_00001-1-1 loss: 0.414084  [   64/  265]
train() client id: f_00001-1-2 loss: 0.351998  [   96/  265]
train() client id: f_00001-1-3 loss: 0.415967  [  128/  265]
train() client id: f_00001-1-4 loss: 0.350696  [  160/  265]
train() client id: f_00001-1-5 loss: 0.443493  [  192/  265]
train() client id: f_00001-1-6 loss: 0.560285  [  224/  265]
train() client id: f_00001-1-7 loss: 0.479527  [  256/  265]
train() client id: f_00001-2-0 loss: 0.358891  [   32/  265]
train() client id: f_00001-2-1 loss: 0.532591  [   64/  265]
train() client id: f_00001-2-2 loss: 0.492915  [   96/  265]
train() client id: f_00001-2-3 loss: 0.353531  [  128/  265]
train() client id: f_00001-2-4 loss: 0.362708  [  160/  265]
train() client id: f_00001-2-5 loss: 0.533648  [  192/  265]
train() client id: f_00001-2-6 loss: 0.550451  [  224/  265]
train() client id: f_00001-2-7 loss: 0.334260  [  256/  265]
train() client id: f_00001-3-0 loss: 0.395902  [   32/  265]
train() client id: f_00001-3-1 loss: 0.345718  [   64/  265]
train() client id: f_00001-3-2 loss: 0.474190  [   96/  265]
train() client id: f_00001-3-3 loss: 0.540521  [  128/  265]
train() client id: f_00001-3-4 loss: 0.533302  [  160/  265]
train() client id: f_00001-3-5 loss: 0.453047  [  192/  265]
train() client id: f_00001-3-6 loss: 0.320984  [  224/  265]
train() client id: f_00001-3-7 loss: 0.374486  [  256/  265]
train() client id: f_00001-4-0 loss: 0.504899  [   32/  265]
train() client id: f_00001-4-1 loss: 0.455266  [   64/  265]
train() client id: f_00001-4-2 loss: 0.521854  [   96/  265]
train() client id: f_00001-4-3 loss: 0.389090  [  128/  265]
train() client id: f_00001-4-4 loss: 0.391882  [  160/  265]
train() client id: f_00001-4-5 loss: 0.461883  [  192/  265]
train() client id: f_00001-4-6 loss: 0.362561  [  224/  265]
train() client id: f_00001-4-7 loss: 0.382996  [  256/  265]
train() client id: f_00001-5-0 loss: 0.554096  [   32/  265]
train() client id: f_00001-5-1 loss: 0.322544  [   64/  265]
train() client id: f_00001-5-2 loss: 0.347260  [   96/  265]
train() client id: f_00001-5-3 loss: 0.350401  [  128/  265]
train() client id: f_00001-5-4 loss: 0.649067  [  160/  265]
train() client id: f_00001-5-5 loss: 0.453260  [  192/  265]
train() client id: f_00001-5-6 loss: 0.399481  [  224/  265]
train() client id: f_00001-5-7 loss: 0.386567  [  256/  265]
train() client id: f_00001-6-0 loss: 0.332476  [   32/  265]
train() client id: f_00001-6-1 loss: 0.413057  [   64/  265]
train() client id: f_00001-6-2 loss: 0.490937  [   96/  265]
train() client id: f_00001-6-3 loss: 0.442719  [  128/  265]
train() client id: f_00001-6-4 loss: 0.579758  [  160/  265]
train() client id: f_00001-6-5 loss: 0.470278  [  192/  265]
train() client id: f_00001-6-6 loss: 0.341450  [  224/  265]
train() client id: f_00001-6-7 loss: 0.403593  [  256/  265]
train() client id: f_00002-0-0 loss: 0.913206  [   32/  124]
train() client id: f_00002-0-1 loss: 0.969818  [   64/  124]
train() client id: f_00002-0-2 loss: 0.922976  [   96/  124]
train() client id: f_00002-1-0 loss: 0.969221  [   32/  124]
train() client id: f_00002-1-1 loss: 1.098598  [   64/  124]
train() client id: f_00002-1-2 loss: 0.805528  [   96/  124]
train() client id: f_00002-2-0 loss: 1.303463  [   32/  124]
train() client id: f_00002-2-1 loss: 0.740921  [   64/  124]
train() client id: f_00002-2-2 loss: 0.772555  [   96/  124]
train() client id: f_00002-3-0 loss: 1.027238  [   32/  124]
train() client id: f_00002-3-1 loss: 0.783951  [   64/  124]
train() client id: f_00002-3-2 loss: 0.988505  [   96/  124]
train() client id: f_00002-4-0 loss: 1.133377  [   32/  124]
train() client id: f_00002-4-1 loss: 0.800863  [   64/  124]
train() client id: f_00002-4-2 loss: 0.884404  [   96/  124]
train() client id: f_00002-5-0 loss: 1.022407  [   32/  124]
train() client id: f_00002-5-1 loss: 0.745243  [   64/  124]
train() client id: f_00002-5-2 loss: 1.014293  [   96/  124]
train() client id: f_00002-6-0 loss: 1.117091  [   32/  124]
train() client id: f_00002-6-1 loss: 0.941823  [   64/  124]
train() client id: f_00002-6-2 loss: 0.835310  [   96/  124]
train() client id: f_00003-0-0 loss: 0.609302  [   32/   43]
train() client id: f_00003-1-0 loss: 0.679772  [   32/   43]
train() client id: f_00003-2-0 loss: 0.749072  [   32/   43]
train() client id: f_00003-3-0 loss: 0.491338  [   32/   43]
train() client id: f_00003-4-0 loss: 0.485547  [   32/   43]
train() client id: f_00003-5-0 loss: 0.548821  [   32/   43]
train() client id: f_00003-6-0 loss: 0.742810  [   32/   43]
train() client id: f_00004-0-0 loss: 0.940585  [   32/  306]
train() client id: f_00004-0-1 loss: 0.831753  [   64/  306]
train() client id: f_00004-0-2 loss: 0.873874  [   96/  306]
train() client id: f_00004-0-3 loss: 0.835455  [  128/  306]
train() client id: f_00004-0-4 loss: 0.752805  [  160/  306]
train() client id: f_00004-0-5 loss: 0.940999  [  192/  306]
train() client id: f_00004-0-6 loss: 0.846995  [  224/  306]
train() client id: f_00004-0-7 loss: 0.770023  [  256/  306]
train() client id: f_00004-0-8 loss: 0.836361  [  288/  306]
train() client id: f_00004-1-0 loss: 0.888428  [   32/  306]
train() client id: f_00004-1-1 loss: 0.793500  [   64/  306]
train() client id: f_00004-1-2 loss: 0.896791  [   96/  306]
train() client id: f_00004-1-3 loss: 0.819632  [  128/  306]
train() client id: f_00004-1-4 loss: 0.857665  [  160/  306]
train() client id: f_00004-1-5 loss: 0.761518  [  192/  306]
train() client id: f_00004-1-6 loss: 0.938527  [  224/  306]
train() client id: f_00004-1-7 loss: 0.760354  [  256/  306]
train() client id: f_00004-1-8 loss: 0.799214  [  288/  306]
train() client id: f_00004-2-0 loss: 0.933264  [   32/  306]
train() client id: f_00004-2-1 loss: 0.890038  [   64/  306]
train() client id: f_00004-2-2 loss: 0.782874  [   96/  306]
train() client id: f_00004-2-3 loss: 0.815675  [  128/  306]
train() client id: f_00004-2-4 loss: 0.758481  [  160/  306]
train() client id: f_00004-2-5 loss: 0.838879  [  192/  306]
train() client id: f_00004-2-6 loss: 0.854459  [  224/  306]
train() client id: f_00004-2-7 loss: 0.878083  [  256/  306]
train() client id: f_00004-2-8 loss: 0.776396  [  288/  306]
train() client id: f_00004-3-0 loss: 0.754294  [   32/  306]
train() client id: f_00004-3-1 loss: 0.811402  [   64/  306]
train() client id: f_00004-3-2 loss: 0.833736  [   96/  306]
train() client id: f_00004-3-3 loss: 0.931792  [  128/  306]
train() client id: f_00004-3-4 loss: 0.877440  [  160/  306]
train() client id: f_00004-3-5 loss: 0.780700  [  192/  306]
train() client id: f_00004-3-6 loss: 0.824478  [  224/  306]
train() client id: f_00004-3-7 loss: 0.945358  [  256/  306]
train() client id: f_00004-3-8 loss: 0.727924  [  288/  306]
train() client id: f_00004-4-0 loss: 0.848559  [   32/  306]
train() client id: f_00004-4-1 loss: 0.884598  [   64/  306]
train() client id: f_00004-4-2 loss: 0.807552  [   96/  306]
train() client id: f_00004-4-3 loss: 0.901141  [  128/  306]
train() client id: f_00004-4-4 loss: 0.788814  [  160/  306]
train() client id: f_00004-4-5 loss: 0.809700  [  192/  306]
train() client id: f_00004-4-6 loss: 0.780255  [  224/  306]
train() client id: f_00004-4-7 loss: 0.723416  [  256/  306]
train() client id: f_00004-4-8 loss: 0.958211  [  288/  306]
train() client id: f_00004-5-0 loss: 0.869091  [   32/  306]
train() client id: f_00004-5-1 loss: 0.881388  [   64/  306]
train() client id: f_00004-5-2 loss: 0.809690  [   96/  306]
train() client id: f_00004-5-3 loss: 0.912403  [  128/  306]
train() client id: f_00004-5-4 loss: 0.722071  [  160/  306]
train() client id: f_00004-5-5 loss: 0.828912  [  192/  306]
train() client id: f_00004-5-6 loss: 0.813744  [  224/  306]
train() client id: f_00004-5-7 loss: 0.890247  [  256/  306]
train() client id: f_00004-5-8 loss: 0.774669  [  288/  306]
train() client id: f_00004-6-0 loss: 0.775552  [   32/  306]
train() client id: f_00004-6-1 loss: 0.896470  [   64/  306]
train() client id: f_00004-6-2 loss: 0.755672  [   96/  306]
train() client id: f_00004-6-3 loss: 0.793929  [  128/  306]
train() client id: f_00004-6-4 loss: 0.798114  [  160/  306]
train() client id: f_00004-6-5 loss: 0.944316  [  192/  306]
train() client id: f_00004-6-6 loss: 0.829095  [  224/  306]
train() client id: f_00004-6-7 loss: 0.837416  [  256/  306]
train() client id: f_00004-6-8 loss: 0.924360  [  288/  306]
train() client id: f_00005-0-0 loss: 0.743051  [   32/  146]
train() client id: f_00005-0-1 loss: 0.573029  [   64/  146]
train() client id: f_00005-0-2 loss: 0.279484  [   96/  146]
train() client id: f_00005-0-3 loss: 0.654502  [  128/  146]
train() client id: f_00005-1-0 loss: 0.442603  [   32/  146]
train() client id: f_00005-1-1 loss: 0.538761  [   64/  146]
train() client id: f_00005-1-2 loss: 0.447634  [   96/  146]
train() client id: f_00005-1-3 loss: 0.680468  [  128/  146]
train() client id: f_00005-2-0 loss: 0.760769  [   32/  146]
train() client id: f_00005-2-1 loss: 0.501999  [   64/  146]
train() client id: f_00005-2-2 loss: 0.576558  [   96/  146]
train() client id: f_00005-2-3 loss: 0.416174  [  128/  146]
train() client id: f_00005-3-0 loss: 0.558640  [   32/  146]
train() client id: f_00005-3-1 loss: 0.891755  [   64/  146]
train() client id: f_00005-3-2 loss: 0.437671  [   96/  146]
train() client id: f_00005-3-3 loss: 0.386708  [  128/  146]
train() client id: f_00005-4-0 loss: 0.626647  [   32/  146]
train() client id: f_00005-4-1 loss: 0.540529  [   64/  146]
train() client id: f_00005-4-2 loss: 0.462268  [   96/  146]
train() client id: f_00005-4-3 loss: 0.598787  [  128/  146]
train() client id: f_00005-5-0 loss: 0.413918  [   32/  146]
train() client id: f_00005-5-1 loss: 0.279910  [   64/  146]
train() client id: f_00005-5-2 loss: 0.621634  [   96/  146]
train() client id: f_00005-5-3 loss: 0.700998  [  128/  146]
train() client id: f_00005-6-0 loss: 0.234821  [   32/  146]
train() client id: f_00005-6-1 loss: 0.686337  [   64/  146]
train() client id: f_00005-6-2 loss: 0.652932  [   96/  146]
train() client id: f_00005-6-3 loss: 0.438156  [  128/  146]
train() client id: f_00006-0-0 loss: 0.499274  [   32/   54]
train() client id: f_00006-1-0 loss: 0.560634  [   32/   54]
train() client id: f_00006-2-0 loss: 0.539221  [   32/   54]
train() client id: f_00006-3-0 loss: 0.513522  [   32/   54]
train() client id: f_00006-4-0 loss: 0.532834  [   32/   54]
train() client id: f_00006-5-0 loss: 0.506897  [   32/   54]
train() client id: f_00006-6-0 loss: 0.500246  [   32/   54]
train() client id: f_00007-0-0 loss: 0.712801  [   32/  179]
train() client id: f_00007-0-1 loss: 0.621715  [   64/  179]
train() client id: f_00007-0-2 loss: 0.518161  [   96/  179]
train() client id: f_00007-0-3 loss: 0.587259  [  128/  179]
train() client id: f_00007-0-4 loss: 0.930010  [  160/  179]
train() client id: f_00007-1-0 loss: 0.550351  [   32/  179]
train() client id: f_00007-1-1 loss: 0.712702  [   64/  179]
train() client id: f_00007-1-2 loss: 0.790122  [   96/  179]
train() client id: f_00007-1-3 loss: 0.598227  [  128/  179]
train() client id: f_00007-1-4 loss: 0.583023  [  160/  179]
train() client id: f_00007-2-0 loss: 0.490184  [   32/  179]
train() client id: f_00007-2-1 loss: 0.773310  [   64/  179]
train() client id: f_00007-2-2 loss: 0.523606  [   96/  179]
train() client id: f_00007-2-3 loss: 0.755999  [  128/  179]
train() client id: f_00007-2-4 loss: 0.739448  [  160/  179]
train() client id: f_00007-3-0 loss: 0.525418  [   32/  179]
train() client id: f_00007-3-1 loss: 0.681050  [   64/  179]
train() client id: f_00007-3-2 loss: 0.603687  [   96/  179]
train() client id: f_00007-3-3 loss: 0.644132  [  128/  179]
train() client id: f_00007-3-4 loss: 0.623946  [  160/  179]
train() client id: f_00007-4-0 loss: 0.694599  [   32/  179]
train() client id: f_00007-4-1 loss: 0.680127  [   64/  179]
train() client id: f_00007-4-2 loss: 0.550850  [   96/  179]
train() client id: f_00007-4-3 loss: 0.707427  [  128/  179]
train() client id: f_00007-4-4 loss: 0.612424  [  160/  179]
train() client id: f_00007-5-0 loss: 0.672870  [   32/  179]
train() client id: f_00007-5-1 loss: 0.596339  [   64/  179]
train() client id: f_00007-5-2 loss: 0.564356  [   96/  179]
train() client id: f_00007-5-3 loss: 0.676081  [  128/  179]
train() client id: f_00007-5-4 loss: 0.549548  [  160/  179]
train() client id: f_00007-6-0 loss: 0.735828  [   32/  179]
train() client id: f_00007-6-1 loss: 0.624666  [   64/  179]
train() client id: f_00007-6-2 loss: 0.598565  [   96/  179]
train() client id: f_00007-6-3 loss: 0.512756  [  128/  179]
train() client id: f_00007-6-4 loss: 0.651778  [  160/  179]
train() client id: f_00008-0-0 loss: 0.553866  [   32/  130]
train() client id: f_00008-0-1 loss: 0.609553  [   64/  130]
train() client id: f_00008-0-2 loss: 0.897173  [   96/  130]
train() client id: f_00008-0-3 loss: 0.799681  [  128/  130]
train() client id: f_00008-1-0 loss: 0.825112  [   32/  130]
train() client id: f_00008-1-1 loss: 0.632526  [   64/  130]
train() client id: f_00008-1-2 loss: 0.730037  [   96/  130]
train() client id: f_00008-1-3 loss: 0.662286  [  128/  130]
train() client id: f_00008-2-0 loss: 0.708479  [   32/  130]
train() client id: f_00008-2-1 loss: 0.822175  [   64/  130]
train() client id: f_00008-2-2 loss: 0.695155  [   96/  130]
train() client id: f_00008-2-3 loss: 0.642771  [  128/  130]
train() client id: f_00008-3-0 loss: 0.664863  [   32/  130]
train() client id: f_00008-3-1 loss: 0.772705  [   64/  130]
train() client id: f_00008-3-2 loss: 0.630800  [   96/  130]
train() client id: f_00008-3-3 loss: 0.785889  [  128/  130]
train() client id: f_00008-4-0 loss: 0.642549  [   32/  130]
train() client id: f_00008-4-1 loss: 0.671108  [   64/  130]
train() client id: f_00008-4-2 loss: 0.658834  [   96/  130]
train() client id: f_00008-4-3 loss: 0.827794  [  128/  130]
train() client id: f_00008-5-0 loss: 0.718766  [   32/  130]
train() client id: f_00008-5-1 loss: 0.716836  [   64/  130]
train() client id: f_00008-5-2 loss: 0.705419  [   96/  130]
train() client id: f_00008-5-3 loss: 0.720982  [  128/  130]
train() client id: f_00008-6-0 loss: 0.670515  [   32/  130]
train() client id: f_00008-6-1 loss: 0.628134  [   64/  130]
train() client id: f_00008-6-2 loss: 0.762004  [   96/  130]
train() client id: f_00008-6-3 loss: 0.791014  [  128/  130]
train() client id: f_00009-0-0 loss: 1.039524  [   32/  118]
train() client id: f_00009-0-1 loss: 0.992841  [   64/  118]
train() client id: f_00009-0-2 loss: 1.109321  [   96/  118]
train() client id: f_00009-1-0 loss: 0.861492  [   32/  118]
train() client id: f_00009-1-1 loss: 1.039427  [   64/  118]
train() client id: f_00009-1-2 loss: 1.011116  [   96/  118]
train() client id: f_00009-2-0 loss: 0.896119  [   32/  118]
train() client id: f_00009-2-1 loss: 1.047355  [   64/  118]
train() client id: f_00009-2-2 loss: 0.942018  [   96/  118]
train() client id: f_00009-3-0 loss: 1.042177  [   32/  118]
train() client id: f_00009-3-1 loss: 1.071170  [   64/  118]
train() client id: f_00009-3-2 loss: 0.803180  [   96/  118]
train() client id: f_00009-4-0 loss: 0.755619  [   32/  118]
train() client id: f_00009-4-1 loss: 0.863857  [   64/  118]
train() client id: f_00009-4-2 loss: 1.094458  [   96/  118]
train() client id: f_00009-5-0 loss: 1.012238  [   32/  118]
train() client id: f_00009-5-1 loss: 0.983512  [   64/  118]
train() client id: f_00009-5-2 loss: 0.878009  [   96/  118]
train() client id: f_00009-6-0 loss: 0.856543  [   32/  118]
train() client id: f_00009-6-1 loss: 1.052201  [   64/  118]
train() client id: f_00009-6-2 loss: 0.939353  [   96/  118]
At round 69 accuracy: 0.649867374005305
At round 69 training accuracy: 0.5935613682092555
At round 69 training loss: 0.8072465270292951
update_location
xs = -3.905658 4.200318 365.009024 18.811294 0.979296 3.956410 -327.443192 -306.324852 349.663977 -292.060879 
ys = 357.587959 340.555839 1.320614 -327.455176 319.350187 302.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -18.71142616811743
ys mean: 101.39414253552872
dists_uav = 371.327891 354.959043 378.461797 342.900506 334.642348 318.923287 342.382731 322.235304 364.106532 308.732196 
uav_gains = -120.234698 -119.437359 -120.549487 -118.771277 -118.270653 -117.207399 -118.740997 -117.443885 -119.896724 -116.437853 
uav_gains_db_mean: -118.69903321731158
dists_bs = 250.223174 243.970264 567.251510 538.538784 227.608632 219.913287 234.071702 218.137734 547.774412 207.228720 
bs_gains = -106.720230 -106.412492 -116.672778 -116.041135 -105.568344 -105.150100 -105.908829 -105.051521 -116.247908 -104.427658 
bs_gains_db_mean: -108.82009940981325
Round 70
-------------------------------
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.73561523 3.43059311 1.71527241 0.65186493 3.95409396 1.90401388
 0.79238643 2.37939759 1.74146138 1.54414415]
obj_prev = 19.848843077614966
eta_min = 1.1829730818327539e-54	eta_max = 0.9523787254678187
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 4.515956513396188	eta = 0.9090909090909091
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 12.490584420921488	eta = 0.32868077856324013
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.913053448586266	eta = 0.518815529157309
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.145329232998529	eta = 0.5745592509885677
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.096743771047384	eta = 0.5784927770574487
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.096517585671171	eta = 0.5785112152005008
eta = 0.5785112152005008
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [0.04516167 0.09498287 0.04444481 0.01541231 0.10967833 0.05233018
 0.019355   0.06415824 0.04659537 0.04229426]
ene_total = [0.7365201  1.0329747  0.74175636 0.39122836 1.17571715 0.60187953
 0.42968967 0.84367489 0.64421599 0.49886084]
ti_comp = [2.36984321 2.57086067 2.35742536 2.41850329 2.57471655 2.57650502
 2.41937284 2.45238669 2.47671475 2.57942159]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.02506317e-06 8.10324115e-06 9.87341466e-07 3.91192234e-08
 1.24389480e-05 1.34919525e-06 7.74201308e-08 2.74447586e-06
 1.03075701e-06 7.10688982e-07]
ene_total = [0.28603811 0.08559175 0.29842467 0.23748935 0.08178872 0.07989408
 0.23662235 0.2037173  0.17943272 0.0769784 ]
optimize_network iter = 0 obj = 1.7659774351917215
eta = 0.5785112152005008
freqs = [ 9528407.84474552 18472970.62942295  9426557.92578857  3186333.13230265
 21299107.60121082 10155265.18397391  4000003.44569053 13080775.34726803
  9406688.61857887  8198399.6009876 ]
eta_min = 0.578511215200502	eta_max = 0.8002666857305597
af = 0.00021898789766085596	bf = 0.7442198316471329	zeta = 0.00024088668742694157	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.78511656e-07 1.41115497e-06 1.71942534e-07 6.81249461e-09
 2.16620522e-06 2.34958278e-07 1.34824820e-08 4.77942182e-07
 1.79503221e-07 1.23764340e-07]
ene_total = [1.32614126 0.39652609 1.38357144 1.10108858 0.3787282  0.37036751
 1.09706736 0.9444053  0.83187829 0.35687372]
ti_comp = [0.9721494  1.17316687 0.95973155 1.02080948 1.17702275 1.17881122
 1.02167903 1.05469288 1.07902094 1.18172779]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [4.11016540e-07 2.62561918e-06 4.01956633e-07 1.48159555e-08
 4.01613458e-06 4.34894323e-07 2.92931455e-08 1.00119980e-06
 3.66424074e-07 2.28467704e-07]
ene_total = [0.60360123 0.18050535 0.62974066 0.50116324 0.17241798 0.16857786
 0.49933314 0.42985932 0.37863531 0.16243412]
optimize_network iter = 1 obj = 3.726268206640733
eta = 0.8002666857305597
freqs = [ 9456245.97160665 16480383.53188894  9426557.92578857  3073300.22792621
 18967836.56712048  9036283.02009842  3856208.50116259 12382497.36052094
  8790118.03381017  7285272.29487025]
eta_min = 0.800266685730563	eta_max = 0.8002666857305585
af = 0.00017982620690873804	bf = 0.7442198316471329	zeta = 0.00019780882759961186	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.75818035e-07 1.12314508e-06 1.71942534e-07 6.33773081e-09
 1.71795736e-06 1.86032089e-07 1.25305500e-08 4.28277123e-07
 1.56742989e-07 9.77302347e-08]
ene_total = [1.32614114 0.39651277 1.38357144 1.10108855 0.37870747 0.37036524
 1.09706732 0.944403   0.83187723 0.35687251]
ti_comp = [0.9721494  1.17316687 0.95973155 1.02080948 1.17702275 1.17881122
 1.02167903 1.05469288 1.07902094 1.18172779]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [4.11016540e-07 2.62561918e-06 4.01956633e-07 1.48159555e-08
 4.01613458e-06 4.34894323e-07 2.92931455e-08 1.00119980e-06
 3.66424074e-07 2.28467704e-07]
ene_total = [0.60360123 0.18050535 0.62974066 0.50116324 0.17241798 0.16857786
 0.49933314 0.42985932 0.37863531 0.16243412]
optimize_network iter = 2 obj = 3.7262682066407105
eta = 0.8002666857305585
freqs = [ 9456245.97160664 16480383.53188894  9426557.92578855  3073300.22792621
 18967836.56712048  9036283.02009842  3856208.50116259 12382497.36052093
  8790118.03381016  7285272.29487025]
Done!
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [3.94344793e-07 2.51911821e-06 3.85652375e-07 1.42149873e-08
 3.85323121e-06 4.17254039e-07 2.81049502e-08 9.60588899e-07
 3.51561096e-07 2.19200545e-07]
ene_total = [0.02867458 0.00857495 0.02991635 0.02380819 0.0081907  0.00800842
 0.02372125 0.02042079 0.01798738 0.00771656]
At round 70 energy consumption: 0.1770191732645484
At round 70 eta: 0.8002666857305585
At round 70 a_n: 4.204393568402164
At round 70 local rounds: 7.295939587071601
At round 70 global rounds: 21.05003656390746
gradient difference: 0.697324812412262
train() client id: f_00000-0-0 loss: 0.969904  [   32/  126]
train() client id: f_00000-0-1 loss: 1.384761  [   64/  126]
train() client id: f_00000-0-2 loss: 1.148993  [   96/  126]
train() client id: f_00000-1-0 loss: 1.056835  [   32/  126]
train() client id: f_00000-1-1 loss: 1.031857  [   64/  126]
train() client id: f_00000-1-2 loss: 1.152577  [   96/  126]
train() client id: f_00000-2-0 loss: 0.999657  [   32/  126]
train() client id: f_00000-2-1 loss: 1.058245  [   64/  126]
train() client id: f_00000-2-2 loss: 1.145535  [   96/  126]
train() client id: f_00000-3-0 loss: 1.056826  [   32/  126]
train() client id: f_00000-3-1 loss: 1.065145  [   64/  126]
train() client id: f_00000-3-2 loss: 1.197575  [   96/  126]
train() client id: f_00000-4-0 loss: 1.075570  [   32/  126]
train() client id: f_00000-4-1 loss: 0.985350  [   64/  126]
train() client id: f_00000-4-2 loss: 0.936018  [   96/  126]
train() client id: f_00000-5-0 loss: 1.118090  [   32/  126]
train() client id: f_00000-5-1 loss: 0.919796  [   64/  126]
train() client id: f_00000-5-2 loss: 1.034505  [   96/  126]
train() client id: f_00000-6-0 loss: 1.009997  [   32/  126]
train() client id: f_00000-6-1 loss: 0.829431  [   64/  126]
train() client id: f_00000-6-2 loss: 1.050181  [   96/  126]
train() client id: f_00001-0-0 loss: 0.460231  [   32/  265]
train() client id: f_00001-0-1 loss: 0.385207  [   64/  265]
train() client id: f_00001-0-2 loss: 0.425447  [   96/  265]
train() client id: f_00001-0-3 loss: 0.415162  [  128/  265]
train() client id: f_00001-0-4 loss: 0.387963  [  160/  265]
train() client id: f_00001-0-5 loss: 0.496645  [  192/  265]
train() client id: f_00001-0-6 loss: 0.414655  [  224/  265]
train() client id: f_00001-0-7 loss: 0.630998  [  256/  265]
train() client id: f_00001-1-0 loss: 0.381008  [   32/  265]
train() client id: f_00001-1-1 loss: 0.586863  [   64/  265]
train() client id: f_00001-1-2 loss: 0.455784  [   96/  265]
train() client id: f_00001-1-3 loss: 0.375331  [  128/  265]
train() client id: f_00001-1-4 loss: 0.585916  [  160/  265]
train() client id: f_00001-1-5 loss: 0.461004  [  192/  265]
train() client id: f_00001-1-6 loss: 0.350963  [  224/  265]
train() client id: f_00001-1-7 loss: 0.433596  [  256/  265]
train() client id: f_00001-2-0 loss: 0.459481  [   32/  265]
train() client id: f_00001-2-1 loss: 0.568953  [   64/  265]
train() client id: f_00001-2-2 loss: 0.339013  [   96/  265]
train() client id: f_00001-2-3 loss: 0.589745  [  128/  265]
train() client id: f_00001-2-4 loss: 0.376489  [  160/  265]
train() client id: f_00001-2-5 loss: 0.438201  [  192/  265]
train() client id: f_00001-2-6 loss: 0.330238  [  224/  265]
train() client id: f_00001-2-7 loss: 0.477462  [  256/  265]
train() client id: f_00001-3-0 loss: 0.338562  [   32/  265]
train() client id: f_00001-3-1 loss: 0.533289  [   64/  265]
train() client id: f_00001-3-2 loss: 0.376013  [   96/  265]
train() client id: f_00001-3-3 loss: 0.394446  [  128/  265]
train() client id: f_00001-3-4 loss: 0.343515  [  160/  265]
train() client id: f_00001-3-5 loss: 0.520475  [  192/  265]
train() client id: f_00001-3-6 loss: 0.584792  [  224/  265]
train() client id: f_00001-3-7 loss: 0.506639  [  256/  265]
train() client id: f_00001-4-0 loss: 0.483716  [   32/  265]
train() client id: f_00001-4-1 loss: 0.543397  [   64/  265]
train() client id: f_00001-4-2 loss: 0.407358  [   96/  265]
train() client id: f_00001-4-3 loss: 0.395320  [  128/  265]
train() client id: f_00001-4-4 loss: 0.549470  [  160/  265]
train() client id: f_00001-4-5 loss: 0.437328  [  192/  265]
train() client id: f_00001-4-6 loss: 0.351305  [  224/  265]
train() client id: f_00001-4-7 loss: 0.404355  [  256/  265]
train() client id: f_00001-5-0 loss: 0.366789  [   32/  265]
train() client id: f_00001-5-1 loss: 0.424134  [   64/  265]
train() client id: f_00001-5-2 loss: 0.426395  [   96/  265]
train() client id: f_00001-5-3 loss: 0.481881  [  128/  265]
train() client id: f_00001-5-4 loss: 0.339113  [  160/  265]
train() client id: f_00001-5-5 loss: 0.460097  [  192/  265]
train() client id: f_00001-5-6 loss: 0.589765  [  224/  265]
train() client id: f_00001-5-7 loss: 0.475684  [  256/  265]
train() client id: f_00001-6-0 loss: 0.479247  [   32/  265]
train() client id: f_00001-6-1 loss: 0.464933  [   64/  265]
train() client id: f_00001-6-2 loss: 0.347165  [   96/  265]
train() client id: f_00001-6-3 loss: 0.485237  [  128/  265]
train() client id: f_00001-6-4 loss: 0.507626  [  160/  265]
train() client id: f_00001-6-5 loss: 0.332696  [  192/  265]
train() client id: f_00001-6-6 loss: 0.583306  [  224/  265]
train() client id: f_00001-6-7 loss: 0.355493  [  256/  265]
train() client id: f_00002-0-0 loss: 1.145398  [   32/  124]
train() client id: f_00002-0-1 loss: 1.018597  [   64/  124]
train() client id: f_00002-0-2 loss: 0.834862  [   96/  124]
train() client id: f_00002-1-0 loss: 1.011323  [   32/  124]
train() client id: f_00002-1-1 loss: 1.052485  [   64/  124]
train() client id: f_00002-1-2 loss: 1.079896  [   96/  124]
train() client id: f_00002-2-0 loss: 1.203730  [   32/  124]
train() client id: f_00002-2-1 loss: 0.899082  [   64/  124]
train() client id: f_00002-2-2 loss: 1.133325  [   96/  124]
train() client id: f_00002-3-0 loss: 1.084513  [   32/  124]
train() client id: f_00002-3-1 loss: 1.108693  [   64/  124]
train() client id: f_00002-3-2 loss: 0.995391  [   96/  124]
train() client id: f_00002-4-0 loss: 0.891981  [   32/  124]
train() client id: f_00002-4-1 loss: 1.174737  [   64/  124]
train() client id: f_00002-4-2 loss: 1.139155  [   96/  124]
train() client id: f_00002-5-0 loss: 1.124525  [   32/  124]
train() client id: f_00002-5-1 loss: 0.874196  [   64/  124]
train() client id: f_00002-5-2 loss: 0.951415  [   96/  124]
train() client id: f_00002-6-0 loss: 0.862895  [   32/  124]
train() client id: f_00002-6-1 loss: 1.116101  [   64/  124]
train() client id: f_00002-6-2 loss: 0.994708  [   96/  124]
train() client id: f_00003-0-0 loss: 0.468423  [   32/   43]
train() client id: f_00003-1-0 loss: 0.576876  [   32/   43]
train() client id: f_00003-2-0 loss: 0.477038  [   32/   43]
train() client id: f_00003-3-0 loss: 0.511513  [   32/   43]
train() client id: f_00003-4-0 loss: 0.572528  [   32/   43]
train() client id: f_00003-5-0 loss: 0.315677  [   32/   43]
train() client id: f_00003-6-0 loss: 0.529765  [   32/   43]
train() client id: f_00004-0-0 loss: 0.530592  [   32/  306]
train() client id: f_00004-0-1 loss: 0.662587  [   64/  306]
train() client id: f_00004-0-2 loss: 0.902792  [   96/  306]
train() client id: f_00004-0-3 loss: 0.770306  [  128/  306]
train() client id: f_00004-0-4 loss: 0.723117  [  160/  306]
train() client id: f_00004-0-5 loss: 0.890280  [  192/  306]
train() client id: f_00004-0-6 loss: 0.733940  [  224/  306]
train() client id: f_00004-0-7 loss: 0.713994  [  256/  306]
train() client id: f_00004-0-8 loss: 0.700203  [  288/  306]
train() client id: f_00004-1-0 loss: 0.627025  [   32/  306]
train() client id: f_00004-1-1 loss: 0.726048  [   64/  306]
train() client id: f_00004-1-2 loss: 0.726077  [   96/  306]
train() client id: f_00004-1-3 loss: 0.806655  [  128/  306]
train() client id: f_00004-1-4 loss: 0.781636  [  160/  306]
train() client id: f_00004-1-5 loss: 0.742107  [  192/  306]
train() client id: f_00004-1-6 loss: 0.659569  [  224/  306]
train() client id: f_00004-1-7 loss: 0.785428  [  256/  306]
train() client id: f_00004-1-8 loss: 0.748661  [  288/  306]
train() client id: f_00004-2-0 loss: 0.853532  [   32/  306]
train() client id: f_00004-2-1 loss: 0.764599  [   64/  306]
train() client id: f_00004-2-2 loss: 0.661224  [   96/  306]
train() client id: f_00004-2-3 loss: 0.758788  [  128/  306]
train() client id: f_00004-2-4 loss: 0.766029  [  160/  306]
train() client id: f_00004-2-5 loss: 0.637853  [  192/  306]
train() client id: f_00004-2-6 loss: 0.689959  [  224/  306]
train() client id: f_00004-2-7 loss: 0.648807  [  256/  306]
train() client id: f_00004-2-8 loss: 0.818915  [  288/  306]
train() client id: f_00004-3-0 loss: 0.797164  [   32/  306]
train() client id: f_00004-3-1 loss: 0.624859  [   64/  306]
train() client id: f_00004-3-2 loss: 0.837786  [   96/  306]
train() client id: f_00004-3-3 loss: 0.620217  [  128/  306]
train() client id: f_00004-3-4 loss: 0.957441  [  160/  306]
train() client id: f_00004-3-5 loss: 0.719818  [  192/  306]
train() client id: f_00004-3-6 loss: 0.613852  [  224/  306]
train() client id: f_00004-3-7 loss: 0.686707  [  256/  306]
train() client id: f_00004-3-8 loss: 0.739561  [  288/  306]
train() client id: f_00004-4-0 loss: 0.821176  [   32/  306]
train() client id: f_00004-4-1 loss: 0.722493  [   64/  306]
train() client id: f_00004-4-2 loss: 0.733762  [   96/  306]
train() client id: f_00004-4-3 loss: 0.853268  [  128/  306]
train() client id: f_00004-4-4 loss: 0.627747  [  160/  306]
train() client id: f_00004-4-5 loss: 0.650191  [  192/  306]
train() client id: f_00004-4-6 loss: 0.747023  [  224/  306]
train() client id: f_00004-4-7 loss: 0.761871  [  256/  306]
train() client id: f_00004-4-8 loss: 0.758700  [  288/  306]
train() client id: f_00004-5-0 loss: 0.795989  [   32/  306]
train() client id: f_00004-5-1 loss: 0.651350  [   64/  306]
train() client id: f_00004-5-2 loss: 0.718177  [   96/  306]
train() client id: f_00004-5-3 loss: 0.674532  [  128/  306]
train() client id: f_00004-5-4 loss: 0.722028  [  160/  306]
train() client id: f_00004-5-5 loss: 0.728161  [  192/  306]
train() client id: f_00004-5-6 loss: 0.795094  [  224/  306]
train() client id: f_00004-5-7 loss: 0.719175  [  256/  306]
train() client id: f_00004-5-8 loss: 0.817991  [  288/  306]
train() client id: f_00004-6-0 loss: 0.864144  [   32/  306]
train() client id: f_00004-6-1 loss: 0.771895  [   64/  306]
train() client id: f_00004-6-2 loss: 0.629182  [   96/  306]
train() client id: f_00004-6-3 loss: 0.890881  [  128/  306]
train() client id: f_00004-6-4 loss: 0.661586  [  160/  306]
train() client id: f_00004-6-5 loss: 0.707907  [  192/  306]
train() client id: f_00004-6-6 loss: 0.707949  [  224/  306]
train() client id: f_00004-6-7 loss: 0.725449  [  256/  306]
train() client id: f_00004-6-8 loss: 0.726650  [  288/  306]
train() client id: f_00005-0-0 loss: 0.792481  [   32/  146]
train() client id: f_00005-0-1 loss: 0.480017  [   64/  146]
train() client id: f_00005-0-2 loss: 0.958213  [   96/  146]
train() client id: f_00005-0-3 loss: 0.874760  [  128/  146]
train() client id: f_00005-1-0 loss: 0.678534  [   32/  146]
train() client id: f_00005-1-1 loss: 0.931861  [   64/  146]
train() client id: f_00005-1-2 loss: 0.871679  [   96/  146]
train() client id: f_00005-1-3 loss: 0.570851  [  128/  146]
train() client id: f_00005-2-0 loss: 0.948704  [   32/  146]
train() client id: f_00005-2-1 loss: 0.688707  [   64/  146]
train() client id: f_00005-2-2 loss: 0.859785  [   96/  146]
train() client id: f_00005-2-3 loss: 0.769561  [  128/  146]
train() client id: f_00005-3-0 loss: 1.154981  [   32/  146]
train() client id: f_00005-3-1 loss: 0.669518  [   64/  146]
train() client id: f_00005-3-2 loss: 0.603827  [   96/  146]
train() client id: f_00005-3-3 loss: 0.631850  [  128/  146]
train() client id: f_00005-4-0 loss: 0.742374  [   32/  146]
train() client id: f_00005-4-1 loss: 0.595490  [   64/  146]
train() client id: f_00005-4-2 loss: 0.853226  [   96/  146]
train() client id: f_00005-4-3 loss: 0.860948  [  128/  146]
train() client id: f_00005-5-0 loss: 0.784624  [   32/  146]
train() client id: f_00005-5-1 loss: 0.471257  [   64/  146]
train() client id: f_00005-5-2 loss: 0.720811  [   96/  146]
train() client id: f_00005-5-3 loss: 1.006306  [  128/  146]
train() client id: f_00005-6-0 loss: 0.659246  [   32/  146]
train() client id: f_00005-6-1 loss: 0.743517  [   64/  146]
train() client id: f_00005-6-2 loss: 0.643861  [   96/  146]
train() client id: f_00005-6-3 loss: 0.813159  [  128/  146]
train() client id: f_00006-0-0 loss: 0.571536  [   32/   54]
train() client id: f_00006-1-0 loss: 0.446623  [   32/   54]
train() client id: f_00006-2-0 loss: 0.515734  [   32/   54]
train() client id: f_00006-3-0 loss: 0.449299  [   32/   54]
train() client id: f_00006-4-0 loss: 0.513961  [   32/   54]
train() client id: f_00006-5-0 loss: 0.525930  [   32/   54]
train() client id: f_00006-6-0 loss: 0.458473  [   32/   54]
train() client id: f_00007-0-0 loss: 0.563363  [   32/  179]
train() client id: f_00007-0-1 loss: 0.527380  [   64/  179]
train() client id: f_00007-0-2 loss: 0.640616  [   96/  179]
train() client id: f_00007-0-3 loss: 0.616593  [  128/  179]
train() client id: f_00007-0-4 loss: 0.603624  [  160/  179]
train() client id: f_00007-1-0 loss: 0.484289  [   32/  179]
train() client id: f_00007-1-1 loss: 0.500788  [   64/  179]
train() client id: f_00007-1-2 loss: 0.638004  [   96/  179]
train() client id: f_00007-1-3 loss: 0.478396  [  128/  179]
train() client id: f_00007-1-4 loss: 0.669987  [  160/  179]
train() client id: f_00007-2-0 loss: 0.480903  [   32/  179]
train() client id: f_00007-2-1 loss: 0.595513  [   64/  179]
train() client id: f_00007-2-2 loss: 0.493104  [   96/  179]
train() client id: f_00007-2-3 loss: 0.741434  [  128/  179]
train() client id: f_00007-2-4 loss: 0.545357  [  160/  179]
train() client id: f_00007-3-0 loss: 0.611062  [   32/  179]
train() client id: f_00007-3-1 loss: 0.515541  [   64/  179]
train() client id: f_00007-3-2 loss: 0.750502  [   96/  179]
train() client id: f_00007-3-3 loss: 0.504590  [  128/  179]
train() client id: f_00007-3-4 loss: 0.490655  [  160/  179]
train() client id: f_00007-4-0 loss: 0.538017  [   32/  179]
train() client id: f_00007-4-1 loss: 0.657030  [   64/  179]
train() client id: f_00007-4-2 loss: 0.724155  [   96/  179]
train() client id: f_00007-4-3 loss: 0.402354  [  128/  179]
train() client id: f_00007-4-4 loss: 0.584968  [  160/  179]
train() client id: f_00007-5-0 loss: 0.499784  [   32/  179]
train() client id: f_00007-5-1 loss: 0.576777  [   64/  179]
train() client id: f_00007-5-2 loss: 0.370306  [   96/  179]
train() client id: f_00007-5-3 loss: 0.418210  [  128/  179]
train() client id: f_00007-5-4 loss: 0.757712  [  160/  179]
train() client id: f_00007-6-0 loss: 0.520941  [   32/  179]
train() client id: f_00007-6-1 loss: 0.480364  [   64/  179]
train() client id: f_00007-6-2 loss: 0.475291  [   96/  179]
train() client id: f_00007-6-3 loss: 0.527714  [  128/  179]
train() client id: f_00007-6-4 loss: 0.762280  [  160/  179]
train() client id: f_00008-0-0 loss: 0.623000  [   32/  130]
train() client id: f_00008-0-1 loss: 0.653453  [   64/  130]
train() client id: f_00008-0-2 loss: 0.716222  [   96/  130]
train() client id: f_00008-0-3 loss: 0.617592  [  128/  130]
train() client id: f_00008-1-0 loss: 0.632199  [   32/  130]
train() client id: f_00008-1-1 loss: 0.671835  [   64/  130]
train() client id: f_00008-1-2 loss: 0.716686  [   96/  130]
train() client id: f_00008-1-3 loss: 0.559353  [  128/  130]
train() client id: f_00008-2-0 loss: 0.620335  [   32/  130]
train() client id: f_00008-2-1 loss: 0.672407  [   64/  130]
train() client id: f_00008-2-2 loss: 0.630903  [   96/  130]
train() client id: f_00008-2-3 loss: 0.687051  [  128/  130]
train() client id: f_00008-3-0 loss: 0.670451  [   32/  130]
train() client id: f_00008-3-1 loss: 0.565158  [   64/  130]
train() client id: f_00008-3-2 loss: 0.721032  [   96/  130]
train() client id: f_00008-3-3 loss: 0.647688  [  128/  130]
train() client id: f_00008-4-0 loss: 0.555369  [   32/  130]
train() client id: f_00008-4-1 loss: 0.719123  [   64/  130]
train() client id: f_00008-4-2 loss: 0.626174  [   96/  130]
train() client id: f_00008-4-3 loss: 0.653081  [  128/  130]
train() client id: f_00008-5-0 loss: 0.680521  [   32/  130]
train() client id: f_00008-5-1 loss: 0.697093  [   64/  130]
train() client id: f_00008-5-2 loss: 0.601339  [   96/  130]
train() client id: f_00008-5-3 loss: 0.630900  [  128/  130]
train() client id: f_00008-6-0 loss: 0.683109  [   32/  130]
train() client id: f_00008-6-1 loss: 0.603855  [   64/  130]
train() client id: f_00008-6-2 loss: 0.737909  [   96/  130]
train() client id: f_00008-6-3 loss: 0.572795  [  128/  130]
train() client id: f_00009-0-0 loss: 0.818789  [   32/  118]
train() client id: f_00009-0-1 loss: 0.899922  [   64/  118]
train() client id: f_00009-0-2 loss: 0.736093  [   96/  118]
train() client id: f_00009-1-0 loss: 0.801067  [   32/  118]
train() client id: f_00009-1-1 loss: 1.018272  [   64/  118]
train() client id: f_00009-1-2 loss: 0.818118  [   96/  118]
train() client id: f_00009-2-0 loss: 0.871348  [   32/  118]
train() client id: f_00009-2-1 loss: 0.859550  [   64/  118]
train() client id: f_00009-2-2 loss: 0.603518  [   96/  118]
train() client id: f_00009-3-0 loss: 0.661224  [   32/  118]
train() client id: f_00009-3-1 loss: 0.825276  [   64/  118]
train() client id: f_00009-3-2 loss: 0.807751  [   96/  118]
train() client id: f_00009-4-0 loss: 0.777553  [   32/  118]
train() client id: f_00009-4-1 loss: 0.665202  [   64/  118]
train() client id: f_00009-4-2 loss: 0.846615  [   96/  118]
train() client id: f_00009-5-0 loss: 0.710495  [   32/  118]
train() client id: f_00009-5-1 loss: 0.870845  [   64/  118]
train() client id: f_00009-5-2 loss: 0.914988  [   96/  118]
train() client id: f_00009-6-0 loss: 0.936755  [   32/  118]
train() client id: f_00009-6-1 loss: 0.622843  [   64/  118]
train() client id: f_00009-6-2 loss: 0.689887  [   96/  118]
At round 70 accuracy: 0.6445623342175066
At round 70 training accuracy: 0.590878604963112
At round 70 training loss: 0.8202281816496284
update_location
xs = -3.905658 4.200318 370.009024 18.811294 0.979296 3.956410 -332.443192 -311.324852 354.663977 -297.060879 
ys = 362.587959 345.555839 1.320614 -332.455176 324.350187 307.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -19.21142616811743
ys mean: 102.8941425355287
dists_uav = 376.145294 359.758920 383.286344 347.678456 339.417151 323.674535 347.167634 326.992109 368.910838 313.466390 
uav_gains = -120.449229 -119.682988 -120.752598 -119.043997 -118.564750 -117.544555 -119.015407 -117.771801 -120.123842 -116.803129 
uav_gains_db_mean: -118.97522960476411
dists_bs = 253.894695 247.390477 572.013431 543.206750 230.812025 222.856445 237.358366 221.183905 552.565332 210.093673 
bs_gains = -106.897361 -106.581783 -116.774433 -116.146084 -105.738295 -105.311765 -106.078386 -105.220158 -116.353801 -104.594622 
bs_gains_db_mean: -108.96966876067157
Round 71
-------------------------------
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.59747704 3.1514096  1.57881481 0.60189948 3.63223305 1.7491537
 0.73097414 2.18851291 1.6003418  1.41859333]
obj_prev = 18.24940985510058
eta_min = 2.2153155362897003e-59	eta_max = 0.954951968660827
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 4.14802660303202	eta = 0.909090909090909
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 11.665880506041221	eta = 0.323244634087488
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 7.328813927254386	eta = 0.5145352730897301
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.605790167817666	eta = 0.5708527185521312
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.559990536518958	eta = 0.5748382188192438
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.55977636966569	eta = 0.5748569864243457
eta = 0.5748569864243457
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [0.04568455 0.09608258 0.0449594  0.01559076 0.11094819 0.05293607
 0.01957909 0.06490107 0.04713485 0.04278394]
ene_total = [0.68305847 0.95139002 0.68780539 0.3652074  1.08286069 0.55421391
 0.40065058 0.78194702 0.59332082 0.45932206]
ti_comp = [2.6166745  2.82525406 2.60419719 2.66565241 2.82917673 2.83103203
 2.66651706 2.69997525 2.72996789 2.83397532]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [8.70339434e-07 6.94542833e-06 8.37517236e-07 3.33330274e-08
 1.06640102e-05 1.15676577e-06 6.59734165e-08 2.34377520e-06
 8.78195905e-07 6.09439114e-07]
ene_total = [0.26808419 0.07867313 0.27941784 0.2235868  0.07514369 0.07337205
 0.22280168 0.19243014 0.16517258 0.0706935 ]
optimize_network iter = 0 obj = 1.6493755938248962
eta = 0.5748569864243457
freqs = [ 8729505.68014144 17004237.96072038  8632103.28335727  2924379.60099508
 19607858.17168588  9349252.32733673  3671285.85880158 12018826.22316111
  8632858.47902665  7548397.32371458]
eta_min = 0.5748569864243458	eta_max = 0.8123930873707683
af = 0.00017023437323749214	bf = 0.7011262629418353	zeta = 0.00018725781056124136	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [1.49832217e-07 1.19568169e-06 1.44181752e-07 5.73840643e-09
 1.83584959e-06 1.99141592e-07 1.13575726e-08 4.03489744e-07
 1.51184738e-07 1.04917243e-07]
ene_total = [1.25368474 0.36767638 1.30668851 1.04561886 0.35103995 0.34308906
 1.04194602 0.8998312  0.77241077 0.33058187]
ti_comp = [0.98979546 1.19837503 0.97731815 1.03877337 1.2022977  1.20415299
 1.03963802 1.07309621 1.10308885 1.20709628]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [3.21501828e-07 2.04039780e-06 3.14308302e-07 1.16018294e-08
 3.12106616e-06 3.37954282e-07 2.29392754e-08 7.84231878e-07
 2.84296680e-07 1.77551736e-07]
ene_total = [0.60750428 0.17818314 0.63318841 0.50667793 0.17013065 0.16625427
 0.5048983  0.436041   0.37429151 0.16019228]
optimize_network iter = 1 obj = 3.7373617634764136
eta = 0.8123930873707683
freqs = [ 8660759.82156696 15044717.2754607   8632103.28335727  2816297.66515272
 17315710.58589685  8249014.9990354   3533804.43880649 11348678.03144383
  8017957.73562188  6650755.80408876]
eta_min = 0.8123930873707704	eta_max = 0.8123930873707671
af = 0.0001378387179910577	bf = 0.7011262629418353	zeta = 0.0001516225897901635	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [1.47481618e-07 9.35985870e-07 1.44181752e-07 5.32207417e-09
 1.43171779e-06 1.55028805e-07 1.05228684e-08 3.59748455e-07
 1.30414606e-07 8.14478020e-08]
ene_total = [1.25368464 0.36766535 1.30668851 1.04561884 0.35102278 0.34308719
 1.04194599 0.89982934 0.77240989 0.33058088]
ti_comp = [0.98979546 1.19837503 0.97731815 1.03877337 1.2022977  1.20415299
 1.03963802 1.07309621 1.10308885 1.20709628]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [3.21501828e-07 2.04039780e-06 3.14308302e-07 1.16018294e-08
 3.12106616e-06 3.37954282e-07 2.29392754e-08 7.84231878e-07
 2.84296680e-07 1.77551736e-07]
ene_total = [0.60750428 0.17818314 0.63318841 0.50667793 0.17013065 0.16625427
 0.5048983  0.436041   0.37429151 0.16019228]
optimize_network iter = 2 obj = 3.7373617634763896
eta = 0.8123930873707671
freqs = [ 8660759.82156695 15044717.2754607   8632103.28335726  2816297.66515272
 17315710.58589685  8249014.9990354   3533804.43880649 11348678.03144382
  8017957.73562187  6650755.80408876]
Done!
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [2.83533115e-07 1.79943097e-06 2.77189130e-07 1.02316770e-08
 2.75247459e-06 2.98042568e-07 2.02301936e-08 6.91615688e-07
 2.50721820e-07 1.56583237e-07]
ene_total = [0.02951226 0.00865582 0.03075999 0.0246142  0.00826451 0.00807652
 0.02452774 0.02118259 0.01818289 0.00778205]
At round 71 energy consumption: 0.18155856217199728
At round 71 eta: 0.8123930873707671
At round 71 a_n: 3.8618477214328486
At round 71 local rounds: 6.803476774877765
At round 71 global rounds: 20.584783723108377
gradient difference: 0.6501563787460327
train() client id: f_00000-0-0 loss: 1.046443  [   32/  126]
train() client id: f_00000-0-1 loss: 1.073035  [   64/  126]
train() client id: f_00000-0-2 loss: 0.947371  [   96/  126]
train() client id: f_00000-1-0 loss: 0.891804  [   32/  126]
train() client id: f_00000-1-1 loss: 0.851160  [   64/  126]
train() client id: f_00000-1-2 loss: 1.075168  [   96/  126]
train() client id: f_00000-2-0 loss: 0.775872  [   32/  126]
train() client id: f_00000-2-1 loss: 0.849469  [   64/  126]
train() client id: f_00000-2-2 loss: 0.970517  [   96/  126]
train() client id: f_00000-3-0 loss: 0.887649  [   32/  126]
train() client id: f_00000-3-1 loss: 0.936697  [   64/  126]
train() client id: f_00000-3-2 loss: 0.719758  [   96/  126]
train() client id: f_00000-4-0 loss: 0.652934  [   32/  126]
train() client id: f_00000-4-1 loss: 0.904759  [   64/  126]
train() client id: f_00000-4-2 loss: 0.934705  [   96/  126]
train() client id: f_00000-5-0 loss: 0.763987  [   32/  126]
train() client id: f_00000-5-1 loss: 0.999069  [   64/  126]
train() client id: f_00000-5-2 loss: 0.799118  [   96/  126]
train() client id: f_00001-0-0 loss: 0.434066  [   32/  265]
train() client id: f_00001-0-1 loss: 0.617390  [   64/  265]
train() client id: f_00001-0-2 loss: 0.512064  [   96/  265]
train() client id: f_00001-0-3 loss: 0.574125  [  128/  265]
train() client id: f_00001-0-4 loss: 0.665921  [  160/  265]
train() client id: f_00001-0-5 loss: 0.482942  [  192/  265]
train() client id: f_00001-0-6 loss: 0.581983  [  224/  265]
train() client id: f_00001-0-7 loss: 0.502367  [  256/  265]
train() client id: f_00001-1-0 loss: 0.548957  [   32/  265]
train() client id: f_00001-1-1 loss: 0.585540  [   64/  265]
train() client id: f_00001-1-2 loss: 0.634429  [   96/  265]
train() client id: f_00001-1-3 loss: 0.440163  [  128/  265]
train() client id: f_00001-1-4 loss: 0.502285  [  160/  265]
train() client id: f_00001-1-5 loss: 0.560365  [  192/  265]
train() client id: f_00001-1-6 loss: 0.557045  [  224/  265]
train() client id: f_00001-1-7 loss: 0.516015  [  256/  265]
train() client id: f_00001-2-0 loss: 0.455568  [   32/  265]
train() client id: f_00001-2-1 loss: 0.527234  [   64/  265]
train() client id: f_00001-2-2 loss: 0.462677  [   96/  265]
train() client id: f_00001-2-3 loss: 0.458014  [  128/  265]
train() client id: f_00001-2-4 loss: 0.613500  [  160/  265]
train() client id: f_00001-2-5 loss: 0.603989  [  192/  265]
train() client id: f_00001-2-6 loss: 0.510595  [  224/  265]
train() client id: f_00001-2-7 loss: 0.616279  [  256/  265]
train() client id: f_00001-3-0 loss: 0.478070  [   32/  265]
train() client id: f_00001-3-1 loss: 0.454925  [   64/  265]
train() client id: f_00001-3-2 loss: 0.536256  [   96/  265]
train() client id: f_00001-3-3 loss: 0.454965  [  128/  265]
train() client id: f_00001-3-4 loss: 0.451967  [  160/  265]
train() client id: f_00001-3-5 loss: 0.664698  [  192/  265]
train() client id: f_00001-3-6 loss: 0.688141  [  224/  265]
train() client id: f_00001-3-7 loss: 0.494532  [  256/  265]
train() client id: f_00001-4-0 loss: 0.437735  [   32/  265]
train() client id: f_00001-4-1 loss: 0.490094  [   64/  265]
train() client id: f_00001-4-2 loss: 0.534030  [   96/  265]
train() client id: f_00001-4-3 loss: 0.531384  [  128/  265]
train() client id: f_00001-4-4 loss: 0.607131  [  160/  265]
train() client id: f_00001-4-5 loss: 0.596643  [  192/  265]
train() client id: f_00001-4-6 loss: 0.490449  [  224/  265]
train() client id: f_00001-4-7 loss: 0.529522  [  256/  265]
train() client id: f_00001-5-0 loss: 0.428269  [   32/  265]
train() client id: f_00001-5-1 loss: 0.498443  [   64/  265]
train() client id: f_00001-5-2 loss: 0.560888  [   96/  265]
train() client id: f_00001-5-3 loss: 0.507009  [  128/  265]
train() client id: f_00001-5-4 loss: 0.539150  [  160/  265]
train() client id: f_00001-5-5 loss: 0.564245  [  192/  265]
train() client id: f_00001-5-6 loss: 0.531471  [  224/  265]
train() client id: f_00001-5-7 loss: 0.617152  [  256/  265]
train() client id: f_00002-0-0 loss: 1.155691  [   32/  124]
train() client id: f_00002-0-1 loss: 1.219599  [   64/  124]
train() client id: f_00002-0-2 loss: 0.960479  [   96/  124]
train() client id: f_00002-1-0 loss: 1.203781  [   32/  124]
train() client id: f_00002-1-1 loss: 0.993924  [   64/  124]
train() client id: f_00002-1-2 loss: 1.027706  [   96/  124]
train() client id: f_00002-2-0 loss: 1.168184  [   32/  124]
train() client id: f_00002-2-1 loss: 1.005871  [   64/  124]
train() client id: f_00002-2-2 loss: 1.079683  [   96/  124]
train() client id: f_00002-3-0 loss: 1.129013  [   32/  124]
train() client id: f_00002-3-1 loss: 1.075718  [   64/  124]
train() client id: f_00002-3-2 loss: 1.069384  [   96/  124]
train() client id: f_00002-4-0 loss: 0.978746  [   32/  124]
train() client id: f_00002-4-1 loss: 1.083252  [   64/  124]
train() client id: f_00002-4-2 loss: 1.194199  [   96/  124]
train() client id: f_00002-5-0 loss: 1.264432  [   32/  124]
train() client id: f_00002-5-1 loss: 1.086219  [   64/  124]
train() client id: f_00002-5-2 loss: 0.918480  [   96/  124]
train() client id: f_00003-0-0 loss: 0.688007  [   32/   43]
train() client id: f_00003-1-0 loss: 0.478849  [   32/   43]
train() client id: f_00003-2-0 loss: 0.588871  [   32/   43]
train() client id: f_00003-3-0 loss: 0.698241  [   32/   43]
train() client id: f_00003-4-0 loss: 0.521376  [   32/   43]
train() client id: f_00003-5-0 loss: 0.686234  [   32/   43]
train() client id: f_00004-0-0 loss: 0.752496  [   32/  306]
train() client id: f_00004-0-1 loss: 0.951613  [   64/  306]
train() client id: f_00004-0-2 loss: 1.050851  [   96/  306]
train() client id: f_00004-0-3 loss: 0.879596  [  128/  306]
train() client id: f_00004-0-4 loss: 0.802668  [  160/  306]
train() client id: f_00004-0-5 loss: 0.882318  [  192/  306]
train() client id: f_00004-0-6 loss: 0.730177  [  224/  306]
train() client id: f_00004-0-7 loss: 0.750492  [  256/  306]
train() client id: f_00004-0-8 loss: 0.830931  [  288/  306]
train() client id: f_00004-1-0 loss: 0.751070  [   32/  306]
train() client id: f_00004-1-1 loss: 0.799884  [   64/  306]
train() client id: f_00004-1-2 loss: 0.614897  [   96/  306]
train() client id: f_00004-1-3 loss: 0.831301  [  128/  306]
train() client id: f_00004-1-4 loss: 0.844301  [  160/  306]
train() client id: f_00004-1-5 loss: 0.968052  [  192/  306]
train() client id: f_00004-1-6 loss: 0.952064  [  224/  306]
train() client id: f_00004-1-7 loss: 0.873581  [  256/  306]
train() client id: f_00004-1-8 loss: 0.799941  [  288/  306]
train() client id: f_00004-2-0 loss: 0.876877  [   32/  306]
train() client id: f_00004-2-1 loss: 0.839327  [   64/  306]
train() client id: f_00004-2-2 loss: 0.893869  [   96/  306]
train() client id: f_00004-2-3 loss: 0.906119  [  128/  306]
train() client id: f_00004-2-4 loss: 0.829601  [  160/  306]
train() client id: f_00004-2-5 loss: 0.750599  [  192/  306]
train() client id: f_00004-2-6 loss: 0.827265  [  224/  306]
train() client id: f_00004-2-7 loss: 0.801089  [  256/  306]
train() client id: f_00004-2-8 loss: 0.779085  [  288/  306]
train() client id: f_00004-3-0 loss: 0.749394  [   32/  306]
train() client id: f_00004-3-1 loss: 0.998664  [   64/  306]
train() client id: f_00004-3-2 loss: 0.779130  [   96/  306]
train() client id: f_00004-3-3 loss: 0.849229  [  128/  306]
train() client id: f_00004-3-4 loss: 0.677637  [  160/  306]
train() client id: f_00004-3-5 loss: 0.918118  [  192/  306]
train() client id: f_00004-3-6 loss: 0.904122  [  224/  306]
train() client id: f_00004-3-7 loss: 0.818534  [  256/  306]
train() client id: f_00004-3-8 loss: 0.854307  [  288/  306]
train() client id: f_00004-4-0 loss: 0.807233  [   32/  306]
train() client id: f_00004-4-1 loss: 0.720983  [   64/  306]
train() client id: f_00004-4-2 loss: 0.991087  [   96/  306]
train() client id: f_00004-4-3 loss: 0.942544  [  128/  306]
train() client id: f_00004-4-4 loss: 0.752320  [  160/  306]
train() client id: f_00004-4-5 loss: 0.805977  [  192/  306]
train() client id: f_00004-4-6 loss: 0.791551  [  224/  306]
train() client id: f_00004-4-7 loss: 0.794138  [  256/  306]
train() client id: f_00004-4-8 loss: 0.829855  [  288/  306]
train() client id: f_00004-5-0 loss: 0.924165  [   32/  306]
train() client id: f_00004-5-1 loss: 0.850266  [   64/  306]
train() client id: f_00004-5-2 loss: 0.779457  [   96/  306]
train() client id: f_00004-5-3 loss: 0.902532  [  128/  306]
train() client id: f_00004-5-4 loss: 0.812994  [  160/  306]
train() client id: f_00004-5-5 loss: 0.705415  [  192/  306]
train() client id: f_00004-5-6 loss: 0.839549  [  224/  306]
train() client id: f_00004-5-7 loss: 0.778103  [  256/  306]
train() client id: f_00004-5-8 loss: 0.991428  [  288/  306]
train() client id: f_00005-0-0 loss: 0.782604  [   32/  146]
train() client id: f_00005-0-1 loss: 0.655563  [   64/  146]
train() client id: f_00005-0-2 loss: 0.299754  [   96/  146]
train() client id: f_00005-0-3 loss: 0.481019  [  128/  146]
train() client id: f_00005-1-0 loss: 0.443027  [   32/  146]
train() client id: f_00005-1-1 loss: 0.525536  [   64/  146]
train() client id: f_00005-1-2 loss: 1.027310  [   96/  146]
train() client id: f_00005-1-3 loss: 0.412684  [  128/  146]
train() client id: f_00005-2-0 loss: 0.591083  [   32/  146]
train() client id: f_00005-2-1 loss: 0.605162  [   64/  146]
train() client id: f_00005-2-2 loss: 0.400342  [   96/  146]
train() client id: f_00005-2-3 loss: 0.759964  [  128/  146]
train() client id: f_00005-3-0 loss: 0.581346  [   32/  146]
train() client id: f_00005-3-1 loss: 0.660394  [   64/  146]
train() client id: f_00005-3-2 loss: 0.712790  [   96/  146]
train() client id: f_00005-3-3 loss: 0.574069  [  128/  146]
train() client id: f_00005-4-0 loss: 0.690288  [   32/  146]
train() client id: f_00005-4-1 loss: 0.705253  [   64/  146]
train() client id: f_00005-4-2 loss: 0.462464  [   96/  146]
train() client id: f_00005-4-3 loss: 0.475972  [  128/  146]
train() client id: f_00005-5-0 loss: 0.566800  [   32/  146]
train() client id: f_00005-5-1 loss: 0.453699  [   64/  146]
train() client id: f_00005-5-2 loss: 0.853417  [   96/  146]
train() client id: f_00005-5-3 loss: 0.538454  [  128/  146]
train() client id: f_00006-0-0 loss: 0.550110  [   32/   54]
train() client id: f_00006-1-0 loss: 0.585747  [   32/   54]
train() client id: f_00006-2-0 loss: 0.469591  [   32/   54]
train() client id: f_00006-3-0 loss: 0.575392  [   32/   54]
train() client id: f_00006-4-0 loss: 0.532657  [   32/   54]
train() client id: f_00006-5-0 loss: 0.564526  [   32/   54]
train() client id: f_00007-0-0 loss: 0.339218  [   32/  179]
train() client id: f_00007-0-1 loss: 0.520509  [   64/  179]
train() client id: f_00007-0-2 loss: 0.323274  [   96/  179]
train() client id: f_00007-0-3 loss: 0.294581  [  128/  179]
train() client id: f_00007-0-4 loss: 0.387669  [  160/  179]
train() client id: f_00007-1-0 loss: 0.383736  [   32/  179]
train() client id: f_00007-1-1 loss: 0.508019  [   64/  179]
train() client id: f_00007-1-2 loss: 0.368300  [   96/  179]
train() client id: f_00007-1-3 loss: 0.221356  [  128/  179]
train() client id: f_00007-1-4 loss: 0.381245  [  160/  179]
train() client id: f_00007-2-0 loss: 0.241214  [   32/  179]
train() client id: f_00007-2-1 loss: 0.510596  [   64/  179]
train() client id: f_00007-2-2 loss: 0.530638  [   96/  179]
train() client id: f_00007-2-3 loss: 0.388528  [  128/  179]
train() client id: f_00007-2-4 loss: 0.242751  [  160/  179]
train() client id: f_00007-3-0 loss: 0.446493  [   32/  179]
train() client id: f_00007-3-1 loss: 0.238038  [   64/  179]
train() client id: f_00007-3-2 loss: 0.357254  [   96/  179]
train() client id: f_00007-3-3 loss: 0.274529  [  128/  179]
train() client id: f_00007-3-4 loss: 0.364610  [  160/  179]
train() client id: f_00007-4-0 loss: 0.415794  [   32/  179]
train() client id: f_00007-4-1 loss: 0.208667  [   64/  179]
train() client id: f_00007-4-2 loss: 0.510353  [   96/  179]
train() client id: f_00007-4-3 loss: 0.361710  [  128/  179]
train() client id: f_00007-4-4 loss: 0.260028  [  160/  179]
train() client id: f_00007-5-0 loss: 0.168251  [   32/  179]
train() client id: f_00007-5-1 loss: 0.244717  [   64/  179]
train() client id: f_00007-5-2 loss: 0.372590  [   96/  179]
train() client id: f_00007-5-3 loss: 0.341792  [  128/  179]
train() client id: f_00007-5-4 loss: 0.592990  [  160/  179]
train() client id: f_00008-0-0 loss: 0.864156  [   32/  130]
train() client id: f_00008-0-1 loss: 0.741700  [   64/  130]
train() client id: f_00008-0-2 loss: 0.844549  [   96/  130]
train() client id: f_00008-0-3 loss: 0.764254  [  128/  130]
train() client id: f_00008-1-0 loss: 0.830397  [   32/  130]
train() client id: f_00008-1-1 loss: 0.755252  [   64/  130]
train() client id: f_00008-1-2 loss: 0.791658  [   96/  130]
train() client id: f_00008-1-3 loss: 0.787562  [  128/  130]
train() client id: f_00008-2-0 loss: 0.809712  [   32/  130]
train() client id: f_00008-2-1 loss: 0.753469  [   64/  130]
train() client id: f_00008-2-2 loss: 0.853763  [   96/  130]
train() client id: f_00008-2-3 loss: 0.798292  [  128/  130]
train() client id: f_00008-3-0 loss: 0.676830  [   32/  130]
train() client id: f_00008-3-1 loss: 0.803402  [   64/  130]
train() client id: f_00008-3-2 loss: 0.889390  [   96/  130]
train() client id: f_00008-3-3 loss: 0.840161  [  128/  130]
train() client id: f_00008-4-0 loss: 0.818020  [   32/  130]
train() client id: f_00008-4-1 loss: 0.733969  [   64/  130]
train() client id: f_00008-4-2 loss: 0.821351  [   96/  130]
train() client id: f_00008-4-3 loss: 0.797498  [  128/  130]
train() client id: f_00008-5-0 loss: 0.741635  [   32/  130]
train() client id: f_00008-5-1 loss: 0.854954  [   64/  130]
train() client id: f_00008-5-2 loss: 0.806250  [   96/  130]
train() client id: f_00008-5-3 loss: 0.800444  [  128/  130]
train() client id: f_00009-0-0 loss: 0.801113  [   32/  118]
train() client id: f_00009-0-1 loss: 1.046623  [   64/  118]
train() client id: f_00009-0-2 loss: 1.079484  [   96/  118]
train() client id: f_00009-1-0 loss: 1.020719  [   32/  118]
train() client id: f_00009-1-1 loss: 0.990212  [   64/  118]
train() client id: f_00009-1-2 loss: 0.832837  [   96/  118]
train() client id: f_00009-2-0 loss: 0.713670  [   32/  118]
train() client id: f_00009-2-1 loss: 1.065223  [   64/  118]
train() client id: f_00009-2-2 loss: 0.881543  [   96/  118]
train() client id: f_00009-3-0 loss: 0.916669  [   32/  118]
train() client id: f_00009-3-1 loss: 1.001153  [   64/  118]
train() client id: f_00009-3-2 loss: 0.845456  [   96/  118]
train() client id: f_00009-4-0 loss: 0.967219  [   32/  118]
train() client id: f_00009-4-1 loss: 0.960848  [   64/  118]
train() client id: f_00009-4-2 loss: 0.741927  [   96/  118]
train() client id: f_00009-5-0 loss: 0.969460  [   32/  118]
train() client id: f_00009-5-1 loss: 0.889430  [   64/  118]
train() client id: f_00009-5-2 loss: 0.783306  [   96/  118]
At round 71 accuracy: 0.6445623342175066
At round 71 training accuracy: 0.5928906773977196
At round 71 training loss: 0.812928532883475
update_location
xs = -3.905658 4.200318 375.009024 18.811294 0.979296 3.956410 -337.443192 -316.324852 359.663977 -302.060879 
ys = 367.587959 350.555839 1.320614 -337.455176 329.350187 312.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -19.71142616811743
ys mean: 104.3941425355287
dists_uav = 380.967403 364.564176 388.115333 352.462566 344.198351 328.433168 351.958518 331.756067 373.720278 318.208715 
uav_gains = -120.655909 -119.918760 -120.948643 -119.305386 -118.846546 -117.868432 -119.278378 -118.086514 -120.342280 -117.155495 
uav_gains_db_mean: -119.24063435838634
dists_bs = 257.610939 250.863721 576.779382 547.880576 234.078390 225.871948 240.704021 224.300174 557.359925 213.037462 
bs_gains = -107.074060 -106.751319 -116.875332 -116.250264 -105.909176 -105.475203 -106.248593 -105.390288 -116.458860 -104.763827 
bs_gains_db_mean: -109.11969218353318
Round 72
-------------------------------
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.45877326 2.87217542 1.44178771 0.55140047 3.31032656 1.59425308
 0.66902829 1.99714791 1.45909395 1.29300424]
obj_prev = 16.646990883599326
eta_min = 4.971520837874105e-65	eta_max = 0.9577477121135591
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 3.7800966926678474	eta = 0.9090909090909091
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 10.808661784694127	eta = 0.3179349680138224
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.733867507222867	eta = 0.5103236045412168
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.058721545799979	eta = 0.5671908690326205
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.015928897442357	eta = 0.5712254245973457
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.015728044345185	eta = 0.5712444966689002
eta = 0.5712444966689002
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [0.04620474 0.09717663 0.04547133 0.01576828 0.11221151 0.05353882
 0.01980203 0.06564007 0.04767156 0.04327111]
ene_total = [0.62840586 0.86936    0.63267678 0.33813836 0.98949538 0.50632297
 0.37054313 0.71901402 0.54216497 0.41960659]
ti_comp = [2.91272976 3.128895   2.90019242 2.96199725 3.13288317 3.13480384
 2.96285607 2.99669538 3.03246495 3.13777295]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [7.26673296e-07 5.85846172e-06 6.98617374e-07 2.79295552e-08
 8.99712661e-06 9.76035873e-07 5.52825571e-08 1.96834785e-06
 7.36319715e-07 5.14317079e-07]
ene_total = [0.24915505 0.07176489 0.2594457  0.20870969 0.06851708 0.06687473
 0.20800497 0.18024477 0.15087435 0.06443384]
optimize_network iter = 0 obj = 1.52802506642978
eta = 0.5712444966689002
freqs = [ 7931518.36924844 15528906.23802998  7839364.44612375  2661765.47781446
 17908664.85509679  8539421.8514933   3341713.30591816 10952075.17012927
  7860199.06137994  6895193.77630764]
eta_min = 0.5712444966689012	eta_max = 0.8252306516812821
af = 0.0001292173845627228	bf = 0.6550768027395515	zeta = 0.00014213912301899509	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [1.23691134e-07 9.97201600e-07 1.18915578e-07 4.75404610e-09
 1.53145134e-06 1.66136535e-07 9.40995383e-09 3.35043518e-07
 1.25333105e-07 8.75447921e-08]
ene_total = [1.17506927 0.33827729 1.22360405 0.98433881 0.32285882 0.31537062
 0.98101428 0.85002707 0.71154662 0.30387348]
ti_comp = [1.00747753 1.22364276 0.99494019 1.05674501 1.22763094 1.22955161
 1.05760384 1.09144314 1.12721272 1.23252071]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.45233236e-07 1.54655995e-06 2.39667719e-07 8.85938112e-09
 2.36574025e-06 2.56155376e-07 1.75175625e-08 5.99094442e-07
 2.15157618e-07 1.34584796e-07]
ene_total = [0.61123365 0.17597144 0.63647978 0.51201976 0.16795702 0.16404692
 0.51029054 0.44216059 0.37012424 0.15806563]
optimize_network iter = 1 obj = 3.7483495686155184
eta = 0.8252306516812821
freqs = [ 7866677.00292003 13622203.68026247  7839364.44612375  2559495.62396928
 15678688.91426082  7468996.16234366  3211640.19547591 10315922.15655571
  7254268.29694454  6022043.97636097]
eta_min = 0.8252306516812807	eta_max = 0.8252306516812821
af = 0.00010312038831994186	bf = 0.6550768027395515	zeta = 0.00011343242715193606	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [1.21677013e-07 7.67354369e-07 1.18915578e-07 4.39574606e-09
 1.17380585e-06 1.27096235e-07 8.69166314e-09 2.97251805e-07
 1.06754438e-07 6.67767395e-08]
ene_total = [1.17506919 0.33826839 1.22360405 0.98433879 0.32284498 0.31536911
 0.98101425 0.85002561 0.7115459  0.30387267]
ti_comp = [1.00747753 1.22364276 0.99494019 1.05674501 1.22763094 1.22955161
 1.05760384 1.09144314 1.12721272 1.23252071]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.45233236e-07 1.54655995e-06 2.39667719e-07 8.85938112e-09
 2.36574025e-06 2.56155376e-07 1.75175625e-08 5.99094442e-07
 2.15157618e-07 1.34584796e-07]
ene_total = [0.61123365 0.17597144 0.63647978 0.51201976 0.16795702 0.16404692
 0.51029054 0.44216059 0.37012424 0.15806563]
optimize_network iter = 2 obj = 3.7483495686155184
eta = 0.8252306516812821
freqs = [ 7866677.00292003 13622203.68026247  7839364.44612375  2559495.62396928
 15678688.91426082  7468996.16234366  3211640.19547591 10315922.15655571
  7254268.29694454  6022043.97636097]
Done!
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.33923815e-07 1.47523724e-06 2.28614962e-07 8.45081303e-09
 2.25663941e-06 2.44342258e-07 1.67097050e-08 5.71466002e-07
 2.05235193e-07 1.28378148e-07]
ene_total = [0.03035399 0.00873871 0.03160772 0.02542702 0.00834068 0.0081466
 0.02534115 0.02195777 0.01838045 0.00784957]
At round 72 energy consumption: 0.18614365042570813
At round 72 eta: 0.8252306516812821
At round 72 a_n: 3.5193018744635296
At round 72 local rounds: 6.290079613751617
At round 72 global rounds: 20.136836970093626
gradient difference: 0.7855410575866699
train() client id: f_00000-0-0 loss: 1.175189  [   32/  126]
train() client id: f_00000-0-1 loss: 1.055466  [   64/  126]
train() client id: f_00000-0-2 loss: 0.916618  [   96/  126]
train() client id: f_00000-1-0 loss: 1.023698  [   32/  126]
train() client id: f_00000-1-1 loss: 1.171849  [   64/  126]
train() client id: f_00000-1-2 loss: 0.979038  [   96/  126]
train() client id: f_00000-2-0 loss: 1.170452  [   32/  126]
train() client id: f_00000-2-1 loss: 0.942182  [   64/  126]
train() client id: f_00000-2-2 loss: 0.877341  [   96/  126]
train() client id: f_00000-3-0 loss: 0.868785  [   32/  126]
train() client id: f_00000-3-1 loss: 0.850901  [   64/  126]
train() client id: f_00000-3-2 loss: 1.011879  [   96/  126]
train() client id: f_00000-4-0 loss: 0.986039  [   32/  126]
train() client id: f_00000-4-1 loss: 0.894889  [   64/  126]
train() client id: f_00000-4-2 loss: 0.904120  [   96/  126]
train() client id: f_00000-5-0 loss: 0.850265  [   32/  126]
train() client id: f_00000-5-1 loss: 1.004514  [   64/  126]
train() client id: f_00000-5-2 loss: 0.735081  [   96/  126]
train() client id: f_00001-0-0 loss: 0.385458  [   32/  265]
train() client id: f_00001-0-1 loss: 0.369172  [   64/  265]
train() client id: f_00001-0-2 loss: 0.435691  [   96/  265]
train() client id: f_00001-0-3 loss: 0.418551  [  128/  265]
train() client id: f_00001-0-4 loss: 0.476371  [  160/  265]
train() client id: f_00001-0-5 loss: 0.311883  [  192/  265]
train() client id: f_00001-0-6 loss: 0.384959  [  224/  265]
train() client id: f_00001-0-7 loss: 0.320888  [  256/  265]
train() client id: f_00001-1-0 loss: 0.369515  [   32/  265]
train() client id: f_00001-1-1 loss: 0.553419  [   64/  265]
train() client id: f_00001-1-2 loss: 0.418971  [   96/  265]
train() client id: f_00001-1-3 loss: 0.395384  [  128/  265]
train() client id: f_00001-1-4 loss: 0.375611  [  160/  265]
train() client id: f_00001-1-5 loss: 0.307471  [  192/  265]
train() client id: f_00001-1-6 loss: 0.262034  [  224/  265]
train() client id: f_00001-1-7 loss: 0.374489  [  256/  265]
train() client id: f_00001-2-0 loss: 0.298691  [   32/  265]
train() client id: f_00001-2-1 loss: 0.501824  [   64/  265]
train() client id: f_00001-2-2 loss: 0.386879  [   96/  265]
train() client id: f_00001-2-3 loss: 0.371558  [  128/  265]
train() client id: f_00001-2-4 loss: 0.377663  [  160/  265]
train() client id: f_00001-2-5 loss: 0.455092  [  192/  265]
train() client id: f_00001-2-6 loss: 0.313443  [  224/  265]
train() client id: f_00001-2-7 loss: 0.303505  [  256/  265]
train() client id: f_00001-3-0 loss: 0.301472  [   32/  265]
train() client id: f_00001-3-1 loss: 0.308196  [   64/  265]
train() client id: f_00001-3-2 loss: 0.341167  [   96/  265]
train() client id: f_00001-3-3 loss: 0.446480  [  128/  265]
train() client id: f_00001-3-4 loss: 0.282764  [  160/  265]
train() client id: f_00001-3-5 loss: 0.332836  [  192/  265]
train() client id: f_00001-3-6 loss: 0.568354  [  224/  265]
train() client id: f_00001-3-7 loss: 0.421206  [  256/  265]
train() client id: f_00001-4-0 loss: 0.443923  [   32/  265]
train() client id: f_00001-4-1 loss: 0.298591  [   64/  265]
train() client id: f_00001-4-2 loss: 0.462105  [   96/  265]
train() client id: f_00001-4-3 loss: 0.336660  [  128/  265]
train() client id: f_00001-4-4 loss: 0.325968  [  160/  265]
train() client id: f_00001-4-5 loss: 0.331246  [  192/  265]
train() client id: f_00001-4-6 loss: 0.413277  [  224/  265]
train() client id: f_00001-4-7 loss: 0.333477  [  256/  265]
train() client id: f_00001-5-0 loss: 0.317697  [   32/  265]
train() client id: f_00001-5-1 loss: 0.263701  [   64/  265]
train() client id: f_00001-5-2 loss: 0.516404  [   96/  265]
train() client id: f_00001-5-3 loss: 0.341105  [  128/  265]
train() client id: f_00001-5-4 loss: 0.492403  [  160/  265]
train() client id: f_00001-5-5 loss: 0.325892  [  192/  265]
train() client id: f_00001-5-6 loss: 0.427448  [  224/  265]
train() client id: f_00001-5-7 loss: 0.272753  [  256/  265]
train() client id: f_00002-0-0 loss: 1.091482  [   32/  124]
train() client id: f_00002-0-1 loss: 1.151218  [   64/  124]
train() client id: f_00002-0-2 loss: 1.059371  [   96/  124]
train() client id: f_00002-1-0 loss: 0.959723  [   32/  124]
train() client id: f_00002-1-1 loss: 1.068581  [   64/  124]
train() client id: f_00002-1-2 loss: 1.071294  [   96/  124]
train() client id: f_00002-2-0 loss: 0.940444  [   32/  124]
train() client id: f_00002-2-1 loss: 1.124289  [   64/  124]
train() client id: f_00002-2-2 loss: 0.989369  [   96/  124]
train() client id: f_00002-3-0 loss: 0.967321  [   32/  124]
train() client id: f_00002-3-1 loss: 1.090538  [   64/  124]
train() client id: f_00002-3-2 loss: 1.171865  [   96/  124]
train() client id: f_00002-4-0 loss: 0.886009  [   32/  124]
train() client id: f_00002-4-1 loss: 0.906428  [   64/  124]
train() client id: f_00002-4-2 loss: 1.163631  [   96/  124]
train() client id: f_00002-5-0 loss: 0.994762  [   32/  124]
train() client id: f_00002-5-1 loss: 1.034679  [   64/  124]
train() client id: f_00002-5-2 loss: 1.239945  [   96/  124]
train() client id: f_00003-0-0 loss: 0.510528  [   32/   43]
train() client id: f_00003-1-0 loss: 0.779887  [   32/   43]
train() client id: f_00003-2-0 loss: 0.535245  [   32/   43]
train() client id: f_00003-3-0 loss: 0.303160  [   32/   43]
train() client id: f_00003-4-0 loss: 0.482065  [   32/   43]
train() client id: f_00003-5-0 loss: 0.599514  [   32/   43]
train() client id: f_00004-0-0 loss: 0.683868  [   32/  306]
train() client id: f_00004-0-1 loss: 0.603921  [   64/  306]
train() client id: f_00004-0-2 loss: 0.545027  [   96/  306]
train() client id: f_00004-0-3 loss: 0.704748  [  128/  306]
train() client id: f_00004-0-4 loss: 0.457712  [  160/  306]
train() client id: f_00004-0-5 loss: 0.775769  [  192/  306]
train() client id: f_00004-0-6 loss: 0.684922  [  224/  306]
train() client id: f_00004-0-7 loss: 0.823825  [  256/  306]
train() client id: f_00004-0-8 loss: 0.557466  [  288/  306]
train() client id: f_00004-1-0 loss: 0.743637  [   32/  306]
train() client id: f_00004-1-1 loss: 0.673805  [   64/  306]
train() client id: f_00004-1-2 loss: 0.636741  [   96/  306]
train() client id: f_00004-1-3 loss: 0.669731  [  128/  306]
train() client id: f_00004-1-4 loss: 0.588806  [  160/  306]
train() client id: f_00004-1-5 loss: 0.607749  [  192/  306]
train() client id: f_00004-1-6 loss: 0.745144  [  224/  306]
train() client id: f_00004-1-7 loss: 0.652706  [  256/  306]
train() client id: f_00004-1-8 loss: 0.752102  [  288/  306]
train() client id: f_00004-2-0 loss: 0.509629  [   32/  306]
train() client id: f_00004-2-1 loss: 0.666852  [   64/  306]
train() client id: f_00004-2-2 loss: 0.585244  [   96/  306]
train() client id: f_00004-2-3 loss: 0.602070  [  128/  306]
train() client id: f_00004-2-4 loss: 0.783828  [  160/  306]
train() client id: f_00004-2-5 loss: 0.702269  [  192/  306]
train() client id: f_00004-2-6 loss: 0.790534  [  224/  306]
train() client id: f_00004-2-7 loss: 0.727357  [  256/  306]
train() client id: f_00004-2-8 loss: 0.611429  [  288/  306]
train() client id: f_00004-3-0 loss: 0.563565  [   32/  306]
train() client id: f_00004-3-1 loss: 0.691845  [   64/  306]
train() client id: f_00004-3-2 loss: 0.622013  [   96/  306]
train() client id: f_00004-3-3 loss: 0.581638  [  128/  306]
train() client id: f_00004-3-4 loss: 0.711757  [  160/  306]
train() client id: f_00004-3-5 loss: 0.796490  [  192/  306]
train() client id: f_00004-3-6 loss: 0.681962  [  224/  306]
train() client id: f_00004-3-7 loss: 0.645841  [  256/  306]
train() client id: f_00004-3-8 loss: 0.717358  [  288/  306]
train() client id: f_00004-4-0 loss: 0.665404  [   32/  306]
train() client id: f_00004-4-1 loss: 0.621543  [   64/  306]
train() client id: f_00004-4-2 loss: 0.652513  [   96/  306]
train() client id: f_00004-4-3 loss: 0.605187  [  128/  306]
train() client id: f_00004-4-4 loss: 0.656480  [  160/  306]
train() client id: f_00004-4-5 loss: 0.809522  [  192/  306]
train() client id: f_00004-4-6 loss: 0.700917  [  224/  306]
train() client id: f_00004-4-7 loss: 0.705083  [  256/  306]
train() client id: f_00004-4-8 loss: 0.749891  [  288/  306]
train() client id: f_00004-5-0 loss: 0.765801  [   32/  306]
train() client id: f_00004-5-1 loss: 0.614785  [   64/  306]
train() client id: f_00004-5-2 loss: 0.670700  [   96/  306]
train() client id: f_00004-5-3 loss: 0.542017  [  128/  306]
train() client id: f_00004-5-4 loss: 0.705948  [  160/  306]
train() client id: f_00004-5-5 loss: 0.729690  [  192/  306]
train() client id: f_00004-5-6 loss: 0.652660  [  224/  306]
train() client id: f_00004-5-7 loss: 0.756721  [  256/  306]
train() client id: f_00004-5-8 loss: 0.676695  [  288/  306]
train() client id: f_00005-0-0 loss: 0.801747  [   32/  146]
train() client id: f_00005-0-1 loss: 0.518248  [   64/  146]
train() client id: f_00005-0-2 loss: 0.793319  [   96/  146]
train() client id: f_00005-0-3 loss: 0.696774  [  128/  146]
train() client id: f_00005-1-0 loss: 0.608681  [   32/  146]
train() client id: f_00005-1-1 loss: 1.050835  [   64/  146]
train() client id: f_00005-1-2 loss: 0.428604  [   96/  146]
train() client id: f_00005-1-3 loss: 0.709156  [  128/  146]
train() client id: f_00005-2-0 loss: 0.816166  [   32/  146]
train() client id: f_00005-2-1 loss: 0.640676  [   64/  146]
train() client id: f_00005-2-2 loss: 0.694193  [   96/  146]
train() client id: f_00005-2-3 loss: 0.528413  [  128/  146]
train() client id: f_00005-3-0 loss: 0.741765  [   32/  146]
train() client id: f_00005-3-1 loss: 0.635915  [   64/  146]
train() client id: f_00005-3-2 loss: 0.442801  [   96/  146]
train() client id: f_00005-3-3 loss: 0.946893  [  128/  146]
train() client id: f_00005-4-0 loss: 0.712340  [   32/  146]
train() client id: f_00005-4-1 loss: 0.710855  [   64/  146]
train() client id: f_00005-4-2 loss: 0.705675  [   96/  146]
train() client id: f_00005-4-3 loss: 0.492186  [  128/  146]
train() client id: f_00005-5-0 loss: 0.829968  [   32/  146]
train() client id: f_00005-5-1 loss: 0.574861  [   64/  146]
train() client id: f_00005-5-2 loss: 0.666180  [   96/  146]
train() client id: f_00005-5-3 loss: 0.742428  [  128/  146]
train() client id: f_00006-0-0 loss: 0.533307  [   32/   54]
train() client id: f_00006-1-0 loss: 0.524466  [   32/   54]
train() client id: f_00006-2-0 loss: 0.503791  [   32/   54]
train() client id: f_00006-3-0 loss: 0.521818  [   32/   54]
train() client id: f_00006-4-0 loss: 0.540213  [   32/   54]
train() client id: f_00006-5-0 loss: 0.526370  [   32/   54]
train() client id: f_00007-0-0 loss: 0.382813  [   32/  179]
train() client id: f_00007-0-1 loss: 0.517691  [   64/  179]
train() client id: f_00007-0-2 loss: 0.388182  [   96/  179]
train() client id: f_00007-0-3 loss: 0.543599  [  128/  179]
train() client id: f_00007-0-4 loss: 0.248735  [  160/  179]
train() client id: f_00007-1-0 loss: 0.387450  [   32/  179]
train() client id: f_00007-1-1 loss: 0.366575  [   64/  179]
train() client id: f_00007-1-2 loss: 0.229013  [   96/  179]
train() client id: f_00007-1-3 loss: 0.318410  [  128/  179]
train() client id: f_00007-1-4 loss: 0.705640  [  160/  179]
train() client id: f_00007-2-0 loss: 0.483797  [   32/  179]
train() client id: f_00007-2-1 loss: 0.419449  [   64/  179]
train() client id: f_00007-2-2 loss: 0.376221  [   96/  179]
train() client id: f_00007-2-3 loss: 0.429826  [  128/  179]
train() client id: f_00007-2-4 loss: 0.262823  [  160/  179]
train() client id: f_00007-3-0 loss: 0.227528  [   32/  179]
train() client id: f_00007-3-1 loss: 0.370828  [   64/  179]
train() client id: f_00007-3-2 loss: 0.292929  [   96/  179]
train() client id: f_00007-3-3 loss: 0.682020  [  128/  179]
train() client id: f_00007-3-4 loss: 0.403604  [  160/  179]
train() client id: f_00007-4-0 loss: 0.547444  [   32/  179]
train() client id: f_00007-4-1 loss: 0.495843  [   64/  179]
train() client id: f_00007-4-2 loss: 0.308788  [   96/  179]
train() client id: f_00007-4-3 loss: 0.436908  [  128/  179]
train() client id: f_00007-4-4 loss: 0.227426  [  160/  179]
train() client id: f_00007-5-0 loss: 0.611697  [   32/  179]
train() client id: f_00007-5-1 loss: 0.170348  [   64/  179]
train() client id: f_00007-5-2 loss: 0.358749  [   96/  179]
train() client id: f_00007-5-3 loss: 0.260578  [  128/  179]
train() client id: f_00007-5-4 loss: 0.456139  [  160/  179]
train() client id: f_00008-0-0 loss: 0.656793  [   32/  130]
train() client id: f_00008-0-1 loss: 0.618387  [   64/  130]
train() client id: f_00008-0-2 loss: 0.718678  [   96/  130]
train() client id: f_00008-0-3 loss: 0.739464  [  128/  130]
train() client id: f_00008-1-0 loss: 0.618006  [   32/  130]
train() client id: f_00008-1-1 loss: 0.756500  [   64/  130]
train() client id: f_00008-1-2 loss: 0.758105  [   96/  130]
train() client id: f_00008-1-3 loss: 0.636733  [  128/  130]
train() client id: f_00008-2-0 loss: 0.609712  [   32/  130]
train() client id: f_00008-2-1 loss: 0.633952  [   64/  130]
train() client id: f_00008-2-2 loss: 0.775213  [   96/  130]
train() client id: f_00008-2-3 loss: 0.730066  [  128/  130]
train() client id: f_00008-3-0 loss: 0.725108  [   32/  130]
train() client id: f_00008-3-1 loss: 0.666116  [   64/  130]
train() client id: f_00008-3-2 loss: 0.686968  [   96/  130]
train() client id: f_00008-3-3 loss: 0.694003  [  128/  130]
train() client id: f_00008-4-0 loss: 0.786681  [   32/  130]
train() client id: f_00008-4-1 loss: 0.633495  [   64/  130]
train() client id: f_00008-4-2 loss: 0.765686  [   96/  130]
train() client id: f_00008-4-3 loss: 0.581215  [  128/  130]
train() client id: f_00008-5-0 loss: 0.628002  [   32/  130]
train() client id: f_00008-5-1 loss: 0.706969  [   64/  130]
train() client id: f_00008-5-2 loss: 0.683605  [   96/  130]
train() client id: f_00008-5-3 loss: 0.757279  [  128/  130]
train() client id: f_00009-0-0 loss: 0.893118  [   32/  118]
train() client id: f_00009-0-1 loss: 1.091306  [   64/  118]
train() client id: f_00009-0-2 loss: 0.796951  [   96/  118]
train() client id: f_00009-1-0 loss: 0.811088  [   32/  118]
train() client id: f_00009-1-1 loss: 1.058524  [   64/  118]
train() client id: f_00009-1-2 loss: 0.954682  [   96/  118]
train() client id: f_00009-2-0 loss: 0.826274  [   32/  118]
train() client id: f_00009-2-1 loss: 0.847302  [   64/  118]
train() client id: f_00009-2-2 loss: 0.965762  [   96/  118]
train() client id: f_00009-3-0 loss: 0.995885  [   32/  118]
train() client id: f_00009-3-1 loss: 0.737422  [   64/  118]
train() client id: f_00009-3-2 loss: 0.849327  [   96/  118]
train() client id: f_00009-4-0 loss: 0.883892  [   32/  118]
train() client id: f_00009-4-1 loss: 0.894097  [   64/  118]
train() client id: f_00009-4-2 loss: 0.880230  [   96/  118]
train() client id: f_00009-5-0 loss: 0.922393  [   32/  118]
train() client id: f_00009-5-1 loss: 0.906860  [   64/  118]
train() client id: f_00009-5-2 loss: 0.875871  [   96/  118]
At round 72 accuracy: 0.6445623342175066
At round 72 training accuracy: 0.5861837692823608
At round 72 training loss: 0.8306241286814001
update_location
xs = -3.905658 4.200318 380.009024 18.811294 0.979296 3.956410 -342.443192 -321.324852 364.663977 -307.060879 
ys = 372.587959 355.555839 1.320614 -342.455176 334.350187 317.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -20.21142616811743
ys mean: 105.8941425355287
dists_uav = 385.794041 369.374603 392.948600 357.252589 348.985682 333.198871 356.755140 336.526874 378.534656 322.958813 
uav_gains = -120.855277 -120.145285 -121.138113 -119.556032 -119.116553 -118.179170 -119.530498 -118.388275 -120.552611 -117.494651 
uav_gains_db_mean: -119.49564667402132
dists_bs = 261.369997 254.387824 581.549263 552.560112 237.405128 228.956935 244.106242 227.483663 562.158097 216.056865 
bs_gains = -107.250220 -106.920956 -116.975481 -116.353686 -106.080782 -105.640166 -106.419268 -105.561665 -116.563096 -104.934965 
bs_gains_db_mean: -109.27002851178204
Round 73
-------------------------------
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.3194998  2.59288881 1.30418738 0.50035792 2.98837254 1.43930991
 0.6065387  1.80528297 1.31771582 1.16737463]
obj_prev = 15.041528473465261
eta_min = 6.730086322824428e-72	eta_max = 0.960768447983313
af = 3.101969802094253	bf = 0.606014338290774	zeta = 3.4121667823036788	eta = 0.9090909090909091
af = 3.101969802094253	bf = 0.606014338290774	zeta = 9.918300016635595	eta = 0.3127521648761819
af = 3.101969802094253	bf = 0.606014338290774	zeta = 6.128169132936568	eta = 0.5061821458912337
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.504089108696306	eta = 0.5635755055624424
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.464521898505789	eta = 0.5676562121459247
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.464335618649047	eta = 0.5676755636142928
eta = 0.5676755636142928
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [0.0467219  0.09826431 0.04598028 0.01594477 0.11346746 0.05413807
 0.02002367 0.06637476 0.04820513 0.04375543]
ene_total = [0.57257327 0.78687954 0.57638183 0.31001941 0.89561579 0.45820012
 0.33936518 0.65485131 0.49074166 0.3797075 ]
ti_comp = [3.27393193 3.49770683 3.26133269 3.32347001 3.50175933 3.50374398
 3.32432233 3.35848843 3.40012884 3.50673809]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [5.94705746e-07 4.84730784e-06 5.71221622e-07 2.29377929e-08
 7.44597339e-06 8.07836791e-07 4.54051187e-08 1.62032140e-06
 6.05576618e-07 4.25764209e-07]
ene_total = [0.22926124 0.06486083 0.2385191  0.19285604 0.06190211 0.060395
 0.19222992 0.16713595 0.13653078 0.05819209]
optimize_network iter = 0 obj = 1.401883055865988
eta = 0.5676755636142928
freqs = [ 7135441.14428652 14046961.66554712  7049308.38877608  2398814.24536511
 16201493.85610097  7725745.89318307  3011692.04068683  9881641.69424418
  7088721.37557428  6238764.68717198]
eta_min = 0.5676755636142934	eta_max = 0.8387833537635655
af = 9.530769728321942e-05	bf = 0.606014338290774	zeta = 0.00010483846701154136	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.00107717e-07 8.15954657e-07 9.61545994e-08 3.86115336e-09
 1.25339196e-06 1.35984388e-07 7.64311227e-09 2.72751152e-07
 1.01937628e-07 7.16695331e-08]
ene_total = [1.09025033 0.30830925 1.13427738 0.91713951 0.29416335 0.28718908
 0.91416128 0.79477943 0.64926391 0.27672413]
ti_comp = [1.02522061 1.24899552 1.01262137 1.0747587  1.25304801 1.25503266
 1.07561102 1.10977712 1.15141753 1.25802677]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.81535984e-07 1.13789349e-06 1.77360386e-07 6.56552073e-09
 1.74066261e-06 1.88466540e-07 1.29824466e-08 4.44194635e-07
 1.58070248e-07 9.90266306e-08]
ene_total = [0.61478717 0.17386006 0.63961375 0.51716947 0.16588653 0.16194522
 0.51549012 0.44817469 0.36611709 0.1560436 ]
optimize_network iter = 1 obj = 3.7590876976936305
eta = 0.8387833537635655
freqs = [ 7074978.59491438 12213971.70314103  7049308.38877607  2303188.06336389
 14058067.6277585   6696837.19701407  2890083.78088009  9285155.45638792
  6499537.15984087  5399630.45377395]
eta_min = 0.8387833537635758	eta_max = 0.8387833537635625
af = 7.4931901688053e-05	bf = 0.606014338290774	zeta = 8.242509185685831e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [9.84183688e-08 6.16900397e-07 9.61545994e-08 3.55944769e-09
 9.43687147e-07 1.02175717e-07 7.03833579e-09 2.40816781e-07
 8.56965960e-08 5.36865431e-08]
ene_total = [1.09025027 0.30830229 1.13427738 0.9171395  0.29415253 0.2871879
 0.91416126 0.79477832 0.64926334 0.2767235 ]
ti_comp = [1.02522061 1.24899552 1.01262137 1.0747587  1.25304801 1.25503266
 1.07561102 1.10977712 1.15141753 1.25802677]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.81535984e-07 1.13789349e-06 1.77360386e-07 6.56552073e-09
 1.74066261e-06 1.88466540e-07 1.29824466e-08 4.44194635e-07
 1.58070248e-07 9.90266306e-08]
ene_total = [0.61478717 0.17386006 0.63961375 0.51716947 0.16588653 0.16194522
 0.51549012 0.44817469 0.36611709 0.1560436 ]
optimize_network iter = 2 obj = 3.759087697693561
eta = 0.8387833537635625
freqs = [ 7074978.59491436 12213971.70314105  7049308.38877605  2303188.06336388
 14058067.62775852  6696837.19701408  2890083.78088009  9285155.45638791
  6499537.15984087  5399630.45377396]
Done!
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.57674265e-07 9.88324819e-07 1.54047521e-07 5.70252590e-09
 1.51186388e-06 1.63693844e-07 1.12759888e-08 3.85808151e-07
 1.37292946e-07 8.60102267e-08]
ene_total = [0.0311997  0.00882304 0.03245962 0.02624574 0.00841832 0.00821851
 0.02616052 0.02274428 0.01857999 0.00791902]
At round 73 energy consumption: 0.19076875112517003
At round 73 eta: 0.8387833537635625
At round 73 a_n: 3.176756027494214
At round 73 local rounds: 5.756677690695127
At round 73 global rounds: 19.70488843214887
gradient difference: 0.6721576452255249
train() client id: f_00000-0-0 loss: 1.222739  [   32/  126]
train() client id: f_00000-0-1 loss: 1.130498  [   64/  126]
train() client id: f_00000-0-2 loss: 1.076638  [   96/  126]
train() client id: f_00000-1-0 loss: 1.319849  [   32/  126]
train() client id: f_00000-1-1 loss: 1.030199  [   64/  126]
train() client id: f_00000-1-2 loss: 1.013697  [   96/  126]
train() client id: f_00000-2-0 loss: 0.939980  [   32/  126]
train() client id: f_00000-2-1 loss: 0.938782  [   64/  126]
train() client id: f_00000-2-2 loss: 1.117040  [   96/  126]
train() client id: f_00000-3-0 loss: 1.052738  [   32/  126]
train() client id: f_00000-3-1 loss: 1.028665  [   64/  126]
train() client id: f_00000-3-2 loss: 1.006953  [   96/  126]
train() client id: f_00000-4-0 loss: 1.075292  [   32/  126]
train() client id: f_00000-4-1 loss: 1.028562  [   64/  126]
train() client id: f_00000-4-2 loss: 0.981164  [   96/  126]
train() client id: f_00001-0-0 loss: 0.705622  [   32/  265]
train() client id: f_00001-0-1 loss: 0.559154  [   64/  265]
train() client id: f_00001-0-2 loss: 0.543006  [   96/  265]
train() client id: f_00001-0-3 loss: 0.503929  [  128/  265]
train() client id: f_00001-0-4 loss: 0.614224  [  160/  265]
train() client id: f_00001-0-5 loss: 0.539399  [  192/  265]
train() client id: f_00001-0-6 loss: 0.471843  [  224/  265]
train() client id: f_00001-0-7 loss: 0.583934  [  256/  265]
train() client id: f_00001-1-0 loss: 0.549020  [   32/  265]
train() client id: f_00001-1-1 loss: 0.481171  [   64/  265]
train() client id: f_00001-1-2 loss: 0.484210  [   96/  265]
train() client id: f_00001-1-3 loss: 0.497720  [  128/  265]
train() client id: f_00001-1-4 loss: 0.472253  [  160/  265]
train() client id: f_00001-1-5 loss: 0.676155  [  192/  265]
train() client id: f_00001-1-6 loss: 0.569230  [  224/  265]
train() client id: f_00001-1-7 loss: 0.724933  [  256/  265]
train() client id: f_00001-2-0 loss: 0.487205  [   32/  265]
train() client id: f_00001-2-1 loss: 0.527123  [   64/  265]
train() client id: f_00001-2-2 loss: 0.630314  [   96/  265]
train() client id: f_00001-2-3 loss: 0.559582  [  128/  265]
train() client id: f_00001-2-4 loss: 0.484552  [  160/  265]
train() client id: f_00001-2-5 loss: 0.565342  [  192/  265]
train() client id: f_00001-2-6 loss: 0.637439  [  224/  265]
train() client id: f_00001-2-7 loss: 0.616778  [  256/  265]
train() client id: f_00001-3-0 loss: 0.662726  [   32/  265]
train() client id: f_00001-3-1 loss: 0.559380  [   64/  265]
train() client id: f_00001-3-2 loss: 0.627809  [   96/  265]
train() client id: f_00001-3-3 loss: 0.463645  [  128/  265]
train() client id: f_00001-3-4 loss: 0.489996  [  160/  265]
train() client id: f_00001-3-5 loss: 0.526705  [  192/  265]
train() client id: f_00001-3-6 loss: 0.528513  [  224/  265]
train() client id: f_00001-3-7 loss: 0.560257  [  256/  265]
train() client id: f_00001-4-0 loss: 0.568851  [   32/  265]
train() client id: f_00001-4-1 loss: 0.507019  [   64/  265]
train() client id: f_00001-4-2 loss: 0.469920  [   96/  265]
train() client id: f_00001-4-3 loss: 0.530012  [  128/  265]
train() client id: f_00001-4-4 loss: 0.593030  [  160/  265]
train() client id: f_00001-4-5 loss: 0.671015  [  192/  265]
train() client id: f_00001-4-6 loss: 0.590586  [  224/  265]
train() client id: f_00001-4-7 loss: 0.478757  [  256/  265]
train() client id: f_00002-0-0 loss: 0.896648  [   32/  124]
train() client id: f_00002-0-1 loss: 1.064033  [   64/  124]
train() client id: f_00002-0-2 loss: 0.529379  [   96/  124]
train() client id: f_00002-1-0 loss: 0.892945  [   32/  124]
train() client id: f_00002-1-1 loss: 0.688154  [   64/  124]
train() client id: f_00002-1-2 loss: 0.830837  [   96/  124]
train() client id: f_00002-2-0 loss: 0.774305  [   32/  124]
train() client id: f_00002-2-1 loss: 0.788605  [   64/  124]
train() client id: f_00002-2-2 loss: 0.662455  [   96/  124]
train() client id: f_00002-3-0 loss: 0.776102  [   32/  124]
train() client id: f_00002-3-1 loss: 0.640200  [   64/  124]
train() client id: f_00002-3-2 loss: 0.738789  [   96/  124]
train() client id: f_00002-4-0 loss: 0.557203  [   32/  124]
train() client id: f_00002-4-1 loss: 0.689715  [   64/  124]
train() client id: f_00002-4-2 loss: 0.710066  [   96/  124]
train() client id: f_00003-0-0 loss: 0.716809  [   32/   43]
train() client id: f_00003-1-0 loss: 0.771468  [   32/   43]
train() client id: f_00003-2-0 loss: 0.596707  [   32/   43]
train() client id: f_00003-3-0 loss: 0.644604  [   32/   43]
train() client id: f_00003-4-0 loss: 0.423076  [   32/   43]
train() client id: f_00004-0-0 loss: 0.898271  [   32/  306]
train() client id: f_00004-0-1 loss: 0.843877  [   64/  306]
train() client id: f_00004-0-2 loss: 0.794661  [   96/  306]
train() client id: f_00004-0-3 loss: 0.842590  [  128/  306]
train() client id: f_00004-0-4 loss: 0.831525  [  160/  306]
train() client id: f_00004-0-5 loss: 0.819340  [  192/  306]
train() client id: f_00004-0-6 loss: 0.783765  [  224/  306]
train() client id: f_00004-0-7 loss: 0.741453  [  256/  306]
train() client id: f_00004-0-8 loss: 0.955303  [  288/  306]
train() client id: f_00004-1-0 loss: 0.801106  [   32/  306]
train() client id: f_00004-1-1 loss: 0.720184  [   64/  306]
train() client id: f_00004-1-2 loss: 0.794208  [   96/  306]
train() client id: f_00004-1-3 loss: 0.956056  [  128/  306]
train() client id: f_00004-1-4 loss: 0.892373  [  160/  306]
train() client id: f_00004-1-5 loss: 0.982648  [  192/  306]
train() client id: f_00004-1-6 loss: 0.741165  [  224/  306]
train() client id: f_00004-1-7 loss: 0.753597  [  256/  306]
train() client id: f_00004-1-8 loss: 0.886591  [  288/  306]
train() client id: f_00004-2-0 loss: 0.943856  [   32/  306]
train() client id: f_00004-2-1 loss: 0.993517  [   64/  306]
train() client id: f_00004-2-2 loss: 0.790114  [   96/  306]
train() client id: f_00004-2-3 loss: 0.797854  [  128/  306]
train() client id: f_00004-2-4 loss: 0.789691  [  160/  306]
train() client id: f_00004-2-5 loss: 0.922085  [  192/  306]
train() client id: f_00004-2-6 loss: 0.767806  [  224/  306]
train() client id: f_00004-2-7 loss: 0.771411  [  256/  306]
train() client id: f_00004-2-8 loss: 0.734447  [  288/  306]
train() client id: f_00004-3-0 loss: 0.831770  [   32/  306]
train() client id: f_00004-3-1 loss: 0.838840  [   64/  306]
train() client id: f_00004-3-2 loss: 0.740465  [   96/  306]
train() client id: f_00004-3-3 loss: 0.902048  [  128/  306]
train() client id: f_00004-3-4 loss: 0.925714  [  160/  306]
train() client id: f_00004-3-5 loss: 0.863146  [  192/  306]
train() client id: f_00004-3-6 loss: 0.731894  [  224/  306]
train() client id: f_00004-3-7 loss: 0.761073  [  256/  306]
train() client id: f_00004-3-8 loss: 0.943326  [  288/  306]
train() client id: f_00004-4-0 loss: 0.871971  [   32/  306]
train() client id: f_00004-4-1 loss: 0.790385  [   64/  306]
train() client id: f_00004-4-2 loss: 0.901293  [   96/  306]
train() client id: f_00004-4-3 loss: 0.942934  [  128/  306]
train() client id: f_00004-4-4 loss: 0.758040  [  160/  306]
train() client id: f_00004-4-5 loss: 0.742246  [  192/  306]
train() client id: f_00004-4-6 loss: 0.770314  [  224/  306]
train() client id: f_00004-4-7 loss: 0.838314  [  256/  306]
train() client id: f_00004-4-8 loss: 0.695022  [  288/  306]
train() client id: f_00005-0-0 loss: 0.883970  [   32/  146]
train() client id: f_00005-0-1 loss: 0.578345  [   64/  146]
train() client id: f_00005-0-2 loss: 0.359482  [   96/  146]
train() client id: f_00005-0-3 loss: 0.594337  [  128/  146]
train() client id: f_00005-1-0 loss: 0.855653  [   32/  146]
train() client id: f_00005-1-1 loss: 0.487279  [   64/  146]
train() client id: f_00005-1-2 loss: 0.684471  [   96/  146]
train() client id: f_00005-1-3 loss: 0.416442  [  128/  146]
train() client id: f_00005-2-0 loss: 0.548118  [   32/  146]
train() client id: f_00005-2-1 loss: 0.673405  [   64/  146]
train() client id: f_00005-2-2 loss: 0.556979  [   96/  146]
train() client id: f_00005-2-3 loss: 0.742589  [  128/  146]
train() client id: f_00005-3-0 loss: 0.476722  [   32/  146]
train() client id: f_00005-3-1 loss: 0.788859  [   64/  146]
train() client id: f_00005-3-2 loss: 0.593873  [   96/  146]
train() client id: f_00005-3-3 loss: 0.605775  [  128/  146]
train() client id: f_00005-4-0 loss: 0.464895  [   32/  146]
train() client id: f_00005-4-1 loss: 0.492031  [   64/  146]
train() client id: f_00005-4-2 loss: 0.523617  [   96/  146]
train() client id: f_00005-4-3 loss: 0.656928  [  128/  146]
train() client id: f_00006-0-0 loss: 0.461680  [   32/   54]
train() client id: f_00006-1-0 loss: 0.472219  [   32/   54]
train() client id: f_00006-2-0 loss: 0.483059  [   32/   54]
train() client id: f_00006-3-0 loss: 0.433222  [   32/   54]
train() client id: f_00006-4-0 loss: 0.390038  [   32/   54]
train() client id: f_00007-0-0 loss: 0.522296  [   32/  179]
train() client id: f_00007-0-1 loss: 0.539441  [   64/  179]
train() client id: f_00007-0-2 loss: 0.533674  [   96/  179]
train() client id: f_00007-0-3 loss: 0.459170  [  128/  179]
train() client id: f_00007-0-4 loss: 0.736983  [  160/  179]
train() client id: f_00007-1-0 loss: 0.565318  [   32/  179]
train() client id: f_00007-1-1 loss: 0.623056  [   64/  179]
train() client id: f_00007-1-2 loss: 0.515044  [   96/  179]
train() client id: f_00007-1-3 loss: 0.683927  [  128/  179]
train() client id: f_00007-1-4 loss: 0.461230  [  160/  179]
train() client id: f_00007-2-0 loss: 0.433991  [   32/  179]
train() client id: f_00007-2-1 loss: 0.547235  [   64/  179]
train() client id: f_00007-2-2 loss: 0.693510  [   96/  179]
train() client id: f_00007-2-3 loss: 0.607416  [  128/  179]
train() client id: f_00007-2-4 loss: 0.529734  [  160/  179]
train() client id: f_00007-3-0 loss: 0.402888  [   32/  179]
train() client id: f_00007-3-1 loss: 0.518452  [   64/  179]
train() client id: f_00007-3-2 loss: 0.536230  [   96/  179]
train() client id: f_00007-3-3 loss: 0.498444  [  128/  179]
train() client id: f_00007-3-4 loss: 0.689611  [  160/  179]
train() client id: f_00007-4-0 loss: 0.886431  [   32/  179]
train() client id: f_00007-4-1 loss: 0.507828  [   64/  179]
train() client id: f_00007-4-2 loss: 0.607687  [   96/  179]
train() client id: f_00007-4-3 loss: 0.367751  [  128/  179]
train() client id: f_00007-4-4 loss: 0.535708  [  160/  179]
train() client id: f_00008-0-0 loss: 0.756178  [   32/  130]
train() client id: f_00008-0-1 loss: 0.713879  [   64/  130]
train() client id: f_00008-0-2 loss: 0.699420  [   96/  130]
train() client id: f_00008-0-3 loss: 0.502787  [  128/  130]
train() client id: f_00008-1-0 loss: 0.647109  [   32/  130]
train() client id: f_00008-1-1 loss: 0.554494  [   64/  130]
train() client id: f_00008-1-2 loss: 0.717244  [   96/  130]
train() client id: f_00008-1-3 loss: 0.744309  [  128/  130]
train() client id: f_00008-2-0 loss: 0.729293  [   32/  130]
train() client id: f_00008-2-1 loss: 0.615385  [   64/  130]
train() client id: f_00008-2-2 loss: 0.577954  [   96/  130]
train() client id: f_00008-2-3 loss: 0.700731  [  128/  130]
train() client id: f_00008-3-0 loss: 0.700261  [   32/  130]
train() client id: f_00008-3-1 loss: 0.672183  [   64/  130]
train() client id: f_00008-3-2 loss: 0.699349  [   96/  130]
train() client id: f_00008-3-3 loss: 0.586526  [  128/  130]
train() client id: f_00008-4-0 loss: 0.609310  [   32/  130]
train() client id: f_00008-4-1 loss: 0.630352  [   64/  130]
train() client id: f_00008-4-2 loss: 0.700005  [   96/  130]
train() client id: f_00008-4-3 loss: 0.720707  [  128/  130]
train() client id: f_00009-0-0 loss: 0.990962  [   32/  118]
train() client id: f_00009-0-1 loss: 0.912094  [   64/  118]
train() client id: f_00009-0-2 loss: 0.649116  [   96/  118]
train() client id: f_00009-1-0 loss: 0.952394  [   32/  118]
train() client id: f_00009-1-1 loss: 0.925248  [   64/  118]
train() client id: f_00009-1-2 loss: 0.684093  [   96/  118]
train() client id: f_00009-2-0 loss: 0.765580  [   32/  118]
train() client id: f_00009-2-1 loss: 0.878366  [   64/  118]
train() client id: f_00009-2-2 loss: 0.833804  [   96/  118]
train() client id: f_00009-3-0 loss: 0.906759  [   32/  118]
train() client id: f_00009-3-1 loss: 0.717587  [   64/  118]
train() client id: f_00009-3-2 loss: 0.847854  [   96/  118]
train() client id: f_00009-4-0 loss: 0.900381  [   32/  118]
train() client id: f_00009-4-1 loss: 0.841292  [   64/  118]
train() client id: f_00009-4-2 loss: 0.584247  [   96/  118]
At round 73 accuracy: 0.6445623342175066
At round 73 training accuracy: 0.5848423876592891
At round 73 training loss: 0.8321006560554487
update_location
xs = -3.905658 4.200318 385.009024 18.811294 0.979296 3.956410 -347.443192 -326.324852 369.663977 -312.060879 
ys = 377.587959 360.555839 1.320614 -347.455176 339.350187 322.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -20.71142616811743
ys mean: 107.3941425355287
dists_uav = 390.625039 374.189999 397.785988 362.048290 353.778898 337.971344 361.557274 341.304241 383.353787 327.716347 
uav_gains = -121.047838 -120.363158 -121.321468 -119.796541 -119.375341 -118.477060 -119.772377 -118.677459 -120.755385 -117.820522 
uav_gains_db_mean: -119.74071484822035
dists_bs = 265.170049 257.960701 586.322979 557.245214 240.789735 232.108638 247.562698 230.731587 566.959757 219.148757 
bs_gains = -107.425744 -107.090558 -117.074893 -116.456357 -106.252922 -105.806416 -106.590245 -105.734057 -116.666522 -105.107751 
bs_gains_db_mean: -109.42054650944169
Round 74
-------------------------------
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.17965282 2.31354803 1.16601007 0.44876366 2.6663691  1.28432213
 0.54349708 1.6129016  1.17620535 1.04170236]
obj_prev = 13.43297219536657
eta_min = 1.985153234470665e-80	eta_max = 0.9640166902686983
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 3.044236871939507	eta = 0.9090909090909091
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 8.994246026442418	eta = 0.3076953929504865
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 5.5116993178884695	eta = 0.5021115822515108
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.941875446838901	eta = 0.5600076519876258
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.905748712654075	eta = 0.5641316397354368
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.905578223300182	eta = 0.5641512456685993
eta = 0.5641512456685993
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [0.04723579 0.09934512 0.04648602 0.01612015 0.11471549 0.05473354
 0.02024391 0.06710481 0.04873534 0.04423669]
ene_total = [0.51557111 0.70394459 0.51893062 0.28085198 0.80121777 0.40983951
 0.30711799 0.58944182 0.4390443  0.33961853]
ti_comp = [3.7239081  3.95531799 3.71124405 3.77370601 3.95943373 3.96148105
 3.77455137 3.80899893 3.85658756 3.96449942]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [4.75002260e-07 3.91702877e-06 4.55835322e-07 1.83844731e-08
 6.01838646e-06 6.53018300e-07 3.63942310e-08 1.30172529e-06
 4.86413493e-07 3.44232011e-07]
ene_total = [0.20841223 0.05795483 0.2166472  0.17602698 0.05529214 0.05392593
 0.17547739 0.15308527 0.12213433 0.05196115]
optimize_network iter = 0 obj = 1.2709174591621142
eta = 0.5641512456685993
freqs = [ 6342233.67091708 12558423.2934157   6262861.66135149  2135851.43012001
 14486350.74522172  6908216.2161498   2681631.30754287  8808720.47508642
  6318453.65727492  5579101.94228267]
eta_min = 0.5641512456686001	eta_max = 0.8530565293483942
af = 6.786276083478417e-05	bf = 0.5538889445570291	zeta = 7.464903691826259e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [7.90879612e-08 6.52185990e-07 7.58966627e-08 3.06101806e-09
 1.00206242e-06 1.08727664e-07 6.05964597e-09 2.16737493e-07
 8.09879336e-08 5.73146914e-08]
ene_total = [0.99918599 0.27775315 1.03866771 0.84393208 0.2649327  0.25852208
 0.84129665 0.73390845 0.58554042 0.2491103 ]
ti_comp = [1.04304861 1.27445851 1.03038457 1.09284653 1.27857424 1.28062156
 1.09369189 1.12813944 1.17572807 1.28363993]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.29575823e-07 8.07435167e-07 1.26557476e-07 4.69146470e-09
 1.23519014e-06 1.33732939e-07 9.27711067e-09 3.17581333e-07
 1.12005080e-07 7.02717911e-08]
ene_total = [0.61816447 0.17183973 0.64259052 0.52211305 0.16390964 0.15993959
 0.52048262 0.45404696 0.36225519 0.1541166 ]
optimize_network iter = 1 obj = 3.769458367807797
eta = 0.8530565293483942
freqs = [ 6286609.35895016 10821081.10112869  6262861.66135149  2047669.00897199
 12455063.70509777  5933111.71394756  2569503.70041527  8257350.63294458
  5754226.36383451  4783979.0224429 ]
eta_min = 0.8530565293483974	eta_max = 0.8530565293483936
af = 5.2542085376783516e-05	bf = 0.5538889445570291	zeta = 5.7796293914461874e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [7.77067690e-08 4.84219790e-07 7.58966627e-08 2.81347673e-09
 7.40744934e-07 8.01997961e-08 5.56349384e-09 1.90453887e-07
 6.71695741e-08 4.21420732e-08]
ene_total = [0.99918595 0.27774791 1.03866771 0.84393207 0.26492456 0.25852119
 0.84129664 0.73390763 0.58553999 0.24910983]
ti_comp = [1.04304861 1.27445851 1.03038457 1.09284653 1.27857424 1.28062156
 1.09369189 1.12813944 1.17572807 1.28363993]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.29575823e-07 8.07435167e-07 1.26557476e-07 4.69146470e-09
 1.23519014e-06 1.33732939e-07 9.27711067e-09 3.17581333e-07
 1.12005080e-07 7.02717911e-08]
ene_total = [0.61816447 0.17183973 0.64259052 0.52211305 0.16390964 0.15993959
 0.52048262 0.45404696 0.36225519 0.1541166 ]
optimize_network iter = 2 obj = 3.7694583678077827
eta = 0.8530565293483936
freqs = [ 6286609.35895015 10821081.10112868  6262861.66135148  2047669.00897199
 12455063.70509776  5933111.71394756  2569503.70041527  8257350.63294457
  5754226.3638345   4783979.0224429 ]
Done!
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.24492590e-07 7.75759650e-07 1.21592652e-07 4.50741950e-09
 1.18673388e-06 1.28486623e-07 8.91317151e-09 3.05122682e-07
 1.07611143e-07 6.75150431e-08]
ene_total = [0.03204957 0.00890923 0.03331597 0.02706966 0.00849807 0.00829228
 0.02698513 0.02354067 0.01878161 0.00799038]
At round 74 energy consumption: 0.1954325657499382
At round 74 eta: 0.8530565293483936
At round 74 a_n: 2.834210180524895
At round 74 local rounds: 5.2041580597357475
At round 74 global rounds: 19.287758537054202
gradient difference: 0.7964571714401245
train() client id: f_00000-0-0 loss: 1.098820  [   32/  126]
train() client id: f_00000-0-1 loss: 0.981114  [   64/  126]
train() client id: f_00000-0-2 loss: 1.051472  [   96/  126]
train() client id: f_00000-1-0 loss: 1.026670  [   32/  126]
train() client id: f_00000-1-1 loss: 0.840900  [   64/  126]
train() client id: f_00000-1-2 loss: 0.867755  [   96/  126]
train() client id: f_00000-2-0 loss: 0.895216  [   32/  126]
train() client id: f_00000-2-1 loss: 0.921101  [   64/  126]
train() client id: f_00000-2-2 loss: 0.966117  [   96/  126]
train() client id: f_00000-3-0 loss: 1.058384  [   32/  126]
train() client id: f_00000-3-1 loss: 1.045897  [   64/  126]
train() client id: f_00000-3-2 loss: 0.729931  [   96/  126]
train() client id: f_00000-4-0 loss: 0.938555  [   32/  126]
train() client id: f_00000-4-1 loss: 0.991209  [   64/  126]
train() client id: f_00000-4-2 loss: 0.724264  [   96/  126]
train() client id: f_00001-0-0 loss: 0.394852  [   32/  265]
train() client id: f_00001-0-1 loss: 0.596805  [   64/  265]
train() client id: f_00001-0-2 loss: 0.473602  [   96/  265]
train() client id: f_00001-0-3 loss: 0.470661  [  128/  265]
train() client id: f_00001-0-4 loss: 0.489611  [  160/  265]
train() client id: f_00001-0-5 loss: 0.610508  [  192/  265]
train() client id: f_00001-0-6 loss: 0.620713  [  224/  265]
train() client id: f_00001-0-7 loss: 0.490457  [  256/  265]
train() client id: f_00001-1-0 loss: 0.475505  [   32/  265]
train() client id: f_00001-1-1 loss: 0.485845  [   64/  265]
train() client id: f_00001-1-2 loss: 0.576722  [   96/  265]
train() client id: f_00001-1-3 loss: 0.397465  [  128/  265]
train() client id: f_00001-1-4 loss: 0.552527  [  160/  265]
train() client id: f_00001-1-5 loss: 0.614045  [  192/  265]
train() client id: f_00001-1-6 loss: 0.514065  [  224/  265]
train() client id: f_00001-1-7 loss: 0.537015  [  256/  265]
train() client id: f_00001-2-0 loss: 0.594528  [   32/  265]
train() client id: f_00001-2-1 loss: 0.473040  [   64/  265]
train() client id: f_00001-2-2 loss: 0.638429  [   96/  265]
train() client id: f_00001-2-3 loss: 0.456451  [  128/  265]
train() client id: f_00001-2-4 loss: 0.580365  [  160/  265]
train() client id: f_00001-2-5 loss: 0.430999  [  192/  265]
train() client id: f_00001-2-6 loss: 0.548974  [  224/  265]
train() client id: f_00001-2-7 loss: 0.416157  [  256/  265]
train() client id: f_00001-3-0 loss: 0.436911  [   32/  265]
train() client id: f_00001-3-1 loss: 0.562873  [   64/  265]
train() client id: f_00001-3-2 loss: 0.502676  [   96/  265]
train() client id: f_00001-3-3 loss: 0.455178  [  128/  265]
train() client id: f_00001-3-4 loss: 0.587455  [  160/  265]
train() client id: f_00001-3-5 loss: 0.580949  [  192/  265]
train() client id: f_00001-3-6 loss: 0.467383  [  224/  265]
train() client id: f_00001-3-7 loss: 0.500003  [  256/  265]
train() client id: f_00001-4-0 loss: 0.439816  [   32/  265]
train() client id: f_00001-4-1 loss: 0.576261  [   64/  265]
train() client id: f_00001-4-2 loss: 0.481391  [   96/  265]
train() client id: f_00001-4-3 loss: 0.533471  [  128/  265]
train() client id: f_00001-4-4 loss: 0.409524  [  160/  265]
train() client id: f_00001-4-5 loss: 0.667770  [  192/  265]
train() client id: f_00001-4-6 loss: 0.417962  [  224/  265]
train() client id: f_00001-4-7 loss: 0.519989  [  256/  265]
train() client id: f_00002-0-0 loss: 1.112309  [   32/  124]
train() client id: f_00002-0-1 loss: 0.802664  [   64/  124]
train() client id: f_00002-0-2 loss: 1.065208  [   96/  124]
train() client id: f_00002-1-0 loss: 0.879407  [   32/  124]
train() client id: f_00002-1-1 loss: 0.959261  [   64/  124]
train() client id: f_00002-1-2 loss: 0.982527  [   96/  124]
train() client id: f_00002-2-0 loss: 1.114215  [   32/  124]
train() client id: f_00002-2-1 loss: 0.784510  [   64/  124]
train() client id: f_00002-2-2 loss: 1.110047  [   96/  124]
train() client id: f_00002-3-0 loss: 1.044878  [   32/  124]
train() client id: f_00002-3-1 loss: 1.036360  [   64/  124]
train() client id: f_00002-3-2 loss: 0.931753  [   96/  124]
train() client id: f_00002-4-0 loss: 0.969881  [   32/  124]
train() client id: f_00002-4-1 loss: 1.072677  [   64/  124]
train() client id: f_00002-4-2 loss: 0.989936  [   96/  124]
train() client id: f_00003-0-0 loss: 0.548014  [   32/   43]
train() client id: f_00003-1-0 loss: 0.556223  [   32/   43]
train() client id: f_00003-2-0 loss: 0.786096  [   32/   43]
train() client id: f_00003-3-0 loss: 0.653464  [   32/   43]
train() client id: f_00003-4-0 loss: 0.837721  [   32/   43]
train() client id: f_00004-0-0 loss: 0.924457  [   32/  306]
train() client id: f_00004-0-1 loss: 0.878578  [   64/  306]
train() client id: f_00004-0-2 loss: 1.030371  [   96/  306]
train() client id: f_00004-0-3 loss: 0.778774  [  128/  306]
train() client id: f_00004-0-4 loss: 0.915444  [  160/  306]
train() client id: f_00004-0-5 loss: 0.938689  [  192/  306]
train() client id: f_00004-0-6 loss: 0.863252  [  224/  306]
train() client id: f_00004-0-7 loss: 1.063735  [  256/  306]
train() client id: f_00004-0-8 loss: 0.940881  [  288/  306]
train() client id: f_00004-1-0 loss: 1.016918  [   32/  306]
train() client id: f_00004-1-1 loss: 0.803494  [   64/  306]
train() client id: f_00004-1-2 loss: 0.871448  [   96/  306]
train() client id: f_00004-1-3 loss: 0.901866  [  128/  306]
train() client id: f_00004-1-4 loss: 0.895882  [  160/  306]
train() client id: f_00004-1-5 loss: 1.005973  [  192/  306]
train() client id: f_00004-1-6 loss: 0.938433  [  224/  306]
train() client id: f_00004-1-7 loss: 1.036306  [  256/  306]
train() client id: f_00004-1-8 loss: 0.792165  [  288/  306]
train() client id: f_00004-2-0 loss: 0.925756  [   32/  306]
train() client id: f_00004-2-1 loss: 0.812860  [   64/  306]
train() client id: f_00004-2-2 loss: 0.947306  [   96/  306]
train() client id: f_00004-2-3 loss: 0.959901  [  128/  306]
train() client id: f_00004-2-4 loss: 0.872263  [  160/  306]
train() client id: f_00004-2-5 loss: 0.853247  [  192/  306]
train() client id: f_00004-2-6 loss: 1.000161  [  224/  306]
train() client id: f_00004-2-7 loss: 1.049549  [  256/  306]
train() client id: f_00004-2-8 loss: 0.888039  [  288/  306]
train() client id: f_00004-3-0 loss: 0.862024  [   32/  306]
train() client id: f_00004-3-1 loss: 0.795146  [   64/  306]
train() client id: f_00004-3-2 loss: 0.866474  [   96/  306]
train() client id: f_00004-3-3 loss: 0.956789  [  128/  306]
train() client id: f_00004-3-4 loss: 0.929812  [  160/  306]
train() client id: f_00004-3-5 loss: 1.135493  [  192/  306]
train() client id: f_00004-3-6 loss: 0.820701  [  224/  306]
train() client id: f_00004-3-7 loss: 0.947697  [  256/  306]
train() client id: f_00004-3-8 loss: 0.913697  [  288/  306]
train() client id: f_00004-4-0 loss: 1.017326  [   32/  306]
train() client id: f_00004-4-1 loss: 0.886448  [   64/  306]
train() client id: f_00004-4-2 loss: 0.853111  [   96/  306]
train() client id: f_00004-4-3 loss: 0.792165  [  128/  306]
train() client id: f_00004-4-4 loss: 0.974156  [  160/  306]
train() client id: f_00004-4-5 loss: 0.936117  [  192/  306]
train() client id: f_00004-4-6 loss: 0.938830  [  224/  306]
train() client id: f_00004-4-7 loss: 0.843238  [  256/  306]
train() client id: f_00004-4-8 loss: 0.909283  [  288/  306]
train() client id: f_00005-0-0 loss: 0.339836  [   32/  146]
train() client id: f_00005-0-1 loss: 0.655293  [   64/  146]
train() client id: f_00005-0-2 loss: 1.050512  [   96/  146]
train() client id: f_00005-0-3 loss: 0.623977  [  128/  146]
train() client id: f_00005-1-0 loss: 0.575830  [   32/  146]
train() client id: f_00005-1-1 loss: 0.436990  [   64/  146]
train() client id: f_00005-1-2 loss: 0.655219  [   96/  146]
train() client id: f_00005-1-3 loss: 0.769799  [  128/  146]
train() client id: f_00005-2-0 loss: 0.623052  [   32/  146]
train() client id: f_00005-2-1 loss: 0.727292  [   64/  146]
train() client id: f_00005-2-2 loss: 0.869053  [   96/  146]
train() client id: f_00005-2-3 loss: 0.428258  [  128/  146]
train() client id: f_00005-3-0 loss: 0.853477  [   32/  146]
train() client id: f_00005-3-1 loss: 0.747976  [   64/  146]
train() client id: f_00005-3-2 loss: 0.711427  [   96/  146]
train() client id: f_00005-3-3 loss: 0.360534  [  128/  146]
train() client id: f_00005-4-0 loss: 0.489704  [   32/  146]
train() client id: f_00005-4-1 loss: 0.750975  [   64/  146]
train() client id: f_00005-4-2 loss: 0.736414  [   96/  146]
train() client id: f_00005-4-3 loss: 0.706335  [  128/  146]
train() client id: f_00006-0-0 loss: 0.496111  [   32/   54]
train() client id: f_00006-1-0 loss: 0.506058  [   32/   54]
train() client id: f_00006-2-0 loss: 0.444201  [   32/   54]
train() client id: f_00006-3-0 loss: 0.479880  [   32/   54]
train() client id: f_00006-4-0 loss: 0.462731  [   32/   54]
train() client id: f_00007-0-0 loss: 0.452460  [   32/  179]
train() client id: f_00007-0-1 loss: 0.841103  [   64/  179]
train() client id: f_00007-0-2 loss: 0.935156  [   96/  179]
train() client id: f_00007-0-3 loss: 0.552997  [  128/  179]
train() client id: f_00007-0-4 loss: 0.508726  [  160/  179]
train() client id: f_00007-1-0 loss: 0.636910  [   32/  179]
train() client id: f_00007-1-1 loss: 0.474220  [   64/  179]
train() client id: f_00007-1-2 loss: 1.006293  [   96/  179]
train() client id: f_00007-1-3 loss: 0.626657  [  128/  179]
train() client id: f_00007-1-4 loss: 0.538746  [  160/  179]
train() client id: f_00007-2-0 loss: 0.719464  [   32/  179]
train() client id: f_00007-2-1 loss: 0.692302  [   64/  179]
train() client id: f_00007-2-2 loss: 0.464053  [   96/  179]
train() client id: f_00007-2-3 loss: 0.620057  [  128/  179]
train() client id: f_00007-2-4 loss: 0.667378  [  160/  179]
train() client id: f_00007-3-0 loss: 0.904143  [   32/  179]
train() client id: f_00007-3-1 loss: 0.667611  [   64/  179]
train() client id: f_00007-3-2 loss: 0.577919  [   96/  179]
train() client id: f_00007-3-3 loss: 0.470236  [  128/  179]
train() client id: f_00007-3-4 loss: 0.620920  [  160/  179]
train() client id: f_00007-4-0 loss: 0.691748  [   32/  179]
train() client id: f_00007-4-1 loss: 0.568140  [   64/  179]
train() client id: f_00007-4-2 loss: 0.833064  [   96/  179]
train() client id: f_00007-4-3 loss: 0.577242  [  128/  179]
train() client id: f_00007-4-4 loss: 0.446016  [  160/  179]
train() client id: f_00008-0-0 loss: 0.741316  [   32/  130]
train() client id: f_00008-0-1 loss: 0.538976  [   64/  130]
train() client id: f_00008-0-2 loss: 0.760969  [   96/  130]
train() client id: f_00008-0-3 loss: 0.651880  [  128/  130]
train() client id: f_00008-1-0 loss: 0.737303  [   32/  130]
train() client id: f_00008-1-1 loss: 0.644277  [   64/  130]
train() client id: f_00008-1-2 loss: 0.670041  [   96/  130]
train() client id: f_00008-1-3 loss: 0.686313  [  128/  130]
train() client id: f_00008-2-0 loss: 0.690724  [   32/  130]
train() client id: f_00008-2-1 loss: 0.736355  [   64/  130]
train() client id: f_00008-2-2 loss: 0.688863  [   96/  130]
train() client id: f_00008-2-3 loss: 0.633948  [  128/  130]
train() client id: f_00008-3-0 loss: 0.651011  [   32/  130]
train() client id: f_00008-3-1 loss: 0.608884  [   64/  130]
train() client id: f_00008-3-2 loss: 0.786789  [   96/  130]
train() client id: f_00008-3-3 loss: 0.690745  [  128/  130]
train() client id: f_00008-4-0 loss: 0.707885  [   32/  130]
train() client id: f_00008-4-1 loss: 0.638365  [   64/  130]
train() client id: f_00008-4-2 loss: 0.748831  [   96/  130]
train() client id: f_00008-4-3 loss: 0.653027  [  128/  130]
train() client id: f_00009-0-0 loss: 0.912733  [   32/  118]
train() client id: f_00009-0-1 loss: 0.972140  [   64/  118]
train() client id: f_00009-0-2 loss: 1.193827  [   96/  118]
train() client id: f_00009-1-0 loss: 1.105163  [   32/  118]
train() client id: f_00009-1-1 loss: 1.063356  [   64/  118]
train() client id: f_00009-1-2 loss: 0.884749  [   96/  118]
train() client id: f_00009-2-0 loss: 0.939776  [   32/  118]
train() client id: f_00009-2-1 loss: 0.998474  [   64/  118]
train() client id: f_00009-2-2 loss: 1.023388  [   96/  118]
train() client id: f_00009-3-0 loss: 1.013508  [   32/  118]
train() client id: f_00009-3-1 loss: 0.970934  [   64/  118]
train() client id: f_00009-3-2 loss: 1.070286  [   96/  118]
train() client id: f_00009-4-0 loss: 0.941346  [   32/  118]
train() client id: f_00009-4-1 loss: 0.951895  [   64/  118]
train() client id: f_00009-4-2 loss: 0.933007  [   96/  118]
At round 74 accuracy: 0.6445623342175066
At round 74 training accuracy: 0.5928906773977196
At round 74 training loss: 0.8135695318003715
update_location
xs = -3.905658 4.200318 390.009024 18.811294 0.979296 3.956410 -352.443192 -331.324852 374.663977 -317.060879 
ys = 382.587959 365.555839 1.320614 -352.455176 344.350187 327.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -21.21142616811743
ys mean: 108.8941425355287
dists_uav = 395.460239 379.010176 402.627350 366.849446 358.577760 342.750304 366.364701 346.087899 388.177492 332.480996 
uav_gains = -121.234069 -120.572956 -121.499136 -120.027520 -119.623507 -118.762508 -120.004626 -118.954535 -120.951122 -118.133222 
uav_gains_db_mean: -119.9763199128185
dists_bs = 269.009358 261.580354 591.100437 561.935744 244.229807 235.324375 251.071148 234.041265 571.764816 222.310113 
bs_gains = -107.600546 -107.260003 -117.173575 -116.558286 -106.425422 -105.973733 -106.761370 -105.907247 -116.769147 -105.281917 
bs_gains_db_mean: -109.5711245261322
Round 75
-------------------------------
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.03922851 2.03415137 1.02725189 0.39661097 2.34431442 1.12928775
 0.47989662 1.41999007 1.0345605  0.91598534]
obj_prev = 11.821277437641127
eta_min = 2.632614097881737e-91	eta_max = 0.9674950324086027
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 2.6763069615753383	eta = 0.9090909090909091
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 8.03601399311375	eta = 0.3027628287842397
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.8844582536009025	eta = 0.4981118073659859
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.372075946270912	eta = 0.5564876636646818
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.3396007499684774	eta = 0.5606521126909028
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.3394472210647415	eta = 0.5606719484671788
eta = 0.5606719484671788
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [0.04774628 0.10041876 0.0469884  0.01629436 0.11595525 0.05532505
 0.02046269 0.06783003 0.04926203 0.04471477]
ene_total = [0.45740881 0.62055196 0.46033181 0.25063986 0.70629813 0.3612359
 0.2738053  0.5227749  0.38706651 0.29933403]
ti_comp = [4.2992849  4.53835717 4.28655232 4.34933896 4.54253515 4.54464388
 4.35017711 4.38486951 4.43846938 4.54768586]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [3.68049423e-07 3.07275006e-06 3.52887078e-07 1.42937379e-08
 4.72231164e-06 5.12443289e-07 2.82979099e-08 1.01445480e-06
 3.79271804e-07 2.70179295e-07]
ene_total = [0.18661622 0.05104089 0.19383747 0.15822588 0.04868069 0.04746084
 0.1577506  0.13808028 0.10767732 0.0457342 ]
optimize_network iter = 0 obj = 1.1351043989055445
eta = 0.5606719484671788
freqs = [ 5552816.27091243 11063338.14405196  5480908.36118572  1873200.15342277
 12763274.63448044  6086841.3557449   2351937.65383736  7734555.17253756
  5549439.33843933  4916211.22498621]
eta_min = 0.5606719484671795	eta_max = 0.8680569114774389
af = 4.622705191075305e-05	bf = 0.4986564559927847	zeta = 5.084975710182836e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [6.06251059e-08 5.06143430e-07 5.81275641e-08 2.35446469e-09
 7.77859235e-07 8.44096653e-08 4.66123208e-09 1.67101009e-07
 6.24736566e-08 4.45039370e-08]
ene_total = [0.90183473 0.24659032 0.93673252 0.76464342 0.23514662 0.22934794
 0.76234626 0.66726472 0.52035367 0.22100929]
ti_comp = [1.06098413 1.30005639 1.04825154 1.11103819 1.30423438 1.30634311
 1.11187634 1.14656874 1.20016861 1.30938509]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [8.83797899e-08 5.47611225e-07 8.62964911e-08 3.20336552e-09
 8.37745486e-07 9.06991369e-08 6.33471974e-09 2.16979151e-07
 7.58585669e-08 4.76617893e-08]
ene_total = [0.62136672 0.16990208 0.64541139 0.52684109 0.16201769 0.15802138
 0.52525835 0.45974786 0.35852513 0.15227597]
optimize_network iter = 1 obj = 3.7793676563367
eta = 0.8680569114774389
freqs = [ 5502474.32221998  9444523.43544286  5480908.36118572  1793229.33817215
 10870816.12174629  5178356.71698614  2250264.85743831  7233513.5913764
  5018771.87691086  4175523.98380345]
eta_min = 0.8680569114774714	eta_max = 0.8680569114774374
af = 3.523192231617816e-05	bf = 0.4986564559927847	zeta = 3.8755114547795976e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [5.95308319e-08 3.68859801e-07 5.81275641e-08 2.15772197e-09
 5.64288349e-07 6.10930969e-08 4.26693859e-09 1.46152750e-07
 5.10967904e-08 3.21040135e-08]
ene_total = [0.9018347  0.24658655 0.93673252 0.76464342 0.23514077 0.2293473
 0.76234625 0.66726414 0.52035336 0.22100895]
ti_comp = [1.06098413 1.30005639 1.04825154 1.11103819 1.30423438 1.30634311
 1.11187634 1.14656874 1.20016861 1.30938509]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [8.83797899e-08 5.47611225e-07 8.62964911e-08 3.20336552e-09
 8.37745486e-07 9.06991369e-08 6.33471974e-09 2.16979151e-07
 7.58585669e-08 4.76617893e-08]
ene_total = [0.62136672 0.16990208 0.64541139 0.52684109 0.16201769 0.15802138
 0.52525835 0.45974786 0.35852513 0.15227597]
optimize_network iter = 2 obj = 3.779367656336656
eta = 0.8680569114774374
freqs = [ 5502474.32221997  9444523.43544286  5480908.3611857   1793229.33817215
 10870816.12174629  5178356.71698613  2250264.85743831  7233513.59137638
  5018771.87691085  4175523.98380344]
Done!
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [7.62986036e-08 4.72754821e-07 7.45000840e-08 2.76547745e-09
 7.23228816e-07 7.83009046e-08 5.46878728e-09 1.87318914e-07
 6.54889850e-08 4.11466013e-08]
ene_total = [0.03290372 0.00899689 0.03417697 0.02789824 0.00857934 0.00836782
 0.02781442 0.02434537 0.01898526 0.00806358]
At round 75 energy consumption: 0.20013159600162664
At round 75 eta: 0.8680569114774374
At round 75 a_n: 2.4916643335555797
At round 75 local rounds: 4.633363424333758
At round 75 global rounds: 18.884386908447258
gradient difference: 0.7458789348602295
train() client id: f_00000-0-0 loss: 1.332346  [   32/  126]
train() client id: f_00000-0-1 loss: 1.001541  [   64/  126]
train() client id: f_00000-0-2 loss: 1.222126  [   96/  126]
train() client id: f_00000-1-0 loss: 0.937771  [   32/  126]
train() client id: f_00000-1-1 loss: 1.228431  [   64/  126]
train() client id: f_00000-1-2 loss: 1.100177  [   96/  126]
train() client id: f_00000-2-0 loss: 0.939354  [   32/  126]
train() client id: f_00000-2-1 loss: 1.289751  [   64/  126]
train() client id: f_00000-2-2 loss: 1.053377  [   96/  126]
train() client id: f_00000-3-0 loss: 1.072645  [   32/  126]
train() client id: f_00000-3-1 loss: 1.122806  [   64/  126]
train() client id: f_00000-3-2 loss: 1.149094  [   96/  126]
train() client id: f_00001-0-0 loss: 0.538978  [   32/  265]
train() client id: f_00001-0-1 loss: 0.394406  [   64/  265]
train() client id: f_00001-0-2 loss: 0.574993  [   96/  265]
train() client id: f_00001-0-3 loss: 0.555122  [  128/  265]
train() client id: f_00001-0-4 loss: 0.462203  [  160/  265]
train() client id: f_00001-0-5 loss: 0.397698  [  192/  265]
train() client id: f_00001-0-6 loss: 0.545648  [  224/  265]
train() client id: f_00001-0-7 loss: 0.519547  [  256/  265]
train() client id: f_00001-1-0 loss: 0.427748  [   32/  265]
train() client id: f_00001-1-1 loss: 0.466319  [   64/  265]
train() client id: f_00001-1-2 loss: 0.407567  [   96/  265]
train() client id: f_00001-1-3 loss: 0.561337  [  128/  265]
train() client id: f_00001-1-4 loss: 0.469791  [  160/  265]
train() client id: f_00001-1-5 loss: 0.465334  [  192/  265]
train() client id: f_00001-1-6 loss: 0.639131  [  224/  265]
train() client id: f_00001-1-7 loss: 0.462625  [  256/  265]
train() client id: f_00001-2-0 loss: 0.598865  [   32/  265]
train() client id: f_00001-2-1 loss: 0.603433  [   64/  265]
train() client id: f_00001-2-2 loss: 0.614984  [   96/  265]
train() client id: f_00001-2-3 loss: 0.502792  [  128/  265]
train() client id: f_00001-2-4 loss: 0.398441  [  160/  265]
train() client id: f_00001-2-5 loss: 0.415402  [  192/  265]
train() client id: f_00001-2-6 loss: 0.418480  [  224/  265]
train() client id: f_00001-2-7 loss: 0.412703  [  256/  265]
train() client id: f_00001-3-0 loss: 0.551812  [   32/  265]
train() client id: f_00001-3-1 loss: 0.504605  [   64/  265]
train() client id: f_00001-3-2 loss: 0.554862  [   96/  265]
train() client id: f_00001-3-3 loss: 0.477538  [  128/  265]
train() client id: f_00001-3-4 loss: 0.441671  [  160/  265]
train() client id: f_00001-3-5 loss: 0.469596  [  192/  265]
train() client id: f_00001-3-6 loss: 0.571340  [  224/  265]
train() client id: f_00001-3-7 loss: 0.395657  [  256/  265]
train() client id: f_00002-0-0 loss: 0.977366  [   32/  124]
train() client id: f_00002-0-1 loss: 0.783724  [   64/  124]
train() client id: f_00002-0-2 loss: 0.979827  [   96/  124]
train() client id: f_00002-1-0 loss: 0.983606  [   32/  124]
train() client id: f_00002-1-1 loss: 0.891221  [   64/  124]
train() client id: f_00002-1-2 loss: 0.974197  [   96/  124]
train() client id: f_00002-2-0 loss: 0.830266  [   32/  124]
train() client id: f_00002-2-1 loss: 0.892283  [   64/  124]
train() client id: f_00002-2-2 loss: 0.924511  [   96/  124]
train() client id: f_00002-3-0 loss: 0.953513  [   32/  124]
train() client id: f_00002-3-1 loss: 0.882504  [   64/  124]
train() client id: f_00002-3-2 loss: 0.897887  [   96/  124]
train() client id: f_00003-0-0 loss: 0.586134  [   32/   43]
train() client id: f_00003-1-0 loss: 0.468724  [   32/   43]
train() client id: f_00003-2-0 loss: 0.557109  [   32/   43]
train() client id: f_00003-3-0 loss: 0.613081  [   32/   43]
train() client id: f_00004-0-0 loss: 0.846274  [   32/  306]
train() client id: f_00004-0-1 loss: 0.777496  [   64/  306]
train() client id: f_00004-0-2 loss: 0.749980  [   96/  306]
train() client id: f_00004-0-3 loss: 0.606531  [  128/  306]
train() client id: f_00004-0-4 loss: 0.820331  [  160/  306]
train() client id: f_00004-0-5 loss: 0.923092  [  192/  306]
train() client id: f_00004-0-6 loss: 0.941271  [  224/  306]
train() client id: f_00004-0-7 loss: 0.778777  [  256/  306]
train() client id: f_00004-0-8 loss: 0.927582  [  288/  306]
train() client id: f_00004-1-0 loss: 0.712459  [   32/  306]
train() client id: f_00004-1-1 loss: 0.863126  [   64/  306]
train() client id: f_00004-1-2 loss: 0.840216  [   96/  306]
train() client id: f_00004-1-3 loss: 0.832183  [  128/  306]
train() client id: f_00004-1-4 loss: 0.788871  [  160/  306]
train() client id: f_00004-1-5 loss: 0.738540  [  192/  306]
train() client id: f_00004-1-6 loss: 0.843830  [  224/  306]
train() client id: f_00004-1-7 loss: 0.880091  [  256/  306]
train() client id: f_00004-1-8 loss: 0.865576  [  288/  306]
train() client id: f_00004-2-0 loss: 1.023326  [   32/  306]
train() client id: f_00004-2-1 loss: 0.701674  [   64/  306]
train() client id: f_00004-2-2 loss: 0.826189  [   96/  306]
train() client id: f_00004-2-3 loss: 0.873226  [  128/  306]
train() client id: f_00004-2-4 loss: 0.773262  [  160/  306]
train() client id: f_00004-2-5 loss: 0.884658  [  192/  306]
train() client id: f_00004-2-6 loss: 0.777791  [  224/  306]
train() client id: f_00004-2-7 loss: 0.746424  [  256/  306]
train() client id: f_00004-2-8 loss: 0.788190  [  288/  306]
train() client id: f_00004-3-0 loss: 0.782494  [   32/  306]
train() client id: f_00004-3-1 loss: 0.895193  [   64/  306]
train() client id: f_00004-3-2 loss: 0.823907  [   96/  306]
train() client id: f_00004-3-3 loss: 0.908324  [  128/  306]
train() client id: f_00004-3-4 loss: 0.769580  [  160/  306]
train() client id: f_00004-3-5 loss: 0.738133  [  192/  306]
train() client id: f_00004-3-6 loss: 0.756948  [  224/  306]
train() client id: f_00004-3-7 loss: 0.875872  [  256/  306]
train() client id: f_00004-3-8 loss: 0.794108  [  288/  306]
train() client id: f_00005-0-0 loss: 0.298223  [   32/  146]
train() client id: f_00005-0-1 loss: 0.752333  [   64/  146]
train() client id: f_00005-0-2 loss: 0.740494  [   96/  146]
train() client id: f_00005-0-3 loss: 0.619201  [  128/  146]
train() client id: f_00005-1-0 loss: 0.615896  [   32/  146]
train() client id: f_00005-1-1 loss: 0.604883  [   64/  146]
train() client id: f_00005-1-2 loss: 0.540929  [   96/  146]
train() client id: f_00005-1-3 loss: 0.535711  [  128/  146]
train() client id: f_00005-2-0 loss: 0.484706  [   32/  146]
train() client id: f_00005-2-1 loss: 0.445229  [   64/  146]
train() client id: f_00005-2-2 loss: 0.699186  [   96/  146]
train() client id: f_00005-2-3 loss: 0.623810  [  128/  146]
train() client id: f_00005-3-0 loss: 0.713456  [   32/  146]
train() client id: f_00005-3-1 loss: 0.580652  [   64/  146]
train() client id: f_00005-3-2 loss: 0.476434  [   96/  146]
train() client id: f_00005-3-3 loss: 0.615300  [  128/  146]
train() client id: f_00006-0-0 loss: 0.491508  [   32/   54]
train() client id: f_00006-1-0 loss: 0.552942  [   32/   54]
train() client id: f_00006-2-0 loss: 0.572490  [   32/   54]
train() client id: f_00006-3-0 loss: 0.518451  [   32/   54]
train() client id: f_00007-0-0 loss: 0.733775  [   32/  179]
train() client id: f_00007-0-1 loss: 1.046467  [   64/  179]
train() client id: f_00007-0-2 loss: 0.829763  [   96/  179]
train() client id: f_00007-0-3 loss: 0.711831  [  128/  179]
train() client id: f_00007-0-4 loss: 0.756746  [  160/  179]
train() client id: f_00007-1-0 loss: 0.709693  [   32/  179]
train() client id: f_00007-1-1 loss: 1.064470  [   64/  179]
train() client id: f_00007-1-2 loss: 0.641015  [   96/  179]
train() client id: f_00007-1-3 loss: 0.817464  [  128/  179]
train() client id: f_00007-1-4 loss: 0.747285  [  160/  179]
train() client id: f_00007-2-0 loss: 0.612617  [   32/  179]
train() client id: f_00007-2-1 loss: 0.854274  [   64/  179]
train() client id: f_00007-2-2 loss: 0.849506  [   96/  179]
train() client id: f_00007-2-3 loss: 0.916787  [  128/  179]
train() client id: f_00007-2-4 loss: 0.734475  [  160/  179]
train() client id: f_00007-3-0 loss: 0.785611  [   32/  179]
train() client id: f_00007-3-1 loss: 0.829760  [   64/  179]
train() client id: f_00007-3-2 loss: 0.901602  [   96/  179]
train() client id: f_00007-3-3 loss: 0.683282  [  128/  179]
train() client id: f_00007-3-4 loss: 0.861254  [  160/  179]
train() client id: f_00008-0-0 loss: 0.788554  [   32/  130]
train() client id: f_00008-0-1 loss: 0.694150  [   64/  130]
train() client id: f_00008-0-2 loss: 0.922228  [   96/  130]
train() client id: f_00008-0-3 loss: 0.703843  [  128/  130]
train() client id: f_00008-1-0 loss: 0.806535  [   32/  130]
train() client id: f_00008-1-1 loss: 0.823955  [   64/  130]
train() client id: f_00008-1-2 loss: 0.713333  [   96/  130]
train() client id: f_00008-1-3 loss: 0.732139  [  128/  130]
train() client id: f_00008-2-0 loss: 0.724988  [   32/  130]
train() client id: f_00008-2-1 loss: 0.738397  [   64/  130]
train() client id: f_00008-2-2 loss: 0.755117  [   96/  130]
train() client id: f_00008-2-3 loss: 0.861983  [  128/  130]
train() client id: f_00008-3-0 loss: 0.830508  [   32/  130]
train() client id: f_00008-3-1 loss: 0.630709  [   64/  130]
train() client id: f_00008-3-2 loss: 0.782130  [   96/  130]
train() client id: f_00008-3-3 loss: 0.847850  [  128/  130]
train() client id: f_00009-0-0 loss: 1.121121  [   32/  118]
train() client id: f_00009-0-1 loss: 0.666854  [   64/  118]
train() client id: f_00009-0-2 loss: 0.853365  [   96/  118]
train() client id: f_00009-1-0 loss: 0.805030  [   32/  118]
train() client id: f_00009-1-1 loss: 0.839188  [   64/  118]
train() client id: f_00009-1-2 loss: 0.908901  [   96/  118]
train() client id: f_00009-2-0 loss: 0.939804  [   32/  118]
train() client id: f_00009-2-1 loss: 0.801275  [   64/  118]
train() client id: f_00009-2-2 loss: 0.802064  [   96/  118]
train() client id: f_00009-3-0 loss: 0.807788  [   32/  118]
train() client id: f_00009-3-1 loss: 0.899694  [   64/  118]
train() client id: f_00009-3-2 loss: 0.798073  [   96/  118]
At round 75 accuracy: 0.6445623342175066
At round 75 training accuracy: 0.5915492957746479
At round 75 training loss: 0.8201590950126292
update_location
xs = -3.905658 4.200318 395.009024 18.811294 0.979296 3.956410 -357.443192 -336.324852 379.663977 -322.060879 
ys = 387.587959 370.555839 1.320614 -357.455176 349.350187 332.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -21.71142616811743
ys mean: 110.3941425355287
dists_uav = 400.299489 383.834954 407.472543 371.655846 363.382047 347.535483 371.177216 350.877589 393.005605 337.252459 
uav_gains = -121.414410 -120.775226 -121.671511 -120.249570 -119.861663 -119.036008 -120.227852 -119.220043 -121.140310 -118.433021 
uav_gains_db_mean: -120.2029615010245
dists_bs = 272.886266 265.244868 595.881546 566.631566 247.723032 238.601557 254.629443 237.410114 576.573191 225.538013 
bs_gains = -107.774546 -107.429175 -117.271537 -116.659481 -106.598118 -106.141911 -106.932501 -106.081037 -116.870984 -105.457212 
bs_gains_db_mean: -109.72165011573568
Round 76
-------------------------------
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.89822293 1.75469717 0.88790864 0.34389424 2.02220671 0.97420485
 0.41573163 1.22653712 0.89277918 0.79022156]
obj_prev = 10.206404031521155
eta_min = 1.1840901563596855e-105	eta_max = 0.9712061927430241
af = 2.098524592010151	bf = 0.440277105517973	zeta = 2.308377051211166	eta = 0.9090909090909091
af = 2.098524592010151	bf = 0.440277105517973	zeta = 7.043166478768837	eta = 0.29795186559000353
af = 2.098524592010151	bf = 0.440277105517973	zeta = 4.246460455912228	eta = 0.49418206381468455
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.794695110359663	eta = 0.5530153361415251
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.766078122653466	eta = 0.557217488237231
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.765942672970741	eta = 0.5572375296819754
eta = 0.5572375296819754
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [0.0482533  0.10148511 0.04748737 0.0164674  0.11718658 0.05591255
 0.02067998 0.06855032 0.04978515 0.0451896 ]
ene_total = [0.39809468 0.53669908 0.4005926  0.21938841 0.6108545  0.31238457
 0.23943255 0.45484532 0.3348021  0.25884885]
ti_comp = [5.06004778 5.30681242 5.04724229 5.11036031 5.31105174 5.31322071
 5.11119117 5.14610005 5.20576198 5.31628572]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [2.74253247e-07 2.31963433e-06 2.62727860e-07 1.06868990e-08
 3.56576398e-06 3.86983308e-07 2.11585747e-08 7.60242285e-07
 2.84584225e-07 2.04069817e-07]
ene_total = [0.16388001 0.0441132  0.1700956  0.13945761 0.04206153 0.04099331
 0.13905437 0.12211358 0.09315205 0.0395047 ]
optimize_network iter = 0 obj = 0.9944259543834252
eta = 0.5572375296819754
freqs = [ 4768067.39444     9561776.65897993  4704288.96039803  1611177.51564226
 11032332.74977597  5261643.95832219  2023010.29856296  6660414.59689484
  4781734.99264716  4250109.82064523]
eta_min = 0.5572375296819757	eta_max = 0.8837926289249317
af = 2.9732682841315033e-05	bf = 0.440277105517973	zeta = 3.2705951125446536e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [4.47003242e-08 3.78075401e-07 4.28218103e-08 1.74184938e-09
 5.81181107e-07 6.30741094e-08 3.44861970e-09 1.23911301e-07
 4.63841622e-08 3.32611812e-08]
ene_total = [0.79815367 0.21480247 0.82842621 0.67921232 0.20478538 0.19964562
 0.67724819 0.59472538 0.45368094 0.19239915]
ti_comp = [1.07904836 1.325813   1.06624286 1.12936088 1.33005232 1.33222129
 1.13019174 1.16510062 1.22476255 1.33528629]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.68571582e-08 3.50371208e-07 5.55019335e-08 2.06298094e-09
 5.36021441e-07 5.80311479e-08 4.07973874e-09 1.39825878e-07
 4.84709564e-08 3.04966807e-08]
ene_total = [0.62439628 0.16803958 0.64807852 0.53134818 0.16020288 0.15618279
 0.52981165 0.46525429 0.35491489 0.15051391]
optimize_network iter = 1 obj = 3.7887429688072136
eta = 0.8837926289249317
freqs = [4723436.47190019 8085223.78253195 4704288.96039802 1540154.03017325
 9306387.77331266 4433073.47420022 1932725.03801711 6214669.51457857
 4293583.32001691 3574670.85257318]
eta_min = 0.8837926289249345	eta_max = 0.8837926289249313
af = 2.229825334963228e-05	bf = 0.440277105517973	zeta = 2.452807868459551e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [4.38674166e-08 2.70324445e-07 4.28218103e-08 1.59166668e-09
 4.13560520e-07 4.47731935e-08 3.14767049e-09 1.07880876e-07
 3.73971494e-08 2.35293258e-08]
ene_total = [0.79815365 0.21479992 0.82842621 0.67921232 0.20478142 0.19964519
 0.67724818 0.594725   0.45368072 0.19239892]
ti_comp = [1.07904836 1.325813   1.06624286 1.12936088 1.33005232 1.33222129
 1.13019174 1.16510062 1.22476255 1.33528629]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.68571582e-08 3.50371208e-07 5.55019335e-08 2.06298094e-09
 5.36021441e-07 5.80311479e-08 4.07973874e-09 1.39825878e-07
 4.84709564e-08 3.04966807e-08]
ene_total = [0.62439628 0.16803958 0.64807852 0.53134818 0.16020288 0.15618279
 0.52981165 0.46525429 0.35491489 0.15051391]
optimize_network iter = 2 obj = 3.788742968807199
eta = 0.8837926289249313
freqs = [4723436.47190018 8085223.78253194 4704288.96039801 1540154.03017325
 9306387.77331265 4433073.47420021 1932725.03801711 6214669.51457855
 4293583.3200169  3574670.85257317]
Done!
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.62233473e-08 3.46465471e-07 5.48832299e-08 2.03998402e-09
 5.30046183e-07 5.73842500e-08 4.03426016e-09 1.38267179e-07
 4.79306301e-08 3.01567212e-08]
ene_total = [0.03376242 0.00908625 0.03504297 0.02873112 0.0086625  0.00844513
 0.02864803 0.02515728 0.019191   0.0081386 ]
At round 76 energy consumption: 0.20486531204581118
At round 76 eta: 0.8837926289249313
At round 76 a_n: 2.1491184865862607
At round 76 local rounds: 4.045092362532633
At round 76 global rounds: 18.49382243745926
gradient difference: 0.8896058797836304
train() client id: f_00000-0-0 loss: 0.957145  [   32/  126]
train() client id: f_00000-0-1 loss: 1.080778  [   64/  126]
train() client id: f_00000-0-2 loss: 0.687261  [   96/  126]
train() client id: f_00000-1-0 loss: 1.020041  [   32/  126]
train() client id: f_00000-1-1 loss: 0.825688  [   64/  126]
train() client id: f_00000-1-2 loss: 0.781342  [   96/  126]
train() client id: f_00000-2-0 loss: 0.861402  [   32/  126]
train() client id: f_00000-2-1 loss: 0.866789  [   64/  126]
train() client id: f_00000-2-2 loss: 0.731728  [   96/  126]
train() client id: f_00000-3-0 loss: 1.007790  [   32/  126]
train() client id: f_00000-3-1 loss: 0.926279  [   64/  126]
train() client id: f_00000-3-2 loss: 0.875215  [   96/  126]
train() client id: f_00001-0-0 loss: 0.568085  [   32/  265]
train() client id: f_00001-0-1 loss: 0.581842  [   64/  265]
train() client id: f_00001-0-2 loss: 0.783496  [   96/  265]
train() client id: f_00001-0-3 loss: 0.551814  [  128/  265]
train() client id: f_00001-0-4 loss: 0.549915  [  160/  265]
train() client id: f_00001-0-5 loss: 0.592174  [  192/  265]
train() client id: f_00001-0-6 loss: 0.611991  [  224/  265]
train() client id: f_00001-0-7 loss: 0.543715  [  256/  265]
train() client id: f_00001-1-0 loss: 0.502574  [   32/  265]
train() client id: f_00001-1-1 loss: 0.696769  [   64/  265]
train() client id: f_00001-1-2 loss: 0.574118  [   96/  265]
train() client id: f_00001-1-3 loss: 0.505461  [  128/  265]
train() client id: f_00001-1-4 loss: 0.681325  [  160/  265]
train() client id: f_00001-1-5 loss: 0.616310  [  192/  265]
train() client id: f_00001-1-6 loss: 0.559326  [  224/  265]
train() client id: f_00001-1-7 loss: 0.517522  [  256/  265]
train() client id: f_00001-2-0 loss: 0.687643  [   32/  265]
train() client id: f_00001-2-1 loss: 0.552599  [   64/  265]
train() client id: f_00001-2-2 loss: 0.550815  [   96/  265]
train() client id: f_00001-2-3 loss: 0.496545  [  128/  265]
train() client id: f_00001-2-4 loss: 0.679134  [  160/  265]
train() client id: f_00001-2-5 loss: 0.579173  [  192/  265]
train() client id: f_00001-2-6 loss: 0.591711  [  224/  265]
train() client id: f_00001-2-7 loss: 0.597819  [  256/  265]
train() client id: f_00001-3-0 loss: 0.648755  [   32/  265]
train() client id: f_00001-3-1 loss: 0.644991  [   64/  265]
train() client id: f_00001-3-2 loss: 0.670989  [   96/  265]
train() client id: f_00001-3-3 loss: 0.569426  [  128/  265]
train() client id: f_00001-3-4 loss: 0.499455  [  160/  265]
train() client id: f_00001-3-5 loss: 0.517274  [  192/  265]
train() client id: f_00001-3-6 loss: 0.687586  [  224/  265]
train() client id: f_00001-3-7 loss: 0.475859  [  256/  265]
train() client id: f_00002-0-0 loss: 0.968539  [   32/  124]
train() client id: f_00002-0-1 loss: 0.940321  [   64/  124]
train() client id: f_00002-0-2 loss: 1.086842  [   96/  124]
train() client id: f_00002-1-0 loss: 1.003850  [   32/  124]
train() client id: f_00002-1-1 loss: 1.030753  [   64/  124]
train() client id: f_00002-1-2 loss: 0.788129  [   96/  124]
train() client id: f_00002-2-0 loss: 0.843144  [   32/  124]
train() client id: f_00002-2-1 loss: 0.842510  [   64/  124]
train() client id: f_00002-2-2 loss: 1.032912  [   96/  124]
train() client id: f_00002-3-0 loss: 0.832716  [   32/  124]
train() client id: f_00002-3-1 loss: 0.796246  [   64/  124]
train() client id: f_00002-3-2 loss: 1.133582  [   96/  124]
train() client id: f_00003-0-0 loss: 0.514822  [   32/   43]
train() client id: f_00003-1-0 loss: 0.415100  [   32/   43]
train() client id: f_00003-2-0 loss: 0.506331  [   32/   43]
train() client id: f_00003-3-0 loss: 0.492775  [   32/   43]
train() client id: f_00004-0-0 loss: 0.766004  [   32/  306]
train() client id: f_00004-0-1 loss: 0.918489  [   64/  306]
train() client id: f_00004-0-2 loss: 1.004265  [   96/  306]
train() client id: f_00004-0-3 loss: 0.819697  [  128/  306]
train() client id: f_00004-0-4 loss: 0.820171  [  160/  306]
train() client id: f_00004-0-5 loss: 0.878633  [  192/  306]
train() client id: f_00004-0-6 loss: 0.958872  [  224/  306]
train() client id: f_00004-0-7 loss: 0.915124  [  256/  306]
train() client id: f_00004-0-8 loss: 0.721708  [  288/  306]
train() client id: f_00004-1-0 loss: 0.787308  [   32/  306]
train() client id: f_00004-1-1 loss: 0.792106  [   64/  306]
train() client id: f_00004-1-2 loss: 0.756780  [   96/  306]
train() client id: f_00004-1-3 loss: 0.812025  [  128/  306]
train() client id: f_00004-1-4 loss: 0.970219  [  160/  306]
train() client id: f_00004-1-5 loss: 0.886613  [  192/  306]
train() client id: f_00004-1-6 loss: 0.875743  [  224/  306]
train() client id: f_00004-1-7 loss: 0.885381  [  256/  306]
train() client id: f_00004-1-8 loss: 1.081331  [  288/  306]
train() client id: f_00004-2-0 loss: 0.846708  [   32/  306]
train() client id: f_00004-2-1 loss: 0.915734  [   64/  306]
train() client id: f_00004-2-2 loss: 0.897563  [   96/  306]
train() client id: f_00004-2-3 loss: 0.944232  [  128/  306]
train() client id: f_00004-2-4 loss: 0.839866  [  160/  306]
train() client id: f_00004-2-5 loss: 0.901260  [  192/  306]
train() client id: f_00004-2-6 loss: 0.798438  [  224/  306]
train() client id: f_00004-2-7 loss: 0.776832  [  256/  306]
train() client id: f_00004-2-8 loss: 0.884754  [  288/  306]
train() client id: f_00004-3-0 loss: 0.865836  [   32/  306]
train() client id: f_00004-3-1 loss: 0.658014  [   64/  306]
train() client id: f_00004-3-2 loss: 0.757660  [   96/  306]
train() client id: f_00004-3-3 loss: 0.915547  [  128/  306]
train() client id: f_00004-3-4 loss: 0.885688  [  160/  306]
train() client id: f_00004-3-5 loss: 0.892416  [  192/  306]
train() client id: f_00004-3-6 loss: 0.885080  [  224/  306]
train() client id: f_00004-3-7 loss: 1.018438  [  256/  306]
train() client id: f_00004-3-8 loss: 0.961320  [  288/  306]
train() client id: f_00005-0-0 loss: 0.666331  [   32/  146]
train() client id: f_00005-0-1 loss: 0.638855  [   64/  146]
train() client id: f_00005-0-2 loss: 0.343074  [   96/  146]
train() client id: f_00005-0-3 loss: 0.348527  [  128/  146]
train() client id: f_00005-1-0 loss: 0.491584  [   32/  146]
train() client id: f_00005-1-1 loss: 0.681095  [   64/  146]
train() client id: f_00005-1-2 loss: 0.498327  [   96/  146]
train() client id: f_00005-1-3 loss: 0.584342  [  128/  146]
train() client id: f_00005-2-0 loss: 0.733532  [   32/  146]
train() client id: f_00005-2-1 loss: 0.920498  [   64/  146]
train() client id: f_00005-2-2 loss: 0.191999  [   96/  146]
train() client id: f_00005-2-3 loss: 0.445089  [  128/  146]
train() client id: f_00005-3-0 loss: 0.631336  [   32/  146]
train() client id: f_00005-3-1 loss: 0.463441  [   64/  146]
train() client id: f_00005-3-2 loss: 0.803160  [   96/  146]
train() client id: f_00005-3-3 loss: 0.524622  [  128/  146]
train() client id: f_00006-0-0 loss: 0.454075  [   32/   54]
train() client id: f_00006-1-0 loss: 0.487296  [   32/   54]
train() client id: f_00006-2-0 loss: 0.420327  [   32/   54]
train() client id: f_00006-3-0 loss: 0.432604  [   32/   54]
train() client id: f_00007-0-0 loss: 0.516138  [   32/  179]
train() client id: f_00007-0-1 loss: 0.616724  [   64/  179]
train() client id: f_00007-0-2 loss: 0.389475  [   96/  179]
train() client id: f_00007-0-3 loss: 0.766374  [  128/  179]
train() client id: f_00007-0-4 loss: 0.566570  [  160/  179]
train() client id: f_00007-1-0 loss: 0.564171  [   32/  179]
train() client id: f_00007-1-1 loss: 0.432902  [   64/  179]
train() client id: f_00007-1-2 loss: 0.603864  [   96/  179]
train() client id: f_00007-1-3 loss: 0.450769  [  128/  179]
train() client id: f_00007-1-4 loss: 0.775600  [  160/  179]
train() client id: f_00007-2-0 loss: 0.621091  [   32/  179]
train() client id: f_00007-2-1 loss: 0.581644  [   64/  179]
train() client id: f_00007-2-2 loss: 0.738234  [   96/  179]
train() client id: f_00007-2-3 loss: 0.399928  [  128/  179]
train() client id: f_00007-2-4 loss: 0.425897  [  160/  179]
train() client id: f_00007-3-0 loss: 0.429281  [   32/  179]
train() client id: f_00007-3-1 loss: 0.506186  [   64/  179]
train() client id: f_00007-3-2 loss: 0.645023  [   96/  179]
train() client id: f_00007-3-3 loss: 0.642082  [  128/  179]
train() client id: f_00007-3-4 loss: 0.628804  [  160/  179]
train() client id: f_00008-0-0 loss: 0.685369  [   32/  130]
train() client id: f_00008-0-1 loss: 0.695186  [   64/  130]
train() client id: f_00008-0-2 loss: 0.646794  [   96/  130]
train() client id: f_00008-0-3 loss: 0.615201  [  128/  130]
train() client id: f_00008-1-0 loss: 0.711483  [   32/  130]
train() client id: f_00008-1-1 loss: 0.620148  [   64/  130]
train() client id: f_00008-1-2 loss: 0.744494  [   96/  130]
train() client id: f_00008-1-3 loss: 0.539259  [  128/  130]
train() client id: f_00008-2-0 loss: 0.722522  [   32/  130]
train() client id: f_00008-2-1 loss: 0.689600  [   64/  130]
train() client id: f_00008-2-2 loss: 0.691060  [   96/  130]
train() client id: f_00008-2-3 loss: 0.534869  [  128/  130]
train() client id: f_00008-3-0 loss: 0.654502  [   32/  130]
train() client id: f_00008-3-1 loss: 0.594093  [   64/  130]
train() client id: f_00008-3-2 loss: 0.645379  [   96/  130]
train() client id: f_00008-3-3 loss: 0.721608  [  128/  130]
train() client id: f_00009-0-0 loss: 0.981550  [   32/  118]
train() client id: f_00009-0-1 loss: 0.710514  [   64/  118]
train() client id: f_00009-0-2 loss: 0.807959  [   96/  118]
train() client id: f_00009-1-0 loss: 0.776799  [   32/  118]
train() client id: f_00009-1-1 loss: 0.878721  [   64/  118]
train() client id: f_00009-1-2 loss: 0.972735  [   96/  118]
train() client id: f_00009-2-0 loss: 0.942059  [   32/  118]
train() client id: f_00009-2-1 loss: 0.605700  [   64/  118]
train() client id: f_00009-2-2 loss: 0.979539  [   96/  118]
train() client id: f_00009-3-0 loss: 0.899253  [   32/  118]
train() client id: f_00009-3-1 loss: 0.791382  [   64/  118]
train() client id: f_00009-3-2 loss: 0.931529  [   96/  118]
At round 76 accuracy: 0.6445623342175066
At round 76 training accuracy: 0.5902079141515761
At round 76 training loss: 0.8282626125074569
update_location
xs = -3.905658 4.200318 400.009024 18.811294 0.979296 3.956410 -362.443192 -341.324852 384.663977 -327.060879 
ys = 392.587959 375.555839 1.320614 -362.455176 354.350187 337.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -22.21142616811743
ys mean: 111.8941425355287
dists_uav = 405.142642 388.664162 412.321432 376.467288 368.191545 352.326629 375.994625 355.673067 397.837963 342.030452 
uav_gains = -121.589275 -120.970489 -121.838960 -120.463275 -120.090421 -119.298114 -120.442644 -119.474564 -121.323406 -118.720312 
uav_gains_db_mean: -120.42114599426947
dists_bs = 276.799194 268.952409 600.666219 571.332551 251.267193 241.937687 258.235523 240.835651 581.384799 228.829640 
bs_gains = -107.947674 -107.597971 -117.368789 -116.759950 -106.770861 -106.310758 -107.103507 -106.255241 -116.972042 -105.633403 
bs_gains_db_mean: -109.87201962888139
Round 77
-------------------------------
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.7566319  1.47518382 0.74797579 0.2906086  1.70004424 0.81907155
 0.35099723 1.03253353 0.75085927 0.66440907]
obj_prev = 8.588314982517188
eta_min = nan	eta_max = nan
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 1.9404471408469977	eta = 0.9090909090909091
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 6.015300613417023	eta = 0.2932593013524188
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.5977300389560636	eta = 0.49032107362544536
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.2097433156161213	eta = 0.5495900082517465
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.1851865241926465	eta = 0.5538271752429276
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.1850702185308974	eta = 0.5538473987330524
eta = 0.5538473987330524
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [0.04875685 0.10254417 0.04798293 0.01663924 0.1184095  0.05649604
 0.02089579 0.06926569 0.05030469 0.04566118]
ene_total = [0.33763567 0.45238391 0.33971862 0.18710392 0.51488509 0.26328129
 0.20400615 0.38565213 0.28224513 0.2181583 ]
ti_comp = [6.11168561 6.36617567 6.09880235 6.16226391 6.37047552 6.37270362
 6.16308753 6.19819213 6.2639569  6.37579113]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [1.93938935e-07 1.66286030e-06 1.85631348e-07 7.58228597e-09
 2.55679530e-06 2.77515029e-07 1.50127409e-08 5.40635308e-07
 2.02771968e-07 1.46370394e-07]
ene_total = [0.14020895 0.03716612 0.14542564 0.1197279  0.03542864 0.0345172
 0.11939442 0.10518191 0.0785509  0.03326647]
optimize_network iter = 0 obj = 0.848868136074443
eta = 0.5538473987330524
freqs = [3988822.08036756 8053828.5629115  3933799.78027688 1350091.76188237
 9293615.50887199 4432658.36750978 1695237.49554636 5587571.86903341
 4015408.34280281 3580824.62742287]
eta_min = 0.5538473987330529	eta_max = 0.9002731777032497
af = 1.770021082250513e-05	bf = 0.3787142685879881	zeta = 1.9470231904755644e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.12834926e-08 2.68229162e-07 2.99434299e-08 1.22306739e-09
 4.12426142e-07 4.47648089e-08 2.42164354e-09 8.72076598e-08
 3.27083129e-08 2.36104067e-08]
ene_total = [0.68809723 0.18237166 0.71369919 0.58758591 0.17382975 0.1693947
 0.58594921 0.51618999 0.38549919 0.16325868]
ti_comp = [1.09726106 1.35175113 1.08437781 1.14783937 1.35605097 1.35827907
 1.14866299 1.18376759 1.24953236 1.36136658]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.38201293e-08 2.07313558e-07 3.30055808e-08 1.22836293e-09
 3.17172712e-07 3.43371396e-08 2.42929128e-09 8.33124237e-08
 2.86430070e-08 1.80459360e-08]
ene_total = [0.6272565  0.16624547 0.65059478 0.53563224 0.1584582  0.15441683
 0.53414025 0.47054899 0.35141372 0.14882344]
optimize_network iter = 1 obj = 3.7975304096863765
eta = 0.9002731777032497
freqs = [3950315.32823697 6744042.83153091 3933799.78027688 1288719.87693548
 7762767.85413468 3697728.61147668 1617231.97677077 5201849.38696053
 3579043.95899813 2981797.2351148 ]
eta_min = 0.8902239180004755	eta_max = 0.9002731777032479
af = 1.3057149041253039e-05	bf = 0.3787142685879881	zeta = 1.4362863945378344e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.06824073e-08 1.88079677e-07 2.99434299e-08 1.11439939e-09
 2.87746454e-07 3.11514510e-08 2.20390950e-09 7.55829668e-08
 2.59856015e-08 1.63716925e-08]
ene_total = [0.68809721 0.18237007 0.71369919 0.58758591 0.17382727 0.16939443
 0.58594921 0.51618975 0.38549905 0.16325854]
ti_comp = [1.09726106 1.35175113 1.08437781 1.14783937 1.35605097 1.35827907
 1.14866299 1.18376759 1.24953236 1.36136658]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.38201293e-08 2.07313558e-07 3.30055808e-08 1.22836293e-09
 3.17172712e-07 3.43371396e-08 2.42929128e-09 8.33124237e-08
 2.86430070e-08 1.80459360e-08]
ene_total = [0.6272565  0.16624547 0.65059478 0.53563224 0.1584582  0.15441683
 0.53414025 0.47054899 0.35141372 0.14882344]
optimize_network iter = 2 obj = 3.7975304096863085
eta = 0.9002731777032479
freqs = [3950315.32823696 6744042.83153091 3933799.78027686 1288719.87693548
 7762767.85413468 3697728.61147668 1617231.97677076 5201849.38696052
 3579043.95899813 2981797.2351148 ]
Done!
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [2.94934334e-08 1.80791402e-07 2.87830921e-08 1.07121530e-09
 2.76595992e-07 2.99443012e-08 2.11850581e-09 7.26540515e-08
 2.49786335e-08 1.57372731e-08]
ene_total = [0.03462595 0.00917709 0.03591427 0.02956809 0.0087472  0.00852415
 0.02948573 0.02597534 0.01939881 0.00821538]
At round 77 energy consumption: 0.20963201619321536
At round 77 eta: 0.9002731777032479
At round 77 a_n: 1.8065726396169453
At round 77 local rounds: 3.4401009464594003
At round 77 global rounds: 18.115213119307235
gradient difference: 0.9221481084823608
train() client id: f_00000-0-0 loss: 1.012953  [   32/  126]
train() client id: f_00000-0-1 loss: 1.004015  [   64/  126]
train() client id: f_00000-0-2 loss: 1.101154  [   96/  126]
train() client id: f_00000-1-0 loss: 0.984186  [   32/  126]
train() client id: f_00000-1-1 loss: 0.995208  [   64/  126]
train() client id: f_00000-1-2 loss: 0.898269  [   96/  126]
train() client id: f_00000-2-0 loss: 0.876819  [   32/  126]
train() client id: f_00000-2-1 loss: 0.793999  [   64/  126]
train() client id: f_00000-2-2 loss: 0.994337  [   96/  126]
train() client id: f_00001-0-0 loss: 0.587689  [   32/  265]
train() client id: f_00001-0-1 loss: 0.601377  [   64/  265]
train() client id: f_00001-0-2 loss: 0.561197  [   96/  265]
train() client id: f_00001-0-3 loss: 0.678877  [  128/  265]
train() client id: f_00001-0-4 loss: 0.477598  [  160/  265]
train() client id: f_00001-0-5 loss: 0.563056  [  192/  265]
train() client id: f_00001-0-6 loss: 0.578944  [  224/  265]
train() client id: f_00001-0-7 loss: 0.531481  [  256/  265]
train() client id: f_00001-1-0 loss: 0.533206  [   32/  265]
train() client id: f_00001-1-1 loss: 0.612285  [   64/  265]
train() client id: f_00001-1-2 loss: 0.545183  [   96/  265]
train() client id: f_00001-1-3 loss: 0.466961  [  128/  265]
train() client id: f_00001-1-4 loss: 0.520322  [  160/  265]
train() client id: f_00001-1-5 loss: 0.589646  [  192/  265]
train() client id: f_00001-1-6 loss: 0.466525  [  224/  265]
train() client id: f_00001-1-7 loss: 0.805764  [  256/  265]
train() client id: f_00001-2-0 loss: 0.554723  [   32/  265]
train() client id: f_00001-2-1 loss: 0.800259  [   64/  265]
train() client id: f_00001-2-2 loss: 0.694877  [   96/  265]
train() client id: f_00001-2-3 loss: 0.450908  [  128/  265]
train() client id: f_00001-2-4 loss: 0.477265  [  160/  265]
train() client id: f_00001-2-5 loss: 0.502691  [  192/  265]
train() client id: f_00001-2-6 loss: 0.509390  [  224/  265]
train() client id: f_00001-2-7 loss: 0.541022  [  256/  265]
train() client id: f_00002-0-0 loss: 0.648469  [   32/  124]
train() client id: f_00002-0-1 loss: 0.670935  [   64/  124]
train() client id: f_00002-0-2 loss: 0.850987  [   96/  124]
train() client id: f_00002-1-0 loss: 0.792974  [   32/  124]
train() client id: f_00002-1-1 loss: 0.516702  [   64/  124]
train() client id: f_00002-1-2 loss: 0.606575  [   96/  124]
train() client id: f_00002-2-0 loss: 0.726490  [   32/  124]
train() client id: f_00002-2-1 loss: 0.646845  [   64/  124]
train() client id: f_00002-2-2 loss: 0.658520  [   96/  124]
train() client id: f_00003-0-0 loss: 0.720644  [   32/   43]
train() client id: f_00003-1-0 loss: 0.803367  [   32/   43]
train() client id: f_00003-2-0 loss: 0.651594  [   32/   43]
train() client id: f_00004-0-0 loss: 0.698905  [   32/  306]
train() client id: f_00004-0-1 loss: 0.913371  [   64/  306]
train() client id: f_00004-0-2 loss: 0.684121  [   96/  306]
train() client id: f_00004-0-3 loss: 0.749259  [  128/  306]
train() client id: f_00004-0-4 loss: 0.704729  [  160/  306]
train() client id: f_00004-0-5 loss: 0.680990  [  192/  306]
train() client id: f_00004-0-6 loss: 0.751706  [  224/  306]
train() client id: f_00004-0-7 loss: 0.772346  [  256/  306]
train() client id: f_00004-0-8 loss: 0.725503  [  288/  306]
train() client id: f_00004-1-0 loss: 0.706254  [   32/  306]
train() client id: f_00004-1-1 loss: 0.714377  [   64/  306]
train() client id: f_00004-1-2 loss: 0.746451  [   96/  306]
train() client id: f_00004-1-3 loss: 0.874926  [  128/  306]
train() client id: f_00004-1-4 loss: 0.770225  [  160/  306]
train() client id: f_00004-1-5 loss: 0.783384  [  192/  306]
train() client id: f_00004-1-6 loss: 0.697139  [  224/  306]
train() client id: f_00004-1-7 loss: 0.682217  [  256/  306]
train() client id: f_00004-1-8 loss: 0.717038  [  288/  306]
train() client id: f_00004-2-0 loss: 0.799673  [   32/  306]
train() client id: f_00004-2-1 loss: 0.942653  [   64/  306]
train() client id: f_00004-2-2 loss: 0.707056  [   96/  306]
train() client id: f_00004-2-3 loss: 0.702551  [  128/  306]
train() client id: f_00004-2-4 loss: 0.863930  [  160/  306]
train() client id: f_00004-2-5 loss: 0.724461  [  192/  306]
train() client id: f_00004-2-6 loss: 0.745050  [  224/  306]
train() client id: f_00004-2-7 loss: 0.725889  [  256/  306]
train() client id: f_00004-2-8 loss: 0.591183  [  288/  306]
train() client id: f_00005-0-0 loss: 0.602189  [   32/  146]
train() client id: f_00005-0-1 loss: 0.638936  [   64/  146]
train() client id: f_00005-0-2 loss: 0.757474  [   96/  146]
train() client id: f_00005-0-3 loss: 0.810849  [  128/  146]
train() client id: f_00005-1-0 loss: 0.499680  [   32/  146]
train() client id: f_00005-1-1 loss: 0.556018  [   64/  146]
train() client id: f_00005-1-2 loss: 0.807317  [   96/  146]
train() client id: f_00005-1-3 loss: 0.715949  [  128/  146]
train() client id: f_00005-2-0 loss: 0.644550  [   32/  146]
train() client id: f_00005-2-1 loss: 0.580740  [   64/  146]
train() client id: f_00005-2-2 loss: 0.660401  [   96/  146]
train() client id: f_00005-2-3 loss: 0.850784  [  128/  146]
train() client id: f_00006-0-0 loss: 0.497451  [   32/   54]
train() client id: f_00006-1-0 loss: 0.539312  [   32/   54]
train() client id: f_00006-2-0 loss: 0.591946  [   32/   54]
train() client id: f_00007-0-0 loss: 0.547708  [   32/  179]
train() client id: f_00007-0-1 loss: 0.541007  [   64/  179]
train() client id: f_00007-0-2 loss: 0.438302  [   96/  179]
train() client id: f_00007-0-3 loss: 0.752648  [  128/  179]
train() client id: f_00007-0-4 loss: 0.746728  [  160/  179]
train() client id: f_00007-1-0 loss: 0.518584  [   32/  179]
train() client id: f_00007-1-1 loss: 0.704270  [   64/  179]
train() client id: f_00007-1-2 loss: 0.798529  [   96/  179]
train() client id: f_00007-1-3 loss: 0.532310  [  128/  179]
train() client id: f_00007-1-4 loss: 0.450848  [  160/  179]
train() client id: f_00007-2-0 loss: 0.567552  [   32/  179]
train() client id: f_00007-2-1 loss: 0.417190  [   64/  179]
train() client id: f_00007-2-2 loss: 0.888626  [   96/  179]
train() client id: f_00007-2-3 loss: 0.479856  [  128/  179]
train() client id: f_00007-2-4 loss: 0.727878  [  160/  179]
train() client id: f_00008-0-0 loss: 0.668797  [   32/  130]
train() client id: f_00008-0-1 loss: 0.617020  [   64/  130]
train() client id: f_00008-0-2 loss: 0.771084  [   96/  130]
train() client id: f_00008-0-3 loss: 0.645833  [  128/  130]
train() client id: f_00008-1-0 loss: 0.615678  [   32/  130]
train() client id: f_00008-1-1 loss: 0.731981  [   64/  130]
train() client id: f_00008-1-2 loss: 0.703409  [   96/  130]
train() client id: f_00008-1-3 loss: 0.676687  [  128/  130]
train() client id: f_00008-2-0 loss: 0.669699  [   32/  130]
train() client id: f_00008-2-1 loss: 0.759557  [   64/  130]
train() client id: f_00008-2-2 loss: 0.657244  [   96/  130]
train() client id: f_00008-2-3 loss: 0.640043  [  128/  130]
train() client id: f_00009-0-0 loss: 0.621688  [   32/  118]
train() client id: f_00009-0-1 loss: 0.752090  [   64/  118]
train() client id: f_00009-0-2 loss: 0.609997  [   96/  118]
train() client id: f_00009-1-0 loss: 0.664361  [   32/  118]
train() client id: f_00009-1-1 loss: 0.616062  [   64/  118]
train() client id: f_00009-1-2 loss: 0.604705  [   96/  118]
train() client id: f_00009-2-0 loss: 0.691569  [   32/  118]
train() client id: f_00009-2-1 loss: 0.593174  [   64/  118]
train() client id: f_00009-2-2 loss: 0.481203  [   96/  118]
At round 77 accuracy: 0.6445623342175066
At round 77 training accuracy: 0.5975855130784709
At round 77 training loss: 0.8052701036408527
update_location
xs = -3.905658 4.200318 405.009024 18.811294 0.979296 3.956410 -367.443192 -346.324852 389.663977 -332.060879 
ys = 397.587959 380.555839 1.320614 -367.455176 359.350187 342.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -22.71142616811743
ys mean: 113.3941425355287
dists_uav = 409.989560 393.497635 417.173889 381.283584 373.006053 357.123501 380.816740 360.474103 402.674415 346.814704 
uav_gains = -121.759041 -121.159231 -122.001820 -120.669193 -120.310381 -119.549417 -120.649568 -119.718705 -121.500836 -118.995576 
uav_gains_db_mean: -120.6313768766259
dists_bs = 280.746636 272.701223 605.454372 576.038571 254.860166 245.330361 261.887413 244.315491 586.199560 232.182284 
bs_gains = -108.119867 -107.766298 -117.465339 -116.859703 -106.943514 -106.480095 -107.274269 -106.429687 -117.072333 -105.810273 
bs_gains_db_mean: -110.02213778805712
Round 78
-------------------------------
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.61445089 1.1956097  0.60744834 0.2367497  1.37782529 0.66388603
 0.28568902 0.83797174 0.60879864 0.53854598]
obj_prev = 6.96697533321086
eta_min = nan	eta_max = nan
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 1.572517230482826	eta = 0.909090909090909
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 4.952035710772682	eta = 0.2886815043580717
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.9382966582345253	eta = 0.48652715668257346
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.6172340285407683	eta = 0.5462106571408896
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.596934549245399	eta = 0.5504802264023727
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.5968383965379944	eta = 0.5505006089430082
eta = 0.5505006089430082
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [0.049257   0.10359608 0.04847515 0.01680993 0.11962415 0.05707558
 0.02111014 0.06997622 0.05082072 0.04612957]
ene_total = [0.27603738 0.36760475 0.27771398 0.15379303 0.4183885  0.21392216
 0.16753295 0.31519772 0.22938986 0.17725807]
ti_comp = [7.65842292 7.92067487 7.64545672 7.70927839 7.9250345  7.92732068
 7.71009494 7.74538131 7.81728173 7.93043026]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.27351857e-07 1.10760673e-06 1.21795299e-07 4.99516971e-09
 1.70346924e-06 1.84917516e-07 9.89086254e-09 3.56981594e-07
 1.34242758e-07 9.75494579e-08]
ene_total = [0.11560694 0.03019425 0.11983004 0.09904284 0.02877625 0.02802669
 0.0987769  0.0872852  0.06386639 0.02701362]
optimize_network iter = 0 obj = 0.6984191265921221
eta = 0.5505006089430082
freqs = [3215871.18363491 6539599.20133967 3170192.94839821 1090240.14955988
 7547232.16704463 3599928.48687781 1368993.7919406  4517286.87955715
 3250536.39445521 2908390.39625094]
eta_min = 0.5505006089430086	eta_max = 0.917509374892285
af = 9.439588067529733e-06	bf = 0.3139333373586834	zeta = 1.0383546874282708e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [2.03340174e-08 1.76849361e-07 1.94468128e-08 7.97568798e-10
 2.71989541e-07 2.95254116e-08 1.57925432e-09 5.69985401e-08
 2.14342739e-08 1.55755277e-08]
ene_total = [0.57161603 0.14928029 0.59249717 0.48971657 0.14226095 0.1385753
 0.48840159 0.43157626 0.31578514 0.13356732]
ti_comp = [1.26571924 1.52797119 1.25275304 1.31657471 1.53233081 1.534617
 1.31739126 1.35267763 1.42457805 1.53772658]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
optimize_network iter = 1 obj = 3.4532738111991463
eta = 0.909090909090909
freqs = [3106837.34109527 5412726.23484139 3089168.32072196 1019314.72951421
 6232384.57566306 2969191.46464435 1279276.24484534 4129943.16102444
 2848016.20722417 2394904.72079761]
eta_min = 0.9009430532547689	eta_max = 0.9090909090909057
af = 6.772388892494084e-06	bf = 0.3139333373586834	zeta = 7.449627781743493e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
ti_comp = [1.26571924 1.52797119 1.25275304 1.31657471 1.53233081 1.534617
 1.31739126 1.35267763 1.42457805 1.53772658]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
optimize_network iter = 2 obj = 3.4532738111990233
eta = 0.9090909090909057
freqs = [3106837.34109525 5412726.23484139 3089168.32072194 1019314.7295142
 6232384.57566307 2969191.46464435 1279276.24484533 4129943.16102442
 2848016.20722416 2394904.72079761]
Done!
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.82431083e-08 1.16457981e-07 1.77499073e-08 6.70156624e-10
 1.78287614e-07 1.93072545e-08 1.32560365e-09 4.57965159e-08
 1.58168328e-08 1.01519565e-08]
ene_total = [0.03549463 0.00926953 0.03679125 0.03040907 0.00883363 0.00860486
 0.03032741 0.02679882 0.01960875 0.00829389]
At round 78 energy consumption: 0.21443183922587233
At round 78 eta: 0.9090909090909057
At round 78 a_n: 1.4640267926476263
At round 78 local rounds: 3.120939520577876
At round 78 global rounds: 16.104294719123292
gradient difference: 1.024619698524475
train() client id: f_00000-0-0 loss: 0.945894  [   32/  126]
train() client id: f_00000-0-1 loss: 0.744353  [   64/  126]
train() client id: f_00000-0-2 loss: 0.806385  [   96/  126]
train() client id: f_00000-1-0 loss: 0.838822  [   32/  126]
train() client id: f_00000-1-1 loss: 0.735445  [   64/  126]
train() client id: f_00000-1-2 loss: 0.843764  [   96/  126]
train() client id: f_00000-2-0 loss: 0.722953  [   32/  126]
train() client id: f_00000-2-1 loss: 0.756613  [   64/  126]
train() client id: f_00000-2-2 loss: 0.903580  [   96/  126]
train() client id: f_00001-0-0 loss: 0.488126  [   32/  265]
train() client id: f_00001-0-1 loss: 0.725587  [   64/  265]
train() client id: f_00001-0-2 loss: 0.610276  [   96/  265]
train() client id: f_00001-0-3 loss: 0.451739  [  128/  265]
train() client id: f_00001-0-4 loss: 0.521365  [  160/  265]
train() client id: f_00001-0-5 loss: 0.525253  [  192/  265]
train() client id: f_00001-0-6 loss: 0.472398  [  224/  265]
train() client id: f_00001-0-7 loss: 0.577736  [  256/  265]
train() client id: f_00001-1-0 loss: 0.674228  [   32/  265]
train() client id: f_00001-1-1 loss: 0.566982  [   64/  265]
train() client id: f_00001-1-2 loss: 0.561538  [   96/  265]
train() client id: f_00001-1-3 loss: 0.598356  [  128/  265]
train() client id: f_00001-1-4 loss: 0.490428  [  160/  265]
train() client id: f_00001-1-5 loss: 0.473923  [  192/  265]
train() client id: f_00001-1-6 loss: 0.525819  [  224/  265]
train() client id: f_00001-1-7 loss: 0.453640  [  256/  265]
train() client id: f_00001-2-0 loss: 0.416619  [   32/  265]
train() client id: f_00001-2-1 loss: 0.544661  [   64/  265]
train() client id: f_00001-2-2 loss: 0.600950  [   96/  265]
train() client id: f_00001-2-3 loss: 0.547318  [  128/  265]
train() client id: f_00001-2-4 loss: 0.532220  [  160/  265]
train() client id: f_00001-2-5 loss: 0.505396  [  192/  265]
train() client id: f_00001-2-6 loss: 0.551304  [  224/  265]
train() client id: f_00001-2-7 loss: 0.671127  [  256/  265]
train() client id: f_00002-0-0 loss: 0.963859  [   32/  124]
train() client id: f_00002-0-1 loss: 1.009526  [   64/  124]
train() client id: f_00002-0-2 loss: 1.078201  [   96/  124]
train() client id: f_00002-1-0 loss: 0.986717  [   32/  124]
train() client id: f_00002-1-1 loss: 1.097964  [   64/  124]
train() client id: f_00002-1-2 loss: 0.879782  [   96/  124]
train() client id: f_00002-2-0 loss: 1.017073  [   32/  124]
train() client id: f_00002-2-1 loss: 0.826252  [   64/  124]
train() client id: f_00002-2-2 loss: 1.012535  [   96/  124]
train() client id: f_00003-0-0 loss: 0.630158  [   32/   43]
train() client id: f_00003-1-0 loss: 0.479184  [   32/   43]
train() client id: f_00003-2-0 loss: 0.552266  [   32/   43]
train() client id: f_00004-0-0 loss: 0.814911  [   32/  306]
train() client id: f_00004-0-1 loss: 0.983110  [   64/  306]
train() client id: f_00004-0-2 loss: 1.073836  [   96/  306]
train() client id: f_00004-0-3 loss: 0.981604  [  128/  306]
train() client id: f_00004-0-4 loss: 1.092750  [  160/  306]
train() client id: f_00004-0-5 loss: 1.024481  [  192/  306]
train() client id: f_00004-0-6 loss: 0.984792  [  224/  306]
train() client id: f_00004-0-7 loss: 0.937287  [  256/  306]
train() client id: f_00004-0-8 loss: 1.051341  [  288/  306]
train() client id: f_00004-1-0 loss: 1.161600  [   32/  306]
train() client id: f_00004-1-1 loss: 0.824377  [   64/  306]
train() client id: f_00004-1-2 loss: 0.968938  [   96/  306]
train() client id: f_00004-1-3 loss: 0.981754  [  128/  306]
train() client id: f_00004-1-4 loss: 0.966176  [  160/  306]
train() client id: f_00004-1-5 loss: 1.005709  [  192/  306]
train() client id: f_00004-1-6 loss: 1.029043  [  224/  306]
train() client id: f_00004-1-7 loss: 1.022906  [  256/  306]
train() client id: f_00004-1-8 loss: 0.923413  [  288/  306]
train() client id: f_00004-2-0 loss: 0.873775  [   32/  306]
train() client id: f_00004-2-1 loss: 0.973777  [   64/  306]
train() client id: f_00004-2-2 loss: 0.963356  [   96/  306]
train() client id: f_00004-2-3 loss: 0.892877  [  128/  306]
train() client id: f_00004-2-4 loss: 0.989715  [  160/  306]
train() client id: f_00004-2-5 loss: 0.943404  [  192/  306]
train() client id: f_00004-2-6 loss: 1.066085  [  224/  306]
train() client id: f_00004-2-7 loss: 1.134155  [  256/  306]
train() client id: f_00004-2-8 loss: 0.981704  [  288/  306]
train() client id: f_00005-0-0 loss: 0.348960  [   32/  146]
train() client id: f_00005-0-1 loss: 0.205457  [   64/  146]
train() client id: f_00005-0-2 loss: 0.675425  [   96/  146]
train() client id: f_00005-0-3 loss: 0.671214  [  128/  146]
train() client id: f_00005-1-0 loss: 0.466195  [   32/  146]
train() client id: f_00005-1-1 loss: 0.393059  [   64/  146]
train() client id: f_00005-1-2 loss: 0.721072  [   96/  146]
train() client id: f_00005-1-3 loss: 0.396705  [  128/  146]
train() client id: f_00005-2-0 loss: 0.425354  [   32/  146]
train() client id: f_00005-2-1 loss: 0.598392  [   64/  146]
train() client id: f_00005-2-2 loss: 0.348268  [   96/  146]
train() client id: f_00005-2-3 loss: 0.639933  [  128/  146]
train() client id: f_00006-0-0 loss: 0.405298  [   32/   54]
train() client id: f_00006-1-0 loss: 0.488103  [   32/   54]
train() client id: f_00006-2-0 loss: 0.357938  [   32/   54]
train() client id: f_00007-0-0 loss: 0.457232  [   32/  179]
train() client id: f_00007-0-1 loss: 0.441466  [   64/  179]
train() client id: f_00007-0-2 loss: 0.510509  [   96/  179]
train() client id: f_00007-0-3 loss: 0.531662  [  128/  179]
train() client id: f_00007-0-4 loss: 0.679800  [  160/  179]
train() client id: f_00007-1-0 loss: 0.416616  [   32/  179]
train() client id: f_00007-1-1 loss: 0.300530  [   64/  179]
train() client id: f_00007-1-2 loss: 0.747528  [   96/  179]
train() client id: f_00007-1-3 loss: 0.341803  [  128/  179]
train() client id: f_00007-1-4 loss: 0.720789  [  160/  179]
train() client id: f_00007-2-0 loss: 0.371206  [   32/  179]
train() client id: f_00007-2-1 loss: 0.350791  [   64/  179]
train() client id: f_00007-2-2 loss: 0.668789  [   96/  179]
train() client id: f_00007-2-3 loss: 0.559630  [  128/  179]
train() client id: f_00007-2-4 loss: 0.526666  [  160/  179]
train() client id: f_00008-0-0 loss: 0.777862  [   32/  130]
train() client id: f_00008-0-1 loss: 0.765460  [   64/  130]
train() client id: f_00008-0-2 loss: 0.739852  [   96/  130]
train() client id: f_00008-0-3 loss: 0.712556  [  128/  130]
train() client id: f_00008-1-0 loss: 0.729358  [   32/  130]
train() client id: f_00008-1-1 loss: 0.711828  [   64/  130]
train() client id: f_00008-1-2 loss: 0.869437  [   96/  130]
train() client id: f_00008-1-3 loss: 0.693638  [  128/  130]
train() client id: f_00008-2-0 loss: 0.659719  [   32/  130]
train() client id: f_00008-2-1 loss: 0.815266  [   64/  130]
train() client id: f_00008-2-2 loss: 0.791003  [   96/  130]
train() client id: f_00008-2-3 loss: 0.697297  [  128/  130]
train() client id: f_00009-0-0 loss: 0.640954  [   32/  118]
train() client id: f_00009-0-1 loss: 0.828238  [   64/  118]
train() client id: f_00009-0-2 loss: 1.192388  [   96/  118]
train() client id: f_00009-1-0 loss: 0.959427  [   32/  118]
train() client id: f_00009-1-1 loss: 1.063128  [   64/  118]
train() client id: f_00009-1-2 loss: 0.990426  [   96/  118]
train() client id: f_00009-2-0 loss: 0.967959  [   32/  118]
train() client id: f_00009-2-1 loss: 0.897561  [   64/  118]
train() client id: f_00009-2-2 loss: 0.965280  [   96/  118]
At round 78 accuracy: 0.6445623342175066
At round 78 training accuracy: 0.5942320590207915
At round 78 training loss: 0.8041507286978014
update_location
xs = -3.905658 4.200318 410.009024 18.811294 0.979296 3.956410 -372.443192 -351.324852 394.663977 -337.060879 
ys = 402.587959 385.555839 1.320614 -372.455176 364.350187 347.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -23.21142616811743
ys mean: 114.8941425355287
dists_uav = 414.840113 398.335220 422.029790 386.104549 377.825380 361.925872 385.643387 365.280478 407.514815 351.604960 
uav_gains = -121.924061 -121.341910 -122.160400 -120.867858 -120.522125 -119.790526 -120.849162 -119.953078 -121.672992 -119.259360 
uav_gains_db_mean: -120.8341471378618
dists_bs = 284.727155 276.489629 610.245924 580.749505 258.499915 248.777264 265.583224 247.847348 591.017398 235.593339 
bs_gains = -108.291068 -107.934067 -117.561196 -116.958747 -107.115950 -106.649758 -107.444677 -106.604219 -117.171867 -105.987623 
bs_gains_db_mean: -110.17191725290937
Round 79
-------------------------------
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.47167495 0.91597325 0.46632083 0.18231339 1.05554823 0.50864652
 0.2198029  0.64284551 0.46659513 0.41263047]
obj_prev = 5.342351171013073
eta_min = 2.6352711032412936e-201	eta_max = 0.9959613407486563
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 1.2045873201186579	eta = 0.909090909090909
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 3.853002462740199	eta = 0.28421455540603213
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.268192123899988	eta = 0.48279833546161194
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0171814832442236	eta = 0.5428759836545998
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0013314548574064	eta = 0.5471754212767701
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0012564070127876	eta = 0.5471959405544855
eta = 0.5471959405544855
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [0.04975385 0.10464104 0.04896411 0.01697949 0.12083078 0.05765129
 0.02132308 0.07068206 0.05133334 0.04659488]
ene_total = [0.21330398 0.28236012 0.21458127 0.11946231 0.32136361 0.16430363
 0.13001973 0.24348682 0.17623076 0.13614417]
ti_comp = [10.15400802 10.42406192 10.14095348 10.20515528 10.42848067 10.43082397
 10.20596501 10.24142523 10.31948805 10.43395522]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [7.46595073e-08 6.59040563e-07 7.13437549e-08 2.93774855e-09
 1.01384295e-06 1.10070217e-07 5.81730210e-09 2.10419888e-07
 7.93893490e-08 5.80759835e-08]
ene_total = [0.09007648 0.02319242 0.09330975 0.07740842 0.02209888 0.02151627
 0.07720788 0.06842579 0.04909128 0.02074061]
optimize_network iter = 0 obj = 0.5430677938747402
eta = 0.5471959405544855
freqs = [2449961.18756652 5019206.37355413 2414176.71155869  831907.42340227
 5793307.05286581 2763505.9294842  1044638.0630291  3450792.08881117
 2487203.73595765 2232848.20767479]
eta_min = 0.5471959405544858	eta_max = 0.9355133020148048
af = 4.251199729612188e-06	bf = 0.24590073836681936	zeta = 4.6763197025734075e-06	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [1.18016929e-08 1.04176878e-07 1.12775602e-08 4.64380326e-10
 1.60261749e-07 1.73991893e-08 9.19561561e-10 3.32618174e-08
 1.25493558e-08 9.18027656e-09]
ene_total = [0.44865618 0.11551099 0.46476064 0.3855593  0.11006058 0.10716806
 0.3845604  0.34081604 0.24451523 0.10330516]
ti_comp = [1.74794103 2.01799493 1.73488648 1.79908829 2.02241368 2.02475698
 1.79989802 1.83535824 1.91342105 2.02788823]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
optimize_network iter = 1 obj = 2.7049111966856447
eta = 0.909090909090909
freqs = [2249723.37868762 4098369.93259797 2230673.32764145  745935.59638761
 4722117.45608519 2250428.94068191  936334.90519755 3043809.99590222
 2120401.76543593 1816031.36876833]
eta_min = 0.9090909090910358	eta_max = 0.909090909090905
af = 2.9326740829202082e-06	bf = 0.24590073836681936	zeta = 3.2259414912122293e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
ti_comp = [1.74794103 2.01799493 1.73488648 1.79908829 2.02241368 2.02475698
 1.79989802 1.83535824 1.91342105 2.02788823]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
optimize_network iter = 2 obj = 2.7049111966855257
eta = 0.909090909090905
freqs = [2249723.37868761 4098369.93259797 2230673.32764143  745935.59638761
 4722117.4560852  2250428.94068191  936334.90519755 3043809.99590221
 2120401.76543593 1816031.36876833]
Done!
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.56577248e-09 6.67666285e-08 9.25518050e-09 3.58890845e-10
 1.02349625e-07 1.10911026e-08 7.10145674e-10 2.48759324e-08
 8.76740211e-09 5.83740678e-09]
ene_total = [0.03636881 0.00936347 0.03767426 0.03125407 0.00892163 0.00868721
 0.0311731  0.0276271  0.0198208  0.00837408]
At round 79 energy consumption: 0.2192645430104472
At round 79 eta: 0.909090909090905
At round 79 a_n: 1.1214809456783108
At round 79 local rounds: 3.120939520577902
At round 79 global rounds: 12.336290402460858
gradient difference: 1.0833479166030884
train() client id: f_00000-0-0 loss: 0.724536  [   32/  126]
train() client id: f_00000-0-1 loss: 0.819685  [   64/  126]
train() client id: f_00000-0-2 loss: 0.722216  [   96/  126]
train() client id: f_00000-1-0 loss: 0.788541  [   32/  126]
train() client id: f_00000-1-1 loss: 0.776884  [   64/  126]
train() client id: f_00000-1-2 loss: 0.808894  [   96/  126]
train() client id: f_00000-2-0 loss: 0.804479  [   32/  126]
train() client id: f_00000-2-1 loss: 0.806637  [   64/  126]
train() client id: f_00000-2-2 loss: 0.912557  [   96/  126]
train() client id: f_00001-0-0 loss: 0.437124  [   32/  265]
train() client id: f_00001-0-1 loss: 0.479659  [   64/  265]
train() client id: f_00001-0-2 loss: 0.435409  [   96/  265]
train() client id: f_00001-0-3 loss: 0.624780  [  128/  265]
train() client id: f_00001-0-4 loss: 0.396556  [  160/  265]
train() client id: f_00001-0-5 loss: 0.565553  [  192/  265]
train() client id: f_00001-0-6 loss: 0.453617  [  224/  265]
train() client id: f_00001-0-7 loss: 0.467290  [  256/  265]
train() client id: f_00001-1-0 loss: 0.616660  [   32/  265]
train() client id: f_00001-1-1 loss: 0.593155  [   64/  265]
train() client id: f_00001-1-2 loss: 0.382510  [   96/  265]
train() client id: f_00001-1-3 loss: 0.451142  [  128/  265]
train() client id: f_00001-1-4 loss: 0.504713  [  160/  265]
train() client id: f_00001-1-5 loss: 0.426470  [  192/  265]
train() client id: f_00001-1-6 loss: 0.433981  [  224/  265]
train() client id: f_00001-1-7 loss: 0.437990  [  256/  265]
train() client id: f_00001-2-0 loss: 0.468382  [   32/  265]
train() client id: f_00001-2-1 loss: 0.553724  [   64/  265]
train() client id: f_00001-2-2 loss: 0.570122  [   96/  265]
train() client id: f_00001-2-3 loss: 0.419180  [  128/  265]
train() client id: f_00001-2-4 loss: 0.574188  [  160/  265]
train() client id: f_00001-2-5 loss: 0.365879  [  192/  265]
train() client id: f_00001-2-6 loss: 0.426196  [  224/  265]
train() client id: f_00001-2-7 loss: 0.408828  [  256/  265]
train() client id: f_00002-0-0 loss: 0.863382  [   32/  124]
train() client id: f_00002-0-1 loss: 0.786747  [   64/  124]
train() client id: f_00002-0-2 loss: 0.906305  [   96/  124]
train() client id: f_00002-1-0 loss: 1.079924  [   32/  124]
train() client id: f_00002-1-1 loss: 0.784307  [   64/  124]
train() client id: f_00002-1-2 loss: 0.811227  [   96/  124]
train() client id: f_00002-2-0 loss: 1.003402  [   32/  124]
train() client id: f_00002-2-1 loss: 0.871291  [   64/  124]
train() client id: f_00002-2-2 loss: 0.816448  [   96/  124]
train() client id: f_00003-0-0 loss: 0.678382  [   32/   43]
train() client id: f_00003-1-0 loss: 0.536607  [   32/   43]
train() client id: f_00003-2-0 loss: 0.790215  [   32/   43]
train() client id: f_00004-0-0 loss: 0.768765  [   32/  306]
train() client id: f_00004-0-1 loss: 0.823312  [   64/  306]
train() client id: f_00004-0-2 loss: 0.928777  [   96/  306]
train() client id: f_00004-0-3 loss: 0.931587  [  128/  306]
train() client id: f_00004-0-4 loss: 0.744574  [  160/  306]
train() client id: f_00004-0-5 loss: 0.969414  [  192/  306]
train() client id: f_00004-0-6 loss: 0.974060  [  224/  306]
train() client id: f_00004-0-7 loss: 0.798788  [  256/  306]
train() client id: f_00004-0-8 loss: 0.846675  [  288/  306]
train() client id: f_00004-1-0 loss: 0.951113  [   32/  306]
train() client id: f_00004-1-1 loss: 0.874652  [   64/  306]
train() client id: f_00004-1-2 loss: 0.862509  [   96/  306]
train() client id: f_00004-1-3 loss: 0.963353  [  128/  306]
train() client id: f_00004-1-4 loss: 0.933292  [  160/  306]
train() client id: f_00004-1-5 loss: 0.883132  [  192/  306]
train() client id: f_00004-1-6 loss: 0.780847  [  224/  306]
train() client id: f_00004-1-7 loss: 0.716000  [  256/  306]
train() client id: f_00004-1-8 loss: 0.746842  [  288/  306]
train() client id: f_00004-2-0 loss: 0.827512  [   32/  306]
train() client id: f_00004-2-1 loss: 0.926978  [   64/  306]
train() client id: f_00004-2-2 loss: 0.874249  [   96/  306]
train() client id: f_00004-2-3 loss: 0.929845  [  128/  306]
train() client id: f_00004-2-4 loss: 0.849319  [  160/  306]
train() client id: f_00004-2-5 loss: 0.835720  [  192/  306]
train() client id: f_00004-2-6 loss: 0.784162  [  224/  306]
train() client id: f_00004-2-7 loss: 0.896104  [  256/  306]
train() client id: f_00004-2-8 loss: 0.786835  [  288/  306]
train() client id: f_00005-0-0 loss: 0.422538  [   32/  146]
train() client id: f_00005-0-1 loss: 0.710656  [   64/  146]
train() client id: f_00005-0-2 loss: 0.467427  [   96/  146]
train() client id: f_00005-0-3 loss: 0.331446  [  128/  146]
train() client id: f_00005-1-0 loss: 0.508536  [   32/  146]
train() client id: f_00005-1-1 loss: 0.739788  [   64/  146]
train() client id: f_00005-1-2 loss: 0.323768  [   96/  146]
train() client id: f_00005-1-3 loss: 0.193291  [  128/  146]
train() client id: f_00005-2-0 loss: 0.664139  [   32/  146]
train() client id: f_00005-2-1 loss: 0.584014  [   64/  146]
train() client id: f_00005-2-2 loss: 0.364797  [   96/  146]
train() client id: f_00005-2-3 loss: 0.294914  [  128/  146]
train() client id: f_00006-0-0 loss: 0.525042  [   32/   54]
train() client id: f_00006-1-0 loss: 0.605948  [   32/   54]
train() client id: f_00006-2-0 loss: 0.593273  [   32/   54]
train() client id: f_00007-0-0 loss: 0.944090  [   32/  179]
train() client id: f_00007-0-1 loss: 0.588615  [   64/  179]
train() client id: f_00007-0-2 loss: 0.560217  [   96/  179]
train() client id: f_00007-0-3 loss: 0.450788  [  128/  179]
train() client id: f_00007-0-4 loss: 0.667617  [  160/  179]
train() client id: f_00007-1-0 loss: 0.561340  [   32/  179]
train() client id: f_00007-1-1 loss: 0.476062  [   64/  179]
train() client id: f_00007-1-2 loss: 0.834090  [   96/  179]
train() client id: f_00007-1-3 loss: 0.646543  [  128/  179]
train() client id: f_00007-1-4 loss: 0.705924  [  160/  179]
train() client id: f_00007-2-0 loss: 0.657164  [   32/  179]
train() client id: f_00007-2-1 loss: 0.477342  [   64/  179]
train() client id: f_00007-2-2 loss: 0.659844  [   96/  179]
train() client id: f_00007-2-3 loss: 0.601445  [  128/  179]
train() client id: f_00007-2-4 loss: 0.662039  [  160/  179]
train() client id: f_00008-0-0 loss: 0.773847  [   32/  130]
train() client id: f_00008-0-1 loss: 0.787883  [   64/  130]
train() client id: f_00008-0-2 loss: 0.800592  [   96/  130]
train() client id: f_00008-0-3 loss: 0.712494  [  128/  130]
train() client id: f_00008-1-0 loss: 0.738651  [   32/  130]
train() client id: f_00008-1-1 loss: 0.759334  [   64/  130]
train() client id: f_00008-1-2 loss: 0.786285  [   96/  130]
train() client id: f_00008-1-3 loss: 0.830267  [  128/  130]
train() client id: f_00008-2-0 loss: 0.772251  [   32/  130]
train() client id: f_00008-2-1 loss: 0.787260  [   64/  130]
train() client id: f_00008-2-2 loss: 0.809289  [   96/  130]
train() client id: f_00008-2-3 loss: 0.729627  [  128/  130]
train() client id: f_00009-0-0 loss: 0.840158  [   32/  118]
train() client id: f_00009-0-1 loss: 0.670401  [   64/  118]
train() client id: f_00009-0-2 loss: 0.559617  [   96/  118]
train() client id: f_00009-1-0 loss: 0.719817  [   32/  118]
train() client id: f_00009-1-1 loss: 0.756591  [   64/  118]
train() client id: f_00009-1-2 loss: 0.677482  [   96/  118]
train() client id: f_00009-2-0 loss: 0.672623  [   32/  118]
train() client id: f_00009-2-1 loss: 0.760562  [   64/  118]
train() client id: f_00009-2-2 loss: 0.811363  [   96/  118]
At round 79 accuracy: 0.6445623342175066
At round 79 training accuracy: 0.5975855130784709
At round 79 training loss: 0.8172342172637498
update_location
xs = -3.905658 4.200318 415.009024 18.811294 0.979296 3.956410 -377.443192 -356.324852 399.663977 -342.060879 
ys = 407.587959 390.555839 1.320614 -377.455176 369.350187 352.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -23.71142616811743
ys mean: 116.3941425355287
dists_uav = 419.694173 403.176767 426.889018 390.930012 382.649343 366.733525 390.474396 370.091984 412.359024 356.400977 
uav_gains = -122.084655 -121.518948 -122.314984 -121.059772 -120.726208 -120.022054 -121.041934 -120.178291 -121.840240 -119.512250 
uav_gains_db_mean: -121.0299335127232
dists_bs = 288.739385 280.316024 615.040793 585.465233 262.184492 252.276176 269.321148 251.429028 595.838237 239.060307 
bs_gains = -108.461228 -108.101201 -117.656369 -117.057090 -107.288055 -106.819594 -107.614632 -106.778691 -117.270653 -106.165268 
bs_gains_db_mean: -110.32127818213232
Round 80
-------------------------------
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.32829867 0.63627291 0.3245873  0.12729558 0.73321141 0.35335129
 0.15333479 0.44714953 0.32424655 0.28666076]
obj_prev = 3.7144087846743608
eta_min = 2.74115185868049e-289	eta_max = 0.9972962516128097
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 0.8366574097544858	eta = 0.9090909090909091
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 2.7178337538132036	eta = 0.2798543671643672
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.5874476546409413	eta = 0.4791324255686323
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.4095987989406396	eta = 0.539584487304448
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.398385343651186	eta = 0.5439113393776205
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.39833229498431	eta = 0.5439319737944585
eta = 0.5439319737944585
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [0.05024753 0.10567934 0.04944995 0.01714797 0.12202973 0.05822333
 0.02153466 0.0713834  0.05184269 0.04705721]
ene_total = [0.14943821 0.1966487  0.15032166 0.08411788 0.22380943 0.11442236
 0.09147288 0.17052575 0.12276251 0.0948129 ]
ti_comp = [14.85036606 15.12826574 14.83721764 14.90182223 15.13274303 15.13514254
 14.90262547 14.93825677 15.02250443 15.13829514]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [3.59542268e-08 3.22308656e-07 3.43299174e-08 1.41918577e-09
 4.95954233e-07 5.38515457e-08 2.81040133e-09 1.01876001e-07
 3.85885102e-08 2.84187337e-08]
ene_total = [0.06361869 0.01615569 0.06586436 0.05483025 0.01539129 0.01498072
 0.05469306 0.04860763 0.03421856 0.01444223]
optimize_network iter = 0 obj = 0.38280246883118135
eta = 0.5439319737944585
freqs = [1691794.45393543 3492777.65574166 1666416.00768811  575364.79907189
 4031976.3853505  1923448.44978197  722512.19806255 2389281.50597988
 1725501.02801081 1554244.18120876]
eta_min = 0.5439319737944591	eta_max = 0.9542982433752891
af = 1.4269447035111872e-06	bf = 0.17458309747545425	zeta = 1.569639173862306e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [5.62757246e-09 5.04479022e-08 5.37333478e-09 2.22131623e-10
 7.76269894e-08 8.42886922e-09 4.39885336e-10 1.59456795e-08
 6.03989174e-09 4.44811354e-09]
ene_total = [0.3191587  0.08104669 0.33042464 0.27506954 0.07721065 0.07515409
 0.2743813  0.24385151 0.17166562 0.07245282]
ti_comp = [2.66191833 2.93981801 2.64876991 2.7133745  2.9442953  2.9466948
 2.71417774 2.74980904 2.8340567  2.9498474 ]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
optimize_network iter = 1 obj = 1.9204150651233343
eta = 0.9090909090909091
freqs = [1477274.39950027 2813265.88968522 1461042.34829177  494588.56272364
 3243585.97051685 1546333.09614372  620927.40517717 2031588.97688467
 1431594.99214584 1248440.38657298]
eta_min = 0.9090909090909186	eta_max = 0.9090909090909067
af = 9.470450164325744e-07	bf = 0.17458309747545425	zeta = 1.0417495180758319e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
ti_comp = [2.66191833 2.93981801 2.64876991 2.7133745  2.9442953  2.9466948
 2.71417774 2.74980904 2.8340567  2.9498474 ]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
optimize_network iter = 2 obj = 1.9204150651232854
eta = 0.9090909090909067
freqs = [1477274.39950027 2813265.88968522 1461042.34829176  494588.56272364
 3243585.97051684 1546333.09614371  620927.40517717 2031588.97688466
 1431594.99214584 1248440.38657297]
Done!
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.12462194e-09 3.14599982e-08 3.97043922e-09 1.57778511e-10
 4.82907013e-08 5.23660984e-09 3.12296182e-10 1.10819449e-08
 3.99645523e-09 2.75872802e-09]
ene_total = [0.03724884 0.0094589  0.03856368 0.03210322 0.00901118 0.00877119
 0.03202289 0.02845977 0.020035   0.00845593]
At round 80 energy consumption: 0.22413059127783166
At round 80 eta: 0.9090909090909067
At round 80 a_n: 0.7789350987089918
At round 80 local rounds: 3.120939520577836
At round 80 global rounds: 8.568286085798688
gradient difference: 1.0708935260772705
train() client id: f_00000-0-0 loss: 0.878550  [   32/  126]
train() client id: f_00000-0-1 loss: 0.820029  [   64/  126]
train() client id: f_00000-0-2 loss: 0.887844  [   96/  126]
train() client id: f_00000-1-0 loss: 0.803116  [   32/  126]
train() client id: f_00000-1-1 loss: 1.078792  [   64/  126]
train() client id: f_00000-1-2 loss: 0.883941  [   96/  126]
train() client id: f_00000-2-0 loss: 0.873491  [   32/  126]
train() client id: f_00000-2-1 loss: 0.813301  [   64/  126]
train() client id: f_00000-2-2 loss: 0.824726  [   96/  126]
train() client id: f_00001-0-0 loss: 0.600022  [   32/  265]
train() client id: f_00001-0-1 loss: 0.575357  [   64/  265]
train() client id: f_00001-0-2 loss: 0.676698  [   96/  265]
train() client id: f_00001-0-3 loss: 0.617831  [  128/  265]
train() client id: f_00001-0-4 loss: 0.610859  [  160/  265]
train() client id: f_00001-0-5 loss: 0.572368  [  192/  265]
train() client id: f_00001-0-6 loss: 0.742807  [  224/  265]
train() client id: f_00001-0-7 loss: 0.504831  [  256/  265]
train() client id: f_00001-1-0 loss: 0.622985  [   32/  265]
train() client id: f_00001-1-1 loss: 0.632242  [   64/  265]
train() client id: f_00001-1-2 loss: 0.590936  [   96/  265]
train() client id: f_00001-1-3 loss: 0.568164  [  128/  265]
train() client id: f_00001-1-4 loss: 0.598231  [  160/  265]
train() client id: f_00001-1-5 loss: 0.523347  [  192/  265]
train() client id: f_00001-1-6 loss: 0.683105  [  224/  265]
train() client id: f_00001-1-7 loss: 0.611343  [  256/  265]
train() client id: f_00001-2-0 loss: 0.613200  [   32/  265]
train() client id: f_00001-2-1 loss: 0.655702  [   64/  265]
train() client id: f_00001-2-2 loss: 0.667490  [   96/  265]
train() client id: f_00001-2-3 loss: 0.506240  [  128/  265]
train() client id: f_00001-2-4 loss: 0.517067  [  160/  265]
train() client id: f_00001-2-5 loss: 0.677666  [  192/  265]
train() client id: f_00001-2-6 loss: 0.578627  [  224/  265]
train() client id: f_00001-2-7 loss: 0.623870  [  256/  265]
train() client id: f_00002-0-0 loss: 0.980170  [   32/  124]
train() client id: f_00002-0-1 loss: 0.867777  [   64/  124]
train() client id: f_00002-0-2 loss: 0.984356  [   96/  124]
train() client id: f_00002-1-0 loss: 0.991740  [   32/  124]
train() client id: f_00002-1-1 loss: 0.800559  [   64/  124]
train() client id: f_00002-1-2 loss: 0.951333  [   96/  124]
train() client id: f_00002-2-0 loss: 1.072628  [   32/  124]
train() client id: f_00002-2-1 loss: 0.759290  [   64/  124]
train() client id: f_00002-2-2 loss: 0.982017  [   96/  124]
train() client id: f_00003-0-0 loss: 0.437979  [   32/   43]
train() client id: f_00003-1-0 loss: 0.370507  [   32/   43]
train() client id: f_00003-2-0 loss: 0.394489  [   32/   43]
train() client id: f_00004-0-0 loss: 0.941957  [   32/  306]
train() client id: f_00004-0-1 loss: 0.773080  [   64/  306]
train() client id: f_00004-0-2 loss: 0.919204  [   96/  306]
train() client id: f_00004-0-3 loss: 0.971973  [  128/  306]
train() client id: f_00004-0-4 loss: 0.861288  [  160/  306]
train() client id: f_00004-0-5 loss: 0.879802  [  192/  306]
train() client id: f_00004-0-6 loss: 0.904627  [  224/  306]
train() client id: f_00004-0-7 loss: 0.867376  [  256/  306]
train() client id: f_00004-0-8 loss: 0.813642  [  288/  306]
train() client id: f_00004-1-0 loss: 0.913771  [   32/  306]
train() client id: f_00004-1-1 loss: 0.764781  [   64/  306]
train() client id: f_00004-1-2 loss: 0.757327  [   96/  306]
train() client id: f_00004-1-3 loss: 0.941106  [  128/  306]
train() client id: f_00004-1-4 loss: 0.886196  [  160/  306]
train() client id: f_00004-1-5 loss: 0.854949  [  192/  306]
train() client id: f_00004-1-6 loss: 0.934050  [  224/  306]
train() client id: f_00004-1-7 loss: 1.025353  [  256/  306]
train() client id: f_00004-1-8 loss: 0.852717  [  288/  306]
train() client id: f_00004-2-0 loss: 0.763889  [   32/  306]
train() client id: f_00004-2-1 loss: 0.978801  [   64/  306]
train() client id: f_00004-2-2 loss: 1.030635  [   96/  306]
train() client id: f_00004-2-3 loss: 0.878261  [  128/  306]
train() client id: f_00004-2-4 loss: 0.895073  [  160/  306]
train() client id: f_00004-2-5 loss: 0.906488  [  192/  306]
train() client id: f_00004-2-6 loss: 0.759707  [  224/  306]
train() client id: f_00004-2-7 loss: 0.808692  [  256/  306]
train() client id: f_00004-2-8 loss: 0.846056  [  288/  306]
train() client id: f_00005-0-0 loss: 0.774159  [   32/  146]
train() client id: f_00005-0-1 loss: 0.902400  [   64/  146]
train() client id: f_00005-0-2 loss: 0.496101  [   96/  146]
train() client id: f_00005-0-3 loss: 1.062936  [  128/  146]
train() client id: f_00005-1-0 loss: 0.728937  [   32/  146]
train() client id: f_00005-1-1 loss: 0.686607  [   64/  146]
train() client id: f_00005-1-2 loss: 0.955159  [   96/  146]
train() client id: f_00005-1-3 loss: 0.661453  [  128/  146]
train() client id: f_00005-2-0 loss: 0.774193  [   32/  146]
train() client id: f_00005-2-1 loss: 0.991262  [   64/  146]
train() client id: f_00005-2-2 loss: 0.642007  [   96/  146]
train() client id: f_00005-2-3 loss: 0.766488  [  128/  146]
train() client id: f_00006-0-0 loss: 0.440663  [   32/   54]
train() client id: f_00006-1-0 loss: 0.419194  [   32/   54]
train() client id: f_00006-2-0 loss: 0.398855  [   32/   54]
train() client id: f_00007-0-0 loss: 0.730045  [   32/  179]
train() client id: f_00007-0-1 loss: 0.674241  [   64/  179]
train() client id: f_00007-0-2 loss: 0.656616  [   96/  179]
train() client id: f_00007-0-3 loss: 0.493167  [  128/  179]
train() client id: f_00007-0-4 loss: 0.552958  [  160/  179]
train() client id: f_00007-1-0 loss: 0.607452  [   32/  179]
train() client id: f_00007-1-1 loss: 0.547230  [   64/  179]
train() client id: f_00007-1-2 loss: 0.693439  [   96/  179]
train() client id: f_00007-1-3 loss: 0.698883  [  128/  179]
train() client id: f_00007-1-4 loss: 0.665673  [  160/  179]
train() client id: f_00007-2-0 loss: 0.393988  [   32/  179]
train() client id: f_00007-2-1 loss: 0.598628  [   64/  179]
train() client id: f_00007-2-2 loss: 0.749125  [   96/  179]
train() client id: f_00007-2-3 loss: 0.646826  [  128/  179]
train() client id: f_00007-2-4 loss: 0.728586  [  160/  179]
train() client id: f_00008-0-0 loss: 0.945857  [   32/  130]
train() client id: f_00008-0-1 loss: 0.838307  [   64/  130]
train() client id: f_00008-0-2 loss: 0.782824  [   96/  130]
train() client id: f_00008-0-3 loss: 0.639216  [  128/  130]
train() client id: f_00008-1-0 loss: 0.690556  [   32/  130]
train() client id: f_00008-1-1 loss: 0.844423  [   64/  130]
train() client id: f_00008-1-2 loss: 0.897233  [   96/  130]
train() client id: f_00008-1-3 loss: 0.742476  [  128/  130]
train() client id: f_00008-2-0 loss: 0.745751  [   32/  130]
train() client id: f_00008-2-1 loss: 0.843974  [   64/  130]
train() client id: f_00008-2-2 loss: 0.910284  [   96/  130]
train() client id: f_00008-2-3 loss: 0.741929  [  128/  130]
train() client id: f_00009-0-0 loss: 0.654760  [   32/  118]
train() client id: f_00009-0-1 loss: 0.670329  [   64/  118]
train() client id: f_00009-0-2 loss: 0.719676  [   96/  118]
train() client id: f_00009-1-0 loss: 0.697494  [   32/  118]
train() client id: f_00009-1-1 loss: 0.851750  [   64/  118]
train() client id: f_00009-1-2 loss: 0.711744  [   96/  118]
train() client id: f_00009-2-0 loss: 0.771841  [   32/  118]
train() client id: f_00009-2-1 loss: 0.655323  [   64/  118]
train() client id: f_00009-2-2 loss: 0.556538  [   96/  118]
At round 80 accuracy: 0.6445623342175066
At round 80 training accuracy: 0.590878604963112
At round 80 training loss: 0.8215872245689358
update_location
xs = -3.905658 4.200318 420.009024 18.811294 0.979296 3.956410 -382.443192 -361.324852 404.663977 -347.060879 
ys = 412.587959 395.555839 1.320614 -382.455176 374.350187 357.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -24.21142616811743
ys mean: 117.8941425355287
dists_uav = 424.551621 408.022137 431.751461 395.759809 387.477769 371.546255 395.309607 374.908422 417.206908 361.202527 
uav_gains = -122.241120 -121.690738 -122.465833 -121.245403 -120.923157 -120.244605 -121.228360 -120.394939 -122.002913 -119.754854 
uav_gains_db_mean: -121.21919231716296
dists_bs = 292.782022 284.178873 619.838905 590.185641 265.912034 255.824960 273.099456 255.058434 600.662005 242.580789 
bs_gains = -108.630302 -108.267629 -117.750866 -117.154741 -107.459723 -106.989461 -107.784043 -106.952970 -117.368704 -106.343038 
bs_gains_db_mean: -110.47014779727887
Round 81
-------------------------------
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.18431614 0.35650715 0.18224127 0.07169206 0.41081325 0.19799863
 0.08628049 0.25087914 0.1817507  0.16063512]
obj_prev = 2.0831139657376503
eta_min = 0.0	eta_max = inf
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.4687274993903175	eta = 0.909090909090909
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 1.5461570538938256	eta = 0.2755967820109389
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.8960917211906734	eta = 0.47552711230325256
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7944965020417399	eta = 0.5363345306638816
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7881017357421507	eta = 0.5406864230991465
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7880715228657817	eta = 0.54070715179138
eta = 0.54070715179138
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [0.05073821 0.10671132 0.04993285 0.01731542 0.12322138 0.0587919
 0.02174495 0.07208048 0.05234895 0.04751674]
ene_total = [0.08444145 0.1104692  0.08493497 0.04776518 0.12572503 0.06427522
 0.05189808 0.09632167 0.06897994 0.05326078]
ti_comp = [26.93025964 27.2160528  26.91701177 26.98204373 27.22058811 27.22304297
 26.98284085 27.01864488 27.10909704 27.22621665]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.12565281e-08 1.02532430e-07 1.07395299e-08 4.45686613e-10
 1.57813146e-07 1.71379604e-08 8.82632352e-10 3.20631268e-08
 1.22003941e-08 9.04576573e-09]
ene_total = [0.03623342 0.00907936 0.03749214 0.03131323 0.00864849 0.00841512
 0.03123749 0.02783567 0.01924149 0.00811357]
optimize_network iter = 0 obj = 0.21760996983692812
eta = 0.54070715179138
freqs = [ 942029.79379861 1960448.1887988   927533.22358696  320869.35975109
 2263385.64084244 1079818.64242409  402940.3153214  1333902.54699206
  965523.69021721  872628.40406785]
eta_min = 0.5407071517913803	eta_max = 0.9738786232505544
af = 2.5132315156122307e-07	bf = 0.0999465483120546	zeta = 2.764554667173454e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.74483827e-09 1.58932227e-08 1.66470005e-09 6.90844506e-11
 2.44621090e-08 2.65650021e-09 1.36814006e-10 4.97000232e-09
 1.89114391e-09 1.40215510e-09]
ene_total = [0.1830592  0.0458705  0.18941855 0.15820137 0.04369346 0.04251496
 0.15781873 0.14063181 0.09721221 0.04099149]
ti_comp = [5.02451092 5.31030408 5.01126305 5.07629501 5.3148394  5.31729426
 5.07709213 5.11289616 5.20334832 5.32046794]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
optimize_network iter = 1 obj = 1.0994121997748156
eta = 0.909090909090909
freqs = [ 782640.11306705 1557441.83511423  772253.41470056  264366.82466173
 1796869.89992426  856934.27544998  331943.42290144 1092625.69941991
  779732.80435494  692177.58267536]
eta_min = 0.9090909090909166	eta_max = 0.9090909090908771
af = 1.605687647826223e-07	bf = 0.0999465483120546	zeta = 1.7662564126088454e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
ti_comp = [5.02451092 5.31030408 5.01126305 5.07629501 5.3148394  5.31729426
 5.07709213 5.11289616 5.20334832 5.32046794]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
optimize_network iter = 2 obj = 1.09941219977443
eta = 0.9090909090908771
freqs = [ 782640.11306704 1557441.83511425  772253.41470055  264366.82466173
 1796869.89992429  856934.27544999  331943.42290144 1092625.69941991
  779732.80435495  692177.58267537]
Done!
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.15767328e-09 9.64185965e-09 1.10925813e-09 4.50789326e-11
 1.48199232e-08 1.60819661e-09 8.92510132e-11 3.20543602e-09
 1.18556733e-09 8.48024357e-10]
ene_total = [0.03813509 0.00955579 0.03945988 0.03295668 0.00910226 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
At round 81 energy consumption: 0.22903076238567763
At round 81 eta: 0.9090909090908771
At round 81 a_n: 0.43638925173967635
At round 81 local rounds: 3.120939520578907
At round 81 global rounds: 4.80028176913475
gradient difference: 0.9192516207695007
train() client id: f_00000-0-0 loss: 0.946992  [   32/  126]
train() client id: f_00000-0-1 loss: 0.973626  [   64/  126]
train() client id: f_00000-0-2 loss: 0.894713  [   96/  126]
train() client id: f_00000-1-0 loss: 1.006246  [   32/  126]
train() client id: f_00000-1-1 loss: 0.933049  [   64/  126]
train() client id: f_00000-1-2 loss: 1.039490  [   96/  126]
train() client id: f_00000-2-0 loss: 0.939032  [   32/  126]
train() client id: f_00000-2-1 loss: 0.862590  [   64/  126]
train() client id: f_00000-2-2 loss: 0.947004  [   96/  126]
train() client id: f_00001-0-0 loss: 0.568521  [   32/  265]
train() client id: f_00001-0-1 loss: 0.490530  [   64/  265]
train() client id: f_00001-0-2 loss: 0.504708  [   96/  265]
train() client id: f_00001-0-3 loss: 0.602925  [  128/  265]
train() client id: f_00001-0-4 loss: 0.490287  [  160/  265]
train() client id: f_00001-0-5 loss: 0.493125  [  192/  265]
train() client id: f_00001-0-6 loss: 0.517408  [  224/  265]
train() client id: f_00001-0-7 loss: 0.459115  [  256/  265]
train() client id: f_00001-1-0 loss: 0.518518  [   32/  265]
train() client id: f_00001-1-1 loss: 0.402913  [   64/  265]
train() client id: f_00001-1-2 loss: 0.489091  [   96/  265]
train() client id: f_00001-1-3 loss: 0.503166  [  128/  265]
train() client id: f_00001-1-4 loss: 0.446606  [  160/  265]
train() client id: f_00001-1-5 loss: 0.654380  [  192/  265]
train() client id: f_00001-1-6 loss: 0.531900  [  224/  265]
train() client id: f_00001-1-7 loss: 0.491822  [  256/  265]
train() client id: f_00001-2-0 loss: 0.454661  [   32/  265]
train() client id: f_00001-2-1 loss: 0.462208  [   64/  265]
train() client id: f_00001-2-2 loss: 0.700584  [   96/  265]
train() client id: f_00001-2-3 loss: 0.452349  [  128/  265]
train() client id: f_00001-2-4 loss: 0.630956  [  160/  265]
train() client id: f_00001-2-5 loss: 0.404066  [  192/  265]
train() client id: f_00001-2-6 loss: 0.479337  [  224/  265]
train() client id: f_00001-2-7 loss: 0.465258  [  256/  265]
train() client id: f_00002-0-0 loss: 0.690514  [   32/  124]
train() client id: f_00002-0-1 loss: 0.828903  [   64/  124]
train() client id: f_00002-0-2 loss: 1.025247  [   96/  124]
train() client id: f_00002-1-0 loss: 0.882987  [   32/  124]
train() client id: f_00002-1-1 loss: 0.667977  [   64/  124]
train() client id: f_00002-1-2 loss: 0.790349  [   96/  124]
train() client id: f_00002-2-0 loss: 1.070581  [   32/  124]
train() client id: f_00002-2-1 loss: 0.722765  [   64/  124]
train() client id: f_00002-2-2 loss: 0.787731  [   96/  124]
train() client id: f_00003-0-0 loss: 0.495556  [   32/   43]
train() client id: f_00003-1-0 loss: 0.628918  [   32/   43]
train() client id: f_00003-2-0 loss: 0.622907  [   32/   43]
train() client id: f_00004-0-0 loss: 0.746397  [   32/  306]
train() client id: f_00004-0-1 loss: 0.826079  [   64/  306]
train() client id: f_00004-0-2 loss: 0.758076  [   96/  306]
train() client id: f_00004-0-3 loss: 0.819036  [  128/  306]
train() client id: f_00004-0-4 loss: 0.830923  [  160/  306]
train() client id: f_00004-0-5 loss: 0.795504  [  192/  306]
train() client id: f_00004-0-6 loss: 0.796569  [  224/  306]
train() client id: f_00004-0-7 loss: 0.802243  [  256/  306]
train() client id: f_00004-0-8 loss: 0.835669  [  288/  306]
train() client id: f_00004-1-0 loss: 0.779699  [   32/  306]
train() client id: f_00004-1-1 loss: 0.791846  [   64/  306]
train() client id: f_00004-1-2 loss: 0.832632  [   96/  306]
train() client id: f_00004-1-3 loss: 0.771955  [  128/  306]
train() client id: f_00004-1-4 loss: 0.758760  [  160/  306]
train() client id: f_00004-1-5 loss: 0.820438  [  192/  306]
train() client id: f_00004-1-6 loss: 0.854763  [  224/  306]
train() client id: f_00004-1-7 loss: 0.678055  [  256/  306]
train() client id: f_00004-1-8 loss: 0.700450  [  288/  306]
train() client id: f_00004-2-0 loss: 0.758640  [   32/  306]
train() client id: f_00004-2-1 loss: 0.854522  [   64/  306]
train() client id: f_00004-2-2 loss: 0.901851  [   96/  306]
train() client id: f_00004-2-3 loss: 0.788696  [  128/  306]
train() client id: f_00004-2-4 loss: 0.818985  [  160/  306]
train() client id: f_00004-2-5 loss: 0.806926  [  192/  306]
train() client id: f_00004-2-6 loss: 0.768752  [  224/  306]
train() client id: f_00004-2-7 loss: 0.798806  [  256/  306]
train() client id: f_00004-2-8 loss: 0.729945  [  288/  306]
train() client id: f_00005-0-0 loss: 0.519422  [   32/  146]
train() client id: f_00005-0-1 loss: 0.857614  [   64/  146]
train() client id: f_00005-0-2 loss: 0.734241  [   96/  146]
train() client id: f_00005-0-3 loss: 0.577041  [  128/  146]
train() client id: f_00005-1-0 loss: 0.601059  [   32/  146]
train() client id: f_00005-1-1 loss: 0.794106  [   64/  146]
train() client id: f_00005-1-2 loss: 0.498693  [   96/  146]
train() client id: f_00005-1-3 loss: 0.849915  [  128/  146]
train() client id: f_00005-2-0 loss: 0.622023  [   32/  146]
train() client id: f_00005-2-1 loss: 0.574863  [   64/  146]
train() client id: f_00005-2-2 loss: 0.521819  [   96/  146]
train() client id: f_00005-2-3 loss: 0.781079  [  128/  146]
train() client id: f_00006-0-0 loss: 0.585986  [   32/   54]
train() client id: f_00006-1-0 loss: 0.526746  [   32/   54]
train() client id: f_00006-2-0 loss: 0.482938  [   32/   54]
train() client id: f_00007-0-0 loss: 0.617422  [   32/  179]
train() client id: f_00007-0-1 loss: 0.668134  [   64/  179]
train() client id: f_00007-0-2 loss: 0.621327  [   96/  179]
train() client id: f_00007-0-3 loss: 0.368683  [  128/  179]
train() client id: f_00007-0-4 loss: 0.690628  [  160/  179]
train() client id: f_00007-1-0 loss: 0.732056  [   32/  179]
train() client id: f_00007-1-1 loss: 0.692859  [   64/  179]
train() client id: f_00007-1-2 loss: 0.531782  [   96/  179]
train() client id: f_00007-1-3 loss: 0.453305  [  128/  179]
train() client id: f_00007-1-4 loss: 0.538512  [  160/  179]
train() client id: f_00007-2-0 loss: 0.564965  [   32/  179]
train() client id: f_00007-2-1 loss: 0.505439  [   64/  179]
train() client id: f_00007-2-2 loss: 0.579528  [   96/  179]
train() client id: f_00007-2-3 loss: 0.499176  [  128/  179]
train() client id: f_00007-2-4 loss: 0.496005  [  160/  179]
train() client id: f_00008-0-0 loss: 0.751094  [   32/  130]
train() client id: f_00008-0-1 loss: 0.783568  [   64/  130]
train() client id: f_00008-0-2 loss: 0.856411  [   96/  130]
train() client id: f_00008-0-3 loss: 0.721863  [  128/  130]
train() client id: f_00008-1-0 loss: 0.749959  [   32/  130]
train() client id: f_00008-1-1 loss: 0.709370  [   64/  130]
train() client id: f_00008-1-2 loss: 0.720300  [   96/  130]
train() client id: f_00008-1-3 loss: 0.927231  [  128/  130]
train() client id: f_00008-2-0 loss: 0.674575  [   32/  130]
train() client id: f_00008-2-1 loss: 0.743773  [   64/  130]
train() client id: f_00008-2-2 loss: 0.856829  [   96/  130]
train() client id: f_00008-2-3 loss: 0.822250  [  128/  130]
train() client id: f_00009-0-0 loss: 1.012921  [   32/  118]
train() client id: f_00009-0-1 loss: 0.843996  [   64/  118]
train() client id: f_00009-0-2 loss: 0.725487  [   96/  118]
train() client id: f_00009-1-0 loss: 0.673328  [   32/  118]
train() client id: f_00009-1-1 loss: 1.033073  [   64/  118]
train() client id: f_00009-1-2 loss: 0.825835  [   96/  118]
train() client id: f_00009-2-0 loss: 0.891539  [   32/  118]
train() client id: f_00009-2-1 loss: 0.936721  [   64/  118]
train() client id: f_00009-2-2 loss: 0.877965  [   96/  118]
At round 81 accuracy: 0.6445623342175066
At round 81 training accuracy: 0.5881958417169685
At round 81 training loss: 0.8188716346132736
update_location
xs = -3.905658 4.200318 425.009024 18.811294 0.979296 3.956410 -387.443192 -366.324852 409.663977 -352.060879 
ys = 417.587959 400.555839 1.320614 -387.455176 379.350187 362.814151 -2.624984 0.822348 17.569006 4.001482 
xs mean: -24.71142616811743
ys mean: 119.3941425355287
dists_uav = 429.412340 412.871194 436.617011 400.593782 392.310493 376.363868 400.148869 379.729606 422.058342 366.009391 
uav_gains = -122.393729 -121.857642 -122.613185 -121.425192 -121.113468 -120.458767 -121.408884 -120.603593 -122.161320 -119.987785 
uav_gains_db_mean: -121.40235663590082
dists_bs = 296.853822 288.076709 624.640183 594.910617 269.680759 259.421572 276.916494 258.733557 605.488632 246.152489 
bs_gains = -108.798253 -108.433287 -117.844697 -117.251707 -107.630858 -107.159230 -107.952826 -107.126936 -117.466027 -106.520778 
bs_gains_db_mean: -110.61845995286363
Round 82
-------------------------------
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.03972093 0.07667447 0.03927574 0.01549837 0.08835219 0.04258691
 0.01863559 0.05403005 0.03910533 0.03455187]
obj_prev = 0.44843144567355053
eta_min = 0.0	eta_max = inf
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.10079758902614538	eta = 0.9090909090909091
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.3375882886973583	eta = 0.2714376502678347
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.19414841544868844	eta = 0.47198001400206313
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17188140883090305	eta = 0.5331243935293785
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17048248758463935	eta = 0.5374990307814271
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17047588964991173	eta = 0.5375198336265012
eta = 0.5375198336265012
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [0.05122607 0.10773737 0.05041296 0.01748191 0.12440617 0.05935719
 0.02195403 0.07277355 0.0528523  0.04797362]
ene_total = [0.01831375 0.02382031 0.0184197  0.01040877 0.02710943 0.01385921
 0.01130017 0.020882   0.01487805 0.0114845 ]
ti_comp = [127.24870803 127.54244623 127.23535515 127.30084038 127.54703912
 127.54954855 127.30163179 127.33761381 127.43428871 127.5527431 ]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [5.18855480e-10 4.80473758e-09 4.94641987e-10 2.06055571e-11
 7.39716068e-09 8.03419283e-10 4.08088986e-11 1.48554980e-09
 5.68197076e-10 4.24138750e-10]
ene_total = [0.00791929 0.00195895 0.00819024 0.00686146 0.00186576 0.00181484
 0.0068454  0.00611528 0.00415361 0.00175002]
optimize_network iter = 0 obj = 0.04747485012945179
eta = 0.5375198336265012
freqs = [201283.26753271 422358.89209453 198109.08366522  68663.77799187
 487687.42285783 232682.88526082  86228.39443656 285750.39151138
 207370.78109251 188054.06037245]
eta_min = 0.537519833626502	eta_max = 0.9942699445819028
af = 2.5030539272411267e-09	bf = 0.021956175032555886	zeta = 2.7533593199652398e-09	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [7.96601739e-11 7.37674065e-10 7.59426628e-11 3.16358278e-12
 1.13569024e-09 1.23349415e-10 6.26541318e-12 2.28077296e-10
 8.72356169e-11 6.51182610e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
ti_comp = [24.69954088 24.99327907 24.686188   24.75167323 24.99787197 25.0003814
 24.75246463 24.78844665 24.88512156 25.00357594]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
optimize_network iter = 1 obj = 0.24151792697303298
eta = 0.909090909090909
freqs = [159208.78105798 330908.549687   156766.40742107  54218.71812527
 382035.51668375 182260.08745128  68086.44578208 225366.34992481
 163038.03730914 147287.27775164]
eta_min = 0.909090909090918	eta_max = 0.9684934797724077
af = 1.5403473503468153e-09	bf = 0.021956175032555886	zeta = 1.694382085381497e-09	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
ti_comp = [24.69954088 24.99327907 24.686188   24.75167323 24.99787197 25.0003814
 24.75246463 24.78844665 24.88512156 25.00357594]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
optimize_network iter = 2 obj = 0.24151792697305713
eta = 0.909090909090918
freqs = [159208.78105798 330908.549687   156766.40742107  54218.71812527
 382035.51668375 182260.08745128  68086.44578208 225366.34992481
 163038.03730914 147287.27775163]
Done!
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.79066540e-11 4.35264361e-10 4.57108141e-11 1.89608676e-12
 6.69915714e-10 7.27490395e-11 3.75496892e-12 1.36371128e-10
 5.18337331e-11 3.83975697e-11]
ene_total = [0.03902796 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
At round 82 energy consumption: 0.23396609732358067
At round 82 eta: 0.909090909090918
At round 82 a_n: 0.09384340477035735
At round 82 local rounds: 3.1209395205774326
At round 82 global rounds: 1.0322774524740326
gradient difference: 1.001996636390686
train() client id: f_00000-0-0 loss: 1.054946  [   32/  126]
train() client id: f_00000-0-1 loss: 0.969131  [   64/  126]
train() client id: f_00000-0-2 loss: 0.717427  [   96/  126]
train() client id: f_00000-1-0 loss: 0.697216  [   32/  126]
train() client id: f_00000-1-1 loss: 0.919429  [   64/  126]
train() client id: f_00000-1-2 loss: 0.745457  [   96/  126]
train() client id: f_00000-2-0 loss: 1.063967  [   32/  126]
train() client id: f_00000-2-1 loss: 1.019549  [   64/  126]
train() client id: f_00000-2-2 loss: 0.769314  [   96/  126]
train() client id: f_00001-0-0 loss: 0.694498  [   32/  265]
train() client id: f_00001-0-1 loss: 0.568852  [   64/  265]
train() client id: f_00001-0-2 loss: 0.657540  [   96/  265]
train() client id: f_00001-0-3 loss: 0.533870  [  128/  265]
train() client id: f_00001-0-4 loss: 0.590804  [  160/  265]
train() client id: f_00001-0-5 loss: 0.569573  [  192/  265]
train() client id: f_00001-0-6 loss: 0.644068  [  224/  265]
train() client id: f_00001-0-7 loss: 0.557282  [  256/  265]
train() client id: f_00001-1-0 loss: 0.602364  [   32/  265]
train() client id: f_00001-1-1 loss: 0.673375  [   64/  265]
train() client id: f_00001-1-2 loss: 0.662544  [   96/  265]
train() client id: f_00001-1-3 loss: 0.644838  [  128/  265]
train() client id: f_00001-1-4 loss: 0.543676  [  160/  265]
train() client id: f_00001-1-5 loss: 0.551937  [  192/  265]
train() client id: f_00001-1-6 loss: 0.589335  [  224/  265]
train() client id: f_00001-1-7 loss: 0.645477  [  256/  265]
train() client id: f_00001-2-0 loss: 0.569959  [   32/  265]
train() client id: f_00001-2-1 loss: 0.650265  [   64/  265]
train() client id: f_00001-2-2 loss: 0.708473  [   96/  265]
train() client id: f_00001-2-3 loss: 0.608968  [  128/  265]
train() client id: f_00001-2-4 loss: 0.630732  [  160/  265]
train() client id: f_00001-2-5 loss: 0.560358  [  192/  265]
train() client id: f_00001-2-6 loss: 0.608029  [  224/  265]
train() client id: f_00001-2-7 loss: 0.594687  [  256/  265]
train() client id: f_00002-0-0 loss: 1.032498  [   32/  124]
train() client id: f_00002-0-1 loss: 0.861071  [   64/  124]
train() client id: f_00002-0-2 loss: 1.155926  [   96/  124]
train() client id: f_00002-1-0 loss: 0.788583  [   32/  124]
train() client id: f_00002-1-1 loss: 1.015258  [   64/  124]
train() client id: f_00002-1-2 loss: 0.785269  [   96/  124]
train() client id: f_00002-2-0 loss: 1.119894  [   32/  124]
train() client id: f_00002-2-1 loss: 0.777124  [   64/  124]
train() client id: f_00002-2-2 loss: 0.878368  [   96/  124]
train() client id: f_00003-0-0 loss: 0.570513  [   32/   43]
train() client id: f_00003-1-0 loss: 0.524854  [   32/   43]
train() client id: f_00003-2-0 loss: 0.662079  [   32/   43]
train() client id: f_00004-0-0 loss: 0.946547  [   32/  306]
train() client id: f_00004-0-1 loss: 0.806924  [   64/  306]
train() client id: f_00004-0-2 loss: 0.824353  [   96/  306]
train() client id: f_00004-0-3 loss: 0.901004  [  128/  306]
train() client id: f_00004-0-4 loss: 0.651835  [  160/  306]
train() client id: f_00004-0-5 loss: 0.901430  [  192/  306]
train() client id: f_00004-0-6 loss: 0.890034  [  224/  306]
train() client id: f_00004-0-7 loss: 1.017842  [  256/  306]
train() client id: f_00004-0-8 loss: 0.923769  [  288/  306]
train() client id: f_00004-1-0 loss: 0.903093  [   32/  306]
train() client id: f_00004-1-1 loss: 0.833174  [   64/  306]
train() client id: f_00004-1-2 loss: 1.016111  [   96/  306]
train() client id: f_00004-1-3 loss: 0.866421  [  128/  306]
train() client id: f_00004-1-4 loss: 0.935452  [  160/  306]
train() client id: f_00004-1-5 loss: 0.858601  [  192/  306]
train() client id: f_00004-1-6 loss: 0.918296  [  224/  306]
train() client id: f_00004-1-7 loss: 0.783451  [  256/  306]
train() client id: f_00004-1-8 loss: 0.944308  [  288/  306]
train() client id: f_00004-2-0 loss: 0.851530  [   32/  306]
train() client id: f_00004-2-1 loss: 0.876943  [   64/  306]
train() client id: f_00004-2-2 loss: 0.983743  [   96/  306]
train() client id: f_00004-2-3 loss: 0.790783  [  128/  306]
train() client id: f_00004-2-4 loss: 0.987708  [  160/  306]
train() client id: f_00004-2-5 loss: 0.988822  [  192/  306]
train() client id: f_00004-2-6 loss: 0.848089  [  224/  306]
train() client id: f_00004-2-7 loss: 0.786475  [  256/  306]
train() client id: f_00004-2-8 loss: 0.925075  [  288/  306]
train() client id: f_00005-0-0 loss: 0.567822  [   32/  146]
train() client id: f_00005-0-1 loss: 0.670744  [   64/  146]
train() client id: f_00005-0-2 loss: 0.424741  [   96/  146]
train() client id: f_00005-0-3 loss: 0.570726  [  128/  146]
train() client id: f_00005-1-0 loss: 0.515036  [   32/  146]
train() client id: f_00005-1-1 loss: 0.593734  [   64/  146]
train() client id: f_00005-1-2 loss: 0.700433  [   96/  146]
train() client id: f_00005-1-3 loss: 0.734591  [  128/  146]
train() client id: f_00005-2-0 loss: 0.391294  [   32/  146]
train() client id: f_00005-2-1 loss: 0.566444  [   64/  146]
train() client id: f_00005-2-2 loss: 0.405708  [   96/  146]
train() client id: f_00005-2-3 loss: 0.621855  [  128/  146]
train() client id: f_00006-0-0 loss: 0.452503  [   32/   54]
train() client id: f_00006-1-0 loss: 0.557711  [   32/   54]
train() client id: f_00006-2-0 loss: 0.543417  [   32/   54]
train() client id: f_00007-0-0 loss: 0.539733  [   32/  179]
train() client id: f_00007-0-1 loss: 0.636790  [   64/  179]
train() client id: f_00007-0-2 loss: 0.619887  [   96/  179]
train() client id: f_00007-0-3 loss: 0.649945  [  128/  179]
train() client id: f_00007-0-4 loss: 0.731727  [  160/  179]
train() client id: f_00007-1-0 loss: 0.726263  [   32/  179]
train() client id: f_00007-1-1 loss: 0.929753  [   64/  179]
train() client id: f_00007-1-2 loss: 0.646777  [   96/  179]
train() client id: f_00007-1-3 loss: 0.779011  [  128/  179]
train() client id: f_00007-1-4 loss: 0.563283  [  160/  179]
train() client id: f_00007-2-0 loss: 0.604198  [   32/  179]
train() client id: f_00007-2-1 loss: 0.724116  [   64/  179]
train() client id: f_00007-2-2 loss: 0.690834  [   96/  179]
train() client id: f_00007-2-3 loss: 0.750188  [  128/  179]
train() client id: f_00007-2-4 loss: 0.672057  [  160/  179]
train() client id: f_00008-0-0 loss: 0.665042  [   32/  130]
train() client id: f_00008-0-1 loss: 0.672149  [   64/  130]
train() client id: f_00008-0-2 loss: 0.646982  [   96/  130]
train() client id: f_00008-0-3 loss: 0.699407  [  128/  130]
train() client id: f_00008-1-0 loss: 0.628493  [   32/  130]
train() client id: f_00008-1-1 loss: 0.657965  [   64/  130]
train() client id: f_00008-1-2 loss: 0.685046  [   96/  130]
train() client id: f_00008-1-3 loss: 0.668936  [  128/  130]
train() client id: f_00008-2-0 loss: 0.737882  [   32/  130]
train() client id: f_00008-2-1 loss: 0.625385  [   64/  130]
train() client id: f_00008-2-2 loss: 0.611642  [   96/  130]
train() client id: f_00008-2-3 loss: 0.704815  [  128/  130]
train() client id: f_00009-0-0 loss: 0.672972  [   32/  118]
train() client id: f_00009-0-1 loss: 0.886743  [   64/  118]
train() client id: f_00009-0-2 loss: 0.785102  [   96/  118]
train() client id: f_00009-1-0 loss: 0.704509  [   32/  118]
train() client id: f_00009-1-1 loss: 0.736579  [   64/  118]
train() client id: f_00009-1-2 loss: 0.745367  [   96/  118]
train() client id: f_00009-2-0 loss: 0.935562  [   32/  118]
train() client id: f_00009-2-1 loss: 0.557002  [   64/  118]
train() client id: f_00009-2-2 loss: 0.832345  [   96/  118]
At round 82 accuracy: 0.649867374005305
At round 82 training accuracy: 0.590878604963112
At round 82 training loss: 0.81424481086333
Done!
