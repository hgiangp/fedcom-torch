v = 22.697160301531774
a_0 = 27.840057009285058	a_alpha = -0.3425458469693173
['f_00000', 'f_00001', 'f_00002', 'f_00003', 'f_00004', 'f_00005', 'f_00006', 'f_00007', 'f_00008', 'f_00009']
10
dict_keys(['x', 'y'])
id = f_00000, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 126
id = f_00001, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 265
id = f_00002, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 124
id = f_00003, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 43
id = f_00004, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 306
id = f_00005, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 146
id = f_00006, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 54
id = f_00007, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 179
id = f_00008, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 130
id = f_00009, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 118
BaseFederated generated!
num_samples = [126 265 124  43 306 146  54 179 130 118]
msize = 502400
xs = 8.927491 116.223621 5.882650 10.934260 -32.581990 119.769243 -5.849135 -5.143845 -55.120581 20.134486 
ys = -107.390647 7.291448 5.684448 -147.290817 -9.642386 0.794442 -156.381692 1.628436 25.881276 -542.232496 
xs mean: 18.317620029478793
ys mean: -92.16579882624052
dists_uav = 147.011738 153.496239 100.334035 178.365195 105.615158 156.029813 185.713344 100.145449 117.081676 551.744032 
uav_gains = -104.187285 -104.658605 -100.036224 -106.323835 -100.593191 -104.837818 -106.787712 -100.015797 -101.712374 -125.426182 
uav_gains_db_mean: -105.45790248269995
dists_bs = 337.007121 336.061536 247.762566 372.079185 233.185978 342.398135 372.056234 242.711377 191.330784 743.303384 
bs_gains = -110.340953 -110.306786 -106.600058 -111.544850 -105.862727 -110.533938 -111.544100 -106.349583 -103.457035 -119.959710 
bs_gains_db_mean: -109.6499740198071
SystemModel __init__!
t_co_uav = [0.07623842 0.07807303 0.06311908 0.08530299 0.06461462 0.07879339
 0.08755301 0.06306553 0.06784165 0.6444196 ]
t_co_bs = [0.10941476 0.10915609 0.08662917 0.11931327 0.08317443 0.11089748
 0.11930659 0.08542495 0.07355657 0.27651075]
difference = [-0.03317635 -0.03108305 -0.02351009 -0.03401028 -0.01855982 -0.0321041
 -0.03175359 -0.02235942 -0.00571492  0.36790885]
decs_opt = [1 1 0 1 0 1 1 0 0 0]
af = 6.796163711028166	bf = 28.1534443839779	zeta = 7.475780082130983	eta = 0.9090909090909091
af = 6.796163711028166	bf = 28.1534443839779	zeta = 316.8130676618117	eta = 0.02145165210887975
af = 6.796163711028166	bf = 28.1534443839779	zeta = 55.45356061340263	eta = 0.12255594836205332
af = 6.796163711028166	bf = 28.1534443839779	zeta = 48.34481140036318	eta = 0.14057689986100788
af = 6.796163711028166	bf = 28.1534443839779	zeta = 48.27368699275389	eta = 0.14078401991644646
af = 6.796163711028166	bf = 28.1534443839779	zeta = 48.27367842444156	eta = 0.14078404490483543
eta_opt = 0.14078404490483543
initialize_feasible_solution eta = 0.14078404490483543, tau = 10.187149047851562
ti_comp = [0.04044452 0.08506189 0.03980254 0.0138025  0.09822241 0.04686429
 0.01733337 0.0574569  0.04172847 0.03787662]
ti_coms = [0.07623842 0.07807303 0.08662917 0.08530299 0.08317443 0.07879339
 0.08755301 0.08542495 0.07355657 0.27651075]
t_total = [ 3.78072541  5.28584871  4.09660252  3.21118612  5.87756598  4.07152223
  3.3984967   4.62961474  3.73543118 10.18668485]
system_model train() tau = 30	t0 = 0.05093574523925781	t_min = 10.187149047851562
Round 0
-------------------------------
ene_coms = [0.00762384 0.0078073  0.00866292 0.0085303  0.00831744 0.00787934
 0.0087553  0.0085425  0.00735566 0.02765107]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.90068848 22.69469841 10.76028486  3.8867935  26.18633112 12.60411091
  4.82605147 15.42155161 11.23239428 10.7853847 ]
obj_prev = 129.2982893362446
eta_min = 2.3983020447302057e-09	eta_max = 0.737949347395985
af = 27.184654844112664	bf = 2.81534443839779	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.81534443839779	zeta = 59.469506574595464	eta = 0.4571192264730437
af = 27.184654844112664	bf = 2.81534443839779	zeta = 44.385053486628834	eta = 0.6124731797901775
af = 27.184654844112664	bf = 2.81534443839779	zeta = 41.65550369368143	eta = 0.6526065569634728
af = 27.184654844112664	bf = 2.81534443839779	zeta = 41.50117038410696	eta = 0.6550334506836736
af = 27.184654844112664	bf = 2.81534443839779	zeta = 41.5006281763004	eta = 0.6550420087288437
af = 27.184654844112664	bf = 2.81534443839779	zeta = 41.500628169574625	eta = 0.6550420088350027
eta = 0.6550420088350027
ene_coms = [0.00762384 0.0078073  0.00866292 0.0085303  0.00831744 0.00787934
 0.0087553  0.0085425  0.00735566 0.02765107]
ene_comp = [0.03490956 0.0734209  0.03435544 0.01191358 0.08478036 0.04045076
 0.01496124 0.04959374 0.0360178  0.03269308]
ene_total = [3.43268556 6.55557444 3.47182423 1.64993642 7.51351817 3.90051175
 1.91405869 4.69192229 3.50048274 4.87011389]
ti_comp = [0.2954828  0.29364818 0.28509204 0.28641822 0.28854678 0.29292782
 0.28416821 0.28629626 0.29816465 0.09521046]
ti_coms = [0.07623842 0.07807303 0.08662917 0.08530299 0.08317443 0.07879339
 0.08755301 0.08542495 0.07355657 0.27651075]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00762384 0.0078073  0.00866292 0.0085303  0.00831744 0.00787934
 0.0087553  0.0085425  0.00735566 0.02765107]
ene_comp = [3.04543074e-05 2.86869576e-04 3.11814546e-05 1.28826608e-06
 4.57439137e-04 4.82101869e-05 2.59198094e-06 9.30099124e-05
 3.28487808e-05 2.40922946e-04]
ene_total = [0.61774488 0.65324544 0.70166285 0.68854724 0.70818256 0.63979796
 0.70681137 0.696934   0.59629408 2.25104167]
optimize_network iter = 0 obj = 8.260262043234723
eta = 0.6550420088350027
freqs = [5.90720679e+07 1.25015077e+08 6.02532429e+07 2.07975243e+07
 1.46909212e+08 6.90456087e+07 2.63246196e+07 8.66126218e+07
 6.03991788e+07 1.71688482e+08]
eta_min = 0.6530478530787287	eta_max = 0.6550420088349949
af = 0.08060155110672583	bf = 2.81534443839779	zeta = 0.08866170621739843	eta = 0.909090909090909
af = 0.08060155110672583	bf = 2.81534443839779	zeta = 31.053292453989027	eta = 0.0025955879308498884
af = 0.08060155110672583	bf = 2.81534443839779	zeta = 3.303816763856903	eta = 0.024396495589129135
af = 0.08060155110672583	bf = 2.81534443839779	zeta = 3.19252999396251	eta = 0.025246920548641315
af = 0.08060155110672583	bf = 2.81534443839779	zeta = 3.1924820022493736	eta = 0.025247300078727216
eta = 0.025247300078727216
ene_coms = [0.00762384 0.0078073  0.00866292 0.0085303  0.00831744 0.00787934
 0.0087553  0.0085425  0.00735566 0.02765107]
ene_comp = [2.64840870e-04 2.49471403e-03 2.71164386e-04 1.12031938e-05
 3.97804413e-03 4.19251952e-04 2.25407354e-05 8.08845387e-04
 2.85664013e-04 2.09514673e-03]
ene_total = [0.22530983 0.29423745 0.25516764 0.24395511 0.35117325 0.23701729
 0.25070523 0.26708503 0.21824489 0.84958627]
ti_comp = [0.29763167 0.29579705 0.28724091 0.28856709 0.29069565 0.29507669
 0.28631708 0.28844513 0.30031352 0.09735933]
ti_coms = [0.07623842 0.07807303 0.08662917 0.08530299 0.08317443 0.07879339
 0.08755301 0.08542495 0.07355657 0.27651075]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00762384 0.0078073  0.00866292 0.0085303  0.00831744 0.00787934
 0.0087553  0.0085425  0.00735566 0.02765107]
ene_comp = [3.06698076e-05 2.88873456e-04 3.13855803e-05 1.29678939e-06
 4.60516200e-04 4.85452166e-05 2.60882213e-06 9.36246826e-05
 3.30855217e-05 2.35422807e-04]
ene_total = [0.61421159 0.64965161 0.69764632 0.68459041 0.70435909 0.63614752
 0.70275022 0.6929776  0.5928858  2.23766215]
optimize_network iter = 1 obj = 8.212882317288411
eta = 0.6530478530787287
freqs = [5.90682318e+07 1.25001320e+08 6.02334800e+07 2.07914230e+07
 1.46874182e+08 6.90367774e+07 2.63153452e+07 8.65869377e+07
 6.03991788e+07 1.69109101e+08]
eta_min = 0.6530478530787285	eta_max = 0.6530478530787238
af = 0.08010402218031798	bf = 2.81534443839779	zeta = 0.08811442439834978	eta = 0.9090909090909091
af = 0.08010402218031798	bf = 2.81534443839779	zeta = 31.052770838703296	eta = 0.0025796094846543805
af = 0.08010402218031798	bf = 2.81534443839779	zeta = 3.3012898417001204	eta = 0.024264462080392638
af = 0.08010402218031798	bf = 2.81534443839779	zeta = 3.190650075047629	eta = 0.02510586253465047
af = 0.08010402218031798	bf = 2.81534443839779	zeta = 3.190602872693924	eta = 0.02510623395530378
eta = 0.02510623395530378
ene_coms = [0.00762384 0.0078073  0.00866292 0.0085303  0.00831744 0.00787934
 0.0087553  0.0085425  0.00735566 0.02765107]
ene_comp = [2.65209764e-04 2.49796354e-03 2.71399236e-04 1.12136735e-05
 3.98220277e-03 4.19783052e-04 2.25591602e-05 8.09596862e-04
 2.86099069e-04 2.03576194e-03]
ene_total = [0.22528776 0.29428767 0.25513742 0.24392011 0.3512412  0.23699816
 0.25066948 0.26706784 0.21822574 0.84776748]
ti_comp = [0.29763167 0.29579705 0.28724091 0.28856709 0.29069565 0.29507669
 0.28631708 0.28844513 0.30031352 0.09735933]
ti_coms = [0.07623842 0.07807303 0.08662917 0.08530299 0.08317443 0.07879339
 0.08755301 0.08542495 0.07355657 0.27651075]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00762384 0.0078073  0.00866292 0.0085303  0.00831744 0.00787934
 0.0087553  0.0085425  0.00735566 0.02765107]
ene_comp = [3.06698076e-05 2.88873456e-04 3.13855803e-05 1.29678939e-06
 4.60516200e-04 4.85452166e-05 2.60882213e-06 9.36246826e-05
 3.30855217e-05 2.35422807e-04]
ene_total = [0.61421159 0.64965161 0.69764632 0.68459041 0.70435909 0.63614752
 0.70275022 0.6929776  0.5928858  2.23766215]
optimize_network iter = 2 obj = 8.212882317288408
eta = 0.6530478530787285
freqs = [5.90682318e+07 1.25001320e+08 6.02334800e+07 2.07914230e+07
 1.46874182e+08 6.90367774e+07 2.63153452e+07 8.65869377e+07
 6.03991788e+07 1.69109101e+08]
Done!
ene_coms = [0.00762384 0.0078073  0.00866292 0.0085303  0.00831744 0.00787934
 0.0087553  0.0085425  0.00735566 0.02765107]
ene_comp = [2.85753687e-05 2.69146308e-04 2.92422612e-05 1.20823174e-06
 4.29067581e-04 4.52300672e-05 2.43066585e-06 8.72310597e-05
 3.08261138e-05 2.19345800e-04]
ene_total = [0.00765242 0.00807645 0.00869216 0.00853151 0.00874651 0.00792457
 0.00875773 0.00862973 0.00738648 0.02787042]
At round 0 energy consumption: 0.10226797419439325
At round 0 eta: 0.6530478530787285
At round 0 a_n: 27.840057009285058
At round 0 local rounds: 13.952838332324164
At round 0 global rounds: 80.24177759477122
gradient difference: 1.0545194149017334
train() client id: f_00000-0-0 loss: 1.030363  [   32/  126]
train() client id: f_00000-0-1 loss: 1.139844  [   64/  126]
train() client id: f_00000-0-2 loss: 1.088268  [   96/  126]
train() client id: f_00000-1-0 loss: 1.052091  [   32/  126]
train() client id: f_00000-1-1 loss: 1.102545  [   64/  126]
train() client id: f_00000-1-2 loss: 1.078798  [   96/  126]
train() client id: f_00000-2-0 loss: 1.078114  [   32/  126]
train() client id: f_00000-2-1 loss: 1.120164  [   64/  126]
train() client id: f_00000-2-2 loss: 1.133776  [   96/  126]
train() client id: f_00000-3-0 loss: 1.117754  [   32/  126]
train() client id: f_00000-3-1 loss: 1.159742  [   64/  126]
train() client id: f_00000-3-2 loss: 1.135882  [   96/  126]
train() client id: f_00000-4-0 loss: 1.215679  [   32/  126]
train() client id: f_00000-4-1 loss: 1.144934  [   64/  126]
train() client id: f_00000-4-2 loss: 1.156738  [   96/  126]
train() client id: f_00000-5-0 loss: 1.124790  [   32/  126]
train() client id: f_00000-5-1 loss: 1.201158  [   64/  126]
train() client id: f_00000-5-2 loss: 1.252313  [   96/  126]
train() client id: f_00000-6-0 loss: 1.179736  [   32/  126]
train() client id: f_00000-6-1 loss: 1.264654  [   64/  126]
train() client id: f_00000-6-2 loss: 1.216791  [   96/  126]
train() client id: f_00000-7-0 loss: 1.229344  [   32/  126]
train() client id: f_00000-7-1 loss: 1.286167  [   64/  126]
train() client id: f_00000-7-2 loss: 1.217048  [   96/  126]
train() client id: f_00000-8-0 loss: 1.220498  [   32/  126]
train() client id: f_00000-8-1 loss: 1.313599  [   64/  126]
train() client id: f_00000-8-2 loss: 1.295951  [   96/  126]
train() client id: f_00000-9-0 loss: 1.256451  [   32/  126]
train() client id: f_00000-9-1 loss: 1.301295  [   64/  126]
train() client id: f_00000-9-2 loss: 1.327806  [   96/  126]
train() client id: f_00000-10-0 loss: 1.282871  [   32/  126]
train() client id: f_00000-10-1 loss: 1.398492  [   64/  126]
train() client id: f_00000-10-2 loss: 1.337124  [   96/  126]
train() client id: f_00000-11-0 loss: 1.369095  [   32/  126]
train() client id: f_00000-11-1 loss: 1.378687  [   64/  126]
train() client id: f_00000-11-2 loss: 1.284877  [   96/  126]
train() client id: f_00000-12-0 loss: 1.355998  [   32/  126]
train() client id: f_00000-12-1 loss: 1.372823  [   64/  126]
train() client id: f_00000-12-2 loss: 1.378718  [   96/  126]
train() client id: f_00001-0-0 loss: 0.848875  [   32/  265]
train() client id: f_00001-0-1 loss: 0.771587  [   64/  265]
train() client id: f_00001-0-2 loss: 0.739796  [   96/  265]
train() client id: f_00001-0-3 loss: 0.851836  [  128/  265]
train() client id: f_00001-0-4 loss: 0.809101  [  160/  265]
train() client id: f_00001-0-5 loss: 0.850860  [  192/  265]
train() client id: f_00001-0-6 loss: 0.871163  [  224/  265]
train() client id: f_00001-0-7 loss: 0.805833  [  256/  265]
train() client id: f_00001-1-0 loss: 0.747997  [   32/  265]
train() client id: f_00001-1-1 loss: 0.773313  [   64/  265]
train() client id: f_00001-1-2 loss: 0.817741  [   96/  265]
train() client id: f_00001-1-3 loss: 0.894561  [  128/  265]
train() client id: f_00001-1-4 loss: 0.819444  [  160/  265]
train() client id: f_00001-1-5 loss: 0.841028  [  192/  265]
train() client id: f_00001-1-6 loss: 0.858386  [  224/  265]
train() client id: f_00001-1-7 loss: 0.838820  [  256/  265]
train() client id: f_00001-2-0 loss: 0.820197  [   32/  265]
train() client id: f_00001-2-1 loss: 0.896513  [   64/  265]
train() client id: f_00001-2-2 loss: 0.795248  [   96/  265]
train() client id: f_00001-2-3 loss: 0.869823  [  128/  265]
train() client id: f_00001-2-4 loss: 0.853282  [  160/  265]
train() client id: f_00001-2-5 loss: 0.829635  [  192/  265]
train() client id: f_00001-2-6 loss: 0.751473  [  224/  265]
train() client id: f_00001-2-7 loss: 0.818705  [  256/  265]
train() client id: f_00001-3-0 loss: 0.765799  [   32/  265]
train() client id: f_00001-3-1 loss: 0.768981  [   64/  265]
train() client id: f_00001-3-2 loss: 0.908451  [   96/  265]
train() client id: f_00001-3-3 loss: 0.843672  [  128/  265]
train() client id: f_00001-3-4 loss: 0.807033  [  160/  265]
train() client id: f_00001-3-5 loss: 0.853840  [  192/  265]
train() client id: f_00001-3-6 loss: 0.836996  [  224/  265]
train() client id: f_00001-3-7 loss: 0.911254  [  256/  265]
train() client id: f_00001-4-0 loss: 0.825888  [   32/  265]
train() client id: f_00001-4-1 loss: 0.835638  [   64/  265]
train() client id: f_00001-4-2 loss: 0.810114  [   96/  265]
train() client id: f_00001-4-3 loss: 0.800211  [  128/  265]
train() client id: f_00001-4-4 loss: 0.911871  [  160/  265]
train() client id: f_00001-4-5 loss: 0.866126  [  192/  265]
train() client id: f_00001-4-6 loss: 0.898197  [  224/  265]
train() client id: f_00001-4-7 loss: 0.800314  [  256/  265]
train() client id: f_00001-5-0 loss: 0.871515  [   32/  265]
train() client id: f_00001-5-1 loss: 0.904306  [   64/  265]
train() client id: f_00001-5-2 loss: 0.840548  [   96/  265]
train() client id: f_00001-5-3 loss: 0.794747  [  128/  265]
train() client id: f_00001-5-4 loss: 0.812825  [  160/  265]
train() client id: f_00001-5-5 loss: 0.878458  [  192/  265]
train() client id: f_00001-5-6 loss: 0.822765  [  224/  265]
train() client id: f_00001-5-7 loss: 0.839840  [  256/  265]
train() client id: f_00001-6-0 loss: 0.833617  [   32/  265]
train() client id: f_00001-6-1 loss: 0.863822  [   64/  265]
train() client id: f_00001-6-2 loss: 0.897023  [   96/  265]
train() client id: f_00001-6-3 loss: 0.930507  [  128/  265]
train() client id: f_00001-6-4 loss: 0.787026  [  160/  265]
train() client id: f_00001-6-5 loss: 0.804255  [  192/  265]
train() client id: f_00001-6-6 loss: 0.859793  [  224/  265]
train() client id: f_00001-6-7 loss: 0.817985  [  256/  265]
train() client id: f_00001-7-0 loss: 0.801420  [   32/  265]
train() client id: f_00001-7-1 loss: 0.878439  [   64/  265]
train() client id: f_00001-7-2 loss: 0.869151  [   96/  265]
train() client id: f_00001-7-3 loss: 0.840505  [  128/  265]
train() client id: f_00001-7-4 loss: 0.981019  [  160/  265]
train() client id: f_00001-7-5 loss: 0.842989  [  192/  265]
train() client id: f_00001-7-6 loss: 0.854049  [  224/  265]
train() client id: f_00001-7-7 loss: 0.907703  [  256/  265]
train() client id: f_00001-8-0 loss: 1.030581  [   32/  265]
train() client id: f_00001-8-1 loss: 0.879686  [   64/  265]
train() client id: f_00001-8-2 loss: 0.805960  [   96/  265]
train() client id: f_00001-8-3 loss: 0.856960  [  128/  265]
train() client id: f_00001-8-4 loss: 0.859514  [  160/  265]
train() client id: f_00001-8-5 loss: 0.853011  [  192/  265]
train() client id: f_00001-8-6 loss: 0.818934  [  224/  265]
train() client id: f_00001-8-7 loss: 0.900859  [  256/  265]
train() client id: f_00001-9-0 loss: 1.000661  [   32/  265]
train() client id: f_00001-9-1 loss: 0.870505  [   64/  265]
train() client id: f_00001-9-2 loss: 0.842991  [   96/  265]
train() client id: f_00001-9-3 loss: 0.861450  [  128/  265]
train() client id: f_00001-9-4 loss: 0.833499  [  160/  265]
train() client id: f_00001-9-5 loss: 0.954280  [  192/  265]
train() client id: f_00001-9-6 loss: 0.838263  [  224/  265]
train() client id: f_00001-9-7 loss: 0.857994  [  256/  265]
train() client id: f_00001-10-0 loss: 0.850705  [   32/  265]
train() client id: f_00001-10-1 loss: 0.914185  [   64/  265]
train() client id: f_00001-10-2 loss: 0.906075  [   96/  265]
train() client id: f_00001-10-3 loss: 0.845765  [  128/  265]
train() client id: f_00001-10-4 loss: 0.866903  [  160/  265]
train() client id: f_00001-10-5 loss: 0.830360  [  192/  265]
train() client id: f_00001-10-6 loss: 1.141483  [  224/  265]
train() client id: f_00001-10-7 loss: 0.841798  [  256/  265]
train() client id: f_00001-11-0 loss: 0.840991  [   32/  265]
train() client id: f_00001-11-1 loss: 0.911872  [   64/  265]
train() client id: f_00001-11-2 loss: 0.901192  [   96/  265]
train() client id: f_00001-11-3 loss: 0.841376  [  128/  265]
train() client id: f_00001-11-4 loss: 1.014744  [  160/  265]
train() client id: f_00001-11-5 loss: 0.990452  [  192/  265]
train() client id: f_00001-11-6 loss: 0.915614  [  224/  265]
train() client id: f_00001-11-7 loss: 0.912584  [  256/  265]
train() client id: f_00001-12-0 loss: 1.008443  [   32/  265]
train() client id: f_00001-12-1 loss: 1.024060  [   64/  265]
train() client id: f_00001-12-2 loss: 0.836846  [   96/  265]
train() client id: f_00001-12-3 loss: 0.930496  [  128/  265]
train() client id: f_00001-12-4 loss: 0.900768  [  160/  265]
train() client id: f_00001-12-5 loss: 0.832752  [  192/  265]
train() client id: f_00001-12-6 loss: 0.945938  [  224/  265]
train() client id: f_00001-12-7 loss: 0.843901  [  256/  265]
train() client id: f_00002-0-0 loss: 1.099134  [   32/  124]
train() client id: f_00002-0-1 loss: 1.054454  [   64/  124]
train() client id: f_00002-0-2 loss: 1.090231  [   96/  124]
train() client id: f_00002-1-0 loss: 1.110194  [   32/  124]
train() client id: f_00002-1-1 loss: 1.108609  [   64/  124]
train() client id: f_00002-1-2 loss: 1.045101  [   96/  124]
train() client id: f_00002-2-0 loss: 1.209941  [   32/  124]
train() client id: f_00002-2-1 loss: 1.012232  [   64/  124]
train() client id: f_00002-2-2 loss: 1.036070  [   96/  124]
train() client id: f_00002-3-0 loss: 1.096439  [   32/  124]
train() client id: f_00002-3-1 loss: 1.140358  [   64/  124]
train() client id: f_00002-3-2 loss: 1.010809  [   96/  124]
train() client id: f_00002-4-0 loss: 1.108872  [   32/  124]
train() client id: f_00002-4-1 loss: 1.063477  [   64/  124]
train() client id: f_00002-4-2 loss: 1.109444  [   96/  124]
train() client id: f_00002-5-0 loss: 1.074537  [   32/  124]
train() client id: f_00002-5-1 loss: 1.151101  [   64/  124]
train() client id: f_00002-5-2 loss: 1.059794  [   96/  124]
train() client id: f_00002-6-0 loss: 1.010466  [   32/  124]
train() client id: f_00002-6-1 loss: 1.022256  [   64/  124]
train() client id: f_00002-6-2 loss: 1.163385  [   96/  124]
train() client id: f_00002-7-0 loss: 0.978085  [   32/  124]
train() client id: f_00002-7-1 loss: 1.148186  [   64/  124]
train() client id: f_00002-7-2 loss: 1.107127  [   96/  124]
train() client id: f_00002-8-0 loss: 1.173017  [   32/  124]
train() client id: f_00002-8-1 loss: 1.070392  [   64/  124]
train() client id: f_00002-8-2 loss: 1.052153  [   96/  124]
train() client id: f_00002-9-0 loss: 1.122849  [   32/  124]
train() client id: f_00002-9-1 loss: 1.063056  [   64/  124]
train() client id: f_00002-9-2 loss: 0.983269  [   96/  124]
train() client id: f_00002-10-0 loss: 1.070091  [   32/  124]
train() client id: f_00002-10-1 loss: 1.164669  [   64/  124]
train() client id: f_00002-10-2 loss: 1.158233  [   96/  124]
train() client id: f_00002-11-0 loss: 1.089632  [   32/  124]
train() client id: f_00002-11-1 loss: 1.058599  [   64/  124]
train() client id: f_00002-11-2 loss: 1.185827  [   96/  124]
train() client id: f_00002-12-0 loss: 1.087649  [   32/  124]
train() client id: f_00002-12-1 loss: 1.157241  [   64/  124]
train() client id: f_00002-12-2 loss: 1.111229  [   96/  124]
train() client id: f_00003-0-0 loss: 1.118165  [   32/   43]
train() client id: f_00003-1-0 loss: 1.001962  [   32/   43]
train() client id: f_00003-2-0 loss: 1.107561  [   32/   43]
train() client id: f_00003-3-0 loss: 1.039678  [   32/   43]
train() client id: f_00003-4-0 loss: 1.012813  [   32/   43]
train() client id: f_00003-5-0 loss: 1.125389  [   32/   43]
train() client id: f_00003-6-0 loss: 0.997503  [   32/   43]
train() client id: f_00003-7-0 loss: 1.041934  [   32/   43]
train() client id: f_00003-8-0 loss: 0.964155  [   32/   43]
train() client id: f_00003-9-0 loss: 1.022729  [   32/   43]
train() client id: f_00003-10-0 loss: 0.978386  [   32/   43]
train() client id: f_00003-11-0 loss: 1.057428  [   32/   43]
train() client id: f_00003-12-0 loss: 0.978272  [   32/   43]
train() client id: f_00004-0-0 loss: 1.018873  [   32/  306]
train() client id: f_00004-0-1 loss: 0.973103  [   64/  306]
train() client id: f_00004-0-2 loss: 1.097593  [   96/  306]
train() client id: f_00004-0-3 loss: 0.986836  [  128/  306]
train() client id: f_00004-0-4 loss: 1.095576  [  160/  306]
train() client id: f_00004-0-5 loss: 1.118878  [  192/  306]
train() client id: f_00004-0-6 loss: 1.071087  [  224/  306]
train() client id: f_00004-0-7 loss: 1.000767  [  256/  306]
train() client id: f_00004-0-8 loss: 1.037332  [  288/  306]
train() client id: f_00004-1-0 loss: 1.031367  [   32/  306]
train() client id: f_00004-1-1 loss: 1.115124  [   64/  306]
train() client id: f_00004-1-2 loss: 1.009212  [   96/  306]
train() client id: f_00004-1-3 loss: 0.997468  [  128/  306]
train() client id: f_00004-1-4 loss: 1.069153  [  160/  306]
train() client id: f_00004-1-5 loss: 1.057496  [  192/  306]
train() client id: f_00004-1-6 loss: 1.036737  [  224/  306]
train() client id: f_00004-1-7 loss: 0.990635  [  256/  306]
train() client id: f_00004-1-8 loss: 1.012611  [  288/  306]
train() client id: f_00004-2-0 loss: 0.995130  [   32/  306]
train() client id: f_00004-2-1 loss: 1.006038  [   64/  306]
train() client id: f_00004-2-2 loss: 1.021530  [   96/  306]
train() client id: f_00004-2-3 loss: 0.947370  [  128/  306]
train() client id: f_00004-2-4 loss: 1.030488  [  160/  306]
train() client id: f_00004-2-5 loss: 1.078211  [  192/  306]
train() client id: f_00004-2-6 loss: 1.091323  [  224/  306]
train() client id: f_00004-2-7 loss: 1.081860  [  256/  306]
train() client id: f_00004-2-8 loss: 0.993838  [  288/  306]
train() client id: f_00004-3-0 loss: 1.011038  [   32/  306]
train() client id: f_00004-3-1 loss: 0.959680  [   64/  306]
train() client id: f_00004-3-2 loss: 1.057606  [   96/  306]
train() client id: f_00004-3-3 loss: 0.975226  [  128/  306]
train() client id: f_00004-3-4 loss: 0.997006  [  160/  306]
train() client id: f_00004-3-5 loss: 1.088884  [  192/  306]
train() client id: f_00004-3-6 loss: 0.993244  [  224/  306]
train() client id: f_00004-3-7 loss: 1.048815  [  256/  306]
train() client id: f_00004-3-8 loss: 1.136163  [  288/  306]
train() client id: f_00004-4-0 loss: 0.980598  [   32/  306]
train() client id: f_00004-4-1 loss: 1.057203  [   64/  306]
train() client id: f_00004-4-2 loss: 0.999185  [   96/  306]
train() client id: f_00004-4-3 loss: 1.039010  [  128/  306]
train() client id: f_00004-4-4 loss: 1.090722  [  160/  306]
train() client id: f_00004-4-5 loss: 0.998355  [  192/  306]
train() client id: f_00004-4-6 loss: 1.049873  [  224/  306]
train() client id: f_00004-4-7 loss: 1.038308  [  256/  306]
train() client id: f_00004-4-8 loss: 1.021007  [  288/  306]
train() client id: f_00004-5-0 loss: 0.985370  [   32/  306]
train() client id: f_00004-5-1 loss: 1.075012  [   64/  306]
train() client id: f_00004-5-2 loss: 1.002183  [   96/  306]
train() client id: f_00004-5-3 loss: 1.094403  [  128/  306]
train() client id: f_00004-5-4 loss: 1.046348  [  160/  306]
train() client id: f_00004-5-5 loss: 1.100845  [  192/  306]
train() client id: f_00004-5-6 loss: 0.972569  [  224/  306]
train() client id: f_00004-5-7 loss: 0.965882  [  256/  306]
train() client id: f_00004-5-8 loss: 1.034670  [  288/  306]
train() client id: f_00004-6-0 loss: 1.010957  [   32/  306]
train() client id: f_00004-6-1 loss: 0.976775  [   64/  306]
train() client id: f_00004-6-2 loss: 1.040854  [   96/  306]
train() client id: f_00004-6-3 loss: 1.052588  [  128/  306]
train() client id: f_00004-6-4 loss: 0.982022  [  160/  306]
train() client id: f_00004-6-5 loss: 1.041935  [  192/  306]
train() client id: f_00004-6-6 loss: 1.086424  [  224/  306]
train() client id: f_00004-6-7 loss: 1.054731  [  256/  306]
train() client id: f_00004-6-8 loss: 0.996778  [  288/  306]
train() client id: f_00004-7-0 loss: 1.035384  [   32/  306]
train() client id: f_00004-7-1 loss: 0.940124  [   64/  306]
train() client id: f_00004-7-2 loss: 1.014616  [   96/  306]
train() client id: f_00004-7-3 loss: 1.153438  [  128/  306]
train() client id: f_00004-7-4 loss: 1.037220  [  160/  306]
train() client id: f_00004-7-5 loss: 1.045889  [  192/  306]
train() client id: f_00004-7-6 loss: 0.969381  [  224/  306]
train() client id: f_00004-7-7 loss: 0.909142  [  256/  306]
train() client id: f_00004-7-8 loss: 1.067611  [  288/  306]
train() client id: f_00004-8-0 loss: 1.011049  [   32/  306]
train() client id: f_00004-8-1 loss: 0.962228  [   64/  306]
train() client id: f_00004-8-2 loss: 1.083948  [   96/  306]
train() client id: f_00004-8-3 loss: 0.989254  [  128/  306]
train() client id: f_00004-8-4 loss: 1.085142  [  160/  306]
train() client id: f_00004-8-5 loss: 1.015759  [  192/  306]
train() client id: f_00004-8-6 loss: 1.089253  [  224/  306]
train() client id: f_00004-8-7 loss: 0.993973  [  256/  306]
train() client id: f_00004-8-8 loss: 0.982137  [  288/  306]
train() client id: f_00004-9-0 loss: 0.951186  [   32/  306]
train() client id: f_00004-9-1 loss: 1.062493  [   64/  306]
train() client id: f_00004-9-2 loss: 1.160806  [   96/  306]
train() client id: f_00004-9-3 loss: 1.090340  [  128/  306]
train() client id: f_00004-9-4 loss: 0.931853  [  160/  306]
train() client id: f_00004-9-5 loss: 0.964259  [  192/  306]
train() client id: f_00004-9-6 loss: 1.047099  [  224/  306]
train() client id: f_00004-9-7 loss: 0.983363  [  256/  306]
train() client id: f_00004-9-8 loss: 1.067518  [  288/  306]
train() client id: f_00004-10-0 loss: 0.959348  [   32/  306]
train() client id: f_00004-10-1 loss: 1.006007  [   64/  306]
train() client id: f_00004-10-2 loss: 1.052191  [   96/  306]
train() client id: f_00004-10-3 loss: 1.004925  [  128/  306]
train() client id: f_00004-10-4 loss: 1.044362  [  160/  306]
train() client id: f_00004-10-5 loss: 1.014758  [  192/  306]
train() client id: f_00004-10-6 loss: 1.131222  [  224/  306]
train() client id: f_00004-10-7 loss: 0.980765  [  256/  306]
train() client id: f_00004-10-8 loss: 1.025187  [  288/  306]
train() client id: f_00004-11-0 loss: 1.075087  [   32/  306]
train() client id: f_00004-11-1 loss: 1.001758  [   64/  306]
train() client id: f_00004-11-2 loss: 0.997757  [   96/  306]
train() client id: f_00004-11-3 loss: 0.999064  [  128/  306]
train() client id: f_00004-11-4 loss: 0.917294  [  160/  306]
train() client id: f_00004-11-5 loss: 1.118041  [  192/  306]
train() client id: f_00004-11-6 loss: 0.969766  [  224/  306]
train() client id: f_00004-11-7 loss: 1.116103  [  256/  306]
train() client id: f_00004-11-8 loss: 1.004180  [  288/  306]
train() client id: f_00004-12-0 loss: 1.033363  [   32/  306]
train() client id: f_00004-12-1 loss: 0.939529  [   64/  306]
train() client id: f_00004-12-2 loss: 1.010915  [   96/  306]
train() client id: f_00004-12-3 loss: 1.031958  [  128/  306]
train() client id: f_00004-12-4 loss: 1.028901  [  160/  306]
train() client id: f_00004-12-5 loss: 1.115352  [  192/  306]
train() client id: f_00004-12-6 loss: 1.082461  [  224/  306]
train() client id: f_00004-12-7 loss: 0.983933  [  256/  306]
train() client id: f_00004-12-8 loss: 0.992285  [  288/  306]
train() client id: f_00005-0-0 loss: 0.971752  [   32/  146]
train() client id: f_00005-0-1 loss: 0.957409  [   64/  146]
train() client id: f_00005-0-2 loss: 0.959363  [   96/  146]
train() client id: f_00005-0-3 loss: 1.006006  [  128/  146]
train() client id: f_00005-1-0 loss: 0.995842  [   32/  146]
train() client id: f_00005-1-1 loss: 0.982808  [   64/  146]
train() client id: f_00005-1-2 loss: 0.939344  [   96/  146]
train() client id: f_00005-1-3 loss: 0.923364  [  128/  146]
train() client id: f_00005-2-0 loss: 0.960303  [   32/  146]
train() client id: f_00005-2-1 loss: 0.952056  [   64/  146]
train() client id: f_00005-2-2 loss: 0.989348  [   96/  146]
train() client id: f_00005-2-3 loss: 0.965978  [  128/  146]
train() client id: f_00005-3-0 loss: 0.860393  [   32/  146]
train() client id: f_00005-3-1 loss: 1.048513  [   64/  146]
train() client id: f_00005-3-2 loss: 0.962898  [   96/  146]
train() client id: f_00005-3-3 loss: 0.961630  [  128/  146]
train() client id: f_00005-4-0 loss: 0.873655  [   32/  146]
train() client id: f_00005-4-1 loss: 0.977293  [   64/  146]
train() client id: f_00005-4-2 loss: 1.024519  [   96/  146]
train() client id: f_00005-4-3 loss: 0.961579  [  128/  146]
train() client id: f_00005-5-0 loss: 0.982110  [   32/  146]
train() client id: f_00005-5-1 loss: 0.962214  [   64/  146]
train() client id: f_00005-5-2 loss: 0.955646  [   96/  146]
train() client id: f_00005-5-3 loss: 0.936719  [  128/  146]
train() client id: f_00005-6-0 loss: 1.000967  [   32/  146]
train() client id: f_00005-6-1 loss: 0.978771  [   64/  146]
train() client id: f_00005-6-2 loss: 0.869434  [   96/  146]
train() client id: f_00005-6-3 loss: 0.915204  [  128/  146]
train() client id: f_00005-7-0 loss: 0.966185  [   32/  146]
train() client id: f_00005-7-1 loss: 1.010726  [   64/  146]
train() client id: f_00005-7-2 loss: 0.913540  [   96/  146]
train() client id: f_00005-7-3 loss: 0.933596  [  128/  146]
train() client id: f_00005-8-0 loss: 1.027138  [   32/  146]
train() client id: f_00005-8-1 loss: 0.966132  [   64/  146]
train() client id: f_00005-8-2 loss: 0.986837  [   96/  146]
train() client id: f_00005-8-3 loss: 0.881183  [  128/  146]
train() client id: f_00005-9-0 loss: 1.001262  [   32/  146]
train() client id: f_00005-9-1 loss: 0.993362  [   64/  146]
train() client id: f_00005-9-2 loss: 0.937966  [   96/  146]
train() client id: f_00005-9-3 loss: 1.005890  [  128/  146]
train() client id: f_00005-10-0 loss: 0.899139  [   32/  146]
train() client id: f_00005-10-1 loss: 0.917243  [   64/  146]
train() client id: f_00005-10-2 loss: 1.143492  [   96/  146]
train() client id: f_00005-10-3 loss: 0.944262  [  128/  146]
train() client id: f_00005-11-0 loss: 0.932326  [   32/  146]
train() client id: f_00005-11-1 loss: 0.957636  [   64/  146]
train() client id: f_00005-11-2 loss: 0.922234  [   96/  146]
train() client id: f_00005-11-3 loss: 1.041988  [  128/  146]
train() client id: f_00005-12-0 loss: 1.023551  [   32/  146]
train() client id: f_00005-12-1 loss: 1.020514  [   64/  146]
train() client id: f_00005-12-2 loss: 0.910824  [   96/  146]
train() client id: f_00005-12-3 loss: 1.062798  [  128/  146]
train() client id: f_00006-0-0 loss: 1.074367  [   32/   54]
train() client id: f_00006-1-0 loss: 1.069526  [   32/   54]
train() client id: f_00006-2-0 loss: 1.077517  [   32/   54]
train() client id: f_00006-3-0 loss: 1.092186  [   32/   54]
train() client id: f_00006-4-0 loss: 1.054001  [   32/   54]
train() client id: f_00006-5-0 loss: 1.071318  [   32/   54]
train() client id: f_00006-6-0 loss: 1.070469  [   32/   54]
train() client id: f_00006-7-0 loss: 1.089414  [   32/   54]
train() client id: f_00006-8-0 loss: 1.078711  [   32/   54]
train() client id: f_00006-9-0 loss: 1.093446  [   32/   54]
train() client id: f_00006-10-0 loss: 1.067776  [   32/   54]
train() client id: f_00006-11-0 loss: 1.080618  [   32/   54]
train() client id: f_00006-12-0 loss: 1.106909  [   32/   54]
train() client id: f_00007-0-0 loss: 1.046168  [   32/  179]
train() client id: f_00007-0-1 loss: 1.086120  [   64/  179]
train() client id: f_00007-0-2 loss: 1.048914  [   96/  179]
train() client id: f_00007-0-3 loss: 1.072989  [  128/  179]
train() client id: f_00007-0-4 loss: 1.013067  [  160/  179]
train() client id: f_00007-1-0 loss: 1.006968  [   32/  179]
train() client id: f_00007-1-1 loss: 1.101729  [   64/  179]
train() client id: f_00007-1-2 loss: 1.004421  [   96/  179]
train() client id: f_00007-1-3 loss: 1.062243  [  128/  179]
train() client id: f_00007-1-4 loss: 1.092774  [  160/  179]
train() client id: f_00007-2-0 loss: 1.064851  [   32/  179]
train() client id: f_00007-2-1 loss: 1.094882  [   64/  179]
train() client id: f_00007-2-2 loss: 1.069953  [   96/  179]
train() client id: f_00007-2-3 loss: 1.042207  [  128/  179]
train() client id: f_00007-2-4 loss: 1.090035  [  160/  179]
train() client id: f_00007-3-0 loss: 1.103950  [   32/  179]
train() client id: f_00007-3-1 loss: 1.100626  [   64/  179]
train() client id: f_00007-3-2 loss: 1.075939  [   96/  179]
train() client id: f_00007-3-3 loss: 1.061998  [  128/  179]
train() client id: f_00007-3-4 loss: 1.020722  [  160/  179]
train() client id: f_00007-4-0 loss: 1.090969  [   32/  179]
train() client id: f_00007-4-1 loss: 1.155407  [   64/  179]
train() client id: f_00007-4-2 loss: 1.116095  [   96/  179]
train() client id: f_00007-4-3 loss: 1.050304  [  128/  179]
train() client id: f_00007-4-4 loss: 1.034464  [  160/  179]
train() client id: f_00007-5-0 loss: 1.057356  [   32/  179]
train() client id: f_00007-5-1 loss: 1.107658  [   64/  179]
train() client id: f_00007-5-2 loss: 1.190074  [   96/  179]
train() client id: f_00007-5-3 loss: 1.156731  [  128/  179]
train() client id: f_00007-5-4 loss: 1.090483  [  160/  179]
train() client id: f_00007-6-0 loss: 1.142697  [   32/  179]
train() client id: f_00007-6-1 loss: 1.124944  [   64/  179]
train() client id: f_00007-6-2 loss: 1.061740  [   96/  179]
train() client id: f_00007-6-3 loss: 1.103791  [  128/  179]
train() client id: f_00007-6-4 loss: 1.253522  [  160/  179]
train() client id: f_00007-7-0 loss: 1.164261  [   32/  179]
train() client id: f_00007-7-1 loss: 1.143130  [   64/  179]
train() client id: f_00007-7-2 loss: 1.170213  [   96/  179]
train() client id: f_00007-7-3 loss: 1.144611  [  128/  179]
train() client id: f_00007-7-4 loss: 1.066761  [  160/  179]
train() client id: f_00007-8-0 loss: 1.117901  [   32/  179]
train() client id: f_00007-8-1 loss: 1.090256  [   64/  179]
train() client id: f_00007-8-2 loss: 1.314850  [   96/  179]
train() client id: f_00007-8-3 loss: 1.091778  [  128/  179]
train() client id: f_00007-8-4 loss: 1.104218  [  160/  179]
train() client id: f_00007-9-0 loss: 1.229717  [   32/  179]
train() client id: f_00007-9-1 loss: 1.241638  [   64/  179]
train() client id: f_00007-9-2 loss: 1.180102  [   96/  179]
train() client id: f_00007-9-3 loss: 1.142380  [  128/  179]
train() client id: f_00007-9-4 loss: 1.126811  [  160/  179]
train() client id: f_00007-10-0 loss: 1.151094  [   32/  179]
train() client id: f_00007-10-1 loss: 1.240296  [   64/  179]
train() client id: f_00007-10-2 loss: 1.175742  [   96/  179]
train() client id: f_00007-10-3 loss: 1.199248  [  128/  179]
train() client id: f_00007-10-4 loss: 1.235789  [  160/  179]
train() client id: f_00007-11-0 loss: 1.205491  [   32/  179]
train() client id: f_00007-11-1 loss: 1.192204  [   64/  179]
train() client id: f_00007-11-2 loss: 1.116729  [   96/  179]
train() client id: f_00007-11-3 loss: 1.195236  [  128/  179]
train() client id: f_00007-11-4 loss: 1.390840  [  160/  179]
train() client id: f_00007-12-0 loss: 1.207423  [   32/  179]
train() client id: f_00007-12-1 loss: 1.179302  [   64/  179]
train() client id: f_00007-12-2 loss: 1.164861  [   96/  179]
train() client id: f_00007-12-3 loss: 1.235560  [  128/  179]
train() client id: f_00007-12-4 loss: 1.396811  [  160/  179]
train() client id: f_00008-0-0 loss: 0.997371  [   32/  130]
train() client id: f_00008-0-1 loss: 0.994904  [   64/  130]
train() client id: f_00008-0-2 loss: 1.016898  [   96/  130]
train() client id: f_00008-0-3 loss: 0.997669  [  128/  130]
train() client id: f_00008-1-0 loss: 1.056563  [   32/  130]
train() client id: f_00008-1-1 loss: 0.997942  [   64/  130]
train() client id: f_00008-1-2 loss: 0.947381  [   96/  130]
train() client id: f_00008-1-3 loss: 0.975999  [  128/  130]
train() client id: f_00008-2-0 loss: 0.980987  [   32/  130]
train() client id: f_00008-2-1 loss: 1.077309  [   64/  130]
train() client id: f_00008-2-2 loss: 0.961365  [   96/  130]
train() client id: f_00008-2-3 loss: 1.002830  [  128/  130]
train() client id: f_00008-3-0 loss: 0.942035  [   32/  130]
train() client id: f_00008-3-1 loss: 1.084056  [   64/  130]
train() client id: f_00008-3-2 loss: 0.993098  [   96/  130]
train() client id: f_00008-3-3 loss: 1.020351  [  128/  130]
train() client id: f_00008-4-0 loss: 0.948303  [   32/  130]
train() client id: f_00008-4-1 loss: 1.037651  [   64/  130]
train() client id: f_00008-4-2 loss: 1.060444  [   96/  130]
train() client id: f_00008-4-3 loss: 1.030485  [  128/  130]
train() client id: f_00008-5-0 loss: 1.020140  [   32/  130]
train() client id: f_00008-5-1 loss: 1.002477  [   64/  130]
train() client id: f_00008-5-2 loss: 1.041210  [   96/  130]
train() client id: f_00008-5-3 loss: 0.998252  [  128/  130]
train() client id: f_00008-6-0 loss: 1.032137  [   32/  130]
train() client id: f_00008-6-1 loss: 1.112486  [   64/  130]
train() client id: f_00008-6-2 loss: 0.954020  [   96/  130]
train() client id: f_00008-6-3 loss: 1.004894  [  128/  130]
train() client id: f_00008-7-0 loss: 1.016771  [   32/  130]
train() client id: f_00008-7-1 loss: 0.986484  [   64/  130]
train() client id: f_00008-7-2 loss: 0.992060  [   96/  130]
train() client id: f_00008-7-3 loss: 1.117678  [  128/  130]
train() client id: f_00008-8-0 loss: 1.023032  [   32/  130]
train() client id: f_00008-8-1 loss: 1.035915  [   64/  130]
train() client id: f_00008-8-2 loss: 1.010026  [   96/  130]
train() client id: f_00008-8-3 loss: 1.074178  [  128/  130]
train() client id: f_00008-9-0 loss: 1.027767  [   32/  130]
train() client id: f_00008-9-1 loss: 1.123991  [   64/  130]
train() client id: f_00008-9-2 loss: 1.037227  [   96/  130]
train() client id: f_00008-9-3 loss: 0.983079  [  128/  130]
train() client id: f_00008-10-0 loss: 1.032847  [   32/  130]
train() client id: f_00008-10-1 loss: 0.978311  [   64/  130]
train() client id: f_00008-10-2 loss: 1.073422  [   96/  130]
train() client id: f_00008-10-3 loss: 1.094780  [  128/  130]
train() client id: f_00008-11-0 loss: 1.059212  [   32/  130]
train() client id: f_00008-11-1 loss: 1.072303  [   64/  130]
train() client id: f_00008-11-2 loss: 1.068474  [   96/  130]
train() client id: f_00008-11-3 loss: 0.981729  [  128/  130]
train() client id: f_00008-12-0 loss: 1.084753  [   32/  130]
train() client id: f_00008-12-1 loss: 0.966530  [   64/  130]
train() client id: f_00008-12-2 loss: 1.063070  [   96/  130]
train() client id: f_00008-12-3 loss: 1.109454  [  128/  130]
train() client id: f_00009-0-0 loss: 0.961334  [   32/  118]
train() client id: f_00009-0-1 loss: 0.984451  [   64/  118]
train() client id: f_00009-0-2 loss: 1.025596  [   96/  118]
train() client id: f_00009-1-0 loss: 1.041101  [   32/  118]
train() client id: f_00009-1-1 loss: 1.008993  [   64/  118]
train() client id: f_00009-1-2 loss: 1.004416  [   96/  118]
train() client id: f_00009-2-0 loss: 0.931143  [   32/  118]
train() client id: f_00009-2-1 loss: 0.957367  [   64/  118]
train() client id: f_00009-2-2 loss: 1.047319  [   96/  118]
train() client id: f_00009-3-0 loss: 0.976435  [   32/  118]
train() client id: f_00009-3-1 loss: 1.053333  [   64/  118]
train() client id: f_00009-3-2 loss: 1.054491  [   96/  118]
train() client id: f_00009-4-0 loss: 1.162312  [   32/  118]
train() client id: f_00009-4-1 loss: 0.946363  [   64/  118]
train() client id: f_00009-4-2 loss: 0.966515  [   96/  118]
train() client id: f_00009-5-0 loss: 1.036523  [   32/  118]
train() client id: f_00009-5-1 loss: 0.977593  [   64/  118]
train() client id: f_00009-5-2 loss: 1.049950  [   96/  118]
train() client id: f_00009-6-0 loss: 1.040476  [   32/  118]
train() client id: f_00009-6-1 loss: 1.094913  [   64/  118]
train() client id: f_00009-6-2 loss: 0.987194  [   96/  118]
train() client id: f_00009-7-0 loss: 1.026340  [   32/  118]
train() client id: f_00009-7-1 loss: 1.197964  [   64/  118]
train() client id: f_00009-7-2 loss: 0.968141  [   96/  118]
train() client id: f_00009-8-0 loss: 1.002680  [   32/  118]
train() client id: f_00009-8-1 loss: 1.085067  [   64/  118]
train() client id: f_00009-8-2 loss: 1.065017  [   96/  118]
train() client id: f_00009-9-0 loss: 1.028651  [   32/  118]
train() client id: f_00009-9-1 loss: 1.043551  [   64/  118]
train() client id: f_00009-9-2 loss: 1.067498  [   96/  118]
train() client id: f_00009-10-0 loss: 1.086443  [   32/  118]
train() client id: f_00009-10-1 loss: 1.109931  [   64/  118]
train() client id: f_00009-10-2 loss: 1.023092  [   96/  118]
train() client id: f_00009-11-0 loss: 1.105406  [   32/  118]
train() client id: f_00009-11-1 loss: 1.061689  [   64/  118]
train() client id: f_00009-11-2 loss: 1.021041  [   96/  118]
train() client id: f_00009-12-0 loss: 1.157560  [   32/  118]
train() client id: f_00009-12-1 loss: 0.948573  [   64/  118]
train() client id: f_00009-12-2 loss: 1.139859  [   96/  118]
At round 0 accuracy: 0.4482758620689655
At round 0 training accuracy: 0.43259557344064387
At round 0 training loss: 1.0525474668175427
update_location
xs = 8.927491 121.223621 5.882650 10.934260 -37.581990 114.769243 -5.849135 -5.143845 -60.120581 20.134486 
ys = -112.390647 7.291448 10.684448 -142.290817 -9.642386 0.794442 -151.381692 6.628436 25.881276 -547.232496 
xs mean: 17.317620029478793
ys mean: -91.16579882624052
dists_uav = 150.702879 157.316024 100.741069 174.259102 107.263142 152.225524 181.523082 100.351359 119.517048 556.658605 
uav_gains = -104.457880 -104.927810 -100.080182 -106.060795 -100.761306 -104.567713 -106.524069 -100.038098 -101.935949 -125.527027 
uav_gains_db_mean: -105.48808286539283
dists_bs = 341.207717 340.403572 244.372940 367.756729 230.166289 338.103224 367.609881 239.165418 188.238345 748.129164 
bs_gains = -110.491587 -110.462894 -106.432546 -111.402757 -105.704227 -110.380440 -111.397900 -106.170614 -103.258886 -120.038404 
bs_gains_db_mean: -109.57402544045618
Round 1
-------------------------------
ene_coms = [0.00772813 0.007916   0.00858202 0.0084076  0.00824663 0.00777126
 0.00862604 0.00845841 0.00728604 0.02793821]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.90362117 22.69775515 10.75800999  3.88334315 26.1843398  12.60107149
  4.82241648 15.41918708 11.23043664 10.79345932]
obj_prev = 129.2936402721433
eta_min = 2.45259235935905e-09	eta_max = 0.7347566418206299
af = 27.184654844112664	bf = 2.8107418649374813	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.8107418649374813	zeta = 59.41887826653207	eta = 0.4575087183937051
af = 27.184654844112664	bf = 2.8107418649374813	zeta = 44.36575735420609	eta = 0.6127395645943015
af = 27.184654844112664	bf = 2.8107418649374813	zeta = 41.64174779948347	eta = 0.6528221383745537
af = 27.184654844112664	bf = 2.8107418649374813	zeta = 41.48782171633626	eta = 0.6552442070827841
af = 27.184654844112664	bf = 2.8107418649374813	zeta = 41.48728168287175	eta = 0.6552527362942652
af = 27.184654844112664	bf = 2.8107418649374813	zeta = 41.48728167619144	eta = 0.6552527363997745
eta = 0.6552527363997745
ene_coms = [0.00772813 0.007916   0.00858202 0.0084076  0.00824663 0.00777126
 0.00862604 0.00845841 0.00728604 0.02793821]
ene_comp = [0.03488302 0.07336508 0.03432932 0.01190452 0.0847159  0.04042
 0.01494986 0.04955603 0.03599042 0.03266822]
ene_total = [3.44106214 6.56385159 3.46530438 1.6403052  7.50718707 3.89168417
 1.90387158 4.68495513 3.4947893  4.89427111]
ti_comp = [0.2935821  0.29170337 0.28504317 0.28678736 0.28839707 0.29315083
 0.284603   0.28627927 0.29800297 0.09148129]
ti_coms = [0.07728129 0.07916002 0.08582023 0.08407603 0.08246632 0.07771257
 0.08626039 0.08458412 0.07286042 0.27938211]
t_total = [29.94906425 29.94906425 29.94906425 29.94906425 29.94906425 29.94906425
 29.94906425 29.94906425 29.94906425 29.94906425]
ene_coms = [0.00772813 0.007916   0.00858202 0.0084076  0.00824663 0.00777126
 0.00862604 0.00845841 0.00728604 0.02793821]
ene_comp = [3.07796034e-05 2.90044912e-04 3.11210562e-05 1.28202325e-06
 4.56870497e-04 4.80271550e-05 2.57817788e-06 9.28089405e-05
 3.28094851e-05 2.60370660e-04]
ene_total = [0.62657047 0.66267909 0.69555421 0.67905934 0.70285111 0.63144607
 0.69680381 0.69055362 0.5910337  2.27717575]
optimize_network iter = 0 obj = 8.253727167519624
eta = 0.6552527363997745
freqs = [5.94093061e+07 1.25752877e+08 6.02177555e+07 2.07549631e+07
 1.46873717e+08 6.89406296e+07 2.62644192e+07 8.65519057e+07
 6.03860015e+07 1.78551401e+08]
eta_min = 0.6552527363992048	eta_max = 0.6552527363997723
af = 0.08210356670074465	bf = 2.8107418649374813	zeta = 0.09031392337081912	eta = 0.9090909090909091
af = 0.08210356670074465	bf = 2.8107418649374813	zeta = 31.004238877065355	eta = 0.0026481400503425615
af = 0.08210356670074465	bf = 2.8107418649374813	zeta = 3.306692620074438	eta = 0.024829512789397518
af = 0.08210356670074465	bf = 2.8107418649374813	zeta = 3.1934660536482795	eta = 0.02570986048433109
af = 0.08210356670074465	bf = 2.8107418649374813	zeta = 3.1934154931769396	eta = 0.025710267541498234
eta = 0.025710267541498234
ene_coms = [0.00772813 0.007916   0.00858202 0.0084076  0.00824663 0.00777126
 0.00862604 0.00845841 0.00728604 0.02793821]
ene_comp = [2.66550358e-04 2.51177944e-03 2.69507328e-04 1.11022793e-05
 3.95648354e-03 4.15913589e-04 2.23269360e-05 8.03722385e-04
 2.84129066e-04 2.25480139e-03]
ene_total = [0.22844573 0.29797093 0.25293    0.24056216 0.34870061 0.2339461
 0.24712465 0.26466289 0.21631553 0.86275689]
ti_comp = [0.2935821  0.29170337 0.28504317 0.28678736 0.28839707 0.29315083
 0.284603   0.28627927 0.29800297 0.09148129]
ti_coms = [0.07728129 0.07916002 0.08582023 0.08407603 0.08246632 0.07771257
 0.08626039 0.08458412 0.07286042 0.27938211]
t_total = [29.94906425 29.94906425 29.94906425 29.94906425 29.94906425 29.94906425
 29.94906425 29.94906425 29.94906425 29.94906425]
ene_coms = [0.00772813 0.007916   0.00858202 0.0084076  0.00824663 0.00777126
 0.00862604 0.00845841 0.00728604 0.02793821]
ene_comp = [3.07796034e-05 2.90044912e-04 3.11210562e-05 1.28202325e-06
 4.56870497e-04 4.80271550e-05 2.57817788e-06 9.28089405e-05
 3.28094851e-05 2.60370660e-04]
ene_total = [0.62657047 0.66267909 0.69555421 0.67905934 0.70285111 0.63144607
 0.69680381 0.69055362 0.5910337  2.27717575]
optimize_network iter = 1 obj = 8.253727167505986
eta = 0.6552527363992048
freqs = [5.94093061e+07 1.25752877e+08 6.02177555e+07 2.07549631e+07
 1.46873717e+08 6.89406296e+07 2.62644192e+07 8.65519057e+07
 6.03860015e+07 1.78551401e+08]
Done!
ene_coms = [0.00772813 0.007916   0.00858202 0.0084076  0.00824663 0.00777126
 0.00862604 0.00845841 0.00728604 0.02793821]
ene_comp = [2.89063237e-05 2.72392467e-04 2.92269953e-05 1.20399794e-06
 4.29064868e-04 4.51041708e-05 2.42126721e-06 8.71604887e-05
 3.08126646e-05 2.44524222e-04]
ene_total = [0.00775704 0.00818839 0.00861125 0.00840881 0.0086757  0.00781636
 0.00862846 0.00854557 0.00731685 0.02818273]
At round 1 energy consumption: 0.10213116621700205
At round 1 eta: 0.6552527363992048
At round 1 a_n: 27.840057009285058
At round 1 local rounds: 13.842467442028378
At round 1 global rounds: 80.75497603230531
gradient difference: 0.2801772654056549
train() client id: f_00000-0-0 loss: 1.880260  [   32/  126]
train() client id: f_00000-0-1 loss: 2.119670  [   64/  126]
train() client id: f_00000-0-2 loss: 1.948599  [   96/  126]
train() client id: f_00000-1-0 loss: 1.647870  [   32/  126]
train() client id: f_00000-1-1 loss: 1.762323  [   64/  126]
train() client id: f_00000-1-2 loss: 1.737260  [   96/  126]
train() client id: f_00000-2-0 loss: 1.633953  [   32/  126]
train() client id: f_00000-2-1 loss: 1.607934  [   64/  126]
train() client id: f_00000-2-2 loss: 1.430977  [   96/  126]
train() client id: f_00000-3-0 loss: 1.609155  [   32/  126]
train() client id: f_00000-3-1 loss: 1.444843  [   64/  126]
train() client id: f_00000-3-2 loss: 1.454300  [   96/  126]
train() client id: f_00000-4-0 loss: 1.314111  [   32/  126]
train() client id: f_00000-4-1 loss: 1.209279  [   64/  126]
train() client id: f_00000-4-2 loss: 1.372877  [   96/  126]
train() client id: f_00000-5-0 loss: 1.229473  [   32/  126]
train() client id: f_00000-5-1 loss: 1.170948  [   64/  126]
train() client id: f_00000-5-2 loss: 1.242063  [   96/  126]
train() client id: f_00000-6-0 loss: 1.109691  [   32/  126]
train() client id: f_00000-6-1 loss: 1.123967  [   64/  126]
train() client id: f_00000-6-2 loss: 1.129372  [   96/  126]
train() client id: f_00000-7-0 loss: 1.040284  [   32/  126]
train() client id: f_00000-7-1 loss: 1.089104  [   64/  126]
train() client id: f_00000-7-2 loss: 1.063079  [   96/  126]
train() client id: f_00000-8-0 loss: 1.011615  [   32/  126]
train() client id: f_00000-8-1 loss: 1.054352  [   64/  126]
train() client id: f_00000-8-2 loss: 0.934012  [   96/  126]
train() client id: f_00000-9-0 loss: 0.974580  [   32/  126]
train() client id: f_00000-9-1 loss: 0.959008  [   64/  126]
train() client id: f_00000-9-2 loss: 0.998250  [   96/  126]
train() client id: f_00000-10-0 loss: 0.941337  [   32/  126]
train() client id: f_00000-10-1 loss: 0.943307  [   64/  126]
train() client id: f_00000-10-2 loss: 0.914457  [   96/  126]
train() client id: f_00000-11-0 loss: 0.875340  [   32/  126]
train() client id: f_00000-11-1 loss: 0.932138  [   64/  126]
train() client id: f_00000-11-2 loss: 0.944023  [   96/  126]
train() client id: f_00000-12-0 loss: 1.034714  [   32/  126]
train() client id: f_00000-12-1 loss: 0.857628  [   64/  126]
train() client id: f_00000-12-2 loss: 0.875808  [   96/  126]
train() client id: f_00001-0-0 loss: 0.808841  [   32/  265]
train() client id: f_00001-0-1 loss: 0.806702  [   64/  265]
train() client id: f_00001-0-2 loss: 0.770494  [   96/  265]
train() client id: f_00001-0-3 loss: 0.819480  [  128/  265]
train() client id: f_00001-0-4 loss: 0.733003  [  160/  265]
train() client id: f_00001-0-5 loss: 0.778508  [  192/  265]
train() client id: f_00001-0-6 loss: 0.798818  [  224/  265]
train() client id: f_00001-0-7 loss: 0.725188  [  256/  265]
train() client id: f_00001-1-0 loss: 0.718498  [   32/  265]
train() client id: f_00001-1-1 loss: 0.764478  [   64/  265]
train() client id: f_00001-1-2 loss: 0.707816  [   96/  265]
train() client id: f_00001-1-3 loss: 0.740480  [  128/  265]
train() client id: f_00001-1-4 loss: 0.660820  [  160/  265]
train() client id: f_00001-1-5 loss: 0.717819  [  192/  265]
train() client id: f_00001-1-6 loss: 0.677550  [  224/  265]
train() client id: f_00001-1-7 loss: 0.652138  [  256/  265]
train() client id: f_00001-2-0 loss: 0.647083  [   32/  265]
train() client id: f_00001-2-1 loss: 0.697681  [   64/  265]
train() client id: f_00001-2-2 loss: 0.687980  [   96/  265]
train() client id: f_00001-2-3 loss: 0.639577  [  128/  265]
train() client id: f_00001-2-4 loss: 0.680447  [  160/  265]
train() client id: f_00001-2-5 loss: 0.646088  [  192/  265]
train() client id: f_00001-2-6 loss: 0.633170  [  224/  265]
train() client id: f_00001-2-7 loss: 0.669545  [  256/  265]
train() client id: f_00001-3-0 loss: 0.635306  [   32/  265]
train() client id: f_00001-3-1 loss: 0.701047  [   64/  265]
train() client id: f_00001-3-2 loss: 0.664801  [   96/  265]
train() client id: f_00001-3-3 loss: 0.584341  [  128/  265]
train() client id: f_00001-3-4 loss: 0.638506  [  160/  265]
train() client id: f_00001-3-5 loss: 0.646329  [  192/  265]
train() client id: f_00001-3-6 loss: 0.592392  [  224/  265]
train() client id: f_00001-3-7 loss: 0.533204  [  256/  265]
train() client id: f_00001-4-0 loss: 0.638168  [   32/  265]
train() client id: f_00001-4-1 loss: 0.643397  [   64/  265]
train() client id: f_00001-4-2 loss: 0.607605  [   96/  265]
train() client id: f_00001-4-3 loss: 0.577356  [  128/  265]
train() client id: f_00001-4-4 loss: 0.567457  [  160/  265]
train() client id: f_00001-4-5 loss: 0.629237  [  192/  265]
train() client id: f_00001-4-6 loss: 0.539273  [  224/  265]
train() client id: f_00001-4-7 loss: 0.652511  [  256/  265]
train() client id: f_00001-5-0 loss: 0.624485  [   32/  265]
train() client id: f_00001-5-1 loss: 0.584143  [   64/  265]
train() client id: f_00001-5-2 loss: 0.623626  [   96/  265]
train() client id: f_00001-5-3 loss: 0.621699  [  128/  265]
train() client id: f_00001-5-4 loss: 0.614268  [  160/  265]
train() client id: f_00001-5-5 loss: 0.546558  [  192/  265]
train() client id: f_00001-5-6 loss: 0.559280  [  224/  265]
train() client id: f_00001-5-7 loss: 0.513197  [  256/  265]
train() client id: f_00001-6-0 loss: 0.647751  [   32/  265]
train() client id: f_00001-6-1 loss: 0.524673  [   64/  265]
train() client id: f_00001-6-2 loss: 0.580296  [   96/  265]
train() client id: f_00001-6-3 loss: 0.551667  [  128/  265]
train() client id: f_00001-6-4 loss: 0.563490  [  160/  265]
train() client id: f_00001-6-5 loss: 0.547485  [  192/  265]
train() client id: f_00001-6-6 loss: 0.575733  [  224/  265]
train() client id: f_00001-6-7 loss: 0.553057  [  256/  265]
train() client id: f_00001-7-0 loss: 0.565517  [   32/  265]
train() client id: f_00001-7-1 loss: 0.576806  [   64/  265]
train() client id: f_00001-7-2 loss: 0.539089  [   96/  265]
train() client id: f_00001-7-3 loss: 0.651664  [  128/  265]
train() client id: f_00001-7-4 loss: 0.552235  [  160/  265]
train() client id: f_00001-7-5 loss: 0.585662  [  192/  265]
train() client id: f_00001-7-6 loss: 0.549770  [  224/  265]
train() client id: f_00001-7-7 loss: 0.504367  [  256/  265]
train() client id: f_00001-8-0 loss: 0.538712  [   32/  265]
train() client id: f_00001-8-1 loss: 0.551334  [   64/  265]
train() client id: f_00001-8-2 loss: 0.525879  [   96/  265]
train() client id: f_00001-8-3 loss: 0.522504  [  128/  265]
train() client id: f_00001-8-4 loss: 0.560111  [  160/  265]
train() client id: f_00001-8-5 loss: 0.538145  [  192/  265]
train() client id: f_00001-8-6 loss: 0.555757  [  224/  265]
train() client id: f_00001-8-7 loss: 0.622483  [  256/  265]
train() client id: f_00001-9-0 loss: 0.552738  [   32/  265]
train() client id: f_00001-9-1 loss: 0.638486  [   64/  265]
train() client id: f_00001-9-2 loss: 0.481554  [   96/  265]
train() client id: f_00001-9-3 loss: 0.615328  [  128/  265]
train() client id: f_00001-9-4 loss: 0.522546  [  160/  265]
train() client id: f_00001-9-5 loss: 0.532807  [  192/  265]
train() client id: f_00001-9-6 loss: 0.471218  [  224/  265]
train() client id: f_00001-9-7 loss: 0.576742  [  256/  265]
train() client id: f_00001-10-0 loss: 0.679758  [   32/  265]
train() client id: f_00001-10-1 loss: 0.576200  [   64/  265]
train() client id: f_00001-10-2 loss: 0.473960  [   96/  265]
train() client id: f_00001-10-3 loss: 0.476442  [  128/  265]
train() client id: f_00001-10-4 loss: 0.468356  [  160/  265]
train() client id: f_00001-10-5 loss: 0.542329  [  192/  265]
train() client id: f_00001-10-6 loss: 0.591405  [  224/  265]
train() client id: f_00001-10-7 loss: 0.481257  [  256/  265]
train() client id: f_00001-11-0 loss: 0.485882  [   32/  265]
train() client id: f_00001-11-1 loss: 0.661547  [   64/  265]
train() client id: f_00001-11-2 loss: 0.595804  [   96/  265]
train() client id: f_00001-11-3 loss: 0.475032  [  128/  265]
train() client id: f_00001-11-4 loss: 0.579723  [  160/  265]
train() client id: f_00001-11-5 loss: 0.516775  [  192/  265]
train() client id: f_00001-11-6 loss: 0.473178  [  224/  265]
train() client id: f_00001-11-7 loss: 0.496564  [  256/  265]
train() client id: f_00001-12-0 loss: 0.514211  [   32/  265]
train() client id: f_00001-12-1 loss: 0.617939  [   64/  265]
train() client id: f_00001-12-2 loss: 0.577309  [   96/  265]
train() client id: f_00001-12-3 loss: 0.587021  [  128/  265]
train() client id: f_00001-12-4 loss: 0.515143  [  160/  265]
train() client id: f_00001-12-5 loss: 0.518013  [  192/  265]
train() client id: f_00001-12-6 loss: 0.488814  [  224/  265]
train() client id: f_00001-12-7 loss: 0.477001  [  256/  265]
train() client id: f_00002-0-0 loss: 1.044716  [   32/  124]
train() client id: f_00002-0-1 loss: 1.084921  [   64/  124]
train() client id: f_00002-0-2 loss: 0.985131  [   96/  124]
train() client id: f_00002-1-0 loss: 1.013772  [   32/  124]
train() client id: f_00002-1-1 loss: 1.016714  [   64/  124]
train() client id: f_00002-1-2 loss: 1.016265  [   96/  124]
train() client id: f_00002-2-0 loss: 0.961996  [   32/  124]
train() client id: f_00002-2-1 loss: 0.922892  [   64/  124]
train() client id: f_00002-2-2 loss: 0.967463  [   96/  124]
train() client id: f_00002-3-0 loss: 0.957227  [   32/  124]
train() client id: f_00002-3-1 loss: 0.930844  [   64/  124]
train() client id: f_00002-3-2 loss: 0.977258  [   96/  124]
train() client id: f_00002-4-0 loss: 0.896439  [   32/  124]
train() client id: f_00002-4-1 loss: 0.977511  [   64/  124]
train() client id: f_00002-4-2 loss: 0.890999  [   96/  124]
train() client id: f_00002-5-0 loss: 0.929585  [   32/  124]
train() client id: f_00002-5-1 loss: 0.963852  [   64/  124]
train() client id: f_00002-5-2 loss: 0.898300  [   96/  124]
train() client id: f_00002-6-0 loss: 0.926778  [   32/  124]
train() client id: f_00002-6-1 loss: 0.847420  [   64/  124]
train() client id: f_00002-6-2 loss: 0.878792  [   96/  124]
train() client id: f_00002-7-0 loss: 0.937973  [   32/  124]
train() client id: f_00002-7-1 loss: 0.908944  [   64/  124]
train() client id: f_00002-7-2 loss: 0.861086  [   96/  124]
train() client id: f_00002-8-0 loss: 0.847817  [   32/  124]
train() client id: f_00002-8-1 loss: 0.870579  [   64/  124]
train() client id: f_00002-8-2 loss: 0.909971  [   96/  124]
train() client id: f_00002-9-0 loss: 0.867926  [   32/  124]
train() client id: f_00002-9-1 loss: 0.800380  [   64/  124]
train() client id: f_00002-9-2 loss: 0.885918  [   96/  124]
train() client id: f_00002-10-0 loss: 0.773374  [   32/  124]
train() client id: f_00002-10-1 loss: 0.885744  [   64/  124]
train() client id: f_00002-10-2 loss: 0.926531  [   96/  124]
train() client id: f_00002-11-0 loss: 0.888810  [   32/  124]
train() client id: f_00002-11-1 loss: 0.831500  [   64/  124]
train() client id: f_00002-11-2 loss: 0.897215  [   96/  124]
train() client id: f_00002-12-0 loss: 0.921452  [   32/  124]
train() client id: f_00002-12-1 loss: 0.883375  [   64/  124]
train() client id: f_00002-12-2 loss: 0.829773  [   96/  124]
train() client id: f_00003-0-0 loss: 0.986683  [   32/   43]
train() client id: f_00003-1-0 loss: 1.118367  [   32/   43]
train() client id: f_00003-2-0 loss: 1.006542  [   32/   43]
train() client id: f_00003-3-0 loss: 1.082272  [   32/   43]
train() client id: f_00003-4-0 loss: 1.103379  [   32/   43]
train() client id: f_00003-5-0 loss: 1.057129  [   32/   43]
train() client id: f_00003-6-0 loss: 0.953480  [   32/   43]
train() client id: f_00003-7-0 loss: 1.041735  [   32/   43]
train() client id: f_00003-8-0 loss: 1.142094  [   32/   43]
train() client id: f_00003-9-0 loss: 0.934282  [   32/   43]
train() client id: f_00003-10-0 loss: 1.072813  [   32/   43]
train() client id: f_00003-11-0 loss: 1.096147  [   32/   43]
train() client id: f_00003-12-0 loss: 1.170156  [   32/   43]
train() client id: f_00004-0-0 loss: 1.027899  [   32/  306]
train() client id: f_00004-0-1 loss: 0.976041  [   64/  306]
train() client id: f_00004-0-2 loss: 1.023745  [   96/  306]
train() client id: f_00004-0-3 loss: 1.057915  [  128/  306]
train() client id: f_00004-0-4 loss: 0.960300  [  160/  306]
train() client id: f_00004-0-5 loss: 0.983038  [  192/  306]
train() client id: f_00004-0-6 loss: 0.895757  [  224/  306]
train() client id: f_00004-0-7 loss: 1.046471  [  256/  306]
train() client id: f_00004-0-8 loss: 1.076494  [  288/  306]
train() client id: f_00004-1-0 loss: 1.113447  [   32/  306]
train() client id: f_00004-1-1 loss: 1.108216  [   64/  306]
train() client id: f_00004-1-2 loss: 1.032952  [   96/  306]
train() client id: f_00004-1-3 loss: 0.936302  [  128/  306]
train() client id: f_00004-1-4 loss: 0.978005  [  160/  306]
train() client id: f_00004-1-5 loss: 0.841935  [  192/  306]
train() client id: f_00004-1-6 loss: 0.991432  [  224/  306]
train() client id: f_00004-1-7 loss: 0.962335  [  256/  306]
train() client id: f_00004-1-8 loss: 1.008854  [  288/  306]
train() client id: f_00004-2-0 loss: 0.979617  [   32/  306]
train() client id: f_00004-2-1 loss: 1.121930  [   64/  306]
train() client id: f_00004-2-2 loss: 0.866283  [   96/  306]
train() client id: f_00004-2-3 loss: 0.960356  [  128/  306]
train() client id: f_00004-2-4 loss: 1.004456  [  160/  306]
train() client id: f_00004-2-5 loss: 0.923769  [  192/  306]
train() client id: f_00004-2-6 loss: 1.103693  [  224/  306]
train() client id: f_00004-2-7 loss: 0.991283  [  256/  306]
train() client id: f_00004-2-8 loss: 0.981926  [  288/  306]
train() client id: f_00004-3-0 loss: 0.984174  [   32/  306]
train() client id: f_00004-3-1 loss: 0.928690  [   64/  306]
train() client id: f_00004-3-2 loss: 0.976772  [   96/  306]
train() client id: f_00004-3-3 loss: 1.071477  [  128/  306]
train() client id: f_00004-3-4 loss: 0.983362  [  160/  306]
train() client id: f_00004-3-5 loss: 0.993346  [  192/  306]
train() client id: f_00004-3-6 loss: 1.054210  [  224/  306]
train() client id: f_00004-3-7 loss: 0.941001  [  256/  306]
train() client id: f_00004-3-8 loss: 0.970309  [  288/  306]
train() client id: f_00004-4-0 loss: 1.073878  [   32/  306]
train() client id: f_00004-4-1 loss: 0.961278  [   64/  306]
train() client id: f_00004-4-2 loss: 0.925499  [   96/  306]
train() client id: f_00004-4-3 loss: 1.010711  [  128/  306]
train() client id: f_00004-4-4 loss: 1.001098  [  160/  306]
train() client id: f_00004-4-5 loss: 0.974093  [  192/  306]
train() client id: f_00004-4-6 loss: 0.904773  [  224/  306]
train() client id: f_00004-4-7 loss: 0.985650  [  256/  306]
train() client id: f_00004-4-8 loss: 0.962034  [  288/  306]
train() client id: f_00004-5-0 loss: 0.960069  [   32/  306]
train() client id: f_00004-5-1 loss: 0.981897  [   64/  306]
train() client id: f_00004-5-2 loss: 1.090096  [   96/  306]
train() client id: f_00004-5-3 loss: 0.959415  [  128/  306]
train() client id: f_00004-5-4 loss: 1.005217  [  160/  306]
train() client id: f_00004-5-5 loss: 0.900755  [  192/  306]
train() client id: f_00004-5-6 loss: 0.944084  [  224/  306]
train() client id: f_00004-5-7 loss: 1.042797  [  256/  306]
train() client id: f_00004-5-8 loss: 0.955562  [  288/  306]
train() client id: f_00004-6-0 loss: 0.951931  [   32/  306]
train() client id: f_00004-6-1 loss: 1.011899  [   64/  306]
train() client id: f_00004-6-2 loss: 0.888551  [   96/  306]
train() client id: f_00004-6-3 loss: 0.911738  [  128/  306]
train() client id: f_00004-6-4 loss: 1.030303  [  160/  306]
train() client id: f_00004-6-5 loss: 0.937250  [  192/  306]
train() client id: f_00004-6-6 loss: 0.957025  [  224/  306]
train() client id: f_00004-6-7 loss: 1.039091  [  256/  306]
train() client id: f_00004-6-8 loss: 0.985809  [  288/  306]
train() client id: f_00004-7-0 loss: 0.968170  [   32/  306]
train() client id: f_00004-7-1 loss: 1.030497  [   64/  306]
train() client id: f_00004-7-2 loss: 0.993969  [   96/  306]
train() client id: f_00004-7-3 loss: 0.908729  [  128/  306]
train() client id: f_00004-7-4 loss: 0.928015  [  160/  306]
train() client id: f_00004-7-5 loss: 0.949547  [  192/  306]
train() client id: f_00004-7-6 loss: 0.992724  [  224/  306]
train() client id: f_00004-7-7 loss: 0.966142  [  256/  306]
train() client id: f_00004-7-8 loss: 0.922676  [  288/  306]
train() client id: f_00004-8-0 loss: 0.957693  [   32/  306]
train() client id: f_00004-8-1 loss: 0.984537  [   64/  306]
train() client id: f_00004-8-2 loss: 1.127765  [   96/  306]
train() client id: f_00004-8-3 loss: 0.884876  [  128/  306]
train() client id: f_00004-8-4 loss: 0.996980  [  160/  306]
train() client id: f_00004-8-5 loss: 0.894004  [  192/  306]
train() client id: f_00004-8-6 loss: 0.961428  [  224/  306]
train() client id: f_00004-8-7 loss: 0.878714  [  256/  306]
train() client id: f_00004-8-8 loss: 1.002417  [  288/  306]
train() client id: f_00004-9-0 loss: 0.980900  [   32/  306]
train() client id: f_00004-9-1 loss: 0.924764  [   64/  306]
train() client id: f_00004-9-2 loss: 0.999980  [   96/  306]
train() client id: f_00004-9-3 loss: 0.895845  [  128/  306]
train() client id: f_00004-9-4 loss: 0.931458  [  160/  306]
train() client id: f_00004-9-5 loss: 0.976720  [  192/  306]
train() client id: f_00004-9-6 loss: 0.964264  [  224/  306]
train() client id: f_00004-9-7 loss: 0.947743  [  256/  306]
train() client id: f_00004-9-8 loss: 1.044985  [  288/  306]
train() client id: f_00004-10-0 loss: 1.078505  [   32/  306]
train() client id: f_00004-10-1 loss: 0.914783  [   64/  306]
train() client id: f_00004-10-2 loss: 0.925062  [   96/  306]
train() client id: f_00004-10-3 loss: 0.977580  [  128/  306]
train() client id: f_00004-10-4 loss: 0.885778  [  160/  306]
train() client id: f_00004-10-5 loss: 1.009998  [  192/  306]
train() client id: f_00004-10-6 loss: 0.934732  [  224/  306]
train() client id: f_00004-10-7 loss: 0.961060  [  256/  306]
train() client id: f_00004-10-8 loss: 0.971128  [  288/  306]
train() client id: f_00004-11-0 loss: 0.948889  [   32/  306]
train() client id: f_00004-11-1 loss: 0.931600  [   64/  306]
train() client id: f_00004-11-2 loss: 1.012912  [   96/  306]
train() client id: f_00004-11-3 loss: 0.939382  [  128/  306]
train() client id: f_00004-11-4 loss: 0.910030  [  160/  306]
train() client id: f_00004-11-5 loss: 0.937098  [  192/  306]
train() client id: f_00004-11-6 loss: 1.032509  [  224/  306]
train() client id: f_00004-11-7 loss: 0.941235  [  256/  306]
train() client id: f_00004-11-8 loss: 1.060868  [  288/  306]
train() client id: f_00004-12-0 loss: 0.964170  [   32/  306]
train() client id: f_00004-12-1 loss: 1.039044  [   64/  306]
train() client id: f_00004-12-2 loss: 0.905412  [   96/  306]
train() client id: f_00004-12-3 loss: 0.914073  [  128/  306]
train() client id: f_00004-12-4 loss: 0.986001  [  160/  306]
train() client id: f_00004-12-5 loss: 1.005513  [  192/  306]
train() client id: f_00004-12-6 loss: 0.876595  [  224/  306]
train() client id: f_00004-12-7 loss: 1.018234  [  256/  306]
train() client id: f_00004-12-8 loss: 0.970634  [  288/  306]
train() client id: f_00005-0-0 loss: 1.075101  [   32/  146]
train() client id: f_00005-0-1 loss: 1.120729  [   64/  146]
train() client id: f_00005-0-2 loss: 1.057703  [   96/  146]
train() client id: f_00005-0-3 loss: 1.066618  [  128/  146]
train() client id: f_00005-1-0 loss: 1.060172  [   32/  146]
train() client id: f_00005-1-1 loss: 1.036771  [   64/  146]
train() client id: f_00005-1-2 loss: 1.008107  [   96/  146]
train() client id: f_00005-1-3 loss: 1.023384  [  128/  146]
train() client id: f_00005-2-0 loss: 1.049730  [   32/  146]
train() client id: f_00005-2-1 loss: 0.964840  [   64/  146]
train() client id: f_00005-2-2 loss: 0.966987  [   96/  146]
train() client id: f_00005-2-3 loss: 0.965230  [  128/  146]
train() client id: f_00005-3-0 loss: 0.933310  [   32/  146]
train() client id: f_00005-3-1 loss: 0.938221  [   64/  146]
train() client id: f_00005-3-2 loss: 0.989730  [   96/  146]
train() client id: f_00005-3-3 loss: 0.982251  [  128/  146]
train() client id: f_00005-4-0 loss: 0.917464  [   32/  146]
train() client id: f_00005-4-1 loss: 0.953517  [   64/  146]
train() client id: f_00005-4-2 loss: 0.937365  [   96/  146]
train() client id: f_00005-4-3 loss: 0.898807  [  128/  146]
train() client id: f_00005-5-0 loss: 0.878227  [   32/  146]
train() client id: f_00005-5-1 loss: 0.956013  [   64/  146]
train() client id: f_00005-5-2 loss: 0.904959  [   96/  146]
train() client id: f_00005-5-3 loss: 0.884824  [  128/  146]
train() client id: f_00005-6-0 loss: 0.875998  [   32/  146]
train() client id: f_00005-6-1 loss: 0.822661  [   64/  146]
train() client id: f_00005-6-2 loss: 0.885230  [   96/  146]
train() client id: f_00005-6-3 loss: 0.885696  [  128/  146]
train() client id: f_00005-7-0 loss: 0.852206  [   32/  146]
train() client id: f_00005-7-1 loss: 0.805032  [   64/  146]
train() client id: f_00005-7-2 loss: 0.961754  [   96/  146]
train() client id: f_00005-7-3 loss: 0.894824  [  128/  146]
train() client id: f_00005-8-0 loss: 0.857349  [   32/  146]
train() client id: f_00005-8-1 loss: 0.838799  [   64/  146]
train() client id: f_00005-8-2 loss: 0.897685  [   96/  146]
train() client id: f_00005-8-3 loss: 0.856774  [  128/  146]
train() client id: f_00005-9-0 loss: 0.836855  [   32/  146]
train() client id: f_00005-9-1 loss: 0.790863  [   64/  146]
train() client id: f_00005-9-2 loss: 0.778438  [   96/  146]
train() client id: f_00005-9-3 loss: 0.932787  [  128/  146]
train() client id: f_00005-10-0 loss: 0.845827  [   32/  146]
train() client id: f_00005-10-1 loss: 0.903802  [   64/  146]
train() client id: f_00005-10-2 loss: 0.796642  [   96/  146]
train() client id: f_00005-10-3 loss: 0.717938  [  128/  146]
train() client id: f_00005-11-0 loss: 0.890978  [   32/  146]
train() client id: f_00005-11-1 loss: 0.801343  [   64/  146]
train() client id: f_00005-11-2 loss: 0.779230  [   96/  146]
train() client id: f_00005-11-3 loss: 0.794951  [  128/  146]
train() client id: f_00005-12-0 loss: 0.752181  [   32/  146]
train() client id: f_00005-12-1 loss: 0.857295  [   64/  146]
train() client id: f_00005-12-2 loss: 0.843351  [   96/  146]
train() client id: f_00005-12-3 loss: 0.875328  [  128/  146]
train() client id: f_00006-0-0 loss: 1.043985  [   32/   54]
train() client id: f_00006-1-0 loss: 1.048275  [   32/   54]
train() client id: f_00006-2-0 loss: 1.025688  [   32/   54]
train() client id: f_00006-3-0 loss: 1.044315  [   32/   54]
train() client id: f_00006-4-0 loss: 1.066210  [   32/   54]
train() client id: f_00006-5-0 loss: 1.003238  [   32/   54]
train() client id: f_00006-6-0 loss: 1.053122  [   32/   54]
train() client id: f_00006-7-0 loss: 1.027982  [   32/   54]
train() client id: f_00006-8-0 loss: 1.021029  [   32/   54]
train() client id: f_00006-9-0 loss: 1.015934  [   32/   54]
train() client id: f_00006-10-0 loss: 1.042729  [   32/   54]
train() client id: f_00006-11-0 loss: 1.065863  [   32/   54]
train() client id: f_00006-12-0 loss: 1.020065  [   32/   54]
train() client id: f_00007-0-0 loss: 1.040671  [   32/  179]
train() client id: f_00007-0-1 loss: 1.016782  [   64/  179]
train() client id: f_00007-0-2 loss: 1.049458  [   96/  179]
train() client id: f_00007-0-3 loss: 0.975800  [  128/  179]
train() client id: f_00007-0-4 loss: 0.921134  [  160/  179]
train() client id: f_00007-1-0 loss: 0.931606  [   32/  179]
train() client id: f_00007-1-1 loss: 0.881078  [   64/  179]
train() client id: f_00007-1-2 loss: 0.966827  [   96/  179]
train() client id: f_00007-1-3 loss: 0.858496  [  128/  179]
train() client id: f_00007-1-4 loss: 0.916347  [  160/  179]
train() client id: f_00007-2-0 loss: 0.892448  [   32/  179]
train() client id: f_00007-2-1 loss: 0.819204  [   64/  179]
train() client id: f_00007-2-2 loss: 0.874353  [   96/  179]
train() client id: f_00007-2-3 loss: 0.776936  [  128/  179]
train() client id: f_00007-2-4 loss: 0.807850  [  160/  179]
train() client id: f_00007-3-0 loss: 0.810246  [   32/  179]
train() client id: f_00007-3-1 loss: 0.721920  [   64/  179]
train() client id: f_00007-3-2 loss: 0.744271  [   96/  179]
train() client id: f_00007-3-3 loss: 0.810896  [  128/  179]
train() client id: f_00007-3-4 loss: 0.761106  [  160/  179]
train() client id: f_00007-4-0 loss: 0.759919  [   32/  179]
train() client id: f_00007-4-1 loss: 0.697211  [   64/  179]
train() client id: f_00007-4-2 loss: 0.718214  [   96/  179]
train() client id: f_00007-4-3 loss: 0.775314  [  128/  179]
train() client id: f_00007-4-4 loss: 0.706273  [  160/  179]
train() client id: f_00007-5-0 loss: 0.664491  [   32/  179]
train() client id: f_00007-5-1 loss: 0.689787  [   64/  179]
train() client id: f_00007-5-2 loss: 0.630914  [   96/  179]
train() client id: f_00007-5-3 loss: 0.787629  [  128/  179]
train() client id: f_00007-5-4 loss: 0.699509  [  160/  179]
train() client id: f_00007-6-0 loss: 0.601490  [   32/  179]
train() client id: f_00007-6-1 loss: 0.667384  [   64/  179]
train() client id: f_00007-6-2 loss: 0.618069  [   96/  179]
train() client id: f_00007-6-3 loss: 0.727290  [  128/  179]
train() client id: f_00007-6-4 loss: 0.711885  [  160/  179]
train() client id: f_00007-7-0 loss: 0.614027  [   32/  179]
train() client id: f_00007-7-1 loss: 0.623861  [   64/  179]
train() client id: f_00007-7-2 loss: 0.608299  [   96/  179]
train() client id: f_00007-7-3 loss: 0.658319  [  128/  179]
train() client id: f_00007-7-4 loss: 0.595314  [  160/  179]
train() client id: f_00007-8-0 loss: 0.620038  [   32/  179]
train() client id: f_00007-8-1 loss: 0.632439  [   64/  179]
train() client id: f_00007-8-2 loss: 0.573014  [   96/  179]
train() client id: f_00007-8-3 loss: 0.568482  [  128/  179]
train() client id: f_00007-8-4 loss: 0.614730  [  160/  179]
train() client id: f_00007-9-0 loss: 0.678220  [   32/  179]
train() client id: f_00007-9-1 loss: 0.667802  [   64/  179]
train() client id: f_00007-9-2 loss: 0.579756  [   96/  179]
train() client id: f_00007-9-3 loss: 0.514615  [  128/  179]
train() client id: f_00007-9-4 loss: 0.511818  [  160/  179]
train() client id: f_00007-10-0 loss: 0.564742  [   32/  179]
train() client id: f_00007-10-1 loss: 0.663620  [   64/  179]
train() client id: f_00007-10-2 loss: 0.637186  [   96/  179]
train() client id: f_00007-10-3 loss: 0.492220  [  128/  179]
train() client id: f_00007-10-4 loss: 0.553686  [  160/  179]
train() client id: f_00007-11-0 loss: 0.582118  [   32/  179]
train() client id: f_00007-11-1 loss: 0.543352  [   64/  179]
train() client id: f_00007-11-2 loss: 0.595754  [   96/  179]
train() client id: f_00007-11-3 loss: 0.526892  [  128/  179]
train() client id: f_00007-11-4 loss: 0.597755  [  160/  179]
train() client id: f_00007-12-0 loss: 0.592787  [   32/  179]
train() client id: f_00007-12-1 loss: 0.497335  [   64/  179]
train() client id: f_00007-12-2 loss: 0.491112  [   96/  179]
train() client id: f_00007-12-3 loss: 0.607533  [  128/  179]
train() client id: f_00007-12-4 loss: 0.601563  [  160/  179]
train() client id: f_00008-0-0 loss: 0.882536  [   32/  130]
train() client id: f_00008-0-1 loss: 0.881250  [   64/  130]
train() client id: f_00008-0-2 loss: 0.924494  [   96/  130]
train() client id: f_00008-0-3 loss: 0.886019  [  128/  130]
train() client id: f_00008-1-0 loss: 0.884860  [   32/  130]
train() client id: f_00008-1-1 loss: 0.904844  [   64/  130]
train() client id: f_00008-1-2 loss: 0.905571  [   96/  130]
train() client id: f_00008-1-3 loss: 0.841113  [  128/  130]
train() client id: f_00008-2-0 loss: 0.911531  [   32/  130]
train() client id: f_00008-2-1 loss: 0.872016  [   64/  130]
train() client id: f_00008-2-2 loss: 0.870569  [   96/  130]
train() client id: f_00008-2-3 loss: 0.854218  [  128/  130]
train() client id: f_00008-3-0 loss: 0.891137  [   32/  130]
train() client id: f_00008-3-1 loss: 0.846899  [   64/  130]
train() client id: f_00008-3-2 loss: 0.834458  [   96/  130]
train() client id: f_00008-3-3 loss: 0.879491  [  128/  130]
train() client id: f_00008-4-0 loss: 0.893980  [   32/  130]
train() client id: f_00008-4-1 loss: 0.760960  [   64/  130]
train() client id: f_00008-4-2 loss: 0.863119  [   96/  130]
train() client id: f_00008-4-3 loss: 0.936359  [  128/  130]
train() client id: f_00008-5-0 loss: 0.865204  [   32/  130]
train() client id: f_00008-5-1 loss: 0.843619  [   64/  130]
train() client id: f_00008-5-2 loss: 0.901066  [   96/  130]
train() client id: f_00008-5-3 loss: 0.813907  [  128/  130]
train() client id: f_00008-6-0 loss: 0.829873  [   32/  130]
train() client id: f_00008-6-1 loss: 0.885978  [   64/  130]
train() client id: f_00008-6-2 loss: 0.825774  [   96/  130]
train() client id: f_00008-6-3 loss: 0.884500  [  128/  130]
train() client id: f_00008-7-0 loss: 0.875600  [   32/  130]
train() client id: f_00008-7-1 loss: 0.909817  [   64/  130]
train() client id: f_00008-7-2 loss: 0.815920  [   96/  130]
train() client id: f_00008-7-3 loss: 0.816936  [  128/  130]
train() client id: f_00008-8-0 loss: 0.832343  [   32/  130]
train() client id: f_00008-8-1 loss: 0.879549  [   64/  130]
train() client id: f_00008-8-2 loss: 0.827066  [   96/  130]
train() client id: f_00008-8-3 loss: 0.874517  [  128/  130]
train() client id: f_00008-9-0 loss: 0.853868  [   32/  130]
train() client id: f_00008-9-1 loss: 0.800743  [   64/  130]
train() client id: f_00008-9-2 loss: 0.895344  [   96/  130]
train() client id: f_00008-9-3 loss: 0.843294  [  128/  130]
train() client id: f_00008-10-0 loss: 0.757830  [   32/  130]
train() client id: f_00008-10-1 loss: 0.903966  [   64/  130]
train() client id: f_00008-10-2 loss: 0.816093  [   96/  130]
train() client id: f_00008-10-3 loss: 0.889521  [  128/  130]
train() client id: f_00008-11-0 loss: 0.915271  [   32/  130]
train() client id: f_00008-11-1 loss: 0.739335  [   64/  130]
train() client id: f_00008-11-2 loss: 0.823187  [   96/  130]
train() client id: f_00008-11-3 loss: 0.895599  [  128/  130]
train() client id: f_00008-12-0 loss: 0.813629  [   32/  130]
train() client id: f_00008-12-1 loss: 0.841950  [   64/  130]
train() client id: f_00008-12-2 loss: 0.831451  [   96/  130]
train() client id: f_00008-12-3 loss: 0.881497  [  128/  130]
train() client id: f_00009-0-0 loss: 1.035682  [   32/  118]
train() client id: f_00009-0-1 loss: 1.023853  [   64/  118]
train() client id: f_00009-0-2 loss: 1.027972  [   96/  118]
train() client id: f_00009-1-0 loss: 0.965033  [   32/  118]
train() client id: f_00009-1-1 loss: 1.022360  [   64/  118]
train() client id: f_00009-1-2 loss: 0.964295  [   96/  118]
train() client id: f_00009-2-0 loss: 0.963780  [   32/  118]
train() client id: f_00009-2-1 loss: 0.954714  [   64/  118]
train() client id: f_00009-2-2 loss: 0.930229  [   96/  118]
train() client id: f_00009-3-0 loss: 0.892834  [   32/  118]
train() client id: f_00009-3-1 loss: 0.927276  [   64/  118]
train() client id: f_00009-3-2 loss: 0.968922  [   96/  118]
train() client id: f_00009-4-0 loss: 0.864193  [   32/  118]
train() client id: f_00009-4-1 loss: 0.893062  [   64/  118]
train() client id: f_00009-4-2 loss: 0.923265  [   96/  118]
train() client id: f_00009-5-0 loss: 0.896601  [   32/  118]
train() client id: f_00009-5-1 loss: 0.871290  [   64/  118]
train() client id: f_00009-5-2 loss: 0.896288  [   96/  118]
train() client id: f_00009-6-0 loss: 0.856793  [   32/  118]
train() client id: f_00009-6-1 loss: 0.880902  [   64/  118]
train() client id: f_00009-6-2 loss: 0.862068  [   96/  118]
train() client id: f_00009-7-0 loss: 0.883808  [   32/  118]
train() client id: f_00009-7-1 loss: 0.842233  [   64/  118]
train() client id: f_00009-7-2 loss: 0.829811  [   96/  118]
train() client id: f_00009-8-0 loss: 0.834680  [   32/  118]
train() client id: f_00009-8-1 loss: 0.781152  [   64/  118]
train() client id: f_00009-8-2 loss: 0.870320  [   96/  118]
train() client id: f_00009-9-0 loss: 0.787976  [   32/  118]
train() client id: f_00009-9-1 loss: 0.865957  [   64/  118]
train() client id: f_00009-9-2 loss: 0.836250  [   96/  118]
train() client id: f_00009-10-0 loss: 0.799772  [   32/  118]
train() client id: f_00009-10-1 loss: 0.791423  [   64/  118]
train() client id: f_00009-10-2 loss: 0.815918  [   96/  118]
train() client id: f_00009-11-0 loss: 0.769231  [   32/  118]
train() client id: f_00009-11-1 loss: 0.786646  [   64/  118]
train() client id: f_00009-11-2 loss: 0.842133  [   96/  118]
train() client id: f_00009-12-0 loss: 0.853134  [   32/  118]
train() client id: f_00009-12-1 loss: 0.786836  [   64/  118]
train() client id: f_00009-12-2 loss: 0.803604  [   96/  118]
At round 1 accuracy: 0.5623342175066313
At round 1 training accuracy: 0.5412474849094567
At round 1 training loss: 0.9921914924861415
update_location
xs = 8.927491 126.223621 5.882650 10.934260 -42.581990 109.769243 -5.849135 -5.143845 -65.120581 20.134486 
ys = -117.390647 7.291448 15.684448 -137.290817 -9.642386 0.794442 -146.381692 11.628436 25.881276 -552.232496 
xs mean: 16.317620029478793
ys mean: -90.16579882624053
dists_uav = 154.467680 161.200396 101.393331 170.200842 109.115542 148.492147 177.374779 100.805157 122.108683 561.574685 
uav_gains = -104.727632 -105.195771 -100.150254 -105.797129 -100.947221 -104.296569 -106.260690 -100.087087 -102.168937 -125.626792 
uav_gains_db_mean: -105.52580822865815
dists_bs = 345.429606 344.763438 241.039370 363.451652 227.216505 333.827947 363.177928 235.672190 185.229265 752.957217 
bs_gains = -110.641127 -110.617653 -106.265523 -111.259565 -105.547376 -110.225694 -111.250404 -105.991693 -103.062928 -120.116628 
bs_gains_db_mean: -109.49785895699449
Round 2
-------------------------------
ene_coms = [0.0078349  0.00802716 0.0085028  0.00828799 0.0081777  0.00766563
 0.00850053 0.00837593 0.00721846 0.02822808]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.77242782 22.42156797 10.62344237  3.83224007 25.86025133 12.44309322
  4.75959513 15.22717758 11.09037941 10.66870712]
obj_prev = 127.69888199483643
eta_min = 1.921053637317014e-09	eta_max = 0.7349116891253644
af = 26.850173107417966	bf = 2.772276491987626	zeta = 29.535190418159765	eta = 0.9090909090909091
af = 26.850173107417966	bf = 2.772276491987626	zeta = 58.64508450496169	eta = 0.4578418350671197
af = 26.850173107417966	bf = 2.772276491987626	zeta = 43.803596157901175	eta = 0.6129673237473404
af = 26.850173107417966	bf = 2.772276491987626	zeta = 41.117776858474635	eta = 0.6530064404949456
af = 26.850173107417966	bf = 2.772276491987626	zeta = 40.96608820728815	eta = 0.6554243834938854
af = 26.850173107417966	bf = 2.772276491987626	zeta = 40.96555665102728	eta = 0.6554328880758576
af = 26.850173107417966	bf = 2.772276491987626	zeta = 40.96555664446739	eta = 0.6554328881808132
eta = 0.6554328881808132
ene_coms = [0.0078349  0.00802716 0.0085028  0.00828799 0.0081777  0.00766563
 0.00850053 0.00837593 0.00721846 0.02822808]
ene_comp = [0.03486033 0.07331737 0.034307   0.01189678 0.08466081 0.04039372
 0.01494014 0.04952381 0.03596701 0.03264698]
ene_total = [3.40721022 6.49154265 3.41635298 1.61080688 7.4087974  3.83528317
 1.8706376  4.62057683 3.44633263 4.85801629]
ti_comp = [0.29629991 0.29437728 0.28962086 0.29176894 0.29287191 0.29799261
 0.28964357 0.29088954 0.30246432 0.09236808]
ti_coms = [0.07834897 0.0802716  0.08502802 0.08287994 0.08177697 0.07665627
 0.08500531 0.08375934 0.07218456 0.2822808 ]
t_total = [29.89812851 29.89812851 29.89812851 29.89812851 29.89812851 29.89812851
 29.89812851 29.89812851 29.89812851 29.89812851]
ene_coms = [0.0078349  0.00802716 0.0085028  0.00828799 0.0081777  0.00766563
 0.00850053 0.00837593 0.00721846 0.02822808]
ene_comp = [3.01586293e-05 2.84244493e-04 3.00862726e-05 1.23620436e-06
 4.42152265e-04 4.63885285e-05 2.48437131e-06 8.97151752e-05
 3.17866514e-05 2.54897304e-04]
ene_total = [0.6276555  0.66327554 0.68095061 0.66150598 0.68789038 0.61544247
 0.67856666 0.67558477 0.5785916  2.27302889]
optimize_network iter = 0 obj = 8.142492415900941
eta = 0.6554328881808132
freqs = [5.88260954e+07 1.24529599e+08 5.92274243e+07 2.03873324e+07
 1.44535560e+08 6.77763795e+07 2.57905657e+07 8.51247668e+07
 5.94566189e+07 1.76722201e+08]
eta_min = 0.6554328881808432	eta_max = 0.6554328881808081
af = 0.07896286752922867	bf = 2.772276491987626	zeta = 0.08685915428215155	eta = 0.909090909090909
af = 0.07896286752922867	bf = 2.772276491987626	zeta = 30.57782702798667	eta = 0.0025823570607864674
af = 0.07896286752922867	bf = 2.772276491987626	zeta = 3.251216182607996	eta = 0.02428717842622443
af = 0.07896286752922867	bf = 2.772276491987626	zeta = 3.1421593552110707	eta = 0.025130128234353802
af = 0.07896286752922867	bf = 2.772276491987626	zeta = 3.1421127418295427	eta = 0.025130501040917883
eta = 0.025130501040917883
ene_coms = [0.0078349  0.00802716 0.0085028  0.00828799 0.0081777  0.00766563
 0.00850053 0.00837593 0.00721846 0.02822808]
ene_comp = [2.62970922e-04 2.47849581e-03 2.62340001e-04 1.07791968e-05
 3.85538704e-03 4.04489009e-04 2.16627025e-05 7.82279661e-04
 2.77166608e-04 2.22260031e-03]
ene_total = [0.22841129 0.29632621 0.24723267 0.23407811 0.33940939 0.22762853
 0.24037998 0.2583198  0.21142417 0.85890259]
ti_comp = [0.29629991 0.29437728 0.28962086 0.29176894 0.29287191 0.29799261
 0.28964357 0.29088954 0.30246432 0.09236808]
ti_coms = [0.07834897 0.0802716  0.08502802 0.08287994 0.08177697 0.07665627
 0.08500531 0.08375934 0.07218456 0.2822808 ]
t_total = [29.89812851 29.89812851 29.89812851 29.89812851 29.89812851 29.89812851
 29.89812851 29.89812851 29.89812851 29.89812851]
ene_coms = [0.0078349  0.00802716 0.0085028  0.00828799 0.0081777  0.00766563
 0.00850053 0.00837593 0.00721846 0.02822808]
ene_comp = [3.01586293e-05 2.84244493e-04 3.00862726e-05 1.23620436e-06
 4.42152265e-04 4.63885285e-05 2.48437131e-06 8.97151752e-05
 3.17866514e-05 2.54897304e-04]
ene_total = [0.6276555  0.66327554 0.68095061 0.66150598 0.68789038 0.61544247
 0.67856666 0.67558477 0.5785916  2.27302889]
optimize_network iter = 1 obj = 8.14249241590165
eta = 0.6554328881808432
freqs = [5.88260954e+07 1.24529599e+08 5.92274243e+07 2.03873324e+07
 1.44535560e+08 6.77763795e+07 2.57905657e+07 8.51247668e+07
 5.94566189e+07 1.76722201e+08]
Done!
ene_coms = [0.0078349  0.00802716 0.0085028  0.00828799 0.0081777  0.00766563
 0.00850053 0.00837593 0.00721846 0.02822808]
ene_comp = [2.83415728e-05 2.67118771e-04 2.82735756e-05 1.16172309e-06
 4.15512604e-04 4.35936210e-05 2.33468801e-06 8.43098386e-05
 2.98715065e-05 2.39539749e-04]
ene_total = [0.00786324 0.00829428 0.00853108 0.00828916 0.00859321 0.00770922
 0.00850287 0.00846024 0.00724833 0.02846762]
At round 2 energy consumption: 0.10195923636016928
At round 2 eta: 0.6554328881808432
At round 2 a_n: 27.49751116231574
At round 2 local rounds: 13.833465917119314
At round 2 global rounds: 79.80306366774663
gradient difference: 0.4477047920227051
train() client id: f_00000-0-0 loss: 1.776635  [   32/  126]
train() client id: f_00000-0-1 loss: 1.587097  [   64/  126]
train() client id: f_00000-0-2 loss: 1.401393  [   96/  126]
train() client id: f_00000-1-0 loss: 1.454791  [   32/  126]
train() client id: f_00000-1-1 loss: 1.378512  [   64/  126]
train() client id: f_00000-1-2 loss: 1.514478  [   96/  126]
train() client id: f_00000-2-0 loss: 1.307710  [   32/  126]
train() client id: f_00000-2-1 loss: 1.191800  [   64/  126]
train() client id: f_00000-2-2 loss: 1.460442  [   96/  126]
train() client id: f_00000-3-0 loss: 1.336628  [   32/  126]
train() client id: f_00000-3-1 loss: 1.170940  [   64/  126]
train() client id: f_00000-3-2 loss: 1.144406  [   96/  126]
train() client id: f_00000-4-0 loss: 1.263414  [   32/  126]
train() client id: f_00000-4-1 loss: 1.063816  [   64/  126]
train() client id: f_00000-4-2 loss: 1.123982  [   96/  126]
train() client id: f_00000-5-0 loss: 1.107073  [   32/  126]
train() client id: f_00000-5-1 loss: 1.062775  [   64/  126]
train() client id: f_00000-5-2 loss: 1.077168  [   96/  126]
train() client id: f_00000-6-0 loss: 1.062137  [   32/  126]
train() client id: f_00000-6-1 loss: 1.079575  [   64/  126]
train() client id: f_00000-6-2 loss: 1.009136  [   96/  126]
train() client id: f_00000-7-0 loss: 1.044731  [   32/  126]
train() client id: f_00000-7-1 loss: 1.001063  [   64/  126]
train() client id: f_00000-7-2 loss: 0.988514  [   96/  126]
train() client id: f_00000-8-0 loss: 0.980823  [   32/  126]
train() client id: f_00000-8-1 loss: 1.009980  [   64/  126]
train() client id: f_00000-8-2 loss: 0.965385  [   96/  126]
train() client id: f_00000-9-0 loss: 1.027884  [   32/  126]
train() client id: f_00000-9-1 loss: 0.991986  [   64/  126]
train() client id: f_00000-9-2 loss: 0.963899  [   96/  126]
train() client id: f_00000-10-0 loss: 1.061995  [   32/  126]
train() client id: f_00000-10-1 loss: 0.956134  [   64/  126]
train() client id: f_00000-10-2 loss: 0.952807  [   96/  126]
train() client id: f_00000-11-0 loss: 0.999915  [   32/  126]
train() client id: f_00000-11-1 loss: 0.962219  [   64/  126]
train() client id: f_00000-11-2 loss: 0.959767  [   96/  126]
train() client id: f_00000-12-0 loss: 0.969407  [   32/  126]
train() client id: f_00000-12-1 loss: 0.987708  [   64/  126]
train() client id: f_00000-12-2 loss: 0.983389  [   96/  126]
train() client id: f_00001-0-0 loss: 0.803373  [   32/  265]
train() client id: f_00001-0-1 loss: 0.718476  [   64/  265]
train() client id: f_00001-0-2 loss: 0.775964  [   96/  265]
train() client id: f_00001-0-3 loss: 0.724057  [  128/  265]
train() client id: f_00001-0-4 loss: 0.726022  [  160/  265]
train() client id: f_00001-0-5 loss: 0.682453  [  192/  265]
train() client id: f_00001-0-6 loss: 0.756769  [  224/  265]
train() client id: f_00001-0-7 loss: 0.704049  [  256/  265]
train() client id: f_00001-1-0 loss: 0.744145  [   32/  265]
train() client id: f_00001-1-1 loss: 0.704086  [   64/  265]
train() client id: f_00001-1-2 loss: 0.728244  [   96/  265]
train() client id: f_00001-1-3 loss: 0.763372  [  128/  265]
train() client id: f_00001-1-4 loss: 0.640911  [  160/  265]
train() client id: f_00001-1-5 loss: 0.642728  [  192/  265]
train() client id: f_00001-1-6 loss: 0.737483  [  224/  265]
train() client id: f_00001-1-7 loss: 0.628079  [  256/  265]
train() client id: f_00001-2-0 loss: 0.693859  [   32/  265]
train() client id: f_00001-2-1 loss: 0.669987  [   64/  265]
train() client id: f_00001-2-2 loss: 0.633077  [   96/  265]
train() client id: f_00001-2-3 loss: 0.670284  [  128/  265]
train() client id: f_00001-2-4 loss: 0.623303  [  160/  265]
train() client id: f_00001-2-5 loss: 0.713568  [  192/  265]
train() client id: f_00001-2-6 loss: 0.655701  [  224/  265]
train() client id: f_00001-2-7 loss: 0.656912  [  256/  265]
train() client id: f_00001-3-0 loss: 0.727874  [   32/  265]
train() client id: f_00001-3-1 loss: 0.610406  [   64/  265]
train() client id: f_00001-3-2 loss: 0.676161  [   96/  265]
train() client id: f_00001-3-3 loss: 0.627084  [  128/  265]
train() client id: f_00001-3-4 loss: 0.680167  [  160/  265]
train() client id: f_00001-3-5 loss: 0.620001  [  192/  265]
train() client id: f_00001-3-6 loss: 0.664696  [  224/  265]
train() client id: f_00001-3-7 loss: 0.607906  [  256/  265]
train() client id: f_00001-4-0 loss: 0.619230  [   32/  265]
train() client id: f_00001-4-1 loss: 0.682988  [   64/  265]
train() client id: f_00001-4-2 loss: 0.654190  [   96/  265]
train() client id: f_00001-4-3 loss: 0.580104  [  128/  265]
train() client id: f_00001-4-4 loss: 0.603912  [  160/  265]
train() client id: f_00001-4-5 loss: 0.561319  [  192/  265]
train() client id: f_00001-4-6 loss: 0.728127  [  224/  265]
train() client id: f_00001-4-7 loss: 0.672096  [  256/  265]
train() client id: f_00001-5-0 loss: 0.674979  [   32/  265]
train() client id: f_00001-5-1 loss: 0.681132  [   64/  265]
train() client id: f_00001-5-2 loss: 0.560711  [   96/  265]
train() client id: f_00001-5-3 loss: 0.573821  [  128/  265]
train() client id: f_00001-5-4 loss: 0.637601  [  160/  265]
train() client id: f_00001-5-5 loss: 0.628592  [  192/  265]
train() client id: f_00001-5-6 loss: 0.677245  [  224/  265]
train() client id: f_00001-5-7 loss: 0.614390  [  256/  265]
train() client id: f_00001-6-0 loss: 0.543723  [   32/  265]
train() client id: f_00001-6-1 loss: 0.663868  [   64/  265]
train() client id: f_00001-6-2 loss: 0.613535  [   96/  265]
train() client id: f_00001-6-3 loss: 0.680448  [  128/  265]
train() client id: f_00001-6-4 loss: 0.596280  [  160/  265]
train() client id: f_00001-6-5 loss: 0.618450  [  192/  265]
train() client id: f_00001-6-6 loss: 0.669556  [  224/  265]
train() client id: f_00001-6-7 loss: 0.565663  [  256/  265]
train() client id: f_00001-7-0 loss: 0.699759  [   32/  265]
train() client id: f_00001-7-1 loss: 0.584197  [   64/  265]
train() client id: f_00001-7-2 loss: 0.623349  [   96/  265]
train() client id: f_00001-7-3 loss: 0.558080  [  128/  265]
train() client id: f_00001-7-4 loss: 0.634660  [  160/  265]
train() client id: f_00001-7-5 loss: 0.680469  [  192/  265]
train() client id: f_00001-7-6 loss: 0.568807  [  224/  265]
train() client id: f_00001-7-7 loss: 0.571094  [  256/  265]
train() client id: f_00001-8-0 loss: 0.614918  [   32/  265]
train() client id: f_00001-8-1 loss: 0.537709  [   64/  265]
train() client id: f_00001-8-2 loss: 0.746134  [   96/  265]
train() client id: f_00001-8-3 loss: 0.589160  [  128/  265]
train() client id: f_00001-8-4 loss: 0.594585  [  160/  265]
train() client id: f_00001-8-5 loss: 0.640589  [  192/  265]
train() client id: f_00001-8-6 loss: 0.624021  [  224/  265]
train() client id: f_00001-8-7 loss: 0.537927  [  256/  265]
train() client id: f_00001-9-0 loss: 0.557079  [   32/  265]
train() client id: f_00001-9-1 loss: 0.589238  [   64/  265]
train() client id: f_00001-9-2 loss: 0.646679  [   96/  265]
train() client id: f_00001-9-3 loss: 0.710712  [  128/  265]
train() client id: f_00001-9-4 loss: 0.593597  [  160/  265]
train() client id: f_00001-9-5 loss: 0.623323  [  192/  265]
train() client id: f_00001-9-6 loss: 0.652622  [  224/  265]
train() client id: f_00001-9-7 loss: 0.579774  [  256/  265]
train() client id: f_00001-10-0 loss: 0.565939  [   32/  265]
train() client id: f_00001-10-1 loss: 0.625745  [   64/  265]
train() client id: f_00001-10-2 loss: 0.612566  [   96/  265]
train() client id: f_00001-10-3 loss: 0.650246  [  128/  265]
train() client id: f_00001-10-4 loss: 0.659008  [  160/  265]
train() client id: f_00001-10-5 loss: 0.661553  [  192/  265]
train() client id: f_00001-10-6 loss: 0.576805  [  224/  265]
train() client id: f_00001-10-7 loss: 0.607318  [  256/  265]
train() client id: f_00001-11-0 loss: 0.643571  [   32/  265]
train() client id: f_00001-11-1 loss: 0.586081  [   64/  265]
train() client id: f_00001-11-2 loss: 0.609903  [   96/  265]
train() client id: f_00001-11-3 loss: 0.639702  [  128/  265]
train() client id: f_00001-11-4 loss: 0.617663  [  160/  265]
train() client id: f_00001-11-5 loss: 0.609353  [  192/  265]
train() client id: f_00001-11-6 loss: 0.634365  [  224/  265]
train() client id: f_00001-11-7 loss: 0.588575  [  256/  265]
train() client id: f_00001-12-0 loss: 0.594222  [   32/  265]
train() client id: f_00001-12-1 loss: 0.556189  [   64/  265]
train() client id: f_00001-12-2 loss: 0.539575  [   96/  265]
train() client id: f_00001-12-3 loss: 0.597393  [  128/  265]
train() client id: f_00001-12-4 loss: 0.668445  [  160/  265]
train() client id: f_00001-12-5 loss: 0.595810  [  192/  265]
train() client id: f_00001-12-6 loss: 0.799203  [  224/  265]
train() client id: f_00001-12-7 loss: 0.588017  [  256/  265]
train() client id: f_00002-0-0 loss: 1.177546  [   32/  124]
train() client id: f_00002-0-1 loss: 1.115992  [   64/  124]
train() client id: f_00002-0-2 loss: 1.088091  [   96/  124]
train() client id: f_00002-1-0 loss: 1.045308  [   32/  124]
train() client id: f_00002-1-1 loss: 1.094083  [   64/  124]
train() client id: f_00002-1-2 loss: 1.156224  [   96/  124]
train() client id: f_00002-2-0 loss: 1.088724  [   32/  124]
train() client id: f_00002-2-1 loss: 1.068598  [   64/  124]
train() client id: f_00002-2-2 loss: 1.115842  [   96/  124]
train() client id: f_00002-3-0 loss: 1.006280  [   32/  124]
train() client id: f_00002-3-1 loss: 1.047443  [   64/  124]
train() client id: f_00002-3-2 loss: 1.125915  [   96/  124]
train() client id: f_00002-4-0 loss: 1.049319  [   32/  124]
train() client id: f_00002-4-1 loss: 1.048261  [   64/  124]
train() client id: f_00002-4-2 loss: 0.982077  [   96/  124]
train() client id: f_00002-5-0 loss: 0.993192  [   32/  124]
train() client id: f_00002-5-1 loss: 1.027378  [   64/  124]
train() client id: f_00002-5-2 loss: 1.109841  [   96/  124]
train() client id: f_00002-6-0 loss: 1.018676  [   32/  124]
train() client id: f_00002-6-1 loss: 1.052222  [   64/  124]
train() client id: f_00002-6-2 loss: 1.002345  [   96/  124]
train() client id: f_00002-7-0 loss: 0.985263  [   32/  124]
train() client id: f_00002-7-1 loss: 0.965566  [   64/  124]
train() client id: f_00002-7-2 loss: 0.993844  [   96/  124]
train() client id: f_00002-8-0 loss: 0.943461  [   32/  124]
train() client id: f_00002-8-1 loss: 0.986951  [   64/  124]
train() client id: f_00002-8-2 loss: 1.031179  [   96/  124]
train() client id: f_00002-9-0 loss: 0.961725  [   32/  124]
train() client id: f_00002-9-1 loss: 1.019541  [   64/  124]
train() client id: f_00002-9-2 loss: 0.992751  [   96/  124]
train() client id: f_00002-10-0 loss: 0.936535  [   32/  124]
train() client id: f_00002-10-1 loss: 0.991462  [   64/  124]
train() client id: f_00002-10-2 loss: 0.970691  [   96/  124]
train() client id: f_00002-11-0 loss: 0.966319  [   32/  124]
train() client id: f_00002-11-1 loss: 1.088568  [   64/  124]
train() client id: f_00002-11-2 loss: 0.917949  [   96/  124]
train() client id: f_00002-12-0 loss: 1.011459  [   32/  124]
train() client id: f_00002-12-1 loss: 1.014121  [   64/  124]
train() client id: f_00002-12-2 loss: 0.976658  [   96/  124]
train() client id: f_00003-0-0 loss: 0.907499  [   32/   43]
train() client id: f_00003-1-0 loss: 0.936421  [   32/   43]
train() client id: f_00003-2-0 loss: 0.871169  [   32/   43]
train() client id: f_00003-3-0 loss: 0.817824  [   32/   43]
train() client id: f_00003-4-0 loss: 0.906840  [   32/   43]
train() client id: f_00003-5-0 loss: 0.913467  [   32/   43]
train() client id: f_00003-6-0 loss: 0.812610  [   32/   43]
train() client id: f_00003-7-0 loss: 0.791447  [   32/   43]
train() client id: f_00003-8-0 loss: 0.796496  [   32/   43]
train() client id: f_00003-9-0 loss: 0.999069  [   32/   43]
train() client id: f_00003-10-0 loss: 0.782279  [   32/   43]
train() client id: f_00003-11-0 loss: 0.951054  [   32/   43]
train() client id: f_00003-12-0 loss: 0.835523  [   32/   43]
train() client id: f_00004-0-0 loss: 1.053234  [   32/  306]
train() client id: f_00004-0-1 loss: 0.978937  [   64/  306]
train() client id: f_00004-0-2 loss: 0.907847  [   96/  306]
train() client id: f_00004-0-3 loss: 0.939523  [  128/  306]
train() client id: f_00004-0-4 loss: 0.948731  [  160/  306]
train() client id: f_00004-0-5 loss: 0.922442  [  192/  306]
train() client id: f_00004-0-6 loss: 0.999922  [  224/  306]
train() client id: f_00004-0-7 loss: 0.972338  [  256/  306]
train() client id: f_00004-0-8 loss: 0.998020  [  288/  306]
train() client id: f_00004-1-0 loss: 0.925040  [   32/  306]
train() client id: f_00004-1-1 loss: 0.949125  [   64/  306]
train() client id: f_00004-1-2 loss: 0.912794  [   96/  306]
train() client id: f_00004-1-3 loss: 0.978057  [  128/  306]
train() client id: f_00004-1-4 loss: 1.028080  [  160/  306]
train() client id: f_00004-1-5 loss: 1.006606  [  192/  306]
train() client id: f_00004-1-6 loss: 0.924236  [  224/  306]
train() client id: f_00004-1-7 loss: 1.044609  [  256/  306]
train() client id: f_00004-1-8 loss: 1.012939  [  288/  306]
train() client id: f_00004-2-0 loss: 0.957924  [   32/  306]
train() client id: f_00004-2-1 loss: 0.939642  [   64/  306]
train() client id: f_00004-2-2 loss: 1.017892  [   96/  306]
train() client id: f_00004-2-3 loss: 0.932988  [  128/  306]
train() client id: f_00004-2-4 loss: 0.977165  [  160/  306]
train() client id: f_00004-2-5 loss: 0.898435  [  192/  306]
train() client id: f_00004-2-6 loss: 0.904703  [  224/  306]
train() client id: f_00004-2-7 loss: 1.004317  [  256/  306]
train() client id: f_00004-2-8 loss: 1.028078  [  288/  306]
train() client id: f_00004-3-0 loss: 0.991027  [   32/  306]
train() client id: f_00004-3-1 loss: 0.928370  [   64/  306]
train() client id: f_00004-3-2 loss: 1.009065  [   96/  306]
train() client id: f_00004-3-3 loss: 1.050108  [  128/  306]
train() client id: f_00004-3-4 loss: 0.868332  [  160/  306]
train() client id: f_00004-3-5 loss: 0.908472  [  192/  306]
train() client id: f_00004-3-6 loss: 0.920235  [  224/  306]
train() client id: f_00004-3-7 loss: 1.069268  [  256/  306]
train() client id: f_00004-3-8 loss: 0.950464  [  288/  306]
train() client id: f_00004-4-0 loss: 0.915463  [   32/  306]
train() client id: f_00004-4-1 loss: 0.980125  [   64/  306]
train() client id: f_00004-4-2 loss: 0.931623  [   96/  306]
train() client id: f_00004-4-3 loss: 0.964703  [  128/  306]
train() client id: f_00004-4-4 loss: 0.931491  [  160/  306]
train() client id: f_00004-4-5 loss: 0.969636  [  192/  306]
train() client id: f_00004-4-6 loss: 0.897430  [  224/  306]
train() client id: f_00004-4-7 loss: 1.005697  [  256/  306]
train() client id: f_00004-4-8 loss: 0.996509  [  288/  306]
train() client id: f_00004-5-0 loss: 0.974632  [   32/  306]
train() client id: f_00004-5-1 loss: 0.872149  [   64/  306]
train() client id: f_00004-5-2 loss: 0.937839  [   96/  306]
train() client id: f_00004-5-3 loss: 0.974837  [  128/  306]
train() client id: f_00004-5-4 loss: 0.901870  [  160/  306]
train() client id: f_00004-5-5 loss: 0.971251  [  192/  306]
train() client id: f_00004-5-6 loss: 1.071081  [  224/  306]
train() client id: f_00004-5-7 loss: 0.987077  [  256/  306]
train() client id: f_00004-5-8 loss: 0.956447  [  288/  306]
train() client id: f_00004-6-0 loss: 0.913536  [   32/  306]
train() client id: f_00004-6-1 loss: 0.907380  [   64/  306]
train() client id: f_00004-6-2 loss: 1.117504  [   96/  306]
train() client id: f_00004-6-3 loss: 0.877844  [  128/  306]
train() client id: f_00004-6-4 loss: 0.977947  [  160/  306]
train() client id: f_00004-6-5 loss: 0.948581  [  192/  306]
train() client id: f_00004-6-6 loss: 0.890845  [  224/  306]
train() client id: f_00004-6-7 loss: 0.963616  [  256/  306]
train() client id: f_00004-6-8 loss: 0.993271  [  288/  306]
train() client id: f_00004-7-0 loss: 0.836762  [   32/  306]
train() client id: f_00004-7-1 loss: 1.004473  [   64/  306]
train() client id: f_00004-7-2 loss: 0.928728  [   96/  306]
train() client id: f_00004-7-3 loss: 0.963179  [  128/  306]
train() client id: f_00004-7-4 loss: 0.988677  [  160/  306]
train() client id: f_00004-7-5 loss: 0.981352  [  192/  306]
train() client id: f_00004-7-6 loss: 0.955815  [  224/  306]
train() client id: f_00004-7-7 loss: 0.948333  [  256/  306]
train() client id: f_00004-7-8 loss: 0.880480  [  288/  306]
train() client id: f_00004-8-0 loss: 1.039732  [   32/  306]
train() client id: f_00004-8-1 loss: 0.956163  [   64/  306]
train() client id: f_00004-8-2 loss: 0.965535  [   96/  306]
train() client id: f_00004-8-3 loss: 0.889760  [  128/  306]
train() client id: f_00004-8-4 loss: 0.931807  [  160/  306]
train() client id: f_00004-8-5 loss: 1.014286  [  192/  306]
train() client id: f_00004-8-6 loss: 0.846242  [  224/  306]
train() client id: f_00004-8-7 loss: 0.865244  [  256/  306]
train() client id: f_00004-8-8 loss: 1.056954  [  288/  306]
train() client id: f_00004-9-0 loss: 0.998367  [   32/  306]
train() client id: f_00004-9-1 loss: 0.928037  [   64/  306]
train() client id: f_00004-9-2 loss: 0.981997  [   96/  306]
train() client id: f_00004-9-3 loss: 0.931305  [  128/  306]
train() client id: f_00004-9-4 loss: 0.936500  [  160/  306]
train() client id: f_00004-9-5 loss: 0.887518  [  192/  306]
train() client id: f_00004-9-6 loss: 0.983053  [  224/  306]
train() client id: f_00004-9-7 loss: 1.009254  [  256/  306]
train() client id: f_00004-9-8 loss: 0.906922  [  288/  306]
train() client id: f_00004-10-0 loss: 0.879536  [   32/  306]
train() client id: f_00004-10-1 loss: 0.944396  [   64/  306]
train() client id: f_00004-10-2 loss: 0.965085  [   96/  306]
train() client id: f_00004-10-3 loss: 0.956405  [  128/  306]
train() client id: f_00004-10-4 loss: 0.972089  [  160/  306]
train() client id: f_00004-10-5 loss: 0.944626  [  192/  306]
train() client id: f_00004-10-6 loss: 1.107473  [  224/  306]
train() client id: f_00004-10-7 loss: 0.893371  [  256/  306]
train() client id: f_00004-10-8 loss: 0.970329  [  288/  306]
train() client id: f_00004-11-0 loss: 0.929990  [   32/  306]
train() client id: f_00004-11-1 loss: 0.904560  [   64/  306]
train() client id: f_00004-11-2 loss: 0.901393  [   96/  306]
train() client id: f_00004-11-3 loss: 0.828801  [  128/  306]
train() client id: f_00004-11-4 loss: 0.972061  [  160/  306]
train() client id: f_00004-11-5 loss: 1.057950  [  192/  306]
train() client id: f_00004-11-6 loss: 0.984655  [  224/  306]
train() client id: f_00004-11-7 loss: 1.030103  [  256/  306]
train() client id: f_00004-11-8 loss: 0.913689  [  288/  306]
train() client id: f_00004-12-0 loss: 0.909450  [   32/  306]
train() client id: f_00004-12-1 loss: 1.021493  [   64/  306]
train() client id: f_00004-12-2 loss: 0.974970  [   96/  306]
train() client id: f_00004-12-3 loss: 0.957747  [  128/  306]
train() client id: f_00004-12-4 loss: 0.932848  [  160/  306]
train() client id: f_00004-12-5 loss: 0.873913  [  192/  306]
train() client id: f_00004-12-6 loss: 0.896924  [  224/  306]
train() client id: f_00004-12-7 loss: 1.042306  [  256/  306]
train() client id: f_00004-12-8 loss: 0.902111  [  288/  306]
train() client id: f_00005-0-0 loss: 1.055420  [   32/  146]
train() client id: f_00005-0-1 loss: 0.994042  [   64/  146]
train() client id: f_00005-0-2 loss: 0.938931  [   96/  146]
train() client id: f_00005-0-3 loss: 0.953782  [  128/  146]
train() client id: f_00005-1-0 loss: 0.923715  [   32/  146]
train() client id: f_00005-1-1 loss: 0.952030  [   64/  146]
train() client id: f_00005-1-2 loss: 0.986323  [   96/  146]
train() client id: f_00005-1-3 loss: 0.947451  [  128/  146]
train() client id: f_00005-2-0 loss: 0.965759  [   32/  146]
train() client id: f_00005-2-1 loss: 0.964561  [   64/  146]
train() client id: f_00005-2-2 loss: 0.929101  [   96/  146]
train() client id: f_00005-2-3 loss: 0.898543  [  128/  146]
train() client id: f_00005-3-0 loss: 0.949178  [   32/  146]
train() client id: f_00005-3-1 loss: 0.935559  [   64/  146]
train() client id: f_00005-3-2 loss: 0.902772  [   96/  146]
train() client id: f_00005-3-3 loss: 0.847330  [  128/  146]
train() client id: f_00005-4-0 loss: 0.873444  [   32/  146]
train() client id: f_00005-4-1 loss: 0.862179  [   64/  146]
train() client id: f_00005-4-2 loss: 0.941324  [   96/  146]
train() client id: f_00005-4-3 loss: 0.903876  [  128/  146]
train() client id: f_00005-5-0 loss: 0.845590  [   32/  146]
train() client id: f_00005-5-1 loss: 0.844611  [   64/  146]
train() client id: f_00005-5-2 loss: 0.858615  [   96/  146]
train() client id: f_00005-5-3 loss: 0.889228  [  128/  146]
train() client id: f_00005-6-0 loss: 1.006680  [   32/  146]
train() client id: f_00005-6-1 loss: 0.859938  [   64/  146]
train() client id: f_00005-6-2 loss: 0.787720  [   96/  146]
train() client id: f_00005-6-3 loss: 0.806846  [  128/  146]
train() client id: f_00005-7-0 loss: 0.867156  [   32/  146]
train() client id: f_00005-7-1 loss: 0.835027  [   64/  146]
train() client id: f_00005-7-2 loss: 0.917590  [   96/  146]
train() client id: f_00005-7-3 loss: 0.857904  [  128/  146]
train() client id: f_00005-8-0 loss: 0.742320  [   32/  146]
train() client id: f_00005-8-1 loss: 0.813142  [   64/  146]
train() client id: f_00005-8-2 loss: 0.891691  [   96/  146]
train() client id: f_00005-8-3 loss: 0.911829  [  128/  146]
train() client id: f_00005-9-0 loss: 0.839979  [   32/  146]
train() client id: f_00005-9-1 loss: 0.739977  [   64/  146]
train() client id: f_00005-9-2 loss: 0.812268  [   96/  146]
train() client id: f_00005-9-3 loss: 0.966129  [  128/  146]
train() client id: f_00005-10-0 loss: 0.843115  [   32/  146]
train() client id: f_00005-10-1 loss: 0.761868  [   64/  146]
train() client id: f_00005-10-2 loss: 0.897163  [   96/  146]
train() client id: f_00005-10-3 loss: 0.836420  [  128/  146]
train() client id: f_00005-11-0 loss: 0.973573  [   32/  146]
train() client id: f_00005-11-1 loss: 0.826130  [   64/  146]
train() client id: f_00005-11-2 loss: 0.733928  [   96/  146]
train() client id: f_00005-11-3 loss: 0.854759  [  128/  146]
train() client id: f_00005-12-0 loss: 0.938653  [   32/  146]
train() client id: f_00005-12-1 loss: 0.805691  [   64/  146]
train() client id: f_00005-12-2 loss: 0.769077  [   96/  146]
train() client id: f_00005-12-3 loss: 0.844296  [  128/  146]
train() client id: f_00006-0-0 loss: 1.018169  [   32/   54]
train() client id: f_00006-1-0 loss: 1.019308  [   32/   54]
train() client id: f_00006-2-0 loss: 1.029646  [   32/   54]
train() client id: f_00006-3-0 loss: 1.023437  [   32/   54]
train() client id: f_00006-4-0 loss: 1.031735  [   32/   54]
train() client id: f_00006-5-0 loss: 1.037365  [   32/   54]
train() client id: f_00006-6-0 loss: 1.013417  [   32/   54]
train() client id: f_00006-7-0 loss: 1.051740  [   32/   54]
train() client id: f_00006-8-0 loss: 1.016110  [   32/   54]
train() client id: f_00006-9-0 loss: 1.044248  [   32/   54]
train() client id: f_00006-10-0 loss: 1.050551  [   32/   54]
train() client id: f_00006-11-0 loss: 1.034473  [   32/   54]
train() client id: f_00006-12-0 loss: 1.047312  [   32/   54]
train() client id: f_00007-0-0 loss: 0.943058  [   32/  179]
train() client id: f_00007-0-1 loss: 0.895237  [   64/  179]
train() client id: f_00007-0-2 loss: 0.906835  [   96/  179]
train() client id: f_00007-0-3 loss: 0.987078  [  128/  179]
train() client id: f_00007-0-4 loss: 0.880100  [  160/  179]
train() client id: f_00007-1-0 loss: 0.921434  [   32/  179]
train() client id: f_00007-1-1 loss: 0.867460  [   64/  179]
train() client id: f_00007-1-2 loss: 0.869767  [   96/  179]
train() client id: f_00007-1-3 loss: 0.837400  [  128/  179]
train() client id: f_00007-1-4 loss: 0.859865  [  160/  179]
train() client id: f_00007-2-0 loss: 0.847460  [   32/  179]
train() client id: f_00007-2-1 loss: 0.837494  [   64/  179]
train() client id: f_00007-2-2 loss: 0.850718  [   96/  179]
train() client id: f_00007-2-3 loss: 0.782173  [  128/  179]
train() client id: f_00007-2-4 loss: 0.792733  [  160/  179]
train() client id: f_00007-3-0 loss: 0.807138  [   32/  179]
train() client id: f_00007-3-1 loss: 0.765420  [   64/  179]
train() client id: f_00007-3-2 loss: 0.793270  [   96/  179]
train() client id: f_00007-3-3 loss: 0.811794  [  128/  179]
train() client id: f_00007-3-4 loss: 0.805022  [  160/  179]
train() client id: f_00007-4-0 loss: 0.712626  [   32/  179]
train() client id: f_00007-4-1 loss: 0.734340  [   64/  179]
train() client id: f_00007-4-2 loss: 0.724547  [   96/  179]
train() client id: f_00007-4-3 loss: 0.775948  [  128/  179]
train() client id: f_00007-4-4 loss: 0.782820  [  160/  179]
train() client id: f_00007-5-0 loss: 0.811374  [   32/  179]
train() client id: f_00007-5-1 loss: 0.725947  [   64/  179]
train() client id: f_00007-5-2 loss: 0.673898  [   96/  179]
train() client id: f_00007-5-3 loss: 0.728619  [  128/  179]
train() client id: f_00007-5-4 loss: 0.750179  [  160/  179]
train() client id: f_00007-6-0 loss: 0.760985  [   32/  179]
train() client id: f_00007-6-1 loss: 0.709761  [   64/  179]
train() client id: f_00007-6-2 loss: 0.656274  [   96/  179]
train() client id: f_00007-6-3 loss: 0.744281  [  128/  179]
train() client id: f_00007-6-4 loss: 0.685286  [  160/  179]
train() client id: f_00007-7-0 loss: 0.630193  [   32/  179]
train() client id: f_00007-7-1 loss: 0.750747  [   64/  179]
train() client id: f_00007-7-2 loss: 0.743580  [   96/  179]
train() client id: f_00007-7-3 loss: 0.620778  [  128/  179]
train() client id: f_00007-7-4 loss: 0.765326  [  160/  179]
train() client id: f_00007-8-0 loss: 0.662334  [   32/  179]
train() client id: f_00007-8-1 loss: 0.774278  [   64/  179]
train() client id: f_00007-8-2 loss: 0.736688  [   96/  179]
train() client id: f_00007-8-3 loss: 0.712445  [  128/  179]
train() client id: f_00007-8-4 loss: 0.615793  [  160/  179]
train() client id: f_00007-9-0 loss: 0.709319  [   32/  179]
train() client id: f_00007-9-1 loss: 0.678964  [   64/  179]
train() client id: f_00007-9-2 loss: 0.697414  [   96/  179]
train() client id: f_00007-9-3 loss: 0.664944  [  128/  179]
train() client id: f_00007-9-4 loss: 0.720510  [  160/  179]
train() client id: f_00007-10-0 loss: 0.604655  [   32/  179]
train() client id: f_00007-10-1 loss: 0.730544  [   64/  179]
train() client id: f_00007-10-2 loss: 0.665109  [   96/  179]
train() client id: f_00007-10-3 loss: 0.787279  [  128/  179]
train() client id: f_00007-10-4 loss: 0.716247  [  160/  179]
train() client id: f_00007-11-0 loss: 0.722145  [   32/  179]
train() client id: f_00007-11-1 loss: 0.684915  [   64/  179]
train() client id: f_00007-11-2 loss: 0.735350  [   96/  179]
train() client id: f_00007-11-3 loss: 0.663235  [  128/  179]
train() client id: f_00007-11-4 loss: 0.623381  [  160/  179]
train() client id: f_00007-12-0 loss: 0.620668  [   32/  179]
train() client id: f_00007-12-1 loss: 0.699823  [   64/  179]
train() client id: f_00007-12-2 loss: 0.659444  [   96/  179]
train() client id: f_00007-12-3 loss: 0.737720  [  128/  179]
train() client id: f_00007-12-4 loss: 0.700771  [  160/  179]
train() client id: f_00008-0-0 loss: 0.840178  [   32/  130]
train() client id: f_00008-0-1 loss: 0.862627  [   64/  130]
train() client id: f_00008-0-2 loss: 0.888954  [   96/  130]
train() client id: f_00008-0-3 loss: 0.938290  [  128/  130]
train() client id: f_00008-1-0 loss: 0.877293  [   32/  130]
train() client id: f_00008-1-1 loss: 0.888052  [   64/  130]
train() client id: f_00008-1-2 loss: 0.851009  [   96/  130]
train() client id: f_00008-1-3 loss: 0.887369  [  128/  130]
train() client id: f_00008-2-0 loss: 0.866677  [   32/  130]
train() client id: f_00008-2-1 loss: 0.830003  [   64/  130]
train() client id: f_00008-2-2 loss: 0.886008  [   96/  130]
train() client id: f_00008-2-3 loss: 0.884056  [  128/  130]
train() client id: f_00008-3-0 loss: 0.841710  [   32/  130]
train() client id: f_00008-3-1 loss: 0.881676  [   64/  130]
train() client id: f_00008-3-2 loss: 0.864815  [   96/  130]
train() client id: f_00008-3-3 loss: 0.847016  [  128/  130]
train() client id: f_00008-4-0 loss: 0.820410  [   32/  130]
train() client id: f_00008-4-1 loss: 0.825763  [   64/  130]
train() client id: f_00008-4-2 loss: 0.890079  [   96/  130]
train() client id: f_00008-4-3 loss: 0.880463  [  128/  130]
train() client id: f_00008-5-0 loss: 0.875741  [   32/  130]
train() client id: f_00008-5-1 loss: 0.879165  [   64/  130]
train() client id: f_00008-5-2 loss: 0.863949  [   96/  130]
train() client id: f_00008-5-3 loss: 0.782256  [  128/  130]
train() client id: f_00008-6-0 loss: 0.891538  [   32/  130]
train() client id: f_00008-6-1 loss: 0.817737  [   64/  130]
train() client id: f_00008-6-2 loss: 0.888309  [   96/  130]
train() client id: f_00008-6-3 loss: 0.782621  [  128/  130]
train() client id: f_00008-7-0 loss: 0.844228  [   32/  130]
train() client id: f_00008-7-1 loss: 0.860699  [   64/  130]
train() client id: f_00008-7-2 loss: 0.870698  [   96/  130]
train() client id: f_00008-7-3 loss: 0.788512  [  128/  130]
train() client id: f_00008-8-0 loss: 0.818361  [   32/  130]
train() client id: f_00008-8-1 loss: 0.870190  [   64/  130]
train() client id: f_00008-8-2 loss: 0.873884  [   96/  130]
train() client id: f_00008-8-3 loss: 0.791788  [  128/  130]
train() client id: f_00008-9-0 loss: 0.863933  [   32/  130]
train() client id: f_00008-9-1 loss: 0.794263  [   64/  130]
train() client id: f_00008-9-2 loss: 0.884177  [   96/  130]
train() client id: f_00008-9-3 loss: 0.799045  [  128/  130]
train() client id: f_00008-10-0 loss: 0.805637  [   32/  130]
train() client id: f_00008-10-1 loss: 0.856545  [   64/  130]
train() client id: f_00008-10-2 loss: 0.799125  [   96/  130]
train() client id: f_00008-10-3 loss: 0.843484  [  128/  130]
train() client id: f_00008-11-0 loss: 0.810413  [   32/  130]
train() client id: f_00008-11-1 loss: 0.797324  [   64/  130]
train() client id: f_00008-11-2 loss: 0.927802  [   96/  130]
train() client id: f_00008-11-3 loss: 0.778665  [  128/  130]
train() client id: f_00008-12-0 loss: 0.736603  [   32/  130]
train() client id: f_00008-12-1 loss: 0.860009  [   64/  130]
train() client id: f_00008-12-2 loss: 0.769943  [   96/  130]
train() client id: f_00008-12-3 loss: 0.928280  [  128/  130]
train() client id: f_00009-0-0 loss: 1.117545  [   32/  118]
train() client id: f_00009-0-1 loss: 1.137862  [   64/  118]
train() client id: f_00009-0-2 loss: 1.055898  [   96/  118]
train() client id: f_00009-1-0 loss: 1.092049  [   32/  118]
train() client id: f_00009-1-1 loss: 1.081164  [   64/  118]
train() client id: f_00009-1-2 loss: 1.055042  [   96/  118]
train() client id: f_00009-2-0 loss: 1.088599  [   32/  118]
train() client id: f_00009-2-1 loss: 1.079876  [   64/  118]
train() client id: f_00009-2-2 loss: 1.004022  [   96/  118]
train() client id: f_00009-3-0 loss: 1.077872  [   32/  118]
train() client id: f_00009-3-1 loss: 0.988631  [   64/  118]
train() client id: f_00009-3-2 loss: 1.009075  [   96/  118]
train() client id: f_00009-4-0 loss: 1.007654  [   32/  118]
train() client id: f_00009-4-1 loss: 0.968098  [   64/  118]
train() client id: f_00009-4-2 loss: 1.008507  [   96/  118]
train() client id: f_00009-5-0 loss: 0.964174  [   32/  118]
train() client id: f_00009-5-1 loss: 1.063310  [   64/  118]
train() client id: f_00009-5-2 loss: 0.945963  [   96/  118]
train() client id: f_00009-6-0 loss: 0.956963  [   32/  118]
train() client id: f_00009-6-1 loss: 0.959659  [   64/  118]
train() client id: f_00009-6-2 loss: 0.923634  [   96/  118]
train() client id: f_00009-7-0 loss: 0.966777  [   32/  118]
train() client id: f_00009-7-1 loss: 0.980276  [   64/  118]
train() client id: f_00009-7-2 loss: 0.937278  [   96/  118]
train() client id: f_00009-8-0 loss: 0.942547  [   32/  118]
train() client id: f_00009-8-1 loss: 0.955527  [   64/  118]
train() client id: f_00009-8-2 loss: 0.940858  [   96/  118]
train() client id: f_00009-9-0 loss: 0.922354  [   32/  118]
train() client id: f_00009-9-1 loss: 0.975624  [   64/  118]
train() client id: f_00009-9-2 loss: 0.901179  [   96/  118]
train() client id: f_00009-10-0 loss: 0.923331  [   32/  118]
train() client id: f_00009-10-1 loss: 0.852660  [   64/  118]
train() client id: f_00009-10-2 loss: 0.973960  [   96/  118]
train() client id: f_00009-11-0 loss: 0.871159  [   32/  118]
train() client id: f_00009-11-1 loss: 1.007924  [   64/  118]
train() client id: f_00009-11-2 loss: 0.919341  [   96/  118]
train() client id: f_00009-12-0 loss: 0.917216  [   32/  118]
train() client id: f_00009-12-1 loss: 0.979250  [   64/  118]
train() client id: f_00009-12-2 loss: 0.897527  [   96/  118]
At round 2 accuracy: 0.610079575596817
At round 2 training accuracy: 0.5586854460093896
At round 2 training loss: 0.9540423980456201
update_location
xs = 8.927491 131.223621 5.882650 10.934260 -47.581990 104.769243 -5.849135 -5.143845 -70.120581 20.134486 
ys = -122.390647 7.291448 20.684448 -132.290817 -9.642386 0.794442 -141.381692 16.628436 25.881276 -557.232496 
xs mean: 15.317620029478785
ys mean: -89.16579882624052
dists_uav = 158.300886 165.144797 102.286128 166.193918 111.162140 144.835166 173.271450 101.503517 124.846852 566.492235 
uav_gains = -104.996283 -105.462429 -100.245441 -105.532502 -101.148995 -104.024706 -105.996993 -100.162047 -102.409813 -125.725509 
uav_gains_db_mean: -105.57047172865705
dists_bs = 349.672016 349.140465 237.764216 359.164580 224.339386 329.573065 358.760910 232.234075 182.307669 757.787500 
bs_gains = -110.789564 -110.771064 -106.099161 -111.115277 -105.392414 -110.069707 -111.101602 -105.812986 -102.869597 -120.194387 
bs_gains_db_mean: -109.42157600036431
Round 3
-------------------------------
ene_coms = [0.00794412 0.00814083 0.00842529 0.00817122 0.00811068 0.00756249
 0.00837836 0.00829509 0.00715297 0.0285207 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.64122795 22.1453727  10.48897635  3.78129747 25.5362632  12.28525651
  4.69695205 15.03527007 10.95042656 10.54382984]
obj_prev = 126.10487269301159
eta_min = 1.4965605828765774e-09	eta_max = 0.7351096842478092
af = 26.51569137072327	bf = 2.7345526047649678	zeta = 29.1672605077956	eta = 0.9090909090909091
af = 26.51569137072327	bf = 2.7345526047649678	zeta = 57.87944708639048	eta = 0.45811929286653563
af = 26.51569137072327	bf = 2.7345526047649678	zeta = 43.24453976531725	eta = 0.6131569792306876
af = 26.51569137072327	bf = 2.7345526047649678	zeta = 40.59601878258399	eta = 0.6531598951298818
af = 26.51569137072327	bf = 2.7345526047649678	zeta = 40.44650198746708	eta = 0.6555744024276693
af = 26.51569137072327	bf = 2.7345526047649678	zeta = 40.4459785574819	eta = 0.6555828865171137
af = 26.51569137072327	bf = 2.7345526047649678	zeta = 40.44597855103506	eta = 0.6555828866216096
eta = 0.6555828866216096
ene_coms = [0.00794412 0.00814083 0.00842529 0.00817122 0.00811068 0.00756249
 0.00837836 0.00829509 0.00715297 0.0285207 ]
ene_comp = [0.03484145 0.07327766 0.03428841 0.01189034 0.08461495 0.04037184
 0.01493205 0.04949698 0.03594753 0.0326293 ]
ene_total = [3.37335367 6.41929811 3.36768697 1.58171821 7.31079069 3.77929888
 1.83786822 4.55651534 3.39818361 4.82126484]
ti_comp = [0.29912249 0.29715541 0.29431083 0.29685151 0.29745688 0.30293875
 0.29478014 0.29561275 0.30703399 0.09335666]
ti_coms = [0.07944121 0.08140828 0.08425287 0.08171218 0.08110682 0.07562494
 0.08378355 0.08295094 0.07152971 0.28520704]
t_total = [29.84719276 29.84719276 29.84719276 29.84719276 29.84719276 29.84719276
 29.84719276 29.84719276 29.84719276 29.84719276]
ene_coms = [0.00794412 0.00814083 0.00842529 0.00817122 0.00811068 0.00756249
 0.00837836 0.00829509 0.00715297 0.0285207 ]
ene_comp = [2.95440900e-05 2.78501438e-04 2.90877203e-05 1.19229560e-06
 4.27930616e-04 4.48132061e-05 2.39464944e-06 8.67301067e-05
 3.07974149e-05 2.49122263e-04]
ene_total = [0.62866966 0.66380732 0.66657036 0.6443394  0.67321195 0.59978486
 0.66076558 0.6608503  0.56639164 2.26830663]
optimize_network iter = 0 obj = 8.032697698797215
eta = 0.6555828866216096
freqs = [5.82394390e+07 1.23298539e+08 5.82520410e+07 2.00274146e+07
 1.42230623e+08 6.66336689e+07 2.53274367e+07 8.37192968e+07
 5.85399844e+07 1.74756128e+08]
eta_min = 0.6555828866216133	eta_max = 0.6555828866216091
af = 0.07589680654547794	bf = 2.7345526047649678	zeta = 0.08348648720002574	eta = 0.909090909090909
af = 0.07589680654547794	bf = 2.7345526047649678	zeta = 30.15964977347726	eta = 0.002516501587900482
af = 0.07589680654547794	bf = 2.7345526047649678	zeta = 3.1968311400422578	eta = 0.023741262275265088
af = 0.07589680654547794	bf = 2.7345526047649678	zeta = 3.0918521735651687	eta = 0.02454735940947735
af = 0.07589680654547794	bf = 2.7345526047649678	zeta = 3.0918092799245898	eta = 0.024547699962699215
eta = 0.024547699962699215
ene_coms = [0.00794412 0.00814083 0.00842529 0.00817122 0.00811068 0.00756249
 0.00837836 0.00829509 0.00715297 0.0285207 ]
ene_comp = [2.59393823e-04 2.44521163e-03 2.55386947e-04 1.04682227e-05
 3.75718319e-03 3.93454962e-04 2.10247556e-05 7.61480685e-04
 2.70397876e-04 2.18726574e-03]
ene_total = [0.22837216 0.29469769 0.24165547 0.2277645  0.33038156 0.22148036
 0.23382473 0.25211995 0.2066542  0.85485866]
ti_comp = [0.29912249 0.29715541 0.29431083 0.29685151 0.29745688 0.30293875
 0.29478014 0.29561275 0.30703399 0.09335666]
ti_coms = [0.07944121 0.08140828 0.08425287 0.08171218 0.08110682 0.07562494
 0.08378355 0.08295094 0.07152971 0.28520704]
t_total = [29.84719276 29.84719276 29.84719276 29.84719276 29.84719276 29.84719276
 29.84719276 29.84719276 29.84719276 29.84719276]
ene_coms = [0.00794412 0.00814083 0.00842529 0.00817122 0.00811068 0.00756249
 0.00837836 0.00829509 0.00715297 0.0285207 ]
ene_comp = [2.95440900e-05 2.78501438e-04 2.90877203e-05 1.19229560e-06
 4.27930616e-04 4.48132061e-05 2.39464944e-06 8.67301067e-05
 3.07974149e-05 2.49122263e-04]
ene_total = [0.62866966 0.66380732 0.66657036 0.6443394  0.67321195 0.59978486
 0.66076558 0.6608503  0.56639164 2.26830663]
optimize_network iter = 1 obj = 8.032697698797302
eta = 0.6555828866216133
freqs = [5.82394390e+07 1.23298539e+08 5.82520410e+07 2.00274146e+07
 1.42230623e+08 6.66336689e+07 2.53274367e+07 8.37192968e+07
 5.85399844e+07 1.74756128e+08]
Done!
ene_coms = [0.00794412 0.00814083 0.00842529 0.00817122 0.00811068 0.00756249
 0.00837836 0.00829509 0.00715297 0.0285207 ]
ene_comp = [2.77791062e-05 2.61863574e-04 2.73500003e-05 1.12106706e-06
 4.02365753e-04 4.21360350e-05 2.25159147e-06 8.15487917e-05
 2.89575566e-05 2.34239531e-04]
ene_total = [0.0079719  0.00840269 0.00845264 0.00817234 0.00851305 0.00760463
 0.00838061 0.00837664 0.00718193 0.02875494]
At round 3 energy consumption: 0.1018113658172017
At round 3 eta: 0.6555828866216133
At round 3 a_n: 27.154965315346423
At round 3 local rounds: 13.8259729316799
At round 3 global rounds: 78.84325215139118
gradient difference: 0.38231486082077026
train() client id: f_00000-0-0 loss: 1.588782  [   32/  126]
train() client id: f_00000-0-1 loss: 1.699636  [   64/  126]
train() client id: f_00000-0-2 loss: 1.460621  [   96/  126]
train() client id: f_00000-1-0 loss: 1.435744  [   32/  126]
train() client id: f_00000-1-1 loss: 1.311783  [   64/  126]
train() client id: f_00000-1-2 loss: 1.520680  [   96/  126]
train() client id: f_00000-2-0 loss: 1.315741  [   32/  126]
train() client id: f_00000-2-1 loss: 1.190784  [   64/  126]
train() client id: f_00000-2-2 loss: 1.378536  [   96/  126]
train() client id: f_00000-3-0 loss: 1.212681  [   32/  126]
train() client id: f_00000-3-1 loss: 1.239164  [   64/  126]
train() client id: f_00000-3-2 loss: 1.245675  [   96/  126]
train() client id: f_00000-4-0 loss: 1.179723  [   32/  126]
train() client id: f_00000-4-1 loss: 1.142447  [   64/  126]
train() client id: f_00000-4-2 loss: 1.066383  [   96/  126]
train() client id: f_00000-5-0 loss: 1.094981  [   32/  126]
train() client id: f_00000-5-1 loss: 1.071757  [   64/  126]
train() client id: f_00000-5-2 loss: 1.034898  [   96/  126]
train() client id: f_00000-6-0 loss: 1.051917  [   32/  126]
train() client id: f_00000-6-1 loss: 1.035338  [   64/  126]
train() client id: f_00000-6-2 loss: 0.993465  [   96/  126]
train() client id: f_00000-7-0 loss: 0.980463  [   32/  126]
train() client id: f_00000-7-1 loss: 1.012834  [   64/  126]
train() client id: f_00000-7-2 loss: 0.969125  [   96/  126]
train() client id: f_00000-8-0 loss: 0.961222  [   32/  126]
train() client id: f_00000-8-1 loss: 1.013614  [   64/  126]
train() client id: f_00000-8-2 loss: 0.952618  [   96/  126]
train() client id: f_00000-9-0 loss: 0.980536  [   32/  126]
train() client id: f_00000-9-1 loss: 0.997174  [   64/  126]
train() client id: f_00000-9-2 loss: 0.931265  [   96/  126]
train() client id: f_00000-10-0 loss: 0.907932  [   32/  126]
train() client id: f_00000-10-1 loss: 0.947196  [   64/  126]
train() client id: f_00000-10-2 loss: 1.038422  [   96/  126]
train() client id: f_00000-11-0 loss: 0.962703  [   32/  126]
train() client id: f_00000-11-1 loss: 0.972604  [   64/  126]
train() client id: f_00000-11-2 loss: 0.897550  [   96/  126]
train() client id: f_00000-12-0 loss: 0.997663  [   32/  126]
train() client id: f_00000-12-1 loss: 0.958156  [   64/  126]
train() client id: f_00000-12-2 loss: 0.951501  [   96/  126]
train() client id: f_00001-0-0 loss: 0.680906  [   32/  265]
train() client id: f_00001-0-1 loss: 0.586540  [   64/  265]
train() client id: f_00001-0-2 loss: 0.692739  [   96/  265]
train() client id: f_00001-0-3 loss: 0.629444  [  128/  265]
train() client id: f_00001-0-4 loss: 0.699048  [  160/  265]
train() client id: f_00001-0-5 loss: 0.631501  [  192/  265]
train() client id: f_00001-0-6 loss: 0.605187  [  224/  265]
train() client id: f_00001-0-7 loss: 0.672581  [  256/  265]
train() client id: f_00001-1-0 loss: 0.595779  [   32/  265]
train() client id: f_00001-1-1 loss: 0.687039  [   64/  265]
train() client id: f_00001-1-2 loss: 0.615849  [   96/  265]
train() client id: f_00001-1-3 loss: 0.693491  [  128/  265]
train() client id: f_00001-1-4 loss: 0.604988  [  160/  265]
train() client id: f_00001-1-5 loss: 0.561676  [  192/  265]
train() client id: f_00001-1-6 loss: 0.553252  [  224/  265]
train() client id: f_00001-1-7 loss: 0.620447  [  256/  265]
train() client id: f_00001-2-0 loss: 0.583789  [   32/  265]
train() client id: f_00001-2-1 loss: 0.547563  [   64/  265]
train() client id: f_00001-2-2 loss: 0.616386  [   96/  265]
train() client id: f_00001-2-3 loss: 0.574260  [  128/  265]
train() client id: f_00001-2-4 loss: 0.567057  [  160/  265]
train() client id: f_00001-2-5 loss: 0.605200  [  192/  265]
train() client id: f_00001-2-6 loss: 0.572914  [  224/  265]
train() client id: f_00001-2-7 loss: 0.672740  [  256/  265]
train() client id: f_00001-3-0 loss: 0.546689  [   32/  265]
train() client id: f_00001-3-1 loss: 0.566906  [   64/  265]
train() client id: f_00001-3-2 loss: 0.547644  [   96/  265]
train() client id: f_00001-3-3 loss: 0.640022  [  128/  265]
train() client id: f_00001-3-4 loss: 0.549204  [  160/  265]
train() client id: f_00001-3-5 loss: 0.627066  [  192/  265]
train() client id: f_00001-3-6 loss: 0.550750  [  224/  265]
train() client id: f_00001-3-7 loss: 0.565362  [  256/  265]
train() client id: f_00001-4-0 loss: 0.605406  [   32/  265]
train() client id: f_00001-4-1 loss: 0.530562  [   64/  265]
train() client id: f_00001-4-2 loss: 0.646189  [   96/  265]
train() client id: f_00001-4-3 loss: 0.601696  [  128/  265]
train() client id: f_00001-4-4 loss: 0.495597  [  160/  265]
train() client id: f_00001-4-5 loss: 0.495500  [  192/  265]
train() client id: f_00001-4-6 loss: 0.537435  [  224/  265]
train() client id: f_00001-4-7 loss: 0.545584  [  256/  265]
train() client id: f_00001-5-0 loss: 0.576639  [   32/  265]
train() client id: f_00001-5-1 loss: 0.517073  [   64/  265]
train() client id: f_00001-5-2 loss: 0.611460  [   96/  265]
train() client id: f_00001-5-3 loss: 0.630280  [  128/  265]
train() client id: f_00001-5-4 loss: 0.523355  [  160/  265]
train() client id: f_00001-5-5 loss: 0.491426  [  192/  265]
train() client id: f_00001-5-6 loss: 0.574631  [  224/  265]
train() client id: f_00001-5-7 loss: 0.503699  [  256/  265]
train() client id: f_00001-6-0 loss: 0.492606  [   32/  265]
train() client id: f_00001-6-1 loss: 0.558668  [   64/  265]
train() client id: f_00001-6-2 loss: 0.514613  [   96/  265]
train() client id: f_00001-6-3 loss: 0.558042  [  128/  265]
train() client id: f_00001-6-4 loss: 0.567207  [  160/  265]
train() client id: f_00001-6-5 loss: 0.528415  [  192/  265]
train() client id: f_00001-6-6 loss: 0.598836  [  224/  265]
train() client id: f_00001-6-7 loss: 0.475391  [  256/  265]
train() client id: f_00001-7-0 loss: 0.517206  [   32/  265]
train() client id: f_00001-7-1 loss: 0.503894  [   64/  265]
train() client id: f_00001-7-2 loss: 0.471042  [   96/  265]
train() client id: f_00001-7-3 loss: 0.579042  [  128/  265]
train() client id: f_00001-7-4 loss: 0.584994  [  160/  265]
train() client id: f_00001-7-5 loss: 0.612674  [  192/  265]
train() client id: f_00001-7-6 loss: 0.495794  [  224/  265]
train() client id: f_00001-7-7 loss: 0.587133  [  256/  265]
train() client id: f_00001-8-0 loss: 0.579733  [   32/  265]
train() client id: f_00001-8-1 loss: 0.523007  [   64/  265]
train() client id: f_00001-8-2 loss: 0.469173  [   96/  265]
train() client id: f_00001-8-3 loss: 0.516189  [  128/  265]
train() client id: f_00001-8-4 loss: 0.613571  [  160/  265]
train() client id: f_00001-8-5 loss: 0.502596  [  192/  265]
train() client id: f_00001-8-6 loss: 0.483828  [  224/  265]
train() client id: f_00001-8-7 loss: 0.637556  [  256/  265]
train() client id: f_00001-9-0 loss: 0.439389  [   32/  265]
train() client id: f_00001-9-1 loss: 0.523029  [   64/  265]
train() client id: f_00001-9-2 loss: 0.547161  [   96/  265]
train() client id: f_00001-9-3 loss: 0.468972  [  128/  265]
train() client id: f_00001-9-4 loss: 0.634491  [  160/  265]
train() client id: f_00001-9-5 loss: 0.582281  [  192/  265]
train() client id: f_00001-9-6 loss: 0.594247  [  224/  265]
train() client id: f_00001-9-7 loss: 0.522546  [  256/  265]
train() client id: f_00001-10-0 loss: 0.434053  [   32/  265]
train() client id: f_00001-10-1 loss: 0.498108  [   64/  265]
train() client id: f_00001-10-2 loss: 0.553822  [   96/  265]
train() client id: f_00001-10-3 loss: 0.452700  [  128/  265]
train() client id: f_00001-10-4 loss: 0.496004  [  160/  265]
train() client id: f_00001-10-5 loss: 0.665843  [  192/  265]
train() client id: f_00001-10-6 loss: 0.604212  [  224/  265]
train() client id: f_00001-10-7 loss: 0.550536  [  256/  265]
train() client id: f_00001-11-0 loss: 0.467327  [   32/  265]
train() client id: f_00001-11-1 loss: 0.591226  [   64/  265]
train() client id: f_00001-11-2 loss: 0.521449  [   96/  265]
train() client id: f_00001-11-3 loss: 0.569718  [  128/  265]
train() client id: f_00001-11-4 loss: 0.478829  [  160/  265]
train() client id: f_00001-11-5 loss: 0.488338  [  192/  265]
train() client id: f_00001-11-6 loss: 0.460092  [  224/  265]
train() client id: f_00001-11-7 loss: 0.677961  [  256/  265]
train() client id: f_00001-12-0 loss: 0.586320  [   32/  265]
train() client id: f_00001-12-1 loss: 0.521414  [   64/  265]
train() client id: f_00001-12-2 loss: 0.536852  [   96/  265]
train() client id: f_00001-12-3 loss: 0.570583  [  128/  265]
train() client id: f_00001-12-4 loss: 0.466599  [  160/  265]
train() client id: f_00001-12-5 loss: 0.481969  [  192/  265]
train() client id: f_00001-12-6 loss: 0.571685  [  224/  265]
train() client id: f_00001-12-7 loss: 0.562987  [  256/  265]
train() client id: f_00002-0-0 loss: 1.134712  [   32/  124]
train() client id: f_00002-0-1 loss: 1.069225  [   64/  124]
train() client id: f_00002-0-2 loss: 1.141619  [   96/  124]
train() client id: f_00002-1-0 loss: 1.113656  [   32/  124]
train() client id: f_00002-1-1 loss: 1.092443  [   64/  124]
train() client id: f_00002-1-2 loss: 1.022656  [   96/  124]
train() client id: f_00002-2-0 loss: 1.079214  [   32/  124]
train() client id: f_00002-2-1 loss: 1.007627  [   64/  124]
train() client id: f_00002-2-2 loss: 1.057585  [   96/  124]
train() client id: f_00002-3-0 loss: 1.001809  [   32/  124]
train() client id: f_00002-3-1 loss: 1.013320  [   64/  124]
train() client id: f_00002-3-2 loss: 1.051655  [   96/  124]
train() client id: f_00002-4-0 loss: 1.007553  [   32/  124]
train() client id: f_00002-4-1 loss: 0.968840  [   64/  124]
train() client id: f_00002-4-2 loss: 1.004798  [   96/  124]
train() client id: f_00002-5-0 loss: 1.068195  [   32/  124]
train() client id: f_00002-5-1 loss: 0.995967  [   64/  124]
train() client id: f_00002-5-2 loss: 0.952502  [   96/  124]
train() client id: f_00002-6-0 loss: 1.023778  [   32/  124]
train() client id: f_00002-6-1 loss: 0.981864  [   64/  124]
train() client id: f_00002-6-2 loss: 0.985827  [   96/  124]
train() client id: f_00002-7-0 loss: 0.964624  [   32/  124]
train() client id: f_00002-7-1 loss: 0.971756  [   64/  124]
train() client id: f_00002-7-2 loss: 1.071809  [   96/  124]
train() client id: f_00002-8-0 loss: 0.957046  [   32/  124]
train() client id: f_00002-8-1 loss: 1.008537  [   64/  124]
train() client id: f_00002-8-2 loss: 0.939168  [   96/  124]
train() client id: f_00002-9-0 loss: 0.999857  [   32/  124]
train() client id: f_00002-9-1 loss: 0.950014  [   64/  124]
train() client id: f_00002-9-2 loss: 0.924791  [   96/  124]
train() client id: f_00002-10-0 loss: 0.999998  [   32/  124]
train() client id: f_00002-10-1 loss: 0.961430  [   64/  124]
train() client id: f_00002-10-2 loss: 0.978996  [   96/  124]
train() client id: f_00002-11-0 loss: 0.996188  [   32/  124]
train() client id: f_00002-11-1 loss: 0.985708  [   64/  124]
train() client id: f_00002-11-2 loss: 0.904142  [   96/  124]
train() client id: f_00002-12-0 loss: 1.030078  [   32/  124]
train() client id: f_00002-12-1 loss: 0.919502  [   64/  124]
train() client id: f_00002-12-2 loss: 0.946237  [   96/  124]
train() client id: f_00003-0-0 loss: 1.056875  [   32/   43]
train() client id: f_00003-1-0 loss: 1.060198  [   32/   43]
train() client id: f_00003-2-0 loss: 0.986834  [   32/   43]
train() client id: f_00003-3-0 loss: 1.057070  [   32/   43]
train() client id: f_00003-4-0 loss: 1.123450  [   32/   43]
train() client id: f_00003-5-0 loss: 1.094675  [   32/   43]
train() client id: f_00003-6-0 loss: 1.084128  [   32/   43]
train() client id: f_00003-7-0 loss: 1.113275  [   32/   43]
train() client id: f_00003-8-0 loss: 0.975952  [   32/   43]
train() client id: f_00003-9-0 loss: 1.061895  [   32/   43]
train() client id: f_00003-10-0 loss: 0.996695  [   32/   43]
train() client id: f_00003-11-0 loss: 1.109393  [   32/   43]
train() client id: f_00003-12-0 loss: 1.145070  [   32/   43]
train() client id: f_00004-0-0 loss: 1.066446  [   32/  306]
train() client id: f_00004-0-1 loss: 1.055708  [   64/  306]
train() client id: f_00004-0-2 loss: 1.000520  [   96/  306]
train() client id: f_00004-0-3 loss: 0.921297  [  128/  306]
train() client id: f_00004-0-4 loss: 0.994892  [  160/  306]
train() client id: f_00004-0-5 loss: 1.028726  [  192/  306]
train() client id: f_00004-0-6 loss: 1.079672  [  224/  306]
train() client id: f_00004-0-7 loss: 0.995427  [  256/  306]
train() client id: f_00004-0-8 loss: 0.910263  [  288/  306]
train() client id: f_00004-1-0 loss: 0.952229  [   32/  306]
train() client id: f_00004-1-1 loss: 0.984176  [   64/  306]
train() client id: f_00004-1-2 loss: 1.015585  [   96/  306]
train() client id: f_00004-1-3 loss: 1.070850  [  128/  306]
train() client id: f_00004-1-4 loss: 0.887843  [  160/  306]
train() client id: f_00004-1-5 loss: 1.060090  [  192/  306]
train() client id: f_00004-1-6 loss: 0.950279  [  224/  306]
train() client id: f_00004-1-7 loss: 1.024623  [  256/  306]
train() client id: f_00004-1-8 loss: 1.033302  [  288/  306]
train() client id: f_00004-2-0 loss: 1.093342  [   32/  306]
train() client id: f_00004-2-1 loss: 1.121292  [   64/  306]
train() client id: f_00004-2-2 loss: 0.989853  [   96/  306]
train() client id: f_00004-2-3 loss: 0.896964  [  128/  306]
train() client id: f_00004-2-4 loss: 1.007027  [  160/  306]
train() client id: f_00004-2-5 loss: 0.971976  [  192/  306]
train() client id: f_00004-2-6 loss: 0.901829  [  224/  306]
train() client id: f_00004-2-7 loss: 0.922694  [  256/  306]
train() client id: f_00004-2-8 loss: 1.059343  [  288/  306]
train() client id: f_00004-3-0 loss: 0.926870  [   32/  306]
train() client id: f_00004-3-1 loss: 0.903581  [   64/  306]
train() client id: f_00004-3-2 loss: 0.960269  [   96/  306]
train() client id: f_00004-3-3 loss: 1.068990  [  128/  306]
train() client id: f_00004-3-4 loss: 0.910928  [  160/  306]
train() client id: f_00004-3-5 loss: 1.053281  [  192/  306]
train() client id: f_00004-3-6 loss: 0.969402  [  224/  306]
train() client id: f_00004-3-7 loss: 1.091451  [  256/  306]
train() client id: f_00004-3-8 loss: 1.032324  [  288/  306]
train() client id: f_00004-4-0 loss: 1.025751  [   32/  306]
train() client id: f_00004-4-1 loss: 0.913214  [   64/  306]
train() client id: f_00004-4-2 loss: 0.998626  [   96/  306]
train() client id: f_00004-4-3 loss: 0.955836  [  128/  306]
train() client id: f_00004-4-4 loss: 0.948992  [  160/  306]
train() client id: f_00004-4-5 loss: 1.021912  [  192/  306]
train() client id: f_00004-4-6 loss: 0.976154  [  224/  306]
train() client id: f_00004-4-7 loss: 0.987359  [  256/  306]
train() client id: f_00004-4-8 loss: 1.045184  [  288/  306]
train() client id: f_00004-5-0 loss: 1.109801  [   32/  306]
train() client id: f_00004-5-1 loss: 0.946338  [   64/  306]
train() client id: f_00004-5-2 loss: 1.060529  [   96/  306]
train() client id: f_00004-5-3 loss: 0.943470  [  128/  306]
train() client id: f_00004-5-4 loss: 0.879780  [  160/  306]
train() client id: f_00004-5-5 loss: 1.009987  [  192/  306]
train() client id: f_00004-5-6 loss: 0.951109  [  224/  306]
train() client id: f_00004-5-7 loss: 0.983219  [  256/  306]
train() client id: f_00004-5-8 loss: 0.929322  [  288/  306]
train() client id: f_00004-6-0 loss: 0.894846  [   32/  306]
train() client id: f_00004-6-1 loss: 1.028334  [   64/  306]
train() client id: f_00004-6-2 loss: 1.094588  [   96/  306]
train() client id: f_00004-6-3 loss: 0.939959  [  128/  306]
train() client id: f_00004-6-4 loss: 0.821368  [  160/  306]
train() client id: f_00004-6-5 loss: 1.070111  [  192/  306]
train() client id: f_00004-6-6 loss: 0.936264  [  224/  306]
train() client id: f_00004-6-7 loss: 1.020481  [  256/  306]
train() client id: f_00004-6-8 loss: 1.070802  [  288/  306]
train() client id: f_00004-7-0 loss: 1.031047  [   32/  306]
train() client id: f_00004-7-1 loss: 1.106689  [   64/  306]
train() client id: f_00004-7-2 loss: 0.885210  [   96/  306]
train() client id: f_00004-7-3 loss: 0.925573  [  128/  306]
train() client id: f_00004-7-4 loss: 0.990896  [  160/  306]
train() client id: f_00004-7-5 loss: 1.032965  [  192/  306]
train() client id: f_00004-7-6 loss: 0.959917  [  224/  306]
train() client id: f_00004-7-7 loss: 0.974297  [  256/  306]
train() client id: f_00004-7-8 loss: 0.957302  [  288/  306]
train() client id: f_00004-8-0 loss: 0.994851  [   32/  306]
train() client id: f_00004-8-1 loss: 0.972886  [   64/  306]
train() client id: f_00004-8-2 loss: 0.937513  [   96/  306]
train() client id: f_00004-8-3 loss: 0.963411  [  128/  306]
train() client id: f_00004-8-4 loss: 0.973373  [  160/  306]
train() client id: f_00004-8-5 loss: 1.011633  [  192/  306]
train() client id: f_00004-8-6 loss: 0.982323  [  224/  306]
train() client id: f_00004-8-7 loss: 1.045483  [  256/  306]
train() client id: f_00004-8-8 loss: 0.999931  [  288/  306]
train() client id: f_00004-9-0 loss: 0.946464  [   32/  306]
train() client id: f_00004-9-1 loss: 0.963947  [   64/  306]
train() client id: f_00004-9-2 loss: 0.896218  [   96/  306]
train() client id: f_00004-9-3 loss: 1.029680  [  128/  306]
train() client id: f_00004-9-4 loss: 0.969329  [  160/  306]
train() client id: f_00004-9-5 loss: 0.999372  [  192/  306]
train() client id: f_00004-9-6 loss: 0.960498  [  224/  306]
train() client id: f_00004-9-7 loss: 1.049134  [  256/  306]
train() client id: f_00004-9-8 loss: 1.042668  [  288/  306]
train() client id: f_00004-10-0 loss: 0.943347  [   32/  306]
train() client id: f_00004-10-1 loss: 0.999813  [   64/  306]
train() client id: f_00004-10-2 loss: 0.860590  [   96/  306]
train() client id: f_00004-10-3 loss: 0.902795  [  128/  306]
train() client id: f_00004-10-4 loss: 0.989190  [  160/  306]
train() client id: f_00004-10-5 loss: 1.065387  [  192/  306]
train() client id: f_00004-10-6 loss: 0.913883  [  224/  306]
train() client id: f_00004-10-7 loss: 1.036929  [  256/  306]
train() client id: f_00004-10-8 loss: 1.024819  [  288/  306]
train() client id: f_00004-11-0 loss: 0.956431  [   32/  306]
train() client id: f_00004-11-1 loss: 0.947970  [   64/  306]
train() client id: f_00004-11-2 loss: 0.891516  [   96/  306]
train() client id: f_00004-11-3 loss: 0.989843  [  128/  306]
train() client id: f_00004-11-4 loss: 1.000867  [  160/  306]
train() client id: f_00004-11-5 loss: 0.987414  [  192/  306]
train() client id: f_00004-11-6 loss: 0.954009  [  224/  306]
train() client id: f_00004-11-7 loss: 1.015531  [  256/  306]
train() client id: f_00004-11-8 loss: 0.964860  [  288/  306]
train() client id: f_00004-12-0 loss: 1.048180  [   32/  306]
train() client id: f_00004-12-1 loss: 0.931963  [   64/  306]
train() client id: f_00004-12-2 loss: 1.016714  [   96/  306]
train() client id: f_00004-12-3 loss: 0.953805  [  128/  306]
train() client id: f_00004-12-4 loss: 1.004064  [  160/  306]
train() client id: f_00004-12-5 loss: 1.010636  [  192/  306]
train() client id: f_00004-12-6 loss: 0.986580  [  224/  306]
train() client id: f_00004-12-7 loss: 0.916954  [  256/  306]
train() client id: f_00004-12-8 loss: 0.890571  [  288/  306]
train() client id: f_00005-0-0 loss: 0.959684  [   32/  146]
train() client id: f_00005-0-1 loss: 0.934885  [   64/  146]
train() client id: f_00005-0-2 loss: 1.016064  [   96/  146]
train() client id: f_00005-0-3 loss: 0.943928  [  128/  146]
train() client id: f_00005-1-0 loss: 0.939956  [   32/  146]
train() client id: f_00005-1-1 loss: 0.997738  [   64/  146]
train() client id: f_00005-1-2 loss: 0.895325  [   96/  146]
train() client id: f_00005-1-3 loss: 0.934589  [  128/  146]
train() client id: f_00005-2-0 loss: 0.883149  [   32/  146]
train() client id: f_00005-2-1 loss: 0.948880  [   64/  146]
train() client id: f_00005-2-2 loss: 1.024070  [   96/  146]
train() client id: f_00005-2-3 loss: 0.875695  [  128/  146]
train() client id: f_00005-3-0 loss: 0.891728  [   32/  146]
train() client id: f_00005-3-1 loss: 0.939306  [   64/  146]
train() client id: f_00005-3-2 loss: 0.994745  [   96/  146]
train() client id: f_00005-3-3 loss: 0.823787  [  128/  146]
train() client id: f_00005-4-0 loss: 0.950340  [   32/  146]
train() client id: f_00005-4-1 loss: 0.896626  [   64/  146]
train() client id: f_00005-4-2 loss: 0.834259  [   96/  146]
train() client id: f_00005-4-3 loss: 0.929511  [  128/  146]
train() client id: f_00005-5-0 loss: 0.849986  [   32/  146]
train() client id: f_00005-5-1 loss: 0.874224  [   64/  146]
train() client id: f_00005-5-2 loss: 0.897847  [   96/  146]
train() client id: f_00005-5-3 loss: 0.880585  [  128/  146]
train() client id: f_00005-6-0 loss: 0.766401  [   32/  146]
train() client id: f_00005-6-1 loss: 0.908013  [   64/  146]
train() client id: f_00005-6-2 loss: 0.865920  [   96/  146]
train() client id: f_00005-6-3 loss: 0.927868  [  128/  146]
train() client id: f_00005-7-0 loss: 0.909242  [   32/  146]
train() client id: f_00005-7-1 loss: 0.811494  [   64/  146]
train() client id: f_00005-7-2 loss: 0.986259  [   96/  146]
train() client id: f_00005-7-3 loss: 0.817409  [  128/  146]
train() client id: f_00005-8-0 loss: 0.827222  [   32/  146]
train() client id: f_00005-8-1 loss: 0.949033  [   64/  146]
train() client id: f_00005-8-2 loss: 0.902088  [   96/  146]
train() client id: f_00005-8-3 loss: 0.796211  [  128/  146]
train() client id: f_00005-9-0 loss: 0.929242  [   32/  146]
train() client id: f_00005-9-1 loss: 0.757947  [   64/  146]
train() client id: f_00005-9-2 loss: 0.885967  [   96/  146]
train() client id: f_00005-9-3 loss: 0.929479  [  128/  146]
train() client id: f_00005-10-0 loss: 0.963845  [   32/  146]
train() client id: f_00005-10-1 loss: 0.776392  [   64/  146]
train() client id: f_00005-10-2 loss: 0.827092  [   96/  146]
train() client id: f_00005-10-3 loss: 0.862381  [  128/  146]
train() client id: f_00005-11-0 loss: 0.906693  [   32/  146]
train() client id: f_00005-11-1 loss: 0.783158  [   64/  146]
train() client id: f_00005-11-2 loss: 0.893169  [   96/  146]
train() client id: f_00005-11-3 loss: 0.787430  [  128/  146]
train() client id: f_00005-12-0 loss: 0.969228  [   32/  146]
train() client id: f_00005-12-1 loss: 0.913110  [   64/  146]
train() client id: f_00005-12-2 loss: 0.768570  [   96/  146]
train() client id: f_00005-12-3 loss: 0.794617  [  128/  146]
train() client id: f_00006-0-0 loss: 0.950165  [   32/   54]
train() client id: f_00006-1-0 loss: 0.939953  [   32/   54]
train() client id: f_00006-2-0 loss: 0.962520  [   32/   54]
train() client id: f_00006-3-0 loss: 0.935517  [   32/   54]
train() client id: f_00006-4-0 loss: 0.932720  [   32/   54]
train() client id: f_00006-5-0 loss: 0.925310  [   32/   54]
train() client id: f_00006-6-0 loss: 0.903680  [   32/   54]
train() client id: f_00006-7-0 loss: 0.885345  [   32/   54]
train() client id: f_00006-8-0 loss: 0.963562  [   32/   54]
train() client id: f_00006-9-0 loss: 0.938267  [   32/   54]
train() client id: f_00006-10-0 loss: 0.920964  [   32/   54]
train() client id: f_00006-11-0 loss: 0.930168  [   32/   54]
train() client id: f_00006-12-0 loss: 0.949990  [   32/   54]
train() client id: f_00007-0-0 loss: 0.861104  [   32/  179]
train() client id: f_00007-0-1 loss: 0.899933  [   64/  179]
train() client id: f_00007-0-2 loss: 0.880304  [   96/  179]
train() client id: f_00007-0-3 loss: 0.864845  [  128/  179]
train() client id: f_00007-0-4 loss: 0.832341  [  160/  179]
train() client id: f_00007-1-0 loss: 0.806821  [   32/  179]
train() client id: f_00007-1-1 loss: 0.862297  [   64/  179]
train() client id: f_00007-1-2 loss: 0.792102  [   96/  179]
train() client id: f_00007-1-3 loss: 0.772955  [  128/  179]
train() client id: f_00007-1-4 loss: 0.845781  [  160/  179]
train() client id: f_00007-2-0 loss: 0.817473  [   32/  179]
train() client id: f_00007-2-1 loss: 0.740982  [   64/  179]
train() client id: f_00007-2-2 loss: 0.795490  [   96/  179]
train() client id: f_00007-2-3 loss: 0.849854  [  128/  179]
train() client id: f_00007-2-4 loss: 0.750501  [  160/  179]
train() client id: f_00007-3-0 loss: 0.754456  [   32/  179]
train() client id: f_00007-3-1 loss: 0.707133  [   64/  179]
train() client id: f_00007-3-2 loss: 0.738155  [   96/  179]
train() client id: f_00007-3-3 loss: 0.793217  [  128/  179]
train() client id: f_00007-3-4 loss: 0.752814  [  160/  179]
train() client id: f_00007-4-0 loss: 0.810449  [   32/  179]
train() client id: f_00007-4-1 loss: 0.801414  [   64/  179]
train() client id: f_00007-4-2 loss: 0.657332  [   96/  179]
train() client id: f_00007-4-3 loss: 0.688205  [  128/  179]
train() client id: f_00007-4-4 loss: 0.749337  [  160/  179]
train() client id: f_00007-5-0 loss: 0.691150  [   32/  179]
train() client id: f_00007-5-1 loss: 0.701784  [   64/  179]
train() client id: f_00007-5-2 loss: 0.681187  [   96/  179]
train() client id: f_00007-5-3 loss: 0.720621  [  128/  179]
train() client id: f_00007-5-4 loss: 0.693744  [  160/  179]
train() client id: f_00007-6-0 loss: 0.742619  [   32/  179]
train() client id: f_00007-6-1 loss: 0.688417  [   64/  179]
train() client id: f_00007-6-2 loss: 0.590562  [   96/  179]
train() client id: f_00007-6-3 loss: 0.709664  [  128/  179]
train() client id: f_00007-6-4 loss: 0.670946  [  160/  179]
train() client id: f_00007-7-0 loss: 0.711368  [   32/  179]
train() client id: f_00007-7-1 loss: 0.630484  [   64/  179]
train() client id: f_00007-7-2 loss: 0.741705  [   96/  179]
train() client id: f_00007-7-3 loss: 0.741777  [  128/  179]
train() client id: f_00007-7-4 loss: 0.643196  [  160/  179]
train() client id: f_00007-8-0 loss: 0.661154  [   32/  179]
train() client id: f_00007-8-1 loss: 0.663700  [   64/  179]
train() client id: f_00007-8-2 loss: 0.821022  [   96/  179]
train() client id: f_00007-8-3 loss: 0.669704  [  128/  179]
train() client id: f_00007-8-4 loss: 0.594625  [  160/  179]
train() client id: f_00007-9-0 loss: 0.709573  [   32/  179]
train() client id: f_00007-9-1 loss: 0.714574  [   64/  179]
train() client id: f_00007-9-2 loss: 0.676372  [   96/  179]
train() client id: f_00007-9-3 loss: 0.619278  [  128/  179]
train() client id: f_00007-9-4 loss: 0.651228  [  160/  179]
train() client id: f_00007-10-0 loss: 0.729927  [   32/  179]
train() client id: f_00007-10-1 loss: 0.746101  [   64/  179]
train() client id: f_00007-10-2 loss: 0.595219  [   96/  179]
train() client id: f_00007-10-3 loss: 0.638823  [  128/  179]
train() client id: f_00007-10-4 loss: 0.695051  [  160/  179]
train() client id: f_00007-11-0 loss: 0.721391  [   32/  179]
train() client id: f_00007-11-1 loss: 0.695036  [   64/  179]
train() client id: f_00007-11-2 loss: 0.650940  [   96/  179]
train() client id: f_00007-11-3 loss: 0.687474  [  128/  179]
train() client id: f_00007-11-4 loss: 0.624012  [  160/  179]
train() client id: f_00007-12-0 loss: 0.565756  [   32/  179]
train() client id: f_00007-12-1 loss: 0.566532  [   64/  179]
train() client id: f_00007-12-2 loss: 0.728035  [   96/  179]
train() client id: f_00007-12-3 loss: 0.645420  [  128/  179]
train() client id: f_00007-12-4 loss: 0.767230  [  160/  179]
train() client id: f_00008-0-0 loss: 0.779936  [   32/  130]
train() client id: f_00008-0-1 loss: 0.816162  [   64/  130]
train() client id: f_00008-0-2 loss: 0.768264  [   96/  130]
train() client id: f_00008-0-3 loss: 0.888529  [  128/  130]
train() client id: f_00008-1-0 loss: 0.793791  [   32/  130]
train() client id: f_00008-1-1 loss: 0.848298  [   64/  130]
train() client id: f_00008-1-2 loss: 0.702239  [   96/  130]
train() client id: f_00008-1-3 loss: 0.863871  [  128/  130]
train() client id: f_00008-2-0 loss: 0.760741  [   32/  130]
train() client id: f_00008-2-1 loss: 0.811096  [   64/  130]
train() client id: f_00008-2-2 loss: 0.799028  [   96/  130]
train() client id: f_00008-2-3 loss: 0.832439  [  128/  130]
train() client id: f_00008-3-0 loss: 0.712412  [   32/  130]
train() client id: f_00008-3-1 loss: 0.871679  [   64/  130]
train() client id: f_00008-3-2 loss: 0.753599  [   96/  130]
train() client id: f_00008-3-3 loss: 0.817658  [  128/  130]
train() client id: f_00008-4-0 loss: 0.763884  [   32/  130]
train() client id: f_00008-4-1 loss: 0.826443  [   64/  130]
train() client id: f_00008-4-2 loss: 0.790469  [   96/  130]
train() client id: f_00008-4-3 loss: 0.765214  [  128/  130]
train() client id: f_00008-5-0 loss: 0.750339  [   32/  130]
train() client id: f_00008-5-1 loss: 0.817995  [   64/  130]
train() client id: f_00008-5-2 loss: 0.755530  [   96/  130]
train() client id: f_00008-5-3 loss: 0.807609  [  128/  130]
train() client id: f_00008-6-0 loss: 0.762103  [   32/  130]
train() client id: f_00008-6-1 loss: 0.800692  [   64/  130]
train() client id: f_00008-6-2 loss: 0.825854  [   96/  130]
train() client id: f_00008-6-3 loss: 0.709763  [  128/  130]
train() client id: f_00008-7-0 loss: 0.819305  [   32/  130]
train() client id: f_00008-7-1 loss: 0.774419  [   64/  130]
train() client id: f_00008-7-2 loss: 0.764410  [   96/  130]
train() client id: f_00008-7-3 loss: 0.720562  [  128/  130]
train() client id: f_00008-8-0 loss: 0.815883  [   32/  130]
train() client id: f_00008-8-1 loss: 0.755140  [   64/  130]
train() client id: f_00008-8-2 loss: 0.776432  [   96/  130]
train() client id: f_00008-8-3 loss: 0.743370  [  128/  130]
train() client id: f_00008-9-0 loss: 0.736523  [   32/  130]
train() client id: f_00008-9-1 loss: 0.812569  [   64/  130]
train() client id: f_00008-9-2 loss: 0.748748  [   96/  130]
train() client id: f_00008-9-3 loss: 0.782762  [  128/  130]
train() client id: f_00008-10-0 loss: 0.741920  [   32/  130]
train() client id: f_00008-10-1 loss: 0.785421  [   64/  130]
train() client id: f_00008-10-2 loss: 0.775163  [   96/  130]
train() client id: f_00008-10-3 loss: 0.769718  [  128/  130]
train() client id: f_00008-11-0 loss: 0.810286  [   32/  130]
train() client id: f_00008-11-1 loss: 0.720597  [   64/  130]
train() client id: f_00008-11-2 loss: 0.725595  [   96/  130]
train() client id: f_00008-11-3 loss: 0.806586  [  128/  130]
train() client id: f_00008-12-0 loss: 0.859863  [   32/  130]
train() client id: f_00008-12-1 loss: 0.748799  [   64/  130]
train() client id: f_00008-12-2 loss: 0.740895  [   96/  130]
train() client id: f_00008-12-3 loss: 0.682557  [  128/  130]
train() client id: f_00009-0-0 loss: 1.164267  [   32/  118]
train() client id: f_00009-0-1 loss: 1.156565  [   64/  118]
train() client id: f_00009-0-2 loss: 1.085765  [   96/  118]
train() client id: f_00009-1-0 loss: 1.113846  [   32/  118]
train() client id: f_00009-1-1 loss: 1.096339  [   64/  118]
train() client id: f_00009-1-2 loss: 1.083367  [   96/  118]
train() client id: f_00009-2-0 loss: 1.052643  [   32/  118]
train() client id: f_00009-2-1 loss: 1.064370  [   64/  118]
train() client id: f_00009-2-2 loss: 1.091243  [   96/  118]
train() client id: f_00009-3-0 loss: 1.006983  [   32/  118]
train() client id: f_00009-3-1 loss: 1.092934  [   64/  118]
train() client id: f_00009-3-2 loss: 1.041728  [   96/  118]
train() client id: f_00009-4-0 loss: 1.049147  [   32/  118]
train() client id: f_00009-4-1 loss: 1.009431  [   64/  118]
train() client id: f_00009-4-2 loss: 1.037330  [   96/  118]
train() client id: f_00009-5-0 loss: 1.023789  [   32/  118]
train() client id: f_00009-5-1 loss: 0.959624  [   64/  118]
train() client id: f_00009-5-2 loss: 1.027666  [   96/  118]
train() client id: f_00009-6-0 loss: 0.987525  [   32/  118]
train() client id: f_00009-6-1 loss: 0.979382  [   64/  118]
train() client id: f_00009-6-2 loss: 1.002182  [   96/  118]
train() client id: f_00009-7-0 loss: 1.006338  [   32/  118]
train() client id: f_00009-7-1 loss: 0.986544  [   64/  118]
train() client id: f_00009-7-2 loss: 0.995932  [   96/  118]
train() client id: f_00009-8-0 loss: 0.940408  [   32/  118]
train() client id: f_00009-8-1 loss: 0.941238  [   64/  118]
train() client id: f_00009-8-2 loss: 1.012259  [   96/  118]
train() client id: f_00009-9-0 loss: 0.918143  [   32/  118]
train() client id: f_00009-9-1 loss: 0.931057  [   64/  118]
train() client id: f_00009-9-2 loss: 1.022703  [   96/  118]
train() client id: f_00009-10-0 loss: 0.899898  [   32/  118]
train() client id: f_00009-10-1 loss: 0.949813  [   64/  118]
train() client id: f_00009-10-2 loss: 0.998038  [   96/  118]
train() client id: f_00009-11-0 loss: 0.883452  [   32/  118]
train() client id: f_00009-11-1 loss: 0.997604  [   64/  118]
train() client id: f_00009-11-2 loss: 0.897420  [   96/  118]
train() client id: f_00009-12-0 loss: 0.976514  [   32/  118]
train() client id: f_00009-12-1 loss: 0.951489  [   64/  118]
train() client id: f_00009-12-2 loss: 0.899048  [   96/  118]
At round 3 accuracy: 0.6180371352785146
At round 3 training accuracy: 0.5660630449362843
At round 3 training loss: 0.9272532687622204
update_location
xs = 8.927491 136.223621 5.882650 10.934260 -52.581990 99.769243 -5.849135 -5.143845 -75.120581 20.134486 
ys = -127.390647 7.291448 25.684448 -127.290817 -9.642386 0.794442 -136.381692 21.628436 25.881276 -562.232496 
xs mean: 14.317620029478785
ys mean: -88.16579882624052
dists_uav = 162.197648 169.145027 103.413232 162.242135 113.392421 141.260514 169.216366 102.441439 127.722129 571.411215 
uav_gains = -105.263686 -105.727841 -100.364429 -105.266707 -101.364697 -103.752560 -105.732533 -100.261914 -102.657166 -125.823207 
uav_gains_db_mean: -105.62147406040214
dists_bs = 353.934211 353.534016 234.549924 354.896164 221.537762 325.339381 354.359385 228.853556 179.477832 762.619971 
bs_gains = -110.936890 -110.923133 -105.933647 -110.969896 -105.239597 -109.912485 -110.951489 -105.634674 -102.679362 -120.271688 
bs_gains_db_mean: -109.34528611473095
Round 4
-------------------------------
ene_coms = [0.00805582 0.00825711 0.00834951 0.0080571  0.00804563 0.00746193
 0.00825919 0.00821593 0.00708966 0.0288161 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.51001945 21.8691695  10.35461107  3.73050756 25.21237462 12.12756086
  4.63447509 14.84346375 10.81057797 10.41882514]
obj_prev = 124.5115850125245
eta_min = 1.1593026189596618e-09	eta_max = 0.7353516096701022
af = 26.181209634028566	bf = 2.697543122666742	zeta = 28.799330597431425	eta = 0.9090909090909091
af = 26.181209634028566	bf = 2.697543122666742	zeta = 57.12166812418799	eta = 0.4583411250719797
af = 26.181209634028566	bf = 2.697543122666742	zeta = 42.68847766564145	eta = 0.6133085803408951
af = 26.181209634028566	bf = 2.697543122666742	zeta = 40.0763951796251	eta = 0.6532825499070618
af = 26.181209634028566	bf = 2.697543122666742	zeta = 39.92898703723732	eta = 0.6556943107425256
af = 26.181209634028566	bf = 2.697543122666742	zeta = 39.928471395887236	eta = 0.6557027784621206
af = 26.181209634028566	bf = 2.697543122666742	zeta = 39.92847138954637	eta = 0.6557027785662499
eta = 0.6557027785662499
ene_coms = [0.00805582 0.00825711 0.00834951 0.0080571  0.00804563 0.00746193
 0.00825919 0.00821593 0.00708966 0.0288161 ]
ene_comp = [0.03482636 0.07324592 0.03427356 0.01188519 0.08457831 0.04035436
 0.01492558 0.04947555 0.03593196 0.03261516]
ene_total = [3.33948373 6.34711306 3.31930569 1.55302114 7.21316273 3.72373159
 1.80553281 4.49276917 3.35034305 4.78400843]
ti_comp = [0.3020544  0.30004149 0.29911746 0.3020416  0.30215627 0.30799322
 0.30002065 0.30045331 0.31171595 0.09445157]
ti_coms = [0.08055816 0.08257107 0.0834951  0.08057096 0.08045629 0.07461934
 0.08259192 0.08215925 0.07089662 0.28816099]
t_total = [29.79625702 29.79625702 29.79625702 29.79625702 29.79625702 29.79625702
 29.79625702 29.79625702 29.79625702 29.79625702]
ene_coms = [0.00805582 0.00825711 0.00834951 0.0080571  0.00804563 0.00746193
 0.00825919 0.00821593 0.00708966 0.0288161 ]
ene_comp = [2.89357031e-05 2.72814678e-04 2.81238171e-05 1.15017670e-06
 4.14184452e-04 4.32981142e-05 2.30872246e-06 8.38489902e-05
 2.98404076e-05 2.43063843e-04]
ene_total = [0.62960649 0.66427446 0.65241492 0.62754232 0.65881472 0.58447591
 0.6433709  0.6463515  0.55443688 2.26300537]
optimize_network iter = 0 obj = 7.924293464078611
eta = 0.6557027785662499
freqs = [5.76491565e+07 1.22059655e+08 5.72911448e+07 1.96747520e+07
 1.39957892e+08 6.55117609e+07 2.48742614e+07 8.23348338e+07
 5.76357448e+07 1.72655488e+08]
eta_min = 0.6557027785663171	eta_max = 0.6557027785662426
af = 0.07290432035213155	bf = 2.697543122666742	zeta = 0.08019475238734472	eta = 0.909090909090909
af = 0.07290432035213155	bf = 2.697543122666742	zeta = 29.749408112028128	eta = 0.002450614145921621
af = 0.07290432035213155	bf = 2.697543122666742	zeta = 3.1435048827999434	eta = 0.02319204934308711
af = 0.07290432035213155	bf = 2.697543122666742	zeta = 3.0425131451138383	eta = 0.023961875224504173
af = 0.07290432035213155	bf = 2.697543122666742	zeta = 3.042473751247778	eta = 0.023962185482202455
eta = 0.023962185482202455
ene_coms = [0.00805582 0.00825711 0.00834951 0.0080571  0.00804563 0.00746193
 0.00825919 0.00821593 0.00708966 0.0288161 ]
ene_comp = [2.55817453e-04 2.41192536e-03 2.48639656e-04 1.01685891e-05
 3.66176040e-03 3.82793991e-04 2.04111681e-05 7.41299944e-04
 2.63815848e-04 2.14890141e-03]
ene_total = [0.22832621 0.29308555 0.236197   0.22161322 0.32160992 0.21549999
 0.22744629 0.24606104 0.20200501 0.85062952]
ti_comp = [0.3020544  0.30004149 0.29911746 0.3020416  0.30215627 0.30799322
 0.30002065 0.30045331 0.31171595 0.09445157]
ti_coms = [0.08055816 0.08257107 0.0834951  0.08057096 0.08045629 0.07461934
 0.08259192 0.08215925 0.07089662 0.28816099]
t_total = [29.79625702 29.79625702 29.79625702 29.79625702 29.79625702 29.79625702
 29.79625702 29.79625702 29.79625702 29.79625702]
ene_coms = [0.00805582 0.00825711 0.00834951 0.0080571  0.00804563 0.00746193
 0.00825919 0.00821593 0.00708966 0.0288161 ]
ene_comp = [2.89357031e-05 2.72814678e-04 2.81238171e-05 1.15017670e-06
 4.14184452e-04 4.32981142e-05 2.30872246e-06 8.38489902e-05
 2.98404076e-05 2.43063843e-04]
ene_total = [0.62960649 0.66427446 0.65241492 0.62754232 0.65881472 0.58447591
 0.6433709  0.6463515  0.55443688 2.26300537]
optimize_network iter = 1 obj = 7.924293464080158
eta = 0.6557027785663171
freqs = [5.76491565e+07 1.22059655e+08 5.72911448e+07 1.96747520e+07
 1.39957892e+08 6.55117609e+07 2.48742614e+07 8.23348338e+07
 5.76357448e+07 1.72655488e+08]
Done!
ene_coms = [0.00805582 0.00825711 0.00834951 0.0080571  0.00804563 0.00746193
 0.00825919 0.00821593 0.00708966 0.0288161 ]
ene_comp = [2.72188528e-05 2.56627687e-04 2.64551387e-05 1.08193294e-06
 3.89609528e-04 4.07290948e-05 2.17173837e-06 7.88739542e-05
 2.80698782e-05 2.28642066e-04]
ene_total = [0.00808304 0.00851374 0.00837597 0.00805818 0.00843524 0.00750266
 0.00826136 0.0082948  0.00711773 0.02904474]
At round 4 energy consumption: 0.1016874509598573
At round 4 eta: 0.6557027785663171
At round 4 a_n: 26.812419468377104
At round 4 local rounds: 13.819985111468196
At round 4 global rounds: 77.87579393388047
gradient difference: 0.448889821767807
train() client id: f_00000-0-0 loss: 1.727562  [   32/  126]
train() client id: f_00000-0-1 loss: 1.309797  [   64/  126]
train() client id: f_00000-0-2 loss: 1.335031  [   96/  126]
train() client id: f_00000-1-0 loss: 1.264087  [   32/  126]
train() client id: f_00000-1-1 loss: 1.612980  [   64/  126]
train() client id: f_00000-1-2 loss: 1.491738  [   96/  126]
train() client id: f_00000-2-0 loss: 1.330187  [   32/  126]
train() client id: f_00000-2-1 loss: 1.346604  [   64/  126]
train() client id: f_00000-2-2 loss: 1.095626  [   96/  126]
train() client id: f_00000-3-0 loss: 1.183842  [   32/  126]
train() client id: f_00000-3-1 loss: 1.162599  [   64/  126]
train() client id: f_00000-3-2 loss: 1.224902  [   96/  126]
train() client id: f_00000-4-0 loss: 1.123945  [   32/  126]
train() client id: f_00000-4-1 loss: 1.063800  [   64/  126]
train() client id: f_00000-4-2 loss: 1.177082  [   96/  126]
train() client id: f_00000-5-0 loss: 1.066493  [   32/  126]
train() client id: f_00000-5-1 loss: 1.092177  [   64/  126]
train() client id: f_00000-5-2 loss: 1.025133  [   96/  126]
train() client id: f_00000-6-0 loss: 0.994798  [   32/  126]
train() client id: f_00000-6-1 loss: 1.008337  [   64/  126]
train() client id: f_00000-6-2 loss: 0.978542  [   96/  126]
train() client id: f_00000-7-0 loss: 0.999246  [   32/  126]
train() client id: f_00000-7-1 loss: 0.969063  [   64/  126]
train() client id: f_00000-7-2 loss: 0.985212  [   96/  126]
train() client id: f_00000-8-0 loss: 0.932114  [   32/  126]
train() client id: f_00000-8-1 loss: 1.003984  [   64/  126]
train() client id: f_00000-8-2 loss: 0.947139  [   96/  126]
train() client id: f_00000-9-0 loss: 0.950254  [   32/  126]
train() client id: f_00000-9-1 loss: 0.942856  [   64/  126]
train() client id: f_00000-9-2 loss: 0.957851  [   96/  126]
train() client id: f_00000-10-0 loss: 0.914287  [   32/  126]
train() client id: f_00000-10-1 loss: 0.941675  [   64/  126]
train() client id: f_00000-10-2 loss: 0.928631  [   96/  126]
train() client id: f_00000-11-0 loss: 0.916723  [   32/  126]
train() client id: f_00000-11-1 loss: 0.868488  [   64/  126]
train() client id: f_00000-11-2 loss: 0.958805  [   96/  126]
train() client id: f_00000-12-0 loss: 0.881722  [   32/  126]
train() client id: f_00000-12-1 loss: 0.923577  [   64/  126]
train() client id: f_00000-12-2 loss: 1.029319  [   96/  126]
train() client id: f_00001-0-0 loss: 0.549542  [   32/  265]
train() client id: f_00001-0-1 loss: 0.531417  [   64/  265]
train() client id: f_00001-0-2 loss: 0.558537  [   96/  265]
train() client id: f_00001-0-3 loss: 0.557985  [  128/  265]
train() client id: f_00001-0-4 loss: 0.657136  [  160/  265]
train() client id: f_00001-0-5 loss: 0.653656  [  192/  265]
train() client id: f_00001-0-6 loss: 0.535562  [  224/  265]
train() client id: f_00001-0-7 loss: 0.555507  [  256/  265]
train() client id: f_00001-1-0 loss: 0.601045  [   32/  265]
train() client id: f_00001-1-1 loss: 0.487945  [   64/  265]
train() client id: f_00001-1-2 loss: 0.613596  [   96/  265]
train() client id: f_00001-1-3 loss: 0.511630  [  128/  265]
train() client id: f_00001-1-4 loss: 0.449335  [  160/  265]
train() client id: f_00001-1-5 loss: 0.545663  [  192/  265]
train() client id: f_00001-1-6 loss: 0.548551  [  224/  265]
train() client id: f_00001-1-7 loss: 0.573983  [  256/  265]
train() client id: f_00001-2-0 loss: 0.616552  [   32/  265]
train() client id: f_00001-2-1 loss: 0.507808  [   64/  265]
train() client id: f_00001-2-2 loss: 0.528916  [   96/  265]
train() client id: f_00001-2-3 loss: 0.521832  [  128/  265]
train() client id: f_00001-2-4 loss: 0.477008  [  160/  265]
train() client id: f_00001-2-5 loss: 0.427677  [  192/  265]
train() client id: f_00001-2-6 loss: 0.557335  [  224/  265]
train() client id: f_00001-2-7 loss: 0.504434  [  256/  265]
train() client id: f_00001-3-0 loss: 0.560633  [   32/  265]
train() client id: f_00001-3-1 loss: 0.483435  [   64/  265]
train() client id: f_00001-3-2 loss: 0.441274  [   96/  265]
train() client id: f_00001-3-3 loss: 0.435912  [  128/  265]
train() client id: f_00001-3-4 loss: 0.474511  [  160/  265]
train() client id: f_00001-3-5 loss: 0.478701  [  192/  265]
train() client id: f_00001-3-6 loss: 0.535265  [  224/  265]
train() client id: f_00001-3-7 loss: 0.575311  [  256/  265]
train() client id: f_00001-4-0 loss: 0.539008  [   32/  265]
train() client id: f_00001-4-1 loss: 0.434110  [   64/  265]
train() client id: f_00001-4-2 loss: 0.548712  [   96/  265]
train() client id: f_00001-4-3 loss: 0.408948  [  128/  265]
train() client id: f_00001-4-4 loss: 0.615128  [  160/  265]
train() client id: f_00001-4-5 loss: 0.439308  [  192/  265]
train() client id: f_00001-4-6 loss: 0.408910  [  224/  265]
train() client id: f_00001-4-7 loss: 0.493288  [  256/  265]
train() client id: f_00001-5-0 loss: 0.519105  [   32/  265]
train() client id: f_00001-5-1 loss: 0.460444  [   64/  265]
train() client id: f_00001-5-2 loss: 0.385869  [   96/  265]
train() client id: f_00001-5-3 loss: 0.471793  [  128/  265]
train() client id: f_00001-5-4 loss: 0.534229  [  160/  265]
train() client id: f_00001-5-5 loss: 0.443389  [  192/  265]
train() client id: f_00001-5-6 loss: 0.453085  [  224/  265]
train() client id: f_00001-5-7 loss: 0.462502  [  256/  265]
train() client id: f_00001-6-0 loss: 0.439018  [   32/  265]
train() client id: f_00001-6-1 loss: 0.450552  [   64/  265]
train() client id: f_00001-6-2 loss: 0.406232  [   96/  265]
train() client id: f_00001-6-3 loss: 0.435899  [  128/  265]
train() client id: f_00001-6-4 loss: 0.438382  [  160/  265]
train() client id: f_00001-6-5 loss: 0.538452  [  192/  265]
train() client id: f_00001-6-6 loss: 0.540650  [  224/  265]
train() client id: f_00001-6-7 loss: 0.430795  [  256/  265]
train() client id: f_00001-7-0 loss: 0.451044  [   32/  265]
train() client id: f_00001-7-1 loss: 0.458755  [   64/  265]
train() client id: f_00001-7-2 loss: 0.513299  [   96/  265]
train() client id: f_00001-7-3 loss: 0.358093  [  128/  265]
train() client id: f_00001-7-4 loss: 0.458870  [  160/  265]
train() client id: f_00001-7-5 loss: 0.487846  [  192/  265]
train() client id: f_00001-7-6 loss: 0.465602  [  224/  265]
train() client id: f_00001-7-7 loss: 0.443748  [  256/  265]
train() client id: f_00001-8-0 loss: 0.367805  [   32/  265]
train() client id: f_00001-8-1 loss: 0.623868  [   64/  265]
train() client id: f_00001-8-2 loss: 0.489395  [   96/  265]
train() client id: f_00001-8-3 loss: 0.432155  [  128/  265]
train() client id: f_00001-8-4 loss: 0.398384  [  160/  265]
train() client id: f_00001-8-5 loss: 0.395885  [  192/  265]
train() client id: f_00001-8-6 loss: 0.463772  [  224/  265]
train() client id: f_00001-8-7 loss: 0.425395  [  256/  265]
train() client id: f_00001-9-0 loss: 0.537513  [   32/  265]
train() client id: f_00001-9-1 loss: 0.529494  [   64/  265]
train() client id: f_00001-9-2 loss: 0.455667  [   96/  265]
train() client id: f_00001-9-3 loss: 0.457633  [  128/  265]
train() client id: f_00001-9-4 loss: 0.376239  [  160/  265]
train() client id: f_00001-9-5 loss: 0.416456  [  192/  265]
train() client id: f_00001-9-6 loss: 0.422142  [  224/  265]
train() client id: f_00001-9-7 loss: 0.406702  [  256/  265]
train() client id: f_00001-10-0 loss: 0.386821  [   32/  265]
train() client id: f_00001-10-1 loss: 0.424258  [   64/  265]
train() client id: f_00001-10-2 loss: 0.406485  [   96/  265]
train() client id: f_00001-10-3 loss: 0.414295  [  128/  265]
train() client id: f_00001-10-4 loss: 0.440285  [  160/  265]
train() client id: f_00001-10-5 loss: 0.490673  [  192/  265]
train() client id: f_00001-10-6 loss: 0.637914  [  224/  265]
train() client id: f_00001-10-7 loss: 0.364068  [  256/  265]
train() client id: f_00001-11-0 loss: 0.364235  [   32/  265]
train() client id: f_00001-11-1 loss: 0.433991  [   64/  265]
train() client id: f_00001-11-2 loss: 0.486858  [   96/  265]
train() client id: f_00001-11-3 loss: 0.364623  [  128/  265]
train() client id: f_00001-11-4 loss: 0.413047  [  160/  265]
train() client id: f_00001-11-5 loss: 0.582020  [  192/  265]
train() client id: f_00001-11-6 loss: 0.462838  [  224/  265]
train() client id: f_00001-11-7 loss: 0.394152  [  256/  265]
train() client id: f_00001-12-0 loss: 0.467569  [   32/  265]
train() client id: f_00001-12-1 loss: 0.370947  [   64/  265]
train() client id: f_00001-12-2 loss: 0.434920  [   96/  265]
train() client id: f_00001-12-3 loss: 0.613918  [  128/  265]
train() client id: f_00001-12-4 loss: 0.474264  [  160/  265]
train() client id: f_00001-12-5 loss: 0.361271  [  192/  265]
train() client id: f_00001-12-6 loss: 0.328452  [  224/  265]
train() client id: f_00001-12-7 loss: 0.478840  [  256/  265]
train() client id: f_00002-0-0 loss: 1.210776  [   32/  124]
train() client id: f_00002-0-1 loss: 1.176306  [   64/  124]
train() client id: f_00002-0-2 loss: 1.192838  [   96/  124]
train() client id: f_00002-1-0 loss: 1.203915  [   32/  124]
train() client id: f_00002-1-1 loss: 1.102820  [   64/  124]
train() client id: f_00002-1-2 loss: 1.194441  [   96/  124]
train() client id: f_00002-2-0 loss: 1.149780  [   32/  124]
train() client id: f_00002-2-1 loss: 1.159365  [   64/  124]
train() client id: f_00002-2-2 loss: 1.055316  [   96/  124]
train() client id: f_00002-3-0 loss: 1.179000  [   32/  124]
train() client id: f_00002-3-1 loss: 1.126985  [   64/  124]
train() client id: f_00002-3-2 loss: 1.066151  [   96/  124]
train() client id: f_00002-4-0 loss: 1.121741  [   32/  124]
train() client id: f_00002-4-1 loss: 1.079843  [   64/  124]
train() client id: f_00002-4-2 loss: 1.057817  [   96/  124]
train() client id: f_00002-5-0 loss: 1.002721  [   32/  124]
train() client id: f_00002-5-1 loss: 1.073199  [   64/  124]
train() client id: f_00002-5-2 loss: 0.994628  [   96/  124]
train() client id: f_00002-6-0 loss: 1.057014  [   32/  124]
train() client id: f_00002-6-1 loss: 0.989782  [   64/  124]
train() client id: f_00002-6-2 loss: 1.019372  [   96/  124]
train() client id: f_00002-7-0 loss: 1.041905  [   32/  124]
train() client id: f_00002-7-1 loss: 1.002317  [   64/  124]
train() client id: f_00002-7-2 loss: 1.076980  [   96/  124]
train() client id: f_00002-8-0 loss: 1.123799  [   32/  124]
train() client id: f_00002-8-1 loss: 1.041696  [   64/  124]
train() client id: f_00002-8-2 loss: 0.918946  [   96/  124]
train() client id: f_00002-9-0 loss: 1.001608  [   32/  124]
train() client id: f_00002-9-1 loss: 1.026029  [   64/  124]
train() client id: f_00002-9-2 loss: 1.050048  [   96/  124]
train() client id: f_00002-10-0 loss: 1.025298  [   32/  124]
train() client id: f_00002-10-1 loss: 0.992726  [   64/  124]
train() client id: f_00002-10-2 loss: 0.981456  [   96/  124]
train() client id: f_00002-11-0 loss: 1.010368  [   32/  124]
train() client id: f_00002-11-1 loss: 1.066997  [   64/  124]
train() client id: f_00002-11-2 loss: 0.937499  [   96/  124]
train() client id: f_00002-12-0 loss: 0.992438  [   32/  124]
train() client id: f_00002-12-1 loss: 1.006197  [   64/  124]
train() client id: f_00002-12-2 loss: 1.040756  [   96/  124]
train() client id: f_00003-0-0 loss: 0.805956  [   32/   43]
train() client id: f_00003-1-0 loss: 0.796604  [   32/   43]
train() client id: f_00003-2-0 loss: 0.741981  [   32/   43]
train() client id: f_00003-3-0 loss: 0.711071  [   32/   43]
train() client id: f_00003-4-0 loss: 0.853410  [   32/   43]
train() client id: f_00003-5-0 loss: 0.886239  [   32/   43]
train() client id: f_00003-6-0 loss: 0.762273  [   32/   43]
train() client id: f_00003-7-0 loss: 0.880894  [   32/   43]
train() client id: f_00003-8-0 loss: 0.761471  [   32/   43]
train() client id: f_00003-9-0 loss: 0.815247  [   32/   43]
train() client id: f_00003-10-0 loss: 0.837029  [   32/   43]
train() client id: f_00003-11-0 loss: 0.854629  [   32/   43]
train() client id: f_00003-12-0 loss: 0.758042  [   32/   43]
train() client id: f_00004-0-0 loss: 0.880407  [   32/  306]
train() client id: f_00004-0-1 loss: 1.071851  [   64/  306]
train() client id: f_00004-0-2 loss: 0.968197  [   96/  306]
train() client id: f_00004-0-3 loss: 1.086787  [  128/  306]
train() client id: f_00004-0-4 loss: 1.074492  [  160/  306]
train() client id: f_00004-0-5 loss: 1.042325  [  192/  306]
train() client id: f_00004-0-6 loss: 1.020205  [  224/  306]
train() client id: f_00004-0-7 loss: 1.002637  [  256/  306]
train() client id: f_00004-0-8 loss: 1.007747  [  288/  306]
train() client id: f_00004-1-0 loss: 1.093552  [   32/  306]
train() client id: f_00004-1-1 loss: 1.056774  [   64/  306]
train() client id: f_00004-1-2 loss: 0.845189  [   96/  306]
train() client id: f_00004-1-3 loss: 1.113053  [  128/  306]
train() client id: f_00004-1-4 loss: 0.976969  [  160/  306]
train() client id: f_00004-1-5 loss: 1.073701  [  192/  306]
train() client id: f_00004-1-6 loss: 0.999900  [  224/  306]
train() client id: f_00004-1-7 loss: 0.987999  [  256/  306]
train() client id: f_00004-1-8 loss: 0.891595  [  288/  306]
train() client id: f_00004-2-0 loss: 1.088180  [   32/  306]
train() client id: f_00004-2-1 loss: 0.977616  [   64/  306]
train() client id: f_00004-2-2 loss: 1.019808  [   96/  306]
train() client id: f_00004-2-3 loss: 1.031560  [  128/  306]
train() client id: f_00004-2-4 loss: 1.057270  [  160/  306]
train() client id: f_00004-2-5 loss: 1.051119  [  192/  306]
train() client id: f_00004-2-6 loss: 0.963748  [  224/  306]
train() client id: f_00004-2-7 loss: 0.994349  [  256/  306]
train() client id: f_00004-2-8 loss: 0.894843  [  288/  306]
train() client id: f_00004-3-0 loss: 1.013264  [   32/  306]
train() client id: f_00004-3-1 loss: 1.055945  [   64/  306]
train() client id: f_00004-3-2 loss: 0.998535  [   96/  306]
train() client id: f_00004-3-3 loss: 1.001292  [  128/  306]
train() client id: f_00004-3-4 loss: 0.945369  [  160/  306]
train() client id: f_00004-3-5 loss: 1.051221  [  192/  306]
train() client id: f_00004-3-6 loss: 1.011708  [  224/  306]
train() client id: f_00004-3-7 loss: 1.105937  [  256/  306]
train() client id: f_00004-3-8 loss: 0.919107  [  288/  306]
train() client id: f_00004-4-0 loss: 1.003777  [   32/  306]
train() client id: f_00004-4-1 loss: 1.155175  [   64/  306]
train() client id: f_00004-4-2 loss: 0.992594  [   96/  306]
train() client id: f_00004-4-3 loss: 0.979954  [  128/  306]
train() client id: f_00004-4-4 loss: 1.096306  [  160/  306]
train() client id: f_00004-4-5 loss: 0.970165  [  192/  306]
train() client id: f_00004-4-6 loss: 0.932200  [  224/  306]
train() client id: f_00004-4-7 loss: 0.928792  [  256/  306]
train() client id: f_00004-4-8 loss: 0.970985  [  288/  306]
train() client id: f_00004-5-0 loss: 1.046207  [   32/  306]
train() client id: f_00004-5-1 loss: 0.952669  [   64/  306]
train() client id: f_00004-5-2 loss: 0.961340  [   96/  306]
train() client id: f_00004-5-3 loss: 1.095673  [  128/  306]
train() client id: f_00004-5-4 loss: 1.080192  [  160/  306]
train() client id: f_00004-5-5 loss: 1.018551  [  192/  306]
train() client id: f_00004-5-6 loss: 0.967074  [  224/  306]
train() client id: f_00004-5-7 loss: 0.881711  [  256/  306]
train() client id: f_00004-5-8 loss: 1.029098  [  288/  306]
train() client id: f_00004-6-0 loss: 0.894896  [   32/  306]
train() client id: f_00004-6-1 loss: 1.172056  [   64/  306]
train() client id: f_00004-6-2 loss: 0.971730  [   96/  306]
train() client id: f_00004-6-3 loss: 0.996372  [  128/  306]
train() client id: f_00004-6-4 loss: 1.090088  [  160/  306]
train() client id: f_00004-6-5 loss: 0.927132  [  192/  306]
train() client id: f_00004-6-6 loss: 1.012508  [  224/  306]
train() client id: f_00004-6-7 loss: 1.036259  [  256/  306]
train() client id: f_00004-6-8 loss: 0.989773  [  288/  306]
train() client id: f_00004-7-0 loss: 0.999445  [   32/  306]
train() client id: f_00004-7-1 loss: 0.963718  [   64/  306]
train() client id: f_00004-7-2 loss: 1.018885  [   96/  306]
train() client id: f_00004-7-3 loss: 1.058719  [  128/  306]
train() client id: f_00004-7-4 loss: 0.973063  [  160/  306]
train() client id: f_00004-7-5 loss: 1.023381  [  192/  306]
train() client id: f_00004-7-6 loss: 0.988243  [  224/  306]
train() client id: f_00004-7-7 loss: 1.007623  [  256/  306]
train() client id: f_00004-7-8 loss: 0.959096  [  288/  306]
train() client id: f_00004-8-0 loss: 1.097387  [   32/  306]
train() client id: f_00004-8-1 loss: 1.100081  [   64/  306]
train() client id: f_00004-8-2 loss: 0.968574  [   96/  306]
train() client id: f_00004-8-3 loss: 1.073377  [  128/  306]
train() client id: f_00004-8-4 loss: 0.999390  [  160/  306]
train() client id: f_00004-8-5 loss: 1.049636  [  192/  306]
train() client id: f_00004-8-6 loss: 0.886615  [  224/  306]
train() client id: f_00004-8-7 loss: 0.923313  [  256/  306]
train() client id: f_00004-8-8 loss: 0.918256  [  288/  306]
train() client id: f_00004-9-0 loss: 1.040435  [   32/  306]
train() client id: f_00004-9-1 loss: 1.097635  [   64/  306]
train() client id: f_00004-9-2 loss: 0.973185  [   96/  306]
train() client id: f_00004-9-3 loss: 0.974088  [  128/  306]
train() client id: f_00004-9-4 loss: 1.012395  [  160/  306]
train() client id: f_00004-9-5 loss: 0.983589  [  192/  306]
train() client id: f_00004-9-6 loss: 0.949411  [  224/  306]
train() client id: f_00004-9-7 loss: 1.046468  [  256/  306]
train() client id: f_00004-9-8 loss: 0.949326  [  288/  306]
train() client id: f_00004-10-0 loss: 1.172006  [   32/  306]
train() client id: f_00004-10-1 loss: 1.018482  [   64/  306]
train() client id: f_00004-10-2 loss: 0.991657  [   96/  306]
train() client id: f_00004-10-3 loss: 0.891211  [  128/  306]
train() client id: f_00004-10-4 loss: 0.962478  [  160/  306]
train() client id: f_00004-10-5 loss: 0.951711  [  192/  306]
train() client id: f_00004-10-6 loss: 0.916680  [  224/  306]
train() client id: f_00004-10-7 loss: 0.993019  [  256/  306]
train() client id: f_00004-10-8 loss: 0.976928  [  288/  306]
train() client id: f_00004-11-0 loss: 1.097800  [   32/  306]
train() client id: f_00004-11-1 loss: 1.017804  [   64/  306]
train() client id: f_00004-11-2 loss: 1.106573  [   96/  306]
train() client id: f_00004-11-3 loss: 1.016589  [  128/  306]
train() client id: f_00004-11-4 loss: 0.875023  [  160/  306]
train() client id: f_00004-11-5 loss: 1.052251  [  192/  306]
train() client id: f_00004-11-6 loss: 1.021579  [  224/  306]
train() client id: f_00004-11-7 loss: 0.919620  [  256/  306]
train() client id: f_00004-11-8 loss: 0.908138  [  288/  306]
train() client id: f_00004-12-0 loss: 0.931273  [   32/  306]
train() client id: f_00004-12-1 loss: 1.022605  [   64/  306]
train() client id: f_00004-12-2 loss: 1.001227  [   96/  306]
train() client id: f_00004-12-3 loss: 0.947389  [  128/  306]
train() client id: f_00004-12-4 loss: 0.995340  [  160/  306]
train() client id: f_00004-12-5 loss: 1.007756  [  192/  306]
train() client id: f_00004-12-6 loss: 1.064026  [  224/  306]
train() client id: f_00004-12-7 loss: 0.993667  [  256/  306]
train() client id: f_00004-12-8 loss: 0.957566  [  288/  306]
train() client id: f_00005-0-0 loss: 1.008126  [   32/  146]
train() client id: f_00005-0-1 loss: 0.888519  [   64/  146]
train() client id: f_00005-0-2 loss: 0.936388  [   96/  146]
train() client id: f_00005-0-3 loss: 0.947451  [  128/  146]
train() client id: f_00005-1-0 loss: 0.961250  [   32/  146]
train() client id: f_00005-1-1 loss: 0.873483  [   64/  146]
train() client id: f_00005-1-2 loss: 0.963094  [   96/  146]
train() client id: f_00005-1-3 loss: 0.864830  [  128/  146]
train() client id: f_00005-2-0 loss: 0.899372  [   32/  146]
train() client id: f_00005-2-1 loss: 0.960059  [   64/  146]
train() client id: f_00005-2-2 loss: 0.856316  [   96/  146]
train() client id: f_00005-2-3 loss: 0.929734  [  128/  146]
train() client id: f_00005-3-0 loss: 0.884733  [   32/  146]
train() client id: f_00005-3-1 loss: 0.922552  [   64/  146]
train() client id: f_00005-3-2 loss: 0.884126  [   96/  146]
train() client id: f_00005-3-3 loss: 0.946250  [  128/  146]
train() client id: f_00005-4-0 loss: 0.876178  [   32/  146]
train() client id: f_00005-4-1 loss: 0.748866  [   64/  146]
train() client id: f_00005-4-2 loss: 0.985739  [   96/  146]
train() client id: f_00005-4-3 loss: 0.923867  [  128/  146]
train() client id: f_00005-5-0 loss: 0.837144  [   32/  146]
train() client id: f_00005-5-1 loss: 0.988990  [   64/  146]
train() client id: f_00005-5-2 loss: 0.782887  [   96/  146]
train() client id: f_00005-5-3 loss: 0.947150  [  128/  146]
train() client id: f_00005-6-0 loss: 0.835901  [   32/  146]
train() client id: f_00005-6-1 loss: 0.830440  [   64/  146]
train() client id: f_00005-6-2 loss: 0.900259  [   96/  146]
train() client id: f_00005-6-3 loss: 0.852578  [  128/  146]
train() client id: f_00005-7-0 loss: 0.834184  [   32/  146]
train() client id: f_00005-7-1 loss: 0.987707  [   64/  146]
train() client id: f_00005-7-2 loss: 0.846333  [   96/  146]
train() client id: f_00005-7-3 loss: 0.913137  [  128/  146]
train() client id: f_00005-8-0 loss: 0.852974  [   32/  146]
train() client id: f_00005-8-1 loss: 0.923203  [   64/  146]
train() client id: f_00005-8-2 loss: 0.877222  [   96/  146]
train() client id: f_00005-8-3 loss: 0.877014  [  128/  146]
train() client id: f_00005-9-0 loss: 0.781871  [   32/  146]
train() client id: f_00005-9-1 loss: 0.863438  [   64/  146]
train() client id: f_00005-9-2 loss: 0.905263  [   96/  146]
train() client id: f_00005-9-3 loss: 0.946425  [  128/  146]
train() client id: f_00005-10-0 loss: 0.770174  [   32/  146]
train() client id: f_00005-10-1 loss: 0.964100  [   64/  146]
train() client id: f_00005-10-2 loss: 0.833988  [   96/  146]
train() client id: f_00005-10-3 loss: 0.854285  [  128/  146]
train() client id: f_00005-11-0 loss: 0.875281  [   32/  146]
train() client id: f_00005-11-1 loss: 0.915978  [   64/  146]
train() client id: f_00005-11-2 loss: 0.840015  [   96/  146]
train() client id: f_00005-11-3 loss: 0.859776  [  128/  146]
train() client id: f_00005-12-0 loss: 0.873224  [   32/  146]
train() client id: f_00005-12-1 loss: 0.919858  [   64/  146]
train() client id: f_00005-12-2 loss: 0.984143  [   96/  146]
train() client id: f_00005-12-3 loss: 0.855984  [  128/  146]
train() client id: f_00006-0-0 loss: 0.849052  [   32/   54]
train() client id: f_00006-1-0 loss: 0.851601  [   32/   54]
train() client id: f_00006-2-0 loss: 0.865412  [   32/   54]
train() client id: f_00006-3-0 loss: 0.866500  [   32/   54]
train() client id: f_00006-4-0 loss: 0.842523  [   32/   54]
train() client id: f_00006-5-0 loss: 0.885604  [   32/   54]
train() client id: f_00006-6-0 loss: 0.866071  [   32/   54]
train() client id: f_00006-7-0 loss: 0.849727  [   32/   54]
train() client id: f_00006-8-0 loss: 0.836339  [   32/   54]
train() client id: f_00006-9-0 loss: 0.873412  [   32/   54]
train() client id: f_00006-10-0 loss: 0.851706  [   32/   54]
train() client id: f_00006-11-0 loss: 0.898371  [   32/   54]
train() client id: f_00006-12-0 loss: 0.867254  [   32/   54]
train() client id: f_00007-0-0 loss: 0.749493  [   32/  179]
train() client id: f_00007-0-1 loss: 0.798191  [   64/  179]
train() client id: f_00007-0-2 loss: 0.758768  [   96/  179]
train() client id: f_00007-0-3 loss: 0.720914  [  128/  179]
train() client id: f_00007-0-4 loss: 0.747489  [  160/  179]
train() client id: f_00007-1-0 loss: 0.669950  [   32/  179]
train() client id: f_00007-1-1 loss: 0.770119  [   64/  179]
train() client id: f_00007-1-2 loss: 0.798907  [   96/  179]
train() client id: f_00007-1-3 loss: 0.740317  [  128/  179]
train() client id: f_00007-1-4 loss: 0.696803  [  160/  179]
train() client id: f_00007-2-0 loss: 0.727289  [   32/  179]
train() client id: f_00007-2-1 loss: 0.804837  [   64/  179]
train() client id: f_00007-2-2 loss: 0.671332  [   96/  179]
train() client id: f_00007-2-3 loss: 0.646448  [  128/  179]
train() client id: f_00007-2-4 loss: 0.657960  [  160/  179]
train() client id: f_00007-3-0 loss: 0.731834  [   32/  179]
train() client id: f_00007-3-1 loss: 0.563195  [   64/  179]
train() client id: f_00007-3-2 loss: 0.648159  [   96/  179]
train() client id: f_00007-3-3 loss: 0.740188  [  128/  179]
train() client id: f_00007-3-4 loss: 0.703257  [  160/  179]
train() client id: f_00007-4-0 loss: 0.626114  [   32/  179]
train() client id: f_00007-4-1 loss: 0.706007  [   64/  179]
train() client id: f_00007-4-2 loss: 0.693263  [   96/  179]
train() client id: f_00007-4-3 loss: 0.647825  [  128/  179]
train() client id: f_00007-4-4 loss: 0.617667  [  160/  179]
train() client id: f_00007-5-0 loss: 0.581871  [   32/  179]
train() client id: f_00007-5-1 loss: 0.769232  [   64/  179]
train() client id: f_00007-5-2 loss: 0.787395  [   96/  179]
train() client id: f_00007-5-3 loss: 0.550778  [  128/  179]
train() client id: f_00007-5-4 loss: 0.562725  [  160/  179]
train() client id: f_00007-6-0 loss: 0.597456  [   32/  179]
train() client id: f_00007-6-1 loss: 0.613956  [   64/  179]
train() client id: f_00007-6-2 loss: 0.824884  [   96/  179]
train() client id: f_00007-6-3 loss: 0.588437  [  128/  179]
train() client id: f_00007-6-4 loss: 0.529202  [  160/  179]
train() client id: f_00007-7-0 loss: 0.529283  [   32/  179]
train() client id: f_00007-7-1 loss: 0.552513  [   64/  179]
train() client id: f_00007-7-2 loss: 0.633320  [   96/  179]
train() client id: f_00007-7-3 loss: 0.655586  [  128/  179]
train() client id: f_00007-7-4 loss: 0.547361  [  160/  179]
train() client id: f_00007-8-0 loss: 0.710781  [   32/  179]
train() client id: f_00007-8-1 loss: 0.485982  [   64/  179]
train() client id: f_00007-8-2 loss: 0.619254  [   96/  179]
train() client id: f_00007-8-3 loss: 0.650730  [  128/  179]
train() client id: f_00007-8-4 loss: 0.594025  [  160/  179]
train() client id: f_00007-9-0 loss: 0.496958  [   32/  179]
train() client id: f_00007-9-1 loss: 0.664824  [   64/  179]
train() client id: f_00007-9-2 loss: 0.618164  [   96/  179]
train() client id: f_00007-9-3 loss: 0.655268  [  128/  179]
train() client id: f_00007-9-4 loss: 0.572499  [  160/  179]
train() client id: f_00007-10-0 loss: 0.682944  [   32/  179]
train() client id: f_00007-10-1 loss: 0.560525  [   64/  179]
train() client id: f_00007-10-2 loss: 0.590164  [   96/  179]
train() client id: f_00007-10-3 loss: 0.552363  [  128/  179]
train() client id: f_00007-10-4 loss: 0.551656  [  160/  179]
train() client id: f_00007-11-0 loss: 0.546473  [   32/  179]
train() client id: f_00007-11-1 loss: 0.659294  [   64/  179]
train() client id: f_00007-11-2 loss: 0.573407  [   96/  179]
train() client id: f_00007-11-3 loss: 0.477851  [  128/  179]
train() client id: f_00007-11-4 loss: 0.738822  [  160/  179]
train() client id: f_00007-12-0 loss: 0.697835  [   32/  179]
train() client id: f_00007-12-1 loss: 0.670270  [   64/  179]
train() client id: f_00007-12-2 loss: 0.568376  [   96/  179]
train() client id: f_00007-12-3 loss: 0.542588  [  128/  179]
train() client id: f_00007-12-4 loss: 0.494222  [  160/  179]
train() client id: f_00008-0-0 loss: 0.892981  [   32/  130]
train() client id: f_00008-0-1 loss: 0.820922  [   64/  130]
train() client id: f_00008-0-2 loss: 0.920038  [   96/  130]
train() client id: f_00008-0-3 loss: 0.876805  [  128/  130]
train() client id: f_00008-1-0 loss: 0.889117  [   32/  130]
train() client id: f_00008-1-1 loss: 0.828871  [   64/  130]
train() client id: f_00008-1-2 loss: 0.913329  [   96/  130]
train() client id: f_00008-1-3 loss: 0.909016  [  128/  130]
train() client id: f_00008-2-0 loss: 0.936797  [   32/  130]
train() client id: f_00008-2-1 loss: 0.794813  [   64/  130]
train() client id: f_00008-2-2 loss: 0.900062  [   96/  130]
train() client id: f_00008-2-3 loss: 0.909584  [  128/  130]
train() client id: f_00008-3-0 loss: 0.969899  [   32/  130]
train() client id: f_00008-3-1 loss: 0.834616  [   64/  130]
train() client id: f_00008-3-2 loss: 0.796307  [   96/  130]
train() client id: f_00008-3-3 loss: 0.941130  [  128/  130]
train() client id: f_00008-4-0 loss: 0.835646  [   32/  130]
train() client id: f_00008-4-1 loss: 0.849703  [   64/  130]
train() client id: f_00008-4-2 loss: 0.836622  [   96/  130]
train() client id: f_00008-4-3 loss: 0.997695  [  128/  130]
train() client id: f_00008-5-0 loss: 0.823808  [   32/  130]
train() client id: f_00008-5-1 loss: 0.897238  [   64/  130]
train() client id: f_00008-5-2 loss: 0.962584  [   96/  130]
train() client id: f_00008-5-3 loss: 0.870914  [  128/  130]
train() client id: f_00008-6-0 loss: 0.814535  [   32/  130]
train() client id: f_00008-6-1 loss: 1.000875  [   64/  130]
train() client id: f_00008-6-2 loss: 0.893488  [   96/  130]
train() client id: f_00008-6-3 loss: 0.854077  [  128/  130]
train() client id: f_00008-7-0 loss: 0.905719  [   32/  130]
train() client id: f_00008-7-1 loss: 0.826888  [   64/  130]
train() client id: f_00008-7-2 loss: 0.882638  [   96/  130]
train() client id: f_00008-7-3 loss: 0.957861  [  128/  130]
train() client id: f_00008-8-0 loss: 0.883378  [   32/  130]
train() client id: f_00008-8-1 loss: 0.856017  [   64/  130]
train() client id: f_00008-8-2 loss: 0.824210  [   96/  130]
train() client id: f_00008-8-3 loss: 1.006882  [  128/  130]
train() client id: f_00008-9-0 loss: 1.002893  [   32/  130]
train() client id: f_00008-9-1 loss: 0.840089  [   64/  130]
train() client id: f_00008-9-2 loss: 0.815362  [   96/  130]
train() client id: f_00008-9-3 loss: 0.919924  [  128/  130]
train() client id: f_00008-10-0 loss: 0.965281  [   32/  130]
train() client id: f_00008-10-1 loss: 0.872661  [   64/  130]
train() client id: f_00008-10-2 loss: 0.867178  [   96/  130]
train() client id: f_00008-10-3 loss: 0.872489  [  128/  130]
train() client id: f_00008-11-0 loss: 0.802203  [   32/  130]
train() client id: f_00008-11-1 loss: 0.912866  [   64/  130]
train() client id: f_00008-11-2 loss: 0.807106  [   96/  130]
train() client id: f_00008-11-3 loss: 1.021976  [  128/  130]
train() client id: f_00008-12-0 loss: 0.918107  [   32/  130]
train() client id: f_00008-12-1 loss: 0.922312  [   64/  130]
train() client id: f_00008-12-2 loss: 0.845172  [   96/  130]
train() client id: f_00008-12-3 loss: 0.909116  [  128/  130]
train() client id: f_00009-0-0 loss: 1.177013  [   32/  118]
train() client id: f_00009-0-1 loss: 1.114101  [   64/  118]
train() client id: f_00009-0-2 loss: 1.153017  [   96/  118]
train() client id: f_00009-1-0 loss: 1.151600  [   32/  118]
train() client id: f_00009-1-1 loss: 1.118605  [   64/  118]
train() client id: f_00009-1-2 loss: 1.079193  [   96/  118]
train() client id: f_00009-2-0 loss: 1.105173  [   32/  118]
train() client id: f_00009-2-1 loss: 1.024001  [   64/  118]
train() client id: f_00009-2-2 loss: 1.067192  [   96/  118]
train() client id: f_00009-3-0 loss: 1.079307  [   32/  118]
train() client id: f_00009-3-1 loss: 1.070655  [   64/  118]
train() client id: f_00009-3-2 loss: 1.065937  [   96/  118]
train() client id: f_00009-4-0 loss: 1.006087  [   32/  118]
train() client id: f_00009-4-1 loss: 1.049036  [   64/  118]
train() client id: f_00009-4-2 loss: 1.091909  [   96/  118]
train() client id: f_00009-5-0 loss: 1.016092  [   32/  118]
train() client id: f_00009-5-1 loss: 1.016798  [   64/  118]
train() client id: f_00009-5-2 loss: 0.992547  [   96/  118]
train() client id: f_00009-6-0 loss: 1.022293  [   32/  118]
train() client id: f_00009-6-1 loss: 1.040931  [   64/  118]
train() client id: f_00009-6-2 loss: 0.971854  [   96/  118]
train() client id: f_00009-7-0 loss: 1.030495  [   32/  118]
train() client id: f_00009-7-1 loss: 1.024775  [   64/  118]
train() client id: f_00009-7-2 loss: 1.000779  [   96/  118]
train() client id: f_00009-8-0 loss: 1.053869  [   32/  118]
train() client id: f_00009-8-1 loss: 0.941724  [   64/  118]
train() client id: f_00009-8-2 loss: 0.959467  [   96/  118]
train() client id: f_00009-9-0 loss: 1.059069  [   32/  118]
train() client id: f_00009-9-1 loss: 1.055977  [   64/  118]
train() client id: f_00009-9-2 loss: 0.919237  [   96/  118]
train() client id: f_00009-10-0 loss: 0.996929  [   32/  118]
train() client id: f_00009-10-1 loss: 0.909705  [   64/  118]
train() client id: f_00009-10-2 loss: 1.055591  [   96/  118]
train() client id: f_00009-11-0 loss: 0.982212  [   32/  118]
train() client id: f_00009-11-1 loss: 0.959315  [   64/  118]
train() client id: f_00009-11-2 loss: 0.963398  [   96/  118]
train() client id: f_00009-12-0 loss: 1.018028  [   32/  118]
train() client id: f_00009-12-1 loss: 0.878726  [   64/  118]
train() client id: f_00009-12-2 loss: 1.071046  [   96/  118]
At round 4 accuracy: 0.6206896551724138
At round 4 training accuracy: 0.5761234071093226
At round 4 training loss: 0.9080887357703716
update_location
xs = 8.927491 141.223621 5.882650 10.934260 -57.581990 94.769243 -5.849135 -5.143845 -80.120581 20.134486 
ys = -132.390647 7.291448 30.684448 -122.290817 -9.642386 0.794442 -131.381692 26.628436 25.881276 -567.232496 
xs mean: 13.317620029478785
ys mean: -87.16579882624052
dists_uav = 166.153494 173.197218 104.767079 158.349620 115.795773 137.774600 165.213079 103.612416 130.725468 576.331590 
uav_gains = -105.529808 -105.992188 -100.505652 -104.999662 -101.592447 -103.480683 -105.467000 -100.385322 -102.909712 -125.919915 
uav_gains_db_mean: -105.67823893336538
dists_bs = 358.215483 357.943483 231.399031 350.647086 218.814533 321.127733 349.973937 225.533222 176.744159 767.454589 
bs_gains = -111.083101 -111.073864 -105.769182 -110.823426 -105.089192 -109.754038 -110.800059 -105.456954 -102.492721 -120.348534 
bs_gains_db_mean: -109.2691070749354
Round 5
-------------------------------
ene_coms = [0.00817005 0.00837616 0.00827551 0.00794551 0.00798259 0.00736405
 0.0081428  0.00813846 0.00702861 0.02911429]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.37880139 21.59296    10.22034569  3.67986451 24.88858477 11.97000639
  4.57215481 14.65175786 10.67083354 10.29369066]
obj_prev = 122.9189996170307
eta_min = 8.927897656238698e-10	eta_max = 0.7356384622531762
af = 25.846727897333864	bf = 2.661228902711813	zeta = 28.431400687067253	eta = 0.909090909090909
af = 25.846727897333864	bf = 2.661228902711813	zeta = 56.371537045561716	eta = 0.45850670838447266
af = 25.846727897333864	bf = 2.661228902711813	zeta = 42.13533194844765	eta = 0.6134217223910137
af = 25.846727897333864	bf = 2.661228902711813	zeta = 39.55885081202542	eta = 0.6533740835938734
af = 25.846727897333864	bf = 2.661228902711813	zeta = 39.413489794904315	eta = 0.655783794630526
af = 25.846727897333864	bf = 2.661228902711813	zeta = 39.412981613979646	eta = 0.6557922501393835
af = 25.846727897333864	bf = 2.661228902711813	zeta = 39.412981607737905	eta = 0.6557922502432398
eta = 0.6557922502432398
ene_coms = [0.00817005 0.00837616 0.00827551 0.00794551 0.00798259 0.00736405
 0.0081428  0.00813846 0.00702861 0.02911429]
ene_comp = [0.0348151  0.07322224 0.03426248 0.01188134 0.08455097 0.04034131
 0.01492076 0.04945955 0.03592034 0.03260462]
ene_total = [3.30559515 6.27498771 3.27120839 1.52470252 7.11591018 3.66858361
 1.77360791 4.42933715 3.30281173 4.74623726]
ti_comp = [0.30510041 0.30303926 0.30404577 0.30734574 0.30697501 0.31316035
 0.30537283 0.30541624 0.31651477 0.09565802]
ti_coms = [0.08170046 0.08376161 0.08275509 0.07945513 0.07982586 0.07364052
 0.08142804 0.08138463 0.07028609 0.29114285]
t_total = [29.74532127 29.74532127 29.74532127 29.74532127 29.74532127 29.74532127
 29.74532127 29.74532127 29.74532127 29.74532127]
ene_coms = [0.00817005 0.00837616 0.00827551 0.00794551 0.00798259 0.00736405
 0.0081428  0.00813846 0.00702861 0.02911429]
ene_comp = [2.83333225e-05 2.67184514e-04 2.71930905e-05 1.10974305e-06
 4.00894136e-04 4.18404631e-05 2.22634236e-06 8.10674102e-05
 2.89143570e-05 2.36741693e-04]
ene_total = [0.63046243 0.66468072 0.638485   0.61110217 0.64469689 0.56951952
 0.62635987 0.63208899 0.54272968 2.25711936]
optimize_network iter = 0 obj = 7.817244623017148
eta = 0.6557922502432398
freqs = [5.70551567e+07 1.20813127e+08 5.63442840e+07 1.93289567e+07
 1.37716368e+08 6.44099906e+07 2.44303966e+07 8.09707303e+07
 5.67435517e+07 1.70422838e+08]
eta_min = 0.6557922502432595	eta_max = 0.6557922502432294
af = 0.06998457505441089	bf = 2.661228902711813	zeta = 0.07698303255985198	eta = 0.9090909090909091
af = 0.06998457505441089	bf = 2.661228902711813	zeta = 29.34689059658155	eta = 0.002384735610203385
af = 0.06998457505441089	bf = 2.661228902711813	zeta = 3.0912141762498435	eta = 0.022639833755975395
af = 0.06998457505441089	bf = 2.661228902711813	zeta = 2.994119956189685	eta = 0.023374005076093614
af = 0.06998457505441089	bf = 2.661228902711813	zeta = 2.99408384947619	eta = 0.023374286951467496
eta = 0.023374286951467496
ene_coms = [0.00817005 0.00837616 0.00827551 0.00794551 0.00798259 0.00736405
 0.0081428  0.00813846 0.00702861 0.02911429]
ene_comp = [2.52241028e-04 2.37864431e-03 2.42089967e-04 9.87962948e-06
 3.56901133e-03 3.72490075e-04 1.98202977e-05 7.21712989e-04
 2.57413762e-04 2.10762321e-03]
ene_total = [0.22827257 0.29149174 0.23085587 0.21561816 0.31308753 0.20968656
 0.22123484 0.24014086 0.19747597 0.84621976]
ti_comp = [0.30510041 0.30303926 0.30404577 0.30734574 0.30697501 0.31316035
 0.30537283 0.30541624 0.31651477 0.09565802]
ti_coms = [0.08170046 0.08376161 0.08275509 0.07945513 0.07982586 0.07364052
 0.08142804 0.08138463 0.07028609 0.29114285]
t_total = [29.74532127 29.74532127 29.74532127 29.74532127 29.74532127 29.74532127
 29.74532127 29.74532127 29.74532127 29.74532127]
ene_coms = [0.00817005 0.00837616 0.00827551 0.00794551 0.00798259 0.00736405
 0.0081428  0.00813846 0.00702861 0.02911429]
ene_comp = [2.83333225e-05 2.67184514e-04 2.71930905e-05 1.10974305e-06
 4.00894136e-04 4.18404631e-05 2.22634236e-06 8.10674102e-05
 2.89143570e-05 2.36741693e-04]
ene_total = [0.63046243 0.66468072 0.638485   0.61110217 0.64469689 0.56951952
 0.62635987 0.63208899 0.54272968 2.25711936]
optimize_network iter = 1 obj = 7.8172446230175945
eta = 0.6557922502432595
freqs = [5.70551567e+07 1.20813127e+08 5.63442840e+07 1.93289567e+07
 1.37716368e+08 6.44099906e+07 2.44303966e+07 8.09707303e+07
 5.67435517e+07 1.70422838e+08]
Done!
ene_coms = [0.00817005 0.00837616 0.00827551 0.00794551 0.00798259 0.00736405
 0.0081428  0.00813846 0.00702861 0.02911429]
ene_comp = [2.66608325e-05 2.51412857e-04 2.55879074e-05 1.04423594e-06
 3.77229723e-04 3.93706590e-05 2.09492342e-06 7.62820754e-05
 2.72075691e-05 2.22767048e-04]
ene_total = [0.00819671 0.00862757 0.0083011  0.00794656 0.00835982 0.00740342
 0.0081449  0.00821474 0.00705582 0.02933705]
At round 5 energy consumption: 0.10158768611166731
At round 5 eta: 0.6557922502432595
At round 5 a_n: 26.46987362140779
At round 5 local rounds: 13.815517298454933
At round 5 global rounds: 76.9008647833021
gradient difference: 0.3445906341075897
train() client id: f_00000-0-0 loss: 1.824218  [   32/  126]
train() client id: f_00000-0-1 loss: 1.926319  [   64/  126]
train() client id: f_00000-0-2 loss: 1.689433  [   96/  126]
train() client id: f_00000-1-0 loss: 1.689636  [   32/  126]
train() client id: f_00000-1-1 loss: 1.569250  [   64/  126]
train() client id: f_00000-1-2 loss: 1.438060  [   96/  126]
train() client id: f_00000-2-0 loss: 1.486908  [   32/  126]
train() client id: f_00000-2-1 loss: 1.486165  [   64/  126]
train() client id: f_00000-2-2 loss: 1.384134  [   96/  126]
train() client id: f_00000-3-0 loss: 1.412296  [   32/  126]
train() client id: f_00000-3-1 loss: 1.296556  [   64/  126]
train() client id: f_00000-3-2 loss: 1.287874  [   96/  126]
train() client id: f_00000-4-0 loss: 1.298933  [   32/  126]
train() client id: f_00000-4-1 loss: 1.204120  [   64/  126]
train() client id: f_00000-4-2 loss: 1.145081  [   96/  126]
train() client id: f_00000-5-0 loss: 1.237242  [   32/  126]
train() client id: f_00000-5-1 loss: 1.064405  [   64/  126]
train() client id: f_00000-5-2 loss: 1.057062  [   96/  126]
train() client id: f_00000-6-0 loss: 1.096903  [   32/  126]
train() client id: f_00000-6-1 loss: 0.998085  [   64/  126]
train() client id: f_00000-6-2 loss: 1.073719  [   96/  126]
train() client id: f_00000-7-0 loss: 1.006185  [   32/  126]
train() client id: f_00000-7-1 loss: 1.009602  [   64/  126]
train() client id: f_00000-7-2 loss: 0.970362  [   96/  126]
train() client id: f_00000-8-0 loss: 0.996782  [   32/  126]
train() client id: f_00000-8-1 loss: 0.961483  [   64/  126]
train() client id: f_00000-8-2 loss: 0.916992  [   96/  126]
train() client id: f_00000-9-0 loss: 0.970787  [   32/  126]
train() client id: f_00000-9-1 loss: 0.874434  [   64/  126]
train() client id: f_00000-9-2 loss: 0.901902  [   96/  126]
train() client id: f_00000-10-0 loss: 0.917605  [   32/  126]
train() client id: f_00000-10-1 loss: 0.959586  [   64/  126]
train() client id: f_00000-10-2 loss: 0.837783  [   96/  126]
train() client id: f_00000-11-0 loss: 0.906050  [   32/  126]
train() client id: f_00000-11-1 loss: 0.871585  [   64/  126]
train() client id: f_00000-11-2 loss: 0.928806  [   96/  126]
train() client id: f_00000-12-0 loss: 0.916806  [   32/  126]
train() client id: f_00000-12-1 loss: 0.915259  [   64/  126]
train() client id: f_00000-12-2 loss: 0.818836  [   96/  126]
train() client id: f_00001-0-0 loss: 0.554325  [   32/  265]
train() client id: f_00001-0-1 loss: 0.584952  [   64/  265]
train() client id: f_00001-0-2 loss: 0.506173  [   96/  265]
train() client id: f_00001-0-3 loss: 0.555213  [  128/  265]
train() client id: f_00001-0-4 loss: 0.482230  [  160/  265]
train() client id: f_00001-0-5 loss: 0.580901  [  192/  265]
train() client id: f_00001-0-6 loss: 0.582106  [  224/  265]
train() client id: f_00001-0-7 loss: 0.591964  [  256/  265]
train() client id: f_00001-1-0 loss: 0.535875  [   32/  265]
train() client id: f_00001-1-1 loss: 0.467572  [   64/  265]
train() client id: f_00001-1-2 loss: 0.610461  [   96/  265]
train() client id: f_00001-1-3 loss: 0.541906  [  128/  265]
train() client id: f_00001-1-4 loss: 0.560039  [  160/  265]
train() client id: f_00001-1-5 loss: 0.502157  [  192/  265]
train() client id: f_00001-1-6 loss: 0.499040  [  224/  265]
train() client id: f_00001-1-7 loss: 0.497660  [  256/  265]
train() client id: f_00001-2-0 loss: 0.487825  [   32/  265]
train() client id: f_00001-2-1 loss: 0.588022  [   64/  265]
train() client id: f_00001-2-2 loss: 0.433698  [   96/  265]
train() client id: f_00001-2-3 loss: 0.438770  [  128/  265]
train() client id: f_00001-2-4 loss: 0.540834  [  160/  265]
train() client id: f_00001-2-5 loss: 0.542258  [  192/  265]
train() client id: f_00001-2-6 loss: 0.567518  [  224/  265]
train() client id: f_00001-2-7 loss: 0.457112  [  256/  265]
train() client id: f_00001-3-0 loss: 0.461645  [   32/  265]
train() client id: f_00001-3-1 loss: 0.428738  [   64/  265]
train() client id: f_00001-3-2 loss: 0.417505  [   96/  265]
train() client id: f_00001-3-3 loss: 0.589798  [  128/  265]
train() client id: f_00001-3-4 loss: 0.518288  [  160/  265]
train() client id: f_00001-3-5 loss: 0.487132  [  192/  265]
train() client id: f_00001-3-6 loss: 0.507590  [  224/  265]
train() client id: f_00001-3-7 loss: 0.531836  [  256/  265]
train() client id: f_00001-4-0 loss: 0.486381  [   32/  265]
train() client id: f_00001-4-1 loss: 0.536156  [   64/  265]
train() client id: f_00001-4-2 loss: 0.519529  [   96/  265]
train() client id: f_00001-4-3 loss: 0.479295  [  128/  265]
train() client id: f_00001-4-4 loss: 0.408034  [  160/  265]
train() client id: f_00001-4-5 loss: 0.458223  [  192/  265]
train() client id: f_00001-4-6 loss: 0.439506  [  224/  265]
train() client id: f_00001-4-7 loss: 0.478223  [  256/  265]
train() client id: f_00001-5-0 loss: 0.502507  [   32/  265]
train() client id: f_00001-5-1 loss: 0.416239  [   64/  265]
train() client id: f_00001-5-2 loss: 0.397168  [   96/  265]
train() client id: f_00001-5-3 loss: 0.478511  [  128/  265]
train() client id: f_00001-5-4 loss: 0.496508  [  160/  265]
train() client id: f_00001-5-5 loss: 0.401646  [  192/  265]
train() client id: f_00001-5-6 loss: 0.562972  [  224/  265]
train() client id: f_00001-5-7 loss: 0.510916  [  256/  265]
train() client id: f_00001-6-0 loss: 0.390115  [   32/  265]
train() client id: f_00001-6-1 loss: 0.439872  [   64/  265]
train() client id: f_00001-6-2 loss: 0.406558  [   96/  265]
train() client id: f_00001-6-3 loss: 0.590724  [  128/  265]
train() client id: f_00001-6-4 loss: 0.527443  [  160/  265]
train() client id: f_00001-6-5 loss: 0.503369  [  192/  265]
train() client id: f_00001-6-6 loss: 0.437879  [  224/  265]
train() client id: f_00001-6-7 loss: 0.423294  [  256/  265]
train() client id: f_00001-7-0 loss: 0.436283  [   32/  265]
train() client id: f_00001-7-1 loss: 0.470889  [   64/  265]
train() client id: f_00001-7-2 loss: 0.546400  [   96/  265]
train() client id: f_00001-7-3 loss: 0.420423  [  128/  265]
train() client id: f_00001-7-4 loss: 0.420765  [  160/  265]
train() client id: f_00001-7-5 loss: 0.397538  [  192/  265]
train() client id: f_00001-7-6 loss: 0.478846  [  224/  265]
train() client id: f_00001-7-7 loss: 0.454612  [  256/  265]
train() client id: f_00001-8-0 loss: 0.529363  [   32/  265]
train() client id: f_00001-8-1 loss: 0.384879  [   64/  265]
train() client id: f_00001-8-2 loss: 0.424815  [   96/  265]
train() client id: f_00001-8-3 loss: 0.363311  [  128/  265]
train() client id: f_00001-8-4 loss: 0.495559  [  160/  265]
train() client id: f_00001-8-5 loss: 0.533432  [  192/  265]
train() client id: f_00001-8-6 loss: 0.431161  [  224/  265]
train() client id: f_00001-8-7 loss: 0.426500  [  256/  265]
train() client id: f_00001-9-0 loss: 0.413805  [   32/  265]
train() client id: f_00001-9-1 loss: 0.424498  [   64/  265]
train() client id: f_00001-9-2 loss: 0.436930  [   96/  265]
train() client id: f_00001-9-3 loss: 0.591493  [  128/  265]
train() client id: f_00001-9-4 loss: 0.508986  [  160/  265]
train() client id: f_00001-9-5 loss: 0.354994  [  192/  265]
train() client id: f_00001-9-6 loss: 0.486197  [  224/  265]
train() client id: f_00001-9-7 loss: 0.391716  [  256/  265]
train() client id: f_00001-10-0 loss: 0.384759  [   32/  265]
train() client id: f_00001-10-1 loss: 0.389342  [   64/  265]
train() client id: f_00001-10-2 loss: 0.501145  [   96/  265]
train() client id: f_00001-10-3 loss: 0.480742  [  128/  265]
train() client id: f_00001-10-4 loss: 0.462332  [  160/  265]
train() client id: f_00001-10-5 loss: 0.493751  [  192/  265]
train() client id: f_00001-10-6 loss: 0.412293  [  224/  265]
train() client id: f_00001-10-7 loss: 0.419207  [  256/  265]
train() client id: f_00001-11-0 loss: 0.461642  [   32/  265]
train() client id: f_00001-11-1 loss: 0.416739  [   64/  265]
train() client id: f_00001-11-2 loss: 0.516061  [   96/  265]
train() client id: f_00001-11-3 loss: 0.509285  [  128/  265]
train() client id: f_00001-11-4 loss: 0.339824  [  160/  265]
train() client id: f_00001-11-5 loss: 0.366768  [  192/  265]
train() client id: f_00001-11-6 loss: 0.436620  [  224/  265]
train() client id: f_00001-11-7 loss: 0.530742  [  256/  265]
train() client id: f_00001-12-0 loss: 0.352056  [   32/  265]
train() client id: f_00001-12-1 loss: 0.487617  [   64/  265]
train() client id: f_00001-12-2 loss: 0.456566  [   96/  265]
train() client id: f_00001-12-3 loss: 0.421419  [  128/  265]
train() client id: f_00001-12-4 loss: 0.496232  [  160/  265]
train() client id: f_00001-12-5 loss: 0.490790  [  192/  265]
train() client id: f_00001-12-6 loss: 0.450038  [  224/  265]
train() client id: f_00001-12-7 loss: 0.341965  [  256/  265]
train() client id: f_00002-0-0 loss: 1.163880  [   32/  124]
train() client id: f_00002-0-1 loss: 1.108013  [   64/  124]
train() client id: f_00002-0-2 loss: 1.123804  [   96/  124]
train() client id: f_00002-1-0 loss: 1.147559  [   32/  124]
train() client id: f_00002-1-1 loss: 1.026879  [   64/  124]
train() client id: f_00002-1-2 loss: 1.180277  [   96/  124]
train() client id: f_00002-2-0 loss: 1.115472  [   32/  124]
train() client id: f_00002-2-1 loss: 1.090717  [   64/  124]
train() client id: f_00002-2-2 loss: 1.054998  [   96/  124]
train() client id: f_00002-3-0 loss: 1.122823  [   32/  124]
train() client id: f_00002-3-1 loss: 1.029230  [   64/  124]
train() client id: f_00002-3-2 loss: 1.048108  [   96/  124]
train() client id: f_00002-4-0 loss: 1.026019  [   32/  124]
train() client id: f_00002-4-1 loss: 1.006217  [   64/  124]
train() client id: f_00002-4-2 loss: 1.001133  [   96/  124]
train() client id: f_00002-5-0 loss: 1.042124  [   32/  124]
train() client id: f_00002-5-1 loss: 0.989235  [   64/  124]
train() client id: f_00002-5-2 loss: 0.977869  [   96/  124]
train() client id: f_00002-6-0 loss: 1.048021  [   32/  124]
train() client id: f_00002-6-1 loss: 0.999201  [   64/  124]
train() client id: f_00002-6-2 loss: 0.929213  [   96/  124]
train() client id: f_00002-7-0 loss: 0.991894  [   32/  124]
train() client id: f_00002-7-1 loss: 0.954777  [   64/  124]
train() client id: f_00002-7-2 loss: 1.031619  [   96/  124]
train() client id: f_00002-8-0 loss: 0.958176  [   32/  124]
train() client id: f_00002-8-1 loss: 0.949631  [   64/  124]
train() client id: f_00002-8-2 loss: 0.992491  [   96/  124]
train() client id: f_00002-9-0 loss: 0.928671  [   32/  124]
train() client id: f_00002-9-1 loss: 0.955292  [   64/  124]
train() client id: f_00002-9-2 loss: 1.023010  [   96/  124]
train() client id: f_00002-10-0 loss: 0.944295  [   32/  124]
train() client id: f_00002-10-1 loss: 1.016194  [   64/  124]
train() client id: f_00002-10-2 loss: 0.964205  [   96/  124]
train() client id: f_00002-11-0 loss: 0.951243  [   32/  124]
train() client id: f_00002-11-1 loss: 0.942466  [   64/  124]
train() client id: f_00002-11-2 loss: 1.012257  [   96/  124]
train() client id: f_00002-12-0 loss: 0.918078  [   32/  124]
train() client id: f_00002-12-1 loss: 0.973497  [   64/  124]
train() client id: f_00002-12-2 loss: 0.979510  [   96/  124]
train() client id: f_00003-0-0 loss: 1.106313  [   32/   43]
train() client id: f_00003-1-0 loss: 1.033526  [   32/   43]
train() client id: f_00003-2-0 loss: 1.027643  [   32/   43]
train() client id: f_00003-3-0 loss: 0.886685  [   32/   43]
train() client id: f_00003-4-0 loss: 1.017565  [   32/   43]
train() client id: f_00003-5-0 loss: 1.119749  [   32/   43]
train() client id: f_00003-6-0 loss: 1.026668  [   32/   43]
train() client id: f_00003-7-0 loss: 0.962810  [   32/   43]
train() client id: f_00003-8-0 loss: 0.917458  [   32/   43]
train() client id: f_00003-9-0 loss: 1.102630  [   32/   43]
train() client id: f_00003-10-0 loss: 1.023967  [   32/   43]
train() client id: f_00003-11-0 loss: 0.883628  [   32/   43]
train() client id: f_00003-12-0 loss: 0.979273  [   32/   43]
train() client id: f_00004-0-0 loss: 0.826291  [   32/  306]
train() client id: f_00004-0-1 loss: 0.777600  [   64/  306]
train() client id: f_00004-0-2 loss: 1.024008  [   96/  306]
train() client id: f_00004-0-3 loss: 1.099272  [  128/  306]
train() client id: f_00004-0-4 loss: 0.932534  [  160/  306]
train() client id: f_00004-0-5 loss: 1.018795  [  192/  306]
train() client id: f_00004-0-6 loss: 0.851082  [  224/  306]
train() client id: f_00004-0-7 loss: 1.027162  [  256/  306]
train() client id: f_00004-0-8 loss: 0.951738  [  288/  306]
train() client id: f_00004-1-0 loss: 0.952079  [   32/  306]
train() client id: f_00004-1-1 loss: 0.918038  [   64/  306]
train() client id: f_00004-1-2 loss: 0.951071  [   96/  306]
train() client id: f_00004-1-3 loss: 0.953987  [  128/  306]
train() client id: f_00004-1-4 loss: 1.006652  [  160/  306]
train() client id: f_00004-1-5 loss: 0.996340  [  192/  306]
train() client id: f_00004-1-6 loss: 0.896360  [  224/  306]
train() client id: f_00004-1-7 loss: 0.982002  [  256/  306]
train() client id: f_00004-1-8 loss: 0.901999  [  288/  306]
train() client id: f_00004-2-0 loss: 0.939706  [   32/  306]
train() client id: f_00004-2-1 loss: 1.041546  [   64/  306]
train() client id: f_00004-2-2 loss: 0.857326  [   96/  306]
train() client id: f_00004-2-3 loss: 0.957746  [  128/  306]
train() client id: f_00004-2-4 loss: 0.948645  [  160/  306]
train() client id: f_00004-2-5 loss: 0.991726  [  192/  306]
train() client id: f_00004-2-6 loss: 0.928484  [  224/  306]
train() client id: f_00004-2-7 loss: 0.910898  [  256/  306]
train() client id: f_00004-2-8 loss: 0.972073  [  288/  306]
train() client id: f_00004-3-0 loss: 0.912093  [   32/  306]
train() client id: f_00004-3-1 loss: 0.851622  [   64/  306]
train() client id: f_00004-3-2 loss: 0.894782  [   96/  306]
train() client id: f_00004-3-3 loss: 1.065116  [  128/  306]
train() client id: f_00004-3-4 loss: 0.964043  [  160/  306]
train() client id: f_00004-3-5 loss: 0.874760  [  192/  306]
train() client id: f_00004-3-6 loss: 0.979303  [  224/  306]
train() client id: f_00004-3-7 loss: 0.942778  [  256/  306]
train() client id: f_00004-3-8 loss: 1.072327  [  288/  306]
train() client id: f_00004-4-0 loss: 0.932409  [   32/  306]
train() client id: f_00004-4-1 loss: 0.973275  [   64/  306]
train() client id: f_00004-4-2 loss: 0.890480  [   96/  306]
train() client id: f_00004-4-3 loss: 0.985143  [  128/  306]
train() client id: f_00004-4-4 loss: 0.935966  [  160/  306]
train() client id: f_00004-4-5 loss: 1.032946  [  192/  306]
train() client id: f_00004-4-6 loss: 0.962221  [  224/  306]
train() client id: f_00004-4-7 loss: 0.968904  [  256/  306]
train() client id: f_00004-4-8 loss: 0.829109  [  288/  306]
train() client id: f_00004-5-0 loss: 0.914053  [   32/  306]
train() client id: f_00004-5-1 loss: 0.987579  [   64/  306]
train() client id: f_00004-5-2 loss: 0.930948  [   96/  306]
train() client id: f_00004-5-3 loss: 0.861963  [  128/  306]
train() client id: f_00004-5-4 loss: 0.962220  [  160/  306]
train() client id: f_00004-5-5 loss: 0.910017  [  192/  306]
train() client id: f_00004-5-6 loss: 1.078982  [  224/  306]
train() client id: f_00004-5-7 loss: 0.943349  [  256/  306]
train() client id: f_00004-5-8 loss: 1.031878  [  288/  306]
train() client id: f_00004-6-0 loss: 0.940987  [   32/  306]
train() client id: f_00004-6-1 loss: 1.029424  [   64/  306]
train() client id: f_00004-6-2 loss: 0.932280  [   96/  306]
train() client id: f_00004-6-3 loss: 0.864521  [  128/  306]
train() client id: f_00004-6-4 loss: 1.033220  [  160/  306]
train() client id: f_00004-6-5 loss: 0.934714  [  192/  306]
train() client id: f_00004-6-6 loss: 0.931086  [  224/  306]
train() client id: f_00004-6-7 loss: 0.914336  [  256/  306]
train() client id: f_00004-6-8 loss: 0.927405  [  288/  306]
train() client id: f_00004-7-0 loss: 0.999230  [   32/  306]
train() client id: f_00004-7-1 loss: 1.024255  [   64/  306]
train() client id: f_00004-7-2 loss: 0.928268  [   96/  306]
train() client id: f_00004-7-3 loss: 0.960411  [  128/  306]
train() client id: f_00004-7-4 loss: 0.987382  [  160/  306]
train() client id: f_00004-7-5 loss: 0.945416  [  192/  306]
train() client id: f_00004-7-6 loss: 0.902265  [  224/  306]
train() client id: f_00004-7-7 loss: 0.985544  [  256/  306]
train() client id: f_00004-7-8 loss: 0.809801  [  288/  306]
train() client id: f_00004-8-0 loss: 1.008831  [   32/  306]
train() client id: f_00004-8-1 loss: 0.954486  [   64/  306]
train() client id: f_00004-8-2 loss: 0.967565  [   96/  306]
train() client id: f_00004-8-3 loss: 1.055241  [  128/  306]
train() client id: f_00004-8-4 loss: 0.889870  [  160/  306]
train() client id: f_00004-8-5 loss: 1.023005  [  192/  306]
train() client id: f_00004-8-6 loss: 0.938421  [  224/  306]
train() client id: f_00004-8-7 loss: 0.851245  [  256/  306]
train() client id: f_00004-8-8 loss: 0.892687  [  288/  306]
train() client id: f_00004-9-0 loss: 0.906996  [   32/  306]
train() client id: f_00004-9-1 loss: 0.985178  [   64/  306]
train() client id: f_00004-9-2 loss: 0.961037  [   96/  306]
train() client id: f_00004-9-3 loss: 0.989221  [  128/  306]
train() client id: f_00004-9-4 loss: 0.917994  [  160/  306]
train() client id: f_00004-9-5 loss: 0.917696  [  192/  306]
train() client id: f_00004-9-6 loss: 1.008199  [  224/  306]
train() client id: f_00004-9-7 loss: 0.973068  [  256/  306]
train() client id: f_00004-9-8 loss: 0.973588  [  288/  306]
train() client id: f_00004-10-0 loss: 0.956346  [   32/  306]
train() client id: f_00004-10-1 loss: 0.897994  [   64/  306]
train() client id: f_00004-10-2 loss: 1.029349  [   96/  306]
train() client id: f_00004-10-3 loss: 0.938406  [  128/  306]
train() client id: f_00004-10-4 loss: 1.085879  [  160/  306]
train() client id: f_00004-10-5 loss: 0.947422  [  192/  306]
train() client id: f_00004-10-6 loss: 0.931120  [  224/  306]
train() client id: f_00004-10-7 loss: 0.823682  [  256/  306]
train() client id: f_00004-10-8 loss: 1.013157  [  288/  306]
train() client id: f_00004-11-0 loss: 1.069223  [   32/  306]
train() client id: f_00004-11-1 loss: 0.966803  [   64/  306]
train() client id: f_00004-11-2 loss: 1.003126  [   96/  306]
train() client id: f_00004-11-3 loss: 0.959877  [  128/  306]
train() client id: f_00004-11-4 loss: 0.914955  [  160/  306]
train() client id: f_00004-11-5 loss: 0.949408  [  192/  306]
train() client id: f_00004-11-6 loss: 0.866508  [  224/  306]
train() client id: f_00004-11-7 loss: 0.917098  [  256/  306]
train() client id: f_00004-11-8 loss: 0.956279  [  288/  306]
train() client id: f_00004-12-0 loss: 0.859854  [   32/  306]
train() client id: f_00004-12-1 loss: 1.009811  [   64/  306]
train() client id: f_00004-12-2 loss: 0.953889  [   96/  306]
train() client id: f_00004-12-3 loss: 0.904822  [  128/  306]
train() client id: f_00004-12-4 loss: 0.965636  [  160/  306]
train() client id: f_00004-12-5 loss: 0.993364  [  192/  306]
train() client id: f_00004-12-6 loss: 0.983744  [  224/  306]
train() client id: f_00004-12-7 loss: 1.029117  [  256/  306]
train() client id: f_00004-12-8 loss: 0.972425  [  288/  306]
train() client id: f_00005-0-0 loss: 0.965240  [   32/  146]
train() client id: f_00005-0-1 loss: 0.911451  [   64/  146]
train() client id: f_00005-0-2 loss: 0.853398  [   96/  146]
train() client id: f_00005-0-3 loss: 0.877536  [  128/  146]
train() client id: f_00005-1-0 loss: 1.016500  [   32/  146]
train() client id: f_00005-1-1 loss: 0.883509  [   64/  146]
train() client id: f_00005-1-2 loss: 0.821798  [   96/  146]
train() client id: f_00005-1-3 loss: 0.799004  [  128/  146]
train() client id: f_00005-2-0 loss: 0.801922  [   32/  146]
train() client id: f_00005-2-1 loss: 0.830324  [   64/  146]
train() client id: f_00005-2-2 loss: 0.867060  [   96/  146]
train() client id: f_00005-2-3 loss: 0.955823  [  128/  146]
train() client id: f_00005-3-0 loss: 0.936324  [   32/  146]
train() client id: f_00005-3-1 loss: 0.857696  [   64/  146]
train() client id: f_00005-3-2 loss: 0.770103  [   96/  146]
train() client id: f_00005-3-3 loss: 0.919552  [  128/  146]
train() client id: f_00005-4-0 loss: 0.937844  [   32/  146]
train() client id: f_00005-4-1 loss: 0.915246  [   64/  146]
train() client id: f_00005-4-2 loss: 0.759368  [   96/  146]
train() client id: f_00005-4-3 loss: 0.851505  [  128/  146]
train() client id: f_00005-5-0 loss: 0.830099  [   32/  146]
train() client id: f_00005-5-1 loss: 0.904130  [   64/  146]
train() client id: f_00005-5-2 loss: 0.896119  [   96/  146]
train() client id: f_00005-5-3 loss: 0.898373  [  128/  146]
train() client id: f_00005-6-0 loss: 0.732229  [   32/  146]
train() client id: f_00005-6-1 loss: 0.960201  [   64/  146]
train() client id: f_00005-6-2 loss: 0.951265  [   96/  146]
train() client id: f_00005-6-3 loss: 0.692943  [  128/  146]
train() client id: f_00005-7-0 loss: 0.735656  [   32/  146]
train() client id: f_00005-7-1 loss: 0.706606  [   64/  146]
train() client id: f_00005-7-2 loss: 0.842939  [   96/  146]
train() client id: f_00005-7-3 loss: 1.008805  [  128/  146]
train() client id: f_00005-8-0 loss: 0.723651  [   32/  146]
train() client id: f_00005-8-1 loss: 0.904095  [   64/  146]
train() client id: f_00005-8-2 loss: 1.112067  [   96/  146]
train() client id: f_00005-8-3 loss: 0.761826  [  128/  146]
train() client id: f_00005-9-0 loss: 0.875207  [   32/  146]
train() client id: f_00005-9-1 loss: 0.608177  [   64/  146]
train() client id: f_00005-9-2 loss: 1.015334  [   96/  146]
train() client id: f_00005-9-3 loss: 0.782963  [  128/  146]
train() client id: f_00005-10-0 loss: 0.755406  [   32/  146]
train() client id: f_00005-10-1 loss: 0.898596  [   64/  146]
train() client id: f_00005-10-2 loss: 0.903523  [   96/  146]
train() client id: f_00005-10-3 loss: 0.779291  [  128/  146]
train() client id: f_00005-11-0 loss: 0.662286  [   32/  146]
train() client id: f_00005-11-1 loss: 0.835170  [   64/  146]
train() client id: f_00005-11-2 loss: 0.981735  [   96/  146]
train() client id: f_00005-11-3 loss: 0.993886  [  128/  146]
train() client id: f_00005-12-0 loss: 0.977306  [   32/  146]
train() client id: f_00005-12-1 loss: 0.708069  [   64/  146]
train() client id: f_00005-12-2 loss: 0.733551  [   96/  146]
train() client id: f_00005-12-3 loss: 1.038885  [  128/  146]
train() client id: f_00006-0-0 loss: 0.797849  [   32/   54]
train() client id: f_00006-1-0 loss: 0.826894  [   32/   54]
train() client id: f_00006-2-0 loss: 0.791276  [   32/   54]
train() client id: f_00006-3-0 loss: 0.818433  [   32/   54]
train() client id: f_00006-4-0 loss: 0.857597  [   32/   54]
train() client id: f_00006-5-0 loss: 0.833329  [   32/   54]
train() client id: f_00006-6-0 loss: 0.822924  [   32/   54]
train() client id: f_00006-7-0 loss: 0.825246  [   32/   54]
train() client id: f_00006-8-0 loss: 0.822039  [   32/   54]
train() client id: f_00006-9-0 loss: 0.789924  [   32/   54]
train() client id: f_00006-10-0 loss: 0.869410  [   32/   54]
train() client id: f_00006-11-0 loss: 0.832024  [   32/   54]
train() client id: f_00006-12-0 loss: 0.855365  [   32/   54]
train() client id: f_00007-0-0 loss: 0.895231  [   32/  179]
train() client id: f_00007-0-1 loss: 0.815885  [   64/  179]
train() client id: f_00007-0-2 loss: 0.841699  [   96/  179]
train() client id: f_00007-0-3 loss: 0.900264  [  128/  179]
train() client id: f_00007-0-4 loss: 0.828182  [  160/  179]
train() client id: f_00007-1-0 loss: 0.839126  [   32/  179]
train() client id: f_00007-1-1 loss: 0.787918  [   64/  179]
train() client id: f_00007-1-2 loss: 0.818306  [   96/  179]
train() client id: f_00007-1-3 loss: 0.866023  [  128/  179]
train() client id: f_00007-1-4 loss: 0.760002  [  160/  179]
train() client id: f_00007-2-0 loss: 0.782176  [   32/  179]
train() client id: f_00007-2-1 loss: 0.775417  [   64/  179]
train() client id: f_00007-2-2 loss: 0.791309  [   96/  179]
train() client id: f_00007-2-3 loss: 0.814622  [  128/  179]
train() client id: f_00007-2-4 loss: 0.902698  [  160/  179]
train() client id: f_00007-3-0 loss: 0.838672  [   32/  179]
train() client id: f_00007-3-1 loss: 0.817727  [   64/  179]
train() client id: f_00007-3-2 loss: 0.678163  [   96/  179]
train() client id: f_00007-3-3 loss: 0.838747  [  128/  179]
train() client id: f_00007-3-4 loss: 0.745167  [  160/  179]
train() client id: f_00007-4-0 loss: 0.724705  [   32/  179]
train() client id: f_00007-4-1 loss: 0.874416  [   64/  179]
train() client id: f_00007-4-2 loss: 0.746202  [   96/  179]
train() client id: f_00007-4-3 loss: 0.780775  [  128/  179]
train() client id: f_00007-4-4 loss: 0.756428  [  160/  179]
train() client id: f_00007-5-0 loss: 0.786841  [   32/  179]
train() client id: f_00007-5-1 loss: 0.676162  [   64/  179]
train() client id: f_00007-5-2 loss: 0.839745  [   96/  179]
train() client id: f_00007-5-3 loss: 0.838017  [  128/  179]
train() client id: f_00007-5-4 loss: 0.752475  [  160/  179]
train() client id: f_00007-6-0 loss: 0.705981  [   32/  179]
train() client id: f_00007-6-1 loss: 0.790402  [   64/  179]
train() client id: f_00007-6-2 loss: 0.844193  [   96/  179]
train() client id: f_00007-6-3 loss: 0.750066  [  128/  179]
train() client id: f_00007-6-4 loss: 0.665981  [  160/  179]
train() client id: f_00007-7-0 loss: 0.737062  [   32/  179]
train() client id: f_00007-7-1 loss: 0.834827  [   64/  179]
train() client id: f_00007-7-2 loss: 0.818779  [   96/  179]
train() client id: f_00007-7-3 loss: 0.805116  [  128/  179]
train() client id: f_00007-7-4 loss: 0.671354  [  160/  179]
train() client id: f_00007-8-0 loss: 0.723514  [   32/  179]
train() client id: f_00007-8-1 loss: 0.752961  [   64/  179]
train() client id: f_00007-8-2 loss: 0.761362  [   96/  179]
train() client id: f_00007-8-3 loss: 0.837403  [  128/  179]
train() client id: f_00007-8-4 loss: 0.751713  [  160/  179]
train() client id: f_00007-9-0 loss: 0.788632  [   32/  179]
train() client id: f_00007-9-1 loss: 0.769191  [   64/  179]
train() client id: f_00007-9-2 loss: 0.794930  [   96/  179]
train() client id: f_00007-9-3 loss: 0.727875  [  128/  179]
train() client id: f_00007-9-4 loss: 0.666224  [  160/  179]
train() client id: f_00007-10-0 loss: 0.813254  [   32/  179]
train() client id: f_00007-10-1 loss: 0.717745  [   64/  179]
train() client id: f_00007-10-2 loss: 0.835101  [   96/  179]
train() client id: f_00007-10-3 loss: 0.670886  [  128/  179]
train() client id: f_00007-10-4 loss: 0.743329  [  160/  179]
train() client id: f_00007-11-0 loss: 0.705153  [   32/  179]
train() client id: f_00007-11-1 loss: 0.798846  [   64/  179]
train() client id: f_00007-11-2 loss: 0.667661  [   96/  179]
train() client id: f_00007-11-3 loss: 0.668924  [  128/  179]
train() client id: f_00007-11-4 loss: 0.931346  [  160/  179]
train() client id: f_00007-12-0 loss: 0.735203  [   32/  179]
train() client id: f_00007-12-1 loss: 0.714780  [   64/  179]
train() client id: f_00007-12-2 loss: 0.659450  [   96/  179]
train() client id: f_00007-12-3 loss: 0.856692  [  128/  179]
train() client id: f_00007-12-4 loss: 0.865382  [  160/  179]
train() client id: f_00008-0-0 loss: 0.858005  [   32/  130]
train() client id: f_00008-0-1 loss: 0.818536  [   64/  130]
train() client id: f_00008-0-2 loss: 0.793643  [   96/  130]
train() client id: f_00008-0-3 loss: 0.901041  [  128/  130]
train() client id: f_00008-1-0 loss: 0.874198  [   32/  130]
train() client id: f_00008-1-1 loss: 0.802800  [   64/  130]
train() client id: f_00008-1-2 loss: 0.818950  [   96/  130]
train() client id: f_00008-1-3 loss: 0.852473  [  128/  130]
train() client id: f_00008-2-0 loss: 0.844720  [   32/  130]
train() client id: f_00008-2-1 loss: 0.765714  [   64/  130]
train() client id: f_00008-2-2 loss: 0.846134  [   96/  130]
train() client id: f_00008-2-3 loss: 0.887231  [  128/  130]
train() client id: f_00008-3-0 loss: 0.925106  [   32/  130]
train() client id: f_00008-3-1 loss: 0.810230  [   64/  130]
train() client id: f_00008-3-2 loss: 0.824819  [   96/  130]
train() client id: f_00008-3-3 loss: 0.803234  [  128/  130]
train() client id: f_00008-4-0 loss: 0.844720  [   32/  130]
train() client id: f_00008-4-1 loss: 0.789096  [   64/  130]
train() client id: f_00008-4-2 loss: 0.852730  [   96/  130]
train() client id: f_00008-4-3 loss: 0.868411  [  128/  130]
train() client id: f_00008-5-0 loss: 0.935455  [   32/  130]
train() client id: f_00008-5-1 loss: 0.792317  [   64/  130]
train() client id: f_00008-5-2 loss: 0.833198  [   96/  130]
train() client id: f_00008-5-3 loss: 0.773026  [  128/  130]
train() client id: f_00008-6-0 loss: 0.819870  [   32/  130]
train() client id: f_00008-6-1 loss: 0.863733  [   64/  130]
train() client id: f_00008-6-2 loss: 0.875973  [   96/  130]
train() client id: f_00008-6-3 loss: 0.778427  [  128/  130]
train() client id: f_00008-7-0 loss: 0.985805  [   32/  130]
train() client id: f_00008-7-1 loss: 0.836981  [   64/  130]
train() client id: f_00008-7-2 loss: 0.737495  [   96/  130]
train() client id: f_00008-7-3 loss: 0.755199  [  128/  130]
train() client id: f_00008-8-0 loss: 0.779619  [   32/  130]
train() client id: f_00008-8-1 loss: 0.813993  [   64/  130]
train() client id: f_00008-8-2 loss: 0.938198  [   96/  130]
train() client id: f_00008-8-3 loss: 0.797393  [  128/  130]
train() client id: f_00008-9-0 loss: 0.859504  [   32/  130]
train() client id: f_00008-9-1 loss: 0.817249  [   64/  130]
train() client id: f_00008-9-2 loss: 0.757238  [   96/  130]
train() client id: f_00008-9-3 loss: 0.883482  [  128/  130]
train() client id: f_00008-10-0 loss: 0.814991  [   32/  130]
train() client id: f_00008-10-1 loss: 0.894443  [   64/  130]
train() client id: f_00008-10-2 loss: 0.798296  [   96/  130]
train() client id: f_00008-10-3 loss: 0.804879  [  128/  130]
train() client id: f_00008-11-0 loss: 0.793407  [   32/  130]
train() client id: f_00008-11-1 loss: 0.833207  [   64/  130]
train() client id: f_00008-11-2 loss: 0.886159  [   96/  130]
train() client id: f_00008-11-3 loss: 0.791887  [  128/  130]
train() client id: f_00008-12-0 loss: 0.821550  [   32/  130]
train() client id: f_00008-12-1 loss: 0.872897  [   64/  130]
train() client id: f_00008-12-2 loss: 0.824454  [   96/  130]
train() client id: f_00008-12-3 loss: 0.825446  [  128/  130]
train() client id: f_00009-0-0 loss: 1.109904  [   32/  118]
train() client id: f_00009-0-1 loss: 1.118626  [   64/  118]
train() client id: f_00009-0-2 loss: 1.214571  [   96/  118]
train() client id: f_00009-1-0 loss: 1.178685  [   32/  118]
train() client id: f_00009-1-1 loss: 1.104774  [   64/  118]
train() client id: f_00009-1-2 loss: 1.079587  [   96/  118]
train() client id: f_00009-2-0 loss: 1.079751  [   32/  118]
train() client id: f_00009-2-1 loss: 1.110874  [   64/  118]
train() client id: f_00009-2-2 loss: 1.089629  [   96/  118]
train() client id: f_00009-3-0 loss: 1.121496  [   32/  118]
train() client id: f_00009-3-1 loss: 1.075375  [   64/  118]
train() client id: f_00009-3-2 loss: 0.941094  [   96/  118]
train() client id: f_00009-4-0 loss: 1.084646  [   32/  118]
train() client id: f_00009-4-1 loss: 0.953619  [   64/  118]
train() client id: f_00009-4-2 loss: 1.036324  [   96/  118]
train() client id: f_00009-5-0 loss: 0.983776  [   32/  118]
train() client id: f_00009-5-1 loss: 0.974963  [   64/  118]
train() client id: f_00009-5-2 loss: 0.984518  [   96/  118]
train() client id: f_00009-6-0 loss: 0.963563  [   32/  118]
train() client id: f_00009-6-1 loss: 0.922068  [   64/  118]
train() client id: f_00009-6-2 loss: 1.008478  [   96/  118]
train() client id: f_00009-7-0 loss: 0.960444  [   32/  118]
train() client id: f_00009-7-1 loss: 0.966799  [   64/  118]
train() client id: f_00009-7-2 loss: 0.897863  [   96/  118]
train() client id: f_00009-8-0 loss: 1.017060  [   32/  118]
train() client id: f_00009-8-1 loss: 0.875538  [   64/  118]
train() client id: f_00009-8-2 loss: 0.895965  [   96/  118]
train() client id: f_00009-9-0 loss: 0.932996  [   32/  118]
train() client id: f_00009-9-1 loss: 0.913385  [   64/  118]
train() client id: f_00009-9-2 loss: 0.912329  [   96/  118]
train() client id: f_00009-10-0 loss: 0.874447  [   32/  118]
train() client id: f_00009-10-1 loss: 0.895872  [   64/  118]
train() client id: f_00009-10-2 loss: 0.879743  [   96/  118]
train() client id: f_00009-11-0 loss: 0.863810  [   32/  118]
train() client id: f_00009-11-1 loss: 0.928774  [   64/  118]
train() client id: f_00009-11-2 loss: 0.905281  [   96/  118]
train() client id: f_00009-12-0 loss: 0.918303  [   32/  118]
train() client id: f_00009-12-1 loss: 0.956870  [   64/  118]
train() client id: f_00009-12-2 loss: 0.798115  [   96/  118]
At round 5 accuracy: 0.6206896551724138
At round 5 training accuracy: 0.574111334674715
At round 5 training loss: 0.8992631252256251
update_location
xs = 8.927491 146.223621 5.882650 10.934260 -62.581990 89.769243 -5.849135 -5.143845 -85.120581 20.134486 
ys = -137.390647 7.291448 35.684448 -117.290817 -9.642386 0.794442 -126.381692 31.628436 25.881276 -572.232496 
xs mean: 12.317620029478787
ys mean: -86.16579882624052
dists_uav = 170.164303 177.297807 106.339012 154.520853 118.361654 134.384330 161.265448 105.008653 133.848249 581.253324 
uav_gains = -105.794736 -106.255775 -100.667354 -104.731399 -101.830452 -103.209750 -105.200212 -100.530659 -103.166297 -126.015659 
uav_gains_db_mean: -105.74022931877306
dists_bs = 362.515156 362.368284 228.314161 346.418058 216.172662 316.938998 345.605179 222.275772 174.111182 772.291312 
bs_gains = -111.228192 -111.223264 -105.605979 -110.675874 -104.941481 -109.594378 -110.647306 -105.280039 -102.310206 -120.424931 
bs_gains_db_mean: -109.1931649320311
Round 6
-------------------------------
ene_coms = [0.00828692 0.00849822 0.00820332 0.00783641 0.0079216  0.00726898
 0.00802903 0.00806275 0.0069699  0.02941528]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.24757419 21.3167475  10.08617937  3.62936408 24.56489283 11.81259381
  4.509984   14.46015164 10.53119312 10.16842399]
obj_prev = 121.32710452107614
eta_min = 6.833582630502772e-10	eta_max = 0.7359712533580488
af = 25.512246160639165	bf = 2.625598079300743	zeta = 28.063470776703085	eta = 0.909090909090909
af = 25.512246160639165	bf = 2.625598079300743	zeta = 55.62892332891798	eta = 0.45861477508368304
af = 25.512246160639165	bf = 2.625598079300743	zeta = 41.58505459736825	eta = 0.6134955552578193
af = 25.512246160639165	bf = 2.625598079300743	zeta = 39.0433516744014	eta = 0.6534338130956661
af = 25.512246160639165	bf = 2.625598079300743	zeta = 38.89997729197728	eta = 0.6558421864657695
af = 25.512246160639165	bf = 2.625598079300743	zeta = 38.899476249162014	eta = 0.6558506340092114
af = 25.512246160639165	bf = 2.625598079300743	zeta = 38.899476243012714	eta = 0.6558506341128894
eta = 0.6558506341128894
ene_coms = [0.00828692 0.00849822 0.00820332 0.00783641 0.0079216  0.00726898
 0.00802903 0.00806275 0.0069699  0.02941528]
ene_comp = [0.03480776 0.07320679 0.03425525 0.01187884 0.08453313 0.0403328
 0.01491761 0.04944912 0.03591277 0.03259774]
ene_total = [3.27168659 6.20292783 3.22339429 1.49675327 7.01903056 3.61385889
 1.74207595 4.36621834 3.25559047 4.70794005]
ti_comp = [0.30826544 0.30615246 0.30910146 0.3127706  0.31191868 0.31844489
 0.3108444  0.31050722 0.3214357  0.09698186]
ti_coms = [0.08286924 0.08498222 0.08203321 0.07836408 0.079216   0.07268979
 0.08029028 0.08062746 0.06969897 0.29415282]
t_total = [29.69438553 29.69438553 29.69438553 29.69438553 29.69438553 29.69438553
 29.69438553 29.69438553 29.69438553 29.69438553]
ene_coms = [0.00828692 0.00849822 0.00820332 0.00783641 0.0079216  0.00726898
 0.00802903 0.00806275 0.0069699  0.02941528]
ene_comp = [2.77369354e-05 2.61612591e-04 2.62941689e-05 1.07090276e-06
 3.88041374e-04 4.04377073e-05 2.14729492e-06 7.83812553e-05
 2.80180782e-05 2.30176792e-04]
ene_total = [0.63123716 0.66503413 0.62478065 0.59501018 0.6308561  0.55492048
 0.60971532 0.61806276 0.53127174 2.25064084]
optimize_network iter = 0 obj = 7.7115293524043675
eta = 0.6558506341128894
freqs = [5.64574438e+07 1.19559373e+08 5.54110177e+07 1.89896971e+07
 1.35505071e+08 6.33277532e+07 2.39953017e+07 7.96263553e+07
 5.58630626e+07 1.68061025e+08]
eta_min = 0.6558506341128975	eta_max = 0.6558506341128674
af = 0.06713695206425405	bf = 2.625598079300743	zeta = 0.07385064727067946	eta = 0.9090909090909091
af = 0.06713695206425405	bf = 2.625598079300743	zeta = 28.951966057008505	eta = 0.0023189082196371936
af = 0.06713695206425405	bf = 2.625598079300743	zeta = 3.0399444123679933	eta = 0.022084927537197
af = 0.06713695206425405	bf = 2.625598079300743	zeta = 2.946658614624388	eta = 0.022784095765641322
af = 0.06713695206425405	bf = 2.625598079300743	zeta = 2.9466255897404965	eta = 0.02278435112286073
eta = 0.02278435112286073
ene_coms = [0.00828692 0.00849822 0.00820332 0.00783641 0.0079216  0.00726898
 0.00802903 0.00806275 0.0069699  0.02941528]
ene_comp = [2.48664601e-04 2.34538494e-03 2.35730044e-04 9.60075812e-06
 3.47883254e-03 3.62528383e-04 1.92507293e-05 7.02696360e-04
 2.51185076e-04 2.06355962e-03]
ene_total = [0.22821177 0.28992012 0.22563071 0.20977482 0.30480768 0.20403981
 0.21518281 0.23435727 0.19306648 0.8416341 ]
ti_comp = [0.30826544 0.30615246 0.30910146 0.3127706  0.31191868 0.31844489
 0.3108444  0.31050722 0.3214357  0.09698186]
ti_coms = [0.08286924 0.08498222 0.08203321 0.07836408 0.079216   0.07268979
 0.08029028 0.08062746 0.06969897 0.29415282]
t_total = [29.69438553 29.69438553 29.69438553 29.69438553 29.69438553 29.69438553
 29.69438553 29.69438553 29.69438553 29.69438553]
ene_coms = [0.00828692 0.00849822 0.00820332 0.00783641 0.0079216  0.00726898
 0.00802903 0.00806275 0.0069699  0.02941528]
ene_comp = [2.77369354e-05 2.61612591e-04 2.62941689e-05 1.07090276e-06
 3.88041374e-04 4.04377073e-05 2.14729492e-06 7.83812553e-05
 2.80180782e-05 2.30176792e-04]
ene_total = [0.63123716 0.66503413 0.62478065 0.59501018 0.6308561  0.55492048
 0.60971532 0.61806276 0.53127174 2.25064084]
optimize_network iter = 1 obj = 7.71152935240455
eta = 0.6558506341128975
freqs = [5.64574438e+07 1.19559373e+08 5.54110177e+07 1.89896971e+07
 1.35505071e+08 6.33277532e+07 2.39953017e+07 7.96263553e+07
 5.58630626e+07 1.68061025e+08]
Done!
ene_coms = [0.00828692 0.00849822 0.00820332 0.00783641 0.0079216  0.00726898
 0.00802903 0.00806275 0.0069699  0.02941528]
ene_comp = [2.61051578e-05 2.46221793e-04 2.47472699e-05 1.00790103e-06
 3.65212709e-04 3.80587371e-05 2.02096850e-06 7.37700474e-05
 2.63697609e-05 2.16635378e-04]
ene_total = [0.00831303 0.00874444 0.00822807 0.00783742 0.00828681 0.00730704
 0.00803105 0.00813652 0.00699627 0.02963192]
At round 6 energy consumption: 0.10151255759382813
At round 6 eta: 0.6558506341128975
At round 6 a_n: 26.12732777443847
At round 6 local rounds: 13.812602199319887
At round 6 global rounds: 75.91857014494539
gradient difference: 0.461765855550766
train() client id: f_00000-0-0 loss: 1.409623  [   32/  126]
train() client id: f_00000-0-1 loss: 1.336974  [   64/  126]
train() client id: f_00000-0-2 loss: 1.258959  [   96/  126]
train() client id: f_00000-1-0 loss: 1.141422  [   32/  126]
train() client id: f_00000-1-1 loss: 1.179945  [   64/  126]
train() client id: f_00000-1-2 loss: 1.157996  [   96/  126]
train() client id: f_00000-2-0 loss: 1.166627  [   32/  126]
train() client id: f_00000-2-1 loss: 1.090420  [   64/  126]
train() client id: f_00000-2-2 loss: 1.130977  [   96/  126]
train() client id: f_00000-3-0 loss: 1.122111  [   32/  126]
train() client id: f_00000-3-1 loss: 1.128002  [   64/  126]
train() client id: f_00000-3-2 loss: 1.052540  [   96/  126]
train() client id: f_00000-4-0 loss: 1.046818  [   32/  126]
train() client id: f_00000-4-1 loss: 1.049181  [   64/  126]
train() client id: f_00000-4-2 loss: 1.041334  [   96/  126]
train() client id: f_00000-5-0 loss: 1.104500  [   32/  126]
train() client id: f_00000-5-1 loss: 0.998677  [   64/  126]
train() client id: f_00000-5-2 loss: 0.942201  [   96/  126]
train() client id: f_00000-6-0 loss: 0.948729  [   32/  126]
train() client id: f_00000-6-1 loss: 0.960870  [   64/  126]
train() client id: f_00000-6-2 loss: 0.991770  [   96/  126]
train() client id: f_00000-7-0 loss: 0.966624  [   32/  126]
train() client id: f_00000-7-1 loss: 0.935776  [   64/  126]
train() client id: f_00000-7-2 loss: 1.032142  [   96/  126]
train() client id: f_00000-8-0 loss: 0.955437  [   32/  126]
train() client id: f_00000-8-1 loss: 0.978758  [   64/  126]
train() client id: f_00000-8-2 loss: 0.930463  [   96/  126]
train() client id: f_00000-9-0 loss: 0.984177  [   32/  126]
train() client id: f_00000-9-1 loss: 0.964089  [   64/  126]
train() client id: f_00000-9-2 loss: 0.980663  [   96/  126]
train() client id: f_00000-10-0 loss: 0.953972  [   32/  126]
train() client id: f_00000-10-1 loss: 0.975020  [   64/  126]
train() client id: f_00000-10-2 loss: 0.946148  [   96/  126]
train() client id: f_00000-11-0 loss: 0.975836  [   32/  126]
train() client id: f_00000-11-1 loss: 0.984962  [   64/  126]
train() client id: f_00000-11-2 loss: 0.923959  [   96/  126]
train() client id: f_00000-12-0 loss: 0.979958  [   32/  126]
train() client id: f_00000-12-1 loss: 0.955263  [   64/  126]
train() client id: f_00000-12-2 loss: 1.021018  [   96/  126]
train() client id: f_00001-0-0 loss: 0.556569  [   32/  265]
train() client id: f_00001-0-1 loss: 0.512504  [   64/  265]
train() client id: f_00001-0-2 loss: 0.578688  [   96/  265]
train() client id: f_00001-0-3 loss: 0.728021  [  128/  265]
train() client id: f_00001-0-4 loss: 0.664874  [  160/  265]
train() client id: f_00001-0-5 loss: 0.593133  [  192/  265]
train() client id: f_00001-0-6 loss: 0.606679  [  224/  265]
train() client id: f_00001-0-7 loss: 0.575568  [  256/  265]
train() client id: f_00001-1-0 loss: 0.566718  [   32/  265]
train() client id: f_00001-1-1 loss: 0.649203  [   64/  265]
train() client id: f_00001-1-2 loss: 0.487690  [   96/  265]
train() client id: f_00001-1-3 loss: 0.568219  [  128/  265]
train() client id: f_00001-1-4 loss: 0.588933  [  160/  265]
train() client id: f_00001-1-5 loss: 0.585871  [  192/  265]
train() client id: f_00001-1-6 loss: 0.557966  [  224/  265]
train() client id: f_00001-1-7 loss: 0.554182  [  256/  265]
train() client id: f_00001-2-0 loss: 0.585774  [   32/  265]
train() client id: f_00001-2-1 loss: 0.565646  [   64/  265]
train() client id: f_00001-2-2 loss: 0.553612  [   96/  265]
train() client id: f_00001-2-3 loss: 0.568070  [  128/  265]
train() client id: f_00001-2-4 loss: 0.546570  [  160/  265]
train() client id: f_00001-2-5 loss: 0.616680  [  192/  265]
train() client id: f_00001-2-6 loss: 0.507690  [  224/  265]
train() client id: f_00001-2-7 loss: 0.598127  [  256/  265]
train() client id: f_00001-3-0 loss: 0.571880  [   32/  265]
train() client id: f_00001-3-1 loss: 0.479210  [   64/  265]
train() client id: f_00001-3-2 loss: 0.624708  [   96/  265]
train() client id: f_00001-3-3 loss: 0.510105  [  128/  265]
train() client id: f_00001-3-4 loss: 0.468697  [  160/  265]
train() client id: f_00001-3-5 loss: 0.592095  [  192/  265]
train() client id: f_00001-3-6 loss: 0.599238  [  224/  265]
train() client id: f_00001-3-7 loss: 0.613247  [  256/  265]
train() client id: f_00001-4-0 loss: 0.507458  [   32/  265]
train() client id: f_00001-4-1 loss: 0.509088  [   64/  265]
train() client id: f_00001-4-2 loss: 0.641017  [   96/  265]
train() client id: f_00001-4-3 loss: 0.519981  [  128/  265]
train() client id: f_00001-4-4 loss: 0.467705  [  160/  265]
train() client id: f_00001-4-5 loss: 0.545912  [  192/  265]
train() client id: f_00001-4-6 loss: 0.588568  [  224/  265]
train() client id: f_00001-4-7 loss: 0.512686  [  256/  265]
train() client id: f_00001-5-0 loss: 0.543883  [   32/  265]
train() client id: f_00001-5-1 loss: 0.595426  [   64/  265]
train() client id: f_00001-5-2 loss: 0.631262  [   96/  265]
train() client id: f_00001-5-3 loss: 0.470438  [  128/  265]
train() client id: f_00001-5-4 loss: 0.474421  [  160/  265]
train() client id: f_00001-5-5 loss: 0.617480  [  192/  265]
train() client id: f_00001-5-6 loss: 0.471984  [  224/  265]
train() client id: f_00001-5-7 loss: 0.569540  [  256/  265]
train() client id: f_00001-6-0 loss: 0.466535  [   32/  265]
train() client id: f_00001-6-1 loss: 0.572044  [   64/  265]
train() client id: f_00001-6-2 loss: 0.515601  [   96/  265]
train() client id: f_00001-6-3 loss: 0.441339  [  128/  265]
train() client id: f_00001-6-4 loss: 0.596346  [  160/  265]
train() client id: f_00001-6-5 loss: 0.611746  [  192/  265]
train() client id: f_00001-6-6 loss: 0.568844  [  224/  265]
train() client id: f_00001-6-7 loss: 0.560526  [  256/  265]
train() client id: f_00001-7-0 loss: 0.543619  [   32/  265]
train() client id: f_00001-7-1 loss: 0.502659  [   64/  265]
train() client id: f_00001-7-2 loss: 0.449025  [   96/  265]
train() client id: f_00001-7-3 loss: 0.612136  [  128/  265]
train() client id: f_00001-7-4 loss: 0.448701  [  160/  265]
train() client id: f_00001-7-5 loss: 0.617354  [  192/  265]
train() client id: f_00001-7-6 loss: 0.580924  [  224/  265]
train() client id: f_00001-7-7 loss: 0.562525  [  256/  265]
train() client id: f_00001-8-0 loss: 0.528985  [   32/  265]
train() client id: f_00001-8-1 loss: 0.514094  [   64/  265]
train() client id: f_00001-8-2 loss: 0.520067  [   96/  265]
train() client id: f_00001-8-3 loss: 0.553992  [  128/  265]
train() client id: f_00001-8-4 loss: 0.550549  [  160/  265]
train() client id: f_00001-8-5 loss: 0.462726  [  192/  265]
train() client id: f_00001-8-6 loss: 0.498393  [  224/  265]
train() client id: f_00001-8-7 loss: 0.545589  [  256/  265]
train() client id: f_00001-9-0 loss: 0.565052  [   32/  265]
train() client id: f_00001-9-1 loss: 0.446404  [   64/  265]
train() client id: f_00001-9-2 loss: 0.564448  [   96/  265]
train() client id: f_00001-9-3 loss: 0.462622  [  128/  265]
train() client id: f_00001-9-4 loss: 0.601002  [  160/  265]
train() client id: f_00001-9-5 loss: 0.567780  [  192/  265]
train() client id: f_00001-9-6 loss: 0.579119  [  224/  265]
train() client id: f_00001-9-7 loss: 0.514383  [  256/  265]
train() client id: f_00001-10-0 loss: 0.450061  [   32/  265]
train() client id: f_00001-10-1 loss: 0.622628  [   64/  265]
train() client id: f_00001-10-2 loss: 0.450728  [   96/  265]
train() client id: f_00001-10-3 loss: 0.642220  [  128/  265]
train() client id: f_00001-10-4 loss: 0.596583  [  160/  265]
train() client id: f_00001-10-5 loss: 0.467357  [  192/  265]
train() client id: f_00001-10-6 loss: 0.437334  [  224/  265]
train() client id: f_00001-10-7 loss: 0.521835  [  256/  265]
train() client id: f_00001-11-0 loss: 0.630314  [   32/  265]
train() client id: f_00001-11-1 loss: 0.443833  [   64/  265]
train() client id: f_00001-11-2 loss: 0.620016  [   96/  265]
train() client id: f_00001-11-3 loss: 0.435394  [  128/  265]
train() client id: f_00001-11-4 loss: 0.568015  [  160/  265]
train() client id: f_00001-11-5 loss: 0.569767  [  192/  265]
train() client id: f_00001-11-6 loss: 0.447238  [  224/  265]
train() client id: f_00001-11-7 loss: 0.504315  [  256/  265]
train() client id: f_00001-12-0 loss: 0.518636  [   32/  265]
train() client id: f_00001-12-1 loss: 0.596808  [   64/  265]
train() client id: f_00001-12-2 loss: 0.524885  [   96/  265]
train() client id: f_00001-12-3 loss: 0.429815  [  128/  265]
train() client id: f_00001-12-4 loss: 0.536111  [  160/  265]
train() client id: f_00001-12-5 loss: 0.566495  [  192/  265]
train() client id: f_00001-12-6 loss: 0.483688  [  224/  265]
train() client id: f_00001-12-7 loss: 0.632550  [  256/  265]
train() client id: f_00002-0-0 loss: 1.180069  [   32/  124]
train() client id: f_00002-0-1 loss: 1.150012  [   64/  124]
train() client id: f_00002-0-2 loss: 1.228472  [   96/  124]
train() client id: f_00002-1-0 loss: 1.137165  [   32/  124]
train() client id: f_00002-1-1 loss: 1.150379  [   64/  124]
train() client id: f_00002-1-2 loss: 1.168779  [   96/  124]
train() client id: f_00002-2-0 loss: 1.159886  [   32/  124]
train() client id: f_00002-2-1 loss: 1.073171  [   64/  124]
train() client id: f_00002-2-2 loss: 1.108848  [   96/  124]
train() client id: f_00002-3-0 loss: 1.058332  [   32/  124]
train() client id: f_00002-3-1 loss: 1.075209  [   64/  124]
train() client id: f_00002-3-2 loss: 1.096715  [   96/  124]
train() client id: f_00002-4-0 loss: 1.015145  [   32/  124]
train() client id: f_00002-4-1 loss: 1.073060  [   64/  124]
train() client id: f_00002-4-2 loss: 0.982588  [   96/  124]
train() client id: f_00002-5-0 loss: 0.976542  [   32/  124]
train() client id: f_00002-5-1 loss: 1.071980  [   64/  124]
train() client id: f_00002-5-2 loss: 1.011884  [   96/  124]
train() client id: f_00002-6-0 loss: 1.051535  [   32/  124]
train() client id: f_00002-6-1 loss: 0.987951  [   64/  124]
train() client id: f_00002-6-2 loss: 0.994663  [   96/  124]
train() client id: f_00002-7-0 loss: 0.988355  [   32/  124]
train() client id: f_00002-7-1 loss: 1.017231  [   64/  124]
train() client id: f_00002-7-2 loss: 1.055676  [   96/  124]
train() client id: f_00002-8-0 loss: 0.990716  [   32/  124]
train() client id: f_00002-8-1 loss: 0.921544  [   64/  124]
train() client id: f_00002-8-2 loss: 1.028101  [   96/  124]
train() client id: f_00002-9-0 loss: 0.974668  [   32/  124]
train() client id: f_00002-9-1 loss: 0.950659  [   64/  124]
train() client id: f_00002-9-2 loss: 1.028130  [   96/  124]
train() client id: f_00002-10-0 loss: 1.040281  [   32/  124]
train() client id: f_00002-10-1 loss: 1.003843  [   64/  124]
train() client id: f_00002-10-2 loss: 0.866162  [   96/  124]
train() client id: f_00002-11-0 loss: 0.906007  [   32/  124]
train() client id: f_00002-11-1 loss: 0.941528  [   64/  124]
train() client id: f_00002-11-2 loss: 1.096498  [   96/  124]
train() client id: f_00002-12-0 loss: 0.900023  [   32/  124]
train() client id: f_00002-12-1 loss: 1.036744  [   64/  124]
train() client id: f_00002-12-2 loss: 1.001258  [   96/  124]
train() client id: f_00003-0-0 loss: 0.850874  [   32/   43]
train() client id: f_00003-1-0 loss: 0.821716  [   32/   43]
train() client id: f_00003-2-0 loss: 0.953070  [   32/   43]
train() client id: f_00003-3-0 loss: 0.803196  [   32/   43]
train() client id: f_00003-4-0 loss: 0.933926  [   32/   43]
train() client id: f_00003-5-0 loss: 0.887407  [   32/   43]
train() client id: f_00003-6-0 loss: 1.024158  [   32/   43]
train() client id: f_00003-7-0 loss: 0.818892  [   32/   43]
train() client id: f_00003-8-0 loss: 0.930624  [   32/   43]
train() client id: f_00003-9-0 loss: 0.877310  [   32/   43]
train() client id: f_00003-10-0 loss: 0.833921  [   32/   43]
train() client id: f_00003-11-0 loss: 0.875813  [   32/   43]
train() client id: f_00003-12-0 loss: 0.881337  [   32/   43]
train() client id: f_00004-0-0 loss: 0.879745  [   32/  306]
train() client id: f_00004-0-1 loss: 1.022842  [   64/  306]
train() client id: f_00004-0-2 loss: 0.949203  [   96/  306]
train() client id: f_00004-0-3 loss: 1.001697  [  128/  306]
train() client id: f_00004-0-4 loss: 0.861119  [  160/  306]
train() client id: f_00004-0-5 loss: 1.003042  [  192/  306]
train() client id: f_00004-0-6 loss: 0.829252  [  224/  306]
train() client id: f_00004-0-7 loss: 0.854762  [  256/  306]
train() client id: f_00004-0-8 loss: 0.965402  [  288/  306]
train() client id: f_00004-1-0 loss: 0.922244  [   32/  306]
train() client id: f_00004-1-1 loss: 1.020288  [   64/  306]
train() client id: f_00004-1-2 loss: 0.962946  [   96/  306]
train() client id: f_00004-1-3 loss: 0.945428  [  128/  306]
train() client id: f_00004-1-4 loss: 0.836215  [  160/  306]
train() client id: f_00004-1-5 loss: 0.914241  [  192/  306]
train() client id: f_00004-1-6 loss: 0.882777  [  224/  306]
train() client id: f_00004-1-7 loss: 0.943016  [  256/  306]
train() client id: f_00004-1-8 loss: 0.930170  [  288/  306]
train() client id: f_00004-2-0 loss: 0.976797  [   32/  306]
train() client id: f_00004-2-1 loss: 0.880118  [   64/  306]
train() client id: f_00004-2-2 loss: 0.977950  [   96/  306]
train() client id: f_00004-2-3 loss: 0.919336  [  128/  306]
train() client id: f_00004-2-4 loss: 0.903664  [  160/  306]
train() client id: f_00004-2-5 loss: 0.807173  [  192/  306]
train() client id: f_00004-2-6 loss: 0.871764  [  224/  306]
train() client id: f_00004-2-7 loss: 0.994567  [  256/  306]
train() client id: f_00004-2-8 loss: 1.049338  [  288/  306]
train() client id: f_00004-3-0 loss: 0.712073  [   32/  306]
train() client id: f_00004-3-1 loss: 0.961737  [   64/  306]
train() client id: f_00004-3-2 loss: 0.927283  [   96/  306]
train() client id: f_00004-3-3 loss: 1.004582  [  128/  306]
train() client id: f_00004-3-4 loss: 1.035399  [  160/  306]
train() client id: f_00004-3-5 loss: 0.835791  [  192/  306]
train() client id: f_00004-3-6 loss: 0.907660  [  224/  306]
train() client id: f_00004-3-7 loss: 0.901805  [  256/  306]
train() client id: f_00004-3-8 loss: 0.962594  [  288/  306]
train() client id: f_00004-4-0 loss: 0.903087  [   32/  306]
train() client id: f_00004-4-1 loss: 0.953302  [   64/  306]
train() client id: f_00004-4-2 loss: 0.999750  [   96/  306]
train() client id: f_00004-4-3 loss: 0.855933  [  128/  306]
train() client id: f_00004-4-4 loss: 0.903840  [  160/  306]
train() client id: f_00004-4-5 loss: 0.880063  [  192/  306]
train() client id: f_00004-4-6 loss: 0.851433  [  224/  306]
train() client id: f_00004-4-7 loss: 1.009732  [  256/  306]
train() client id: f_00004-4-8 loss: 0.956031  [  288/  306]
train() client id: f_00004-5-0 loss: 1.005795  [   32/  306]
train() client id: f_00004-5-1 loss: 0.976516  [   64/  306]
train() client id: f_00004-5-2 loss: 0.850242  [   96/  306]
train() client id: f_00004-5-3 loss: 0.959314  [  128/  306]
train() client id: f_00004-5-4 loss: 0.968661  [  160/  306]
train() client id: f_00004-5-5 loss: 0.927667  [  192/  306]
train() client id: f_00004-5-6 loss: 0.859825  [  224/  306]
train() client id: f_00004-5-7 loss: 0.941245  [  256/  306]
train() client id: f_00004-5-8 loss: 0.867864  [  288/  306]
train() client id: f_00004-6-0 loss: 0.785603  [   32/  306]
train() client id: f_00004-6-1 loss: 1.048683  [   64/  306]
train() client id: f_00004-6-2 loss: 0.891193  [   96/  306]
train() client id: f_00004-6-3 loss: 0.824548  [  128/  306]
train() client id: f_00004-6-4 loss: 1.009061  [  160/  306]
train() client id: f_00004-6-5 loss: 0.879603  [  192/  306]
train() client id: f_00004-6-6 loss: 0.997535  [  224/  306]
train() client id: f_00004-6-7 loss: 0.918074  [  256/  306]
train() client id: f_00004-6-8 loss: 0.915627  [  288/  306]
train() client id: f_00004-7-0 loss: 1.000397  [   32/  306]
train() client id: f_00004-7-1 loss: 0.937308  [   64/  306]
train() client id: f_00004-7-2 loss: 0.875479  [   96/  306]
train() client id: f_00004-7-3 loss: 0.987758  [  128/  306]
train() client id: f_00004-7-4 loss: 0.865645  [  160/  306]
train() client id: f_00004-7-5 loss: 1.026096  [  192/  306]
train() client id: f_00004-7-6 loss: 0.818303  [  224/  306]
train() client id: f_00004-7-7 loss: 1.005160  [  256/  306]
train() client id: f_00004-7-8 loss: 0.836474  [  288/  306]
train() client id: f_00004-8-0 loss: 0.931223  [   32/  306]
train() client id: f_00004-8-1 loss: 0.955540  [   64/  306]
train() client id: f_00004-8-2 loss: 0.931113  [   96/  306]
train() client id: f_00004-8-3 loss: 0.933954  [  128/  306]
train() client id: f_00004-8-4 loss: 0.890538  [  160/  306]
train() client id: f_00004-8-5 loss: 0.946624  [  192/  306]
train() client id: f_00004-8-6 loss: 0.846364  [  224/  306]
train() client id: f_00004-8-7 loss: 0.889604  [  256/  306]
train() client id: f_00004-8-8 loss: 0.915206  [  288/  306]
train() client id: f_00004-9-0 loss: 0.788285  [   32/  306]
train() client id: f_00004-9-1 loss: 0.986009  [   64/  306]
train() client id: f_00004-9-2 loss: 0.959078  [   96/  306]
train() client id: f_00004-9-3 loss: 0.904766  [  128/  306]
train() client id: f_00004-9-4 loss: 0.931122  [  160/  306]
train() client id: f_00004-9-5 loss: 0.884301  [  192/  306]
train() client id: f_00004-9-6 loss: 0.874236  [  224/  306]
train() client id: f_00004-9-7 loss: 0.956683  [  256/  306]
train() client id: f_00004-9-8 loss: 0.960377  [  288/  306]
train() client id: f_00004-10-0 loss: 0.961364  [   32/  306]
train() client id: f_00004-10-1 loss: 0.903368  [   64/  306]
train() client id: f_00004-10-2 loss: 0.907669  [   96/  306]
train() client id: f_00004-10-3 loss: 0.919190  [  128/  306]
train() client id: f_00004-10-4 loss: 0.776268  [  160/  306]
train() client id: f_00004-10-5 loss: 0.944780  [  192/  306]
train() client id: f_00004-10-6 loss: 0.890581  [  224/  306]
train() client id: f_00004-10-7 loss: 1.070895  [  256/  306]
train() client id: f_00004-10-8 loss: 0.904214  [  288/  306]
train() client id: f_00004-11-0 loss: 1.024279  [   32/  306]
train() client id: f_00004-11-1 loss: 0.999018  [   64/  306]
train() client id: f_00004-11-2 loss: 0.961857  [   96/  306]
train() client id: f_00004-11-3 loss: 0.875478  [  128/  306]
train() client id: f_00004-11-4 loss: 0.900654  [  160/  306]
train() client id: f_00004-11-5 loss: 0.840990  [  192/  306]
train() client id: f_00004-11-6 loss: 0.907663  [  224/  306]
train() client id: f_00004-11-7 loss: 0.966418  [  256/  306]
train() client id: f_00004-11-8 loss: 0.925791  [  288/  306]
train() client id: f_00004-12-0 loss: 0.982088  [   32/  306]
train() client id: f_00004-12-1 loss: 0.941792  [   64/  306]
train() client id: f_00004-12-2 loss: 0.774785  [   96/  306]
train() client id: f_00004-12-3 loss: 0.903114  [  128/  306]
train() client id: f_00004-12-4 loss: 0.986226  [  160/  306]
train() client id: f_00004-12-5 loss: 0.982530  [  192/  306]
train() client id: f_00004-12-6 loss: 0.941273  [  224/  306]
train() client id: f_00004-12-7 loss: 0.913465  [  256/  306]
train() client id: f_00004-12-8 loss: 0.898499  [  288/  306]
train() client id: f_00005-0-0 loss: 0.730903  [   32/  146]
train() client id: f_00005-0-1 loss: 0.688853  [   64/  146]
train() client id: f_00005-0-2 loss: 0.743355  [   96/  146]
train() client id: f_00005-0-3 loss: 0.758447  [  128/  146]
train() client id: f_00005-1-0 loss: 0.748788  [   32/  146]
train() client id: f_00005-1-1 loss: 0.620049  [   64/  146]
train() client id: f_00005-1-2 loss: 0.752043  [   96/  146]
train() client id: f_00005-1-3 loss: 0.577158  [  128/  146]
train() client id: f_00005-2-0 loss: 0.694510  [   32/  146]
train() client id: f_00005-2-1 loss: 0.708540  [   64/  146]
train() client id: f_00005-2-2 loss: 0.506301  [   96/  146]
train() client id: f_00005-2-3 loss: 0.833844  [  128/  146]
train() client id: f_00005-3-0 loss: 0.818013  [   32/  146]
train() client id: f_00005-3-1 loss: 0.654790  [   64/  146]
train() client id: f_00005-3-2 loss: 0.665618  [   96/  146]
train() client id: f_00005-3-3 loss: 0.639508  [  128/  146]
train() client id: f_00005-4-0 loss: 0.734296  [   32/  146]
train() client id: f_00005-4-1 loss: 0.640542  [   64/  146]
train() client id: f_00005-4-2 loss: 0.538145  [   96/  146]
train() client id: f_00005-4-3 loss: 0.674393  [  128/  146]
train() client id: f_00005-5-0 loss: 0.598001  [   32/  146]
train() client id: f_00005-5-1 loss: 0.509331  [   64/  146]
train() client id: f_00005-5-2 loss: 0.769251  [   96/  146]
train() client id: f_00005-5-3 loss: 0.639651  [  128/  146]
train() client id: f_00005-6-0 loss: 0.733365  [   32/  146]
train() client id: f_00005-6-1 loss: 0.585431  [   64/  146]
train() client id: f_00005-6-2 loss: 0.718684  [   96/  146]
train() client id: f_00005-6-3 loss: 0.594301  [  128/  146]
train() client id: f_00005-7-0 loss: 0.649422  [   32/  146]
train() client id: f_00005-7-1 loss: 0.549666  [   64/  146]
train() client id: f_00005-7-2 loss: 0.665413  [   96/  146]
train() client id: f_00005-7-3 loss: 0.581241  [  128/  146]
train() client id: f_00005-8-0 loss: 0.633767  [   32/  146]
train() client id: f_00005-8-1 loss: 0.481018  [   64/  146]
train() client id: f_00005-8-2 loss: 0.789539  [   96/  146]
train() client id: f_00005-8-3 loss: 0.535668  [  128/  146]
train() client id: f_00005-9-0 loss: 0.574295  [   32/  146]
train() client id: f_00005-9-1 loss: 0.946239  [   64/  146]
train() client id: f_00005-9-2 loss: 0.474641  [   96/  146]
train() client id: f_00005-9-3 loss: 0.560793  [  128/  146]
train() client id: f_00005-10-0 loss: 0.531856  [   32/  146]
train() client id: f_00005-10-1 loss: 0.505268  [   64/  146]
train() client id: f_00005-10-2 loss: 0.690133  [   96/  146]
train() client id: f_00005-10-3 loss: 0.647606  [  128/  146]
train() client id: f_00005-11-0 loss: 0.736570  [   32/  146]
train() client id: f_00005-11-1 loss: 0.619055  [   64/  146]
train() client id: f_00005-11-2 loss: 0.482055  [   96/  146]
train() client id: f_00005-11-3 loss: 0.606566  [  128/  146]
train() client id: f_00005-12-0 loss: 0.644196  [   32/  146]
train() client id: f_00005-12-1 loss: 0.685852  [   64/  146]
train() client id: f_00005-12-2 loss: 0.565859  [   96/  146]
train() client id: f_00005-12-3 loss: 0.540361  [  128/  146]
train() client id: f_00006-0-0 loss: 0.784208  [   32/   54]
train() client id: f_00006-1-0 loss: 0.728095  [   32/   54]
train() client id: f_00006-2-0 loss: 0.801135  [   32/   54]
train() client id: f_00006-3-0 loss: 0.778205  [   32/   54]
train() client id: f_00006-4-0 loss: 0.773060  [   32/   54]
train() client id: f_00006-5-0 loss: 0.798997  [   32/   54]
train() client id: f_00006-6-0 loss: 0.785819  [   32/   54]
train() client id: f_00006-7-0 loss: 0.767574  [   32/   54]
train() client id: f_00006-8-0 loss: 0.762582  [   32/   54]
train() client id: f_00006-9-0 loss: 0.755648  [   32/   54]
train() client id: f_00006-10-0 loss: 0.747938  [   32/   54]
train() client id: f_00006-11-0 loss: 0.725962  [   32/   54]
train() client id: f_00006-12-0 loss: 0.759416  [   32/   54]
train() client id: f_00007-0-0 loss: 0.642311  [   32/  179]
train() client id: f_00007-0-1 loss: 0.743942  [   64/  179]
train() client id: f_00007-0-2 loss: 0.733099  [   96/  179]
train() client id: f_00007-0-3 loss: 0.692056  [  128/  179]
train() client id: f_00007-0-4 loss: 0.585455  [  160/  179]
train() client id: f_00007-1-0 loss: 0.622684  [   32/  179]
train() client id: f_00007-1-1 loss: 0.656253  [   64/  179]
train() client id: f_00007-1-2 loss: 0.735558  [   96/  179]
train() client id: f_00007-1-3 loss: 0.644229  [  128/  179]
train() client id: f_00007-1-4 loss: 0.593285  [  160/  179]
train() client id: f_00007-2-0 loss: 0.694682  [   32/  179]
train() client id: f_00007-2-1 loss: 0.618659  [   64/  179]
train() client id: f_00007-2-2 loss: 0.577622  [   96/  179]
train() client id: f_00007-2-3 loss: 0.595093  [  128/  179]
train() client id: f_00007-2-4 loss: 0.608862  [  160/  179]
train() client id: f_00007-3-0 loss: 0.547003  [   32/  179]
train() client id: f_00007-3-1 loss: 0.595178  [   64/  179]
train() client id: f_00007-3-2 loss: 0.604423  [   96/  179]
train() client id: f_00007-3-3 loss: 0.575721  [  128/  179]
train() client id: f_00007-3-4 loss: 0.520715  [  160/  179]
train() client id: f_00007-4-0 loss: 0.536727  [   32/  179]
train() client id: f_00007-4-1 loss: 0.629732  [   64/  179]
train() client id: f_00007-4-2 loss: 0.609069  [   96/  179]
train() client id: f_00007-4-3 loss: 0.632832  [  128/  179]
train() client id: f_00007-4-4 loss: 0.479190  [  160/  179]
train() client id: f_00007-5-0 loss: 0.567833  [   32/  179]
train() client id: f_00007-5-1 loss: 0.618388  [   64/  179]
train() client id: f_00007-5-2 loss: 0.520075  [   96/  179]
train() client id: f_00007-5-3 loss: 0.632395  [  128/  179]
train() client id: f_00007-5-4 loss: 0.560166  [  160/  179]
train() client id: f_00007-6-0 loss: 0.579469  [   32/  179]
train() client id: f_00007-6-1 loss: 0.443288  [   64/  179]
train() client id: f_00007-6-2 loss: 0.711533  [   96/  179]
train() client id: f_00007-6-3 loss: 0.626142  [  128/  179]
train() client id: f_00007-6-4 loss: 0.467869  [  160/  179]
train() client id: f_00007-7-0 loss: 0.594522  [   32/  179]
train() client id: f_00007-7-1 loss: 0.510463  [   64/  179]
train() client id: f_00007-7-2 loss: 0.451907  [   96/  179]
train() client id: f_00007-7-3 loss: 0.588915  [  128/  179]
train() client id: f_00007-7-4 loss: 0.607974  [  160/  179]
train() client id: f_00007-8-0 loss: 0.502333  [   32/  179]
train() client id: f_00007-8-1 loss: 0.461498  [   64/  179]
train() client id: f_00007-8-2 loss: 0.707893  [   96/  179]
train() client id: f_00007-8-3 loss: 0.527969  [  128/  179]
train() client id: f_00007-8-4 loss: 0.490646  [  160/  179]
train() client id: f_00007-9-0 loss: 0.522426  [   32/  179]
train() client id: f_00007-9-1 loss: 0.514552  [   64/  179]
train() client id: f_00007-9-2 loss: 0.507178  [   96/  179]
train() client id: f_00007-9-3 loss: 0.637583  [  128/  179]
train() client id: f_00007-9-4 loss: 0.502987  [  160/  179]
train() client id: f_00007-10-0 loss: 0.493802  [   32/  179]
train() client id: f_00007-10-1 loss: 0.568617  [   64/  179]
train() client id: f_00007-10-2 loss: 0.619174  [   96/  179]
train() client id: f_00007-10-3 loss: 0.430854  [  128/  179]
train() client id: f_00007-10-4 loss: 0.625692  [  160/  179]
train() client id: f_00007-11-0 loss: 0.440886  [   32/  179]
train() client id: f_00007-11-1 loss: 0.615089  [   64/  179]
train() client id: f_00007-11-2 loss: 0.564323  [   96/  179]
train() client id: f_00007-11-3 loss: 0.511965  [  128/  179]
train() client id: f_00007-11-4 loss: 0.508545  [  160/  179]
train() client id: f_00007-12-0 loss: 0.474877  [   32/  179]
train() client id: f_00007-12-1 loss: 0.561710  [   64/  179]
train() client id: f_00007-12-2 loss: 0.508960  [   96/  179]
train() client id: f_00007-12-3 loss: 0.511485  [  128/  179]
train() client id: f_00007-12-4 loss: 0.583714  [  160/  179]
train() client id: f_00008-0-0 loss: 0.899697  [   32/  130]
train() client id: f_00008-0-1 loss: 0.789834  [   64/  130]
train() client id: f_00008-0-2 loss: 0.833292  [   96/  130]
train() client id: f_00008-0-3 loss: 0.828526  [  128/  130]
train() client id: f_00008-1-0 loss: 0.751483  [   32/  130]
train() client id: f_00008-1-1 loss: 0.822888  [   64/  130]
train() client id: f_00008-1-2 loss: 0.866672  [   96/  130]
train() client id: f_00008-1-3 loss: 0.907200  [  128/  130]
train() client id: f_00008-2-0 loss: 0.771913  [   32/  130]
train() client id: f_00008-2-1 loss: 0.869845  [   64/  130]
train() client id: f_00008-2-2 loss: 0.814341  [   96/  130]
train() client id: f_00008-2-3 loss: 0.850804  [  128/  130]
train() client id: f_00008-3-0 loss: 0.783324  [   32/  130]
train() client id: f_00008-3-1 loss: 0.851246  [   64/  130]
train() client id: f_00008-3-2 loss: 0.785857  [   96/  130]
train() client id: f_00008-3-3 loss: 0.927547  [  128/  130]
train() client id: f_00008-4-0 loss: 0.763452  [   32/  130]
train() client id: f_00008-4-1 loss: 0.799950  [   64/  130]
train() client id: f_00008-4-2 loss: 0.903040  [   96/  130]
train() client id: f_00008-4-3 loss: 0.887460  [  128/  130]
train() client id: f_00008-5-0 loss: 0.860435  [   32/  130]
train() client id: f_00008-5-1 loss: 0.802934  [   64/  130]
train() client id: f_00008-5-2 loss: 0.902769  [   96/  130]
train() client id: f_00008-5-3 loss: 0.763818  [  128/  130]
train() client id: f_00008-6-0 loss: 0.831307  [   32/  130]
train() client id: f_00008-6-1 loss: 0.803152  [   64/  130]
train() client id: f_00008-6-2 loss: 0.869593  [   96/  130]
train() client id: f_00008-6-3 loss: 0.851139  [  128/  130]
train() client id: f_00008-7-0 loss: 0.989054  [   32/  130]
train() client id: f_00008-7-1 loss: 0.802633  [   64/  130]
train() client id: f_00008-7-2 loss: 0.797477  [   96/  130]
train() client id: f_00008-7-3 loss: 0.755829  [  128/  130]
train() client id: f_00008-8-0 loss: 0.915476  [   32/  130]
train() client id: f_00008-8-1 loss: 0.724198  [   64/  130]
train() client id: f_00008-8-2 loss: 0.784339  [   96/  130]
train() client id: f_00008-8-3 loss: 0.928834  [  128/  130]
train() client id: f_00008-9-0 loss: 0.858373  [   32/  130]
train() client id: f_00008-9-1 loss: 0.845198  [   64/  130]
train() client id: f_00008-9-2 loss: 0.772142  [   96/  130]
train() client id: f_00008-9-3 loss: 0.863112  [  128/  130]
train() client id: f_00008-10-0 loss: 0.767004  [   32/  130]
train() client id: f_00008-10-1 loss: 0.865819  [   64/  130]
train() client id: f_00008-10-2 loss: 0.854129  [   96/  130]
train() client id: f_00008-10-3 loss: 0.805228  [  128/  130]
train() client id: f_00008-11-0 loss: 0.850349  [   32/  130]
train() client id: f_00008-11-1 loss: 0.779527  [   64/  130]
train() client id: f_00008-11-2 loss: 0.828435  [   96/  130]
train() client id: f_00008-11-3 loss: 0.900069  [  128/  130]
train() client id: f_00008-12-0 loss: 0.794506  [   32/  130]
train() client id: f_00008-12-1 loss: 0.820860  [   64/  130]
train() client id: f_00008-12-2 loss: 0.836306  [   96/  130]
train() client id: f_00008-12-3 loss: 0.879784  [  128/  130]
train() client id: f_00009-0-0 loss: 1.147057  [   32/  118]
train() client id: f_00009-0-1 loss: 1.260313  [   64/  118]
train() client id: f_00009-0-2 loss: 1.220843  [   96/  118]
train() client id: f_00009-1-0 loss: 1.184886  [   32/  118]
train() client id: f_00009-1-1 loss: 1.189147  [   64/  118]
train() client id: f_00009-1-2 loss: 1.149153  [   96/  118]
train() client id: f_00009-2-0 loss: 1.128703  [   32/  118]
train() client id: f_00009-2-1 loss: 1.079450  [   64/  118]
train() client id: f_00009-2-2 loss: 1.121052  [   96/  118]
train() client id: f_00009-3-0 loss: 1.111022  [   32/  118]
train() client id: f_00009-3-1 loss: 1.075799  [   64/  118]
train() client id: f_00009-3-2 loss: 1.116546  [   96/  118]
train() client id: f_00009-4-0 loss: 1.101849  [   32/  118]
train() client id: f_00009-4-1 loss: 1.077180  [   64/  118]
train() client id: f_00009-4-2 loss: 1.063329  [   96/  118]
train() client id: f_00009-5-0 loss: 1.056259  [   32/  118]
train() client id: f_00009-5-1 loss: 1.076530  [   64/  118]
train() client id: f_00009-5-2 loss: 1.073371  [   96/  118]
train() client id: f_00009-6-0 loss: 1.059168  [   32/  118]
train() client id: f_00009-6-1 loss: 1.117861  [   64/  118]
train() client id: f_00009-6-2 loss: 1.001192  [   96/  118]
train() client id: f_00009-7-0 loss: 1.077152  [   32/  118]
train() client id: f_00009-7-1 loss: 1.083490  [   64/  118]
train() client id: f_00009-7-2 loss: 0.954446  [   96/  118]
train() client id: f_00009-8-0 loss: 1.047851  [   32/  118]
train() client id: f_00009-8-1 loss: 1.005452  [   64/  118]
train() client id: f_00009-8-2 loss: 1.014587  [   96/  118]
train() client id: f_00009-9-0 loss: 1.027498  [   32/  118]
train() client id: f_00009-9-1 loss: 1.025974  [   64/  118]
train() client id: f_00009-9-2 loss: 1.014516  [   96/  118]
train() client id: f_00009-10-0 loss: 0.955325  [   32/  118]
train() client id: f_00009-10-1 loss: 1.068947  [   64/  118]
train() client id: f_00009-10-2 loss: 1.052866  [   96/  118]
train() client id: f_00009-11-0 loss: 1.003307  [   32/  118]
train() client id: f_00009-11-1 loss: 0.962716  [   64/  118]
train() client id: f_00009-11-2 loss: 1.049081  [   96/  118]
train() client id: f_00009-12-0 loss: 1.017017  [   32/  118]
train() client id: f_00009-12-1 loss: 0.943963  [   64/  118]
train() client id: f_00009-12-2 loss: 1.017244  [   96/  118]
At round 6 accuracy: 0.623342175066313
At round 6 training accuracy: 0.5734406438631791
At round 6 training loss: 0.8892735397275359
update_location
xs = 8.927491 151.223621 5.882650 10.934260 -67.581990 84.769243 -5.849135 -5.143845 -90.120581 20.134486 
ys = -142.390647 7.291448 40.684448 -112.290817 -9.642386 0.794442 -121.381692 36.628436 25.881276 -577.232496 
xs mean: 11.317620029478787
ys mean: -85.16579882624052
dists_uav = 174.226280 181.443514 108.119517 150.760690 121.079730 131.097123 157.377659 106.621299 137.082310 586.176383 
uav_gains = -106.058678 -106.519043 -100.847651 -104.462068 -102.077029 -102.940565 -104.932106 -100.696139 -103.425893 -126.110464 
uav_gains_db_mean: -105.80696362861099
dists_bs = 366.832584 366.807864 225.298026 342.209823 213.615167 312.774097 341.253751 219.084009 171.583535 777.130102 
bs_gains = -111.372161 -111.371341 -105.444266 -110.527248 -104.796758 -109.433521 -110.493227 -105.104158 -102.132376 -120.500884 
bs_gains_db_mean: -109.11759397212558
Round 7
-------------------------------
ene_coms = [0.00840663 0.00862361 0.00813299 0.00772977 0.00786272 0.00717687
 0.00791776 0.00798882 0.00691361 0.02971911]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.11633976 21.04053718  9.95211128  3.57900327 24.24129796 11.65532429
  4.44795725 14.26864436 10.39165655 10.04302271]
obj_prev = 119.73589462192687
eta_min = 5.197398341955362e-10	eta_max = 0.736351008967473
af = 25.177764423944467	bf = 2.5906456007269107	zeta = 27.695540866338916	eta = 0.909090909090909
af = 25.177764423944467	bf = 2.5906456007269107	zeta = 54.89377140548384	eta = 0.45866341078961154
af = 25.177764423944467	bf = 2.5906456007269107	zeta = 41.03762557253169	eta = 0.6135287817625849
af = 25.177764423944467	bf = 2.5906456007269107	zeta = 38.52988362906232	eta = 0.6534606921302348
af = 25.177764423944467	bf = 2.5906456007269107	zeta = 38.388435829297784	eta = 0.6558684635107996
af = 25.177764423944467	bf = 2.5906456007269107	zeta = 38.387941604853324	eta = 0.655876907470373
af = 25.177764423944467	bf = 2.5906456007269107	zeta = 38.387941598789816	eta = 0.6558769075739711
eta = 0.6558769075739711
ene_coms = [0.00840663 0.00862361 0.00813299 0.00772977 0.00786272 0.00717687
 0.00791776 0.00798882 0.00691361 0.02971911]
ene_comp = [0.03480445 0.07319984 0.034252   0.01187771 0.0845251  0.04032897
 0.01491619 0.04944442 0.03590936 0.03259465]
ene_total = [3.23776098 6.13094518 3.17586257 1.4691675  6.92252217 3.5595628
 1.71092418 4.30341208 3.20868004 4.66910409]
ti_comp = [0.31155448 0.30938468 0.31429091 0.31832313 0.31699358 0.32385205
 0.31644318 0.31573262 0.32648465 0.0984297 ]
ti_coms = [0.0840663  0.08623611 0.08132987 0.07729765 0.0786272  0.07176873
 0.0791776  0.07988816 0.06913613 0.29719108]
t_total = [29.64344978 29.64344978 29.64344978 29.64344978 29.64344978 29.64344978
 29.64344978 29.64344978 29.64344978 29.64344978]
ene_coms = [0.00840663 0.00862361 0.00813299 0.00772977 0.00786272 0.00717687
 0.00791776 0.00798882 0.00691361 0.02971911]
ene_comp = [2.71466600e-05 2.56101901e-04 2.54257739e-05 1.03357441e-06
 3.75609118e-04 3.90875133e-05 2.07139335e-06 7.57866956e-05
 2.71504665e-05 2.23391395e-04]
ene_total = [0.63193402 0.66534753 0.61130129 0.57926063 0.61728937 0.54068435
 0.59342466 0.60427225 0.52006412 2.24356002]
optimize_network iter = 0 obj = 7.607138237842802
eta = 0.6558769075739711
freqs = [5.58561243e+07 1.18299071e+08 5.44909170e+07 1.86566866e+07
 1.33323043e+08 6.22644938e+07 2.35685181e+07 7.83010956e+07
 5.49939406e+07 1.65573216e+08]
eta_min = 0.6558769075740172	eta_max = 0.6558769075739571
af = 0.06436103160347911	bf = 2.5906456007269107	zeta = 0.07079713476382703	eta = 0.9090909090909091
af = 0.06436103160347911	bf = 2.5906456007269107	zeta = 28.56457848443572	eta = 0.0022531763120029295
af = 0.06436103160347911	bf = 2.5906456007269107	zeta = 2.9896890538038474	eta = 0.021527667407950386
af = 0.06436103160347911	bf = 2.5906456007269107	zeta = 2.900122916060128	eta = 0.02219251854708103
af = 0.06436103160347911	bf = 2.5906456007269107	zeta = 2.900092774941176	eta = 0.022192749197406134
eta = 0.022192749197406134
ene_coms = [0.00840663 0.00862361 0.00813299 0.00772977 0.00786272 0.00717687
 0.00791776 0.00798882 0.00691361 0.02971911]
ene_comp = [2.45089084e-04 2.31217322e-03 2.29552352e-04 9.33145390e-06
 3.39112416e-03 3.52895083e-04 1.87012287e-05 6.84227519e-04
 2.45123450e-04 2.01685188e-03]
ene_total = [0.22814587 0.28837667 0.2205202  0.20408001 0.29676393 0.19856002
 0.20928453 0.22870821 0.18877592 0.83687741]
ti_comp = [0.31155448 0.30938468 0.31429091 0.31832313 0.31699358 0.32385205
 0.31644318 0.31573262 0.32648465 0.0984297 ]
ti_coms = [0.0840663  0.08623611 0.08132987 0.07729765 0.0786272  0.07176873
 0.0791776  0.07988816 0.06913613 0.29719108]
t_total = [29.64344978 29.64344978 29.64344978 29.64344978 29.64344978 29.64344978
 29.64344978 29.64344978 29.64344978 29.64344978]
ene_coms = [0.00840663 0.00862361 0.00813299 0.00772977 0.00786272 0.00717687
 0.00791776 0.00798882 0.00691361 0.02971911]
ene_comp = [2.71466600e-05 2.56101901e-04 2.54257739e-05 1.03357441e-06
 3.75609118e-04 3.90875133e-05 2.07139335e-06 7.57866956e-05
 2.71504665e-05 2.23391395e-04]
ene_total = [0.63193402 0.66534753 0.61130129 0.57926063 0.61728937 0.54068435
 0.59342466 0.60427225 0.52006412 2.24356002]
optimize_network iter = 1 obj = 7.607138237843822
eta = 0.6558769075740172
freqs = [5.58561243e+07 1.18299071e+08 5.44909170e+07 1.86566866e+07
 1.33323043e+08 6.22644938e+07 2.35685181e+07 7.83010956e+07
 5.49939406e+07 1.65573216e+08]
Done!
ene_coms = [0.00840663 0.00862361 0.00813299 0.00772977 0.00786272 0.00717687
 0.00791776 0.00798882 0.00691361 0.02971911]
ene_comp = [2.55520352e-05 2.41058192e-04 2.39322359e-05 9.72861111e-07
 3.53545423e-04 3.67914696e-05 1.94971741e-06 7.13349015e-05
 2.55556181e-05 2.10269137e-04]
ene_total = [0.00843218 0.00886467 0.00815692 0.00773074 0.00821627 0.00721366
 0.00791971 0.00806015 0.00693917 0.02992938]
At round 7 energy consumption: 0.10146284489157387
At round 7 eta: 0.6558769075740172
At round 7 a_n: 25.784781927469155
At round 7 local rounds: 13.811290453435129
At round 7 global rounds: 74.92894982923933
gradient difference: 0.3815405070781708
train() client id: f_00000-0-0 loss: 1.511740  [   32/  126]
train() client id: f_00000-0-1 loss: 1.762821  [   64/  126]
train() client id: f_00000-0-2 loss: 1.617887  [   96/  126]
train() client id: f_00000-1-0 loss: 1.343630  [   32/  126]
train() client id: f_00000-1-1 loss: 1.252084  [   64/  126]
train() client id: f_00000-1-2 loss: 1.640283  [   96/  126]
train() client id: f_00000-2-0 loss: 1.442368  [   32/  126]
train() client id: f_00000-2-1 loss: 1.413759  [   64/  126]
train() client id: f_00000-2-2 loss: 1.071385  [   96/  126]
train() client id: f_00000-3-0 loss: 1.229558  [   32/  126]
train() client id: f_00000-3-1 loss: 1.193139  [   64/  126]
train() client id: f_00000-3-2 loss: 1.162775  [   96/  126]
train() client id: f_00000-4-0 loss: 1.077454  [   32/  126]
train() client id: f_00000-4-1 loss: 1.170433  [   64/  126]
train() client id: f_00000-4-2 loss: 1.184483  [   96/  126]
train() client id: f_00000-5-0 loss: 1.100948  [   32/  126]
train() client id: f_00000-5-1 loss: 1.076233  [   64/  126]
train() client id: f_00000-5-2 loss: 1.013889  [   96/  126]
train() client id: f_00000-6-0 loss: 0.977011  [   32/  126]
train() client id: f_00000-6-1 loss: 1.100727  [   64/  126]
train() client id: f_00000-6-2 loss: 0.988780  [   96/  126]
train() client id: f_00000-7-0 loss: 0.976818  [   32/  126]
train() client id: f_00000-7-1 loss: 0.958634  [   64/  126]
train() client id: f_00000-7-2 loss: 0.996679  [   96/  126]
train() client id: f_00000-8-0 loss: 0.959316  [   32/  126]
train() client id: f_00000-8-1 loss: 0.929154  [   64/  126]
train() client id: f_00000-8-2 loss: 0.928064  [   96/  126]
train() client id: f_00000-9-0 loss: 0.965235  [   32/  126]
train() client id: f_00000-9-1 loss: 0.947706  [   64/  126]
train() client id: f_00000-9-2 loss: 0.886176  [   96/  126]
train() client id: f_00000-10-0 loss: 0.888382  [   32/  126]
train() client id: f_00000-10-1 loss: 0.879577  [   64/  126]
train() client id: f_00000-10-2 loss: 0.915794  [   96/  126]
train() client id: f_00000-11-0 loss: 0.844197  [   32/  126]
train() client id: f_00000-11-1 loss: 0.865851  [   64/  126]
train() client id: f_00000-11-2 loss: 0.966456  [   96/  126]
train() client id: f_00000-12-0 loss: 0.920727  [   32/  126]
train() client id: f_00000-12-1 loss: 0.930145  [   64/  126]
train() client id: f_00000-12-2 loss: 0.832088  [   96/  126]
train() client id: f_00001-0-0 loss: 0.523317  [   32/  265]
train() client id: f_00001-0-1 loss: 0.432743  [   64/  265]
train() client id: f_00001-0-2 loss: 0.499981  [   96/  265]
train() client id: f_00001-0-3 loss: 0.471019  [  128/  265]
train() client id: f_00001-0-4 loss: 0.516932  [  160/  265]
train() client id: f_00001-0-5 loss: 0.516362  [  192/  265]
train() client id: f_00001-0-6 loss: 0.463897  [  224/  265]
train() client id: f_00001-0-7 loss: 0.484590  [  256/  265]
train() client id: f_00001-1-0 loss: 0.418840  [   32/  265]
train() client id: f_00001-1-1 loss: 0.420096  [   64/  265]
train() client id: f_00001-1-2 loss: 0.550238  [   96/  265]
train() client id: f_00001-1-3 loss: 0.395534  [  128/  265]
train() client id: f_00001-1-4 loss: 0.481466  [  160/  265]
train() client id: f_00001-1-5 loss: 0.563474  [  192/  265]
train() client id: f_00001-1-6 loss: 0.407251  [  224/  265]
train() client id: f_00001-1-7 loss: 0.514841  [  256/  265]
train() client id: f_00001-2-0 loss: 0.407005  [   32/  265]
train() client id: f_00001-2-1 loss: 0.404950  [   64/  265]
train() client id: f_00001-2-2 loss: 0.384619  [   96/  265]
train() client id: f_00001-2-3 loss: 0.407729  [  128/  265]
train() client id: f_00001-2-4 loss: 0.480563  [  160/  265]
train() client id: f_00001-2-5 loss: 0.478799  [  192/  265]
train() client id: f_00001-2-6 loss: 0.409516  [  224/  265]
train() client id: f_00001-2-7 loss: 0.652078  [  256/  265]
train() client id: f_00001-3-0 loss: 0.456064  [   32/  265]
train() client id: f_00001-3-1 loss: 0.377555  [   64/  265]
train() client id: f_00001-3-2 loss: 0.503232  [   96/  265]
train() client id: f_00001-3-3 loss: 0.455137  [  128/  265]
train() client id: f_00001-3-4 loss: 0.465894  [  160/  265]
train() client id: f_00001-3-5 loss: 0.471519  [  192/  265]
train() client id: f_00001-3-6 loss: 0.422592  [  224/  265]
train() client id: f_00001-3-7 loss: 0.371901  [  256/  265]
train() client id: f_00001-4-0 loss: 0.464166  [   32/  265]
train() client id: f_00001-4-1 loss: 0.379230  [   64/  265]
train() client id: f_00001-4-2 loss: 0.487293  [   96/  265]
train() client id: f_00001-4-3 loss: 0.436000  [  128/  265]
train() client id: f_00001-4-4 loss: 0.423691  [  160/  265]
train() client id: f_00001-4-5 loss: 0.411543  [  192/  265]
train() client id: f_00001-4-6 loss: 0.402585  [  224/  265]
train() client id: f_00001-4-7 loss: 0.369059  [  256/  265]
train() client id: f_00001-5-0 loss: 0.361521  [   32/  265]
train() client id: f_00001-5-1 loss: 0.400833  [   64/  265]
train() client id: f_00001-5-2 loss: 0.476256  [   96/  265]
train() client id: f_00001-5-3 loss: 0.403187  [  128/  265]
train() client id: f_00001-5-4 loss: 0.528498  [  160/  265]
train() client id: f_00001-5-5 loss: 0.414463  [  192/  265]
train() client id: f_00001-5-6 loss: 0.357401  [  224/  265]
train() client id: f_00001-5-7 loss: 0.438056  [  256/  265]
train() client id: f_00001-6-0 loss: 0.406952  [   32/  265]
train() client id: f_00001-6-1 loss: 0.522722  [   64/  265]
train() client id: f_00001-6-2 loss: 0.460772  [   96/  265]
train() client id: f_00001-6-3 loss: 0.417338  [  128/  265]
train() client id: f_00001-6-4 loss: 0.439145  [  160/  265]
train() client id: f_00001-6-5 loss: 0.333570  [  192/  265]
train() client id: f_00001-6-6 loss: 0.373505  [  224/  265]
train() client id: f_00001-6-7 loss: 0.371757  [  256/  265]
train() client id: f_00001-7-0 loss: 0.329564  [   32/  265]
train() client id: f_00001-7-1 loss: 0.484572  [   64/  265]
train() client id: f_00001-7-2 loss: 0.385760  [   96/  265]
train() client id: f_00001-7-3 loss: 0.465651  [  128/  265]
train() client id: f_00001-7-4 loss: 0.398476  [  160/  265]
train() client id: f_00001-7-5 loss: 0.426214  [  192/  265]
train() client id: f_00001-7-6 loss: 0.400410  [  224/  265]
train() client id: f_00001-7-7 loss: 0.394756  [  256/  265]
train() client id: f_00001-8-0 loss: 0.370934  [   32/  265]
train() client id: f_00001-8-1 loss: 0.457737  [   64/  265]
train() client id: f_00001-8-2 loss: 0.366643  [   96/  265]
train() client id: f_00001-8-3 loss: 0.299130  [  128/  265]
train() client id: f_00001-8-4 loss: 0.319773  [  160/  265]
train() client id: f_00001-8-5 loss: 0.462539  [  192/  265]
train() client id: f_00001-8-6 loss: 0.475233  [  224/  265]
train() client id: f_00001-8-7 loss: 0.505431  [  256/  265]
train() client id: f_00001-9-0 loss: 0.319427  [   32/  265]
train() client id: f_00001-9-1 loss: 0.463707  [   64/  265]
train() client id: f_00001-9-2 loss: 0.298659  [   96/  265]
train() client id: f_00001-9-3 loss: 0.505887  [  128/  265]
train() client id: f_00001-9-4 loss: 0.352669  [  160/  265]
train() client id: f_00001-9-5 loss: 0.597199  [  192/  265]
train() client id: f_00001-9-6 loss: 0.391666  [  224/  265]
train() client id: f_00001-9-7 loss: 0.298364  [  256/  265]
train() client id: f_00001-10-0 loss: 0.469837  [   32/  265]
train() client id: f_00001-10-1 loss: 0.407742  [   64/  265]
train() client id: f_00001-10-2 loss: 0.385244  [   96/  265]
train() client id: f_00001-10-3 loss: 0.487677  [  128/  265]
train() client id: f_00001-10-4 loss: 0.416735  [  160/  265]
train() client id: f_00001-10-5 loss: 0.298996  [  192/  265]
train() client id: f_00001-10-6 loss: 0.321384  [  224/  265]
train() client id: f_00001-10-7 loss: 0.405987  [  256/  265]
train() client id: f_00001-11-0 loss: 0.472922  [   32/  265]
train() client id: f_00001-11-1 loss: 0.290222  [   64/  265]
train() client id: f_00001-11-2 loss: 0.370609  [   96/  265]
train() client id: f_00001-11-3 loss: 0.318200  [  128/  265]
train() client id: f_00001-11-4 loss: 0.440396  [  160/  265]
train() client id: f_00001-11-5 loss: 0.383385  [  192/  265]
train() client id: f_00001-11-6 loss: 0.402190  [  224/  265]
train() client id: f_00001-11-7 loss: 0.530882  [  256/  265]
train() client id: f_00001-12-0 loss: 0.388054  [   32/  265]
train() client id: f_00001-12-1 loss: 0.291146  [   64/  265]
train() client id: f_00001-12-2 loss: 0.294388  [   96/  265]
train() client id: f_00001-12-3 loss: 0.415627  [  128/  265]
train() client id: f_00001-12-4 loss: 0.350727  [  160/  265]
train() client id: f_00001-12-5 loss: 0.452339  [  192/  265]
train() client id: f_00001-12-6 loss: 0.481637  [  224/  265]
train() client id: f_00001-12-7 loss: 0.524652  [  256/  265]
train() client id: f_00002-0-0 loss: 1.179011  [   32/  124]
train() client id: f_00002-0-1 loss: 1.122914  [   64/  124]
train() client id: f_00002-0-2 loss: 1.054304  [   96/  124]
train() client id: f_00002-1-0 loss: 1.086210  [   32/  124]
train() client id: f_00002-1-1 loss: 1.168183  [   64/  124]
train() client id: f_00002-1-2 loss: 1.045213  [   96/  124]
train() client id: f_00002-2-0 loss: 1.106516  [   32/  124]
train() client id: f_00002-2-1 loss: 1.094735  [   64/  124]
train() client id: f_00002-2-2 loss: 1.097399  [   96/  124]
train() client id: f_00002-3-0 loss: 1.049239  [   32/  124]
train() client id: f_00002-3-1 loss: 1.045835  [   64/  124]
train() client id: f_00002-3-2 loss: 1.047707  [   96/  124]
train() client id: f_00002-4-0 loss: 0.982773  [   32/  124]
train() client id: f_00002-4-1 loss: 1.042099  [   64/  124]
train() client id: f_00002-4-2 loss: 1.017761  [   96/  124]
train() client id: f_00002-5-0 loss: 0.932091  [   32/  124]
train() client id: f_00002-5-1 loss: 0.996394  [   64/  124]
train() client id: f_00002-5-2 loss: 1.056723  [   96/  124]
train() client id: f_00002-6-0 loss: 1.018232  [   32/  124]
train() client id: f_00002-6-1 loss: 0.996824  [   64/  124]
train() client id: f_00002-6-2 loss: 0.949618  [   96/  124]
train() client id: f_00002-7-0 loss: 1.014977  [   32/  124]
train() client id: f_00002-7-1 loss: 0.943095  [   64/  124]
train() client id: f_00002-7-2 loss: 0.988988  [   96/  124]
train() client id: f_00002-8-0 loss: 0.986730  [   32/  124]
train() client id: f_00002-8-1 loss: 0.963302  [   64/  124]
train() client id: f_00002-8-2 loss: 0.934552  [   96/  124]
train() client id: f_00002-9-0 loss: 0.998500  [   32/  124]
train() client id: f_00002-9-1 loss: 0.973402  [   64/  124]
train() client id: f_00002-9-2 loss: 0.959501  [   96/  124]
train() client id: f_00002-10-0 loss: 0.903035  [   32/  124]
train() client id: f_00002-10-1 loss: 1.050934  [   64/  124]
train() client id: f_00002-10-2 loss: 0.961198  [   96/  124]
train() client id: f_00002-11-0 loss: 0.946802  [   32/  124]
train() client id: f_00002-11-1 loss: 0.902120  [   64/  124]
train() client id: f_00002-11-2 loss: 0.964571  [   96/  124]
train() client id: f_00002-12-0 loss: 0.914298  [   32/  124]
train() client id: f_00002-12-1 loss: 0.964125  [   64/  124]
train() client id: f_00002-12-2 loss: 1.022318  [   96/  124]
train() client id: f_00003-0-0 loss: 0.860840  [   32/   43]
train() client id: f_00003-1-0 loss: 0.839125  [   32/   43]
train() client id: f_00003-2-0 loss: 1.016346  [   32/   43]
train() client id: f_00003-3-0 loss: 1.004870  [   32/   43]
train() client id: f_00003-4-0 loss: 0.904032  [   32/   43]
train() client id: f_00003-5-0 loss: 0.982628  [   32/   43]
train() client id: f_00003-6-0 loss: 0.875596  [   32/   43]
train() client id: f_00003-7-0 loss: 0.854631  [   32/   43]
train() client id: f_00003-8-0 loss: 0.920916  [   32/   43]
train() client id: f_00003-9-0 loss: 0.901379  [   32/   43]
train() client id: f_00003-10-0 loss: 0.748623  [   32/   43]
train() client id: f_00003-11-0 loss: 0.824359  [   32/   43]
train() client id: f_00003-12-0 loss: 0.879856  [   32/   43]
train() client id: f_00004-0-0 loss: 0.988437  [   32/  306]
train() client id: f_00004-0-1 loss: 0.836614  [   64/  306]
train() client id: f_00004-0-2 loss: 0.998065  [   96/  306]
train() client id: f_00004-0-3 loss: 0.996328  [  128/  306]
train() client id: f_00004-0-4 loss: 0.869264  [  160/  306]
train() client id: f_00004-0-5 loss: 0.967833  [  192/  306]
train() client id: f_00004-0-6 loss: 0.937424  [  224/  306]
train() client id: f_00004-0-7 loss: 0.868688  [  256/  306]
train() client id: f_00004-0-8 loss: 0.961892  [  288/  306]
train() client id: f_00004-1-0 loss: 1.058136  [   32/  306]
train() client id: f_00004-1-1 loss: 0.923799  [   64/  306]
train() client id: f_00004-1-2 loss: 0.947241  [   96/  306]
train() client id: f_00004-1-3 loss: 0.949812  [  128/  306]
train() client id: f_00004-1-4 loss: 0.886047  [  160/  306]
train() client id: f_00004-1-5 loss: 0.954476  [  192/  306]
train() client id: f_00004-1-6 loss: 0.909573  [  224/  306]
train() client id: f_00004-1-7 loss: 0.890327  [  256/  306]
train() client id: f_00004-1-8 loss: 0.954570  [  288/  306]
train() client id: f_00004-2-0 loss: 0.793366  [   32/  306]
train() client id: f_00004-2-1 loss: 0.976777  [   64/  306]
train() client id: f_00004-2-2 loss: 1.001971  [   96/  306]
train() client id: f_00004-2-3 loss: 1.014405  [  128/  306]
train() client id: f_00004-2-4 loss: 0.974496  [  160/  306]
train() client id: f_00004-2-5 loss: 0.889349  [  192/  306]
train() client id: f_00004-2-6 loss: 1.000256  [  224/  306]
train() client id: f_00004-2-7 loss: 0.825879  [  256/  306]
train() client id: f_00004-2-8 loss: 0.935139  [  288/  306]
train() client id: f_00004-3-0 loss: 0.855520  [   32/  306]
train() client id: f_00004-3-1 loss: 1.034695  [   64/  306]
train() client id: f_00004-3-2 loss: 0.904750  [   96/  306]
train() client id: f_00004-3-3 loss: 0.904899  [  128/  306]
train() client id: f_00004-3-4 loss: 0.827908  [  160/  306]
train() client id: f_00004-3-5 loss: 0.973917  [  192/  306]
train() client id: f_00004-3-6 loss: 0.964431  [  224/  306]
train() client id: f_00004-3-7 loss: 0.928197  [  256/  306]
train() client id: f_00004-3-8 loss: 0.979568  [  288/  306]
train() client id: f_00004-4-0 loss: 0.897479  [   32/  306]
train() client id: f_00004-4-1 loss: 0.883131  [   64/  306]
train() client id: f_00004-4-2 loss: 0.906908  [   96/  306]
train() client id: f_00004-4-3 loss: 0.846914  [  128/  306]
train() client id: f_00004-4-4 loss: 1.083283  [  160/  306]
train() client id: f_00004-4-5 loss: 0.881168  [  192/  306]
train() client id: f_00004-4-6 loss: 0.907978  [  224/  306]
train() client id: f_00004-4-7 loss: 0.986711  [  256/  306]
train() client id: f_00004-4-8 loss: 1.016256  [  288/  306]
train() client id: f_00004-5-0 loss: 0.977214  [   32/  306]
train() client id: f_00004-5-1 loss: 1.085647  [   64/  306]
train() client id: f_00004-5-2 loss: 0.817115  [   96/  306]
train() client id: f_00004-5-3 loss: 0.827825  [  128/  306]
train() client id: f_00004-5-4 loss: 0.932294  [  160/  306]
train() client id: f_00004-5-5 loss: 0.999379  [  192/  306]
train() client id: f_00004-5-6 loss: 0.992267  [  224/  306]
train() client id: f_00004-5-7 loss: 0.870491  [  256/  306]
train() client id: f_00004-5-8 loss: 0.923977  [  288/  306]
train() client id: f_00004-6-0 loss: 0.897335  [   32/  306]
train() client id: f_00004-6-1 loss: 0.808686  [   64/  306]
train() client id: f_00004-6-2 loss: 0.867673  [   96/  306]
train() client id: f_00004-6-3 loss: 0.909599  [  128/  306]
train() client id: f_00004-6-4 loss: 0.967067  [  160/  306]
train() client id: f_00004-6-5 loss: 0.854470  [  192/  306]
train() client id: f_00004-6-6 loss: 1.024975  [  224/  306]
train() client id: f_00004-6-7 loss: 1.064869  [  256/  306]
train() client id: f_00004-6-8 loss: 0.940084  [  288/  306]
train() client id: f_00004-7-0 loss: 0.925386  [   32/  306]
train() client id: f_00004-7-1 loss: 0.792437  [   64/  306]
train() client id: f_00004-7-2 loss: 0.978601  [   96/  306]
train() client id: f_00004-7-3 loss: 1.085350  [  128/  306]
train() client id: f_00004-7-4 loss: 0.925489  [  160/  306]
train() client id: f_00004-7-5 loss: 0.908774  [  192/  306]
train() client id: f_00004-7-6 loss: 0.899339  [  224/  306]
train() client id: f_00004-7-7 loss: 0.988876  [  256/  306]
train() client id: f_00004-7-8 loss: 0.866868  [  288/  306]
train() client id: f_00004-8-0 loss: 0.872810  [   32/  306]
train() client id: f_00004-8-1 loss: 0.981957  [   64/  306]
train() client id: f_00004-8-2 loss: 1.014510  [   96/  306]
train() client id: f_00004-8-3 loss: 0.908024  [  128/  306]
train() client id: f_00004-8-4 loss: 0.782732  [  160/  306]
train() client id: f_00004-8-5 loss: 0.913636  [  192/  306]
train() client id: f_00004-8-6 loss: 0.955994  [  224/  306]
train() client id: f_00004-8-7 loss: 1.026390  [  256/  306]
train() client id: f_00004-8-8 loss: 0.886231  [  288/  306]
train() client id: f_00004-9-0 loss: 0.968363  [   32/  306]
train() client id: f_00004-9-1 loss: 0.969406  [   64/  306]
train() client id: f_00004-9-2 loss: 0.882186  [   96/  306]
train() client id: f_00004-9-3 loss: 1.005799  [  128/  306]
train() client id: f_00004-9-4 loss: 0.917335  [  160/  306]
train() client id: f_00004-9-5 loss: 0.924515  [  192/  306]
train() client id: f_00004-9-6 loss: 0.826288  [  224/  306]
train() client id: f_00004-9-7 loss: 0.961383  [  256/  306]
train() client id: f_00004-9-8 loss: 1.009295  [  288/  306]
train() client id: f_00004-10-0 loss: 0.927074  [   32/  306]
train() client id: f_00004-10-1 loss: 0.928507  [   64/  306]
train() client id: f_00004-10-2 loss: 0.954038  [   96/  306]
train() client id: f_00004-10-3 loss: 0.823193  [  128/  306]
train() client id: f_00004-10-4 loss: 0.915813  [  160/  306]
train() client id: f_00004-10-5 loss: 0.918405  [  192/  306]
train() client id: f_00004-10-6 loss: 1.057637  [  224/  306]
train() client id: f_00004-10-7 loss: 1.010753  [  256/  306]
train() client id: f_00004-10-8 loss: 0.901291  [  288/  306]
train() client id: f_00004-11-0 loss: 0.911885  [   32/  306]
train() client id: f_00004-11-1 loss: 0.866003  [   64/  306]
train() client id: f_00004-11-2 loss: 0.966149  [   96/  306]
train() client id: f_00004-11-3 loss: 0.975720  [  128/  306]
train() client id: f_00004-11-4 loss: 0.910406  [  160/  306]
train() client id: f_00004-11-5 loss: 0.942132  [  192/  306]
train() client id: f_00004-11-6 loss: 0.878946  [  224/  306]
train() client id: f_00004-11-7 loss: 0.935506  [  256/  306]
train() client id: f_00004-11-8 loss: 0.955303  [  288/  306]
train() client id: f_00004-12-0 loss: 0.982315  [   32/  306]
train() client id: f_00004-12-1 loss: 0.910940  [   64/  306]
train() client id: f_00004-12-2 loss: 0.820483  [   96/  306]
train() client id: f_00004-12-3 loss: 0.890632  [  128/  306]
train() client id: f_00004-12-4 loss: 0.930920  [  160/  306]
train() client id: f_00004-12-5 loss: 1.030320  [  192/  306]
train() client id: f_00004-12-6 loss: 0.914826  [  224/  306]
train() client id: f_00004-12-7 loss: 0.944967  [  256/  306]
train() client id: f_00004-12-8 loss: 0.894410  [  288/  306]
train() client id: f_00005-0-0 loss: 0.810308  [   32/  146]
train() client id: f_00005-0-1 loss: 0.887196  [   64/  146]
train() client id: f_00005-0-2 loss: 0.753958  [   96/  146]
train() client id: f_00005-0-3 loss: 0.908180  [  128/  146]
train() client id: f_00005-1-0 loss: 0.792475  [   32/  146]
train() client id: f_00005-1-1 loss: 0.849322  [   64/  146]
train() client id: f_00005-1-2 loss: 0.897049  [   96/  146]
train() client id: f_00005-1-3 loss: 0.802894  [  128/  146]
train() client id: f_00005-2-0 loss: 0.822029  [   32/  146]
train() client id: f_00005-2-1 loss: 0.842092  [   64/  146]
train() client id: f_00005-2-2 loss: 0.908053  [   96/  146]
train() client id: f_00005-2-3 loss: 0.823653  [  128/  146]
train() client id: f_00005-3-0 loss: 0.875287  [   32/  146]
train() client id: f_00005-3-1 loss: 0.769876  [   64/  146]
train() client id: f_00005-3-2 loss: 0.765384  [   96/  146]
train() client id: f_00005-3-3 loss: 0.987077  [  128/  146]
train() client id: f_00005-4-0 loss: 0.833506  [   32/  146]
train() client id: f_00005-4-1 loss: 0.770636  [   64/  146]
train() client id: f_00005-4-2 loss: 0.925385  [   96/  146]
train() client id: f_00005-4-3 loss: 0.845673  [  128/  146]
train() client id: f_00005-5-0 loss: 0.761857  [   32/  146]
train() client id: f_00005-5-1 loss: 0.796183  [   64/  146]
train() client id: f_00005-5-2 loss: 0.913039  [   96/  146]
train() client id: f_00005-5-3 loss: 0.907731  [  128/  146]
train() client id: f_00005-6-0 loss: 0.751387  [   32/  146]
train() client id: f_00005-6-1 loss: 0.652345  [   64/  146]
train() client id: f_00005-6-2 loss: 0.882765  [   96/  146]
train() client id: f_00005-6-3 loss: 1.021508  [  128/  146]
train() client id: f_00005-7-0 loss: 0.779400  [   32/  146]
train() client id: f_00005-7-1 loss: 0.874267  [   64/  146]
train() client id: f_00005-7-2 loss: 0.983120  [   96/  146]
train() client id: f_00005-7-3 loss: 0.818614  [  128/  146]
train() client id: f_00005-8-0 loss: 0.759929  [   32/  146]
train() client id: f_00005-8-1 loss: 0.864677  [   64/  146]
train() client id: f_00005-8-2 loss: 0.834800  [   96/  146]
train() client id: f_00005-8-3 loss: 1.016831  [  128/  146]
train() client id: f_00005-9-0 loss: 0.808220  [   32/  146]
train() client id: f_00005-9-1 loss: 0.864007  [   64/  146]
train() client id: f_00005-9-2 loss: 0.959927  [   96/  146]
train() client id: f_00005-9-3 loss: 0.829562  [  128/  146]
train() client id: f_00005-10-0 loss: 0.803816  [   32/  146]
train() client id: f_00005-10-1 loss: 1.038126  [   64/  146]
train() client id: f_00005-10-2 loss: 0.793002  [   96/  146]
train() client id: f_00005-10-3 loss: 0.722906  [  128/  146]
train() client id: f_00005-11-0 loss: 0.704828  [   32/  146]
train() client id: f_00005-11-1 loss: 0.802610  [   64/  146]
train() client id: f_00005-11-2 loss: 0.899636  [   96/  146]
train() client id: f_00005-11-3 loss: 0.725486  [  128/  146]
train() client id: f_00005-12-0 loss: 0.850476  [   32/  146]
train() client id: f_00005-12-1 loss: 0.827966  [   64/  146]
train() client id: f_00005-12-2 loss: 0.939138  [   96/  146]
train() client id: f_00005-12-3 loss: 0.648327  [  128/  146]
train() client id: f_00006-0-0 loss: 0.775978  [   32/   54]
train() client id: f_00006-1-0 loss: 0.764376  [   32/   54]
train() client id: f_00006-2-0 loss: 0.742408  [   32/   54]
train() client id: f_00006-3-0 loss: 0.812990  [   32/   54]
train() client id: f_00006-4-0 loss: 0.754465  [   32/   54]
train() client id: f_00006-5-0 loss: 0.790265  [   32/   54]
train() client id: f_00006-6-0 loss: 0.769552  [   32/   54]
train() client id: f_00006-7-0 loss: 0.725713  [   32/   54]
train() client id: f_00006-8-0 loss: 0.725028  [   32/   54]
train() client id: f_00006-9-0 loss: 0.801731  [   32/   54]
train() client id: f_00006-10-0 loss: 0.742356  [   32/   54]
train() client id: f_00006-11-0 loss: 0.762973  [   32/   54]
train() client id: f_00006-12-0 loss: 0.773073  [   32/   54]
train() client id: f_00007-0-0 loss: 0.791746  [   32/  179]
train() client id: f_00007-0-1 loss: 0.653885  [   64/  179]
train() client id: f_00007-0-2 loss: 0.585017  [   96/  179]
train() client id: f_00007-0-3 loss: 0.727544  [  128/  179]
train() client id: f_00007-0-4 loss: 0.624047  [  160/  179]
train() client id: f_00007-1-0 loss: 0.650597  [   32/  179]
train() client id: f_00007-1-1 loss: 0.624363  [   64/  179]
train() client id: f_00007-1-2 loss: 0.679070  [   96/  179]
train() client id: f_00007-1-3 loss: 0.609612  [  128/  179]
train() client id: f_00007-1-4 loss: 0.655492  [  160/  179]
train() client id: f_00007-2-0 loss: 0.580631  [   32/  179]
train() client id: f_00007-2-1 loss: 0.760328  [   64/  179]
train() client id: f_00007-2-2 loss: 0.600325  [   96/  179]
train() client id: f_00007-2-3 loss: 0.576949  [  128/  179]
train() client id: f_00007-2-4 loss: 0.583672  [  160/  179]
train() client id: f_00007-3-0 loss: 0.534606  [   32/  179]
train() client id: f_00007-3-1 loss: 0.560635  [   64/  179]
train() client id: f_00007-3-2 loss: 0.628672  [   96/  179]
train() client id: f_00007-3-3 loss: 0.616907  [  128/  179]
train() client id: f_00007-3-4 loss: 0.695972  [  160/  179]
train() client id: f_00007-4-0 loss: 0.582902  [   32/  179]
train() client id: f_00007-4-1 loss: 0.475395  [   64/  179]
train() client id: f_00007-4-2 loss: 0.658272  [   96/  179]
train() client id: f_00007-4-3 loss: 0.614335  [  128/  179]
train() client id: f_00007-4-4 loss: 0.691465  [  160/  179]
train() client id: f_00007-5-0 loss: 0.665351  [   32/  179]
train() client id: f_00007-5-1 loss: 0.538799  [   64/  179]
train() client id: f_00007-5-2 loss: 0.660949  [   96/  179]
train() client id: f_00007-5-3 loss: 0.617720  [  128/  179]
train() client id: f_00007-5-4 loss: 0.478000  [  160/  179]
train() client id: f_00007-6-0 loss: 0.673425  [   32/  179]
train() client id: f_00007-6-1 loss: 0.595236  [   64/  179]
train() client id: f_00007-6-2 loss: 0.471924  [   96/  179]
train() client id: f_00007-6-3 loss: 0.480214  [  128/  179]
train() client id: f_00007-6-4 loss: 0.616790  [  160/  179]
train() client id: f_00007-7-0 loss: 0.532274  [   32/  179]
train() client id: f_00007-7-1 loss: 0.569445  [   64/  179]
train() client id: f_00007-7-2 loss: 0.452635  [   96/  179]
train() client id: f_00007-7-3 loss: 0.635600  [  128/  179]
train() client id: f_00007-7-4 loss: 0.533772  [  160/  179]
train() client id: f_00007-8-0 loss: 0.613963  [   32/  179]
train() client id: f_00007-8-1 loss: 0.564545  [   64/  179]
train() client id: f_00007-8-2 loss: 0.474331  [   96/  179]
train() client id: f_00007-8-3 loss: 0.463693  [  128/  179]
train() client id: f_00007-8-4 loss: 0.514321  [  160/  179]
train() client id: f_00007-9-0 loss: 0.699273  [   32/  179]
train() client id: f_00007-9-1 loss: 0.590383  [   64/  179]
train() client id: f_00007-9-2 loss: 0.523714  [   96/  179]
train() client id: f_00007-9-3 loss: 0.519776  [  128/  179]
train() client id: f_00007-9-4 loss: 0.506705  [  160/  179]
train() client id: f_00007-10-0 loss: 0.675073  [   32/  179]
train() client id: f_00007-10-1 loss: 0.465102  [   64/  179]
train() client id: f_00007-10-2 loss: 0.525304  [   96/  179]
train() client id: f_00007-10-3 loss: 0.576156  [  128/  179]
train() client id: f_00007-10-4 loss: 0.446473  [  160/  179]
train() client id: f_00007-11-0 loss: 0.568299  [   32/  179]
train() client id: f_00007-11-1 loss: 0.579381  [   64/  179]
train() client id: f_00007-11-2 loss: 0.775918  [   96/  179]
train() client id: f_00007-11-3 loss: 0.484423  [  128/  179]
train() client id: f_00007-11-4 loss: 0.424096  [  160/  179]
train() client id: f_00007-12-0 loss: 0.641562  [   32/  179]
train() client id: f_00007-12-1 loss: 0.525515  [   64/  179]
train() client id: f_00007-12-2 loss: 0.601000  [   96/  179]
train() client id: f_00007-12-3 loss: 0.423754  [  128/  179]
train() client id: f_00007-12-4 loss: 0.639799  [  160/  179]
train() client id: f_00008-0-0 loss: 0.736678  [   32/  130]
train() client id: f_00008-0-1 loss: 0.771363  [   64/  130]
train() client id: f_00008-0-2 loss: 0.731124  [   96/  130]
train() client id: f_00008-0-3 loss: 0.751489  [  128/  130]
train() client id: f_00008-1-0 loss: 0.712258  [   32/  130]
train() client id: f_00008-1-1 loss: 0.796379  [   64/  130]
train() client id: f_00008-1-2 loss: 0.748854  [   96/  130]
train() client id: f_00008-1-3 loss: 0.716600  [  128/  130]
train() client id: f_00008-2-0 loss: 0.684024  [   32/  130]
train() client id: f_00008-2-1 loss: 0.754004  [   64/  130]
train() client id: f_00008-2-2 loss: 0.766346  [   96/  130]
train() client id: f_00008-2-3 loss: 0.757881  [  128/  130]
train() client id: f_00008-3-0 loss: 0.805859  [   32/  130]
train() client id: f_00008-3-1 loss: 0.733737  [   64/  130]
train() client id: f_00008-3-2 loss: 0.688292  [   96/  130]
train() client id: f_00008-3-3 loss: 0.715803  [  128/  130]
train() client id: f_00008-4-0 loss: 0.822917  [   32/  130]
train() client id: f_00008-4-1 loss: 0.657513  [   64/  130]
train() client id: f_00008-4-2 loss: 0.756903  [   96/  130]
train() client id: f_00008-4-3 loss: 0.681659  [  128/  130]
train() client id: f_00008-5-0 loss: 0.653239  [   32/  130]
train() client id: f_00008-5-1 loss: 0.789986  [   64/  130]
train() client id: f_00008-5-2 loss: 0.764917  [   96/  130]
train() client id: f_00008-5-3 loss: 0.746400  [  128/  130]
train() client id: f_00008-6-0 loss: 0.572507  [   32/  130]
train() client id: f_00008-6-1 loss: 0.851705  [   64/  130]
train() client id: f_00008-6-2 loss: 0.767277  [   96/  130]
train() client id: f_00008-6-3 loss: 0.715705  [  128/  130]
train() client id: f_00008-7-0 loss: 0.784460  [   32/  130]
train() client id: f_00008-7-1 loss: 0.803542  [   64/  130]
train() client id: f_00008-7-2 loss: 0.679886  [   96/  130]
train() client id: f_00008-7-3 loss: 0.669869  [  128/  130]
train() client id: f_00008-8-0 loss: 0.662408  [   32/  130]
train() client id: f_00008-8-1 loss: 0.710707  [   64/  130]
train() client id: f_00008-8-2 loss: 0.706986  [   96/  130]
train() client id: f_00008-8-3 loss: 0.845682  [  128/  130]
train() client id: f_00008-9-0 loss: 0.790115  [   32/  130]
train() client id: f_00008-9-1 loss: 0.707141  [   64/  130]
train() client id: f_00008-9-2 loss: 0.639340  [   96/  130]
train() client id: f_00008-9-3 loss: 0.779102  [  128/  130]
train() client id: f_00008-10-0 loss: 0.742891  [   32/  130]
train() client id: f_00008-10-1 loss: 0.695540  [   64/  130]
train() client id: f_00008-10-2 loss: 0.703638  [   96/  130]
train() client id: f_00008-10-3 loss: 0.743495  [  128/  130]
train() client id: f_00008-11-0 loss: 0.774819  [   32/  130]
train() client id: f_00008-11-1 loss: 0.743289  [   64/  130]
train() client id: f_00008-11-2 loss: 0.733248  [   96/  130]
train() client id: f_00008-11-3 loss: 0.637520  [  128/  130]
train() client id: f_00008-12-0 loss: 0.636474  [   32/  130]
train() client id: f_00008-12-1 loss: 0.729439  [   64/  130]
train() client id: f_00008-12-2 loss: 0.755147  [   96/  130]
train() client id: f_00008-12-3 loss: 0.779248  [  128/  130]
train() client id: f_00009-0-0 loss: 1.125402  [   32/  118]
train() client id: f_00009-0-1 loss: 1.205647  [   64/  118]
train() client id: f_00009-0-2 loss: 1.111511  [   96/  118]
train() client id: f_00009-1-0 loss: 1.096230  [   32/  118]
train() client id: f_00009-1-1 loss: 1.073940  [   64/  118]
train() client id: f_00009-1-2 loss: 1.128590  [   96/  118]
train() client id: f_00009-2-0 loss: 1.013314  [   32/  118]
train() client id: f_00009-2-1 loss: 1.028674  [   64/  118]
train() client id: f_00009-2-2 loss: 1.134317  [   96/  118]
train() client id: f_00009-3-0 loss: 0.928539  [   32/  118]
train() client id: f_00009-3-1 loss: 1.031157  [   64/  118]
train() client id: f_00009-3-2 loss: 1.019136  [   96/  118]
train() client id: f_00009-4-0 loss: 0.925696  [   32/  118]
train() client id: f_00009-4-1 loss: 0.959493  [   64/  118]
train() client id: f_00009-4-2 loss: 1.016362  [   96/  118]
train() client id: f_00009-5-0 loss: 1.024461  [   32/  118]
train() client id: f_00009-5-1 loss: 0.927597  [   64/  118]
train() client id: f_00009-5-2 loss: 0.927528  [   96/  118]
train() client id: f_00009-6-0 loss: 0.966614  [   32/  118]
train() client id: f_00009-6-1 loss: 0.905427  [   64/  118]
train() client id: f_00009-6-2 loss: 0.949795  [   96/  118]
train() client id: f_00009-7-0 loss: 1.017497  [   32/  118]
train() client id: f_00009-7-1 loss: 0.943426  [   64/  118]
train() client id: f_00009-7-2 loss: 0.929740  [   96/  118]
train() client id: f_00009-8-0 loss: 0.965827  [   32/  118]
train() client id: f_00009-8-1 loss: 0.961087  [   64/  118]
train() client id: f_00009-8-2 loss: 0.885947  [   96/  118]
train() client id: f_00009-9-0 loss: 0.941262  [   32/  118]
train() client id: f_00009-9-1 loss: 0.937770  [   64/  118]
train() client id: f_00009-9-2 loss: 0.859516  [   96/  118]
train() client id: f_00009-10-0 loss: 0.918310  [   32/  118]
train() client id: f_00009-10-1 loss: 0.880569  [   64/  118]
train() client id: f_00009-10-2 loss: 0.894945  [   96/  118]
train() client id: f_00009-11-0 loss: 0.879461  [   32/  118]
train() client id: f_00009-11-1 loss: 0.935575  [   64/  118]
train() client id: f_00009-11-2 loss: 0.927810  [   96/  118]
train() client id: f_00009-12-0 loss: 0.857485  [   32/  118]
train() client id: f_00009-12-1 loss: 0.913951  [   64/  118]
train() client id: f_00009-12-2 loss: 0.945506  [   96/  118]
At round 7 accuracy: 0.623342175066313
At round 7 training accuracy: 0.5687458081824279
At round 7 training loss: 0.8886065243711921
update_location
xs = 8.927491 156.223621 5.882650 10.934260 -72.581990 79.769243 -5.849135 -5.143845 -95.120581 20.134486 
ys = -147.390647 7.291448 45.684448 -107.290817 -9.642386 0.794442 -116.381692 41.628436 25.881276 -582.232496 
xs mean: 10.317620029478787
ys mean: -84.16579882624052
dists_uav = 178.335927 185.631314 110.098476 147.074395 123.939989 127.920926 153.554260 108.440702 140.419961 591.100733 
uav_gains = -106.321971 -106.782568 -101.044596 -104.191932 -102.330623 -102.674064 -104.662739 -100.879859 -103.687604 -126.204354 
uav_gains_db_mean: -105.87803101222687
dists_bs = 371.167148 371.261694 222.353424 338.023157 211.145115 308.633996 336.920326 215.960847 169.165940 781.970920 
bs_gains = -111.515006 -111.518103 -105.284286 -110.377560 -104.655328 -109.271484 -110.337821 -104.929560 -101.959821 -120.576396 
bs_gains_db_mean: -109.04253657332941
Round 8
-------------------------------
ene_coms = [0.00852942 0.00875274 0.00806455 0.00762561 0.007806   0.00708791
 0.00780895 0.00791672 0.00685985 0.03002578]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.98510169 20.76433631  9.81814058  3.52878017 23.91779928 11.49819938
  4.38607068 14.07723529 10.25222364  9.91748438]
obj_prev = 118.14537140231496
eta_min = 3.926877638470098e-10	eta_max = 0.7367787698081033
af = 24.843282687249765	bf = 2.5563729348950814	zeta = 27.327610955974745	eta = 0.909090909090909
af = 24.843282687249765	bf = 2.5563729348950814	zeta = 54.166097422211735	eta = 0.45865003885368255
af = 24.843282687249765	bf = 2.5563729348950814	zeta = 40.493051568631614	eta = 0.6135196465779548
af = 24.843282687249765	bf = 2.5563729348950814	zeta = 38.01845151904842	eta = 0.6534533021367931
af = 24.843282687249765	bf = 2.5563729348950814	zeta = 37.87887011604833	eta = 0.6558612390268812
af = 24.843282687249765	bf = 2.5563729348950814	zeta = 37.87838238963342	eta = 0.6558696839717445
af = 24.843282687249765	bf = 2.5563729348950814	zeta = 37.87838238364904	eta = 0.655869684075365
eta = 0.655869684075365
ene_coms = [0.00852942 0.00875274 0.00806455 0.00762561 0.007806   0.00708791
 0.00780895 0.00791672 0.00685985 0.03002578]
ene_comp = [0.03480536 0.07320175 0.03425289 0.01187802 0.0845273  0.04033002
 0.01491658 0.04944571 0.03591029 0.0325955 ]
ene_total = [3.20382602 6.0590581  3.12861238 1.44194193 6.826384   3.50570191
 1.68014373 4.240918   3.16208112 4.62971519]
ti_comp = [0.31497258 0.31273932 0.31962127 0.32401066 0.32220677 0.32938761
 0.32217725 0.32109958 0.33166829 0.10000893]
ti_coms = [0.08529418 0.08752743 0.08064548 0.07625609 0.07805998 0.07087914
 0.0780895  0.07916718 0.06859846 0.30025782]
t_total = [29.59251404 29.59251404 29.59251404 29.59251404 29.59251404 29.59251404
 29.59251404 29.59251404 29.59251404 29.59251404]
ene_coms = [0.00852942 0.00875274 0.00806455 0.00762561 0.007806   0.00708791
 0.00780895 0.00791672 0.00685985 0.03002578]
ene_comp = [2.65627448e-05 2.50656771e-04 2.45867149e-05 9.97685278e-07
 3.63581478e-04 3.77877339e-05 1.99847328e-06 7.32801637e-05
 2.63104911e-05 2.16408956e-04]
ene_total = [0.63256058 0.66563918 0.59804576 0.56385014 0.60399319 0.52681726
 0.57747893 0.59071637 0.50910721 2.23586509]
optimize_network iter = 0 obj = 7.504073708927552
eta = 0.655869684075365
freqs = [5.52514145e+07 1.17033174e+08 5.35835655e+07 1.83296745e+07
 1.31169348e+08 6.12196996e+07 2.31496533e+07 7.69943574e+07
 5.41358549e+07 1.62962924e+08]
eta_min = 0.6558696840754216	eta_max = 0.6558696840753606
af = 0.061656573925149276	bf = 2.5563729348950814	zeta = 0.06782223131766421	eta = 0.909090909090909
af = 0.061656573925149276	bf = 2.5563729348950814	zeta = 28.18474377446202	eta = 0.002187586817128202
af = 0.061656573925149276	bf = 2.5563729348950814	zeta = 2.9404492408731366	eta = 0.020968419746242917
af = 0.061656573925149276	bf = 2.5563729348950814	zeta = 2.8545140757497154	eta = 0.021599674161338886
af = 0.061656573925149276	bf = 2.5563729348950814	zeta = 2.8544866275612315	eta = 0.021599881859606533
eta = 0.021599881859606533
ene_coms = [0.00852942 0.00875274 0.00806455 0.00762561 0.007806   0.00708791
 0.00780895 0.00791672 0.00685985 0.03002578]
ene_comp = [2.41516285e-04 2.27904505e-03 2.23549640e-04 9.07124786e-06
 3.30578968e-03 3.43577186e-04 1.81707066e-05 6.66284792e-04
 2.39222720e-04 1.96765385e-03]
ene_total = [0.22807864 0.28686971 0.21552301 0.1985316  0.28895001 0.19324789
 0.20353581 0.22319169 0.1846036  0.83195467]
ti_comp = [0.31497258 0.31273932 0.31962127 0.32401066 0.32220677 0.32938761
 0.32217725 0.32109958 0.33166829 0.10000893]
ti_coms = [0.08529418 0.08752743 0.08064548 0.07625609 0.07805998 0.07087914
 0.0780895  0.07916718 0.06859846 0.30025782]
t_total = [29.59251404 29.59251404 29.59251404 29.59251404 29.59251404 29.59251404
 29.59251404 29.59251404 29.59251404 29.59251404]
ene_coms = [0.00852942 0.00875274 0.00806455 0.00762561 0.007806   0.00708791
 0.00780895 0.00791672 0.00685985 0.03002578]
ene_comp = [2.65627448e-05 2.50656771e-04 2.45867149e-05 9.97685278e-07
 3.63581478e-04 3.77877339e-05 1.99847328e-06 7.32801637e-05
 2.63104911e-05 2.16408956e-04]
ene_total = [0.63256058 0.66563918 0.59804576 0.56385014 0.60399319 0.52681726
 0.57747893 0.59071637 0.50910721 2.23586509]
optimize_network iter = 1 obj = 7.504073708928786
eta = 0.6558696840754216
freqs = [5.52514145e+07 1.17033174e+08 5.35835655e+07 1.83296745e+07
 1.31169348e+08 6.12196996e+07 2.31496533e+07 7.69943574e+07
 5.41358549e+07 1.62962924e+08]
Done!
ene_coms = [0.00852942 0.00875274 0.00806455 0.00762561 0.007806   0.00708791
 0.00780895 0.00791672 0.00685985 0.03002578]
ene_comp = [2.50017670e-05 2.35926755e-04 2.31418598e-05 9.39055623e-07
 3.42215364e-04 3.55671119e-05 1.88103163e-06 6.89738049e-05
 2.47643372e-05 2.03691536e-04]
ene_total = [0.00855442 0.00898867 0.00808769 0.00762655 0.00814821 0.00712348
 0.00781083 0.00798569 0.00688461 0.03022947]
At round 8 energy consumption: 0.10143962852981907
At round 8 eta: 0.6558696840754216
At round 8 a_n: 25.442236080499836
At round 8 local rounds: 13.811651093280204
At round 8 global rounds: 73.9319812965153
gradient difference: 0.4144442677497864
train() client id: f_00000-0-0 loss: 1.523589  [   32/  126]
train() client id: f_00000-0-1 loss: 1.289498  [   64/  126]
train() client id: f_00000-0-2 loss: 1.451287  [   96/  126]
train() client id: f_00000-1-0 loss: 1.214187  [   32/  126]
train() client id: f_00000-1-1 loss: 1.266911  [   64/  126]
train() client id: f_00000-1-2 loss: 1.293131  [   96/  126]
train() client id: f_00000-2-0 loss: 1.152755  [   32/  126]
train() client id: f_00000-2-1 loss: 1.189703  [   64/  126]
train() client id: f_00000-2-2 loss: 1.161108  [   96/  126]
train() client id: f_00000-3-0 loss: 1.107809  [   32/  126]
train() client id: f_00000-3-1 loss: 1.088776  [   64/  126]
train() client id: f_00000-3-2 loss: 1.161491  [   96/  126]
train() client id: f_00000-4-0 loss: 1.084289  [   32/  126]
train() client id: f_00000-4-1 loss: 1.027799  [   64/  126]
train() client id: f_00000-4-2 loss: 0.999381  [   96/  126]
train() client id: f_00000-5-0 loss: 0.987102  [   32/  126]
train() client id: f_00000-5-1 loss: 1.037125  [   64/  126]
train() client id: f_00000-5-2 loss: 0.975422  [   96/  126]
train() client id: f_00000-6-0 loss: 0.971682  [   32/  126]
train() client id: f_00000-6-1 loss: 0.956813  [   64/  126]
train() client id: f_00000-6-2 loss: 1.012252  [   96/  126]
train() client id: f_00000-7-0 loss: 1.053021  [   32/  126]
train() client id: f_00000-7-1 loss: 0.952471  [   64/  126]
train() client id: f_00000-7-2 loss: 0.921550  [   96/  126]
train() client id: f_00000-8-0 loss: 1.009433  [   32/  126]
train() client id: f_00000-8-1 loss: 0.968746  [   64/  126]
train() client id: f_00000-8-2 loss: 0.938342  [   96/  126]
train() client id: f_00000-9-0 loss: 0.959461  [   32/  126]
train() client id: f_00000-9-1 loss: 0.992987  [   64/  126]
train() client id: f_00000-9-2 loss: 0.929678  [   96/  126]
train() client id: f_00000-10-0 loss: 1.010236  [   32/  126]
train() client id: f_00000-10-1 loss: 0.954897  [   64/  126]
train() client id: f_00000-10-2 loss: 0.932073  [   96/  126]
train() client id: f_00000-11-0 loss: 0.943162  [   32/  126]
train() client id: f_00000-11-1 loss: 0.954743  [   64/  126]
train() client id: f_00000-11-2 loss: 1.031184  [   96/  126]
train() client id: f_00000-12-0 loss: 0.964441  [   32/  126]
train() client id: f_00000-12-1 loss: 0.937149  [   64/  126]
train() client id: f_00000-12-2 loss: 0.991763  [   96/  126]
train() client id: f_00001-0-0 loss: 0.473680  [   32/  265]
train() client id: f_00001-0-1 loss: 0.474912  [   64/  265]
train() client id: f_00001-0-2 loss: 0.420694  [   96/  265]
train() client id: f_00001-0-3 loss: 0.417470  [  128/  265]
train() client id: f_00001-0-4 loss: 0.539869  [  160/  265]
train() client id: f_00001-0-5 loss: 0.565636  [  192/  265]
train() client id: f_00001-0-6 loss: 0.490622  [  224/  265]
train() client id: f_00001-0-7 loss: 0.489461  [  256/  265]
train() client id: f_00001-1-0 loss: 0.572847  [   32/  265]
train() client id: f_00001-1-1 loss: 0.439970  [   64/  265]
train() client id: f_00001-1-2 loss: 0.484307  [   96/  265]
train() client id: f_00001-1-3 loss: 0.488647  [  128/  265]
train() client id: f_00001-1-4 loss: 0.428363  [  160/  265]
train() client id: f_00001-1-5 loss: 0.510353  [  192/  265]
train() client id: f_00001-1-6 loss: 0.385624  [  224/  265]
train() client id: f_00001-1-7 loss: 0.458911  [  256/  265]
train() client id: f_00001-2-0 loss: 0.444915  [   32/  265]
train() client id: f_00001-2-1 loss: 0.371835  [   64/  265]
train() client id: f_00001-2-2 loss: 0.454402  [   96/  265]
train() client id: f_00001-2-3 loss: 0.424269  [  128/  265]
train() client id: f_00001-2-4 loss: 0.363881  [  160/  265]
train() client id: f_00001-2-5 loss: 0.437253  [  192/  265]
train() client id: f_00001-2-6 loss: 0.655438  [  224/  265]
train() client id: f_00001-2-7 loss: 0.522710  [  256/  265]
train() client id: f_00001-3-0 loss: 0.468266  [   32/  265]
train() client id: f_00001-3-1 loss: 0.418835  [   64/  265]
train() client id: f_00001-3-2 loss: 0.360099  [   96/  265]
train() client id: f_00001-3-3 loss: 0.473094  [  128/  265]
train() client id: f_00001-3-4 loss: 0.479244  [  160/  265]
train() client id: f_00001-3-5 loss: 0.558543  [  192/  265]
train() client id: f_00001-3-6 loss: 0.468171  [  224/  265]
train() client id: f_00001-3-7 loss: 0.355509  [  256/  265]
train() client id: f_00001-4-0 loss: 0.412813  [   32/  265]
train() client id: f_00001-4-1 loss: 0.588698  [   64/  265]
train() client id: f_00001-4-2 loss: 0.438986  [   96/  265]
train() client id: f_00001-4-3 loss: 0.363716  [  128/  265]
train() client id: f_00001-4-4 loss: 0.401571  [  160/  265]
train() client id: f_00001-4-5 loss: 0.432839  [  192/  265]
train() client id: f_00001-4-6 loss: 0.476073  [  224/  265]
train() client id: f_00001-4-7 loss: 0.385894  [  256/  265]
train() client id: f_00001-5-0 loss: 0.355796  [   32/  265]
train() client id: f_00001-5-1 loss: 0.423814  [   64/  265]
train() client id: f_00001-5-2 loss: 0.403495  [   96/  265]
train() client id: f_00001-5-3 loss: 0.525526  [  128/  265]
train() client id: f_00001-5-4 loss: 0.359437  [  160/  265]
train() client id: f_00001-5-5 loss: 0.521561  [  192/  265]
train() client id: f_00001-5-6 loss: 0.396451  [  224/  265]
train() client id: f_00001-5-7 loss: 0.413821  [  256/  265]
train() client id: f_00001-6-0 loss: 0.399579  [   32/  265]
train() client id: f_00001-6-1 loss: 0.506518  [   64/  265]
train() client id: f_00001-6-2 loss: 0.336970  [   96/  265]
train() client id: f_00001-6-3 loss: 0.455293  [  128/  265]
train() client id: f_00001-6-4 loss: 0.533347  [  160/  265]
train() client id: f_00001-6-5 loss: 0.405238  [  192/  265]
train() client id: f_00001-6-6 loss: 0.383592  [  224/  265]
train() client id: f_00001-6-7 loss: 0.409474  [  256/  265]
train() client id: f_00001-7-0 loss: 0.578132  [   32/  265]
train() client id: f_00001-7-1 loss: 0.359074  [   64/  265]
train() client id: f_00001-7-2 loss: 0.380220  [   96/  265]
train() client id: f_00001-7-3 loss: 0.433286  [  128/  265]
train() client id: f_00001-7-4 loss: 0.391939  [  160/  265]
train() client id: f_00001-7-5 loss: 0.382380  [  192/  265]
train() client id: f_00001-7-6 loss: 0.485671  [  224/  265]
train() client id: f_00001-7-7 loss: 0.369092  [  256/  265]
train() client id: f_00001-8-0 loss: 0.444784  [   32/  265]
train() client id: f_00001-8-1 loss: 0.523624  [   64/  265]
train() client id: f_00001-8-2 loss: 0.392446  [   96/  265]
train() client id: f_00001-8-3 loss: 0.442302  [  128/  265]
train() client id: f_00001-8-4 loss: 0.378932  [  160/  265]
train() client id: f_00001-8-5 loss: 0.391934  [  192/  265]
train() client id: f_00001-8-6 loss: 0.362475  [  224/  265]
train() client id: f_00001-8-7 loss: 0.432950  [  256/  265]
train() client id: f_00001-9-0 loss: 0.331678  [   32/  265]
train() client id: f_00001-9-1 loss: 0.465349  [   64/  265]
train() client id: f_00001-9-2 loss: 0.414217  [   96/  265]
train() client id: f_00001-9-3 loss: 0.388128  [  128/  265]
train() client id: f_00001-9-4 loss: 0.460618  [  160/  265]
train() client id: f_00001-9-5 loss: 0.406344  [  192/  265]
train() client id: f_00001-9-6 loss: 0.440798  [  224/  265]
train() client id: f_00001-9-7 loss: 0.381175  [  256/  265]
train() client id: f_00001-10-0 loss: 0.414140  [   32/  265]
train() client id: f_00001-10-1 loss: 0.435002  [   64/  265]
train() client id: f_00001-10-2 loss: 0.440987  [   96/  265]
train() client id: f_00001-10-3 loss: 0.325394  [  128/  265]
train() client id: f_00001-10-4 loss: 0.457009  [  160/  265]
train() client id: f_00001-10-5 loss: 0.340550  [  192/  265]
train() client id: f_00001-10-6 loss: 0.515259  [  224/  265]
train() client id: f_00001-10-7 loss: 0.410922  [  256/  265]
train() client id: f_00001-11-0 loss: 0.381706  [   32/  265]
train() client id: f_00001-11-1 loss: 0.372299  [   64/  265]
train() client id: f_00001-11-2 loss: 0.401547  [   96/  265]
train() client id: f_00001-11-3 loss: 0.624450  [  128/  265]
train() client id: f_00001-11-4 loss: 0.366897  [  160/  265]
train() client id: f_00001-11-5 loss: 0.387586  [  192/  265]
train() client id: f_00001-11-6 loss: 0.330582  [  224/  265]
train() client id: f_00001-11-7 loss: 0.453496  [  256/  265]
train() client id: f_00001-12-0 loss: 0.576058  [   32/  265]
train() client id: f_00001-12-1 loss: 0.383534  [   64/  265]
train() client id: f_00001-12-2 loss: 0.449846  [   96/  265]
train() client id: f_00001-12-3 loss: 0.324013  [  128/  265]
train() client id: f_00001-12-4 loss: 0.448599  [  160/  265]
train() client id: f_00001-12-5 loss: 0.356839  [  192/  265]
train() client id: f_00001-12-6 loss: 0.444359  [  224/  265]
train() client id: f_00001-12-7 loss: 0.322369  [  256/  265]
train() client id: f_00002-0-0 loss: 1.153397  [   32/  124]
train() client id: f_00002-0-1 loss: 1.095600  [   64/  124]
train() client id: f_00002-0-2 loss: 1.250782  [   96/  124]
train() client id: f_00002-1-0 loss: 1.136333  [   32/  124]
train() client id: f_00002-1-1 loss: 1.154619  [   64/  124]
train() client id: f_00002-1-2 loss: 1.099167  [   96/  124]
train() client id: f_00002-2-0 loss: 1.137303  [   32/  124]
train() client id: f_00002-2-1 loss: 1.055068  [   64/  124]
train() client id: f_00002-2-2 loss: 1.116700  [   96/  124]
train() client id: f_00002-3-0 loss: 1.069394  [   32/  124]
train() client id: f_00002-3-1 loss: 1.062737  [   64/  124]
train() client id: f_00002-3-2 loss: 1.065739  [   96/  124]
train() client id: f_00002-4-0 loss: 1.064368  [   32/  124]
train() client id: f_00002-4-1 loss: 1.020214  [   64/  124]
train() client id: f_00002-4-2 loss: 1.070047  [   96/  124]
train() client id: f_00002-5-0 loss: 0.950518  [   32/  124]
train() client id: f_00002-5-1 loss: 0.998775  [   64/  124]
train() client id: f_00002-5-2 loss: 1.005128  [   96/  124]
train() client id: f_00002-6-0 loss: 0.990886  [   32/  124]
train() client id: f_00002-6-1 loss: 1.023422  [   64/  124]
train() client id: f_00002-6-2 loss: 0.956394  [   96/  124]
train() client id: f_00002-7-0 loss: 0.943268  [   32/  124]
train() client id: f_00002-7-1 loss: 1.011309  [   64/  124]
train() client id: f_00002-7-2 loss: 0.932640  [   96/  124]
train() client id: f_00002-8-0 loss: 1.036370  [   32/  124]
train() client id: f_00002-8-1 loss: 0.826220  [   64/  124]
train() client id: f_00002-8-2 loss: 0.906917  [   96/  124]
train() client id: f_00002-9-0 loss: 1.010570  [   32/  124]
train() client id: f_00002-9-1 loss: 0.850713  [   64/  124]
train() client id: f_00002-9-2 loss: 0.925727  [   96/  124]
train() client id: f_00002-10-0 loss: 0.949495  [   32/  124]
train() client id: f_00002-10-1 loss: 0.940088  [   64/  124]
train() client id: f_00002-10-2 loss: 0.836577  [   96/  124]
train() client id: f_00002-11-0 loss: 0.877700  [   32/  124]
train() client id: f_00002-11-1 loss: 0.975409  [   64/  124]
train() client id: f_00002-11-2 loss: 0.858101  [   96/  124]
train() client id: f_00002-12-0 loss: 0.885498  [   32/  124]
train() client id: f_00002-12-1 loss: 0.913989  [   64/  124]
train() client id: f_00002-12-2 loss: 0.935950  [   96/  124]
train() client id: f_00003-0-0 loss: 0.941805  [   32/   43]
train() client id: f_00003-1-0 loss: 0.773149  [   32/   43]
train() client id: f_00003-2-0 loss: 0.912118  [   32/   43]
train() client id: f_00003-3-0 loss: 0.867668  [   32/   43]
train() client id: f_00003-4-0 loss: 0.863466  [   32/   43]
train() client id: f_00003-5-0 loss: 0.971769  [   32/   43]
train() client id: f_00003-6-0 loss: 0.832876  [   32/   43]
train() client id: f_00003-7-0 loss: 0.841050  [   32/   43]
train() client id: f_00003-8-0 loss: 0.929674  [   32/   43]
train() client id: f_00003-9-0 loss: 0.822211  [   32/   43]
train() client id: f_00003-10-0 loss: 0.853224  [   32/   43]
train() client id: f_00003-11-0 loss: 0.819540  [   32/   43]
train() client id: f_00003-12-0 loss: 0.863965  [   32/   43]
train() client id: f_00004-0-0 loss: 0.841939  [   32/  306]
train() client id: f_00004-0-1 loss: 1.027682  [   64/  306]
train() client id: f_00004-0-2 loss: 0.974829  [   96/  306]
train() client id: f_00004-0-3 loss: 0.947071  [  128/  306]
train() client id: f_00004-0-4 loss: 0.851201  [  160/  306]
train() client id: f_00004-0-5 loss: 0.920457  [  192/  306]
train() client id: f_00004-0-6 loss: 0.829071  [  224/  306]
train() client id: f_00004-0-7 loss: 1.035167  [  256/  306]
train() client id: f_00004-0-8 loss: 0.906445  [  288/  306]
train() client id: f_00004-1-0 loss: 1.095685  [   32/  306]
train() client id: f_00004-1-1 loss: 0.799514  [   64/  306]
train() client id: f_00004-1-2 loss: 1.134434  [   96/  306]
train() client id: f_00004-1-3 loss: 0.957023  [  128/  306]
train() client id: f_00004-1-4 loss: 0.970505  [  160/  306]
train() client id: f_00004-1-5 loss: 0.745591  [  192/  306]
train() client id: f_00004-1-6 loss: 0.845625  [  224/  306]
train() client id: f_00004-1-7 loss: 0.884572  [  256/  306]
train() client id: f_00004-1-8 loss: 0.917794  [  288/  306]
train() client id: f_00004-2-0 loss: 0.923949  [   32/  306]
train() client id: f_00004-2-1 loss: 0.884523  [   64/  306]
train() client id: f_00004-2-2 loss: 0.992360  [   96/  306]
train() client id: f_00004-2-3 loss: 0.883012  [  128/  306]
train() client id: f_00004-2-4 loss: 0.931538  [  160/  306]
train() client id: f_00004-2-5 loss: 0.955683  [  192/  306]
train() client id: f_00004-2-6 loss: 0.759134  [  224/  306]
train() client id: f_00004-2-7 loss: 0.991758  [  256/  306]
train() client id: f_00004-2-8 loss: 0.919553  [  288/  306]
train() client id: f_00004-3-0 loss: 0.874887  [   32/  306]
train() client id: f_00004-3-1 loss: 0.919498  [   64/  306]
train() client id: f_00004-3-2 loss: 0.935045  [   96/  306]
train() client id: f_00004-3-3 loss: 0.996760  [  128/  306]
train() client id: f_00004-3-4 loss: 0.918648  [  160/  306]
train() client id: f_00004-3-5 loss: 0.867848  [  192/  306]
train() client id: f_00004-3-6 loss: 0.865117  [  224/  306]
train() client id: f_00004-3-7 loss: 0.958940  [  256/  306]
train() client id: f_00004-3-8 loss: 0.883514  [  288/  306]
train() client id: f_00004-4-0 loss: 0.813971  [   32/  306]
train() client id: f_00004-4-1 loss: 0.919573  [   64/  306]
train() client id: f_00004-4-2 loss: 0.847145  [   96/  306]
train() client id: f_00004-4-3 loss: 0.972440  [  128/  306]
train() client id: f_00004-4-4 loss: 1.003727  [  160/  306]
train() client id: f_00004-4-5 loss: 0.917297  [  192/  306]
train() client id: f_00004-4-6 loss: 1.076620  [  224/  306]
train() client id: f_00004-4-7 loss: 0.943693  [  256/  306]
train() client id: f_00004-4-8 loss: 0.820423  [  288/  306]
train() client id: f_00004-5-0 loss: 0.885185  [   32/  306]
train() client id: f_00004-5-1 loss: 0.937642  [   64/  306]
train() client id: f_00004-5-2 loss: 0.928602  [   96/  306]
train() client id: f_00004-5-3 loss: 0.975244  [  128/  306]
train() client id: f_00004-5-4 loss: 0.892975  [  160/  306]
train() client id: f_00004-5-5 loss: 0.814915  [  192/  306]
train() client id: f_00004-5-6 loss: 0.987299  [  224/  306]
train() client id: f_00004-5-7 loss: 0.890038  [  256/  306]
train() client id: f_00004-5-8 loss: 0.878073  [  288/  306]
train() client id: f_00004-6-0 loss: 1.033051  [   32/  306]
train() client id: f_00004-6-1 loss: 0.744693  [   64/  306]
train() client id: f_00004-6-2 loss: 0.907715  [   96/  306]
train() client id: f_00004-6-3 loss: 0.924887  [  128/  306]
train() client id: f_00004-6-4 loss: 1.071323  [  160/  306]
train() client id: f_00004-6-5 loss: 0.814589  [  192/  306]
train() client id: f_00004-6-6 loss: 1.021903  [  224/  306]
train() client id: f_00004-6-7 loss: 0.929287  [  256/  306]
train() client id: f_00004-6-8 loss: 0.795946  [  288/  306]
train() client id: f_00004-7-0 loss: 0.841553  [   32/  306]
train() client id: f_00004-7-1 loss: 0.958511  [   64/  306]
train() client id: f_00004-7-2 loss: 0.908180  [   96/  306]
train() client id: f_00004-7-3 loss: 0.902554  [  128/  306]
train() client id: f_00004-7-4 loss: 0.997582  [  160/  306]
train() client id: f_00004-7-5 loss: 0.931723  [  192/  306]
train() client id: f_00004-7-6 loss: 0.936888  [  224/  306]
train() client id: f_00004-7-7 loss: 0.948517  [  256/  306]
train() client id: f_00004-7-8 loss: 0.825125  [  288/  306]
train() client id: f_00004-8-0 loss: 0.739602  [   32/  306]
train() client id: f_00004-8-1 loss: 0.924719  [   64/  306]
train() client id: f_00004-8-2 loss: 0.910846  [   96/  306]
train() client id: f_00004-8-3 loss: 1.046674  [  128/  306]
train() client id: f_00004-8-4 loss: 0.952247  [  160/  306]
train() client id: f_00004-8-5 loss: 0.883974  [  192/  306]
train() client id: f_00004-8-6 loss: 0.872606  [  224/  306]
train() client id: f_00004-8-7 loss: 0.927296  [  256/  306]
train() client id: f_00004-8-8 loss: 0.902514  [  288/  306]
train() client id: f_00004-9-0 loss: 0.860524  [   32/  306]
train() client id: f_00004-9-1 loss: 0.963528  [   64/  306]
train() client id: f_00004-9-2 loss: 0.963172  [   96/  306]
train() client id: f_00004-9-3 loss: 0.867579  [  128/  306]
train() client id: f_00004-9-4 loss: 0.841061  [  160/  306]
train() client id: f_00004-9-5 loss: 0.901322  [  192/  306]
train() client id: f_00004-9-6 loss: 0.977635  [  224/  306]
train() client id: f_00004-9-7 loss: 0.842667  [  256/  306]
train() client id: f_00004-9-8 loss: 0.928413  [  288/  306]
train() client id: f_00004-10-0 loss: 0.949721  [   32/  306]
train() client id: f_00004-10-1 loss: 0.845238  [   64/  306]
train() client id: f_00004-10-2 loss: 0.771127  [   96/  306]
train() client id: f_00004-10-3 loss: 1.187732  [  128/  306]
train() client id: f_00004-10-4 loss: 0.786433  [  160/  306]
train() client id: f_00004-10-5 loss: 0.958543  [  192/  306]
train() client id: f_00004-10-6 loss: 0.808386  [  224/  306]
train() client id: f_00004-10-7 loss: 0.903459  [  256/  306]
train() client id: f_00004-10-8 loss: 0.906533  [  288/  306]
train() client id: f_00004-11-0 loss: 0.830742  [   32/  306]
train() client id: f_00004-11-1 loss: 0.889849  [   64/  306]
train() client id: f_00004-11-2 loss: 0.876859  [   96/  306]
train() client id: f_00004-11-3 loss: 0.996183  [  128/  306]
train() client id: f_00004-11-4 loss: 1.003229  [  160/  306]
train() client id: f_00004-11-5 loss: 0.879263  [  192/  306]
train() client id: f_00004-11-6 loss: 0.964266  [  224/  306]
train() client id: f_00004-11-7 loss: 0.862440  [  256/  306]
train() client id: f_00004-11-8 loss: 0.876148  [  288/  306]
train() client id: f_00004-12-0 loss: 0.966232  [   32/  306]
train() client id: f_00004-12-1 loss: 0.912424  [   64/  306]
train() client id: f_00004-12-2 loss: 0.904234  [   96/  306]
train() client id: f_00004-12-3 loss: 0.945796  [  128/  306]
train() client id: f_00004-12-4 loss: 0.962301  [  160/  306]
train() client id: f_00004-12-5 loss: 0.825231  [  192/  306]
train() client id: f_00004-12-6 loss: 0.862968  [  224/  306]
train() client id: f_00004-12-7 loss: 0.889436  [  256/  306]
train() client id: f_00004-12-8 loss: 0.874223  [  288/  306]
train() client id: f_00005-0-0 loss: 0.951757  [   32/  146]
train() client id: f_00005-0-1 loss: 0.896007  [   64/  146]
train() client id: f_00005-0-2 loss: 0.975862  [   96/  146]
train() client id: f_00005-0-3 loss: 0.879146  [  128/  146]
train() client id: f_00005-1-0 loss: 1.020999  [   32/  146]
train() client id: f_00005-1-1 loss: 0.950730  [   64/  146]
train() client id: f_00005-1-2 loss: 0.851592  [   96/  146]
train() client id: f_00005-1-3 loss: 0.860819  [  128/  146]
train() client id: f_00005-2-0 loss: 0.944215  [   32/  146]
train() client id: f_00005-2-1 loss: 0.811462  [   64/  146]
train() client id: f_00005-2-2 loss: 1.082149  [   96/  146]
train() client id: f_00005-2-3 loss: 0.818084  [  128/  146]
train() client id: f_00005-3-0 loss: 0.899857  [   32/  146]
train() client id: f_00005-3-1 loss: 1.040647  [   64/  146]
train() client id: f_00005-3-2 loss: 0.963262  [   96/  146]
train() client id: f_00005-3-3 loss: 0.802392  [  128/  146]
train() client id: f_00005-4-0 loss: 1.251009  [   32/  146]
train() client id: f_00005-4-1 loss: 0.839768  [   64/  146]
train() client id: f_00005-4-2 loss: 0.852650  [   96/  146]
train() client id: f_00005-4-3 loss: 0.760365  [  128/  146]
train() client id: f_00005-5-0 loss: 0.964286  [   32/  146]
train() client id: f_00005-5-1 loss: 0.972692  [   64/  146]
train() client id: f_00005-5-2 loss: 0.940766  [   96/  146]
train() client id: f_00005-5-3 loss: 0.804157  [  128/  146]
train() client id: f_00005-6-0 loss: 0.729721  [   32/  146]
train() client id: f_00005-6-1 loss: 0.965781  [   64/  146]
train() client id: f_00005-6-2 loss: 0.893601  [   96/  146]
train() client id: f_00005-6-3 loss: 1.137493  [  128/  146]
train() client id: f_00005-7-0 loss: 0.925577  [   32/  146]
train() client id: f_00005-7-1 loss: 0.974711  [   64/  146]
train() client id: f_00005-7-2 loss: 0.961210  [   96/  146]
train() client id: f_00005-7-3 loss: 0.941119  [  128/  146]
train() client id: f_00005-8-0 loss: 0.917842  [   32/  146]
train() client id: f_00005-8-1 loss: 0.887362  [   64/  146]
train() client id: f_00005-8-2 loss: 1.147979  [   96/  146]
train() client id: f_00005-8-3 loss: 0.665839  [  128/  146]
train() client id: f_00005-9-0 loss: 1.004166  [   32/  146]
train() client id: f_00005-9-1 loss: 1.013423  [   64/  146]
train() client id: f_00005-9-2 loss: 0.891281  [   96/  146]
train() client id: f_00005-9-3 loss: 0.864584  [  128/  146]
train() client id: f_00005-10-0 loss: 0.850976  [   32/  146]
train() client id: f_00005-10-1 loss: 0.956411  [   64/  146]
train() client id: f_00005-10-2 loss: 0.993768  [   96/  146]
train() client id: f_00005-10-3 loss: 0.977758  [  128/  146]
train() client id: f_00005-11-0 loss: 1.028157  [   32/  146]
train() client id: f_00005-11-1 loss: 0.838956  [   64/  146]
train() client id: f_00005-11-2 loss: 1.030537  [   96/  146]
train() client id: f_00005-11-3 loss: 0.858279  [  128/  146]
train() client id: f_00005-12-0 loss: 0.975116  [   32/  146]
train() client id: f_00005-12-1 loss: 0.902949  [   64/  146]
train() client id: f_00005-12-2 loss: 0.889407  [   96/  146]
train() client id: f_00005-12-3 loss: 0.906021  [  128/  146]
train() client id: f_00006-0-0 loss: 0.702410  [   32/   54]
train() client id: f_00006-1-0 loss: 0.749031  [   32/   54]
train() client id: f_00006-2-0 loss: 0.726746  [   32/   54]
train() client id: f_00006-3-0 loss: 0.664442  [   32/   54]
train() client id: f_00006-4-0 loss: 0.763581  [   32/   54]
train() client id: f_00006-5-0 loss: 0.758256  [   32/   54]
train() client id: f_00006-6-0 loss: 0.774532  [   32/   54]
train() client id: f_00006-7-0 loss: 0.762705  [   32/   54]
train() client id: f_00006-8-0 loss: 0.682177  [   32/   54]
train() client id: f_00006-9-0 loss: 0.768813  [   32/   54]
train() client id: f_00006-10-0 loss: 0.728835  [   32/   54]
train() client id: f_00006-11-0 loss: 0.717972  [   32/   54]
train() client id: f_00006-12-0 loss: 0.726348  [   32/   54]
train() client id: f_00007-0-0 loss: 0.566647  [   32/  179]
train() client id: f_00007-0-1 loss: 0.515083  [   64/  179]
train() client id: f_00007-0-2 loss: 0.615713  [   96/  179]
train() client id: f_00007-0-3 loss: 0.740303  [  128/  179]
train() client id: f_00007-0-4 loss: 0.644348  [  160/  179]
train() client id: f_00007-1-0 loss: 0.710237  [   32/  179]
train() client id: f_00007-1-1 loss: 0.494389  [   64/  179]
train() client id: f_00007-1-2 loss: 0.492447  [   96/  179]
train() client id: f_00007-1-3 loss: 0.670089  [  128/  179]
train() client id: f_00007-1-4 loss: 0.630980  [  160/  179]
train() client id: f_00007-2-0 loss: 0.562335  [   32/  179]
train() client id: f_00007-2-1 loss: 0.466109  [   64/  179]
train() client id: f_00007-2-2 loss: 0.727989  [   96/  179]
train() client id: f_00007-2-3 loss: 0.474372  [  128/  179]
train() client id: f_00007-2-4 loss: 0.573412  [  160/  179]
train() client id: f_00007-3-0 loss: 0.577732  [   32/  179]
train() client id: f_00007-3-1 loss: 0.517526  [   64/  179]
train() client id: f_00007-3-2 loss: 0.631710  [   96/  179]
train() client id: f_00007-3-3 loss: 0.554713  [  128/  179]
train() client id: f_00007-3-4 loss: 0.464572  [  160/  179]
train() client id: f_00007-4-0 loss: 0.558138  [   32/  179]
train() client id: f_00007-4-1 loss: 0.498522  [   64/  179]
train() client id: f_00007-4-2 loss: 0.590511  [   96/  179]
train() client id: f_00007-4-3 loss: 0.535698  [  128/  179]
train() client id: f_00007-4-4 loss: 0.541023  [  160/  179]
train() client id: f_00007-5-0 loss: 0.461626  [   32/  179]
train() client id: f_00007-5-1 loss: 0.492872  [   64/  179]
train() client id: f_00007-5-2 loss: 0.601575  [   96/  179]
train() client id: f_00007-5-3 loss: 0.569076  [  128/  179]
train() client id: f_00007-5-4 loss: 0.500696  [  160/  179]
train() client id: f_00007-6-0 loss: 0.557880  [   32/  179]
train() client id: f_00007-6-1 loss: 0.591285  [   64/  179]
train() client id: f_00007-6-2 loss: 0.501074  [   96/  179]
train() client id: f_00007-6-3 loss: 0.486934  [  128/  179]
train() client id: f_00007-6-4 loss: 0.491947  [  160/  179]
train() client id: f_00007-7-0 loss: 0.493263  [   32/  179]
train() client id: f_00007-7-1 loss: 0.597149  [   64/  179]
train() client id: f_00007-7-2 loss: 0.471874  [   96/  179]
train() client id: f_00007-7-3 loss: 0.591508  [  128/  179]
train() client id: f_00007-7-4 loss: 0.486200  [  160/  179]
train() client id: f_00007-8-0 loss: 0.470893  [   32/  179]
train() client id: f_00007-8-1 loss: 0.493891  [   64/  179]
train() client id: f_00007-8-2 loss: 0.535720  [   96/  179]
train() client id: f_00007-8-3 loss: 0.442639  [  128/  179]
train() client id: f_00007-8-4 loss: 0.523983  [  160/  179]
train() client id: f_00007-9-0 loss: 0.460519  [   32/  179]
train() client id: f_00007-9-1 loss: 0.400779  [   64/  179]
train() client id: f_00007-9-2 loss: 0.550601  [   96/  179]
train() client id: f_00007-9-3 loss: 0.483155  [  128/  179]
train() client id: f_00007-9-4 loss: 0.551028  [  160/  179]
train() client id: f_00007-10-0 loss: 0.388155  [   32/  179]
train() client id: f_00007-10-1 loss: 0.590400  [   64/  179]
train() client id: f_00007-10-2 loss: 0.609586  [   96/  179]
train() client id: f_00007-10-3 loss: 0.532013  [  128/  179]
train() client id: f_00007-10-4 loss: 0.396405  [  160/  179]
train() client id: f_00007-11-0 loss: 0.470593  [   32/  179]
train() client id: f_00007-11-1 loss: 0.460660  [   64/  179]
train() client id: f_00007-11-2 loss: 0.631363  [   96/  179]
train() client id: f_00007-11-3 loss: 0.520741  [  128/  179]
train() client id: f_00007-11-4 loss: 0.391969  [  160/  179]
train() client id: f_00007-12-0 loss: 0.538961  [   32/  179]
train() client id: f_00007-12-1 loss: 0.441159  [   64/  179]
train() client id: f_00007-12-2 loss: 0.498420  [   96/  179]
train() client id: f_00007-12-3 loss: 0.430502  [  128/  179]
train() client id: f_00007-12-4 loss: 0.450959  [  160/  179]
train() client id: f_00008-0-0 loss: 0.701193  [   32/  130]
train() client id: f_00008-0-1 loss: 0.828121  [   64/  130]
train() client id: f_00008-0-2 loss: 0.809093  [   96/  130]
train() client id: f_00008-0-3 loss: 0.748449  [  128/  130]
train() client id: f_00008-1-0 loss: 0.762609  [   32/  130]
train() client id: f_00008-1-1 loss: 0.752871  [   64/  130]
train() client id: f_00008-1-2 loss: 0.782934  [   96/  130]
train() client id: f_00008-1-3 loss: 0.792092  [  128/  130]
train() client id: f_00008-2-0 loss: 0.846130  [   32/  130]
train() client id: f_00008-2-1 loss: 0.722146  [   64/  130]
train() client id: f_00008-2-2 loss: 0.750847  [   96/  130]
train() client id: f_00008-2-3 loss: 0.768632  [  128/  130]
train() client id: f_00008-3-0 loss: 0.755443  [   32/  130]
train() client id: f_00008-3-1 loss: 0.808761  [   64/  130]
train() client id: f_00008-3-2 loss: 0.735244  [   96/  130]
train() client id: f_00008-3-3 loss: 0.755418  [  128/  130]
train() client id: f_00008-4-0 loss: 0.771626  [   32/  130]
train() client id: f_00008-4-1 loss: 0.761725  [   64/  130]
train() client id: f_00008-4-2 loss: 0.762272  [   96/  130]
train() client id: f_00008-4-3 loss: 0.787936  [  128/  130]
train() client id: f_00008-5-0 loss: 0.799506  [   32/  130]
train() client id: f_00008-5-1 loss: 0.744241  [   64/  130]
train() client id: f_00008-5-2 loss: 0.801088  [   96/  130]
train() client id: f_00008-5-3 loss: 0.726997  [  128/  130]
train() client id: f_00008-6-0 loss: 0.842934  [   32/  130]
train() client id: f_00008-6-1 loss: 0.701021  [   64/  130]
train() client id: f_00008-6-2 loss: 0.764036  [   96/  130]
train() client id: f_00008-6-3 loss: 0.748898  [  128/  130]
train() client id: f_00008-7-0 loss: 0.803135  [   32/  130]
train() client id: f_00008-7-1 loss: 0.719182  [   64/  130]
train() client id: f_00008-7-2 loss: 0.756584  [   96/  130]
train() client id: f_00008-7-3 loss: 0.794855  [  128/  130]
train() client id: f_00008-8-0 loss: 0.706033  [   32/  130]
train() client id: f_00008-8-1 loss: 0.865953  [   64/  130]
train() client id: f_00008-8-2 loss: 0.786029  [   96/  130]
train() client id: f_00008-8-3 loss: 0.700966  [  128/  130]
train() client id: f_00008-9-0 loss: 0.706888  [   32/  130]
train() client id: f_00008-9-1 loss: 0.832355  [   64/  130]
train() client id: f_00008-9-2 loss: 0.790870  [   96/  130]
train() client id: f_00008-9-3 loss: 0.734847  [  128/  130]
train() client id: f_00008-10-0 loss: 0.733539  [   32/  130]
train() client id: f_00008-10-1 loss: 0.768125  [   64/  130]
train() client id: f_00008-10-2 loss: 0.721259  [   96/  130]
train() client id: f_00008-10-3 loss: 0.836922  [  128/  130]
train() client id: f_00008-11-0 loss: 0.725098  [   32/  130]
train() client id: f_00008-11-1 loss: 0.726704  [   64/  130]
train() client id: f_00008-11-2 loss: 0.779772  [   96/  130]
train() client id: f_00008-11-3 loss: 0.809219  [  128/  130]
train() client id: f_00008-12-0 loss: 0.812511  [   32/  130]
train() client id: f_00008-12-1 loss: 0.767806  [   64/  130]
train() client id: f_00008-12-2 loss: 0.768712  [   96/  130]
train() client id: f_00008-12-3 loss: 0.716972  [  128/  130]
train() client id: f_00009-0-0 loss: 1.288853  [   32/  118]
train() client id: f_00009-0-1 loss: 1.138200  [   64/  118]
train() client id: f_00009-0-2 loss: 1.201328  [   96/  118]
train() client id: f_00009-1-0 loss: 1.083348  [   32/  118]
train() client id: f_00009-1-1 loss: 1.228851  [   64/  118]
train() client id: f_00009-1-2 loss: 1.095776  [   96/  118]
train() client id: f_00009-2-0 loss: 0.972357  [   32/  118]
train() client id: f_00009-2-1 loss: 1.072039  [   64/  118]
train() client id: f_00009-2-2 loss: 1.091818  [   96/  118]
train() client id: f_00009-3-0 loss: 1.091190  [   32/  118]
train() client id: f_00009-3-1 loss: 1.048149  [   64/  118]
train() client id: f_00009-3-2 loss: 1.077170  [   96/  118]
train() client id: f_00009-4-0 loss: 1.076517  [   32/  118]
train() client id: f_00009-4-1 loss: 1.056849  [   64/  118]
train() client id: f_00009-4-2 loss: 0.993496  [   96/  118]
train() client id: f_00009-5-0 loss: 0.941292  [   32/  118]
train() client id: f_00009-5-1 loss: 1.051336  [   64/  118]
train() client id: f_00009-5-2 loss: 1.023434  [   96/  118]
train() client id: f_00009-6-0 loss: 0.982772  [   32/  118]
train() client id: f_00009-6-1 loss: 1.011930  [   64/  118]
train() client id: f_00009-6-2 loss: 0.895769  [   96/  118]
train() client id: f_00009-7-0 loss: 1.003517  [   32/  118]
train() client id: f_00009-7-1 loss: 0.957783  [   64/  118]
train() client id: f_00009-7-2 loss: 0.943845  [   96/  118]
train() client id: f_00009-8-0 loss: 0.957065  [   32/  118]
train() client id: f_00009-8-1 loss: 0.884224  [   64/  118]
train() client id: f_00009-8-2 loss: 0.968731  [   96/  118]
train() client id: f_00009-9-0 loss: 1.034168  [   32/  118]
train() client id: f_00009-9-1 loss: 0.924401  [   64/  118]
train() client id: f_00009-9-2 loss: 0.784996  [   96/  118]
train() client id: f_00009-10-0 loss: 0.958181  [   32/  118]
train() client id: f_00009-10-1 loss: 0.898498  [   64/  118]
train() client id: f_00009-10-2 loss: 0.869859  [   96/  118]
train() client id: f_00009-11-0 loss: 0.786486  [   32/  118]
train() client id: f_00009-11-1 loss: 1.001127  [   64/  118]
train() client id: f_00009-11-2 loss: 0.936761  [   96/  118]
train() client id: f_00009-12-0 loss: 0.926169  [   32/  118]
train() client id: f_00009-12-1 loss: 0.890628  [   64/  118]
train() client id: f_00009-12-2 loss: 0.872932  [   96/  118]
At round 8 accuracy: 0.623342175066313
At round 8 training accuracy: 0.5700871898054997
At round 8 training loss: 0.8779846940158785
update_location
xs = 8.927491 161.223621 5.882650 10.934260 -77.581990 74.769243 -5.849135 -5.143845 -100.120581 20.134486 
ys = -152.390647 7.291448 50.684448 -102.290817 -9.642386 0.794442 -111.381692 46.628436 25.881276 -587.232496 
xs mean: 9.317620029478787
ys mean: -83.16579882624052
dists_uav = 182.490026 189.858424 112.265395 143.467660 126.932820 124.864209 149.800180 110.456644 143.853993 596.026343 
uav_gains = -106.585088 -107.047075 -101.256231 -103.921365 -102.589818 -102.411323 -104.392280 -101.079862 -103.950655 -126.297353 
uav_gains_db_mean: -105.95310504214791
dists_bs = 375.518252 375.729266 219.483233 333.858872 208.765609 304.519705 332.605606 212.909304 166.863181 786.813730 
bs_gains = -111.656729 -111.663560 -105.126297 -110.226821 -104.517510 -109.108290 -110.181087 -104.756509 -101.793154 -120.651473 
bs_gains_db_mean: -108.96814294572383
Round 9
-------------------------------
ene_coms = [0.00865563 0.00888615 0.00799805 0.007524   0.00775149 0.00700231
 0.00770259 0.0078465  0.00680869 0.03033532]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.85386543 20.48815446  9.68426644  3.47869369 23.59439588 11.34122097
  4.32432154 13.88592372 10.11289411  9.79180652]
obj_prev = 116.55554276760321
eta_min = 2.946535530009937e-10	eta_max = 0.737255591473257
af = 24.50880095055506	bf = 2.5227879081144007	zeta = 26.95968104561057	eta = 0.909090909090909
af = 24.50880095055506	bf = 2.5227879081144007	zeta = 53.44598746850227	eta = 0.45857139350262766
af = 24.50880095055506	bf = 2.5227879081144007	zeta = 39.95136529702159	eta = 0.6134659170804913
af = 24.50880095055506	bf = 2.5227879081144007	zeta = 37.50907865061097	eta = 0.6534098365584847
af = 24.50880095055506	bf = 2.5227879081144007	zeta = 37.37130276674642	eta = 0.6558187469012555
af = 24.50880095055506	bf = 2.5227879081144007	zeta = 37.370821214305465	eta = 0.6558271976419171
af = 24.50880095055506	bf = 2.5227879081144007	zeta = 37.37082120839353	eta = 0.6558271977456668
eta = 0.6558271977456668
ene_coms = [0.00865563 0.00888615 0.00799805 0.007524   0.00775149 0.00700231
 0.00770259 0.0078465  0.00680869 0.03033532]
ene_comp = [0.03481071 0.07321299 0.03425816 0.01187984 0.08454029 0.04033622
 0.01491887 0.0494533  0.03591581 0.0326005 ]
ene_total = [3.16989467 5.98729201 3.08164286 1.41507532 6.73061579 3.45228385
 1.64972883 4.17873597 3.11579431 4.5897576 ]
ti_comp = [0.31852472 0.31621951 0.32510051 0.32984102 0.32756615 0.33505795
 0.32805509 0.32661604 0.33699411 0.10172778]
ti_coms = [0.08655628 0.0888615  0.0799805  0.07523998 0.07751486 0.07002305
 0.07702592 0.07846496 0.0680869  0.30335323]
t_total = [29.54157829 29.54157829 29.54157829 29.54157829 29.54157829 29.54157829
 29.54157829 29.54157829 29.54157829 29.54157829]
ene_coms = [0.00865563 0.00888615 0.00799805 0.007524   0.00775149 0.00700231
 0.00770259 0.0078465  0.00680869 0.03033532]
ene_comp = [2.59855686e-05 2.45282858e-04 2.37758824e-05 9.63169936e-07
 3.51943650e-04 3.65363867e-05 1.92838887e-06 7.08583374e-05
 2.54971903e-05 2.09254022e-04]
ene_total = [0.6331291  0.66593332 0.58501234 0.54877725 0.5909635  0.51332571
 0.56187205 0.57739354 0.49840072 2.22754215]
optimize_network iter = 0 obj = 7.402349700586152
eta = 0.6558271977456668
freqs = [5.46436486e+07 1.15762931e+08 5.26885601e+07 1.80084395e+07
 1.29043075e+08 6.01928935e+07 2.27383674e+07 7.57055666e+07
 5.32884805e+07 1.60234029e+08]
eta_min = 0.6558271977456908	eta_max = 0.6558271977456669
af = 0.05902349837730659	bf = 2.5227879081144007	zeta = 0.06492584821503726	eta = 0.909090909090909
af = 0.05902349837730659	bf = 2.5227879081144007	zeta = 27.812547931931597	eta = 0.002122189542711464
af = 0.05902349837730659	bf = 2.5227879081144007	zeta = 2.8922335238645664	eta = 0.020407583927884262
af = 0.05902349837730659	bf = 2.5227879081144007	zeta = 2.809840488309169	eta = 0.021005996113617174
af = 0.05902349837730659	bf = 2.5227879081144007	zeta = 2.809815549435106	eta = 0.021006182554998268
eta = 0.021006182554998268
ene_coms = [0.00865563 0.00888615 0.00799805 0.007524   0.00775149 0.00700231
 0.00770259 0.0078465  0.00680869 0.03033532]
ene_comp = [2.37948946e-04 2.24604658e-03 2.17714926e-04 8.81971353e-06
 3.22273573e-03 3.34562419e-04 1.76581897e-05 6.48847326e-04
 2.33476882e-04 1.91613179e-03]
ene_total = [0.22801577 0.28541006 0.21063784 0.19312829 0.28135985 0.18810447
 0.19793372 0.21780577 0.18054884 0.82687093]
ti_comp = [0.31852472 0.31621951 0.32510051 0.32984102 0.32756615 0.33505795
 0.32805509 0.32661604 0.33699411 0.10172778]
ti_coms = [0.08655628 0.0888615  0.0799805  0.07523998 0.07751486 0.07002305
 0.07702592 0.07846496 0.0680869  0.30335323]
t_total = [29.54157829 29.54157829 29.54157829 29.54157829 29.54157829 29.54157829
 29.54157829 29.54157829 29.54157829 29.54157829]
ene_coms = [0.00865563 0.00888615 0.00799805 0.007524   0.00775149 0.00700231
 0.00770259 0.0078465  0.00680869 0.03033532]
ene_comp = [2.59855686e-05 2.45282858e-04 2.37758824e-05 9.63169936e-07
 3.51943650e-04 3.65363867e-05 1.92838887e-06 7.08583374e-05
 2.54971903e-05 2.09254022e-04]
ene_total = [0.6331291  0.66593332 0.58501234 0.54877725 0.5909635  0.51332571
 0.56187205 0.57739354 0.49840072 2.22754215]
optimize_network iter = 1 obj = 7.402349700586667
eta = 0.6558271977456908
freqs = [5.46436486e+07 1.15762931e+08 5.26885601e+07 1.80084395e+07
 1.29043075e+08 6.01928935e+07 2.27383674e+07 7.57055666e+07
 5.32884805e+07 1.60234029e+08]
Done!
ene_coms = [0.00865563 0.00888615 0.00799805 0.007524   0.00775149 0.00700231
 0.00770259 0.0078465  0.00680869 0.03033532]
ene_comp = [2.44547531e-05 2.30833191e-04 2.23752399e-05 9.06429385e-07
 3.31210573e-04 3.43840202e-05 1.81478706e-06 6.66840573e-05
 2.39951452e-05 1.96926822e-04]
ene_total = [0.00868008 0.00911698 0.00802042 0.0075249  0.0080827  0.00703669
 0.00770441 0.00791318 0.00683268 0.03053225]
At round 9 energy consumption: 0.10144430386140213
At round 9 eta: 0.6558271977456908
At round 9 a_n: 25.09969023353052
At round 9 local rounds: 13.813772342955486
At round 9 global rounds: 72.92758192724469
gradient difference: 0.3652009069919586
train() client id: f_00000-0-0 loss: 1.684739  [   32/  126]
train() client id: f_00000-0-1 loss: 1.938939  [   64/  126]
train() client id: f_00000-0-2 loss: 1.433963  [   96/  126]
train() client id: f_00000-1-0 loss: 1.675710  [   32/  126]
train() client id: f_00000-1-1 loss: 1.390868  [   64/  126]
train() client id: f_00000-1-2 loss: 1.376999  [   96/  126]
train() client id: f_00000-2-0 loss: 1.308604  [   32/  126]
train() client id: f_00000-2-1 loss: 1.401648  [   64/  126]
train() client id: f_00000-2-2 loss: 1.179212  [   96/  126]
train() client id: f_00000-3-0 loss: 1.167466  [   32/  126]
train() client id: f_00000-3-1 loss: 1.153623  [   64/  126]
train() client id: f_00000-3-2 loss: 1.191463  [   96/  126]
train() client id: f_00000-4-0 loss: 1.184364  [   32/  126]
train() client id: f_00000-4-1 loss: 1.106078  [   64/  126]
train() client id: f_00000-4-2 loss: 1.066075  [   96/  126]
train() client id: f_00000-5-0 loss: 1.043613  [   32/  126]
train() client id: f_00000-5-1 loss: 1.030038  [   64/  126]
train() client id: f_00000-5-2 loss: 1.069004  [   96/  126]
train() client id: f_00000-6-0 loss: 1.043487  [   32/  126]
train() client id: f_00000-6-1 loss: 1.068891  [   64/  126]
train() client id: f_00000-6-2 loss: 0.946777  [   96/  126]
train() client id: f_00000-7-0 loss: 1.013069  [   32/  126]
train() client id: f_00000-7-1 loss: 0.946401  [   64/  126]
train() client id: f_00000-7-2 loss: 0.925698  [   96/  126]
train() client id: f_00000-8-0 loss: 0.984681  [   32/  126]
train() client id: f_00000-8-1 loss: 0.948636  [   64/  126]
train() client id: f_00000-8-2 loss: 0.915264  [   96/  126]
train() client id: f_00000-9-0 loss: 1.006679  [   32/  126]
train() client id: f_00000-9-1 loss: 0.847259  [   64/  126]
train() client id: f_00000-9-2 loss: 0.963819  [   96/  126]
train() client id: f_00000-10-0 loss: 1.036510  [   32/  126]
train() client id: f_00000-10-1 loss: 0.817528  [   64/  126]
train() client id: f_00000-10-2 loss: 0.915236  [   96/  126]
train() client id: f_00000-11-0 loss: 0.922120  [   32/  126]
train() client id: f_00000-11-1 loss: 0.908284  [   64/  126]
train() client id: f_00000-11-2 loss: 0.878838  [   96/  126]
train() client id: f_00000-12-0 loss: 0.871869  [   32/  126]
train() client id: f_00000-12-1 loss: 0.882820  [   64/  126]
train() client id: f_00000-12-2 loss: 0.927846  [   96/  126]
train() client id: f_00001-0-0 loss: 0.726861  [   32/  265]
train() client id: f_00001-0-1 loss: 0.549699  [   64/  265]
train() client id: f_00001-0-2 loss: 0.555083  [   96/  265]
train() client id: f_00001-0-3 loss: 0.475640  [  128/  265]
train() client id: f_00001-0-4 loss: 0.523879  [  160/  265]
train() client id: f_00001-0-5 loss: 0.504706  [  192/  265]
train() client id: f_00001-0-6 loss: 0.562702  [  224/  265]
train() client id: f_00001-0-7 loss: 0.613390  [  256/  265]
train() client id: f_00001-1-0 loss: 0.643130  [   32/  265]
train() client id: f_00001-1-1 loss: 0.554818  [   64/  265]
train() client id: f_00001-1-2 loss: 0.513893  [   96/  265]
train() client id: f_00001-1-3 loss: 0.527648  [  128/  265]
train() client id: f_00001-1-4 loss: 0.620174  [  160/  265]
train() client id: f_00001-1-5 loss: 0.554852  [  192/  265]
train() client id: f_00001-1-6 loss: 0.524687  [  224/  265]
train() client id: f_00001-1-7 loss: 0.540026  [  256/  265]
train() client id: f_00001-2-0 loss: 0.527347  [   32/  265]
train() client id: f_00001-2-1 loss: 0.511317  [   64/  265]
train() client id: f_00001-2-2 loss: 0.546576  [   96/  265]
train() client id: f_00001-2-3 loss: 0.541960  [  128/  265]
train() client id: f_00001-2-4 loss: 0.516572  [  160/  265]
train() client id: f_00001-2-5 loss: 0.669987  [  192/  265]
train() client id: f_00001-2-6 loss: 0.538134  [  224/  265]
train() client id: f_00001-2-7 loss: 0.483353  [  256/  265]
train() client id: f_00001-3-0 loss: 0.461488  [   32/  265]
train() client id: f_00001-3-1 loss: 0.515527  [   64/  265]
train() client id: f_00001-3-2 loss: 0.619144  [   96/  265]
train() client id: f_00001-3-3 loss: 0.555462  [  128/  265]
train() client id: f_00001-3-4 loss: 0.591434  [  160/  265]
train() client id: f_00001-3-5 loss: 0.596270  [  192/  265]
train() client id: f_00001-3-6 loss: 0.500879  [  224/  265]
train() client id: f_00001-3-7 loss: 0.506248  [  256/  265]
train() client id: f_00001-4-0 loss: 0.476738  [   32/  265]
train() client id: f_00001-4-1 loss: 0.593460  [   64/  265]
train() client id: f_00001-4-2 loss: 0.503689  [   96/  265]
train() client id: f_00001-4-3 loss: 0.694319  [  128/  265]
train() client id: f_00001-4-4 loss: 0.538340  [  160/  265]
train() client id: f_00001-4-5 loss: 0.511352  [  192/  265]
train() client id: f_00001-4-6 loss: 0.444135  [  224/  265]
train() client id: f_00001-4-7 loss: 0.518643  [  256/  265]
train() client id: f_00001-5-0 loss: 0.458150  [   32/  265]
train() client id: f_00001-5-1 loss: 0.539891  [   64/  265]
train() client id: f_00001-5-2 loss: 0.675647  [   96/  265]
train() client id: f_00001-5-3 loss: 0.558681  [  128/  265]
train() client id: f_00001-5-4 loss: 0.499779  [  160/  265]
train() client id: f_00001-5-5 loss: 0.463215  [  192/  265]
train() client id: f_00001-5-6 loss: 0.537143  [  224/  265]
train() client id: f_00001-5-7 loss: 0.540947  [  256/  265]
train() client id: f_00001-6-0 loss: 0.594625  [   32/  265]
train() client id: f_00001-6-1 loss: 0.520915  [   64/  265]
train() client id: f_00001-6-2 loss: 0.617740  [   96/  265]
train() client id: f_00001-6-3 loss: 0.506523  [  128/  265]
train() client id: f_00001-6-4 loss: 0.524572  [  160/  265]
train() client id: f_00001-6-5 loss: 0.550843  [  192/  265]
train() client id: f_00001-6-6 loss: 0.501541  [  224/  265]
train() client id: f_00001-6-7 loss: 0.454497  [  256/  265]
train() client id: f_00001-7-0 loss: 0.482155  [   32/  265]
train() client id: f_00001-7-1 loss: 0.485280  [   64/  265]
train() client id: f_00001-7-2 loss: 0.507841  [   96/  265]
train() client id: f_00001-7-3 loss: 0.662959  [  128/  265]
train() client id: f_00001-7-4 loss: 0.503033  [  160/  265]
train() client id: f_00001-7-5 loss: 0.577295  [  192/  265]
train() client id: f_00001-7-6 loss: 0.537720  [  224/  265]
train() client id: f_00001-7-7 loss: 0.510621  [  256/  265]
train() client id: f_00001-8-0 loss: 0.538528  [   32/  265]
train() client id: f_00001-8-1 loss: 0.509795  [   64/  265]
train() client id: f_00001-8-2 loss: 0.514974  [   96/  265]
train() client id: f_00001-8-3 loss: 0.613259  [  128/  265]
train() client id: f_00001-8-4 loss: 0.570725  [  160/  265]
train() client id: f_00001-8-5 loss: 0.545969  [  192/  265]
train() client id: f_00001-8-6 loss: 0.469678  [  224/  265]
train() client id: f_00001-8-7 loss: 0.500339  [  256/  265]
train() client id: f_00001-9-0 loss: 0.503981  [   32/  265]
train() client id: f_00001-9-1 loss: 0.562842  [   64/  265]
train() client id: f_00001-9-2 loss: 0.458856  [   96/  265]
train() client id: f_00001-9-3 loss: 0.691414  [  128/  265]
train() client id: f_00001-9-4 loss: 0.701581  [  160/  265]
train() client id: f_00001-9-5 loss: 0.473913  [  192/  265]
train() client id: f_00001-9-6 loss: 0.434844  [  224/  265]
train() client id: f_00001-9-7 loss: 0.437194  [  256/  265]
train() client id: f_00001-10-0 loss: 0.537023  [   32/  265]
train() client id: f_00001-10-1 loss: 0.647185  [   64/  265]
train() client id: f_00001-10-2 loss: 0.456793  [   96/  265]
train() client id: f_00001-10-3 loss: 0.448137  [  128/  265]
train() client id: f_00001-10-4 loss: 0.632116  [  160/  265]
train() client id: f_00001-10-5 loss: 0.497927  [  192/  265]
train() client id: f_00001-10-6 loss: 0.607567  [  224/  265]
train() client id: f_00001-10-7 loss: 0.440468  [  256/  265]
train() client id: f_00001-11-0 loss: 0.461075  [   32/  265]
train() client id: f_00001-11-1 loss: 0.639602  [   64/  265]
train() client id: f_00001-11-2 loss: 0.574311  [   96/  265]
train() client id: f_00001-11-3 loss: 0.650942  [  128/  265]
train() client id: f_00001-11-4 loss: 0.470796  [  160/  265]
train() client id: f_00001-11-5 loss: 0.534823  [  192/  265]
train() client id: f_00001-11-6 loss: 0.442169  [  224/  265]
train() client id: f_00001-11-7 loss: 0.490707  [  256/  265]
train() client id: f_00001-12-0 loss: 0.575132  [   32/  265]
train() client id: f_00001-12-1 loss: 0.501303  [   64/  265]
train() client id: f_00001-12-2 loss: 0.625042  [   96/  265]
train() client id: f_00001-12-3 loss: 0.558632  [  128/  265]
train() client id: f_00001-12-4 loss: 0.475483  [  160/  265]
train() client id: f_00001-12-5 loss: 0.572689  [  192/  265]
train() client id: f_00001-12-6 loss: 0.453351  [  224/  265]
train() client id: f_00001-12-7 loss: 0.513999  [  256/  265]
train() client id: f_00002-0-0 loss: 1.397063  [   32/  124]
train() client id: f_00002-0-1 loss: 1.193012  [   64/  124]
train() client id: f_00002-0-2 loss: 1.160395  [   96/  124]
train() client id: f_00002-1-0 loss: 1.194481  [   32/  124]
train() client id: f_00002-1-1 loss: 1.304039  [   64/  124]
train() client id: f_00002-1-2 loss: 1.150221  [   96/  124]
train() client id: f_00002-2-0 loss: 1.056383  [   32/  124]
train() client id: f_00002-2-1 loss: 1.153618  [   64/  124]
train() client id: f_00002-2-2 loss: 1.241654  [   96/  124]
train() client id: f_00002-3-0 loss: 1.237808  [   32/  124]
train() client id: f_00002-3-1 loss: 1.177372  [   64/  124]
train() client id: f_00002-3-2 loss: 1.037482  [   96/  124]
train() client id: f_00002-4-0 loss: 1.135981  [   32/  124]
train() client id: f_00002-4-1 loss: 1.149359  [   64/  124]
train() client id: f_00002-4-2 loss: 1.053843  [   96/  124]
train() client id: f_00002-5-0 loss: 1.097737  [   32/  124]
train() client id: f_00002-5-1 loss: 1.056183  [   64/  124]
train() client id: f_00002-5-2 loss: 1.127315  [   96/  124]
train() client id: f_00002-6-0 loss: 1.002798  [   32/  124]
train() client id: f_00002-6-1 loss: 1.139046  [   64/  124]
train() client id: f_00002-6-2 loss: 1.111138  [   96/  124]
train() client id: f_00002-7-0 loss: 0.997421  [   32/  124]
train() client id: f_00002-7-1 loss: 1.022277  [   64/  124]
train() client id: f_00002-7-2 loss: 0.987175  [   96/  124]
train() client id: f_00002-8-0 loss: 1.073205  [   32/  124]
train() client id: f_00002-8-1 loss: 0.966522  [   64/  124]
train() client id: f_00002-8-2 loss: 0.992356  [   96/  124]
train() client id: f_00002-9-0 loss: 1.004867  [   32/  124]
train() client id: f_00002-9-1 loss: 0.904366  [   64/  124]
train() client id: f_00002-9-2 loss: 1.017227  [   96/  124]
train() client id: f_00002-10-0 loss: 0.998255  [   32/  124]
train() client id: f_00002-10-1 loss: 0.957120  [   64/  124]
train() client id: f_00002-10-2 loss: 0.924662  [   96/  124]
train() client id: f_00002-11-0 loss: 1.032445  [   32/  124]
train() client id: f_00002-11-1 loss: 1.097886  [   64/  124]
train() client id: f_00002-11-2 loss: 0.926304  [   96/  124]
train() client id: f_00002-12-0 loss: 0.918574  [   32/  124]
train() client id: f_00002-12-1 loss: 1.041687  [   64/  124]
train() client id: f_00002-12-2 loss: 0.905496  [   96/  124]
train() client id: f_00003-0-0 loss: 0.904684  [   32/   43]
train() client id: f_00003-1-0 loss: 0.801064  [   32/   43]
train() client id: f_00003-2-0 loss: 0.821909  [   32/   43]
train() client id: f_00003-3-0 loss: 0.823816  [   32/   43]
train() client id: f_00003-4-0 loss: 0.961854  [   32/   43]
train() client id: f_00003-5-0 loss: 0.752443  [   32/   43]
train() client id: f_00003-6-0 loss: 0.898614  [   32/   43]
train() client id: f_00003-7-0 loss: 0.785294  [   32/   43]
train() client id: f_00003-8-0 loss: 0.842997  [   32/   43]
train() client id: f_00003-9-0 loss: 0.923203  [   32/   43]
train() client id: f_00003-10-0 loss: 0.900431  [   32/   43]
train() client id: f_00003-11-0 loss: 0.838276  [   32/   43]
train() client id: f_00003-12-0 loss: 0.854414  [   32/   43]
train() client id: f_00004-0-0 loss: 0.896886  [   32/  306]
train() client id: f_00004-0-1 loss: 1.078084  [   64/  306]
train() client id: f_00004-0-2 loss: 0.955823  [   96/  306]
train() client id: f_00004-0-3 loss: 1.003150  [  128/  306]
train() client id: f_00004-0-4 loss: 0.790648  [  160/  306]
train() client id: f_00004-0-5 loss: 0.899979  [  192/  306]
train() client id: f_00004-0-6 loss: 0.866651  [  224/  306]
train() client id: f_00004-0-7 loss: 0.896384  [  256/  306]
train() client id: f_00004-0-8 loss: 1.038670  [  288/  306]
train() client id: f_00004-1-0 loss: 0.764451  [   32/  306]
train() client id: f_00004-1-1 loss: 1.051549  [   64/  306]
train() client id: f_00004-1-2 loss: 0.803647  [   96/  306]
train() client id: f_00004-1-3 loss: 1.008641  [  128/  306]
train() client id: f_00004-1-4 loss: 0.949538  [  160/  306]
train() client id: f_00004-1-5 loss: 1.045281  [  192/  306]
train() client id: f_00004-1-6 loss: 0.909131  [  224/  306]
train() client id: f_00004-1-7 loss: 0.949598  [  256/  306]
train() client id: f_00004-1-8 loss: 0.988879  [  288/  306]
train() client id: f_00004-2-0 loss: 0.962402  [   32/  306]
train() client id: f_00004-2-1 loss: 0.871166  [   64/  306]
train() client id: f_00004-2-2 loss: 0.903625  [   96/  306]
train() client id: f_00004-2-3 loss: 0.852911  [  128/  306]
train() client id: f_00004-2-4 loss: 1.014002  [  160/  306]
train() client id: f_00004-2-5 loss: 1.033765  [  192/  306]
train() client id: f_00004-2-6 loss: 0.920674  [  224/  306]
train() client id: f_00004-2-7 loss: 0.930968  [  256/  306]
train() client id: f_00004-2-8 loss: 0.986086  [  288/  306]
train() client id: f_00004-3-0 loss: 1.007201  [   32/  306]
train() client id: f_00004-3-1 loss: 0.873461  [   64/  306]
train() client id: f_00004-3-2 loss: 1.040263  [   96/  306]
train() client id: f_00004-3-3 loss: 0.924455  [  128/  306]
train() client id: f_00004-3-4 loss: 1.032223  [  160/  306]
train() client id: f_00004-3-5 loss: 0.899931  [  192/  306]
train() client id: f_00004-3-6 loss: 0.824787  [  224/  306]
train() client id: f_00004-3-7 loss: 0.999296  [  256/  306]
train() client id: f_00004-3-8 loss: 0.910221  [  288/  306]
train() client id: f_00004-4-0 loss: 0.973179  [   32/  306]
train() client id: f_00004-4-1 loss: 0.957155  [   64/  306]
train() client id: f_00004-4-2 loss: 0.977859  [   96/  306]
train() client id: f_00004-4-3 loss: 0.878335  [  128/  306]
train() client id: f_00004-4-4 loss: 0.962732  [  160/  306]
train() client id: f_00004-4-5 loss: 0.732661  [  192/  306]
train() client id: f_00004-4-6 loss: 1.069066  [  224/  306]
train() client id: f_00004-4-7 loss: 0.959224  [  256/  306]
train() client id: f_00004-4-8 loss: 0.951129  [  288/  306]
train() client id: f_00004-5-0 loss: 0.929075  [   32/  306]
train() client id: f_00004-5-1 loss: 0.911283  [   64/  306]
train() client id: f_00004-5-2 loss: 0.943740  [   96/  306]
train() client id: f_00004-5-3 loss: 0.944131  [  128/  306]
train() client id: f_00004-5-4 loss: 0.952186  [  160/  306]
train() client id: f_00004-5-5 loss: 0.941498  [  192/  306]
train() client id: f_00004-5-6 loss: 0.893896  [  224/  306]
train() client id: f_00004-5-7 loss: 0.881795  [  256/  306]
train() client id: f_00004-5-8 loss: 1.015783  [  288/  306]
train() client id: f_00004-6-0 loss: 0.954263  [   32/  306]
train() client id: f_00004-6-1 loss: 0.965334  [   64/  306]
train() client id: f_00004-6-2 loss: 0.901460  [   96/  306]
train() client id: f_00004-6-3 loss: 0.945135  [  128/  306]
train() client id: f_00004-6-4 loss: 0.818252  [  160/  306]
train() client id: f_00004-6-5 loss: 0.843883  [  192/  306]
train() client id: f_00004-6-6 loss: 0.865023  [  224/  306]
train() client id: f_00004-6-7 loss: 1.005054  [  256/  306]
train() client id: f_00004-6-8 loss: 1.035206  [  288/  306]
train() client id: f_00004-7-0 loss: 0.932838  [   32/  306]
train() client id: f_00004-7-1 loss: 0.861708  [   64/  306]
train() client id: f_00004-7-2 loss: 0.806647  [   96/  306]
train() client id: f_00004-7-3 loss: 0.919533  [  128/  306]
train() client id: f_00004-7-4 loss: 1.010833  [  160/  306]
train() client id: f_00004-7-5 loss: 1.030396  [  192/  306]
train() client id: f_00004-7-6 loss: 1.026663  [  224/  306]
train() client id: f_00004-7-7 loss: 0.862779  [  256/  306]
train() client id: f_00004-7-8 loss: 0.921544  [  288/  306]
train() client id: f_00004-8-0 loss: 0.912704  [   32/  306]
train() client id: f_00004-8-1 loss: 0.936926  [   64/  306]
train() client id: f_00004-8-2 loss: 0.912187  [   96/  306]
train() client id: f_00004-8-3 loss: 0.884745  [  128/  306]
train() client id: f_00004-8-4 loss: 0.959374  [  160/  306]
train() client id: f_00004-8-5 loss: 0.975850  [  192/  306]
train() client id: f_00004-8-6 loss: 0.977843  [  224/  306]
train() client id: f_00004-8-7 loss: 1.005102  [  256/  306]
train() client id: f_00004-8-8 loss: 0.900192  [  288/  306]
train() client id: f_00004-9-0 loss: 0.953622  [   32/  306]
train() client id: f_00004-9-1 loss: 1.011530  [   64/  306]
train() client id: f_00004-9-2 loss: 0.936077  [   96/  306]
train() client id: f_00004-9-3 loss: 0.967515  [  128/  306]
train() client id: f_00004-9-4 loss: 0.960136  [  160/  306]
train() client id: f_00004-9-5 loss: 0.879710  [  192/  306]
train() client id: f_00004-9-6 loss: 0.872919  [  224/  306]
train() client id: f_00004-9-7 loss: 0.876126  [  256/  306]
train() client id: f_00004-9-8 loss: 1.017904  [  288/  306]
train() client id: f_00004-10-0 loss: 0.957054  [   32/  306]
train() client id: f_00004-10-1 loss: 0.932346  [   64/  306]
train() client id: f_00004-10-2 loss: 0.956932  [   96/  306]
train() client id: f_00004-10-3 loss: 0.973864  [  128/  306]
train() client id: f_00004-10-4 loss: 0.909476  [  160/  306]
train() client id: f_00004-10-5 loss: 0.941404  [  192/  306]
train() client id: f_00004-10-6 loss: 0.997002  [  224/  306]
train() client id: f_00004-10-7 loss: 0.946844  [  256/  306]
train() client id: f_00004-10-8 loss: 0.802479  [  288/  306]
train() client id: f_00004-11-0 loss: 0.775104  [   32/  306]
train() client id: f_00004-11-1 loss: 0.953227  [   64/  306]
train() client id: f_00004-11-2 loss: 0.915761  [   96/  306]
train() client id: f_00004-11-3 loss: 0.929653  [  128/  306]
train() client id: f_00004-11-4 loss: 0.870491  [  160/  306]
train() client id: f_00004-11-5 loss: 0.972211  [  192/  306]
train() client id: f_00004-11-6 loss: 1.058365  [  224/  306]
train() client id: f_00004-11-7 loss: 0.980884  [  256/  306]
train() client id: f_00004-11-8 loss: 0.983034  [  288/  306]
train() client id: f_00004-12-0 loss: 0.968166  [   32/  306]
train() client id: f_00004-12-1 loss: 0.912829  [   64/  306]
train() client id: f_00004-12-2 loss: 0.980116  [   96/  306]
train() client id: f_00004-12-3 loss: 0.853143  [  128/  306]
train() client id: f_00004-12-4 loss: 0.932899  [  160/  306]
train() client id: f_00004-12-5 loss: 0.938431  [  192/  306]
train() client id: f_00004-12-6 loss: 0.868048  [  224/  306]
train() client id: f_00004-12-7 loss: 0.958198  [  256/  306]
train() client id: f_00004-12-8 loss: 1.059817  [  288/  306]
train() client id: f_00005-0-0 loss: 1.052440  [   32/  146]
train() client id: f_00005-0-1 loss: 0.806078  [   64/  146]
train() client id: f_00005-0-2 loss: 0.672698  [   96/  146]
train() client id: f_00005-0-3 loss: 0.820665  [  128/  146]
train() client id: f_00005-1-0 loss: 0.814901  [   32/  146]
train() client id: f_00005-1-1 loss: 0.849782  [   64/  146]
train() client id: f_00005-1-2 loss: 0.846826  [   96/  146]
train() client id: f_00005-1-3 loss: 0.791385  [  128/  146]
train() client id: f_00005-2-0 loss: 0.854269  [   32/  146]
train() client id: f_00005-2-1 loss: 0.714763  [   64/  146]
train() client id: f_00005-2-2 loss: 0.892532  [   96/  146]
train() client id: f_00005-2-3 loss: 0.807713  [  128/  146]
train() client id: f_00005-3-0 loss: 0.726041  [   32/  146]
train() client id: f_00005-3-1 loss: 0.876344  [   64/  146]
train() client id: f_00005-3-2 loss: 0.842301  [   96/  146]
train() client id: f_00005-3-3 loss: 0.800457  [  128/  146]
train() client id: f_00005-4-0 loss: 0.678000  [   32/  146]
train() client id: f_00005-4-1 loss: 0.773081  [   64/  146]
train() client id: f_00005-4-2 loss: 0.815397  [   96/  146]
train() client id: f_00005-4-3 loss: 0.826249  [  128/  146]
train() client id: f_00005-5-0 loss: 0.687654  [   32/  146]
train() client id: f_00005-5-1 loss: 0.762995  [   64/  146]
train() client id: f_00005-5-2 loss: 0.794705  [   96/  146]
train() client id: f_00005-5-3 loss: 0.845623  [  128/  146]
train() client id: f_00005-6-0 loss: 0.623438  [   32/  146]
train() client id: f_00005-6-1 loss: 0.656766  [   64/  146]
train() client id: f_00005-6-2 loss: 1.005323  [   96/  146]
train() client id: f_00005-6-3 loss: 0.892808  [  128/  146]
train() client id: f_00005-7-0 loss: 0.848494  [   32/  146]
train() client id: f_00005-7-1 loss: 0.695546  [   64/  146]
train() client id: f_00005-7-2 loss: 0.805806  [   96/  146]
train() client id: f_00005-7-3 loss: 0.848243  [  128/  146]
train() client id: f_00005-8-0 loss: 0.756229  [   32/  146]
train() client id: f_00005-8-1 loss: 0.875479  [   64/  146]
train() client id: f_00005-8-2 loss: 0.752811  [   96/  146]
train() client id: f_00005-8-3 loss: 0.763054  [  128/  146]
train() client id: f_00005-9-0 loss: 0.810774  [   32/  146]
train() client id: f_00005-9-1 loss: 0.810489  [   64/  146]
train() client id: f_00005-9-2 loss: 0.840869  [   96/  146]
train() client id: f_00005-9-3 loss: 0.692023  [  128/  146]
train() client id: f_00005-10-0 loss: 0.725044  [   32/  146]
train() client id: f_00005-10-1 loss: 0.778290  [   64/  146]
train() client id: f_00005-10-2 loss: 0.900544  [   96/  146]
train() client id: f_00005-10-3 loss: 0.679707  [  128/  146]
train() client id: f_00005-11-0 loss: 1.070072  [   32/  146]
train() client id: f_00005-11-1 loss: 0.894701  [   64/  146]
train() client id: f_00005-11-2 loss: 0.685589  [   96/  146]
train() client id: f_00005-11-3 loss: 0.647842  [  128/  146]
train() client id: f_00005-12-0 loss: 0.842996  [   32/  146]
train() client id: f_00005-12-1 loss: 0.665828  [   64/  146]
train() client id: f_00005-12-2 loss: 0.721446  [   96/  146]
train() client id: f_00005-12-3 loss: 0.888572  [  128/  146]
train() client id: f_00006-0-0 loss: 0.632523  [   32/   54]
train() client id: f_00006-1-0 loss: 0.720190  [   32/   54]
train() client id: f_00006-2-0 loss: 0.705261  [   32/   54]
train() client id: f_00006-3-0 loss: 0.710251  [   32/   54]
train() client id: f_00006-4-0 loss: 0.655412  [   32/   54]
train() client id: f_00006-5-0 loss: 0.731441  [   32/   54]
train() client id: f_00006-6-0 loss: 0.647965  [   32/   54]
train() client id: f_00006-7-0 loss: 0.691346  [   32/   54]
train() client id: f_00006-8-0 loss: 0.720603  [   32/   54]
train() client id: f_00006-9-0 loss: 0.717310  [   32/   54]
train() client id: f_00006-10-0 loss: 0.696420  [   32/   54]
train() client id: f_00006-11-0 loss: 0.724383  [   32/   54]
train() client id: f_00006-12-0 loss: 0.646314  [   32/   54]
train() client id: f_00007-0-0 loss: 0.628926  [   32/  179]
train() client id: f_00007-0-1 loss: 0.702135  [   64/  179]
train() client id: f_00007-0-2 loss: 0.767616  [   96/  179]
train() client id: f_00007-0-3 loss: 0.687016  [  128/  179]
train() client id: f_00007-0-4 loss: 0.713933  [  160/  179]
train() client id: f_00007-1-0 loss: 0.802167  [   32/  179]
train() client id: f_00007-1-1 loss: 0.630677  [   64/  179]
train() client id: f_00007-1-2 loss: 0.795777  [   96/  179]
train() client id: f_00007-1-3 loss: 0.602600  [  128/  179]
train() client id: f_00007-1-4 loss: 0.626875  [  160/  179]
train() client id: f_00007-2-0 loss: 0.610734  [   32/  179]
train() client id: f_00007-2-1 loss: 0.675762  [   64/  179]
train() client id: f_00007-2-2 loss: 0.834787  [   96/  179]
train() client id: f_00007-2-3 loss: 0.631665  [  128/  179]
train() client id: f_00007-2-4 loss: 0.572768  [  160/  179]
train() client id: f_00007-3-0 loss: 0.745314  [   32/  179]
train() client id: f_00007-3-1 loss: 0.584697  [   64/  179]
train() client id: f_00007-3-2 loss: 0.643740  [   96/  179]
train() client id: f_00007-3-3 loss: 0.729485  [  128/  179]
train() client id: f_00007-3-4 loss: 0.639444  [  160/  179]
train() client id: f_00007-4-0 loss: 0.666951  [   32/  179]
train() client id: f_00007-4-1 loss: 0.791642  [   64/  179]
train() client id: f_00007-4-2 loss: 0.547636  [   96/  179]
train() client id: f_00007-4-3 loss: 0.572478  [  128/  179]
train() client id: f_00007-4-4 loss: 0.748294  [  160/  179]
train() client id: f_00007-5-0 loss: 0.610227  [   32/  179]
train() client id: f_00007-5-1 loss: 0.775692  [   64/  179]
train() client id: f_00007-5-2 loss: 0.599758  [   96/  179]
train() client id: f_00007-5-3 loss: 0.682691  [  128/  179]
train() client id: f_00007-5-4 loss: 0.626904  [  160/  179]
train() client id: f_00007-6-0 loss: 0.558554  [   32/  179]
train() client id: f_00007-6-1 loss: 0.691787  [   64/  179]
train() client id: f_00007-6-2 loss: 0.631096  [   96/  179]
train() client id: f_00007-6-3 loss: 0.629943  [  128/  179]
train() client id: f_00007-6-4 loss: 0.672247  [  160/  179]
train() client id: f_00007-7-0 loss: 0.558531  [   32/  179]
train() client id: f_00007-7-1 loss: 0.572268  [   64/  179]
train() client id: f_00007-7-2 loss: 0.756778  [   96/  179]
train() client id: f_00007-7-3 loss: 0.548391  [  128/  179]
train() client id: f_00007-7-4 loss: 0.758306  [  160/  179]
train() client id: f_00007-8-0 loss: 0.522372  [   32/  179]
train() client id: f_00007-8-1 loss: 0.590242  [   64/  179]
train() client id: f_00007-8-2 loss: 0.746824  [   96/  179]
train() client id: f_00007-8-3 loss: 0.671217  [  128/  179]
train() client id: f_00007-8-4 loss: 0.727213  [  160/  179]
train() client id: f_00007-9-0 loss: 0.664332  [   32/  179]
train() client id: f_00007-9-1 loss: 0.545445  [   64/  179]
train() client id: f_00007-9-2 loss: 0.499353  [   96/  179]
train() client id: f_00007-9-3 loss: 0.708507  [  128/  179]
train() client id: f_00007-9-4 loss: 0.625118  [  160/  179]
train() client id: f_00007-10-0 loss: 0.505589  [   32/  179]
train() client id: f_00007-10-1 loss: 0.648172  [   64/  179]
train() client id: f_00007-10-2 loss: 0.732618  [   96/  179]
train() client id: f_00007-10-3 loss: 0.705515  [  128/  179]
train() client id: f_00007-10-4 loss: 0.590061  [  160/  179]
train() client id: f_00007-11-0 loss: 0.709154  [   32/  179]
train() client id: f_00007-11-1 loss: 0.517991  [   64/  179]
train() client id: f_00007-11-2 loss: 0.813253  [   96/  179]
train() client id: f_00007-11-3 loss: 0.643887  [  128/  179]
train() client id: f_00007-11-4 loss: 0.535249  [  160/  179]
train() client id: f_00007-12-0 loss: 0.659678  [   32/  179]
train() client id: f_00007-12-1 loss: 0.694102  [   64/  179]
train() client id: f_00007-12-2 loss: 0.575592  [   96/  179]
train() client id: f_00007-12-3 loss: 0.625594  [  128/  179]
train() client id: f_00007-12-4 loss: 0.604317  [  160/  179]
train() client id: f_00008-0-0 loss: 0.860001  [   32/  130]
train() client id: f_00008-0-1 loss: 0.844365  [   64/  130]
train() client id: f_00008-0-2 loss: 0.792003  [   96/  130]
train() client id: f_00008-0-3 loss: 0.846567  [  128/  130]
train() client id: f_00008-1-0 loss: 0.777851  [   32/  130]
train() client id: f_00008-1-1 loss: 0.864792  [   64/  130]
train() client id: f_00008-1-2 loss: 0.790514  [   96/  130]
train() client id: f_00008-1-3 loss: 0.866546  [  128/  130]
train() client id: f_00008-2-0 loss: 0.751335  [   32/  130]
train() client id: f_00008-2-1 loss: 0.863481  [   64/  130]
train() client id: f_00008-2-2 loss: 0.855706  [   96/  130]
train() client id: f_00008-2-3 loss: 0.862815  [  128/  130]
train() client id: f_00008-3-0 loss: 0.745948  [   32/  130]
train() client id: f_00008-3-1 loss: 0.788121  [   64/  130]
train() client id: f_00008-3-2 loss: 0.886560  [   96/  130]
train() client id: f_00008-3-3 loss: 0.911151  [  128/  130]
train() client id: f_00008-4-0 loss: 0.771799  [   32/  130]
train() client id: f_00008-4-1 loss: 0.890248  [   64/  130]
train() client id: f_00008-4-2 loss: 0.862973  [   96/  130]
train() client id: f_00008-4-3 loss: 0.785523  [  128/  130]
train() client id: f_00008-5-0 loss: 0.855253  [   32/  130]
train() client id: f_00008-5-1 loss: 0.801104  [   64/  130]
train() client id: f_00008-5-2 loss: 0.834523  [   96/  130]
train() client id: f_00008-5-3 loss: 0.843713  [  128/  130]
train() client id: f_00008-6-0 loss: 0.874525  [   32/  130]
train() client id: f_00008-6-1 loss: 0.719730  [   64/  130]
train() client id: f_00008-6-2 loss: 0.823260  [   96/  130]
train() client id: f_00008-6-3 loss: 0.917388  [  128/  130]
train() client id: f_00008-7-0 loss: 0.812741  [   32/  130]
train() client id: f_00008-7-1 loss: 0.877921  [   64/  130]
train() client id: f_00008-7-2 loss: 0.830503  [   96/  130]
train() client id: f_00008-7-3 loss: 0.810894  [  128/  130]
train() client id: f_00008-8-0 loss: 0.838496  [   32/  130]
train() client id: f_00008-8-1 loss: 0.845320  [   64/  130]
train() client id: f_00008-8-2 loss: 0.810526  [   96/  130]
train() client id: f_00008-8-3 loss: 0.845078  [  128/  130]
train() client id: f_00008-9-0 loss: 0.917891  [   32/  130]
train() client id: f_00008-9-1 loss: 0.781422  [   64/  130]
train() client id: f_00008-9-2 loss: 0.806000  [   96/  130]
train() client id: f_00008-9-3 loss: 0.829684  [  128/  130]
train() client id: f_00008-10-0 loss: 0.846145  [   32/  130]
train() client id: f_00008-10-1 loss: 0.800735  [   64/  130]
train() client id: f_00008-10-2 loss: 0.847863  [   96/  130]
train() client id: f_00008-10-3 loss: 0.813690  [  128/  130]
train() client id: f_00008-11-0 loss: 0.842059  [   32/  130]
train() client id: f_00008-11-1 loss: 0.807994  [   64/  130]
train() client id: f_00008-11-2 loss: 0.852500  [   96/  130]
train() client id: f_00008-11-3 loss: 0.828076  [  128/  130]
train() client id: f_00008-12-0 loss: 0.889241  [   32/  130]
train() client id: f_00008-12-1 loss: 0.740064  [   64/  130]
train() client id: f_00008-12-2 loss: 0.850026  [   96/  130]
train() client id: f_00008-12-3 loss: 0.826151  [  128/  130]
train() client id: f_00009-0-0 loss: 1.218141  [   32/  118]
train() client id: f_00009-0-1 loss: 1.215972  [   64/  118]
train() client id: f_00009-0-2 loss: 1.182423  [   96/  118]
train() client id: f_00009-1-0 loss: 1.178059  [   32/  118]
train() client id: f_00009-1-1 loss: 1.113719  [   64/  118]
train() client id: f_00009-1-2 loss: 1.136109  [   96/  118]
train() client id: f_00009-2-0 loss: 1.097448  [   32/  118]
train() client id: f_00009-2-1 loss: 1.075240  [   64/  118]
train() client id: f_00009-2-2 loss: 1.184105  [   96/  118]
train() client id: f_00009-3-0 loss: 1.032489  [   32/  118]
train() client id: f_00009-3-1 loss: 1.048611  [   64/  118]
train() client id: f_00009-3-2 loss: 1.131235  [   96/  118]
train() client id: f_00009-4-0 loss: 1.073226  [   32/  118]
train() client id: f_00009-4-1 loss: 1.029159  [   64/  118]
train() client id: f_00009-4-2 loss: 1.000586  [   96/  118]
train() client id: f_00009-5-0 loss: 1.024867  [   32/  118]
train() client id: f_00009-5-1 loss: 1.055896  [   64/  118]
train() client id: f_00009-5-2 loss: 1.002865  [   96/  118]
train() client id: f_00009-6-0 loss: 0.916747  [   32/  118]
train() client id: f_00009-6-1 loss: 1.066666  [   64/  118]
train() client id: f_00009-6-2 loss: 0.975634  [   96/  118]
train() client id: f_00009-7-0 loss: 1.061284  [   32/  118]
train() client id: f_00009-7-1 loss: 0.903964  [   64/  118]
train() client id: f_00009-7-2 loss: 0.977437  [   96/  118]
train() client id: f_00009-8-0 loss: 1.027768  [   32/  118]
train() client id: f_00009-8-1 loss: 0.920869  [   64/  118]
train() client id: f_00009-8-2 loss: 0.911317  [   96/  118]
train() client id: f_00009-9-0 loss: 0.892473  [   32/  118]
train() client id: f_00009-9-1 loss: 0.915864  [   64/  118]
train() client id: f_00009-9-2 loss: 0.946205  [   96/  118]
train() client id: f_00009-10-0 loss: 1.021221  [   32/  118]
train() client id: f_00009-10-1 loss: 0.935083  [   64/  118]
train() client id: f_00009-10-2 loss: 0.951679  [   96/  118]
train() client id: f_00009-11-0 loss: 0.880639  [   32/  118]
train() client id: f_00009-11-1 loss: 0.915335  [   64/  118]
train() client id: f_00009-11-2 loss: 0.974961  [   96/  118]
train() client id: f_00009-12-0 loss: 0.998802  [   32/  118]
train() client id: f_00009-12-1 loss: 0.903767  [   64/  118]
train() client id: f_00009-12-2 loss: 0.911619  [   96/  118]
At round 9 accuracy: 0.6259946949602122
At round 9 training accuracy: 0.5767940979208585
At round 9 training loss: 0.8721525716184971
update_location
xs = 8.927491 166.223621 5.882650 10.934260 -82.581990 69.769243 -5.849135 -5.143845 -105.120581 20.134486 
ys = -157.390647 7.291448 55.684448 -97.290817 -9.642386 0.794442 -106.381692 51.628436 25.881276 -592.232496 
xs mean: 8.317620029478787
ys mean: -82.16579882624053
dists_uav = 186.685607 194.122274 114.609613 139.946637 130.049070 121.935960 146.120761 112.658575 147.377668 600.953182 
uav_gains = -106.848640 -107.313430 -101.480638 -103.650860 -102.853340 -102.153563 -104.121012 -101.294194 -104.214394 -126.389481 
uav_gains_db_mean: -106.03195516979125
dists_bs = 379.885331 380.210097 216.690410 329.717816 206.479780 300.432286 328.310329 209.932504 164.680075 791.658494 
bs_gains = -111.797330 -111.807721 -104.970571 -110.075047 -104.383630 -108.943964 -110.023026 -104.585290 -101.633009 -120.726120 
bs_gains_db_mean: -108.89457074004788
Round 10
-------------------------------
ene_coms = [0.0087857  0.00902449 0.00793354 0.00742502 0.00769924 0.00692027
 0.00759872 0.0077782  0.00676024 0.03064775]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.72263852 20.2120037   9.55048801  3.42874342 23.27108682 11.18439121
  4.26270812 13.69470893  9.97366762  9.66598663]
obj_prev = 114.96642297248276
eta_min = 2.1950845008911493e-10	eta_max = 0.7377825445462977
af = 24.17431921386036	bf = 2.4899046325291194	zeta = 26.5917511352464	eta = 0.909090909090909
af = 24.17431921386036	bf = 2.4899046325291194	zeta = 52.73359677794219	eta = 0.4584234850442096
af = 24.17431921386036	bf = 2.4899046325291194	zeta = 39.41262510324096	eta = 0.6133648583553108
af = 24.17431921386036	bf = 2.4899046325291194	zeta = 37.00180651043815	eta = 0.6533280802665898
af = 24.17431921386036	bf = 2.4899046325291194	zeta = 36.865774025487966	eta = 0.6557388215190304
af = 24.17431921386036	bf = 2.4899046325291194	zeta = 36.86529831616084	eta = 0.6557472831642036
af = 24.17431921386036	bf = 2.4899046325291194	zeta = 36.86529831031445	eta = 0.6557472832681972
eta = 0.6557472832681972
ene_coms = [0.0087857  0.00902449 0.00793354 0.00742502 0.00769924 0.00692027
 0.00759872 0.0077782  0.00676024 0.03064775]
ene_comp = [0.03482076 0.07323414 0.03426805 0.01188328 0.08456471 0.04034787
 0.01492318 0.04946759 0.03592618 0.03260992]
ene_total = [3.1359857  5.91567994 3.03495313 1.38856811 6.63521792 3.39931708
 1.61967625 4.11686615 3.06982003 4.54921401]
ti_comp = [0.32221584 0.31982797 0.33073751 0.33582266 0.33308052 0.34087017
 0.33408572 0.33229087 0.34247051 0.10359539]
ti_coms = [0.08785705 0.09024492 0.07933539 0.07425023 0.07699237 0.06920272
 0.07598717 0.07778202 0.06760238 0.30647751]
t_total = [29.49064255 29.49064255 29.49064255 29.49064255 29.49064255 29.49064255
 29.49064255 29.49064255 29.49064255 29.49064255]
ene_coms = [0.0087857  0.00902449 0.00793354 0.00742502 0.00769924 0.00692027
 0.00759872 0.0077782  0.00676024 0.03064775]
ene_comp = [2.54156397e-05 2.39987118e-04 2.29922437e-05 9.29969181e-07
 3.40681849e-04 3.53316366e-05 1.86100972e-06 6.85181247e-05
 2.47096662e-05 2.01952103e-04]
ene_total = [0.63365712 0.66626069 0.57219879 0.53404194 0.57819567 0.50021648
 0.54660022 0.56430169 0.48794365 2.21857521]
optimize_network iter = 0 obj = 7.301991458957539
eta = 0.6557472832681972
freqs = [5.40332864e+07 1.14489896e+08 5.18055117e+07 1.76927843e+07
 1.26943339e+08 5.91836278e+07 2.23343632e+07 7.44341703e+07
 5.24514980e+07 1.57390792e+08]
eta_min = 0.6557472832682063	eta_max = 0.6557472832681971
af = 0.056461860472393176	bf = 2.4899046325291194	zeta = 0.0621080465196325	eta = 0.9090909090909091
af = 0.056461860472393176	bf = 2.4899046325291194	zeta = 27.44814624863112	eta = 0.002057037293555262
af = 0.056461860472393176	bf = 2.4899046325291194	zeta = 2.8450576749382686	eta = 0.01984559433355539
af = 0.056461860472393176	bf = 2.4899046325291194	zeta = 2.7661175697059894	eta = 0.020411952510895805
af = 0.056461860472393176	bf = 2.4899046325291194	zeta = 2.766094963753629	eta = 0.020412119327882238
eta = 0.020412119327882238
ene_coms = [0.0087857  0.00902449 0.00793354 0.00742502 0.00769924 0.00692027
 0.00759872 0.0077782  0.00676024 0.03064775]
ene_comp = [2.34390782e-04 2.21323441e-03 2.12041485e-04 8.57645947e-06
 3.14187194e-03 3.25839130e-04 1.71627993e-05 6.31895047e-04
 2.27880079e-04 1.86246390e-03]
ene_total = [0.22796506 0.28401129 0.20586341 0.18786951 0.27398757 0.18313111
 0.19247629 0.21254856 0.17661085 0.82163131]
ti_comp = [0.32221584 0.31982797 0.33073751 0.33582266 0.33308052 0.34087017
 0.33408572 0.33229087 0.34247051 0.10359539]
ti_coms = [0.08785705 0.09024492 0.07933539 0.07425023 0.07699237 0.06920272
 0.07598717 0.07778202 0.06760238 0.30647751]
t_total = [29.49064255 29.49064255 29.49064255 29.49064255 29.49064255 29.49064255
 29.49064255 29.49064255 29.49064255 29.49064255]
ene_coms = [0.0087857  0.00902449 0.00793354 0.00742502 0.00769924 0.00692027
 0.00759872 0.0077782  0.00676024 0.03064775]
ene_comp = [2.54156397e-05 2.39987118e-04 2.29922437e-05 9.29969181e-07
 3.40681849e-04 3.53316366e-05 1.86100972e-06 6.85181247e-05
 2.47096662e-05 2.01952103e-04]
ene_total = [0.63365712 0.66626069 0.57219879 0.53404194 0.57819567 0.50021648
 0.54660022 0.56430169 0.48794365 2.21857521]
optimize_network iter = 1 obj = 7.3019914589577315
eta = 0.6557472832682063
freqs = [5.40332864e+07 1.14489896e+08 5.18055117e+07 1.76927843e+07
 1.26943339e+08 5.91836278e+07 2.23343632e+07 7.44341703e+07
 5.24514980e+07 1.57390792e+08]
Done!
ene_coms = [0.0087857  0.00902449 0.00793354 0.00742502 0.00769924 0.00692027
 0.00759872 0.0077782  0.00676024 0.03064775]
ene_comp = [2.39114916e-05 2.25784203e-04 2.16315170e-05 8.74931756e-07
 3.20519620e-04 3.32406401e-05 1.75087146e-06 6.44630858e-05
 2.32472990e-05 1.90000176e-04]
ene_total = [0.00880962 0.00925028 0.00795517 0.0074259  0.00801976 0.00695351
 0.00760047 0.00784267 0.00678349 0.03083775]
At round 10 energy consumption: 0.10147859954830141
At round 10 eta: 0.6557472832682063
At round 10 a_n: 24.7571443865612
At round 10 local rounds: 13.817762670684786
At round 10 global rounds: 71.91561078034839
gradient difference: 0.42134833335876465
train() client id: f_00000-0-0 loss: 1.759734  [   32/  126]
train() client id: f_00000-0-1 loss: 1.456893  [   64/  126]
train() client id: f_00000-0-2 loss: 1.386545  [   96/  126]
train() client id: f_00000-1-0 loss: 1.440685  [   32/  126]
train() client id: f_00000-1-1 loss: 1.424496  [   64/  126]
train() client id: f_00000-1-2 loss: 1.294315  [   96/  126]
train() client id: f_00000-2-0 loss: 1.293775  [   32/  126]
train() client id: f_00000-2-1 loss: 1.360346  [   64/  126]
train() client id: f_00000-2-2 loss: 1.181805  [   96/  126]
train() client id: f_00000-3-0 loss: 1.247504  [   32/  126]
train() client id: f_00000-3-1 loss: 1.053853  [   64/  126]
train() client id: f_00000-3-2 loss: 1.099638  [   96/  126]
train() client id: f_00000-4-0 loss: 1.143715  [   32/  126]
train() client id: f_00000-4-1 loss: 0.979999  [   64/  126]
train() client id: f_00000-4-2 loss: 1.111846  [   96/  126]
train() client id: f_00000-5-0 loss: 0.986617  [   32/  126]
train() client id: f_00000-5-1 loss: 0.977932  [   64/  126]
train() client id: f_00000-5-2 loss: 0.993151  [   96/  126]
train() client id: f_00000-6-0 loss: 0.985766  [   32/  126]
train() client id: f_00000-6-1 loss: 0.952527  [   64/  126]
train() client id: f_00000-6-2 loss: 0.903371  [   96/  126]
train() client id: f_00000-7-0 loss: 0.924074  [   32/  126]
train() client id: f_00000-7-1 loss: 1.007370  [   64/  126]
train() client id: f_00000-7-2 loss: 0.890029  [   96/  126]
train() client id: f_00000-8-0 loss: 0.950129  [   32/  126]
train() client id: f_00000-8-1 loss: 0.890945  [   64/  126]
train() client id: f_00000-8-2 loss: 0.880107  [   96/  126]
train() client id: f_00000-9-0 loss: 0.862075  [   32/  126]
train() client id: f_00000-9-1 loss: 0.870924  [   64/  126]
train() client id: f_00000-9-2 loss: 0.877388  [   96/  126]
train() client id: f_00000-10-0 loss: 0.896913  [   32/  126]
train() client id: f_00000-10-1 loss: 0.839963  [   64/  126]
train() client id: f_00000-10-2 loss: 0.848497  [   96/  126]
train() client id: f_00000-11-0 loss: 0.908225  [   32/  126]
train() client id: f_00000-11-1 loss: 0.889683  [   64/  126]
train() client id: f_00000-11-2 loss: 0.865107  [   96/  126]
train() client id: f_00000-12-0 loss: 0.799036  [   32/  126]
train() client id: f_00000-12-1 loss: 0.896001  [   64/  126]
train() client id: f_00000-12-2 loss: 0.922607  [   96/  126]
train() client id: f_00001-0-0 loss: 0.549471  [   32/  265]
train() client id: f_00001-0-1 loss: 0.667691  [   64/  265]
train() client id: f_00001-0-2 loss: 0.489929  [   96/  265]
train() client id: f_00001-0-3 loss: 0.482370  [  128/  265]
train() client id: f_00001-0-4 loss: 0.614195  [  160/  265]
train() client id: f_00001-0-5 loss: 0.513957  [  192/  265]
train() client id: f_00001-0-6 loss: 0.520992  [  224/  265]
train() client id: f_00001-0-7 loss: 0.450929  [  256/  265]
train() client id: f_00001-1-0 loss: 0.549643  [   32/  265]
train() client id: f_00001-1-1 loss: 0.554440  [   64/  265]
train() client id: f_00001-1-2 loss: 0.589997  [   96/  265]
train() client id: f_00001-1-3 loss: 0.482395  [  128/  265]
train() client id: f_00001-1-4 loss: 0.426777  [  160/  265]
train() client id: f_00001-1-5 loss: 0.573234  [  192/  265]
train() client id: f_00001-1-6 loss: 0.469468  [  224/  265]
train() client id: f_00001-1-7 loss: 0.520493  [  256/  265]
train() client id: f_00001-2-0 loss: 0.599284  [   32/  265]
train() client id: f_00001-2-1 loss: 0.682321  [   64/  265]
train() client id: f_00001-2-2 loss: 0.491921  [   96/  265]
train() client id: f_00001-2-3 loss: 0.464086  [  128/  265]
train() client id: f_00001-2-4 loss: 0.423914  [  160/  265]
train() client id: f_00001-2-5 loss: 0.500667  [  192/  265]
train() client id: f_00001-2-6 loss: 0.448272  [  224/  265]
train() client id: f_00001-2-7 loss: 0.473744  [  256/  265]
train() client id: f_00001-3-0 loss: 0.467820  [   32/  265]
train() client id: f_00001-3-1 loss: 0.537070  [   64/  265]
train() client id: f_00001-3-2 loss: 0.469995  [   96/  265]
train() client id: f_00001-3-3 loss: 0.524537  [  128/  265]
train() client id: f_00001-3-4 loss: 0.459861  [  160/  265]
train() client id: f_00001-3-5 loss: 0.478635  [  192/  265]
train() client id: f_00001-3-6 loss: 0.534666  [  224/  265]
train() client id: f_00001-3-7 loss: 0.501189  [  256/  265]
train() client id: f_00001-4-0 loss: 0.450948  [   32/  265]
train() client id: f_00001-4-1 loss: 0.525063  [   64/  265]
train() client id: f_00001-4-2 loss: 0.564774  [   96/  265]
train() client id: f_00001-4-3 loss: 0.499859  [  128/  265]
train() client id: f_00001-4-4 loss: 0.427455  [  160/  265]
train() client id: f_00001-4-5 loss: 0.564403  [  192/  265]
train() client id: f_00001-4-6 loss: 0.565383  [  224/  265]
train() client id: f_00001-4-7 loss: 0.401437  [  256/  265]
train() client id: f_00001-5-0 loss: 0.426115  [   32/  265]
train() client id: f_00001-5-1 loss: 0.488500  [   64/  265]
train() client id: f_00001-5-2 loss: 0.594906  [   96/  265]
train() client id: f_00001-5-3 loss: 0.420615  [  128/  265]
train() client id: f_00001-5-4 loss: 0.623495  [  160/  265]
train() client id: f_00001-5-5 loss: 0.488780  [  192/  265]
train() client id: f_00001-5-6 loss: 0.389418  [  224/  265]
train() client id: f_00001-5-7 loss: 0.537922  [  256/  265]
train() client id: f_00001-6-0 loss: 0.432942  [   32/  265]
train() client id: f_00001-6-1 loss: 0.502897  [   64/  265]
train() client id: f_00001-6-2 loss: 0.599707  [   96/  265]
train() client id: f_00001-6-3 loss: 0.615270  [  128/  265]
train() client id: f_00001-6-4 loss: 0.516916  [  160/  265]
train() client id: f_00001-6-5 loss: 0.413717  [  192/  265]
train() client id: f_00001-6-6 loss: 0.441139  [  224/  265]
train() client id: f_00001-6-7 loss: 0.410633  [  256/  265]
train() client id: f_00001-7-0 loss: 0.529023  [   32/  265]
train() client id: f_00001-7-1 loss: 0.457411  [   64/  265]
train() client id: f_00001-7-2 loss: 0.469495  [   96/  265]
train() client id: f_00001-7-3 loss: 0.446328  [  128/  265]
train() client id: f_00001-7-4 loss: 0.491962  [  160/  265]
train() client id: f_00001-7-5 loss: 0.509277  [  192/  265]
train() client id: f_00001-7-6 loss: 0.481596  [  224/  265]
train() client id: f_00001-7-7 loss: 0.453543  [  256/  265]
train() client id: f_00001-8-0 loss: 0.455945  [   32/  265]
train() client id: f_00001-8-1 loss: 0.526386  [   64/  265]
train() client id: f_00001-8-2 loss: 0.531323  [   96/  265]
train() client id: f_00001-8-3 loss: 0.508264  [  128/  265]
train() client id: f_00001-8-4 loss: 0.465634  [  160/  265]
train() client id: f_00001-8-5 loss: 0.449235  [  192/  265]
train() client id: f_00001-8-6 loss: 0.517582  [  224/  265]
train() client id: f_00001-8-7 loss: 0.461543  [  256/  265]
train() client id: f_00001-9-0 loss: 0.472275  [   32/  265]
train() client id: f_00001-9-1 loss: 0.385940  [   64/  265]
train() client id: f_00001-9-2 loss: 0.462678  [   96/  265]
train() client id: f_00001-9-3 loss: 0.619770  [  128/  265]
train() client id: f_00001-9-4 loss: 0.451253  [  160/  265]
train() client id: f_00001-9-5 loss: 0.492586  [  192/  265]
train() client id: f_00001-9-6 loss: 0.625888  [  224/  265]
train() client id: f_00001-9-7 loss: 0.389073  [  256/  265]
train() client id: f_00001-10-0 loss: 0.405806  [   32/  265]
train() client id: f_00001-10-1 loss: 0.388510  [   64/  265]
train() client id: f_00001-10-2 loss: 0.534999  [   96/  265]
train() client id: f_00001-10-3 loss: 0.648242  [  128/  265]
train() client id: f_00001-10-4 loss: 0.555378  [  160/  265]
train() client id: f_00001-10-5 loss: 0.411852  [  192/  265]
train() client id: f_00001-10-6 loss: 0.441411  [  224/  265]
train() client id: f_00001-10-7 loss: 0.448950  [  256/  265]
train() client id: f_00001-11-0 loss: 0.444627  [   32/  265]
train() client id: f_00001-11-1 loss: 0.420301  [   64/  265]
train() client id: f_00001-11-2 loss: 0.556930  [   96/  265]
train() client id: f_00001-11-3 loss: 0.405144  [  128/  265]
train() client id: f_00001-11-4 loss: 0.576656  [  160/  265]
train() client id: f_00001-11-5 loss: 0.537486  [  192/  265]
train() client id: f_00001-11-6 loss: 0.457271  [  224/  265]
train() client id: f_00001-11-7 loss: 0.440662  [  256/  265]
train() client id: f_00001-12-0 loss: 0.500104  [   32/  265]
train() client id: f_00001-12-1 loss: 0.493465  [   64/  265]
train() client id: f_00001-12-2 loss: 0.541899  [   96/  265]
train() client id: f_00001-12-3 loss: 0.398033  [  128/  265]
train() client id: f_00001-12-4 loss: 0.535556  [  160/  265]
train() client id: f_00001-12-5 loss: 0.563037  [  192/  265]
train() client id: f_00001-12-6 loss: 0.382092  [  224/  265]
train() client id: f_00001-12-7 loss: 0.474859  [  256/  265]
train() client id: f_00002-0-0 loss: 1.181549  [   32/  124]
train() client id: f_00002-0-1 loss: 1.252284  [   64/  124]
train() client id: f_00002-0-2 loss: 1.213116  [   96/  124]
train() client id: f_00002-1-0 loss: 1.197194  [   32/  124]
train() client id: f_00002-1-1 loss: 1.317748  [   64/  124]
train() client id: f_00002-1-2 loss: 1.080827  [   96/  124]
train() client id: f_00002-2-0 loss: 1.178086  [   32/  124]
train() client id: f_00002-2-1 loss: 1.256334  [   64/  124]
train() client id: f_00002-2-2 loss: 1.072197  [   96/  124]
train() client id: f_00002-3-0 loss: 1.104349  [   32/  124]
train() client id: f_00002-3-1 loss: 1.181713  [   64/  124]
train() client id: f_00002-3-2 loss: 1.146108  [   96/  124]
train() client id: f_00002-4-0 loss: 1.159451  [   32/  124]
train() client id: f_00002-4-1 loss: 1.171280  [   64/  124]
train() client id: f_00002-4-2 loss: 1.020757  [   96/  124]
train() client id: f_00002-5-0 loss: 1.099850  [   32/  124]
train() client id: f_00002-5-1 loss: 1.051933  [   64/  124]
train() client id: f_00002-5-2 loss: 1.151427  [   96/  124]
train() client id: f_00002-6-0 loss: 1.086537  [   32/  124]
train() client id: f_00002-6-1 loss: 1.123369  [   64/  124]
train() client id: f_00002-6-2 loss: 1.044264  [   96/  124]
train() client id: f_00002-7-0 loss: 1.072913  [   32/  124]
train() client id: f_00002-7-1 loss: 1.018010  [   64/  124]
train() client id: f_00002-7-2 loss: 1.144834  [   96/  124]
train() client id: f_00002-8-0 loss: 0.989778  [   32/  124]
train() client id: f_00002-8-1 loss: 1.057726  [   64/  124]
train() client id: f_00002-8-2 loss: 1.120181  [   96/  124]
train() client id: f_00002-9-0 loss: 1.048177  [   32/  124]
train() client id: f_00002-9-1 loss: 1.039656  [   64/  124]
train() client id: f_00002-9-2 loss: 1.012501  [   96/  124]
train() client id: f_00002-10-0 loss: 1.125457  [   32/  124]
train() client id: f_00002-10-1 loss: 1.081247  [   64/  124]
train() client id: f_00002-10-2 loss: 0.991585  [   96/  124]
train() client id: f_00002-11-0 loss: 1.015281  [   32/  124]
train() client id: f_00002-11-1 loss: 1.113754  [   64/  124]
train() client id: f_00002-11-2 loss: 1.128254  [   96/  124]
train() client id: f_00002-12-0 loss: 1.013802  [   32/  124]
train() client id: f_00002-12-1 loss: 1.037624  [   64/  124]
train() client id: f_00002-12-2 loss: 1.127162  [   96/  124]
train() client id: f_00003-0-0 loss: 0.753119  [   32/   43]
train() client id: f_00003-1-0 loss: 0.812723  [   32/   43]
train() client id: f_00003-2-0 loss: 1.014290  [   32/   43]
train() client id: f_00003-3-0 loss: 1.055474  [   32/   43]
train() client id: f_00003-4-0 loss: 0.738190  [   32/   43]
train() client id: f_00003-5-0 loss: 0.893457  [   32/   43]
train() client id: f_00003-6-0 loss: 0.761578  [   32/   43]
train() client id: f_00003-7-0 loss: 0.876166  [   32/   43]
train() client id: f_00003-8-0 loss: 0.707252  [   32/   43]
train() client id: f_00003-9-0 loss: 0.803420  [   32/   43]
train() client id: f_00003-10-0 loss: 0.704438  [   32/   43]
train() client id: f_00003-11-0 loss: 0.851982  [   32/   43]
train() client id: f_00003-12-0 loss: 0.833816  [   32/   43]
train() client id: f_00004-0-0 loss: 0.680139  [   32/  306]
train() client id: f_00004-0-1 loss: 0.943728  [   64/  306]
train() client id: f_00004-0-2 loss: 0.734416  [   96/  306]
train() client id: f_00004-0-3 loss: 0.796997  [  128/  306]
train() client id: f_00004-0-4 loss: 0.934711  [  160/  306]
train() client id: f_00004-0-5 loss: 0.624874  [  192/  306]
train() client id: f_00004-0-6 loss: 0.760855  [  224/  306]
train() client id: f_00004-0-7 loss: 0.714776  [  256/  306]
train() client id: f_00004-0-8 loss: 0.788486  [  288/  306]
train() client id: f_00004-1-0 loss: 0.672868  [   32/  306]
train() client id: f_00004-1-1 loss: 0.764744  [   64/  306]
train() client id: f_00004-1-2 loss: 0.694336  [   96/  306]
train() client id: f_00004-1-3 loss: 0.934389  [  128/  306]
train() client id: f_00004-1-4 loss: 0.804604  [  160/  306]
train() client id: f_00004-1-5 loss: 0.931275  [  192/  306]
train() client id: f_00004-1-6 loss: 0.732790  [  224/  306]
train() client id: f_00004-1-7 loss: 0.808702  [  256/  306]
train() client id: f_00004-1-8 loss: 0.661450  [  288/  306]
train() client id: f_00004-2-0 loss: 0.893575  [   32/  306]
train() client id: f_00004-2-1 loss: 0.835979  [   64/  306]
train() client id: f_00004-2-2 loss: 0.752189  [   96/  306]
train() client id: f_00004-2-3 loss: 0.650451  [  128/  306]
train() client id: f_00004-2-4 loss: 0.809942  [  160/  306]
train() client id: f_00004-2-5 loss: 0.762854  [  192/  306]
train() client id: f_00004-2-6 loss: 0.706535  [  224/  306]
train() client id: f_00004-2-7 loss: 0.810856  [  256/  306]
train() client id: f_00004-2-8 loss: 0.737970  [  288/  306]
train() client id: f_00004-3-0 loss: 0.796374  [   32/  306]
train() client id: f_00004-3-1 loss: 0.770520  [   64/  306]
train() client id: f_00004-3-2 loss: 0.830900  [   96/  306]
train() client id: f_00004-3-3 loss: 0.743505  [  128/  306]
train() client id: f_00004-3-4 loss: 0.780672  [  160/  306]
train() client id: f_00004-3-5 loss: 0.703294  [  192/  306]
train() client id: f_00004-3-6 loss: 0.691350  [  224/  306]
train() client id: f_00004-3-7 loss: 0.718388  [  256/  306]
train() client id: f_00004-3-8 loss: 0.891656  [  288/  306]
train() client id: f_00004-4-0 loss: 0.800274  [   32/  306]
train() client id: f_00004-4-1 loss: 0.852345  [   64/  306]
train() client id: f_00004-4-2 loss: 0.729413  [   96/  306]
train() client id: f_00004-4-3 loss: 0.806945  [  128/  306]
train() client id: f_00004-4-4 loss: 0.754494  [  160/  306]
train() client id: f_00004-4-5 loss: 0.746006  [  192/  306]
train() client id: f_00004-4-6 loss: 0.721859  [  224/  306]
train() client id: f_00004-4-7 loss: 0.862746  [  256/  306]
train() client id: f_00004-4-8 loss: 0.802364  [  288/  306]
train() client id: f_00004-5-0 loss: 0.784153  [   32/  306]
train() client id: f_00004-5-1 loss: 0.591028  [   64/  306]
train() client id: f_00004-5-2 loss: 0.894635  [   96/  306]
train() client id: f_00004-5-3 loss: 0.743650  [  128/  306]
train() client id: f_00004-5-4 loss: 0.618958  [  160/  306]
train() client id: f_00004-5-5 loss: 0.911170  [  192/  306]
train() client id: f_00004-5-6 loss: 0.796678  [  224/  306]
train() client id: f_00004-5-7 loss: 0.813642  [  256/  306]
train() client id: f_00004-5-8 loss: 0.915976  [  288/  306]
train() client id: f_00004-6-0 loss: 0.742550  [   32/  306]
train() client id: f_00004-6-1 loss: 0.919116  [   64/  306]
train() client id: f_00004-6-2 loss: 0.800494  [   96/  306]
train() client id: f_00004-6-3 loss: 0.916415  [  128/  306]
train() client id: f_00004-6-4 loss: 0.714281  [  160/  306]
train() client id: f_00004-6-5 loss: 0.789776  [  192/  306]
train() client id: f_00004-6-6 loss: 0.839787  [  224/  306]
train() client id: f_00004-6-7 loss: 0.654926  [  256/  306]
train() client id: f_00004-6-8 loss: 0.704842  [  288/  306]
train() client id: f_00004-7-0 loss: 0.876679  [   32/  306]
train() client id: f_00004-7-1 loss: 1.010799  [   64/  306]
train() client id: f_00004-7-2 loss: 0.675994  [   96/  306]
train() client id: f_00004-7-3 loss: 0.707600  [  128/  306]
train() client id: f_00004-7-4 loss: 0.760620  [  160/  306]
train() client id: f_00004-7-5 loss: 0.625907  [  192/  306]
train() client id: f_00004-7-6 loss: 0.740828  [  224/  306]
train() client id: f_00004-7-7 loss: 0.892957  [  256/  306]
train() client id: f_00004-7-8 loss: 0.714676  [  288/  306]
train() client id: f_00004-8-0 loss: 0.782315  [   32/  306]
train() client id: f_00004-8-1 loss: 0.742668  [   64/  306]
train() client id: f_00004-8-2 loss: 0.825462  [   96/  306]
train() client id: f_00004-8-3 loss: 0.866060  [  128/  306]
train() client id: f_00004-8-4 loss: 0.755656  [  160/  306]
train() client id: f_00004-8-5 loss: 0.865612  [  192/  306]
train() client id: f_00004-8-6 loss: 0.796910  [  224/  306]
train() client id: f_00004-8-7 loss: 0.692446  [  256/  306]
train() client id: f_00004-8-8 loss: 0.839141  [  288/  306]
train() client id: f_00004-9-0 loss: 0.763182  [   32/  306]
train() client id: f_00004-9-1 loss: 0.860046  [   64/  306]
train() client id: f_00004-9-2 loss: 0.770691  [   96/  306]
train() client id: f_00004-9-3 loss: 0.667272  [  128/  306]
train() client id: f_00004-9-4 loss: 0.886891  [  160/  306]
train() client id: f_00004-9-5 loss: 0.899325  [  192/  306]
train() client id: f_00004-9-6 loss: 0.730492  [  224/  306]
train() client id: f_00004-9-7 loss: 0.732359  [  256/  306]
train() client id: f_00004-9-8 loss: 0.837647  [  288/  306]
train() client id: f_00004-10-0 loss: 0.888097  [   32/  306]
train() client id: f_00004-10-1 loss: 0.739664  [   64/  306]
train() client id: f_00004-10-2 loss: 0.715307  [   96/  306]
train() client id: f_00004-10-3 loss: 0.910075  [  128/  306]
train() client id: f_00004-10-4 loss: 0.894210  [  160/  306]
train() client id: f_00004-10-5 loss: 0.700224  [  192/  306]
train() client id: f_00004-10-6 loss: 0.807331  [  224/  306]
train() client id: f_00004-10-7 loss: 0.841426  [  256/  306]
train() client id: f_00004-10-8 loss: 0.752475  [  288/  306]
train() client id: f_00004-11-0 loss: 0.927060  [   32/  306]
train() client id: f_00004-11-1 loss: 0.728797  [   64/  306]
train() client id: f_00004-11-2 loss: 0.896522  [   96/  306]
train() client id: f_00004-11-3 loss: 0.672207  [  128/  306]
train() client id: f_00004-11-4 loss: 0.780721  [  160/  306]
train() client id: f_00004-11-5 loss: 0.792793  [  192/  306]
train() client id: f_00004-11-6 loss: 0.802760  [  224/  306]
train() client id: f_00004-11-7 loss: 0.835180  [  256/  306]
train() client id: f_00004-11-8 loss: 0.669770  [  288/  306]
train() client id: f_00004-12-0 loss: 0.804885  [   32/  306]
train() client id: f_00004-12-1 loss: 0.903269  [   64/  306]
train() client id: f_00004-12-2 loss: 0.754130  [   96/  306]
train() client id: f_00004-12-3 loss: 0.779680  [  128/  306]
train() client id: f_00004-12-4 loss: 0.828920  [  160/  306]
train() client id: f_00004-12-5 loss: 0.791920  [  192/  306]
train() client id: f_00004-12-6 loss: 0.755494  [  224/  306]
train() client id: f_00004-12-7 loss: 0.814564  [  256/  306]
train() client id: f_00004-12-8 loss: 0.738933  [  288/  306]
train() client id: f_00005-0-0 loss: 0.635457  [   32/  146]
train() client id: f_00005-0-1 loss: 0.763981  [   64/  146]
train() client id: f_00005-0-2 loss: 0.609599  [   96/  146]
train() client id: f_00005-0-3 loss: 0.623240  [  128/  146]
train() client id: f_00005-1-0 loss: 0.662001  [   32/  146]
train() client id: f_00005-1-1 loss: 0.748343  [   64/  146]
train() client id: f_00005-1-2 loss: 0.518965  [   96/  146]
train() client id: f_00005-1-3 loss: 0.611007  [  128/  146]
train() client id: f_00005-2-0 loss: 0.663922  [   32/  146]
train() client id: f_00005-2-1 loss: 0.728274  [   64/  146]
train() client id: f_00005-2-2 loss: 0.595897  [   96/  146]
train() client id: f_00005-2-3 loss: 0.569852  [  128/  146]
train() client id: f_00005-3-0 loss: 0.655438  [   32/  146]
train() client id: f_00005-3-1 loss: 0.739047  [   64/  146]
train() client id: f_00005-3-2 loss: 0.383684  [   96/  146]
train() client id: f_00005-3-3 loss: 0.681664  [  128/  146]
train() client id: f_00005-4-0 loss: 0.693727  [   32/  146]
train() client id: f_00005-4-1 loss: 0.620335  [   64/  146]
train() client id: f_00005-4-2 loss: 0.582883  [   96/  146]
train() client id: f_00005-4-3 loss: 0.626655  [  128/  146]
train() client id: f_00005-5-0 loss: 0.466323  [   32/  146]
train() client id: f_00005-5-1 loss: 0.609266  [   64/  146]
train() client id: f_00005-5-2 loss: 0.717136  [   96/  146]
train() client id: f_00005-5-3 loss: 0.617664  [  128/  146]
train() client id: f_00005-6-0 loss: 0.461168  [   32/  146]
train() client id: f_00005-6-1 loss: 0.652913  [   64/  146]
train() client id: f_00005-6-2 loss: 0.540342  [   96/  146]
train() client id: f_00005-6-3 loss: 0.761156  [  128/  146]
train() client id: f_00005-7-0 loss: 0.673103  [   32/  146]
train() client id: f_00005-7-1 loss: 0.630814  [   64/  146]
train() client id: f_00005-7-2 loss: 0.549586  [   96/  146]
train() client id: f_00005-7-3 loss: 0.458226  [  128/  146]
train() client id: f_00005-8-0 loss: 0.623683  [   32/  146]
train() client id: f_00005-8-1 loss: 0.523599  [   64/  146]
train() client id: f_00005-8-2 loss: 0.560160  [   96/  146]
train() client id: f_00005-8-3 loss: 0.607368  [  128/  146]
train() client id: f_00005-9-0 loss: 0.588103  [   32/  146]
train() client id: f_00005-9-1 loss: 0.480422  [   64/  146]
train() client id: f_00005-9-2 loss: 0.764513  [   96/  146]
train() client id: f_00005-9-3 loss: 0.591351  [  128/  146]
train() client id: f_00005-10-0 loss: 0.387340  [   32/  146]
train() client id: f_00005-10-1 loss: 0.761702  [   64/  146]
train() client id: f_00005-10-2 loss: 0.512214  [   96/  146]
train() client id: f_00005-10-3 loss: 0.810862  [  128/  146]
train() client id: f_00005-11-0 loss: 0.582409  [   32/  146]
train() client id: f_00005-11-1 loss: 0.766031  [   64/  146]
train() client id: f_00005-11-2 loss: 0.418077  [   96/  146]
train() client id: f_00005-11-3 loss: 0.646458  [  128/  146]
train() client id: f_00005-12-0 loss: 0.750188  [   32/  146]
train() client id: f_00005-12-1 loss: 0.591017  [   64/  146]
train() client id: f_00005-12-2 loss: 0.371329  [   96/  146]
train() client id: f_00005-12-3 loss: 0.729337  [  128/  146]
train() client id: f_00006-0-0 loss: 0.732181  [   32/   54]
train() client id: f_00006-1-0 loss: 0.740067  [   32/   54]
train() client id: f_00006-2-0 loss: 0.697609  [   32/   54]
train() client id: f_00006-3-0 loss: 0.721264  [   32/   54]
train() client id: f_00006-4-0 loss: 0.743039  [   32/   54]
train() client id: f_00006-5-0 loss: 0.697784  [   32/   54]
train() client id: f_00006-6-0 loss: 0.727075  [   32/   54]
train() client id: f_00006-7-0 loss: 0.676842  [   32/   54]
train() client id: f_00006-8-0 loss: 0.690502  [   32/   54]
train() client id: f_00006-9-0 loss: 0.739689  [   32/   54]
train() client id: f_00006-10-0 loss: 0.734066  [   32/   54]
train() client id: f_00006-11-0 loss: 0.724037  [   32/   54]
train() client id: f_00006-12-0 loss: 0.688825  [   32/   54]
train() client id: f_00007-0-0 loss: 0.680445  [   32/  179]
train() client id: f_00007-0-1 loss: 0.697767  [   64/  179]
train() client id: f_00007-0-2 loss: 0.697178  [   96/  179]
train() client id: f_00007-0-3 loss: 0.669246  [  128/  179]
train() client id: f_00007-0-4 loss: 0.842190  [  160/  179]
train() client id: f_00007-1-0 loss: 0.691232  [   32/  179]
train() client id: f_00007-1-1 loss: 0.849627  [   64/  179]
train() client id: f_00007-1-2 loss: 0.717094  [   96/  179]
train() client id: f_00007-1-3 loss: 0.636464  [  128/  179]
train() client id: f_00007-1-4 loss: 0.598722  [  160/  179]
train() client id: f_00007-2-0 loss: 0.752907  [   32/  179]
train() client id: f_00007-2-1 loss: 0.550268  [   64/  179]
train() client id: f_00007-2-2 loss: 0.770474  [   96/  179]
train() client id: f_00007-2-3 loss: 0.662234  [  128/  179]
train() client id: f_00007-2-4 loss: 0.586289  [  160/  179]
train() client id: f_00007-3-0 loss: 0.651858  [   32/  179]
train() client id: f_00007-3-1 loss: 0.636540  [   64/  179]
train() client id: f_00007-3-2 loss: 0.612740  [   96/  179]
train() client id: f_00007-3-3 loss: 0.651349  [  128/  179]
train() client id: f_00007-3-4 loss: 0.747470  [  160/  179]
train() client id: f_00007-4-0 loss: 0.695072  [   32/  179]
train() client id: f_00007-4-1 loss: 0.685833  [   64/  179]
train() client id: f_00007-4-2 loss: 0.647328  [   96/  179]
train() client id: f_00007-4-3 loss: 0.759585  [  128/  179]
train() client id: f_00007-4-4 loss: 0.544147  [  160/  179]
train() client id: f_00007-5-0 loss: 0.651529  [   32/  179]
train() client id: f_00007-5-1 loss: 0.623805  [   64/  179]
train() client id: f_00007-5-2 loss: 0.609596  [   96/  179]
train() client id: f_00007-5-3 loss: 0.815856  [  128/  179]
train() client id: f_00007-5-4 loss: 0.543419  [  160/  179]
train() client id: f_00007-6-0 loss: 0.616408  [   32/  179]
train() client id: f_00007-6-1 loss: 0.546959  [   64/  179]
train() client id: f_00007-6-2 loss: 0.955747  [   96/  179]
train() client id: f_00007-6-3 loss: 0.602653  [  128/  179]
train() client id: f_00007-6-4 loss: 0.530191  [  160/  179]
train() client id: f_00007-7-0 loss: 0.760791  [   32/  179]
train() client id: f_00007-7-1 loss: 0.527630  [   64/  179]
train() client id: f_00007-7-2 loss: 0.632687  [   96/  179]
train() client id: f_00007-7-3 loss: 0.738797  [  128/  179]
train() client id: f_00007-7-4 loss: 0.605361  [  160/  179]
train() client id: f_00007-8-0 loss: 0.702573  [   32/  179]
train() client id: f_00007-8-1 loss: 0.663679  [   64/  179]
train() client id: f_00007-8-2 loss: 0.607760  [   96/  179]
train() client id: f_00007-8-3 loss: 0.623854  [  128/  179]
train() client id: f_00007-8-4 loss: 0.585078  [  160/  179]
train() client id: f_00007-9-0 loss: 0.587086  [   32/  179]
train() client id: f_00007-9-1 loss: 0.756486  [   64/  179]
train() client id: f_00007-9-2 loss: 0.588471  [   96/  179]
train() client id: f_00007-9-3 loss: 0.673104  [  128/  179]
train() client id: f_00007-9-4 loss: 0.607976  [  160/  179]
train() client id: f_00007-10-0 loss: 0.521074  [   32/  179]
train() client id: f_00007-10-1 loss: 0.596326  [   64/  179]
train() client id: f_00007-10-2 loss: 0.640059  [   96/  179]
train() client id: f_00007-10-3 loss: 0.750718  [  128/  179]
train() client id: f_00007-10-4 loss: 0.683239  [  160/  179]
train() client id: f_00007-11-0 loss: 0.693301  [   32/  179]
train() client id: f_00007-11-1 loss: 0.538305  [   64/  179]
train() client id: f_00007-11-2 loss: 0.661017  [   96/  179]
train() client id: f_00007-11-3 loss: 0.664413  [  128/  179]
train() client id: f_00007-11-4 loss: 0.608678  [  160/  179]
train() client id: f_00007-12-0 loss: 0.580257  [   32/  179]
train() client id: f_00007-12-1 loss: 0.673359  [   64/  179]
train() client id: f_00007-12-2 loss: 0.576498  [   96/  179]
train() client id: f_00007-12-3 loss: 0.680037  [  128/  179]
train() client id: f_00007-12-4 loss: 0.736714  [  160/  179]
train() client id: f_00008-0-0 loss: 0.731990  [   32/  130]
train() client id: f_00008-0-1 loss: 0.906485  [   64/  130]
train() client id: f_00008-0-2 loss: 0.792250  [   96/  130]
train() client id: f_00008-0-3 loss: 0.771490  [  128/  130]
train() client id: f_00008-1-0 loss: 0.782919  [   32/  130]
train() client id: f_00008-1-1 loss: 0.881921  [   64/  130]
train() client id: f_00008-1-2 loss: 0.828773  [   96/  130]
train() client id: f_00008-1-3 loss: 0.690780  [  128/  130]
train() client id: f_00008-2-0 loss: 0.790408  [   32/  130]
train() client id: f_00008-2-1 loss: 0.737346  [   64/  130]
train() client id: f_00008-2-2 loss: 0.900317  [   96/  130]
train() client id: f_00008-2-3 loss: 0.760779  [  128/  130]
train() client id: f_00008-3-0 loss: 0.799602  [   32/  130]
train() client id: f_00008-3-1 loss: 0.767251  [   64/  130]
train() client id: f_00008-3-2 loss: 0.897600  [   96/  130]
train() client id: f_00008-3-3 loss: 0.717219  [  128/  130]
train() client id: f_00008-4-0 loss: 0.845101  [   32/  130]
train() client id: f_00008-4-1 loss: 0.746771  [   64/  130]
train() client id: f_00008-4-2 loss: 0.785830  [   96/  130]
train() client id: f_00008-4-3 loss: 0.805223  [  128/  130]
train() client id: f_00008-5-0 loss: 0.799946  [   32/  130]
train() client id: f_00008-5-1 loss: 0.806697  [   64/  130]
train() client id: f_00008-5-2 loss: 0.789030  [   96/  130]
train() client id: f_00008-5-3 loss: 0.784305  [  128/  130]
train() client id: f_00008-6-0 loss: 0.723427  [   32/  130]
train() client id: f_00008-6-1 loss: 0.789758  [   64/  130]
train() client id: f_00008-6-2 loss: 0.768986  [   96/  130]
train() client id: f_00008-6-3 loss: 0.885359  [  128/  130]
train() client id: f_00008-7-0 loss: 0.710154  [   32/  130]
train() client id: f_00008-7-1 loss: 0.844815  [   64/  130]
train() client id: f_00008-7-2 loss: 0.787894  [   96/  130]
train() client id: f_00008-7-3 loss: 0.825694  [  128/  130]
train() client id: f_00008-8-0 loss: 0.854742  [   32/  130]
train() client id: f_00008-8-1 loss: 0.753517  [   64/  130]
train() client id: f_00008-8-2 loss: 0.816432  [   96/  130]
train() client id: f_00008-8-3 loss: 0.742175  [  128/  130]
train() client id: f_00008-9-0 loss: 0.696864  [   32/  130]
train() client id: f_00008-9-1 loss: 0.909438  [   64/  130]
train() client id: f_00008-9-2 loss: 0.818880  [   96/  130]
train() client id: f_00008-9-3 loss: 0.737985  [  128/  130]
train() client id: f_00008-10-0 loss: 0.835256  [   32/  130]
train() client id: f_00008-10-1 loss: 0.756819  [   64/  130]
train() client id: f_00008-10-2 loss: 0.766440  [   96/  130]
train() client id: f_00008-10-3 loss: 0.774554  [  128/  130]
train() client id: f_00008-11-0 loss: 0.742272  [   32/  130]
train() client id: f_00008-11-1 loss: 0.863408  [   64/  130]
train() client id: f_00008-11-2 loss: 0.788101  [   96/  130]
train() client id: f_00008-11-3 loss: 0.727066  [  128/  130]
train() client id: f_00008-12-0 loss: 0.832871  [   32/  130]
train() client id: f_00008-12-1 loss: 0.803782  [   64/  130]
train() client id: f_00008-12-2 loss: 0.780825  [   96/  130]
train() client id: f_00008-12-3 loss: 0.710486  [  128/  130]
train() client id: f_00009-0-0 loss: 1.146763  [   32/  118]
train() client id: f_00009-0-1 loss: 1.232990  [   64/  118]
train() client id: f_00009-0-2 loss: 1.159698  [   96/  118]
train() client id: f_00009-1-0 loss: 1.054543  [   32/  118]
train() client id: f_00009-1-1 loss: 1.118300  [   64/  118]
train() client id: f_00009-1-2 loss: 1.194695  [   96/  118]
train() client id: f_00009-2-0 loss: 1.028039  [   32/  118]
train() client id: f_00009-2-1 loss: 1.027036  [   64/  118]
train() client id: f_00009-2-2 loss: 1.221113  [   96/  118]
train() client id: f_00009-3-0 loss: 1.033651  [   32/  118]
train() client id: f_00009-3-1 loss: 1.028533  [   64/  118]
train() client id: f_00009-3-2 loss: 1.113309  [   96/  118]
train() client id: f_00009-4-0 loss: 1.028331  [   32/  118]
train() client id: f_00009-4-1 loss: 0.959663  [   64/  118]
train() client id: f_00009-4-2 loss: 1.051891  [   96/  118]
train() client id: f_00009-5-0 loss: 0.960070  [   32/  118]
train() client id: f_00009-5-1 loss: 0.968002  [   64/  118]
train() client id: f_00009-5-2 loss: 1.021310  [   96/  118]
train() client id: f_00009-6-0 loss: 0.993617  [   32/  118]
train() client id: f_00009-6-1 loss: 0.980656  [   64/  118]
train() client id: f_00009-6-2 loss: 0.929379  [   96/  118]
train() client id: f_00009-7-0 loss: 1.094246  [   32/  118]
train() client id: f_00009-7-1 loss: 1.012104  [   64/  118]
train() client id: f_00009-7-2 loss: 0.874126  [   96/  118]
train() client id: f_00009-8-0 loss: 0.973789  [   32/  118]
train() client id: f_00009-8-1 loss: 0.974274  [   64/  118]
train() client id: f_00009-8-2 loss: 0.922003  [   96/  118]
train() client id: f_00009-9-0 loss: 0.903766  [   32/  118]
train() client id: f_00009-9-1 loss: 0.951216  [   64/  118]
train() client id: f_00009-9-2 loss: 1.026327  [   96/  118]
train() client id: f_00009-10-0 loss: 0.864286  [   32/  118]
train() client id: f_00009-10-1 loss: 1.003049  [   64/  118]
train() client id: f_00009-10-2 loss: 0.984077  [   96/  118]
train() client id: f_00009-11-0 loss: 1.032036  [   32/  118]
train() client id: f_00009-11-1 loss: 0.877839  [   64/  118]
train() client id: f_00009-11-2 loss: 0.980968  [   96/  118]
train() client id: f_00009-12-0 loss: 1.094023  [   32/  118]
train() client id: f_00009-12-1 loss: 0.938360  [   64/  118]
train() client id: f_00009-12-2 loss: 0.880295  [   96/  118]
At round 10 accuracy: 0.6259946949602122
At round 10 training accuracy: 0.5767940979208585
At round 10 training loss: 0.8679277189175886
update_location
xs = 8.927491 171.223621 5.882650 10.934260 -87.581990 64.769243 -5.849135 -5.143845 -110.120581 20.134486 
ys = -162.390647 7.291448 60.684448 -92.290817 -9.642386 0.794442 -101.381692 56.628436 25.881276 -597.232496 
xs mean: 7.317620029478787
ys mean: -81.16579882624053
dists_uav = 190.919937 198.420497 117.120484 136.517959 133.280083 119.145650 142.521788 115.035816 150.984711 605.881219 
uav_gains = -107.113384 -107.582643 -101.715973 -103.381028 -103.120055 -101.902148 -103.849332 -101.520945 -104.478286 -126.480759 
uav_gains_db_mean: -106.11445543279251
dists_bs = 384.267837 384.703722 213.977986 325.600875 204.290772 296.372850 324.035268 207.033670 162.621440 796.505176 
bs_gains = -111.936812 -111.950598 -104.817394 -109.922255 -104.254024 -108.778535 -109.863643 -104.416207 -101.480038 -120.800340 
bs_gains_db_mean: -108.82198451240146
Round 11
-------------------------------
ene_coms = [0.00892021 0.00916858 0.00787106 0.0073288  0.00764931 0.00684206
 0.00749739 0.00771189 0.00671459 0.03096308]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.59143076 19.93589879  9.41680444  3.37892952 22.94787108 11.02771244
  4.20122943 13.50359021  9.83454372  9.54002219]
obj_prev = 113.37803258395074
eta_min = 1.623046665923297e-10	eta_max = 0.7383607147247861
af = 23.839837477165666	bf = 2.457743469466343	zeta = 26.223821224882236	eta = 0.909090909090909
af = 23.839837477165666	bf = 2.457743469466343	zeta = 52.02914932512967	eta = 0.45820156174744936
af = 23.839837477165666	bf = 2.457743469466343	zeta = 38.87691469484034	eta = 0.6132132054277867
af = 23.839837477165666	bf = 2.457743469466343	zeta = 36.49669455670716	eta = 0.6532053865898525
af = 23.839837477165666	bf = 2.457743469466343	zeta = 36.36234156124055	eta = 0.6556188752865435
af = 23.839837477165666	bf = 2.457743469466343	zeta = 36.36187135427114	eta = 0.6556273533035695
af = 23.839837477165666	bf = 2.457743469466343	zeta = 36.361871348483206	eta = 0.6556273534079295
eta = 0.6556273534079295
ene_coms = [0.00892021 0.00916858 0.00787106 0.0073288  0.00764931 0.00684206
 0.00749739 0.00771189 0.00671459 0.03096308]
ene_comp = [0.03483585 0.07326589 0.0342829  0.01188843 0.08460136 0.04036536
 0.01492965 0.04948903 0.03594176 0.03262405]
ene_total = [3.10212422 5.84426288 2.98854231 1.36242202 6.54019147 3.34681075
 1.58998475 4.05530896 3.02415849 4.5080655 ]
ti_comp = [0.32605069 0.32356702 0.33654213 0.34196477 0.33875971 0.34683218
 0.34027886 0.3381339  0.34810691 0.10562194]
ti_coms = [0.08920209 0.09168576 0.07871064 0.073288   0.07649307 0.06842059
 0.07497391 0.07711887 0.06714587 0.30963083]
t_total = [29.4397068 29.4397068 29.4397068 29.4397068 29.4397068 29.4397068
 29.4397068 29.4397068 29.4397068 29.4397068]
ene_coms = [0.00892021 0.00916858 0.00787106 0.0073288  0.00764931 0.00684206
 0.00749739 0.00771189 0.00671459 0.03096308]
ene_comp = [2.48535941e-05 2.34777750e-04 2.22348380e-05 8.98029149e-07
 3.29783247e-04 3.41717790e-05 1.79621840e-06 6.62566490e-05
 2.39470810e-05 1.94529533e-04]
ene_total = [0.63416797 0.66665892 0.55960228 0.51964533 0.56568453 0.48749636
 0.53166141 0.55143828 0.47773422 2.20894608]
optimize_network iter = 0 obj = 7.203035394434626
eta = 0.6556273534079295
freqs = [5.34209195e+07 1.13215936e+08 5.09340457e+07 1.73825313e+07
 1.24869281e+08 5.81914794e+07 2.19373781e+07 7.31796367e+07
 5.16245937e+07 1.54437869e+08]
eta_min = 0.6556273534079305	eta_max = 0.6556273534079271
af = 0.05397182711064295	bf = 2.457743469466343	zeta = 0.05936900982170725	eta = 0.9090909090909091
af = 0.05397182711064295	bf = 2.457743469466343	zeta = 27.09176287413888	eta = 0.0019921858670246635
af = 0.05397182711064295	bf = 2.457743469466343	zeta = 2.7989445255788827	eta = 0.019282921336027697
af = 0.05397182711064295	bf = 2.457743469466343	zeta = 2.7233676274460654	eta = 0.019818046805989593
af = 0.05397182711064295	bf = 2.457743469466343	zeta = 2.723347185265265	eta = 0.019818195565610885
eta = 0.019818195565610885
ene_coms = [0.00892021 0.00916858 0.00787106 0.0073288  0.00764931 0.00684206
 0.00749739 0.00771189 0.00671459 0.03096308]
ene_comp = [2.30846513e-04 2.18067555e-03 2.06522840e-04 8.34112350e-06
 3.06311081e-03 3.17396189e-04 1.66837341e-05 6.15408634e-04
 2.22426588e-04 1.80683986e-03]
ene_total = [0.22793663 0.28268981 0.20119843 0.18275524 0.26682741 0.17832939
 0.18716234 0.20741824 0.17278876 0.81624094]
ti_comp = [0.32605069 0.32356702 0.33654213 0.34196477 0.33875971 0.34683218
 0.34027886 0.3381339  0.34810691 0.10562194]
ti_coms = [0.08920209 0.09168576 0.07871064 0.073288   0.07649307 0.06842059
 0.07497391 0.07711887 0.06714587 0.30963083]
t_total = [29.4397068 29.4397068 29.4397068 29.4397068 29.4397068 29.4397068
 29.4397068 29.4397068 29.4397068 29.4397068]
ene_coms = [0.00892021 0.00916858 0.00787106 0.0073288  0.00764931 0.00684206
 0.00749739 0.00771189 0.00671459 0.03096308]
ene_comp = [2.48535941e-05 2.34777750e-04 2.22348380e-05 8.98029149e-07
 3.29783247e-04 3.41717790e-05 1.79621840e-06 6.62566490e-05
 2.39470810e-05 1.94529533e-04]
ene_total = [0.63416797 0.66665892 0.55960228 0.51964533 0.56568453 0.48749636
 0.53166141 0.55143828 0.47773422 2.20894608]
optimize_network iter = 1 obj = 7.203035394434647
eta = 0.6556273534079305
freqs = [5.34209195e+07 1.13215936e+08 5.09340457e+07 1.73825313e+07
 1.24869281e+08 5.81914794e+07 2.19373781e+07 7.31796367e+07
 5.16245937e+07 1.54437869e+08]
Done!
ene_coms = [0.00892021 0.00916858 0.00787106 0.0073288  0.00764931 0.00684206
 0.00749739 0.00771189 0.00671459 0.03096308]
ene_comp = [2.33725781e-05 2.20787436e-04 2.09098727e-05 8.44515943e-07
 3.10131592e-04 3.21354961e-05 1.68918245e-06 6.23084411e-05
 2.25200838e-05 1.82937594e-04]
ene_total = [0.00894358 0.00938936 0.00789197 0.00732964 0.00795944 0.00687419
 0.00749908 0.0077742  0.00673711 0.03114602]
At round 11 energy consumption: 0.10154459903975105
At round 11 eta: 0.6556273534079305
At round 11 a_n: 24.414598539591886
At round 11 local rounds: 13.823751978153917
At round 11 global rounds: 70.89587045080405
gradient difference: 0.3721681237220764
train() client id: f_00000-0-0 loss: 1.493597  [   32/  126]
train() client id: f_00000-0-1 loss: 1.351560  [   64/  126]
train() client id: f_00000-0-2 loss: 1.471554  [   96/  126]
train() client id: f_00000-1-0 loss: 1.440274  [   32/  126]
train() client id: f_00000-1-1 loss: 1.331934  [   64/  126]
train() client id: f_00000-1-2 loss: 1.444896  [   96/  126]
train() client id: f_00000-2-0 loss: 1.332465  [   32/  126]
train() client id: f_00000-2-1 loss: 1.131077  [   64/  126]
train() client id: f_00000-2-2 loss: 1.120493  [   96/  126]
train() client id: f_00000-3-0 loss: 1.181775  [   32/  126]
train() client id: f_00000-3-1 loss: 1.164850  [   64/  126]
train() client id: f_00000-3-2 loss: 1.042114  [   96/  126]
train() client id: f_00000-4-0 loss: 1.006485  [   32/  126]
train() client id: f_00000-4-1 loss: 1.048913  [   64/  126]
train() client id: f_00000-4-2 loss: 1.062511  [   96/  126]
train() client id: f_00000-5-0 loss: 0.999772  [   32/  126]
train() client id: f_00000-5-1 loss: 1.024417  [   64/  126]
train() client id: f_00000-5-2 loss: 0.963265  [   96/  126]
train() client id: f_00000-6-0 loss: 0.944096  [   32/  126]
train() client id: f_00000-6-1 loss: 0.942926  [   64/  126]
train() client id: f_00000-6-2 loss: 0.915790  [   96/  126]
train() client id: f_00000-7-0 loss: 0.904796  [   32/  126]
train() client id: f_00000-7-1 loss: 0.942796  [   64/  126]
train() client id: f_00000-7-2 loss: 0.876620  [   96/  126]
train() client id: f_00000-8-0 loss: 0.918447  [   32/  126]
train() client id: f_00000-8-1 loss: 0.819159  [   64/  126]
train() client id: f_00000-8-2 loss: 0.931266  [   96/  126]
train() client id: f_00000-9-0 loss: 0.822525  [   32/  126]
train() client id: f_00000-9-1 loss: 0.869623  [   64/  126]
train() client id: f_00000-9-2 loss: 0.867856  [   96/  126]
train() client id: f_00000-10-0 loss: 0.905297  [   32/  126]
train() client id: f_00000-10-1 loss: 0.869042  [   64/  126]
train() client id: f_00000-10-2 loss: 0.865902  [   96/  126]
train() client id: f_00000-11-0 loss: 0.770038  [   32/  126]
train() client id: f_00000-11-1 loss: 0.854371  [   64/  126]
train() client id: f_00000-11-2 loss: 0.881625  [   96/  126]
train() client id: f_00000-12-0 loss: 0.806398  [   32/  126]
train() client id: f_00000-12-1 loss: 0.878234  [   64/  126]
train() client id: f_00000-12-2 loss: 0.831825  [   96/  126]
train() client id: f_00001-0-0 loss: 0.573435  [   32/  265]
train() client id: f_00001-0-1 loss: 0.528015  [   64/  265]
train() client id: f_00001-0-2 loss: 0.480257  [   96/  265]
train() client id: f_00001-0-3 loss: 0.671054  [  128/  265]
train() client id: f_00001-0-4 loss: 0.503711  [  160/  265]
train() client id: f_00001-0-5 loss: 0.503903  [  192/  265]
train() client id: f_00001-0-6 loss: 0.505584  [  224/  265]
train() client id: f_00001-0-7 loss: 0.421270  [  256/  265]
train() client id: f_00001-1-0 loss: 0.422735  [   32/  265]
train() client id: f_00001-1-1 loss: 0.510707  [   64/  265]
train() client id: f_00001-1-2 loss: 0.578313  [   96/  265]
train() client id: f_00001-1-3 loss: 0.507571  [  128/  265]
train() client id: f_00001-1-4 loss: 0.540447  [  160/  265]
train() client id: f_00001-1-5 loss: 0.524898  [  192/  265]
train() client id: f_00001-1-6 loss: 0.582906  [  224/  265]
train() client id: f_00001-1-7 loss: 0.410426  [  256/  265]
train() client id: f_00001-2-0 loss: 0.736731  [   32/  265]
train() client id: f_00001-2-1 loss: 0.446407  [   64/  265]
train() client id: f_00001-2-2 loss: 0.563961  [   96/  265]
train() client id: f_00001-2-3 loss: 0.522889  [  128/  265]
train() client id: f_00001-2-4 loss: 0.408967  [  160/  265]
train() client id: f_00001-2-5 loss: 0.435552  [  192/  265]
train() client id: f_00001-2-6 loss: 0.457662  [  224/  265]
train() client id: f_00001-2-7 loss: 0.454270  [  256/  265]
train() client id: f_00001-3-0 loss: 0.453792  [   32/  265]
train() client id: f_00001-3-1 loss: 0.503211  [   64/  265]
train() client id: f_00001-3-2 loss: 0.470956  [   96/  265]
train() client id: f_00001-3-3 loss: 0.520346  [  128/  265]
train() client id: f_00001-3-4 loss: 0.404351  [  160/  265]
train() client id: f_00001-3-5 loss: 0.542796  [  192/  265]
train() client id: f_00001-3-6 loss: 0.562806  [  224/  265]
train() client id: f_00001-3-7 loss: 0.461065  [  256/  265]
train() client id: f_00001-4-0 loss: 0.652439  [   32/  265]
train() client id: f_00001-4-1 loss: 0.399105  [   64/  265]
train() client id: f_00001-4-2 loss: 0.461674  [   96/  265]
train() client id: f_00001-4-3 loss: 0.501295  [  128/  265]
train() client id: f_00001-4-4 loss: 0.450335  [  160/  265]
train() client id: f_00001-4-5 loss: 0.585618  [  192/  265]
train() client id: f_00001-4-6 loss: 0.383274  [  224/  265]
train() client id: f_00001-4-7 loss: 0.486406  [  256/  265]
train() client id: f_00001-5-0 loss: 0.632421  [   32/  265]
train() client id: f_00001-5-1 loss: 0.535958  [   64/  265]
train() client id: f_00001-5-2 loss: 0.457460  [   96/  265]
train() client id: f_00001-5-3 loss: 0.532295  [  128/  265]
train() client id: f_00001-5-4 loss: 0.491480  [  160/  265]
train() client id: f_00001-5-5 loss: 0.397296  [  192/  265]
train() client id: f_00001-5-6 loss: 0.439551  [  224/  265]
train() client id: f_00001-5-7 loss: 0.418163  [  256/  265]
train() client id: f_00001-6-0 loss: 0.463119  [   32/  265]
train() client id: f_00001-6-1 loss: 0.373128  [   64/  265]
train() client id: f_00001-6-2 loss: 0.464436  [   96/  265]
train() client id: f_00001-6-3 loss: 0.481829  [  128/  265]
train() client id: f_00001-6-4 loss: 0.514577  [  160/  265]
train() client id: f_00001-6-5 loss: 0.523864  [  192/  265]
train() client id: f_00001-6-6 loss: 0.409186  [  224/  265]
train() client id: f_00001-6-7 loss: 0.573694  [  256/  265]
train() client id: f_00001-7-0 loss: 0.458197  [   32/  265]
train() client id: f_00001-7-1 loss: 0.532793  [   64/  265]
train() client id: f_00001-7-2 loss: 0.466611  [   96/  265]
train() client id: f_00001-7-3 loss: 0.473223  [  128/  265]
train() client id: f_00001-7-4 loss: 0.456824  [  160/  265]
train() client id: f_00001-7-5 loss: 0.508164  [  192/  265]
train() client id: f_00001-7-6 loss: 0.503998  [  224/  265]
train() client id: f_00001-7-7 loss: 0.471146  [  256/  265]
train() client id: f_00001-8-0 loss: 0.499433  [   32/  265]
train() client id: f_00001-8-1 loss: 0.489385  [   64/  265]
train() client id: f_00001-8-2 loss: 0.449267  [   96/  265]
train() client id: f_00001-8-3 loss: 0.388451  [  128/  265]
train() client id: f_00001-8-4 loss: 0.532835  [  160/  265]
train() client id: f_00001-8-5 loss: 0.458322  [  192/  265]
train() client id: f_00001-8-6 loss: 0.637784  [  224/  265]
train() client id: f_00001-8-7 loss: 0.387583  [  256/  265]
train() client id: f_00001-9-0 loss: 0.445811  [   32/  265]
train() client id: f_00001-9-1 loss: 0.439629  [   64/  265]
train() client id: f_00001-9-2 loss: 0.458691  [   96/  265]
train() client id: f_00001-9-3 loss: 0.646406  [  128/  265]
train() client id: f_00001-9-4 loss: 0.393664  [  160/  265]
train() client id: f_00001-9-5 loss: 0.407101  [  192/  265]
train() client id: f_00001-9-6 loss: 0.482397  [  224/  265]
train() client id: f_00001-9-7 loss: 0.490984  [  256/  265]
train() client id: f_00001-10-0 loss: 0.497327  [   32/  265]
train() client id: f_00001-10-1 loss: 0.504617  [   64/  265]
train() client id: f_00001-10-2 loss: 0.530249  [   96/  265]
train() client id: f_00001-10-3 loss: 0.519374  [  128/  265]
train() client id: f_00001-10-4 loss: 0.458505  [  160/  265]
train() client id: f_00001-10-5 loss: 0.391979  [  192/  265]
train() client id: f_00001-10-6 loss: 0.562907  [  224/  265]
train() client id: f_00001-10-7 loss: 0.383369  [  256/  265]
train() client id: f_00001-11-0 loss: 0.390303  [   32/  265]
train() client id: f_00001-11-1 loss: 0.467918  [   64/  265]
train() client id: f_00001-11-2 loss: 0.373811  [   96/  265]
train() client id: f_00001-11-3 loss: 0.709336  [  128/  265]
train() client id: f_00001-11-4 loss: 0.390658  [  160/  265]
train() client id: f_00001-11-5 loss: 0.539695  [  192/  265]
train() client id: f_00001-11-6 loss: 0.511233  [  224/  265]
train() client id: f_00001-11-7 loss: 0.465713  [  256/  265]
train() client id: f_00001-12-0 loss: 0.421746  [   32/  265]
train() client id: f_00001-12-1 loss: 0.394770  [   64/  265]
train() client id: f_00001-12-2 loss: 0.541666  [   96/  265]
train() client id: f_00001-12-3 loss: 0.381380  [  128/  265]
train() client id: f_00001-12-4 loss: 0.520676  [  160/  265]
train() client id: f_00001-12-5 loss: 0.582198  [  192/  265]
train() client id: f_00001-12-6 loss: 0.468158  [  224/  265]
train() client id: f_00001-12-7 loss: 0.544329  [  256/  265]
train() client id: f_00002-0-0 loss: 1.247848  [   32/  124]
train() client id: f_00002-0-1 loss: 1.261935  [   64/  124]
train() client id: f_00002-0-2 loss: 1.211793  [   96/  124]
train() client id: f_00002-1-0 loss: 1.220467  [   32/  124]
train() client id: f_00002-1-1 loss: 1.149189  [   64/  124]
train() client id: f_00002-1-2 loss: 1.142503  [   96/  124]
train() client id: f_00002-2-0 loss: 1.081352  [   32/  124]
train() client id: f_00002-2-1 loss: 1.108501  [   64/  124]
train() client id: f_00002-2-2 loss: 1.166341  [   96/  124]
train() client id: f_00002-3-0 loss: 1.121696  [   32/  124]
train() client id: f_00002-3-1 loss: 1.137315  [   64/  124]
train() client id: f_00002-3-2 loss: 0.984850  [   96/  124]
train() client id: f_00002-4-0 loss: 1.141230  [   32/  124]
train() client id: f_00002-4-1 loss: 1.012959  [   64/  124]
train() client id: f_00002-4-2 loss: 1.048505  [   96/  124]
train() client id: f_00002-5-0 loss: 0.992231  [   32/  124]
train() client id: f_00002-5-1 loss: 1.018884  [   64/  124]
train() client id: f_00002-5-2 loss: 0.935849  [   96/  124]
train() client id: f_00002-6-0 loss: 1.003349  [   32/  124]
train() client id: f_00002-6-1 loss: 1.064317  [   64/  124]
train() client id: f_00002-6-2 loss: 0.991970  [   96/  124]
train() client id: f_00002-7-0 loss: 1.007316  [   32/  124]
train() client id: f_00002-7-1 loss: 0.994274  [   64/  124]
train() client id: f_00002-7-2 loss: 0.919513  [   96/  124]
train() client id: f_00002-8-0 loss: 0.966185  [   32/  124]
train() client id: f_00002-8-1 loss: 0.949855  [   64/  124]
train() client id: f_00002-8-2 loss: 0.963803  [   96/  124]
train() client id: f_00002-9-0 loss: 0.950465  [   32/  124]
train() client id: f_00002-9-1 loss: 0.983494  [   64/  124]
train() client id: f_00002-9-2 loss: 0.959680  [   96/  124]
train() client id: f_00002-10-0 loss: 0.990157  [   32/  124]
train() client id: f_00002-10-1 loss: 0.827311  [   64/  124]
train() client id: f_00002-10-2 loss: 1.001541  [   96/  124]
train() client id: f_00002-11-0 loss: 1.054765  [   32/  124]
train() client id: f_00002-11-1 loss: 0.848491  [   64/  124]
train() client id: f_00002-11-2 loss: 0.969430  [   96/  124]
train() client id: f_00002-12-0 loss: 1.034501  [   32/  124]
train() client id: f_00002-12-1 loss: 0.829366  [   64/  124]
train() client id: f_00002-12-2 loss: 0.795010  [   96/  124]
train() client id: f_00003-0-0 loss: 0.819868  [   32/   43]
train() client id: f_00003-1-0 loss: 0.831010  [   32/   43]
train() client id: f_00003-2-0 loss: 0.794239  [   32/   43]
train() client id: f_00003-3-0 loss: 0.784742  [   32/   43]
train() client id: f_00003-4-0 loss: 0.908830  [   32/   43]
train() client id: f_00003-5-0 loss: 0.993760  [   32/   43]
train() client id: f_00003-6-0 loss: 0.681667  [   32/   43]
train() client id: f_00003-7-0 loss: 0.714480  [   32/   43]
train() client id: f_00003-8-0 loss: 0.874542  [   32/   43]
train() client id: f_00003-9-0 loss: 0.836084  [   32/   43]
train() client id: f_00003-10-0 loss: 0.837486  [   32/   43]
train() client id: f_00003-11-0 loss: 0.848922  [   32/   43]
train() client id: f_00003-12-0 loss: 0.893237  [   32/   43]
train() client id: f_00004-0-0 loss: 1.149080  [   32/  306]
train() client id: f_00004-0-1 loss: 1.022236  [   64/  306]
train() client id: f_00004-0-2 loss: 0.971276  [   96/  306]
train() client id: f_00004-0-3 loss: 0.878590  [  128/  306]
train() client id: f_00004-0-4 loss: 0.896740  [  160/  306]
train() client id: f_00004-0-5 loss: 0.872253  [  192/  306]
train() client id: f_00004-0-6 loss: 1.046151  [  224/  306]
train() client id: f_00004-0-7 loss: 1.093486  [  256/  306]
train() client id: f_00004-0-8 loss: 0.830251  [  288/  306]
train() client id: f_00004-1-0 loss: 1.012875  [   32/  306]
train() client id: f_00004-1-1 loss: 0.873685  [   64/  306]
train() client id: f_00004-1-2 loss: 1.223282  [   96/  306]
train() client id: f_00004-1-3 loss: 0.999551  [  128/  306]
train() client id: f_00004-1-4 loss: 0.893372  [  160/  306]
train() client id: f_00004-1-5 loss: 0.989122  [  192/  306]
train() client id: f_00004-1-6 loss: 1.016393  [  224/  306]
train() client id: f_00004-1-7 loss: 0.989331  [  256/  306]
train() client id: f_00004-1-8 loss: 0.848354  [  288/  306]
train() client id: f_00004-2-0 loss: 0.914068  [   32/  306]
train() client id: f_00004-2-1 loss: 0.874337  [   64/  306]
train() client id: f_00004-2-2 loss: 1.008829  [   96/  306]
train() client id: f_00004-2-3 loss: 0.902084  [  128/  306]
train() client id: f_00004-2-4 loss: 0.988384  [  160/  306]
train() client id: f_00004-2-5 loss: 1.050673  [  192/  306]
train() client id: f_00004-2-6 loss: 0.993497  [  224/  306]
train() client id: f_00004-2-7 loss: 1.081652  [  256/  306]
train() client id: f_00004-2-8 loss: 0.971414  [  288/  306]
train() client id: f_00004-3-0 loss: 1.068787  [   32/  306]
train() client id: f_00004-3-1 loss: 0.942373  [   64/  306]
train() client id: f_00004-3-2 loss: 0.971887  [   96/  306]
train() client id: f_00004-3-3 loss: 1.035667  [  128/  306]
train() client id: f_00004-3-4 loss: 0.965578  [  160/  306]
train() client id: f_00004-3-5 loss: 0.868037  [  192/  306]
train() client id: f_00004-3-6 loss: 0.992190  [  224/  306]
train() client id: f_00004-3-7 loss: 0.900339  [  256/  306]
train() client id: f_00004-3-8 loss: 0.959288  [  288/  306]
train() client id: f_00004-4-0 loss: 0.866339  [   32/  306]
train() client id: f_00004-4-1 loss: 1.070232  [   64/  306]
train() client id: f_00004-4-2 loss: 1.014143  [   96/  306]
train() client id: f_00004-4-3 loss: 0.975228  [  128/  306]
train() client id: f_00004-4-4 loss: 0.853008  [  160/  306]
train() client id: f_00004-4-5 loss: 1.083528  [  192/  306]
train() client id: f_00004-4-6 loss: 0.846031  [  224/  306]
train() client id: f_00004-4-7 loss: 0.963132  [  256/  306]
train() client id: f_00004-4-8 loss: 0.855756  [  288/  306]
train() client id: f_00004-5-0 loss: 1.013324  [   32/  306]
train() client id: f_00004-5-1 loss: 0.792105  [   64/  306]
train() client id: f_00004-5-2 loss: 0.934844  [   96/  306]
train() client id: f_00004-5-3 loss: 1.001986  [  128/  306]
train() client id: f_00004-5-4 loss: 0.975129  [  160/  306]
train() client id: f_00004-5-5 loss: 1.062974  [  192/  306]
train() client id: f_00004-5-6 loss: 1.003472  [  224/  306]
train() client id: f_00004-5-7 loss: 0.978196  [  256/  306]
train() client id: f_00004-5-8 loss: 0.947555  [  288/  306]
train() client id: f_00004-6-0 loss: 0.886022  [   32/  306]
train() client id: f_00004-6-1 loss: 1.066802  [   64/  306]
train() client id: f_00004-6-2 loss: 0.840566  [   96/  306]
train() client id: f_00004-6-3 loss: 0.919902  [  128/  306]
train() client id: f_00004-6-4 loss: 1.049146  [  160/  306]
train() client id: f_00004-6-5 loss: 0.938590  [  192/  306]
train() client id: f_00004-6-6 loss: 0.979379  [  224/  306]
train() client id: f_00004-6-7 loss: 0.909747  [  256/  306]
train() client id: f_00004-6-8 loss: 1.051232  [  288/  306]
train() client id: f_00004-7-0 loss: 0.966105  [   32/  306]
train() client id: f_00004-7-1 loss: 1.056742  [   64/  306]
train() client id: f_00004-7-2 loss: 0.850604  [   96/  306]
train() client id: f_00004-7-3 loss: 0.966863  [  128/  306]
train() client id: f_00004-7-4 loss: 1.016009  [  160/  306]
train() client id: f_00004-7-5 loss: 0.988396  [  192/  306]
train() client id: f_00004-7-6 loss: 0.879978  [  224/  306]
train() client id: f_00004-7-7 loss: 0.937892  [  256/  306]
train() client id: f_00004-7-8 loss: 0.909187  [  288/  306]
train() client id: f_00004-8-0 loss: 1.107023  [   32/  306]
train() client id: f_00004-8-1 loss: 0.877218  [   64/  306]
train() client id: f_00004-8-2 loss: 0.946443  [   96/  306]
train() client id: f_00004-8-3 loss: 1.028296  [  128/  306]
train() client id: f_00004-8-4 loss: 0.922399  [  160/  306]
train() client id: f_00004-8-5 loss: 0.952367  [  192/  306]
train() client id: f_00004-8-6 loss: 0.870194  [  224/  306]
train() client id: f_00004-8-7 loss: 0.848772  [  256/  306]
train() client id: f_00004-8-8 loss: 0.995268  [  288/  306]
train() client id: f_00004-9-0 loss: 0.997612  [   32/  306]
train() client id: f_00004-9-1 loss: 0.967750  [   64/  306]
train() client id: f_00004-9-2 loss: 0.962363  [   96/  306]
train() client id: f_00004-9-3 loss: 0.864220  [  128/  306]
train() client id: f_00004-9-4 loss: 0.901910  [  160/  306]
train() client id: f_00004-9-5 loss: 0.868488  [  192/  306]
train() client id: f_00004-9-6 loss: 0.983729  [  224/  306]
train() client id: f_00004-9-7 loss: 1.027912  [  256/  306]
train() client id: f_00004-9-8 loss: 0.899834  [  288/  306]
train() client id: f_00004-10-0 loss: 0.829468  [   32/  306]
train() client id: f_00004-10-1 loss: 1.035808  [   64/  306]
train() client id: f_00004-10-2 loss: 0.863961  [   96/  306]
train() client id: f_00004-10-3 loss: 1.038427  [  128/  306]
train() client id: f_00004-10-4 loss: 1.132394  [  160/  306]
train() client id: f_00004-10-5 loss: 0.882609  [  192/  306]
train() client id: f_00004-10-6 loss: 0.853312  [  224/  306]
train() client id: f_00004-10-7 loss: 0.944239  [  256/  306]
train() client id: f_00004-10-8 loss: 0.981925  [  288/  306]
train() client id: f_00004-11-0 loss: 0.970401  [   32/  306]
train() client id: f_00004-11-1 loss: 1.161318  [   64/  306]
train() client id: f_00004-11-2 loss: 0.871656  [   96/  306]
train() client id: f_00004-11-3 loss: 0.984253  [  128/  306]
train() client id: f_00004-11-4 loss: 0.952840  [  160/  306]
train() client id: f_00004-11-5 loss: 0.943627  [  192/  306]
train() client id: f_00004-11-6 loss: 0.894497  [  224/  306]
train() client id: f_00004-11-7 loss: 0.856727  [  256/  306]
train() client id: f_00004-11-8 loss: 0.963973  [  288/  306]
train() client id: f_00004-12-0 loss: 1.001382  [   32/  306]
train() client id: f_00004-12-1 loss: 0.889280  [   64/  306]
train() client id: f_00004-12-2 loss: 1.100162  [   96/  306]
train() client id: f_00004-12-3 loss: 0.872696  [  128/  306]
train() client id: f_00004-12-4 loss: 0.976348  [  160/  306]
train() client id: f_00004-12-5 loss: 0.937810  [  192/  306]
train() client id: f_00004-12-6 loss: 1.005154  [  224/  306]
train() client id: f_00004-12-7 loss: 0.918586  [  256/  306]
train() client id: f_00004-12-8 loss: 0.813339  [  288/  306]
train() client id: f_00005-0-0 loss: 0.695715  [   32/  146]
train() client id: f_00005-0-1 loss: 0.606854  [   64/  146]
train() client id: f_00005-0-2 loss: 0.525552  [   96/  146]
train() client id: f_00005-0-3 loss: 0.672256  [  128/  146]
train() client id: f_00005-1-0 loss: 0.593057  [   32/  146]
train() client id: f_00005-1-1 loss: 0.591816  [   64/  146]
train() client id: f_00005-1-2 loss: 0.484800  [   96/  146]
train() client id: f_00005-1-3 loss: 0.616286  [  128/  146]
train() client id: f_00005-2-0 loss: 0.596133  [   32/  146]
train() client id: f_00005-2-1 loss: 0.441347  [   64/  146]
train() client id: f_00005-2-2 loss: 0.466509  [   96/  146]
train() client id: f_00005-2-3 loss: 0.731611  [  128/  146]
train() client id: f_00005-3-0 loss: 0.605476  [   32/  146]
train() client id: f_00005-3-1 loss: 0.525271  [   64/  146]
train() client id: f_00005-3-2 loss: 0.522000  [   96/  146]
train() client id: f_00005-3-3 loss: 0.573904  [  128/  146]
train() client id: f_00005-4-0 loss: 0.649094  [   32/  146]
train() client id: f_00005-4-1 loss: 0.443947  [   64/  146]
train() client id: f_00005-4-2 loss: 0.632187  [   96/  146]
train() client id: f_00005-4-3 loss: 0.555454  [  128/  146]
train() client id: f_00005-5-0 loss: 0.591037  [   32/  146]
train() client id: f_00005-5-1 loss: 0.534584  [   64/  146]
train() client id: f_00005-5-2 loss: 0.486078  [   96/  146]
train() client id: f_00005-5-3 loss: 0.555258  [  128/  146]
train() client id: f_00005-6-0 loss: 0.605525  [   32/  146]
train() client id: f_00005-6-1 loss: 0.692003  [   64/  146]
train() client id: f_00005-6-2 loss: 0.543707  [   96/  146]
train() client id: f_00005-6-3 loss: 0.445826  [  128/  146]
train() client id: f_00005-7-0 loss: 0.518563  [   32/  146]
train() client id: f_00005-7-1 loss: 0.533796  [   64/  146]
train() client id: f_00005-7-2 loss: 0.565729  [   96/  146]
train() client id: f_00005-7-3 loss: 0.628655  [  128/  146]
train() client id: f_00005-8-0 loss: 0.519509  [   32/  146]
train() client id: f_00005-8-1 loss: 0.546387  [   64/  146]
train() client id: f_00005-8-2 loss: 0.578022  [   96/  146]
train() client id: f_00005-8-3 loss: 0.485244  [  128/  146]
train() client id: f_00005-9-0 loss: 0.593710  [   32/  146]
train() client id: f_00005-9-1 loss: 0.622099  [   64/  146]
train() client id: f_00005-9-2 loss: 0.440732  [   96/  146]
train() client id: f_00005-9-3 loss: 0.568660  [  128/  146]
train() client id: f_00005-10-0 loss: 0.603846  [   32/  146]
train() client id: f_00005-10-1 loss: 0.587996  [   64/  146]
train() client id: f_00005-10-2 loss: 0.371878  [   96/  146]
train() client id: f_00005-10-3 loss: 0.698059  [  128/  146]
train() client id: f_00005-11-0 loss: 0.605893  [   32/  146]
train() client id: f_00005-11-1 loss: 0.522380  [   64/  146]
train() client id: f_00005-11-2 loss: 0.584180  [   96/  146]
train() client id: f_00005-11-3 loss: 0.271149  [  128/  146]
train() client id: f_00005-12-0 loss: 0.519622  [   32/  146]
train() client id: f_00005-12-1 loss: 0.702938  [   64/  146]
train() client id: f_00005-12-2 loss: 0.427324  [   96/  146]
train() client id: f_00005-12-3 loss: 0.569510  [  128/  146]
train() client id: f_00006-0-0 loss: 0.653209  [   32/   54]
train() client id: f_00006-1-0 loss: 0.700085  [   32/   54]
train() client id: f_00006-2-0 loss: 0.692554  [   32/   54]
train() client id: f_00006-3-0 loss: 0.673982  [   32/   54]
train() client id: f_00006-4-0 loss: 0.678930  [   32/   54]
train() client id: f_00006-5-0 loss: 0.598588  [   32/   54]
train() client id: f_00006-6-0 loss: 0.634055  [   32/   54]
train() client id: f_00006-7-0 loss: 0.688037  [   32/   54]
train() client id: f_00006-8-0 loss: 0.691719  [   32/   54]
train() client id: f_00006-9-0 loss: 0.685578  [   32/   54]
train() client id: f_00006-10-0 loss: 0.577594  [   32/   54]
train() client id: f_00006-11-0 loss: 0.637961  [   32/   54]
train() client id: f_00006-12-0 loss: 0.603614  [   32/   54]
train() client id: f_00007-0-0 loss: 0.642945  [   32/  179]
train() client id: f_00007-0-1 loss: 0.435048  [   64/  179]
train() client id: f_00007-0-2 loss: 0.447024  [   96/  179]
train() client id: f_00007-0-3 loss: 0.690457  [  128/  179]
train() client id: f_00007-0-4 loss: 0.675243  [  160/  179]
train() client id: f_00007-1-0 loss: 0.484057  [   32/  179]
train() client id: f_00007-1-1 loss: 0.636218  [   64/  179]
train() client id: f_00007-1-2 loss: 0.704727  [   96/  179]
train() client id: f_00007-1-3 loss: 0.492817  [  128/  179]
train() client id: f_00007-1-4 loss: 0.513973  [  160/  179]
train() client id: f_00007-2-0 loss: 0.547187  [   32/  179]
train() client id: f_00007-2-1 loss: 0.606810  [   64/  179]
train() client id: f_00007-2-2 loss: 0.478167  [   96/  179]
train() client id: f_00007-2-3 loss: 0.428230  [  128/  179]
train() client id: f_00007-2-4 loss: 0.711715  [  160/  179]
train() client id: f_00007-3-0 loss: 0.551984  [   32/  179]
train() client id: f_00007-3-1 loss: 0.585504  [   64/  179]
train() client id: f_00007-3-2 loss: 0.641673  [   96/  179]
train() client id: f_00007-3-3 loss: 0.446154  [  128/  179]
train() client id: f_00007-3-4 loss: 0.437471  [  160/  179]
train() client id: f_00007-4-0 loss: 0.580075  [   32/  179]
train() client id: f_00007-4-1 loss: 0.609573  [   64/  179]
train() client id: f_00007-4-2 loss: 0.461854  [   96/  179]
train() client id: f_00007-4-3 loss: 0.575919  [  128/  179]
train() client id: f_00007-4-4 loss: 0.446183  [  160/  179]
train() client id: f_00007-5-0 loss: 0.396637  [   32/  179]
train() client id: f_00007-5-1 loss: 0.445227  [   64/  179]
train() client id: f_00007-5-2 loss: 0.475385  [   96/  179]
train() client id: f_00007-5-3 loss: 0.549067  [  128/  179]
train() client id: f_00007-5-4 loss: 0.480705  [  160/  179]
train() client id: f_00007-6-0 loss: 0.481671  [   32/  179]
train() client id: f_00007-6-1 loss: 0.474516  [   64/  179]
train() client id: f_00007-6-2 loss: 0.408894  [   96/  179]
train() client id: f_00007-6-3 loss: 0.593550  [  128/  179]
train() client id: f_00007-6-4 loss: 0.518051  [  160/  179]
train() client id: f_00007-7-0 loss: 0.458990  [   32/  179]
train() client id: f_00007-7-1 loss: 0.649925  [   64/  179]
train() client id: f_00007-7-2 loss: 0.487853  [   96/  179]
train() client id: f_00007-7-3 loss: 0.392821  [  128/  179]
train() client id: f_00007-7-4 loss: 0.624135  [  160/  179]
train() client id: f_00007-8-0 loss: 0.431311  [   32/  179]
train() client id: f_00007-8-1 loss: 0.401523  [   64/  179]
train() client id: f_00007-8-2 loss: 0.540502  [   96/  179]
train() client id: f_00007-8-3 loss: 0.716936  [  128/  179]
train() client id: f_00007-8-4 loss: 0.506115  [  160/  179]
train() client id: f_00007-9-0 loss: 0.475604  [   32/  179]
train() client id: f_00007-9-1 loss: 0.445097  [   64/  179]
train() client id: f_00007-9-2 loss: 0.644596  [   96/  179]
train() client id: f_00007-9-3 loss: 0.604681  [  128/  179]
train() client id: f_00007-9-4 loss: 0.382914  [  160/  179]
train() client id: f_00007-10-0 loss: 0.574153  [   32/  179]
train() client id: f_00007-10-1 loss: 0.592092  [   64/  179]
train() client id: f_00007-10-2 loss: 0.449131  [   96/  179]
train() client id: f_00007-10-3 loss: 0.595830  [  128/  179]
train() client id: f_00007-10-4 loss: 0.358894  [  160/  179]
train() client id: f_00007-11-0 loss: 0.549057  [   32/  179]
train() client id: f_00007-11-1 loss: 0.596163  [   64/  179]
train() client id: f_00007-11-2 loss: 0.489325  [   96/  179]
train() client id: f_00007-11-3 loss: 0.590274  [  128/  179]
train() client id: f_00007-11-4 loss: 0.381546  [  160/  179]
train() client id: f_00007-12-0 loss: 0.490673  [   32/  179]
train() client id: f_00007-12-1 loss: 0.429115  [   64/  179]
train() client id: f_00007-12-2 loss: 0.654261  [   96/  179]
train() client id: f_00007-12-3 loss: 0.477106  [  128/  179]
train() client id: f_00007-12-4 loss: 0.548947  [  160/  179]
train() client id: f_00008-0-0 loss: 0.850100  [   32/  130]
train() client id: f_00008-0-1 loss: 0.852336  [   64/  130]
train() client id: f_00008-0-2 loss: 0.862331  [   96/  130]
train() client id: f_00008-0-3 loss: 0.853075  [  128/  130]
train() client id: f_00008-1-0 loss: 0.786493  [   32/  130]
train() client id: f_00008-1-1 loss: 0.878193  [   64/  130]
train() client id: f_00008-1-2 loss: 0.755648  [   96/  130]
train() client id: f_00008-1-3 loss: 0.960115  [  128/  130]
train() client id: f_00008-2-0 loss: 0.842446  [   32/  130]
train() client id: f_00008-2-1 loss: 0.856937  [   64/  130]
train() client id: f_00008-2-2 loss: 0.857899  [   96/  130]
train() client id: f_00008-2-3 loss: 0.827687  [  128/  130]
train() client id: f_00008-3-0 loss: 0.857474  [   32/  130]
train() client id: f_00008-3-1 loss: 0.799957  [   64/  130]
train() client id: f_00008-3-2 loss: 0.810470  [   96/  130]
train() client id: f_00008-3-3 loss: 0.926340  [  128/  130]
train() client id: f_00008-4-0 loss: 0.931096  [   32/  130]
train() client id: f_00008-4-1 loss: 0.928890  [   64/  130]
train() client id: f_00008-4-2 loss: 0.783273  [   96/  130]
train() client id: f_00008-4-3 loss: 0.758046  [  128/  130]
train() client id: f_00008-5-0 loss: 0.757433  [   32/  130]
train() client id: f_00008-5-1 loss: 0.927097  [   64/  130]
train() client id: f_00008-5-2 loss: 0.866644  [   96/  130]
train() client id: f_00008-5-3 loss: 0.853849  [  128/  130]
train() client id: f_00008-6-0 loss: 0.886058  [   32/  130]
train() client id: f_00008-6-1 loss: 0.808951  [   64/  130]
train() client id: f_00008-6-2 loss: 0.849992  [   96/  130]
train() client id: f_00008-6-3 loss: 0.895851  [  128/  130]
train() client id: f_00008-7-0 loss: 0.823880  [   32/  130]
train() client id: f_00008-7-1 loss: 0.903404  [   64/  130]
train() client id: f_00008-7-2 loss: 0.871746  [   96/  130]
train() client id: f_00008-7-3 loss: 0.835230  [  128/  130]
train() client id: f_00008-8-0 loss: 0.782599  [   32/  130]
train() client id: f_00008-8-1 loss: 0.892242  [   64/  130]
train() client id: f_00008-8-2 loss: 0.967101  [   96/  130]
train() client id: f_00008-8-3 loss: 0.796964  [  128/  130]
train() client id: f_00008-9-0 loss: 0.906307  [   32/  130]
train() client id: f_00008-9-1 loss: 0.821439  [   64/  130]
train() client id: f_00008-9-2 loss: 0.811451  [   96/  130]
train() client id: f_00008-9-3 loss: 0.906841  [  128/  130]
train() client id: f_00008-10-0 loss: 0.916043  [   32/  130]
train() client id: f_00008-10-1 loss: 0.841074  [   64/  130]
train() client id: f_00008-10-2 loss: 0.809408  [   96/  130]
train() client id: f_00008-10-3 loss: 0.859190  [  128/  130]
train() client id: f_00008-11-0 loss: 0.925424  [   32/  130]
train() client id: f_00008-11-1 loss: 0.857340  [   64/  130]
train() client id: f_00008-11-2 loss: 0.779236  [   96/  130]
train() client id: f_00008-11-3 loss: 0.841840  [  128/  130]
train() client id: f_00008-12-0 loss: 0.794567  [   32/  130]
train() client id: f_00008-12-1 loss: 0.906179  [   64/  130]
train() client id: f_00008-12-2 loss: 0.831710  [   96/  130]
train() client id: f_00008-12-3 loss: 0.893141  [  128/  130]
train() client id: f_00009-0-0 loss: 1.232115  [   32/  118]
train() client id: f_00009-0-1 loss: 1.148422  [   64/  118]
train() client id: f_00009-0-2 loss: 1.285354  [   96/  118]
train() client id: f_00009-1-0 loss: 1.125265  [   32/  118]
train() client id: f_00009-1-1 loss: 1.231977  [   64/  118]
train() client id: f_00009-1-2 loss: 1.091030  [   96/  118]
train() client id: f_00009-2-0 loss: 1.061428  [   32/  118]
train() client id: f_00009-2-1 loss: 1.019862  [   64/  118]
train() client id: f_00009-2-2 loss: 1.222797  [   96/  118]
train() client id: f_00009-3-0 loss: 0.994981  [   32/  118]
train() client id: f_00009-3-1 loss: 1.102935  [   64/  118]
train() client id: f_00009-3-2 loss: 1.082279  [   96/  118]
train() client id: f_00009-4-0 loss: 1.078551  [   32/  118]
train() client id: f_00009-4-1 loss: 1.075065  [   64/  118]
train() client id: f_00009-4-2 loss: 1.009459  [   96/  118]
train() client id: f_00009-5-0 loss: 0.978154  [   32/  118]
train() client id: f_00009-5-1 loss: 0.994174  [   64/  118]
train() client id: f_00009-5-2 loss: 1.107216  [   96/  118]
train() client id: f_00009-6-0 loss: 1.057055  [   32/  118]
train() client id: f_00009-6-1 loss: 0.977854  [   64/  118]
train() client id: f_00009-6-2 loss: 0.940519  [   96/  118]
train() client id: f_00009-7-0 loss: 0.976029  [   32/  118]
train() client id: f_00009-7-1 loss: 0.953008  [   64/  118]
train() client id: f_00009-7-2 loss: 0.992741  [   96/  118]
train() client id: f_00009-8-0 loss: 0.952316  [   32/  118]
train() client id: f_00009-8-1 loss: 0.983918  [   64/  118]
train() client id: f_00009-8-2 loss: 0.905295  [   96/  118]
train() client id: f_00009-9-0 loss: 0.928639  [   32/  118]
train() client id: f_00009-9-1 loss: 1.000451  [   64/  118]
train() client id: f_00009-9-2 loss: 1.022677  [   96/  118]
train() client id: f_00009-10-0 loss: 0.952016  [   32/  118]
train() client id: f_00009-10-1 loss: 1.006789  [   64/  118]
train() client id: f_00009-10-2 loss: 0.891054  [   96/  118]
train() client id: f_00009-11-0 loss: 0.965592  [   32/  118]
train() client id: f_00009-11-1 loss: 0.931481  [   64/  118]
train() client id: f_00009-11-2 loss: 1.007750  [   96/  118]
train() client id: f_00009-12-0 loss: 0.924590  [   32/  118]
train() client id: f_00009-12-1 loss: 0.911202  [   64/  118]
train() client id: f_00009-12-2 loss: 1.056151  [   96/  118]
At round 11 accuracy: 0.6312997347480106
At round 11 training accuracy: 0.5788061703554661
At round 11 training loss: 0.8553875789590966
update_location
xs = 8.927491 176.223621 5.882650 10.934260 -92.581990 59.769243 -5.849135 -5.143845 -115.120581 20.134486 
ys = -167.390647 7.291448 65.684448 -87.290817 -9.642386 0.794442 -96.381692 61.628436 25.881276 -602.232496 
xs mean: 6.317620029478787
ys mean: -80.16579882624053
dists_uav = 195.190494 202.750906 119.787530 133.188756 136.617716 116.503191 139.009507 117.577733 154.669288 610.810426 
uav_gains = -107.380222 -107.855858 -101.960499 -103.112604 -103.388972 -101.658586 -103.577754 -101.758288 -104.741909 -126.571207 
uav_gains_db_mean: -106.20058997057795
dists_bs = 388.665251 389.209700 211.349055 321.508976 202.201729 292.342562 319.781235 204.216123 160.692061 801.353742 
bs_gains = -112.075179 -112.092201 -104.667068 -109.768466 -104.129035 -108.612037 -109.702942 -104.249580 -101.334903 -120.874139 
bs_gains_db_mean: -108.75055503479201
Round 12
-------------------------------
ene_coms = [0.00905983 0.00931937 0.00781068 0.00723547 0.00760175 0.00676793
 0.00739871 0.00764761 0.00667183 0.03128134]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.46025445 19.65985728  9.28321485  3.32925263 22.62474762 10.87118711
  4.13988518 13.31256683  9.69552186  9.41391063]
obj_prev = 111.79039841911093
eta_min = 1.1907155661518542e-10	eta_max = 0.7389912029454719
af = 23.50535574047096	bf = 2.426330967858896	zeta = 25.855891314518058	eta = 0.9090909090909091
af = 23.50535574047096	bf = 2.426330967858896	zeta = 51.33293714832583	eta = 0.45790007442107883
af = 23.50535574047096	bf = 2.426330967858896	zeta = 38.34434271904953	eta = 0.6130071367423248
af = 23.50535574047096	bf = 2.426330967858896	zeta = 35.99381989776697	eta = 0.6530386551700564
af = 23.50535574047096	bf = 2.426330967858896	zeta = 35.86108015345631	eta = 0.6554558769531516
af = 23.50535574047096	bf = 2.426330967858896	zeta = 35.860615095105715	eta = 0.6554643772320289
af = 23.50535574047096	bf = 2.426330967858896	zeta = 35.860615089368856	eta = 0.6554643773368878
eta = 0.6554643773368878
ene_coms = [0.00905983 0.00931937 0.00781068 0.00723547 0.00760175 0.00676793
 0.00739871 0.00764761 0.00667183 0.03128134]
ene_comp = [0.03485637 0.07330903 0.03430309 0.01189543 0.08465118 0.04038913
 0.01493844 0.04951818 0.03596292 0.03264327]
ene_total = [3.06834212 5.77309002 2.9424095  1.33663988 6.44553811 3.29477448
 1.56065474 3.99406513 2.97880964 4.46629147]
ti_comp = [0.33003378 0.32743845 0.34252536 0.34827739 0.34461462 0.35295281
 0.34664502 0.34415606 0.3539138  0.10781872]
ti_coms = [0.09059834 0.09319367 0.07810676 0.07235473 0.0760175  0.06767931
 0.0739871  0.07647606 0.06671832 0.3128134 ]
t_total = [29.38877106 29.38877106 29.38877106 29.38877106 29.38877106 29.38877106
 29.38877106 29.38877106 29.38877106 29.38877106]
ene_coms = [0.00905983 0.00931937 0.00781068 0.00723547 0.00760175 0.00676793
 0.00739871 0.00764761 0.00667183 0.03128134]
ene_comp = [2.43001909e-05 2.29664100e-04 2.15027714e-05 8.67300612e-07
 3.19235907e-04 3.30552254e-05 1.73390857e-06 6.40712347e-05
 2.32086521e-05 1.87013325e-04]
ene_total = [0.63469126 0.66717274 0.54721952 0.50558942 0.55342432 0.47517204
 0.51705502 0.5388003  0.46776986 2.19863438]
optimize_network iter = 0 obj = 7.105528869216636
eta = 0.6554643773368878
freqs = [5.28072754e+07 1.11943224e+08 5.00738020e+07 1.70775195e+07
 1.22820071e+08 5.72160437e+07 2.15471784e+07 7.19414558e+07
 5.08074588e+07 1.51380335e+08]
eta_min = 0.6554643773368997	eta_max = 0.6554643773368838
af = 0.0515536500518396	bf = 2.426330967858896	zeta = 0.05670901505702357	eta = 0.909090909090909
af = 0.0515536500518396	bf = 2.426330967858896	zeta = 26.743690110663923	eta = 0.0019276939658855387
af = 0.0515536500518396	bf = 2.426330967858896	zeta = 2.7539237672546455	eta = 0.018720071581078235
af = 0.0515536500518396	bf = 2.426330967858896	zeta = 2.6816196966536783	eta = 0.019224817790595747
af = 0.0515536500518396	bf = 2.426330967858896	zeta = 2.681601256366252	eta = 0.01922494999189336
eta = 0.01922494999189336
ene_coms = [0.00905983 0.00931937 0.00781068 0.00723547 0.00760175 0.00676793
 0.00739871 0.00764761 0.00667183 0.03128134]
ene_comp = [2.27321868e-04 2.14844701e-03 2.01152748e-04 8.11336820e-06
 2.98636761e-03 3.09222904e-04 1.62202568e-05 5.99369481e-04
 2.17110811e-04 1.74946027e-03]
ene_total = [0.22794309 0.28146497 0.19664159 0.17778592 0.25987379 0.17370101
 0.18199137 0.202413   0.16908159 0.81070493]
ti_comp = [0.33003378 0.32743845 0.34252536 0.34827739 0.34461462 0.35295281
 0.34664502 0.34415606 0.3539138  0.10781872]
ti_coms = [0.09059834 0.09319367 0.07810676 0.07235473 0.0760175  0.06767931
 0.0739871  0.07647606 0.06671832 0.3128134 ]
t_total = [29.38877106 29.38877106 29.38877106 29.38877106 29.38877106 29.38877106
 29.38877106 29.38877106 29.38877106 29.38877106]
ene_coms = [0.00905983 0.00931937 0.00781068 0.00723547 0.00760175 0.00676793
 0.00739871 0.00764761 0.00667183 0.03128134]
ene_comp = [2.43001909e-05 2.29664100e-04 2.15027714e-05 8.67300612e-07
 3.19235907e-04 3.30552254e-05 1.73390857e-06 6.40712347e-05
 2.32086521e-05 1.87013325e-04]
ene_total = [0.63469126 0.66717274 0.54721952 0.50558942 0.55342432 0.47517204
 0.51705502 0.5388003  0.46776986 2.19863438]
optimize_network iter = 1 obj = 7.10552886921688
eta = 0.6554643773368997
freqs = [5.28072754e+07 1.11943224e+08 5.00738020e+07 1.70775195e+07
 1.22820071e+08 5.72160437e+07 2.15471784e+07 7.19414558e+07
 5.08074588e+07 1.51380335e+08]
Done!
ene_coms = [0.00905983 0.00931937 0.00781068 0.00723547 0.00760175 0.00676793
 0.00739871 0.00764761 0.00667183 0.03128134]
ene_comp = [2.28387023e-05 2.15851392e-04 2.02095283e-05 8.15138474e-07
 3.00036074e-04 3.10671820e-05 1.62962596e-06 6.02177926e-05
 2.18128120e-05 1.75765765e-04]
ene_total = [0.00908267 0.00953522 0.00783089 0.00723629 0.00790179 0.006799
 0.00740034 0.00770782 0.00669365 0.03145711]
At round 12 energy consumption: 0.1016447628198453
At round 12 eta: 0.6554643773368997
At round 12 a_n: 24.072052692622567
At round 12 local rounds: 13.831892773318181
At round 12 global rounds: 69.86810973726543
gradient difference: 0.41504842042922974
train() client id: f_00000-0-0 loss: 1.423171  [   32/  126]
train() client id: f_00000-0-1 loss: 1.737648  [   64/  126]
train() client id: f_00000-0-2 loss: 1.612874  [   96/  126]
train() client id: f_00000-1-0 loss: 1.514185  [   32/  126]
train() client id: f_00000-1-1 loss: 1.597842  [   64/  126]
train() client id: f_00000-1-2 loss: 1.332373  [   96/  126]
train() client id: f_00000-2-0 loss: 1.298445  [   32/  126]
train() client id: f_00000-2-1 loss: 1.225322  [   64/  126]
train() client id: f_00000-2-2 loss: 1.457941  [   96/  126]
train() client id: f_00000-3-0 loss: 1.257908  [   32/  126]
train() client id: f_00000-3-1 loss: 1.266258  [   64/  126]
train() client id: f_00000-3-2 loss: 1.109602  [   96/  126]
train() client id: f_00000-4-0 loss: 1.204328  [   32/  126]
train() client id: f_00000-4-1 loss: 1.138911  [   64/  126]
train() client id: f_00000-4-2 loss: 1.079812  [   96/  126]
train() client id: f_00000-5-0 loss: 1.082138  [   32/  126]
train() client id: f_00000-5-1 loss: 1.031009  [   64/  126]
train() client id: f_00000-5-2 loss: 1.105655  [   96/  126]
train() client id: f_00000-6-0 loss: 0.993581  [   32/  126]
train() client id: f_00000-6-1 loss: 0.994930  [   64/  126]
train() client id: f_00000-6-2 loss: 1.020570  [   96/  126]
train() client id: f_00000-7-0 loss: 1.015778  [   32/  126]
train() client id: f_00000-7-1 loss: 0.931812  [   64/  126]
train() client id: f_00000-7-2 loss: 0.945435  [   96/  126]
train() client id: f_00000-8-0 loss: 0.974135  [   32/  126]
train() client id: f_00000-8-1 loss: 0.903912  [   64/  126]
train() client id: f_00000-8-2 loss: 0.959064  [   96/  126]
train() client id: f_00000-9-0 loss: 0.911280  [   32/  126]
train() client id: f_00000-9-1 loss: 0.958809  [   64/  126]
train() client id: f_00000-9-2 loss: 0.939069  [   96/  126]
train() client id: f_00000-10-0 loss: 0.912627  [   32/  126]
train() client id: f_00000-10-1 loss: 0.880574  [   64/  126]
train() client id: f_00000-10-2 loss: 0.976020  [   96/  126]
train() client id: f_00000-11-0 loss: 0.965931  [   32/  126]
train() client id: f_00000-11-1 loss: 0.885678  [   64/  126]
train() client id: f_00000-11-2 loss: 0.907828  [   96/  126]
train() client id: f_00000-12-0 loss: 0.875306  [   32/  126]
train() client id: f_00000-12-1 loss: 0.891002  [   64/  126]
train() client id: f_00000-12-2 loss: 0.961298  [   96/  126]
train() client id: f_00001-0-0 loss: 0.484179  [   32/  265]
train() client id: f_00001-0-1 loss: 0.438095  [   64/  265]
train() client id: f_00001-0-2 loss: 0.436639  [   96/  265]
train() client id: f_00001-0-3 loss: 0.420559  [  128/  265]
train() client id: f_00001-0-4 loss: 0.372912  [  160/  265]
train() client id: f_00001-0-5 loss: 0.367566  [  192/  265]
train() client id: f_00001-0-6 loss: 0.405644  [  224/  265]
train() client id: f_00001-0-7 loss: 0.413961  [  256/  265]
train() client id: f_00001-1-0 loss: 0.519391  [   32/  265]
train() client id: f_00001-1-1 loss: 0.422193  [   64/  265]
train() client id: f_00001-1-2 loss: 0.440785  [   96/  265]
train() client id: f_00001-1-3 loss: 0.402669  [  128/  265]
train() client id: f_00001-1-4 loss: 0.321260  [  160/  265]
train() client id: f_00001-1-5 loss: 0.376934  [  192/  265]
train() client id: f_00001-1-6 loss: 0.324624  [  224/  265]
train() client id: f_00001-1-7 loss: 0.385598  [  256/  265]
train() client id: f_00001-2-0 loss: 0.446009  [   32/  265]
train() client id: f_00001-2-1 loss: 0.431065  [   64/  265]
train() client id: f_00001-2-2 loss: 0.376426  [   96/  265]
train() client id: f_00001-2-3 loss: 0.412393  [  128/  265]
train() client id: f_00001-2-4 loss: 0.428008  [  160/  265]
train() client id: f_00001-2-5 loss: 0.331593  [  192/  265]
train() client id: f_00001-2-6 loss: 0.358213  [  224/  265]
train() client id: f_00001-2-7 loss: 0.359322  [  256/  265]
train() client id: f_00001-3-0 loss: 0.315205  [   32/  265]
train() client id: f_00001-3-1 loss: 0.349602  [   64/  265]
train() client id: f_00001-3-2 loss: 0.379145  [   96/  265]
train() client id: f_00001-3-3 loss: 0.479272  [  128/  265]
train() client id: f_00001-3-4 loss: 0.284163  [  160/  265]
train() client id: f_00001-3-5 loss: 0.433801  [  192/  265]
train() client id: f_00001-3-6 loss: 0.432149  [  224/  265]
train() client id: f_00001-3-7 loss: 0.348235  [  256/  265]
train() client id: f_00001-4-0 loss: 0.399429  [   32/  265]
train() client id: f_00001-4-1 loss: 0.306187  [   64/  265]
train() client id: f_00001-4-2 loss: 0.329464  [   96/  265]
train() client id: f_00001-4-3 loss: 0.395049  [  128/  265]
train() client id: f_00001-4-4 loss: 0.406887  [  160/  265]
train() client id: f_00001-4-5 loss: 0.295179  [  192/  265]
train() client id: f_00001-4-6 loss: 0.419962  [  224/  265]
train() client id: f_00001-4-7 loss: 0.393909  [  256/  265]
train() client id: f_00001-5-0 loss: 0.379103  [   32/  265]
train() client id: f_00001-5-1 loss: 0.410658  [   64/  265]
train() client id: f_00001-5-2 loss: 0.359530  [   96/  265]
train() client id: f_00001-5-3 loss: 0.340255  [  128/  265]
train() client id: f_00001-5-4 loss: 0.275759  [  160/  265]
train() client id: f_00001-5-5 loss: 0.333832  [  192/  265]
train() client id: f_00001-5-6 loss: 0.506242  [  224/  265]
train() client id: f_00001-5-7 loss: 0.330403  [  256/  265]
train() client id: f_00001-6-0 loss: 0.310964  [   32/  265]
train() client id: f_00001-6-1 loss: 0.339726  [   64/  265]
train() client id: f_00001-6-2 loss: 0.485440  [   96/  265]
train() client id: f_00001-6-3 loss: 0.331135  [  128/  265]
train() client id: f_00001-6-4 loss: 0.325765  [  160/  265]
train() client id: f_00001-6-5 loss: 0.420655  [  192/  265]
train() client id: f_00001-6-6 loss: 0.277405  [  224/  265]
train() client id: f_00001-6-7 loss: 0.426388  [  256/  265]
train() client id: f_00001-7-0 loss: 0.267581  [   32/  265]
train() client id: f_00001-7-1 loss: 0.269284  [   64/  265]
train() client id: f_00001-7-2 loss: 0.401651  [   96/  265]
train() client id: f_00001-7-3 loss: 0.357040  [  128/  265]
train() client id: f_00001-7-4 loss: 0.499990  [  160/  265]
train() client id: f_00001-7-5 loss: 0.242714  [  192/  265]
train() client id: f_00001-7-6 loss: 0.439910  [  224/  265]
train() client id: f_00001-7-7 loss: 0.402733  [  256/  265]
train() client id: f_00001-8-0 loss: 0.396187  [   32/  265]
train() client id: f_00001-8-1 loss: 0.428929  [   64/  265]
train() client id: f_00001-8-2 loss: 0.343723  [   96/  265]
train() client id: f_00001-8-3 loss: 0.338241  [  128/  265]
train() client id: f_00001-8-4 loss: 0.323545  [  160/  265]
train() client id: f_00001-8-5 loss: 0.444536  [  192/  265]
train() client id: f_00001-8-6 loss: 0.325907  [  224/  265]
train() client id: f_00001-8-7 loss: 0.261344  [  256/  265]
train() client id: f_00001-9-0 loss: 0.300558  [   32/  265]
train() client id: f_00001-9-1 loss: 0.272035  [   64/  265]
train() client id: f_00001-9-2 loss: 0.363854  [   96/  265]
train() client id: f_00001-9-3 loss: 0.401464  [  128/  265]
train() client id: f_00001-9-4 loss: 0.252835  [  160/  265]
train() client id: f_00001-9-5 loss: 0.433844  [  192/  265]
train() client id: f_00001-9-6 loss: 0.402079  [  224/  265]
train() client id: f_00001-9-7 loss: 0.405911  [  256/  265]
train() client id: f_00001-10-0 loss: 0.405354  [   32/  265]
train() client id: f_00001-10-1 loss: 0.378805  [   64/  265]
train() client id: f_00001-10-2 loss: 0.300597  [   96/  265]
train() client id: f_00001-10-3 loss: 0.503088  [  128/  265]
train() client id: f_00001-10-4 loss: 0.336569  [  160/  265]
train() client id: f_00001-10-5 loss: 0.264333  [  192/  265]
train() client id: f_00001-10-6 loss: 0.302026  [  224/  265]
train() client id: f_00001-10-7 loss: 0.330194  [  256/  265]
train() client id: f_00001-11-0 loss: 0.248130  [   32/  265]
train() client id: f_00001-11-1 loss: 0.312351  [   64/  265]
train() client id: f_00001-11-2 loss: 0.371391  [   96/  265]
train() client id: f_00001-11-3 loss: 0.317732  [  128/  265]
train() client id: f_00001-11-4 loss: 0.468173  [  160/  265]
train() client id: f_00001-11-5 loss: 0.338527  [  192/  265]
train() client id: f_00001-11-6 loss: 0.498737  [  224/  265]
train() client id: f_00001-11-7 loss: 0.259458  [  256/  265]
train() client id: f_00001-12-0 loss: 0.528838  [   32/  265]
train() client id: f_00001-12-1 loss: 0.421240  [   64/  265]
train() client id: f_00001-12-2 loss: 0.241211  [   96/  265]
train() client id: f_00001-12-3 loss: 0.380480  [  128/  265]
train() client id: f_00001-12-4 loss: 0.265908  [  160/  265]
train() client id: f_00001-12-5 loss: 0.257880  [  192/  265]
train() client id: f_00001-12-6 loss: 0.310248  [  224/  265]
train() client id: f_00001-12-7 loss: 0.247436  [  256/  265]
train() client id: f_00002-0-0 loss: 1.436928  [   32/  124]
train() client id: f_00002-0-1 loss: 1.332864  [   64/  124]
train() client id: f_00002-0-2 loss: 1.232280  [   96/  124]
train() client id: f_00002-1-0 loss: 1.223749  [   32/  124]
train() client id: f_00002-1-1 loss: 1.185063  [   64/  124]
train() client id: f_00002-1-2 loss: 1.254831  [   96/  124]
train() client id: f_00002-2-0 loss: 1.223829  [   32/  124]
train() client id: f_00002-2-1 loss: 1.172765  [   64/  124]
train() client id: f_00002-2-2 loss: 1.165681  [   96/  124]
train() client id: f_00002-3-0 loss: 1.193915  [   32/  124]
train() client id: f_00002-3-1 loss: 1.231083  [   64/  124]
train() client id: f_00002-3-2 loss: 1.237467  [   96/  124]
train() client id: f_00002-4-0 loss: 1.143540  [   32/  124]
train() client id: f_00002-4-1 loss: 1.242190  [   64/  124]
train() client id: f_00002-4-2 loss: 1.187562  [   96/  124]
train() client id: f_00002-5-0 loss: 1.124881  [   32/  124]
train() client id: f_00002-5-1 loss: 1.200775  [   64/  124]
train() client id: f_00002-5-2 loss: 1.159767  [   96/  124]
train() client id: f_00002-6-0 loss: 1.115644  [   32/  124]
train() client id: f_00002-6-1 loss: 1.256677  [   64/  124]
train() client id: f_00002-6-2 loss: 1.156856  [   96/  124]
train() client id: f_00002-7-0 loss: 1.092417  [   32/  124]
train() client id: f_00002-7-1 loss: 1.113312  [   64/  124]
train() client id: f_00002-7-2 loss: 1.085186  [   96/  124]
train() client id: f_00002-8-0 loss: 1.058999  [   32/  124]
train() client id: f_00002-8-1 loss: 1.150250  [   64/  124]
train() client id: f_00002-8-2 loss: 1.154210  [   96/  124]
train() client id: f_00002-9-0 loss: 1.096417  [   32/  124]
train() client id: f_00002-9-1 loss: 1.034316  [   64/  124]
train() client id: f_00002-9-2 loss: 1.246923  [   96/  124]
train() client id: f_00002-10-0 loss: 1.046626  [   32/  124]
train() client id: f_00002-10-1 loss: 1.278042  [   64/  124]
train() client id: f_00002-10-2 loss: 1.125219  [   96/  124]
train() client id: f_00002-11-0 loss: 1.116488  [   32/  124]
train() client id: f_00002-11-1 loss: 1.148348  [   64/  124]
train() client id: f_00002-11-2 loss: 1.083461  [   96/  124]
train() client id: f_00002-12-0 loss: 0.995074  [   32/  124]
train() client id: f_00002-12-1 loss: 1.347991  [   64/  124]
train() client id: f_00002-12-2 loss: 1.121963  [   96/  124]
train() client id: f_00003-0-0 loss: 0.575209  [   32/   43]
train() client id: f_00003-1-0 loss: 0.849823  [   32/   43]
train() client id: f_00003-2-0 loss: 0.727506  [   32/   43]
train() client id: f_00003-3-0 loss: 0.829024  [   32/   43]
train() client id: f_00003-4-0 loss: 0.757124  [   32/   43]
train() client id: f_00003-5-0 loss: 0.801600  [   32/   43]
train() client id: f_00003-6-0 loss: 0.709561  [   32/   43]
train() client id: f_00003-7-0 loss: 0.731908  [   32/   43]
train() client id: f_00003-8-0 loss: 0.779741  [   32/   43]
train() client id: f_00003-9-0 loss: 0.582415  [   32/   43]
train() client id: f_00003-10-0 loss: 0.779756  [   32/   43]
train() client id: f_00003-11-0 loss: 0.609288  [   32/   43]
train() client id: f_00003-12-0 loss: 0.828997  [   32/   43]
train() client id: f_00004-0-0 loss: 1.059602  [   32/  306]
train() client id: f_00004-0-1 loss: 0.993059  [   64/  306]
train() client id: f_00004-0-2 loss: 1.232325  [   96/  306]
train() client id: f_00004-0-3 loss: 1.018554  [  128/  306]
train() client id: f_00004-0-4 loss: 1.061284  [  160/  306]
train() client id: f_00004-0-5 loss: 1.075500  [  192/  306]
train() client id: f_00004-0-6 loss: 1.094319  [  224/  306]
train() client id: f_00004-0-7 loss: 1.082297  [  256/  306]
train() client id: f_00004-0-8 loss: 0.943105  [  288/  306]
train() client id: f_00004-1-0 loss: 0.895270  [   32/  306]
train() client id: f_00004-1-1 loss: 1.059274  [   64/  306]
train() client id: f_00004-1-2 loss: 1.069142  [   96/  306]
train() client id: f_00004-1-3 loss: 1.134123  [  128/  306]
train() client id: f_00004-1-4 loss: 1.128340  [  160/  306]
train() client id: f_00004-1-5 loss: 1.090397  [  192/  306]
train() client id: f_00004-1-6 loss: 1.009505  [  224/  306]
train() client id: f_00004-1-7 loss: 1.088795  [  256/  306]
train() client id: f_00004-1-8 loss: 1.058533  [  288/  306]
train() client id: f_00004-2-0 loss: 0.991763  [   32/  306]
train() client id: f_00004-2-1 loss: 1.082571  [   64/  306]
train() client id: f_00004-2-2 loss: 1.038820  [   96/  306]
train() client id: f_00004-2-3 loss: 1.081695  [  128/  306]
train() client id: f_00004-2-4 loss: 1.039763  [  160/  306]
train() client id: f_00004-2-5 loss: 1.036559  [  192/  306]
train() client id: f_00004-2-6 loss: 1.123138  [  224/  306]
train() client id: f_00004-2-7 loss: 1.109500  [  256/  306]
train() client id: f_00004-2-8 loss: 0.994748  [  288/  306]
train() client id: f_00004-3-0 loss: 1.000168  [   32/  306]
train() client id: f_00004-3-1 loss: 1.005187  [   64/  306]
train() client id: f_00004-3-2 loss: 1.004294  [   96/  306]
train() client id: f_00004-3-3 loss: 0.992717  [  128/  306]
train() client id: f_00004-3-4 loss: 1.057674  [  160/  306]
train() client id: f_00004-3-5 loss: 1.067346  [  192/  306]
train() client id: f_00004-3-6 loss: 1.107320  [  224/  306]
train() client id: f_00004-3-7 loss: 0.960604  [  256/  306]
train() client id: f_00004-3-8 loss: 1.144912  [  288/  306]
train() client id: f_00004-4-0 loss: 0.902864  [   32/  306]
train() client id: f_00004-4-1 loss: 1.048206  [   64/  306]
train() client id: f_00004-4-2 loss: 0.933005  [   96/  306]
train() client id: f_00004-4-3 loss: 0.986206  [  128/  306]
train() client id: f_00004-4-4 loss: 1.009503  [  160/  306]
train() client id: f_00004-4-5 loss: 1.131681  [  192/  306]
train() client id: f_00004-4-6 loss: 1.104141  [  224/  306]
train() client id: f_00004-4-7 loss: 1.152830  [  256/  306]
train() client id: f_00004-4-8 loss: 1.073622  [  288/  306]
train() client id: f_00004-5-0 loss: 0.994401  [   32/  306]
train() client id: f_00004-5-1 loss: 0.934602  [   64/  306]
train() client id: f_00004-5-2 loss: 1.001047  [   96/  306]
train() client id: f_00004-5-3 loss: 1.087304  [  128/  306]
train() client id: f_00004-5-4 loss: 1.100184  [  160/  306]
train() client id: f_00004-5-5 loss: 1.105700  [  192/  306]
train() client id: f_00004-5-6 loss: 1.035319  [  224/  306]
train() client id: f_00004-5-7 loss: 0.887475  [  256/  306]
train() client id: f_00004-5-8 loss: 1.107940  [  288/  306]
train() client id: f_00004-6-0 loss: 0.981232  [   32/  306]
train() client id: f_00004-6-1 loss: 0.989153  [   64/  306]
train() client id: f_00004-6-2 loss: 0.990916  [   96/  306]
train() client id: f_00004-6-3 loss: 0.930786  [  128/  306]
train() client id: f_00004-6-4 loss: 1.079924  [  160/  306]
train() client id: f_00004-6-5 loss: 1.073099  [  192/  306]
train() client id: f_00004-6-6 loss: 1.090002  [  224/  306]
train() client id: f_00004-6-7 loss: 1.120749  [  256/  306]
train() client id: f_00004-6-8 loss: 1.023010  [  288/  306]
train() client id: f_00004-7-0 loss: 1.062152  [   32/  306]
train() client id: f_00004-7-1 loss: 1.033316  [   64/  306]
train() client id: f_00004-7-2 loss: 1.077484  [   96/  306]
train() client id: f_00004-7-3 loss: 0.991005  [  128/  306]
train() client id: f_00004-7-4 loss: 1.052814  [  160/  306]
train() client id: f_00004-7-5 loss: 1.055448  [  192/  306]
train() client id: f_00004-7-6 loss: 0.952652  [  224/  306]
train() client id: f_00004-7-7 loss: 0.983081  [  256/  306]
train() client id: f_00004-7-8 loss: 1.024481  [  288/  306]
train() client id: f_00004-8-0 loss: 1.091068  [   32/  306]
train() client id: f_00004-8-1 loss: 1.034410  [   64/  306]
train() client id: f_00004-8-2 loss: 0.974104  [   96/  306]
train() client id: f_00004-8-3 loss: 0.907827  [  128/  306]
train() client id: f_00004-8-4 loss: 1.052984  [  160/  306]
train() client id: f_00004-8-5 loss: 0.975211  [  192/  306]
train() client id: f_00004-8-6 loss: 0.990625  [  224/  306]
train() client id: f_00004-8-7 loss: 1.189410  [  256/  306]
train() client id: f_00004-8-8 loss: 1.006651  [  288/  306]
train() client id: f_00004-9-0 loss: 1.076701  [   32/  306]
train() client id: f_00004-9-1 loss: 1.008079  [   64/  306]
train() client id: f_00004-9-2 loss: 1.095581  [   96/  306]
train() client id: f_00004-9-3 loss: 1.027648  [  128/  306]
train() client id: f_00004-9-4 loss: 0.973812  [  160/  306]
train() client id: f_00004-9-5 loss: 0.978550  [  192/  306]
train() client id: f_00004-9-6 loss: 0.933795  [  224/  306]
train() client id: f_00004-9-7 loss: 0.966591  [  256/  306]
train() client id: f_00004-9-8 loss: 1.034972  [  288/  306]
train() client id: f_00004-10-0 loss: 0.986081  [   32/  306]
train() client id: f_00004-10-1 loss: 0.944864  [   64/  306]
train() client id: f_00004-10-2 loss: 1.181370  [   96/  306]
train() client id: f_00004-10-3 loss: 0.998781  [  128/  306]
train() client id: f_00004-10-4 loss: 1.086197  [  160/  306]
train() client id: f_00004-10-5 loss: 1.020756  [  192/  306]
train() client id: f_00004-10-6 loss: 0.963769  [  224/  306]
train() client id: f_00004-10-7 loss: 0.948038  [  256/  306]
train() client id: f_00004-10-8 loss: 0.998324  [  288/  306]
train() client id: f_00004-11-0 loss: 0.916082  [   32/  306]
train() client id: f_00004-11-1 loss: 1.042436  [   64/  306]
train() client id: f_00004-11-2 loss: 0.989821  [   96/  306]
train() client id: f_00004-11-3 loss: 0.843510  [  128/  306]
train() client id: f_00004-11-4 loss: 1.168247  [  160/  306]
train() client id: f_00004-11-5 loss: 1.074109  [  192/  306]
train() client id: f_00004-11-6 loss: 1.098412  [  224/  306]
train() client id: f_00004-11-7 loss: 1.033537  [  256/  306]
train() client id: f_00004-11-8 loss: 0.969080  [  288/  306]
train() client id: f_00004-12-0 loss: 1.069902  [   32/  306]
train() client id: f_00004-12-1 loss: 0.889988  [   64/  306]
train() client id: f_00004-12-2 loss: 1.034828  [   96/  306]
train() client id: f_00004-12-3 loss: 1.042329  [  128/  306]
train() client id: f_00004-12-4 loss: 0.963536  [  160/  306]
train() client id: f_00004-12-5 loss: 1.009328  [  192/  306]
train() client id: f_00004-12-6 loss: 0.929827  [  224/  306]
train() client id: f_00004-12-7 loss: 1.111527  [  256/  306]
train() client id: f_00004-12-8 loss: 1.028696  [  288/  306]
train() client id: f_00005-0-0 loss: 0.653513  [   32/  146]
train() client id: f_00005-0-1 loss: 0.676310  [   64/  146]
train() client id: f_00005-0-2 loss: 0.846132  [   96/  146]
train() client id: f_00005-0-3 loss: 0.575683  [  128/  146]
train() client id: f_00005-1-0 loss: 0.954710  [   32/  146]
train() client id: f_00005-1-1 loss: 0.764275  [   64/  146]
train() client id: f_00005-1-2 loss: 0.488080  [   96/  146]
train() client id: f_00005-1-3 loss: 0.715852  [  128/  146]
train() client id: f_00005-2-0 loss: 0.721198  [   32/  146]
train() client id: f_00005-2-1 loss: 0.734914  [   64/  146]
train() client id: f_00005-2-2 loss: 0.565202  [   96/  146]
train() client id: f_00005-2-3 loss: 0.727488  [  128/  146]
train() client id: f_00005-3-0 loss: 0.705159  [   32/  146]
train() client id: f_00005-3-1 loss: 0.674412  [   64/  146]
train() client id: f_00005-3-2 loss: 0.800389  [   96/  146]
train() client id: f_00005-3-3 loss: 0.589457  [  128/  146]
train() client id: f_00005-4-0 loss: 0.719422  [   32/  146]
train() client id: f_00005-4-1 loss: 0.690548  [   64/  146]
train() client id: f_00005-4-2 loss: 0.656478  [   96/  146]
train() client id: f_00005-4-3 loss: 0.680349  [  128/  146]
train() client id: f_00005-5-0 loss: 0.607454  [   32/  146]
train() client id: f_00005-5-1 loss: 0.819062  [   64/  146]
train() client id: f_00005-5-2 loss: 0.721065  [   96/  146]
train() client id: f_00005-5-3 loss: 0.629309  [  128/  146]
train() client id: f_00005-6-0 loss: 0.808257  [   32/  146]
train() client id: f_00005-6-1 loss: 0.686340  [   64/  146]
train() client id: f_00005-6-2 loss: 0.629444  [   96/  146]
train() client id: f_00005-6-3 loss: 0.638428  [  128/  146]
train() client id: f_00005-7-0 loss: 0.654633  [   32/  146]
train() client id: f_00005-7-1 loss: 0.726701  [   64/  146]
train() client id: f_00005-7-2 loss: 0.554682  [   96/  146]
train() client id: f_00005-7-3 loss: 0.762913  [  128/  146]
train() client id: f_00005-8-0 loss: 0.737879  [   32/  146]
train() client id: f_00005-8-1 loss: 0.647844  [   64/  146]
train() client id: f_00005-8-2 loss: 0.815062  [   96/  146]
train() client id: f_00005-8-3 loss: 0.627856  [  128/  146]
train() client id: f_00005-9-0 loss: 0.619111  [   32/  146]
train() client id: f_00005-9-1 loss: 0.572052  [   64/  146]
train() client id: f_00005-9-2 loss: 0.589985  [   96/  146]
train() client id: f_00005-9-3 loss: 0.932421  [  128/  146]
train() client id: f_00005-10-0 loss: 0.628864  [   32/  146]
train() client id: f_00005-10-1 loss: 0.694059  [   64/  146]
train() client id: f_00005-10-2 loss: 0.533217  [   96/  146]
train() client id: f_00005-10-3 loss: 0.768910  [  128/  146]
train() client id: f_00005-11-0 loss: 0.621540  [   32/  146]
train() client id: f_00005-11-1 loss: 0.734340  [   64/  146]
train() client id: f_00005-11-2 loss: 0.617097  [   96/  146]
train() client id: f_00005-11-3 loss: 0.622673  [  128/  146]
train() client id: f_00005-12-0 loss: 0.738799  [   32/  146]
train() client id: f_00005-12-1 loss: 0.908639  [   64/  146]
train() client id: f_00005-12-2 loss: 0.617876  [   96/  146]
train() client id: f_00005-12-3 loss: 0.511125  [  128/  146]
train() client id: f_00006-0-0 loss: 0.671988  [   32/   54]
train() client id: f_00006-1-0 loss: 0.703578  [   32/   54]
train() client id: f_00006-2-0 loss: 0.647884  [   32/   54]
train() client id: f_00006-3-0 loss: 0.725760  [   32/   54]
train() client id: f_00006-4-0 loss: 0.710260  [   32/   54]
train() client id: f_00006-5-0 loss: 0.708260  [   32/   54]
train() client id: f_00006-6-0 loss: 0.697182  [   32/   54]
train() client id: f_00006-7-0 loss: 0.655363  [   32/   54]
train() client id: f_00006-8-0 loss: 0.704649  [   32/   54]
train() client id: f_00006-9-0 loss: 0.721469  [   32/   54]
train() client id: f_00006-10-0 loss: 0.631608  [   32/   54]
train() client id: f_00006-11-0 loss: 0.660457  [   32/   54]
train() client id: f_00006-12-0 loss: 0.721429  [   32/   54]
train() client id: f_00007-0-0 loss: 0.637205  [   32/  179]
train() client id: f_00007-0-1 loss: 0.553031  [   64/  179]
train() client id: f_00007-0-2 loss: 0.670187  [   96/  179]
train() client id: f_00007-0-3 loss: 0.671077  [  128/  179]
train() client id: f_00007-0-4 loss: 0.606471  [  160/  179]
train() client id: f_00007-1-0 loss: 0.589624  [   32/  179]
train() client id: f_00007-1-1 loss: 0.552795  [   64/  179]
train() client id: f_00007-1-2 loss: 0.674290  [   96/  179]
train() client id: f_00007-1-3 loss: 0.632231  [  128/  179]
train() client id: f_00007-1-4 loss: 0.588644  [  160/  179]
train() client id: f_00007-2-0 loss: 0.551357  [   32/  179]
train() client id: f_00007-2-1 loss: 0.560768  [   64/  179]
train() client id: f_00007-2-2 loss: 0.612704  [   96/  179]
train() client id: f_00007-2-3 loss: 0.673402  [  128/  179]
train() client id: f_00007-2-4 loss: 0.614427  [  160/  179]
train() client id: f_00007-3-0 loss: 0.503833  [   32/  179]
train() client id: f_00007-3-1 loss: 0.661807  [   64/  179]
train() client id: f_00007-3-2 loss: 0.643345  [   96/  179]
train() client id: f_00007-3-3 loss: 0.579977  [  128/  179]
train() client id: f_00007-3-4 loss: 0.552950  [  160/  179]
train() client id: f_00007-4-0 loss: 0.597431  [   32/  179]
train() client id: f_00007-4-1 loss: 0.475768  [   64/  179]
train() client id: f_00007-4-2 loss: 0.643221  [   96/  179]
train() client id: f_00007-4-3 loss: 0.538043  [  128/  179]
train() client id: f_00007-4-4 loss: 0.580036  [  160/  179]
train() client id: f_00007-5-0 loss: 0.519292  [   32/  179]
train() client id: f_00007-5-1 loss: 0.510351  [   64/  179]
train() client id: f_00007-5-2 loss: 0.695039  [   96/  179]
train() client id: f_00007-5-3 loss: 0.457012  [  128/  179]
train() client id: f_00007-5-4 loss: 0.546027  [  160/  179]
train() client id: f_00007-6-0 loss: 0.856222  [   32/  179]
train() client id: f_00007-6-1 loss: 0.410024  [   64/  179]
train() client id: f_00007-6-2 loss: 0.542882  [   96/  179]
train() client id: f_00007-6-3 loss: 0.469153  [  128/  179]
train() client id: f_00007-6-4 loss: 0.550803  [  160/  179]
train() client id: f_00007-7-0 loss: 0.769158  [   32/  179]
train() client id: f_00007-7-1 loss: 0.531939  [   64/  179]
train() client id: f_00007-7-2 loss: 0.463185  [   96/  179]
train() client id: f_00007-7-3 loss: 0.417404  [  128/  179]
train() client id: f_00007-7-4 loss: 0.487624  [  160/  179]
train() client id: f_00007-8-0 loss: 0.572151  [   32/  179]
train() client id: f_00007-8-1 loss: 0.588758  [   64/  179]
train() client id: f_00007-8-2 loss: 0.514800  [   96/  179]
train() client id: f_00007-8-3 loss: 0.425850  [  128/  179]
train() client id: f_00007-8-4 loss: 0.568659  [  160/  179]
train() client id: f_00007-9-0 loss: 0.530060  [   32/  179]
train() client id: f_00007-9-1 loss: 0.435475  [   64/  179]
train() client id: f_00007-9-2 loss: 0.506928  [   96/  179]
train() client id: f_00007-9-3 loss: 0.740647  [  128/  179]
train() client id: f_00007-9-4 loss: 0.427646  [  160/  179]
train() client id: f_00007-10-0 loss: 0.515781  [   32/  179]
train() client id: f_00007-10-1 loss: 0.420328  [   64/  179]
train() client id: f_00007-10-2 loss: 0.620382  [   96/  179]
train() client id: f_00007-10-3 loss: 0.627965  [  128/  179]
train() client id: f_00007-10-4 loss: 0.496654  [  160/  179]
train() client id: f_00007-11-0 loss: 0.623402  [   32/  179]
train() client id: f_00007-11-1 loss: 0.573069  [   64/  179]
train() client id: f_00007-11-2 loss: 0.439080  [   96/  179]
train() client id: f_00007-11-3 loss: 0.436484  [  128/  179]
train() client id: f_00007-11-4 loss: 0.702658  [  160/  179]
train() client id: f_00007-12-0 loss: 0.578158  [   32/  179]
train() client id: f_00007-12-1 loss: 0.423714  [   64/  179]
train() client id: f_00007-12-2 loss: 0.706657  [   96/  179]
train() client id: f_00007-12-3 loss: 0.572945  [  128/  179]
train() client id: f_00007-12-4 loss: 0.484061  [  160/  179]
train() client id: f_00008-0-0 loss: 0.844294  [   32/  130]
train() client id: f_00008-0-1 loss: 0.710471  [   64/  130]
train() client id: f_00008-0-2 loss: 0.757162  [   96/  130]
train() client id: f_00008-0-3 loss: 0.780877  [  128/  130]
train() client id: f_00008-1-0 loss: 0.787340  [   32/  130]
train() client id: f_00008-1-1 loss: 0.751193  [   64/  130]
train() client id: f_00008-1-2 loss: 0.729538  [   96/  130]
train() client id: f_00008-1-3 loss: 0.801438  [  128/  130]
train() client id: f_00008-2-0 loss: 0.745962  [   32/  130]
train() client id: f_00008-2-1 loss: 0.802479  [   64/  130]
train() client id: f_00008-2-2 loss: 0.775024  [   96/  130]
train() client id: f_00008-2-3 loss: 0.750473  [  128/  130]
train() client id: f_00008-3-0 loss: 0.692148  [   32/  130]
train() client id: f_00008-3-1 loss: 0.741632  [   64/  130]
train() client id: f_00008-3-2 loss: 0.951669  [   96/  130]
train() client id: f_00008-3-3 loss: 0.712884  [  128/  130]
train() client id: f_00008-4-0 loss: 0.832853  [   32/  130]
train() client id: f_00008-4-1 loss: 0.769712  [   64/  130]
train() client id: f_00008-4-2 loss: 0.803444  [   96/  130]
train() client id: f_00008-4-3 loss: 0.695545  [  128/  130]
train() client id: f_00008-5-0 loss: 0.779198  [   32/  130]
train() client id: f_00008-5-1 loss: 0.706583  [   64/  130]
train() client id: f_00008-5-2 loss: 0.763977  [   96/  130]
train() client id: f_00008-5-3 loss: 0.837697  [  128/  130]
train() client id: f_00008-6-0 loss: 0.869616  [   32/  130]
train() client id: f_00008-6-1 loss: 0.766978  [   64/  130]
train() client id: f_00008-6-2 loss: 0.679039  [   96/  130]
train() client id: f_00008-6-3 loss: 0.778305  [  128/  130]
train() client id: f_00008-7-0 loss: 0.688034  [   32/  130]
train() client id: f_00008-7-1 loss: 0.788374  [   64/  130]
train() client id: f_00008-7-2 loss: 0.742211  [   96/  130]
train() client id: f_00008-7-3 loss: 0.872720  [  128/  130]
train() client id: f_00008-8-0 loss: 0.863675  [   32/  130]
train() client id: f_00008-8-1 loss: 0.746310  [   64/  130]
train() client id: f_00008-8-2 loss: 0.820648  [   96/  130]
train() client id: f_00008-8-3 loss: 0.667110  [  128/  130]
train() client id: f_00008-9-0 loss: 0.739703  [   32/  130]
train() client id: f_00008-9-1 loss: 0.741565  [   64/  130]
train() client id: f_00008-9-2 loss: 0.803644  [   96/  130]
train() client id: f_00008-9-3 loss: 0.819712  [  128/  130]
train() client id: f_00008-10-0 loss: 0.795822  [   32/  130]
train() client id: f_00008-10-1 loss: 0.874734  [   64/  130]
train() client id: f_00008-10-2 loss: 0.701565  [   96/  130]
train() client id: f_00008-10-3 loss: 0.706318  [  128/  130]
train() client id: f_00008-11-0 loss: 0.816739  [   32/  130]
train() client id: f_00008-11-1 loss: 0.783373  [   64/  130]
train() client id: f_00008-11-2 loss: 0.710509  [   96/  130]
train() client id: f_00008-11-3 loss: 0.755649  [  128/  130]
train() client id: f_00008-12-0 loss: 0.728168  [   32/  130]
train() client id: f_00008-12-1 loss: 0.854322  [   64/  130]
train() client id: f_00008-12-2 loss: 0.833445  [   96/  130]
train() client id: f_00008-12-3 loss: 0.689027  [  128/  130]
train() client id: f_00009-0-0 loss: 1.165125  [   32/  118]
train() client id: f_00009-0-1 loss: 1.147616  [   64/  118]
train() client id: f_00009-0-2 loss: 1.224918  [   96/  118]
train() client id: f_00009-1-0 loss: 1.183656  [   32/  118]
train() client id: f_00009-1-1 loss: 1.046846  [   64/  118]
train() client id: f_00009-1-2 loss: 1.119970  [   96/  118]
train() client id: f_00009-2-0 loss: 1.053151  [   32/  118]
train() client id: f_00009-2-1 loss: 1.109405  [   64/  118]
train() client id: f_00009-2-2 loss: 0.990826  [   96/  118]
train() client id: f_00009-3-0 loss: 0.949346  [   32/  118]
train() client id: f_00009-3-1 loss: 1.084659  [   64/  118]
train() client id: f_00009-3-2 loss: 0.945821  [   96/  118]
train() client id: f_00009-4-0 loss: 1.087181  [   32/  118]
train() client id: f_00009-4-1 loss: 0.860788  [   64/  118]
train() client id: f_00009-4-2 loss: 0.936328  [   96/  118]
train() client id: f_00009-5-0 loss: 0.940955  [   32/  118]
train() client id: f_00009-5-1 loss: 0.990434  [   64/  118]
train() client id: f_00009-5-2 loss: 0.849442  [   96/  118]
train() client id: f_00009-6-0 loss: 0.819541  [   32/  118]
train() client id: f_00009-6-1 loss: 1.001616  [   64/  118]
train() client id: f_00009-6-2 loss: 0.906298  [   96/  118]
train() client id: f_00009-7-0 loss: 0.842959  [   32/  118]
train() client id: f_00009-7-1 loss: 0.884823  [   64/  118]
train() client id: f_00009-7-2 loss: 0.925496  [   96/  118]
train() client id: f_00009-8-0 loss: 0.859031  [   32/  118]
train() client id: f_00009-8-1 loss: 0.875544  [   64/  118]
train() client id: f_00009-8-2 loss: 0.889910  [   96/  118]
train() client id: f_00009-9-0 loss: 0.792238  [   32/  118]
train() client id: f_00009-9-1 loss: 0.876013  [   64/  118]
train() client id: f_00009-9-2 loss: 0.800688  [   96/  118]
train() client id: f_00009-10-0 loss: 0.832327  [   32/  118]
train() client id: f_00009-10-1 loss: 0.743373  [   64/  118]
train() client id: f_00009-10-2 loss: 0.846574  [   96/  118]
train() client id: f_00009-11-0 loss: 0.792048  [   32/  118]
train() client id: f_00009-11-1 loss: 0.842992  [   64/  118]
train() client id: f_00009-11-2 loss: 0.818680  [   96/  118]
train() client id: f_00009-12-0 loss: 0.814572  [   32/  118]
train() client id: f_00009-12-1 loss: 0.769752  [   64/  118]
train() client id: f_00009-12-2 loss: 0.889989  [   96/  118]
At round 12 accuracy: 0.6312997347480106
At round 12 training accuracy: 0.5781354795439303
At round 12 training loss: 0.8614529823764856
update_location
xs = 8.927491 181.223621 5.882650 10.934260 -97.581990 54.769243 -5.849135 -5.143845 -120.120581 20.134486 
ys = -172.390647 7.291448 70.684448 -82.290817 -9.642386 0.794442 -91.381692 66.628436 25.881276 -607.232496 
xs mean: 5.317620029478787
ys mean: -79.16579882624053
dists_uav = 199.494951 207.111482 122.600558 129.966675 140.054348 114.018863 135.590656 120.273886 158.425990 615.740775 
uav_gains = -107.650197 -108.134329 -102.212600 -102.846453 -103.659232 -101.424521 -103.306913 -102.004505 -105.004955 -126.660844 
uav_gains_db_mean: -106.29045495256537
dists_bs = 393.077071 393.727604 208.806770 317.443087 200.215781 288.342645 315.549079 201.483273 158.896647 806.204159 
bs_gains = -112.212435 -112.232543 -104.519907 -109.613704 -104.009012 -108.444508 -109.540933 -104.085751 -101.198272 -120.947520 
bs_gains_db_mean: -108.68045844488546
Round 13
-------------------------------
ene_coms = [0.00920542 0.00947801 0.00775243 0.00714521 0.00755662 0.00669817
 0.0073028  0.00758542 0.00663207 0.03160254]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.32912449 19.38389952  9.14971832  3.27971373 22.30171529 10.71481774
  4.07867555 13.12163805  9.55660131  9.28764938]
obj_prev = 110.20355338896061
eta_min = 8.664248031538558e-11	eta_max = 0.7396751255101508
af = 23.170874003776266	bf = 2.3956997095940338	zeta = 25.487961404153893	eta = 0.9090909090909091
af = 23.170874003776266	bf = 2.3956997095940338	zeta = 50.64531864829036	eta = 0.45751265116303985
af = 23.170874003776266	bf = 2.3956997095940338	zeta = 37.81504189868754	eta = 0.612742253885496
af = 23.170874003776266	bf = 2.3956997095940338	zeta = 35.493276650085164	eta = 0.6528243146500442
af = 23.170874003776266	bf = 2.3956997095940338	zeta = 35.3620810657759	eta = 0.6552463346452051
af = 23.170874003776266	bf = 2.3956997095940338	zeta = 35.361620786278095	eta = 0.655254863565745
af = 23.170874003776266	bf = 2.3956997095940338	zeta = 35.361620780584516	eta = 0.6552548636712476
eta = 0.6552548636712476
ene_coms = [0.00920542 0.00947801 0.00775243 0.00714521 0.00755662 0.00669817
 0.0073028  0.00758542 0.00663207 0.03160254]
ene_comp = [0.03488275 0.07336451 0.03432906 0.01190443 0.08471525 0.04041969
 0.01494975 0.04955565 0.03599014 0.03266797]
ene_total = [3.03467839 5.70221863 2.89655382 1.31122533 6.35126015 3.24321809
 1.53168791 3.9331356  2.93377312 4.42386973]
ti_comp = [0.33416937 0.33144355 0.34869933 0.35477152 0.35065738 0.35924192
 0.35319564 0.35036945 0.3599029  0.1101982 ]
ti_coms = [0.09205423 0.09478006 0.07752428 0.07145209 0.07556623 0.06698169
 0.07302797 0.07585415 0.06632071 0.3160254 ]
t_total = [29.33783531 29.33783531 29.33783531 29.33783531 29.33783531 29.33783531
 29.33783531 29.33783531 29.33783531 29.33783531]
ene_coms = [0.00920542 0.00947801 0.00775243 0.00714521 0.00755662 0.00669817
 0.0073028  0.00758542 0.00663207 0.03160254]
ene_comp = [2.37563041e-05 2.24656509e-04 2.07952120e-05 8.37738369e-07
 3.09028723e-04 3.19804877e-05 1.67398336e-06 6.19593922e-05
 2.24936477e-05 1.79431018e-04]
ene_total = [0.63526316 0.66785392 0.53504668 0.4918769  0.54140876 0.46324983
 0.50278153 0.52638431 0.45804715 2.18761755]
optimize_network iter = 0 obj = 7.009529793226833
eta = 0.6552548636712476
freqs = [5.21932182e+07 1.10674222e+08 4.92244354e+07 1.67776019e+07
 1.20794906e+08 5.62569293e+07 2.11635539e+07 7.07191397e+07
 4.99997898e+07 1.48223706e+08]
eta_min = 0.6552548636712541	eta_max = 0.6552548636712463
af = 0.04920763765679714	bf = 2.3956997095940338	zeta = 0.05412840142247686	eta = 0.9090909090909091
af = 0.04920763765679714	bf = 2.3956997095940338	zeta = 26.404286682255336	eta = 0.0018636230642756393
af = 0.04920763765679714	bf = 2.3956997095940338	zeta = 2.7100316453779825	eta = 0.018157587842460007
af = 0.04920763765679714	bf = 2.3956997095940338	zeta = 2.640909272250013	eta = 0.01863283914137384
af = 0.04920763765679714	bf = 2.3956997095940338	zeta = 2.6408926792838883	eta = 0.018632956213177287
eta = 0.018632956213177287
ene_coms = [0.00920542 0.00947801 0.00775243 0.00714521 0.00755662 0.00669817
 0.0073028  0.00758542 0.00663207 0.03160254]
ene_comp = [2.23823570e-04 2.11663488e-03 1.95925198e-04 7.89287724e-06
 2.91156030e-03 3.01308944e-04 1.57716844e-05 5.83759673e-04
 2.11927263e-04 1.69053615e-03]
ene_total = [0.22799969 0.28035902 0.19219159 0.17296238 0.25312123 0.16924774
 0.17696337 0.19753108 0.16548823 0.80502835]
ti_comp = [0.33416937 0.33144355 0.34869933 0.35477152 0.35065738 0.35924192
 0.35319564 0.35036945 0.3599029  0.1101982 ]
ti_coms = [0.09205423 0.09478006 0.07752428 0.07145209 0.07556623 0.06698169
 0.07302797 0.07585415 0.06632071 0.3160254 ]
t_total = [29.33783531 29.33783531 29.33783531 29.33783531 29.33783531 29.33783531
 29.33783531 29.33783531 29.33783531 29.33783531]
ene_coms = [0.00920542 0.00947801 0.00775243 0.00714521 0.00755662 0.00669817
 0.0073028  0.00758542 0.00663207 0.03160254]
ene_comp = [2.37563041e-05 2.24656509e-04 2.07952120e-05 8.37738369e-07
 3.09028723e-04 3.19804877e-05 1.67398336e-06 6.19593922e-05
 2.24936477e-05 1.79431018e-04]
ene_total = [0.63526316 0.66785392 0.53504668 0.4918769  0.54140876 0.46324983
 0.50278153 0.52638431 0.45804715 2.18761755]
optimize_network iter = 1 obj = 7.009529793226963
eta = 0.6552548636712541
freqs = [5.21932182e+07 1.10674222e+08 4.92244354e+07 1.67776019e+07
 1.20794906e+08 5.62569293e+07 2.11635539e+07 7.07191397e+07
 4.99997898e+07 1.48223706e+08]
Done!
ene_coms = [0.00920542 0.00947801 0.00775243 0.00714521 0.00755662 0.00669817
 0.0073028  0.00758542 0.00663207 0.03160254]
ene_comp = [2.23106413e-05 2.10985293e-04 1.95297430e-05 7.86758754e-07
 2.90223132e-04 3.00343515e-05 1.57211501e-06 5.81889239e-05
 2.11248224e-05 1.68511948e-04]
ene_total = [0.00922773 0.00968899 0.00777196 0.007146   0.00784685 0.0067282
 0.00730437 0.0076436  0.0066532  0.03177105]
At round 13 energy consumption: 0.10178194863082127
At round 13 eta: 0.6552548636712541
At round 13 a_n: 23.72950684565325
At round 13 local rounds: 13.842361135594555
At round 13 global rounds: 68.83202791010517
gradient difference: 0.438056081533432
train() client id: f_00000-0-0 loss: 1.544016  [   32/  126]
train() client id: f_00000-0-1 loss: 1.451272  [   64/  126]
train() client id: f_00000-0-2 loss: 1.418299  [   96/  126]
train() client id: f_00000-1-0 loss: 1.401655  [   32/  126]
train() client id: f_00000-1-1 loss: 1.410419  [   64/  126]
train() client id: f_00000-1-2 loss: 1.321090  [   96/  126]
train() client id: f_00000-2-0 loss: 1.233446  [   32/  126]
train() client id: f_00000-2-1 loss: 1.337835  [   64/  126]
train() client id: f_00000-2-2 loss: 1.302409  [   96/  126]
train() client id: f_00000-3-0 loss: 1.180854  [   32/  126]
train() client id: f_00000-3-1 loss: 1.171454  [   64/  126]
train() client id: f_00000-3-2 loss: 1.280942  [   96/  126]
train() client id: f_00000-4-0 loss: 1.103921  [   32/  126]
train() client id: f_00000-4-1 loss: 1.097290  [   64/  126]
train() client id: f_00000-4-2 loss: 1.085903  [   96/  126]
train() client id: f_00000-5-0 loss: 1.064186  [   32/  126]
train() client id: f_00000-5-1 loss: 1.120046  [   64/  126]
train() client id: f_00000-5-2 loss: 0.981827  [   96/  126]
train() client id: f_00000-6-0 loss: 1.000740  [   32/  126]
train() client id: f_00000-6-1 loss: 1.027668  [   64/  126]
train() client id: f_00000-6-2 loss: 0.973497  [   96/  126]
train() client id: f_00000-7-0 loss: 0.973722  [   32/  126]
train() client id: f_00000-7-1 loss: 0.884512  [   64/  126]
train() client id: f_00000-7-2 loss: 0.909338  [   96/  126]
train() client id: f_00000-8-0 loss: 0.915079  [   32/  126]
train() client id: f_00000-8-1 loss: 0.916373  [   64/  126]
train() client id: f_00000-8-2 loss: 0.911349  [   96/  126]
train() client id: f_00000-9-0 loss: 0.962020  [   32/  126]
train() client id: f_00000-9-1 loss: 0.885427  [   64/  126]
train() client id: f_00000-9-2 loss: 0.846472  [   96/  126]
train() client id: f_00000-10-0 loss: 0.880862  [   32/  126]
train() client id: f_00000-10-1 loss: 0.829147  [   64/  126]
train() client id: f_00000-10-2 loss: 0.976701  [   96/  126]
train() client id: f_00000-11-0 loss: 0.864433  [   32/  126]
train() client id: f_00000-11-1 loss: 0.812232  [   64/  126]
train() client id: f_00000-11-2 loss: 0.869641  [   96/  126]
train() client id: f_00000-12-0 loss: 0.816329  [   32/  126]
train() client id: f_00000-12-1 loss: 0.839898  [   64/  126]
train() client id: f_00000-12-2 loss: 0.914972  [   96/  126]
train() client id: f_00001-0-0 loss: 0.496041  [   32/  265]
train() client id: f_00001-0-1 loss: 0.665910  [   64/  265]
train() client id: f_00001-0-2 loss: 0.554692  [   96/  265]
train() client id: f_00001-0-3 loss: 0.469533  [  128/  265]
train() client id: f_00001-0-4 loss: 0.573582  [  160/  265]
train() client id: f_00001-0-5 loss: 0.546451  [  192/  265]
train() client id: f_00001-0-6 loss: 0.499696  [  224/  265]
train() client id: f_00001-0-7 loss: 0.552621  [  256/  265]
train() client id: f_00001-1-0 loss: 0.558130  [   32/  265]
train() client id: f_00001-1-1 loss: 0.580833  [   64/  265]
train() client id: f_00001-1-2 loss: 0.474725  [   96/  265]
train() client id: f_00001-1-3 loss: 0.470995  [  128/  265]
train() client id: f_00001-1-4 loss: 0.518357  [  160/  265]
train() client id: f_00001-1-5 loss: 0.490362  [  192/  265]
train() client id: f_00001-1-6 loss: 0.599534  [  224/  265]
train() client id: f_00001-1-7 loss: 0.629492  [  256/  265]
train() client id: f_00001-2-0 loss: 0.642104  [   32/  265]
train() client id: f_00001-2-1 loss: 0.479006  [   64/  265]
train() client id: f_00001-2-2 loss: 0.510209  [   96/  265]
train() client id: f_00001-2-3 loss: 0.486988  [  128/  265]
train() client id: f_00001-2-4 loss: 0.465067  [  160/  265]
train() client id: f_00001-2-5 loss: 0.580066  [  192/  265]
train() client id: f_00001-2-6 loss: 0.460243  [  224/  265]
train() client id: f_00001-2-7 loss: 0.633226  [  256/  265]
train() client id: f_00001-3-0 loss: 0.524262  [   32/  265]
train() client id: f_00001-3-1 loss: 0.430197  [   64/  265]
train() client id: f_00001-3-2 loss: 0.445995  [   96/  265]
train() client id: f_00001-3-3 loss: 0.542172  [  128/  265]
train() client id: f_00001-3-4 loss: 0.568449  [  160/  265]
train() client id: f_00001-3-5 loss: 0.493894  [  192/  265]
train() client id: f_00001-3-6 loss: 0.640699  [  224/  265]
train() client id: f_00001-3-7 loss: 0.551411  [  256/  265]
train() client id: f_00001-4-0 loss: 0.658651  [   32/  265]
train() client id: f_00001-4-1 loss: 0.561674  [   64/  265]
train() client id: f_00001-4-2 loss: 0.467912  [   96/  265]
train() client id: f_00001-4-3 loss: 0.487286  [  128/  265]
train() client id: f_00001-4-4 loss: 0.502866  [  160/  265]
train() client id: f_00001-4-5 loss: 0.510383  [  192/  265]
train() client id: f_00001-4-6 loss: 0.485984  [  224/  265]
train() client id: f_00001-4-7 loss: 0.475533  [  256/  265]
train() client id: f_00001-5-0 loss: 0.618632  [   32/  265]
train() client id: f_00001-5-1 loss: 0.504898  [   64/  265]
train() client id: f_00001-5-2 loss: 0.458731  [   96/  265]
train() client id: f_00001-5-3 loss: 0.522106  [  128/  265]
train() client id: f_00001-5-4 loss: 0.599070  [  160/  265]
train() client id: f_00001-5-5 loss: 0.456057  [  192/  265]
train() client id: f_00001-5-6 loss: 0.479195  [  224/  265]
train() client id: f_00001-5-7 loss: 0.413584  [  256/  265]
train() client id: f_00001-6-0 loss: 0.507973  [   32/  265]
train() client id: f_00001-6-1 loss: 0.460860  [   64/  265]
train() client id: f_00001-6-2 loss: 0.460405  [   96/  265]
train() client id: f_00001-6-3 loss: 0.448783  [  128/  265]
train() client id: f_00001-6-4 loss: 0.458884  [  160/  265]
train() client id: f_00001-6-5 loss: 0.539816  [  192/  265]
train() client id: f_00001-6-6 loss: 0.537764  [  224/  265]
train() client id: f_00001-6-7 loss: 0.631946  [  256/  265]
train() client id: f_00001-7-0 loss: 0.484955  [   32/  265]
train() client id: f_00001-7-1 loss: 0.617002  [   64/  265]
train() client id: f_00001-7-2 loss: 0.404064  [   96/  265]
train() client id: f_00001-7-3 loss: 0.548550  [  128/  265]
train() client id: f_00001-7-4 loss: 0.531721  [  160/  265]
train() client id: f_00001-7-5 loss: 0.484679  [  192/  265]
train() client id: f_00001-7-6 loss: 0.523737  [  224/  265]
train() client id: f_00001-7-7 loss: 0.428650  [  256/  265]
train() client id: f_00001-8-0 loss: 0.507109  [   32/  265]
train() client id: f_00001-8-1 loss: 0.611709  [   64/  265]
train() client id: f_00001-8-2 loss: 0.480581  [   96/  265]
train() client id: f_00001-8-3 loss: 0.495183  [  128/  265]
train() client id: f_00001-8-4 loss: 0.596086  [  160/  265]
train() client id: f_00001-8-5 loss: 0.420922  [  192/  265]
train() client id: f_00001-8-6 loss: 0.544196  [  224/  265]
train() client id: f_00001-8-7 loss: 0.419688  [  256/  265]
train() client id: f_00001-9-0 loss: 0.472241  [   32/  265]
train() client id: f_00001-9-1 loss: 0.465717  [   64/  265]
train() client id: f_00001-9-2 loss: 0.493511  [   96/  265]
train() client id: f_00001-9-3 loss: 0.429662  [  128/  265]
train() client id: f_00001-9-4 loss: 0.544001  [  160/  265]
train() client id: f_00001-9-5 loss: 0.568768  [  192/  265]
train() client id: f_00001-9-6 loss: 0.574198  [  224/  265]
train() client id: f_00001-9-7 loss: 0.513287  [  256/  265]
train() client id: f_00001-10-0 loss: 0.399825  [   32/  265]
train() client id: f_00001-10-1 loss: 0.475843  [   64/  265]
train() client id: f_00001-10-2 loss: 0.533234  [   96/  265]
train() client id: f_00001-10-3 loss: 0.458766  [  128/  265]
train() client id: f_00001-10-4 loss: 0.433176  [  160/  265]
train() client id: f_00001-10-5 loss: 0.465958  [  192/  265]
train() client id: f_00001-10-6 loss: 0.674217  [  224/  265]
train() client id: f_00001-10-7 loss: 0.609025  [  256/  265]
train() client id: f_00001-11-0 loss: 0.486973  [   32/  265]
train() client id: f_00001-11-1 loss: 0.470783  [   64/  265]
train() client id: f_00001-11-2 loss: 0.406705  [   96/  265]
train() client id: f_00001-11-3 loss: 0.541340  [  128/  265]
train() client id: f_00001-11-4 loss: 0.663618  [  160/  265]
train() client id: f_00001-11-5 loss: 0.460015  [  192/  265]
train() client id: f_00001-11-6 loss: 0.566735  [  224/  265]
train() client id: f_00001-11-7 loss: 0.462053  [  256/  265]
train() client id: f_00001-12-0 loss: 0.536022  [   32/  265]
train() client id: f_00001-12-1 loss: 0.499916  [   64/  265]
train() client id: f_00001-12-2 loss: 0.543948  [   96/  265]
train() client id: f_00001-12-3 loss: 0.648842  [  128/  265]
train() client id: f_00001-12-4 loss: 0.459126  [  160/  265]
train() client id: f_00001-12-5 loss: 0.490994  [  192/  265]
train() client id: f_00001-12-6 loss: 0.461007  [  224/  265]
train() client id: f_00001-12-7 loss: 0.415682  [  256/  265]
train() client id: f_00002-0-0 loss: 1.251833  [   32/  124]
train() client id: f_00002-0-1 loss: 1.202460  [   64/  124]
train() client id: f_00002-0-2 loss: 1.249336  [   96/  124]
train() client id: f_00002-1-0 loss: 1.211092  [   32/  124]
train() client id: f_00002-1-1 loss: 1.233420  [   64/  124]
train() client id: f_00002-1-2 loss: 1.166075  [   96/  124]
train() client id: f_00002-2-0 loss: 1.061105  [   32/  124]
train() client id: f_00002-2-1 loss: 1.192422  [   64/  124]
train() client id: f_00002-2-2 loss: 1.151998  [   96/  124]
train() client id: f_00002-3-0 loss: 1.159625  [   32/  124]
train() client id: f_00002-3-1 loss: 1.176183  [   64/  124]
train() client id: f_00002-3-2 loss: 1.003088  [   96/  124]
train() client id: f_00002-4-0 loss: 1.094669  [   32/  124]
train() client id: f_00002-4-1 loss: 1.114437  [   64/  124]
train() client id: f_00002-4-2 loss: 1.046973  [   96/  124]
train() client id: f_00002-5-0 loss: 1.065384  [   32/  124]
train() client id: f_00002-5-1 loss: 1.085731  [   64/  124]
train() client id: f_00002-5-2 loss: 1.066798  [   96/  124]
train() client id: f_00002-6-0 loss: 0.998754  [   32/  124]
train() client id: f_00002-6-1 loss: 1.054079  [   64/  124]
train() client id: f_00002-6-2 loss: 1.022694  [   96/  124]
train() client id: f_00002-7-0 loss: 1.004420  [   32/  124]
train() client id: f_00002-7-1 loss: 1.027310  [   64/  124]
train() client id: f_00002-7-2 loss: 1.074406  [   96/  124]
train() client id: f_00002-8-0 loss: 0.996348  [   32/  124]
train() client id: f_00002-8-1 loss: 0.981116  [   64/  124]
train() client id: f_00002-8-2 loss: 0.982869  [   96/  124]
train() client id: f_00002-9-0 loss: 0.900621  [   32/  124]
train() client id: f_00002-9-1 loss: 1.059773  [   64/  124]
train() client id: f_00002-9-2 loss: 1.019727  [   96/  124]
train() client id: f_00002-10-0 loss: 0.933409  [   32/  124]
train() client id: f_00002-10-1 loss: 0.971473  [   64/  124]
train() client id: f_00002-10-2 loss: 0.909534  [   96/  124]
train() client id: f_00002-11-0 loss: 0.992875  [   32/  124]
train() client id: f_00002-11-1 loss: 0.954324  [   64/  124]
train() client id: f_00002-11-2 loss: 0.983811  [   96/  124]
train() client id: f_00002-12-0 loss: 0.985364  [   32/  124]
train() client id: f_00002-12-1 loss: 1.019044  [   64/  124]
train() client id: f_00002-12-2 loss: 0.972661  [   96/  124]
train() client id: f_00003-0-0 loss: 0.740297  [   32/   43]
train() client id: f_00003-1-0 loss: 0.671650  [   32/   43]
train() client id: f_00003-2-0 loss: 0.743381  [   32/   43]
train() client id: f_00003-3-0 loss: 0.805527  [   32/   43]
train() client id: f_00003-4-0 loss: 0.685986  [   32/   43]
train() client id: f_00003-5-0 loss: 0.688246  [   32/   43]
train() client id: f_00003-6-0 loss: 0.651424  [   32/   43]
train() client id: f_00003-7-0 loss: 0.748750  [   32/   43]
train() client id: f_00003-8-0 loss: 0.726722  [   32/   43]
train() client id: f_00003-9-0 loss: 0.762709  [   32/   43]
train() client id: f_00003-10-0 loss: 0.766017  [   32/   43]
train() client id: f_00003-11-0 loss: 0.655010  [   32/   43]
train() client id: f_00003-12-0 loss: 0.838913  [   32/   43]
train() client id: f_00004-0-0 loss: 0.700014  [   32/  306]
train() client id: f_00004-0-1 loss: 0.995119  [   64/  306]
train() client id: f_00004-0-2 loss: 0.874827  [   96/  306]
train() client id: f_00004-0-3 loss: 0.867788  [  128/  306]
train() client id: f_00004-0-4 loss: 0.856409  [  160/  306]
train() client id: f_00004-0-5 loss: 0.873550  [  192/  306]
train() client id: f_00004-0-6 loss: 0.773110  [  224/  306]
train() client id: f_00004-0-7 loss: 0.744079  [  256/  306]
train() client id: f_00004-0-8 loss: 0.845842  [  288/  306]
train() client id: f_00004-1-0 loss: 0.790986  [   32/  306]
train() client id: f_00004-1-1 loss: 0.793422  [   64/  306]
train() client id: f_00004-1-2 loss: 0.811657  [   96/  306]
train() client id: f_00004-1-3 loss: 0.856462  [  128/  306]
train() client id: f_00004-1-4 loss: 0.741221  [  160/  306]
train() client id: f_00004-1-5 loss: 0.858812  [  192/  306]
train() client id: f_00004-1-6 loss: 0.824638  [  224/  306]
train() client id: f_00004-1-7 loss: 0.966571  [  256/  306]
train() client id: f_00004-1-8 loss: 0.891734  [  288/  306]
train() client id: f_00004-2-0 loss: 0.722698  [   32/  306]
train() client id: f_00004-2-1 loss: 0.785919  [   64/  306]
train() client id: f_00004-2-2 loss: 0.844927  [   96/  306]
train() client id: f_00004-2-3 loss: 0.954007  [  128/  306]
train() client id: f_00004-2-4 loss: 0.911421  [  160/  306]
train() client id: f_00004-2-5 loss: 0.799434  [  192/  306]
train() client id: f_00004-2-6 loss: 0.751643  [  224/  306]
train() client id: f_00004-2-7 loss: 0.893929  [  256/  306]
train() client id: f_00004-2-8 loss: 0.947258  [  288/  306]
train() client id: f_00004-3-0 loss: 0.808834  [   32/  306]
train() client id: f_00004-3-1 loss: 0.714043  [   64/  306]
train() client id: f_00004-3-2 loss: 0.873872  [   96/  306]
train() client id: f_00004-3-3 loss: 0.833963  [  128/  306]
train() client id: f_00004-3-4 loss: 0.827343  [  160/  306]
train() client id: f_00004-3-5 loss: 0.970704  [  192/  306]
train() client id: f_00004-3-6 loss: 0.792883  [  224/  306]
train() client id: f_00004-3-7 loss: 0.848809  [  256/  306]
train() client id: f_00004-3-8 loss: 0.791394  [  288/  306]
train() client id: f_00004-4-0 loss: 0.948807  [   32/  306]
train() client id: f_00004-4-1 loss: 0.728439  [   64/  306]
train() client id: f_00004-4-2 loss: 0.800048  [   96/  306]
train() client id: f_00004-4-3 loss: 0.771045  [  128/  306]
train() client id: f_00004-4-4 loss: 0.921321  [  160/  306]
train() client id: f_00004-4-5 loss: 0.779792  [  192/  306]
train() client id: f_00004-4-6 loss: 0.758934  [  224/  306]
train() client id: f_00004-4-7 loss: 0.853522  [  256/  306]
train() client id: f_00004-4-8 loss: 1.017428  [  288/  306]
train() client id: f_00004-5-0 loss: 0.854853  [   32/  306]
train() client id: f_00004-5-1 loss: 0.725040  [   64/  306]
train() client id: f_00004-5-2 loss: 0.901531  [   96/  306]
train() client id: f_00004-5-3 loss: 0.864328  [  128/  306]
train() client id: f_00004-5-4 loss: 0.840624  [  160/  306]
train() client id: f_00004-5-5 loss: 0.829966  [  192/  306]
train() client id: f_00004-5-6 loss: 0.852407  [  224/  306]
train() client id: f_00004-5-7 loss: 0.819906  [  256/  306]
train() client id: f_00004-5-8 loss: 0.872004  [  288/  306]
train() client id: f_00004-6-0 loss: 0.882753  [   32/  306]
train() client id: f_00004-6-1 loss: 0.816911  [   64/  306]
train() client id: f_00004-6-2 loss: 0.808737  [   96/  306]
train() client id: f_00004-6-3 loss: 0.820793  [  128/  306]
train() client id: f_00004-6-4 loss: 0.878539  [  160/  306]
train() client id: f_00004-6-5 loss: 0.841943  [  192/  306]
train() client id: f_00004-6-6 loss: 0.821545  [  224/  306]
train() client id: f_00004-6-7 loss: 0.816816  [  256/  306]
train() client id: f_00004-6-8 loss: 0.912409  [  288/  306]
train() client id: f_00004-7-0 loss: 0.851611  [   32/  306]
train() client id: f_00004-7-1 loss: 0.764874  [   64/  306]
train() client id: f_00004-7-2 loss: 0.868230  [   96/  306]
train() client id: f_00004-7-3 loss: 0.841235  [  128/  306]
train() client id: f_00004-7-4 loss: 0.851203  [  160/  306]
train() client id: f_00004-7-5 loss: 0.773928  [  192/  306]
train() client id: f_00004-7-6 loss: 0.927243  [  224/  306]
train() client id: f_00004-7-7 loss: 0.767583  [  256/  306]
train() client id: f_00004-7-8 loss: 0.883686  [  288/  306]
train() client id: f_00004-8-0 loss: 0.815488  [   32/  306]
train() client id: f_00004-8-1 loss: 0.881619  [   64/  306]
train() client id: f_00004-8-2 loss: 0.851737  [   96/  306]
train() client id: f_00004-8-3 loss: 0.892195  [  128/  306]
train() client id: f_00004-8-4 loss: 0.888371  [  160/  306]
train() client id: f_00004-8-5 loss: 0.847040  [  192/  306]
train() client id: f_00004-8-6 loss: 0.735939  [  224/  306]
train() client id: f_00004-8-7 loss: 0.894249  [  256/  306]
train() client id: f_00004-8-8 loss: 0.871079  [  288/  306]
train() client id: f_00004-9-0 loss: 0.941383  [   32/  306]
train() client id: f_00004-9-1 loss: 0.886850  [   64/  306]
train() client id: f_00004-9-2 loss: 0.831518  [   96/  306]
train() client id: f_00004-9-3 loss: 0.843864  [  128/  306]
train() client id: f_00004-9-4 loss: 0.914992  [  160/  306]
train() client id: f_00004-9-5 loss: 0.790569  [  192/  306]
train() client id: f_00004-9-6 loss: 0.829294  [  224/  306]
train() client id: f_00004-9-7 loss: 0.856637  [  256/  306]
train() client id: f_00004-9-8 loss: 0.791247  [  288/  306]
train() client id: f_00004-10-0 loss: 0.770792  [   32/  306]
train() client id: f_00004-10-1 loss: 0.837450  [   64/  306]
train() client id: f_00004-10-2 loss: 0.927496  [   96/  306]
train() client id: f_00004-10-3 loss: 0.935761  [  128/  306]
train() client id: f_00004-10-4 loss: 0.907469  [  160/  306]
train() client id: f_00004-10-5 loss: 0.765241  [  192/  306]
train() client id: f_00004-10-6 loss: 0.788387  [  224/  306]
train() client id: f_00004-10-7 loss: 0.796447  [  256/  306]
train() client id: f_00004-10-8 loss: 0.868591  [  288/  306]
train() client id: f_00004-11-0 loss: 0.782175  [   32/  306]
train() client id: f_00004-11-1 loss: 0.837389  [   64/  306]
train() client id: f_00004-11-2 loss: 0.840000  [   96/  306]
train() client id: f_00004-11-3 loss: 0.883332  [  128/  306]
train() client id: f_00004-11-4 loss: 0.896631  [  160/  306]
train() client id: f_00004-11-5 loss: 0.887691  [  192/  306]
train() client id: f_00004-11-6 loss: 0.789410  [  224/  306]
train() client id: f_00004-11-7 loss: 0.864130  [  256/  306]
train() client id: f_00004-11-8 loss: 0.867776  [  288/  306]
train() client id: f_00004-12-0 loss: 0.830538  [   32/  306]
train() client id: f_00004-12-1 loss: 0.816582  [   64/  306]
train() client id: f_00004-12-2 loss: 0.845312  [   96/  306]
train() client id: f_00004-12-3 loss: 0.736792  [  128/  306]
train() client id: f_00004-12-4 loss: 0.861309  [  160/  306]
train() client id: f_00004-12-5 loss: 0.956706  [  192/  306]
train() client id: f_00004-12-6 loss: 0.703394  [  224/  306]
train() client id: f_00004-12-7 loss: 0.937512  [  256/  306]
train() client id: f_00004-12-8 loss: 0.981634  [  288/  306]
train() client id: f_00005-0-0 loss: 0.988403  [   32/  146]
train() client id: f_00005-0-1 loss: 1.016070  [   64/  146]
train() client id: f_00005-0-2 loss: 0.687575  [   96/  146]
train() client id: f_00005-0-3 loss: 0.781399  [  128/  146]
train() client id: f_00005-1-0 loss: 0.928937  [   32/  146]
train() client id: f_00005-1-1 loss: 0.850941  [   64/  146]
train() client id: f_00005-1-2 loss: 0.830031  [   96/  146]
train() client id: f_00005-1-3 loss: 0.880728  [  128/  146]
train() client id: f_00005-2-0 loss: 0.836056  [   32/  146]
train() client id: f_00005-2-1 loss: 0.840922  [   64/  146]
train() client id: f_00005-2-2 loss: 0.711622  [   96/  146]
train() client id: f_00005-2-3 loss: 1.147813  [  128/  146]
train() client id: f_00005-3-0 loss: 0.829482  [   32/  146]
train() client id: f_00005-3-1 loss: 1.055539  [   64/  146]
train() client id: f_00005-3-2 loss: 0.766226  [   96/  146]
train() client id: f_00005-3-3 loss: 0.892970  [  128/  146]
train() client id: f_00005-4-0 loss: 0.852802  [   32/  146]
train() client id: f_00005-4-1 loss: 0.710215  [   64/  146]
train() client id: f_00005-4-2 loss: 0.820801  [   96/  146]
train() client id: f_00005-4-3 loss: 1.039415  [  128/  146]
train() client id: f_00005-5-0 loss: 0.832848  [   32/  146]
train() client id: f_00005-5-1 loss: 0.978328  [   64/  146]
train() client id: f_00005-5-2 loss: 0.718739  [   96/  146]
train() client id: f_00005-5-3 loss: 0.714391  [  128/  146]
train() client id: f_00005-6-0 loss: 0.867076  [   32/  146]
train() client id: f_00005-6-1 loss: 0.907643  [   64/  146]
train() client id: f_00005-6-2 loss: 0.782531  [   96/  146]
train() client id: f_00005-6-3 loss: 0.973948  [  128/  146]
train() client id: f_00005-7-0 loss: 0.933039  [   32/  146]
train() client id: f_00005-7-1 loss: 1.017662  [   64/  146]
train() client id: f_00005-7-2 loss: 0.744709  [   96/  146]
train() client id: f_00005-7-3 loss: 0.826855  [  128/  146]
train() client id: f_00005-8-0 loss: 0.941404  [   32/  146]
train() client id: f_00005-8-1 loss: 0.879272  [   64/  146]
train() client id: f_00005-8-2 loss: 0.586675  [   96/  146]
train() client id: f_00005-8-3 loss: 1.040999  [  128/  146]
train() client id: f_00005-9-0 loss: 0.950330  [   32/  146]
train() client id: f_00005-9-1 loss: 0.869590  [   64/  146]
train() client id: f_00005-9-2 loss: 0.733788  [   96/  146]
train() client id: f_00005-9-3 loss: 0.943965  [  128/  146]
train() client id: f_00005-10-0 loss: 0.888714  [   32/  146]
train() client id: f_00005-10-1 loss: 0.790970  [   64/  146]
train() client id: f_00005-10-2 loss: 1.043587  [   96/  146]
train() client id: f_00005-10-3 loss: 0.695996  [  128/  146]
train() client id: f_00005-11-0 loss: 0.886360  [   32/  146]
train() client id: f_00005-11-1 loss: 1.014709  [   64/  146]
train() client id: f_00005-11-2 loss: 0.700012  [   96/  146]
train() client id: f_00005-11-3 loss: 0.967342  [  128/  146]
train() client id: f_00005-12-0 loss: 1.045946  [   32/  146]
train() client id: f_00005-12-1 loss: 0.790785  [   64/  146]
train() client id: f_00005-12-2 loss: 0.849279  [   96/  146]
train() client id: f_00005-12-3 loss: 0.861432  [  128/  146]
train() client id: f_00006-0-0 loss: 0.611424  [   32/   54]
train() client id: f_00006-1-0 loss: 0.519708  [   32/   54]
train() client id: f_00006-2-0 loss: 0.573103  [   32/   54]
train() client id: f_00006-3-0 loss: 0.603254  [   32/   54]
train() client id: f_00006-4-0 loss: 0.564039  [   32/   54]
train() client id: f_00006-5-0 loss: 0.582702  [   32/   54]
train() client id: f_00006-6-0 loss: 0.621209  [   32/   54]
train() client id: f_00006-7-0 loss: 0.559030  [   32/   54]
train() client id: f_00006-8-0 loss: 0.622669  [   32/   54]
train() client id: f_00006-9-0 loss: 0.521742  [   32/   54]
train() client id: f_00006-10-0 loss: 0.585131  [   32/   54]
train() client id: f_00006-11-0 loss: 0.606636  [   32/   54]
train() client id: f_00006-12-0 loss: 0.512269  [   32/   54]
train() client id: f_00007-0-0 loss: 0.624377  [   32/  179]
train() client id: f_00007-0-1 loss: 0.794132  [   64/  179]
train() client id: f_00007-0-2 loss: 0.594315  [   96/  179]
train() client id: f_00007-0-3 loss: 0.639674  [  128/  179]
train() client id: f_00007-0-4 loss: 0.745065  [  160/  179]
train() client id: f_00007-1-0 loss: 0.594567  [   32/  179]
train() client id: f_00007-1-1 loss: 0.654665  [   64/  179]
train() client id: f_00007-1-2 loss: 0.844875  [   96/  179]
train() client id: f_00007-1-3 loss: 0.755028  [  128/  179]
train() client id: f_00007-1-4 loss: 0.534390  [  160/  179]
train() client id: f_00007-2-0 loss: 0.585151  [   32/  179]
train() client id: f_00007-2-1 loss: 0.651975  [   64/  179]
train() client id: f_00007-2-2 loss: 0.808245  [   96/  179]
train() client id: f_00007-2-3 loss: 0.679693  [  128/  179]
train() client id: f_00007-2-4 loss: 0.532615  [  160/  179]
train() client id: f_00007-3-0 loss: 0.584185  [   32/  179]
train() client id: f_00007-3-1 loss: 0.483675  [   64/  179]
train() client id: f_00007-3-2 loss: 0.717645  [   96/  179]
train() client id: f_00007-3-3 loss: 0.623776  [  128/  179]
train() client id: f_00007-3-4 loss: 0.709801  [  160/  179]
train() client id: f_00007-4-0 loss: 0.585754  [   32/  179]
train() client id: f_00007-4-1 loss: 0.548710  [   64/  179]
train() client id: f_00007-4-2 loss: 0.642084  [   96/  179]
train() client id: f_00007-4-3 loss: 0.820686  [  128/  179]
train() client id: f_00007-4-4 loss: 0.586907  [  160/  179]
train() client id: f_00007-5-0 loss: 0.634077  [   32/  179]
train() client id: f_00007-5-1 loss: 0.732940  [   64/  179]
train() client id: f_00007-5-2 loss: 0.569638  [   96/  179]
train() client id: f_00007-5-3 loss: 0.618217  [  128/  179]
train() client id: f_00007-5-4 loss: 0.582630  [  160/  179]
train() client id: f_00007-6-0 loss: 0.661846  [   32/  179]
train() client id: f_00007-6-1 loss: 0.702062  [   64/  179]
train() client id: f_00007-6-2 loss: 0.600007  [   96/  179]
train() client id: f_00007-6-3 loss: 0.469826  [  128/  179]
train() client id: f_00007-6-4 loss: 0.581755  [  160/  179]
train() client id: f_00007-7-0 loss: 0.672834  [   32/  179]
train() client id: f_00007-7-1 loss: 0.546868  [   64/  179]
train() client id: f_00007-7-2 loss: 0.580141  [   96/  179]
train() client id: f_00007-7-3 loss: 0.729512  [  128/  179]
train() client id: f_00007-7-4 loss: 0.642281  [  160/  179]
train() client id: f_00007-8-0 loss: 0.684100  [   32/  179]
train() client id: f_00007-8-1 loss: 0.556764  [   64/  179]
train() client id: f_00007-8-2 loss: 0.614212  [   96/  179]
train() client id: f_00007-8-3 loss: 0.560562  [  128/  179]
train() client id: f_00007-8-4 loss: 0.597696  [  160/  179]
train() client id: f_00007-9-0 loss: 0.583777  [   32/  179]
train() client id: f_00007-9-1 loss: 0.564435  [   64/  179]
train() client id: f_00007-9-2 loss: 0.546172  [   96/  179]
train() client id: f_00007-9-3 loss: 0.673548  [  128/  179]
train() client id: f_00007-9-4 loss: 0.503969  [  160/  179]
train() client id: f_00007-10-0 loss: 0.660433  [   32/  179]
train() client id: f_00007-10-1 loss: 0.557327  [   64/  179]
train() client id: f_00007-10-2 loss: 0.602805  [   96/  179]
train() client id: f_00007-10-3 loss: 0.584854  [  128/  179]
train() client id: f_00007-10-4 loss: 0.665402  [  160/  179]
train() client id: f_00007-11-0 loss: 0.485245  [   32/  179]
train() client id: f_00007-11-1 loss: 0.568819  [   64/  179]
train() client id: f_00007-11-2 loss: 0.637512  [   96/  179]
train() client id: f_00007-11-3 loss: 0.573615  [  128/  179]
train() client id: f_00007-11-4 loss: 0.676749  [  160/  179]
train() client id: f_00007-12-0 loss: 0.646835  [   32/  179]
train() client id: f_00007-12-1 loss: 0.695868  [   64/  179]
train() client id: f_00007-12-2 loss: 0.657577  [   96/  179]
train() client id: f_00007-12-3 loss: 0.601091  [  128/  179]
train() client id: f_00007-12-4 loss: 0.558480  [  160/  179]
train() client id: f_00008-0-0 loss: 0.789432  [   32/  130]
train() client id: f_00008-0-1 loss: 0.846300  [   64/  130]
train() client id: f_00008-0-2 loss: 0.830475  [   96/  130]
train() client id: f_00008-0-3 loss: 0.822026  [  128/  130]
train() client id: f_00008-1-0 loss: 0.769002  [   32/  130]
train() client id: f_00008-1-1 loss: 0.758887  [   64/  130]
train() client id: f_00008-1-2 loss: 0.886970  [   96/  130]
train() client id: f_00008-1-3 loss: 0.856070  [  128/  130]
train() client id: f_00008-2-0 loss: 0.825777  [   32/  130]
train() client id: f_00008-2-1 loss: 0.752856  [   64/  130]
train() client id: f_00008-2-2 loss: 0.867958  [   96/  130]
train() client id: f_00008-2-3 loss: 0.803714  [  128/  130]
train() client id: f_00008-3-0 loss: 0.706947  [   32/  130]
train() client id: f_00008-3-1 loss: 0.863336  [   64/  130]
train() client id: f_00008-3-2 loss: 0.793323  [   96/  130]
train() client id: f_00008-3-3 loss: 0.929213  [  128/  130]
train() client id: f_00008-4-0 loss: 0.782319  [   32/  130]
train() client id: f_00008-4-1 loss: 0.963599  [   64/  130]
train() client id: f_00008-4-2 loss: 0.727471  [   96/  130]
train() client id: f_00008-4-3 loss: 0.823036  [  128/  130]
train() client id: f_00008-5-0 loss: 0.693521  [   32/  130]
train() client id: f_00008-5-1 loss: 0.887219  [   64/  130]
train() client id: f_00008-5-2 loss: 0.819614  [   96/  130]
train() client id: f_00008-5-3 loss: 0.890864  [  128/  130]
train() client id: f_00008-6-0 loss: 0.780737  [   32/  130]
train() client id: f_00008-6-1 loss: 0.809887  [   64/  130]
train() client id: f_00008-6-2 loss: 0.785113  [   96/  130]
train() client id: f_00008-6-3 loss: 0.911772  [  128/  130]
train() client id: f_00008-7-0 loss: 0.885169  [   32/  130]
train() client id: f_00008-7-1 loss: 0.829122  [   64/  130]
train() client id: f_00008-7-2 loss: 0.791659  [   96/  130]
train() client id: f_00008-7-3 loss: 0.759997  [  128/  130]
train() client id: f_00008-8-0 loss: 0.930639  [   32/  130]
train() client id: f_00008-8-1 loss: 0.769365  [   64/  130]
train() client id: f_00008-8-2 loss: 0.734283  [   96/  130]
train() client id: f_00008-8-3 loss: 0.860671  [  128/  130]
train() client id: f_00008-9-0 loss: 0.849895  [   32/  130]
train() client id: f_00008-9-1 loss: 0.865767  [   64/  130]
train() client id: f_00008-9-2 loss: 0.778604  [   96/  130]
train() client id: f_00008-9-3 loss: 0.780531  [  128/  130]
train() client id: f_00008-10-0 loss: 0.731727  [   32/  130]
train() client id: f_00008-10-1 loss: 0.892528  [   64/  130]
train() client id: f_00008-10-2 loss: 0.875579  [   96/  130]
train() client id: f_00008-10-3 loss: 0.775360  [  128/  130]
train() client id: f_00008-11-0 loss: 0.816441  [   32/  130]
train() client id: f_00008-11-1 loss: 0.754478  [   64/  130]
train() client id: f_00008-11-2 loss: 0.878112  [   96/  130]
train() client id: f_00008-11-3 loss: 0.844288  [  128/  130]
train() client id: f_00008-12-0 loss: 0.698220  [   32/  130]
train() client id: f_00008-12-1 loss: 0.821908  [   64/  130]
train() client id: f_00008-12-2 loss: 0.841577  [   96/  130]
train() client id: f_00008-12-3 loss: 0.926713  [  128/  130]
train() client id: f_00009-0-0 loss: 1.022305  [   32/  118]
train() client id: f_00009-0-1 loss: 0.980110  [   64/  118]
train() client id: f_00009-0-2 loss: 1.017576  [   96/  118]
train() client id: f_00009-1-0 loss: 1.040805  [   32/  118]
train() client id: f_00009-1-1 loss: 0.848371  [   64/  118]
train() client id: f_00009-1-2 loss: 1.045293  [   96/  118]
train() client id: f_00009-2-0 loss: 1.049562  [   32/  118]
train() client id: f_00009-2-1 loss: 0.872345  [   64/  118]
train() client id: f_00009-2-2 loss: 0.978498  [   96/  118]
train() client id: f_00009-3-0 loss: 0.928835  [   32/  118]
train() client id: f_00009-3-1 loss: 0.862869  [   64/  118]
train() client id: f_00009-3-2 loss: 0.921085  [   96/  118]
train() client id: f_00009-4-0 loss: 0.792068  [   32/  118]
train() client id: f_00009-4-1 loss: 0.863229  [   64/  118]
train() client id: f_00009-4-2 loss: 0.937572  [   96/  118]
train() client id: f_00009-5-0 loss: 0.818216  [   32/  118]
train() client id: f_00009-5-1 loss: 0.874537  [   64/  118]
train() client id: f_00009-5-2 loss: 0.860979  [   96/  118]
train() client id: f_00009-6-0 loss: 0.852845  [   32/  118]
train() client id: f_00009-6-1 loss: 0.815188  [   64/  118]
train() client id: f_00009-6-2 loss: 0.867846  [   96/  118]
train() client id: f_00009-7-0 loss: 0.818047  [   32/  118]
train() client id: f_00009-7-1 loss: 0.802527  [   64/  118]
train() client id: f_00009-7-2 loss: 0.810683  [   96/  118]
train() client id: f_00009-8-0 loss: 0.748948  [   32/  118]
train() client id: f_00009-8-1 loss: 0.819672  [   64/  118]
train() client id: f_00009-8-2 loss: 0.909775  [   96/  118]
train() client id: f_00009-9-0 loss: 0.700379  [   32/  118]
train() client id: f_00009-9-1 loss: 0.852939  [   64/  118]
train() client id: f_00009-9-2 loss: 0.768358  [   96/  118]
train() client id: f_00009-10-0 loss: 0.694172  [   32/  118]
train() client id: f_00009-10-1 loss: 0.838264  [   64/  118]
train() client id: f_00009-10-2 loss: 0.807736  [   96/  118]
train() client id: f_00009-11-0 loss: 0.750006  [   32/  118]
train() client id: f_00009-11-1 loss: 0.774191  [   64/  118]
train() client id: f_00009-11-2 loss: 0.761639  [   96/  118]
train() client id: f_00009-12-0 loss: 0.836865  [   32/  118]
train() client id: f_00009-12-1 loss: 0.793479  [   64/  118]
train() client id: f_00009-12-2 loss: 0.768672  [   96/  118]
At round 13 accuracy: 0.6312997347480106
At round 13 training accuracy: 0.5734406438631791
At round 13 training loss: 0.8698137237927124
update_location
xs = 8.927491 186.223621 5.882650 10.934260 -102.581990 49.769243 -5.849135 -5.143845 -125.120581 20.134486 
ys = -177.390647 7.291448 75.684448 -77.290817 -9.642386 0.794442 -86.381692 71.628436 25.881276 -612.232496 
xs mean: 4.317620029478787
ys mean: -78.16579882624053
dists_uav = 203.831160 211.500360 125.549756 126.859877 143.582869 111.703217 132.272481 123.114142 162.249808 620.672238 
uav_gains = -107.924478 -108.419395 -102.470800 -102.583574 -103.930108 -101.201720 -103.037570 -102.258005 -105.267228 -126.749688 
uav_gains_db_mean: -106.38425656001658
dists_bs = 397.502818 398.257031 206.354336 313.404221 198.336026 284.374381 311.339693 198.838612 157.239786 811.056391 
bs_gains = -112.348585 -112.371636 -104.376240 -109.457995 -103.894304 -108.275992 -109.377625 -103.925079 -101.070808 -121.020489 
bs_gains_db_mean: -108.61187523288137
Round 14
-------------------------------
ene_coms = [0.00935799 0.00964581 0.00769637 0.0070582  0.00751398 0.00663307
 0.0072098  0.00752538 0.0065954  0.0319267 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.19805849 19.10804858  9.01631394  3.23031416 21.97877291 10.55860676
  4.0176012  12.93080311  9.4177812   9.16123584]
obj_prev = 108.61753617399962
eta_min = 6.250848166386827e-11	eta_max = 0.7404136142125904
af = 22.83639226708156	bf = 2.3658879883666137	zeta = 25.120031493789718	eta = 0.9090909090909091
af = 22.83639226708156	bf = 2.3658879883666137	zeta = 49.96671505566677	eta = 0.45703209109584375
af = 22.83639226708156	bf = 2.3658879883666137	zeta = 37.28916741320865	eta = 0.6124135734656387
af = 22.83639226708156	bf = 2.3658879883666137	zeta = 34.995174751961244	eta = 0.6525583149374539
af = 22.83639226708156	bf = 2.3658879883666137	zeta = 34.86545089095519	eta = 0.6549862882457599
af = 22.83639226708156	bf = 2.3658879883666137	zeta = 34.86499500158761	eta = 0.6549948527467646
af = 22.83639226708156	bf = 2.3658879883666137	zeta = 34.86499499592908	eta = 0.6549948528530691
eta = 0.6549948528530691
ene_coms = [0.00935799 0.00964581 0.00769637 0.0070582  0.00751398 0.00663307
 0.0072098  0.00752538 0.0065954  0.0319267 ]
ene_comp = [0.0349155  0.07343339 0.03436129 0.01191561 0.08479479 0.04045764
 0.01496379 0.04960218 0.03602393 0.03269864]
ene_total = [3.00117926 5.63171359 2.85097436 1.28618272 6.2573604  3.1921513
 1.50308709 3.87252161 2.88904815 4.38077652]
ti_comp = [0.33846136 0.33558308 0.35507749 0.36145923 0.35690141 0.36571053
 0.35994322 0.35678746 0.36608726 0.11277419]
ti_coms = [0.09357986 0.09645814 0.07696373 0.07058199 0.07513981 0.06633068
 0.072098   0.07525375 0.06595396 0.31926703]
t_total = [29.28689957 29.28689957 29.28689957 29.28689957 29.28689957 29.28689957
 29.28689957 29.28689957 29.28689957 29.28689957]
ene_coms = [0.00935799 0.00964581 0.00769637 0.0070582  0.00751398 0.00663307
 0.0072098  0.00752538 0.0065954  0.0319267 ]
ene_comp = [2.32229099e-05 2.19766098e-04 2.01113844e-05 8.09300701e-07
 2.99151339e-04 3.09461634e-05 1.61635398e-06 5.99188006e-05
 2.18013815e-05 1.71810538e-04]
ene_total = [0.63592664 0.66876083 0.52307949 0.47851096 0.52963101 0.45173542
 0.4888423  0.51418648 0.4485618  2.17587097]
optimize_network iter = 0 obj = 6.9151058929375395
eta = 0.6549948528530691
freqs = [5.15797437e+07 1.09411644e+08 4.83856160e+07 1.64826435e+07
 1.18793013e+08 5.53137513e+07 2.07863144e+07 6.95122225e+07
 4.92012872e+07 1.44973969e+08]
eta_min = 0.6549948528530748	eta_max = 0.6549948528530659
af = 0.04693412485208932	bf = 2.3658879883666137	zeta = 0.05162753733729826	eta = 0.9090909090909091
af = 0.04693412485208932	bf = 2.3658879883666137	zeta = 26.073974170697465	eta = 0.0018000372534247187
af = 0.04693412485208932	bf = 2.3658879883666137	zeta = 2.667310471220893	eta = 0.01759604866343378
af = 0.04693412485208932	bf = 2.3658879883666137	zeta = 2.601277862052335	eta = 0.018042718748645954
af = 0.04693412485208932	bf = 2.3658879883666137	zeta = 2.6012629691746234	eta = 0.018042822047699948
eta = 0.018042822047699948
ene_coms = [0.00935799 0.00964581 0.00769637 0.0070582  0.00751398 0.00663307
 0.0072098  0.00752538 0.0065954  0.0319267 ]
ene_comp = [2.20359266e-04 2.08533281e-03 1.90834392e-04 7.67935238e-06
 2.83860936e-03 2.93644245e-04 1.53373793e-05 5.68561949e-04
 2.06870562e-04 1.63028854e-03]
ene_total = [0.22812439 0.27939697 0.1878471  0.16828576 0.24656435 0.1649713
 0.17207879 0.19277073 0.16200741 0.79921617]
ti_comp = [0.33846136 0.33558308 0.35507749 0.36145923 0.35690141 0.36571053
 0.35994322 0.35678746 0.36608726 0.11277419]
ti_coms = [0.09357986 0.09645814 0.07696373 0.07058199 0.07513981 0.06633068
 0.072098   0.07525375 0.06595396 0.31926703]
t_total = [29.28689957 29.28689957 29.28689957 29.28689957 29.28689957 29.28689957
 29.28689957 29.28689957 29.28689957 29.28689957]
ene_coms = [0.00935799 0.00964581 0.00769637 0.0070582  0.00751398 0.00663307
 0.0072098  0.00752538 0.0065954  0.0319267 ]
ene_comp = [2.32229099e-05 2.19766098e-04 2.01113844e-05 8.09300701e-07
 2.99151339e-04 3.09461634e-05 1.61635398e-06 5.99188006e-05
 2.18013815e-05 1.71810538e-04]
ene_total = [0.63592664 0.66876083 0.52307949 0.47851096 0.52963101 0.45173542
 0.4888423  0.51418648 0.4485618  2.17587097]
optimize_network iter = 1 obj = 6.915105892937653
eta = 0.6549948528530748
freqs = [5.15797437e+07 1.09411644e+08 4.83856160e+07 1.64826435e+07
 1.18793013e+08 5.53137513e+07 2.07863144e+07 6.95122225e+07
 4.92012872e+07 1.44973969e+08]
Done!
ene_coms = [0.00935799 0.00964581 0.00769637 0.0070582  0.00751398 0.00663307
 0.0072098  0.00752538 0.0065954  0.0319267 ]
ene_comp = [2.17892490e-05 2.06198889e-04 1.88698128e-05 7.59338711e-07
 2.80683301e-04 2.90357092e-05 1.51656874e-06 5.62197274e-05
 2.04554783e-05 1.61203854e-04]
ene_total = [0.00937978 0.00985201 0.00771524 0.00705896 0.00779466 0.0066621
 0.00721132 0.0075816  0.00661585 0.03208791]
At round 14 energy consumption: 0.10195942632426683
At round 14 eta: 0.6549948528530748
At round 14 a_n: 23.386960998683932
At round 14 local rounds: 13.85535724649609
At round 14 global rounds: 67.78728141329518
gradient difference: 0.4508322775363922
train() client id: f_00000-0-0 loss: 1.493165  [   32/  126]
train() client id: f_00000-0-1 loss: 1.513792  [   64/  126]
train() client id: f_00000-0-2 loss: 1.292081  [   96/  126]
train() client id: f_00000-1-0 loss: 1.358305  [   32/  126]
train() client id: f_00000-1-1 loss: 1.445576  [   64/  126]
train() client id: f_00000-1-2 loss: 1.264386  [   96/  126]
train() client id: f_00000-2-0 loss: 1.321403  [   32/  126]
train() client id: f_00000-2-1 loss: 1.133718  [   64/  126]
train() client id: f_00000-2-2 loss: 1.205609  [   96/  126]
train() client id: f_00000-3-0 loss: 1.219844  [   32/  126]
train() client id: f_00000-3-1 loss: 1.042438  [   64/  126]
train() client id: f_00000-3-2 loss: 1.104328  [   96/  126]
train() client id: f_00000-4-0 loss: 1.124229  [   32/  126]
train() client id: f_00000-4-1 loss: 1.049067  [   64/  126]
train() client id: f_00000-4-2 loss: 1.027616  [   96/  126]
train() client id: f_00000-5-0 loss: 1.015671  [   32/  126]
train() client id: f_00000-5-1 loss: 1.058065  [   64/  126]
train() client id: f_00000-5-2 loss: 0.941611  [   96/  126]
train() client id: f_00000-6-0 loss: 0.978094  [   32/  126]
train() client id: f_00000-6-1 loss: 0.975422  [   64/  126]
train() client id: f_00000-6-2 loss: 0.948685  [   96/  126]
train() client id: f_00000-7-0 loss: 0.970122  [   32/  126]
train() client id: f_00000-7-1 loss: 0.909245  [   64/  126]
train() client id: f_00000-7-2 loss: 0.831910  [   96/  126]
train() client id: f_00000-8-0 loss: 0.967417  [   32/  126]
train() client id: f_00000-8-1 loss: 0.948144  [   64/  126]
train() client id: f_00000-8-2 loss: 0.852738  [   96/  126]
train() client id: f_00000-9-0 loss: 0.831852  [   32/  126]
train() client id: f_00000-9-1 loss: 0.860361  [   64/  126]
train() client id: f_00000-9-2 loss: 0.901132  [   96/  126]
train() client id: f_00000-10-0 loss: 0.902076  [   32/  126]
train() client id: f_00000-10-1 loss: 0.845304  [   64/  126]
train() client id: f_00000-10-2 loss: 0.864973  [   96/  126]
train() client id: f_00000-11-0 loss: 0.822387  [   32/  126]
train() client id: f_00000-11-1 loss: 0.818910  [   64/  126]
train() client id: f_00000-11-2 loss: 0.882144  [   96/  126]
train() client id: f_00000-12-0 loss: 0.846706  [   32/  126]
train() client id: f_00000-12-1 loss: 0.852120  [   64/  126]
train() client id: f_00000-12-2 loss: 0.813941  [   96/  126]
train() client id: f_00001-0-0 loss: 0.585096  [   32/  265]
train() client id: f_00001-0-1 loss: 0.533204  [   64/  265]
train() client id: f_00001-0-2 loss: 0.561632  [   96/  265]
train() client id: f_00001-0-3 loss: 0.510695  [  128/  265]
train() client id: f_00001-0-4 loss: 0.517098  [  160/  265]
train() client id: f_00001-0-5 loss: 0.619368  [  192/  265]
train() client id: f_00001-0-6 loss: 0.595614  [  224/  265]
train() client id: f_00001-0-7 loss: 0.520550  [  256/  265]
train() client id: f_00001-1-0 loss: 0.537556  [   32/  265]
train() client id: f_00001-1-1 loss: 0.527832  [   64/  265]
train() client id: f_00001-1-2 loss: 0.458372  [   96/  265]
train() client id: f_00001-1-3 loss: 0.463940  [  128/  265]
train() client id: f_00001-1-4 loss: 0.628922  [  160/  265]
train() client id: f_00001-1-5 loss: 0.663523  [  192/  265]
train() client id: f_00001-1-6 loss: 0.583504  [  224/  265]
train() client id: f_00001-1-7 loss: 0.507191  [  256/  265]
train() client id: f_00001-2-0 loss: 0.517990  [   32/  265]
train() client id: f_00001-2-1 loss: 0.571798  [   64/  265]
train() client id: f_00001-2-2 loss: 0.487215  [   96/  265]
train() client id: f_00001-2-3 loss: 0.532596  [  128/  265]
train() client id: f_00001-2-4 loss: 0.582530  [  160/  265]
train() client id: f_00001-2-5 loss: 0.612493  [  192/  265]
train() client id: f_00001-2-6 loss: 0.520846  [  224/  265]
train() client id: f_00001-2-7 loss: 0.487013  [  256/  265]
train() client id: f_00001-3-0 loss: 0.640904  [   32/  265]
train() client id: f_00001-3-1 loss: 0.585915  [   64/  265]
train() client id: f_00001-3-2 loss: 0.475366  [   96/  265]
train() client id: f_00001-3-3 loss: 0.523725  [  128/  265]
train() client id: f_00001-3-4 loss: 0.506783  [  160/  265]
train() client id: f_00001-3-5 loss: 0.534431  [  192/  265]
train() client id: f_00001-3-6 loss: 0.547613  [  224/  265]
train() client id: f_00001-3-7 loss: 0.445071  [  256/  265]
train() client id: f_00001-4-0 loss: 0.623211  [   32/  265]
train() client id: f_00001-4-1 loss: 0.476889  [   64/  265]
train() client id: f_00001-4-2 loss: 0.552751  [   96/  265]
train() client id: f_00001-4-3 loss: 0.617599  [  128/  265]
train() client id: f_00001-4-4 loss: 0.460284  [  160/  265]
train() client id: f_00001-4-5 loss: 0.453708  [  192/  265]
train() client id: f_00001-4-6 loss: 0.449013  [  224/  265]
train() client id: f_00001-4-7 loss: 0.611311  [  256/  265]
train() client id: f_00001-5-0 loss: 0.519750  [   32/  265]
train() client id: f_00001-5-1 loss: 0.498377  [   64/  265]
train() client id: f_00001-5-2 loss: 0.612137  [   96/  265]
train() client id: f_00001-5-3 loss: 0.536358  [  128/  265]
train() client id: f_00001-5-4 loss: 0.482081  [  160/  265]
train() client id: f_00001-5-5 loss: 0.475734  [  192/  265]
train() client id: f_00001-5-6 loss: 0.510864  [  224/  265]
train() client id: f_00001-5-7 loss: 0.452914  [  256/  265]
train() client id: f_00001-6-0 loss: 0.509385  [   32/  265]
train() client id: f_00001-6-1 loss: 0.588714  [   64/  265]
train() client id: f_00001-6-2 loss: 0.573227  [   96/  265]
train() client id: f_00001-6-3 loss: 0.488087  [  128/  265]
train() client id: f_00001-6-4 loss: 0.543233  [  160/  265]
train() client id: f_00001-6-5 loss: 0.438553  [  192/  265]
train() client id: f_00001-6-6 loss: 0.546715  [  224/  265]
train() client id: f_00001-6-7 loss: 0.474402  [  256/  265]
train() client id: f_00001-7-0 loss: 0.507020  [   32/  265]
train() client id: f_00001-7-1 loss: 0.510785  [   64/  265]
train() client id: f_00001-7-2 loss: 0.589062  [   96/  265]
train() client id: f_00001-7-3 loss: 0.476095  [  128/  265]
train() client id: f_00001-7-4 loss: 0.604085  [  160/  265]
train() client id: f_00001-7-5 loss: 0.435494  [  192/  265]
train() client id: f_00001-7-6 loss: 0.510945  [  224/  265]
train() client id: f_00001-7-7 loss: 0.504159  [  256/  265]
train() client id: f_00001-8-0 loss: 0.431226  [   32/  265]
train() client id: f_00001-8-1 loss: 0.535432  [   64/  265]
train() client id: f_00001-8-2 loss: 0.441596  [   96/  265]
train() client id: f_00001-8-3 loss: 0.555312  [  128/  265]
train() client id: f_00001-8-4 loss: 0.449440  [  160/  265]
train() client id: f_00001-8-5 loss: 0.730108  [  192/  265]
train() client id: f_00001-8-6 loss: 0.494047  [  224/  265]
train() client id: f_00001-8-7 loss: 0.568630  [  256/  265]
train() client id: f_00001-9-0 loss: 0.480778  [   32/  265]
train() client id: f_00001-9-1 loss: 0.544199  [   64/  265]
train() client id: f_00001-9-2 loss: 0.665954  [   96/  265]
train() client id: f_00001-9-3 loss: 0.485272  [  128/  265]
train() client id: f_00001-9-4 loss: 0.511224  [  160/  265]
train() client id: f_00001-9-5 loss: 0.491520  [  192/  265]
train() client id: f_00001-9-6 loss: 0.522656  [  224/  265]
train() client id: f_00001-9-7 loss: 0.404171  [  256/  265]
train() client id: f_00001-10-0 loss: 0.552476  [   32/  265]
train() client id: f_00001-10-1 loss: 0.616506  [   64/  265]
train() client id: f_00001-10-2 loss: 0.471259  [   96/  265]
train() client id: f_00001-10-3 loss: 0.475318  [  128/  265]
train() client id: f_00001-10-4 loss: 0.476609  [  160/  265]
train() client id: f_00001-10-5 loss: 0.675639  [  192/  265]
train() client id: f_00001-10-6 loss: 0.436535  [  224/  265]
train() client id: f_00001-10-7 loss: 0.487965  [  256/  265]
train() client id: f_00001-11-0 loss: 0.612402  [   32/  265]
train() client id: f_00001-11-1 loss: 0.426772  [   64/  265]
train() client id: f_00001-11-2 loss: 0.565175  [   96/  265]
train() client id: f_00001-11-3 loss: 0.471944  [  128/  265]
train() client id: f_00001-11-4 loss: 0.570217  [  160/  265]
train() client id: f_00001-11-5 loss: 0.495239  [  192/  265]
train() client id: f_00001-11-6 loss: 0.553614  [  224/  265]
train() client id: f_00001-11-7 loss: 0.498781  [  256/  265]
train() client id: f_00001-12-0 loss: 0.454851  [   32/  265]
train() client id: f_00001-12-1 loss: 0.635717  [   64/  265]
train() client id: f_00001-12-2 loss: 0.556549  [   96/  265]
train() client id: f_00001-12-3 loss: 0.528688  [  128/  265]
train() client id: f_00001-12-4 loss: 0.500350  [  160/  265]
train() client id: f_00001-12-5 loss: 0.427551  [  192/  265]
train() client id: f_00001-12-6 loss: 0.592366  [  224/  265]
train() client id: f_00001-12-7 loss: 0.501343  [  256/  265]
train() client id: f_00002-0-0 loss: 1.161684  [   32/  124]
train() client id: f_00002-0-1 loss: 1.162902  [   64/  124]
train() client id: f_00002-0-2 loss: 1.153750  [   96/  124]
train() client id: f_00002-1-0 loss: 1.284402  [   32/  124]
train() client id: f_00002-1-1 loss: 1.174933  [   64/  124]
train() client id: f_00002-1-2 loss: 1.053535  [   96/  124]
train() client id: f_00002-2-0 loss: 1.156279  [   32/  124]
train() client id: f_00002-2-1 loss: 1.179925  [   64/  124]
train() client id: f_00002-2-2 loss: 1.152801  [   96/  124]
train() client id: f_00002-3-0 loss: 1.130282  [   32/  124]
train() client id: f_00002-3-1 loss: 0.970172  [   64/  124]
train() client id: f_00002-3-2 loss: 1.213991  [   96/  124]
train() client id: f_00002-4-0 loss: 1.135828  [   32/  124]
train() client id: f_00002-4-1 loss: 1.078467  [   64/  124]
train() client id: f_00002-4-2 loss: 1.074932  [   96/  124]
train() client id: f_00002-5-0 loss: 0.957635  [   32/  124]
train() client id: f_00002-5-1 loss: 1.013101  [   64/  124]
train() client id: f_00002-5-2 loss: 1.173183  [   96/  124]
train() client id: f_00002-6-0 loss: 1.029817  [   32/  124]
train() client id: f_00002-6-1 loss: 0.953508  [   64/  124]
train() client id: f_00002-6-2 loss: 1.047218  [   96/  124]
train() client id: f_00002-7-0 loss: 1.070286  [   32/  124]
train() client id: f_00002-7-1 loss: 1.066135  [   64/  124]
train() client id: f_00002-7-2 loss: 0.895594  [   96/  124]
train() client id: f_00002-8-0 loss: 1.097860  [   32/  124]
train() client id: f_00002-8-1 loss: 0.981843  [   64/  124]
train() client id: f_00002-8-2 loss: 0.961754  [   96/  124]
train() client id: f_00002-9-0 loss: 0.953079  [   32/  124]
train() client id: f_00002-9-1 loss: 0.989911  [   64/  124]
train() client id: f_00002-9-2 loss: 1.125189  [   96/  124]
train() client id: f_00002-10-0 loss: 0.958941  [   32/  124]
train() client id: f_00002-10-1 loss: 0.958153  [   64/  124]
train() client id: f_00002-10-2 loss: 1.058903  [   96/  124]
train() client id: f_00002-11-0 loss: 1.042054  [   32/  124]
train() client id: f_00002-11-1 loss: 0.870660  [   64/  124]
train() client id: f_00002-11-2 loss: 0.971430  [   96/  124]
train() client id: f_00002-12-0 loss: 0.964602  [   32/  124]
train() client id: f_00002-12-1 loss: 1.043451  [   64/  124]
train() client id: f_00002-12-2 loss: 0.985669  [   96/  124]
train() client id: f_00003-0-0 loss: 0.684824  [   32/   43]
train() client id: f_00003-1-0 loss: 0.638275  [   32/   43]
train() client id: f_00003-2-0 loss: 0.658832  [   32/   43]
train() client id: f_00003-3-0 loss: 0.668711  [   32/   43]
train() client id: f_00003-4-0 loss: 0.592639  [   32/   43]
train() client id: f_00003-5-0 loss: 0.716267  [   32/   43]
train() client id: f_00003-6-0 loss: 0.611049  [   32/   43]
train() client id: f_00003-7-0 loss: 0.721116  [   32/   43]
train() client id: f_00003-8-0 loss: 0.684052  [   32/   43]
train() client id: f_00003-9-0 loss: 0.571025  [   32/   43]
train() client id: f_00003-10-0 loss: 0.665270  [   32/   43]
train() client id: f_00003-11-0 loss: 0.606656  [   32/   43]
train() client id: f_00003-12-0 loss: 0.579589  [   32/   43]
train() client id: f_00004-0-0 loss: 0.835496  [   32/  306]
train() client id: f_00004-0-1 loss: 0.706854  [   64/  306]
train() client id: f_00004-0-2 loss: 0.641934  [   96/  306]
train() client id: f_00004-0-3 loss: 0.759641  [  128/  306]
train() client id: f_00004-0-4 loss: 0.854802  [  160/  306]
train() client id: f_00004-0-5 loss: 0.647171  [  192/  306]
train() client id: f_00004-0-6 loss: 0.726193  [  224/  306]
train() client id: f_00004-0-7 loss: 0.808659  [  256/  306]
train() client id: f_00004-0-8 loss: 0.689119  [  288/  306]
train() client id: f_00004-1-0 loss: 0.740173  [   32/  306]
train() client id: f_00004-1-1 loss: 0.774080  [   64/  306]
train() client id: f_00004-1-2 loss: 0.709983  [   96/  306]
train() client id: f_00004-1-3 loss: 0.694493  [  128/  306]
train() client id: f_00004-1-4 loss: 0.682340  [  160/  306]
train() client id: f_00004-1-5 loss: 0.605897  [  192/  306]
train() client id: f_00004-1-6 loss: 0.785341  [  224/  306]
train() client id: f_00004-1-7 loss: 0.758774  [  256/  306]
train() client id: f_00004-1-8 loss: 0.745368  [  288/  306]
train() client id: f_00004-2-0 loss: 0.686318  [   32/  306]
train() client id: f_00004-2-1 loss: 0.683572  [   64/  306]
train() client id: f_00004-2-2 loss: 0.712514  [   96/  306]
train() client id: f_00004-2-3 loss: 0.718784  [  128/  306]
train() client id: f_00004-2-4 loss: 0.812920  [  160/  306]
train() client id: f_00004-2-5 loss: 0.934488  [  192/  306]
train() client id: f_00004-2-6 loss: 0.807409  [  224/  306]
train() client id: f_00004-2-7 loss: 0.660835  [  256/  306]
train() client id: f_00004-2-8 loss: 0.674715  [  288/  306]
train() client id: f_00004-3-0 loss: 0.760093  [   32/  306]
train() client id: f_00004-3-1 loss: 0.810972  [   64/  306]
train() client id: f_00004-3-2 loss: 0.724246  [   96/  306]
train() client id: f_00004-3-3 loss: 0.783928  [  128/  306]
train() client id: f_00004-3-4 loss: 0.775141  [  160/  306]
train() client id: f_00004-3-5 loss: 0.651813  [  192/  306]
train() client id: f_00004-3-6 loss: 0.697086  [  224/  306]
train() client id: f_00004-3-7 loss: 0.698106  [  256/  306]
train() client id: f_00004-3-8 loss: 0.672164  [  288/  306]
train() client id: f_00004-4-0 loss: 0.685225  [   32/  306]
train() client id: f_00004-4-1 loss: 0.762544  [   64/  306]
train() client id: f_00004-4-2 loss: 0.628446  [   96/  306]
train() client id: f_00004-4-3 loss: 0.718688  [  128/  306]
train() client id: f_00004-4-4 loss: 0.712313  [  160/  306]
train() client id: f_00004-4-5 loss: 0.858478  [  192/  306]
train() client id: f_00004-4-6 loss: 0.705043  [  224/  306]
train() client id: f_00004-4-7 loss: 0.793940  [  256/  306]
train() client id: f_00004-4-8 loss: 0.731334  [  288/  306]
train() client id: f_00004-5-0 loss: 0.846195  [   32/  306]
train() client id: f_00004-5-1 loss: 0.760243  [   64/  306]
train() client id: f_00004-5-2 loss: 0.852577  [   96/  306]
train() client id: f_00004-5-3 loss: 0.564117  [  128/  306]
train() client id: f_00004-5-4 loss: 0.871868  [  160/  306]
train() client id: f_00004-5-5 loss: 0.780624  [  192/  306]
train() client id: f_00004-5-6 loss: 0.632207  [  224/  306]
train() client id: f_00004-5-7 loss: 0.790335  [  256/  306]
train() client id: f_00004-5-8 loss: 0.628202  [  288/  306]
train() client id: f_00004-6-0 loss: 0.700464  [   32/  306]
train() client id: f_00004-6-1 loss: 0.799834  [   64/  306]
train() client id: f_00004-6-2 loss: 0.837410  [   96/  306]
train() client id: f_00004-6-3 loss: 0.751534  [  128/  306]
train() client id: f_00004-6-4 loss: 0.683366  [  160/  306]
train() client id: f_00004-6-5 loss: 0.775273  [  192/  306]
train() client id: f_00004-6-6 loss: 0.795447  [  224/  306]
train() client id: f_00004-6-7 loss: 0.615644  [  256/  306]
train() client id: f_00004-6-8 loss: 0.743705  [  288/  306]
train() client id: f_00004-7-0 loss: 0.721224  [   32/  306]
train() client id: f_00004-7-1 loss: 0.721384  [   64/  306]
train() client id: f_00004-7-2 loss: 0.702611  [   96/  306]
train() client id: f_00004-7-3 loss: 0.711472  [  128/  306]
train() client id: f_00004-7-4 loss: 0.706797  [  160/  306]
train() client id: f_00004-7-5 loss: 0.836149  [  192/  306]
train() client id: f_00004-7-6 loss: 0.709559  [  224/  306]
train() client id: f_00004-7-7 loss: 0.837160  [  256/  306]
train() client id: f_00004-7-8 loss: 0.726427  [  288/  306]
train() client id: f_00004-8-0 loss: 0.743914  [   32/  306]
train() client id: f_00004-8-1 loss: 0.645268  [   64/  306]
train() client id: f_00004-8-2 loss: 0.811604  [   96/  306]
train() client id: f_00004-8-3 loss: 0.858078  [  128/  306]
train() client id: f_00004-8-4 loss: 0.687119  [  160/  306]
train() client id: f_00004-8-5 loss: 0.864747  [  192/  306]
train() client id: f_00004-8-6 loss: 0.728649  [  224/  306]
train() client id: f_00004-8-7 loss: 0.765416  [  256/  306]
train() client id: f_00004-8-8 loss: 0.697357  [  288/  306]
train() client id: f_00004-9-0 loss: 0.755658  [   32/  306]
train() client id: f_00004-9-1 loss: 0.722545  [   64/  306]
train() client id: f_00004-9-2 loss: 0.753490  [   96/  306]
train() client id: f_00004-9-3 loss: 0.739892  [  128/  306]
train() client id: f_00004-9-4 loss: 0.785659  [  160/  306]
train() client id: f_00004-9-5 loss: 0.653412  [  192/  306]
train() client id: f_00004-9-6 loss: 0.811774  [  224/  306]
train() client id: f_00004-9-7 loss: 0.706794  [  256/  306]
train() client id: f_00004-9-8 loss: 0.763086  [  288/  306]
train() client id: f_00004-10-0 loss: 0.655975  [   32/  306]
train() client id: f_00004-10-1 loss: 0.803177  [   64/  306]
train() client id: f_00004-10-2 loss: 0.834904  [   96/  306]
train() client id: f_00004-10-3 loss: 0.824968  [  128/  306]
train() client id: f_00004-10-4 loss: 0.696427  [  160/  306]
train() client id: f_00004-10-5 loss: 0.767997  [  192/  306]
train() client id: f_00004-10-6 loss: 0.670412  [  224/  306]
train() client id: f_00004-10-7 loss: 0.740894  [  256/  306]
train() client id: f_00004-10-8 loss: 0.818801  [  288/  306]
train() client id: f_00004-11-0 loss: 0.733356  [   32/  306]
train() client id: f_00004-11-1 loss: 0.798139  [   64/  306]
train() client id: f_00004-11-2 loss: 0.792575  [   96/  306]
train() client id: f_00004-11-3 loss: 0.887033  [  128/  306]
train() client id: f_00004-11-4 loss: 0.803405  [  160/  306]
train() client id: f_00004-11-5 loss: 0.735907  [  192/  306]
train() client id: f_00004-11-6 loss: 0.688537  [  224/  306]
train() client id: f_00004-11-7 loss: 0.637668  [  256/  306]
train() client id: f_00004-11-8 loss: 0.754587  [  288/  306]
train() client id: f_00004-12-0 loss: 0.820028  [   32/  306]
train() client id: f_00004-12-1 loss: 0.683653  [   64/  306]
train() client id: f_00004-12-2 loss: 0.789950  [   96/  306]
train() client id: f_00004-12-3 loss: 0.864440  [  128/  306]
train() client id: f_00004-12-4 loss: 0.757706  [  160/  306]
train() client id: f_00004-12-5 loss: 0.842747  [  192/  306]
train() client id: f_00004-12-6 loss: 0.671749  [  224/  306]
train() client id: f_00004-12-7 loss: 0.666383  [  256/  306]
train() client id: f_00004-12-8 loss: 0.754967  [  288/  306]
train() client id: f_00005-0-0 loss: 0.823888  [   32/  146]
train() client id: f_00005-0-1 loss: 0.852169  [   64/  146]
train() client id: f_00005-0-2 loss: 0.730096  [   96/  146]
train() client id: f_00005-0-3 loss: 0.822791  [  128/  146]
train() client id: f_00005-1-0 loss: 0.874387  [   32/  146]
train() client id: f_00005-1-1 loss: 0.536330  [   64/  146]
train() client id: f_00005-1-2 loss: 0.774606  [   96/  146]
train() client id: f_00005-1-3 loss: 0.791471  [  128/  146]
train() client id: f_00005-2-0 loss: 0.520291  [   32/  146]
train() client id: f_00005-2-1 loss: 0.653690  [   64/  146]
train() client id: f_00005-2-2 loss: 1.148507  [   96/  146]
train() client id: f_00005-2-3 loss: 0.701230  [  128/  146]
train() client id: f_00005-3-0 loss: 0.827965  [   32/  146]
train() client id: f_00005-3-1 loss: 0.715448  [   64/  146]
train() client id: f_00005-3-2 loss: 0.605294  [   96/  146]
train() client id: f_00005-3-3 loss: 0.770230  [  128/  146]
train() client id: f_00005-4-0 loss: 0.640743  [   32/  146]
train() client id: f_00005-4-1 loss: 0.747322  [   64/  146]
train() client id: f_00005-4-2 loss: 0.845520  [   96/  146]
train() client id: f_00005-4-3 loss: 0.618436  [  128/  146]
train() client id: f_00005-5-0 loss: 0.695493  [   32/  146]
train() client id: f_00005-5-1 loss: 0.805764  [   64/  146]
train() client id: f_00005-5-2 loss: 0.824646  [   96/  146]
train() client id: f_00005-5-3 loss: 0.782535  [  128/  146]
train() client id: f_00005-6-0 loss: 0.635555  [   32/  146]
train() client id: f_00005-6-1 loss: 0.702387  [   64/  146]
train() client id: f_00005-6-2 loss: 0.996964  [   96/  146]
train() client id: f_00005-6-3 loss: 0.726504  [  128/  146]
train() client id: f_00005-7-0 loss: 0.747547  [   32/  146]
train() client id: f_00005-7-1 loss: 0.726097  [   64/  146]
train() client id: f_00005-7-2 loss: 0.828637  [   96/  146]
train() client id: f_00005-7-3 loss: 0.796651  [  128/  146]
train() client id: f_00005-8-0 loss: 0.900205  [   32/  146]
train() client id: f_00005-8-1 loss: 0.708830  [   64/  146]
train() client id: f_00005-8-2 loss: 0.602302  [   96/  146]
train() client id: f_00005-8-3 loss: 0.858327  [  128/  146]
train() client id: f_00005-9-0 loss: 0.914694  [   32/  146]
train() client id: f_00005-9-1 loss: 0.690802  [   64/  146]
train() client id: f_00005-9-2 loss: 0.707874  [   96/  146]
train() client id: f_00005-9-3 loss: 0.656592  [  128/  146]
train() client id: f_00005-10-0 loss: 0.752965  [   32/  146]
train() client id: f_00005-10-1 loss: 0.705435  [   64/  146]
train() client id: f_00005-10-2 loss: 0.685361  [   96/  146]
train() client id: f_00005-10-3 loss: 0.807585  [  128/  146]
train() client id: f_00005-11-0 loss: 0.776800  [   32/  146]
train() client id: f_00005-11-1 loss: 0.742032  [   64/  146]
train() client id: f_00005-11-2 loss: 0.891599  [   96/  146]
train() client id: f_00005-11-3 loss: 0.713386  [  128/  146]
train() client id: f_00005-12-0 loss: 0.589655  [   32/  146]
train() client id: f_00005-12-1 loss: 1.012123  [   64/  146]
train() client id: f_00005-12-2 loss: 0.825990  [   96/  146]
train() client id: f_00005-12-3 loss: 0.660702  [  128/  146]
train() client id: f_00006-0-0 loss: 0.663073  [   32/   54]
train() client id: f_00006-1-0 loss: 0.620497  [   32/   54]
train() client id: f_00006-2-0 loss: 0.627762  [   32/   54]
train() client id: f_00006-3-0 loss: 0.623741  [   32/   54]
train() client id: f_00006-4-0 loss: 0.683789  [   32/   54]
train() client id: f_00006-5-0 loss: 0.632072  [   32/   54]
train() client id: f_00006-6-0 loss: 0.623383  [   32/   54]
train() client id: f_00006-7-0 loss: 0.616381  [   32/   54]
train() client id: f_00006-8-0 loss: 0.588055  [   32/   54]
train() client id: f_00006-9-0 loss: 0.633796  [   32/   54]
train() client id: f_00006-10-0 loss: 0.638828  [   32/   54]
train() client id: f_00006-11-0 loss: 0.646172  [   32/   54]
train() client id: f_00006-12-0 loss: 0.627864  [   32/   54]
train() client id: f_00007-0-0 loss: 0.699745  [   32/  179]
train() client id: f_00007-0-1 loss: 0.757740  [   64/  179]
train() client id: f_00007-0-2 loss: 0.533297  [   96/  179]
train() client id: f_00007-0-3 loss: 0.680040  [  128/  179]
train() client id: f_00007-0-4 loss: 0.585130  [  160/  179]
train() client id: f_00007-1-0 loss: 0.543145  [   32/  179]
train() client id: f_00007-1-1 loss: 0.762084  [   64/  179]
train() client id: f_00007-1-2 loss: 0.550264  [   96/  179]
train() client id: f_00007-1-3 loss: 0.593383  [  128/  179]
train() client id: f_00007-1-4 loss: 0.893513  [  160/  179]
train() client id: f_00007-2-0 loss: 0.522698  [   32/  179]
train() client id: f_00007-2-1 loss: 0.622899  [   64/  179]
train() client id: f_00007-2-2 loss: 0.614714  [   96/  179]
train() client id: f_00007-2-3 loss: 0.733494  [  128/  179]
train() client id: f_00007-2-4 loss: 0.576347  [  160/  179]
train() client id: f_00007-3-0 loss: 0.497275  [   32/  179]
train() client id: f_00007-3-1 loss: 0.653843  [   64/  179]
train() client id: f_00007-3-2 loss: 0.627204  [   96/  179]
train() client id: f_00007-3-3 loss: 0.641125  [  128/  179]
train() client id: f_00007-3-4 loss: 0.734669  [  160/  179]
train() client id: f_00007-4-0 loss: 0.634532  [   32/  179]
train() client id: f_00007-4-1 loss: 0.736796  [   64/  179]
train() client id: f_00007-4-2 loss: 0.678090  [   96/  179]
train() client id: f_00007-4-3 loss: 0.501720  [  128/  179]
train() client id: f_00007-4-4 loss: 0.541587  [  160/  179]
train() client id: f_00007-5-0 loss: 0.573138  [   32/  179]
train() client id: f_00007-5-1 loss: 0.601770  [   64/  179]
train() client id: f_00007-5-2 loss: 0.685306  [   96/  179]
train() client id: f_00007-5-3 loss: 0.466801  [  128/  179]
train() client id: f_00007-5-4 loss: 0.724863  [  160/  179]
train() client id: f_00007-6-0 loss: 0.762711  [   32/  179]
train() client id: f_00007-6-1 loss: 0.512315  [   64/  179]
train() client id: f_00007-6-2 loss: 0.647391  [   96/  179]
train() client id: f_00007-6-3 loss: 0.479105  [  128/  179]
train() client id: f_00007-6-4 loss: 0.633363  [  160/  179]
train() client id: f_00007-7-0 loss: 0.607657  [   32/  179]
train() client id: f_00007-7-1 loss: 0.643034  [   64/  179]
train() client id: f_00007-7-2 loss: 0.717303  [   96/  179]
train() client id: f_00007-7-3 loss: 0.497401  [  128/  179]
train() client id: f_00007-7-4 loss: 0.592817  [  160/  179]
train() client id: f_00007-8-0 loss: 0.549839  [   32/  179]
train() client id: f_00007-8-1 loss: 0.486994  [   64/  179]
train() client id: f_00007-8-2 loss: 0.747374  [   96/  179]
train() client id: f_00007-8-3 loss: 0.692565  [  128/  179]
train() client id: f_00007-8-4 loss: 0.499200  [  160/  179]
train() client id: f_00007-9-0 loss: 0.588543  [   32/  179]
train() client id: f_00007-9-1 loss: 0.540669  [   64/  179]
train() client id: f_00007-9-2 loss: 0.485643  [   96/  179]
train() client id: f_00007-9-3 loss: 0.620754  [  128/  179]
train() client id: f_00007-9-4 loss: 0.546580  [  160/  179]
train() client id: f_00007-10-0 loss: 0.496875  [   32/  179]
train() client id: f_00007-10-1 loss: 0.632998  [   64/  179]
train() client id: f_00007-10-2 loss: 0.638756  [   96/  179]
train() client id: f_00007-10-3 loss: 0.470815  [  128/  179]
train() client id: f_00007-10-4 loss: 0.543374  [  160/  179]
train() client id: f_00007-11-0 loss: 0.721851  [   32/  179]
train() client id: f_00007-11-1 loss: 0.605765  [   64/  179]
train() client id: f_00007-11-2 loss: 0.559458  [   96/  179]
train() client id: f_00007-11-3 loss: 0.469695  [  128/  179]
train() client id: f_00007-11-4 loss: 0.563780  [  160/  179]
train() client id: f_00007-12-0 loss: 0.595831  [   32/  179]
train() client id: f_00007-12-1 loss: 0.614209  [   64/  179]
train() client id: f_00007-12-2 loss: 0.538845  [   96/  179]
train() client id: f_00007-12-3 loss: 0.569379  [  128/  179]
train() client id: f_00007-12-4 loss: 0.720238  [  160/  179]
train() client id: f_00008-0-0 loss: 0.747908  [   32/  130]
train() client id: f_00008-0-1 loss: 0.771829  [   64/  130]
train() client id: f_00008-0-2 loss: 0.612266  [   96/  130]
train() client id: f_00008-0-3 loss: 0.698048  [  128/  130]
train() client id: f_00008-1-0 loss: 0.677128  [   32/  130]
train() client id: f_00008-1-1 loss: 0.865319  [   64/  130]
train() client id: f_00008-1-2 loss: 0.624648  [   96/  130]
train() client id: f_00008-1-3 loss: 0.661693  [  128/  130]
train() client id: f_00008-2-0 loss: 0.735233  [   32/  130]
train() client id: f_00008-2-1 loss: 0.702559  [   64/  130]
train() client id: f_00008-2-2 loss: 0.616436  [   96/  130]
train() client id: f_00008-2-3 loss: 0.764314  [  128/  130]
train() client id: f_00008-3-0 loss: 0.712445  [   32/  130]
train() client id: f_00008-3-1 loss: 0.754885  [   64/  130]
train() client id: f_00008-3-2 loss: 0.702223  [   96/  130]
train() client id: f_00008-3-3 loss: 0.647820  [  128/  130]
train() client id: f_00008-4-0 loss: 0.622431  [   32/  130]
train() client id: f_00008-4-1 loss: 0.760675  [   64/  130]
train() client id: f_00008-4-2 loss: 0.656650  [   96/  130]
train() client id: f_00008-4-3 loss: 0.739029  [  128/  130]
train() client id: f_00008-5-0 loss: 0.709833  [   32/  130]
train() client id: f_00008-5-1 loss: 0.649477  [   64/  130]
train() client id: f_00008-5-2 loss: 0.680070  [   96/  130]
train() client id: f_00008-5-3 loss: 0.721765  [  128/  130]
train() client id: f_00008-6-0 loss: 0.795368  [   32/  130]
train() client id: f_00008-6-1 loss: 0.660119  [   64/  130]
train() client id: f_00008-6-2 loss: 0.619557  [   96/  130]
train() client id: f_00008-6-3 loss: 0.736871  [  128/  130]
train() client id: f_00008-7-0 loss: 0.637404  [   32/  130]
train() client id: f_00008-7-1 loss: 0.647518  [   64/  130]
train() client id: f_00008-7-2 loss: 0.701343  [   96/  130]
train() client id: f_00008-7-3 loss: 0.825215  [  128/  130]
train() client id: f_00008-8-0 loss: 0.708123  [   32/  130]
train() client id: f_00008-8-1 loss: 0.692019  [   64/  130]
train() client id: f_00008-8-2 loss: 0.649925  [   96/  130]
train() client id: f_00008-8-3 loss: 0.736678  [  128/  130]
train() client id: f_00008-9-0 loss: 0.711729  [   32/  130]
train() client id: f_00008-9-1 loss: 0.706831  [   64/  130]
train() client id: f_00008-9-2 loss: 0.640144  [   96/  130]
train() client id: f_00008-9-3 loss: 0.734165  [  128/  130]
train() client id: f_00008-10-0 loss: 0.704488  [   32/  130]
train() client id: f_00008-10-1 loss: 0.656771  [   64/  130]
train() client id: f_00008-10-2 loss: 0.688144  [   96/  130]
train() client id: f_00008-10-3 loss: 0.747800  [  128/  130]
train() client id: f_00008-11-0 loss: 0.684212  [   32/  130]
train() client id: f_00008-11-1 loss: 0.624621  [   64/  130]
train() client id: f_00008-11-2 loss: 0.673837  [   96/  130]
train() client id: f_00008-11-3 loss: 0.806514  [  128/  130]
train() client id: f_00008-12-0 loss: 0.735971  [   32/  130]
train() client id: f_00008-12-1 loss: 0.731307  [   64/  130]
train() client id: f_00008-12-2 loss: 0.639103  [   96/  130]
train() client id: f_00008-12-3 loss: 0.693875  [  128/  130]
train() client id: f_00009-0-0 loss: 1.182812  [   32/  118]
train() client id: f_00009-0-1 loss: 1.200288  [   64/  118]
train() client id: f_00009-0-2 loss: 1.108131  [   96/  118]
train() client id: f_00009-1-0 loss: 1.266928  [   32/  118]
train() client id: f_00009-1-1 loss: 1.090814  [   64/  118]
train() client id: f_00009-1-2 loss: 0.958137  [   96/  118]
train() client id: f_00009-2-0 loss: 1.059333  [   32/  118]
train() client id: f_00009-2-1 loss: 0.919536  [   64/  118]
train() client id: f_00009-2-2 loss: 1.121564  [   96/  118]
train() client id: f_00009-3-0 loss: 1.038516  [   32/  118]
train() client id: f_00009-3-1 loss: 0.962747  [   64/  118]
train() client id: f_00009-3-2 loss: 1.034827  [   96/  118]
train() client id: f_00009-4-0 loss: 1.038895  [   32/  118]
train() client id: f_00009-4-1 loss: 1.053528  [   64/  118]
train() client id: f_00009-4-2 loss: 0.851915  [   96/  118]
train() client id: f_00009-5-0 loss: 0.974075  [   32/  118]
train() client id: f_00009-5-1 loss: 1.028879  [   64/  118]
train() client id: f_00009-5-2 loss: 0.873070  [   96/  118]
train() client id: f_00009-6-0 loss: 0.932548  [   32/  118]
train() client id: f_00009-6-1 loss: 0.949350  [   64/  118]
train() client id: f_00009-6-2 loss: 0.937827  [   96/  118]
train() client id: f_00009-7-0 loss: 0.926839  [   32/  118]
train() client id: f_00009-7-1 loss: 0.909173  [   64/  118]
train() client id: f_00009-7-2 loss: 0.924591  [   96/  118]
train() client id: f_00009-8-0 loss: 0.750966  [   32/  118]
train() client id: f_00009-8-1 loss: 0.893974  [   64/  118]
train() client id: f_00009-8-2 loss: 1.114047  [   96/  118]
train() client id: f_00009-9-0 loss: 0.830143  [   32/  118]
train() client id: f_00009-9-1 loss: 1.069389  [   64/  118]
train() client id: f_00009-9-2 loss: 0.891744  [   96/  118]
train() client id: f_00009-10-0 loss: 0.997102  [   32/  118]
train() client id: f_00009-10-1 loss: 0.881328  [   64/  118]
train() client id: f_00009-10-2 loss: 0.758137  [   96/  118]
train() client id: f_00009-11-0 loss: 0.858911  [   32/  118]
train() client id: f_00009-11-1 loss: 0.886121  [   64/  118]
train() client id: f_00009-11-2 loss: 0.848149  [   96/  118]
train() client id: f_00009-12-0 loss: 0.815890  [   32/  118]
train() client id: f_00009-12-1 loss: 0.962728  [   64/  118]
train() client id: f_00009-12-2 loss: 0.879028  [   96/  118]
At round 14 accuracy: 0.6312997347480106
At round 14 training accuracy: 0.5707578806170356
At round 14 training loss: 0.8706513070894278
update_location
xs = 8.927491 191.223621 5.882650 10.934260 -107.581990 44.769243 -5.849135 -5.143845 -130.120581 20.134486 
ys = -182.390647 7.291448 80.684448 -72.290817 -9.642386 0.794442 -81.381692 76.628436 25.881276 -617.232496 
xs mean: 3.3176200294787863
ys mean: -77.16579882624053
dists_uav = 208.197138 215.915813 128.625759 123.877037 147.196671 109.566949 129.062745 126.088764 166.136107 625.604789 
uav_gains = -108.204343 -108.712435 -102.733764 -102.325105 -104.200994 -100.992048 -102.770615 -102.517338 -105.528649 -126.837756 
uav_gains_db_mean: -106.48230471738083
dists_bs = 401.942032 402.797590 203.994991 309.393435 196.565508 280.439113 307.154012 196.285705 155.725900 815.910409 
bs_gains = -112.483635 -112.509491 -104.236405 -109.301370 -103.785264 -108.106540 -109.213033 -103.767942 -100.953163 -121.093048 
bs_gains_db_mean: -108.54498906895205
Round 15
-------------------------------
ene_coms = [0.0095187  0.0098243  0.00764257 0.00697465 0.00747388 0.00657293
 0.00711989 0.00746755 0.0065619  0.03225385]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.06707676 18.83232998  8.8830007   3.18105547 21.65591917 10.40255643
  3.95666311 12.74006122  9.27906045  9.03466737]
obj_prev = 107.03239065601804
eta_min = 4.469529355199117e-11	eta_max = 0.741207816466506
af = 22.501910530386866	bf = 2.3369392471487775	zeta = 24.752101583425553	eta = 0.9090909090909091
af = 22.501910530386866	bf = 2.3369392471487775	zeta = 49.297604243148584	eta = 0.45645038690727446
af = 22.501910530386866	bf = 2.3369392471487775	zeta = 36.76689420773402	eta = 0.6120155377620535
af = 22.501910530386866	bf = 2.3369392471487775	zeta = 34.49963800705082	eta = 0.6522361343556146
af = 22.501910530386866	bf = 2.3369392471487775	zeta = 34.37130964783793	eta = 0.6546713163082021
af = 22.501910530386866	bf = 2.3369392471487775	zeta = 34.37085773821628	eta = 0.6546799239568419
af = 22.501910530386866	bf = 2.3369392471487775	zeta = 34.37085773258402	eta = 0.6546799240641226
eta = 0.6546799240641226
ene_coms = [0.0095187  0.0098243  0.00764257 0.00697465 0.00747388 0.00657293
 0.00711989 0.00746755 0.0065619  0.03225385]
ene_comp = [0.03495519 0.07351686 0.03440034 0.01192915 0.08489116 0.04050363
 0.01498079 0.04965856 0.03606487 0.03273581]
ene_total = [2.96789799 5.56164636 2.8056702  1.26151692 6.16384209 3.1415834
 1.47485598 3.81222455 2.8446335  4.33698674]
ti_comp = [0.34291327 0.33985734 0.36167462 0.36835376 0.3633615  0.37237096
 0.36690138 0.36342482 0.37248131 0.11556184]
ti_coms = [0.09518704 0.09824296 0.07642568 0.06974654 0.0747388  0.06572934
 0.07119892 0.07467548 0.065619   0.32253847]
t_total = [29.23596382 29.23596382 29.23596382 29.23596382 29.23596382 29.23596382
 29.23596382 29.23596382 29.23596382 29.23596382]
ene_coms = [0.0095187  0.0098243  0.00764257 0.00697465 0.00747388 0.00657293
 0.00711989 0.00746755 0.0065619  0.03225385]
ene_comp = [2.27010668e-05 2.15004481e-04 1.94505632e-05 7.81948860e-07
 2.89594064e-04 2.99509190e-05 1.56093850e-06 5.79472889e-05
 2.11312065e-05 1.64180043e-04]
ene_total = [0.63673128 0.66995759 0.51131326 0.46549521 0.51808374 0.44063357
 0.47523945 0.50220259 0.43930864 2.16336819]
optimize_network iter = 0 obj = 6.822333515923747
eta = 0.6546799240641226
freqs = [5.09679687e+07 1.08158407e+08 4.75570287e+07 1.61925189e+07
 1.16813646e+08 5.43861246e+07 2.04152866e+07 6.83202599e+07
 4.84116557e+07 1.41637628e+08]
eta_min = 0.6546799240641209	eta_max = 0.6546799240641226
af = 0.04473344123735321	bf = 2.3369392471487775	zeta = 0.04920678536108854	eta = 0.9090909090909091
af = 0.04473344123735321	bf = 2.3369392471487775	zeta = 25.753230794240125	eta = 0.0017370030810797586
af = 0.04473344123735321	bf = 2.3369392471487775	zeta = 2.625807875047648	eta = 0.017036067894549017
af = 0.04473344123735321	bf = 2.3369392471487775	zeta = 2.562772284238401	eta = 0.017455097947044872
af = 0.04473344123735321	bf = 2.3369392471487775	zeta = 2.562758951581582	eta = 0.01745518875653381
eta = 0.01745518875653381
ene_coms = [0.0095187  0.0098243  0.00764257 0.00697465 0.00747388 0.00657293
 0.00711989 0.00746755 0.0065619  0.03225385]
ene_comp = [2.16937416e-04 2.05463985e-03 1.85874742e-04 7.47251071e-06
 2.76743769e-03 2.86218926e-04 1.49167423e-05 5.53759663e-04
 2.01935415e-04 1.56894804e-03]
ene_total = [0.22833784 0.27860625 0.18360677 0.16375744 0.24019787 0.16087324
 0.16733842 0.18813019 0.15863767 0.79327326]
ti_comp = [0.34291327 0.33985734 0.36167462 0.36835376 0.3633615  0.37237096
 0.36690138 0.36342482 0.37248131 0.11556184]
ti_coms = [0.09518704 0.09824296 0.07642568 0.06974654 0.0747388  0.06572934
 0.07119892 0.07467548 0.065619   0.32253847]
t_total = [29.23596382 29.23596382 29.23596382 29.23596382 29.23596382 29.23596382
 29.23596382 29.23596382 29.23596382 29.23596382]
ene_coms = [0.0095187  0.0098243  0.00764257 0.00697465 0.00747388 0.00657293
 0.00711989 0.00746755 0.0065619  0.03225385]
ene_comp = [2.27010668e-05 2.15004481e-04 1.94505632e-05 7.81948860e-07
 2.89594064e-04 2.99509190e-05 1.56093850e-06 5.79472889e-05
 2.11312065e-05 1.64180043e-04]
ene_total = [0.63673128 0.66995759 0.51131326 0.46549521 0.51808374 0.44063357
 0.47523945 0.50220259 0.43930864 2.16336819]
optimize_network iter = 1 obj = 6.822333515923713
eta = 0.6546799240641209
freqs = [5.09679687e+07 1.08158407e+08 4.75570287e+07 1.61925189e+07
 1.16813646e+08 5.43861246e+07 2.04152866e+07 6.83202599e+07
 4.84116557e+07 1.41637628e+08]
Done!
ene_coms = [0.0095187  0.0098243  0.00764257 0.00697465 0.00747388 0.00657293
 0.00711989 0.00746755 0.0065619  0.03225385]
ene_comp = [2.12754401e-05 2.01502202e-04 1.82290681e-05 7.32842481e-07
 2.71407560e-04 2.80700017e-05 1.46291158e-06 5.43081999e-05
 1.98041670e-05 1.53869538e-04]
ene_total = [0.00953998 0.0100258  0.0076608  0.00697539 0.00774529 0.006601
 0.00712136 0.00752186 0.0065817  0.03240772]
At round 15 energy consumption: 0.10218088353016533
At round 15 eta: 0.6546799240641209
At round 15 a_n: 23.044415151714617
At round 15 local rounds: 13.87110523059259
At round 15 global rounds: 66.73349381515145
gradient difference: 0.4232236444950104
train() client id: f_00000-0-0 loss: 1.249784  [   32/  126]
train() client id: f_00000-0-1 loss: 1.029678  [   64/  126]
train() client id: f_00000-0-2 loss: 1.325278  [   96/  126]
train() client id: f_00000-1-0 loss: 1.264498  [   32/  126]
train() client id: f_00000-1-1 loss: 1.134538  [   64/  126]
train() client id: f_00000-1-2 loss: 1.089379  [   96/  126]
train() client id: f_00000-2-0 loss: 1.111180  [   32/  126]
train() client id: f_00000-2-1 loss: 0.945430  [   64/  126]
train() client id: f_00000-2-2 loss: 1.082946  [   96/  126]
train() client id: f_00000-3-0 loss: 1.000182  [   32/  126]
train() client id: f_00000-3-1 loss: 1.041938  [   64/  126]
train() client id: f_00000-3-2 loss: 0.952859  [   96/  126]
train() client id: f_00000-4-0 loss: 0.935026  [   32/  126]
train() client id: f_00000-4-1 loss: 0.878604  [   64/  126]
train() client id: f_00000-4-2 loss: 0.948696  [   96/  126]
train() client id: f_00000-5-0 loss: 0.843649  [   32/  126]
train() client id: f_00000-5-1 loss: 0.985289  [   64/  126]
train() client id: f_00000-5-2 loss: 0.862951  [   96/  126]
train() client id: f_00000-6-0 loss: 0.929578  [   32/  126]
train() client id: f_00000-6-1 loss: 0.845326  [   64/  126]
train() client id: f_00000-6-2 loss: 0.892370  [   96/  126]
train() client id: f_00000-7-0 loss: 0.864141  [   32/  126]
train() client id: f_00000-7-1 loss: 0.874049  [   64/  126]
train() client id: f_00000-7-2 loss: 0.874223  [   96/  126]
train() client id: f_00000-8-0 loss: 0.937649  [   32/  126]
train() client id: f_00000-8-1 loss: 0.849531  [   64/  126]
train() client id: f_00000-8-2 loss: 0.807933  [   96/  126]
train() client id: f_00000-9-0 loss: 0.838155  [   32/  126]
train() client id: f_00000-9-1 loss: 0.889333  [   64/  126]
train() client id: f_00000-9-2 loss: 0.935861  [   96/  126]
train() client id: f_00000-10-0 loss: 0.862034  [   32/  126]
train() client id: f_00000-10-1 loss: 0.836743  [   64/  126]
train() client id: f_00000-10-2 loss: 0.912961  [   96/  126]
train() client id: f_00000-11-0 loss: 0.971511  [   32/  126]
train() client id: f_00000-11-1 loss: 0.876927  [   64/  126]
train() client id: f_00000-11-2 loss: 0.892701  [   96/  126]
train() client id: f_00000-12-0 loss: 0.856377  [   32/  126]
train() client id: f_00000-12-1 loss: 0.864798  [   64/  126]
train() client id: f_00000-12-2 loss: 0.905429  [   96/  126]
train() client id: f_00001-0-0 loss: 0.532534  [   32/  265]
train() client id: f_00001-0-1 loss: 0.597184  [   64/  265]
train() client id: f_00001-0-2 loss: 0.467351  [   96/  265]
train() client id: f_00001-0-3 loss: 0.513337  [  128/  265]
train() client id: f_00001-0-4 loss: 0.515962  [  160/  265]
train() client id: f_00001-0-5 loss: 0.521460  [  192/  265]
train() client id: f_00001-0-6 loss: 0.483910  [  224/  265]
train() client id: f_00001-0-7 loss: 0.495790  [  256/  265]
train() client id: f_00001-1-0 loss: 0.523206  [   32/  265]
train() client id: f_00001-1-1 loss: 0.555184  [   64/  265]
train() client id: f_00001-1-2 loss: 0.416273  [   96/  265]
train() client id: f_00001-1-3 loss: 0.504637  [  128/  265]
train() client id: f_00001-1-4 loss: 0.532516  [  160/  265]
train() client id: f_00001-1-5 loss: 0.527431  [  192/  265]
train() client id: f_00001-1-6 loss: 0.454348  [  224/  265]
train() client id: f_00001-1-7 loss: 0.543255  [  256/  265]
train() client id: f_00001-2-0 loss: 0.469870  [   32/  265]
train() client id: f_00001-2-1 loss: 0.466445  [   64/  265]
train() client id: f_00001-2-2 loss: 0.469502  [   96/  265]
train() client id: f_00001-2-3 loss: 0.449486  [  128/  265]
train() client id: f_00001-2-4 loss: 0.589132  [  160/  265]
train() client id: f_00001-2-5 loss: 0.579715  [  192/  265]
train() client id: f_00001-2-6 loss: 0.560736  [  224/  265]
train() client id: f_00001-2-7 loss: 0.413847  [  256/  265]
train() client id: f_00001-3-0 loss: 0.409440  [   32/  265]
train() client id: f_00001-3-1 loss: 0.519921  [   64/  265]
train() client id: f_00001-3-2 loss: 0.652436  [   96/  265]
train() client id: f_00001-3-3 loss: 0.501401  [  128/  265]
train() client id: f_00001-3-4 loss: 0.448876  [  160/  265]
train() client id: f_00001-3-5 loss: 0.570910  [  192/  265]
train() client id: f_00001-3-6 loss: 0.401147  [  224/  265]
train() client id: f_00001-3-7 loss: 0.411799  [  256/  265]
train() client id: f_00001-4-0 loss: 0.494529  [   32/  265]
train() client id: f_00001-4-1 loss: 0.512782  [   64/  265]
train() client id: f_00001-4-2 loss: 0.545520  [   96/  265]
train() client id: f_00001-4-3 loss: 0.633888  [  128/  265]
train() client id: f_00001-4-4 loss: 0.402070  [  160/  265]
train() client id: f_00001-4-5 loss: 0.501351  [  192/  265]
train() client id: f_00001-4-6 loss: 0.474385  [  224/  265]
train() client id: f_00001-4-7 loss: 0.381108  [  256/  265]
train() client id: f_00001-5-0 loss: 0.552445  [   32/  265]
train() client id: f_00001-5-1 loss: 0.494147  [   64/  265]
train() client id: f_00001-5-2 loss: 0.502766  [   96/  265]
train() client id: f_00001-5-3 loss: 0.515078  [  128/  265]
train() client id: f_00001-5-4 loss: 0.452188  [  160/  265]
train() client id: f_00001-5-5 loss: 0.409329  [  192/  265]
train() client id: f_00001-5-6 loss: 0.417278  [  224/  265]
train() client id: f_00001-5-7 loss: 0.570054  [  256/  265]
train() client id: f_00001-6-0 loss: 0.451170  [   32/  265]
train() client id: f_00001-6-1 loss: 0.457842  [   64/  265]
train() client id: f_00001-6-2 loss: 0.454807  [   96/  265]
train() client id: f_00001-6-3 loss: 0.543991  [  128/  265]
train() client id: f_00001-6-4 loss: 0.455603  [  160/  265]
train() client id: f_00001-6-5 loss: 0.496138  [  192/  265]
train() client id: f_00001-6-6 loss: 0.543015  [  224/  265]
train() client id: f_00001-6-7 loss: 0.452132  [  256/  265]
train() client id: f_00001-7-0 loss: 0.441218  [   32/  265]
train() client id: f_00001-7-1 loss: 0.443908  [   64/  265]
train() client id: f_00001-7-2 loss: 0.417695  [   96/  265]
train() client id: f_00001-7-3 loss: 0.461545  [  128/  265]
train() client id: f_00001-7-4 loss: 0.461640  [  160/  265]
train() client id: f_00001-7-5 loss: 0.509450  [  192/  265]
train() client id: f_00001-7-6 loss: 0.570779  [  224/  265]
train() client id: f_00001-7-7 loss: 0.588488  [  256/  265]
train() client id: f_00001-8-0 loss: 0.465945  [   32/  265]
train() client id: f_00001-8-1 loss: 0.443204  [   64/  265]
train() client id: f_00001-8-2 loss: 0.626341  [   96/  265]
train() client id: f_00001-8-3 loss: 0.438353  [  128/  265]
train() client id: f_00001-8-4 loss: 0.388459  [  160/  265]
train() client id: f_00001-8-5 loss: 0.512945  [  192/  265]
train() client id: f_00001-8-6 loss: 0.444471  [  224/  265]
train() client id: f_00001-8-7 loss: 0.516197  [  256/  265]
train() client id: f_00001-9-0 loss: 0.432100  [   32/  265]
train() client id: f_00001-9-1 loss: 0.493165  [   64/  265]
train() client id: f_00001-9-2 loss: 0.470932  [   96/  265]
train() client id: f_00001-9-3 loss: 0.492220  [  128/  265]
train() client id: f_00001-9-4 loss: 0.392809  [  160/  265]
train() client id: f_00001-9-5 loss: 0.456455  [  192/  265]
train() client id: f_00001-9-6 loss: 0.563820  [  224/  265]
train() client id: f_00001-9-7 loss: 0.516762  [  256/  265]
train() client id: f_00001-10-0 loss: 0.469840  [   32/  265]
train() client id: f_00001-10-1 loss: 0.634744  [   64/  265]
train() client id: f_00001-10-2 loss: 0.492498  [   96/  265]
train() client id: f_00001-10-3 loss: 0.411211  [  128/  265]
train() client id: f_00001-10-4 loss: 0.454080  [  160/  265]
train() client id: f_00001-10-5 loss: 0.585454  [  192/  265]
train() client id: f_00001-10-6 loss: 0.444322  [  224/  265]
train() client id: f_00001-10-7 loss: 0.405332  [  256/  265]
train() client id: f_00001-11-0 loss: 0.534644  [   32/  265]
train() client id: f_00001-11-1 loss: 0.542679  [   64/  265]
train() client id: f_00001-11-2 loss: 0.411778  [   96/  265]
train() client id: f_00001-11-3 loss: 0.501504  [  128/  265]
train() client id: f_00001-11-4 loss: 0.464730  [  160/  265]
train() client id: f_00001-11-5 loss: 0.478810  [  192/  265]
train() client id: f_00001-11-6 loss: 0.394783  [  224/  265]
train() client id: f_00001-11-7 loss: 0.581538  [  256/  265]
train() client id: f_00001-12-0 loss: 0.583004  [   32/  265]
train() client id: f_00001-12-1 loss: 0.400971  [   64/  265]
train() client id: f_00001-12-2 loss: 0.461088  [   96/  265]
train() client id: f_00001-12-3 loss: 0.485699  [  128/  265]
train() client id: f_00001-12-4 loss: 0.478629  [  160/  265]
train() client id: f_00001-12-5 loss: 0.458626  [  192/  265]
train() client id: f_00001-12-6 loss: 0.511916  [  224/  265]
train() client id: f_00001-12-7 loss: 0.491014  [  256/  265]
train() client id: f_00002-0-0 loss: 1.264004  [   32/  124]
train() client id: f_00002-0-1 loss: 1.329814  [   64/  124]
train() client id: f_00002-0-2 loss: 1.182678  [   96/  124]
train() client id: f_00002-1-0 loss: 1.232516  [   32/  124]
train() client id: f_00002-1-1 loss: 1.053002  [   64/  124]
train() client id: f_00002-1-2 loss: 1.213399  [   96/  124]
train() client id: f_00002-2-0 loss: 1.177261  [   32/  124]
train() client id: f_00002-2-1 loss: 1.167940  [   64/  124]
train() client id: f_00002-2-2 loss: 1.066860  [   96/  124]
train() client id: f_00002-3-0 loss: 1.098523  [   32/  124]
train() client id: f_00002-3-1 loss: 1.182135  [   64/  124]
train() client id: f_00002-3-2 loss: 1.098019  [   96/  124]
train() client id: f_00002-4-0 loss: 1.189727  [   32/  124]
train() client id: f_00002-4-1 loss: 1.061377  [   64/  124]
train() client id: f_00002-4-2 loss: 1.031941  [   96/  124]
train() client id: f_00002-5-0 loss: 1.099397  [   32/  124]
train() client id: f_00002-5-1 loss: 0.985053  [   64/  124]
train() client id: f_00002-5-2 loss: 1.048628  [   96/  124]
train() client id: f_00002-6-0 loss: 1.037863  [   32/  124]
train() client id: f_00002-6-1 loss: 1.044891  [   64/  124]
train() client id: f_00002-6-2 loss: 1.145296  [   96/  124]
train() client id: f_00002-7-0 loss: 1.103810  [   32/  124]
train() client id: f_00002-7-1 loss: 1.111193  [   64/  124]
train() client id: f_00002-7-2 loss: 0.897248  [   96/  124]
train() client id: f_00002-8-0 loss: 1.061544  [   32/  124]
train() client id: f_00002-8-1 loss: 1.060949  [   64/  124]
train() client id: f_00002-8-2 loss: 1.013150  [   96/  124]
train() client id: f_00002-9-0 loss: 1.083443  [   32/  124]
train() client id: f_00002-9-1 loss: 1.033908  [   64/  124]
train() client id: f_00002-9-2 loss: 1.034588  [   96/  124]
train() client id: f_00002-10-0 loss: 0.933121  [   32/  124]
train() client id: f_00002-10-1 loss: 0.931114  [   64/  124]
train() client id: f_00002-10-2 loss: 1.094753  [   96/  124]
train() client id: f_00002-11-0 loss: 1.086297  [   32/  124]
train() client id: f_00002-11-1 loss: 1.044157  [   64/  124]
train() client id: f_00002-11-2 loss: 0.896096  [   96/  124]
train() client id: f_00002-12-0 loss: 1.016171  [   32/  124]
train() client id: f_00002-12-1 loss: 1.005637  [   64/  124]
train() client id: f_00002-12-2 loss: 0.999591  [   96/  124]
train() client id: f_00003-0-0 loss: 0.888877  [   32/   43]
train() client id: f_00003-1-0 loss: 0.860233  [   32/   43]
train() client id: f_00003-2-0 loss: 0.825633  [   32/   43]
train() client id: f_00003-3-0 loss: 0.718526  [   32/   43]
train() client id: f_00003-4-0 loss: 1.021089  [   32/   43]
train() client id: f_00003-5-0 loss: 0.837194  [   32/   43]
train() client id: f_00003-6-0 loss: 0.782473  [   32/   43]
train() client id: f_00003-7-0 loss: 0.841189  [   32/   43]
train() client id: f_00003-8-0 loss: 0.808223  [   32/   43]
train() client id: f_00003-9-0 loss: 0.897483  [   32/   43]
train() client id: f_00003-10-0 loss: 0.809781  [   32/   43]
train() client id: f_00003-11-0 loss: 0.929563  [   32/   43]
train() client id: f_00003-12-0 loss: 0.768463  [   32/   43]
train() client id: f_00004-0-0 loss: 0.885566  [   32/  306]
train() client id: f_00004-0-1 loss: 0.937779  [   64/  306]
train() client id: f_00004-0-2 loss: 1.013434  [   96/  306]
train() client id: f_00004-0-3 loss: 1.028342  [  128/  306]
train() client id: f_00004-0-4 loss: 0.823864  [  160/  306]
train() client id: f_00004-0-5 loss: 0.697594  [  192/  306]
train() client id: f_00004-0-6 loss: 0.819499  [  224/  306]
train() client id: f_00004-0-7 loss: 0.905988  [  256/  306]
train() client id: f_00004-0-8 loss: 0.942607  [  288/  306]
train() client id: f_00004-1-0 loss: 0.893718  [   32/  306]
train() client id: f_00004-1-1 loss: 0.892888  [   64/  306]
train() client id: f_00004-1-2 loss: 0.948793  [   96/  306]
train() client id: f_00004-1-3 loss: 0.809199  [  128/  306]
train() client id: f_00004-1-4 loss: 0.908478  [  160/  306]
train() client id: f_00004-1-5 loss: 0.863585  [  192/  306]
train() client id: f_00004-1-6 loss: 0.927286  [  224/  306]
train() client id: f_00004-1-7 loss: 0.910956  [  256/  306]
train() client id: f_00004-1-8 loss: 0.858519  [  288/  306]
train() client id: f_00004-2-0 loss: 0.812674  [   32/  306]
train() client id: f_00004-2-1 loss: 0.876546  [   64/  306]
train() client id: f_00004-2-2 loss: 0.732562  [   96/  306]
train() client id: f_00004-2-3 loss: 0.857841  [  128/  306]
train() client id: f_00004-2-4 loss: 0.989020  [  160/  306]
train() client id: f_00004-2-5 loss: 0.955846  [  192/  306]
train() client id: f_00004-2-6 loss: 0.872099  [  224/  306]
train() client id: f_00004-2-7 loss: 0.862963  [  256/  306]
train() client id: f_00004-2-8 loss: 0.917208  [  288/  306]
train() client id: f_00004-3-0 loss: 1.026035  [   32/  306]
train() client id: f_00004-3-1 loss: 0.876059  [   64/  306]
train() client id: f_00004-3-2 loss: 0.780254  [   96/  306]
train() client id: f_00004-3-3 loss: 0.954456  [  128/  306]
train() client id: f_00004-3-4 loss: 0.927375  [  160/  306]
train() client id: f_00004-3-5 loss: 0.800400  [  192/  306]
train() client id: f_00004-3-6 loss: 0.912539  [  224/  306]
train() client id: f_00004-3-7 loss: 0.822212  [  256/  306]
train() client id: f_00004-3-8 loss: 0.928109  [  288/  306]
train() client id: f_00004-4-0 loss: 0.955705  [   32/  306]
train() client id: f_00004-4-1 loss: 0.912956  [   64/  306]
train() client id: f_00004-4-2 loss: 0.886559  [   96/  306]
train() client id: f_00004-4-3 loss: 0.890506  [  128/  306]
train() client id: f_00004-4-4 loss: 0.956647  [  160/  306]
train() client id: f_00004-4-5 loss: 0.918564  [  192/  306]
train() client id: f_00004-4-6 loss: 0.980339  [  224/  306]
train() client id: f_00004-4-7 loss: 0.840212  [  256/  306]
train() client id: f_00004-4-8 loss: 0.727259  [  288/  306]
train() client id: f_00004-5-0 loss: 0.800063  [   32/  306]
train() client id: f_00004-5-1 loss: 0.852683  [   64/  306]
train() client id: f_00004-5-2 loss: 0.836679  [   96/  306]
train() client id: f_00004-5-3 loss: 0.988694  [  128/  306]
train() client id: f_00004-5-4 loss: 1.021321  [  160/  306]
train() client id: f_00004-5-5 loss: 0.990280  [  192/  306]
train() client id: f_00004-5-6 loss: 0.850239  [  224/  306]
train() client id: f_00004-5-7 loss: 0.900891  [  256/  306]
train() client id: f_00004-5-8 loss: 0.841108  [  288/  306]
train() client id: f_00004-6-0 loss: 0.823071  [   32/  306]
train() client id: f_00004-6-1 loss: 0.879634  [   64/  306]
train() client id: f_00004-6-2 loss: 1.031389  [   96/  306]
train() client id: f_00004-6-3 loss: 0.816930  [  128/  306]
train() client id: f_00004-6-4 loss: 0.849814  [  160/  306]
train() client id: f_00004-6-5 loss: 0.967975  [  192/  306]
train() client id: f_00004-6-6 loss: 0.918696  [  224/  306]
train() client id: f_00004-6-7 loss: 0.838455  [  256/  306]
train() client id: f_00004-6-8 loss: 0.929913  [  288/  306]
train() client id: f_00004-7-0 loss: 0.887190  [   32/  306]
train() client id: f_00004-7-1 loss: 0.938927  [   64/  306]
train() client id: f_00004-7-2 loss: 0.885886  [   96/  306]
train() client id: f_00004-7-3 loss: 0.841516  [  128/  306]
train() client id: f_00004-7-4 loss: 0.878994  [  160/  306]
train() client id: f_00004-7-5 loss: 0.896637  [  192/  306]
train() client id: f_00004-7-6 loss: 0.838649  [  224/  306]
train() client id: f_00004-7-7 loss: 1.008691  [  256/  306]
train() client id: f_00004-7-8 loss: 0.844094  [  288/  306]
train() client id: f_00004-8-0 loss: 0.968430  [   32/  306]
train() client id: f_00004-8-1 loss: 0.819201  [   64/  306]
train() client id: f_00004-8-2 loss: 0.882437  [   96/  306]
train() client id: f_00004-8-3 loss: 0.886936  [  128/  306]
train() client id: f_00004-8-4 loss: 0.904160  [  160/  306]
train() client id: f_00004-8-5 loss: 0.901179  [  192/  306]
train() client id: f_00004-8-6 loss: 0.930365  [  224/  306]
train() client id: f_00004-8-7 loss: 0.824819  [  256/  306]
train() client id: f_00004-8-8 loss: 0.834418  [  288/  306]
train() client id: f_00004-9-0 loss: 0.958440  [   32/  306]
train() client id: f_00004-9-1 loss: 0.852452  [   64/  306]
train() client id: f_00004-9-2 loss: 0.963124  [   96/  306]
train() client id: f_00004-9-3 loss: 0.987714  [  128/  306]
train() client id: f_00004-9-4 loss: 0.943319  [  160/  306]
train() client id: f_00004-9-5 loss: 0.784309  [  192/  306]
train() client id: f_00004-9-6 loss: 0.754160  [  224/  306]
train() client id: f_00004-9-7 loss: 0.941932  [  256/  306]
train() client id: f_00004-9-8 loss: 0.852822  [  288/  306]
train() client id: f_00004-10-0 loss: 0.996935  [   32/  306]
train() client id: f_00004-10-1 loss: 0.805932  [   64/  306]
train() client id: f_00004-10-2 loss: 0.944049  [   96/  306]
train() client id: f_00004-10-3 loss: 0.891694  [  128/  306]
train() client id: f_00004-10-4 loss: 0.880872  [  160/  306]
train() client id: f_00004-10-5 loss: 0.836522  [  192/  306]
train() client id: f_00004-10-6 loss: 0.855675  [  224/  306]
train() client id: f_00004-10-7 loss: 0.937020  [  256/  306]
train() client id: f_00004-10-8 loss: 0.833354  [  288/  306]
train() client id: f_00004-11-0 loss: 0.880976  [   32/  306]
train() client id: f_00004-11-1 loss: 0.908327  [   64/  306]
train() client id: f_00004-11-2 loss: 0.919173  [   96/  306]
train() client id: f_00004-11-3 loss: 0.989234  [  128/  306]
train() client id: f_00004-11-4 loss: 0.726843  [  160/  306]
train() client id: f_00004-11-5 loss: 0.909901  [  192/  306]
train() client id: f_00004-11-6 loss: 0.840165  [  224/  306]
train() client id: f_00004-11-7 loss: 0.963024  [  256/  306]
train() client id: f_00004-11-8 loss: 0.826366  [  288/  306]
train() client id: f_00004-12-0 loss: 0.961496  [   32/  306]
train() client id: f_00004-12-1 loss: 0.884870  [   64/  306]
train() client id: f_00004-12-2 loss: 0.830477  [   96/  306]
train() client id: f_00004-12-3 loss: 0.878867  [  128/  306]
train() client id: f_00004-12-4 loss: 0.907133  [  160/  306]
train() client id: f_00004-12-5 loss: 0.802537  [  192/  306]
train() client id: f_00004-12-6 loss: 1.080259  [  224/  306]
train() client id: f_00004-12-7 loss: 0.804870  [  256/  306]
train() client id: f_00004-12-8 loss: 0.789985  [  288/  306]
train() client id: f_00005-0-0 loss: 1.015518  [   32/  146]
train() client id: f_00005-0-1 loss: 0.585456  [   64/  146]
train() client id: f_00005-0-2 loss: 0.722711  [   96/  146]
train() client id: f_00005-0-3 loss: 0.745473  [  128/  146]
train() client id: f_00005-1-0 loss: 0.745074  [   32/  146]
train() client id: f_00005-1-1 loss: 0.673107  [   64/  146]
train() client id: f_00005-1-2 loss: 0.718339  [   96/  146]
train() client id: f_00005-1-3 loss: 0.786010  [  128/  146]
train() client id: f_00005-2-0 loss: 0.942463  [   32/  146]
train() client id: f_00005-2-1 loss: 0.724080  [   64/  146]
train() client id: f_00005-2-2 loss: 0.684396  [   96/  146]
train() client id: f_00005-2-3 loss: 0.714687  [  128/  146]
train() client id: f_00005-3-0 loss: 0.790849  [   32/  146]
train() client id: f_00005-3-1 loss: 0.752389  [   64/  146]
train() client id: f_00005-3-2 loss: 0.779565  [   96/  146]
train() client id: f_00005-3-3 loss: 0.663756  [  128/  146]
train() client id: f_00005-4-0 loss: 0.835811  [   32/  146]
train() client id: f_00005-4-1 loss: 0.649630  [   64/  146]
train() client id: f_00005-4-2 loss: 0.771982  [   96/  146]
train() client id: f_00005-4-3 loss: 0.819336  [  128/  146]
train() client id: f_00005-5-0 loss: 0.833942  [   32/  146]
train() client id: f_00005-5-1 loss: 0.725058  [   64/  146]
train() client id: f_00005-5-2 loss: 0.665492  [   96/  146]
train() client id: f_00005-5-3 loss: 0.602588  [  128/  146]
train() client id: f_00005-6-0 loss: 0.666424  [   32/  146]
train() client id: f_00005-6-1 loss: 0.836735  [   64/  146]
train() client id: f_00005-6-2 loss: 0.767511  [   96/  146]
train() client id: f_00005-6-3 loss: 0.666140  [  128/  146]
train() client id: f_00005-7-0 loss: 0.535270  [   32/  146]
train() client id: f_00005-7-1 loss: 0.768429  [   64/  146]
train() client id: f_00005-7-2 loss: 1.004526  [   96/  146]
train() client id: f_00005-7-3 loss: 0.708799  [  128/  146]
train() client id: f_00005-8-0 loss: 0.717607  [   32/  146]
train() client id: f_00005-8-1 loss: 0.708763  [   64/  146]
train() client id: f_00005-8-2 loss: 0.875853  [   96/  146]
train() client id: f_00005-8-3 loss: 0.703792  [  128/  146]
train() client id: f_00005-9-0 loss: 0.692195  [   32/  146]
train() client id: f_00005-9-1 loss: 0.759064  [   64/  146]
train() client id: f_00005-9-2 loss: 0.797961  [   96/  146]
train() client id: f_00005-9-3 loss: 0.778990  [  128/  146]
train() client id: f_00005-10-0 loss: 0.550839  [   32/  146]
train() client id: f_00005-10-1 loss: 0.728212  [   64/  146]
train() client id: f_00005-10-2 loss: 0.851273  [   96/  146]
train() client id: f_00005-10-3 loss: 0.880247  [  128/  146]
train() client id: f_00005-11-0 loss: 0.801342  [   32/  146]
train() client id: f_00005-11-1 loss: 0.864721  [   64/  146]
train() client id: f_00005-11-2 loss: 0.740038  [   96/  146]
train() client id: f_00005-11-3 loss: 0.647341  [  128/  146]
train() client id: f_00005-12-0 loss: 0.726145  [   32/  146]
train() client id: f_00005-12-1 loss: 0.755707  [   64/  146]
train() client id: f_00005-12-2 loss: 0.581225  [   96/  146]
train() client id: f_00005-12-3 loss: 0.931547  [  128/  146]
train() client id: f_00006-0-0 loss: 0.589494  [   32/   54]
train() client id: f_00006-1-0 loss: 0.629525  [   32/   54]
train() client id: f_00006-2-0 loss: 0.611728  [   32/   54]
train() client id: f_00006-3-0 loss: 0.632449  [   32/   54]
train() client id: f_00006-4-0 loss: 0.638695  [   32/   54]
train() client id: f_00006-5-0 loss: 0.539224  [   32/   54]
train() client id: f_00006-6-0 loss: 0.623246  [   32/   54]
train() client id: f_00006-7-0 loss: 0.580028  [   32/   54]
train() client id: f_00006-8-0 loss: 0.599863  [   32/   54]
train() client id: f_00006-9-0 loss: 0.549365  [   32/   54]
train() client id: f_00006-10-0 loss: 0.584364  [   32/   54]
train() client id: f_00006-11-0 loss: 0.573206  [   32/   54]
train() client id: f_00006-12-0 loss: 0.550373  [   32/   54]
train() client id: f_00007-0-0 loss: 0.442865  [   32/  179]
train() client id: f_00007-0-1 loss: 0.483455  [   64/  179]
train() client id: f_00007-0-2 loss: 0.564526  [   96/  179]
train() client id: f_00007-0-3 loss: 0.514938  [  128/  179]
train() client id: f_00007-0-4 loss: 0.686686  [  160/  179]
train() client id: f_00007-1-0 loss: 0.611336  [   32/  179]
train() client id: f_00007-1-1 loss: 0.541245  [   64/  179]
train() client id: f_00007-1-2 loss: 0.497223  [   96/  179]
train() client id: f_00007-1-3 loss: 0.393920  [  128/  179]
train() client id: f_00007-1-4 loss: 0.522302  [  160/  179]
train() client id: f_00007-2-0 loss: 0.379135  [   32/  179]
train() client id: f_00007-2-1 loss: 0.555297  [   64/  179]
train() client id: f_00007-2-2 loss: 0.484840  [   96/  179]
train() client id: f_00007-2-3 loss: 0.521849  [  128/  179]
train() client id: f_00007-2-4 loss: 0.457666  [  160/  179]
train() client id: f_00007-3-0 loss: 0.479017  [   32/  179]
train() client id: f_00007-3-1 loss: 0.543170  [   64/  179]
train() client id: f_00007-3-2 loss: 0.447266  [   96/  179]
train() client id: f_00007-3-3 loss: 0.491381  [  128/  179]
train() client id: f_00007-3-4 loss: 0.466847  [  160/  179]
train() client id: f_00007-4-0 loss: 0.369695  [   32/  179]
train() client id: f_00007-4-1 loss: 0.787727  [   64/  179]
train() client id: f_00007-4-2 loss: 0.419415  [   96/  179]
train() client id: f_00007-4-3 loss: 0.323330  [  128/  179]
train() client id: f_00007-4-4 loss: 0.500977  [  160/  179]
train() client id: f_00007-5-0 loss: 0.472826  [   32/  179]
train() client id: f_00007-5-1 loss: 0.691073  [   64/  179]
train() client id: f_00007-5-2 loss: 0.487798  [   96/  179]
train() client id: f_00007-5-3 loss: 0.402222  [  128/  179]
train() client id: f_00007-5-4 loss: 0.328146  [  160/  179]
train() client id: f_00007-6-0 loss: 0.458312  [   32/  179]
train() client id: f_00007-6-1 loss: 0.398573  [   64/  179]
train() client id: f_00007-6-2 loss: 0.406922  [   96/  179]
train() client id: f_00007-6-3 loss: 0.405590  [  128/  179]
train() client id: f_00007-6-4 loss: 0.538143  [  160/  179]
train() client id: f_00007-7-0 loss: 0.306111  [   32/  179]
train() client id: f_00007-7-1 loss: 0.345417  [   64/  179]
train() client id: f_00007-7-2 loss: 0.682659  [   96/  179]
train() client id: f_00007-7-3 loss: 0.460022  [  128/  179]
train() client id: f_00007-7-4 loss: 0.343649  [  160/  179]
train() client id: f_00007-8-0 loss: 0.530881  [   32/  179]
train() client id: f_00007-8-1 loss: 0.465430  [   64/  179]
train() client id: f_00007-8-2 loss: 0.474659  [   96/  179]
train() client id: f_00007-8-3 loss: 0.345993  [  128/  179]
train() client id: f_00007-8-4 loss: 0.304842  [  160/  179]
train() client id: f_00007-9-0 loss: 0.432252  [   32/  179]
train() client id: f_00007-9-1 loss: 0.453841  [   64/  179]
train() client id: f_00007-9-2 loss: 0.332611  [   96/  179]
train() client id: f_00007-9-3 loss: 0.625793  [  128/  179]
train() client id: f_00007-9-4 loss: 0.353830  [  160/  179]
train() client id: f_00007-10-0 loss: 0.455490  [   32/  179]
train() client id: f_00007-10-1 loss: 0.610319  [   64/  179]
train() client id: f_00007-10-2 loss: 0.392395  [   96/  179]
train() client id: f_00007-10-3 loss: 0.368356  [  128/  179]
train() client id: f_00007-10-4 loss: 0.335292  [  160/  179]
train() client id: f_00007-11-0 loss: 0.448190  [   32/  179]
train() client id: f_00007-11-1 loss: 0.399445  [   64/  179]
train() client id: f_00007-11-2 loss: 0.531276  [   96/  179]
train() client id: f_00007-11-3 loss: 0.450838  [  128/  179]
train() client id: f_00007-11-4 loss: 0.324074  [  160/  179]
train() client id: f_00007-12-0 loss: 0.450290  [   32/  179]
train() client id: f_00007-12-1 loss: 0.313101  [   64/  179]
train() client id: f_00007-12-2 loss: 0.443771  [   96/  179]
train() client id: f_00007-12-3 loss: 0.517690  [  128/  179]
train() client id: f_00007-12-4 loss: 0.375918  [  160/  179]
train() client id: f_00008-0-0 loss: 0.798245  [   32/  130]
train() client id: f_00008-0-1 loss: 0.789901  [   64/  130]
train() client id: f_00008-0-2 loss: 0.753749  [   96/  130]
train() client id: f_00008-0-3 loss: 0.739887  [  128/  130]
train() client id: f_00008-1-0 loss: 0.702622  [   32/  130]
train() client id: f_00008-1-1 loss: 0.781710  [   64/  130]
train() client id: f_00008-1-2 loss: 0.864096  [   96/  130]
train() client id: f_00008-1-3 loss: 0.767024  [  128/  130]
train() client id: f_00008-2-0 loss: 0.755235  [   32/  130]
train() client id: f_00008-2-1 loss: 0.809877  [   64/  130]
train() client id: f_00008-2-2 loss: 0.717832  [   96/  130]
train() client id: f_00008-2-3 loss: 0.829599  [  128/  130]
train() client id: f_00008-3-0 loss: 0.813929  [   32/  130]
train() client id: f_00008-3-1 loss: 0.772068  [   64/  130]
train() client id: f_00008-3-2 loss: 0.785899  [   96/  130]
train() client id: f_00008-3-3 loss: 0.675753  [  128/  130]
train() client id: f_00008-4-0 loss: 0.738583  [   32/  130]
train() client id: f_00008-4-1 loss: 0.805366  [   64/  130]
train() client id: f_00008-4-2 loss: 0.732548  [   96/  130]
train() client id: f_00008-4-3 loss: 0.846830  [  128/  130]
train() client id: f_00008-5-0 loss: 0.891695  [   32/  130]
train() client id: f_00008-5-1 loss: 0.736755  [   64/  130]
train() client id: f_00008-5-2 loss: 0.767654  [   96/  130]
train() client id: f_00008-5-3 loss: 0.725434  [  128/  130]
train() client id: f_00008-6-0 loss: 0.862548  [   32/  130]
train() client id: f_00008-6-1 loss: 0.845674  [   64/  130]
train() client id: f_00008-6-2 loss: 0.718004  [   96/  130]
train() client id: f_00008-6-3 loss: 0.672480  [  128/  130]
train() client id: f_00008-7-0 loss: 0.708432  [   32/  130]
train() client id: f_00008-7-1 loss: 0.883098  [   64/  130]
train() client id: f_00008-7-2 loss: 0.788545  [   96/  130]
train() client id: f_00008-7-3 loss: 0.732859  [  128/  130]
train() client id: f_00008-8-0 loss: 0.777297  [   32/  130]
train() client id: f_00008-8-1 loss: 0.752004  [   64/  130]
train() client id: f_00008-8-2 loss: 0.738934  [   96/  130]
train() client id: f_00008-8-3 loss: 0.810483  [  128/  130]
train() client id: f_00008-9-0 loss: 0.839227  [   32/  130]
train() client id: f_00008-9-1 loss: 0.709375  [   64/  130]
train() client id: f_00008-9-2 loss: 0.814351  [   96/  130]
train() client id: f_00008-9-3 loss: 0.753320  [  128/  130]
train() client id: f_00008-10-0 loss: 0.765780  [   32/  130]
train() client id: f_00008-10-1 loss: 0.704581  [   64/  130]
train() client id: f_00008-10-2 loss: 0.807730  [   96/  130]
train() client id: f_00008-10-3 loss: 0.832294  [  128/  130]
train() client id: f_00008-11-0 loss: 0.763097  [   32/  130]
train() client id: f_00008-11-1 loss: 0.691454  [   64/  130]
train() client id: f_00008-11-2 loss: 0.848922  [   96/  130]
train() client id: f_00008-11-3 loss: 0.813963  [  128/  130]
train() client id: f_00008-12-0 loss: 0.847235  [   32/  130]
train() client id: f_00008-12-1 loss: 0.697271  [   64/  130]
train() client id: f_00008-12-2 loss: 0.717286  [   96/  130]
train() client id: f_00008-12-3 loss: 0.843838  [  128/  130]
train() client id: f_00009-0-0 loss: 1.238105  [   32/  118]
train() client id: f_00009-0-1 loss: 1.258854  [   64/  118]
train() client id: f_00009-0-2 loss: 1.158834  [   96/  118]
train() client id: f_00009-1-0 loss: 1.142582  [   32/  118]
train() client id: f_00009-1-1 loss: 1.200146  [   64/  118]
train() client id: f_00009-1-2 loss: 1.166857  [   96/  118]
train() client id: f_00009-2-0 loss: 1.263407  [   32/  118]
train() client id: f_00009-2-1 loss: 1.060804  [   64/  118]
train() client id: f_00009-2-2 loss: 1.070155  [   96/  118]
train() client id: f_00009-3-0 loss: 1.105821  [   32/  118]
train() client id: f_00009-3-1 loss: 0.987575  [   64/  118]
train() client id: f_00009-3-2 loss: 1.165904  [   96/  118]
train() client id: f_00009-4-0 loss: 1.129811  [   32/  118]
train() client id: f_00009-4-1 loss: 0.944452  [   64/  118]
train() client id: f_00009-4-2 loss: 1.066120  [   96/  118]
train() client id: f_00009-5-0 loss: 1.050591  [   32/  118]
train() client id: f_00009-5-1 loss: 0.962335  [   64/  118]
train() client id: f_00009-5-2 loss: 1.055546  [   96/  118]
train() client id: f_00009-6-0 loss: 0.961859  [   32/  118]
train() client id: f_00009-6-1 loss: 0.966021  [   64/  118]
train() client id: f_00009-6-2 loss: 0.995503  [   96/  118]
train() client id: f_00009-7-0 loss: 0.966330  [   32/  118]
train() client id: f_00009-7-1 loss: 1.010294  [   64/  118]
train() client id: f_00009-7-2 loss: 1.017569  [   96/  118]
train() client id: f_00009-8-0 loss: 0.945146  [   32/  118]
train() client id: f_00009-8-1 loss: 0.884910  [   64/  118]
train() client id: f_00009-8-2 loss: 0.952400  [   96/  118]
train() client id: f_00009-9-0 loss: 1.001876  [   32/  118]
train() client id: f_00009-9-1 loss: 0.911611  [   64/  118]
train() client id: f_00009-9-2 loss: 0.896826  [   96/  118]
train() client id: f_00009-10-0 loss: 0.940289  [   32/  118]
train() client id: f_00009-10-1 loss: 1.050283  [   64/  118]
train() client id: f_00009-10-2 loss: 0.870982  [   96/  118]
train() client id: f_00009-11-0 loss: 0.944239  [   32/  118]
train() client id: f_00009-11-1 loss: 0.828820  [   64/  118]
train() client id: f_00009-11-2 loss: 0.972985  [   96/  118]
train() client id: f_00009-12-0 loss: 1.011624  [   32/  118]
train() client id: f_00009-12-1 loss: 0.862944  [   64/  118]
train() client id: f_00009-12-2 loss: 0.886618  [   96/  118]
At round 15 accuracy: 0.6312997347480106
At round 15 training accuracy: 0.5767940979208585
At round 15 training loss: 0.8512728401250503
update_location
xs = 8.927491 196.223621 5.882650 10.934260 -112.581990 39.769243 -5.849135 -5.143845 -135.120581 20.134486 
ys = -187.390647 7.291448 85.684448 -67.290817 -9.642386 0.794442 -76.381692 81.628436 25.881276 -622.232496 
xs mean: 2.317620029478786
ys mean: -76.16579882624053
dists_uav = 212.591050 220.356245 131.819688 121.027320 150.889629 107.620741 125.969740 129.188470 170.080604 630.538402 
uav_gains = -108.491145 -109.014811 -103.000301 -102.072327 -104.471405 -100.797445 -102.507079 -102.781194 -105.789253 -126.925064 
uav_gains_db_mean: -106.58500237762416
dists_bs = 406.394271 407.348911 201.732002 305.411835 194.907206 276.538250 302.993020 193.828178 154.359197 820.766179 
bs_gains = -112.617591 -112.646123 -104.100754 -109.143863 -103.682240 -107.936205 -109.047173 -103.614733 -100.845969 -121.165204 
bs_gains_db_mean: -108.47998548100608
Round 16
-------------------------------
ene_coms = [0.00968894 0.01001514 0.00759107 0.00689481 0.00743637 0.00651808
 0.00703327 0.007412   0.00653167 0.03258399]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.93620213 18.55677129  8.74977759  3.13193942 21.33315269 10.2466687
  3.89586255 12.54941154  9.1404378   8.90794131]
obj_prev = 105.44816503495147
eta_min = 3.166053923913545e-11	eta_max = 0.7420588954347109
af = 22.167428793692164	bf = 2.3089012038768644	zeta = 24.38417167306138	eta = 0.9090909090909091
af = 22.167428793692164	bf = 2.3089012038768644	zeta = 48.638511108035566	eta = 0.4557587863751422
af = 22.167428793692164	bf = 2.3089012038768644	zeta = 36.24841293576273	eta = 0.6115420510403022
af = 22.167428793692164	bf = 2.3089012038768644	zeta = 34.006801148727035	eta = 0.6518528072294724
af = 22.167428793692164	bf = 2.3089012038768644	zeta = 33.87978792779356	eta = 0.654296562922312
af = 22.167428793692164	bf = 2.3089012038768644	zeta = 33.879339563535076	eta = 0.6543052219811083
af = 22.167428793692164	bf = 2.3089012038768644	zeta = 33.879339557919735	eta = 0.6543052220895563
eta = 0.6543052220895563
ene_coms = [0.00968894 0.01001514 0.00759107 0.00689481 0.00743637 0.00651808
 0.00703327 0.007412   0.00653167 0.03258399]
ene_comp = [0.03500243 0.07361622 0.03444683 0.01194527 0.08500589 0.04055837
 0.01500104 0.04972567 0.03611362 0.03278005]
ene_total = [2.93489431 5.49209352 2.76064038 1.2372332  6.0707087  3.09152277
 1.44699904 3.75224601 2.80052739 4.29247424]
ti_comp = [0.34752824 0.34426629 0.36850697 0.37546958 0.37005393 0.37923691
 0.37408498 0.3702977  0.37910097 0.11857776]
ti_coms = [0.09688943 0.10015137 0.07591069 0.06894808 0.07436374 0.06518076
 0.07033269 0.07411997 0.0653167  0.32583991]
t_total = [29.18502808 29.18502808 29.18502808 29.18502808 29.18502808 29.18502808
 29.18502808 29.18502808 29.18502808 29.18502808]
ene_coms = [0.00968894 0.01001514 0.00759107 0.00689481 0.00743637 0.00651808
 0.00703327 0.007412   0.00653167 0.03258399]
ene_comp = [2.21918902e-05 2.10383404e-04 1.88120660e-05 7.55646552e-07
 2.80347770e-04 2.89934726e-05 1.50766068e-06 5.60428132e-05
 2.04825081e-05 1.56567783e-04]
ene_total = [0.63773283 0.67151271 0.49974296 0.45283354 0.5067592  0.42994781
 0.46197569 0.49042817 0.43028162 2.15008131]
optimize_network iter = 0 obj = 6.731295843946853
eta = 0.6543052220895563
freqs = [5.03591120e+07 1.06917548e+08 4.67383733e+07 1.59071110e+07
 1.14856090e+08 5.34736560e+07 2.00503110e+07 6.71428291e+07
 4.76306030e+07 1.38221750e+08]
eta_min = 0.6543052220895584	eta_max = 0.6543052220895538
af = 0.04260587728414012	bf = 2.3089012038768644	zeta = 0.046866465012554134	eta = 0.9090909090909091
af = 0.04260587728414012	bf = 2.3089012038768644	zeta = 25.442581754716894	eta = 0.0016745893830621675
af = 0.04260587728414012	bf = 2.3089012038768644	zeta = 2.5855757287195034	eta = 0.016478294103279088
af = 0.04260587728414012	bf = 2.3089012038768644	zeta = 2.525443637539435	eta = 0.016870650625824875
af = 0.04260587728414012	bf = 2.3089012038768644	zeta = 2.5254317326152824	eta = 0.016870730154332225
eta = 0.016870730154332225
ene_coms = [0.00968894 0.01001514 0.00759107 0.00689481 0.00743637 0.00651808
 0.00703327 0.007412   0.00653167 0.03258399]
ene_comp = [2.13567109e-04 2.02465743e-03 1.81040845e-04 7.27208220e-06
 2.69797039e-03 2.79023196e-04 1.45092072e-05 5.39336734e-04
 1.97116604e-04 1.50675443e-03]
ene_total = [0.2286632  0.27801618 0.1794692  0.15937897 0.23401659 0.1569548
 0.16274334 0.18360773 0.15537736 0.78720435]
ti_comp = [0.34752824 0.34426629 0.36850697 0.37546958 0.37005393 0.37923691
 0.37408498 0.3702977  0.37910097 0.11857776]
ti_coms = [0.09688943 0.10015137 0.07591069 0.06894808 0.07436374 0.06518076
 0.07033269 0.07411997 0.0653167  0.32583991]
t_total = [29.18502808 29.18502808 29.18502808 29.18502808 29.18502808 29.18502808
 29.18502808 29.18502808 29.18502808 29.18502808]
ene_coms = [0.00968894 0.01001514 0.00759107 0.00689481 0.00743637 0.00651808
 0.00703327 0.007412   0.00653167 0.03258399]
ene_comp = [2.21918902e-05 2.10383404e-04 1.88120660e-05 7.55646552e-07
 2.80347770e-04 2.89934726e-05 1.50766068e-06 5.60428132e-05
 2.04825081e-05 1.56567783e-04]
ene_total = [0.63773283 0.67151271 0.49974296 0.45283354 0.5067592  0.42994781
 0.46197569 0.49042817 0.43028162 2.15008131]
optimize_network iter = 1 obj = 6.731295843946893
eta = 0.6543052220895584
freqs = [5.03591120e+07 1.06917548e+08 4.67383733e+07 1.59071110e+07
 1.14856090e+08 5.34736560e+07 2.00503110e+07 6.71428291e+07
 4.76306030e+07 1.38221750e+08]
Done!
ene_coms = [0.00968894 0.01001514 0.00759107 0.00689481 0.00743637 0.00651808
 0.00703327 0.007412   0.00653167 0.03258399]
ene_comp = [2.07701689e-05 1.96905211e-04 1.76068728e-05 7.07236128e-07
 2.62387317e-04 2.71360086e-05 1.41107255e-06 5.24524357e-05
 1.91702982e-05 1.46537283e-04]
ene_total = [0.00970971 0.01021204 0.00760868 0.00689552 0.00769876 0.00654521
 0.00703468 0.00746445 0.00655084 0.03273053]
At round 16 energy consumption: 0.10245041807200983
At round 16 eta: 0.6543052220895584
At round 16 a_n: 22.701869304745298
At round 16 local rounds: 13.889852038732757
At round 16 global rounds: 65.6702697158666
gradient difference: 0.363635390996933
train() client id: f_00000-0-0 loss: 1.594701  [   32/  126]
train() client id: f_00000-0-1 loss: 1.371575  [   64/  126]
train() client id: f_00000-0-2 loss: 1.409493  [   96/  126]
train() client id: f_00000-1-0 loss: 1.395168  [   32/  126]
train() client id: f_00000-1-1 loss: 1.322742  [   64/  126]
train() client id: f_00000-1-2 loss: 1.233170  [   96/  126]
train() client id: f_00000-2-0 loss: 1.444767  [   32/  126]
train() client id: f_00000-2-1 loss: 1.151476  [   64/  126]
train() client id: f_00000-2-2 loss: 1.076846  [   96/  126]
train() client id: f_00000-3-0 loss: 1.090353  [   32/  126]
train() client id: f_00000-3-1 loss: 1.136403  [   64/  126]
train() client id: f_00000-3-2 loss: 1.127494  [   96/  126]
train() client id: f_00000-4-0 loss: 1.017349  [   32/  126]
train() client id: f_00000-4-1 loss: 0.976541  [   64/  126]
train() client id: f_00000-4-2 loss: 1.186532  [   96/  126]
train() client id: f_00000-5-0 loss: 1.050337  [   32/  126]
train() client id: f_00000-5-1 loss: 1.062500  [   64/  126]
train() client id: f_00000-5-2 loss: 0.968521  [   96/  126]
train() client id: f_00000-6-0 loss: 0.954209  [   32/  126]
train() client id: f_00000-6-1 loss: 1.054380  [   64/  126]
train() client id: f_00000-6-2 loss: 0.899096  [   96/  126]
train() client id: f_00000-7-0 loss: 0.902606  [   32/  126]
train() client id: f_00000-7-1 loss: 0.919936  [   64/  126]
train() client id: f_00000-7-2 loss: 0.921997  [   96/  126]
train() client id: f_00000-8-0 loss: 0.868953  [   32/  126]
train() client id: f_00000-8-1 loss: 0.929454  [   64/  126]
train() client id: f_00000-8-2 loss: 0.900525  [   96/  126]
train() client id: f_00000-9-0 loss: 0.952335  [   32/  126]
train() client id: f_00000-9-1 loss: 0.854117  [   64/  126]
train() client id: f_00000-9-2 loss: 0.825135  [   96/  126]
train() client id: f_00000-10-0 loss: 0.963266  [   32/  126]
train() client id: f_00000-10-1 loss: 0.821167  [   64/  126]
train() client id: f_00000-10-2 loss: 0.929316  [   96/  126]
train() client id: f_00000-11-0 loss: 0.882909  [   32/  126]
train() client id: f_00000-11-1 loss: 0.890314  [   64/  126]
train() client id: f_00000-11-2 loss: 0.878854  [   96/  126]
train() client id: f_00000-12-0 loss: 0.986231  [   32/  126]
train() client id: f_00000-12-1 loss: 0.874318  [   64/  126]
train() client id: f_00000-12-2 loss: 0.872771  [   96/  126]
train() client id: f_00001-0-0 loss: 0.491119  [   32/  265]
train() client id: f_00001-0-1 loss: 0.434559  [   64/  265]
train() client id: f_00001-0-2 loss: 0.541862  [   96/  265]
train() client id: f_00001-0-3 loss: 0.543623  [  128/  265]
train() client id: f_00001-0-4 loss: 0.442859  [  160/  265]
train() client id: f_00001-0-5 loss: 0.373090  [  192/  265]
train() client id: f_00001-0-6 loss: 0.420999  [  224/  265]
train() client id: f_00001-0-7 loss: 0.464311  [  256/  265]
train() client id: f_00001-1-0 loss: 0.479937  [   32/  265]
train() client id: f_00001-1-1 loss: 0.419121  [   64/  265]
train() client id: f_00001-1-2 loss: 0.477701  [   96/  265]
train() client id: f_00001-1-3 loss: 0.376377  [  128/  265]
train() client id: f_00001-1-4 loss: 0.441943  [  160/  265]
train() client id: f_00001-1-5 loss: 0.533606  [  192/  265]
train() client id: f_00001-1-6 loss: 0.452206  [  224/  265]
train() client id: f_00001-1-7 loss: 0.463543  [  256/  265]
train() client id: f_00001-2-0 loss: 0.466637  [   32/  265]
train() client id: f_00001-2-1 loss: 0.383110  [   64/  265]
train() client id: f_00001-2-2 loss: 0.475908  [   96/  265]
train() client id: f_00001-2-3 loss: 0.450276  [  128/  265]
train() client id: f_00001-2-4 loss: 0.439174  [  160/  265]
train() client id: f_00001-2-5 loss: 0.372433  [  192/  265]
train() client id: f_00001-2-6 loss: 0.642403  [  224/  265]
train() client id: f_00001-2-7 loss: 0.325695  [  256/  265]
train() client id: f_00001-3-0 loss: 0.392958  [   32/  265]
train() client id: f_00001-3-1 loss: 0.437532  [   64/  265]
train() client id: f_00001-3-2 loss: 0.480304  [   96/  265]
train() client id: f_00001-3-3 loss: 0.410100  [  128/  265]
train() client id: f_00001-3-4 loss: 0.492051  [  160/  265]
train() client id: f_00001-3-5 loss: 0.372968  [  192/  265]
train() client id: f_00001-3-6 loss: 0.434738  [  224/  265]
train() client id: f_00001-3-7 loss: 0.486043  [  256/  265]
train() client id: f_00001-4-0 loss: 0.390345  [   32/  265]
train() client id: f_00001-4-1 loss: 0.420265  [   64/  265]
train() client id: f_00001-4-2 loss: 0.492251  [   96/  265]
train() client id: f_00001-4-3 loss: 0.413843  [  128/  265]
train() client id: f_00001-4-4 loss: 0.518709  [  160/  265]
train() client id: f_00001-4-5 loss: 0.462097  [  192/  265]
train() client id: f_00001-4-6 loss: 0.401525  [  224/  265]
train() client id: f_00001-4-7 loss: 0.390149  [  256/  265]
train() client id: f_00001-5-0 loss: 0.458798  [   32/  265]
train() client id: f_00001-5-1 loss: 0.398577  [   64/  265]
train() client id: f_00001-5-2 loss: 0.570569  [   96/  265]
train() client id: f_00001-5-3 loss: 0.386203  [  128/  265]
train() client id: f_00001-5-4 loss: 0.432840  [  160/  265]
train() client id: f_00001-5-5 loss: 0.359734  [  192/  265]
train() client id: f_00001-5-6 loss: 0.397628  [  224/  265]
train() client id: f_00001-5-7 loss: 0.446310  [  256/  265]
train() client id: f_00001-6-0 loss: 0.384319  [   32/  265]
train() client id: f_00001-6-1 loss: 0.396724  [   64/  265]
train() client id: f_00001-6-2 loss: 0.486012  [   96/  265]
train() client id: f_00001-6-3 loss: 0.313506  [  128/  265]
train() client id: f_00001-6-4 loss: 0.352305  [  160/  265]
train() client id: f_00001-6-5 loss: 0.418027  [  192/  265]
train() client id: f_00001-6-6 loss: 0.551748  [  224/  265]
train() client id: f_00001-6-7 loss: 0.527093  [  256/  265]
train() client id: f_00001-7-0 loss: 0.453086  [   32/  265]
train() client id: f_00001-7-1 loss: 0.472044  [   64/  265]
train() client id: f_00001-7-2 loss: 0.430462  [   96/  265]
train() client id: f_00001-7-3 loss: 0.446939  [  128/  265]
train() client id: f_00001-7-4 loss: 0.401417  [  160/  265]
train() client id: f_00001-7-5 loss: 0.382827  [  192/  265]
train() client id: f_00001-7-6 loss: 0.393948  [  224/  265]
train() client id: f_00001-7-7 loss: 0.364546  [  256/  265]
train() client id: f_00001-8-0 loss: 0.500126  [   32/  265]
train() client id: f_00001-8-1 loss: 0.414522  [   64/  265]
train() client id: f_00001-8-2 loss: 0.385723  [   96/  265]
train() client id: f_00001-8-3 loss: 0.481096  [  128/  265]
train() client id: f_00001-8-4 loss: 0.370853  [  160/  265]
train() client id: f_00001-8-5 loss: 0.384984  [  192/  265]
train() client id: f_00001-8-6 loss: 0.468974  [  224/  265]
train() client id: f_00001-8-7 loss: 0.387968  [  256/  265]
train() client id: f_00001-9-0 loss: 0.476661  [   32/  265]
train() client id: f_00001-9-1 loss: 0.455860  [   64/  265]
train() client id: f_00001-9-2 loss: 0.355970  [   96/  265]
train() client id: f_00001-9-3 loss: 0.387838  [  128/  265]
train() client id: f_00001-9-4 loss: 0.385492  [  160/  265]
train() client id: f_00001-9-5 loss: 0.388530  [  192/  265]
train() client id: f_00001-9-6 loss: 0.488396  [  224/  265]
train() client id: f_00001-9-7 loss: 0.443588  [  256/  265]
train() client id: f_00001-10-0 loss: 0.391317  [   32/  265]
train() client id: f_00001-10-1 loss: 0.663724  [   64/  265]
train() client id: f_00001-10-2 loss: 0.517167  [   96/  265]
train() client id: f_00001-10-3 loss: 0.354260  [  128/  265]
train() client id: f_00001-10-4 loss: 0.341026  [  160/  265]
train() client id: f_00001-10-5 loss: 0.336153  [  192/  265]
train() client id: f_00001-10-6 loss: 0.385164  [  224/  265]
train() client id: f_00001-10-7 loss: 0.398841  [  256/  265]
train() client id: f_00001-11-0 loss: 0.446079  [   32/  265]
train() client id: f_00001-11-1 loss: 0.384875  [   64/  265]
train() client id: f_00001-11-2 loss: 0.438163  [   96/  265]
train() client id: f_00001-11-3 loss: 0.340029  [  128/  265]
train() client id: f_00001-11-4 loss: 0.326885  [  160/  265]
train() client id: f_00001-11-5 loss: 0.494012  [  192/  265]
train() client id: f_00001-11-6 loss: 0.404662  [  224/  265]
train() client id: f_00001-11-7 loss: 0.548646  [  256/  265]
train() client id: f_00001-12-0 loss: 0.366368  [   32/  265]
train() client id: f_00001-12-1 loss: 0.381083  [   64/  265]
train() client id: f_00001-12-2 loss: 0.403030  [   96/  265]
train() client id: f_00001-12-3 loss: 0.444024  [  128/  265]
train() client id: f_00001-12-4 loss: 0.371964  [  160/  265]
train() client id: f_00001-12-5 loss: 0.339471  [  192/  265]
train() client id: f_00001-12-6 loss: 0.558324  [  224/  265]
train() client id: f_00001-12-7 loss: 0.438702  [  256/  265]
train() client id: f_00002-0-0 loss: 1.320489  [   32/  124]
train() client id: f_00002-0-1 loss: 1.232451  [   64/  124]
train() client id: f_00002-0-2 loss: 1.122942  [   96/  124]
train() client id: f_00002-1-0 loss: 1.142663  [   32/  124]
train() client id: f_00002-1-1 loss: 1.126325  [   64/  124]
train() client id: f_00002-1-2 loss: 1.233236  [   96/  124]
train() client id: f_00002-2-0 loss: 1.146975  [   32/  124]
train() client id: f_00002-2-1 loss: 1.077084  [   64/  124]
train() client id: f_00002-2-2 loss: 1.287941  [   96/  124]
train() client id: f_00002-3-0 loss: 1.071907  [   32/  124]
train() client id: f_00002-3-1 loss: 1.211604  [   64/  124]
train() client id: f_00002-3-2 loss: 1.095742  [   96/  124]
train() client id: f_00002-4-0 loss: 1.048972  [   32/  124]
train() client id: f_00002-4-1 loss: 1.115612  [   64/  124]
train() client id: f_00002-4-2 loss: 1.137086  [   96/  124]
train() client id: f_00002-5-0 loss: 1.060889  [   32/  124]
train() client id: f_00002-5-1 loss: 1.103809  [   64/  124]
train() client id: f_00002-5-2 loss: 1.103355  [   96/  124]
train() client id: f_00002-6-0 loss: 1.042304  [   32/  124]
train() client id: f_00002-6-1 loss: 0.988976  [   64/  124]
train() client id: f_00002-6-2 loss: 1.084851  [   96/  124]
train() client id: f_00002-7-0 loss: 0.947618  [   32/  124]
train() client id: f_00002-7-1 loss: 1.069901  [   64/  124]
train() client id: f_00002-7-2 loss: 1.029807  [   96/  124]
train() client id: f_00002-8-0 loss: 1.133238  [   32/  124]
train() client id: f_00002-8-1 loss: 1.024119  [   64/  124]
train() client id: f_00002-8-2 loss: 0.932348  [   96/  124]
train() client id: f_00002-9-0 loss: 0.929193  [   32/  124]
train() client id: f_00002-9-1 loss: 1.098140  [   64/  124]
train() client id: f_00002-9-2 loss: 0.963682  [   96/  124]
train() client id: f_00002-10-0 loss: 1.021261  [   32/  124]
train() client id: f_00002-10-1 loss: 0.985521  [   64/  124]
train() client id: f_00002-10-2 loss: 1.061624  [   96/  124]
train() client id: f_00002-11-0 loss: 0.873785  [   32/  124]
train() client id: f_00002-11-1 loss: 0.955167  [   64/  124]
train() client id: f_00002-11-2 loss: 1.085934  [   96/  124]
train() client id: f_00002-12-0 loss: 1.069453  [   32/  124]
train() client id: f_00002-12-1 loss: 0.953734  [   64/  124]
train() client id: f_00002-12-2 loss: 0.931888  [   96/  124]
train() client id: f_00003-0-0 loss: 0.678791  [   32/   43]
train() client id: f_00003-1-0 loss: 0.980832  [   32/   43]
train() client id: f_00003-2-0 loss: 0.738835  [   32/   43]
train() client id: f_00003-3-0 loss: 0.834514  [   32/   43]
train() client id: f_00003-4-0 loss: 0.793372  [   32/   43]
train() client id: f_00003-5-0 loss: 0.671993  [   32/   43]
train() client id: f_00003-6-0 loss: 0.703832  [   32/   43]
train() client id: f_00003-7-0 loss: 0.862313  [   32/   43]
train() client id: f_00003-8-0 loss: 0.734677  [   32/   43]
train() client id: f_00003-9-0 loss: 0.885144  [   32/   43]
train() client id: f_00003-10-0 loss: 0.831342  [   32/   43]
train() client id: f_00003-11-0 loss: 0.883742  [   32/   43]
train() client id: f_00003-12-0 loss: 0.964175  [   32/   43]
train() client id: f_00004-0-0 loss: 0.936644  [   32/  306]
train() client id: f_00004-0-1 loss: 1.054701  [   64/  306]
train() client id: f_00004-0-2 loss: 0.872721  [   96/  306]
train() client id: f_00004-0-3 loss: 0.991119  [  128/  306]
train() client id: f_00004-0-4 loss: 1.009083  [  160/  306]
train() client id: f_00004-0-5 loss: 1.003078  [  192/  306]
train() client id: f_00004-0-6 loss: 0.896984  [  224/  306]
train() client id: f_00004-0-7 loss: 0.943011  [  256/  306]
train() client id: f_00004-0-8 loss: 0.810547  [  288/  306]
train() client id: f_00004-1-0 loss: 0.813754  [   32/  306]
train() client id: f_00004-1-1 loss: 0.893816  [   64/  306]
train() client id: f_00004-1-2 loss: 1.048497  [   96/  306]
train() client id: f_00004-1-3 loss: 0.992191  [  128/  306]
train() client id: f_00004-1-4 loss: 1.064087  [  160/  306]
train() client id: f_00004-1-5 loss: 0.971843  [  192/  306]
train() client id: f_00004-1-6 loss: 0.952656  [  224/  306]
train() client id: f_00004-1-7 loss: 0.950796  [  256/  306]
train() client id: f_00004-1-8 loss: 0.795438  [  288/  306]
train() client id: f_00004-2-0 loss: 0.966070  [   32/  306]
train() client id: f_00004-2-1 loss: 0.813040  [   64/  306]
train() client id: f_00004-2-2 loss: 0.913830  [   96/  306]
train() client id: f_00004-2-3 loss: 0.862084  [  128/  306]
train() client id: f_00004-2-4 loss: 1.048924  [  160/  306]
train() client id: f_00004-2-5 loss: 0.946709  [  192/  306]
train() client id: f_00004-2-6 loss: 0.907930  [  224/  306]
train() client id: f_00004-2-7 loss: 0.992658  [  256/  306]
train() client id: f_00004-2-8 loss: 0.996200  [  288/  306]
train() client id: f_00004-3-0 loss: 0.962813  [   32/  306]
train() client id: f_00004-3-1 loss: 1.039421  [   64/  306]
train() client id: f_00004-3-2 loss: 0.858375  [   96/  306]
train() client id: f_00004-3-3 loss: 1.037594  [  128/  306]
train() client id: f_00004-3-4 loss: 0.995657  [  160/  306]
train() client id: f_00004-3-5 loss: 0.883947  [  192/  306]
train() client id: f_00004-3-6 loss: 0.878794  [  224/  306]
train() client id: f_00004-3-7 loss: 0.890620  [  256/  306]
train() client id: f_00004-3-8 loss: 0.836011  [  288/  306]
train() client id: f_00004-4-0 loss: 0.898598  [   32/  306]
train() client id: f_00004-4-1 loss: 0.907707  [   64/  306]
train() client id: f_00004-4-2 loss: 1.092255  [   96/  306]
train() client id: f_00004-4-3 loss: 0.936457  [  128/  306]
train() client id: f_00004-4-4 loss: 0.884552  [  160/  306]
train() client id: f_00004-4-5 loss: 1.038874  [  192/  306]
train() client id: f_00004-4-6 loss: 0.889707  [  224/  306]
train() client id: f_00004-4-7 loss: 0.879544  [  256/  306]
train() client id: f_00004-4-8 loss: 0.898595  [  288/  306]
train() client id: f_00004-5-0 loss: 1.080137  [   32/  306]
train() client id: f_00004-5-1 loss: 0.836733  [   64/  306]
train() client id: f_00004-5-2 loss: 0.916445  [   96/  306]
train() client id: f_00004-5-3 loss: 0.931059  [  128/  306]
train() client id: f_00004-5-4 loss: 0.835303  [  160/  306]
train() client id: f_00004-5-5 loss: 0.926057  [  192/  306]
train() client id: f_00004-5-6 loss: 1.030284  [  224/  306]
train() client id: f_00004-5-7 loss: 0.953641  [  256/  306]
train() client id: f_00004-5-8 loss: 0.868236  [  288/  306]
train() client id: f_00004-6-0 loss: 1.066139  [   32/  306]
train() client id: f_00004-6-1 loss: 1.015813  [   64/  306]
train() client id: f_00004-6-2 loss: 0.977531  [   96/  306]
train() client id: f_00004-6-3 loss: 0.970310  [  128/  306]
train() client id: f_00004-6-4 loss: 0.884538  [  160/  306]
train() client id: f_00004-6-5 loss: 0.812850  [  192/  306]
train() client id: f_00004-6-6 loss: 0.833973  [  224/  306]
train() client id: f_00004-6-7 loss: 0.888134  [  256/  306]
train() client id: f_00004-6-8 loss: 0.971582  [  288/  306]
train() client id: f_00004-7-0 loss: 0.949767  [   32/  306]
train() client id: f_00004-7-1 loss: 0.926134  [   64/  306]
train() client id: f_00004-7-2 loss: 0.977077  [   96/  306]
train() client id: f_00004-7-3 loss: 0.905032  [  128/  306]
train() client id: f_00004-7-4 loss: 0.907991  [  160/  306]
train() client id: f_00004-7-5 loss: 0.965079  [  192/  306]
train() client id: f_00004-7-6 loss: 0.942223  [  224/  306]
train() client id: f_00004-7-7 loss: 0.949235  [  256/  306]
train() client id: f_00004-7-8 loss: 0.717891  [  288/  306]
train() client id: f_00004-8-0 loss: 0.871819  [   32/  306]
train() client id: f_00004-8-1 loss: 0.903875  [   64/  306]
train() client id: f_00004-8-2 loss: 0.978755  [   96/  306]
train() client id: f_00004-8-3 loss: 0.829034  [  128/  306]
train() client id: f_00004-8-4 loss: 1.011926  [  160/  306]
train() client id: f_00004-8-5 loss: 0.921581  [  192/  306]
train() client id: f_00004-8-6 loss: 0.918422  [  224/  306]
train() client id: f_00004-8-7 loss: 0.886237  [  256/  306]
train() client id: f_00004-8-8 loss: 0.971424  [  288/  306]
train() client id: f_00004-9-0 loss: 0.860682  [   32/  306]
train() client id: f_00004-9-1 loss: 0.798589  [   64/  306]
train() client id: f_00004-9-2 loss: 0.859547  [   96/  306]
train() client id: f_00004-9-3 loss: 1.007872  [  128/  306]
train() client id: f_00004-9-4 loss: 0.936054  [  160/  306]
train() client id: f_00004-9-5 loss: 0.970222  [  192/  306]
train() client id: f_00004-9-6 loss: 1.035451  [  224/  306]
train() client id: f_00004-9-7 loss: 0.873258  [  256/  306]
train() client id: f_00004-9-8 loss: 0.957319  [  288/  306]
train() client id: f_00004-10-0 loss: 0.843834  [   32/  306]
train() client id: f_00004-10-1 loss: 0.923308  [   64/  306]
train() client id: f_00004-10-2 loss: 1.007508  [   96/  306]
train() client id: f_00004-10-3 loss: 0.872772  [  128/  306]
train() client id: f_00004-10-4 loss: 0.938407  [  160/  306]
train() client id: f_00004-10-5 loss: 0.849620  [  192/  306]
train() client id: f_00004-10-6 loss: 0.986830  [  224/  306]
train() client id: f_00004-10-7 loss: 0.875242  [  256/  306]
train() client id: f_00004-10-8 loss: 0.976805  [  288/  306]
train() client id: f_00004-11-0 loss: 0.808464  [   32/  306]
train() client id: f_00004-11-1 loss: 0.931456  [   64/  306]
train() client id: f_00004-11-2 loss: 0.941424  [   96/  306]
train() client id: f_00004-11-3 loss: 0.923521  [  128/  306]
train() client id: f_00004-11-4 loss: 1.030779  [  160/  306]
train() client id: f_00004-11-5 loss: 0.930774  [  192/  306]
train() client id: f_00004-11-6 loss: 0.890840  [  224/  306]
train() client id: f_00004-11-7 loss: 1.009394  [  256/  306]
train() client id: f_00004-11-8 loss: 0.853421  [  288/  306]
train() client id: f_00004-12-0 loss: 0.953813  [   32/  306]
train() client id: f_00004-12-1 loss: 0.963560  [   64/  306]
train() client id: f_00004-12-2 loss: 0.860837  [   96/  306]
train() client id: f_00004-12-3 loss: 0.917801  [  128/  306]
train() client id: f_00004-12-4 loss: 0.841338  [  160/  306]
train() client id: f_00004-12-5 loss: 0.918201  [  192/  306]
train() client id: f_00004-12-6 loss: 1.018327  [  224/  306]
train() client id: f_00004-12-7 loss: 1.001785  [  256/  306]
train() client id: f_00004-12-8 loss: 0.834168  [  288/  306]
train() client id: f_00005-0-0 loss: 0.530524  [   32/  146]
train() client id: f_00005-0-1 loss: 0.689512  [   64/  146]
train() client id: f_00005-0-2 loss: 0.461324  [   96/  146]
train() client id: f_00005-0-3 loss: 0.415424  [  128/  146]
train() client id: f_00005-1-0 loss: 0.664241  [   32/  146]
train() client id: f_00005-1-1 loss: 0.449058  [   64/  146]
train() client id: f_00005-1-2 loss: 0.574099  [   96/  146]
train() client id: f_00005-1-3 loss: 0.352494  [  128/  146]
train() client id: f_00005-2-0 loss: 0.538528  [   32/  146]
train() client id: f_00005-2-1 loss: 0.404412  [   64/  146]
train() client id: f_00005-2-2 loss: 0.412141  [   96/  146]
train() client id: f_00005-2-3 loss: 0.547812  [  128/  146]
train() client id: f_00005-3-0 loss: 0.690429  [   32/  146]
train() client id: f_00005-3-1 loss: 0.572383  [   64/  146]
train() client id: f_00005-3-2 loss: 0.457125  [   96/  146]
train() client id: f_00005-3-3 loss: 0.392871  [  128/  146]
train() client id: f_00005-4-0 loss: 0.484864  [   32/  146]
train() client id: f_00005-4-1 loss: 0.526305  [   64/  146]
train() client id: f_00005-4-2 loss: 0.505176  [   96/  146]
train() client id: f_00005-4-3 loss: 0.551667  [  128/  146]
train() client id: f_00005-5-0 loss: 0.488581  [   32/  146]
train() client id: f_00005-5-1 loss: 0.356498  [   64/  146]
train() client id: f_00005-5-2 loss: 0.561526  [   96/  146]
train() client id: f_00005-5-3 loss: 0.634200  [  128/  146]
train() client id: f_00005-6-0 loss: 0.406640  [   32/  146]
train() client id: f_00005-6-1 loss: 0.535322  [   64/  146]
train() client id: f_00005-6-2 loss: 0.472565  [   96/  146]
train() client id: f_00005-6-3 loss: 0.554780  [  128/  146]
train() client id: f_00005-7-0 loss: 0.539973  [   32/  146]
train() client id: f_00005-7-1 loss: 0.531328  [   64/  146]
train() client id: f_00005-7-2 loss: 0.513362  [   96/  146]
train() client id: f_00005-7-3 loss: 0.429712  [  128/  146]
train() client id: f_00005-8-0 loss: 0.525489  [   32/  146]
train() client id: f_00005-8-1 loss: 0.427914  [   64/  146]
train() client id: f_00005-8-2 loss: 0.597940  [   96/  146]
train() client id: f_00005-8-3 loss: 0.355921  [  128/  146]
train() client id: f_00005-9-0 loss: 0.516616  [   32/  146]
train() client id: f_00005-9-1 loss: 0.633428  [   64/  146]
train() client id: f_00005-9-2 loss: 0.416223  [   96/  146]
train() client id: f_00005-9-3 loss: 0.409652  [  128/  146]
train() client id: f_00005-10-0 loss: 0.460345  [   32/  146]
train() client id: f_00005-10-1 loss: 0.484281  [   64/  146]
train() client id: f_00005-10-2 loss: 0.426362  [   96/  146]
train() client id: f_00005-10-3 loss: 0.594661  [  128/  146]
train() client id: f_00005-11-0 loss: 0.576557  [   32/  146]
train() client id: f_00005-11-1 loss: 0.318730  [   64/  146]
train() client id: f_00005-11-2 loss: 0.468287  [   96/  146]
train() client id: f_00005-11-3 loss: 0.518468  [  128/  146]
train() client id: f_00005-12-0 loss: 0.528519  [   32/  146]
train() client id: f_00005-12-1 loss: 0.556677  [   64/  146]
train() client id: f_00005-12-2 loss: 0.421047  [   96/  146]
train() client id: f_00005-12-3 loss: 0.463492  [  128/  146]
train() client id: f_00006-0-0 loss: 0.639168  [   32/   54]
train() client id: f_00006-1-0 loss: 0.606877  [   32/   54]
train() client id: f_00006-2-0 loss: 0.585812  [   32/   54]
train() client id: f_00006-3-0 loss: 0.613219  [   32/   54]
train() client id: f_00006-4-0 loss: 0.616024  [   32/   54]
train() client id: f_00006-5-0 loss: 0.586348  [   32/   54]
train() client id: f_00006-6-0 loss: 0.643287  [   32/   54]
train() client id: f_00006-7-0 loss: 0.561925  [   32/   54]
train() client id: f_00006-8-0 loss: 0.632229  [   32/   54]
train() client id: f_00006-9-0 loss: 0.689403  [   32/   54]
train() client id: f_00006-10-0 loss: 0.625178  [   32/   54]
train() client id: f_00006-11-0 loss: 0.614925  [   32/   54]
train() client id: f_00006-12-0 loss: 0.613768  [   32/   54]
train() client id: f_00007-0-0 loss: 0.546463  [   32/  179]
train() client id: f_00007-0-1 loss: 0.523628  [   64/  179]
train() client id: f_00007-0-2 loss: 0.726291  [   96/  179]
train() client id: f_00007-0-3 loss: 0.680054  [  128/  179]
train() client id: f_00007-0-4 loss: 0.828814  [  160/  179]
train() client id: f_00007-1-0 loss: 0.549291  [   32/  179]
train() client id: f_00007-1-1 loss: 0.653982  [   64/  179]
train() client id: f_00007-1-2 loss: 0.710273  [   96/  179]
train() client id: f_00007-1-3 loss: 0.696150  [  128/  179]
train() client id: f_00007-1-4 loss: 0.689081  [  160/  179]
train() client id: f_00007-2-0 loss: 0.669758  [   32/  179]
train() client id: f_00007-2-1 loss: 0.550546  [   64/  179]
train() client id: f_00007-2-2 loss: 0.593088  [   96/  179]
train() client id: f_00007-2-3 loss: 0.689251  [  128/  179]
train() client id: f_00007-2-4 loss: 0.562468  [  160/  179]
train() client id: f_00007-3-0 loss: 0.545108  [   32/  179]
train() client id: f_00007-3-1 loss: 0.665386  [   64/  179]
train() client id: f_00007-3-2 loss: 0.758822  [   96/  179]
train() client id: f_00007-3-3 loss: 0.576385  [  128/  179]
train() client id: f_00007-3-4 loss: 0.554486  [  160/  179]
train() client id: f_00007-4-0 loss: 0.599526  [   32/  179]
train() client id: f_00007-4-1 loss: 0.788749  [   64/  179]
train() client id: f_00007-4-2 loss: 0.508433  [   96/  179]
train() client id: f_00007-4-3 loss: 0.542243  [  128/  179]
train() client id: f_00007-4-4 loss: 0.590619  [  160/  179]
train() client id: f_00007-5-0 loss: 0.497673  [   32/  179]
train() client id: f_00007-5-1 loss: 0.720652  [   64/  179]
train() client id: f_00007-5-2 loss: 0.573816  [   96/  179]
train() client id: f_00007-5-3 loss: 0.458616  [  128/  179]
train() client id: f_00007-5-4 loss: 0.678500  [  160/  179]
train() client id: f_00007-6-0 loss: 0.634966  [   32/  179]
train() client id: f_00007-6-1 loss: 0.580980  [   64/  179]
train() client id: f_00007-6-2 loss: 0.582783  [   96/  179]
train() client id: f_00007-6-3 loss: 0.730233  [  128/  179]
train() client id: f_00007-6-4 loss: 0.528970  [  160/  179]
train() client id: f_00007-7-0 loss: 0.608502  [   32/  179]
train() client id: f_00007-7-1 loss: 0.530356  [   64/  179]
train() client id: f_00007-7-2 loss: 0.566256  [   96/  179]
train() client id: f_00007-7-3 loss: 0.685617  [  128/  179]
train() client id: f_00007-7-4 loss: 0.630122  [  160/  179]
train() client id: f_00007-8-0 loss: 0.562837  [   32/  179]
train() client id: f_00007-8-1 loss: 0.506753  [   64/  179]
train() client id: f_00007-8-2 loss: 0.540350  [   96/  179]
train() client id: f_00007-8-3 loss: 0.756502  [  128/  179]
train() client id: f_00007-8-4 loss: 0.554529  [  160/  179]
train() client id: f_00007-9-0 loss: 0.630619  [   32/  179]
train() client id: f_00007-9-1 loss: 0.637652  [   64/  179]
train() client id: f_00007-9-2 loss: 0.520970  [   96/  179]
train() client id: f_00007-9-3 loss: 0.655610  [  128/  179]
train() client id: f_00007-9-4 loss: 0.481268  [  160/  179]
train() client id: f_00007-10-0 loss: 0.558095  [   32/  179]
train() client id: f_00007-10-1 loss: 0.655818  [   64/  179]
train() client id: f_00007-10-2 loss: 0.524626  [   96/  179]
train() client id: f_00007-10-3 loss: 0.556071  [  128/  179]
train() client id: f_00007-10-4 loss: 0.632320  [  160/  179]
train() client id: f_00007-11-0 loss: 0.608220  [   32/  179]
train() client id: f_00007-11-1 loss: 0.589486  [   64/  179]
train() client id: f_00007-11-2 loss: 0.581599  [   96/  179]
train() client id: f_00007-11-3 loss: 0.512565  [  128/  179]
train() client id: f_00007-11-4 loss: 0.630080  [  160/  179]
train() client id: f_00007-12-0 loss: 0.668958  [   32/  179]
train() client id: f_00007-12-1 loss: 0.419135  [   64/  179]
train() client id: f_00007-12-2 loss: 0.523499  [   96/  179]
train() client id: f_00007-12-3 loss: 0.615898  [  128/  179]
train() client id: f_00007-12-4 loss: 0.653015  [  160/  179]
train() client id: f_00008-0-0 loss: 0.741466  [   32/  130]
train() client id: f_00008-0-1 loss: 0.709873  [   64/  130]
train() client id: f_00008-0-2 loss: 0.670522  [   96/  130]
train() client id: f_00008-0-3 loss: 0.709886  [  128/  130]
train() client id: f_00008-1-0 loss: 0.625740  [   32/  130]
train() client id: f_00008-1-1 loss: 0.782995  [   64/  130]
train() client id: f_00008-1-2 loss: 0.711308  [   96/  130]
train() client id: f_00008-1-3 loss: 0.703351  [  128/  130]
train() client id: f_00008-2-0 loss: 0.763862  [   32/  130]
train() client id: f_00008-2-1 loss: 0.661320  [   64/  130]
train() client id: f_00008-2-2 loss: 0.704382  [   96/  130]
train() client id: f_00008-2-3 loss: 0.675962  [  128/  130]
train() client id: f_00008-3-0 loss: 0.738979  [   32/  130]
train() client id: f_00008-3-1 loss: 0.693133  [   64/  130]
train() client id: f_00008-3-2 loss: 0.634013  [   96/  130]
train() client id: f_00008-3-3 loss: 0.728919  [  128/  130]
train() client id: f_00008-4-0 loss: 0.759616  [   32/  130]
train() client id: f_00008-4-1 loss: 0.609649  [   64/  130]
train() client id: f_00008-4-2 loss: 0.735935  [   96/  130]
train() client id: f_00008-4-3 loss: 0.692074  [  128/  130]
train() client id: f_00008-5-0 loss: 0.697965  [   32/  130]
train() client id: f_00008-5-1 loss: 0.658797  [   64/  130]
train() client id: f_00008-5-2 loss: 0.752490  [   96/  130]
train() client id: f_00008-5-3 loss: 0.689204  [  128/  130]
train() client id: f_00008-6-0 loss: 0.806503  [   32/  130]
train() client id: f_00008-6-1 loss: 0.601108  [   64/  130]
train() client id: f_00008-6-2 loss: 0.676707  [   96/  130]
train() client id: f_00008-6-3 loss: 0.682921  [  128/  130]
train() client id: f_00008-7-0 loss: 0.586467  [   32/  130]
train() client id: f_00008-7-1 loss: 0.697985  [   64/  130]
train() client id: f_00008-7-2 loss: 0.755926  [   96/  130]
train() client id: f_00008-7-3 loss: 0.713178  [  128/  130]
train() client id: f_00008-8-0 loss: 0.736657  [   32/  130]
train() client id: f_00008-8-1 loss: 0.684096  [   64/  130]
train() client id: f_00008-8-2 loss: 0.681985  [   96/  130]
train() client id: f_00008-8-3 loss: 0.642021  [  128/  130]
train() client id: f_00008-9-0 loss: 0.726064  [   32/  130]
train() client id: f_00008-9-1 loss: 0.781220  [   64/  130]
train() client id: f_00008-9-2 loss: 0.709658  [   96/  130]
train() client id: f_00008-9-3 loss: 0.522047  [  128/  130]
train() client id: f_00008-10-0 loss: 0.674593  [   32/  130]
train() client id: f_00008-10-1 loss: 0.763766  [   64/  130]
train() client id: f_00008-10-2 loss: 0.643148  [   96/  130]
train() client id: f_00008-10-3 loss: 0.702251  [  128/  130]
train() client id: f_00008-11-0 loss: 0.609035  [   32/  130]
train() client id: f_00008-11-1 loss: 0.660612  [   64/  130]
train() client id: f_00008-11-2 loss: 0.721058  [   96/  130]
train() client id: f_00008-11-3 loss: 0.757967  [  128/  130]
train() client id: f_00008-12-0 loss: 0.696378  [   32/  130]
train() client id: f_00008-12-1 loss: 0.737877  [   64/  130]
train() client id: f_00008-12-2 loss: 0.632019  [   96/  130]
train() client id: f_00008-12-3 loss: 0.701138  [  128/  130]
train() client id: f_00009-0-0 loss: 1.261091  [   32/  118]
train() client id: f_00009-0-1 loss: 1.259370  [   64/  118]
train() client id: f_00009-0-2 loss: 1.106993  [   96/  118]
train() client id: f_00009-1-0 loss: 1.143943  [   32/  118]
train() client id: f_00009-1-1 loss: 1.112067  [   64/  118]
train() client id: f_00009-1-2 loss: 1.204388  [   96/  118]
train() client id: f_00009-2-0 loss: 1.152886  [   32/  118]
train() client id: f_00009-2-1 loss: 1.060180  [   64/  118]
train() client id: f_00009-2-2 loss: 1.084519  [   96/  118]
train() client id: f_00009-3-0 loss: 1.170328  [   32/  118]
train() client id: f_00009-3-1 loss: 0.940677  [   64/  118]
train() client id: f_00009-3-2 loss: 1.053480  [   96/  118]
train() client id: f_00009-4-0 loss: 1.117124  [   32/  118]
train() client id: f_00009-4-1 loss: 0.998449  [   64/  118]
train() client id: f_00009-4-2 loss: 0.944613  [   96/  118]
train() client id: f_00009-5-0 loss: 0.933910  [   32/  118]
train() client id: f_00009-5-1 loss: 1.092969  [   64/  118]
train() client id: f_00009-5-2 loss: 1.050980  [   96/  118]
train() client id: f_00009-6-0 loss: 1.018934  [   32/  118]
train() client id: f_00009-6-1 loss: 0.990342  [   64/  118]
train() client id: f_00009-6-2 loss: 0.950751  [   96/  118]
train() client id: f_00009-7-0 loss: 0.814515  [   32/  118]
train() client id: f_00009-7-1 loss: 0.987034  [   64/  118]
train() client id: f_00009-7-2 loss: 1.086423  [   96/  118]
train() client id: f_00009-8-0 loss: 0.961658  [   32/  118]
train() client id: f_00009-8-1 loss: 1.037392  [   64/  118]
train() client id: f_00009-8-2 loss: 0.826007  [   96/  118]
train() client id: f_00009-9-0 loss: 0.934952  [   32/  118]
train() client id: f_00009-9-1 loss: 0.906755  [   64/  118]
train() client id: f_00009-9-2 loss: 0.943165  [   96/  118]
train() client id: f_00009-10-0 loss: 0.872648  [   32/  118]
train() client id: f_00009-10-1 loss: 0.860884  [   64/  118]
train() client id: f_00009-10-2 loss: 1.046674  [   96/  118]
train() client id: f_00009-11-0 loss: 0.888203  [   32/  118]
train() client id: f_00009-11-1 loss: 0.932343  [   64/  118]
train() client id: f_00009-11-2 loss: 0.969419  [   96/  118]
train() client id: f_00009-12-0 loss: 0.945128  [   32/  118]
train() client id: f_00009-12-1 loss: 0.991250  [   64/  118]
train() client id: f_00009-12-2 loss: 0.928783  [   96/  118]
At round 16 accuracy: 0.6312997347480106
At round 16 training accuracy: 0.5747820254862508
At round 16 training loss: 0.8601050018694558
update_location
xs = 8.927491 201.223621 5.882650 10.934260 -117.581990 34.769243 -5.849135 -5.143845 -140.120581 20.134486 
ys = -192.390647 7.291448 90.684448 -62.290817 -9.642386 0.794442 -71.381692 86.628436 25.881276 -627.232496 
xs mean: 1.3176200294787859
ys mean: -75.16579882624053
dists_uav = 217.011201 224.820174 135.123184 118.320345 154.656070 105.875074 123.002270 132.404475 174.079343 635.473053 
uav_gains = -108.786261 -109.327799 -103.269362 -101.826661 -104.740973 -100.619879 -102.248131 -103.048410 -106.049199 -127.011628 
uav_gains_db_mean: -106.69283036078897
dists_bs = 410.859112 411.910635 199.568648 301.460579 193.364006 272.673269 298.857748 191.469702 153.143618 825.623670 
bs_gains = -112.750461 -112.781543 -103.969644 -108.985514 -103.585577 -107.765051 -108.880066 -103.465861 -100.749828 -121.236959 
bs_gains_db_mean: -108.41705039939444
Round 17
-------------------------------
ene_coms = [0.00987025 0.01022019 0.00754194 0.00681891 0.00740152 0.0064688
 0.00695015 0.00735879 0.00650479 0.03291715]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.80545973 18.28140142  8.61664352  3.08296787 21.01047199 10.0909451
  3.83520104 12.3588532   9.00191172  8.78105498]
obj_prev = 103.86491057241888
eta_min = 2.220832982696516e-11	eta_max = 0.7429680301595717
af = 21.83294705699746	bf = 2.2818246075535065	zeta = 24.01624176269721	eta = 0.9090909090909091
af = 21.83294705699746	bf = 2.2818246075535065	zeta = 47.98999388935665	eta = 0.4549479024176253
af = 21.83294705699746	bf = 2.2818246075535065	zeta = 35.733924300341144	eta = 0.6109865480626494
af = 21.83294705699746	bf = 2.2818246075535065	zeta = 33.51680575942046	eta = 0.6514029771724577
af = 21.83294705699746	bf = 2.2818246075535065	zeta = 33.39102293003569	eta = 0.6538567896750004
af = 21.83294705699746	bf = 2.2818246075535065	zeta = 33.390577650966705	eta = 0.6538655091630428
af = 21.83294705699746	bf = 2.2818246075535065	zeta = 33.390577645358206	eta = 0.6538655092728702
eta = 0.6538655092728702
ene_coms = [0.00987025 0.01022019 0.00754194 0.00681891 0.00740152 0.0064688
 0.00695015 0.00735879 0.00650479 0.03291715]
ene_comp = [0.0350579  0.07373289 0.03450143 0.0119642  0.08514061 0.04062265
 0.01502481 0.04980448 0.03617085 0.032832  ]
ene_total = [2.90223343 5.42313449 2.71588392 1.21333707 5.97796378 3.04197642
 1.41952134 3.69258759 2.75672739 4.24721222]
ti_comp = [0.35230908 0.34880973 0.37559225 0.38282246 0.37699643 0.38632356
 0.38151012 0.37742372 0.3859637  0.12184006]
ti_coms = [0.09870252 0.10220187 0.07541935 0.06818914 0.07401517 0.06468804
 0.06950148 0.07358788 0.0650479  0.32917154]
t_total = [29.13409233 29.13409233 29.13409233 29.13409233 29.13409233 29.13409233
 29.13409233 29.13409233 29.13409233 29.13409233]
ene_coms = [0.00987025 0.01022019 0.00754194 0.00681891 0.00740152 0.0064688
 0.00695015 0.00735879 0.00650479 0.03291715]
ene_comp = [2.16965184e-05 2.05914307e-04 1.81952445e-05 7.30359411e-07
 2.71403774e-04 2.80725751e-05 1.45644887e-06 5.42034318e-05
 1.98546952e-05 1.49001935e-04]
ene_total = [0.63899232 0.67349709 0.48836335 0.44053002 0.49564929 0.4196801
 0.44905429 0.47885856 0.42147388 2.13598152]
optimize_network iter = 0 obj = 6.642080413117766
eta = 0.6538655092728702
freqs = [4.97544662e+07 1.05692128e+08 4.59293632e+07 1.56263089e+07
 1.12919656e+08 5.25759365e+07 1.96912398e+07 6.59795266e+07
 4.68578389e+07 1.34734020e+08]
eta_min = 0.6538655092728702	eta_max = 0.6538655092728702
af = 0.04055164869647471	bf = 2.2818246075535065	zeta = 0.04460681356612218	eta = 0.9090909090909091
af = 0.04055164869647471	bf = 2.2818246075535065	zeta = 25.142585517303413	eta = 0.0016128670883337197
af = 0.04055164869647471	bf = 2.2818246075535065	zeta = 2.5466686797215634	eta = 0.0159234096760904
af = 0.04055164869647471	bf = 2.2818246075535065	zeta = 2.489345886022547	eta = 0.01629008203487051
af = 0.04055164869647471	bf = 2.2818246075535065	zeta = 2.4893352837191203	eta = 0.016290151415798707
eta = 0.016290151415798707
ene_coms = [0.00987025 0.01022019 0.00754194 0.00681891 0.00740152 0.0064688
 0.00695015 0.00735879 0.00650479 0.03291715]
ene_comp = [2.10257815e-04 1.99548569e-03 1.76327477e-04 7.07780716e-06
 2.63013462e-03 2.72047255e-04 1.41142348e-05 5.25277599e-04
 1.92408973e-04 1.44395615e-03]
ene_total = [0.22912588 0.27765726 0.17543296 0.15515201 0.22801534 0.15321679
 0.15829484 0.17920158 0.15222461 0.78101401]
ti_comp = [0.35230908 0.34880973 0.37559225 0.38282246 0.37699643 0.38632356
 0.38151012 0.37742372 0.3859637  0.12184006]
ti_coms = [0.09870252 0.10220187 0.07541935 0.06818914 0.07401517 0.06468804
 0.06950148 0.07358788 0.0650479  0.32917154]
t_total = [29.13409233 29.13409233 29.13409233 29.13409233 29.13409233 29.13409233
 29.13409233 29.13409233 29.13409233 29.13409233]
ene_coms = [0.00987025 0.01022019 0.00754194 0.00681891 0.00740152 0.0064688
 0.00695015 0.00735879 0.00650479 0.03291715]
ene_comp = [2.16965184e-05 2.05914307e-04 1.81952445e-05 7.30359411e-07
 2.71403774e-04 2.80725751e-05 1.45644887e-06 5.42034318e-05
 1.98546952e-05 1.49001935e-04]
ene_total = [0.63899232 0.67349709 0.48836335 0.44053002 0.49564929 0.4196801
 0.44905429 0.47885856 0.42147388 2.13598152]
optimize_network iter = 1 obj = 6.642080413117766
eta = 0.6538655092728702
freqs = [4.97544662e+07 1.05692128e+08 4.59293632e+07 1.56263089e+07
 1.12919656e+08 5.25759365e+07 1.96912398e+07 6.59795266e+07
 4.68578389e+07 1.34734020e+08]
Done!
ene_coms = [0.00987025 0.01022019 0.00754194 0.00681891 0.00740152 0.0064688
 0.00695015 0.00735879 0.00650479 0.03291715]
ene_comp = [2.02744016e-05 1.92417477e-04 1.70026216e-05 6.82487378e-07
 2.53614381e-04 2.62325341e-05 1.36098468e-06 5.06506214e-05
 1.85533022e-05 1.39235475e-04]
ene_total = [0.00989053 0.0104126  0.00755894 0.0068196  0.00765513 0.00649504
 0.00695151 0.00740944 0.00652334 0.03305639]
At round 17 energy consumption: 0.10277251312273443
At round 17 eta: 0.6538655092728702
At round 17 a_n: 22.359323457775982
At round 17 local rounds: 13.911865116727016
At round 17 global rounds: 64.59721309715604
gradient difference: 0.4929707646369934
train() client id: f_00000-0-0 loss: 1.238050  [   32/  126]
train() client id: f_00000-0-1 loss: 1.177787  [   64/  126]
train() client id: f_00000-0-2 loss: 1.093337  [   96/  126]
train() client id: f_00000-1-0 loss: 1.154876  [   32/  126]
train() client id: f_00000-1-1 loss: 0.956963  [   64/  126]
train() client id: f_00000-1-2 loss: 1.176246  [   96/  126]
train() client id: f_00000-2-0 loss: 1.036890  [   32/  126]
train() client id: f_00000-2-1 loss: 1.113534  [   64/  126]
train() client id: f_00000-2-2 loss: 0.948574  [   96/  126]
train() client id: f_00000-3-0 loss: 1.021019  [   32/  126]
train() client id: f_00000-3-1 loss: 0.957962  [   64/  126]
train() client id: f_00000-3-2 loss: 0.968746  [   96/  126]
train() client id: f_00000-4-0 loss: 0.888026  [   32/  126]
train() client id: f_00000-4-1 loss: 1.001849  [   64/  126]
train() client id: f_00000-4-2 loss: 0.934694  [   96/  126]
train() client id: f_00000-5-0 loss: 0.955593  [   32/  126]
train() client id: f_00000-5-1 loss: 0.910781  [   64/  126]
train() client id: f_00000-5-2 loss: 0.842950  [   96/  126]
train() client id: f_00000-6-0 loss: 0.956623  [   32/  126]
train() client id: f_00000-6-1 loss: 0.883811  [   64/  126]
train() client id: f_00000-6-2 loss: 0.906090  [   96/  126]
train() client id: f_00000-7-0 loss: 0.863826  [   32/  126]
train() client id: f_00000-7-1 loss: 0.831493  [   64/  126]
train() client id: f_00000-7-2 loss: 0.861191  [   96/  126]
train() client id: f_00000-8-0 loss: 0.884899  [   32/  126]
train() client id: f_00000-8-1 loss: 0.953285  [   64/  126]
train() client id: f_00000-8-2 loss: 0.856803  [   96/  126]
train() client id: f_00000-9-0 loss: 0.790157  [   32/  126]
train() client id: f_00000-9-1 loss: 0.878637  [   64/  126]
train() client id: f_00000-9-2 loss: 0.937962  [   96/  126]
train() client id: f_00000-10-0 loss: 0.984594  [   32/  126]
train() client id: f_00000-10-1 loss: 0.874417  [   64/  126]
train() client id: f_00000-10-2 loss: 0.806140  [   96/  126]
train() client id: f_00000-11-0 loss: 0.876190  [   32/  126]
train() client id: f_00000-11-1 loss: 0.890536  [   64/  126]
train() client id: f_00000-11-2 loss: 0.967422  [   96/  126]
train() client id: f_00000-12-0 loss: 0.917993  [   32/  126]
train() client id: f_00000-12-1 loss: 0.907656  [   64/  126]
train() client id: f_00000-12-2 loss: 0.942740  [   96/  126]
train() client id: f_00001-0-0 loss: 0.448033  [   32/  265]
train() client id: f_00001-0-1 loss: 0.455361  [   64/  265]
train() client id: f_00001-0-2 loss: 0.399661  [   96/  265]
train() client id: f_00001-0-3 loss: 0.576498  [  128/  265]
train() client id: f_00001-0-4 loss: 0.539417  [  160/  265]
train() client id: f_00001-0-5 loss: 0.453952  [  192/  265]
train() client id: f_00001-0-6 loss: 0.437410  [  224/  265]
train() client id: f_00001-0-7 loss: 0.465260  [  256/  265]
train() client id: f_00001-1-0 loss: 0.470927  [   32/  265]
train() client id: f_00001-1-1 loss: 0.527499  [   64/  265]
train() client id: f_00001-1-2 loss: 0.465564  [   96/  265]
train() client id: f_00001-1-3 loss: 0.451302  [  128/  265]
train() client id: f_00001-1-4 loss: 0.370273  [  160/  265]
train() client id: f_00001-1-5 loss: 0.532970  [  192/  265]
train() client id: f_00001-1-6 loss: 0.509031  [  224/  265]
train() client id: f_00001-1-7 loss: 0.374527  [  256/  265]
train() client id: f_00001-2-0 loss: 0.520747  [   32/  265]
train() client id: f_00001-2-1 loss: 0.418175  [   64/  265]
train() client id: f_00001-2-2 loss: 0.559712  [   96/  265]
train() client id: f_00001-2-3 loss: 0.366342  [  128/  265]
train() client id: f_00001-2-4 loss: 0.519332  [  160/  265]
train() client id: f_00001-2-5 loss: 0.357299  [  192/  265]
train() client id: f_00001-2-6 loss: 0.456622  [  224/  265]
train() client id: f_00001-2-7 loss: 0.438983  [  256/  265]
train() client id: f_00001-3-0 loss: 0.461506  [   32/  265]
train() client id: f_00001-3-1 loss: 0.509501  [   64/  265]
train() client id: f_00001-3-2 loss: 0.456499  [   96/  265]
train() client id: f_00001-3-3 loss: 0.481688  [  128/  265]
train() client id: f_00001-3-4 loss: 0.410132  [  160/  265]
train() client id: f_00001-3-5 loss: 0.468362  [  192/  265]
train() client id: f_00001-3-6 loss: 0.353565  [  224/  265]
train() client id: f_00001-3-7 loss: 0.408287  [  256/  265]
train() client id: f_00001-4-0 loss: 0.399254  [   32/  265]
train() client id: f_00001-4-1 loss: 0.417776  [   64/  265]
train() client id: f_00001-4-2 loss: 0.458880  [   96/  265]
train() client id: f_00001-4-3 loss: 0.472579  [  128/  265]
train() client id: f_00001-4-4 loss: 0.447014  [  160/  265]
train() client id: f_00001-4-5 loss: 0.421423  [  192/  265]
train() client id: f_00001-4-6 loss: 0.496939  [  224/  265]
train() client id: f_00001-4-7 loss: 0.478074  [  256/  265]
train() client id: f_00001-5-0 loss: 0.518187  [   32/  265]
train() client id: f_00001-5-1 loss: 0.435410  [   64/  265]
train() client id: f_00001-5-2 loss: 0.396809  [   96/  265]
train() client id: f_00001-5-3 loss: 0.336451  [  128/  265]
train() client id: f_00001-5-4 loss: 0.455784  [  160/  265]
train() client id: f_00001-5-5 loss: 0.433294  [  192/  265]
train() client id: f_00001-5-6 loss: 0.361303  [  224/  265]
train() client id: f_00001-5-7 loss: 0.547864  [  256/  265]
train() client id: f_00001-6-0 loss: 0.525095  [   32/  265]
train() client id: f_00001-6-1 loss: 0.480358  [   64/  265]
train() client id: f_00001-6-2 loss: 0.466637  [   96/  265]
train() client id: f_00001-6-3 loss: 0.433927  [  128/  265]
train() client id: f_00001-6-4 loss: 0.373621  [  160/  265]
train() client id: f_00001-6-5 loss: 0.454040  [  192/  265]
train() client id: f_00001-6-6 loss: 0.396700  [  224/  265]
train() client id: f_00001-6-7 loss: 0.347595  [  256/  265]
train() client id: f_00001-7-0 loss: 0.540039  [   32/  265]
train() client id: f_00001-7-1 loss: 0.489436  [   64/  265]
train() client id: f_00001-7-2 loss: 0.408348  [   96/  265]
train() client id: f_00001-7-3 loss: 0.455214  [  128/  265]
train() client id: f_00001-7-4 loss: 0.421626  [  160/  265]
train() client id: f_00001-7-5 loss: 0.436044  [  192/  265]
train() client id: f_00001-7-6 loss: 0.447318  [  224/  265]
train() client id: f_00001-7-7 loss: 0.346241  [  256/  265]
train() client id: f_00001-8-0 loss: 0.344184  [   32/  265]
train() client id: f_00001-8-1 loss: 0.381828  [   64/  265]
train() client id: f_00001-8-2 loss: 0.409522  [   96/  265]
train() client id: f_00001-8-3 loss: 0.355097  [  128/  265]
train() client id: f_00001-8-4 loss: 0.557079  [  160/  265]
train() client id: f_00001-8-5 loss: 0.411837  [  192/  265]
train() client id: f_00001-8-6 loss: 0.510758  [  224/  265]
train() client id: f_00001-8-7 loss: 0.569029  [  256/  265]
train() client id: f_00001-9-0 loss: 0.345723  [   32/  265]
train() client id: f_00001-9-1 loss: 0.426628  [   64/  265]
train() client id: f_00001-9-2 loss: 0.441151  [   96/  265]
train() client id: f_00001-9-3 loss: 0.590203  [  128/  265]
train() client id: f_00001-9-4 loss: 0.474419  [  160/  265]
train() client id: f_00001-9-5 loss: 0.342729  [  192/  265]
train() client id: f_00001-9-6 loss: 0.518039  [  224/  265]
train() client id: f_00001-9-7 loss: 0.396474  [  256/  265]
train() client id: f_00001-10-0 loss: 0.415013  [   32/  265]
train() client id: f_00001-10-1 loss: 0.338073  [   64/  265]
train() client id: f_00001-10-2 loss: 0.509974  [   96/  265]
train() client id: f_00001-10-3 loss: 0.494479  [  128/  265]
train() client id: f_00001-10-4 loss: 0.536199  [  160/  265]
train() client id: f_00001-10-5 loss: 0.456304  [  192/  265]
train() client id: f_00001-10-6 loss: 0.395980  [  224/  265]
train() client id: f_00001-10-7 loss: 0.395475  [  256/  265]
train() client id: f_00001-11-0 loss: 0.517115  [   32/  265]
train() client id: f_00001-11-1 loss: 0.507445  [   64/  265]
train() client id: f_00001-11-2 loss: 0.397771  [   96/  265]
train() client id: f_00001-11-3 loss: 0.420439  [  128/  265]
train() client id: f_00001-11-4 loss: 0.356782  [  160/  265]
train() client id: f_00001-11-5 loss: 0.397277  [  192/  265]
train() client id: f_00001-11-6 loss: 0.509204  [  224/  265]
train() client id: f_00001-11-7 loss: 0.440661  [  256/  265]
train() client id: f_00001-12-0 loss: 0.426819  [   32/  265]
train() client id: f_00001-12-1 loss: 0.509611  [   64/  265]
train() client id: f_00001-12-2 loss: 0.341501  [   96/  265]
train() client id: f_00001-12-3 loss: 0.441882  [  128/  265]
train() client id: f_00001-12-4 loss: 0.473167  [  160/  265]
train() client id: f_00001-12-5 loss: 0.348721  [  192/  265]
train() client id: f_00001-12-6 loss: 0.422052  [  224/  265]
train() client id: f_00001-12-7 loss: 0.473117  [  256/  265]
train() client id: f_00002-0-0 loss: 1.232510  [   32/  124]
train() client id: f_00002-0-1 loss: 1.350482  [   64/  124]
train() client id: f_00002-0-2 loss: 1.256965  [   96/  124]
train() client id: f_00002-1-0 loss: 1.300840  [   32/  124]
train() client id: f_00002-1-1 loss: 1.148156  [   64/  124]
train() client id: f_00002-1-2 loss: 1.257211  [   96/  124]
train() client id: f_00002-2-0 loss: 1.226062  [   32/  124]
train() client id: f_00002-2-1 loss: 1.232331  [   64/  124]
train() client id: f_00002-2-2 loss: 1.215187  [   96/  124]
train() client id: f_00002-3-0 loss: 1.307871  [   32/  124]
train() client id: f_00002-3-1 loss: 1.220738  [   64/  124]
train() client id: f_00002-3-2 loss: 1.044011  [   96/  124]
train() client id: f_00002-4-0 loss: 1.190250  [   32/  124]
train() client id: f_00002-4-1 loss: 1.093808  [   64/  124]
train() client id: f_00002-4-2 loss: 1.187134  [   96/  124]
train() client id: f_00002-5-0 loss: 1.127472  [   32/  124]
train() client id: f_00002-5-1 loss: 1.163285  [   64/  124]
train() client id: f_00002-5-2 loss: 1.091671  [   96/  124]
train() client id: f_00002-6-0 loss: 1.102884  [   32/  124]
train() client id: f_00002-6-1 loss: 0.987640  [   64/  124]
train() client id: f_00002-6-2 loss: 1.146166  [   96/  124]
train() client id: f_00002-7-0 loss: 1.222600  [   32/  124]
train() client id: f_00002-7-1 loss: 0.981323  [   64/  124]
train() client id: f_00002-7-2 loss: 1.073880  [   96/  124]
train() client id: f_00002-8-0 loss: 1.112512  [   32/  124]
train() client id: f_00002-8-1 loss: 1.072861  [   64/  124]
train() client id: f_00002-8-2 loss: 1.032617  [   96/  124]
train() client id: f_00002-9-0 loss: 1.155517  [   32/  124]
train() client id: f_00002-9-1 loss: 0.927619  [   64/  124]
train() client id: f_00002-9-2 loss: 1.102938  [   96/  124]
train() client id: f_00002-10-0 loss: 1.086596  [   32/  124]
train() client id: f_00002-10-1 loss: 0.954860  [   64/  124]
train() client id: f_00002-10-2 loss: 0.893994  [   96/  124]
train() client id: f_00002-11-0 loss: 1.004844  [   32/  124]
train() client id: f_00002-11-1 loss: 1.073129  [   64/  124]
train() client id: f_00002-11-2 loss: 0.986768  [   96/  124]
train() client id: f_00002-12-0 loss: 1.144920  [   32/  124]
train() client id: f_00002-12-1 loss: 1.011142  [   64/  124]
train() client id: f_00002-12-2 loss: 0.932197  [   96/  124]
train() client id: f_00003-0-0 loss: 0.575261  [   32/   43]
train() client id: f_00003-1-0 loss: 0.591320  [   32/   43]
train() client id: f_00003-2-0 loss: 0.461564  [   32/   43]
train() client id: f_00003-3-0 loss: 0.658903  [   32/   43]
train() client id: f_00003-4-0 loss: 0.483355  [   32/   43]
train() client id: f_00003-5-0 loss: 0.529401  [   32/   43]
train() client id: f_00003-6-0 loss: 0.672570  [   32/   43]
train() client id: f_00003-7-0 loss: 0.551521  [   32/   43]
train() client id: f_00003-8-0 loss: 0.571513  [   32/   43]
train() client id: f_00003-9-0 loss: 0.783897  [   32/   43]
train() client id: f_00003-10-0 loss: 0.548835  [   32/   43]
train() client id: f_00003-11-0 loss: 0.645895  [   32/   43]
train() client id: f_00003-12-0 loss: 0.509509  [   32/   43]
train() client id: f_00004-0-0 loss: 1.040479  [   32/  306]
train() client id: f_00004-0-1 loss: 0.810288  [   64/  306]
train() client id: f_00004-0-2 loss: 0.957596  [   96/  306]
train() client id: f_00004-0-3 loss: 0.890916  [  128/  306]
train() client id: f_00004-0-4 loss: 1.042291  [  160/  306]
train() client id: f_00004-0-5 loss: 0.936896  [  192/  306]
train() client id: f_00004-0-6 loss: 1.005969  [  224/  306]
train() client id: f_00004-0-7 loss: 0.997203  [  256/  306]
train() client id: f_00004-0-8 loss: 0.877336  [  288/  306]
train() client id: f_00004-1-0 loss: 0.862905  [   32/  306]
train() client id: f_00004-1-1 loss: 0.963442  [   64/  306]
train() client id: f_00004-1-2 loss: 0.900223  [   96/  306]
train() client id: f_00004-1-3 loss: 0.849655  [  128/  306]
train() client id: f_00004-1-4 loss: 0.991277  [  160/  306]
train() client id: f_00004-1-5 loss: 1.077942  [  192/  306]
train() client id: f_00004-1-6 loss: 0.935459  [  224/  306]
train() client id: f_00004-1-7 loss: 0.962775  [  256/  306]
train() client id: f_00004-1-8 loss: 1.152843  [  288/  306]
train() client id: f_00004-2-0 loss: 0.905841  [   32/  306]
train() client id: f_00004-2-1 loss: 0.851244  [   64/  306]
train() client id: f_00004-2-2 loss: 1.074857  [   96/  306]
train() client id: f_00004-2-3 loss: 1.062393  [  128/  306]
train() client id: f_00004-2-4 loss: 1.030557  [  160/  306]
train() client id: f_00004-2-5 loss: 0.925972  [  192/  306]
train() client id: f_00004-2-6 loss: 0.735227  [  224/  306]
train() client id: f_00004-2-7 loss: 1.215158  [  256/  306]
train() client id: f_00004-2-8 loss: 0.944995  [  288/  306]
train() client id: f_00004-3-0 loss: 0.916636  [   32/  306]
train() client id: f_00004-3-1 loss: 0.909421  [   64/  306]
train() client id: f_00004-3-2 loss: 0.969516  [   96/  306]
train() client id: f_00004-3-3 loss: 0.988742  [  128/  306]
train() client id: f_00004-3-4 loss: 1.040066  [  160/  306]
train() client id: f_00004-3-5 loss: 1.091269  [  192/  306]
train() client id: f_00004-3-6 loss: 0.918984  [  224/  306]
train() client id: f_00004-3-7 loss: 0.813192  [  256/  306]
train() client id: f_00004-3-8 loss: 0.944936  [  288/  306]
train() client id: f_00004-4-0 loss: 0.911276  [   32/  306]
train() client id: f_00004-4-1 loss: 0.972127  [   64/  306]
train() client id: f_00004-4-2 loss: 1.053059  [   96/  306]
train() client id: f_00004-4-3 loss: 0.807798  [  128/  306]
train() client id: f_00004-4-4 loss: 1.077445  [  160/  306]
train() client id: f_00004-4-5 loss: 1.009929  [  192/  306]
train() client id: f_00004-4-6 loss: 0.992484  [  224/  306]
train() client id: f_00004-4-7 loss: 0.808887  [  256/  306]
train() client id: f_00004-4-8 loss: 0.925618  [  288/  306]
train() client id: f_00004-5-0 loss: 1.018183  [   32/  306]
train() client id: f_00004-5-1 loss: 0.932841  [   64/  306]
train() client id: f_00004-5-2 loss: 0.876110  [   96/  306]
train() client id: f_00004-5-3 loss: 0.888611  [  128/  306]
train() client id: f_00004-5-4 loss: 0.986729  [  160/  306]
train() client id: f_00004-5-5 loss: 1.046372  [  192/  306]
train() client id: f_00004-5-6 loss: 0.827883  [  224/  306]
train() client id: f_00004-5-7 loss: 0.905301  [  256/  306]
train() client id: f_00004-5-8 loss: 1.095975  [  288/  306]
train() client id: f_00004-6-0 loss: 1.011161  [   32/  306]
train() client id: f_00004-6-1 loss: 0.938450  [   64/  306]
train() client id: f_00004-6-2 loss: 0.843611  [   96/  306]
train() client id: f_00004-6-3 loss: 1.049603  [  128/  306]
train() client id: f_00004-6-4 loss: 0.938439  [  160/  306]
train() client id: f_00004-6-5 loss: 1.120831  [  192/  306]
train() client id: f_00004-6-6 loss: 0.879932  [  224/  306]
train() client id: f_00004-6-7 loss: 0.977929  [  256/  306]
train() client id: f_00004-6-8 loss: 0.814147  [  288/  306]
train() client id: f_00004-7-0 loss: 1.045004  [   32/  306]
train() client id: f_00004-7-1 loss: 0.908507  [   64/  306]
train() client id: f_00004-7-2 loss: 1.011543  [   96/  306]
train() client id: f_00004-7-3 loss: 1.060378  [  128/  306]
train() client id: f_00004-7-4 loss: 0.909468  [  160/  306]
train() client id: f_00004-7-5 loss: 0.985342  [  192/  306]
train() client id: f_00004-7-6 loss: 0.978304  [  224/  306]
train() client id: f_00004-7-7 loss: 0.809163  [  256/  306]
train() client id: f_00004-7-8 loss: 0.936891  [  288/  306]
train() client id: f_00004-8-0 loss: 0.960642  [   32/  306]
train() client id: f_00004-8-1 loss: 0.815520  [   64/  306]
train() client id: f_00004-8-2 loss: 0.875314  [   96/  306]
train() client id: f_00004-8-3 loss: 0.940938  [  128/  306]
train() client id: f_00004-8-4 loss: 0.990048  [  160/  306]
train() client id: f_00004-8-5 loss: 1.060469  [  192/  306]
train() client id: f_00004-8-6 loss: 0.888231  [  224/  306]
train() client id: f_00004-8-7 loss: 1.029552  [  256/  306]
train() client id: f_00004-8-8 loss: 0.989232  [  288/  306]
train() client id: f_00004-9-0 loss: 0.896755  [   32/  306]
train() client id: f_00004-9-1 loss: 0.863174  [   64/  306]
train() client id: f_00004-9-2 loss: 1.100367  [   96/  306]
train() client id: f_00004-9-3 loss: 1.015464  [  128/  306]
train() client id: f_00004-9-4 loss: 1.168519  [  160/  306]
train() client id: f_00004-9-5 loss: 0.904456  [  192/  306]
train() client id: f_00004-9-6 loss: 0.918729  [  224/  306]
train() client id: f_00004-9-7 loss: 0.885041  [  256/  306]
train() client id: f_00004-9-8 loss: 0.908270  [  288/  306]
train() client id: f_00004-10-0 loss: 0.926022  [   32/  306]
train() client id: f_00004-10-1 loss: 1.061046  [   64/  306]
train() client id: f_00004-10-2 loss: 0.944392  [   96/  306]
train() client id: f_00004-10-3 loss: 0.930821  [  128/  306]
train() client id: f_00004-10-4 loss: 0.950711  [  160/  306]
train() client id: f_00004-10-5 loss: 0.914884  [  192/  306]
train() client id: f_00004-10-6 loss: 1.031728  [  224/  306]
train() client id: f_00004-10-7 loss: 0.903169  [  256/  306]
train() client id: f_00004-10-8 loss: 0.944312  [  288/  306]
train() client id: f_00004-11-0 loss: 0.880898  [   32/  306]
train() client id: f_00004-11-1 loss: 0.860565  [   64/  306]
train() client id: f_00004-11-2 loss: 0.871879  [   96/  306]
train() client id: f_00004-11-3 loss: 1.038796  [  128/  306]
train() client id: f_00004-11-4 loss: 0.984300  [  160/  306]
train() client id: f_00004-11-5 loss: 1.041597  [  192/  306]
train() client id: f_00004-11-6 loss: 0.949828  [  224/  306]
train() client id: f_00004-11-7 loss: 0.955267  [  256/  306]
train() client id: f_00004-11-8 loss: 0.979396  [  288/  306]
train() client id: f_00004-12-0 loss: 0.944926  [   32/  306]
train() client id: f_00004-12-1 loss: 0.889355  [   64/  306]
train() client id: f_00004-12-2 loss: 1.044152  [   96/  306]
train() client id: f_00004-12-3 loss: 0.865897  [  128/  306]
train() client id: f_00004-12-4 loss: 1.057422  [  160/  306]
train() client id: f_00004-12-5 loss: 0.990565  [  192/  306]
train() client id: f_00004-12-6 loss: 0.984942  [  224/  306]
train() client id: f_00004-12-7 loss: 0.900576  [  256/  306]
train() client id: f_00004-12-8 loss: 0.902368  [  288/  306]
train() client id: f_00005-0-0 loss: 0.657906  [   32/  146]
train() client id: f_00005-0-1 loss: 0.609241  [   64/  146]
train() client id: f_00005-0-2 loss: 0.652494  [   96/  146]
train() client id: f_00005-0-3 loss: 0.842047  [  128/  146]
train() client id: f_00005-1-0 loss: 0.499898  [   32/  146]
train() client id: f_00005-1-1 loss: 0.579560  [   64/  146]
train() client id: f_00005-1-2 loss: 0.850274  [   96/  146]
train() client id: f_00005-1-3 loss: 0.727179  [  128/  146]
train() client id: f_00005-2-0 loss: 0.632207  [   32/  146]
train() client id: f_00005-2-1 loss: 0.699991  [   64/  146]
train() client id: f_00005-2-2 loss: 0.661918  [   96/  146]
train() client id: f_00005-2-3 loss: 0.788203  [  128/  146]
train() client id: f_00005-3-0 loss: 0.608187  [   32/  146]
train() client id: f_00005-3-1 loss: 0.767740  [   64/  146]
train() client id: f_00005-3-2 loss: 0.650435  [   96/  146]
train() client id: f_00005-3-3 loss: 0.669640  [  128/  146]
train() client id: f_00005-4-0 loss: 0.715837  [   32/  146]
train() client id: f_00005-4-1 loss: 0.681102  [   64/  146]
train() client id: f_00005-4-2 loss: 0.628545  [   96/  146]
train() client id: f_00005-4-3 loss: 0.756531  [  128/  146]
train() client id: f_00005-5-0 loss: 0.920550  [   32/  146]
train() client id: f_00005-5-1 loss: 0.631456  [   64/  146]
train() client id: f_00005-5-2 loss: 0.539012  [   96/  146]
train() client id: f_00005-5-3 loss: 0.569283  [  128/  146]
train() client id: f_00005-6-0 loss: 0.942448  [   32/  146]
train() client id: f_00005-6-1 loss: 0.682317  [   64/  146]
train() client id: f_00005-6-2 loss: 0.585284  [   96/  146]
train() client id: f_00005-6-3 loss: 0.638227  [  128/  146]
train() client id: f_00005-7-0 loss: 0.679549  [   32/  146]
train() client id: f_00005-7-1 loss: 0.647017  [   64/  146]
train() client id: f_00005-7-2 loss: 0.580855  [   96/  146]
train() client id: f_00005-7-3 loss: 0.663631  [  128/  146]
train() client id: f_00005-8-0 loss: 0.888953  [   32/  146]
train() client id: f_00005-8-1 loss: 0.640727  [   64/  146]
train() client id: f_00005-8-2 loss: 0.778186  [   96/  146]
train() client id: f_00005-8-3 loss: 0.539154  [  128/  146]
train() client id: f_00005-9-0 loss: 0.627763  [   32/  146]
train() client id: f_00005-9-1 loss: 0.714040  [   64/  146]
train() client id: f_00005-9-2 loss: 0.840158  [   96/  146]
train() client id: f_00005-9-3 loss: 0.645221  [  128/  146]
train() client id: f_00005-10-0 loss: 0.587176  [   32/  146]
train() client id: f_00005-10-1 loss: 0.845916  [   64/  146]
train() client id: f_00005-10-2 loss: 0.756259  [   96/  146]
train() client id: f_00005-10-3 loss: 0.711087  [  128/  146]
train() client id: f_00005-11-0 loss: 0.666095  [   32/  146]
train() client id: f_00005-11-1 loss: 0.860191  [   64/  146]
train() client id: f_00005-11-2 loss: 0.711283  [   96/  146]
train() client id: f_00005-11-3 loss: 0.542251  [  128/  146]
train() client id: f_00005-12-0 loss: 0.903456  [   32/  146]
train() client id: f_00005-12-1 loss: 0.472070  [   64/  146]
train() client id: f_00005-12-2 loss: 0.842114  [   96/  146]
train() client id: f_00005-12-3 loss: 0.573624  [  128/  146]
train() client id: f_00006-0-0 loss: 0.588734  [   32/   54]
train() client id: f_00006-1-0 loss: 0.583923  [   32/   54]
train() client id: f_00006-2-0 loss: 0.591973  [   32/   54]
train() client id: f_00006-3-0 loss: 0.570569  [   32/   54]
train() client id: f_00006-4-0 loss: 0.577192  [   32/   54]
train() client id: f_00006-5-0 loss: 0.560898  [   32/   54]
train() client id: f_00006-6-0 loss: 0.582183  [   32/   54]
train() client id: f_00006-7-0 loss: 0.618182  [   32/   54]
train() client id: f_00006-8-0 loss: 0.628327  [   32/   54]
train() client id: f_00006-9-0 loss: 0.586707  [   32/   54]
train() client id: f_00006-10-0 loss: 0.618532  [   32/   54]
train() client id: f_00006-11-0 loss: 0.516811  [   32/   54]
train() client id: f_00006-12-0 loss: 0.533169  [   32/   54]
train() client id: f_00007-0-0 loss: 0.590570  [   32/  179]
train() client id: f_00007-0-1 loss: 0.655272  [   64/  179]
train() client id: f_00007-0-2 loss: 0.558521  [   96/  179]
train() client id: f_00007-0-3 loss: 0.654098  [  128/  179]
train() client id: f_00007-0-4 loss: 0.457066  [  160/  179]
train() client id: f_00007-1-0 loss: 0.656417  [   32/  179]
train() client id: f_00007-1-1 loss: 0.471284  [   64/  179]
train() client id: f_00007-1-2 loss: 0.495684  [   96/  179]
train() client id: f_00007-1-3 loss: 0.577956  [  128/  179]
train() client id: f_00007-1-4 loss: 0.709082  [  160/  179]
train() client id: f_00007-2-0 loss: 0.556189  [   32/  179]
train() client id: f_00007-2-1 loss: 0.581776  [   64/  179]
train() client id: f_00007-2-2 loss: 0.600163  [   96/  179]
train() client id: f_00007-2-3 loss: 0.716404  [  128/  179]
train() client id: f_00007-2-4 loss: 0.411808  [  160/  179]
train() client id: f_00007-3-0 loss: 0.584050  [   32/  179]
train() client id: f_00007-3-1 loss: 0.671004  [   64/  179]
train() client id: f_00007-3-2 loss: 0.541314  [   96/  179]
train() client id: f_00007-3-3 loss: 0.477868  [  128/  179]
train() client id: f_00007-3-4 loss: 0.521855  [  160/  179]
train() client id: f_00007-4-0 loss: 0.547901  [   32/  179]
train() client id: f_00007-4-1 loss: 0.700181  [   64/  179]
train() client id: f_00007-4-2 loss: 0.577626  [   96/  179]
train() client id: f_00007-4-3 loss: 0.410744  [  128/  179]
train() client id: f_00007-4-4 loss: 0.602010  [  160/  179]
train() client id: f_00007-5-0 loss: 0.563599  [   32/  179]
train() client id: f_00007-5-1 loss: 0.552114  [   64/  179]
train() client id: f_00007-5-2 loss: 0.404762  [   96/  179]
train() client id: f_00007-5-3 loss: 0.741284  [  128/  179]
train() client id: f_00007-5-4 loss: 0.544283  [  160/  179]
train() client id: f_00007-6-0 loss: 0.587846  [   32/  179]
train() client id: f_00007-6-1 loss: 0.641776  [   64/  179]
train() client id: f_00007-6-2 loss: 0.529677  [   96/  179]
train() client id: f_00007-6-3 loss: 0.440791  [  128/  179]
train() client id: f_00007-6-4 loss: 0.525090  [  160/  179]
train() client id: f_00007-7-0 loss: 0.746668  [   32/  179]
train() client id: f_00007-7-1 loss: 0.463803  [   64/  179]
train() client id: f_00007-7-2 loss: 0.573338  [   96/  179]
train() client id: f_00007-7-3 loss: 0.510766  [  128/  179]
train() client id: f_00007-7-4 loss: 0.513273  [  160/  179]
train() client id: f_00007-8-0 loss: 0.618190  [   32/  179]
train() client id: f_00007-8-1 loss: 0.677230  [   64/  179]
train() client id: f_00007-8-2 loss: 0.505618  [   96/  179]
train() client id: f_00007-8-3 loss: 0.405859  [  128/  179]
train() client id: f_00007-8-4 loss: 0.577272  [  160/  179]
train() client id: f_00007-9-0 loss: 0.401576  [   32/  179]
train() client id: f_00007-9-1 loss: 0.639374  [   64/  179]
train() client id: f_00007-9-2 loss: 0.547604  [   96/  179]
train() client id: f_00007-9-3 loss: 0.600000  [  128/  179]
train() client id: f_00007-9-4 loss: 0.598691  [  160/  179]
train() client id: f_00007-10-0 loss: 0.559790  [   32/  179]
train() client id: f_00007-10-1 loss: 0.500558  [   64/  179]
train() client id: f_00007-10-2 loss: 0.515908  [   96/  179]
train() client id: f_00007-10-3 loss: 0.545477  [  128/  179]
train() client id: f_00007-10-4 loss: 0.484677  [  160/  179]
train() client id: f_00007-11-0 loss: 0.670275  [   32/  179]
train() client id: f_00007-11-1 loss: 0.605648  [   64/  179]
train() client id: f_00007-11-2 loss: 0.530602  [   96/  179]
train() client id: f_00007-11-3 loss: 0.405724  [  128/  179]
train() client id: f_00007-11-4 loss: 0.570204  [  160/  179]
train() client id: f_00007-12-0 loss: 0.623184  [   32/  179]
train() client id: f_00007-12-1 loss: 0.499113  [   64/  179]
train() client id: f_00007-12-2 loss: 0.563935  [   96/  179]
train() client id: f_00007-12-3 loss: 0.422541  [  128/  179]
train() client id: f_00007-12-4 loss: 0.522978  [  160/  179]
train() client id: f_00008-0-0 loss: 0.809468  [   32/  130]
train() client id: f_00008-0-1 loss: 0.817984  [   64/  130]
train() client id: f_00008-0-2 loss: 0.720470  [   96/  130]
train() client id: f_00008-0-3 loss: 0.803092  [  128/  130]
train() client id: f_00008-1-0 loss: 0.821742  [   32/  130]
train() client id: f_00008-1-1 loss: 0.736946  [   64/  130]
train() client id: f_00008-1-2 loss: 0.818766  [   96/  130]
train() client id: f_00008-1-3 loss: 0.705583  [  128/  130]
train() client id: f_00008-2-0 loss: 0.705847  [   32/  130]
train() client id: f_00008-2-1 loss: 0.687893  [   64/  130]
train() client id: f_00008-2-2 loss: 0.846838  [   96/  130]
train() client id: f_00008-2-3 loss: 0.901337  [  128/  130]
train() client id: f_00008-3-0 loss: 0.802804  [   32/  130]
train() client id: f_00008-3-1 loss: 0.716568  [   64/  130]
train() client id: f_00008-3-2 loss: 0.899958  [   96/  130]
train() client id: f_00008-3-3 loss: 0.724581  [  128/  130]
train() client id: f_00008-4-0 loss: 0.753130  [   32/  130]
train() client id: f_00008-4-1 loss: 0.766489  [   64/  130]
train() client id: f_00008-4-2 loss: 0.797928  [   96/  130]
train() client id: f_00008-4-3 loss: 0.815631  [  128/  130]
train() client id: f_00008-5-0 loss: 0.736348  [   32/  130]
train() client id: f_00008-5-1 loss: 0.742012  [   64/  130]
train() client id: f_00008-5-2 loss: 0.871435  [   96/  130]
train() client id: f_00008-5-3 loss: 0.758605  [  128/  130]
train() client id: f_00008-6-0 loss: 0.831436  [   32/  130]
train() client id: f_00008-6-1 loss: 0.697732  [   64/  130]
train() client id: f_00008-6-2 loss: 0.856043  [   96/  130]
train() client id: f_00008-6-3 loss: 0.741053  [  128/  130]
train() client id: f_00008-7-0 loss: 0.748667  [   32/  130]
train() client id: f_00008-7-1 loss: 0.796386  [   64/  130]
train() client id: f_00008-7-2 loss: 0.752704  [   96/  130]
train() client id: f_00008-7-3 loss: 0.828550  [  128/  130]
train() client id: f_00008-8-0 loss: 0.798108  [   32/  130]
train() client id: f_00008-8-1 loss: 0.839094  [   64/  130]
train() client id: f_00008-8-2 loss: 0.679080  [   96/  130]
train() client id: f_00008-8-3 loss: 0.784510  [  128/  130]
train() client id: f_00008-9-0 loss: 0.811908  [   32/  130]
train() client id: f_00008-9-1 loss: 0.787676  [   64/  130]
train() client id: f_00008-9-2 loss: 0.785959  [   96/  130]
train() client id: f_00008-9-3 loss: 0.732732  [  128/  130]
train() client id: f_00008-10-0 loss: 0.858499  [   32/  130]
train() client id: f_00008-10-1 loss: 0.762926  [   64/  130]
train() client id: f_00008-10-2 loss: 0.737923  [   96/  130]
train() client id: f_00008-10-3 loss: 0.758386  [  128/  130]
train() client id: f_00008-11-0 loss: 0.717602  [   32/  130]
train() client id: f_00008-11-1 loss: 0.814474  [   64/  130]
train() client id: f_00008-11-2 loss: 0.743595  [   96/  130]
train() client id: f_00008-11-3 loss: 0.793393  [  128/  130]
train() client id: f_00008-12-0 loss: 0.854232  [   32/  130]
train() client id: f_00008-12-1 loss: 0.890069  [   64/  130]
train() client id: f_00008-12-2 loss: 0.623982  [   96/  130]
train() client id: f_00008-12-3 loss: 0.752604  [  128/  130]
train() client id: f_00009-0-0 loss: 1.076973  [   32/  118]
train() client id: f_00009-0-1 loss: 1.072066  [   64/  118]
train() client id: f_00009-0-2 loss: 1.098102  [   96/  118]
train() client id: f_00009-1-0 loss: 1.072449  [   32/  118]
train() client id: f_00009-1-1 loss: 1.148408  [   64/  118]
train() client id: f_00009-1-2 loss: 0.903110  [   96/  118]
train() client id: f_00009-2-0 loss: 0.956128  [   32/  118]
train() client id: f_00009-2-1 loss: 1.096536  [   64/  118]
train() client id: f_00009-2-2 loss: 0.983638  [   96/  118]
train() client id: f_00009-3-0 loss: 0.981428  [   32/  118]
train() client id: f_00009-3-1 loss: 0.949984  [   64/  118]
train() client id: f_00009-3-2 loss: 1.047709  [   96/  118]
train() client id: f_00009-4-0 loss: 0.872707  [   32/  118]
train() client id: f_00009-4-1 loss: 0.887884  [   64/  118]
train() client id: f_00009-4-2 loss: 1.003508  [   96/  118]
train() client id: f_00009-5-0 loss: 0.975737  [   32/  118]
train() client id: f_00009-5-1 loss: 0.833433  [   64/  118]
train() client id: f_00009-5-2 loss: 0.945625  [   96/  118]
train() client id: f_00009-6-0 loss: 1.016267  [   32/  118]
train() client id: f_00009-6-1 loss: 0.839110  [   64/  118]
train() client id: f_00009-6-2 loss: 0.830639  [   96/  118]
train() client id: f_00009-7-0 loss: 0.951298  [   32/  118]
train() client id: f_00009-7-1 loss: 0.805079  [   64/  118]
train() client id: f_00009-7-2 loss: 0.963811  [   96/  118]
train() client id: f_00009-8-0 loss: 0.868721  [   32/  118]
train() client id: f_00009-8-1 loss: 0.873624  [   64/  118]
train() client id: f_00009-8-2 loss: 0.785754  [   96/  118]
train() client id: f_00009-9-0 loss: 0.880171  [   32/  118]
train() client id: f_00009-9-1 loss: 0.846567  [   64/  118]
train() client id: f_00009-9-2 loss: 0.907340  [   96/  118]
train() client id: f_00009-10-0 loss: 0.772386  [   32/  118]
train() client id: f_00009-10-1 loss: 0.934096  [   64/  118]
train() client id: f_00009-10-2 loss: 0.801132  [   96/  118]
train() client id: f_00009-11-0 loss: 0.899773  [   32/  118]
train() client id: f_00009-11-1 loss: 0.862673  [   64/  118]
train() client id: f_00009-11-2 loss: 0.842729  [   96/  118]
train() client id: f_00009-12-0 loss: 0.868645  [   32/  118]
train() client id: f_00009-12-1 loss: 0.822724  [   64/  118]
train() client id: f_00009-12-2 loss: 0.909874  [   96/  118]
At round 17 accuracy: 0.6312997347480106
At round 17 training accuracy: 0.5814889336016097
At round 17 training loss: 0.8433271095424845
update_location
xs = 8.927491 206.223621 5.882650 10.934260 -122.581990 29.769243 -5.849135 -5.143845 -145.120581 20.134486 
ys = -197.390647 7.291448 95.684448 -57.290817 -9.642386 0.794442 -66.381692 91.628436 25.881276 -632.232496 
xs mean: 0.31762002947878576
ys mean: -74.16579882624053
dists_uav = 221.456017 229.306230 138.528406 115.766125 158.490756 104.340016 120.169636 135.728514 178.128671 640.408718 
uav_gains = -109.091038 -109.652511 -103.540037 -101.589666 -105.009441 -100.461302 -101.995087 -103.317963 -106.308771 -127.097464 
uav_gains_db_mean: -106.80632804277562
dists_bs = 415.336148 416.482422 197.508202 297.540876 191.938685 268.845716 294.749277 189.213982 152.082785 830.482854 
bs_gains = -112.882252 -112.915766 -103.843443 -108.826365 -103.495610 -107.593147 -108.711736 -103.321749 -100.665301 -121.308318 
bs_gains_db_mean: -108.35636859220662
Round 18
-------------------------------
ene_coms = [0.01006436 0.01044143 0.00749522 0.00674724 0.00736936 0.00642542
 0.00687077 0.00730799 0.00648134 0.03325336]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.67487643 18.00624985  8.48359733  3.03414272 20.68787548  9.9353865
  3.77468023 12.16838526  8.86348045  8.65400566]
obj_prev = 102.28267992605012
eta_min = 1.541876397797246e-11	eta_max = 0.7439364156947647
af = 21.498465320302763	bf = 2.2557615892324097	zeta = 23.64831185233304	eta = 0.9090909090909091
af = 21.498465320302763	bf = 2.2557615892324097	zeta = 47.35262602870259	eta = 0.4540078792519672
af = 21.498465320302763	bf = 2.2557615892324097	zeta = 35.22363166042938	eta = 0.6103421000865841
af = 21.498465320302763	bf = 2.2557615892324097	zeta = 33.02979495303369	eta = 0.6508809803655228
af = 21.498465320302763	bf = 2.2557615892324097	zeta = 32.90515329719392	eta = 0.6533464568948264
af = 21.498465320302763	bf = 2.2557615892324097	zeta = 32.904710616299596	eta = 0.6533552466391647
af = 21.498465320302763	bf = 2.2557615892324097	zeta = 32.90471061068717	eta = 0.653355246750605
eta = 0.653355246750605
ene_coms = [0.01006436 0.01044143 0.00749522 0.00674724 0.00736936 0.00642542
 0.00687077 0.00730799 0.00648134 0.03325336]
ene_comp = [0.03512232 0.07386837 0.03456482 0.01198619 0.08529706 0.04069729
 0.01505242 0.04989599 0.03623731 0.03289233]
ene_total = [2.8699844  5.35484875 2.67139973 1.18983413 5.88561062 2.99294944
 1.39242845 3.63325087 2.71323033 4.20117389]
ti_comp = [0.35725829 0.35348754 0.38294963 0.39042946 0.38420827 0.39364769
 0.38919419 0.38482199 0.3930885  0.12536831]
ti_coms = [0.10064358 0.10441433 0.07495224 0.06747241 0.0736936  0.06425418
 0.06870768 0.07307988 0.06481337 0.33253356]
t_total = [29.08315659 29.08315659 29.08315659 29.08315659 29.08315659 29.08315659
 29.08315659 29.08315659 29.08315659 29.08315659]
ene_coms = [0.01006436 0.01044143 0.00749522 0.00674724 0.00736936 0.00642542
 0.00687077 0.00730799 0.00648134 0.03325336]
ene_comp = [2.12160715e-05 2.01607821e-04 1.75994753e-05 7.06054439e-07
 2.62753703e-04 2.71869902e-05 1.40723476e-06 5.24272762e-05
 1.92471913e-05 1.41510422e-04]
ene_total = [0.64057462 0.67598156 0.47716909 0.42858879 0.48474571 0.40983044
 0.43647898 0.46748902 0.4128778  2.12103986]
optimize_network iter = 0 obj = 6.554775884075678
eta = 0.653355246750605
freqs = [4.91553601e+07 1.04485114e+08 4.51297250e+07 1.53500055e+07
 1.11003681e+08 5.16925322e+07 1.93379337e+07 6.48299671e+07
 4.60930742e+07 1.31182795e+08]
eta_min = 0.6533552467506075	eta_max = 0.6533552467506032
af = 0.03857085923453088	bf = 2.2557615892324097	zeta = 0.04242794515798397	eta = 0.9090909090909091
af = 0.03857085923453088	bf = 2.2557615892324097	zeta = 24.853815632373852	eta = 0.0015519089625936392
af = 0.03857085923453088	bf = 2.2557615892324097	zeta = 2.5091422626319018	eta = 0.015372129276589102
af = 0.03857085923453088	bf = 2.2557615892324097	zeta = 2.454534024045863	eta = 0.015714126940865818
af = 0.03857085923453088	bf = 2.2557615892324097	zeta = 2.4545246066036603	eta = 0.015714187232329927
eta = 0.015714187232329927
ene_coms = [0.01006436 0.01044143 0.00749522 0.00674724 0.00736936 0.00642542
 0.00687077 0.00730799 0.00648134 0.03325336]
ene_comp = [2.07019052e-04 1.96721905e-03 1.71729564e-04 6.88943383e-06
 2.56385932e-03 2.65281202e-04 1.37313077e-05 5.11567140e-04
 1.87807404e-04 1.38080952e-03]
ene_total = [0.229753   0.27756016 0.17149654 0.15107827 0.22218901 0.14965942
 0.15399438 0.17490996 0.14917728 0.7747066 ]
ti_comp = [0.35725829 0.35348754 0.38294963 0.39042946 0.38420827 0.39364769
 0.38919419 0.38482199 0.3930885  0.12536831]
ti_coms = [0.10064358 0.10441433 0.07495224 0.06747241 0.0736936  0.06425418
 0.06870768 0.07307988 0.06481337 0.33253356]
t_total = [29.08315659 29.08315659 29.08315659 29.08315659 29.08315659 29.08315659
 29.08315659 29.08315659 29.08315659 29.08315659]
ene_coms = [0.01006436 0.01044143 0.00749522 0.00674724 0.00736936 0.00642542
 0.00687077 0.00730799 0.00648134 0.03325336]
ene_comp = [2.12160715e-05 2.01607821e-04 1.75994753e-05 7.06054439e-07
 2.62753703e-04 2.71869902e-05 1.40723476e-06 5.24272762e-05
 1.92471913e-05 1.41510422e-04]
ene_total = [0.64057462 0.67598156 0.47716909 0.42858879 0.48474571 0.40983044
 0.43647898 0.46748902 0.4128778  2.12103986]
optimize_network iter = 1 obj = 6.554775884075727
eta = 0.6533552467506075
freqs = [4.91553601e+07 1.04485114e+08 4.51297250e+07 1.53500055e+07
 1.11003681e+08 5.16925322e+07 1.93379337e+07 6.48299671e+07
 4.60930742e+07 1.31182795e+08]
Done!
ene_coms = [0.01006436 0.01044143 0.00749522 0.00674724 0.00736936 0.00642542
 0.00687077 0.00730799 0.00648134 0.03325336]
ene_comp = [1.97890828e-05 1.88047720e-04 1.64157382e-05 6.58565360e-07
 2.45080942e-04 2.53583987e-05 1.31258443e-06 4.89010282e-05
 1.79526291e-05 1.31992460e-04]
ene_total = [0.01008415 0.01062948 0.00751164 0.0067479  0.00761444 0.00645078
 0.00687208 0.00735689 0.00649929 0.03338535]
At round 18 energy consumption: 0.10315199161333799
At round 18 eta: 0.6533552467506075
At round 18 a_n: 22.016777610806663
At round 18 local rounds: 13.937428646368062
At round 18 global rounds: 63.513950245676334
gradient difference: 0.39596760272979736
train() client id: f_00000-0-0 loss: 1.442906  [   32/  126]
train() client id: f_00000-0-1 loss: 1.493902  [   64/  126]
train() client id: f_00000-0-2 loss: 1.486606  [   96/  126]
train() client id: f_00000-1-0 loss: 1.570692  [   32/  126]
train() client id: f_00000-1-1 loss: 1.284848  [   64/  126]
train() client id: f_00000-1-2 loss: 1.348273  [   96/  126]
train() client id: f_00000-2-0 loss: 1.310743  [   32/  126]
train() client id: f_00000-2-1 loss: 1.084221  [   64/  126]
train() client id: f_00000-2-2 loss: 1.201519  [   96/  126]
train() client id: f_00000-3-0 loss: 1.082277  [   32/  126]
train() client id: f_00000-3-1 loss: 1.202885  [   64/  126]
train() client id: f_00000-3-2 loss: 1.087580  [   96/  126]
train() client id: f_00000-4-0 loss: 1.147541  [   32/  126]
train() client id: f_00000-4-1 loss: 1.034782  [   64/  126]
train() client id: f_00000-4-2 loss: 0.959108  [   96/  126]
train() client id: f_00000-5-0 loss: 1.025341  [   32/  126]
train() client id: f_00000-5-1 loss: 0.961185  [   64/  126]
train() client id: f_00000-5-2 loss: 0.973637  [   96/  126]
train() client id: f_00000-6-0 loss: 0.877667  [   32/  126]
train() client id: f_00000-6-1 loss: 1.011883  [   64/  126]
train() client id: f_00000-6-2 loss: 0.947723  [   96/  126]
train() client id: f_00000-7-0 loss: 0.983904  [   32/  126]
train() client id: f_00000-7-1 loss: 0.868447  [   64/  126]
train() client id: f_00000-7-2 loss: 0.952022  [   96/  126]
train() client id: f_00000-8-0 loss: 0.865673  [   32/  126]
train() client id: f_00000-8-1 loss: 0.913015  [   64/  126]
train() client id: f_00000-8-2 loss: 0.882666  [   96/  126]
train() client id: f_00000-9-0 loss: 0.849434  [   32/  126]
train() client id: f_00000-9-1 loss: 0.940955  [   64/  126]
train() client id: f_00000-9-2 loss: 0.813077  [   96/  126]
train() client id: f_00000-10-0 loss: 0.955710  [   32/  126]
train() client id: f_00000-10-1 loss: 0.904473  [   64/  126]
train() client id: f_00000-10-2 loss: 0.814457  [   96/  126]
train() client id: f_00000-11-0 loss: 0.914400  [   32/  126]
train() client id: f_00000-11-1 loss: 0.836773  [   64/  126]
train() client id: f_00000-11-2 loss: 0.908247  [   96/  126]
train() client id: f_00000-12-0 loss: 0.857386  [   32/  126]
train() client id: f_00000-12-1 loss: 0.857355  [   64/  126]
train() client id: f_00000-12-2 loss: 0.840018  [   96/  126]
train() client id: f_00001-0-0 loss: 0.459468  [   32/  265]
train() client id: f_00001-0-1 loss: 0.380839  [   64/  265]
train() client id: f_00001-0-2 loss: 0.495868  [   96/  265]
train() client id: f_00001-0-3 loss: 0.386602  [  128/  265]
train() client id: f_00001-0-4 loss: 0.486970  [  160/  265]
train() client id: f_00001-0-5 loss: 0.437876  [  192/  265]
train() client id: f_00001-0-6 loss: 0.506123  [  224/  265]
train() client id: f_00001-0-7 loss: 0.420188  [  256/  265]
train() client id: f_00001-1-0 loss: 0.415601  [   32/  265]
train() client id: f_00001-1-1 loss: 0.508349  [   64/  265]
train() client id: f_00001-1-2 loss: 0.472357  [   96/  265]
train() client id: f_00001-1-3 loss: 0.337601  [  128/  265]
train() client id: f_00001-1-4 loss: 0.374733  [  160/  265]
train() client id: f_00001-1-5 loss: 0.545796  [  192/  265]
train() client id: f_00001-1-6 loss: 0.400589  [  224/  265]
train() client id: f_00001-1-7 loss: 0.432017  [  256/  265]
train() client id: f_00001-2-0 loss: 0.513878  [   32/  265]
train() client id: f_00001-2-1 loss: 0.510975  [   64/  265]
train() client id: f_00001-2-2 loss: 0.354595  [   96/  265]
train() client id: f_00001-2-3 loss: 0.403936  [  128/  265]
train() client id: f_00001-2-4 loss: 0.366613  [  160/  265]
train() client id: f_00001-2-5 loss: 0.378489  [  192/  265]
train() client id: f_00001-2-6 loss: 0.385169  [  224/  265]
train() client id: f_00001-2-7 loss: 0.510155  [  256/  265]
train() client id: f_00001-3-0 loss: 0.490543  [   32/  265]
train() client id: f_00001-3-1 loss: 0.415549  [   64/  265]
train() client id: f_00001-3-2 loss: 0.420616  [   96/  265]
train() client id: f_00001-3-3 loss: 0.433473  [  128/  265]
train() client id: f_00001-3-4 loss: 0.444015  [  160/  265]
train() client id: f_00001-3-5 loss: 0.344243  [  192/  265]
train() client id: f_00001-3-6 loss: 0.403721  [  224/  265]
train() client id: f_00001-3-7 loss: 0.389748  [  256/  265]
train() client id: f_00001-4-0 loss: 0.433714  [   32/  265]
train() client id: f_00001-4-1 loss: 0.441118  [   64/  265]
train() client id: f_00001-4-2 loss: 0.344714  [   96/  265]
train() client id: f_00001-4-3 loss: 0.539839  [  128/  265]
train() client id: f_00001-4-4 loss: 0.325802  [  160/  265]
train() client id: f_00001-4-5 loss: 0.408951  [  192/  265]
train() client id: f_00001-4-6 loss: 0.390801  [  224/  265]
train() client id: f_00001-4-7 loss: 0.460876  [  256/  265]
train() client id: f_00001-5-0 loss: 0.324074  [   32/  265]
train() client id: f_00001-5-1 loss: 0.327088  [   64/  265]
train() client id: f_00001-5-2 loss: 0.514829  [   96/  265]
train() client id: f_00001-5-3 loss: 0.381847  [  128/  265]
train() client id: f_00001-5-4 loss: 0.404887  [  160/  265]
train() client id: f_00001-5-5 loss: 0.484302  [  192/  265]
train() client id: f_00001-5-6 loss: 0.433382  [  224/  265]
train() client id: f_00001-5-7 loss: 0.392125  [  256/  265]
train() client id: f_00001-6-0 loss: 0.411500  [   32/  265]
train() client id: f_00001-6-1 loss: 0.407909  [   64/  265]
train() client id: f_00001-6-2 loss: 0.421849  [   96/  265]
train() client id: f_00001-6-3 loss: 0.318221  [  128/  265]
train() client id: f_00001-6-4 loss: 0.393755  [  160/  265]
train() client id: f_00001-6-5 loss: 0.450802  [  192/  265]
train() client id: f_00001-6-6 loss: 0.398441  [  224/  265]
train() client id: f_00001-6-7 loss: 0.434493  [  256/  265]
train() client id: f_00001-7-0 loss: 0.440900  [   32/  265]
train() client id: f_00001-7-1 loss: 0.329181  [   64/  265]
train() client id: f_00001-7-2 loss: 0.442321  [   96/  265]
train() client id: f_00001-7-3 loss: 0.307386  [  128/  265]
train() client id: f_00001-7-4 loss: 0.431613  [  160/  265]
train() client id: f_00001-7-5 loss: 0.317507  [  192/  265]
train() client id: f_00001-7-6 loss: 0.488997  [  224/  265]
train() client id: f_00001-7-7 loss: 0.523403  [  256/  265]
train() client id: f_00001-8-0 loss: 0.353938  [   32/  265]
train() client id: f_00001-8-1 loss: 0.346752  [   64/  265]
train() client id: f_00001-8-2 loss: 0.433956  [   96/  265]
train() client id: f_00001-8-3 loss: 0.309256  [  128/  265]
train() client id: f_00001-8-4 loss: 0.396195  [  160/  265]
train() client id: f_00001-8-5 loss: 0.437171  [  192/  265]
train() client id: f_00001-8-6 loss: 0.502547  [  224/  265]
train() client id: f_00001-8-7 loss: 0.497746  [  256/  265]
train() client id: f_00001-9-0 loss: 0.313289  [   32/  265]
train() client id: f_00001-9-1 loss: 0.369146  [   64/  265]
train() client id: f_00001-9-2 loss: 0.447401  [   96/  265]
train() client id: f_00001-9-3 loss: 0.318186  [  128/  265]
train() client id: f_00001-9-4 loss: 0.473824  [  160/  265]
train() client id: f_00001-9-5 loss: 0.329084  [  192/  265]
train() client id: f_00001-9-6 loss: 0.406195  [  224/  265]
train() client id: f_00001-9-7 loss: 0.493321  [  256/  265]
train() client id: f_00001-10-0 loss: 0.419921  [   32/  265]
train() client id: f_00001-10-1 loss: 0.477867  [   64/  265]
train() client id: f_00001-10-2 loss: 0.313324  [   96/  265]
train() client id: f_00001-10-3 loss: 0.371165  [  128/  265]
train() client id: f_00001-10-4 loss: 0.402203  [  160/  265]
train() client id: f_00001-10-5 loss: 0.420336  [  192/  265]
train() client id: f_00001-10-6 loss: 0.430326  [  224/  265]
train() client id: f_00001-10-7 loss: 0.439631  [  256/  265]
train() client id: f_00001-11-0 loss: 0.327821  [   32/  265]
train() client id: f_00001-11-1 loss: 0.592282  [   64/  265]
train() client id: f_00001-11-2 loss: 0.385152  [   96/  265]
train() client id: f_00001-11-3 loss: 0.314705  [  128/  265]
train() client id: f_00001-11-4 loss: 0.306203  [  160/  265]
train() client id: f_00001-11-5 loss: 0.504171  [  192/  265]
train() client id: f_00001-11-6 loss: 0.440932  [  224/  265]
train() client id: f_00001-11-7 loss: 0.299597  [  256/  265]
train() client id: f_00001-12-0 loss: 0.368161  [   32/  265]
train() client id: f_00001-12-1 loss: 0.306699  [   64/  265]
train() client id: f_00001-12-2 loss: 0.529820  [   96/  265]
train() client id: f_00001-12-3 loss: 0.411044  [  128/  265]
train() client id: f_00001-12-4 loss: 0.370293  [  160/  265]
train() client id: f_00001-12-5 loss: 0.467251  [  192/  265]
train() client id: f_00001-12-6 loss: 0.417101  [  224/  265]
train() client id: f_00001-12-7 loss: 0.402209  [  256/  265]
train() client id: f_00002-0-0 loss: 1.200307  [   32/  124]
train() client id: f_00002-0-1 loss: 1.435148  [   64/  124]
train() client id: f_00002-0-2 loss: 1.495388  [   96/  124]
train() client id: f_00002-1-0 loss: 1.452746  [   32/  124]
train() client id: f_00002-1-1 loss: 1.191737  [   64/  124]
train() client id: f_00002-1-2 loss: 1.406047  [   96/  124]
train() client id: f_00002-2-0 loss: 1.417262  [   32/  124]
train() client id: f_00002-2-1 loss: 1.280233  [   64/  124]
train() client id: f_00002-2-2 loss: 1.201109  [   96/  124]
train() client id: f_00002-3-0 loss: 1.374209  [   32/  124]
train() client id: f_00002-3-1 loss: 1.188878  [   64/  124]
train() client id: f_00002-3-2 loss: 1.194289  [   96/  124]
train() client id: f_00002-4-0 loss: 1.211945  [   32/  124]
train() client id: f_00002-4-1 loss: 1.314801  [   64/  124]
train() client id: f_00002-4-2 loss: 1.108207  [   96/  124]
train() client id: f_00002-5-0 loss: 1.318712  [   32/  124]
train() client id: f_00002-5-1 loss: 1.115462  [   64/  124]
train() client id: f_00002-5-2 loss: 1.122323  [   96/  124]
train() client id: f_00002-6-0 loss: 1.171676  [   32/  124]
train() client id: f_00002-6-1 loss: 1.082280  [   64/  124]
train() client id: f_00002-6-2 loss: 1.146874  [   96/  124]
train() client id: f_00002-7-0 loss: 1.231666  [   32/  124]
train() client id: f_00002-7-1 loss: 1.233210  [   64/  124]
train() client id: f_00002-7-2 loss: 1.109827  [   96/  124]
train() client id: f_00002-8-0 loss: 1.074763  [   32/  124]
train() client id: f_00002-8-1 loss: 1.150307  [   64/  124]
train() client id: f_00002-8-2 loss: 1.142610  [   96/  124]
train() client id: f_00002-9-0 loss: 1.057641  [   32/  124]
train() client id: f_00002-9-1 loss: 1.155131  [   64/  124]
train() client id: f_00002-9-2 loss: 1.170045  [   96/  124]
train() client id: f_00002-10-0 loss: 1.163385  [   32/  124]
train() client id: f_00002-10-1 loss: 1.115456  [   64/  124]
train() client id: f_00002-10-2 loss: 1.049614  [   96/  124]
train() client id: f_00002-11-0 loss: 1.030264  [   32/  124]
train() client id: f_00002-11-1 loss: 1.180558  [   64/  124]
train() client id: f_00002-11-2 loss: 1.244964  [   96/  124]
train() client id: f_00002-12-0 loss: 1.034943  [   32/  124]
train() client id: f_00002-12-1 loss: 1.277519  [   64/  124]
train() client id: f_00002-12-2 loss: 1.032868  [   96/  124]
train() client id: f_00003-0-0 loss: 0.933766  [   32/   43]
train() client id: f_00003-1-0 loss: 0.735671  [   32/   43]
train() client id: f_00003-2-0 loss: 0.850217  [   32/   43]
train() client id: f_00003-3-0 loss: 0.819564  [   32/   43]
train() client id: f_00003-4-0 loss: 0.856767  [   32/   43]
train() client id: f_00003-5-0 loss: 0.960138  [   32/   43]
train() client id: f_00003-6-0 loss: 0.781549  [   32/   43]
train() client id: f_00003-7-0 loss: 0.827667  [   32/   43]
train() client id: f_00003-8-0 loss: 0.861278  [   32/   43]
train() client id: f_00003-9-0 loss: 0.773080  [   32/   43]
train() client id: f_00003-10-0 loss: 0.849171  [   32/   43]
train() client id: f_00003-11-0 loss: 0.777045  [   32/   43]
train() client id: f_00003-12-0 loss: 0.798238  [   32/   43]
train() client id: f_00004-0-0 loss: 0.921163  [   32/  306]
train() client id: f_00004-0-1 loss: 0.848761  [   64/  306]
train() client id: f_00004-0-2 loss: 0.865007  [   96/  306]
train() client id: f_00004-0-3 loss: 1.015714  [  128/  306]
train() client id: f_00004-0-4 loss: 0.991195  [  160/  306]
train() client id: f_00004-0-5 loss: 0.997522  [  192/  306]
train() client id: f_00004-0-6 loss: 0.868626  [  224/  306]
train() client id: f_00004-0-7 loss: 0.898467  [  256/  306]
train() client id: f_00004-0-8 loss: 1.123370  [  288/  306]
train() client id: f_00004-1-0 loss: 0.792866  [   32/  306]
train() client id: f_00004-1-1 loss: 0.900909  [   64/  306]
train() client id: f_00004-1-2 loss: 0.985939  [   96/  306]
train() client id: f_00004-1-3 loss: 1.000042  [  128/  306]
train() client id: f_00004-1-4 loss: 0.998058  [  160/  306]
train() client id: f_00004-1-5 loss: 0.938369  [  192/  306]
train() client id: f_00004-1-6 loss: 0.867706  [  224/  306]
train() client id: f_00004-1-7 loss: 1.017119  [  256/  306]
train() client id: f_00004-1-8 loss: 0.965881  [  288/  306]
train() client id: f_00004-2-0 loss: 0.996846  [   32/  306]
train() client id: f_00004-2-1 loss: 0.878407  [   64/  306]
train() client id: f_00004-2-2 loss: 1.002108  [   96/  306]
train() client id: f_00004-2-3 loss: 1.012908  [  128/  306]
train() client id: f_00004-2-4 loss: 0.908545  [  160/  306]
train() client id: f_00004-2-5 loss: 0.868831  [  192/  306]
train() client id: f_00004-2-6 loss: 1.094571  [  224/  306]
train() client id: f_00004-2-7 loss: 0.818386  [  256/  306]
train() client id: f_00004-2-8 loss: 0.907315  [  288/  306]
train() client id: f_00004-3-0 loss: 0.891356  [   32/  306]
train() client id: f_00004-3-1 loss: 0.886152  [   64/  306]
train() client id: f_00004-3-2 loss: 0.953914  [   96/  306]
train() client id: f_00004-3-3 loss: 1.092459  [  128/  306]
train() client id: f_00004-3-4 loss: 0.970230  [  160/  306]
train() client id: f_00004-3-5 loss: 0.952100  [  192/  306]
train() client id: f_00004-3-6 loss: 0.992615  [  224/  306]
train() client id: f_00004-3-7 loss: 0.875171  [  256/  306]
train() client id: f_00004-3-8 loss: 0.875970  [  288/  306]
train() client id: f_00004-4-0 loss: 0.943020  [   32/  306]
train() client id: f_00004-4-1 loss: 0.929680  [   64/  306]
train() client id: f_00004-4-2 loss: 1.045174  [   96/  306]
train() client id: f_00004-4-3 loss: 0.916308  [  128/  306]
train() client id: f_00004-4-4 loss: 0.847061  [  160/  306]
train() client id: f_00004-4-5 loss: 0.891912  [  192/  306]
train() client id: f_00004-4-6 loss: 0.958192  [  224/  306]
train() client id: f_00004-4-7 loss: 0.971054  [  256/  306]
train() client id: f_00004-4-8 loss: 0.951974  [  288/  306]
train() client id: f_00004-5-0 loss: 1.027976  [   32/  306]
train() client id: f_00004-5-1 loss: 0.984940  [   64/  306]
train() client id: f_00004-5-2 loss: 0.887990  [   96/  306]
train() client id: f_00004-5-3 loss: 0.949647  [  128/  306]
train() client id: f_00004-5-4 loss: 0.896983  [  160/  306]
train() client id: f_00004-5-5 loss: 0.896060  [  192/  306]
train() client id: f_00004-5-6 loss: 0.779994  [  224/  306]
train() client id: f_00004-5-7 loss: 1.027173  [  256/  306]
train() client id: f_00004-5-8 loss: 0.961546  [  288/  306]
train() client id: f_00004-6-0 loss: 1.019794  [   32/  306]
train() client id: f_00004-6-1 loss: 0.994711  [   64/  306]
train() client id: f_00004-6-2 loss: 0.854676  [   96/  306]
train() client id: f_00004-6-3 loss: 0.892176  [  128/  306]
train() client id: f_00004-6-4 loss: 0.897043  [  160/  306]
train() client id: f_00004-6-5 loss: 1.019746  [  192/  306]
train() client id: f_00004-6-6 loss: 0.950713  [  224/  306]
train() client id: f_00004-6-7 loss: 0.888538  [  256/  306]
train() client id: f_00004-6-8 loss: 0.974859  [  288/  306]
train() client id: f_00004-7-0 loss: 0.992492  [   32/  306]
train() client id: f_00004-7-1 loss: 0.919480  [   64/  306]
train() client id: f_00004-7-2 loss: 0.912234  [   96/  306]
train() client id: f_00004-7-3 loss: 0.929352  [  128/  306]
train() client id: f_00004-7-4 loss: 0.856219  [  160/  306]
train() client id: f_00004-7-5 loss: 0.957901  [  192/  306]
train() client id: f_00004-7-6 loss: 0.908886  [  224/  306]
train() client id: f_00004-7-7 loss: 1.057837  [  256/  306]
train() client id: f_00004-7-8 loss: 0.974360  [  288/  306]
train() client id: f_00004-8-0 loss: 0.851579  [   32/  306]
train() client id: f_00004-8-1 loss: 1.015022  [   64/  306]
train() client id: f_00004-8-2 loss: 0.927297  [   96/  306]
train() client id: f_00004-8-3 loss: 0.918100  [  128/  306]
train() client id: f_00004-8-4 loss: 0.906816  [  160/  306]
train() client id: f_00004-8-5 loss: 0.916424  [  192/  306]
train() client id: f_00004-8-6 loss: 0.967564  [  224/  306]
train() client id: f_00004-8-7 loss: 1.009344  [  256/  306]
train() client id: f_00004-8-8 loss: 0.989539  [  288/  306]
train() client id: f_00004-9-0 loss: 0.854151  [   32/  306]
train() client id: f_00004-9-1 loss: 0.861216  [   64/  306]
train() client id: f_00004-9-2 loss: 0.930861  [   96/  306]
train() client id: f_00004-9-3 loss: 0.915678  [  128/  306]
train() client id: f_00004-9-4 loss: 0.996105  [  160/  306]
train() client id: f_00004-9-5 loss: 0.948879  [  192/  306]
train() client id: f_00004-9-6 loss: 0.975661  [  224/  306]
train() client id: f_00004-9-7 loss: 1.102519  [  256/  306]
train() client id: f_00004-9-8 loss: 0.954799  [  288/  306]
train() client id: f_00004-10-0 loss: 1.004450  [   32/  306]
train() client id: f_00004-10-1 loss: 0.941059  [   64/  306]
train() client id: f_00004-10-2 loss: 0.951322  [   96/  306]
train() client id: f_00004-10-3 loss: 0.875592  [  128/  306]
train() client id: f_00004-10-4 loss: 0.893332  [  160/  306]
train() client id: f_00004-10-5 loss: 0.988930  [  192/  306]
train() client id: f_00004-10-6 loss: 0.966466  [  224/  306]
train() client id: f_00004-10-7 loss: 0.940561  [  256/  306]
train() client id: f_00004-10-8 loss: 0.953063  [  288/  306]
train() client id: f_00004-11-0 loss: 0.997084  [   32/  306]
train() client id: f_00004-11-1 loss: 0.861910  [   64/  306]
train() client id: f_00004-11-2 loss: 0.910468  [   96/  306]
train() client id: f_00004-11-3 loss: 0.926493  [  128/  306]
train() client id: f_00004-11-4 loss: 0.936600  [  160/  306]
train() client id: f_00004-11-5 loss: 0.900383  [  192/  306]
train() client id: f_00004-11-6 loss: 0.979900  [  224/  306]
train() client id: f_00004-11-7 loss: 1.015187  [  256/  306]
train() client id: f_00004-11-8 loss: 0.933932  [  288/  306]
train() client id: f_00004-12-0 loss: 0.963179  [   32/  306]
train() client id: f_00004-12-1 loss: 0.861655  [   64/  306]
train() client id: f_00004-12-2 loss: 0.993819  [   96/  306]
train() client id: f_00004-12-3 loss: 0.972165  [  128/  306]
train() client id: f_00004-12-4 loss: 0.913475  [  160/  306]
train() client id: f_00004-12-5 loss: 0.971738  [  192/  306]
train() client id: f_00004-12-6 loss: 0.908212  [  224/  306]
train() client id: f_00004-12-7 loss: 1.051528  [  256/  306]
train() client id: f_00004-12-8 loss: 0.932671  [  288/  306]
train() client id: f_00005-0-0 loss: 0.799987  [   32/  146]
train() client id: f_00005-0-1 loss: 0.720709  [   64/  146]
train() client id: f_00005-0-2 loss: 0.852929  [   96/  146]
train() client id: f_00005-0-3 loss: 0.694635  [  128/  146]
train() client id: f_00005-1-0 loss: 0.584460  [   32/  146]
train() client id: f_00005-1-1 loss: 0.680615  [   64/  146]
train() client id: f_00005-1-2 loss: 1.070366  [   96/  146]
train() client id: f_00005-1-3 loss: 0.845961  [  128/  146]
train() client id: f_00005-2-0 loss: 0.784586  [   32/  146]
train() client id: f_00005-2-1 loss: 1.017393  [   64/  146]
train() client id: f_00005-2-2 loss: 0.722117  [   96/  146]
train() client id: f_00005-2-3 loss: 0.761624  [  128/  146]
train() client id: f_00005-3-0 loss: 0.748010  [   32/  146]
train() client id: f_00005-3-1 loss: 0.804844  [   64/  146]
train() client id: f_00005-3-2 loss: 0.752347  [   96/  146]
train() client id: f_00005-3-3 loss: 0.948885  [  128/  146]
train() client id: f_00005-4-0 loss: 0.779081  [   32/  146]
train() client id: f_00005-4-1 loss: 0.719603  [   64/  146]
train() client id: f_00005-4-2 loss: 0.980479  [   96/  146]
train() client id: f_00005-4-3 loss: 0.695106  [  128/  146]
train() client id: f_00005-5-0 loss: 0.871634  [   32/  146]
train() client id: f_00005-5-1 loss: 0.605809  [   64/  146]
train() client id: f_00005-5-2 loss: 0.824575  [   96/  146]
train() client id: f_00005-5-3 loss: 0.860888  [  128/  146]
train() client id: f_00005-6-0 loss: 0.893706  [   32/  146]
train() client id: f_00005-6-1 loss: 0.670509  [   64/  146]
train() client id: f_00005-6-2 loss: 0.653442  [   96/  146]
train() client id: f_00005-6-3 loss: 0.987721  [  128/  146]
train() client id: f_00005-7-0 loss: 0.817791  [   32/  146]
train() client id: f_00005-7-1 loss: 0.765972  [   64/  146]
train() client id: f_00005-7-2 loss: 0.798511  [   96/  146]
train() client id: f_00005-7-3 loss: 0.775028  [  128/  146]
train() client id: f_00005-8-0 loss: 0.770052  [   32/  146]
train() client id: f_00005-8-1 loss: 0.644256  [   64/  146]
train() client id: f_00005-8-2 loss: 1.009104  [   96/  146]
train() client id: f_00005-8-3 loss: 0.748036  [  128/  146]
train() client id: f_00005-9-0 loss: 0.782906  [   32/  146]
train() client id: f_00005-9-1 loss: 0.826598  [   64/  146]
train() client id: f_00005-9-2 loss: 0.628508  [   96/  146]
train() client id: f_00005-9-3 loss: 0.971500  [  128/  146]
train() client id: f_00005-10-0 loss: 0.689125  [   32/  146]
train() client id: f_00005-10-1 loss: 0.672342  [   64/  146]
train() client id: f_00005-10-2 loss: 0.635832  [   96/  146]
train() client id: f_00005-10-3 loss: 1.037528  [  128/  146]
train() client id: f_00005-11-0 loss: 0.830766  [   32/  146]
train() client id: f_00005-11-1 loss: 0.776658  [   64/  146]
train() client id: f_00005-11-2 loss: 0.718070  [   96/  146]
train() client id: f_00005-11-3 loss: 0.808287  [  128/  146]
train() client id: f_00005-12-0 loss: 0.865808  [   32/  146]
train() client id: f_00005-12-1 loss: 0.840577  [   64/  146]
train() client id: f_00005-12-2 loss: 0.781224  [   96/  146]
train() client id: f_00005-12-3 loss: 0.844415  [  128/  146]
train() client id: f_00006-0-0 loss: 0.540141  [   32/   54]
train() client id: f_00006-1-0 loss: 0.551910  [   32/   54]
train() client id: f_00006-2-0 loss: 0.593935  [   32/   54]
train() client id: f_00006-3-0 loss: 0.596793  [   32/   54]
train() client id: f_00006-4-0 loss: 0.636898  [   32/   54]
train() client id: f_00006-5-0 loss: 0.597413  [   32/   54]
train() client id: f_00006-6-0 loss: 0.647875  [   32/   54]
train() client id: f_00006-7-0 loss: 0.656286  [   32/   54]
train() client id: f_00006-8-0 loss: 0.629932  [   32/   54]
train() client id: f_00006-9-0 loss: 0.662148  [   32/   54]
train() client id: f_00006-10-0 loss: 0.572491  [   32/   54]
train() client id: f_00006-11-0 loss: 0.626829  [   32/   54]
train() client id: f_00006-12-0 loss: 0.604869  [   32/   54]
train() client id: f_00007-0-0 loss: 0.764463  [   32/  179]
train() client id: f_00007-0-1 loss: 0.589231  [   64/  179]
train() client id: f_00007-0-2 loss: 0.662379  [   96/  179]
train() client id: f_00007-0-3 loss: 0.572836  [  128/  179]
train() client id: f_00007-0-4 loss: 0.706347  [  160/  179]
train() client id: f_00007-1-0 loss: 0.597522  [   32/  179]
train() client id: f_00007-1-1 loss: 0.505080  [   64/  179]
train() client id: f_00007-1-2 loss: 0.487085  [   96/  179]
train() client id: f_00007-1-3 loss: 0.762074  [  128/  179]
train() client id: f_00007-1-4 loss: 0.823948  [  160/  179]
train() client id: f_00007-2-0 loss: 0.541190  [   32/  179]
train() client id: f_00007-2-1 loss: 0.645374  [   64/  179]
train() client id: f_00007-2-2 loss: 0.683954  [   96/  179]
train() client id: f_00007-2-3 loss: 0.493883  [  128/  179]
train() client id: f_00007-2-4 loss: 0.686302  [  160/  179]
train() client id: f_00007-3-0 loss: 0.621421  [   32/  179]
train() client id: f_00007-3-1 loss: 0.594269  [   64/  179]
train() client id: f_00007-3-2 loss: 0.512230  [   96/  179]
train() client id: f_00007-3-3 loss: 0.783646  [  128/  179]
train() client id: f_00007-3-4 loss: 0.532503  [  160/  179]
train() client id: f_00007-4-0 loss: 0.542313  [   32/  179]
train() client id: f_00007-4-1 loss: 0.600200  [   64/  179]
train() client id: f_00007-4-2 loss: 0.762817  [   96/  179]
train() client id: f_00007-4-3 loss: 0.533576  [  128/  179]
train() client id: f_00007-4-4 loss: 0.470112  [  160/  179]
train() client id: f_00007-5-0 loss: 0.625267  [   32/  179]
train() client id: f_00007-5-1 loss: 0.790848  [   64/  179]
train() client id: f_00007-5-2 loss: 0.630670  [   96/  179]
train() client id: f_00007-5-3 loss: 0.471441  [  128/  179]
train() client id: f_00007-5-4 loss: 0.543594  [  160/  179]
train() client id: f_00007-6-0 loss: 0.636156  [   32/  179]
train() client id: f_00007-6-1 loss: 0.487327  [   64/  179]
train() client id: f_00007-6-2 loss: 0.481681  [   96/  179]
train() client id: f_00007-6-3 loss: 0.511927  [  128/  179]
train() client id: f_00007-6-4 loss: 0.794207  [  160/  179]
train() client id: f_00007-7-0 loss: 0.496758  [   32/  179]
train() client id: f_00007-7-1 loss: 0.511163  [   64/  179]
train() client id: f_00007-7-2 loss: 0.600807  [   96/  179]
train() client id: f_00007-7-3 loss: 0.648781  [  128/  179]
train() client id: f_00007-7-4 loss: 0.720789  [  160/  179]
train() client id: f_00007-8-0 loss: 0.603446  [   32/  179]
train() client id: f_00007-8-1 loss: 0.557101  [   64/  179]
train() client id: f_00007-8-2 loss: 0.741775  [   96/  179]
train() client id: f_00007-8-3 loss: 0.544272  [  128/  179]
train() client id: f_00007-8-4 loss: 0.560336  [  160/  179]
train() client id: f_00007-9-0 loss: 0.618683  [   32/  179]
train() client id: f_00007-9-1 loss: 0.652142  [   64/  179]
train() client id: f_00007-9-2 loss: 0.437901  [   96/  179]
train() client id: f_00007-9-3 loss: 0.551278  [  128/  179]
train() client id: f_00007-9-4 loss: 0.705265  [  160/  179]
train() client id: f_00007-10-0 loss: 0.587195  [   32/  179]
train() client id: f_00007-10-1 loss: 0.478202  [   64/  179]
train() client id: f_00007-10-2 loss: 0.526464  [   96/  179]
train() client id: f_00007-10-3 loss: 0.655171  [  128/  179]
train() client id: f_00007-10-4 loss: 0.621859  [  160/  179]
train() client id: f_00007-11-0 loss: 0.477847  [   32/  179]
train() client id: f_00007-11-1 loss: 0.937111  [   64/  179]
train() client id: f_00007-11-2 loss: 0.519994  [   96/  179]
train() client id: f_00007-11-3 loss: 0.552916  [  128/  179]
train() client id: f_00007-11-4 loss: 0.565544  [  160/  179]
train() client id: f_00007-12-0 loss: 0.708913  [   32/  179]
train() client id: f_00007-12-1 loss: 0.782611  [   64/  179]
train() client id: f_00007-12-2 loss: 0.519638  [   96/  179]
train() client id: f_00007-12-3 loss: 0.421774  [  128/  179]
train() client id: f_00007-12-4 loss: 0.597668  [  160/  179]
train() client id: f_00008-0-0 loss: 0.727564  [   32/  130]
train() client id: f_00008-0-1 loss: 0.796688  [   64/  130]
train() client id: f_00008-0-2 loss: 0.777778  [   96/  130]
train() client id: f_00008-0-3 loss: 0.784475  [  128/  130]
train() client id: f_00008-1-0 loss: 0.726531  [   32/  130]
train() client id: f_00008-1-1 loss: 0.683105  [   64/  130]
train() client id: f_00008-1-2 loss: 0.769006  [   96/  130]
train() client id: f_00008-1-3 loss: 0.886879  [  128/  130]
train() client id: f_00008-2-0 loss: 0.830379  [   32/  130]
train() client id: f_00008-2-1 loss: 0.722695  [   64/  130]
train() client id: f_00008-2-2 loss: 0.758240  [   96/  130]
train() client id: f_00008-2-3 loss: 0.723337  [  128/  130]
train() client id: f_00008-3-0 loss: 0.706948  [   32/  130]
train() client id: f_00008-3-1 loss: 0.723306  [   64/  130]
train() client id: f_00008-3-2 loss: 0.779519  [   96/  130]
train() client id: f_00008-3-3 loss: 0.904320  [  128/  130]
train() client id: f_00008-4-0 loss: 0.774273  [   32/  130]
train() client id: f_00008-4-1 loss: 0.694170  [   64/  130]
train() client id: f_00008-4-2 loss: 0.738119  [   96/  130]
train() client id: f_00008-4-3 loss: 0.870838  [  128/  130]
train() client id: f_00008-5-0 loss: 0.715467  [   32/  130]
train() client id: f_00008-5-1 loss: 0.802132  [   64/  130]
train() client id: f_00008-5-2 loss: 0.747651  [   96/  130]
train() client id: f_00008-5-3 loss: 0.844874  [  128/  130]
train() client id: f_00008-6-0 loss: 0.626688  [   32/  130]
train() client id: f_00008-6-1 loss: 0.883531  [   64/  130]
train() client id: f_00008-6-2 loss: 0.766027  [   96/  130]
train() client id: f_00008-6-3 loss: 0.796249  [  128/  130]
train() client id: f_00008-7-0 loss: 0.785543  [   32/  130]
train() client id: f_00008-7-1 loss: 0.838218  [   64/  130]
train() client id: f_00008-7-2 loss: 0.774783  [   96/  130]
train() client id: f_00008-7-3 loss: 0.695283  [  128/  130]
train() client id: f_00008-8-0 loss: 0.760885  [   32/  130]
train() client id: f_00008-8-1 loss: 0.754688  [   64/  130]
train() client id: f_00008-8-2 loss: 0.816494  [   96/  130]
train() client id: f_00008-8-3 loss: 0.753381  [  128/  130]
train() client id: f_00008-9-0 loss: 0.822559  [   32/  130]
train() client id: f_00008-9-1 loss: 0.810220  [   64/  130]
train() client id: f_00008-9-2 loss: 0.774020  [   96/  130]
train() client id: f_00008-9-3 loss: 0.693411  [  128/  130]
train() client id: f_00008-10-0 loss: 0.841447  [   32/  130]
train() client id: f_00008-10-1 loss: 0.824903  [   64/  130]
train() client id: f_00008-10-2 loss: 0.679236  [   96/  130]
train() client id: f_00008-10-3 loss: 0.761586  [  128/  130]
train() client id: f_00008-11-0 loss: 0.788212  [   32/  130]
train() client id: f_00008-11-1 loss: 0.711943  [   64/  130]
train() client id: f_00008-11-2 loss: 0.785651  [   96/  130]
train() client id: f_00008-11-3 loss: 0.799947  [  128/  130]
train() client id: f_00008-12-0 loss: 0.842661  [   32/  130]
train() client id: f_00008-12-1 loss: 0.716583  [   64/  130]
train() client id: f_00008-12-2 loss: 0.786066  [   96/  130]
train() client id: f_00008-12-3 loss: 0.773747  [  128/  130]
train() client id: f_00009-0-0 loss: 1.089006  [   32/  118]
train() client id: f_00009-0-1 loss: 0.998163  [   64/  118]
train() client id: f_00009-0-2 loss: 1.102001  [   96/  118]
train() client id: f_00009-1-0 loss: 1.034884  [   32/  118]
train() client id: f_00009-1-1 loss: 1.033751  [   64/  118]
train() client id: f_00009-1-2 loss: 0.974544  [   96/  118]
train() client id: f_00009-2-0 loss: 1.063693  [   32/  118]
train() client id: f_00009-2-1 loss: 1.002198  [   64/  118]
train() client id: f_00009-2-2 loss: 0.964596  [   96/  118]
train() client id: f_00009-3-0 loss: 1.003369  [   32/  118]
train() client id: f_00009-3-1 loss: 0.883791  [   64/  118]
train() client id: f_00009-3-2 loss: 0.847680  [   96/  118]
train() client id: f_00009-4-0 loss: 0.880208  [   32/  118]
train() client id: f_00009-4-1 loss: 0.997036  [   64/  118]
train() client id: f_00009-4-2 loss: 0.884657  [   96/  118]
train() client id: f_00009-5-0 loss: 0.941996  [   32/  118]
train() client id: f_00009-5-1 loss: 0.804476  [   64/  118]
train() client id: f_00009-5-2 loss: 0.934481  [   96/  118]
train() client id: f_00009-6-0 loss: 0.898284  [   32/  118]
train() client id: f_00009-6-1 loss: 0.968577  [   64/  118]
train() client id: f_00009-6-2 loss: 0.820696  [   96/  118]
train() client id: f_00009-7-0 loss: 0.854808  [   32/  118]
train() client id: f_00009-7-1 loss: 0.873618  [   64/  118]
train() client id: f_00009-7-2 loss: 0.868946  [   96/  118]
train() client id: f_00009-8-0 loss: 0.802877  [   32/  118]
train() client id: f_00009-8-1 loss: 0.795184  [   64/  118]
train() client id: f_00009-8-2 loss: 0.750341  [   96/  118]
train() client id: f_00009-9-0 loss: 0.842305  [   32/  118]
train() client id: f_00009-9-1 loss: 0.880630  [   64/  118]
train() client id: f_00009-9-2 loss: 0.796478  [   96/  118]
train() client id: f_00009-10-0 loss: 0.900672  [   32/  118]
train() client id: f_00009-10-1 loss: 0.801643  [   64/  118]
train() client id: f_00009-10-2 loss: 0.759862  [   96/  118]
train() client id: f_00009-11-0 loss: 0.841569  [   32/  118]
train() client id: f_00009-11-1 loss: 0.796360  [   64/  118]
train() client id: f_00009-11-2 loss: 0.734841  [   96/  118]
train() client id: f_00009-12-0 loss: 0.878685  [   32/  118]
train() client id: f_00009-12-1 loss: 0.820539  [   64/  118]
train() client id: f_00009-12-2 loss: 0.762663  [   96/  118]
At round 18 accuracy: 0.636604774535809
At round 18 training accuracy: 0.579476861167002
At round 18 training loss: 0.8449383637623223
update_location
xs = 8.927491 211.223621 5.882650 10.934260 -127.581990 24.769243 -5.849135 -5.143845 -150.120581 20.134486 
ys = -202.390647 7.291448 100.684448 -52.290817 -9.642386 0.794442 -61.381692 96.628436 25.881276 -637.232496 
xs mean: -0.6823799705212142
ys mean: -73.16579882624053
dists_uav = 225.924045 233.813137 142.028038 113.374987 162.388853 103.024980 117.481592 139.152843 182.225216 645.345374 
uav_gains = -109.406718 -109.989811 -103.811548 -101.363027 -105.276668 -100.323588 -101.749404 -103.588967 -106.568390 -127.182586 
uav_gains_db_mean: -106.92607058168795
dists_bs = 419.824990 421.063943 195.553917 293.653988 190.633886 265.057214 290.668746 187.064736 151.179957 835.343699 
bs_gains = -113.012971 -113.048805 -103.722522 -108.666464 -103.412662 -107.420569 -108.542213 -103.182833 -100.592897 -121.379285 
bs_gains_db_mean: -108.29812202155239
Round 19
-------------------------------
ene_coms = [0.01027314 0.01068097 0.00745099 0.00668008 0.00733995 0.0063882
 0.00679539 0.00725966 0.00646138 0.03359261]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.54448022 17.73134553  8.3506378   2.98546584 20.36536143  9.77999306
  3.71430189 11.97800672  8.72514196  8.52679062]
obj_prev = 100.70152507122066
eta_min = 1.0590119463276246e-11	eta_max = 0.7449652632385562
af = 21.163983583608065	bf = 2.230763604535209	zeta = 23.280381941968873	eta = 0.9090909090909091
af = 21.163983583608065	bf = 2.230763604535209	zeta = 46.726973537911405	eta = 0.45292861876528134
af = 21.163983583608065	bf = 2.230763604535209	zeta = 34.71773191471006	eta = 0.6096015614038656
af = 21.163983583608065	bf = 2.230763604535209	zeta = 32.545906833706155	eta = 0.6502809613431817
af = 21.163983583608065	bf = 2.230763604535209	zeta = 32.42231276478373	eta = 0.6527598366331173
af = 21.163983583608065	bf = 2.230763604535209	zeta = 32.421872168107754	eta = 0.6527687073057529
af = 21.163983583608065	bf = 2.230763604535209	zeta = 32.42187216247984	eta = 0.6527687074190629
eta = 0.6527687074190629
ene_coms = [0.01027314 0.01068097 0.00745099 0.00668008 0.00733995 0.0063882
 0.00679539 0.00725966 0.00646138 0.03359261]
ene_comp = [0.03519643 0.07402424 0.03463776 0.01201148 0.08547705 0.04078317
 0.01508419 0.05000128 0.03631378 0.03296174]
ene_total = [2.83821807 5.28731241 2.62718659 1.16672987 5.79365197 2.94444445
 1.36572625 3.57423722 2.67003221 4.15433313]
ti_comp = [0.36237827 0.35830005 0.39059976 0.39830895 0.39171016 0.40122766
 0.39715585 0.39251306 0.4004959  0.12918356]
ti_coms = [0.10273144 0.10680966 0.07450995 0.06680075 0.07339954 0.06388205
 0.06795386 0.07259665 0.06461381 0.33592614]
t_total = [29.03222084 29.03222084 29.03222084 29.03222084 29.03222084 29.03222084
 29.03222084 29.03222084 29.03222084 29.03222084]
ene_coms = [0.01027314 0.01068097 0.00745099 0.00668008 0.00733995 0.0063882
 0.00679539 0.00725966 0.00646138 0.03359261]
ene_comp = [2.07516033e-05 1.97473219e-04 1.70241494e-05 6.82699433e-07
 2.54389343e-04 2.63354746e-05 1.35995226e-06 5.07125196e-05
 1.86594236e-05 1.34120703e-04]
ene_total = [0.64254656 0.67903388 0.4661549  0.41701396 0.47404012 0.4003966
 0.42425392 0.45631491 0.40448512 2.10522809]
optimize_network iter = 0 obj = 6.46946806840999
eta = 0.6527687074190629
freqs = [4.85631112e+07 1.03299237e+08 4.43391961e+07 1.50780955e+07
 1.09107521e+08 5.08229761e+07 1.89902594e+07 6.36937803e+07
 4.53360190e+07 1.27577137e+08]
eta_min = 0.6527687074190671	eta_max = 0.6527687074190573
af = 0.03666346264556864	bf = 2.230763604535209	zeta = 0.04032980891012551	eta = 0.909090909090909
af = 0.03666346264556864	bf = 2.230763604535209	zeta = 24.57683806327424	eta = 0.0014917892428300504
af = 0.03666346264556864	bf = 2.230763604535209	zeta = 2.473050588734721	eta = 0.014825197192721662
af = 0.03666346264556864	bf = 2.230763604535209	zeta = 2.4210618211519934	eta = 0.015143546655955846
af = 0.03666346264556864	bf = 2.230763604535209	zeta = 2.4210534781161326	eta = 0.015143598841152892
eta = 0.015143598841152892
ene_coms = [0.01027314 0.01068097 0.00745099 0.00668008 0.00733995 0.0063882
 0.00679539 0.00725966 0.00646138 0.03359261]
ene_comp = [2.03860001e-04 1.93994122e-03 1.67242168e-04 6.70671588e-06
 2.49907494e-03 2.58714944e-04 1.33599253e-05 4.98190630e-04
 1.83306806e-04 1.31757755e-03]
ene_total = [0.23057271 0.27775467 0.16765838 0.14715939 0.21653248 0.14628211
 0.1498435  0.17073104 0.14623299 0.76828621]
ti_comp = [0.36237827 0.35830005 0.39059976 0.39830895 0.39171016 0.40122766
 0.39715585 0.39251306 0.4004959  0.12918356]
ti_coms = [0.10273144 0.10680966 0.07450995 0.06680075 0.07339954 0.06388205
 0.06795386 0.07259665 0.06461381 0.33592614]
t_total = [29.03222084 29.03222084 29.03222084 29.03222084 29.03222084 29.03222084
 29.03222084 29.03222084 29.03222084 29.03222084]
ene_coms = [0.01027314 0.01068097 0.00745099 0.00668008 0.00733995 0.0063882
 0.00679539 0.00725966 0.00646138 0.03359261]
ene_comp = [2.07516033e-05 1.97473219e-04 1.70241494e-05 6.82699433e-07
 2.54389343e-04 2.63354746e-05 1.35995226e-06 5.07125196e-05
 1.86594236e-05 1.34120703e-04]
ene_total = [0.64254656 0.67903388 0.4661549  0.41701396 0.47404012 0.4003966
 0.42425392 0.45631491 0.40448512 2.10522809]
optimize_network iter = 1 obj = 6.469468068410068
eta = 0.6527687074190671
freqs = [4.85631112e+07 1.03299237e+08 4.43391961e+07 1.50780955e+07
 1.09107521e+08 5.08229761e+07 1.89902594e+07 6.36937803e+07
 4.53360190e+07 1.27577137e+08]
Done!
ene_coms = [0.01027314 0.01068097 0.00745099 0.00668008 0.00733995 0.0063882
 0.00679539 0.00725966 0.00646138 0.03359261]
ene_comp = [1.93150975e-05 1.83803364e-04 1.58456724e-05 6.35440354e-07
 2.36779535e-04 2.45124318e-05 1.26581114e-06 4.72020041e-05
 1.73677466e-05 1.24836352e-04]
ene_total = [0.01029246 0.01086477 0.00746684 0.00668071 0.00757673 0.00641272
 0.00679665 0.00730687 0.00647875 0.03371745]
At round 19 energy consumption: 0.10359394745613343
At round 19 eta: 0.6527687074190671
At round 19 a_n: 21.674231763837348
At round 19 local rounds: 13.966838227168651
At round 19 global rounds: 62.42015690099562
gradient difference: 0.45323142409324646
train() client id: f_00000-0-0 loss: 1.396237  [   32/  126]
train() client id: f_00000-0-1 loss: 1.338672  [   64/  126]
train() client id: f_00000-0-2 loss: 1.256686  [   96/  126]
train() client id: f_00000-1-0 loss: 1.020586  [   32/  126]
train() client id: f_00000-1-1 loss: 1.325448  [   64/  126]
train() client id: f_00000-1-2 loss: 1.079806  [   96/  126]
train() client id: f_00000-2-0 loss: 1.051481  [   32/  126]
train() client id: f_00000-2-1 loss: 1.019001  [   64/  126]
train() client id: f_00000-2-2 loss: 1.109735  [   96/  126]
train() client id: f_00000-3-0 loss: 1.007259  [   32/  126]
train() client id: f_00000-3-1 loss: 1.004136  [   64/  126]
train() client id: f_00000-3-2 loss: 0.941033  [   96/  126]
train() client id: f_00000-4-0 loss: 0.994088  [   32/  126]
train() client id: f_00000-4-1 loss: 0.941392  [   64/  126]
train() client id: f_00000-4-2 loss: 0.945267  [   96/  126]
train() client id: f_00000-5-0 loss: 1.058191  [   32/  126]
train() client id: f_00000-5-1 loss: 0.933732  [   64/  126]
train() client id: f_00000-5-2 loss: 0.896574  [   96/  126]
train() client id: f_00000-6-0 loss: 0.953354  [   32/  126]
train() client id: f_00000-6-1 loss: 0.924090  [   64/  126]
train() client id: f_00000-6-2 loss: 0.923010  [   96/  126]
train() client id: f_00000-7-0 loss: 0.884020  [   32/  126]
train() client id: f_00000-7-1 loss: 0.850957  [   64/  126]
train() client id: f_00000-7-2 loss: 0.896629  [   96/  126]
train() client id: f_00000-8-0 loss: 0.975197  [   32/  126]
train() client id: f_00000-8-1 loss: 0.849648  [   64/  126]
train() client id: f_00000-8-2 loss: 0.855049  [   96/  126]
train() client id: f_00000-9-0 loss: 0.855503  [   32/  126]
train() client id: f_00000-9-1 loss: 0.945891  [   64/  126]
train() client id: f_00000-9-2 loss: 0.882073  [   96/  126]
train() client id: f_00000-10-0 loss: 0.807326  [   32/  126]
train() client id: f_00000-10-1 loss: 0.975916  [   64/  126]
train() client id: f_00000-10-2 loss: 0.886620  [   96/  126]
train() client id: f_00000-11-0 loss: 0.813764  [   32/  126]
train() client id: f_00000-11-1 loss: 0.999330  [   64/  126]
train() client id: f_00000-11-2 loss: 0.866335  [   96/  126]
train() client id: f_00000-12-0 loss: 0.867253  [   32/  126]
train() client id: f_00000-12-1 loss: 0.950287  [   64/  126]
train() client id: f_00000-12-2 loss: 0.825645  [   96/  126]
train() client id: f_00001-0-0 loss: 0.537603  [   32/  265]
train() client id: f_00001-0-1 loss: 0.462717  [   64/  265]
train() client id: f_00001-0-2 loss: 0.385456  [   96/  265]
train() client id: f_00001-0-3 loss: 0.485074  [  128/  265]
train() client id: f_00001-0-4 loss: 0.463002  [  160/  265]
train() client id: f_00001-0-5 loss: 0.543137  [  192/  265]
train() client id: f_00001-0-6 loss: 0.383638  [  224/  265]
train() client id: f_00001-0-7 loss: 0.474290  [  256/  265]
train() client id: f_00001-1-0 loss: 0.488194  [   32/  265]
train() client id: f_00001-1-1 loss: 0.459681  [   64/  265]
train() client id: f_00001-1-2 loss: 0.458723  [   96/  265]
train() client id: f_00001-1-3 loss: 0.471168  [  128/  265]
train() client id: f_00001-1-4 loss: 0.423394  [  160/  265]
train() client id: f_00001-1-5 loss: 0.420515  [  192/  265]
train() client id: f_00001-1-6 loss: 0.489646  [  224/  265]
train() client id: f_00001-1-7 loss: 0.451077  [  256/  265]
train() client id: f_00001-2-0 loss: 0.422790  [   32/  265]
train() client id: f_00001-2-1 loss: 0.371020  [   64/  265]
train() client id: f_00001-2-2 loss: 0.474298  [   96/  265]
train() client id: f_00001-2-3 loss: 0.544895  [  128/  265]
train() client id: f_00001-2-4 loss: 0.428969  [  160/  265]
train() client id: f_00001-2-5 loss: 0.410044  [  192/  265]
train() client id: f_00001-2-6 loss: 0.392590  [  224/  265]
train() client id: f_00001-2-7 loss: 0.482858  [  256/  265]
train() client id: f_00001-3-0 loss: 0.655518  [   32/  265]
train() client id: f_00001-3-1 loss: 0.506835  [   64/  265]
train() client id: f_00001-3-2 loss: 0.399877  [   96/  265]
train() client id: f_00001-3-3 loss: 0.396082  [  128/  265]
train() client id: f_00001-3-4 loss: 0.401579  [  160/  265]
train() client id: f_00001-3-5 loss: 0.443086  [  192/  265]
train() client id: f_00001-3-6 loss: 0.339434  [  224/  265]
train() client id: f_00001-3-7 loss: 0.358309  [  256/  265]
train() client id: f_00001-4-0 loss: 0.508531  [   32/  265]
train() client id: f_00001-4-1 loss: 0.520991  [   64/  265]
train() client id: f_00001-4-2 loss: 0.349767  [   96/  265]
train() client id: f_00001-4-3 loss: 0.443670  [  128/  265]
train() client id: f_00001-4-4 loss: 0.406637  [  160/  265]
train() client id: f_00001-4-5 loss: 0.403612  [  192/  265]
train() client id: f_00001-4-6 loss: 0.453056  [  224/  265]
train() client id: f_00001-4-7 loss: 0.404862  [  256/  265]
train() client id: f_00001-5-0 loss: 0.409353  [   32/  265]
train() client id: f_00001-5-1 loss: 0.397537  [   64/  265]
train() client id: f_00001-5-2 loss: 0.418164  [   96/  265]
train() client id: f_00001-5-3 loss: 0.443774  [  128/  265]
train() client id: f_00001-5-4 loss: 0.489046  [  160/  265]
train() client id: f_00001-5-5 loss: 0.417065  [  192/  265]
train() client id: f_00001-5-6 loss: 0.407127  [  224/  265]
train() client id: f_00001-5-7 loss: 0.477416  [  256/  265]
train() client id: f_00001-6-0 loss: 0.411044  [   32/  265]
train() client id: f_00001-6-1 loss: 0.337362  [   64/  265]
train() client id: f_00001-6-2 loss: 0.698312  [   96/  265]
train() client id: f_00001-6-3 loss: 0.328566  [  128/  265]
train() client id: f_00001-6-4 loss: 0.495790  [  160/  265]
train() client id: f_00001-6-5 loss: 0.390958  [  192/  265]
train() client id: f_00001-6-6 loss: 0.390693  [  224/  265]
train() client id: f_00001-6-7 loss: 0.408341  [  256/  265]
train() client id: f_00001-7-0 loss: 0.415482  [   32/  265]
train() client id: f_00001-7-1 loss: 0.437882  [   64/  265]
train() client id: f_00001-7-2 loss: 0.489745  [   96/  265]
train() client id: f_00001-7-3 loss: 0.367640  [  128/  265]
train() client id: f_00001-7-4 loss: 0.399527  [  160/  265]
train() client id: f_00001-7-5 loss: 0.589500  [  192/  265]
train() client id: f_00001-7-6 loss: 0.393868  [  224/  265]
train() client id: f_00001-7-7 loss: 0.348633  [  256/  265]
train() client id: f_00001-8-0 loss: 0.588537  [   32/  265]
train() client id: f_00001-8-1 loss: 0.335876  [   64/  265]
train() client id: f_00001-8-2 loss: 0.327312  [   96/  265]
train() client id: f_00001-8-3 loss: 0.426832  [  128/  265]
train() client id: f_00001-8-4 loss: 0.465388  [  160/  265]
train() client id: f_00001-8-5 loss: 0.316330  [  192/  265]
train() client id: f_00001-8-6 loss: 0.349730  [  224/  265]
train() client id: f_00001-8-7 loss: 0.552254  [  256/  265]
train() client id: f_00001-9-0 loss: 0.344175  [   32/  265]
train() client id: f_00001-9-1 loss: 0.554354  [   64/  265]
train() client id: f_00001-9-2 loss: 0.482150  [   96/  265]
train() client id: f_00001-9-3 loss: 0.496069  [  128/  265]
train() client id: f_00001-9-4 loss: 0.440190  [  160/  265]
train() client id: f_00001-9-5 loss: 0.397623  [  192/  265]
train() client id: f_00001-9-6 loss: 0.375669  [  224/  265]
train() client id: f_00001-9-7 loss: 0.346463  [  256/  265]
train() client id: f_00001-10-0 loss: 0.472577  [   32/  265]
train() client id: f_00001-10-1 loss: 0.376424  [   64/  265]
train() client id: f_00001-10-2 loss: 0.409167  [   96/  265]
train() client id: f_00001-10-3 loss: 0.340911  [  128/  265]
train() client id: f_00001-10-4 loss: 0.362226  [  160/  265]
train() client id: f_00001-10-5 loss: 0.462736  [  192/  265]
train() client id: f_00001-10-6 loss: 0.446743  [  224/  265]
train() client id: f_00001-10-7 loss: 0.498809  [  256/  265]
train() client id: f_00001-11-0 loss: 0.399970  [   32/  265]
train() client id: f_00001-11-1 loss: 0.417721  [   64/  265]
train() client id: f_00001-11-2 loss: 0.516726  [   96/  265]
train() client id: f_00001-11-3 loss: 0.565062  [  128/  265]
train() client id: f_00001-11-4 loss: 0.452053  [  160/  265]
train() client id: f_00001-11-5 loss: 0.331334  [  192/  265]
train() client id: f_00001-11-6 loss: 0.332453  [  224/  265]
train() client id: f_00001-11-7 loss: 0.407226  [  256/  265]
train() client id: f_00001-12-0 loss: 0.428533  [   32/  265]
train() client id: f_00001-12-1 loss: 0.555133  [   64/  265]
train() client id: f_00001-12-2 loss: 0.328722  [   96/  265]
train() client id: f_00001-12-3 loss: 0.578949  [  128/  265]
train() client id: f_00001-12-4 loss: 0.340306  [  160/  265]
train() client id: f_00001-12-5 loss: 0.385284  [  192/  265]
train() client id: f_00001-12-6 loss: 0.377647  [  224/  265]
train() client id: f_00001-12-7 loss: 0.435129  [  256/  265]
train() client id: f_00002-0-0 loss: 1.343056  [   32/  124]
train() client id: f_00002-0-1 loss: 1.320157  [   64/  124]
train() client id: f_00002-0-2 loss: 1.233021  [   96/  124]
train() client id: f_00002-1-0 loss: 1.154302  [   32/  124]
train() client id: f_00002-1-1 loss: 1.299053  [   64/  124]
train() client id: f_00002-1-2 loss: 1.327641  [   96/  124]
train() client id: f_00002-2-0 loss: 1.162370  [   32/  124]
train() client id: f_00002-2-1 loss: 1.210696  [   64/  124]
train() client id: f_00002-2-2 loss: 1.267227  [   96/  124]
train() client id: f_00002-3-0 loss: 1.060034  [   32/  124]
train() client id: f_00002-3-1 loss: 1.150128  [   64/  124]
train() client id: f_00002-3-2 loss: 1.185669  [   96/  124]
train() client id: f_00002-4-0 loss: 1.076503  [   32/  124]
train() client id: f_00002-4-1 loss: 1.084592  [   64/  124]
train() client id: f_00002-4-2 loss: 1.302489  [   96/  124]
train() client id: f_00002-5-0 loss: 1.148599  [   32/  124]
train() client id: f_00002-5-1 loss: 1.068828  [   64/  124]
train() client id: f_00002-5-2 loss: 1.070545  [   96/  124]
train() client id: f_00002-6-0 loss: 1.194922  [   32/  124]
train() client id: f_00002-6-1 loss: 1.137238  [   64/  124]
train() client id: f_00002-6-2 loss: 0.937868  [   96/  124]
train() client id: f_00002-7-0 loss: 1.189861  [   32/  124]
train() client id: f_00002-7-1 loss: 0.982255  [   64/  124]
train() client id: f_00002-7-2 loss: 1.094541  [   96/  124]
train() client id: f_00002-8-0 loss: 1.182338  [   32/  124]
train() client id: f_00002-8-1 loss: 1.021414  [   64/  124]
train() client id: f_00002-8-2 loss: 1.039522  [   96/  124]
train() client id: f_00002-9-0 loss: 1.100150  [   32/  124]
train() client id: f_00002-9-1 loss: 1.127150  [   64/  124]
train() client id: f_00002-9-2 loss: 1.041143  [   96/  124]
train() client id: f_00002-10-0 loss: 1.017114  [   32/  124]
train() client id: f_00002-10-1 loss: 1.131763  [   64/  124]
train() client id: f_00002-10-2 loss: 1.043457  [   96/  124]
train() client id: f_00002-11-0 loss: 1.059207  [   32/  124]
train() client id: f_00002-11-1 loss: 0.973033  [   64/  124]
train() client id: f_00002-11-2 loss: 1.139568  [   96/  124]
train() client id: f_00002-12-0 loss: 1.126416  [   32/  124]
train() client id: f_00002-12-1 loss: 1.063494  [   64/  124]
train() client id: f_00002-12-2 loss: 1.074362  [   96/  124]
train() client id: f_00003-0-0 loss: 0.764825  [   32/   43]
train() client id: f_00003-1-0 loss: 0.801982  [   32/   43]
train() client id: f_00003-2-0 loss: 0.825712  [   32/   43]
train() client id: f_00003-3-0 loss: 0.693813  [   32/   43]
train() client id: f_00003-4-0 loss: 0.881683  [   32/   43]
train() client id: f_00003-5-0 loss: 0.817345  [   32/   43]
train() client id: f_00003-6-0 loss: 0.772936  [   32/   43]
train() client id: f_00003-7-0 loss: 0.693596  [   32/   43]
train() client id: f_00003-8-0 loss: 0.704301  [   32/   43]
train() client id: f_00003-9-0 loss: 0.737609  [   32/   43]
train() client id: f_00003-10-0 loss: 0.885706  [   32/   43]
train() client id: f_00003-11-0 loss: 0.691290  [   32/   43]
train() client id: f_00003-12-0 loss: 0.830721  [   32/   43]
train() client id: f_00004-0-0 loss: 0.957190  [   32/  306]
train() client id: f_00004-0-1 loss: 0.955637  [   64/  306]
train() client id: f_00004-0-2 loss: 0.796243  [   96/  306]
train() client id: f_00004-0-3 loss: 0.844315  [  128/  306]
train() client id: f_00004-0-4 loss: 0.761446  [  160/  306]
train() client id: f_00004-0-5 loss: 0.881738  [  192/  306]
train() client id: f_00004-0-6 loss: 0.831173  [  224/  306]
train() client id: f_00004-0-7 loss: 0.920101  [  256/  306]
train() client id: f_00004-0-8 loss: 0.722469  [  288/  306]
train() client id: f_00004-1-0 loss: 0.934781  [   32/  306]
train() client id: f_00004-1-1 loss: 0.825476  [   64/  306]
train() client id: f_00004-1-2 loss: 0.742488  [   96/  306]
train() client id: f_00004-1-3 loss: 0.893097  [  128/  306]
train() client id: f_00004-1-4 loss: 0.991238  [  160/  306]
train() client id: f_00004-1-5 loss: 0.860825  [  192/  306]
train() client id: f_00004-1-6 loss: 0.779822  [  224/  306]
train() client id: f_00004-1-7 loss: 0.937045  [  256/  306]
train() client id: f_00004-1-8 loss: 0.665742  [  288/  306]
train() client id: f_00004-2-0 loss: 0.725289  [   32/  306]
train() client id: f_00004-2-1 loss: 0.882627  [   64/  306]
train() client id: f_00004-2-2 loss: 0.853415  [   96/  306]
train() client id: f_00004-2-3 loss: 0.916042  [  128/  306]
train() client id: f_00004-2-4 loss: 0.860410  [  160/  306]
train() client id: f_00004-2-5 loss: 0.821980  [  192/  306]
train() client id: f_00004-2-6 loss: 0.878133  [  224/  306]
train() client id: f_00004-2-7 loss: 0.796478  [  256/  306]
train() client id: f_00004-2-8 loss: 0.779285  [  288/  306]
train() client id: f_00004-3-0 loss: 0.852492  [   32/  306]
train() client id: f_00004-3-1 loss: 0.989949  [   64/  306]
train() client id: f_00004-3-2 loss: 0.861462  [   96/  306]
train() client id: f_00004-3-3 loss: 0.821636  [  128/  306]
train() client id: f_00004-3-4 loss: 0.786639  [  160/  306]
train() client id: f_00004-3-5 loss: 0.815132  [  192/  306]
train() client id: f_00004-3-6 loss: 0.724970  [  224/  306]
train() client id: f_00004-3-7 loss: 0.862231  [  256/  306]
train() client id: f_00004-3-8 loss: 0.996132  [  288/  306]
train() client id: f_00004-4-0 loss: 0.958708  [   32/  306]
train() client id: f_00004-4-1 loss: 0.804733  [   64/  306]
train() client id: f_00004-4-2 loss: 0.832399  [   96/  306]
train() client id: f_00004-4-3 loss: 0.812991  [  128/  306]
train() client id: f_00004-4-4 loss: 0.910343  [  160/  306]
train() client id: f_00004-4-5 loss: 0.780993  [  192/  306]
train() client id: f_00004-4-6 loss: 0.835258  [  224/  306]
train() client id: f_00004-4-7 loss: 0.812239  [  256/  306]
train() client id: f_00004-4-8 loss: 0.945224  [  288/  306]
train() client id: f_00004-5-0 loss: 0.900638  [   32/  306]
train() client id: f_00004-5-1 loss: 0.784858  [   64/  306]
train() client id: f_00004-5-2 loss: 0.833109  [   96/  306]
train() client id: f_00004-5-3 loss: 0.801144  [  128/  306]
train() client id: f_00004-5-4 loss: 0.928129  [  160/  306]
train() client id: f_00004-5-5 loss: 0.843105  [  192/  306]
train() client id: f_00004-5-6 loss: 0.924331  [  224/  306]
train() client id: f_00004-5-7 loss: 0.822751  [  256/  306]
train() client id: f_00004-5-8 loss: 0.842667  [  288/  306]
train() client id: f_00004-6-0 loss: 0.972784  [   32/  306]
train() client id: f_00004-6-1 loss: 0.828016  [   64/  306]
train() client id: f_00004-6-2 loss: 0.961955  [   96/  306]
train() client id: f_00004-6-3 loss: 0.994923  [  128/  306]
train() client id: f_00004-6-4 loss: 0.818925  [  160/  306]
train() client id: f_00004-6-5 loss: 0.735200  [  192/  306]
train() client id: f_00004-6-6 loss: 0.724662  [  224/  306]
train() client id: f_00004-6-7 loss: 0.753831  [  256/  306]
train() client id: f_00004-6-8 loss: 0.813183  [  288/  306]
train() client id: f_00004-7-0 loss: 0.814166  [   32/  306]
train() client id: f_00004-7-1 loss: 0.926386  [   64/  306]
train() client id: f_00004-7-2 loss: 0.784830  [   96/  306]
train() client id: f_00004-7-3 loss: 0.869251  [  128/  306]
train() client id: f_00004-7-4 loss: 0.828192  [  160/  306]
train() client id: f_00004-7-5 loss: 0.844778  [  192/  306]
train() client id: f_00004-7-6 loss: 0.791194  [  224/  306]
train() client id: f_00004-7-7 loss: 0.753694  [  256/  306]
train() client id: f_00004-7-8 loss: 0.934758  [  288/  306]
train() client id: f_00004-8-0 loss: 0.746355  [   32/  306]
train() client id: f_00004-8-1 loss: 0.766135  [   64/  306]
train() client id: f_00004-8-2 loss: 0.949191  [   96/  306]
train() client id: f_00004-8-3 loss: 0.886211  [  128/  306]
train() client id: f_00004-8-4 loss: 0.816346  [  160/  306]
train() client id: f_00004-8-5 loss: 0.948605  [  192/  306]
train() client id: f_00004-8-6 loss: 0.983701  [  224/  306]
train() client id: f_00004-8-7 loss: 0.788183  [  256/  306]
train() client id: f_00004-8-8 loss: 0.813056  [  288/  306]
train() client id: f_00004-9-0 loss: 0.810072  [   32/  306]
train() client id: f_00004-9-1 loss: 0.791005  [   64/  306]
train() client id: f_00004-9-2 loss: 0.885488  [   96/  306]
train() client id: f_00004-9-3 loss: 0.783192  [  128/  306]
train() client id: f_00004-9-4 loss: 0.874406  [  160/  306]
train() client id: f_00004-9-5 loss: 0.930977  [  192/  306]
train() client id: f_00004-9-6 loss: 0.819718  [  224/  306]
train() client id: f_00004-9-7 loss: 0.856657  [  256/  306]
train() client id: f_00004-9-8 loss: 0.810774  [  288/  306]
train() client id: f_00004-10-0 loss: 0.897762  [   32/  306]
train() client id: f_00004-10-1 loss: 0.957325  [   64/  306]
train() client id: f_00004-10-2 loss: 0.809007  [   96/  306]
train() client id: f_00004-10-3 loss: 0.877305  [  128/  306]
train() client id: f_00004-10-4 loss: 0.798433  [  160/  306]
train() client id: f_00004-10-5 loss: 0.807137  [  192/  306]
train() client id: f_00004-10-6 loss: 0.735819  [  224/  306]
train() client id: f_00004-10-7 loss: 0.830286  [  256/  306]
train() client id: f_00004-10-8 loss: 0.947945  [  288/  306]
train() client id: f_00004-11-0 loss: 0.821083  [   32/  306]
train() client id: f_00004-11-1 loss: 0.829799  [   64/  306]
train() client id: f_00004-11-2 loss: 0.859317  [   96/  306]
train() client id: f_00004-11-3 loss: 0.893122  [  128/  306]
train() client id: f_00004-11-4 loss: 0.790779  [  160/  306]
train() client id: f_00004-11-5 loss: 0.785973  [  192/  306]
train() client id: f_00004-11-6 loss: 0.906362  [  224/  306]
train() client id: f_00004-11-7 loss: 0.900040  [  256/  306]
train() client id: f_00004-11-8 loss: 0.909625  [  288/  306]
train() client id: f_00004-12-0 loss: 0.714773  [   32/  306]
train() client id: f_00004-12-1 loss: 0.953932  [   64/  306]
train() client id: f_00004-12-2 loss: 0.853458  [   96/  306]
train() client id: f_00004-12-3 loss: 0.906350  [  128/  306]
train() client id: f_00004-12-4 loss: 0.799461  [  160/  306]
train() client id: f_00004-12-5 loss: 0.875944  [  192/  306]
train() client id: f_00004-12-6 loss: 0.830224  [  224/  306]
train() client id: f_00004-12-7 loss: 0.797885  [  256/  306]
train() client id: f_00004-12-8 loss: 0.847680  [  288/  306]
train() client id: f_00005-0-0 loss: 0.668294  [   32/  146]
train() client id: f_00005-0-1 loss: 0.608396  [   64/  146]
train() client id: f_00005-0-2 loss: 0.680890  [   96/  146]
train() client id: f_00005-0-3 loss: 0.669540  [  128/  146]
train() client id: f_00005-1-0 loss: 0.538393  [   32/  146]
train() client id: f_00005-1-1 loss: 0.827829  [   64/  146]
train() client id: f_00005-1-2 loss: 0.498457  [   96/  146]
train() client id: f_00005-1-3 loss: 0.743899  [  128/  146]
train() client id: f_00005-2-0 loss: 0.656803  [   32/  146]
train() client id: f_00005-2-1 loss: 0.457940  [   64/  146]
train() client id: f_00005-2-2 loss: 0.787766  [   96/  146]
train() client id: f_00005-2-3 loss: 0.585043  [  128/  146]
train() client id: f_00005-3-0 loss: 0.571204  [   32/  146]
train() client id: f_00005-3-1 loss: 0.555263  [   64/  146]
train() client id: f_00005-3-2 loss: 0.593528  [   96/  146]
train() client id: f_00005-3-3 loss: 0.637615  [  128/  146]
train() client id: f_00005-4-0 loss: 0.690894  [   32/  146]
train() client id: f_00005-4-1 loss: 0.593366  [   64/  146]
train() client id: f_00005-4-2 loss: 0.370676  [   96/  146]
train() client id: f_00005-4-3 loss: 0.772833  [  128/  146]
train() client id: f_00005-5-0 loss: 0.690158  [   32/  146]
train() client id: f_00005-5-1 loss: 0.561031  [   64/  146]
train() client id: f_00005-5-2 loss: 0.669458  [   96/  146]
train() client id: f_00005-5-3 loss: 0.614443  [  128/  146]
train() client id: f_00005-6-0 loss: 0.611837  [   32/  146]
train() client id: f_00005-6-1 loss: 0.650595  [   64/  146]
train() client id: f_00005-6-2 loss: 0.816769  [   96/  146]
train() client id: f_00005-6-3 loss: 0.521015  [  128/  146]
train() client id: f_00005-7-0 loss: 0.707623  [   32/  146]
train() client id: f_00005-7-1 loss: 0.637077  [   64/  146]
train() client id: f_00005-7-2 loss: 0.492599  [   96/  146]
train() client id: f_00005-7-3 loss: 0.731183  [  128/  146]
train() client id: f_00005-8-0 loss: 0.771705  [   32/  146]
train() client id: f_00005-8-1 loss: 0.601064  [   64/  146]
train() client id: f_00005-8-2 loss: 0.525196  [   96/  146]
train() client id: f_00005-8-3 loss: 0.522263  [  128/  146]
train() client id: f_00005-9-0 loss: 0.825343  [   32/  146]
train() client id: f_00005-9-1 loss: 0.713353  [   64/  146]
train() client id: f_00005-9-2 loss: 0.419232  [   96/  146]
train() client id: f_00005-9-3 loss: 0.622491  [  128/  146]
train() client id: f_00005-10-0 loss: 0.613041  [   32/  146]
train() client id: f_00005-10-1 loss: 0.800846  [   64/  146]
train() client id: f_00005-10-2 loss: 0.650592  [   96/  146]
train() client id: f_00005-10-3 loss: 0.543411  [  128/  146]
train() client id: f_00005-11-0 loss: 0.421045  [   32/  146]
train() client id: f_00005-11-1 loss: 0.661412  [   64/  146]
train() client id: f_00005-11-2 loss: 0.692075  [   96/  146]
train() client id: f_00005-11-3 loss: 0.524835  [  128/  146]
train() client id: f_00005-12-0 loss: 0.603916  [   32/  146]
train() client id: f_00005-12-1 loss: 0.542763  [   64/  146]
train() client id: f_00005-12-2 loss: 0.543772  [   96/  146]
train() client id: f_00005-12-3 loss: 0.672829  [  128/  146]
train() client id: f_00006-0-0 loss: 0.620458  [   32/   54]
train() client id: f_00006-1-0 loss: 0.563051  [   32/   54]
train() client id: f_00006-2-0 loss: 0.618347  [   32/   54]
train() client id: f_00006-3-0 loss: 0.570586  [   32/   54]
train() client id: f_00006-4-0 loss: 0.527314  [   32/   54]
train() client id: f_00006-5-0 loss: 0.565659  [   32/   54]
train() client id: f_00006-6-0 loss: 0.548695  [   32/   54]
train() client id: f_00006-7-0 loss: 0.613822  [   32/   54]
train() client id: f_00006-8-0 loss: 0.559738  [   32/   54]
train() client id: f_00006-9-0 loss: 0.606507  [   32/   54]
train() client id: f_00006-10-0 loss: 0.562087  [   32/   54]
train() client id: f_00006-11-0 loss: 0.630375  [   32/   54]
train() client id: f_00006-12-0 loss: 0.576450  [   32/   54]
train() client id: f_00007-0-0 loss: 0.562597  [   32/  179]
train() client id: f_00007-0-1 loss: 0.600220  [   64/  179]
train() client id: f_00007-0-2 loss: 0.524817  [   96/  179]
train() client id: f_00007-0-3 loss: 0.734906  [  128/  179]
train() client id: f_00007-0-4 loss: 0.593280  [  160/  179]
train() client id: f_00007-1-0 loss: 0.627684  [   32/  179]
train() client id: f_00007-1-1 loss: 0.678297  [   64/  179]
train() client id: f_00007-1-2 loss: 0.675469  [   96/  179]
train() client id: f_00007-1-3 loss: 0.554030  [  128/  179]
train() client id: f_00007-1-4 loss: 0.683621  [  160/  179]
train() client id: f_00007-2-0 loss: 0.607756  [   32/  179]
train() client id: f_00007-2-1 loss: 0.532820  [   64/  179]
train() client id: f_00007-2-2 loss: 0.507081  [   96/  179]
train() client id: f_00007-2-3 loss: 0.857297  [  128/  179]
train() client id: f_00007-2-4 loss: 0.744506  [  160/  179]
train() client id: f_00007-3-0 loss: 0.662832  [   32/  179]
train() client id: f_00007-3-1 loss: 0.536688  [   64/  179]
train() client id: f_00007-3-2 loss: 0.683080  [   96/  179]
train() client id: f_00007-3-3 loss: 0.661036  [  128/  179]
train() client id: f_00007-3-4 loss: 0.580678  [  160/  179]
train() client id: f_00007-4-0 loss: 0.588365  [   32/  179]
train() client id: f_00007-4-1 loss: 0.555540  [   64/  179]
train() client id: f_00007-4-2 loss: 0.640547  [   96/  179]
train() client id: f_00007-4-3 loss: 0.651134  [  128/  179]
train() client id: f_00007-4-4 loss: 0.715030  [  160/  179]
train() client id: f_00007-5-0 loss: 0.549482  [   32/  179]
train() client id: f_00007-5-1 loss: 0.562312  [   64/  179]
train() client id: f_00007-5-2 loss: 0.571039  [   96/  179]
train() client id: f_00007-5-3 loss: 0.675407  [  128/  179]
train() client id: f_00007-5-4 loss: 0.550670  [  160/  179]
train() client id: f_00007-6-0 loss: 0.656024  [   32/  179]
train() client id: f_00007-6-1 loss: 0.479723  [   64/  179]
train() client id: f_00007-6-2 loss: 0.588224  [   96/  179]
train() client id: f_00007-6-3 loss: 0.584402  [  128/  179]
train() client id: f_00007-6-4 loss: 0.836496  [  160/  179]
train() client id: f_00007-7-0 loss: 0.469617  [   32/  179]
train() client id: f_00007-7-1 loss: 0.674645  [   64/  179]
train() client id: f_00007-7-2 loss: 0.625394  [   96/  179]
train() client id: f_00007-7-3 loss: 0.633895  [  128/  179]
train() client id: f_00007-7-4 loss: 0.605013  [  160/  179]
train() client id: f_00007-8-0 loss: 0.704752  [   32/  179]
train() client id: f_00007-8-1 loss: 0.649898  [   64/  179]
train() client id: f_00007-8-2 loss: 0.586287  [   96/  179]
train() client id: f_00007-8-3 loss: 0.663216  [  128/  179]
train() client id: f_00007-8-4 loss: 0.549367  [  160/  179]
train() client id: f_00007-9-0 loss: 0.761430  [   32/  179]
train() client id: f_00007-9-1 loss: 0.572473  [   64/  179]
train() client id: f_00007-9-2 loss: 0.581447  [   96/  179]
train() client id: f_00007-9-3 loss: 0.459827  [  128/  179]
train() client id: f_00007-9-4 loss: 0.618928  [  160/  179]
train() client id: f_00007-10-0 loss: 0.594849  [   32/  179]
train() client id: f_00007-10-1 loss: 0.627306  [   64/  179]
train() client id: f_00007-10-2 loss: 0.557850  [   96/  179]
train() client id: f_00007-10-3 loss: 0.732650  [  128/  179]
train() client id: f_00007-10-4 loss: 0.542802  [  160/  179]
train() client id: f_00007-11-0 loss: 0.684681  [   32/  179]
train() client id: f_00007-11-1 loss: 0.463476  [   64/  179]
train() client id: f_00007-11-2 loss: 0.494296  [   96/  179]
train() client id: f_00007-11-3 loss: 0.780032  [  128/  179]
train() client id: f_00007-11-4 loss: 0.653118  [  160/  179]
train() client id: f_00007-12-0 loss: 0.572963  [   32/  179]
train() client id: f_00007-12-1 loss: 0.515807  [   64/  179]
train() client id: f_00007-12-2 loss: 0.556660  [   96/  179]
train() client id: f_00007-12-3 loss: 0.715681  [  128/  179]
train() client id: f_00007-12-4 loss: 0.735495  [  160/  179]
train() client id: f_00008-0-0 loss: 0.619670  [   32/  130]
train() client id: f_00008-0-1 loss: 0.786314  [   64/  130]
train() client id: f_00008-0-2 loss: 0.731083  [   96/  130]
train() client id: f_00008-0-3 loss: 0.798683  [  128/  130]
train() client id: f_00008-1-0 loss: 0.602729  [   32/  130]
train() client id: f_00008-1-1 loss: 0.751452  [   64/  130]
train() client id: f_00008-1-2 loss: 0.803841  [   96/  130]
train() client id: f_00008-1-3 loss: 0.781345  [  128/  130]
train() client id: f_00008-2-0 loss: 0.805172  [   32/  130]
train() client id: f_00008-2-1 loss: 0.726468  [   64/  130]
train() client id: f_00008-2-2 loss: 0.825834  [   96/  130]
train() client id: f_00008-2-3 loss: 0.609121  [  128/  130]
train() client id: f_00008-3-0 loss: 0.732950  [   32/  130]
train() client id: f_00008-3-1 loss: 0.672188  [   64/  130]
train() client id: f_00008-3-2 loss: 0.705972  [   96/  130]
train() client id: f_00008-3-3 loss: 0.807996  [  128/  130]
train() client id: f_00008-4-0 loss: 0.942484  [   32/  130]
train() client id: f_00008-4-1 loss: 0.671016  [   64/  130]
train() client id: f_00008-4-2 loss: 0.682780  [   96/  130]
train() client id: f_00008-4-3 loss: 0.620316  [  128/  130]
train() client id: f_00008-5-0 loss: 0.713449  [   32/  130]
train() client id: f_00008-5-1 loss: 0.883849  [   64/  130]
train() client id: f_00008-5-2 loss: 0.678192  [   96/  130]
train() client id: f_00008-5-3 loss: 0.662911  [  128/  130]
train() client id: f_00008-6-0 loss: 0.775723  [   32/  130]
train() client id: f_00008-6-1 loss: 0.744143  [   64/  130]
train() client id: f_00008-6-2 loss: 0.754387  [   96/  130]
train() client id: f_00008-6-3 loss: 0.665120  [  128/  130]
train() client id: f_00008-7-0 loss: 0.667473  [   32/  130]
train() client id: f_00008-7-1 loss: 0.733831  [   64/  130]
train() client id: f_00008-7-2 loss: 0.752477  [   96/  130]
train() client id: f_00008-7-3 loss: 0.797712  [  128/  130]
train() client id: f_00008-8-0 loss: 0.733624  [   32/  130]
train() client id: f_00008-8-1 loss: 0.735756  [   64/  130]
train() client id: f_00008-8-2 loss: 0.788471  [   96/  130]
train() client id: f_00008-8-3 loss: 0.667243  [  128/  130]
train() client id: f_00008-9-0 loss: 0.732276  [   32/  130]
train() client id: f_00008-9-1 loss: 0.805405  [   64/  130]
train() client id: f_00008-9-2 loss: 0.649399  [   96/  130]
train() client id: f_00008-9-3 loss: 0.754808  [  128/  130]
train() client id: f_00008-10-0 loss: 0.784552  [   32/  130]
train() client id: f_00008-10-1 loss: 0.664820  [   64/  130]
train() client id: f_00008-10-2 loss: 0.719447  [   96/  130]
train() client id: f_00008-10-3 loss: 0.749606  [  128/  130]
train() client id: f_00008-11-0 loss: 0.720757  [   32/  130]
train() client id: f_00008-11-1 loss: 0.775509  [   64/  130]
train() client id: f_00008-11-2 loss: 0.675131  [   96/  130]
train() client id: f_00008-11-3 loss: 0.756292  [  128/  130]
train() client id: f_00008-12-0 loss: 0.806852  [   32/  130]
train() client id: f_00008-12-1 loss: 0.735692  [   64/  130]
train() client id: f_00008-12-2 loss: 0.696123  [   96/  130]
train() client id: f_00008-12-3 loss: 0.658188  [  128/  130]
train() client id: f_00009-0-0 loss: 1.063983  [   32/  118]
train() client id: f_00009-0-1 loss: 1.044140  [   64/  118]
train() client id: f_00009-0-2 loss: 1.152391  [   96/  118]
train() client id: f_00009-1-0 loss: 1.039583  [   32/  118]
train() client id: f_00009-1-1 loss: 1.130895  [   64/  118]
train() client id: f_00009-1-2 loss: 0.953349  [   96/  118]
train() client id: f_00009-2-0 loss: 0.957469  [   32/  118]
train() client id: f_00009-2-1 loss: 1.117347  [   64/  118]
train() client id: f_00009-2-2 loss: 0.894245  [   96/  118]
train() client id: f_00009-3-0 loss: 0.906021  [   32/  118]
train() client id: f_00009-3-1 loss: 0.921148  [   64/  118]
train() client id: f_00009-3-2 loss: 1.144808  [   96/  118]
train() client id: f_00009-4-0 loss: 0.860189  [   32/  118]
train() client id: f_00009-4-1 loss: 0.989700  [   64/  118]
train() client id: f_00009-4-2 loss: 1.062493  [   96/  118]
train() client id: f_00009-5-0 loss: 0.945540  [   32/  118]
train() client id: f_00009-5-1 loss: 0.800134  [   64/  118]
train() client id: f_00009-5-2 loss: 1.013673  [   96/  118]
train() client id: f_00009-6-0 loss: 0.849447  [   32/  118]
train() client id: f_00009-6-1 loss: 0.940991  [   64/  118]
train() client id: f_00009-6-2 loss: 0.921758  [   96/  118]
train() client id: f_00009-7-0 loss: 0.944422  [   32/  118]
train() client id: f_00009-7-1 loss: 0.853928  [   64/  118]
train() client id: f_00009-7-2 loss: 0.950494  [   96/  118]
train() client id: f_00009-8-0 loss: 0.911833  [   32/  118]
train() client id: f_00009-8-1 loss: 0.873857  [   64/  118]
train() client id: f_00009-8-2 loss: 0.887309  [   96/  118]
train() client id: f_00009-9-0 loss: 0.821923  [   32/  118]
train() client id: f_00009-9-1 loss: 0.902482  [   64/  118]
train() client id: f_00009-9-2 loss: 0.906281  [   96/  118]
train() client id: f_00009-10-0 loss: 1.052685  [   32/  118]
train() client id: f_00009-10-1 loss: 0.810815  [   64/  118]
train() client id: f_00009-10-2 loss: 0.736136  [   96/  118]
train() client id: f_00009-11-0 loss: 0.831593  [   32/  118]
train() client id: f_00009-11-1 loss: 0.811679  [   64/  118]
train() client id: f_00009-11-2 loss: 0.944600  [   96/  118]
train() client id: f_00009-12-0 loss: 0.870982  [   32/  118]
train() client id: f_00009-12-1 loss: 0.848846  [   64/  118]
train() client id: f_00009-12-2 loss: 0.927015  [   96/  118]
At round 19 accuracy: 0.636604774535809
At round 19 training accuracy: 0.5788061703554661
At round 19 training loss: 0.8490300481716325
update_location
xs = 8.927491 216.223621 5.882650 10.934260 -132.581990 19.769243 -5.849135 -5.143845 -155.120581 20.134486 
ys = -207.390647 7.291448 105.684448 -47.290817 -9.642386 0.794442 -56.381692 101.628436 25.881276 -642.232496 
xs mean: -1.6823799705212141
ys mean: -72.16579882624053
dists_uav = 230.413933 238.339714 145.615275 111.157453 166.345903 101.938482 114.948282 142.670243 186.365864 650.282997 
uav_gains = -109.734358 -110.340235 -104.083242 -101.148537 -105.542625 -100.208475 -101.512679 -103.860668 -106.828611 -127.267008 
uav_gains_db_mean: -107.05264380618956
dists_bs = 424.325263 425.654884 193.709005 289.801236 189.452101 261.309461 286.617345 185.025673 150.437977 840.206177 
bs_gains = -113.142628 -113.180673 -103.607254 -108.505866 -103.337043 -107.247403 -108.371528 -103.049555 -100.533069 -121.449864 
bs_gains_db_mean: -108.24248815719366
Round 20
-------------------------------
ene_coms = [0.01049863 0.01094093 0.00740931 0.00661771 0.00731335 0.00635742
 0.00672428 0.00721389 0.00644498 0.03393495]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.41429929 17.45671572  8.21776362  2.93693894 20.04292802  9.62476397
  3.65406779 11.78771648  8.58689393  8.3994071 ]
obj_prev = 99.12149484755476
eta_min = 7.19178723275621e-12	eta_max = 0.7460558002685413
af = 20.82950184691336	bf = 2.2068790046898963	zeta = 22.912452031604698	eta = 0.9090909090909091
af = 20.82950184691336	bf = 2.2068790046898963	zeta = 46.11356828049099	eta = 0.4517000662411454
af = 20.82950184691336	bf = 2.2068790046898963	zeta = 34.21640485587947	eta = 0.6087577562472695
af = 20.82950184691336	bf = 2.2068790046898963	zeta = 32.06526687453652	eta = 0.649597021238403
af = 20.82950184691336	bf = 2.2068790046898963	zeta = 31.94262276488066	eta = 0.6520911573302105
af = 20.82950184691336	bf = 2.2068790046898963	zeta = 31.94218371257475	eta = 0.652100120465883
af = 20.82950184691336	bf = 2.2068790046898963	zeta = 31.942183706919007	eta = 0.652100120581345
eta = 0.652100120581345
ene_coms = [0.01049863 0.01094093 0.00740931 0.00661771 0.00731335 0.00635742
 0.00672428 0.00721389 0.00644498 0.03393495]
ene_comp = [0.03528099 0.07420209 0.03472098 0.01204034 0.08568241 0.04088115
 0.01512043 0.05012141 0.03640102 0.03304093]
ene_total = [2.8070043  5.22059437 2.58324309 1.14402942 5.7020896  2.89646098
 1.33942078 3.51554762 2.62712808 4.10666546]
ti_comp = [0.36767144 0.36324838 0.39856464 0.40648057 0.39952424 0.40908346
 0.40541493 0.40051881 0.40820788 0.13330821]
ti_coms = [0.10498626 0.10940932 0.07409306 0.06617713 0.07313346 0.06357424
 0.06724277 0.07213889 0.06444982 0.33934949]
t_total = [28.9812851 28.9812851 28.9812851 28.9812851 28.9812851 28.9812851
 28.9812851 28.9812851 28.9812851 28.9812851]
ene_coms = [0.01049863 0.01094093 0.00740931 0.00661771 0.00731335 0.00635742
 0.00672428 0.00721389 0.00644498 0.03393495]
ene_comp = [2.03040480e-05 1.93517843e-04 1.64686612e-05 6.60262389e-07
 2.46302482e-04 2.55167581e-05 1.31453628e-06 4.90573438e-05
 1.80908119e-05 1.26859510e-04]
ene_total = [0.64497441 0.68271532 0.45531573 0.40580944 0.4635243  0.39137375
 0.41238359 0.44533181 0.39628709 2.08851986]
optimize_network iter = 0 obj = 6.386235296114965
eta = 0.652100120581345
freqs = [4.79789691e+07 1.02136845e+08 4.35575230e+07 1.48104727e+07
 1.07230554e+08 4.99667601e+07 1.86480866e+07 6.25706071e+07
 4.45863809e+07 1.23926841e+08]
eta_min = 0.6521001205813501	eta_max = 0.652100120581341
af = 0.03482922477742368	bf = 2.2068790046898963	zeta = 0.03831214725516605	eta = 0.909090909090909
af = 0.03482922477742368	bf = 2.2068790046898963	zeta = 24.312184428024615	eta = 0.0014325831099436745
af = 0.03482922477742368	bf = 2.2068790046898963	zeta = 2.438443657858908	eta = 0.014283383036213236
af = 0.03482922477742368	bf = 2.2068790046898963	zeta = 2.3889791894258665	eta = 0.014579124394044657
af = 0.03482922477742368	bf = 2.2068790046898963	zeta = 2.388971817571779	eta = 0.01457916938209222
eta = 0.01457916938209222
ene_coms = [0.01049863 0.01094093 0.00740931 0.00661771 0.00731335 0.00635742
 0.00672428 0.00721389 0.00644498 0.03393495]
ene_comp = [2.00789042e-04 1.91371998e-03 1.62860465e-04 6.52940993e-06
 2.43571328e-03 2.52338126e-04 1.29995989e-05 4.85133658e-04
 1.78902098e-04 1.25452814e-03]
ene_total = [0.2316133  0.27826831 0.16391684 0.14339686 0.21104067 0.1430834
 0.14584374 0.16666293 0.1433891  0.76175666]
ti_comp = [0.36767144 0.36324838 0.39856464 0.40648057 0.39952424 0.40908346
 0.40541493 0.40051881 0.40820788 0.13330821]
ti_coms = [0.10498626 0.10940932 0.07409306 0.06617713 0.07313346 0.06357424
 0.06724277 0.07213889 0.06444982 0.33934949]
t_total = [28.9812851 28.9812851 28.9812851 28.9812851 28.9812851 28.9812851
 28.9812851 28.9812851 28.9812851 28.9812851]
ene_coms = [0.01049863 0.01094093 0.00740931 0.00661771 0.00731335 0.00635742
 0.00672428 0.00721389 0.00644498 0.03393495]
ene_comp = [2.03040480e-05 1.93517843e-04 1.64686612e-05 6.60262389e-07
 2.46302482e-04 2.55167581e-05 1.31453628e-06 4.90573438e-05
 1.80908119e-05 1.26859510e-04]
ene_total = [0.64497441 0.68271532 0.45531573 0.40580944 0.4635243  0.39137375
 0.41238359 0.44533181 0.39628709 2.08851986]
optimize_network iter = 1 obj = 6.386235296115059
eta = 0.6521001205813501
freqs = [4.79789691e+07 1.02136845e+08 4.35575230e+07 1.48104727e+07
 1.07230554e+08 4.99667601e+07 1.86480866e+07 6.25706071e+07
 4.45863809e+07 1.23926841e+08]
Done!
ene_coms = [0.01049863 0.01094093 0.00740931 0.00661771 0.00731335 0.00635742
 0.00672428 0.00721389 0.00644498 0.03393495]
ene_comp = [2.03034766e-05 1.93512398e-04 1.64681978e-05 6.60243808e-07
 2.46295551e-04 2.55160401e-05 1.31449928e-06 4.90559633e-05
 1.80903028e-05 1.26855940e-04]
ene_total = [0.01051893 0.01113444 0.00742577 0.00661837 0.00755964 0.00638294
 0.00672559 0.00726294 0.00646307 0.03406181]
At round 20 energy consumption: 0.1041535150598907
At round 20 eta: 0.6521001205813501
At round 20 a_n: 21.33168591686803
At round 20 local rounds: 14.000393985941368
At round 20 global rounds: 61.315588704755676
gradient difference: 0.47089236974716187
train() client id: f_00000-0-0 loss: 1.378749  [   32/  126]
train() client id: f_00000-0-1 loss: 1.444887  [   64/  126]
train() client id: f_00000-0-2 loss: 1.376014  [   96/  126]
train() client id: f_00000-1-0 loss: 1.201246  [   32/  126]
train() client id: f_00000-1-1 loss: 1.212896  [   64/  126]
train() client id: f_00000-1-2 loss: 1.368669  [   96/  126]
train() client id: f_00000-2-0 loss: 1.124064  [   32/  126]
train() client id: f_00000-2-1 loss: 1.152092  [   64/  126]
train() client id: f_00000-2-2 loss: 1.242176  [   96/  126]
train() client id: f_00000-3-0 loss: 1.235169  [   32/  126]
train() client id: f_00000-3-1 loss: 1.093536  [   64/  126]
train() client id: f_00000-3-2 loss: 1.080286  [   96/  126]
train() client id: f_00000-4-0 loss: 1.146587  [   32/  126]
train() client id: f_00000-4-1 loss: 0.919506  [   64/  126]
train() client id: f_00000-4-2 loss: 1.095109  [   96/  126]
train() client id: f_00000-5-0 loss: 1.028778  [   32/  126]
train() client id: f_00000-5-1 loss: 0.913055  [   64/  126]
train() client id: f_00000-5-2 loss: 1.012078  [   96/  126]
train() client id: f_00000-6-0 loss: 1.057979  [   32/  126]
train() client id: f_00000-6-1 loss: 0.859337  [   64/  126]
train() client id: f_00000-6-2 loss: 0.965521  [   96/  126]
train() client id: f_00000-7-0 loss: 1.015986  [   32/  126]
train() client id: f_00000-7-1 loss: 0.919922  [   64/  126]
train() client id: f_00000-7-2 loss: 1.069643  [   96/  126]
train() client id: f_00000-8-0 loss: 0.907084  [   32/  126]
train() client id: f_00000-8-1 loss: 1.058705  [   64/  126]
train() client id: f_00000-8-2 loss: 0.882380  [   96/  126]
train() client id: f_00000-9-0 loss: 1.137120  [   32/  126]
train() client id: f_00000-9-1 loss: 0.886392  [   64/  126]
train() client id: f_00000-9-2 loss: 0.892417  [   96/  126]
train() client id: f_00000-10-0 loss: 0.948849  [   32/  126]
train() client id: f_00000-10-1 loss: 0.838709  [   64/  126]
train() client id: f_00000-10-2 loss: 1.024524  [   96/  126]
train() client id: f_00000-11-0 loss: 0.928434  [   32/  126]
train() client id: f_00000-11-1 loss: 0.987872  [   64/  126]
train() client id: f_00000-11-2 loss: 0.960021  [   96/  126]
train() client id: f_00000-12-0 loss: 0.970519  [   32/  126]
train() client id: f_00000-12-1 loss: 0.862786  [   64/  126]
train() client id: f_00000-12-2 loss: 0.987736  [   96/  126]
train() client id: f_00000-13-0 loss: 0.939274  [   32/  126]
train() client id: f_00000-13-1 loss: 0.899785  [   64/  126]
train() client id: f_00000-13-2 loss: 0.949976  [   96/  126]
train() client id: f_00001-0-0 loss: 0.496652  [   32/  265]
train() client id: f_00001-0-1 loss: 0.523870  [   64/  265]
train() client id: f_00001-0-2 loss: 0.510121  [   96/  265]
train() client id: f_00001-0-3 loss: 0.461099  [  128/  265]
train() client id: f_00001-0-4 loss: 0.388569  [  160/  265]
train() client id: f_00001-0-5 loss: 0.395165  [  192/  265]
train() client id: f_00001-0-6 loss: 0.464037  [  224/  265]
train() client id: f_00001-0-7 loss: 0.399571  [  256/  265]
train() client id: f_00001-1-0 loss: 0.395266  [   32/  265]
train() client id: f_00001-1-1 loss: 0.537492  [   64/  265]
train() client id: f_00001-1-2 loss: 0.428804  [   96/  265]
train() client id: f_00001-1-3 loss: 0.540685  [  128/  265]
train() client id: f_00001-1-4 loss: 0.434637  [  160/  265]
train() client id: f_00001-1-5 loss: 0.356205  [  192/  265]
train() client id: f_00001-1-6 loss: 0.399226  [  224/  265]
train() client id: f_00001-1-7 loss: 0.471815  [  256/  265]
train() client id: f_00001-2-0 loss: 0.367648  [   32/  265]
train() client id: f_00001-2-1 loss: 0.425341  [   64/  265]
train() client id: f_00001-2-2 loss: 0.478812  [   96/  265]
train() client id: f_00001-2-3 loss: 0.497675  [  128/  265]
train() client id: f_00001-2-4 loss: 0.340299  [  160/  265]
train() client id: f_00001-2-5 loss: 0.478385  [  192/  265]
train() client id: f_00001-2-6 loss: 0.502953  [  224/  265]
train() client id: f_00001-2-7 loss: 0.424747  [  256/  265]
train() client id: f_00001-3-0 loss: 0.493398  [   32/  265]
train() client id: f_00001-3-1 loss: 0.478839  [   64/  265]
train() client id: f_00001-3-2 loss: 0.335690  [   96/  265]
train() client id: f_00001-3-3 loss: 0.513526  [  128/  265]
train() client id: f_00001-3-4 loss: 0.424572  [  160/  265]
train() client id: f_00001-3-5 loss: 0.344456  [  192/  265]
train() client id: f_00001-3-6 loss: 0.492904  [  224/  265]
train() client id: f_00001-3-7 loss: 0.393445  [  256/  265]
train() client id: f_00001-4-0 loss: 0.475153  [   32/  265]
train() client id: f_00001-4-1 loss: 0.356158  [   64/  265]
train() client id: f_00001-4-2 loss: 0.445733  [   96/  265]
train() client id: f_00001-4-3 loss: 0.565055  [  128/  265]
train() client id: f_00001-4-4 loss: 0.331828  [  160/  265]
train() client id: f_00001-4-5 loss: 0.456280  [  192/  265]
train() client id: f_00001-4-6 loss: 0.373117  [  224/  265]
train() client id: f_00001-4-7 loss: 0.430144  [  256/  265]
train() client id: f_00001-5-0 loss: 0.415087  [   32/  265]
train() client id: f_00001-5-1 loss: 0.463520  [   64/  265]
train() client id: f_00001-5-2 loss: 0.461991  [   96/  265]
train() client id: f_00001-5-3 loss: 0.340153  [  128/  265]
train() client id: f_00001-5-4 loss: 0.390995  [  160/  265]
train() client id: f_00001-5-5 loss: 0.541969  [  192/  265]
train() client id: f_00001-5-6 loss: 0.335556  [  224/  265]
train() client id: f_00001-5-7 loss: 0.473402  [  256/  265]
train() client id: f_00001-6-0 loss: 0.373089  [   32/  265]
train() client id: f_00001-6-1 loss: 0.611090  [   64/  265]
train() client id: f_00001-6-2 loss: 0.337112  [   96/  265]
train() client id: f_00001-6-3 loss: 0.505953  [  128/  265]
train() client id: f_00001-6-4 loss: 0.326629  [  160/  265]
train() client id: f_00001-6-5 loss: 0.358236  [  192/  265]
train() client id: f_00001-6-6 loss: 0.498167  [  224/  265]
train() client id: f_00001-6-7 loss: 0.390323  [  256/  265]
train() client id: f_00001-7-0 loss: 0.403308  [   32/  265]
train() client id: f_00001-7-1 loss: 0.319802  [   64/  265]
train() client id: f_00001-7-2 loss: 0.435995  [   96/  265]
train() client id: f_00001-7-3 loss: 0.445966  [  128/  265]
train() client id: f_00001-7-4 loss: 0.330377  [  160/  265]
train() client id: f_00001-7-5 loss: 0.492648  [  192/  265]
train() client id: f_00001-7-6 loss: 0.408236  [  224/  265]
train() client id: f_00001-7-7 loss: 0.550844  [  256/  265]
train() client id: f_00001-8-0 loss: 0.545443  [   32/  265]
train() client id: f_00001-8-1 loss: 0.498292  [   64/  265]
train() client id: f_00001-8-2 loss: 0.339226  [   96/  265]
train() client id: f_00001-8-3 loss: 0.313157  [  128/  265]
train() client id: f_00001-8-4 loss: 0.349618  [  160/  265]
train() client id: f_00001-8-5 loss: 0.361310  [  192/  265]
train() client id: f_00001-8-6 loss: 0.459626  [  224/  265]
train() client id: f_00001-8-7 loss: 0.457304  [  256/  265]
train() client id: f_00001-9-0 loss: 0.412309  [   32/  265]
train() client id: f_00001-9-1 loss: 0.311777  [   64/  265]
train() client id: f_00001-9-2 loss: 0.400258  [   96/  265]
train() client id: f_00001-9-3 loss: 0.391114  [  128/  265]
train() client id: f_00001-9-4 loss: 0.411392  [  160/  265]
train() client id: f_00001-9-5 loss: 0.381473  [  192/  265]
train() client id: f_00001-9-6 loss: 0.489674  [  224/  265]
train() client id: f_00001-9-7 loss: 0.506959  [  256/  265]
train() client id: f_00001-10-0 loss: 0.385103  [   32/  265]
train() client id: f_00001-10-1 loss: 0.384556  [   64/  265]
train() client id: f_00001-10-2 loss: 0.370618  [   96/  265]
train() client id: f_00001-10-3 loss: 0.375526  [  128/  265]
train() client id: f_00001-10-4 loss: 0.463799  [  160/  265]
train() client id: f_00001-10-5 loss: 0.461646  [  192/  265]
train() client id: f_00001-10-6 loss: 0.322719  [  224/  265]
train() client id: f_00001-10-7 loss: 0.599835  [  256/  265]
train() client id: f_00001-11-0 loss: 0.478400  [   32/  265]
train() client id: f_00001-11-1 loss: 0.336127  [   64/  265]
train() client id: f_00001-11-2 loss: 0.383188  [   96/  265]
train() client id: f_00001-11-3 loss: 0.414916  [  128/  265]
train() client id: f_00001-11-4 loss: 0.431884  [  160/  265]
train() client id: f_00001-11-5 loss: 0.473362  [  192/  265]
train() client id: f_00001-11-6 loss: 0.534690  [  224/  265]
train() client id: f_00001-11-7 loss: 0.330940  [  256/  265]
train() client id: f_00001-12-0 loss: 0.501660  [   32/  265]
train() client id: f_00001-12-1 loss: 0.376508  [   64/  265]
train() client id: f_00001-12-2 loss: 0.514242  [   96/  265]
train() client id: f_00001-12-3 loss: 0.479162  [  128/  265]
train() client id: f_00001-12-4 loss: 0.320554  [  160/  265]
train() client id: f_00001-12-5 loss: 0.354569  [  192/  265]
train() client id: f_00001-12-6 loss: 0.348061  [  224/  265]
train() client id: f_00001-12-7 loss: 0.410415  [  256/  265]
train() client id: f_00001-13-0 loss: 0.594377  [   32/  265]
train() client id: f_00001-13-1 loss: 0.384298  [   64/  265]
train() client id: f_00001-13-2 loss: 0.324871  [   96/  265]
train() client id: f_00001-13-3 loss: 0.472529  [  128/  265]
train() client id: f_00001-13-4 loss: 0.428694  [  160/  265]
train() client id: f_00001-13-5 loss: 0.394110  [  192/  265]
train() client id: f_00001-13-6 loss: 0.328900  [  224/  265]
train() client id: f_00001-13-7 loss: 0.385348  [  256/  265]
train() client id: f_00002-0-0 loss: 1.243455  [   32/  124]
train() client id: f_00002-0-1 loss: 1.190623  [   64/  124]
train() client id: f_00002-0-2 loss: 1.151897  [   96/  124]
train() client id: f_00002-1-0 loss: 1.171379  [   32/  124]
train() client id: f_00002-1-1 loss: 1.056267  [   64/  124]
train() client id: f_00002-1-2 loss: 1.354171  [   96/  124]
train() client id: f_00002-2-0 loss: 1.091013  [   32/  124]
train() client id: f_00002-2-1 loss: 1.219424  [   64/  124]
train() client id: f_00002-2-2 loss: 1.088784  [   96/  124]
train() client id: f_00002-3-0 loss: 1.000617  [   32/  124]
train() client id: f_00002-3-1 loss: 1.216445  [   64/  124]
train() client id: f_00002-3-2 loss: 0.908288  [   96/  124]
train() client id: f_00002-4-0 loss: 1.100715  [   32/  124]
train() client id: f_00002-4-1 loss: 0.978097  [   64/  124]
train() client id: f_00002-4-2 loss: 1.076720  [   96/  124]
train() client id: f_00002-5-0 loss: 1.234539  [   32/  124]
train() client id: f_00002-5-1 loss: 0.979697  [   64/  124]
train() client id: f_00002-5-2 loss: 0.985077  [   96/  124]
train() client id: f_00002-6-0 loss: 1.065359  [   32/  124]
train() client id: f_00002-6-1 loss: 1.053539  [   64/  124]
train() client id: f_00002-6-2 loss: 0.997668  [   96/  124]
train() client id: f_00002-7-0 loss: 0.973086  [   32/  124]
train() client id: f_00002-7-1 loss: 0.994052  [   64/  124]
train() client id: f_00002-7-2 loss: 1.084576  [   96/  124]
train() client id: f_00002-8-0 loss: 0.924412  [   32/  124]
train() client id: f_00002-8-1 loss: 0.968916  [   64/  124]
train() client id: f_00002-8-2 loss: 0.988671  [   96/  124]
train() client id: f_00002-9-0 loss: 0.999137  [   32/  124]
train() client id: f_00002-9-1 loss: 0.978921  [   64/  124]
train() client id: f_00002-9-2 loss: 0.985999  [   96/  124]
train() client id: f_00002-10-0 loss: 0.902629  [   32/  124]
train() client id: f_00002-10-1 loss: 1.055842  [   64/  124]
train() client id: f_00002-10-2 loss: 1.042551  [   96/  124]
train() client id: f_00002-11-0 loss: 1.142301  [   32/  124]
train() client id: f_00002-11-1 loss: 0.950004  [   64/  124]
train() client id: f_00002-11-2 loss: 0.896226  [   96/  124]
train() client id: f_00002-12-0 loss: 1.172218  [   32/  124]
train() client id: f_00002-12-1 loss: 0.947991  [   64/  124]
train() client id: f_00002-12-2 loss: 0.952206  [   96/  124]
train() client id: f_00002-13-0 loss: 0.891693  [   32/  124]
train() client id: f_00002-13-1 loss: 0.970842  [   64/  124]
train() client id: f_00002-13-2 loss: 1.017700  [   96/  124]
train() client id: f_00003-0-0 loss: 0.730547  [   32/   43]
train() client id: f_00003-1-0 loss: 0.649634  [   32/   43]
train() client id: f_00003-2-0 loss: 0.648378  [   32/   43]
train() client id: f_00003-3-0 loss: 0.638159  [   32/   43]
train() client id: f_00003-4-0 loss: 0.542351  [   32/   43]
train() client id: f_00003-5-0 loss: 0.796620  [   32/   43]
train() client id: f_00003-6-0 loss: 0.770572  [   32/   43]
train() client id: f_00003-7-0 loss: 0.644408  [   32/   43]
train() client id: f_00003-8-0 loss: 0.655960  [   32/   43]
train() client id: f_00003-9-0 loss: 0.661988  [   32/   43]
train() client id: f_00003-10-0 loss: 0.664908  [   32/   43]
train() client id: f_00003-11-0 loss: 0.714150  [   32/   43]
train() client id: f_00003-12-0 loss: 0.606152  [   32/   43]
train() client id: f_00003-13-0 loss: 0.635137  [   32/   43]
train() client id: f_00004-0-0 loss: 0.842630  [   32/  306]
train() client id: f_00004-0-1 loss: 0.850321  [   64/  306]
train() client id: f_00004-0-2 loss: 1.052434  [   96/  306]
train() client id: f_00004-0-3 loss: 0.908883  [  128/  306]
train() client id: f_00004-0-4 loss: 0.926709  [  160/  306]
train() client id: f_00004-0-5 loss: 0.920826  [  192/  306]
train() client id: f_00004-0-6 loss: 0.906062  [  224/  306]
train() client id: f_00004-0-7 loss: 0.900331  [  256/  306]
train() client id: f_00004-0-8 loss: 0.878780  [  288/  306]
train() client id: f_00004-1-0 loss: 0.912028  [   32/  306]
train() client id: f_00004-1-1 loss: 0.907088  [   64/  306]
train() client id: f_00004-1-2 loss: 0.942787  [   96/  306]
train() client id: f_00004-1-3 loss: 0.885320  [  128/  306]
train() client id: f_00004-1-4 loss: 0.914840  [  160/  306]
train() client id: f_00004-1-5 loss: 0.854902  [  192/  306]
train() client id: f_00004-1-6 loss: 0.902655  [  224/  306]
train() client id: f_00004-1-7 loss: 0.988453  [  256/  306]
train() client id: f_00004-1-8 loss: 0.848724  [  288/  306]
train() client id: f_00004-2-0 loss: 0.816689  [   32/  306]
train() client id: f_00004-2-1 loss: 0.876657  [   64/  306]
train() client id: f_00004-2-2 loss: 0.822017  [   96/  306]
train() client id: f_00004-2-3 loss: 0.969970  [  128/  306]
train() client id: f_00004-2-4 loss: 0.957863  [  160/  306]
train() client id: f_00004-2-5 loss: 0.922046  [  192/  306]
train() client id: f_00004-2-6 loss: 0.818678  [  224/  306]
train() client id: f_00004-2-7 loss: 0.875530  [  256/  306]
train() client id: f_00004-2-8 loss: 0.950462  [  288/  306]
train() client id: f_00004-3-0 loss: 0.881392  [   32/  306]
train() client id: f_00004-3-1 loss: 0.855155  [   64/  306]
train() client id: f_00004-3-2 loss: 0.923401  [   96/  306]
train() client id: f_00004-3-3 loss: 0.859578  [  128/  306]
train() client id: f_00004-3-4 loss: 0.845319  [  160/  306]
train() client id: f_00004-3-5 loss: 0.865637  [  192/  306]
train() client id: f_00004-3-6 loss: 1.074914  [  224/  306]
train() client id: f_00004-3-7 loss: 0.990597  [  256/  306]
train() client id: f_00004-3-8 loss: 0.820784  [  288/  306]
train() client id: f_00004-4-0 loss: 0.863684  [   32/  306]
train() client id: f_00004-4-1 loss: 0.797938  [   64/  306]
train() client id: f_00004-4-2 loss: 0.809187  [   96/  306]
train() client id: f_00004-4-3 loss: 0.986520  [  128/  306]
train() client id: f_00004-4-4 loss: 0.971532  [  160/  306]
train() client id: f_00004-4-5 loss: 0.880555  [  192/  306]
train() client id: f_00004-4-6 loss: 0.919552  [  224/  306]
train() client id: f_00004-4-7 loss: 0.888834  [  256/  306]
train() client id: f_00004-4-8 loss: 0.865469  [  288/  306]
train() client id: f_00004-5-0 loss: 1.092681  [   32/  306]
train() client id: f_00004-5-1 loss: 0.838298  [   64/  306]
train() client id: f_00004-5-2 loss: 0.782663  [   96/  306]
train() client id: f_00004-5-3 loss: 0.829203  [  128/  306]
train() client id: f_00004-5-4 loss: 0.858656  [  160/  306]
train() client id: f_00004-5-5 loss: 0.834202  [  192/  306]
train() client id: f_00004-5-6 loss: 0.816920  [  224/  306]
train() client id: f_00004-5-7 loss: 0.859109  [  256/  306]
train() client id: f_00004-5-8 loss: 0.992563  [  288/  306]
train() client id: f_00004-6-0 loss: 0.830235  [   32/  306]
train() client id: f_00004-6-1 loss: 0.894193  [   64/  306]
train() client id: f_00004-6-2 loss: 1.000162  [   96/  306]
train() client id: f_00004-6-3 loss: 0.934577  [  128/  306]
train() client id: f_00004-6-4 loss: 0.827675  [  160/  306]
train() client id: f_00004-6-5 loss: 1.012356  [  192/  306]
train() client id: f_00004-6-6 loss: 0.924502  [  224/  306]
train() client id: f_00004-6-7 loss: 0.820568  [  256/  306]
train() client id: f_00004-6-8 loss: 0.787088  [  288/  306]
train() client id: f_00004-7-0 loss: 0.791710  [   32/  306]
train() client id: f_00004-7-1 loss: 0.885964  [   64/  306]
train() client id: f_00004-7-2 loss: 0.828108  [   96/  306]
train() client id: f_00004-7-3 loss: 0.848818  [  128/  306]
train() client id: f_00004-7-4 loss: 0.902999  [  160/  306]
train() client id: f_00004-7-5 loss: 0.807762  [  192/  306]
train() client id: f_00004-7-6 loss: 0.986300  [  224/  306]
train() client id: f_00004-7-7 loss: 0.932824  [  256/  306]
train() client id: f_00004-7-8 loss: 0.864506  [  288/  306]
train() client id: f_00004-8-0 loss: 0.836436  [   32/  306]
train() client id: f_00004-8-1 loss: 0.799351  [   64/  306]
train() client id: f_00004-8-2 loss: 0.876216  [   96/  306]
train() client id: f_00004-8-3 loss: 0.941426  [  128/  306]
train() client id: f_00004-8-4 loss: 1.036324  [  160/  306]
train() client id: f_00004-8-5 loss: 0.895550  [  192/  306]
train() client id: f_00004-8-6 loss: 0.860671  [  224/  306]
train() client id: f_00004-8-7 loss: 0.849115  [  256/  306]
train() client id: f_00004-8-8 loss: 0.826709  [  288/  306]
train() client id: f_00004-9-0 loss: 0.771809  [   32/  306]
train() client id: f_00004-9-1 loss: 0.893620  [   64/  306]
train() client id: f_00004-9-2 loss: 0.946211  [   96/  306]
train() client id: f_00004-9-3 loss: 0.866354  [  128/  306]
train() client id: f_00004-9-4 loss: 1.017196  [  160/  306]
train() client id: f_00004-9-5 loss: 0.844763  [  192/  306]
train() client id: f_00004-9-6 loss: 0.974974  [  224/  306]
train() client id: f_00004-9-7 loss: 0.927860  [  256/  306]
train() client id: f_00004-9-8 loss: 0.753870  [  288/  306]
train() client id: f_00004-10-0 loss: 0.910704  [   32/  306]
train() client id: f_00004-10-1 loss: 0.875251  [   64/  306]
train() client id: f_00004-10-2 loss: 0.983017  [   96/  306]
train() client id: f_00004-10-3 loss: 0.876587  [  128/  306]
train() client id: f_00004-10-4 loss: 0.842925  [  160/  306]
train() client id: f_00004-10-5 loss: 0.781589  [  192/  306]
train() client id: f_00004-10-6 loss: 0.889181  [  224/  306]
train() client id: f_00004-10-7 loss: 0.864502  [  256/  306]
train() client id: f_00004-10-8 loss: 0.713788  [  288/  306]
train() client id: f_00004-11-0 loss: 0.864310  [   32/  306]
train() client id: f_00004-11-1 loss: 0.762555  [   64/  306]
train() client id: f_00004-11-2 loss: 0.943676  [   96/  306]
train() client id: f_00004-11-3 loss: 0.807963  [  128/  306]
train() client id: f_00004-11-4 loss: 0.889769  [  160/  306]
train() client id: f_00004-11-5 loss: 0.795171  [  192/  306]
train() client id: f_00004-11-6 loss: 0.972677  [  224/  306]
train() client id: f_00004-11-7 loss: 1.010819  [  256/  306]
train() client id: f_00004-11-8 loss: 0.762132  [  288/  306]
train() client id: f_00004-12-0 loss: 0.899221  [   32/  306]
train() client id: f_00004-12-1 loss: 0.879383  [   64/  306]
train() client id: f_00004-12-2 loss: 0.837708  [   96/  306]
train() client id: f_00004-12-3 loss: 0.769591  [  128/  306]
train() client id: f_00004-12-4 loss: 0.949585  [  160/  306]
train() client id: f_00004-12-5 loss: 0.903824  [  192/  306]
train() client id: f_00004-12-6 loss: 0.777151  [  224/  306]
train() client id: f_00004-12-7 loss: 0.934724  [  256/  306]
train() client id: f_00004-12-8 loss: 0.830227  [  288/  306]
train() client id: f_00004-13-0 loss: 0.740560  [   32/  306]
train() client id: f_00004-13-1 loss: 0.858424  [   64/  306]
train() client id: f_00004-13-2 loss: 0.986195  [   96/  306]
train() client id: f_00004-13-3 loss: 0.814938  [  128/  306]
train() client id: f_00004-13-4 loss: 0.967656  [  160/  306]
train() client id: f_00004-13-5 loss: 0.844803  [  192/  306]
train() client id: f_00004-13-6 loss: 0.830258  [  224/  306]
train() client id: f_00004-13-7 loss: 0.825084  [  256/  306]
train() client id: f_00004-13-8 loss: 0.878005  [  288/  306]
train() client id: f_00005-0-0 loss: 0.660124  [   32/  146]
train() client id: f_00005-0-1 loss: 0.506513  [   64/  146]
train() client id: f_00005-0-2 loss: 0.579051  [   96/  146]
train() client id: f_00005-0-3 loss: 0.684681  [  128/  146]
train() client id: f_00005-1-0 loss: 0.631573  [   32/  146]
train() client id: f_00005-1-1 loss: 0.820979  [   64/  146]
train() client id: f_00005-1-2 loss: 0.605865  [   96/  146]
train() client id: f_00005-1-3 loss: 0.438380  [  128/  146]
train() client id: f_00005-2-0 loss: 0.562968  [   32/  146]
train() client id: f_00005-2-1 loss: 0.814935  [   64/  146]
train() client id: f_00005-2-2 loss: 0.578087  [   96/  146]
train() client id: f_00005-2-3 loss: 0.530814  [  128/  146]
train() client id: f_00005-3-0 loss: 0.511570  [   32/  146]
train() client id: f_00005-3-1 loss: 0.657456  [   64/  146]
train() client id: f_00005-3-2 loss: 0.546106  [   96/  146]
train() client id: f_00005-3-3 loss: 0.484904  [  128/  146]
train() client id: f_00005-4-0 loss: 0.749912  [   32/  146]
train() client id: f_00005-4-1 loss: 0.642769  [   64/  146]
train() client id: f_00005-4-2 loss: 0.692635  [   96/  146]
train() client id: f_00005-4-3 loss: 0.541909  [  128/  146]
train() client id: f_00005-5-0 loss: 0.703110  [   32/  146]
train() client id: f_00005-5-1 loss: 0.378226  [   64/  146]
train() client id: f_00005-5-2 loss: 0.651355  [   96/  146]
train() client id: f_00005-5-3 loss: 0.697583  [  128/  146]
train() client id: f_00005-6-0 loss: 0.713739  [   32/  146]
train() client id: f_00005-6-1 loss: 0.489791  [   64/  146]
train() client id: f_00005-6-2 loss: 0.560116  [   96/  146]
train() client id: f_00005-6-3 loss: 0.673913  [  128/  146]
train() client id: f_00005-7-0 loss: 0.664877  [   32/  146]
train() client id: f_00005-7-1 loss: 0.798089  [   64/  146]
train() client id: f_00005-7-2 loss: 0.480840  [   96/  146]
train() client id: f_00005-7-3 loss: 0.619897  [  128/  146]
train() client id: f_00005-8-0 loss: 0.615177  [   32/  146]
train() client id: f_00005-8-1 loss: 0.450605  [   64/  146]
train() client id: f_00005-8-2 loss: 0.813410  [   96/  146]
train() client id: f_00005-8-3 loss: 0.520942  [  128/  146]
train() client id: f_00005-9-0 loss: 0.424727  [   32/  146]
train() client id: f_00005-9-1 loss: 0.524795  [   64/  146]
train() client id: f_00005-9-2 loss: 0.871462  [   96/  146]
train() client id: f_00005-9-3 loss: 0.513402  [  128/  146]
train() client id: f_00005-10-0 loss: 0.693472  [   32/  146]
train() client id: f_00005-10-1 loss: 0.613550  [   64/  146]
train() client id: f_00005-10-2 loss: 0.557211  [   96/  146]
train() client id: f_00005-10-3 loss: 0.575719  [  128/  146]
train() client id: f_00005-11-0 loss: 0.470025  [   32/  146]
train() client id: f_00005-11-1 loss: 0.613638  [   64/  146]
train() client id: f_00005-11-2 loss: 0.592929  [   96/  146]
train() client id: f_00005-11-3 loss: 0.852180  [  128/  146]
train() client id: f_00005-12-0 loss: 0.567560  [   32/  146]
train() client id: f_00005-12-1 loss: 0.638102  [   64/  146]
train() client id: f_00005-12-2 loss: 0.625294  [   96/  146]
train() client id: f_00005-12-3 loss: 0.647547  [  128/  146]
train() client id: f_00005-13-0 loss: 0.602414  [   32/  146]
train() client id: f_00005-13-1 loss: 0.574978  [   64/  146]
train() client id: f_00005-13-2 loss: 0.736782  [   96/  146]
train() client id: f_00005-13-3 loss: 0.560729  [  128/  146]
train() client id: f_00006-0-0 loss: 0.600423  [   32/   54]
train() client id: f_00006-1-0 loss: 0.502175  [   32/   54]
train() client id: f_00006-2-0 loss: 0.586966  [   32/   54]
train() client id: f_00006-3-0 loss: 0.591378  [   32/   54]
train() client id: f_00006-4-0 loss: 0.599676  [   32/   54]
train() client id: f_00006-5-0 loss: 0.589414  [   32/   54]
train() client id: f_00006-6-0 loss: 0.527756  [   32/   54]
train() client id: f_00006-7-0 loss: 0.527648  [   32/   54]
train() client id: f_00006-8-0 loss: 0.493823  [   32/   54]
train() client id: f_00006-9-0 loss: 0.544678  [   32/   54]
train() client id: f_00006-10-0 loss: 0.544444  [   32/   54]
train() client id: f_00006-11-0 loss: 0.553341  [   32/   54]
train() client id: f_00006-12-0 loss: 0.497504  [   32/   54]
train() client id: f_00006-13-0 loss: 0.593625  [   32/   54]
train() client id: f_00007-0-0 loss: 0.631682  [   32/  179]
train() client id: f_00007-0-1 loss: 0.345913  [   64/  179]
train() client id: f_00007-0-2 loss: 0.749410  [   96/  179]
train() client id: f_00007-0-3 loss: 0.599495  [  128/  179]
train() client id: f_00007-0-4 loss: 0.360238  [  160/  179]
train() client id: f_00007-1-0 loss: 0.581631  [   32/  179]
train() client id: f_00007-1-1 loss: 0.532047  [   64/  179]
train() client id: f_00007-1-2 loss: 0.483556  [   96/  179]
train() client id: f_00007-1-3 loss: 0.440807  [  128/  179]
train() client id: f_00007-1-4 loss: 0.611492  [  160/  179]
train() client id: f_00007-2-0 loss: 0.510710  [   32/  179]
train() client id: f_00007-2-1 loss: 0.512627  [   64/  179]
train() client id: f_00007-2-2 loss: 0.490233  [   96/  179]
train() client id: f_00007-2-3 loss: 0.408390  [  128/  179]
train() client id: f_00007-2-4 loss: 0.471927  [  160/  179]
train() client id: f_00007-3-0 loss: 0.424210  [   32/  179]
train() client id: f_00007-3-1 loss: 0.592910  [   64/  179]
train() client id: f_00007-3-2 loss: 0.487748  [   96/  179]
train() client id: f_00007-3-3 loss: 0.477779  [  128/  179]
train() client id: f_00007-3-4 loss: 0.512793  [  160/  179]
train() client id: f_00007-4-0 loss: 0.748367  [   32/  179]
train() client id: f_00007-4-1 loss: 0.370375  [   64/  179]
train() client id: f_00007-4-2 loss: 0.465002  [   96/  179]
train() client id: f_00007-4-3 loss: 0.385837  [  128/  179]
train() client id: f_00007-4-4 loss: 0.474342  [  160/  179]
train() client id: f_00007-5-0 loss: 0.497457  [   32/  179]
train() client id: f_00007-5-1 loss: 0.348171  [   64/  179]
train() client id: f_00007-5-2 loss: 0.463231  [   96/  179]
train() client id: f_00007-5-3 loss: 0.481044  [  128/  179]
train() client id: f_00007-5-4 loss: 0.620912  [  160/  179]
train() client id: f_00007-6-0 loss: 0.396243  [   32/  179]
train() client id: f_00007-6-1 loss: 0.356008  [   64/  179]
train() client id: f_00007-6-2 loss: 0.622560  [   96/  179]
train() client id: f_00007-6-3 loss: 0.501886  [  128/  179]
train() client id: f_00007-6-4 loss: 0.490709  [  160/  179]
train() client id: f_00007-7-0 loss: 0.394103  [   32/  179]
train() client id: f_00007-7-1 loss: 0.376055  [   64/  179]
train() client id: f_00007-7-2 loss: 0.421835  [   96/  179]
train() client id: f_00007-7-3 loss: 0.469356  [  128/  179]
train() client id: f_00007-7-4 loss: 0.473152  [  160/  179]
train() client id: f_00007-8-0 loss: 0.361733  [   32/  179]
train() client id: f_00007-8-1 loss: 0.568532  [   64/  179]
train() client id: f_00007-8-2 loss: 0.333763  [   96/  179]
train() client id: f_00007-8-3 loss: 0.282975  [  128/  179]
train() client id: f_00007-8-4 loss: 0.704310  [  160/  179]
train() client id: f_00007-9-0 loss: 0.456110  [   32/  179]
train() client id: f_00007-9-1 loss: 0.409487  [   64/  179]
train() client id: f_00007-9-2 loss: 0.484918  [   96/  179]
train() client id: f_00007-9-3 loss: 0.482557  [  128/  179]
train() client id: f_00007-9-4 loss: 0.405390  [  160/  179]
train() client id: f_00007-10-0 loss: 0.430675  [   32/  179]
train() client id: f_00007-10-1 loss: 0.388386  [   64/  179]
train() client id: f_00007-10-2 loss: 0.376924  [   96/  179]
train() client id: f_00007-10-3 loss: 0.405816  [  128/  179]
train() client id: f_00007-10-4 loss: 0.563590  [  160/  179]
train() client id: f_00007-11-0 loss: 0.430114  [   32/  179]
train() client id: f_00007-11-1 loss: 0.460987  [   64/  179]
train() client id: f_00007-11-2 loss: 0.417209  [   96/  179]
train() client id: f_00007-11-3 loss: 0.686868  [  128/  179]
train() client id: f_00007-11-4 loss: 0.307975  [  160/  179]
train() client id: f_00007-12-0 loss: 0.493485  [   32/  179]
train() client id: f_00007-12-1 loss: 0.306517  [   64/  179]
train() client id: f_00007-12-2 loss: 0.329299  [   96/  179]
train() client id: f_00007-12-3 loss: 0.512763  [  128/  179]
train() client id: f_00007-12-4 loss: 0.647913  [  160/  179]
train() client id: f_00007-13-0 loss: 0.324106  [   32/  179]
train() client id: f_00007-13-1 loss: 0.287997  [   64/  179]
train() client id: f_00007-13-2 loss: 0.493705  [   96/  179]
train() client id: f_00007-13-3 loss: 0.625828  [  128/  179]
train() client id: f_00007-13-4 loss: 0.481155  [  160/  179]
train() client id: f_00008-0-0 loss: 0.848790  [   32/  130]
train() client id: f_00008-0-1 loss: 0.804910  [   64/  130]
train() client id: f_00008-0-2 loss: 0.752829  [   96/  130]
train() client id: f_00008-0-3 loss: 0.838626  [  128/  130]
train() client id: f_00008-1-0 loss: 0.794575  [   32/  130]
train() client id: f_00008-1-1 loss: 0.799888  [   64/  130]
train() client id: f_00008-1-2 loss: 0.823195  [   96/  130]
train() client id: f_00008-1-3 loss: 0.863544  [  128/  130]
train() client id: f_00008-2-0 loss: 0.812307  [   32/  130]
train() client id: f_00008-2-1 loss: 0.783779  [   64/  130]
train() client id: f_00008-2-2 loss: 0.918263  [   96/  130]
train() client id: f_00008-2-3 loss: 0.793210  [  128/  130]
train() client id: f_00008-3-0 loss: 0.920962  [   32/  130]
train() client id: f_00008-3-1 loss: 0.799264  [   64/  130]
train() client id: f_00008-3-2 loss: 0.898543  [   96/  130]
train() client id: f_00008-3-3 loss: 0.672872  [  128/  130]
train() client id: f_00008-4-0 loss: 0.950062  [   32/  130]
train() client id: f_00008-4-1 loss: 0.775558  [   64/  130]
train() client id: f_00008-4-2 loss: 0.797868  [   96/  130]
train() client id: f_00008-4-3 loss: 0.791640  [  128/  130]
train() client id: f_00008-5-0 loss: 0.761741  [   32/  130]
train() client id: f_00008-5-1 loss: 0.921593  [   64/  130]
train() client id: f_00008-5-2 loss: 0.793916  [   96/  130]
train() client id: f_00008-5-3 loss: 0.847636  [  128/  130]
train() client id: f_00008-6-0 loss: 0.780542  [   32/  130]
train() client id: f_00008-6-1 loss: 0.827743  [   64/  130]
train() client id: f_00008-6-2 loss: 0.836408  [   96/  130]
train() client id: f_00008-6-3 loss: 0.865988  [  128/  130]
train() client id: f_00008-7-0 loss: 0.916182  [   32/  130]
train() client id: f_00008-7-1 loss: 0.857947  [   64/  130]
train() client id: f_00008-7-2 loss: 0.751424  [   96/  130]
train() client id: f_00008-7-3 loss: 0.792588  [  128/  130]
train() client id: f_00008-8-0 loss: 0.766261  [   32/  130]
train() client id: f_00008-8-1 loss: 0.735954  [   64/  130]
train() client id: f_00008-8-2 loss: 0.849454  [   96/  130]
train() client id: f_00008-8-3 loss: 0.974755  [  128/  130]
train() client id: f_00008-9-0 loss: 0.845211  [   32/  130]
train() client id: f_00008-9-1 loss: 0.763193  [   64/  130]
train() client id: f_00008-9-2 loss: 0.907015  [   96/  130]
train() client id: f_00008-9-3 loss: 0.782524  [  128/  130]
train() client id: f_00008-10-0 loss: 0.831440  [   32/  130]
train() client id: f_00008-10-1 loss: 0.845173  [   64/  130]
train() client id: f_00008-10-2 loss: 0.833180  [   96/  130]
train() client id: f_00008-10-3 loss: 0.787166  [  128/  130]
train() client id: f_00008-11-0 loss: 0.796707  [   32/  130]
train() client id: f_00008-11-1 loss: 0.848493  [   64/  130]
train() client id: f_00008-11-2 loss: 0.776497  [   96/  130]
train() client id: f_00008-11-3 loss: 0.886001  [  128/  130]
train() client id: f_00008-12-0 loss: 0.813380  [   32/  130]
train() client id: f_00008-12-1 loss: 0.785982  [   64/  130]
train() client id: f_00008-12-2 loss: 0.842761  [   96/  130]
train() client id: f_00008-12-3 loss: 0.832440  [  128/  130]
train() client id: f_00008-13-0 loss: 0.865087  [   32/  130]
train() client id: f_00008-13-1 loss: 0.791747  [   64/  130]
train() client id: f_00008-13-2 loss: 0.822243  [   96/  130]
train() client id: f_00008-13-3 loss: 0.832528  [  128/  130]
train() client id: f_00009-0-0 loss: 0.963951  [   32/  118]
train() client id: f_00009-0-1 loss: 1.189616  [   64/  118]
train() client id: f_00009-0-2 loss: 0.890807  [   96/  118]
train() client id: f_00009-1-0 loss: 0.959837  [   32/  118]
train() client id: f_00009-1-1 loss: 0.914163  [   64/  118]
train() client id: f_00009-1-2 loss: 1.037223  [   96/  118]
train() client id: f_00009-2-0 loss: 0.944564  [   32/  118]
train() client id: f_00009-2-1 loss: 0.948249  [   64/  118]
train() client id: f_00009-2-2 loss: 0.969754  [   96/  118]
train() client id: f_00009-3-0 loss: 1.004388  [   32/  118]
train() client id: f_00009-3-1 loss: 0.830189  [   64/  118]
train() client id: f_00009-3-2 loss: 0.893676  [   96/  118]
train() client id: f_00009-4-0 loss: 0.882985  [   32/  118]
train() client id: f_00009-4-1 loss: 0.992395  [   64/  118]
train() client id: f_00009-4-2 loss: 0.867857  [   96/  118]
train() client id: f_00009-5-0 loss: 0.802637  [   32/  118]
train() client id: f_00009-5-1 loss: 0.846240  [   64/  118]
train() client id: f_00009-5-2 loss: 0.893780  [   96/  118]
train() client id: f_00009-6-0 loss: 0.731991  [   32/  118]
train() client id: f_00009-6-1 loss: 0.921809  [   64/  118]
train() client id: f_00009-6-2 loss: 0.927980  [   96/  118]
train() client id: f_00009-7-0 loss: 0.843091  [   32/  118]
train() client id: f_00009-7-1 loss: 0.836489  [   64/  118]
train() client id: f_00009-7-2 loss: 0.966975  [   96/  118]
train() client id: f_00009-8-0 loss: 0.747002  [   32/  118]
train() client id: f_00009-8-1 loss: 0.898533  [   64/  118]
train() client id: f_00009-8-2 loss: 0.873966  [   96/  118]
train() client id: f_00009-9-0 loss: 0.905338  [   32/  118]
train() client id: f_00009-9-1 loss: 0.728397  [   64/  118]
train() client id: f_00009-9-2 loss: 0.799758  [   96/  118]
train() client id: f_00009-10-0 loss: 0.775792  [   32/  118]
train() client id: f_00009-10-1 loss: 0.845431  [   64/  118]
train() client id: f_00009-10-2 loss: 0.822840  [   96/  118]
train() client id: f_00009-11-0 loss: 0.750192  [   32/  118]
train() client id: f_00009-11-1 loss: 0.768455  [   64/  118]
train() client id: f_00009-11-2 loss: 0.886826  [   96/  118]
train() client id: f_00009-12-0 loss: 0.719459  [   32/  118]
train() client id: f_00009-12-1 loss: 0.936190  [   64/  118]
train() client id: f_00009-12-2 loss: 0.777118  [   96/  118]
train() client id: f_00009-13-0 loss: 0.855614  [   32/  118]
train() client id: f_00009-13-1 loss: 0.864075  [   64/  118]
train() client id: f_00009-13-2 loss: 0.751654  [   96/  118]
At round 20 accuracy: 0.636604774535809
At round 20 training accuracy: 0.5808182427900738
At round 20 training loss: 0.8463579494541933
update_location
xs = 8.927491 221.223621 5.882650 10.934260 -137.581990 14.769243 -5.849135 -5.143845 -160.120581 20.134486 
ys = -212.390647 7.291448 110.684448 -42.290817 -9.642386 0.794442 -51.381692 106.628436 25.881276 -647.232496 
xs mean: -2.682379970521214
ys mean: -71.16579882624053
dists_uav = 234.924428 242.884861 149.283799 109.124109 170.357798 101.087891 112.580152 146.273998 190.547739 655.221566 
uav_gains = -110.074750 -110.703921 -104.354589 -100.948073 -105.807403 -100.117497 -101.286632 -104.132437 -107.090136 -127.350743 
uav_gains_db_mean: -107.18661829374324
dists_bs = 428.836607 430.254943 191.976622 285.984000 188.395643 257.604235 282.596330 183.100476 149.859237 845.070260 
bs_gains = -113.271231 -113.311383 -103.498013 -108.344628 -103.269043 -107.073743 -108.199722 -102.922364 -100.486198 -121.520058 
bs_gains_db_mean: -108.18963828857432
Round 21
-------------------------------
ene_coms = [0.01074291 0.01122347 0.00737022 0.00656046 0.00728958 0.0063333
 0.00665773 0.00717073 0.00643219 0.03428038]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.28436099 17.18238464  8.08497339  2.88856348 19.72057327  9.46969739
  3.59397965 11.59751337  8.44873376  8.27185229]
obj_prev = 97.54263221245489
eta_min = 4.8262383917667524e-12	eta_max = 0.7472092706780775
af = 20.495020110218665	bf = 2.1841503175249932	zeta = 22.544522121240533	eta = 0.9090909090909091
af = 20.495020110218665	bf = 2.1841503175249932	zeta = 45.51287806255507	eta = 0.45031254850658603
af = 20.495020110218665	bf = 2.1841503175249932	zeta = 33.719801386029154	eta = 0.6078037019135646
af = 20.495020110218665	bf = 2.1841503175249932	zeta = 31.587979501880397	eta = 0.6488233952728322
af = 20.495020110218665	bf = 2.1841503175249932	zeta = 31.46618426223224	eta = 0.651334777023413
af = 20.495020110218665	bf = 2.1841503175249932	zeta = 31.465746190928108	eta = 0.6513438450135209
af = 20.495020110218665	bf = 2.1841503175249932	zeta = 31.465746185231364	eta = 0.651343845131444
eta = 0.651343845131444
ene_coms = [0.01074291 0.01122347 0.00737022 0.00656046 0.00728958 0.0063333
 0.00665773 0.00717073 0.00643219 0.03428038]
ene_comp = [0.03537675 0.07440348 0.03481521 0.01207302 0.08591496 0.04099211
 0.01516146 0.05025744 0.03649982 0.03313061]
ene_total = [2.77640887 5.15475218 2.53956758 1.12173733 5.61092388 2.84899495
 1.31351801 3.45718249 2.58451194 4.05814895]
ti_comp = [0.37314055 0.36833496 0.40686752 0.4149651  0.40767389 0.41723665
 0.41399237 0.4088624  0.41624777 0.13776589]
ti_coms = [0.10742913 0.11223472 0.07370216 0.06560458 0.07289579 0.06333303
 0.06657731 0.07170728 0.06432191 0.34280379]
t_total = [28.93034935 28.93034935 28.93034935 28.93034935 28.93034935 28.93034935
 28.93034935 28.93034935 28.93034935 28.93034935]
ene_coms = [0.01074291 0.01122347 0.00737022 0.00656046 0.00728958 0.0063333
 0.00665773 0.00717073 0.00643219 0.03428038]
ene_comp = [1.98741638e-05 1.89746541e-04 1.59323973e-05 6.38710919e-07
 2.38484749e-04 2.47295258e-05 1.27092151e-06 4.74599055e-05
 1.75407575e-05 1.19752546e-04]
ene_total = [0.64792102 0.68707707 0.44464696 0.39497877 0.45319031 0.38275428
 0.40087269 0.43453574 0.3882746  2.07089185]
optimize_network iter = 0 obj = 6.305143294597697
eta = 0.651343845131444
freqs = [4.74040526e+07 1.00999752e+08 4.27844577e+07 1.45470273e+07
 1.05372165e+08 4.91233275e+07 1.83112840e+07 6.14600954e+07
 4.38438625e+07 1.20242414e+08]
eta_min = 0.6513438451314457	eta_max = 0.651343845131444
af = 0.03306768741826558	bf = 2.1841503175249932	zeta = 0.03637445616009214	eta = 0.9090909090909091
af = 0.03306768741826558	bf = 2.1841503175249932	zeta = 24.06032205234395	eta = 0.0013743659518075377
af = 0.03306768741826558	bf = 2.1841503175249932	zeta = 2.4053643842116266	eta = 0.013747475282878492
af = 0.03306768741826558	bf = 2.1841503175249932	zeta = 2.35832926286252	eta = 0.014021658442268702
af = 0.03306768741826558	bf = 2.1841503175249932	zeta = 2.358322766093164	eta = 0.01402169706950082
eta = 0.01402169706950082
ene_coms = [0.01074291 0.01122347 0.00737022 0.00656046 0.00728958 0.0063333
 0.00665773 0.00717073 0.00643219 0.03428038]
ene_comp = [1.97813260e-04 1.88860181e-03 1.58579726e-04 6.35727321e-06
 2.37370720e-03 2.46140072e-04 1.26498468e-05 4.72382069e-04
 1.74588197e-04 1.19193149e-03]
ene_total = [0.23290212 0.27912497 0.1602702  0.13979193 0.20570846 0.14006074
 0.1419966  0.16270369 0.14064267 0.75512139]
ti_comp = [0.37314055 0.36833496 0.40686752 0.4149651  0.40767389 0.41723665
 0.41399237 0.4088624  0.41624777 0.13776589]
ti_coms = [0.10742913 0.11223472 0.07370216 0.06560458 0.07289579 0.06333303
 0.06657731 0.07170728 0.06432191 0.34280379]
t_total = [28.93034935 28.93034935 28.93034935 28.93034935 28.93034935 28.93034935
 28.93034935 28.93034935 28.93034935 28.93034935]
ene_coms = [0.01074291 0.01122347 0.00737022 0.00656046 0.00728958 0.0063333
 0.00665773 0.00717073 0.00643219 0.03428038]
ene_comp = [1.98741638e-05 1.89746541e-04 1.59323973e-05 6.38710919e-07
 2.38484749e-04 2.47295258e-05 1.27092151e-06 4.74599055e-05
 1.75407575e-05 1.19752546e-04]
ene_total = [0.64792102 0.68707707 0.44464696 0.39497877 0.45319031 0.38275428
 0.40087269 0.43453574 0.3882746  2.07089185]
optimize_network iter = 1 obj = 6.305143294597727
eta = 0.6513438451314457
freqs = [4.74040526e+07 1.00999752e+08 4.27844577e+07 1.45470273e+07
 1.05372165e+08 4.91233275e+07 1.83112840e+07 6.14600954e+07
 4.38438625e+07 1.20242414e+08]
Done!
ene_coms = [0.01074291 0.01122347 0.00737022 0.00656046 0.00728958 0.0063333
 0.00665773 0.00717073 0.00643219 0.03428038]
ene_comp = [1.98198119e-05 1.89227621e-04 1.58888253e-05 6.36964170e-07
 2.37832540e-04 2.46618954e-05 1.26744579e-06 4.73301119e-05
 1.74927869e-05 1.19425047e-04]
ene_total = [0.01076273 0.0114127  0.0073861  0.00656109 0.00752741 0.00635796
 0.006659   0.00721806 0.00644968 0.0343998 ]
At round 21 energy consumption: 0.10473455506880089
At round 21 eta: 0.6513438451314457
At round 21 a_n: 20.989140069898713
At round 21 local rounds: 14.038392249004932
At round 21 global rounds: 60.200113426397884
gradient difference: 0.39777782559394836
train() client id: f_00000-0-0 loss: 1.494024  [   32/  126]
train() client id: f_00000-0-1 loss: 1.193754  [   64/  126]
train() client id: f_00000-0-2 loss: 1.229848  [   96/  126]
train() client id: f_00000-1-0 loss: 1.302303  [   32/  126]
train() client id: f_00000-1-1 loss: 1.259759  [   64/  126]
train() client id: f_00000-1-2 loss: 1.045866  [   96/  126]
train() client id: f_00000-2-0 loss: 1.149417  [   32/  126]
train() client id: f_00000-2-1 loss: 1.259675  [   64/  126]
train() client id: f_00000-2-2 loss: 0.866180  [   96/  126]
train() client id: f_00000-3-0 loss: 1.114911  [   32/  126]
train() client id: f_00000-3-1 loss: 0.935446  [   64/  126]
train() client id: f_00000-3-2 loss: 0.979530  [   96/  126]
train() client id: f_00000-4-0 loss: 0.961542  [   32/  126]
train() client id: f_00000-4-1 loss: 0.868192  [   64/  126]
train() client id: f_00000-4-2 loss: 1.031216  [   96/  126]
train() client id: f_00000-5-0 loss: 0.938951  [   32/  126]
train() client id: f_00000-5-1 loss: 0.901109  [   64/  126]
train() client id: f_00000-5-2 loss: 0.945361  [   96/  126]
train() client id: f_00000-6-0 loss: 1.010494  [   32/  126]
train() client id: f_00000-6-1 loss: 0.939442  [   64/  126]
train() client id: f_00000-6-2 loss: 0.841304  [   96/  126]
train() client id: f_00000-7-0 loss: 0.860273  [   32/  126]
train() client id: f_00000-7-1 loss: 0.852840  [   64/  126]
train() client id: f_00000-7-2 loss: 0.947018  [   96/  126]
train() client id: f_00000-8-0 loss: 0.831298  [   32/  126]
train() client id: f_00000-8-1 loss: 0.850055  [   64/  126]
train() client id: f_00000-8-2 loss: 0.954987  [   96/  126]
train() client id: f_00000-9-0 loss: 0.869635  [   32/  126]
train() client id: f_00000-9-1 loss: 0.871762  [   64/  126]
train() client id: f_00000-9-2 loss: 0.895256  [   96/  126]
train() client id: f_00000-10-0 loss: 0.845550  [   32/  126]
train() client id: f_00000-10-1 loss: 0.877980  [   64/  126]
train() client id: f_00000-10-2 loss: 0.829226  [   96/  126]
train() client id: f_00000-11-0 loss: 0.832031  [   32/  126]
train() client id: f_00000-11-1 loss: 0.878605  [   64/  126]
train() client id: f_00000-11-2 loss: 0.779765  [   96/  126]
train() client id: f_00000-12-0 loss: 0.855220  [   32/  126]
train() client id: f_00000-12-1 loss: 0.856460  [   64/  126]
train() client id: f_00000-12-2 loss: 0.896405  [   96/  126]
train() client id: f_00000-13-0 loss: 0.848166  [   32/  126]
train() client id: f_00000-13-1 loss: 0.877463  [   64/  126]
train() client id: f_00000-13-2 loss: 0.966967  [   96/  126]
train() client id: f_00001-0-0 loss: 0.409516  [   32/  265]
train() client id: f_00001-0-1 loss: 0.517924  [   64/  265]
train() client id: f_00001-0-2 loss: 0.533398  [   96/  265]
train() client id: f_00001-0-3 loss: 0.541261  [  128/  265]
train() client id: f_00001-0-4 loss: 0.472090  [  160/  265]
train() client id: f_00001-0-5 loss: 0.443818  [  192/  265]
train() client id: f_00001-0-6 loss: 0.418245  [  224/  265]
train() client id: f_00001-0-7 loss: 0.478843  [  256/  265]
train() client id: f_00001-1-0 loss: 0.401103  [   32/  265]
train() client id: f_00001-1-1 loss: 0.503015  [   64/  265]
train() client id: f_00001-1-2 loss: 0.467064  [   96/  265]
train() client id: f_00001-1-3 loss: 0.478916  [  128/  265]
train() client id: f_00001-1-4 loss: 0.449682  [  160/  265]
train() client id: f_00001-1-5 loss: 0.460155  [  192/  265]
train() client id: f_00001-1-6 loss: 0.483640  [  224/  265]
train() client id: f_00001-1-7 loss: 0.442642  [  256/  265]
train() client id: f_00001-2-0 loss: 0.389028  [   32/  265]
train() client id: f_00001-2-1 loss: 0.519050  [   64/  265]
train() client id: f_00001-2-2 loss: 0.418684  [   96/  265]
train() client id: f_00001-2-3 loss: 0.423459  [  128/  265]
train() client id: f_00001-2-4 loss: 0.374533  [  160/  265]
train() client id: f_00001-2-5 loss: 0.531777  [  192/  265]
train() client id: f_00001-2-6 loss: 0.515827  [  224/  265]
train() client id: f_00001-2-7 loss: 0.361644  [  256/  265]
train() client id: f_00001-3-0 loss: 0.476379  [   32/  265]
train() client id: f_00001-3-1 loss: 0.486655  [   64/  265]
train() client id: f_00001-3-2 loss: 0.446617  [   96/  265]
train() client id: f_00001-3-3 loss: 0.474567  [  128/  265]
train() client id: f_00001-3-4 loss: 0.464499  [  160/  265]
train() client id: f_00001-3-5 loss: 0.394880  [  192/  265]
train() client id: f_00001-3-6 loss: 0.437630  [  224/  265]
train() client id: f_00001-3-7 loss: 0.473782  [  256/  265]
train() client id: f_00001-4-0 loss: 0.594285  [   32/  265]
train() client id: f_00001-4-1 loss: 0.468943  [   64/  265]
train() client id: f_00001-4-2 loss: 0.372333  [   96/  265]
train() client id: f_00001-4-3 loss: 0.382656  [  128/  265]
train() client id: f_00001-4-4 loss: 0.620926  [  160/  265]
train() client id: f_00001-4-5 loss: 0.364694  [  192/  265]
train() client id: f_00001-4-6 loss: 0.413105  [  224/  265]
train() client id: f_00001-4-7 loss: 0.376695  [  256/  265]
train() client id: f_00001-5-0 loss: 0.535191  [   32/  265]
train() client id: f_00001-5-1 loss: 0.485486  [   64/  265]
train() client id: f_00001-5-2 loss: 0.464142  [   96/  265]
train() client id: f_00001-5-3 loss: 0.354653  [  128/  265]
train() client id: f_00001-5-4 loss: 0.382345  [  160/  265]
train() client id: f_00001-5-5 loss: 0.502708  [  192/  265]
train() client id: f_00001-5-6 loss: 0.518272  [  224/  265]
train() client id: f_00001-5-7 loss: 0.344209  [  256/  265]
train() client id: f_00001-6-0 loss: 0.449497  [   32/  265]
train() client id: f_00001-6-1 loss: 0.390244  [   64/  265]
train() client id: f_00001-6-2 loss: 0.476192  [   96/  265]
train() client id: f_00001-6-3 loss: 0.429050  [  128/  265]
train() client id: f_00001-6-4 loss: 0.454437  [  160/  265]
train() client id: f_00001-6-5 loss: 0.502996  [  192/  265]
train() client id: f_00001-6-6 loss: 0.451819  [  224/  265]
train() client id: f_00001-6-7 loss: 0.399087  [  256/  265]
train() client id: f_00001-7-0 loss: 0.464797  [   32/  265]
train() client id: f_00001-7-1 loss: 0.345563  [   64/  265]
train() client id: f_00001-7-2 loss: 0.414921  [   96/  265]
train() client id: f_00001-7-3 loss: 0.416805  [  128/  265]
train() client id: f_00001-7-4 loss: 0.546888  [  160/  265]
train() client id: f_00001-7-5 loss: 0.371415  [  192/  265]
train() client id: f_00001-7-6 loss: 0.594837  [  224/  265]
train() client id: f_00001-7-7 loss: 0.404441  [  256/  265]
train() client id: f_00001-8-0 loss: 0.408998  [   32/  265]
train() client id: f_00001-8-1 loss: 0.601613  [   64/  265]
train() client id: f_00001-8-2 loss: 0.531979  [   96/  265]
train() client id: f_00001-8-3 loss: 0.465244  [  128/  265]
train() client id: f_00001-8-4 loss: 0.357548  [  160/  265]
train() client id: f_00001-8-5 loss: 0.364582  [  192/  265]
train() client id: f_00001-8-6 loss: 0.358424  [  224/  265]
train() client id: f_00001-8-7 loss: 0.454547  [  256/  265]
train() client id: f_00001-9-0 loss: 0.353619  [   32/  265]
train() client id: f_00001-9-1 loss: 0.514690  [   64/  265]
train() client id: f_00001-9-2 loss: 0.478511  [   96/  265]
train() client id: f_00001-9-3 loss: 0.400816  [  128/  265]
train() client id: f_00001-9-4 loss: 0.394368  [  160/  265]
train() client id: f_00001-9-5 loss: 0.414402  [  192/  265]
train() client id: f_00001-9-6 loss: 0.546683  [  224/  265]
train() client id: f_00001-9-7 loss: 0.430557  [  256/  265]
train() client id: f_00001-10-0 loss: 0.396846  [   32/  265]
train() client id: f_00001-10-1 loss: 0.406476  [   64/  265]
train() client id: f_00001-10-2 loss: 0.354991  [   96/  265]
train() client id: f_00001-10-3 loss: 0.522554  [  128/  265]
train() client id: f_00001-10-4 loss: 0.413249  [  160/  265]
train() client id: f_00001-10-5 loss: 0.540464  [  192/  265]
train() client id: f_00001-10-6 loss: 0.482201  [  224/  265]
train() client id: f_00001-10-7 loss: 0.397388  [  256/  265]
train() client id: f_00001-11-0 loss: 0.426259  [   32/  265]
train() client id: f_00001-11-1 loss: 0.471903  [   64/  265]
train() client id: f_00001-11-2 loss: 0.541877  [   96/  265]
train() client id: f_00001-11-3 loss: 0.389969  [  128/  265]
train() client id: f_00001-11-4 loss: 0.434529  [  160/  265]
train() client id: f_00001-11-5 loss: 0.345069  [  192/  265]
train() client id: f_00001-11-6 loss: 0.403459  [  224/  265]
train() client id: f_00001-11-7 loss: 0.446909  [  256/  265]
train() client id: f_00001-12-0 loss: 0.576267  [   32/  265]
train() client id: f_00001-12-1 loss: 0.338659  [   64/  265]
train() client id: f_00001-12-2 loss: 0.390266  [   96/  265]
train() client id: f_00001-12-3 loss: 0.413562  [  128/  265]
train() client id: f_00001-12-4 loss: 0.421822  [  160/  265]
train() client id: f_00001-12-5 loss: 0.355804  [  192/  265]
train() client id: f_00001-12-6 loss: 0.539015  [  224/  265]
train() client id: f_00001-12-7 loss: 0.482767  [  256/  265]
train() client id: f_00001-13-0 loss: 0.345014  [   32/  265]
train() client id: f_00001-13-1 loss: 0.430904  [   64/  265]
train() client id: f_00001-13-2 loss: 0.430620  [   96/  265]
train() client id: f_00001-13-3 loss: 0.360644  [  128/  265]
train() client id: f_00001-13-4 loss: 0.544064  [  160/  265]
train() client id: f_00001-13-5 loss: 0.482609  [  192/  265]
train() client id: f_00001-13-6 loss: 0.359811  [  224/  265]
train() client id: f_00001-13-7 loss: 0.535149  [  256/  265]
train() client id: f_00002-0-0 loss: 1.123373  [   32/  124]
train() client id: f_00002-0-1 loss: 1.178063  [   64/  124]
train() client id: f_00002-0-2 loss: 1.175673  [   96/  124]
train() client id: f_00002-1-0 loss: 1.145076  [   32/  124]
train() client id: f_00002-1-1 loss: 1.194692  [   64/  124]
train() client id: f_00002-1-2 loss: 1.181211  [   96/  124]
train() client id: f_00002-2-0 loss: 1.138927  [   32/  124]
train() client id: f_00002-2-1 loss: 1.055205  [   64/  124]
train() client id: f_00002-2-2 loss: 1.043704  [   96/  124]
train() client id: f_00002-3-0 loss: 1.056806  [   32/  124]
train() client id: f_00002-3-1 loss: 1.144051  [   64/  124]
train() client id: f_00002-3-2 loss: 1.053051  [   96/  124]
train() client id: f_00002-4-0 loss: 1.115586  [   32/  124]
train() client id: f_00002-4-1 loss: 1.025239  [   64/  124]
train() client id: f_00002-4-2 loss: 1.032658  [   96/  124]
train() client id: f_00002-5-0 loss: 1.005258  [   32/  124]
train() client id: f_00002-5-1 loss: 0.960698  [   64/  124]
train() client id: f_00002-5-2 loss: 0.992759  [   96/  124]
train() client id: f_00002-6-0 loss: 0.854100  [   32/  124]
train() client id: f_00002-6-1 loss: 1.071674  [   64/  124]
train() client id: f_00002-6-2 loss: 0.937025  [   96/  124]
train() client id: f_00002-7-0 loss: 0.969167  [   32/  124]
train() client id: f_00002-7-1 loss: 0.948377  [   64/  124]
train() client id: f_00002-7-2 loss: 0.872124  [   96/  124]
train() client id: f_00002-8-0 loss: 1.029889  [   32/  124]
train() client id: f_00002-8-1 loss: 0.936128  [   64/  124]
train() client id: f_00002-8-2 loss: 0.897234  [   96/  124]
train() client id: f_00002-9-0 loss: 0.918615  [   32/  124]
train() client id: f_00002-9-1 loss: 0.995658  [   64/  124]
train() client id: f_00002-9-2 loss: 0.993928  [   96/  124]
train() client id: f_00002-10-0 loss: 0.944667  [   32/  124]
train() client id: f_00002-10-1 loss: 0.889099  [   64/  124]
train() client id: f_00002-10-2 loss: 0.912237  [   96/  124]
train() client id: f_00002-11-0 loss: 0.855042  [   32/  124]
train() client id: f_00002-11-1 loss: 1.091515  [   64/  124]
train() client id: f_00002-11-2 loss: 0.887394  [   96/  124]
train() client id: f_00002-12-0 loss: 1.039599  [   32/  124]
train() client id: f_00002-12-1 loss: 0.862162  [   64/  124]
train() client id: f_00002-12-2 loss: 0.932893  [   96/  124]
train() client id: f_00002-13-0 loss: 0.790162  [   32/  124]
train() client id: f_00002-13-1 loss: 0.941580  [   64/  124]
train() client id: f_00002-13-2 loss: 1.010553  [   96/  124]
train() client id: f_00003-0-0 loss: 0.778992  [   32/   43]
train() client id: f_00003-1-0 loss: 0.781646  [   32/   43]
train() client id: f_00003-2-0 loss: 0.675664  [   32/   43]
train() client id: f_00003-3-0 loss: 0.751894  [   32/   43]
train() client id: f_00003-4-0 loss: 0.634855  [   32/   43]
train() client id: f_00003-5-0 loss: 0.748101  [   32/   43]
train() client id: f_00003-6-0 loss: 0.687842  [   32/   43]
train() client id: f_00003-7-0 loss: 0.700689  [   32/   43]
train() client id: f_00003-8-0 loss: 0.782739  [   32/   43]
train() client id: f_00003-9-0 loss: 0.669917  [   32/   43]
train() client id: f_00003-10-0 loss: 0.709703  [   32/   43]
train() client id: f_00003-11-0 loss: 0.740304  [   32/   43]
train() client id: f_00003-12-0 loss: 0.678792  [   32/   43]
train() client id: f_00003-13-0 loss: 0.813792  [   32/   43]
train() client id: f_00004-0-0 loss: 0.822601  [   32/  306]
train() client id: f_00004-0-1 loss: 0.844288  [   64/  306]
train() client id: f_00004-0-2 loss: 0.739383  [   96/  306]
train() client id: f_00004-0-3 loss: 0.895546  [  128/  306]
train() client id: f_00004-0-4 loss: 0.924930  [  160/  306]
train() client id: f_00004-0-5 loss: 1.072311  [  192/  306]
train() client id: f_00004-0-6 loss: 0.853525  [  224/  306]
train() client id: f_00004-0-7 loss: 0.964781  [  256/  306]
train() client id: f_00004-0-8 loss: 0.747519  [  288/  306]
train() client id: f_00004-1-0 loss: 0.922402  [   32/  306]
train() client id: f_00004-1-1 loss: 0.846097  [   64/  306]
train() client id: f_00004-1-2 loss: 0.890891  [   96/  306]
train() client id: f_00004-1-3 loss: 0.888318  [  128/  306]
train() client id: f_00004-1-4 loss: 0.811669  [  160/  306]
train() client id: f_00004-1-5 loss: 0.992372  [  192/  306]
train() client id: f_00004-1-6 loss: 0.916489  [  224/  306]
train() client id: f_00004-1-7 loss: 0.851601  [  256/  306]
train() client id: f_00004-1-8 loss: 0.811155  [  288/  306]
train() client id: f_00004-2-0 loss: 0.921924  [   32/  306]
train() client id: f_00004-2-1 loss: 0.981825  [   64/  306]
train() client id: f_00004-2-2 loss: 0.906993  [   96/  306]
train() client id: f_00004-2-3 loss: 0.878967  [  128/  306]
train() client id: f_00004-2-4 loss: 0.774659  [  160/  306]
train() client id: f_00004-2-5 loss: 0.902942  [  192/  306]
train() client id: f_00004-2-6 loss: 1.000489  [  224/  306]
train() client id: f_00004-2-7 loss: 0.848917  [  256/  306]
train() client id: f_00004-2-8 loss: 0.805407  [  288/  306]
train() client id: f_00004-3-0 loss: 0.837984  [   32/  306]
train() client id: f_00004-3-1 loss: 0.900211  [   64/  306]
train() client id: f_00004-3-2 loss: 0.761579  [   96/  306]
train() client id: f_00004-3-3 loss: 0.991602  [  128/  306]
train() client id: f_00004-3-4 loss: 0.892971  [  160/  306]
train() client id: f_00004-3-5 loss: 0.895221  [  192/  306]
train() client id: f_00004-3-6 loss: 0.821189  [  224/  306]
train() client id: f_00004-3-7 loss: 0.881923  [  256/  306]
train() client id: f_00004-3-8 loss: 0.920343  [  288/  306]
train() client id: f_00004-4-0 loss: 0.955589  [   32/  306]
train() client id: f_00004-4-1 loss: 0.939281  [   64/  306]
train() client id: f_00004-4-2 loss: 0.959365  [   96/  306]
train() client id: f_00004-4-3 loss: 0.914900  [  128/  306]
train() client id: f_00004-4-4 loss: 0.819082  [  160/  306]
train() client id: f_00004-4-5 loss: 0.876165  [  192/  306]
train() client id: f_00004-4-6 loss: 0.861425  [  224/  306]
train() client id: f_00004-4-7 loss: 0.916163  [  256/  306]
train() client id: f_00004-4-8 loss: 0.749400  [  288/  306]
train() client id: f_00004-5-0 loss: 0.999945  [   32/  306]
train() client id: f_00004-5-1 loss: 0.806510  [   64/  306]
train() client id: f_00004-5-2 loss: 0.810464  [   96/  306]
train() client id: f_00004-5-3 loss: 0.882283  [  128/  306]
train() client id: f_00004-5-4 loss: 0.912507  [  160/  306]
train() client id: f_00004-5-5 loss: 0.769311  [  192/  306]
train() client id: f_00004-5-6 loss: 0.877396  [  224/  306]
train() client id: f_00004-5-7 loss: 0.949559  [  256/  306]
train() client id: f_00004-5-8 loss: 0.972981  [  288/  306]
train() client id: f_00004-6-0 loss: 0.863800  [   32/  306]
train() client id: f_00004-6-1 loss: 0.871412  [   64/  306]
train() client id: f_00004-6-2 loss: 0.882819  [   96/  306]
train() client id: f_00004-6-3 loss: 0.798756  [  128/  306]
train() client id: f_00004-6-4 loss: 0.834581  [  160/  306]
train() client id: f_00004-6-5 loss: 0.955222  [  192/  306]
train() client id: f_00004-6-6 loss: 0.893993  [  224/  306]
train() client id: f_00004-6-7 loss: 0.879319  [  256/  306]
train() client id: f_00004-6-8 loss: 0.910453  [  288/  306]
train() client id: f_00004-7-0 loss: 0.792044  [   32/  306]
train() client id: f_00004-7-1 loss: 0.864466  [   64/  306]
train() client id: f_00004-7-2 loss: 0.926520  [   96/  306]
train() client id: f_00004-7-3 loss: 1.055786  [  128/  306]
train() client id: f_00004-7-4 loss: 0.834435  [  160/  306]
train() client id: f_00004-7-5 loss: 0.915654  [  192/  306]
train() client id: f_00004-7-6 loss: 0.959365  [  224/  306]
train() client id: f_00004-7-7 loss: 0.880065  [  256/  306]
train() client id: f_00004-7-8 loss: 0.809880  [  288/  306]
train() client id: f_00004-8-0 loss: 0.915749  [   32/  306]
train() client id: f_00004-8-1 loss: 0.900619  [   64/  306]
train() client id: f_00004-8-2 loss: 0.907610  [   96/  306]
train() client id: f_00004-8-3 loss: 0.778536  [  128/  306]
train() client id: f_00004-8-4 loss: 0.824170  [  160/  306]
train() client id: f_00004-8-5 loss: 0.882310  [  192/  306]
train() client id: f_00004-8-6 loss: 0.848641  [  224/  306]
train() client id: f_00004-8-7 loss: 0.889178  [  256/  306]
train() client id: f_00004-8-8 loss: 0.930861  [  288/  306]
train() client id: f_00004-9-0 loss: 0.780317  [   32/  306]
train() client id: f_00004-9-1 loss: 0.861162  [   64/  306]
train() client id: f_00004-9-2 loss: 0.722821  [   96/  306]
train() client id: f_00004-9-3 loss: 0.859762  [  128/  306]
train() client id: f_00004-9-4 loss: 0.938412  [  160/  306]
train() client id: f_00004-9-5 loss: 1.011382  [  192/  306]
train() client id: f_00004-9-6 loss: 0.815931  [  224/  306]
train() client id: f_00004-9-7 loss: 1.069608  [  256/  306]
train() client id: f_00004-9-8 loss: 0.915443  [  288/  306]
train() client id: f_00004-10-0 loss: 0.951577  [   32/  306]
train() client id: f_00004-10-1 loss: 0.873053  [   64/  306]
train() client id: f_00004-10-2 loss: 0.719188  [   96/  306]
train() client id: f_00004-10-3 loss: 0.866512  [  128/  306]
train() client id: f_00004-10-4 loss: 0.930917  [  160/  306]
train() client id: f_00004-10-5 loss: 0.913398  [  192/  306]
train() client id: f_00004-10-6 loss: 0.873475  [  224/  306]
train() client id: f_00004-10-7 loss: 0.939850  [  256/  306]
train() client id: f_00004-10-8 loss: 0.948038  [  288/  306]
train() client id: f_00004-11-0 loss: 0.941208  [   32/  306]
train() client id: f_00004-11-1 loss: 1.016385  [   64/  306]
train() client id: f_00004-11-2 loss: 0.844159  [   96/  306]
train() client id: f_00004-11-3 loss: 0.873118  [  128/  306]
train() client id: f_00004-11-4 loss: 0.780762  [  160/  306]
train() client id: f_00004-11-5 loss: 0.853466  [  192/  306]
train() client id: f_00004-11-6 loss: 0.910627  [  224/  306]
train() client id: f_00004-11-7 loss: 0.973252  [  256/  306]
train() client id: f_00004-11-8 loss: 0.829807  [  288/  306]
train() client id: f_00004-12-0 loss: 0.913073  [   32/  306]
train() client id: f_00004-12-1 loss: 0.849440  [   64/  306]
train() client id: f_00004-12-2 loss: 0.932289  [   96/  306]
train() client id: f_00004-12-3 loss: 0.881570  [  128/  306]
train() client id: f_00004-12-4 loss: 0.995533  [  160/  306]
train() client id: f_00004-12-5 loss: 0.909941  [  192/  306]
train() client id: f_00004-12-6 loss: 0.910946  [  224/  306]
train() client id: f_00004-12-7 loss: 0.811732  [  256/  306]
train() client id: f_00004-12-8 loss: 0.782909  [  288/  306]
train() client id: f_00004-13-0 loss: 0.857335  [   32/  306]
train() client id: f_00004-13-1 loss: 0.867553  [   64/  306]
train() client id: f_00004-13-2 loss: 0.878373  [   96/  306]
train() client id: f_00004-13-3 loss: 0.873478  [  128/  306]
train() client id: f_00004-13-4 loss: 0.899811  [  160/  306]
train() client id: f_00004-13-5 loss: 0.875151  [  192/  306]
train() client id: f_00004-13-6 loss: 0.834297  [  224/  306]
train() client id: f_00004-13-7 loss: 0.985137  [  256/  306]
train() client id: f_00004-13-8 loss: 0.843026  [  288/  306]
train() client id: f_00005-0-0 loss: 0.810168  [   32/  146]
train() client id: f_00005-0-1 loss: 0.816714  [   64/  146]
train() client id: f_00005-0-2 loss: 0.730096  [   96/  146]
train() client id: f_00005-0-3 loss: 0.656881  [  128/  146]
train() client id: f_00005-1-0 loss: 0.671118  [   32/  146]
train() client id: f_00005-1-1 loss: 0.694057  [   64/  146]
train() client id: f_00005-1-2 loss: 0.820864  [   96/  146]
train() client id: f_00005-1-3 loss: 0.648753  [  128/  146]
train() client id: f_00005-2-0 loss: 0.671325  [   32/  146]
train() client id: f_00005-2-1 loss: 0.828286  [   64/  146]
train() client id: f_00005-2-2 loss: 0.686440  [   96/  146]
train() client id: f_00005-2-3 loss: 0.609052  [  128/  146]
train() client id: f_00005-3-0 loss: 0.832305  [   32/  146]
train() client id: f_00005-3-1 loss: 0.702742  [   64/  146]
train() client id: f_00005-3-2 loss: 0.544811  [   96/  146]
train() client id: f_00005-3-3 loss: 0.715058  [  128/  146]
train() client id: f_00005-4-0 loss: 0.659837  [   32/  146]
train() client id: f_00005-4-1 loss: 0.657340  [   64/  146]
train() client id: f_00005-4-2 loss: 0.894747  [   96/  146]
train() client id: f_00005-4-3 loss: 0.625681  [  128/  146]
train() client id: f_00005-5-0 loss: 0.811107  [   32/  146]
train() client id: f_00005-5-1 loss: 0.713093  [   64/  146]
train() client id: f_00005-5-2 loss: 0.621410  [   96/  146]
train() client id: f_00005-5-3 loss: 0.652128  [  128/  146]
train() client id: f_00005-6-0 loss: 0.593923  [   32/  146]
train() client id: f_00005-6-1 loss: 0.685469  [   64/  146]
train() client id: f_00005-6-2 loss: 0.736910  [   96/  146]
train() client id: f_00005-6-3 loss: 0.892141  [  128/  146]
train() client id: f_00005-7-0 loss: 0.920753  [   32/  146]
train() client id: f_00005-7-1 loss: 0.789083  [   64/  146]
train() client id: f_00005-7-2 loss: 0.523014  [   96/  146]
train() client id: f_00005-7-3 loss: 0.677243  [  128/  146]
train() client id: f_00005-8-0 loss: 0.824000  [   32/  146]
train() client id: f_00005-8-1 loss: 0.528045  [   64/  146]
train() client id: f_00005-8-2 loss: 0.687986  [   96/  146]
train() client id: f_00005-8-3 loss: 0.802335  [  128/  146]
train() client id: f_00005-9-0 loss: 0.709271  [   32/  146]
train() client id: f_00005-9-1 loss: 0.774469  [   64/  146]
train() client id: f_00005-9-2 loss: 0.691621  [   96/  146]
train() client id: f_00005-9-3 loss: 0.718658  [  128/  146]
train() client id: f_00005-10-0 loss: 0.747510  [   32/  146]
train() client id: f_00005-10-1 loss: 0.679451  [   64/  146]
train() client id: f_00005-10-2 loss: 0.727475  [   96/  146]
train() client id: f_00005-10-3 loss: 0.560511  [  128/  146]
train() client id: f_00005-11-0 loss: 0.788850  [   32/  146]
train() client id: f_00005-11-1 loss: 0.558589  [   64/  146]
train() client id: f_00005-11-2 loss: 0.824744  [   96/  146]
train() client id: f_00005-11-3 loss: 0.663024  [  128/  146]
train() client id: f_00005-12-0 loss: 0.498522  [   32/  146]
train() client id: f_00005-12-1 loss: 0.739832  [   64/  146]
train() client id: f_00005-12-2 loss: 0.787124  [   96/  146]
train() client id: f_00005-12-3 loss: 0.822842  [  128/  146]
train() client id: f_00005-13-0 loss: 0.886091  [   32/  146]
train() client id: f_00005-13-1 loss: 0.534720  [   64/  146]
train() client id: f_00005-13-2 loss: 0.641280  [   96/  146]
train() client id: f_00005-13-3 loss: 0.804557  [  128/  146]
train() client id: f_00006-0-0 loss: 0.499122  [   32/   54]
train() client id: f_00006-1-0 loss: 0.455253  [   32/   54]
train() client id: f_00006-2-0 loss: 0.557506  [   32/   54]
train() client id: f_00006-3-0 loss: 0.454201  [   32/   54]
train() client id: f_00006-4-0 loss: 0.502194  [   32/   54]
train() client id: f_00006-5-0 loss: 0.494052  [   32/   54]
train() client id: f_00006-6-0 loss: 0.557514  [   32/   54]
train() client id: f_00006-7-0 loss: 0.502602  [   32/   54]
train() client id: f_00006-8-0 loss: 0.550733  [   32/   54]
train() client id: f_00006-9-0 loss: 0.514735  [   32/   54]
train() client id: f_00006-10-0 loss: 0.523730  [   32/   54]
train() client id: f_00006-11-0 loss: 0.491311  [   32/   54]
train() client id: f_00006-12-0 loss: 0.537555  [   32/   54]
train() client id: f_00006-13-0 loss: 0.447576  [   32/   54]
train() client id: f_00007-0-0 loss: 0.587281  [   32/  179]
train() client id: f_00007-0-1 loss: 0.685785  [   64/  179]
train() client id: f_00007-0-2 loss: 0.396475  [   96/  179]
train() client id: f_00007-0-3 loss: 0.536907  [  128/  179]
train() client id: f_00007-0-4 loss: 0.564721  [  160/  179]
train() client id: f_00007-1-0 loss: 0.624123  [   32/  179]
train() client id: f_00007-1-1 loss: 0.512125  [   64/  179]
train() client id: f_00007-1-2 loss: 0.477363  [   96/  179]
train() client id: f_00007-1-3 loss: 0.683936  [  128/  179]
train() client id: f_00007-1-4 loss: 0.445164  [  160/  179]
train() client id: f_00007-2-0 loss: 0.419661  [   32/  179]
train() client id: f_00007-2-1 loss: 0.463494  [   64/  179]
train() client id: f_00007-2-2 loss: 0.657900  [   96/  179]
train() client id: f_00007-2-3 loss: 0.484329  [  128/  179]
train() client id: f_00007-2-4 loss: 0.637507  [  160/  179]
train() client id: f_00007-3-0 loss: 0.593814  [   32/  179]
train() client id: f_00007-3-1 loss: 0.508805  [   64/  179]
train() client id: f_00007-3-2 loss: 0.613072  [   96/  179]
train() client id: f_00007-3-3 loss: 0.396325  [  128/  179]
train() client id: f_00007-3-4 loss: 0.585812  [  160/  179]
train() client id: f_00007-4-0 loss: 0.377192  [   32/  179]
train() client id: f_00007-4-1 loss: 0.520342  [   64/  179]
train() client id: f_00007-4-2 loss: 0.586786  [   96/  179]
train() client id: f_00007-4-3 loss: 0.507416  [  128/  179]
train() client id: f_00007-4-4 loss: 0.653367  [  160/  179]
train() client id: f_00007-5-0 loss: 0.564945  [   32/  179]
train() client id: f_00007-5-1 loss: 0.444376  [   64/  179]
train() client id: f_00007-5-2 loss: 0.446771  [   96/  179]
train() client id: f_00007-5-3 loss: 0.550548  [  128/  179]
train() client id: f_00007-5-4 loss: 0.408873  [  160/  179]
train() client id: f_00007-6-0 loss: 0.452394  [   32/  179]
train() client id: f_00007-6-1 loss: 0.540929  [   64/  179]
train() client id: f_00007-6-2 loss: 0.428810  [   96/  179]
train() client id: f_00007-6-3 loss: 0.586153  [  128/  179]
train() client id: f_00007-6-4 loss: 0.474680  [  160/  179]
train() client id: f_00007-7-0 loss: 0.348631  [   32/  179]
train() client id: f_00007-7-1 loss: 0.458835  [   64/  179]
train() client id: f_00007-7-2 loss: 0.430128  [   96/  179]
train() client id: f_00007-7-3 loss: 0.704668  [  128/  179]
train() client id: f_00007-7-4 loss: 0.456896  [  160/  179]
train() client id: f_00007-8-0 loss: 0.525424  [   32/  179]
train() client id: f_00007-8-1 loss: 0.584064  [   64/  179]
train() client id: f_00007-8-2 loss: 0.610758  [   96/  179]
train() client id: f_00007-8-3 loss: 0.371033  [  128/  179]
train() client id: f_00007-8-4 loss: 0.374066  [  160/  179]
train() client id: f_00007-9-0 loss: 0.384728  [   32/  179]
train() client id: f_00007-9-1 loss: 0.513484  [   64/  179]
train() client id: f_00007-9-2 loss: 0.622814  [   96/  179]
train() client id: f_00007-9-3 loss: 0.433366  [  128/  179]
train() client id: f_00007-9-4 loss: 0.378681  [  160/  179]
train() client id: f_00007-10-0 loss: 0.566430  [   32/  179]
train() client id: f_00007-10-1 loss: 0.470970  [   64/  179]
train() client id: f_00007-10-2 loss: 0.461058  [   96/  179]
train() client id: f_00007-10-3 loss: 0.348531  [  128/  179]
train() client id: f_00007-10-4 loss: 0.522423  [  160/  179]
train() client id: f_00007-11-0 loss: 0.591431  [   32/  179]
train() client id: f_00007-11-1 loss: 0.460805  [   64/  179]
train() client id: f_00007-11-2 loss: 0.464068  [   96/  179]
train() client id: f_00007-11-3 loss: 0.434096  [  128/  179]
train() client id: f_00007-11-4 loss: 0.427216  [  160/  179]
train() client id: f_00007-12-0 loss: 0.659147  [   32/  179]
train() client id: f_00007-12-1 loss: 0.436602  [   64/  179]
train() client id: f_00007-12-2 loss: 0.556820  [   96/  179]
train() client id: f_00007-12-3 loss: 0.341546  [  128/  179]
train() client id: f_00007-12-4 loss: 0.425931  [  160/  179]
train() client id: f_00007-13-0 loss: 0.339941  [   32/  179]
train() client id: f_00007-13-1 loss: 0.650777  [   64/  179]
train() client id: f_00007-13-2 loss: 0.605282  [   96/  179]
train() client id: f_00007-13-3 loss: 0.330726  [  128/  179]
train() client id: f_00007-13-4 loss: 0.375095  [  160/  179]
train() client id: f_00008-0-0 loss: 0.820192  [   32/  130]
train() client id: f_00008-0-1 loss: 0.831260  [   64/  130]
train() client id: f_00008-0-2 loss: 0.784176  [   96/  130]
train() client id: f_00008-0-3 loss: 0.620221  [  128/  130]
train() client id: f_00008-1-0 loss: 0.748234  [   32/  130]
train() client id: f_00008-1-1 loss: 0.783011  [   64/  130]
train() client id: f_00008-1-2 loss: 0.777361  [   96/  130]
train() client id: f_00008-1-3 loss: 0.745252  [  128/  130]
train() client id: f_00008-2-0 loss: 0.721838  [   32/  130]
train() client id: f_00008-2-1 loss: 0.765445  [   64/  130]
train() client id: f_00008-2-2 loss: 0.844670  [   96/  130]
train() client id: f_00008-2-3 loss: 0.681723  [  128/  130]
train() client id: f_00008-3-0 loss: 0.732476  [   32/  130]
train() client id: f_00008-3-1 loss: 0.803557  [   64/  130]
train() client id: f_00008-3-2 loss: 0.782891  [   96/  130]
train() client id: f_00008-3-3 loss: 0.735584  [  128/  130]
train() client id: f_00008-4-0 loss: 0.723117  [   32/  130]
train() client id: f_00008-4-1 loss: 0.701227  [   64/  130]
train() client id: f_00008-4-2 loss: 0.778125  [   96/  130]
train() client id: f_00008-4-3 loss: 0.856766  [  128/  130]
train() client id: f_00008-5-0 loss: 0.776182  [   32/  130]
train() client id: f_00008-5-1 loss: 0.765252  [   64/  130]
train() client id: f_00008-5-2 loss: 0.763106  [   96/  130]
train() client id: f_00008-5-3 loss: 0.745527  [  128/  130]
train() client id: f_00008-6-0 loss: 0.738520  [   32/  130]
train() client id: f_00008-6-1 loss: 0.824862  [   64/  130]
train() client id: f_00008-6-2 loss: 0.722325  [   96/  130]
train() client id: f_00008-6-3 loss: 0.776029  [  128/  130]
train() client id: f_00008-7-0 loss: 0.636494  [   32/  130]
train() client id: f_00008-7-1 loss: 0.734379  [   64/  130]
train() client id: f_00008-7-2 loss: 0.814669  [   96/  130]
train() client id: f_00008-7-3 loss: 0.867350  [  128/  130]
train() client id: f_00008-8-0 loss: 0.876397  [   32/  130]
train() client id: f_00008-8-1 loss: 0.755755  [   64/  130]
train() client id: f_00008-8-2 loss: 0.689925  [   96/  130]
train() client id: f_00008-8-3 loss: 0.740230  [  128/  130]
train() client id: f_00008-9-0 loss: 0.753424  [   32/  130]
train() client id: f_00008-9-1 loss: 0.786229  [   64/  130]
train() client id: f_00008-9-2 loss: 0.784070  [   96/  130]
train() client id: f_00008-9-3 loss: 0.715327  [  128/  130]
train() client id: f_00008-10-0 loss: 0.810935  [   32/  130]
train() client id: f_00008-10-1 loss: 0.746194  [   64/  130]
train() client id: f_00008-10-2 loss: 0.763270  [   96/  130]
train() client id: f_00008-10-3 loss: 0.702863  [  128/  130]
train() client id: f_00008-11-0 loss: 0.670134  [   32/  130]
train() client id: f_00008-11-1 loss: 0.754217  [   64/  130]
train() client id: f_00008-11-2 loss: 0.806004  [   96/  130]
train() client id: f_00008-11-3 loss: 0.813165  [  128/  130]
train() client id: f_00008-12-0 loss: 0.784347  [   32/  130]
train() client id: f_00008-12-1 loss: 0.794330  [   64/  130]
train() client id: f_00008-12-2 loss: 0.698509  [   96/  130]
train() client id: f_00008-12-3 loss: 0.785960  [  128/  130]
train() client id: f_00008-13-0 loss: 0.711773  [   32/  130]
train() client id: f_00008-13-1 loss: 0.689385  [   64/  130]
train() client id: f_00008-13-2 loss: 0.792583  [   96/  130]
train() client id: f_00008-13-3 loss: 0.849077  [  128/  130]
train() client id: f_00009-0-0 loss: 1.201715  [   32/  118]
train() client id: f_00009-0-1 loss: 1.079647  [   64/  118]
train() client id: f_00009-0-2 loss: 1.200640  [   96/  118]
train() client id: f_00009-1-0 loss: 1.118532  [   32/  118]
train() client id: f_00009-1-1 loss: 1.133558  [   64/  118]
train() client id: f_00009-1-2 loss: 1.107889  [   96/  118]
train() client id: f_00009-2-0 loss: 1.080756  [   32/  118]
train() client id: f_00009-2-1 loss: 1.138734  [   64/  118]
train() client id: f_00009-2-2 loss: 1.017333  [   96/  118]
train() client id: f_00009-3-0 loss: 1.022438  [   32/  118]
train() client id: f_00009-3-1 loss: 1.073080  [   64/  118]
train() client id: f_00009-3-2 loss: 0.948592  [   96/  118]
train() client id: f_00009-4-0 loss: 1.013033  [   32/  118]
train() client id: f_00009-4-1 loss: 1.033897  [   64/  118]
train() client id: f_00009-4-2 loss: 1.071044  [   96/  118]
train() client id: f_00009-5-0 loss: 1.041178  [   32/  118]
train() client id: f_00009-5-1 loss: 1.003126  [   64/  118]
train() client id: f_00009-5-2 loss: 0.974946  [   96/  118]
train() client id: f_00009-6-0 loss: 1.061509  [   32/  118]
train() client id: f_00009-6-1 loss: 1.018600  [   64/  118]
train() client id: f_00009-6-2 loss: 0.830505  [   96/  118]
train() client id: f_00009-7-0 loss: 0.932128  [   32/  118]
train() client id: f_00009-7-1 loss: 1.037642  [   64/  118]
train() client id: f_00009-7-2 loss: 0.914424  [   96/  118]
train() client id: f_00009-8-0 loss: 0.980500  [   32/  118]
train() client id: f_00009-8-1 loss: 0.863405  [   64/  118]
train() client id: f_00009-8-2 loss: 0.969032  [   96/  118]
train() client id: f_00009-9-0 loss: 0.994114  [   32/  118]
train() client id: f_00009-9-1 loss: 0.879125  [   64/  118]
train() client id: f_00009-9-2 loss: 0.939629  [   96/  118]
train() client id: f_00009-10-0 loss: 0.864735  [   32/  118]
train() client id: f_00009-10-1 loss: 0.987110  [   64/  118]
train() client id: f_00009-10-2 loss: 0.913836  [   96/  118]
train() client id: f_00009-11-0 loss: 0.928078  [   32/  118]
train() client id: f_00009-11-1 loss: 0.972406  [   64/  118]
train() client id: f_00009-11-2 loss: 0.854607  [   96/  118]
train() client id: f_00009-12-0 loss: 0.967932  [   32/  118]
train() client id: f_00009-12-1 loss: 0.878609  [   64/  118]
train() client id: f_00009-12-2 loss: 1.005621  [   96/  118]
train() client id: f_00009-13-0 loss: 0.953712  [   32/  118]
train() client id: f_00009-13-1 loss: 0.853236  [   64/  118]
train() client id: f_00009-13-2 loss: 0.958455  [   96/  118]
At round 21 accuracy: 0.636604774535809
At round 21 training accuracy: 0.5821596244131455
At round 21 training loss: 0.843092623320607
update_location
xs = 8.927491 226.223621 5.882650 10.934260 -142.581990 9.769243 -5.849135 -5.143845 -165.120581 20.134486 
ys = -217.390647 7.291448 115.684448 -37.290817 -9.642386 0.794442 -46.381692 111.628436 25.881276 -652.232496 
xs mean: -3.6823799705212137
ys mean: -70.16579882624053
dists_uav = 239.454366 247.447554 153.027766 107.285428 174.420754 100.479198 110.387833 149.957884 194.768187 660.161061 
uav_gains = -110.428336 -111.080555 -104.625176 -100.763562 -106.071216 -100.051921 -101.073096 -104.403767 -107.353810 -127.433805 
uav_gains_db_mean: -107.32852443534762
dists_bs = 433.358676 434.863832 190.359837 282.203721 187.466632 253.943398 278.607015 181.292770 149.445631 849.935921 
bs_gains = -113.398789 -113.440951 -103.395168 -108.182816 -103.208931 -106.899693 -108.026836 -102.801713 -100.452589 -121.589872 
bs_gains_db_mean: -108.13973587924765
Round 22
-------------------------------
ene_coms = [0.01100815 0.01153066 0.00733378 0.00650862 0.0072687  0.00631603
 0.00659605 0.00713025 0.00642305 0.03462892]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.15469062 16.90837212  7.95226561  2.84034054 19.39829507  9.31479028
  3.53403901 11.40739612  8.31065857  8.14412339]
obj_prev = 95.96497132689575
eta_min = 3.198533550072216e-12	eta_max = 0.7484269349143726
af = 20.160538373523963	bf = 2.1626113624053893	zeta = 22.17659221087636	eta = 0.909090909090909
af = 20.160538373523963	bf = 2.1626113624053893	zeta = 44.92527489711738	eta = 0.44875715106236463
af = 20.160538373523963	bf = 2.1626113624053893	zeta = 33.22803117089022	eta = 0.6067328596701758
af = 20.160538373523963	bf = 2.1626113624053893	zeta = 31.114119304820235	eta = 0.6479546528704307
af = 20.160538373523963	bf = 2.1626113624053893	zeta = 30.993069231054456	eta = 0.6504853786253442
af = 20.160538373523963	bf = 2.1626113624053893	zeta = 30.992631557683687	eta = 0.6504945646839003
af = 20.160538373523963	bf = 2.1626113624053893	zeta = 30.99263155193202	eta = 0.6504945648046203
eta = 0.6504945648046203
ene_coms = [0.01100815 0.01153066 0.00733378 0.00650862 0.0072687  0.00631603
 0.00659605 0.00713025 0.00642305 0.03462892]
ene_comp = [0.03548441 0.07462992 0.03492117 0.01210976 0.08617643 0.04111686
 0.01520761 0.0504104  0.0366109  0.03323143]
ene_total = [2.74648988 5.08982791 2.49615804 1.09985719 5.52015332 2.80203823
 1.28802356 3.39914143 2.54217667 4.00876532]
ti_comp = [0.37878902 0.37356392 0.41553274 0.42378439 0.41618361 0.42571027
 0.42291005 0.41756803 0.42464005 0.14258132]
ti_coms = [0.11008153 0.11530664 0.07333782 0.06508617 0.07268695 0.06316029
 0.06596051 0.07130252 0.06423051 0.34628924]
t_total = [28.8794136 28.8794136 28.8794136 28.8794136 28.8794136 28.8794136
 28.8794136 28.8794136 28.8794136 28.8794136]
ene_coms = [0.01100815 0.01153066 0.00733378 0.00650862 0.0072687  0.00631603
 0.00659605 0.00713025 0.00642305 0.03462892]
ene_comp = [1.94624766e-05 1.86161136e-04 1.54147258e-05 6.18011697e-07
 2.30927462e-04 2.39724025e-05 1.22904138e-06 4.59183042e-05
 1.70086326e-05 1.12824131e-04]
ene_total = [0.65144254 0.69215672 0.43414455 0.38452486 0.44303078 0.37452762
 0.389726   0.42392325 0.38043844 2.05232508]
optimize_network iter = 0 obj = 6.226239830792208
eta = 0.6504945648046203
freqs = [4.68392829e+07 9.98890844e+07 4.20197552e+07 1.42876426e+07
 1.03531746e+08 4.82920688e+07 1.79797161e+07 6.03618947e+07
 4.31081597e+07 1.16535019e+08]
eta_min = 0.6504945648046225	eta_max = 0.6504945648046191
af = 0.03137813583595596	bf = 2.1626113624053893	zeta = 0.03451594941955156	eta = 0.9090909090909091
af = 0.03137813583595596	bf = 2.1626113624053893	zeta = 23.821622199912216	eta = 0.0013172123868235802
af = 0.03137813583595596	bf = 2.1626113624053893	zeta = 2.3738454734896126	eta = 0.01321827228704542
af = 0.03137813583595596	bf = 2.1626113624053893	zeta = 2.329145323190004	eta = 0.013471952790382513
af = 0.03137813583595596	bf = 2.1626113624053893	zeta = 2.329139612402953	eta = 0.013471985822087931
eta = 0.013471985822087931
ene_coms = [0.01100815 0.01153066 0.00733378 0.00650862 0.0072687  0.00631603
 0.00659605 0.00713025 0.00642305 0.03462892]
ene_comp = [1.94937937e-04 1.86460689e-03 1.54395297e-04 6.19006142e-06
 2.31299049e-03 2.40109766e-04 1.23101903e-05 4.59921916e-04
 1.70360013e-04 1.13005676e-03]
ene_total = [0.23446438 0.28034351 0.15671664 0.13634541 0.20053072 0.13721044
 0.13830336 0.15885129 0.13799049 0.74838337]
ti_comp = [0.37878902 0.37356392 0.41553274 0.42378439 0.41618361 0.42571027
 0.42291005 0.41756803 0.42464005 0.14258132]
ti_coms = [0.11008153 0.11530664 0.07333782 0.06508617 0.07268695 0.06316029
 0.06596051 0.07130252 0.06423051 0.34628924]
t_total = [28.8794136 28.8794136 28.8794136 28.8794136 28.8794136 28.8794136
 28.8794136 28.8794136 28.8794136 28.8794136]
ene_coms = [0.01100815 0.01153066 0.00733378 0.00650862 0.0072687  0.00631603
 0.00659605 0.00713025 0.00642305 0.03462892]
ene_comp = [1.94624766e-05 1.86161136e-04 1.54147258e-05 6.18011697e-07
 2.30927462e-04 2.39724025e-05 1.22904138e-06 4.59183042e-05
 1.70086326e-05 1.12824131e-04]
ene_total = [0.65144254 0.69215672 0.43414455 0.38452486 0.44303078 0.37452762
 0.389726   0.42392325 0.38043844 2.05232508]
optimize_network iter = 1 obj = 6.226239830792247
eta = 0.6504945648046225
freqs = [4.68392829e+07 9.98890844e+07 4.20197552e+07 1.42876426e+07
 1.03531746e+08 4.82920688e+07 1.79797161e+07 6.03618947e+07
 4.31081597e+07 1.16535019e+08]
Done!
ene_coms = [0.01100815 0.01153066 0.00733378 0.00650862 0.0072687  0.00631603
 0.00659605 0.00713025 0.00642305 0.03462892]
ene_comp = [1.93503605e-05 1.85088731e-04 1.53259273e-05 6.14451561e-07
 2.29597175e-04 2.38343064e-05 1.22196133e-06 4.56537859e-05
 1.69106522e-05 1.12174193e-04]
ene_total = [0.0110275  0.01171575 0.00734911 0.00650923 0.00749829 0.00633986
 0.00659727 0.00717591 0.00643996 0.0347411 ]
At round 22 energy consumption: 0.10539398935817462
At round 22 eta: 0.6504945648046225
At round 22 a_n: 20.646594222929394
At round 22 local rounds: 14.081116071707665
At round 22 global rounds: 59.07374290585128
gradient difference: 0.3824450373649597
train() client id: f_00000-0-0 loss: 1.392193  [   32/  126]
train() client id: f_00000-0-1 loss: 1.314230  [   64/  126]
train() client id: f_00000-0-2 loss: 1.429967  [   96/  126]
train() client id: f_00000-1-0 loss: 1.255473  [   32/  126]
train() client id: f_00000-1-1 loss: 1.269375  [   64/  126]
train() client id: f_00000-1-2 loss: 1.108698  [   96/  126]
train() client id: f_00000-2-0 loss: 1.171187  [   32/  126]
train() client id: f_00000-2-1 loss: 1.170988  [   64/  126]
train() client id: f_00000-2-2 loss: 1.204399  [   96/  126]
train() client id: f_00000-3-0 loss: 1.158635  [   32/  126]
train() client id: f_00000-3-1 loss: 0.944899  [   64/  126]
train() client id: f_00000-3-2 loss: 1.071613  [   96/  126]
train() client id: f_00000-4-0 loss: 0.920100  [   32/  126]
train() client id: f_00000-4-1 loss: 1.109035  [   64/  126]
train() client id: f_00000-4-2 loss: 0.913598  [   96/  126]
train() client id: f_00000-5-0 loss: 0.852261  [   32/  126]
train() client id: f_00000-5-1 loss: 0.969431  [   64/  126]
train() client id: f_00000-5-2 loss: 0.937735  [   96/  126]
train() client id: f_00000-6-0 loss: 0.880241  [   32/  126]
train() client id: f_00000-6-1 loss: 0.894581  [   64/  126]
train() client id: f_00000-6-2 loss: 0.866007  [   96/  126]
train() client id: f_00000-7-0 loss: 0.865639  [   32/  126]
train() client id: f_00000-7-1 loss: 0.885102  [   64/  126]
train() client id: f_00000-7-2 loss: 0.761204  [   96/  126]
train() client id: f_00000-8-0 loss: 0.781455  [   32/  126]
train() client id: f_00000-8-1 loss: 0.883056  [   64/  126]
train() client id: f_00000-8-2 loss: 0.795692  [   96/  126]
train() client id: f_00000-9-0 loss: 0.770300  [   32/  126]
train() client id: f_00000-9-1 loss: 0.808595  [   64/  126]
train() client id: f_00000-9-2 loss: 0.838296  [   96/  126]
train() client id: f_00000-10-0 loss: 0.834439  [   32/  126]
train() client id: f_00000-10-1 loss: 0.802531  [   64/  126]
train() client id: f_00000-10-2 loss: 0.803999  [   96/  126]
train() client id: f_00000-11-0 loss: 0.788272  [   32/  126]
train() client id: f_00000-11-1 loss: 0.780547  [   64/  126]
train() client id: f_00000-11-2 loss: 0.813564  [   96/  126]
train() client id: f_00000-12-0 loss: 0.662287  [   32/  126]
train() client id: f_00000-12-1 loss: 0.766966  [   64/  126]
train() client id: f_00000-12-2 loss: 0.757603  [   96/  126]
train() client id: f_00000-13-0 loss: 0.716964  [   32/  126]
train() client id: f_00000-13-1 loss: 0.701316  [   64/  126]
train() client id: f_00000-13-2 loss: 0.880219  [   96/  126]
train() client id: f_00001-0-0 loss: 0.368887  [   32/  265]
train() client id: f_00001-0-1 loss: 0.315274  [   64/  265]
train() client id: f_00001-0-2 loss: 0.406877  [   96/  265]
train() client id: f_00001-0-3 loss: 0.438733  [  128/  265]
train() client id: f_00001-0-4 loss: 0.293960  [  160/  265]
train() client id: f_00001-0-5 loss: 0.283876  [  192/  265]
train() client id: f_00001-0-6 loss: 0.505389  [  224/  265]
train() client id: f_00001-0-7 loss: 0.429046  [  256/  265]
train() client id: f_00001-1-0 loss: 0.371992  [   32/  265]
train() client id: f_00001-1-1 loss: 0.284093  [   64/  265]
train() client id: f_00001-1-2 loss: 0.302204  [   96/  265]
train() client id: f_00001-1-3 loss: 0.418283  [  128/  265]
train() client id: f_00001-1-4 loss: 0.379703  [  160/  265]
train() client id: f_00001-1-5 loss: 0.396749  [  192/  265]
train() client id: f_00001-1-6 loss: 0.383788  [  224/  265]
train() client id: f_00001-1-7 loss: 0.359764  [  256/  265]
train() client id: f_00001-2-0 loss: 0.335408  [   32/  265]
train() client id: f_00001-2-1 loss: 0.439106  [   64/  265]
train() client id: f_00001-2-2 loss: 0.355442  [   96/  265]
train() client id: f_00001-2-3 loss: 0.361480  [  128/  265]
train() client id: f_00001-2-4 loss: 0.361333  [  160/  265]
train() client id: f_00001-2-5 loss: 0.342675  [  192/  265]
train() client id: f_00001-2-6 loss: 0.354944  [  224/  265]
train() client id: f_00001-2-7 loss: 0.259010  [  256/  265]
train() client id: f_00001-3-0 loss: 0.331541  [   32/  265]
train() client id: f_00001-3-1 loss: 0.391726  [   64/  265]
train() client id: f_00001-3-2 loss: 0.520691  [   96/  265]
train() client id: f_00001-3-3 loss: 0.384218  [  128/  265]
train() client id: f_00001-3-4 loss: 0.326072  [  160/  265]
train() client id: f_00001-3-5 loss: 0.269338  [  192/  265]
train() client id: f_00001-3-6 loss: 0.294724  [  224/  265]
train() client id: f_00001-3-7 loss: 0.267595  [  256/  265]
train() client id: f_00001-4-0 loss: 0.369855  [   32/  265]
train() client id: f_00001-4-1 loss: 0.383648  [   64/  265]
train() client id: f_00001-4-2 loss: 0.264742  [   96/  265]
train() client id: f_00001-4-3 loss: 0.293035  [  128/  265]
train() client id: f_00001-4-4 loss: 0.379806  [  160/  265]
train() client id: f_00001-4-5 loss: 0.424675  [  192/  265]
train() client id: f_00001-4-6 loss: 0.374562  [  224/  265]
train() client id: f_00001-4-7 loss: 0.285369  [  256/  265]
train() client id: f_00001-5-0 loss: 0.318511  [   32/  265]
train() client id: f_00001-5-1 loss: 0.232999  [   64/  265]
train() client id: f_00001-5-2 loss: 0.393072  [   96/  265]
train() client id: f_00001-5-3 loss: 0.452674  [  128/  265]
train() client id: f_00001-5-4 loss: 0.251234  [  160/  265]
train() client id: f_00001-5-5 loss: 0.335038  [  192/  265]
train() client id: f_00001-5-6 loss: 0.322090  [  224/  265]
train() client id: f_00001-5-7 loss: 0.366091  [  256/  265]
train() client id: f_00001-6-0 loss: 0.367011  [   32/  265]
train() client id: f_00001-6-1 loss: 0.386958  [   64/  265]
train() client id: f_00001-6-2 loss: 0.246399  [   96/  265]
train() client id: f_00001-6-3 loss: 0.215121  [  128/  265]
train() client id: f_00001-6-4 loss: 0.396005  [  160/  265]
train() client id: f_00001-6-5 loss: 0.426188  [  192/  265]
train() client id: f_00001-6-6 loss: 0.300939  [  224/  265]
train() client id: f_00001-6-7 loss: 0.277907  [  256/  265]
train() client id: f_00001-7-0 loss: 0.378927  [   32/  265]
train() client id: f_00001-7-1 loss: 0.397303  [   64/  265]
train() client id: f_00001-7-2 loss: 0.483081  [   96/  265]
train() client id: f_00001-7-3 loss: 0.243704  [  128/  265]
train() client id: f_00001-7-4 loss: 0.354740  [  160/  265]
train() client id: f_00001-7-5 loss: 0.246808  [  192/  265]
train() client id: f_00001-7-6 loss: 0.277479  [  224/  265]
train() client id: f_00001-7-7 loss: 0.282786  [  256/  265]
train() client id: f_00001-8-0 loss: 0.441886  [   32/  265]
train() client id: f_00001-8-1 loss: 0.225211  [   64/  265]
train() client id: f_00001-8-2 loss: 0.333182  [   96/  265]
train() client id: f_00001-8-3 loss: 0.367234  [  128/  265]
train() client id: f_00001-8-4 loss: 0.291738  [  160/  265]
train() client id: f_00001-8-5 loss: 0.284644  [  192/  265]
train() client id: f_00001-8-6 loss: 0.235543  [  224/  265]
train() client id: f_00001-8-7 loss: 0.382780  [  256/  265]
train() client id: f_00001-9-0 loss: 0.579778  [   32/  265]
train() client id: f_00001-9-1 loss: 0.343703  [   64/  265]
train() client id: f_00001-9-2 loss: 0.331837  [   96/  265]
train() client id: f_00001-9-3 loss: 0.321818  [  128/  265]
train() client id: f_00001-9-4 loss: 0.356543  [  160/  265]
train() client id: f_00001-9-5 loss: 0.248081  [  192/  265]
train() client id: f_00001-9-6 loss: 0.214570  [  224/  265]
train() client id: f_00001-9-7 loss: 0.221573  [  256/  265]
train() client id: f_00001-10-0 loss: 0.272762  [   32/  265]
train() client id: f_00001-10-1 loss: 0.346715  [   64/  265]
train() client id: f_00001-10-2 loss: 0.338097  [   96/  265]
train() client id: f_00001-10-3 loss: 0.317618  [  128/  265]
train() client id: f_00001-10-4 loss: 0.286192  [  160/  265]
train() client id: f_00001-10-5 loss: 0.320864  [  192/  265]
train() client id: f_00001-10-6 loss: 0.291007  [  224/  265]
train() client id: f_00001-10-7 loss: 0.338919  [  256/  265]
train() client id: f_00001-11-0 loss: 0.338484  [   32/  265]
train() client id: f_00001-11-1 loss: 0.421780  [   64/  265]
train() client id: f_00001-11-2 loss: 0.411030  [   96/  265]
train() client id: f_00001-11-3 loss: 0.231105  [  128/  265]
train() client id: f_00001-11-4 loss: 0.446623  [  160/  265]
train() client id: f_00001-11-5 loss: 0.225623  [  192/  265]
train() client id: f_00001-11-6 loss: 0.213697  [  224/  265]
train() client id: f_00001-11-7 loss: 0.286800  [  256/  265]
train() client id: f_00001-12-0 loss: 0.236881  [   32/  265]
train() client id: f_00001-12-1 loss: 0.328459  [   64/  265]
train() client id: f_00001-12-2 loss: 0.284759  [   96/  265]
train() client id: f_00001-12-3 loss: 0.327802  [  128/  265]
train() client id: f_00001-12-4 loss: 0.353659  [  160/  265]
train() client id: f_00001-12-5 loss: 0.414476  [  192/  265]
train() client id: f_00001-12-6 loss: 0.313473  [  224/  265]
train() client id: f_00001-12-7 loss: 0.324488  [  256/  265]
train() client id: f_00001-13-0 loss: 0.319004  [   32/  265]
train() client id: f_00001-13-1 loss: 0.291615  [   64/  265]
train() client id: f_00001-13-2 loss: 0.484668  [   96/  265]
train() client id: f_00001-13-3 loss: 0.317971  [  128/  265]
train() client id: f_00001-13-4 loss: 0.254850  [  160/  265]
train() client id: f_00001-13-5 loss: 0.205145  [  192/  265]
train() client id: f_00001-13-6 loss: 0.336551  [  224/  265]
train() client id: f_00001-13-7 loss: 0.310617  [  256/  265]
train() client id: f_00002-0-0 loss: 1.228842  [   32/  124]
train() client id: f_00002-0-1 loss: 1.104846  [   64/  124]
train() client id: f_00002-0-2 loss: 1.130197  [   96/  124]
train() client id: f_00002-1-0 loss: 1.127439  [   32/  124]
train() client id: f_00002-1-1 loss: 1.067693  [   64/  124]
train() client id: f_00002-1-2 loss: 1.197052  [   96/  124]
train() client id: f_00002-2-0 loss: 1.190069  [   32/  124]
train() client id: f_00002-2-1 loss: 1.003484  [   64/  124]
train() client id: f_00002-2-2 loss: 1.032134  [   96/  124]
train() client id: f_00002-3-0 loss: 1.027856  [   32/  124]
train() client id: f_00002-3-1 loss: 1.024670  [   64/  124]
train() client id: f_00002-3-2 loss: 1.153625  [   96/  124]
train() client id: f_00002-4-0 loss: 1.016672  [   32/  124]
train() client id: f_00002-4-1 loss: 0.943213  [   64/  124]
train() client id: f_00002-4-2 loss: 1.007337  [   96/  124]
train() client id: f_00002-5-0 loss: 1.100461  [   32/  124]
train() client id: f_00002-5-1 loss: 0.872426  [   64/  124]
train() client id: f_00002-5-2 loss: 1.030497  [   96/  124]
train() client id: f_00002-6-0 loss: 1.073358  [   32/  124]
train() client id: f_00002-6-1 loss: 0.892605  [   64/  124]
train() client id: f_00002-6-2 loss: 0.900141  [   96/  124]
train() client id: f_00002-7-0 loss: 0.921317  [   32/  124]
train() client id: f_00002-7-1 loss: 1.106563  [   64/  124]
train() client id: f_00002-7-2 loss: 0.852835  [   96/  124]
train() client id: f_00002-8-0 loss: 1.148305  [   32/  124]
train() client id: f_00002-8-1 loss: 0.773763  [   64/  124]
train() client id: f_00002-8-2 loss: 0.910201  [   96/  124]
train() client id: f_00002-9-0 loss: 1.040621  [   32/  124]
train() client id: f_00002-9-1 loss: 0.851179  [   64/  124]
train() client id: f_00002-9-2 loss: 0.849437  [   96/  124]
train() client id: f_00002-10-0 loss: 1.079254  [   32/  124]
train() client id: f_00002-10-1 loss: 0.915557  [   64/  124]
train() client id: f_00002-10-2 loss: 0.855450  [   96/  124]
train() client id: f_00002-11-0 loss: 1.054985  [   32/  124]
train() client id: f_00002-11-1 loss: 0.907327  [   64/  124]
train() client id: f_00002-11-2 loss: 0.863082  [   96/  124]
train() client id: f_00002-12-0 loss: 0.931130  [   32/  124]
train() client id: f_00002-12-1 loss: 0.883498  [   64/  124]
train() client id: f_00002-12-2 loss: 0.841773  [   96/  124]
train() client id: f_00002-13-0 loss: 0.975460  [   32/  124]
train() client id: f_00002-13-1 loss: 0.795656  [   64/  124]
train() client id: f_00002-13-2 loss: 0.897705  [   96/  124]
train() client id: f_00003-0-0 loss: 0.571190  [   32/   43]
train() client id: f_00003-1-0 loss: 0.603363  [   32/   43]
train() client id: f_00003-2-0 loss: 0.469174  [   32/   43]
train() client id: f_00003-3-0 loss: 0.552731  [   32/   43]
train() client id: f_00003-4-0 loss: 0.736183  [   32/   43]
train() client id: f_00003-5-0 loss: 0.621841  [   32/   43]
train() client id: f_00003-6-0 loss: 0.711145  [   32/   43]
train() client id: f_00003-7-0 loss: 0.677565  [   32/   43]
train() client id: f_00003-8-0 loss: 0.534894  [   32/   43]
train() client id: f_00003-9-0 loss: 0.587996  [   32/   43]
train() client id: f_00003-10-0 loss: 0.584697  [   32/   43]
train() client id: f_00003-11-0 loss: 0.662110  [   32/   43]
train() client id: f_00003-12-0 loss: 0.650310  [   32/   43]
train() client id: f_00003-13-0 loss: 0.621984  [   32/   43]
train() client id: f_00004-0-0 loss: 0.869515  [   32/  306]
train() client id: f_00004-0-1 loss: 0.906478  [   64/  306]
train() client id: f_00004-0-2 loss: 0.984103  [   96/  306]
train() client id: f_00004-0-3 loss: 0.873601  [  128/  306]
train() client id: f_00004-0-4 loss: 0.900940  [  160/  306]
train() client id: f_00004-0-5 loss: 0.886888  [  192/  306]
train() client id: f_00004-0-6 loss: 0.672592  [  224/  306]
train() client id: f_00004-0-7 loss: 0.903586  [  256/  306]
train() client id: f_00004-0-8 loss: 0.931360  [  288/  306]
train() client id: f_00004-1-0 loss: 0.864553  [   32/  306]
train() client id: f_00004-1-1 loss: 0.872851  [   64/  306]
train() client id: f_00004-1-2 loss: 0.883276  [   96/  306]
train() client id: f_00004-1-3 loss: 0.884888  [  128/  306]
train() client id: f_00004-1-4 loss: 0.831297  [  160/  306]
train() client id: f_00004-1-5 loss: 0.924827  [  192/  306]
train() client id: f_00004-1-6 loss: 0.985007  [  224/  306]
train() client id: f_00004-1-7 loss: 0.745445  [  256/  306]
train() client id: f_00004-1-8 loss: 0.867053  [  288/  306]
train() client id: f_00004-2-0 loss: 0.841888  [   32/  306]
train() client id: f_00004-2-1 loss: 0.867819  [   64/  306]
train() client id: f_00004-2-2 loss: 0.990893  [   96/  306]
train() client id: f_00004-2-3 loss: 0.845467  [  128/  306]
train() client id: f_00004-2-4 loss: 0.817821  [  160/  306]
train() client id: f_00004-2-5 loss: 0.918192  [  192/  306]
train() client id: f_00004-2-6 loss: 0.903136  [  224/  306]
train() client id: f_00004-2-7 loss: 0.895244  [  256/  306]
train() client id: f_00004-2-8 loss: 0.951715  [  288/  306]
train() client id: f_00004-3-0 loss: 0.865355  [   32/  306]
train() client id: f_00004-3-1 loss: 0.808755  [   64/  306]
train() client id: f_00004-3-2 loss: 0.865654  [   96/  306]
train() client id: f_00004-3-3 loss: 1.012639  [  128/  306]
train() client id: f_00004-3-4 loss: 0.780332  [  160/  306]
train() client id: f_00004-3-5 loss: 0.910394  [  192/  306]
train() client id: f_00004-3-6 loss: 0.837914  [  224/  306]
train() client id: f_00004-3-7 loss: 0.981603  [  256/  306]
train() client id: f_00004-3-8 loss: 0.851271  [  288/  306]
train() client id: f_00004-4-0 loss: 0.860650  [   32/  306]
train() client id: f_00004-4-1 loss: 1.009348  [   64/  306]
train() client id: f_00004-4-2 loss: 0.822064  [   96/  306]
train() client id: f_00004-4-3 loss: 0.880464  [  128/  306]
train() client id: f_00004-4-4 loss: 0.885632  [  160/  306]
train() client id: f_00004-4-5 loss: 0.840783  [  192/  306]
train() client id: f_00004-4-6 loss: 0.872797  [  224/  306]
train() client id: f_00004-4-7 loss: 0.828154  [  256/  306]
train() client id: f_00004-4-8 loss: 0.928704  [  288/  306]
train() client id: f_00004-5-0 loss: 0.797909  [   32/  306]
train() client id: f_00004-5-1 loss: 0.852800  [   64/  306]
train() client id: f_00004-5-2 loss: 0.917310  [   96/  306]
train() client id: f_00004-5-3 loss: 0.952662  [  128/  306]
train() client id: f_00004-5-4 loss: 0.930142  [  160/  306]
train() client id: f_00004-5-5 loss: 0.954628  [  192/  306]
train() client id: f_00004-5-6 loss: 0.835214  [  224/  306]
train() client id: f_00004-5-7 loss: 0.799452  [  256/  306]
train() client id: f_00004-5-8 loss: 0.916060  [  288/  306]
train() client id: f_00004-6-0 loss: 0.852256  [   32/  306]
train() client id: f_00004-6-1 loss: 0.818789  [   64/  306]
train() client id: f_00004-6-2 loss: 0.844631  [   96/  306]
train() client id: f_00004-6-3 loss: 0.942086  [  128/  306]
train() client id: f_00004-6-4 loss: 0.863675  [  160/  306]
train() client id: f_00004-6-5 loss: 0.938902  [  192/  306]
train() client id: f_00004-6-6 loss: 0.830698  [  224/  306]
train() client id: f_00004-6-7 loss: 0.851019  [  256/  306]
train() client id: f_00004-6-8 loss: 0.923490  [  288/  306]
train() client id: f_00004-7-0 loss: 0.836345  [   32/  306]
train() client id: f_00004-7-1 loss: 0.773677  [   64/  306]
train() client id: f_00004-7-2 loss: 0.911738  [   96/  306]
train() client id: f_00004-7-3 loss: 0.911513  [  128/  306]
train() client id: f_00004-7-4 loss: 0.906572  [  160/  306]
train() client id: f_00004-7-5 loss: 0.911655  [  192/  306]
train() client id: f_00004-7-6 loss: 0.912419  [  224/  306]
train() client id: f_00004-7-7 loss: 0.992339  [  256/  306]
train() client id: f_00004-7-8 loss: 0.852284  [  288/  306]
train() client id: f_00004-8-0 loss: 0.861547  [   32/  306]
train() client id: f_00004-8-1 loss: 0.945941  [   64/  306]
train() client id: f_00004-8-2 loss: 1.029036  [   96/  306]
train() client id: f_00004-8-3 loss: 0.811621  [  128/  306]
train() client id: f_00004-8-4 loss: 0.825572  [  160/  306]
train() client id: f_00004-8-5 loss: 0.824669  [  192/  306]
train() client id: f_00004-8-6 loss: 0.891393  [  224/  306]
train() client id: f_00004-8-7 loss: 0.900099  [  256/  306]
train() client id: f_00004-8-8 loss: 0.874422  [  288/  306]
train() client id: f_00004-9-0 loss: 0.884269  [   32/  306]
train() client id: f_00004-9-1 loss: 0.973932  [   64/  306]
train() client id: f_00004-9-2 loss: 0.958672  [   96/  306]
train() client id: f_00004-9-3 loss: 0.946522  [  128/  306]
train() client id: f_00004-9-4 loss: 0.898382  [  160/  306]
train() client id: f_00004-9-5 loss: 0.844428  [  192/  306]
train() client id: f_00004-9-6 loss: 0.819976  [  224/  306]
train() client id: f_00004-9-7 loss: 0.868531  [  256/  306]
train() client id: f_00004-9-8 loss: 0.804155  [  288/  306]
train() client id: f_00004-10-0 loss: 0.908176  [   32/  306]
train() client id: f_00004-10-1 loss: 1.041403  [   64/  306]
train() client id: f_00004-10-2 loss: 0.816980  [   96/  306]
train() client id: f_00004-10-3 loss: 0.702537  [  128/  306]
train() client id: f_00004-10-4 loss: 0.953995  [  160/  306]
train() client id: f_00004-10-5 loss: 0.829266  [  192/  306]
train() client id: f_00004-10-6 loss: 0.836526  [  224/  306]
train() client id: f_00004-10-7 loss: 1.057541  [  256/  306]
train() client id: f_00004-10-8 loss: 0.834566  [  288/  306]
train() client id: f_00004-11-0 loss: 0.805204  [   32/  306]
train() client id: f_00004-11-1 loss: 0.880178  [   64/  306]
train() client id: f_00004-11-2 loss: 0.895100  [   96/  306]
train() client id: f_00004-11-3 loss: 0.959211  [  128/  306]
train() client id: f_00004-11-4 loss: 0.893043  [  160/  306]
train() client id: f_00004-11-5 loss: 0.843173  [  192/  306]
train() client id: f_00004-11-6 loss: 0.804019  [  224/  306]
train() client id: f_00004-11-7 loss: 0.821356  [  256/  306]
train() client id: f_00004-11-8 loss: 1.011128  [  288/  306]
train() client id: f_00004-12-0 loss: 0.891124  [   32/  306]
train() client id: f_00004-12-1 loss: 0.806747  [   64/  306]
train() client id: f_00004-12-2 loss: 0.866386  [   96/  306]
train() client id: f_00004-12-3 loss: 0.960436  [  128/  306]
train() client id: f_00004-12-4 loss: 0.862506  [  160/  306]
train() client id: f_00004-12-5 loss: 0.946942  [  192/  306]
train() client id: f_00004-12-6 loss: 0.894253  [  224/  306]
train() client id: f_00004-12-7 loss: 0.863515  [  256/  306]
train() client id: f_00004-12-8 loss: 0.887358  [  288/  306]
train() client id: f_00004-13-0 loss: 0.927074  [   32/  306]
train() client id: f_00004-13-1 loss: 0.824644  [   64/  306]
train() client id: f_00004-13-2 loss: 0.946423  [   96/  306]
train() client id: f_00004-13-3 loss: 0.967177  [  128/  306]
train() client id: f_00004-13-4 loss: 0.894968  [  160/  306]
train() client id: f_00004-13-5 loss: 0.893023  [  192/  306]
train() client id: f_00004-13-6 loss: 0.872515  [  224/  306]
train() client id: f_00004-13-7 loss: 0.841656  [  256/  306]
train() client id: f_00004-13-8 loss: 0.829090  [  288/  306]
train() client id: f_00005-0-0 loss: 0.662772  [   32/  146]
train() client id: f_00005-0-1 loss: 0.543249  [   64/  146]
train() client id: f_00005-0-2 loss: 0.676505  [   96/  146]
train() client id: f_00005-0-3 loss: 0.499367  [  128/  146]
train() client id: f_00005-1-0 loss: 0.466882  [   32/  146]
train() client id: f_00005-1-1 loss: 0.813497  [   64/  146]
train() client id: f_00005-1-2 loss: 0.387931  [   96/  146]
train() client id: f_00005-1-3 loss: 0.744878  [  128/  146]
train() client id: f_00005-2-0 loss: 0.497078  [   32/  146]
train() client id: f_00005-2-1 loss: 0.523283  [   64/  146]
train() client id: f_00005-2-2 loss: 0.411647  [   96/  146]
train() client id: f_00005-2-3 loss: 0.836129  [  128/  146]
train() client id: f_00005-3-0 loss: 0.651652  [   32/  146]
train() client id: f_00005-3-1 loss: 0.550513  [   64/  146]
train() client id: f_00005-3-2 loss: 0.539308  [   96/  146]
train() client id: f_00005-3-3 loss: 0.537211  [  128/  146]
train() client id: f_00005-4-0 loss: 0.522959  [   32/  146]
train() client id: f_00005-4-1 loss: 0.772812  [   64/  146]
train() client id: f_00005-4-2 loss: 0.409971  [   96/  146]
train() client id: f_00005-4-3 loss: 0.633440  [  128/  146]
train() client id: f_00005-5-0 loss: 0.591384  [   32/  146]
train() client id: f_00005-5-1 loss: 0.433440  [   64/  146]
train() client id: f_00005-5-2 loss: 0.663272  [   96/  146]
train() client id: f_00005-5-3 loss: 0.563389  [  128/  146]
train() client id: f_00005-6-0 loss: 0.597091  [   32/  146]
train() client id: f_00005-6-1 loss: 0.708023  [   64/  146]
train() client id: f_00005-6-2 loss: 0.535267  [   96/  146]
train() client id: f_00005-6-3 loss: 0.619076  [  128/  146]
train() client id: f_00005-7-0 loss: 0.571414  [   32/  146]
train() client id: f_00005-7-1 loss: 0.681669  [   64/  146]
train() client id: f_00005-7-2 loss: 0.649792  [   96/  146]
train() client id: f_00005-7-3 loss: 0.426007  [  128/  146]
train() client id: f_00005-8-0 loss: 0.716843  [   32/  146]
train() client id: f_00005-8-1 loss: 0.385480  [   64/  146]
train() client id: f_00005-8-2 loss: 0.652054  [   96/  146]
train() client id: f_00005-8-3 loss: 0.489449  [  128/  146]
train() client id: f_00005-9-0 loss: 0.546182  [   32/  146]
train() client id: f_00005-9-1 loss: 0.501376  [   64/  146]
train() client id: f_00005-9-2 loss: 0.605215  [   96/  146]
train() client id: f_00005-9-3 loss: 0.615601  [  128/  146]
train() client id: f_00005-10-0 loss: 0.562805  [   32/  146]
train() client id: f_00005-10-1 loss: 0.666810  [   64/  146]
train() client id: f_00005-10-2 loss: 0.508006  [   96/  146]
train() client id: f_00005-10-3 loss: 0.731523  [  128/  146]
train() client id: f_00005-11-0 loss: 0.455795  [   32/  146]
train() client id: f_00005-11-1 loss: 0.699074  [   64/  146]
train() client id: f_00005-11-2 loss: 0.639694  [   96/  146]
train() client id: f_00005-11-3 loss: 0.599041  [  128/  146]
train() client id: f_00005-12-0 loss: 0.696018  [   32/  146]
train() client id: f_00005-12-1 loss: 0.454816  [   64/  146]
train() client id: f_00005-12-2 loss: 0.482935  [   96/  146]
train() client id: f_00005-12-3 loss: 0.757526  [  128/  146]
train() client id: f_00005-13-0 loss: 0.511010  [   32/  146]
train() client id: f_00005-13-1 loss: 0.420874  [   64/  146]
train() client id: f_00005-13-2 loss: 0.638149  [   96/  146]
train() client id: f_00005-13-3 loss: 0.808048  [  128/  146]
train() client id: f_00006-0-0 loss: 0.588022  [   32/   54]
train() client id: f_00006-1-0 loss: 0.531736  [   32/   54]
train() client id: f_00006-2-0 loss: 0.651776  [   32/   54]
train() client id: f_00006-3-0 loss: 0.636172  [   32/   54]
train() client id: f_00006-4-0 loss: 0.539763  [   32/   54]
train() client id: f_00006-5-0 loss: 0.551385  [   32/   54]
train() client id: f_00006-6-0 loss: 0.582757  [   32/   54]
train() client id: f_00006-7-0 loss: 0.621582  [   32/   54]
train() client id: f_00006-8-0 loss: 0.605854  [   32/   54]
train() client id: f_00006-9-0 loss: 0.629250  [   32/   54]
train() client id: f_00006-10-0 loss: 0.643098  [   32/   54]
train() client id: f_00006-11-0 loss: 0.578163  [   32/   54]
train() client id: f_00006-12-0 loss: 0.631162  [   32/   54]
train() client id: f_00006-13-0 loss: 0.603124  [   32/   54]
train() client id: f_00007-0-0 loss: 0.732769  [   32/  179]
train() client id: f_00007-0-1 loss: 0.646790  [   64/  179]
train() client id: f_00007-0-2 loss: 0.635759  [   96/  179]
train() client id: f_00007-0-3 loss: 0.920992  [  128/  179]
train() client id: f_00007-0-4 loss: 0.747965  [  160/  179]
train() client id: f_00007-1-0 loss: 0.684108  [   32/  179]
train() client id: f_00007-1-1 loss: 0.692864  [   64/  179]
train() client id: f_00007-1-2 loss: 0.615080  [   96/  179]
train() client id: f_00007-1-3 loss: 0.804242  [  128/  179]
train() client id: f_00007-1-4 loss: 0.826137  [  160/  179]
train() client id: f_00007-2-0 loss: 0.722732  [   32/  179]
train() client id: f_00007-2-1 loss: 0.767028  [   64/  179]
train() client id: f_00007-2-2 loss: 0.760393  [   96/  179]
train() client id: f_00007-2-3 loss: 0.590569  [  128/  179]
train() client id: f_00007-2-4 loss: 0.775011  [  160/  179]
train() client id: f_00007-3-0 loss: 0.708186  [   32/  179]
train() client id: f_00007-3-1 loss: 0.711557  [   64/  179]
train() client id: f_00007-3-2 loss: 0.872390  [   96/  179]
train() client id: f_00007-3-3 loss: 0.525645  [  128/  179]
train() client id: f_00007-3-4 loss: 0.653161  [  160/  179]
train() client id: f_00007-4-0 loss: 0.806635  [   32/  179]
train() client id: f_00007-4-1 loss: 0.722412  [   64/  179]
train() client id: f_00007-4-2 loss: 0.664624  [   96/  179]
train() client id: f_00007-4-3 loss: 0.703821  [  128/  179]
train() client id: f_00007-4-4 loss: 0.671198  [  160/  179]
train() client id: f_00007-5-0 loss: 0.739743  [   32/  179]
train() client id: f_00007-5-1 loss: 0.565279  [   64/  179]
train() client id: f_00007-5-2 loss: 0.920664  [   96/  179]
train() client id: f_00007-5-3 loss: 0.621381  [  128/  179]
train() client id: f_00007-5-4 loss: 0.603336  [  160/  179]
train() client id: f_00007-6-0 loss: 0.650223  [   32/  179]
train() client id: f_00007-6-1 loss: 0.806471  [   64/  179]
train() client id: f_00007-6-2 loss: 0.718132  [   96/  179]
train() client id: f_00007-6-3 loss: 0.699312  [  128/  179]
train() client id: f_00007-6-4 loss: 0.638183  [  160/  179]
train() client id: f_00007-7-0 loss: 0.619261  [   32/  179]
train() client id: f_00007-7-1 loss: 0.765435  [   64/  179]
train() client id: f_00007-7-2 loss: 0.694970  [   96/  179]
train() client id: f_00007-7-3 loss: 0.786471  [  128/  179]
train() client id: f_00007-7-4 loss: 0.611789  [  160/  179]
train() client id: f_00007-8-0 loss: 0.719438  [   32/  179]
train() client id: f_00007-8-1 loss: 0.527897  [   64/  179]
train() client id: f_00007-8-2 loss: 0.704571  [   96/  179]
train() client id: f_00007-8-3 loss: 0.795359  [  128/  179]
train() client id: f_00007-8-4 loss: 0.549089  [  160/  179]
train() client id: f_00007-9-0 loss: 0.669761  [   32/  179]
train() client id: f_00007-9-1 loss: 0.759807  [   64/  179]
train() client id: f_00007-9-2 loss: 0.718006  [   96/  179]
train() client id: f_00007-9-3 loss: 0.641044  [  128/  179]
train() client id: f_00007-9-4 loss: 0.616815  [  160/  179]
train() client id: f_00007-10-0 loss: 0.615407  [   32/  179]
train() client id: f_00007-10-1 loss: 0.628706  [   64/  179]
train() client id: f_00007-10-2 loss: 0.538715  [   96/  179]
train() client id: f_00007-10-3 loss: 0.810268  [  128/  179]
train() client id: f_00007-10-4 loss: 0.854055  [  160/  179]
train() client id: f_00007-11-0 loss: 0.763796  [   32/  179]
train() client id: f_00007-11-1 loss: 0.674841  [   64/  179]
train() client id: f_00007-11-2 loss: 0.651138  [   96/  179]
train() client id: f_00007-11-3 loss: 0.755624  [  128/  179]
train() client id: f_00007-11-4 loss: 0.538459  [  160/  179]
train() client id: f_00007-12-0 loss: 0.597479  [   32/  179]
train() client id: f_00007-12-1 loss: 0.728167  [   64/  179]
train() client id: f_00007-12-2 loss: 0.725130  [   96/  179]
train() client id: f_00007-12-3 loss: 0.710794  [  128/  179]
train() client id: f_00007-12-4 loss: 0.635018  [  160/  179]
train() client id: f_00007-13-0 loss: 0.585235  [   32/  179]
train() client id: f_00007-13-1 loss: 0.757794  [   64/  179]
train() client id: f_00007-13-2 loss: 0.713333  [   96/  179]
train() client id: f_00007-13-3 loss: 0.725223  [  128/  179]
train() client id: f_00007-13-4 loss: 0.525017  [  160/  179]
train() client id: f_00008-0-0 loss: 0.702074  [   32/  130]
train() client id: f_00008-0-1 loss: 0.706444  [   64/  130]
train() client id: f_00008-0-2 loss: 0.600155  [   96/  130]
train() client id: f_00008-0-3 loss: 0.620134  [  128/  130]
train() client id: f_00008-1-0 loss: 0.662796  [   32/  130]
train() client id: f_00008-1-1 loss: 0.624854  [   64/  130]
train() client id: f_00008-1-2 loss: 0.691179  [   96/  130]
train() client id: f_00008-1-3 loss: 0.637289  [  128/  130]
train() client id: f_00008-2-0 loss: 0.692965  [   32/  130]
train() client id: f_00008-2-1 loss: 0.645038  [   64/  130]
train() client id: f_00008-2-2 loss: 0.588887  [   96/  130]
train() client id: f_00008-2-3 loss: 0.676624  [  128/  130]
train() client id: f_00008-3-0 loss: 0.702215  [   32/  130]
train() client id: f_00008-3-1 loss: 0.718571  [   64/  130]
train() client id: f_00008-3-2 loss: 0.627886  [   96/  130]
train() client id: f_00008-3-3 loss: 0.579258  [  128/  130]
train() client id: f_00008-4-0 loss: 0.649575  [   32/  130]
train() client id: f_00008-4-1 loss: 0.658489  [   64/  130]
train() client id: f_00008-4-2 loss: 0.615553  [   96/  130]
train() client id: f_00008-4-3 loss: 0.687674  [  128/  130]
train() client id: f_00008-5-0 loss: 0.602503  [   32/  130]
train() client id: f_00008-5-1 loss: 0.820108  [   64/  130]
train() client id: f_00008-5-2 loss: 0.576499  [   96/  130]
train() client id: f_00008-5-3 loss: 0.640310  [  128/  130]
train() client id: f_00008-6-0 loss: 0.695176  [   32/  130]
train() client id: f_00008-6-1 loss: 0.579088  [   64/  130]
train() client id: f_00008-6-2 loss: 0.635674  [   96/  130]
train() client id: f_00008-6-3 loss: 0.688328  [  128/  130]
train() client id: f_00008-7-0 loss: 0.646784  [   32/  130]
train() client id: f_00008-7-1 loss: 0.697221  [   64/  130]
train() client id: f_00008-7-2 loss: 0.635143  [   96/  130]
train() client id: f_00008-7-3 loss: 0.654618  [  128/  130]
train() client id: f_00008-8-0 loss: 0.690429  [   32/  130]
train() client id: f_00008-8-1 loss: 0.550650  [   64/  130]
train() client id: f_00008-8-2 loss: 0.735458  [   96/  130]
train() client id: f_00008-8-3 loss: 0.642011  [  128/  130]
train() client id: f_00008-9-0 loss: 0.599980  [   32/  130]
train() client id: f_00008-9-1 loss: 0.511684  [   64/  130]
train() client id: f_00008-9-2 loss: 0.797777  [   96/  130]
train() client id: f_00008-9-3 loss: 0.707769  [  128/  130]
train() client id: f_00008-10-0 loss: 0.707246  [   32/  130]
train() client id: f_00008-10-1 loss: 0.722672  [   64/  130]
train() client id: f_00008-10-2 loss: 0.630437  [   96/  130]
train() client id: f_00008-10-3 loss: 0.574960  [  128/  130]
train() client id: f_00008-11-0 loss: 0.662235  [   32/  130]
train() client id: f_00008-11-1 loss: 0.769936  [   64/  130]
train() client id: f_00008-11-2 loss: 0.626697  [   96/  130]
train() client id: f_00008-11-3 loss: 0.575993  [  128/  130]
train() client id: f_00008-12-0 loss: 0.580600  [   32/  130]
train() client id: f_00008-12-1 loss: 0.671183  [   64/  130]
train() client id: f_00008-12-2 loss: 0.764339  [   96/  130]
train() client id: f_00008-12-3 loss: 0.588507  [  128/  130]
train() client id: f_00008-13-0 loss: 0.729153  [   32/  130]
train() client id: f_00008-13-1 loss: 0.676820  [   64/  130]
train() client id: f_00008-13-2 loss: 0.589353  [   96/  130]
train() client id: f_00008-13-3 loss: 0.640307  [  128/  130]
train() client id: f_00009-0-0 loss: 1.005588  [   32/  118]
train() client id: f_00009-0-1 loss: 1.283738  [   64/  118]
train() client id: f_00009-0-2 loss: 0.938119  [   96/  118]
train() client id: f_00009-1-0 loss: 1.035653  [   32/  118]
train() client id: f_00009-1-1 loss: 1.106118  [   64/  118]
train() client id: f_00009-1-2 loss: 1.066195  [   96/  118]
train() client id: f_00009-2-0 loss: 1.048087  [   32/  118]
train() client id: f_00009-2-1 loss: 1.148771  [   64/  118]
train() client id: f_00009-2-2 loss: 0.881634  [   96/  118]
train() client id: f_00009-3-0 loss: 1.013021  [   32/  118]
train() client id: f_00009-3-1 loss: 1.016140  [   64/  118]
train() client id: f_00009-3-2 loss: 0.899128  [   96/  118]
train() client id: f_00009-4-0 loss: 1.101435  [   32/  118]
train() client id: f_00009-4-1 loss: 0.796868  [   64/  118]
train() client id: f_00009-4-2 loss: 0.898156  [   96/  118]
train() client id: f_00009-5-0 loss: 0.941256  [   32/  118]
train() client id: f_00009-5-1 loss: 0.854772  [   64/  118]
train() client id: f_00009-5-2 loss: 0.877179  [   96/  118]
train() client id: f_00009-6-0 loss: 0.984882  [   32/  118]
train() client id: f_00009-6-1 loss: 0.909476  [   64/  118]
train() client id: f_00009-6-2 loss: 0.847374  [   96/  118]
train() client id: f_00009-7-0 loss: 0.949643  [   32/  118]
train() client id: f_00009-7-1 loss: 0.777715  [   64/  118]
train() client id: f_00009-7-2 loss: 0.833883  [   96/  118]
train() client id: f_00009-8-0 loss: 0.858101  [   32/  118]
train() client id: f_00009-8-1 loss: 0.821614  [   64/  118]
train() client id: f_00009-8-2 loss: 0.870429  [   96/  118]
train() client id: f_00009-9-0 loss: 0.903252  [   32/  118]
train() client id: f_00009-9-1 loss: 0.652487  [   64/  118]
train() client id: f_00009-9-2 loss: 0.786558  [   96/  118]
train() client id: f_00009-10-0 loss: 0.828272  [   32/  118]
train() client id: f_00009-10-1 loss: 0.854296  [   64/  118]
train() client id: f_00009-10-2 loss: 0.815427  [   96/  118]
train() client id: f_00009-11-0 loss: 0.934947  [   32/  118]
train() client id: f_00009-11-1 loss: 0.712160  [   64/  118]
train() client id: f_00009-11-2 loss: 0.814751  [   96/  118]
train() client id: f_00009-12-0 loss: 0.761484  [   32/  118]
train() client id: f_00009-12-1 loss: 0.800367  [   64/  118]
train() client id: f_00009-12-2 loss: 0.842346  [   96/  118]
train() client id: f_00009-13-0 loss: 0.737236  [   32/  118]
train() client id: f_00009-13-1 loss: 0.743137  [   64/  118]
train() client id: f_00009-13-2 loss: 0.933842  [   96/  118]
At round 22 accuracy: 0.636604774535809
At round 22 training accuracy: 0.5788061703554661
At round 22 training loss: 0.8485096853114822
update_location
xs = 8.927491 231.223621 5.882650 10.934260 -147.581990 4.769243 -5.849135 -5.143845 -170.120581 20.134486 
ys = -222.390647 7.291448 120.684448 -32.290817 -9.642386 0.794442 -41.381692 116.628436 25.881276 -657.232496 
xs mean: -4.682379970521214
ys mean: -69.16579882624053
dists_uav = 244.002664 252.026840 156.841773 105.651573 178.531284 100.116816 108.381995 153.716139 199.024754 665.101459 
uav_gains = -110.795146 -111.469352 -104.894704 -100.596934 -106.334407 -100.012692 -100.873979 -104.674265 -107.620618 -127.516206 
uav_gains_db_mean: -107.47883032303781
dists_bs = 437.891138 439.481272 188.861622 278.461904 186.666971 250.328898 274.650782 179.606105 149.198534 854.803132 
bs_gains = -113.525312 -113.569390 -103.299083 -108.020501 -103.156949 -106.725367 -107.852923 -102.688050 -100.432467 -121.659310 
bs_gains_db_mean: -108.09293500858566
Round 23
-------------------------------
ene_coms = [0.01129648 0.01186445 0.00730006 0.00646249 0.00725073 0.00630574
 0.00653954 0.00709253 0.00641759 0.0349806 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.02531015 16.63469238  7.81963865  2.79227068 19.0760912   9.16003839
  3.47424715 11.21736333  8.17266517  8.01621754]
obj_prev = 94.3885346329133
eta_min = 2.0920832767108668e-12	eta_max = 0.7497100701184737
af = 19.826056636829264	bf = 2.1422843569467167	zeta = 21.808662300512193	eta = 0.909090909090909
af = 19.826056636829264	bf = 2.1422843569467167	zeta = 44.351003177950005	eta = 0.4470261147708647
af = 19.826056636829264	bf = 2.1422843569467167	zeta = 32.74115045668956	eta = 0.6055394010377076
af = 19.826056636829264	bf = 2.1422843569467167	zeta = 30.643722393075763	eta = 0.6469859106055976
af = 19.826056636829264	bf = 2.1422843569467167	zeta = 30.5233122811913	eta = 0.6495381777110157
af = 19.826056636829264	bf = 2.1422843569467167	zeta = 30.52287440831353	eta = 0.6495474958095437
af = 19.826056636829264	bf = 2.1422843569467167	zeta = 30.522874402492306	eta = 0.6495474959334234
eta = 0.6495474959334234
ene_coms = [0.01129648 0.01186445 0.00730006 0.00646249 0.00725073 0.00630574
 0.00653954 0.00709253 0.00641759 0.0349806 ]
ene_comp = [0.03560464 0.07488277 0.03503949 0.01215079 0.08646841 0.04125617
 0.01525913 0.05058119 0.03673495 0.03334403]
ene_total = [2.71729418 5.02584435 2.45301199 1.07839131 5.42977413 2.75557826
 1.26294238 3.34142299 2.5001139  3.9585009 ]
ti_comp = [0.38462132 0.37894164 0.4245855  0.4329612  0.4250788  0.4345287
 0.43219068 0.42666081 0.43341019 0.14778009]
ti_coms = [0.11296478 0.11864446 0.0730006  0.0646249  0.0725073  0.0630574
 0.06539542 0.07092529 0.06417591 0.34980601]
t_total = [28.82847786 28.82847786 28.82847786 28.82847786 28.82847786 28.82847786
 28.82847786 28.82847786 28.82847786 28.82847786]
ene_coms = [0.01129648 0.01186445 0.00730006 0.00646249 0.00725073 0.00630574
 0.00653954 0.00709253 0.00641759 0.0349806 ]
ene_comp = [1.90692264e-05 1.82759987e-04 1.49149872e-05 5.98129969e-07
 2.23621490e-04 2.32439418e-05 1.18882695e-06 4.44305544e-05
 1.64937712e-05 1.06096805e-04]
ene_total = [0.65558504 0.69797493 0.42380523 0.37444977 0.43303898 0.3666802
 0.37894813 0.41349161 0.37276938 2.03280615]
optimize_network iter = 0 obj = 6.149549430847754
eta = 0.6495474959334234
freqs = [4.62853166e+07 9.88051532e+07 4.12631686e+07 1.40321923e+07
 1.01708682e+08 4.74723177e+07 1.76532395e+07 5.92756499e+07
 4.23789586e+07 1.12816371e+08]
eta_min = 0.649547495933427	eta_max = 0.6495474959334195
af = 0.029759572291746438	bf = 2.1422843569467167	zeta = 0.03273552952092108	eta = 0.9090909090909091
af = 0.029759572291746438	bf = 2.1422843569467167	zeta = 23.59632821846016	eta = 0.0012611950476457848
af = 0.029759572291746438	bf = 2.1422843569467167	zeta = 2.343906324521617	eta = 0.012696570669401755
af = 0.029759572291746438	bf = 2.1422843569467167	zeta = 2.3014477421281305	eta = 0.012930805139302445
af = 0.029759572291746438	bf = 2.1422843569467167	zeta = 2.3014427350512676	eta = 0.012930833271888256
eta = 0.012930833271888256
ene_coms = [0.01129648 0.01186445 0.00730006 0.00646249 0.00725073 0.00630574
 0.00653954 0.00709253 0.00641759 0.0349806 ]
ene_comp = [1.92166054e-04 1.84172472e-03 1.50302597e-04 6.02752696e-06
 2.25349778e-03 2.34235858e-04 1.19801495e-05 4.47739419e-04
 1.66212455e-04 1.06916789e-03]
ene_total = [0.23632182 0.28193643 0.15325423 0.13305764 0.19550231 0.13452754
 0.13476505 0.15510359 0.13542907 0.74154504]
ti_comp = [0.38462132 0.37894164 0.4245855  0.4329612  0.4250788  0.4345287
 0.43219068 0.42666081 0.43341019 0.14778009]
ti_coms = [0.11296478 0.11864446 0.0730006  0.0646249  0.0725073  0.0630574
 0.06539542 0.07092529 0.06417591 0.34980601]
t_total = [28.82847786 28.82847786 28.82847786 28.82847786 28.82847786 28.82847786
 28.82847786 28.82847786 28.82847786 28.82847786]
ene_coms = [0.01129648 0.01186445 0.00730006 0.00646249 0.00725073 0.00630574
 0.00653954 0.00709253 0.00641759 0.0349806 ]
ene_comp = [1.90692264e-05 1.82759987e-04 1.49149872e-05 5.98129969e-07
 2.23621490e-04 2.32439418e-05 1.18882695e-06 4.44305544e-05
 1.64937712e-05 1.06096805e-04]
ene_total = [0.65558504 0.69797493 0.42380523 0.37444977 0.43303898 0.3666802
 0.37894813 0.41349161 0.37276938 2.03280615]
optimize_network iter = 1 obj = 6.1495494308478165
eta = 0.649547495933427
freqs = [4.62853166e+07 9.88051532e+07 4.12631686e+07 1.40321923e+07
 1.01708682e+08 4.74723177e+07 1.76532395e+07 5.92756499e+07
 4.23789586e+07 1.12816371e+08]
Done!
ene_coms = [0.01129648 0.01186445 0.00730006 0.00646249 0.00725073 0.00630574
 0.00653954 0.00709253 0.00641759 0.0349806 ]
ene_comp = [1.88953553e-05 1.81093601e-04 1.47789940e-05 5.92676285e-07
 2.21582533e-04 2.30320061e-05 1.17798735e-06 4.40254415e-05
 1.63433828e-05 1.05129426e-04]
ene_total = [0.01131537 0.01204554 0.00731484 0.00646308 0.00747231 0.00632877
 0.00654072 0.00713655 0.00643393 0.03508573]
At round 23 energy consumption: 0.10613685889033539
At round 23 eta: 0.649547495933427
At round 23 a_n: 20.30404837596008
At round 23 local rounds: 14.12882506967381
At round 23 global rounds: 57.93666228763217
gradient difference: 0.3650067448616028
train() client id: f_00000-0-0 loss: 1.155592  [   32/  126]
train() client id: f_00000-0-1 loss: 1.274469  [   64/  126]
train() client id: f_00000-0-2 loss: 1.288383  [   96/  126]
train() client id: f_00000-1-0 loss: 1.101652  [   32/  126]
train() client id: f_00000-1-1 loss: 0.937238  [   64/  126]
train() client id: f_00000-1-2 loss: 1.248148  [   96/  126]
train() client id: f_00000-2-0 loss: 0.968712  [   32/  126]
train() client id: f_00000-2-1 loss: 1.066952  [   64/  126]
train() client id: f_00000-2-2 loss: 0.964444  [   96/  126]
train() client id: f_00000-3-0 loss: 0.980996  [   32/  126]
train() client id: f_00000-3-1 loss: 1.046561  [   64/  126]
train() client id: f_00000-3-2 loss: 0.854196  [   96/  126]
train() client id: f_00000-4-0 loss: 0.990931  [   32/  126]
train() client id: f_00000-4-1 loss: 0.827970  [   64/  126]
train() client id: f_00000-4-2 loss: 0.845615  [   96/  126]
train() client id: f_00000-5-0 loss: 0.889637  [   32/  126]
train() client id: f_00000-5-1 loss: 0.897084  [   64/  126]
train() client id: f_00000-5-2 loss: 0.829521  [   96/  126]
train() client id: f_00000-6-0 loss: 0.802132  [   32/  126]
train() client id: f_00000-6-1 loss: 0.776562  [   64/  126]
train() client id: f_00000-6-2 loss: 0.835068  [   96/  126]
train() client id: f_00000-7-0 loss: 0.763332  [   32/  126]
train() client id: f_00000-7-1 loss: 0.815393  [   64/  126]
train() client id: f_00000-7-2 loss: 0.828693  [   96/  126]
train() client id: f_00000-8-0 loss: 0.789568  [   32/  126]
train() client id: f_00000-8-1 loss: 0.742557  [   64/  126]
train() client id: f_00000-8-2 loss: 0.861574  [   96/  126]
train() client id: f_00000-9-0 loss: 0.687004  [   32/  126]
train() client id: f_00000-9-1 loss: 0.819410  [   64/  126]
train() client id: f_00000-9-2 loss: 0.858460  [   96/  126]
train() client id: f_00000-10-0 loss: 0.860837  [   32/  126]
train() client id: f_00000-10-1 loss: 0.809363  [   64/  126]
train() client id: f_00000-10-2 loss: 0.700807  [   96/  126]
train() client id: f_00000-11-0 loss: 0.763613  [   32/  126]
train() client id: f_00000-11-1 loss: 0.802181  [   64/  126]
train() client id: f_00000-11-2 loss: 0.747837  [   96/  126]
train() client id: f_00000-12-0 loss: 0.735691  [   32/  126]
train() client id: f_00000-12-1 loss: 0.760298  [   64/  126]
train() client id: f_00000-12-2 loss: 0.816903  [   96/  126]
train() client id: f_00000-13-0 loss: 0.822288  [   32/  126]
train() client id: f_00000-13-1 loss: 0.710046  [   64/  126]
train() client id: f_00000-13-2 loss: 0.742438  [   96/  126]
train() client id: f_00001-0-0 loss: 0.427146  [   32/  265]
train() client id: f_00001-0-1 loss: 0.515703  [   64/  265]
train() client id: f_00001-0-2 loss: 0.454267  [   96/  265]
train() client id: f_00001-0-3 loss: 0.478174  [  128/  265]
train() client id: f_00001-0-4 loss: 0.401324  [  160/  265]
train() client id: f_00001-0-5 loss: 0.550269  [  192/  265]
train() client id: f_00001-0-6 loss: 0.476100  [  224/  265]
train() client id: f_00001-0-7 loss: 0.344703  [  256/  265]
train() client id: f_00001-1-0 loss: 0.426285  [   32/  265]
train() client id: f_00001-1-1 loss: 0.375228  [   64/  265]
train() client id: f_00001-1-2 loss: 0.399137  [   96/  265]
train() client id: f_00001-1-3 loss: 0.402769  [  128/  265]
train() client id: f_00001-1-4 loss: 0.478399  [  160/  265]
train() client id: f_00001-1-5 loss: 0.514564  [  192/  265]
train() client id: f_00001-1-6 loss: 0.447796  [  224/  265]
train() client id: f_00001-1-7 loss: 0.521255  [  256/  265]
train() client id: f_00001-2-0 loss: 0.405491  [   32/  265]
train() client id: f_00001-2-1 loss: 0.344430  [   64/  265]
train() client id: f_00001-2-2 loss: 0.413534  [   96/  265]
train() client id: f_00001-2-3 loss: 0.437382  [  128/  265]
train() client id: f_00001-2-4 loss: 0.504159  [  160/  265]
train() client id: f_00001-2-5 loss: 0.528865  [  192/  265]
train() client id: f_00001-2-6 loss: 0.387737  [  224/  265]
train() client id: f_00001-2-7 loss: 0.412334  [  256/  265]
train() client id: f_00001-3-0 loss: 0.416609  [   32/  265]
train() client id: f_00001-3-1 loss: 0.412745  [   64/  265]
train() client id: f_00001-3-2 loss: 0.387753  [   96/  265]
train() client id: f_00001-3-3 loss: 0.371519  [  128/  265]
train() client id: f_00001-3-4 loss: 0.409074  [  160/  265]
train() client id: f_00001-3-5 loss: 0.513873  [  192/  265]
train() client id: f_00001-3-6 loss: 0.388998  [  224/  265]
train() client id: f_00001-3-7 loss: 0.430108  [  256/  265]
train() client id: f_00001-4-0 loss: 0.362671  [   32/  265]
train() client id: f_00001-4-1 loss: 0.409631  [   64/  265]
train() client id: f_00001-4-2 loss: 0.370210  [   96/  265]
train() client id: f_00001-4-3 loss: 0.343848  [  128/  265]
train() client id: f_00001-4-4 loss: 0.577555  [  160/  265]
train() client id: f_00001-4-5 loss: 0.362651  [  192/  265]
train() client id: f_00001-4-6 loss: 0.415738  [  224/  265]
train() client id: f_00001-4-7 loss: 0.503823  [  256/  265]
train() client id: f_00001-5-0 loss: 0.412603  [   32/  265]
train() client id: f_00001-5-1 loss: 0.369592  [   64/  265]
train() client id: f_00001-5-2 loss: 0.407884  [   96/  265]
train() client id: f_00001-5-3 loss: 0.317697  [  128/  265]
train() client id: f_00001-5-4 loss: 0.382636  [  160/  265]
train() client id: f_00001-5-5 loss: 0.468815  [  192/  265]
train() client id: f_00001-5-6 loss: 0.458159  [  224/  265]
train() client id: f_00001-5-7 loss: 0.485718  [  256/  265]
train() client id: f_00001-6-0 loss: 0.357158  [   32/  265]
train() client id: f_00001-6-1 loss: 0.457633  [   64/  265]
train() client id: f_00001-6-2 loss: 0.305704  [   96/  265]
train() client id: f_00001-6-3 loss: 0.353814  [  128/  265]
train() client id: f_00001-6-4 loss: 0.545115  [  160/  265]
train() client id: f_00001-6-5 loss: 0.564246  [  192/  265]
train() client id: f_00001-6-6 loss: 0.382330  [  224/  265]
train() client id: f_00001-6-7 loss: 0.328169  [  256/  265]
train() client id: f_00001-7-0 loss: 0.377757  [   32/  265]
train() client id: f_00001-7-1 loss: 0.332913  [   64/  265]
train() client id: f_00001-7-2 loss: 0.525783  [   96/  265]
train() client id: f_00001-7-3 loss: 0.400712  [  128/  265]
train() client id: f_00001-7-4 loss: 0.372136  [  160/  265]
train() client id: f_00001-7-5 loss: 0.457421  [  192/  265]
train() client id: f_00001-7-6 loss: 0.395395  [  224/  265]
train() client id: f_00001-7-7 loss: 0.436429  [  256/  265]
train() client id: f_00001-8-0 loss: 0.323089  [   32/  265]
train() client id: f_00001-8-1 loss: 0.458547  [   64/  265]
train() client id: f_00001-8-2 loss: 0.400519  [   96/  265]
train() client id: f_00001-8-3 loss: 0.414716  [  128/  265]
train() client id: f_00001-8-4 loss: 0.413355  [  160/  265]
train() client id: f_00001-8-5 loss: 0.465114  [  192/  265]
train() client id: f_00001-8-6 loss: 0.333711  [  224/  265]
train() client id: f_00001-8-7 loss: 0.482885  [  256/  265]
train() client id: f_00001-9-0 loss: 0.499984  [   32/  265]
train() client id: f_00001-9-1 loss: 0.524326  [   64/  265]
train() client id: f_00001-9-2 loss: 0.350947  [   96/  265]
train() client id: f_00001-9-3 loss: 0.329983  [  128/  265]
train() client id: f_00001-9-4 loss: 0.377622  [  160/  265]
train() client id: f_00001-9-5 loss: 0.463124  [  192/  265]
train() client id: f_00001-9-6 loss: 0.297105  [  224/  265]
train() client id: f_00001-9-7 loss: 0.367299  [  256/  265]
train() client id: f_00001-10-0 loss: 0.369607  [   32/  265]
train() client id: f_00001-10-1 loss: 0.413950  [   64/  265]
train() client id: f_00001-10-2 loss: 0.365411  [   96/  265]
train() client id: f_00001-10-3 loss: 0.424547  [  128/  265]
train() client id: f_00001-10-4 loss: 0.352148  [  160/  265]
train() client id: f_00001-10-5 loss: 0.386808  [  192/  265]
train() client id: f_00001-10-6 loss: 0.506183  [  224/  265]
train() client id: f_00001-10-7 loss: 0.428838  [  256/  265]
train() client id: f_00001-11-0 loss: 0.416789  [   32/  265]
train() client id: f_00001-11-1 loss: 0.328734  [   64/  265]
train() client id: f_00001-11-2 loss: 0.311002  [   96/  265]
train() client id: f_00001-11-3 loss: 0.477060  [  128/  265]
train() client id: f_00001-11-4 loss: 0.403359  [  160/  265]
train() client id: f_00001-11-5 loss: 0.387744  [  192/  265]
train() client id: f_00001-11-6 loss: 0.468233  [  224/  265]
train() client id: f_00001-11-7 loss: 0.457639  [  256/  265]
train() client id: f_00001-12-0 loss: 0.349102  [   32/  265]
train() client id: f_00001-12-1 loss: 0.302866  [   64/  265]
train() client id: f_00001-12-2 loss: 0.389685  [   96/  265]
train() client id: f_00001-12-3 loss: 0.374520  [  128/  265]
train() client id: f_00001-12-4 loss: 0.508789  [  160/  265]
train() client id: f_00001-12-5 loss: 0.408544  [  192/  265]
train() client id: f_00001-12-6 loss: 0.419386  [  224/  265]
train() client id: f_00001-12-7 loss: 0.485640  [  256/  265]
train() client id: f_00001-13-0 loss: 0.315938  [   32/  265]
train() client id: f_00001-13-1 loss: 0.414992  [   64/  265]
train() client id: f_00001-13-2 loss: 0.304361  [   96/  265]
train() client id: f_00001-13-3 loss: 0.579881  [  128/  265]
train() client id: f_00001-13-4 loss: 0.387929  [  160/  265]
train() client id: f_00001-13-5 loss: 0.570730  [  192/  265]
train() client id: f_00001-13-6 loss: 0.300035  [  224/  265]
train() client id: f_00001-13-7 loss: 0.357568  [  256/  265]
train() client id: f_00002-0-0 loss: 1.059397  [   32/  124]
train() client id: f_00002-0-1 loss: 1.236086  [   64/  124]
train() client id: f_00002-0-2 loss: 1.296196  [   96/  124]
train() client id: f_00002-1-0 loss: 1.213060  [   32/  124]
train() client id: f_00002-1-1 loss: 1.184956  [   64/  124]
train() client id: f_00002-1-2 loss: 1.147857  [   96/  124]
train() client id: f_00002-2-0 loss: 1.257488  [   32/  124]
train() client id: f_00002-2-1 loss: 0.985321  [   64/  124]
train() client id: f_00002-2-2 loss: 1.147568  [   96/  124]
train() client id: f_00002-3-0 loss: 0.998049  [   32/  124]
train() client id: f_00002-3-1 loss: 1.150927  [   64/  124]
train() client id: f_00002-3-2 loss: 1.181442  [   96/  124]
train() client id: f_00002-4-0 loss: 1.255367  [   32/  124]
train() client id: f_00002-4-1 loss: 1.211446  [   64/  124]
train() client id: f_00002-4-2 loss: 0.919372  [   96/  124]
train() client id: f_00002-5-0 loss: 1.271352  [   32/  124]
train() client id: f_00002-5-1 loss: 0.943564  [   64/  124]
train() client id: f_00002-5-2 loss: 0.974087  [   96/  124]
train() client id: f_00002-6-0 loss: 1.125761  [   32/  124]
train() client id: f_00002-6-1 loss: 0.980313  [   64/  124]
train() client id: f_00002-6-2 loss: 0.997046  [   96/  124]
train() client id: f_00002-7-0 loss: 1.050776  [   32/  124]
train() client id: f_00002-7-1 loss: 1.027641  [   64/  124]
train() client id: f_00002-7-2 loss: 0.950182  [   96/  124]
train() client id: f_00002-8-0 loss: 1.059970  [   32/  124]
train() client id: f_00002-8-1 loss: 0.950735  [   64/  124]
train() client id: f_00002-8-2 loss: 1.081459  [   96/  124]
train() client id: f_00002-9-0 loss: 0.996452  [   32/  124]
train() client id: f_00002-9-1 loss: 0.989901  [   64/  124]
train() client id: f_00002-9-2 loss: 1.053510  [   96/  124]
train() client id: f_00002-10-0 loss: 1.140714  [   32/  124]
train() client id: f_00002-10-1 loss: 1.046415  [   64/  124]
train() client id: f_00002-10-2 loss: 0.834180  [   96/  124]
train() client id: f_00002-11-0 loss: 0.921801  [   32/  124]
train() client id: f_00002-11-1 loss: 1.045290  [   64/  124]
train() client id: f_00002-11-2 loss: 1.077507  [   96/  124]
train() client id: f_00002-12-0 loss: 0.882268  [   32/  124]
train() client id: f_00002-12-1 loss: 1.028034  [   64/  124]
train() client id: f_00002-12-2 loss: 1.076287  [   96/  124]
train() client id: f_00002-13-0 loss: 1.104908  [   32/  124]
train() client id: f_00002-13-1 loss: 0.907976  [   64/  124]
train() client id: f_00002-13-2 loss: 0.987643  [   96/  124]
train() client id: f_00003-0-0 loss: 0.639091  [   32/   43]
train() client id: f_00003-1-0 loss: 0.786342  [   32/   43]
train() client id: f_00003-2-0 loss: 0.784930  [   32/   43]
train() client id: f_00003-3-0 loss: 0.694822  [   32/   43]
train() client id: f_00003-4-0 loss: 0.652552  [   32/   43]
train() client id: f_00003-5-0 loss: 0.780632  [   32/   43]
train() client id: f_00003-6-0 loss: 0.598787  [   32/   43]
train() client id: f_00003-7-0 loss: 0.838385  [   32/   43]
train() client id: f_00003-8-0 loss: 0.779556  [   32/   43]
train() client id: f_00003-9-0 loss: 0.812199  [   32/   43]
train() client id: f_00003-10-0 loss: 0.626536  [   32/   43]
train() client id: f_00003-11-0 loss: 0.754652  [   32/   43]
train() client id: f_00003-12-0 loss: 0.826743  [   32/   43]
train() client id: f_00003-13-0 loss: 0.767666  [   32/   43]
train() client id: f_00004-0-0 loss: 1.008949  [   32/  306]
train() client id: f_00004-0-1 loss: 0.941380  [   64/  306]
train() client id: f_00004-0-2 loss: 0.914902  [   96/  306]
train() client id: f_00004-0-3 loss: 0.908030  [  128/  306]
train() client id: f_00004-0-4 loss: 0.953033  [  160/  306]
train() client id: f_00004-0-5 loss: 0.912579  [  192/  306]
train() client id: f_00004-0-6 loss: 0.946288  [  224/  306]
train() client id: f_00004-0-7 loss: 1.032759  [  256/  306]
train() client id: f_00004-0-8 loss: 0.900999  [  288/  306]
train() client id: f_00004-1-0 loss: 0.968684  [   32/  306]
train() client id: f_00004-1-1 loss: 0.870047  [   64/  306]
train() client id: f_00004-1-2 loss: 1.008463  [   96/  306]
train() client id: f_00004-1-3 loss: 0.954182  [  128/  306]
train() client id: f_00004-1-4 loss: 0.714579  [  160/  306]
train() client id: f_00004-1-5 loss: 0.946254  [  192/  306]
train() client id: f_00004-1-6 loss: 0.992186  [  224/  306]
train() client id: f_00004-1-7 loss: 1.165424  [  256/  306]
train() client id: f_00004-1-8 loss: 0.843381  [  288/  306]
train() client id: f_00004-2-0 loss: 0.928786  [   32/  306]
train() client id: f_00004-2-1 loss: 1.095762  [   64/  306]
train() client id: f_00004-2-2 loss: 0.923788  [   96/  306]
train() client id: f_00004-2-3 loss: 0.852922  [  128/  306]
train() client id: f_00004-2-4 loss: 0.919894  [  160/  306]
train() client id: f_00004-2-5 loss: 1.011499  [  192/  306]
train() client id: f_00004-2-6 loss: 1.036833  [  224/  306]
train() client id: f_00004-2-7 loss: 0.937988  [  256/  306]
train() client id: f_00004-2-8 loss: 0.742341  [  288/  306]
train() client id: f_00004-3-0 loss: 0.980749  [   32/  306]
train() client id: f_00004-3-1 loss: 0.927795  [   64/  306]
train() client id: f_00004-3-2 loss: 0.882653  [   96/  306]
train() client id: f_00004-3-3 loss: 1.182496  [  128/  306]
train() client id: f_00004-3-4 loss: 0.882731  [  160/  306]
train() client id: f_00004-3-5 loss: 0.857392  [  192/  306]
train() client id: f_00004-3-6 loss: 0.893623  [  224/  306]
train() client id: f_00004-3-7 loss: 0.888627  [  256/  306]
train() client id: f_00004-3-8 loss: 0.807476  [  288/  306]
train() client id: f_00004-4-0 loss: 0.954084  [   32/  306]
train() client id: f_00004-4-1 loss: 0.945142  [   64/  306]
train() client id: f_00004-4-2 loss: 0.856408  [   96/  306]
train() client id: f_00004-4-3 loss: 0.977967  [  128/  306]
train() client id: f_00004-4-4 loss: 0.984400  [  160/  306]
train() client id: f_00004-4-5 loss: 0.933528  [  192/  306]
train() client id: f_00004-4-6 loss: 0.977829  [  224/  306]
train() client id: f_00004-4-7 loss: 0.827267  [  256/  306]
train() client id: f_00004-4-8 loss: 1.031069  [  288/  306]
train() client id: f_00004-5-0 loss: 0.854822  [   32/  306]
train() client id: f_00004-5-1 loss: 0.833589  [   64/  306]
train() client id: f_00004-5-2 loss: 0.906498  [   96/  306]
train() client id: f_00004-5-3 loss: 1.019337  [  128/  306]
train() client id: f_00004-5-4 loss: 0.902114  [  160/  306]
train() client id: f_00004-5-5 loss: 0.959770  [  192/  306]
train() client id: f_00004-5-6 loss: 0.929900  [  224/  306]
train() client id: f_00004-5-7 loss: 0.886911  [  256/  306]
train() client id: f_00004-5-8 loss: 1.085631  [  288/  306]
train() client id: f_00004-6-0 loss: 0.924051  [   32/  306]
train() client id: f_00004-6-1 loss: 0.835800  [   64/  306]
train() client id: f_00004-6-2 loss: 1.075685  [   96/  306]
train() client id: f_00004-6-3 loss: 0.872806  [  128/  306]
train() client id: f_00004-6-4 loss: 0.947564  [  160/  306]
train() client id: f_00004-6-5 loss: 0.962299  [  192/  306]
train() client id: f_00004-6-6 loss: 0.863488  [  224/  306]
train() client id: f_00004-6-7 loss: 0.930775  [  256/  306]
train() client id: f_00004-6-8 loss: 0.997250  [  288/  306]
train() client id: f_00004-7-0 loss: 0.805599  [   32/  306]
train() client id: f_00004-7-1 loss: 0.961428  [   64/  306]
train() client id: f_00004-7-2 loss: 1.036780  [   96/  306]
train() client id: f_00004-7-3 loss: 0.728279  [  128/  306]
train() client id: f_00004-7-4 loss: 0.947501  [  160/  306]
train() client id: f_00004-7-5 loss: 1.074407  [  192/  306]
train() client id: f_00004-7-6 loss: 0.913827  [  224/  306]
train() client id: f_00004-7-7 loss: 1.079776  [  256/  306]
train() client id: f_00004-7-8 loss: 0.919321  [  288/  306]
train() client id: f_00004-8-0 loss: 1.013220  [   32/  306]
train() client id: f_00004-8-1 loss: 1.012154  [   64/  306]
train() client id: f_00004-8-2 loss: 0.971336  [   96/  306]
train() client id: f_00004-8-3 loss: 0.895747  [  128/  306]
train() client id: f_00004-8-4 loss: 0.920410  [  160/  306]
train() client id: f_00004-8-5 loss: 0.920129  [  192/  306]
train() client id: f_00004-8-6 loss: 0.892363  [  224/  306]
train() client id: f_00004-8-7 loss: 0.837065  [  256/  306]
train() client id: f_00004-8-8 loss: 0.934021  [  288/  306]
train() client id: f_00004-9-0 loss: 0.974967  [   32/  306]
train() client id: f_00004-9-1 loss: 0.873733  [   64/  306]
train() client id: f_00004-9-2 loss: 0.985674  [   96/  306]
train() client id: f_00004-9-3 loss: 0.888990  [  128/  306]
train() client id: f_00004-9-4 loss: 0.765499  [  160/  306]
train() client id: f_00004-9-5 loss: 1.012934  [  192/  306]
train() client id: f_00004-9-6 loss: 1.022379  [  224/  306]
train() client id: f_00004-9-7 loss: 0.800687  [  256/  306]
train() client id: f_00004-9-8 loss: 0.904104  [  288/  306]
train() client id: f_00004-10-0 loss: 0.903889  [   32/  306]
train() client id: f_00004-10-1 loss: 1.023883  [   64/  306]
train() client id: f_00004-10-2 loss: 1.000803  [   96/  306]
train() client id: f_00004-10-3 loss: 0.798867  [  128/  306]
train() client id: f_00004-10-4 loss: 0.989363  [  160/  306]
train() client id: f_00004-10-5 loss: 0.867069  [  192/  306]
train() client id: f_00004-10-6 loss: 0.873817  [  224/  306]
train() client id: f_00004-10-7 loss: 0.958059  [  256/  306]
train() client id: f_00004-10-8 loss: 0.922190  [  288/  306]
train() client id: f_00004-11-0 loss: 1.012515  [   32/  306]
train() client id: f_00004-11-1 loss: 0.898943  [   64/  306]
train() client id: f_00004-11-2 loss: 1.058779  [   96/  306]
train() client id: f_00004-11-3 loss: 0.949312  [  128/  306]
train() client id: f_00004-11-4 loss: 0.825339  [  160/  306]
train() client id: f_00004-11-5 loss: 0.891609  [  192/  306]
train() client id: f_00004-11-6 loss: 0.850590  [  224/  306]
train() client id: f_00004-11-7 loss: 0.843385  [  256/  306]
train() client id: f_00004-11-8 loss: 0.971212  [  288/  306]
train() client id: f_00004-12-0 loss: 0.867311  [   32/  306]
train() client id: f_00004-12-1 loss: 0.932057  [   64/  306]
train() client id: f_00004-12-2 loss: 0.871901  [   96/  306]
train() client id: f_00004-12-3 loss: 0.930248  [  128/  306]
train() client id: f_00004-12-4 loss: 0.852791  [  160/  306]
train() client id: f_00004-12-5 loss: 0.977350  [  192/  306]
train() client id: f_00004-12-6 loss: 0.899532  [  224/  306]
train() client id: f_00004-12-7 loss: 0.906675  [  256/  306]
train() client id: f_00004-12-8 loss: 0.990219  [  288/  306]
train() client id: f_00004-13-0 loss: 0.871088  [   32/  306]
train() client id: f_00004-13-1 loss: 0.746995  [   64/  306]
train() client id: f_00004-13-2 loss: 0.925731  [   96/  306]
train() client id: f_00004-13-3 loss: 0.952019  [  128/  306]
train() client id: f_00004-13-4 loss: 0.860016  [  160/  306]
train() client id: f_00004-13-5 loss: 1.066638  [  192/  306]
train() client id: f_00004-13-6 loss: 0.967861  [  224/  306]
train() client id: f_00004-13-7 loss: 0.947667  [  256/  306]
train() client id: f_00004-13-8 loss: 0.943266  [  288/  306]
train() client id: f_00005-0-0 loss: 0.658603  [   32/  146]
train() client id: f_00005-0-1 loss: 0.421461  [   64/  146]
train() client id: f_00005-0-2 loss: 0.459117  [   96/  146]
train() client id: f_00005-0-3 loss: 0.338530  [  128/  146]
train() client id: f_00005-1-0 loss: 0.308921  [   32/  146]
train() client id: f_00005-1-1 loss: 0.379655  [   64/  146]
train() client id: f_00005-1-2 loss: 0.614654  [   96/  146]
train() client id: f_00005-1-3 loss: 0.525799  [  128/  146]
train() client id: f_00005-2-0 loss: 0.523308  [   32/  146]
train() client id: f_00005-2-1 loss: 0.266069  [   64/  146]
train() client id: f_00005-2-2 loss: 0.735201  [   96/  146]
train() client id: f_00005-2-3 loss: 0.375271  [  128/  146]
train() client id: f_00005-3-0 loss: 0.444307  [   32/  146]
train() client id: f_00005-3-1 loss: 0.330859  [   64/  146]
train() client id: f_00005-3-2 loss: 0.246959  [   96/  146]
train() client id: f_00005-3-3 loss: 0.585502  [  128/  146]
train() client id: f_00005-4-0 loss: 0.381920  [   32/  146]
train() client id: f_00005-4-1 loss: 0.304202  [   64/  146]
train() client id: f_00005-4-2 loss: 0.636393  [   96/  146]
train() client id: f_00005-4-3 loss: 0.361819  [  128/  146]
train() client id: f_00005-5-0 loss: 0.331971  [   32/  146]
train() client id: f_00005-5-1 loss: 0.298379  [   64/  146]
train() client id: f_00005-5-2 loss: 0.578352  [   96/  146]
train() client id: f_00005-5-3 loss: 0.607557  [  128/  146]
train() client id: f_00005-6-0 loss: 0.416441  [   32/  146]
train() client id: f_00005-6-1 loss: 0.559143  [   64/  146]
train() client id: f_00005-6-2 loss: 0.464078  [   96/  146]
train() client id: f_00005-6-3 loss: 0.246356  [  128/  146]
train() client id: f_00005-7-0 loss: 0.618702  [   32/  146]
train() client id: f_00005-7-1 loss: 0.462527  [   64/  146]
train() client id: f_00005-7-2 loss: 0.217278  [   96/  146]
train() client id: f_00005-7-3 loss: 0.364636  [  128/  146]
train() client id: f_00005-8-0 loss: 0.436201  [   32/  146]
train() client id: f_00005-8-1 loss: 0.338214  [   64/  146]
train() client id: f_00005-8-2 loss: 0.549197  [   96/  146]
train() client id: f_00005-8-3 loss: 0.374753  [  128/  146]
train() client id: f_00005-9-0 loss: 0.386722  [   32/  146]
train() client id: f_00005-9-1 loss: 0.304995  [   64/  146]
train() client id: f_00005-9-2 loss: 0.499384  [   96/  146]
train() client id: f_00005-9-3 loss: 0.355768  [  128/  146]
train() client id: f_00005-10-0 loss: 0.337740  [   32/  146]
train() client id: f_00005-10-1 loss: 0.489476  [   64/  146]
train() client id: f_00005-10-2 loss: 0.398995  [   96/  146]
train() client id: f_00005-10-3 loss: 0.388833  [  128/  146]
train() client id: f_00005-11-0 loss: 0.440843  [   32/  146]
train() client id: f_00005-11-1 loss: 0.453670  [   64/  146]
train() client id: f_00005-11-2 loss: 0.140830  [   96/  146]
train() client id: f_00005-11-3 loss: 0.562289  [  128/  146]
train() client id: f_00005-12-0 loss: 0.410323  [   32/  146]
train() client id: f_00005-12-1 loss: 0.473769  [   64/  146]
train() client id: f_00005-12-2 loss: 0.414118  [   96/  146]
train() client id: f_00005-12-3 loss: 0.353122  [  128/  146]
train() client id: f_00005-13-0 loss: 0.349002  [   32/  146]
train() client id: f_00005-13-1 loss: 0.533242  [   64/  146]
train() client id: f_00005-13-2 loss: 0.478195  [   96/  146]
train() client id: f_00005-13-3 loss: 0.195587  [  128/  146]
train() client id: f_00006-0-0 loss: 0.535009  [   32/   54]
train() client id: f_00006-1-0 loss: 0.537570  [   32/   54]
train() client id: f_00006-2-0 loss: 0.477885  [   32/   54]
train() client id: f_00006-3-0 loss: 0.514755  [   32/   54]
train() client id: f_00006-4-0 loss: 0.475090  [   32/   54]
train() client id: f_00006-5-0 loss: 0.526217  [   32/   54]
train() client id: f_00006-6-0 loss: 0.504158  [   32/   54]
train() client id: f_00006-7-0 loss: 0.574647  [   32/   54]
train() client id: f_00006-8-0 loss: 0.580607  [   32/   54]
train() client id: f_00006-9-0 loss: 0.577459  [   32/   54]
train() client id: f_00006-10-0 loss: 0.525688  [   32/   54]
train() client id: f_00006-11-0 loss: 0.536569  [   32/   54]
train() client id: f_00006-12-0 loss: 0.529032  [   32/   54]
train() client id: f_00006-13-0 loss: 0.486373  [   32/   54]
train() client id: f_00007-0-0 loss: 0.721634  [   32/  179]
train() client id: f_00007-0-1 loss: 0.664784  [   64/  179]
train() client id: f_00007-0-2 loss: 0.868633  [   96/  179]
train() client id: f_00007-0-3 loss: 0.649100  [  128/  179]
train() client id: f_00007-0-4 loss: 0.674301  [  160/  179]
train() client id: f_00007-1-0 loss: 0.558270  [   32/  179]
train() client id: f_00007-1-1 loss: 0.870009  [   64/  179]
train() client id: f_00007-1-2 loss: 0.651246  [   96/  179]
train() client id: f_00007-1-3 loss: 0.711412  [  128/  179]
train() client id: f_00007-1-4 loss: 0.743352  [  160/  179]
train() client id: f_00007-2-0 loss: 0.748955  [   32/  179]
train() client id: f_00007-2-1 loss: 0.562508  [   64/  179]
train() client id: f_00007-2-2 loss: 0.793506  [   96/  179]
train() client id: f_00007-2-3 loss: 0.668562  [  128/  179]
train() client id: f_00007-2-4 loss: 0.682481  [  160/  179]
train() client id: f_00007-3-0 loss: 0.877275  [   32/  179]
train() client id: f_00007-3-1 loss: 0.594143  [   64/  179]
train() client id: f_00007-3-2 loss: 0.656048  [   96/  179]
train() client id: f_00007-3-3 loss: 0.686352  [  128/  179]
train() client id: f_00007-3-4 loss: 0.631249  [  160/  179]
train() client id: f_00007-4-0 loss: 0.566482  [   32/  179]
train() client id: f_00007-4-1 loss: 0.624933  [   64/  179]
train() client id: f_00007-4-2 loss: 0.748676  [   96/  179]
train() client id: f_00007-4-3 loss: 0.619805  [  128/  179]
train() client id: f_00007-4-4 loss: 0.745077  [  160/  179]
train() client id: f_00007-5-0 loss: 0.625929  [   32/  179]
train() client id: f_00007-5-1 loss: 0.542294  [   64/  179]
train() client id: f_00007-5-2 loss: 0.593916  [   96/  179]
train() client id: f_00007-5-3 loss: 0.758288  [  128/  179]
train() client id: f_00007-5-4 loss: 0.689488  [  160/  179]
train() client id: f_00007-6-0 loss: 0.632250  [   32/  179]
train() client id: f_00007-6-1 loss: 0.805037  [   64/  179]
train() client id: f_00007-6-2 loss: 0.645268  [   96/  179]
train() client id: f_00007-6-3 loss: 0.685160  [  128/  179]
train() client id: f_00007-6-4 loss: 0.548891  [  160/  179]
train() client id: f_00007-7-0 loss: 0.804123  [   32/  179]
train() client id: f_00007-7-1 loss: 0.558320  [   64/  179]
train() client id: f_00007-7-2 loss: 0.532098  [   96/  179]
train() client id: f_00007-7-3 loss: 0.598243  [  128/  179]
train() client id: f_00007-7-4 loss: 0.662241  [  160/  179]
train() client id: f_00007-8-0 loss: 0.656010  [   32/  179]
train() client id: f_00007-8-1 loss: 0.588952  [   64/  179]
train() client id: f_00007-8-2 loss: 0.690608  [   96/  179]
train() client id: f_00007-8-3 loss: 0.841043  [  128/  179]
train() client id: f_00007-8-4 loss: 0.621984  [  160/  179]
train() client id: f_00007-9-0 loss: 0.503569  [   32/  179]
train() client id: f_00007-9-1 loss: 0.861850  [   64/  179]
train() client id: f_00007-9-2 loss: 0.713112  [   96/  179]
train() client id: f_00007-9-3 loss: 0.620600  [  128/  179]
train() client id: f_00007-9-4 loss: 0.719599  [  160/  179]
train() client id: f_00007-10-0 loss: 0.630341  [   32/  179]
train() client id: f_00007-10-1 loss: 0.680065  [   64/  179]
train() client id: f_00007-10-2 loss: 0.621905  [   96/  179]
train() client id: f_00007-10-3 loss: 0.522443  [  128/  179]
train() client id: f_00007-10-4 loss: 0.761602  [  160/  179]
train() client id: f_00007-11-0 loss: 0.523663  [   32/  179]
train() client id: f_00007-11-1 loss: 0.839546  [   64/  179]
train() client id: f_00007-11-2 loss: 0.692184  [   96/  179]
train() client id: f_00007-11-3 loss: 0.536541  [  128/  179]
train() client id: f_00007-11-4 loss: 0.808530  [  160/  179]
train() client id: f_00007-12-0 loss: 0.630873  [   32/  179]
train() client id: f_00007-12-1 loss: 0.640103  [   64/  179]
train() client id: f_00007-12-2 loss: 0.726275  [   96/  179]
train() client id: f_00007-12-3 loss: 0.735352  [  128/  179]
train() client id: f_00007-12-4 loss: 0.672375  [  160/  179]
train() client id: f_00007-13-0 loss: 0.734085  [   32/  179]
train() client id: f_00007-13-1 loss: 0.645031  [   64/  179]
train() client id: f_00007-13-2 loss: 0.539934  [   96/  179]
train() client id: f_00007-13-3 loss: 0.634057  [  128/  179]
train() client id: f_00007-13-4 loss: 0.653165  [  160/  179]
train() client id: f_00008-0-0 loss: 0.766299  [   32/  130]
train() client id: f_00008-0-1 loss: 0.669883  [   64/  130]
train() client id: f_00008-0-2 loss: 0.779213  [   96/  130]
train() client id: f_00008-0-3 loss: 0.882529  [  128/  130]
train() client id: f_00008-1-0 loss: 0.829398  [   32/  130]
train() client id: f_00008-1-1 loss: 0.716263  [   64/  130]
train() client id: f_00008-1-2 loss: 0.727046  [   96/  130]
train() client id: f_00008-1-3 loss: 0.776417  [  128/  130]
train() client id: f_00008-2-0 loss: 0.800971  [   32/  130]
train() client id: f_00008-2-1 loss: 0.714567  [   64/  130]
train() client id: f_00008-2-2 loss: 0.823029  [   96/  130]
train() client id: f_00008-2-3 loss: 0.754505  [  128/  130]
train() client id: f_00008-3-0 loss: 0.685245  [   32/  130]
train() client id: f_00008-3-1 loss: 0.786522  [   64/  130]
train() client id: f_00008-3-2 loss: 0.834214  [   96/  130]
train() client id: f_00008-3-3 loss: 0.775553  [  128/  130]
train() client id: f_00008-4-0 loss: 0.745300  [   32/  130]
train() client id: f_00008-4-1 loss: 0.676280  [   64/  130]
train() client id: f_00008-4-2 loss: 0.855328  [   96/  130]
train() client id: f_00008-4-3 loss: 0.818620  [  128/  130]
train() client id: f_00008-5-0 loss: 0.695528  [   32/  130]
train() client id: f_00008-5-1 loss: 0.910896  [   64/  130]
train() client id: f_00008-5-2 loss: 0.753131  [   96/  130]
train() client id: f_00008-5-3 loss: 0.680905  [  128/  130]
train() client id: f_00008-6-0 loss: 0.866353  [   32/  130]
train() client id: f_00008-6-1 loss: 0.829411  [   64/  130]
train() client id: f_00008-6-2 loss: 0.670334  [   96/  130]
train() client id: f_00008-6-3 loss: 0.721516  [  128/  130]
train() client id: f_00008-7-0 loss: 0.716803  [   32/  130]
train() client id: f_00008-7-1 loss: 0.793324  [   64/  130]
train() client id: f_00008-7-2 loss: 0.737515  [   96/  130]
train() client id: f_00008-7-3 loss: 0.776608  [  128/  130]
train() client id: f_00008-8-0 loss: 0.715554  [   32/  130]
train() client id: f_00008-8-1 loss: 0.681493  [   64/  130]
train() client id: f_00008-8-2 loss: 0.730013  [   96/  130]
train() client id: f_00008-8-3 loss: 0.959241  [  128/  130]
train() client id: f_00008-9-0 loss: 0.778605  [   32/  130]
train() client id: f_00008-9-1 loss: 0.819785  [   64/  130]
train() client id: f_00008-9-2 loss: 0.691206  [   96/  130]
train() client id: f_00008-9-3 loss: 0.754812  [  128/  130]
train() client id: f_00008-10-0 loss: 0.775349  [   32/  130]
train() client id: f_00008-10-1 loss: 0.738738  [   64/  130]
train() client id: f_00008-10-2 loss: 0.754551  [   96/  130]
train() client id: f_00008-10-3 loss: 0.811534  [  128/  130]
train() client id: f_00008-11-0 loss: 0.779026  [   32/  130]
train() client id: f_00008-11-1 loss: 0.734837  [   64/  130]
train() client id: f_00008-11-2 loss: 0.792738  [   96/  130]
train() client id: f_00008-11-3 loss: 0.769710  [  128/  130]
train() client id: f_00008-12-0 loss: 0.736847  [   32/  130]
train() client id: f_00008-12-1 loss: 0.855356  [   64/  130]
train() client id: f_00008-12-2 loss: 0.702930  [   96/  130]
train() client id: f_00008-12-3 loss: 0.795572  [  128/  130]
train() client id: f_00008-13-0 loss: 0.737932  [   32/  130]
train() client id: f_00008-13-1 loss: 0.676048  [   64/  130]
train() client id: f_00008-13-2 loss: 0.857407  [   96/  130]
train() client id: f_00008-13-3 loss: 0.806111  [  128/  130]
train() client id: f_00009-0-0 loss: 1.194446  [   32/  118]
train() client id: f_00009-0-1 loss: 1.128663  [   64/  118]
train() client id: f_00009-0-2 loss: 0.925893  [   96/  118]
train() client id: f_00009-1-0 loss: 1.052954  [   32/  118]
train() client id: f_00009-1-1 loss: 0.978895  [   64/  118]
train() client id: f_00009-1-2 loss: 0.969462  [   96/  118]
train() client id: f_00009-2-0 loss: 0.977652  [   32/  118]
train() client id: f_00009-2-1 loss: 0.859796  [   64/  118]
train() client id: f_00009-2-2 loss: 1.051341  [   96/  118]
train() client id: f_00009-3-0 loss: 0.907298  [   32/  118]
train() client id: f_00009-3-1 loss: 0.899714  [   64/  118]
train() client id: f_00009-3-2 loss: 0.878374  [   96/  118]
train() client id: f_00009-4-0 loss: 0.856080  [   32/  118]
train() client id: f_00009-4-1 loss: 0.926813  [   64/  118]
train() client id: f_00009-4-2 loss: 0.744906  [   96/  118]
train() client id: f_00009-5-0 loss: 0.801422  [   32/  118]
train() client id: f_00009-5-1 loss: 0.805020  [   64/  118]
train() client id: f_00009-5-2 loss: 0.781027  [   96/  118]
train() client id: f_00009-6-0 loss: 0.857108  [   32/  118]
train() client id: f_00009-6-1 loss: 0.832912  [   64/  118]
train() client id: f_00009-6-2 loss: 0.789110  [   96/  118]
train() client id: f_00009-7-0 loss: 0.759285  [   32/  118]
train() client id: f_00009-7-1 loss: 0.745955  [   64/  118]
train() client id: f_00009-7-2 loss: 0.821256  [   96/  118]
train() client id: f_00009-8-0 loss: 0.955318  [   32/  118]
train() client id: f_00009-8-1 loss: 0.641860  [   64/  118]
train() client id: f_00009-8-2 loss: 0.645929  [   96/  118]
train() client id: f_00009-9-0 loss: 0.813346  [   32/  118]
train() client id: f_00009-9-1 loss: 0.712905  [   64/  118]
train() client id: f_00009-9-2 loss: 0.743251  [   96/  118]
train() client id: f_00009-10-0 loss: 0.759616  [   32/  118]
train() client id: f_00009-10-1 loss: 0.657619  [   64/  118]
train() client id: f_00009-10-2 loss: 0.839284  [   96/  118]
train() client id: f_00009-11-0 loss: 0.633925  [   32/  118]
train() client id: f_00009-11-1 loss: 0.760212  [   64/  118]
train() client id: f_00009-11-2 loss: 0.833317  [   96/  118]
train() client id: f_00009-12-0 loss: 0.707034  [   32/  118]
train() client id: f_00009-12-1 loss: 0.777957  [   64/  118]
train() client id: f_00009-12-2 loss: 0.671285  [   96/  118]
train() client id: f_00009-13-0 loss: 0.777081  [   32/  118]
train() client id: f_00009-13-1 loss: 0.690411  [   64/  118]
train() client id: f_00009-13-2 loss: 0.590562  [   96/  118]
At round 23 accuracy: 0.6392572944297082
At round 23 training accuracy: 0.5828303152246814
At round 23 training loss: 0.849950881910496
update_location
xs = 8.927491 236.223621 5.882650 10.934260 -152.581990 -0.230757 -5.849135 -5.143845 -175.120581 20.134486 
ys = -227.390647 7.291448 125.684448 -27.290817 -9.642386 0.794442 -36.381692 121.628436 25.881276 -662.232496 
xs mean: -5.682379970521214
ys mean: -68.16579882624053
dists_uav = 248.568314 256.621831 160.720833 104.232177 182.686177 100.003422 106.573167 157.543440 203.315170 670.042742 
uav_gains = -111.174754 -111.869063 -105.162988 -100.450074 -106.597452 -100.000387 -100.691236 -104.943655 -107.891674 -127.597958 
uav_gains_db_mean: -107.63792405270306
dists_bs = 442.433673 444.106997 187.484817 274.760120 185.998328 246.762770 270.729081 178.043920 149.118772 859.671867 
bs_gains = -113.650808 -113.696712 -103.210110 -107.857763 -103.113312 -106.550889 -107.678037 -102.581819 -100.425964 -121.728375 
bs_gains_db_mean: -108.04937894430043
Round 24
-------------------------------
ene_coms = [0.01160993 0.01222655 0.0072691  0.00642237 0.00723572 0.00630252
 0.00648852 0.00705762 0.00641583 0.03533543]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.89623689 16.3613528   7.6870908   2.74435381 18.75395926  9.00543614
  3.41460491 11.02741349  8.0347501   7.88813189]
obj_prev = 92.81333010163479
eta_min = 1.3495486555291524e-12	eta_max = 0.7510599702670334
af = 19.491574900134562	bf = 2.1231771925649463	zeta = 21.44073239014802	eta = 0.909090909090909
af = 19.491574900134562	bf = 2.1231771925649463	zeta = 43.790149710628555	eta = 0.44511322817888543
af = 19.491574900134562	bf = 2.1231771925649463	zeta = 32.259150851286556	eta = 0.6042184740072661
af = 19.491574900134562	bf = 2.1231771925649463	zeta = 30.176778480587643	eta = 0.6459130457770121
af = 19.491574900134562	bf = 2.1231771925649463	zeta = 30.056902994306427	eta = 0.648489130893718
af = 19.491574900134562	bf = 2.1231771925649463	zeta = 30.05646431692845	eta = 0.648498595663379
af = 19.491574900134562	bf = 2.1231771925649463	zeta = 30.05646431102241	eta = 0.6484985957908078
eta = 0.6484985957908078
ene_coms = [0.01160993 0.01222655 0.0072691  0.00642237 0.00723572 0.00630252
 0.00648852 0.00705762 0.00641583 0.03533543]
ene_comp = [0.035738   0.07516325 0.03517073 0.0121963  0.08679228 0.0414107
 0.01531628 0.05077065 0.03687254 0.03346892]
ene_total = [2.68885376 4.96280177 2.41012638 1.05734032 5.33977977 2.70959789
 1.23827835 3.28402446 2.45831399 3.90734763]
ti_comp = [0.39064339 0.38447719 0.4340517  0.44251906 0.43438558 0.44371754
 0.44185758 0.43616649 0.44258446 0.15338844]
ti_coms = [0.11609935 0.12226555 0.07269104 0.06422368 0.07235716 0.06302519
 0.06488516 0.07057625 0.06415828 0.3533543 ]
t_total = [28.77754211 28.77754211 28.77754211 28.77754211 28.77754211 28.77754211
 28.77754211 28.77754211 28.77754211 28.77754211]
ene_coms = [0.01160993 0.01222655 0.0072691  0.00642237 0.00723572 0.00630252
 0.00648852 0.00705762 0.00641583 0.03533543]
ene_comp = [1.86943213e-05 1.79537632e-04 1.44324866e-05 5.79029166e-07
 2.16557145e-04 2.25426203e-05 1.15020615e-06 4.29945625e-05
 1.59954614e-05 9.95909333e-05]
ene_total = [0.66038117 0.70453273 0.41362659 0.36475438 0.42320904 0.35919555
 0.36854328 0.40323889 0.36525847 2.01232841]
optimize_network iter = 0 obj = 6.075068524977453
eta = 0.6484985957908078
freqs = [4.57424834e+07 9.77473448e+07 4.05144454e+07 1.37805373e+07
 9.99023503e+07 4.66633517e+07 1.73316984e+07 5.82009953e+07
 4.16559334e+07 1.09098570e+08]
eta_min = 0.6484985957908087	eta_max = 0.6484985957908052
af = 0.028210697892066216	bf = 2.1231771925649463	zeta = 0.03103176768127284	eta = 0.9090909090909091
af = 0.028210697892066216	bf = 2.1231771925649463	zeta = 23.384525551787885	eta = 0.0012063831626427564
af = 0.028210697892066216	bf = 2.1231771925649463	zeta = 2.3155501486797134	eta = 0.012183151337988281
af = 0.028210697892066216	bf = 2.1231771925649463	zeta = 2.275241129907411	eta = 0.01239899258203645
af = 0.028210697892066216	bf = 2.1231771925649463	zeta = 2.2752367509023763	eta = 0.012399016445597426
eta = 0.012399016445597426
ene_coms = [0.01160993 0.01222655 0.0072691  0.00642237 0.00723572 0.00630252
 0.00648852 0.00705762 0.00641583 0.03533543]
ene_comp = [1.89497845e-04 1.81991064e-03 1.46297106e-04 5.86941764e-06
 2.19516460e-03 2.28506716e-04 1.16592405e-05 4.35820954e-04
 1.62140440e-04 1.00951872e-03]
ene_total = [0.23849146 0.28390874 0.14988092 0.12992826 0.19061803 0.13200584
 0.13138227 0.15145838 0.13295465 0.73460821]
ti_comp = [0.39064339 0.38447719 0.4340517  0.44251906 0.43438558 0.44371754
 0.44185758 0.43616649 0.44258446 0.15338844]
ti_coms = [0.11609935 0.12226555 0.07269104 0.06422368 0.07235716 0.06302519
 0.06488516 0.07057625 0.06415828 0.3533543 ]
t_total = [28.77754211 28.77754211 28.77754211 28.77754211 28.77754211 28.77754211
 28.77754211 28.77754211 28.77754211 28.77754211]
ene_coms = [0.01160993 0.01222655 0.0072691  0.00642237 0.00723572 0.00630252
 0.00648852 0.00705762 0.00641583 0.03533543]
ene_comp = [1.86943213e-05 1.79537632e-04 1.44324866e-05 5.79029166e-07
 2.16557145e-04 2.25426203e-05 1.15020615e-06 4.29945625e-05
 1.59954614e-05 9.95909333e-05]
ene_total = [0.66038117 0.70453273 0.41362659 0.36475438 0.42320904 0.35919555
 0.36854328 0.40323889 0.36525847 2.01232841]
optimize_network iter = 1 obj = 6.075068524977468
eta = 0.6484985957908087
freqs = [4.57424834e+07 9.77473448e+07 4.05144454e+07 1.37805373e+07
 9.99023503e+07 4.66633517e+07 1.73316984e+07 5.82009953e+07
 4.16559334e+07 1.09098570e+08]
Done!
ene_coms = [0.01160993 0.01222655 0.0072691  0.00642237 0.00723572 0.00630252
 0.00648852 0.00705762 0.00641583 0.03533543]
ene_comp = [1.84547456e-05 1.77236780e-04 1.42475281e-05 5.71608660e-07
 2.13781873e-04 2.22537270e-05 1.13546577e-06 4.24435688e-05
 1.57904728e-05 9.83146330e-05]
ene_total = [0.01162839 0.01240379 0.00728335 0.00642294 0.0074495  0.00632477
 0.00648965 0.00710007 0.00643162 0.03543374]
At round 24 energy consumption: 0.10696782650406136
At round 24 eta: 0.6484985957908087
At round 24 a_n: 19.96150252899076
At round 24 local rounds: 14.181745112214239
At round 24 global rounds: 56.78925401137499
gradient difference: 0.4482467770576477
train() client id: f_00000-0-0 loss: 1.131232  [   32/  126]
train() client id: f_00000-0-1 loss: 1.133755  [   64/  126]
train() client id: f_00000-0-2 loss: 1.108820  [   96/  126]
train() client id: f_00000-1-0 loss: 1.219874  [   32/  126]
train() client id: f_00000-1-1 loss: 1.096468  [   64/  126]
train() client id: f_00000-1-2 loss: 0.992125  [   96/  126]
train() client id: f_00000-2-0 loss: 1.228382  [   32/  126]
train() client id: f_00000-2-1 loss: 0.932116  [   64/  126]
train() client id: f_00000-2-2 loss: 0.985685  [   96/  126]
train() client id: f_00000-3-0 loss: 0.974728  [   32/  126]
train() client id: f_00000-3-1 loss: 1.013570  [   64/  126]
train() client id: f_00000-3-2 loss: 0.904175  [   96/  126]
train() client id: f_00000-4-0 loss: 0.946637  [   32/  126]
train() client id: f_00000-4-1 loss: 0.979242  [   64/  126]
train() client id: f_00000-4-2 loss: 0.928804  [   96/  126]
train() client id: f_00000-5-0 loss: 0.883290  [   32/  126]
train() client id: f_00000-5-1 loss: 0.914704  [   64/  126]
train() client id: f_00000-5-2 loss: 0.909716  [   96/  126]
train() client id: f_00000-6-0 loss: 0.842700  [   32/  126]
train() client id: f_00000-6-1 loss: 0.966771  [   64/  126]
train() client id: f_00000-6-2 loss: 0.849830  [   96/  126]
train() client id: f_00000-7-0 loss: 0.875053  [   32/  126]
train() client id: f_00000-7-1 loss: 0.830869  [   64/  126]
train() client id: f_00000-7-2 loss: 0.864765  [   96/  126]
train() client id: f_00000-8-0 loss: 0.892112  [   32/  126]
train() client id: f_00000-8-1 loss: 0.917122  [   64/  126]
train() client id: f_00000-8-2 loss: 0.793965  [   96/  126]
train() client id: f_00000-9-0 loss: 0.829770  [   32/  126]
train() client id: f_00000-9-1 loss: 0.861573  [   64/  126]
train() client id: f_00000-9-2 loss: 0.871210  [   96/  126]
train() client id: f_00000-10-0 loss: 0.963765  [   32/  126]
train() client id: f_00000-10-1 loss: 0.835942  [   64/  126]
train() client id: f_00000-10-2 loss: 0.796813  [   96/  126]
train() client id: f_00000-11-0 loss: 0.862142  [   32/  126]
train() client id: f_00000-11-1 loss: 0.803556  [   64/  126]
train() client id: f_00000-11-2 loss: 0.937000  [   96/  126]
train() client id: f_00000-12-0 loss: 0.900639  [   32/  126]
train() client id: f_00000-12-1 loss: 0.846290  [   64/  126]
train() client id: f_00000-12-2 loss: 0.935025  [   96/  126]
train() client id: f_00000-13-0 loss: 0.855905  [   32/  126]
train() client id: f_00000-13-1 loss: 0.986008  [   64/  126]
train() client id: f_00000-13-2 loss: 0.964189  [   96/  126]
train() client id: f_00001-0-0 loss: 0.420035  [   32/  265]
train() client id: f_00001-0-1 loss: 0.472302  [   64/  265]
train() client id: f_00001-0-2 loss: 0.378273  [   96/  265]
train() client id: f_00001-0-3 loss: 0.556918  [  128/  265]
train() client id: f_00001-0-4 loss: 0.414577  [  160/  265]
train() client id: f_00001-0-5 loss: 0.497679  [  192/  265]
train() client id: f_00001-0-6 loss: 0.469167  [  224/  265]
train() client id: f_00001-0-7 loss: 0.413302  [  256/  265]
train() client id: f_00001-1-0 loss: 0.443710  [   32/  265]
train() client id: f_00001-1-1 loss: 0.398631  [   64/  265]
train() client id: f_00001-1-2 loss: 0.408679  [   96/  265]
train() client id: f_00001-1-3 loss: 0.543059  [  128/  265]
train() client id: f_00001-1-4 loss: 0.398676  [  160/  265]
train() client id: f_00001-1-5 loss: 0.398058  [  192/  265]
train() client id: f_00001-1-6 loss: 0.474966  [  224/  265]
train() client id: f_00001-1-7 loss: 0.488145  [  256/  265]
train() client id: f_00001-2-0 loss: 0.400107  [   32/  265]
train() client id: f_00001-2-1 loss: 0.486996  [   64/  265]
train() client id: f_00001-2-2 loss: 0.327142  [   96/  265]
train() client id: f_00001-2-3 loss: 0.433083  [  128/  265]
train() client id: f_00001-2-4 loss: 0.511399  [  160/  265]
train() client id: f_00001-2-5 loss: 0.479251  [  192/  265]
train() client id: f_00001-2-6 loss: 0.495439  [  224/  265]
train() client id: f_00001-2-7 loss: 0.372189  [  256/  265]
train() client id: f_00001-3-0 loss: 0.444550  [   32/  265]
train() client id: f_00001-3-1 loss: 0.403843  [   64/  265]
train() client id: f_00001-3-2 loss: 0.409363  [   96/  265]
train() client id: f_00001-3-3 loss: 0.341021  [  128/  265]
train() client id: f_00001-3-4 loss: 0.398812  [  160/  265]
train() client id: f_00001-3-5 loss: 0.459012  [  192/  265]
train() client id: f_00001-3-6 loss: 0.547655  [  224/  265]
train() client id: f_00001-3-7 loss: 0.396812  [  256/  265]
train() client id: f_00001-4-0 loss: 0.449075  [   32/  265]
train() client id: f_00001-4-1 loss: 0.539622  [   64/  265]
train() client id: f_00001-4-2 loss: 0.325813  [   96/  265]
train() client id: f_00001-4-3 loss: 0.332285  [  128/  265]
train() client id: f_00001-4-4 loss: 0.455082  [  160/  265]
train() client id: f_00001-4-5 loss: 0.455130  [  192/  265]
train() client id: f_00001-4-6 loss: 0.440888  [  224/  265]
train() client id: f_00001-4-7 loss: 0.433784  [  256/  265]
train() client id: f_00001-5-0 loss: 0.368933  [   32/  265]
train() client id: f_00001-5-1 loss: 0.372034  [   64/  265]
train() client id: f_00001-5-2 loss: 0.560785  [   96/  265]
train() client id: f_00001-5-3 loss: 0.450677  [  128/  265]
train() client id: f_00001-5-4 loss: 0.324806  [  160/  265]
train() client id: f_00001-5-5 loss: 0.540256  [  192/  265]
train() client id: f_00001-5-6 loss: 0.414208  [  224/  265]
train() client id: f_00001-5-7 loss: 0.366874  [  256/  265]
train() client id: f_00001-6-0 loss: 0.556702  [   32/  265]
train() client id: f_00001-6-1 loss: 0.418358  [   64/  265]
train() client id: f_00001-6-2 loss: 0.397950  [   96/  265]
train() client id: f_00001-6-3 loss: 0.350608  [  128/  265]
train() client id: f_00001-6-4 loss: 0.362651  [  160/  265]
train() client id: f_00001-6-5 loss: 0.376726  [  192/  265]
train() client id: f_00001-6-6 loss: 0.486367  [  224/  265]
train() client id: f_00001-6-7 loss: 0.423022  [  256/  265]
train() client id: f_00001-7-0 loss: 0.480751  [   32/  265]
train() client id: f_00001-7-1 loss: 0.368348  [   64/  265]
train() client id: f_00001-7-2 loss: 0.373156  [   96/  265]
train() client id: f_00001-7-3 loss: 0.394243  [  128/  265]
train() client id: f_00001-7-4 loss: 0.413729  [  160/  265]
train() client id: f_00001-7-5 loss: 0.431444  [  192/  265]
train() client id: f_00001-7-6 loss: 0.386900  [  224/  265]
train() client id: f_00001-7-7 loss: 0.523572  [  256/  265]
train() client id: f_00001-8-0 loss: 0.317566  [   32/  265]
train() client id: f_00001-8-1 loss: 0.327994  [   64/  265]
train() client id: f_00001-8-2 loss: 0.469812  [   96/  265]
train() client id: f_00001-8-3 loss: 0.693763  [  128/  265]
train() client id: f_00001-8-4 loss: 0.360243  [  160/  265]
train() client id: f_00001-8-5 loss: 0.430234  [  192/  265]
train() client id: f_00001-8-6 loss: 0.365349  [  224/  265]
train() client id: f_00001-8-7 loss: 0.318838  [  256/  265]
train() client id: f_00001-9-0 loss: 0.321452  [   32/  265]
train() client id: f_00001-9-1 loss: 0.420478  [   64/  265]
train() client id: f_00001-9-2 loss: 0.461802  [   96/  265]
train() client id: f_00001-9-3 loss: 0.471994  [  128/  265]
train() client id: f_00001-9-4 loss: 0.420299  [  160/  265]
train() client id: f_00001-9-5 loss: 0.388609  [  192/  265]
train() client id: f_00001-9-6 loss: 0.396257  [  224/  265]
train() client id: f_00001-9-7 loss: 0.474162  [  256/  265]
train() client id: f_00001-10-0 loss: 0.482573  [   32/  265]
train() client id: f_00001-10-1 loss: 0.396863  [   64/  265]
train() client id: f_00001-10-2 loss: 0.370534  [   96/  265]
train() client id: f_00001-10-3 loss: 0.377315  [  128/  265]
train() client id: f_00001-10-4 loss: 0.413757  [  160/  265]
train() client id: f_00001-10-5 loss: 0.438866  [  192/  265]
train() client id: f_00001-10-6 loss: 0.418924  [  224/  265]
train() client id: f_00001-10-7 loss: 0.443007  [  256/  265]
train() client id: f_00001-11-0 loss: 0.382570  [   32/  265]
train() client id: f_00001-11-1 loss: 0.428246  [   64/  265]
train() client id: f_00001-11-2 loss: 0.387258  [   96/  265]
train() client id: f_00001-11-3 loss: 0.394950  [  128/  265]
train() client id: f_00001-11-4 loss: 0.376252  [  160/  265]
train() client id: f_00001-11-5 loss: 0.480807  [  192/  265]
train() client id: f_00001-11-6 loss: 0.387966  [  224/  265]
train() client id: f_00001-11-7 loss: 0.370247  [  256/  265]
train() client id: f_00001-12-0 loss: 0.513034  [   32/  265]
train() client id: f_00001-12-1 loss: 0.337506  [   64/  265]
train() client id: f_00001-12-2 loss: 0.432395  [   96/  265]
train() client id: f_00001-12-3 loss: 0.329888  [  128/  265]
train() client id: f_00001-12-4 loss: 0.367940  [  160/  265]
train() client id: f_00001-12-5 loss: 0.454780  [  192/  265]
train() client id: f_00001-12-6 loss: 0.372519  [  224/  265]
train() client id: f_00001-12-7 loss: 0.450760  [  256/  265]
train() client id: f_00001-13-0 loss: 0.332097  [   32/  265]
train() client id: f_00001-13-1 loss: 0.319669  [   64/  265]
train() client id: f_00001-13-2 loss: 0.497644  [   96/  265]
train() client id: f_00001-13-3 loss: 0.452455  [  128/  265]
train() client id: f_00001-13-4 loss: 0.383103  [  160/  265]
train() client id: f_00001-13-5 loss: 0.575197  [  192/  265]
train() client id: f_00001-13-6 loss: 0.333279  [  224/  265]
train() client id: f_00001-13-7 loss: 0.455160  [  256/  265]
train() client id: f_00002-0-0 loss: 1.213916  [   32/  124]
train() client id: f_00002-0-1 loss: 1.228641  [   64/  124]
train() client id: f_00002-0-2 loss: 1.106113  [   96/  124]
train() client id: f_00002-1-0 loss: 1.054749  [   32/  124]
train() client id: f_00002-1-1 loss: 1.042005  [   64/  124]
train() client id: f_00002-1-2 loss: 1.248962  [   96/  124]
train() client id: f_00002-2-0 loss: 1.028966  [   32/  124]
train() client id: f_00002-2-1 loss: 1.043437  [   64/  124]
train() client id: f_00002-2-2 loss: 1.074639  [   96/  124]
train() client id: f_00002-3-0 loss: 1.030323  [   32/  124]
train() client id: f_00002-3-1 loss: 1.056332  [   64/  124]
train() client id: f_00002-3-2 loss: 0.972837  [   96/  124]
train() client id: f_00002-4-0 loss: 0.978462  [   32/  124]
train() client id: f_00002-4-1 loss: 1.055347  [   64/  124]
train() client id: f_00002-4-2 loss: 1.149556  [   96/  124]
train() client id: f_00002-5-0 loss: 1.126597  [   32/  124]
train() client id: f_00002-5-1 loss: 0.923039  [   64/  124]
train() client id: f_00002-5-2 loss: 1.109454  [   96/  124]
train() client id: f_00002-6-0 loss: 0.841502  [   32/  124]
train() client id: f_00002-6-1 loss: 1.089288  [   64/  124]
train() client id: f_00002-6-2 loss: 0.978071  [   96/  124]
train() client id: f_00002-7-0 loss: 1.003552  [   32/  124]
train() client id: f_00002-7-1 loss: 1.004177  [   64/  124]
train() client id: f_00002-7-2 loss: 0.912211  [   96/  124]
train() client id: f_00002-8-0 loss: 0.944932  [   32/  124]
train() client id: f_00002-8-1 loss: 0.954068  [   64/  124]
train() client id: f_00002-8-2 loss: 0.911954  [   96/  124]
train() client id: f_00002-9-0 loss: 0.867447  [   32/  124]
train() client id: f_00002-9-1 loss: 1.012384  [   64/  124]
train() client id: f_00002-9-2 loss: 1.065227  [   96/  124]
train() client id: f_00002-10-0 loss: 0.913753  [   32/  124]
train() client id: f_00002-10-1 loss: 1.050227  [   64/  124]
train() client id: f_00002-10-2 loss: 0.964960  [   96/  124]
train() client id: f_00002-11-0 loss: 0.993512  [   32/  124]
train() client id: f_00002-11-1 loss: 0.834644  [   64/  124]
train() client id: f_00002-11-2 loss: 0.975372  [   96/  124]
train() client id: f_00002-12-0 loss: 0.869090  [   32/  124]
train() client id: f_00002-12-1 loss: 0.853044  [   64/  124]
train() client id: f_00002-12-2 loss: 1.076532  [   96/  124]
train() client id: f_00002-13-0 loss: 0.924476  [   32/  124]
train() client id: f_00002-13-1 loss: 0.989079  [   64/  124]
train() client id: f_00002-13-2 loss: 0.902805  [   96/  124]
train() client id: f_00003-0-0 loss: 0.783150  [   32/   43]
train() client id: f_00003-1-0 loss: 0.588325  [   32/   43]
train() client id: f_00003-2-0 loss: 0.721740  [   32/   43]
train() client id: f_00003-3-0 loss: 0.553489  [   32/   43]
train() client id: f_00003-4-0 loss: 0.706625  [   32/   43]
train() client id: f_00003-5-0 loss: 0.571535  [   32/   43]
train() client id: f_00003-6-0 loss: 0.689173  [   32/   43]
train() client id: f_00003-7-0 loss: 0.503139  [   32/   43]
train() client id: f_00003-8-0 loss: 0.744755  [   32/   43]
train() client id: f_00003-9-0 loss: 0.609536  [   32/   43]
train() client id: f_00003-10-0 loss: 0.830532  [   32/   43]
train() client id: f_00003-11-0 loss: 0.774979  [   32/   43]
train() client id: f_00003-12-0 loss: 0.614454  [   32/   43]
train() client id: f_00003-13-0 loss: 0.625102  [   32/   43]
train() client id: f_00004-0-0 loss: 0.744838  [   32/  306]
train() client id: f_00004-0-1 loss: 0.919119  [   64/  306]
train() client id: f_00004-0-2 loss: 0.825887  [   96/  306]
train() client id: f_00004-0-3 loss: 0.814053  [  128/  306]
train() client id: f_00004-0-4 loss: 0.838321  [  160/  306]
train() client id: f_00004-0-5 loss: 0.811942  [  192/  306]
train() client id: f_00004-0-6 loss: 1.032236  [  224/  306]
train() client id: f_00004-0-7 loss: 1.034126  [  256/  306]
train() client id: f_00004-0-8 loss: 0.788133  [  288/  306]
train() client id: f_00004-1-0 loss: 0.843324  [   32/  306]
train() client id: f_00004-1-1 loss: 0.817718  [   64/  306]
train() client id: f_00004-1-2 loss: 0.724188  [   96/  306]
train() client id: f_00004-1-3 loss: 0.875571  [  128/  306]
train() client id: f_00004-1-4 loss: 0.863856  [  160/  306]
train() client id: f_00004-1-5 loss: 0.990235  [  192/  306]
train() client id: f_00004-1-6 loss: 0.841392  [  224/  306]
train() client id: f_00004-1-7 loss: 0.971956  [  256/  306]
train() client id: f_00004-1-8 loss: 0.837481  [  288/  306]
train() client id: f_00004-2-0 loss: 0.878694  [   32/  306]
train() client id: f_00004-2-1 loss: 0.894225  [   64/  306]
train() client id: f_00004-2-2 loss: 0.868836  [   96/  306]
train() client id: f_00004-2-3 loss: 0.911124  [  128/  306]
train() client id: f_00004-2-4 loss: 0.831868  [  160/  306]
train() client id: f_00004-2-5 loss: 0.896935  [  192/  306]
train() client id: f_00004-2-6 loss: 0.830907  [  224/  306]
train() client id: f_00004-2-7 loss: 0.881613  [  256/  306]
train() client id: f_00004-2-8 loss: 0.816823  [  288/  306]
train() client id: f_00004-3-0 loss: 0.821911  [   32/  306]
train() client id: f_00004-3-1 loss: 0.782825  [   64/  306]
train() client id: f_00004-3-2 loss: 0.863702  [   96/  306]
train() client id: f_00004-3-3 loss: 0.764284  [  128/  306]
train() client id: f_00004-3-4 loss: 1.005187  [  160/  306]
train() client id: f_00004-3-5 loss: 0.983577  [  192/  306]
train() client id: f_00004-3-6 loss: 0.903112  [  224/  306]
train() client id: f_00004-3-7 loss: 0.808460  [  256/  306]
train() client id: f_00004-3-8 loss: 0.863672  [  288/  306]
train() client id: f_00004-4-0 loss: 0.777836  [   32/  306]
train() client id: f_00004-4-1 loss: 0.885044  [   64/  306]
train() client id: f_00004-4-2 loss: 1.002489  [   96/  306]
train() client id: f_00004-4-3 loss: 0.846627  [  128/  306]
train() client id: f_00004-4-4 loss: 0.877234  [  160/  306]
train() client id: f_00004-4-5 loss: 0.805311  [  192/  306]
train() client id: f_00004-4-6 loss: 0.767783  [  224/  306]
train() client id: f_00004-4-7 loss: 0.804327  [  256/  306]
train() client id: f_00004-4-8 loss: 0.855462  [  288/  306]
train() client id: f_00004-5-0 loss: 0.928634  [   32/  306]
train() client id: f_00004-5-1 loss: 0.818053  [   64/  306]
train() client id: f_00004-5-2 loss: 0.839319  [   96/  306]
train() client id: f_00004-5-3 loss: 0.870438  [  128/  306]
train() client id: f_00004-5-4 loss: 0.721483  [  160/  306]
train() client id: f_00004-5-5 loss: 0.972084  [  192/  306]
train() client id: f_00004-5-6 loss: 1.015209  [  224/  306]
train() client id: f_00004-5-7 loss: 0.829662  [  256/  306]
train() client id: f_00004-5-8 loss: 0.760604  [  288/  306]
train() client id: f_00004-6-0 loss: 0.800718  [   32/  306]
train() client id: f_00004-6-1 loss: 0.857188  [   64/  306]
train() client id: f_00004-6-2 loss: 0.886991  [   96/  306]
train() client id: f_00004-6-3 loss: 0.974029  [  128/  306]
train() client id: f_00004-6-4 loss: 0.802914  [  160/  306]
train() client id: f_00004-6-5 loss: 0.729534  [  192/  306]
train() client id: f_00004-6-6 loss: 0.928554  [  224/  306]
train() client id: f_00004-6-7 loss: 0.780879  [  256/  306]
train() client id: f_00004-6-8 loss: 0.920850  [  288/  306]
train() client id: f_00004-7-0 loss: 0.715866  [   32/  306]
train() client id: f_00004-7-1 loss: 0.959164  [   64/  306]
train() client id: f_00004-7-2 loss: 0.878471  [   96/  306]
train() client id: f_00004-7-3 loss: 0.744386  [  128/  306]
train() client id: f_00004-7-4 loss: 0.799391  [  160/  306]
train() client id: f_00004-7-5 loss: 0.988703  [  192/  306]
train() client id: f_00004-7-6 loss: 0.880391  [  224/  306]
train() client id: f_00004-7-7 loss: 0.835438  [  256/  306]
train() client id: f_00004-7-8 loss: 0.870093  [  288/  306]
train() client id: f_00004-8-0 loss: 0.947540  [   32/  306]
train() client id: f_00004-8-1 loss: 0.829752  [   64/  306]
train() client id: f_00004-8-2 loss: 0.878381  [   96/  306]
train() client id: f_00004-8-3 loss: 0.854883  [  128/  306]
train() client id: f_00004-8-4 loss: 0.740426  [  160/  306]
train() client id: f_00004-8-5 loss: 0.830319  [  192/  306]
train() client id: f_00004-8-6 loss: 0.898599  [  224/  306]
train() client id: f_00004-8-7 loss: 0.893221  [  256/  306]
train() client id: f_00004-8-8 loss: 0.801037  [  288/  306]
train() client id: f_00004-9-0 loss: 0.855417  [   32/  306]
train() client id: f_00004-9-1 loss: 0.859766  [   64/  306]
train() client id: f_00004-9-2 loss: 0.773681  [   96/  306]
train() client id: f_00004-9-3 loss: 0.834881  [  128/  306]
train() client id: f_00004-9-4 loss: 0.838762  [  160/  306]
train() client id: f_00004-9-5 loss: 0.981959  [  192/  306]
train() client id: f_00004-9-6 loss: 0.809612  [  224/  306]
train() client id: f_00004-9-7 loss: 0.929126  [  256/  306]
train() client id: f_00004-9-8 loss: 0.814341  [  288/  306]
train() client id: f_00004-10-0 loss: 0.862983  [   32/  306]
train() client id: f_00004-10-1 loss: 0.951445  [   64/  306]
train() client id: f_00004-10-2 loss: 0.857670  [   96/  306]
train() client id: f_00004-10-3 loss: 0.829529  [  128/  306]
train() client id: f_00004-10-4 loss: 0.842057  [  160/  306]
train() client id: f_00004-10-5 loss: 0.807925  [  192/  306]
train() client id: f_00004-10-6 loss: 0.802201  [  224/  306]
train() client id: f_00004-10-7 loss: 0.970438  [  256/  306]
train() client id: f_00004-10-8 loss: 0.751575  [  288/  306]
train() client id: f_00004-11-0 loss: 0.910037  [   32/  306]
train() client id: f_00004-11-1 loss: 0.811498  [   64/  306]
train() client id: f_00004-11-2 loss: 0.812193  [   96/  306]
train() client id: f_00004-11-3 loss: 0.851537  [  128/  306]
train() client id: f_00004-11-4 loss: 0.978102  [  160/  306]
train() client id: f_00004-11-5 loss: 0.826911  [  192/  306]
train() client id: f_00004-11-6 loss: 0.836284  [  224/  306]
train() client id: f_00004-11-7 loss: 0.853113  [  256/  306]
train() client id: f_00004-11-8 loss: 0.809740  [  288/  306]
train() client id: f_00004-12-0 loss: 0.918830  [   32/  306]
train() client id: f_00004-12-1 loss: 0.775490  [   64/  306]
train() client id: f_00004-12-2 loss: 0.841527  [   96/  306]
train() client id: f_00004-12-3 loss: 0.808909  [  128/  306]
train() client id: f_00004-12-4 loss: 0.945900  [  160/  306]
train() client id: f_00004-12-5 loss: 0.790151  [  192/  306]
train() client id: f_00004-12-6 loss: 0.879190  [  224/  306]
train() client id: f_00004-12-7 loss: 0.840034  [  256/  306]
train() client id: f_00004-12-8 loss: 0.845017  [  288/  306]
train() client id: f_00004-13-0 loss: 0.838623  [   32/  306]
train() client id: f_00004-13-1 loss: 0.844943  [   64/  306]
train() client id: f_00004-13-2 loss: 0.848870  [   96/  306]
train() client id: f_00004-13-3 loss: 0.805546  [  128/  306]
train() client id: f_00004-13-4 loss: 0.835889  [  160/  306]
train() client id: f_00004-13-5 loss: 0.820282  [  192/  306]
train() client id: f_00004-13-6 loss: 0.716383  [  224/  306]
train() client id: f_00004-13-7 loss: 0.865608  [  256/  306]
train() client id: f_00004-13-8 loss: 0.959879  [  288/  306]
train() client id: f_00005-0-0 loss: 0.709409  [   32/  146]
train() client id: f_00005-0-1 loss: 0.805088  [   64/  146]
train() client id: f_00005-0-2 loss: 0.856730  [   96/  146]
train() client id: f_00005-0-3 loss: 1.047150  [  128/  146]
train() client id: f_00005-1-0 loss: 0.942041  [   32/  146]
train() client id: f_00005-1-1 loss: 0.871886  [   64/  146]
train() client id: f_00005-1-2 loss: 0.648392  [   96/  146]
train() client id: f_00005-1-3 loss: 0.922048  [  128/  146]
train() client id: f_00005-2-0 loss: 0.756164  [   32/  146]
train() client id: f_00005-2-1 loss: 0.850292  [   64/  146]
train() client id: f_00005-2-2 loss: 0.789046  [   96/  146]
train() client id: f_00005-2-3 loss: 0.830791  [  128/  146]
train() client id: f_00005-3-0 loss: 0.797945  [   32/  146]
train() client id: f_00005-3-1 loss: 0.779505  [   64/  146]
train() client id: f_00005-3-2 loss: 1.000216  [   96/  146]
train() client id: f_00005-3-3 loss: 0.675767  [  128/  146]
train() client id: f_00005-4-0 loss: 0.894938  [   32/  146]
train() client id: f_00005-4-1 loss: 0.839953  [   64/  146]
train() client id: f_00005-4-2 loss: 0.804281  [   96/  146]
train() client id: f_00005-4-3 loss: 0.691500  [  128/  146]
train() client id: f_00005-5-0 loss: 0.711683  [   32/  146]
train() client id: f_00005-5-1 loss: 0.790437  [   64/  146]
train() client id: f_00005-5-2 loss: 0.834502  [   96/  146]
train() client id: f_00005-5-3 loss: 1.006336  [  128/  146]
train() client id: f_00005-6-0 loss: 1.029579  [   32/  146]
train() client id: f_00005-6-1 loss: 0.914331  [   64/  146]
train() client id: f_00005-6-2 loss: 0.651338  [   96/  146]
train() client id: f_00005-6-3 loss: 0.828093  [  128/  146]
train() client id: f_00005-7-0 loss: 0.912412  [   32/  146]
train() client id: f_00005-7-1 loss: 0.724282  [   64/  146]
train() client id: f_00005-7-2 loss: 0.939450  [   96/  146]
train() client id: f_00005-7-3 loss: 0.758113  [  128/  146]
train() client id: f_00005-8-0 loss: 0.678229  [   32/  146]
train() client id: f_00005-8-1 loss: 0.777585  [   64/  146]
train() client id: f_00005-8-2 loss: 0.933375  [   96/  146]
train() client id: f_00005-8-3 loss: 0.922696  [  128/  146]
train() client id: f_00005-9-0 loss: 0.691915  [   32/  146]
train() client id: f_00005-9-1 loss: 0.737698  [   64/  146]
train() client id: f_00005-9-2 loss: 1.115472  [   96/  146]
train() client id: f_00005-9-3 loss: 0.690575  [  128/  146]
train() client id: f_00005-10-0 loss: 0.906459  [   32/  146]
train() client id: f_00005-10-1 loss: 0.881718  [   64/  146]
train() client id: f_00005-10-2 loss: 0.778006  [   96/  146]
train() client id: f_00005-10-3 loss: 0.692390  [  128/  146]
train() client id: f_00005-11-0 loss: 0.804212  [   32/  146]
train() client id: f_00005-11-1 loss: 0.808843  [   64/  146]
train() client id: f_00005-11-2 loss: 0.861141  [   96/  146]
train() client id: f_00005-11-3 loss: 0.725168  [  128/  146]
train() client id: f_00005-12-0 loss: 0.960592  [   32/  146]
train() client id: f_00005-12-1 loss: 0.826769  [   64/  146]
train() client id: f_00005-12-2 loss: 0.896984  [   96/  146]
train() client id: f_00005-12-3 loss: 0.695840  [  128/  146]
train() client id: f_00005-13-0 loss: 0.901635  [   32/  146]
train() client id: f_00005-13-1 loss: 0.958797  [   64/  146]
train() client id: f_00005-13-2 loss: 0.570000  [   96/  146]
train() client id: f_00005-13-3 loss: 0.909128  [  128/  146]
train() client id: f_00006-0-0 loss: 0.610301  [   32/   54]
train() client id: f_00006-1-0 loss: 0.645923  [   32/   54]
train() client id: f_00006-2-0 loss: 0.573963  [   32/   54]
train() client id: f_00006-3-0 loss: 0.604638  [   32/   54]
train() client id: f_00006-4-0 loss: 0.565482  [   32/   54]
train() client id: f_00006-5-0 loss: 0.563576  [   32/   54]
train() client id: f_00006-6-0 loss: 0.585531  [   32/   54]
train() client id: f_00006-7-0 loss: 0.570510  [   32/   54]
train() client id: f_00006-8-0 loss: 0.524084  [   32/   54]
train() client id: f_00006-9-0 loss: 0.577537  [   32/   54]
train() client id: f_00006-10-0 loss: 0.585457  [   32/   54]
train() client id: f_00006-11-0 loss: 0.532260  [   32/   54]
train() client id: f_00006-12-0 loss: 0.569291  [   32/   54]
train() client id: f_00006-13-0 loss: 0.570498  [   32/   54]
train() client id: f_00007-0-0 loss: 0.496024  [   32/  179]
train() client id: f_00007-0-1 loss: 0.659241  [   64/  179]
train() client id: f_00007-0-2 loss: 0.679747  [   96/  179]
train() client id: f_00007-0-3 loss: 0.549938  [  128/  179]
train() client id: f_00007-0-4 loss: 0.598793  [  160/  179]
train() client id: f_00007-1-0 loss: 0.677542  [   32/  179]
train() client id: f_00007-1-1 loss: 0.565662  [   64/  179]
train() client id: f_00007-1-2 loss: 0.530610  [   96/  179]
train() client id: f_00007-1-3 loss: 0.637091  [  128/  179]
train() client id: f_00007-1-4 loss: 0.487899  [  160/  179]
train() client id: f_00007-2-0 loss: 0.548191  [   32/  179]
train() client id: f_00007-2-1 loss: 0.561936  [   64/  179]
train() client id: f_00007-2-2 loss: 0.595922  [   96/  179]
train() client id: f_00007-2-3 loss: 0.528665  [  128/  179]
train() client id: f_00007-2-4 loss: 0.558952  [  160/  179]
train() client id: f_00007-3-0 loss: 0.600902  [   32/  179]
train() client id: f_00007-3-1 loss: 0.417106  [   64/  179]
train() client id: f_00007-3-2 loss: 0.424173  [   96/  179]
train() client id: f_00007-3-3 loss: 0.508890  [  128/  179]
train() client id: f_00007-3-4 loss: 0.867556  [  160/  179]
train() client id: f_00007-4-0 loss: 0.532297  [   32/  179]
train() client id: f_00007-4-1 loss: 0.486901  [   64/  179]
train() client id: f_00007-4-2 loss: 0.595381  [   96/  179]
train() client id: f_00007-4-3 loss: 0.432267  [  128/  179]
train() client id: f_00007-4-4 loss: 0.729842  [  160/  179]
train() client id: f_00007-5-0 loss: 0.663463  [   32/  179]
train() client id: f_00007-5-1 loss: 0.488344  [   64/  179]
train() client id: f_00007-5-2 loss: 0.432614  [   96/  179]
train() client id: f_00007-5-3 loss: 0.464268  [  128/  179]
train() client id: f_00007-5-4 loss: 0.676725  [  160/  179]
train() client id: f_00007-6-0 loss: 0.457227  [   32/  179]
train() client id: f_00007-6-1 loss: 0.673969  [   64/  179]
train() client id: f_00007-6-2 loss: 0.521319  [   96/  179]
train() client id: f_00007-6-3 loss: 0.614364  [  128/  179]
train() client id: f_00007-6-4 loss: 0.389128  [  160/  179]
train() client id: f_00007-7-0 loss: 0.329851  [   32/  179]
train() client id: f_00007-7-1 loss: 0.697024  [   64/  179]
train() client id: f_00007-7-2 loss: 0.525103  [   96/  179]
train() client id: f_00007-7-3 loss: 0.669595  [  128/  179]
train() client id: f_00007-7-4 loss: 0.464205  [  160/  179]
train() client id: f_00007-8-0 loss: 0.396197  [   32/  179]
train() client id: f_00007-8-1 loss: 0.564689  [   64/  179]
train() client id: f_00007-8-2 loss: 0.724490  [   96/  179]
train() client id: f_00007-8-3 loss: 0.376244  [  128/  179]
train() client id: f_00007-8-4 loss: 0.573525  [  160/  179]
train() client id: f_00007-9-0 loss: 0.373906  [   32/  179]
train() client id: f_00007-9-1 loss: 0.476314  [   64/  179]
train() client id: f_00007-9-2 loss: 0.459067  [   96/  179]
train() client id: f_00007-9-3 loss: 0.540773  [  128/  179]
train() client id: f_00007-9-4 loss: 0.742943  [  160/  179]
train() client id: f_00007-10-0 loss: 0.465654  [   32/  179]
train() client id: f_00007-10-1 loss: 0.452927  [   64/  179]
train() client id: f_00007-10-2 loss: 0.538427  [   96/  179]
train() client id: f_00007-10-3 loss: 0.613398  [  128/  179]
train() client id: f_00007-10-4 loss: 0.466453  [  160/  179]
train() client id: f_00007-11-0 loss: 0.628684  [   32/  179]
train() client id: f_00007-11-1 loss: 0.426627  [   64/  179]
train() client id: f_00007-11-2 loss: 0.493843  [   96/  179]
train() client id: f_00007-11-3 loss: 0.549342  [  128/  179]
train() client id: f_00007-11-4 loss: 0.538390  [  160/  179]
train() client id: f_00007-12-0 loss: 0.514295  [   32/  179]
train() client id: f_00007-12-1 loss: 0.623245  [   64/  179]
train() client id: f_00007-12-2 loss: 0.399160  [   96/  179]
train() client id: f_00007-12-3 loss: 0.468793  [  128/  179]
train() client id: f_00007-12-4 loss: 0.461962  [  160/  179]
train() client id: f_00007-13-0 loss: 0.400073  [   32/  179]
train() client id: f_00007-13-1 loss: 0.369108  [   64/  179]
train() client id: f_00007-13-2 loss: 0.612427  [   96/  179]
train() client id: f_00007-13-3 loss: 0.536265  [  128/  179]
train() client id: f_00007-13-4 loss: 0.609764  [  160/  179]
train() client id: f_00008-0-0 loss: 0.654116  [   32/  130]
train() client id: f_00008-0-1 loss: 0.686671  [   64/  130]
train() client id: f_00008-0-2 loss: 0.783313  [   96/  130]
train() client id: f_00008-0-3 loss: 0.740125  [  128/  130]
train() client id: f_00008-1-0 loss: 0.652873  [   32/  130]
train() client id: f_00008-1-1 loss: 0.803584  [   64/  130]
train() client id: f_00008-1-2 loss: 0.767869  [   96/  130]
train() client id: f_00008-1-3 loss: 0.638605  [  128/  130]
train() client id: f_00008-2-0 loss: 0.718855  [   32/  130]
train() client id: f_00008-2-1 loss: 0.665439  [   64/  130]
train() client id: f_00008-2-2 loss: 0.644257  [   96/  130]
train() client id: f_00008-2-3 loss: 0.802130  [  128/  130]
train() client id: f_00008-3-0 loss: 0.698805  [   32/  130]
train() client id: f_00008-3-1 loss: 0.616938  [   64/  130]
train() client id: f_00008-3-2 loss: 0.780973  [   96/  130]
train() client id: f_00008-3-3 loss: 0.764924  [  128/  130]
train() client id: f_00008-4-0 loss: 0.728502  [   32/  130]
train() client id: f_00008-4-1 loss: 0.677734  [   64/  130]
train() client id: f_00008-4-2 loss: 0.671679  [   96/  130]
train() client id: f_00008-4-3 loss: 0.791907  [  128/  130]
train() client id: f_00008-5-0 loss: 0.786468  [   32/  130]
train() client id: f_00008-5-1 loss: 0.704846  [   64/  130]
train() client id: f_00008-5-2 loss: 0.701620  [   96/  130]
train() client id: f_00008-5-3 loss: 0.662023  [  128/  130]
train() client id: f_00008-6-0 loss: 0.736514  [   32/  130]
train() client id: f_00008-6-1 loss: 0.616336  [   64/  130]
train() client id: f_00008-6-2 loss: 0.705117  [   96/  130]
train() client id: f_00008-6-3 loss: 0.745816  [  128/  130]
train() client id: f_00008-7-0 loss: 0.789057  [   32/  130]
train() client id: f_00008-7-1 loss: 0.621226  [   64/  130]
train() client id: f_00008-7-2 loss: 0.742119  [   96/  130]
train() client id: f_00008-7-3 loss: 0.691125  [  128/  130]
train() client id: f_00008-8-0 loss: 0.689285  [   32/  130]
train() client id: f_00008-8-1 loss: 0.773272  [   64/  130]
train() client id: f_00008-8-2 loss: 0.720468  [   96/  130]
train() client id: f_00008-8-3 loss: 0.680256  [  128/  130]
train() client id: f_00008-9-0 loss: 0.797658  [   32/  130]
train() client id: f_00008-9-1 loss: 0.724450  [   64/  130]
train() client id: f_00008-9-2 loss: 0.621554  [   96/  130]
train() client id: f_00008-9-3 loss: 0.683172  [  128/  130]
train() client id: f_00008-10-0 loss: 0.719541  [   32/  130]
train() client id: f_00008-10-1 loss: 0.848939  [   64/  130]
train() client id: f_00008-10-2 loss: 0.548429  [   96/  130]
train() client id: f_00008-10-3 loss: 0.728566  [  128/  130]
train() client id: f_00008-11-0 loss: 0.707041  [   32/  130]
train() client id: f_00008-11-1 loss: 0.743459  [   64/  130]
train() client id: f_00008-11-2 loss: 0.690119  [   96/  130]
train() client id: f_00008-11-3 loss: 0.717408  [  128/  130]
train() client id: f_00008-12-0 loss: 0.642794  [   32/  130]
train() client id: f_00008-12-1 loss: 0.741842  [   64/  130]
train() client id: f_00008-12-2 loss: 0.804878  [   96/  130]
train() client id: f_00008-12-3 loss: 0.668211  [  128/  130]
train() client id: f_00008-13-0 loss: 0.791313  [   32/  130]
train() client id: f_00008-13-1 loss: 0.686344  [   64/  130]
train() client id: f_00008-13-2 loss: 0.679299  [   96/  130]
train() client id: f_00008-13-3 loss: 0.669404  [  128/  130]
train() client id: f_00009-0-0 loss: 1.203479  [   32/  118]
train() client id: f_00009-0-1 loss: 1.183960  [   64/  118]
train() client id: f_00009-0-2 loss: 1.276424  [   96/  118]
train() client id: f_00009-1-0 loss: 1.178244  [   32/  118]
train() client id: f_00009-1-1 loss: 1.210258  [   64/  118]
train() client id: f_00009-1-2 loss: 1.055191  [   96/  118]
train() client id: f_00009-2-0 loss: 1.310191  [   32/  118]
train() client id: f_00009-2-1 loss: 1.086085  [   64/  118]
train() client id: f_00009-2-2 loss: 1.013641  [   96/  118]
train() client id: f_00009-3-0 loss: 1.145448  [   32/  118]
train() client id: f_00009-3-1 loss: 1.150991  [   64/  118]
train() client id: f_00009-3-2 loss: 1.034134  [   96/  118]
train() client id: f_00009-4-0 loss: 1.081831  [   32/  118]
train() client id: f_00009-4-1 loss: 1.091810  [   64/  118]
train() client id: f_00009-4-2 loss: 0.981787  [   96/  118]
train() client id: f_00009-5-0 loss: 1.009419  [   32/  118]
train() client id: f_00009-5-1 loss: 1.008074  [   64/  118]
train() client id: f_00009-5-2 loss: 0.992790  [   96/  118]
train() client id: f_00009-6-0 loss: 1.019509  [   32/  118]
train() client id: f_00009-6-1 loss: 0.877694  [   64/  118]
train() client id: f_00009-6-2 loss: 1.041852  [   96/  118]
train() client id: f_00009-7-0 loss: 1.103943  [   32/  118]
train() client id: f_00009-7-1 loss: 0.913486  [   64/  118]
train() client id: f_00009-7-2 loss: 0.945074  [   96/  118]
train() client id: f_00009-8-0 loss: 1.064795  [   32/  118]
train() client id: f_00009-8-1 loss: 0.913556  [   64/  118]
train() client id: f_00009-8-2 loss: 0.966968  [   96/  118]
train() client id: f_00009-9-0 loss: 0.895258  [   32/  118]
train() client id: f_00009-9-1 loss: 0.921991  [   64/  118]
train() client id: f_00009-9-2 loss: 0.998759  [   96/  118]
train() client id: f_00009-10-0 loss: 0.854766  [   32/  118]
train() client id: f_00009-10-1 loss: 1.048034  [   64/  118]
train() client id: f_00009-10-2 loss: 0.886469  [   96/  118]
train() client id: f_00009-11-0 loss: 1.049349  [   32/  118]
train() client id: f_00009-11-1 loss: 0.829726  [   64/  118]
train() client id: f_00009-11-2 loss: 1.019121  [   96/  118]
train() client id: f_00009-12-0 loss: 0.906375  [   32/  118]
train() client id: f_00009-12-1 loss: 0.942998  [   64/  118]
train() client id: f_00009-12-2 loss: 0.998114  [   96/  118]
train() client id: f_00009-13-0 loss: 0.918498  [   32/  118]
train() client id: f_00009-13-1 loss: 0.965166  [   64/  118]
train() client id: f_00009-13-2 loss: 1.004588  [   96/  118]
At round 24 accuracy: 0.6445623342175066
At round 24 training accuracy: 0.5828303152246814
At round 24 training loss: 0.8379808483536634
update_location
xs = 8.927491 241.223621 5.882650 10.934260 -157.581990 -5.230757 -5.849135 -5.143845 -180.120581 20.134486 
ys = -232.390647 7.291448 130.684448 -22.290817 -9.642386 0.794442 -31.381692 126.628436 25.881276 -667.232496 
xs mean: -6.682379970521214
ys mean: -67.16579882624052
dists_uav = 253.150376 261.231699 164.660349 103.036103 186.882474 100.139862 104.971534 161.434878 207.637338 674.984889 
uav_gains = -111.566259 -112.278013 -105.429955 -100.324760 -106.860969 -100.015191 -100.526821 -105.211771 -108.168204 -127.679073 
uav_gains_db_mean: -107.80610152080371
dists_bs = 446.985975 448.740750 186.232116 271.100010 185.462120 243.247142 266.843434 176.609517 149.206616 864.542101 
bs_gains = -113.775288 -113.822933 -103.128587 -107.694686 -103.078205 -106.376396 -107.502242 -102.483454 -100.433125 -121.797071 
bs_gains_db_mean: -108.00919888574239
Round 25
-------------------------------
ene_coms = [0.01195042 0.01261846 0.00724096 0.00638852 0.00722368 0.00630639
 0.00644327 0.0070256  0.00641777 0.03569343]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.76748233 16.08835304  7.5546202   2.69658903 18.43189675  8.85097673
  3.35511261 10.83754496  7.89690966  7.75986352]
obj_prev = 91.23934883191376
eta_min = 8.579330228777061e-13	eta_max = 0.7524779463161836
af = 19.15709316343986	bf = 2.1052810571251035	zeta = 21.07280247978385	eta = 0.909090909090909
af = 19.15709316343986	bf = 2.1052810571251035	zeta = 43.2426175616683	eta = 0.44301418932653486
af = 19.15709316343986	bf = 2.1052810571251035	zeta = 31.78194986565751	eta = 0.6027664521659938
af = 19.15709316343986	bf = 2.1052810571251035	zeta = 29.71322426516981	eta = 0.6447328971260796
af = 19.15709316343986	bf = 2.1052810571251035	zeta = 29.59377952356495	eta = 0.6473351316341814
af = 19.15709316343986	bf = 2.1052810571251035	zeta = 29.593339437349393	eta = 0.6473447582350887
af = 19.15709316343986	bf = 2.1052810571251035	zeta = 29.593339431342798	eta = 0.6473447583664811
eta = 0.6473447583664811
ene_coms = [0.01195042 0.01261846 0.00724096 0.00638852 0.00722368 0.00630639
 0.00644327 0.0070256  0.00641777 0.03569343]
ene_comp = [0.03588495 0.07547231 0.03531535 0.01224645 0.08714916 0.04158097
 0.01537926 0.05097941 0.03702415 0.03360654]
ene_total = [2.66118256 4.90067554 2.3674975  1.03670277 5.25016057 2.66407537
 1.21403386 3.22694158 2.41676594 3.85530375]
ti_comp = [0.39686314 0.39018278 0.44395768 0.45248214 0.44413052 0.45330339
 0.4519346  0.44611128 0.45218964 0.15943303]
ti_coms = [0.11950419 0.12618456 0.07240965 0.0638852  0.07223681 0.06306394
 0.06443274 0.07025605 0.06417769 0.3569343 ]
t_total = [28.72660637 28.72660637 28.72660637 28.72660637 28.72660637 28.72660637
 28.72660637 28.72660637 28.72660637 28.72660637]
ene_coms = [0.01195042 0.01261846 0.00724096 0.00638852 0.00722368 0.00630639
 0.00644327 0.0070256  0.00641777 0.03569343]
ene_comp = [1.83373014e-05 1.76484577e-04 1.39664885e-05 5.60670641e-07
 2.09724114e-04 2.18668367e-05 1.11310320e-06 4.16081114e-05
 1.55129420e-05 9.33243008e-05]
ene_total = [0.6658472  0.71180957 0.40360717 0.35543804 0.41353606 0.35205455
 0.35851486 0.393164   0.3578971  1.99089281]
optimize_network iter = 0 obj = 6.002761361704724
eta = 0.6473447583664811
freqs = [4.52107332e+07 9.67140463e+07 3.97733235e+07 1.35325235e+07
 9.81121025e+07 4.58643945e+07 1.70149213e+07 5.71375477e+07
 4.09387445e+07 1.05393901e+08]
eta_min = 0.6473447583664828	eta_max = 0.6473447583664791
af = 0.026729904960422775	bf = 2.1052810571251035	zeta = 0.029402895456465055	eta = 0.909090909090909
af = 0.026729904960422775	bf = 2.1052810571251035	zeta = 23.186115580903337	eta = 0.0011528410124220286
af = 0.026729904960422775	bf = 2.1052810571251035	zeta = 2.2887615005687802	eta = 0.011678763800326127
af = 0.026729904960422775	bf = 2.1052810571251035	zeta = 2.2505118803791975	eta = 0.011877255656130528
af = 0.026729904960422775	bf = 2.1052810571251035	zeta = 2.25050806021237	eta = 0.011877275817399382
eta = 0.011877275817399382
ene_coms = [0.01195042 0.01261846 0.00724096 0.00638852 0.00722368 0.00630639
 0.00644327 0.0070256  0.00641777 0.03569343]
ene_comp = [1.86930434e-04 1.79908362e-03 1.42374372e-04 5.71547601e-06
 2.13792743e-03 2.22910514e-04 1.13469730e-05 4.24153050e-04
 1.58138917e-04 9.51347838e-04]
ene_total = [0.24098437 0.28625703 0.14659456 0.12695611 0.18587266 0.12963789
 0.12815506 0.14791329 0.1305632  0.72757391]
ti_comp = [0.39686314 0.39018278 0.44395768 0.45248214 0.44413052 0.45330339
 0.4519346  0.44611128 0.45218964 0.15943303]
ti_coms = [0.11950419 0.12618456 0.07240965 0.0638852  0.07223681 0.06306394
 0.06443274 0.07025605 0.06417769 0.3569343 ]
t_total = [28.72660637 28.72660637 28.72660637 28.72660637 28.72660637 28.72660637
 28.72660637 28.72660637 28.72660637 28.72660637]
ene_coms = [0.01195042 0.01261846 0.00724096 0.00638852 0.00722368 0.00630639
 0.00644327 0.0070256  0.00641777 0.03569343]
ene_comp = [1.83373014e-05 1.76484577e-04 1.39664885e-05 5.60670641e-07
 2.09724114e-04 2.18668367e-05 1.11310320e-06 4.16081114e-05
 1.55129420e-05 9.33243008e-05]
ene_total = [0.6658472  0.71180957 0.40360717 0.35543804 0.41353606 0.35205455
 0.35851486 0.393164   0.3578971  1.99089281]
optimize_network iter = 1 obj = 6.002761361704754
eta = 0.6473447583664828
freqs = [4.52107332e+07 9.67140463e+07 3.97733235e+07 1.35325235e+07
 9.81121025e+07 4.58643945e+07 1.70149213e+07 5.71375477e+07
 4.09387445e+07 1.05393901e+08]
Done!
ene_coms = [0.01195042 0.01261846 0.00724096 0.00638852 0.00722368 0.00630639
 0.00644327 0.0070256  0.00641777 0.03569343]
ene_comp = [1.80281717e-05 1.73509405e-04 1.37310419e-05 5.51218871e-07
 2.06188591e-04 2.14982062e-05 1.09433853e-06 4.09066830e-05
 1.52514253e-05 9.17510424e-05]
ene_total = [0.01196845 0.01279197 0.0072547  0.00638907 0.00742987 0.00632789
 0.00644437 0.00706651 0.00643302 0.03578518]
At round 25 energy consumption: 0.10789102256204058
At round 25 eta: 0.6473447583664828
At round 25 a_n: 19.61895668202144
At round 25 local rounds: 14.24005850222929
At round 25 global rounds: 55.63211421768587
gradient difference: 0.36743390560150146
train() client id: f_00000-0-0 loss: 1.106392  [   32/  126]
train() client id: f_00000-0-1 loss: 1.106863  [   64/  126]
train() client id: f_00000-0-2 loss: 1.338650  [   96/  126]
train() client id: f_00000-1-0 loss: 1.213351  [   32/  126]
train() client id: f_00000-1-1 loss: 0.974177  [   64/  126]
train() client id: f_00000-1-2 loss: 0.995771  [   96/  126]
train() client id: f_00000-2-0 loss: 0.980291  [   32/  126]
train() client id: f_00000-2-1 loss: 0.908916  [   64/  126]
train() client id: f_00000-2-2 loss: 0.933863  [   96/  126]
train() client id: f_00000-3-0 loss: 0.953971  [   32/  126]
train() client id: f_00000-3-1 loss: 1.000627  [   64/  126]
train() client id: f_00000-3-2 loss: 1.009906  [   96/  126]
train() client id: f_00000-4-0 loss: 1.114219  [   32/  126]
train() client id: f_00000-4-1 loss: 0.836254  [   64/  126]
train() client id: f_00000-4-2 loss: 0.844276  [   96/  126]
train() client id: f_00000-5-0 loss: 0.922873  [   32/  126]
train() client id: f_00000-5-1 loss: 0.861529  [   64/  126]
train() client id: f_00000-5-2 loss: 0.902201  [   96/  126]
train() client id: f_00000-6-0 loss: 0.923366  [   32/  126]
train() client id: f_00000-6-1 loss: 0.745422  [   64/  126]
train() client id: f_00000-6-2 loss: 0.854259  [   96/  126]
train() client id: f_00000-7-0 loss: 0.868990  [   32/  126]
train() client id: f_00000-7-1 loss: 0.707257  [   64/  126]
train() client id: f_00000-7-2 loss: 0.766280  [   96/  126]
train() client id: f_00000-8-0 loss: 0.891896  [   32/  126]
train() client id: f_00000-8-1 loss: 0.764335  [   64/  126]
train() client id: f_00000-8-2 loss: 0.781788  [   96/  126]
train() client id: f_00000-9-0 loss: 0.846412  [   32/  126]
train() client id: f_00000-9-1 loss: 0.816568  [   64/  126]
train() client id: f_00000-9-2 loss: 0.707779  [   96/  126]
train() client id: f_00000-10-0 loss: 0.807425  [   32/  126]
train() client id: f_00000-10-1 loss: 0.765557  [   64/  126]
train() client id: f_00000-10-2 loss: 0.823760  [   96/  126]
train() client id: f_00000-11-0 loss: 0.745855  [   32/  126]
train() client id: f_00000-11-1 loss: 0.770566  [   64/  126]
train() client id: f_00000-11-2 loss: 0.799960  [   96/  126]
train() client id: f_00000-12-0 loss: 0.831945  [   32/  126]
train() client id: f_00000-12-1 loss: 0.857631  [   64/  126]
train() client id: f_00000-12-2 loss: 0.840259  [   96/  126]
train() client id: f_00000-13-0 loss: 0.835808  [   32/  126]
train() client id: f_00000-13-1 loss: 0.826291  [   64/  126]
train() client id: f_00000-13-2 loss: 0.790747  [   96/  126]
train() client id: f_00001-0-0 loss: 0.567838  [   32/  265]
train() client id: f_00001-0-1 loss: 0.343876  [   64/  265]
train() client id: f_00001-0-2 loss: 0.443725  [   96/  265]
train() client id: f_00001-0-3 loss: 0.408065  [  128/  265]
train() client id: f_00001-0-4 loss: 0.421430  [  160/  265]
train() client id: f_00001-0-5 loss: 0.527631  [  192/  265]
train() client id: f_00001-0-6 loss: 0.374303  [  224/  265]
train() client id: f_00001-0-7 loss: 0.365284  [  256/  265]
train() client id: f_00001-1-0 loss: 0.404324  [   32/  265]
train() client id: f_00001-1-1 loss: 0.463604  [   64/  265]
train() client id: f_00001-1-2 loss: 0.423241  [   96/  265]
train() client id: f_00001-1-3 loss: 0.425956  [  128/  265]
train() client id: f_00001-1-4 loss: 0.371949  [  160/  265]
train() client id: f_00001-1-5 loss: 0.434550  [  192/  265]
train() client id: f_00001-1-6 loss: 0.413247  [  224/  265]
train() client id: f_00001-1-7 loss: 0.439674  [  256/  265]
train() client id: f_00001-2-0 loss: 0.364468  [   32/  265]
train() client id: f_00001-2-1 loss: 0.457250  [   64/  265]
train() client id: f_00001-2-2 loss: 0.348346  [   96/  265]
train() client id: f_00001-2-3 loss: 0.416666  [  128/  265]
train() client id: f_00001-2-4 loss: 0.437915  [  160/  265]
train() client id: f_00001-2-5 loss: 0.391717  [  192/  265]
train() client id: f_00001-2-6 loss: 0.517618  [  224/  265]
train() client id: f_00001-2-7 loss: 0.397037  [  256/  265]
train() client id: f_00001-3-0 loss: 0.386722  [   32/  265]
train() client id: f_00001-3-1 loss: 0.392743  [   64/  265]
train() client id: f_00001-3-2 loss: 0.485678  [   96/  265]
train() client id: f_00001-3-3 loss: 0.423496  [  128/  265]
train() client id: f_00001-3-4 loss: 0.409612  [  160/  265]
train() client id: f_00001-3-5 loss: 0.459816  [  192/  265]
train() client id: f_00001-3-6 loss: 0.328646  [  224/  265]
train() client id: f_00001-3-7 loss: 0.322247  [  256/  265]
train() client id: f_00001-4-0 loss: 0.332349  [   32/  265]
train() client id: f_00001-4-1 loss: 0.463716  [   64/  265]
train() client id: f_00001-4-2 loss: 0.376642  [   96/  265]
train() client id: f_00001-4-3 loss: 0.357283  [  128/  265]
train() client id: f_00001-4-4 loss: 0.392145  [  160/  265]
train() client id: f_00001-4-5 loss: 0.553294  [  192/  265]
train() client id: f_00001-4-6 loss: 0.305969  [  224/  265]
train() client id: f_00001-4-7 loss: 0.367854  [  256/  265]
train() client id: f_00001-5-0 loss: 0.330938  [   32/  265]
train() client id: f_00001-5-1 loss: 0.460909  [   64/  265]
train() client id: f_00001-5-2 loss: 0.354671  [   96/  265]
train() client id: f_00001-5-3 loss: 0.344415  [  128/  265]
train() client id: f_00001-5-4 loss: 0.324918  [  160/  265]
train() client id: f_00001-5-5 loss: 0.369338  [  192/  265]
train() client id: f_00001-5-6 loss: 0.480872  [  224/  265]
train() client id: f_00001-5-7 loss: 0.569557  [  256/  265]
train() client id: f_00001-6-0 loss: 0.439491  [   32/  265]
train() client id: f_00001-6-1 loss: 0.296600  [   64/  265]
train() client id: f_00001-6-2 loss: 0.395205  [   96/  265]
train() client id: f_00001-6-3 loss: 0.454746  [  128/  265]
train() client id: f_00001-6-4 loss: 0.431724  [  160/  265]
train() client id: f_00001-6-5 loss: 0.565297  [  192/  265]
train() client id: f_00001-6-6 loss: 0.315388  [  224/  265]
train() client id: f_00001-6-7 loss: 0.317976  [  256/  265]
train() client id: f_00001-7-0 loss: 0.359849  [   32/  265]
train() client id: f_00001-7-1 loss: 0.400133  [   64/  265]
train() client id: f_00001-7-2 loss: 0.344564  [   96/  265]
train() client id: f_00001-7-3 loss: 0.438772  [  128/  265]
train() client id: f_00001-7-4 loss: 0.551740  [  160/  265]
train() client id: f_00001-7-5 loss: 0.300297  [  192/  265]
train() client id: f_00001-7-6 loss: 0.437120  [  224/  265]
train() client id: f_00001-7-7 loss: 0.377660  [  256/  265]
train() client id: f_00001-8-0 loss: 0.474289  [   32/  265]
train() client id: f_00001-8-1 loss: 0.444113  [   64/  265]
train() client id: f_00001-8-2 loss: 0.442249  [   96/  265]
train() client id: f_00001-8-3 loss: 0.314004  [  128/  265]
train() client id: f_00001-8-4 loss: 0.478310  [  160/  265]
train() client id: f_00001-8-5 loss: 0.298893  [  192/  265]
train() client id: f_00001-8-6 loss: 0.346665  [  224/  265]
train() client id: f_00001-8-7 loss: 0.360079  [  256/  265]
train() client id: f_00001-9-0 loss: 0.452476  [   32/  265]
train() client id: f_00001-9-1 loss: 0.548231  [   64/  265]
train() client id: f_00001-9-2 loss: 0.390397  [   96/  265]
train() client id: f_00001-9-3 loss: 0.363027  [  128/  265]
train() client id: f_00001-9-4 loss: 0.293840  [  160/  265]
train() client id: f_00001-9-5 loss: 0.329067  [  192/  265]
train() client id: f_00001-9-6 loss: 0.484948  [  224/  265]
train() client id: f_00001-9-7 loss: 0.294745  [  256/  265]
train() client id: f_00001-10-0 loss: 0.475701  [   32/  265]
train() client id: f_00001-10-1 loss: 0.447379  [   64/  265]
train() client id: f_00001-10-2 loss: 0.306473  [   96/  265]
train() client id: f_00001-10-3 loss: 0.330706  [  128/  265]
train() client id: f_00001-10-4 loss: 0.328302  [  160/  265]
train() client id: f_00001-10-5 loss: 0.478429  [  192/  265]
train() client id: f_00001-10-6 loss: 0.447624  [  224/  265]
train() client id: f_00001-10-7 loss: 0.381984  [  256/  265]
train() client id: f_00001-11-0 loss: 0.423205  [   32/  265]
train() client id: f_00001-11-1 loss: 0.348068  [   64/  265]
train() client id: f_00001-11-2 loss: 0.387186  [   96/  265]
train() client id: f_00001-11-3 loss: 0.470247  [  128/  265]
train() client id: f_00001-11-4 loss: 0.465076  [  160/  265]
train() client id: f_00001-11-5 loss: 0.362211  [  192/  265]
train() client id: f_00001-11-6 loss: 0.444823  [  224/  265]
train() client id: f_00001-11-7 loss: 0.295666  [  256/  265]
train() client id: f_00001-12-0 loss: 0.489666  [   32/  265]
train() client id: f_00001-12-1 loss: 0.354723  [   64/  265]
train() client id: f_00001-12-2 loss: 0.440422  [   96/  265]
train() client id: f_00001-12-3 loss: 0.310826  [  128/  265]
train() client id: f_00001-12-4 loss: 0.401908  [  160/  265]
train() client id: f_00001-12-5 loss: 0.373018  [  192/  265]
train() client id: f_00001-12-6 loss: 0.345636  [  224/  265]
train() client id: f_00001-12-7 loss: 0.382087  [  256/  265]
train() client id: f_00001-13-0 loss: 0.364138  [   32/  265]
train() client id: f_00001-13-1 loss: 0.441425  [   64/  265]
train() client id: f_00001-13-2 loss: 0.407825  [   96/  265]
train() client id: f_00001-13-3 loss: 0.370150  [  128/  265]
train() client id: f_00001-13-4 loss: 0.308530  [  160/  265]
train() client id: f_00001-13-5 loss: 0.565770  [  192/  265]
train() client id: f_00001-13-6 loss: 0.435281  [  224/  265]
train() client id: f_00001-13-7 loss: 0.300396  [  256/  265]
train() client id: f_00002-0-0 loss: 1.390798  [   32/  124]
train() client id: f_00002-0-1 loss: 1.142305  [   64/  124]
train() client id: f_00002-0-2 loss: 1.398044  [   96/  124]
train() client id: f_00002-1-0 loss: 1.446767  [   32/  124]
train() client id: f_00002-1-1 loss: 1.320063  [   64/  124]
train() client id: f_00002-1-2 loss: 1.096127  [   96/  124]
train() client id: f_00002-2-0 loss: 1.187109  [   32/  124]
train() client id: f_00002-2-1 loss: 1.302819  [   64/  124]
train() client id: f_00002-2-2 loss: 1.239381  [   96/  124]
train() client id: f_00002-3-0 loss: 1.339674  [   32/  124]
train() client id: f_00002-3-1 loss: 1.160308  [   64/  124]
train() client id: f_00002-3-2 loss: 1.075711  [   96/  124]
train() client id: f_00002-4-0 loss: 1.095546  [   32/  124]
train() client id: f_00002-4-1 loss: 1.159795  [   64/  124]
train() client id: f_00002-4-2 loss: 1.242457  [   96/  124]
train() client id: f_00002-5-0 loss: 1.387631  [   32/  124]
train() client id: f_00002-5-1 loss: 1.022336  [   64/  124]
train() client id: f_00002-5-2 loss: 1.094199  [   96/  124]
train() client id: f_00002-6-0 loss: 1.059615  [   32/  124]
train() client id: f_00002-6-1 loss: 1.154073  [   64/  124]
train() client id: f_00002-6-2 loss: 1.119183  [   96/  124]
train() client id: f_00002-7-0 loss: 0.922687  [   32/  124]
train() client id: f_00002-7-1 loss: 1.053772  [   64/  124]
train() client id: f_00002-7-2 loss: 1.243221  [   96/  124]
train() client id: f_00002-8-0 loss: 1.194749  [   32/  124]
train() client id: f_00002-8-1 loss: 0.953006  [   64/  124]
train() client id: f_00002-8-2 loss: 1.009838  [   96/  124]
train() client id: f_00002-9-0 loss: 1.060403  [   32/  124]
train() client id: f_00002-9-1 loss: 1.081268  [   64/  124]
train() client id: f_00002-9-2 loss: 1.100282  [   96/  124]
train() client id: f_00002-10-0 loss: 1.025053  [   32/  124]
train() client id: f_00002-10-1 loss: 1.136824  [   64/  124]
train() client id: f_00002-10-2 loss: 1.062059  [   96/  124]
train() client id: f_00002-11-0 loss: 1.139802  [   32/  124]
train() client id: f_00002-11-1 loss: 0.995769  [   64/  124]
train() client id: f_00002-11-2 loss: 1.027816  [   96/  124]
train() client id: f_00002-12-0 loss: 0.939144  [   32/  124]
train() client id: f_00002-12-1 loss: 1.080053  [   64/  124]
train() client id: f_00002-12-2 loss: 1.094296  [   96/  124]
train() client id: f_00002-13-0 loss: 0.959070  [   32/  124]
train() client id: f_00002-13-1 loss: 1.158965  [   64/  124]
train() client id: f_00002-13-2 loss: 1.063757  [   96/  124]
train() client id: f_00003-0-0 loss: 0.709524  [   32/   43]
train() client id: f_00003-1-0 loss: 0.833042  [   32/   43]
train() client id: f_00003-2-0 loss: 0.709310  [   32/   43]
train() client id: f_00003-3-0 loss: 0.837248  [   32/   43]
train() client id: f_00003-4-0 loss: 0.683307  [   32/   43]
train() client id: f_00003-5-0 loss: 0.702433  [   32/   43]
train() client id: f_00003-6-0 loss: 0.725062  [   32/   43]
train() client id: f_00003-7-0 loss: 0.703771  [   32/   43]
train() client id: f_00003-8-0 loss: 0.832934  [   32/   43]
train() client id: f_00003-9-0 loss: 0.842278  [   32/   43]
train() client id: f_00003-10-0 loss: 0.785987  [   32/   43]
train() client id: f_00003-11-0 loss: 0.763694  [   32/   43]
train() client id: f_00003-12-0 loss: 0.709452  [   32/   43]
train() client id: f_00003-13-0 loss: 0.889266  [   32/   43]
train() client id: f_00004-0-0 loss: 0.592307  [   32/  306]
train() client id: f_00004-0-1 loss: 0.626659  [   64/  306]
train() client id: f_00004-0-2 loss: 0.564851  [   96/  306]
train() client id: f_00004-0-3 loss: 0.767744  [  128/  306]
train() client id: f_00004-0-4 loss: 0.513211  [  160/  306]
train() client id: f_00004-0-5 loss: 0.686730  [  192/  306]
train() client id: f_00004-0-6 loss: 0.786975  [  224/  306]
train() client id: f_00004-0-7 loss: 0.827409  [  256/  306]
train() client id: f_00004-0-8 loss: 0.843979  [  288/  306]
train() client id: f_00004-1-0 loss: 0.682884  [   32/  306]
train() client id: f_00004-1-1 loss: 0.734904  [   64/  306]
train() client id: f_00004-1-2 loss: 0.779030  [   96/  306]
train() client id: f_00004-1-3 loss: 0.689338  [  128/  306]
train() client id: f_00004-1-4 loss: 0.636057  [  160/  306]
train() client id: f_00004-1-5 loss: 0.694682  [  192/  306]
train() client id: f_00004-1-6 loss: 0.773228  [  224/  306]
train() client id: f_00004-1-7 loss: 0.786859  [  256/  306]
train() client id: f_00004-1-8 loss: 0.732052  [  288/  306]
train() client id: f_00004-2-0 loss: 0.617262  [   32/  306]
train() client id: f_00004-2-1 loss: 0.604044  [   64/  306]
train() client id: f_00004-2-2 loss: 0.736016  [   96/  306]
train() client id: f_00004-2-3 loss: 0.825200  [  128/  306]
train() client id: f_00004-2-4 loss: 0.664547  [  160/  306]
train() client id: f_00004-2-5 loss: 0.739659  [  192/  306]
train() client id: f_00004-2-6 loss: 0.750896  [  224/  306]
train() client id: f_00004-2-7 loss: 0.754174  [  256/  306]
train() client id: f_00004-2-8 loss: 0.684008  [  288/  306]
train() client id: f_00004-3-0 loss: 0.661380  [   32/  306]
train() client id: f_00004-3-1 loss: 0.663751  [   64/  306]
train() client id: f_00004-3-2 loss: 0.744294  [   96/  306]
train() client id: f_00004-3-3 loss: 0.701877  [  128/  306]
train() client id: f_00004-3-4 loss: 0.736216  [  160/  306]
train() client id: f_00004-3-5 loss: 0.536815  [  192/  306]
train() client id: f_00004-3-6 loss: 0.844753  [  224/  306]
train() client id: f_00004-3-7 loss: 0.732999  [  256/  306]
train() client id: f_00004-3-8 loss: 0.723355  [  288/  306]
train() client id: f_00004-4-0 loss: 0.813455  [   32/  306]
train() client id: f_00004-4-1 loss: 0.663041  [   64/  306]
train() client id: f_00004-4-2 loss: 0.666411  [   96/  306]
train() client id: f_00004-4-3 loss: 0.629063  [  128/  306]
train() client id: f_00004-4-4 loss: 0.647185  [  160/  306]
train() client id: f_00004-4-5 loss: 0.635467  [  192/  306]
train() client id: f_00004-4-6 loss: 0.831044  [  224/  306]
train() client id: f_00004-4-7 loss: 0.806282  [  256/  306]
train() client id: f_00004-4-8 loss: 0.641600  [  288/  306]
train() client id: f_00004-5-0 loss: 0.677680  [   32/  306]
train() client id: f_00004-5-1 loss: 0.711858  [   64/  306]
train() client id: f_00004-5-2 loss: 0.697471  [   96/  306]
train() client id: f_00004-5-3 loss: 0.717111  [  128/  306]
train() client id: f_00004-5-4 loss: 0.680627  [  160/  306]
train() client id: f_00004-5-5 loss: 0.756306  [  192/  306]
train() client id: f_00004-5-6 loss: 0.702861  [  224/  306]
train() client id: f_00004-5-7 loss: 0.783727  [  256/  306]
train() client id: f_00004-5-8 loss: 0.714363  [  288/  306]
train() client id: f_00004-6-0 loss: 0.794296  [   32/  306]
train() client id: f_00004-6-1 loss: 0.758571  [   64/  306]
train() client id: f_00004-6-2 loss: 0.772280  [   96/  306]
train() client id: f_00004-6-3 loss: 0.666870  [  128/  306]
train() client id: f_00004-6-4 loss: 0.679806  [  160/  306]
train() client id: f_00004-6-5 loss: 0.713002  [  192/  306]
train() client id: f_00004-6-6 loss: 0.688850  [  224/  306]
train() client id: f_00004-6-7 loss: 0.696516  [  256/  306]
train() client id: f_00004-6-8 loss: 0.760078  [  288/  306]
train() client id: f_00004-7-0 loss: 0.717988  [   32/  306]
train() client id: f_00004-7-1 loss: 0.724929  [   64/  306]
train() client id: f_00004-7-2 loss: 0.674117  [   96/  306]
train() client id: f_00004-7-3 loss: 0.734982  [  128/  306]
train() client id: f_00004-7-4 loss: 0.785621  [  160/  306]
train() client id: f_00004-7-5 loss: 0.762677  [  192/  306]
train() client id: f_00004-7-6 loss: 0.659337  [  224/  306]
train() client id: f_00004-7-7 loss: 0.898671  [  256/  306]
train() client id: f_00004-7-8 loss: 0.637277  [  288/  306]
train() client id: f_00004-8-0 loss: 0.815862  [   32/  306]
train() client id: f_00004-8-1 loss: 0.777569  [   64/  306]
train() client id: f_00004-8-2 loss: 0.716169  [   96/  306]
train() client id: f_00004-8-3 loss: 0.595272  [  128/  306]
train() client id: f_00004-8-4 loss: 0.728740  [  160/  306]
train() client id: f_00004-8-5 loss: 0.621575  [  192/  306]
train() client id: f_00004-8-6 loss: 0.710651  [  224/  306]
train() client id: f_00004-8-7 loss: 0.758370  [  256/  306]
train() client id: f_00004-8-8 loss: 0.799911  [  288/  306]
train() client id: f_00004-9-0 loss: 0.827158  [   32/  306]
train() client id: f_00004-9-1 loss: 0.765901  [   64/  306]
train() client id: f_00004-9-2 loss: 0.611857  [   96/  306]
train() client id: f_00004-9-3 loss: 0.680421  [  128/  306]
train() client id: f_00004-9-4 loss: 0.756214  [  160/  306]
train() client id: f_00004-9-5 loss: 0.829624  [  192/  306]
train() client id: f_00004-9-6 loss: 0.746205  [  224/  306]
train() client id: f_00004-9-7 loss: 0.640047  [  256/  306]
train() client id: f_00004-9-8 loss: 0.632601  [  288/  306]
train() client id: f_00004-10-0 loss: 0.697851  [   32/  306]
train() client id: f_00004-10-1 loss: 0.674390  [   64/  306]
train() client id: f_00004-10-2 loss: 0.800982  [   96/  306]
train() client id: f_00004-10-3 loss: 0.858179  [  128/  306]
train() client id: f_00004-10-4 loss: 0.677899  [  160/  306]
train() client id: f_00004-10-5 loss: 0.747775  [  192/  306]
train() client id: f_00004-10-6 loss: 0.709848  [  224/  306]
train() client id: f_00004-10-7 loss: 0.637657  [  256/  306]
train() client id: f_00004-10-8 loss: 0.656916  [  288/  306]
train() client id: f_00004-11-0 loss: 0.783901  [   32/  306]
train() client id: f_00004-11-1 loss: 0.763797  [   64/  306]
train() client id: f_00004-11-2 loss: 0.643135  [   96/  306]
train() client id: f_00004-11-3 loss: 0.687325  [  128/  306]
train() client id: f_00004-11-4 loss: 0.748466  [  160/  306]
train() client id: f_00004-11-5 loss: 0.727095  [  192/  306]
train() client id: f_00004-11-6 loss: 0.841394  [  224/  306]
train() client id: f_00004-11-7 loss: 0.703749  [  256/  306]
train() client id: f_00004-11-8 loss: 0.675289  [  288/  306]
train() client id: f_00004-12-0 loss: 0.675511  [   32/  306]
train() client id: f_00004-12-1 loss: 0.657263  [   64/  306]
train() client id: f_00004-12-2 loss: 0.788964  [   96/  306]
train() client id: f_00004-12-3 loss: 0.799097  [  128/  306]
train() client id: f_00004-12-4 loss: 0.734507  [  160/  306]
train() client id: f_00004-12-5 loss: 0.638409  [  192/  306]
train() client id: f_00004-12-6 loss: 0.791487  [  224/  306]
train() client id: f_00004-12-7 loss: 0.693102  [  256/  306]
train() client id: f_00004-12-8 loss: 0.741975  [  288/  306]
train() client id: f_00004-13-0 loss: 0.763300  [   32/  306]
train() client id: f_00004-13-1 loss: 0.822581  [   64/  306]
train() client id: f_00004-13-2 loss: 0.867464  [   96/  306]
train() client id: f_00004-13-3 loss: 0.657168  [  128/  306]
train() client id: f_00004-13-4 loss: 0.752225  [  160/  306]
train() client id: f_00004-13-5 loss: 0.680008  [  192/  306]
train() client id: f_00004-13-6 loss: 0.611845  [  224/  306]
train() client id: f_00004-13-7 loss: 0.750633  [  256/  306]
train() client id: f_00004-13-8 loss: 0.802706  [  288/  306]
train() client id: f_00005-0-0 loss: 0.541708  [   32/  146]
train() client id: f_00005-0-1 loss: 0.601387  [   64/  146]
train() client id: f_00005-0-2 loss: 0.539164  [   96/  146]
train() client id: f_00005-0-3 loss: 0.771409  [  128/  146]
train() client id: f_00005-1-0 loss: 0.758538  [   32/  146]
train() client id: f_00005-1-1 loss: 0.729521  [   64/  146]
train() client id: f_00005-1-2 loss: 0.369453  [   96/  146]
train() client id: f_00005-1-3 loss: 0.464842  [  128/  146]
train() client id: f_00005-2-0 loss: 0.479808  [   32/  146]
train() client id: f_00005-2-1 loss: 0.684326  [   64/  146]
train() client id: f_00005-2-2 loss: 0.473182  [   96/  146]
train() client id: f_00005-2-3 loss: 0.669432  [  128/  146]
train() client id: f_00005-3-0 loss: 0.806481  [   32/  146]
train() client id: f_00005-3-1 loss: 0.444242  [   64/  146]
train() client id: f_00005-3-2 loss: 0.773633  [   96/  146]
train() client id: f_00005-3-3 loss: 0.394576  [  128/  146]
train() client id: f_00005-4-0 loss: 0.569179  [   32/  146]
train() client id: f_00005-4-1 loss: 0.645296  [   64/  146]
train() client id: f_00005-4-2 loss: 0.477962  [   96/  146]
train() client id: f_00005-4-3 loss: 0.779156  [  128/  146]
train() client id: f_00005-5-0 loss: 0.590127  [   32/  146]
train() client id: f_00005-5-1 loss: 0.432636  [   64/  146]
train() client id: f_00005-5-2 loss: 0.687594  [   96/  146]
train() client id: f_00005-5-3 loss: 0.434470  [  128/  146]
train() client id: f_00005-6-0 loss: 0.653237  [   32/  146]
train() client id: f_00005-6-1 loss: 0.656856  [   64/  146]
train() client id: f_00005-6-2 loss: 0.492565  [   96/  146]
train() client id: f_00005-6-3 loss: 0.456797  [  128/  146]
train() client id: f_00005-7-0 loss: 0.579302  [   32/  146]
train() client id: f_00005-7-1 loss: 0.568541  [   64/  146]
train() client id: f_00005-7-2 loss: 0.433582  [   96/  146]
train() client id: f_00005-7-3 loss: 0.669371  [  128/  146]
train() client id: f_00005-8-0 loss: 0.370601  [   32/  146]
train() client id: f_00005-8-1 loss: 0.609121  [   64/  146]
train() client id: f_00005-8-2 loss: 0.625939  [   96/  146]
train() client id: f_00005-8-3 loss: 0.553951  [  128/  146]
train() client id: f_00005-9-0 loss: 0.622139  [   32/  146]
train() client id: f_00005-9-1 loss: 0.629410  [   64/  146]
train() client id: f_00005-9-2 loss: 0.600789  [   96/  146]
train() client id: f_00005-9-3 loss: 0.532383  [  128/  146]
train() client id: f_00005-10-0 loss: 0.655513  [   32/  146]
train() client id: f_00005-10-1 loss: 0.525233  [   64/  146]
train() client id: f_00005-10-2 loss: 0.398485  [   96/  146]
train() client id: f_00005-10-3 loss: 0.589797  [  128/  146]
train() client id: f_00005-11-0 loss: 0.300930  [   32/  146]
train() client id: f_00005-11-1 loss: 0.633696  [   64/  146]
train() client id: f_00005-11-2 loss: 0.572336  [   96/  146]
train() client id: f_00005-11-3 loss: 0.971388  [  128/  146]
train() client id: f_00005-12-0 loss: 0.442049  [   32/  146]
train() client id: f_00005-12-1 loss: 0.547586  [   64/  146]
train() client id: f_00005-12-2 loss: 0.679866  [   96/  146]
train() client id: f_00005-12-3 loss: 0.596390  [  128/  146]
train() client id: f_00005-13-0 loss: 0.612708  [   32/  146]
train() client id: f_00005-13-1 loss: 0.722861  [   64/  146]
train() client id: f_00005-13-2 loss: 0.360310  [   96/  146]
train() client id: f_00005-13-3 loss: 0.610629  [  128/  146]
train() client id: f_00006-0-0 loss: 0.534962  [   32/   54]
train() client id: f_00006-1-0 loss: 0.552040  [   32/   54]
train() client id: f_00006-2-0 loss: 0.583458  [   32/   54]
train() client id: f_00006-3-0 loss: 0.554247  [   32/   54]
train() client id: f_00006-4-0 loss: 0.541469  [   32/   54]
train() client id: f_00006-5-0 loss: 0.601465  [   32/   54]
train() client id: f_00006-6-0 loss: 0.587448  [   32/   54]
train() client id: f_00006-7-0 loss: 0.557885  [   32/   54]
train() client id: f_00006-8-0 loss: 0.546752  [   32/   54]
train() client id: f_00006-9-0 loss: 0.488891  [   32/   54]
train() client id: f_00006-10-0 loss: 0.594126  [   32/   54]
train() client id: f_00006-11-0 loss: 0.522798  [   32/   54]
train() client id: f_00006-12-0 loss: 0.538512  [   32/   54]
train() client id: f_00006-13-0 loss: 0.547358  [   32/   54]
train() client id: f_00007-0-0 loss: 0.449517  [   32/  179]
train() client id: f_00007-0-1 loss: 0.416226  [   64/  179]
train() client id: f_00007-0-2 loss: 0.532685  [   96/  179]
train() client id: f_00007-0-3 loss: 0.638895  [  128/  179]
train() client id: f_00007-0-4 loss: 0.467397  [  160/  179]
train() client id: f_00007-1-0 loss: 0.443920  [   32/  179]
train() client id: f_00007-1-1 loss: 0.475209  [   64/  179]
train() client id: f_00007-1-2 loss: 0.550108  [   96/  179]
train() client id: f_00007-1-3 loss: 0.449801  [  128/  179]
train() client id: f_00007-1-4 loss: 0.483319  [  160/  179]
train() client id: f_00007-2-0 loss: 0.385686  [   32/  179]
train() client id: f_00007-2-1 loss: 0.539113  [   64/  179]
train() client id: f_00007-2-2 loss: 0.458459  [   96/  179]
train() client id: f_00007-2-3 loss: 0.454328  [  128/  179]
train() client id: f_00007-2-4 loss: 0.519688  [  160/  179]
train() client id: f_00007-3-0 loss: 0.332503  [   32/  179]
train() client id: f_00007-3-1 loss: 0.582218  [   64/  179]
train() client id: f_00007-3-2 loss: 0.411983  [   96/  179]
train() client id: f_00007-3-3 loss: 0.559193  [  128/  179]
train() client id: f_00007-3-4 loss: 0.382147  [  160/  179]
train() client id: f_00007-4-0 loss: 0.409380  [   32/  179]
train() client id: f_00007-4-1 loss: 0.463504  [   64/  179]
train() client id: f_00007-4-2 loss: 0.326939  [   96/  179]
train() client id: f_00007-4-3 loss: 0.447163  [  128/  179]
train() client id: f_00007-4-4 loss: 0.565202  [  160/  179]
train() client id: f_00007-5-0 loss: 0.368834  [   32/  179]
train() client id: f_00007-5-1 loss: 0.393310  [   64/  179]
train() client id: f_00007-5-2 loss: 0.657301  [   96/  179]
train() client id: f_00007-5-3 loss: 0.481653  [  128/  179]
train() client id: f_00007-5-4 loss: 0.437768  [  160/  179]
train() client id: f_00007-6-0 loss: 0.324921  [   32/  179]
train() client id: f_00007-6-1 loss: 0.581284  [   64/  179]
train() client id: f_00007-6-2 loss: 0.406282  [   96/  179]
train() client id: f_00007-6-3 loss: 0.481567  [  128/  179]
train() client id: f_00007-6-4 loss: 0.516169  [  160/  179]
train() client id: f_00007-7-0 loss: 0.439750  [   32/  179]
train() client id: f_00007-7-1 loss: 0.316386  [   64/  179]
train() client id: f_00007-7-2 loss: 0.564041  [   96/  179]
train() client id: f_00007-7-3 loss: 0.556725  [  128/  179]
train() client id: f_00007-7-4 loss: 0.439760  [  160/  179]
train() client id: f_00007-8-0 loss: 0.317564  [   32/  179]
train() client id: f_00007-8-1 loss: 0.382009  [   64/  179]
train() client id: f_00007-8-2 loss: 0.493629  [   96/  179]
train() client id: f_00007-8-3 loss: 0.296736  [  128/  179]
train() client id: f_00007-8-4 loss: 0.483551  [  160/  179]
train() client id: f_00007-9-0 loss: 0.624194  [   32/  179]
train() client id: f_00007-9-1 loss: 0.563028  [   64/  179]
train() client id: f_00007-9-2 loss: 0.347644  [   96/  179]
train() client id: f_00007-9-3 loss: 0.355326  [  128/  179]
train() client id: f_00007-9-4 loss: 0.345196  [  160/  179]
train() client id: f_00007-10-0 loss: 0.554471  [   32/  179]
train() client id: f_00007-10-1 loss: 0.294840  [   64/  179]
train() client id: f_00007-10-2 loss: 0.477001  [   96/  179]
train() client id: f_00007-10-3 loss: 0.295754  [  128/  179]
train() client id: f_00007-10-4 loss: 0.460220  [  160/  179]
train() client id: f_00007-11-0 loss: 0.298685  [   32/  179]
train() client id: f_00007-11-1 loss: 0.507713  [   64/  179]
train() client id: f_00007-11-2 loss: 0.437068  [   96/  179]
train() client id: f_00007-11-3 loss: 0.402113  [  128/  179]
train() client id: f_00007-11-4 loss: 0.293651  [  160/  179]
train() client id: f_00007-12-0 loss: 0.394503  [   32/  179]
train() client id: f_00007-12-1 loss: 0.435351  [   64/  179]
train() client id: f_00007-12-2 loss: 0.451278  [   96/  179]
train() client id: f_00007-12-3 loss: 0.385859  [  128/  179]
train() client id: f_00007-12-4 loss: 0.458068  [  160/  179]
train() client id: f_00007-13-0 loss: 0.565088  [   32/  179]
train() client id: f_00007-13-1 loss: 0.291150  [   64/  179]
train() client id: f_00007-13-2 loss: 0.524106  [   96/  179]
train() client id: f_00007-13-3 loss: 0.365903  [  128/  179]
train() client id: f_00007-13-4 loss: 0.385272  [  160/  179]
train() client id: f_00008-0-0 loss: 0.837068  [   32/  130]
train() client id: f_00008-0-1 loss: 0.772987  [   64/  130]
train() client id: f_00008-0-2 loss: 0.802980  [   96/  130]
train() client id: f_00008-0-3 loss: 0.824604  [  128/  130]
train() client id: f_00008-1-0 loss: 0.771853  [   32/  130]
train() client id: f_00008-1-1 loss: 0.768308  [   64/  130]
train() client id: f_00008-1-2 loss: 0.912246  [   96/  130]
train() client id: f_00008-1-3 loss: 0.721048  [  128/  130]
train() client id: f_00008-2-0 loss: 0.743581  [   32/  130]
train() client id: f_00008-2-1 loss: 0.842194  [   64/  130]
train() client id: f_00008-2-2 loss: 0.784357  [   96/  130]
train() client id: f_00008-2-3 loss: 0.840998  [  128/  130]
train() client id: f_00008-3-0 loss: 0.771412  [   32/  130]
train() client id: f_00008-3-1 loss: 0.773421  [   64/  130]
train() client id: f_00008-3-2 loss: 0.870197  [   96/  130]
train() client id: f_00008-3-3 loss: 0.790003  [  128/  130]
train() client id: f_00008-4-0 loss: 0.753771  [   32/  130]
train() client id: f_00008-4-1 loss: 0.724269  [   64/  130]
train() client id: f_00008-4-2 loss: 0.864034  [   96/  130]
train() client id: f_00008-4-3 loss: 0.850079  [  128/  130]
train() client id: f_00008-5-0 loss: 0.790417  [   32/  130]
train() client id: f_00008-5-1 loss: 0.918837  [   64/  130]
train() client id: f_00008-5-2 loss: 0.760775  [   96/  130]
train() client id: f_00008-5-3 loss: 0.772978  [  128/  130]
train() client id: f_00008-6-0 loss: 0.909165  [   32/  130]
train() client id: f_00008-6-1 loss: 0.780272  [   64/  130]
train() client id: f_00008-6-2 loss: 0.777110  [   96/  130]
train() client id: f_00008-6-3 loss: 0.767144  [  128/  130]
train() client id: f_00008-7-0 loss: 0.829827  [   32/  130]
train() client id: f_00008-7-1 loss: 0.833733  [   64/  130]
train() client id: f_00008-7-2 loss: 0.796005  [   96/  130]
train() client id: f_00008-7-3 loss: 0.776787  [  128/  130]
train() client id: f_00008-8-0 loss: 0.810449  [   32/  130]
train() client id: f_00008-8-1 loss: 0.882519  [   64/  130]
train() client id: f_00008-8-2 loss: 0.753701  [   96/  130]
train() client id: f_00008-8-3 loss: 0.787549  [  128/  130]
train() client id: f_00008-9-0 loss: 0.804598  [   32/  130]
train() client id: f_00008-9-1 loss: 0.875975  [   64/  130]
train() client id: f_00008-9-2 loss: 0.764718  [   96/  130]
train() client id: f_00008-9-3 loss: 0.799333  [  128/  130]
train() client id: f_00008-10-0 loss: 0.814068  [   32/  130]
train() client id: f_00008-10-1 loss: 0.749688  [   64/  130]
train() client id: f_00008-10-2 loss: 0.811579  [   96/  130]
train() client id: f_00008-10-3 loss: 0.851577  [  128/  130]
train() client id: f_00008-11-0 loss: 0.712996  [   32/  130]
train() client id: f_00008-11-1 loss: 0.801477  [   64/  130]
train() client id: f_00008-11-2 loss: 0.785994  [   96/  130]
train() client id: f_00008-11-3 loss: 0.941547  [  128/  130]
train() client id: f_00008-12-0 loss: 0.769898  [   32/  130]
train() client id: f_00008-12-1 loss: 0.835920  [   64/  130]
train() client id: f_00008-12-2 loss: 0.798196  [   96/  130]
train() client id: f_00008-12-3 loss: 0.817915  [  128/  130]
train() client id: f_00008-13-0 loss: 0.907143  [   32/  130]
train() client id: f_00008-13-1 loss: 0.776597  [   64/  130]
train() client id: f_00008-13-2 loss: 0.785929  [   96/  130]
train() client id: f_00008-13-3 loss: 0.760837  [  128/  130]
train() client id: f_00009-0-0 loss: 1.189708  [   32/  118]
train() client id: f_00009-0-1 loss: 1.041909  [   64/  118]
train() client id: f_00009-0-2 loss: 1.154605  [   96/  118]
train() client id: f_00009-1-0 loss: 1.064742  [   32/  118]
train() client id: f_00009-1-1 loss: 1.098399  [   64/  118]
train() client id: f_00009-1-2 loss: 1.117716  [   96/  118]
train() client id: f_00009-2-0 loss: 1.113979  [   32/  118]
train() client id: f_00009-2-1 loss: 1.157853  [   64/  118]
train() client id: f_00009-2-2 loss: 0.837734  [   96/  118]
train() client id: f_00009-3-0 loss: 1.087577  [   32/  118]
train() client id: f_00009-3-1 loss: 1.013657  [   64/  118]
train() client id: f_00009-3-2 loss: 0.862510  [   96/  118]
train() client id: f_00009-4-0 loss: 1.051176  [   32/  118]
train() client id: f_00009-4-1 loss: 0.873333  [   64/  118]
train() client id: f_00009-4-2 loss: 0.928832  [   96/  118]
train() client id: f_00009-5-0 loss: 0.817126  [   32/  118]
train() client id: f_00009-5-1 loss: 0.938716  [   64/  118]
train() client id: f_00009-5-2 loss: 0.943368  [   96/  118]
train() client id: f_00009-6-0 loss: 0.842804  [   32/  118]
train() client id: f_00009-6-1 loss: 0.954904  [   64/  118]
train() client id: f_00009-6-2 loss: 0.854591  [   96/  118]
train() client id: f_00009-7-0 loss: 0.917323  [   32/  118]
train() client id: f_00009-7-1 loss: 0.770470  [   64/  118]
train() client id: f_00009-7-2 loss: 0.870086  [   96/  118]
train() client id: f_00009-8-0 loss: 0.896413  [   32/  118]
train() client id: f_00009-8-1 loss: 0.739847  [   64/  118]
train() client id: f_00009-8-2 loss: 0.866388  [   96/  118]
train() client id: f_00009-9-0 loss: 0.760233  [   32/  118]
train() client id: f_00009-9-1 loss: 0.940754  [   64/  118]
train() client id: f_00009-9-2 loss: 0.800524  [   96/  118]
train() client id: f_00009-10-0 loss: 0.882079  [   32/  118]
train() client id: f_00009-10-1 loss: 0.725366  [   64/  118]
train() client id: f_00009-10-2 loss: 0.750732  [   96/  118]
train() client id: f_00009-11-0 loss: 0.870115  [   32/  118]
train() client id: f_00009-11-1 loss: 0.666048  [   64/  118]
train() client id: f_00009-11-2 loss: 0.933850  [   96/  118]
train() client id: f_00009-12-0 loss: 0.964809  [   32/  118]
train() client id: f_00009-12-1 loss: 0.688578  [   64/  118]
train() client id: f_00009-12-2 loss: 0.827924  [   96/  118]
train() client id: f_00009-13-0 loss: 0.718029  [   32/  118]
train() client id: f_00009-13-1 loss: 0.836178  [   64/  118]
train() client id: f_00009-13-2 loss: 0.923773  [   96/  118]
At round 25 accuracy: 0.6445623342175066
At round 25 training accuracy: 0.5848423876592891
At round 25 training loss: 0.8431933018808868
update_location
xs = 8.927491 246.223621 5.882650 10.934260 -162.581990 -10.230757 -5.849135 -5.143845 -185.120581 20.134486 
ys = -237.390647 7.291448 135.684448 -17.290817 -9.642386 0.794442 -26.381692 131.628436 25.881276 -672.232496 
xs mean: -7.682379970521214
ys mean: -66.16579882624052
dists_uav = 257.747977 265.855669 168.656085 102.071203 191.117448 100.525119 103.586708 165.385925 211.989316 679.927883 
uav_gains = -111.968304 -112.694175 -105.695651 -100.222602 -107.125721 -100.056882 -100.382627 -105.478565 -108.451512 -127.759563 
uav_gains_db_mean: -107.9835600971193
dists_bs = 451.547747 453.382286 185.106039 267.483284 185.059498 239.784236 262.995439 175.306035 149.461767 869.413808 
bs_gains = -113.898762 -113.948066 -103.054836 -107.531365 -103.051778 -106.202037 -107.325609 -102.393371 -100.453902 -121.865402 
bs_gains_db_mean: -107.97251291238612
Round 26
-------------------------------
ene_coms = [0.0123196  0.01304129 0.00721569 0.00636119 0.00721465 0.00631733
 0.00640411 0.00699653 0.00642341 0.03605462]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.639051   15.81568437  7.42222487  2.64897452 18.10990101  8.69665217
  3.29576991 10.64775594  7.75913986  7.63140952]
obj_prev = 89.66656316277331
eta_min = 5.370607526868417e-13	eta_max = 0.753965326347389
af = 18.822611426745162	bf = 2.08856856625999	zeta = 20.70487256941968	eta = 0.909090909090909
af = 18.822611426745162	bf = 2.08856856625999	zeta = 42.708105503030076	eta = 0.4407269113215459
af = 18.822611426745162	bf = 2.08856856625999	zeta = 31.30938392294353	eta = 0.6011811498134253
af = 18.822611426745162	bf = 2.08856856625999	zeta = 29.25293860734249	eta = 0.6434434392864956
af = 18.822611426745162	bf = 2.08856856625999	zeta = 29.13382394423821	eta = 0.6460741804018386
af = 18.822611426745162	bf = 2.08856856625999	zeta = 29.133381854915633	eta = 0.646083984361371
af = 18.822611426745162	bf = 2.08856856625999	zeta = 29.133381848792425	eta = 0.646083984497164
eta = 0.646083984497164
ene_coms = [0.0123196  0.01304129 0.00721569 0.00636119 0.00721465 0.00631733
 0.00640411 0.00699653 0.00642341 0.03605462]
ene_comp = [0.03604582 0.07581064 0.03547366 0.01230135 0.08753984 0.04176737
 0.01544821 0.05120795 0.03719013 0.03375719]
ene_total = [2.6342739  4.83941475 2.32512079 1.01647477 5.16090352 2.61898451
 1.19020938 3.17016843 2.37545743 3.80237436]
ti_comp = [0.40329092 0.39607408 0.45433005 0.46287511 0.45434049 0.46331364
 0.46244591 0.45652166 0.46225289 0.16594077]
ti_coms = [0.12319605 0.13041289 0.07215692 0.06361185 0.07214648 0.06317333
 0.06404106 0.0699653  0.06423408 0.3605462 ]
t_total = [28.67567062 28.67567062 28.67567062 28.67567062 28.67567062 28.67567062
 28.67567062 28.67567062 28.67567062 28.67567062]
ene_coms = [0.0123196  0.01304129 0.00721569 0.00636119 0.00721465 0.00631733
 0.00640411 0.00699653 0.00642341 0.03605462]
ene_comp = [1.79973149e-05 1.73587223e-04 1.35162152e-05 5.43013558e-07
 2.03111428e-04 2.12149182e-05 1.07743820e-06 4.02688548e-05
 1.50454006e-05 8.73117709e-05]
ene_total = [0.67198057 0.71976225 0.39374647 0.34649828 0.4040161  0.34523571
 0.34886511 0.38326673 0.35067717 1.96850861]
optimize_network iter = 0 obj = 5.932556996163166
eta = 0.646083984497164
freqs = [4.46895953e+07 9.57026091e+07 3.90395266e+07 1.32879794e+07
 9.63372630e+07 4.50746217e+07 1.67027177e+07 5.60849014e+07
 4.02270364e+07 1.01714585e+08]
eta_min = 0.6460839844971653	eta_max = 0.6460839844971613
af = 0.025315281642470985	bf = 2.08856856625999	zeta = 0.027846809806718085	eta = 0.9090909090909091
af = 0.025315281642470985	bf = 2.08856856625999	zeta = 23.000795073356436	eta = 0.0011006263723376934
af = 0.025315281642470985	bf = 2.08856856625999	zeta = 2.263504394086679	eta = 0.011184109785077606
af = 0.025315281642470985	bf = 2.08856856625999	zeta = 2.2272262842669064	eta = 0.011366281828343065
af = 0.025315281642470985	bf = 2.08856856625999	zeta = 2.2272229598485924	eta = 0.011366298793988693
eta = 0.011366298793988693
ene_coms = [0.0123196  0.01304129 0.00721569 0.00636119 0.00721465 0.00631733
 0.00640411 0.00699653 0.00642341 0.03605462]
ene_comp = [1.84457577e-04 1.77912532e-03 1.38530015e-04 5.56543939e-06
 2.08172398e-03 2.17435347e-04 1.10428495e-05 4.12722421e-04
 1.54202899e-04 8.94873363e-04]
ene_total = [0.2438046  0.28896891 0.14339284 0.12413911 0.18126094 0.12741511
 0.12508278 0.14446584 0.12825046 0.72044238]
ti_comp = [0.40329092 0.39607408 0.45433005 0.46287511 0.45434049 0.46331364
 0.46244591 0.45652166 0.46225289 0.16594077]
ti_coms = [0.12319605 0.13041289 0.07215692 0.06361185 0.07214648 0.06317333
 0.06404106 0.0699653  0.06423408 0.3605462 ]
t_total = [28.67567062 28.67567062 28.67567062 28.67567062 28.67567062 28.67567062
 28.67567062 28.67567062 28.67567062 28.67567062]
ene_coms = [0.0123196  0.01304129 0.00721569 0.00636119 0.00721465 0.00631733
 0.00640411 0.00699653 0.00642341 0.03605462]
ene_comp = [1.79973149e-05 1.73587223e-04 1.35162152e-05 5.43013558e-07
 2.03111428e-04 2.12149182e-05 1.07743820e-06 4.02688548e-05
 1.50454006e-05 8.73117709e-05]
ene_total = [0.67198057 0.71976225 0.39374647 0.34649828 0.4040161  0.34523571
 0.34886511 0.38326673 0.35067717 1.96850861]
optimize_network iter = 1 obj = 5.932556996163188
eta = 0.6460839844971653
freqs = [4.46895953e+07 9.57026091e+07 3.90395266e+07 1.32879794e+07
 9.63372630e+07 4.50746217e+07 1.67027177e+07 5.60849014e+07
 4.02270364e+07 1.01714585e+08]
Done!
ene_coms = [0.0123196  0.01304129 0.00721569 0.00636119 0.00721465 0.00631733
 0.00640411 0.00699653 0.00642341 0.03605462]
ene_comp = [1.76149506e-05 1.69899253e-04 1.32290547e-05 5.31476893e-07
 1.98796198e-04 2.07641939e-05 1.05454735e-06 3.94133176e-05
 1.47257516e-05 8.54567773e-05]
ene_total = [0.01233722 0.01321119 0.00722892 0.00636172 0.00741344 0.0063381
 0.00640516 0.00703594 0.00643813 0.03614008]
At round 26 energy consumption: 0.10890990080060367
At round 26 eta: 0.6460839844971653
At round 26 a_n: 19.276410835052125
At round 26 local rounds: 14.303895269865631
At round 26 global rounds: 54.466059716638426
gradient difference: 0.42112308740615845
train() client id: f_00000-0-0 loss: 1.417925  [   32/  126]
train() client id: f_00000-0-1 loss: 1.224773  [   64/  126]
train() client id: f_00000-0-2 loss: 1.213052  [   96/  126]
train() client id: f_00000-1-0 loss: 1.155553  [   32/  126]
train() client id: f_00000-1-1 loss: 1.411854  [   64/  126]
train() client id: f_00000-1-2 loss: 1.136092  [   96/  126]
train() client id: f_00000-2-0 loss: 0.973824  [   32/  126]
train() client id: f_00000-2-1 loss: 1.254727  [   64/  126]
train() client id: f_00000-2-2 loss: 1.051034  [   96/  126]
train() client id: f_00000-3-0 loss: 1.077867  [   32/  126]
train() client id: f_00000-3-1 loss: 0.804706  [   64/  126]
train() client id: f_00000-3-2 loss: 1.060188  [   96/  126]
train() client id: f_00000-4-0 loss: 1.025376  [   32/  126]
train() client id: f_00000-4-1 loss: 0.970832  [   64/  126]
train() client id: f_00000-4-2 loss: 0.902593  [   96/  126]
train() client id: f_00000-5-0 loss: 0.993222  [   32/  126]
train() client id: f_00000-5-1 loss: 0.887747  [   64/  126]
train() client id: f_00000-5-2 loss: 0.923662  [   96/  126]
train() client id: f_00000-6-0 loss: 0.867079  [   32/  126]
train() client id: f_00000-6-1 loss: 0.876165  [   64/  126]
train() client id: f_00000-6-2 loss: 0.854422  [   96/  126]
train() client id: f_00000-7-0 loss: 0.791929  [   32/  126]
train() client id: f_00000-7-1 loss: 0.958436  [   64/  126]
train() client id: f_00000-7-2 loss: 0.866126  [   96/  126]
train() client id: f_00000-8-0 loss: 0.903537  [   32/  126]
train() client id: f_00000-8-1 loss: 0.869888  [   64/  126]
train() client id: f_00000-8-2 loss: 0.798006  [   96/  126]
train() client id: f_00000-9-0 loss: 0.874929  [   32/  126]
train() client id: f_00000-9-1 loss: 0.817175  [   64/  126]
train() client id: f_00000-9-2 loss: 0.785481  [   96/  126]
train() client id: f_00000-10-0 loss: 0.891080  [   32/  126]
train() client id: f_00000-10-1 loss: 0.842539  [   64/  126]
train() client id: f_00000-10-2 loss: 0.801755  [   96/  126]
train() client id: f_00000-11-0 loss: 0.789471  [   32/  126]
train() client id: f_00000-11-1 loss: 0.836168  [   64/  126]
train() client id: f_00000-11-2 loss: 0.820926  [   96/  126]
train() client id: f_00000-12-0 loss: 0.721202  [   32/  126]
train() client id: f_00000-12-1 loss: 0.804051  [   64/  126]
train() client id: f_00000-12-2 loss: 0.936824  [   96/  126]
train() client id: f_00000-13-0 loss: 0.730187  [   32/  126]
train() client id: f_00000-13-1 loss: 0.742710  [   64/  126]
train() client id: f_00000-13-2 loss: 0.814980  [   96/  126]
train() client id: f_00001-0-0 loss: 0.520124  [   32/  265]
train() client id: f_00001-0-1 loss: 0.482026  [   64/  265]
train() client id: f_00001-0-2 loss: 0.541583  [   96/  265]
train() client id: f_00001-0-3 loss: 0.522245  [  128/  265]
train() client id: f_00001-0-4 loss: 0.500319  [  160/  265]
train() client id: f_00001-0-5 loss: 0.500539  [  192/  265]
train() client id: f_00001-0-6 loss: 0.466070  [  224/  265]
train() client id: f_00001-0-7 loss: 0.536875  [  256/  265]
train() client id: f_00001-1-0 loss: 0.453082  [   32/  265]
train() client id: f_00001-1-1 loss: 0.450528  [   64/  265]
train() client id: f_00001-1-2 loss: 0.518427  [   96/  265]
train() client id: f_00001-1-3 loss: 0.487260  [  128/  265]
train() client id: f_00001-1-4 loss: 0.578358  [  160/  265]
train() client id: f_00001-1-5 loss: 0.379244  [  192/  265]
train() client id: f_00001-1-6 loss: 0.509648  [  224/  265]
train() client id: f_00001-1-7 loss: 0.588017  [  256/  265]
train() client id: f_00001-2-0 loss: 0.583895  [   32/  265]
train() client id: f_00001-2-1 loss: 0.505781  [   64/  265]
train() client id: f_00001-2-2 loss: 0.534045  [   96/  265]
train() client id: f_00001-2-3 loss: 0.438184  [  128/  265]
train() client id: f_00001-2-4 loss: 0.476079  [  160/  265]
train() client id: f_00001-2-5 loss: 0.403269  [  192/  265]
train() client id: f_00001-2-6 loss: 0.506517  [  224/  265]
train() client id: f_00001-2-7 loss: 0.467765  [  256/  265]
train() client id: f_00001-3-0 loss: 0.447401  [   32/  265]
train() client id: f_00001-3-1 loss: 0.469672  [   64/  265]
train() client id: f_00001-3-2 loss: 0.506353  [   96/  265]
train() client id: f_00001-3-3 loss: 0.659329  [  128/  265]
train() client id: f_00001-3-4 loss: 0.391919  [  160/  265]
train() client id: f_00001-3-5 loss: 0.532375  [  192/  265]
train() client id: f_00001-3-6 loss: 0.456449  [  224/  265]
train() client id: f_00001-3-7 loss: 0.469369  [  256/  265]
train() client id: f_00001-4-0 loss: 0.561775  [   32/  265]
train() client id: f_00001-4-1 loss: 0.406803  [   64/  265]
train() client id: f_00001-4-2 loss: 0.484065  [   96/  265]
train() client id: f_00001-4-3 loss: 0.414488  [  128/  265]
train() client id: f_00001-4-4 loss: 0.717714  [  160/  265]
train() client id: f_00001-4-5 loss: 0.401762  [  192/  265]
train() client id: f_00001-4-6 loss: 0.473319  [  224/  265]
train() client id: f_00001-4-7 loss: 0.422495  [  256/  265]
train() client id: f_00001-5-0 loss: 0.543226  [   32/  265]
train() client id: f_00001-5-1 loss: 0.573530  [   64/  265]
train() client id: f_00001-5-2 loss: 0.382010  [   96/  265]
train() client id: f_00001-5-3 loss: 0.444543  [  128/  265]
train() client id: f_00001-5-4 loss: 0.391589  [  160/  265]
train() client id: f_00001-5-5 loss: 0.462289  [  192/  265]
train() client id: f_00001-5-6 loss: 0.620254  [  224/  265]
train() client id: f_00001-5-7 loss: 0.466874  [  256/  265]
train() client id: f_00001-6-0 loss: 0.483454  [   32/  265]
train() client id: f_00001-6-1 loss: 0.470687  [   64/  265]
train() client id: f_00001-6-2 loss: 0.500124  [   96/  265]
train() client id: f_00001-6-3 loss: 0.386083  [  128/  265]
train() client id: f_00001-6-4 loss: 0.630885  [  160/  265]
train() client id: f_00001-6-5 loss: 0.375472  [  192/  265]
train() client id: f_00001-6-6 loss: 0.500372  [  224/  265]
train() client id: f_00001-6-7 loss: 0.421627  [  256/  265]
train() client id: f_00001-7-0 loss: 0.494576  [   32/  265]
train() client id: f_00001-7-1 loss: 0.457013  [   64/  265]
train() client id: f_00001-7-2 loss: 0.453011  [   96/  265]
train() client id: f_00001-7-3 loss: 0.511586  [  128/  265]
train() client id: f_00001-7-4 loss: 0.432070  [  160/  265]
train() client id: f_00001-7-5 loss: 0.575773  [  192/  265]
train() client id: f_00001-7-6 loss: 0.518753  [  224/  265]
train() client id: f_00001-7-7 loss: 0.414841  [  256/  265]
train() client id: f_00001-8-0 loss: 0.586041  [   32/  265]
train() client id: f_00001-8-1 loss: 0.386834  [   64/  265]
train() client id: f_00001-8-2 loss: 0.499222  [   96/  265]
train() client id: f_00001-8-3 loss: 0.381221  [  128/  265]
train() client id: f_00001-8-4 loss: 0.519894  [  160/  265]
train() client id: f_00001-8-5 loss: 0.484097  [  192/  265]
train() client id: f_00001-8-6 loss: 0.422280  [  224/  265]
train() client id: f_00001-8-7 loss: 0.511342  [  256/  265]
train() client id: f_00001-9-0 loss: 0.391316  [   32/  265]
train() client id: f_00001-9-1 loss: 0.465407  [   64/  265]
train() client id: f_00001-9-2 loss: 0.464596  [   96/  265]
train() client id: f_00001-9-3 loss: 0.559957  [  128/  265]
train() client id: f_00001-9-4 loss: 0.631161  [  160/  265]
train() client id: f_00001-9-5 loss: 0.409396  [  192/  265]
train() client id: f_00001-9-6 loss: 0.434299  [  224/  265]
train() client id: f_00001-9-7 loss: 0.499763  [  256/  265]
train() client id: f_00001-10-0 loss: 0.436991  [   32/  265]
train() client id: f_00001-10-1 loss: 0.528842  [   64/  265]
train() client id: f_00001-10-2 loss: 0.403649  [   96/  265]
train() client id: f_00001-10-3 loss: 0.492054  [  128/  265]
train() client id: f_00001-10-4 loss: 0.494690  [  160/  265]
train() client id: f_00001-10-5 loss: 0.440983  [  192/  265]
train() client id: f_00001-10-6 loss: 0.584988  [  224/  265]
train() client id: f_00001-10-7 loss: 0.476717  [  256/  265]
train() client id: f_00001-11-0 loss: 0.386148  [   32/  265]
train() client id: f_00001-11-1 loss: 0.470531  [   64/  265]
train() client id: f_00001-11-2 loss: 0.495127  [   96/  265]
train() client id: f_00001-11-3 loss: 0.624207  [  128/  265]
train() client id: f_00001-11-4 loss: 0.383681  [  160/  265]
train() client id: f_00001-11-5 loss: 0.504884  [  192/  265]
train() client id: f_00001-11-6 loss: 0.449495  [  224/  265]
train() client id: f_00001-11-7 loss: 0.544338  [  256/  265]
train() client id: f_00001-12-0 loss: 0.536521  [   32/  265]
train() client id: f_00001-12-1 loss: 0.496590  [   64/  265]
train() client id: f_00001-12-2 loss: 0.518394  [   96/  265]
train() client id: f_00001-12-3 loss: 0.385874  [  128/  265]
train() client id: f_00001-12-4 loss: 0.462833  [  160/  265]
train() client id: f_00001-12-5 loss: 0.380951  [  192/  265]
train() client id: f_00001-12-6 loss: 0.643409  [  224/  265]
train() client id: f_00001-12-7 loss: 0.443881  [  256/  265]
train() client id: f_00001-13-0 loss: 0.558918  [   32/  265]
train() client id: f_00001-13-1 loss: 0.419560  [   64/  265]
train() client id: f_00001-13-2 loss: 0.585866  [   96/  265]
train() client id: f_00001-13-3 loss: 0.445952  [  128/  265]
train() client id: f_00001-13-4 loss: 0.452771  [  160/  265]
train() client id: f_00001-13-5 loss: 0.406915  [  192/  265]
train() client id: f_00001-13-6 loss: 0.400796  [  224/  265]
train() client id: f_00001-13-7 loss: 0.548844  [  256/  265]
train() client id: f_00002-0-0 loss: 1.141890  [   32/  124]
train() client id: f_00002-0-1 loss: 1.210449  [   64/  124]
train() client id: f_00002-0-2 loss: 1.052959  [   96/  124]
train() client id: f_00002-1-0 loss: 1.111199  [   32/  124]
train() client id: f_00002-1-1 loss: 1.210919  [   64/  124]
train() client id: f_00002-1-2 loss: 1.109286  [   96/  124]
train() client id: f_00002-2-0 loss: 1.109401  [   32/  124]
train() client id: f_00002-2-1 loss: 0.943122  [   64/  124]
train() client id: f_00002-2-2 loss: 1.203249  [   96/  124]
train() client id: f_00002-3-0 loss: 1.121128  [   32/  124]
train() client id: f_00002-3-1 loss: 1.014938  [   64/  124]
train() client id: f_00002-3-2 loss: 0.948132  [   96/  124]
train() client id: f_00002-4-0 loss: 1.002510  [   32/  124]
train() client id: f_00002-4-1 loss: 1.079805  [   64/  124]
train() client id: f_00002-4-2 loss: 0.996127  [   96/  124]
train() client id: f_00002-5-0 loss: 0.975836  [   32/  124]
train() client id: f_00002-5-1 loss: 1.016935  [   64/  124]
train() client id: f_00002-5-2 loss: 0.951822  [   96/  124]
train() client id: f_00002-6-0 loss: 1.090710  [   32/  124]
train() client id: f_00002-6-1 loss: 1.121965  [   64/  124]
train() client id: f_00002-6-2 loss: 0.804943  [   96/  124]
train() client id: f_00002-7-0 loss: 0.837707  [   32/  124]
train() client id: f_00002-7-1 loss: 0.934740  [   64/  124]
train() client id: f_00002-7-2 loss: 0.957541  [   96/  124]
train() client id: f_00002-8-0 loss: 0.859545  [   32/  124]
train() client id: f_00002-8-1 loss: 0.898483  [   64/  124]
train() client id: f_00002-8-2 loss: 0.986970  [   96/  124]
train() client id: f_00002-9-0 loss: 0.986920  [   32/  124]
train() client id: f_00002-9-1 loss: 0.915633  [   64/  124]
train() client id: f_00002-9-2 loss: 0.938207  [   96/  124]
train() client id: f_00002-10-0 loss: 0.889320  [   32/  124]
train() client id: f_00002-10-1 loss: 0.864554  [   64/  124]
train() client id: f_00002-10-2 loss: 1.064917  [   96/  124]
train() client id: f_00002-11-0 loss: 0.951624  [   32/  124]
train() client id: f_00002-11-1 loss: 0.940248  [   64/  124]
train() client id: f_00002-11-2 loss: 0.868375  [   96/  124]
train() client id: f_00002-12-0 loss: 0.808270  [   32/  124]
train() client id: f_00002-12-1 loss: 1.011208  [   64/  124]
train() client id: f_00002-12-2 loss: 1.025988  [   96/  124]
train() client id: f_00002-13-0 loss: 1.038625  [   32/  124]
train() client id: f_00002-13-1 loss: 0.898374  [   64/  124]
train() client id: f_00002-13-2 loss: 0.849513  [   96/  124]
train() client id: f_00003-0-0 loss: 0.552910  [   32/   43]
train() client id: f_00003-1-0 loss: 0.516572  [   32/   43]
train() client id: f_00003-2-0 loss: 0.506646  [   32/   43]
train() client id: f_00003-3-0 loss: 0.517706  [   32/   43]
train() client id: f_00003-4-0 loss: 0.314283  [   32/   43]
train() client id: f_00003-5-0 loss: 0.660693  [   32/   43]
train() client id: f_00003-6-0 loss: 0.521337  [   32/   43]
train() client id: f_00003-7-0 loss: 0.480229  [   32/   43]
train() client id: f_00003-8-0 loss: 0.619606  [   32/   43]
train() client id: f_00003-9-0 loss: 0.538004  [   32/   43]
train() client id: f_00003-10-0 loss: 0.440108  [   32/   43]
train() client id: f_00003-11-0 loss: 0.580956  [   32/   43]
train() client id: f_00003-12-0 loss: 0.414859  [   32/   43]
train() client id: f_00003-13-0 loss: 0.553654  [   32/   43]
train() client id: f_00004-0-0 loss: 0.695391  [   32/  306]
train() client id: f_00004-0-1 loss: 0.895173  [   64/  306]
train() client id: f_00004-0-2 loss: 0.829781  [   96/  306]
train() client id: f_00004-0-3 loss: 0.649781  [  128/  306]
train() client id: f_00004-0-4 loss: 0.856685  [  160/  306]
train() client id: f_00004-0-5 loss: 0.947142  [  192/  306]
train() client id: f_00004-0-6 loss: 0.740304  [  224/  306]
train() client id: f_00004-0-7 loss: 0.899271  [  256/  306]
train() client id: f_00004-0-8 loss: 0.754614  [  288/  306]
train() client id: f_00004-1-0 loss: 0.799546  [   32/  306]
train() client id: f_00004-1-1 loss: 0.802695  [   64/  306]
train() client id: f_00004-1-2 loss: 0.814704  [   96/  306]
train() client id: f_00004-1-3 loss: 0.763686  [  128/  306]
train() client id: f_00004-1-4 loss: 0.839900  [  160/  306]
train() client id: f_00004-1-5 loss: 0.643743  [  192/  306]
train() client id: f_00004-1-6 loss: 0.931154  [  224/  306]
train() client id: f_00004-1-7 loss: 0.724356  [  256/  306]
train() client id: f_00004-1-8 loss: 0.820825  [  288/  306]
train() client id: f_00004-2-0 loss: 0.693184  [   32/  306]
train() client id: f_00004-2-1 loss: 0.765534  [   64/  306]
train() client id: f_00004-2-2 loss: 0.734727  [   96/  306]
train() client id: f_00004-2-3 loss: 0.976003  [  128/  306]
train() client id: f_00004-2-4 loss: 0.852850  [  160/  306]
train() client id: f_00004-2-5 loss: 0.833428  [  192/  306]
train() client id: f_00004-2-6 loss: 0.818256  [  224/  306]
train() client id: f_00004-2-7 loss: 0.754491  [  256/  306]
train() client id: f_00004-2-8 loss: 0.792289  [  288/  306]
train() client id: f_00004-3-0 loss: 0.829032  [   32/  306]
train() client id: f_00004-3-1 loss: 0.727737  [   64/  306]
train() client id: f_00004-3-2 loss: 0.768598  [   96/  306]
train() client id: f_00004-3-3 loss: 0.701222  [  128/  306]
train() client id: f_00004-3-4 loss: 0.778759  [  160/  306]
train() client id: f_00004-3-5 loss: 0.895635  [  192/  306]
train() client id: f_00004-3-6 loss: 0.841771  [  224/  306]
train() client id: f_00004-3-7 loss: 0.738881  [  256/  306]
train() client id: f_00004-3-8 loss: 0.829526  [  288/  306]
train() client id: f_00004-4-0 loss: 0.711520  [   32/  306]
train() client id: f_00004-4-1 loss: 0.955725  [   64/  306]
train() client id: f_00004-4-2 loss: 0.859836  [   96/  306]
train() client id: f_00004-4-3 loss: 0.726780  [  128/  306]
train() client id: f_00004-4-4 loss: 0.711037  [  160/  306]
train() client id: f_00004-4-5 loss: 0.703917  [  192/  306]
train() client id: f_00004-4-6 loss: 0.774680  [  224/  306]
train() client id: f_00004-4-7 loss: 0.915451  [  256/  306]
train() client id: f_00004-4-8 loss: 0.737642  [  288/  306]
train() client id: f_00004-5-0 loss: 0.842227  [   32/  306]
train() client id: f_00004-5-1 loss: 0.911166  [   64/  306]
train() client id: f_00004-5-2 loss: 0.804529  [   96/  306]
train() client id: f_00004-5-3 loss: 0.777762  [  128/  306]
train() client id: f_00004-5-4 loss: 0.716337  [  160/  306]
train() client id: f_00004-5-5 loss: 0.775239  [  192/  306]
train() client id: f_00004-5-6 loss: 0.740185  [  224/  306]
train() client id: f_00004-5-7 loss: 0.688484  [  256/  306]
train() client id: f_00004-5-8 loss: 0.857664  [  288/  306]
train() client id: f_00004-6-0 loss: 0.694208  [   32/  306]
train() client id: f_00004-6-1 loss: 0.824867  [   64/  306]
train() client id: f_00004-6-2 loss: 0.679341  [   96/  306]
train() client id: f_00004-6-3 loss: 0.630563  [  128/  306]
train() client id: f_00004-6-4 loss: 0.883825  [  160/  306]
train() client id: f_00004-6-5 loss: 0.826916  [  192/  306]
train() client id: f_00004-6-6 loss: 0.734568  [  224/  306]
train() client id: f_00004-6-7 loss: 0.887272  [  256/  306]
train() client id: f_00004-6-8 loss: 0.901190  [  288/  306]
train() client id: f_00004-7-0 loss: 0.767194  [   32/  306]
train() client id: f_00004-7-1 loss: 0.753502  [   64/  306]
train() client id: f_00004-7-2 loss: 0.970818  [   96/  306]
train() client id: f_00004-7-3 loss: 0.833546  [  128/  306]
train() client id: f_00004-7-4 loss: 0.689316  [  160/  306]
train() client id: f_00004-7-5 loss: 0.713090  [  192/  306]
train() client id: f_00004-7-6 loss: 0.934350  [  224/  306]
train() client id: f_00004-7-7 loss: 0.720281  [  256/  306]
train() client id: f_00004-7-8 loss: 0.688819  [  288/  306]
train() client id: f_00004-8-0 loss: 0.725061  [   32/  306]
train() client id: f_00004-8-1 loss: 0.776784  [   64/  306]
train() client id: f_00004-8-2 loss: 0.675314  [   96/  306]
train() client id: f_00004-8-3 loss: 0.825138  [  128/  306]
train() client id: f_00004-8-4 loss: 0.761605  [  160/  306]
train() client id: f_00004-8-5 loss: 0.800951  [  192/  306]
train() client id: f_00004-8-6 loss: 0.871483  [  224/  306]
train() client id: f_00004-8-7 loss: 0.830924  [  256/  306]
train() client id: f_00004-8-8 loss: 0.925386  [  288/  306]
train() client id: f_00004-9-0 loss: 0.829220  [   32/  306]
train() client id: f_00004-9-1 loss: 0.779698  [   64/  306]
train() client id: f_00004-9-2 loss: 0.803984  [   96/  306]
train() client id: f_00004-9-3 loss: 0.736554  [  128/  306]
train() client id: f_00004-9-4 loss: 0.779993  [  160/  306]
train() client id: f_00004-9-5 loss: 0.813017  [  192/  306]
train() client id: f_00004-9-6 loss: 0.822200  [  224/  306]
train() client id: f_00004-9-7 loss: 0.847359  [  256/  306]
train() client id: f_00004-9-8 loss: 0.679522  [  288/  306]
train() client id: f_00004-10-0 loss: 0.841820  [   32/  306]
train() client id: f_00004-10-1 loss: 0.649734  [   64/  306]
train() client id: f_00004-10-2 loss: 0.913497  [   96/  306]
train() client id: f_00004-10-3 loss: 0.768708  [  128/  306]
train() client id: f_00004-10-4 loss: 0.849725  [  160/  306]
train() client id: f_00004-10-5 loss: 0.841427  [  192/  306]
train() client id: f_00004-10-6 loss: 0.732413  [  224/  306]
train() client id: f_00004-10-7 loss: 0.725424  [  256/  306]
train() client id: f_00004-10-8 loss: 0.801354  [  288/  306]
train() client id: f_00004-11-0 loss: 0.763149  [   32/  306]
train() client id: f_00004-11-1 loss: 0.799038  [   64/  306]
train() client id: f_00004-11-2 loss: 0.770595  [   96/  306]
train() client id: f_00004-11-3 loss: 0.761626  [  128/  306]
train() client id: f_00004-11-4 loss: 0.758639  [  160/  306]
train() client id: f_00004-11-5 loss: 0.755962  [  192/  306]
train() client id: f_00004-11-6 loss: 0.879533  [  224/  306]
train() client id: f_00004-11-7 loss: 0.844896  [  256/  306]
train() client id: f_00004-11-8 loss: 0.792111  [  288/  306]
train() client id: f_00004-12-0 loss: 0.769630  [   32/  306]
train() client id: f_00004-12-1 loss: 0.818836  [   64/  306]
train() client id: f_00004-12-2 loss: 0.856879  [   96/  306]
train() client id: f_00004-12-3 loss: 0.654811  [  128/  306]
train() client id: f_00004-12-4 loss: 0.889845  [  160/  306]
train() client id: f_00004-12-5 loss: 0.780868  [  192/  306]
train() client id: f_00004-12-6 loss: 0.807206  [  224/  306]
train() client id: f_00004-12-7 loss: 0.650079  [  256/  306]
train() client id: f_00004-12-8 loss: 0.767533  [  288/  306]
train() client id: f_00004-13-0 loss: 0.753898  [   32/  306]
train() client id: f_00004-13-1 loss: 0.805688  [   64/  306]
train() client id: f_00004-13-2 loss: 0.770213  [   96/  306]
train() client id: f_00004-13-3 loss: 0.742197  [  128/  306]
train() client id: f_00004-13-4 loss: 0.997556  [  160/  306]
train() client id: f_00004-13-5 loss: 0.800272  [  192/  306]
train() client id: f_00004-13-6 loss: 0.922074  [  224/  306]
train() client id: f_00004-13-7 loss: 0.685564  [  256/  306]
train() client id: f_00004-13-8 loss: 0.695035  [  288/  306]
train() client id: f_00005-0-0 loss: 0.645173  [   32/  146]
train() client id: f_00005-0-1 loss: 0.631866  [   64/  146]
train() client id: f_00005-0-2 loss: 0.571141  [   96/  146]
train() client id: f_00005-0-3 loss: 0.435251  [  128/  146]
train() client id: f_00005-1-0 loss: 0.458726  [   32/  146]
train() client id: f_00005-1-1 loss: 0.498198  [   64/  146]
train() client id: f_00005-1-2 loss: 0.604191  [   96/  146]
train() client id: f_00005-1-3 loss: 0.513913  [  128/  146]
train() client id: f_00005-2-0 loss: 0.633431  [   32/  146]
train() client id: f_00005-2-1 loss: 0.457856  [   64/  146]
train() client id: f_00005-2-2 loss: 0.424524  [   96/  146]
train() client id: f_00005-2-3 loss: 0.754012  [  128/  146]
train() client id: f_00005-3-0 loss: 0.587323  [   32/  146]
train() client id: f_00005-3-1 loss: 0.412755  [   64/  146]
train() client id: f_00005-3-2 loss: 0.552412  [   96/  146]
train() client id: f_00005-3-3 loss: 0.714096  [  128/  146]
train() client id: f_00005-4-0 loss: 0.616177  [   32/  146]
train() client id: f_00005-4-1 loss: 0.377529  [   64/  146]
train() client id: f_00005-4-2 loss: 0.721002  [   96/  146]
train() client id: f_00005-4-3 loss: 0.229837  [  128/  146]
train() client id: f_00005-5-0 loss: 0.578816  [   32/  146]
train() client id: f_00005-5-1 loss: 0.695071  [   64/  146]
train() client id: f_00005-5-2 loss: 0.535128  [   96/  146]
train() client id: f_00005-5-3 loss: 0.608697  [  128/  146]
train() client id: f_00005-6-0 loss: 0.475004  [   32/  146]
train() client id: f_00005-6-1 loss: 0.475480  [   64/  146]
train() client id: f_00005-6-2 loss: 0.755616  [   96/  146]
train() client id: f_00005-6-3 loss: 0.675097  [  128/  146]
train() client id: f_00005-7-0 loss: 0.402393  [   32/  146]
train() client id: f_00005-7-1 loss: 0.712536  [   64/  146]
train() client id: f_00005-7-2 loss: 0.626532  [   96/  146]
train() client id: f_00005-7-3 loss: 0.646246  [  128/  146]
train() client id: f_00005-8-0 loss: 0.420250  [   32/  146]
train() client id: f_00005-8-1 loss: 1.002556  [   64/  146]
train() client id: f_00005-8-2 loss: 0.446386  [   96/  146]
train() client id: f_00005-8-3 loss: 0.406428  [  128/  146]
train() client id: f_00005-9-0 loss: 0.552577  [   32/  146]
train() client id: f_00005-9-1 loss: 0.647810  [   64/  146]
train() client id: f_00005-9-2 loss: 0.580127  [   96/  146]
train() client id: f_00005-9-3 loss: 0.545092  [  128/  146]
train() client id: f_00005-10-0 loss: 0.502127  [   32/  146]
train() client id: f_00005-10-1 loss: 0.457292  [   64/  146]
train() client id: f_00005-10-2 loss: 0.807026  [   96/  146]
train() client id: f_00005-10-3 loss: 0.462067  [  128/  146]
train() client id: f_00005-11-0 loss: 0.636036  [   32/  146]
train() client id: f_00005-11-1 loss: 0.815566  [   64/  146]
train() client id: f_00005-11-2 loss: 0.349326  [   96/  146]
train() client id: f_00005-11-3 loss: 0.557163  [  128/  146]
train() client id: f_00005-12-0 loss: 0.473598  [   32/  146]
train() client id: f_00005-12-1 loss: 0.358457  [   64/  146]
train() client id: f_00005-12-2 loss: 0.964225  [   96/  146]
train() client id: f_00005-12-3 loss: 0.600396  [  128/  146]
train() client id: f_00005-13-0 loss: 0.500168  [   32/  146]
train() client id: f_00005-13-1 loss: 0.689933  [   64/  146]
train() client id: f_00005-13-2 loss: 0.570595  [   96/  146]
train() client id: f_00005-13-3 loss: 0.397839  [  128/  146]
train() client id: f_00006-0-0 loss: 0.519187  [   32/   54]
train() client id: f_00006-1-0 loss: 0.514205  [   32/   54]
train() client id: f_00006-2-0 loss: 0.525151  [   32/   54]
train() client id: f_00006-3-0 loss: 0.464816  [   32/   54]
train() client id: f_00006-4-0 loss: 0.566561  [   32/   54]
train() client id: f_00006-5-0 loss: 0.472355  [   32/   54]
train() client id: f_00006-6-0 loss: 0.525876  [   32/   54]
train() client id: f_00006-7-0 loss: 0.531945  [   32/   54]
train() client id: f_00006-8-0 loss: 0.470461  [   32/   54]
train() client id: f_00006-9-0 loss: 0.458079  [   32/   54]
train() client id: f_00006-10-0 loss: 0.536058  [   32/   54]
train() client id: f_00006-11-0 loss: 0.498219  [   32/   54]
train() client id: f_00006-12-0 loss: 0.523391  [   32/   54]
train() client id: f_00006-13-0 loss: 0.453982  [   32/   54]
train() client id: f_00007-0-0 loss: 0.728128  [   32/  179]
train() client id: f_00007-0-1 loss: 0.673743  [   64/  179]
train() client id: f_00007-0-2 loss: 0.696377  [   96/  179]
train() client id: f_00007-0-3 loss: 0.655330  [  128/  179]
train() client id: f_00007-0-4 loss: 0.492760  [  160/  179]
train() client id: f_00007-1-0 loss: 0.664626  [   32/  179]
train() client id: f_00007-1-1 loss: 0.791272  [   64/  179]
train() client id: f_00007-1-2 loss: 0.502341  [   96/  179]
train() client id: f_00007-1-3 loss: 0.509677  [  128/  179]
train() client id: f_00007-1-4 loss: 0.600668  [  160/  179]
train() client id: f_00007-2-0 loss: 0.629332  [   32/  179]
train() client id: f_00007-2-1 loss: 0.583559  [   64/  179]
train() client id: f_00007-2-2 loss: 0.563227  [   96/  179]
train() client id: f_00007-2-3 loss: 0.555987  [  128/  179]
train() client id: f_00007-2-4 loss: 0.570234  [  160/  179]
train() client id: f_00007-3-0 loss: 0.692710  [   32/  179]
train() client id: f_00007-3-1 loss: 0.617642  [   64/  179]
train() client id: f_00007-3-2 loss: 0.613452  [   96/  179]
train() client id: f_00007-3-3 loss: 0.471892  [  128/  179]
train() client id: f_00007-3-4 loss: 0.706902  [  160/  179]
train() client id: f_00007-4-0 loss: 0.866122  [   32/  179]
train() client id: f_00007-4-1 loss: 0.494017  [   64/  179]
train() client id: f_00007-4-2 loss: 0.661540  [   96/  179]
train() client id: f_00007-4-3 loss: 0.553195  [  128/  179]
train() client id: f_00007-4-4 loss: 0.494506  [  160/  179]
train() client id: f_00007-5-0 loss: 0.602518  [   32/  179]
train() client id: f_00007-5-1 loss: 0.525311  [   64/  179]
train() client id: f_00007-5-2 loss: 0.514706  [   96/  179]
train() client id: f_00007-5-3 loss: 0.726379  [  128/  179]
train() client id: f_00007-5-4 loss: 0.660316  [  160/  179]
train() client id: f_00007-6-0 loss: 0.629312  [   32/  179]
train() client id: f_00007-6-1 loss: 0.513436  [   64/  179]
train() client id: f_00007-6-2 loss: 0.698367  [   96/  179]
train() client id: f_00007-6-3 loss: 0.525281  [  128/  179]
train() client id: f_00007-6-4 loss: 0.614920  [  160/  179]
train() client id: f_00007-7-0 loss: 0.445389  [   32/  179]
train() client id: f_00007-7-1 loss: 0.456691  [   64/  179]
train() client id: f_00007-7-2 loss: 0.532880  [   96/  179]
train() client id: f_00007-7-3 loss: 0.746335  [  128/  179]
train() client id: f_00007-7-4 loss: 0.608524  [  160/  179]
train() client id: f_00007-8-0 loss: 0.706755  [   32/  179]
train() client id: f_00007-8-1 loss: 0.574077  [   64/  179]
train() client id: f_00007-8-2 loss: 0.429179  [   96/  179]
train() client id: f_00007-8-3 loss: 0.522428  [  128/  179]
train() client id: f_00007-8-4 loss: 0.590529  [  160/  179]
train() client id: f_00007-9-0 loss: 0.488715  [   32/  179]
train() client id: f_00007-9-1 loss: 0.654157  [   64/  179]
train() client id: f_00007-9-2 loss: 0.602205  [   96/  179]
train() client id: f_00007-9-3 loss: 0.662566  [  128/  179]
train() client id: f_00007-9-4 loss: 0.435416  [  160/  179]
train() client id: f_00007-10-0 loss: 0.568166  [   32/  179]
train() client id: f_00007-10-1 loss: 0.710247  [   64/  179]
train() client id: f_00007-10-2 loss: 0.767570  [   96/  179]
train() client id: f_00007-10-3 loss: 0.442395  [  128/  179]
train() client id: f_00007-10-4 loss: 0.432218  [  160/  179]
train() client id: f_00007-11-0 loss: 0.684508  [   32/  179]
train() client id: f_00007-11-1 loss: 0.452931  [   64/  179]
train() client id: f_00007-11-2 loss: 0.546637  [   96/  179]
train() client id: f_00007-11-3 loss: 0.439476  [  128/  179]
train() client id: f_00007-11-4 loss: 0.662688  [  160/  179]
train() client id: f_00007-12-0 loss: 0.436360  [   32/  179]
train() client id: f_00007-12-1 loss: 0.540690  [   64/  179]
train() client id: f_00007-12-2 loss: 0.503728  [   96/  179]
train() client id: f_00007-12-3 loss: 0.449909  [  128/  179]
train() client id: f_00007-12-4 loss: 0.763017  [  160/  179]
train() client id: f_00007-13-0 loss: 0.559308  [   32/  179]
train() client id: f_00007-13-1 loss: 0.696494  [   64/  179]
train() client id: f_00007-13-2 loss: 0.421127  [   96/  179]
train() client id: f_00007-13-3 loss: 0.570660  [  128/  179]
train() client id: f_00007-13-4 loss: 0.547118  [  160/  179]
train() client id: f_00008-0-0 loss: 0.763047  [   32/  130]
train() client id: f_00008-0-1 loss: 0.796506  [   64/  130]
train() client id: f_00008-0-2 loss: 0.773530  [   96/  130]
train() client id: f_00008-0-3 loss: 0.676210  [  128/  130]
train() client id: f_00008-1-0 loss: 0.797990  [   32/  130]
train() client id: f_00008-1-1 loss: 0.749523  [   64/  130]
train() client id: f_00008-1-2 loss: 0.683716  [   96/  130]
train() client id: f_00008-1-3 loss: 0.733873  [  128/  130]
train() client id: f_00008-2-0 loss: 0.746463  [   32/  130]
train() client id: f_00008-2-1 loss: 0.798403  [   64/  130]
train() client id: f_00008-2-2 loss: 0.754655  [   96/  130]
train() client id: f_00008-2-3 loss: 0.698749  [  128/  130]
train() client id: f_00008-3-0 loss: 0.856027  [   32/  130]
train() client id: f_00008-3-1 loss: 0.694465  [   64/  130]
train() client id: f_00008-3-2 loss: 0.693720  [   96/  130]
train() client id: f_00008-3-3 loss: 0.759664  [  128/  130]
train() client id: f_00008-4-0 loss: 0.742427  [   32/  130]
train() client id: f_00008-4-1 loss: 0.664295  [   64/  130]
train() client id: f_00008-4-2 loss: 0.761824  [   96/  130]
train() client id: f_00008-4-3 loss: 0.792948  [  128/  130]
train() client id: f_00008-5-0 loss: 0.659656  [   32/  130]
train() client id: f_00008-5-1 loss: 0.723406  [   64/  130]
train() client id: f_00008-5-2 loss: 0.811958  [   96/  130]
train() client id: f_00008-5-3 loss: 0.766356  [  128/  130]
train() client id: f_00008-6-0 loss: 0.630417  [   32/  130]
train() client id: f_00008-6-1 loss: 0.846114  [   64/  130]
train() client id: f_00008-6-2 loss: 0.780764  [   96/  130]
train() client id: f_00008-6-3 loss: 0.736635  [  128/  130]
train() client id: f_00008-7-0 loss: 0.704253  [   32/  130]
train() client id: f_00008-7-1 loss: 0.736197  [   64/  130]
train() client id: f_00008-7-2 loss: 0.697014  [   96/  130]
train() client id: f_00008-7-3 loss: 0.853572  [  128/  130]
train() client id: f_00008-8-0 loss: 0.755217  [   32/  130]
train() client id: f_00008-8-1 loss: 0.672594  [   64/  130]
train() client id: f_00008-8-2 loss: 0.789424  [   96/  130]
train() client id: f_00008-8-3 loss: 0.774171  [  128/  130]
train() client id: f_00008-9-0 loss: 0.737458  [   32/  130]
train() client id: f_00008-9-1 loss: 0.680257  [   64/  130]
train() client id: f_00008-9-2 loss: 0.693175  [   96/  130]
train() client id: f_00008-9-3 loss: 0.858887  [  128/  130]
train() client id: f_00008-10-0 loss: 0.729340  [   32/  130]
train() client id: f_00008-10-1 loss: 0.701218  [   64/  130]
train() client id: f_00008-10-2 loss: 0.777540  [   96/  130]
train() client id: f_00008-10-3 loss: 0.784983  [  128/  130]
train() client id: f_00008-11-0 loss: 0.811569  [   32/  130]
train() client id: f_00008-11-1 loss: 0.682230  [   64/  130]
train() client id: f_00008-11-2 loss: 0.761479  [   96/  130]
train() client id: f_00008-11-3 loss: 0.738259  [  128/  130]
train() client id: f_00008-12-0 loss: 0.665852  [   32/  130]
train() client id: f_00008-12-1 loss: 0.721148  [   64/  130]
train() client id: f_00008-12-2 loss: 0.686458  [   96/  130]
train() client id: f_00008-12-3 loss: 0.917940  [  128/  130]
train() client id: f_00008-13-0 loss: 0.719846  [   32/  130]
train() client id: f_00008-13-1 loss: 0.756587  [   64/  130]
train() client id: f_00008-13-2 loss: 0.697881  [   96/  130]
train() client id: f_00008-13-3 loss: 0.817363  [  128/  130]
train() client id: f_00009-0-0 loss: 1.019286  [   32/  118]
train() client id: f_00009-0-1 loss: 1.040725  [   64/  118]
train() client id: f_00009-0-2 loss: 0.983220  [   96/  118]
train() client id: f_00009-1-0 loss: 1.020303  [   32/  118]
train() client id: f_00009-1-1 loss: 0.967750  [   64/  118]
train() client id: f_00009-1-2 loss: 0.977404  [   96/  118]
train() client id: f_00009-2-0 loss: 0.914908  [   32/  118]
train() client id: f_00009-2-1 loss: 1.078524  [   64/  118]
train() client id: f_00009-2-2 loss: 0.817364  [   96/  118]
train() client id: f_00009-3-0 loss: 0.914143  [   32/  118]
train() client id: f_00009-3-1 loss: 0.857699  [   64/  118]
train() client id: f_00009-3-2 loss: 0.835319  [   96/  118]
train() client id: f_00009-4-0 loss: 0.831378  [   32/  118]
train() client id: f_00009-4-1 loss: 0.867102  [   64/  118]
train() client id: f_00009-4-2 loss: 0.786857  [   96/  118]
train() client id: f_00009-5-0 loss: 0.843602  [   32/  118]
train() client id: f_00009-5-1 loss: 0.830523  [   64/  118]
train() client id: f_00009-5-2 loss: 0.807561  [   96/  118]
train() client id: f_00009-6-0 loss: 0.817402  [   32/  118]
train() client id: f_00009-6-1 loss: 0.730426  [   64/  118]
train() client id: f_00009-6-2 loss: 0.755507  [   96/  118]
train() client id: f_00009-7-0 loss: 0.727148  [   32/  118]
train() client id: f_00009-7-1 loss: 0.806145  [   64/  118]
train() client id: f_00009-7-2 loss: 0.720647  [   96/  118]
train() client id: f_00009-8-0 loss: 0.726217  [   32/  118]
train() client id: f_00009-8-1 loss: 0.711486  [   64/  118]
train() client id: f_00009-8-2 loss: 0.787387  [   96/  118]
train() client id: f_00009-9-0 loss: 0.786400  [   32/  118]
train() client id: f_00009-9-1 loss: 0.649935  [   64/  118]
train() client id: f_00009-9-2 loss: 0.706805  [   96/  118]
train() client id: f_00009-10-0 loss: 0.806956  [   32/  118]
train() client id: f_00009-10-1 loss: 0.775028  [   64/  118]
train() client id: f_00009-10-2 loss: 0.665722  [   96/  118]
train() client id: f_00009-11-0 loss: 0.666044  [   32/  118]
train() client id: f_00009-11-1 loss: 0.785118  [   64/  118]
train() client id: f_00009-11-2 loss: 0.704285  [   96/  118]
train() client id: f_00009-12-0 loss: 0.676629  [   32/  118]
train() client id: f_00009-12-1 loss: 0.827121  [   64/  118]
train() client id: f_00009-12-2 loss: 0.625805  [   96/  118]
train() client id: f_00009-13-0 loss: 0.671543  [   32/  118]
train() client id: f_00009-13-1 loss: 0.700310  [   64/  118]
train() client id: f_00009-13-2 loss: 0.781726  [   96/  118]
At round 26 accuracy: 0.6445623342175066
At round 26 training accuracy: 0.5835010060362174
At round 26 training loss: 0.8403659768723221
update_location
xs = 8.927491 251.223621 5.882650 10.934260 -167.581990 -15.230757 -5.849135 -5.143845 -190.120581 20.134486 
ys = -242.390647 7.291448 140.684448 -12.290817 -9.642386 0.794442 -21.381692 136.628436 25.881276 -677.232496 
xs mean: -8.68237997052122
ys mean: -65.16579882624052
dists_uav = 262.360298 270.493018 172.704139 101.344078 195.388584 101.156350 102.427482 169.392410 216.369304 684.871704 
uav_gains = -112.379121 -113.115258 -105.960242 -100.144979 -107.392615 -100.124848 -100.260435 -105.744103 -108.742941 -127.839437 
uav_gains_db_mean: -108.1703978194611
dists_bs = 456.118706 458.031367 184.108908 263.911726 184.791336 236.376368 259.186775 174.136413 149.883374 874.286964 
bs_gains = -114.021240 -114.072125 -102.989154 -107.367902 -103.034144 -106.027973 -107.148219 -102.311968 -100.488156 -121.933371 
bs_gains_db_mean: -107.93942516477587
Round 27
-------------------------------
ene_coms = [0.01271888 0.01349582 0.00719333 0.00634057 0.00720863 0.00633525
 0.00637128 0.00697046 0.00643272 0.03641902]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.51093973 15.54332939  7.2899027   2.60150741 17.78796924  8.54245342
  3.23657561 10.45804449  7.62143651  7.50276694]
obj_prev = 88.0949254313018
eta_min = 3.3076735212841165e-13	eta_max = 0.7555234557155275
af = 18.488129690050464	bf = 2.0729925336871395	zeta = 20.336942659055513	eta = 0.909090909090909
af = 18.488129690050464	bf = 2.0729925336871395	zeta = 42.18609448560674	eta = 0.4382517489586133
af = 18.488129690050464	bf = 2.0729925336871395	zeta = 30.84120438527741	eta = 0.5994619879007091
af = 18.488129690050464	bf = 2.0729925336871395	zeta = 28.795739894894282	eta = 0.6420439189106775
af = 18.488129690050464	bf = 2.0729925336871395	zeta = 28.676859728748255	eta = 0.6447055174425638
af = 18.488129690050464	bf = 2.0729925336871395	zeta = 28.676415062469054	eta = 0.6447155144663549
af = 18.488129690050464	bf = 2.0729925336871395	zeta = 28.676415056213063	eta = 0.6447155146070047
eta = 0.6447155146070047
ene_coms = [0.01271888 0.01349582 0.00719333 0.00634057 0.00720863 0.00633525
 0.00637128 0.00697046 0.00643272 0.03641902]
ene_comp = [0.03622078 0.07617863 0.03564585 0.01236106 0.08796476 0.04197011
 0.01552319 0.05145651 0.03737065 0.03392105]
ene_total = [2.60809875 4.77894208 2.28299079 0.99664962 5.07199204 2.57429512
 1.166803   3.11369721 2.33437479 3.74857166]
ti_comp = [0.40993996 0.40217056 0.46519551 0.4737231  0.46504248 0.47377635
 0.47341599 0.4674242  0.47280155 0.17293862]
ti_coms = [0.12718884 0.13495824 0.07193329 0.0634057  0.07208632 0.06335245
 0.06371281 0.0697046  0.06432725 0.36419018]
t_total = [28.62473488 28.62473488 28.62473488 28.62473488 28.62473488 28.62473488
 28.62473488 28.62473488 28.62473488 28.62473488]
ene_coms = [0.01271888 0.01349582 0.00719333 0.00634057 0.00720863 0.00633525
 0.00637128 0.00697046 0.00643272 0.03641902]
ene_comp = [1.76731093e-05 1.70827943e-04 1.30808471e-05 5.26014949e-07
 1.96707479e-04 2.05851321e-05 1.04312723e-06 3.89743197e-05
 1.45919762e-05 8.15650114e-05]
ene_total = [0.67875817 0.72832491 0.38404489 0.33793048 0.39464628 0.3387157
 0.3395947  0.37354763 0.34359121 1.94519363]
optimize_network iter = 0 obj = 5.864347588021289
eta = 0.6447155146070047
freqs = [44178154.25912695 94709354.43641855 38312760.91340925 13046714.92788263
 94577119.09839286 44293169.54457252 16394875.35046778 55042622.00402122
 39520436.97206013 98072513.18761629]
eta_min = 0.644715514607006	eta_max = 0.6447155146070045
af = 0.023964629732100586	bf = 2.0729925336871395	zeta = 0.026361092705310646	eta = 0.9090909090909091
af = 0.023964629732100586	bf = 2.0729925336871395	zeta = 22.828042675414345	eta = 0.001049789071837961
af = 0.023964629732100586	bf = 2.0729925336871395	zeta = 2.2397211423816734	eta = 0.010699827437721595
af = 0.023964629732100586	bf = 2.0729925336871395	zeta = 2.2053293475390814	eta = 0.0108666897118304
af = 0.023964629732100586	bf = 2.0729925336871395	zeta = 2.20532646163274	eta = 0.010866703932060056
eta = 0.010866703932060056
ene_coms = [0.01271888 0.01349582 0.00719333 0.00634057 0.00720863 0.00633525
 0.00637128 0.00697046 0.00643272 0.03641902]
ene_comp = [1.82069536e-04 1.75988072e-03 1.34759748e-04 5.41904064e-06
 2.02649340e-03 2.12069386e-04 1.07463654e-05 4.01516008e-04
 1.50327500e-04 8.40288118e-04]
ene_total = [0.24694842 0.29202278 0.14027335 0.12147412 0.17677761 0.12532787
 0.12216397 0.14111343 0.12601196 0.71321295]
ti_comp = [0.40993996 0.40217056 0.46519551 0.4737231  0.46504248 0.47377635
 0.47341599 0.4674242  0.47280155 0.17293862]
ti_coms = [0.12718884 0.13495824 0.07193329 0.0634057  0.07208632 0.06335245
 0.06371281 0.0697046  0.06432725 0.36419018]
t_total = [28.62473488 28.62473488 28.62473488 28.62473488 28.62473488 28.62473488
 28.62473488 28.62473488 28.62473488 28.62473488]
ene_coms = [0.01271888 0.01349582 0.00719333 0.00634057 0.00720863 0.00633525
 0.00637128 0.00697046 0.00643272 0.03641902]
ene_comp = [1.76731093e-05 1.70827943e-04 1.30808471e-05 5.26014949e-07
 1.96707479e-04 2.05851321e-05 1.04312723e-06 3.89743197e-05
 1.45919762e-05 8.15650114e-05]
ene_total = [0.67875817 0.72832491 0.38404489 0.33793048 0.39464628 0.3387157
 0.3395947  0.37354763 0.34359121 1.94519363]
optimize_network iter = 1 obj = 5.86434758802131
eta = 0.644715514607006
freqs = [44178154.25912695 94709354.43641855 38312760.91340922 13046714.92788262
 94577119.09839278 44293169.54457249 16394875.35046777 55042622.00402118
 39520436.97206011 98072513.18761694]
Done!
ene_coms = [0.01271888 0.01349582 0.00719333 0.00634057 0.00720863 0.00633525
 0.00637128 0.00697046 0.00643272 0.03641902]
ene_comp = [1.72140761e-05 1.66390937e-04 1.27410912e-05 5.12352479e-07
 1.91598290e-04 2.00504634e-05 1.01603352e-06 3.79620187e-05
 1.42129709e-05 7.94464794e-05]
ene_total = [0.0127361  0.01366221 0.00720607 0.00634108 0.00740023 0.0063553
 0.0063723  0.00700842 0.00644694 0.03649846]
At round 27 energy consumption: 0.11002711257941736
At round 27 eta: 0.644715514607006
At round 27 a_n: 18.93386498808281
At round 27 local rounds: 14.373326151554522
At round 27 global rounds: 53.292124386290965
gradient difference: 0.4084780812263489
train() client id: f_00000-0-0 loss: 1.279146  [   32/  126]
train() client id: f_00000-0-1 loss: 1.182453  [   64/  126]
train() client id: f_00000-0-2 loss: 1.132324  [   96/  126]
train() client id: f_00000-1-0 loss: 1.049745  [   32/  126]
train() client id: f_00000-1-1 loss: 1.176172  [   64/  126]
train() client id: f_00000-1-2 loss: 1.076795  [   96/  126]
train() client id: f_00000-2-0 loss: 1.017207  [   32/  126]
train() client id: f_00000-2-1 loss: 1.009459  [   64/  126]
train() client id: f_00000-2-2 loss: 0.898983  [   96/  126]
train() client id: f_00000-3-0 loss: 1.080622  [   32/  126]
train() client id: f_00000-3-1 loss: 0.942407  [   64/  126]
train() client id: f_00000-3-2 loss: 0.906659  [   96/  126]
train() client id: f_00000-4-0 loss: 0.970832  [   32/  126]
train() client id: f_00000-4-1 loss: 0.895804  [   64/  126]
train() client id: f_00000-4-2 loss: 0.857126  [   96/  126]
train() client id: f_00000-5-0 loss: 0.908389  [   32/  126]
train() client id: f_00000-5-1 loss: 1.046326  [   64/  126]
train() client id: f_00000-5-2 loss: 0.747208  [   96/  126]
train() client id: f_00000-6-0 loss: 0.922292  [   32/  126]
train() client id: f_00000-6-1 loss: 0.790125  [   64/  126]
train() client id: f_00000-6-2 loss: 0.951345  [   96/  126]
train() client id: f_00000-7-0 loss: 0.819644  [   32/  126]
train() client id: f_00000-7-1 loss: 0.881321  [   64/  126]
train() client id: f_00000-7-2 loss: 0.810997  [   96/  126]
train() client id: f_00000-8-0 loss: 0.828509  [   32/  126]
train() client id: f_00000-8-1 loss: 0.840399  [   64/  126]
train() client id: f_00000-8-2 loss: 0.865507  [   96/  126]
train() client id: f_00000-9-0 loss: 0.870311  [   32/  126]
train() client id: f_00000-9-1 loss: 0.740433  [   64/  126]
train() client id: f_00000-9-2 loss: 0.835624  [   96/  126]
train() client id: f_00000-10-0 loss: 0.819790  [   32/  126]
train() client id: f_00000-10-1 loss: 0.781918  [   64/  126]
train() client id: f_00000-10-2 loss: 0.876458  [   96/  126]
train() client id: f_00000-11-0 loss: 0.919979  [   32/  126]
train() client id: f_00000-11-1 loss: 0.811737  [   64/  126]
train() client id: f_00000-11-2 loss: 0.783871  [   96/  126]
train() client id: f_00000-12-0 loss: 0.865567  [   32/  126]
train() client id: f_00000-12-1 loss: 0.834032  [   64/  126]
train() client id: f_00000-12-2 loss: 0.841051  [   96/  126]
train() client id: f_00000-13-0 loss: 0.749588  [   32/  126]
train() client id: f_00000-13-1 loss: 1.001051  [   64/  126]
train() client id: f_00000-13-2 loss: 0.759178  [   96/  126]
train() client id: f_00001-0-0 loss: 0.401956  [   32/  265]
train() client id: f_00001-0-1 loss: 0.358268  [   64/  265]
train() client id: f_00001-0-2 loss: 0.428412  [   96/  265]
train() client id: f_00001-0-3 loss: 0.444298  [  128/  265]
train() client id: f_00001-0-4 loss: 0.297207  [  160/  265]
train() client id: f_00001-0-5 loss: 0.425206  [  192/  265]
train() client id: f_00001-0-6 loss: 0.373918  [  224/  265]
train() client id: f_00001-0-7 loss: 0.312736  [  256/  265]
train() client id: f_00001-1-0 loss: 0.501141  [   32/  265]
train() client id: f_00001-1-1 loss: 0.345734  [   64/  265]
train() client id: f_00001-1-2 loss: 0.293729  [   96/  265]
train() client id: f_00001-1-3 loss: 0.408067  [  128/  265]
train() client id: f_00001-1-4 loss: 0.329282  [  160/  265]
train() client id: f_00001-1-5 loss: 0.339105  [  192/  265]
train() client id: f_00001-1-6 loss: 0.391274  [  224/  265]
train() client id: f_00001-1-7 loss: 0.359897  [  256/  265]
train() client id: f_00001-2-0 loss: 0.551064  [   32/  265]
train() client id: f_00001-2-1 loss: 0.374998  [   64/  265]
train() client id: f_00001-2-2 loss: 0.304023  [   96/  265]
train() client id: f_00001-2-3 loss: 0.273711  [  128/  265]
train() client id: f_00001-2-4 loss: 0.322547  [  160/  265]
train() client id: f_00001-2-5 loss: 0.262117  [  192/  265]
train() client id: f_00001-2-6 loss: 0.390860  [  224/  265]
train() client id: f_00001-2-7 loss: 0.258711  [  256/  265]
train() client id: f_00001-3-0 loss: 0.290980  [   32/  265]
train() client id: f_00001-3-1 loss: 0.423558  [   64/  265]
train() client id: f_00001-3-2 loss: 0.335856  [   96/  265]
train() client id: f_00001-3-3 loss: 0.369828  [  128/  265]
train() client id: f_00001-3-4 loss: 0.355030  [  160/  265]
train() client id: f_00001-3-5 loss: 0.347468  [  192/  265]
train() client id: f_00001-3-6 loss: 0.266421  [  224/  265]
train() client id: f_00001-3-7 loss: 0.458640  [  256/  265]
train() client id: f_00001-4-0 loss: 0.247806  [   32/  265]
train() client id: f_00001-4-1 loss: 0.388243  [   64/  265]
train() client id: f_00001-4-2 loss: 0.314013  [   96/  265]
train() client id: f_00001-4-3 loss: 0.384350  [  128/  265]
train() client id: f_00001-4-4 loss: 0.425162  [  160/  265]
train() client id: f_00001-4-5 loss: 0.318907  [  192/  265]
train() client id: f_00001-4-6 loss: 0.432124  [  224/  265]
train() client id: f_00001-4-7 loss: 0.293407  [  256/  265]
train() client id: f_00001-5-0 loss: 0.325247  [   32/  265]
train() client id: f_00001-5-1 loss: 0.367890  [   64/  265]
train() client id: f_00001-5-2 loss: 0.309894  [   96/  265]
train() client id: f_00001-5-3 loss: 0.385994  [  128/  265]
train() client id: f_00001-5-4 loss: 0.277917  [  160/  265]
train() client id: f_00001-5-5 loss: 0.398359  [  192/  265]
train() client id: f_00001-5-6 loss: 0.367151  [  224/  265]
train() client id: f_00001-5-7 loss: 0.334560  [  256/  265]
train() client id: f_00001-6-0 loss: 0.380701  [   32/  265]
train() client id: f_00001-6-1 loss: 0.341153  [   64/  265]
train() client id: f_00001-6-2 loss: 0.433610  [   96/  265]
train() client id: f_00001-6-3 loss: 0.358095  [  128/  265]
train() client id: f_00001-6-4 loss: 0.243794  [  160/  265]
train() client id: f_00001-6-5 loss: 0.327792  [  192/  265]
train() client id: f_00001-6-6 loss: 0.259871  [  224/  265]
train() client id: f_00001-6-7 loss: 0.397662  [  256/  265]
train() client id: f_00001-7-0 loss: 0.453302  [   32/  265]
train() client id: f_00001-7-1 loss: 0.294999  [   64/  265]
train() client id: f_00001-7-2 loss: 0.409178  [   96/  265]
train() client id: f_00001-7-3 loss: 0.287169  [  128/  265]
train() client id: f_00001-7-4 loss: 0.310445  [  160/  265]
train() client id: f_00001-7-5 loss: 0.339081  [  192/  265]
train() client id: f_00001-7-6 loss: 0.365499  [  224/  265]
train() client id: f_00001-7-7 loss: 0.268895  [  256/  265]
train() client id: f_00001-8-0 loss: 0.337345  [   32/  265]
train() client id: f_00001-8-1 loss: 0.267528  [   64/  265]
train() client id: f_00001-8-2 loss: 0.438381  [   96/  265]
train() client id: f_00001-8-3 loss: 0.336130  [  128/  265]
train() client id: f_00001-8-4 loss: 0.247468  [  160/  265]
train() client id: f_00001-8-5 loss: 0.339972  [  192/  265]
train() client id: f_00001-8-6 loss: 0.347204  [  224/  265]
train() client id: f_00001-8-7 loss: 0.319005  [  256/  265]
train() client id: f_00001-9-0 loss: 0.251704  [   32/  265]
train() client id: f_00001-9-1 loss: 0.312543  [   64/  265]
train() client id: f_00001-9-2 loss: 0.275867  [   96/  265]
train() client id: f_00001-9-3 loss: 0.421904  [  128/  265]
train() client id: f_00001-9-4 loss: 0.304813  [  160/  265]
train() client id: f_00001-9-5 loss: 0.224999  [  192/  265]
train() client id: f_00001-9-6 loss: 0.386453  [  224/  265]
train() client id: f_00001-9-7 loss: 0.411097  [  256/  265]
train() client id: f_00001-10-0 loss: 0.261919  [   32/  265]
train() client id: f_00001-10-1 loss: 0.319507  [   64/  265]
train() client id: f_00001-10-2 loss: 0.494662  [   96/  265]
train() client id: f_00001-10-3 loss: 0.278112  [  128/  265]
train() client id: f_00001-10-4 loss: 0.305732  [  160/  265]
train() client id: f_00001-10-5 loss: 0.325633  [  192/  265]
train() client id: f_00001-10-6 loss: 0.313866  [  224/  265]
train() client id: f_00001-10-7 loss: 0.290623  [  256/  265]
train() client id: f_00001-11-0 loss: 0.273507  [   32/  265]
train() client id: f_00001-11-1 loss: 0.360840  [   64/  265]
train() client id: f_00001-11-2 loss: 0.495216  [   96/  265]
train() client id: f_00001-11-3 loss: 0.233811  [  128/  265]
train() client id: f_00001-11-4 loss: 0.243508  [  160/  265]
train() client id: f_00001-11-5 loss: 0.306579  [  192/  265]
train() client id: f_00001-11-6 loss: 0.300754  [  224/  265]
train() client id: f_00001-11-7 loss: 0.286436  [  256/  265]
train() client id: f_00001-12-0 loss: 0.252576  [   32/  265]
train() client id: f_00001-12-1 loss: 0.421213  [   64/  265]
train() client id: f_00001-12-2 loss: 0.344673  [   96/  265]
train() client id: f_00001-12-3 loss: 0.345228  [  128/  265]
train() client id: f_00001-12-4 loss: 0.225235  [  160/  265]
train() client id: f_00001-12-5 loss: 0.292187  [  192/  265]
train() client id: f_00001-12-6 loss: 0.334716  [  224/  265]
train() client id: f_00001-12-7 loss: 0.441315  [  256/  265]
train() client id: f_00001-13-0 loss: 0.235910  [   32/  265]
train() client id: f_00001-13-1 loss: 0.311076  [   64/  265]
train() client id: f_00001-13-2 loss: 0.301017  [   96/  265]
train() client id: f_00001-13-3 loss: 0.344901  [  128/  265]
train() client id: f_00001-13-4 loss: 0.312786  [  160/  265]
train() client id: f_00001-13-5 loss: 0.505896  [  192/  265]
train() client id: f_00001-13-6 loss: 0.227503  [  224/  265]
train() client id: f_00001-13-7 loss: 0.324994  [  256/  265]
train() client id: f_00002-0-0 loss: 1.155163  [   32/  124]
train() client id: f_00002-0-1 loss: 1.334313  [   64/  124]
train() client id: f_00002-0-2 loss: 1.296571  [   96/  124]
train() client id: f_00002-1-0 loss: 1.115074  [   32/  124]
train() client id: f_00002-1-1 loss: 1.322538  [   64/  124]
train() client id: f_00002-1-2 loss: 1.188691  [   96/  124]
train() client id: f_00002-2-0 loss: 1.261697  [   32/  124]
train() client id: f_00002-2-1 loss: 1.104785  [   64/  124]
train() client id: f_00002-2-2 loss: 1.219072  [   96/  124]
train() client id: f_00002-3-0 loss: 1.373906  [   32/  124]
train() client id: f_00002-3-1 loss: 1.111006  [   64/  124]
train() client id: f_00002-3-2 loss: 1.040531  [   96/  124]
train() client id: f_00002-4-0 loss: 1.262434  [   32/  124]
train() client id: f_00002-4-1 loss: 1.143975  [   64/  124]
train() client id: f_00002-4-2 loss: 1.060670  [   96/  124]
train() client id: f_00002-5-0 loss: 1.066215  [   32/  124]
train() client id: f_00002-5-1 loss: 1.180805  [   64/  124]
train() client id: f_00002-5-2 loss: 1.178414  [   96/  124]
train() client id: f_00002-6-0 loss: 1.199391  [   32/  124]
train() client id: f_00002-6-1 loss: 1.083481  [   64/  124]
train() client id: f_00002-6-2 loss: 0.951972  [   96/  124]
train() client id: f_00002-7-0 loss: 1.080279  [   32/  124]
train() client id: f_00002-7-1 loss: 1.052591  [   64/  124]
train() client id: f_00002-7-2 loss: 1.126313  [   96/  124]
train() client id: f_00002-8-0 loss: 1.075084  [   32/  124]
train() client id: f_00002-8-1 loss: 1.147803  [   64/  124]
train() client id: f_00002-8-2 loss: 0.991219  [   96/  124]
train() client id: f_00002-9-0 loss: 1.194901  [   32/  124]
train() client id: f_00002-9-1 loss: 1.016736  [   64/  124]
train() client id: f_00002-9-2 loss: 0.954440  [   96/  124]
train() client id: f_00002-10-0 loss: 1.089253  [   32/  124]
train() client id: f_00002-10-1 loss: 1.011116  [   64/  124]
train() client id: f_00002-10-2 loss: 1.019014  [   96/  124]
train() client id: f_00002-11-0 loss: 0.920343  [   32/  124]
train() client id: f_00002-11-1 loss: 1.271561  [   64/  124]
train() client id: f_00002-11-2 loss: 0.941414  [   96/  124]
train() client id: f_00002-12-0 loss: 1.153059  [   32/  124]
train() client id: f_00002-12-1 loss: 1.027412  [   64/  124]
train() client id: f_00002-12-2 loss: 1.018530  [   96/  124]
train() client id: f_00002-13-0 loss: 0.976105  [   32/  124]
train() client id: f_00002-13-1 loss: 1.014184  [   64/  124]
train() client id: f_00002-13-2 loss: 1.011046  [   96/  124]
train() client id: f_00003-0-0 loss: 0.644287  [   32/   43]
train() client id: f_00003-1-0 loss: 0.814428  [   32/   43]
train() client id: f_00003-2-0 loss: 0.784368  [   32/   43]
train() client id: f_00003-3-0 loss: 0.787347  [   32/   43]
train() client id: f_00003-4-0 loss: 0.709287  [   32/   43]
train() client id: f_00003-5-0 loss: 0.777024  [   32/   43]
train() client id: f_00003-6-0 loss: 0.576099  [   32/   43]
train() client id: f_00003-7-0 loss: 0.520269  [   32/   43]
train() client id: f_00003-8-0 loss: 0.680828  [   32/   43]
train() client id: f_00003-9-0 loss: 0.798920  [   32/   43]
train() client id: f_00003-10-0 loss: 0.777851  [   32/   43]
train() client id: f_00003-11-0 loss: 0.645745  [   32/   43]
train() client id: f_00003-12-0 loss: 0.863363  [   32/   43]
train() client id: f_00003-13-0 loss: 0.724453  [   32/   43]
train() client id: f_00004-0-0 loss: 0.971481  [   32/  306]
train() client id: f_00004-0-1 loss: 1.078562  [   64/  306]
train() client id: f_00004-0-2 loss: 0.933314  [   96/  306]
train() client id: f_00004-0-3 loss: 0.912138  [  128/  306]
train() client id: f_00004-0-4 loss: 0.941003  [  160/  306]
train() client id: f_00004-0-5 loss: 0.973501  [  192/  306]
train() client id: f_00004-0-6 loss: 1.167877  [  224/  306]
train() client id: f_00004-0-7 loss: 1.096141  [  256/  306]
train() client id: f_00004-0-8 loss: 0.827758  [  288/  306]
train() client id: f_00004-1-0 loss: 0.956135  [   32/  306]
train() client id: f_00004-1-1 loss: 1.150819  [   64/  306]
train() client id: f_00004-1-2 loss: 1.014297  [   96/  306]
train() client id: f_00004-1-3 loss: 0.984856  [  128/  306]
train() client id: f_00004-1-4 loss: 0.872349  [  160/  306]
train() client id: f_00004-1-5 loss: 0.841921  [  192/  306]
train() client id: f_00004-1-6 loss: 1.040695  [  224/  306]
train() client id: f_00004-1-7 loss: 0.908967  [  256/  306]
train() client id: f_00004-1-8 loss: 1.015504  [  288/  306]
train() client id: f_00004-2-0 loss: 1.059848  [   32/  306]
train() client id: f_00004-2-1 loss: 0.840489  [   64/  306]
train() client id: f_00004-2-2 loss: 1.080889  [   96/  306]
train() client id: f_00004-2-3 loss: 0.990283  [  128/  306]
train() client id: f_00004-2-4 loss: 0.998934  [  160/  306]
train() client id: f_00004-2-5 loss: 0.926711  [  192/  306]
train() client id: f_00004-2-6 loss: 0.921988  [  224/  306]
train() client id: f_00004-2-7 loss: 0.987254  [  256/  306]
train() client id: f_00004-2-8 loss: 0.978982  [  288/  306]
train() client id: f_00004-3-0 loss: 0.835137  [   32/  306]
train() client id: f_00004-3-1 loss: 0.990518  [   64/  306]
train() client id: f_00004-3-2 loss: 0.917693  [   96/  306]
train() client id: f_00004-3-3 loss: 0.957658  [  128/  306]
train() client id: f_00004-3-4 loss: 0.938788  [  160/  306]
train() client id: f_00004-3-5 loss: 1.054516  [  192/  306]
train() client id: f_00004-3-6 loss: 0.988449  [  224/  306]
train() client id: f_00004-3-7 loss: 0.991242  [  256/  306]
train() client id: f_00004-3-8 loss: 0.982877  [  288/  306]
train() client id: f_00004-4-0 loss: 1.088194  [   32/  306]
train() client id: f_00004-4-1 loss: 0.900482  [   64/  306]
train() client id: f_00004-4-2 loss: 0.878715  [   96/  306]
train() client id: f_00004-4-3 loss: 0.918880  [  128/  306]
train() client id: f_00004-4-4 loss: 0.843962  [  160/  306]
train() client id: f_00004-4-5 loss: 1.108087  [  192/  306]
train() client id: f_00004-4-6 loss: 0.904268  [  224/  306]
train() client id: f_00004-4-7 loss: 0.906268  [  256/  306]
train() client id: f_00004-4-8 loss: 1.113416  [  288/  306]
train() client id: f_00004-5-0 loss: 0.942343  [   32/  306]
train() client id: f_00004-5-1 loss: 0.937011  [   64/  306]
train() client id: f_00004-5-2 loss: 0.940087  [   96/  306]
train() client id: f_00004-5-3 loss: 0.998381  [  128/  306]
train() client id: f_00004-5-4 loss: 0.936368  [  160/  306]
train() client id: f_00004-5-5 loss: 0.948658  [  192/  306]
train() client id: f_00004-5-6 loss: 1.068622  [  224/  306]
train() client id: f_00004-5-7 loss: 0.900832  [  256/  306]
train() client id: f_00004-5-8 loss: 0.901513  [  288/  306]
train() client id: f_00004-6-0 loss: 1.027457  [   32/  306]
train() client id: f_00004-6-1 loss: 1.065140  [   64/  306]
train() client id: f_00004-6-2 loss: 0.852927  [   96/  306]
train() client id: f_00004-6-3 loss: 0.856405  [  128/  306]
train() client id: f_00004-6-4 loss: 0.999000  [  160/  306]
train() client id: f_00004-6-5 loss: 0.884484  [  192/  306]
train() client id: f_00004-6-6 loss: 0.967343  [  224/  306]
train() client id: f_00004-6-7 loss: 1.049947  [  256/  306]
train() client id: f_00004-6-8 loss: 0.966594  [  288/  306]
train() client id: f_00004-7-0 loss: 1.126922  [   32/  306]
train() client id: f_00004-7-1 loss: 1.011612  [   64/  306]
train() client id: f_00004-7-2 loss: 0.779421  [   96/  306]
train() client id: f_00004-7-3 loss: 0.938936  [  128/  306]
train() client id: f_00004-7-4 loss: 0.928915  [  160/  306]
train() client id: f_00004-7-5 loss: 0.978748  [  192/  306]
train() client id: f_00004-7-6 loss: 1.083037  [  224/  306]
train() client id: f_00004-7-7 loss: 0.935602  [  256/  306]
train() client id: f_00004-7-8 loss: 0.879314  [  288/  306]
train() client id: f_00004-8-0 loss: 0.937795  [   32/  306]
train() client id: f_00004-8-1 loss: 0.969598  [   64/  306]
train() client id: f_00004-8-2 loss: 0.949281  [   96/  306]
train() client id: f_00004-8-3 loss: 1.019457  [  128/  306]
train() client id: f_00004-8-4 loss: 0.896764  [  160/  306]
train() client id: f_00004-8-5 loss: 0.939323  [  192/  306]
train() client id: f_00004-8-6 loss: 0.958596  [  224/  306]
train() client id: f_00004-8-7 loss: 0.987191  [  256/  306]
train() client id: f_00004-8-8 loss: 0.936353  [  288/  306]
train() client id: f_00004-9-0 loss: 0.906437  [   32/  306]
train() client id: f_00004-9-1 loss: 1.022501  [   64/  306]
train() client id: f_00004-9-2 loss: 0.994854  [   96/  306]
train() client id: f_00004-9-3 loss: 0.926604  [  128/  306]
train() client id: f_00004-9-4 loss: 0.873246  [  160/  306]
train() client id: f_00004-9-5 loss: 0.928279  [  192/  306]
train() client id: f_00004-9-6 loss: 0.916384  [  224/  306]
train() client id: f_00004-9-7 loss: 1.002186  [  256/  306]
train() client id: f_00004-9-8 loss: 0.906402  [  288/  306]
train() client id: f_00004-10-0 loss: 0.998460  [   32/  306]
train() client id: f_00004-10-1 loss: 1.030073  [   64/  306]
train() client id: f_00004-10-2 loss: 1.022065  [   96/  306]
train() client id: f_00004-10-3 loss: 0.969415  [  128/  306]
train() client id: f_00004-10-4 loss: 0.862640  [  160/  306]
train() client id: f_00004-10-5 loss: 0.821252  [  192/  306]
train() client id: f_00004-10-6 loss: 0.907183  [  224/  306]
train() client id: f_00004-10-7 loss: 0.983177  [  256/  306]
train() client id: f_00004-10-8 loss: 0.969466  [  288/  306]
train() client id: f_00004-11-0 loss: 0.887755  [   32/  306]
train() client id: f_00004-11-1 loss: 0.904174  [   64/  306]
train() client id: f_00004-11-2 loss: 0.957476  [   96/  306]
train() client id: f_00004-11-3 loss: 1.049747  [  128/  306]
train() client id: f_00004-11-4 loss: 0.841390  [  160/  306]
train() client id: f_00004-11-5 loss: 0.954873  [  192/  306]
train() client id: f_00004-11-6 loss: 0.987666  [  224/  306]
train() client id: f_00004-11-7 loss: 0.933220  [  256/  306]
train() client id: f_00004-11-8 loss: 1.074691  [  288/  306]
train() client id: f_00004-12-0 loss: 0.893606  [   32/  306]
train() client id: f_00004-12-1 loss: 0.968831  [   64/  306]
train() client id: f_00004-12-2 loss: 0.940452  [   96/  306]
train() client id: f_00004-12-3 loss: 1.000754  [  128/  306]
train() client id: f_00004-12-4 loss: 0.940766  [  160/  306]
train() client id: f_00004-12-5 loss: 0.896319  [  192/  306]
train() client id: f_00004-12-6 loss: 1.042346  [  224/  306]
train() client id: f_00004-12-7 loss: 0.955495  [  256/  306]
train() client id: f_00004-12-8 loss: 0.955925  [  288/  306]
train() client id: f_00004-13-0 loss: 0.985829  [   32/  306]
train() client id: f_00004-13-1 loss: 0.969506  [   64/  306]
train() client id: f_00004-13-2 loss: 0.923630  [   96/  306]
train() client id: f_00004-13-3 loss: 0.930708  [  128/  306]
train() client id: f_00004-13-4 loss: 0.947641  [  160/  306]
train() client id: f_00004-13-5 loss: 1.017424  [  192/  306]
train() client id: f_00004-13-6 loss: 0.988771  [  224/  306]
train() client id: f_00004-13-7 loss: 0.849303  [  256/  306]
train() client id: f_00004-13-8 loss: 0.886358  [  288/  306]
train() client id: f_00005-0-0 loss: 0.749006  [   32/  146]
train() client id: f_00005-0-1 loss: 0.897757  [   64/  146]
train() client id: f_00005-0-2 loss: 0.742971  [   96/  146]
train() client id: f_00005-0-3 loss: 0.640076  [  128/  146]
train() client id: f_00005-1-0 loss: 0.746267  [   32/  146]
train() client id: f_00005-1-1 loss: 0.809387  [   64/  146]
train() client id: f_00005-1-2 loss: 0.959917  [   96/  146]
train() client id: f_00005-1-3 loss: 0.739788  [  128/  146]
train() client id: f_00005-2-0 loss: 0.802661  [   32/  146]
train() client id: f_00005-2-1 loss: 0.947817  [   64/  146]
train() client id: f_00005-2-2 loss: 0.734782  [   96/  146]
train() client id: f_00005-2-3 loss: 0.684257  [  128/  146]
train() client id: f_00005-3-0 loss: 0.705718  [   32/  146]
train() client id: f_00005-3-1 loss: 0.475712  [   64/  146]
train() client id: f_00005-3-2 loss: 0.975789  [   96/  146]
train() client id: f_00005-3-3 loss: 0.937817  [  128/  146]
train() client id: f_00005-4-0 loss: 0.808209  [   32/  146]
train() client id: f_00005-4-1 loss: 0.919321  [   64/  146]
train() client id: f_00005-4-2 loss: 0.623307  [   96/  146]
train() client id: f_00005-4-3 loss: 0.774783  [  128/  146]
train() client id: f_00005-5-0 loss: 0.635507  [   32/  146]
train() client id: f_00005-5-1 loss: 0.792985  [   64/  146]
train() client id: f_00005-5-2 loss: 0.776521  [   96/  146]
train() client id: f_00005-5-3 loss: 0.775444  [  128/  146]
train() client id: f_00005-6-0 loss: 0.896238  [   32/  146]
train() client id: f_00005-6-1 loss: 0.713984  [   64/  146]
train() client id: f_00005-6-2 loss: 0.854717  [   96/  146]
train() client id: f_00005-6-3 loss: 0.872230  [  128/  146]
train() client id: f_00005-7-0 loss: 0.782098  [   32/  146]
train() client id: f_00005-7-1 loss: 0.679978  [   64/  146]
train() client id: f_00005-7-2 loss: 0.929633  [   96/  146]
train() client id: f_00005-7-3 loss: 0.829428  [  128/  146]
train() client id: f_00005-8-0 loss: 0.816157  [   32/  146]
train() client id: f_00005-8-1 loss: 0.876907  [   64/  146]
train() client id: f_00005-8-2 loss: 0.663608  [   96/  146]
train() client id: f_00005-8-3 loss: 0.715079  [  128/  146]
train() client id: f_00005-9-0 loss: 0.709723  [   32/  146]
train() client id: f_00005-9-1 loss: 0.685331  [   64/  146]
train() client id: f_00005-9-2 loss: 0.872540  [   96/  146]
train() client id: f_00005-9-3 loss: 0.746929  [  128/  146]
train() client id: f_00005-10-0 loss: 0.863039  [   32/  146]
train() client id: f_00005-10-1 loss: 0.651644  [   64/  146]
train() client id: f_00005-10-2 loss: 1.041365  [   96/  146]
train() client id: f_00005-10-3 loss: 0.675179  [  128/  146]
train() client id: f_00005-11-0 loss: 0.976826  [   32/  146]
train() client id: f_00005-11-1 loss: 0.574398  [   64/  146]
train() client id: f_00005-11-2 loss: 0.995597  [   96/  146]
train() client id: f_00005-11-3 loss: 0.750581  [  128/  146]
train() client id: f_00005-12-0 loss: 0.744297  [   32/  146]
train() client id: f_00005-12-1 loss: 0.618473  [   64/  146]
train() client id: f_00005-12-2 loss: 0.873885  [   96/  146]
train() client id: f_00005-12-3 loss: 0.880607  [  128/  146]
train() client id: f_00005-13-0 loss: 0.804429  [   32/  146]
train() client id: f_00005-13-1 loss: 0.738720  [   64/  146]
train() client id: f_00005-13-2 loss: 0.669879  [   96/  146]
train() client id: f_00005-13-3 loss: 0.777142  [  128/  146]
train() client id: f_00006-0-0 loss: 0.599864  [   32/   54]
train() client id: f_00006-1-0 loss: 0.588519  [   32/   54]
train() client id: f_00006-2-0 loss: 0.552198  [   32/   54]
train() client id: f_00006-3-0 loss: 0.548975  [   32/   54]
train() client id: f_00006-4-0 loss: 0.561588  [   32/   54]
train() client id: f_00006-5-0 loss: 0.562245  [   32/   54]
train() client id: f_00006-6-0 loss: 0.551651  [   32/   54]
train() client id: f_00006-7-0 loss: 0.544920  [   32/   54]
train() client id: f_00006-8-0 loss: 0.558146  [   32/   54]
train() client id: f_00006-9-0 loss: 0.532719  [   32/   54]
train() client id: f_00006-10-0 loss: 0.533430  [   32/   54]
train() client id: f_00006-11-0 loss: 0.549432  [   32/   54]
train() client id: f_00006-12-0 loss: 0.606172  [   32/   54]
train() client id: f_00006-13-0 loss: 0.502347  [   32/   54]
train() client id: f_00007-0-0 loss: 0.432036  [   32/  179]
train() client id: f_00007-0-1 loss: 0.505092  [   64/  179]
train() client id: f_00007-0-2 loss: 0.573241  [   96/  179]
train() client id: f_00007-0-3 loss: 0.975905  [  128/  179]
train() client id: f_00007-0-4 loss: 0.631534  [  160/  179]
train() client id: f_00007-1-0 loss: 0.641036  [   32/  179]
train() client id: f_00007-1-1 loss: 0.474148  [   64/  179]
train() client id: f_00007-1-2 loss: 0.686144  [   96/  179]
train() client id: f_00007-1-3 loss: 0.475497  [  128/  179]
train() client id: f_00007-1-4 loss: 0.686153  [  160/  179]
train() client id: f_00007-2-0 loss: 0.540761  [   32/  179]
train() client id: f_00007-2-1 loss: 0.636372  [   64/  179]
train() client id: f_00007-2-2 loss: 0.635579  [   96/  179]
train() client id: f_00007-2-3 loss: 0.518138  [  128/  179]
train() client id: f_00007-2-4 loss: 0.548973  [  160/  179]
train() client id: f_00007-3-0 loss: 0.774009  [   32/  179]
train() client id: f_00007-3-1 loss: 0.599514  [   64/  179]
train() client id: f_00007-3-2 loss: 0.458404  [   96/  179]
train() client id: f_00007-3-3 loss: 0.564961  [  128/  179]
train() client id: f_00007-3-4 loss: 0.518121  [  160/  179]
train() client id: f_00007-4-0 loss: 0.447745  [   32/  179]
train() client id: f_00007-4-1 loss: 0.503414  [   64/  179]
train() client id: f_00007-4-2 loss: 0.563033  [   96/  179]
train() client id: f_00007-4-3 loss: 0.767441  [  128/  179]
train() client id: f_00007-4-4 loss: 0.655282  [  160/  179]
train() client id: f_00007-5-0 loss: 0.616689  [   32/  179]
train() client id: f_00007-5-1 loss: 0.661305  [   64/  179]
train() client id: f_00007-5-2 loss: 0.530261  [   96/  179]
train() client id: f_00007-5-3 loss: 0.569250  [  128/  179]
train() client id: f_00007-5-4 loss: 0.511563  [  160/  179]
train() client id: f_00007-6-0 loss: 0.542441  [   32/  179]
train() client id: f_00007-6-1 loss: 0.529699  [   64/  179]
train() client id: f_00007-6-2 loss: 0.776276  [   96/  179]
train() client id: f_00007-6-3 loss: 0.585573  [  128/  179]
train() client id: f_00007-6-4 loss: 0.462888  [  160/  179]
train() client id: f_00007-7-0 loss: 0.629302  [   32/  179]
train() client id: f_00007-7-1 loss: 0.711244  [   64/  179]
train() client id: f_00007-7-2 loss: 0.409632  [   96/  179]
train() client id: f_00007-7-3 loss: 0.590162  [  128/  179]
train() client id: f_00007-7-4 loss: 0.429857  [  160/  179]
train() client id: f_00007-8-0 loss: 0.702106  [   32/  179]
train() client id: f_00007-8-1 loss: 0.590455  [   64/  179]
train() client id: f_00007-8-2 loss: 0.501710  [   96/  179]
train() client id: f_00007-8-3 loss: 0.455216  [  128/  179]
train() client id: f_00007-8-4 loss: 0.585923  [  160/  179]
train() client id: f_00007-9-0 loss: 0.654097  [   32/  179]
train() client id: f_00007-9-1 loss: 0.448499  [   64/  179]
train() client id: f_00007-9-2 loss: 0.595546  [   96/  179]
train() client id: f_00007-9-3 loss: 0.611606  [  128/  179]
train() client id: f_00007-9-4 loss: 0.527075  [  160/  179]
train() client id: f_00007-10-0 loss: 0.424558  [   32/  179]
train() client id: f_00007-10-1 loss: 0.446193  [   64/  179]
train() client id: f_00007-10-2 loss: 0.761292  [   96/  179]
train() client id: f_00007-10-3 loss: 0.548124  [  128/  179]
train() client id: f_00007-10-4 loss: 0.749903  [  160/  179]
train() client id: f_00007-11-0 loss: 0.451967  [   32/  179]
train() client id: f_00007-11-1 loss: 0.600252  [   64/  179]
train() client id: f_00007-11-2 loss: 0.767436  [   96/  179]
train() client id: f_00007-11-3 loss: 0.420034  [  128/  179]
train() client id: f_00007-11-4 loss: 0.684992  [  160/  179]
train() client id: f_00007-12-0 loss: 0.686131  [   32/  179]
train() client id: f_00007-12-1 loss: 0.603084  [   64/  179]
train() client id: f_00007-12-2 loss: 0.589859  [   96/  179]
train() client id: f_00007-12-3 loss: 0.606812  [  128/  179]
train() client id: f_00007-12-4 loss: 0.457147  [  160/  179]
train() client id: f_00007-13-0 loss: 0.499390  [   32/  179]
train() client id: f_00007-13-1 loss: 0.508405  [   64/  179]
train() client id: f_00007-13-2 loss: 0.675603  [   96/  179]
train() client id: f_00007-13-3 loss: 0.606748  [  128/  179]
train() client id: f_00007-13-4 loss: 0.628149  [  160/  179]
train() client id: f_00008-0-0 loss: 0.702506  [   32/  130]
train() client id: f_00008-0-1 loss: 0.568005  [   64/  130]
train() client id: f_00008-0-2 loss: 0.649658  [   96/  130]
train() client id: f_00008-0-3 loss: 0.676431  [  128/  130]
train() client id: f_00008-1-0 loss: 0.611738  [   32/  130]
train() client id: f_00008-1-1 loss: 0.598672  [   64/  130]
train() client id: f_00008-1-2 loss: 0.689416  [   96/  130]
train() client id: f_00008-1-3 loss: 0.675691  [  128/  130]
train() client id: f_00008-2-0 loss: 0.633452  [   32/  130]
train() client id: f_00008-2-1 loss: 0.607477  [   64/  130]
train() client id: f_00008-2-2 loss: 0.610165  [   96/  130]
train() client id: f_00008-2-3 loss: 0.666582  [  128/  130]
train() client id: f_00008-3-0 loss: 0.581210  [   32/  130]
train() client id: f_00008-3-1 loss: 0.737476  [   64/  130]
train() client id: f_00008-3-2 loss: 0.595727  [   96/  130]
train() client id: f_00008-3-3 loss: 0.644205  [  128/  130]
train() client id: f_00008-4-0 loss: 0.639499  [   32/  130]
train() client id: f_00008-4-1 loss: 0.589277  [   64/  130]
train() client id: f_00008-4-2 loss: 0.695815  [   96/  130]
train() client id: f_00008-4-3 loss: 0.622391  [  128/  130]
train() client id: f_00008-5-0 loss: 0.676195  [   32/  130]
train() client id: f_00008-5-1 loss: 0.575751  [   64/  130]
train() client id: f_00008-5-2 loss: 0.640338  [   96/  130]
train() client id: f_00008-5-3 loss: 0.678278  [  128/  130]
train() client id: f_00008-6-0 loss: 0.573178  [   32/  130]
train() client id: f_00008-6-1 loss: 0.738184  [   64/  130]
train() client id: f_00008-6-2 loss: 0.668670  [   96/  130]
train() client id: f_00008-6-3 loss: 0.584503  [  128/  130]
train() client id: f_00008-7-0 loss: 0.708683  [   32/  130]
train() client id: f_00008-7-1 loss: 0.520778  [   64/  130]
train() client id: f_00008-7-2 loss: 0.724717  [   96/  130]
train() client id: f_00008-7-3 loss: 0.617302  [  128/  130]
train() client id: f_00008-8-0 loss: 0.679250  [   32/  130]
train() client id: f_00008-8-1 loss: 0.617355  [   64/  130]
train() client id: f_00008-8-2 loss: 0.640960  [   96/  130]
train() client id: f_00008-8-3 loss: 0.579511  [  128/  130]
train() client id: f_00008-9-0 loss: 0.704766  [   32/  130]
train() client id: f_00008-9-1 loss: 0.613761  [   64/  130]
train() client id: f_00008-9-2 loss: 0.566629  [   96/  130]
train() client id: f_00008-9-3 loss: 0.659135  [  128/  130]
train() client id: f_00008-10-0 loss: 0.603020  [   32/  130]
train() client id: f_00008-10-1 loss: 0.649451  [   64/  130]
train() client id: f_00008-10-2 loss: 0.670111  [   96/  130]
train() client id: f_00008-10-3 loss: 0.643275  [  128/  130]
train() client id: f_00008-11-0 loss: 0.656283  [   32/  130]
train() client id: f_00008-11-1 loss: 0.570514  [   64/  130]
train() client id: f_00008-11-2 loss: 0.616944  [   96/  130]
train() client id: f_00008-11-3 loss: 0.716637  [  128/  130]
train() client id: f_00008-12-0 loss: 0.587643  [   32/  130]
train() client id: f_00008-12-1 loss: 0.557451  [   64/  130]
train() client id: f_00008-12-2 loss: 0.775931  [   96/  130]
train() client id: f_00008-12-3 loss: 0.636965  [  128/  130]
train() client id: f_00008-13-0 loss: 0.742152  [   32/  130]
train() client id: f_00008-13-1 loss: 0.627687  [   64/  130]
train() client id: f_00008-13-2 loss: 0.645248  [   96/  130]
train() client id: f_00008-13-3 loss: 0.544648  [  128/  130]
train() client id: f_00009-0-0 loss: 1.130127  [   32/  118]
train() client id: f_00009-0-1 loss: 1.041989  [   64/  118]
train() client id: f_00009-0-2 loss: 1.088716  [   96/  118]
train() client id: f_00009-1-0 loss: 0.944022  [   32/  118]
train() client id: f_00009-1-1 loss: 1.039018  [   64/  118]
train() client id: f_00009-1-2 loss: 1.143441  [   96/  118]
train() client id: f_00009-2-0 loss: 0.953389  [   32/  118]
train() client id: f_00009-2-1 loss: 0.937510  [   64/  118]
train() client id: f_00009-2-2 loss: 1.037834  [   96/  118]
train() client id: f_00009-3-0 loss: 0.946921  [   32/  118]
train() client id: f_00009-3-1 loss: 0.840120  [   64/  118]
train() client id: f_00009-3-2 loss: 1.010009  [   96/  118]
train() client id: f_00009-4-0 loss: 0.978260  [   32/  118]
train() client id: f_00009-4-1 loss: 0.868516  [   64/  118]
train() client id: f_00009-4-2 loss: 0.907983  [   96/  118]
train() client id: f_00009-5-0 loss: 0.845728  [   32/  118]
train() client id: f_00009-5-1 loss: 0.938868  [   64/  118]
train() client id: f_00009-5-2 loss: 0.835902  [   96/  118]
train() client id: f_00009-6-0 loss: 0.829009  [   32/  118]
train() client id: f_00009-6-1 loss: 0.770510  [   64/  118]
train() client id: f_00009-6-2 loss: 1.016964  [   96/  118]
train() client id: f_00009-7-0 loss: 0.889641  [   32/  118]
train() client id: f_00009-7-1 loss: 0.825283  [   64/  118]
train() client id: f_00009-7-2 loss: 0.728045  [   96/  118]
train() client id: f_00009-8-0 loss: 0.941801  [   32/  118]
train() client id: f_00009-8-1 loss: 0.781867  [   64/  118]
train() client id: f_00009-8-2 loss: 0.756746  [   96/  118]
train() client id: f_00009-9-0 loss: 0.754110  [   32/  118]
train() client id: f_00009-9-1 loss: 0.805600  [   64/  118]
train() client id: f_00009-9-2 loss: 0.778410  [   96/  118]
train() client id: f_00009-10-0 loss: 0.832435  [   32/  118]
train() client id: f_00009-10-1 loss: 0.760899  [   64/  118]
train() client id: f_00009-10-2 loss: 0.837623  [   96/  118]
train() client id: f_00009-11-0 loss: 0.875360  [   32/  118]
train() client id: f_00009-11-1 loss: 0.714306  [   64/  118]
train() client id: f_00009-11-2 loss: 0.840462  [   96/  118]
train() client id: f_00009-12-0 loss: 0.761616  [   32/  118]
train() client id: f_00009-12-1 loss: 0.721183  [   64/  118]
train() client id: f_00009-12-2 loss: 0.754975  [   96/  118]
train() client id: f_00009-13-0 loss: 0.890160  [   32/  118]
train() client id: f_00009-13-1 loss: 0.707751  [   64/  118]
train() client id: f_00009-13-2 loss: 0.792051  [   96/  118]
At round 27 accuracy: 0.6419098143236074
At round 27 training accuracy: 0.5821596244131455
At round 27 training loss: 0.8354684577130069
update_location
xs = 8.927491 256.223621 5.882650 10.934260 -172.581990 -20.230757 -5.849135 -5.143845 -195.120581 20.134486 
ys = -247.390647 7.291448 145.684448 -7.290817 -9.642386 0.794442 -16.381692 141.628436 25.881276 -682.232496 
xs mean: -9.68237997052122
ys mean: -64.16579882624052
dists_uav = 266.986577 275.143070 176.800916 100.859873 199.693562 102.028989 101.501587 173.450492 220.775636 689.816335 
uav_gains = -112.796612 -113.538815 -106.224020 -100.092978 -107.662699 -100.218111 -100.161841 -106.008575 -109.043814 -127.918707 
uav_gains_db_mean: -108.36661713615104
dists_bs = 460.698579 462.687767 183.242830 260.387194 184.658218 233.025953 255.419199 173.103364 150.470035 879.161544 
bs_gains = -114.142732 -114.195124 -102.931815 -107.204409 -103.025381 -105.854379 -106.970158 -102.239613 -100.535660 -122.000982 
bs_gains_db_mean: -107.91002527700392
Round 28
-------------------------------
ene_coms = [0.01314931 0.01398242 0.00717392 0.00632683 0.00720565 0.00635999
 0.00634504 0.00694745 0.00644569 0.03678664]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.38313704 15.27126201  7.15765142  2.55418376 17.46609853  8.38837057
  3.1775276  10.2684085   7.48379523  7.37393281]
obj_prev = 86.52436746665306
eta_min = 2.0023810947974488e-13	eta_max = 0.757153697199189
af = 18.15364795335576	bf = 2.058485470268846	zeta = 19.96901274869134	eta = 0.909090909090909
af = 18.15364795335576	bf = 2.058485470268846	zeta = 41.67584212888353	eta = 0.4355916287717756
af = 18.15364795335576	bf = 2.058485470268846	zeta = 30.377076947525353	eta = 0.5976100987173697
af = 18.15364795335576	bf = 2.058485470268846	zeta = 28.341385831633218	eta = 0.640534942828857
af = 18.15364795335576	bf = 2.058485470268846	zeta = 28.222651575216826	eta = 0.643229708766877
af = 18.15364795335576	bf = 2.058485470268846	zeta = 28.222203789510914	eta = 0.6432399145279633
af = 18.15364795335576	bf = 2.058485470268846	zeta = 28.22220378310609	eta = 0.6432399146739419
eta = 0.6432399146739419
ene_coms = [0.01314931 0.01398242 0.00717392 0.00632683 0.00720565 0.00635999
 0.00634504 0.00694745 0.00644569 0.03678664]
ene_comp = [0.03640986 0.0765763  0.03583193 0.01242559 0.08842395 0.0421892
 0.01560423 0.05172512 0.03756573 0.03409812]
ene_total = [2.58260492 4.71915478 2.241101   0.97721763 4.98340595 2.52997347
 1.14381002 3.05751813 2.29350309 3.69391478]
ti_comp = [0.41682685 0.4084957  0.47658076 0.48505161 0.47626348 0.48472005
 0.48486957 0.47884547 0.48386304 0.1804535 ]
ti_coms = [0.13149309 0.13982424 0.07173918 0.06326833 0.07205646 0.06359989
 0.06345037 0.06947447 0.0644569  0.36786644]
t_total = [28.57379913 28.57379913 28.57379913 28.57379913 28.57379913 28.57379913
 28.57379913 28.57379913 28.57379913 28.57379913]
ene_coms = [0.01314931 0.01398242 0.00717392 0.00632683 0.00720565 0.00635999
 0.00634504 0.00694745 0.00644569 0.03678664]
ene_comp = [1.73630382e-05 1.68185326e-04 1.26595275e-05 5.09629939e-07
 1.90500090e-04 1.99757024e-05 1.01008258e-06 3.77219195e-05
 1.41517653e-05 7.60923279e-05]
ene_total = [0.68613561 0.73741011 0.37450364 0.32972762 0.38542462 0.33246982
 0.33070232 0.36400794 0.33663234 1.92097415]
optimize_network iter = 0 obj = 5.797988156679197
eta = 0.6432399146739419
freqs = [43675043.11581251 93729622.53782259 37592712.29813948 12808521.27833026
 92830915.62454365 43519145.20649479 16091158.61309033 54010243.0371808
 38818557.04517632 94478977.9992683 ]
eta_min = 0.6432399146739429	eta_max = 0.6432399146739403
af = 0.022675495797116044	bf = 2.058485470268846	zeta = 0.02494304537682765	eta = 0.9090909090909091
af = 0.022675495797116044	bf = 2.058485470268846	zeta = 22.66711343435463	eta = 0.001000369802832888
af = 0.022675495797116044	bf = 2.058485470268846	zeta = 2.2173320141422335	eta = 0.010226477429853
af = 0.022675495797116044	bf = 2.058485470268846	zeta = 2.184744407056424	eta = 0.010379015377669493
af = 0.022675495797116044	bf = 2.058485470268846	zeta = 2.184741907961085	eta = 0.010379027250078249
eta = 0.010379027250078249
ene_coms = [0.01314931 0.01398242 0.00717392 0.00632683 0.00720565 0.00635999
 0.00634504 0.00694745 0.00644569 0.03678664]
ene_comp = [1.79753089e-04 1.74116024e-03 1.31059389e-04 5.27600959e-06
 1.97217672e-03 2.06801032e-04 1.04570100e-05 3.90521030e-04
 1.46507973e-04 7.87755626e-04]
ene_total = [0.25040379 0.29538801 0.13723351 0.11895693 0.17241736 0.12336571
 0.11939623 0.13785329 0.12384303 0.70588405]
ti_comp = [0.41682685 0.4084957  0.47658076 0.48505161 0.47626348 0.48472005
 0.48486957 0.47884547 0.48386304 0.1804535 ]
ti_coms = [0.13149309 0.13982424 0.07173918 0.06326833 0.07205646 0.06359989
 0.06345037 0.06947447 0.0644569  0.36786644]
t_total = [28.57379913 28.57379913 28.57379913 28.57379913 28.57379913 28.57379913
 28.57379913 28.57379913 28.57379913 28.57379913]
ene_coms = [0.01314931 0.01398242 0.00717392 0.00632683 0.00720565 0.00635999
 0.00634504 0.00694745 0.00644569 0.03678664]
ene_comp = [1.73630382e-05 1.68185326e-04 1.26595275e-05 5.09629939e-07
 1.90500090e-04 1.99757024e-05 1.01008258e-06 3.77219195e-05
 1.41517653e-05 7.60923279e-05]
ene_total = [0.68613561 0.73741011 0.37450364 0.32972762 0.38542462 0.33246982
 0.33070232 0.36400794 0.33663234 1.92097415]
optimize_network iter = 1 obj = 5.797988156679214
eta = 0.6432399146739429
freqs = [43675043.11581251 93729622.53782262 37592712.29813947 12808521.27833026
 92830915.62454364 43519145.20649476 16091158.61309032 54010243.03718079
 38818557.0451763  94478977.99926877]
Done!
ene_coms = [0.01314931 0.01398242 0.00717392 0.00632683 0.00720565 0.00635999
 0.00634504 0.00694745 0.00644569 0.03678664]
ene_comp = [1.68242328e-05 1.62966242e-04 1.22666803e-05 4.93815234e-07
 1.84588540e-04 1.93558215e-05 9.78737957e-07 3.65513426e-05
 1.37126114e-05 7.37310504e-05]
ene_total = [0.01316613 0.01414539 0.00718618 0.00632733 0.00739023 0.00637934
 0.00634602 0.006984   0.0064594  0.03686038]
At round 28 energy consumption: 0.11124440673332936
At round 28 eta: 0.6432399146739429
At round 28 a_n: 18.59131914111349
At round 28 local rounds: 14.448357722986097
At round 28 global rounds: 52.11154472093523
gradient difference: 0.42414307594299316
train() client id: f_00000-0-0 loss: 1.413431  [   32/  126]
train() client id: f_00000-0-1 loss: 1.142155  [   64/  126]
train() client id: f_00000-0-2 loss: 1.369197  [   96/  126]
train() client id: f_00000-1-0 loss: 0.968585  [   32/  126]
train() client id: f_00000-1-1 loss: 1.177810  [   64/  126]
train() client id: f_00000-1-2 loss: 1.262991  [   96/  126]
train() client id: f_00000-2-0 loss: 1.076891  [   32/  126]
train() client id: f_00000-2-1 loss: 1.108059  [   64/  126]
train() client id: f_00000-2-2 loss: 1.115342  [   96/  126]
train() client id: f_00000-3-0 loss: 1.204212  [   32/  126]
train() client id: f_00000-3-1 loss: 0.980062  [   64/  126]
train() client id: f_00000-3-2 loss: 1.000061  [   96/  126]
train() client id: f_00000-4-0 loss: 1.030219  [   32/  126]
train() client id: f_00000-4-1 loss: 1.043285  [   64/  126]
train() client id: f_00000-4-2 loss: 1.049446  [   96/  126]
train() client id: f_00000-5-0 loss: 1.026531  [   32/  126]
train() client id: f_00000-5-1 loss: 0.966246  [   64/  126]
train() client id: f_00000-5-2 loss: 1.001257  [   96/  126]
train() client id: f_00000-6-0 loss: 0.917359  [   32/  126]
train() client id: f_00000-6-1 loss: 0.935038  [   64/  126]
train() client id: f_00000-6-2 loss: 0.969771  [   96/  126]
train() client id: f_00000-7-0 loss: 0.870068  [   32/  126]
train() client id: f_00000-7-1 loss: 1.067041  [   64/  126]
train() client id: f_00000-7-2 loss: 0.958189  [   96/  126]
train() client id: f_00000-8-0 loss: 0.952964  [   32/  126]
train() client id: f_00000-8-1 loss: 0.990168  [   64/  126]
train() client id: f_00000-8-2 loss: 0.915325  [   96/  126]
train() client id: f_00000-9-0 loss: 0.984124  [   32/  126]
train() client id: f_00000-9-1 loss: 0.919969  [   64/  126]
train() client id: f_00000-9-2 loss: 0.859590  [   96/  126]
train() client id: f_00000-10-0 loss: 0.814031  [   32/  126]
train() client id: f_00000-10-1 loss: 0.991862  [   64/  126]
train() client id: f_00000-10-2 loss: 0.984239  [   96/  126]
train() client id: f_00000-11-0 loss: 0.967557  [   32/  126]
train() client id: f_00000-11-1 loss: 0.947856  [   64/  126]
train() client id: f_00000-11-2 loss: 0.838123  [   96/  126]
train() client id: f_00000-12-0 loss: 0.907300  [   32/  126]
train() client id: f_00000-12-1 loss: 0.967374  [   64/  126]
train() client id: f_00000-12-2 loss: 0.855365  [   96/  126]
train() client id: f_00000-13-0 loss: 0.841412  [   32/  126]
train() client id: f_00000-13-1 loss: 1.008027  [   64/  126]
train() client id: f_00000-13-2 loss: 0.946965  [   96/  126]
train() client id: f_00001-0-0 loss: 0.402702  [   32/  265]
train() client id: f_00001-0-1 loss: 0.477625  [   64/  265]
train() client id: f_00001-0-2 loss: 0.394101  [   96/  265]
train() client id: f_00001-0-3 loss: 0.553552  [  128/  265]
train() client id: f_00001-0-4 loss: 0.351996  [  160/  265]
train() client id: f_00001-0-5 loss: 0.563662  [  192/  265]
train() client id: f_00001-0-6 loss: 0.499969  [  224/  265]
train() client id: f_00001-0-7 loss: 0.372693  [  256/  265]
train() client id: f_00001-1-0 loss: 0.389843  [   32/  265]
train() client id: f_00001-1-1 loss: 0.409168  [   64/  265]
train() client id: f_00001-1-2 loss: 0.424561  [   96/  265]
train() client id: f_00001-1-3 loss: 0.383331  [  128/  265]
train() client id: f_00001-1-4 loss: 0.522307  [  160/  265]
train() client id: f_00001-1-5 loss: 0.474886  [  192/  265]
train() client id: f_00001-1-6 loss: 0.476456  [  224/  265]
train() client id: f_00001-1-7 loss: 0.487109  [  256/  265]
train() client id: f_00001-2-0 loss: 0.568199  [   32/  265]
train() client id: f_00001-2-1 loss: 0.406955  [   64/  265]
train() client id: f_00001-2-2 loss: 0.470217  [   96/  265]
train() client id: f_00001-2-3 loss: 0.353691  [  128/  265]
train() client id: f_00001-2-4 loss: 0.455687  [  160/  265]
train() client id: f_00001-2-5 loss: 0.473420  [  192/  265]
train() client id: f_00001-2-6 loss: 0.464175  [  224/  265]
train() client id: f_00001-2-7 loss: 0.357181  [  256/  265]
train() client id: f_00001-3-0 loss: 0.381458  [   32/  265]
train() client id: f_00001-3-1 loss: 0.420758  [   64/  265]
train() client id: f_00001-3-2 loss: 0.433228  [   96/  265]
train() client id: f_00001-3-3 loss: 0.572144  [  128/  265]
train() client id: f_00001-3-4 loss: 0.359696  [  160/  265]
train() client id: f_00001-3-5 loss: 0.424167  [  192/  265]
train() client id: f_00001-3-6 loss: 0.467107  [  224/  265]
train() client id: f_00001-3-7 loss: 0.425221  [  256/  265]
train() client id: f_00001-4-0 loss: 0.336220  [   32/  265]
train() client id: f_00001-4-1 loss: 0.394023  [   64/  265]
train() client id: f_00001-4-2 loss: 0.514058  [   96/  265]
train() client id: f_00001-4-3 loss: 0.443223  [  128/  265]
train() client id: f_00001-4-4 loss: 0.374137  [  160/  265]
train() client id: f_00001-4-5 loss: 0.462499  [  192/  265]
train() client id: f_00001-4-6 loss: 0.360712  [  224/  265]
train() client id: f_00001-4-7 loss: 0.624402  [  256/  265]
train() client id: f_00001-5-0 loss: 0.428760  [   32/  265]
train() client id: f_00001-5-1 loss: 0.470146  [   64/  265]
train() client id: f_00001-5-2 loss: 0.342905  [   96/  265]
train() client id: f_00001-5-3 loss: 0.339510  [  128/  265]
train() client id: f_00001-5-4 loss: 0.422172  [  160/  265]
train() client id: f_00001-5-5 loss: 0.583821  [  192/  265]
train() client id: f_00001-5-6 loss: 0.547654  [  224/  265]
train() client id: f_00001-5-7 loss: 0.330907  [  256/  265]
train() client id: f_00001-6-0 loss: 0.402703  [   32/  265]
train() client id: f_00001-6-1 loss: 0.407925  [   64/  265]
train() client id: f_00001-6-2 loss: 0.493416  [   96/  265]
train() client id: f_00001-6-3 loss: 0.452806  [  128/  265]
train() client id: f_00001-6-4 loss: 0.448896  [  160/  265]
train() client id: f_00001-6-5 loss: 0.412810  [  192/  265]
train() client id: f_00001-6-6 loss: 0.403571  [  224/  265]
train() client id: f_00001-6-7 loss: 0.445107  [  256/  265]
train() client id: f_00001-7-0 loss: 0.429410  [   32/  265]
train() client id: f_00001-7-1 loss: 0.382652  [   64/  265]
train() client id: f_00001-7-2 loss: 0.413989  [   96/  265]
train() client id: f_00001-7-3 loss: 0.390749  [  128/  265]
train() client id: f_00001-7-4 loss: 0.477226  [  160/  265]
train() client id: f_00001-7-5 loss: 0.544288  [  192/  265]
train() client id: f_00001-7-6 loss: 0.331802  [  224/  265]
train() client id: f_00001-7-7 loss: 0.441377  [  256/  265]
train() client id: f_00001-8-0 loss: 0.460926  [   32/  265]
train() client id: f_00001-8-1 loss: 0.491920  [   64/  265]
train() client id: f_00001-8-2 loss: 0.418163  [   96/  265]
train() client id: f_00001-8-3 loss: 0.477846  [  128/  265]
train() client id: f_00001-8-4 loss: 0.421922  [  160/  265]
train() client id: f_00001-8-5 loss: 0.327368  [  192/  265]
train() client id: f_00001-8-6 loss: 0.471087  [  224/  265]
train() client id: f_00001-8-7 loss: 0.348258  [  256/  265]
train() client id: f_00001-9-0 loss: 0.372706  [   32/  265]
train() client id: f_00001-9-1 loss: 0.378124  [   64/  265]
train() client id: f_00001-9-2 loss: 0.428638  [   96/  265]
train() client id: f_00001-9-3 loss: 0.397447  [  128/  265]
train() client id: f_00001-9-4 loss: 0.559207  [  160/  265]
train() client id: f_00001-9-5 loss: 0.419210  [  192/  265]
train() client id: f_00001-9-6 loss: 0.357438  [  224/  265]
train() client id: f_00001-9-7 loss: 0.519655  [  256/  265]
train() client id: f_00001-10-0 loss: 0.498310  [   32/  265]
train() client id: f_00001-10-1 loss: 0.376751  [   64/  265]
train() client id: f_00001-10-2 loss: 0.364879  [   96/  265]
train() client id: f_00001-10-3 loss: 0.381413  [  128/  265]
train() client id: f_00001-10-4 loss: 0.495615  [  160/  265]
train() client id: f_00001-10-5 loss: 0.398048  [  192/  265]
train() client id: f_00001-10-6 loss: 0.366418  [  224/  265]
train() client id: f_00001-10-7 loss: 0.554259  [  256/  265]
train() client id: f_00001-11-0 loss: 0.319715  [   32/  265]
train() client id: f_00001-11-1 loss: 0.437583  [   64/  265]
train() client id: f_00001-11-2 loss: 0.541004  [   96/  265]
train() client id: f_00001-11-3 loss: 0.447896  [  128/  265]
train() client id: f_00001-11-4 loss: 0.449563  [  160/  265]
train() client id: f_00001-11-5 loss: 0.367603  [  192/  265]
train() client id: f_00001-11-6 loss: 0.341048  [  224/  265]
train() client id: f_00001-11-7 loss: 0.521576  [  256/  265]
train() client id: f_00001-12-0 loss: 0.516093  [   32/  265]
train() client id: f_00001-12-1 loss: 0.406585  [   64/  265]
train() client id: f_00001-12-2 loss: 0.416717  [   96/  265]
train() client id: f_00001-12-3 loss: 0.393875  [  128/  265]
train() client id: f_00001-12-4 loss: 0.443132  [  160/  265]
train() client id: f_00001-12-5 loss: 0.435781  [  192/  265]
train() client id: f_00001-12-6 loss: 0.391418  [  224/  265]
train() client id: f_00001-12-7 loss: 0.364216  [  256/  265]
train() client id: f_00001-13-0 loss: 0.385695  [   32/  265]
train() client id: f_00001-13-1 loss: 0.473176  [   64/  265]
train() client id: f_00001-13-2 loss: 0.452077  [   96/  265]
train() client id: f_00001-13-3 loss: 0.406541  [  128/  265]
train() client id: f_00001-13-4 loss: 0.345559  [  160/  265]
train() client id: f_00001-13-5 loss: 0.484667  [  192/  265]
train() client id: f_00001-13-6 loss: 0.416519  [  224/  265]
train() client id: f_00001-13-7 loss: 0.462273  [  256/  265]
train() client id: f_00002-0-0 loss: 1.259387  [   32/  124]
train() client id: f_00002-0-1 loss: 1.119136  [   64/  124]
train() client id: f_00002-0-2 loss: 1.393191  [   96/  124]
train() client id: f_00002-1-0 loss: 1.295257  [   32/  124]
train() client id: f_00002-1-1 loss: 1.203575  [   64/  124]
train() client id: f_00002-1-2 loss: 1.211415  [   96/  124]
train() client id: f_00002-2-0 loss: 1.110785  [   32/  124]
train() client id: f_00002-2-1 loss: 1.180253  [   64/  124]
train() client id: f_00002-2-2 loss: 1.102787  [   96/  124]
train() client id: f_00002-3-0 loss: 1.069548  [   32/  124]
train() client id: f_00002-3-1 loss: 1.162024  [   64/  124]
train() client id: f_00002-3-2 loss: 1.127042  [   96/  124]
train() client id: f_00002-4-0 loss: 1.095254  [   32/  124]
train() client id: f_00002-4-1 loss: 1.122966  [   64/  124]
train() client id: f_00002-4-2 loss: 1.173739  [   96/  124]
train() client id: f_00002-5-0 loss: 1.038593  [   32/  124]
train() client id: f_00002-5-1 loss: 0.996647  [   64/  124]
train() client id: f_00002-5-2 loss: 1.264724  [   96/  124]
train() client id: f_00002-6-0 loss: 1.024407  [   32/  124]
train() client id: f_00002-6-1 loss: 1.017887  [   64/  124]
train() client id: f_00002-6-2 loss: 1.146802  [   96/  124]
train() client id: f_00002-7-0 loss: 0.990135  [   32/  124]
train() client id: f_00002-7-1 loss: 1.021261  [   64/  124]
train() client id: f_00002-7-2 loss: 1.241659  [   96/  124]
train() client id: f_00002-8-0 loss: 1.005144  [   32/  124]
train() client id: f_00002-8-1 loss: 0.936034  [   64/  124]
train() client id: f_00002-8-2 loss: 1.057876  [   96/  124]
train() client id: f_00002-9-0 loss: 1.120228  [   32/  124]
train() client id: f_00002-9-1 loss: 0.945030  [   64/  124]
train() client id: f_00002-9-2 loss: 1.042734  [   96/  124]
train() client id: f_00002-10-0 loss: 0.979522  [   32/  124]
train() client id: f_00002-10-1 loss: 1.049547  [   64/  124]
train() client id: f_00002-10-2 loss: 0.937331  [   96/  124]
train() client id: f_00002-11-0 loss: 0.978448  [   32/  124]
train() client id: f_00002-11-1 loss: 0.894688  [   64/  124]
train() client id: f_00002-11-2 loss: 1.126021  [   96/  124]
train() client id: f_00002-12-0 loss: 0.936648  [   32/  124]
train() client id: f_00002-12-1 loss: 0.961659  [   64/  124]
train() client id: f_00002-12-2 loss: 1.105650  [   96/  124]
train() client id: f_00002-13-0 loss: 1.188285  [   32/  124]
train() client id: f_00002-13-1 loss: 1.021981  [   64/  124]
train() client id: f_00002-13-2 loss: 0.732098  [   96/  124]
train() client id: f_00003-0-0 loss: 0.818049  [   32/   43]
train() client id: f_00003-1-0 loss: 0.573943  [   32/   43]
train() client id: f_00003-2-0 loss: 0.862742  [   32/   43]
train() client id: f_00003-3-0 loss: 0.589720  [   32/   43]
train() client id: f_00003-4-0 loss: 0.672802  [   32/   43]
train() client id: f_00003-5-0 loss: 0.699199  [   32/   43]
train() client id: f_00003-6-0 loss: 0.785624  [   32/   43]
train() client id: f_00003-7-0 loss: 0.706046  [   32/   43]
train() client id: f_00003-8-0 loss: 0.723995  [   32/   43]
train() client id: f_00003-9-0 loss: 0.612830  [   32/   43]
train() client id: f_00003-10-0 loss: 0.659252  [   32/   43]
train() client id: f_00003-11-0 loss: 0.765163  [   32/   43]
train() client id: f_00003-12-0 loss: 0.514758  [   32/   43]
train() client id: f_00003-13-0 loss: 0.662497  [   32/   43]
train() client id: f_00004-0-0 loss: 0.928689  [   32/  306]
train() client id: f_00004-0-1 loss: 0.786963  [   64/  306]
train() client id: f_00004-0-2 loss: 0.968593  [   96/  306]
train() client id: f_00004-0-3 loss: 0.937515  [  128/  306]
train() client id: f_00004-0-4 loss: 0.868606  [  160/  306]
train() client id: f_00004-0-5 loss: 0.925181  [  192/  306]
train() client id: f_00004-0-6 loss: 0.937767  [  224/  306]
train() client id: f_00004-0-7 loss: 0.836323  [  256/  306]
train() client id: f_00004-0-8 loss: 0.876106  [  288/  306]
train() client id: f_00004-1-0 loss: 0.926557  [   32/  306]
train() client id: f_00004-1-1 loss: 0.930351  [   64/  306]
train() client id: f_00004-1-2 loss: 0.986378  [   96/  306]
train() client id: f_00004-1-3 loss: 0.717531  [  128/  306]
train() client id: f_00004-1-4 loss: 0.837833  [  160/  306]
train() client id: f_00004-1-5 loss: 0.924578  [  192/  306]
train() client id: f_00004-1-6 loss: 0.786079  [  224/  306]
train() client id: f_00004-1-7 loss: 1.011037  [  256/  306]
train() client id: f_00004-1-8 loss: 0.920494  [  288/  306]
train() client id: f_00004-2-0 loss: 0.947493  [   32/  306]
train() client id: f_00004-2-1 loss: 0.919604  [   64/  306]
train() client id: f_00004-2-2 loss: 0.861306  [   96/  306]
train() client id: f_00004-2-3 loss: 0.970608  [  128/  306]
train() client id: f_00004-2-4 loss: 0.905924  [  160/  306]
train() client id: f_00004-2-5 loss: 0.814681  [  192/  306]
train() client id: f_00004-2-6 loss: 0.885831  [  224/  306]
train() client id: f_00004-2-7 loss: 0.828655  [  256/  306]
train() client id: f_00004-2-8 loss: 0.951414  [  288/  306]
train() client id: f_00004-3-0 loss: 1.025187  [   32/  306]
train() client id: f_00004-3-1 loss: 0.857149  [   64/  306]
train() client id: f_00004-3-2 loss: 0.845718  [   96/  306]
train() client id: f_00004-3-3 loss: 0.726524  [  128/  306]
train() client id: f_00004-3-4 loss: 1.005514  [  160/  306]
train() client id: f_00004-3-5 loss: 0.784292  [  192/  306]
train() client id: f_00004-3-6 loss: 0.962017  [  224/  306]
train() client id: f_00004-3-7 loss: 0.784575  [  256/  306]
train() client id: f_00004-3-8 loss: 1.007441  [  288/  306]
train() client id: f_00004-4-0 loss: 0.982358  [   32/  306]
train() client id: f_00004-4-1 loss: 0.855446  [   64/  306]
train() client id: f_00004-4-2 loss: 0.887773  [   96/  306]
train() client id: f_00004-4-3 loss: 0.822873  [  128/  306]
train() client id: f_00004-4-4 loss: 0.965528  [  160/  306]
train() client id: f_00004-4-5 loss: 1.078831  [  192/  306]
train() client id: f_00004-4-6 loss: 0.995663  [  224/  306]
train() client id: f_00004-4-7 loss: 0.701574  [  256/  306]
train() client id: f_00004-4-8 loss: 0.850114  [  288/  306]
train() client id: f_00004-5-0 loss: 0.871971  [   32/  306]
train() client id: f_00004-5-1 loss: 1.016916  [   64/  306]
train() client id: f_00004-5-2 loss: 0.931218  [   96/  306]
train() client id: f_00004-5-3 loss: 0.839114  [  128/  306]
train() client id: f_00004-5-4 loss: 0.838841  [  160/  306]
train() client id: f_00004-5-5 loss: 1.015223  [  192/  306]
train() client id: f_00004-5-6 loss: 0.846126  [  224/  306]
train() client id: f_00004-5-7 loss: 0.867196  [  256/  306]
train() client id: f_00004-5-8 loss: 0.848725  [  288/  306]
train() client id: f_00004-6-0 loss: 0.940202  [   32/  306]
train() client id: f_00004-6-1 loss: 0.788566  [   64/  306]
train() client id: f_00004-6-2 loss: 1.018735  [   96/  306]
train() client id: f_00004-6-3 loss: 0.939342  [  128/  306]
train() client id: f_00004-6-4 loss: 0.819911  [  160/  306]
train() client id: f_00004-6-5 loss: 0.839761  [  192/  306]
train() client id: f_00004-6-6 loss: 0.873017  [  224/  306]
train() client id: f_00004-6-7 loss: 0.983567  [  256/  306]
train() client id: f_00004-6-8 loss: 0.924149  [  288/  306]
train() client id: f_00004-7-0 loss: 0.888003  [   32/  306]
train() client id: f_00004-7-1 loss: 1.093092  [   64/  306]
train() client id: f_00004-7-2 loss: 0.902632  [   96/  306]
train() client id: f_00004-7-3 loss: 0.884461  [  128/  306]
train() client id: f_00004-7-4 loss: 0.889638  [  160/  306]
train() client id: f_00004-7-5 loss: 0.797407  [  192/  306]
train() client id: f_00004-7-6 loss: 0.826916  [  224/  306]
train() client id: f_00004-7-7 loss: 0.933335  [  256/  306]
train() client id: f_00004-7-8 loss: 0.877736  [  288/  306]
train() client id: f_00004-8-0 loss: 0.869768  [   32/  306]
train() client id: f_00004-8-1 loss: 0.931167  [   64/  306]
train() client id: f_00004-8-2 loss: 0.957737  [   96/  306]
train() client id: f_00004-8-3 loss: 0.958043  [  128/  306]
train() client id: f_00004-8-4 loss: 0.852189  [  160/  306]
train() client id: f_00004-8-5 loss: 0.945075  [  192/  306]
train() client id: f_00004-8-6 loss: 0.875010  [  224/  306]
train() client id: f_00004-8-7 loss: 0.827909  [  256/  306]
train() client id: f_00004-8-8 loss: 0.878023  [  288/  306]
train() client id: f_00004-9-0 loss: 0.861294  [   32/  306]
train() client id: f_00004-9-1 loss: 0.854018  [   64/  306]
train() client id: f_00004-9-2 loss: 0.942992  [   96/  306]
train() client id: f_00004-9-3 loss: 0.885055  [  128/  306]
train() client id: f_00004-9-4 loss: 0.951606  [  160/  306]
train() client id: f_00004-9-5 loss: 0.866645  [  192/  306]
train() client id: f_00004-9-6 loss: 0.885652  [  224/  306]
train() client id: f_00004-9-7 loss: 0.885673  [  256/  306]
train() client id: f_00004-9-8 loss: 0.915879  [  288/  306]
train() client id: f_00004-10-0 loss: 0.964620  [   32/  306]
train() client id: f_00004-10-1 loss: 0.829224  [   64/  306]
train() client id: f_00004-10-2 loss: 0.847220  [   96/  306]
train() client id: f_00004-10-3 loss: 0.936061  [  128/  306]
train() client id: f_00004-10-4 loss: 0.831492  [  160/  306]
train() client id: f_00004-10-5 loss: 0.990026  [  192/  306]
train() client id: f_00004-10-6 loss: 0.798227  [  224/  306]
train() client id: f_00004-10-7 loss: 0.925777  [  256/  306]
train() client id: f_00004-10-8 loss: 0.974293  [  288/  306]
train() client id: f_00004-11-0 loss: 0.872898  [   32/  306]
train() client id: f_00004-11-1 loss: 0.934179  [   64/  306]
train() client id: f_00004-11-2 loss: 0.851294  [   96/  306]
train() client id: f_00004-11-3 loss: 0.904936  [  128/  306]
train() client id: f_00004-11-4 loss: 0.828528  [  160/  306]
train() client id: f_00004-11-5 loss: 0.926508  [  192/  306]
train() client id: f_00004-11-6 loss: 0.922215  [  224/  306]
train() client id: f_00004-11-7 loss: 0.989572  [  256/  306]
train() client id: f_00004-11-8 loss: 0.874774  [  288/  306]
train() client id: f_00004-12-0 loss: 0.951671  [   32/  306]
train() client id: f_00004-12-1 loss: 0.870736  [   64/  306]
train() client id: f_00004-12-2 loss: 0.949384  [   96/  306]
train() client id: f_00004-12-3 loss: 0.846637  [  128/  306]
train() client id: f_00004-12-4 loss: 0.778568  [  160/  306]
train() client id: f_00004-12-5 loss: 0.936828  [  192/  306]
train() client id: f_00004-12-6 loss: 0.972367  [  224/  306]
train() client id: f_00004-12-7 loss: 0.891253  [  256/  306]
train() client id: f_00004-12-8 loss: 0.928800  [  288/  306]
train() client id: f_00004-13-0 loss: 0.963925  [   32/  306]
train() client id: f_00004-13-1 loss: 0.939193  [   64/  306]
train() client id: f_00004-13-2 loss: 0.842952  [   96/  306]
train() client id: f_00004-13-3 loss: 0.976990  [  128/  306]
train() client id: f_00004-13-4 loss: 0.895923  [  160/  306]
train() client id: f_00004-13-5 loss: 0.812332  [  192/  306]
train() client id: f_00004-13-6 loss: 0.882869  [  224/  306]
train() client id: f_00004-13-7 loss: 0.917881  [  256/  306]
train() client id: f_00004-13-8 loss: 0.856957  [  288/  306]
train() client id: f_00005-0-0 loss: 0.586361  [   32/  146]
train() client id: f_00005-0-1 loss: 0.477616  [   64/  146]
train() client id: f_00005-0-2 loss: 0.697551  [   96/  146]
train() client id: f_00005-0-3 loss: 0.692358  [  128/  146]
train() client id: f_00005-1-0 loss: 0.684321  [   32/  146]
train() client id: f_00005-1-1 loss: 0.623138  [   64/  146]
train() client id: f_00005-1-2 loss: 0.781599  [   96/  146]
train() client id: f_00005-1-3 loss: 0.608412  [  128/  146]
train() client id: f_00005-2-0 loss: 0.669506  [   32/  146]
train() client id: f_00005-2-1 loss: 0.636502  [   64/  146]
train() client id: f_00005-2-2 loss: 0.602173  [   96/  146]
train() client id: f_00005-2-3 loss: 0.588804  [  128/  146]
train() client id: f_00005-3-0 loss: 0.664227  [   32/  146]
train() client id: f_00005-3-1 loss: 0.589080  [   64/  146]
train() client id: f_00005-3-2 loss: 0.608423  [   96/  146]
train() client id: f_00005-3-3 loss: 0.663627  [  128/  146]
train() client id: f_00005-4-0 loss: 0.552399  [   32/  146]
train() client id: f_00005-4-1 loss: 0.834013  [   64/  146]
train() client id: f_00005-4-2 loss: 0.566513  [   96/  146]
train() client id: f_00005-4-3 loss: 0.643892  [  128/  146]
train() client id: f_00005-5-0 loss: 0.660628  [   32/  146]
train() client id: f_00005-5-1 loss: 0.513080  [   64/  146]
train() client id: f_00005-5-2 loss: 0.695872  [   96/  146]
train() client id: f_00005-5-3 loss: 0.596654  [  128/  146]
train() client id: f_00005-6-0 loss: 0.575165  [   32/  146]
train() client id: f_00005-6-1 loss: 0.755354  [   64/  146]
train() client id: f_00005-6-2 loss: 0.590683  [   96/  146]
train() client id: f_00005-6-3 loss: 0.700791  [  128/  146]
train() client id: f_00005-7-0 loss: 0.786215  [   32/  146]
train() client id: f_00005-7-1 loss: 0.678512  [   64/  146]
train() client id: f_00005-7-2 loss: 0.533527  [   96/  146]
train() client id: f_00005-7-3 loss: 0.570461  [  128/  146]
train() client id: f_00005-8-0 loss: 0.431511  [   32/  146]
train() client id: f_00005-8-1 loss: 0.678249  [   64/  146]
train() client id: f_00005-8-2 loss: 0.703539  [   96/  146]
train() client id: f_00005-8-3 loss: 0.786096  [  128/  146]
train() client id: f_00005-9-0 loss: 0.544335  [   32/  146]
train() client id: f_00005-9-1 loss: 0.652701  [   64/  146]
train() client id: f_00005-9-2 loss: 0.504399  [   96/  146]
train() client id: f_00005-9-3 loss: 0.757666  [  128/  146]
train() client id: f_00005-10-0 loss: 0.510718  [   32/  146]
train() client id: f_00005-10-1 loss: 0.952500  [   64/  146]
train() client id: f_00005-10-2 loss: 0.561739  [   96/  146]
train() client id: f_00005-10-3 loss: 0.545967  [  128/  146]
train() client id: f_00005-11-0 loss: 0.507320  [   32/  146]
train() client id: f_00005-11-1 loss: 0.613180  [   64/  146]
train() client id: f_00005-11-2 loss: 0.688707  [   96/  146]
train() client id: f_00005-11-3 loss: 0.756836  [  128/  146]
train() client id: f_00005-12-0 loss: 0.616711  [   32/  146]
train() client id: f_00005-12-1 loss: 0.666840  [   64/  146]
train() client id: f_00005-12-2 loss: 0.537254  [   96/  146]
train() client id: f_00005-12-3 loss: 0.858863  [  128/  146]
train() client id: f_00005-13-0 loss: 0.541380  [   32/  146]
train() client id: f_00005-13-1 loss: 0.597166  [   64/  146]
train() client id: f_00005-13-2 loss: 0.712942  [   96/  146]
train() client id: f_00005-13-3 loss: 0.735616  [  128/  146]
train() client id: f_00006-0-0 loss: 0.555800  [   32/   54]
train() client id: f_00006-1-0 loss: 0.554627  [   32/   54]
train() client id: f_00006-2-0 loss: 0.551530  [   32/   54]
train() client id: f_00006-3-0 loss: 0.551461  [   32/   54]
train() client id: f_00006-4-0 loss: 0.507763  [   32/   54]
train() client id: f_00006-5-0 loss: 0.564425  [   32/   54]
train() client id: f_00006-6-0 loss: 0.545273  [   32/   54]
train() client id: f_00006-7-0 loss: 0.609745  [   32/   54]
train() client id: f_00006-8-0 loss: 0.555671  [   32/   54]
train() client id: f_00006-9-0 loss: 0.609875  [   32/   54]
train() client id: f_00006-10-0 loss: 0.559684  [   32/   54]
train() client id: f_00006-11-0 loss: 0.517330  [   32/   54]
train() client id: f_00006-12-0 loss: 0.510682  [   32/   54]
train() client id: f_00006-13-0 loss: 0.571156  [   32/   54]
train() client id: f_00007-0-0 loss: 0.497257  [   32/  179]
train() client id: f_00007-0-1 loss: 0.562570  [   64/  179]
train() client id: f_00007-0-2 loss: 0.497902  [   96/  179]
train() client id: f_00007-0-3 loss: 0.764540  [  128/  179]
train() client id: f_00007-0-4 loss: 0.493512  [  160/  179]
train() client id: f_00007-1-0 loss: 0.562894  [   32/  179]
train() client id: f_00007-1-1 loss: 0.479915  [   64/  179]
train() client id: f_00007-1-2 loss: 0.630135  [   96/  179]
train() client id: f_00007-1-3 loss: 0.415465  [  128/  179]
train() client id: f_00007-1-4 loss: 0.551918  [  160/  179]
train() client id: f_00007-2-0 loss: 0.469019  [   32/  179]
train() client id: f_00007-2-1 loss: 0.492226  [   64/  179]
train() client id: f_00007-2-2 loss: 0.620838  [   96/  179]
train() client id: f_00007-2-3 loss: 0.535328  [  128/  179]
train() client id: f_00007-2-4 loss: 0.554044  [  160/  179]
train() client id: f_00007-3-0 loss: 0.594395  [   32/  179]
train() client id: f_00007-3-1 loss: 0.405110  [   64/  179]
train() client id: f_00007-3-2 loss: 0.571709  [   96/  179]
train() client id: f_00007-3-3 loss: 0.557107  [  128/  179]
train() client id: f_00007-3-4 loss: 0.373534  [  160/  179]
train() client id: f_00007-4-0 loss: 0.633249  [   32/  179]
train() client id: f_00007-4-1 loss: 0.358903  [   64/  179]
train() client id: f_00007-4-2 loss: 0.685033  [   96/  179]
train() client id: f_00007-4-3 loss: 0.398406  [  128/  179]
train() client id: f_00007-4-4 loss: 0.466277  [  160/  179]
train() client id: f_00007-5-0 loss: 0.571062  [   32/  179]
train() client id: f_00007-5-1 loss: 0.455789  [   64/  179]
train() client id: f_00007-5-2 loss: 0.446639  [   96/  179]
train() client id: f_00007-5-3 loss: 0.563061  [  128/  179]
train() client id: f_00007-5-4 loss: 0.544787  [  160/  179]
train() client id: f_00007-6-0 loss: 0.440853  [   32/  179]
train() client id: f_00007-6-1 loss: 0.680202  [   64/  179]
train() client id: f_00007-6-2 loss: 0.441476  [   96/  179]
train() client id: f_00007-6-3 loss: 0.434864  [  128/  179]
train() client id: f_00007-6-4 loss: 0.477945  [  160/  179]
train() client id: f_00007-7-0 loss: 0.495702  [   32/  179]
train() client id: f_00007-7-1 loss: 0.375807  [   64/  179]
train() client id: f_00007-7-2 loss: 0.431725  [   96/  179]
train() client id: f_00007-7-3 loss: 0.423184  [  128/  179]
train() client id: f_00007-7-4 loss: 0.648702  [  160/  179]
train() client id: f_00007-8-0 loss: 0.424666  [   32/  179]
train() client id: f_00007-8-1 loss: 0.579452  [   64/  179]
train() client id: f_00007-8-2 loss: 0.444854  [   96/  179]
train() client id: f_00007-8-3 loss: 0.426000  [  128/  179]
train() client id: f_00007-8-4 loss: 0.471186  [  160/  179]
train() client id: f_00007-9-0 loss: 0.559800  [   32/  179]
train() client id: f_00007-9-1 loss: 0.372671  [   64/  179]
train() client id: f_00007-9-2 loss: 0.493673  [   96/  179]
train() client id: f_00007-9-3 loss: 0.658248  [  128/  179]
train() client id: f_00007-9-4 loss: 0.424145  [  160/  179]
train() client id: f_00007-10-0 loss: 0.456918  [   32/  179]
train() client id: f_00007-10-1 loss: 0.424602  [   64/  179]
train() client id: f_00007-10-2 loss: 0.466763  [   96/  179]
train() client id: f_00007-10-3 loss: 0.583673  [  128/  179]
train() client id: f_00007-10-4 loss: 0.501664  [  160/  179]
train() client id: f_00007-11-0 loss: 0.645563  [   32/  179]
train() client id: f_00007-11-1 loss: 0.479432  [   64/  179]
train() client id: f_00007-11-2 loss: 0.326863  [   96/  179]
train() client id: f_00007-11-3 loss: 0.518156  [  128/  179]
train() client id: f_00007-11-4 loss: 0.369988  [  160/  179]
train() client id: f_00007-12-0 loss: 0.353420  [   32/  179]
train() client id: f_00007-12-1 loss: 0.338126  [   64/  179]
train() client id: f_00007-12-2 loss: 0.685847  [   96/  179]
train() client id: f_00007-12-3 loss: 0.436158  [  128/  179]
train() client id: f_00007-12-4 loss: 0.449210  [  160/  179]
train() client id: f_00007-13-0 loss: 0.606817  [   32/  179]
train() client id: f_00007-13-1 loss: 0.609574  [   64/  179]
train() client id: f_00007-13-2 loss: 0.311891  [   96/  179]
train() client id: f_00007-13-3 loss: 0.465860  [  128/  179]
train() client id: f_00007-13-4 loss: 0.534169  [  160/  179]
train() client id: f_00008-0-0 loss: 0.770476  [   32/  130]
train() client id: f_00008-0-1 loss: 0.802474  [   64/  130]
train() client id: f_00008-0-2 loss: 0.821994  [   96/  130]
train() client id: f_00008-0-3 loss: 0.724762  [  128/  130]
train() client id: f_00008-1-0 loss: 0.804098  [   32/  130]
train() client id: f_00008-1-1 loss: 0.723600  [   64/  130]
train() client id: f_00008-1-2 loss: 0.771728  [   96/  130]
train() client id: f_00008-1-3 loss: 0.829320  [  128/  130]
train() client id: f_00008-2-0 loss: 0.864532  [   32/  130]
train() client id: f_00008-2-1 loss: 0.727707  [   64/  130]
train() client id: f_00008-2-2 loss: 0.726335  [   96/  130]
train() client id: f_00008-2-3 loss: 0.767588  [  128/  130]
train() client id: f_00008-3-0 loss: 0.843099  [   32/  130]
train() client id: f_00008-3-1 loss: 0.770441  [   64/  130]
train() client id: f_00008-3-2 loss: 0.697763  [   96/  130]
train() client id: f_00008-3-3 loss: 0.799305  [  128/  130]
train() client id: f_00008-4-0 loss: 0.825410  [   32/  130]
train() client id: f_00008-4-1 loss: 0.731778  [   64/  130]
train() client id: f_00008-4-2 loss: 0.707561  [   96/  130]
train() client id: f_00008-4-3 loss: 0.825967  [  128/  130]
train() client id: f_00008-5-0 loss: 0.689112  [   32/  130]
train() client id: f_00008-5-1 loss: 0.882565  [   64/  130]
train() client id: f_00008-5-2 loss: 0.861404  [   96/  130]
train() client id: f_00008-5-3 loss: 0.682410  [  128/  130]
train() client id: f_00008-6-0 loss: 0.741528  [   32/  130]
train() client id: f_00008-6-1 loss: 0.840320  [   64/  130]
train() client id: f_00008-6-2 loss: 0.731773  [   96/  130]
train() client id: f_00008-6-3 loss: 0.796412  [  128/  130]
train() client id: f_00008-7-0 loss: 0.820330  [   32/  130]
train() client id: f_00008-7-1 loss: 0.835095  [   64/  130]
train() client id: f_00008-7-2 loss: 0.727215  [   96/  130]
train() client id: f_00008-7-3 loss: 0.730900  [  128/  130]
train() client id: f_00008-8-0 loss: 0.736093  [   32/  130]
train() client id: f_00008-8-1 loss: 0.809693  [   64/  130]
train() client id: f_00008-8-2 loss: 0.791523  [   96/  130]
train() client id: f_00008-8-3 loss: 0.771283  [  128/  130]
train() client id: f_00008-9-0 loss: 0.742714  [   32/  130]
train() client id: f_00008-9-1 loss: 0.809134  [   64/  130]
train() client id: f_00008-9-2 loss: 0.736995  [   96/  130]
train() client id: f_00008-9-3 loss: 0.821832  [  128/  130]
train() client id: f_00008-10-0 loss: 0.797425  [   32/  130]
train() client id: f_00008-10-1 loss: 0.835876  [   64/  130]
train() client id: f_00008-10-2 loss: 0.746540  [   96/  130]
train() client id: f_00008-10-3 loss: 0.726974  [  128/  130]
train() client id: f_00008-11-0 loss: 0.743102  [   32/  130]
train() client id: f_00008-11-1 loss: 0.797744  [   64/  130]
train() client id: f_00008-11-2 loss: 0.742901  [   96/  130]
train() client id: f_00008-11-3 loss: 0.824239  [  128/  130]
train() client id: f_00008-12-0 loss: 0.704134  [   32/  130]
train() client id: f_00008-12-1 loss: 0.738737  [   64/  130]
train() client id: f_00008-12-2 loss: 0.820888  [   96/  130]
train() client id: f_00008-12-3 loss: 0.846470  [  128/  130]
train() client id: f_00008-13-0 loss: 0.771130  [   32/  130]
train() client id: f_00008-13-1 loss: 0.800115  [   64/  130]
train() client id: f_00008-13-2 loss: 0.813205  [   96/  130]
train() client id: f_00008-13-3 loss: 0.689981  [  128/  130]
train() client id: f_00009-0-0 loss: 1.113440  [   32/  118]
train() client id: f_00009-0-1 loss: 1.054795  [   64/  118]
train() client id: f_00009-0-2 loss: 1.131550  [   96/  118]
train() client id: f_00009-1-0 loss: 1.033732  [   32/  118]
train() client id: f_00009-1-1 loss: 1.091369  [   64/  118]
train() client id: f_00009-1-2 loss: 0.881810  [   96/  118]
train() client id: f_00009-2-0 loss: 1.116061  [   32/  118]
train() client id: f_00009-2-1 loss: 0.810849  [   64/  118]
train() client id: f_00009-2-2 loss: 0.919914  [   96/  118]
train() client id: f_00009-3-0 loss: 0.899999  [   32/  118]
train() client id: f_00009-3-1 loss: 0.941969  [   64/  118]
train() client id: f_00009-3-2 loss: 1.034015  [   96/  118]
train() client id: f_00009-4-0 loss: 0.982619  [   32/  118]
train() client id: f_00009-4-1 loss: 0.862409  [   64/  118]
train() client id: f_00009-4-2 loss: 0.849988  [   96/  118]
train() client id: f_00009-5-0 loss: 0.692543  [   32/  118]
train() client id: f_00009-5-1 loss: 1.094105  [   64/  118]
train() client id: f_00009-5-2 loss: 0.840802  [   96/  118]
train() client id: f_00009-6-0 loss: 0.893992  [   32/  118]
train() client id: f_00009-6-1 loss: 0.778025  [   64/  118]
train() client id: f_00009-6-2 loss: 0.891448  [   96/  118]
train() client id: f_00009-7-0 loss: 0.711497  [   32/  118]
train() client id: f_00009-7-1 loss: 0.828064  [   64/  118]
train() client id: f_00009-7-2 loss: 0.950494  [   96/  118]
train() client id: f_00009-8-0 loss: 0.984814  [   32/  118]
train() client id: f_00009-8-1 loss: 0.762090  [   64/  118]
train() client id: f_00009-8-2 loss: 0.786510  [   96/  118]
train() client id: f_00009-9-0 loss: 0.867712  [   32/  118]
train() client id: f_00009-9-1 loss: 0.769469  [   64/  118]
train() client id: f_00009-9-2 loss: 0.834278  [   96/  118]
train() client id: f_00009-10-0 loss: 0.739731  [   32/  118]
train() client id: f_00009-10-1 loss: 0.798073  [   64/  118]
train() client id: f_00009-10-2 loss: 0.887667  [   96/  118]
train() client id: f_00009-11-0 loss: 0.870655  [   32/  118]
train() client id: f_00009-11-1 loss: 0.806708  [   64/  118]
train() client id: f_00009-11-2 loss: 0.754346  [   96/  118]
train() client id: f_00009-12-0 loss: 0.726434  [   32/  118]
train() client id: f_00009-12-1 loss: 0.813513  [   64/  118]
train() client id: f_00009-12-2 loss: 0.731864  [   96/  118]
train() client id: f_00009-13-0 loss: 0.839141  [   32/  118]
train() client id: f_00009-13-1 loss: 0.723995  [   64/  118]
train() client id: f_00009-13-2 loss: 0.882345  [   96/  118]
At round 28 accuracy: 0.6419098143236074
At round 28 training accuracy: 0.5881958417169685
At round 28 training loss: 0.8417401449999273
update_location
xs = 8.927491 261.223621 5.882650 10.934260 -177.581990 -25.230757 -5.849135 -5.143845 -200.120581 20.134486 
ys = -252.390647 7.291448 150.684448 -2.290817 -9.642386 0.794442 -11.381692 146.628436 25.881276 -687.232496 
xs mean: -10.68237997052122
ys mean: -63.16579882624052
dists_uav = 271.626101 279.805192 180.943109 100.622094 204.030240 103.136910 100.815452 177.556632 225.206766 694.761758 
uav_gains = -113.218438 -113.962352 -106.487409 -100.067351 -107.937148 -100.335378 -100.088195 -106.272298 -109.355366 -127.997383 
uav_gains_db_mean: -108.57213183870627
dists_bs = 465.287102 467.351266 182.509669 256.911625 184.660438 229.735505 251.694558 172.209347 151.219831 884.037524 
bs_gains = -114.263248 -114.317075 -102.883064 -107.041004 -103.025527 -105.681447 -106.791526 -102.176647 -100.596104 -122.068239 
bs_gains_db_mean: -107.88438807112067
Round 29
-------------------------------
ene_coms = [0.01361155 0.01450103 0.00715749 0.00632009 0.0072057  0.00639137
 0.00632557 0.00692754 0.00646226 0.03715752]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.25562301 14.99944792  7.02546866  2.50699842 17.14428583  8.234393
  3.11862274 10.07884572  7.34621146  7.24490411]
obj_prev = 84.95480086563914
eta_min = 1.1903110808501735e-13	eta_max = 0.7588574311533149
af = 17.81916621666106	bf = 2.0449598568490113	zeta = 19.601082838327166	eta = 0.9090909090909091
af = 17.81916621666106	bf = 2.0449598568490113	zeta = 41.17638572214341	eta = 0.43275207146406863
af = 17.81916621666106	bf = 2.0449598568490113	zeta = 29.916584526968297	eta = 0.5956283612722562
af = 17.81916621666106	bf = 2.0449598568490113	zeta = 27.889575727731582	eta = 0.6389185117270478
af = 17.81916621666106	bf = 2.0449598568490113	zeta = 27.770907661733393	eta = 0.6416486790316465
af = 17.81916621666106	bf = 2.0449598568490113	zeta = 27.77045625669304	eta = 0.6416591089448308
af = 17.81916621666106	bf = 2.0449598568490113	zeta = 27.770456250123747	eta = 0.6416591090966198
eta = 0.6416591090966198
ene_coms = [0.01361155 0.01450103 0.00715749 0.00632009 0.0072057  0.00639137
 0.00632557 0.00692754 0.00646226 0.03715752]
ene_comp = [0.0366129  0.07700333 0.03603175 0.01249488 0.08891705 0.04242448
 0.01569124 0.05201357 0.03777522 0.03428827]
ene_total = [2.55771735 4.65992687 2.19944383 0.95816593 4.8951216  2.48598302
 1.12122261 3.0016194  2.2528262  3.63842945]
ti_comp = [0.42397189 0.41507709 0.48851247 0.49688656 0.48803045 0.49617368
 0.49683169 0.490812   0.49546479 0.18851225]
ti_coms = [0.13611552 0.14501032 0.07157494 0.06320085 0.07205696 0.06391374
 0.06325573 0.06927541 0.06462262 0.37157517]
t_total = [28.52286339 28.52286339 28.52286339 28.52286339 28.52286339 28.52286339
 28.52286339 28.52286339 28.52286339 28.52286339]
ene_coms = [0.01361155 0.01450103 0.00715749 0.00632009 0.0072057  0.00639137
 0.00632557 0.00692754 0.00646226 0.03715752]
ene_comp = [1.70650831e-05 1.65634571e-04 1.22513696e-05 4.93812140e-07
 1.84476620e-04 1.93848313e-05 9.78213398e-07 3.65089754e-05
 1.37238308e-05 7.08986172e-05]
ene_total = [0.69404737 0.74691079 0.36512457 0.32188014 0.37634998 0.3264726
 0.32218427 0.35464939 0.32979434 1.89588441]
optimize_network iter = 0 obj = 5.733297845690878
eta = 0.6416591090966198
freqs = [43178456.42085559 92757863.89996848 36879044.4420329  12573171.37533349
 91097851.62230442 42751638.73014574 15791308.04294813 52987263.22954685
 38120991.06823234 90944420.63723534]
eta_min = 0.6416591090966209	eta_max = 0.6416591090966196
af = 0.02144521468789676	bf = 2.0449598568490113	zeta = 0.023589736156686438	eta = 0.9090909090909091
af = 0.02144521468789676	bf = 2.0449598568490113	zeta = 22.517041845285423	eta = 0.0009523992909569035
af = 0.02144521468789676	bf = 2.0449598568490113	zeta = 2.196235747798026	eta = 0.009764532204431154
af = 0.02144521468789676	bf = 2.0449598568490113	zeta = 2.165373586186129	eta = 0.009903701986902038
af = 0.02144521468789676	bf = 2.0449598568490113	zeta = 2.165371427396533	eta = 0.009903711860500878
eta = 0.009903711860500878
ene_coms = [0.01361155 0.01450103 0.00715749 0.00632009 0.0072057  0.00639137
 0.00632557 0.00692754 0.00646226 0.03715752]
ene_comp = [1.77491688e-04 1.72274342e-03 1.27424886e-04 5.13607521e-06
 1.91871710e-03 2.01619084e-04 1.01742691e-05 3.79725059e-04
 1.42739762e-04 7.37407206e-04]
ene_total = [0.25415017 0.29902547 0.13427061 0.11658213 0.16817491 0.12151751
 0.11677613 0.13468251 0.12173885 0.69845315]
ti_comp = [0.42397189 0.41507709 0.48851247 0.49688656 0.48803045 0.49617368
 0.49683169 0.490812   0.49546479 0.18851225]
ti_coms = [0.13611552 0.14501032 0.07157494 0.06320085 0.07205696 0.06391374
 0.06325573 0.06927541 0.06462262 0.37157517]
t_total = [28.52286339 28.52286339 28.52286339 28.52286339 28.52286339 28.52286339
 28.52286339 28.52286339 28.52286339 28.52286339]
ene_coms = [0.01361155 0.01450103 0.00715749 0.00632009 0.0072057  0.00639137
 0.00632557 0.00692754 0.00646226 0.03715752]
ene_comp = [1.70650831e-05 1.65634571e-04 1.22513696e-05 4.93812140e-07
 1.84476620e-04 1.93848313e-05 9.78213398e-07 3.65089754e-05
 1.37238308e-05 7.08986172e-05]
ene_total = [0.69404737 0.74691079 0.36512457 0.32188014 0.37634998 0.3264726
 0.32218427 0.35464939 0.32979434 1.89588441]
optimize_network iter = 1 obj = 5.733297845690895
eta = 0.6416591090966209
freqs = [43178456.4208556  92757863.89996849 36879044.44203289 12573171.37533349
 91097851.62230438 42751638.73014572 15791308.04294813 52987263.22954683
 38120991.06823232 90944420.6372358 ]
Done!
ene_coms = [0.01361155 0.01450103 0.00715749 0.00632009 0.0072057  0.00639137
 0.00632557 0.00692754 0.00646226 0.03715752]
ene_comp = [1.64438237e-05 1.59604595e-04 1.18053548e-05 4.75834762e-07
 1.77760694e-04 1.86791207e-05 9.42601249e-07 3.51798553e-05
 1.32242106e-05 6.83175319e-05]
ene_total = [0.013628   0.01466064 0.0071693  0.00632056 0.00738346 0.00641005
 0.00632652 0.00696272 0.00647549 0.03722583]
At round 29 energy consumption: 0.11256256083668652
At round 29 eta: 0.6416591090966209
At round 29 a_n: 18.248773294144172
At round 29 local rounds: 14.528930018888493
At round 29 global rounds: 50.925735123722355
gradient difference: 0.34034326672554016
train() client id: f_00000-0-0 loss: 1.515377  [   32/  126]
train() client id: f_00000-0-1 loss: 1.427316  [   64/  126]
train() client id: f_00000-0-2 loss: 1.218228  [   96/  126]
train() client id: f_00000-1-0 loss: 1.159424  [   32/  126]
train() client id: f_00000-1-1 loss: 1.395869  [   64/  126]
train() client id: f_00000-1-2 loss: 1.222263  [   96/  126]
train() client id: f_00000-2-0 loss: 1.102444  [   32/  126]
train() client id: f_00000-2-1 loss: 1.309508  [   64/  126]
train() client id: f_00000-2-2 loss: 1.167907  [   96/  126]
train() client id: f_00000-3-0 loss: 1.200797  [   32/  126]
train() client id: f_00000-3-1 loss: 0.992516  [   64/  126]
train() client id: f_00000-3-2 loss: 1.025914  [   96/  126]
train() client id: f_00000-4-0 loss: 0.964276  [   32/  126]
train() client id: f_00000-4-1 loss: 0.997585  [   64/  126]
train() client id: f_00000-4-2 loss: 1.159572  [   96/  126]
train() client id: f_00000-5-0 loss: 1.026988  [   32/  126]
train() client id: f_00000-5-1 loss: 0.985064  [   64/  126]
train() client id: f_00000-5-2 loss: 0.928761  [   96/  126]
train() client id: f_00000-6-0 loss: 0.898730  [   32/  126]
train() client id: f_00000-6-1 loss: 1.036562  [   64/  126]
train() client id: f_00000-6-2 loss: 0.844757  [   96/  126]
train() client id: f_00000-7-0 loss: 0.921988  [   32/  126]
train() client id: f_00000-7-1 loss: 0.870146  [   64/  126]
train() client id: f_00000-7-2 loss: 0.861170  [   96/  126]
train() client id: f_00000-8-0 loss: 0.883909  [   32/  126]
train() client id: f_00000-8-1 loss: 0.882779  [   64/  126]
train() client id: f_00000-8-2 loss: 0.822355  [   96/  126]
train() client id: f_00000-9-0 loss: 0.897333  [   32/  126]
train() client id: f_00000-9-1 loss: 0.856950  [   64/  126]
train() client id: f_00000-9-2 loss: 0.838543  [   96/  126]
train() client id: f_00000-10-0 loss: 0.894034  [   32/  126]
train() client id: f_00000-10-1 loss: 0.813786  [   64/  126]
train() client id: f_00000-10-2 loss: 0.814971  [   96/  126]
train() client id: f_00000-11-0 loss: 0.781385  [   32/  126]
train() client id: f_00000-11-1 loss: 0.818290  [   64/  126]
train() client id: f_00000-11-2 loss: 0.775840  [   96/  126]
train() client id: f_00000-12-0 loss: 0.783262  [   32/  126]
train() client id: f_00000-12-1 loss: 0.900941  [   64/  126]
train() client id: f_00000-12-2 loss: 0.864308  [   96/  126]
train() client id: f_00000-13-0 loss: 0.808286  [   32/  126]
train() client id: f_00000-13-1 loss: 0.801851  [   64/  126]
train() client id: f_00000-13-2 loss: 0.875011  [   96/  126]
train() client id: f_00001-0-0 loss: 0.438822  [   32/  265]
train() client id: f_00001-0-1 loss: 0.339980  [   64/  265]
train() client id: f_00001-0-2 loss: 0.419046  [   96/  265]
train() client id: f_00001-0-3 loss: 0.438951  [  128/  265]
train() client id: f_00001-0-4 loss: 0.444785  [  160/  265]
train() client id: f_00001-0-5 loss: 0.410046  [  192/  265]
train() client id: f_00001-0-6 loss: 0.429999  [  224/  265]
train() client id: f_00001-0-7 loss: 0.380428  [  256/  265]
train() client id: f_00001-1-0 loss: 0.429007  [   32/  265]
train() client id: f_00001-1-1 loss: 0.403934  [   64/  265]
train() client id: f_00001-1-2 loss: 0.425959  [   96/  265]
train() client id: f_00001-1-3 loss: 0.380923  [  128/  265]
train() client id: f_00001-1-4 loss: 0.470403  [  160/  265]
train() client id: f_00001-1-5 loss: 0.342240  [  192/  265]
train() client id: f_00001-1-6 loss: 0.309243  [  224/  265]
train() client id: f_00001-1-7 loss: 0.482376  [  256/  265]
train() client id: f_00001-2-0 loss: 0.390860  [   32/  265]
train() client id: f_00001-2-1 loss: 0.418341  [   64/  265]
train() client id: f_00001-2-2 loss: 0.577957  [   96/  265]
train() client id: f_00001-2-3 loss: 0.331703  [  128/  265]
train() client id: f_00001-2-4 loss: 0.366794  [  160/  265]
train() client id: f_00001-2-5 loss: 0.321340  [  192/  265]
train() client id: f_00001-2-6 loss: 0.388257  [  224/  265]
train() client id: f_00001-2-7 loss: 0.395930  [  256/  265]
train() client id: f_00001-3-0 loss: 0.359199  [   32/  265]
train() client id: f_00001-3-1 loss: 0.360987  [   64/  265]
train() client id: f_00001-3-2 loss: 0.315857  [   96/  265]
train() client id: f_00001-3-3 loss: 0.349393  [  128/  265]
train() client id: f_00001-3-4 loss: 0.433513  [  160/  265]
train() client id: f_00001-3-5 loss: 0.623723  [  192/  265]
train() client id: f_00001-3-6 loss: 0.325023  [  224/  265]
train() client id: f_00001-3-7 loss: 0.366878  [  256/  265]
train() client id: f_00001-4-0 loss: 0.435654  [   32/  265]
train() client id: f_00001-4-1 loss: 0.397447  [   64/  265]
train() client id: f_00001-4-2 loss: 0.489950  [   96/  265]
train() client id: f_00001-4-3 loss: 0.326198  [  128/  265]
train() client id: f_00001-4-4 loss: 0.346666  [  160/  265]
train() client id: f_00001-4-5 loss: 0.339986  [  192/  265]
train() client id: f_00001-4-6 loss: 0.358221  [  224/  265]
train() client id: f_00001-4-7 loss: 0.416597  [  256/  265]
train() client id: f_00001-5-0 loss: 0.339569  [   32/  265]
train() client id: f_00001-5-1 loss: 0.436233  [   64/  265]
train() client id: f_00001-5-2 loss: 0.338358  [   96/  265]
train() client id: f_00001-5-3 loss: 0.340234  [  128/  265]
train() client id: f_00001-5-4 loss: 0.363063  [  160/  265]
train() client id: f_00001-5-5 loss: 0.315559  [  192/  265]
train() client id: f_00001-5-6 loss: 0.368427  [  224/  265]
train() client id: f_00001-5-7 loss: 0.475144  [  256/  265]
train() client id: f_00001-6-0 loss: 0.526276  [   32/  265]
train() client id: f_00001-6-1 loss: 0.305913  [   64/  265]
train() client id: f_00001-6-2 loss: 0.391835  [   96/  265]
train() client id: f_00001-6-3 loss: 0.303165  [  128/  265]
train() client id: f_00001-6-4 loss: 0.355663  [  160/  265]
train() client id: f_00001-6-5 loss: 0.327667  [  192/  265]
train() client id: f_00001-6-6 loss: 0.294700  [  224/  265]
train() client id: f_00001-6-7 loss: 0.484340  [  256/  265]
train() client id: f_00001-7-0 loss: 0.289327  [   32/  265]
train() client id: f_00001-7-1 loss: 0.442228  [   64/  265]
train() client id: f_00001-7-2 loss: 0.289932  [   96/  265]
train() client id: f_00001-7-3 loss: 0.341568  [  128/  265]
train() client id: f_00001-7-4 loss: 0.405476  [  160/  265]
train() client id: f_00001-7-5 loss: 0.364337  [  192/  265]
train() client id: f_00001-7-6 loss: 0.428012  [  224/  265]
train() client id: f_00001-7-7 loss: 0.426518  [  256/  265]
train() client id: f_00001-8-0 loss: 0.297292  [   32/  265]
train() client id: f_00001-8-1 loss: 0.319821  [   64/  265]
train() client id: f_00001-8-2 loss: 0.303120  [   96/  265]
train() client id: f_00001-8-3 loss: 0.330902  [  128/  265]
train() client id: f_00001-8-4 loss: 0.384547  [  160/  265]
train() client id: f_00001-8-5 loss: 0.370037  [  192/  265]
train() client id: f_00001-8-6 loss: 0.443561  [  224/  265]
train() client id: f_00001-8-7 loss: 0.572977  [  256/  265]
train() client id: f_00001-9-0 loss: 0.388174  [   32/  265]
train() client id: f_00001-9-1 loss: 0.518229  [   64/  265]
train() client id: f_00001-9-2 loss: 0.328650  [   96/  265]
train() client id: f_00001-9-3 loss: 0.340578  [  128/  265]
train() client id: f_00001-9-4 loss: 0.277455  [  160/  265]
train() client id: f_00001-9-5 loss: 0.360831  [  192/  265]
train() client id: f_00001-9-6 loss: 0.341889  [  224/  265]
train() client id: f_00001-9-7 loss: 0.415935  [  256/  265]
train() client id: f_00001-10-0 loss: 0.402820  [   32/  265]
train() client id: f_00001-10-1 loss: 0.458861  [   64/  265]
train() client id: f_00001-10-2 loss: 0.396499  [   96/  265]
train() client id: f_00001-10-3 loss: 0.285402  [  128/  265]
train() client id: f_00001-10-4 loss: 0.273401  [  160/  265]
train() client id: f_00001-10-5 loss: 0.346088  [  192/  265]
train() client id: f_00001-10-6 loss: 0.296675  [  224/  265]
train() client id: f_00001-10-7 loss: 0.492275  [  256/  265]
train() client id: f_00001-11-0 loss: 0.338962  [   32/  265]
train() client id: f_00001-11-1 loss: 0.479167  [   64/  265]
train() client id: f_00001-11-2 loss: 0.327324  [   96/  265]
train() client id: f_00001-11-3 loss: 0.274768  [  128/  265]
train() client id: f_00001-11-4 loss: 0.469220  [  160/  265]
train() client id: f_00001-11-5 loss: 0.367547  [  192/  265]
train() client id: f_00001-11-6 loss: 0.269103  [  224/  265]
train() client id: f_00001-11-7 loss: 0.356887  [  256/  265]
train() client id: f_00001-12-0 loss: 0.282139  [   32/  265]
train() client id: f_00001-12-1 loss: 0.458569  [   64/  265]
train() client id: f_00001-12-2 loss: 0.621848  [   96/  265]
train() client id: f_00001-12-3 loss: 0.387122  [  128/  265]
train() client id: f_00001-12-4 loss: 0.349503  [  160/  265]
train() client id: f_00001-12-5 loss: 0.328725  [  192/  265]
train() client id: f_00001-12-6 loss: 0.287199  [  224/  265]
train() client id: f_00001-12-7 loss: 0.279078  [  256/  265]
train() client id: f_00001-13-0 loss: 0.352748  [   32/  265]
train() client id: f_00001-13-1 loss: 0.353861  [   64/  265]
train() client id: f_00001-13-2 loss: 0.556235  [   96/  265]
train() client id: f_00001-13-3 loss: 0.313555  [  128/  265]
train() client id: f_00001-13-4 loss: 0.279742  [  160/  265]
train() client id: f_00001-13-5 loss: 0.384498  [  192/  265]
train() client id: f_00001-13-6 loss: 0.390203  [  224/  265]
train() client id: f_00001-13-7 loss: 0.361827  [  256/  265]
train() client id: f_00002-0-0 loss: 1.438277  [   32/  124]
train() client id: f_00002-0-1 loss: 1.479663  [   64/  124]
train() client id: f_00002-0-2 loss: 1.392009  [   96/  124]
train() client id: f_00002-1-0 loss: 1.445553  [   32/  124]
train() client id: f_00002-1-1 loss: 1.416829  [   64/  124]
train() client id: f_00002-1-2 loss: 1.476049  [   96/  124]
train() client id: f_00002-2-0 loss: 1.310234  [   32/  124]
train() client id: f_00002-2-1 loss: 1.545434  [   64/  124]
train() client id: f_00002-2-2 loss: 1.393330  [   96/  124]
train() client id: f_00002-3-0 loss: 1.442564  [   32/  124]
train() client id: f_00002-3-1 loss: 1.223087  [   64/  124]
train() client id: f_00002-3-2 loss: 1.281254  [   96/  124]
train() client id: f_00002-4-0 loss: 1.211696  [   32/  124]
train() client id: f_00002-4-1 loss: 1.384009  [   64/  124]
train() client id: f_00002-4-2 loss: 1.197509  [   96/  124]
train() client id: f_00002-5-0 loss: 1.271966  [   32/  124]
train() client id: f_00002-5-1 loss: 1.172571  [   64/  124]
train() client id: f_00002-5-2 loss: 1.411018  [   96/  124]
train() client id: f_00002-6-0 loss: 1.298264  [   32/  124]
train() client id: f_00002-6-1 loss: 1.154230  [   64/  124]
train() client id: f_00002-6-2 loss: 1.267373  [   96/  124]
train() client id: f_00002-7-0 loss: 1.170257  [   32/  124]
train() client id: f_00002-7-1 loss: 1.288967  [   64/  124]
train() client id: f_00002-7-2 loss: 1.334254  [   96/  124]
train() client id: f_00002-8-0 loss: 1.209394  [   32/  124]
train() client id: f_00002-8-1 loss: 1.292600  [   64/  124]
train() client id: f_00002-8-2 loss: 1.230825  [   96/  124]
train() client id: f_00002-9-0 loss: 1.157904  [   32/  124]
train() client id: f_00002-9-1 loss: 1.392208  [   64/  124]
train() client id: f_00002-9-2 loss: 1.116095  [   96/  124]
train() client id: f_00002-10-0 loss: 1.157918  [   32/  124]
train() client id: f_00002-10-1 loss: 1.151700  [   64/  124]
train() client id: f_00002-10-2 loss: 1.287836  [   96/  124]
train() client id: f_00002-11-0 loss: 1.030755  [   32/  124]
train() client id: f_00002-11-1 loss: 1.376911  [   64/  124]
train() client id: f_00002-11-2 loss: 1.178065  [   96/  124]
train() client id: f_00002-12-0 loss: 1.363702  [   32/  124]
train() client id: f_00002-12-1 loss: 1.173161  [   64/  124]
train() client id: f_00002-12-2 loss: 1.250385  [   96/  124]
train() client id: f_00002-13-0 loss: 1.293662  [   32/  124]
train() client id: f_00002-13-1 loss: 1.102486  [   64/  124]
train() client id: f_00002-13-2 loss: 1.275260  [   96/  124]
train() client id: f_00003-0-0 loss: 0.647721  [   32/   43]
train() client id: f_00003-1-0 loss: 0.650951  [   32/   43]
train() client id: f_00003-2-0 loss: 0.683055  [   32/   43]
train() client id: f_00003-3-0 loss: 0.673601  [   32/   43]
train() client id: f_00003-4-0 loss: 0.932170  [   32/   43]
train() client id: f_00003-5-0 loss: 0.820089  [   32/   43]
train() client id: f_00003-6-0 loss: 0.924364  [   32/   43]
train() client id: f_00003-7-0 loss: 0.726444  [   32/   43]
train() client id: f_00003-8-0 loss: 0.748230  [   32/   43]
train() client id: f_00003-9-0 loss: 0.734214  [   32/   43]
train() client id: f_00003-10-0 loss: 0.675801  [   32/   43]
train() client id: f_00003-11-0 loss: 0.728901  [   32/   43]
train() client id: f_00003-12-0 loss: 0.731834  [   32/   43]
train() client id: f_00003-13-0 loss: 0.742054  [   32/   43]
train() client id: f_00004-0-0 loss: 0.943612  [   32/  306]
train() client id: f_00004-0-1 loss: 0.832832  [   64/  306]
train() client id: f_00004-0-2 loss: 0.997204  [   96/  306]
train() client id: f_00004-0-3 loss: 0.891954  [  128/  306]
train() client id: f_00004-0-4 loss: 0.867321  [  160/  306]
train() client id: f_00004-0-5 loss: 0.881415  [  192/  306]
train() client id: f_00004-0-6 loss: 0.805828  [  224/  306]
train() client id: f_00004-0-7 loss: 0.799059  [  256/  306]
train() client id: f_00004-0-8 loss: 0.957822  [  288/  306]
train() client id: f_00004-1-0 loss: 0.963096  [   32/  306]
train() client id: f_00004-1-1 loss: 0.912989  [   64/  306]
train() client id: f_00004-1-2 loss: 0.974018  [   96/  306]
train() client id: f_00004-1-3 loss: 0.770732  [  128/  306]
train() client id: f_00004-1-4 loss: 0.852937  [  160/  306]
train() client id: f_00004-1-5 loss: 0.956406  [  192/  306]
train() client id: f_00004-1-6 loss: 1.026612  [  224/  306]
train() client id: f_00004-1-7 loss: 0.903404  [  256/  306]
train() client id: f_00004-1-8 loss: 0.782486  [  288/  306]
train() client id: f_00004-2-0 loss: 0.859597  [   32/  306]
train() client id: f_00004-2-1 loss: 0.919882  [   64/  306]
train() client id: f_00004-2-2 loss: 0.861974  [   96/  306]
train() client id: f_00004-2-3 loss: 1.027022  [  128/  306]
train() client id: f_00004-2-4 loss: 0.908055  [  160/  306]
train() client id: f_00004-2-5 loss: 0.921046  [  192/  306]
train() client id: f_00004-2-6 loss: 0.905999  [  224/  306]
train() client id: f_00004-2-7 loss: 0.881471  [  256/  306]
train() client id: f_00004-2-8 loss: 0.764776  [  288/  306]
train() client id: f_00004-3-0 loss: 0.859970  [   32/  306]
train() client id: f_00004-3-1 loss: 0.988479  [   64/  306]
train() client id: f_00004-3-2 loss: 0.838564  [   96/  306]
train() client id: f_00004-3-3 loss: 1.004392  [  128/  306]
train() client id: f_00004-3-4 loss: 0.908310  [  160/  306]
train() client id: f_00004-3-5 loss: 0.798859  [  192/  306]
train() client id: f_00004-3-6 loss: 0.788140  [  224/  306]
train() client id: f_00004-3-7 loss: 0.923770  [  256/  306]
train() client id: f_00004-3-8 loss: 0.972686  [  288/  306]
train() client id: f_00004-4-0 loss: 0.899821  [   32/  306]
train() client id: f_00004-4-1 loss: 0.722286  [   64/  306]
train() client id: f_00004-4-2 loss: 0.785049  [   96/  306]
train() client id: f_00004-4-3 loss: 0.996639  [  128/  306]
train() client id: f_00004-4-4 loss: 0.938414  [  160/  306]
train() client id: f_00004-4-5 loss: 0.970265  [  192/  306]
train() client id: f_00004-4-6 loss: 0.911621  [  224/  306]
train() client id: f_00004-4-7 loss: 0.827458  [  256/  306]
train() client id: f_00004-4-8 loss: 0.910186  [  288/  306]
train() client id: f_00004-5-0 loss: 0.873457  [   32/  306]
train() client id: f_00004-5-1 loss: 1.047470  [   64/  306]
train() client id: f_00004-5-2 loss: 0.992830  [   96/  306]
train() client id: f_00004-5-3 loss: 0.866991  [  128/  306]
train() client id: f_00004-5-4 loss: 0.827883  [  160/  306]
train() client id: f_00004-5-5 loss: 0.783954  [  192/  306]
train() client id: f_00004-5-6 loss: 0.990184  [  224/  306]
train() client id: f_00004-5-7 loss: 0.941527  [  256/  306]
train() client id: f_00004-5-8 loss: 0.793593  [  288/  306]
train() client id: f_00004-6-0 loss: 0.816297  [   32/  306]
train() client id: f_00004-6-1 loss: 0.823391  [   64/  306]
train() client id: f_00004-6-2 loss: 0.899169  [   96/  306]
train() client id: f_00004-6-3 loss: 0.823962  [  128/  306]
train() client id: f_00004-6-4 loss: 1.055811  [  160/  306]
train() client id: f_00004-6-5 loss: 0.961078  [  192/  306]
train() client id: f_00004-6-6 loss: 0.860319  [  224/  306]
train() client id: f_00004-6-7 loss: 0.941982  [  256/  306]
train() client id: f_00004-6-8 loss: 0.786205  [  288/  306]
train() client id: f_00004-7-0 loss: 0.957724  [   32/  306]
train() client id: f_00004-7-1 loss: 0.886035  [   64/  306]
train() client id: f_00004-7-2 loss: 0.912077  [   96/  306]
train() client id: f_00004-7-3 loss: 0.926445  [  128/  306]
train() client id: f_00004-7-4 loss: 0.804690  [  160/  306]
train() client id: f_00004-7-5 loss: 0.865058  [  192/  306]
train() client id: f_00004-7-6 loss: 0.768985  [  224/  306]
train() client id: f_00004-7-7 loss: 0.922691  [  256/  306]
train() client id: f_00004-7-8 loss: 0.921207  [  288/  306]
train() client id: f_00004-8-0 loss: 0.924646  [   32/  306]
train() client id: f_00004-8-1 loss: 0.903796  [   64/  306]
train() client id: f_00004-8-2 loss: 0.693699  [   96/  306]
train() client id: f_00004-8-3 loss: 0.887895  [  128/  306]
train() client id: f_00004-8-4 loss: 0.982474  [  160/  306]
train() client id: f_00004-8-5 loss: 0.991007  [  192/  306]
train() client id: f_00004-8-6 loss: 0.847234  [  224/  306]
train() client id: f_00004-8-7 loss: 0.847107  [  256/  306]
train() client id: f_00004-8-8 loss: 0.987441  [  288/  306]
train() client id: f_00004-9-0 loss: 0.940383  [   32/  306]
train() client id: f_00004-9-1 loss: 0.855859  [   64/  306]
train() client id: f_00004-9-2 loss: 0.948908  [   96/  306]
train() client id: f_00004-9-3 loss: 0.782747  [  128/  306]
train() client id: f_00004-9-4 loss: 0.894356  [  160/  306]
train() client id: f_00004-9-5 loss: 0.832280  [  192/  306]
train() client id: f_00004-9-6 loss: 0.742467  [  224/  306]
train() client id: f_00004-9-7 loss: 0.898890  [  256/  306]
train() client id: f_00004-9-8 loss: 1.051671  [  288/  306]
train() client id: f_00004-10-0 loss: 0.843510  [   32/  306]
train() client id: f_00004-10-1 loss: 0.719505  [   64/  306]
train() client id: f_00004-10-2 loss: 0.885687  [   96/  306]
train() client id: f_00004-10-3 loss: 0.887173  [  128/  306]
train() client id: f_00004-10-4 loss: 1.001760  [  160/  306]
train() client id: f_00004-10-5 loss: 0.824062  [  192/  306]
train() client id: f_00004-10-6 loss: 0.926138  [  224/  306]
train() client id: f_00004-10-7 loss: 0.919698  [  256/  306]
train() client id: f_00004-10-8 loss: 1.102411  [  288/  306]
train() client id: f_00004-11-0 loss: 0.908889  [   32/  306]
train() client id: f_00004-11-1 loss: 0.943150  [   64/  306]
train() client id: f_00004-11-2 loss: 0.865874  [   96/  306]
train() client id: f_00004-11-3 loss: 0.838508  [  128/  306]
train() client id: f_00004-11-4 loss: 0.949862  [  160/  306]
train() client id: f_00004-11-5 loss: 0.920563  [  192/  306]
train() client id: f_00004-11-6 loss: 0.831314  [  224/  306]
train() client id: f_00004-11-7 loss: 0.844698  [  256/  306]
train() client id: f_00004-11-8 loss: 0.822450  [  288/  306]
train() client id: f_00004-12-0 loss: 0.882117  [   32/  306]
train() client id: f_00004-12-1 loss: 0.876428  [   64/  306]
train() client id: f_00004-12-2 loss: 0.857363  [   96/  306]
train() client id: f_00004-12-3 loss: 0.932398  [  128/  306]
train() client id: f_00004-12-4 loss: 0.892091  [  160/  306]
train() client id: f_00004-12-5 loss: 0.885105  [  192/  306]
train() client id: f_00004-12-6 loss: 0.885374  [  224/  306]
train() client id: f_00004-12-7 loss: 0.850413  [  256/  306]
train() client id: f_00004-12-8 loss: 0.939064  [  288/  306]
train() client id: f_00004-13-0 loss: 0.768568  [   32/  306]
train() client id: f_00004-13-1 loss: 0.986711  [   64/  306]
train() client id: f_00004-13-2 loss: 0.929369  [   96/  306]
train() client id: f_00004-13-3 loss: 0.981858  [  128/  306]
train() client id: f_00004-13-4 loss: 0.933563  [  160/  306]
train() client id: f_00004-13-5 loss: 0.755632  [  192/  306]
train() client id: f_00004-13-6 loss: 0.840269  [  224/  306]
train() client id: f_00004-13-7 loss: 0.749044  [  256/  306]
train() client id: f_00004-13-8 loss: 0.911960  [  288/  306]
train() client id: f_00005-0-0 loss: 0.555325  [   32/  146]
train() client id: f_00005-0-1 loss: 0.586769  [   64/  146]
train() client id: f_00005-0-2 loss: 0.443164  [   96/  146]
train() client id: f_00005-0-3 loss: 0.728146  [  128/  146]
train() client id: f_00005-1-0 loss: 0.550314  [   32/  146]
train() client id: f_00005-1-1 loss: 0.629068  [   64/  146]
train() client id: f_00005-1-2 loss: 0.705766  [   96/  146]
train() client id: f_00005-1-3 loss: 0.460856  [  128/  146]
train() client id: f_00005-2-0 loss: 0.542055  [   32/  146]
train() client id: f_00005-2-1 loss: 0.514819  [   64/  146]
train() client id: f_00005-2-2 loss: 0.624557  [   96/  146]
train() client id: f_00005-2-3 loss: 0.505611  [  128/  146]
train() client id: f_00005-3-0 loss: 0.487403  [   32/  146]
train() client id: f_00005-3-1 loss: 0.509523  [   64/  146]
train() client id: f_00005-3-2 loss: 0.648740  [   96/  146]
train() client id: f_00005-3-3 loss: 0.592146  [  128/  146]
train() client id: f_00005-4-0 loss: 0.519076  [   32/  146]
train() client id: f_00005-4-1 loss: 0.483932  [   64/  146]
train() client id: f_00005-4-2 loss: 0.717225  [   96/  146]
train() client id: f_00005-4-3 loss: 0.627548  [  128/  146]
train() client id: f_00005-5-0 loss: 0.467454  [   32/  146]
train() client id: f_00005-5-1 loss: 0.532558  [   64/  146]
train() client id: f_00005-5-2 loss: 0.503824  [   96/  146]
train() client id: f_00005-5-3 loss: 0.778705  [  128/  146]
train() client id: f_00005-6-0 loss: 0.413209  [   32/  146]
train() client id: f_00005-6-1 loss: 0.485509  [   64/  146]
train() client id: f_00005-6-2 loss: 0.532156  [   96/  146]
train() client id: f_00005-6-3 loss: 0.698402  [  128/  146]
train() client id: f_00005-7-0 loss: 0.440778  [   32/  146]
train() client id: f_00005-7-1 loss: 0.696455  [   64/  146]
train() client id: f_00005-7-2 loss: 0.485667  [   96/  146]
train() client id: f_00005-7-3 loss: 0.460704  [  128/  146]
train() client id: f_00005-8-0 loss: 0.434753  [   32/  146]
train() client id: f_00005-8-1 loss: 0.717956  [   64/  146]
train() client id: f_00005-8-2 loss: 0.424359  [   96/  146]
train() client id: f_00005-8-3 loss: 0.600005  [  128/  146]
train() client id: f_00005-9-0 loss: 0.388788  [   32/  146]
train() client id: f_00005-9-1 loss: 0.569890  [   64/  146]
train() client id: f_00005-9-2 loss: 0.544225  [   96/  146]
train() client id: f_00005-9-3 loss: 0.638378  [  128/  146]
train() client id: f_00005-10-0 loss: 0.295840  [   32/  146]
train() client id: f_00005-10-1 loss: 0.708939  [   64/  146]
train() client id: f_00005-10-2 loss: 0.501500  [   96/  146]
train() client id: f_00005-10-3 loss: 0.517788  [  128/  146]
train() client id: f_00005-11-0 loss: 0.342543  [   32/  146]
train() client id: f_00005-11-1 loss: 0.381026  [   64/  146]
train() client id: f_00005-11-2 loss: 0.604132  [   96/  146]
train() client id: f_00005-11-3 loss: 0.758520  [  128/  146]
train() client id: f_00005-12-0 loss: 0.519670  [   32/  146]
train() client id: f_00005-12-1 loss: 0.455259  [   64/  146]
train() client id: f_00005-12-2 loss: 0.563231  [   96/  146]
train() client id: f_00005-12-3 loss: 0.580554  [  128/  146]
train() client id: f_00005-13-0 loss: 0.430052  [   32/  146]
train() client id: f_00005-13-1 loss: 0.664418  [   64/  146]
train() client id: f_00005-13-2 loss: 0.563747  [   96/  146]
train() client id: f_00005-13-3 loss: 0.513852  [  128/  146]
train() client id: f_00006-0-0 loss: 0.551069  [   32/   54]
train() client id: f_00006-1-0 loss: 0.538147  [   32/   54]
train() client id: f_00006-2-0 loss: 0.589700  [   32/   54]
train() client id: f_00006-3-0 loss: 0.575808  [   32/   54]
train() client id: f_00006-4-0 loss: 0.468344  [   32/   54]
train() client id: f_00006-5-0 loss: 0.523056  [   32/   54]
train() client id: f_00006-6-0 loss: 0.597892  [   32/   54]
train() client id: f_00006-7-0 loss: 0.547478  [   32/   54]
train() client id: f_00006-8-0 loss: 0.604133  [   32/   54]
train() client id: f_00006-9-0 loss: 0.551025  [   32/   54]
train() client id: f_00006-10-0 loss: 0.498354  [   32/   54]
train() client id: f_00006-11-0 loss: 0.595512  [   32/   54]
train() client id: f_00006-12-0 loss: 0.537993  [   32/   54]
train() client id: f_00006-13-0 loss: 0.591452  [   32/   54]
train() client id: f_00007-0-0 loss: 0.556563  [   32/  179]
train() client id: f_00007-0-1 loss: 0.536622  [   64/  179]
train() client id: f_00007-0-2 loss: 0.385202  [   96/  179]
train() client id: f_00007-0-3 loss: 0.930461  [  128/  179]
train() client id: f_00007-0-4 loss: 0.545679  [  160/  179]
train() client id: f_00007-1-0 loss: 0.739015  [   32/  179]
train() client id: f_00007-1-1 loss: 0.510247  [   64/  179]
train() client id: f_00007-1-2 loss: 0.511593  [   96/  179]
train() client id: f_00007-1-3 loss: 0.517170  [  128/  179]
train() client id: f_00007-1-4 loss: 0.550798  [  160/  179]
train() client id: f_00007-2-0 loss: 0.716958  [   32/  179]
train() client id: f_00007-2-1 loss: 0.532437  [   64/  179]
train() client id: f_00007-2-2 loss: 0.555516  [   96/  179]
train() client id: f_00007-2-3 loss: 0.473644  [  128/  179]
train() client id: f_00007-2-4 loss: 0.439088  [  160/  179]
train() client id: f_00007-3-0 loss: 0.397286  [   32/  179]
train() client id: f_00007-3-1 loss: 0.503045  [   64/  179]
train() client id: f_00007-3-2 loss: 0.656247  [   96/  179]
train() client id: f_00007-3-3 loss: 0.531369  [  128/  179]
train() client id: f_00007-3-4 loss: 0.471783  [  160/  179]
train() client id: f_00007-4-0 loss: 0.517755  [   32/  179]
train() client id: f_00007-4-1 loss: 0.410377  [   64/  179]
train() client id: f_00007-4-2 loss: 0.639454  [   96/  179]
train() client id: f_00007-4-3 loss: 0.768963  [  128/  179]
train() client id: f_00007-4-4 loss: 0.396946  [  160/  179]
train() client id: f_00007-5-0 loss: 0.595443  [   32/  179]
train() client id: f_00007-5-1 loss: 0.679994  [   64/  179]
train() client id: f_00007-5-2 loss: 0.397916  [   96/  179]
train() client id: f_00007-5-3 loss: 0.554597  [  128/  179]
train() client id: f_00007-5-4 loss: 0.408056  [  160/  179]
train() client id: f_00007-6-0 loss: 0.411530  [   32/  179]
train() client id: f_00007-6-1 loss: 0.507580  [   64/  179]
train() client id: f_00007-6-2 loss: 0.464559  [   96/  179]
train() client id: f_00007-6-3 loss: 0.544697  [  128/  179]
train() client id: f_00007-6-4 loss: 0.655534  [  160/  179]
train() client id: f_00007-7-0 loss: 0.394615  [   32/  179]
train() client id: f_00007-7-1 loss: 0.453859  [   64/  179]
train() client id: f_00007-7-2 loss: 0.565028  [   96/  179]
train() client id: f_00007-7-3 loss: 0.487116  [  128/  179]
train() client id: f_00007-7-4 loss: 0.524063  [  160/  179]
train() client id: f_00007-8-0 loss: 0.498373  [   32/  179]
train() client id: f_00007-8-1 loss: 0.658800  [   64/  179]
train() client id: f_00007-8-2 loss: 0.634843  [   96/  179]
train() client id: f_00007-8-3 loss: 0.395562  [  128/  179]
train() client id: f_00007-8-4 loss: 0.464180  [  160/  179]
train() client id: f_00007-9-0 loss: 0.536810  [   32/  179]
train() client id: f_00007-9-1 loss: 0.490891  [   64/  179]
train() client id: f_00007-9-2 loss: 0.342978  [   96/  179]
train() client id: f_00007-9-3 loss: 0.582251  [  128/  179]
train() client id: f_00007-9-4 loss: 0.587663  [  160/  179]
train() client id: f_00007-10-0 loss: 0.355028  [   32/  179]
train() client id: f_00007-10-1 loss: 0.540030  [   64/  179]
train() client id: f_00007-10-2 loss: 0.530569  [   96/  179]
train() client id: f_00007-10-3 loss: 0.474286  [  128/  179]
train() client id: f_00007-10-4 loss: 0.555681  [  160/  179]
train() client id: f_00007-11-0 loss: 0.599411  [   32/  179]
train() client id: f_00007-11-1 loss: 0.362538  [   64/  179]
train() client id: f_00007-11-2 loss: 0.365021  [   96/  179]
train() client id: f_00007-11-3 loss: 0.601153  [  128/  179]
train() client id: f_00007-11-4 loss: 0.530758  [  160/  179]
train() client id: f_00007-12-0 loss: 0.473467  [   32/  179]
train() client id: f_00007-12-1 loss: 0.337604  [   64/  179]
train() client id: f_00007-12-2 loss: 0.353682  [   96/  179]
train() client id: f_00007-12-3 loss: 0.448755  [  128/  179]
train() client id: f_00007-12-4 loss: 0.728670  [  160/  179]
train() client id: f_00007-13-0 loss: 0.376048  [   32/  179]
train() client id: f_00007-13-1 loss: 0.642268  [   64/  179]
train() client id: f_00007-13-2 loss: 0.450309  [   96/  179]
train() client id: f_00007-13-3 loss: 0.438060  [  128/  179]
train() client id: f_00007-13-4 loss: 0.544478  [  160/  179]
train() client id: f_00008-0-0 loss: 0.782634  [   32/  130]
train() client id: f_00008-0-1 loss: 0.712170  [   64/  130]
train() client id: f_00008-0-2 loss: 0.710397  [   96/  130]
train() client id: f_00008-0-3 loss: 0.862448  [  128/  130]
train() client id: f_00008-1-0 loss: 0.709015  [   32/  130]
train() client id: f_00008-1-1 loss: 0.831900  [   64/  130]
train() client id: f_00008-1-2 loss: 0.776854  [   96/  130]
train() client id: f_00008-1-3 loss: 0.742871  [  128/  130]
train() client id: f_00008-2-0 loss: 0.808016  [   32/  130]
train() client id: f_00008-2-1 loss: 0.724535  [   64/  130]
train() client id: f_00008-2-2 loss: 0.759606  [   96/  130]
train() client id: f_00008-2-3 loss: 0.767258  [  128/  130]
train() client id: f_00008-3-0 loss: 0.682653  [   32/  130]
train() client id: f_00008-3-1 loss: 0.838522  [   64/  130]
train() client id: f_00008-3-2 loss: 0.869711  [   96/  130]
train() client id: f_00008-3-3 loss: 0.682229  [  128/  130]
train() client id: f_00008-4-0 loss: 0.777152  [   32/  130]
train() client id: f_00008-4-1 loss: 0.910084  [   64/  130]
train() client id: f_00008-4-2 loss: 0.671690  [   96/  130]
train() client id: f_00008-4-3 loss: 0.711052  [  128/  130]
train() client id: f_00008-5-0 loss: 0.710446  [   32/  130]
train() client id: f_00008-5-1 loss: 0.788658  [   64/  130]
train() client id: f_00008-5-2 loss: 0.739435  [   96/  130]
train() client id: f_00008-5-3 loss: 0.804138  [  128/  130]
train() client id: f_00008-6-0 loss: 0.732182  [   32/  130]
train() client id: f_00008-6-1 loss: 0.772123  [   64/  130]
train() client id: f_00008-6-2 loss: 0.779698  [   96/  130]
train() client id: f_00008-6-3 loss: 0.780676  [  128/  130]
train() client id: f_00008-7-0 loss: 0.651728  [   32/  130]
train() client id: f_00008-7-1 loss: 0.819688  [   64/  130]
train() client id: f_00008-7-2 loss: 0.806196  [   96/  130]
train() client id: f_00008-7-3 loss: 0.750361  [  128/  130]
train() client id: f_00008-8-0 loss: 0.776289  [   32/  130]
train() client id: f_00008-8-1 loss: 0.799733  [   64/  130]
train() client id: f_00008-8-2 loss: 0.789977  [   96/  130]
train() client id: f_00008-8-3 loss: 0.711144  [  128/  130]
train() client id: f_00008-9-0 loss: 0.584573  [   32/  130]
train() client id: f_00008-9-1 loss: 0.868791  [   64/  130]
train() client id: f_00008-9-2 loss: 0.768985  [   96/  130]
train() client id: f_00008-9-3 loss: 0.811073  [  128/  130]
train() client id: f_00008-10-0 loss: 0.765836  [   32/  130]
train() client id: f_00008-10-1 loss: 0.760296  [   64/  130]
train() client id: f_00008-10-2 loss: 0.707782  [   96/  130]
train() client id: f_00008-10-3 loss: 0.835690  [  128/  130]
train() client id: f_00008-11-0 loss: 0.773980  [   32/  130]
train() client id: f_00008-11-1 loss: 0.792983  [   64/  130]
train() client id: f_00008-11-2 loss: 0.763660  [   96/  130]
train() client id: f_00008-11-3 loss: 0.732634  [  128/  130]
train() client id: f_00008-12-0 loss: 0.798176  [   32/  130]
train() client id: f_00008-12-1 loss: 0.716824  [   64/  130]
train() client id: f_00008-12-2 loss: 0.710863  [   96/  130]
train() client id: f_00008-12-3 loss: 0.845700  [  128/  130]
train() client id: f_00008-13-0 loss: 0.721045  [   32/  130]
train() client id: f_00008-13-1 loss: 0.652448  [   64/  130]
train() client id: f_00008-13-2 loss: 0.920362  [   96/  130]
train() client id: f_00008-13-3 loss: 0.782361  [  128/  130]
train() client id: f_00009-0-0 loss: 1.166119  [   32/  118]
train() client id: f_00009-0-1 loss: 1.102857  [   64/  118]
train() client id: f_00009-0-2 loss: 1.054532  [   96/  118]
train() client id: f_00009-1-0 loss: 1.117366  [   32/  118]
train() client id: f_00009-1-1 loss: 0.948104  [   64/  118]
train() client id: f_00009-1-2 loss: 1.020411  [   96/  118]
train() client id: f_00009-2-0 loss: 0.960427  [   32/  118]
train() client id: f_00009-2-1 loss: 1.089501  [   64/  118]
train() client id: f_00009-2-2 loss: 0.806461  [   96/  118]
train() client id: f_00009-3-0 loss: 1.022923  [   32/  118]
train() client id: f_00009-3-1 loss: 0.945568  [   64/  118]
train() client id: f_00009-3-2 loss: 0.834553  [   96/  118]
train() client id: f_00009-4-0 loss: 0.908373  [   32/  118]
train() client id: f_00009-4-1 loss: 0.794746  [   64/  118]
train() client id: f_00009-4-2 loss: 0.986326  [   96/  118]
train() client id: f_00009-5-0 loss: 0.769990  [   32/  118]
train() client id: f_00009-5-1 loss: 0.807554  [   64/  118]
train() client id: f_00009-5-2 loss: 0.975945  [   96/  118]
train() client id: f_00009-6-0 loss: 0.865717  [   32/  118]
train() client id: f_00009-6-1 loss: 0.764060  [   64/  118]
train() client id: f_00009-6-2 loss: 0.900685  [   96/  118]
train() client id: f_00009-7-0 loss: 0.956729  [   32/  118]
train() client id: f_00009-7-1 loss: 0.705874  [   64/  118]
train() client id: f_00009-7-2 loss: 0.804696  [   96/  118]
train() client id: f_00009-8-0 loss: 0.802677  [   32/  118]
train() client id: f_00009-8-1 loss: 0.802061  [   64/  118]
train() client id: f_00009-8-2 loss: 0.737341  [   96/  118]
train() client id: f_00009-9-0 loss: 0.789291  [   32/  118]
train() client id: f_00009-9-1 loss: 0.767418  [   64/  118]
train() client id: f_00009-9-2 loss: 0.821008  [   96/  118]
train() client id: f_00009-10-0 loss: 0.865416  [   32/  118]
train() client id: f_00009-10-1 loss: 0.691671  [   64/  118]
train() client id: f_00009-10-2 loss: 0.716222  [   96/  118]
train() client id: f_00009-11-0 loss: 0.729856  [   32/  118]
train() client id: f_00009-11-1 loss: 0.696883  [   64/  118]
train() client id: f_00009-11-2 loss: 0.739439  [   96/  118]
train() client id: f_00009-12-0 loss: 0.729164  [   32/  118]
train() client id: f_00009-12-1 loss: 0.693478  [   64/  118]
train() client id: f_00009-12-2 loss: 0.739216  [   96/  118]
train() client id: f_00009-13-0 loss: 0.682998  [   32/  118]
train() client id: f_00009-13-1 loss: 0.650288  [   64/  118]
train() client id: f_00009-13-2 loss: 0.724136  [   96/  118]
At round 29 accuracy: 0.6419098143236074
At round 29 training accuracy: 0.5895372233400402
At round 29 training loss: 0.8323217912419061
update_location
xs = 8.927491 266.223621 5.882650 10.934260 -182.581990 -30.230757 -5.849135 -5.143845 -205.120581 20.134486 
ys = -257.390647 7.291448 155.684448 2.709183 -9.642386 0.794442 -6.381692 151.628436 25.881276 -692.232496 
xs mean: -11.68237997052122
ys mean: -62.16579882624052
dists_uav = 276.278203 284.478789 185.127667 100.632488 208.396638 104.472627 100.373993 181.707572 229.661258 699.707957 
uav_gains = -113.642133 -114.383431 -106.750973 -100.068473 -108.217243 -100.475093 -100.040547 -106.535721 -109.678670 -128.075475 
uav_gains_db_mean: -108.78677573100583
dists_bs = 469.884022 472.021654 181.911032 253.487031 184.797991 226.507638 248.014785 171.456535 152.130349 888.914883 
bs_gains = -114.382798 -114.437993 -102.843112 -106.877820 -103.034582 -105.509379 -106.612431 -102.123372 -100.669103 -122.135144 
bs_gains_db_mean: -107.86257351528859
Round 30
-------------------------------
ene_coms = [0.01410588 0.01505117 0.00714409 0.00632038 0.00720878 0.00642917
 0.00631304 0.00691079 0.00648239 0.03753165]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.12836936 14.72784524  6.89335187  2.45994515 16.82252801  8.08050965
  3.05985675  9.8893537   7.20868051  7.11567782]
obj_prev = 83.38611804989347
eta_min = 6.940564154785509e-14	eta_max = 0.7606360556642507
af = 17.48468447996636	bf = 2.032309190844758	zeta = 19.233152927962998	eta = 0.9090909090909091
af = 17.48468447996636	bf = 2.032309190844758	zeta = 40.686553736974645	eta = 0.42974110299434964
af = 17.48468447996636	bf = 2.032309190844758	zeta = 29.459233558497782	eta = 0.5935213638619171
af = 17.48468447996636	bf = 2.032309190844758	zeta = 27.439955209779942	eta = 0.6371979963631501
af = 17.48468447996636	bf = 2.032309190844758	zeta = 27.321284243733917	eta = 0.6399656884348853
af = 17.48468447996636	bf = 2.032309190844758	zeta = 27.320828772999107	eta = 0.6399763574246435
af = 17.48468447996636	bf = 2.032309190844758	zeta = 27.3208287662504	eta = 0.6399763575827285
eta = 0.6399763575827285
ene_coms = [0.01410588 0.01505117 0.00714409 0.00632038 0.00720878 0.00642917
 0.00631304 0.00691079 0.00648239 0.03753165]
ene_comp = [0.03682959 0.07745906 0.03624499 0.01256883 0.08944329 0.04267556
 0.01578411 0.0523214  0.03799878 0.0344912 ]
ene_total = [2.53333937 4.60111233 2.15801051 0.93947849 4.80711205 2.44228512
 1.0990296  2.94598723 2.21232691 3.58214717]
ti_comp = [0.43139945 0.42194651 0.50101732 0.50925442 0.50037041 0.50816654
 0.5093278  0.50335035 0.50763433 0.19714168]
ti_coms = [0.14105877 0.15051171 0.0714409  0.0632038  0.07208781 0.06429168
 0.06313042 0.06910787 0.06482389 0.37531654]
t_total = [28.47192764 28.47192764 28.47192764 28.47192764 28.47192764 28.47192764
 28.47192764 28.47192764 28.47192764 28.47192764]
ene_coms = [0.01410588 0.01505117 0.00714409 0.00632038 0.00720878 0.00642917
 0.00631304 0.00691079 0.00648239 0.03753165]
ene_comp = [1.67768921e-05 1.63148042e-04 1.18554659e-05 4.78514173e-07
 1.78624117e-04 1.88107231e-05 9.47426593e-07 3.53327459e-05
 1.33072137e-05 6.59854465e-05]
ene_total = [0.70240791 0.75670323 0.35590993 0.31437584 0.36742188 0.32069833
 0.3140342  0.34547398 0.32307159 1.86996576]
optimize_network iter = 0 obj = 5.670062653587219
eta = 0.6399763575827285
freqs = [42686181.52856694 91787770.35134667 36171398.05232421 12340421.64654824
 89377079.2173253  41989735.69562592 15495040.98402542 51973145.71242731
 37427318.56776133 87478212.45710152]
eta_min = 0.6399763575827295	eta_max = 0.6399763575827264
af = 0.020270963537734353	bf = 2.032309190844758	zeta = 0.02229805989150779	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01410588 0.01505117 0.00714409 0.00632038 0.00720878 0.00642917
 0.00631304 0.00691079 0.00648239 0.03753165]
ene_comp = [3.58261810e-06 3.48394164e-05 2.53167313e-06 1.02184214e-07
 3.81442517e-05 4.01693214e-06 2.02318024e-07 7.54512424e-06
 2.84168632e-06 1.40908491e-05]
ene_total = [2.77911915 2.97146883 1.4076594  1.24493597 1.42741608 1.26713487
 1.24351035 1.36269351 1.27738614 7.3953291 ]
ti_comp = [0.43139945 0.42194651 0.50101732 0.50925442 0.50037041 0.50816654
 0.5093278  0.50335035 0.50763433 0.19714168]
ti_coms = [0.14105877 0.15051171 0.0714409  0.0632038  0.07208781 0.06429168
 0.06313042 0.06910787 0.06482389 0.37531654]
t_total = [28.47192764 28.47192764 28.47192764 28.47192764 28.47192764 28.47192764
 28.47192764 28.47192764 28.47192764 28.47192764]
ene_coms = [0.01410588 0.01505117 0.00714409 0.00632038 0.00720878 0.00642917
 0.00631304 0.00691079 0.00648239 0.03753165]
ene_comp = [1.67768921e-05 1.63148042e-04 1.18554659e-05 4.78514173e-07
 1.78624117e-04 1.88107231e-05 9.47426593e-07 3.53327459e-05
 1.33072137e-05 6.59854465e-05]
ene_total = [0.70240791 0.75670323 0.35590993 0.31437584 0.36742188 0.32069833
 0.3140342  0.34547398 0.32307159 1.86996576]
optimize_network iter = 1 obj = 5.670062653587185
eta = 0.6399763575827264
freqs = [42686181.52856693 91787770.35134663 36171398.05232424 12340421.64654825
 89377079.21732537 41989735.69562595 15495040.98402543 51973145.71242735
 37427318.56776136 87478212.4571007 ]
Done!
ene_coms = [0.01410588 0.01505117 0.00714409 0.00632038 0.00720878 0.00642917
 0.00631304 0.00691079 0.00648239 0.03753165]
ene_comp = [1.60710110e-05 1.56283653e-04 1.13566519e-05 4.58380879e-07
 1.71108578e-04 1.80192694e-05 9.07563995e-07 3.38461347e-05
 1.27473180e-05 6.32091350e-05]
ene_total = [0.01412195 0.01520745 0.00715545 0.00632084 0.00737989 0.00644719
 0.00631395 0.00694463 0.00649514 0.03759486]
At round 30 energy consumption: 0.11398134804017605
At round 30 eta: 0.6399763575827264
At round 30 a_n: 17.906227447174857
At round 30 local rounds: 14.614916817886705
At round 30 global rounds: 49.73625433860044
gradient difference: 0.373507559299469
train() client id: f_00000-0-0 loss: 1.183835  [   32/  126]
train() client id: f_00000-0-1 loss: 1.134857  [   64/  126]
train() client id: f_00000-0-2 loss: 1.147448  [   96/  126]
train() client id: f_00000-1-0 loss: 1.123337  [   32/  126]
train() client id: f_00000-1-1 loss: 0.950424  [   64/  126]
train() client id: f_00000-1-2 loss: 1.088331  [   96/  126]
train() client id: f_00000-2-0 loss: 0.950085  [   32/  126]
train() client id: f_00000-2-1 loss: 0.906983  [   64/  126]
train() client id: f_00000-2-2 loss: 1.033675  [   96/  126]
train() client id: f_00000-3-0 loss: 1.019892  [   32/  126]
train() client id: f_00000-3-1 loss: 0.979011  [   64/  126]
train() client id: f_00000-3-2 loss: 0.868865  [   96/  126]
train() client id: f_00000-4-0 loss: 0.895256  [   32/  126]
train() client id: f_00000-4-1 loss: 0.995005  [   64/  126]
train() client id: f_00000-4-2 loss: 0.831693  [   96/  126]
train() client id: f_00000-5-0 loss: 0.824132  [   32/  126]
train() client id: f_00000-5-1 loss: 0.866994  [   64/  126]
train() client id: f_00000-5-2 loss: 0.837753  [   96/  126]
train() client id: f_00000-6-0 loss: 0.898170  [   32/  126]
train() client id: f_00000-6-1 loss: 0.896679  [   64/  126]
train() client id: f_00000-6-2 loss: 0.772320  [   96/  126]
train() client id: f_00000-7-0 loss: 0.784805  [   32/  126]
train() client id: f_00000-7-1 loss: 0.865704  [   64/  126]
train() client id: f_00000-7-2 loss: 0.782767  [   96/  126]
train() client id: f_00000-8-0 loss: 0.875545  [   32/  126]
train() client id: f_00000-8-1 loss: 0.823665  [   64/  126]
train() client id: f_00000-8-2 loss: 0.765909  [   96/  126]
train() client id: f_00000-9-0 loss: 0.831411  [   32/  126]
train() client id: f_00000-9-1 loss: 0.855308  [   64/  126]
train() client id: f_00000-9-2 loss: 0.763870  [   96/  126]
train() client id: f_00000-10-0 loss: 0.919293  [   32/  126]
train() client id: f_00000-10-1 loss: 0.750184  [   64/  126]
train() client id: f_00000-10-2 loss: 0.827747  [   96/  126]
train() client id: f_00000-11-0 loss: 0.820885  [   32/  126]
train() client id: f_00000-11-1 loss: 0.815273  [   64/  126]
train() client id: f_00000-11-2 loss: 0.730993  [   96/  126]
train() client id: f_00000-12-0 loss: 0.935009  [   32/  126]
train() client id: f_00000-12-1 loss: 0.843535  [   64/  126]
train() client id: f_00000-12-2 loss: 0.696684  [   96/  126]
train() client id: f_00000-13-0 loss: 0.719301  [   32/  126]
train() client id: f_00000-13-1 loss: 0.912551  [   64/  126]
train() client id: f_00000-13-2 loss: 0.890306  [   96/  126]
train() client id: f_00001-0-0 loss: 0.511535  [   32/  265]
train() client id: f_00001-0-1 loss: 0.382359  [   64/  265]
train() client id: f_00001-0-2 loss: 0.461248  [   96/  265]
train() client id: f_00001-0-3 loss: 0.474147  [  128/  265]
train() client id: f_00001-0-4 loss: 0.327842  [  160/  265]
train() client id: f_00001-0-5 loss: 0.338406  [  192/  265]
train() client id: f_00001-0-6 loss: 0.389061  [  224/  265]
train() client id: f_00001-0-7 loss: 0.502887  [  256/  265]
train() client id: f_00001-1-0 loss: 0.437769  [   32/  265]
train() client id: f_00001-1-1 loss: 0.470970  [   64/  265]
train() client id: f_00001-1-2 loss: 0.350564  [   96/  265]
train() client id: f_00001-1-3 loss: 0.406071  [  128/  265]
train() client id: f_00001-1-4 loss: 0.400479  [  160/  265]
train() client id: f_00001-1-5 loss: 0.440729  [  192/  265]
train() client id: f_00001-1-6 loss: 0.356210  [  224/  265]
train() client id: f_00001-1-7 loss: 0.437583  [  256/  265]
train() client id: f_00001-2-0 loss: 0.375051  [   32/  265]
train() client id: f_00001-2-1 loss: 0.354208  [   64/  265]
train() client id: f_00001-2-2 loss: 0.356168  [   96/  265]
train() client id: f_00001-2-3 loss: 0.416163  [  128/  265]
train() client id: f_00001-2-4 loss: 0.352861  [  160/  265]
train() client id: f_00001-2-5 loss: 0.489857  [  192/  265]
train() client id: f_00001-2-6 loss: 0.486541  [  224/  265]
train() client id: f_00001-2-7 loss: 0.400584  [  256/  265]
train() client id: f_00001-3-0 loss: 0.351053  [   32/  265]
train() client id: f_00001-3-1 loss: 0.387180  [   64/  265]
train() client id: f_00001-3-2 loss: 0.490425  [   96/  265]
train() client id: f_00001-3-3 loss: 0.348594  [  128/  265]
train() client id: f_00001-3-4 loss: 0.422197  [  160/  265]
train() client id: f_00001-3-5 loss: 0.366335  [  192/  265]
train() client id: f_00001-3-6 loss: 0.287172  [  224/  265]
train() client id: f_00001-3-7 loss: 0.439659  [  256/  265]
train() client id: f_00001-4-0 loss: 0.321345  [   32/  265]
train() client id: f_00001-4-1 loss: 0.503556  [   64/  265]
train() client id: f_00001-4-2 loss: 0.465750  [   96/  265]
train() client id: f_00001-4-3 loss: 0.345787  [  128/  265]
train() client id: f_00001-4-4 loss: 0.392904  [  160/  265]
train() client id: f_00001-4-5 loss: 0.296207  [  192/  265]
train() client id: f_00001-4-6 loss: 0.341065  [  224/  265]
train() client id: f_00001-4-7 loss: 0.487269  [  256/  265]
train() client id: f_00001-5-0 loss: 0.477855  [   32/  265]
train() client id: f_00001-5-1 loss: 0.420959  [   64/  265]
train() client id: f_00001-5-2 loss: 0.391306  [   96/  265]
train() client id: f_00001-5-3 loss: 0.388226  [  128/  265]
train() client id: f_00001-5-4 loss: 0.324105  [  160/  265]
train() client id: f_00001-5-5 loss: 0.437666  [  192/  265]
train() client id: f_00001-5-6 loss: 0.311666  [  224/  265]
train() client id: f_00001-5-7 loss: 0.358580  [  256/  265]
train() client id: f_00001-6-0 loss: 0.493628  [   32/  265]
train() client id: f_00001-6-1 loss: 0.447049  [   64/  265]
train() client id: f_00001-6-2 loss: 0.387399  [   96/  265]
train() client id: f_00001-6-3 loss: 0.307091  [  128/  265]
train() client id: f_00001-6-4 loss: 0.279638  [  160/  265]
train() client id: f_00001-6-5 loss: 0.392411  [  192/  265]
train() client id: f_00001-6-6 loss: 0.486520  [  224/  265]
train() client id: f_00001-6-7 loss: 0.294937  [  256/  265]
train() client id: f_00001-7-0 loss: 0.382112  [   32/  265]
train() client id: f_00001-7-1 loss: 0.464468  [   64/  265]
train() client id: f_00001-7-2 loss: 0.479108  [   96/  265]
train() client id: f_00001-7-3 loss: 0.312560  [  128/  265]
train() client id: f_00001-7-4 loss: 0.347541  [  160/  265]
train() client id: f_00001-7-5 loss: 0.333730  [  192/  265]
train() client id: f_00001-7-6 loss: 0.287352  [  224/  265]
train() client id: f_00001-7-7 loss: 0.463185  [  256/  265]
train() client id: f_00001-8-0 loss: 0.283060  [   32/  265]
train() client id: f_00001-8-1 loss: 0.391434  [   64/  265]
train() client id: f_00001-8-2 loss: 0.361680  [   96/  265]
train() client id: f_00001-8-3 loss: 0.483549  [  128/  265]
train() client id: f_00001-8-4 loss: 0.347611  [  160/  265]
train() client id: f_00001-8-5 loss: 0.338912  [  192/  265]
train() client id: f_00001-8-6 loss: 0.424092  [  224/  265]
train() client id: f_00001-8-7 loss: 0.371307  [  256/  265]
train() client id: f_00001-9-0 loss: 0.285229  [   32/  265]
train() client id: f_00001-9-1 loss: 0.281611  [   64/  265]
train() client id: f_00001-9-2 loss: 0.482251  [   96/  265]
train() client id: f_00001-9-3 loss: 0.571743  [  128/  265]
train() client id: f_00001-9-4 loss: 0.325626  [  160/  265]
train() client id: f_00001-9-5 loss: 0.406852  [  192/  265]
train() client id: f_00001-9-6 loss: 0.275456  [  224/  265]
train() client id: f_00001-9-7 loss: 0.396828  [  256/  265]
train() client id: f_00001-10-0 loss: 0.367705  [   32/  265]
train() client id: f_00001-10-1 loss: 0.423995  [   64/  265]
train() client id: f_00001-10-2 loss: 0.364137  [   96/  265]
train() client id: f_00001-10-3 loss: 0.563876  [  128/  265]
train() client id: f_00001-10-4 loss: 0.419959  [  160/  265]
train() client id: f_00001-10-5 loss: 0.278803  [  192/  265]
train() client id: f_00001-10-6 loss: 0.277044  [  224/  265]
train() client id: f_00001-10-7 loss: 0.337033  [  256/  265]
train() client id: f_00001-11-0 loss: 0.330597  [   32/  265]
train() client id: f_00001-11-1 loss: 0.343246  [   64/  265]
train() client id: f_00001-11-2 loss: 0.297196  [   96/  265]
train() client id: f_00001-11-3 loss: 0.355393  [  128/  265]
train() client id: f_00001-11-4 loss: 0.362633  [  160/  265]
train() client id: f_00001-11-5 loss: 0.387230  [  192/  265]
train() client id: f_00001-11-6 loss: 0.442272  [  224/  265]
train() client id: f_00001-11-7 loss: 0.416326  [  256/  265]
train() client id: f_00001-12-0 loss: 0.316918  [   32/  265]
train() client id: f_00001-12-1 loss: 0.317535  [   64/  265]
train() client id: f_00001-12-2 loss: 0.302202  [   96/  265]
train() client id: f_00001-12-3 loss: 0.559202  [  128/  265]
train() client id: f_00001-12-4 loss: 0.280632  [  160/  265]
train() client id: f_00001-12-5 loss: 0.267674  [  192/  265]
train() client id: f_00001-12-6 loss: 0.553767  [  224/  265]
train() client id: f_00001-12-7 loss: 0.415398  [  256/  265]
train() client id: f_00001-13-0 loss: 0.271649  [   32/  265]
train() client id: f_00001-13-1 loss: 0.415983  [   64/  265]
train() client id: f_00001-13-2 loss: 0.351505  [   96/  265]
train() client id: f_00001-13-3 loss: 0.396352  [  128/  265]
train() client id: f_00001-13-4 loss: 0.500883  [  160/  265]
train() client id: f_00001-13-5 loss: 0.341815  [  192/  265]
train() client id: f_00001-13-6 loss: 0.319679  [  224/  265]
train() client id: f_00001-13-7 loss: 0.321862  [  256/  265]
train() client id: f_00002-0-0 loss: 1.028216  [   32/  124]
train() client id: f_00002-0-1 loss: 1.219355  [   64/  124]
train() client id: f_00002-0-2 loss: 1.177952  [   96/  124]
train() client id: f_00002-1-0 loss: 1.182935  [   32/  124]
train() client id: f_00002-1-1 loss: 0.975156  [   64/  124]
train() client id: f_00002-1-2 loss: 0.994926  [   96/  124]
train() client id: f_00002-2-0 loss: 1.016200  [   32/  124]
train() client id: f_00002-2-1 loss: 1.045825  [   64/  124]
train() client id: f_00002-2-2 loss: 1.227761  [   96/  124]
train() client id: f_00002-3-0 loss: 0.995541  [   32/  124]
train() client id: f_00002-3-1 loss: 0.871117  [   64/  124]
train() client id: f_00002-3-2 loss: 1.002777  [   96/  124]
train() client id: f_00002-4-0 loss: 0.975095  [   32/  124]
train() client id: f_00002-4-1 loss: 0.964752  [   64/  124]
train() client id: f_00002-4-2 loss: 0.977313  [   96/  124]
train() client id: f_00002-5-0 loss: 0.932190  [   32/  124]
train() client id: f_00002-5-1 loss: 1.015353  [   64/  124]
train() client id: f_00002-5-2 loss: 0.976151  [   96/  124]
train() client id: f_00002-6-0 loss: 0.923947  [   32/  124]
train() client id: f_00002-6-1 loss: 0.872611  [   64/  124]
train() client id: f_00002-6-2 loss: 0.995644  [   96/  124]
train() client id: f_00002-7-0 loss: 0.851963  [   32/  124]
train() client id: f_00002-7-1 loss: 0.877842  [   64/  124]
train() client id: f_00002-7-2 loss: 0.869921  [   96/  124]
train() client id: f_00002-8-0 loss: 0.812404  [   32/  124]
train() client id: f_00002-8-1 loss: 0.982890  [   64/  124]
train() client id: f_00002-8-2 loss: 1.032954  [   96/  124]
train() client id: f_00002-9-0 loss: 0.930408  [   32/  124]
train() client id: f_00002-9-1 loss: 0.867074  [   64/  124]
train() client id: f_00002-9-2 loss: 0.805615  [   96/  124]
train() client id: f_00002-10-0 loss: 0.979627  [   32/  124]
train() client id: f_00002-10-1 loss: 0.876543  [   64/  124]
train() client id: f_00002-10-2 loss: 0.834609  [   96/  124]
train() client id: f_00002-11-0 loss: 1.064183  [   32/  124]
train() client id: f_00002-11-1 loss: 0.738973  [   64/  124]
train() client id: f_00002-11-2 loss: 0.743351  [   96/  124]
train() client id: f_00002-12-0 loss: 0.945476  [   32/  124]
train() client id: f_00002-12-1 loss: 0.660611  [   64/  124]
train() client id: f_00002-12-2 loss: 1.029086  [   96/  124]
train() client id: f_00002-13-0 loss: 0.832156  [   32/  124]
train() client id: f_00002-13-1 loss: 0.780359  [   64/  124]
train() client id: f_00002-13-2 loss: 0.827090  [   96/  124]
train() client id: f_00003-0-0 loss: 0.738785  [   32/   43]
train() client id: f_00003-1-0 loss: 0.681691  [   32/   43]
train() client id: f_00003-2-0 loss: 0.657801  [   32/   43]
train() client id: f_00003-3-0 loss: 0.646546  [   32/   43]
train() client id: f_00003-4-0 loss: 0.710730  [   32/   43]
train() client id: f_00003-5-0 loss: 0.695095  [   32/   43]
train() client id: f_00003-6-0 loss: 0.654762  [   32/   43]
train() client id: f_00003-7-0 loss: 0.671104  [   32/   43]
train() client id: f_00003-8-0 loss: 0.572318  [   32/   43]
train() client id: f_00003-9-0 loss: 0.597564  [   32/   43]
train() client id: f_00003-10-0 loss: 0.619267  [   32/   43]
train() client id: f_00003-11-0 loss: 0.733792  [   32/   43]
train() client id: f_00003-12-0 loss: 0.754605  [   32/   43]
train() client id: f_00003-13-0 loss: 0.732738  [   32/   43]
train() client id: f_00004-0-0 loss: 0.858791  [   32/  306]
train() client id: f_00004-0-1 loss: 0.918324  [   64/  306]
train() client id: f_00004-0-2 loss: 0.887667  [   96/  306]
train() client id: f_00004-0-3 loss: 0.838215  [  128/  306]
train() client id: f_00004-0-4 loss: 0.848629  [  160/  306]
train() client id: f_00004-0-5 loss: 0.827693  [  192/  306]
train() client id: f_00004-0-6 loss: 0.806304  [  224/  306]
train() client id: f_00004-0-7 loss: 0.922515  [  256/  306]
train() client id: f_00004-0-8 loss: 0.783157  [  288/  306]
train() client id: f_00004-1-0 loss: 0.983247  [   32/  306]
train() client id: f_00004-1-1 loss: 0.910976  [   64/  306]
train() client id: f_00004-1-2 loss: 0.845022  [   96/  306]
train() client id: f_00004-1-3 loss: 0.903244  [  128/  306]
train() client id: f_00004-1-4 loss: 0.815080  [  160/  306]
train() client id: f_00004-1-5 loss: 0.781875  [  192/  306]
train() client id: f_00004-1-6 loss: 0.796377  [  224/  306]
train() client id: f_00004-1-7 loss: 0.803210  [  256/  306]
train() client id: f_00004-1-8 loss: 0.829834  [  288/  306]
train() client id: f_00004-2-0 loss: 0.795879  [   32/  306]
train() client id: f_00004-2-1 loss: 0.857721  [   64/  306]
train() client id: f_00004-2-2 loss: 0.826625  [   96/  306]
train() client id: f_00004-2-3 loss: 0.706738  [  128/  306]
train() client id: f_00004-2-4 loss: 0.852887  [  160/  306]
train() client id: f_00004-2-5 loss: 0.715509  [  192/  306]
train() client id: f_00004-2-6 loss: 1.004282  [  224/  306]
train() client id: f_00004-2-7 loss: 0.852680  [  256/  306]
train() client id: f_00004-2-8 loss: 0.957632  [  288/  306]
train() client id: f_00004-3-0 loss: 0.725375  [   32/  306]
train() client id: f_00004-3-1 loss: 0.957644  [   64/  306]
train() client id: f_00004-3-2 loss: 0.789728  [   96/  306]
train() client id: f_00004-3-3 loss: 0.816871  [  128/  306]
train() client id: f_00004-3-4 loss: 0.767660  [  160/  306]
train() client id: f_00004-3-5 loss: 0.744661  [  192/  306]
train() client id: f_00004-3-6 loss: 0.956729  [  224/  306]
train() client id: f_00004-3-7 loss: 0.874710  [  256/  306]
train() client id: f_00004-3-8 loss: 0.990760  [  288/  306]
train() client id: f_00004-4-0 loss: 0.875818  [   32/  306]
train() client id: f_00004-4-1 loss: 0.830439  [   64/  306]
train() client id: f_00004-4-2 loss: 0.826471  [   96/  306]
train() client id: f_00004-4-3 loss: 0.998373  [  128/  306]
train() client id: f_00004-4-4 loss: 0.865473  [  160/  306]
train() client id: f_00004-4-5 loss: 0.864512  [  192/  306]
train() client id: f_00004-4-6 loss: 0.706538  [  224/  306]
train() client id: f_00004-4-7 loss: 0.798830  [  256/  306]
train() client id: f_00004-4-8 loss: 0.828421  [  288/  306]
train() client id: f_00004-5-0 loss: 0.934035  [   32/  306]
train() client id: f_00004-5-1 loss: 0.863766  [   64/  306]
train() client id: f_00004-5-2 loss: 0.878803  [   96/  306]
train() client id: f_00004-5-3 loss: 0.799547  [  128/  306]
train() client id: f_00004-5-4 loss: 0.837354  [  160/  306]
train() client id: f_00004-5-5 loss: 0.806851  [  192/  306]
train() client id: f_00004-5-6 loss: 0.888841  [  224/  306]
train() client id: f_00004-5-7 loss: 0.845467  [  256/  306]
train() client id: f_00004-5-8 loss: 0.875161  [  288/  306]
train() client id: f_00004-6-0 loss: 0.861320  [   32/  306]
train() client id: f_00004-6-1 loss: 0.904530  [   64/  306]
train() client id: f_00004-6-2 loss: 0.709169  [   96/  306]
train() client id: f_00004-6-3 loss: 0.935730  [  128/  306]
train() client id: f_00004-6-4 loss: 0.819498  [  160/  306]
train() client id: f_00004-6-5 loss: 0.875221  [  192/  306]
train() client id: f_00004-6-6 loss: 0.847987  [  224/  306]
train() client id: f_00004-6-7 loss: 0.896399  [  256/  306]
train() client id: f_00004-6-8 loss: 0.790740  [  288/  306]
train() client id: f_00004-7-0 loss: 0.675000  [   32/  306]
train() client id: f_00004-7-1 loss: 0.676387  [   64/  306]
train() client id: f_00004-7-2 loss: 0.970312  [   96/  306]
train() client id: f_00004-7-3 loss: 0.881936  [  128/  306]
train() client id: f_00004-7-4 loss: 0.808164  [  160/  306]
train() client id: f_00004-7-5 loss: 1.019165  [  192/  306]
train() client id: f_00004-7-6 loss: 0.868165  [  224/  306]
train() client id: f_00004-7-7 loss: 0.739884  [  256/  306]
train() client id: f_00004-7-8 loss: 0.881827  [  288/  306]
train() client id: f_00004-8-0 loss: 0.809271  [   32/  306]
train() client id: f_00004-8-1 loss: 0.763909  [   64/  306]
train() client id: f_00004-8-2 loss: 0.937429  [   96/  306]
train() client id: f_00004-8-3 loss: 0.868343  [  128/  306]
train() client id: f_00004-8-4 loss: 0.808730  [  160/  306]
train() client id: f_00004-8-5 loss: 0.873049  [  192/  306]
train() client id: f_00004-8-6 loss: 0.890405  [  224/  306]
train() client id: f_00004-8-7 loss: 0.786554  [  256/  306]
train() client id: f_00004-8-8 loss: 0.836308  [  288/  306]
train() client id: f_00004-9-0 loss: 0.734318  [   32/  306]
train() client id: f_00004-9-1 loss: 0.770975  [   64/  306]
train() client id: f_00004-9-2 loss: 0.876661  [   96/  306]
train() client id: f_00004-9-3 loss: 0.895105  [  128/  306]
train() client id: f_00004-9-4 loss: 0.910511  [  160/  306]
train() client id: f_00004-9-5 loss: 0.872848  [  192/  306]
train() client id: f_00004-9-6 loss: 0.873593  [  224/  306]
train() client id: f_00004-9-7 loss: 0.914226  [  256/  306]
train() client id: f_00004-9-8 loss: 0.839939  [  288/  306]
train() client id: f_00004-10-0 loss: 1.019862  [   32/  306]
train() client id: f_00004-10-1 loss: 0.759242  [   64/  306]
train() client id: f_00004-10-2 loss: 0.952219  [   96/  306]
train() client id: f_00004-10-3 loss: 0.814877  [  128/  306]
train() client id: f_00004-10-4 loss: 0.831312  [  160/  306]
train() client id: f_00004-10-5 loss: 0.845925  [  192/  306]
train() client id: f_00004-10-6 loss: 0.779075  [  224/  306]
train() client id: f_00004-10-7 loss: 0.822216  [  256/  306]
train() client id: f_00004-10-8 loss: 0.764043  [  288/  306]
train() client id: f_00004-11-0 loss: 0.757111  [   32/  306]
train() client id: f_00004-11-1 loss: 0.812984  [   64/  306]
train() client id: f_00004-11-2 loss: 0.948396  [   96/  306]
train() client id: f_00004-11-3 loss: 0.944827  [  128/  306]
train() client id: f_00004-11-4 loss: 0.947740  [  160/  306]
train() client id: f_00004-11-5 loss: 0.767028  [  192/  306]
train() client id: f_00004-11-6 loss: 0.791113  [  224/  306]
train() client id: f_00004-11-7 loss: 0.753264  [  256/  306]
train() client id: f_00004-11-8 loss: 0.885462  [  288/  306]
train() client id: f_00004-12-0 loss: 0.833928  [   32/  306]
train() client id: f_00004-12-1 loss: 0.788382  [   64/  306]
train() client id: f_00004-12-2 loss: 0.884418  [   96/  306]
train() client id: f_00004-12-3 loss: 0.880862  [  128/  306]
train() client id: f_00004-12-4 loss: 0.838647  [  160/  306]
train() client id: f_00004-12-5 loss: 0.820724  [  192/  306]
train() client id: f_00004-12-6 loss: 0.884461  [  224/  306]
train() client id: f_00004-12-7 loss: 0.876581  [  256/  306]
train() client id: f_00004-12-8 loss: 0.934311  [  288/  306]
train() client id: f_00004-13-0 loss: 1.011088  [   32/  306]
train() client id: f_00004-13-1 loss: 0.851833  [   64/  306]
train() client id: f_00004-13-2 loss: 0.783105  [   96/  306]
train() client id: f_00004-13-3 loss: 0.796088  [  128/  306]
train() client id: f_00004-13-4 loss: 0.801001  [  160/  306]
train() client id: f_00004-13-5 loss: 0.839649  [  192/  306]
train() client id: f_00004-13-6 loss: 0.864840  [  224/  306]
train() client id: f_00004-13-7 loss: 0.792453  [  256/  306]
train() client id: f_00004-13-8 loss: 0.907009  [  288/  306]
train() client id: f_00005-0-0 loss: 0.845516  [   32/  146]
train() client id: f_00005-0-1 loss: 0.390110  [   64/  146]
train() client id: f_00005-0-2 loss: 0.559446  [   96/  146]
train() client id: f_00005-0-3 loss: 0.574236  [  128/  146]
train() client id: f_00005-1-0 loss: 0.761721  [   32/  146]
train() client id: f_00005-1-1 loss: 0.582277  [   64/  146]
train() client id: f_00005-1-2 loss: 0.631447  [   96/  146]
train() client id: f_00005-1-3 loss: 0.536813  [  128/  146]
train() client id: f_00005-2-0 loss: 0.710955  [   32/  146]
train() client id: f_00005-2-1 loss: 0.700229  [   64/  146]
train() client id: f_00005-2-2 loss: 0.734643  [   96/  146]
train() client id: f_00005-2-3 loss: 0.516921  [  128/  146]
train() client id: f_00005-3-0 loss: 0.544770  [   32/  146]
train() client id: f_00005-3-1 loss: 0.761503  [   64/  146]
train() client id: f_00005-3-2 loss: 0.433657  [   96/  146]
train() client id: f_00005-3-3 loss: 0.619376  [  128/  146]
train() client id: f_00005-4-0 loss: 0.603195  [   32/  146]
train() client id: f_00005-4-1 loss: 0.796172  [   64/  146]
train() client id: f_00005-4-2 loss: 0.625660  [   96/  146]
train() client id: f_00005-4-3 loss: 0.650754  [  128/  146]
train() client id: f_00005-5-0 loss: 0.719536  [   32/  146]
train() client id: f_00005-5-1 loss: 0.463076  [   64/  146]
train() client id: f_00005-5-2 loss: 0.527290  [   96/  146]
train() client id: f_00005-5-3 loss: 0.694582  [  128/  146]
train() client id: f_00005-6-0 loss: 0.545249  [   32/  146]
train() client id: f_00005-6-1 loss: 0.673389  [   64/  146]
train() client id: f_00005-6-2 loss: 0.652720  [   96/  146]
train() client id: f_00005-6-3 loss: 0.596547  [  128/  146]
train() client id: f_00005-7-0 loss: 0.388431  [   32/  146]
train() client id: f_00005-7-1 loss: 0.700078  [   64/  146]
train() client id: f_00005-7-2 loss: 0.677623  [   96/  146]
train() client id: f_00005-7-3 loss: 0.471220  [  128/  146]
train() client id: f_00005-8-0 loss: 0.521976  [   32/  146]
train() client id: f_00005-8-1 loss: 0.724569  [   64/  146]
train() client id: f_00005-8-2 loss: 0.700628  [   96/  146]
train() client id: f_00005-8-3 loss: 0.400244  [  128/  146]
train() client id: f_00005-9-0 loss: 0.710956  [   32/  146]
train() client id: f_00005-9-1 loss: 0.850199  [   64/  146]
train() client id: f_00005-9-2 loss: 0.607003  [   96/  146]
train() client id: f_00005-9-3 loss: 0.392104  [  128/  146]
train() client id: f_00005-10-0 loss: 0.596635  [   32/  146]
train() client id: f_00005-10-1 loss: 0.681182  [   64/  146]
train() client id: f_00005-10-2 loss: 0.731753  [   96/  146]
train() client id: f_00005-10-3 loss: 0.448743  [  128/  146]
train() client id: f_00005-11-0 loss: 0.540212  [   32/  146]
train() client id: f_00005-11-1 loss: 0.791415  [   64/  146]
train() client id: f_00005-11-2 loss: 0.698303  [   96/  146]
train() client id: f_00005-11-3 loss: 0.438435  [  128/  146]
train() client id: f_00005-12-0 loss: 0.621337  [   32/  146]
train() client id: f_00005-12-1 loss: 0.532560  [   64/  146]
train() client id: f_00005-12-2 loss: 0.646401  [   96/  146]
train() client id: f_00005-12-3 loss: 0.731457  [  128/  146]
train() client id: f_00005-13-0 loss: 0.855746  [   32/  146]
train() client id: f_00005-13-1 loss: 0.522608  [   64/  146]
train() client id: f_00005-13-2 loss: 0.518594  [   96/  146]
train() client id: f_00005-13-3 loss: 0.733647  [  128/  146]
train() client id: f_00006-0-0 loss: 0.480958  [   32/   54]
train() client id: f_00006-1-0 loss: 0.417658  [   32/   54]
train() client id: f_00006-2-0 loss: 0.480417  [   32/   54]
train() client id: f_00006-3-0 loss: 0.473113  [   32/   54]
train() client id: f_00006-4-0 loss: 0.522328  [   32/   54]
train() client id: f_00006-5-0 loss: 0.451368  [   32/   54]
train() client id: f_00006-6-0 loss: 0.475217  [   32/   54]
train() client id: f_00006-7-0 loss: 0.446450  [   32/   54]
train() client id: f_00006-8-0 loss: 0.481520  [   32/   54]
train() client id: f_00006-9-0 loss: 0.518885  [   32/   54]
train() client id: f_00006-10-0 loss: 0.515944  [   32/   54]
train() client id: f_00006-11-0 loss: 0.442844  [   32/   54]
train() client id: f_00006-12-0 loss: 0.465122  [   32/   54]
train() client id: f_00006-13-0 loss: 0.509868  [   32/   54]
train() client id: f_00007-0-0 loss: 0.505109  [   32/  179]
train() client id: f_00007-0-1 loss: 0.450750  [   64/  179]
train() client id: f_00007-0-2 loss: 0.655083  [   96/  179]
train() client id: f_00007-0-3 loss: 0.597345  [  128/  179]
train() client id: f_00007-0-4 loss: 0.605936  [  160/  179]
train() client id: f_00007-1-0 loss: 0.681817  [   32/  179]
train() client id: f_00007-1-1 loss: 0.531243  [   64/  179]
train() client id: f_00007-1-2 loss: 0.701316  [   96/  179]
train() client id: f_00007-1-3 loss: 0.525679  [  128/  179]
train() client id: f_00007-1-4 loss: 0.481623  [  160/  179]
train() client id: f_00007-2-0 loss: 0.648601  [   32/  179]
train() client id: f_00007-2-1 loss: 0.718653  [   64/  179]
train() client id: f_00007-2-2 loss: 0.585362  [   96/  179]
train() client id: f_00007-2-3 loss: 0.413688  [  128/  179]
train() client id: f_00007-2-4 loss: 0.393742  [  160/  179]
train() client id: f_00007-3-0 loss: 0.503615  [   32/  179]
train() client id: f_00007-3-1 loss: 0.622889  [   64/  179]
train() client id: f_00007-3-2 loss: 0.520906  [   96/  179]
train() client id: f_00007-3-3 loss: 0.459382  [  128/  179]
train() client id: f_00007-3-4 loss: 0.685797  [  160/  179]
train() client id: f_00007-4-0 loss: 0.599019  [   32/  179]
train() client id: f_00007-4-1 loss: 0.439727  [   64/  179]
train() client id: f_00007-4-2 loss: 0.742000  [   96/  179]
train() client id: f_00007-4-3 loss: 0.502216  [  128/  179]
train() client id: f_00007-4-4 loss: 0.458731  [  160/  179]
train() client id: f_00007-5-0 loss: 0.453484  [   32/  179]
train() client id: f_00007-5-1 loss: 0.491467  [   64/  179]
train() client id: f_00007-5-2 loss: 0.603916  [   96/  179]
train() client id: f_00007-5-3 loss: 0.481276  [  128/  179]
train() client id: f_00007-5-4 loss: 0.598306  [  160/  179]
train() client id: f_00007-6-0 loss: 0.664204  [   32/  179]
train() client id: f_00007-6-1 loss: 0.423492  [   64/  179]
train() client id: f_00007-6-2 loss: 0.474031  [   96/  179]
train() client id: f_00007-6-3 loss: 0.493753  [  128/  179]
train() client id: f_00007-6-4 loss: 0.543878  [  160/  179]
train() client id: f_00007-7-0 loss: 0.571459  [   32/  179]
train() client id: f_00007-7-1 loss: 0.444242  [   64/  179]
train() client id: f_00007-7-2 loss: 0.372106  [   96/  179]
train() client id: f_00007-7-3 loss: 0.630489  [  128/  179]
train() client id: f_00007-7-4 loss: 0.530592  [  160/  179]
train() client id: f_00007-8-0 loss: 0.408928  [   32/  179]
train() client id: f_00007-8-1 loss: 0.677942  [   64/  179]
train() client id: f_00007-8-2 loss: 0.470423  [   96/  179]
train() client id: f_00007-8-3 loss: 0.458219  [  128/  179]
train() client id: f_00007-8-4 loss: 0.605830  [  160/  179]
train() client id: f_00007-9-0 loss: 0.479792  [   32/  179]
train() client id: f_00007-9-1 loss: 0.548297  [   64/  179]
train() client id: f_00007-9-2 loss: 0.545628  [   96/  179]
train() client id: f_00007-9-3 loss: 0.443448  [  128/  179]
train() client id: f_00007-9-4 loss: 0.591609  [  160/  179]
train() client id: f_00007-10-0 loss: 0.778010  [   32/  179]
train() client id: f_00007-10-1 loss: 0.369708  [   64/  179]
train() client id: f_00007-10-2 loss: 0.417501  [   96/  179]
train() client id: f_00007-10-3 loss: 0.469750  [  128/  179]
train() client id: f_00007-10-4 loss: 0.457932  [  160/  179]
train() client id: f_00007-11-0 loss: 0.536098  [   32/  179]
train() client id: f_00007-11-1 loss: 0.349210  [   64/  179]
train() client id: f_00007-11-2 loss: 0.627345  [   96/  179]
train() client id: f_00007-11-3 loss: 0.470290  [  128/  179]
train() client id: f_00007-11-4 loss: 0.523771  [  160/  179]
train() client id: f_00007-12-0 loss: 0.542076  [   32/  179]
train() client id: f_00007-12-1 loss: 0.362026  [   64/  179]
train() client id: f_00007-12-2 loss: 0.628937  [   96/  179]
train() client id: f_00007-12-3 loss: 0.391570  [  128/  179]
train() client id: f_00007-12-4 loss: 0.523828  [  160/  179]
train() client id: f_00007-13-0 loss: 0.370075  [   32/  179]
train() client id: f_00007-13-1 loss: 0.761067  [   64/  179]
train() client id: f_00007-13-2 loss: 0.409595  [   96/  179]
train() client id: f_00007-13-3 loss: 0.555846  [  128/  179]
train() client id: f_00007-13-4 loss: 0.461236  [  160/  179]
train() client id: f_00008-0-0 loss: 0.685812  [   32/  130]
train() client id: f_00008-0-1 loss: 0.777608  [   64/  130]
train() client id: f_00008-0-2 loss: 0.678177  [   96/  130]
train() client id: f_00008-0-3 loss: 0.689495  [  128/  130]
train() client id: f_00008-1-0 loss: 0.688787  [   32/  130]
train() client id: f_00008-1-1 loss: 0.829190  [   64/  130]
train() client id: f_00008-1-2 loss: 0.677299  [   96/  130]
train() client id: f_00008-1-3 loss: 0.671879  [  128/  130]
train() client id: f_00008-2-0 loss: 0.708261  [   32/  130]
train() client id: f_00008-2-1 loss: 0.661896  [   64/  130]
train() client id: f_00008-2-2 loss: 0.744034  [   96/  130]
train() client id: f_00008-2-3 loss: 0.717507  [  128/  130]
train() client id: f_00008-3-0 loss: 0.770206  [   32/  130]
train() client id: f_00008-3-1 loss: 0.790713  [   64/  130]
train() client id: f_00008-3-2 loss: 0.658164  [   96/  130]
train() client id: f_00008-3-3 loss: 0.588765  [  128/  130]
train() client id: f_00008-4-0 loss: 0.635046  [   32/  130]
train() client id: f_00008-4-1 loss: 0.732898  [   64/  130]
train() client id: f_00008-4-2 loss: 0.768118  [   96/  130]
train() client id: f_00008-4-3 loss: 0.732129  [  128/  130]
train() client id: f_00008-5-0 loss: 0.674790  [   32/  130]
train() client id: f_00008-5-1 loss: 0.778198  [   64/  130]
train() client id: f_00008-5-2 loss: 0.702783  [   96/  130]
train() client id: f_00008-5-3 loss: 0.709559  [  128/  130]
train() client id: f_00008-6-0 loss: 0.830263  [   32/  130]
train() client id: f_00008-6-1 loss: 0.597708  [   64/  130]
train() client id: f_00008-6-2 loss: 0.750531  [   96/  130]
train() client id: f_00008-6-3 loss: 0.698426  [  128/  130]
train() client id: f_00008-7-0 loss: 0.796661  [   32/  130]
train() client id: f_00008-7-1 loss: 0.735129  [   64/  130]
train() client id: f_00008-7-2 loss: 0.709282  [   96/  130]
train() client id: f_00008-7-3 loss: 0.584644  [  128/  130]
train() client id: f_00008-8-0 loss: 0.671500  [   32/  130]
train() client id: f_00008-8-1 loss: 0.572488  [   64/  130]
train() client id: f_00008-8-2 loss: 0.700137  [   96/  130]
train() client id: f_00008-8-3 loss: 0.888683  [  128/  130]
train() client id: f_00008-9-0 loss: 0.581481  [   32/  130]
train() client id: f_00008-9-1 loss: 0.757853  [   64/  130]
train() client id: f_00008-9-2 loss: 0.757214  [   96/  130]
train() client id: f_00008-9-3 loss: 0.759739  [  128/  130]
train() client id: f_00008-10-0 loss: 0.760158  [   32/  130]
train() client id: f_00008-10-1 loss: 0.693629  [   64/  130]
train() client id: f_00008-10-2 loss: 0.766198  [   96/  130]
train() client id: f_00008-10-3 loss: 0.646750  [  128/  130]
train() client id: f_00008-11-0 loss: 0.668295  [   32/  130]
train() client id: f_00008-11-1 loss: 0.822611  [   64/  130]
train() client id: f_00008-11-2 loss: 0.769003  [   96/  130]
train() client id: f_00008-11-3 loss: 0.609312  [  128/  130]
train() client id: f_00008-12-0 loss: 0.774473  [   32/  130]
train() client id: f_00008-12-1 loss: 0.723380  [   64/  130]
train() client id: f_00008-12-2 loss: 0.656516  [   96/  130]
train() client id: f_00008-12-3 loss: 0.679829  [  128/  130]
train() client id: f_00008-13-0 loss: 0.706837  [   32/  130]
train() client id: f_00008-13-1 loss: 0.744559  [   64/  130]
train() client id: f_00008-13-2 loss: 0.630576  [   96/  130]
train() client id: f_00008-13-3 loss: 0.751134  [  128/  130]
train() client id: f_00009-0-0 loss: 1.177749  [   32/  118]
train() client id: f_00009-0-1 loss: 1.270285  [   64/  118]
train() client id: f_00009-0-2 loss: 1.024791  [   96/  118]
train() client id: f_00009-1-0 loss: 1.095821  [   32/  118]
train() client id: f_00009-1-1 loss: 1.078327  [   64/  118]
train() client id: f_00009-1-2 loss: 1.059489  [   96/  118]
train() client id: f_00009-2-0 loss: 1.124158  [   32/  118]
train() client id: f_00009-2-1 loss: 0.981095  [   64/  118]
train() client id: f_00009-2-2 loss: 0.938987  [   96/  118]
train() client id: f_00009-3-0 loss: 0.964439  [   32/  118]
train() client id: f_00009-3-1 loss: 0.902015  [   64/  118]
train() client id: f_00009-3-2 loss: 1.000702  [   96/  118]
train() client id: f_00009-4-0 loss: 0.921763  [   32/  118]
train() client id: f_00009-4-1 loss: 0.969251  [   64/  118]
train() client id: f_00009-4-2 loss: 0.925859  [   96/  118]
train() client id: f_00009-5-0 loss: 0.905000  [   32/  118]
train() client id: f_00009-5-1 loss: 0.822784  [   64/  118]
train() client id: f_00009-5-2 loss: 0.995649  [   96/  118]
train() client id: f_00009-6-0 loss: 0.983765  [   32/  118]
train() client id: f_00009-6-1 loss: 0.869584  [   64/  118]
train() client id: f_00009-6-2 loss: 0.803444  [   96/  118]
train() client id: f_00009-7-0 loss: 0.840680  [   32/  118]
train() client id: f_00009-7-1 loss: 0.925145  [   64/  118]
train() client id: f_00009-7-2 loss: 0.915729  [   96/  118]
train() client id: f_00009-8-0 loss: 0.983463  [   32/  118]
train() client id: f_00009-8-1 loss: 0.823800  [   64/  118]
train() client id: f_00009-8-2 loss: 0.801482  [   96/  118]
train() client id: f_00009-9-0 loss: 0.939935  [   32/  118]
train() client id: f_00009-9-1 loss: 0.637993  [   64/  118]
train() client id: f_00009-9-2 loss: 0.889039  [   96/  118]
train() client id: f_00009-10-0 loss: 0.821119  [   32/  118]
train() client id: f_00009-10-1 loss: 0.881243  [   64/  118]
train() client id: f_00009-10-2 loss: 0.759149  [   96/  118]
train() client id: f_00009-11-0 loss: 0.879418  [   32/  118]
train() client id: f_00009-11-1 loss: 0.729263  [   64/  118]
train() client id: f_00009-11-2 loss: 0.836872  [   96/  118]
train() client id: f_00009-12-0 loss: 0.753168  [   32/  118]
train() client id: f_00009-12-1 loss: 0.953939  [   64/  118]
train() client id: f_00009-12-2 loss: 0.719074  [   96/  118]
train() client id: f_00009-13-0 loss: 0.873272  [   32/  118]
train() client id: f_00009-13-1 loss: 0.905760  [   64/  118]
train() client id: f_00009-13-2 loss: 0.757618  [   96/  118]
At round 30 accuracy: 0.6419098143236074
At round 30 training accuracy: 0.5875251509054326
At round 30 training loss: 0.8420789403293306
update_location
xs = 8.927491 271.223621 5.882650 10.934260 -187.581990 -35.230757 -5.849135 -5.143845 -210.120581 20.134486 
ys = -262.390647 7.291448 160.684448 -2.290817 -9.642386 0.794442 -1.381692 156.628436 25.881276 -697.232496 
xs mean: -12.68237997052122
ys mean: -62.16579882624052
dists_uav = 280.942257 289.163306 189.351782 100.622094 212.790927 106.027531 100.180444 185.900312 234.137778 704.654916 
uav_gains = -114.065209 -114.799757 -107.015417 -100.067351 -108.504337 -100.635503 -100.019590 -106.799435 -110.014551 -128.152992 
uav_gains_db_mean: -109.00741422260896
dists_bs = 474.489094 476.698729 181.448252 256.911625 185.070574 223.345064 244.381907 170.846796 153.198724 893.793597 
bs_gains = -114.501394 -114.557891 -102.812137 -107.041004 -103.052506 -105.338397 -106.432993 -102.080051 -100.754203 -122.201702 
bs_gains_db_mean: -107.8772277260196
Round 31
-------------------------------
ene_coms = [0.01463212 0.01563196 0.00713373 0.00632009 0.0072149  0.00647311
 0.00630755 0.00689722 0.00650601 0.03790908]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.00133995 14.45640558  6.76129838  2.4128812  16.5008218   7.92670919
  3.00122424  9.69992985  7.07119761  6.98625088]
obj_prev = 81.81805868186419
eta_min = 3.965002302131713e-14	eta_max = 0.762490986707317
af = 17.150202743271663	bf = 2.0202757380797167	zeta = 18.86522301759883	eta = 0.9090909090909091
af = 17.150202743271663	bf = 2.0202757380797167	zeta = 40.20351109743721	eta = 0.42658470056773007
af = 17.150202743271663	bf = 2.0202757380797167	zeta = 29.003924143857176	eta = 0.591306288701074
af = 17.150202743271663	bf = 2.0202757380797167	zeta = 26.991741061691982	eta = 0.6353870505823754
af = 17.150202743271663	bf = 2.0202757380797167	zeta = 26.873021862631468	eta = 0.6381940531637805
af = 17.150202743271663	bf = 2.0202757380797167	zeta = 26.872562009155697	eta = 0.6382049741825305
af = 17.150202743271663	bf = 2.0202757380797167	zeta = 26.872562002215254	eta = 0.6382049743473613
eta = 0.6382049743473613
ene_coms = [0.01463212 0.01563196 0.00713373 0.00632009 0.0072149  0.00647311
 0.00630755 0.00689722 0.00650601 0.03790908]
ene_comp = [0.03705831 0.07794009 0.03647008 0.01264688 0.08999874 0.04294058
 0.01588213 0.05264632 0.03823476 0.0347054 ]
ene_total = [2.50936079 4.54254346 2.11678824 0.92076938 4.7193281  2.39883415
 1.07721891 2.89059773 2.17198296 3.52513828]
ti_comp = [0.43912399 0.42912565 0.51410791 0.52224437 0.51329626 0.52071413
 0.52236976 0.51647301 0.52038514 0.20635447]
ti_coms = [0.14632123 0.15631957 0.07133732 0.06320085 0.07214896 0.06473109
 0.06307547 0.06897221 0.06506009 0.37909075]
t_total = [28.4209919 28.4209919 28.4209919 28.4209919 28.4209919 28.4209919
 28.4209919 28.4209919 28.4209919 28.4209919]
ene_coms = [0.01463212 0.01563196 0.00713373 0.00632009 0.0072149  0.00647311
 0.00630755 0.00689722 0.00650601 0.03790908]
ene_comp = [1.64953828e-05 1.60691817e-04 1.14704797e-05 4.63535420e-07
 1.72923161e-04 1.82509274e-05 9.17593490e-07 3.41891989e-05
 1.29004655e-05 6.13540665e-05]
ene_total = [0.71113104 0.76666907 0.3468706  0.30683702 0.35864867 0.31512923
 0.30625037 0.33649188 0.31646661 1.8433104 ]
optimize_network iter = 0 obj = 5.607804882119487
eta = 0.6382049743473613
freqs = [42195720.77881956 90812666.23870334 35469284.81326063 12108203.68158258
 87667445.71352808 41232391.39657684 15202001.297432   50967159.48583438
 36736983.94598855 84091704.61581711]
eta_min = 0.6382049743473622	eta_max = 0.6382049743473613
af = 0.019149975707451233	bf = 2.0202757380797167	zeta = 0.02106497327819636	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01463212 0.01563196 0.00713373 0.00632009 0.0072149  0.00647311
 0.00630755 0.00689722 0.00650601 0.03790908]
ene_comp = [3.50076309e-06 3.41031177e-05 2.43434376e-06 9.83746609e-08
 3.66989374e-05 3.87333678e-06 1.94737974e-07 7.25586591e-06
 2.73782514e-06 1.30209801e-05]
ene_total = [2.82760984 3.02669063 1.37871083 1.22106263 1.40101175 1.25135631
 1.2186588  1.33394845 1.25749307 7.32656788]
ti_comp = [0.43912399 0.42912565 0.51410791 0.52224437 0.51329626 0.52071413
 0.52236976 0.51647301 0.52038514 0.20635447]
ti_coms = [0.14632123 0.15631957 0.07133732 0.06320085 0.07214896 0.06473109
 0.06307547 0.06897221 0.06506009 0.37909075]
t_total = [28.4209919 28.4209919 28.4209919 28.4209919 28.4209919 28.4209919
 28.4209919 28.4209919 28.4209919 28.4209919]
ene_coms = [0.01463212 0.01563196 0.00713373 0.00632009 0.0072149  0.00647311
 0.00630755 0.00689722 0.00650601 0.03790908]
ene_comp = [1.64953828e-05 1.60691817e-04 1.14704797e-05 4.63535420e-07
 1.72923161e-04 1.82509274e-05 9.17593490e-07 3.41891989e-05
 1.29004655e-05 6.13540665e-05]
ene_total = [0.71113104 0.76666907 0.3468706  0.30683702 0.35864867 0.31512923
 0.30625037 0.33649188 0.31646661 1.8433104 ]
optimize_network iter = 1 obj = 5.607804882119487
eta = 0.6382049743473613
freqs = [42195720.77881956 90812666.23870334 35469284.81326063 12108203.68158258
 87667445.71352808 41232391.39657684 15202001.297432   50967159.48583438
 36736983.94598855 84091704.61581711]
Done!
ene_coms = [0.01463212 0.01563196 0.00713373 0.00632009 0.0072149  0.00647311
 0.00630755 0.00689722 0.00650601 0.03790908]
ene_comp = [1.57038235e-05 1.52980743e-04 1.09200490e-05 4.41291875e-07
 1.64625146e-04 1.73751252e-05 8.73561188e-07 3.25485714e-05
 1.22814145e-05 5.84098861e-05]
ene_total = [0.01464783 0.01578494 0.00714465 0.00632053 0.00737952 0.00649048
 0.00630842 0.00692977 0.00651829 0.03796749]
At round 31 energy consumption: 0.11549191469389239
At round 31 eta: 0.6382049743473613
At round 31 a_n: 17.56368160020554
At round 31 local rounds: 14.705677217084375
At round 31 global rounds: 48.545945507466776
gradient difference: 0.4216631054878235
train() client id: f_00000-0-0 loss: 1.493820  [   32/  126]
train() client id: f_00000-0-1 loss: 1.323545  [   64/  126]
train() client id: f_00000-0-2 loss: 1.350446  [   96/  126]
train() client id: f_00000-1-0 loss: 1.281690  [   32/  126]
train() client id: f_00000-1-1 loss: 1.030259  [   64/  126]
train() client id: f_00000-1-2 loss: 1.344625  [   96/  126]
train() client id: f_00000-2-0 loss: 1.138142  [   32/  126]
train() client id: f_00000-2-1 loss: 0.931916  [   64/  126]
train() client id: f_00000-2-2 loss: 1.085713  [   96/  126]
train() client id: f_00000-3-0 loss: 0.860797  [   32/  126]
train() client id: f_00000-3-1 loss: 1.075405  [   64/  126]
train() client id: f_00000-3-2 loss: 1.080616  [   96/  126]
train() client id: f_00000-4-0 loss: 1.006867  [   32/  126]
train() client id: f_00000-4-1 loss: 0.979318  [   64/  126]
train() client id: f_00000-4-2 loss: 0.908991  [   96/  126]
train() client id: f_00000-5-0 loss: 0.916045  [   32/  126]
train() client id: f_00000-5-1 loss: 0.876255  [   64/  126]
train() client id: f_00000-5-2 loss: 0.890251  [   96/  126]
train() client id: f_00000-6-0 loss: 0.971949  [   32/  126]
train() client id: f_00000-6-1 loss: 0.870340  [   64/  126]
train() client id: f_00000-6-2 loss: 0.846254  [   96/  126]
train() client id: f_00000-7-0 loss: 0.875310  [   32/  126]
train() client id: f_00000-7-1 loss: 0.821105  [   64/  126]
train() client id: f_00000-7-2 loss: 0.787890  [   96/  126]
train() client id: f_00000-8-0 loss: 0.902714  [   32/  126]
train() client id: f_00000-8-1 loss: 0.737484  [   64/  126]
train() client id: f_00000-8-2 loss: 0.767201  [   96/  126]
train() client id: f_00000-9-0 loss: 0.749373  [   32/  126]
train() client id: f_00000-9-1 loss: 0.775726  [   64/  126]
train() client id: f_00000-9-2 loss: 0.780673  [   96/  126]
train() client id: f_00000-10-0 loss: 0.758171  [   32/  126]
train() client id: f_00000-10-1 loss: 0.781568  [   64/  126]
train() client id: f_00000-10-2 loss: 0.679387  [   96/  126]
train() client id: f_00000-11-0 loss: 0.750050  [   32/  126]
train() client id: f_00000-11-1 loss: 0.788828  [   64/  126]
train() client id: f_00000-11-2 loss: 0.822567  [   96/  126]
train() client id: f_00000-12-0 loss: 0.793331  [   32/  126]
train() client id: f_00000-12-1 loss: 0.773443  [   64/  126]
train() client id: f_00000-12-2 loss: 0.699817  [   96/  126]
train() client id: f_00000-13-0 loss: 0.786793  [   32/  126]
train() client id: f_00000-13-1 loss: 0.700292  [   64/  126]
train() client id: f_00000-13-2 loss: 0.780981  [   96/  126]
train() client id: f_00001-0-0 loss: 0.414460  [   32/  265]
train() client id: f_00001-0-1 loss: 0.461784  [   64/  265]
train() client id: f_00001-0-2 loss: 0.538289  [   96/  265]
train() client id: f_00001-0-3 loss: 0.541301  [  128/  265]
train() client id: f_00001-0-4 loss: 0.398486  [  160/  265]
train() client id: f_00001-0-5 loss: 0.408471  [  192/  265]
train() client id: f_00001-0-6 loss: 0.392738  [  224/  265]
train() client id: f_00001-0-7 loss: 0.401100  [  256/  265]
train() client id: f_00001-1-0 loss: 0.445340  [   32/  265]
train() client id: f_00001-1-1 loss: 0.503938  [   64/  265]
train() client id: f_00001-1-2 loss: 0.414776  [   96/  265]
train() client id: f_00001-1-3 loss: 0.490287  [  128/  265]
train() client id: f_00001-1-4 loss: 0.441378  [  160/  265]
train() client id: f_00001-1-5 loss: 0.487037  [  192/  265]
train() client id: f_00001-1-6 loss: 0.389954  [  224/  265]
train() client id: f_00001-1-7 loss: 0.361346  [  256/  265]
train() client id: f_00001-2-0 loss: 0.375388  [   32/  265]
train() client id: f_00001-2-1 loss: 0.434980  [   64/  265]
train() client id: f_00001-2-2 loss: 0.453089  [   96/  265]
train() client id: f_00001-2-3 loss: 0.331745  [  128/  265]
train() client id: f_00001-2-4 loss: 0.467313  [  160/  265]
train() client id: f_00001-2-5 loss: 0.552641  [  192/  265]
train() client id: f_00001-2-6 loss: 0.516204  [  224/  265]
train() client id: f_00001-2-7 loss: 0.434969  [  256/  265]
train() client id: f_00001-3-0 loss: 0.503140  [   32/  265]
train() client id: f_00001-3-1 loss: 0.367812  [   64/  265]
train() client id: f_00001-3-2 loss: 0.396068  [   96/  265]
train() client id: f_00001-3-3 loss: 0.484587  [  128/  265]
train() client id: f_00001-3-4 loss: 0.544567  [  160/  265]
train() client id: f_00001-3-5 loss: 0.398796  [  192/  265]
train() client id: f_00001-3-6 loss: 0.347245  [  224/  265]
train() client id: f_00001-3-7 loss: 0.476174  [  256/  265]
train() client id: f_00001-4-0 loss: 0.376392  [   32/  265]
train() client id: f_00001-4-1 loss: 0.511239  [   64/  265]
train() client id: f_00001-4-2 loss: 0.412535  [   96/  265]
train() client id: f_00001-4-3 loss: 0.465918  [  128/  265]
train() client id: f_00001-4-4 loss: 0.419905  [  160/  265]
train() client id: f_00001-4-5 loss: 0.474082  [  192/  265]
train() client id: f_00001-4-6 loss: 0.418545  [  224/  265]
train() client id: f_00001-4-7 loss: 0.394945  [  256/  265]
train() client id: f_00001-5-0 loss: 0.413700  [   32/  265]
train() client id: f_00001-5-1 loss: 0.454670  [   64/  265]
train() client id: f_00001-5-2 loss: 0.382140  [   96/  265]
train() client id: f_00001-5-3 loss: 0.542660  [  128/  265]
train() client id: f_00001-5-4 loss: 0.476359  [  160/  265]
train() client id: f_00001-5-5 loss: 0.348693  [  192/  265]
train() client id: f_00001-5-6 loss: 0.407198  [  224/  265]
train() client id: f_00001-5-7 loss: 0.438034  [  256/  265]
train() client id: f_00001-6-0 loss: 0.396456  [   32/  265]
train() client id: f_00001-6-1 loss: 0.411711  [   64/  265]
train() client id: f_00001-6-2 loss: 0.451820  [   96/  265]
train() client id: f_00001-6-3 loss: 0.439428  [  128/  265]
train() client id: f_00001-6-4 loss: 0.502131  [  160/  265]
train() client id: f_00001-6-5 loss: 0.335115  [  192/  265]
train() client id: f_00001-6-6 loss: 0.423243  [  224/  265]
train() client id: f_00001-6-7 loss: 0.483171  [  256/  265]
train() client id: f_00001-7-0 loss: 0.395113  [   32/  265]
train() client id: f_00001-7-1 loss: 0.559748  [   64/  265]
train() client id: f_00001-7-2 loss: 0.347004  [   96/  265]
train() client id: f_00001-7-3 loss: 0.396377  [  128/  265]
train() client id: f_00001-7-4 loss: 0.459061  [  160/  265]
train() client id: f_00001-7-5 loss: 0.588180  [  192/  265]
train() client id: f_00001-7-6 loss: 0.349954  [  224/  265]
train() client id: f_00001-7-7 loss: 0.319154  [  256/  265]
train() client id: f_00001-8-0 loss: 0.483477  [   32/  265]
train() client id: f_00001-8-1 loss: 0.455674  [   64/  265]
train() client id: f_00001-8-2 loss: 0.444437  [   96/  265]
train() client id: f_00001-8-3 loss: 0.323643  [  128/  265]
train() client id: f_00001-8-4 loss: 0.388142  [  160/  265]
train() client id: f_00001-8-5 loss: 0.330462  [  192/  265]
train() client id: f_00001-8-6 loss: 0.427186  [  224/  265]
train() client id: f_00001-8-7 loss: 0.479266  [  256/  265]
train() client id: f_00001-9-0 loss: 0.329960  [   32/  265]
train() client id: f_00001-9-1 loss: 0.391952  [   64/  265]
train() client id: f_00001-9-2 loss: 0.411840  [   96/  265]
train() client id: f_00001-9-3 loss: 0.568282  [  128/  265]
train() client id: f_00001-9-4 loss: 0.383613  [  160/  265]
train() client id: f_00001-9-5 loss: 0.571784  [  192/  265]
train() client id: f_00001-9-6 loss: 0.328875  [  224/  265]
train() client id: f_00001-9-7 loss: 0.418035  [  256/  265]
train() client id: f_00001-10-0 loss: 0.330354  [   32/  265]
train() client id: f_00001-10-1 loss: 0.466781  [   64/  265]
train() client id: f_00001-10-2 loss: 0.336308  [   96/  265]
train() client id: f_00001-10-3 loss: 0.550503  [  128/  265]
train() client id: f_00001-10-4 loss: 0.516451  [  160/  265]
train() client id: f_00001-10-5 loss: 0.401774  [  192/  265]
train() client id: f_00001-10-6 loss: 0.344525  [  224/  265]
train() client id: f_00001-10-7 loss: 0.442888  [  256/  265]
train() client id: f_00001-11-0 loss: 0.420465  [   32/  265]
train() client id: f_00001-11-1 loss: 0.400154  [   64/  265]
train() client id: f_00001-11-2 loss: 0.370995  [   96/  265]
train() client id: f_00001-11-3 loss: 0.524810  [  128/  265]
train() client id: f_00001-11-4 loss: 0.359506  [  160/  265]
train() client id: f_00001-11-5 loss: 0.457830  [  192/  265]
train() client id: f_00001-11-6 loss: 0.350207  [  224/  265]
train() client id: f_00001-11-7 loss: 0.506209  [  256/  265]
train() client id: f_00001-12-0 loss: 0.330995  [   32/  265]
train() client id: f_00001-12-1 loss: 0.587933  [   64/  265]
train() client id: f_00001-12-2 loss: 0.384781  [   96/  265]
train() client id: f_00001-12-3 loss: 0.388577  [  128/  265]
train() client id: f_00001-12-4 loss: 0.370883  [  160/  265]
train() client id: f_00001-12-5 loss: 0.443792  [  192/  265]
train() client id: f_00001-12-6 loss: 0.333467  [  224/  265]
train() client id: f_00001-12-7 loss: 0.550572  [  256/  265]
train() client id: f_00001-13-0 loss: 0.379115  [   32/  265]
train() client id: f_00001-13-1 loss: 0.506885  [   64/  265]
train() client id: f_00001-13-2 loss: 0.377774  [   96/  265]
train() client id: f_00001-13-3 loss: 0.448389  [  128/  265]
train() client id: f_00001-13-4 loss: 0.408172  [  160/  265]
train() client id: f_00001-13-5 loss: 0.377297  [  192/  265]
train() client id: f_00001-13-6 loss: 0.435700  [  224/  265]
train() client id: f_00001-13-7 loss: 0.461981  [  256/  265]
train() client id: f_00002-0-0 loss: 1.215057  [   32/  124]
train() client id: f_00002-0-1 loss: 1.127033  [   64/  124]
train() client id: f_00002-0-2 loss: 1.241508  [   96/  124]
train() client id: f_00002-1-0 loss: 1.190537  [   32/  124]
train() client id: f_00002-1-1 loss: 1.243551  [   64/  124]
train() client id: f_00002-1-2 loss: 1.138032  [   96/  124]
train() client id: f_00002-2-0 loss: 1.214956  [   32/  124]
train() client id: f_00002-2-1 loss: 1.116852  [   64/  124]
train() client id: f_00002-2-2 loss: 1.021436  [   96/  124]
train() client id: f_00002-3-0 loss: 1.014643  [   32/  124]
train() client id: f_00002-3-1 loss: 1.162287  [   64/  124]
train() client id: f_00002-3-2 loss: 1.145960  [   96/  124]
train() client id: f_00002-4-0 loss: 1.025295  [   32/  124]
train() client id: f_00002-4-1 loss: 1.170494  [   64/  124]
train() client id: f_00002-4-2 loss: 0.922888  [   96/  124]
train() client id: f_00002-5-0 loss: 1.189612  [   32/  124]
train() client id: f_00002-5-1 loss: 0.904468  [   64/  124]
train() client id: f_00002-5-2 loss: 1.034137  [   96/  124]
train() client id: f_00002-6-0 loss: 1.202539  [   32/  124]
train() client id: f_00002-6-1 loss: 0.981566  [   64/  124]
train() client id: f_00002-6-2 loss: 0.966933  [   96/  124]
train() client id: f_00002-7-0 loss: 1.079558  [   32/  124]
train() client id: f_00002-7-1 loss: 1.076550  [   64/  124]
train() client id: f_00002-7-2 loss: 0.912177  [   96/  124]
train() client id: f_00002-8-0 loss: 0.920088  [   32/  124]
train() client id: f_00002-8-1 loss: 0.993268  [   64/  124]
train() client id: f_00002-8-2 loss: 1.090922  [   96/  124]
train() client id: f_00002-9-0 loss: 0.999979  [   32/  124]
train() client id: f_00002-9-1 loss: 1.140510  [   64/  124]
train() client id: f_00002-9-2 loss: 0.930553  [   96/  124]
train() client id: f_00002-10-0 loss: 0.998845  [   32/  124]
train() client id: f_00002-10-1 loss: 0.996350  [   64/  124]
train() client id: f_00002-10-2 loss: 0.999052  [   96/  124]
train() client id: f_00002-11-0 loss: 0.973162  [   32/  124]
train() client id: f_00002-11-1 loss: 1.164203  [   64/  124]
train() client id: f_00002-11-2 loss: 0.980425  [   96/  124]
train() client id: f_00002-12-0 loss: 0.931298  [   32/  124]
train() client id: f_00002-12-1 loss: 0.943664  [   64/  124]
train() client id: f_00002-12-2 loss: 1.143639  [   96/  124]
train() client id: f_00002-13-0 loss: 1.037399  [   32/  124]
train() client id: f_00002-13-1 loss: 1.094160  [   64/  124]
train() client id: f_00002-13-2 loss: 0.895126  [   96/  124]
train() client id: f_00003-0-0 loss: 0.641610  [   32/   43]
train() client id: f_00003-1-0 loss: 0.666767  [   32/   43]
train() client id: f_00003-2-0 loss: 0.754622  [   32/   43]
train() client id: f_00003-3-0 loss: 0.446423  [   32/   43]
train() client id: f_00003-4-0 loss: 0.558951  [   32/   43]
train() client id: f_00003-5-0 loss: 0.785010  [   32/   43]
train() client id: f_00003-6-0 loss: 0.695555  [   32/   43]
train() client id: f_00003-7-0 loss: 0.722918  [   32/   43]
train() client id: f_00003-8-0 loss: 0.549606  [   32/   43]
train() client id: f_00003-9-0 loss: 0.623532  [   32/   43]
train() client id: f_00003-10-0 loss: 0.738150  [   32/   43]
train() client id: f_00003-11-0 loss: 0.871679  [   32/   43]
train() client id: f_00003-12-0 loss: 0.694833  [   32/   43]
train() client id: f_00003-13-0 loss: 0.655332  [   32/   43]
train() client id: f_00004-0-0 loss: 0.825694  [   32/  306]
train() client id: f_00004-0-1 loss: 0.719176  [   64/  306]
train() client id: f_00004-0-2 loss: 0.883149  [   96/  306]
train() client id: f_00004-0-3 loss: 0.805018  [  128/  306]
train() client id: f_00004-0-4 loss: 0.820035  [  160/  306]
train() client id: f_00004-0-5 loss: 0.697080  [  192/  306]
train() client id: f_00004-0-6 loss: 0.928251  [  224/  306]
train() client id: f_00004-0-7 loss: 0.861153  [  256/  306]
train() client id: f_00004-0-8 loss: 0.899901  [  288/  306]
train() client id: f_00004-1-0 loss: 0.957898  [   32/  306]
train() client id: f_00004-1-1 loss: 0.806178  [   64/  306]
train() client id: f_00004-1-2 loss: 0.756464  [   96/  306]
train() client id: f_00004-1-3 loss: 0.867742  [  128/  306]
train() client id: f_00004-1-4 loss: 0.755902  [  160/  306]
train() client id: f_00004-1-5 loss: 0.649472  [  192/  306]
train() client id: f_00004-1-6 loss: 0.904497  [  224/  306]
train() client id: f_00004-1-7 loss: 0.910020  [  256/  306]
train() client id: f_00004-1-8 loss: 0.896895  [  288/  306]
train() client id: f_00004-2-0 loss: 0.897985  [   32/  306]
train() client id: f_00004-2-1 loss: 0.835845  [   64/  306]
train() client id: f_00004-2-2 loss: 0.815069  [   96/  306]
train() client id: f_00004-2-3 loss: 0.820847  [  128/  306]
train() client id: f_00004-2-4 loss: 0.857590  [  160/  306]
train() client id: f_00004-2-5 loss: 0.754676  [  192/  306]
train() client id: f_00004-2-6 loss: 0.927918  [  224/  306]
train() client id: f_00004-2-7 loss: 0.830417  [  256/  306]
train() client id: f_00004-2-8 loss: 0.731036  [  288/  306]
train() client id: f_00004-3-0 loss: 0.778127  [   32/  306]
train() client id: f_00004-3-1 loss: 0.731794  [   64/  306]
train() client id: f_00004-3-2 loss: 0.833426  [   96/  306]
train() client id: f_00004-3-3 loss: 0.892075  [  128/  306]
train() client id: f_00004-3-4 loss: 0.843535  [  160/  306]
train() client id: f_00004-3-5 loss: 0.934634  [  192/  306]
train() client id: f_00004-3-6 loss: 0.821042  [  224/  306]
train() client id: f_00004-3-7 loss: 0.767308  [  256/  306]
train() client id: f_00004-3-8 loss: 0.839092  [  288/  306]
train() client id: f_00004-4-0 loss: 0.755626  [   32/  306]
train() client id: f_00004-4-1 loss: 0.885249  [   64/  306]
train() client id: f_00004-4-2 loss: 0.952985  [   96/  306]
train() client id: f_00004-4-3 loss: 0.657726  [  128/  306]
train() client id: f_00004-4-4 loss: 0.944250  [  160/  306]
train() client id: f_00004-4-5 loss: 0.747626  [  192/  306]
train() client id: f_00004-4-6 loss: 0.915440  [  224/  306]
train() client id: f_00004-4-7 loss: 0.838679  [  256/  306]
train() client id: f_00004-4-8 loss: 0.778362  [  288/  306]
train() client id: f_00004-5-0 loss: 0.932237  [   32/  306]
train() client id: f_00004-5-1 loss: 0.901330  [   64/  306]
train() client id: f_00004-5-2 loss: 0.722922  [   96/  306]
train() client id: f_00004-5-3 loss: 0.841959  [  128/  306]
train() client id: f_00004-5-4 loss: 0.800667  [  160/  306]
train() client id: f_00004-5-5 loss: 0.778998  [  192/  306]
train() client id: f_00004-5-6 loss: 0.835217  [  224/  306]
train() client id: f_00004-5-7 loss: 0.746077  [  256/  306]
train() client id: f_00004-5-8 loss: 0.883624  [  288/  306]
train() client id: f_00004-6-0 loss: 0.828256  [   32/  306]
train() client id: f_00004-6-1 loss: 0.786566  [   64/  306]
train() client id: f_00004-6-2 loss: 0.854208  [   96/  306]
train() client id: f_00004-6-3 loss: 0.723542  [  128/  306]
train() client id: f_00004-6-4 loss: 0.729860  [  160/  306]
train() client id: f_00004-6-5 loss: 0.998444  [  192/  306]
train() client id: f_00004-6-6 loss: 0.803954  [  224/  306]
train() client id: f_00004-6-7 loss: 0.812657  [  256/  306]
train() client id: f_00004-6-8 loss: 0.799749  [  288/  306]
train() client id: f_00004-7-0 loss: 0.875428  [   32/  306]
train() client id: f_00004-7-1 loss: 0.781924  [   64/  306]
train() client id: f_00004-7-2 loss: 0.888524  [   96/  306]
train() client id: f_00004-7-3 loss: 0.849841  [  128/  306]
train() client id: f_00004-7-4 loss: 0.793727  [  160/  306]
train() client id: f_00004-7-5 loss: 0.656554  [  192/  306]
train() client id: f_00004-7-6 loss: 0.700488  [  224/  306]
train() client id: f_00004-7-7 loss: 0.980796  [  256/  306]
train() client id: f_00004-7-8 loss: 0.875423  [  288/  306]
train() client id: f_00004-8-0 loss: 0.792489  [   32/  306]
train() client id: f_00004-8-1 loss: 0.836724  [   64/  306]
train() client id: f_00004-8-2 loss: 0.825292  [   96/  306]
train() client id: f_00004-8-3 loss: 0.732036  [  128/  306]
train() client id: f_00004-8-4 loss: 0.821978  [  160/  306]
train() client id: f_00004-8-5 loss: 0.769832  [  192/  306]
train() client id: f_00004-8-6 loss: 0.965943  [  224/  306]
train() client id: f_00004-8-7 loss: 0.776521  [  256/  306]
train() client id: f_00004-8-8 loss: 0.911235  [  288/  306]
train() client id: f_00004-9-0 loss: 0.764454  [   32/  306]
train() client id: f_00004-9-1 loss: 0.845668  [   64/  306]
train() client id: f_00004-9-2 loss: 0.870743  [   96/  306]
train() client id: f_00004-9-3 loss: 0.870154  [  128/  306]
train() client id: f_00004-9-4 loss: 0.803946  [  160/  306]
train() client id: f_00004-9-5 loss: 0.723444  [  192/  306]
train() client id: f_00004-9-6 loss: 0.833096  [  224/  306]
train() client id: f_00004-9-7 loss: 0.813578  [  256/  306]
train() client id: f_00004-9-8 loss: 0.769724  [  288/  306]
train() client id: f_00004-10-0 loss: 1.054475  [   32/  306]
train() client id: f_00004-10-1 loss: 0.722486  [   64/  306]
train() client id: f_00004-10-2 loss: 0.699095  [   96/  306]
train() client id: f_00004-10-3 loss: 0.906498  [  128/  306]
train() client id: f_00004-10-4 loss: 0.822983  [  160/  306]
train() client id: f_00004-10-5 loss: 0.789060  [  192/  306]
train() client id: f_00004-10-6 loss: 0.942003  [  224/  306]
train() client id: f_00004-10-7 loss: 0.803986  [  256/  306]
train() client id: f_00004-10-8 loss: 0.784043  [  288/  306]
train() client id: f_00004-11-0 loss: 0.861630  [   32/  306]
train() client id: f_00004-11-1 loss: 0.938448  [   64/  306]
train() client id: f_00004-11-2 loss: 0.710571  [   96/  306]
train() client id: f_00004-11-3 loss: 0.868380  [  128/  306]
train() client id: f_00004-11-4 loss: 0.773647  [  160/  306]
train() client id: f_00004-11-5 loss: 0.771355  [  192/  306]
train() client id: f_00004-11-6 loss: 0.827376  [  224/  306]
train() client id: f_00004-11-7 loss: 0.813802  [  256/  306]
train() client id: f_00004-11-8 loss: 0.863900  [  288/  306]
train() client id: f_00004-12-0 loss: 0.779275  [   32/  306]
train() client id: f_00004-12-1 loss: 0.739982  [   64/  306]
train() client id: f_00004-12-2 loss: 0.845419  [   96/  306]
train() client id: f_00004-12-3 loss: 0.750791  [  128/  306]
train() client id: f_00004-12-4 loss: 0.830188  [  160/  306]
train() client id: f_00004-12-5 loss: 0.898887  [  192/  306]
train() client id: f_00004-12-6 loss: 0.833293  [  224/  306]
train() client id: f_00004-12-7 loss: 0.939190  [  256/  306]
train() client id: f_00004-12-8 loss: 0.774228  [  288/  306]
train() client id: f_00004-13-0 loss: 0.666125  [   32/  306]
train() client id: f_00004-13-1 loss: 0.780447  [   64/  306]
train() client id: f_00004-13-2 loss: 0.868346  [   96/  306]
train() client id: f_00004-13-3 loss: 0.862406  [  128/  306]
train() client id: f_00004-13-4 loss: 0.829322  [  160/  306]
train() client id: f_00004-13-5 loss: 0.823095  [  192/  306]
train() client id: f_00004-13-6 loss: 0.819697  [  224/  306]
train() client id: f_00004-13-7 loss: 0.984758  [  256/  306]
train() client id: f_00004-13-8 loss: 0.816262  [  288/  306]
train() client id: f_00005-0-0 loss: 0.410685  [   32/  146]
train() client id: f_00005-0-1 loss: 0.527351  [   64/  146]
train() client id: f_00005-0-2 loss: 0.406295  [   96/  146]
train() client id: f_00005-0-3 loss: 0.816786  [  128/  146]
train() client id: f_00005-1-0 loss: 0.497048  [   32/  146]
train() client id: f_00005-1-1 loss: 0.612801  [   64/  146]
train() client id: f_00005-1-2 loss: 0.388153  [   96/  146]
train() client id: f_00005-1-3 loss: 0.374361  [  128/  146]
train() client id: f_00005-2-0 loss: 0.487825  [   32/  146]
train() client id: f_00005-2-1 loss: 0.477778  [   64/  146]
train() client id: f_00005-2-2 loss: 0.494000  [   96/  146]
train() client id: f_00005-2-3 loss: 0.479157  [  128/  146]
train() client id: f_00005-3-0 loss: 0.750534  [   32/  146]
train() client id: f_00005-3-1 loss: 0.552277  [   64/  146]
train() client id: f_00005-3-2 loss: 0.352795  [   96/  146]
train() client id: f_00005-3-3 loss: 0.501685  [  128/  146]
train() client id: f_00005-4-0 loss: 0.523410  [   32/  146]
train() client id: f_00005-4-1 loss: 0.601128  [   64/  146]
train() client id: f_00005-4-2 loss: 0.619721  [   96/  146]
train() client id: f_00005-4-3 loss: 0.386216  [  128/  146]
train() client id: f_00005-5-0 loss: 0.397104  [   32/  146]
train() client id: f_00005-5-1 loss: 0.642817  [   64/  146]
train() client id: f_00005-5-2 loss: 0.388008  [   96/  146]
train() client id: f_00005-5-3 loss: 0.670763  [  128/  146]
train() client id: f_00005-6-0 loss: 0.706911  [   32/  146]
train() client id: f_00005-6-1 loss: 0.349955  [   64/  146]
train() client id: f_00005-6-2 loss: 0.354721  [   96/  146]
train() client id: f_00005-6-3 loss: 0.597380  [  128/  146]
train() client id: f_00005-7-0 loss: 0.390948  [   32/  146]
train() client id: f_00005-7-1 loss: 0.461167  [   64/  146]
train() client id: f_00005-7-2 loss: 0.581050  [   96/  146]
train() client id: f_00005-7-3 loss: 0.383277  [  128/  146]
train() client id: f_00005-8-0 loss: 0.803799  [   32/  146]
train() client id: f_00005-8-1 loss: 0.444947  [   64/  146]
train() client id: f_00005-8-2 loss: 0.508792  [   96/  146]
train() client id: f_00005-8-3 loss: 0.480543  [  128/  146]
train() client id: f_00005-9-0 loss: 0.736585  [   32/  146]
train() client id: f_00005-9-1 loss: 0.348877  [   64/  146]
train() client id: f_00005-9-2 loss: 0.470136  [   96/  146]
train() client id: f_00005-9-3 loss: 0.565757  [  128/  146]
train() client id: f_00005-10-0 loss: 0.601165  [   32/  146]
train() client id: f_00005-10-1 loss: 0.508522  [   64/  146]
train() client id: f_00005-10-2 loss: 0.528600  [   96/  146]
train() client id: f_00005-10-3 loss: 0.477211  [  128/  146]
train() client id: f_00005-11-0 loss: 0.656968  [   32/  146]
train() client id: f_00005-11-1 loss: 0.639014  [   64/  146]
train() client id: f_00005-11-2 loss: 0.404937  [   96/  146]
train() client id: f_00005-11-3 loss: 0.507046  [  128/  146]
train() client id: f_00005-12-0 loss: 0.583493  [   32/  146]
train() client id: f_00005-12-1 loss: 0.579785  [   64/  146]
train() client id: f_00005-12-2 loss: 0.709991  [   96/  146]
train() client id: f_00005-12-3 loss: 0.454259  [  128/  146]
train() client id: f_00005-13-0 loss: 0.526086  [   32/  146]
train() client id: f_00005-13-1 loss: 0.270517  [   64/  146]
train() client id: f_00005-13-2 loss: 0.375772  [   96/  146]
train() client id: f_00005-13-3 loss: 0.930510  [  128/  146]
train() client id: f_00006-0-0 loss: 0.463775  [   32/   54]
train() client id: f_00006-1-0 loss: 0.417817  [   32/   54]
train() client id: f_00006-2-0 loss: 0.469817  [   32/   54]
train() client id: f_00006-3-0 loss: 0.461141  [   32/   54]
train() client id: f_00006-4-0 loss: 0.447107  [   32/   54]
train() client id: f_00006-5-0 loss: 0.518167  [   32/   54]
train() client id: f_00006-6-0 loss: 0.452846  [   32/   54]
train() client id: f_00006-7-0 loss: 0.504254  [   32/   54]
train() client id: f_00006-8-0 loss: 0.527016  [   32/   54]
train() client id: f_00006-9-0 loss: 0.408365  [   32/   54]
train() client id: f_00006-10-0 loss: 0.511049  [   32/   54]
train() client id: f_00006-11-0 loss: 0.459862  [   32/   54]
train() client id: f_00006-12-0 loss: 0.472960  [   32/   54]
train() client id: f_00006-13-0 loss: 0.459243  [   32/   54]
train() client id: f_00007-0-0 loss: 0.704051  [   32/  179]
train() client id: f_00007-0-1 loss: 0.510646  [   64/  179]
train() client id: f_00007-0-2 loss: 0.510654  [   96/  179]
train() client id: f_00007-0-3 loss: 0.774534  [  128/  179]
train() client id: f_00007-0-4 loss: 0.645715  [  160/  179]
train() client id: f_00007-1-0 loss: 0.541183  [   32/  179]
train() client id: f_00007-1-1 loss: 0.842269  [   64/  179]
train() client id: f_00007-1-2 loss: 0.797300  [   96/  179]
train() client id: f_00007-1-3 loss: 0.524377  [  128/  179]
train() client id: f_00007-1-4 loss: 0.441378  [  160/  179]
train() client id: f_00007-2-0 loss: 0.655427  [   32/  179]
train() client id: f_00007-2-1 loss: 0.493551  [   64/  179]
train() client id: f_00007-2-2 loss: 0.576467  [   96/  179]
train() client id: f_00007-2-3 loss: 0.669964  [  128/  179]
train() client id: f_00007-2-4 loss: 0.524890  [  160/  179]
train() client id: f_00007-3-0 loss: 0.453825  [   32/  179]
train() client id: f_00007-3-1 loss: 0.634416  [   64/  179]
train() client id: f_00007-3-2 loss: 0.551001  [   96/  179]
train() client id: f_00007-3-3 loss: 0.811167  [  128/  179]
train() client id: f_00007-3-4 loss: 0.408941  [  160/  179]
train() client id: f_00007-4-0 loss: 0.561067  [   32/  179]
train() client id: f_00007-4-1 loss: 0.683239  [   64/  179]
train() client id: f_00007-4-2 loss: 0.617237  [   96/  179]
train() client id: f_00007-4-3 loss: 0.572247  [  128/  179]
train() client id: f_00007-4-4 loss: 0.441350  [  160/  179]
train() client id: f_00007-5-0 loss: 0.536488  [   32/  179]
train() client id: f_00007-5-1 loss: 0.779093  [   64/  179]
train() client id: f_00007-5-2 loss: 0.531379  [   96/  179]
train() client id: f_00007-5-3 loss: 0.612840  [  128/  179]
train() client id: f_00007-5-4 loss: 0.432635  [  160/  179]
train() client id: f_00007-6-0 loss: 0.436262  [   32/  179]
train() client id: f_00007-6-1 loss: 0.901579  [   64/  179]
train() client id: f_00007-6-2 loss: 0.445475  [   96/  179]
train() client id: f_00007-6-3 loss: 0.459257  [  128/  179]
train() client id: f_00007-6-4 loss: 0.742694  [  160/  179]
train() client id: f_00007-7-0 loss: 0.693284  [   32/  179]
train() client id: f_00007-7-1 loss: 0.516026  [   64/  179]
train() client id: f_00007-7-2 loss: 0.538724  [   96/  179]
train() client id: f_00007-7-3 loss: 0.675639  [  128/  179]
train() client id: f_00007-7-4 loss: 0.514001  [  160/  179]
train() client id: f_00007-8-0 loss: 0.536442  [   32/  179]
train() client id: f_00007-8-1 loss: 0.502597  [   64/  179]
train() client id: f_00007-8-2 loss: 0.797635  [   96/  179]
train() client id: f_00007-8-3 loss: 0.542261  [  128/  179]
train() client id: f_00007-8-4 loss: 0.510112  [  160/  179]
train() client id: f_00007-9-0 loss: 0.677292  [   32/  179]
train() client id: f_00007-9-1 loss: 0.494338  [   64/  179]
train() client id: f_00007-9-2 loss: 0.482141  [   96/  179]
train() client id: f_00007-9-3 loss: 0.577386  [  128/  179]
train() client id: f_00007-9-4 loss: 0.613036  [  160/  179]
train() client id: f_00007-10-0 loss: 0.505841  [   32/  179]
train() client id: f_00007-10-1 loss: 0.558470  [   64/  179]
train() client id: f_00007-10-2 loss: 0.675120  [   96/  179]
train() client id: f_00007-10-3 loss: 0.583696  [  128/  179]
train() client id: f_00007-10-4 loss: 0.512472  [  160/  179]
train() client id: f_00007-11-0 loss: 0.424445  [   32/  179]
train() client id: f_00007-11-1 loss: 0.699752  [   64/  179]
train() client id: f_00007-11-2 loss: 0.570344  [   96/  179]
train() client id: f_00007-11-3 loss: 0.540439  [  128/  179]
train() client id: f_00007-11-4 loss: 0.693163  [  160/  179]
train() client id: f_00007-12-0 loss: 0.523225  [   32/  179]
train() client id: f_00007-12-1 loss: 0.705318  [   64/  179]
train() client id: f_00007-12-2 loss: 0.423136  [   96/  179]
train() client id: f_00007-12-3 loss: 0.560979  [  128/  179]
train() client id: f_00007-12-4 loss: 0.508882  [  160/  179]
train() client id: f_00007-13-0 loss: 0.564813  [   32/  179]
train() client id: f_00007-13-1 loss: 0.416544  [   64/  179]
train() client id: f_00007-13-2 loss: 0.618655  [   96/  179]
train() client id: f_00007-13-3 loss: 0.685261  [  128/  179]
train() client id: f_00007-13-4 loss: 0.583621  [  160/  179]
train() client id: f_00008-0-0 loss: 0.688081  [   32/  130]
train() client id: f_00008-0-1 loss: 0.768627  [   64/  130]
train() client id: f_00008-0-2 loss: 0.707613  [   96/  130]
train() client id: f_00008-0-3 loss: 0.707545  [  128/  130]
train() client id: f_00008-1-0 loss: 0.726191  [   32/  130]
train() client id: f_00008-1-1 loss: 0.726928  [   64/  130]
train() client id: f_00008-1-2 loss: 0.828239  [   96/  130]
train() client id: f_00008-1-3 loss: 0.589808  [  128/  130]
train() client id: f_00008-2-0 loss: 0.707587  [   32/  130]
train() client id: f_00008-2-1 loss: 0.658477  [   64/  130]
train() client id: f_00008-2-2 loss: 0.814390  [   96/  130]
train() client id: f_00008-2-3 loss: 0.717489  [  128/  130]
train() client id: f_00008-3-0 loss: 0.740186  [   32/  130]
train() client id: f_00008-3-1 loss: 0.591739  [   64/  130]
train() client id: f_00008-3-2 loss: 0.694897  [   96/  130]
train() client id: f_00008-3-3 loss: 0.857187  [  128/  130]
train() client id: f_00008-4-0 loss: 0.755438  [   32/  130]
train() client id: f_00008-4-1 loss: 0.760361  [   64/  130]
train() client id: f_00008-4-2 loss: 0.662297  [   96/  130]
train() client id: f_00008-4-3 loss: 0.724266  [  128/  130]
train() client id: f_00008-5-0 loss: 0.800733  [   32/  130]
train() client id: f_00008-5-1 loss: 0.586982  [   64/  130]
train() client id: f_00008-5-2 loss: 0.680785  [   96/  130]
train() client id: f_00008-5-3 loss: 0.801118  [  128/  130]
train() client id: f_00008-6-0 loss: 0.759782  [   32/  130]
train() client id: f_00008-6-1 loss: 0.740623  [   64/  130]
train() client id: f_00008-6-2 loss: 0.701233  [   96/  130]
train() client id: f_00008-6-3 loss: 0.679157  [  128/  130]
train() client id: f_00008-7-0 loss: 0.715683  [   32/  130]
train() client id: f_00008-7-1 loss: 0.790069  [   64/  130]
train() client id: f_00008-7-2 loss: 0.624271  [   96/  130]
train() client id: f_00008-7-3 loss: 0.739478  [  128/  130]
train() client id: f_00008-8-0 loss: 0.622646  [   32/  130]
train() client id: f_00008-8-1 loss: 0.676357  [   64/  130]
train() client id: f_00008-8-2 loss: 0.742077  [   96/  130]
train() client id: f_00008-8-3 loss: 0.801112  [  128/  130]
train() client id: f_00008-9-0 loss: 0.711046  [   32/  130]
train() client id: f_00008-9-1 loss: 0.718491  [   64/  130]
train() client id: f_00008-9-2 loss: 0.621475  [   96/  130]
train() client id: f_00008-9-3 loss: 0.809543  [  128/  130]
train() client id: f_00008-10-0 loss: 0.723735  [   32/  130]
train() client id: f_00008-10-1 loss: 0.827870  [   64/  130]
train() client id: f_00008-10-2 loss: 0.668051  [   96/  130]
train() client id: f_00008-10-3 loss: 0.615599  [  128/  130]
train() client id: f_00008-11-0 loss: 0.733354  [   32/  130]
train() client id: f_00008-11-1 loss: 0.726257  [   64/  130]
train() client id: f_00008-11-2 loss: 0.776279  [   96/  130]
train() client id: f_00008-11-3 loss: 0.614182  [  128/  130]
train() client id: f_00008-12-0 loss: 0.722571  [   32/  130]
train() client id: f_00008-12-1 loss: 0.652302  [   64/  130]
train() client id: f_00008-12-2 loss: 0.749184  [   96/  130]
train() client id: f_00008-12-3 loss: 0.756732  [  128/  130]
train() client id: f_00008-13-0 loss: 0.684367  [   32/  130]
train() client id: f_00008-13-1 loss: 0.740527  [   64/  130]
train() client id: f_00008-13-2 loss: 0.670821  [   96/  130]
train() client id: f_00008-13-3 loss: 0.733828  [  128/  130]
train() client id: f_00009-0-0 loss: 1.085333  [   32/  118]
train() client id: f_00009-0-1 loss: 1.261670  [   64/  118]
train() client id: f_00009-0-2 loss: 0.997331  [   96/  118]
train() client id: f_00009-1-0 loss: 1.058623  [   32/  118]
train() client id: f_00009-1-1 loss: 1.152363  [   64/  118]
train() client id: f_00009-1-2 loss: 1.150541  [   96/  118]
train() client id: f_00009-2-0 loss: 0.957890  [   32/  118]
train() client id: f_00009-2-1 loss: 1.077788  [   64/  118]
train() client id: f_00009-2-2 loss: 1.142235  [   96/  118]
train() client id: f_00009-3-0 loss: 0.952843  [   32/  118]
train() client id: f_00009-3-1 loss: 1.066652  [   64/  118]
train() client id: f_00009-3-2 loss: 0.989344  [   96/  118]
train() client id: f_00009-4-0 loss: 1.098196  [   32/  118]
train() client id: f_00009-4-1 loss: 0.917701  [   64/  118]
train() client id: f_00009-4-2 loss: 0.896330  [   96/  118]
train() client id: f_00009-5-0 loss: 0.897532  [   32/  118]
train() client id: f_00009-5-1 loss: 0.906311  [   64/  118]
train() client id: f_00009-5-2 loss: 1.115908  [   96/  118]
train() client id: f_00009-6-0 loss: 0.880678  [   32/  118]
train() client id: f_00009-6-1 loss: 0.996292  [   64/  118]
train() client id: f_00009-6-2 loss: 0.853802  [   96/  118]
train() client id: f_00009-7-0 loss: 0.962251  [   32/  118]
train() client id: f_00009-7-1 loss: 0.862541  [   64/  118]
train() client id: f_00009-7-2 loss: 0.960538  [   96/  118]
train() client id: f_00009-8-0 loss: 0.885001  [   32/  118]
train() client id: f_00009-8-1 loss: 0.895106  [   64/  118]
train() client id: f_00009-8-2 loss: 0.859973  [   96/  118]
train() client id: f_00009-9-0 loss: 0.922694  [   32/  118]
train() client id: f_00009-9-1 loss: 0.963915  [   64/  118]
train() client id: f_00009-9-2 loss: 0.912784  [   96/  118]
train() client id: f_00009-10-0 loss: 0.919229  [   32/  118]
train() client id: f_00009-10-1 loss: 0.907684  [   64/  118]
train() client id: f_00009-10-2 loss: 0.876900  [   96/  118]
train() client id: f_00009-11-0 loss: 0.957377  [   32/  118]
train() client id: f_00009-11-1 loss: 0.815797  [   64/  118]
train() client id: f_00009-11-2 loss: 0.824517  [   96/  118]
train() client id: f_00009-12-0 loss: 0.836256  [   32/  118]
train() client id: f_00009-12-1 loss: 0.852099  [   64/  118]
train() client id: f_00009-12-2 loss: 1.026443  [   96/  118]
train() client id: f_00009-13-0 loss: 0.859607  [   32/  118]
train() client id: f_00009-13-1 loss: 0.893652  [   64/  118]
train() client id: f_00009-13-2 loss: 0.944995  [   96/  118]
At round 31 accuracy: 0.6419098143236074
At round 31 training accuracy: 0.5835010060362174
At round 31 training loss: 0.8390232286531134
update_location
xs = 8.927491 276.223621 5.882650 5.934260 -192.581990 -40.230757 -5.849135 -5.143845 -215.120581 20.134486 
ys = -267.390647 7.291448 165.684448 -2.290817 -9.642386 0.794442 3.618308 161.628436 25.881276 -702.232496 
xs mean: -14.18237997052122
ys mean: -61.66579882624052
dists_uav = 285.617678 293.858221 193.612866 100.202112 217.211414 107.792138 100.236244 190.132087 238.635087 709.602618 
uav_gains = -114.485255 -115.209257 -107.281595 -100.021938 -108.799807 -100.814724 -100.025636 -107.064172 -110.363512 -128.229944 
uav_gains_db_mean: -109.22958392396043
dists_bs = 479.102084 481.382296 181.122369 253.316482 185.477592 220.250596 240.798047 170.381666 154.421678 898.673645 
bs_gains = -114.619045 -114.676782 -102.790278 -106.869636 -103.079220 -105.168738 -106.253342 -102.046899 -100.850891 -122.267915 
bs_gains_db_mean: -107.86227452656739
Round 32
-------------------------------
ene_coms = [0.01518971 0.01624214 0.00712644 0.00630816 0.00722403 0.00652291
 0.00630913 0.00688688 0.00653305 0.0382898 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.87449159 14.18507531  6.6293054   2.36561519 16.17916385  7.77298025
  2.94271869  9.51057141  6.93375791  6.85662022]
obj_prev = 80.25029981486693
eta_min = 2.2164583632667748e-14	eta_max = 0.7644236583069326
af = 16.815721006576958	bf = 2.008539781336388	zeta = 18.497293107234654	eta = 0.9090909090909091
af = 16.815721006576958	bf = 2.008539781336388	zeta = 39.7237409141386	eta = 0.4233166519468425
af = 16.815721006576958	bf = 2.008539781336388	zeta = 28.54932208014636	eta = 0.5890059651633854
af = 16.815721006576958	bf = 2.008539781336388	zeta = 26.54398590281847	eta = 0.6335039910035307
af = 16.815721006576958	bf = 2.008539781336388	zeta = 26.42520208222339	eta = 0.6363516522694498
af = 16.815721006576958	bf = 2.008539781336388	zeta = 26.424737690742024	eta = 0.6363628355890317
af = 16.815721006576958	bf = 2.008539781336388	zeta = 26.4247376836011	eta = 0.6363628357610001
eta = 0.6363628357610001
ene_coms = [0.01518971 0.01624214 0.00712644 0.00630816 0.00722403 0.00652291
 0.00630913 0.00688688 0.00653305 0.0382898 ]
ene_comp = [0.03729683 0.07844175 0.03670482 0.01272828 0.09057802 0.04321697
 0.01598436 0.05298518 0.03848086 0.03492878]
ene_total = [2.48565883 4.48404141 2.07576159 0.90152839 4.6317114  2.35558196
 1.05577544 2.83542211 2.13176966 3.46748688]
ti_comp = [0.44715783 0.43663359 0.52779057 0.53597334 0.52681467 0.53382586
 0.53596365 0.5301862  0.53372444 0.21615696]
ti_coms = [0.15189713 0.16242137 0.07126439 0.06308162 0.07224028 0.0652291
 0.06309131 0.06886876 0.06533052 0.38289799]
t_total = [28.37005615 28.37005615 28.37005615 28.37005615 28.37005615 28.37005615
 28.37005615 28.37005615 28.37005615 28.37005615]
ene_coms = [0.01518971 0.01624214 0.00712644 0.00630816 0.00722403 0.00652291
 0.00630913 0.00688688 0.00653305 0.0382898 ]
ene_comp = [1.62171376e-05 1.58229548e-04 1.10949680e-05 4.48645421e-07
 1.67352729e-04 1.77028668e-05 8.88576573e-07 3.30739830e-05
 1.25020180e-05 5.70022245e-05]
ene_total = [0.72012272 0.77668888 0.33801948 0.29876331 0.35004117 0.30975048
 0.29883004 0.32771512 0.30998446 1.81602857]
optimize_network iter = 0 obj = 5.545944219200192
eta = 0.6363628357610001
freqs = [41704327.15544253 89825604.29284693 34772144.48931189 11873990.11533099
 85967634.45465189 40478523.5519902  14911792.11629514 49968466.91888497
 36049369.66732018 80794944.968495  ]
eta_min = 0.6363628357610013	eta_max = 0.6363628357610001
af = 0.018079410994844513	bf = 2.008539781336388	zeta = 0.019887352094328965	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01518971 0.01624214 0.00712644 0.00630816 0.00722403 0.00652291
 0.00630913 0.00688688 0.00653305 0.0382898 ]
ene_comp = [3.41970103e-06 3.33657988e-05 2.33959128e-06 9.46056728e-08
 3.52896001e-05 3.73299613e-06 1.87373771e-07 6.97429701e-06
 2.63629533e-06 1.20200353e-05]
ene_total = [2.87807298 3.0831091  1.35042235 1.19498879 1.37515072 1.23635828
 1.19518996 1.30591922 1.23807169 7.25560917]
ti_comp = [0.44715783 0.43663359 0.52779057 0.53597334 0.52681467 0.53382586
 0.53596365 0.5301862  0.53372444 0.21615696]
ti_coms = [0.15189713 0.16242137 0.07126439 0.06308162 0.07224028 0.0652291
 0.06309131 0.06886876 0.06533052 0.38289799]
t_total = [28.37005615 28.37005615 28.37005615 28.37005615 28.37005615 28.37005615
 28.37005615 28.37005615 28.37005615 28.37005615]
ene_coms = [0.01518971 0.01624214 0.00712644 0.00630816 0.00722403 0.00652291
 0.00630913 0.00688688 0.00653305 0.0382898 ]
ene_comp = [1.62171376e-05 1.58229548e-04 1.10949680e-05 4.48645421e-07
 1.67352729e-04 1.77028668e-05 8.88576573e-07 3.30739830e-05
 1.25020180e-05 5.70022245e-05]
ene_total = [0.72012272 0.77668888 0.33801948 0.29876331 0.35004117 0.30975048
 0.29883004 0.32771512 0.30998446 1.81602857]
optimize_network iter = 1 obj = 5.545944219200192
eta = 0.6363628357610001
freqs = [41704327.15544253 89825604.29284693 34772144.48931189 11873990.11533099
 85967634.45465189 40478523.5519902  14911792.11629514 49968466.91888497
 36049369.66732018 80794944.968495  ]
Done!
ene_coms = [0.01518971 0.01624214 0.00712644 0.00630816 0.00722403 0.00652291
 0.00630913 0.00688688 0.00653305 0.0382898 ]
ene_comp = [1.53401930e-05 1.49673257e-04 1.04950056e-05 4.24384840e-07
 1.58303100e-04 1.67455811e-05 8.40526637e-07 3.12855015e-05
 1.18259692e-05 5.39198191e-05]
ene_total = [0.01520505 0.01639181 0.00713693 0.00630859 0.00738233 0.00653966
 0.00630997 0.00691816 0.00654488 0.03834372]
At round 32 energy consumption: 0.11708110164696114
At round 32 eta: 0.6363628357610001
At round 32 a_n: 17.221135753236222
At round 32 local rounds: 14.80033050744229
At round 32 global rounds: 47.35801905527362
gradient difference: 0.4021991789340973
train() client id: f_00000-0-0 loss: 1.348466  [   32/  126]
train() client id: f_00000-0-1 loss: 1.285663  [   64/  126]
train() client id: f_00000-0-2 loss: 1.282553  [   96/  126]
train() client id: f_00000-1-0 loss: 1.304301  [   32/  126]
train() client id: f_00000-1-1 loss: 1.244632  [   64/  126]
train() client id: f_00000-1-2 loss: 1.097796  [   96/  126]
train() client id: f_00000-2-0 loss: 1.206911  [   32/  126]
train() client id: f_00000-2-1 loss: 1.089519  [   64/  126]
train() client id: f_00000-2-2 loss: 1.178556  [   96/  126]
train() client id: f_00000-3-0 loss: 1.241585  [   32/  126]
train() client id: f_00000-3-1 loss: 1.064182  [   64/  126]
train() client id: f_00000-3-2 loss: 0.922538  [   96/  126]
train() client id: f_00000-4-0 loss: 0.925502  [   32/  126]
train() client id: f_00000-4-1 loss: 1.114746  [   64/  126]
train() client id: f_00000-4-2 loss: 0.934241  [   96/  126]
train() client id: f_00000-5-0 loss: 1.005338  [   32/  126]
train() client id: f_00000-5-1 loss: 0.890140  [   64/  126]
train() client id: f_00000-5-2 loss: 0.917464  [   96/  126]
train() client id: f_00000-6-0 loss: 0.965995  [   32/  126]
train() client id: f_00000-6-1 loss: 0.826342  [   64/  126]
train() client id: f_00000-6-2 loss: 0.913129  [   96/  126]
train() client id: f_00000-7-0 loss: 0.884977  [   32/  126]
train() client id: f_00000-7-1 loss: 0.922210  [   64/  126]
train() client id: f_00000-7-2 loss: 0.802148  [   96/  126]
train() client id: f_00000-8-0 loss: 0.808239  [   32/  126]
train() client id: f_00000-8-1 loss: 0.806122  [   64/  126]
train() client id: f_00000-8-2 loss: 0.853547  [   96/  126]
train() client id: f_00000-9-0 loss: 0.843858  [   32/  126]
train() client id: f_00000-9-1 loss: 0.731027  [   64/  126]
train() client id: f_00000-9-2 loss: 0.932061  [   96/  126]
train() client id: f_00000-10-0 loss: 0.741177  [   32/  126]
train() client id: f_00000-10-1 loss: 0.859078  [   64/  126]
train() client id: f_00000-10-2 loss: 0.832033  [   96/  126]
train() client id: f_00000-11-0 loss: 0.836798  [   32/  126]
train() client id: f_00000-11-1 loss: 0.842412  [   64/  126]
train() client id: f_00000-11-2 loss: 0.743291  [   96/  126]
train() client id: f_00000-12-0 loss: 0.872103  [   32/  126]
train() client id: f_00000-12-1 loss: 0.753160  [   64/  126]
train() client id: f_00000-12-2 loss: 0.866457  [   96/  126]
train() client id: f_00000-13-0 loss: 0.766360  [   32/  126]
train() client id: f_00000-13-1 loss: 0.743625  [   64/  126]
train() client id: f_00000-13-2 loss: 0.904003  [   96/  126]
train() client id: f_00001-0-0 loss: 0.356192  [   32/  265]
train() client id: f_00001-0-1 loss: 0.565816  [   64/  265]
train() client id: f_00001-0-2 loss: 0.345141  [   96/  265]
train() client id: f_00001-0-3 loss: 0.488290  [  128/  265]
train() client id: f_00001-0-4 loss: 0.533267  [  160/  265]
train() client id: f_00001-0-5 loss: 0.365069  [  192/  265]
train() client id: f_00001-0-6 loss: 0.350025  [  224/  265]
train() client id: f_00001-0-7 loss: 0.406626  [  256/  265]
train() client id: f_00001-1-0 loss: 0.387093  [   32/  265]
train() client id: f_00001-1-1 loss: 0.395726  [   64/  265]
train() client id: f_00001-1-2 loss: 0.413456  [   96/  265]
train() client id: f_00001-1-3 loss: 0.424428  [  128/  265]
train() client id: f_00001-1-4 loss: 0.497236  [  160/  265]
train() client id: f_00001-1-5 loss: 0.534305  [  192/  265]
train() client id: f_00001-1-6 loss: 0.334602  [  224/  265]
train() client id: f_00001-1-7 loss: 0.380719  [  256/  265]
train() client id: f_00001-2-0 loss: 0.397693  [   32/  265]
train() client id: f_00001-2-1 loss: 0.398812  [   64/  265]
train() client id: f_00001-2-2 loss: 0.533186  [   96/  265]
train() client id: f_00001-2-3 loss: 0.323391  [  128/  265]
train() client id: f_00001-2-4 loss: 0.368225  [  160/  265]
train() client id: f_00001-2-5 loss: 0.546933  [  192/  265]
train() client id: f_00001-2-6 loss: 0.307538  [  224/  265]
train() client id: f_00001-2-7 loss: 0.434845  [  256/  265]
train() client id: f_00001-3-0 loss: 0.411058  [   32/  265]
train() client id: f_00001-3-1 loss: 0.439151  [   64/  265]
train() client id: f_00001-3-2 loss: 0.464362  [   96/  265]
train() client id: f_00001-3-3 loss: 0.323565  [  128/  265]
train() client id: f_00001-3-4 loss: 0.398311  [  160/  265]
train() client id: f_00001-3-5 loss: 0.377318  [  192/  265]
train() client id: f_00001-3-6 loss: 0.464919  [  224/  265]
train() client id: f_00001-3-7 loss: 0.404602  [  256/  265]
train() client id: f_00001-4-0 loss: 0.338746  [   32/  265]
train() client id: f_00001-4-1 loss: 0.386312  [   64/  265]
train() client id: f_00001-4-2 loss: 0.487744  [   96/  265]
train() client id: f_00001-4-3 loss: 0.365182  [  128/  265]
train() client id: f_00001-4-4 loss: 0.554698  [  160/  265]
train() client id: f_00001-4-5 loss: 0.318070  [  192/  265]
train() client id: f_00001-4-6 loss: 0.308869  [  224/  265]
train() client id: f_00001-4-7 loss: 0.486959  [  256/  265]
train() client id: f_00001-5-0 loss: 0.426507  [   32/  265]
train() client id: f_00001-5-1 loss: 0.466714  [   64/  265]
train() client id: f_00001-5-2 loss: 0.377141  [   96/  265]
train() client id: f_00001-5-3 loss: 0.376682  [  128/  265]
train() client id: f_00001-5-4 loss: 0.346969  [  160/  265]
train() client id: f_00001-5-5 loss: 0.304755  [  192/  265]
train() client id: f_00001-5-6 loss: 0.397649  [  224/  265]
train() client id: f_00001-5-7 loss: 0.530791  [  256/  265]
train() client id: f_00001-6-0 loss: 0.308228  [   32/  265]
train() client id: f_00001-6-1 loss: 0.316070  [   64/  265]
train() client id: f_00001-6-2 loss: 0.638321  [   96/  265]
train() client id: f_00001-6-3 loss: 0.504532  [  128/  265]
train() client id: f_00001-6-4 loss: 0.370693  [  160/  265]
train() client id: f_00001-6-5 loss: 0.363931  [  192/  265]
train() client id: f_00001-6-6 loss: 0.360314  [  224/  265]
train() client id: f_00001-6-7 loss: 0.348724  [  256/  265]
train() client id: f_00001-7-0 loss: 0.352350  [   32/  265]
train() client id: f_00001-7-1 loss: 0.338621  [   64/  265]
train() client id: f_00001-7-2 loss: 0.364983  [   96/  265]
train() client id: f_00001-7-3 loss: 0.369088  [  128/  265]
train() client id: f_00001-7-4 loss: 0.516800  [  160/  265]
train() client id: f_00001-7-5 loss: 0.439227  [  192/  265]
train() client id: f_00001-7-6 loss: 0.427535  [  224/  265]
train() client id: f_00001-7-7 loss: 0.378646  [  256/  265]
train() client id: f_00001-8-0 loss: 0.298153  [   32/  265]
train() client id: f_00001-8-1 loss: 0.401226  [   64/  265]
train() client id: f_00001-8-2 loss: 0.557137  [   96/  265]
train() client id: f_00001-8-3 loss: 0.317599  [  128/  265]
train() client id: f_00001-8-4 loss: 0.406201  [  160/  265]
train() client id: f_00001-8-5 loss: 0.374012  [  192/  265]
train() client id: f_00001-8-6 loss: 0.471944  [  224/  265]
train() client id: f_00001-8-7 loss: 0.349757  [  256/  265]
train() client id: f_00001-9-0 loss: 0.295081  [   32/  265]
train() client id: f_00001-9-1 loss: 0.488314  [   64/  265]
train() client id: f_00001-9-2 loss: 0.448449  [   96/  265]
train() client id: f_00001-9-3 loss: 0.369277  [  128/  265]
train() client id: f_00001-9-4 loss: 0.388480  [  160/  265]
train() client id: f_00001-9-5 loss: 0.290654  [  192/  265]
train() client id: f_00001-9-6 loss: 0.451892  [  224/  265]
train() client id: f_00001-9-7 loss: 0.406446  [  256/  265]
train() client id: f_00001-10-0 loss: 0.379615  [   32/  265]
train() client id: f_00001-10-1 loss: 0.458903  [   64/  265]
train() client id: f_00001-10-2 loss: 0.453653  [   96/  265]
train() client id: f_00001-10-3 loss: 0.290112  [  128/  265]
train() client id: f_00001-10-4 loss: 0.371255  [  160/  265]
train() client id: f_00001-10-5 loss: 0.394410  [  192/  265]
train() client id: f_00001-10-6 loss: 0.332227  [  224/  265]
train() client id: f_00001-10-7 loss: 0.391247  [  256/  265]
train() client id: f_00001-11-0 loss: 0.300637  [   32/  265]
train() client id: f_00001-11-1 loss: 0.397286  [   64/  265]
train() client id: f_00001-11-2 loss: 0.418143  [   96/  265]
train() client id: f_00001-11-3 loss: 0.453306  [  128/  265]
train() client id: f_00001-11-4 loss: 0.318983  [  160/  265]
train() client id: f_00001-11-5 loss: 0.463846  [  192/  265]
train() client id: f_00001-11-6 loss: 0.318200  [  224/  265]
train() client id: f_00001-11-7 loss: 0.500430  [  256/  265]
train() client id: f_00001-12-0 loss: 0.393904  [   32/  265]
train() client id: f_00001-12-1 loss: 0.406609  [   64/  265]
train() client id: f_00001-12-2 loss: 0.290872  [   96/  265]
train() client id: f_00001-12-3 loss: 0.369653  [  128/  265]
train() client id: f_00001-12-4 loss: 0.452817  [  160/  265]
train() client id: f_00001-12-5 loss: 0.499155  [  192/  265]
train() client id: f_00001-12-6 loss: 0.386366  [  224/  265]
train() client id: f_00001-12-7 loss: 0.299642  [  256/  265]
train() client id: f_00001-13-0 loss: 0.380154  [   32/  265]
train() client id: f_00001-13-1 loss: 0.421083  [   64/  265]
train() client id: f_00001-13-2 loss: 0.404585  [   96/  265]
train() client id: f_00001-13-3 loss: 0.493500  [  128/  265]
train() client id: f_00001-13-4 loss: 0.347584  [  160/  265]
train() client id: f_00001-13-5 loss: 0.349885  [  192/  265]
train() client id: f_00001-13-6 loss: 0.438221  [  224/  265]
train() client id: f_00001-13-7 loss: 0.331735  [  256/  265]
train() client id: f_00002-0-0 loss: 1.284338  [   32/  124]
train() client id: f_00002-0-1 loss: 1.417296  [   64/  124]
train() client id: f_00002-0-2 loss: 1.252899  [   96/  124]
train() client id: f_00002-1-0 loss: 1.172873  [   32/  124]
train() client id: f_00002-1-1 loss: 1.202427  [   64/  124]
train() client id: f_00002-1-2 loss: 1.239107  [   96/  124]
train() client id: f_00002-2-0 loss: 1.153225  [   32/  124]
train() client id: f_00002-2-1 loss: 1.060161  [   64/  124]
train() client id: f_00002-2-2 loss: 1.341213  [   96/  124]
train() client id: f_00002-3-0 loss: 1.270352  [   32/  124]
train() client id: f_00002-3-1 loss: 1.169288  [   64/  124]
train() client id: f_00002-3-2 loss: 0.980847  [   96/  124]
train() client id: f_00002-4-0 loss: 1.235847  [   32/  124]
train() client id: f_00002-4-1 loss: 1.075800  [   64/  124]
train() client id: f_00002-4-2 loss: 1.176589  [   96/  124]
train() client id: f_00002-5-0 loss: 1.137273  [   32/  124]
train() client id: f_00002-5-1 loss: 0.942053  [   64/  124]
train() client id: f_00002-5-2 loss: 1.076322  [   96/  124]
train() client id: f_00002-6-0 loss: 0.943504  [   32/  124]
train() client id: f_00002-6-1 loss: 1.235982  [   64/  124]
train() client id: f_00002-6-2 loss: 1.100182  [   96/  124]
train() client id: f_00002-7-0 loss: 0.972982  [   32/  124]
train() client id: f_00002-7-1 loss: 1.210868  [   64/  124]
train() client id: f_00002-7-2 loss: 1.068870  [   96/  124]
train() client id: f_00002-8-0 loss: 1.107616  [   32/  124]
train() client id: f_00002-8-1 loss: 1.020052  [   64/  124]
train() client id: f_00002-8-2 loss: 1.178612  [   96/  124]
train() client id: f_00002-9-0 loss: 1.044170  [   32/  124]
train() client id: f_00002-9-1 loss: 1.017064  [   64/  124]
train() client id: f_00002-9-2 loss: 1.042850  [   96/  124]
train() client id: f_00002-10-0 loss: 0.984053  [   32/  124]
train() client id: f_00002-10-1 loss: 0.885469  [   64/  124]
train() client id: f_00002-10-2 loss: 1.300879  [   96/  124]
train() client id: f_00002-11-0 loss: 1.144184  [   32/  124]
train() client id: f_00002-11-1 loss: 1.048463  [   64/  124]
train() client id: f_00002-11-2 loss: 0.939032  [   96/  124]
train() client id: f_00002-12-0 loss: 0.977170  [   32/  124]
train() client id: f_00002-12-1 loss: 1.042652  [   64/  124]
train() client id: f_00002-12-2 loss: 1.058729  [   96/  124]
train() client id: f_00002-13-0 loss: 0.972756  [   32/  124]
train() client id: f_00002-13-1 loss: 1.140356  [   64/  124]
train() client id: f_00002-13-2 loss: 0.999744  [   96/  124]
train() client id: f_00003-0-0 loss: 0.801740  [   32/   43]
train() client id: f_00003-1-0 loss: 0.648152  [   32/   43]
train() client id: f_00003-2-0 loss: 0.855548  [   32/   43]
train() client id: f_00003-3-0 loss: 0.707619  [   32/   43]
train() client id: f_00003-4-0 loss: 0.590103  [   32/   43]
train() client id: f_00003-5-0 loss: 0.677772  [   32/   43]
train() client id: f_00003-6-0 loss: 0.640221  [   32/   43]
train() client id: f_00003-7-0 loss: 0.749260  [   32/   43]
train() client id: f_00003-8-0 loss: 0.694448  [   32/   43]
train() client id: f_00003-9-0 loss: 0.765597  [   32/   43]
train() client id: f_00003-10-0 loss: 0.637232  [   32/   43]
train() client id: f_00003-11-0 loss: 0.628454  [   32/   43]
train() client id: f_00003-12-0 loss: 0.643303  [   32/   43]
train() client id: f_00003-13-0 loss: 0.535494  [   32/   43]
train() client id: f_00004-0-0 loss: 0.860751  [   32/  306]
train() client id: f_00004-0-1 loss: 0.895085  [   64/  306]
train() client id: f_00004-0-2 loss: 0.718987  [   96/  306]
train() client id: f_00004-0-3 loss: 0.832269  [  128/  306]
train() client id: f_00004-0-4 loss: 0.823864  [  160/  306]
train() client id: f_00004-0-5 loss: 0.965573  [  192/  306]
train() client id: f_00004-0-6 loss: 0.743154  [  224/  306]
train() client id: f_00004-0-7 loss: 0.813432  [  256/  306]
train() client id: f_00004-0-8 loss: 0.899036  [  288/  306]
train() client id: f_00004-1-0 loss: 0.865372  [   32/  306]
train() client id: f_00004-1-1 loss: 0.855021  [   64/  306]
train() client id: f_00004-1-2 loss: 0.914909  [   96/  306]
train() client id: f_00004-1-3 loss: 0.839473  [  128/  306]
train() client id: f_00004-1-4 loss: 0.797415  [  160/  306]
train() client id: f_00004-1-5 loss: 0.785793  [  192/  306]
train() client id: f_00004-1-6 loss: 0.799979  [  224/  306]
train() client id: f_00004-1-7 loss: 0.862052  [  256/  306]
train() client id: f_00004-1-8 loss: 0.833053  [  288/  306]
train() client id: f_00004-2-0 loss: 0.733869  [   32/  306]
train() client id: f_00004-2-1 loss: 0.874695  [   64/  306]
train() client id: f_00004-2-2 loss: 1.011166  [   96/  306]
train() client id: f_00004-2-3 loss: 0.748285  [  128/  306]
train() client id: f_00004-2-4 loss: 0.757168  [  160/  306]
train() client id: f_00004-2-5 loss: 0.743598  [  192/  306]
train() client id: f_00004-2-6 loss: 0.805372  [  224/  306]
train() client id: f_00004-2-7 loss: 1.040475  [  256/  306]
train() client id: f_00004-2-8 loss: 0.832220  [  288/  306]
train() client id: f_00004-3-0 loss: 0.738918  [   32/  306]
train() client id: f_00004-3-1 loss: 0.871134  [   64/  306]
train() client id: f_00004-3-2 loss: 0.852638  [   96/  306]
train() client id: f_00004-3-3 loss: 0.752810  [  128/  306]
train() client id: f_00004-3-4 loss: 0.923962  [  160/  306]
train() client id: f_00004-3-5 loss: 0.882861  [  192/  306]
train() client id: f_00004-3-6 loss: 0.764533  [  224/  306]
train() client id: f_00004-3-7 loss: 0.943481  [  256/  306]
train() client id: f_00004-3-8 loss: 0.817264  [  288/  306]
train() client id: f_00004-4-0 loss: 0.762796  [   32/  306]
train() client id: f_00004-4-1 loss: 0.823050  [   64/  306]
train() client id: f_00004-4-2 loss: 0.811879  [   96/  306]
train() client id: f_00004-4-3 loss: 0.886344  [  128/  306]
train() client id: f_00004-4-4 loss: 0.820909  [  160/  306]
train() client id: f_00004-4-5 loss: 0.941732  [  192/  306]
train() client id: f_00004-4-6 loss: 0.800297  [  224/  306]
train() client id: f_00004-4-7 loss: 0.920340  [  256/  306]
train() client id: f_00004-4-8 loss: 0.687791  [  288/  306]
train() client id: f_00004-5-0 loss: 0.858907  [   32/  306]
train() client id: f_00004-5-1 loss: 0.940410  [   64/  306]
train() client id: f_00004-5-2 loss: 0.857633  [   96/  306]
train() client id: f_00004-5-3 loss: 0.850347  [  128/  306]
train() client id: f_00004-5-4 loss: 0.801125  [  160/  306]
train() client id: f_00004-5-5 loss: 0.752740  [  192/  306]
train() client id: f_00004-5-6 loss: 0.717349  [  224/  306]
train() client id: f_00004-5-7 loss: 0.817883  [  256/  306]
train() client id: f_00004-5-8 loss: 0.820852  [  288/  306]
train() client id: f_00004-6-0 loss: 0.690431  [   32/  306]
train() client id: f_00004-6-1 loss: 0.696466  [   64/  306]
train() client id: f_00004-6-2 loss: 0.934167  [   96/  306]
train() client id: f_00004-6-3 loss: 0.794081  [  128/  306]
train() client id: f_00004-6-4 loss: 0.874485  [  160/  306]
train() client id: f_00004-6-5 loss: 0.909981  [  192/  306]
train() client id: f_00004-6-6 loss: 0.912114  [  224/  306]
train() client id: f_00004-6-7 loss: 0.874613  [  256/  306]
train() client id: f_00004-6-8 loss: 0.807656  [  288/  306]
train() client id: f_00004-7-0 loss: 0.871257  [   32/  306]
train() client id: f_00004-7-1 loss: 0.847891  [   64/  306]
train() client id: f_00004-7-2 loss: 0.695120  [   96/  306]
train() client id: f_00004-7-3 loss: 0.885499  [  128/  306]
train() client id: f_00004-7-4 loss: 0.795140  [  160/  306]
train() client id: f_00004-7-5 loss: 0.743245  [  192/  306]
train() client id: f_00004-7-6 loss: 0.998156  [  224/  306]
train() client id: f_00004-7-7 loss: 0.801286  [  256/  306]
train() client id: f_00004-7-8 loss: 0.875803  [  288/  306]
train() client id: f_00004-8-0 loss: 0.780983  [   32/  306]
train() client id: f_00004-8-1 loss: 0.740228  [   64/  306]
train() client id: f_00004-8-2 loss: 0.816192  [   96/  306]
train() client id: f_00004-8-3 loss: 0.910684  [  128/  306]
train() client id: f_00004-8-4 loss: 0.872186  [  160/  306]
train() client id: f_00004-8-5 loss: 0.789658  [  192/  306]
train() client id: f_00004-8-6 loss: 0.848438  [  224/  306]
train() client id: f_00004-8-7 loss: 0.941455  [  256/  306]
train() client id: f_00004-8-8 loss: 0.808488  [  288/  306]
train() client id: f_00004-9-0 loss: 0.805059  [   32/  306]
train() client id: f_00004-9-1 loss: 0.832839  [   64/  306]
train() client id: f_00004-9-2 loss: 0.833715  [   96/  306]
train() client id: f_00004-9-3 loss: 0.879671  [  128/  306]
train() client id: f_00004-9-4 loss: 0.746043  [  160/  306]
train() client id: f_00004-9-5 loss: 0.880080  [  192/  306]
train() client id: f_00004-9-6 loss: 0.803184  [  224/  306]
train() client id: f_00004-9-7 loss: 0.850259  [  256/  306]
train() client id: f_00004-9-8 loss: 0.829188  [  288/  306]
train() client id: f_00004-10-0 loss: 0.833878  [   32/  306]
train() client id: f_00004-10-1 loss: 0.878721  [   64/  306]
train() client id: f_00004-10-2 loss: 0.831736  [   96/  306]
train() client id: f_00004-10-3 loss: 0.764903  [  128/  306]
train() client id: f_00004-10-4 loss: 0.825647  [  160/  306]
train() client id: f_00004-10-5 loss: 0.671257  [  192/  306]
train() client id: f_00004-10-6 loss: 1.022627  [  224/  306]
train() client id: f_00004-10-7 loss: 0.814663  [  256/  306]
train() client id: f_00004-10-8 loss: 0.848530  [  288/  306]
train() client id: f_00004-11-0 loss: 0.881681  [   32/  306]
train() client id: f_00004-11-1 loss: 0.889906  [   64/  306]
train() client id: f_00004-11-2 loss: 0.882074  [   96/  306]
train() client id: f_00004-11-3 loss: 0.848581  [  128/  306]
train() client id: f_00004-11-4 loss: 0.852945  [  160/  306]
train() client id: f_00004-11-5 loss: 0.777849  [  192/  306]
train() client id: f_00004-11-6 loss: 0.800212  [  224/  306]
train() client id: f_00004-11-7 loss: 0.714343  [  256/  306]
train() client id: f_00004-11-8 loss: 0.808510  [  288/  306]
train() client id: f_00004-12-0 loss: 0.734874  [   32/  306]
train() client id: f_00004-12-1 loss: 0.826585  [   64/  306]
train() client id: f_00004-12-2 loss: 0.797475  [   96/  306]
train() client id: f_00004-12-3 loss: 0.914747  [  128/  306]
train() client id: f_00004-12-4 loss: 0.786463  [  160/  306]
train() client id: f_00004-12-5 loss: 0.936398  [  192/  306]
train() client id: f_00004-12-6 loss: 0.903649  [  224/  306]
train() client id: f_00004-12-7 loss: 0.868739  [  256/  306]
train() client id: f_00004-12-8 loss: 0.788960  [  288/  306]
train() client id: f_00004-13-0 loss: 0.886337  [   32/  306]
train() client id: f_00004-13-1 loss: 0.748872  [   64/  306]
train() client id: f_00004-13-2 loss: 0.870575  [   96/  306]
train() client id: f_00004-13-3 loss: 0.830732  [  128/  306]
train() client id: f_00004-13-4 loss: 0.770553  [  160/  306]
train() client id: f_00004-13-5 loss: 0.960248  [  192/  306]
train() client id: f_00004-13-6 loss: 0.781352  [  224/  306]
train() client id: f_00004-13-7 loss: 0.831772  [  256/  306]
train() client id: f_00004-13-8 loss: 0.780099  [  288/  306]
train() client id: f_00005-0-0 loss: 0.357539  [   32/  146]
train() client id: f_00005-0-1 loss: 0.204385  [   64/  146]
train() client id: f_00005-0-2 loss: 0.372105  [   96/  146]
train() client id: f_00005-0-3 loss: 0.300654  [  128/  146]
train() client id: f_00005-1-0 loss: 0.397538  [   32/  146]
train() client id: f_00005-1-1 loss: 0.343689  [   64/  146]
train() client id: f_00005-1-2 loss: 0.201089  [   96/  146]
train() client id: f_00005-1-3 loss: 0.215073  [  128/  146]
train() client id: f_00005-2-0 loss: 0.243987  [   32/  146]
train() client id: f_00005-2-1 loss: 0.070188  [   64/  146]
train() client id: f_00005-2-2 loss: 0.180546  [   96/  146]
train() client id: f_00005-2-3 loss: 0.567852  [  128/  146]
train() client id: f_00005-3-0 loss: 0.167925  [   32/  146]
train() client id: f_00005-3-1 loss: 0.417933  [   64/  146]
train() client id: f_00005-3-2 loss: 0.127501  [   96/  146]
train() client id: f_00005-3-3 loss: 0.086004  [  128/  146]
train() client id: f_00005-4-0 loss: 0.188161  [   32/  146]
train() client id: f_00005-4-1 loss: 0.276384  [   64/  146]
train() client id: f_00005-4-2 loss: 0.067283  [   96/  146]
train() client id: f_00005-4-3 loss: 0.481152  [  128/  146]
train() client id: f_00005-5-0 loss: 0.305020  [   32/  146]
train() client id: f_00005-5-1 loss: 0.452265  [   64/  146]
train() client id: f_00005-5-2 loss: 0.091542  [   96/  146]
train() client id: f_00005-5-3 loss: 0.127878  [  128/  146]
train() client id: f_00005-6-0 loss: 0.213132  [   32/  146]
train() client id: f_00005-6-1 loss: 0.295239  [   64/  146]
train() client id: f_00005-6-2 loss: 0.552765  [   96/  146]
train() client id: f_00005-6-3 loss: 0.053997  [  128/  146]
train() client id: f_00005-7-0 loss: 0.179206  [   32/  146]
train() client id: f_00005-7-1 loss: 0.496234  [   64/  146]
train() client id: f_00005-7-2 loss: 0.213326  [   96/  146]
train() client id: f_00005-7-3 loss: 0.320096  [  128/  146]
train() client id: f_00005-8-0 loss: 0.408096  [   32/  146]
train() client id: f_00005-8-1 loss: 0.176675  [   64/  146]
train() client id: f_00005-8-2 loss: 0.138846  [   96/  146]
train() client id: f_00005-8-3 loss: 0.296531  [  128/  146]
train() client id: f_00005-9-0 loss: 0.090498  [   32/  146]
train() client id: f_00005-9-1 loss: 0.461663  [   64/  146]
train() client id: f_00005-9-2 loss: 0.397582  [   96/  146]
train() client id: f_00005-9-3 loss: 0.136488  [  128/  146]
train() client id: f_00005-10-0 loss: 0.352772  [   32/  146]
train() client id: f_00005-10-1 loss: 0.279046  [   64/  146]
train() client id: f_00005-10-2 loss: 0.166306  [   96/  146]
train() client id: f_00005-10-3 loss: 0.276801  [  128/  146]
train() client id: f_00005-11-0 loss: 0.203502  [   32/  146]
train() client id: f_00005-11-1 loss: 0.257316  [   64/  146]
train() client id: f_00005-11-2 loss: 0.205138  [   96/  146]
train() client id: f_00005-11-3 loss: 0.513406  [  128/  146]
train() client id: f_00005-12-0 loss: 0.136080  [   32/  146]
train() client id: f_00005-12-1 loss: 0.107187  [   64/  146]
train() client id: f_00005-12-2 loss: 0.237564  [   96/  146]
train() client id: f_00005-12-3 loss: 0.044322  [  128/  146]
train() client id: f_00005-13-0 loss: 0.105047  [   32/  146]
train() client id: f_00005-13-1 loss: 0.388940  [   64/  146]
train() client id: f_00005-13-2 loss: 0.374644  [   96/  146]
train() client id: f_00005-13-3 loss: 0.182387  [  128/  146]
train() client id: f_00006-0-0 loss: 0.528139  [   32/   54]
train() client id: f_00006-1-0 loss: 0.530864  [   32/   54]
train() client id: f_00006-2-0 loss: 0.523191  [   32/   54]
train() client id: f_00006-3-0 loss: 0.557228  [   32/   54]
train() client id: f_00006-4-0 loss: 0.506172  [   32/   54]
train() client id: f_00006-5-0 loss: 0.523419  [   32/   54]
train() client id: f_00006-6-0 loss: 0.471036  [   32/   54]
train() client id: f_00006-7-0 loss: 0.531660  [   32/   54]
train() client id: f_00006-8-0 loss: 0.527860  [   32/   54]
train() client id: f_00006-9-0 loss: 0.474285  [   32/   54]
train() client id: f_00006-10-0 loss: 0.532279  [   32/   54]
train() client id: f_00006-11-0 loss: 0.514031  [   32/   54]
train() client id: f_00006-12-0 loss: 0.585692  [   32/   54]
train() client id: f_00006-13-0 loss: 0.479713  [   32/   54]
train() client id: f_00007-0-0 loss: 0.666176  [   32/  179]
train() client id: f_00007-0-1 loss: 0.502911  [   64/  179]
train() client id: f_00007-0-2 loss: 0.521377  [   96/  179]
train() client id: f_00007-0-3 loss: 0.486217  [  128/  179]
train() client id: f_00007-0-4 loss: 0.627246  [  160/  179]
train() client id: f_00007-1-0 loss: 0.545558  [   32/  179]
train() client id: f_00007-1-1 loss: 0.655845  [   64/  179]
train() client id: f_00007-1-2 loss: 0.709865  [   96/  179]
train() client id: f_00007-1-3 loss: 0.489581  [  128/  179]
train() client id: f_00007-1-4 loss: 0.466808  [  160/  179]
train() client id: f_00007-2-0 loss: 0.693466  [   32/  179]
train() client id: f_00007-2-1 loss: 0.486458  [   64/  179]
train() client id: f_00007-2-2 loss: 0.421459  [   96/  179]
train() client id: f_00007-2-3 loss: 0.406832  [  128/  179]
train() client id: f_00007-2-4 loss: 0.593762  [  160/  179]
train() client id: f_00007-3-0 loss: 0.544166  [   32/  179]
train() client id: f_00007-3-1 loss: 0.655718  [   64/  179]
train() client id: f_00007-3-2 loss: 0.393578  [   96/  179]
train() client id: f_00007-3-3 loss: 0.513956  [  128/  179]
train() client id: f_00007-3-4 loss: 0.470227  [  160/  179]
train() client id: f_00007-4-0 loss: 0.410415  [   32/  179]
train() client id: f_00007-4-1 loss: 0.496100  [   64/  179]
train() client id: f_00007-4-2 loss: 0.489104  [   96/  179]
train() client id: f_00007-4-3 loss: 0.539925  [  128/  179]
train() client id: f_00007-4-4 loss: 0.777681  [  160/  179]
train() client id: f_00007-5-0 loss: 0.541456  [   32/  179]
train() client id: f_00007-5-1 loss: 0.452348  [   64/  179]
train() client id: f_00007-5-2 loss: 0.422160  [   96/  179]
train() client id: f_00007-5-3 loss: 0.500396  [  128/  179]
train() client id: f_00007-5-4 loss: 0.608373  [  160/  179]
train() client id: f_00007-6-0 loss: 0.506354  [   32/  179]
train() client id: f_00007-6-1 loss: 0.426445  [   64/  179]
train() client id: f_00007-6-2 loss: 0.716081  [   96/  179]
train() client id: f_00007-6-3 loss: 0.454278  [  128/  179]
train() client id: f_00007-6-4 loss: 0.571649  [  160/  179]
train() client id: f_00007-7-0 loss: 0.553472  [   32/  179]
train() client id: f_00007-7-1 loss: 0.614947  [   64/  179]
train() client id: f_00007-7-2 loss: 0.529298  [   96/  179]
train() client id: f_00007-7-3 loss: 0.521706  [  128/  179]
train() client id: f_00007-7-4 loss: 0.426412  [  160/  179]
train() client id: f_00007-8-0 loss: 0.376553  [   32/  179]
train() client id: f_00007-8-1 loss: 0.543193  [   64/  179]
train() client id: f_00007-8-2 loss: 0.491522  [   96/  179]
train() client id: f_00007-8-3 loss: 0.557306  [  128/  179]
train() client id: f_00007-8-4 loss: 0.601510  [  160/  179]
train() client id: f_00007-9-0 loss: 0.530047  [   32/  179]
train() client id: f_00007-9-1 loss: 0.529065  [   64/  179]
train() client id: f_00007-9-2 loss: 0.472443  [   96/  179]
train() client id: f_00007-9-3 loss: 0.511624  [  128/  179]
train() client id: f_00007-9-4 loss: 0.491059  [  160/  179]
train() client id: f_00007-10-0 loss: 0.373050  [   32/  179]
train() client id: f_00007-10-1 loss: 0.436285  [   64/  179]
train() client id: f_00007-10-2 loss: 0.651659  [   96/  179]
train() client id: f_00007-10-3 loss: 0.599494  [  128/  179]
train() client id: f_00007-10-4 loss: 0.549509  [  160/  179]
train() client id: f_00007-11-0 loss: 0.437619  [   32/  179]
train() client id: f_00007-11-1 loss: 0.702323  [   64/  179]
train() client id: f_00007-11-2 loss: 0.650568  [   96/  179]
train() client id: f_00007-11-3 loss: 0.466633  [  128/  179]
train() client id: f_00007-11-4 loss: 0.356597  [  160/  179]
train() client id: f_00007-12-0 loss: 0.489548  [   32/  179]
train() client id: f_00007-12-1 loss: 0.592798  [   64/  179]
train() client id: f_00007-12-2 loss: 0.565886  [   96/  179]
train() client id: f_00007-12-3 loss: 0.386271  [  128/  179]
train() client id: f_00007-12-4 loss: 0.511125  [  160/  179]
train() client id: f_00007-13-0 loss: 0.632509  [   32/  179]
train() client id: f_00007-13-1 loss: 0.578912  [   64/  179]
train() client id: f_00007-13-2 loss: 0.342197  [   96/  179]
train() client id: f_00007-13-3 loss: 0.359459  [  128/  179]
train() client id: f_00007-13-4 loss: 0.672297  [  160/  179]
train() client id: f_00008-0-0 loss: 0.733808  [   32/  130]
train() client id: f_00008-0-1 loss: 0.761662  [   64/  130]
train() client id: f_00008-0-2 loss: 0.638724  [   96/  130]
train() client id: f_00008-0-3 loss: 0.659967  [  128/  130]
train() client id: f_00008-1-0 loss: 0.697525  [   32/  130]
train() client id: f_00008-1-1 loss: 0.720147  [   64/  130]
train() client id: f_00008-1-2 loss: 0.702273  [   96/  130]
train() client id: f_00008-1-3 loss: 0.691174  [  128/  130]
train() client id: f_00008-2-0 loss: 0.613512  [   32/  130]
train() client id: f_00008-2-1 loss: 0.844584  [   64/  130]
train() client id: f_00008-2-2 loss: 0.666023  [   96/  130]
train() client id: f_00008-2-3 loss: 0.741051  [  128/  130]
train() client id: f_00008-3-0 loss: 0.743516  [   32/  130]
train() client id: f_00008-3-1 loss: 0.637122  [   64/  130]
train() client id: f_00008-3-2 loss: 0.660238  [   96/  130]
train() client id: f_00008-3-3 loss: 0.795997  [  128/  130]
train() client id: f_00008-4-0 loss: 0.645855  [   32/  130]
train() client id: f_00008-4-1 loss: 0.750110  [   64/  130]
train() client id: f_00008-4-2 loss: 0.695601  [   96/  130]
train() client id: f_00008-4-3 loss: 0.771171  [  128/  130]
train() client id: f_00008-5-0 loss: 0.780276  [   32/  130]
train() client id: f_00008-5-1 loss: 0.655150  [   64/  130]
train() client id: f_00008-5-2 loss: 0.702302  [   96/  130]
train() client id: f_00008-5-3 loss: 0.721611  [  128/  130]
train() client id: f_00008-6-0 loss: 0.721461  [   32/  130]
train() client id: f_00008-6-1 loss: 0.757761  [   64/  130]
train() client id: f_00008-6-2 loss: 0.679639  [   96/  130]
train() client id: f_00008-6-3 loss: 0.663003  [  128/  130]
train() client id: f_00008-7-0 loss: 0.682152  [   32/  130]
train() client id: f_00008-7-1 loss: 0.606716  [   64/  130]
train() client id: f_00008-7-2 loss: 0.814559  [   96/  130]
train() client id: f_00008-7-3 loss: 0.759926  [  128/  130]
train() client id: f_00008-8-0 loss: 0.733688  [   32/  130]
train() client id: f_00008-8-1 loss: 0.697377  [   64/  130]
train() client id: f_00008-8-2 loss: 0.663549  [   96/  130]
train() client id: f_00008-8-3 loss: 0.738625  [  128/  130]
train() client id: f_00008-9-0 loss: 0.764407  [   32/  130]
train() client id: f_00008-9-1 loss: 0.697552  [   64/  130]
train() client id: f_00008-9-2 loss: 0.733839  [   96/  130]
train() client id: f_00008-9-3 loss: 0.654219  [  128/  130]
train() client id: f_00008-10-0 loss: 0.715788  [   32/  130]
train() client id: f_00008-10-1 loss: 0.723575  [   64/  130]
train() client id: f_00008-10-2 loss: 0.667357  [   96/  130]
train() client id: f_00008-10-3 loss: 0.739249  [  128/  130]
train() client id: f_00008-11-0 loss: 0.802904  [   32/  130]
train() client id: f_00008-11-1 loss: 0.717718  [   64/  130]
train() client id: f_00008-11-2 loss: 0.711851  [   96/  130]
train() client id: f_00008-11-3 loss: 0.607644  [  128/  130]
train() client id: f_00008-12-0 loss: 0.760862  [   32/  130]
train() client id: f_00008-12-1 loss: 0.704499  [   64/  130]
train() client id: f_00008-12-2 loss: 0.657656  [   96/  130]
train() client id: f_00008-12-3 loss: 0.709340  [  128/  130]
train() client id: f_00008-13-0 loss: 0.776936  [   32/  130]
train() client id: f_00008-13-1 loss: 0.726041  [   64/  130]
train() client id: f_00008-13-2 loss: 0.619313  [   96/  130]
train() client id: f_00008-13-3 loss: 0.727020  [  128/  130]
train() client id: f_00009-0-0 loss: 0.930768  [   32/  118]
train() client id: f_00009-0-1 loss: 1.043806  [   64/  118]
train() client id: f_00009-0-2 loss: 0.863104  [   96/  118]
train() client id: f_00009-1-0 loss: 0.876951  [   32/  118]
train() client id: f_00009-1-1 loss: 1.044808  [   64/  118]
train() client id: f_00009-1-2 loss: 1.000679  [   96/  118]
train() client id: f_00009-2-0 loss: 0.871576  [   32/  118]
train() client id: f_00009-2-1 loss: 0.931180  [   64/  118]
train() client id: f_00009-2-2 loss: 0.870930  [   96/  118]
train() client id: f_00009-3-0 loss: 0.860756  [   32/  118]
train() client id: f_00009-3-1 loss: 0.796635  [   64/  118]
train() client id: f_00009-3-2 loss: 0.843210  [   96/  118]
train() client id: f_00009-4-0 loss: 0.841797  [   32/  118]
train() client id: f_00009-4-1 loss: 0.854814  [   64/  118]
train() client id: f_00009-4-2 loss: 0.791516  [   96/  118]
train() client id: f_00009-5-0 loss: 0.740112  [   32/  118]
train() client id: f_00009-5-1 loss: 0.759445  [   64/  118]
train() client id: f_00009-5-2 loss: 0.762735  [   96/  118]
train() client id: f_00009-6-0 loss: 0.778753  [   32/  118]
train() client id: f_00009-6-1 loss: 0.722394  [   64/  118]
train() client id: f_00009-6-2 loss: 0.662041  [   96/  118]
train() client id: f_00009-7-0 loss: 0.656309  [   32/  118]
train() client id: f_00009-7-1 loss: 0.718267  [   64/  118]
train() client id: f_00009-7-2 loss: 0.832270  [   96/  118]
train() client id: f_00009-8-0 loss: 0.771972  [   32/  118]
train() client id: f_00009-8-1 loss: 0.766294  [   64/  118]
train() client id: f_00009-8-2 loss: 0.801703  [   96/  118]
train() client id: f_00009-9-0 loss: 0.780630  [   32/  118]
train() client id: f_00009-9-1 loss: 0.710985  [   64/  118]
train() client id: f_00009-9-2 loss: 0.772812  [   96/  118]
train() client id: f_00009-10-0 loss: 0.900757  [   32/  118]
train() client id: f_00009-10-1 loss: 0.661772  [   64/  118]
train() client id: f_00009-10-2 loss: 0.751159  [   96/  118]
train() client id: f_00009-11-0 loss: 0.711336  [   32/  118]
train() client id: f_00009-11-1 loss: 0.788695  [   64/  118]
train() client id: f_00009-11-2 loss: 0.765203  [   96/  118]
train() client id: f_00009-12-0 loss: 0.691270  [   32/  118]
train() client id: f_00009-12-1 loss: 0.797137  [   64/  118]
train() client id: f_00009-12-2 loss: 0.795318  [   96/  118]
train() client id: f_00009-13-0 loss: 0.810156  [   32/  118]
train() client id: f_00009-13-1 loss: 0.671238  [   64/  118]
train() client id: f_00009-13-2 loss: 0.810579  [   96/  118]
At round 32 accuracy: 0.6392572944297082
At round 32 training accuracy: 0.5861837692823608
At round 32 training loss: 0.8371043882119744
update_location
xs = 8.927491 281.223621 5.882650 0.934260 -197.581990 -45.230757 -5.849135 -5.143845 -220.120581 20.134486 
ys = -272.390647 7.291448 170.684448 -2.290817 -9.642386 0.794442 -1.381692 166.628436 25.881276 -707.232496 
xs mean: -15.68237997052122
ys mean: -62.16579882624052
dists_uav = 290.303918 298.563042 197.908531 100.030599 221.656532 109.756333 100.180444 194.400347 243.152032 714.551048 
uav_gains = -114.900027 -115.610122 -107.550499 -100.003338 -109.104996 -101.010800 -100.019590 -107.330812 -110.725662 -128.306340 
uav_gains_db_mean: -109.45621864668647
dists_bs = 483.722765 486.072166 180.934123 249.769689 186.018163 217.227145 244.381907 170.062331 155.795573 903.555004 
bs_gains = -114.735762 -114.794680 -102.777633 -106.698172 -103.114609 -105.000654 -106.432993 -102.024087 -100.958602 -122.333788 
bs_gains_db_mean: -107.88709786295115
Round 33
-------------------------------
ene_coms = [0.01577767 0.01688013 0.00712223 0.00630329 0.00723616 0.00657827
 0.00630755 0.00687977 0.00656344 0.03867385]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.74777512 13.91379703  6.49736999  2.31847766 15.85755073  7.6193116
  2.88415802  9.32127543  6.79635654  6.72678272]
obj_prev = 78.68285483705375
eta_min = 1.2107415930279335e-14	eta_max = 0.7664355226994126
af = 16.48123926988226	bf = 1.9971145748853085	zeta = 18.129363196870486	eta = 0.9090909090909091
af = 16.48123926988226	bf = 1.9971145748853085	zeta = 39.24738898405475	eta = 0.4199321202380668
af = 16.48123926988226	bf = 1.9971145748853085	zeta = 28.095440990111538	eta = 0.5866161444372057
af = 16.48123926988226	bf = 1.9971145748853085	zeta = 26.096698402125636	eta = 0.6315449953063728
af = 16.48123926988226	bf = 1.9971145748853085	zeta = 25.977833761503863	eta = 0.6344347038784097
af = 16.48123926988226	bf = 1.9971145748853085	zeta = 25.977364677750952	eta = 0.6344461601217802
af = 16.48123926988226	bf = 1.9971145748853085	zeta = 25.97736467040059	eta = 0.6344461603012984
eta = 0.6344461603012984
ene_coms = [0.01577767 0.01688013 0.00712223 0.00630329 0.00723616 0.00657827
 0.00630755 0.00687977 0.00656344 0.03867385]
ene_comp = [0.03754574 0.07896526 0.03694978 0.01281323 0.09118252 0.04350539
 0.01609103 0.0533388  0.03873767 0.03516189]
ene_total = [2.46208343 4.42543573 2.03492144 0.88266048 4.54425156 2.31249511
 1.03420183 2.78045104 2.09167252 3.40919154]
ti_comp = [0.45555434 0.44452972 0.54210877 0.55029813 0.54096943 0.54754836
 0.55025558 0.5445333  0.54769664 0.22659259]
ti_coms = [0.1577767  0.16880133 0.07122227 0.06303291 0.07236161 0.06578268
 0.06307547 0.06879774 0.06563441 0.38673845]
t_total = [28.31912041 28.31912041 28.31912041 28.31912041 28.31912041 28.31912041
 28.31912041 28.31912041 28.31912041 28.31912041]
ene_coms = [0.01577767 0.01688013 0.00712223 0.00630329 0.00723616 0.00657827
 0.00630755 0.00687977 0.00656344 0.03867385]
ene_comp = [1.59397608e-05 1.55735123e-04 1.07285921e-05 4.34170021e-07
 1.61908397e-04 1.71658036e-05 8.60008842e-07 3.19860601e-05
 1.21115702e-05 5.29182098e-05]
ene_total = [0.72923285 0.7865912  0.32934749 0.29105972 0.3415885  0.30452867
 0.29127587 0.31913431 0.30361069 1.78811734]
optimize_network iter = 0 obj = 5.484486620358283
eta = 0.6344461603012984
freqs = [41208853.7646025  88818872.56430642 34079673.95450465 11642080.45279372
 84276963.9078595  39727437.16889541 14621417.76520615 48976615.08937065
 35364169.0182789  77588342.79934844]
eta_min = 0.6344461603012986	eta_max = 0.6344461603012982
af = 0.01705608652119456	bf = 1.9971145748853085	zeta = 0.018761695173314018	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01577767 0.01688013 0.00712223 0.00630329 0.00723616 0.00657827
 0.00630755 0.00687977 0.00656344 0.03867385]
ene_comp = [3.33892737e-06 3.26220871e-05 2.24733546e-06 9.09462937e-08
 3.39152128e-05 3.59574853e-06 1.80147438e-07 6.70017153e-06
 2.53703013e-06 1.10848626e-05]
ene_total = [2.92997303 3.14009803 1.32276198 1.17031424 1.34979508 1.22201839
 1.1711209  1.27857379 1.21906892 7.18241777]
ti_comp = [0.45555434 0.44452972 0.54210877 0.55029813 0.54096943 0.54754836
 0.55025558 0.5445333  0.54769664 0.22659259]
ti_coms = [0.1577767  0.16880133 0.07122227 0.06303291 0.07236161 0.06578268
 0.06307547 0.06879774 0.06563441 0.38673845]
t_total = [28.31912041 28.31912041 28.31912041 28.31912041 28.31912041 28.31912041
 28.31912041 28.31912041 28.31912041 28.31912041]
ene_coms = [0.01577767 0.01688013 0.00712223 0.00630329 0.00723616 0.00657827
 0.00630755 0.00687977 0.00656344 0.03867385]
ene_comp = [1.59397608e-05 1.55735123e-04 1.07285921e-05 4.34170021e-07
 1.61908397e-04 1.71658036e-05 8.60008842e-07 3.19860601e-05
 1.21115702e-05 5.29182098e-05]
ene_total = [0.72923285 0.7865912  0.32934749 0.29105972 0.3415885  0.30452867
 0.29127587 0.31913431 0.30361069 1.78811734]
optimize_network iter = 1 obj = 5.484486620358281
eta = 0.6344461603012982
freqs = [41208853.7646025  88818872.56430641 34079673.95450465 11642080.45279373
 84276963.9078595  39727437.16889541 14621417.76520615 48976615.08937065
 35364169.0182789  77588342.79934838]
Done!
ene_coms = [0.01577767 0.01688013 0.00712223 0.00630329 0.00723616 0.00657827
 0.00630755 0.00687977 0.00656344 0.03867385]
ene_comp = [1.49778561e-05 1.46337094e-04 1.00811619e-05 4.07969492e-07
 1.52137834e-04 1.61299119e-05 8.08110541e-07 3.00558216e-05
 1.13806825e-05 4.97247948e-05]
ene_total = [0.01579265 0.01702647 0.00713231 0.0063037  0.0073883  0.0065944
 0.00630835 0.00690983 0.00657482 0.03872357]
At round 33 energy consumption: 0.1187544002714591
At round 33 eta: 0.6344461603012982
At round 33 a_n: 16.878589906266903
At round 33 local rounds: 14.899104973492939
At round 33 global rounds: 46.17265112077236
gradient difference: 0.341427743434906
train() client id: f_00000-0-0 loss: 1.391704  [   32/  126]
train() client id: f_00000-0-1 loss: 1.129945  [   64/  126]
train() client id: f_00000-0-2 loss: 1.413370  [   96/  126]
train() client id: f_00000-1-0 loss: 1.245947  [   32/  126]
train() client id: f_00000-1-1 loss: 1.141219  [   64/  126]
train() client id: f_00000-1-2 loss: 1.104451  [   96/  126]
train() client id: f_00000-2-0 loss: 0.814880  [   32/  126]
train() client id: f_00000-2-1 loss: 1.159803  [   64/  126]
train() client id: f_00000-2-2 loss: 0.969170  [   96/  126]
train() client id: f_00000-3-0 loss: 1.069000  [   32/  126]
train() client id: f_00000-3-1 loss: 0.870802  [   64/  126]
train() client id: f_00000-3-2 loss: 0.838367  [   96/  126]
train() client id: f_00000-4-0 loss: 0.843984  [   32/  126]
train() client id: f_00000-4-1 loss: 0.893237  [   64/  126]
train() client id: f_00000-4-2 loss: 0.906612  [   96/  126]
train() client id: f_00000-5-0 loss: 0.927441  [   32/  126]
train() client id: f_00000-5-1 loss: 0.729723  [   64/  126]
train() client id: f_00000-5-2 loss: 0.743121  [   96/  126]
train() client id: f_00000-6-0 loss: 0.759486  [   32/  126]
train() client id: f_00000-6-1 loss: 0.775982  [   64/  126]
train() client id: f_00000-6-2 loss: 0.814182  [   96/  126]
train() client id: f_00000-7-0 loss: 0.694450  [   32/  126]
train() client id: f_00000-7-1 loss: 0.865382  [   64/  126]
train() client id: f_00000-7-2 loss: 0.748993  [   96/  126]
train() client id: f_00000-8-0 loss: 0.727463  [   32/  126]
train() client id: f_00000-8-1 loss: 0.856861  [   64/  126]
train() client id: f_00000-8-2 loss: 0.732234  [   96/  126]
train() client id: f_00000-9-0 loss: 0.671594  [   32/  126]
train() client id: f_00000-9-1 loss: 0.774971  [   64/  126]
train() client id: f_00000-9-2 loss: 0.679359  [   96/  126]
train() client id: f_00000-10-0 loss: 0.852346  [   32/  126]
train() client id: f_00000-10-1 loss: 0.702912  [   64/  126]
train() client id: f_00000-10-2 loss: 0.605502  [   96/  126]
train() client id: f_00000-11-0 loss: 0.645864  [   32/  126]
train() client id: f_00000-11-1 loss: 0.769440  [   64/  126]
train() client id: f_00000-11-2 loss: 0.613991  [   96/  126]
train() client id: f_00000-12-0 loss: 0.672897  [   32/  126]
train() client id: f_00000-12-1 loss: 0.758654  [   64/  126]
train() client id: f_00000-12-2 loss: 0.689711  [   96/  126]
train() client id: f_00000-13-0 loss: 0.559344  [   32/  126]
train() client id: f_00000-13-1 loss: 0.765731  [   64/  126]
train() client id: f_00000-13-2 loss: 0.731766  [   96/  126]
train() client id: f_00001-0-0 loss: 0.387525  [   32/  265]
train() client id: f_00001-0-1 loss: 0.545585  [   64/  265]
train() client id: f_00001-0-2 loss: 0.539531  [   96/  265]
train() client id: f_00001-0-3 loss: 0.431536  [  128/  265]
train() client id: f_00001-0-4 loss: 0.412578  [  160/  265]
train() client id: f_00001-0-5 loss: 0.435473  [  192/  265]
train() client id: f_00001-0-6 loss: 0.465138  [  224/  265]
train() client id: f_00001-0-7 loss: 0.607222  [  256/  265]
train() client id: f_00001-1-0 loss: 0.520716  [   32/  265]
train() client id: f_00001-1-1 loss: 0.457991  [   64/  265]
train() client id: f_00001-1-2 loss: 0.476626  [   96/  265]
train() client id: f_00001-1-3 loss: 0.412014  [  128/  265]
train() client id: f_00001-1-4 loss: 0.456889  [  160/  265]
train() client id: f_00001-1-5 loss: 0.548741  [  192/  265]
train() client id: f_00001-1-6 loss: 0.537707  [  224/  265]
train() client id: f_00001-1-7 loss: 0.346122  [  256/  265]
train() client id: f_00001-2-0 loss: 0.495756  [   32/  265]
train() client id: f_00001-2-1 loss: 0.381133  [   64/  265]
train() client id: f_00001-2-2 loss: 0.362042  [   96/  265]
train() client id: f_00001-2-3 loss: 0.504542  [  128/  265]
train() client id: f_00001-2-4 loss: 0.480785  [  160/  265]
train() client id: f_00001-2-5 loss: 0.485266  [  192/  265]
train() client id: f_00001-2-6 loss: 0.545753  [  224/  265]
train() client id: f_00001-2-7 loss: 0.376102  [  256/  265]
train() client id: f_00001-3-0 loss: 0.580472  [   32/  265]
train() client id: f_00001-3-1 loss: 0.440329  [   64/  265]
train() client id: f_00001-3-2 loss: 0.442926  [   96/  265]
train() client id: f_00001-3-3 loss: 0.411030  [  128/  265]
train() client id: f_00001-3-4 loss: 0.525728  [  160/  265]
train() client id: f_00001-3-5 loss: 0.372974  [  192/  265]
train() client id: f_00001-3-6 loss: 0.538096  [  224/  265]
train() client id: f_00001-3-7 loss: 0.375504  [  256/  265]
train() client id: f_00001-4-0 loss: 0.432276  [   32/  265]
train() client id: f_00001-4-1 loss: 0.641848  [   64/  265]
train() client id: f_00001-4-2 loss: 0.622119  [   96/  265]
train() client id: f_00001-4-3 loss: 0.438706  [  128/  265]
train() client id: f_00001-4-4 loss: 0.366968  [  160/  265]
train() client id: f_00001-4-5 loss: 0.368330  [  192/  265]
train() client id: f_00001-4-6 loss: 0.424733  [  224/  265]
train() client id: f_00001-4-7 loss: 0.356002  [  256/  265]
train() client id: f_00001-5-0 loss: 0.543190  [   32/  265]
train() client id: f_00001-5-1 loss: 0.441519  [   64/  265]
train() client id: f_00001-5-2 loss: 0.496930  [   96/  265]
train() client id: f_00001-5-3 loss: 0.512084  [  128/  265]
train() client id: f_00001-5-4 loss: 0.346364  [  160/  265]
train() client id: f_00001-5-5 loss: 0.436168  [  192/  265]
train() client id: f_00001-5-6 loss: 0.362978  [  224/  265]
train() client id: f_00001-5-7 loss: 0.469159  [  256/  265]
train() client id: f_00001-6-0 loss: 0.404722  [   32/  265]
train() client id: f_00001-6-1 loss: 0.433713  [   64/  265]
train() client id: f_00001-6-2 loss: 0.463715  [   96/  265]
train() client id: f_00001-6-3 loss: 0.526482  [  128/  265]
train() client id: f_00001-6-4 loss: 0.407119  [  160/  265]
train() client id: f_00001-6-5 loss: 0.406982  [  192/  265]
train() client id: f_00001-6-6 loss: 0.413742  [  224/  265]
train() client id: f_00001-6-7 loss: 0.461112  [  256/  265]
train() client id: f_00001-7-0 loss: 0.426112  [   32/  265]
train() client id: f_00001-7-1 loss: 0.457935  [   64/  265]
train() client id: f_00001-7-2 loss: 0.384144  [   96/  265]
train() client id: f_00001-7-3 loss: 0.488979  [  128/  265]
train() client id: f_00001-7-4 loss: 0.415407  [  160/  265]
train() client id: f_00001-7-5 loss: 0.403930  [  192/  265]
train() client id: f_00001-7-6 loss: 0.399657  [  224/  265]
train() client id: f_00001-7-7 loss: 0.394298  [  256/  265]
train() client id: f_00001-8-0 loss: 0.478699  [   32/  265]
train() client id: f_00001-8-1 loss: 0.572341  [   64/  265]
train() client id: f_00001-8-2 loss: 0.401231  [   96/  265]
train() client id: f_00001-8-3 loss: 0.395740  [  128/  265]
train() client id: f_00001-8-4 loss: 0.527550  [  160/  265]
train() client id: f_00001-8-5 loss: 0.393342  [  192/  265]
train() client id: f_00001-8-6 loss: 0.368364  [  224/  265]
train() client id: f_00001-8-7 loss: 0.451752  [  256/  265]
train() client id: f_00001-9-0 loss: 0.476830  [   32/  265]
train() client id: f_00001-9-1 loss: 0.369066  [   64/  265]
train() client id: f_00001-9-2 loss: 0.392826  [   96/  265]
train() client id: f_00001-9-3 loss: 0.359773  [  128/  265]
train() client id: f_00001-9-4 loss: 0.616177  [  160/  265]
train() client id: f_00001-9-5 loss: 0.342826  [  192/  265]
train() client id: f_00001-9-6 loss: 0.447766  [  224/  265]
train() client id: f_00001-9-7 loss: 0.566606  [  256/  265]
train() client id: f_00001-10-0 loss: 0.381831  [   32/  265]
train() client id: f_00001-10-1 loss: 0.451448  [   64/  265]
train() client id: f_00001-10-2 loss: 0.452760  [   96/  265]
train() client id: f_00001-10-3 loss: 0.394632  [  128/  265]
train() client id: f_00001-10-4 loss: 0.531542  [  160/  265]
train() client id: f_00001-10-5 loss: 0.593172  [  192/  265]
train() client id: f_00001-10-6 loss: 0.412828  [  224/  265]
train() client id: f_00001-10-7 loss: 0.370800  [  256/  265]
train() client id: f_00001-11-0 loss: 0.333129  [   32/  265]
train() client id: f_00001-11-1 loss: 0.471516  [   64/  265]
train() client id: f_00001-11-2 loss: 0.494350  [   96/  265]
train() client id: f_00001-11-3 loss: 0.483421  [  128/  265]
train() client id: f_00001-11-4 loss: 0.492896  [  160/  265]
train() client id: f_00001-11-5 loss: 0.457835  [  192/  265]
train() client id: f_00001-11-6 loss: 0.431404  [  224/  265]
train() client id: f_00001-11-7 loss: 0.419235  [  256/  265]
train() client id: f_00001-12-0 loss: 0.350011  [   32/  265]
train() client id: f_00001-12-1 loss: 0.512246  [   64/  265]
train() client id: f_00001-12-2 loss: 0.477598  [   96/  265]
train() client id: f_00001-12-3 loss: 0.401146  [  128/  265]
train() client id: f_00001-12-4 loss: 0.549612  [  160/  265]
train() client id: f_00001-12-5 loss: 0.356458  [  192/  265]
train() client id: f_00001-12-6 loss: 0.457987  [  224/  265]
train() client id: f_00001-12-7 loss: 0.489167  [  256/  265]
train() client id: f_00001-13-0 loss: 0.601341  [   32/  265]
train() client id: f_00001-13-1 loss: 0.553466  [   64/  265]
train() client id: f_00001-13-2 loss: 0.346433  [   96/  265]
train() client id: f_00001-13-3 loss: 0.476266  [  128/  265]
train() client id: f_00001-13-4 loss: 0.483391  [  160/  265]
train() client id: f_00001-13-5 loss: 0.376518  [  192/  265]
train() client id: f_00001-13-6 loss: 0.407156  [  224/  265]
train() client id: f_00001-13-7 loss: 0.350965  [  256/  265]
train() client id: f_00002-0-0 loss: 1.174928  [   32/  124]
train() client id: f_00002-0-1 loss: 1.167231  [   64/  124]
train() client id: f_00002-0-2 loss: 1.134573  [   96/  124]
train() client id: f_00002-1-0 loss: 1.144686  [   32/  124]
train() client id: f_00002-1-1 loss: 1.208097  [   64/  124]
train() client id: f_00002-1-2 loss: 1.088813  [   96/  124]
train() client id: f_00002-2-0 loss: 1.207690  [   32/  124]
train() client id: f_00002-2-1 loss: 1.029062  [   64/  124]
train() client id: f_00002-2-2 loss: 0.981823  [   96/  124]
train() client id: f_00002-3-0 loss: 1.051001  [   32/  124]
train() client id: f_00002-3-1 loss: 0.992365  [   64/  124]
train() client id: f_00002-3-2 loss: 1.078673  [   96/  124]
train() client id: f_00002-4-0 loss: 1.314193  [   32/  124]
train() client id: f_00002-4-1 loss: 0.968566  [   64/  124]
train() client id: f_00002-4-2 loss: 0.985915  [   96/  124]
train() client id: f_00002-5-0 loss: 1.117585  [   32/  124]
train() client id: f_00002-5-1 loss: 1.032305  [   64/  124]
train() client id: f_00002-5-2 loss: 1.040777  [   96/  124]
train() client id: f_00002-6-0 loss: 1.152948  [   32/  124]
train() client id: f_00002-6-1 loss: 0.987349  [   64/  124]
train() client id: f_00002-6-2 loss: 1.031030  [   96/  124]
train() client id: f_00002-7-0 loss: 0.844287  [   32/  124]
train() client id: f_00002-7-1 loss: 1.302262  [   64/  124]
train() client id: f_00002-7-2 loss: 1.025124  [   96/  124]
train() client id: f_00002-8-0 loss: 0.933038  [   32/  124]
train() client id: f_00002-8-1 loss: 1.008023  [   64/  124]
train() client id: f_00002-8-2 loss: 1.168579  [   96/  124]
train() client id: f_00002-9-0 loss: 1.090702  [   32/  124]
train() client id: f_00002-9-1 loss: 0.963028  [   64/  124]
train() client id: f_00002-9-2 loss: 1.097227  [   96/  124]
train() client id: f_00002-10-0 loss: 0.830511  [   32/  124]
train() client id: f_00002-10-1 loss: 1.178416  [   64/  124]
train() client id: f_00002-10-2 loss: 0.883023  [   96/  124]
train() client id: f_00002-11-0 loss: 1.126479  [   32/  124]
train() client id: f_00002-11-1 loss: 1.036649  [   64/  124]
train() client id: f_00002-11-2 loss: 0.929971  [   96/  124]
train() client id: f_00002-12-0 loss: 1.167938  [   32/  124]
train() client id: f_00002-12-1 loss: 0.857506  [   64/  124]
train() client id: f_00002-12-2 loss: 0.913673  [   96/  124]
train() client id: f_00002-13-0 loss: 1.002515  [   32/  124]
train() client id: f_00002-13-1 loss: 1.071123  [   64/  124]
train() client id: f_00002-13-2 loss: 0.963702  [   96/  124]
train() client id: f_00003-0-0 loss: 0.806778  [   32/   43]
train() client id: f_00003-1-0 loss: 0.626369  [   32/   43]
train() client id: f_00003-2-0 loss: 0.931885  [   32/   43]
train() client id: f_00003-3-0 loss: 0.942911  [   32/   43]
train() client id: f_00003-4-0 loss: 0.848295  [   32/   43]
train() client id: f_00003-5-0 loss: 1.021082  [   32/   43]
train() client id: f_00003-6-0 loss: 0.689016  [   32/   43]
train() client id: f_00003-7-0 loss: 0.724999  [   32/   43]
train() client id: f_00003-8-0 loss: 0.815751  [   32/   43]
train() client id: f_00003-9-0 loss: 0.776090  [   32/   43]
train() client id: f_00003-10-0 loss: 0.695590  [   32/   43]
train() client id: f_00003-11-0 loss: 0.740626  [   32/   43]
train() client id: f_00003-12-0 loss: 0.947121  [   32/   43]
train() client id: f_00003-13-0 loss: 0.739779  [   32/   43]
train() client id: f_00004-0-0 loss: 0.844966  [   32/  306]
train() client id: f_00004-0-1 loss: 0.974645  [   64/  306]
train() client id: f_00004-0-2 loss: 0.980896  [   96/  306]
train() client id: f_00004-0-3 loss: 0.836741  [  128/  306]
train() client id: f_00004-0-4 loss: 1.118334  [  160/  306]
train() client id: f_00004-0-5 loss: 1.019643  [  192/  306]
train() client id: f_00004-0-6 loss: 0.936797  [  224/  306]
train() client id: f_00004-0-7 loss: 1.058760  [  256/  306]
train() client id: f_00004-0-8 loss: 0.904611  [  288/  306]
train() client id: f_00004-1-0 loss: 1.072478  [   32/  306]
train() client id: f_00004-1-1 loss: 1.022775  [   64/  306]
train() client id: f_00004-1-2 loss: 1.022698  [   96/  306]
train() client id: f_00004-1-3 loss: 0.850688  [  128/  306]
train() client id: f_00004-1-4 loss: 1.032622  [  160/  306]
train() client id: f_00004-1-5 loss: 0.884631  [  192/  306]
train() client id: f_00004-1-6 loss: 0.946080  [  224/  306]
train() client id: f_00004-1-7 loss: 0.791653  [  256/  306]
train() client id: f_00004-1-8 loss: 0.901358  [  288/  306]
train() client id: f_00004-2-0 loss: 0.907349  [   32/  306]
train() client id: f_00004-2-1 loss: 1.061691  [   64/  306]
train() client id: f_00004-2-2 loss: 1.068775  [   96/  306]
train() client id: f_00004-2-3 loss: 1.022297  [  128/  306]
train() client id: f_00004-2-4 loss: 0.999566  [  160/  306]
train() client id: f_00004-2-5 loss: 0.926071  [  192/  306]
train() client id: f_00004-2-6 loss: 0.968694  [  224/  306]
train() client id: f_00004-2-7 loss: 0.802958  [  256/  306]
train() client id: f_00004-2-8 loss: 0.855790  [  288/  306]
train() client id: f_00004-3-0 loss: 0.774838  [   32/  306]
train() client id: f_00004-3-1 loss: 0.959564  [   64/  306]
train() client id: f_00004-3-2 loss: 0.907919  [   96/  306]
train() client id: f_00004-3-3 loss: 0.970743  [  128/  306]
train() client id: f_00004-3-4 loss: 0.919876  [  160/  306]
train() client id: f_00004-3-5 loss: 0.985097  [  192/  306]
train() client id: f_00004-3-6 loss: 0.965157  [  224/  306]
train() client id: f_00004-3-7 loss: 0.926336  [  256/  306]
train() client id: f_00004-3-8 loss: 1.082428  [  288/  306]
train() client id: f_00004-4-0 loss: 0.860275  [   32/  306]
train() client id: f_00004-4-1 loss: 0.876124  [   64/  306]
train() client id: f_00004-4-2 loss: 0.874922  [   96/  306]
train() client id: f_00004-4-3 loss: 1.011671  [  128/  306]
train() client id: f_00004-4-4 loss: 0.961622  [  160/  306]
train() client id: f_00004-4-5 loss: 0.929510  [  192/  306]
train() client id: f_00004-4-6 loss: 1.028496  [  224/  306]
train() client id: f_00004-4-7 loss: 0.940248  [  256/  306]
train() client id: f_00004-4-8 loss: 0.993161  [  288/  306]
train() client id: f_00004-5-0 loss: 0.901039  [   32/  306]
train() client id: f_00004-5-1 loss: 0.952111  [   64/  306]
train() client id: f_00004-5-2 loss: 1.097685  [   96/  306]
train() client id: f_00004-5-3 loss: 0.986662  [  128/  306]
train() client id: f_00004-5-4 loss: 0.922308  [  160/  306]
train() client id: f_00004-5-5 loss: 0.887870  [  192/  306]
train() client id: f_00004-5-6 loss: 0.817321  [  224/  306]
train() client id: f_00004-5-7 loss: 0.879724  [  256/  306]
train() client id: f_00004-5-8 loss: 0.963432  [  288/  306]
train() client id: f_00004-6-0 loss: 0.938246  [   32/  306]
train() client id: f_00004-6-1 loss: 0.998195  [   64/  306]
train() client id: f_00004-6-2 loss: 0.933036  [   96/  306]
train() client id: f_00004-6-3 loss: 0.806259  [  128/  306]
train() client id: f_00004-6-4 loss: 0.869938  [  160/  306]
train() client id: f_00004-6-5 loss: 0.985058  [  192/  306]
train() client id: f_00004-6-6 loss: 0.924405  [  224/  306]
train() client id: f_00004-6-7 loss: 0.923066  [  256/  306]
train() client id: f_00004-6-8 loss: 0.930302  [  288/  306]
train() client id: f_00004-7-0 loss: 0.890988  [   32/  306]
train() client id: f_00004-7-1 loss: 0.933393  [   64/  306]
train() client id: f_00004-7-2 loss: 0.883935  [   96/  306]
train() client id: f_00004-7-3 loss: 1.046358  [  128/  306]
train() client id: f_00004-7-4 loss: 0.945333  [  160/  306]
train() client id: f_00004-7-5 loss: 0.932594  [  192/  306]
train() client id: f_00004-7-6 loss: 0.831824  [  224/  306]
train() client id: f_00004-7-7 loss: 0.912230  [  256/  306]
train() client id: f_00004-7-8 loss: 0.964523  [  288/  306]
train() client id: f_00004-8-0 loss: 0.865593  [   32/  306]
train() client id: f_00004-8-1 loss: 1.049663  [   64/  306]
train() client id: f_00004-8-2 loss: 0.922915  [   96/  306]
train() client id: f_00004-8-3 loss: 0.974347  [  128/  306]
train() client id: f_00004-8-4 loss: 0.915508  [  160/  306]
train() client id: f_00004-8-5 loss: 0.946959  [  192/  306]
train() client id: f_00004-8-6 loss: 0.910128  [  224/  306]
train() client id: f_00004-8-7 loss: 0.954851  [  256/  306]
train() client id: f_00004-8-8 loss: 0.886078  [  288/  306]
train() client id: f_00004-9-0 loss: 0.882887  [   32/  306]
train() client id: f_00004-9-1 loss: 0.838613  [   64/  306]
train() client id: f_00004-9-2 loss: 1.058564  [   96/  306]
train() client id: f_00004-9-3 loss: 0.889043  [  128/  306]
train() client id: f_00004-9-4 loss: 0.860797  [  160/  306]
train() client id: f_00004-9-5 loss: 0.909411  [  192/  306]
train() client id: f_00004-9-6 loss: 0.982642  [  224/  306]
train() client id: f_00004-9-7 loss: 1.052001  [  256/  306]
train() client id: f_00004-9-8 loss: 0.834775  [  288/  306]
train() client id: f_00004-10-0 loss: 0.927723  [   32/  306]
train() client id: f_00004-10-1 loss: 1.075460  [   64/  306]
train() client id: f_00004-10-2 loss: 0.860119  [   96/  306]
train() client id: f_00004-10-3 loss: 0.919936  [  128/  306]
train() client id: f_00004-10-4 loss: 0.807150  [  160/  306]
train() client id: f_00004-10-5 loss: 0.876899  [  192/  306]
train() client id: f_00004-10-6 loss: 0.981714  [  224/  306]
train() client id: f_00004-10-7 loss: 0.986497  [  256/  306]
train() client id: f_00004-10-8 loss: 0.874785  [  288/  306]
train() client id: f_00004-11-0 loss: 0.933147  [   32/  306]
train() client id: f_00004-11-1 loss: 1.046556  [   64/  306]
train() client id: f_00004-11-2 loss: 0.915024  [   96/  306]
train() client id: f_00004-11-3 loss: 0.923083  [  128/  306]
train() client id: f_00004-11-4 loss: 0.842332  [  160/  306]
train() client id: f_00004-11-5 loss: 0.791834  [  192/  306]
train() client id: f_00004-11-6 loss: 0.951894  [  224/  306]
train() client id: f_00004-11-7 loss: 0.959349  [  256/  306]
train() client id: f_00004-11-8 loss: 0.987204  [  288/  306]
train() client id: f_00004-12-0 loss: 1.009475  [   32/  306]
train() client id: f_00004-12-1 loss: 0.830232  [   64/  306]
train() client id: f_00004-12-2 loss: 0.918130  [   96/  306]
train() client id: f_00004-12-3 loss: 0.888236  [  128/  306]
train() client id: f_00004-12-4 loss: 1.037868  [  160/  306]
train() client id: f_00004-12-5 loss: 0.982697  [  192/  306]
train() client id: f_00004-12-6 loss: 0.881003  [  224/  306]
train() client id: f_00004-12-7 loss: 0.887268  [  256/  306]
train() client id: f_00004-12-8 loss: 0.835528  [  288/  306]
train() client id: f_00004-13-0 loss: 0.843788  [   32/  306]
train() client id: f_00004-13-1 loss: 0.903530  [   64/  306]
train() client id: f_00004-13-2 loss: 0.935861  [   96/  306]
train() client id: f_00004-13-3 loss: 0.976712  [  128/  306]
train() client id: f_00004-13-4 loss: 0.935366  [  160/  306]
train() client id: f_00004-13-5 loss: 0.858763  [  192/  306]
train() client id: f_00004-13-6 loss: 0.907973  [  224/  306]
train() client id: f_00004-13-7 loss: 0.880589  [  256/  306]
train() client id: f_00004-13-8 loss: 1.027434  [  288/  306]
train() client id: f_00005-0-0 loss: 0.814332  [   32/  146]
train() client id: f_00005-0-1 loss: 0.715405  [   64/  146]
train() client id: f_00005-0-2 loss: 0.654595  [   96/  146]
train() client id: f_00005-0-3 loss: 0.680657  [  128/  146]
train() client id: f_00005-1-0 loss: 0.571536  [   32/  146]
train() client id: f_00005-1-1 loss: 0.917804  [   64/  146]
train() client id: f_00005-1-2 loss: 0.511309  [   96/  146]
train() client id: f_00005-1-3 loss: 0.778981  [  128/  146]
train() client id: f_00005-2-0 loss: 0.635403  [   32/  146]
train() client id: f_00005-2-1 loss: 0.631301  [   64/  146]
train() client id: f_00005-2-2 loss: 0.586654  [   96/  146]
train() client id: f_00005-2-3 loss: 0.811085  [  128/  146]
train() client id: f_00005-3-0 loss: 0.462484  [   32/  146]
train() client id: f_00005-3-1 loss: 0.565387  [   64/  146]
train() client id: f_00005-3-2 loss: 0.813692  [   96/  146]
train() client id: f_00005-3-3 loss: 0.823289  [  128/  146]
train() client id: f_00005-4-0 loss: 1.050965  [   32/  146]
train() client id: f_00005-4-1 loss: 0.664262  [   64/  146]
train() client id: f_00005-4-2 loss: 0.525168  [   96/  146]
train() client id: f_00005-4-3 loss: 0.615753  [  128/  146]
train() client id: f_00005-5-0 loss: 0.642218  [   32/  146]
train() client id: f_00005-5-1 loss: 0.676294  [   64/  146]
train() client id: f_00005-5-2 loss: 0.696345  [   96/  146]
train() client id: f_00005-5-3 loss: 0.879475  [  128/  146]
train() client id: f_00005-6-0 loss: 0.590862  [   32/  146]
train() client id: f_00005-6-1 loss: 0.650580  [   64/  146]
train() client id: f_00005-6-2 loss: 0.781551  [   96/  146]
train() client id: f_00005-6-3 loss: 0.811277  [  128/  146]
train() client id: f_00005-7-0 loss: 0.404757  [   32/  146]
train() client id: f_00005-7-1 loss: 0.755640  [   64/  146]
train() client id: f_00005-7-2 loss: 0.578400  [   96/  146]
train() client id: f_00005-7-3 loss: 0.807945  [  128/  146]
train() client id: f_00005-8-0 loss: 0.711340  [   32/  146]
train() client id: f_00005-8-1 loss: 0.750355  [   64/  146]
train() client id: f_00005-8-2 loss: 0.756762  [   96/  146]
train() client id: f_00005-8-3 loss: 0.521330  [  128/  146]
train() client id: f_00005-9-0 loss: 0.697163  [   32/  146]
train() client id: f_00005-9-1 loss: 0.640194  [   64/  146]
train() client id: f_00005-9-2 loss: 0.622566  [   96/  146]
train() client id: f_00005-9-3 loss: 0.900381  [  128/  146]
train() client id: f_00005-10-0 loss: 0.589725  [   32/  146]
train() client id: f_00005-10-1 loss: 0.485911  [   64/  146]
train() client id: f_00005-10-2 loss: 0.647702  [   96/  146]
train() client id: f_00005-10-3 loss: 0.886951  [  128/  146]
train() client id: f_00005-11-0 loss: 0.890468  [   32/  146]
train() client id: f_00005-11-1 loss: 0.711803  [   64/  146]
train() client id: f_00005-11-2 loss: 0.455080  [   96/  146]
train() client id: f_00005-11-3 loss: 0.778464  [  128/  146]
train() client id: f_00005-12-0 loss: 0.546601  [   32/  146]
train() client id: f_00005-12-1 loss: 0.937258  [   64/  146]
train() client id: f_00005-12-2 loss: 0.665999  [   96/  146]
train() client id: f_00005-12-3 loss: 0.635921  [  128/  146]
train() client id: f_00005-13-0 loss: 0.664139  [   32/  146]
train() client id: f_00005-13-1 loss: 0.562008  [   64/  146]
train() client id: f_00005-13-2 loss: 0.833066  [   96/  146]
train() client id: f_00005-13-3 loss: 0.739595  [  128/  146]
train() client id: f_00006-0-0 loss: 0.556151  [   32/   54]
train() client id: f_00006-1-0 loss: 0.527532  [   32/   54]
train() client id: f_00006-2-0 loss: 0.556666  [   32/   54]
train() client id: f_00006-3-0 loss: 0.598446  [   32/   54]
train() client id: f_00006-4-0 loss: 0.564183  [   32/   54]
train() client id: f_00006-5-0 loss: 0.633474  [   32/   54]
train() client id: f_00006-6-0 loss: 0.557753  [   32/   54]
train() client id: f_00006-7-0 loss: 0.544255  [   32/   54]
train() client id: f_00006-8-0 loss: 0.630392  [   32/   54]
train() client id: f_00006-9-0 loss: 0.578310  [   32/   54]
train() client id: f_00006-10-0 loss: 0.632967  [   32/   54]
train() client id: f_00006-11-0 loss: 0.517654  [   32/   54]
train() client id: f_00006-12-0 loss: 0.608527  [   32/   54]
train() client id: f_00006-13-0 loss: 0.625698  [   32/   54]
train() client id: f_00007-0-0 loss: 0.453416  [   32/  179]
train() client id: f_00007-0-1 loss: 0.500233  [   64/  179]
train() client id: f_00007-0-2 loss: 0.359738  [   96/  179]
train() client id: f_00007-0-3 loss: 0.316470  [  128/  179]
train() client id: f_00007-0-4 loss: 0.358726  [  160/  179]
train() client id: f_00007-1-0 loss: 0.354937  [   32/  179]
train() client id: f_00007-1-1 loss: 0.464220  [   64/  179]
train() client id: f_00007-1-2 loss: 0.248385  [   96/  179]
train() client id: f_00007-1-3 loss: 0.468540  [  128/  179]
train() client id: f_00007-1-4 loss: 0.506648  [  160/  179]
train() client id: f_00007-2-0 loss: 0.471374  [   32/  179]
train() client id: f_00007-2-1 loss: 0.472476  [   64/  179]
train() client id: f_00007-2-2 loss: 0.438145  [   96/  179]
train() client id: f_00007-2-3 loss: 0.345769  [  128/  179]
train() client id: f_00007-2-4 loss: 0.243867  [  160/  179]
train() client id: f_00007-3-0 loss: 0.367840  [   32/  179]
train() client id: f_00007-3-1 loss: 0.305413  [   64/  179]
train() client id: f_00007-3-2 loss: 0.403653  [   96/  179]
train() client id: f_00007-3-3 loss: 0.399213  [  128/  179]
train() client id: f_00007-3-4 loss: 0.382985  [  160/  179]
train() client id: f_00007-4-0 loss: 0.436618  [   32/  179]
train() client id: f_00007-4-1 loss: 0.243667  [   64/  179]
train() client id: f_00007-4-2 loss: 0.216570  [   96/  179]
train() client id: f_00007-4-3 loss: 0.360095  [  128/  179]
train() client id: f_00007-4-4 loss: 0.401298  [  160/  179]
train() client id: f_00007-5-0 loss: 0.220512  [   32/  179]
train() client id: f_00007-5-1 loss: 0.366111  [   64/  179]
train() client id: f_00007-5-2 loss: 0.530656  [   96/  179]
train() client id: f_00007-5-3 loss: 0.313677  [  128/  179]
train() client id: f_00007-5-4 loss: 0.376817  [  160/  179]
train() client id: f_00007-6-0 loss: 0.452612  [   32/  179]
train() client id: f_00007-6-1 loss: 0.162657  [   64/  179]
train() client id: f_00007-6-2 loss: 0.443687  [   96/  179]
train() client id: f_00007-6-3 loss: 0.165471  [  128/  179]
train() client id: f_00007-6-4 loss: 0.324364  [  160/  179]
train() client id: f_00007-7-0 loss: 0.305261  [   32/  179]
train() client id: f_00007-7-1 loss: 0.281005  [   64/  179]
train() client id: f_00007-7-2 loss: 0.245085  [   96/  179]
train() client id: f_00007-7-3 loss: 0.202409  [  128/  179]
train() client id: f_00007-7-4 loss: 0.477273  [  160/  179]
train() client id: f_00007-8-0 loss: 0.325419  [   32/  179]
train() client id: f_00007-8-1 loss: 0.147127  [   64/  179]
train() client id: f_00007-8-2 loss: 0.246303  [   96/  179]
train() client id: f_00007-8-3 loss: 0.495949  [  128/  179]
train() client id: f_00007-8-4 loss: 0.485692  [  160/  179]
train() client id: f_00007-9-0 loss: 0.361152  [   32/  179]
train() client id: f_00007-9-1 loss: 0.297929  [   64/  179]
train() client id: f_00007-9-2 loss: 0.273568  [   96/  179]
train() client id: f_00007-9-3 loss: 0.170958  [  128/  179]
train() client id: f_00007-9-4 loss: 0.327892  [  160/  179]
train() client id: f_00007-10-0 loss: 0.218196  [   32/  179]
train() client id: f_00007-10-1 loss: 0.591336  [   64/  179]
train() client id: f_00007-10-2 loss: 0.151114  [   96/  179]
train() client id: f_00007-10-3 loss: 0.320502  [  128/  179]
train() client id: f_00007-10-4 loss: 0.369691  [  160/  179]
train() client id: f_00007-11-0 loss: 0.174550  [   32/  179]
train() client id: f_00007-11-1 loss: 0.530795  [   64/  179]
train() client id: f_00007-11-2 loss: 0.253870  [   96/  179]
train() client id: f_00007-11-3 loss: 0.356404  [  128/  179]
train() client id: f_00007-11-4 loss: 0.127464  [  160/  179]
train() client id: f_00007-12-0 loss: 0.223363  [   32/  179]
train() client id: f_00007-12-1 loss: 0.261633  [   64/  179]
train() client id: f_00007-12-2 loss: 0.370557  [   96/  179]
train() client id: f_00007-12-3 loss: 0.418828  [  128/  179]
train() client id: f_00007-12-4 loss: 0.281234  [  160/  179]
train() client id: f_00007-13-0 loss: 0.294601  [   32/  179]
train() client id: f_00007-13-1 loss: 0.369851  [   64/  179]
train() client id: f_00007-13-2 loss: 0.296909  [   96/  179]
train() client id: f_00007-13-3 loss: 0.332461  [  128/  179]
train() client id: f_00007-13-4 loss: 0.231887  [  160/  179]
train() client id: f_00008-0-0 loss: 0.654322  [   32/  130]
train() client id: f_00008-0-1 loss: 0.655049  [   64/  130]
train() client id: f_00008-0-2 loss: 0.696967  [   96/  130]
train() client id: f_00008-0-3 loss: 0.784636  [  128/  130]
train() client id: f_00008-1-0 loss: 0.663181  [   32/  130]
train() client id: f_00008-1-1 loss: 0.681340  [   64/  130]
train() client id: f_00008-1-2 loss: 0.816830  [   96/  130]
train() client id: f_00008-1-3 loss: 0.662510  [  128/  130]
train() client id: f_00008-2-0 loss: 0.604854  [   32/  130]
train() client id: f_00008-2-1 loss: 0.733448  [   64/  130]
train() client id: f_00008-2-2 loss: 0.733575  [   96/  130]
train() client id: f_00008-2-3 loss: 0.756991  [  128/  130]
train() client id: f_00008-3-0 loss: 0.676391  [   32/  130]
train() client id: f_00008-3-1 loss: 0.642192  [   64/  130]
train() client id: f_00008-3-2 loss: 0.729896  [   96/  130]
train() client id: f_00008-3-3 loss: 0.767205  [  128/  130]
train() client id: f_00008-4-0 loss: 0.655285  [   32/  130]
train() client id: f_00008-4-1 loss: 0.777378  [   64/  130]
train() client id: f_00008-4-2 loss: 0.705806  [   96/  130]
train() client id: f_00008-4-3 loss: 0.674583  [  128/  130]
train() client id: f_00008-5-0 loss: 0.664141  [   32/  130]
train() client id: f_00008-5-1 loss: 0.624160  [   64/  130]
train() client id: f_00008-5-2 loss: 0.863162  [   96/  130]
train() client id: f_00008-5-3 loss: 0.673054  [  128/  130]
train() client id: f_00008-6-0 loss: 0.684906  [   32/  130]
train() client id: f_00008-6-1 loss: 0.683846  [   64/  130]
train() client id: f_00008-6-2 loss: 0.731914  [   96/  130]
train() client id: f_00008-6-3 loss: 0.720067  [  128/  130]
train() client id: f_00008-7-0 loss: 0.683526  [   32/  130]
train() client id: f_00008-7-1 loss: 0.727251  [   64/  130]
train() client id: f_00008-7-2 loss: 0.759386  [   96/  130]
train() client id: f_00008-7-3 loss: 0.656420  [  128/  130]
train() client id: f_00008-8-0 loss: 0.822404  [   32/  130]
train() client id: f_00008-8-1 loss: 0.721834  [   64/  130]
train() client id: f_00008-8-2 loss: 0.698482  [   96/  130]
train() client id: f_00008-8-3 loss: 0.585536  [  128/  130]
train() client id: f_00008-9-0 loss: 0.821119  [   32/  130]
train() client id: f_00008-9-1 loss: 0.647289  [   64/  130]
train() client id: f_00008-9-2 loss: 0.680464  [   96/  130]
train() client id: f_00008-9-3 loss: 0.646459  [  128/  130]
train() client id: f_00008-10-0 loss: 0.666233  [   32/  130]
train() client id: f_00008-10-1 loss: 0.833815  [   64/  130]
train() client id: f_00008-10-2 loss: 0.708265  [   96/  130]
train() client id: f_00008-10-3 loss: 0.613552  [  128/  130]
train() client id: f_00008-11-0 loss: 0.657204  [   32/  130]
train() client id: f_00008-11-1 loss: 0.708713  [   64/  130]
train() client id: f_00008-11-2 loss: 0.778232  [   96/  130]
train() client id: f_00008-11-3 loss: 0.675084  [  128/  130]
train() client id: f_00008-12-0 loss: 0.698407  [   32/  130]
train() client id: f_00008-12-1 loss: 0.656459  [   64/  130]
train() client id: f_00008-12-2 loss: 0.780205  [   96/  130]
train() client id: f_00008-12-3 loss: 0.651704  [  128/  130]
train() client id: f_00008-13-0 loss: 0.809800  [   32/  130]
train() client id: f_00008-13-1 loss: 0.623609  [   64/  130]
train() client id: f_00008-13-2 loss: 0.756375  [   96/  130]
train() client id: f_00008-13-3 loss: 0.634127  [  128/  130]
train() client id: f_00009-0-0 loss: 1.130082  [   32/  118]
train() client id: f_00009-0-1 loss: 1.328618  [   64/  118]
train() client id: f_00009-0-2 loss: 1.206520  [   96/  118]
train() client id: f_00009-1-0 loss: 1.078618  [   32/  118]
train() client id: f_00009-1-1 loss: 1.058283  [   64/  118]
train() client id: f_00009-1-2 loss: 1.189169  [   96/  118]
train() client id: f_00009-2-0 loss: 1.101748  [   32/  118]
train() client id: f_00009-2-1 loss: 1.147204  [   64/  118]
train() client id: f_00009-2-2 loss: 0.959318  [   96/  118]
train() client id: f_00009-3-0 loss: 1.035995  [   32/  118]
train() client id: f_00009-3-1 loss: 1.012238  [   64/  118]
train() client id: f_00009-3-2 loss: 1.021724  [   96/  118]
train() client id: f_00009-4-0 loss: 0.998142  [   32/  118]
train() client id: f_00009-4-1 loss: 0.974356  [   64/  118]
train() client id: f_00009-4-2 loss: 0.866589  [   96/  118]
train() client id: f_00009-5-0 loss: 0.997194  [   32/  118]
train() client id: f_00009-5-1 loss: 0.957043  [   64/  118]
train() client id: f_00009-5-2 loss: 0.815097  [   96/  118]
train() client id: f_00009-6-0 loss: 0.966641  [   32/  118]
train() client id: f_00009-6-1 loss: 0.829194  [   64/  118]
train() client id: f_00009-6-2 loss: 0.844221  [   96/  118]
train() client id: f_00009-7-0 loss: 0.964437  [   32/  118]
train() client id: f_00009-7-1 loss: 0.835369  [   64/  118]
train() client id: f_00009-7-2 loss: 0.918333  [   96/  118]
train() client id: f_00009-8-0 loss: 0.844022  [   32/  118]
train() client id: f_00009-8-1 loss: 0.875960  [   64/  118]
train() client id: f_00009-8-2 loss: 0.837197  [   96/  118]
train() client id: f_00009-9-0 loss: 0.902015  [   32/  118]
train() client id: f_00009-9-1 loss: 0.753422  [   64/  118]
train() client id: f_00009-9-2 loss: 0.856688  [   96/  118]
train() client id: f_00009-10-0 loss: 0.886716  [   32/  118]
train() client id: f_00009-10-1 loss: 0.806244  [   64/  118]
train() client id: f_00009-10-2 loss: 0.765140  [   96/  118]
train() client id: f_00009-11-0 loss: 0.801364  [   32/  118]
train() client id: f_00009-11-1 loss: 0.668626  [   64/  118]
train() client id: f_00009-11-2 loss: 0.905121  [   96/  118]
train() client id: f_00009-12-0 loss: 0.719183  [   32/  118]
train() client id: f_00009-12-1 loss: 0.855180  [   64/  118]
train() client id: f_00009-12-2 loss: 0.867501  [   96/  118]
train() client id: f_00009-13-0 loss: 0.835380  [   32/  118]
train() client id: f_00009-13-1 loss: 0.724032  [   64/  118]
train() client id: f_00009-13-2 loss: 0.840061  [   96/  118]
At round 33 accuracy: 0.6445623342175066
At round 33 training accuracy: 0.5902079141515761
At round 33 training loss: 0.8346084501292819
update_location
xs = 8.927491 286.223621 5.882650 -4.065740 -202.581990 -50.230757 -10.849135 -5.143845 -225.120581 20.134486 
ys = -277.390647 7.291448 175.684448 -2.290817 -9.642386 0.794442 -1.381692 171.628436 25.881276 -712.232496 
xs mean: -17.682379970521218
ys mean: -62.16579882624052
dists_uav = 295.000460 303.277309 202.236572 100.108831 226.124829 111.909607 100.596286 198.702741 247.687538 719.500192 
uav_gains = -115.307512 -116.000844 -107.823260 -100.011826 -109.421140 -101.221764 -100.064566 -107.600376 -111.100671 -128.382189 
uav_gains_db_mean: -109.69341474212742
dists_bs = 488.350919 490.768160 180.883945 246.273334 186.691127 214.277718 240.948144 169.889614 157.316453 908.437653 
bs_gains = -114.851555 -114.911598 -102.774260 -106.526746 -103.158522 -104.834415 -106.260920 -102.011730 -101.076735 -122.399323 
bs_gains_db_mean: -107.8805804365575
Round 34
-------------------------------
ene_coms = [0.01639466 0.0175441  0.0071211  0.00630551 0.00725127 0.00663887
 0.00631935 0.00687593 0.00659709 0.03906123]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.62113669 13.64251108  6.36548909  2.27146197 15.53597897  7.46569229
  2.82582211  9.13203883  6.65898864  6.59673525]
obj_prev = 77.11585491137254
eta_min = 6.4532429491006e-15	eta_max = 0.7685280504984884
af = 16.14675753318756	bf = 1.9861299700448378	zeta = 17.761433286506318	eta = 0.9090909090909091
af = 16.14675753318756	bf = 1.9861299700448378	zeta = 38.775883671687595	eta = 0.41641236779800833
af = 16.14675753318756	bf = 1.9861299700448378	zeta = 27.642753484656556	eta = 0.584122617963873
af = 16.14675753318756	bf = 1.9861299700448378	zeta = 25.65021202173006	eta = 0.6294980142662576
af = 16.14675753318756	bf = 1.9861299700448378	zeta = 25.531240806872024	eta = 0.6324313673325849
af = 16.14675753318756	bf = 1.9861299700448378	zeta = 25.5307668222377	eta = 0.6324431085682659
af = 16.14675753318756	bf = 1.9861299700448378	zeta = 25.530766814667267	eta = 0.6324431087557992
eta = 0.6324431087557992
ene_coms = [0.01639466 0.0175441  0.0071211  0.00630551 0.00725127 0.00663887
 0.00631935 0.00687593 0.00659709 0.03906123]
ene_comp = [0.03780668 0.07951405 0.03720657 0.01290228 0.09181622 0.04380774
 0.01620286 0.05370949 0.03900689 0.03540625]
ene_total = [2.43846797 4.36655617 1.99426114 0.86414078 4.45695467 2.26954636
 1.01325359 2.72568199 2.05168094 3.35022319]
ti_comp = [0.46438785 0.45289342 0.5571234  0.56527931 0.55582172 0.56194571
 0.56514092 0.55957511 0.56236352 0.23772213]
ti_coms = [0.16394659 0.17544102 0.07121105 0.06305513 0.07251273 0.06638874
 0.06319353 0.06875934 0.06597092 0.39061231]
t_total = [28.26818466 28.26818466 28.26818466 28.26818466 28.26818466 28.26818466
 28.26818466 28.26818466 28.26818466 28.26818466]
ene_coms = [0.01639466 0.0175441  0.0071211  0.00630551 0.00725127 0.00663887
 0.00631935 0.00687593 0.00659709 0.03906123]
ene_comp = [1.56611585e-05 1.53186165e-04 1.03713720e-05 4.20100434e-07
 1.56591146e-04 1.66396353e-05 8.32418503e-07 3.09254675e-05
 1.17292360e-05 4.90886097e-05]
ene_total = [0.73828512 0.79618461 0.32083851 0.28369809 0.33327293 0.29942539
 0.28433928 0.31073321 0.29732476 1.75953706]
optimize_network iter = 0 obj = 5.423638954382952
eta = 0.6324431087557992
freqs = [40705929.51283132 87784500.47422525 33391680.01564826 11412304.32719335
 82595026.1076169  38978621.87714291 14335240.69452246 47991312.54755988
 34681206.10753796 74469831.19055285]
eta_min = 0.6324431087557999	eta_max = 0.6324431087557976
af = 0.01607696596967838	bf = 1.9861299700448378	zeta = 0.01768466256664622	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01639466 0.0175441  0.0071211  0.00630551 0.00725127 0.00663887
 0.00631935 0.00687593 0.00659709 0.03906123]
ene_comp = [3.25792629e-06 3.18666871e-05 2.15751380e-06 8.73917631e-08
 3.25750112e-05 3.46147479e-06 1.73164593e-07 6.43329763e-06
 2.43998465e-06 1.02117013e-05]
ene_total = [2.9827235  3.19700696 1.29569639 1.14696657 1.32490627 1.20821745
 1.14949955 1.25187842 1.20043174 7.10695811]
ti_comp = [0.46438785 0.45289342 0.5571234  0.56527931 0.55582172 0.56194571
 0.56514092 0.55957511 0.56236352 0.23772213]
ti_coms = [0.16394659 0.17544102 0.07121105 0.06305513 0.07251273 0.06638874
 0.06319353 0.06875934 0.06597092 0.39061231]
t_total = [28.26818466 28.26818466 28.26818466 28.26818466 28.26818466 28.26818466
 28.26818466 28.26818466 28.26818466 28.26818466]
ene_coms = [0.01639466 0.0175441  0.0071211  0.00630551 0.00725127 0.00663887
 0.00631935 0.00687593 0.00659709 0.03906123]
ene_comp = [1.56611585e-05 1.53186165e-04 1.03713720e-05 4.20100434e-07
 1.56591146e-04 1.66396353e-05 8.32418503e-07 3.09254675e-05
 1.17292360e-05 4.90886097e-05]
ene_total = [0.73828512 0.79618461 0.32083851 0.28369809 0.33327293 0.29942539
 0.28433928 0.31073321 0.29732476 1.75953706]
optimize_network iter = 1 obj = 5.423638954382927
eta = 0.6324431087557976
freqs = [40705929.51283131 87784500.47422521 33391680.01564828 11412304.32719336
 82595026.10761696 38978621.87714294 14335240.69452247 47991312.54755991
 34681206.10753799 74469831.19055238]
Done!
ene_coms = [0.01639466 0.0175441  0.0071211  0.00630551 0.00725127 0.00663887
 0.00631935 0.00687593 0.00659709 0.03906123]
ene_comp = [1.56583920e-05 1.53159106e-04 1.03695399e-05 4.20026225e-07
 1.56563485e-04 1.66366959e-05 8.32271459e-07 3.09200046e-05
 1.17271641e-05 4.90799385e-05]
ene_total = [0.01641032 0.01769726 0.00713147 0.00630593 0.00740784 0.00665551
 0.00632019 0.00690685 0.00660882 0.03911031]
At round 34 energy consumption: 0.12055450293915981
At round 34 eta: 0.6324431087557976
At round 34 a_n: 16.536044059297588
At round 34 local rounds: 15.002650153925485
At round 34 global rounds: 44.989073673253884
gradient difference: 0.4004562795162201
train() client id: f_00000-0-0 loss: 1.527178  [   32/  126]
train() client id: f_00000-0-1 loss: 1.346873  [   64/  126]
train() client id: f_00000-0-2 loss: 1.242450  [   96/  126]
train() client id: f_00000-1-0 loss: 1.374610  [   32/  126]
train() client id: f_00000-1-1 loss: 1.449063  [   64/  126]
train() client id: f_00000-1-2 loss: 1.197314  [   96/  126]
train() client id: f_00000-2-0 loss: 1.220383  [   32/  126]
train() client id: f_00000-2-1 loss: 1.142047  [   64/  126]
train() client id: f_00000-2-2 loss: 1.139736  [   96/  126]
train() client id: f_00000-3-0 loss: 1.215608  [   32/  126]
train() client id: f_00000-3-1 loss: 0.930284  [   64/  126]
train() client id: f_00000-3-2 loss: 1.055240  [   96/  126]
train() client id: f_00000-4-0 loss: 0.937049  [   32/  126]
train() client id: f_00000-4-1 loss: 1.046914  [   64/  126]
train() client id: f_00000-4-2 loss: 0.939563  [   96/  126]
train() client id: f_00000-5-0 loss: 0.875598  [   32/  126]
train() client id: f_00000-5-1 loss: 0.890949  [   64/  126]
train() client id: f_00000-5-2 loss: 1.038671  [   96/  126]
train() client id: f_00000-6-0 loss: 0.787612  [   32/  126]
train() client id: f_00000-6-1 loss: 0.875666  [   64/  126]
train() client id: f_00000-6-2 loss: 0.938546  [   96/  126]
train() client id: f_00000-7-0 loss: 0.751278  [   32/  126]
train() client id: f_00000-7-1 loss: 0.867795  [   64/  126]
train() client id: f_00000-7-2 loss: 0.900907  [   96/  126]
train() client id: f_00000-8-0 loss: 0.923320  [   32/  126]
train() client id: f_00000-8-1 loss: 0.777139  [   64/  126]
train() client id: f_00000-8-2 loss: 0.736669  [   96/  126]
train() client id: f_00000-9-0 loss: 0.761189  [   32/  126]
train() client id: f_00000-9-1 loss: 0.824177  [   64/  126]
train() client id: f_00000-9-2 loss: 0.827163  [   96/  126]
train() client id: f_00000-10-0 loss: 0.808217  [   32/  126]
train() client id: f_00000-10-1 loss: 0.775934  [   64/  126]
train() client id: f_00000-10-2 loss: 0.803011  [   96/  126]
train() client id: f_00000-11-0 loss: 0.790101  [   32/  126]
train() client id: f_00000-11-1 loss: 0.771026  [   64/  126]
train() client id: f_00000-11-2 loss: 0.789932  [   96/  126]
train() client id: f_00000-12-0 loss: 0.744178  [   32/  126]
train() client id: f_00000-12-1 loss: 0.839692  [   64/  126]
train() client id: f_00000-12-2 loss: 0.856580  [   96/  126]
train() client id: f_00000-13-0 loss: 0.700668  [   32/  126]
train() client id: f_00000-13-1 loss: 0.817248  [   64/  126]
train() client id: f_00000-13-2 loss: 0.816780  [   96/  126]
train() client id: f_00000-14-0 loss: 0.721798  [   32/  126]
train() client id: f_00000-14-1 loss: 0.774965  [   64/  126]
train() client id: f_00000-14-2 loss: 0.912908  [   96/  126]
train() client id: f_00001-0-0 loss: 0.509894  [   32/  265]
train() client id: f_00001-0-1 loss: 0.565339  [   64/  265]
train() client id: f_00001-0-2 loss: 0.484553  [   96/  265]
train() client id: f_00001-0-3 loss: 0.506594  [  128/  265]
train() client id: f_00001-0-4 loss: 0.390883  [  160/  265]
train() client id: f_00001-0-5 loss: 0.591677  [  192/  265]
train() client id: f_00001-0-6 loss: 0.577856  [  224/  265]
train() client id: f_00001-0-7 loss: 0.421816  [  256/  265]
train() client id: f_00001-1-0 loss: 0.435159  [   32/  265]
train() client id: f_00001-1-1 loss: 0.417065  [   64/  265]
train() client id: f_00001-1-2 loss: 0.607773  [   96/  265]
train() client id: f_00001-1-3 loss: 0.637562  [  128/  265]
train() client id: f_00001-1-4 loss: 0.415638  [  160/  265]
train() client id: f_00001-1-5 loss: 0.512949  [  192/  265]
train() client id: f_00001-1-6 loss: 0.576332  [  224/  265]
train() client id: f_00001-1-7 loss: 0.390536  [  256/  265]
train() client id: f_00001-2-0 loss: 0.400470  [   32/  265]
train() client id: f_00001-2-1 loss: 0.493282  [   64/  265]
train() client id: f_00001-2-2 loss: 0.509139  [   96/  265]
train() client id: f_00001-2-3 loss: 0.399020  [  128/  265]
train() client id: f_00001-2-4 loss: 0.483921  [  160/  265]
train() client id: f_00001-2-5 loss: 0.524970  [  192/  265]
train() client id: f_00001-2-6 loss: 0.635686  [  224/  265]
train() client id: f_00001-2-7 loss: 0.471445  [  256/  265]
train() client id: f_00001-3-0 loss: 0.564072  [   32/  265]
train() client id: f_00001-3-1 loss: 0.490129  [   64/  265]
train() client id: f_00001-3-2 loss: 0.484593  [   96/  265]
train() client id: f_00001-3-3 loss: 0.512343  [  128/  265]
train() client id: f_00001-3-4 loss: 0.445045  [  160/  265]
train() client id: f_00001-3-5 loss: 0.413225  [  192/  265]
train() client id: f_00001-3-6 loss: 0.508581  [  224/  265]
train() client id: f_00001-3-7 loss: 0.501178  [  256/  265]
train() client id: f_00001-4-0 loss: 0.566058  [   32/  265]
train() client id: f_00001-4-1 loss: 0.394745  [   64/  265]
train() client id: f_00001-4-2 loss: 0.528567  [   96/  265]
train() client id: f_00001-4-3 loss: 0.549728  [  128/  265]
train() client id: f_00001-4-4 loss: 0.493887  [  160/  265]
train() client id: f_00001-4-5 loss: 0.435669  [  192/  265]
train() client id: f_00001-4-6 loss: 0.437167  [  224/  265]
train() client id: f_00001-4-7 loss: 0.466365  [  256/  265]
train() client id: f_00001-5-0 loss: 0.505488  [   32/  265]
train() client id: f_00001-5-1 loss: 0.516808  [   64/  265]
train() client id: f_00001-5-2 loss: 0.401770  [   96/  265]
train() client id: f_00001-5-3 loss: 0.372140  [  128/  265]
train() client id: f_00001-5-4 loss: 0.439695  [  160/  265]
train() client id: f_00001-5-5 loss: 0.646208  [  192/  265]
train() client id: f_00001-5-6 loss: 0.598886  [  224/  265]
train() client id: f_00001-5-7 loss: 0.382733  [  256/  265]
train() client id: f_00001-6-0 loss: 0.479596  [   32/  265]
train() client id: f_00001-6-1 loss: 0.537447  [   64/  265]
train() client id: f_00001-6-2 loss: 0.391566  [   96/  265]
train() client id: f_00001-6-3 loss: 0.416894  [  128/  265]
train() client id: f_00001-6-4 loss: 0.582431  [  160/  265]
train() client id: f_00001-6-5 loss: 0.545830  [  192/  265]
train() client id: f_00001-6-6 loss: 0.451572  [  224/  265]
train() client id: f_00001-6-7 loss: 0.436307  [  256/  265]
train() client id: f_00001-7-0 loss: 0.479603  [   32/  265]
train() client id: f_00001-7-1 loss: 0.571369  [   64/  265]
train() client id: f_00001-7-2 loss: 0.459730  [   96/  265]
train() client id: f_00001-7-3 loss: 0.405492  [  128/  265]
train() client id: f_00001-7-4 loss: 0.448742  [  160/  265]
train() client id: f_00001-7-5 loss: 0.610066  [  192/  265]
train() client id: f_00001-7-6 loss: 0.428587  [  224/  265]
train() client id: f_00001-7-7 loss: 0.432480  [  256/  265]
train() client id: f_00001-8-0 loss: 0.392174  [   32/  265]
train() client id: f_00001-8-1 loss: 0.497736  [   64/  265]
train() client id: f_00001-8-2 loss: 0.518114  [   96/  265]
train() client id: f_00001-8-3 loss: 0.482110  [  128/  265]
train() client id: f_00001-8-4 loss: 0.480002  [  160/  265]
train() client id: f_00001-8-5 loss: 0.528710  [  192/  265]
train() client id: f_00001-8-6 loss: 0.381727  [  224/  265]
train() client id: f_00001-8-7 loss: 0.480879  [  256/  265]
train() client id: f_00001-9-0 loss: 0.474338  [   32/  265]
train() client id: f_00001-9-1 loss: 0.391407  [   64/  265]
train() client id: f_00001-9-2 loss: 0.504764  [   96/  265]
train() client id: f_00001-9-3 loss: 0.414120  [  128/  265]
train() client id: f_00001-9-4 loss: 0.549497  [  160/  265]
train() client id: f_00001-9-5 loss: 0.372544  [  192/  265]
train() client id: f_00001-9-6 loss: 0.472503  [  224/  265]
train() client id: f_00001-9-7 loss: 0.653205  [  256/  265]
train() client id: f_00001-10-0 loss: 0.472949  [   32/  265]
train() client id: f_00001-10-1 loss: 0.532549  [   64/  265]
train() client id: f_00001-10-2 loss: 0.452956  [   96/  265]
train() client id: f_00001-10-3 loss: 0.436255  [  128/  265]
train() client id: f_00001-10-4 loss: 0.486405  [  160/  265]
train() client id: f_00001-10-5 loss: 0.443149  [  192/  265]
train() client id: f_00001-10-6 loss: 0.511539  [  224/  265]
train() client id: f_00001-10-7 loss: 0.478052  [  256/  265]
train() client id: f_00001-11-0 loss: 0.388274  [   32/  265]
train() client id: f_00001-11-1 loss: 0.506076  [   64/  265]
train() client id: f_00001-11-2 loss: 0.440620  [   96/  265]
train() client id: f_00001-11-3 loss: 0.482991  [  128/  265]
train() client id: f_00001-11-4 loss: 0.660642  [  160/  265]
train() client id: f_00001-11-5 loss: 0.385460  [  192/  265]
train() client id: f_00001-11-6 loss: 0.564938  [  224/  265]
train() client id: f_00001-11-7 loss: 0.387337  [  256/  265]
train() client id: f_00001-12-0 loss: 0.463533  [   32/  265]
train() client id: f_00001-12-1 loss: 0.435222  [   64/  265]
train() client id: f_00001-12-2 loss: 0.595806  [   96/  265]
train() client id: f_00001-12-3 loss: 0.533903  [  128/  265]
train() client id: f_00001-12-4 loss: 0.387574  [  160/  265]
train() client id: f_00001-12-5 loss: 0.528879  [  192/  265]
train() client id: f_00001-12-6 loss: 0.425688  [  224/  265]
train() client id: f_00001-12-7 loss: 0.452338  [  256/  265]
train() client id: f_00001-13-0 loss: 0.381260  [   32/  265]
train() client id: f_00001-13-1 loss: 0.412496  [   64/  265]
train() client id: f_00001-13-2 loss: 0.510280  [   96/  265]
train() client id: f_00001-13-3 loss: 0.389933  [  128/  265]
train() client id: f_00001-13-4 loss: 0.446720  [  160/  265]
train() client id: f_00001-13-5 loss: 0.574896  [  192/  265]
train() client id: f_00001-13-6 loss: 0.515599  [  224/  265]
train() client id: f_00001-13-7 loss: 0.592908  [  256/  265]
train() client id: f_00001-14-0 loss: 0.589392  [   32/  265]
train() client id: f_00001-14-1 loss: 0.426678  [   64/  265]
train() client id: f_00001-14-2 loss: 0.538293  [   96/  265]
train() client id: f_00001-14-3 loss: 0.643037  [  128/  265]
train() client id: f_00001-14-4 loss: 0.366202  [  160/  265]
train() client id: f_00001-14-5 loss: 0.375339  [  192/  265]
train() client id: f_00001-14-6 loss: 0.422244  [  224/  265]
train() client id: f_00001-14-7 loss: 0.469785  [  256/  265]
train() client id: f_00002-0-0 loss: 1.151607  [   32/  124]
train() client id: f_00002-0-1 loss: 1.113365  [   64/  124]
train() client id: f_00002-0-2 loss: 1.140615  [   96/  124]
train() client id: f_00002-1-0 loss: 1.188785  [   32/  124]
train() client id: f_00002-1-1 loss: 1.010298  [   64/  124]
train() client id: f_00002-1-2 loss: 1.082357  [   96/  124]
train() client id: f_00002-2-0 loss: 0.978563  [   32/  124]
train() client id: f_00002-2-1 loss: 1.063674  [   64/  124]
train() client id: f_00002-2-2 loss: 1.181925  [   96/  124]
train() client id: f_00002-3-0 loss: 0.969641  [   32/  124]
train() client id: f_00002-3-1 loss: 1.150754  [   64/  124]
train() client id: f_00002-3-2 loss: 1.184477  [   96/  124]
train() client id: f_00002-4-0 loss: 1.097973  [   32/  124]
train() client id: f_00002-4-1 loss: 1.102272  [   64/  124]
train() client id: f_00002-4-2 loss: 0.957728  [   96/  124]
train() client id: f_00002-5-0 loss: 0.962176  [   32/  124]
train() client id: f_00002-5-1 loss: 1.029674  [   64/  124]
train() client id: f_00002-5-2 loss: 1.108986  [   96/  124]
train() client id: f_00002-6-0 loss: 1.087291  [   32/  124]
train() client id: f_00002-6-1 loss: 1.002987  [   64/  124]
train() client id: f_00002-6-2 loss: 1.050676  [   96/  124]
train() client id: f_00002-7-0 loss: 1.128638  [   32/  124]
train() client id: f_00002-7-1 loss: 0.977561  [   64/  124]
train() client id: f_00002-7-2 loss: 0.910945  [   96/  124]
train() client id: f_00002-8-0 loss: 0.951661  [   32/  124]
train() client id: f_00002-8-1 loss: 0.889615  [   64/  124]
train() client id: f_00002-8-2 loss: 1.124868  [   96/  124]
train() client id: f_00002-9-0 loss: 0.839630  [   32/  124]
train() client id: f_00002-9-1 loss: 1.012748  [   64/  124]
train() client id: f_00002-9-2 loss: 1.090473  [   96/  124]
train() client id: f_00002-10-0 loss: 1.080550  [   32/  124]
train() client id: f_00002-10-1 loss: 0.977982  [   64/  124]
train() client id: f_00002-10-2 loss: 0.905102  [   96/  124]
train() client id: f_00002-11-0 loss: 0.874020  [   32/  124]
train() client id: f_00002-11-1 loss: 1.205806  [   64/  124]
train() client id: f_00002-11-2 loss: 0.939914  [   96/  124]
train() client id: f_00002-12-0 loss: 0.866666  [   32/  124]
train() client id: f_00002-12-1 loss: 0.902691  [   64/  124]
train() client id: f_00002-12-2 loss: 1.132165  [   96/  124]
train() client id: f_00002-13-0 loss: 0.954369  [   32/  124]
train() client id: f_00002-13-1 loss: 1.017572  [   64/  124]
train() client id: f_00002-13-2 loss: 0.898459  [   96/  124]
train() client id: f_00002-14-0 loss: 0.946898  [   32/  124]
train() client id: f_00002-14-1 loss: 1.016519  [   64/  124]
train() client id: f_00002-14-2 loss: 0.945461  [   96/  124]
train() client id: f_00003-0-0 loss: 0.718188  [   32/   43]
train() client id: f_00003-1-0 loss: 0.892376  [   32/   43]
train() client id: f_00003-2-0 loss: 0.800583  [   32/   43]
train() client id: f_00003-3-0 loss: 0.966637  [   32/   43]
train() client id: f_00003-4-0 loss: 0.917311  [   32/   43]
train() client id: f_00003-5-0 loss: 0.866849  [   32/   43]
train() client id: f_00003-6-0 loss: 0.668641  [   32/   43]
train() client id: f_00003-7-0 loss: 0.684635  [   32/   43]
train() client id: f_00003-8-0 loss: 0.820789  [   32/   43]
train() client id: f_00003-9-0 loss: 0.647329  [   32/   43]
train() client id: f_00003-10-0 loss: 0.794135  [   32/   43]
train() client id: f_00003-11-0 loss: 0.669373  [   32/   43]
train() client id: f_00003-12-0 loss: 0.828655  [   32/   43]
train() client id: f_00003-13-0 loss: 0.789727  [   32/   43]
train() client id: f_00003-14-0 loss: 0.780894  [   32/   43]
train() client id: f_00004-0-0 loss: 0.665949  [   32/  306]
train() client id: f_00004-0-1 loss: 0.559348  [   64/  306]
train() client id: f_00004-0-2 loss: 0.726129  [   96/  306]
train() client id: f_00004-0-3 loss: 0.644796  [  128/  306]
train() client id: f_00004-0-4 loss: 0.624862  [  160/  306]
train() client id: f_00004-0-5 loss: 0.714901  [  192/  306]
train() client id: f_00004-0-6 loss: 0.717713  [  224/  306]
train() client id: f_00004-0-7 loss: 0.839461  [  256/  306]
train() client id: f_00004-0-8 loss: 0.815218  [  288/  306]
train() client id: f_00004-1-0 loss: 0.679740  [   32/  306]
train() client id: f_00004-1-1 loss: 0.645352  [   64/  306]
train() client id: f_00004-1-2 loss: 0.784688  [   96/  306]
train() client id: f_00004-1-3 loss: 0.527098  [  128/  306]
train() client id: f_00004-1-4 loss: 0.696163  [  160/  306]
train() client id: f_00004-1-5 loss: 0.705103  [  192/  306]
train() client id: f_00004-1-6 loss: 0.601135  [  224/  306]
train() client id: f_00004-1-7 loss: 0.786367  [  256/  306]
train() client id: f_00004-1-8 loss: 0.750420  [  288/  306]
train() client id: f_00004-2-0 loss: 0.803407  [   32/  306]
train() client id: f_00004-2-1 loss: 0.664627  [   64/  306]
train() client id: f_00004-2-2 loss: 0.710696  [   96/  306]
train() client id: f_00004-2-3 loss: 0.654938  [  128/  306]
train() client id: f_00004-2-4 loss: 0.618723  [  160/  306]
train() client id: f_00004-2-5 loss: 0.662341  [  192/  306]
train() client id: f_00004-2-6 loss: 0.776955  [  224/  306]
train() client id: f_00004-2-7 loss: 0.657666  [  256/  306]
train() client id: f_00004-2-8 loss: 0.698875  [  288/  306]
train() client id: f_00004-3-0 loss: 0.782109  [   32/  306]
train() client id: f_00004-3-1 loss: 0.698608  [   64/  306]
train() client id: f_00004-3-2 loss: 0.599586  [   96/  306]
train() client id: f_00004-3-3 loss: 0.816014  [  128/  306]
train() client id: f_00004-3-4 loss: 0.594566  [  160/  306]
train() client id: f_00004-3-5 loss: 0.774801  [  192/  306]
train() client id: f_00004-3-6 loss: 0.658353  [  224/  306]
train() client id: f_00004-3-7 loss: 0.666090  [  256/  306]
train() client id: f_00004-3-8 loss: 0.702165  [  288/  306]
train() client id: f_00004-4-0 loss: 0.747476  [   32/  306]
train() client id: f_00004-4-1 loss: 0.752986  [   64/  306]
train() client id: f_00004-4-2 loss: 0.600571  [   96/  306]
train() client id: f_00004-4-3 loss: 0.691828  [  128/  306]
train() client id: f_00004-4-4 loss: 0.798991  [  160/  306]
train() client id: f_00004-4-5 loss: 0.628532  [  192/  306]
train() client id: f_00004-4-6 loss: 0.649826  [  224/  306]
train() client id: f_00004-4-7 loss: 0.665138  [  256/  306]
train() client id: f_00004-4-8 loss: 0.755880  [  288/  306]
train() client id: f_00004-5-0 loss: 0.637162  [   32/  306]
train() client id: f_00004-5-1 loss: 0.634929  [   64/  306]
train() client id: f_00004-5-2 loss: 0.826985  [   96/  306]
train() client id: f_00004-5-3 loss: 0.731370  [  128/  306]
train() client id: f_00004-5-4 loss: 0.730496  [  160/  306]
train() client id: f_00004-5-5 loss: 0.723578  [  192/  306]
train() client id: f_00004-5-6 loss: 0.691369  [  224/  306]
train() client id: f_00004-5-7 loss: 0.785865  [  256/  306]
train() client id: f_00004-5-8 loss: 0.639803  [  288/  306]
train() client id: f_00004-6-0 loss: 0.795590  [   32/  306]
train() client id: f_00004-6-1 loss: 0.613283  [   64/  306]
train() client id: f_00004-6-2 loss: 0.605541  [   96/  306]
train() client id: f_00004-6-3 loss: 0.816505  [  128/  306]
train() client id: f_00004-6-4 loss: 0.744005  [  160/  306]
train() client id: f_00004-6-5 loss: 0.683444  [  192/  306]
train() client id: f_00004-6-6 loss: 0.747903  [  224/  306]
train() client id: f_00004-6-7 loss: 0.668431  [  256/  306]
train() client id: f_00004-6-8 loss: 0.637566  [  288/  306]
train() client id: f_00004-7-0 loss: 0.619686  [   32/  306]
train() client id: f_00004-7-1 loss: 0.675545  [   64/  306]
train() client id: f_00004-7-2 loss: 0.710972  [   96/  306]
train() client id: f_00004-7-3 loss: 0.801802  [  128/  306]
train() client id: f_00004-7-4 loss: 0.739046  [  160/  306]
train() client id: f_00004-7-5 loss: 0.773559  [  192/  306]
train() client id: f_00004-7-6 loss: 0.718038  [  224/  306]
train() client id: f_00004-7-7 loss: 0.592856  [  256/  306]
train() client id: f_00004-7-8 loss: 0.624024  [  288/  306]
train() client id: f_00004-8-0 loss: 0.655300  [   32/  306]
train() client id: f_00004-8-1 loss: 0.668586  [   64/  306]
train() client id: f_00004-8-2 loss: 0.793823  [   96/  306]
train() client id: f_00004-8-3 loss: 0.716762  [  128/  306]
train() client id: f_00004-8-4 loss: 0.694908  [  160/  306]
train() client id: f_00004-8-5 loss: 0.649390  [  192/  306]
train() client id: f_00004-8-6 loss: 0.646099  [  224/  306]
train() client id: f_00004-8-7 loss: 0.734666  [  256/  306]
train() client id: f_00004-8-8 loss: 0.861463  [  288/  306]
train() client id: f_00004-9-0 loss: 0.738872  [   32/  306]
train() client id: f_00004-9-1 loss: 0.685947  [   64/  306]
train() client id: f_00004-9-2 loss: 0.662084  [   96/  306]
train() client id: f_00004-9-3 loss: 0.684266  [  128/  306]
train() client id: f_00004-9-4 loss: 0.660984  [  160/  306]
train() client id: f_00004-9-5 loss: 0.721035  [  192/  306]
train() client id: f_00004-9-6 loss: 0.816721  [  224/  306]
train() client id: f_00004-9-7 loss: 0.701946  [  256/  306]
train() client id: f_00004-9-8 loss: 0.765458  [  288/  306]
train() client id: f_00004-10-0 loss: 0.685732  [   32/  306]
train() client id: f_00004-10-1 loss: 0.779753  [   64/  306]
train() client id: f_00004-10-2 loss: 0.701485  [   96/  306]
train() client id: f_00004-10-3 loss: 0.764676  [  128/  306]
train() client id: f_00004-10-4 loss: 0.757492  [  160/  306]
train() client id: f_00004-10-5 loss: 0.621541  [  192/  306]
train() client id: f_00004-10-6 loss: 0.740053  [  224/  306]
train() client id: f_00004-10-7 loss: 0.683616  [  256/  306]
train() client id: f_00004-10-8 loss: 0.688498  [  288/  306]
train() client id: f_00004-11-0 loss: 0.686086  [   32/  306]
train() client id: f_00004-11-1 loss: 0.696089  [   64/  306]
train() client id: f_00004-11-2 loss: 0.705021  [   96/  306]
train() client id: f_00004-11-3 loss: 0.719218  [  128/  306]
train() client id: f_00004-11-4 loss: 0.687544  [  160/  306]
train() client id: f_00004-11-5 loss: 0.681579  [  192/  306]
train() client id: f_00004-11-6 loss: 0.684240  [  224/  306]
train() client id: f_00004-11-7 loss: 0.746052  [  256/  306]
train() client id: f_00004-11-8 loss: 0.835123  [  288/  306]
train() client id: f_00004-12-0 loss: 0.637976  [   32/  306]
train() client id: f_00004-12-1 loss: 0.777477  [   64/  306]
train() client id: f_00004-12-2 loss: 0.853428  [   96/  306]
train() client id: f_00004-12-3 loss: 0.589579  [  128/  306]
train() client id: f_00004-12-4 loss: 0.734552  [  160/  306]
train() client id: f_00004-12-5 loss: 0.730812  [  192/  306]
train() client id: f_00004-12-6 loss: 0.702522  [  224/  306]
train() client id: f_00004-12-7 loss: 0.717985  [  256/  306]
train() client id: f_00004-12-8 loss: 0.743611  [  288/  306]
train() client id: f_00004-13-0 loss: 0.687401  [   32/  306]
train() client id: f_00004-13-1 loss: 0.593370  [   64/  306]
train() client id: f_00004-13-2 loss: 0.712591  [   96/  306]
train() client id: f_00004-13-3 loss: 0.776395  [  128/  306]
train() client id: f_00004-13-4 loss: 0.762807  [  160/  306]
train() client id: f_00004-13-5 loss: 0.611784  [  192/  306]
train() client id: f_00004-13-6 loss: 0.802538  [  224/  306]
train() client id: f_00004-13-7 loss: 0.754811  [  256/  306]
train() client id: f_00004-13-8 loss: 0.674418  [  288/  306]
train() client id: f_00004-14-0 loss: 0.620903  [   32/  306]
train() client id: f_00004-14-1 loss: 0.681004  [   64/  306]
train() client id: f_00004-14-2 loss: 0.712558  [   96/  306]
train() client id: f_00004-14-3 loss: 0.695746  [  128/  306]
train() client id: f_00004-14-4 loss: 0.750757  [  160/  306]
train() client id: f_00004-14-5 loss: 0.758821  [  192/  306]
train() client id: f_00004-14-6 loss: 0.668286  [  224/  306]
train() client id: f_00004-14-7 loss: 0.863653  [  256/  306]
train() client id: f_00004-14-8 loss: 0.662220  [  288/  306]
train() client id: f_00005-0-0 loss: 0.383261  [   32/  146]
train() client id: f_00005-0-1 loss: 0.717896  [   64/  146]
train() client id: f_00005-0-2 loss: 0.548056  [   96/  146]
train() client id: f_00005-0-3 loss: 0.637163  [  128/  146]
train() client id: f_00005-1-0 loss: 0.562072  [   32/  146]
train() client id: f_00005-1-1 loss: 0.751966  [   64/  146]
train() client id: f_00005-1-2 loss: 0.622659  [   96/  146]
train() client id: f_00005-1-3 loss: 0.425407  [  128/  146]
train() client id: f_00005-2-0 loss: 0.393193  [   32/  146]
train() client id: f_00005-2-1 loss: 0.495864  [   64/  146]
train() client id: f_00005-2-2 loss: 0.765226  [   96/  146]
train() client id: f_00005-2-3 loss: 0.586187  [  128/  146]
train() client id: f_00005-3-0 loss: 0.643151  [   32/  146]
train() client id: f_00005-3-1 loss: 0.576298  [   64/  146]
train() client id: f_00005-3-2 loss: 0.485056  [   96/  146]
train() client id: f_00005-3-3 loss: 0.638277  [  128/  146]
train() client id: f_00005-4-0 loss: 0.539337  [   32/  146]
train() client id: f_00005-4-1 loss: 0.670055  [   64/  146]
train() client id: f_00005-4-2 loss: 0.654466  [   96/  146]
train() client id: f_00005-4-3 loss: 0.765954  [  128/  146]
train() client id: f_00005-5-0 loss: 0.459457  [   32/  146]
train() client id: f_00005-5-1 loss: 0.491563  [   64/  146]
train() client id: f_00005-5-2 loss: 0.608265  [   96/  146]
train() client id: f_00005-5-3 loss: 0.622856  [  128/  146]
train() client id: f_00005-6-0 loss: 0.865948  [   32/  146]
train() client id: f_00005-6-1 loss: 0.573499  [   64/  146]
train() client id: f_00005-6-2 loss: 0.477743  [   96/  146]
train() client id: f_00005-6-3 loss: 0.531554  [  128/  146]
train() client id: f_00005-7-0 loss: 0.755024  [   32/  146]
train() client id: f_00005-7-1 loss: 0.618619  [   64/  146]
train() client id: f_00005-7-2 loss: 0.641989  [   96/  146]
train() client id: f_00005-7-3 loss: 0.531145  [  128/  146]
train() client id: f_00005-8-0 loss: 0.595207  [   32/  146]
train() client id: f_00005-8-1 loss: 0.644187  [   64/  146]
train() client id: f_00005-8-2 loss: 0.600188  [   96/  146]
train() client id: f_00005-8-3 loss: 0.689163  [  128/  146]
train() client id: f_00005-9-0 loss: 0.562065  [   32/  146]
train() client id: f_00005-9-1 loss: 0.673132  [   64/  146]
train() client id: f_00005-9-2 loss: 0.778815  [   96/  146]
train() client id: f_00005-9-3 loss: 0.616626  [  128/  146]
train() client id: f_00005-10-0 loss: 0.687775  [   32/  146]
train() client id: f_00005-10-1 loss: 0.815888  [   64/  146]
train() client id: f_00005-10-2 loss: 0.581903  [   96/  146]
train() client id: f_00005-10-3 loss: 0.508885  [  128/  146]
train() client id: f_00005-11-0 loss: 0.337685  [   32/  146]
train() client id: f_00005-11-1 loss: 0.616387  [   64/  146]
train() client id: f_00005-11-2 loss: 0.784873  [   96/  146]
train() client id: f_00005-11-3 loss: 0.777603  [  128/  146]
train() client id: f_00005-12-0 loss: 0.558584  [   32/  146]
train() client id: f_00005-12-1 loss: 0.495033  [   64/  146]
train() client id: f_00005-12-2 loss: 0.696939  [   96/  146]
train() client id: f_00005-12-3 loss: 0.787156  [  128/  146]
train() client id: f_00005-13-0 loss: 0.575031  [   32/  146]
train() client id: f_00005-13-1 loss: 0.457612  [   64/  146]
train() client id: f_00005-13-2 loss: 0.808103  [   96/  146]
train() client id: f_00005-13-3 loss: 0.571268  [  128/  146]
train() client id: f_00005-14-0 loss: 0.768438  [   32/  146]
train() client id: f_00005-14-1 loss: 0.709786  [   64/  146]
train() client id: f_00005-14-2 loss: 0.309680  [   96/  146]
train() client id: f_00005-14-3 loss: 0.615246  [  128/  146]
train() client id: f_00006-0-0 loss: 0.516123  [   32/   54]
train() client id: f_00006-1-0 loss: 0.481576  [   32/   54]
train() client id: f_00006-2-0 loss: 0.486713  [   32/   54]
train() client id: f_00006-3-0 loss: 0.511678  [   32/   54]
train() client id: f_00006-4-0 loss: 0.504957  [   32/   54]
train() client id: f_00006-5-0 loss: 0.522337  [   32/   54]
train() client id: f_00006-6-0 loss: 0.454627  [   32/   54]
train() client id: f_00006-7-0 loss: 0.509439  [   32/   54]
train() client id: f_00006-8-0 loss: 0.442674  [   32/   54]
train() client id: f_00006-9-0 loss: 0.457436  [   32/   54]
train() client id: f_00006-10-0 loss: 0.403992  [   32/   54]
train() client id: f_00006-11-0 loss: 0.452555  [   32/   54]
train() client id: f_00006-12-0 loss: 0.460487  [   32/   54]
train() client id: f_00006-13-0 loss: 0.489377  [   32/   54]
train() client id: f_00006-14-0 loss: 0.516956  [   32/   54]
train() client id: f_00007-0-0 loss: 0.570367  [   32/  179]
train() client id: f_00007-0-1 loss: 0.634591  [   64/  179]
train() client id: f_00007-0-2 loss: 0.640927  [   96/  179]
train() client id: f_00007-0-3 loss: 0.722747  [  128/  179]
train() client id: f_00007-0-4 loss: 0.608220  [  160/  179]
train() client id: f_00007-1-0 loss: 0.510912  [   32/  179]
train() client id: f_00007-1-1 loss: 0.601500  [   64/  179]
train() client id: f_00007-1-2 loss: 0.775959  [   96/  179]
train() client id: f_00007-1-3 loss: 0.708173  [  128/  179]
train() client id: f_00007-1-4 loss: 0.503751  [  160/  179]
train() client id: f_00007-2-0 loss: 0.752321  [   32/  179]
train() client id: f_00007-2-1 loss: 0.509821  [   64/  179]
train() client id: f_00007-2-2 loss: 0.602459  [   96/  179]
train() client id: f_00007-2-3 loss: 0.590650  [  128/  179]
train() client id: f_00007-2-4 loss: 0.634899  [  160/  179]
train() client id: f_00007-3-0 loss: 0.539881  [   32/  179]
train() client id: f_00007-3-1 loss: 0.634277  [   64/  179]
train() client id: f_00007-3-2 loss: 0.639938  [   96/  179]
train() client id: f_00007-3-3 loss: 0.696802  [  128/  179]
train() client id: f_00007-3-4 loss: 0.516855  [  160/  179]
train() client id: f_00007-4-0 loss: 0.388032  [   32/  179]
train() client id: f_00007-4-1 loss: 0.578996  [   64/  179]
train() client id: f_00007-4-2 loss: 0.654716  [   96/  179]
train() client id: f_00007-4-3 loss: 0.542625  [  128/  179]
train() client id: f_00007-4-4 loss: 0.849887  [  160/  179]
train() client id: f_00007-5-0 loss: 0.617639  [   32/  179]
train() client id: f_00007-5-1 loss: 0.570190  [   64/  179]
train() client id: f_00007-5-2 loss: 0.671965  [   96/  179]
train() client id: f_00007-5-3 loss: 0.639582  [  128/  179]
train() client id: f_00007-5-4 loss: 0.428670  [  160/  179]
train() client id: f_00007-6-0 loss: 0.794776  [   32/  179]
train() client id: f_00007-6-1 loss: 0.623560  [   64/  179]
train() client id: f_00007-6-2 loss: 0.406826  [   96/  179]
train() client id: f_00007-6-3 loss: 0.498682  [  128/  179]
train() client id: f_00007-6-4 loss: 0.520187  [  160/  179]
train() client id: f_00007-7-0 loss: 0.409407  [   32/  179]
train() client id: f_00007-7-1 loss: 0.701016  [   64/  179]
train() client id: f_00007-7-2 loss: 0.603743  [   96/  179]
train() client id: f_00007-7-3 loss: 0.406370  [  128/  179]
train() client id: f_00007-7-4 loss: 0.685015  [  160/  179]
train() client id: f_00007-8-0 loss: 0.422952  [   32/  179]
train() client id: f_00007-8-1 loss: 0.692856  [   64/  179]
train() client id: f_00007-8-2 loss: 0.534816  [   96/  179]
train() client id: f_00007-8-3 loss: 0.654761  [  128/  179]
train() client id: f_00007-8-4 loss: 0.597448  [  160/  179]
train() client id: f_00007-9-0 loss: 0.925448  [   32/  179]
train() client id: f_00007-9-1 loss: 0.483852  [   64/  179]
train() client id: f_00007-9-2 loss: 0.619217  [   96/  179]
train() client id: f_00007-9-3 loss: 0.429649  [  128/  179]
train() client id: f_00007-9-4 loss: 0.437188  [  160/  179]
train() client id: f_00007-10-0 loss: 0.656157  [   32/  179]
train() client id: f_00007-10-1 loss: 0.510889  [   64/  179]
train() client id: f_00007-10-2 loss: 0.619840  [   96/  179]
train() client id: f_00007-10-3 loss: 0.481732  [  128/  179]
train() client id: f_00007-10-4 loss: 0.590954  [  160/  179]
train() client id: f_00007-11-0 loss: 0.444708  [   32/  179]
train() client id: f_00007-11-1 loss: 0.559267  [   64/  179]
train() client id: f_00007-11-2 loss: 0.645702  [   96/  179]
train() client id: f_00007-11-3 loss: 0.525718  [  128/  179]
train() client id: f_00007-11-4 loss: 0.702362  [  160/  179]
train() client id: f_00007-12-0 loss: 0.538653  [   32/  179]
train() client id: f_00007-12-1 loss: 0.658082  [   64/  179]
train() client id: f_00007-12-2 loss: 0.571225  [   96/  179]
train() client id: f_00007-12-3 loss: 0.503128  [  128/  179]
train() client id: f_00007-12-4 loss: 0.604404  [  160/  179]
train() client id: f_00007-13-0 loss: 0.417463  [   32/  179]
train() client id: f_00007-13-1 loss: 0.497444  [   64/  179]
train() client id: f_00007-13-2 loss: 0.792483  [   96/  179]
train() client id: f_00007-13-3 loss: 0.502902  [  128/  179]
train() client id: f_00007-13-4 loss: 0.645666  [  160/  179]
train() client id: f_00007-14-0 loss: 0.497221  [   32/  179]
train() client id: f_00007-14-1 loss: 0.563757  [   64/  179]
train() client id: f_00007-14-2 loss: 0.691874  [   96/  179]
train() client id: f_00007-14-3 loss: 0.569291  [  128/  179]
train() client id: f_00007-14-4 loss: 0.501179  [  160/  179]
train() client id: f_00008-0-0 loss: 0.821928  [   32/  130]
train() client id: f_00008-0-1 loss: 0.829898  [   64/  130]
train() client id: f_00008-0-2 loss: 0.819330  [   96/  130]
train() client id: f_00008-0-3 loss: 0.686747  [  128/  130]
train() client id: f_00008-1-0 loss: 0.840133  [   32/  130]
train() client id: f_00008-1-1 loss: 0.822526  [   64/  130]
train() client id: f_00008-1-2 loss: 0.841405  [   96/  130]
train() client id: f_00008-1-3 loss: 0.690364  [  128/  130]
train() client id: f_00008-2-0 loss: 0.774938  [   32/  130]
train() client id: f_00008-2-1 loss: 0.824042  [   64/  130]
train() client id: f_00008-2-2 loss: 0.808185  [   96/  130]
train() client id: f_00008-2-3 loss: 0.740623  [  128/  130]
train() client id: f_00008-3-0 loss: 0.818190  [   32/  130]
train() client id: f_00008-3-1 loss: 0.785345  [   64/  130]
train() client id: f_00008-3-2 loss: 0.781298  [   96/  130]
train() client id: f_00008-3-3 loss: 0.795738  [  128/  130]
train() client id: f_00008-4-0 loss: 0.690428  [   32/  130]
train() client id: f_00008-4-1 loss: 0.749265  [   64/  130]
train() client id: f_00008-4-2 loss: 0.816478  [   96/  130]
train() client id: f_00008-4-3 loss: 0.909002  [  128/  130]
train() client id: f_00008-5-0 loss: 0.789409  [   32/  130]
train() client id: f_00008-5-1 loss: 0.817400  [   64/  130]
train() client id: f_00008-5-2 loss: 0.777168  [   96/  130]
train() client id: f_00008-5-3 loss: 0.814181  [  128/  130]
train() client id: f_00008-6-0 loss: 0.739511  [   32/  130]
train() client id: f_00008-6-1 loss: 0.842154  [   64/  130]
train() client id: f_00008-6-2 loss: 0.747158  [   96/  130]
train() client id: f_00008-6-3 loss: 0.830284  [  128/  130]
train() client id: f_00008-7-0 loss: 0.681024  [   32/  130]
train() client id: f_00008-7-1 loss: 0.786853  [   64/  130]
train() client id: f_00008-7-2 loss: 0.868874  [   96/  130]
train() client id: f_00008-7-3 loss: 0.848959  [  128/  130]
train() client id: f_00008-8-0 loss: 0.747641  [   32/  130]
train() client id: f_00008-8-1 loss: 0.766713  [   64/  130]
train() client id: f_00008-8-2 loss: 0.862706  [   96/  130]
train() client id: f_00008-8-3 loss: 0.827205  [  128/  130]
train() client id: f_00008-9-0 loss: 0.899644  [   32/  130]
train() client id: f_00008-9-1 loss: 0.724016  [   64/  130]
train() client id: f_00008-9-2 loss: 0.730961  [   96/  130]
train() client id: f_00008-9-3 loss: 0.812748  [  128/  130]
train() client id: f_00008-10-0 loss: 0.806219  [   32/  130]
train() client id: f_00008-10-1 loss: 0.747491  [   64/  130]
train() client id: f_00008-10-2 loss: 0.900864  [   96/  130]
train() client id: f_00008-10-3 loss: 0.728528  [  128/  130]
train() client id: f_00008-11-0 loss: 0.783467  [   32/  130]
train() client id: f_00008-11-1 loss: 0.834491  [   64/  130]
train() client id: f_00008-11-2 loss: 0.797404  [   96/  130]
train() client id: f_00008-11-3 loss: 0.789889  [  128/  130]
train() client id: f_00008-12-0 loss: 0.741754  [   32/  130]
train() client id: f_00008-12-1 loss: 0.767860  [   64/  130]
train() client id: f_00008-12-2 loss: 0.810001  [   96/  130]
train() client id: f_00008-12-3 loss: 0.874577  [  128/  130]
train() client id: f_00008-13-0 loss: 0.835877  [   32/  130]
train() client id: f_00008-13-1 loss: 0.705594  [   64/  130]
train() client id: f_00008-13-2 loss: 0.758813  [   96/  130]
train() client id: f_00008-13-3 loss: 0.870961  [  128/  130]
train() client id: f_00008-14-0 loss: 0.854391  [   32/  130]
train() client id: f_00008-14-1 loss: 0.743652  [   64/  130]
train() client id: f_00008-14-2 loss: 0.822245  [   96/  130]
train() client id: f_00008-14-3 loss: 0.786350  [  128/  130]
train() client id: f_00009-0-0 loss: 1.115621  [   32/  118]
train() client id: f_00009-0-1 loss: 1.141615  [   64/  118]
train() client id: f_00009-0-2 loss: 0.979810  [   96/  118]
train() client id: f_00009-1-0 loss: 1.031674  [   32/  118]
train() client id: f_00009-1-1 loss: 1.123929  [   64/  118]
train() client id: f_00009-1-2 loss: 0.893386  [   96/  118]
train() client id: f_00009-2-0 loss: 1.126924  [   32/  118]
train() client id: f_00009-2-1 loss: 0.982659  [   64/  118]
train() client id: f_00009-2-2 loss: 0.884729  [   96/  118]
train() client id: f_00009-3-0 loss: 0.927193  [   32/  118]
train() client id: f_00009-3-1 loss: 0.845768  [   64/  118]
train() client id: f_00009-3-2 loss: 0.951274  [   96/  118]
train() client id: f_00009-4-0 loss: 0.948392  [   32/  118]
train() client id: f_00009-4-1 loss: 0.856740  [   64/  118]
train() client id: f_00009-4-2 loss: 0.878352  [   96/  118]
train() client id: f_00009-5-0 loss: 0.975574  [   32/  118]
train() client id: f_00009-5-1 loss: 0.813423  [   64/  118]
train() client id: f_00009-5-2 loss: 0.842509  [   96/  118]
train() client id: f_00009-6-0 loss: 0.885969  [   32/  118]
train() client id: f_00009-6-1 loss: 0.850074  [   64/  118]
train() client id: f_00009-6-2 loss: 0.838760  [   96/  118]
train() client id: f_00009-7-0 loss: 0.834038  [   32/  118]
train() client id: f_00009-7-1 loss: 0.864604  [   64/  118]
train() client id: f_00009-7-2 loss: 0.810616  [   96/  118]
train() client id: f_00009-8-0 loss: 0.823786  [   32/  118]
train() client id: f_00009-8-1 loss: 0.895230  [   64/  118]
train() client id: f_00009-8-2 loss: 0.721380  [   96/  118]
train() client id: f_00009-9-0 loss: 0.741107  [   32/  118]
train() client id: f_00009-9-1 loss: 0.801262  [   64/  118]
train() client id: f_00009-9-2 loss: 0.749931  [   96/  118]
train() client id: f_00009-10-0 loss: 0.895359  [   32/  118]
train() client id: f_00009-10-1 loss: 0.811972  [   64/  118]
train() client id: f_00009-10-2 loss: 0.707115  [   96/  118]
train() client id: f_00009-11-0 loss: 0.931894  [   32/  118]
train() client id: f_00009-11-1 loss: 0.667112  [   64/  118]
train() client id: f_00009-11-2 loss: 0.712682  [   96/  118]
train() client id: f_00009-12-0 loss: 0.778340  [   32/  118]
train() client id: f_00009-12-1 loss: 0.702255  [   64/  118]
train() client id: f_00009-12-2 loss: 0.694498  [   96/  118]
train() client id: f_00009-13-0 loss: 0.781481  [   32/  118]
train() client id: f_00009-13-1 loss: 0.780162  [   64/  118]
train() client id: f_00009-13-2 loss: 0.759766  [   96/  118]
train() client id: f_00009-14-0 loss: 0.814514  [   32/  118]
train() client id: f_00009-14-1 loss: 0.677387  [   64/  118]
train() client id: f_00009-14-2 loss: 0.639303  [   96/  118]
At round 34 accuracy: 0.6445623342175066
At round 34 training accuracy: 0.5881958417169685
At round 34 training loss: 0.8381119279798454
update_location
xs = 8.927491 291.223621 5.882650 0.934260 -207.581990 -55.230757 -15.849135 -5.143845 -230.120581 20.134486 
ys = -282.390647 7.291448 180.684448 -2.290817 -9.642386 0.794442 -1.381692 176.628436 25.881276 -717.232496 
xs mean: -18.682379970521218
ys mean: -62.16579882624052
dists_uav = 299.706820 308.000589 206.594955 100.030599 230.614956 114.241270 101.257613 203.037099 252.240604 724.450033 
uav_gains = -115.705972 -116.380226 -108.101123 -100.003338 -109.749285 -101.445682 -100.135711 -107.874017 -111.487746 -128.457498 
uav_gains_db_mean: -109.93405979715791
dists_bs = 492.986335 495.470103 180.971948 249.769689 187.495058 211.405411 237.569988 169.863961 158.980100 913.321572 
bs_gains = -114.966436 -115.027548 -102.780174 -106.698172 -103.210774 -104.670310 -106.089223 -102.009894 -101.204657 -122.464523 
bs_gains_db_mean: -107.9121711139694
Round 35
-------------------------------
ene_coms = [0.01703903 0.01823201 0.00712307 0.00630329 0.00726933 0.00670442
 0.00633812 0.00687536 0.00663392 0.03945198]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.49451922 13.37115714  6.23365954  2.22437206 15.214445    7.31211181
  2.76759185  8.94285838  6.52164937  6.46647464]
obj_prev = 75.54883902748519
eta_min = 3.3507624180332416e-15	eta_max = 0.7707027308636462
af = 15.812275796492862	bf = 1.9751295665803024	zeta = 17.39350337614215	eta = 0.9090909090909091
af = 15.812275796492862	bf = 1.9751295665803024	zeta = 38.304204574455724	eta = 0.4128078359062895
af = 15.812275796492862	bf = 1.9751295665803024	zeta = 27.189405007839724	eta = 0.5815601993472674
af = 15.812275796492862	bf = 1.9751295665803024	zeta = 25.203214381441626	eta = 0.6273912349900981
af = 15.812275796492862	bf = 1.9751295665803024	zeta = 25.084151271367023	eta = 0.6303691771521969
af = 15.812275796492862	bf = 1.9751295665803024	zeta = 25.083672410598776	eta = 0.6303812112380958
af = 15.812275796492862	bf = 1.9751295665803024	zeta = 25.08367240280343	eta = 0.6303812114340017
eta = 0.6303812114340017
ene_coms = [0.01703903 0.01823201 0.00712307 0.00630329 0.00726933 0.00670442
 0.00633812 0.00687536 0.00663392 0.03945198]
ene_comp = [0.03807614 0.08008078 0.03747176 0.01299424 0.09247063 0.04411997
 0.01631835 0.0540923  0.03928491 0.03565861]
ene_total = [2.41467025 4.30721571 1.95375985 0.84545086 4.36974259 2.22668525
 0.9926103  2.67107557 2.01176583 3.29069619]
ti_comp = [0.4736722  0.46174247 0.57283179 0.58102961 0.57136919 0.57701835
 0.58068135 0.57530889 0.57772334 0.24954276]
ti_coms = [0.17039033 0.18232006 0.07123074 0.06303291 0.07269334 0.06704417
 0.06338118 0.06875364 0.06633918 0.39451976]
t_total = [28.21724892 28.21724892 28.21724892 28.21724892 28.21724892 28.21724892
 28.21724892 28.21724892 28.21724892 28.21724892]
ene_coms = [0.01703903 0.01823201 0.00712307 0.00630329 0.00726933 0.00670442
 0.00633812 0.00687536 0.00663392 0.03945198]
ene_comp = [1.53774053e-05 1.50544604e-04 1.00216227e-05 4.06196379e-07
 1.51376293e-04 1.61215403e-05 8.05439158e-07 2.98871250e-05
 1.13531778e-05 4.55075311e-05]
ene_total = [0.74717674 0.80536435 0.31251054 0.2761735  0.32511134 0.29443589
 0.27771678 0.30252836 0.29113832 1.73043809]
optimize_network iter = 0 obj = 5.362593889047402
eta = 0.6303812114340017
freqs = [40192503.06094473 86715844.10829912 32707472.00422397 11182079.92187091
 80920212.58648662 38230997.90585633 14051034.03893162 47011526.15965614
 33999759.13393606 71447893.58522967]
eta_min = 0.6303812114340023	eta_max = 0.6303812114340013
af = 0.015139635499451978	bf = 1.9751295665803024	zeta = 0.01665359904939718	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01703903 0.01823201 0.00712307 0.00630329 0.00726933 0.00670442
 0.00633812 0.00687536 0.00663392 0.03945198]
ene_comp = [3.17625974e-06 3.10955429e-05 2.07000311e-06 8.39013592e-08
 3.12673312e-05 3.32996356e-06 1.66366427e-07 6.17329582e-06
 2.34504073e-06 9.39974828e-06]
ene_total = [3.03570279 3.25317845 1.26919102 1.12281064 1.30044502 1.19484086
 1.12902889 1.22579772 1.1821075  7.02919492]
ti_comp = [0.4736722  0.46174247 0.57283179 0.58102961 0.57136919 0.57701835
 0.58068135 0.57530889 0.57772334 0.24954276]
ti_coms = [0.17039033 0.18232006 0.07123074 0.06303291 0.07269334 0.06704417
 0.06338118 0.06875364 0.06633918 0.39451976]
t_total = [28.21724892 28.21724892 28.21724892 28.21724892 28.21724892 28.21724892
 28.21724892 28.21724892 28.21724892 28.21724892]
ene_coms = [0.01703903 0.01823201 0.00712307 0.00630329 0.00726933 0.00670442
 0.00633812 0.00687536 0.00663392 0.03945198]
ene_comp = [1.53774053e-05 1.50544604e-04 1.00216227e-05 4.06196379e-07
 1.51376293e-04 1.61215403e-05 8.05439158e-07 2.98871250e-05
 1.13531778e-05 4.55075311e-05]
ene_total = [0.74717674 0.80536435 0.31251054 0.2761735  0.32511134 0.29443589
 0.27771678 0.30252836 0.29113832 1.73043809]
optimize_network iter = 1 obj = 5.362593889047396
eta = 0.6303812114340013
freqs = [40192503.06094471 86715844.1082991  32707472.00422397 11182079.92187092
 80920212.58648664 38230997.90585634 14051034.03893163 47011526.15965614
 33999759.13393606 71447893.58522955]
Done!
ene_coms = [0.01703903 0.01823201 0.00712307 0.00630329 0.00726933 0.00670442
 0.00633812 0.00687536 0.00663392 0.03945198]
ene_comp = [1.52658825e-05 1.49452798e-04 9.94894214e-06 4.03250489e-07
 1.50278454e-04 1.60046207e-05 7.99597808e-07 2.96703722e-05
 1.12708403e-05 4.51774933e-05]
ene_total = [0.0170543  0.01838146 0.00713302 0.00630369 0.00741961 0.00672042
 0.00633892 0.00690503 0.00664519 0.03949715]
At round 35 energy consumption: 0.12239880268814596
At round 35 eta: 0.6303812114340013
At round 35 a_n: 16.193498212328272
At round 35 local rounds: 15.109580394227928
At round 35 global rounds: 43.81135027024412
gradient difference: 0.38517987728118896
train() client id: f_00000-0-0 loss: 1.305867  [   32/  126]
train() client id: f_00000-0-1 loss: 1.153782  [   64/  126]
train() client id: f_00000-0-2 loss: 1.242784  [   96/  126]
train() client id: f_00000-1-0 loss: 1.192422  [   32/  126]
train() client id: f_00000-1-1 loss: 1.077579  [   64/  126]
train() client id: f_00000-1-2 loss: 1.100813  [   96/  126]
train() client id: f_00000-2-0 loss: 0.918017  [   32/  126]
train() client id: f_00000-2-1 loss: 1.101359  [   64/  126]
train() client id: f_00000-2-2 loss: 1.055196  [   96/  126]
train() client id: f_00000-3-0 loss: 1.155839  [   32/  126]
train() client id: f_00000-3-1 loss: 0.797365  [   64/  126]
train() client id: f_00000-3-2 loss: 0.911289  [   96/  126]
train() client id: f_00000-4-0 loss: 0.994596  [   32/  126]
train() client id: f_00000-4-1 loss: 0.764363  [   64/  126]
train() client id: f_00000-4-2 loss: 0.855694  [   96/  126]
train() client id: f_00000-5-0 loss: 0.922393  [   32/  126]
train() client id: f_00000-5-1 loss: 0.763059  [   64/  126]
train() client id: f_00000-5-2 loss: 0.852360  [   96/  126]
train() client id: f_00000-6-0 loss: 0.899355  [   32/  126]
train() client id: f_00000-6-1 loss: 0.808616  [   64/  126]
train() client id: f_00000-6-2 loss: 0.724503  [   96/  126]
train() client id: f_00000-7-0 loss: 0.706050  [   32/  126]
train() client id: f_00000-7-1 loss: 0.833277  [   64/  126]
train() client id: f_00000-7-2 loss: 0.738113  [   96/  126]
train() client id: f_00000-8-0 loss: 0.804065  [   32/  126]
train() client id: f_00000-8-1 loss: 0.832483  [   64/  126]
train() client id: f_00000-8-2 loss: 0.746294  [   96/  126]
train() client id: f_00000-9-0 loss: 0.734103  [   32/  126]
train() client id: f_00000-9-1 loss: 0.758570  [   64/  126]
train() client id: f_00000-9-2 loss: 0.635004  [   96/  126]
train() client id: f_00000-10-0 loss: 0.644662  [   32/  126]
train() client id: f_00000-10-1 loss: 0.653795  [   64/  126]
train() client id: f_00000-10-2 loss: 0.726231  [   96/  126]
train() client id: f_00000-11-0 loss: 0.728139  [   32/  126]
train() client id: f_00000-11-1 loss: 0.747141  [   64/  126]
train() client id: f_00000-11-2 loss: 0.719469  [   96/  126]
train() client id: f_00000-12-0 loss: 0.659114  [   32/  126]
train() client id: f_00000-12-1 loss: 0.756898  [   64/  126]
train() client id: f_00000-12-2 loss: 0.745707  [   96/  126]
train() client id: f_00000-13-0 loss: 0.615442  [   32/  126]
train() client id: f_00000-13-1 loss: 0.845957  [   64/  126]
train() client id: f_00000-13-2 loss: 0.669641  [   96/  126]
train() client id: f_00000-14-0 loss: 0.591787  [   32/  126]
train() client id: f_00000-14-1 loss: 0.697826  [   64/  126]
train() client id: f_00000-14-2 loss: 0.553468  [   96/  126]
train() client id: f_00001-0-0 loss: 0.459884  [   32/  265]
train() client id: f_00001-0-1 loss: 0.507818  [   64/  265]
train() client id: f_00001-0-2 loss: 0.384246  [   96/  265]
train() client id: f_00001-0-3 loss: 0.564236  [  128/  265]
train() client id: f_00001-0-4 loss: 0.407799  [  160/  265]
train() client id: f_00001-0-5 loss: 0.471149  [  192/  265]
train() client id: f_00001-0-6 loss: 0.428566  [  224/  265]
train() client id: f_00001-0-7 loss: 0.348713  [  256/  265]
train() client id: f_00001-1-0 loss: 0.477927  [   32/  265]
train() client id: f_00001-1-1 loss: 0.427803  [   64/  265]
train() client id: f_00001-1-2 loss: 0.630146  [   96/  265]
train() client id: f_00001-1-3 loss: 0.452167  [  128/  265]
train() client id: f_00001-1-4 loss: 0.341938  [  160/  265]
train() client id: f_00001-1-5 loss: 0.431027  [  192/  265]
train() client id: f_00001-1-6 loss: 0.364146  [  224/  265]
train() client id: f_00001-1-7 loss: 0.353411  [  256/  265]
train() client id: f_00001-2-0 loss: 0.419366  [   32/  265]
train() client id: f_00001-2-1 loss: 0.504364  [   64/  265]
train() client id: f_00001-2-2 loss: 0.573914  [   96/  265]
train() client id: f_00001-2-3 loss: 0.424647  [  128/  265]
train() client id: f_00001-2-4 loss: 0.437633  [  160/  265]
train() client id: f_00001-2-5 loss: 0.449017  [  192/  265]
train() client id: f_00001-2-6 loss: 0.346440  [  224/  265]
train() client id: f_00001-2-7 loss: 0.348370  [  256/  265]
train() client id: f_00001-3-0 loss: 0.348102  [   32/  265]
train() client id: f_00001-3-1 loss: 0.347602  [   64/  265]
train() client id: f_00001-3-2 loss: 0.376006  [   96/  265]
train() client id: f_00001-3-3 loss: 0.403885  [  128/  265]
train() client id: f_00001-3-4 loss: 0.410477  [  160/  265]
train() client id: f_00001-3-5 loss: 0.559665  [  192/  265]
train() client id: f_00001-3-6 loss: 0.339672  [  224/  265]
train() client id: f_00001-3-7 loss: 0.538104  [  256/  265]
train() client id: f_00001-4-0 loss: 0.458076  [   32/  265]
train() client id: f_00001-4-1 loss: 0.396231  [   64/  265]
train() client id: f_00001-4-2 loss: 0.543041  [   96/  265]
train() client id: f_00001-4-3 loss: 0.461358  [  128/  265]
train() client id: f_00001-4-4 loss: 0.368621  [  160/  265]
train() client id: f_00001-4-5 loss: 0.446345  [  192/  265]
train() client id: f_00001-4-6 loss: 0.426876  [  224/  265]
train() client id: f_00001-4-7 loss: 0.310010  [  256/  265]
train() client id: f_00001-5-0 loss: 0.552234  [   32/  265]
train() client id: f_00001-5-1 loss: 0.410271  [   64/  265]
train() client id: f_00001-5-2 loss: 0.469288  [   96/  265]
train() client id: f_00001-5-3 loss: 0.392658  [  128/  265]
train() client id: f_00001-5-4 loss: 0.405042  [  160/  265]
train() client id: f_00001-5-5 loss: 0.330025  [  192/  265]
train() client id: f_00001-5-6 loss: 0.490848  [  224/  265]
train() client id: f_00001-5-7 loss: 0.341230  [  256/  265]
train() client id: f_00001-6-0 loss: 0.473059  [   32/  265]
train() client id: f_00001-6-1 loss: 0.470790  [   64/  265]
train() client id: f_00001-6-2 loss: 0.406872  [   96/  265]
train() client id: f_00001-6-3 loss: 0.406890  [  128/  265]
train() client id: f_00001-6-4 loss: 0.324763  [  160/  265]
train() client id: f_00001-6-5 loss: 0.322806  [  192/  265]
train() client id: f_00001-6-6 loss: 0.437120  [  224/  265]
train() client id: f_00001-6-7 loss: 0.508784  [  256/  265]
train() client id: f_00001-7-0 loss: 0.403959  [   32/  265]
train() client id: f_00001-7-1 loss: 0.454094  [   64/  265]
train() client id: f_00001-7-2 loss: 0.336062  [   96/  265]
train() client id: f_00001-7-3 loss: 0.414055  [  128/  265]
train() client id: f_00001-7-4 loss: 0.543969  [  160/  265]
train() client id: f_00001-7-5 loss: 0.542762  [  192/  265]
train() client id: f_00001-7-6 loss: 0.312255  [  224/  265]
train() client id: f_00001-7-7 loss: 0.311997  [  256/  265]
train() client id: f_00001-8-0 loss: 0.350948  [   32/  265]
train() client id: f_00001-8-1 loss: 0.509181  [   64/  265]
train() client id: f_00001-8-2 loss: 0.423089  [   96/  265]
train() client id: f_00001-8-3 loss: 0.433041  [  128/  265]
train() client id: f_00001-8-4 loss: 0.378479  [  160/  265]
train() client id: f_00001-8-5 loss: 0.337610  [  192/  265]
train() client id: f_00001-8-6 loss: 0.369962  [  224/  265]
train() client id: f_00001-8-7 loss: 0.525728  [  256/  265]
train() client id: f_00001-9-0 loss: 0.465132  [   32/  265]
train() client id: f_00001-9-1 loss: 0.418961  [   64/  265]
train() client id: f_00001-9-2 loss: 0.328009  [   96/  265]
train() client id: f_00001-9-3 loss: 0.462054  [  128/  265]
train() client id: f_00001-9-4 loss: 0.423299  [  160/  265]
train() client id: f_00001-9-5 loss: 0.400566  [  192/  265]
train() client id: f_00001-9-6 loss: 0.454985  [  224/  265]
train() client id: f_00001-9-7 loss: 0.391535  [  256/  265]
train() client id: f_00001-10-0 loss: 0.314899  [   32/  265]
train() client id: f_00001-10-1 loss: 0.535759  [   64/  265]
train() client id: f_00001-10-2 loss: 0.353264  [   96/  265]
train() client id: f_00001-10-3 loss: 0.419596  [  128/  265]
train() client id: f_00001-10-4 loss: 0.412893  [  160/  265]
train() client id: f_00001-10-5 loss: 0.397883  [  192/  265]
train() client id: f_00001-10-6 loss: 0.420986  [  224/  265]
train() client id: f_00001-10-7 loss: 0.397063  [  256/  265]
train() client id: f_00001-11-0 loss: 0.451007  [   32/  265]
train() client id: f_00001-11-1 loss: 0.300454  [   64/  265]
train() client id: f_00001-11-2 loss: 0.408371  [   96/  265]
train() client id: f_00001-11-3 loss: 0.364437  [  128/  265]
train() client id: f_00001-11-4 loss: 0.452691  [  160/  265]
train() client id: f_00001-11-5 loss: 0.407295  [  192/  265]
train() client id: f_00001-11-6 loss: 0.455487  [  224/  265]
train() client id: f_00001-11-7 loss: 0.351837  [  256/  265]
train() client id: f_00001-12-0 loss: 0.323277  [   32/  265]
train() client id: f_00001-12-1 loss: 0.494101  [   64/  265]
train() client id: f_00001-12-2 loss: 0.466857  [   96/  265]
train() client id: f_00001-12-3 loss: 0.310414  [  128/  265]
train() client id: f_00001-12-4 loss: 0.370403  [  160/  265]
train() client id: f_00001-12-5 loss: 0.490466  [  192/  265]
train() client id: f_00001-12-6 loss: 0.396593  [  224/  265]
train() client id: f_00001-12-7 loss: 0.471334  [  256/  265]
train() client id: f_00001-13-0 loss: 0.489817  [   32/  265]
train() client id: f_00001-13-1 loss: 0.380912  [   64/  265]
train() client id: f_00001-13-2 loss: 0.388603  [   96/  265]
train() client id: f_00001-13-3 loss: 0.352141  [  128/  265]
train() client id: f_00001-13-4 loss: 0.329876  [  160/  265]
train() client id: f_00001-13-5 loss: 0.364085  [  192/  265]
train() client id: f_00001-13-6 loss: 0.366491  [  224/  265]
train() client id: f_00001-13-7 loss: 0.597356  [  256/  265]
train() client id: f_00001-14-0 loss: 0.318506  [   32/  265]
train() client id: f_00001-14-1 loss: 0.315074  [   64/  265]
train() client id: f_00001-14-2 loss: 0.445757  [   96/  265]
train() client id: f_00001-14-3 loss: 0.403779  [  128/  265]
train() client id: f_00001-14-4 loss: 0.444794  [  160/  265]
train() client id: f_00001-14-5 loss: 0.543156  [  192/  265]
train() client id: f_00001-14-6 loss: 0.444327  [  224/  265]
train() client id: f_00001-14-7 loss: 0.421851  [  256/  265]
train() client id: f_00002-0-0 loss: 1.219185  [   32/  124]
train() client id: f_00002-0-1 loss: 1.038180  [   64/  124]
train() client id: f_00002-0-2 loss: 1.157937  [   96/  124]
train() client id: f_00002-1-0 loss: 1.249777  [   32/  124]
train() client id: f_00002-1-1 loss: 0.987932  [   64/  124]
train() client id: f_00002-1-2 loss: 1.273751  [   96/  124]
train() client id: f_00002-2-0 loss: 1.305759  [   32/  124]
train() client id: f_00002-2-1 loss: 1.166884  [   64/  124]
train() client id: f_00002-2-2 loss: 0.975689  [   96/  124]
train() client id: f_00002-3-0 loss: 1.056849  [   32/  124]
train() client id: f_00002-3-1 loss: 1.009237  [   64/  124]
train() client id: f_00002-3-2 loss: 0.960884  [   96/  124]
train() client id: f_00002-4-0 loss: 1.095405  [   32/  124]
train() client id: f_00002-4-1 loss: 1.002220  [   64/  124]
train() client id: f_00002-4-2 loss: 1.010541  [   96/  124]
train() client id: f_00002-5-0 loss: 1.061627  [   32/  124]
train() client id: f_00002-5-1 loss: 0.978338  [   64/  124]
train() client id: f_00002-5-2 loss: 1.143126  [   96/  124]
train() client id: f_00002-6-0 loss: 1.060043  [   32/  124]
train() client id: f_00002-6-1 loss: 1.049581  [   64/  124]
train() client id: f_00002-6-2 loss: 0.799547  [   96/  124]
train() client id: f_00002-7-0 loss: 0.947888  [   32/  124]
train() client id: f_00002-7-1 loss: 0.998118  [   64/  124]
train() client id: f_00002-7-2 loss: 1.093506  [   96/  124]
train() client id: f_00002-8-0 loss: 1.012762  [   32/  124]
train() client id: f_00002-8-1 loss: 1.058870  [   64/  124]
train() client id: f_00002-8-2 loss: 0.883032  [   96/  124]
train() client id: f_00002-9-0 loss: 0.981365  [   32/  124]
train() client id: f_00002-9-1 loss: 1.041102  [   64/  124]
train() client id: f_00002-9-2 loss: 0.929952  [   96/  124]
train() client id: f_00002-10-0 loss: 0.929356  [   32/  124]
train() client id: f_00002-10-1 loss: 1.042045  [   64/  124]
train() client id: f_00002-10-2 loss: 1.008080  [   96/  124]
train() client id: f_00002-11-0 loss: 0.791810  [   32/  124]
train() client id: f_00002-11-1 loss: 0.952118  [   64/  124]
train() client id: f_00002-11-2 loss: 1.029295  [   96/  124]
train() client id: f_00002-12-0 loss: 0.841217  [   32/  124]
train() client id: f_00002-12-1 loss: 0.862487  [   64/  124]
train() client id: f_00002-12-2 loss: 1.009869  [   96/  124]
train() client id: f_00002-13-0 loss: 0.862316  [   32/  124]
train() client id: f_00002-13-1 loss: 0.739489  [   64/  124]
train() client id: f_00002-13-2 loss: 1.256879  [   96/  124]
train() client id: f_00002-14-0 loss: 1.026846  [   32/  124]
train() client id: f_00002-14-1 loss: 1.100866  [   64/  124]
train() client id: f_00002-14-2 loss: 0.885356  [   96/  124]
train() client id: f_00003-0-0 loss: 0.630184  [   32/   43]
train() client id: f_00003-1-0 loss: 0.516002  [   32/   43]
train() client id: f_00003-2-0 loss: 0.661691  [   32/   43]
train() client id: f_00003-3-0 loss: 0.453941  [   32/   43]
train() client id: f_00003-4-0 loss: 0.703469  [   32/   43]
train() client id: f_00003-5-0 loss: 0.684594  [   32/   43]
train() client id: f_00003-6-0 loss: 0.809093  [   32/   43]
train() client id: f_00003-7-0 loss: 0.709499  [   32/   43]
train() client id: f_00003-8-0 loss: 0.563625  [   32/   43]
train() client id: f_00003-9-0 loss: 0.831147  [   32/   43]
train() client id: f_00003-10-0 loss: 0.739684  [   32/   43]
train() client id: f_00003-11-0 loss: 0.721799  [   32/   43]
train() client id: f_00003-12-0 loss: 0.730608  [   32/   43]
train() client id: f_00003-13-0 loss: 0.659492  [   32/   43]
train() client id: f_00003-14-0 loss: 0.781803  [   32/   43]
train() client id: f_00004-0-0 loss: 0.753919  [   32/  306]
train() client id: f_00004-0-1 loss: 0.932088  [   64/  306]
train() client id: f_00004-0-2 loss: 1.009159  [   96/  306]
train() client id: f_00004-0-3 loss: 0.903725  [  128/  306]
train() client id: f_00004-0-4 loss: 0.750056  [  160/  306]
train() client id: f_00004-0-5 loss: 0.953906  [  192/  306]
train() client id: f_00004-0-6 loss: 0.950497  [  224/  306]
train() client id: f_00004-0-7 loss: 0.770687  [  256/  306]
train() client id: f_00004-0-8 loss: 0.977985  [  288/  306]
train() client id: f_00004-1-0 loss: 0.786007  [   32/  306]
train() client id: f_00004-1-1 loss: 0.944903  [   64/  306]
train() client id: f_00004-1-2 loss: 0.879258  [   96/  306]
train() client id: f_00004-1-3 loss: 0.836480  [  128/  306]
train() client id: f_00004-1-4 loss: 1.027449  [  160/  306]
train() client id: f_00004-1-5 loss: 0.947887  [  192/  306]
train() client id: f_00004-1-6 loss: 0.842679  [  224/  306]
train() client id: f_00004-1-7 loss: 0.872265  [  256/  306]
train() client id: f_00004-1-8 loss: 0.826859  [  288/  306]
train() client id: f_00004-2-0 loss: 0.877716  [   32/  306]
train() client id: f_00004-2-1 loss: 0.954051  [   64/  306]
train() client id: f_00004-2-2 loss: 0.828658  [   96/  306]
train() client id: f_00004-2-3 loss: 0.918364  [  128/  306]
train() client id: f_00004-2-4 loss: 1.018558  [  160/  306]
train() client id: f_00004-2-5 loss: 0.890820  [  192/  306]
train() client id: f_00004-2-6 loss: 0.833791  [  224/  306]
train() client id: f_00004-2-7 loss: 0.955547  [  256/  306]
train() client id: f_00004-2-8 loss: 0.782107  [  288/  306]
train() client id: f_00004-3-0 loss: 0.857544  [   32/  306]
train() client id: f_00004-3-1 loss: 1.049428  [   64/  306]
train() client id: f_00004-3-2 loss: 0.950000  [   96/  306]
train() client id: f_00004-3-3 loss: 0.819726  [  128/  306]
train() client id: f_00004-3-4 loss: 0.642169  [  160/  306]
train() client id: f_00004-3-5 loss: 0.897262  [  192/  306]
train() client id: f_00004-3-6 loss: 1.011193  [  224/  306]
train() client id: f_00004-3-7 loss: 0.950160  [  256/  306]
train() client id: f_00004-3-8 loss: 0.880066  [  288/  306]
train() client id: f_00004-4-0 loss: 0.960116  [   32/  306]
train() client id: f_00004-4-1 loss: 0.818536  [   64/  306]
train() client id: f_00004-4-2 loss: 0.879420  [   96/  306]
train() client id: f_00004-4-3 loss: 0.863786  [  128/  306]
train() client id: f_00004-4-4 loss: 0.810001  [  160/  306]
train() client id: f_00004-4-5 loss: 0.934424  [  192/  306]
train() client id: f_00004-4-6 loss: 0.973481  [  224/  306]
train() client id: f_00004-4-7 loss: 0.994402  [  256/  306]
train() client id: f_00004-4-8 loss: 0.814415  [  288/  306]
train() client id: f_00004-5-0 loss: 0.912344  [   32/  306]
train() client id: f_00004-5-1 loss: 0.933208  [   64/  306]
train() client id: f_00004-5-2 loss: 0.866039  [   96/  306]
train() client id: f_00004-5-3 loss: 0.982278  [  128/  306]
train() client id: f_00004-5-4 loss: 0.833715  [  160/  306]
train() client id: f_00004-5-5 loss: 0.808342  [  192/  306]
train() client id: f_00004-5-6 loss: 0.850711  [  224/  306]
train() client id: f_00004-5-7 loss: 0.804571  [  256/  306]
train() client id: f_00004-5-8 loss: 0.915395  [  288/  306]
train() client id: f_00004-6-0 loss: 0.952469  [   32/  306]
train() client id: f_00004-6-1 loss: 0.841898  [   64/  306]
train() client id: f_00004-6-2 loss: 0.913952  [   96/  306]
train() client id: f_00004-6-3 loss: 0.847776  [  128/  306]
train() client id: f_00004-6-4 loss: 0.932059  [  160/  306]
train() client id: f_00004-6-5 loss: 0.932524  [  192/  306]
train() client id: f_00004-6-6 loss: 0.690889  [  224/  306]
train() client id: f_00004-6-7 loss: 0.978935  [  256/  306]
train() client id: f_00004-6-8 loss: 0.841185  [  288/  306]
train() client id: f_00004-7-0 loss: 0.935535  [   32/  306]
train() client id: f_00004-7-1 loss: 0.935672  [   64/  306]
train() client id: f_00004-7-2 loss: 0.789891  [   96/  306]
train() client id: f_00004-7-3 loss: 0.886181  [  128/  306]
train() client id: f_00004-7-4 loss: 0.828471  [  160/  306]
train() client id: f_00004-7-5 loss: 0.937759  [  192/  306]
train() client id: f_00004-7-6 loss: 0.830910  [  224/  306]
train() client id: f_00004-7-7 loss: 0.817571  [  256/  306]
train() client id: f_00004-7-8 loss: 0.911912  [  288/  306]
train() client id: f_00004-8-0 loss: 0.994500  [   32/  306]
train() client id: f_00004-8-1 loss: 0.806370  [   64/  306]
train() client id: f_00004-8-2 loss: 0.946733  [   96/  306]
train() client id: f_00004-8-3 loss: 0.874109  [  128/  306]
train() client id: f_00004-8-4 loss: 0.863450  [  160/  306]
train() client id: f_00004-8-5 loss: 1.032950  [  192/  306]
train() client id: f_00004-8-6 loss: 0.795112  [  224/  306]
train() client id: f_00004-8-7 loss: 0.720122  [  256/  306]
train() client id: f_00004-8-8 loss: 0.904144  [  288/  306]
train() client id: f_00004-9-0 loss: 0.966932  [   32/  306]
train() client id: f_00004-9-1 loss: 0.901263  [   64/  306]
train() client id: f_00004-9-2 loss: 0.877311  [   96/  306]
train() client id: f_00004-9-3 loss: 0.861050  [  128/  306]
train() client id: f_00004-9-4 loss: 0.885496  [  160/  306]
train() client id: f_00004-9-5 loss: 0.850485  [  192/  306]
train() client id: f_00004-9-6 loss: 0.883112  [  224/  306]
train() client id: f_00004-9-7 loss: 0.906997  [  256/  306]
train() client id: f_00004-9-8 loss: 0.814045  [  288/  306]
train() client id: f_00004-10-0 loss: 1.097223  [   32/  306]
train() client id: f_00004-10-1 loss: 0.837871  [   64/  306]
train() client id: f_00004-10-2 loss: 0.744380  [   96/  306]
train() client id: f_00004-10-3 loss: 0.866681  [  128/  306]
train() client id: f_00004-10-4 loss: 0.868015  [  160/  306]
train() client id: f_00004-10-5 loss: 0.870748  [  192/  306]
train() client id: f_00004-10-6 loss: 0.819753  [  224/  306]
train() client id: f_00004-10-7 loss: 0.957434  [  256/  306]
train() client id: f_00004-10-8 loss: 0.802316  [  288/  306]
train() client id: f_00004-11-0 loss: 0.892988  [   32/  306]
train() client id: f_00004-11-1 loss: 0.940447  [   64/  306]
train() client id: f_00004-11-2 loss: 0.790417  [   96/  306]
train() client id: f_00004-11-3 loss: 0.875130  [  128/  306]
train() client id: f_00004-11-4 loss: 0.794051  [  160/  306]
train() client id: f_00004-11-5 loss: 0.910146  [  192/  306]
train() client id: f_00004-11-6 loss: 0.944784  [  224/  306]
train() client id: f_00004-11-7 loss: 0.892908  [  256/  306]
train() client id: f_00004-11-8 loss: 0.885584  [  288/  306]
train() client id: f_00004-12-0 loss: 0.833489  [   32/  306]
train() client id: f_00004-12-1 loss: 0.910865  [   64/  306]
train() client id: f_00004-12-2 loss: 0.923587  [   96/  306]
train() client id: f_00004-12-3 loss: 0.782137  [  128/  306]
train() client id: f_00004-12-4 loss: 0.920603  [  160/  306]
train() client id: f_00004-12-5 loss: 0.900809  [  192/  306]
train() client id: f_00004-12-6 loss: 0.918631  [  224/  306]
train() client id: f_00004-12-7 loss: 0.875838  [  256/  306]
train() client id: f_00004-12-8 loss: 0.920107  [  288/  306]
train() client id: f_00004-13-0 loss: 0.789443  [   32/  306]
train() client id: f_00004-13-1 loss: 0.974268  [   64/  306]
train() client id: f_00004-13-2 loss: 0.975331  [   96/  306]
train() client id: f_00004-13-3 loss: 0.893477  [  128/  306]
train() client id: f_00004-13-4 loss: 0.876064  [  160/  306]
train() client id: f_00004-13-5 loss: 0.956569  [  192/  306]
train() client id: f_00004-13-6 loss: 0.866972  [  224/  306]
train() client id: f_00004-13-7 loss: 0.792325  [  256/  306]
train() client id: f_00004-13-8 loss: 0.841771  [  288/  306]
train() client id: f_00004-14-0 loss: 0.906500  [   32/  306]
train() client id: f_00004-14-1 loss: 0.836945  [   64/  306]
train() client id: f_00004-14-2 loss: 0.944979  [   96/  306]
train() client id: f_00004-14-3 loss: 0.946067  [  128/  306]
train() client id: f_00004-14-4 loss: 0.806298  [  160/  306]
train() client id: f_00004-14-5 loss: 0.815000  [  192/  306]
train() client id: f_00004-14-6 loss: 0.901349  [  224/  306]
train() client id: f_00004-14-7 loss: 1.046495  [  256/  306]
train() client id: f_00004-14-8 loss: 0.771649  [  288/  306]
train() client id: f_00005-0-0 loss: 0.392225  [   32/  146]
train() client id: f_00005-0-1 loss: 0.734618  [   64/  146]
train() client id: f_00005-0-2 loss: 0.846593  [   96/  146]
train() client id: f_00005-0-3 loss: 0.426026  [  128/  146]
train() client id: f_00005-1-0 loss: 0.507035  [   32/  146]
train() client id: f_00005-1-1 loss: 0.626356  [   64/  146]
train() client id: f_00005-1-2 loss: 0.604030  [   96/  146]
train() client id: f_00005-1-3 loss: 0.752591  [  128/  146]
train() client id: f_00005-2-0 loss: 0.699317  [   32/  146]
train() client id: f_00005-2-1 loss: 0.400255  [   64/  146]
train() client id: f_00005-2-2 loss: 0.681378  [   96/  146]
train() client id: f_00005-2-3 loss: 0.645563  [  128/  146]
train() client id: f_00005-3-0 loss: 0.622994  [   32/  146]
train() client id: f_00005-3-1 loss: 0.591714  [   64/  146]
train() client id: f_00005-3-2 loss: 0.553150  [   96/  146]
train() client id: f_00005-3-3 loss: 0.563695  [  128/  146]
train() client id: f_00005-4-0 loss: 0.490561  [   32/  146]
train() client id: f_00005-4-1 loss: 0.753051  [   64/  146]
train() client id: f_00005-4-2 loss: 0.448805  [   96/  146]
train() client id: f_00005-4-3 loss: 0.599735  [  128/  146]
train() client id: f_00005-5-0 loss: 0.625368  [   32/  146]
train() client id: f_00005-5-1 loss: 0.544690  [   64/  146]
train() client id: f_00005-5-2 loss: 0.754313  [   96/  146]
train() client id: f_00005-5-3 loss: 0.434069  [  128/  146]
train() client id: f_00005-6-0 loss: 0.554576  [   32/  146]
train() client id: f_00005-6-1 loss: 0.611016  [   64/  146]
train() client id: f_00005-6-2 loss: 0.362977  [   96/  146]
train() client id: f_00005-6-3 loss: 0.746392  [  128/  146]
train() client id: f_00005-7-0 loss: 0.710125  [   32/  146]
train() client id: f_00005-7-1 loss: 0.554501  [   64/  146]
train() client id: f_00005-7-2 loss: 0.595926  [   96/  146]
train() client id: f_00005-7-3 loss: 0.580788  [  128/  146]
train() client id: f_00005-8-0 loss: 0.667918  [   32/  146]
train() client id: f_00005-8-1 loss: 0.616280  [   64/  146]
train() client id: f_00005-8-2 loss: 0.524879  [   96/  146]
train() client id: f_00005-8-3 loss: 0.468729  [  128/  146]
train() client id: f_00005-9-0 loss: 0.559937  [   32/  146]
train() client id: f_00005-9-1 loss: 0.494521  [   64/  146]
train() client id: f_00005-9-2 loss: 0.592256  [   96/  146]
train() client id: f_00005-9-3 loss: 0.405227  [  128/  146]
train() client id: f_00005-10-0 loss: 0.634520  [   32/  146]
train() client id: f_00005-10-1 loss: 0.741457  [   64/  146]
train() client id: f_00005-10-2 loss: 0.583284  [   96/  146]
train() client id: f_00005-10-3 loss: 0.461518  [  128/  146]
train() client id: f_00005-11-0 loss: 0.509838  [   32/  146]
train() client id: f_00005-11-1 loss: 0.532153  [   64/  146]
train() client id: f_00005-11-2 loss: 0.516870  [   96/  146]
train() client id: f_00005-11-3 loss: 0.749720  [  128/  146]
train() client id: f_00005-12-0 loss: 0.223948  [   32/  146]
train() client id: f_00005-12-1 loss: 0.818243  [   64/  146]
train() client id: f_00005-12-2 loss: 0.783940  [   96/  146]
train() client id: f_00005-12-3 loss: 0.554306  [  128/  146]
train() client id: f_00005-13-0 loss: 0.729453  [   32/  146]
train() client id: f_00005-13-1 loss: 0.688907  [   64/  146]
train() client id: f_00005-13-2 loss: 0.506069  [   96/  146]
train() client id: f_00005-13-3 loss: 0.522151  [  128/  146]
train() client id: f_00005-14-0 loss: 0.528804  [   32/  146]
train() client id: f_00005-14-1 loss: 0.629781  [   64/  146]
train() client id: f_00005-14-2 loss: 0.605422  [   96/  146]
train() client id: f_00005-14-3 loss: 0.754606  [  128/  146]
train() client id: f_00006-0-0 loss: 0.445674  [   32/   54]
train() client id: f_00006-1-0 loss: 0.437173  [   32/   54]
train() client id: f_00006-2-0 loss: 0.433855  [   32/   54]
train() client id: f_00006-3-0 loss: 0.493789  [   32/   54]
train() client id: f_00006-4-0 loss: 0.443336  [   32/   54]
train() client id: f_00006-5-0 loss: 0.412902  [   32/   54]
train() client id: f_00006-6-0 loss: 0.517244  [   32/   54]
train() client id: f_00006-7-0 loss: 0.451177  [   32/   54]
train() client id: f_00006-8-0 loss: 0.512455  [   32/   54]
train() client id: f_00006-9-0 loss: 0.466007  [   32/   54]
train() client id: f_00006-10-0 loss: 0.517130  [   32/   54]
train() client id: f_00006-11-0 loss: 0.442475  [   32/   54]
train() client id: f_00006-12-0 loss: 0.461005  [   32/   54]
train() client id: f_00006-13-0 loss: 0.389749  [   32/   54]
train() client id: f_00006-14-0 loss: 0.515448  [   32/   54]
train() client id: f_00007-0-0 loss: 0.801450  [   32/  179]
train() client id: f_00007-0-1 loss: 0.612123  [   64/  179]
train() client id: f_00007-0-2 loss: 0.462080  [   96/  179]
train() client id: f_00007-0-3 loss: 0.542605  [  128/  179]
train() client id: f_00007-0-4 loss: 0.557627  [  160/  179]
train() client id: f_00007-1-0 loss: 0.517361  [   32/  179]
train() client id: f_00007-1-1 loss: 0.522165  [   64/  179]
train() client id: f_00007-1-2 loss: 0.597276  [   96/  179]
train() client id: f_00007-1-3 loss: 0.604022  [  128/  179]
train() client id: f_00007-1-4 loss: 0.585783  [  160/  179]
train() client id: f_00007-2-0 loss: 0.522583  [   32/  179]
train() client id: f_00007-2-1 loss: 0.595821  [   64/  179]
train() client id: f_00007-2-2 loss: 0.781557  [   96/  179]
train() client id: f_00007-2-3 loss: 0.484731  [  128/  179]
train() client id: f_00007-2-4 loss: 0.538658  [  160/  179]
train() client id: f_00007-3-0 loss: 0.427311  [   32/  179]
train() client id: f_00007-3-1 loss: 0.642437  [   64/  179]
train() client id: f_00007-3-2 loss: 0.620613  [   96/  179]
train() client id: f_00007-3-3 loss: 0.625836  [  128/  179]
train() client id: f_00007-3-4 loss: 0.476505  [  160/  179]
train() client id: f_00007-4-0 loss: 0.572991  [   32/  179]
train() client id: f_00007-4-1 loss: 0.589507  [   64/  179]
train() client id: f_00007-4-2 loss: 0.683066  [   96/  179]
train() client id: f_00007-4-3 loss: 0.511194  [  128/  179]
train() client id: f_00007-4-4 loss: 0.415017  [  160/  179]
train() client id: f_00007-5-0 loss: 0.598834  [   32/  179]
train() client id: f_00007-5-1 loss: 0.545249  [   64/  179]
train() client id: f_00007-5-2 loss: 0.417894  [   96/  179]
train() client id: f_00007-5-3 loss: 0.542484  [  128/  179]
train() client id: f_00007-5-4 loss: 0.429260  [  160/  179]
train() client id: f_00007-6-0 loss: 0.537722  [   32/  179]
train() client id: f_00007-6-1 loss: 0.453126  [   64/  179]
train() client id: f_00007-6-2 loss: 0.699397  [   96/  179]
train() client id: f_00007-6-3 loss: 0.548667  [  128/  179]
train() client id: f_00007-6-4 loss: 0.530334  [  160/  179]
train() client id: f_00007-7-0 loss: 0.657219  [   32/  179]
train() client id: f_00007-7-1 loss: 0.455779  [   64/  179]
train() client id: f_00007-7-2 loss: 0.475226  [   96/  179]
train() client id: f_00007-7-3 loss: 0.471923  [  128/  179]
train() client id: f_00007-7-4 loss: 0.701769  [  160/  179]
train() client id: f_00007-8-0 loss: 0.622616  [   32/  179]
train() client id: f_00007-8-1 loss: 0.505058  [   64/  179]
train() client id: f_00007-8-2 loss: 0.446089  [   96/  179]
train() client id: f_00007-8-3 loss: 0.503412  [  128/  179]
train() client id: f_00007-8-4 loss: 0.663483  [  160/  179]
train() client id: f_00007-9-0 loss: 0.545827  [   32/  179]
train() client id: f_00007-9-1 loss: 0.502785  [   64/  179]
train() client id: f_00007-9-2 loss: 0.378388  [   96/  179]
train() client id: f_00007-9-3 loss: 0.630143  [  128/  179]
train() client id: f_00007-9-4 loss: 0.545234  [  160/  179]
train() client id: f_00007-10-0 loss: 0.378855  [   32/  179]
train() client id: f_00007-10-1 loss: 0.594705  [   64/  179]
train() client id: f_00007-10-2 loss: 0.548707  [   96/  179]
train() client id: f_00007-10-3 loss: 0.543048  [  128/  179]
train() client id: f_00007-10-4 loss: 0.639262  [  160/  179]
train() client id: f_00007-11-0 loss: 0.462808  [   32/  179]
train() client id: f_00007-11-1 loss: 0.533429  [   64/  179]
train() client id: f_00007-11-2 loss: 0.464955  [   96/  179]
train() client id: f_00007-11-3 loss: 0.453322  [  128/  179]
train() client id: f_00007-11-4 loss: 0.678039  [  160/  179]
train() client id: f_00007-12-0 loss: 0.513470  [   32/  179]
train() client id: f_00007-12-1 loss: 0.539906  [   64/  179]
train() client id: f_00007-12-2 loss: 0.681252  [   96/  179]
train() client id: f_00007-12-3 loss: 0.556764  [  128/  179]
train() client id: f_00007-12-4 loss: 0.402074  [  160/  179]
train() client id: f_00007-13-0 loss: 0.788391  [   32/  179]
train() client id: f_00007-13-1 loss: 0.440277  [   64/  179]
train() client id: f_00007-13-2 loss: 0.493376  [   96/  179]
train() client id: f_00007-13-3 loss: 0.448142  [  128/  179]
train() client id: f_00007-13-4 loss: 0.436714  [  160/  179]
train() client id: f_00007-14-0 loss: 0.754876  [   32/  179]
train() client id: f_00007-14-1 loss: 0.398797  [   64/  179]
train() client id: f_00007-14-2 loss: 0.473891  [   96/  179]
train() client id: f_00007-14-3 loss: 0.541352  [  128/  179]
train() client id: f_00007-14-4 loss: 0.553901  [  160/  179]
train() client id: f_00008-0-0 loss: 0.836282  [   32/  130]
train() client id: f_00008-0-1 loss: 0.729482  [   64/  130]
train() client id: f_00008-0-2 loss: 0.752152  [   96/  130]
train() client id: f_00008-0-3 loss: 0.697416  [  128/  130]
train() client id: f_00008-1-0 loss: 0.681658  [   32/  130]
train() client id: f_00008-1-1 loss: 0.726554  [   64/  130]
train() client id: f_00008-1-2 loss: 0.842782  [   96/  130]
train() client id: f_00008-1-3 loss: 0.757569  [  128/  130]
train() client id: f_00008-2-0 loss: 0.880479  [   32/  130]
train() client id: f_00008-2-1 loss: 0.620681  [   64/  130]
train() client id: f_00008-2-2 loss: 0.785309  [   96/  130]
train() client id: f_00008-2-3 loss: 0.729761  [  128/  130]
train() client id: f_00008-3-0 loss: 0.824006  [   32/  130]
train() client id: f_00008-3-1 loss: 0.739308  [   64/  130]
train() client id: f_00008-3-2 loss: 0.724115  [   96/  130]
train() client id: f_00008-3-3 loss: 0.720336  [  128/  130]
train() client id: f_00008-4-0 loss: 0.791473  [   32/  130]
train() client id: f_00008-4-1 loss: 0.696306  [   64/  130]
train() client id: f_00008-4-2 loss: 0.843336  [   96/  130]
train() client id: f_00008-4-3 loss: 0.694888  [  128/  130]
train() client id: f_00008-5-0 loss: 0.694185  [   32/  130]
train() client id: f_00008-5-1 loss: 0.815709  [   64/  130]
train() client id: f_00008-5-2 loss: 0.751435  [   96/  130]
train() client id: f_00008-5-3 loss: 0.738134  [  128/  130]
train() client id: f_00008-6-0 loss: 0.908252  [   32/  130]
train() client id: f_00008-6-1 loss: 0.735002  [   64/  130]
train() client id: f_00008-6-2 loss: 0.733468  [   96/  130]
train() client id: f_00008-6-3 loss: 0.642570  [  128/  130]
train() client id: f_00008-7-0 loss: 0.827479  [   32/  130]
train() client id: f_00008-7-1 loss: 0.750255  [   64/  130]
train() client id: f_00008-7-2 loss: 0.785626  [   96/  130]
train() client id: f_00008-7-3 loss: 0.643385  [  128/  130]
train() client id: f_00008-8-0 loss: 0.700093  [   32/  130]
train() client id: f_00008-8-1 loss: 0.726023  [   64/  130]
train() client id: f_00008-8-2 loss: 0.763260  [   96/  130]
train() client id: f_00008-8-3 loss: 0.820463  [  128/  130]
train() client id: f_00008-9-0 loss: 0.748095  [   32/  130]
train() client id: f_00008-9-1 loss: 0.743999  [   64/  130]
train() client id: f_00008-9-2 loss: 0.692334  [   96/  130]
train() client id: f_00008-9-3 loss: 0.836518  [  128/  130]
train() client id: f_00008-10-0 loss: 0.723978  [   32/  130]
train() client id: f_00008-10-1 loss: 0.767039  [   64/  130]
train() client id: f_00008-10-2 loss: 0.786857  [   96/  130]
train() client id: f_00008-10-3 loss: 0.731860  [  128/  130]
train() client id: f_00008-11-0 loss: 0.634628  [   32/  130]
train() client id: f_00008-11-1 loss: 0.868776  [   64/  130]
train() client id: f_00008-11-2 loss: 0.723864  [   96/  130]
train() client id: f_00008-11-3 loss: 0.781474  [  128/  130]
train() client id: f_00008-12-0 loss: 0.712397  [   32/  130]
train() client id: f_00008-12-1 loss: 0.878658  [   64/  130]
train() client id: f_00008-12-2 loss: 0.792658  [   96/  130]
train() client id: f_00008-12-3 loss: 0.612855  [  128/  130]
train() client id: f_00008-13-0 loss: 0.671095  [   32/  130]
train() client id: f_00008-13-1 loss: 0.782679  [   64/  130]
train() client id: f_00008-13-2 loss: 0.841767  [   96/  130]
train() client id: f_00008-13-3 loss: 0.728080  [  128/  130]
train() client id: f_00008-14-0 loss: 0.758032  [   32/  130]
train() client id: f_00008-14-1 loss: 0.760993  [   64/  130]
train() client id: f_00008-14-2 loss: 0.707914  [   96/  130]
train() client id: f_00008-14-3 loss: 0.755182  [  128/  130]
train() client id: f_00009-0-0 loss: 1.226285  [   32/  118]
train() client id: f_00009-0-1 loss: 1.255269  [   64/  118]
train() client id: f_00009-0-2 loss: 1.188969  [   96/  118]
train() client id: f_00009-1-0 loss: 1.073586  [   32/  118]
train() client id: f_00009-1-1 loss: 1.142388  [   64/  118]
train() client id: f_00009-1-2 loss: 1.175653  [   96/  118]
train() client id: f_00009-2-0 loss: 1.101764  [   32/  118]
train() client id: f_00009-2-1 loss: 0.996901  [   64/  118]
train() client id: f_00009-2-2 loss: 1.145652  [   96/  118]
train() client id: f_00009-3-0 loss: 1.095611  [   32/  118]
train() client id: f_00009-3-1 loss: 1.120354  [   64/  118]
train() client id: f_00009-3-2 loss: 0.969705  [   96/  118]
train() client id: f_00009-4-0 loss: 1.057229  [   32/  118]
train() client id: f_00009-4-1 loss: 0.902804  [   64/  118]
train() client id: f_00009-4-2 loss: 1.158387  [   96/  118]
train() client id: f_00009-5-0 loss: 0.938667  [   32/  118]
train() client id: f_00009-5-1 loss: 1.071032  [   64/  118]
train() client id: f_00009-5-2 loss: 0.930425  [   96/  118]
train() client id: f_00009-6-0 loss: 0.937606  [   32/  118]
train() client id: f_00009-6-1 loss: 1.054402  [   64/  118]
train() client id: f_00009-6-2 loss: 0.916346  [   96/  118]
train() client id: f_00009-7-0 loss: 1.014094  [   32/  118]
train() client id: f_00009-7-1 loss: 0.897796  [   64/  118]
train() client id: f_00009-7-2 loss: 0.961100  [   96/  118]
train() client id: f_00009-8-0 loss: 1.011965  [   32/  118]
train() client id: f_00009-8-1 loss: 1.090976  [   64/  118]
train() client id: f_00009-8-2 loss: 0.870426  [   96/  118]
train() client id: f_00009-9-0 loss: 0.927695  [   32/  118]
train() client id: f_00009-9-1 loss: 0.861433  [   64/  118]
train() client id: f_00009-9-2 loss: 1.007389  [   96/  118]
train() client id: f_00009-10-0 loss: 1.025384  [   32/  118]
train() client id: f_00009-10-1 loss: 0.981404  [   64/  118]
train() client id: f_00009-10-2 loss: 0.864104  [   96/  118]
train() client id: f_00009-11-0 loss: 0.880179  [   32/  118]
train() client id: f_00009-11-1 loss: 0.932503  [   64/  118]
train() client id: f_00009-11-2 loss: 0.976846  [   96/  118]
train() client id: f_00009-12-0 loss: 0.925160  [   32/  118]
train() client id: f_00009-12-1 loss: 1.026900  [   64/  118]
train() client id: f_00009-12-2 loss: 0.855558  [   96/  118]
train() client id: f_00009-13-0 loss: 1.060620  [   32/  118]
train() client id: f_00009-13-1 loss: 0.795493  [   64/  118]
train() client id: f_00009-13-2 loss: 0.906949  [   96/  118]
train() client id: f_00009-14-0 loss: 0.983888  [   32/  118]
train() client id: f_00009-14-1 loss: 0.798563  [   64/  118]
train() client id: f_00009-14-2 loss: 0.933099  [   96/  118]
At round 35 accuracy: 0.6445623342175066
At round 35 training accuracy: 0.5875251509054326
At round 35 training loss: 0.8309123846100342
update_location
xs = 8.927491 296.223621 5.882650 0.934260 -212.581990 -60.230757 -20.849135 -5.143845 -235.120581 20.134486 
ys = -287.390647 7.291448 185.684448 -7.290817 -9.642386 0.794442 -1.381692 181.628436 25.881276 -722.232496 
xs mean: -20.182379970521218
ys mean: -62.66579882624052
dists_uav = 304.422542 312.732472 210.981800 100.269780 235.125664 116.740633 102.159657 207.401417 256.810296 729.400559 
uav_gains = -116.093977 -116.747378 -108.385418 -100.029268 -110.090207 -101.680696 -100.232007 -108.152998 -111.885639 -128.532278 
uav_gains_db_mean: -110.18298653495702
dists_bs = 497.628810 500.177828 181.197932 253.343257 188.428280 208.613411 234.249847 169.985439 160.782082 918.206741 
bs_gains = -115.080414 -115.142543 -102.795350 -106.870921 -103.271150 -104.508641 -105.918080 -102.018587 -101.341713 -122.529392 
bs_gains_db_mean: -107.94767917561404
Round 36
-------------------------------
ene_coms = [0.0177089  0.01894168 0.00712813 0.00631008 0.00729031 0.00677459
 0.00636369 0.00687806 0.00667383 0.0398461 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.36786395 13.09967574  6.10187805  2.17742802 14.89294526  7.15856016
  2.70945764  8.75373071  6.38433397  6.33599772]
obj_prev = 73.98187121752709
eta_min = 1.6919689440835392e-15	eta_max = 0.7729610716713488
af = 15.47779405979816	bf = 1.964176756305749	zeta = 17.025573465777978	eta = 0.9090909090909091
af = 15.47779405979816	bf = 1.964176756305749	zeta = 37.83304900231365	eta = 0.4091077633962737
af = 15.47779405979816	bf = 1.964176756305749	zeta = 26.735616288085712	eta = 0.578920414364848
af = 15.47779405979816	bf = 1.964176756305749	zeta = 24.75586245520396	eta = 0.6252173232827344
af = 15.47779405979816	bf = 1.964176756305749	zeta = 24.636718157449742	eta = 0.6282409029028052
af = 15.47779405979816	bf = 1.964176756305749	zeta = 24.63623442624124	eta = 0.6282532383809442
af = 15.47779405979816	bf = 1.964176756305749	zeta = 24.636234418215594	eta = 0.6282532385856077
eta = 0.6282532385856077
ene_coms = [0.0177089  0.01894168 0.00712813 0.00631008 0.00729031 0.00677459
 0.00636369 0.00687806 0.00667383 0.0398461 ]
ene_comp = [0.03835517 0.08066761 0.03774636 0.01308946 0.09314826 0.04444329
 0.01643793 0.05448869 0.03957279 0.03591992]
ene_total = [2.39052195 4.247252   1.91340829 0.82717943 4.28261193 2.18388518
 0.97224092 2.61662406 1.97191483 3.23059584]
ti_comp = [0.4834856  0.47115776 0.58929325 0.59747371 0.58767142 0.5928286
 0.59693763 0.5917939  0.59383628 0.26211356]
ti_coms = [0.17708895 0.18941679 0.0712813  0.06310084 0.07290313 0.06774595
 0.06363692 0.06878065 0.06673827 0.39846099]
t_total = [28.16631317 28.16631317 28.16631317 28.16631317 28.16631317 28.16631317
 28.16631317 28.16631317 28.16631317 28.16631317]
ene_coms = [0.0177089  0.01894168 0.00712813 0.00631008 0.00729031 0.00677459
 0.00636369 0.00687806 0.00667383 0.0398461 ]
ene_comp = [1.50863645e-05 1.47789975e-04 9.67926030e-06 3.92651993e-07
 1.46263240e-04 1.56113447e-05 7.79046563e-07 2.88707567e-05
 1.09833960e-05 4.21606297e-05]
ene_total = [0.75573486 0.81395804 0.30434986 0.26907311 0.31708903 0.28952836
 0.27137542 0.29450561 0.28503437 1.70080003]
optimize_network iter = 0 obj = 5.3014486815568
eta = 0.6282532385856077
freqs = [39665264.12164751 85605737.8692321  32026801.38348342 10954006.63265285
 79251995.48700608 37484097.00010987 13768548.30491526 46036880.9677183
 33319614.63649878 68519765.1013675 ]
eta_min = 0.6282532385856078	eta_max = 0.6282532385856076
af = 0.014241505411372425	bf = 1.964176756305749	zeta = 0.01566565595250967	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0177089  0.01894168 0.00712813 0.00631008 0.00729031 0.00677459
 0.00636369 0.00687806 0.00667383 0.0398461 ]
ene_comp = [3.09347495e-06 3.03044901e-05 1.98474253e-06 8.05137049e-08
 2.99914315e-05 3.20112268e-06 1.59744319e-07 5.91997909e-06
 2.25215693e-06 8.64508157e-06]
ene_total = [3.08827076 3.3079641  1.2432102  1.10024321 1.27637182 1.18177968
 1.10960427 1.20029485 1.16404426 6.94909213]
ti_comp = [0.4834856  0.47115776 0.58929325 0.59747371 0.58767142 0.5928286
 0.59693763 0.5917939  0.59383628 0.26211356]
ti_coms = [0.17708895 0.18941679 0.0712813  0.06310084 0.07290313 0.06774595
 0.06363692 0.06878065 0.06673827 0.39846099]
t_total = [28.16631317 28.16631317 28.16631317 28.16631317 28.16631317 28.16631317
 28.16631317 28.16631317 28.16631317 28.16631317]
ene_coms = [0.0177089  0.01894168 0.00712813 0.00631008 0.00729031 0.00677459
 0.00636369 0.00687806 0.00667383 0.0398461 ]
ene_comp = [1.50863645e-05 1.47789975e-04 9.67926030e-06 3.92651993e-07
 1.46263240e-04 1.56113447e-05 7.79046563e-07 2.88707567e-05
 1.09833960e-05 4.21606297e-05]
ene_total = [0.75573486 0.81395804 0.30434986 0.26907311 0.31708903 0.28952836
 0.27137542 0.29450561 0.28503437 1.70080003]
optimize_network iter = 1 obj = 5.301448681556797
eta = 0.6282532385856076
freqs = [39665264.12164751 85605737.8692321  32026801.38348342 10954006.63265285
 79251995.4870061  37484097.00010987 13768548.30491526 46036880.96771831
 33319614.63649878 68519765.10136747]
Done!
ene_coms = [0.0177089  0.01894168 0.00712813 0.00631008 0.00729031 0.00677459
 0.00636369 0.00687806 0.00667383 0.0398461 ]
ene_comp = [1.48679985e-05 1.45650804e-04 9.53915886e-06 3.86968593e-07
 1.44146168e-04 1.53853799e-05 7.67770336e-07 2.84528699e-05
 1.08244180e-05 4.15503802e-05]
ene_total = [0.01772376 0.01908733 0.00713767 0.00631047 0.00743446 0.00678998
 0.00636446 0.00690652 0.00668465 0.03988765]
At round 36 energy consumption: 0.12432695006151157
At round 36 eta: 0.6282532385856076
At round 36 a_n: 15.850952365358953
At round 36 local rounds: 15.22030469364191
At round 36 global rounds: 42.63911353269229
gradient difference: 0.39911362528800964
train() client id: f_00000-0-0 loss: 1.542314  [   32/  126]
train() client id: f_00000-0-1 loss: 1.373341  [   64/  126]
train() client id: f_00000-0-2 loss: 1.248075  [   96/  126]
train() client id: f_00000-1-0 loss: 1.071872  [   32/  126]
train() client id: f_00000-1-1 loss: 1.301332  [   64/  126]
train() client id: f_00000-1-2 loss: 1.127059  [   96/  126]
train() client id: f_00000-2-0 loss: 1.122657  [   32/  126]
train() client id: f_00000-2-1 loss: 1.138751  [   64/  126]
train() client id: f_00000-2-2 loss: 1.244256  [   96/  126]
train() client id: f_00000-3-0 loss: 1.047299  [   32/  126]
train() client id: f_00000-3-1 loss: 1.146289  [   64/  126]
train() client id: f_00000-3-2 loss: 0.951494  [   96/  126]
train() client id: f_00000-4-0 loss: 0.949345  [   32/  126]
train() client id: f_00000-4-1 loss: 1.004626  [   64/  126]
train() client id: f_00000-4-2 loss: 1.121658  [   96/  126]
train() client id: f_00000-5-0 loss: 1.098667  [   32/  126]
train() client id: f_00000-5-1 loss: 0.871666  [   64/  126]
train() client id: f_00000-5-2 loss: 1.036338  [   96/  126]
train() client id: f_00000-6-0 loss: 0.944738  [   32/  126]
train() client id: f_00000-6-1 loss: 0.972424  [   64/  126]
train() client id: f_00000-6-2 loss: 1.073947  [   96/  126]
train() client id: f_00000-7-0 loss: 0.939158  [   32/  126]
train() client id: f_00000-7-1 loss: 0.892818  [   64/  126]
train() client id: f_00000-7-2 loss: 0.985343  [   96/  126]
train() client id: f_00000-8-0 loss: 0.836311  [   32/  126]
train() client id: f_00000-8-1 loss: 1.032620  [   64/  126]
train() client id: f_00000-8-2 loss: 0.920043  [   96/  126]
train() client id: f_00000-9-0 loss: 0.901898  [   32/  126]
train() client id: f_00000-9-1 loss: 0.914378  [   64/  126]
train() client id: f_00000-9-2 loss: 0.975131  [   96/  126]
train() client id: f_00000-10-0 loss: 0.924550  [   32/  126]
train() client id: f_00000-10-1 loss: 0.977121  [   64/  126]
train() client id: f_00000-10-2 loss: 0.886329  [   96/  126]
train() client id: f_00000-11-0 loss: 0.882662  [   32/  126]
train() client id: f_00000-11-1 loss: 0.872000  [   64/  126]
train() client id: f_00000-11-2 loss: 0.928634  [   96/  126]
train() client id: f_00000-12-0 loss: 0.936969  [   32/  126]
train() client id: f_00000-12-1 loss: 0.921733  [   64/  126]
train() client id: f_00000-12-2 loss: 0.911368  [   96/  126]
train() client id: f_00000-13-0 loss: 1.026151  [   32/  126]
train() client id: f_00000-13-1 loss: 0.794472  [   64/  126]
train() client id: f_00000-13-2 loss: 0.867988  [   96/  126]
train() client id: f_00000-14-0 loss: 1.085254  [   32/  126]
train() client id: f_00000-14-1 loss: 0.814204  [   64/  126]
train() client id: f_00000-14-2 loss: 0.908756  [   96/  126]
train() client id: f_00001-0-0 loss: 0.435001  [   32/  265]
train() client id: f_00001-0-1 loss: 0.389224  [   64/  265]
train() client id: f_00001-0-2 loss: 0.485563  [   96/  265]
train() client id: f_00001-0-3 loss: 0.584473  [  128/  265]
train() client id: f_00001-0-4 loss: 0.440601  [  160/  265]
train() client id: f_00001-0-5 loss: 0.428516  [  192/  265]
train() client id: f_00001-0-6 loss: 0.314798  [  224/  265]
train() client id: f_00001-0-7 loss: 0.445992  [  256/  265]
train() client id: f_00001-1-0 loss: 0.371538  [   32/  265]
train() client id: f_00001-1-1 loss: 0.489464  [   64/  265]
train() client id: f_00001-1-2 loss: 0.377577  [   96/  265]
train() client id: f_00001-1-3 loss: 0.487441  [  128/  265]
train() client id: f_00001-1-4 loss: 0.424065  [  160/  265]
train() client id: f_00001-1-5 loss: 0.471836  [  192/  265]
train() client id: f_00001-1-6 loss: 0.362737  [  224/  265]
train() client id: f_00001-1-7 loss: 0.413938  [  256/  265]
train() client id: f_00001-2-0 loss: 0.513416  [   32/  265]
train() client id: f_00001-2-1 loss: 0.482080  [   64/  265]
train() client id: f_00001-2-2 loss: 0.380918  [   96/  265]
train() client id: f_00001-2-3 loss: 0.455163  [  128/  265]
train() client id: f_00001-2-4 loss: 0.341958  [  160/  265]
train() client id: f_00001-2-5 loss: 0.347782  [  192/  265]
train() client id: f_00001-2-6 loss: 0.489749  [  224/  265]
train() client id: f_00001-2-7 loss: 0.394711  [  256/  265]
train() client id: f_00001-3-0 loss: 0.394060  [   32/  265]
train() client id: f_00001-3-1 loss: 0.411845  [   64/  265]
train() client id: f_00001-3-2 loss: 0.441476  [   96/  265]
train() client id: f_00001-3-3 loss: 0.332734  [  128/  265]
train() client id: f_00001-3-4 loss: 0.394450  [  160/  265]
train() client id: f_00001-3-5 loss: 0.471366  [  192/  265]
train() client id: f_00001-3-6 loss: 0.421385  [  224/  265]
train() client id: f_00001-3-7 loss: 0.470655  [  256/  265]
train() client id: f_00001-4-0 loss: 0.383755  [   32/  265]
train() client id: f_00001-4-1 loss: 0.466076  [   64/  265]
train() client id: f_00001-4-2 loss: 0.407399  [   96/  265]
train() client id: f_00001-4-3 loss: 0.361809  [  128/  265]
train() client id: f_00001-4-4 loss: 0.406312  [  160/  265]
train() client id: f_00001-4-5 loss: 0.430250  [  192/  265]
train() client id: f_00001-4-6 loss: 0.459983  [  224/  265]
train() client id: f_00001-4-7 loss: 0.417096  [  256/  265]
train() client id: f_00001-5-0 loss: 0.508398  [   32/  265]
train() client id: f_00001-5-1 loss: 0.486479  [   64/  265]
train() client id: f_00001-5-2 loss: 0.421280  [   96/  265]
train() client id: f_00001-5-3 loss: 0.414432  [  128/  265]
train() client id: f_00001-5-4 loss: 0.429812  [  160/  265]
train() client id: f_00001-5-5 loss: 0.312984  [  192/  265]
train() client id: f_00001-5-6 loss: 0.369263  [  224/  265]
train() client id: f_00001-5-7 loss: 0.368181  [  256/  265]
train() client id: f_00001-6-0 loss: 0.358022  [   32/  265]
train() client id: f_00001-6-1 loss: 0.361935  [   64/  265]
train() client id: f_00001-6-2 loss: 0.384130  [   96/  265]
train() client id: f_00001-6-3 loss: 0.447165  [  128/  265]
train() client id: f_00001-6-4 loss: 0.514450  [  160/  265]
train() client id: f_00001-6-5 loss: 0.332657  [  192/  265]
train() client id: f_00001-6-6 loss: 0.305333  [  224/  265]
train() client id: f_00001-6-7 loss: 0.541662  [  256/  265]
train() client id: f_00001-7-0 loss: 0.413208  [   32/  265]
train() client id: f_00001-7-1 loss: 0.422375  [   64/  265]
train() client id: f_00001-7-2 loss: 0.308769  [   96/  265]
train() client id: f_00001-7-3 loss: 0.417256  [  128/  265]
train() client id: f_00001-7-4 loss: 0.342600  [  160/  265]
train() client id: f_00001-7-5 loss: 0.351818  [  192/  265]
train() client id: f_00001-7-6 loss: 0.432176  [  224/  265]
train() client id: f_00001-7-7 loss: 0.606992  [  256/  265]
train() client id: f_00001-8-0 loss: 0.318610  [   32/  265]
train() client id: f_00001-8-1 loss: 0.534513  [   64/  265]
train() client id: f_00001-8-2 loss: 0.321960  [   96/  265]
train() client id: f_00001-8-3 loss: 0.362254  [  128/  265]
train() client id: f_00001-8-4 loss: 0.400820  [  160/  265]
train() client id: f_00001-8-5 loss: 0.438115  [  192/  265]
train() client id: f_00001-8-6 loss: 0.358777  [  224/  265]
train() client id: f_00001-8-7 loss: 0.464868  [  256/  265]
train() client id: f_00001-9-0 loss: 0.424746  [   32/  265]
train() client id: f_00001-9-1 loss: 0.333315  [   64/  265]
train() client id: f_00001-9-2 loss: 0.407912  [   96/  265]
train() client id: f_00001-9-3 loss: 0.426767  [  128/  265]
train() client id: f_00001-9-4 loss: 0.353753  [  160/  265]
train() client id: f_00001-9-5 loss: 0.453956  [  192/  265]
train() client id: f_00001-9-6 loss: 0.335447  [  224/  265]
train() client id: f_00001-9-7 loss: 0.440300  [  256/  265]
train() client id: f_00001-10-0 loss: 0.334324  [   32/  265]
train() client id: f_00001-10-1 loss: 0.461541  [   64/  265]
train() client id: f_00001-10-2 loss: 0.384053  [   96/  265]
train() client id: f_00001-10-3 loss: 0.364692  [  128/  265]
train() client id: f_00001-10-4 loss: 0.511804  [  160/  265]
train() client id: f_00001-10-5 loss: 0.391057  [  192/  265]
train() client id: f_00001-10-6 loss: 0.423117  [  224/  265]
train() client id: f_00001-10-7 loss: 0.397660  [  256/  265]
train() client id: f_00001-11-0 loss: 0.502621  [   32/  265]
train() client id: f_00001-11-1 loss: 0.615823  [   64/  265]
train() client id: f_00001-11-2 loss: 0.382366  [   96/  265]
train() client id: f_00001-11-3 loss: 0.423836  [  128/  265]
train() client id: f_00001-11-4 loss: 0.414628  [  160/  265]
train() client id: f_00001-11-5 loss: 0.312047  [  192/  265]
train() client id: f_00001-11-6 loss: 0.316088  [  224/  265]
train() client id: f_00001-11-7 loss: 0.307862  [  256/  265]
train() client id: f_00001-12-0 loss: 0.304945  [   32/  265]
train() client id: f_00001-12-1 loss: 0.447908  [   64/  265]
train() client id: f_00001-12-2 loss: 0.404523  [   96/  265]
train() client id: f_00001-12-3 loss: 0.337983  [  128/  265]
train() client id: f_00001-12-4 loss: 0.344557  [  160/  265]
train() client id: f_00001-12-5 loss: 0.573009  [  192/  265]
train() client id: f_00001-12-6 loss: 0.405829  [  224/  265]
train() client id: f_00001-12-7 loss: 0.458573  [  256/  265]
train() client id: f_00001-13-0 loss: 0.427440  [   32/  265]
train() client id: f_00001-13-1 loss: 0.561077  [   64/  265]
train() client id: f_00001-13-2 loss: 0.307379  [   96/  265]
train() client id: f_00001-13-3 loss: 0.425463  [  128/  265]
train() client id: f_00001-13-4 loss: 0.385400  [  160/  265]
train() client id: f_00001-13-5 loss: 0.377984  [  192/  265]
train() client id: f_00001-13-6 loss: 0.336338  [  224/  265]
train() client id: f_00001-13-7 loss: 0.340873  [  256/  265]
train() client id: f_00001-14-0 loss: 0.357204  [   32/  265]
train() client id: f_00001-14-1 loss: 0.366102  [   64/  265]
train() client id: f_00001-14-2 loss: 0.416158  [   96/  265]
train() client id: f_00001-14-3 loss: 0.446700  [  128/  265]
train() client id: f_00001-14-4 loss: 0.379352  [  160/  265]
train() client id: f_00001-14-5 loss: 0.514529  [  192/  265]
train() client id: f_00001-14-6 loss: 0.394209  [  224/  265]
train() client id: f_00001-14-7 loss: 0.419313  [  256/  265]
train() client id: f_00002-0-0 loss: 1.354999  [   32/  124]
train() client id: f_00002-0-1 loss: 1.152515  [   64/  124]
train() client id: f_00002-0-2 loss: 1.024985  [   96/  124]
train() client id: f_00002-1-0 loss: 1.163749  [   32/  124]
train() client id: f_00002-1-1 loss: 1.060391  [   64/  124]
train() client id: f_00002-1-2 loss: 1.397070  [   96/  124]
train() client id: f_00002-2-0 loss: 1.095153  [   32/  124]
train() client id: f_00002-2-1 loss: 1.161664  [   64/  124]
train() client id: f_00002-2-2 loss: 1.183078  [   96/  124]
train() client id: f_00002-3-0 loss: 0.927626  [   32/  124]
train() client id: f_00002-3-1 loss: 1.302342  [   64/  124]
train() client id: f_00002-3-2 loss: 1.035537  [   96/  124]
train() client id: f_00002-4-0 loss: 1.164959  [   32/  124]
train() client id: f_00002-4-1 loss: 1.105715  [   64/  124]
train() client id: f_00002-4-2 loss: 1.087534  [   96/  124]
train() client id: f_00002-5-0 loss: 1.154786  [   32/  124]
train() client id: f_00002-5-1 loss: 0.934346  [   64/  124]
train() client id: f_00002-5-2 loss: 1.035150  [   96/  124]
train() client id: f_00002-6-0 loss: 1.045938  [   32/  124]
train() client id: f_00002-6-1 loss: 1.067927  [   64/  124]
train() client id: f_00002-6-2 loss: 1.012159  [   96/  124]
train() client id: f_00002-7-0 loss: 1.008271  [   32/  124]
train() client id: f_00002-7-1 loss: 1.313246  [   64/  124]
train() client id: f_00002-7-2 loss: 0.971751  [   96/  124]
train() client id: f_00002-8-0 loss: 1.068767  [   32/  124]
train() client id: f_00002-8-1 loss: 1.031241  [   64/  124]
train() client id: f_00002-8-2 loss: 1.064434  [   96/  124]
train() client id: f_00002-9-0 loss: 1.025638  [   32/  124]
train() client id: f_00002-9-1 loss: 1.050792  [   64/  124]
train() client id: f_00002-9-2 loss: 1.049319  [   96/  124]
train() client id: f_00002-10-0 loss: 0.967907  [   32/  124]
train() client id: f_00002-10-1 loss: 1.002570  [   64/  124]
train() client id: f_00002-10-2 loss: 1.011095  [   96/  124]
train() client id: f_00002-11-0 loss: 1.006097  [   32/  124]
train() client id: f_00002-11-1 loss: 0.914425  [   64/  124]
train() client id: f_00002-11-2 loss: 1.021387  [   96/  124]
train() client id: f_00002-12-0 loss: 1.038515  [   32/  124]
train() client id: f_00002-12-1 loss: 1.229981  [   64/  124]
train() client id: f_00002-12-2 loss: 0.880491  [   96/  124]
train() client id: f_00002-13-0 loss: 1.097918  [   32/  124]
train() client id: f_00002-13-1 loss: 1.111837  [   64/  124]
train() client id: f_00002-13-2 loss: 0.980978  [   96/  124]
train() client id: f_00002-14-0 loss: 1.075983  [   32/  124]
train() client id: f_00002-14-1 loss: 1.170277  [   64/  124]
train() client id: f_00002-14-2 loss: 0.844408  [   96/  124]
train() client id: f_00003-0-0 loss: 0.484400  [   32/   43]
train() client id: f_00003-1-0 loss: 0.490484  [   32/   43]
train() client id: f_00003-2-0 loss: 0.759040  [   32/   43]
train() client id: f_00003-3-0 loss: 0.502133  [   32/   43]
train() client id: f_00003-4-0 loss: 0.371740  [   32/   43]
train() client id: f_00003-5-0 loss: 0.470644  [   32/   43]
train() client id: f_00003-6-0 loss: 0.570571  [   32/   43]
train() client id: f_00003-7-0 loss: 0.614918  [   32/   43]
train() client id: f_00003-8-0 loss: 0.343442  [   32/   43]
train() client id: f_00003-9-0 loss: 0.675341  [   32/   43]
train() client id: f_00003-10-0 loss: 0.474696  [   32/   43]
train() client id: f_00003-11-0 loss: 0.605020  [   32/   43]
train() client id: f_00003-12-0 loss: 0.589970  [   32/   43]
train() client id: f_00003-13-0 loss: 0.572584  [   32/   43]
train() client id: f_00003-14-0 loss: 0.393467  [   32/   43]
train() client id: f_00004-0-0 loss: 0.858120  [   32/  306]
train() client id: f_00004-0-1 loss: 1.061083  [   64/  306]
train() client id: f_00004-0-2 loss: 1.019135  [   96/  306]
train() client id: f_00004-0-3 loss: 0.855787  [  128/  306]
train() client id: f_00004-0-4 loss: 1.029255  [  160/  306]
train() client id: f_00004-0-5 loss: 0.944298  [  192/  306]
train() client id: f_00004-0-6 loss: 0.813283  [  224/  306]
train() client id: f_00004-0-7 loss: 0.901311  [  256/  306]
train() client id: f_00004-0-8 loss: 1.036600  [  288/  306]
train() client id: f_00004-1-0 loss: 1.140874  [   32/  306]
train() client id: f_00004-1-1 loss: 0.849182  [   64/  306]
train() client id: f_00004-1-2 loss: 0.913216  [   96/  306]
train() client id: f_00004-1-3 loss: 0.973210  [  128/  306]
train() client id: f_00004-1-4 loss: 0.946038  [  160/  306]
train() client id: f_00004-1-5 loss: 0.864931  [  192/  306]
train() client id: f_00004-1-6 loss: 0.999144  [  224/  306]
train() client id: f_00004-1-7 loss: 0.969420  [  256/  306]
train() client id: f_00004-1-8 loss: 0.901582  [  288/  306]
train() client id: f_00004-2-0 loss: 0.944350  [   32/  306]
train() client id: f_00004-2-1 loss: 1.035298  [   64/  306]
train() client id: f_00004-2-2 loss: 0.873759  [   96/  306]
train() client id: f_00004-2-3 loss: 0.851202  [  128/  306]
train() client id: f_00004-2-4 loss: 0.852802  [  160/  306]
train() client id: f_00004-2-5 loss: 0.959433  [  192/  306]
train() client id: f_00004-2-6 loss: 0.857687  [  224/  306]
train() client id: f_00004-2-7 loss: 0.981057  [  256/  306]
train() client id: f_00004-2-8 loss: 0.935690  [  288/  306]
train() client id: f_00004-3-0 loss: 0.996254  [   32/  306]
train() client id: f_00004-3-1 loss: 0.882941  [   64/  306]
train() client id: f_00004-3-2 loss: 0.913671  [   96/  306]
train() client id: f_00004-3-3 loss: 1.088449  [  128/  306]
train() client id: f_00004-3-4 loss: 0.849311  [  160/  306]
train() client id: f_00004-3-5 loss: 0.901099  [  192/  306]
train() client id: f_00004-3-6 loss: 0.924675  [  224/  306]
train() client id: f_00004-3-7 loss: 0.833375  [  256/  306]
train() client id: f_00004-3-8 loss: 1.061643  [  288/  306]
train() client id: f_00004-4-0 loss: 1.002788  [   32/  306]
train() client id: f_00004-4-1 loss: 0.954510  [   64/  306]
train() client id: f_00004-4-2 loss: 0.899516  [   96/  306]
train() client id: f_00004-4-3 loss: 0.909876  [  128/  306]
train() client id: f_00004-4-4 loss: 1.021338  [  160/  306]
train() client id: f_00004-4-5 loss: 0.805591  [  192/  306]
train() client id: f_00004-4-6 loss: 1.039640  [  224/  306]
train() client id: f_00004-4-7 loss: 0.864630  [  256/  306]
train() client id: f_00004-4-8 loss: 0.908448  [  288/  306]
train() client id: f_00004-5-0 loss: 0.959089  [   32/  306]
train() client id: f_00004-5-1 loss: 1.048085  [   64/  306]
train() client id: f_00004-5-2 loss: 0.974200  [   96/  306]
train() client id: f_00004-5-3 loss: 0.827489  [  128/  306]
train() client id: f_00004-5-4 loss: 1.018853  [  160/  306]
train() client id: f_00004-5-5 loss: 0.895689  [  192/  306]
train() client id: f_00004-5-6 loss: 0.983981  [  224/  306]
train() client id: f_00004-5-7 loss: 0.910875  [  256/  306]
train() client id: f_00004-5-8 loss: 0.792015  [  288/  306]
train() client id: f_00004-6-0 loss: 0.997341  [   32/  306]
train() client id: f_00004-6-1 loss: 0.898705  [   64/  306]
train() client id: f_00004-6-2 loss: 0.976908  [   96/  306]
train() client id: f_00004-6-3 loss: 0.826211  [  128/  306]
train() client id: f_00004-6-4 loss: 0.995709  [  160/  306]
train() client id: f_00004-6-5 loss: 0.904486  [  192/  306]
train() client id: f_00004-6-6 loss: 0.850715  [  224/  306]
train() client id: f_00004-6-7 loss: 0.879046  [  256/  306]
train() client id: f_00004-6-8 loss: 0.940678  [  288/  306]
train() client id: f_00004-7-0 loss: 0.991490  [   32/  306]
train() client id: f_00004-7-1 loss: 0.875696  [   64/  306]
train() client id: f_00004-7-2 loss: 1.137635  [   96/  306]
train() client id: f_00004-7-3 loss: 0.827273  [  128/  306]
train() client id: f_00004-7-4 loss: 1.052146  [  160/  306]
train() client id: f_00004-7-5 loss: 0.924146  [  192/  306]
train() client id: f_00004-7-6 loss: 0.887761  [  224/  306]
train() client id: f_00004-7-7 loss: 0.957151  [  256/  306]
train() client id: f_00004-7-8 loss: 0.727613  [  288/  306]
train() client id: f_00004-8-0 loss: 1.012993  [   32/  306]
train() client id: f_00004-8-1 loss: 0.798697  [   64/  306]
train() client id: f_00004-8-2 loss: 0.996292  [   96/  306]
train() client id: f_00004-8-3 loss: 0.949450  [  128/  306]
train() client id: f_00004-8-4 loss: 1.046665  [  160/  306]
train() client id: f_00004-8-5 loss: 0.847184  [  192/  306]
train() client id: f_00004-8-6 loss: 0.852700  [  224/  306]
train() client id: f_00004-8-7 loss: 0.920822  [  256/  306]
train() client id: f_00004-8-8 loss: 0.952341  [  288/  306]
train() client id: f_00004-9-0 loss: 0.939247  [   32/  306]
train() client id: f_00004-9-1 loss: 0.992841  [   64/  306]
train() client id: f_00004-9-2 loss: 0.914361  [   96/  306]
train() client id: f_00004-9-3 loss: 0.948718  [  128/  306]
train() client id: f_00004-9-4 loss: 0.775122  [  160/  306]
train() client id: f_00004-9-5 loss: 0.928828  [  192/  306]
train() client id: f_00004-9-6 loss: 1.061653  [  224/  306]
train() client id: f_00004-9-7 loss: 0.887381  [  256/  306]
train() client id: f_00004-9-8 loss: 0.870946  [  288/  306]
train() client id: f_00004-10-0 loss: 0.872919  [   32/  306]
train() client id: f_00004-10-1 loss: 0.889246  [   64/  306]
train() client id: f_00004-10-2 loss: 0.904080  [   96/  306]
train() client id: f_00004-10-3 loss: 0.882275  [  128/  306]
train() client id: f_00004-10-4 loss: 0.963635  [  160/  306]
train() client id: f_00004-10-5 loss: 0.853388  [  192/  306]
train() client id: f_00004-10-6 loss: 0.975940  [  224/  306]
train() client id: f_00004-10-7 loss: 0.876709  [  256/  306]
train() client id: f_00004-10-8 loss: 1.137823  [  288/  306]
train() client id: f_00004-11-0 loss: 0.894094  [   32/  306]
train() client id: f_00004-11-1 loss: 0.919635  [   64/  306]
train() client id: f_00004-11-2 loss: 0.944025  [   96/  306]
train() client id: f_00004-11-3 loss: 0.900807  [  128/  306]
train() client id: f_00004-11-4 loss: 1.013322  [  160/  306]
train() client id: f_00004-11-5 loss: 0.944458  [  192/  306]
train() client id: f_00004-11-6 loss: 0.870774  [  224/  306]
train() client id: f_00004-11-7 loss: 0.891312  [  256/  306]
train() client id: f_00004-11-8 loss: 0.870252  [  288/  306]
train() client id: f_00004-12-0 loss: 0.972292  [   32/  306]
train() client id: f_00004-12-1 loss: 0.894346  [   64/  306]
train() client id: f_00004-12-2 loss: 0.876688  [   96/  306]
train() client id: f_00004-12-3 loss: 0.899079  [  128/  306]
train() client id: f_00004-12-4 loss: 0.917100  [  160/  306]
train() client id: f_00004-12-5 loss: 1.035500  [  192/  306]
train() client id: f_00004-12-6 loss: 0.914943  [  224/  306]
train() client id: f_00004-12-7 loss: 0.859861  [  256/  306]
train() client id: f_00004-12-8 loss: 0.903423  [  288/  306]
train() client id: f_00004-13-0 loss: 0.931785  [   32/  306]
train() client id: f_00004-13-1 loss: 0.805042  [   64/  306]
train() client id: f_00004-13-2 loss: 0.938550  [   96/  306]
train() client id: f_00004-13-3 loss: 0.838556  [  128/  306]
train() client id: f_00004-13-4 loss: 0.865986  [  160/  306]
train() client id: f_00004-13-5 loss: 0.941856  [  192/  306]
train() client id: f_00004-13-6 loss: 0.915371  [  224/  306]
train() client id: f_00004-13-7 loss: 0.904291  [  256/  306]
train() client id: f_00004-13-8 loss: 1.024126  [  288/  306]
train() client id: f_00004-14-0 loss: 0.959176  [   32/  306]
train() client id: f_00004-14-1 loss: 0.930575  [   64/  306]
train() client id: f_00004-14-2 loss: 0.957237  [   96/  306]
train() client id: f_00004-14-3 loss: 0.793316  [  128/  306]
train() client id: f_00004-14-4 loss: 0.922270  [  160/  306]
train() client id: f_00004-14-5 loss: 0.944087  [  192/  306]
train() client id: f_00004-14-6 loss: 0.805609  [  224/  306]
train() client id: f_00004-14-7 loss: 0.923489  [  256/  306]
train() client id: f_00004-14-8 loss: 1.023532  [  288/  306]
train() client id: f_00005-0-0 loss: 0.399015  [   32/  146]
train() client id: f_00005-0-1 loss: 0.483374  [   64/  146]
train() client id: f_00005-0-2 loss: 0.380559  [   96/  146]
train() client id: f_00005-0-3 loss: 0.335976  [  128/  146]
train() client id: f_00005-1-0 loss: 0.633034  [   32/  146]
train() client id: f_00005-1-1 loss: 0.491904  [   64/  146]
train() client id: f_00005-1-2 loss: 0.290910  [   96/  146]
train() client id: f_00005-1-3 loss: 0.399035  [  128/  146]
train() client id: f_00005-2-0 loss: 0.558372  [   32/  146]
train() client id: f_00005-2-1 loss: 0.526391  [   64/  146]
train() client id: f_00005-2-2 loss: 0.270311  [   96/  146]
train() client id: f_00005-2-3 loss: 0.412873  [  128/  146]
train() client id: f_00005-3-0 loss: 0.329619  [   32/  146]
train() client id: f_00005-3-1 loss: 0.436682  [   64/  146]
train() client id: f_00005-3-2 loss: 0.380090  [   96/  146]
train() client id: f_00005-3-3 loss: 0.621973  [  128/  146]
train() client id: f_00005-4-0 loss: 0.449868  [   32/  146]
train() client id: f_00005-4-1 loss: 0.679455  [   64/  146]
train() client id: f_00005-4-2 loss: 0.268515  [   96/  146]
train() client id: f_00005-4-3 loss: 0.412099  [  128/  146]
train() client id: f_00005-5-0 loss: 0.535522  [   32/  146]
train() client id: f_00005-5-1 loss: 0.574985  [   64/  146]
train() client id: f_00005-5-2 loss: 0.299702  [   96/  146]
train() client id: f_00005-5-3 loss: 0.386758  [  128/  146]
train() client id: f_00005-6-0 loss: 0.445645  [   32/  146]
train() client id: f_00005-6-1 loss: 0.397742  [   64/  146]
train() client id: f_00005-6-2 loss: 0.631929  [   96/  146]
train() client id: f_00005-6-3 loss: 0.449151  [  128/  146]
train() client id: f_00005-7-0 loss: 0.226603  [   32/  146]
train() client id: f_00005-7-1 loss: 0.406395  [   64/  146]
train() client id: f_00005-7-2 loss: 0.627550  [   96/  146]
train() client id: f_00005-7-3 loss: 0.526693  [  128/  146]
train() client id: f_00005-8-0 loss: 0.294153  [   32/  146]
train() client id: f_00005-8-1 loss: 0.581633  [   64/  146]
train() client id: f_00005-8-2 loss: 0.363286  [   96/  146]
train() client id: f_00005-8-3 loss: 0.520551  [  128/  146]
train() client id: f_00005-9-0 loss: 0.402729  [   32/  146]
train() client id: f_00005-9-1 loss: 0.566094  [   64/  146]
train() client id: f_00005-9-2 loss: 0.336770  [   96/  146]
train() client id: f_00005-9-3 loss: 0.417548  [  128/  146]
train() client id: f_00005-10-0 loss: 0.790709  [   32/  146]
train() client id: f_00005-10-1 loss: 0.397671  [   64/  146]
train() client id: f_00005-10-2 loss: 0.207757  [   96/  146]
train() client id: f_00005-10-3 loss: 0.456232  [  128/  146]
train() client id: f_00005-11-0 loss: 0.503491  [   32/  146]
train() client id: f_00005-11-1 loss: 0.599894  [   64/  146]
train() client id: f_00005-11-2 loss: 0.411713  [   96/  146]
train() client id: f_00005-11-3 loss: 0.219840  [  128/  146]
train() client id: f_00005-12-0 loss: 0.551853  [   32/  146]
train() client id: f_00005-12-1 loss: 0.436766  [   64/  146]
train() client id: f_00005-12-2 loss: 0.406072  [   96/  146]
train() client id: f_00005-12-3 loss: 0.231054  [  128/  146]
train() client id: f_00005-13-0 loss: 0.248783  [   32/  146]
train() client id: f_00005-13-1 loss: 0.911701  [   64/  146]
train() client id: f_00005-13-2 loss: 0.407415  [   96/  146]
train() client id: f_00005-13-3 loss: 0.102543  [  128/  146]
train() client id: f_00005-14-0 loss: 0.467622  [   32/  146]
train() client id: f_00005-14-1 loss: 0.424343  [   64/  146]
train() client id: f_00005-14-2 loss: 0.278481  [   96/  146]
train() client id: f_00005-14-3 loss: 0.540105  [  128/  146]
train() client id: f_00006-0-0 loss: 0.420173  [   32/   54]
train() client id: f_00006-1-0 loss: 0.492016  [   32/   54]
train() client id: f_00006-2-0 loss: 0.473708  [   32/   54]
train() client id: f_00006-3-0 loss: 0.508690  [   32/   54]
train() client id: f_00006-4-0 loss: 0.486682  [   32/   54]
train() client id: f_00006-5-0 loss: 0.421543  [   32/   54]
train() client id: f_00006-6-0 loss: 0.529378  [   32/   54]
train() client id: f_00006-7-0 loss: 0.406170  [   32/   54]
train() client id: f_00006-8-0 loss: 0.467812  [   32/   54]
train() client id: f_00006-9-0 loss: 0.398871  [   32/   54]
train() client id: f_00006-10-0 loss: 0.507432  [   32/   54]
train() client id: f_00006-11-0 loss: 0.510911  [   32/   54]
train() client id: f_00006-12-0 loss: 0.516285  [   32/   54]
train() client id: f_00006-13-0 loss: 0.461101  [   32/   54]
train() client id: f_00006-14-0 loss: 0.520675  [   32/   54]
train() client id: f_00007-0-0 loss: 0.505615  [   32/  179]
train() client id: f_00007-0-1 loss: 0.585286  [   64/  179]
train() client id: f_00007-0-2 loss: 0.508256  [   96/  179]
train() client id: f_00007-0-3 loss: 0.451566  [  128/  179]
train() client id: f_00007-0-4 loss: 0.574062  [  160/  179]
train() client id: f_00007-1-0 loss: 0.548359  [   32/  179]
train() client id: f_00007-1-1 loss: 0.422641  [   64/  179]
train() client id: f_00007-1-2 loss: 0.459249  [   96/  179]
train() client id: f_00007-1-3 loss: 0.391421  [  128/  179]
train() client id: f_00007-1-4 loss: 0.704821  [  160/  179]
train() client id: f_00007-2-0 loss: 0.510177  [   32/  179]
train() client id: f_00007-2-1 loss: 0.433547  [   64/  179]
train() client id: f_00007-2-2 loss: 0.686020  [   96/  179]
train() client id: f_00007-2-3 loss: 0.506760  [  128/  179]
train() client id: f_00007-2-4 loss: 0.308289  [  160/  179]
train() client id: f_00007-3-0 loss: 0.375332  [   32/  179]
train() client id: f_00007-3-1 loss: 0.475592  [   64/  179]
train() client id: f_00007-3-2 loss: 0.496335  [   96/  179]
train() client id: f_00007-3-3 loss: 0.451120  [  128/  179]
train() client id: f_00007-3-4 loss: 0.576394  [  160/  179]
train() client id: f_00007-4-0 loss: 0.787850  [   32/  179]
train() client id: f_00007-4-1 loss: 0.398709  [   64/  179]
train() client id: f_00007-4-2 loss: 0.520127  [   96/  179]
train() client id: f_00007-4-3 loss: 0.361640  [  128/  179]
train() client id: f_00007-4-4 loss: 0.319535  [  160/  179]
train() client id: f_00007-5-0 loss: 0.613575  [   32/  179]
train() client id: f_00007-5-1 loss: 0.417154  [   64/  179]
train() client id: f_00007-5-2 loss: 0.357676  [   96/  179]
train() client id: f_00007-5-3 loss: 0.441288  [  128/  179]
train() client id: f_00007-5-4 loss: 0.389234  [  160/  179]
train() client id: f_00007-6-0 loss: 0.358639  [   32/  179]
train() client id: f_00007-6-1 loss: 0.616846  [   64/  179]
train() client id: f_00007-6-2 loss: 0.322875  [   96/  179]
train() client id: f_00007-6-3 loss: 0.361099  [  128/  179]
train() client id: f_00007-6-4 loss: 0.568133  [  160/  179]
train() client id: f_00007-7-0 loss: 0.732054  [   32/  179]
train() client id: f_00007-7-1 loss: 0.383938  [   64/  179]
train() client id: f_00007-7-2 loss: 0.316329  [   96/  179]
train() client id: f_00007-7-3 loss: 0.439788  [  128/  179]
train() client id: f_00007-7-4 loss: 0.397286  [  160/  179]
train() client id: f_00007-8-0 loss: 0.394521  [   32/  179]
train() client id: f_00007-8-1 loss: 0.510216  [   64/  179]
train() client id: f_00007-8-2 loss: 0.512914  [   96/  179]
train() client id: f_00007-8-3 loss: 0.319258  [  128/  179]
train() client id: f_00007-8-4 loss: 0.606026  [  160/  179]
train() client id: f_00007-9-0 loss: 0.453371  [   32/  179]
train() client id: f_00007-9-1 loss: 0.350262  [   64/  179]
train() client id: f_00007-9-2 loss: 0.412027  [   96/  179]
train() client id: f_00007-9-3 loss: 0.450016  [  128/  179]
train() client id: f_00007-9-4 loss: 0.303948  [  160/  179]
train() client id: f_00007-10-0 loss: 0.268681  [   32/  179]
train() client id: f_00007-10-1 loss: 0.500703  [   64/  179]
train() client id: f_00007-10-2 loss: 0.517865  [   96/  179]
train() client id: f_00007-10-3 loss: 0.452969  [  128/  179]
train() client id: f_00007-10-4 loss: 0.392565  [  160/  179]
train() client id: f_00007-11-0 loss: 0.749485  [   32/  179]
train() client id: f_00007-11-1 loss: 0.399720  [   64/  179]
train() client id: f_00007-11-2 loss: 0.405425  [   96/  179]
train() client id: f_00007-11-3 loss: 0.398216  [  128/  179]
train() client id: f_00007-11-4 loss: 0.349926  [  160/  179]
train() client id: f_00007-12-0 loss: 0.635916  [   32/  179]
train() client id: f_00007-12-1 loss: 0.464817  [   64/  179]
train() client id: f_00007-12-2 loss: 0.408348  [   96/  179]
train() client id: f_00007-12-3 loss: 0.298437  [  128/  179]
train() client id: f_00007-12-4 loss: 0.517203  [  160/  179]
train() client id: f_00007-13-0 loss: 0.553825  [   32/  179]
train() client id: f_00007-13-1 loss: 0.486930  [   64/  179]
train() client id: f_00007-13-2 loss: 0.394482  [   96/  179]
train() client id: f_00007-13-3 loss: 0.281358  [  128/  179]
train() client id: f_00007-13-4 loss: 0.514930  [  160/  179]
train() client id: f_00007-14-0 loss: 0.308371  [   32/  179]
train() client id: f_00007-14-1 loss: 0.780135  [   64/  179]
train() client id: f_00007-14-2 loss: 0.386642  [   96/  179]
train() client id: f_00007-14-3 loss: 0.381738  [  128/  179]
train() client id: f_00007-14-4 loss: 0.470604  [  160/  179]
train() client id: f_00008-0-0 loss: 0.718921  [   32/  130]
train() client id: f_00008-0-1 loss: 0.776508  [   64/  130]
train() client id: f_00008-0-2 loss: 0.700572  [   96/  130]
train() client id: f_00008-0-3 loss: 0.632930  [  128/  130]
train() client id: f_00008-1-0 loss: 0.669826  [   32/  130]
train() client id: f_00008-1-1 loss: 0.741729  [   64/  130]
train() client id: f_00008-1-2 loss: 0.776538  [   96/  130]
train() client id: f_00008-1-3 loss: 0.640119  [  128/  130]
train() client id: f_00008-2-0 loss: 0.618409  [   32/  130]
train() client id: f_00008-2-1 loss: 0.787361  [   64/  130]
train() client id: f_00008-2-2 loss: 0.700329  [   96/  130]
train() client id: f_00008-2-3 loss: 0.714082  [  128/  130]
train() client id: f_00008-3-0 loss: 0.774930  [   32/  130]
train() client id: f_00008-3-1 loss: 0.797567  [   64/  130]
train() client id: f_00008-3-2 loss: 0.687895  [   96/  130]
train() client id: f_00008-3-3 loss: 0.616795  [  128/  130]
train() client id: f_00008-4-0 loss: 0.734258  [   32/  130]
train() client id: f_00008-4-1 loss: 0.702084  [   64/  130]
train() client id: f_00008-4-2 loss: 0.612320  [   96/  130]
train() client id: f_00008-4-3 loss: 0.795995  [  128/  130]
train() client id: f_00008-5-0 loss: 0.602792  [   32/  130]
train() client id: f_00008-5-1 loss: 0.839118  [   64/  130]
train() client id: f_00008-5-2 loss: 0.784552  [   96/  130]
train() client id: f_00008-5-3 loss: 0.651352  [  128/  130]
train() client id: f_00008-6-0 loss: 0.739761  [   32/  130]
train() client id: f_00008-6-1 loss: 0.585337  [   64/  130]
train() client id: f_00008-6-2 loss: 0.804884  [   96/  130]
train() client id: f_00008-6-3 loss: 0.698881  [  128/  130]
train() client id: f_00008-7-0 loss: 0.618827  [   32/  130]
train() client id: f_00008-7-1 loss: 0.709675  [   64/  130]
train() client id: f_00008-7-2 loss: 0.827110  [   96/  130]
train() client id: f_00008-7-3 loss: 0.725385  [  128/  130]
train() client id: f_00008-8-0 loss: 0.695175  [   32/  130]
train() client id: f_00008-8-1 loss: 0.730578  [   64/  130]
train() client id: f_00008-8-2 loss: 0.675580  [   96/  130]
train() client id: f_00008-8-3 loss: 0.763278  [  128/  130]
train() client id: f_00008-9-0 loss: 0.755846  [   32/  130]
train() client id: f_00008-9-1 loss: 0.681646  [   64/  130]
train() client id: f_00008-9-2 loss: 0.786287  [   96/  130]
train() client id: f_00008-9-3 loss: 0.628897  [  128/  130]
train() client id: f_00008-10-0 loss: 0.928919  [   32/  130]
train() client id: f_00008-10-1 loss: 0.625995  [   64/  130]
train() client id: f_00008-10-2 loss: 0.670644  [   96/  130]
train() client id: f_00008-10-3 loss: 0.617337  [  128/  130]
train() client id: f_00008-11-0 loss: 0.766327  [   32/  130]
train() client id: f_00008-11-1 loss: 0.816682  [   64/  130]
train() client id: f_00008-11-2 loss: 0.630521  [   96/  130]
train() client id: f_00008-11-3 loss: 0.656665  [  128/  130]
train() client id: f_00008-12-0 loss: 0.601553  [   32/  130]
train() client id: f_00008-12-1 loss: 0.666085  [   64/  130]
train() client id: f_00008-12-2 loss: 0.824459  [   96/  130]
train() client id: f_00008-12-3 loss: 0.749996  [  128/  130]
train() client id: f_00008-13-0 loss: 0.728533  [   32/  130]
train() client id: f_00008-13-1 loss: 0.708222  [   64/  130]
train() client id: f_00008-13-2 loss: 0.575351  [   96/  130]
train() client id: f_00008-13-3 loss: 0.812085  [  128/  130]
train() client id: f_00008-14-0 loss: 0.676079  [   32/  130]
train() client id: f_00008-14-1 loss: 0.700767  [   64/  130]
train() client id: f_00008-14-2 loss: 0.812015  [   96/  130]
train() client id: f_00008-14-3 loss: 0.686558  [  128/  130]
train() client id: f_00009-0-0 loss: 0.993531  [   32/  118]
train() client id: f_00009-0-1 loss: 1.164497  [   64/  118]
train() client id: f_00009-0-2 loss: 1.217617  [   96/  118]
train() client id: f_00009-1-0 loss: 1.059550  [   32/  118]
train() client id: f_00009-1-1 loss: 1.013736  [   64/  118]
train() client id: f_00009-1-2 loss: 1.093308  [   96/  118]
train() client id: f_00009-2-0 loss: 1.073542  [   32/  118]
train() client id: f_00009-2-1 loss: 0.906880  [   64/  118]
train() client id: f_00009-2-2 loss: 1.061517  [   96/  118]
train() client id: f_00009-3-0 loss: 1.064275  [   32/  118]
train() client id: f_00009-3-1 loss: 1.055099  [   64/  118]
train() client id: f_00009-3-2 loss: 0.886940  [   96/  118]
train() client id: f_00009-4-0 loss: 0.902020  [   32/  118]
train() client id: f_00009-4-1 loss: 0.905225  [   64/  118]
train() client id: f_00009-4-2 loss: 0.923541  [   96/  118]
train() client id: f_00009-5-0 loss: 0.974883  [   32/  118]
train() client id: f_00009-5-1 loss: 0.953454  [   64/  118]
train() client id: f_00009-5-2 loss: 0.824317  [   96/  118]
train() client id: f_00009-6-0 loss: 0.791684  [   32/  118]
train() client id: f_00009-6-1 loss: 0.822921  [   64/  118]
train() client id: f_00009-6-2 loss: 1.010298  [   96/  118]
train() client id: f_00009-7-0 loss: 0.795189  [   32/  118]
train() client id: f_00009-7-1 loss: 0.944433  [   64/  118]
train() client id: f_00009-7-2 loss: 0.824186  [   96/  118]
train() client id: f_00009-8-0 loss: 0.846265  [   32/  118]
train() client id: f_00009-8-1 loss: 0.933240  [   64/  118]
train() client id: f_00009-8-2 loss: 0.718381  [   96/  118]
train() client id: f_00009-9-0 loss: 0.756375  [   32/  118]
train() client id: f_00009-9-1 loss: 0.811053  [   64/  118]
train() client id: f_00009-9-2 loss: 0.934091  [   96/  118]
train() client id: f_00009-10-0 loss: 0.796054  [   32/  118]
train() client id: f_00009-10-1 loss: 0.789233  [   64/  118]
train() client id: f_00009-10-2 loss: 0.877786  [   96/  118]
train() client id: f_00009-11-0 loss: 0.721303  [   32/  118]
train() client id: f_00009-11-1 loss: 0.959329  [   64/  118]
train() client id: f_00009-11-2 loss: 0.724529  [   96/  118]
train() client id: f_00009-12-0 loss: 0.808598  [   32/  118]
train() client id: f_00009-12-1 loss: 0.689083  [   64/  118]
train() client id: f_00009-12-2 loss: 0.834427  [   96/  118]
train() client id: f_00009-13-0 loss: 0.863846  [   32/  118]
train() client id: f_00009-13-1 loss: 0.722119  [   64/  118]
train() client id: f_00009-13-2 loss: 0.852491  [   96/  118]
train() client id: f_00009-14-0 loss: 0.803565  [   32/  118]
train() client id: f_00009-14-1 loss: 0.766996  [   64/  118]
train() client id: f_00009-14-2 loss: 0.684898  [   96/  118]
At round 36 accuracy: 0.6445623342175066
At round 36 training accuracy: 0.5888665325285044
At round 36 training loss: 0.8245979778910013
update_location
xs = 8.927491 301.223621 5.882650 0.934260 -217.581990 -65.230757 -25.849135 -5.143845 -240.120581 20.134486 
ys = -292.390647 7.291448 190.684448 -12.290817 -9.642386 0.794442 -1.381692 186.628436 25.881276 -727.232496 
xs mean: -21.682379970521218
ys mean: -63.16579882624052
dists_uav = 309.147199 317.472574 215.395368 100.756821 239.655790 119.397164 103.296112 211.793844 261.395742 734.351755 
uav_gains = -116.470404 -117.101697 -108.677524 -100.081879 -110.444332 -101.925050 -100.352125 -108.438663 -112.292688 -128.606535 
uav_gains_db_mean: -110.43908973848178
dists_bs = 502.278149 504.891172 181.561381 256.964422 189.488883 205.904985 230.990220 170.253734 162.717804 923.093140 
bs_gains = -115.193499 -115.256597 -102.819716 -107.043503 -103.339404 -104.349731 -105.747680 -102.037765 -101.487241 -122.593934 
bs_gains_db_mean: -107.9869070643514
Round 37
-------------------------------
ene_coms = [0.01840217 0.01967091 0.00713626 0.00632391 0.00731417 0.00684911
 0.00639588 0.00688403 0.00671672 0.04024362]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.24111191 12.82800955  5.97014128  2.13058944 14.57147613  7.00502793
  2.65140934  8.56465231  6.24703776  6.20530126]
obj_prev = 72.4147569079641
eta_min = 8.292804300683585e-16	eta_max = 0.7753045996891654
af = 15.14331232310346	bf = 1.953078911422346	zeta = 16.65764355541381	eta = 0.909090909090909
af = 15.14331232310346	bf = 1.953078911422346	zeta = 37.36029804947419	eta = 0.4053316786458717
af = 15.14331232310346	bf = 1.953078911422346	zeta = 26.28060095986801	eta = 0.5762163637820981
af = 15.14331232310346	bf = 1.953078911422346	zeta = 24.307601731210188	eta = 0.6229866891253171
af = 15.14331232310346	bf = 1.953078911422346	zeta = 24.188404626631844	eta = 0.6260566811599644
af = 15.14331232310346	bf = 1.953078911422346	zeta = 24.18791613742914	eta = 0.6260693247431193
af = 15.14331232310346	bf = 1.953078911422346	zeta = 24.187916129170638	eta = 0.6260693249568787
eta = 0.6260693249568787
ene_coms = [0.01840217 0.01967091 0.00713626 0.00632391 0.00731417 0.00684911
 0.00639588 0.00688403 0.00671672 0.04024362]
ene_comp = [0.03864251 0.08127195 0.03802914 0.01318752 0.0938461  0.04477624
 0.01656108 0.0548969  0.03986926 0.03618902]
ene_total = [2.3658721  4.18650562 1.87319061 0.80921748 4.19552282 2.1411108
 0.95211721 2.56230338 1.93210772 3.16996838]
ti_comp = [0.49388172 0.48119437 0.6065408  0.61466434 0.6047617  0.60941232
 0.61394463 0.60906313 0.61073621 0.27546726]
ti_coms = [0.18402172 0.19670907 0.07136264 0.06323909 0.07314174 0.06849111
 0.06395881 0.06884031 0.06716723 0.40243618]
t_total = [28.11537743 28.11537743 28.11537743 28.11537743 28.11537743 28.11537743
 28.11537743 28.11537743 28.11537743 28.11537743]
ene_coms = [0.01840217 0.01967091 0.00713626 0.00632391 0.00731417 0.00684911
 0.00639588 0.00688403 0.00671672 0.04024362]
ene_comp = [1.47853015e-05 1.44897537e-04 9.34349740e-06 3.79396899e-07
 1.41240803e-04 1.51077767e-05 7.53160139e-07 2.78739764e-05
 1.06190869e-05 3.90364675e-05]
ene_total = [0.76382515 0.82184097 0.29635702 0.26229364 0.30920598 0.28468672
 0.2652941  0.28666444 0.27900988 1.67068343]
optimize_network iter = 0 obj = 5.23986132264994
eta = 0.6260693249568787
freqs = [39121221.74935056 84448152.75645153 31349201.38660758 10727419.15222327
 77589323.44178721 36737232.74702043 13487435.26631524 45066677.21114372
 32640326.99602573 65686608.85331379]
eta_min = 0.62606932495688	eta_max = 0.6260693249568784
af = 0.013380414433693701	bf = 1.953078911422346	zeta = 0.014718455877063072	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01840217 0.01967091 0.00713626 0.00632391 0.00731417 0.00684911
 0.00639588 0.00688403 0.00671672 0.04024362]
ene_comp = [3.00919770e-06 2.94904595e-05 1.90164745e-06 7.72172468e-08
 2.87462179e-05 3.07482988e-06 1.53287896e-07 5.67308728e-06
 2.16126346e-06 7.94494779e-06]
ene_total = [3.13978528 3.36073945 1.21771725 1.07882445 1.25264687 1.16893135
 1.09111528 1.17533161 1.14619105 6.86661362]
ti_comp = [0.49388172 0.48119437 0.6065408  0.61466434 0.6047617  0.60941232
 0.61394463 0.60906313 0.61073621 0.27546726]
ti_coms = [0.18402172 0.19670907 0.07136264 0.06323909 0.07314174 0.06849111
 0.06395881 0.06884031 0.06716723 0.40243618]
t_total = [28.11537743 28.11537743 28.11537743 28.11537743 28.11537743 28.11537743
 28.11537743 28.11537743 28.11537743 28.11537743]
ene_coms = [0.01840217 0.01967091 0.00713626 0.00632391 0.00731417 0.00684911
 0.00639588 0.00688403 0.00671672 0.04024362]
ene_comp = [1.47853015e-05 1.44897537e-04 9.34349740e-06 3.79396899e-07
 1.41240803e-04 1.51077767e-05 7.53160139e-07 2.78739764e-05
 1.06190869e-05 3.90364675e-05]
ene_total = [0.76382515 0.82184097 0.29635702 0.26229364 0.30920598 0.28468672
 0.2652941  0.28666444 0.27900988 1.67068343]
optimize_network iter = 1 obj = 5.2398613226499355
eta = 0.6260693249568784
freqs = [39121221.74935056 84448152.75645152 31349201.38660758 10727419.15222327
 77589323.44178723 36737232.74702044 13487435.26631524 45066677.21114372
 32640326.99602573 65686608.85331371]
Done!
ene_coms = [0.01840217 0.01967091 0.00713626 0.00632391 0.00731417 0.00684911
 0.00639588 0.00688403 0.00671672 0.04024362]
ene_comp = [1.44629414e-05 1.41738374e-04 9.13978358e-06 3.71125007e-07
 1.38161366e-04 1.47783858e-05 7.36739186e-07 2.72662474e-05
 1.03875617e-05 3.81853657e-05]
ene_total = [0.01841663 0.01981265 0.0071454  0.00632428 0.00745234 0.00686389
 0.00639662 0.0069113  0.00672711 0.0402818 ]
At round 37 energy consumption: 0.12633201637342756
At round 37 eta: 0.6260693249568784
At round 37 a_n: 15.508406518389636
At round 37 local rounds: 15.334330387123918
At round 37 global rounds: 41.474015247883074
gradient difference: 0.3551298975944519
train() client id: f_00000-0-0 loss: 1.379554  [   32/  126]
train() client id: f_00000-0-1 loss: 1.414862  [   64/  126]
train() client id: f_00000-0-2 loss: 1.106026  [   96/  126]
train() client id: f_00000-1-0 loss: 1.491059  [   32/  126]
train() client id: f_00000-1-1 loss: 1.310490  [   64/  126]
train() client id: f_00000-1-2 loss: 0.842504  [   96/  126]
train() client id: f_00000-2-0 loss: 1.192488  [   32/  126]
train() client id: f_00000-2-1 loss: 0.983477  [   64/  126]
train() client id: f_00000-2-2 loss: 1.053744  [   96/  126]
train() client id: f_00000-3-0 loss: 0.915486  [   32/  126]
train() client id: f_00000-3-1 loss: 0.969126  [   64/  126]
train() client id: f_00000-3-2 loss: 0.984044  [   96/  126]
train() client id: f_00000-4-0 loss: 0.977404  [   32/  126]
train() client id: f_00000-4-1 loss: 0.965997  [   64/  126]
train() client id: f_00000-4-2 loss: 1.043983  [   96/  126]
train() client id: f_00000-5-0 loss: 0.837498  [   32/  126]
train() client id: f_00000-5-1 loss: 0.868125  [   64/  126]
train() client id: f_00000-5-2 loss: 1.050238  [   96/  126]
train() client id: f_00000-6-0 loss: 0.844429  [   32/  126]
train() client id: f_00000-6-1 loss: 1.008052  [   64/  126]
train() client id: f_00000-6-2 loss: 0.905849  [   96/  126]
train() client id: f_00000-7-0 loss: 0.885368  [   32/  126]
train() client id: f_00000-7-1 loss: 0.957569  [   64/  126]
train() client id: f_00000-7-2 loss: 0.808938  [   96/  126]
train() client id: f_00000-8-0 loss: 0.820628  [   32/  126]
train() client id: f_00000-8-1 loss: 0.875194  [   64/  126]
train() client id: f_00000-8-2 loss: 0.864301  [   96/  126]
train() client id: f_00000-9-0 loss: 1.006191  [   32/  126]
train() client id: f_00000-9-1 loss: 0.706760  [   64/  126]
train() client id: f_00000-9-2 loss: 0.855344  [   96/  126]
train() client id: f_00000-10-0 loss: 0.897474  [   32/  126]
train() client id: f_00000-10-1 loss: 0.928152  [   64/  126]
train() client id: f_00000-10-2 loss: 0.737822  [   96/  126]
train() client id: f_00000-11-0 loss: 0.963884  [   32/  126]
train() client id: f_00000-11-1 loss: 0.804179  [   64/  126]
train() client id: f_00000-11-2 loss: 0.795077  [   96/  126]
train() client id: f_00000-12-0 loss: 0.826707  [   32/  126]
train() client id: f_00000-12-1 loss: 0.969779  [   64/  126]
train() client id: f_00000-12-2 loss: 0.677812  [   96/  126]
train() client id: f_00000-13-0 loss: 0.721967  [   32/  126]
train() client id: f_00000-13-1 loss: 0.826577  [   64/  126]
train() client id: f_00000-13-2 loss: 0.921410  [   96/  126]
train() client id: f_00000-14-0 loss: 0.853327  [   32/  126]
train() client id: f_00000-14-1 loss: 0.776486  [   64/  126]
train() client id: f_00000-14-2 loss: 0.879841  [   96/  126]
train() client id: f_00001-0-0 loss: 0.429450  [   32/  265]
train() client id: f_00001-0-1 loss: 0.414718  [   64/  265]
train() client id: f_00001-0-2 loss: 0.383938  [   96/  265]
train() client id: f_00001-0-3 loss: 0.396665  [  128/  265]
train() client id: f_00001-0-4 loss: 0.431514  [  160/  265]
train() client id: f_00001-0-5 loss: 0.400274  [  192/  265]
train() client id: f_00001-0-6 loss: 0.383965  [  224/  265]
train() client id: f_00001-0-7 loss: 0.421292  [  256/  265]
train() client id: f_00001-1-0 loss: 0.398968  [   32/  265]
train() client id: f_00001-1-1 loss: 0.448660  [   64/  265]
train() client id: f_00001-1-2 loss: 0.391922  [   96/  265]
train() client id: f_00001-1-3 loss: 0.281271  [  128/  265]
train() client id: f_00001-1-4 loss: 0.423591  [  160/  265]
train() client id: f_00001-1-5 loss: 0.375661  [  192/  265]
train() client id: f_00001-1-6 loss: 0.466331  [  224/  265]
train() client id: f_00001-1-7 loss: 0.389329  [  256/  265]
train() client id: f_00001-2-0 loss: 0.333501  [   32/  265]
train() client id: f_00001-2-1 loss: 0.563414  [   64/  265]
train() client id: f_00001-2-2 loss: 0.381642  [   96/  265]
train() client id: f_00001-2-3 loss: 0.392561  [  128/  265]
train() client id: f_00001-2-4 loss: 0.340059  [  160/  265]
train() client id: f_00001-2-5 loss: 0.510403  [  192/  265]
train() client id: f_00001-2-6 loss: 0.327425  [  224/  265]
train() client id: f_00001-2-7 loss: 0.277252  [  256/  265]
train() client id: f_00001-3-0 loss: 0.438915  [   32/  265]
train() client id: f_00001-3-1 loss: 0.363829  [   64/  265]
train() client id: f_00001-3-2 loss: 0.365769  [   96/  265]
train() client id: f_00001-3-3 loss: 0.345279  [  128/  265]
train() client id: f_00001-3-4 loss: 0.396768  [  160/  265]
train() client id: f_00001-3-5 loss: 0.440573  [  192/  265]
train() client id: f_00001-3-6 loss: 0.420872  [  224/  265]
train() client id: f_00001-3-7 loss: 0.294484  [  256/  265]
train() client id: f_00001-4-0 loss: 0.295583  [   32/  265]
train() client id: f_00001-4-1 loss: 0.388043  [   64/  265]
train() client id: f_00001-4-2 loss: 0.374321  [   96/  265]
train() client id: f_00001-4-3 loss: 0.401806  [  128/  265]
train() client id: f_00001-4-4 loss: 0.403509  [  160/  265]
train() client id: f_00001-4-5 loss: 0.464150  [  192/  265]
train() client id: f_00001-4-6 loss: 0.388333  [  224/  265]
train() client id: f_00001-4-7 loss: 0.296168  [  256/  265]
train() client id: f_00001-5-0 loss: 0.476132  [   32/  265]
train() client id: f_00001-5-1 loss: 0.363170  [   64/  265]
train() client id: f_00001-5-2 loss: 0.326007  [   96/  265]
train() client id: f_00001-5-3 loss: 0.427889  [  128/  265]
train() client id: f_00001-5-4 loss: 0.376539  [  160/  265]
train() client id: f_00001-5-5 loss: 0.270806  [  192/  265]
train() client id: f_00001-5-6 loss: 0.367298  [  224/  265]
train() client id: f_00001-5-7 loss: 0.287202  [  256/  265]
train() client id: f_00001-6-0 loss: 0.431844  [   32/  265]
train() client id: f_00001-6-1 loss: 0.421549  [   64/  265]
train() client id: f_00001-6-2 loss: 0.311303  [   96/  265]
train() client id: f_00001-6-3 loss: 0.341040  [  128/  265]
train() client id: f_00001-6-4 loss: 0.267768  [  160/  265]
train() client id: f_00001-6-5 loss: 0.371408  [  192/  265]
train() client id: f_00001-6-6 loss: 0.307806  [  224/  265]
train() client id: f_00001-6-7 loss: 0.506032  [  256/  265]
train() client id: f_00001-7-0 loss: 0.371120  [   32/  265]
train() client id: f_00001-7-1 loss: 0.342673  [   64/  265]
train() client id: f_00001-7-2 loss: 0.400818  [   96/  265]
train() client id: f_00001-7-3 loss: 0.278938  [  128/  265]
train() client id: f_00001-7-4 loss: 0.490721  [  160/  265]
train() client id: f_00001-7-5 loss: 0.280370  [  192/  265]
train() client id: f_00001-7-6 loss: 0.291917  [  224/  265]
train() client id: f_00001-7-7 loss: 0.440862  [  256/  265]
train() client id: f_00001-8-0 loss: 0.484832  [   32/  265]
train() client id: f_00001-8-1 loss: 0.378134  [   64/  265]
train() client id: f_00001-8-2 loss: 0.295546  [   96/  265]
train() client id: f_00001-8-3 loss: 0.439467  [  128/  265]
train() client id: f_00001-8-4 loss: 0.381730  [  160/  265]
train() client id: f_00001-8-5 loss: 0.252607  [  192/  265]
train() client id: f_00001-8-6 loss: 0.317287  [  224/  265]
train() client id: f_00001-8-7 loss: 0.369980  [  256/  265]
train() client id: f_00001-9-0 loss: 0.382098  [   32/  265]
train() client id: f_00001-9-1 loss: 0.480283  [   64/  265]
train() client id: f_00001-9-2 loss: 0.350274  [   96/  265]
train() client id: f_00001-9-3 loss: 0.348702  [  128/  265]
train() client id: f_00001-9-4 loss: 0.340019  [  160/  265]
train() client id: f_00001-9-5 loss: 0.357269  [  192/  265]
train() client id: f_00001-9-6 loss: 0.258497  [  224/  265]
train() client id: f_00001-9-7 loss: 0.391252  [  256/  265]
train() client id: f_00001-10-0 loss: 0.351258  [   32/  265]
train() client id: f_00001-10-1 loss: 0.523694  [   64/  265]
train() client id: f_00001-10-2 loss: 0.279564  [   96/  265]
train() client id: f_00001-10-3 loss: 0.330572  [  128/  265]
train() client id: f_00001-10-4 loss: 0.312341  [  160/  265]
train() client id: f_00001-10-5 loss: 0.360364  [  192/  265]
train() client id: f_00001-10-6 loss: 0.314263  [  224/  265]
train() client id: f_00001-10-7 loss: 0.432297  [  256/  265]
train() client id: f_00001-11-0 loss: 0.421059  [   32/  265]
train() client id: f_00001-11-1 loss: 0.362847  [   64/  265]
train() client id: f_00001-11-2 loss: 0.422137  [   96/  265]
train() client id: f_00001-11-3 loss: 0.261468  [  128/  265]
train() client id: f_00001-11-4 loss: 0.353908  [  160/  265]
train() client id: f_00001-11-5 loss: 0.256058  [  192/  265]
train() client id: f_00001-11-6 loss: 0.454737  [  224/  265]
train() client id: f_00001-11-7 loss: 0.364905  [  256/  265]
train() client id: f_00001-12-0 loss: 0.270236  [   32/  265]
train() client id: f_00001-12-1 loss: 0.413563  [   64/  265]
train() client id: f_00001-12-2 loss: 0.259852  [   96/  265]
train() client id: f_00001-12-3 loss: 0.349040  [  128/  265]
train() client id: f_00001-12-4 loss: 0.255763  [  160/  265]
train() client id: f_00001-12-5 loss: 0.490449  [  192/  265]
train() client id: f_00001-12-6 loss: 0.318515  [  224/  265]
train() client id: f_00001-12-7 loss: 0.465152  [  256/  265]
train() client id: f_00001-13-0 loss: 0.388877  [   32/  265]
train() client id: f_00001-13-1 loss: 0.267861  [   64/  265]
train() client id: f_00001-13-2 loss: 0.396026  [   96/  265]
train() client id: f_00001-13-3 loss: 0.261978  [  128/  265]
train() client id: f_00001-13-4 loss: 0.524817  [  160/  265]
train() client id: f_00001-13-5 loss: 0.268174  [  192/  265]
train() client id: f_00001-13-6 loss: 0.408999  [  224/  265]
train() client id: f_00001-13-7 loss: 0.365496  [  256/  265]
train() client id: f_00001-14-0 loss: 0.395657  [   32/  265]
train() client id: f_00001-14-1 loss: 0.422889  [   64/  265]
train() client id: f_00001-14-2 loss: 0.310571  [   96/  265]
train() client id: f_00001-14-3 loss: 0.405719  [  128/  265]
train() client id: f_00001-14-4 loss: 0.411163  [  160/  265]
train() client id: f_00001-14-5 loss: 0.335558  [  192/  265]
train() client id: f_00001-14-6 loss: 0.335592  [  224/  265]
train() client id: f_00001-14-7 loss: 0.268205  [  256/  265]
train() client id: f_00002-0-0 loss: 1.086621  [   32/  124]
train() client id: f_00002-0-1 loss: 1.475363  [   64/  124]
train() client id: f_00002-0-2 loss: 1.337964  [   96/  124]
train() client id: f_00002-1-0 loss: 1.451033  [   32/  124]
train() client id: f_00002-1-1 loss: 1.437519  [   64/  124]
train() client id: f_00002-1-2 loss: 1.075692  [   96/  124]
train() client id: f_00002-2-0 loss: 1.364619  [   32/  124]
train() client id: f_00002-2-1 loss: 1.114747  [   64/  124]
train() client id: f_00002-2-2 loss: 1.290652  [   96/  124]
train() client id: f_00002-3-0 loss: 1.273459  [   32/  124]
train() client id: f_00002-3-1 loss: 1.274809  [   64/  124]
train() client id: f_00002-3-2 loss: 1.129523  [   96/  124]
train() client id: f_00002-4-0 loss: 1.385065  [   32/  124]
train() client id: f_00002-4-1 loss: 1.066077  [   64/  124]
train() client id: f_00002-4-2 loss: 1.117093  [   96/  124]
train() client id: f_00002-5-0 loss: 1.169123  [   32/  124]
train() client id: f_00002-5-1 loss: 1.189022  [   64/  124]
train() client id: f_00002-5-2 loss: 1.147447  [   96/  124]
train() client id: f_00002-6-0 loss: 1.216776  [   32/  124]
train() client id: f_00002-6-1 loss: 1.153608  [   64/  124]
train() client id: f_00002-6-2 loss: 1.077207  [   96/  124]
train() client id: f_00002-7-0 loss: 1.181014  [   32/  124]
train() client id: f_00002-7-1 loss: 1.138328  [   64/  124]
train() client id: f_00002-7-2 loss: 1.145457  [   96/  124]
train() client id: f_00002-8-0 loss: 1.239440  [   32/  124]
train() client id: f_00002-8-1 loss: 1.267194  [   64/  124]
train() client id: f_00002-8-2 loss: 0.906073  [   96/  124]
train() client id: f_00002-9-0 loss: 0.975228  [   32/  124]
train() client id: f_00002-9-1 loss: 1.095701  [   64/  124]
train() client id: f_00002-9-2 loss: 1.095709  [   96/  124]
train() client id: f_00002-10-0 loss: 1.062930  [   32/  124]
train() client id: f_00002-10-1 loss: 1.269246  [   64/  124]
train() client id: f_00002-10-2 loss: 0.951966  [   96/  124]
train() client id: f_00002-11-0 loss: 1.004344  [   32/  124]
train() client id: f_00002-11-1 loss: 1.141986  [   64/  124]
train() client id: f_00002-11-2 loss: 1.283420  [   96/  124]
train() client id: f_00002-12-0 loss: 1.281275  [   32/  124]
train() client id: f_00002-12-1 loss: 0.937250  [   64/  124]
train() client id: f_00002-12-2 loss: 1.013828  [   96/  124]
train() client id: f_00002-13-0 loss: 1.333769  [   32/  124]
train() client id: f_00002-13-1 loss: 1.048048  [   64/  124]
train() client id: f_00002-13-2 loss: 1.021743  [   96/  124]
train() client id: f_00002-14-0 loss: 0.974229  [   32/  124]
train() client id: f_00002-14-1 loss: 1.178311  [   64/  124]
train() client id: f_00002-14-2 loss: 1.131470  [   96/  124]
train() client id: f_00003-0-0 loss: 0.712903  [   32/   43]
train() client id: f_00003-1-0 loss: 0.794456  [   32/   43]
train() client id: f_00003-2-0 loss: 0.837904  [   32/   43]
train() client id: f_00003-3-0 loss: 0.815837  [   32/   43]
train() client id: f_00003-4-0 loss: 0.978291  [   32/   43]
train() client id: f_00003-5-0 loss: 0.927673  [   32/   43]
train() client id: f_00003-6-0 loss: 0.812611  [   32/   43]
train() client id: f_00003-7-0 loss: 0.876341  [   32/   43]
train() client id: f_00003-8-0 loss: 0.855718  [   32/   43]
train() client id: f_00003-9-0 loss: 0.767502  [   32/   43]
train() client id: f_00003-10-0 loss: 0.800943  [   32/   43]
train() client id: f_00003-11-0 loss: 0.757278  [   32/   43]
train() client id: f_00003-12-0 loss: 0.929187  [   32/   43]
train() client id: f_00003-13-0 loss: 0.907317  [   32/   43]
train() client id: f_00003-14-0 loss: 0.961780  [   32/   43]
train() client id: f_00004-0-0 loss: 0.778081  [   32/  306]
train() client id: f_00004-0-1 loss: 0.674682  [   64/  306]
train() client id: f_00004-0-2 loss: 0.706561  [   96/  306]
train() client id: f_00004-0-3 loss: 0.922358  [  128/  306]
train() client id: f_00004-0-4 loss: 0.640947  [  160/  306]
train() client id: f_00004-0-5 loss: 0.609689  [  192/  306]
train() client id: f_00004-0-6 loss: 0.805180  [  224/  306]
train() client id: f_00004-0-7 loss: 0.854432  [  256/  306]
train() client id: f_00004-0-8 loss: 0.668217  [  288/  306]
train() client id: f_00004-1-0 loss: 0.762075  [   32/  306]
train() client id: f_00004-1-1 loss: 0.836788  [   64/  306]
train() client id: f_00004-1-2 loss: 0.644677  [   96/  306]
train() client id: f_00004-1-3 loss: 0.612930  [  128/  306]
train() client id: f_00004-1-4 loss: 0.870386  [  160/  306]
train() client id: f_00004-1-5 loss: 0.714834  [  192/  306]
train() client id: f_00004-1-6 loss: 0.581696  [  224/  306]
train() client id: f_00004-1-7 loss: 0.834109  [  256/  306]
train() client id: f_00004-1-8 loss: 0.824789  [  288/  306]
train() client id: f_00004-2-0 loss: 0.677560  [   32/  306]
train() client id: f_00004-2-1 loss: 0.822867  [   64/  306]
train() client id: f_00004-2-2 loss: 0.913363  [   96/  306]
train() client id: f_00004-2-3 loss: 0.625275  [  128/  306]
train() client id: f_00004-2-4 loss: 0.786575  [  160/  306]
train() client id: f_00004-2-5 loss: 0.647483  [  192/  306]
train() client id: f_00004-2-6 loss: 0.776588  [  224/  306]
train() client id: f_00004-2-7 loss: 0.817159  [  256/  306]
train() client id: f_00004-2-8 loss: 0.668202  [  288/  306]
train() client id: f_00004-3-0 loss: 0.845373  [   32/  306]
train() client id: f_00004-3-1 loss: 0.574075  [   64/  306]
train() client id: f_00004-3-2 loss: 0.801495  [   96/  306]
train() client id: f_00004-3-3 loss: 0.690883  [  128/  306]
train() client id: f_00004-3-4 loss: 0.683277  [  160/  306]
train() client id: f_00004-3-5 loss: 0.730507  [  192/  306]
train() client id: f_00004-3-6 loss: 0.872169  [  224/  306]
train() client id: f_00004-3-7 loss: 0.773464  [  256/  306]
train() client id: f_00004-3-8 loss: 0.764701  [  288/  306]
train() client id: f_00004-4-0 loss: 0.853594  [   32/  306]
train() client id: f_00004-4-1 loss: 0.714414  [   64/  306]
train() client id: f_00004-4-2 loss: 0.807110  [   96/  306]
train() client id: f_00004-4-3 loss: 0.767145  [  128/  306]
train() client id: f_00004-4-4 loss: 0.657681  [  160/  306]
train() client id: f_00004-4-5 loss: 0.879052  [  192/  306]
train() client id: f_00004-4-6 loss: 0.736918  [  224/  306]
train() client id: f_00004-4-7 loss: 0.695675  [  256/  306]
train() client id: f_00004-4-8 loss: 0.604295  [  288/  306]
train() client id: f_00004-5-0 loss: 0.862850  [   32/  306]
train() client id: f_00004-5-1 loss: 0.880342  [   64/  306]
train() client id: f_00004-5-2 loss: 0.616099  [   96/  306]
train() client id: f_00004-5-3 loss: 0.763471  [  128/  306]
train() client id: f_00004-5-4 loss: 0.711653  [  160/  306]
train() client id: f_00004-5-5 loss: 0.757406  [  192/  306]
train() client id: f_00004-5-6 loss: 0.780260  [  224/  306]
train() client id: f_00004-5-7 loss: 0.713895  [  256/  306]
train() client id: f_00004-5-8 loss: 0.728021  [  288/  306]
train() client id: f_00004-6-0 loss: 0.643360  [   32/  306]
train() client id: f_00004-6-1 loss: 0.798096  [   64/  306]
train() client id: f_00004-6-2 loss: 0.744587  [   96/  306]
train() client id: f_00004-6-3 loss: 0.739053  [  128/  306]
train() client id: f_00004-6-4 loss: 0.730208  [  160/  306]
train() client id: f_00004-6-5 loss: 0.917473  [  192/  306]
train() client id: f_00004-6-6 loss: 0.752093  [  224/  306]
train() client id: f_00004-6-7 loss: 0.758762  [  256/  306]
train() client id: f_00004-6-8 loss: 0.715978  [  288/  306]
train() client id: f_00004-7-0 loss: 0.914975  [   32/  306]
train() client id: f_00004-7-1 loss: 0.873482  [   64/  306]
train() client id: f_00004-7-2 loss: 0.633879  [   96/  306]
train() client id: f_00004-7-3 loss: 0.752383  [  128/  306]
train() client id: f_00004-7-4 loss: 0.787302  [  160/  306]
train() client id: f_00004-7-5 loss: 0.741698  [  192/  306]
train() client id: f_00004-7-6 loss: 0.869350  [  224/  306]
train() client id: f_00004-7-7 loss: 0.621671  [  256/  306]
train() client id: f_00004-7-8 loss: 0.664807  [  288/  306]
train() client id: f_00004-8-0 loss: 0.790308  [   32/  306]
train() client id: f_00004-8-1 loss: 0.741577  [   64/  306]
train() client id: f_00004-8-2 loss: 0.691477  [   96/  306]
train() client id: f_00004-8-3 loss: 0.998049  [  128/  306]
train() client id: f_00004-8-4 loss: 0.616363  [  160/  306]
train() client id: f_00004-8-5 loss: 0.747724  [  192/  306]
train() client id: f_00004-8-6 loss: 0.684456  [  224/  306]
train() client id: f_00004-8-7 loss: 0.881127  [  256/  306]
train() client id: f_00004-8-8 loss: 0.726178  [  288/  306]
train() client id: f_00004-9-0 loss: 0.838341  [   32/  306]
train() client id: f_00004-9-1 loss: 0.611428  [   64/  306]
train() client id: f_00004-9-2 loss: 0.810828  [   96/  306]
train() client id: f_00004-9-3 loss: 0.813312  [  128/  306]
train() client id: f_00004-9-4 loss: 0.852507  [  160/  306]
train() client id: f_00004-9-5 loss: 0.571961  [  192/  306]
train() client id: f_00004-9-6 loss: 0.747652  [  224/  306]
train() client id: f_00004-9-7 loss: 0.762968  [  256/  306]
train() client id: f_00004-9-8 loss: 0.812556  [  288/  306]
train() client id: f_00004-10-0 loss: 0.781548  [   32/  306]
train() client id: f_00004-10-1 loss: 0.825126  [   64/  306]
train() client id: f_00004-10-2 loss: 0.824803  [   96/  306]
train() client id: f_00004-10-3 loss: 0.732542  [  128/  306]
train() client id: f_00004-10-4 loss: 0.706427  [  160/  306]
train() client id: f_00004-10-5 loss: 0.745404  [  192/  306]
train() client id: f_00004-10-6 loss: 0.713974  [  224/  306]
train() client id: f_00004-10-7 loss: 0.696185  [  256/  306]
train() client id: f_00004-10-8 loss: 0.807510  [  288/  306]
train() client id: f_00004-11-0 loss: 0.733376  [   32/  306]
train() client id: f_00004-11-1 loss: 0.734153  [   64/  306]
train() client id: f_00004-11-2 loss: 0.856051  [   96/  306]
train() client id: f_00004-11-3 loss: 0.754464  [  128/  306]
train() client id: f_00004-11-4 loss: 0.859059  [  160/  306]
train() client id: f_00004-11-5 loss: 0.742960  [  192/  306]
train() client id: f_00004-11-6 loss: 0.717829  [  224/  306]
train() client id: f_00004-11-7 loss: 0.694528  [  256/  306]
train() client id: f_00004-11-8 loss: 0.732370  [  288/  306]
train() client id: f_00004-12-0 loss: 0.827354  [   32/  306]
train() client id: f_00004-12-1 loss: 0.742897  [   64/  306]
train() client id: f_00004-12-2 loss: 0.782257  [   96/  306]
train() client id: f_00004-12-3 loss: 0.659400  [  128/  306]
train() client id: f_00004-12-4 loss: 0.676512  [  160/  306]
train() client id: f_00004-12-5 loss: 0.765374  [  192/  306]
train() client id: f_00004-12-6 loss: 0.859267  [  224/  306]
train() client id: f_00004-12-7 loss: 0.755514  [  256/  306]
train() client id: f_00004-12-8 loss: 0.704976  [  288/  306]
train() client id: f_00004-13-0 loss: 0.790441  [   32/  306]
train() client id: f_00004-13-1 loss: 0.691272  [   64/  306]
train() client id: f_00004-13-2 loss: 0.718915  [   96/  306]
train() client id: f_00004-13-3 loss: 0.728458  [  128/  306]
train() client id: f_00004-13-4 loss: 0.848462  [  160/  306]
train() client id: f_00004-13-5 loss: 0.652352  [  192/  306]
train() client id: f_00004-13-6 loss: 0.933164  [  224/  306]
train() client id: f_00004-13-7 loss: 0.686419  [  256/  306]
train() client id: f_00004-13-8 loss: 0.772322  [  288/  306]
train() client id: f_00004-14-0 loss: 0.725240  [   32/  306]
train() client id: f_00004-14-1 loss: 0.704543  [   64/  306]
train() client id: f_00004-14-2 loss: 0.848563  [   96/  306]
train() client id: f_00004-14-3 loss: 0.744442  [  128/  306]
train() client id: f_00004-14-4 loss: 0.708111  [  160/  306]
train() client id: f_00004-14-5 loss: 0.779299  [  192/  306]
train() client id: f_00004-14-6 loss: 0.716399  [  224/  306]
train() client id: f_00004-14-7 loss: 0.677233  [  256/  306]
train() client id: f_00004-14-8 loss: 0.902423  [  288/  306]
train() client id: f_00005-0-0 loss: 0.718092  [   32/  146]
train() client id: f_00005-0-1 loss: 0.585406  [   64/  146]
train() client id: f_00005-0-2 loss: 0.317775  [   96/  146]
train() client id: f_00005-0-3 loss: 0.547023  [  128/  146]
train() client id: f_00005-1-0 loss: 0.590613  [   32/  146]
train() client id: f_00005-1-1 loss: 0.614347  [   64/  146]
train() client id: f_00005-1-2 loss: 0.472881  [   96/  146]
train() client id: f_00005-1-3 loss: 0.326997  [  128/  146]
train() client id: f_00005-2-0 loss: 0.601009  [   32/  146]
train() client id: f_00005-2-1 loss: 0.304141  [   64/  146]
train() client id: f_00005-2-2 loss: 0.689621  [   96/  146]
train() client id: f_00005-2-3 loss: 0.558639  [  128/  146]
train() client id: f_00005-3-0 loss: 0.716972  [   32/  146]
train() client id: f_00005-3-1 loss: 0.518461  [   64/  146]
train() client id: f_00005-3-2 loss: 0.403051  [   96/  146]
train() client id: f_00005-3-3 loss: 0.391333  [  128/  146]
train() client id: f_00005-4-0 loss: 0.676022  [   32/  146]
train() client id: f_00005-4-1 loss: 0.518160  [   64/  146]
train() client id: f_00005-4-2 loss: 0.409715  [   96/  146]
train() client id: f_00005-4-3 loss: 0.454805  [  128/  146]
train() client id: f_00005-5-0 loss: 0.668711  [   32/  146]
train() client id: f_00005-5-1 loss: 0.461478  [   64/  146]
train() client id: f_00005-5-2 loss: 0.269645  [   96/  146]
train() client id: f_00005-5-3 loss: 0.624661  [  128/  146]
train() client id: f_00005-6-0 loss: 0.544807  [   32/  146]
train() client id: f_00005-6-1 loss: 0.558922  [   64/  146]
train() client id: f_00005-6-2 loss: 0.397033  [   96/  146]
train() client id: f_00005-6-3 loss: 0.528373  [  128/  146]
train() client id: f_00005-7-0 loss: 0.352341  [   32/  146]
train() client id: f_00005-7-1 loss: 0.718422  [   64/  146]
train() client id: f_00005-7-2 loss: 0.629092  [   96/  146]
train() client id: f_00005-7-3 loss: 0.398736  [  128/  146]
train() client id: f_00005-8-0 loss: 0.442346  [   32/  146]
train() client id: f_00005-8-1 loss: 0.552637  [   64/  146]
train() client id: f_00005-8-2 loss: 0.510484  [   96/  146]
train() client id: f_00005-8-3 loss: 0.583473  [  128/  146]
train() client id: f_00005-9-0 loss: 0.675290  [   32/  146]
train() client id: f_00005-9-1 loss: 0.385164  [   64/  146]
train() client id: f_00005-9-2 loss: 0.537817  [   96/  146]
train() client id: f_00005-9-3 loss: 0.538301  [  128/  146]
train() client id: f_00005-10-0 loss: 0.418427  [   32/  146]
train() client id: f_00005-10-1 loss: 0.546575  [   64/  146]
train() client id: f_00005-10-2 loss: 0.490789  [   96/  146]
train() client id: f_00005-10-3 loss: 0.478746  [  128/  146]
train() client id: f_00005-11-0 loss: 0.420680  [   32/  146]
train() client id: f_00005-11-1 loss: 0.628006  [   64/  146]
train() client id: f_00005-11-2 loss: 0.446064  [   96/  146]
train() client id: f_00005-11-3 loss: 0.476910  [  128/  146]
train() client id: f_00005-12-0 loss: 0.354579  [   32/  146]
train() client id: f_00005-12-1 loss: 0.710056  [   64/  146]
train() client id: f_00005-12-2 loss: 0.485379  [   96/  146]
train() client id: f_00005-12-3 loss: 0.388163  [  128/  146]
train() client id: f_00005-13-0 loss: 0.585636  [   32/  146]
train() client id: f_00005-13-1 loss: 0.462954  [   64/  146]
train() client id: f_00005-13-2 loss: 0.449125  [   96/  146]
train() client id: f_00005-13-3 loss: 0.307914  [  128/  146]
train() client id: f_00005-14-0 loss: 0.605131  [   32/  146]
train() client id: f_00005-14-1 loss: 0.725252  [   64/  146]
train() client id: f_00005-14-2 loss: 0.477646  [   96/  146]
train() client id: f_00005-14-3 loss: 0.319046  [  128/  146]
train() client id: f_00006-0-0 loss: 0.516373  [   32/   54]
train() client id: f_00006-1-0 loss: 0.454761  [   32/   54]
train() client id: f_00006-2-0 loss: 0.470739  [   32/   54]
train() client id: f_00006-3-0 loss: 0.456519  [   32/   54]
train() client id: f_00006-4-0 loss: 0.449394  [   32/   54]
train() client id: f_00006-5-0 loss: 0.463003  [   32/   54]
train() client id: f_00006-6-0 loss: 0.454406  [   32/   54]
train() client id: f_00006-7-0 loss: 0.466911  [   32/   54]
train() client id: f_00006-8-0 loss: 0.467088  [   32/   54]
train() client id: f_00006-9-0 loss: 0.447883  [   32/   54]
train() client id: f_00006-10-0 loss: 0.514992  [   32/   54]
train() client id: f_00006-11-0 loss: 0.405129  [   32/   54]
train() client id: f_00006-12-0 loss: 0.524227  [   32/   54]
train() client id: f_00006-13-0 loss: 0.464055  [   32/   54]
train() client id: f_00006-14-0 loss: 0.406052  [   32/   54]
train() client id: f_00007-0-0 loss: 0.476383  [   32/  179]
train() client id: f_00007-0-1 loss: 0.496833  [   64/  179]
train() client id: f_00007-0-2 loss: 0.638417  [   96/  179]
train() client id: f_00007-0-3 loss: 0.591186  [  128/  179]
train() client id: f_00007-0-4 loss: 0.557973  [  160/  179]
train() client id: f_00007-1-0 loss: 0.436174  [   32/  179]
train() client id: f_00007-1-1 loss: 0.321901  [   64/  179]
train() client id: f_00007-1-2 loss: 0.636709  [   96/  179]
train() client id: f_00007-1-3 loss: 0.658846  [  128/  179]
train() client id: f_00007-1-4 loss: 0.484991  [  160/  179]
train() client id: f_00007-2-0 loss: 0.413878  [   32/  179]
train() client id: f_00007-2-1 loss: 0.617534  [   64/  179]
train() client id: f_00007-2-2 loss: 0.342143  [   96/  179]
train() client id: f_00007-2-3 loss: 0.621717  [  128/  179]
train() client id: f_00007-2-4 loss: 0.638602  [  160/  179]
train() client id: f_00007-3-0 loss: 0.351168  [   32/  179]
train() client id: f_00007-3-1 loss: 0.513515  [   64/  179]
train() client id: f_00007-3-2 loss: 0.688126  [   96/  179]
train() client id: f_00007-3-3 loss: 0.360113  [  128/  179]
train() client id: f_00007-3-4 loss: 0.550571  [  160/  179]
train() client id: f_00007-4-0 loss: 0.613038  [   32/  179]
train() client id: f_00007-4-1 loss: 0.365192  [   64/  179]
train() client id: f_00007-4-2 loss: 0.579404  [   96/  179]
train() client id: f_00007-4-3 loss: 0.400801  [  128/  179]
train() client id: f_00007-4-4 loss: 0.342896  [  160/  179]
train() client id: f_00007-5-0 loss: 0.475500  [   32/  179]
train() client id: f_00007-5-1 loss: 0.340287  [   64/  179]
train() client id: f_00007-5-2 loss: 0.676531  [   96/  179]
train() client id: f_00007-5-3 loss: 0.580034  [  128/  179]
train() client id: f_00007-5-4 loss: 0.524775  [  160/  179]
train() client id: f_00007-6-0 loss: 0.536586  [   32/  179]
train() client id: f_00007-6-1 loss: 0.548034  [   64/  179]
train() client id: f_00007-6-2 loss: 0.503833  [   96/  179]
train() client id: f_00007-6-3 loss: 0.428595  [  128/  179]
train() client id: f_00007-6-4 loss: 0.559900  [  160/  179]
train() client id: f_00007-7-0 loss: 0.476019  [   32/  179]
train() client id: f_00007-7-1 loss: 0.379784  [   64/  179]
train() client id: f_00007-7-2 loss: 0.667559  [   96/  179]
train() client id: f_00007-7-3 loss: 0.505841  [  128/  179]
train() client id: f_00007-7-4 loss: 0.445133  [  160/  179]
train() client id: f_00007-8-0 loss: 0.373584  [   32/  179]
train() client id: f_00007-8-1 loss: 0.372338  [   64/  179]
train() client id: f_00007-8-2 loss: 0.561568  [   96/  179]
train() client id: f_00007-8-3 loss: 0.728172  [  128/  179]
train() client id: f_00007-8-4 loss: 0.489588  [  160/  179]
train() client id: f_00007-9-0 loss: 0.438158  [   32/  179]
train() client id: f_00007-9-1 loss: 0.453499  [   64/  179]
train() client id: f_00007-9-2 loss: 0.451954  [   96/  179]
train() client id: f_00007-9-3 loss: 0.566496  [  128/  179]
train() client id: f_00007-9-4 loss: 0.545536  [  160/  179]
train() client id: f_00007-10-0 loss: 0.507526  [   32/  179]
train() client id: f_00007-10-1 loss: 0.506589  [   64/  179]
train() client id: f_00007-10-2 loss: 0.457000  [   96/  179]
train() client id: f_00007-10-3 loss: 0.618687  [  128/  179]
train() client id: f_00007-10-4 loss: 0.420777  [  160/  179]
train() client id: f_00007-11-0 loss: 0.508278  [   32/  179]
train() client id: f_00007-11-1 loss: 0.677757  [   64/  179]
train() client id: f_00007-11-2 loss: 0.427970  [   96/  179]
train() client id: f_00007-11-3 loss: 0.440605  [  128/  179]
train() client id: f_00007-11-4 loss: 0.468642  [  160/  179]
train() client id: f_00007-12-0 loss: 0.311879  [   32/  179]
train() client id: f_00007-12-1 loss: 0.354247  [   64/  179]
train() client id: f_00007-12-2 loss: 0.776617  [   96/  179]
train() client id: f_00007-12-3 loss: 0.435159  [  128/  179]
train() client id: f_00007-12-4 loss: 0.464983  [  160/  179]
train() client id: f_00007-13-0 loss: 0.462251  [   32/  179]
train() client id: f_00007-13-1 loss: 0.529178  [   64/  179]
train() client id: f_00007-13-2 loss: 0.517109  [   96/  179]
train() client id: f_00007-13-3 loss: 0.505133  [  128/  179]
train() client id: f_00007-13-4 loss: 0.528075  [  160/  179]
train() client id: f_00007-14-0 loss: 0.620062  [   32/  179]
train() client id: f_00007-14-1 loss: 0.387508  [   64/  179]
train() client id: f_00007-14-2 loss: 0.370257  [   96/  179]
train() client id: f_00007-14-3 loss: 0.446433  [  128/  179]
train() client id: f_00007-14-4 loss: 0.704580  [  160/  179]
train() client id: f_00008-0-0 loss: 0.745305  [   32/  130]
train() client id: f_00008-0-1 loss: 0.725490  [   64/  130]
train() client id: f_00008-0-2 loss: 0.705662  [   96/  130]
train() client id: f_00008-0-3 loss: 0.832959  [  128/  130]
train() client id: f_00008-1-0 loss: 0.840044  [   32/  130]
train() client id: f_00008-1-1 loss: 0.679578  [   64/  130]
train() client id: f_00008-1-2 loss: 0.816186  [   96/  130]
train() client id: f_00008-1-3 loss: 0.673523  [  128/  130]
train() client id: f_00008-2-0 loss: 0.706881  [   32/  130]
train() client id: f_00008-2-1 loss: 0.769213  [   64/  130]
train() client id: f_00008-2-2 loss: 0.784046  [   96/  130]
train() client id: f_00008-2-3 loss: 0.753142  [  128/  130]
train() client id: f_00008-3-0 loss: 0.685389  [   32/  130]
train() client id: f_00008-3-1 loss: 0.740329  [   64/  130]
train() client id: f_00008-3-2 loss: 0.719686  [   96/  130]
train() client id: f_00008-3-3 loss: 0.861992  [  128/  130]
train() client id: f_00008-4-0 loss: 0.729529  [   32/  130]
train() client id: f_00008-4-1 loss: 0.817894  [   64/  130]
train() client id: f_00008-4-2 loss: 0.756377  [   96/  130]
train() client id: f_00008-4-3 loss: 0.686288  [  128/  130]
train() client id: f_00008-5-0 loss: 0.705361  [   32/  130]
train() client id: f_00008-5-1 loss: 0.821599  [   64/  130]
train() client id: f_00008-5-2 loss: 0.669865  [   96/  130]
train() client id: f_00008-5-3 loss: 0.815212  [  128/  130]
train() client id: f_00008-6-0 loss: 0.725140  [   32/  130]
train() client id: f_00008-6-1 loss: 0.670791  [   64/  130]
train() client id: f_00008-6-2 loss: 0.817390  [   96/  130]
train() client id: f_00008-6-3 loss: 0.755473  [  128/  130]
train() client id: f_00008-7-0 loss: 0.767542  [   32/  130]
train() client id: f_00008-7-1 loss: 0.709359  [   64/  130]
train() client id: f_00008-7-2 loss: 0.698706  [   96/  130]
train() client id: f_00008-7-3 loss: 0.810385  [  128/  130]
train() client id: f_00008-8-0 loss: 0.826785  [   32/  130]
train() client id: f_00008-8-1 loss: 0.815684  [   64/  130]
train() client id: f_00008-8-2 loss: 0.593270  [   96/  130]
train() client id: f_00008-8-3 loss: 0.758010  [  128/  130]
train() client id: f_00008-9-0 loss: 0.715542  [   32/  130]
train() client id: f_00008-9-1 loss: 0.739621  [   64/  130]
train() client id: f_00008-9-2 loss: 0.833100  [   96/  130]
train() client id: f_00008-9-3 loss: 0.696884  [  128/  130]
train() client id: f_00008-10-0 loss: 0.778036  [   32/  130]
train() client id: f_00008-10-1 loss: 0.804945  [   64/  130]
train() client id: f_00008-10-2 loss: 0.712899  [   96/  130]
train() client id: f_00008-10-3 loss: 0.712143  [  128/  130]
train() client id: f_00008-11-0 loss: 0.761829  [   32/  130]
train() client id: f_00008-11-1 loss: 0.748827  [   64/  130]
train() client id: f_00008-11-2 loss: 0.666565  [   96/  130]
train() client id: f_00008-11-3 loss: 0.830011  [  128/  130]
train() client id: f_00008-12-0 loss: 0.797781  [   32/  130]
train() client id: f_00008-12-1 loss: 0.737465  [   64/  130]
train() client id: f_00008-12-2 loss: 0.792980  [   96/  130]
train() client id: f_00008-12-3 loss: 0.675584  [  128/  130]
train() client id: f_00008-13-0 loss: 0.879205  [   32/  130]
train() client id: f_00008-13-1 loss: 0.654795  [   64/  130]
train() client id: f_00008-13-2 loss: 0.768916  [   96/  130]
train() client id: f_00008-13-3 loss: 0.703065  [  128/  130]
train() client id: f_00008-14-0 loss: 0.698692  [   32/  130]
train() client id: f_00008-14-1 loss: 0.834904  [   64/  130]
train() client id: f_00008-14-2 loss: 0.696427  [   96/  130]
train() client id: f_00008-14-3 loss: 0.724979  [  128/  130]
train() client id: f_00009-0-0 loss: 1.284055  [   32/  118]
train() client id: f_00009-0-1 loss: 1.341220  [   64/  118]
train() client id: f_00009-0-2 loss: 1.168781  [   96/  118]
train() client id: f_00009-1-0 loss: 1.275587  [   32/  118]
train() client id: f_00009-1-1 loss: 1.150876  [   64/  118]
train() client id: f_00009-1-2 loss: 1.157859  [   96/  118]
train() client id: f_00009-2-0 loss: 1.175866  [   32/  118]
train() client id: f_00009-2-1 loss: 1.152994  [   64/  118]
train() client id: f_00009-2-2 loss: 1.090936  [   96/  118]
train() client id: f_00009-3-0 loss: 1.297902  [   32/  118]
train() client id: f_00009-3-1 loss: 1.123950  [   64/  118]
train() client id: f_00009-3-2 loss: 1.038492  [   96/  118]
train() client id: f_00009-4-0 loss: 1.091787  [   32/  118]
train() client id: f_00009-4-1 loss: 1.139679  [   64/  118]
train() client id: f_00009-4-2 loss: 1.035522  [   96/  118]
train() client id: f_00009-5-0 loss: 1.101083  [   32/  118]
train() client id: f_00009-5-1 loss: 1.007539  [   64/  118]
train() client id: f_00009-5-2 loss: 1.036094  [   96/  118]
train() client id: f_00009-6-0 loss: 0.977793  [   32/  118]
train() client id: f_00009-6-1 loss: 1.100721  [   64/  118]
train() client id: f_00009-6-2 loss: 0.988442  [   96/  118]
train() client id: f_00009-7-0 loss: 1.025852  [   32/  118]
train() client id: f_00009-7-1 loss: 0.846354  [   64/  118]
train() client id: f_00009-7-2 loss: 1.088966  [   96/  118]
train() client id: f_00009-8-0 loss: 0.989990  [   32/  118]
train() client id: f_00009-8-1 loss: 0.905335  [   64/  118]
train() client id: f_00009-8-2 loss: 0.918168  [   96/  118]
train() client id: f_00009-9-0 loss: 0.962334  [   32/  118]
train() client id: f_00009-9-1 loss: 1.001206  [   64/  118]
train() client id: f_00009-9-2 loss: 0.970445  [   96/  118]
train() client id: f_00009-10-0 loss: 0.931272  [   32/  118]
train() client id: f_00009-10-1 loss: 0.934344  [   64/  118]
train() client id: f_00009-10-2 loss: 1.036391  [   96/  118]
train() client id: f_00009-11-0 loss: 0.996223  [   32/  118]
train() client id: f_00009-11-1 loss: 0.833388  [   64/  118]
train() client id: f_00009-11-2 loss: 0.849665  [   96/  118]
train() client id: f_00009-12-0 loss: 0.918203  [   32/  118]
train() client id: f_00009-12-1 loss: 0.894227  [   64/  118]
train() client id: f_00009-12-2 loss: 0.917247  [   96/  118]
train() client id: f_00009-13-0 loss: 0.822608  [   32/  118]
train() client id: f_00009-13-1 loss: 0.897977  [   64/  118]
train() client id: f_00009-13-2 loss: 0.919614  [   96/  118]
train() client id: f_00009-14-0 loss: 0.697957  [   32/  118]
train() client id: f_00009-14-1 loss: 0.938213  [   64/  118]
train() client id: f_00009-14-2 loss: 1.009153  [   96/  118]
At round 37 accuracy: 0.6445623342175066
At round 37 training accuracy: 0.5855130784708249
At round 37 training loss: 0.8354199053247003
update_location
xs = 8.927491 306.223621 5.882650 0.934260 -222.581990 -70.230757 -30.849135 -5.143845 -245.120581 20.134486 
ys = -297.390647 7.291448 195.684448 -17.290817 -9.642386 0.794442 -1.381692 191.628436 25.881276 -732.232496 
xs mean: -23.182379970521218
ys mean: -63.66579882624052
dists_uav = 313.880387 322.220532 219.834048 101.488153 244.204254 122.200616 104.659344 216.212665 265.996127 739.303608 
uav_gains = -116.834434 -117.442845 -108.978810 -100.160403 -110.811671 -102.177111 -100.494481 -108.732395 -112.706886 -128.680278 
uav_gains_db_mean: -110.70193149946692
dists_bs = 506.934163 509.609980 182.061472 260.631200 190.674740 203.283473 227.793708 170.668153 164.782553 927.980748 
bs_gains = -115.305703 -115.369721 -102.853165 -107.215798 -103.415268 -104.193917 -105.578227 -102.067329 -101.640573 -122.658150 
bs_gains_db_mean: -108.02978516025375
Round 38
-------------------------------
ene_coms = [0.01911668 0.02041749 0.00714746 0.00634466 0.00734087 0.00692769
 0.00643445 0.00689325 0.00676251 0.04064455]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.11420536 12.55610456  5.83844577  2.08384734 14.250034    6.85150632
  2.59343648  8.37561958  6.10975617  6.07438204]
obj_prev = 70.84733761740952
eta_min = 3.937073061038546e-16	eta_max = 0.77773486075296
af = 14.80883058640876	bf = 1.9416791353572658	zeta = 16.289713645049638	eta = 0.9090909090909091
af = 14.80883058640876	bf = 1.9416791353572658	zeta = 36.88422585363637	eta = 0.4014949546500723
af = 14.80883058640876	bf = 1.9416791353572658	zeta = 25.82372582257542	eta = 0.5734583261979453
af = 14.80883058640876	bf = 1.9416791353572658	zeta = 23.857987025436678	eta = 0.6207074624787089
af = 14.80883058640876	bf = 1.9416791353572658	zeta = 23.738779907950644	eta = 0.623824419107949
af = 14.80883058640876	bf = 1.9416791353572658	zeta = 23.738286862892913	eta = 0.6238373759631975
af = 14.80883058640876	bf = 1.9416791353572658	zeta = 23.738286854401544	eta = 0.623837376186349
eta = 0.623837376186349
ene_coms = [0.01911668 0.02041749 0.00714746 0.00634466 0.00734087 0.00692769
 0.00643445 0.00689325 0.00676251 0.04064455]
ene_comp = [0.03893721 0.08189176 0.03831916 0.0132881  0.09456181 0.04511773
 0.01668738 0.05531557 0.04017332 0.03646501]
ene_total = [2.3405762  4.12483253 1.83309136 0.79153957 4.10844083 2.09833043
 0.93220955 2.50809134 1.89232579 3.10884925]
ti_comp = [0.50492254 0.49191446 0.62461476 0.63264278 0.6226806  0.62681247
 0.63174486 0.62715687 0.62846423 0.28964382]
ti_coms = [0.1911668  0.20417488 0.07147458 0.06344656 0.07340875 0.06927688
 0.06434448 0.06893248 0.06762511 0.40644552]
t_total = [28.06444168 28.06444168 28.06444168 28.06444168 28.06444168 28.06444168
 28.06444168 28.06444168 28.06444168 28.06444168]
ene_coms = [0.01911668 0.02041749 0.00714746 0.00634466 0.00734087 0.00692769
 0.00643445 0.00689325 0.00676251 0.04064455]
ene_comp = [1.44718865e-05 1.41847431e-04 9.01371045e-06 3.66396474e-07
 1.36300249e-04 1.46098700e-05 7.27714244e-07 2.68948895e-05
 1.02596363e-05 3.61227262e-05]
ene_total = [0.77131636 0.82889686 0.28852961 0.25581416 0.30145949 0.27989468
 0.2594489  0.27900145 0.27305983 1.6401349 ]
optimize_network iter = 0 obj = 5.1775562410463305
eta = 0.623837376186349
freqs = [38557611.90759263 83237806.18779309 30674238.12497328 10502053.85218277
 75931230.24941418 35989811.61756157 13207371.18814979 44100264.3426425
 31961498.40070312 62948021.05267238]
eta_min = 0.6238373761863508	eta_max = 0.6238373761863478
af = 0.012554434406433638	bf = 1.9416791353572658	zeta = 0.013809877847077004	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01911668 0.02041749 0.00714746 0.00634466 0.00734087 0.00692769
 0.00643445 0.00689325 0.00676251 0.04064455]
ene_comp = [2.92311672e-06 2.86511779e-05 1.82064223e-06 7.40069139e-08
 2.75307255e-05 2.95098743e-06 1.46988001e-07 5.43238790e-06
 2.07230165e-06 7.29628059e-06]
ene_total = [3.18961762 3.41091629 1.19267463 1.05845621 1.2292303  1.1562001
 1.07344788 1.15086863 1.12849806 6.78172298]
ti_comp = [0.50492254 0.49191446 0.62461476 0.63264278 0.6226806  0.62681247
 0.63174486 0.62715687 0.62846423 0.28964382]
ti_coms = [0.1911668  0.20417488 0.07147458 0.06344656 0.07340875 0.06927688
 0.06434448 0.06893248 0.06762511 0.40644552]
t_total = [28.06444168 28.06444168 28.06444168 28.06444168 28.06444168 28.06444168
 28.06444168 28.06444168 28.06444168 28.06444168]
ene_coms = [0.01911668 0.02041749 0.00714746 0.00634466 0.00734087 0.00692769
 0.00643445 0.00689325 0.00676251 0.04064455]
ene_comp = [1.44718865e-05 1.41847431e-04 9.01371045e-06 3.66396474e-07
 1.36300249e-04 1.46098700e-05 7.27714244e-07 2.68948895e-05
 1.02596363e-05 3.61227262e-05]
ene_total = [0.77131636 0.82889686 0.28852961 0.25581416 0.30145949 0.27989468
 0.2594489  0.27900145 0.27305983 1.6401349 ]
optimize_network iter = 1 obj = 5.1775562410463145
eta = 0.6238373761863478
freqs = [38557611.90759262 83237806.18779308 30674238.1249733  10502053.85218278
 75931230.24941424 35989811.6175616  13207371.18814981 44100264.34264254
 31961498.40070315 62948021.05267217]
Done!
ene_coms = [0.01911668 0.02041749 0.00714746 0.00634466 0.00734087 0.00692769
 0.00643445 0.00689325 0.00676251 0.04064455]
ene_comp = [1.40492152e-05 1.37704581e-04 8.75045263e-06 3.55695361e-07
 1.32319412e-04 1.41831686e-05 7.06460347e-07 2.61093873e-05
 9.95998946e-06 3.50677122e-05]
ene_total = [0.01913073 0.02055519 0.00715621 0.00634501 0.00747319 0.00694187
 0.00643515 0.00691936 0.00677247 0.04067962]
At round 38 energy consumption: 0.1284088100264597
At round 38 eta: 0.6238373761863478
At round 38 a_n: 15.165860671420319
At round 38 local rounds: 15.451275787964557
At round 38 global rounds: 40.317298187853346
gradient difference: 0.4625820517539978
train() client id: f_00000-0-0 loss: 1.273960  [   32/  126]
train() client id: f_00000-0-1 loss: 1.808590  [   64/  126]
train() client id: f_00000-0-2 loss: 1.343820  [   96/  126]
train() client id: f_00000-1-0 loss: 1.354923  [   32/  126]
train() client id: f_00000-1-1 loss: 1.239819  [   64/  126]
train() client id: f_00000-1-2 loss: 1.345586  [   96/  126]
train() client id: f_00000-2-0 loss: 1.074973  [   32/  126]
train() client id: f_00000-2-1 loss: 1.453161  [   64/  126]
train() client id: f_00000-2-2 loss: 1.058975  [   96/  126]
train() client id: f_00000-3-0 loss: 1.171980  [   32/  126]
train() client id: f_00000-3-1 loss: 1.056857  [   64/  126]
train() client id: f_00000-3-2 loss: 1.054444  [   96/  126]
train() client id: f_00000-4-0 loss: 0.957203  [   32/  126]
train() client id: f_00000-4-1 loss: 0.975406  [   64/  126]
train() client id: f_00000-4-2 loss: 1.129701  [   96/  126]
train() client id: f_00000-5-0 loss: 1.042587  [   32/  126]
train() client id: f_00000-5-1 loss: 0.925183  [   64/  126]
train() client id: f_00000-5-2 loss: 0.983734  [   96/  126]
train() client id: f_00000-6-0 loss: 0.913481  [   32/  126]
train() client id: f_00000-6-1 loss: 0.818532  [   64/  126]
train() client id: f_00000-6-2 loss: 0.824844  [   96/  126]
train() client id: f_00000-7-0 loss: 0.837513  [   32/  126]
train() client id: f_00000-7-1 loss: 0.764499  [   64/  126]
train() client id: f_00000-7-2 loss: 0.903329  [   96/  126]
train() client id: f_00000-8-0 loss: 0.841659  [   32/  126]
train() client id: f_00000-8-1 loss: 0.773285  [   64/  126]
train() client id: f_00000-8-2 loss: 0.845621  [   96/  126]
train() client id: f_00000-9-0 loss: 0.762315  [   32/  126]
train() client id: f_00000-9-1 loss: 0.836352  [   64/  126]
train() client id: f_00000-9-2 loss: 0.828021  [   96/  126]
train() client id: f_00000-10-0 loss: 0.745068  [   32/  126]
train() client id: f_00000-10-1 loss: 0.821175  [   64/  126]
train() client id: f_00000-10-2 loss: 0.674855  [   96/  126]
train() client id: f_00000-11-0 loss: 0.637340  [   32/  126]
train() client id: f_00000-11-1 loss: 0.693486  [   64/  126]
train() client id: f_00000-11-2 loss: 0.853240  [   96/  126]
train() client id: f_00000-12-0 loss: 0.703905  [   32/  126]
train() client id: f_00000-12-1 loss: 0.777291  [   64/  126]
train() client id: f_00000-12-2 loss: 0.777512  [   96/  126]
train() client id: f_00000-13-0 loss: 0.790888  [   32/  126]
train() client id: f_00000-13-1 loss: 0.654350  [   64/  126]
train() client id: f_00000-13-2 loss: 0.765647  [   96/  126]
train() client id: f_00000-14-0 loss: 0.694148  [   32/  126]
train() client id: f_00000-14-1 loss: 0.684180  [   64/  126]
train() client id: f_00000-14-2 loss: 0.704166  [   96/  126]
train() client id: f_00001-0-0 loss: 0.425857  [   32/  265]
train() client id: f_00001-0-1 loss: 0.507082  [   64/  265]
train() client id: f_00001-0-2 loss: 0.469195  [   96/  265]
train() client id: f_00001-0-3 loss: 0.571687  [  128/  265]
train() client id: f_00001-0-4 loss: 0.419372  [  160/  265]
train() client id: f_00001-0-5 loss: 0.416084  [  192/  265]
train() client id: f_00001-0-6 loss: 0.521209  [  224/  265]
train() client id: f_00001-0-7 loss: 0.525538  [  256/  265]
train() client id: f_00001-1-0 loss: 0.388399  [   32/  265]
train() client id: f_00001-1-1 loss: 0.653262  [   64/  265]
train() client id: f_00001-1-2 loss: 0.567419  [   96/  265]
train() client id: f_00001-1-3 loss: 0.446093  [  128/  265]
train() client id: f_00001-1-4 loss: 0.447159  [  160/  265]
train() client id: f_00001-1-5 loss: 0.519287  [  192/  265]
train() client id: f_00001-1-6 loss: 0.424728  [  224/  265]
train() client id: f_00001-1-7 loss: 0.507010  [  256/  265]
train() client id: f_00001-2-0 loss: 0.476631  [   32/  265]
train() client id: f_00001-2-1 loss: 0.494097  [   64/  265]
train() client id: f_00001-2-2 loss: 0.399689  [   96/  265]
train() client id: f_00001-2-3 loss: 0.557188  [  128/  265]
train() client id: f_00001-2-4 loss: 0.475607  [  160/  265]
train() client id: f_00001-2-5 loss: 0.475503  [  192/  265]
train() client id: f_00001-2-6 loss: 0.465330  [  224/  265]
train() client id: f_00001-2-7 loss: 0.520768  [  256/  265]
train() client id: f_00001-3-0 loss: 0.392655  [   32/  265]
train() client id: f_00001-3-1 loss: 0.516011  [   64/  265]
train() client id: f_00001-3-2 loss: 0.473177  [   96/  265]
train() client id: f_00001-3-3 loss: 0.440247  [  128/  265]
train() client id: f_00001-3-4 loss: 0.604185  [  160/  265]
train() client id: f_00001-3-5 loss: 0.465554  [  192/  265]
train() client id: f_00001-3-6 loss: 0.542425  [  224/  265]
train() client id: f_00001-3-7 loss: 0.402479  [  256/  265]
train() client id: f_00001-4-0 loss: 0.380784  [   32/  265]
train() client id: f_00001-4-1 loss: 0.468872  [   64/  265]
train() client id: f_00001-4-2 loss: 0.418059  [   96/  265]
train() client id: f_00001-4-3 loss: 0.487499  [  128/  265]
train() client id: f_00001-4-4 loss: 0.458060  [  160/  265]
train() client id: f_00001-4-5 loss: 0.535249  [  192/  265]
train() client id: f_00001-4-6 loss: 0.485557  [  224/  265]
train() client id: f_00001-4-7 loss: 0.636688  [  256/  265]
train() client id: f_00001-5-0 loss: 0.418023  [   32/  265]
train() client id: f_00001-5-1 loss: 0.445089  [   64/  265]
train() client id: f_00001-5-2 loss: 0.549963  [   96/  265]
train() client id: f_00001-5-3 loss: 0.439233  [  128/  265]
train() client id: f_00001-5-4 loss: 0.611618  [  160/  265]
train() client id: f_00001-5-5 loss: 0.537352  [  192/  265]
train() client id: f_00001-5-6 loss: 0.380621  [  224/  265]
train() client id: f_00001-5-7 loss: 0.400757  [  256/  265]
train() client id: f_00001-6-0 loss: 0.449879  [   32/  265]
train() client id: f_00001-6-1 loss: 0.395171  [   64/  265]
train() client id: f_00001-6-2 loss: 0.502051  [   96/  265]
train() client id: f_00001-6-3 loss: 0.536474  [  128/  265]
train() client id: f_00001-6-4 loss: 0.455038  [  160/  265]
train() client id: f_00001-6-5 loss: 0.391429  [  192/  265]
train() client id: f_00001-6-6 loss: 0.599771  [  224/  265]
train() client id: f_00001-6-7 loss: 0.507821  [  256/  265]
train() client id: f_00001-7-0 loss: 0.503760  [   32/  265]
train() client id: f_00001-7-1 loss: 0.473197  [   64/  265]
train() client id: f_00001-7-2 loss: 0.477370  [   96/  265]
train() client id: f_00001-7-3 loss: 0.502643  [  128/  265]
train() client id: f_00001-7-4 loss: 0.381985  [  160/  265]
train() client id: f_00001-7-5 loss: 0.658615  [  192/  265]
train() client id: f_00001-7-6 loss: 0.369762  [  224/  265]
train() client id: f_00001-7-7 loss: 0.382632  [  256/  265]
train() client id: f_00001-8-0 loss: 0.512694  [   32/  265]
train() client id: f_00001-8-1 loss: 0.386595  [   64/  265]
train() client id: f_00001-8-2 loss: 0.421223  [   96/  265]
train() client id: f_00001-8-3 loss: 0.501487  [  128/  265]
train() client id: f_00001-8-4 loss: 0.418535  [  160/  265]
train() client id: f_00001-8-5 loss: 0.449425  [  192/  265]
train() client id: f_00001-8-6 loss: 0.430294  [  224/  265]
train() client id: f_00001-8-7 loss: 0.546245  [  256/  265]
train() client id: f_00001-9-0 loss: 0.487742  [   32/  265]
train() client id: f_00001-9-1 loss: 0.402564  [   64/  265]
train() client id: f_00001-9-2 loss: 0.548880  [   96/  265]
train() client id: f_00001-9-3 loss: 0.550867  [  128/  265]
train() client id: f_00001-9-4 loss: 0.441361  [  160/  265]
train() client id: f_00001-9-5 loss: 0.439472  [  192/  265]
train() client id: f_00001-9-6 loss: 0.414990  [  224/  265]
train() client id: f_00001-9-7 loss: 0.567470  [  256/  265]
train() client id: f_00001-10-0 loss: 0.418525  [   32/  265]
train() client id: f_00001-10-1 loss: 0.486928  [   64/  265]
train() client id: f_00001-10-2 loss: 0.502425  [   96/  265]
train() client id: f_00001-10-3 loss: 0.448989  [  128/  265]
train() client id: f_00001-10-4 loss: 0.569339  [  160/  265]
train() client id: f_00001-10-5 loss: 0.461926  [  192/  265]
train() client id: f_00001-10-6 loss: 0.440264  [  224/  265]
train() client id: f_00001-10-7 loss: 0.509213  [  256/  265]
train() client id: f_00001-11-0 loss: 0.544964  [   32/  265]
train() client id: f_00001-11-1 loss: 0.547216  [   64/  265]
train() client id: f_00001-11-2 loss: 0.486048  [   96/  265]
train() client id: f_00001-11-3 loss: 0.628959  [  128/  265]
train() client id: f_00001-11-4 loss: 0.371838  [  160/  265]
train() client id: f_00001-11-5 loss: 0.428643  [  192/  265]
train() client id: f_00001-11-6 loss: 0.426131  [  224/  265]
train() client id: f_00001-11-7 loss: 0.374921  [  256/  265]
train() client id: f_00001-12-0 loss: 0.535226  [   32/  265]
train() client id: f_00001-12-1 loss: 0.541270  [   64/  265]
train() client id: f_00001-12-2 loss: 0.362737  [   96/  265]
train() client id: f_00001-12-3 loss: 0.499201  [  128/  265]
train() client id: f_00001-12-4 loss: 0.449261  [  160/  265]
train() client id: f_00001-12-5 loss: 0.507299  [  192/  265]
train() client id: f_00001-12-6 loss: 0.482515  [  224/  265]
train() client id: f_00001-12-7 loss: 0.471722  [  256/  265]
train() client id: f_00001-13-0 loss: 0.499389  [   32/  265]
train() client id: f_00001-13-1 loss: 0.412816  [   64/  265]
train() client id: f_00001-13-2 loss: 0.374921  [   96/  265]
train() client id: f_00001-13-3 loss: 0.420807  [  128/  265]
train() client id: f_00001-13-4 loss: 0.531893  [  160/  265]
train() client id: f_00001-13-5 loss: 0.473814  [  192/  265]
train() client id: f_00001-13-6 loss: 0.603593  [  224/  265]
train() client id: f_00001-13-7 loss: 0.549230  [  256/  265]
train() client id: f_00001-14-0 loss: 0.600359  [   32/  265]
train() client id: f_00001-14-1 loss: 0.496630  [   64/  265]
train() client id: f_00001-14-2 loss: 0.384207  [   96/  265]
train() client id: f_00001-14-3 loss: 0.567547  [  128/  265]
train() client id: f_00001-14-4 loss: 0.428683  [  160/  265]
train() client id: f_00001-14-5 loss: 0.396530  [  192/  265]
train() client id: f_00001-14-6 loss: 0.387077  [  224/  265]
train() client id: f_00001-14-7 loss: 0.549709  [  256/  265]
train() client id: f_00002-0-0 loss: 1.160459  [   32/  124]
train() client id: f_00002-0-1 loss: 1.163182  [   64/  124]
train() client id: f_00002-0-2 loss: 1.047248  [   96/  124]
train() client id: f_00002-1-0 loss: 1.179341  [   32/  124]
train() client id: f_00002-1-1 loss: 1.150292  [   64/  124]
train() client id: f_00002-1-2 loss: 1.005020  [   96/  124]
train() client id: f_00002-2-0 loss: 1.069399  [   32/  124]
train() client id: f_00002-2-1 loss: 0.932963  [   64/  124]
train() client id: f_00002-2-2 loss: 1.197280  [   96/  124]
train() client id: f_00002-3-0 loss: 1.102129  [   32/  124]
train() client id: f_00002-3-1 loss: 1.081248  [   64/  124]
train() client id: f_00002-3-2 loss: 0.979463  [   96/  124]
train() client id: f_00002-4-0 loss: 1.100528  [   32/  124]
train() client id: f_00002-4-1 loss: 1.183154  [   64/  124]
train() client id: f_00002-4-2 loss: 0.902137  [   96/  124]
train() client id: f_00002-5-0 loss: 0.998740  [   32/  124]
train() client id: f_00002-5-1 loss: 1.011907  [   64/  124]
train() client id: f_00002-5-2 loss: 0.969868  [   96/  124]
train() client id: f_00002-6-0 loss: 1.047816  [   32/  124]
train() client id: f_00002-6-1 loss: 0.949464  [   64/  124]
train() client id: f_00002-6-2 loss: 0.867649  [   96/  124]
train() client id: f_00002-7-0 loss: 1.075240  [   32/  124]
train() client id: f_00002-7-1 loss: 1.101680  [   64/  124]
train() client id: f_00002-7-2 loss: 0.830518  [   96/  124]
train() client id: f_00002-8-0 loss: 0.889917  [   32/  124]
train() client id: f_00002-8-1 loss: 1.045484  [   64/  124]
train() client id: f_00002-8-2 loss: 0.805470  [   96/  124]
train() client id: f_00002-9-0 loss: 0.930392  [   32/  124]
train() client id: f_00002-9-1 loss: 0.989665  [   64/  124]
train() client id: f_00002-9-2 loss: 0.903511  [   96/  124]
train() client id: f_00002-10-0 loss: 0.908327  [   32/  124]
train() client id: f_00002-10-1 loss: 0.944557  [   64/  124]
train() client id: f_00002-10-2 loss: 0.955773  [   96/  124]
train() client id: f_00002-11-0 loss: 1.116619  [   32/  124]
train() client id: f_00002-11-1 loss: 0.847602  [   64/  124]
train() client id: f_00002-11-2 loss: 0.907105  [   96/  124]
train() client id: f_00002-12-0 loss: 0.905748  [   32/  124]
train() client id: f_00002-12-1 loss: 0.929640  [   64/  124]
train() client id: f_00002-12-2 loss: 0.965198  [   96/  124]
train() client id: f_00002-13-0 loss: 0.785668  [   32/  124]
train() client id: f_00002-13-1 loss: 0.782603  [   64/  124]
train() client id: f_00002-13-2 loss: 1.014067  [   96/  124]
train() client id: f_00002-14-0 loss: 0.867270  [   32/  124]
train() client id: f_00002-14-1 loss: 0.988733  [   64/  124]
train() client id: f_00002-14-2 loss: 0.827002  [   96/  124]
train() client id: f_00003-0-0 loss: 0.716080  [   32/   43]
train() client id: f_00003-1-0 loss: 0.507829  [   32/   43]
train() client id: f_00003-2-0 loss: 0.650263  [   32/   43]
train() client id: f_00003-3-0 loss: 0.533858  [   32/   43]
train() client id: f_00003-4-0 loss: 0.380754  [   32/   43]
train() client id: f_00003-5-0 loss: 0.548436  [   32/   43]
train() client id: f_00003-6-0 loss: 0.519426  [   32/   43]
train() client id: f_00003-7-0 loss: 0.683051  [   32/   43]
train() client id: f_00003-8-0 loss: 0.656033  [   32/   43]
train() client id: f_00003-9-0 loss: 0.585326  [   32/   43]
train() client id: f_00003-10-0 loss: 0.473189  [   32/   43]
train() client id: f_00003-11-0 loss: 0.485545  [   32/   43]
train() client id: f_00003-12-0 loss: 0.552006  [   32/   43]
train() client id: f_00003-13-0 loss: 0.609686  [   32/   43]
train() client id: f_00003-14-0 loss: 0.481141  [   32/   43]
train() client id: f_00004-0-0 loss: 0.502423  [   32/  306]
train() client id: f_00004-0-1 loss: 0.676273  [   64/  306]
train() client id: f_00004-0-2 loss: 0.717049  [   96/  306]
train() client id: f_00004-0-3 loss: 0.595132  [  128/  306]
train() client id: f_00004-0-4 loss: 0.605428  [  160/  306]
train() client id: f_00004-0-5 loss: 0.653730  [  192/  306]
train() client id: f_00004-0-6 loss: 0.640092  [  224/  306]
train() client id: f_00004-0-7 loss: 0.660563  [  256/  306]
train() client id: f_00004-0-8 loss: 0.594417  [  288/  306]
train() client id: f_00004-1-0 loss: 0.510022  [   32/  306]
train() client id: f_00004-1-1 loss: 0.599129  [   64/  306]
train() client id: f_00004-1-2 loss: 0.811552  [   96/  306]
train() client id: f_00004-1-3 loss: 0.636954  [  128/  306]
train() client id: f_00004-1-4 loss: 0.623505  [  160/  306]
train() client id: f_00004-1-5 loss: 0.697820  [  192/  306]
train() client id: f_00004-1-6 loss: 0.621260  [  224/  306]
train() client id: f_00004-1-7 loss: 0.642027  [  256/  306]
train() client id: f_00004-1-8 loss: 0.546176  [  288/  306]
train() client id: f_00004-2-0 loss: 0.556953  [   32/  306]
train() client id: f_00004-2-1 loss: 0.689592  [   64/  306]
train() client id: f_00004-2-2 loss: 0.823021  [   96/  306]
train() client id: f_00004-2-3 loss: 0.574761  [  128/  306]
train() client id: f_00004-2-4 loss: 0.467206  [  160/  306]
train() client id: f_00004-2-5 loss: 0.817341  [  192/  306]
train() client id: f_00004-2-6 loss: 0.696809  [  224/  306]
train() client id: f_00004-2-7 loss: 0.666548  [  256/  306]
train() client id: f_00004-2-8 loss: 0.493036  [  288/  306]
train() client id: f_00004-3-0 loss: 0.645597  [   32/  306]
train() client id: f_00004-3-1 loss: 0.689261  [   64/  306]
train() client id: f_00004-3-2 loss: 0.557612  [   96/  306]
train() client id: f_00004-3-3 loss: 0.769008  [  128/  306]
train() client id: f_00004-3-4 loss: 0.665772  [  160/  306]
train() client id: f_00004-3-5 loss: 0.573732  [  192/  306]
train() client id: f_00004-3-6 loss: 0.580967  [  224/  306]
train() client id: f_00004-3-7 loss: 0.689515  [  256/  306]
train() client id: f_00004-3-8 loss: 0.711982  [  288/  306]
train() client id: f_00004-4-0 loss: 0.692822  [   32/  306]
train() client id: f_00004-4-1 loss: 0.861081  [   64/  306]
train() client id: f_00004-4-2 loss: 0.678306  [   96/  306]
train() client id: f_00004-4-3 loss: 0.599994  [  128/  306]
train() client id: f_00004-4-4 loss: 0.627517  [  160/  306]
train() client id: f_00004-4-5 loss: 0.674817  [  192/  306]
train() client id: f_00004-4-6 loss: 0.533254  [  224/  306]
train() client id: f_00004-4-7 loss: 0.622620  [  256/  306]
train() client id: f_00004-4-8 loss: 0.600533  [  288/  306]
train() client id: f_00004-5-0 loss: 0.714915  [   32/  306]
train() client id: f_00004-5-1 loss: 0.634465  [   64/  306]
train() client id: f_00004-5-2 loss: 0.656974  [   96/  306]
train() client id: f_00004-5-3 loss: 0.776919  [  128/  306]
train() client id: f_00004-5-4 loss: 0.680129  [  160/  306]
train() client id: f_00004-5-5 loss: 0.661195  [  192/  306]
train() client id: f_00004-5-6 loss: 0.590716  [  224/  306]
train() client id: f_00004-5-7 loss: 0.667518  [  256/  306]
train() client id: f_00004-5-8 loss: 0.581944  [  288/  306]
train() client id: f_00004-6-0 loss: 0.740732  [   32/  306]
train() client id: f_00004-6-1 loss: 0.710151  [   64/  306]
train() client id: f_00004-6-2 loss: 0.544496  [   96/  306]
train() client id: f_00004-6-3 loss: 0.668117  [  128/  306]
train() client id: f_00004-6-4 loss: 0.617039  [  160/  306]
train() client id: f_00004-6-5 loss: 0.767929  [  192/  306]
train() client id: f_00004-6-6 loss: 0.615208  [  224/  306]
train() client id: f_00004-6-7 loss: 0.582036  [  256/  306]
train() client id: f_00004-6-8 loss: 0.734984  [  288/  306]
train() client id: f_00004-7-0 loss: 0.552852  [   32/  306]
train() client id: f_00004-7-1 loss: 0.689898  [   64/  306]
train() client id: f_00004-7-2 loss: 0.605273  [   96/  306]
train() client id: f_00004-7-3 loss: 0.783679  [  128/  306]
train() client id: f_00004-7-4 loss: 0.649539  [  160/  306]
train() client id: f_00004-7-5 loss: 0.573888  [  192/  306]
train() client id: f_00004-7-6 loss: 0.804002  [  224/  306]
train() client id: f_00004-7-7 loss: 0.631350  [  256/  306]
train() client id: f_00004-7-8 loss: 0.727198  [  288/  306]
train() client id: f_00004-8-0 loss: 0.682941  [   32/  306]
train() client id: f_00004-8-1 loss: 0.629864  [   64/  306]
train() client id: f_00004-8-2 loss: 0.684034  [   96/  306]
train() client id: f_00004-8-3 loss: 0.580817  [  128/  306]
train() client id: f_00004-8-4 loss: 0.656618  [  160/  306]
train() client id: f_00004-8-5 loss: 0.783486  [  192/  306]
train() client id: f_00004-8-6 loss: 0.627460  [  224/  306]
train() client id: f_00004-8-7 loss: 0.590073  [  256/  306]
train() client id: f_00004-8-8 loss: 0.780946  [  288/  306]
train() client id: f_00004-9-0 loss: 0.737818  [   32/  306]
train() client id: f_00004-9-1 loss: 0.633896  [   64/  306]
train() client id: f_00004-9-2 loss: 0.597589  [   96/  306]
train() client id: f_00004-9-3 loss: 0.817732  [  128/  306]
train() client id: f_00004-9-4 loss: 0.532672  [  160/  306]
train() client id: f_00004-9-5 loss: 0.745932  [  192/  306]
train() client id: f_00004-9-6 loss: 0.515184  [  224/  306]
train() client id: f_00004-9-7 loss: 0.759146  [  256/  306]
train() client id: f_00004-9-8 loss: 0.757616  [  288/  306]
train() client id: f_00004-10-0 loss: 0.797948  [   32/  306]
train() client id: f_00004-10-1 loss: 0.626855  [   64/  306]
train() client id: f_00004-10-2 loss: 0.662615  [   96/  306]
train() client id: f_00004-10-3 loss: 0.619551  [  128/  306]
train() client id: f_00004-10-4 loss: 0.725373  [  160/  306]
train() client id: f_00004-10-5 loss: 0.729021  [  192/  306]
train() client id: f_00004-10-6 loss: 0.691225  [  224/  306]
train() client id: f_00004-10-7 loss: 0.631548  [  256/  306]
train() client id: f_00004-10-8 loss: 0.648621  [  288/  306]
train() client id: f_00004-11-0 loss: 0.712627  [   32/  306]
train() client id: f_00004-11-1 loss: 0.695693  [   64/  306]
train() client id: f_00004-11-2 loss: 0.669624  [   96/  306]
train() client id: f_00004-11-3 loss: 0.657744  [  128/  306]
train() client id: f_00004-11-4 loss: 0.777211  [  160/  306]
train() client id: f_00004-11-5 loss: 0.647049  [  192/  306]
train() client id: f_00004-11-6 loss: 0.653571  [  224/  306]
train() client id: f_00004-11-7 loss: 0.630335  [  256/  306]
train() client id: f_00004-11-8 loss: 0.678628  [  288/  306]
train() client id: f_00004-12-0 loss: 0.630488  [   32/  306]
train() client id: f_00004-12-1 loss: 0.639849  [   64/  306]
train() client id: f_00004-12-2 loss: 0.754432  [   96/  306]
train() client id: f_00004-12-3 loss: 0.798948  [  128/  306]
train() client id: f_00004-12-4 loss: 0.632821  [  160/  306]
train() client id: f_00004-12-5 loss: 0.645453  [  192/  306]
train() client id: f_00004-12-6 loss: 0.555483  [  224/  306]
train() client id: f_00004-12-7 loss: 0.746167  [  256/  306]
train() client id: f_00004-12-8 loss: 0.768393  [  288/  306]
train() client id: f_00004-13-0 loss: 0.780048  [   32/  306]
train() client id: f_00004-13-1 loss: 0.618338  [   64/  306]
train() client id: f_00004-13-2 loss: 0.724242  [   96/  306]
train() client id: f_00004-13-3 loss: 0.624870  [  128/  306]
train() client id: f_00004-13-4 loss: 0.639377  [  160/  306]
train() client id: f_00004-13-5 loss: 0.779557  [  192/  306]
train() client id: f_00004-13-6 loss: 0.683487  [  224/  306]
train() client id: f_00004-13-7 loss: 0.679719  [  256/  306]
train() client id: f_00004-13-8 loss: 0.806836  [  288/  306]
train() client id: f_00004-14-0 loss: 0.664663  [   32/  306]
train() client id: f_00004-14-1 loss: 0.648733  [   64/  306]
train() client id: f_00004-14-2 loss: 0.563327  [   96/  306]
train() client id: f_00004-14-3 loss: 0.755145  [  128/  306]
train() client id: f_00004-14-4 loss: 0.759910  [  160/  306]
train() client id: f_00004-14-5 loss: 0.698425  [  192/  306]
train() client id: f_00004-14-6 loss: 0.807348  [  224/  306]
train() client id: f_00004-14-7 loss: 0.691640  [  256/  306]
train() client id: f_00004-14-8 loss: 0.726184  [  288/  306]
train() client id: f_00005-0-0 loss: 0.840222  [   32/  146]
train() client id: f_00005-0-1 loss: 0.692844  [   64/  146]
train() client id: f_00005-0-2 loss: 0.754008  [   96/  146]
train() client id: f_00005-0-3 loss: 0.994552  [  128/  146]
train() client id: f_00005-1-0 loss: 0.861062  [   32/  146]
train() client id: f_00005-1-1 loss: 0.942109  [   64/  146]
train() client id: f_00005-1-2 loss: 0.830663  [   96/  146]
train() client id: f_00005-1-3 loss: 0.625047  [  128/  146]
train() client id: f_00005-2-0 loss: 0.797812  [   32/  146]
train() client id: f_00005-2-1 loss: 0.618355  [   64/  146]
train() client id: f_00005-2-2 loss: 0.767504  [   96/  146]
train() client id: f_00005-2-3 loss: 0.826835  [  128/  146]
train() client id: f_00005-3-0 loss: 0.725769  [   32/  146]
train() client id: f_00005-3-1 loss: 0.933418  [   64/  146]
train() client id: f_00005-3-2 loss: 0.708301  [   96/  146]
train() client id: f_00005-3-3 loss: 0.767936  [  128/  146]
train() client id: f_00005-4-0 loss: 0.645231  [   32/  146]
train() client id: f_00005-4-1 loss: 0.888412  [   64/  146]
train() client id: f_00005-4-2 loss: 0.971278  [   96/  146]
train() client id: f_00005-4-3 loss: 0.763576  [  128/  146]
train() client id: f_00005-5-0 loss: 1.050664  [   32/  146]
train() client id: f_00005-5-1 loss: 0.686787  [   64/  146]
train() client id: f_00005-5-2 loss: 0.865167  [   96/  146]
train() client id: f_00005-5-3 loss: 0.567320  [  128/  146]
train() client id: f_00005-6-0 loss: 0.850558  [   32/  146]
train() client id: f_00005-6-1 loss: 0.748390  [   64/  146]
train() client id: f_00005-6-2 loss: 0.778543  [   96/  146]
train() client id: f_00005-6-3 loss: 0.846285  [  128/  146]
train() client id: f_00005-7-0 loss: 0.582637  [   32/  146]
train() client id: f_00005-7-1 loss: 0.964966  [   64/  146]
train() client id: f_00005-7-2 loss: 0.802145  [   96/  146]
train() client id: f_00005-7-3 loss: 0.838368  [  128/  146]
train() client id: f_00005-8-0 loss: 0.696296  [   32/  146]
train() client id: f_00005-8-1 loss: 0.775987  [   64/  146]
train() client id: f_00005-8-2 loss: 0.872050  [   96/  146]
train() client id: f_00005-8-3 loss: 0.781906  [  128/  146]
train() client id: f_00005-9-0 loss: 0.998407  [   32/  146]
train() client id: f_00005-9-1 loss: 0.821111  [   64/  146]
train() client id: f_00005-9-2 loss: 0.560605  [   96/  146]
train() client id: f_00005-9-3 loss: 0.768233  [  128/  146]
train() client id: f_00005-10-0 loss: 0.664290  [   32/  146]
train() client id: f_00005-10-1 loss: 0.870396  [   64/  146]
train() client id: f_00005-10-2 loss: 0.798022  [   96/  146]
train() client id: f_00005-10-3 loss: 0.785992  [  128/  146]
train() client id: f_00005-11-0 loss: 0.710357  [   32/  146]
train() client id: f_00005-11-1 loss: 0.594373  [   64/  146]
train() client id: f_00005-11-2 loss: 0.601145  [   96/  146]
train() client id: f_00005-11-3 loss: 0.889915  [  128/  146]
train() client id: f_00005-12-0 loss: 0.891853  [   32/  146]
train() client id: f_00005-12-1 loss: 0.556966  [   64/  146]
train() client id: f_00005-12-2 loss: 0.662925  [   96/  146]
train() client id: f_00005-12-3 loss: 0.983229  [  128/  146]
train() client id: f_00005-13-0 loss: 0.734433  [   32/  146]
train() client id: f_00005-13-1 loss: 0.728663  [   64/  146]
train() client id: f_00005-13-2 loss: 0.734097  [   96/  146]
train() client id: f_00005-13-3 loss: 1.006154  [  128/  146]
train() client id: f_00005-14-0 loss: 0.880198  [   32/  146]
train() client id: f_00005-14-1 loss: 0.832459  [   64/  146]
train() client id: f_00005-14-2 loss: 0.826606  [   96/  146]
train() client id: f_00005-14-3 loss: 0.644695  [  128/  146]
train() client id: f_00006-0-0 loss: 0.569084  [   32/   54]
train() client id: f_00006-1-0 loss: 0.611698  [   32/   54]
train() client id: f_00006-2-0 loss: 0.522607  [   32/   54]
train() client id: f_00006-3-0 loss: 0.510716  [   32/   54]
train() client id: f_00006-4-0 loss: 0.514012  [   32/   54]
train() client id: f_00006-5-0 loss: 0.555371  [   32/   54]
train() client id: f_00006-6-0 loss: 0.616652  [   32/   54]
train() client id: f_00006-7-0 loss: 0.559639  [   32/   54]
train() client id: f_00006-8-0 loss: 0.520826  [   32/   54]
train() client id: f_00006-9-0 loss: 0.554507  [   32/   54]
train() client id: f_00006-10-0 loss: 0.507498  [   32/   54]
train() client id: f_00006-11-0 loss: 0.600566  [   32/   54]
train() client id: f_00006-12-0 loss: 0.577479  [   32/   54]
train() client id: f_00006-13-0 loss: 0.586397  [   32/   54]
train() client id: f_00006-14-0 loss: 0.567544  [   32/   54]
train() client id: f_00007-0-0 loss: 0.296432  [   32/  179]
train() client id: f_00007-0-1 loss: 0.241801  [   64/  179]
train() client id: f_00007-0-2 loss: 0.714007  [   96/  179]
train() client id: f_00007-0-3 loss: 0.435719  [  128/  179]
train() client id: f_00007-0-4 loss: 0.322076  [  160/  179]
train() client id: f_00007-1-0 loss: 0.435236  [   32/  179]
train() client id: f_00007-1-1 loss: 0.220904  [   64/  179]
train() client id: f_00007-1-2 loss: 0.396795  [   96/  179]
train() client id: f_00007-1-3 loss: 0.364685  [  128/  179]
train() client id: f_00007-1-4 loss: 0.456538  [  160/  179]
train() client id: f_00007-2-0 loss: 0.279201  [   32/  179]
train() client id: f_00007-2-1 loss: 0.335764  [   64/  179]
train() client id: f_00007-2-2 loss: 0.422868  [   96/  179]
train() client id: f_00007-2-3 loss: 0.284209  [  128/  179]
train() client id: f_00007-2-4 loss: 0.377993  [  160/  179]
train() client id: f_00007-3-0 loss: 0.486134  [   32/  179]
train() client id: f_00007-3-1 loss: 0.198220  [   64/  179]
train() client id: f_00007-3-2 loss: 0.423887  [   96/  179]
train() client id: f_00007-3-3 loss: 0.318273  [  128/  179]
train() client id: f_00007-3-4 loss: 0.244993  [  160/  179]
train() client id: f_00007-4-0 loss: 0.398782  [   32/  179]
train() client id: f_00007-4-1 loss: 0.332472  [   64/  179]
train() client id: f_00007-4-2 loss: 0.556384  [   96/  179]
train() client id: f_00007-4-3 loss: 0.250686  [  128/  179]
train() client id: f_00007-4-4 loss: 0.165579  [  160/  179]
train() client id: f_00007-5-0 loss: 0.343226  [   32/  179]
train() client id: f_00007-5-1 loss: 0.359935  [   64/  179]
train() client id: f_00007-5-2 loss: 0.262392  [   96/  179]
train() client id: f_00007-5-3 loss: 0.370421  [  128/  179]
train() client id: f_00007-5-4 loss: 0.140799  [  160/  179]
train() client id: f_00007-6-0 loss: 0.357274  [   32/  179]
train() client id: f_00007-6-1 loss: 0.203590  [   64/  179]
train() client id: f_00007-6-2 loss: 0.282629  [   96/  179]
train() client id: f_00007-6-3 loss: 0.375376  [  128/  179]
train() client id: f_00007-6-4 loss: 0.256188  [  160/  179]
train() client id: f_00007-7-0 loss: 0.201556  [   32/  179]
train() client id: f_00007-7-1 loss: 0.423795  [   64/  179]
train() client id: f_00007-7-2 loss: 0.274295  [   96/  179]
train() client id: f_00007-7-3 loss: 0.489438  [  128/  179]
train() client id: f_00007-7-4 loss: 0.180749  [  160/  179]
train() client id: f_00007-8-0 loss: 0.260128  [   32/  179]
train() client id: f_00007-8-1 loss: 0.407907  [   64/  179]
train() client id: f_00007-8-2 loss: 0.268217  [   96/  179]
train() client id: f_00007-8-3 loss: 0.234308  [  128/  179]
train() client id: f_00007-8-4 loss: 0.234274  [  160/  179]
train() client id: f_00007-9-0 loss: 0.243818  [   32/  179]
train() client id: f_00007-9-1 loss: 0.261983  [   64/  179]
train() client id: f_00007-9-2 loss: 0.136433  [   96/  179]
train() client id: f_00007-9-3 loss: 0.411570  [  128/  179]
train() client id: f_00007-9-4 loss: 0.405445  [  160/  179]
train() client id: f_00007-10-0 loss: 0.351331  [   32/  179]
train() client id: f_00007-10-1 loss: 0.323605  [   64/  179]
train() client id: f_00007-10-2 loss: 0.406197  [   96/  179]
train() client id: f_00007-10-3 loss: 0.348225  [  128/  179]
train() client id: f_00007-10-4 loss: 0.145146  [  160/  179]
train() client id: f_00007-11-0 loss: 0.338580  [   32/  179]
train() client id: f_00007-11-1 loss: 0.140313  [   64/  179]
train() client id: f_00007-11-2 loss: 0.402084  [   96/  179]
train() client id: f_00007-11-3 loss: 0.273407  [  128/  179]
train() client id: f_00007-11-4 loss: 0.410229  [  160/  179]
train() client id: f_00007-12-0 loss: 0.322107  [   32/  179]
train() client id: f_00007-12-1 loss: 0.167338  [   64/  179]
train() client id: f_00007-12-2 loss: 0.432988  [   96/  179]
train() client id: f_00007-12-3 loss: 0.232265  [  128/  179]
train() client id: f_00007-12-4 loss: 0.396483  [  160/  179]
train() client id: f_00007-13-0 loss: 0.279069  [   32/  179]
train() client id: f_00007-13-1 loss: 0.145321  [   64/  179]
train() client id: f_00007-13-2 loss: 0.402069  [   96/  179]
train() client id: f_00007-13-3 loss: 0.232232  [  128/  179]
train() client id: f_00007-13-4 loss: 0.313408  [  160/  179]
train() client id: f_00007-14-0 loss: 0.311829  [   32/  179]
train() client id: f_00007-14-1 loss: 0.311470  [   64/  179]
train() client id: f_00007-14-2 loss: 0.429053  [   96/  179]
train() client id: f_00007-14-3 loss: 0.226993  [  128/  179]
train() client id: f_00007-14-4 loss: 0.231531  [  160/  179]
train() client id: f_00008-0-0 loss: 0.825913  [   32/  130]
train() client id: f_00008-0-1 loss: 0.702010  [   64/  130]
train() client id: f_00008-0-2 loss: 0.727454  [   96/  130]
train() client id: f_00008-0-3 loss: 0.687044  [  128/  130]
train() client id: f_00008-1-0 loss: 0.789650  [   32/  130]
train() client id: f_00008-1-1 loss: 0.705601  [   64/  130]
train() client id: f_00008-1-2 loss: 0.675418  [   96/  130]
train() client id: f_00008-1-3 loss: 0.784585  [  128/  130]
train() client id: f_00008-2-0 loss: 0.792385  [   32/  130]
train() client id: f_00008-2-1 loss: 0.708595  [   64/  130]
train() client id: f_00008-2-2 loss: 0.686608  [   96/  130]
train() client id: f_00008-2-3 loss: 0.760620  [  128/  130]
train() client id: f_00008-3-0 loss: 0.832256  [   32/  130]
train() client id: f_00008-3-1 loss: 0.702708  [   64/  130]
train() client id: f_00008-3-2 loss: 0.723803  [   96/  130]
train() client id: f_00008-3-3 loss: 0.644883  [  128/  130]
train() client id: f_00008-4-0 loss: 0.712093  [   32/  130]
train() client id: f_00008-4-1 loss: 0.666638  [   64/  130]
train() client id: f_00008-4-2 loss: 0.793547  [   96/  130]
train() client id: f_00008-4-3 loss: 0.723426  [  128/  130]
train() client id: f_00008-5-0 loss: 0.707209  [   32/  130]
train() client id: f_00008-5-1 loss: 0.897310  [   64/  130]
train() client id: f_00008-5-2 loss: 0.651517  [   96/  130]
train() client id: f_00008-5-3 loss: 0.695068  [  128/  130]
train() client id: f_00008-6-0 loss: 0.655558  [   32/  130]
train() client id: f_00008-6-1 loss: 0.747776  [   64/  130]
train() client id: f_00008-6-2 loss: 0.714437  [   96/  130]
train() client id: f_00008-6-3 loss: 0.820958  [  128/  130]
train() client id: f_00008-7-0 loss: 0.709157  [   32/  130]
train() client id: f_00008-7-1 loss: 0.781572  [   64/  130]
train() client id: f_00008-7-2 loss: 0.747572  [   96/  130]
train() client id: f_00008-7-3 loss: 0.683615  [  128/  130]
train() client id: f_00008-8-0 loss: 0.726246  [   32/  130]
train() client id: f_00008-8-1 loss: 0.577584  [   64/  130]
train() client id: f_00008-8-2 loss: 0.813161  [   96/  130]
train() client id: f_00008-8-3 loss: 0.765237  [  128/  130]
train() client id: f_00008-9-0 loss: 0.740698  [   32/  130]
train() client id: f_00008-9-1 loss: 0.716448  [   64/  130]
train() client id: f_00008-9-2 loss: 0.712832  [   96/  130]
train() client id: f_00008-9-3 loss: 0.775712  [  128/  130]
train() client id: f_00008-10-0 loss: 0.651990  [   32/  130]
train() client id: f_00008-10-1 loss: 0.708124  [   64/  130]
train() client id: f_00008-10-2 loss: 0.723154  [   96/  130]
train() client id: f_00008-10-3 loss: 0.860897  [  128/  130]
train() client id: f_00008-11-0 loss: 0.644369  [   32/  130]
train() client id: f_00008-11-1 loss: 0.770751  [   64/  130]
train() client id: f_00008-11-2 loss: 0.707121  [   96/  130]
train() client id: f_00008-11-3 loss: 0.809763  [  128/  130]
train() client id: f_00008-12-0 loss: 0.688500  [   32/  130]
train() client id: f_00008-12-1 loss: 0.706245  [   64/  130]
train() client id: f_00008-12-2 loss: 0.806940  [   96/  130]
train() client id: f_00008-12-3 loss: 0.747833  [  128/  130]
train() client id: f_00008-13-0 loss: 0.716593  [   32/  130]
train() client id: f_00008-13-1 loss: 0.815629  [   64/  130]
train() client id: f_00008-13-2 loss: 0.703122  [   96/  130]
train() client id: f_00008-13-3 loss: 0.720521  [  128/  130]
train() client id: f_00008-14-0 loss: 0.818072  [   32/  130]
train() client id: f_00008-14-1 loss: 0.659132  [   64/  130]
train() client id: f_00008-14-2 loss: 0.723297  [   96/  130]
train() client id: f_00008-14-3 loss: 0.714644  [  128/  130]
train() client id: f_00009-0-0 loss: 1.075042  [   32/  118]
train() client id: f_00009-0-1 loss: 0.915021  [   64/  118]
train() client id: f_00009-0-2 loss: 1.039049  [   96/  118]
train() client id: f_00009-1-0 loss: 0.914153  [   32/  118]
train() client id: f_00009-1-1 loss: 0.891299  [   64/  118]
train() client id: f_00009-1-2 loss: 0.995717  [   96/  118]
train() client id: f_00009-2-0 loss: 0.846427  [   32/  118]
train() client id: f_00009-2-1 loss: 0.830330  [   64/  118]
train() client id: f_00009-2-2 loss: 1.080881  [   96/  118]
train() client id: f_00009-3-0 loss: 0.952209  [   32/  118]
train() client id: f_00009-3-1 loss: 0.795063  [   64/  118]
train() client id: f_00009-3-2 loss: 0.786794  [   96/  118]
train() client id: f_00009-4-0 loss: 0.810788  [   32/  118]
train() client id: f_00009-4-1 loss: 0.834954  [   64/  118]
train() client id: f_00009-4-2 loss: 0.831404  [   96/  118]
train() client id: f_00009-5-0 loss: 0.819144  [   32/  118]
train() client id: f_00009-5-1 loss: 0.794040  [   64/  118]
train() client id: f_00009-5-2 loss: 0.761813  [   96/  118]
train() client id: f_00009-6-0 loss: 0.697646  [   32/  118]
train() client id: f_00009-6-1 loss: 0.806700  [   64/  118]
train() client id: f_00009-6-2 loss: 0.776510  [   96/  118]
train() client id: f_00009-7-0 loss: 0.676214  [   32/  118]
train() client id: f_00009-7-1 loss: 0.774772  [   64/  118]
train() client id: f_00009-7-2 loss: 0.801472  [   96/  118]
train() client id: f_00009-8-0 loss: 0.737991  [   32/  118]
train() client id: f_00009-8-1 loss: 0.727093  [   64/  118]
train() client id: f_00009-8-2 loss: 0.846268  [   96/  118]
train() client id: f_00009-9-0 loss: 0.698606  [   32/  118]
train() client id: f_00009-9-1 loss: 0.787231  [   64/  118]
train() client id: f_00009-9-2 loss: 0.764108  [   96/  118]
train() client id: f_00009-10-0 loss: 0.742902  [   32/  118]
train() client id: f_00009-10-1 loss: 0.733992  [   64/  118]
train() client id: f_00009-10-2 loss: 0.718061  [   96/  118]
train() client id: f_00009-11-0 loss: 0.812874  [   32/  118]
train() client id: f_00009-11-1 loss: 0.751086  [   64/  118]
train() client id: f_00009-11-2 loss: 0.556081  [   96/  118]
train() client id: f_00009-12-0 loss: 0.839521  [   32/  118]
train() client id: f_00009-12-1 loss: 0.565548  [   64/  118]
train() client id: f_00009-12-2 loss: 0.535088  [   96/  118]
train() client id: f_00009-13-0 loss: 0.603536  [   32/  118]
train() client id: f_00009-13-1 loss: 0.699351  [   64/  118]
train() client id: f_00009-13-2 loss: 0.870051  [   96/  118]
train() client id: f_00009-14-0 loss: 0.770970  [   32/  118]
train() client id: f_00009-14-1 loss: 0.723840  [   64/  118]
train() client id: f_00009-14-2 loss: 0.670532  [   96/  118]
At round 38 accuracy: 0.6445623342175066
At round 38 training accuracy: 0.5848423876592891
At round 38 training loss: 0.8360285274575326
update_location
xs = 8.927491 311.223621 5.882650 0.934260 -227.581990 -75.230757 -35.849135 -5.143845 -250.120581 20.134486 
ys = -302.390647 7.291448 200.684448 -22.290817 -9.642386 0.794442 -1.381692 196.628436 25.881276 -737.232496 
xs mean: -24.682379970521218
ys mean: -64.16579882624052
dists_uav = 318.621725 326.976004 224.296352 102.458545 248.770050 125.141112 106.240621 220.656296 270.610690 744.256105 
uav_gains = -117.185533 -117.770714 -109.290566 -100.263727 -111.191777 -102.435386 -100.657303 -109.035553 -113.125970 -128.753513 
uav_gains_db_mean: -110.97100418451896
dists_bs = 511.596669 514.334101 182.697082 264.341692 191.983532 200.752281 224.663002 171.227634 166.971541 932.869548 
bs_gains = -115.417035 -115.481928 -102.895544 -107.387698 -103.498450 -104.041553 -105.409943 -102.107127 -101.801048 -122.722045 
bs_gains_db_mean: -108.07623710660137
Round 39
-------------------------------
ene_coms = [0.0198502  0.0211793  0.00716169 0.00637216 0.00737037 0.00701006
 0.00647913 0.00690569 0.0068111  0.04104892]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.98708908 12.28391098  5.70678801  2.03719206 13.92861526  6.69798719
  2.53552844  8.18662883  5.97248476  5.94323677]
obj_prev = 69.27946138584238
eta_min = 1.8064601938019838e-16	eta_max = 0.7802534199471203
af = 14.474348849714058	bf = 1.929826987689753	zeta = 15.921783734685466	eta = 0.909090909090909
af = 14.474348849714058	bf = 1.929826987689753	zeta = 36.4031775701717	eta = 0.39761223650910504
af = 14.474348849714058	bf = 1.929826987689753	zeta = 25.364394561447263	eta = 0.5706561934545213
af = 14.474348849714058	bf = 1.929826987689753	zeta = 23.406600082382074	eta = 0.6183874974908792
af = 14.474348849714058	bf = 1.929826987689753	zeta = 23.28743935814448	eta = 0.6215517570269847
af = 14.474348849714058	bf = 1.929826987689753	zeta = 23.28694204579015	eta = 0.621565030790754
af = 14.474348849714058	bf = 1.929826987689753	zeta = 23.286942037068496	eta = 0.6215650310235487
eta = 0.6215650310235487
ene_coms = [0.0198502  0.0211793  0.00716169 0.00637216 0.00737037 0.00701006
 0.00647913 0.00690569 0.0068111  0.04104892]
ene_comp = [0.03923834 0.08252507 0.03861551 0.01339086 0.0952931  0.04546664
 0.01681643 0.05574335 0.040484   0.03674701]
ene_total = [2.31450078 4.06210495 1.79309489 0.77411847 4.02133293 2.0555149
 0.91248801 2.45396592 1.85255096 3.04727023]
ti_comp = [0.51667472 0.50338376 0.64355984 0.65145515 0.64147304 0.64507615
 0.65038549 0.64611982 0.6470658  0.30468756]
ti_coms = [0.19850203 0.211793   0.07161692 0.06372161 0.07370372 0.07010061
 0.06479127 0.06905694 0.06811096 0.4104892 ]
t_total = [28.01350594 28.01350594 28.01350594 28.01350594 28.01350594 28.01350594
 28.01350594 28.01350594 28.01350594 28.01350594]
ene_coms = [0.0198502  0.0211793  0.00716169 0.00637216 0.00737037 0.00701006
 0.00647913 0.00690569 0.0068111  0.04104892]
ene_comp = [1.41441660e-05 1.38624299e-04 8.68935068e-06 3.53619715e-07
 1.31433962e-04 1.41168156e-05 7.02651107e-07 2.59318265e-05
 9.90451706e-06 3.34069423e-05]
ene_total = [0.77808739 0.83502405 0.28086415 0.2496117  0.29384606 0.27513737
 0.25381522 0.27151209 0.26717889 1.60919661]
optimize_network iter = 0 obj = 5.114273531371892
eta = 0.6215650310235487
freqs = [37971991.4591022  81970336.74083851 30001487.96121102 10277653.74904941
 74276778.90532666 35241299.82394452 12928048.19107527 43137007.43834528
 31282751.95524241 60302780.09788696]
eta_min = 0.6215650310235493	eta_max = 0.6215650310235484
af = 0.011761901464206581	bf = 1.929826987689753	zeta = 0.01293809161062724	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0198502  0.0211793  0.00716169 0.00637216 0.00737037 0.00701006
 0.00647913 0.00690569 0.0068111  0.04104892]
ene_comp = [2.83499731e-06 2.77852731e-05 1.74165701e-06 7.08780527e-08
 2.63440722e-05 2.82951532e-06 1.40836441e-07 5.19766655e-06
 1.98521986e-06 6.69594740e-06]
ene_total = [3.23716623 3.45795229 1.16804411 1.03903364 1.20608229 1.14349715
 1.05648649 1.12686553 1.11091689 6.69438356]
ti_comp = [0.51667472 0.50338376 0.64355984 0.65145515 0.64147304 0.64507615
 0.65038549 0.64611982 0.6470658  0.30468756]
ti_coms = [0.19850203 0.211793   0.07161692 0.06372161 0.07370372 0.07010061
 0.06479127 0.06905694 0.06811096 0.4104892 ]
t_total = [28.01350594 28.01350594 28.01350594 28.01350594 28.01350594 28.01350594
 28.01350594 28.01350594 28.01350594 28.01350594]
ene_coms = [0.0198502  0.0211793  0.00716169 0.00637216 0.00737037 0.00701006
 0.00647913 0.00690569 0.0068111  0.04104892]
ene_comp = [1.41441660e-05 1.38624299e-04 8.68935068e-06 3.53619715e-07
 1.31433962e-04 1.41168156e-05 7.02651107e-07 2.59318265e-05
 9.90451706e-06 3.34069423e-05]
ene_total = [0.77808739 0.83502405 0.28086415 0.2496117  0.29384606 0.27513737
 0.25381522 0.27151209 0.26717889 1.60919661]
optimize_network iter = 1 obj = 5.114273531371888
eta = 0.6215650310235484
freqs = [37971991.4591022  81970336.7408385  30001487.96121102 10277653.74904941
 74276778.90532666 35241299.82394452 12928048.19107527 43137007.43834528
 31282751.95524242 60302780.0978869 ]
Done!
ene_coms = [0.0198502  0.0211793  0.00716169 0.00637216 0.00737037 0.00701006
 0.00647913 0.00690569 0.0068111  0.04104892]
ene_comp = [1.36256917e-05 1.33542830e-04 8.37083030e-06 3.40657287e-07
 1.26616065e-04 1.35993439e-05 6.76894442e-07 2.49812589e-05
 9.54145306e-06 3.21823638e-05]
ene_total = [0.01986383 0.02131284 0.00717006 0.0063725  0.00749699 0.00702366
 0.0064798  0.00693068 0.00682064 0.0410811 ]
At round 39 energy consumption: 0.1305521032491229
At round 39 eta: 0.6215650310235484
At round 39 a_n: 14.823314824451002
At round 39 local rounds: 15.57076842370787
At round 39 global rounds: 39.17004515873213
gradient difference: 0.38454481959342957
train() client id: f_00000-0-0 loss: 1.252872  [   32/  126]
train() client id: f_00000-0-1 loss: 1.348408  [   64/  126]
train() client id: f_00000-0-2 loss: 1.248884  [   96/  126]
train() client id: f_00000-1-0 loss: 1.143029  [   32/  126]
train() client id: f_00000-1-1 loss: 1.108237  [   64/  126]
train() client id: f_00000-1-2 loss: 1.129290  [   96/  126]
train() client id: f_00000-2-0 loss: 1.101604  [   32/  126]
train() client id: f_00000-2-1 loss: 1.225697  [   64/  126]
train() client id: f_00000-2-2 loss: 0.899042  [   96/  126]
train() client id: f_00000-3-0 loss: 1.018955  [   32/  126]
train() client id: f_00000-3-1 loss: 1.024894  [   64/  126]
train() client id: f_00000-3-2 loss: 0.994940  [   96/  126]
train() client id: f_00000-4-0 loss: 0.936922  [   32/  126]
train() client id: f_00000-4-1 loss: 0.957167  [   64/  126]
train() client id: f_00000-4-2 loss: 0.938487  [   96/  126]
train() client id: f_00000-5-0 loss: 0.926734  [   32/  126]
train() client id: f_00000-5-1 loss: 0.950553  [   64/  126]
train() client id: f_00000-5-2 loss: 0.839856  [   96/  126]
train() client id: f_00000-6-0 loss: 0.833024  [   32/  126]
train() client id: f_00000-6-1 loss: 0.806011  [   64/  126]
train() client id: f_00000-6-2 loss: 1.001984  [   96/  126]
train() client id: f_00000-7-0 loss: 0.745407  [   32/  126]
train() client id: f_00000-7-1 loss: 0.875456  [   64/  126]
train() client id: f_00000-7-2 loss: 0.904714  [   96/  126]
train() client id: f_00000-8-0 loss: 0.861945  [   32/  126]
train() client id: f_00000-8-1 loss: 0.829845  [   64/  126]
train() client id: f_00000-8-2 loss: 0.829382  [   96/  126]
train() client id: f_00000-9-0 loss: 0.851229  [   32/  126]
train() client id: f_00000-9-1 loss: 0.718140  [   64/  126]
train() client id: f_00000-9-2 loss: 0.845851  [   96/  126]
train() client id: f_00000-10-0 loss: 0.795276  [   32/  126]
train() client id: f_00000-10-1 loss: 0.791092  [   64/  126]
train() client id: f_00000-10-2 loss: 0.832150  [   96/  126]
train() client id: f_00000-11-0 loss: 0.813095  [   32/  126]
train() client id: f_00000-11-1 loss: 0.789897  [   64/  126]
train() client id: f_00000-11-2 loss: 0.848355  [   96/  126]
train() client id: f_00000-12-0 loss: 0.757952  [   32/  126]
train() client id: f_00000-12-1 loss: 0.897718  [   64/  126]
train() client id: f_00000-12-2 loss: 0.798142  [   96/  126]
train() client id: f_00000-13-0 loss: 0.938158  [   32/  126]
train() client id: f_00000-13-1 loss: 0.736478  [   64/  126]
train() client id: f_00000-13-2 loss: 0.868826  [   96/  126]
train() client id: f_00000-14-0 loss: 0.762804  [   32/  126]
train() client id: f_00000-14-1 loss: 0.737330  [   64/  126]
train() client id: f_00000-14-2 loss: 0.957407  [   96/  126]
train() client id: f_00001-0-0 loss: 0.403602  [   32/  265]
train() client id: f_00001-0-1 loss: 0.455368  [   64/  265]
train() client id: f_00001-0-2 loss: 0.339535  [   96/  265]
train() client id: f_00001-0-3 loss: 0.325650  [  128/  265]
train() client id: f_00001-0-4 loss: 0.356882  [  160/  265]
train() client id: f_00001-0-5 loss: 0.376531  [  192/  265]
train() client id: f_00001-0-6 loss: 0.388914  [  224/  265]
train() client id: f_00001-0-7 loss: 0.424219  [  256/  265]
train() client id: f_00001-1-0 loss: 0.358165  [   32/  265]
train() client id: f_00001-1-1 loss: 0.371810  [   64/  265]
train() client id: f_00001-1-2 loss: 0.372176  [   96/  265]
train() client id: f_00001-1-3 loss: 0.404106  [  128/  265]
train() client id: f_00001-1-4 loss: 0.413377  [  160/  265]
train() client id: f_00001-1-5 loss: 0.275041  [  192/  265]
train() client id: f_00001-1-6 loss: 0.337731  [  224/  265]
train() client id: f_00001-1-7 loss: 0.333779  [  256/  265]
train() client id: f_00001-2-0 loss: 0.324788  [   32/  265]
train() client id: f_00001-2-1 loss: 0.295959  [   64/  265]
train() client id: f_00001-2-2 loss: 0.435548  [   96/  265]
train() client id: f_00001-2-3 loss: 0.389356  [  128/  265]
train() client id: f_00001-2-4 loss: 0.339342  [  160/  265]
train() client id: f_00001-2-5 loss: 0.322923  [  192/  265]
train() client id: f_00001-2-6 loss: 0.420255  [  224/  265]
train() client id: f_00001-2-7 loss: 0.349429  [  256/  265]
train() client id: f_00001-3-0 loss: 0.344109  [   32/  265]
train() client id: f_00001-3-1 loss: 0.425594  [   64/  265]
train() client id: f_00001-3-2 loss: 0.267616  [   96/  265]
train() client id: f_00001-3-3 loss: 0.417740  [  128/  265]
train() client id: f_00001-3-4 loss: 0.362731  [  160/  265]
train() client id: f_00001-3-5 loss: 0.315738  [  192/  265]
train() client id: f_00001-3-6 loss: 0.418654  [  224/  265]
train() client id: f_00001-3-7 loss: 0.252464  [  256/  265]
train() client id: f_00001-4-0 loss: 0.379876  [   32/  265]
train() client id: f_00001-4-1 loss: 0.381221  [   64/  265]
train() client id: f_00001-4-2 loss: 0.303171  [   96/  265]
train() client id: f_00001-4-3 loss: 0.462248  [  128/  265]
train() client id: f_00001-4-4 loss: 0.246871  [  160/  265]
train() client id: f_00001-4-5 loss: 0.368899  [  192/  265]
train() client id: f_00001-4-6 loss: 0.247769  [  224/  265]
train() client id: f_00001-4-7 loss: 0.318320  [  256/  265]
train() client id: f_00001-5-0 loss: 0.334757  [   32/  265]
train() client id: f_00001-5-1 loss: 0.325936  [   64/  265]
train() client id: f_00001-5-2 loss: 0.378717  [   96/  265]
train() client id: f_00001-5-3 loss: 0.373160  [  128/  265]
train() client id: f_00001-5-4 loss: 0.285871  [  160/  265]
train() client id: f_00001-5-5 loss: 0.390723  [  192/  265]
train() client id: f_00001-5-6 loss: 0.412573  [  224/  265]
train() client id: f_00001-5-7 loss: 0.253906  [  256/  265]
train() client id: f_00001-6-0 loss: 0.366142  [   32/  265]
train() client id: f_00001-6-1 loss: 0.479653  [   64/  265]
train() client id: f_00001-6-2 loss: 0.339748  [   96/  265]
train() client id: f_00001-6-3 loss: 0.257079  [  128/  265]
train() client id: f_00001-6-4 loss: 0.284136  [  160/  265]
train() client id: f_00001-6-5 loss: 0.418470  [  192/  265]
train() client id: f_00001-6-6 loss: 0.279515  [  224/  265]
train() client id: f_00001-6-7 loss: 0.256307  [  256/  265]
train() client id: f_00001-7-0 loss: 0.321047  [   32/  265]
train() client id: f_00001-7-1 loss: 0.344089  [   64/  265]
train() client id: f_00001-7-2 loss: 0.241705  [   96/  265]
train() client id: f_00001-7-3 loss: 0.315148  [  128/  265]
train() client id: f_00001-7-4 loss: 0.396245  [  160/  265]
train() client id: f_00001-7-5 loss: 0.449138  [  192/  265]
train() client id: f_00001-7-6 loss: 0.315072  [  224/  265]
train() client id: f_00001-7-7 loss: 0.312319  [  256/  265]
train() client id: f_00001-8-0 loss: 0.310332  [   32/  265]
train() client id: f_00001-8-1 loss: 0.319401  [   64/  265]
train() client id: f_00001-8-2 loss: 0.245717  [   96/  265]
train() client id: f_00001-8-3 loss: 0.236214  [  128/  265]
train() client id: f_00001-8-4 loss: 0.475493  [  160/  265]
train() client id: f_00001-8-5 loss: 0.311776  [  192/  265]
train() client id: f_00001-8-6 loss: 0.435994  [  224/  265]
train() client id: f_00001-8-7 loss: 0.279639  [  256/  265]
train() client id: f_00001-9-0 loss: 0.281554  [   32/  265]
train() client id: f_00001-9-1 loss: 0.341206  [   64/  265]
train() client id: f_00001-9-2 loss: 0.282806  [   96/  265]
train() client id: f_00001-9-3 loss: 0.390495  [  128/  265]
train() client id: f_00001-9-4 loss: 0.395862  [  160/  265]
train() client id: f_00001-9-5 loss: 0.318183  [  192/  265]
train() client id: f_00001-9-6 loss: 0.430794  [  224/  265]
train() client id: f_00001-9-7 loss: 0.225791  [  256/  265]
train() client id: f_00001-10-0 loss: 0.226213  [   32/  265]
train() client id: f_00001-10-1 loss: 0.378061  [   64/  265]
train() client id: f_00001-10-2 loss: 0.391515  [   96/  265]
train() client id: f_00001-10-3 loss: 0.308774  [  128/  265]
train() client id: f_00001-10-4 loss: 0.221895  [  160/  265]
train() client id: f_00001-10-5 loss: 0.364857  [  192/  265]
train() client id: f_00001-10-6 loss: 0.372013  [  224/  265]
train() client id: f_00001-10-7 loss: 0.375171  [  256/  265]
train() client id: f_00001-11-0 loss: 0.307472  [   32/  265]
train() client id: f_00001-11-1 loss: 0.298354  [   64/  265]
train() client id: f_00001-11-2 loss: 0.305157  [   96/  265]
train() client id: f_00001-11-3 loss: 0.365922  [  128/  265]
train() client id: f_00001-11-4 loss: 0.444206  [  160/  265]
train() client id: f_00001-11-5 loss: 0.271520  [  192/  265]
train() client id: f_00001-11-6 loss: 0.284325  [  224/  265]
train() client id: f_00001-11-7 loss: 0.324030  [  256/  265]
train() client id: f_00001-12-0 loss: 0.419080  [   32/  265]
train() client id: f_00001-12-1 loss: 0.318963  [   64/  265]
train() client id: f_00001-12-2 loss: 0.326852  [   96/  265]
train() client id: f_00001-12-3 loss: 0.233633  [  128/  265]
train() client id: f_00001-12-4 loss: 0.305089  [  160/  265]
train() client id: f_00001-12-5 loss: 0.353340  [  192/  265]
train() client id: f_00001-12-6 loss: 0.295277  [  224/  265]
train() client id: f_00001-12-7 loss: 0.313352  [  256/  265]
train() client id: f_00001-13-0 loss: 0.433343  [   32/  265]
train() client id: f_00001-13-1 loss: 0.325220  [   64/  265]
train() client id: f_00001-13-2 loss: 0.390007  [   96/  265]
train() client id: f_00001-13-3 loss: 0.255703  [  128/  265]
train() client id: f_00001-13-4 loss: 0.312290  [  160/  265]
train() client id: f_00001-13-5 loss: 0.245639  [  192/  265]
train() client id: f_00001-13-6 loss: 0.281258  [  224/  265]
train() client id: f_00001-13-7 loss: 0.371436  [  256/  265]
train() client id: f_00001-14-0 loss: 0.230524  [   32/  265]
train() client id: f_00001-14-1 loss: 0.411071  [   64/  265]
train() client id: f_00001-14-2 loss: 0.358603  [   96/  265]
train() client id: f_00001-14-3 loss: 0.241295  [  128/  265]
train() client id: f_00001-14-4 loss: 0.365045  [  160/  265]
train() client id: f_00001-14-5 loss: 0.395472  [  192/  265]
train() client id: f_00001-14-6 loss: 0.263311  [  224/  265]
train() client id: f_00001-14-7 loss: 0.356804  [  256/  265]
train() client id: f_00002-0-0 loss: 1.170993  [   32/  124]
train() client id: f_00002-0-1 loss: 1.174876  [   64/  124]
train() client id: f_00002-0-2 loss: 1.167337  [   96/  124]
train() client id: f_00002-1-0 loss: 1.291301  [   32/  124]
train() client id: f_00002-1-1 loss: 0.942503  [   64/  124]
train() client id: f_00002-1-2 loss: 1.183858  [   96/  124]
train() client id: f_00002-2-0 loss: 1.309972  [   32/  124]
train() client id: f_00002-2-1 loss: 1.074742  [   64/  124]
train() client id: f_00002-2-2 loss: 1.094987  [   96/  124]
train() client id: f_00002-3-0 loss: 1.022505  [   32/  124]
train() client id: f_00002-3-1 loss: 1.036505  [   64/  124]
train() client id: f_00002-3-2 loss: 1.047769  [   96/  124]
train() client id: f_00002-4-0 loss: 1.186608  [   32/  124]
train() client id: f_00002-4-1 loss: 0.896524  [   64/  124]
train() client id: f_00002-4-2 loss: 1.064956  [   96/  124]
train() client id: f_00002-5-0 loss: 1.145203  [   32/  124]
train() client id: f_00002-5-1 loss: 1.155336  [   64/  124]
train() client id: f_00002-5-2 loss: 0.993925  [   96/  124]
train() client id: f_00002-6-0 loss: 1.093671  [   32/  124]
train() client id: f_00002-6-1 loss: 1.055137  [   64/  124]
train() client id: f_00002-6-2 loss: 1.000736  [   96/  124]
train() client id: f_00002-7-0 loss: 0.986114  [   32/  124]
train() client id: f_00002-7-1 loss: 1.172254  [   64/  124]
train() client id: f_00002-7-2 loss: 0.966270  [   96/  124]
train() client id: f_00002-8-0 loss: 0.908282  [   32/  124]
train() client id: f_00002-8-1 loss: 1.181254  [   64/  124]
train() client id: f_00002-8-2 loss: 0.990265  [   96/  124]
train() client id: f_00002-9-0 loss: 1.076196  [   32/  124]
train() client id: f_00002-9-1 loss: 1.119778  [   64/  124]
train() client id: f_00002-9-2 loss: 1.033088  [   96/  124]
train() client id: f_00002-10-0 loss: 1.051281  [   32/  124]
train() client id: f_00002-10-1 loss: 1.111699  [   64/  124]
train() client id: f_00002-10-2 loss: 0.939420  [   96/  124]
train() client id: f_00002-11-0 loss: 0.941171  [   32/  124]
train() client id: f_00002-11-1 loss: 0.976436  [   64/  124]
train() client id: f_00002-11-2 loss: 0.998703  [   96/  124]
train() client id: f_00002-12-0 loss: 0.869684  [   32/  124]
train() client id: f_00002-12-1 loss: 1.203606  [   64/  124]
train() client id: f_00002-12-2 loss: 0.995412  [   96/  124]
train() client id: f_00002-13-0 loss: 0.993576  [   32/  124]
train() client id: f_00002-13-1 loss: 1.000816  [   64/  124]
train() client id: f_00002-13-2 loss: 0.980213  [   96/  124]
train() client id: f_00002-14-0 loss: 0.962281  [   32/  124]
train() client id: f_00002-14-1 loss: 1.106758  [   64/  124]
train() client id: f_00002-14-2 loss: 1.033787  [   96/  124]
train() client id: f_00003-0-0 loss: 0.813144  [   32/   43]
train() client id: f_00003-1-0 loss: 0.657989  [   32/   43]
train() client id: f_00003-2-0 loss: 0.842353  [   32/   43]
train() client id: f_00003-3-0 loss: 0.477320  [   32/   43]
train() client id: f_00003-4-0 loss: 0.873148  [   32/   43]
train() client id: f_00003-5-0 loss: 0.777721  [   32/   43]
train() client id: f_00003-6-0 loss: 0.859344  [   32/   43]
train() client id: f_00003-7-0 loss: 0.858299  [   32/   43]
train() client id: f_00003-8-0 loss: 0.797375  [   32/   43]
train() client id: f_00003-9-0 loss: 0.676106  [   32/   43]
train() client id: f_00003-10-0 loss: 0.655830  [   32/   43]
train() client id: f_00003-11-0 loss: 0.769602  [   32/   43]
train() client id: f_00003-12-0 loss: 0.720496  [   32/   43]
train() client id: f_00003-13-0 loss: 0.761769  [   32/   43]
train() client id: f_00003-14-0 loss: 0.713387  [   32/   43]
train() client id: f_00004-0-0 loss: 0.772132  [   32/  306]
train() client id: f_00004-0-1 loss: 1.004552  [   64/  306]
train() client id: f_00004-0-2 loss: 0.765451  [   96/  306]
train() client id: f_00004-0-3 loss: 0.843547  [  128/  306]
train() client id: f_00004-0-4 loss: 0.963384  [  160/  306]
train() client id: f_00004-0-5 loss: 0.859354  [  192/  306]
train() client id: f_00004-0-6 loss: 0.895204  [  224/  306]
train() client id: f_00004-0-7 loss: 0.840385  [  256/  306]
train() client id: f_00004-0-8 loss: 0.984559  [  288/  306]
train() client id: f_00004-1-0 loss: 1.008077  [   32/  306]
train() client id: f_00004-1-1 loss: 0.670154  [   64/  306]
train() client id: f_00004-1-2 loss: 0.906518  [   96/  306]
train() client id: f_00004-1-3 loss: 0.873439  [  128/  306]
train() client id: f_00004-1-4 loss: 0.817644  [  160/  306]
train() client id: f_00004-1-5 loss: 0.981325  [  192/  306]
train() client id: f_00004-1-6 loss: 0.711904  [  224/  306]
train() client id: f_00004-1-7 loss: 0.841487  [  256/  306]
train() client id: f_00004-1-8 loss: 0.947169  [  288/  306]
train() client id: f_00004-2-0 loss: 0.859676  [   32/  306]
train() client id: f_00004-2-1 loss: 0.907676  [   64/  306]
train() client id: f_00004-2-2 loss: 0.734202  [   96/  306]
train() client id: f_00004-2-3 loss: 0.769899  [  128/  306]
train() client id: f_00004-2-4 loss: 0.969259  [  160/  306]
train() client id: f_00004-2-5 loss: 0.813833  [  192/  306]
train() client id: f_00004-2-6 loss: 0.830881  [  224/  306]
train() client id: f_00004-2-7 loss: 0.874655  [  256/  306]
train() client id: f_00004-2-8 loss: 0.987859  [  288/  306]
train() client id: f_00004-3-0 loss: 0.879708  [   32/  306]
train() client id: f_00004-3-1 loss: 0.869676  [   64/  306]
train() client id: f_00004-3-2 loss: 0.924937  [   96/  306]
train() client id: f_00004-3-3 loss: 0.838458  [  128/  306]
train() client id: f_00004-3-4 loss: 0.865262  [  160/  306]
train() client id: f_00004-3-5 loss: 0.848885  [  192/  306]
train() client id: f_00004-3-6 loss: 0.908057  [  224/  306]
train() client id: f_00004-3-7 loss: 0.766338  [  256/  306]
train() client id: f_00004-3-8 loss: 0.785537  [  288/  306]
train() client id: f_00004-4-0 loss: 0.976793  [   32/  306]
train() client id: f_00004-4-1 loss: 0.807548  [   64/  306]
train() client id: f_00004-4-2 loss: 0.961829  [   96/  306]
train() client id: f_00004-4-3 loss: 0.822782  [  128/  306]
train() client id: f_00004-4-4 loss: 0.877358  [  160/  306]
train() client id: f_00004-4-5 loss: 0.809602  [  192/  306]
train() client id: f_00004-4-6 loss: 0.734007  [  224/  306]
train() client id: f_00004-4-7 loss: 0.819024  [  256/  306]
train() client id: f_00004-4-8 loss: 0.899590  [  288/  306]
train() client id: f_00004-5-0 loss: 0.792569  [   32/  306]
train() client id: f_00004-5-1 loss: 0.750647  [   64/  306]
train() client id: f_00004-5-2 loss: 0.913847  [   96/  306]
train() client id: f_00004-5-3 loss: 0.860103  [  128/  306]
train() client id: f_00004-5-4 loss: 0.859701  [  160/  306]
train() client id: f_00004-5-5 loss: 0.861950  [  192/  306]
train() client id: f_00004-5-6 loss: 1.027811  [  224/  306]
train() client id: f_00004-5-7 loss: 0.890945  [  256/  306]
train() client id: f_00004-5-8 loss: 0.769341  [  288/  306]
train() client id: f_00004-6-0 loss: 0.925829  [   32/  306]
train() client id: f_00004-6-1 loss: 0.838230  [   64/  306]
train() client id: f_00004-6-2 loss: 0.836740  [   96/  306]
train() client id: f_00004-6-3 loss: 0.887293  [  128/  306]
train() client id: f_00004-6-4 loss: 0.795035  [  160/  306]
train() client id: f_00004-6-5 loss: 0.732815  [  192/  306]
train() client id: f_00004-6-6 loss: 1.072798  [  224/  306]
train() client id: f_00004-6-7 loss: 0.647630  [  256/  306]
train() client id: f_00004-6-8 loss: 0.867477  [  288/  306]
train() client id: f_00004-7-0 loss: 0.914926  [   32/  306]
train() client id: f_00004-7-1 loss: 0.984776  [   64/  306]
train() client id: f_00004-7-2 loss: 0.758689  [   96/  306]
train() client id: f_00004-7-3 loss: 0.835418  [  128/  306]
train() client id: f_00004-7-4 loss: 0.861798  [  160/  306]
train() client id: f_00004-7-5 loss: 0.717266  [  192/  306]
train() client id: f_00004-7-6 loss: 0.880004  [  224/  306]
train() client id: f_00004-7-7 loss: 0.865300  [  256/  306]
train() client id: f_00004-7-8 loss: 0.745633  [  288/  306]
train() client id: f_00004-8-0 loss: 0.789090  [   32/  306]
train() client id: f_00004-8-1 loss: 0.895086  [   64/  306]
train() client id: f_00004-8-2 loss: 0.853416  [   96/  306]
train() client id: f_00004-8-3 loss: 0.876848  [  128/  306]
train() client id: f_00004-8-4 loss: 0.755625  [  160/  306]
train() client id: f_00004-8-5 loss: 0.717963  [  192/  306]
train() client id: f_00004-8-6 loss: 0.834972  [  224/  306]
train() client id: f_00004-8-7 loss: 0.825758  [  256/  306]
train() client id: f_00004-8-8 loss: 0.984186  [  288/  306]
train() client id: f_00004-9-0 loss: 0.950190  [   32/  306]
train() client id: f_00004-9-1 loss: 0.937186  [   64/  306]
train() client id: f_00004-9-2 loss: 0.822800  [   96/  306]
train() client id: f_00004-9-3 loss: 0.850407  [  128/  306]
train() client id: f_00004-9-4 loss: 0.728308  [  160/  306]
train() client id: f_00004-9-5 loss: 0.814316  [  192/  306]
train() client id: f_00004-9-6 loss: 0.810982  [  224/  306]
train() client id: f_00004-9-7 loss: 0.751027  [  256/  306]
train() client id: f_00004-9-8 loss: 0.744773  [  288/  306]
train() client id: f_00004-10-0 loss: 0.742452  [   32/  306]
train() client id: f_00004-10-1 loss: 0.781279  [   64/  306]
train() client id: f_00004-10-2 loss: 0.756703  [   96/  306]
train() client id: f_00004-10-3 loss: 0.861117  [  128/  306]
train() client id: f_00004-10-4 loss: 0.766898  [  160/  306]
train() client id: f_00004-10-5 loss: 0.898354  [  192/  306]
train() client id: f_00004-10-6 loss: 0.812732  [  224/  306]
train() client id: f_00004-10-7 loss: 0.835247  [  256/  306]
train() client id: f_00004-10-8 loss: 1.032152  [  288/  306]
train() client id: f_00004-11-0 loss: 0.881306  [   32/  306]
train() client id: f_00004-11-1 loss: 0.726073  [   64/  306]
train() client id: f_00004-11-2 loss: 0.778058  [   96/  306]
train() client id: f_00004-11-3 loss: 0.817540  [  128/  306]
train() client id: f_00004-11-4 loss: 0.860255  [  160/  306]
train() client id: f_00004-11-5 loss: 0.751676  [  192/  306]
train() client id: f_00004-11-6 loss: 0.797869  [  224/  306]
train() client id: f_00004-11-7 loss: 0.901014  [  256/  306]
train() client id: f_00004-11-8 loss: 0.939125  [  288/  306]
train() client id: f_00004-12-0 loss: 0.848449  [   32/  306]
train() client id: f_00004-12-1 loss: 0.790371  [   64/  306]
train() client id: f_00004-12-2 loss: 0.756066  [   96/  306]
train() client id: f_00004-12-3 loss: 0.832086  [  128/  306]
train() client id: f_00004-12-4 loss: 1.057357  [  160/  306]
train() client id: f_00004-12-5 loss: 0.755208  [  192/  306]
train() client id: f_00004-12-6 loss: 0.846406  [  224/  306]
train() client id: f_00004-12-7 loss: 0.814853  [  256/  306]
train() client id: f_00004-12-8 loss: 0.824319  [  288/  306]
train() client id: f_00004-13-0 loss: 0.829201  [   32/  306]
train() client id: f_00004-13-1 loss: 0.766490  [   64/  306]
train() client id: f_00004-13-2 loss: 0.867488  [   96/  306]
train() client id: f_00004-13-3 loss: 0.899960  [  128/  306]
train() client id: f_00004-13-4 loss: 0.795161  [  160/  306]
train() client id: f_00004-13-5 loss: 0.849203  [  192/  306]
train() client id: f_00004-13-6 loss: 0.862280  [  224/  306]
train() client id: f_00004-13-7 loss: 0.862131  [  256/  306]
train() client id: f_00004-13-8 loss: 0.745655  [  288/  306]
train() client id: f_00004-14-0 loss: 0.804595  [   32/  306]
train() client id: f_00004-14-1 loss: 0.707383  [   64/  306]
train() client id: f_00004-14-2 loss: 0.945083  [   96/  306]
train() client id: f_00004-14-3 loss: 0.733537  [  128/  306]
train() client id: f_00004-14-4 loss: 0.875261  [  160/  306]
train() client id: f_00004-14-5 loss: 0.781149  [  192/  306]
train() client id: f_00004-14-6 loss: 0.765283  [  224/  306]
train() client id: f_00004-14-7 loss: 0.990438  [  256/  306]
train() client id: f_00004-14-8 loss: 0.897440  [  288/  306]
train() client id: f_00005-0-0 loss: 0.935651  [   32/  146]
train() client id: f_00005-0-1 loss: 0.581789  [   64/  146]
train() client id: f_00005-0-2 loss: 0.809660  [   96/  146]
train() client id: f_00005-0-3 loss: 0.463127  [  128/  146]
train() client id: f_00005-1-0 loss: 0.719241  [   32/  146]
train() client id: f_00005-1-1 loss: 0.959193  [   64/  146]
train() client id: f_00005-1-2 loss: 0.596353  [   96/  146]
train() client id: f_00005-1-3 loss: 0.672807  [  128/  146]
train() client id: f_00005-2-0 loss: 0.783764  [   32/  146]
train() client id: f_00005-2-1 loss: 0.872185  [   64/  146]
train() client id: f_00005-2-2 loss: 0.618511  [   96/  146]
train() client id: f_00005-2-3 loss: 0.496254  [  128/  146]
train() client id: f_00005-3-0 loss: 0.940339  [   32/  146]
train() client id: f_00005-3-1 loss: 0.704201  [   64/  146]
train() client id: f_00005-3-2 loss: 0.561789  [   96/  146]
train() client id: f_00005-3-3 loss: 0.612719  [  128/  146]
train() client id: f_00005-4-0 loss: 0.523755  [   32/  146]
train() client id: f_00005-4-1 loss: 0.664668  [   64/  146]
train() client id: f_00005-4-2 loss: 0.818883  [   96/  146]
train() client id: f_00005-4-3 loss: 0.834986  [  128/  146]
train() client id: f_00005-5-0 loss: 0.932180  [   32/  146]
train() client id: f_00005-5-1 loss: 0.638688  [   64/  146]
train() client id: f_00005-5-2 loss: 0.816665  [   96/  146]
train() client id: f_00005-5-3 loss: 0.560451  [  128/  146]
train() client id: f_00005-6-0 loss: 0.674659  [   32/  146]
train() client id: f_00005-6-1 loss: 0.823830  [   64/  146]
train() client id: f_00005-6-2 loss: 0.680299  [   96/  146]
train() client id: f_00005-6-3 loss: 0.606169  [  128/  146]
train() client id: f_00005-7-0 loss: 0.772011  [   32/  146]
train() client id: f_00005-7-1 loss: 0.870267  [   64/  146]
train() client id: f_00005-7-2 loss: 0.617840  [   96/  146]
train() client id: f_00005-7-3 loss: 0.613030  [  128/  146]
train() client id: f_00005-8-0 loss: 0.615297  [   32/  146]
train() client id: f_00005-8-1 loss: 0.715739  [   64/  146]
train() client id: f_00005-8-2 loss: 0.626898  [   96/  146]
train() client id: f_00005-8-3 loss: 0.874083  [  128/  146]
train() client id: f_00005-9-0 loss: 0.473688  [   32/  146]
train() client id: f_00005-9-1 loss: 0.692173  [   64/  146]
train() client id: f_00005-9-2 loss: 0.744555  [   96/  146]
train() client id: f_00005-9-3 loss: 1.064067  [  128/  146]
train() client id: f_00005-10-0 loss: 0.685254  [   32/  146]
train() client id: f_00005-10-1 loss: 0.833264  [   64/  146]
train() client id: f_00005-10-2 loss: 0.617197  [   96/  146]
train() client id: f_00005-10-3 loss: 0.739913  [  128/  146]
train() client id: f_00005-11-0 loss: 0.719672  [   32/  146]
train() client id: f_00005-11-1 loss: 0.696013  [   64/  146]
train() client id: f_00005-11-2 loss: 0.700228  [   96/  146]
train() client id: f_00005-11-3 loss: 0.575996  [  128/  146]
train() client id: f_00005-12-0 loss: 0.474298  [   32/  146]
train() client id: f_00005-12-1 loss: 0.832792  [   64/  146]
train() client id: f_00005-12-2 loss: 0.893820  [   96/  146]
train() client id: f_00005-12-3 loss: 0.736073  [  128/  146]
train() client id: f_00005-13-0 loss: 1.024819  [   32/  146]
train() client id: f_00005-13-1 loss: 0.466628  [   64/  146]
train() client id: f_00005-13-2 loss: 0.851754  [   96/  146]
train() client id: f_00005-13-3 loss: 0.652417  [  128/  146]
train() client id: f_00005-14-0 loss: 0.657143  [   32/  146]
train() client id: f_00005-14-1 loss: 0.698767  [   64/  146]
train() client id: f_00005-14-2 loss: 0.606107  [   96/  146]
train() client id: f_00005-14-3 loss: 0.875639  [  128/  146]
train() client id: f_00006-0-0 loss: 0.559846  [   32/   54]
train() client id: f_00006-1-0 loss: 0.506307  [   32/   54]
train() client id: f_00006-2-0 loss: 0.577317  [   32/   54]
train() client id: f_00006-3-0 loss: 0.580423  [   32/   54]
train() client id: f_00006-4-0 loss: 0.467887  [   32/   54]
train() client id: f_00006-5-0 loss: 0.503391  [   32/   54]
train() client id: f_00006-6-0 loss: 0.455361  [   32/   54]
train() client id: f_00006-7-0 loss: 0.474638  [   32/   54]
train() client id: f_00006-8-0 loss: 0.499109  [   32/   54]
train() client id: f_00006-9-0 loss: 0.463156  [   32/   54]
train() client id: f_00006-10-0 loss: 0.497831  [   32/   54]
train() client id: f_00006-11-0 loss: 0.519409  [   32/   54]
train() client id: f_00006-12-0 loss: 0.470983  [   32/   54]
train() client id: f_00006-13-0 loss: 0.511040  [   32/   54]
train() client id: f_00006-14-0 loss: 0.568905  [   32/   54]
train() client id: f_00007-0-0 loss: 0.597980  [   32/  179]
train() client id: f_00007-0-1 loss: 0.622165  [   64/  179]
train() client id: f_00007-0-2 loss: 0.603384  [   96/  179]
train() client id: f_00007-0-3 loss: 0.683758  [  128/  179]
train() client id: f_00007-0-4 loss: 0.491624  [  160/  179]
train() client id: f_00007-1-0 loss: 0.649685  [   32/  179]
train() client id: f_00007-1-1 loss: 0.607445  [   64/  179]
train() client id: f_00007-1-2 loss: 0.709873  [   96/  179]
train() client id: f_00007-1-3 loss: 0.449878  [  128/  179]
train() client id: f_00007-1-4 loss: 0.464401  [  160/  179]
train() client id: f_00007-2-0 loss: 0.431065  [   32/  179]
train() client id: f_00007-2-1 loss: 0.451190  [   64/  179]
train() client id: f_00007-2-2 loss: 0.815123  [   96/  179]
train() client id: f_00007-2-3 loss: 0.472570  [  128/  179]
train() client id: f_00007-2-4 loss: 0.609665  [  160/  179]
train() client id: f_00007-3-0 loss: 0.564187  [   32/  179]
train() client id: f_00007-3-1 loss: 0.398906  [   64/  179]
train() client id: f_00007-3-2 loss: 0.672325  [   96/  179]
train() client id: f_00007-3-3 loss: 0.700618  [  128/  179]
train() client id: f_00007-3-4 loss: 0.514565  [  160/  179]
train() client id: f_00007-4-0 loss: 0.813243  [   32/  179]
train() client id: f_00007-4-1 loss: 0.398110  [   64/  179]
train() client id: f_00007-4-2 loss: 0.491237  [   96/  179]
train() client id: f_00007-4-3 loss: 0.401951  [  128/  179]
train() client id: f_00007-4-4 loss: 0.741966  [  160/  179]
train() client id: f_00007-5-0 loss: 0.578756  [   32/  179]
train() client id: f_00007-5-1 loss: 0.616646  [   64/  179]
train() client id: f_00007-5-2 loss: 0.507789  [   96/  179]
train() client id: f_00007-5-3 loss: 0.433303  [  128/  179]
train() client id: f_00007-5-4 loss: 0.592959  [  160/  179]
train() client id: f_00007-6-0 loss: 0.396806  [   32/  179]
train() client id: f_00007-6-1 loss: 0.616965  [   64/  179]
train() client id: f_00007-6-2 loss: 0.691618  [   96/  179]
train() client id: f_00007-6-3 loss: 0.625500  [  128/  179]
train() client id: f_00007-6-4 loss: 0.411123  [  160/  179]
train() client id: f_00007-7-0 loss: 0.578025  [   32/  179]
train() client id: f_00007-7-1 loss: 0.581776  [   64/  179]
train() client id: f_00007-7-2 loss: 0.502021  [   96/  179]
train() client id: f_00007-7-3 loss: 0.528098  [  128/  179]
train() client id: f_00007-7-4 loss: 0.501025  [  160/  179]
train() client id: f_00007-8-0 loss: 0.499318  [   32/  179]
train() client id: f_00007-8-1 loss: 0.638532  [   64/  179]
train() client id: f_00007-8-2 loss: 0.645697  [   96/  179]
train() client id: f_00007-8-3 loss: 0.401311  [  128/  179]
train() client id: f_00007-8-4 loss: 0.578527  [  160/  179]
train() client id: f_00007-9-0 loss: 0.482253  [   32/  179]
train() client id: f_00007-9-1 loss: 0.490809  [   64/  179]
train() client id: f_00007-9-2 loss: 0.624536  [   96/  179]
train() client id: f_00007-9-3 loss: 0.482894  [  128/  179]
train() client id: f_00007-9-4 loss: 0.489343  [  160/  179]
train() client id: f_00007-10-0 loss: 0.523965  [   32/  179]
train() client id: f_00007-10-1 loss: 0.632237  [   64/  179]
train() client id: f_00007-10-2 loss: 0.645529  [   96/  179]
train() client id: f_00007-10-3 loss: 0.379598  [  128/  179]
train() client id: f_00007-10-4 loss: 0.491190  [  160/  179]
train() client id: f_00007-11-0 loss: 0.781287  [   32/  179]
train() client id: f_00007-11-1 loss: 0.614147  [   64/  179]
train() client id: f_00007-11-2 loss: 0.497479  [   96/  179]
train() client id: f_00007-11-3 loss: 0.374398  [  128/  179]
train() client id: f_00007-11-4 loss: 0.466414  [  160/  179]
train() client id: f_00007-12-0 loss: 0.873505  [   32/  179]
train() client id: f_00007-12-1 loss: 0.422796  [   64/  179]
train() client id: f_00007-12-2 loss: 0.485358  [   96/  179]
train() client id: f_00007-12-3 loss: 0.464367  [  128/  179]
train() client id: f_00007-12-4 loss: 0.370201  [  160/  179]
train() client id: f_00007-13-0 loss: 0.395399  [   32/  179]
train() client id: f_00007-13-1 loss: 0.582185  [   64/  179]
train() client id: f_00007-13-2 loss: 0.722393  [   96/  179]
train() client id: f_00007-13-3 loss: 0.512186  [  128/  179]
train() client id: f_00007-13-4 loss: 0.397221  [  160/  179]
train() client id: f_00007-14-0 loss: 0.574269  [   32/  179]
train() client id: f_00007-14-1 loss: 0.485165  [   64/  179]
train() client id: f_00007-14-2 loss: 0.527662  [   96/  179]
train() client id: f_00007-14-3 loss: 0.632486  [  128/  179]
train() client id: f_00007-14-4 loss: 0.350933  [  160/  179]
train() client id: f_00008-0-0 loss: 0.682566  [   32/  130]
train() client id: f_00008-0-1 loss: 0.731510  [   64/  130]
train() client id: f_00008-0-2 loss: 0.773989  [   96/  130]
train() client id: f_00008-0-3 loss: 0.683561  [  128/  130]
train() client id: f_00008-1-0 loss: 0.765300  [   32/  130]
train() client id: f_00008-1-1 loss: 0.629341  [   64/  130]
train() client id: f_00008-1-2 loss: 0.753382  [   96/  130]
train() client id: f_00008-1-3 loss: 0.736948  [  128/  130]
train() client id: f_00008-2-0 loss: 0.691926  [   32/  130]
train() client id: f_00008-2-1 loss: 0.685700  [   64/  130]
train() client id: f_00008-2-2 loss: 0.825733  [   96/  130]
train() client id: f_00008-2-3 loss: 0.672676  [  128/  130]
train() client id: f_00008-3-0 loss: 0.682539  [   32/  130]
train() client id: f_00008-3-1 loss: 0.705073  [   64/  130]
train() client id: f_00008-3-2 loss: 0.681182  [   96/  130]
train() client id: f_00008-3-3 loss: 0.767933  [  128/  130]
train() client id: f_00008-4-0 loss: 0.660896  [   32/  130]
train() client id: f_00008-4-1 loss: 0.806209  [   64/  130]
train() client id: f_00008-4-2 loss: 0.697987  [   96/  130]
train() client id: f_00008-4-3 loss: 0.706239  [  128/  130]
train() client id: f_00008-5-0 loss: 0.658103  [   32/  130]
train() client id: f_00008-5-1 loss: 0.744452  [   64/  130]
train() client id: f_00008-5-2 loss: 0.650564  [   96/  130]
train() client id: f_00008-5-3 loss: 0.818752  [  128/  130]
train() client id: f_00008-6-0 loss: 0.716561  [   32/  130]
train() client id: f_00008-6-1 loss: 0.701595  [   64/  130]
train() client id: f_00008-6-2 loss: 0.737656  [   96/  130]
train() client id: f_00008-6-3 loss: 0.724189  [  128/  130]
train() client id: f_00008-7-0 loss: 0.589313  [   32/  130]
train() client id: f_00008-7-1 loss: 0.640512  [   64/  130]
train() client id: f_00008-7-2 loss: 0.912294  [   96/  130]
train() client id: f_00008-7-3 loss: 0.748104  [  128/  130]
train() client id: f_00008-8-0 loss: 0.751017  [   32/  130]
train() client id: f_00008-8-1 loss: 0.735924  [   64/  130]
train() client id: f_00008-8-2 loss: 0.734105  [   96/  130]
train() client id: f_00008-8-3 loss: 0.652280  [  128/  130]
train() client id: f_00008-9-0 loss: 0.669542  [   32/  130]
train() client id: f_00008-9-1 loss: 0.704101  [   64/  130]
train() client id: f_00008-9-2 loss: 0.749847  [   96/  130]
train() client id: f_00008-9-3 loss: 0.753660  [  128/  130]
train() client id: f_00008-10-0 loss: 0.650616  [   32/  130]
train() client id: f_00008-10-1 loss: 0.776997  [   64/  130]
train() client id: f_00008-10-2 loss: 0.773817  [   96/  130]
train() client id: f_00008-10-3 loss: 0.668825  [  128/  130]
train() client id: f_00008-11-0 loss: 0.559151  [   32/  130]
train() client id: f_00008-11-1 loss: 0.755604  [   64/  130]
train() client id: f_00008-11-2 loss: 0.706791  [   96/  130]
train() client id: f_00008-11-3 loss: 0.826326  [  128/  130]
train() client id: f_00008-12-0 loss: 0.736650  [   32/  130]
train() client id: f_00008-12-1 loss: 0.755901  [   64/  130]
train() client id: f_00008-12-2 loss: 0.583377  [   96/  130]
train() client id: f_00008-12-3 loss: 0.773842  [  128/  130]
train() client id: f_00008-13-0 loss: 0.760951  [   32/  130]
train() client id: f_00008-13-1 loss: 0.712228  [   64/  130]
train() client id: f_00008-13-2 loss: 0.686325  [   96/  130]
train() client id: f_00008-13-3 loss: 0.655661  [  128/  130]
train() client id: f_00008-14-0 loss: 0.813423  [   32/  130]
train() client id: f_00008-14-1 loss: 0.675427  [   64/  130]
train() client id: f_00008-14-2 loss: 0.666813  [   96/  130]
train() client id: f_00008-14-3 loss: 0.706175  [  128/  130]
train() client id: f_00009-0-0 loss: 1.145715  [   32/  118]
train() client id: f_00009-0-1 loss: 1.194709  [   64/  118]
train() client id: f_00009-0-2 loss: 1.147719  [   96/  118]
train() client id: f_00009-1-0 loss: 1.109591  [   32/  118]
train() client id: f_00009-1-1 loss: 1.113870  [   64/  118]
train() client id: f_00009-1-2 loss: 1.006355  [   96/  118]
train() client id: f_00009-2-0 loss: 1.031015  [   32/  118]
train() client id: f_00009-2-1 loss: 1.012142  [   64/  118]
train() client id: f_00009-2-2 loss: 1.079646  [   96/  118]
train() client id: f_00009-3-0 loss: 0.928511  [   32/  118]
train() client id: f_00009-3-1 loss: 1.075876  [   64/  118]
train() client id: f_00009-3-2 loss: 1.037915  [   96/  118]
train() client id: f_00009-4-0 loss: 0.874578  [   32/  118]
train() client id: f_00009-4-1 loss: 0.975550  [   64/  118]
train() client id: f_00009-4-2 loss: 1.027092  [   96/  118]
train() client id: f_00009-5-0 loss: 1.166437  [   32/  118]
train() client id: f_00009-5-1 loss: 0.896139  [   64/  118]
train() client id: f_00009-5-2 loss: 0.865956  [   96/  118]
train() client id: f_00009-6-0 loss: 0.889018  [   32/  118]
train() client id: f_00009-6-1 loss: 1.045531  [   64/  118]
train() client id: f_00009-6-2 loss: 0.750105  [   96/  118]
train() client id: f_00009-7-0 loss: 0.961542  [   32/  118]
train() client id: f_00009-7-1 loss: 0.885061  [   64/  118]
train() client id: f_00009-7-2 loss: 0.810188  [   96/  118]
train() client id: f_00009-8-0 loss: 0.837610  [   32/  118]
train() client id: f_00009-8-1 loss: 0.957341  [   64/  118]
train() client id: f_00009-8-2 loss: 0.792568  [   96/  118]
train() client id: f_00009-9-0 loss: 0.752303  [   32/  118]
train() client id: f_00009-9-1 loss: 0.860913  [   64/  118]
train() client id: f_00009-9-2 loss: 0.916863  [   96/  118]
train() client id: f_00009-10-0 loss: 0.759071  [   32/  118]
train() client id: f_00009-10-1 loss: 0.874850  [   64/  118]
train() client id: f_00009-10-2 loss: 0.791894  [   96/  118]
train() client id: f_00009-11-0 loss: 0.771144  [   32/  118]
train() client id: f_00009-11-1 loss: 0.903493  [   64/  118]
train() client id: f_00009-11-2 loss: 0.808022  [   96/  118]
train() client id: f_00009-12-0 loss: 0.833166  [   32/  118]
train() client id: f_00009-12-1 loss: 0.818872  [   64/  118]
train() client id: f_00009-12-2 loss: 0.759875  [   96/  118]
train() client id: f_00009-13-0 loss: 0.701325  [   32/  118]
train() client id: f_00009-13-1 loss: 0.932110  [   64/  118]
train() client id: f_00009-13-2 loss: 0.826305  [   96/  118]
train() client id: f_00009-14-0 loss: 0.774516  [   32/  118]
train() client id: f_00009-14-1 loss: 0.788136  [   64/  118]
train() client id: f_00009-14-2 loss: 0.858474  [   96/  118]
At round 39 accuracy: 0.6445623342175066
At round 39 training accuracy: 0.5881958417169685
At round 39 training loss: 0.8437094493180499
update_location
xs = 8.927491 316.223621 5.882650 0.934260 -232.581990 -80.230757 -40.849135 -5.143845 -255.120581 20.134486 
ys = -307.390647 7.291448 205.684448 -27.290817 -9.642386 0.794442 -1.381692 201.628436 25.881276 -742.232496 
xs mean: -26.182379970521218
ys mean: -64.66579882624052
dists_uav = 323.370855 331.738668 228.780895 103.661283 253.352240 128.209226 108.030370 225.123267 275.238717 749.209233 
uav_gains = -117.523419 -118.085389 -109.613929 -100.390441 -111.583729 -102.698522 -100.838695 -109.349405 -113.547524 -128.826250 
uav_gains_db_mean: -111.2457304173324
dists_bs = 516.265492 519.063391 183.466805 268.094085 193.412762 198.314865 221.600894 171.930762 169.279949 937.759521 
bs_gains = -115.527506 -115.593230 -102.946669 -107.559102 -103.588643 -103.893007 -105.243061 -102.156960 -101.968014 -122.785621 
bs_gains_db_mean: -108.12618116453098
Round 40
-------------------------------
ene_coms = [0.02060056 0.02195435 0.00717894 0.00640622 0.00740262 0.00709599
 0.00652963 0.00692134 0.00686238 0.04145674]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.85971135 12.01138381  5.57516444  1.99061356 13.60721632  6.54446304
  2.47767465  7.99767627  5.83521922  5.81186218]
obj_prev = 67.71098484975822
eta_min = 7.990857297191186e-17	eta_max = 0.7828618617879196
af = 14.139867113019362	bf = 1.9173805385503793	zeta = 15.5538538243213	eta = 0.909090909090909
af = 14.139867113019362	bf = 1.9173805385503793	zeta = 35.915591970516616	eta = 0.3936971754392044
af = 14.139867113019362	bf = 1.9173805385503793	zeta = 24.902055351563654	eta = 0.5678192789066903
af = 14.139867113019362	bf = 1.9173805385503793	zeta = 22.95305478417999	eta = 0.6160342161848117
af = 14.139867113019362	bf = 1.9173805385503793	zeta = 22.834009476716105	eta = 0.6192459159412113
af = 14.139867113019362	bf = 1.9173805385503793	zeta = 22.833508267290995	eta = 0.6192595087665405
af = 14.139867113019362	bf = 1.9173805385503793	zeta = 22.833508258344217	eta = 0.6192595090091828
eta = 0.6192595090091828
ene_coms = [0.02060056 0.02195435 0.00717894 0.00640622 0.00740262 0.00709599
 0.00652963 0.00692134 0.00686238 0.04145674]
ene_comp = [0.03954498 0.08317    0.03891728 0.01349551 0.09603781 0.04582196
 0.01694785 0.05617898 0.04080038 0.03703419]
ene_total = [2.28752563 3.99821251 1.75318551 0.75692579 3.93416776 2.01263758
 0.89292297 2.39990547 1.81276593 2.9852591 ]
ti_comp = [0.52920973 0.51567182 0.66342592 0.67115312 0.66118911 0.6642554
 0.669919   0.66600188 0.66659148 0.32064788]
ti_coms = [0.20600556 0.21954347 0.07178936 0.06406217 0.07402617 0.07095988
 0.06529628 0.06921341 0.0686238  0.4145674 ]
t_total = [27.96257019 27.96257019 27.96257019 27.96257019 27.96257019 27.96257019
 27.96257019 27.96257019 27.96257019 27.96257019]
ene_coms = [0.02060056 0.02195435 0.00717894 0.00640622 0.00740262 0.00709599
 0.00652963 0.00692134 0.00686238 0.04145674]
ene_comp = [1.38006172e-05 1.35217637e-04 8.36994676e-06 3.41039476e-07
 1.26635481e-04 1.36279612e-05 6.77921175e-07 2.49833513e-05
 9.55329194e-06 3.08767645e-05]
ene_total = [0.78402939 0.84013623 0.27335607 0.24366171 0.28636138 0.27040126
 0.24836827 0.26419074 0.26136144 1.57790573]
optimize_network iter = 0 obj = 5.049772221521822
eta = 0.6192595090091828
freqs = [37362296.51455241 80642375.75941849 29330541.76811203 10053972.15306489
 72625070.8952252  34491223.14762534 12649177.87120335 42176293.58086064
 30603734.70428854 57749000.07856601]
eta_min = 0.6192595090091831	eta_max = 0.619259509009182
af = 0.011001412801664218	bf = 1.9173805385503793	zeta = 0.01210155408183064	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02060056 0.02195435 0.00717894 0.00640622 0.00740262 0.00709599
 0.00652963 0.00692134 0.00686238 0.04145674]
ene_comp = [2.74468828e-06 2.68922945e-05 1.66462806e-06 6.78264628e-08
 2.51854620e-05 2.71035017e-06 1.34826020e-07 4.96872787e-06
 1.89997360e-06 6.14081907e-06]
ene_total = [3.28186799 3.50135772 1.14378688 1.02044717 1.18316327 1.13074076
 1.04011593 1.10328113 1.09340064 6.60455846]
ti_comp = [0.52920973 0.51567182 0.66342592 0.67115312 0.66118911 0.6642554
 0.669919   0.66600188 0.66659148 0.32064788]
ti_coms = [0.20600556 0.21954347 0.07178936 0.06406217 0.07402617 0.07095988
 0.06529628 0.06921341 0.0686238  0.4145674 ]
t_total = [27.96257019 27.96257019 27.96257019 27.96257019 27.96257019 27.96257019
 27.96257019 27.96257019 27.96257019 27.96257019]
ene_coms = [0.02060056 0.02195435 0.00717894 0.00640622 0.00740262 0.00709599
 0.00652963 0.00692134 0.00686238 0.04145674]
ene_comp = [1.38006172e-05 1.35217637e-04 8.36994676e-06 3.41039476e-07
 1.26635481e-04 1.36279612e-05 6.77921175e-07 2.49833513e-05
 9.55329194e-06 3.08767645e-05]
ene_total = [0.78402939 0.84013623 0.27335607 0.24366171 0.28636138 0.27040126
 0.24836827 0.26419074 0.26136144 1.57790573]
optimize_network iter = 1 obj = 5.0497722215218115
eta = 0.619259509009182
freqs = [37362296.51455239 80642375.75941846 29330541.76811204 10053972.15306489
 72625070.89522521 34491223.14762535 12649177.87120335 42176293.58086064
 30603734.70428855 57749000.07856589]
Done!
ene_coms = [0.02060056 0.02195435 0.00717894 0.00640622 0.00740262 0.00709599
 0.00652963 0.00692134 0.00686238 0.04145674]
ene_comp = [1.31916443e-05 1.29250956e-04 8.00061033e-06 3.25990598e-07
 1.21047501e-04 1.30266070e-05 6.48006888e-07 2.38809235e-05
 9.13173863e-06 2.95142810e-05]
ene_total = [0.02061375 0.0220836  0.00718694 0.00640654 0.00752366 0.00710901
 0.00653028 0.00694522 0.00687151 0.04148625]
At round 40 energy consumption: 0.13275676958590032
At round 40 eta: 0.619259509009182
At round 40 a_n: 14.480768977481684
At round 40 local rounds: 15.69245297885618
At round 40 global rounds: 38.03317304077046
gradient difference: 0.37032413482666016
train() client id: f_00000-0-0 loss: 1.240211  [   32/  126]
train() client id: f_00000-0-1 loss: 0.820755  [   64/  126]
train() client id: f_00000-0-2 loss: 1.074419  [   96/  126]
train() client id: f_00000-1-0 loss: 0.953683  [   32/  126]
train() client id: f_00000-1-1 loss: 1.001221  [   64/  126]
train() client id: f_00000-1-2 loss: 1.046765  [   96/  126]
train() client id: f_00000-2-0 loss: 0.764021  [   32/  126]
train() client id: f_00000-2-1 loss: 0.884345  [   64/  126]
train() client id: f_00000-2-2 loss: 0.795559  [   96/  126]
train() client id: f_00000-3-0 loss: 0.984959  [   32/  126]
train() client id: f_00000-3-1 loss: 0.831258  [   64/  126]
train() client id: f_00000-3-2 loss: 0.880250  [   96/  126]
train() client id: f_00000-4-0 loss: 0.982084  [   32/  126]
train() client id: f_00000-4-1 loss: 0.677969  [   64/  126]
train() client id: f_00000-4-2 loss: 0.713192  [   96/  126]
train() client id: f_00000-5-0 loss: 0.820312  [   32/  126]
train() client id: f_00000-5-1 loss: 0.883627  [   64/  126]
train() client id: f_00000-5-2 loss: 0.720361  [   96/  126]
train() client id: f_00000-6-0 loss: 0.777604  [   32/  126]
train() client id: f_00000-6-1 loss: 0.775962  [   64/  126]
train() client id: f_00000-6-2 loss: 0.747942  [   96/  126]
train() client id: f_00000-7-0 loss: 0.716987  [   32/  126]
train() client id: f_00000-7-1 loss: 0.816372  [   64/  126]
train() client id: f_00000-7-2 loss: 0.655506  [   96/  126]
train() client id: f_00000-8-0 loss: 0.822671  [   32/  126]
train() client id: f_00000-8-1 loss: 0.664040  [   64/  126]
train() client id: f_00000-8-2 loss: 0.698435  [   96/  126]
train() client id: f_00000-9-0 loss: 0.718141  [   32/  126]
train() client id: f_00000-9-1 loss: 0.725441  [   64/  126]
train() client id: f_00000-9-2 loss: 0.772659  [   96/  126]
train() client id: f_00000-10-0 loss: 0.760421  [   32/  126]
train() client id: f_00000-10-1 loss: 0.711913  [   64/  126]
train() client id: f_00000-10-2 loss: 0.581267  [   96/  126]
train() client id: f_00000-11-0 loss: 0.651494  [   32/  126]
train() client id: f_00000-11-1 loss: 0.729869  [   64/  126]
train() client id: f_00000-11-2 loss: 0.678562  [   96/  126]
train() client id: f_00000-12-0 loss: 0.599855  [   32/  126]
train() client id: f_00000-12-1 loss: 0.611368  [   64/  126]
train() client id: f_00000-12-2 loss: 0.840984  [   96/  126]
train() client id: f_00000-13-0 loss: 0.650628  [   32/  126]
train() client id: f_00000-13-1 loss: 0.788780  [   64/  126]
train() client id: f_00000-13-2 loss: 0.694463  [   96/  126]
train() client id: f_00000-14-0 loss: 0.792162  [   32/  126]
train() client id: f_00000-14-1 loss: 0.566915  [   64/  126]
train() client id: f_00000-14-2 loss: 0.812508  [   96/  126]
train() client id: f_00001-0-0 loss: 0.439870  [   32/  265]
train() client id: f_00001-0-1 loss: 0.349993  [   64/  265]
train() client id: f_00001-0-2 loss: 0.493618  [   96/  265]
train() client id: f_00001-0-3 loss: 0.369901  [  128/  265]
train() client id: f_00001-0-4 loss: 0.293585  [  160/  265]
train() client id: f_00001-0-5 loss: 0.363593  [  192/  265]
train() client id: f_00001-0-6 loss: 0.421983  [  224/  265]
train() client id: f_00001-0-7 loss: 0.368457  [  256/  265]
train() client id: f_00001-1-0 loss: 0.473071  [   32/  265]
train() client id: f_00001-1-1 loss: 0.417140  [   64/  265]
train() client id: f_00001-1-2 loss: 0.383069  [   96/  265]
train() client id: f_00001-1-3 loss: 0.274399  [  128/  265]
train() client id: f_00001-1-4 loss: 0.358040  [  160/  265]
train() client id: f_00001-1-5 loss: 0.388159  [  192/  265]
train() client id: f_00001-1-6 loss: 0.316908  [  224/  265]
train() client id: f_00001-1-7 loss: 0.429718  [  256/  265]
train() client id: f_00001-2-0 loss: 0.526623  [   32/  265]
train() client id: f_00001-2-1 loss: 0.295974  [   64/  265]
train() client id: f_00001-2-2 loss: 0.378733  [   96/  265]
train() client id: f_00001-2-3 loss: 0.317061  [  128/  265]
train() client id: f_00001-2-4 loss: 0.418173  [  160/  265]
train() client id: f_00001-2-5 loss: 0.291830  [  192/  265]
train() client id: f_00001-2-6 loss: 0.406916  [  224/  265]
train() client id: f_00001-2-7 loss: 0.365396  [  256/  265]
train() client id: f_00001-3-0 loss: 0.337388  [   32/  265]
train() client id: f_00001-3-1 loss: 0.306801  [   64/  265]
train() client id: f_00001-3-2 loss: 0.487939  [   96/  265]
train() client id: f_00001-3-3 loss: 0.409510  [  128/  265]
train() client id: f_00001-3-4 loss: 0.301000  [  160/  265]
train() client id: f_00001-3-5 loss: 0.431322  [  192/  265]
train() client id: f_00001-3-6 loss: 0.380146  [  224/  265]
train() client id: f_00001-3-7 loss: 0.277381  [  256/  265]
train() client id: f_00001-4-0 loss: 0.324540  [   32/  265]
train() client id: f_00001-4-1 loss: 0.275252  [   64/  265]
train() client id: f_00001-4-2 loss: 0.381760  [   96/  265]
train() client id: f_00001-4-3 loss: 0.382119  [  128/  265]
train() client id: f_00001-4-4 loss: 0.337360  [  160/  265]
train() client id: f_00001-4-5 loss: 0.310806  [  192/  265]
train() client id: f_00001-4-6 loss: 0.428298  [  224/  265]
train() client id: f_00001-4-7 loss: 0.454033  [  256/  265]
train() client id: f_00001-5-0 loss: 0.361820  [   32/  265]
train() client id: f_00001-5-1 loss: 0.290925  [   64/  265]
train() client id: f_00001-5-2 loss: 0.404871  [   96/  265]
train() client id: f_00001-5-3 loss: 0.354027  [  128/  265]
train() client id: f_00001-5-4 loss: 0.330227  [  160/  265]
train() client id: f_00001-5-5 loss: 0.395335  [  192/  265]
train() client id: f_00001-5-6 loss: 0.341190  [  224/  265]
train() client id: f_00001-5-7 loss: 0.388450  [  256/  265]
train() client id: f_00001-6-0 loss: 0.252225  [   32/  265]
train() client id: f_00001-6-1 loss: 0.356001  [   64/  265]
train() client id: f_00001-6-2 loss: 0.422541  [   96/  265]
train() client id: f_00001-6-3 loss: 0.314524  [  128/  265]
train() client id: f_00001-6-4 loss: 0.324890  [  160/  265]
train() client id: f_00001-6-5 loss: 0.386256  [  192/  265]
train() client id: f_00001-6-6 loss: 0.459150  [  224/  265]
train() client id: f_00001-6-7 loss: 0.308191  [  256/  265]
train() client id: f_00001-7-0 loss: 0.367361  [   32/  265]
train() client id: f_00001-7-1 loss: 0.349253  [   64/  265]
train() client id: f_00001-7-2 loss: 0.274648  [   96/  265]
train() client id: f_00001-7-3 loss: 0.446567  [  128/  265]
train() client id: f_00001-7-4 loss: 0.435213  [  160/  265]
train() client id: f_00001-7-5 loss: 0.314714  [  192/  265]
train() client id: f_00001-7-6 loss: 0.336295  [  224/  265]
train() client id: f_00001-7-7 loss: 0.295639  [  256/  265]
train() client id: f_00001-8-0 loss: 0.500537  [   32/  265]
train() client id: f_00001-8-1 loss: 0.340880  [   64/  265]
train() client id: f_00001-8-2 loss: 0.341121  [   96/  265]
train() client id: f_00001-8-3 loss: 0.313301  [  128/  265]
train() client id: f_00001-8-4 loss: 0.350183  [  160/  265]
train() client id: f_00001-8-5 loss: 0.296962  [  192/  265]
train() client id: f_00001-8-6 loss: 0.296146  [  224/  265]
train() client id: f_00001-8-7 loss: 0.332663  [  256/  265]
train() client id: f_00001-9-0 loss: 0.412936  [   32/  265]
train() client id: f_00001-9-1 loss: 0.457878  [   64/  265]
train() client id: f_00001-9-2 loss: 0.305845  [   96/  265]
train() client id: f_00001-9-3 loss: 0.316970  [  128/  265]
train() client id: f_00001-9-4 loss: 0.291787  [  160/  265]
train() client id: f_00001-9-5 loss: 0.359424  [  192/  265]
train() client id: f_00001-9-6 loss: 0.253125  [  224/  265]
train() client id: f_00001-9-7 loss: 0.397146  [  256/  265]
train() client id: f_00001-10-0 loss: 0.409058  [   32/  265]
train() client id: f_00001-10-1 loss: 0.238119  [   64/  265]
train() client id: f_00001-10-2 loss: 0.388930  [   96/  265]
train() client id: f_00001-10-3 loss: 0.257244  [  128/  265]
train() client id: f_00001-10-4 loss: 0.386587  [  160/  265]
train() client id: f_00001-10-5 loss: 0.279686  [  192/  265]
train() client id: f_00001-10-6 loss: 0.367078  [  224/  265]
train() client id: f_00001-10-7 loss: 0.449304  [  256/  265]
train() client id: f_00001-11-0 loss: 0.656693  [   32/  265]
train() client id: f_00001-11-1 loss: 0.248366  [   64/  265]
train() client id: f_00001-11-2 loss: 0.373293  [   96/  265]
train() client id: f_00001-11-3 loss: 0.250585  [  128/  265]
train() client id: f_00001-11-4 loss: 0.304166  [  160/  265]
train() client id: f_00001-11-5 loss: 0.257512  [  192/  265]
train() client id: f_00001-11-6 loss: 0.291143  [  224/  265]
train() client id: f_00001-11-7 loss: 0.397652  [  256/  265]
train() client id: f_00001-12-0 loss: 0.388031  [   32/  265]
train() client id: f_00001-12-1 loss: 0.349746  [   64/  265]
train() client id: f_00001-12-2 loss: 0.240018  [   96/  265]
train() client id: f_00001-12-3 loss: 0.290850  [  128/  265]
train() client id: f_00001-12-4 loss: 0.248386  [  160/  265]
train() client id: f_00001-12-5 loss: 0.398460  [  192/  265]
train() client id: f_00001-12-6 loss: 0.307281  [  224/  265]
train() client id: f_00001-12-7 loss: 0.553118  [  256/  265]
train() client id: f_00001-13-0 loss: 0.310169  [   32/  265]
train() client id: f_00001-13-1 loss: 0.240203  [   64/  265]
train() client id: f_00001-13-2 loss: 0.308933  [   96/  265]
train() client id: f_00001-13-3 loss: 0.331040  [  128/  265]
train() client id: f_00001-13-4 loss: 0.313293  [  160/  265]
train() client id: f_00001-13-5 loss: 0.413266  [  192/  265]
train() client id: f_00001-13-6 loss: 0.263220  [  224/  265]
train() client id: f_00001-13-7 loss: 0.493860  [  256/  265]
train() client id: f_00001-14-0 loss: 0.297654  [   32/  265]
train() client id: f_00001-14-1 loss: 0.514321  [   64/  265]
train() client id: f_00001-14-2 loss: 0.413923  [   96/  265]
train() client id: f_00001-14-3 loss: 0.357404  [  128/  265]
train() client id: f_00001-14-4 loss: 0.315995  [  160/  265]
train() client id: f_00001-14-5 loss: 0.238300  [  192/  265]
train() client id: f_00001-14-6 loss: 0.235374  [  224/  265]
train() client id: f_00001-14-7 loss: 0.342121  [  256/  265]
train() client id: f_00002-0-0 loss: 1.193339  [   32/  124]
train() client id: f_00002-0-1 loss: 1.317596  [   64/  124]
train() client id: f_00002-0-2 loss: 1.189709  [   96/  124]
train() client id: f_00002-1-0 loss: 1.282791  [   32/  124]
train() client id: f_00002-1-1 loss: 1.231145  [   64/  124]
train() client id: f_00002-1-2 loss: 1.410704  [   96/  124]
train() client id: f_00002-2-0 loss: 0.913838  [   32/  124]
train() client id: f_00002-2-1 loss: 1.245267  [   64/  124]
train() client id: f_00002-2-2 loss: 1.269142  [   96/  124]
train() client id: f_00002-3-0 loss: 1.041475  [   32/  124]
train() client id: f_00002-3-1 loss: 1.200391  [   64/  124]
train() client id: f_00002-3-2 loss: 1.007081  [   96/  124]
train() client id: f_00002-4-0 loss: 1.178137  [   32/  124]
train() client id: f_00002-4-1 loss: 0.981736  [   64/  124]
train() client id: f_00002-4-2 loss: 1.177245  [   96/  124]
train() client id: f_00002-5-0 loss: 1.110525  [   32/  124]
train() client id: f_00002-5-1 loss: 1.146383  [   64/  124]
train() client id: f_00002-5-2 loss: 1.131896  [   96/  124]
train() client id: f_00002-6-0 loss: 0.978962  [   32/  124]
train() client id: f_00002-6-1 loss: 1.105353  [   64/  124]
train() client id: f_00002-6-2 loss: 1.076781  [   96/  124]
train() client id: f_00002-7-0 loss: 0.894129  [   32/  124]
train() client id: f_00002-7-1 loss: 1.069630  [   64/  124]
train() client id: f_00002-7-2 loss: 1.144912  [   96/  124]
train() client id: f_00002-8-0 loss: 0.939550  [   32/  124]
train() client id: f_00002-8-1 loss: 1.027149  [   64/  124]
train() client id: f_00002-8-2 loss: 1.088409  [   96/  124]
train() client id: f_00002-9-0 loss: 0.939680  [   32/  124]
train() client id: f_00002-9-1 loss: 1.154942  [   64/  124]
train() client id: f_00002-9-2 loss: 1.039993  [   96/  124]
train() client id: f_00002-10-0 loss: 1.049297  [   32/  124]
train() client id: f_00002-10-1 loss: 0.980395  [   64/  124]
train() client id: f_00002-10-2 loss: 1.034787  [   96/  124]
train() client id: f_00002-11-0 loss: 1.022494  [   32/  124]
train() client id: f_00002-11-1 loss: 0.884718  [   64/  124]
train() client id: f_00002-11-2 loss: 1.018339  [   96/  124]
train() client id: f_00002-12-0 loss: 1.045368  [   32/  124]
train() client id: f_00002-12-1 loss: 0.878584  [   64/  124]
train() client id: f_00002-12-2 loss: 0.978630  [   96/  124]
train() client id: f_00002-13-0 loss: 0.925288  [   32/  124]
train() client id: f_00002-13-1 loss: 1.036611  [   64/  124]
train() client id: f_00002-13-2 loss: 1.112803  [   96/  124]
train() client id: f_00002-14-0 loss: 0.859262  [   32/  124]
train() client id: f_00002-14-1 loss: 0.916818  [   64/  124]
train() client id: f_00002-14-2 loss: 0.980024  [   96/  124]
train() client id: f_00003-0-0 loss: 0.716156  [   32/   43]
train() client id: f_00003-1-0 loss: 0.521221  [   32/   43]
train() client id: f_00003-2-0 loss: 0.669899  [   32/   43]
train() client id: f_00003-3-0 loss: 0.744555  [   32/   43]
train() client id: f_00003-4-0 loss: 0.474816  [   32/   43]
train() client id: f_00003-5-0 loss: 0.701405  [   32/   43]
train() client id: f_00003-6-0 loss: 0.579830  [   32/   43]
train() client id: f_00003-7-0 loss: 0.716594  [   32/   43]
train() client id: f_00003-8-0 loss: 0.923279  [   32/   43]
train() client id: f_00003-9-0 loss: 0.694034  [   32/   43]
train() client id: f_00003-10-0 loss: 0.760617  [   32/   43]
train() client id: f_00003-11-0 loss: 0.692182  [   32/   43]
train() client id: f_00003-12-0 loss: 0.777957  [   32/   43]
train() client id: f_00003-13-0 loss: 0.694577  [   32/   43]
train() client id: f_00003-14-0 loss: 0.706669  [   32/   43]
train() client id: f_00004-0-0 loss: 0.786936  [   32/  306]
train() client id: f_00004-0-1 loss: 0.696031  [   64/  306]
train() client id: f_00004-0-2 loss: 0.726584  [   96/  306]
train() client id: f_00004-0-3 loss: 0.788814  [  128/  306]
train() client id: f_00004-0-4 loss: 0.802575  [  160/  306]
train() client id: f_00004-0-5 loss: 0.686353  [  192/  306]
train() client id: f_00004-0-6 loss: 0.730819  [  224/  306]
train() client id: f_00004-0-7 loss: 0.823471  [  256/  306]
train() client id: f_00004-0-8 loss: 0.780945  [  288/  306]
train() client id: f_00004-1-0 loss: 0.882035  [   32/  306]
train() client id: f_00004-1-1 loss: 0.663469  [   64/  306]
train() client id: f_00004-1-2 loss: 0.636367  [   96/  306]
train() client id: f_00004-1-3 loss: 0.729024  [  128/  306]
train() client id: f_00004-1-4 loss: 0.739567  [  160/  306]
train() client id: f_00004-1-5 loss: 0.731464  [  192/  306]
train() client id: f_00004-1-6 loss: 0.782352  [  224/  306]
train() client id: f_00004-1-7 loss: 0.740493  [  256/  306]
train() client id: f_00004-1-8 loss: 0.819563  [  288/  306]
train() client id: f_00004-2-0 loss: 0.691049  [   32/  306]
train() client id: f_00004-2-1 loss: 1.002318  [   64/  306]
train() client id: f_00004-2-2 loss: 0.770553  [   96/  306]
train() client id: f_00004-2-3 loss: 0.952629  [  128/  306]
train() client id: f_00004-2-4 loss: 0.664045  [  160/  306]
train() client id: f_00004-2-5 loss: 0.724473  [  192/  306]
train() client id: f_00004-2-6 loss: 0.655635  [  224/  306]
train() client id: f_00004-2-7 loss: 0.686546  [  256/  306]
train() client id: f_00004-2-8 loss: 0.691471  [  288/  306]
train() client id: f_00004-3-0 loss: 0.752379  [   32/  306]
train() client id: f_00004-3-1 loss: 0.841269  [   64/  306]
train() client id: f_00004-3-2 loss: 0.783839  [   96/  306]
train() client id: f_00004-3-3 loss: 0.709804  [  128/  306]
train() client id: f_00004-3-4 loss: 0.699104  [  160/  306]
train() client id: f_00004-3-5 loss: 0.640934  [  192/  306]
train() client id: f_00004-3-6 loss: 0.755742  [  224/  306]
train() client id: f_00004-3-7 loss: 0.814498  [  256/  306]
train() client id: f_00004-3-8 loss: 0.763098  [  288/  306]
train() client id: f_00004-4-0 loss: 0.766854  [   32/  306]
train() client id: f_00004-4-1 loss: 0.825712  [   64/  306]
train() client id: f_00004-4-2 loss: 0.575778  [   96/  306]
train() client id: f_00004-4-3 loss: 0.786772  [  128/  306]
train() client id: f_00004-4-4 loss: 0.901988  [  160/  306]
train() client id: f_00004-4-5 loss: 0.636072  [  192/  306]
train() client id: f_00004-4-6 loss: 0.599604  [  224/  306]
train() client id: f_00004-4-7 loss: 0.868493  [  256/  306]
train() client id: f_00004-4-8 loss: 0.787538  [  288/  306]
train() client id: f_00004-5-0 loss: 0.834590  [   32/  306]
train() client id: f_00004-5-1 loss: 0.849285  [   64/  306]
train() client id: f_00004-5-2 loss: 0.613151  [   96/  306]
train() client id: f_00004-5-3 loss: 0.802290  [  128/  306]
train() client id: f_00004-5-4 loss: 0.798633  [  160/  306]
train() client id: f_00004-5-5 loss: 0.738644  [  192/  306]
train() client id: f_00004-5-6 loss: 0.664916  [  224/  306]
train() client id: f_00004-5-7 loss: 0.723729  [  256/  306]
train() client id: f_00004-5-8 loss: 0.722258  [  288/  306]
train() client id: f_00004-6-0 loss: 0.745483  [   32/  306]
train() client id: f_00004-6-1 loss: 0.743873  [   64/  306]
train() client id: f_00004-6-2 loss: 0.791414  [   96/  306]
train() client id: f_00004-6-3 loss: 0.815097  [  128/  306]
train() client id: f_00004-6-4 loss: 0.773119  [  160/  306]
train() client id: f_00004-6-5 loss: 0.593230  [  192/  306]
train() client id: f_00004-6-6 loss: 0.784288  [  224/  306]
train() client id: f_00004-6-7 loss: 0.735658  [  256/  306]
train() client id: f_00004-6-8 loss: 0.721973  [  288/  306]
train() client id: f_00004-7-0 loss: 0.689090  [   32/  306]
train() client id: f_00004-7-1 loss: 0.801439  [   64/  306]
train() client id: f_00004-7-2 loss: 0.806937  [   96/  306]
train() client id: f_00004-7-3 loss: 0.752832  [  128/  306]
train() client id: f_00004-7-4 loss: 0.715377  [  160/  306]
train() client id: f_00004-7-5 loss: 0.890447  [  192/  306]
train() client id: f_00004-7-6 loss: 0.769682  [  224/  306]
train() client id: f_00004-7-7 loss: 0.618183  [  256/  306]
train() client id: f_00004-7-8 loss: 0.737309  [  288/  306]
train() client id: f_00004-8-0 loss: 0.722334  [   32/  306]
train() client id: f_00004-8-1 loss: 0.739035  [   64/  306]
train() client id: f_00004-8-2 loss: 0.720087  [   96/  306]
train() client id: f_00004-8-3 loss: 0.594989  [  128/  306]
train() client id: f_00004-8-4 loss: 0.781635  [  160/  306]
train() client id: f_00004-8-5 loss: 0.874117  [  192/  306]
train() client id: f_00004-8-6 loss: 0.757838  [  224/  306]
train() client id: f_00004-8-7 loss: 0.721149  [  256/  306]
train() client id: f_00004-8-8 loss: 0.834055  [  288/  306]
train() client id: f_00004-9-0 loss: 0.871282  [   32/  306]
train() client id: f_00004-9-1 loss: 0.718265  [   64/  306]
train() client id: f_00004-9-2 loss: 0.611159  [   96/  306]
train() client id: f_00004-9-3 loss: 0.661803  [  128/  306]
train() client id: f_00004-9-4 loss: 0.828034  [  160/  306]
train() client id: f_00004-9-5 loss: 0.714213  [  192/  306]
train() client id: f_00004-9-6 loss: 0.705609  [  224/  306]
train() client id: f_00004-9-7 loss: 0.899198  [  256/  306]
train() client id: f_00004-9-8 loss: 0.665914  [  288/  306]
train() client id: f_00004-10-0 loss: 0.592435  [   32/  306]
train() client id: f_00004-10-1 loss: 0.730126  [   64/  306]
train() client id: f_00004-10-2 loss: 0.780190  [   96/  306]
train() client id: f_00004-10-3 loss: 0.768661  [  128/  306]
train() client id: f_00004-10-4 loss: 0.743721  [  160/  306]
train() client id: f_00004-10-5 loss: 0.753590  [  192/  306]
train() client id: f_00004-10-6 loss: 0.798184  [  224/  306]
train() client id: f_00004-10-7 loss: 0.759551  [  256/  306]
train() client id: f_00004-10-8 loss: 0.787386  [  288/  306]
train() client id: f_00004-11-0 loss: 0.823933  [   32/  306]
train() client id: f_00004-11-1 loss: 0.750279  [   64/  306]
train() client id: f_00004-11-2 loss: 0.717857  [   96/  306]
train() client id: f_00004-11-3 loss: 0.654324  [  128/  306]
train() client id: f_00004-11-4 loss: 0.773124  [  160/  306]
train() client id: f_00004-11-5 loss: 0.695874  [  192/  306]
train() client id: f_00004-11-6 loss: 0.753544  [  224/  306]
train() client id: f_00004-11-7 loss: 0.747030  [  256/  306]
train() client id: f_00004-11-8 loss: 0.836214  [  288/  306]
train() client id: f_00004-12-0 loss: 0.723532  [   32/  306]
train() client id: f_00004-12-1 loss: 0.766581  [   64/  306]
train() client id: f_00004-12-2 loss: 0.668561  [   96/  306]
train() client id: f_00004-12-3 loss: 0.702758  [  128/  306]
train() client id: f_00004-12-4 loss: 0.737206  [  160/  306]
train() client id: f_00004-12-5 loss: 0.735582  [  192/  306]
train() client id: f_00004-12-6 loss: 0.740870  [  224/  306]
train() client id: f_00004-12-7 loss: 0.771881  [  256/  306]
train() client id: f_00004-12-8 loss: 0.861711  [  288/  306]
train() client id: f_00004-13-0 loss: 0.734489  [   32/  306]
train() client id: f_00004-13-1 loss: 0.742287  [   64/  306]
train() client id: f_00004-13-2 loss: 0.761687  [   96/  306]
train() client id: f_00004-13-3 loss: 0.658087  [  128/  306]
train() client id: f_00004-13-4 loss: 0.795389  [  160/  306]
train() client id: f_00004-13-5 loss: 0.798917  [  192/  306]
train() client id: f_00004-13-6 loss: 0.724360  [  224/  306]
train() client id: f_00004-13-7 loss: 0.782246  [  256/  306]
train() client id: f_00004-13-8 loss: 0.720133  [  288/  306]
train() client id: f_00004-14-0 loss: 0.668141  [   32/  306]
train() client id: f_00004-14-1 loss: 0.746736  [   64/  306]
train() client id: f_00004-14-2 loss: 0.689090  [   96/  306]
train() client id: f_00004-14-3 loss: 0.705351  [  128/  306]
train() client id: f_00004-14-4 loss: 0.764168  [  160/  306]
train() client id: f_00004-14-5 loss: 0.825663  [  192/  306]
train() client id: f_00004-14-6 loss: 0.793963  [  224/  306]
train() client id: f_00004-14-7 loss: 0.731347  [  256/  306]
train() client id: f_00004-14-8 loss: 0.832712  [  288/  306]
train() client id: f_00005-0-0 loss: 0.490265  [   32/  146]
train() client id: f_00005-0-1 loss: 0.625712  [   64/  146]
train() client id: f_00005-0-2 loss: 0.370909  [   96/  146]
train() client id: f_00005-0-3 loss: 0.547776  [  128/  146]
train() client id: f_00005-1-0 loss: 0.816678  [   32/  146]
train() client id: f_00005-1-1 loss: 0.360522  [   64/  146]
train() client id: f_00005-1-2 loss: 0.497461  [   96/  146]
train() client id: f_00005-1-3 loss: 0.345849  [  128/  146]
train() client id: f_00005-2-0 loss: 0.597276  [   32/  146]
train() client id: f_00005-2-1 loss: 0.360027  [   64/  146]
train() client id: f_00005-2-2 loss: 0.602699  [   96/  146]
train() client id: f_00005-2-3 loss: 0.379202  [  128/  146]
train() client id: f_00005-3-0 loss: 0.225398  [   32/  146]
train() client id: f_00005-3-1 loss: 0.633141  [   64/  146]
train() client id: f_00005-3-2 loss: 0.392773  [   96/  146]
train() client id: f_00005-3-3 loss: 0.410819  [  128/  146]
train() client id: f_00005-4-0 loss: 0.601671  [   32/  146]
train() client id: f_00005-4-1 loss: 0.420584  [   64/  146]
train() client id: f_00005-4-2 loss: 0.356587  [   96/  146]
train() client id: f_00005-4-3 loss: 0.441129  [  128/  146]
train() client id: f_00005-5-0 loss: 0.460790  [   32/  146]
train() client id: f_00005-5-1 loss: 0.620014  [   64/  146]
train() client id: f_00005-5-2 loss: 0.317208  [   96/  146]
train() client id: f_00005-5-3 loss: 0.484677  [  128/  146]
train() client id: f_00005-6-0 loss: 0.420995  [   32/  146]
train() client id: f_00005-6-1 loss: 0.502166  [   64/  146]
train() client id: f_00005-6-2 loss: 0.385061  [   96/  146]
train() client id: f_00005-6-3 loss: 0.603339  [  128/  146]
train() client id: f_00005-7-0 loss: 0.375755  [   32/  146]
train() client id: f_00005-7-1 loss: 0.471225  [   64/  146]
train() client id: f_00005-7-2 loss: 0.315423  [   96/  146]
train() client id: f_00005-7-3 loss: 0.598770  [  128/  146]
train() client id: f_00005-8-0 loss: 0.112437  [   32/  146]
train() client id: f_00005-8-1 loss: 0.576127  [   64/  146]
train() client id: f_00005-8-2 loss: 0.545668  [   96/  146]
train() client id: f_00005-8-3 loss: 0.645869  [  128/  146]
train() client id: f_00005-9-0 loss: 0.577517  [   32/  146]
train() client id: f_00005-9-1 loss: 0.375035  [   64/  146]
train() client id: f_00005-9-2 loss: 0.589240  [   96/  146]
train() client id: f_00005-9-3 loss: 0.324959  [  128/  146]
train() client id: f_00005-10-0 loss: 0.279111  [   32/  146]
train() client id: f_00005-10-1 loss: 0.488750  [   64/  146]
train() client id: f_00005-10-2 loss: 0.270647  [   96/  146]
train() client id: f_00005-10-3 loss: 0.707696  [  128/  146]
train() client id: f_00005-11-0 loss: 0.360412  [   32/  146]
train() client id: f_00005-11-1 loss: 0.144163  [   64/  146]
train() client id: f_00005-11-2 loss: 0.585900  [   96/  146]
train() client id: f_00005-11-3 loss: 0.616208  [  128/  146]
train() client id: f_00005-12-0 loss: 0.592146  [   32/  146]
train() client id: f_00005-12-1 loss: 0.521724  [   64/  146]
train() client id: f_00005-12-2 loss: 0.368320  [   96/  146]
train() client id: f_00005-12-3 loss: 0.340721  [  128/  146]
train() client id: f_00005-13-0 loss: 0.477429  [   32/  146]
train() client id: f_00005-13-1 loss: 0.702363  [   64/  146]
train() client id: f_00005-13-2 loss: 0.155597  [   96/  146]
train() client id: f_00005-13-3 loss: 0.518057  [  128/  146]
train() client id: f_00005-14-0 loss: 0.358546  [   32/  146]
train() client id: f_00005-14-1 loss: 0.431641  [   64/  146]
train() client id: f_00005-14-2 loss: 0.393736  [   96/  146]
train() client id: f_00005-14-3 loss: 0.632193  [  128/  146]
train() client id: f_00006-0-0 loss: 0.464546  [   32/   54]
train() client id: f_00006-1-0 loss: 0.454537  [   32/   54]
train() client id: f_00006-2-0 loss: 0.458520  [   32/   54]
train() client id: f_00006-3-0 loss: 0.455638  [   32/   54]
train() client id: f_00006-4-0 loss: 0.438612  [   32/   54]
train() client id: f_00006-5-0 loss: 0.474266  [   32/   54]
train() client id: f_00006-6-0 loss: 0.467313  [   32/   54]
train() client id: f_00006-7-0 loss: 0.418278  [   32/   54]
train() client id: f_00006-8-0 loss: 0.412036  [   32/   54]
train() client id: f_00006-9-0 loss: 0.429206  [   32/   54]
train() client id: f_00006-10-0 loss: 0.454760  [   32/   54]
train() client id: f_00006-11-0 loss: 0.444822  [   32/   54]
train() client id: f_00006-12-0 loss: 0.517627  [   32/   54]
train() client id: f_00006-13-0 loss: 0.513279  [   32/   54]
train() client id: f_00006-14-0 loss: 0.462775  [   32/   54]
train() client id: f_00007-0-0 loss: 0.641441  [   32/  179]
train() client id: f_00007-0-1 loss: 0.716554  [   64/  179]
train() client id: f_00007-0-2 loss: 0.748408  [   96/  179]
train() client id: f_00007-0-3 loss: 0.791120  [  128/  179]
train() client id: f_00007-0-4 loss: 0.703667  [  160/  179]
train() client id: f_00007-1-0 loss: 0.622611  [   32/  179]
train() client id: f_00007-1-1 loss: 0.712585  [   64/  179]
train() client id: f_00007-1-2 loss: 0.677808  [   96/  179]
train() client id: f_00007-1-3 loss: 0.654188  [  128/  179]
train() client id: f_00007-1-4 loss: 0.858508  [  160/  179]
train() client id: f_00007-2-0 loss: 0.692737  [   32/  179]
train() client id: f_00007-2-1 loss: 0.533382  [   64/  179]
train() client id: f_00007-2-2 loss: 0.909775  [   96/  179]
train() client id: f_00007-2-3 loss: 0.689635  [  128/  179]
train() client id: f_00007-2-4 loss: 0.721386  [  160/  179]
train() client id: f_00007-3-0 loss: 0.781599  [   32/  179]
train() client id: f_00007-3-1 loss: 0.672177  [   64/  179]
train() client id: f_00007-3-2 loss: 0.692041  [   96/  179]
train() client id: f_00007-3-3 loss: 0.728231  [  128/  179]
train() client id: f_00007-3-4 loss: 0.606980  [  160/  179]
train() client id: f_00007-4-0 loss: 0.598830  [   32/  179]
train() client id: f_00007-4-1 loss: 0.749973  [   64/  179]
train() client id: f_00007-4-2 loss: 0.838271  [   96/  179]
train() client id: f_00007-4-3 loss: 0.653119  [  128/  179]
train() client id: f_00007-4-4 loss: 0.663668  [  160/  179]
train() client id: f_00007-5-0 loss: 0.742772  [   32/  179]
train() client id: f_00007-5-1 loss: 0.742884  [   64/  179]
train() client id: f_00007-5-2 loss: 0.631466  [   96/  179]
train() client id: f_00007-5-3 loss: 0.518889  [  128/  179]
train() client id: f_00007-5-4 loss: 0.671470  [  160/  179]
train() client id: f_00007-6-0 loss: 0.783082  [   32/  179]
train() client id: f_00007-6-1 loss: 0.567168  [   64/  179]
train() client id: f_00007-6-2 loss: 0.549451  [   96/  179]
train() client id: f_00007-6-3 loss: 0.900456  [  128/  179]
train() client id: f_00007-6-4 loss: 0.595325  [  160/  179]
train() client id: f_00007-7-0 loss: 0.789173  [   32/  179]
train() client id: f_00007-7-1 loss: 0.622410  [   64/  179]
train() client id: f_00007-7-2 loss: 0.627820  [   96/  179]
train() client id: f_00007-7-3 loss: 0.738102  [  128/  179]
train() client id: f_00007-7-4 loss: 0.610347  [  160/  179]
train() client id: f_00007-8-0 loss: 0.708721  [   32/  179]
train() client id: f_00007-8-1 loss: 0.726665  [   64/  179]
train() client id: f_00007-8-2 loss: 0.603391  [   96/  179]
train() client id: f_00007-8-3 loss: 0.648645  [  128/  179]
train() client id: f_00007-8-4 loss: 0.735623  [  160/  179]
train() client id: f_00007-9-0 loss: 0.873875  [   32/  179]
train() client id: f_00007-9-1 loss: 0.655975  [   64/  179]
train() client id: f_00007-9-2 loss: 0.674544  [   96/  179]
train() client id: f_00007-9-3 loss: 0.668025  [  128/  179]
train() client id: f_00007-9-4 loss: 0.479835  [  160/  179]
train() client id: f_00007-10-0 loss: 0.674822  [   32/  179]
train() client id: f_00007-10-1 loss: 0.600161  [   64/  179]
train() client id: f_00007-10-2 loss: 0.718778  [   96/  179]
train() client id: f_00007-10-3 loss: 0.678761  [  128/  179]
train() client id: f_00007-10-4 loss: 0.619005  [  160/  179]
train() client id: f_00007-11-0 loss: 0.469911  [   32/  179]
train() client id: f_00007-11-1 loss: 0.820993  [   64/  179]
train() client id: f_00007-11-2 loss: 0.723524  [   96/  179]
train() client id: f_00007-11-3 loss: 0.611357  [  128/  179]
train() client id: f_00007-11-4 loss: 0.557964  [  160/  179]
train() client id: f_00007-12-0 loss: 0.884712  [   32/  179]
train() client id: f_00007-12-1 loss: 0.518114  [   64/  179]
train() client id: f_00007-12-2 loss: 0.522269  [   96/  179]
train() client id: f_00007-12-3 loss: 0.789517  [  128/  179]
train() client id: f_00007-12-4 loss: 0.623656  [  160/  179]
train() client id: f_00007-13-0 loss: 0.675071  [   32/  179]
train() client id: f_00007-13-1 loss: 0.521859  [   64/  179]
train() client id: f_00007-13-2 loss: 0.698159  [   96/  179]
train() client id: f_00007-13-3 loss: 0.738608  [  128/  179]
train() client id: f_00007-13-4 loss: 0.668780  [  160/  179]
train() client id: f_00007-14-0 loss: 0.674889  [   32/  179]
train() client id: f_00007-14-1 loss: 0.590374  [   64/  179]
train() client id: f_00007-14-2 loss: 0.715815  [   96/  179]
train() client id: f_00007-14-3 loss: 0.493205  [  128/  179]
train() client id: f_00007-14-4 loss: 0.782963  [  160/  179]
train() client id: f_00008-0-0 loss: 0.744616  [   32/  130]
train() client id: f_00008-0-1 loss: 0.603000  [   64/  130]
train() client id: f_00008-0-2 loss: 0.696222  [   96/  130]
train() client id: f_00008-0-3 loss: 0.798834  [  128/  130]
train() client id: f_00008-1-0 loss: 0.760488  [   32/  130]
train() client id: f_00008-1-1 loss: 0.672213  [   64/  130]
train() client id: f_00008-1-2 loss: 0.693817  [   96/  130]
train() client id: f_00008-1-3 loss: 0.739524  [  128/  130]
train() client id: f_00008-2-0 loss: 0.831879  [   32/  130]
train() client id: f_00008-2-1 loss: 0.705692  [   64/  130]
train() client id: f_00008-2-2 loss: 0.701679  [   96/  130]
train() client id: f_00008-2-3 loss: 0.626321  [  128/  130]
train() client id: f_00008-3-0 loss: 0.847512  [   32/  130]
train() client id: f_00008-3-1 loss: 0.633114  [   64/  130]
train() client id: f_00008-3-2 loss: 0.628299  [   96/  130]
train() client id: f_00008-3-3 loss: 0.730234  [  128/  130]
train() client id: f_00008-4-0 loss: 0.649867  [   32/  130]
train() client id: f_00008-4-1 loss: 0.708504  [   64/  130]
train() client id: f_00008-4-2 loss: 0.773149  [   96/  130]
train() client id: f_00008-4-3 loss: 0.726033  [  128/  130]
train() client id: f_00008-5-0 loss: 0.660341  [   32/  130]
train() client id: f_00008-5-1 loss: 0.714178  [   64/  130]
train() client id: f_00008-5-2 loss: 0.745428  [   96/  130]
train() client id: f_00008-5-3 loss: 0.695543  [  128/  130]
train() client id: f_00008-6-0 loss: 0.678146  [   32/  130]
train() client id: f_00008-6-1 loss: 0.690927  [   64/  130]
train() client id: f_00008-6-2 loss: 0.659715  [   96/  130]
train() client id: f_00008-6-3 loss: 0.832223  [  128/  130]
train() client id: f_00008-7-0 loss: 0.674334  [   32/  130]
train() client id: f_00008-7-1 loss: 0.653608  [   64/  130]
train() client id: f_00008-7-2 loss: 0.901184  [   96/  130]
train() client id: f_00008-7-3 loss: 0.626332  [  128/  130]
train() client id: f_00008-8-0 loss: 0.794747  [   32/  130]
train() client id: f_00008-8-1 loss: 0.745294  [   64/  130]
train() client id: f_00008-8-2 loss: 0.632210  [   96/  130]
train() client id: f_00008-8-3 loss: 0.668762  [  128/  130]
train() client id: f_00008-9-0 loss: 0.796656  [   32/  130]
train() client id: f_00008-9-1 loss: 0.698504  [   64/  130]
train() client id: f_00008-9-2 loss: 0.688595  [   96/  130]
train() client id: f_00008-9-3 loss: 0.666625  [  128/  130]
train() client id: f_00008-10-0 loss: 0.730562  [   32/  130]
train() client id: f_00008-10-1 loss: 0.679819  [   64/  130]
train() client id: f_00008-10-2 loss: 0.673977  [   96/  130]
train() client id: f_00008-10-3 loss: 0.775869  [  128/  130]
train() client id: f_00008-11-0 loss: 0.655457  [   32/  130]
train() client id: f_00008-11-1 loss: 0.796862  [   64/  130]
train() client id: f_00008-11-2 loss: 0.664340  [   96/  130]
train() client id: f_00008-11-3 loss: 0.727353  [  128/  130]
train() client id: f_00008-12-0 loss: 0.810067  [   32/  130]
train() client id: f_00008-12-1 loss: 0.614302  [   64/  130]
train() client id: f_00008-12-2 loss: 0.694256  [   96/  130]
train() client id: f_00008-12-3 loss: 0.702255  [  128/  130]
train() client id: f_00008-13-0 loss: 0.679086  [   32/  130]
train() client id: f_00008-13-1 loss: 0.754561  [   64/  130]
train() client id: f_00008-13-2 loss: 0.785869  [   96/  130]
train() client id: f_00008-13-3 loss: 0.626230  [  128/  130]
train() client id: f_00008-14-0 loss: 0.780536  [   32/  130]
train() client id: f_00008-14-1 loss: 0.721501  [   64/  130]
train() client id: f_00008-14-2 loss: 0.663017  [   96/  130]
train() client id: f_00008-14-3 loss: 0.654286  [  128/  130]
train() client id: f_00009-0-0 loss: 0.950115  [   32/  118]
train() client id: f_00009-0-1 loss: 1.139259  [   64/  118]
train() client id: f_00009-0-2 loss: 1.012325  [   96/  118]
train() client id: f_00009-1-0 loss: 1.035786  [   32/  118]
train() client id: f_00009-1-1 loss: 0.869755  [   64/  118]
train() client id: f_00009-1-2 loss: 1.087673  [   96/  118]
train() client id: f_00009-2-0 loss: 0.827898  [   32/  118]
train() client id: f_00009-2-1 loss: 0.927811  [   64/  118]
train() client id: f_00009-2-2 loss: 0.940854  [   96/  118]
train() client id: f_00009-3-0 loss: 1.024170  [   32/  118]
train() client id: f_00009-3-1 loss: 0.928831  [   64/  118]
train() client id: f_00009-3-2 loss: 0.813016  [   96/  118]
train() client id: f_00009-4-0 loss: 0.858181  [   32/  118]
train() client id: f_00009-4-1 loss: 0.802563  [   64/  118]
train() client id: f_00009-4-2 loss: 1.018243  [   96/  118]
train() client id: f_00009-5-0 loss: 0.724063  [   32/  118]
train() client id: f_00009-5-1 loss: 0.859891  [   64/  118]
train() client id: f_00009-5-2 loss: 0.957943  [   96/  118]
train() client id: f_00009-6-0 loss: 0.755227  [   32/  118]
train() client id: f_00009-6-1 loss: 0.932559  [   64/  118]
train() client id: f_00009-6-2 loss: 0.881621  [   96/  118]
train() client id: f_00009-7-0 loss: 0.848405  [   32/  118]
train() client id: f_00009-7-1 loss: 0.931203  [   64/  118]
train() client id: f_00009-7-2 loss: 0.627224  [   96/  118]
train() client id: f_00009-8-0 loss: 0.894296  [   32/  118]
train() client id: f_00009-8-1 loss: 0.771941  [   64/  118]
train() client id: f_00009-8-2 loss: 0.882747  [   96/  118]
train() client id: f_00009-9-0 loss: 0.902403  [   32/  118]
train() client id: f_00009-9-1 loss: 0.731595  [   64/  118]
train() client id: f_00009-9-2 loss: 0.608857  [   96/  118]
train() client id: f_00009-10-0 loss: 0.763925  [   32/  118]
train() client id: f_00009-10-1 loss: 0.770986  [   64/  118]
train() client id: f_00009-10-2 loss: 0.776996  [   96/  118]
train() client id: f_00009-11-0 loss: 0.745265  [   32/  118]
train() client id: f_00009-11-1 loss: 0.701105  [   64/  118]
train() client id: f_00009-11-2 loss: 0.867581  [   96/  118]
train() client id: f_00009-12-0 loss: 0.765088  [   32/  118]
train() client id: f_00009-12-1 loss: 0.621961  [   64/  118]
train() client id: f_00009-12-2 loss: 0.926680  [   96/  118]
train() client id: f_00009-13-0 loss: 0.671861  [   32/  118]
train() client id: f_00009-13-1 loss: 0.881908  [   64/  118]
train() client id: f_00009-13-2 loss: 0.747037  [   96/  118]
train() client id: f_00009-14-0 loss: 0.640596  [   32/  118]
train() client id: f_00009-14-1 loss: 0.906470  [   64/  118]
train() client id: f_00009-14-2 loss: 0.807079  [   96/  118]
At round 40 accuracy: 0.6445623342175066
At round 40 training accuracy: 0.5868544600938967
At round 40 training loss: 0.842630789717195
update_location
xs = 8.927491 321.223621 5.882650 0.934260 -237.581990 -85.230757 -45.849135 -5.143845 -260.120581 20.134486 
ys = -312.390647 7.291448 210.684448 -32.290817 -9.642386 0.794442 -1.381692 206.628436 25.881276 -747.232496 
xs mean: -27.682379970521218
ys mean: -65.16579882624052
dists_uav = 328.127439 336.508217 233.286395 105.088390 257.949951 131.396017 110.018418 229.612216 279.879540 754.162980 
uav_gains = -117.848036 -118.387121 -109.949797 -100.538901 -111.986151 -102.965314 -101.036697 -109.675052 -113.969084 -128.898494 
uav_gains_db_mean: -111.52546474240904
dists_bs = 520.940462 523.797709 184.368959 271.886643 194.959781 195.974726 218.610264 172.775784 171.702962 942.650648 
bs_gains = -115.637126 -115.703640 -103.006318 -107.729920 -103.685520 -103.748661 -105.077834 -102.216580 -102.140837 -122.848881 
bs_gains_db_mean: -108.17953155712135
Round 41
-------------------------------
ene_coms = [0.02136564 0.0227408  0.00719916 0.00644658 0.00743756 0.00718525
 0.00658565 0.00694015 0.00691627 0.04186803]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.73202478 11.73848321  5.44357148  1.94410153 13.2858336   6.39092698
  2.41986473  7.80875809  5.69795541  5.68025493]
obj_prev = 66.1417747635601
eta_min = 3.3985354857431606e-17	eta_max = 0.7855617904100776
af = 13.805385376324658	bf = 1.90420787479819	zeta = 15.185923913957124	eta = 0.9090909090909091
af = 13.805385376324658	bf = 1.90420787479819	zeta = 35.42001801012059	eta = 0.3897622348012368
af = 13.805385376324658	bf = 1.90420787479819	zeta = 24.436205970108727	eta = 0.5649561717237086
af = 13.805385376324658	bf = 1.90420787479819	zeta = 22.497000588250973	eta = 0.6136544879469177
af = 13.805385376324658	bf = 1.90420787479819	zeta = 22.37815120681488	eta = 0.6169135800691375
af = 13.805385376324658	bf = 1.90420787479819	zeta = 22.37764654629887	eta = 0.616927492699539
af = 13.805385376324658	bf = 1.90420787479819	zeta = 22.377646537134677	eta = 0.6169274929521857
eta = 0.6169274929521857
ene_coms = [0.02136564 0.0227408  0.00719916 0.00644658 0.00743756 0.00718525
 0.00658565 0.00694015 0.00691627 0.04186803]
ene_comp = [0.03985631 0.08382479 0.03922367 0.01360176 0.09679391 0.04618271
 0.01708128 0.05662127 0.04112159 0.03732575]
ene_total = [2.25954527 3.93306241 1.71334765 0.73993258 3.84691598 1.9696744
 0.87348552 2.34588903 1.7729543  2.9228394 ]
ti_comp = [0.54260409 0.52885249 0.68426891 0.69179474 0.68188488 0.68440805
 0.69040402 0.68685898 0.6870978  0.33758019]
ti_coms = [0.21365642 0.22740802 0.0719916  0.06446577 0.07437562 0.07185246
 0.06585649 0.06940152 0.06916271 0.41868032]
t_total = [27.91163445 27.91163445 27.91163445 27.91163445 27.91163445 27.91163445
 27.91163445 27.91163445 27.91163445 27.91163445]
ene_coms = [0.02136564 0.0227408  0.00719916 0.00644658 0.00743756 0.00718525
 0.00658565 0.00694015 0.00691627 0.04186803]
ene_comp = [1.34401863e-05 1.31621943e-04 8.05510520e-06 3.28632592e-07
 1.21899502e-04 1.31428070e-05 6.53483219e-07 2.40482625e-05
 9.20561314e-06 2.85201559e-05]
ene_total = [0.78904704 0.84416246 0.2659998  0.23793873 0.27900033 0.26567405
 0.24308352 0.25703077 0.25560156 1.54629421]
optimize_network iter = 0 obj = 4.983832470361207
eta = 0.6169274929521857
freqs = [36726883.18132965 79251577.0803692  28661008.50648779  9830775.77625334
 70975253.87467305 33739165.71853161 12370493.97651994 41217537.21118562
 29924119.88475796 55284278.49183078]
eta_min = 0.6169274929521864	eta_max = 0.6169274929521857
af = 0.010271812687474558	bf = 1.90420787479819	zeta = 0.011298993956222015	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02136564 0.0227408  0.00719916 0.00644658 0.00743756 0.00718525
 0.00658565 0.00694015 0.00691627 0.04186803]
ene_comp = [2.65212537e-06 2.59726975e-05 1.58949797e-06 6.48484192e-08
 2.40541876e-05 2.59344410e-06 1.28950551e-07 4.74539606e-06
 1.81652543e-06 5.62782593e-06]
ene_total = [3.32320676 3.54069912 1.11986377 1.0025846  1.16043398 1.11785604
 1.02422319 1.08007363 1.07590407 6.51221056]
ti_comp = [0.54260409 0.52885249 0.68426891 0.69179474 0.68188488 0.68440805
 0.69040402 0.68685898 0.6870978  0.33758019]
ti_coms = [0.21365642 0.22740802 0.0719916  0.06446577 0.07437562 0.07185246
 0.06585649 0.06940152 0.06916271 0.41868032]
t_total = [27.91163445 27.91163445 27.91163445 27.91163445 27.91163445 27.91163445
 27.91163445 27.91163445 27.91163445 27.91163445]
ene_coms = [0.02136564 0.0227408  0.00719916 0.00644658 0.00743756 0.00718525
 0.00658565 0.00694015 0.00691627 0.04186803]
ene_comp = [1.34401863e-05 1.31621943e-04 8.05510520e-06 3.28632592e-07
 1.21899502e-04 1.31428070e-05 6.53483219e-07 2.40482625e-05
 9.20561314e-06 2.85201559e-05]
ene_total = [0.78904704 0.84416246 0.2659998  0.23793873 0.27900033 0.26567405
 0.24308352 0.25703077 0.25560156 1.54629421]
optimize_network iter = 1 obj = 4.983832470361207
eta = 0.6169274929521857
freqs = [36726883.18132965 79251577.0803692  28661008.50648779  9830775.77625334
 70975253.87467305 33739165.71853161 12370493.97651994 41217537.21118562
 29924119.88475796 55284278.49183078]
Done!
ene_coms = [0.02136564 0.0227408  0.00719916 0.00644658 0.00743756 0.00718525
 0.00658565 0.00694015 0.00691627 0.04186803]
ene_comp = [1.27467643e-05 1.24831148e-04 7.63951670e-06 3.11677391e-07
 1.15610319e-04 1.24647278e-05 6.19767941e-07 2.28075361e-05
 8.73066627e-06 2.70487103e-05]
ene_total = [0.02137839 0.02286563 0.0072068  0.00644689 0.00755317 0.00719771
 0.00658627 0.00696296 0.006925   0.04189508]
At round 41 energy consumption: 0.1350179022499654
At round 41 eta: 0.6169274929521857
At round 41 a_n: 14.138223130512367
At round 41 local rounds: 15.815997627410859
At round 41 global rounds: 36.90743363304761
gradient difference: 0.3717458248138428
train() client id: f_00000-0-0 loss: 0.957334  [   32/  126]
train() client id: f_00000-0-1 loss: 0.972482  [   64/  126]
train() client id: f_00000-0-2 loss: 1.111969  [   96/  126]
train() client id: f_00000-1-0 loss: 0.939494  [   32/  126]
train() client id: f_00000-1-1 loss: 0.871258  [   64/  126]
train() client id: f_00000-1-2 loss: 0.906820  [   96/  126]
train() client id: f_00000-2-0 loss: 0.938564  [   32/  126]
train() client id: f_00000-2-1 loss: 0.890564  [   64/  126]
train() client id: f_00000-2-2 loss: 0.806961  [   96/  126]
train() client id: f_00000-3-0 loss: 0.739240  [   32/  126]
train() client id: f_00000-3-1 loss: 0.716026  [   64/  126]
train() client id: f_00000-3-2 loss: 0.835899  [   96/  126]
train() client id: f_00000-4-0 loss: 0.773781  [   32/  126]
train() client id: f_00000-4-1 loss: 0.605340  [   64/  126]
train() client id: f_00000-4-2 loss: 0.964090  [   96/  126]
train() client id: f_00000-5-0 loss: 0.612116  [   32/  126]
train() client id: f_00000-5-1 loss: 0.725490  [   64/  126]
train() client id: f_00000-5-2 loss: 0.899211  [   96/  126]
train() client id: f_00000-6-0 loss: 0.714045  [   32/  126]
train() client id: f_00000-6-1 loss: 0.769240  [   64/  126]
train() client id: f_00000-6-2 loss: 0.806811  [   96/  126]
train() client id: f_00000-7-0 loss: 0.713190  [   32/  126]
train() client id: f_00000-7-1 loss: 0.561246  [   64/  126]
train() client id: f_00000-7-2 loss: 0.688456  [   96/  126]
train() client id: f_00000-8-0 loss: 0.724979  [   32/  126]
train() client id: f_00000-8-1 loss: 0.812274  [   64/  126]
train() client id: f_00000-8-2 loss: 0.723680  [   96/  126]
train() client id: f_00000-9-0 loss: 0.678185  [   32/  126]
train() client id: f_00000-9-1 loss: 0.695145  [   64/  126]
train() client id: f_00000-9-2 loss: 0.693845  [   96/  126]
train() client id: f_00000-10-0 loss: 0.654505  [   32/  126]
train() client id: f_00000-10-1 loss: 0.834759  [   64/  126]
train() client id: f_00000-10-2 loss: 0.703726  [   96/  126]
train() client id: f_00000-11-0 loss: 0.702173  [   32/  126]
train() client id: f_00000-11-1 loss: 0.681878  [   64/  126]
train() client id: f_00000-11-2 loss: 0.718089  [   96/  126]
train() client id: f_00000-12-0 loss: 0.649615  [   32/  126]
train() client id: f_00000-12-1 loss: 0.683501  [   64/  126]
train() client id: f_00000-12-2 loss: 0.734345  [   96/  126]
train() client id: f_00000-13-0 loss: 0.740294  [   32/  126]
train() client id: f_00000-13-1 loss: 0.540879  [   64/  126]
train() client id: f_00000-13-2 loss: 0.863558  [   96/  126]
train() client id: f_00000-14-0 loss: 0.657225  [   32/  126]
train() client id: f_00000-14-1 loss: 0.642901  [   64/  126]
train() client id: f_00000-14-2 loss: 0.775536  [   96/  126]
train() client id: f_00001-0-0 loss: 0.419938  [   32/  265]
train() client id: f_00001-0-1 loss: 0.328045  [   64/  265]
train() client id: f_00001-0-2 loss: 0.344890  [   96/  265]
train() client id: f_00001-0-3 loss: 0.469709  [  128/  265]
train() client id: f_00001-0-4 loss: 0.458850  [  160/  265]
train() client id: f_00001-0-5 loss: 0.426147  [  192/  265]
train() client id: f_00001-0-6 loss: 0.417671  [  224/  265]
train() client id: f_00001-0-7 loss: 0.514738  [  256/  265]
train() client id: f_00001-1-0 loss: 0.418937  [   32/  265]
train() client id: f_00001-1-1 loss: 0.420790  [   64/  265]
train() client id: f_00001-1-2 loss: 0.459388  [   96/  265]
train() client id: f_00001-1-3 loss: 0.391369  [  128/  265]
train() client id: f_00001-1-4 loss: 0.379477  [  160/  265]
train() client id: f_00001-1-5 loss: 0.314665  [  192/  265]
train() client id: f_00001-1-6 loss: 0.576700  [  224/  265]
train() client id: f_00001-1-7 loss: 0.345705  [  256/  265]
train() client id: f_00001-2-0 loss: 0.315169  [   32/  265]
train() client id: f_00001-2-1 loss: 0.383820  [   64/  265]
train() client id: f_00001-2-2 loss: 0.420829  [   96/  265]
train() client id: f_00001-2-3 loss: 0.309605  [  128/  265]
train() client id: f_00001-2-4 loss: 0.432688  [  160/  265]
train() client id: f_00001-2-5 loss: 0.443296  [  192/  265]
train() client id: f_00001-2-6 loss: 0.413073  [  224/  265]
train() client id: f_00001-2-7 loss: 0.504415  [  256/  265]
train() client id: f_00001-3-0 loss: 0.347779  [   32/  265]
train() client id: f_00001-3-1 loss: 0.425249  [   64/  265]
train() client id: f_00001-3-2 loss: 0.393179  [   96/  265]
train() client id: f_00001-3-3 loss: 0.485777  [  128/  265]
train() client id: f_00001-3-4 loss: 0.512775  [  160/  265]
train() client id: f_00001-3-5 loss: 0.372211  [  192/  265]
train() client id: f_00001-3-6 loss: 0.329527  [  224/  265]
train() client id: f_00001-3-7 loss: 0.362306  [  256/  265]
train() client id: f_00001-4-0 loss: 0.530214  [   32/  265]
train() client id: f_00001-4-1 loss: 0.312213  [   64/  265]
train() client id: f_00001-4-2 loss: 0.458127  [   96/  265]
train() client id: f_00001-4-3 loss: 0.379292  [  128/  265]
train() client id: f_00001-4-4 loss: 0.325979  [  160/  265]
train() client id: f_00001-4-5 loss: 0.305129  [  192/  265]
train() client id: f_00001-4-6 loss: 0.408904  [  224/  265]
train() client id: f_00001-4-7 loss: 0.335493  [  256/  265]
train() client id: f_00001-5-0 loss: 0.306905  [   32/  265]
train() client id: f_00001-5-1 loss: 0.431322  [   64/  265]
train() client id: f_00001-5-2 loss: 0.454044  [   96/  265]
train() client id: f_00001-5-3 loss: 0.322231  [  128/  265]
train() client id: f_00001-5-4 loss: 0.287723  [  160/  265]
train() client id: f_00001-5-5 loss: 0.470144  [  192/  265]
train() client id: f_00001-5-6 loss: 0.389118  [  224/  265]
train() client id: f_00001-5-7 loss: 0.417751  [  256/  265]
train() client id: f_00001-6-0 loss: 0.319923  [   32/  265]
train() client id: f_00001-6-1 loss: 0.479291  [   64/  265]
train() client id: f_00001-6-2 loss: 0.359625  [   96/  265]
train() client id: f_00001-6-3 loss: 0.293298  [  128/  265]
train() client id: f_00001-6-4 loss: 0.382049  [  160/  265]
train() client id: f_00001-6-5 loss: 0.438068  [  192/  265]
train() client id: f_00001-6-6 loss: 0.353728  [  224/  265]
train() client id: f_00001-6-7 loss: 0.410756  [  256/  265]
train() client id: f_00001-7-0 loss: 0.413427  [   32/  265]
train() client id: f_00001-7-1 loss: 0.299033  [   64/  265]
train() client id: f_00001-7-2 loss: 0.461348  [   96/  265]
train() client id: f_00001-7-3 loss: 0.482019  [  128/  265]
train() client id: f_00001-7-4 loss: 0.380902  [  160/  265]
train() client id: f_00001-7-5 loss: 0.294482  [  192/  265]
train() client id: f_00001-7-6 loss: 0.288152  [  224/  265]
train() client id: f_00001-7-7 loss: 0.466842  [  256/  265]
train() client id: f_00001-8-0 loss: 0.558950  [   32/  265]
train() client id: f_00001-8-1 loss: 0.322965  [   64/  265]
train() client id: f_00001-8-2 loss: 0.442729  [   96/  265]
train() client id: f_00001-8-3 loss: 0.447001  [  128/  265]
train() client id: f_00001-8-4 loss: 0.291688  [  160/  265]
train() client id: f_00001-8-5 loss: 0.395329  [  192/  265]
train() client id: f_00001-8-6 loss: 0.318573  [  224/  265]
train() client id: f_00001-8-7 loss: 0.339842  [  256/  265]
train() client id: f_00001-9-0 loss: 0.490218  [   32/  265]
train() client id: f_00001-9-1 loss: 0.356177  [   64/  265]
train() client id: f_00001-9-2 loss: 0.370750  [   96/  265]
train() client id: f_00001-9-3 loss: 0.377379  [  128/  265]
train() client id: f_00001-9-4 loss: 0.333606  [  160/  265]
train() client id: f_00001-9-5 loss: 0.374235  [  192/  265]
train() client id: f_00001-9-6 loss: 0.392905  [  224/  265]
train() client id: f_00001-9-7 loss: 0.356831  [  256/  265]
train() client id: f_00001-10-0 loss: 0.341480  [   32/  265]
train() client id: f_00001-10-1 loss: 0.343337  [   64/  265]
train() client id: f_00001-10-2 loss: 0.467022  [   96/  265]
train() client id: f_00001-10-3 loss: 0.341964  [  128/  265]
train() client id: f_00001-10-4 loss: 0.398562  [  160/  265]
train() client id: f_00001-10-5 loss: 0.374773  [  192/  265]
train() client id: f_00001-10-6 loss: 0.447543  [  224/  265]
train() client id: f_00001-10-7 loss: 0.335709  [  256/  265]
train() client id: f_00001-11-0 loss: 0.312274  [   32/  265]
train() client id: f_00001-11-1 loss: 0.419855  [   64/  265]
train() client id: f_00001-11-2 loss: 0.403210  [   96/  265]
train() client id: f_00001-11-3 loss: 0.562693  [  128/  265]
train() client id: f_00001-11-4 loss: 0.382466  [  160/  265]
train() client id: f_00001-11-5 loss: 0.284006  [  192/  265]
train() client id: f_00001-11-6 loss: 0.325239  [  224/  265]
train() client id: f_00001-11-7 loss: 0.409984  [  256/  265]
train() client id: f_00001-12-0 loss: 0.454556  [   32/  265]
train() client id: f_00001-12-1 loss: 0.318002  [   64/  265]
train() client id: f_00001-12-2 loss: 0.422492  [   96/  265]
train() client id: f_00001-12-3 loss: 0.430783  [  128/  265]
train() client id: f_00001-12-4 loss: 0.343608  [  160/  265]
train() client id: f_00001-12-5 loss: 0.318422  [  192/  265]
train() client id: f_00001-12-6 loss: 0.338423  [  224/  265]
train() client id: f_00001-12-7 loss: 0.384278  [  256/  265]
train() client id: f_00001-13-0 loss: 0.272569  [   32/  265]
train() client id: f_00001-13-1 loss: 0.372397  [   64/  265]
train() client id: f_00001-13-2 loss: 0.290020  [   96/  265]
train() client id: f_00001-13-3 loss: 0.371083  [  128/  265]
train() client id: f_00001-13-4 loss: 0.278896  [  160/  265]
train() client id: f_00001-13-5 loss: 0.568945  [  192/  265]
train() client id: f_00001-13-6 loss: 0.451527  [  224/  265]
train() client id: f_00001-13-7 loss: 0.401484  [  256/  265]
train() client id: f_00001-14-0 loss: 0.343492  [   32/  265]
train() client id: f_00001-14-1 loss: 0.305996  [   64/  265]
train() client id: f_00001-14-2 loss: 0.446430  [   96/  265]
train() client id: f_00001-14-3 loss: 0.469816  [  128/  265]
train() client id: f_00001-14-4 loss: 0.362493  [  160/  265]
train() client id: f_00001-14-5 loss: 0.417809  [  192/  265]
train() client id: f_00001-14-6 loss: 0.276379  [  224/  265]
train() client id: f_00001-14-7 loss: 0.447521  [  256/  265]
train() client id: f_00002-0-0 loss: 1.471730  [   32/  124]
train() client id: f_00002-0-1 loss: 1.206034  [   64/  124]
train() client id: f_00002-0-2 loss: 1.193610  [   96/  124]
train() client id: f_00002-1-0 loss: 1.262607  [   32/  124]
train() client id: f_00002-1-1 loss: 1.358944  [   64/  124]
train() client id: f_00002-1-2 loss: 1.139235  [   96/  124]
train() client id: f_00002-2-0 loss: 1.255153  [   32/  124]
train() client id: f_00002-2-1 loss: 1.248832  [   64/  124]
train() client id: f_00002-2-2 loss: 1.138385  [   96/  124]
train() client id: f_00002-3-0 loss: 1.251307  [   32/  124]
train() client id: f_00002-3-1 loss: 0.898851  [   64/  124]
train() client id: f_00002-3-2 loss: 1.205535  [   96/  124]
train() client id: f_00002-4-0 loss: 1.146702  [   32/  124]
train() client id: f_00002-4-1 loss: 1.017660  [   64/  124]
train() client id: f_00002-4-2 loss: 1.284451  [   96/  124]
train() client id: f_00002-5-0 loss: 1.046641  [   32/  124]
train() client id: f_00002-5-1 loss: 0.988035  [   64/  124]
train() client id: f_00002-5-2 loss: 1.078627  [   96/  124]
train() client id: f_00002-6-0 loss: 1.113557  [   32/  124]
train() client id: f_00002-6-1 loss: 0.922694  [   64/  124]
train() client id: f_00002-6-2 loss: 0.950613  [   96/  124]
train() client id: f_00002-7-0 loss: 1.257726  [   32/  124]
train() client id: f_00002-7-1 loss: 0.989897  [   64/  124]
train() client id: f_00002-7-2 loss: 0.899787  [   96/  124]
train() client id: f_00002-8-0 loss: 1.011965  [   32/  124]
train() client id: f_00002-8-1 loss: 0.876590  [   64/  124]
train() client id: f_00002-8-2 loss: 1.019918  [   96/  124]
train() client id: f_00002-9-0 loss: 0.952619  [   32/  124]
train() client id: f_00002-9-1 loss: 0.828938  [   64/  124]
train() client id: f_00002-9-2 loss: 1.031275  [   96/  124]
train() client id: f_00002-10-0 loss: 1.087888  [   32/  124]
train() client id: f_00002-10-1 loss: 1.021041  [   64/  124]
train() client id: f_00002-10-2 loss: 0.809763  [   96/  124]
train() client id: f_00002-11-0 loss: 0.812382  [   32/  124]
train() client id: f_00002-11-1 loss: 0.821166  [   64/  124]
train() client id: f_00002-11-2 loss: 0.867348  [   96/  124]
train() client id: f_00002-12-0 loss: 0.922096  [   32/  124]
train() client id: f_00002-12-1 loss: 0.831822  [   64/  124]
train() client id: f_00002-12-2 loss: 0.724328  [   96/  124]
train() client id: f_00002-13-0 loss: 1.025901  [   32/  124]
train() client id: f_00002-13-1 loss: 0.785671  [   64/  124]
train() client id: f_00002-13-2 loss: 0.923608  [   96/  124]
train() client id: f_00002-14-0 loss: 0.737570  [   32/  124]
train() client id: f_00002-14-1 loss: 1.014321  [   64/  124]
train() client id: f_00002-14-2 loss: 0.858441  [   96/  124]
train() client id: f_00003-0-0 loss: 0.702774  [   32/   43]
train() client id: f_00003-1-0 loss: 0.593477  [   32/   43]
train() client id: f_00003-2-0 loss: 0.561048  [   32/   43]
train() client id: f_00003-3-0 loss: 0.539988  [   32/   43]
train() client id: f_00003-4-0 loss: 0.765955  [   32/   43]
train() client id: f_00003-5-0 loss: 0.695314  [   32/   43]
train() client id: f_00003-6-0 loss: 0.603140  [   32/   43]
train() client id: f_00003-7-0 loss: 0.834807  [   32/   43]
train() client id: f_00003-8-0 loss: 0.673258  [   32/   43]
train() client id: f_00003-9-0 loss: 0.675656  [   32/   43]
train() client id: f_00003-10-0 loss: 0.703615  [   32/   43]
train() client id: f_00003-11-0 loss: 0.611925  [   32/   43]
train() client id: f_00003-12-0 loss: 0.791978  [   32/   43]
train() client id: f_00003-13-0 loss: 0.618119  [   32/   43]
train() client id: f_00003-14-0 loss: 0.786970  [   32/   43]
train() client id: f_00004-0-0 loss: 0.742022  [   32/  306]
train() client id: f_00004-0-1 loss: 0.673886  [   64/  306]
train() client id: f_00004-0-2 loss: 0.760372  [   96/  306]
train() client id: f_00004-0-3 loss: 0.712036  [  128/  306]
train() client id: f_00004-0-4 loss: 0.712918  [  160/  306]
train() client id: f_00004-0-5 loss: 0.752239  [  192/  306]
train() client id: f_00004-0-6 loss: 0.607311  [  224/  306]
train() client id: f_00004-0-7 loss: 0.699485  [  256/  306]
train() client id: f_00004-0-8 loss: 0.751703  [  288/  306]
train() client id: f_00004-1-0 loss: 0.802472  [   32/  306]
train() client id: f_00004-1-1 loss: 0.642217  [   64/  306]
train() client id: f_00004-1-2 loss: 0.928643  [   96/  306]
train() client id: f_00004-1-3 loss: 0.743971  [  128/  306]
train() client id: f_00004-1-4 loss: 0.675810  [  160/  306]
train() client id: f_00004-1-5 loss: 0.654040  [  192/  306]
train() client id: f_00004-1-6 loss: 0.692018  [  224/  306]
train() client id: f_00004-1-7 loss: 0.722768  [  256/  306]
train() client id: f_00004-1-8 loss: 0.632826  [  288/  306]
train() client id: f_00004-2-0 loss: 0.760113  [   32/  306]
train() client id: f_00004-2-1 loss: 0.786624  [   64/  306]
train() client id: f_00004-2-2 loss: 0.704490  [   96/  306]
train() client id: f_00004-2-3 loss: 0.713966  [  128/  306]
train() client id: f_00004-2-4 loss: 0.633704  [  160/  306]
train() client id: f_00004-2-5 loss: 0.608031  [  192/  306]
train() client id: f_00004-2-6 loss: 0.749549  [  224/  306]
train() client id: f_00004-2-7 loss: 0.934980  [  256/  306]
train() client id: f_00004-2-8 loss: 0.733728  [  288/  306]
train() client id: f_00004-3-0 loss: 0.721578  [   32/  306]
train() client id: f_00004-3-1 loss: 0.800465  [   64/  306]
train() client id: f_00004-3-2 loss: 0.728007  [   96/  306]
train() client id: f_00004-3-3 loss: 0.610670  [  128/  306]
train() client id: f_00004-3-4 loss: 0.678210  [  160/  306]
train() client id: f_00004-3-5 loss: 0.804365  [  192/  306]
train() client id: f_00004-3-6 loss: 0.878697  [  224/  306]
train() client id: f_00004-3-7 loss: 0.810017  [  256/  306]
train() client id: f_00004-3-8 loss: 0.739388  [  288/  306]
train() client id: f_00004-4-0 loss: 0.698997  [   32/  306]
train() client id: f_00004-4-1 loss: 0.643392  [   64/  306]
train() client id: f_00004-4-2 loss: 0.823193  [   96/  306]
train() client id: f_00004-4-3 loss: 0.706193  [  128/  306]
train() client id: f_00004-4-4 loss: 0.787657  [  160/  306]
train() client id: f_00004-4-5 loss: 0.702379  [  192/  306]
train() client id: f_00004-4-6 loss: 0.618194  [  224/  306]
train() client id: f_00004-4-7 loss: 0.899951  [  256/  306]
train() client id: f_00004-4-8 loss: 0.697179  [  288/  306]
train() client id: f_00004-5-0 loss: 0.763700  [   32/  306]
train() client id: f_00004-5-1 loss: 0.754784  [   64/  306]
train() client id: f_00004-5-2 loss: 0.706782  [   96/  306]
train() client id: f_00004-5-3 loss: 0.638333  [  128/  306]
train() client id: f_00004-5-4 loss: 0.986217  [  160/  306]
train() client id: f_00004-5-5 loss: 0.703353  [  192/  306]
train() client id: f_00004-5-6 loss: 0.727381  [  224/  306]
train() client id: f_00004-5-7 loss: 0.727550  [  256/  306]
train() client id: f_00004-5-8 loss: 0.667889  [  288/  306]
train() client id: f_00004-6-0 loss: 0.846410  [   32/  306]
train() client id: f_00004-6-1 loss: 0.626371  [   64/  306]
train() client id: f_00004-6-2 loss: 0.634776  [   96/  306]
train() client id: f_00004-6-3 loss: 0.905260  [  128/  306]
train() client id: f_00004-6-4 loss: 0.599910  [  160/  306]
train() client id: f_00004-6-5 loss: 0.768127  [  192/  306]
train() client id: f_00004-6-6 loss: 0.698653  [  224/  306]
train() client id: f_00004-6-7 loss: 0.759735  [  256/  306]
train() client id: f_00004-6-8 loss: 0.773571  [  288/  306]
train() client id: f_00004-7-0 loss: 0.679885  [   32/  306]
train() client id: f_00004-7-1 loss: 0.847968  [   64/  306]
train() client id: f_00004-7-2 loss: 0.734097  [   96/  306]
train() client id: f_00004-7-3 loss: 0.704034  [  128/  306]
train() client id: f_00004-7-4 loss: 0.672470  [  160/  306]
train() client id: f_00004-7-5 loss: 0.681768  [  192/  306]
train() client id: f_00004-7-6 loss: 0.752981  [  224/  306]
train() client id: f_00004-7-7 loss: 0.697009  [  256/  306]
train() client id: f_00004-7-8 loss: 0.928150  [  288/  306]
train() client id: f_00004-8-0 loss: 0.702809  [   32/  306]
train() client id: f_00004-8-1 loss: 0.767635  [   64/  306]
train() client id: f_00004-8-2 loss: 0.759793  [   96/  306]
train() client id: f_00004-8-3 loss: 0.831363  [  128/  306]
train() client id: f_00004-8-4 loss: 0.604728  [  160/  306]
train() client id: f_00004-8-5 loss: 0.643166  [  192/  306]
train() client id: f_00004-8-6 loss: 0.810202  [  224/  306]
train() client id: f_00004-8-7 loss: 1.044742  [  256/  306]
train() client id: f_00004-8-8 loss: 0.634077  [  288/  306]
train() client id: f_00004-9-0 loss: 0.759549  [   32/  306]
train() client id: f_00004-9-1 loss: 0.739402  [   64/  306]
train() client id: f_00004-9-2 loss: 0.702681  [   96/  306]
train() client id: f_00004-9-3 loss: 0.821193  [  128/  306]
train() client id: f_00004-9-4 loss: 0.751631  [  160/  306]
train() client id: f_00004-9-5 loss: 0.839873  [  192/  306]
train() client id: f_00004-9-6 loss: 0.787381  [  224/  306]
train() client id: f_00004-9-7 loss: 0.611417  [  256/  306]
train() client id: f_00004-9-8 loss: 0.767692  [  288/  306]
train() client id: f_00004-10-0 loss: 0.644269  [   32/  306]
train() client id: f_00004-10-1 loss: 0.778873  [   64/  306]
train() client id: f_00004-10-2 loss: 0.747293  [   96/  306]
train() client id: f_00004-10-3 loss: 0.694497  [  128/  306]
train() client id: f_00004-10-4 loss: 0.722618  [  160/  306]
train() client id: f_00004-10-5 loss: 0.754209  [  192/  306]
train() client id: f_00004-10-6 loss: 0.834087  [  224/  306]
train() client id: f_00004-10-7 loss: 0.836708  [  256/  306]
train() client id: f_00004-10-8 loss: 0.753011  [  288/  306]
train() client id: f_00004-11-0 loss: 0.609260  [   32/  306]
train() client id: f_00004-11-1 loss: 0.781766  [   64/  306]
train() client id: f_00004-11-2 loss: 0.704923  [   96/  306]
train() client id: f_00004-11-3 loss: 0.745368  [  128/  306]
train() client id: f_00004-11-4 loss: 0.913171  [  160/  306]
train() client id: f_00004-11-5 loss: 0.934240  [  192/  306]
train() client id: f_00004-11-6 loss: 0.757140  [  224/  306]
train() client id: f_00004-11-7 loss: 0.642333  [  256/  306]
train() client id: f_00004-11-8 loss: 0.707147  [  288/  306]
train() client id: f_00004-12-0 loss: 0.787725  [   32/  306]
train() client id: f_00004-12-1 loss: 0.771845  [   64/  306]
train() client id: f_00004-12-2 loss: 0.842175  [   96/  306]
train() client id: f_00004-12-3 loss: 0.750608  [  128/  306]
train() client id: f_00004-12-4 loss: 0.783300  [  160/  306]
train() client id: f_00004-12-5 loss: 0.740585  [  192/  306]
train() client id: f_00004-12-6 loss: 0.715791  [  224/  306]
train() client id: f_00004-12-7 loss: 0.707908  [  256/  306]
train() client id: f_00004-12-8 loss: 0.763561  [  288/  306]
train() client id: f_00004-13-0 loss: 0.708911  [   32/  306]
train() client id: f_00004-13-1 loss: 0.798352  [   64/  306]
train() client id: f_00004-13-2 loss: 0.796411  [   96/  306]
train() client id: f_00004-13-3 loss: 0.697731  [  128/  306]
train() client id: f_00004-13-4 loss: 0.883553  [  160/  306]
train() client id: f_00004-13-5 loss: 0.768025  [  192/  306]
train() client id: f_00004-13-6 loss: 0.640535  [  224/  306]
train() client id: f_00004-13-7 loss: 0.816105  [  256/  306]
train() client id: f_00004-13-8 loss: 0.770459  [  288/  306]
train() client id: f_00004-14-0 loss: 0.690129  [   32/  306]
train() client id: f_00004-14-1 loss: 0.909085  [   64/  306]
train() client id: f_00004-14-2 loss: 0.806560  [   96/  306]
train() client id: f_00004-14-3 loss: 0.710986  [  128/  306]
train() client id: f_00004-14-4 loss: 0.731201  [  160/  306]
train() client id: f_00004-14-5 loss: 0.650992  [  192/  306]
train() client id: f_00004-14-6 loss: 0.734980  [  224/  306]
train() client id: f_00004-14-7 loss: 0.768306  [  256/  306]
train() client id: f_00004-14-8 loss: 0.912533  [  288/  306]
train() client id: f_00005-0-0 loss: 0.542657  [   32/  146]
train() client id: f_00005-0-1 loss: 0.541853  [   64/  146]
train() client id: f_00005-0-2 loss: 0.476690  [   96/  146]
train() client id: f_00005-0-3 loss: 0.392756  [  128/  146]
train() client id: f_00005-1-0 loss: 0.289398  [   32/  146]
train() client id: f_00005-1-1 loss: 0.380230  [   64/  146]
train() client id: f_00005-1-2 loss: 0.556459  [   96/  146]
train() client id: f_00005-1-3 loss: 0.527073  [  128/  146]
train() client id: f_00005-2-0 loss: 0.392505  [   32/  146]
train() client id: f_00005-2-1 loss: 0.651048  [   64/  146]
train() client id: f_00005-2-2 loss: 0.300491  [   96/  146]
train() client id: f_00005-2-3 loss: 0.438341  [  128/  146]
train() client id: f_00005-3-0 loss: 0.610992  [   32/  146]
train() client id: f_00005-3-1 loss: 0.555016  [   64/  146]
train() client id: f_00005-3-2 loss: 0.393865  [   96/  146]
train() client id: f_00005-3-3 loss: 0.457669  [  128/  146]
train() client id: f_00005-4-0 loss: 0.593953  [   32/  146]
train() client id: f_00005-4-1 loss: 0.408112  [   64/  146]
train() client id: f_00005-4-2 loss: 0.337930  [   96/  146]
train() client id: f_00005-4-3 loss: 0.449496  [  128/  146]
train() client id: f_00005-5-0 loss: 0.292677  [   32/  146]
train() client id: f_00005-5-1 loss: 0.383224  [   64/  146]
train() client id: f_00005-5-2 loss: 0.499001  [   96/  146]
train() client id: f_00005-5-3 loss: 0.519216  [  128/  146]
train() client id: f_00005-6-0 loss: 0.672494  [   32/  146]
train() client id: f_00005-6-1 loss: 0.258876  [   64/  146]
train() client id: f_00005-6-2 loss: 0.280769  [   96/  146]
train() client id: f_00005-6-3 loss: 0.583094  [  128/  146]
train() client id: f_00005-7-0 loss: 0.265295  [   32/  146]
train() client id: f_00005-7-1 loss: 0.574662  [   64/  146]
train() client id: f_00005-7-2 loss: 0.372491  [   96/  146]
train() client id: f_00005-7-3 loss: 0.762957  [  128/  146]
train() client id: f_00005-8-0 loss: 0.678202  [   32/  146]
train() client id: f_00005-8-1 loss: 0.540986  [   64/  146]
train() client id: f_00005-8-2 loss: 0.235445  [   96/  146]
train() client id: f_00005-8-3 loss: 0.300517  [  128/  146]
train() client id: f_00005-9-0 loss: 0.312479  [   32/  146]
train() client id: f_00005-9-1 loss: 0.280633  [   64/  146]
train() client id: f_00005-9-2 loss: 0.685486  [   96/  146]
train() client id: f_00005-9-3 loss: 0.703461  [  128/  146]
train() client id: f_00005-10-0 loss: 0.350699  [   32/  146]
train() client id: f_00005-10-1 loss: 0.540040  [   64/  146]
train() client id: f_00005-10-2 loss: 0.550892  [   96/  146]
train() client id: f_00005-10-3 loss: 0.354688  [  128/  146]
train() client id: f_00005-11-0 loss: 0.346337  [   32/  146]
train() client id: f_00005-11-1 loss: 0.547142  [   64/  146]
train() client id: f_00005-11-2 loss: 0.487837  [   96/  146]
train() client id: f_00005-11-3 loss: 0.392527  [  128/  146]
train() client id: f_00005-12-0 loss: 0.444272  [   32/  146]
train() client id: f_00005-12-1 loss: 0.221074  [   64/  146]
train() client id: f_00005-12-2 loss: 0.562199  [   96/  146]
train() client id: f_00005-12-3 loss: 0.371597  [  128/  146]
train() client id: f_00005-13-0 loss: 0.475133  [   32/  146]
train() client id: f_00005-13-1 loss: 0.333296  [   64/  146]
train() client id: f_00005-13-2 loss: 0.492068  [   96/  146]
train() client id: f_00005-13-3 loss: 0.321699  [  128/  146]
train() client id: f_00005-14-0 loss: 0.466438  [   32/  146]
train() client id: f_00005-14-1 loss: 0.375307  [   64/  146]
train() client id: f_00005-14-2 loss: 0.273371  [   96/  146]
train() client id: f_00005-14-3 loss: 0.581594  [  128/  146]
train() client id: f_00006-0-0 loss: 0.476767  [   32/   54]
train() client id: f_00006-1-0 loss: 0.473613  [   32/   54]
train() client id: f_00006-2-0 loss: 0.427082  [   32/   54]
train() client id: f_00006-3-0 loss: 0.480516  [   32/   54]
train() client id: f_00006-4-0 loss: 0.480641  [   32/   54]
train() client id: f_00006-5-0 loss: 0.433499  [   32/   54]
train() client id: f_00006-6-0 loss: 0.475906  [   32/   54]
train() client id: f_00006-7-0 loss: 0.491076  [   32/   54]
train() client id: f_00006-8-0 loss: 0.489034  [   32/   54]
train() client id: f_00006-9-0 loss: 0.535731  [   32/   54]
train() client id: f_00006-10-0 loss: 0.478065  [   32/   54]
train() client id: f_00006-11-0 loss: 0.465051  [   32/   54]
train() client id: f_00006-12-0 loss: 0.471214  [   32/   54]
train() client id: f_00006-13-0 loss: 0.509794  [   32/   54]
train() client id: f_00006-14-0 loss: 0.483713  [   32/   54]
train() client id: f_00007-0-0 loss: 0.601507  [   32/  179]
train() client id: f_00007-0-1 loss: 0.403761  [   64/  179]
train() client id: f_00007-0-2 loss: 0.421899  [   96/  179]
train() client id: f_00007-0-3 loss: 0.571437  [  128/  179]
train() client id: f_00007-0-4 loss: 0.527051  [  160/  179]
train() client id: f_00007-1-0 loss: 0.584292  [   32/  179]
train() client id: f_00007-1-1 loss: 0.422473  [   64/  179]
train() client id: f_00007-1-2 loss: 0.448500  [   96/  179]
train() client id: f_00007-1-3 loss: 0.469500  [  128/  179]
train() client id: f_00007-1-4 loss: 0.336319  [  160/  179]
train() client id: f_00007-2-0 loss: 0.399037  [   32/  179]
train() client id: f_00007-2-1 loss: 0.418954  [   64/  179]
train() client id: f_00007-2-2 loss: 0.501777  [   96/  179]
train() client id: f_00007-2-3 loss: 0.311135  [  128/  179]
train() client id: f_00007-2-4 loss: 0.506977  [  160/  179]
train() client id: f_00007-3-0 loss: 0.438497  [   32/  179]
train() client id: f_00007-3-1 loss: 0.664275  [   64/  179]
train() client id: f_00007-3-2 loss: 0.460330  [   96/  179]
train() client id: f_00007-3-3 loss: 0.310057  [  128/  179]
train() client id: f_00007-3-4 loss: 0.485064  [  160/  179]
train() client id: f_00007-4-0 loss: 0.469559  [   32/  179]
train() client id: f_00007-4-1 loss: 0.415090  [   64/  179]
train() client id: f_00007-4-2 loss: 0.355360  [   96/  179]
train() client id: f_00007-4-3 loss: 0.413657  [  128/  179]
train() client id: f_00007-4-4 loss: 0.629496  [  160/  179]
train() client id: f_00007-5-0 loss: 0.289383  [   32/  179]
train() client id: f_00007-5-1 loss: 0.404010  [   64/  179]
train() client id: f_00007-5-2 loss: 0.402373  [   96/  179]
train() client id: f_00007-5-3 loss: 0.414608  [  128/  179]
train() client id: f_00007-5-4 loss: 0.622335  [  160/  179]
train() client id: f_00007-6-0 loss: 0.261070  [   32/  179]
train() client id: f_00007-6-1 loss: 0.379919  [   64/  179]
train() client id: f_00007-6-2 loss: 0.495857  [   96/  179]
train() client id: f_00007-6-3 loss: 0.620646  [  128/  179]
train() client id: f_00007-6-4 loss: 0.415963  [  160/  179]
train() client id: f_00007-7-0 loss: 0.415203  [   32/  179]
train() client id: f_00007-7-1 loss: 0.374586  [   64/  179]
train() client id: f_00007-7-2 loss: 0.486548  [   96/  179]
train() client id: f_00007-7-3 loss: 0.282303  [  128/  179]
train() client id: f_00007-7-4 loss: 0.598380  [  160/  179]
train() client id: f_00007-8-0 loss: 0.426374  [   32/  179]
train() client id: f_00007-8-1 loss: 0.284678  [   64/  179]
train() client id: f_00007-8-2 loss: 0.259588  [   96/  179]
train() client id: f_00007-8-3 loss: 0.594573  [  128/  179]
train() client id: f_00007-8-4 loss: 0.464795  [  160/  179]
train() client id: f_00007-9-0 loss: 0.268395  [   32/  179]
train() client id: f_00007-9-1 loss: 0.440415  [   64/  179]
train() client id: f_00007-9-2 loss: 0.280985  [   96/  179]
train() client id: f_00007-9-3 loss: 0.478169  [  128/  179]
train() client id: f_00007-9-4 loss: 0.421757  [  160/  179]
train() client id: f_00007-10-0 loss: 0.263876  [   32/  179]
train() client id: f_00007-10-1 loss: 0.352018  [   64/  179]
train() client id: f_00007-10-2 loss: 0.257499  [   96/  179]
train() client id: f_00007-10-3 loss: 0.390458  [  128/  179]
train() client id: f_00007-10-4 loss: 0.517893  [  160/  179]
train() client id: f_00007-11-0 loss: 0.351490  [   32/  179]
train() client id: f_00007-11-1 loss: 0.569220  [   64/  179]
train() client id: f_00007-11-2 loss: 0.259845  [   96/  179]
train() client id: f_00007-11-3 loss: 0.267109  [  128/  179]
train() client id: f_00007-11-4 loss: 0.514494  [  160/  179]
train() client id: f_00007-12-0 loss: 0.273877  [   32/  179]
train() client id: f_00007-12-1 loss: 0.360544  [   64/  179]
train() client id: f_00007-12-2 loss: 0.483916  [   96/  179]
train() client id: f_00007-12-3 loss: 0.231525  [  128/  179]
train() client id: f_00007-12-4 loss: 0.450829  [  160/  179]
train() client id: f_00007-13-0 loss: 0.503659  [   32/  179]
train() client id: f_00007-13-1 loss: 0.270975  [   64/  179]
train() client id: f_00007-13-2 loss: 0.446844  [   96/  179]
train() client id: f_00007-13-3 loss: 0.352751  [  128/  179]
train() client id: f_00007-13-4 loss: 0.537787  [  160/  179]
train() client id: f_00007-14-0 loss: 0.429411  [   32/  179]
train() client id: f_00007-14-1 loss: 0.529465  [   64/  179]
train() client id: f_00007-14-2 loss: 0.245325  [   96/  179]
train() client id: f_00007-14-3 loss: 0.388175  [  128/  179]
train() client id: f_00007-14-4 loss: 0.428925  [  160/  179]
train() client id: f_00008-0-0 loss: 0.791037  [   32/  130]
train() client id: f_00008-0-1 loss: 0.689238  [   64/  130]
train() client id: f_00008-0-2 loss: 0.870377  [   96/  130]
train() client id: f_00008-0-3 loss: 0.713538  [  128/  130]
train() client id: f_00008-1-0 loss: 0.743373  [   32/  130]
train() client id: f_00008-1-1 loss: 0.807014  [   64/  130]
train() client id: f_00008-1-2 loss: 0.775530  [   96/  130]
train() client id: f_00008-1-3 loss: 0.727972  [  128/  130]
train() client id: f_00008-2-0 loss: 0.753672  [   32/  130]
train() client id: f_00008-2-1 loss: 0.718906  [   64/  130]
train() client id: f_00008-2-2 loss: 0.764513  [   96/  130]
train() client id: f_00008-2-3 loss: 0.791018  [  128/  130]
train() client id: f_00008-3-0 loss: 0.698879  [   32/  130]
train() client id: f_00008-3-1 loss: 0.799128  [   64/  130]
train() client id: f_00008-3-2 loss: 0.690490  [   96/  130]
train() client id: f_00008-3-3 loss: 0.823214  [  128/  130]
train() client id: f_00008-4-0 loss: 0.800312  [   32/  130]
train() client id: f_00008-4-1 loss: 0.691496  [   64/  130]
train() client id: f_00008-4-2 loss: 0.705159  [   96/  130]
train() client id: f_00008-4-3 loss: 0.849513  [  128/  130]
train() client id: f_00008-5-0 loss: 0.797179  [   32/  130]
train() client id: f_00008-5-1 loss: 0.660397  [   64/  130]
train() client id: f_00008-5-2 loss: 0.810737  [   96/  130]
train() client id: f_00008-5-3 loss: 0.780502  [  128/  130]
train() client id: f_00008-6-0 loss: 0.847411  [   32/  130]
train() client id: f_00008-6-1 loss: 0.756758  [   64/  130]
train() client id: f_00008-6-2 loss: 0.733689  [   96/  130]
train() client id: f_00008-6-3 loss: 0.715784  [  128/  130]
train() client id: f_00008-7-0 loss: 0.734478  [   32/  130]
train() client id: f_00008-7-1 loss: 0.754197  [   64/  130]
train() client id: f_00008-7-2 loss: 0.828091  [   96/  130]
train() client id: f_00008-7-3 loss: 0.739251  [  128/  130]
train() client id: f_00008-8-0 loss: 0.686826  [   32/  130]
train() client id: f_00008-8-1 loss: 0.808762  [   64/  130]
train() client id: f_00008-8-2 loss: 0.814673  [   96/  130]
train() client id: f_00008-8-3 loss: 0.723035  [  128/  130]
train() client id: f_00008-9-0 loss: 0.805806  [   32/  130]
train() client id: f_00008-9-1 loss: 0.794676  [   64/  130]
train() client id: f_00008-9-2 loss: 0.729538  [   96/  130]
train() client id: f_00008-9-3 loss: 0.691443  [  128/  130]
train() client id: f_00008-10-0 loss: 0.908674  [   32/  130]
train() client id: f_00008-10-1 loss: 0.670364  [   64/  130]
train() client id: f_00008-10-2 loss: 0.775000  [   96/  130]
train() client id: f_00008-10-3 loss: 0.702135  [  128/  130]
train() client id: f_00008-11-0 loss: 0.817841  [   32/  130]
train() client id: f_00008-11-1 loss: 0.761908  [   64/  130]
train() client id: f_00008-11-2 loss: 0.679868  [   96/  130]
train() client id: f_00008-11-3 loss: 0.748264  [  128/  130]
train() client id: f_00008-12-0 loss: 0.728631  [   32/  130]
train() client id: f_00008-12-1 loss: 0.811685  [   64/  130]
train() client id: f_00008-12-2 loss: 0.690392  [   96/  130]
train() client id: f_00008-12-3 loss: 0.817700  [  128/  130]
train() client id: f_00008-13-0 loss: 0.781715  [   32/  130]
train() client id: f_00008-13-1 loss: 0.776824  [   64/  130]
train() client id: f_00008-13-2 loss: 0.779489  [   96/  130]
train() client id: f_00008-13-3 loss: 0.719769  [  128/  130]
train() client id: f_00008-14-0 loss: 0.689162  [   32/  130]
train() client id: f_00008-14-1 loss: 0.879174  [   64/  130]
train() client id: f_00008-14-2 loss: 0.779967  [   96/  130]
train() client id: f_00008-14-3 loss: 0.704173  [  128/  130]
train() client id: f_00009-0-0 loss: 1.075344  [   32/  118]
train() client id: f_00009-0-1 loss: 1.053819  [   64/  118]
train() client id: f_00009-0-2 loss: 0.976488  [   96/  118]
train() client id: f_00009-1-0 loss: 0.915026  [   32/  118]
train() client id: f_00009-1-1 loss: 1.109384  [   64/  118]
train() client id: f_00009-1-2 loss: 1.025345  [   96/  118]
train() client id: f_00009-2-0 loss: 0.951648  [   32/  118]
train() client id: f_00009-2-1 loss: 0.991422  [   64/  118]
train() client id: f_00009-2-2 loss: 0.868937  [   96/  118]
train() client id: f_00009-3-0 loss: 0.968910  [   32/  118]
train() client id: f_00009-3-1 loss: 1.013936  [   64/  118]
train() client id: f_00009-3-2 loss: 0.805459  [   96/  118]
train() client id: f_00009-4-0 loss: 1.045667  [   32/  118]
train() client id: f_00009-4-1 loss: 0.862867  [   64/  118]
train() client id: f_00009-4-2 loss: 0.870773  [   96/  118]
train() client id: f_00009-5-0 loss: 0.998561  [   32/  118]
train() client id: f_00009-5-1 loss: 0.884311  [   64/  118]
train() client id: f_00009-5-2 loss: 0.639212  [   96/  118]
train() client id: f_00009-6-0 loss: 0.968420  [   32/  118]
train() client id: f_00009-6-1 loss: 0.764708  [   64/  118]
train() client id: f_00009-6-2 loss: 0.834238  [   96/  118]
train() client id: f_00009-7-0 loss: 0.844410  [   32/  118]
train() client id: f_00009-7-1 loss: 0.721744  [   64/  118]
train() client id: f_00009-7-2 loss: 0.752688  [   96/  118]
train() client id: f_00009-8-0 loss: 0.928267  [   32/  118]
train() client id: f_00009-8-1 loss: 0.705714  [   64/  118]
train() client id: f_00009-8-2 loss: 0.707347  [   96/  118]
train() client id: f_00009-9-0 loss: 0.765585  [   32/  118]
train() client id: f_00009-9-1 loss: 0.686922  [   64/  118]
train() client id: f_00009-9-2 loss: 0.747098  [   96/  118]
train() client id: f_00009-10-0 loss: 0.847056  [   32/  118]
train() client id: f_00009-10-1 loss: 0.682654  [   64/  118]
train() client id: f_00009-10-2 loss: 0.795041  [   96/  118]
train() client id: f_00009-11-0 loss: 0.728868  [   32/  118]
train() client id: f_00009-11-1 loss: 0.743815  [   64/  118]
train() client id: f_00009-11-2 loss: 0.879081  [   96/  118]
train() client id: f_00009-12-0 loss: 0.790958  [   32/  118]
train() client id: f_00009-12-1 loss: 0.586597  [   64/  118]
train() client id: f_00009-12-2 loss: 0.782771  [   96/  118]
train() client id: f_00009-13-0 loss: 0.627940  [   32/  118]
train() client id: f_00009-13-1 loss: 0.760074  [   64/  118]
train() client id: f_00009-13-2 loss: 0.921440  [   96/  118]
train() client id: f_00009-14-0 loss: 0.794148  [   32/  118]
train() client id: f_00009-14-1 loss: 0.872645  [   64/  118]
train() client id: f_00009-14-2 loss: 0.631395  [   96/  118]
At round 41 accuracy: 0.6445623342175066
At round 41 training accuracy: 0.5942320590207915
At round 41 training loss: 0.8284096100885423
update_location
xs = 8.927491 326.223621 5.882650 0.934260 -242.581990 -90.230757 -50.849135 -5.143845 -265.120581 20.134486 
ys = -317.390647 7.291448 215.684448 -37.290817 -9.642386 0.794442 -1.381692 211.628436 25.881276 -752.232496 
xs mean: -29.182379970521218
ys mean: -65.66579882624052
dists_uav = 332.891158 341.284362 237.811662 106.730867 262.562368 134.693061 112.194223 234.121879 284.532534 759.117333 
uav_gains = -118.159512 -118.676282 -110.298748 -100.707291 -112.397259 -103.234698 -101.249345 -110.013338 -114.388244 -128.970253 
uav_gains_db_mean: -111.80949703595793
dists_bs = 525.621415 528.536921 185.401611 275.717709 196.621810 193.735389 215.694086 173.760628 174.235797 947.542911 
bs_gains = -115.745905 -115.813168 -103.074237 -107.900070 -103.788746 -103.608910 -104.914530 -102.285698 -102.318906 -122.911828 
bs_gains_db_mean: -108.23619977278068
Round 42
-------------------------------
ene_coms = [0.0221435  0.02353703 0.00722232 0.00649297 0.00747515 0.00727763
 0.00664688 0.00696209 0.00697267 0.04228281]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.6039868  11.46517461  5.31200551  1.89764561 12.96446359  6.23737277
  2.36208864  7.61987042  5.56068933  5.5484117 ]
obj_prev = 64.57170896286192
eta_min = 1.3855764232437774e-17	eta_max = 0.7883548297565753
af = 13.47090363962996	bf = 1.8901880536909468	zeta = 14.817994003592956	eta = 0.9090909090909091
af = 13.47090363962996	bf = 1.8901880536909468	zeta = 34.91512531881893	eta = 0.38581856764435746
af = 13.47090363962996	bf = 1.8901880536909468	zeta = 23.966396498860938	eta = 0.5620746381405397
af = 13.47090363962996	bf = 1.8901880536909468	zeta = 22.038124263732612	eta = 0.6112545459142621
af = 13.47090363962996	bf = 1.8901880536909468	zeta = 21.919561591569312	eta = 0.6145608151584168
af = 13.47090363962996	bf = 1.8901880536909468	zeta = 21.919053994959587	eta = 0.6145750470219961
af = 13.47090363962996	bf = 1.8901880536909468	zeta = 21.919053985588125	eta = 0.6145750472847569
eta = 0.6145750472847569
ene_coms = [0.0221435  0.02353703 0.00722232 0.00649297 0.00747515 0.00727763
 0.00664688 0.00696209 0.00697267 0.04228281]
ene_comp = [0.04017157 0.08448782 0.03953392 0.01370934 0.09755952 0.04654801
 0.01721639 0.05706913 0.04144686 0.03762099]
ene_total = [2.23046958 3.86657905 1.67356603 0.72310984 3.75955036 1.92660379
 0.85414789 2.29189637 1.73310061 2.86003048]
ti_comp = [0.55693983 0.54300454 0.70615163 0.7134452  0.70362333 0.70559854
 0.71190608 0.708754   0.70864813 0.35554674]
ti_coms = [0.22143504 0.23537033 0.07222323 0.06492967 0.07475154 0.07277633
 0.06646879 0.06962087 0.06972674 0.42282812]
t_total = [27.8606987 27.8606987 27.8606987 27.8606987 27.8606987 27.8606987
 27.8606987 27.8606987 27.8606987 27.8606987]
ene_coms = [0.0221435  0.02353703 0.00722232 0.00649297 0.00747515 0.00727763
 0.00664688 0.00696209 0.00697267 0.04228281]
ene_comp = [1.30623055e-05 1.27836671e-04 7.74450871e-06 3.16379899e-07
 1.17221851e-04 1.26609990e-05 6.29304199e-07 2.31255890e-05
 8.86122012e-06 2.63255446e-05]
ene_total = [0.79305929 0.84704661 0.25878886 0.23241683 0.27175711 0.26094455
 0.23793705 0.25002465 0.24989303 1.51438875]
optimize_network iter = 0 obj = 4.916256730517617
eta = 0.6145750472847569
freqs = [36064550.04783657 77796608.11984546 27992518.03450371  9607847.18626642
 69326527.5448494  32984767.93545427 12091754.13148032 40260184.32609583
 29243608.37942735 52905832.40553154]
eta_min = 0.6145750472847575	eta_max = 0.6145750472847564
af = 0.00957216982492007	bf = 1.8901880536909468	zeta = 0.010529386807412078	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.0221435  0.02353703 0.00722232 0.00649297 0.00747515 0.00727763
 0.00664688 0.00696209 0.00697267 0.04228281]
ene_comp = [2.55733097e-06 2.50277931e-05 1.51621565e-06 6.19406825e-08
 2.29496295e-05 2.47876342e-06 1.23204830e-07 4.52751508e-06
 1.73484480e-06 5.15400064e-06]
ene_total = [3.36071906 3.57560041 1.09623536 0.98533307 1.13785562 1.10477479
 1.00869885 1.05720082 1.05838367 6.41730252]
ti_comp = [0.55693983 0.54300454 0.70615163 0.7134452  0.70362333 0.70559854
 0.71190608 0.708754   0.70864813 0.35554674]
ti_coms = [0.22143504 0.23537033 0.07222323 0.06492967 0.07475154 0.07277633
 0.06646879 0.06962087 0.06972674 0.42282812]
t_total = [27.8606987 27.8606987 27.8606987 27.8606987 27.8606987 27.8606987
 27.8606987 27.8606987 27.8606987 27.8606987]
ene_coms = [0.0221435  0.02353703 0.00722232 0.00649297 0.00747515 0.00727763
 0.00664688 0.00696209 0.00697267 0.04228281]
ene_comp = [1.30623055e-05 1.27836671e-04 7.74450871e-06 3.16379899e-07
 1.17221851e-04 1.26609990e-05 6.29304199e-07 2.31255890e-05
 8.86122012e-06 2.63255446e-05]
ene_total = [0.79305929 0.84704661 0.25878886 0.23241683 0.27175711 0.26094455
 0.23793705 0.25002465 0.24989303 1.51438875]
optimize_network iter = 1 obj = 4.9162567305176115
eta = 0.6145750472847564
freqs = [36064550.04783656 77796608.11984544 27992518.03450371  9607847.18626642
 69326527.5448494  32984767.93545427 12091754.13148032 40260184.32609583
 29243608.37942735 52905832.40553147]
Done!
ene_coms = [0.0221435  0.02353703 0.00722232 0.00649297 0.00747515 0.00727763
 0.00664688 0.00696209 0.00697267 0.04228281]
ene_comp = [1.22911592e-05 1.20289706e-04 7.28730391e-06 2.97702096e-07
 1.10301542e-04 1.19135443e-05 5.92152598e-07 2.17603468e-05
 8.33808915e-06 2.47713899e-05]
ene_total = [0.0221558  0.02365732 0.00722961 0.00649326 0.00758546 0.00728955
 0.00664747 0.00698385 0.00698101 0.04230758]
At round 42 energy consumption: 0.13733090977143841
At round 42 eta: 0.6145750472847564
At round 42 a_n: 13.79567728354305
At round 42 local rounds: 15.941098655117342
At round 42 global rounds: 35.79342018817203
gradient difference: 0.48791441321372986
train() client id: f_00000-0-0 loss: 0.918487  [   32/  126]
train() client id: f_00000-0-1 loss: 1.047307  [   64/  126]
train() client id: f_00000-0-2 loss: 1.009891  [   96/  126]
train() client id: f_00000-1-0 loss: 0.965324  [   32/  126]
train() client id: f_00000-1-1 loss: 1.010885  [   64/  126]
train() client id: f_00000-1-2 loss: 0.900981  [   96/  126]
train() client id: f_00000-2-0 loss: 0.850168  [   32/  126]
train() client id: f_00000-2-1 loss: 0.882164  [   64/  126]
train() client id: f_00000-2-2 loss: 1.063181  [   96/  126]
train() client id: f_00000-3-0 loss: 0.712991  [   32/  126]
train() client id: f_00000-3-1 loss: 0.988730  [   64/  126]
train() client id: f_00000-3-2 loss: 0.903419  [   96/  126]
train() client id: f_00000-4-0 loss: 0.878927  [   32/  126]
train() client id: f_00000-4-1 loss: 0.707130  [   64/  126]
train() client id: f_00000-4-2 loss: 0.947506  [   96/  126]
train() client id: f_00000-5-0 loss: 0.699118  [   32/  126]
train() client id: f_00000-5-1 loss: 1.003518  [   64/  126]
train() client id: f_00000-5-2 loss: 0.768395  [   96/  126]
train() client id: f_00000-6-0 loss: 0.824265  [   32/  126]
train() client id: f_00000-6-1 loss: 0.739544  [   64/  126]
train() client id: f_00000-6-2 loss: 0.743382  [   96/  126]
train() client id: f_00000-7-0 loss: 0.784618  [   32/  126]
train() client id: f_00000-7-1 loss: 0.778490  [   64/  126]
train() client id: f_00000-7-2 loss: 0.882996  [   96/  126]
train() client id: f_00000-8-0 loss: 0.734114  [   32/  126]
train() client id: f_00000-8-1 loss: 0.794570  [   64/  126]
train() client id: f_00000-8-2 loss: 0.821862  [   96/  126]
train() client id: f_00000-9-0 loss: 0.726849  [   32/  126]
train() client id: f_00000-9-1 loss: 0.928655  [   64/  126]
train() client id: f_00000-9-2 loss: 0.827021  [   96/  126]
train() client id: f_00000-10-0 loss: 0.696338  [   32/  126]
train() client id: f_00000-10-1 loss: 0.811710  [   64/  126]
train() client id: f_00000-10-2 loss: 0.876525  [   96/  126]
train() client id: f_00000-11-0 loss: 0.686698  [   32/  126]
train() client id: f_00000-11-1 loss: 0.829279  [   64/  126]
train() client id: f_00000-11-2 loss: 0.834375  [   96/  126]
train() client id: f_00000-12-0 loss: 0.810622  [   32/  126]
train() client id: f_00000-12-1 loss: 0.875199  [   64/  126]
train() client id: f_00000-12-2 loss: 0.818113  [   96/  126]
train() client id: f_00000-13-0 loss: 0.759033  [   32/  126]
train() client id: f_00000-13-1 loss: 0.800323  [   64/  126]
train() client id: f_00000-13-2 loss: 0.838701  [   96/  126]
train() client id: f_00000-14-0 loss: 0.917958  [   32/  126]
train() client id: f_00000-14-1 loss: 0.779310  [   64/  126]
train() client id: f_00000-14-2 loss: 0.798233  [   96/  126]
train() client id: f_00001-0-0 loss: 0.336711  [   32/  265]
train() client id: f_00001-0-1 loss: 0.481009  [   64/  265]
train() client id: f_00001-0-2 loss: 0.362130  [   96/  265]
train() client id: f_00001-0-3 loss: 0.417335  [  128/  265]
train() client id: f_00001-0-4 loss: 0.448182  [  160/  265]
train() client id: f_00001-0-5 loss: 0.356067  [  192/  265]
train() client id: f_00001-0-6 loss: 0.413016  [  224/  265]
train() client id: f_00001-0-7 loss: 0.385532  [  256/  265]
train() client id: f_00001-1-0 loss: 0.435931  [   32/  265]
train() client id: f_00001-1-1 loss: 0.375553  [   64/  265]
train() client id: f_00001-1-2 loss: 0.407315  [   96/  265]
train() client id: f_00001-1-3 loss: 0.517439  [  128/  265]
train() client id: f_00001-1-4 loss: 0.415784  [  160/  265]
train() client id: f_00001-1-5 loss: 0.445901  [  192/  265]
train() client id: f_00001-1-6 loss: 0.317851  [  224/  265]
train() client id: f_00001-1-7 loss: 0.353792  [  256/  265]
train() client id: f_00001-2-0 loss: 0.491611  [   32/  265]
train() client id: f_00001-2-1 loss: 0.308347  [   64/  265]
train() client id: f_00001-2-2 loss: 0.348374  [   96/  265]
train() client id: f_00001-2-3 loss: 0.408640  [  128/  265]
train() client id: f_00001-2-4 loss: 0.458995  [  160/  265]
train() client id: f_00001-2-5 loss: 0.539209  [  192/  265]
train() client id: f_00001-2-6 loss: 0.327123  [  224/  265]
train() client id: f_00001-2-7 loss: 0.343880  [  256/  265]
train() client id: f_00001-3-0 loss: 0.558833  [   32/  265]
train() client id: f_00001-3-1 loss: 0.365928  [   64/  265]
train() client id: f_00001-3-2 loss: 0.352706  [   96/  265]
train() client id: f_00001-3-3 loss: 0.441132  [  128/  265]
train() client id: f_00001-3-4 loss: 0.346306  [  160/  265]
train() client id: f_00001-3-5 loss: 0.312472  [  192/  265]
train() client id: f_00001-3-6 loss: 0.372461  [  224/  265]
train() client id: f_00001-3-7 loss: 0.382554  [  256/  265]
train() client id: f_00001-4-0 loss: 0.393703  [   32/  265]
train() client id: f_00001-4-1 loss: 0.429502  [   64/  265]
train() client id: f_00001-4-2 loss: 0.432589  [   96/  265]
train() client id: f_00001-4-3 loss: 0.370781  [  128/  265]
train() client id: f_00001-4-4 loss: 0.411607  [  160/  265]
train() client id: f_00001-4-5 loss: 0.355979  [  192/  265]
train() client id: f_00001-4-6 loss: 0.364117  [  224/  265]
train() client id: f_00001-4-7 loss: 0.376944  [  256/  265]
train() client id: f_00001-5-0 loss: 0.602342  [   32/  265]
train() client id: f_00001-5-1 loss: 0.398497  [   64/  265]
train() client id: f_00001-5-2 loss: 0.370386  [   96/  265]
train() client id: f_00001-5-3 loss: 0.466151  [  128/  265]
train() client id: f_00001-5-4 loss: 0.357325  [  160/  265]
train() client id: f_00001-5-5 loss: 0.294580  [  192/  265]
train() client id: f_00001-5-6 loss: 0.310440  [  224/  265]
train() client id: f_00001-5-7 loss: 0.308974  [  256/  265]
train() client id: f_00001-6-0 loss: 0.331870  [   32/  265]
train() client id: f_00001-6-1 loss: 0.507640  [   64/  265]
train() client id: f_00001-6-2 loss: 0.382743  [   96/  265]
train() client id: f_00001-6-3 loss: 0.286081  [  128/  265]
train() client id: f_00001-6-4 loss: 0.350063  [  160/  265]
train() client id: f_00001-6-5 loss: 0.422430  [  192/  265]
train() client id: f_00001-6-6 loss: 0.402928  [  224/  265]
train() client id: f_00001-6-7 loss: 0.342745  [  256/  265]
train() client id: f_00001-7-0 loss: 0.389678  [   32/  265]
train() client id: f_00001-7-1 loss: 0.314238  [   64/  265]
train() client id: f_00001-7-2 loss: 0.295228  [   96/  265]
train() client id: f_00001-7-3 loss: 0.368523  [  128/  265]
train() client id: f_00001-7-4 loss: 0.359495  [  160/  265]
train() client id: f_00001-7-5 loss: 0.503114  [  192/  265]
train() client id: f_00001-7-6 loss: 0.356536  [  224/  265]
train() client id: f_00001-7-7 loss: 0.406834  [  256/  265]
train() client id: f_00001-8-0 loss: 0.700415  [   32/  265]
train() client id: f_00001-8-1 loss: 0.284745  [   64/  265]
train() client id: f_00001-8-2 loss: 0.375126  [   96/  265]
train() client id: f_00001-8-3 loss: 0.356515  [  128/  265]
train() client id: f_00001-8-4 loss: 0.299299  [  160/  265]
train() client id: f_00001-8-5 loss: 0.374207  [  192/  265]
train() client id: f_00001-8-6 loss: 0.334452  [  224/  265]
train() client id: f_00001-8-7 loss: 0.322024  [  256/  265]
train() client id: f_00001-9-0 loss: 0.272505  [   32/  265]
train() client id: f_00001-9-1 loss: 0.341371  [   64/  265]
train() client id: f_00001-9-2 loss: 0.422126  [   96/  265]
train() client id: f_00001-9-3 loss: 0.382109  [  128/  265]
train() client id: f_00001-9-4 loss: 0.278148  [  160/  265]
train() client id: f_00001-9-5 loss: 0.496816  [  192/  265]
train() client id: f_00001-9-6 loss: 0.414005  [  224/  265]
train() client id: f_00001-9-7 loss: 0.430172  [  256/  265]
train() client id: f_00001-10-0 loss: 0.381782  [   32/  265]
train() client id: f_00001-10-1 loss: 0.297187  [   64/  265]
train() client id: f_00001-10-2 loss: 0.437198  [   96/  265]
train() client id: f_00001-10-3 loss: 0.528988  [  128/  265]
train() client id: f_00001-10-4 loss: 0.347931  [  160/  265]
train() client id: f_00001-10-5 loss: 0.392946  [  192/  265]
train() client id: f_00001-10-6 loss: 0.301636  [  224/  265]
train() client id: f_00001-10-7 loss: 0.345765  [  256/  265]
train() client id: f_00001-11-0 loss: 0.283792  [   32/  265]
train() client id: f_00001-11-1 loss: 0.307728  [   64/  265]
train() client id: f_00001-11-2 loss: 0.290992  [   96/  265]
train() client id: f_00001-11-3 loss: 0.503995  [  128/  265]
train() client id: f_00001-11-4 loss: 0.281624  [  160/  265]
train() client id: f_00001-11-5 loss: 0.332272  [  192/  265]
train() client id: f_00001-11-6 loss: 0.454928  [  224/  265]
train() client id: f_00001-11-7 loss: 0.486234  [  256/  265]
train() client id: f_00001-12-0 loss: 0.355810  [   32/  265]
train() client id: f_00001-12-1 loss: 0.368736  [   64/  265]
train() client id: f_00001-12-2 loss: 0.372689  [   96/  265]
train() client id: f_00001-12-3 loss: 0.282386  [  128/  265]
train() client id: f_00001-12-4 loss: 0.502624  [  160/  265]
train() client id: f_00001-12-5 loss: 0.261882  [  192/  265]
train() client id: f_00001-12-6 loss: 0.383799  [  224/  265]
train() client id: f_00001-12-7 loss: 0.487028  [  256/  265]
train() client id: f_00001-13-0 loss: 0.281669  [   32/  265]
train() client id: f_00001-13-1 loss: 0.368645  [   64/  265]
train() client id: f_00001-13-2 loss: 0.526048  [   96/  265]
train() client id: f_00001-13-3 loss: 0.267446  [  128/  265]
train() client id: f_00001-13-4 loss: 0.498616  [  160/  265]
train() client id: f_00001-13-5 loss: 0.319162  [  192/  265]
train() client id: f_00001-13-6 loss: 0.445736  [  224/  265]
train() client id: f_00001-13-7 loss: 0.300224  [  256/  265]
train() client id: f_00001-14-0 loss: 0.408556  [   32/  265]
train() client id: f_00001-14-1 loss: 0.288731  [   64/  265]
train() client id: f_00001-14-2 loss: 0.336328  [   96/  265]
train() client id: f_00001-14-3 loss: 0.307656  [  128/  265]
train() client id: f_00001-14-4 loss: 0.424774  [  160/  265]
train() client id: f_00001-14-5 loss: 0.288334  [  192/  265]
train() client id: f_00001-14-6 loss: 0.483026  [  224/  265]
train() client id: f_00001-14-7 loss: 0.470129  [  256/  265]
train() client id: f_00002-0-0 loss: 1.333168  [   32/  124]
train() client id: f_00002-0-1 loss: 1.251827  [   64/  124]
train() client id: f_00002-0-2 loss: 1.106486  [   96/  124]
train() client id: f_00002-1-0 loss: 1.344861  [   32/  124]
train() client id: f_00002-1-1 loss: 1.062643  [   64/  124]
train() client id: f_00002-1-2 loss: 1.069578  [   96/  124]
train() client id: f_00002-2-0 loss: 0.946365  [   32/  124]
train() client id: f_00002-2-1 loss: 1.317777  [   64/  124]
train() client id: f_00002-2-2 loss: 1.208904  [   96/  124]
train() client id: f_00002-3-0 loss: 1.242982  [   32/  124]
train() client id: f_00002-3-1 loss: 1.280591  [   64/  124]
train() client id: f_00002-3-2 loss: 0.945559  [   96/  124]
train() client id: f_00002-4-0 loss: 1.075833  [   32/  124]
train() client id: f_00002-4-1 loss: 1.159239  [   64/  124]
train() client id: f_00002-4-2 loss: 0.970429  [   96/  124]
train() client id: f_00002-5-0 loss: 1.085830  [   32/  124]
train() client id: f_00002-5-1 loss: 0.930425  [   64/  124]
train() client id: f_00002-5-2 loss: 1.126457  [   96/  124]
train() client id: f_00002-6-0 loss: 1.009404  [   32/  124]
train() client id: f_00002-6-1 loss: 0.940922  [   64/  124]
train() client id: f_00002-6-2 loss: 1.163646  [   96/  124]
train() client id: f_00002-7-0 loss: 1.245003  [   32/  124]
train() client id: f_00002-7-1 loss: 1.013586  [   64/  124]
train() client id: f_00002-7-2 loss: 0.906696  [   96/  124]
train() client id: f_00002-8-0 loss: 1.198189  [   32/  124]
train() client id: f_00002-8-1 loss: 0.872904  [   64/  124]
train() client id: f_00002-8-2 loss: 1.073169  [   96/  124]
train() client id: f_00002-9-0 loss: 0.945115  [   32/  124]
train() client id: f_00002-9-1 loss: 1.002131  [   64/  124]
train() client id: f_00002-9-2 loss: 1.044685  [   96/  124]
train() client id: f_00002-10-0 loss: 0.974371  [   32/  124]
train() client id: f_00002-10-1 loss: 1.055955  [   64/  124]
train() client id: f_00002-10-2 loss: 0.920347  [   96/  124]
train() client id: f_00002-11-0 loss: 0.875793  [   32/  124]
train() client id: f_00002-11-1 loss: 1.126631  [   64/  124]
train() client id: f_00002-11-2 loss: 0.899070  [   96/  124]
train() client id: f_00002-12-0 loss: 0.859203  [   32/  124]
train() client id: f_00002-12-1 loss: 1.082487  [   64/  124]
train() client id: f_00002-12-2 loss: 0.999197  [   96/  124]
train() client id: f_00002-13-0 loss: 1.164309  [   32/  124]
train() client id: f_00002-13-1 loss: 1.130118  [   64/  124]
train() client id: f_00002-13-2 loss: 0.816036  [   96/  124]
train() client id: f_00002-14-0 loss: 0.975564  [   32/  124]
train() client id: f_00002-14-1 loss: 0.930523  [   64/  124]
train() client id: f_00002-14-2 loss: 0.977333  [   96/  124]
train() client id: f_00003-0-0 loss: 0.544962  [   32/   43]
train() client id: f_00003-1-0 loss: 0.403626  [   32/   43]
train() client id: f_00003-2-0 loss: 0.632869  [   32/   43]
train() client id: f_00003-3-0 loss: 0.598652  [   32/   43]
train() client id: f_00003-4-0 loss: 0.532148  [   32/   43]
train() client id: f_00003-5-0 loss: 0.479148  [   32/   43]
train() client id: f_00003-6-0 loss: 0.623110  [   32/   43]
train() client id: f_00003-7-0 loss: 0.468691  [   32/   43]
train() client id: f_00003-8-0 loss: 0.465442  [   32/   43]
train() client id: f_00003-9-0 loss: 0.584438  [   32/   43]
train() client id: f_00003-10-0 loss: 0.498182  [   32/   43]
train() client id: f_00003-11-0 loss: 0.456825  [   32/   43]
train() client id: f_00003-12-0 loss: 0.541877  [   32/   43]
train() client id: f_00003-13-0 loss: 0.506267  [   32/   43]
train() client id: f_00003-14-0 loss: 0.490805  [   32/   43]
train() client id: f_00004-0-0 loss: 0.657911  [   32/  306]
train() client id: f_00004-0-1 loss: 0.717132  [   64/  306]
train() client id: f_00004-0-2 loss: 0.925122  [   96/  306]
train() client id: f_00004-0-3 loss: 0.677713  [  128/  306]
train() client id: f_00004-0-4 loss: 0.885841  [  160/  306]
train() client id: f_00004-0-5 loss: 0.724937  [  192/  306]
train() client id: f_00004-0-6 loss: 0.845900  [  224/  306]
train() client id: f_00004-0-7 loss: 0.717342  [  256/  306]
train() client id: f_00004-0-8 loss: 0.626261  [  288/  306]
train() client id: f_00004-1-0 loss: 0.744101  [   32/  306]
train() client id: f_00004-1-1 loss: 0.719492  [   64/  306]
train() client id: f_00004-1-2 loss: 0.634820  [   96/  306]
train() client id: f_00004-1-3 loss: 0.739985  [  128/  306]
train() client id: f_00004-1-4 loss: 0.840146  [  160/  306]
train() client id: f_00004-1-5 loss: 0.548662  [  192/  306]
train() client id: f_00004-1-6 loss: 0.720913  [  224/  306]
train() client id: f_00004-1-7 loss: 0.789749  [  256/  306]
train() client id: f_00004-1-8 loss: 0.856109  [  288/  306]
train() client id: f_00004-2-0 loss: 0.757170  [   32/  306]
train() client id: f_00004-2-1 loss: 0.714196  [   64/  306]
train() client id: f_00004-2-2 loss: 0.729213  [   96/  306]
train() client id: f_00004-2-3 loss: 0.707491  [  128/  306]
train() client id: f_00004-2-4 loss: 0.695975  [  160/  306]
train() client id: f_00004-2-5 loss: 0.711716  [  192/  306]
train() client id: f_00004-2-6 loss: 0.791833  [  224/  306]
train() client id: f_00004-2-7 loss: 0.823838  [  256/  306]
train() client id: f_00004-2-8 loss: 0.791364  [  288/  306]
train() client id: f_00004-3-0 loss: 0.738787  [   32/  306]
train() client id: f_00004-3-1 loss: 0.670402  [   64/  306]
train() client id: f_00004-3-2 loss: 0.723698  [   96/  306]
train() client id: f_00004-3-3 loss: 0.742378  [  128/  306]
train() client id: f_00004-3-4 loss: 0.785985  [  160/  306]
train() client id: f_00004-3-5 loss: 0.775854  [  192/  306]
train() client id: f_00004-3-6 loss: 0.878780  [  224/  306]
train() client id: f_00004-3-7 loss: 0.748177  [  256/  306]
train() client id: f_00004-3-8 loss: 0.655388  [  288/  306]
train() client id: f_00004-4-0 loss: 0.762977  [   32/  306]
train() client id: f_00004-4-1 loss: 0.603803  [   64/  306]
train() client id: f_00004-4-2 loss: 0.721121  [   96/  306]
train() client id: f_00004-4-3 loss: 0.764350  [  128/  306]
train() client id: f_00004-4-4 loss: 0.779158  [  160/  306]
train() client id: f_00004-4-5 loss: 0.754655  [  192/  306]
train() client id: f_00004-4-6 loss: 0.795861  [  224/  306]
train() client id: f_00004-4-7 loss: 0.889405  [  256/  306]
train() client id: f_00004-4-8 loss: 0.705296  [  288/  306]
train() client id: f_00004-5-0 loss: 0.822965  [   32/  306]
train() client id: f_00004-5-1 loss: 0.832646  [   64/  306]
train() client id: f_00004-5-2 loss: 0.653611  [   96/  306]
train() client id: f_00004-5-3 loss: 0.684618  [  128/  306]
train() client id: f_00004-5-4 loss: 0.786082  [  160/  306]
train() client id: f_00004-5-5 loss: 0.918549  [  192/  306]
train() client id: f_00004-5-6 loss: 0.611038  [  224/  306]
train() client id: f_00004-5-7 loss: 0.609485  [  256/  306]
train() client id: f_00004-5-8 loss: 0.677668  [  288/  306]
train() client id: f_00004-6-0 loss: 0.649972  [   32/  306]
train() client id: f_00004-6-1 loss: 0.815570  [   64/  306]
train() client id: f_00004-6-2 loss: 0.793743  [   96/  306]
train() client id: f_00004-6-3 loss: 0.812889  [  128/  306]
train() client id: f_00004-6-4 loss: 0.776504  [  160/  306]
train() client id: f_00004-6-5 loss: 0.729138  [  192/  306]
train() client id: f_00004-6-6 loss: 0.712590  [  224/  306]
train() client id: f_00004-6-7 loss: 0.684396  [  256/  306]
train() client id: f_00004-6-8 loss: 0.729983  [  288/  306]
train() client id: f_00004-7-0 loss: 0.716239  [   32/  306]
train() client id: f_00004-7-1 loss: 0.606793  [   64/  306]
train() client id: f_00004-7-2 loss: 0.855967  [   96/  306]
train() client id: f_00004-7-3 loss: 0.759922  [  128/  306]
train() client id: f_00004-7-4 loss: 0.761335  [  160/  306]
train() client id: f_00004-7-5 loss: 0.636847  [  192/  306]
train() client id: f_00004-7-6 loss: 0.689058  [  224/  306]
train() client id: f_00004-7-7 loss: 0.723575  [  256/  306]
train() client id: f_00004-7-8 loss: 0.943222  [  288/  306]
train() client id: f_00004-8-0 loss: 0.721083  [   32/  306]
train() client id: f_00004-8-1 loss: 0.842679  [   64/  306]
train() client id: f_00004-8-2 loss: 0.896571  [   96/  306]
train() client id: f_00004-8-3 loss: 0.775136  [  128/  306]
train() client id: f_00004-8-4 loss: 0.674214  [  160/  306]
train() client id: f_00004-8-5 loss: 0.744955  [  192/  306]
train() client id: f_00004-8-6 loss: 0.680197  [  224/  306]
train() client id: f_00004-8-7 loss: 0.789165  [  256/  306]
train() client id: f_00004-8-8 loss: 0.671855  [  288/  306]
train() client id: f_00004-9-0 loss: 0.623755  [   32/  306]
train() client id: f_00004-9-1 loss: 0.638390  [   64/  306]
train() client id: f_00004-9-2 loss: 0.671875  [   96/  306]
train() client id: f_00004-9-3 loss: 0.741834  [  128/  306]
train() client id: f_00004-9-4 loss: 0.929122  [  160/  306]
train() client id: f_00004-9-5 loss: 0.788339  [  192/  306]
train() client id: f_00004-9-6 loss: 0.779752  [  224/  306]
train() client id: f_00004-9-7 loss: 0.777691  [  256/  306]
train() client id: f_00004-9-8 loss: 0.750165  [  288/  306]
train() client id: f_00004-10-0 loss: 0.784942  [   32/  306]
train() client id: f_00004-10-1 loss: 0.642593  [   64/  306]
train() client id: f_00004-10-2 loss: 0.851070  [   96/  306]
train() client id: f_00004-10-3 loss: 0.702166  [  128/  306]
train() client id: f_00004-10-4 loss: 0.778376  [  160/  306]
train() client id: f_00004-10-5 loss: 0.730638  [  192/  306]
train() client id: f_00004-10-6 loss: 0.831016  [  224/  306]
train() client id: f_00004-10-7 loss: 0.695830  [  256/  306]
train() client id: f_00004-10-8 loss: 0.742925  [  288/  306]
train() client id: f_00004-11-0 loss: 0.830163  [   32/  306]
train() client id: f_00004-11-1 loss: 0.790430  [   64/  306]
train() client id: f_00004-11-2 loss: 0.690710  [   96/  306]
train() client id: f_00004-11-3 loss: 0.795572  [  128/  306]
train() client id: f_00004-11-4 loss: 0.750082  [  160/  306]
train() client id: f_00004-11-5 loss: 0.771230  [  192/  306]
train() client id: f_00004-11-6 loss: 0.595301  [  224/  306]
train() client id: f_00004-11-7 loss: 0.669022  [  256/  306]
train() client id: f_00004-11-8 loss: 0.749697  [  288/  306]
train() client id: f_00004-12-0 loss: 0.747217  [   32/  306]
train() client id: f_00004-12-1 loss: 0.707244  [   64/  306]
train() client id: f_00004-12-2 loss: 0.792433  [   96/  306]
train() client id: f_00004-12-3 loss: 0.752687  [  128/  306]
train() client id: f_00004-12-4 loss: 0.778563  [  160/  306]
train() client id: f_00004-12-5 loss: 0.744479  [  192/  306]
train() client id: f_00004-12-6 loss: 0.602664  [  224/  306]
train() client id: f_00004-12-7 loss: 0.786219  [  256/  306]
train() client id: f_00004-12-8 loss: 0.690355  [  288/  306]
train() client id: f_00004-13-0 loss: 0.765674  [   32/  306]
train() client id: f_00004-13-1 loss: 0.730514  [   64/  306]
train() client id: f_00004-13-2 loss: 0.701960  [   96/  306]
train() client id: f_00004-13-3 loss: 0.729558  [  128/  306]
train() client id: f_00004-13-4 loss: 0.631023  [  160/  306]
train() client id: f_00004-13-5 loss: 0.670345  [  192/  306]
train() client id: f_00004-13-6 loss: 0.654592  [  224/  306]
train() client id: f_00004-13-7 loss: 0.926432  [  256/  306]
train() client id: f_00004-13-8 loss: 0.740779  [  288/  306]
train() client id: f_00004-14-0 loss: 0.762526  [   32/  306]
train() client id: f_00004-14-1 loss: 0.770830  [   64/  306]
train() client id: f_00004-14-2 loss: 0.665629  [   96/  306]
train() client id: f_00004-14-3 loss: 0.880858  [  128/  306]
train() client id: f_00004-14-4 loss: 0.698458  [  160/  306]
train() client id: f_00004-14-5 loss: 0.613159  [  192/  306]
train() client id: f_00004-14-6 loss: 0.750671  [  224/  306]
train() client id: f_00004-14-7 loss: 0.731056  [  256/  306]
train() client id: f_00004-14-8 loss: 0.847246  [  288/  306]
train() client id: f_00005-0-0 loss: 0.565062  [   32/  146]
train() client id: f_00005-0-1 loss: 0.479035  [   64/  146]
train() client id: f_00005-0-2 loss: 0.522605  [   96/  146]
train() client id: f_00005-0-3 loss: 0.571196  [  128/  146]
train() client id: f_00005-1-0 loss: 0.564058  [   32/  146]
train() client id: f_00005-1-1 loss: 0.390012  [   64/  146]
train() client id: f_00005-1-2 loss: 0.777128  [   96/  146]
train() client id: f_00005-1-3 loss: 0.482921  [  128/  146]
train() client id: f_00005-2-0 loss: 0.594472  [   32/  146]
train() client id: f_00005-2-1 loss: 0.687916  [   64/  146]
train() client id: f_00005-2-2 loss: 0.621222  [   96/  146]
train() client id: f_00005-2-3 loss: 0.290038  [  128/  146]
train() client id: f_00005-3-0 loss: 0.660030  [   32/  146]
train() client id: f_00005-3-1 loss: 0.378267  [   64/  146]
train() client id: f_00005-3-2 loss: 0.609456  [   96/  146]
train() client id: f_00005-3-3 loss: 0.366258  [  128/  146]
train() client id: f_00005-4-0 loss: 0.656741  [   32/  146]
train() client id: f_00005-4-1 loss: 0.413555  [   64/  146]
train() client id: f_00005-4-2 loss: 0.467400  [   96/  146]
train() client id: f_00005-4-3 loss: 0.596214  [  128/  146]
train() client id: f_00005-5-0 loss: 0.440949  [   32/  146]
train() client id: f_00005-5-1 loss: 0.636914  [   64/  146]
train() client id: f_00005-5-2 loss: 0.698070  [   96/  146]
train() client id: f_00005-5-3 loss: 0.428894  [  128/  146]
train() client id: f_00005-6-0 loss: 0.741657  [   32/  146]
train() client id: f_00005-6-1 loss: 0.444479  [   64/  146]
train() client id: f_00005-6-2 loss: 0.452225  [   96/  146]
train() client id: f_00005-6-3 loss: 0.586406  [  128/  146]
train() client id: f_00005-7-0 loss: 0.740465  [   32/  146]
train() client id: f_00005-7-1 loss: 0.305364  [   64/  146]
train() client id: f_00005-7-2 loss: 0.380931  [   96/  146]
train() client id: f_00005-7-3 loss: 0.518391  [  128/  146]
train() client id: f_00005-8-0 loss: 0.624282  [   32/  146]
train() client id: f_00005-8-1 loss: 0.577002  [   64/  146]
train() client id: f_00005-8-2 loss: 0.740989  [   96/  146]
train() client id: f_00005-8-3 loss: 0.389355  [  128/  146]
train() client id: f_00005-9-0 loss: 0.553433  [   32/  146]
train() client id: f_00005-9-1 loss: 0.615104  [   64/  146]
train() client id: f_00005-9-2 loss: 0.586466  [   96/  146]
train() client id: f_00005-9-3 loss: 0.392872  [  128/  146]
train() client id: f_00005-10-0 loss: 0.651585  [   32/  146]
train() client id: f_00005-10-1 loss: 0.452811  [   64/  146]
train() client id: f_00005-10-2 loss: 0.465568  [   96/  146]
train() client id: f_00005-10-3 loss: 0.379502  [  128/  146]
train() client id: f_00005-11-0 loss: 0.598512  [   32/  146]
train() client id: f_00005-11-1 loss: 0.537621  [   64/  146]
train() client id: f_00005-11-2 loss: 0.515360  [   96/  146]
train() client id: f_00005-11-3 loss: 0.570609  [  128/  146]
train() client id: f_00005-12-0 loss: 0.448707  [   32/  146]
train() client id: f_00005-12-1 loss: 0.640964  [   64/  146]
train() client id: f_00005-12-2 loss: 0.394289  [   96/  146]
train() client id: f_00005-12-3 loss: 0.685402  [  128/  146]
train() client id: f_00005-13-0 loss: 0.602606  [   32/  146]
train() client id: f_00005-13-1 loss: 0.605208  [   64/  146]
train() client id: f_00005-13-2 loss: 0.515747  [   96/  146]
train() client id: f_00005-13-3 loss: 0.386488  [  128/  146]
train() client id: f_00005-14-0 loss: 0.486199  [   32/  146]
train() client id: f_00005-14-1 loss: 0.456170  [   64/  146]
train() client id: f_00005-14-2 loss: 0.425451  [   96/  146]
train() client id: f_00005-14-3 loss: 0.833865  [  128/  146]
train() client id: f_00006-0-0 loss: 0.507928  [   32/   54]
train() client id: f_00006-1-0 loss: 0.451353  [   32/   54]
train() client id: f_00006-2-0 loss: 0.536278  [   32/   54]
train() client id: f_00006-3-0 loss: 0.471127  [   32/   54]
train() client id: f_00006-4-0 loss: 0.497194  [   32/   54]
train() client id: f_00006-5-0 loss: 0.435907  [   32/   54]
train() client id: f_00006-6-0 loss: 0.538867  [   32/   54]
train() client id: f_00006-7-0 loss: 0.524854  [   32/   54]
train() client id: f_00006-8-0 loss: 0.527729  [   32/   54]
train() client id: f_00006-9-0 loss: 0.520842  [   32/   54]
train() client id: f_00006-10-0 loss: 0.472602  [   32/   54]
train() client id: f_00006-11-0 loss: 0.478565  [   32/   54]
train() client id: f_00006-12-0 loss: 0.454510  [   32/   54]
train() client id: f_00006-13-0 loss: 0.534425  [   32/   54]
train() client id: f_00006-14-0 loss: 0.546685  [   32/   54]
train() client id: f_00007-0-0 loss: 0.387394  [   32/  179]
train() client id: f_00007-0-1 loss: 0.801535  [   64/  179]
train() client id: f_00007-0-2 loss: 0.415769  [   96/  179]
train() client id: f_00007-0-3 loss: 0.390212  [  128/  179]
train() client id: f_00007-0-4 loss: 0.648993  [  160/  179]
train() client id: f_00007-1-0 loss: 0.642943  [   32/  179]
train() client id: f_00007-1-1 loss: 0.346807  [   64/  179]
train() client id: f_00007-1-2 loss: 0.493554  [   96/  179]
train() client id: f_00007-1-3 loss: 0.396655  [  128/  179]
train() client id: f_00007-1-4 loss: 0.717865  [  160/  179]
train() client id: f_00007-2-0 loss: 0.425692  [   32/  179]
train() client id: f_00007-2-1 loss: 0.409419  [   64/  179]
train() client id: f_00007-2-2 loss: 0.552652  [   96/  179]
train() client id: f_00007-2-3 loss: 0.635415  [  128/  179]
train() client id: f_00007-2-4 loss: 0.586707  [  160/  179]
train() client id: f_00007-3-0 loss: 0.652221  [   32/  179]
train() client id: f_00007-3-1 loss: 0.340209  [   64/  179]
train() client id: f_00007-3-2 loss: 0.466976  [   96/  179]
train() client id: f_00007-3-3 loss: 0.616067  [  128/  179]
train() client id: f_00007-3-4 loss: 0.507774  [  160/  179]
train() client id: f_00007-4-0 loss: 0.486620  [   32/  179]
train() client id: f_00007-4-1 loss: 0.506961  [   64/  179]
train() client id: f_00007-4-2 loss: 0.651559  [   96/  179]
train() client id: f_00007-4-3 loss: 0.353870  [  128/  179]
train() client id: f_00007-4-4 loss: 0.498712  [  160/  179]
train() client id: f_00007-5-0 loss: 0.575922  [   32/  179]
train() client id: f_00007-5-1 loss: 0.442238  [   64/  179]
train() client id: f_00007-5-2 loss: 0.528493  [   96/  179]
train() client id: f_00007-5-3 loss: 0.504101  [  128/  179]
train() client id: f_00007-5-4 loss: 0.502715  [  160/  179]
train() client id: f_00007-6-0 loss: 0.422421  [   32/  179]
train() client id: f_00007-6-1 loss: 0.360157  [   64/  179]
train() client id: f_00007-6-2 loss: 0.468526  [   96/  179]
train() client id: f_00007-6-3 loss: 0.634423  [  128/  179]
train() client id: f_00007-6-4 loss: 0.509733  [  160/  179]
train() client id: f_00007-7-0 loss: 0.580208  [   32/  179]
train() client id: f_00007-7-1 loss: 0.479902  [   64/  179]
train() client id: f_00007-7-2 loss: 0.452708  [   96/  179]
train() client id: f_00007-7-3 loss: 0.414512  [  128/  179]
train() client id: f_00007-7-4 loss: 0.549960  [  160/  179]
train() client id: f_00007-8-0 loss: 0.322326  [   32/  179]
train() client id: f_00007-8-1 loss: 0.460649  [   64/  179]
train() client id: f_00007-8-2 loss: 0.430062  [   96/  179]
train() client id: f_00007-8-3 loss: 0.338354  [  128/  179]
train() client id: f_00007-8-4 loss: 0.779624  [  160/  179]
train() client id: f_00007-9-0 loss: 0.589894  [   32/  179]
train() client id: f_00007-9-1 loss: 0.536924  [   64/  179]
train() client id: f_00007-9-2 loss: 0.416657  [   96/  179]
train() client id: f_00007-9-3 loss: 0.621740  [  128/  179]
train() client id: f_00007-9-4 loss: 0.348327  [  160/  179]
train() client id: f_00007-10-0 loss: 0.614248  [   32/  179]
train() client id: f_00007-10-1 loss: 0.439150  [   64/  179]
train() client id: f_00007-10-2 loss: 0.500322  [   96/  179]
train() client id: f_00007-10-3 loss: 0.344492  [  128/  179]
train() client id: f_00007-10-4 loss: 0.531462  [  160/  179]
train() client id: f_00007-11-0 loss: 0.597730  [   32/  179]
train() client id: f_00007-11-1 loss: 0.349651  [   64/  179]
train() client id: f_00007-11-2 loss: 0.491606  [   96/  179]
train() client id: f_00007-11-3 loss: 0.498854  [  128/  179]
train() client id: f_00007-11-4 loss: 0.528990  [  160/  179]
train() client id: f_00007-12-0 loss: 0.711832  [   32/  179]
train() client id: f_00007-12-1 loss: 0.409252  [   64/  179]
train() client id: f_00007-12-2 loss: 0.595278  [   96/  179]
train() client id: f_00007-12-3 loss: 0.437974  [  128/  179]
train() client id: f_00007-12-4 loss: 0.348037  [  160/  179]
train() client id: f_00007-13-0 loss: 0.607551  [   32/  179]
train() client id: f_00007-13-1 loss: 0.522933  [   64/  179]
train() client id: f_00007-13-2 loss: 0.352619  [   96/  179]
train() client id: f_00007-13-3 loss: 0.374753  [  128/  179]
train() client id: f_00007-13-4 loss: 0.544798  [  160/  179]
train() client id: f_00007-14-0 loss: 0.423866  [   32/  179]
train() client id: f_00007-14-1 loss: 0.431550  [   64/  179]
train() client id: f_00007-14-2 loss: 0.443039  [   96/  179]
train() client id: f_00007-14-3 loss: 0.440507  [  128/  179]
train() client id: f_00007-14-4 loss: 0.593987  [  160/  179]
train() client id: f_00008-0-0 loss: 0.802440  [   32/  130]
train() client id: f_00008-0-1 loss: 0.746529  [   64/  130]
train() client id: f_00008-0-2 loss: 0.680674  [   96/  130]
train() client id: f_00008-0-3 loss: 0.754711  [  128/  130]
train() client id: f_00008-1-0 loss: 0.649685  [   32/  130]
train() client id: f_00008-1-1 loss: 0.753359  [   64/  130]
train() client id: f_00008-1-2 loss: 0.802023  [   96/  130]
train() client id: f_00008-1-3 loss: 0.729001  [  128/  130]
train() client id: f_00008-2-0 loss: 0.745706  [   32/  130]
train() client id: f_00008-2-1 loss: 0.761320  [   64/  130]
train() client id: f_00008-2-2 loss: 0.690794  [   96/  130]
train() client id: f_00008-2-3 loss: 0.790828  [  128/  130]
train() client id: f_00008-3-0 loss: 0.729593  [   32/  130]
train() client id: f_00008-3-1 loss: 0.843252  [   64/  130]
train() client id: f_00008-3-2 loss: 0.759484  [   96/  130]
train() client id: f_00008-3-3 loss: 0.652139  [  128/  130]
train() client id: f_00008-4-0 loss: 0.738799  [   32/  130]
train() client id: f_00008-4-1 loss: 0.610697  [   64/  130]
train() client id: f_00008-4-2 loss: 0.734931  [   96/  130]
train() client id: f_00008-4-3 loss: 0.874567  [  128/  130]
train() client id: f_00008-5-0 loss: 0.737225  [   32/  130]
train() client id: f_00008-5-1 loss: 0.726739  [   64/  130]
train() client id: f_00008-5-2 loss: 0.792347  [   96/  130]
train() client id: f_00008-5-3 loss: 0.703467  [  128/  130]
train() client id: f_00008-6-0 loss: 0.697629  [   32/  130]
train() client id: f_00008-6-1 loss: 0.793137  [   64/  130]
train() client id: f_00008-6-2 loss: 0.692252  [   96/  130]
train() client id: f_00008-6-3 loss: 0.787910  [  128/  130]
train() client id: f_00008-7-0 loss: 0.814844  [   32/  130]
train() client id: f_00008-7-1 loss: 0.679527  [   64/  130]
train() client id: f_00008-7-2 loss: 0.725072  [   96/  130]
train() client id: f_00008-7-3 loss: 0.729429  [  128/  130]
train() client id: f_00008-8-0 loss: 0.707426  [   32/  130]
train() client id: f_00008-8-1 loss: 0.680747  [   64/  130]
train() client id: f_00008-8-2 loss: 0.794015  [   96/  130]
train() client id: f_00008-8-3 loss: 0.792332  [  128/  130]
train() client id: f_00008-9-0 loss: 0.802287  [   32/  130]
train() client id: f_00008-9-1 loss: 0.795956  [   64/  130]
train() client id: f_00008-9-2 loss: 0.755312  [   96/  130]
train() client id: f_00008-9-3 loss: 0.611604  [  128/  130]
train() client id: f_00008-10-0 loss: 0.745264  [   32/  130]
train() client id: f_00008-10-1 loss: 0.763657  [   64/  130]
train() client id: f_00008-10-2 loss: 0.674118  [   96/  130]
train() client id: f_00008-10-3 loss: 0.780490  [  128/  130]
train() client id: f_00008-11-0 loss: 0.838170  [   32/  130]
train() client id: f_00008-11-1 loss: 0.687884  [   64/  130]
train() client id: f_00008-11-2 loss: 0.720304  [   96/  130]
train() client id: f_00008-11-3 loss: 0.718393  [  128/  130]
train() client id: f_00008-12-0 loss: 0.656465  [   32/  130]
train() client id: f_00008-12-1 loss: 0.754248  [   64/  130]
train() client id: f_00008-12-2 loss: 0.799963  [   96/  130]
train() client id: f_00008-12-3 loss: 0.723676  [  128/  130]
train() client id: f_00008-13-0 loss: 0.619386  [   32/  130]
train() client id: f_00008-13-1 loss: 0.800857  [   64/  130]
train() client id: f_00008-13-2 loss: 0.853397  [   96/  130]
train() client id: f_00008-13-3 loss: 0.675017  [  128/  130]
train() client id: f_00008-14-0 loss: 0.684339  [   32/  130]
train() client id: f_00008-14-1 loss: 0.671764  [   64/  130]
train() client id: f_00008-14-2 loss: 0.873559  [   96/  130]
train() client id: f_00008-14-3 loss: 0.709187  [  128/  130]
train() client id: f_00009-0-0 loss: 1.104738  [   32/  118]
train() client id: f_00009-0-1 loss: 1.346165  [   64/  118]
train() client id: f_00009-0-2 loss: 1.037935  [   96/  118]
train() client id: f_00009-1-0 loss: 1.278332  [   32/  118]
train() client id: f_00009-1-1 loss: 1.045229  [   64/  118]
train() client id: f_00009-1-2 loss: 0.933327  [   96/  118]
train() client id: f_00009-2-0 loss: 1.076754  [   32/  118]
train() client id: f_00009-2-1 loss: 1.077242  [   64/  118]
train() client id: f_00009-2-2 loss: 1.052059  [   96/  118]
train() client id: f_00009-3-0 loss: 1.046958  [   32/  118]
train() client id: f_00009-3-1 loss: 0.892300  [   64/  118]
train() client id: f_00009-3-2 loss: 1.152936  [   96/  118]
train() client id: f_00009-4-0 loss: 0.993877  [   32/  118]
train() client id: f_00009-4-1 loss: 0.982895  [   64/  118]
train() client id: f_00009-4-2 loss: 1.036833  [   96/  118]
train() client id: f_00009-5-0 loss: 0.989405  [   32/  118]
train() client id: f_00009-5-1 loss: 1.002319  [   64/  118]
train() client id: f_00009-5-2 loss: 1.017805  [   96/  118]
train() client id: f_00009-6-0 loss: 1.112623  [   32/  118]
train() client id: f_00009-6-1 loss: 1.043676  [   64/  118]
train() client id: f_00009-6-2 loss: 0.829190  [   96/  118]
train() client id: f_00009-7-0 loss: 0.849699  [   32/  118]
train() client id: f_00009-7-1 loss: 1.124531  [   64/  118]
train() client id: f_00009-7-2 loss: 1.009634  [   96/  118]
train() client id: f_00009-8-0 loss: 0.954547  [   32/  118]
train() client id: f_00009-8-1 loss: 0.948367  [   64/  118]
train() client id: f_00009-8-2 loss: 0.939795  [   96/  118]
train() client id: f_00009-9-0 loss: 1.091601  [   32/  118]
train() client id: f_00009-9-1 loss: 0.753838  [   64/  118]
train() client id: f_00009-9-2 loss: 0.890239  [   96/  118]
train() client id: f_00009-10-0 loss: 0.845722  [   32/  118]
train() client id: f_00009-10-1 loss: 0.819861  [   64/  118]
train() client id: f_00009-10-2 loss: 1.099691  [   96/  118]
train() client id: f_00009-11-0 loss: 0.956258  [   32/  118]
train() client id: f_00009-11-1 loss: 0.932536  [   64/  118]
train() client id: f_00009-11-2 loss: 0.931893  [   96/  118]
train() client id: f_00009-12-0 loss: 0.831793  [   32/  118]
train() client id: f_00009-12-1 loss: 0.981303  [   64/  118]
train() client id: f_00009-12-2 loss: 1.028888  [   96/  118]
train() client id: f_00009-13-0 loss: 0.959841  [   32/  118]
train() client id: f_00009-13-1 loss: 1.024065  [   64/  118]
train() client id: f_00009-13-2 loss: 0.858492  [   96/  118]
train() client id: f_00009-14-0 loss: 0.950133  [   32/  118]
train() client id: f_00009-14-1 loss: 0.902805  [   64/  118]
train() client id: f_00009-14-2 loss: 0.952040  [   96/  118]
At round 42 accuracy: 0.6445623342175066
At round 42 training accuracy: 0.5848423876592891
At round 42 training loss: 0.83924821687926
update_location
xs = 8.927491 331.223621 5.882650 0.934260 -247.581990 -95.230757 -55.849135 -5.143845 -270.120581 20.134486 
ys = -322.390647 7.291448 220.684448 -42.290817 -9.642386 0.794442 -1.381692 216.628436 25.881276 -757.232496 
xs mean: -30.682379970521218
ys mean: -66.16579882624052
dists_uav = 337.661709 346.066832 242.355588 108.578939 267.188730 138.092463 114.547086 238.651081 289.197111 764.072281 
uav_gains = -118.458128 -118.953342 -110.660969 -100.893692 -112.814943 -103.505750 -101.474712 -110.364773 -114.802736 -129.041535 
uav_gains_db_mean: -112.09705787210947
dists_bs = 530.308191 533.280895 186.562595 279.585699 198.395958 191.600387 212.855421 174.882933 176.873737 952.436294 
bs_gains = -115.853853 -115.921828 -103.150147 -108.069478 -103.897978 -103.474158 -104.753431 -102.363987 -102.501633 -122.974465 
bs_gains_db_mean: -108.2960957995133
Round 43
-------------------------------
ene_coms = [0.02293236 0.02434162 0.00724839 0.00654509 0.00751534 0.00737297
 0.00671301 0.0069871  0.0070315  0.0427011 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.47555994 11.19142856  5.18046289  1.85123554 12.64310282  6.08379473
  2.30433675  7.43100935  5.42341714  5.4163291 ]
obj_prev = 63.00067680396934
eta_min = 5.397402373576392e-18	eta_max = 0.7912426237717026
af = 13.13642190293526	bf = 1.875211537971249	zeta = 14.450064093228786	eta = 0.9090909090909091
af = 13.13642190293526	bf = 1.875211537971249	zeta = 34.399708986780276	eta = 0.3818759602874418
af = 13.13642190293526	bf = 1.875211537971249	zeta = 23.49222985046666	eta = 0.5591815671203435
af = 13.13642190293526	bf = 1.875211537971249	zeta = 21.57615010155531	eta = 0.6088399385944355
af = 13.13642190293526	bf = 1.875211537971249	zeta = 21.457973956233484	eta = 0.612193021099234
af = 13.13642190293526	bf = 1.875211537971249	zeta = 21.457464000316143	eta = 0.6122075704166026
af = 13.13642190293526	bf = 1.875211537971249	zeta = 21.457463990749872	eta = 0.6122075706895399
eta = 0.6122075706895399
ene_coms = [0.02293236 0.02434162 0.00724839 0.00654509 0.00751534 0.00737297
 0.00671301 0.0069871  0.0070315  0.0427011 ]
ene_comp = [0.04049006 0.08515766 0.03984736 0.01381804 0.098333   0.04691705
 0.01735288 0.05752159 0.04177546 0.03791926]
ene_total = [2.20022383 3.79870291 1.63382572 0.70642911 3.67204591 1.88340657
 0.83488372 2.23790821 1.6931904  2.79684761]
ti_comp = [0.57230503 0.55821245 0.72914479 0.73617771 0.72647525 0.72789894
 0.73449856 0.73175767 0.73131364 0.37461763]
ti_coms = [0.22932361 0.2434162  0.07248386 0.06545094 0.0751534  0.07372971
 0.06713008 0.06987098 0.07031501 0.42701101]
t_total = [27.80976295 27.80976295 27.80976295 27.80976295 27.80976295 27.80976295
 27.80976295 27.80976295 27.80976295 27.80976295]
ene_coms = [0.02293236 0.02434162 0.00724839 0.00654509 0.00751534 0.00737297
 0.00671301 0.0069871  0.0070315  0.0427011 ]
ene_comp = [1.26668926e-05 1.23866017e-04 7.43791292e-06 3.04266143e-07
 1.12599437e-04 1.21823195e-05 6.05358945e-07 2.22145802e-05
 8.51993535e-06 2.42819284e-05]
ene_total = [0.79599924 0.84874632 0.25171598 0.2270702  0.26462525 0.25620263
 0.23290586 0.2431641  0.24422944 1.48221098]
optimize_network iter = 0 obj = 4.846869975875352
eta = 0.6122075706895399
freqs = [35374543.07335393 76277107.77469982 27324723.12628388  9384986.55587862
 67678147.68539055 32227723.72035291 11812740.67069776 39303715.49317078
 28561929.39959996 50610619.26367704]
eta_min = 0.6122075706895401	eta_max = 0.612207570689538
af = 0.008901748454350301	bf = 1.875211537971249	zeta = 0.009791923299785333	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02293236 0.02434162 0.00724839 0.00654509 0.00751534 0.00737297
 0.00671301 0.0069871  0.0070315  0.0427011 ]
ene_comp = [2.46041055e-06 2.40596700e-05 1.44473629e-06 5.91004955e-08
 2.18712554e-05 2.36628732e-06 1.17584603e-07 4.31494836e-06
 1.65490776e-06 4.71650899e-06]
ene_total = [3.39399686 3.60574158 1.07286217 0.96858086 1.11538998 1.09143521
 0.99343822 1.0346203  1.04079768 6.31979677]
ti_comp = [0.57230503 0.55821245 0.72914479 0.73617771 0.72647525 0.72789894
 0.73449856 0.73175767 0.73131364 0.37461763]
ti_coms = [0.22932361 0.2434162  0.07248386 0.06545094 0.0751534  0.07372971
 0.06713008 0.06987098 0.07031501 0.42701101]
t_total = [27.80976295 27.80976295 27.80976295 27.80976295 27.80976295 27.80976295
 27.80976295 27.80976295 27.80976295 27.80976295]
ene_coms = [0.02293236 0.02434162 0.00724839 0.00654509 0.00751534 0.00737297
 0.00671301 0.0069871  0.0070315  0.0427011 ]
ene_comp = [1.26668926e-05 1.23866017e-04 7.43791292e-06 3.04266143e-07
 1.12599437e-04 1.21823195e-05 6.05358945e-07 2.22145802e-05
 8.51993535e-06 2.42819284e-05]
ene_total = [0.79599924 0.84874632 0.25171598 0.2270702  0.26462525 0.25620263
 0.23290586 0.2431641  0.24422944 1.48221098]
optimize_network iter = 1 obj = 4.846869975875329
eta = 0.612207570689538
freqs = [35374543.07335391 76277107.77469976 27324723.12628391  9384986.55587863
 67678147.68539059 32227723.72035293 11812740.67069777 39303715.49317081
 28561929.39959998 50610619.26367682]
Done!
ene_coms = [0.02293236 0.02434162 0.00724839 0.00654509 0.00751534 0.00737297
 0.00671301 0.0069871  0.0070315  0.0427011 ]
ene_comp = [1.26136916e-05 1.23345780e-04 7.40667370e-06 3.02988226e-07
 1.12126519e-04 1.21311537e-05 6.02816438e-07 2.21212790e-05
 8.48415163e-06 2.41799443e-05]
ene_total = [0.02294497 0.02446497 0.00725579 0.0065454  0.00762747 0.0073851
 0.00671361 0.00700922 0.00703999 0.04272528]
At round 43 energy consumption: 0.13971179468493647
At round 43 eta: 0.612207570689538
At round 43 a_n: 13.453131436573733
At round 43 local rounds: 16.0674834066673
At round 43 global rounds: 34.691578328372465
gradient difference: 0.40835851430892944
train() client id: f_00000-0-0 loss: 1.219394  [   32/  126]
train() client id: f_00000-0-1 loss: 1.404325  [   64/  126]
train() client id: f_00000-0-2 loss: 1.300648  [   96/  126]
train() client id: f_00000-1-0 loss: 1.126283  [   32/  126]
train() client id: f_00000-1-1 loss: 0.976631  [   64/  126]
train() client id: f_00000-1-2 loss: 1.401124  [   96/  126]
train() client id: f_00000-2-0 loss: 1.233952  [   32/  126]
train() client id: f_00000-2-1 loss: 1.096395  [   64/  126]
train() client id: f_00000-2-2 loss: 0.986771  [   96/  126]
train() client id: f_00000-3-0 loss: 1.009380  [   32/  126]
train() client id: f_00000-3-1 loss: 0.935899  [   64/  126]
train() client id: f_00000-3-2 loss: 1.043591  [   96/  126]
train() client id: f_00000-4-0 loss: 0.994231  [   32/  126]
train() client id: f_00000-4-1 loss: 0.984935  [   64/  126]
train() client id: f_00000-4-2 loss: 0.903604  [   96/  126]
train() client id: f_00000-5-0 loss: 0.927701  [   32/  126]
train() client id: f_00000-5-1 loss: 0.924728  [   64/  126]
train() client id: f_00000-5-2 loss: 0.925543  [   96/  126]
train() client id: f_00000-6-0 loss: 0.846227  [   32/  126]
train() client id: f_00000-6-1 loss: 0.996188  [   64/  126]
train() client id: f_00000-6-2 loss: 0.920103  [   96/  126]
train() client id: f_00000-7-0 loss: 0.995135  [   32/  126]
train() client id: f_00000-7-1 loss: 0.957202  [   64/  126]
train() client id: f_00000-7-2 loss: 0.797527  [   96/  126]
train() client id: f_00000-8-0 loss: 0.848639  [   32/  126]
train() client id: f_00000-8-1 loss: 0.786977  [   64/  126]
train() client id: f_00000-8-2 loss: 0.976055  [   96/  126]
train() client id: f_00000-9-0 loss: 0.909338  [   32/  126]
train() client id: f_00000-9-1 loss: 0.779116  [   64/  126]
train() client id: f_00000-9-2 loss: 0.873466  [   96/  126]
train() client id: f_00000-10-0 loss: 0.892512  [   32/  126]
train() client id: f_00000-10-1 loss: 0.848416  [   64/  126]
train() client id: f_00000-10-2 loss: 0.832553  [   96/  126]
train() client id: f_00000-11-0 loss: 0.842064  [   32/  126]
train() client id: f_00000-11-1 loss: 0.853554  [   64/  126]
train() client id: f_00000-11-2 loss: 0.711801  [   96/  126]
train() client id: f_00000-12-0 loss: 0.811251  [   32/  126]
train() client id: f_00000-12-1 loss: 0.846612  [   64/  126]
train() client id: f_00000-12-2 loss: 0.755940  [   96/  126]
train() client id: f_00000-13-0 loss: 0.712645  [   32/  126]
train() client id: f_00000-13-1 loss: 1.019569  [   64/  126]
train() client id: f_00000-13-2 loss: 0.700390  [   96/  126]
train() client id: f_00000-14-0 loss: 0.944799  [   32/  126]
train() client id: f_00000-14-1 loss: 0.699469  [   64/  126]
train() client id: f_00000-14-2 loss: 0.763878  [   96/  126]
train() client id: f_00000-15-0 loss: 0.867547  [   32/  126]
train() client id: f_00000-15-1 loss: 0.833267  [   64/  126]
train() client id: f_00000-15-2 loss: 0.711894  [   96/  126]
train() client id: f_00001-0-0 loss: 0.297761  [   32/  265]
train() client id: f_00001-0-1 loss: 0.368114  [   64/  265]
train() client id: f_00001-0-2 loss: 0.389292  [   96/  265]
train() client id: f_00001-0-3 loss: 0.313351  [  128/  265]
train() client id: f_00001-0-4 loss: 0.368645  [  160/  265]
train() client id: f_00001-0-5 loss: 0.302306  [  192/  265]
train() client id: f_00001-0-6 loss: 0.328885  [  224/  265]
train() client id: f_00001-0-7 loss: 0.356478  [  256/  265]
train() client id: f_00001-1-0 loss: 0.335831  [   32/  265]
train() client id: f_00001-1-1 loss: 0.275284  [   64/  265]
train() client id: f_00001-1-2 loss: 0.351206  [   96/  265]
train() client id: f_00001-1-3 loss: 0.259561  [  128/  265]
train() client id: f_00001-1-4 loss: 0.389499  [  160/  265]
train() client id: f_00001-1-5 loss: 0.413094  [  192/  265]
train() client id: f_00001-1-6 loss: 0.298198  [  224/  265]
train() client id: f_00001-1-7 loss: 0.312571  [  256/  265]
train() client id: f_00001-2-0 loss: 0.254082  [   32/  265]
train() client id: f_00001-2-1 loss: 0.305498  [   64/  265]
train() client id: f_00001-2-2 loss: 0.276698  [   96/  265]
train() client id: f_00001-2-3 loss: 0.394577  [  128/  265]
train() client id: f_00001-2-4 loss: 0.350285  [  160/  265]
train() client id: f_00001-2-5 loss: 0.284040  [  192/  265]
train() client id: f_00001-2-6 loss: 0.236463  [  224/  265]
train() client id: f_00001-2-7 loss: 0.351115  [  256/  265]
train() client id: f_00001-3-0 loss: 0.245036  [   32/  265]
train() client id: f_00001-3-1 loss: 0.370818  [   64/  265]
train() client id: f_00001-3-2 loss: 0.227612  [   96/  265]
train() client id: f_00001-3-3 loss: 0.329125  [  128/  265]
train() client id: f_00001-3-4 loss: 0.319592  [  160/  265]
train() client id: f_00001-3-5 loss: 0.328940  [  192/  265]
train() client id: f_00001-3-6 loss: 0.429052  [  224/  265]
train() client id: f_00001-3-7 loss: 0.226302  [  256/  265]
train() client id: f_00001-4-0 loss: 0.210087  [   32/  265]
train() client id: f_00001-4-1 loss: 0.380983  [   64/  265]
train() client id: f_00001-4-2 loss: 0.208872  [   96/  265]
train() client id: f_00001-4-3 loss: 0.371238  [  128/  265]
train() client id: f_00001-4-4 loss: 0.304974  [  160/  265]
train() client id: f_00001-4-5 loss: 0.206895  [  192/  265]
train() client id: f_00001-4-6 loss: 0.411912  [  224/  265]
train() client id: f_00001-4-7 loss: 0.292281  [  256/  265]
train() client id: f_00001-5-0 loss: 0.516954  [   32/  265]
train() client id: f_00001-5-1 loss: 0.351130  [   64/  265]
train() client id: f_00001-5-2 loss: 0.315034  [   96/  265]
train() client id: f_00001-5-3 loss: 0.289959  [  128/  265]
train() client id: f_00001-5-4 loss: 0.205879  [  160/  265]
train() client id: f_00001-5-5 loss: 0.200372  [  192/  265]
train() client id: f_00001-5-6 loss: 0.261394  [  224/  265]
train() client id: f_00001-5-7 loss: 0.282487  [  256/  265]
train() client id: f_00001-6-0 loss: 0.421895  [   32/  265]
train() client id: f_00001-6-1 loss: 0.402570  [   64/  265]
train() client id: f_00001-6-2 loss: 0.220427  [   96/  265]
train() client id: f_00001-6-3 loss: 0.291582  [  128/  265]
train() client id: f_00001-6-4 loss: 0.216671  [  160/  265]
train() client id: f_00001-6-5 loss: 0.291995  [  192/  265]
train() client id: f_00001-6-6 loss: 0.200669  [  224/  265]
train() client id: f_00001-6-7 loss: 0.334058  [  256/  265]
train() client id: f_00001-7-0 loss: 0.250654  [   32/  265]
train() client id: f_00001-7-1 loss: 0.193988  [   64/  265]
train() client id: f_00001-7-2 loss: 0.291607  [   96/  265]
train() client id: f_00001-7-3 loss: 0.323565  [  128/  265]
train() client id: f_00001-7-4 loss: 0.217284  [  160/  265]
train() client id: f_00001-7-5 loss: 0.293878  [  192/  265]
train() client id: f_00001-7-6 loss: 0.347047  [  224/  265]
train() client id: f_00001-7-7 loss: 0.340549  [  256/  265]
train() client id: f_00001-8-0 loss: 0.346436  [   32/  265]
train() client id: f_00001-8-1 loss: 0.330861  [   64/  265]
train() client id: f_00001-8-2 loss: 0.294136  [   96/  265]
train() client id: f_00001-8-3 loss: 0.217212  [  128/  265]
train() client id: f_00001-8-4 loss: 0.303300  [  160/  265]
train() client id: f_00001-8-5 loss: 0.227886  [  192/  265]
train() client id: f_00001-8-6 loss: 0.252689  [  224/  265]
train() client id: f_00001-8-7 loss: 0.291213  [  256/  265]
train() client id: f_00001-9-0 loss: 0.288510  [   32/  265]
train() client id: f_00001-9-1 loss: 0.226539  [   64/  265]
train() client id: f_00001-9-2 loss: 0.251987  [   96/  265]
train() client id: f_00001-9-3 loss: 0.193280  [  128/  265]
train() client id: f_00001-9-4 loss: 0.294371  [  160/  265]
train() client id: f_00001-9-5 loss: 0.346507  [  192/  265]
train() client id: f_00001-9-6 loss: 0.276360  [  224/  265]
train() client id: f_00001-9-7 loss: 0.367022  [  256/  265]
train() client id: f_00001-10-0 loss: 0.241536  [   32/  265]
train() client id: f_00001-10-1 loss: 0.342039  [   64/  265]
train() client id: f_00001-10-2 loss: 0.357120  [   96/  265]
train() client id: f_00001-10-3 loss: 0.252939  [  128/  265]
train() client id: f_00001-10-4 loss: 0.338829  [  160/  265]
train() client id: f_00001-10-5 loss: 0.330433  [  192/  265]
train() client id: f_00001-10-6 loss: 0.195052  [  224/  265]
train() client id: f_00001-10-7 loss: 0.268241  [  256/  265]
train() client id: f_00001-11-0 loss: 0.347160  [   32/  265]
train() client id: f_00001-11-1 loss: 0.382284  [   64/  265]
train() client id: f_00001-11-2 loss: 0.197669  [   96/  265]
train() client id: f_00001-11-3 loss: 0.281261  [  128/  265]
train() client id: f_00001-11-4 loss: 0.280757  [  160/  265]
train() client id: f_00001-11-5 loss: 0.201268  [  192/  265]
train() client id: f_00001-11-6 loss: 0.347948  [  224/  265]
train() client id: f_00001-11-7 loss: 0.227570  [  256/  265]
train() client id: f_00001-12-0 loss: 0.255998  [   32/  265]
train() client id: f_00001-12-1 loss: 0.255236  [   64/  265]
train() client id: f_00001-12-2 loss: 0.204325  [   96/  265]
train() client id: f_00001-12-3 loss: 0.365396  [  128/  265]
train() client id: f_00001-12-4 loss: 0.250911  [  160/  265]
train() client id: f_00001-12-5 loss: 0.273228  [  192/  265]
train() client id: f_00001-12-6 loss: 0.329025  [  224/  265]
train() client id: f_00001-12-7 loss: 0.360138  [  256/  265]
train() client id: f_00001-13-0 loss: 0.196591  [   32/  265]
train() client id: f_00001-13-1 loss: 0.269932  [   64/  265]
train() client id: f_00001-13-2 loss: 0.284430  [   96/  265]
train() client id: f_00001-13-3 loss: 0.454771  [  128/  265]
train() client id: f_00001-13-4 loss: 0.246132  [  160/  265]
train() client id: f_00001-13-5 loss: 0.174701  [  192/  265]
train() client id: f_00001-13-6 loss: 0.390590  [  224/  265]
train() client id: f_00001-13-7 loss: 0.238806  [  256/  265]
train() client id: f_00001-14-0 loss: 0.258595  [   32/  265]
train() client id: f_00001-14-1 loss: 0.163488  [   64/  265]
train() client id: f_00001-14-2 loss: 0.346176  [   96/  265]
train() client id: f_00001-14-3 loss: 0.268300  [  128/  265]
train() client id: f_00001-14-4 loss: 0.202653  [  160/  265]
train() client id: f_00001-14-5 loss: 0.414035  [  192/  265]
train() client id: f_00001-14-6 loss: 0.284049  [  224/  265]
train() client id: f_00001-14-7 loss: 0.296688  [  256/  265]
train() client id: f_00001-15-0 loss: 0.285198  [   32/  265]
train() client id: f_00001-15-1 loss: 0.304704  [   64/  265]
train() client id: f_00001-15-2 loss: 0.293475  [   96/  265]
train() client id: f_00001-15-3 loss: 0.185662  [  128/  265]
train() client id: f_00001-15-4 loss: 0.312450  [  160/  265]
train() client id: f_00001-15-5 loss: 0.344938  [  192/  265]
train() client id: f_00001-15-6 loss: 0.272398  [  224/  265]
train() client id: f_00001-15-7 loss: 0.288888  [  256/  265]
train() client id: f_00002-0-0 loss: 1.157534  [   32/  124]
train() client id: f_00002-0-1 loss: 1.189400  [   64/  124]
train() client id: f_00002-0-2 loss: 1.307471  [   96/  124]
train() client id: f_00002-1-0 loss: 1.174702  [   32/  124]
train() client id: f_00002-1-1 loss: 1.162728  [   64/  124]
train() client id: f_00002-1-2 loss: 1.214733  [   96/  124]
train() client id: f_00002-2-0 loss: 1.160967  [   32/  124]
train() client id: f_00002-2-1 loss: 1.152838  [   64/  124]
train() client id: f_00002-2-2 loss: 1.066234  [   96/  124]
train() client id: f_00002-3-0 loss: 1.165073  [   32/  124]
train() client id: f_00002-3-1 loss: 0.985476  [   64/  124]
train() client id: f_00002-3-2 loss: 1.184764  [   96/  124]
train() client id: f_00002-4-0 loss: 0.969318  [   32/  124]
train() client id: f_00002-4-1 loss: 1.179652  [   64/  124]
train() client id: f_00002-4-2 loss: 1.127045  [   96/  124]
train() client id: f_00002-5-0 loss: 1.040018  [   32/  124]
train() client id: f_00002-5-1 loss: 1.024333  [   64/  124]
train() client id: f_00002-5-2 loss: 1.100116  [   96/  124]
train() client id: f_00002-6-0 loss: 1.000520  [   32/  124]
train() client id: f_00002-6-1 loss: 1.120531  [   64/  124]
train() client id: f_00002-6-2 loss: 1.039420  [   96/  124]
train() client id: f_00002-7-0 loss: 1.275742  [   32/  124]
train() client id: f_00002-7-1 loss: 0.979885  [   64/  124]
train() client id: f_00002-7-2 loss: 0.901622  [   96/  124]
train() client id: f_00002-8-0 loss: 1.080491  [   32/  124]
train() client id: f_00002-8-1 loss: 0.897266  [   64/  124]
train() client id: f_00002-8-2 loss: 1.031525  [   96/  124]
train() client id: f_00002-9-0 loss: 1.209171  [   32/  124]
train() client id: f_00002-9-1 loss: 0.949634  [   64/  124]
train() client id: f_00002-9-2 loss: 0.961419  [   96/  124]
train() client id: f_00002-10-0 loss: 0.988032  [   32/  124]
train() client id: f_00002-10-1 loss: 1.081732  [   64/  124]
train() client id: f_00002-10-2 loss: 0.987229  [   96/  124]
train() client id: f_00002-11-0 loss: 0.978303  [   32/  124]
train() client id: f_00002-11-1 loss: 1.111875  [   64/  124]
train() client id: f_00002-11-2 loss: 1.015489  [   96/  124]
train() client id: f_00002-12-0 loss: 0.836698  [   32/  124]
train() client id: f_00002-12-1 loss: 0.880272  [   64/  124]
train() client id: f_00002-12-2 loss: 1.088916  [   96/  124]
train() client id: f_00002-13-0 loss: 0.912633  [   32/  124]
train() client id: f_00002-13-1 loss: 0.943011  [   64/  124]
train() client id: f_00002-13-2 loss: 0.996592  [   96/  124]
train() client id: f_00002-14-0 loss: 0.875072  [   32/  124]
train() client id: f_00002-14-1 loss: 1.174819  [   64/  124]
train() client id: f_00002-14-2 loss: 0.894865  [   96/  124]
train() client id: f_00002-15-0 loss: 1.030338  [   32/  124]
train() client id: f_00002-15-1 loss: 0.974825  [   64/  124]
train() client id: f_00002-15-2 loss: 1.013005  [   96/  124]
train() client id: f_00003-0-0 loss: 0.645128  [   32/   43]
train() client id: f_00003-1-0 loss: 0.888437  [   32/   43]
train() client id: f_00003-2-0 loss: 0.568976  [   32/   43]
train() client id: f_00003-3-0 loss: 0.491865  [   32/   43]
train() client id: f_00003-4-0 loss: 0.867012  [   32/   43]
train() client id: f_00003-5-0 loss: 0.763781  [   32/   43]
train() client id: f_00003-6-0 loss: 0.674805  [   32/   43]
train() client id: f_00003-7-0 loss: 0.671472  [   32/   43]
train() client id: f_00003-8-0 loss: 0.750566  [   32/   43]
train() client id: f_00003-9-0 loss: 0.840637  [   32/   43]
train() client id: f_00003-10-0 loss: 0.557373  [   32/   43]
train() client id: f_00003-11-0 loss: 0.756379  [   32/   43]
train() client id: f_00003-12-0 loss: 0.640005  [   32/   43]
train() client id: f_00003-13-0 loss: 0.612295  [   32/   43]
train() client id: f_00003-14-0 loss: 0.663605  [   32/   43]
train() client id: f_00003-15-0 loss: 0.706674  [   32/   43]
train() client id: f_00004-0-0 loss: 0.751649  [   32/  306]
train() client id: f_00004-0-1 loss: 0.760044  [   64/  306]
train() client id: f_00004-0-2 loss: 0.732299  [   96/  306]
train() client id: f_00004-0-3 loss: 0.779960  [  128/  306]
train() client id: f_00004-0-4 loss: 0.663462  [  160/  306]
train() client id: f_00004-0-5 loss: 0.781825  [  192/  306]
train() client id: f_00004-0-6 loss: 0.628733  [  224/  306]
train() client id: f_00004-0-7 loss: 0.662342  [  256/  306]
train() client id: f_00004-0-8 loss: 0.733685  [  288/  306]
train() client id: f_00004-1-0 loss: 0.738055  [   32/  306]
train() client id: f_00004-1-1 loss: 0.859510  [   64/  306]
train() client id: f_00004-1-2 loss: 0.868871  [   96/  306]
train() client id: f_00004-1-3 loss: 0.657863  [  128/  306]
train() client id: f_00004-1-4 loss: 0.815294  [  160/  306]
train() client id: f_00004-1-5 loss: 0.597580  [  192/  306]
train() client id: f_00004-1-6 loss: 0.752190  [  224/  306]
train() client id: f_00004-1-7 loss: 0.689864  [  256/  306]
train() client id: f_00004-1-8 loss: 0.597843  [  288/  306]
train() client id: f_00004-2-0 loss: 0.802965  [   32/  306]
train() client id: f_00004-2-1 loss: 0.804219  [   64/  306]
train() client id: f_00004-2-2 loss: 0.689819  [   96/  306]
train() client id: f_00004-2-3 loss: 0.614276  [  128/  306]
train() client id: f_00004-2-4 loss: 0.817345  [  160/  306]
train() client id: f_00004-2-5 loss: 0.608151  [  192/  306]
train() client id: f_00004-2-6 loss: 0.731187  [  224/  306]
train() client id: f_00004-2-7 loss: 0.722230  [  256/  306]
train() client id: f_00004-2-8 loss: 0.669090  [  288/  306]
train() client id: f_00004-3-0 loss: 0.661577  [   32/  306]
train() client id: f_00004-3-1 loss: 0.659632  [   64/  306]
train() client id: f_00004-3-2 loss: 0.720521  [   96/  306]
train() client id: f_00004-3-3 loss: 0.714936  [  128/  306]
train() client id: f_00004-3-4 loss: 0.731951  [  160/  306]
train() client id: f_00004-3-5 loss: 0.674239  [  192/  306]
train() client id: f_00004-3-6 loss: 0.701342  [  224/  306]
train() client id: f_00004-3-7 loss: 0.862265  [  256/  306]
train() client id: f_00004-3-8 loss: 0.841752  [  288/  306]
train() client id: f_00004-4-0 loss: 0.772537  [   32/  306]
train() client id: f_00004-4-1 loss: 0.654404  [   64/  306]
train() client id: f_00004-4-2 loss: 0.821176  [   96/  306]
train() client id: f_00004-4-3 loss: 0.698761  [  128/  306]
train() client id: f_00004-4-4 loss: 0.960009  [  160/  306]
train() client id: f_00004-4-5 loss: 0.599342  [  192/  306]
train() client id: f_00004-4-6 loss: 0.788600  [  224/  306]
train() client id: f_00004-4-7 loss: 0.564581  [  256/  306]
train() client id: f_00004-4-8 loss: 0.723243  [  288/  306]
train() client id: f_00004-5-0 loss: 0.664062  [   32/  306]
train() client id: f_00004-5-1 loss: 0.671130  [   64/  306]
train() client id: f_00004-5-2 loss: 0.698804  [   96/  306]
train() client id: f_00004-5-3 loss: 0.783709  [  128/  306]
train() client id: f_00004-5-4 loss: 0.734021  [  160/  306]
train() client id: f_00004-5-5 loss: 0.752819  [  192/  306]
train() client id: f_00004-5-6 loss: 0.780167  [  224/  306]
train() client id: f_00004-5-7 loss: 0.774530  [  256/  306]
train() client id: f_00004-5-8 loss: 0.732747  [  288/  306]
train() client id: f_00004-6-0 loss: 0.812940  [   32/  306]
train() client id: f_00004-6-1 loss: 0.743743  [   64/  306]
train() client id: f_00004-6-2 loss: 0.759103  [   96/  306]
train() client id: f_00004-6-3 loss: 0.675928  [  128/  306]
train() client id: f_00004-6-4 loss: 0.624032  [  160/  306]
train() client id: f_00004-6-5 loss: 0.803857  [  192/  306]
train() client id: f_00004-6-6 loss: 0.731675  [  224/  306]
train() client id: f_00004-6-7 loss: 0.809360  [  256/  306]
train() client id: f_00004-6-8 loss: 0.710522  [  288/  306]
train() client id: f_00004-7-0 loss: 0.729322  [   32/  306]
train() client id: f_00004-7-1 loss: 0.640896  [   64/  306]
train() client id: f_00004-7-2 loss: 0.752341  [   96/  306]
train() client id: f_00004-7-3 loss: 0.706994  [  128/  306]
train() client id: f_00004-7-4 loss: 0.684507  [  160/  306]
train() client id: f_00004-7-5 loss: 0.857913  [  192/  306]
train() client id: f_00004-7-6 loss: 0.702889  [  224/  306]
train() client id: f_00004-7-7 loss: 0.769172  [  256/  306]
train() client id: f_00004-7-8 loss: 0.711865  [  288/  306]
train() client id: f_00004-8-0 loss: 0.608749  [   32/  306]
train() client id: f_00004-8-1 loss: 0.852997  [   64/  306]
train() client id: f_00004-8-2 loss: 0.845477  [   96/  306]
train() client id: f_00004-8-3 loss: 0.830141  [  128/  306]
train() client id: f_00004-8-4 loss: 0.736122  [  160/  306]
train() client id: f_00004-8-5 loss: 0.636018  [  192/  306]
train() client id: f_00004-8-6 loss: 0.735017  [  224/  306]
train() client id: f_00004-8-7 loss: 0.832139  [  256/  306]
train() client id: f_00004-8-8 loss: 0.645561  [  288/  306]
train() client id: f_00004-9-0 loss: 0.652926  [   32/  306]
train() client id: f_00004-9-1 loss: 0.643213  [   64/  306]
train() client id: f_00004-9-2 loss: 0.664173  [   96/  306]
train() client id: f_00004-9-3 loss: 0.892577  [  128/  306]
train() client id: f_00004-9-4 loss: 0.712504  [  160/  306]
train() client id: f_00004-9-5 loss: 0.918319  [  192/  306]
train() client id: f_00004-9-6 loss: 0.763823  [  224/  306]
train() client id: f_00004-9-7 loss: 0.833733  [  256/  306]
train() client id: f_00004-9-8 loss: 0.660418  [  288/  306]
train() client id: f_00004-10-0 loss: 0.724844  [   32/  306]
train() client id: f_00004-10-1 loss: 0.693901  [   64/  306]
train() client id: f_00004-10-2 loss: 0.634173  [   96/  306]
train() client id: f_00004-10-3 loss: 0.859216  [  128/  306]
train() client id: f_00004-10-4 loss: 0.859642  [  160/  306]
train() client id: f_00004-10-5 loss: 0.732412  [  192/  306]
train() client id: f_00004-10-6 loss: 0.756280  [  224/  306]
train() client id: f_00004-10-7 loss: 0.780434  [  256/  306]
train() client id: f_00004-10-8 loss: 0.683800  [  288/  306]
train() client id: f_00004-11-0 loss: 0.688925  [   32/  306]
train() client id: f_00004-11-1 loss: 0.734906  [   64/  306]
train() client id: f_00004-11-2 loss: 0.738820  [   96/  306]
train() client id: f_00004-11-3 loss: 0.683911  [  128/  306]
train() client id: f_00004-11-4 loss: 0.907675  [  160/  306]
train() client id: f_00004-11-5 loss: 0.815526  [  192/  306]
train() client id: f_00004-11-6 loss: 0.762714  [  224/  306]
train() client id: f_00004-11-7 loss: 0.682906  [  256/  306]
train() client id: f_00004-11-8 loss: 0.752434  [  288/  306]
train() client id: f_00004-12-0 loss: 0.754381  [   32/  306]
train() client id: f_00004-12-1 loss: 0.787131  [   64/  306]
train() client id: f_00004-12-2 loss: 0.861235  [   96/  306]
train() client id: f_00004-12-3 loss: 0.809767  [  128/  306]
train() client id: f_00004-12-4 loss: 0.774428  [  160/  306]
train() client id: f_00004-12-5 loss: 0.672739  [  192/  306]
train() client id: f_00004-12-6 loss: 0.756956  [  224/  306]
train() client id: f_00004-12-7 loss: 0.781184  [  256/  306]
train() client id: f_00004-12-8 loss: 0.604008  [  288/  306]
train() client id: f_00004-13-0 loss: 0.674285  [   32/  306]
train() client id: f_00004-13-1 loss: 0.716968  [   64/  306]
train() client id: f_00004-13-2 loss: 0.742061  [   96/  306]
train() client id: f_00004-13-3 loss: 0.717254  [  128/  306]
train() client id: f_00004-13-4 loss: 0.808215  [  160/  306]
train() client id: f_00004-13-5 loss: 0.757096  [  192/  306]
train() client id: f_00004-13-6 loss: 0.622244  [  224/  306]
train() client id: f_00004-13-7 loss: 0.846007  [  256/  306]
train() client id: f_00004-13-8 loss: 0.930164  [  288/  306]
train() client id: f_00004-14-0 loss: 0.858645  [   32/  306]
train() client id: f_00004-14-1 loss: 0.846883  [   64/  306]
train() client id: f_00004-14-2 loss: 0.634634  [   96/  306]
train() client id: f_00004-14-3 loss: 0.722656  [  128/  306]
train() client id: f_00004-14-4 loss: 0.638072  [  160/  306]
train() client id: f_00004-14-5 loss: 0.713824  [  192/  306]
train() client id: f_00004-14-6 loss: 0.711938  [  224/  306]
train() client id: f_00004-14-7 loss: 0.815052  [  256/  306]
train() client id: f_00004-14-8 loss: 0.832392  [  288/  306]
train() client id: f_00004-15-0 loss: 0.711047  [   32/  306]
train() client id: f_00004-15-1 loss: 0.741540  [   64/  306]
train() client id: f_00004-15-2 loss: 0.758862  [   96/  306]
train() client id: f_00004-15-3 loss: 0.757950  [  128/  306]
train() client id: f_00004-15-4 loss: 0.769083  [  160/  306]
train() client id: f_00004-15-5 loss: 0.706996  [  192/  306]
train() client id: f_00004-15-6 loss: 0.738214  [  224/  306]
train() client id: f_00004-15-7 loss: 0.808479  [  256/  306]
train() client id: f_00004-15-8 loss: 0.778553  [  288/  306]
train() client id: f_00005-0-0 loss: 0.139918  [   32/  146]
train() client id: f_00005-0-1 loss: 0.505338  [   64/  146]
train() client id: f_00005-0-2 loss: 0.363754  [   96/  146]
train() client id: f_00005-0-3 loss: 0.384295  [  128/  146]
train() client id: f_00005-1-0 loss: 0.324571  [   32/  146]
train() client id: f_00005-1-1 loss: 0.437661  [   64/  146]
train() client id: f_00005-1-2 loss: 0.298652  [   96/  146]
train() client id: f_00005-1-3 loss: 0.445566  [  128/  146]
train() client id: f_00005-2-0 loss: 0.233180  [   32/  146]
train() client id: f_00005-2-1 loss: 0.338823  [   64/  146]
train() client id: f_00005-2-2 loss: 0.522584  [   96/  146]
train() client id: f_00005-2-3 loss: 0.191974  [  128/  146]
train() client id: f_00005-3-0 loss: 0.036141  [   32/  146]
train() client id: f_00005-3-1 loss: 0.534104  [   64/  146]
train() client id: f_00005-3-2 loss: 0.492351  [   96/  146]
train() client id: f_00005-3-3 loss: 0.384549  [  128/  146]
train() client id: f_00005-4-0 loss: 0.381886  [   32/  146]
train() client id: f_00005-4-1 loss: 0.208752  [   64/  146]
train() client id: f_00005-4-2 loss: 0.364801  [   96/  146]
train() client id: f_00005-4-3 loss: 0.347537  [  128/  146]
train() client id: f_00005-5-0 loss: 0.244311  [   32/  146]
train() client id: f_00005-5-1 loss: 0.127820  [   64/  146]
train() client id: f_00005-5-2 loss: 0.269587  [   96/  146]
train() client id: f_00005-5-3 loss: 0.679922  [  128/  146]
train() client id: f_00005-6-0 loss: 0.437726  [   32/  146]
train() client id: f_00005-6-1 loss: 0.356296  [   64/  146]
train() client id: f_00005-6-2 loss: 0.467761  [   96/  146]
train() client id: f_00005-6-3 loss: 0.160668  [  128/  146]
train() client id: f_00005-7-0 loss: 0.141747  [   32/  146]
train() client id: f_00005-7-1 loss: 0.613022  [   64/  146]
train() client id: f_00005-7-2 loss: 0.308298  [   96/  146]
train() client id: f_00005-7-3 loss: 0.395273  [  128/  146]
train() client id: f_00005-8-0 loss: 0.292451  [   32/  146]
train() client id: f_00005-8-1 loss: 0.228312  [   64/  146]
train() client id: f_00005-8-2 loss: 0.479229  [   96/  146]
train() client id: f_00005-8-3 loss: 0.436814  [  128/  146]
train() client id: f_00005-9-0 loss: 0.358817  [   32/  146]
train() client id: f_00005-9-1 loss: 0.693944  [   64/  146]
train() client id: f_00005-9-2 loss: 0.146731  [   96/  146]
train() client id: f_00005-9-3 loss: 0.202486  [  128/  146]
train() client id: f_00005-10-0 loss: 0.464503  [   32/  146]
train() client id: f_00005-10-1 loss: 0.121383  [   64/  146]
train() client id: f_00005-10-2 loss: 0.373537  [   96/  146]
train() client id: f_00005-10-3 loss: 0.461280  [  128/  146]
train() client id: f_00005-11-0 loss: 0.231484  [   32/  146]
train() client id: f_00005-11-1 loss: 0.503003  [   64/  146]
train() client id: f_00005-11-2 loss: 0.283264  [   96/  146]
train() client id: f_00005-11-3 loss: 0.250066  [  128/  146]
train() client id: f_00005-12-0 loss: 0.334679  [   32/  146]
train() client id: f_00005-12-1 loss: 0.174930  [   64/  146]
train() client id: f_00005-12-2 loss: 0.439783  [   96/  146]
train() client id: f_00005-12-3 loss: 0.348709  [  128/  146]
train() client id: f_00005-13-0 loss: 0.404444  [   32/  146]
train() client id: f_00005-13-1 loss: 0.332227  [   64/  146]
train() client id: f_00005-13-2 loss: 0.414450  [   96/  146]
train() client id: f_00005-13-3 loss: 0.096820  [  128/  146]
train() client id: f_00005-14-0 loss: 0.326262  [   32/  146]
train() client id: f_00005-14-1 loss: 0.446168  [   64/  146]
train() client id: f_00005-14-2 loss: 0.315441  [   96/  146]
train() client id: f_00005-14-3 loss: 0.312949  [  128/  146]
train() client id: f_00005-15-0 loss: 0.244705  [   32/  146]
train() client id: f_00005-15-1 loss: 0.176519  [   64/  146]
train() client id: f_00005-15-2 loss: 0.352011  [   96/  146]
train() client id: f_00005-15-3 loss: 0.391581  [  128/  146]
train() client id: f_00006-0-0 loss: 0.474890  [   32/   54]
train() client id: f_00006-1-0 loss: 0.521420  [   32/   54]
train() client id: f_00006-2-0 loss: 0.464946  [   32/   54]
train() client id: f_00006-3-0 loss: 0.509176  [   32/   54]
train() client id: f_00006-4-0 loss: 0.447522  [   32/   54]
train() client id: f_00006-5-0 loss: 0.471045  [   32/   54]
train() client id: f_00006-6-0 loss: 0.465594  [   32/   54]
train() client id: f_00006-7-0 loss: 0.511402  [   32/   54]
train() client id: f_00006-8-0 loss: 0.496131  [   32/   54]
train() client id: f_00006-9-0 loss: 0.461427  [   32/   54]
train() client id: f_00006-10-0 loss: 0.461302  [   32/   54]
train() client id: f_00006-11-0 loss: 0.512778  [   32/   54]
train() client id: f_00006-12-0 loss: 0.461011  [   32/   54]
train() client id: f_00006-13-0 loss: 0.495543  [   32/   54]
train() client id: f_00006-14-0 loss: 0.426413  [   32/   54]
train() client id: f_00006-15-0 loss: 0.421981  [   32/   54]
train() client id: f_00007-0-0 loss: 0.496959  [   32/  179]
train() client id: f_00007-0-1 loss: 0.464420  [   64/  179]
train() client id: f_00007-0-2 loss: 0.350882  [   96/  179]
train() client id: f_00007-0-3 loss: 0.568431  [  128/  179]
train() client id: f_00007-0-4 loss: 0.397653  [  160/  179]
train() client id: f_00007-1-0 loss: 0.392411  [   32/  179]
train() client id: f_00007-1-1 loss: 0.502133  [   64/  179]
train() client id: f_00007-1-2 loss: 0.609242  [   96/  179]
train() client id: f_00007-1-3 loss: 0.327751  [  128/  179]
train() client id: f_00007-1-4 loss: 0.461159  [  160/  179]
train() client id: f_00007-2-0 loss: 0.329426  [   32/  179]
train() client id: f_00007-2-1 loss: 0.291439  [   64/  179]
train() client id: f_00007-2-2 loss: 0.578394  [   96/  179]
train() client id: f_00007-2-3 loss: 0.433662  [  128/  179]
train() client id: f_00007-2-4 loss: 0.561807  [  160/  179]
train() client id: f_00007-3-0 loss: 0.475030  [   32/  179]
train() client id: f_00007-3-1 loss: 0.400181  [   64/  179]
train() client id: f_00007-3-2 loss: 0.549528  [   96/  179]
train() client id: f_00007-3-3 loss: 0.278528  [  128/  179]
train() client id: f_00007-3-4 loss: 0.428170  [  160/  179]
train() client id: f_00007-4-0 loss: 0.291833  [   32/  179]
train() client id: f_00007-4-1 loss: 0.582485  [   64/  179]
train() client id: f_00007-4-2 loss: 0.475330  [   96/  179]
train() client id: f_00007-4-3 loss: 0.301271  [  128/  179]
train() client id: f_00007-4-4 loss: 0.547904  [  160/  179]
train() client id: f_00007-5-0 loss: 0.425867  [   32/  179]
train() client id: f_00007-5-1 loss: 0.304035  [   64/  179]
train() client id: f_00007-5-2 loss: 0.536882  [   96/  179]
train() client id: f_00007-5-3 loss: 0.352199  [  128/  179]
train() client id: f_00007-5-4 loss: 0.291986  [  160/  179]
train() client id: f_00007-6-0 loss: 0.340298  [   32/  179]
train() client id: f_00007-6-1 loss: 0.344861  [   64/  179]
train() client id: f_00007-6-2 loss: 0.451616  [   96/  179]
train() client id: f_00007-6-3 loss: 0.528675  [  128/  179]
train() client id: f_00007-6-4 loss: 0.493754  [  160/  179]
train() client id: f_00007-7-0 loss: 0.525693  [   32/  179]
train() client id: f_00007-7-1 loss: 0.314728  [   64/  179]
train() client id: f_00007-7-2 loss: 0.236159  [   96/  179]
train() client id: f_00007-7-3 loss: 0.402533  [  128/  179]
train() client id: f_00007-7-4 loss: 0.568955  [  160/  179]
train() client id: f_00007-8-0 loss: 0.568526  [   32/  179]
train() client id: f_00007-8-1 loss: 0.352163  [   64/  179]
train() client id: f_00007-8-2 loss: 0.374909  [   96/  179]
train() client id: f_00007-8-3 loss: 0.340363  [  128/  179]
train() client id: f_00007-8-4 loss: 0.387353  [  160/  179]
train() client id: f_00007-9-0 loss: 0.427911  [   32/  179]
train() client id: f_00007-9-1 loss: 0.274662  [   64/  179]
train() client id: f_00007-9-2 loss: 0.222379  [   96/  179]
train() client id: f_00007-9-3 loss: 0.360750  [  128/  179]
train() client id: f_00007-9-4 loss: 0.830009  [  160/  179]
train() client id: f_00007-10-0 loss: 0.340179  [   32/  179]
train() client id: f_00007-10-1 loss: 0.533030  [   64/  179]
train() client id: f_00007-10-2 loss: 0.436796  [   96/  179]
train() client id: f_00007-10-3 loss: 0.257438  [  128/  179]
train() client id: f_00007-10-4 loss: 0.506895  [  160/  179]
train() client id: f_00007-11-0 loss: 0.558597  [   32/  179]
train() client id: f_00007-11-1 loss: 0.500409  [   64/  179]
train() client id: f_00007-11-2 loss: 0.336901  [   96/  179]
train() client id: f_00007-11-3 loss: 0.332120  [  128/  179]
train() client id: f_00007-11-4 loss: 0.340534  [  160/  179]
train() client id: f_00007-12-0 loss: 0.396753  [   32/  179]
train() client id: f_00007-12-1 loss: 0.213903  [   64/  179]
train() client id: f_00007-12-2 loss: 0.310782  [   96/  179]
train() client id: f_00007-12-3 loss: 0.618424  [  128/  179]
train() client id: f_00007-12-4 loss: 0.361124  [  160/  179]
train() client id: f_00007-13-0 loss: 0.448690  [   32/  179]
train() client id: f_00007-13-1 loss: 0.343107  [   64/  179]
train() client id: f_00007-13-2 loss: 0.632865  [   96/  179]
train() client id: f_00007-13-3 loss: 0.420400  [  128/  179]
train() client id: f_00007-13-4 loss: 0.236670  [  160/  179]
train() client id: f_00007-14-0 loss: 0.340880  [   32/  179]
train() client id: f_00007-14-1 loss: 0.584457  [   64/  179]
train() client id: f_00007-14-2 loss: 0.326581  [   96/  179]
train() client id: f_00007-14-3 loss: 0.309377  [  128/  179]
train() client id: f_00007-14-4 loss: 0.508428  [  160/  179]
train() client id: f_00007-15-0 loss: 0.248403  [   32/  179]
train() client id: f_00007-15-1 loss: 0.394332  [   64/  179]
train() client id: f_00007-15-2 loss: 0.506130  [   96/  179]
train() client id: f_00007-15-3 loss: 0.443504  [  128/  179]
train() client id: f_00007-15-4 loss: 0.310707  [  160/  179]
train() client id: f_00008-0-0 loss: 0.718536  [   32/  130]
train() client id: f_00008-0-1 loss: 0.732076  [   64/  130]
train() client id: f_00008-0-2 loss: 0.731524  [   96/  130]
train() client id: f_00008-0-3 loss: 0.770742  [  128/  130]
train() client id: f_00008-1-0 loss: 0.848088  [   32/  130]
train() client id: f_00008-1-1 loss: 0.668808  [   64/  130]
train() client id: f_00008-1-2 loss: 0.808738  [   96/  130]
train() client id: f_00008-1-3 loss: 0.661970  [  128/  130]
train() client id: f_00008-2-0 loss: 0.645316  [   32/  130]
train() client id: f_00008-2-1 loss: 0.771903  [   64/  130]
train() client id: f_00008-2-2 loss: 0.721490  [   96/  130]
train() client id: f_00008-2-3 loss: 0.832545  [  128/  130]
train() client id: f_00008-3-0 loss: 0.763611  [   32/  130]
train() client id: f_00008-3-1 loss: 0.586713  [   64/  130]
train() client id: f_00008-3-2 loss: 0.792157  [   96/  130]
train() client id: f_00008-3-3 loss: 0.813094  [  128/  130]
train() client id: f_00008-4-0 loss: 0.657218  [   32/  130]
train() client id: f_00008-4-1 loss: 0.857454  [   64/  130]
train() client id: f_00008-4-2 loss: 0.758231  [   96/  130]
train() client id: f_00008-4-3 loss: 0.702970  [  128/  130]
train() client id: f_00008-5-0 loss: 0.774711  [   32/  130]
train() client id: f_00008-5-1 loss: 0.675166  [   64/  130]
train() client id: f_00008-5-2 loss: 0.736453  [   96/  130]
train() client id: f_00008-5-3 loss: 0.780352  [  128/  130]
train() client id: f_00008-6-0 loss: 0.773179  [   32/  130]
train() client id: f_00008-6-1 loss: 0.728394  [   64/  130]
train() client id: f_00008-6-2 loss: 0.750505  [   96/  130]
train() client id: f_00008-6-3 loss: 0.725522  [  128/  130]
train() client id: f_00008-7-0 loss: 0.704724  [   32/  130]
train() client id: f_00008-7-1 loss: 0.696470  [   64/  130]
train() client id: f_00008-7-2 loss: 0.769866  [   96/  130]
train() client id: f_00008-7-3 loss: 0.801806  [  128/  130]
train() client id: f_00008-8-0 loss: 0.788811  [   32/  130]
train() client id: f_00008-8-1 loss: 0.687029  [   64/  130]
train() client id: f_00008-8-2 loss: 0.830013  [   96/  130]
train() client id: f_00008-8-3 loss: 0.665597  [  128/  130]
train() client id: f_00008-9-0 loss: 0.796394  [   32/  130]
train() client id: f_00008-9-1 loss: 0.656606  [   64/  130]
train() client id: f_00008-9-2 loss: 0.783238  [   96/  130]
train() client id: f_00008-9-3 loss: 0.726047  [  128/  130]
train() client id: f_00008-10-0 loss: 0.639965  [   32/  130]
train() client id: f_00008-10-1 loss: 0.758913  [   64/  130]
train() client id: f_00008-10-2 loss: 0.821634  [   96/  130]
train() client id: f_00008-10-3 loss: 0.748627  [  128/  130]
train() client id: f_00008-11-0 loss: 0.688902  [   32/  130]
train() client id: f_00008-11-1 loss: 0.703345  [   64/  130]
train() client id: f_00008-11-2 loss: 0.748458  [   96/  130]
train() client id: f_00008-11-3 loss: 0.786248  [  128/  130]
train() client id: f_00008-12-0 loss: 0.706624  [   32/  130]
train() client id: f_00008-12-1 loss: 0.800438  [   64/  130]
train() client id: f_00008-12-2 loss: 0.750199  [   96/  130]
train() client id: f_00008-12-3 loss: 0.714784  [  128/  130]
train() client id: f_00008-13-0 loss: 0.721276  [   32/  130]
train() client id: f_00008-13-1 loss: 0.623523  [   64/  130]
train() client id: f_00008-13-2 loss: 0.794651  [   96/  130]
train() client id: f_00008-13-3 loss: 0.820701  [  128/  130]
train() client id: f_00008-14-0 loss: 0.637130  [   32/  130]
train() client id: f_00008-14-1 loss: 0.764100  [   64/  130]
train() client id: f_00008-14-2 loss: 0.741947  [   96/  130]
train() client id: f_00008-14-3 loss: 0.788394  [  128/  130]
train() client id: f_00008-15-0 loss: 0.830033  [   32/  130]
train() client id: f_00008-15-1 loss: 0.769453  [   64/  130]
train() client id: f_00008-15-2 loss: 0.643904  [   96/  130]
train() client id: f_00008-15-3 loss: 0.728200  [  128/  130]
train() client id: f_00009-0-0 loss: 1.071098  [   32/  118]
train() client id: f_00009-0-1 loss: 0.881391  [   64/  118]
train() client id: f_00009-0-2 loss: 1.056834  [   96/  118]
train() client id: f_00009-1-0 loss: 0.990948  [   32/  118]
train() client id: f_00009-1-1 loss: 0.915613  [   64/  118]
train() client id: f_00009-1-2 loss: 0.895357  [   96/  118]
train() client id: f_00009-2-0 loss: 0.900145  [   32/  118]
train() client id: f_00009-2-1 loss: 0.833933  [   64/  118]
train() client id: f_00009-2-2 loss: 0.902675  [   96/  118]
train() client id: f_00009-3-0 loss: 0.915306  [   32/  118]
train() client id: f_00009-3-1 loss: 0.996849  [   64/  118]
train() client id: f_00009-3-2 loss: 0.802860  [   96/  118]
train() client id: f_00009-4-0 loss: 0.741123  [   32/  118]
train() client id: f_00009-4-1 loss: 0.822128  [   64/  118]
train() client id: f_00009-4-2 loss: 0.925704  [   96/  118]
train() client id: f_00009-5-0 loss: 0.749048  [   32/  118]
train() client id: f_00009-5-1 loss: 0.824035  [   64/  118]
train() client id: f_00009-5-2 loss: 0.865291  [   96/  118]
train() client id: f_00009-6-0 loss: 0.622745  [   32/  118]
train() client id: f_00009-6-1 loss: 0.811242  [   64/  118]
train() client id: f_00009-6-2 loss: 0.915714  [   96/  118]
train() client id: f_00009-7-0 loss: 0.970720  [   32/  118]
train() client id: f_00009-7-1 loss: 0.692339  [   64/  118]
train() client id: f_00009-7-2 loss: 0.741951  [   96/  118]
train() client id: f_00009-8-0 loss: 0.909960  [   32/  118]
train() client id: f_00009-8-1 loss: 0.724864  [   64/  118]
train() client id: f_00009-8-2 loss: 0.693797  [   96/  118]
train() client id: f_00009-9-0 loss: 0.802080  [   32/  118]
train() client id: f_00009-9-1 loss: 0.817448  [   64/  118]
train() client id: f_00009-9-2 loss: 0.794639  [   96/  118]
train() client id: f_00009-10-0 loss: 0.869736  [   32/  118]
train() client id: f_00009-10-1 loss: 0.684080  [   64/  118]
train() client id: f_00009-10-2 loss: 0.894959  [   96/  118]
train() client id: f_00009-11-0 loss: 0.716352  [   32/  118]
train() client id: f_00009-11-1 loss: 0.742369  [   64/  118]
train() client id: f_00009-11-2 loss: 0.962563  [   96/  118]
train() client id: f_00009-12-0 loss: 0.816968  [   32/  118]
train() client id: f_00009-12-1 loss: 0.801655  [   64/  118]
train() client id: f_00009-12-2 loss: 0.752863  [   96/  118]
train() client id: f_00009-13-0 loss: 0.817523  [   32/  118]
train() client id: f_00009-13-1 loss: 0.794034  [   64/  118]
train() client id: f_00009-13-2 loss: 0.835080  [   96/  118]
train() client id: f_00009-14-0 loss: 0.849105  [   32/  118]
train() client id: f_00009-14-1 loss: 0.768116  [   64/  118]
train() client id: f_00009-14-2 loss: 0.776615  [   96/  118]
train() client id: f_00009-15-0 loss: 1.010603  [   32/  118]
train() client id: f_00009-15-1 loss: 0.748193  [   64/  118]
train() client id: f_00009-15-2 loss: 0.816260  [   96/  118]
At round 43 accuracy: 0.6445623342175066
At round 43 training accuracy: 0.590878604963112
At round 43 training loss: 0.8370718286385838
update_location
xs = 8.927491 336.223621 5.882650 0.934260 -252.581990 -100.230757 -60.849135 -5.143845 -275.120581 20.134486 
ys = -327.390647 7.291448 225.684448 -47.290817 -9.642386 0.794442 -1.381692 221.628436 25.881276 -762.232496 
xs mean: -32.18237997052122
ys mean: -66.66579882624052
dists_uav = 342.438806 350.855367 246.917144 110.622304 271.828323 141.586849 117.066333 243.198731 293.872718 769.027812 
uav_gains = -118.744283 -119.218838 -111.036200 -101.096135 -113.236858 -103.777678 -101.710951 -110.729466 -115.210507 -129.112345 
uav_gains_db_mean: -112.38732619368005
dists_bs = 535.000640 538.029506 187.849531 283.489103 200.279245 189.573247 210.097409 176.140071 179.612150 957.330778 
bs_gains = -115.960980 -116.029630 -103.233742 -108.238077 -104.012866 -103.344817 -104.594839 -102.451088 -102.688459 -123.036796 
bs_gains_db_mean: -108.35912926399787
Round 44
-------------------------------
ene_coms = [0.02373063 0.02515336 0.0072773  0.00660265 0.00755806 0.00747111
 0.00678373 0.00701513 0.00709266 0.04312292]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.34671184 10.91722047  5.04894001  1.8048613  12.32174785  5.93018774
  2.24659997  7.24217102  5.28613518  5.28400375]
obj_prev = 61.42857914336427
eta_min = 2.0016200073748885e-18	eta_max = 0.7942268365975043
af = 12.80194016624056	bf = 1.8591801755561617	zeta = 14.082134182864616	eta = 0.9090909090909091
af = 12.80194016624056	bf = 1.8591801755561617	zeta = 33.87268934109234	eta = 0.37794283286240293
af = 12.80194016624056	bf = 1.8591801755561617	zeta = 23.013360458427144	eta = 0.5562829552583957
af = 12.80194016624056	bf = 1.8591801755561617	zeta = 21.110838845230937	eta = 0.6064155129076073
af = 12.80194016624056	bf = 1.8591801755561617	zeta = 20.993156855861404	eta = 0.6098149151239342
af = 12.80194016624056	bf = 1.8591801755561617	zeta = 20.992645171794173	eta = 0.6098297790238132
af = 12.80194016624056	bf = 1.8591801755561617	zeta = 20.99264516204772	eta = 0.6098297793069446
eta = 0.6098297793069446
ene_coms = [0.02373063 0.02515336 0.0072773  0.00660265 0.00755806 0.00747111
 0.00678373 0.00701513 0.00709266 0.04312292]
ene_comp = [0.04081118 0.08583303 0.04016338 0.01392762 0.09911286 0.04728914
 0.0174905  0.05797778 0.04210677 0.03821999]
ene_total = [2.16874809 3.72938921 1.59411223 0.68986283 3.58437984 1.8400659
 0.81566832 2.18390623 1.65321022 2.7333023 ]
ti_comp = [0.58879467 0.57456738 0.75332798 0.76007448 0.75052035 0.75138993
 0.75826365 0.75594968 0.75517435 0.39487182]
ti_coms = [0.23730632 0.25153362 0.07277301 0.06602651 0.07558064 0.07471106
 0.06783734 0.07015131 0.07092664 0.43122917]
t_total = [27.75882721 27.75882721 27.75882721 27.75882721 27.75882721 27.75882721
 27.75882721 27.75882721 27.75882721 27.75882721]
ene_coms = [0.02373063 0.02515336 0.0072773  0.00660265 0.00755806 0.00747111
 0.00678373 0.00701513 0.00709266 0.04312292]
ene_comp = [1.22543324e-05 1.19718552e-04 7.13514194e-06 2.92279816e-07
 1.08030178e-04 1.17066755e-05 5.81629688e-07 2.13146928e-05
 8.18165862e-06 2.23789387e-05]
ene_total = [0.79781369 0.8492316  0.24477323 0.22187356 0.25759778 0.25143911
 0.22796808 0.2364402  0.23860418 1.44977771]
optimize_network iter = 0 obj = 4.7755191390358815
eta = 0.6098297793069446
freqs = [34656544.67921353 74693617.96983218 26657300.73319389  9162012.71652742
 66029428.43405407 31467777.29427525 11533260.69239163 38347647.73645755
 27878840.46958886 48395440.3267988 ]
eta_min = 0.6098297793069456	eta_max = 0.6098297793069429
af = 0.008259975635494037	bf = 1.8591801755561617	zeta = 0.009085973199043442	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02373063 0.02515336 0.0072773  0.00660265 0.00755806 0.00747111
 0.00678373 0.00701513 0.00709266 0.04312292]
ene_comp = [2.36154608e-06 2.30710958e-05 1.37502117e-06 5.63255697e-08
 2.08186162e-05 2.25600651e-06 1.12086507e-07 4.10757826e-06
 1.57669657e-06 4.31267028e-06]
ene_total = [3.4226879  3.63085552 1.04970476 0.95221899 1.09299947 1.07778163
 0.97834218 1.01228965 1.02310616 6.21965554]
ti_comp = [0.58879467 0.57456738 0.75332798 0.76007448 0.75052035 0.75138993
 0.75826365 0.75594968 0.75517435 0.39487182]
ti_coms = [0.23730632 0.25153362 0.07277301 0.06602651 0.07558064 0.07471106
 0.06783734 0.07015131 0.07092664 0.43122917]
t_total = [27.75882721 27.75882721 27.75882721 27.75882721 27.75882721 27.75882721
 27.75882721 27.75882721 27.75882721 27.75882721]
ene_coms = [0.02373063 0.02515336 0.0072773  0.00660265 0.00755806 0.00747111
 0.00678373 0.00701513 0.00709266 0.04312292]
ene_comp = [1.22543324e-05 1.19718552e-04 7.13514194e-06 2.92279816e-07
 1.08030178e-04 1.17066755e-05 5.81629688e-07 2.13146928e-05
 8.18165862e-06 2.23789387e-05]
ene_total = [0.79781369 0.8492316  0.24477323 0.22187356 0.25759778 0.25143911
 0.22796808 0.2364402  0.23860418 1.44977771]
optimize_network iter = 1 obj = 4.775519139035861
eta = 0.6098297793069429
freqs = [34656544.67921352 74693617.96983212 26657300.73319392  9162012.71652743
 66029428.43405412 31467777.29427527 11533260.69239164 38347647.73645758
 27878840.46958889 48395440.32679863]
Done!
ene_coms = [0.02373063 0.02515336 0.0072773  0.00660265 0.00755806 0.00747111
 0.00678373 0.00701513 0.00709266 0.04312292]
ene_comp = [1.21068470e-05 1.18277695e-04 7.04926789e-06 2.88762121e-07
 1.06729995e-04 1.15657814e-05 5.74629561e-07 2.10581627e-05
 8.08318936e-06 2.21096000e-05]
ene_total = [0.02374274 0.02527164 0.00728435 0.00660294 0.00766479 0.00748267
 0.00678431 0.00703619 0.00710075 0.04314503]
At round 44 energy consumption: 0.14211540567866887
At round 44 eta: 0.6098297793069429
At round 44 a_n: 13.110585589604415
At round 44 local rounds: 16.194911702955718
At round 44 global rounds: 33.602219990844404
gradient difference: 0.3085768520832062
train() client id: f_00000-0-0 loss: 1.148237  [   32/  126]
train() client id: f_00000-0-1 loss: 1.280883  [   64/  126]
train() client id: f_00000-0-2 loss: 1.224854  [   96/  126]
train() client id: f_00000-1-0 loss: 1.194038  [   32/  126]
train() client id: f_00000-1-1 loss: 1.191682  [   64/  126]
train() client id: f_00000-1-2 loss: 1.118583  [   96/  126]
train() client id: f_00000-2-0 loss: 1.134083  [   32/  126]
train() client id: f_00000-2-1 loss: 1.116925  [   64/  126]
train() client id: f_00000-2-2 loss: 1.014557  [   96/  126]
train() client id: f_00000-3-0 loss: 0.957972  [   32/  126]
train() client id: f_00000-3-1 loss: 1.209862  [   64/  126]
train() client id: f_00000-3-2 loss: 0.977094  [   96/  126]
train() client id: f_00000-4-0 loss: 1.022973  [   32/  126]
train() client id: f_00000-4-1 loss: 0.879917  [   64/  126]
train() client id: f_00000-4-2 loss: 0.876681  [   96/  126]
train() client id: f_00000-5-0 loss: 1.063196  [   32/  126]
train() client id: f_00000-5-1 loss: 0.860255  [   64/  126]
train() client id: f_00000-5-2 loss: 0.959049  [   96/  126]
train() client id: f_00000-6-0 loss: 0.786151  [   32/  126]
train() client id: f_00000-6-1 loss: 0.934060  [   64/  126]
train() client id: f_00000-6-2 loss: 0.938213  [   96/  126]
train() client id: f_00000-7-0 loss: 1.004627  [   32/  126]
train() client id: f_00000-7-1 loss: 0.821615  [   64/  126]
train() client id: f_00000-7-2 loss: 0.855795  [   96/  126]
train() client id: f_00000-8-0 loss: 0.853541  [   32/  126]
train() client id: f_00000-8-1 loss: 0.960624  [   64/  126]
train() client id: f_00000-8-2 loss: 0.857756  [   96/  126]
train() client id: f_00000-9-0 loss: 0.923106  [   32/  126]
train() client id: f_00000-9-1 loss: 0.804294  [   64/  126]
train() client id: f_00000-9-2 loss: 0.967340  [   96/  126]
train() client id: f_00000-10-0 loss: 0.851043  [   32/  126]
train() client id: f_00000-10-1 loss: 0.885801  [   64/  126]
train() client id: f_00000-10-2 loss: 0.858997  [   96/  126]
train() client id: f_00000-11-0 loss: 0.744880  [   32/  126]
train() client id: f_00000-11-1 loss: 0.954433  [   64/  126]
train() client id: f_00000-11-2 loss: 0.846964  [   96/  126]
train() client id: f_00000-12-0 loss: 0.941087  [   32/  126]
train() client id: f_00000-12-1 loss: 0.910913  [   64/  126]
train() client id: f_00000-12-2 loss: 0.735806  [   96/  126]
train() client id: f_00000-13-0 loss: 0.916310  [   32/  126]
train() client id: f_00000-13-1 loss: 0.828270  [   64/  126]
train() client id: f_00000-13-2 loss: 0.725348  [   96/  126]
train() client id: f_00000-14-0 loss: 0.811999  [   32/  126]
train() client id: f_00000-14-1 loss: 0.787345  [   64/  126]
train() client id: f_00000-14-2 loss: 0.839988  [   96/  126]
train() client id: f_00000-15-0 loss: 0.884288  [   32/  126]
train() client id: f_00000-15-1 loss: 0.773685  [   64/  126]
train() client id: f_00000-15-2 loss: 0.855359  [   96/  126]
train() client id: f_00001-0-0 loss: 0.439325  [   32/  265]
train() client id: f_00001-0-1 loss: 0.461702  [   64/  265]
train() client id: f_00001-0-2 loss: 0.331153  [   96/  265]
train() client id: f_00001-0-3 loss: 0.422370  [  128/  265]
train() client id: f_00001-0-4 loss: 0.344000  [  160/  265]
train() client id: f_00001-0-5 loss: 0.443858  [  192/  265]
train() client id: f_00001-0-6 loss: 0.452595  [  224/  265]
train() client id: f_00001-0-7 loss: 0.496318  [  256/  265]
train() client id: f_00001-1-0 loss: 0.359942  [   32/  265]
train() client id: f_00001-1-1 loss: 0.486451  [   64/  265]
train() client id: f_00001-1-2 loss: 0.367732  [   96/  265]
train() client id: f_00001-1-3 loss: 0.487588  [  128/  265]
train() client id: f_00001-1-4 loss: 0.461496  [  160/  265]
train() client id: f_00001-1-5 loss: 0.356874  [  192/  265]
train() client id: f_00001-1-6 loss: 0.464769  [  224/  265]
train() client id: f_00001-1-7 loss: 0.342921  [  256/  265]
train() client id: f_00001-2-0 loss: 0.435847  [   32/  265]
train() client id: f_00001-2-1 loss: 0.394680  [   64/  265]
train() client id: f_00001-2-2 loss: 0.449648  [   96/  265]
train() client id: f_00001-2-3 loss: 0.317171  [  128/  265]
train() client id: f_00001-2-4 loss: 0.464960  [  160/  265]
train() client id: f_00001-2-5 loss: 0.396770  [  192/  265]
train() client id: f_00001-2-6 loss: 0.364039  [  224/  265]
train() client id: f_00001-2-7 loss: 0.366031  [  256/  265]
train() client id: f_00001-3-0 loss: 0.547662  [   32/  265]
train() client id: f_00001-3-1 loss: 0.369163  [   64/  265]
train() client id: f_00001-3-2 loss: 0.480584  [   96/  265]
train() client id: f_00001-3-3 loss: 0.383848  [  128/  265]
train() client id: f_00001-3-4 loss: 0.374750  [  160/  265]
train() client id: f_00001-3-5 loss: 0.436553  [  192/  265]
train() client id: f_00001-3-6 loss: 0.336043  [  224/  265]
train() client id: f_00001-3-7 loss: 0.296495  [  256/  265]
train() client id: f_00001-4-0 loss: 0.384422  [   32/  265]
train() client id: f_00001-4-1 loss: 0.436101  [   64/  265]
train() client id: f_00001-4-2 loss: 0.295016  [   96/  265]
train() client id: f_00001-4-3 loss: 0.355413  [  128/  265]
train() client id: f_00001-4-4 loss: 0.444582  [  160/  265]
train() client id: f_00001-4-5 loss: 0.515946  [  192/  265]
train() client id: f_00001-4-6 loss: 0.308006  [  224/  265]
train() client id: f_00001-4-7 loss: 0.456055  [  256/  265]
train() client id: f_00001-5-0 loss: 0.377073  [   32/  265]
train() client id: f_00001-5-1 loss: 0.409584  [   64/  265]
train() client id: f_00001-5-2 loss: 0.429049  [   96/  265]
train() client id: f_00001-5-3 loss: 0.547855  [  128/  265]
train() client id: f_00001-5-4 loss: 0.389229  [  160/  265]
train() client id: f_00001-5-5 loss: 0.319327  [  192/  265]
train() client id: f_00001-5-6 loss: 0.307274  [  224/  265]
train() client id: f_00001-5-7 loss: 0.398942  [  256/  265]
train() client id: f_00001-6-0 loss: 0.358960  [   32/  265]
train() client id: f_00001-6-1 loss: 0.450237  [   64/  265]
train() client id: f_00001-6-2 loss: 0.449533  [   96/  265]
train() client id: f_00001-6-3 loss: 0.411494  [  128/  265]
train() client id: f_00001-6-4 loss: 0.391738  [  160/  265]
train() client id: f_00001-6-5 loss: 0.320107  [  192/  265]
train() client id: f_00001-6-6 loss: 0.359975  [  224/  265]
train() client id: f_00001-6-7 loss: 0.332744  [  256/  265]
train() client id: f_00001-7-0 loss: 0.552231  [   32/  265]
train() client id: f_00001-7-1 loss: 0.310375  [   64/  265]
train() client id: f_00001-7-2 loss: 0.430928  [   96/  265]
train() client id: f_00001-7-3 loss: 0.314697  [  128/  265]
train() client id: f_00001-7-4 loss: 0.289219  [  160/  265]
train() client id: f_00001-7-5 loss: 0.380816  [  192/  265]
train() client id: f_00001-7-6 loss: 0.526392  [  224/  265]
train() client id: f_00001-7-7 loss: 0.341901  [  256/  265]
train() client id: f_00001-8-0 loss: 0.328901  [   32/  265]
train() client id: f_00001-8-1 loss: 0.334249  [   64/  265]
train() client id: f_00001-8-2 loss: 0.583023  [   96/  265]
train() client id: f_00001-8-3 loss: 0.381202  [  128/  265]
train() client id: f_00001-8-4 loss: 0.516461  [  160/  265]
train() client id: f_00001-8-5 loss: 0.336744  [  192/  265]
train() client id: f_00001-8-6 loss: 0.378353  [  224/  265]
train() client id: f_00001-8-7 loss: 0.286980  [  256/  265]
train() client id: f_00001-9-0 loss: 0.278181  [   32/  265]
train() client id: f_00001-9-1 loss: 0.393860  [   64/  265]
train() client id: f_00001-9-2 loss: 0.455243  [   96/  265]
train() client id: f_00001-9-3 loss: 0.447460  [  128/  265]
train() client id: f_00001-9-4 loss: 0.571225  [  160/  265]
train() client id: f_00001-9-5 loss: 0.346457  [  192/  265]
train() client id: f_00001-9-6 loss: 0.311623  [  224/  265]
train() client id: f_00001-9-7 loss: 0.342933  [  256/  265]
train() client id: f_00001-10-0 loss: 0.371542  [   32/  265]
train() client id: f_00001-10-1 loss: 0.400957  [   64/  265]
train() client id: f_00001-10-2 loss: 0.288618  [   96/  265]
train() client id: f_00001-10-3 loss: 0.422637  [  128/  265]
train() client id: f_00001-10-4 loss: 0.342891  [  160/  265]
train() client id: f_00001-10-5 loss: 0.435177  [  192/  265]
train() client id: f_00001-10-6 loss: 0.453995  [  224/  265]
train() client id: f_00001-10-7 loss: 0.296520  [  256/  265]
train() client id: f_00001-11-0 loss: 0.294739  [   32/  265]
train() client id: f_00001-11-1 loss: 0.445477  [   64/  265]
train() client id: f_00001-11-2 loss: 0.433828  [   96/  265]
train() client id: f_00001-11-3 loss: 0.435243  [  128/  265]
train() client id: f_00001-11-4 loss: 0.281367  [  160/  265]
train() client id: f_00001-11-5 loss: 0.539410  [  192/  265]
train() client id: f_00001-11-6 loss: 0.380948  [  224/  265]
train() client id: f_00001-11-7 loss: 0.269502  [  256/  265]
train() client id: f_00001-12-0 loss: 0.356931  [   32/  265]
train() client id: f_00001-12-1 loss: 0.352040  [   64/  265]
train() client id: f_00001-12-2 loss: 0.424200  [   96/  265]
train() client id: f_00001-12-3 loss: 0.457550  [  128/  265]
train() client id: f_00001-12-4 loss: 0.391589  [  160/  265]
train() client id: f_00001-12-5 loss: 0.411809  [  192/  265]
train() client id: f_00001-12-6 loss: 0.437825  [  224/  265]
train() client id: f_00001-12-7 loss: 0.314556  [  256/  265]
train() client id: f_00001-13-0 loss: 0.290107  [   32/  265]
train() client id: f_00001-13-1 loss: 0.473648  [   64/  265]
train() client id: f_00001-13-2 loss: 0.541957  [   96/  265]
train() client id: f_00001-13-3 loss: 0.452132  [  128/  265]
train() client id: f_00001-13-4 loss: 0.389963  [  160/  265]
train() client id: f_00001-13-5 loss: 0.327813  [  192/  265]
train() client id: f_00001-13-6 loss: 0.298875  [  224/  265]
train() client id: f_00001-13-7 loss: 0.378345  [  256/  265]
train() client id: f_00001-14-0 loss: 0.285930  [   32/  265]
train() client id: f_00001-14-1 loss: 0.429872  [   64/  265]
train() client id: f_00001-14-2 loss: 0.335244  [   96/  265]
train() client id: f_00001-14-3 loss: 0.452509  [  128/  265]
train() client id: f_00001-14-4 loss: 0.512696  [  160/  265]
train() client id: f_00001-14-5 loss: 0.401514  [  192/  265]
train() client id: f_00001-14-6 loss: 0.293576  [  224/  265]
train() client id: f_00001-14-7 loss: 0.432841  [  256/  265]
train() client id: f_00001-15-0 loss: 0.311585  [   32/  265]
train() client id: f_00001-15-1 loss: 0.551748  [   64/  265]
train() client id: f_00001-15-2 loss: 0.397701  [   96/  265]
train() client id: f_00001-15-3 loss: 0.309908  [  128/  265]
train() client id: f_00001-15-4 loss: 0.359520  [  160/  265]
train() client id: f_00001-15-5 loss: 0.578969  [  192/  265]
train() client id: f_00001-15-6 loss: 0.356587  [  224/  265]
train() client id: f_00001-15-7 loss: 0.292280  [  256/  265]
train() client id: f_00002-0-0 loss: 1.316751  [   32/  124]
train() client id: f_00002-0-1 loss: 1.130766  [   64/  124]
train() client id: f_00002-0-2 loss: 1.196215  [   96/  124]
train() client id: f_00002-1-0 loss: 1.128457  [   32/  124]
train() client id: f_00002-1-1 loss: 1.044355  [   64/  124]
train() client id: f_00002-1-2 loss: 1.312822  [   96/  124]
train() client id: f_00002-2-0 loss: 1.188536  [   32/  124]
train() client id: f_00002-2-1 loss: 0.980616  [   64/  124]
train() client id: f_00002-2-2 loss: 1.015872  [   96/  124]
train() client id: f_00002-3-0 loss: 1.179511  [   32/  124]
train() client id: f_00002-3-1 loss: 0.939484  [   64/  124]
train() client id: f_00002-3-2 loss: 0.971839  [   96/  124]
train() client id: f_00002-4-0 loss: 0.966791  [   32/  124]
train() client id: f_00002-4-1 loss: 0.959802  [   64/  124]
train() client id: f_00002-4-2 loss: 1.139911  [   96/  124]
train() client id: f_00002-5-0 loss: 1.169886  [   32/  124]
train() client id: f_00002-5-1 loss: 1.071202  [   64/  124]
train() client id: f_00002-5-2 loss: 0.904284  [   96/  124]
train() client id: f_00002-6-0 loss: 0.962935  [   32/  124]
train() client id: f_00002-6-1 loss: 0.947428  [   64/  124]
train() client id: f_00002-6-2 loss: 1.212432  [   96/  124]
train() client id: f_00002-7-0 loss: 1.153197  [   32/  124]
train() client id: f_00002-7-1 loss: 0.838317  [   64/  124]
train() client id: f_00002-7-2 loss: 0.917346  [   96/  124]
train() client id: f_00002-8-0 loss: 0.944652  [   32/  124]
train() client id: f_00002-8-1 loss: 0.893994  [   64/  124]
train() client id: f_00002-8-2 loss: 1.046367  [   96/  124]
train() client id: f_00002-9-0 loss: 0.894308  [   32/  124]
train() client id: f_00002-9-1 loss: 0.951417  [   64/  124]
train() client id: f_00002-9-2 loss: 1.018571  [   96/  124]
train() client id: f_00002-10-0 loss: 0.886932  [   32/  124]
train() client id: f_00002-10-1 loss: 0.872778  [   64/  124]
train() client id: f_00002-10-2 loss: 0.953715  [   96/  124]
train() client id: f_00002-11-0 loss: 0.842274  [   32/  124]
train() client id: f_00002-11-1 loss: 1.059990  [   64/  124]
train() client id: f_00002-11-2 loss: 0.966046  [   96/  124]
train() client id: f_00002-12-0 loss: 1.017610  [   32/  124]
train() client id: f_00002-12-1 loss: 0.702464  [   64/  124]
train() client id: f_00002-12-2 loss: 0.845169  [   96/  124]
train() client id: f_00002-13-0 loss: 0.910190  [   32/  124]
train() client id: f_00002-13-1 loss: 0.958728  [   64/  124]
train() client id: f_00002-13-2 loss: 0.881138  [   96/  124]
train() client id: f_00002-14-0 loss: 0.851279  [   32/  124]
train() client id: f_00002-14-1 loss: 0.854679  [   64/  124]
train() client id: f_00002-14-2 loss: 0.989959  [   96/  124]
train() client id: f_00002-15-0 loss: 0.985190  [   32/  124]
train() client id: f_00002-15-1 loss: 0.810438  [   64/  124]
train() client id: f_00002-15-2 loss: 0.825863  [   96/  124]
train() client id: f_00003-0-0 loss: 0.618642  [   32/   43]
train() client id: f_00003-1-0 loss: 0.497447  [   32/   43]
train() client id: f_00003-2-0 loss: 0.685974  [   32/   43]
train() client id: f_00003-3-0 loss: 0.604239  [   32/   43]
train() client id: f_00003-4-0 loss: 0.647951  [   32/   43]
train() client id: f_00003-5-0 loss: 0.426336  [   32/   43]
train() client id: f_00003-6-0 loss: 0.655013  [   32/   43]
train() client id: f_00003-7-0 loss: 0.625327  [   32/   43]
train() client id: f_00003-8-0 loss: 0.616694  [   32/   43]
train() client id: f_00003-9-0 loss: 0.760775  [   32/   43]
train() client id: f_00003-10-0 loss: 0.701427  [   32/   43]
train() client id: f_00003-11-0 loss: 0.671007  [   32/   43]
train() client id: f_00003-12-0 loss: 0.558070  [   32/   43]
train() client id: f_00003-13-0 loss: 0.750754  [   32/   43]
train() client id: f_00003-14-0 loss: 0.580690  [   32/   43]
train() client id: f_00003-15-0 loss: 0.723705  [   32/   43]
train() client id: f_00004-0-0 loss: 0.794464  [   32/  306]
train() client id: f_00004-0-1 loss: 0.900100  [   64/  306]
train() client id: f_00004-0-2 loss: 0.943826  [   96/  306]
train() client id: f_00004-0-3 loss: 0.759315  [  128/  306]
train() client id: f_00004-0-4 loss: 0.767877  [  160/  306]
train() client id: f_00004-0-5 loss: 0.890615  [  192/  306]
train() client id: f_00004-0-6 loss: 0.937141  [  224/  306]
train() client id: f_00004-0-7 loss: 0.713309  [  256/  306]
train() client id: f_00004-0-8 loss: 0.861904  [  288/  306]
train() client id: f_00004-1-0 loss: 0.756564  [   32/  306]
train() client id: f_00004-1-1 loss: 0.902659  [   64/  306]
train() client id: f_00004-1-2 loss: 0.687066  [   96/  306]
train() client id: f_00004-1-3 loss: 0.863128  [  128/  306]
train() client id: f_00004-1-4 loss: 0.612154  [  160/  306]
train() client id: f_00004-1-5 loss: 0.927782  [  192/  306]
train() client id: f_00004-1-6 loss: 0.931071  [  224/  306]
train() client id: f_00004-1-7 loss: 0.948609  [  256/  306]
train() client id: f_00004-1-8 loss: 0.843506  [  288/  306]
train() client id: f_00004-2-0 loss: 1.075920  [   32/  306]
train() client id: f_00004-2-1 loss: 0.702002  [   64/  306]
train() client id: f_00004-2-2 loss: 0.884407  [   96/  306]
train() client id: f_00004-2-3 loss: 0.782926  [  128/  306]
train() client id: f_00004-2-4 loss: 0.874223  [  160/  306]
train() client id: f_00004-2-5 loss: 0.888732  [  192/  306]
train() client id: f_00004-2-6 loss: 0.663167  [  224/  306]
train() client id: f_00004-2-7 loss: 0.829353  [  256/  306]
train() client id: f_00004-2-8 loss: 0.752137  [  288/  306]
train() client id: f_00004-3-0 loss: 0.924179  [   32/  306]
train() client id: f_00004-3-1 loss: 0.831110  [   64/  306]
train() client id: f_00004-3-2 loss: 0.812756  [   96/  306]
train() client id: f_00004-3-3 loss: 0.815855  [  128/  306]
train() client id: f_00004-3-4 loss: 0.772401  [  160/  306]
train() client id: f_00004-3-5 loss: 0.829984  [  192/  306]
train() client id: f_00004-3-6 loss: 0.692976  [  224/  306]
train() client id: f_00004-3-7 loss: 0.793574  [  256/  306]
train() client id: f_00004-3-8 loss: 0.896728  [  288/  306]
train() client id: f_00004-4-0 loss: 0.808199  [   32/  306]
train() client id: f_00004-4-1 loss: 0.808424  [   64/  306]
train() client id: f_00004-4-2 loss: 0.982715  [   96/  306]
train() client id: f_00004-4-3 loss: 0.661728  [  128/  306]
train() client id: f_00004-4-4 loss: 0.734367  [  160/  306]
train() client id: f_00004-4-5 loss: 0.811113  [  192/  306]
train() client id: f_00004-4-6 loss: 0.868747  [  224/  306]
train() client id: f_00004-4-7 loss: 0.810145  [  256/  306]
train() client id: f_00004-4-8 loss: 0.820224  [  288/  306]
train() client id: f_00004-5-0 loss: 0.860482  [   32/  306]
train() client id: f_00004-5-1 loss: 0.703622  [   64/  306]
train() client id: f_00004-5-2 loss: 0.800794  [   96/  306]
train() client id: f_00004-5-3 loss: 0.743489  [  128/  306]
train() client id: f_00004-5-4 loss: 0.809465  [  160/  306]
train() client id: f_00004-5-5 loss: 0.839968  [  192/  306]
train() client id: f_00004-5-6 loss: 0.935428  [  224/  306]
train() client id: f_00004-5-7 loss: 0.946670  [  256/  306]
train() client id: f_00004-5-8 loss: 0.685640  [  288/  306]
train() client id: f_00004-6-0 loss: 0.755793  [   32/  306]
train() client id: f_00004-6-1 loss: 0.881934  [   64/  306]
train() client id: f_00004-6-2 loss: 0.759996  [   96/  306]
train() client id: f_00004-6-3 loss: 0.737491  [  128/  306]
train() client id: f_00004-6-4 loss: 0.801124  [  160/  306]
train() client id: f_00004-6-5 loss: 0.855272  [  192/  306]
train() client id: f_00004-6-6 loss: 0.806869  [  224/  306]
train() client id: f_00004-6-7 loss: 0.866652  [  256/  306]
train() client id: f_00004-6-8 loss: 0.941682  [  288/  306]
train() client id: f_00004-7-0 loss: 0.780587  [   32/  306]
train() client id: f_00004-7-1 loss: 0.916713  [   64/  306]
train() client id: f_00004-7-2 loss: 0.763580  [   96/  306]
train() client id: f_00004-7-3 loss: 0.844467  [  128/  306]
train() client id: f_00004-7-4 loss: 0.896620  [  160/  306]
train() client id: f_00004-7-5 loss: 0.821945  [  192/  306]
train() client id: f_00004-7-6 loss: 0.751154  [  224/  306]
train() client id: f_00004-7-7 loss: 0.848469  [  256/  306]
train() client id: f_00004-7-8 loss: 0.793855  [  288/  306]
train() client id: f_00004-8-0 loss: 0.729240  [   32/  306]
train() client id: f_00004-8-1 loss: 0.903000  [   64/  306]
train() client id: f_00004-8-2 loss: 0.978635  [   96/  306]
train() client id: f_00004-8-3 loss: 0.785415  [  128/  306]
train() client id: f_00004-8-4 loss: 0.701163  [  160/  306]
train() client id: f_00004-8-5 loss: 0.734425  [  192/  306]
train() client id: f_00004-8-6 loss: 0.755652  [  224/  306]
train() client id: f_00004-8-7 loss: 0.875269  [  256/  306]
train() client id: f_00004-8-8 loss: 0.892758  [  288/  306]
train() client id: f_00004-9-0 loss: 0.879438  [   32/  306]
train() client id: f_00004-9-1 loss: 0.955877  [   64/  306]
train() client id: f_00004-9-2 loss: 0.920295  [   96/  306]
train() client id: f_00004-9-3 loss: 0.665748  [  128/  306]
train() client id: f_00004-9-4 loss: 0.747480  [  160/  306]
train() client id: f_00004-9-5 loss: 0.802903  [  192/  306]
train() client id: f_00004-9-6 loss: 0.780301  [  224/  306]
train() client id: f_00004-9-7 loss: 0.721612  [  256/  306]
train() client id: f_00004-9-8 loss: 0.840420  [  288/  306]
train() client id: f_00004-10-0 loss: 0.871748  [   32/  306]
train() client id: f_00004-10-1 loss: 0.798610  [   64/  306]
train() client id: f_00004-10-2 loss: 0.769676  [   96/  306]
train() client id: f_00004-10-3 loss: 0.702355  [  128/  306]
train() client id: f_00004-10-4 loss: 0.928543  [  160/  306]
train() client id: f_00004-10-5 loss: 0.730405  [  192/  306]
train() client id: f_00004-10-6 loss: 0.818001  [  224/  306]
train() client id: f_00004-10-7 loss: 0.935330  [  256/  306]
train() client id: f_00004-10-8 loss: 0.741225  [  288/  306]
train() client id: f_00004-11-0 loss: 0.958827  [   32/  306]
train() client id: f_00004-11-1 loss: 0.820847  [   64/  306]
train() client id: f_00004-11-2 loss: 0.856995  [   96/  306]
train() client id: f_00004-11-3 loss: 0.668426  [  128/  306]
train() client id: f_00004-11-4 loss: 0.913412  [  160/  306]
train() client id: f_00004-11-5 loss: 0.845627  [  192/  306]
train() client id: f_00004-11-6 loss: 0.738817  [  224/  306]
train() client id: f_00004-11-7 loss: 0.767496  [  256/  306]
train() client id: f_00004-11-8 loss: 0.800842  [  288/  306]
train() client id: f_00004-12-0 loss: 0.832445  [   32/  306]
train() client id: f_00004-12-1 loss: 0.732649  [   64/  306]
train() client id: f_00004-12-2 loss: 0.825599  [   96/  306]
train() client id: f_00004-12-3 loss: 0.855398  [  128/  306]
train() client id: f_00004-12-4 loss: 0.910320  [  160/  306]
train() client id: f_00004-12-5 loss: 0.762772  [  192/  306]
train() client id: f_00004-12-6 loss: 0.822519  [  224/  306]
train() client id: f_00004-12-7 loss: 0.801504  [  256/  306]
train() client id: f_00004-12-8 loss: 0.791861  [  288/  306]
train() client id: f_00004-13-0 loss: 0.869703  [   32/  306]
train() client id: f_00004-13-1 loss: 0.784186  [   64/  306]
train() client id: f_00004-13-2 loss: 0.794377  [   96/  306]
train() client id: f_00004-13-3 loss: 0.902957  [  128/  306]
train() client id: f_00004-13-4 loss: 0.766901  [  160/  306]
train() client id: f_00004-13-5 loss: 0.698979  [  192/  306]
train() client id: f_00004-13-6 loss: 0.878644  [  224/  306]
train() client id: f_00004-13-7 loss: 0.843661  [  256/  306]
train() client id: f_00004-13-8 loss: 0.777300  [  288/  306]
train() client id: f_00004-14-0 loss: 0.829530  [   32/  306]
train() client id: f_00004-14-1 loss: 0.873821  [   64/  306]
train() client id: f_00004-14-2 loss: 0.704680  [   96/  306]
train() client id: f_00004-14-3 loss: 0.883732  [  128/  306]
train() client id: f_00004-14-4 loss: 0.722596  [  160/  306]
train() client id: f_00004-14-5 loss: 0.800562  [  192/  306]
train() client id: f_00004-14-6 loss: 0.881946  [  224/  306]
train() client id: f_00004-14-7 loss: 0.848015  [  256/  306]
train() client id: f_00004-14-8 loss: 0.815810  [  288/  306]
train() client id: f_00004-15-0 loss: 0.813695  [   32/  306]
train() client id: f_00004-15-1 loss: 0.848695  [   64/  306]
train() client id: f_00004-15-2 loss: 0.798140  [   96/  306]
train() client id: f_00004-15-3 loss: 0.706038  [  128/  306]
train() client id: f_00004-15-4 loss: 0.853640  [  160/  306]
train() client id: f_00004-15-5 loss: 0.925859  [  192/  306]
train() client id: f_00004-15-6 loss: 0.741278  [  224/  306]
train() client id: f_00004-15-7 loss: 0.856363  [  256/  306]
train() client id: f_00004-15-8 loss: 0.794162  [  288/  306]
train() client id: f_00005-0-0 loss: 0.642503  [   32/  146]
train() client id: f_00005-0-1 loss: 0.630382  [   64/  146]
train() client id: f_00005-0-2 loss: 0.528191  [   96/  146]
train() client id: f_00005-0-3 loss: 0.503980  [  128/  146]
train() client id: f_00005-1-0 loss: 0.554040  [   32/  146]
train() client id: f_00005-1-1 loss: 0.503688  [   64/  146]
train() client id: f_00005-1-2 loss: 0.542704  [   96/  146]
train() client id: f_00005-1-3 loss: 0.509053  [  128/  146]
train() client id: f_00005-2-0 loss: 0.439852  [   32/  146]
train() client id: f_00005-2-1 loss: 0.594664  [   64/  146]
train() client id: f_00005-2-2 loss: 0.677627  [   96/  146]
train() client id: f_00005-2-3 loss: 0.605059  [  128/  146]
train() client id: f_00005-3-0 loss: 0.436586  [   32/  146]
train() client id: f_00005-3-1 loss: 0.526789  [   64/  146]
train() client id: f_00005-3-2 loss: 0.571079  [   96/  146]
train() client id: f_00005-3-3 loss: 0.759649  [  128/  146]
train() client id: f_00005-4-0 loss: 0.677151  [   32/  146]
train() client id: f_00005-4-1 loss: 0.504531  [   64/  146]
train() client id: f_00005-4-2 loss: 0.486331  [   96/  146]
train() client id: f_00005-4-3 loss: 0.622029  [  128/  146]
train() client id: f_00005-5-0 loss: 0.620886  [   32/  146]
train() client id: f_00005-5-1 loss: 0.648989  [   64/  146]
train() client id: f_00005-5-2 loss: 0.556967  [   96/  146]
train() client id: f_00005-5-3 loss: 0.465677  [  128/  146]
train() client id: f_00005-6-0 loss: 0.635831  [   32/  146]
train() client id: f_00005-6-1 loss: 0.239786  [   64/  146]
train() client id: f_00005-6-2 loss: 0.596128  [   96/  146]
train() client id: f_00005-6-3 loss: 0.697349  [  128/  146]
train() client id: f_00005-7-0 loss: 0.805312  [   32/  146]
train() client id: f_00005-7-1 loss: 0.437155  [   64/  146]
train() client id: f_00005-7-2 loss: 0.516907  [   96/  146]
train() client id: f_00005-7-3 loss: 0.467972  [  128/  146]
train() client id: f_00005-8-0 loss: 0.657627  [   32/  146]
train() client id: f_00005-8-1 loss: 0.537893  [   64/  146]
train() client id: f_00005-8-2 loss: 0.419976  [   96/  146]
train() client id: f_00005-8-3 loss: 0.643340  [  128/  146]
train() client id: f_00005-9-0 loss: 0.603335  [   32/  146]
train() client id: f_00005-9-1 loss: 0.504785  [   64/  146]
train() client id: f_00005-9-2 loss: 0.511593  [   96/  146]
train() client id: f_00005-9-3 loss: 0.585786  [  128/  146]
train() client id: f_00005-10-0 loss: 0.404210  [   32/  146]
train() client id: f_00005-10-1 loss: 0.671183  [   64/  146]
train() client id: f_00005-10-2 loss: 0.489857  [   96/  146]
train() client id: f_00005-10-3 loss: 0.457218  [  128/  146]
train() client id: f_00005-11-0 loss: 0.615842  [   32/  146]
train() client id: f_00005-11-1 loss: 0.322034  [   64/  146]
train() client id: f_00005-11-2 loss: 0.658323  [   96/  146]
train() client id: f_00005-11-3 loss: 0.550721  [  128/  146]
train() client id: f_00005-12-0 loss: 0.589724  [   32/  146]
train() client id: f_00005-12-1 loss: 0.367447  [   64/  146]
train() client id: f_00005-12-2 loss: 0.598542  [   96/  146]
train() client id: f_00005-12-3 loss: 0.532603  [  128/  146]
train() client id: f_00005-13-0 loss: 0.689291  [   32/  146]
train() client id: f_00005-13-1 loss: 0.780913  [   64/  146]
train() client id: f_00005-13-2 loss: 0.449099  [   96/  146]
train() client id: f_00005-13-3 loss: 0.340330  [  128/  146]
train() client id: f_00005-14-0 loss: 0.868563  [   32/  146]
train() client id: f_00005-14-1 loss: 0.343784  [   64/  146]
train() client id: f_00005-14-2 loss: 0.611581  [   96/  146]
train() client id: f_00005-14-3 loss: 0.471277  [  128/  146]
train() client id: f_00005-15-0 loss: 0.548284  [   32/  146]
train() client id: f_00005-15-1 loss: 0.392388  [   64/  146]
train() client id: f_00005-15-2 loss: 0.705174  [   96/  146]
train() client id: f_00005-15-3 loss: 0.456339  [  128/  146]
train() client id: f_00006-0-0 loss: 0.488728  [   32/   54]
train() client id: f_00006-1-0 loss: 0.494437  [   32/   54]
train() client id: f_00006-2-0 loss: 0.502475  [   32/   54]
train() client id: f_00006-3-0 loss: 0.477988  [   32/   54]
train() client id: f_00006-4-0 loss: 0.453996  [   32/   54]
train() client id: f_00006-5-0 loss: 0.484915  [   32/   54]
train() client id: f_00006-6-0 loss: 0.421075  [   32/   54]
train() client id: f_00006-7-0 loss: 0.539722  [   32/   54]
train() client id: f_00006-8-0 loss: 0.542604  [   32/   54]
train() client id: f_00006-9-0 loss: 0.516754  [   32/   54]
train() client id: f_00006-10-0 loss: 0.517522  [   32/   54]
train() client id: f_00006-11-0 loss: 0.544103  [   32/   54]
train() client id: f_00006-12-0 loss: 0.495480  [   32/   54]
train() client id: f_00006-13-0 loss: 0.464076  [   32/   54]
train() client id: f_00006-14-0 loss: 0.432371  [   32/   54]
train() client id: f_00006-15-0 loss: 0.496169  [   32/   54]
train() client id: f_00007-0-0 loss: 0.918791  [   32/  179]
train() client id: f_00007-0-1 loss: 0.561562  [   64/  179]
train() client id: f_00007-0-2 loss: 0.555402  [   96/  179]
train() client id: f_00007-0-3 loss: 0.589336  [  128/  179]
train() client id: f_00007-0-4 loss: 0.518636  [  160/  179]
train() client id: f_00007-1-0 loss: 0.711696  [   32/  179]
train() client id: f_00007-1-1 loss: 0.587482  [   64/  179]
train() client id: f_00007-1-2 loss: 0.626667  [   96/  179]
train() client id: f_00007-1-3 loss: 0.654146  [  128/  179]
train() client id: f_00007-1-4 loss: 0.650320  [  160/  179]
train() client id: f_00007-2-0 loss: 0.629564  [   32/  179]
train() client id: f_00007-2-1 loss: 0.567586  [   64/  179]
train() client id: f_00007-2-2 loss: 0.681496  [   96/  179]
train() client id: f_00007-2-3 loss: 0.471254  [  128/  179]
train() client id: f_00007-2-4 loss: 0.746467  [  160/  179]
train() client id: f_00007-3-0 loss: 0.511087  [   32/  179]
train() client id: f_00007-3-1 loss: 0.633467  [   64/  179]
train() client id: f_00007-3-2 loss: 0.573801  [   96/  179]
train() client id: f_00007-3-3 loss: 0.797886  [  128/  179]
train() client id: f_00007-3-4 loss: 0.447060  [  160/  179]
train() client id: f_00007-4-0 loss: 0.660298  [   32/  179]
train() client id: f_00007-4-1 loss: 0.489893  [   64/  179]
train() client id: f_00007-4-2 loss: 0.577106  [   96/  179]
train() client id: f_00007-4-3 loss: 0.575624  [  128/  179]
train() client id: f_00007-4-4 loss: 0.518326  [  160/  179]
train() client id: f_00007-5-0 loss: 0.532410  [   32/  179]
train() client id: f_00007-5-1 loss: 0.533419  [   64/  179]
train() client id: f_00007-5-2 loss: 0.621100  [   96/  179]
train() client id: f_00007-5-3 loss: 0.571761  [  128/  179]
train() client id: f_00007-5-4 loss: 0.616437  [  160/  179]
train() client id: f_00007-6-0 loss: 0.518550  [   32/  179]
train() client id: f_00007-6-1 loss: 0.557742  [   64/  179]
train() client id: f_00007-6-2 loss: 0.413609  [   96/  179]
train() client id: f_00007-6-3 loss: 0.551006  [  128/  179]
train() client id: f_00007-6-4 loss: 0.800294  [  160/  179]
train() client id: f_00007-7-0 loss: 0.513276  [   32/  179]
train() client id: f_00007-7-1 loss: 0.559484  [   64/  179]
train() client id: f_00007-7-2 loss: 0.793201  [   96/  179]
train() client id: f_00007-7-3 loss: 0.470595  [  128/  179]
train() client id: f_00007-7-4 loss: 0.614373  [  160/  179]
train() client id: f_00007-8-0 loss: 0.861958  [   32/  179]
train() client id: f_00007-8-1 loss: 0.655787  [   64/  179]
train() client id: f_00007-8-2 loss: 0.406115  [   96/  179]
train() client id: f_00007-8-3 loss: 0.435679  [  128/  179]
train() client id: f_00007-8-4 loss: 0.540725  [  160/  179]
train() client id: f_00007-9-0 loss: 0.620057  [   32/  179]
train() client id: f_00007-9-1 loss: 0.518536  [   64/  179]
train() client id: f_00007-9-2 loss: 0.618385  [   96/  179]
train() client id: f_00007-9-3 loss: 0.607117  [  128/  179]
train() client id: f_00007-9-4 loss: 0.466374  [  160/  179]
train() client id: f_00007-10-0 loss: 0.518595  [   32/  179]
train() client id: f_00007-10-1 loss: 0.586648  [   64/  179]
train() client id: f_00007-10-2 loss: 0.618960  [   96/  179]
train() client id: f_00007-10-3 loss: 0.625665  [  128/  179]
train() client id: f_00007-10-4 loss: 0.522079  [  160/  179]
train() client id: f_00007-11-0 loss: 0.588344  [   32/  179]
train() client id: f_00007-11-1 loss: 0.595705  [   64/  179]
train() client id: f_00007-11-2 loss: 0.582212  [   96/  179]
train() client id: f_00007-11-3 loss: 0.675497  [  128/  179]
train() client id: f_00007-11-4 loss: 0.402275  [  160/  179]
train() client id: f_00007-12-0 loss: 0.635194  [   32/  179]
train() client id: f_00007-12-1 loss: 0.410885  [   64/  179]
train() client id: f_00007-12-2 loss: 0.591134  [   96/  179]
train() client id: f_00007-12-3 loss: 0.607065  [  128/  179]
train() client id: f_00007-12-4 loss: 0.623461  [  160/  179]
train() client id: f_00007-13-0 loss: 0.451886  [   32/  179]
train() client id: f_00007-13-1 loss: 0.786212  [   64/  179]
train() client id: f_00007-13-2 loss: 0.604442  [   96/  179]
train() client id: f_00007-13-3 loss: 0.395314  [  128/  179]
train() client id: f_00007-13-4 loss: 0.580306  [  160/  179]
train() client id: f_00007-14-0 loss: 0.657384  [   32/  179]
train() client id: f_00007-14-1 loss: 0.571560  [   64/  179]
train() client id: f_00007-14-2 loss: 0.825555  [   96/  179]
train() client id: f_00007-14-3 loss: 0.405593  [  128/  179]
train() client id: f_00007-14-4 loss: 0.395705  [  160/  179]
train() client id: f_00007-15-0 loss: 0.762384  [   32/  179]
train() client id: f_00007-15-1 loss: 0.648488  [   64/  179]
train() client id: f_00007-15-2 loss: 0.401721  [   96/  179]
train() client id: f_00007-15-3 loss: 0.462605  [  128/  179]
train() client id: f_00007-15-4 loss: 0.484341  [  160/  179]
train() client id: f_00008-0-0 loss: 0.726840  [   32/  130]
train() client id: f_00008-0-1 loss: 0.717636  [   64/  130]
train() client id: f_00008-0-2 loss: 0.726429  [   96/  130]
train() client id: f_00008-0-3 loss: 0.703836  [  128/  130]
train() client id: f_00008-1-0 loss: 0.678943  [   32/  130]
train() client id: f_00008-1-1 loss: 0.666259  [   64/  130]
train() client id: f_00008-1-2 loss: 0.739054  [   96/  130]
train() client id: f_00008-1-3 loss: 0.796237  [  128/  130]
train() client id: f_00008-2-0 loss: 0.664627  [   32/  130]
train() client id: f_00008-2-1 loss: 0.650682  [   64/  130]
train() client id: f_00008-2-2 loss: 0.814757  [   96/  130]
train() client id: f_00008-2-3 loss: 0.742574  [  128/  130]
train() client id: f_00008-3-0 loss: 0.653532  [   32/  130]
train() client id: f_00008-3-1 loss: 0.630146  [   64/  130]
train() client id: f_00008-3-2 loss: 0.827400  [   96/  130]
train() client id: f_00008-3-3 loss: 0.729609  [  128/  130]
train() client id: f_00008-4-0 loss: 0.753661  [   32/  130]
train() client id: f_00008-4-1 loss: 0.676375  [   64/  130]
train() client id: f_00008-4-2 loss: 0.721374  [   96/  130]
train() client id: f_00008-4-3 loss: 0.709152  [  128/  130]
train() client id: f_00008-5-0 loss: 0.568047  [   32/  130]
train() client id: f_00008-5-1 loss: 0.773513  [   64/  130]
train() client id: f_00008-5-2 loss: 0.700899  [   96/  130]
train() client id: f_00008-5-3 loss: 0.800664  [  128/  130]
train() client id: f_00008-6-0 loss: 0.591921  [   32/  130]
train() client id: f_00008-6-1 loss: 0.724539  [   64/  130]
train() client id: f_00008-6-2 loss: 0.772020  [   96/  130]
train() client id: f_00008-6-3 loss: 0.740260  [  128/  130]
train() client id: f_00008-7-0 loss: 0.732947  [   32/  130]
train() client id: f_00008-7-1 loss: 0.767881  [   64/  130]
train() client id: f_00008-7-2 loss: 0.642448  [   96/  130]
train() client id: f_00008-7-3 loss: 0.699179  [  128/  130]
train() client id: f_00008-8-0 loss: 0.667662  [   32/  130]
train() client id: f_00008-8-1 loss: 0.683391  [   64/  130]
train() client id: f_00008-8-2 loss: 0.658849  [   96/  130]
train() client id: f_00008-8-3 loss: 0.836257  [  128/  130]
train() client id: f_00008-9-0 loss: 0.666905  [   32/  130]
train() client id: f_00008-9-1 loss: 0.731901  [   64/  130]
train() client id: f_00008-9-2 loss: 0.696420  [   96/  130]
train() client id: f_00008-9-3 loss: 0.753342  [  128/  130]
train() client id: f_00008-10-0 loss: 0.827902  [   32/  130]
train() client id: f_00008-10-1 loss: 0.718928  [   64/  130]
train() client id: f_00008-10-2 loss: 0.608209  [   96/  130]
train() client id: f_00008-10-3 loss: 0.652829  [  128/  130]
train() client id: f_00008-11-0 loss: 0.658307  [   32/  130]
train() client id: f_00008-11-1 loss: 0.714651  [   64/  130]
train() client id: f_00008-11-2 loss: 0.724940  [   96/  130]
train() client id: f_00008-11-3 loss: 0.704689  [  128/  130]
train() client id: f_00008-12-0 loss: 0.695162  [   32/  130]
train() client id: f_00008-12-1 loss: 0.695082  [   64/  130]
train() client id: f_00008-12-2 loss: 0.741057  [   96/  130]
train() client id: f_00008-12-3 loss: 0.710514  [  128/  130]
train() client id: f_00008-13-0 loss: 0.663788  [   32/  130]
train() client id: f_00008-13-1 loss: 0.823199  [   64/  130]
train() client id: f_00008-13-2 loss: 0.706673  [   96/  130]
train() client id: f_00008-13-3 loss: 0.649244  [  128/  130]
train() client id: f_00008-14-0 loss: 0.715436  [   32/  130]
train() client id: f_00008-14-1 loss: 0.641919  [   64/  130]
train() client id: f_00008-14-2 loss: 0.678999  [   96/  130]
train() client id: f_00008-14-3 loss: 0.802371  [  128/  130]
train() client id: f_00008-15-0 loss: 0.724541  [   32/  130]
train() client id: f_00008-15-1 loss: 0.678933  [   64/  130]
train() client id: f_00008-15-2 loss: 0.778779  [   96/  130]
train() client id: f_00008-15-3 loss: 0.647180  [  128/  130]
train() client id: f_00009-0-0 loss: 1.196273  [   32/  118]
train() client id: f_00009-0-1 loss: 1.043045  [   64/  118]
train() client id: f_00009-0-2 loss: 1.184062  [   96/  118]
train() client id: f_00009-1-0 loss: 1.024022  [   32/  118]
train() client id: f_00009-1-1 loss: 1.163961  [   64/  118]
train() client id: f_00009-1-2 loss: 1.153525  [   96/  118]
train() client id: f_00009-2-0 loss: 1.060333  [   32/  118]
train() client id: f_00009-2-1 loss: 1.035449  [   64/  118]
train() client id: f_00009-2-2 loss: 0.979574  [   96/  118]
train() client id: f_00009-3-0 loss: 0.971676  [   32/  118]
train() client id: f_00009-3-1 loss: 0.942900  [   64/  118]
train() client id: f_00009-3-2 loss: 1.044374  [   96/  118]
train() client id: f_00009-4-0 loss: 0.991973  [   32/  118]
train() client id: f_00009-4-1 loss: 1.008021  [   64/  118]
train() client id: f_00009-4-2 loss: 0.990811  [   96/  118]
train() client id: f_00009-5-0 loss: 1.065594  [   32/  118]
train() client id: f_00009-5-1 loss: 0.959113  [   64/  118]
train() client id: f_00009-5-2 loss: 0.893793  [   96/  118]
train() client id: f_00009-6-0 loss: 0.971007  [   32/  118]
train() client id: f_00009-6-1 loss: 0.914962  [   64/  118]
train() client id: f_00009-6-2 loss: 0.805522  [   96/  118]
train() client id: f_00009-7-0 loss: 1.042287  [   32/  118]
train() client id: f_00009-7-1 loss: 0.834298  [   64/  118]
train() client id: f_00009-7-2 loss: 0.771155  [   96/  118]
train() client id: f_00009-8-0 loss: 0.827193  [   32/  118]
train() client id: f_00009-8-1 loss: 0.824356  [   64/  118]
train() client id: f_00009-8-2 loss: 0.827914  [   96/  118]
train() client id: f_00009-9-0 loss: 0.877867  [   32/  118]
train() client id: f_00009-9-1 loss: 0.886040  [   64/  118]
train() client id: f_00009-9-2 loss: 0.839687  [   96/  118]
train() client id: f_00009-10-0 loss: 0.743391  [   32/  118]
train() client id: f_00009-10-1 loss: 0.884896  [   64/  118]
train() client id: f_00009-10-2 loss: 0.860138  [   96/  118]
train() client id: f_00009-11-0 loss: 0.758296  [   32/  118]
train() client id: f_00009-11-1 loss: 0.935655  [   64/  118]
train() client id: f_00009-11-2 loss: 0.761117  [   96/  118]
train() client id: f_00009-12-0 loss: 0.652884  [   32/  118]
train() client id: f_00009-12-1 loss: 0.984928  [   64/  118]
train() client id: f_00009-12-2 loss: 0.883455  [   96/  118]
train() client id: f_00009-13-0 loss: 0.863606  [   32/  118]
train() client id: f_00009-13-1 loss: 0.984061  [   64/  118]
train() client id: f_00009-13-2 loss: 0.692539  [   96/  118]
train() client id: f_00009-14-0 loss: 0.751516  [   32/  118]
train() client id: f_00009-14-1 loss: 0.946565  [   64/  118]
train() client id: f_00009-14-2 loss: 0.789791  [   96/  118]
train() client id: f_00009-15-0 loss: 1.029144  [   32/  118]
train() client id: f_00009-15-1 loss: 0.746350  [   64/  118]
train() client id: f_00009-15-2 loss: 0.658389  [   96/  118]
At round 44 accuracy: 0.6419098143236074
At round 44 training accuracy: 0.5955734406438632
At round 44 training loss: 0.8301083998508516
update_location
xs = 8.927491 341.223621 5.882650 0.934260 -257.581990 -105.230757 -65.849135 -5.143845 -280.120581 20.134486 
ys = -332.390647 7.291448 230.684448 -52.290817 -9.642386 0.794442 -1.381692 226.628436 25.881276 -767.232496 
xs mean: -33.68237997052122
ys mean: -67.16579882624052
dists_uav = 347.222180 355.649722 251.495368 112.850354 276.480482 145.169361 119.741462 247.763813 298.558839 773.983915 
uav_gains = -119.018466 -119.473352 -111.423711 -101.312663 -113.660535 -104.049819 -101.956322 -111.107071 -115.609768 -129.182690 
uav_gains_db_mean: -112.67943968003651
dists_bs = 539.698611 542.782632 189.259850 287.426477 202.268623 187.657463 207.423270 177.529178 182.446514 962.226348 
bs_gains = -116.067296 -116.136586 -103.324697 -108.405809 -104.133058 -103.221303 -104.439068 -102.546612 -102.878855 -123.098822 
bs_gains_db_mean: -108.425210452413
Round 45
-------------------------------
ene_coms = [0.02453695 0.02597127 0.00730902 0.00666533 0.00760327 0.00757191
 0.00685876 0.00704613 0.00715608 0.04354828]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.21741512 10.6425302   4.91743328  1.75851327 12.00039534  5.77654724
  2.18886976  7.05335155  5.14883995  5.15143221]
obj_prev = 59.855327939831824
eta_min = 7.038490346919195e-19	eta_max = 0.7973091527735665
af = 12.467458429545857	bf = 1.8420068056429986	zeta = 13.714204272500444	eta = 0.9090909090909091
af = 12.467458429545857	bf = 1.8420068056429986	zeta = 33.33310761292556	eta = 0.37402628564734713
af = 12.467458429545857	bf = 1.8420068056429986	zeta = 22.52949152833088	eta = 0.5533839240831514
af = 12.467458429545857	bf = 1.8420068056429986	zeta = 20.64198562716759	eta = 0.6039854234341211
af = 12.467458429545857	bf = 1.8420068056429986	zeta = 20.524912064302303	eta = 0.6074305405298047
af = 12.467458429545857	bf = 1.8420068056429986	zeta = 20.524399330265712	eta = 0.6074457151669759
af = 12.467458429545857	bf = 1.8420068056429986	zeta = 20.52439932035563	eta = 0.6074457154602774
eta = 0.6074457154602774
ene_coms = [0.02453695 0.02597127 0.00730902 0.00666533 0.00760327 0.00757191
 0.00685876 0.00704613 0.00715608 0.04354828]
ene_comp = [0.0411344  0.08651283 0.04048148 0.01403793 0.09989784 0.04766367
 0.01762903 0.05843697 0.04244026 0.0385227 ]
ene_total = [2.13599618 3.65860614 1.55441165 0.67338468 3.49653151 1.79656705
 0.79647871 2.12987317 1.61314761 2.66940262]
ti_comp = [0.60651158 0.59216836 0.77879089 0.7852278  0.77584837 0.77616201
 0.78329344 0.78141977 0.78032028 0.41639831]
ti_coms = [0.2453695  0.25971272 0.0730902  0.06665328 0.07603272 0.07571907
 0.06858764 0.07046131 0.0715608  0.43548277]
t_total = [27.70789146 27.70789146 27.70789146 27.70789146 27.70789146 27.70789146
 27.70789146 27.70789146 27.70789146 27.70789146]
ene_coms = [0.02453695 0.02597127 0.00730902 0.00666533 0.00760327 0.00757191
 0.00685876 0.00704613 0.00715608 0.04354828]
ene_comp = [1.18254433e-05 1.15406750e-04 6.83608297e-06 2.80412922e-07
 1.03512922e-04 1.12340873e-05 5.58105487e-07 2.04255752e-05
 7.84636046e-06 2.06068729e-05]
ene_total = [0.79846216 0.84848329 0.23795215 0.21680251 0.25066732 0.24664576
 0.22310316 0.22984357 0.23301056 1.41710144]
optimize_network iter = 0 obj = 4.702071928906088
eta = 0.6074457154602774
freqs = [33910649.64851204 73047496.21640308 25989952.56293675  8938763.57166855
 64379742.99946091 30704719.64923211 11253145.47129976 37391535.40603455
 27194126.81693179 46257026.24683371]
eta_min = 0.6074457154602785	eta_max = 0.6074457154602757
af = 0.007646406977961308	bf = 1.8420068056429986	zeta = 0.00841104767575744	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02453695 0.02597127 0.00730902 0.00666533 0.00760327 0.00757191
 0.00685876 0.00704613 0.00715608 0.04354828]
ene_comp = [2.26098730e-06 2.20654049e-05 1.30703740e-06 5.36140622e-08
 1.97913428e-05 2.14792190e-06 1.06708001e-07 3.90530526e-06
 1.50019926e-06 3.93996881e-06]
ene_total = [3.44649386 3.6507236  1.02672395 0.93614245 1.07064728 1.06376422
 0.96331772 0.99016663 1.00527094 6.11684082]
ti_comp = [0.60651158 0.59216836 0.77879089 0.7852278  0.77584837 0.77616201
 0.78329344 0.78141977 0.78032028 0.41639831]
ti_coms = [0.2453695  0.25971272 0.0730902  0.06665328 0.07603272 0.07571907
 0.06858764 0.07046131 0.0715608  0.43548277]
t_total = [27.70789146 27.70789146 27.70789146 27.70789146 27.70789146 27.70789146
 27.70789146 27.70789146 27.70789146 27.70789146]
ene_coms = [0.02453695 0.02597127 0.00730902 0.00666533 0.00760327 0.00757191
 0.00685876 0.00704613 0.00715608 0.04354828]
ene_comp = [1.18254433e-05 1.15406750e-04 6.83608297e-06 2.80412922e-07
 1.03512922e-04 1.12340873e-05 5.58105487e-07 2.04255752e-05
 7.84636046e-06 2.06068729e-05]
ene_total = [0.79846216 0.84848329 0.23795215 0.21680251 0.25066732 0.24664576
 0.22310316 0.22984357 0.23301056 1.41710144]
optimize_network iter = 1 obj = 4.702071928906069
eta = 0.6074457154602757
freqs = [33910649.64851204 73047496.21640305 25989952.56293678  8938763.57166856
 64379742.99946097 30704719.64923215 11253145.47129977 37391535.40603459
 27194126.81693182 46257026.24683358]
Done!
ene_coms = [0.02453695 0.02597127 0.00730902 0.00666533 0.00760327 0.00757191
 0.00685876 0.00704613 0.00715608 0.04354828]
ene_comp = [1.15913162e-05 1.13121858e-04 6.70073813e-06 2.74861140e-07
 1.01463512e-04 1.10116682e-05 5.47055783e-07 2.00211775e-05
 7.69101355e-06 2.01988858e-05]
ene_total = [0.02454854 0.02608439 0.00731572 0.0066656  0.00770474 0.00758292
 0.00685931 0.00706615 0.00716377 0.04356848]
At round 45 energy consumption: 0.14455962334672004
At round 45 eta: 0.6074457154602757
At round 45 a_n: 12.768039742635098
At round 45 local rounds: 16.323175950430894
At round 45 global rounds: 32.525539130481825
gradient difference: 0.3867199718952179
train() client id: f_00000-0-0 loss: 1.389553  [   32/  126]
train() client id: f_00000-0-1 loss: 1.197046  [   64/  126]
train() client id: f_00000-0-2 loss: 1.200770  [   96/  126]
train() client id: f_00000-1-0 loss: 1.228220  [   32/  126]
train() client id: f_00000-1-1 loss: 1.312576  [   64/  126]
train() client id: f_00000-1-2 loss: 1.099864  [   96/  126]
train() client id: f_00000-2-0 loss: 1.031374  [   32/  126]
train() client id: f_00000-2-1 loss: 1.215977  [   64/  126]
train() client id: f_00000-2-2 loss: 1.056456  [   96/  126]
train() client id: f_00000-3-0 loss: 1.116500  [   32/  126]
train() client id: f_00000-3-1 loss: 0.940569  [   64/  126]
train() client id: f_00000-3-2 loss: 1.020583  [   96/  126]
train() client id: f_00000-4-0 loss: 0.939746  [   32/  126]
train() client id: f_00000-4-1 loss: 0.852759  [   64/  126]
train() client id: f_00000-4-2 loss: 0.906077  [   96/  126]
train() client id: f_00000-5-0 loss: 0.969792  [   32/  126]
train() client id: f_00000-5-1 loss: 0.900863  [   64/  126]
train() client id: f_00000-5-2 loss: 0.932137  [   96/  126]
train() client id: f_00000-6-0 loss: 0.847157  [   32/  126]
train() client id: f_00000-6-1 loss: 1.000834  [   64/  126]
train() client id: f_00000-6-2 loss: 0.834943  [   96/  126]
train() client id: f_00000-7-0 loss: 0.919059  [   32/  126]
train() client id: f_00000-7-1 loss: 0.784865  [   64/  126]
train() client id: f_00000-7-2 loss: 0.943319  [   96/  126]
train() client id: f_00000-8-0 loss: 0.818208  [   32/  126]
train() client id: f_00000-8-1 loss: 0.920539  [   64/  126]
train() client id: f_00000-8-2 loss: 0.811518  [   96/  126]
train() client id: f_00000-9-0 loss: 0.736591  [   32/  126]
train() client id: f_00000-9-1 loss: 0.933077  [   64/  126]
train() client id: f_00000-9-2 loss: 0.825948  [   96/  126]
train() client id: f_00000-10-0 loss: 0.903423  [   32/  126]
train() client id: f_00000-10-1 loss: 0.807328  [   64/  126]
train() client id: f_00000-10-2 loss: 0.787582  [   96/  126]
train() client id: f_00000-11-0 loss: 0.799518  [   32/  126]
train() client id: f_00000-11-1 loss: 0.747836  [   64/  126]
train() client id: f_00000-11-2 loss: 0.724846  [   96/  126]
train() client id: f_00000-12-0 loss: 0.824522  [   32/  126]
train() client id: f_00000-12-1 loss: 0.759168  [   64/  126]
train() client id: f_00000-12-2 loss: 0.714506  [   96/  126]
train() client id: f_00000-13-0 loss: 0.745227  [   32/  126]
train() client id: f_00000-13-1 loss: 0.724278  [   64/  126]
train() client id: f_00000-13-2 loss: 0.793107  [   96/  126]
train() client id: f_00000-14-0 loss: 0.719182  [   32/  126]
train() client id: f_00000-14-1 loss: 0.804962  [   64/  126]
train() client id: f_00000-14-2 loss: 0.889070  [   96/  126]
train() client id: f_00000-15-0 loss: 0.714653  [   32/  126]
train() client id: f_00000-15-1 loss: 0.921601  [   64/  126]
train() client id: f_00000-15-2 loss: 0.809826  [   96/  126]
train() client id: f_00001-0-0 loss: 0.604177  [   32/  265]
train() client id: f_00001-0-1 loss: 0.403757  [   64/  265]
train() client id: f_00001-0-2 loss: 0.542448  [   96/  265]
train() client id: f_00001-0-3 loss: 0.416192  [  128/  265]
train() client id: f_00001-0-4 loss: 0.486907  [  160/  265]
train() client id: f_00001-0-5 loss: 0.396745  [  192/  265]
train() client id: f_00001-0-6 loss: 0.428916  [  224/  265]
train() client id: f_00001-0-7 loss: 0.542759  [  256/  265]
train() client id: f_00001-1-0 loss: 0.434051  [   32/  265]
train() client id: f_00001-1-1 loss: 0.420475  [   64/  265]
train() client id: f_00001-1-2 loss: 0.515613  [   96/  265]
train() client id: f_00001-1-3 loss: 0.486475  [  128/  265]
train() client id: f_00001-1-4 loss: 0.605543  [  160/  265]
train() client id: f_00001-1-5 loss: 0.594050  [  192/  265]
train() client id: f_00001-1-6 loss: 0.393417  [  224/  265]
train() client id: f_00001-1-7 loss: 0.416028  [  256/  265]
train() client id: f_00001-2-0 loss: 0.609635  [   32/  265]
train() client id: f_00001-2-1 loss: 0.483975  [   64/  265]
train() client id: f_00001-2-2 loss: 0.556931  [   96/  265]
train() client id: f_00001-2-3 loss: 0.386905  [  128/  265]
train() client id: f_00001-2-4 loss: 0.368320  [  160/  265]
train() client id: f_00001-2-5 loss: 0.516614  [  192/  265]
train() client id: f_00001-2-6 loss: 0.418464  [  224/  265]
train() client id: f_00001-2-7 loss: 0.456651  [  256/  265]
train() client id: f_00001-3-0 loss: 0.412557  [   32/  265]
train() client id: f_00001-3-1 loss: 0.443910  [   64/  265]
train() client id: f_00001-3-2 loss: 0.403148  [   96/  265]
train() client id: f_00001-3-3 loss: 0.386748  [  128/  265]
train() client id: f_00001-3-4 loss: 0.484919  [  160/  265]
train() client id: f_00001-3-5 loss: 0.704495  [  192/  265]
train() client id: f_00001-3-6 loss: 0.377576  [  224/  265]
train() client id: f_00001-3-7 loss: 0.475945  [  256/  265]
train() client id: f_00001-4-0 loss: 0.531856  [   32/  265]
train() client id: f_00001-4-1 loss: 0.438823  [   64/  265]
train() client id: f_00001-4-2 loss: 0.361024  [   96/  265]
train() client id: f_00001-4-3 loss: 0.489627  [  128/  265]
train() client id: f_00001-4-4 loss: 0.579262  [  160/  265]
train() client id: f_00001-4-5 loss: 0.453679  [  192/  265]
train() client id: f_00001-4-6 loss: 0.472321  [  224/  265]
train() client id: f_00001-4-7 loss: 0.390587  [  256/  265]
train() client id: f_00001-5-0 loss: 0.422578  [   32/  265]
train() client id: f_00001-5-1 loss: 0.516366  [   64/  265]
train() client id: f_00001-5-2 loss: 0.599596  [   96/  265]
train() client id: f_00001-5-3 loss: 0.458523  [  128/  265]
train() client id: f_00001-5-4 loss: 0.446160  [  160/  265]
train() client id: f_00001-5-5 loss: 0.417306  [  192/  265]
train() client id: f_00001-5-6 loss: 0.451676  [  224/  265]
train() client id: f_00001-5-7 loss: 0.376832  [  256/  265]
train() client id: f_00001-6-0 loss: 0.528578  [   32/  265]
train() client id: f_00001-6-1 loss: 0.439975  [   64/  265]
train() client id: f_00001-6-2 loss: 0.469888  [   96/  265]
train() client id: f_00001-6-3 loss: 0.375028  [  128/  265]
train() client id: f_00001-6-4 loss: 0.518683  [  160/  265]
train() client id: f_00001-6-5 loss: 0.356554  [  192/  265]
train() client id: f_00001-6-6 loss: 0.515609  [  224/  265]
train() client id: f_00001-6-7 loss: 0.458487  [  256/  265]
train() client id: f_00001-7-0 loss: 0.459229  [   32/  265]
train() client id: f_00001-7-1 loss: 0.448897  [   64/  265]
train() client id: f_00001-7-2 loss: 0.624750  [   96/  265]
train() client id: f_00001-7-3 loss: 0.370286  [  128/  265]
train() client id: f_00001-7-4 loss: 0.401561  [  160/  265]
train() client id: f_00001-7-5 loss: 0.373751  [  192/  265]
train() client id: f_00001-7-6 loss: 0.651040  [  224/  265]
train() client id: f_00001-7-7 loss: 0.343552  [  256/  265]
train() client id: f_00001-8-0 loss: 0.466794  [   32/  265]
train() client id: f_00001-8-1 loss: 0.403678  [   64/  265]
train() client id: f_00001-8-2 loss: 0.506476  [   96/  265]
train() client id: f_00001-8-3 loss: 0.483213  [  128/  265]
train() client id: f_00001-8-4 loss: 0.359601  [  160/  265]
train() client id: f_00001-8-5 loss: 0.489046  [  192/  265]
train() client id: f_00001-8-6 loss: 0.571985  [  224/  265]
train() client id: f_00001-8-7 loss: 0.368286  [  256/  265]
train() client id: f_00001-9-0 loss: 0.555813  [   32/  265]
train() client id: f_00001-9-1 loss: 0.457638  [   64/  265]
train() client id: f_00001-9-2 loss: 0.531844  [   96/  265]
train() client id: f_00001-9-3 loss: 0.422997  [  128/  265]
train() client id: f_00001-9-4 loss: 0.366677  [  160/  265]
train() client id: f_00001-9-5 loss: 0.418710  [  192/  265]
train() client id: f_00001-9-6 loss: 0.491943  [  224/  265]
train() client id: f_00001-9-7 loss: 0.393654  [  256/  265]
train() client id: f_00001-10-0 loss: 0.503075  [   32/  265]
train() client id: f_00001-10-1 loss: 0.410275  [   64/  265]
train() client id: f_00001-10-2 loss: 0.357848  [   96/  265]
train() client id: f_00001-10-3 loss: 0.610880  [  128/  265]
train() client id: f_00001-10-4 loss: 0.375701  [  160/  265]
train() client id: f_00001-10-5 loss: 0.532027  [  192/  265]
train() client id: f_00001-10-6 loss: 0.363758  [  224/  265]
train() client id: f_00001-10-7 loss: 0.435621  [  256/  265]
train() client id: f_00001-11-0 loss: 0.525670  [   32/  265]
train() client id: f_00001-11-1 loss: 0.504923  [   64/  265]
train() client id: f_00001-11-2 loss: 0.449126  [   96/  265]
train() client id: f_00001-11-3 loss: 0.466642  [  128/  265]
train() client id: f_00001-11-4 loss: 0.372838  [  160/  265]
train() client id: f_00001-11-5 loss: 0.375884  [  192/  265]
train() client id: f_00001-11-6 loss: 0.442590  [  224/  265]
train() client id: f_00001-11-7 loss: 0.490529  [  256/  265]
train() client id: f_00001-12-0 loss: 0.477781  [   32/  265]
train() client id: f_00001-12-1 loss: 0.460644  [   64/  265]
train() client id: f_00001-12-2 loss: 0.485597  [   96/  265]
train() client id: f_00001-12-3 loss: 0.429687  [  128/  265]
train() client id: f_00001-12-4 loss: 0.356916  [  160/  265]
train() client id: f_00001-12-5 loss: 0.520098  [  192/  265]
train() client id: f_00001-12-6 loss: 0.440597  [  224/  265]
train() client id: f_00001-12-7 loss: 0.454496  [  256/  265]
train() client id: f_00001-13-0 loss: 0.401878  [   32/  265]
train() client id: f_00001-13-1 loss: 0.467960  [   64/  265]
train() client id: f_00001-13-2 loss: 0.444422  [   96/  265]
train() client id: f_00001-13-3 loss: 0.622877  [  128/  265]
train() client id: f_00001-13-4 loss: 0.363225  [  160/  265]
train() client id: f_00001-13-5 loss: 0.456666  [  192/  265]
train() client id: f_00001-13-6 loss: 0.407112  [  224/  265]
train() client id: f_00001-13-7 loss: 0.368733  [  256/  265]
train() client id: f_00001-14-0 loss: 0.515714  [   32/  265]
train() client id: f_00001-14-1 loss: 0.529760  [   64/  265]
train() client id: f_00001-14-2 loss: 0.342916  [   96/  265]
train() client id: f_00001-14-3 loss: 0.357607  [  128/  265]
train() client id: f_00001-14-4 loss: 0.447219  [  160/  265]
train() client id: f_00001-14-5 loss: 0.505785  [  192/  265]
train() client id: f_00001-14-6 loss: 0.485900  [  224/  265]
train() client id: f_00001-14-7 loss: 0.403160  [  256/  265]
train() client id: f_00001-15-0 loss: 0.431558  [   32/  265]
train() client id: f_00001-15-1 loss: 0.420941  [   64/  265]
train() client id: f_00001-15-2 loss: 0.367319  [   96/  265]
train() client id: f_00001-15-3 loss: 0.408900  [  128/  265]
train() client id: f_00001-15-4 loss: 0.357005  [  160/  265]
train() client id: f_00001-15-5 loss: 0.666168  [  192/  265]
train() client id: f_00001-15-6 loss: 0.485136  [  224/  265]
train() client id: f_00001-15-7 loss: 0.483808  [  256/  265]
train() client id: f_00002-0-0 loss: 1.158604  [   32/  124]
train() client id: f_00002-0-1 loss: 1.437371  [   64/  124]
train() client id: f_00002-0-2 loss: 1.059923  [   96/  124]
train() client id: f_00002-1-0 loss: 1.285190  [   32/  124]
train() client id: f_00002-1-1 loss: 1.084763  [   64/  124]
train() client id: f_00002-1-2 loss: 0.896355  [   96/  124]
train() client id: f_00002-2-0 loss: 1.112852  [   32/  124]
train() client id: f_00002-2-1 loss: 1.076674  [   64/  124]
train() client id: f_00002-2-2 loss: 1.173272  [   96/  124]
train() client id: f_00002-3-0 loss: 0.994779  [   32/  124]
train() client id: f_00002-3-1 loss: 1.139744  [   64/  124]
train() client id: f_00002-3-2 loss: 1.273425  [   96/  124]
train() client id: f_00002-4-0 loss: 0.946476  [   32/  124]
train() client id: f_00002-4-1 loss: 1.185575  [   64/  124]
train() client id: f_00002-4-2 loss: 1.086850  [   96/  124]
train() client id: f_00002-5-0 loss: 1.028174  [   32/  124]
train() client id: f_00002-5-1 loss: 1.091856  [   64/  124]
train() client id: f_00002-5-2 loss: 1.118698  [   96/  124]
train() client id: f_00002-6-0 loss: 0.966495  [   32/  124]
train() client id: f_00002-6-1 loss: 1.029710  [   64/  124]
train() client id: f_00002-6-2 loss: 1.092207  [   96/  124]
train() client id: f_00002-7-0 loss: 1.038762  [   32/  124]
train() client id: f_00002-7-1 loss: 1.029209  [   64/  124]
train() client id: f_00002-7-2 loss: 0.913619  [   96/  124]
train() client id: f_00002-8-0 loss: 0.877824  [   32/  124]
train() client id: f_00002-8-1 loss: 0.847817  [   64/  124]
train() client id: f_00002-8-2 loss: 1.190402  [   96/  124]
train() client id: f_00002-9-0 loss: 1.012079  [   32/  124]
train() client id: f_00002-9-1 loss: 1.009058  [   64/  124]
train() client id: f_00002-9-2 loss: 0.956726  [   96/  124]
train() client id: f_00002-10-0 loss: 1.121487  [   32/  124]
train() client id: f_00002-10-1 loss: 0.957999  [   64/  124]
train() client id: f_00002-10-2 loss: 0.990335  [   96/  124]
train() client id: f_00002-11-0 loss: 0.910403  [   32/  124]
train() client id: f_00002-11-1 loss: 1.025539  [   64/  124]
train() client id: f_00002-11-2 loss: 1.047591  [   96/  124]
train() client id: f_00002-12-0 loss: 1.148395  [   32/  124]
train() client id: f_00002-12-1 loss: 1.011872  [   64/  124]
train() client id: f_00002-12-2 loss: 0.803510  [   96/  124]
train() client id: f_00002-13-0 loss: 0.981241  [   32/  124]
train() client id: f_00002-13-1 loss: 1.012199  [   64/  124]
train() client id: f_00002-13-2 loss: 0.940451  [   96/  124]
train() client id: f_00002-14-0 loss: 1.126158  [   32/  124]
train() client id: f_00002-14-1 loss: 0.862566  [   64/  124]
train() client id: f_00002-14-2 loss: 0.882041  [   96/  124]
train() client id: f_00002-15-0 loss: 0.764686  [   32/  124]
train() client id: f_00002-15-1 loss: 0.946208  [   64/  124]
train() client id: f_00002-15-2 loss: 1.202096  [   96/  124]
train() client id: f_00003-0-0 loss: 0.666956  [   32/   43]
train() client id: f_00003-1-0 loss: 0.543179  [   32/   43]
train() client id: f_00003-2-0 loss: 0.528516  [   32/   43]
train() client id: f_00003-3-0 loss: 0.691150  [   32/   43]
train() client id: f_00003-4-0 loss: 0.422608  [   32/   43]
train() client id: f_00003-5-0 loss: 0.490600  [   32/   43]
train() client id: f_00003-6-0 loss: 0.510133  [   32/   43]
train() client id: f_00003-7-0 loss: 0.715380  [   32/   43]
train() client id: f_00003-8-0 loss: 0.679664  [   32/   43]
train() client id: f_00003-9-0 loss: 0.443989  [   32/   43]
train() client id: f_00003-10-0 loss: 0.591719  [   32/   43]
train() client id: f_00003-11-0 loss: 0.603313  [   32/   43]
train() client id: f_00003-12-0 loss: 0.702302  [   32/   43]
train() client id: f_00003-13-0 loss: 0.740281  [   32/   43]
train() client id: f_00003-14-0 loss: 0.494563  [   32/   43]
train() client id: f_00003-15-0 loss: 0.579185  [   32/   43]
train() client id: f_00004-0-0 loss: 0.862698  [   32/  306]
train() client id: f_00004-0-1 loss: 1.092850  [   64/  306]
train() client id: f_00004-0-2 loss: 0.863317  [   96/  306]
train() client id: f_00004-0-3 loss: 0.849387  [  128/  306]
train() client id: f_00004-0-4 loss: 1.030045  [  160/  306]
train() client id: f_00004-0-5 loss: 0.808701  [  192/  306]
train() client id: f_00004-0-6 loss: 0.899057  [  224/  306]
train() client id: f_00004-0-7 loss: 0.813283  [  256/  306]
train() client id: f_00004-0-8 loss: 0.797293  [  288/  306]
train() client id: f_00004-1-0 loss: 0.945728  [   32/  306]
train() client id: f_00004-1-1 loss: 0.943087  [   64/  306]
train() client id: f_00004-1-2 loss: 0.867515  [   96/  306]
train() client id: f_00004-1-3 loss: 0.859071  [  128/  306]
train() client id: f_00004-1-4 loss: 0.872414  [  160/  306]
train() client id: f_00004-1-5 loss: 0.915098  [  192/  306]
train() client id: f_00004-1-6 loss: 0.818016  [  224/  306]
train() client id: f_00004-1-7 loss: 0.914470  [  256/  306]
train() client id: f_00004-1-8 loss: 0.854062  [  288/  306]
train() client id: f_00004-2-0 loss: 0.774790  [   32/  306]
train() client id: f_00004-2-1 loss: 0.757158  [   64/  306]
train() client id: f_00004-2-2 loss: 0.889229  [   96/  306]
train() client id: f_00004-2-3 loss: 0.976839  [  128/  306]
train() client id: f_00004-2-4 loss: 0.786999  [  160/  306]
train() client id: f_00004-2-5 loss: 0.835218  [  192/  306]
train() client id: f_00004-2-6 loss: 0.863656  [  224/  306]
train() client id: f_00004-2-7 loss: 0.949596  [  256/  306]
train() client id: f_00004-2-8 loss: 0.960822  [  288/  306]
train() client id: f_00004-3-0 loss: 0.872278  [   32/  306]
train() client id: f_00004-3-1 loss: 0.845382  [   64/  306]
train() client id: f_00004-3-2 loss: 0.889227  [   96/  306]
train() client id: f_00004-3-3 loss: 0.881512  [  128/  306]
train() client id: f_00004-3-4 loss: 0.820291  [  160/  306]
train() client id: f_00004-3-5 loss: 0.943887  [  192/  306]
train() client id: f_00004-3-6 loss: 0.912835  [  224/  306]
train() client id: f_00004-3-7 loss: 0.859910  [  256/  306]
train() client id: f_00004-3-8 loss: 0.735658  [  288/  306]
train() client id: f_00004-4-0 loss: 0.884643  [   32/  306]
train() client id: f_00004-4-1 loss: 0.921267  [   64/  306]
train() client id: f_00004-4-2 loss: 0.817133  [   96/  306]
train() client id: f_00004-4-3 loss: 0.873860  [  128/  306]
train() client id: f_00004-4-4 loss: 0.768710  [  160/  306]
train() client id: f_00004-4-5 loss: 0.886976  [  192/  306]
train() client id: f_00004-4-6 loss: 0.986430  [  224/  306]
train() client id: f_00004-4-7 loss: 0.944106  [  256/  306]
train() client id: f_00004-4-8 loss: 0.697856  [  288/  306]
train() client id: f_00004-5-0 loss: 1.016486  [   32/  306]
train() client id: f_00004-5-1 loss: 0.785595  [   64/  306]
train() client id: f_00004-5-2 loss: 0.732261  [   96/  306]
train() client id: f_00004-5-3 loss: 0.966702  [  128/  306]
train() client id: f_00004-5-4 loss: 0.818117  [  160/  306]
train() client id: f_00004-5-5 loss: 0.957778  [  192/  306]
train() client id: f_00004-5-6 loss: 0.791306  [  224/  306]
train() client id: f_00004-5-7 loss: 0.880522  [  256/  306]
train() client id: f_00004-5-8 loss: 0.905448  [  288/  306]
train() client id: f_00004-6-0 loss: 0.961860  [   32/  306]
train() client id: f_00004-6-1 loss: 0.919062  [   64/  306]
train() client id: f_00004-6-2 loss: 0.981597  [   96/  306]
train() client id: f_00004-6-3 loss: 0.717782  [  128/  306]
train() client id: f_00004-6-4 loss: 0.949735  [  160/  306]
train() client id: f_00004-6-5 loss: 0.749568  [  192/  306]
train() client id: f_00004-6-6 loss: 0.737124  [  224/  306]
train() client id: f_00004-6-7 loss: 0.885459  [  256/  306]
train() client id: f_00004-6-8 loss: 0.854649  [  288/  306]
train() client id: f_00004-7-0 loss: 0.813231  [   32/  306]
train() client id: f_00004-7-1 loss: 0.840349  [   64/  306]
train() client id: f_00004-7-2 loss: 0.931420  [   96/  306]
train() client id: f_00004-7-3 loss: 0.903380  [  128/  306]
train() client id: f_00004-7-4 loss: 0.795430  [  160/  306]
train() client id: f_00004-7-5 loss: 0.748759  [  192/  306]
train() client id: f_00004-7-6 loss: 0.927176  [  224/  306]
train() client id: f_00004-7-7 loss: 0.941600  [  256/  306]
train() client id: f_00004-7-8 loss: 0.927696  [  288/  306]
train() client id: f_00004-8-0 loss: 0.843068  [   32/  306]
train() client id: f_00004-8-1 loss: 0.795740  [   64/  306]
train() client id: f_00004-8-2 loss: 0.901890  [   96/  306]
train() client id: f_00004-8-3 loss: 0.880630  [  128/  306]
train() client id: f_00004-8-4 loss: 0.910801  [  160/  306]
train() client id: f_00004-8-5 loss: 0.816257  [  192/  306]
train() client id: f_00004-8-6 loss: 0.727620  [  224/  306]
train() client id: f_00004-8-7 loss: 1.008752  [  256/  306]
train() client id: f_00004-8-8 loss: 0.821175  [  288/  306]
train() client id: f_00004-9-0 loss: 0.927741  [   32/  306]
train() client id: f_00004-9-1 loss: 0.850353  [   64/  306]
train() client id: f_00004-9-2 loss: 0.936364  [   96/  306]
train() client id: f_00004-9-3 loss: 0.802065  [  128/  306]
train() client id: f_00004-9-4 loss: 0.741887  [  160/  306]
train() client id: f_00004-9-5 loss: 0.871818  [  192/  306]
train() client id: f_00004-9-6 loss: 0.836954  [  224/  306]
train() client id: f_00004-9-7 loss: 0.956802  [  256/  306]
train() client id: f_00004-9-8 loss: 0.763454  [  288/  306]
train() client id: f_00004-10-0 loss: 0.815808  [   32/  306]
train() client id: f_00004-10-1 loss: 0.936436  [   64/  306]
train() client id: f_00004-10-2 loss: 0.691910  [   96/  306]
train() client id: f_00004-10-3 loss: 0.855316  [  128/  306]
train() client id: f_00004-10-4 loss: 0.874340  [  160/  306]
train() client id: f_00004-10-5 loss: 0.898468  [  192/  306]
train() client id: f_00004-10-6 loss: 0.960785  [  224/  306]
train() client id: f_00004-10-7 loss: 0.848662  [  256/  306]
train() client id: f_00004-10-8 loss: 0.881020  [  288/  306]
train() client id: f_00004-11-0 loss: 0.887082  [   32/  306]
train() client id: f_00004-11-1 loss: 0.786194  [   64/  306]
train() client id: f_00004-11-2 loss: 0.976294  [   96/  306]
train() client id: f_00004-11-3 loss: 0.768205  [  128/  306]
train() client id: f_00004-11-4 loss: 0.786985  [  160/  306]
train() client id: f_00004-11-5 loss: 0.874397  [  192/  306]
train() client id: f_00004-11-6 loss: 0.923696  [  224/  306]
train() client id: f_00004-11-7 loss: 0.777467  [  256/  306]
train() client id: f_00004-11-8 loss: 0.896745  [  288/  306]
train() client id: f_00004-12-0 loss: 0.829425  [   32/  306]
train() client id: f_00004-12-1 loss: 0.861374  [   64/  306]
train() client id: f_00004-12-2 loss: 0.860920  [   96/  306]
train() client id: f_00004-12-3 loss: 0.776351  [  128/  306]
train() client id: f_00004-12-4 loss: 0.947314  [  160/  306]
train() client id: f_00004-12-5 loss: 0.965865  [  192/  306]
train() client id: f_00004-12-6 loss: 0.886695  [  224/  306]
train() client id: f_00004-12-7 loss: 0.791180  [  256/  306]
train() client id: f_00004-12-8 loss: 0.794618  [  288/  306]
train() client id: f_00004-13-0 loss: 0.894640  [   32/  306]
train() client id: f_00004-13-1 loss: 0.915639  [   64/  306]
train() client id: f_00004-13-2 loss: 0.828247  [   96/  306]
train() client id: f_00004-13-3 loss: 0.713036  [  128/  306]
train() client id: f_00004-13-4 loss: 0.860799  [  160/  306]
train() client id: f_00004-13-5 loss: 0.910382  [  192/  306]
train() client id: f_00004-13-6 loss: 0.860966  [  224/  306]
train() client id: f_00004-13-7 loss: 0.804486  [  256/  306]
train() client id: f_00004-13-8 loss: 0.878907  [  288/  306]
train() client id: f_00004-14-0 loss: 0.806364  [   32/  306]
train() client id: f_00004-14-1 loss: 0.755002  [   64/  306]
train() client id: f_00004-14-2 loss: 0.866777  [   96/  306]
train() client id: f_00004-14-3 loss: 0.926970  [  128/  306]
train() client id: f_00004-14-4 loss: 0.740580  [  160/  306]
train() client id: f_00004-14-5 loss: 0.903295  [  192/  306]
train() client id: f_00004-14-6 loss: 0.907217  [  224/  306]
train() client id: f_00004-14-7 loss: 0.790376  [  256/  306]
train() client id: f_00004-14-8 loss: 0.894266  [  288/  306]
train() client id: f_00004-15-0 loss: 0.857113  [   32/  306]
train() client id: f_00004-15-1 loss: 0.895305  [   64/  306]
train() client id: f_00004-15-2 loss: 0.808048  [   96/  306]
train() client id: f_00004-15-3 loss: 0.949628  [  128/  306]
train() client id: f_00004-15-4 loss: 0.784556  [  160/  306]
train() client id: f_00004-15-5 loss: 0.868409  [  192/  306]
train() client id: f_00004-15-6 loss: 0.739894  [  224/  306]
train() client id: f_00004-15-7 loss: 0.897541  [  256/  306]
train() client id: f_00004-15-8 loss: 0.921538  [  288/  306]
train() client id: f_00005-0-0 loss: 0.557337  [   32/  146]
train() client id: f_00005-0-1 loss: 0.875087  [   64/  146]
train() client id: f_00005-0-2 loss: 0.646563  [   96/  146]
train() client id: f_00005-0-3 loss: 0.502841  [  128/  146]
train() client id: f_00005-1-0 loss: 0.467925  [   32/  146]
train() client id: f_00005-1-1 loss: 0.641114  [   64/  146]
train() client id: f_00005-1-2 loss: 0.919799  [   96/  146]
train() client id: f_00005-1-3 loss: 0.588087  [  128/  146]
train() client id: f_00005-2-0 loss: 0.590695  [   32/  146]
train() client id: f_00005-2-1 loss: 0.729205  [   64/  146]
train() client id: f_00005-2-2 loss: 0.703214  [   96/  146]
train() client id: f_00005-2-3 loss: 0.581264  [  128/  146]
train() client id: f_00005-3-0 loss: 0.472010  [   32/  146]
train() client id: f_00005-3-1 loss: 0.940292  [   64/  146]
train() client id: f_00005-3-2 loss: 0.488467  [   96/  146]
train() client id: f_00005-3-3 loss: 0.724150  [  128/  146]
train() client id: f_00005-4-0 loss: 0.569092  [   32/  146]
train() client id: f_00005-4-1 loss: 0.593875  [   64/  146]
train() client id: f_00005-4-2 loss: 0.913599  [   96/  146]
train() client id: f_00005-4-3 loss: 0.677955  [  128/  146]
train() client id: f_00005-5-0 loss: 0.749272  [   32/  146]
train() client id: f_00005-5-1 loss: 0.657012  [   64/  146]
train() client id: f_00005-5-2 loss: 0.682457  [   96/  146]
train() client id: f_00005-5-3 loss: 0.614397  [  128/  146]
train() client id: f_00005-6-0 loss: 0.793807  [   32/  146]
train() client id: f_00005-6-1 loss: 0.688302  [   64/  146]
train() client id: f_00005-6-2 loss: 0.799527  [   96/  146]
train() client id: f_00005-6-3 loss: 0.572464  [  128/  146]
train() client id: f_00005-7-0 loss: 0.561127  [   32/  146]
train() client id: f_00005-7-1 loss: 0.694326  [   64/  146]
train() client id: f_00005-7-2 loss: 0.615616  [   96/  146]
train() client id: f_00005-7-3 loss: 0.732408  [  128/  146]
train() client id: f_00005-8-0 loss: 0.457700  [   32/  146]
train() client id: f_00005-8-1 loss: 0.835607  [   64/  146]
train() client id: f_00005-8-2 loss: 0.894948  [   96/  146]
train() client id: f_00005-8-3 loss: 0.523304  [  128/  146]
train() client id: f_00005-9-0 loss: 0.494210  [   32/  146]
train() client id: f_00005-9-1 loss: 0.685629  [   64/  146]
train() client id: f_00005-9-2 loss: 0.774204  [   96/  146]
train() client id: f_00005-9-3 loss: 0.698749  [  128/  146]
train() client id: f_00005-10-0 loss: 0.547040  [   32/  146]
train() client id: f_00005-10-1 loss: 0.548698  [   64/  146]
train() client id: f_00005-10-2 loss: 1.110915  [   96/  146]
train() client id: f_00005-10-3 loss: 0.454883  [  128/  146]
train() client id: f_00005-11-0 loss: 0.553604  [   32/  146]
train() client id: f_00005-11-1 loss: 0.942696  [   64/  146]
train() client id: f_00005-11-2 loss: 0.485681  [   96/  146]
train() client id: f_00005-11-3 loss: 0.686362  [  128/  146]
train() client id: f_00005-12-0 loss: 0.761283  [   32/  146]
train() client id: f_00005-12-1 loss: 0.521340  [   64/  146]
train() client id: f_00005-12-2 loss: 0.522793  [   96/  146]
train() client id: f_00005-12-3 loss: 0.803156  [  128/  146]
train() client id: f_00005-13-0 loss: 0.702412  [   32/  146]
train() client id: f_00005-13-1 loss: 0.588940  [   64/  146]
train() client id: f_00005-13-2 loss: 0.599917  [   96/  146]
train() client id: f_00005-13-3 loss: 0.614510  [  128/  146]
train() client id: f_00005-14-0 loss: 0.425898  [   32/  146]
train() client id: f_00005-14-1 loss: 0.605261  [   64/  146]
train() client id: f_00005-14-2 loss: 0.654904  [   96/  146]
train() client id: f_00005-14-3 loss: 0.908520  [  128/  146]
train() client id: f_00005-15-0 loss: 1.015116  [   32/  146]
train() client id: f_00005-15-1 loss: 0.632961  [   64/  146]
train() client id: f_00005-15-2 loss: 0.539191  [   96/  146]
train() client id: f_00005-15-3 loss: 0.614425  [  128/  146]
train() client id: f_00006-0-0 loss: 0.531702  [   32/   54]
train() client id: f_00006-1-0 loss: 0.536989  [   32/   54]
train() client id: f_00006-2-0 loss: 0.507633  [   32/   54]
train() client id: f_00006-3-0 loss: 0.403686  [   32/   54]
train() client id: f_00006-4-0 loss: 0.518386  [   32/   54]
train() client id: f_00006-5-0 loss: 0.476617  [   32/   54]
train() client id: f_00006-6-0 loss: 0.545640  [   32/   54]
train() client id: f_00006-7-0 loss: 0.476565  [   32/   54]
train() client id: f_00006-8-0 loss: 0.529375  [   32/   54]
train() client id: f_00006-9-0 loss: 0.524649  [   32/   54]
train() client id: f_00006-10-0 loss: 0.520158  [   32/   54]
train() client id: f_00006-11-0 loss: 0.523925  [   32/   54]
train() client id: f_00006-12-0 loss: 0.485870  [   32/   54]
train() client id: f_00006-13-0 loss: 0.475344  [   32/   54]
train() client id: f_00006-14-0 loss: 0.502075  [   32/   54]
train() client id: f_00006-15-0 loss: 0.520003  [   32/   54]
train() client id: f_00007-0-0 loss: 0.581863  [   32/  179]
train() client id: f_00007-0-1 loss: 0.817400  [   64/  179]
train() client id: f_00007-0-2 loss: 0.615903  [   96/  179]
train() client id: f_00007-0-3 loss: 0.549512  [  128/  179]
train() client id: f_00007-0-4 loss: 0.657461  [  160/  179]
train() client id: f_00007-1-0 loss: 0.587437  [   32/  179]
train() client id: f_00007-1-1 loss: 0.654975  [   64/  179]
train() client id: f_00007-1-2 loss: 0.700641  [   96/  179]
train() client id: f_00007-1-3 loss: 0.441317  [  128/  179]
train() client id: f_00007-1-4 loss: 0.678525  [  160/  179]
train() client id: f_00007-2-0 loss: 0.494899  [   32/  179]
train() client id: f_00007-2-1 loss: 0.774669  [   64/  179]
train() client id: f_00007-2-2 loss: 0.613757  [   96/  179]
train() client id: f_00007-2-3 loss: 0.584921  [  128/  179]
train() client id: f_00007-2-4 loss: 0.647836  [  160/  179]
train() client id: f_00007-3-0 loss: 0.601207  [   32/  179]
train() client id: f_00007-3-1 loss: 0.515935  [   64/  179]
train() client id: f_00007-3-2 loss: 0.501597  [   96/  179]
train() client id: f_00007-3-3 loss: 0.441056  [  128/  179]
train() client id: f_00007-3-4 loss: 0.827028  [  160/  179]
train() client id: f_00007-4-0 loss: 0.631067  [   32/  179]
train() client id: f_00007-4-1 loss: 0.407292  [   64/  179]
train() client id: f_00007-4-2 loss: 0.484391  [   96/  179]
train() client id: f_00007-4-3 loss: 0.786737  [  128/  179]
train() client id: f_00007-4-4 loss: 0.658780  [  160/  179]
train() client id: f_00007-5-0 loss: 0.654040  [   32/  179]
train() client id: f_00007-5-1 loss: 0.498513  [   64/  179]
train() client id: f_00007-5-2 loss: 0.693814  [   96/  179]
train() client id: f_00007-5-3 loss: 0.509513  [  128/  179]
train() client id: f_00007-5-4 loss: 0.496481  [  160/  179]
train() client id: f_00007-6-0 loss: 0.765834  [   32/  179]
train() client id: f_00007-6-1 loss: 0.500170  [   64/  179]
train() client id: f_00007-6-2 loss: 0.577291  [   96/  179]
train() client id: f_00007-6-3 loss: 0.471869  [  128/  179]
train() client id: f_00007-6-4 loss: 0.586139  [  160/  179]
train() client id: f_00007-7-0 loss: 0.510826  [   32/  179]
train() client id: f_00007-7-1 loss: 0.780904  [   64/  179]
train() client id: f_00007-7-2 loss: 0.425596  [   96/  179]
train() client id: f_00007-7-3 loss: 0.533317  [  128/  179]
train() client id: f_00007-7-4 loss: 0.527870  [  160/  179]
train() client id: f_00007-8-0 loss: 0.651250  [   32/  179]
train() client id: f_00007-8-1 loss: 0.491889  [   64/  179]
train() client id: f_00007-8-2 loss: 0.630719  [   96/  179]
train() client id: f_00007-8-3 loss: 0.448765  [  128/  179]
train() client id: f_00007-8-4 loss: 0.577339  [  160/  179]
train() client id: f_00007-9-0 loss: 0.691821  [   32/  179]
train() client id: f_00007-9-1 loss: 0.526668  [   64/  179]
train() client id: f_00007-9-2 loss: 0.602585  [   96/  179]
train() client id: f_00007-9-3 loss: 0.382175  [  128/  179]
train() client id: f_00007-9-4 loss: 0.561054  [  160/  179]
train() client id: f_00007-10-0 loss: 0.427887  [   32/  179]
train() client id: f_00007-10-1 loss: 0.511353  [   64/  179]
train() client id: f_00007-10-2 loss: 0.480169  [   96/  179]
train() client id: f_00007-10-3 loss: 0.846920  [  128/  179]
train() client id: f_00007-10-4 loss: 0.388100  [  160/  179]
train() client id: f_00007-11-0 loss: 0.635096  [   32/  179]
train() client id: f_00007-11-1 loss: 0.460269  [   64/  179]
train() client id: f_00007-11-2 loss: 0.489493  [   96/  179]
train() client id: f_00007-11-3 loss: 0.623190  [  128/  179]
train() client id: f_00007-11-4 loss: 0.454588  [  160/  179]
train() client id: f_00007-12-0 loss: 0.704420  [   32/  179]
train() client id: f_00007-12-1 loss: 0.400092  [   64/  179]
train() client id: f_00007-12-2 loss: 0.578099  [   96/  179]
train() client id: f_00007-12-3 loss: 0.645137  [  128/  179]
train() client id: f_00007-12-4 loss: 0.507669  [  160/  179]
train() client id: f_00007-13-0 loss: 0.485924  [   32/  179]
train() client id: f_00007-13-1 loss: 0.615743  [   64/  179]
train() client id: f_00007-13-2 loss: 0.493886  [   96/  179]
train() client id: f_00007-13-3 loss: 0.493928  [  128/  179]
train() client id: f_00007-13-4 loss: 0.646580  [  160/  179]
train() client id: f_00007-14-0 loss: 0.410649  [   32/  179]
train() client id: f_00007-14-1 loss: 0.476807  [   64/  179]
train() client id: f_00007-14-2 loss: 0.591926  [   96/  179]
train() client id: f_00007-14-3 loss: 0.671342  [  128/  179]
train() client id: f_00007-14-4 loss: 0.399807  [  160/  179]
train() client id: f_00007-15-0 loss: 0.612197  [   32/  179]
train() client id: f_00007-15-1 loss: 0.545621  [   64/  179]
train() client id: f_00007-15-2 loss: 0.669693  [   96/  179]
train() client id: f_00007-15-3 loss: 0.639246  [  128/  179]
train() client id: f_00007-15-4 loss: 0.376586  [  160/  179]
train() client id: f_00008-0-0 loss: 0.787212  [   32/  130]
train() client id: f_00008-0-1 loss: 0.817348  [   64/  130]
train() client id: f_00008-0-2 loss: 0.775399  [   96/  130]
train() client id: f_00008-0-3 loss: 0.738576  [  128/  130]
train() client id: f_00008-1-0 loss: 0.770106  [   32/  130]
train() client id: f_00008-1-1 loss: 0.880692  [   64/  130]
train() client id: f_00008-1-2 loss: 0.713474  [   96/  130]
train() client id: f_00008-1-3 loss: 0.760159  [  128/  130]
train() client id: f_00008-2-0 loss: 0.825429  [   32/  130]
train() client id: f_00008-2-1 loss: 0.761221  [   64/  130]
train() client id: f_00008-2-2 loss: 0.751034  [   96/  130]
train() client id: f_00008-2-3 loss: 0.772631  [  128/  130]
train() client id: f_00008-3-0 loss: 0.736883  [   32/  130]
train() client id: f_00008-3-1 loss: 0.655820  [   64/  130]
train() client id: f_00008-3-2 loss: 0.797771  [   96/  130]
train() client id: f_00008-3-3 loss: 0.934156  [  128/  130]
train() client id: f_00008-4-0 loss: 0.711264  [   32/  130]
train() client id: f_00008-4-1 loss: 0.873047  [   64/  130]
train() client id: f_00008-4-2 loss: 0.694858  [   96/  130]
train() client id: f_00008-4-3 loss: 0.795176  [  128/  130]
train() client id: f_00008-5-0 loss: 0.766186  [   32/  130]
train() client id: f_00008-5-1 loss: 0.809730  [   64/  130]
train() client id: f_00008-5-2 loss: 0.759245  [   96/  130]
train() client id: f_00008-5-3 loss: 0.760415  [  128/  130]
train() client id: f_00008-6-0 loss: 0.839323  [   32/  130]
train() client id: f_00008-6-1 loss: 0.758918  [   64/  130]
train() client id: f_00008-6-2 loss: 0.729649  [   96/  130]
train() client id: f_00008-6-3 loss: 0.744228  [  128/  130]
train() client id: f_00008-7-0 loss: 0.765280  [   32/  130]
train() client id: f_00008-7-1 loss: 0.801748  [   64/  130]
train() client id: f_00008-7-2 loss: 0.738998  [   96/  130]
train() client id: f_00008-7-3 loss: 0.817901  [  128/  130]
train() client id: f_00008-8-0 loss: 0.876299  [   32/  130]
train() client id: f_00008-8-1 loss: 0.692923  [   64/  130]
train() client id: f_00008-8-2 loss: 0.793820  [   96/  130]
train() client id: f_00008-8-3 loss: 0.723563  [  128/  130]
train() client id: f_00008-9-0 loss: 0.769657  [   32/  130]
train() client id: f_00008-9-1 loss: 0.804627  [   64/  130]
train() client id: f_00008-9-2 loss: 0.799252  [   96/  130]
train() client id: f_00008-9-3 loss: 0.714762  [  128/  130]
train() client id: f_00008-10-0 loss: 0.784172  [   32/  130]
train() client id: f_00008-10-1 loss: 0.736067  [   64/  130]
train() client id: f_00008-10-2 loss: 0.833246  [   96/  130]
train() client id: f_00008-10-3 loss: 0.757673  [  128/  130]
train() client id: f_00008-11-0 loss: 0.771490  [   32/  130]
train() client id: f_00008-11-1 loss: 0.808954  [   64/  130]
train() client id: f_00008-11-2 loss: 0.797166  [   96/  130]
train() client id: f_00008-11-3 loss: 0.736811  [  128/  130]
train() client id: f_00008-12-0 loss: 0.692103  [   32/  130]
train() client id: f_00008-12-1 loss: 0.807208  [   64/  130]
train() client id: f_00008-12-2 loss: 0.684449  [   96/  130]
train() client id: f_00008-12-3 loss: 0.912556  [  128/  130]
train() client id: f_00008-13-0 loss: 0.744407  [   32/  130]
train() client id: f_00008-13-1 loss: 0.760703  [   64/  130]
train() client id: f_00008-13-2 loss: 0.806125  [   96/  130]
train() client id: f_00008-13-3 loss: 0.803004  [  128/  130]
train() client id: f_00008-14-0 loss: 0.747519  [   32/  130]
train() client id: f_00008-14-1 loss: 0.765233  [   64/  130]
train() client id: f_00008-14-2 loss: 0.855117  [   96/  130]
train() client id: f_00008-14-3 loss: 0.745589  [  128/  130]
train() client id: f_00008-15-0 loss: 0.819654  [   32/  130]
train() client id: f_00008-15-1 loss: 0.784989  [   64/  130]
train() client id: f_00008-15-2 loss: 0.748902  [   96/  130]
train() client id: f_00008-15-3 loss: 0.761737  [  128/  130]
train() client id: f_00009-0-0 loss: 1.183469  [   32/  118]
train() client id: f_00009-0-1 loss: 1.127500  [   64/  118]
train() client id: f_00009-0-2 loss: 1.274630  [   96/  118]
train() client id: f_00009-1-0 loss: 1.049124  [   32/  118]
train() client id: f_00009-1-1 loss: 1.044017  [   64/  118]
train() client id: f_00009-1-2 loss: 1.263975  [   96/  118]
train() client id: f_00009-2-0 loss: 1.093333  [   32/  118]
train() client id: f_00009-2-1 loss: 1.116731  [   64/  118]
train() client id: f_00009-2-2 loss: 1.013308  [   96/  118]
train() client id: f_00009-3-0 loss: 0.933001  [   32/  118]
train() client id: f_00009-3-1 loss: 1.170380  [   64/  118]
train() client id: f_00009-3-2 loss: 1.036983  [   96/  118]
train() client id: f_00009-4-0 loss: 1.002599  [   32/  118]
train() client id: f_00009-4-1 loss: 0.867310  [   64/  118]
train() client id: f_00009-4-2 loss: 1.161142  [   96/  118]
train() client id: f_00009-5-0 loss: 1.024316  [   32/  118]
train() client id: f_00009-5-1 loss: 0.965138  [   64/  118]
train() client id: f_00009-5-2 loss: 0.870157  [   96/  118]
train() client id: f_00009-6-0 loss: 1.040446  [   32/  118]
train() client id: f_00009-6-1 loss: 0.918473  [   64/  118]
train() client id: f_00009-6-2 loss: 0.983700  [   96/  118]
train() client id: f_00009-7-0 loss: 0.933179  [   32/  118]
train() client id: f_00009-7-1 loss: 0.922859  [   64/  118]
train() client id: f_00009-7-2 loss: 0.932427  [   96/  118]
train() client id: f_00009-8-0 loss: 0.929437  [   32/  118]
train() client id: f_00009-8-1 loss: 0.677270  [   64/  118]
train() client id: f_00009-8-2 loss: 0.998389  [   96/  118]
train() client id: f_00009-9-0 loss: 1.018292  [   32/  118]
train() client id: f_00009-9-1 loss: 1.063757  [   64/  118]
train() client id: f_00009-9-2 loss: 0.709603  [   96/  118]
train() client id: f_00009-10-0 loss: 0.940084  [   32/  118]
train() client id: f_00009-10-1 loss: 1.013493  [   64/  118]
train() client id: f_00009-10-2 loss: 0.815869  [   96/  118]
train() client id: f_00009-11-0 loss: 0.830292  [   32/  118]
train() client id: f_00009-11-1 loss: 0.918463  [   64/  118]
train() client id: f_00009-11-2 loss: 0.984826  [   96/  118]
train() client id: f_00009-12-0 loss: 0.860319  [   32/  118]
train() client id: f_00009-12-1 loss: 0.999253  [   64/  118]
train() client id: f_00009-12-2 loss: 1.010677  [   96/  118]
train() client id: f_00009-13-0 loss: 0.968110  [   32/  118]
train() client id: f_00009-13-1 loss: 0.879637  [   64/  118]
train() client id: f_00009-13-2 loss: 0.847142  [   96/  118]
train() client id: f_00009-14-0 loss: 0.854926  [   32/  118]
train() client id: f_00009-14-1 loss: 0.784214  [   64/  118]
train() client id: f_00009-14-2 loss: 0.992332  [   96/  118]
train() client id: f_00009-15-0 loss: 0.976613  [   32/  118]
train() client id: f_00009-15-1 loss: 0.819530  [   64/  118]
train() client id: f_00009-15-2 loss: 0.843252  [   96/  118]
At round 45 accuracy: 0.6419098143236074
At round 45 training accuracy: 0.5841716968477532
At round 45 training loss: 0.8452371228013247
update_location
xs = 8.927491 346.223621 5.882650 0.934260 -262.581990 -110.230757 -70.849135 -5.143845 -285.120581 20.134486 
ys = -337.390647 7.291448 235.684448 -57.290817 -9.642386 0.794442 -1.381692 231.628436 25.881276 -772.232496 
xs mean: -35.18237997052122
ys mean: -67.66579882624052
dists_uav = 352.011575 360.449665 256.089369 115.252378 281.144584 148.833635 122.562266 252.345381 303.254985 778.940579 
uav_gains = -119.281227 -119.717488 -111.822304 -101.541368 -114.083487 -104.321632 -102.209208 -111.496769 -115.999021 -129.252577 
uav_gains_db_mean: -112.97250808155034
dists_bs = 544.401963 547.540155 190.790816 291.396444 204.360994 185.856480 204.836286 179.047182 185.372425 967.122985 
bs_gains = -116.172810 -116.242706 -103.422668 -108.572618 -104.258203 -103.104035 -104.286452 -102.650149 -103.072323 -123.160547 
bs_gains_db_mean: -108.49425119356258
Round 46
-------------------------------
ene_coms = [0.02535017 0.02679457 0.00734349 0.00673282 0.00765091 0.00767527
 0.00693782 0.00708004 0.00722167 0.0439772 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.087647   10.36734157  4.78593914  1.71218225 11.67904203  5.62286918
  2.13113817  6.86454709  5.01152811  5.01861105]
obj_prev = 58.280845570038565
eta_min = 2.336390545060867e-19	eta_max = 0.8004912774402068
af = 12.132976692851157	bf = 1.823614581131632	zeta = 13.346274362136274	eta = 0.9090909090909091
af = 12.132976692851157	bf = 1.823614581131632	zeta = 32.780118484178544	eta = 0.37013217931799686
af = 12.132976692851157	bf = 1.823614581131632	zeta = 22.04037126050531	eta = 0.5504887621649341
af = 12.132976692851157	bf = 1.823614581131632	zeta = 20.169417201662224	eta = 0.6015531619749153
af = 12.132976692851157	bf = 1.823614581131632	zeta = 20.053071886103123	eta = 0.6050432952000421
af = 12.132976692851157	bf = 1.823614581131632	zeta = 20.05255882022399	eta = 0.6050587758712597
af = 12.132976692851157	bf = 1.823614581131632	zeta = 20.052558810168648	eta = 0.605058776174666
eta = 0.605058776174666
ene_coms = [0.02535017 0.02679457 0.00734349 0.00673282 0.00765091 0.00767527
 0.00693782 0.00708004 0.00722167 0.0439772 ]
ene_comp = [0.04145929 0.08719613 0.04080121 0.01414881 0.10068685 0.04804013
 0.01776827 0.05889852 0.04277546 0.03882696]
ene_total = [2.10193436 3.58633308 1.51471058 0.65696985 3.40848232 1.75289738
 0.77729378 2.07579278 1.57299108 2.60515361]
ti_comp = [0.62556777 0.61112373 0.80563454 0.81174127 0.80256037 0.80231671
 0.80969123 0.80826907 0.80685275 0.43929743]
ti_coms = [0.25350167 0.26794571 0.07343489 0.06732817 0.07650907 0.07675273
 0.06937821 0.07080037 0.07221669 0.43977201]
t_total = [27.65695572 27.65695572 27.65695572 27.65695572 27.65695572 27.65695572
 27.65695572 27.65695572 27.65695572 27.65695572]
ene_coms = [0.02535017 0.02679457 0.00734349 0.00673282 0.00765091 0.00767527
 0.00693782 0.00708004 0.00722167 0.0439772 ]
ene_comp = [1.13814330e-05 1.10946441e-04 6.54068056e-06 2.68660707e-07
 9.90473593e-05 1.07646754e-05 5.34781592e-07 1.95470497e-05
 7.51407506e-06 1.89567003e-05]
ene_total = [0.79791559 0.84649139 0.23124391 0.21183382 0.24382618 0.24181527
 0.21829196 0.22336448 0.22744185 1.38419075]
optimize_network iter = 0 obj = 4.626415204282626
eta = 0.605058776174666
freqs = [33137330.89630951 71340816.3541179  25322405.07803667  8715095.95858755
 62728523.05390996 29938384.86914278 10972249.38110444 36434970.1825881
 26507600.28905275 44192105.43040895]
eta_min = 0.6050587761746664	eta_max = 0.6050587761746643
af = 0.00706069276681494	bf = 1.823614581131632	zeta = 0.007766762043496435	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02535017 0.02679457 0.00734349 0.00673282 0.00765091 0.00767527
 0.00693782 0.00708004 0.00722167 0.0439772 ]
ene_comp = [2.15904131e-06 2.10463787e-05 1.24075761e-06 5.09645459e-08
 1.87891402e-05 2.04204330e-06 1.01447292e-07 3.70804695e-06
 1.42540913e-06 3.59605852e-06]
ene_total = [3.46516685 3.66517021 1.00388091 0.92025124 1.04829737 1.04933879
 0.9482782  0.96820935 0.98725561 6.01131439]
ti_comp = [0.62556777 0.61112373 0.80563454 0.81174127 0.80256037 0.80231671
 0.80969123 0.80826907 0.80685275 0.43929743]
ti_coms = [0.25350167 0.26794571 0.07343489 0.06732817 0.07650907 0.07675273
 0.06937821 0.07080037 0.07221669 0.43977201]
t_total = [27.65695572 27.65695572 27.65695572 27.65695572 27.65695572 27.65695572
 27.65695572 27.65695572 27.65695572 27.65695572]
ene_coms = [0.02535017 0.02679457 0.00734349 0.00673282 0.00765091 0.00767527
 0.00693782 0.00708004 0.00722167 0.0439772 ]
ene_comp = [1.13814330e-05 1.10946441e-04 6.54068056e-06 2.68660707e-07
 9.90473593e-05 1.07646754e-05 5.34781592e-07 1.95470497e-05
 7.51407506e-06 1.89567003e-05]
ene_total = [0.79791559 0.84649139 0.23124391 0.21183382 0.24382618 0.24181527
 0.21829196 0.22336448 0.22744185 1.38419075]
optimize_network iter = 1 obj = 4.6264152042826066
eta = 0.6050587761746643
freqs = [33137330.89630949 71340816.35411787 25322405.0780367   8715095.95858756
 62728523.05391003 29938384.86914281 10972249.38110445 36434970.18258815
 26507600.28905278 44192105.43040884]
Done!
ene_coms = [0.02535017 0.02679457 0.00734349 0.00673282 0.00765091 0.00767527
 0.00693782 0.00708004 0.00722167 0.0439772 ]
ene_comp = [1.10686736e-05 1.07897656e-04 6.36094405e-06 2.61277968e-07
 9.63255590e-05 1.04688645e-05 5.20085908e-07 1.90099010e-05
 7.30758988e-06 1.84357742e-05]
ene_total = [0.02536124 0.02690247 0.00734985 0.00673308 0.00774723 0.00768574
 0.00693834 0.00709905 0.00722898 0.04399564]
At round 46 energy consumption: 0.14704160763812318
At round 46 eta: 0.6050587761746643
At round 46 a_n: 12.425493895665781
At round 46 local rounds: 16.45210020732877
At round 46 global rounds: 31.461628075475364
gradient difference: 0.32952016592025757
train() client id: f_00000-0-0 loss: 1.607037  [   32/  126]
train() client id: f_00000-0-1 loss: 1.117092  [   64/  126]
train() client id: f_00000-0-2 loss: 1.343354  [   96/  126]
train() client id: f_00000-1-0 loss: 1.353201  [   32/  126]
train() client id: f_00000-1-1 loss: 1.426335  [   64/  126]
train() client id: f_00000-1-2 loss: 1.013498  [   96/  126]
train() client id: f_00000-2-0 loss: 1.182173  [   32/  126]
train() client id: f_00000-2-1 loss: 1.172518  [   64/  126]
train() client id: f_00000-2-2 loss: 1.155575  [   96/  126]
train() client id: f_00000-3-0 loss: 1.034057  [   32/  126]
train() client id: f_00000-3-1 loss: 1.101807  [   64/  126]
train() client id: f_00000-3-2 loss: 1.073858  [   96/  126]
train() client id: f_00000-4-0 loss: 0.995698  [   32/  126]
train() client id: f_00000-4-1 loss: 1.033188  [   64/  126]
train() client id: f_00000-4-2 loss: 0.961606  [   96/  126]
train() client id: f_00000-5-0 loss: 0.975356  [   32/  126]
train() client id: f_00000-5-1 loss: 0.858564  [   64/  126]
train() client id: f_00000-5-2 loss: 1.099481  [   96/  126]
train() client id: f_00000-6-0 loss: 0.865406  [   32/  126]
train() client id: f_00000-6-1 loss: 0.841056  [   64/  126]
train() client id: f_00000-6-2 loss: 0.932957  [   96/  126]
train() client id: f_00000-7-0 loss: 0.857992  [   32/  126]
train() client id: f_00000-7-1 loss: 0.904312  [   64/  126]
train() client id: f_00000-7-2 loss: 0.838128  [   96/  126]
train() client id: f_00000-8-0 loss: 0.763097  [   32/  126]
train() client id: f_00000-8-1 loss: 0.788911  [   64/  126]
train() client id: f_00000-8-2 loss: 0.967561  [   96/  126]
train() client id: f_00000-9-0 loss: 0.808140  [   32/  126]
train() client id: f_00000-9-1 loss: 0.772484  [   64/  126]
train() client id: f_00000-9-2 loss: 0.831453  [   96/  126]
train() client id: f_00000-10-0 loss: 0.818336  [   32/  126]
train() client id: f_00000-10-1 loss: 0.874532  [   64/  126]
train() client id: f_00000-10-2 loss: 0.858893  [   96/  126]
train() client id: f_00000-11-0 loss: 0.854814  [   32/  126]
train() client id: f_00000-11-1 loss: 0.732449  [   64/  126]
train() client id: f_00000-11-2 loss: 0.741104  [   96/  126]
train() client id: f_00000-12-0 loss: 0.863601  [   32/  126]
train() client id: f_00000-12-1 loss: 0.877175  [   64/  126]
train() client id: f_00000-12-2 loss: 0.734064  [   96/  126]
train() client id: f_00000-13-0 loss: 0.676948  [   32/  126]
train() client id: f_00000-13-1 loss: 0.827434  [   64/  126]
train() client id: f_00000-13-2 loss: 0.917445  [   96/  126]
train() client id: f_00000-14-0 loss: 0.819867  [   32/  126]
train() client id: f_00000-14-1 loss: 0.821587  [   64/  126]
train() client id: f_00000-14-2 loss: 0.793738  [   96/  126]
train() client id: f_00000-15-0 loss: 0.826556  [   32/  126]
train() client id: f_00000-15-1 loss: 0.804626  [   64/  126]
train() client id: f_00000-15-2 loss: 0.778100  [   96/  126]
train() client id: f_00001-0-0 loss: 0.343854  [   32/  265]
train() client id: f_00001-0-1 loss: 0.383990  [   64/  265]
train() client id: f_00001-0-2 loss: 0.427232  [   96/  265]
train() client id: f_00001-0-3 loss: 0.319980  [  128/  265]
train() client id: f_00001-0-4 loss: 0.473600  [  160/  265]
train() client id: f_00001-0-5 loss: 0.488370  [  192/  265]
train() client id: f_00001-0-6 loss: 0.379967  [  224/  265]
train() client id: f_00001-0-7 loss: 0.344875  [  256/  265]
train() client id: f_00001-1-0 loss: 0.323848  [   32/  265]
train() client id: f_00001-1-1 loss: 0.415008  [   64/  265]
train() client id: f_00001-1-2 loss: 0.330761  [   96/  265]
train() client id: f_00001-1-3 loss: 0.365863  [  128/  265]
train() client id: f_00001-1-4 loss: 0.489354  [  160/  265]
train() client id: f_00001-1-5 loss: 0.351203  [  192/  265]
train() client id: f_00001-1-6 loss: 0.330001  [  224/  265]
train() client id: f_00001-1-7 loss: 0.460424  [  256/  265]
train() client id: f_00001-2-0 loss: 0.276091  [   32/  265]
train() client id: f_00001-2-1 loss: 0.367089  [   64/  265]
train() client id: f_00001-2-2 loss: 0.351344  [   96/  265]
train() client id: f_00001-2-3 loss: 0.486699  [  128/  265]
train() client id: f_00001-2-4 loss: 0.288949  [  160/  265]
train() client id: f_00001-2-5 loss: 0.446736  [  192/  265]
train() client id: f_00001-2-6 loss: 0.422013  [  224/  265]
train() client id: f_00001-2-7 loss: 0.402402  [  256/  265]
train() client id: f_00001-3-0 loss: 0.381643  [   32/  265]
train() client id: f_00001-3-1 loss: 0.322137  [   64/  265]
train() client id: f_00001-3-2 loss: 0.260934  [   96/  265]
train() client id: f_00001-3-3 loss: 0.278992  [  128/  265]
train() client id: f_00001-3-4 loss: 0.463512  [  160/  265]
train() client id: f_00001-3-5 loss: 0.388035  [  192/  265]
train() client id: f_00001-3-6 loss: 0.444440  [  224/  265]
train() client id: f_00001-3-7 loss: 0.410115  [  256/  265]
train() client id: f_00001-4-0 loss: 0.326541  [   32/  265]
train() client id: f_00001-4-1 loss: 0.355759  [   64/  265]
train() client id: f_00001-4-2 loss: 0.292644  [   96/  265]
train() client id: f_00001-4-3 loss: 0.340402  [  128/  265]
train() client id: f_00001-4-4 loss: 0.539238  [  160/  265]
train() client id: f_00001-4-5 loss: 0.427952  [  192/  265]
train() client id: f_00001-4-6 loss: 0.266797  [  224/  265]
train() client id: f_00001-4-7 loss: 0.389853  [  256/  265]
train() client id: f_00001-5-0 loss: 0.322120  [   32/  265]
train() client id: f_00001-5-1 loss: 0.465078  [   64/  265]
train() client id: f_00001-5-2 loss: 0.355731  [   96/  265]
train() client id: f_00001-5-3 loss: 0.418526  [  128/  265]
train() client id: f_00001-5-4 loss: 0.315923  [  160/  265]
train() client id: f_00001-5-5 loss: 0.319151  [  192/  265]
train() client id: f_00001-5-6 loss: 0.367827  [  224/  265]
train() client id: f_00001-5-7 loss: 0.336213  [  256/  265]
train() client id: f_00001-6-0 loss: 0.366195  [   32/  265]
train() client id: f_00001-6-1 loss: 0.299722  [   64/  265]
train() client id: f_00001-6-2 loss: 0.409939  [   96/  265]
train() client id: f_00001-6-3 loss: 0.334765  [  128/  265]
train() client id: f_00001-6-4 loss: 0.409047  [  160/  265]
train() client id: f_00001-6-5 loss: 0.346804  [  192/  265]
train() client id: f_00001-6-6 loss: 0.364639  [  224/  265]
train() client id: f_00001-6-7 loss: 0.249017  [  256/  265]
train() client id: f_00001-7-0 loss: 0.372801  [   32/  265]
train() client id: f_00001-7-1 loss: 0.370790  [   64/  265]
train() client id: f_00001-7-2 loss: 0.516920  [   96/  265]
train() client id: f_00001-7-3 loss: 0.332495  [  128/  265]
train() client id: f_00001-7-4 loss: 0.394654  [  160/  265]
train() client id: f_00001-7-5 loss: 0.244548  [  192/  265]
train() client id: f_00001-7-6 loss: 0.239617  [  224/  265]
train() client id: f_00001-7-7 loss: 0.382579  [  256/  265]
train() client id: f_00001-8-0 loss: 0.306530  [   32/  265]
train() client id: f_00001-8-1 loss: 0.395406  [   64/  265]
train() client id: f_00001-8-2 loss: 0.231783  [   96/  265]
train() client id: f_00001-8-3 loss: 0.415794  [  128/  265]
train() client id: f_00001-8-4 loss: 0.378677  [  160/  265]
train() client id: f_00001-8-5 loss: 0.345865  [  192/  265]
train() client id: f_00001-8-6 loss: 0.376865  [  224/  265]
train() client id: f_00001-8-7 loss: 0.361383  [  256/  265]
train() client id: f_00001-9-0 loss: 0.350332  [   32/  265]
train() client id: f_00001-9-1 loss: 0.387580  [   64/  265]
train() client id: f_00001-9-2 loss: 0.528006  [   96/  265]
train() client id: f_00001-9-3 loss: 0.302318  [  128/  265]
train() client id: f_00001-9-4 loss: 0.419965  [  160/  265]
train() client id: f_00001-9-5 loss: 0.259470  [  192/  265]
train() client id: f_00001-9-6 loss: 0.269926  [  224/  265]
train() client id: f_00001-9-7 loss: 0.246680  [  256/  265]
train() client id: f_00001-10-0 loss: 0.409248  [   32/  265]
train() client id: f_00001-10-1 loss: 0.260931  [   64/  265]
train() client id: f_00001-10-2 loss: 0.436782  [   96/  265]
train() client id: f_00001-10-3 loss: 0.321431  [  128/  265]
train() client id: f_00001-10-4 loss: 0.435555  [  160/  265]
train() client id: f_00001-10-5 loss: 0.233443  [  192/  265]
train() client id: f_00001-10-6 loss: 0.351132  [  224/  265]
train() client id: f_00001-10-7 loss: 0.339939  [  256/  265]
train() client id: f_00001-11-0 loss: 0.286860  [   32/  265]
train() client id: f_00001-11-1 loss: 0.528230  [   64/  265]
train() client id: f_00001-11-2 loss: 0.386218  [   96/  265]
train() client id: f_00001-11-3 loss: 0.258008  [  128/  265]
train() client id: f_00001-11-4 loss: 0.240775  [  160/  265]
train() client id: f_00001-11-5 loss: 0.402014  [  192/  265]
train() client id: f_00001-11-6 loss: 0.413642  [  224/  265]
train() client id: f_00001-11-7 loss: 0.264379  [  256/  265]
train() client id: f_00001-12-0 loss: 0.258299  [   32/  265]
train() client id: f_00001-12-1 loss: 0.482114  [   64/  265]
train() client id: f_00001-12-2 loss: 0.244827  [   96/  265]
train() client id: f_00001-12-3 loss: 0.389407  [  128/  265]
train() client id: f_00001-12-4 loss: 0.418999  [  160/  265]
train() client id: f_00001-12-5 loss: 0.351782  [  192/  265]
train() client id: f_00001-12-6 loss: 0.295352  [  224/  265]
train() client id: f_00001-12-7 loss: 0.335554  [  256/  265]
train() client id: f_00001-13-0 loss: 0.296543  [   32/  265]
train() client id: f_00001-13-1 loss: 0.344246  [   64/  265]
train() client id: f_00001-13-2 loss: 0.248434  [   96/  265]
train() client id: f_00001-13-3 loss: 0.346827  [  128/  265]
train() client id: f_00001-13-4 loss: 0.381540  [  160/  265]
train() client id: f_00001-13-5 loss: 0.413272  [  192/  265]
train() client id: f_00001-13-6 loss: 0.320515  [  224/  265]
train() client id: f_00001-13-7 loss: 0.424647  [  256/  265]
train() client id: f_00001-14-0 loss: 0.243193  [   32/  265]
train() client id: f_00001-14-1 loss: 0.491913  [   64/  265]
train() client id: f_00001-14-2 loss: 0.249927  [   96/  265]
train() client id: f_00001-14-3 loss: 0.255958  [  128/  265]
train() client id: f_00001-14-4 loss: 0.245776  [  160/  265]
train() client id: f_00001-14-5 loss: 0.298210  [  192/  265]
train() client id: f_00001-14-6 loss: 0.467305  [  224/  265]
train() client id: f_00001-14-7 loss: 0.511589  [  256/  265]
train() client id: f_00001-15-0 loss: 0.384135  [   32/  265]
train() client id: f_00001-15-1 loss: 0.347298  [   64/  265]
train() client id: f_00001-15-2 loss: 0.536742  [   96/  265]
train() client id: f_00001-15-3 loss: 0.267299  [  128/  265]
train() client id: f_00001-15-4 loss: 0.302581  [  160/  265]
train() client id: f_00001-15-5 loss: 0.327453  [  192/  265]
train() client id: f_00001-15-6 loss: 0.320058  [  224/  265]
train() client id: f_00001-15-7 loss: 0.265488  [  256/  265]
train() client id: f_00002-0-0 loss: 1.337264  [   32/  124]
train() client id: f_00002-0-1 loss: 0.997231  [   64/  124]
train() client id: f_00002-0-2 loss: 0.898488  [   96/  124]
train() client id: f_00002-1-0 loss: 1.058577  [   32/  124]
train() client id: f_00002-1-1 loss: 0.953207  [   64/  124]
train() client id: f_00002-1-2 loss: 1.093636  [   96/  124]
train() client id: f_00002-2-0 loss: 0.830697  [   32/  124]
train() client id: f_00002-2-1 loss: 1.170851  [   64/  124]
train() client id: f_00002-2-2 loss: 0.906613  [   96/  124]
train() client id: f_00002-3-0 loss: 0.869983  [   32/  124]
train() client id: f_00002-3-1 loss: 0.859419  [   64/  124]
train() client id: f_00002-3-2 loss: 0.832349  [   96/  124]
train() client id: f_00002-4-0 loss: 0.794380  [   32/  124]
train() client id: f_00002-4-1 loss: 0.939535  [   64/  124]
train() client id: f_00002-4-2 loss: 0.824799  [   96/  124]
train() client id: f_00002-5-0 loss: 1.046462  [   32/  124]
train() client id: f_00002-5-1 loss: 0.724320  [   64/  124]
train() client id: f_00002-5-2 loss: 0.876033  [   96/  124]
train() client id: f_00002-6-0 loss: 0.769101  [   32/  124]
train() client id: f_00002-6-1 loss: 0.706000  [   64/  124]
train() client id: f_00002-6-2 loss: 0.990595  [   96/  124]
train() client id: f_00002-7-0 loss: 0.925982  [   32/  124]
train() client id: f_00002-7-1 loss: 0.750172  [   64/  124]
train() client id: f_00002-7-2 loss: 0.691946  [   96/  124]
train() client id: f_00002-8-0 loss: 0.710124  [   32/  124]
train() client id: f_00002-8-1 loss: 0.717598  [   64/  124]
train() client id: f_00002-8-2 loss: 0.797679  [   96/  124]
train() client id: f_00002-9-0 loss: 0.627294  [   32/  124]
train() client id: f_00002-9-1 loss: 0.789282  [   64/  124]
train() client id: f_00002-9-2 loss: 0.815690  [   96/  124]
train() client id: f_00002-10-0 loss: 0.846811  [   32/  124]
train() client id: f_00002-10-1 loss: 0.760090  [   64/  124]
train() client id: f_00002-10-2 loss: 0.717636  [   96/  124]
train() client id: f_00002-11-0 loss: 0.670972  [   32/  124]
train() client id: f_00002-11-1 loss: 0.880141  [   64/  124]
train() client id: f_00002-11-2 loss: 0.603386  [   96/  124]
train() client id: f_00002-12-0 loss: 0.790224  [   32/  124]
train() client id: f_00002-12-1 loss: 0.783341  [   64/  124]
train() client id: f_00002-12-2 loss: 0.550542  [   96/  124]
train() client id: f_00002-13-0 loss: 0.800692  [   32/  124]
train() client id: f_00002-13-1 loss: 0.907727  [   64/  124]
train() client id: f_00002-13-2 loss: 0.611708  [   96/  124]
train() client id: f_00002-14-0 loss: 0.873934  [   32/  124]
train() client id: f_00002-14-1 loss: 0.493658  [   64/  124]
train() client id: f_00002-14-2 loss: 0.726080  [   96/  124]
train() client id: f_00002-15-0 loss: 0.754151  [   32/  124]
train() client id: f_00002-15-1 loss: 0.560128  [   64/  124]
train() client id: f_00002-15-2 loss: 0.817328  [   96/  124]
train() client id: f_00003-0-0 loss: 0.561243  [   32/   43]
train() client id: f_00003-1-0 loss: 0.781960  [   32/   43]
train() client id: f_00003-2-0 loss: 0.707822  [   32/   43]
train() client id: f_00003-3-0 loss: 0.664049  [   32/   43]
train() client id: f_00003-4-0 loss: 0.596090  [   32/   43]
train() client id: f_00003-5-0 loss: 0.608034  [   32/   43]
train() client id: f_00003-6-0 loss: 0.741483  [   32/   43]
train() client id: f_00003-7-0 loss: 0.455954  [   32/   43]
train() client id: f_00003-8-0 loss: 0.837764  [   32/   43]
train() client id: f_00003-9-0 loss: 0.652144  [   32/   43]
train() client id: f_00003-10-0 loss: 0.495312  [   32/   43]
train() client id: f_00003-11-0 loss: 0.472805  [   32/   43]
train() client id: f_00003-12-0 loss: 0.572659  [   32/   43]
train() client id: f_00003-13-0 loss: 0.523805  [   32/   43]
train() client id: f_00003-14-0 loss: 0.772279  [   32/   43]
train() client id: f_00003-15-0 loss: 0.539411  [   32/   43]
train() client id: f_00004-0-0 loss: 0.793264  [   32/  306]
train() client id: f_00004-0-1 loss: 0.842631  [   64/  306]
train() client id: f_00004-0-2 loss: 0.824003  [   96/  306]
train() client id: f_00004-0-3 loss: 0.809836  [  128/  306]
train() client id: f_00004-0-4 loss: 0.788980  [  160/  306]
train() client id: f_00004-0-5 loss: 0.818130  [  192/  306]
train() client id: f_00004-0-6 loss: 0.795623  [  224/  306]
train() client id: f_00004-0-7 loss: 0.915046  [  256/  306]
train() client id: f_00004-0-8 loss: 0.848325  [  288/  306]
train() client id: f_00004-1-0 loss: 0.838958  [   32/  306]
train() client id: f_00004-1-1 loss: 0.812464  [   64/  306]
train() client id: f_00004-1-2 loss: 0.740652  [   96/  306]
train() client id: f_00004-1-3 loss: 0.935377  [  128/  306]
train() client id: f_00004-1-4 loss: 0.694596  [  160/  306]
train() client id: f_00004-1-5 loss: 0.848526  [  192/  306]
train() client id: f_00004-1-6 loss: 0.839964  [  224/  306]
train() client id: f_00004-1-7 loss: 0.919015  [  256/  306]
train() client id: f_00004-1-8 loss: 0.758220  [  288/  306]
train() client id: f_00004-2-0 loss: 0.928700  [   32/  306]
train() client id: f_00004-2-1 loss: 0.772577  [   64/  306]
train() client id: f_00004-2-2 loss: 0.878400  [   96/  306]
train() client id: f_00004-2-3 loss: 0.753072  [  128/  306]
train() client id: f_00004-2-4 loss: 0.772975  [  160/  306]
train() client id: f_00004-2-5 loss: 0.713680  [  192/  306]
train() client id: f_00004-2-6 loss: 0.903565  [  224/  306]
train() client id: f_00004-2-7 loss: 0.868245  [  256/  306]
train() client id: f_00004-2-8 loss: 0.702914  [  288/  306]
train() client id: f_00004-3-0 loss: 0.768992  [   32/  306]
train() client id: f_00004-3-1 loss: 0.916087  [   64/  306]
train() client id: f_00004-3-2 loss: 0.626341  [   96/  306]
train() client id: f_00004-3-3 loss: 0.771156  [  128/  306]
train() client id: f_00004-3-4 loss: 0.854063  [  160/  306]
train() client id: f_00004-3-5 loss: 0.800780  [  192/  306]
train() client id: f_00004-3-6 loss: 0.836508  [  224/  306]
train() client id: f_00004-3-7 loss: 0.952562  [  256/  306]
train() client id: f_00004-3-8 loss: 0.837070  [  288/  306]
train() client id: f_00004-4-0 loss: 0.795197  [   32/  306]
train() client id: f_00004-4-1 loss: 0.897403  [   64/  306]
train() client id: f_00004-4-2 loss: 0.910218  [   96/  306]
train() client id: f_00004-4-3 loss: 0.786935  [  128/  306]
train() client id: f_00004-4-4 loss: 0.799793  [  160/  306]
train() client id: f_00004-4-5 loss: 0.873838  [  192/  306]
train() client id: f_00004-4-6 loss: 0.760330  [  224/  306]
train() client id: f_00004-4-7 loss: 0.780599  [  256/  306]
train() client id: f_00004-4-8 loss: 0.808339  [  288/  306]
train() client id: f_00004-5-0 loss: 0.880903  [   32/  306]
train() client id: f_00004-5-1 loss: 0.885904  [   64/  306]
train() client id: f_00004-5-2 loss: 0.900548  [   96/  306]
train() client id: f_00004-5-3 loss: 0.758948  [  128/  306]
train() client id: f_00004-5-4 loss: 0.873523  [  160/  306]
train() client id: f_00004-5-5 loss: 0.797946  [  192/  306]
train() client id: f_00004-5-6 loss: 0.825460  [  224/  306]
train() client id: f_00004-5-7 loss: 0.720559  [  256/  306]
train() client id: f_00004-5-8 loss: 0.789721  [  288/  306]
train() client id: f_00004-6-0 loss: 0.871000  [   32/  306]
train() client id: f_00004-6-1 loss: 0.900811  [   64/  306]
train() client id: f_00004-6-2 loss: 0.690099  [   96/  306]
train() client id: f_00004-6-3 loss: 0.758409  [  128/  306]
train() client id: f_00004-6-4 loss: 0.836440  [  160/  306]
train() client id: f_00004-6-5 loss: 0.893220  [  192/  306]
train() client id: f_00004-6-6 loss: 0.859999  [  224/  306]
train() client id: f_00004-6-7 loss: 0.739726  [  256/  306]
train() client id: f_00004-6-8 loss: 0.819554  [  288/  306]
train() client id: f_00004-7-0 loss: 0.610844  [   32/  306]
train() client id: f_00004-7-1 loss: 0.845921  [   64/  306]
train() client id: f_00004-7-2 loss: 0.838545  [   96/  306]
train() client id: f_00004-7-3 loss: 0.832438  [  128/  306]
train() client id: f_00004-7-4 loss: 0.792981  [  160/  306]
train() client id: f_00004-7-5 loss: 0.839228  [  192/  306]
train() client id: f_00004-7-6 loss: 1.003711  [  224/  306]
train() client id: f_00004-7-7 loss: 0.835991  [  256/  306]
train() client id: f_00004-7-8 loss: 0.826267  [  288/  306]
train() client id: f_00004-8-0 loss: 0.874360  [   32/  306]
train() client id: f_00004-8-1 loss: 0.924350  [   64/  306]
train() client id: f_00004-8-2 loss: 0.707715  [   96/  306]
train() client id: f_00004-8-3 loss: 0.748618  [  128/  306]
train() client id: f_00004-8-4 loss: 0.842121  [  160/  306]
train() client id: f_00004-8-5 loss: 1.033818  [  192/  306]
train() client id: f_00004-8-6 loss: 0.792364  [  224/  306]
train() client id: f_00004-8-7 loss: 0.729631  [  256/  306]
train() client id: f_00004-8-8 loss: 0.729987  [  288/  306]
train() client id: f_00004-9-0 loss: 0.808411  [   32/  306]
train() client id: f_00004-9-1 loss: 0.692999  [   64/  306]
train() client id: f_00004-9-2 loss: 0.794589  [   96/  306]
train() client id: f_00004-9-3 loss: 0.889082  [  128/  306]
train() client id: f_00004-9-4 loss: 0.913749  [  160/  306]
train() client id: f_00004-9-5 loss: 0.889099  [  192/  306]
train() client id: f_00004-9-6 loss: 0.723399  [  224/  306]
train() client id: f_00004-9-7 loss: 0.934141  [  256/  306]
train() client id: f_00004-9-8 loss: 0.810833  [  288/  306]
train() client id: f_00004-10-0 loss: 0.905915  [   32/  306]
train() client id: f_00004-10-1 loss: 0.739196  [   64/  306]
train() client id: f_00004-10-2 loss: 0.865765  [   96/  306]
train() client id: f_00004-10-3 loss: 0.818922  [  128/  306]
train() client id: f_00004-10-4 loss: 0.689192  [  160/  306]
train() client id: f_00004-10-5 loss: 0.716689  [  192/  306]
train() client id: f_00004-10-6 loss: 0.976315  [  224/  306]
train() client id: f_00004-10-7 loss: 0.979380  [  256/  306]
train() client id: f_00004-10-8 loss: 0.800969  [  288/  306]
train() client id: f_00004-11-0 loss: 0.819277  [   32/  306]
train() client id: f_00004-11-1 loss: 0.782952  [   64/  306]
train() client id: f_00004-11-2 loss: 0.809569  [   96/  306]
train() client id: f_00004-11-3 loss: 0.950841  [  128/  306]
train() client id: f_00004-11-4 loss: 0.916351  [  160/  306]
train() client id: f_00004-11-5 loss: 0.718117  [  192/  306]
train() client id: f_00004-11-6 loss: 0.775014  [  224/  306]
train() client id: f_00004-11-7 loss: 0.924748  [  256/  306]
train() client id: f_00004-11-8 loss: 0.732549  [  288/  306]
train() client id: f_00004-12-0 loss: 0.839908  [   32/  306]
train() client id: f_00004-12-1 loss: 0.833761  [   64/  306]
train() client id: f_00004-12-2 loss: 0.778264  [   96/  306]
train() client id: f_00004-12-3 loss: 0.984365  [  128/  306]
train() client id: f_00004-12-4 loss: 0.715484  [  160/  306]
train() client id: f_00004-12-5 loss: 0.829645  [  192/  306]
train() client id: f_00004-12-6 loss: 0.782581  [  224/  306]
train() client id: f_00004-12-7 loss: 0.807585  [  256/  306]
train() client id: f_00004-12-8 loss: 0.753647  [  288/  306]
train() client id: f_00004-13-0 loss: 0.926782  [   32/  306]
train() client id: f_00004-13-1 loss: 0.815795  [   64/  306]
train() client id: f_00004-13-2 loss: 0.694547  [   96/  306]
train() client id: f_00004-13-3 loss: 0.792238  [  128/  306]
train() client id: f_00004-13-4 loss: 0.819050  [  160/  306]
train() client id: f_00004-13-5 loss: 0.782164  [  192/  306]
train() client id: f_00004-13-6 loss: 0.859583  [  224/  306]
train() client id: f_00004-13-7 loss: 0.932703  [  256/  306]
train() client id: f_00004-13-8 loss: 0.887002  [  288/  306]
train() client id: f_00004-14-0 loss: 0.843496  [   32/  306]
train() client id: f_00004-14-1 loss: 0.735188  [   64/  306]
train() client id: f_00004-14-2 loss: 0.678674  [   96/  306]
train() client id: f_00004-14-3 loss: 0.774056  [  128/  306]
train() client id: f_00004-14-4 loss: 0.823687  [  160/  306]
train() client id: f_00004-14-5 loss: 0.901065  [  192/  306]
train() client id: f_00004-14-6 loss: 0.844709  [  224/  306]
train() client id: f_00004-14-7 loss: 0.893717  [  256/  306]
train() client id: f_00004-14-8 loss: 0.910582  [  288/  306]
train() client id: f_00004-15-0 loss: 0.861916  [   32/  306]
train() client id: f_00004-15-1 loss: 0.840599  [   64/  306]
train() client id: f_00004-15-2 loss: 0.856838  [   96/  306]
train() client id: f_00004-15-3 loss: 0.895429  [  128/  306]
train() client id: f_00004-15-4 loss: 0.801859  [  160/  306]
train() client id: f_00004-15-5 loss: 0.706423  [  192/  306]
train() client id: f_00004-15-6 loss: 0.783287  [  224/  306]
train() client id: f_00004-15-7 loss: 0.911229  [  256/  306]
train() client id: f_00004-15-8 loss: 0.819024  [  288/  306]
train() client id: f_00005-0-0 loss: 0.712114  [   32/  146]
train() client id: f_00005-0-1 loss: 0.519737  [   64/  146]
train() client id: f_00005-0-2 loss: 0.761528  [   96/  146]
train() client id: f_00005-0-3 loss: 0.719932  [  128/  146]
train() client id: f_00005-1-0 loss: 0.847613  [   32/  146]
train() client id: f_00005-1-1 loss: 0.607070  [   64/  146]
train() client id: f_00005-1-2 loss: 0.679888  [   96/  146]
train() client id: f_00005-1-3 loss: 0.665896  [  128/  146]
train() client id: f_00005-2-0 loss: 0.660417  [   32/  146]
train() client id: f_00005-2-1 loss: 0.639109  [   64/  146]
train() client id: f_00005-2-2 loss: 0.768265  [   96/  146]
train() client id: f_00005-2-3 loss: 0.638992  [  128/  146]
train() client id: f_00005-3-0 loss: 0.439937  [   32/  146]
train() client id: f_00005-3-1 loss: 0.619498  [   64/  146]
train() client id: f_00005-3-2 loss: 0.691397  [   96/  146]
train() client id: f_00005-3-3 loss: 1.092665  [  128/  146]
train() client id: f_00005-4-0 loss: 0.591358  [   32/  146]
train() client id: f_00005-4-1 loss: 0.641851  [   64/  146]
train() client id: f_00005-4-2 loss: 0.811787  [   96/  146]
train() client id: f_00005-4-3 loss: 0.588595  [  128/  146]
train() client id: f_00005-5-0 loss: 0.671507  [   32/  146]
train() client id: f_00005-5-1 loss: 0.685609  [   64/  146]
train() client id: f_00005-5-2 loss: 0.940664  [   96/  146]
train() client id: f_00005-5-3 loss: 0.480061  [  128/  146]
train() client id: f_00005-6-0 loss: 0.775339  [   32/  146]
train() client id: f_00005-6-1 loss: 0.620179  [   64/  146]
train() client id: f_00005-6-2 loss: 0.911156  [   96/  146]
train() client id: f_00005-6-3 loss: 0.600140  [  128/  146]
train() client id: f_00005-7-0 loss: 0.871715  [   32/  146]
train() client id: f_00005-7-1 loss: 0.475887  [   64/  146]
train() client id: f_00005-7-2 loss: 0.734315  [   96/  146]
train() client id: f_00005-7-3 loss: 0.741844  [  128/  146]
train() client id: f_00005-8-0 loss: 0.809308  [   32/  146]
train() client id: f_00005-8-1 loss: 0.506336  [   64/  146]
train() client id: f_00005-8-2 loss: 0.432992  [   96/  146]
train() client id: f_00005-8-3 loss: 0.916257  [  128/  146]
train() client id: f_00005-9-0 loss: 0.769567  [   32/  146]
train() client id: f_00005-9-1 loss: 1.084069  [   64/  146]
train() client id: f_00005-9-2 loss: 0.368007  [   96/  146]
train() client id: f_00005-9-3 loss: 0.634053  [  128/  146]
train() client id: f_00005-10-0 loss: 0.726059  [   32/  146]
train() client id: f_00005-10-1 loss: 0.740297  [   64/  146]
train() client id: f_00005-10-2 loss: 0.575002  [   96/  146]
train() client id: f_00005-10-3 loss: 0.755420  [  128/  146]
train() client id: f_00005-11-0 loss: 0.751869  [   32/  146]
train() client id: f_00005-11-1 loss: 0.700241  [   64/  146]
train() client id: f_00005-11-2 loss: 0.522080  [   96/  146]
train() client id: f_00005-11-3 loss: 0.661537  [  128/  146]
train() client id: f_00005-12-0 loss: 0.748523  [   32/  146]
train() client id: f_00005-12-1 loss: 0.770765  [   64/  146]
train() client id: f_00005-12-2 loss: 0.641988  [   96/  146]
train() client id: f_00005-12-3 loss: 0.517266  [  128/  146]
train() client id: f_00005-13-0 loss: 0.702487  [   32/  146]
train() client id: f_00005-13-1 loss: 0.515819  [   64/  146]
train() client id: f_00005-13-2 loss: 0.894845  [   96/  146]
train() client id: f_00005-13-3 loss: 0.821464  [  128/  146]
train() client id: f_00005-14-0 loss: 0.716356  [   32/  146]
train() client id: f_00005-14-1 loss: 0.604369  [   64/  146]
train() client id: f_00005-14-2 loss: 0.727544  [   96/  146]
train() client id: f_00005-14-3 loss: 0.687407  [  128/  146]
train() client id: f_00005-15-0 loss: 0.368105  [   32/  146]
train() client id: f_00005-15-1 loss: 1.030843  [   64/  146]
train() client id: f_00005-15-2 loss: 0.810701  [   96/  146]
train() client id: f_00005-15-3 loss: 0.531551  [  128/  146]
train() client id: f_00006-0-0 loss: 0.452613  [   32/   54]
train() client id: f_00006-1-0 loss: 0.446381  [   32/   54]
train() client id: f_00006-2-0 loss: 0.498429  [   32/   54]
train() client id: f_00006-3-0 loss: 0.398590  [   32/   54]
train() client id: f_00006-4-0 loss: 0.488311  [   32/   54]
train() client id: f_00006-5-0 loss: 0.477921  [   32/   54]
train() client id: f_00006-6-0 loss: 0.446785  [   32/   54]
train() client id: f_00006-7-0 loss: 0.388672  [   32/   54]
train() client id: f_00006-8-0 loss: 0.440730  [   32/   54]
train() client id: f_00006-9-0 loss: 0.505278  [   32/   54]
train() client id: f_00006-10-0 loss: 0.451721  [   32/   54]
train() client id: f_00006-11-0 loss: 0.489719  [   32/   54]
train() client id: f_00006-12-0 loss: 0.478290  [   32/   54]
train() client id: f_00006-13-0 loss: 0.492812  [   32/   54]
train() client id: f_00006-14-0 loss: 0.393354  [   32/   54]
train() client id: f_00006-15-0 loss: 0.499701  [   32/   54]
train() client id: f_00007-0-0 loss: 0.606817  [   32/  179]
train() client id: f_00007-0-1 loss: 0.590332  [   64/  179]
train() client id: f_00007-0-2 loss: 0.855234  [   96/  179]
train() client id: f_00007-0-3 loss: 0.682812  [  128/  179]
train() client id: f_00007-0-4 loss: 0.824515  [  160/  179]
train() client id: f_00007-1-0 loss: 0.950483  [   32/  179]
train() client id: f_00007-1-1 loss: 0.797162  [   64/  179]
train() client id: f_00007-1-2 loss: 0.576040  [   96/  179]
train() client id: f_00007-1-3 loss: 0.602027  [  128/  179]
train() client id: f_00007-1-4 loss: 0.563122  [  160/  179]
train() client id: f_00007-2-0 loss: 0.665024  [   32/  179]
train() client id: f_00007-2-1 loss: 0.716206  [   64/  179]
train() client id: f_00007-2-2 loss: 0.657379  [   96/  179]
train() client id: f_00007-2-3 loss: 0.710564  [  128/  179]
train() client id: f_00007-2-4 loss: 0.661593  [  160/  179]
train() client id: f_00007-3-0 loss: 0.789709  [   32/  179]
train() client id: f_00007-3-1 loss: 0.738659  [   64/  179]
train() client id: f_00007-3-2 loss: 0.726849  [   96/  179]
train() client id: f_00007-3-3 loss: 0.539860  [  128/  179]
train() client id: f_00007-3-4 loss: 0.599513  [  160/  179]
train() client id: f_00007-4-0 loss: 0.681374  [   32/  179]
train() client id: f_00007-4-1 loss: 0.539327  [   64/  179]
train() client id: f_00007-4-2 loss: 0.760266  [   96/  179]
train() client id: f_00007-4-3 loss: 0.605986  [  128/  179]
train() client id: f_00007-4-4 loss: 0.739816  [  160/  179]
train() client id: f_00007-5-0 loss: 0.680105  [   32/  179]
train() client id: f_00007-5-1 loss: 0.720105  [   64/  179]
train() client id: f_00007-5-2 loss: 0.629797  [   96/  179]
train() client id: f_00007-5-3 loss: 0.469273  [  128/  179]
train() client id: f_00007-5-4 loss: 0.746662  [  160/  179]
train() client id: f_00007-6-0 loss: 0.638377  [   32/  179]
train() client id: f_00007-6-1 loss: 0.705296  [   64/  179]
train() client id: f_00007-6-2 loss: 0.650079  [   96/  179]
train() client id: f_00007-6-3 loss: 0.655620  [  128/  179]
train() client id: f_00007-6-4 loss: 0.605827  [  160/  179]
train() client id: f_00007-7-0 loss: 0.818481  [   32/  179]
train() client id: f_00007-7-1 loss: 0.594473  [   64/  179]
train() client id: f_00007-7-2 loss: 0.536481  [   96/  179]
train() client id: f_00007-7-3 loss: 0.494391  [  128/  179]
train() client id: f_00007-7-4 loss: 0.646288  [  160/  179]
train() client id: f_00007-8-0 loss: 0.671248  [   32/  179]
train() client id: f_00007-8-1 loss: 0.726879  [   64/  179]
train() client id: f_00007-8-2 loss: 0.610916  [   96/  179]
train() client id: f_00007-8-3 loss: 0.522980  [  128/  179]
train() client id: f_00007-8-4 loss: 0.681428  [  160/  179]
train() client id: f_00007-9-0 loss: 0.632072  [   32/  179]
train() client id: f_00007-9-1 loss: 0.595985  [   64/  179]
train() client id: f_00007-9-2 loss: 0.651781  [   96/  179]
train() client id: f_00007-9-3 loss: 0.732464  [  128/  179]
train() client id: f_00007-9-4 loss: 0.701130  [  160/  179]
train() client id: f_00007-10-0 loss: 0.925953  [   32/  179]
train() client id: f_00007-10-1 loss: 0.588296  [   64/  179]
train() client id: f_00007-10-2 loss: 0.617320  [   96/  179]
train() client id: f_00007-10-3 loss: 0.604098  [  128/  179]
train() client id: f_00007-10-4 loss: 0.581353  [  160/  179]
train() client id: f_00007-11-0 loss: 0.719021  [   32/  179]
train() client id: f_00007-11-1 loss: 0.645674  [   64/  179]
train() client id: f_00007-11-2 loss: 0.608081  [   96/  179]
train() client id: f_00007-11-3 loss: 0.673763  [  128/  179]
train() client id: f_00007-11-4 loss: 0.551490  [  160/  179]
train() client id: f_00007-12-0 loss: 0.517350  [   32/  179]
train() client id: f_00007-12-1 loss: 0.830250  [   64/  179]
train() client id: f_00007-12-2 loss: 0.540182  [   96/  179]
train() client id: f_00007-12-3 loss: 0.630729  [  128/  179]
train() client id: f_00007-12-4 loss: 0.610373  [  160/  179]
train() client id: f_00007-13-0 loss: 0.568619  [   32/  179]
train() client id: f_00007-13-1 loss: 0.747377  [   64/  179]
train() client id: f_00007-13-2 loss: 0.656564  [   96/  179]
train() client id: f_00007-13-3 loss: 0.638390  [  128/  179]
train() client id: f_00007-13-4 loss: 0.690255  [  160/  179]
train() client id: f_00007-14-0 loss: 0.603505  [   32/  179]
train() client id: f_00007-14-1 loss: 0.500612  [   64/  179]
train() client id: f_00007-14-2 loss: 0.448586  [   96/  179]
train() client id: f_00007-14-3 loss: 0.816947  [  128/  179]
train() client id: f_00007-14-4 loss: 0.652646  [  160/  179]
train() client id: f_00007-15-0 loss: 0.513242  [   32/  179]
train() client id: f_00007-15-1 loss: 0.582135  [   64/  179]
train() client id: f_00007-15-2 loss: 0.762809  [   96/  179]
train() client id: f_00007-15-3 loss: 0.676732  [  128/  179]
train() client id: f_00007-15-4 loss: 0.729845  [  160/  179]
train() client id: f_00008-0-0 loss: 0.546814  [   32/  130]
train() client id: f_00008-0-1 loss: 0.661400  [   64/  130]
train() client id: f_00008-0-2 loss: 0.801600  [   96/  130]
train() client id: f_00008-0-3 loss: 0.792771  [  128/  130]
train() client id: f_00008-1-0 loss: 0.718126  [   32/  130]
train() client id: f_00008-1-1 loss: 0.658094  [   64/  130]
train() client id: f_00008-1-2 loss: 0.687531  [   96/  130]
train() client id: f_00008-1-3 loss: 0.745443  [  128/  130]
train() client id: f_00008-2-0 loss: 0.804350  [   32/  130]
train() client id: f_00008-2-1 loss: 0.657718  [   64/  130]
train() client id: f_00008-2-2 loss: 0.686975  [   96/  130]
train() client id: f_00008-2-3 loss: 0.655234  [  128/  130]
train() client id: f_00008-3-0 loss: 0.722149  [   32/  130]
train() client id: f_00008-3-1 loss: 0.655004  [   64/  130]
train() client id: f_00008-3-2 loss: 0.651662  [   96/  130]
train() client id: f_00008-3-3 loss: 0.780815  [  128/  130]
train() client id: f_00008-4-0 loss: 0.710390  [   32/  130]
train() client id: f_00008-4-1 loss: 0.730535  [   64/  130]
train() client id: f_00008-4-2 loss: 0.709669  [   96/  130]
train() client id: f_00008-4-3 loss: 0.643835  [  128/  130]
train() client id: f_00008-5-0 loss: 0.757489  [   32/  130]
train() client id: f_00008-5-1 loss: 0.616287  [   64/  130]
train() client id: f_00008-5-2 loss: 0.614594  [   96/  130]
train() client id: f_00008-5-3 loss: 0.799139  [  128/  130]
train() client id: f_00008-6-0 loss: 0.750491  [   32/  130]
train() client id: f_00008-6-1 loss: 0.713163  [   64/  130]
train() client id: f_00008-6-2 loss: 0.576338  [   96/  130]
train() client id: f_00008-6-3 loss: 0.764386  [  128/  130]
train() client id: f_00008-7-0 loss: 0.646842  [   32/  130]
train() client id: f_00008-7-1 loss: 0.718891  [   64/  130]
train() client id: f_00008-7-2 loss: 0.776615  [   96/  130]
train() client id: f_00008-7-3 loss: 0.651800  [  128/  130]
train() client id: f_00008-8-0 loss: 0.619069  [   32/  130]
train() client id: f_00008-8-1 loss: 0.693662  [   64/  130]
train() client id: f_00008-8-2 loss: 0.729467  [   96/  130]
train() client id: f_00008-8-3 loss: 0.711257  [  128/  130]
train() client id: f_00008-9-0 loss: 0.771553  [   32/  130]
train() client id: f_00008-9-1 loss: 0.607334  [   64/  130]
train() client id: f_00008-9-2 loss: 0.679154  [   96/  130]
train() client id: f_00008-9-3 loss: 0.737192  [  128/  130]
train() client id: f_00008-10-0 loss: 0.613724  [   32/  130]
train() client id: f_00008-10-1 loss: 0.741214  [   64/  130]
train() client id: f_00008-10-2 loss: 0.765989  [   96/  130]
train() client id: f_00008-10-3 loss: 0.663561  [  128/  130]
train() client id: f_00008-11-0 loss: 0.603807  [   32/  130]
train() client id: f_00008-11-1 loss: 0.776329  [   64/  130]
train() client id: f_00008-11-2 loss: 0.628620  [   96/  130]
train() client id: f_00008-11-3 loss: 0.762903  [  128/  130]
train() client id: f_00008-12-0 loss: 0.675600  [   32/  130]
train() client id: f_00008-12-1 loss: 0.717819  [   64/  130]
train() client id: f_00008-12-2 loss: 0.668504  [   96/  130]
train() client id: f_00008-12-3 loss: 0.705842  [  128/  130]
train() client id: f_00008-13-0 loss: 0.731133  [   32/  130]
train() client id: f_00008-13-1 loss: 0.691966  [   64/  130]
train() client id: f_00008-13-2 loss: 0.682480  [   96/  130]
train() client id: f_00008-13-3 loss: 0.698657  [  128/  130]
train() client id: f_00008-14-0 loss: 0.617987  [   32/  130]
train() client id: f_00008-14-1 loss: 0.672162  [   64/  130]
train() client id: f_00008-14-2 loss: 0.837627  [   96/  130]
train() client id: f_00008-14-3 loss: 0.668309  [  128/  130]
train() client id: f_00008-15-0 loss: 0.572977  [   32/  130]
train() client id: f_00008-15-1 loss: 0.719950  [   64/  130]
train() client id: f_00008-15-2 loss: 0.789191  [   96/  130]
train() client id: f_00008-15-3 loss: 0.717464  [  128/  130]
train() client id: f_00009-0-0 loss: 1.061065  [   32/  118]
train() client id: f_00009-0-1 loss: 1.106238  [   64/  118]
train() client id: f_00009-0-2 loss: 0.986864  [   96/  118]
train() client id: f_00009-1-0 loss: 1.085377  [   32/  118]
train() client id: f_00009-1-1 loss: 1.027037  [   64/  118]
train() client id: f_00009-1-2 loss: 0.846569  [   96/  118]
train() client id: f_00009-2-0 loss: 0.762418  [   32/  118]
train() client id: f_00009-2-1 loss: 1.141354  [   64/  118]
train() client id: f_00009-2-2 loss: 1.063520  [   96/  118]
train() client id: f_00009-3-0 loss: 1.080521  [   32/  118]
train() client id: f_00009-3-1 loss: 0.850910  [   64/  118]
train() client id: f_00009-3-2 loss: 0.810151  [   96/  118]
train() client id: f_00009-4-0 loss: 0.923076  [   32/  118]
train() client id: f_00009-4-1 loss: 0.829778  [   64/  118]
train() client id: f_00009-4-2 loss: 0.811846  [   96/  118]
train() client id: f_00009-5-0 loss: 0.888021  [   32/  118]
train() client id: f_00009-5-1 loss: 0.962442  [   64/  118]
train() client id: f_00009-5-2 loss: 0.790676  [   96/  118]
train() client id: f_00009-6-0 loss: 0.874668  [   32/  118]
train() client id: f_00009-6-1 loss: 0.714885  [   64/  118]
train() client id: f_00009-6-2 loss: 0.777569  [   96/  118]
train() client id: f_00009-7-0 loss: 0.894620  [   32/  118]
train() client id: f_00009-7-1 loss: 0.798099  [   64/  118]
train() client id: f_00009-7-2 loss: 0.855090  [   96/  118]
train() client id: f_00009-8-0 loss: 0.783943  [   32/  118]
train() client id: f_00009-8-1 loss: 0.942092  [   64/  118]
train() client id: f_00009-8-2 loss: 0.747402  [   96/  118]
train() client id: f_00009-9-0 loss: 0.843308  [   32/  118]
train() client id: f_00009-9-1 loss: 0.777382  [   64/  118]
train() client id: f_00009-9-2 loss: 0.735978  [   96/  118]
train() client id: f_00009-10-0 loss: 0.752884  [   32/  118]
train() client id: f_00009-10-1 loss: 0.758193  [   64/  118]
train() client id: f_00009-10-2 loss: 0.709980  [   96/  118]
train() client id: f_00009-11-0 loss: 0.721617  [   32/  118]
train() client id: f_00009-11-1 loss: 0.696468  [   64/  118]
train() client id: f_00009-11-2 loss: 0.915435  [   96/  118]
train() client id: f_00009-12-0 loss: 0.845710  [   32/  118]
train() client id: f_00009-12-1 loss: 0.884961  [   64/  118]
train() client id: f_00009-12-2 loss: 0.640319  [   96/  118]
train() client id: f_00009-13-0 loss: 0.863423  [   32/  118]
train() client id: f_00009-13-1 loss: 0.811421  [   64/  118]
train() client id: f_00009-13-2 loss: 0.717250  [   96/  118]
train() client id: f_00009-14-0 loss: 0.653399  [   32/  118]
train() client id: f_00009-14-1 loss: 0.690226  [   64/  118]
train() client id: f_00009-14-2 loss: 0.825949  [   96/  118]
train() client id: f_00009-15-0 loss: 0.706797  [   32/  118]
train() client id: f_00009-15-1 loss: 0.702248  [   64/  118]
train() client id: f_00009-15-2 loss: 0.953722  [   96/  118]
At round 46 accuracy: 0.6445623342175066
At round 46 training accuracy: 0.5935613682092555
At round 46 training loss: 0.8233546606932319
update_location
xs = 8.927491 351.223621 5.882650 0.934260 -267.581990 -115.230757 -75.849135 -5.143845 -290.120581 20.134486 
ys = -342.390647 7.291448 240.684448 -62.290817 -9.642386 0.794442 -1.381692 236.628436 25.881276 -777.232496 
xs mean: -36.68237997052122
ys mean: -68.16579882624052
dists_uav = 356.806748 365.254975 260.698311 117.817735 285.820043 152.573781 125.518924 256.942554 307.960699 783.897793 
uav_gains = -119.533153 -119.951859 -112.230351 -101.780432 -114.503310 -104.592692 -102.468132 -111.897279 -116.377075 -129.322012 
uav_gains_db_mean: -113.26562948978278
dists_bs = 549.110557 552.301962 192.439548 295.397691 206.553227 184.173665 202.339802 180.690834 188.385620 972.020676 
bs_gains = -116.277533 -116.348004 -103.527300 -108.738458 -104.387955 -102.993430 -104.137336 -102.761271 -103.268396 -123.221973 
bs_gains_db_mean: -108.56616558901358
Round 47
-------------------------------
ene_coms = [0.02616935 0.02762267 0.00738066 0.00680482 0.00770091 0.00778113
 0.00702064 0.00711678 0.00728935 0.04440971]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.9573889  10.0916418   4.65445405  1.66585958 11.35768469  5.46914998
  2.07339783  6.67575386  4.87419647  4.88553678]
obj_prev = 56.70506394737483
eta_min = 7.284969441585803e-20	eta_max = 0.8037749365451154
af = 11.79849495615646	bf = 1.803936096278495	zeta = 12.978344451772108	eta = 0.9090909090909091
af = 11.79849495615646	bf = 1.803936096278495	zeta = 32.21298049167206	eta = 0.36626523767978236
af = 11.79849495615646	bf = 1.803936096278495	zeta = 21.545788430736753	eta = 0.5476009844840481
af = 11.79849495615646	bf = 1.803936096278495	zeta = 19.69298874707325	eta = 0.5991216014841799
af = 11.79849495615646	bf = 1.803936096278495	zeta = 19.57749605463623	eta = 0.6026559741465205
af = 11.79849495615646	bf = 1.803936096278495	zeta = 19.576983408331436	eta = 0.6026717554010563
af = 11.79849495615646	bf = 1.803936096278495	zeta = 19.576983398150773	eta = 0.6026717557144651
eta = 0.6026717557144651
ene_coms = [0.02616935 0.02762267 0.00738066 0.00680482 0.00770091 0.00778113
 0.00702064 0.00711678 0.00728935 0.04440971]
ene_comp = [0.04178548 0.08788215 0.04112222 0.01426012 0.10147902 0.04841809
 0.01790806 0.05936191 0.043112   0.03913243]
ene_total = [2.06653973 3.51255876 1.47499627 0.64059519 3.32021556 1.70904611
 0.75809421 2.02164986 1.53273006 2.54055765]
ti_comp = [0.64608594 0.63155269 0.83397285 0.83973125 0.83077027 0.8299681
 0.83757298 0.83661156 0.83488587 0.46368234]
ti_coms = [0.26169347 0.27622671 0.07380656 0.06804816 0.07700914 0.07781131
 0.07020643 0.07116785 0.07289354 0.44409707]
t_total = [27.60601997 27.60601997 27.60601997 27.60601997 27.60601997 27.60601997
 27.60601997 27.60601997 27.60601997 27.60601997]
ene_coms = [0.02616935 0.02762267 0.00738066 0.00680482 0.00770091 0.00778113
 0.00702064 0.00711678 0.00728935 0.04440971]
ene_comp = [1.09238449e-05 1.06356224e-04 6.24893067e-06 2.57021354e-07
 9.46339268e-05 1.02986485e-05 5.11658786e-07 1.86790956e-05
 7.18489307e-06 1.74200496e-05]
ene_total = [0.79615495 0.84325342 0.22463943 0.20694562 0.23706646 0.23694121
 0.21351678 0.21699298 0.22189135 1.35105087]
optimize_network iter = 0 obj = 4.548453069517167
eta = 0.6026717557144651
freqs = [32337398.28574765 69576263.91770394 24654409.0273699   8490885.06521707
 61075257.08025264 29168646.42527911 10690448.47195778 35477580.38508608
 25819097.9208985  42197456.63974381]
eta_min = 0.6026717557144661	eta_max = 0.6026717557144636
af = 0.006502546022702353	bf = 1.803936096278495	zeta = 0.007152800624972588	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02616935 0.02762267 0.00738066 0.00680482 0.00770091 0.00778113
 0.00702064 0.00711678 0.00728935 0.04440971]
ene_comp = [2.05606128e-06 2.00181270e-05 1.17615954e-06 4.83759757e-08
 1.78117828e-05 1.93838823e-06 9.63032549e-08 3.51573695e-06
 1.35232426e-06 3.27876217e-06]
ene_total = [3.47850469 3.67405701 0.98113726 0.90445102 1.0259146  1.03446659
 0.93314351 0.94637645 0.96902548 5.9030378 ]
ti_comp = [0.64608594 0.63155269 0.83397285 0.83973125 0.83077027 0.8299681
 0.83757298 0.83661156 0.83488587 0.46368234]
ti_coms = [0.26169347 0.27622671 0.07380656 0.06804816 0.07700914 0.07781131
 0.07020643 0.07116785 0.07289354 0.44409707]
t_total = [27.60601997 27.60601997 27.60601997 27.60601997 27.60601997 27.60601997
 27.60601997 27.60601997 27.60601997 27.60601997]
ene_coms = [0.02616935 0.02762267 0.00738066 0.00680482 0.00770091 0.00778113
 0.00702064 0.00711678 0.00728935 0.04440971]
ene_comp = [1.09238449e-05 1.06356224e-04 6.24893067e-06 2.57021354e-07
 9.46339268e-05 1.02986485e-05 5.11658786e-07 1.86790956e-05
 7.18489307e-06 1.74200496e-05]
ene_total = [0.79615495 0.84325342 0.22463943 0.20694562 0.23706646 0.23694121
 0.21351678 0.21699298 0.22189135 1.35105087]
optimize_network iter = 1 obj = 4.5484530695171514
eta = 0.6026717557144636
freqs = [32337398.28574765 69576263.91770391 24654409.02736992  8490885.06521708
 61075257.08025268 29168646.42527914 10690448.47195779 35477580.38508611
 25819097.92089853 42197456.63974372]
Done!
ene_coms = [0.02616935 0.02762267 0.00738066 0.00680482 0.00770091 0.00778113
 0.00702064 0.00711678 0.00728935 0.04440971]
ene_comp = [1.05407299e-05 1.02626158e-04 6.02977165e-06 2.48007244e-07
 9.13149784e-05 9.93746003e-06 4.93714174e-07 1.80239927e-05
 6.93290850e-06 1.68091033e-05]
ene_total = [0.02617989 0.0277253  0.00738669 0.00680506 0.00779223 0.00779107
 0.00702114 0.00713481 0.00729629 0.04442652]
At round 47 energy consumption: 0.14955898036104803
At round 47 eta: 0.6026717557144636
At round 47 a_n: 12.082948048696464
At round 47 local rounds: 16.581538483665245
At round 47 global rounds: 30.410493647195043
gradient difference: 0.31829318404197693
train() client id: f_00000-0-0 loss: 1.309989  [   32/  126]
train() client id: f_00000-0-1 loss: 1.330716  [   64/  126]
train() client id: f_00000-0-2 loss: 1.067017  [   96/  126]
train() client id: f_00000-1-0 loss: 1.392242  [   32/  126]
train() client id: f_00000-1-1 loss: 1.138946  [   64/  126]
train() client id: f_00000-1-2 loss: 0.986483  [   96/  126]
train() client id: f_00000-2-0 loss: 1.094220  [   32/  126]
train() client id: f_00000-2-1 loss: 0.997535  [   64/  126]
train() client id: f_00000-2-2 loss: 1.026480  [   96/  126]
train() client id: f_00000-3-0 loss: 0.981936  [   32/  126]
train() client id: f_00000-3-1 loss: 1.110161  [   64/  126]
train() client id: f_00000-3-2 loss: 0.821492  [   96/  126]
train() client id: f_00000-4-0 loss: 0.904251  [   32/  126]
train() client id: f_00000-4-1 loss: 0.975823  [   64/  126]
train() client id: f_00000-4-2 loss: 0.977121  [   96/  126]
train() client id: f_00000-5-0 loss: 0.933570  [   32/  126]
train() client id: f_00000-5-1 loss: 0.742401  [   64/  126]
train() client id: f_00000-5-2 loss: 0.988334  [   96/  126]
train() client id: f_00000-6-0 loss: 0.813383  [   32/  126]
train() client id: f_00000-6-1 loss: 0.973293  [   64/  126]
train() client id: f_00000-6-2 loss: 0.824829  [   96/  126]
train() client id: f_00000-7-0 loss: 0.675458  [   32/  126]
train() client id: f_00000-7-1 loss: 0.989452  [   64/  126]
train() client id: f_00000-7-2 loss: 0.824874  [   96/  126]
train() client id: f_00000-8-0 loss: 0.890535  [   32/  126]
train() client id: f_00000-8-1 loss: 0.832117  [   64/  126]
train() client id: f_00000-8-2 loss: 0.742316  [   96/  126]
train() client id: f_00000-9-0 loss: 0.710412  [   32/  126]
train() client id: f_00000-9-1 loss: 0.869064  [   64/  126]
train() client id: f_00000-9-2 loss: 0.879882  [   96/  126]
train() client id: f_00000-10-0 loss: 0.889962  [   32/  126]
train() client id: f_00000-10-1 loss: 0.670985  [   64/  126]
train() client id: f_00000-10-2 loss: 0.829291  [   96/  126]
train() client id: f_00000-11-0 loss: 0.760518  [   32/  126]
train() client id: f_00000-11-1 loss: 0.812445  [   64/  126]
train() client id: f_00000-11-2 loss: 0.847850  [   96/  126]
train() client id: f_00000-12-0 loss: 0.752135  [   32/  126]
train() client id: f_00000-12-1 loss: 0.841071  [   64/  126]
train() client id: f_00000-12-2 loss: 0.663369  [   96/  126]
train() client id: f_00000-13-0 loss: 0.811392  [   32/  126]
train() client id: f_00000-13-1 loss: 0.687015  [   64/  126]
train() client id: f_00000-13-2 loss: 0.805196  [   96/  126]
train() client id: f_00000-14-0 loss: 0.768187  [   32/  126]
train() client id: f_00000-14-1 loss: 0.682321  [   64/  126]
train() client id: f_00000-14-2 loss: 0.867811  [   96/  126]
train() client id: f_00000-15-0 loss: 0.630456  [   32/  126]
train() client id: f_00000-15-1 loss: 0.887291  [   64/  126]
train() client id: f_00000-15-2 loss: 0.763981  [   96/  126]
train() client id: f_00001-0-0 loss: 0.494276  [   32/  265]
train() client id: f_00001-0-1 loss: 0.557442  [   64/  265]
train() client id: f_00001-0-2 loss: 0.512739  [   96/  265]
train() client id: f_00001-0-3 loss: 0.405858  [  128/  265]
train() client id: f_00001-0-4 loss: 0.547894  [  160/  265]
train() client id: f_00001-0-5 loss: 0.483213  [  192/  265]
train() client id: f_00001-0-6 loss: 0.462527  [  224/  265]
train() client id: f_00001-0-7 loss: 0.483989  [  256/  265]
train() client id: f_00001-1-0 loss: 0.448670  [   32/  265]
train() client id: f_00001-1-1 loss: 0.493952  [   64/  265]
train() client id: f_00001-1-2 loss: 0.407950  [   96/  265]
train() client id: f_00001-1-3 loss: 0.540087  [  128/  265]
train() client id: f_00001-1-4 loss: 0.595052  [  160/  265]
train() client id: f_00001-1-5 loss: 0.395371  [  192/  265]
train() client id: f_00001-1-6 loss: 0.604449  [  224/  265]
train() client id: f_00001-1-7 loss: 0.459551  [  256/  265]
train() client id: f_00001-2-0 loss: 0.606570  [   32/  265]
train() client id: f_00001-2-1 loss: 0.428087  [   64/  265]
train() client id: f_00001-2-2 loss: 0.513305  [   96/  265]
train() client id: f_00001-2-3 loss: 0.586733  [  128/  265]
train() client id: f_00001-2-4 loss: 0.484183  [  160/  265]
train() client id: f_00001-2-5 loss: 0.408714  [  192/  265]
train() client id: f_00001-2-6 loss: 0.384097  [  224/  265]
train() client id: f_00001-2-7 loss: 0.483268  [  256/  265]
train() client id: f_00001-3-0 loss: 0.519775  [   32/  265]
train() client id: f_00001-3-1 loss: 0.379875  [   64/  265]
train() client id: f_00001-3-2 loss: 0.446177  [   96/  265]
train() client id: f_00001-3-3 loss: 0.529176  [  128/  265]
train() client id: f_00001-3-4 loss: 0.526541  [  160/  265]
train() client id: f_00001-3-5 loss: 0.649290  [  192/  265]
train() client id: f_00001-3-6 loss: 0.393501  [  224/  265]
train() client id: f_00001-3-7 loss: 0.429175  [  256/  265]
train() client id: f_00001-4-0 loss: 0.484797  [   32/  265]
train() client id: f_00001-4-1 loss: 0.512946  [   64/  265]
train() client id: f_00001-4-2 loss: 0.507885  [   96/  265]
train() client id: f_00001-4-3 loss: 0.563224  [  128/  265]
train() client id: f_00001-4-4 loss: 0.483041  [  160/  265]
train() client id: f_00001-4-5 loss: 0.417367  [  192/  265]
train() client id: f_00001-4-6 loss: 0.424056  [  224/  265]
train() client id: f_00001-4-7 loss: 0.414042  [  256/  265]
train() client id: f_00001-5-0 loss: 0.445574  [   32/  265]
train() client id: f_00001-5-1 loss: 0.485273  [   64/  265]
train() client id: f_00001-5-2 loss: 0.454828  [   96/  265]
train() client id: f_00001-5-3 loss: 0.423279  [  128/  265]
train() client id: f_00001-5-4 loss: 0.580769  [  160/  265]
train() client id: f_00001-5-5 loss: 0.445533  [  192/  265]
train() client id: f_00001-5-6 loss: 0.527784  [  224/  265]
train() client id: f_00001-5-7 loss: 0.409496  [  256/  265]
train() client id: f_00001-6-0 loss: 0.467374  [   32/  265]
train() client id: f_00001-6-1 loss: 0.411313  [   64/  265]
train() client id: f_00001-6-2 loss: 0.435740  [   96/  265]
train() client id: f_00001-6-3 loss: 0.594336  [  128/  265]
train() client id: f_00001-6-4 loss: 0.387537  [  160/  265]
train() client id: f_00001-6-5 loss: 0.523934  [  192/  265]
train() client id: f_00001-6-6 loss: 0.474159  [  224/  265]
train() client id: f_00001-6-7 loss: 0.534672  [  256/  265]
train() client id: f_00001-7-0 loss: 0.394768  [   32/  265]
train() client id: f_00001-7-1 loss: 0.497553  [   64/  265]
train() client id: f_00001-7-2 loss: 0.498752  [   96/  265]
train() client id: f_00001-7-3 loss: 0.521404  [  128/  265]
train() client id: f_00001-7-4 loss: 0.485899  [  160/  265]
train() client id: f_00001-7-5 loss: 0.550814  [  192/  265]
train() client id: f_00001-7-6 loss: 0.476272  [  224/  265]
train() client id: f_00001-7-7 loss: 0.385838  [  256/  265]
train() client id: f_00001-8-0 loss: 0.429994  [   32/  265]
train() client id: f_00001-8-1 loss: 0.466804  [   64/  265]
train() client id: f_00001-8-2 loss: 0.695514  [   96/  265]
train() client id: f_00001-8-3 loss: 0.432518  [  128/  265]
train() client id: f_00001-8-4 loss: 0.377330  [  160/  265]
train() client id: f_00001-8-5 loss: 0.449175  [  192/  265]
train() client id: f_00001-8-6 loss: 0.585924  [  224/  265]
train() client id: f_00001-8-7 loss: 0.382005  [  256/  265]
train() client id: f_00001-9-0 loss: 0.494793  [   32/  265]
train() client id: f_00001-9-1 loss: 0.437500  [   64/  265]
train() client id: f_00001-9-2 loss: 0.422745  [   96/  265]
train() client id: f_00001-9-3 loss: 0.462283  [  128/  265]
train() client id: f_00001-9-4 loss: 0.541721  [  160/  265]
train() client id: f_00001-9-5 loss: 0.531603  [  192/  265]
train() client id: f_00001-9-6 loss: 0.472049  [  224/  265]
train() client id: f_00001-9-7 loss: 0.361125  [  256/  265]
train() client id: f_00001-10-0 loss: 0.378498  [   32/  265]
train() client id: f_00001-10-1 loss: 0.421869  [   64/  265]
train() client id: f_00001-10-2 loss: 0.666652  [   96/  265]
train() client id: f_00001-10-3 loss: 0.557266  [  128/  265]
train() client id: f_00001-10-4 loss: 0.377805  [  160/  265]
train() client id: f_00001-10-5 loss: 0.379524  [  192/  265]
train() client id: f_00001-10-6 loss: 0.546308  [  224/  265]
train() client id: f_00001-10-7 loss: 0.496772  [  256/  265]
train() client id: f_00001-11-0 loss: 0.407883  [   32/  265]
train() client id: f_00001-11-1 loss: 0.439331  [   64/  265]
train() client id: f_00001-11-2 loss: 0.404798  [   96/  265]
train() client id: f_00001-11-3 loss: 0.538667  [  128/  265]
train() client id: f_00001-11-4 loss: 0.366648  [  160/  265]
train() client id: f_00001-11-5 loss: 0.379275  [  192/  265]
train() client id: f_00001-11-6 loss: 0.531157  [  224/  265]
train() client id: f_00001-11-7 loss: 0.637038  [  256/  265]
train() client id: f_00001-12-0 loss: 0.375712  [   32/  265]
train() client id: f_00001-12-1 loss: 0.432851  [   64/  265]
train() client id: f_00001-12-2 loss: 0.482858  [   96/  265]
train() client id: f_00001-12-3 loss: 0.397981  [  128/  265]
train() client id: f_00001-12-4 loss: 0.397691  [  160/  265]
train() client id: f_00001-12-5 loss: 0.730481  [  192/  265]
train() client id: f_00001-12-6 loss: 0.447882  [  224/  265]
train() client id: f_00001-12-7 loss: 0.571061  [  256/  265]
train() client id: f_00001-13-0 loss: 0.557926  [   32/  265]
train() client id: f_00001-13-1 loss: 0.396037  [   64/  265]
train() client id: f_00001-13-2 loss: 0.469537  [   96/  265]
train() client id: f_00001-13-3 loss: 0.487491  [  128/  265]
train() client id: f_00001-13-4 loss: 0.404359  [  160/  265]
train() client id: f_00001-13-5 loss: 0.442520  [  192/  265]
train() client id: f_00001-13-6 loss: 0.511909  [  224/  265]
train() client id: f_00001-13-7 loss: 0.560180  [  256/  265]
train() client id: f_00001-14-0 loss: 0.486849  [   32/  265]
train() client id: f_00001-14-1 loss: 0.490116  [   64/  265]
train() client id: f_00001-14-2 loss: 0.521617  [   96/  265]
train() client id: f_00001-14-3 loss: 0.401980  [  128/  265]
train() client id: f_00001-14-4 loss: 0.503539  [  160/  265]
train() client id: f_00001-14-5 loss: 0.534249  [  192/  265]
train() client id: f_00001-14-6 loss: 0.451828  [  224/  265]
train() client id: f_00001-14-7 loss: 0.443636  [  256/  265]
train() client id: f_00001-15-0 loss: 0.512459  [   32/  265]
train() client id: f_00001-15-1 loss: 0.408480  [   64/  265]
train() client id: f_00001-15-2 loss: 0.512504  [   96/  265]
train() client id: f_00001-15-3 loss: 0.542961  [  128/  265]
train() client id: f_00001-15-4 loss: 0.510012  [  160/  265]
train() client id: f_00001-15-5 loss: 0.386107  [  192/  265]
train() client id: f_00001-15-6 loss: 0.443119  [  224/  265]
train() client id: f_00001-15-7 loss: 0.515589  [  256/  265]
train() client id: f_00002-0-0 loss: 1.130331  [   32/  124]
train() client id: f_00002-0-1 loss: 1.244596  [   64/  124]
train() client id: f_00002-0-2 loss: 0.948614  [   96/  124]
train() client id: f_00002-1-0 loss: 1.148447  [   32/  124]
train() client id: f_00002-1-1 loss: 1.042622  [   64/  124]
train() client id: f_00002-1-2 loss: 0.951434  [   96/  124]
train() client id: f_00002-2-0 loss: 1.056170  [   32/  124]
train() client id: f_00002-2-1 loss: 0.787740  [   64/  124]
train() client id: f_00002-2-2 loss: 1.048040  [   96/  124]
train() client id: f_00002-3-0 loss: 0.875615  [   32/  124]
train() client id: f_00002-3-1 loss: 0.973278  [   64/  124]
train() client id: f_00002-3-2 loss: 1.055991  [   96/  124]
train() client id: f_00002-4-0 loss: 0.858492  [   32/  124]
train() client id: f_00002-4-1 loss: 1.012155  [   64/  124]
train() client id: f_00002-4-2 loss: 1.027527  [   96/  124]
train() client id: f_00002-5-0 loss: 0.863265  [   32/  124]
train() client id: f_00002-5-1 loss: 1.018926  [   64/  124]
train() client id: f_00002-5-2 loss: 0.930766  [   96/  124]
train() client id: f_00002-6-0 loss: 1.044768  [   32/  124]
train() client id: f_00002-6-1 loss: 0.873114  [   64/  124]
train() client id: f_00002-6-2 loss: 0.917586  [   96/  124]
train() client id: f_00002-7-0 loss: 0.785804  [   32/  124]
train() client id: f_00002-7-1 loss: 0.845779  [   64/  124]
train() client id: f_00002-7-2 loss: 0.934184  [   96/  124]
train() client id: f_00002-8-0 loss: 0.801079  [   32/  124]
train() client id: f_00002-8-1 loss: 1.040284  [   64/  124]
train() client id: f_00002-8-2 loss: 0.900310  [   96/  124]
train() client id: f_00002-9-0 loss: 0.945387  [   32/  124]
train() client id: f_00002-9-1 loss: 0.795516  [   64/  124]
train() client id: f_00002-9-2 loss: 0.854481  [   96/  124]
train() client id: f_00002-10-0 loss: 0.658146  [   32/  124]
train() client id: f_00002-10-1 loss: 0.941841  [   64/  124]
train() client id: f_00002-10-2 loss: 0.822506  [   96/  124]
train() client id: f_00002-11-0 loss: 0.784168  [   32/  124]
train() client id: f_00002-11-1 loss: 0.816700  [   64/  124]
train() client id: f_00002-11-2 loss: 0.774740  [   96/  124]
train() client id: f_00002-12-0 loss: 0.844836  [   32/  124]
train() client id: f_00002-12-1 loss: 0.861684  [   64/  124]
train() client id: f_00002-12-2 loss: 0.820178  [   96/  124]
train() client id: f_00002-13-0 loss: 0.855962  [   32/  124]
train() client id: f_00002-13-1 loss: 0.924512  [   64/  124]
train() client id: f_00002-13-2 loss: 0.764508  [   96/  124]
train() client id: f_00002-14-0 loss: 0.755634  [   32/  124]
train() client id: f_00002-14-1 loss: 0.964954  [   64/  124]
train() client id: f_00002-14-2 loss: 0.675694  [   96/  124]
train() client id: f_00002-15-0 loss: 0.857719  [   32/  124]
train() client id: f_00002-15-1 loss: 0.812543  [   64/  124]
train() client id: f_00002-15-2 loss: 0.852307  [   96/  124]
train() client id: f_00003-0-0 loss: 0.895083  [   32/   43]
train() client id: f_00003-1-0 loss: 0.699221  [   32/   43]
train() client id: f_00003-2-0 loss: 0.734509  [   32/   43]
train() client id: f_00003-3-0 loss: 0.940579  [   32/   43]
train() client id: f_00003-4-0 loss: 0.676714  [   32/   43]
train() client id: f_00003-5-0 loss: 0.796397  [   32/   43]
train() client id: f_00003-6-0 loss: 0.691145  [   32/   43]
train() client id: f_00003-7-0 loss: 0.713518  [   32/   43]
train() client id: f_00003-8-0 loss: 0.714180  [   32/   43]
train() client id: f_00003-9-0 loss: 0.506989  [   32/   43]
train() client id: f_00003-10-0 loss: 0.694054  [   32/   43]
train() client id: f_00003-11-0 loss: 0.723317  [   32/   43]
train() client id: f_00003-12-0 loss: 0.893171  [   32/   43]
train() client id: f_00003-13-0 loss: 0.574814  [   32/   43]
train() client id: f_00003-14-0 loss: 0.820957  [   32/   43]
train() client id: f_00003-15-0 loss: 0.823636  [   32/   43]
train() client id: f_00004-0-0 loss: 0.772002  [   32/  306]
train() client id: f_00004-0-1 loss: 0.872192  [   64/  306]
train() client id: f_00004-0-2 loss: 0.740378  [   96/  306]
train() client id: f_00004-0-3 loss: 0.871752  [  128/  306]
train() client id: f_00004-0-4 loss: 0.865726  [  160/  306]
train() client id: f_00004-0-5 loss: 0.831316  [  192/  306]
train() client id: f_00004-0-6 loss: 0.931205  [  224/  306]
train() client id: f_00004-0-7 loss: 0.894724  [  256/  306]
train() client id: f_00004-0-8 loss: 0.810068  [  288/  306]
train() client id: f_00004-1-0 loss: 0.594659  [   32/  306]
train() client id: f_00004-1-1 loss: 0.883327  [   64/  306]
train() client id: f_00004-1-2 loss: 0.969361  [   96/  306]
train() client id: f_00004-1-3 loss: 0.735169  [  128/  306]
train() client id: f_00004-1-4 loss: 0.917266  [  160/  306]
train() client id: f_00004-1-5 loss: 0.975693  [  192/  306]
train() client id: f_00004-1-6 loss: 0.857162  [  224/  306]
train() client id: f_00004-1-7 loss: 0.847327  [  256/  306]
train() client id: f_00004-1-8 loss: 0.770040  [  288/  306]
train() client id: f_00004-2-0 loss: 0.834218  [   32/  306]
train() client id: f_00004-2-1 loss: 0.943444  [   64/  306]
train() client id: f_00004-2-2 loss: 0.903560  [   96/  306]
train() client id: f_00004-2-3 loss: 0.750898  [  128/  306]
train() client id: f_00004-2-4 loss: 0.727889  [  160/  306]
train() client id: f_00004-2-5 loss: 0.824250  [  192/  306]
train() client id: f_00004-2-6 loss: 0.845506  [  224/  306]
train() client id: f_00004-2-7 loss: 0.887240  [  256/  306]
train() client id: f_00004-2-8 loss: 0.899411  [  288/  306]
train() client id: f_00004-3-0 loss: 0.848776  [   32/  306]
train() client id: f_00004-3-1 loss: 1.002845  [   64/  306]
train() client id: f_00004-3-2 loss: 0.746877  [   96/  306]
train() client id: f_00004-3-3 loss: 0.918651  [  128/  306]
train() client id: f_00004-3-4 loss: 0.879418  [  160/  306]
train() client id: f_00004-3-5 loss: 0.868746  [  192/  306]
train() client id: f_00004-3-6 loss: 0.811852  [  224/  306]
train() client id: f_00004-3-7 loss: 0.731136  [  256/  306]
train() client id: f_00004-3-8 loss: 0.733928  [  288/  306]
train() client id: f_00004-4-0 loss: 0.930491  [   32/  306]
train() client id: f_00004-4-1 loss: 0.775665  [   64/  306]
train() client id: f_00004-4-2 loss: 0.785375  [   96/  306]
train() client id: f_00004-4-3 loss: 0.895027  [  128/  306]
train() client id: f_00004-4-4 loss: 0.860868  [  160/  306]
train() client id: f_00004-4-5 loss: 0.802646  [  192/  306]
train() client id: f_00004-4-6 loss: 0.899581  [  224/  306]
train() client id: f_00004-4-7 loss: 0.821686  [  256/  306]
train() client id: f_00004-4-8 loss: 0.789673  [  288/  306]
train() client id: f_00004-5-0 loss: 0.939396  [   32/  306]
train() client id: f_00004-5-1 loss: 0.793478  [   64/  306]
train() client id: f_00004-5-2 loss: 0.786258  [   96/  306]
train() client id: f_00004-5-3 loss: 0.819324  [  128/  306]
train() client id: f_00004-5-4 loss: 0.822504  [  160/  306]
train() client id: f_00004-5-5 loss: 0.847214  [  192/  306]
train() client id: f_00004-5-6 loss: 0.808924  [  224/  306]
train() client id: f_00004-5-7 loss: 0.893837  [  256/  306]
train() client id: f_00004-5-8 loss: 0.771823  [  288/  306]
train() client id: f_00004-6-0 loss: 0.825898  [   32/  306]
train() client id: f_00004-6-1 loss: 0.808665  [   64/  306]
train() client id: f_00004-6-2 loss: 0.708289  [   96/  306]
train() client id: f_00004-6-3 loss: 0.916404  [  128/  306]
train() client id: f_00004-6-4 loss: 0.812892  [  160/  306]
train() client id: f_00004-6-5 loss: 0.842705  [  192/  306]
train() client id: f_00004-6-6 loss: 0.802517  [  224/  306]
train() client id: f_00004-6-7 loss: 0.937103  [  256/  306]
train() client id: f_00004-6-8 loss: 0.793718  [  288/  306]
train() client id: f_00004-7-0 loss: 0.712246  [   32/  306]
train() client id: f_00004-7-1 loss: 0.815732  [   64/  306]
train() client id: f_00004-7-2 loss: 0.786807  [   96/  306]
train() client id: f_00004-7-3 loss: 0.971249  [  128/  306]
train() client id: f_00004-7-4 loss: 0.681436  [  160/  306]
train() client id: f_00004-7-5 loss: 0.836633  [  192/  306]
train() client id: f_00004-7-6 loss: 0.775922  [  224/  306]
train() client id: f_00004-7-7 loss: 0.844625  [  256/  306]
train() client id: f_00004-7-8 loss: 0.884107  [  288/  306]
train() client id: f_00004-8-0 loss: 0.789884  [   32/  306]
train() client id: f_00004-8-1 loss: 0.825675  [   64/  306]
train() client id: f_00004-8-2 loss: 0.878325  [   96/  306]
train() client id: f_00004-8-3 loss: 0.827696  [  128/  306]
train() client id: f_00004-8-4 loss: 0.848917  [  160/  306]
train() client id: f_00004-8-5 loss: 0.837552  [  192/  306]
train() client id: f_00004-8-6 loss: 0.743279  [  224/  306]
train() client id: f_00004-8-7 loss: 0.745510  [  256/  306]
train() client id: f_00004-8-8 loss: 0.825667  [  288/  306]
train() client id: f_00004-9-0 loss: 0.881562  [   32/  306]
train() client id: f_00004-9-1 loss: 0.778118  [   64/  306]
train() client id: f_00004-9-2 loss: 0.900322  [   96/  306]
train() client id: f_00004-9-3 loss: 0.701594  [  128/  306]
train() client id: f_00004-9-4 loss: 0.822590  [  160/  306]
train() client id: f_00004-9-5 loss: 0.846630  [  192/  306]
train() client id: f_00004-9-6 loss: 0.855338  [  224/  306]
train() client id: f_00004-9-7 loss: 0.867211  [  256/  306]
train() client id: f_00004-9-8 loss: 0.798747  [  288/  306]
train() client id: f_00004-10-0 loss: 0.898901  [   32/  306]
train() client id: f_00004-10-1 loss: 0.702336  [   64/  306]
train() client id: f_00004-10-2 loss: 0.758748  [   96/  306]
train() client id: f_00004-10-3 loss: 0.788433  [  128/  306]
train() client id: f_00004-10-4 loss: 0.818403  [  160/  306]
train() client id: f_00004-10-5 loss: 0.858960  [  192/  306]
train() client id: f_00004-10-6 loss: 0.811492  [  224/  306]
train() client id: f_00004-10-7 loss: 0.865599  [  256/  306]
train() client id: f_00004-10-8 loss: 0.833979  [  288/  306]
train() client id: f_00004-11-0 loss: 0.834805  [   32/  306]
train() client id: f_00004-11-1 loss: 0.764356  [   64/  306]
train() client id: f_00004-11-2 loss: 0.799144  [   96/  306]
train() client id: f_00004-11-3 loss: 0.841833  [  128/  306]
train() client id: f_00004-11-4 loss: 0.803355  [  160/  306]
train() client id: f_00004-11-5 loss: 0.854542  [  192/  306]
train() client id: f_00004-11-6 loss: 0.757649  [  224/  306]
train() client id: f_00004-11-7 loss: 0.870542  [  256/  306]
train() client id: f_00004-11-8 loss: 0.858596  [  288/  306]
train() client id: f_00004-12-0 loss: 0.790798  [   32/  306]
train() client id: f_00004-12-1 loss: 0.813964  [   64/  306]
train() client id: f_00004-12-2 loss: 0.706181  [   96/  306]
train() client id: f_00004-12-3 loss: 0.773087  [  128/  306]
train() client id: f_00004-12-4 loss: 0.904554  [  160/  306]
train() client id: f_00004-12-5 loss: 0.870651  [  192/  306]
train() client id: f_00004-12-6 loss: 0.862648  [  224/  306]
train() client id: f_00004-12-7 loss: 0.848798  [  256/  306]
train() client id: f_00004-12-8 loss: 0.821284  [  288/  306]
train() client id: f_00004-13-0 loss: 0.864100  [   32/  306]
train() client id: f_00004-13-1 loss: 0.714774  [   64/  306]
train() client id: f_00004-13-2 loss: 0.815209  [   96/  306]
train() client id: f_00004-13-3 loss: 0.835933  [  128/  306]
train() client id: f_00004-13-4 loss: 0.800004  [  160/  306]
train() client id: f_00004-13-5 loss: 0.787186  [  192/  306]
train() client id: f_00004-13-6 loss: 0.902950  [  224/  306]
train() client id: f_00004-13-7 loss: 0.792786  [  256/  306]
train() client id: f_00004-13-8 loss: 0.852531  [  288/  306]
train() client id: f_00004-14-0 loss: 0.797923  [   32/  306]
train() client id: f_00004-14-1 loss: 0.834850  [   64/  306]
train() client id: f_00004-14-2 loss: 0.761421  [   96/  306]
train() client id: f_00004-14-3 loss: 0.837617  [  128/  306]
train() client id: f_00004-14-4 loss: 0.818152  [  160/  306]
train() client id: f_00004-14-5 loss: 0.880807  [  192/  306]
train() client id: f_00004-14-6 loss: 0.869117  [  224/  306]
train() client id: f_00004-14-7 loss: 0.825498  [  256/  306]
train() client id: f_00004-14-8 loss: 0.797726  [  288/  306]
train() client id: f_00004-15-0 loss: 0.835224  [   32/  306]
train() client id: f_00004-15-1 loss: 0.892943  [   64/  306]
train() client id: f_00004-15-2 loss: 0.843046  [   96/  306]
train() client id: f_00004-15-3 loss: 0.794430  [  128/  306]
train() client id: f_00004-15-4 loss: 0.850475  [  160/  306]
train() client id: f_00004-15-5 loss: 0.686517  [  192/  306]
train() client id: f_00004-15-6 loss: 0.828352  [  224/  306]
train() client id: f_00004-15-7 loss: 0.855270  [  256/  306]
train() client id: f_00004-15-8 loss: 0.825826  [  288/  306]
train() client id: f_00005-0-0 loss: 0.654181  [   32/  146]
train() client id: f_00005-0-1 loss: 0.892988  [   64/  146]
train() client id: f_00005-0-2 loss: 0.793194  [   96/  146]
train() client id: f_00005-0-3 loss: 0.747406  [  128/  146]
train() client id: f_00005-1-0 loss: 0.692707  [   32/  146]
train() client id: f_00005-1-1 loss: 0.751676  [   64/  146]
train() client id: f_00005-1-2 loss: 0.792982  [   96/  146]
train() client id: f_00005-1-3 loss: 0.847875  [  128/  146]
train() client id: f_00005-2-0 loss: 0.755667  [   32/  146]
train() client id: f_00005-2-1 loss: 0.675315  [   64/  146]
train() client id: f_00005-2-2 loss: 0.707326  [   96/  146]
train() client id: f_00005-2-3 loss: 0.694769  [  128/  146]
train() client id: f_00005-3-0 loss: 0.561985  [   32/  146]
train() client id: f_00005-3-1 loss: 0.705021  [   64/  146]
train() client id: f_00005-3-2 loss: 0.768502  [   96/  146]
train() client id: f_00005-3-3 loss: 0.878340  [  128/  146]
train() client id: f_00005-4-0 loss: 0.577335  [   32/  146]
train() client id: f_00005-4-1 loss: 0.639765  [   64/  146]
train() client id: f_00005-4-2 loss: 0.831464  [   96/  146]
train() client id: f_00005-4-3 loss: 0.696722  [  128/  146]
train() client id: f_00005-5-0 loss: 0.743550  [   32/  146]
train() client id: f_00005-5-1 loss: 0.825961  [   64/  146]
train() client id: f_00005-5-2 loss: 0.602562  [   96/  146]
train() client id: f_00005-5-3 loss: 0.900829  [  128/  146]
train() client id: f_00005-6-0 loss: 0.849083  [   32/  146]
train() client id: f_00005-6-1 loss: 1.035179  [   64/  146]
train() client id: f_00005-6-2 loss: 0.638933  [   96/  146]
train() client id: f_00005-6-3 loss: 0.543146  [  128/  146]
train() client id: f_00005-7-0 loss: 0.663978  [   32/  146]
train() client id: f_00005-7-1 loss: 0.767790  [   64/  146]
train() client id: f_00005-7-2 loss: 0.881900  [   96/  146]
train() client id: f_00005-7-3 loss: 0.646223  [  128/  146]
train() client id: f_00005-8-0 loss: 0.425535  [   32/  146]
train() client id: f_00005-8-1 loss: 0.798661  [   64/  146]
train() client id: f_00005-8-2 loss: 0.741170  [   96/  146]
train() client id: f_00005-8-3 loss: 1.150973  [  128/  146]
train() client id: f_00005-9-0 loss: 0.564722  [   32/  146]
train() client id: f_00005-9-1 loss: 0.586873  [   64/  146]
train() client id: f_00005-9-2 loss: 0.961009  [   96/  146]
train() client id: f_00005-9-3 loss: 0.698577  [  128/  146]
train() client id: f_00005-10-0 loss: 0.668556  [   32/  146]
train() client id: f_00005-10-1 loss: 0.812009  [   64/  146]
train() client id: f_00005-10-2 loss: 0.789972  [   96/  146]
train() client id: f_00005-10-3 loss: 0.720502  [  128/  146]
train() client id: f_00005-11-0 loss: 0.662940  [   32/  146]
train() client id: f_00005-11-1 loss: 0.681627  [   64/  146]
train() client id: f_00005-11-2 loss: 0.606360  [   96/  146]
train() client id: f_00005-11-3 loss: 1.010682  [  128/  146]
train() client id: f_00005-12-0 loss: 0.581154  [   32/  146]
train() client id: f_00005-12-1 loss: 1.040226  [   64/  146]
train() client id: f_00005-12-2 loss: 0.687437  [   96/  146]
train() client id: f_00005-12-3 loss: 0.702263  [  128/  146]
train() client id: f_00005-13-0 loss: 0.715720  [   32/  146]
train() client id: f_00005-13-1 loss: 0.506427  [   64/  146]
train() client id: f_00005-13-2 loss: 0.784238  [   96/  146]
train() client id: f_00005-13-3 loss: 0.837858  [  128/  146]
train() client id: f_00005-14-0 loss: 0.737387  [   32/  146]
train() client id: f_00005-14-1 loss: 0.688641  [   64/  146]
train() client id: f_00005-14-2 loss: 0.788543  [   96/  146]
train() client id: f_00005-14-3 loss: 0.724882  [  128/  146]
train() client id: f_00005-15-0 loss: 0.656807  [   32/  146]
train() client id: f_00005-15-1 loss: 0.884316  [   64/  146]
train() client id: f_00005-15-2 loss: 0.609774  [   96/  146]
train() client id: f_00005-15-3 loss: 0.730930  [  128/  146]
train() client id: f_00006-0-0 loss: 0.507200  [   32/   54]
train() client id: f_00006-1-0 loss: 0.471045  [   32/   54]
train() client id: f_00006-2-0 loss: 0.463223  [   32/   54]
train() client id: f_00006-3-0 loss: 0.413011  [   32/   54]
train() client id: f_00006-4-0 loss: 0.403927  [   32/   54]
train() client id: f_00006-5-0 loss: 0.470085  [   32/   54]
train() client id: f_00006-6-0 loss: 0.494260  [   32/   54]
train() client id: f_00006-7-0 loss: 0.472798  [   32/   54]
train() client id: f_00006-8-0 loss: 0.466177  [   32/   54]
train() client id: f_00006-9-0 loss: 0.492656  [   32/   54]
train() client id: f_00006-10-0 loss: 0.436589  [   32/   54]
train() client id: f_00006-11-0 loss: 0.454841  [   32/   54]
train() client id: f_00006-12-0 loss: 0.431362  [   32/   54]
train() client id: f_00006-13-0 loss: 0.388483  [   32/   54]
train() client id: f_00006-14-0 loss: 0.395711  [   32/   54]
train() client id: f_00006-15-0 loss: 0.504975  [   32/   54]
train() client id: f_00007-0-0 loss: 0.377984  [   32/  179]
train() client id: f_00007-0-1 loss: 0.294183  [   64/  179]
train() client id: f_00007-0-2 loss: 0.496807  [   96/  179]
train() client id: f_00007-0-3 loss: 0.584141  [  128/  179]
train() client id: f_00007-0-4 loss: 0.692492  [  160/  179]
train() client id: f_00007-1-0 loss: 0.371109  [   32/  179]
train() client id: f_00007-1-1 loss: 0.721670  [   64/  179]
train() client id: f_00007-1-2 loss: 0.377909  [   96/  179]
train() client id: f_00007-1-3 loss: 0.425267  [  128/  179]
train() client id: f_00007-1-4 loss: 0.555817  [  160/  179]
train() client id: f_00007-2-0 loss: 0.608552  [   32/  179]
train() client id: f_00007-2-1 loss: 0.444502  [   64/  179]
train() client id: f_00007-2-2 loss: 0.458591  [   96/  179]
train() client id: f_00007-2-3 loss: 0.365343  [  128/  179]
train() client id: f_00007-2-4 loss: 0.452298  [  160/  179]
train() client id: f_00007-3-0 loss: 0.444942  [   32/  179]
train() client id: f_00007-3-1 loss: 0.381963  [   64/  179]
train() client id: f_00007-3-2 loss: 0.642071  [   96/  179]
train() client id: f_00007-3-3 loss: 0.331187  [  128/  179]
train() client id: f_00007-3-4 loss: 0.519582  [  160/  179]
train() client id: f_00007-4-0 loss: 0.392201  [   32/  179]
train() client id: f_00007-4-1 loss: 0.306042  [   64/  179]
train() client id: f_00007-4-2 loss: 0.500589  [   96/  179]
train() client id: f_00007-4-3 loss: 0.348468  [  128/  179]
train() client id: f_00007-4-4 loss: 0.614849  [  160/  179]
train() client id: f_00007-5-0 loss: 0.373825  [   32/  179]
train() client id: f_00007-5-1 loss: 0.351104  [   64/  179]
train() client id: f_00007-5-2 loss: 0.435525  [   96/  179]
train() client id: f_00007-5-3 loss: 0.375391  [  128/  179]
train() client id: f_00007-5-4 loss: 0.435383  [  160/  179]
train() client id: f_00007-6-0 loss: 0.343994  [   32/  179]
train() client id: f_00007-6-1 loss: 0.568942  [   64/  179]
train() client id: f_00007-6-2 loss: 0.507269  [   96/  179]
train() client id: f_00007-6-3 loss: 0.287100  [  128/  179]
train() client id: f_00007-6-4 loss: 0.346091  [  160/  179]
train() client id: f_00007-7-0 loss: 0.502136  [   32/  179]
train() client id: f_00007-7-1 loss: 0.648671  [   64/  179]
train() client id: f_00007-7-2 loss: 0.258084  [   96/  179]
train() client id: f_00007-7-3 loss: 0.280442  [  128/  179]
train() client id: f_00007-7-4 loss: 0.320517  [  160/  179]
train() client id: f_00007-8-0 loss: 0.335864  [   32/  179]
train() client id: f_00007-8-1 loss: 0.441931  [   64/  179]
train() client id: f_00007-8-2 loss: 0.463600  [   96/  179]
train() client id: f_00007-8-3 loss: 0.453533  [  128/  179]
train() client id: f_00007-8-4 loss: 0.326438  [  160/  179]
train() client id: f_00007-9-0 loss: 0.341701  [   32/  179]
train() client id: f_00007-9-1 loss: 0.288558  [   64/  179]
train() client id: f_00007-9-2 loss: 0.355465  [   96/  179]
train() client id: f_00007-9-3 loss: 0.417914  [  128/  179]
train() client id: f_00007-9-4 loss: 0.605554  [  160/  179]
train() client id: f_00007-10-0 loss: 0.438709  [   32/  179]
train() client id: f_00007-10-1 loss: 0.297828  [   64/  179]
train() client id: f_00007-10-2 loss: 0.397841  [   96/  179]
train() client id: f_00007-10-3 loss: 0.449371  [  128/  179]
train() client id: f_00007-10-4 loss: 0.335083  [  160/  179]
train() client id: f_00007-11-0 loss: 0.217636  [   32/  179]
train() client id: f_00007-11-1 loss: 0.521210  [   64/  179]
train() client id: f_00007-11-2 loss: 0.520741  [   96/  179]
train() client id: f_00007-11-3 loss: 0.409440  [  128/  179]
train() client id: f_00007-11-4 loss: 0.334711  [  160/  179]
train() client id: f_00007-12-0 loss: 0.217497  [   32/  179]
train() client id: f_00007-12-1 loss: 0.347193  [   64/  179]
train() client id: f_00007-12-2 loss: 0.335188  [   96/  179]
train() client id: f_00007-12-3 loss: 0.385748  [  128/  179]
train() client id: f_00007-12-4 loss: 0.564757  [  160/  179]
train() client id: f_00007-13-0 loss: 0.473606  [   32/  179]
train() client id: f_00007-13-1 loss: 0.203474  [   64/  179]
train() client id: f_00007-13-2 loss: 0.274079  [   96/  179]
train() client id: f_00007-13-3 loss: 0.658478  [  128/  179]
train() client id: f_00007-13-4 loss: 0.302117  [  160/  179]
train() client id: f_00007-14-0 loss: 0.350475  [   32/  179]
train() client id: f_00007-14-1 loss: 0.322195  [   64/  179]
train() client id: f_00007-14-2 loss: 0.197356  [   96/  179]
train() client id: f_00007-14-3 loss: 0.540521  [  128/  179]
train() client id: f_00007-14-4 loss: 0.310825  [  160/  179]
train() client id: f_00007-15-0 loss: 0.399570  [   32/  179]
train() client id: f_00007-15-1 loss: 0.328447  [   64/  179]
train() client id: f_00007-15-2 loss: 0.280395  [   96/  179]
train() client id: f_00007-15-3 loss: 0.340064  [  128/  179]
train() client id: f_00007-15-4 loss: 0.475352  [  160/  179]
train() client id: f_00008-0-0 loss: 0.655184  [   32/  130]
train() client id: f_00008-0-1 loss: 0.734302  [   64/  130]
train() client id: f_00008-0-2 loss: 0.771411  [   96/  130]
train() client id: f_00008-0-3 loss: 0.707109  [  128/  130]
train() client id: f_00008-1-0 loss: 0.695183  [   32/  130]
train() client id: f_00008-1-1 loss: 0.679008  [   64/  130]
train() client id: f_00008-1-2 loss: 0.745719  [   96/  130]
train() client id: f_00008-1-3 loss: 0.747142  [  128/  130]
train() client id: f_00008-2-0 loss: 0.658195  [   32/  130]
train() client id: f_00008-2-1 loss: 0.809012  [   64/  130]
train() client id: f_00008-2-2 loss: 0.753513  [   96/  130]
train() client id: f_00008-2-3 loss: 0.589044  [  128/  130]
train() client id: f_00008-3-0 loss: 0.703870  [   32/  130]
train() client id: f_00008-3-1 loss: 0.731298  [   64/  130]
train() client id: f_00008-3-2 loss: 0.701884  [   96/  130]
train() client id: f_00008-3-3 loss: 0.725866  [  128/  130]
train() client id: f_00008-4-0 loss: 0.624214  [   32/  130]
train() client id: f_00008-4-1 loss: 0.688083  [   64/  130]
train() client id: f_00008-4-2 loss: 0.756037  [   96/  130]
train() client id: f_00008-4-3 loss: 0.786557  [  128/  130]
train() client id: f_00008-5-0 loss: 0.692552  [   32/  130]
train() client id: f_00008-5-1 loss: 0.711809  [   64/  130]
train() client id: f_00008-5-2 loss: 0.796520  [   96/  130]
train() client id: f_00008-5-3 loss: 0.670042  [  128/  130]
train() client id: f_00008-6-0 loss: 0.747666  [   32/  130]
train() client id: f_00008-6-1 loss: 0.685458  [   64/  130]
train() client id: f_00008-6-2 loss: 0.703935  [   96/  130]
train() client id: f_00008-6-3 loss: 0.741174  [  128/  130]
train() client id: f_00008-7-0 loss: 0.731904  [   32/  130]
train() client id: f_00008-7-1 loss: 0.783222  [   64/  130]
train() client id: f_00008-7-2 loss: 0.591873  [   96/  130]
train() client id: f_00008-7-3 loss: 0.769814  [  128/  130]
train() client id: f_00008-8-0 loss: 0.699182  [   32/  130]
train() client id: f_00008-8-1 loss: 0.711387  [   64/  130]
train() client id: f_00008-8-2 loss: 0.656019  [   96/  130]
train() client id: f_00008-8-3 loss: 0.784982  [  128/  130]
train() client id: f_00008-9-0 loss: 0.753677  [   32/  130]
train() client id: f_00008-9-1 loss: 0.637524  [   64/  130]
train() client id: f_00008-9-2 loss: 0.783375  [   96/  130]
train() client id: f_00008-9-3 loss: 0.699891  [  128/  130]
train() client id: f_00008-10-0 loss: 0.661362  [   32/  130]
train() client id: f_00008-10-1 loss: 0.807727  [   64/  130]
train() client id: f_00008-10-2 loss: 0.712213  [   96/  130]
train() client id: f_00008-10-3 loss: 0.694785  [  128/  130]
train() client id: f_00008-11-0 loss: 0.669980  [   32/  130]
train() client id: f_00008-11-1 loss: 0.679361  [   64/  130]
train() client id: f_00008-11-2 loss: 0.758402  [   96/  130]
train() client id: f_00008-11-3 loss: 0.761423  [  128/  130]
train() client id: f_00008-12-0 loss: 0.824817  [   32/  130]
train() client id: f_00008-12-1 loss: 0.587979  [   64/  130]
train() client id: f_00008-12-2 loss: 0.676311  [   96/  130]
train() client id: f_00008-12-3 loss: 0.766664  [  128/  130]
train() client id: f_00008-13-0 loss: 0.703571  [   32/  130]
train() client id: f_00008-13-1 loss: 0.723450  [   64/  130]
train() client id: f_00008-13-2 loss: 0.690547  [   96/  130]
train() client id: f_00008-13-3 loss: 0.755949  [  128/  130]
train() client id: f_00008-14-0 loss: 0.783039  [   32/  130]
train() client id: f_00008-14-1 loss: 0.748091  [   64/  130]
train() client id: f_00008-14-2 loss: 0.663189  [   96/  130]
train() client id: f_00008-14-3 loss: 0.672139  [  128/  130]
train() client id: f_00008-15-0 loss: 0.626333  [   32/  130]
train() client id: f_00008-15-1 loss: 0.765636  [   64/  130]
train() client id: f_00008-15-2 loss: 0.805309  [   96/  130]
train() client id: f_00008-15-3 loss: 0.665107  [  128/  130]
train() client id: f_00009-0-0 loss: 0.870847  [   32/  118]
train() client id: f_00009-0-1 loss: 1.121505  [   64/  118]
train() client id: f_00009-0-2 loss: 1.068802  [   96/  118]
train() client id: f_00009-1-0 loss: 0.946450  [   32/  118]
train() client id: f_00009-1-1 loss: 1.105890  [   64/  118]
train() client id: f_00009-1-2 loss: 0.843806  [   96/  118]
train() client id: f_00009-2-0 loss: 0.966988  [   32/  118]
train() client id: f_00009-2-1 loss: 0.925506  [   64/  118]
train() client id: f_00009-2-2 loss: 1.030576  [   96/  118]
train() client id: f_00009-3-0 loss: 0.889494  [   32/  118]
train() client id: f_00009-3-1 loss: 0.740247  [   64/  118]
train() client id: f_00009-3-2 loss: 0.823539  [   96/  118]
train() client id: f_00009-4-0 loss: 0.793074  [   32/  118]
train() client id: f_00009-4-1 loss: 0.781590  [   64/  118]
train() client id: f_00009-4-2 loss: 0.844078  [   96/  118]
train() client id: f_00009-5-0 loss: 0.755075  [   32/  118]
train() client id: f_00009-5-1 loss: 0.784986  [   64/  118]
train() client id: f_00009-5-2 loss: 0.777582  [   96/  118]
train() client id: f_00009-6-0 loss: 0.654704  [   32/  118]
train() client id: f_00009-6-1 loss: 0.701975  [   64/  118]
train() client id: f_00009-6-2 loss: 0.806827  [   96/  118]
train() client id: f_00009-7-0 loss: 0.785344  [   32/  118]
train() client id: f_00009-7-1 loss: 0.660194  [   64/  118]
train() client id: f_00009-7-2 loss: 0.726786  [   96/  118]
train() client id: f_00009-8-0 loss: 0.722491  [   32/  118]
train() client id: f_00009-8-1 loss: 0.546592  [   64/  118]
train() client id: f_00009-8-2 loss: 0.730944  [   96/  118]
train() client id: f_00009-9-0 loss: 0.615051  [   32/  118]
train() client id: f_00009-9-1 loss: 0.586304  [   64/  118]
train() client id: f_00009-9-2 loss: 0.787783  [   96/  118]
train() client id: f_00009-10-0 loss: 0.750403  [   32/  118]
train() client id: f_00009-10-1 loss: 0.624217  [   64/  118]
train() client id: f_00009-10-2 loss: 0.555532  [   96/  118]
train() client id: f_00009-11-0 loss: 0.588123  [   32/  118]
train() client id: f_00009-11-1 loss: 0.596614  [   64/  118]
train() client id: f_00009-11-2 loss: 0.743461  [   96/  118]
train() client id: f_00009-12-0 loss: 0.636911  [   32/  118]
train() client id: f_00009-12-1 loss: 0.647851  [   64/  118]
train() client id: f_00009-12-2 loss: 0.600492  [   96/  118]
train() client id: f_00009-13-0 loss: 0.749986  [   32/  118]
train() client id: f_00009-13-1 loss: 0.549546  [   64/  118]
train() client id: f_00009-13-2 loss: 0.576930  [   96/  118]
train() client id: f_00009-14-0 loss: 0.492786  [   32/  118]
train() client id: f_00009-14-1 loss: 0.513108  [   64/  118]
train() client id: f_00009-14-2 loss: 0.656480  [   96/  118]
train() client id: f_00009-15-0 loss: 0.457679  [   32/  118]
train() client id: f_00009-15-1 loss: 0.744573  [   64/  118]
train() client id: f_00009-15-2 loss: 0.624876  [   96/  118]
At round 47 accuracy: 0.6445623342175066
At round 47 training accuracy: 0.5902079141515761
At round 47 training loss: 0.8340639874624768
update_location
xs = 8.927491 356.223621 5.882650 0.934260 -272.581990 -120.230757 -80.849135 -5.143845 -295.120581 20.134486 
ys = -347.390647 7.291448 245.684448 -67.290817 -9.642386 0.794442 -1.381692 241.628436 25.881276 -782.232496 
xs mean: -38.18237997052122
ys mean: -68.66579882624052
dists_uav = 361.607469 370.065445 265.321416 120.535999 290.506311 156.384354 128.602067 261.554507 312.675547 788.855548 
uav_gains = -119.774852 -120.177074 -112.645858 -102.028147 -114.917770 -104.862690 -102.731763 -112.306898 -116.743040 -129.391001 
uav_gains_db_mean: -113.55790909132705
dists_bs = 553.824259 557.067944 194.203049 299.428963 208.842178 182.612284 199.937207 182.456740 191.481978 976.919402 
bs_gains = -116.381474 -116.452488 -103.638228 -108.903286 -104.521969 -102.889899 -103.992081 -102.879537 -103.466640 -123.283104 
bs_gains_db_mean: -108.64087057955504
Round 48
-------------------------------
ene_coms = [0.02699376 0.02845516 0.00742046 0.00688104 0.00775324 0.00788944
 0.00710699 0.00715631 0.00735906 0.04484581]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.82662597  9.81542101  4.52297456  1.61953717 11.03632022  5.31538659
  2.01564197  6.48696811  4.73684201  4.75220591]
obj_prev = 55.127923525978645
eta_min = 2.121896640570689e-20	eta_max = 0.8071618770535002
af = 11.464013219461759	bf = 1.7829124006802095	zeta = 12.610414541407936	eta = 0.909090909090909
af = 11.464013219461759	bf = 1.7829124006802095	zeta = 31.631045180968894	eta = 0.36242916267462405
af = 11.464013219461759	bf = 1.7829124006802095	zeta = 21.045567667113886	eta = 0.5447234021335331
af = 11.464013219461759	bf = 1.7829124006802095	zeta = 19.21258047394358	eta = 0.5966930488598053
af = 11.464013219461759	bf = 1.7829124006802095	zeta = 19.098068445080155	eta = 0.6002708207077873
af = 11.464013219461759	bf = 1.7829124006802095	zeta = 19.097556996916268	eta = 0.6002868964503094
af = 11.464013219461759	bf = 1.7829124006802095	zeta = 19.0975569866316	eta = 0.6002868967735839
eta = 0.6002868967735839
ene_coms = [0.02699376 0.02845516 0.00742046 0.00688104 0.00775324 0.00788944
 0.00710699 0.00715631 0.00735906 0.04484581]
ene_comp = [0.04211266 0.08857027 0.0414442  0.01437178 0.1022736  0.04879721
 0.01804828 0.05982671 0.04344957 0.03943884]
ene_total = [2.02979869 3.4372795  1.43525654 0.6242393  3.23171624 1.66500427
 0.73886249 1.96743019 1.49235484 2.47561492]
ti_comp = [0.66820134 0.65358733 0.86393429 0.86932854 0.86060654 0.85924454
 0.86706902 0.86657583 0.86454828 0.48968078]
ti_coms = [0.26993758 0.28455159 0.07420463 0.06881037 0.07753238 0.07889438
 0.0710699  0.07156309 0.07359064 0.44845813]
t_total = [27.55508423 27.55508423 27.55508423 27.55508423 27.55508423 27.55508423
 27.55508423 27.55508423 27.55508423 27.55508423]
ene_coms = [0.02699376 0.02845516 0.00742046 0.00688104 0.00775324 0.00788944
 0.00710699 0.00715631 0.00735906 0.04484581]
ene_comp = [1.04545014e-05 1.01656872e-04 5.96087485e-06 2.45495680e-07
 9.02737246e-05 9.83629119e-06 4.88742731e-07 1.78218308e-05
 6.85895456e-06 1.59891827e-05]
ene_total = [0.79316969 0.83877286 0.21812947 0.20211754 0.23038019 0.23201807
 0.20876139 0.21071909 0.21635243 1.31768411]
optimize_network iter = 0 obj = 4.468104837771096
eta = 0.6002868967735839
freqs = [31511953.49584862 67757031.49881376 23985738.62461054  8266023.51319228
 59419487.94579609 28395413.53957701 10407638.83398568 34519029.74944266
 25128480.27156982 40269947.75053986]
eta_min = 0.6002868967735846	eta_max = 0.6002868967735836
af = 0.005971713602698017	bf = 1.7829124006802095	zeta = 0.00656888496296782	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02699376 0.02845516 0.00742046 0.00688104 0.00775324 0.00788944
 0.00710699 0.00715631 0.00735906 0.04484581]
ene_comp = [1.95243487e-06 1.89849725e-05 1.11322573e-06 4.58476506e-08
 1.68591080e-05 1.83698076e-06 9.12753575e-08 3.32832363e-06
 1.28094699e-06 2.98606665e-06]
ene_total = [3.48634553 3.67727693 0.95845524 0.88865353 1.00346469 1.01911421
 0.91783994 0.92462723 0.95054754 5.79197239]
ti_comp = [0.66820134 0.65358733 0.86393429 0.86932854 0.86060654 0.85924454
 0.86706902 0.86657583 0.86454828 0.48968078]
ti_coms = [0.26993758 0.28455159 0.07420463 0.06881037 0.07753238 0.07889438
 0.0710699  0.07156309 0.07359064 0.44845813]
t_total = [27.55508423 27.55508423 27.55508423 27.55508423 27.55508423 27.55508423
 27.55508423 27.55508423 27.55508423 27.55508423]
ene_coms = [0.02699376 0.02845516 0.00742046 0.00688104 0.00775324 0.00788944
 0.00710699 0.00715631 0.00735906 0.04484581]
ene_comp = [1.04545014e-05 1.01656872e-04 5.96087485e-06 2.45495680e-07
 9.02737246e-05 9.83629119e-06 4.88742731e-07 1.78218308e-05
 6.85895456e-06 1.59891827e-05]
ene_total = [0.79316969 0.83877286 0.21812947 0.20211754 0.23038019 0.23201807
 0.20876139 0.21071909 0.21635243 1.31768411]
optimize_network iter = 1 obj = 4.468104837771093
eta = 0.6002868967735836
freqs = [31511953.49584862 67757031.49881375 23985738.62461054  8266023.51319229
 59419487.94579609 28395413.53957701 10407638.83398568 34519029.74944267
 25128480.27156982 40269947.75053985]
Done!
ene_coms = [0.02699376 0.02845516 0.00742046 0.00688104 0.00775324 0.00788944
 0.00710699 0.00715631 0.00735906 0.04484581]
ene_comp = [1.00094724e-05 9.73295247e-05 5.70713132e-06 2.35045378e-07
 8.64309374e-05 9.41757828e-06 4.67937847e-07 1.70631881e-05
 6.56698142e-06 1.53085525e-05]
ene_total = [0.02700377 0.02855249 0.00742617 0.00688127 0.00783967 0.00789886
 0.00710746 0.00717337 0.00736563 0.04486112]
At round 48 energy consumption: 0.15210980737271995
At round 48 eta: 0.6002868967735836
At round 48 a_n: 11.740402201727147
At round 48 local rounds: 16.71137253973701
At round 48 global rounds: 29.372072386320614
gradient difference: 0.32608360052108765
train() client id: f_00000-0-0 loss: 1.054620  [   32/  126]
train() client id: f_00000-0-1 loss: 1.223296  [   64/  126]
train() client id: f_00000-0-2 loss: 1.233972  [   96/  126]
train() client id: f_00000-1-0 loss: 0.994469  [   32/  126]
train() client id: f_00000-1-1 loss: 1.138188  [   64/  126]
train() client id: f_00000-1-2 loss: 1.164629  [   96/  126]
train() client id: f_00000-2-0 loss: 0.957206  [   32/  126]
train() client id: f_00000-2-1 loss: 1.117385  [   64/  126]
train() client id: f_00000-2-2 loss: 1.005792  [   96/  126]
train() client id: f_00000-3-0 loss: 1.102715  [   32/  126]
train() client id: f_00000-3-1 loss: 0.935618  [   64/  126]
train() client id: f_00000-3-2 loss: 0.837362  [   96/  126]
train() client id: f_00000-4-0 loss: 0.809247  [   32/  126]
train() client id: f_00000-4-1 loss: 0.900989  [   64/  126]
train() client id: f_00000-4-2 loss: 0.874015  [   96/  126]
train() client id: f_00000-5-0 loss: 0.813145  [   32/  126]
train() client id: f_00000-5-1 loss: 0.838058  [   64/  126]
train() client id: f_00000-5-2 loss: 0.981179  [   96/  126]
train() client id: f_00000-6-0 loss: 0.932403  [   32/  126]
train() client id: f_00000-6-1 loss: 0.850690  [   64/  126]
train() client id: f_00000-6-2 loss: 0.781382  [   96/  126]
train() client id: f_00000-7-0 loss: 0.891436  [   32/  126]
train() client id: f_00000-7-1 loss: 0.833704  [   64/  126]
train() client id: f_00000-7-2 loss: 0.831420  [   96/  126]
train() client id: f_00000-8-0 loss: 0.740136  [   32/  126]
train() client id: f_00000-8-1 loss: 0.782698  [   64/  126]
train() client id: f_00000-8-2 loss: 0.944668  [   96/  126]
train() client id: f_00000-9-0 loss: 0.908688  [   32/  126]
train() client id: f_00000-9-1 loss: 0.727489  [   64/  126]
train() client id: f_00000-9-2 loss: 0.759108  [   96/  126]
train() client id: f_00000-10-0 loss: 0.713452  [   32/  126]
train() client id: f_00000-10-1 loss: 0.851370  [   64/  126]
train() client id: f_00000-10-2 loss: 0.820981  [   96/  126]
train() client id: f_00000-11-0 loss: 0.819865  [   32/  126]
train() client id: f_00000-11-1 loss: 0.855381  [   64/  126]
train() client id: f_00000-11-2 loss: 0.758665  [   96/  126]
train() client id: f_00000-12-0 loss: 0.719000  [   32/  126]
train() client id: f_00000-12-1 loss: 0.855062  [   64/  126]
train() client id: f_00000-12-2 loss: 0.823428  [   96/  126]
train() client id: f_00000-13-0 loss: 0.907518  [   32/  126]
train() client id: f_00000-13-1 loss: 0.848340  [   64/  126]
train() client id: f_00000-13-2 loss: 0.808267  [   96/  126]
train() client id: f_00000-14-0 loss: 0.688078  [   32/  126]
train() client id: f_00000-14-1 loss: 0.783608  [   64/  126]
train() client id: f_00000-14-2 loss: 0.888350  [   96/  126]
train() client id: f_00000-15-0 loss: 0.707589  [   32/  126]
train() client id: f_00000-15-1 loss: 0.867903  [   64/  126]
train() client id: f_00000-15-2 loss: 0.801736  [   96/  126]
train() client id: f_00001-0-0 loss: 0.436625  [   32/  265]
train() client id: f_00001-0-1 loss: 0.335795  [   64/  265]
train() client id: f_00001-0-2 loss: 0.423809  [   96/  265]
train() client id: f_00001-0-3 loss: 0.285653  [  128/  265]
train() client id: f_00001-0-4 loss: 0.423626  [  160/  265]
train() client id: f_00001-0-5 loss: 0.305871  [  192/  265]
train() client id: f_00001-0-6 loss: 0.346769  [  224/  265]
train() client id: f_00001-0-7 loss: 0.515574  [  256/  265]
train() client id: f_00001-1-0 loss: 0.393300  [   32/  265]
train() client id: f_00001-1-1 loss: 0.286113  [   64/  265]
train() client id: f_00001-1-2 loss: 0.296855  [   96/  265]
train() client id: f_00001-1-3 loss: 0.353748  [  128/  265]
train() client id: f_00001-1-4 loss: 0.538166  [  160/  265]
train() client id: f_00001-1-5 loss: 0.349910  [  192/  265]
train() client id: f_00001-1-6 loss: 0.371592  [  224/  265]
train() client id: f_00001-1-7 loss: 0.405595  [  256/  265]
train() client id: f_00001-2-0 loss: 0.270565  [   32/  265]
train() client id: f_00001-2-1 loss: 0.369788  [   64/  265]
train() client id: f_00001-2-2 loss: 0.333409  [   96/  265]
train() client id: f_00001-2-3 loss: 0.381288  [  128/  265]
train() client id: f_00001-2-4 loss: 0.406964  [  160/  265]
train() client id: f_00001-2-5 loss: 0.292612  [  192/  265]
train() client id: f_00001-2-6 loss: 0.404750  [  224/  265]
train() client id: f_00001-2-7 loss: 0.398417  [  256/  265]
train() client id: f_00001-3-0 loss: 0.464614  [   32/  265]
train() client id: f_00001-3-1 loss: 0.396153  [   64/  265]
train() client id: f_00001-3-2 loss: 0.267949  [   96/  265]
train() client id: f_00001-3-3 loss: 0.323407  [  128/  265]
train() client id: f_00001-3-4 loss: 0.307267  [  160/  265]
train() client id: f_00001-3-5 loss: 0.392670  [  192/  265]
train() client id: f_00001-3-6 loss: 0.380075  [  224/  265]
train() client id: f_00001-3-7 loss: 0.344370  [  256/  265]
train() client id: f_00001-4-0 loss: 0.350023  [   32/  265]
train() client id: f_00001-4-1 loss: 0.341510  [   64/  265]
train() client id: f_00001-4-2 loss: 0.296475  [   96/  265]
train() client id: f_00001-4-3 loss: 0.364359  [  128/  265]
train() client id: f_00001-4-4 loss: 0.416531  [  160/  265]
train() client id: f_00001-4-5 loss: 0.497750  [  192/  265]
train() client id: f_00001-4-6 loss: 0.311168  [  224/  265]
train() client id: f_00001-4-7 loss: 0.269047  [  256/  265]
train() client id: f_00001-5-0 loss: 0.379332  [   32/  265]
train() client id: f_00001-5-1 loss: 0.314660  [   64/  265]
train() client id: f_00001-5-2 loss: 0.440195  [   96/  265]
train() client id: f_00001-5-3 loss: 0.433366  [  128/  265]
train() client id: f_00001-5-4 loss: 0.271344  [  160/  265]
train() client id: f_00001-5-5 loss: 0.255575  [  192/  265]
train() client id: f_00001-5-6 loss: 0.316786  [  224/  265]
train() client id: f_00001-5-7 loss: 0.400388  [  256/  265]
train() client id: f_00001-6-0 loss: 0.354587  [   32/  265]
train() client id: f_00001-6-1 loss: 0.265146  [   64/  265]
train() client id: f_00001-6-2 loss: 0.312558  [   96/  265]
train() client id: f_00001-6-3 loss: 0.394253  [  128/  265]
train() client id: f_00001-6-4 loss: 0.271391  [  160/  265]
train() client id: f_00001-6-5 loss: 0.251056  [  192/  265]
train() client id: f_00001-6-6 loss: 0.594595  [  224/  265]
train() client id: f_00001-6-7 loss: 0.332187  [  256/  265]
train() client id: f_00001-7-0 loss: 0.448543  [   32/  265]
train() client id: f_00001-7-1 loss: 0.344618  [   64/  265]
train() client id: f_00001-7-2 loss: 0.421443  [   96/  265]
train() client id: f_00001-7-3 loss: 0.326960  [  128/  265]
train() client id: f_00001-7-4 loss: 0.258472  [  160/  265]
train() client id: f_00001-7-5 loss: 0.272672  [  192/  265]
train() client id: f_00001-7-6 loss: 0.258501  [  224/  265]
train() client id: f_00001-7-7 loss: 0.421122  [  256/  265]
train() client id: f_00001-8-0 loss: 0.370830  [   32/  265]
train() client id: f_00001-8-1 loss: 0.255537  [   64/  265]
train() client id: f_00001-8-2 loss: 0.340004  [   96/  265]
train() client id: f_00001-8-3 loss: 0.357809  [  128/  265]
train() client id: f_00001-8-4 loss: 0.253489  [  160/  265]
train() client id: f_00001-8-5 loss: 0.429087  [  192/  265]
train() client id: f_00001-8-6 loss: 0.377513  [  224/  265]
train() client id: f_00001-8-7 loss: 0.248894  [  256/  265]
train() client id: f_00001-9-0 loss: 0.319173  [   32/  265]
train() client id: f_00001-9-1 loss: 0.308970  [   64/  265]
train() client id: f_00001-9-2 loss: 0.405065  [   96/  265]
train() client id: f_00001-9-3 loss: 0.243503  [  128/  265]
train() client id: f_00001-9-4 loss: 0.299929  [  160/  265]
train() client id: f_00001-9-5 loss: 0.313386  [  192/  265]
train() client id: f_00001-9-6 loss: 0.332605  [  224/  265]
train() client id: f_00001-9-7 loss: 0.415957  [  256/  265]
train() client id: f_00001-10-0 loss: 0.381142  [   32/  265]
train() client id: f_00001-10-1 loss: 0.331389  [   64/  265]
train() client id: f_00001-10-2 loss: 0.409103  [   96/  265]
train() client id: f_00001-10-3 loss: 0.348459  [  128/  265]
train() client id: f_00001-10-4 loss: 0.297311  [  160/  265]
train() client id: f_00001-10-5 loss: 0.321841  [  192/  265]
train() client id: f_00001-10-6 loss: 0.360772  [  224/  265]
train() client id: f_00001-10-7 loss: 0.252237  [  256/  265]
train() client id: f_00001-11-0 loss: 0.373555  [   32/  265]
train() client id: f_00001-11-1 loss: 0.356737  [   64/  265]
train() client id: f_00001-11-2 loss: 0.239543  [   96/  265]
train() client id: f_00001-11-3 loss: 0.309901  [  128/  265]
train() client id: f_00001-11-4 loss: 0.321147  [  160/  265]
train() client id: f_00001-11-5 loss: 0.278152  [  192/  265]
train() client id: f_00001-11-6 loss: 0.369507  [  224/  265]
train() client id: f_00001-11-7 loss: 0.239021  [  256/  265]
train() client id: f_00001-12-0 loss: 0.246393  [   32/  265]
train() client id: f_00001-12-1 loss: 0.345652  [   64/  265]
train() client id: f_00001-12-2 loss: 0.388853  [   96/  265]
train() client id: f_00001-12-3 loss: 0.252332  [  128/  265]
train() client id: f_00001-12-4 loss: 0.560028  [  160/  265]
train() client id: f_00001-12-5 loss: 0.277966  [  192/  265]
train() client id: f_00001-12-6 loss: 0.353606  [  224/  265]
train() client id: f_00001-12-7 loss: 0.238178  [  256/  265]
train() client id: f_00001-13-0 loss: 0.326366  [   32/  265]
train() client id: f_00001-13-1 loss: 0.415988  [   64/  265]
train() client id: f_00001-13-2 loss: 0.468451  [   96/  265]
train() client id: f_00001-13-3 loss: 0.215001  [  128/  265]
train() client id: f_00001-13-4 loss: 0.355462  [  160/  265]
train() client id: f_00001-13-5 loss: 0.324894  [  192/  265]
train() client id: f_00001-13-6 loss: 0.220868  [  224/  265]
train() client id: f_00001-13-7 loss: 0.293256  [  256/  265]
train() client id: f_00001-14-0 loss: 0.409585  [   32/  265]
train() client id: f_00001-14-1 loss: 0.247195  [   64/  265]
train() client id: f_00001-14-2 loss: 0.271843  [   96/  265]
train() client id: f_00001-14-3 loss: 0.223615  [  128/  265]
train() client id: f_00001-14-4 loss: 0.349008  [  160/  265]
train() client id: f_00001-14-5 loss: 0.363437  [  192/  265]
train() client id: f_00001-14-6 loss: 0.320567  [  224/  265]
train() client id: f_00001-14-7 loss: 0.283471  [  256/  265]
train() client id: f_00001-15-0 loss: 0.313123  [   32/  265]
train() client id: f_00001-15-1 loss: 0.370652  [   64/  265]
train() client id: f_00001-15-2 loss: 0.322682  [   96/  265]
train() client id: f_00001-15-3 loss: 0.322336  [  128/  265]
train() client id: f_00001-15-4 loss: 0.388710  [  160/  265]
train() client id: f_00001-15-5 loss: 0.229502  [  192/  265]
train() client id: f_00001-15-6 loss: 0.252217  [  224/  265]
train() client id: f_00001-15-7 loss: 0.356595  [  256/  265]
train() client id: f_00002-0-0 loss: 1.243135  [   32/  124]
train() client id: f_00002-0-1 loss: 1.135612  [   64/  124]
train() client id: f_00002-0-2 loss: 1.394678  [   96/  124]
train() client id: f_00002-1-0 loss: 1.324822  [   32/  124]
train() client id: f_00002-1-1 loss: 1.181921  [   64/  124]
train() client id: f_00002-1-2 loss: 1.223171  [   96/  124]
train() client id: f_00002-2-0 loss: 1.376909  [   32/  124]
train() client id: f_00002-2-1 loss: 1.099876  [   64/  124]
train() client id: f_00002-2-2 loss: 1.053829  [   96/  124]
train() client id: f_00002-3-0 loss: 1.020902  [   32/  124]
train() client id: f_00002-3-1 loss: 1.287859  [   64/  124]
train() client id: f_00002-3-2 loss: 1.079077  [   96/  124]
train() client id: f_00002-4-0 loss: 0.986363  [   32/  124]
train() client id: f_00002-4-1 loss: 1.251904  [   64/  124]
train() client id: f_00002-4-2 loss: 1.095261  [   96/  124]
train() client id: f_00002-5-0 loss: 1.030535  [   32/  124]
train() client id: f_00002-5-1 loss: 1.120617  [   64/  124]
train() client id: f_00002-5-2 loss: 1.006266  [   96/  124]
train() client id: f_00002-6-0 loss: 1.146711  [   32/  124]
train() client id: f_00002-6-1 loss: 1.027397  [   64/  124]
train() client id: f_00002-6-2 loss: 0.978035  [   96/  124]
train() client id: f_00002-7-0 loss: 1.150763  [   32/  124]
train() client id: f_00002-7-1 loss: 0.879510  [   64/  124]
train() client id: f_00002-7-2 loss: 1.097058  [   96/  124]
train() client id: f_00002-8-0 loss: 0.852126  [   32/  124]
train() client id: f_00002-8-1 loss: 1.197598  [   64/  124]
train() client id: f_00002-8-2 loss: 1.017133  [   96/  124]
train() client id: f_00002-9-0 loss: 0.984951  [   32/  124]
train() client id: f_00002-9-1 loss: 0.969424  [   64/  124]
train() client id: f_00002-9-2 loss: 1.090183  [   96/  124]
train() client id: f_00002-10-0 loss: 0.996854  [   32/  124]
train() client id: f_00002-10-1 loss: 0.926168  [   64/  124]
train() client id: f_00002-10-2 loss: 0.812217  [   96/  124]
train() client id: f_00002-11-0 loss: 0.855298  [   32/  124]
train() client id: f_00002-11-1 loss: 1.008505  [   64/  124]
train() client id: f_00002-11-2 loss: 0.909724  [   96/  124]
train() client id: f_00002-12-0 loss: 0.824552  [   32/  124]
train() client id: f_00002-12-1 loss: 0.815577  [   64/  124]
train() client id: f_00002-12-2 loss: 1.143761  [   96/  124]
train() client id: f_00002-13-0 loss: 0.781695  [   32/  124]
train() client id: f_00002-13-1 loss: 0.929487  [   64/  124]
train() client id: f_00002-13-2 loss: 1.073228  [   96/  124]
train() client id: f_00002-14-0 loss: 0.894943  [   32/  124]
train() client id: f_00002-14-1 loss: 0.784636  [   64/  124]
train() client id: f_00002-14-2 loss: 1.066885  [   96/  124]
train() client id: f_00002-15-0 loss: 0.730435  [   32/  124]
train() client id: f_00002-15-1 loss: 0.838571  [   64/  124]
train() client id: f_00002-15-2 loss: 1.227140  [   96/  124]
train() client id: f_00003-0-0 loss: 0.481033  [   32/   43]
train() client id: f_00003-1-0 loss: 0.457682  [   32/   43]
train() client id: f_00003-2-0 loss: 0.369177  [   32/   43]
train() client id: f_00003-3-0 loss: 0.799022  [   32/   43]
train() client id: f_00003-4-0 loss: 0.616511  [   32/   43]
train() client id: f_00003-5-0 loss: 0.547161  [   32/   43]
train() client id: f_00003-6-0 loss: 0.696248  [   32/   43]
train() client id: f_00003-7-0 loss: 0.533717  [   32/   43]
train() client id: f_00003-8-0 loss: 0.638341  [   32/   43]
train() client id: f_00003-9-0 loss: 0.513175  [   32/   43]
train() client id: f_00003-10-0 loss: 0.398882  [   32/   43]
train() client id: f_00003-11-0 loss: 0.588569  [   32/   43]
train() client id: f_00003-12-0 loss: 0.435850  [   32/   43]
train() client id: f_00003-13-0 loss: 0.655202  [   32/   43]
train() client id: f_00003-14-0 loss: 0.558565  [   32/   43]
train() client id: f_00003-15-0 loss: 0.535265  [   32/   43]
train() client id: f_00004-0-0 loss: 0.950946  [   32/  306]
train() client id: f_00004-0-1 loss: 0.775841  [   64/  306]
train() client id: f_00004-0-2 loss: 0.902734  [   96/  306]
train() client id: f_00004-0-3 loss: 0.847483  [  128/  306]
train() client id: f_00004-0-4 loss: 0.863910  [  160/  306]
train() client id: f_00004-0-5 loss: 0.768501  [  192/  306]
train() client id: f_00004-0-6 loss: 0.953340  [  224/  306]
train() client id: f_00004-0-7 loss: 0.915159  [  256/  306]
train() client id: f_00004-0-8 loss: 0.903717  [  288/  306]
train() client id: f_00004-1-0 loss: 0.952779  [   32/  306]
train() client id: f_00004-1-1 loss: 0.937263  [   64/  306]
train() client id: f_00004-1-2 loss: 0.874666  [   96/  306]
train() client id: f_00004-1-3 loss: 0.748995  [  128/  306]
train() client id: f_00004-1-4 loss: 0.715932  [  160/  306]
train() client id: f_00004-1-5 loss: 1.003080  [  192/  306]
train() client id: f_00004-1-6 loss: 0.904400  [  224/  306]
train() client id: f_00004-1-7 loss: 0.772001  [  256/  306]
train() client id: f_00004-1-8 loss: 0.945691  [  288/  306]
train() client id: f_00004-2-0 loss: 0.840465  [   32/  306]
train() client id: f_00004-2-1 loss: 0.799566  [   64/  306]
train() client id: f_00004-2-2 loss: 0.903462  [   96/  306]
train() client id: f_00004-2-3 loss: 0.778380  [  128/  306]
train() client id: f_00004-2-4 loss: 0.898344  [  160/  306]
train() client id: f_00004-2-5 loss: 0.914041  [  192/  306]
train() client id: f_00004-2-6 loss: 0.769636  [  224/  306]
train() client id: f_00004-2-7 loss: 0.980294  [  256/  306]
train() client id: f_00004-2-8 loss: 0.926363  [  288/  306]
train() client id: f_00004-3-0 loss: 0.833578  [   32/  306]
train() client id: f_00004-3-1 loss: 0.708038  [   64/  306]
train() client id: f_00004-3-2 loss: 0.801704  [   96/  306]
train() client id: f_00004-3-3 loss: 0.936360  [  128/  306]
train() client id: f_00004-3-4 loss: 0.960442  [  160/  306]
train() client id: f_00004-3-5 loss: 0.858949  [  192/  306]
train() client id: f_00004-3-6 loss: 0.949855  [  224/  306]
train() client id: f_00004-3-7 loss: 0.908591  [  256/  306]
train() client id: f_00004-3-8 loss: 0.820099  [  288/  306]
train() client id: f_00004-4-0 loss: 0.929466  [   32/  306]
train() client id: f_00004-4-1 loss: 0.879579  [   64/  306]
train() client id: f_00004-4-2 loss: 0.875292  [   96/  306]
train() client id: f_00004-4-3 loss: 0.870888  [  128/  306]
train() client id: f_00004-4-4 loss: 0.820332  [  160/  306]
train() client id: f_00004-4-5 loss: 0.834900  [  192/  306]
train() client id: f_00004-4-6 loss: 0.758745  [  224/  306]
train() client id: f_00004-4-7 loss: 0.808857  [  256/  306]
train() client id: f_00004-4-8 loss: 0.853279  [  288/  306]
train() client id: f_00004-5-0 loss: 0.838715  [   32/  306]
train() client id: f_00004-5-1 loss: 0.837633  [   64/  306]
train() client id: f_00004-5-2 loss: 0.953213  [   96/  306]
train() client id: f_00004-5-3 loss: 0.800194  [  128/  306]
train() client id: f_00004-5-4 loss: 0.795309  [  160/  306]
train() client id: f_00004-5-5 loss: 0.895508  [  192/  306]
train() client id: f_00004-5-6 loss: 0.776615  [  224/  306]
train() client id: f_00004-5-7 loss: 0.919335  [  256/  306]
train() client id: f_00004-5-8 loss: 0.858350  [  288/  306]
train() client id: f_00004-6-0 loss: 0.806355  [   32/  306]
train() client id: f_00004-6-1 loss: 0.814763  [   64/  306]
train() client id: f_00004-6-2 loss: 0.885430  [   96/  306]
train() client id: f_00004-6-3 loss: 0.898501  [  128/  306]
train() client id: f_00004-6-4 loss: 0.994464  [  160/  306]
train() client id: f_00004-6-5 loss: 0.834510  [  192/  306]
train() client id: f_00004-6-6 loss: 0.790376  [  224/  306]
train() client id: f_00004-6-7 loss: 0.792957  [  256/  306]
train() client id: f_00004-6-8 loss: 0.827325  [  288/  306]
train() client id: f_00004-7-0 loss: 0.817864  [   32/  306]
train() client id: f_00004-7-1 loss: 0.690283  [   64/  306]
train() client id: f_00004-7-2 loss: 0.735216  [   96/  306]
train() client id: f_00004-7-3 loss: 0.886138  [  128/  306]
train() client id: f_00004-7-4 loss: 0.945248  [  160/  306]
train() client id: f_00004-7-5 loss: 0.896559  [  192/  306]
train() client id: f_00004-7-6 loss: 0.909897  [  224/  306]
train() client id: f_00004-7-7 loss: 0.942531  [  256/  306]
train() client id: f_00004-7-8 loss: 0.816232  [  288/  306]
train() client id: f_00004-8-0 loss: 0.834415  [   32/  306]
train() client id: f_00004-8-1 loss: 0.794520  [   64/  306]
train() client id: f_00004-8-2 loss: 0.959541  [   96/  306]
train() client id: f_00004-8-3 loss: 0.913510  [  128/  306]
train() client id: f_00004-8-4 loss: 0.869866  [  160/  306]
train() client id: f_00004-8-5 loss: 0.917793  [  192/  306]
train() client id: f_00004-8-6 loss: 0.858049  [  224/  306]
train() client id: f_00004-8-7 loss: 0.843632  [  256/  306]
train() client id: f_00004-8-8 loss: 0.732755  [  288/  306]
train() client id: f_00004-9-0 loss: 0.788475  [   32/  306]
train() client id: f_00004-9-1 loss: 0.827109  [   64/  306]
train() client id: f_00004-9-2 loss: 0.742187  [   96/  306]
train() client id: f_00004-9-3 loss: 0.922918  [  128/  306]
train() client id: f_00004-9-4 loss: 0.825091  [  160/  306]
train() client id: f_00004-9-5 loss: 0.881342  [  192/  306]
train() client id: f_00004-9-6 loss: 0.893387  [  224/  306]
train() client id: f_00004-9-7 loss: 0.914248  [  256/  306]
train() client id: f_00004-9-8 loss: 0.815103  [  288/  306]
train() client id: f_00004-10-0 loss: 1.000115  [   32/  306]
train() client id: f_00004-10-1 loss: 0.843518  [   64/  306]
train() client id: f_00004-10-2 loss: 0.773630  [   96/  306]
train() client id: f_00004-10-3 loss: 0.771422  [  128/  306]
train() client id: f_00004-10-4 loss: 0.786088  [  160/  306]
train() client id: f_00004-10-5 loss: 0.817277  [  192/  306]
train() client id: f_00004-10-6 loss: 0.878625  [  224/  306]
train() client id: f_00004-10-7 loss: 1.005936  [  256/  306]
train() client id: f_00004-10-8 loss: 0.745951  [  288/  306]
train() client id: f_00004-11-0 loss: 0.848296  [   32/  306]
train() client id: f_00004-11-1 loss: 1.047664  [   64/  306]
train() client id: f_00004-11-2 loss: 0.835224  [   96/  306]
train() client id: f_00004-11-3 loss: 0.917599  [  128/  306]
train() client id: f_00004-11-4 loss: 0.817958  [  160/  306]
train() client id: f_00004-11-5 loss: 0.698665  [  192/  306]
train() client id: f_00004-11-6 loss: 0.703951  [  224/  306]
train() client id: f_00004-11-7 loss: 0.833705  [  256/  306]
train() client id: f_00004-11-8 loss: 0.910239  [  288/  306]
train() client id: f_00004-12-0 loss: 1.076685  [   32/  306]
train() client id: f_00004-12-1 loss: 0.776913  [   64/  306]
train() client id: f_00004-12-2 loss: 0.749707  [   96/  306]
train() client id: f_00004-12-3 loss: 0.754193  [  128/  306]
train() client id: f_00004-12-4 loss: 0.971707  [  160/  306]
train() client id: f_00004-12-5 loss: 0.815891  [  192/  306]
train() client id: f_00004-12-6 loss: 0.696855  [  224/  306]
train() client id: f_00004-12-7 loss: 0.889269  [  256/  306]
train() client id: f_00004-12-8 loss: 0.807212  [  288/  306]
train() client id: f_00004-13-0 loss: 0.769572  [   32/  306]
train() client id: f_00004-13-1 loss: 0.858218  [   64/  306]
train() client id: f_00004-13-2 loss: 0.893380  [   96/  306]
train() client id: f_00004-13-3 loss: 0.807501  [  128/  306]
train() client id: f_00004-13-4 loss: 0.813237  [  160/  306]
train() client id: f_00004-13-5 loss: 0.814447  [  192/  306]
train() client id: f_00004-13-6 loss: 0.921273  [  224/  306]
train() client id: f_00004-13-7 loss: 0.760665  [  256/  306]
train() client id: f_00004-13-8 loss: 0.850071  [  288/  306]
train() client id: f_00004-14-0 loss: 0.831767  [   32/  306]
train() client id: f_00004-14-1 loss: 0.910457  [   64/  306]
train() client id: f_00004-14-2 loss: 0.764559  [   96/  306]
train() client id: f_00004-14-3 loss: 0.904524  [  128/  306]
train() client id: f_00004-14-4 loss: 0.768716  [  160/  306]
train() client id: f_00004-14-5 loss: 0.860903  [  192/  306]
train() client id: f_00004-14-6 loss: 0.727397  [  224/  306]
train() client id: f_00004-14-7 loss: 0.827174  [  256/  306]
train() client id: f_00004-14-8 loss: 0.871000  [  288/  306]
train() client id: f_00004-15-0 loss: 0.712228  [   32/  306]
train() client id: f_00004-15-1 loss: 0.879152  [   64/  306]
train() client id: f_00004-15-2 loss: 0.813057  [   96/  306]
train() client id: f_00004-15-3 loss: 0.859232  [  128/  306]
train() client id: f_00004-15-4 loss: 0.897832  [  160/  306]
train() client id: f_00004-15-5 loss: 0.851565  [  192/  306]
train() client id: f_00004-15-6 loss: 0.853405  [  224/  306]
train() client id: f_00004-15-7 loss: 0.942761  [  256/  306]
train() client id: f_00004-15-8 loss: 0.714043  [  288/  306]
train() client id: f_00005-0-0 loss: 0.454436  [   32/  146]
train() client id: f_00005-0-1 loss: 0.754408  [   64/  146]
train() client id: f_00005-0-2 loss: 0.580952  [   96/  146]
train() client id: f_00005-0-3 loss: 0.642398  [  128/  146]
train() client id: f_00005-1-0 loss: 0.857085  [   32/  146]
train() client id: f_00005-1-1 loss: 0.499940  [   64/  146]
train() client id: f_00005-1-2 loss: 0.616970  [   96/  146]
train() client id: f_00005-1-3 loss: 0.595376  [  128/  146]
train() client id: f_00005-2-0 loss: 0.562652  [   32/  146]
train() client id: f_00005-2-1 loss: 0.736420  [   64/  146]
train() client id: f_00005-2-2 loss: 0.660422  [   96/  146]
train() client id: f_00005-2-3 loss: 0.478952  [  128/  146]
train() client id: f_00005-3-0 loss: 0.570544  [   32/  146]
train() client id: f_00005-3-1 loss: 0.619696  [   64/  146]
train() client id: f_00005-3-2 loss: 0.598355  [   96/  146]
train() client id: f_00005-3-3 loss: 0.743698  [  128/  146]
train() client id: f_00005-4-0 loss: 0.481772  [   32/  146]
train() client id: f_00005-4-1 loss: 0.692931  [   64/  146]
train() client id: f_00005-4-2 loss: 0.522919  [   96/  146]
train() client id: f_00005-4-3 loss: 0.557372  [  128/  146]
train() client id: f_00005-5-0 loss: 0.433175  [   32/  146]
train() client id: f_00005-5-1 loss: 0.834437  [   64/  146]
train() client id: f_00005-5-2 loss: 0.363266  [   96/  146]
train() client id: f_00005-5-3 loss: 0.744025  [  128/  146]
train() client id: f_00005-6-0 loss: 0.527535  [   32/  146]
train() client id: f_00005-6-1 loss: 0.804833  [   64/  146]
train() client id: f_00005-6-2 loss: 0.429934  [   96/  146]
train() client id: f_00005-6-3 loss: 0.699866  [  128/  146]
train() client id: f_00005-7-0 loss: 0.686112  [   32/  146]
train() client id: f_00005-7-1 loss: 0.482973  [   64/  146]
train() client id: f_00005-7-2 loss: 0.890043  [   96/  146]
train() client id: f_00005-7-3 loss: 0.470570  [  128/  146]
train() client id: f_00005-8-0 loss: 0.754442  [   32/  146]
train() client id: f_00005-8-1 loss: 0.767283  [   64/  146]
train() client id: f_00005-8-2 loss: 0.746469  [   96/  146]
train() client id: f_00005-8-3 loss: 0.309184  [  128/  146]
train() client id: f_00005-9-0 loss: 0.437477  [   32/  146]
train() client id: f_00005-9-1 loss: 0.722187  [   64/  146]
train() client id: f_00005-9-2 loss: 0.584065  [   96/  146]
train() client id: f_00005-9-3 loss: 0.516641  [  128/  146]
train() client id: f_00005-10-0 loss: 0.682559  [   32/  146]
train() client id: f_00005-10-1 loss: 0.567512  [   64/  146]
train() client id: f_00005-10-2 loss: 0.416214  [   96/  146]
train() client id: f_00005-10-3 loss: 0.792053  [  128/  146]
train() client id: f_00005-11-0 loss: 0.511758  [   32/  146]
train() client id: f_00005-11-1 loss: 0.756293  [   64/  146]
train() client id: f_00005-11-2 loss: 0.404594  [   96/  146]
train() client id: f_00005-11-3 loss: 0.665566  [  128/  146]
train() client id: f_00005-12-0 loss: 0.447386  [   32/  146]
train() client id: f_00005-12-1 loss: 0.646727  [   64/  146]
train() client id: f_00005-12-2 loss: 0.812758  [   96/  146]
train() client id: f_00005-12-3 loss: 0.483756  [  128/  146]
train() client id: f_00005-13-0 loss: 0.714980  [   32/  146]
train() client id: f_00005-13-1 loss: 0.598719  [   64/  146]
train() client id: f_00005-13-2 loss: 0.667372  [   96/  146]
train() client id: f_00005-13-3 loss: 0.530094  [  128/  146]
train() client id: f_00005-14-0 loss: 0.366096  [   32/  146]
train() client id: f_00005-14-1 loss: 0.505871  [   64/  146]
train() client id: f_00005-14-2 loss: 0.783842  [   96/  146]
train() client id: f_00005-14-3 loss: 0.659907  [  128/  146]
train() client id: f_00005-15-0 loss: 0.731859  [   32/  146]
train() client id: f_00005-15-1 loss: 0.421967  [   64/  146]
train() client id: f_00005-15-2 loss: 0.694249  [   96/  146]
train() client id: f_00005-15-3 loss: 0.567878  [  128/  146]
train() client id: f_00006-0-0 loss: 0.461148  [   32/   54]
train() client id: f_00006-1-0 loss: 0.488229  [   32/   54]
train() client id: f_00006-2-0 loss: 0.442367  [   32/   54]
train() client id: f_00006-3-0 loss: 0.389514  [   32/   54]
train() client id: f_00006-4-0 loss: 0.458950  [   32/   54]
train() client id: f_00006-5-0 loss: 0.485022  [   32/   54]
train() client id: f_00006-6-0 loss: 0.452966  [   32/   54]
train() client id: f_00006-7-0 loss: 0.445538  [   32/   54]
train() client id: f_00006-8-0 loss: 0.481686  [   32/   54]
train() client id: f_00006-9-0 loss: 0.501522  [   32/   54]
train() client id: f_00006-10-0 loss: 0.493161  [   32/   54]
train() client id: f_00006-11-0 loss: 0.464033  [   32/   54]
train() client id: f_00006-12-0 loss: 0.444045  [   32/   54]
train() client id: f_00006-13-0 loss: 0.513140  [   32/   54]
train() client id: f_00006-14-0 loss: 0.394094  [   32/   54]
train() client id: f_00006-15-0 loss: 0.398164  [   32/   54]
train() client id: f_00007-0-0 loss: 0.576483  [   32/  179]
train() client id: f_00007-0-1 loss: 0.549928  [   64/  179]
train() client id: f_00007-0-2 loss: 0.392525  [   96/  179]
train() client id: f_00007-0-3 loss: 0.451126  [  128/  179]
train() client id: f_00007-0-4 loss: 0.448246  [  160/  179]
train() client id: f_00007-1-0 loss: 0.584218  [   32/  179]
train() client id: f_00007-1-1 loss: 0.298054  [   64/  179]
train() client id: f_00007-1-2 loss: 0.429244  [   96/  179]
train() client id: f_00007-1-3 loss: 0.596039  [  128/  179]
train() client id: f_00007-1-4 loss: 0.420981  [  160/  179]
train() client id: f_00007-2-0 loss: 0.365396  [   32/  179]
train() client id: f_00007-2-1 loss: 0.407089  [   64/  179]
train() client id: f_00007-2-2 loss: 0.524924  [   96/  179]
train() client id: f_00007-2-3 loss: 0.558807  [  128/  179]
train() client id: f_00007-2-4 loss: 0.426332  [  160/  179]
train() client id: f_00007-3-0 loss: 0.488323  [   32/  179]
train() client id: f_00007-3-1 loss: 0.316239  [   64/  179]
train() client id: f_00007-3-2 loss: 0.299640  [   96/  179]
train() client id: f_00007-3-3 loss: 0.419777  [  128/  179]
train() client id: f_00007-3-4 loss: 0.571309  [  160/  179]
train() client id: f_00007-4-0 loss: 0.419405  [   32/  179]
train() client id: f_00007-4-1 loss: 0.235375  [   64/  179]
train() client id: f_00007-4-2 loss: 0.286987  [   96/  179]
train() client id: f_00007-4-3 loss: 0.608601  [  128/  179]
train() client id: f_00007-4-4 loss: 0.413597  [  160/  179]
train() client id: f_00007-5-0 loss: 0.452327  [   32/  179]
train() client id: f_00007-5-1 loss: 0.438427  [   64/  179]
train() client id: f_00007-5-2 loss: 0.437764  [   96/  179]
train() client id: f_00007-5-3 loss: 0.398561  [  128/  179]
train() client id: f_00007-5-4 loss: 0.332068  [  160/  179]
train() client id: f_00007-6-0 loss: 0.377219  [   32/  179]
train() client id: f_00007-6-1 loss: 0.399259  [   64/  179]
train() client id: f_00007-6-2 loss: 0.439149  [   96/  179]
train() client id: f_00007-6-3 loss: 0.632739  [  128/  179]
train() client id: f_00007-6-4 loss: 0.210446  [  160/  179]
train() client id: f_00007-7-0 loss: 0.475484  [   32/  179]
train() client id: f_00007-7-1 loss: 0.231766  [   64/  179]
train() client id: f_00007-7-2 loss: 0.174690  [   96/  179]
train() client id: f_00007-7-3 loss: 0.463961  [  128/  179]
train() client id: f_00007-7-4 loss: 0.532263  [  160/  179]
train() client id: f_00007-8-0 loss: 0.515014  [   32/  179]
train() client id: f_00007-8-1 loss: 0.357631  [   64/  179]
train() client id: f_00007-8-2 loss: 0.284683  [   96/  179]
train() client id: f_00007-8-3 loss: 0.429113  [  128/  179]
train() client id: f_00007-8-4 loss: 0.395734  [  160/  179]
train() client id: f_00007-9-0 loss: 0.237831  [   32/  179]
train() client id: f_00007-9-1 loss: 0.293069  [   64/  179]
train() client id: f_00007-9-2 loss: 0.573460  [   96/  179]
train() client id: f_00007-9-3 loss: 0.231279  [  128/  179]
train() client id: f_00007-9-4 loss: 0.407069  [  160/  179]
train() client id: f_00007-10-0 loss: 0.668399  [   32/  179]
train() client id: f_00007-10-1 loss: 0.232023  [   64/  179]
train() client id: f_00007-10-2 loss: 0.303859  [   96/  179]
train() client id: f_00007-10-3 loss: 0.273746  [  128/  179]
train() client id: f_00007-10-4 loss: 0.477856  [  160/  179]
train() client id: f_00007-11-0 loss: 0.418926  [   32/  179]
train() client id: f_00007-11-1 loss: 0.421450  [   64/  179]
train() client id: f_00007-11-2 loss: 0.192522  [   96/  179]
train() client id: f_00007-11-3 loss: 0.463686  [  128/  179]
train() client id: f_00007-11-4 loss: 0.264013  [  160/  179]
train() client id: f_00007-12-0 loss: 0.559018  [   32/  179]
train() client id: f_00007-12-1 loss: 0.200470  [   64/  179]
train() client id: f_00007-12-2 loss: 0.216393  [   96/  179]
train() client id: f_00007-12-3 loss: 0.317559  [  128/  179]
train() client id: f_00007-12-4 loss: 0.390643  [  160/  179]
train() client id: f_00007-13-0 loss: 0.273392  [   32/  179]
train() client id: f_00007-13-1 loss: 0.242323  [   64/  179]
train() client id: f_00007-13-2 loss: 0.489772  [   96/  179]
train() client id: f_00007-13-3 loss: 0.373691  [  128/  179]
train() client id: f_00007-13-4 loss: 0.304431  [  160/  179]
train() client id: f_00007-14-0 loss: 0.259211  [   32/  179]
train() client id: f_00007-14-1 loss: 0.264331  [   64/  179]
train() client id: f_00007-14-2 loss: 0.421810  [   96/  179]
train() client id: f_00007-14-3 loss: 0.485431  [  128/  179]
train() client id: f_00007-14-4 loss: 0.353998  [  160/  179]
train() client id: f_00007-15-0 loss: 0.300267  [   32/  179]
train() client id: f_00007-15-1 loss: 0.414375  [   64/  179]
train() client id: f_00007-15-2 loss: 0.397890  [   96/  179]
train() client id: f_00007-15-3 loss: 0.373389  [  128/  179]
train() client id: f_00007-15-4 loss: 0.235748  [  160/  179]
train() client id: f_00008-0-0 loss: 0.830790  [   32/  130]
train() client id: f_00008-0-1 loss: 0.768908  [   64/  130]
train() client id: f_00008-0-2 loss: 0.703592  [   96/  130]
train() client id: f_00008-0-3 loss: 0.666301  [  128/  130]
train() client id: f_00008-1-0 loss: 0.806876  [   32/  130]
train() client id: f_00008-1-1 loss: 0.792479  [   64/  130]
train() client id: f_00008-1-2 loss: 0.675428  [   96/  130]
train() client id: f_00008-1-3 loss: 0.660195  [  128/  130]
train() client id: f_00008-2-0 loss: 0.750615  [   32/  130]
train() client id: f_00008-2-1 loss: 0.798642  [   64/  130]
train() client id: f_00008-2-2 loss: 0.721838  [   96/  130]
train() client id: f_00008-2-3 loss: 0.693137  [  128/  130]
train() client id: f_00008-3-0 loss: 0.723353  [   32/  130]
train() client id: f_00008-3-1 loss: 0.660287  [   64/  130]
train() client id: f_00008-3-2 loss: 0.732090  [   96/  130]
train() client id: f_00008-3-3 loss: 0.845541  [  128/  130]
train() client id: f_00008-4-0 loss: 0.771083  [   32/  130]
train() client id: f_00008-4-1 loss: 0.827947  [   64/  130]
train() client id: f_00008-4-2 loss: 0.703142  [   96/  130]
train() client id: f_00008-4-3 loss: 0.651715  [  128/  130]
train() client id: f_00008-5-0 loss: 0.755025  [   32/  130]
train() client id: f_00008-5-1 loss: 0.760199  [   64/  130]
train() client id: f_00008-5-2 loss: 0.739851  [   96/  130]
train() client id: f_00008-5-3 loss: 0.698822  [  128/  130]
train() client id: f_00008-6-0 loss: 0.722013  [   32/  130]
train() client id: f_00008-6-1 loss: 0.825449  [   64/  130]
train() client id: f_00008-6-2 loss: 0.673822  [   96/  130]
train() client id: f_00008-6-3 loss: 0.735258  [  128/  130]
train() client id: f_00008-7-0 loss: 0.690716  [   32/  130]
train() client id: f_00008-7-1 loss: 0.783152  [   64/  130]
train() client id: f_00008-7-2 loss: 0.716688  [   96/  130]
train() client id: f_00008-7-3 loss: 0.757041  [  128/  130]
train() client id: f_00008-8-0 loss: 0.678037  [   32/  130]
train() client id: f_00008-8-1 loss: 0.779263  [   64/  130]
train() client id: f_00008-8-2 loss: 0.765463  [   96/  130]
train() client id: f_00008-8-3 loss: 0.726141  [  128/  130]
train() client id: f_00008-9-0 loss: 0.778882  [   32/  130]
train() client id: f_00008-9-1 loss: 0.678889  [   64/  130]
train() client id: f_00008-9-2 loss: 0.742413  [   96/  130]
train() client id: f_00008-9-3 loss: 0.720154  [  128/  130]
train() client id: f_00008-10-0 loss: 0.725421  [   32/  130]
train() client id: f_00008-10-1 loss: 0.817410  [   64/  130]
train() client id: f_00008-10-2 loss: 0.666627  [   96/  130]
train() client id: f_00008-10-3 loss: 0.718732  [  128/  130]
train() client id: f_00008-11-0 loss: 0.768584  [   32/  130]
train() client id: f_00008-11-1 loss: 0.798226  [   64/  130]
train() client id: f_00008-11-2 loss: 0.787116  [   96/  130]
train() client id: f_00008-11-3 loss: 0.582630  [  128/  130]
train() client id: f_00008-12-0 loss: 0.687526  [   32/  130]
train() client id: f_00008-12-1 loss: 0.726307  [   64/  130]
train() client id: f_00008-12-2 loss: 0.772583  [   96/  130]
train() client id: f_00008-12-3 loss: 0.733839  [  128/  130]
train() client id: f_00008-13-0 loss: 0.783850  [   32/  130]
train() client id: f_00008-13-1 loss: 0.680089  [   64/  130]
train() client id: f_00008-13-2 loss: 0.732898  [   96/  130]
train() client id: f_00008-13-3 loss: 0.735904  [  128/  130]
train() client id: f_00008-14-0 loss: 0.723960  [   32/  130]
train() client id: f_00008-14-1 loss: 0.776433  [   64/  130]
train() client id: f_00008-14-2 loss: 0.741212  [   96/  130]
train() client id: f_00008-14-3 loss: 0.678299  [  128/  130]
train() client id: f_00008-15-0 loss: 0.715039  [   32/  130]
train() client id: f_00008-15-1 loss: 0.757610  [   64/  130]
train() client id: f_00008-15-2 loss: 0.680731  [   96/  130]
train() client id: f_00008-15-3 loss: 0.754617  [  128/  130]
train() client id: f_00009-0-0 loss: 1.228850  [   32/  118]
train() client id: f_00009-0-1 loss: 1.137498  [   64/  118]
train() client id: f_00009-0-2 loss: 1.096397  [   96/  118]
train() client id: f_00009-1-0 loss: 0.979205  [   32/  118]
train() client id: f_00009-1-1 loss: 1.163825  [   64/  118]
train() client id: f_00009-1-2 loss: 0.998511  [   96/  118]
train() client id: f_00009-2-0 loss: 1.171068  [   32/  118]
train() client id: f_00009-2-1 loss: 1.187513  [   64/  118]
train() client id: f_00009-2-2 loss: 0.886864  [   96/  118]
train() client id: f_00009-3-0 loss: 1.162602  [   32/  118]
train() client id: f_00009-3-1 loss: 0.935112  [   64/  118]
train() client id: f_00009-3-2 loss: 0.832845  [   96/  118]
train() client id: f_00009-4-0 loss: 0.931381  [   32/  118]
train() client id: f_00009-4-1 loss: 0.929261  [   64/  118]
train() client id: f_00009-4-2 loss: 0.970624  [   96/  118]
train() client id: f_00009-5-0 loss: 0.954517  [   32/  118]
train() client id: f_00009-5-1 loss: 0.738948  [   64/  118]
train() client id: f_00009-5-2 loss: 0.895933  [   96/  118]
train() client id: f_00009-6-0 loss: 0.841744  [   32/  118]
train() client id: f_00009-6-1 loss: 1.011810  [   64/  118]
train() client id: f_00009-6-2 loss: 0.768509  [   96/  118]
train() client id: f_00009-7-0 loss: 0.862728  [   32/  118]
train() client id: f_00009-7-1 loss: 0.893730  [   64/  118]
train() client id: f_00009-7-2 loss: 0.765801  [   96/  118]
train() client id: f_00009-8-0 loss: 0.945457  [   32/  118]
train() client id: f_00009-8-1 loss: 0.673440  [   64/  118]
train() client id: f_00009-8-2 loss: 0.803476  [   96/  118]
train() client id: f_00009-9-0 loss: 0.751400  [   32/  118]
train() client id: f_00009-9-1 loss: 0.763444  [   64/  118]
train() client id: f_00009-9-2 loss: 0.830790  [   96/  118]
train() client id: f_00009-10-0 loss: 0.755798  [   32/  118]
train() client id: f_00009-10-1 loss: 0.702819  [   64/  118]
train() client id: f_00009-10-2 loss: 0.768115  [   96/  118]
train() client id: f_00009-11-0 loss: 0.611094  [   32/  118]
train() client id: f_00009-11-1 loss: 0.844270  [   64/  118]
train() client id: f_00009-11-2 loss: 0.836414  [   96/  118]
train() client id: f_00009-12-0 loss: 0.804080  [   32/  118]
train() client id: f_00009-12-1 loss: 0.887454  [   64/  118]
train() client id: f_00009-12-2 loss: 0.648813  [   96/  118]
train() client id: f_00009-13-0 loss: 0.739760  [   32/  118]
train() client id: f_00009-13-1 loss: 0.752937  [   64/  118]
train() client id: f_00009-13-2 loss: 0.727081  [   96/  118]
train() client id: f_00009-14-0 loss: 0.835075  [   32/  118]
train() client id: f_00009-14-1 loss: 0.635056  [   64/  118]
train() client id: f_00009-14-2 loss: 0.809576  [   96/  118]
train() client id: f_00009-15-0 loss: 0.821974  [   32/  118]
train() client id: f_00009-15-1 loss: 0.725817  [   64/  118]
train() client id: f_00009-15-2 loss: 0.703112  [   96/  118]
At round 48 accuracy: 0.6445623342175066
At round 48 training accuracy: 0.5895372233400402
At round 48 training loss: 0.8307884480014673
update_location
xs = 8.927491 361.223621 5.882650 0.934260 -277.581990 -125.230757 -85.849135 -5.143845 -300.120581 20.134486 
ys = -352.390647 7.291448 250.684448 -72.290817 -9.642386 0.794442 -1.381692 246.628436 25.881276 -787.232496 
xs mean: -39.68237997052122
ys mean: -69.16579882624052
dists_uav = 366.413521 374.880874 269.957956 123.397063 295.202874 160.260331 131.802819 266.180474 317.399124 793.813832 
uav_gains = -120.006936 -120.393724 -113.066559 -102.282937 -115.324863 -105.131430 -102.998910 -112.723574 -117.096311 -129.459550 
uav_gains_db_mean: -113.84847951746849
dists_bs = 558.542941 561.837993 196.078221 303.489064 211.224704 181.175478 197.631926 184.341385 194.657529 981.819150 
bs_gains = -116.484643 -116.556170 -103.755081 -109.067064 -104.659911 -102.793843 -103.851058 -103.004499 -103.666653 -123.343941 
bs_gains_db_mean: -108.71828634217991
Round 49
-------------------------------
ene_coms = [0.02782286 0.02929177 0.00746285 0.00696121 0.00780782 0.00800019
 0.00719664 0.00719854 0.00743073 0.04528554]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.69534651  9.53867164  4.39149726  1.57320747 10.71494559  5.16157643
  1.95786441  6.2981862   4.59946183  4.61861491]
obj_prev = 53.54937225954528
eta_min = 5.737789311222622e-21	eta_max = 0.8106538671616837
af = 11.129531482767057	bf = 1.7604919684951459	zeta = 12.242484631043764	eta = 0.909090909090909
af = 11.129531482767057	bf = 1.7604919684951459	zeta = 31.03374576781121	eta = 0.3586267531491741
af = 11.129531482767057	bf = 1.7604919684951459	zeta = 20.53956469846074	eta = 0.5418581964203515
af = 11.129531482767057	bf = 1.7604919684951459	zeta = 18.72809423081896	eta = 0.594269301809273
af = 11.129531482767057	bf = 1.7604919684951459	zeta = 18.61469378728275	eta = 0.5978895817437818
af = 11.129531482767057	bf = 1.7604919684951459	zeta = 18.614184337401365	eta = 0.5979059453281849
af = 11.129531482767057	bf = 1.7604919684951459	zeta = 18.614184327035197	eta = 0.5979059456611564
eta = 0.5979059456611564
ene_coms = [0.02782286 0.02929177 0.00746285 0.00696121 0.00780782 0.00800019
 0.00719664 0.00719854 0.00743073 0.04528554]
ene_comp = [0.0424406  0.08926    0.04176694 0.0144837  0.10307004 0.04917721
 0.01818883 0.0602926  0.04378792 0.03974596]
ene_total = [1.99170526 3.36049757 1.39547986 0.60788256 3.14297095 1.62076458
 0.7195828  1.9131205  1.45185654 2.41032371]
ti_comp = [0.69206393 0.67737476 0.89566397 0.90068042 0.89221427 0.89029057
 0.89832608 0.89830709 0.89598519 0.51743712]
ti_coms = [0.27822858 0.29291774 0.07462854 0.06961208 0.07807823 0.08000193
 0.07196642 0.07198541 0.07430731 0.45285539]
t_total = [27.50414848 27.50414848 27.50414848 27.50414848 27.50414848 27.50414848
 27.50414848 27.50414848 27.50414848 27.50414848]
ene_coms = [0.02782286 0.02929177 0.00746285 0.00696121 0.00780782 0.00800019
 0.00719664 0.00719854 0.00743073 0.04528554]
ene_comp = [9.97544442e-06 9.68707459e-05 5.67659475e-06 2.34086822e-07
 8.59684294e-05 9.37795325e-06 4.66043335e-07 1.69754963e-05
 6.53644238e-06 1.46569602e-05]
ene_total = [0.78895629 0.83305772 0.21170478 0.19733075 0.22375935 0.22704127
 0.20401098 0.2045328  0.21081861 1.28409042]
optimize_network iter = 0 obj = 4.385302980709291
eta = 0.5979059456611564
freqs = [30662343.57480779 65886718.26007664 23316190.47898632  8040420.2128352
 57760809.95466749 27618627.67513246 10123734.85703241 33559015.83340571
 24435629.63431644 38406562.78856418]
eta_min = 0.597905945661157	eta_max = 0.5979059456611544
af = 0.0054679510359059635	bf = 1.7604919684951459	zeta = 0.006014746139496561	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02782286 0.02929177 0.00746285 0.00696121 0.00780782 0.00800019
 0.00719664 0.00719854 0.00743073 0.04528554]
ene_comp = [1.84857295e-06 1.79513446e-05 1.05194306e-06 4.33791768e-08
 1.59310108e-05 1.73785046e-06 8.63635810e-08 3.14576894e-06
 1.21128343e-06 2.71611558e-06]
ene_total = [3.48856206 3.67474847 0.93579775 0.87277682 0.98091432 1.00325349
 0.90230003 0.90292174 0.93179037 5.67807925]
ti_comp = [0.69206393 0.67737476 0.89566397 0.90068042 0.89221427 0.89029057
 0.89832608 0.89830709 0.89598519 0.51743712]
ti_coms = [0.27822858 0.29291774 0.07462854 0.06961208 0.07807823 0.08000193
 0.07196642 0.07198541 0.07430731 0.45285539]
t_total = [27.50414848 27.50414848 27.50414848 27.50414848 27.50414848 27.50414848
 27.50414848 27.50414848 27.50414848 27.50414848]
ene_coms = [0.02782286 0.02929177 0.00746285 0.00696121 0.00780782 0.00800019
 0.00719664 0.00719854 0.00743073 0.04528554]
ene_comp = [9.97544442e-06 9.68707459e-05 5.67659475e-06 2.34086822e-07
 8.59684294e-05 9.37795325e-06 4.66043335e-07 1.69754963e-05
 6.53644238e-06 1.46569602e-05]
ene_total = [0.78895629 0.83305772 0.21170478 0.19733075 0.22375935 0.22704127
 0.20401098 0.2045328  0.21081861 1.28409042]
optimize_network iter = 1 obj = 4.38530298070927
eta = 0.5979059456611544
freqs = [30662343.57480777 65886718.26007658 23316190.47898634  8040420.21283521
 57760809.95466755 27618627.67513249 10123734.85703241 33559015.83340574
 24435629.63431647 38406562.78856407]
Done!
ene_coms = [0.02782286 0.02929177 0.00746285 0.00696121 0.00780782 0.00800019
 0.00719664 0.00719854 0.00743073 0.04528554]
ene_comp = [9.47700748e-06 9.20304644e-05 5.39295581e-06 2.22390349e-07
 8.16728974e-05 8.90937079e-06 4.42756832e-07 1.61272920e-05
 6.20983995e-06 1.39246048e-05]
ene_total = [0.02783233 0.0293838  0.00746825 0.00696143 0.0078895  0.0080091
 0.00719708 0.00721467 0.00743694 0.04529946]
At round 49 energy consumption: 0.15469257379067514
At round 49 eta: 0.5979059456611544
At round 49 a_n: 11.397856354757828
At round 49 local rounds: 16.841509419144327
At round 49 global rounds: 28.346244446461842
gradient difference: 0.36439764499664307
train() client id: f_00000-0-0 loss: 1.143075  [   32/  126]
train() client id: f_00000-0-1 loss: 1.621833  [   64/  126]
train() client id: f_00000-0-2 loss: 1.405057  [   96/  126]
train() client id: f_00000-1-0 loss: 1.274385  [   32/  126]
train() client id: f_00000-1-1 loss: 1.396483  [   64/  126]
train() client id: f_00000-1-2 loss: 1.230573  [   96/  126]
train() client id: f_00000-2-0 loss: 1.377819  [   32/  126]
train() client id: f_00000-2-1 loss: 1.212738  [   64/  126]
train() client id: f_00000-2-2 loss: 1.117386  [   96/  126]
train() client id: f_00000-3-0 loss: 1.280849  [   32/  126]
train() client id: f_00000-3-1 loss: 1.057095  [   64/  126]
train() client id: f_00000-3-2 loss: 1.015498  [   96/  126]
train() client id: f_00000-4-0 loss: 1.102966  [   32/  126]
train() client id: f_00000-4-1 loss: 1.057990  [   64/  126]
train() client id: f_00000-4-2 loss: 1.078913  [   96/  126]
train() client id: f_00000-5-0 loss: 1.001310  [   32/  126]
train() client id: f_00000-5-1 loss: 0.935473  [   64/  126]
train() client id: f_00000-5-2 loss: 0.986910  [   96/  126]
train() client id: f_00000-6-0 loss: 0.964324  [   32/  126]
train() client id: f_00000-6-1 loss: 0.927676  [   64/  126]
train() client id: f_00000-6-2 loss: 0.854906  [   96/  126]
train() client id: f_00000-7-0 loss: 0.948138  [   32/  126]
train() client id: f_00000-7-1 loss: 0.945758  [   64/  126]
train() client id: f_00000-7-2 loss: 0.853372  [   96/  126]
train() client id: f_00000-8-0 loss: 1.010014  [   32/  126]
train() client id: f_00000-8-1 loss: 0.849223  [   64/  126]
train() client id: f_00000-8-2 loss: 0.728976  [   96/  126]
train() client id: f_00000-9-0 loss: 0.812148  [   32/  126]
train() client id: f_00000-9-1 loss: 0.722369  [   64/  126]
train() client id: f_00000-9-2 loss: 0.831407  [   96/  126]
train() client id: f_00000-10-0 loss: 0.855478  [   32/  126]
train() client id: f_00000-10-1 loss: 0.891508  [   64/  126]
train() client id: f_00000-10-2 loss: 0.717956  [   96/  126]
train() client id: f_00000-11-0 loss: 0.849016  [   32/  126]
train() client id: f_00000-11-1 loss: 0.842187  [   64/  126]
train() client id: f_00000-11-2 loss: 0.790453  [   96/  126]
train() client id: f_00000-12-0 loss: 0.680455  [   32/  126]
train() client id: f_00000-12-1 loss: 0.935991  [   64/  126]
train() client id: f_00000-12-2 loss: 0.834544  [   96/  126]
train() client id: f_00000-13-0 loss: 0.892104  [   32/  126]
train() client id: f_00000-13-1 loss: 0.670932  [   64/  126]
train() client id: f_00000-13-2 loss: 0.762371  [   96/  126]
train() client id: f_00000-14-0 loss: 0.841093  [   32/  126]
train() client id: f_00000-14-1 loss: 0.678112  [   64/  126]
train() client id: f_00000-14-2 loss: 0.816055  [   96/  126]
train() client id: f_00000-15-0 loss: 0.744122  [   32/  126]
train() client id: f_00000-15-1 loss: 0.733563  [   64/  126]
train() client id: f_00000-15-2 loss: 0.734453  [   96/  126]
train() client id: f_00001-0-0 loss: 0.317915  [   32/  265]
train() client id: f_00001-0-1 loss: 0.455308  [   64/  265]
train() client id: f_00001-0-2 loss: 0.390307  [   96/  265]
train() client id: f_00001-0-3 loss: 0.377775  [  128/  265]
train() client id: f_00001-0-4 loss: 0.461186  [  160/  265]
train() client id: f_00001-0-5 loss: 0.398647  [  192/  265]
train() client id: f_00001-0-6 loss: 0.436084  [  224/  265]
train() client id: f_00001-0-7 loss: 0.422375  [  256/  265]
train() client id: f_00001-1-0 loss: 0.569290  [   32/  265]
train() client id: f_00001-1-1 loss: 0.447010  [   64/  265]
train() client id: f_00001-1-2 loss: 0.298127  [   96/  265]
train() client id: f_00001-1-3 loss: 0.395654  [  128/  265]
train() client id: f_00001-1-4 loss: 0.326998  [  160/  265]
train() client id: f_00001-1-5 loss: 0.364657  [  192/  265]
train() client id: f_00001-1-6 loss: 0.327826  [  224/  265]
train() client id: f_00001-1-7 loss: 0.438529  [  256/  265]
train() client id: f_00001-2-0 loss: 0.503148  [   32/  265]
train() client id: f_00001-2-1 loss: 0.478364  [   64/  265]
train() client id: f_00001-2-2 loss: 0.304902  [   96/  265]
train() client id: f_00001-2-3 loss: 0.300722  [  128/  265]
train() client id: f_00001-2-4 loss: 0.346124  [  160/  265]
train() client id: f_00001-2-5 loss: 0.292426  [  192/  265]
train() client id: f_00001-2-6 loss: 0.423025  [  224/  265]
train() client id: f_00001-2-7 loss: 0.401428  [  256/  265]
train() client id: f_00001-3-0 loss: 0.486044  [   32/  265]
train() client id: f_00001-3-1 loss: 0.358234  [   64/  265]
train() client id: f_00001-3-2 loss: 0.322711  [   96/  265]
train() client id: f_00001-3-3 loss: 0.389065  [  128/  265]
train() client id: f_00001-3-4 loss: 0.376755  [  160/  265]
train() client id: f_00001-3-5 loss: 0.412466  [  192/  265]
train() client id: f_00001-3-6 loss: 0.322165  [  224/  265]
train() client id: f_00001-3-7 loss: 0.381671  [  256/  265]
train() client id: f_00001-4-0 loss: 0.392333  [   32/  265]
train() client id: f_00001-4-1 loss: 0.459268  [   64/  265]
train() client id: f_00001-4-2 loss: 0.419277  [   96/  265]
train() client id: f_00001-4-3 loss: 0.350503  [  128/  265]
train() client id: f_00001-4-4 loss: 0.334385  [  160/  265]
train() client id: f_00001-4-5 loss: 0.343726  [  192/  265]
train() client id: f_00001-4-6 loss: 0.343239  [  224/  265]
train() client id: f_00001-4-7 loss: 0.362439  [  256/  265]
train() client id: f_00001-5-0 loss: 0.318899  [   32/  265]
train() client id: f_00001-5-1 loss: 0.361255  [   64/  265]
train() client id: f_00001-5-2 loss: 0.371648  [   96/  265]
train() client id: f_00001-5-3 loss: 0.328481  [  128/  265]
train() client id: f_00001-5-4 loss: 0.423762  [  160/  265]
train() client id: f_00001-5-5 loss: 0.368306  [  192/  265]
train() client id: f_00001-5-6 loss: 0.444607  [  224/  265]
train() client id: f_00001-5-7 loss: 0.361258  [  256/  265]
train() client id: f_00001-6-0 loss: 0.335166  [   32/  265]
train() client id: f_00001-6-1 loss: 0.438185  [   64/  265]
train() client id: f_00001-6-2 loss: 0.447511  [   96/  265]
train() client id: f_00001-6-3 loss: 0.351846  [  128/  265]
train() client id: f_00001-6-4 loss: 0.374639  [  160/  265]
train() client id: f_00001-6-5 loss: 0.313845  [  192/  265]
train() client id: f_00001-6-6 loss: 0.424095  [  224/  265]
train() client id: f_00001-6-7 loss: 0.261898  [  256/  265]
train() client id: f_00001-7-0 loss: 0.511451  [   32/  265]
train() client id: f_00001-7-1 loss: 0.325884  [   64/  265]
train() client id: f_00001-7-2 loss: 0.279060  [   96/  265]
train() client id: f_00001-7-3 loss: 0.292838  [  128/  265]
train() client id: f_00001-7-4 loss: 0.417013  [  160/  265]
train() client id: f_00001-7-5 loss: 0.463632  [  192/  265]
train() client id: f_00001-7-6 loss: 0.322254  [  224/  265]
train() client id: f_00001-7-7 loss: 0.315089  [  256/  265]
train() client id: f_00001-8-0 loss: 0.353095  [   32/  265]
train() client id: f_00001-8-1 loss: 0.332571  [   64/  265]
train() client id: f_00001-8-2 loss: 0.371819  [   96/  265]
train() client id: f_00001-8-3 loss: 0.407528  [  128/  265]
train() client id: f_00001-8-4 loss: 0.418087  [  160/  265]
train() client id: f_00001-8-5 loss: 0.278845  [  192/  265]
train() client id: f_00001-8-6 loss: 0.338282  [  224/  265]
train() client id: f_00001-8-7 loss: 0.416128  [  256/  265]
train() client id: f_00001-9-0 loss: 0.347397  [   32/  265]
train() client id: f_00001-9-1 loss: 0.416802  [   64/  265]
train() client id: f_00001-9-2 loss: 0.341567  [   96/  265]
train() client id: f_00001-9-3 loss: 0.245107  [  128/  265]
train() client id: f_00001-9-4 loss: 0.333332  [  160/  265]
train() client id: f_00001-9-5 loss: 0.398930  [  192/  265]
train() client id: f_00001-9-6 loss: 0.376551  [  224/  265]
train() client id: f_00001-9-7 loss: 0.444501  [  256/  265]
train() client id: f_00001-10-0 loss: 0.441338  [   32/  265]
train() client id: f_00001-10-1 loss: 0.340796  [   64/  265]
train() client id: f_00001-10-2 loss: 0.318373  [   96/  265]
train() client id: f_00001-10-3 loss: 0.305313  [  128/  265]
train() client id: f_00001-10-4 loss: 0.386495  [  160/  265]
train() client id: f_00001-10-5 loss: 0.447180  [  192/  265]
train() client id: f_00001-10-6 loss: 0.337791  [  224/  265]
train() client id: f_00001-10-7 loss: 0.264884  [  256/  265]
train() client id: f_00001-11-0 loss: 0.333839  [   32/  265]
train() client id: f_00001-11-1 loss: 0.279377  [   64/  265]
train() client id: f_00001-11-2 loss: 0.332653  [   96/  265]
train() client id: f_00001-11-3 loss: 0.362425  [  128/  265]
train() client id: f_00001-11-4 loss: 0.256758  [  160/  265]
train() client id: f_00001-11-5 loss: 0.513501  [  192/  265]
train() client id: f_00001-11-6 loss: 0.343845  [  224/  265]
train() client id: f_00001-11-7 loss: 0.451314  [  256/  265]
train() client id: f_00001-12-0 loss: 0.283056  [   32/  265]
train() client id: f_00001-12-1 loss: 0.301083  [   64/  265]
train() client id: f_00001-12-2 loss: 0.336373  [   96/  265]
train() client id: f_00001-12-3 loss: 0.368826  [  128/  265]
train() client id: f_00001-12-4 loss: 0.339196  [  160/  265]
train() client id: f_00001-12-5 loss: 0.316770  [  192/  265]
train() client id: f_00001-12-6 loss: 0.465993  [  224/  265]
train() client id: f_00001-12-7 loss: 0.321862  [  256/  265]
train() client id: f_00001-13-0 loss: 0.338550  [   32/  265]
train() client id: f_00001-13-1 loss: 0.370609  [   64/  265]
train() client id: f_00001-13-2 loss: 0.274916  [   96/  265]
train() client id: f_00001-13-3 loss: 0.425714  [  128/  265]
train() client id: f_00001-13-4 loss: 0.355344  [  160/  265]
train() client id: f_00001-13-5 loss: 0.336757  [  192/  265]
train() client id: f_00001-13-6 loss: 0.325637  [  224/  265]
train() client id: f_00001-13-7 loss: 0.439130  [  256/  265]
train() client id: f_00001-14-0 loss: 0.349668  [   32/  265]
train() client id: f_00001-14-1 loss: 0.432099  [   64/  265]
train() client id: f_00001-14-2 loss: 0.265709  [   96/  265]
train() client id: f_00001-14-3 loss: 0.330703  [  128/  265]
train() client id: f_00001-14-4 loss: 0.361299  [  160/  265]
train() client id: f_00001-14-5 loss: 0.321744  [  192/  265]
train() client id: f_00001-14-6 loss: 0.407524  [  224/  265]
train() client id: f_00001-14-7 loss: 0.345117  [  256/  265]
train() client id: f_00001-15-0 loss: 0.329313  [   32/  265]
train() client id: f_00001-15-1 loss: 0.353474  [   64/  265]
train() client id: f_00001-15-2 loss: 0.257127  [   96/  265]
train() client id: f_00001-15-3 loss: 0.411996  [  128/  265]
train() client id: f_00001-15-4 loss: 0.310793  [  160/  265]
train() client id: f_00001-15-5 loss: 0.389287  [  192/  265]
train() client id: f_00001-15-6 loss: 0.420223  [  224/  265]
train() client id: f_00001-15-7 loss: 0.298679  [  256/  265]
train() client id: f_00002-0-0 loss: 0.964413  [   32/  124]
train() client id: f_00002-0-1 loss: 0.888732  [   64/  124]
train() client id: f_00002-0-2 loss: 0.792037  [   96/  124]
train() client id: f_00002-1-0 loss: 0.962683  [   32/  124]
train() client id: f_00002-1-1 loss: 0.861636  [   64/  124]
train() client id: f_00002-1-2 loss: 0.817692  [   96/  124]
train() client id: f_00002-2-0 loss: 0.788229  [   32/  124]
train() client id: f_00002-2-1 loss: 0.751109  [   64/  124]
train() client id: f_00002-2-2 loss: 0.782274  [   96/  124]
train() client id: f_00002-3-0 loss: 0.561629  [   32/  124]
train() client id: f_00002-3-1 loss: 0.705101  [   64/  124]
train() client id: f_00002-3-2 loss: 0.944288  [   96/  124]
train() client id: f_00002-4-0 loss: 0.654319  [   32/  124]
train() client id: f_00002-4-1 loss: 0.613735  [   64/  124]
train() client id: f_00002-4-2 loss: 0.828166  [   96/  124]
train() client id: f_00002-5-0 loss: 0.848565  [   32/  124]
train() client id: f_00002-5-1 loss: 0.654653  [   64/  124]
train() client id: f_00002-5-2 loss: 0.790810  [   96/  124]
train() client id: f_00002-6-0 loss: 0.563084  [   32/  124]
train() client id: f_00002-6-1 loss: 0.559295  [   64/  124]
train() client id: f_00002-6-2 loss: 0.983562  [   96/  124]
train() client id: f_00002-7-0 loss: 0.619802  [   32/  124]
train() client id: f_00002-7-1 loss: 0.604755  [   64/  124]
train() client id: f_00002-7-2 loss: 0.674281  [   96/  124]
train() client id: f_00002-8-0 loss: 0.627438  [   32/  124]
train() client id: f_00002-8-1 loss: 0.660673  [   64/  124]
train() client id: f_00002-8-2 loss: 0.408739  [   96/  124]
train() client id: f_00002-9-0 loss: 0.864727  [   32/  124]
train() client id: f_00002-9-1 loss: 0.626898  [   64/  124]
train() client id: f_00002-9-2 loss: 0.373360  [   96/  124]
train() client id: f_00002-10-0 loss: 0.495787  [   32/  124]
train() client id: f_00002-10-1 loss: 0.630359  [   64/  124]
train() client id: f_00002-10-2 loss: 0.630287  [   96/  124]
train() client id: f_00002-11-0 loss: 0.485226  [   32/  124]
train() client id: f_00002-11-1 loss: 0.765580  [   64/  124]
train() client id: f_00002-11-2 loss: 0.705115  [   96/  124]
train() client id: f_00002-12-0 loss: 0.416321  [   32/  124]
train() client id: f_00002-12-1 loss: 0.625715  [   64/  124]
train() client id: f_00002-12-2 loss: 0.639194  [   96/  124]
train() client id: f_00002-13-0 loss: 0.562086  [   32/  124]
train() client id: f_00002-13-1 loss: 0.594839  [   64/  124]
train() client id: f_00002-13-2 loss: 0.611659  [   96/  124]
train() client id: f_00002-14-0 loss: 0.476349  [   32/  124]
train() client id: f_00002-14-1 loss: 0.743096  [   64/  124]
train() client id: f_00002-14-2 loss: 0.686247  [   96/  124]
train() client id: f_00002-15-0 loss: 0.520076  [   32/  124]
train() client id: f_00002-15-1 loss: 0.411133  [   64/  124]
train() client id: f_00002-15-2 loss: 0.712264  [   96/  124]
train() client id: f_00003-0-0 loss: 0.583881  [   32/   43]
train() client id: f_00003-1-0 loss: 0.483183  [   32/   43]
train() client id: f_00003-2-0 loss: 0.724218  [   32/   43]
train() client id: f_00003-3-0 loss: 0.773191  [   32/   43]
train() client id: f_00003-4-0 loss: 0.798342  [   32/   43]
train() client id: f_00003-5-0 loss: 0.870728  [   32/   43]
train() client id: f_00003-6-0 loss: 0.596040  [   32/   43]
train() client id: f_00003-7-0 loss: 0.619695  [   32/   43]
train() client id: f_00003-8-0 loss: 0.722396  [   32/   43]
train() client id: f_00003-9-0 loss: 0.629719  [   32/   43]
train() client id: f_00003-10-0 loss: 0.682786  [   32/   43]
train() client id: f_00003-11-0 loss: 0.782234  [   32/   43]
train() client id: f_00003-12-0 loss: 0.555573  [   32/   43]
train() client id: f_00003-13-0 loss: 0.636911  [   32/   43]
train() client id: f_00003-14-0 loss: 0.654667  [   32/   43]
train() client id: f_00003-15-0 loss: 0.679658  [   32/   43]
train() client id: f_00004-0-0 loss: 0.869964  [   32/  306]
train() client id: f_00004-0-1 loss: 0.922026  [   64/  306]
train() client id: f_00004-0-2 loss: 1.029431  [   96/  306]
train() client id: f_00004-0-3 loss: 0.894663  [  128/  306]
train() client id: f_00004-0-4 loss: 1.113287  [  160/  306]
train() client id: f_00004-0-5 loss: 0.910432  [  192/  306]
train() client id: f_00004-0-6 loss: 0.988457  [  224/  306]
train() client id: f_00004-0-7 loss: 1.128646  [  256/  306]
train() client id: f_00004-0-8 loss: 0.912869  [  288/  306]
train() client id: f_00004-1-0 loss: 0.848192  [   32/  306]
train() client id: f_00004-1-1 loss: 1.021534  [   64/  306]
train() client id: f_00004-1-2 loss: 0.996491  [   96/  306]
train() client id: f_00004-1-3 loss: 0.976984  [  128/  306]
train() client id: f_00004-1-4 loss: 0.831603  [  160/  306]
train() client id: f_00004-1-5 loss: 1.001326  [  192/  306]
train() client id: f_00004-1-6 loss: 1.001058  [  224/  306]
train() client id: f_00004-1-7 loss: 1.008414  [  256/  306]
train() client id: f_00004-1-8 loss: 0.977795  [  288/  306]
train() client id: f_00004-2-0 loss: 0.910415  [   32/  306]
train() client id: f_00004-2-1 loss: 0.895556  [   64/  306]
train() client id: f_00004-2-2 loss: 1.051809  [   96/  306]
train() client id: f_00004-2-3 loss: 0.929919  [  128/  306]
train() client id: f_00004-2-4 loss: 0.901431  [  160/  306]
train() client id: f_00004-2-5 loss: 1.034456  [  192/  306]
train() client id: f_00004-2-6 loss: 0.980314  [  224/  306]
train() client id: f_00004-2-7 loss: 0.968929  [  256/  306]
train() client id: f_00004-2-8 loss: 1.032418  [  288/  306]
train() client id: f_00004-3-0 loss: 1.040878  [   32/  306]
train() client id: f_00004-3-1 loss: 1.008845  [   64/  306]
train() client id: f_00004-3-2 loss: 1.055742  [   96/  306]
train() client id: f_00004-3-3 loss: 0.947603  [  128/  306]
train() client id: f_00004-3-4 loss: 1.104406  [  160/  306]
train() client id: f_00004-3-5 loss: 0.916687  [  192/  306]
train() client id: f_00004-3-6 loss: 0.859774  [  224/  306]
train() client id: f_00004-3-7 loss: 0.915517  [  256/  306]
train() client id: f_00004-3-8 loss: 0.862848  [  288/  306]
train() client id: f_00004-4-0 loss: 0.899031  [   32/  306]
train() client id: f_00004-4-1 loss: 1.116409  [   64/  306]
train() client id: f_00004-4-2 loss: 1.014067  [   96/  306]
train() client id: f_00004-4-3 loss: 1.075494  [  128/  306]
train() client id: f_00004-4-4 loss: 0.954095  [  160/  306]
train() client id: f_00004-4-5 loss: 0.992696  [  192/  306]
train() client id: f_00004-4-6 loss: 0.817301  [  224/  306]
train() client id: f_00004-4-7 loss: 0.784848  [  256/  306]
train() client id: f_00004-4-8 loss: 1.021112  [  288/  306]
train() client id: f_00004-5-0 loss: 0.902465  [   32/  306]
train() client id: f_00004-5-1 loss: 1.114741  [   64/  306]
train() client id: f_00004-5-2 loss: 0.933983  [   96/  306]
train() client id: f_00004-5-3 loss: 0.951055  [  128/  306]
train() client id: f_00004-5-4 loss: 0.988003  [  160/  306]
train() client id: f_00004-5-5 loss: 0.799401  [  192/  306]
train() client id: f_00004-5-6 loss: 0.901582  [  224/  306]
train() client id: f_00004-5-7 loss: 1.034886  [  256/  306]
train() client id: f_00004-5-8 loss: 0.900357  [  288/  306]
train() client id: f_00004-6-0 loss: 0.967349  [   32/  306]
train() client id: f_00004-6-1 loss: 0.926661  [   64/  306]
train() client id: f_00004-6-2 loss: 0.877268  [   96/  306]
train() client id: f_00004-6-3 loss: 0.883938  [  128/  306]
train() client id: f_00004-6-4 loss: 0.919327  [  160/  306]
train() client id: f_00004-6-5 loss: 0.939451  [  192/  306]
train() client id: f_00004-6-6 loss: 1.032017  [  224/  306]
train() client id: f_00004-6-7 loss: 1.000937  [  256/  306]
train() client id: f_00004-6-8 loss: 1.047473  [  288/  306]
train() client id: f_00004-7-0 loss: 0.905725  [   32/  306]
train() client id: f_00004-7-1 loss: 0.883199  [   64/  306]
train() client id: f_00004-7-2 loss: 1.095371  [   96/  306]
train() client id: f_00004-7-3 loss: 0.888610  [  128/  306]
train() client id: f_00004-7-4 loss: 0.921597  [  160/  306]
train() client id: f_00004-7-5 loss: 0.911376  [  192/  306]
train() client id: f_00004-7-6 loss: 1.040683  [  224/  306]
train() client id: f_00004-7-7 loss: 0.920186  [  256/  306]
train() client id: f_00004-7-8 loss: 1.022993  [  288/  306]
train() client id: f_00004-8-0 loss: 0.884034  [   32/  306]
train() client id: f_00004-8-1 loss: 0.973450  [   64/  306]
train() client id: f_00004-8-2 loss: 0.879448  [   96/  306]
train() client id: f_00004-8-3 loss: 1.075563  [  128/  306]
train() client id: f_00004-8-4 loss: 0.869013  [  160/  306]
train() client id: f_00004-8-5 loss: 1.026914  [  192/  306]
train() client id: f_00004-8-6 loss: 0.864538  [  224/  306]
train() client id: f_00004-8-7 loss: 1.059601  [  256/  306]
train() client id: f_00004-8-8 loss: 0.931016  [  288/  306]
train() client id: f_00004-9-0 loss: 0.947291  [   32/  306]
train() client id: f_00004-9-1 loss: 0.938775  [   64/  306]
train() client id: f_00004-9-2 loss: 1.070781  [   96/  306]
train() client id: f_00004-9-3 loss: 0.970154  [  128/  306]
train() client id: f_00004-9-4 loss: 0.935960  [  160/  306]
train() client id: f_00004-9-5 loss: 0.949394  [  192/  306]
train() client id: f_00004-9-6 loss: 0.919408  [  224/  306]
train() client id: f_00004-9-7 loss: 0.902500  [  256/  306]
train() client id: f_00004-9-8 loss: 0.914160  [  288/  306]
train() client id: f_00004-10-0 loss: 1.019753  [   32/  306]
train() client id: f_00004-10-1 loss: 0.812441  [   64/  306]
train() client id: f_00004-10-2 loss: 0.865533  [   96/  306]
train() client id: f_00004-10-3 loss: 0.999581  [  128/  306]
train() client id: f_00004-10-4 loss: 0.983973  [  160/  306]
train() client id: f_00004-10-5 loss: 1.007719  [  192/  306]
train() client id: f_00004-10-6 loss: 0.879520  [  224/  306]
train() client id: f_00004-10-7 loss: 0.874268  [  256/  306]
train() client id: f_00004-10-8 loss: 1.018513  [  288/  306]
train() client id: f_00004-11-0 loss: 0.954939  [   32/  306]
train() client id: f_00004-11-1 loss: 0.852223  [   64/  306]
train() client id: f_00004-11-2 loss: 1.025986  [   96/  306]
train() client id: f_00004-11-3 loss: 0.937460  [  128/  306]
train() client id: f_00004-11-4 loss: 0.863335  [  160/  306]
train() client id: f_00004-11-5 loss: 0.949920  [  192/  306]
train() client id: f_00004-11-6 loss: 0.946613  [  224/  306]
train() client id: f_00004-11-7 loss: 0.983981  [  256/  306]
train() client id: f_00004-11-8 loss: 1.055546  [  288/  306]
train() client id: f_00004-12-0 loss: 0.881595  [   32/  306]
train() client id: f_00004-12-1 loss: 1.011778  [   64/  306]
train() client id: f_00004-12-2 loss: 0.986203  [   96/  306]
train() client id: f_00004-12-3 loss: 1.052014  [  128/  306]
train() client id: f_00004-12-4 loss: 0.949497  [  160/  306]
train() client id: f_00004-12-5 loss: 0.871038  [  192/  306]
train() client id: f_00004-12-6 loss: 0.898124  [  224/  306]
train() client id: f_00004-12-7 loss: 0.909806  [  256/  306]
train() client id: f_00004-12-8 loss: 0.972625  [  288/  306]
train() client id: f_00004-13-0 loss: 1.042473  [   32/  306]
train() client id: f_00004-13-1 loss: 0.945576  [   64/  306]
train() client id: f_00004-13-2 loss: 0.931436  [   96/  306]
train() client id: f_00004-13-3 loss: 0.846052  [  128/  306]
train() client id: f_00004-13-4 loss: 1.051580  [  160/  306]
train() client id: f_00004-13-5 loss: 0.997107  [  192/  306]
train() client id: f_00004-13-6 loss: 0.956551  [  224/  306]
train() client id: f_00004-13-7 loss: 0.815877  [  256/  306]
train() client id: f_00004-13-8 loss: 0.967036  [  288/  306]
train() client id: f_00004-14-0 loss: 0.916951  [   32/  306]
train() client id: f_00004-14-1 loss: 0.956784  [   64/  306]
train() client id: f_00004-14-2 loss: 0.850643  [   96/  306]
train() client id: f_00004-14-3 loss: 0.922234  [  128/  306]
train() client id: f_00004-14-4 loss: 0.957487  [  160/  306]
train() client id: f_00004-14-5 loss: 0.900561  [  192/  306]
train() client id: f_00004-14-6 loss: 0.996799  [  224/  306]
train() client id: f_00004-14-7 loss: 1.002886  [  256/  306]
train() client id: f_00004-14-8 loss: 1.019926  [  288/  306]
train() client id: f_00004-15-0 loss: 0.931218  [   32/  306]
train() client id: f_00004-15-1 loss: 0.994592  [   64/  306]
train() client id: f_00004-15-2 loss: 0.934166  [   96/  306]
train() client id: f_00004-15-3 loss: 0.915823  [  128/  306]
train() client id: f_00004-15-4 loss: 1.029841  [  160/  306]
train() client id: f_00004-15-5 loss: 0.855592  [  192/  306]
train() client id: f_00004-15-6 loss: 0.949705  [  224/  306]
train() client id: f_00004-15-7 loss: 1.063913  [  256/  306]
train() client id: f_00004-15-8 loss: 0.916674  [  288/  306]
train() client id: f_00005-0-0 loss: 0.587149  [   32/  146]
train() client id: f_00005-0-1 loss: 0.555196  [   64/  146]
train() client id: f_00005-0-2 loss: 0.597150  [   96/  146]
train() client id: f_00005-0-3 loss: 0.859239  [  128/  146]
train() client id: f_00005-1-0 loss: 0.616946  [   32/  146]
train() client id: f_00005-1-1 loss: 0.711700  [   64/  146]
train() client id: f_00005-1-2 loss: 0.473969  [   96/  146]
train() client id: f_00005-1-3 loss: 0.679695  [  128/  146]
train() client id: f_00005-2-0 loss: 0.547158  [   32/  146]
train() client id: f_00005-2-1 loss: 0.753181  [   64/  146]
train() client id: f_00005-2-2 loss: 0.472254  [   96/  146]
train() client id: f_00005-2-3 loss: 0.760509  [  128/  146]
train() client id: f_00005-3-0 loss: 0.740568  [   32/  146]
train() client id: f_00005-3-1 loss: 0.507018  [   64/  146]
train() client id: f_00005-3-2 loss: 0.733390  [   96/  146]
train() client id: f_00005-3-3 loss: 0.660501  [  128/  146]
train() client id: f_00005-4-0 loss: 0.533596  [   32/  146]
train() client id: f_00005-4-1 loss: 0.397573  [   64/  146]
train() client id: f_00005-4-2 loss: 0.820174  [   96/  146]
train() client id: f_00005-4-3 loss: 0.755022  [  128/  146]
train() client id: f_00005-5-0 loss: 0.641528  [   32/  146]
train() client id: f_00005-5-1 loss: 0.541091  [   64/  146]
train() client id: f_00005-5-2 loss: 0.606526  [   96/  146]
train() client id: f_00005-5-3 loss: 0.634157  [  128/  146]
train() client id: f_00005-6-0 loss: 0.484655  [   32/  146]
train() client id: f_00005-6-1 loss: 0.371772  [   64/  146]
train() client id: f_00005-6-2 loss: 0.835307  [   96/  146]
train() client id: f_00005-6-3 loss: 0.705717  [  128/  146]
train() client id: f_00005-7-0 loss: 0.811878  [   32/  146]
train() client id: f_00005-7-1 loss: 0.728441  [   64/  146]
train() client id: f_00005-7-2 loss: 0.325881  [   96/  146]
train() client id: f_00005-7-3 loss: 0.668990  [  128/  146]
train() client id: f_00005-8-0 loss: 0.432959  [   32/  146]
train() client id: f_00005-8-1 loss: 0.578079  [   64/  146]
train() client id: f_00005-8-2 loss: 0.643210  [   96/  146]
train() client id: f_00005-8-3 loss: 0.795635  [  128/  146]
train() client id: f_00005-9-0 loss: 0.375885  [   32/  146]
train() client id: f_00005-9-1 loss: 0.811800  [   64/  146]
train() client id: f_00005-9-2 loss: 0.614139  [   96/  146]
train() client id: f_00005-9-3 loss: 0.826566  [  128/  146]
train() client id: f_00005-10-0 loss: 0.918626  [   32/  146]
train() client id: f_00005-10-1 loss: 0.598035  [   64/  146]
train() client id: f_00005-10-2 loss: 0.467318  [   96/  146]
train() client id: f_00005-10-3 loss: 0.506652  [  128/  146]
train() client id: f_00005-11-0 loss: 0.796789  [   32/  146]
train() client id: f_00005-11-1 loss: 0.615264  [   64/  146]
train() client id: f_00005-11-2 loss: 0.788389  [   96/  146]
train() client id: f_00005-11-3 loss: 0.408994  [  128/  146]
train() client id: f_00005-12-0 loss: 0.927364  [   32/  146]
train() client id: f_00005-12-1 loss: 0.603520  [   64/  146]
train() client id: f_00005-12-2 loss: 0.576498  [   96/  146]
train() client id: f_00005-12-3 loss: 0.355115  [  128/  146]
train() client id: f_00005-13-0 loss: 0.563374  [   32/  146]
train() client id: f_00005-13-1 loss: 0.657166  [   64/  146]
train() client id: f_00005-13-2 loss: 0.678206  [   96/  146]
train() client id: f_00005-13-3 loss: 0.710736  [  128/  146]
train() client id: f_00005-14-0 loss: 0.881443  [   32/  146]
train() client id: f_00005-14-1 loss: 0.556240  [   64/  146]
train() client id: f_00005-14-2 loss: 0.581550  [   96/  146]
train() client id: f_00005-14-3 loss: 0.476395  [  128/  146]
train() client id: f_00005-15-0 loss: 0.421346  [   32/  146]
train() client id: f_00005-15-1 loss: 0.495301  [   64/  146]
train() client id: f_00005-15-2 loss: 0.639201  [   96/  146]
train() client id: f_00005-15-3 loss: 1.035366  [  128/  146]
train() client id: f_00006-0-0 loss: 0.513003  [   32/   54]
train() client id: f_00006-1-0 loss: 0.592816  [   32/   54]
train() client id: f_00006-2-0 loss: 0.467861  [   32/   54]
train() client id: f_00006-3-0 loss: 0.552790  [   32/   54]
train() client id: f_00006-4-0 loss: 0.550294  [   32/   54]
train() client id: f_00006-5-0 loss: 0.484437  [   32/   54]
train() client id: f_00006-6-0 loss: 0.492181  [   32/   54]
train() client id: f_00006-7-0 loss: 0.517790  [   32/   54]
train() client id: f_00006-8-0 loss: 0.540333  [   32/   54]
train() client id: f_00006-9-0 loss: 0.595118  [   32/   54]
train() client id: f_00006-10-0 loss: 0.479475  [   32/   54]
train() client id: f_00006-11-0 loss: 0.599690  [   32/   54]
train() client id: f_00006-12-0 loss: 0.509546  [   32/   54]
train() client id: f_00006-13-0 loss: 0.590481  [   32/   54]
train() client id: f_00006-14-0 loss: 0.580252  [   32/   54]
train() client id: f_00006-15-0 loss: 0.538145  [   32/   54]
train() client id: f_00007-0-0 loss: 0.617371  [   32/  179]
train() client id: f_00007-0-1 loss: 0.990527  [   64/  179]
train() client id: f_00007-0-2 loss: 0.763334  [   96/  179]
train() client id: f_00007-0-3 loss: 0.588627  [  128/  179]
train() client id: f_00007-0-4 loss: 0.585367  [  160/  179]
train() client id: f_00007-1-0 loss: 0.569069  [   32/  179]
train() client id: f_00007-1-1 loss: 0.602211  [   64/  179]
train() client id: f_00007-1-2 loss: 0.947687  [   96/  179]
train() client id: f_00007-1-3 loss: 0.694016  [  128/  179]
train() client id: f_00007-1-4 loss: 0.555422  [  160/  179]
train() client id: f_00007-2-0 loss: 0.596503  [   32/  179]
train() client id: f_00007-2-1 loss: 0.707046  [   64/  179]
train() client id: f_00007-2-2 loss: 0.563660  [   96/  179]
train() client id: f_00007-2-3 loss: 0.655244  [  128/  179]
train() client id: f_00007-2-4 loss: 0.886637  [  160/  179]
train() client id: f_00007-3-0 loss: 0.530386  [   32/  179]
train() client id: f_00007-3-1 loss: 0.566679  [   64/  179]
train() client id: f_00007-3-2 loss: 0.631602  [   96/  179]
train() client id: f_00007-3-3 loss: 0.625074  [  128/  179]
train() client id: f_00007-3-4 loss: 0.825331  [  160/  179]
train() client id: f_00007-4-0 loss: 0.457494  [   32/  179]
train() client id: f_00007-4-1 loss: 0.477233  [   64/  179]
train() client id: f_00007-4-2 loss: 0.656387  [   96/  179]
train() client id: f_00007-4-3 loss: 1.163309  [  128/  179]
train() client id: f_00007-4-4 loss: 0.606447  [  160/  179]
train() client id: f_00007-5-0 loss: 0.568273  [   32/  179]
train() client id: f_00007-5-1 loss: 0.670553  [   64/  179]
train() client id: f_00007-5-2 loss: 0.509591  [   96/  179]
train() client id: f_00007-5-3 loss: 0.491697  [  128/  179]
train() client id: f_00007-5-4 loss: 1.032264  [  160/  179]
train() client id: f_00007-6-0 loss: 0.564726  [   32/  179]
train() client id: f_00007-6-1 loss: 0.530188  [   64/  179]
train() client id: f_00007-6-2 loss: 0.738930  [   96/  179]
train() client id: f_00007-6-3 loss: 0.772635  [  128/  179]
train() client id: f_00007-6-4 loss: 0.653284  [  160/  179]
train() client id: f_00007-7-0 loss: 0.746934  [   32/  179]
train() client id: f_00007-7-1 loss: 0.616843  [   64/  179]
train() client id: f_00007-7-2 loss: 0.661577  [   96/  179]
train() client id: f_00007-7-3 loss: 0.553214  [  128/  179]
train() client id: f_00007-7-4 loss: 0.580964  [  160/  179]
train() client id: f_00007-8-0 loss: 0.758471  [   32/  179]
train() client id: f_00007-8-1 loss: 0.575575  [   64/  179]
train() client id: f_00007-8-2 loss: 0.499421  [   96/  179]
train() client id: f_00007-8-3 loss: 0.678367  [  128/  179]
train() client id: f_00007-8-4 loss: 0.714532  [  160/  179]
train() client id: f_00007-9-0 loss: 0.491561  [   32/  179]
train() client id: f_00007-9-1 loss: 0.770352  [   64/  179]
train() client id: f_00007-9-2 loss: 0.791490  [   96/  179]
train() client id: f_00007-9-3 loss: 0.473914  [  128/  179]
train() client id: f_00007-9-4 loss: 0.800236  [  160/  179]
train() client id: f_00007-10-0 loss: 0.492838  [   32/  179]
train() client id: f_00007-10-1 loss: 0.835052  [   64/  179]
train() client id: f_00007-10-2 loss: 0.915760  [   96/  179]
train() client id: f_00007-10-3 loss: 0.620107  [  128/  179]
train() client id: f_00007-10-4 loss: 0.476055  [  160/  179]
train() client id: f_00007-11-0 loss: 0.622061  [   32/  179]
train() client id: f_00007-11-1 loss: 0.774054  [   64/  179]
train() client id: f_00007-11-2 loss: 0.680250  [   96/  179]
train() client id: f_00007-11-3 loss: 0.668149  [  128/  179]
train() client id: f_00007-11-4 loss: 0.473577  [  160/  179]
train() client id: f_00007-12-0 loss: 0.516886  [   32/  179]
train() client id: f_00007-12-1 loss: 0.947472  [   64/  179]
train() client id: f_00007-12-2 loss: 0.493617  [   96/  179]
train() client id: f_00007-12-3 loss: 0.740811  [  128/  179]
train() client id: f_00007-12-4 loss: 0.509838  [  160/  179]
train() client id: f_00007-13-0 loss: 0.731045  [   32/  179]
train() client id: f_00007-13-1 loss: 0.916193  [   64/  179]
train() client id: f_00007-13-2 loss: 0.501541  [   96/  179]
train() client id: f_00007-13-3 loss: 0.496306  [  128/  179]
train() client id: f_00007-13-4 loss: 0.679097  [  160/  179]
train() client id: f_00007-14-0 loss: 0.858905  [   32/  179]
train() client id: f_00007-14-1 loss: 0.565951  [   64/  179]
train() client id: f_00007-14-2 loss: 0.585839  [   96/  179]
train() client id: f_00007-14-3 loss: 0.670872  [  128/  179]
train() client id: f_00007-14-4 loss: 0.602616  [  160/  179]
train() client id: f_00007-15-0 loss: 0.735417  [   32/  179]
train() client id: f_00007-15-1 loss: 0.718579  [   64/  179]
train() client id: f_00007-15-2 loss: 0.578211  [   96/  179]
train() client id: f_00007-15-3 loss: 0.470416  [  128/  179]
train() client id: f_00007-15-4 loss: 0.836275  [  160/  179]
train() client id: f_00008-0-0 loss: 0.755073  [   32/  130]
train() client id: f_00008-0-1 loss: 0.653543  [   64/  130]
train() client id: f_00008-0-2 loss: 0.568499  [   96/  130]
train() client id: f_00008-0-3 loss: 0.606409  [  128/  130]
train() client id: f_00008-1-0 loss: 0.567538  [   32/  130]
train() client id: f_00008-1-1 loss: 0.662793  [   64/  130]
train() client id: f_00008-1-2 loss: 0.586939  [   96/  130]
train() client id: f_00008-1-3 loss: 0.743253  [  128/  130]
train() client id: f_00008-2-0 loss: 0.645732  [   32/  130]
train() client id: f_00008-2-1 loss: 0.581753  [   64/  130]
train() client id: f_00008-2-2 loss: 0.665452  [   96/  130]
train() client id: f_00008-2-3 loss: 0.662183  [  128/  130]
train() client id: f_00008-3-0 loss: 0.614103  [   32/  130]
train() client id: f_00008-3-1 loss: 0.695096  [   64/  130]
train() client id: f_00008-3-2 loss: 0.568078  [   96/  130]
train() client id: f_00008-3-3 loss: 0.672671  [  128/  130]
train() client id: f_00008-4-0 loss: 0.629083  [   32/  130]
train() client id: f_00008-4-1 loss: 0.586832  [   64/  130]
train() client id: f_00008-4-2 loss: 0.701214  [   96/  130]
train() client id: f_00008-4-3 loss: 0.648307  [  128/  130]
train() client id: f_00008-5-0 loss: 0.621014  [   32/  130]
train() client id: f_00008-5-1 loss: 0.569509  [   64/  130]
train() client id: f_00008-5-2 loss: 0.717681  [   96/  130]
train() client id: f_00008-5-3 loss: 0.669495  [  128/  130]
train() client id: f_00008-6-0 loss: 0.643218  [   32/  130]
train() client id: f_00008-6-1 loss: 0.581219  [   64/  130]
train() client id: f_00008-6-2 loss: 0.591430  [   96/  130]
train() client id: f_00008-6-3 loss: 0.730881  [  128/  130]
train() client id: f_00008-7-0 loss: 0.689386  [   32/  130]
train() client id: f_00008-7-1 loss: 0.676558  [   64/  130]
train() client id: f_00008-7-2 loss: 0.560931  [   96/  130]
train() client id: f_00008-7-3 loss: 0.620185  [  128/  130]
train() client id: f_00008-8-0 loss: 0.582677  [   32/  130]
train() client id: f_00008-8-1 loss: 0.683634  [   64/  130]
train() client id: f_00008-8-2 loss: 0.660807  [   96/  130]
train() client id: f_00008-8-3 loss: 0.649017  [  128/  130]
train() client id: f_00008-9-0 loss: 0.712367  [   32/  130]
train() client id: f_00008-9-1 loss: 0.658677  [   64/  130]
train() client id: f_00008-9-2 loss: 0.473641  [   96/  130]
train() client id: f_00008-9-3 loss: 0.730234  [  128/  130]
train() client id: f_00008-10-0 loss: 0.751245  [   32/  130]
train() client id: f_00008-10-1 loss: 0.643597  [   64/  130]
train() client id: f_00008-10-2 loss: 0.571403  [   96/  130]
train() client id: f_00008-10-3 loss: 0.591653  [  128/  130]
train() client id: f_00008-11-0 loss: 0.555847  [   32/  130]
train() client id: f_00008-11-1 loss: 0.644623  [   64/  130]
train() client id: f_00008-11-2 loss: 0.774388  [   96/  130]
train() client id: f_00008-11-3 loss: 0.600726  [  128/  130]
train() client id: f_00008-12-0 loss: 0.636824  [   32/  130]
train() client id: f_00008-12-1 loss: 0.639330  [   64/  130]
train() client id: f_00008-12-2 loss: 0.683378  [   96/  130]
train() client id: f_00008-12-3 loss: 0.624771  [  128/  130]
train() client id: f_00008-13-0 loss: 0.766477  [   32/  130]
train() client id: f_00008-13-1 loss: 0.497957  [   64/  130]
train() client id: f_00008-13-2 loss: 0.663513  [   96/  130]
train() client id: f_00008-13-3 loss: 0.655167  [  128/  130]
train() client id: f_00008-14-0 loss: 0.611338  [   32/  130]
train() client id: f_00008-14-1 loss: 0.646199  [   64/  130]
train() client id: f_00008-14-2 loss: 0.687988  [   96/  130]
train() client id: f_00008-14-3 loss: 0.595480  [  128/  130]
train() client id: f_00008-15-0 loss: 0.679112  [   32/  130]
train() client id: f_00008-15-1 loss: 0.611727  [   64/  130]
train() client id: f_00008-15-2 loss: 0.639612  [   96/  130]
train() client id: f_00008-15-3 loss: 0.636947  [  128/  130]
train() client id: f_00009-0-0 loss: 1.272102  [   32/  118]
train() client id: f_00009-0-1 loss: 1.120157  [   64/  118]
train() client id: f_00009-0-2 loss: 0.992464  [   96/  118]
train() client id: f_00009-1-0 loss: 1.183939  [   32/  118]
train() client id: f_00009-1-1 loss: 1.020073  [   64/  118]
train() client id: f_00009-1-2 loss: 1.094609  [   96/  118]
train() client id: f_00009-2-0 loss: 1.043772  [   32/  118]
train() client id: f_00009-2-1 loss: 1.135321  [   64/  118]
train() client id: f_00009-2-2 loss: 1.013593  [   96/  118]
train() client id: f_00009-3-0 loss: 1.084394  [   32/  118]
train() client id: f_00009-3-1 loss: 0.941639  [   64/  118]
train() client id: f_00009-3-2 loss: 1.008594  [   96/  118]
train() client id: f_00009-4-0 loss: 1.003199  [   32/  118]
train() client id: f_00009-4-1 loss: 0.834783  [   64/  118]
train() client id: f_00009-4-2 loss: 1.013332  [   96/  118]
train() client id: f_00009-5-0 loss: 0.916713  [   32/  118]
train() client id: f_00009-5-1 loss: 0.965782  [   64/  118]
train() client id: f_00009-5-2 loss: 0.914270  [   96/  118]
train() client id: f_00009-6-0 loss: 0.690914  [   32/  118]
train() client id: f_00009-6-1 loss: 1.009986  [   64/  118]
train() client id: f_00009-6-2 loss: 0.996130  [   96/  118]
train() client id: f_00009-7-0 loss: 0.801191  [   32/  118]
train() client id: f_00009-7-1 loss: 0.781446  [   64/  118]
train() client id: f_00009-7-2 loss: 1.095843  [   96/  118]
train() client id: f_00009-8-0 loss: 0.820046  [   32/  118]
train() client id: f_00009-8-1 loss: 0.845709  [   64/  118]
train() client id: f_00009-8-2 loss: 0.975847  [   96/  118]
train() client id: f_00009-9-0 loss: 0.822940  [   32/  118]
train() client id: f_00009-9-1 loss: 0.884949  [   64/  118]
train() client id: f_00009-9-2 loss: 0.926131  [   96/  118]
train() client id: f_00009-10-0 loss: 0.777151  [   32/  118]
train() client id: f_00009-10-1 loss: 0.931615  [   64/  118]
train() client id: f_00009-10-2 loss: 0.866356  [   96/  118]
train() client id: f_00009-11-0 loss: 0.900376  [   32/  118]
train() client id: f_00009-11-1 loss: 0.877582  [   64/  118]
train() client id: f_00009-11-2 loss: 0.700898  [   96/  118]
train() client id: f_00009-12-0 loss: 0.856466  [   32/  118]
train() client id: f_00009-12-1 loss: 0.808064  [   64/  118]
train() client id: f_00009-12-2 loss: 0.841610  [   96/  118]
train() client id: f_00009-13-0 loss: 0.854304  [   32/  118]
train() client id: f_00009-13-1 loss: 0.771365  [   64/  118]
train() client id: f_00009-13-2 loss: 0.904491  [   96/  118]
train() client id: f_00009-14-0 loss: 0.741784  [   32/  118]
train() client id: f_00009-14-1 loss: 0.820026  [   64/  118]
train() client id: f_00009-14-2 loss: 0.892096  [   96/  118]
train() client id: f_00009-15-0 loss: 0.726668  [   32/  118]
train() client id: f_00009-15-1 loss: 0.977841  [   64/  118]
train() client id: f_00009-15-2 loss: 0.757446  [   96/  118]
At round 49 accuracy: 0.6472148541114059
At round 49 training accuracy: 0.5841716968477532
At round 49 training loss: 0.8443510719426534
update_location
xs = 8.927491 366.223621 5.882650 0.934260 -282.581990 -130.230757 -90.849135 -5.143845 -305.120581 20.134486 
ys = -357.390647 7.291448 255.684448 -77.290817 -9.642386 0.794442 -1.381692 251.628436 25.881276 -792.232496 
xs mean: -41.18237997052122
ys mean: -69.66579882624052
dists_uav = 371.224696 379.701074 274.607252 126.391231 299.909247 164.197081 135.112821 270.819735 322.131044 798.772637 
uav_gains = -120.230010 -120.602382 -113.490020 -102.543367 -115.722868 -105.398830 -103.268528 -113.145003 -117.436543 -129.527666 
uav_gains_db_mean: -114.13652156911395
dists_bs = 563.266476 566.612007 198.061892 307.576853 213.697673 179.866232 195.427402 186.341167 197.908462 986.719904 
bs_gains = -116.587048 -116.659061 -103.877485 -109.229761 -104.801453 -102.705649 -103.714652 -103.135706 -103.868061 -123.404488 
bs_gains_db_mean: -108.79833651682
Round 50
-------------------------------
ene_coms = [0.02865627 0.03013239 0.00750777 0.00704507 0.00786462 0.00811344
 0.0072894  0.00724341 0.00750429 0.0457289 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.56354146  9.26138803  4.26001881  1.52686355 10.39355784  5.0077174
  1.90005949  6.10940455  4.4620532   4.48476024]
obj_prev = 51.969364570074276
eta_min = 1.4304272371355466e-21	eta_max = 0.8142526965142594
af = 10.795049746072358	bf = 1.7366296775027883	zeta = 11.874554720679596	eta = 0.909090909090909
af = 10.795049746072358	bf = 1.7366296775027883	zeta = 30.420585907773297	eta = 0.35486002073727074
af = 10.795049746072358	bf = 1.7366296775027883	zeta = 20.027661782698928	eta = 0.5390069925885086
af = 10.795049746072358	bf = 1.7366296775027883	zeta = 18.239450251766925	eta = 0.5918517058937454
af = 10.795049746072358	bf = 1.7366296775027883	zeta = 18.12729451736999	eta = 0.5955135630266444
af = 10.795049746072358	bf = 1.7366296775027883	zeta = 18.126787882491726	eta = 0.595530207340213
af = 10.795049746072358	bf = 1.7366296775027883	zeta = 18.126787872067567	eta = 0.5955302076826842
eta = 0.5955302076826842
ene_coms = [0.02865627 0.03013239 0.00750777 0.00704507 0.00786462 0.00811344
 0.0072894  0.00724341 0.00750429 0.0457289 ]
ene_comp = [0.04276913 0.08995096 0.04209026 0.01459582 0.1038679  0.04955789
 0.01832963 0.06075933 0.04412689 0.04005363]
ene_total = [1.95225959 3.28221973 1.35565526 0.5915071  3.05396765 1.57632135
 0.70024098 1.85870841 1.411227   2.34468082]
ti_comp = [0.71784104 0.70307982 0.92932606 0.93395301 0.92575757 0.92326939
 0.93150974 0.93196961 0.92936083 0.54711473]
ti_coms = [0.2865627  0.30132393 0.07507768 0.07045074 0.07864617 0.08113435
 0.072894   0.07243413 0.07504292 0.45728901]
t_total = [27.45321274 27.45321274 27.45321274 27.45321274 27.45321274 27.45321274
 27.45321274 27.45321274 27.45321274 27.45321274]
ene_coms = [0.02865627 0.03013239 0.00750777 0.00704507 0.00786462 0.00811344
 0.0072894  0.00724341 0.00750429 0.0457289 ]
ene_comp = [9.48887757e-06 9.20212521e-05 5.39620701e-06 2.22799954e-07
 8.17202175e-05 8.92403921e-06 4.43574176e-07 1.61404400e-05
 6.21757609e-06 1.34168011e-05]
ene_total = [0.78351677 0.82611925 0.20535617 0.192568   0.21719602 0.22200719
 0.19925217 0.19842427 0.20528359 1.25026772]
optimize_network iter = 0 obj = 4.299991154777635
eta = 0.5955302076826842
freqs = [29790115.32372395 63969236.55025838 22645582.36981031  7813999.08338439
 56098865.59563506 26838259.1794899   9838667.47394884 32597268.18245428
 23740448.20655555 36604419.36733938]
eta_min = 0.5955302076826845	eta_max = 0.5955302076826815
af = 0.004991001428643444	bf = 1.7366296775027883	zeta = 0.005490101571507789	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02865627 0.03013239 0.00750777 0.00704507 0.00786462 0.00811344
 0.0072894  0.00724341 0.00750429 0.0457289 ]
ene_comp = [1.74489891e-06 1.69216834e-05 9.92302373e-07 4.09704302e-08
 1.50274379e-05 1.64103143e-06 8.15683509e-08 2.96804717e-06
 1.14334300e-06 2.46720030e-06]
ene_total = [3.4850558  3.66641034 0.91312847 0.85674523 0.95823112 0.98686156
 0.8864623  0.88122098 0.91272405 5.56131923]
ti_comp = [0.71784104 0.70307982 0.92932606 0.93395301 0.92575757 0.92326939
 0.93150974 0.93196961 0.92936083 0.54711473]
ti_coms = [0.2865627  0.30132393 0.07507768 0.07045074 0.07864617 0.08113435
 0.072894   0.07243413 0.07504292 0.45728901]
t_total = [27.45321274 27.45321274 27.45321274 27.45321274 27.45321274 27.45321274
 27.45321274 27.45321274 27.45321274 27.45321274]
ene_coms = [0.02865627 0.03013239 0.00750777 0.00704507 0.00786462 0.00811344
 0.0072894  0.00724341 0.00750429 0.0457289 ]
ene_comp = [9.48887757e-06 9.20212521e-05 5.39620701e-06 2.22799954e-07
 8.17202175e-05 8.92403921e-06 4.43574176e-07 1.61404400e-05
 6.21757609e-06 1.34168011e-05]
ene_total = [0.78351677 0.82611925 0.20535617 0.192568   0.21719602 0.22200719
 0.19925217 0.19842427 0.20528359 1.25026772]
optimize_network iter = 1 obj = 4.299991154777607
eta = 0.5955302076826815
freqs = [29790115.32372392 63969236.55025832 22645582.36981034  7813999.0833844
 56098865.59563512 26838259.17948993  9838667.47394885 32597268.18245432
 23740448.20655558 36604419.36733924]
Done!
ene_coms = [0.02865627 0.03013239 0.00750777 0.00704507 0.00786462 0.00811344
 0.0072894  0.00724341 0.00750429 0.0457289 ]
ene_comp = [8.94550579e-06 8.67517404e-05 5.08719822e-06 2.10041521e-07
 7.70405850e-05 8.41301238e-06 4.18173312e-07 1.52161727e-05
 5.86153236e-06 1.26485004e-05]
ene_total = [0.02866522 0.03021914 0.00751286 0.00704528 0.00794166 0.00812185
 0.00728982 0.00725863 0.00751015 0.04574155]
At round 50 energy consumption: 0.15730615566532016
At round 50 eta: 0.5955302076826815
At round 50 a_n: 11.055310507788512
At round 50 local rounds: 16.971878914242023
At round 50 global rounds: 27.332845907847908
gradient difference: 0.38202959299087524
train() client id: f_00000-0-0 loss: 1.221013  [   32/  126]
train() client id: f_00000-0-1 loss: 1.090870  [   64/  126]
train() client id: f_00000-0-2 loss: 1.105888  [   96/  126]
train() client id: f_00000-1-0 loss: 1.041579  [   32/  126]
train() client id: f_00000-1-1 loss: 0.916424  [   64/  126]
train() client id: f_00000-1-2 loss: 1.070618  [   96/  126]
train() client id: f_00000-2-0 loss: 0.946763  [   32/  126]
train() client id: f_00000-2-1 loss: 0.951257  [   64/  126]
train() client id: f_00000-2-2 loss: 1.138390  [   96/  126]
train() client id: f_00000-3-0 loss: 0.804793  [   32/  126]
train() client id: f_00000-3-1 loss: 0.854659  [   64/  126]
train() client id: f_00000-3-2 loss: 1.129380  [   96/  126]
train() client id: f_00000-4-0 loss: 0.914459  [   32/  126]
train() client id: f_00000-4-1 loss: 0.787077  [   64/  126]
train() client id: f_00000-4-2 loss: 1.027277  [   96/  126]
train() client id: f_00000-5-0 loss: 0.814795  [   32/  126]
train() client id: f_00000-5-1 loss: 0.959951  [   64/  126]
train() client id: f_00000-5-2 loss: 0.808441  [   96/  126]
train() client id: f_00000-6-0 loss: 0.914658  [   32/  126]
train() client id: f_00000-6-1 loss: 0.855105  [   64/  126]
train() client id: f_00000-6-2 loss: 0.797710  [   96/  126]
train() client id: f_00000-7-0 loss: 0.840255  [   32/  126]
train() client id: f_00000-7-1 loss: 0.980584  [   64/  126]
train() client id: f_00000-7-2 loss: 0.802848  [   96/  126]
train() client id: f_00000-8-0 loss: 0.966310  [   32/  126]
train() client id: f_00000-8-1 loss: 0.843477  [   64/  126]
train() client id: f_00000-8-2 loss: 0.745253  [   96/  126]
train() client id: f_00000-9-0 loss: 0.828405  [   32/  126]
train() client id: f_00000-9-1 loss: 0.866239  [   64/  126]
train() client id: f_00000-9-2 loss: 0.858757  [   96/  126]
train() client id: f_00000-10-0 loss: 0.662271  [   32/  126]
train() client id: f_00000-10-1 loss: 0.989760  [   64/  126]
train() client id: f_00000-10-2 loss: 0.796116  [   96/  126]
train() client id: f_00000-11-0 loss: 0.810787  [   32/  126]
train() client id: f_00000-11-1 loss: 0.867922  [   64/  126]
train() client id: f_00000-11-2 loss: 0.913102  [   96/  126]
train() client id: f_00000-12-0 loss: 0.798782  [   32/  126]
train() client id: f_00000-12-1 loss: 0.861135  [   64/  126]
train() client id: f_00000-12-2 loss: 0.888605  [   96/  126]
train() client id: f_00000-13-0 loss: 0.906791  [   32/  126]
train() client id: f_00000-13-1 loss: 0.791295  [   64/  126]
train() client id: f_00000-13-2 loss: 0.973388  [   96/  126]
train() client id: f_00000-14-0 loss: 0.801292  [   32/  126]
train() client id: f_00000-14-1 loss: 0.940909  [   64/  126]
train() client id: f_00000-14-2 loss: 0.916184  [   96/  126]
train() client id: f_00000-15-0 loss: 0.966299  [   32/  126]
train() client id: f_00000-15-1 loss: 0.738702  [   64/  126]
train() client id: f_00000-15-2 loss: 0.981076  [   96/  126]
train() client id: f_00001-0-0 loss: 0.580760  [   32/  265]
train() client id: f_00001-0-1 loss: 0.448632  [   64/  265]
train() client id: f_00001-0-2 loss: 0.486853  [   96/  265]
train() client id: f_00001-0-3 loss: 0.456381  [  128/  265]
train() client id: f_00001-0-4 loss: 0.429302  [  160/  265]
train() client id: f_00001-0-5 loss: 0.448029  [  192/  265]
train() client id: f_00001-0-6 loss: 0.402194  [  224/  265]
train() client id: f_00001-0-7 loss: 0.480052  [  256/  265]
train() client id: f_00001-1-0 loss: 0.557499  [   32/  265]
train() client id: f_00001-1-1 loss: 0.417535  [   64/  265]
train() client id: f_00001-1-2 loss: 0.448640  [   96/  265]
train() client id: f_00001-1-3 loss: 0.458253  [  128/  265]
train() client id: f_00001-1-4 loss: 0.478866  [  160/  265]
train() client id: f_00001-1-5 loss: 0.440267  [  192/  265]
train() client id: f_00001-1-6 loss: 0.435479  [  224/  265]
train() client id: f_00001-1-7 loss: 0.392593  [  256/  265]
train() client id: f_00001-2-0 loss: 0.434818  [   32/  265]
train() client id: f_00001-2-1 loss: 0.411176  [   64/  265]
train() client id: f_00001-2-2 loss: 0.485617  [   96/  265]
train() client id: f_00001-2-3 loss: 0.578456  [  128/  265]
train() client id: f_00001-2-4 loss: 0.356079  [  160/  265]
train() client id: f_00001-2-5 loss: 0.459531  [  192/  265]
train() client id: f_00001-2-6 loss: 0.408664  [  224/  265]
train() client id: f_00001-2-7 loss: 0.414744  [  256/  265]
train() client id: f_00001-3-0 loss: 0.624296  [   32/  265]
train() client id: f_00001-3-1 loss: 0.442997  [   64/  265]
train() client id: f_00001-3-2 loss: 0.562602  [   96/  265]
train() client id: f_00001-3-3 loss: 0.352290  [  128/  265]
train() client id: f_00001-3-4 loss: 0.327910  [  160/  265]
train() client id: f_00001-3-5 loss: 0.459514  [  192/  265]
train() client id: f_00001-3-6 loss: 0.437245  [  224/  265]
train() client id: f_00001-3-7 loss: 0.355496  [  256/  265]
train() client id: f_00001-4-0 loss: 0.461882  [   32/  265]
train() client id: f_00001-4-1 loss: 0.504231  [   64/  265]
train() client id: f_00001-4-2 loss: 0.482104  [   96/  265]
train() client id: f_00001-4-3 loss: 0.399398  [  128/  265]
train() client id: f_00001-4-4 loss: 0.412244  [  160/  265]
train() client id: f_00001-4-5 loss: 0.381773  [  192/  265]
train() client id: f_00001-4-6 loss: 0.349804  [  224/  265]
train() client id: f_00001-4-7 loss: 0.465193  [  256/  265]
train() client id: f_00001-5-0 loss: 0.334520  [   32/  265]
train() client id: f_00001-5-1 loss: 0.466006  [   64/  265]
train() client id: f_00001-5-2 loss: 0.636649  [   96/  265]
train() client id: f_00001-5-3 loss: 0.452049  [  128/  265]
train() client id: f_00001-5-4 loss: 0.416508  [  160/  265]
train() client id: f_00001-5-5 loss: 0.382719  [  192/  265]
train() client id: f_00001-5-6 loss: 0.462652  [  224/  265]
train() client id: f_00001-5-7 loss: 0.344314  [  256/  265]
train() client id: f_00001-6-0 loss: 0.434463  [   32/  265]
train() client id: f_00001-6-1 loss: 0.431157  [   64/  265]
train() client id: f_00001-6-2 loss: 0.485843  [   96/  265]
train() client id: f_00001-6-3 loss: 0.361361  [  128/  265]
train() client id: f_00001-6-4 loss: 0.489208  [  160/  265]
train() client id: f_00001-6-5 loss: 0.402395  [  192/  265]
train() client id: f_00001-6-6 loss: 0.413577  [  224/  265]
train() client id: f_00001-6-7 loss: 0.450815  [  256/  265]
train() client id: f_00001-7-0 loss: 0.436150  [   32/  265]
train() client id: f_00001-7-1 loss: 0.547481  [   64/  265]
train() client id: f_00001-7-2 loss: 0.380151  [   96/  265]
train() client id: f_00001-7-3 loss: 0.475558  [  128/  265]
train() client id: f_00001-7-4 loss: 0.386108  [  160/  265]
train() client id: f_00001-7-5 loss: 0.388263  [  192/  265]
train() client id: f_00001-7-6 loss: 0.390119  [  224/  265]
train() client id: f_00001-7-7 loss: 0.478252  [  256/  265]
train() client id: f_00001-8-0 loss: 0.339139  [   32/  265]
train() client id: f_00001-8-1 loss: 0.461274  [   64/  265]
train() client id: f_00001-8-2 loss: 0.475635  [   96/  265]
train() client id: f_00001-8-3 loss: 0.531108  [  128/  265]
train() client id: f_00001-8-4 loss: 0.412372  [  160/  265]
train() client id: f_00001-8-5 loss: 0.318562  [  192/  265]
train() client id: f_00001-8-6 loss: 0.394674  [  224/  265]
train() client id: f_00001-8-7 loss: 0.435101  [  256/  265]
train() client id: f_00001-9-0 loss: 0.456978  [   32/  265]
train() client id: f_00001-9-1 loss: 0.498114  [   64/  265]
train() client id: f_00001-9-2 loss: 0.467481  [   96/  265]
train() client id: f_00001-9-3 loss: 0.479285  [  128/  265]
train() client id: f_00001-9-4 loss: 0.333670  [  160/  265]
train() client id: f_00001-9-5 loss: 0.407683  [  192/  265]
train() client id: f_00001-9-6 loss: 0.447436  [  224/  265]
train() client id: f_00001-9-7 loss: 0.350429  [  256/  265]
train() client id: f_00001-10-0 loss: 0.500522  [   32/  265]
train() client id: f_00001-10-1 loss: 0.331740  [   64/  265]
train() client id: f_00001-10-2 loss: 0.344898  [   96/  265]
train() client id: f_00001-10-3 loss: 0.446963  [  128/  265]
train() client id: f_00001-10-4 loss: 0.464095  [  160/  265]
train() client id: f_00001-10-5 loss: 0.309605  [  192/  265]
train() client id: f_00001-10-6 loss: 0.613868  [  224/  265]
train() client id: f_00001-10-7 loss: 0.417535  [  256/  265]
train() client id: f_00001-11-0 loss: 0.458665  [   32/  265]
train() client id: f_00001-11-1 loss: 0.430306  [   64/  265]
train() client id: f_00001-11-2 loss: 0.548492  [   96/  265]
train() client id: f_00001-11-3 loss: 0.321926  [  128/  265]
train() client id: f_00001-11-4 loss: 0.420615  [  160/  265]
train() client id: f_00001-11-5 loss: 0.420163  [  192/  265]
train() client id: f_00001-11-6 loss: 0.448361  [  224/  265]
train() client id: f_00001-11-7 loss: 0.388825  [  256/  265]
train() client id: f_00001-12-0 loss: 0.376262  [   32/  265]
train() client id: f_00001-12-1 loss: 0.520154  [   64/  265]
train() client id: f_00001-12-2 loss: 0.379991  [   96/  265]
train() client id: f_00001-12-3 loss: 0.355347  [  128/  265]
train() client id: f_00001-12-4 loss: 0.346936  [  160/  265]
train() client id: f_00001-12-5 loss: 0.382150  [  192/  265]
train() client id: f_00001-12-6 loss: 0.555353  [  224/  265]
train() client id: f_00001-12-7 loss: 0.484754  [  256/  265]
train() client id: f_00001-13-0 loss: 0.426329  [   32/  265]
train() client id: f_00001-13-1 loss: 0.555052  [   64/  265]
train() client id: f_00001-13-2 loss: 0.345071  [   96/  265]
train() client id: f_00001-13-3 loss: 0.423970  [  128/  265]
train() client id: f_00001-13-4 loss: 0.403100  [  160/  265]
train() client id: f_00001-13-5 loss: 0.366459  [  192/  265]
train() client id: f_00001-13-6 loss: 0.369827  [  224/  265]
train() client id: f_00001-13-7 loss: 0.471619  [  256/  265]
train() client id: f_00001-14-0 loss: 0.518686  [   32/  265]
train() client id: f_00001-14-1 loss: 0.419699  [   64/  265]
train() client id: f_00001-14-2 loss: 0.453842  [   96/  265]
train() client id: f_00001-14-3 loss: 0.526319  [  128/  265]
train() client id: f_00001-14-4 loss: 0.345226  [  160/  265]
train() client id: f_00001-14-5 loss: 0.430676  [  192/  265]
train() client id: f_00001-14-6 loss: 0.348815  [  224/  265]
train() client id: f_00001-14-7 loss: 0.397991  [  256/  265]
train() client id: f_00001-15-0 loss: 0.475355  [   32/  265]
train() client id: f_00001-15-1 loss: 0.413550  [   64/  265]
train() client id: f_00001-15-2 loss: 0.425347  [   96/  265]
train() client id: f_00001-15-3 loss: 0.420268  [  128/  265]
train() client id: f_00001-15-4 loss: 0.326764  [  160/  265]
train() client id: f_00001-15-5 loss: 0.431446  [  192/  265]
train() client id: f_00001-15-6 loss: 0.484334  [  224/  265]
train() client id: f_00001-15-7 loss: 0.457185  [  256/  265]
train() client id: f_00002-0-0 loss: 1.089136  [   32/  124]
train() client id: f_00002-0-1 loss: 1.041390  [   64/  124]
train() client id: f_00002-0-2 loss: 1.242380  [   96/  124]
train() client id: f_00002-1-0 loss: 1.053127  [   32/  124]
train() client id: f_00002-1-1 loss: 1.065874  [   64/  124]
train() client id: f_00002-1-2 loss: 1.046935  [   96/  124]
train() client id: f_00002-2-0 loss: 1.054378  [   32/  124]
train() client id: f_00002-2-1 loss: 1.064250  [   64/  124]
train() client id: f_00002-2-2 loss: 1.056704  [   96/  124]
train() client id: f_00002-3-0 loss: 1.157089  [   32/  124]
train() client id: f_00002-3-1 loss: 0.903900  [   64/  124]
train() client id: f_00002-3-2 loss: 0.946915  [   96/  124]
train() client id: f_00002-4-0 loss: 0.928041  [   32/  124]
train() client id: f_00002-4-1 loss: 1.015984  [   64/  124]
train() client id: f_00002-4-2 loss: 0.859492  [   96/  124]
train() client id: f_00002-5-0 loss: 0.918404  [   32/  124]
train() client id: f_00002-5-1 loss: 0.888193  [   64/  124]
train() client id: f_00002-5-2 loss: 1.004524  [   96/  124]
train() client id: f_00002-6-0 loss: 0.903721  [   32/  124]
train() client id: f_00002-6-1 loss: 1.055955  [   64/  124]
train() client id: f_00002-6-2 loss: 0.785789  [   96/  124]
train() client id: f_00002-7-0 loss: 1.046530  [   32/  124]
train() client id: f_00002-7-1 loss: 0.908562  [   64/  124]
train() client id: f_00002-7-2 loss: 0.836035  [   96/  124]
train() client id: f_00002-8-0 loss: 1.006495  [   32/  124]
train() client id: f_00002-8-1 loss: 0.756570  [   64/  124]
train() client id: f_00002-8-2 loss: 0.911938  [   96/  124]
train() client id: f_00002-9-0 loss: 0.921467  [   32/  124]
train() client id: f_00002-9-1 loss: 0.971084  [   64/  124]
train() client id: f_00002-9-2 loss: 0.730509  [   96/  124]
train() client id: f_00002-10-0 loss: 1.002298  [   32/  124]
train() client id: f_00002-10-1 loss: 0.910893  [   64/  124]
train() client id: f_00002-10-2 loss: 0.720812  [   96/  124]
train() client id: f_00002-11-0 loss: 0.834463  [   32/  124]
train() client id: f_00002-11-1 loss: 0.892861  [   64/  124]
train() client id: f_00002-11-2 loss: 0.733564  [   96/  124]
train() client id: f_00002-12-0 loss: 1.041103  [   32/  124]
train() client id: f_00002-12-1 loss: 0.727571  [   64/  124]
train() client id: f_00002-12-2 loss: 0.745118  [   96/  124]
train() client id: f_00002-13-0 loss: 0.684950  [   32/  124]
train() client id: f_00002-13-1 loss: 0.875880  [   64/  124]
train() client id: f_00002-13-2 loss: 0.836921  [   96/  124]
train() client id: f_00002-14-0 loss: 0.878352  [   32/  124]
train() client id: f_00002-14-1 loss: 0.853591  [   64/  124]
train() client id: f_00002-14-2 loss: 0.717963  [   96/  124]
train() client id: f_00002-15-0 loss: 0.752491  [   32/  124]
train() client id: f_00002-15-1 loss: 0.886410  [   64/  124]
train() client id: f_00002-15-2 loss: 0.841350  [   96/  124]
train() client id: f_00003-0-0 loss: 0.684741  [   32/   43]
train() client id: f_00003-1-0 loss: 0.648049  [   32/   43]
train() client id: f_00003-2-0 loss: 0.596617  [   32/   43]
train() client id: f_00003-3-0 loss: 0.556865  [   32/   43]
train() client id: f_00003-4-0 loss: 0.858976  [   32/   43]
train() client id: f_00003-5-0 loss: 0.677439  [   32/   43]
train() client id: f_00003-6-0 loss: 0.597139  [   32/   43]
train() client id: f_00003-7-0 loss: 0.578412  [   32/   43]
train() client id: f_00003-8-0 loss: 0.750020  [   32/   43]
train() client id: f_00003-9-0 loss: 0.772401  [   32/   43]
train() client id: f_00003-10-0 loss: 0.519762  [   32/   43]
train() client id: f_00003-11-0 loss: 0.620377  [   32/   43]
train() client id: f_00003-12-0 loss: 0.634731  [   32/   43]
train() client id: f_00003-13-0 loss: 0.721214  [   32/   43]
train() client id: f_00003-14-0 loss: 0.579150  [   32/   43]
train() client id: f_00003-15-0 loss: 0.678588  [   32/   43]
train() client id: f_00004-0-0 loss: 0.768406  [   32/  306]
train() client id: f_00004-0-1 loss: 0.842906  [   64/  306]
train() client id: f_00004-0-2 loss: 0.774116  [   96/  306]
train() client id: f_00004-0-3 loss: 0.790301  [  128/  306]
train() client id: f_00004-0-4 loss: 0.909327  [  160/  306]
train() client id: f_00004-0-5 loss: 0.871513  [  192/  306]
train() client id: f_00004-0-6 loss: 1.005118  [  224/  306]
train() client id: f_00004-0-7 loss: 0.879295  [  256/  306]
train() client id: f_00004-0-8 loss: 0.895238  [  288/  306]
train() client id: f_00004-1-0 loss: 0.844347  [   32/  306]
train() client id: f_00004-1-1 loss: 0.915113  [   64/  306]
train() client id: f_00004-1-2 loss: 0.890247  [   96/  306]
train() client id: f_00004-1-3 loss: 0.844619  [  128/  306]
train() client id: f_00004-1-4 loss: 0.769730  [  160/  306]
train() client id: f_00004-1-5 loss: 0.987699  [  192/  306]
train() client id: f_00004-1-6 loss: 0.735122  [  224/  306]
train() client id: f_00004-1-7 loss: 0.808446  [  256/  306]
train() client id: f_00004-1-8 loss: 0.869704  [  288/  306]
train() client id: f_00004-2-0 loss: 0.848339  [   32/  306]
train() client id: f_00004-2-1 loss: 0.769638  [   64/  306]
train() client id: f_00004-2-2 loss: 0.867206  [   96/  306]
train() client id: f_00004-2-3 loss: 0.881014  [  128/  306]
train() client id: f_00004-2-4 loss: 0.978104  [  160/  306]
train() client id: f_00004-2-5 loss: 0.846982  [  192/  306]
train() client id: f_00004-2-6 loss: 0.877209  [  224/  306]
train() client id: f_00004-2-7 loss: 0.883666  [  256/  306]
train() client id: f_00004-2-8 loss: 0.766078  [  288/  306]
train() client id: f_00004-3-0 loss: 0.847037  [   32/  306]
train() client id: f_00004-3-1 loss: 0.769221  [   64/  306]
train() client id: f_00004-3-2 loss: 0.874878  [   96/  306]
train() client id: f_00004-3-3 loss: 0.926574  [  128/  306]
train() client id: f_00004-3-4 loss: 0.905362  [  160/  306]
train() client id: f_00004-3-5 loss: 1.015186  [  192/  306]
train() client id: f_00004-3-6 loss: 0.754296  [  224/  306]
train() client id: f_00004-3-7 loss: 0.896230  [  256/  306]
train() client id: f_00004-3-8 loss: 0.772956  [  288/  306]
train() client id: f_00004-4-0 loss: 0.907392  [   32/  306]
train() client id: f_00004-4-1 loss: 0.860312  [   64/  306]
train() client id: f_00004-4-2 loss: 0.886044  [   96/  306]
train() client id: f_00004-4-3 loss: 0.789054  [  128/  306]
train() client id: f_00004-4-4 loss: 0.895994  [  160/  306]
train() client id: f_00004-4-5 loss: 0.779879  [  192/  306]
train() client id: f_00004-4-6 loss: 0.912194  [  224/  306]
train() client id: f_00004-4-7 loss: 0.851108  [  256/  306]
train() client id: f_00004-4-8 loss: 0.824191  [  288/  306]
train() client id: f_00004-5-0 loss: 1.030076  [   32/  306]
train() client id: f_00004-5-1 loss: 0.895291  [   64/  306]
train() client id: f_00004-5-2 loss: 0.860025  [   96/  306]
train() client id: f_00004-5-3 loss: 0.971101  [  128/  306]
train() client id: f_00004-5-4 loss: 0.735758  [  160/  306]
train() client id: f_00004-5-5 loss: 0.904996  [  192/  306]
train() client id: f_00004-5-6 loss: 0.787298  [  224/  306]
train() client id: f_00004-5-7 loss: 0.872362  [  256/  306]
train() client id: f_00004-5-8 loss: 0.763897  [  288/  306]
train() client id: f_00004-6-0 loss: 0.850673  [   32/  306]
train() client id: f_00004-6-1 loss: 0.812117  [   64/  306]
train() client id: f_00004-6-2 loss: 0.964039  [   96/  306]
train() client id: f_00004-6-3 loss: 0.860416  [  128/  306]
train() client id: f_00004-6-4 loss: 0.893413  [  160/  306]
train() client id: f_00004-6-5 loss: 0.799362  [  192/  306]
train() client id: f_00004-6-6 loss: 0.867510  [  224/  306]
train() client id: f_00004-6-7 loss: 0.885574  [  256/  306]
train() client id: f_00004-6-8 loss: 0.695621  [  288/  306]
train() client id: f_00004-7-0 loss: 0.811542  [   32/  306]
train() client id: f_00004-7-1 loss: 0.955233  [   64/  306]
train() client id: f_00004-7-2 loss: 0.885432  [   96/  306]
train() client id: f_00004-7-3 loss: 0.797452  [  128/  306]
train() client id: f_00004-7-4 loss: 0.835801  [  160/  306]
train() client id: f_00004-7-5 loss: 0.934424  [  192/  306]
train() client id: f_00004-7-6 loss: 0.867821  [  224/  306]
train() client id: f_00004-7-7 loss: 0.913318  [  256/  306]
train() client id: f_00004-7-8 loss: 0.803139  [  288/  306]
train() client id: f_00004-8-0 loss: 0.814760  [   32/  306]
train() client id: f_00004-8-1 loss: 1.045947  [   64/  306]
train() client id: f_00004-8-2 loss: 0.900212  [   96/  306]
train() client id: f_00004-8-3 loss: 0.942272  [  128/  306]
train() client id: f_00004-8-4 loss: 0.912673  [  160/  306]
train() client id: f_00004-8-5 loss: 0.691432  [  192/  306]
train() client id: f_00004-8-6 loss: 0.813034  [  224/  306]
train() client id: f_00004-8-7 loss: 0.708321  [  256/  306]
train() client id: f_00004-8-8 loss: 0.833014  [  288/  306]
train() client id: f_00004-9-0 loss: 0.812590  [   32/  306]
train() client id: f_00004-9-1 loss: 0.898661  [   64/  306]
train() client id: f_00004-9-2 loss: 1.023530  [   96/  306]
train() client id: f_00004-9-3 loss: 0.799628  [  128/  306]
train() client id: f_00004-9-4 loss: 0.848931  [  160/  306]
train() client id: f_00004-9-5 loss: 0.765700  [  192/  306]
train() client id: f_00004-9-6 loss: 0.836152  [  224/  306]
train() client id: f_00004-9-7 loss: 0.933617  [  256/  306]
train() client id: f_00004-9-8 loss: 0.808115  [  288/  306]
train() client id: f_00004-10-0 loss: 0.874042  [   32/  306]
train() client id: f_00004-10-1 loss: 0.809798  [   64/  306]
train() client id: f_00004-10-2 loss: 0.902346  [   96/  306]
train() client id: f_00004-10-3 loss: 0.842665  [  128/  306]
train() client id: f_00004-10-4 loss: 0.865100  [  160/  306]
train() client id: f_00004-10-5 loss: 0.871163  [  192/  306]
train() client id: f_00004-10-6 loss: 0.824235  [  224/  306]
train() client id: f_00004-10-7 loss: 0.871634  [  256/  306]
train() client id: f_00004-10-8 loss: 0.860236  [  288/  306]
train() client id: f_00004-11-0 loss: 0.990437  [   32/  306]
train() client id: f_00004-11-1 loss: 0.795984  [   64/  306]
train() client id: f_00004-11-2 loss: 0.834144  [   96/  306]
train() client id: f_00004-11-3 loss: 0.782678  [  128/  306]
train() client id: f_00004-11-4 loss: 0.881282  [  160/  306]
train() client id: f_00004-11-5 loss: 0.922519  [  192/  306]
train() client id: f_00004-11-6 loss: 0.784259  [  224/  306]
train() client id: f_00004-11-7 loss: 0.784697  [  256/  306]
train() client id: f_00004-11-8 loss: 0.905923  [  288/  306]
train() client id: f_00004-12-0 loss: 0.920761  [   32/  306]
train() client id: f_00004-12-1 loss: 0.824251  [   64/  306]
train() client id: f_00004-12-2 loss: 0.909835  [   96/  306]
train() client id: f_00004-12-3 loss: 0.891979  [  128/  306]
train() client id: f_00004-12-4 loss: 0.915441  [  160/  306]
train() client id: f_00004-12-5 loss: 0.714797  [  192/  306]
train() client id: f_00004-12-6 loss: 0.809582  [  224/  306]
train() client id: f_00004-12-7 loss: 0.870268  [  256/  306]
train() client id: f_00004-12-8 loss: 0.872173  [  288/  306]
train() client id: f_00004-13-0 loss: 0.854937  [   32/  306]
train() client id: f_00004-13-1 loss: 0.891104  [   64/  306]
train() client id: f_00004-13-2 loss: 0.979515  [   96/  306]
train() client id: f_00004-13-3 loss: 0.788535  [  128/  306]
train() client id: f_00004-13-4 loss: 0.871125  [  160/  306]
train() client id: f_00004-13-5 loss: 0.868897  [  192/  306]
train() client id: f_00004-13-6 loss: 0.780505  [  224/  306]
train() client id: f_00004-13-7 loss: 0.758621  [  256/  306]
train() client id: f_00004-13-8 loss: 0.844092  [  288/  306]
train() client id: f_00004-14-0 loss: 1.057270  [   32/  306]
train() client id: f_00004-14-1 loss: 0.788945  [   64/  306]
train() client id: f_00004-14-2 loss: 0.883085  [   96/  306]
train() client id: f_00004-14-3 loss: 0.894114  [  128/  306]
train() client id: f_00004-14-4 loss: 0.847980  [  160/  306]
train() client id: f_00004-14-5 loss: 0.927904  [  192/  306]
train() client id: f_00004-14-6 loss: 0.874317  [  224/  306]
train() client id: f_00004-14-7 loss: 0.787098  [  256/  306]
train() client id: f_00004-14-8 loss: 0.737680  [  288/  306]
train() client id: f_00004-15-0 loss: 0.888517  [   32/  306]
train() client id: f_00004-15-1 loss: 0.852581  [   64/  306]
train() client id: f_00004-15-2 loss: 0.777545  [   96/  306]
train() client id: f_00004-15-3 loss: 0.864052  [  128/  306]
train() client id: f_00004-15-4 loss: 0.917778  [  160/  306]
train() client id: f_00004-15-5 loss: 0.984983  [  192/  306]
train() client id: f_00004-15-6 loss: 0.852346  [  224/  306]
train() client id: f_00004-15-7 loss: 0.882954  [  256/  306]
train() client id: f_00004-15-8 loss: 0.762137  [  288/  306]
train() client id: f_00005-0-0 loss: 0.150247  [   32/  146]
train() client id: f_00005-0-1 loss: 0.401397  [   64/  146]
train() client id: f_00005-0-2 loss: 0.250993  [   96/  146]
train() client id: f_00005-0-3 loss: 0.324146  [  128/  146]
train() client id: f_00005-1-0 loss: 0.241634  [   32/  146]
train() client id: f_00005-1-1 loss: 0.308008  [   64/  146]
train() client id: f_00005-1-2 loss: 0.093867  [   96/  146]
train() client id: f_00005-1-3 loss: 0.442074  [  128/  146]
train() client id: f_00005-2-0 loss: 0.295168  [   32/  146]
train() client id: f_00005-2-1 loss: 0.454106  [   64/  146]
train() client id: f_00005-2-2 loss: 0.228271  [   96/  146]
train() client id: f_00005-2-3 loss: 0.256948  [  128/  146]
train() client id: f_00005-3-0 loss: 0.319877  [   32/  146]
train() client id: f_00005-3-1 loss: 0.442411  [   64/  146]
train() client id: f_00005-3-2 loss: 0.289125  [   96/  146]
train() client id: f_00005-3-3 loss: 0.108846  [  128/  146]
train() client id: f_00005-4-0 loss: 0.340713  [   32/  146]
train() client id: f_00005-4-1 loss: 0.309501  [   64/  146]
train() client id: f_00005-4-2 loss: 0.285752  [   96/  146]
train() client id: f_00005-4-3 loss: 0.139136  [  128/  146]
train() client id: f_00005-5-0 loss: 0.252987  [   32/  146]
train() client id: f_00005-5-1 loss: 0.162919  [   64/  146]
train() client id: f_00005-5-2 loss: 0.277091  [   96/  146]
train() client id: f_00005-5-3 loss: 0.328393  [  128/  146]
train() client id: f_00005-6-0 loss: 0.217170  [   32/  146]
train() client id: f_00005-6-1 loss: 0.212773  [   64/  146]
train() client id: f_00005-6-2 loss: 0.400411  [   96/  146]
train() client id: f_00005-6-3 loss: 0.243913  [  128/  146]
train() client id: f_00005-7-0 loss: 0.103525  [   32/  146]
train() client id: f_00005-7-1 loss: 0.317124  [   64/  146]
train() client id: f_00005-7-2 loss: 0.221292  [   96/  146]
train() client id: f_00005-7-3 loss: 0.339113  [  128/  146]
train() client id: f_00005-8-0 loss: 0.135637  [   32/  146]
train() client id: f_00005-8-1 loss: 0.109147  [   64/  146]
train() client id: f_00005-8-2 loss: 0.270048  [   96/  146]
train() client id: f_00005-8-3 loss: 0.171413  [  128/  146]
train() client id: f_00005-9-0 loss: 0.263673  [   32/  146]
train() client id: f_00005-9-1 loss: 0.331464  [   64/  146]
train() client id: f_00005-9-2 loss: 0.302869  [   96/  146]
train() client id: f_00005-9-3 loss: 0.143383  [  128/  146]
train() client id: f_00005-10-0 loss: 0.269435  [   32/  146]
train() client id: f_00005-10-1 loss: 0.108255  [   64/  146]
train() client id: f_00005-10-2 loss: 0.495448  [   96/  146]
train() client id: f_00005-10-3 loss: 0.123375  [  128/  146]
train() client id: f_00005-11-0 loss: 0.201159  [   32/  146]
train() client id: f_00005-11-1 loss: 0.313670  [   64/  146]
train() client id: f_00005-11-2 loss: 0.272268  [   96/  146]
train() client id: f_00005-11-3 loss: 0.349991  [  128/  146]
train() client id: f_00005-12-0 loss: 0.217601  [   32/  146]
train() client id: f_00005-12-1 loss: 0.356360  [   64/  146]
train() client id: f_00005-12-2 loss: 0.195531  [   96/  146]
train() client id: f_00005-12-3 loss: 0.217175  [  128/  146]
train() client id: f_00005-13-0 loss: 0.320001  [   32/  146]
train() client id: f_00005-13-1 loss: 0.267363  [   64/  146]
train() client id: f_00005-13-2 loss: 0.291798  [   96/  146]
train() client id: f_00005-13-3 loss: 0.218746  [  128/  146]
train() client id: f_00005-14-0 loss: 0.425109  [   32/  146]
train() client id: f_00005-14-1 loss: 0.286731  [   64/  146]
train() client id: f_00005-14-2 loss: 0.078018  [   96/  146]
train() client id: f_00005-14-3 loss: 0.082159  [  128/  146]
train() client id: f_00005-15-0 loss: 0.177842  [   32/  146]
train() client id: f_00005-15-1 loss: 0.027310  [   64/  146]
train() client id: f_00005-15-2 loss: 0.276537  [   96/  146]
train() client id: f_00005-15-3 loss: 0.481201  [  128/  146]
train() client id: f_00006-0-0 loss: 0.537629  [   32/   54]
train() client id: f_00006-1-0 loss: 0.495048  [   32/   54]
train() client id: f_00006-2-0 loss: 0.560123  [   32/   54]
train() client id: f_00006-3-0 loss: 0.466434  [   32/   54]
train() client id: f_00006-4-0 loss: 0.499444  [   32/   54]
train() client id: f_00006-5-0 loss: 0.495548  [   32/   54]
train() client id: f_00006-6-0 loss: 0.490603  [   32/   54]
train() client id: f_00006-7-0 loss: 0.492559  [   32/   54]
train() client id: f_00006-8-0 loss: 0.460599  [   32/   54]
train() client id: f_00006-9-0 loss: 0.447592  [   32/   54]
train() client id: f_00006-10-0 loss: 0.500789  [   32/   54]
train() client id: f_00006-11-0 loss: 0.490463  [   32/   54]
train() client id: f_00006-12-0 loss: 0.555724  [   32/   54]
train() client id: f_00006-13-0 loss: 0.515239  [   32/   54]
train() client id: f_00006-14-0 loss: 0.554902  [   32/   54]
train() client id: f_00006-15-0 loss: 0.497573  [   32/   54]
train() client id: f_00007-0-0 loss: 0.279504  [   32/  179]
train() client id: f_00007-0-1 loss: 0.432314  [   64/  179]
train() client id: f_00007-0-2 loss: 0.244538  [   96/  179]
train() client id: f_00007-0-3 loss: 0.341775  [  128/  179]
train() client id: f_00007-0-4 loss: 0.620903  [  160/  179]
train() client id: f_00007-1-0 loss: 0.372667  [   32/  179]
train() client id: f_00007-1-1 loss: 0.258927  [   64/  179]
train() client id: f_00007-1-2 loss: 0.412994  [   96/  179]
train() client id: f_00007-1-3 loss: 0.394908  [  128/  179]
train() client id: f_00007-1-4 loss: 0.333629  [  160/  179]
train() client id: f_00007-2-0 loss: 0.245715  [   32/  179]
train() client id: f_00007-2-1 loss: 0.147250  [   64/  179]
train() client id: f_00007-2-2 loss: 0.487648  [   96/  179]
train() client id: f_00007-2-3 loss: 0.651113  [  128/  179]
train() client id: f_00007-2-4 loss: 0.288131  [  160/  179]
train() client id: f_00007-3-0 loss: 0.251288  [   32/  179]
train() client id: f_00007-3-1 loss: 0.385805  [   64/  179]
train() client id: f_00007-3-2 loss: 0.456982  [   96/  179]
train() client id: f_00007-3-3 loss: 0.291991  [  128/  179]
train() client id: f_00007-3-4 loss: 0.386851  [  160/  179]
train() client id: f_00007-4-0 loss: 0.386325  [   32/  179]
train() client id: f_00007-4-1 loss: 0.471721  [   64/  179]
train() client id: f_00007-4-2 loss: 0.363439  [   96/  179]
train() client id: f_00007-4-3 loss: 0.285089  [  128/  179]
train() client id: f_00007-4-4 loss: 0.235123  [  160/  179]
train() client id: f_00007-5-0 loss: 0.188028  [   32/  179]
train() client id: f_00007-5-1 loss: 0.378461  [   64/  179]
train() client id: f_00007-5-2 loss: 0.208411  [   96/  179]
train() client id: f_00007-5-3 loss: 0.306245  [  128/  179]
train() client id: f_00007-5-4 loss: 0.251511  [  160/  179]
train() client id: f_00007-6-0 loss: 0.298291  [   32/  179]
train() client id: f_00007-6-1 loss: 0.275269  [   64/  179]
train() client id: f_00007-6-2 loss: 0.285053  [   96/  179]
train() client id: f_00007-6-3 loss: 0.360968  [  128/  179]
train() client id: f_00007-6-4 loss: 0.345386  [  160/  179]
train() client id: f_00007-7-0 loss: 0.226884  [   32/  179]
train() client id: f_00007-7-1 loss: 0.199754  [   64/  179]
train() client id: f_00007-7-2 loss: 0.332648  [   96/  179]
train() client id: f_00007-7-3 loss: 0.577453  [  128/  179]
train() client id: f_00007-7-4 loss: 0.170296  [  160/  179]
train() client id: f_00007-8-0 loss: 0.143560  [   32/  179]
train() client id: f_00007-8-1 loss: 0.304499  [   64/  179]
train() client id: f_00007-8-2 loss: 0.284664  [   96/  179]
train() client id: f_00007-8-3 loss: 0.223888  [  128/  179]
train() client id: f_00007-8-4 loss: 0.252784  [  160/  179]
train() client id: f_00007-9-0 loss: 0.463778  [   32/  179]
train() client id: f_00007-9-1 loss: 0.272761  [   64/  179]
train() client id: f_00007-9-2 loss: 0.347502  [   96/  179]
train() client id: f_00007-9-3 loss: 0.114729  [  128/  179]
train() client id: f_00007-9-4 loss: 0.153713  [  160/  179]
train() client id: f_00007-10-0 loss: 0.291084  [   32/  179]
train() client id: f_00007-10-1 loss: 0.344582  [   64/  179]
train() client id: f_00007-10-2 loss: 0.329042  [   96/  179]
train() client id: f_00007-10-3 loss: 0.248855  [  128/  179]
train() client id: f_00007-10-4 loss: 0.223899  [  160/  179]
train() client id: f_00007-11-0 loss: 0.354288  [   32/  179]
train() client id: f_00007-11-1 loss: 0.254404  [   64/  179]
train() client id: f_00007-11-2 loss: 0.295404  [   96/  179]
train() client id: f_00007-11-3 loss: 0.189521  [  128/  179]
train() client id: f_00007-11-4 loss: 0.421462  [  160/  179]
train() client id: f_00007-12-0 loss: 0.247727  [   32/  179]
train() client id: f_00007-12-1 loss: 0.520476  [   64/  179]
train() client id: f_00007-12-2 loss: 0.199571  [   96/  179]
train() client id: f_00007-12-3 loss: 0.232873  [  128/  179]
train() client id: f_00007-12-4 loss: 0.285308  [  160/  179]
train() client id: f_00007-13-0 loss: 0.251455  [   32/  179]
train() client id: f_00007-13-1 loss: 0.294979  [   64/  179]
train() client id: f_00007-13-2 loss: 0.128213  [   96/  179]
train() client id: f_00007-13-3 loss: 0.528417  [  128/  179]
train() client id: f_00007-13-4 loss: 0.116069  [  160/  179]
train() client id: f_00007-14-0 loss: 0.228018  [   32/  179]
train() client id: f_00007-14-1 loss: 0.321573  [   64/  179]
train() client id: f_00007-14-2 loss: 0.332139  [   96/  179]
train() client id: f_00007-14-3 loss: 0.282957  [  128/  179]
train() client id: f_00007-14-4 loss: 0.210140  [  160/  179]
train() client id: f_00007-15-0 loss: 0.130372  [   32/  179]
train() client id: f_00007-15-1 loss: 0.396662  [   64/  179]
train() client id: f_00007-15-2 loss: 0.392224  [   96/  179]
train() client id: f_00007-15-3 loss: 0.129601  [  128/  179]
train() client id: f_00007-15-4 loss: 0.109901  [  160/  179]
train() client id: f_00008-0-0 loss: 0.678074  [   32/  130]
train() client id: f_00008-0-1 loss: 0.699705  [   64/  130]
train() client id: f_00008-0-2 loss: 0.759368  [   96/  130]
train() client id: f_00008-0-3 loss: 0.686803  [  128/  130]
train() client id: f_00008-1-0 loss: 0.721003  [   32/  130]
train() client id: f_00008-1-1 loss: 0.717669  [   64/  130]
train() client id: f_00008-1-2 loss: 0.630867  [   96/  130]
train() client id: f_00008-1-3 loss: 0.729154  [  128/  130]
train() client id: f_00008-2-0 loss: 0.648755  [   32/  130]
train() client id: f_00008-2-1 loss: 0.736858  [   64/  130]
train() client id: f_00008-2-2 loss: 0.742621  [   96/  130]
train() client id: f_00008-2-3 loss: 0.652153  [  128/  130]
train() client id: f_00008-3-0 loss: 0.705800  [   32/  130]
train() client id: f_00008-3-1 loss: 0.690899  [   64/  130]
train() client id: f_00008-3-2 loss: 0.687954  [   96/  130]
train() client id: f_00008-3-3 loss: 0.710547  [  128/  130]
train() client id: f_00008-4-0 loss: 0.674941  [   32/  130]
train() client id: f_00008-4-1 loss: 0.671290  [   64/  130]
train() client id: f_00008-4-2 loss: 0.726074  [   96/  130]
train() client id: f_00008-4-3 loss: 0.741788  [  128/  130]
train() client id: f_00008-5-0 loss: 0.775848  [   32/  130]
train() client id: f_00008-5-1 loss: 0.741474  [   64/  130]
train() client id: f_00008-5-2 loss: 0.682681  [   96/  130]
train() client id: f_00008-5-3 loss: 0.622529  [  128/  130]
train() client id: f_00008-6-0 loss: 0.722344  [   32/  130]
train() client id: f_00008-6-1 loss: 0.603177  [   64/  130]
train() client id: f_00008-6-2 loss: 0.766380  [   96/  130]
train() client id: f_00008-6-3 loss: 0.694423  [  128/  130]
train() client id: f_00008-7-0 loss: 0.731170  [   32/  130]
train() client id: f_00008-7-1 loss: 0.674024  [   64/  130]
train() client id: f_00008-7-2 loss: 0.719715  [   96/  130]
train() client id: f_00008-7-3 loss: 0.697102  [  128/  130]
train() client id: f_00008-8-0 loss: 0.744759  [   32/  130]
train() client id: f_00008-8-1 loss: 0.714806  [   64/  130]
train() client id: f_00008-8-2 loss: 0.651581  [   96/  130]
train() client id: f_00008-8-3 loss: 0.690971  [  128/  130]
train() client id: f_00008-9-0 loss: 0.754761  [   32/  130]
train() client id: f_00008-9-1 loss: 0.657324  [   64/  130]
train() client id: f_00008-9-2 loss: 0.703200  [   96/  130]
train() client id: f_00008-9-3 loss: 0.693314  [  128/  130]
train() client id: f_00008-10-0 loss: 0.628064  [   32/  130]
train() client id: f_00008-10-1 loss: 0.712293  [   64/  130]
train() client id: f_00008-10-2 loss: 0.608642  [   96/  130]
train() client id: f_00008-10-3 loss: 0.857902  [  128/  130]
train() client id: f_00008-11-0 loss: 0.771754  [   32/  130]
train() client id: f_00008-11-1 loss: 0.641676  [   64/  130]
train() client id: f_00008-11-2 loss: 0.663810  [   96/  130]
train() client id: f_00008-11-3 loss: 0.759517  [  128/  130]
train() client id: f_00008-12-0 loss: 0.741728  [   32/  130]
train() client id: f_00008-12-1 loss: 0.803996  [   64/  130]
train() client id: f_00008-12-2 loss: 0.612597  [   96/  130]
train() client id: f_00008-12-3 loss: 0.683416  [  128/  130]
train() client id: f_00008-13-0 loss: 0.748313  [   32/  130]
train() client id: f_00008-13-1 loss: 0.748690  [   64/  130]
train() client id: f_00008-13-2 loss: 0.704216  [   96/  130]
train() client id: f_00008-13-3 loss: 0.600206  [  128/  130]
train() client id: f_00008-14-0 loss: 0.639520  [   32/  130]
train() client id: f_00008-14-1 loss: 0.733744  [   64/  130]
train() client id: f_00008-14-2 loss: 0.701432  [   96/  130]
train() client id: f_00008-14-3 loss: 0.762347  [  128/  130]
train() client id: f_00008-15-0 loss: 0.673236  [   32/  130]
train() client id: f_00008-15-1 loss: 0.721153  [   64/  130]
train() client id: f_00008-15-2 loss: 0.707731  [   96/  130]
train() client id: f_00008-15-3 loss: 0.732770  [  128/  130]
train() client id: f_00009-0-0 loss: 1.169817  [   32/  118]
train() client id: f_00009-0-1 loss: 1.304250  [   64/  118]
train() client id: f_00009-0-2 loss: 1.234952  [   96/  118]
train() client id: f_00009-1-0 loss: 1.102085  [   32/  118]
train() client id: f_00009-1-1 loss: 1.311576  [   64/  118]
train() client id: f_00009-1-2 loss: 1.088659  [   96/  118]
train() client id: f_00009-2-0 loss: 1.111313  [   32/  118]
train() client id: f_00009-2-1 loss: 1.137666  [   64/  118]
train() client id: f_00009-2-2 loss: 1.095475  [   96/  118]
train() client id: f_00009-3-0 loss: 1.086183  [   32/  118]
train() client id: f_00009-3-1 loss: 1.100139  [   64/  118]
train() client id: f_00009-3-2 loss: 1.065480  [   96/  118]
train() client id: f_00009-4-0 loss: 0.990891  [   32/  118]
train() client id: f_00009-4-1 loss: 0.983573  [   64/  118]
train() client id: f_00009-4-2 loss: 0.979271  [   96/  118]
train() client id: f_00009-5-0 loss: 0.936331  [   32/  118]
train() client id: f_00009-5-1 loss: 0.984648  [   64/  118]
train() client id: f_00009-5-2 loss: 1.019803  [   96/  118]
train() client id: f_00009-6-0 loss: 0.946464  [   32/  118]
train() client id: f_00009-6-1 loss: 1.000530  [   64/  118]
train() client id: f_00009-6-2 loss: 0.912556  [   96/  118]
train() client id: f_00009-7-0 loss: 0.907461  [   32/  118]
train() client id: f_00009-7-1 loss: 0.967911  [   64/  118]
train() client id: f_00009-7-2 loss: 0.962041  [   96/  118]
train() client id: f_00009-8-0 loss: 0.790267  [   32/  118]
train() client id: f_00009-8-1 loss: 0.901000  [   64/  118]
train() client id: f_00009-8-2 loss: 1.067756  [   96/  118]
train() client id: f_00009-9-0 loss: 1.018198  [   32/  118]
train() client id: f_00009-9-1 loss: 0.913818  [   64/  118]
train() client id: f_00009-9-2 loss: 0.792008  [   96/  118]
train() client id: f_00009-10-0 loss: 0.748231  [   32/  118]
train() client id: f_00009-10-1 loss: 0.861609  [   64/  118]
train() client id: f_00009-10-2 loss: 1.064514  [   96/  118]
train() client id: f_00009-11-0 loss: 1.063047  [   32/  118]
train() client id: f_00009-11-1 loss: 0.941382  [   64/  118]
train() client id: f_00009-11-2 loss: 0.784817  [   96/  118]
train() client id: f_00009-12-0 loss: 0.974422  [   32/  118]
train() client id: f_00009-12-1 loss: 0.837620  [   64/  118]
train() client id: f_00009-12-2 loss: 0.777287  [   96/  118]
train() client id: f_00009-13-0 loss: 0.971356  [   32/  118]
train() client id: f_00009-13-1 loss: 0.835732  [   64/  118]
train() client id: f_00009-13-2 loss: 0.922422  [   96/  118]
train() client id: f_00009-14-0 loss: 0.738259  [   32/  118]
train() client id: f_00009-14-1 loss: 0.997641  [   64/  118]
train() client id: f_00009-14-2 loss: 0.910101  [   96/  118]
train() client id: f_00009-15-0 loss: 0.821565  [   32/  118]
train() client id: f_00009-15-1 loss: 0.825248  [   64/  118]
train() client id: f_00009-15-2 loss: 1.013678  [   96/  118]
At round 50 accuracy: 0.6472148541114059
At round 50 training accuracy: 0.5928906773977196
At round 50 training loss: 0.8310206180638697
update_location
xs = 8.927491 371.223621 5.882650 0.934260 -287.581990 -135.230757 -95.849135 -5.143845 -310.120581 20.134486 
ys = -362.390647 7.291448 260.684448 -82.290817 -9.642386 0.794442 -1.381692 256.628436 25.881276 -797.232496 
xs mean: -42.682379970521225
ys mean: -70.16579882624052
dists_uav = 376.040797 384.525866 279.268665 129.509272 304.624977 168.190335 138.524242 275.471620 326.870946 803.731952 
uav_gains = -120.444663 -120.803591 -113.913744 -102.808143 -116.110364 -105.664925 -103.539710 -113.568727 -117.763619 -129.595353 
uav_gains_db_mean: -114.42128395344835
dists_bs = 567.994744 571.389887 200.150837 311.691239 216.257983 178.687350 193.327083 188.452421 201.231124 991.621648 
bs_gains = -116.688700 -116.761171 -104.005067 -109.391348 -104.946279 -102.625686 -103.583255 -103.272707 -104.070523 -123.464747 
bs_gains_db_mean: -108.88094826706188
Round 51
-------------------------------
ene_coms = [0.02949377 0.030977   0.00755515 0.0071324  0.00792357 0.00822925
 0.00738509 0.00729086 0.00757968 0.04617592]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.43120391  8.98356591  4.12853597  1.48049903 10.07215411  4.85380787
  1.8422221   5.92061965  4.3246135   4.35063831]
obj_prev = 50.387860366927
eta_min = 3.2619605945949497e-22	eta_max = 0.8179601764247351
af = 10.460568009377656	bf = 1.7112858379709448	zeta = 11.506624810315422	eta = 0.9090909090909091
af = 10.460568009377656	bf = 1.7112858379709448	zeta = 29.791129013801072	eta = 0.3511302980337429
af = 10.460568009377656	bf = 1.7112858379709448	zeta = 19.509763458859602	eta = 0.5361709295674415
af = 10.460568009377656	bf = 1.7112858379709448	zeta = 17.746584143997737	eta = 0.5894412087700628
af = 10.460568009377656	bf = 1.7112858379709448	zeta = 17.635807862896613	eta = 0.5931436819169081
af = 10.460568009377656	bf = 1.7112858379709448	zeta = 17.635304871888923	eta = 0.5931605994547925
af = 10.460568009377656	bf = 1.7112858379709448	zeta = 17.635304861431123	eta = 0.5931605998065389
eta = 0.5931605998065389
ene_coms = [0.02949377 0.030977   0.00755515 0.0071324  0.00792357 0.00822925
 0.00738509 0.00729086 0.00757968 0.04617592]
ene_comp = [0.04309813 0.09064288 0.04241403 0.01470809 0.10466688 0.0499391
 0.01847063 0.0612267  0.04446632 0.04036174]
ene_total = [1.91146655 3.20245591 1.31577237 0.57509675 2.96469552 1.5316704
 0.68082438 1.80418241 1.37045874 2.27868183]
ti_comp = [0.74572046 0.73088812 0.96510667 0.96933418 0.96142249 0.95836563
 0.96680727 0.9677496  0.96486131 0.57889897]
ti_coms = [0.2949377  0.30977003 0.07555149 0.07132398 0.07923567 0.08229253
 0.07385089 0.07290856 0.07579685 0.46175919]
t_total = [27.40227699 27.40227699 27.40227699 27.40227699 27.40227699 27.40227699
 27.40227699 27.40227699 27.40227699 27.40227699]
ene_coms = [0.02949377 0.030977   0.00755515 0.0071324  0.00792357 0.00822925
 0.00738509 0.00729086 0.00757968 0.04617592]
ene_comp = [8.99711098e-06 8.71323377e-05 5.11985869e-06 2.11642007e-07
 7.75316956e-05 8.47499918e-06 4.21351960e-07 1.53171036e-05
 5.90260646e-06 1.22626413e-05]
ene_total = [0.77685741 0.81797086 0.19907457 0.18781359 0.21068237 0.21691322
 0.19447288 0.19238381 0.19974126 1.21621239]
optimize_network iter = 0 obj = 4.212122366477622
eta = 0.5931605998065389
freqs = [28896972.12604236 62008727.47431243 21973751.93966112  7586697.7157155
 54433342.16168091 26054304.07502432  9552382.45201615 31633546.36729429
 23042856.28738734 34860778.51262455]
eta_min = 0.5931605998065396	eta_max = 0.5931605998065371
af = 0.004540578484321147	bf = 1.7112858379709448	zeta = 0.004994636332753263	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02949377 0.030977   0.00755515 0.0071324  0.00792357 0.00822925
 0.00738509 0.00729086 0.00757968 0.04617592]
ene_comp = [1.64183905e-06 1.59003568e-05 9.34298125e-07 3.86215210e-08
 1.41483822e-05 1.54656140e-06 7.68904710e-08 2.79514378e-06
 1.07713796e-06 2.23774980e-06]
ene_total = [3.47575149 3.65221653 0.89041192 0.84048928 0.93538365 0.96992082
 0.87027095 0.85948687 0.89332012 5.44165297]
ti_comp = [0.74572046 0.73088812 0.96510667 0.96933418 0.96142249 0.95836563
 0.96680727 0.9677496  0.96486131 0.57889897]
ti_coms = [0.2949377  0.30977003 0.07555149 0.07132398 0.07923567 0.08229253
 0.07385089 0.07290856 0.07579685 0.46175919]
t_total = [27.40227699 27.40227699 27.40227699 27.40227699 27.40227699 27.40227699
 27.40227699 27.40227699 27.40227699 27.40227699]
ene_coms = [0.02949377 0.030977   0.00755515 0.0071324  0.00792357 0.00822925
 0.00738509 0.00729086 0.00757968 0.04617592]
ene_comp = [8.99711098e-06 8.71323377e-05 5.11985869e-06 2.11642007e-07
 7.75316956e-05 8.47499918e-06 4.21351960e-07 1.53171036e-05
 5.90260646e-06 1.22626413e-05]
ene_total = [0.77685741 0.81797086 0.19907457 0.18781359 0.21068237 0.21691322
 0.19447288 0.19238381 0.19974126 1.21621239]
optimize_network iter = 1 obj = 4.212122366477603
eta = 0.5931605998065371
freqs = [28896972.12604235 62008727.47431239 21973751.93966115  7586697.71571551
 54433342.16168096 26054304.07502434  9552382.45201616 31633546.36729432
 23042856.28738737 34860778.51262448]
Done!
ene_coms = [0.02949377 0.030977   0.00755515 0.0071324  0.00792357 0.00822925
 0.00738509 0.00729086 0.00757968 0.04617592]
ene_comp = [8.94322483e-06 8.66104784e-05 5.08919446e-06 2.10374425e-07
 7.70673371e-05 8.42424010e-06 4.18828368e-07 1.52253653e-05
 5.86725415e-06 1.21891970e-05]
ene_total = [0.02950271 0.03106361 0.00756024 0.00713261 0.00800063 0.00823768
 0.00738551 0.00730608 0.00758555 0.04618811]
At round 51 energy consumption: 0.1599627349074556
At round 51 eta: 0.5931605998065371
At round 51 a_n: 10.712764660819193
At round 51 local rounds: 17.102431120433863
At round 51 global rounds: 26.331679418770626
gradient difference: 0.32080206274986267
train() client id: f_00000-0-0 loss: 1.191819  [   32/  126]
train() client id: f_00000-0-1 loss: 1.300414  [   64/  126]
train() client id: f_00000-0-2 loss: 1.372826  [   96/  126]
train() client id: f_00000-1-0 loss: 1.186618  [   32/  126]
train() client id: f_00000-1-1 loss: 1.121415  [   64/  126]
train() client id: f_00000-1-2 loss: 0.992878  [   96/  126]
train() client id: f_00000-2-0 loss: 1.215460  [   32/  126]
train() client id: f_00000-2-1 loss: 0.864463  [   64/  126]
train() client id: f_00000-2-2 loss: 1.078466  [   96/  126]
train() client id: f_00000-3-0 loss: 0.947244  [   32/  126]
train() client id: f_00000-3-1 loss: 1.012258  [   64/  126]
train() client id: f_00000-3-2 loss: 0.960660  [   96/  126]
train() client id: f_00000-4-0 loss: 0.790563  [   32/  126]
train() client id: f_00000-4-1 loss: 0.951143  [   64/  126]
train() client id: f_00000-4-2 loss: 0.824333  [   96/  126]
train() client id: f_00000-5-0 loss: 0.900896  [   32/  126]
train() client id: f_00000-5-1 loss: 0.826988  [   64/  126]
train() client id: f_00000-5-2 loss: 0.848661  [   96/  126]
train() client id: f_00000-6-0 loss: 0.778298  [   32/  126]
train() client id: f_00000-6-1 loss: 0.742208  [   64/  126]
train() client id: f_00000-6-2 loss: 0.795742  [   96/  126]
train() client id: f_00000-7-0 loss: 0.772490  [   32/  126]
train() client id: f_00000-7-1 loss: 0.712929  [   64/  126]
train() client id: f_00000-7-2 loss: 0.746484  [   96/  126]
train() client id: f_00000-8-0 loss: 0.794185  [   32/  126]
train() client id: f_00000-8-1 loss: 0.754583  [   64/  126]
train() client id: f_00000-8-2 loss: 0.743483  [   96/  126]
train() client id: f_00000-9-0 loss: 0.893799  [   32/  126]
train() client id: f_00000-9-1 loss: 0.683740  [   64/  126]
train() client id: f_00000-9-2 loss: 0.622152  [   96/  126]
train() client id: f_00000-10-0 loss: 0.604431  [   32/  126]
train() client id: f_00000-10-1 loss: 0.683607  [   64/  126]
train() client id: f_00000-10-2 loss: 0.755017  [   96/  126]
train() client id: f_00000-11-0 loss: 0.820287  [   32/  126]
train() client id: f_00000-11-1 loss: 0.680622  [   64/  126]
train() client id: f_00000-11-2 loss: 0.659476  [   96/  126]
train() client id: f_00000-12-0 loss: 0.851828  [   32/  126]
train() client id: f_00000-12-1 loss: 0.581966  [   64/  126]
train() client id: f_00000-12-2 loss: 0.586546  [   96/  126]
train() client id: f_00000-13-0 loss: 0.584743  [   32/  126]
train() client id: f_00000-13-1 loss: 0.754504  [   64/  126]
train() client id: f_00000-13-2 loss: 0.739496  [   96/  126]
train() client id: f_00000-14-0 loss: 0.725533  [   32/  126]
train() client id: f_00000-14-1 loss: 0.651100  [   64/  126]
train() client id: f_00000-14-2 loss: 0.677845  [   96/  126]
train() client id: f_00000-15-0 loss: 0.746539  [   32/  126]
train() client id: f_00000-15-1 loss: 0.651112  [   64/  126]
train() client id: f_00000-15-2 loss: 0.609796  [   96/  126]
train() client id: f_00000-16-0 loss: 0.670256  [   32/  126]
train() client id: f_00000-16-1 loss: 0.786616  [   64/  126]
train() client id: f_00000-16-2 loss: 0.597472  [   96/  126]
train() client id: f_00001-0-0 loss: 0.322929  [   32/  265]
train() client id: f_00001-0-1 loss: 0.361289  [   64/  265]
train() client id: f_00001-0-2 loss: 0.404737  [   96/  265]
train() client id: f_00001-0-3 loss: 0.369319  [  128/  265]
train() client id: f_00001-0-4 loss: 0.355900  [  160/  265]
train() client id: f_00001-0-5 loss: 0.299104  [  192/  265]
train() client id: f_00001-0-6 loss: 0.309238  [  224/  265]
train() client id: f_00001-0-7 loss: 0.497072  [  256/  265]
train() client id: f_00001-1-0 loss: 0.373187  [   32/  265]
train() client id: f_00001-1-1 loss: 0.499858  [   64/  265]
train() client id: f_00001-1-2 loss: 0.286337  [   96/  265]
train() client id: f_00001-1-3 loss: 0.388644  [  128/  265]
train() client id: f_00001-1-4 loss: 0.387904  [  160/  265]
train() client id: f_00001-1-5 loss: 0.335832  [  192/  265]
train() client id: f_00001-1-6 loss: 0.331305  [  224/  265]
train() client id: f_00001-1-7 loss: 0.256186  [  256/  265]
train() client id: f_00001-2-0 loss: 0.427360  [   32/  265]
train() client id: f_00001-2-1 loss: 0.440566  [   64/  265]
train() client id: f_00001-2-2 loss: 0.293253  [   96/  265]
train() client id: f_00001-2-3 loss: 0.388673  [  128/  265]
train() client id: f_00001-2-4 loss: 0.345883  [  160/  265]
train() client id: f_00001-2-5 loss: 0.354355  [  192/  265]
train() client id: f_00001-2-6 loss: 0.308908  [  224/  265]
train() client id: f_00001-2-7 loss: 0.237613  [  256/  265]
train() client id: f_00001-3-0 loss: 0.381541  [   32/  265]
train() client id: f_00001-3-1 loss: 0.313446  [   64/  265]
train() client id: f_00001-3-2 loss: 0.336969  [   96/  265]
train() client id: f_00001-3-3 loss: 0.439314  [  128/  265]
train() client id: f_00001-3-4 loss: 0.410623  [  160/  265]
train() client id: f_00001-3-5 loss: 0.281854  [  192/  265]
train() client id: f_00001-3-6 loss: 0.342169  [  224/  265]
train() client id: f_00001-3-7 loss: 0.223993  [  256/  265]
train() client id: f_00001-4-0 loss: 0.292863  [   32/  265]
train() client id: f_00001-4-1 loss: 0.621162  [   64/  265]
train() client id: f_00001-4-2 loss: 0.332086  [   96/  265]
train() client id: f_00001-4-3 loss: 0.226045  [  128/  265]
train() client id: f_00001-4-4 loss: 0.277236  [  160/  265]
train() client id: f_00001-4-5 loss: 0.272306  [  192/  265]
train() client id: f_00001-4-6 loss: 0.286196  [  224/  265]
train() client id: f_00001-4-7 loss: 0.364094  [  256/  265]
train() client id: f_00001-5-0 loss: 0.394008  [   32/  265]
train() client id: f_00001-5-1 loss: 0.406633  [   64/  265]
train() client id: f_00001-5-2 loss: 0.310059  [   96/  265]
train() client id: f_00001-5-3 loss: 0.368331  [  128/  265]
train() client id: f_00001-5-4 loss: 0.288247  [  160/  265]
train() client id: f_00001-5-5 loss: 0.407430  [  192/  265]
train() client id: f_00001-5-6 loss: 0.231322  [  224/  265]
train() client id: f_00001-5-7 loss: 0.238426  [  256/  265]
train() client id: f_00001-6-0 loss: 0.237820  [   32/  265]
train() client id: f_00001-6-1 loss: 0.235954  [   64/  265]
train() client id: f_00001-6-2 loss: 0.345175  [   96/  265]
train() client id: f_00001-6-3 loss: 0.318186  [  128/  265]
train() client id: f_00001-6-4 loss: 0.439503  [  160/  265]
train() client id: f_00001-6-5 loss: 0.347684  [  192/  265]
train() client id: f_00001-6-6 loss: 0.448361  [  224/  265]
train() client id: f_00001-6-7 loss: 0.224407  [  256/  265]
train() client id: f_00001-7-0 loss: 0.332064  [   32/  265]
train() client id: f_00001-7-1 loss: 0.234331  [   64/  265]
train() client id: f_00001-7-2 loss: 0.307460  [   96/  265]
train() client id: f_00001-7-3 loss: 0.355424  [  128/  265]
train() client id: f_00001-7-4 loss: 0.292630  [  160/  265]
train() client id: f_00001-7-5 loss: 0.355231  [  192/  265]
train() client id: f_00001-7-6 loss: 0.353127  [  224/  265]
train() client id: f_00001-7-7 loss: 0.344769  [  256/  265]
train() client id: f_00001-8-0 loss: 0.419068  [   32/  265]
train() client id: f_00001-8-1 loss: 0.308976  [   64/  265]
train() client id: f_00001-8-2 loss: 0.222573  [   96/  265]
train() client id: f_00001-8-3 loss: 0.355764  [  128/  265]
train() client id: f_00001-8-4 loss: 0.287978  [  160/  265]
train() client id: f_00001-8-5 loss: 0.310287  [  192/  265]
train() client id: f_00001-8-6 loss: 0.313347  [  224/  265]
train() client id: f_00001-8-7 loss: 0.348562  [  256/  265]
train() client id: f_00001-9-0 loss: 0.309437  [   32/  265]
train() client id: f_00001-9-1 loss: 0.287289  [   64/  265]
train() client id: f_00001-9-2 loss: 0.289950  [   96/  265]
train() client id: f_00001-9-3 loss: 0.280796  [  128/  265]
train() client id: f_00001-9-4 loss: 0.481019  [  160/  265]
train() client id: f_00001-9-5 loss: 0.275039  [  192/  265]
train() client id: f_00001-9-6 loss: 0.389071  [  224/  265]
train() client id: f_00001-9-7 loss: 0.228699  [  256/  265]
train() client id: f_00001-10-0 loss: 0.259278  [   32/  265]
train() client id: f_00001-10-1 loss: 0.439981  [   64/  265]
train() client id: f_00001-10-2 loss: 0.393195  [   96/  265]
train() client id: f_00001-10-3 loss: 0.251950  [  128/  265]
train() client id: f_00001-10-4 loss: 0.230571  [  160/  265]
train() client id: f_00001-10-5 loss: 0.338080  [  192/  265]
train() client id: f_00001-10-6 loss: 0.241936  [  224/  265]
train() client id: f_00001-10-7 loss: 0.375066  [  256/  265]
train() client id: f_00001-11-0 loss: 0.279704  [   32/  265]
train() client id: f_00001-11-1 loss: 0.226734  [   64/  265]
train() client id: f_00001-11-2 loss: 0.270225  [   96/  265]
train() client id: f_00001-11-3 loss: 0.439594  [  128/  265]
train() client id: f_00001-11-4 loss: 0.298555  [  160/  265]
train() client id: f_00001-11-5 loss: 0.363847  [  192/  265]
train() client id: f_00001-11-6 loss: 0.225314  [  224/  265]
train() client id: f_00001-11-7 loss: 0.416935  [  256/  265]
train() client id: f_00001-12-0 loss: 0.221806  [   32/  265]
train() client id: f_00001-12-1 loss: 0.365652  [   64/  265]
train() client id: f_00001-12-2 loss: 0.371693  [   96/  265]
train() client id: f_00001-12-3 loss: 0.261807  [  128/  265]
train() client id: f_00001-12-4 loss: 0.391058  [  160/  265]
train() client id: f_00001-12-5 loss: 0.294755  [  192/  265]
train() client id: f_00001-12-6 loss: 0.226084  [  224/  265]
train() client id: f_00001-12-7 loss: 0.304561  [  256/  265]
train() client id: f_00001-13-0 loss: 0.357235  [   32/  265]
train() client id: f_00001-13-1 loss: 0.352496  [   64/  265]
train() client id: f_00001-13-2 loss: 0.194425  [   96/  265]
train() client id: f_00001-13-3 loss: 0.342817  [  128/  265]
train() client id: f_00001-13-4 loss: 0.239009  [  160/  265]
train() client id: f_00001-13-5 loss: 0.420336  [  192/  265]
train() client id: f_00001-13-6 loss: 0.314338  [  224/  265]
train() client id: f_00001-13-7 loss: 0.276356  [  256/  265]
train() client id: f_00001-14-0 loss: 0.351020  [   32/  265]
train() client id: f_00001-14-1 loss: 0.249692  [   64/  265]
train() client id: f_00001-14-2 loss: 0.285954  [   96/  265]
train() client id: f_00001-14-3 loss: 0.275300  [  128/  265]
train() client id: f_00001-14-4 loss: 0.392951  [  160/  265]
train() client id: f_00001-14-5 loss: 0.316467  [  192/  265]
train() client id: f_00001-14-6 loss: 0.232061  [  224/  265]
train() client id: f_00001-14-7 loss: 0.317589  [  256/  265]
train() client id: f_00001-15-0 loss: 0.255549  [   32/  265]
train() client id: f_00001-15-1 loss: 0.479271  [   64/  265]
train() client id: f_00001-15-2 loss: 0.210804  [   96/  265]
train() client id: f_00001-15-3 loss: 0.323363  [  128/  265]
train() client id: f_00001-15-4 loss: 0.287375  [  160/  265]
train() client id: f_00001-15-5 loss: 0.391475  [  192/  265]
train() client id: f_00001-15-6 loss: 0.234062  [  224/  265]
train() client id: f_00001-15-7 loss: 0.304718  [  256/  265]
train() client id: f_00001-16-0 loss: 0.318239  [   32/  265]
train() client id: f_00001-16-1 loss: 0.289665  [   64/  265]
train() client id: f_00001-16-2 loss: 0.345842  [   96/  265]
train() client id: f_00001-16-3 loss: 0.216439  [  128/  265]
train() client id: f_00001-16-4 loss: 0.305609  [  160/  265]
train() client id: f_00001-16-5 loss: 0.417623  [  192/  265]
train() client id: f_00001-16-6 loss: 0.311204  [  224/  265]
train() client id: f_00001-16-7 loss: 0.263220  [  256/  265]
train() client id: f_00002-0-0 loss: 1.190408  [   32/  124]
train() client id: f_00002-0-1 loss: 1.298358  [   64/  124]
train() client id: f_00002-0-2 loss: 1.131682  [   96/  124]
train() client id: f_00002-1-0 loss: 1.253111  [   32/  124]
train() client id: f_00002-1-1 loss: 0.954731  [   64/  124]
train() client id: f_00002-1-2 loss: 1.240705  [   96/  124]
train() client id: f_00002-2-0 loss: 1.155713  [   32/  124]
train() client id: f_00002-2-1 loss: 1.156510  [   64/  124]
train() client id: f_00002-2-2 loss: 1.124002  [   96/  124]
train() client id: f_00002-3-0 loss: 1.085592  [   32/  124]
train() client id: f_00002-3-1 loss: 1.096016  [   64/  124]
train() client id: f_00002-3-2 loss: 1.172620  [   96/  124]
train() client id: f_00002-4-0 loss: 0.915176  [   32/  124]
train() client id: f_00002-4-1 loss: 1.152334  [   64/  124]
train() client id: f_00002-4-2 loss: 1.011681  [   96/  124]
train() client id: f_00002-5-0 loss: 0.955252  [   32/  124]
train() client id: f_00002-5-1 loss: 1.011146  [   64/  124]
train() client id: f_00002-5-2 loss: 1.145847  [   96/  124]
train() client id: f_00002-6-0 loss: 1.134197  [   32/  124]
train() client id: f_00002-6-1 loss: 0.877542  [   64/  124]
train() client id: f_00002-6-2 loss: 0.925040  [   96/  124]
train() client id: f_00002-7-0 loss: 1.085738  [   32/  124]
train() client id: f_00002-7-1 loss: 0.839286  [   64/  124]
train() client id: f_00002-7-2 loss: 1.001706  [   96/  124]
train() client id: f_00002-8-0 loss: 1.068120  [   32/  124]
train() client id: f_00002-8-1 loss: 0.907443  [   64/  124]
train() client id: f_00002-8-2 loss: 0.813598  [   96/  124]
train() client id: f_00002-9-0 loss: 0.940292  [   32/  124]
train() client id: f_00002-9-1 loss: 0.913816  [   64/  124]
train() client id: f_00002-9-2 loss: 0.699569  [   96/  124]
train() client id: f_00002-10-0 loss: 1.056654  [   32/  124]
train() client id: f_00002-10-1 loss: 0.761758  [   64/  124]
train() client id: f_00002-10-2 loss: 0.826803  [   96/  124]
train() client id: f_00002-11-0 loss: 0.909816  [   32/  124]
train() client id: f_00002-11-1 loss: 0.831020  [   64/  124]
train() client id: f_00002-11-2 loss: 0.977854  [   96/  124]
train() client id: f_00002-12-0 loss: 0.784957  [   32/  124]
train() client id: f_00002-12-1 loss: 0.924679  [   64/  124]
train() client id: f_00002-12-2 loss: 0.987185  [   96/  124]
train() client id: f_00002-13-0 loss: 0.824008  [   32/  124]
train() client id: f_00002-13-1 loss: 0.790268  [   64/  124]
train() client id: f_00002-13-2 loss: 1.128318  [   96/  124]
train() client id: f_00002-14-0 loss: 0.826337  [   32/  124]
train() client id: f_00002-14-1 loss: 0.791064  [   64/  124]
train() client id: f_00002-14-2 loss: 1.076467  [   96/  124]
train() client id: f_00002-15-0 loss: 0.806848  [   32/  124]
train() client id: f_00002-15-1 loss: 0.739131  [   64/  124]
train() client id: f_00002-15-2 loss: 0.824082  [   96/  124]
train() client id: f_00002-16-0 loss: 0.856032  [   32/  124]
train() client id: f_00002-16-1 loss: 0.982730  [   64/  124]
train() client id: f_00002-16-2 loss: 0.926736  [   96/  124]
train() client id: f_00003-0-0 loss: 0.598494  [   32/   43]
train() client id: f_00003-1-0 loss: 0.748481  [   32/   43]
train() client id: f_00003-2-0 loss: 0.632051  [   32/   43]
train() client id: f_00003-3-0 loss: 0.520415  [   32/   43]
train() client id: f_00003-4-0 loss: 0.713806  [   32/   43]
train() client id: f_00003-5-0 loss: 0.821807  [   32/   43]
train() client id: f_00003-6-0 loss: 0.545430  [   32/   43]
train() client id: f_00003-7-0 loss: 0.401777  [   32/   43]
train() client id: f_00003-8-0 loss: 0.967838  [   32/   43]
train() client id: f_00003-9-0 loss: 0.731759  [   32/   43]
train() client id: f_00003-10-0 loss: 0.571052  [   32/   43]
train() client id: f_00003-11-0 loss: 0.585803  [   32/   43]
train() client id: f_00003-12-0 loss: 0.571643  [   32/   43]
train() client id: f_00003-13-0 loss: 0.549617  [   32/   43]
train() client id: f_00003-14-0 loss: 0.656560  [   32/   43]
train() client id: f_00003-15-0 loss: 0.507449  [   32/   43]
train() client id: f_00003-16-0 loss: 0.555692  [   32/   43]
train() client id: f_00004-0-0 loss: 1.129006  [   32/  306]
train() client id: f_00004-0-1 loss: 1.048842  [   64/  306]
train() client id: f_00004-0-2 loss: 1.032493  [   96/  306]
train() client id: f_00004-0-3 loss: 1.077370  [  128/  306]
train() client id: f_00004-0-4 loss: 0.897484  [  160/  306]
train() client id: f_00004-0-5 loss: 0.888933  [  192/  306]
train() client id: f_00004-0-6 loss: 0.822028  [  224/  306]
train() client id: f_00004-0-7 loss: 0.987648  [  256/  306]
train() client id: f_00004-0-8 loss: 0.809664  [  288/  306]
train() client id: f_00004-1-0 loss: 1.002295  [   32/  306]
train() client id: f_00004-1-1 loss: 0.975013  [   64/  306]
train() client id: f_00004-1-2 loss: 0.997877  [   96/  306]
train() client id: f_00004-1-3 loss: 0.829865  [  128/  306]
train() client id: f_00004-1-4 loss: 1.067195  [  160/  306]
train() client id: f_00004-1-5 loss: 0.898071  [  192/  306]
train() client id: f_00004-1-6 loss: 0.834398  [  224/  306]
train() client id: f_00004-1-7 loss: 0.937591  [  256/  306]
train() client id: f_00004-1-8 loss: 1.052915  [  288/  306]
train() client id: f_00004-2-0 loss: 0.966397  [   32/  306]
train() client id: f_00004-2-1 loss: 0.953534  [   64/  306]
train() client id: f_00004-2-2 loss: 0.818189  [   96/  306]
train() client id: f_00004-2-3 loss: 1.087878  [  128/  306]
train() client id: f_00004-2-4 loss: 1.102483  [  160/  306]
train() client id: f_00004-2-5 loss: 1.034042  [  192/  306]
train() client id: f_00004-2-6 loss: 0.884951  [  224/  306]
train() client id: f_00004-2-7 loss: 0.991000  [  256/  306]
train() client id: f_00004-2-8 loss: 0.820403  [  288/  306]
train() client id: f_00004-3-0 loss: 0.957897  [   32/  306]
train() client id: f_00004-3-1 loss: 0.871144  [   64/  306]
train() client id: f_00004-3-2 loss: 0.871021  [   96/  306]
train() client id: f_00004-3-3 loss: 0.962128  [  128/  306]
train() client id: f_00004-3-4 loss: 0.838930  [  160/  306]
train() client id: f_00004-3-5 loss: 0.991056  [  192/  306]
train() client id: f_00004-3-6 loss: 0.943565  [  224/  306]
train() client id: f_00004-3-7 loss: 1.020603  [  256/  306]
train() client id: f_00004-3-8 loss: 1.142969  [  288/  306]
train() client id: f_00004-4-0 loss: 0.897884  [   32/  306]
train() client id: f_00004-4-1 loss: 0.953826  [   64/  306]
train() client id: f_00004-4-2 loss: 1.014798  [   96/  306]
train() client id: f_00004-4-3 loss: 0.948654  [  128/  306]
train() client id: f_00004-4-4 loss: 0.875300  [  160/  306]
train() client id: f_00004-4-5 loss: 0.956349  [  192/  306]
train() client id: f_00004-4-6 loss: 1.024451  [  224/  306]
train() client id: f_00004-4-7 loss: 0.963493  [  256/  306]
train() client id: f_00004-4-8 loss: 0.998690  [  288/  306]
train() client id: f_00004-5-0 loss: 1.004570  [   32/  306]
train() client id: f_00004-5-1 loss: 0.872128  [   64/  306]
train() client id: f_00004-5-2 loss: 0.948717  [   96/  306]
train() client id: f_00004-5-3 loss: 0.913162  [  128/  306]
train() client id: f_00004-5-4 loss: 1.108757  [  160/  306]
train() client id: f_00004-5-5 loss: 0.879129  [  192/  306]
train() client id: f_00004-5-6 loss: 1.004855  [  224/  306]
train() client id: f_00004-5-7 loss: 0.934118  [  256/  306]
train() client id: f_00004-5-8 loss: 0.973291  [  288/  306]
train() client id: f_00004-6-0 loss: 0.891509  [   32/  306]
train() client id: f_00004-6-1 loss: 0.994854  [   64/  306]
train() client id: f_00004-6-2 loss: 1.036931  [   96/  306]
train() client id: f_00004-6-3 loss: 0.836333  [  128/  306]
train() client id: f_00004-6-4 loss: 0.979240  [  160/  306]
train() client id: f_00004-6-5 loss: 0.949955  [  192/  306]
train() client id: f_00004-6-6 loss: 1.045092  [  224/  306]
train() client id: f_00004-6-7 loss: 0.981742  [  256/  306]
train() client id: f_00004-6-8 loss: 0.832927  [  288/  306]
train() client id: f_00004-7-0 loss: 0.980999  [   32/  306]
train() client id: f_00004-7-1 loss: 0.933608  [   64/  306]
train() client id: f_00004-7-2 loss: 0.871216  [   96/  306]
train() client id: f_00004-7-3 loss: 0.780720  [  128/  306]
train() client id: f_00004-7-4 loss: 0.859682  [  160/  306]
train() client id: f_00004-7-5 loss: 1.010897  [  192/  306]
train() client id: f_00004-7-6 loss: 0.939024  [  224/  306]
train() client id: f_00004-7-7 loss: 1.104518  [  256/  306]
train() client id: f_00004-7-8 loss: 1.045401  [  288/  306]
train() client id: f_00004-8-0 loss: 1.029690  [   32/  306]
train() client id: f_00004-8-1 loss: 0.870407  [   64/  306]
train() client id: f_00004-8-2 loss: 0.913498  [   96/  306]
train() client id: f_00004-8-3 loss: 0.982811  [  128/  306]
train() client id: f_00004-8-4 loss: 0.908245  [  160/  306]
train() client id: f_00004-8-5 loss: 0.852184  [  192/  306]
train() client id: f_00004-8-6 loss: 1.002563  [  224/  306]
train() client id: f_00004-8-7 loss: 0.865646  [  256/  306]
train() client id: f_00004-8-8 loss: 0.992059  [  288/  306]
train() client id: f_00004-9-0 loss: 0.817343  [   32/  306]
train() client id: f_00004-9-1 loss: 0.992304  [   64/  306]
train() client id: f_00004-9-2 loss: 0.982062  [   96/  306]
train() client id: f_00004-9-3 loss: 0.923392  [  128/  306]
train() client id: f_00004-9-4 loss: 0.906340  [  160/  306]
train() client id: f_00004-9-5 loss: 0.892247  [  192/  306]
train() client id: f_00004-9-6 loss: 1.074970  [  224/  306]
train() client id: f_00004-9-7 loss: 0.915242  [  256/  306]
train() client id: f_00004-9-8 loss: 1.010357  [  288/  306]
train() client id: f_00004-10-0 loss: 1.035130  [   32/  306]
train() client id: f_00004-10-1 loss: 0.865856  [   64/  306]
train() client id: f_00004-10-2 loss: 0.915614  [   96/  306]
train() client id: f_00004-10-3 loss: 0.968454  [  128/  306]
train() client id: f_00004-10-4 loss: 0.955567  [  160/  306]
train() client id: f_00004-10-5 loss: 0.952848  [  192/  306]
train() client id: f_00004-10-6 loss: 0.874466  [  224/  306]
train() client id: f_00004-10-7 loss: 1.018992  [  256/  306]
train() client id: f_00004-10-8 loss: 0.832651  [  288/  306]
train() client id: f_00004-11-0 loss: 0.922073  [   32/  306]
train() client id: f_00004-11-1 loss: 0.990125  [   64/  306]
train() client id: f_00004-11-2 loss: 1.043950  [   96/  306]
train() client id: f_00004-11-3 loss: 0.998778  [  128/  306]
train() client id: f_00004-11-4 loss: 0.828188  [  160/  306]
train() client id: f_00004-11-5 loss: 0.836791  [  192/  306]
train() client id: f_00004-11-6 loss: 0.978241  [  224/  306]
train() client id: f_00004-11-7 loss: 0.945325  [  256/  306]
train() client id: f_00004-11-8 loss: 0.941896  [  288/  306]
train() client id: f_00004-12-0 loss: 0.910908  [   32/  306]
train() client id: f_00004-12-1 loss: 0.877727  [   64/  306]
train() client id: f_00004-12-2 loss: 1.028745  [   96/  306]
train() client id: f_00004-12-3 loss: 0.985072  [  128/  306]
train() client id: f_00004-12-4 loss: 0.905688  [  160/  306]
train() client id: f_00004-12-5 loss: 0.877350  [  192/  306]
train() client id: f_00004-12-6 loss: 0.872526  [  224/  306]
train() client id: f_00004-12-7 loss: 0.953989  [  256/  306]
train() client id: f_00004-12-8 loss: 0.948566  [  288/  306]
train() client id: f_00004-13-0 loss: 0.975515  [   32/  306]
train() client id: f_00004-13-1 loss: 0.880020  [   64/  306]
train() client id: f_00004-13-2 loss: 1.068767  [   96/  306]
train() client id: f_00004-13-3 loss: 1.003317  [  128/  306]
train() client id: f_00004-13-4 loss: 0.920878  [  160/  306]
train() client id: f_00004-13-5 loss: 0.769404  [  192/  306]
train() client id: f_00004-13-6 loss: 0.883347  [  224/  306]
train() client id: f_00004-13-7 loss: 1.086540  [  256/  306]
train() client id: f_00004-13-8 loss: 0.844776  [  288/  306]
train() client id: f_00004-14-0 loss: 0.965064  [   32/  306]
train() client id: f_00004-14-1 loss: 0.905374  [   64/  306]
train() client id: f_00004-14-2 loss: 0.825386  [   96/  306]
train() client id: f_00004-14-3 loss: 0.949685  [  128/  306]
train() client id: f_00004-14-4 loss: 0.890143  [  160/  306]
train() client id: f_00004-14-5 loss: 1.022525  [  192/  306]
train() client id: f_00004-14-6 loss: 0.864301  [  224/  306]
train() client id: f_00004-14-7 loss: 0.959587  [  256/  306]
train() client id: f_00004-14-8 loss: 0.977089  [  288/  306]
train() client id: f_00004-15-0 loss: 0.924471  [   32/  306]
train() client id: f_00004-15-1 loss: 0.867819  [   64/  306]
train() client id: f_00004-15-2 loss: 0.889123  [   96/  306]
train() client id: f_00004-15-3 loss: 1.074451  [  128/  306]
train() client id: f_00004-15-4 loss: 0.849741  [  160/  306]
train() client id: f_00004-15-5 loss: 1.063869  [  192/  306]
train() client id: f_00004-15-6 loss: 0.903621  [  224/  306]
train() client id: f_00004-15-7 loss: 0.891272  [  256/  306]
train() client id: f_00004-15-8 loss: 0.900987  [  288/  306]
train() client id: f_00004-16-0 loss: 1.003592  [   32/  306]
train() client id: f_00004-16-1 loss: 0.854570  [   64/  306]
train() client id: f_00004-16-2 loss: 0.892531  [   96/  306]
train() client id: f_00004-16-3 loss: 1.049982  [  128/  306]
train() client id: f_00004-16-4 loss: 0.952648  [  160/  306]
train() client id: f_00004-16-5 loss: 0.979223  [  192/  306]
train() client id: f_00004-16-6 loss: 0.877846  [  224/  306]
train() client id: f_00004-16-7 loss: 0.819252  [  256/  306]
train() client id: f_00004-16-8 loss: 0.881789  [  288/  306]
train() client id: f_00005-0-0 loss: 0.171626  [   32/  146]
train() client id: f_00005-0-1 loss: 0.442735  [   64/  146]
train() client id: f_00005-0-2 loss: 0.714088  [   96/  146]
train() client id: f_00005-0-3 loss: 0.190908  [  128/  146]
train() client id: f_00005-1-0 loss: 0.278942  [   32/  146]
train() client id: f_00005-1-1 loss: 0.285695  [   64/  146]
train() client id: f_00005-1-2 loss: 0.337786  [   96/  146]
train() client id: f_00005-1-3 loss: 0.364504  [  128/  146]
train() client id: f_00005-2-0 loss: 0.324440  [   32/  146]
train() client id: f_00005-2-1 loss: 0.368307  [   64/  146]
train() client id: f_00005-2-2 loss: 0.258940  [   96/  146]
train() client id: f_00005-2-3 loss: 0.408313  [  128/  146]
train() client id: f_00005-3-0 loss: 0.382866  [   32/  146]
train() client id: f_00005-3-1 loss: 0.504459  [   64/  146]
train() client id: f_00005-3-2 loss: 0.425737  [   96/  146]
train() client id: f_00005-3-3 loss: 0.356482  [  128/  146]
train() client id: f_00005-4-0 loss: 0.328505  [   32/  146]
train() client id: f_00005-4-1 loss: 0.541805  [   64/  146]
train() client id: f_00005-4-2 loss: 0.193333  [   96/  146]
train() client id: f_00005-4-3 loss: 0.496109  [  128/  146]
train() client id: f_00005-5-0 loss: 0.559841  [   32/  146]
train() client id: f_00005-5-1 loss: 0.307689  [   64/  146]
train() client id: f_00005-5-2 loss: 0.376219  [   96/  146]
train() client id: f_00005-5-3 loss: 0.250349  [  128/  146]
train() client id: f_00005-6-0 loss: 0.322787  [   32/  146]
train() client id: f_00005-6-1 loss: 0.277387  [   64/  146]
train() client id: f_00005-6-2 loss: 0.323900  [   96/  146]
train() client id: f_00005-6-3 loss: 0.547264  [  128/  146]
train() client id: f_00005-7-0 loss: 0.427751  [   32/  146]
train() client id: f_00005-7-1 loss: 0.385654  [   64/  146]
train() client id: f_00005-7-2 loss: 0.228407  [   96/  146]
train() client id: f_00005-7-3 loss: 0.321176  [  128/  146]
train() client id: f_00005-8-0 loss: 0.178908  [   32/  146]
train() client id: f_00005-8-1 loss: 0.429429  [   64/  146]
train() client id: f_00005-8-2 loss: 0.352153  [   96/  146]
train() client id: f_00005-8-3 loss: 0.372621  [  128/  146]
train() client id: f_00005-9-0 loss: 0.411516  [   32/  146]
train() client id: f_00005-9-1 loss: 0.277658  [   64/  146]
train() client id: f_00005-9-2 loss: 0.457668  [   96/  146]
train() client id: f_00005-9-3 loss: 0.326985  [  128/  146]
train() client id: f_00005-10-0 loss: 0.428603  [   32/  146]
train() client id: f_00005-10-1 loss: 0.331749  [   64/  146]
train() client id: f_00005-10-2 loss: 0.293427  [   96/  146]
train() client id: f_00005-10-3 loss: 0.380160  [  128/  146]
train() client id: f_00005-11-0 loss: 0.258762  [   32/  146]
train() client id: f_00005-11-1 loss: 0.337542  [   64/  146]
train() client id: f_00005-11-2 loss: 0.713425  [   96/  146]
train() client id: f_00005-11-3 loss: 0.161200  [  128/  146]
train() client id: f_00005-12-0 loss: 0.183594  [   32/  146]
train() client id: f_00005-12-1 loss: 0.314590  [   64/  146]
train() client id: f_00005-12-2 loss: 0.366738  [   96/  146]
train() client id: f_00005-12-3 loss: 0.569559  [  128/  146]
train() client id: f_00005-13-0 loss: 0.297064  [   32/  146]
train() client id: f_00005-13-1 loss: 0.608619  [   64/  146]
train() client id: f_00005-13-2 loss: 0.148316  [   96/  146]
train() client id: f_00005-13-3 loss: 0.209090  [  128/  146]
train() client id: f_00005-14-0 loss: 0.210962  [   32/  146]
train() client id: f_00005-14-1 loss: 0.426940  [   64/  146]
train() client id: f_00005-14-2 loss: 0.470310  [   96/  146]
train() client id: f_00005-14-3 loss: 0.238365  [  128/  146]
train() client id: f_00005-15-0 loss: 0.260013  [   32/  146]
train() client id: f_00005-15-1 loss: 0.562386  [   64/  146]
train() client id: f_00005-15-2 loss: 0.434070  [   96/  146]
train() client id: f_00005-15-3 loss: 0.161621  [  128/  146]
train() client id: f_00005-16-0 loss: 0.230463  [   32/  146]
train() client id: f_00005-16-1 loss: 0.870050  [   64/  146]
train() client id: f_00005-16-2 loss: 0.153433  [   96/  146]
train() client id: f_00005-16-3 loss: 0.310416  [  128/  146]
train() client id: f_00006-0-0 loss: 0.540497  [   32/   54]
train() client id: f_00006-1-0 loss: 0.428766  [   32/   54]
train() client id: f_00006-2-0 loss: 0.525572  [   32/   54]
train() client id: f_00006-3-0 loss: 0.481409  [   32/   54]
train() client id: f_00006-4-0 loss: 0.459476  [   32/   54]
train() client id: f_00006-5-0 loss: 0.461614  [   32/   54]
train() client id: f_00006-6-0 loss: 0.501381  [   32/   54]
train() client id: f_00006-7-0 loss: 0.455795  [   32/   54]
train() client id: f_00006-8-0 loss: 0.436261  [   32/   54]
train() client id: f_00006-9-0 loss: 0.485897  [   32/   54]
train() client id: f_00006-10-0 loss: 0.415610  [   32/   54]
train() client id: f_00006-11-0 loss: 0.480743  [   32/   54]
train() client id: f_00006-12-0 loss: 0.540443  [   32/   54]
train() client id: f_00006-13-0 loss: 0.393083  [   32/   54]
train() client id: f_00006-14-0 loss: 0.529940  [   32/   54]
train() client id: f_00006-15-0 loss: 0.508089  [   32/   54]
train() client id: f_00006-16-0 loss: 0.540057  [   32/   54]
train() client id: f_00007-0-0 loss: 0.836931  [   32/  179]
train() client id: f_00007-0-1 loss: 0.665780  [   64/  179]
train() client id: f_00007-0-2 loss: 0.639580  [   96/  179]
train() client id: f_00007-0-3 loss: 0.528132  [  128/  179]
train() client id: f_00007-0-4 loss: 0.851293  [  160/  179]
train() client id: f_00007-1-0 loss: 0.510390  [   32/  179]
train() client id: f_00007-1-1 loss: 0.758925  [   64/  179]
train() client id: f_00007-1-2 loss: 0.624019  [   96/  179]
train() client id: f_00007-1-3 loss: 0.772051  [  128/  179]
train() client id: f_00007-1-4 loss: 0.846278  [  160/  179]
train() client id: f_00007-2-0 loss: 0.668688  [   32/  179]
train() client id: f_00007-2-1 loss: 0.893079  [   64/  179]
train() client id: f_00007-2-2 loss: 0.645886  [   96/  179]
train() client id: f_00007-2-3 loss: 0.727887  [  128/  179]
train() client id: f_00007-2-4 loss: 0.529606  [  160/  179]
train() client id: f_00007-3-0 loss: 0.595227  [   32/  179]
train() client id: f_00007-3-1 loss: 0.565244  [   64/  179]
train() client id: f_00007-3-2 loss: 0.701076  [   96/  179]
train() client id: f_00007-3-3 loss: 0.653009  [  128/  179]
train() client id: f_00007-3-4 loss: 0.852119  [  160/  179]
train() client id: f_00007-4-0 loss: 0.670170  [   32/  179]
train() client id: f_00007-4-1 loss: 0.553130  [   64/  179]
train() client id: f_00007-4-2 loss: 0.776055  [   96/  179]
train() client id: f_00007-4-3 loss: 0.792326  [  128/  179]
train() client id: f_00007-4-4 loss: 0.500258  [  160/  179]
train() client id: f_00007-5-0 loss: 0.711469  [   32/  179]
train() client id: f_00007-5-1 loss: 0.652783  [   64/  179]
train() client id: f_00007-5-2 loss: 0.679371  [   96/  179]
train() client id: f_00007-5-3 loss: 0.483634  [  128/  179]
train() client id: f_00007-5-4 loss: 0.753153  [  160/  179]
train() client id: f_00007-6-0 loss: 0.704285  [   32/  179]
train() client id: f_00007-6-1 loss: 0.667730  [   64/  179]
train() client id: f_00007-6-2 loss: 0.571156  [   96/  179]
train() client id: f_00007-6-3 loss: 0.844143  [  128/  179]
train() client id: f_00007-6-4 loss: 0.502249  [  160/  179]
train() client id: f_00007-7-0 loss: 0.586496  [   32/  179]
train() client id: f_00007-7-1 loss: 0.500446  [   64/  179]
train() client id: f_00007-7-2 loss: 0.654878  [   96/  179]
train() client id: f_00007-7-3 loss: 0.932893  [  128/  179]
train() client id: f_00007-7-4 loss: 0.482865  [  160/  179]
train() client id: f_00007-8-0 loss: 0.544621  [   32/  179]
train() client id: f_00007-8-1 loss: 0.679754  [   64/  179]
train() client id: f_00007-8-2 loss: 1.008094  [   96/  179]
train() client id: f_00007-8-3 loss: 0.537083  [  128/  179]
train() client id: f_00007-8-4 loss: 0.544421  [  160/  179]
train() client id: f_00007-9-0 loss: 0.586658  [   32/  179]
train() client id: f_00007-9-1 loss: 0.534467  [   64/  179]
train() client id: f_00007-9-2 loss: 0.599474  [   96/  179]
train() client id: f_00007-9-3 loss: 0.784152  [  128/  179]
train() client id: f_00007-9-4 loss: 0.667833  [  160/  179]
train() client id: f_00007-10-0 loss: 0.722137  [   32/  179]
train() client id: f_00007-10-1 loss: 0.593587  [   64/  179]
train() client id: f_00007-10-2 loss: 0.631305  [   96/  179]
train() client id: f_00007-10-3 loss: 0.457213  [  128/  179]
train() client id: f_00007-10-4 loss: 0.652035  [  160/  179]
train() client id: f_00007-11-0 loss: 0.630877  [   32/  179]
train() client id: f_00007-11-1 loss: 0.687218  [   64/  179]
train() client id: f_00007-11-2 loss: 0.458195  [   96/  179]
train() client id: f_00007-11-3 loss: 0.563322  [  128/  179]
train() client id: f_00007-11-4 loss: 0.646251  [  160/  179]
train() client id: f_00007-12-0 loss: 0.729052  [   32/  179]
train() client id: f_00007-12-1 loss: 0.481150  [   64/  179]
train() client id: f_00007-12-2 loss: 0.666396  [   96/  179]
train() client id: f_00007-12-3 loss: 0.692836  [  128/  179]
train() client id: f_00007-12-4 loss: 0.601468  [  160/  179]
train() client id: f_00007-13-0 loss: 0.452621  [   32/  179]
train() client id: f_00007-13-1 loss: 0.684446  [   64/  179]
train() client id: f_00007-13-2 loss: 0.629411  [   96/  179]
train() client id: f_00007-13-3 loss: 0.565713  [  128/  179]
train() client id: f_00007-13-4 loss: 0.465983  [  160/  179]
train() client id: f_00007-14-0 loss: 0.536006  [   32/  179]
train() client id: f_00007-14-1 loss: 0.573178  [   64/  179]
train() client id: f_00007-14-2 loss: 0.713963  [   96/  179]
train() client id: f_00007-14-3 loss: 0.769569  [  128/  179]
train() client id: f_00007-14-4 loss: 0.663163  [  160/  179]
train() client id: f_00007-15-0 loss: 0.559029  [   32/  179]
train() client id: f_00007-15-1 loss: 0.686108  [   64/  179]
train() client id: f_00007-15-2 loss: 0.653652  [   96/  179]
train() client id: f_00007-15-3 loss: 0.872455  [  128/  179]
train() client id: f_00007-15-4 loss: 0.459464  [  160/  179]
train() client id: f_00007-16-0 loss: 0.567590  [   32/  179]
train() client id: f_00007-16-1 loss: 0.639206  [   64/  179]
train() client id: f_00007-16-2 loss: 0.610574  [   96/  179]
train() client id: f_00007-16-3 loss: 0.581735  [  128/  179]
train() client id: f_00007-16-4 loss: 0.594739  [  160/  179]
train() client id: f_00008-0-0 loss: 0.626688  [   32/  130]
train() client id: f_00008-0-1 loss: 0.636611  [   64/  130]
train() client id: f_00008-0-2 loss: 0.693137  [   96/  130]
train() client id: f_00008-0-3 loss: 0.640713  [  128/  130]
train() client id: f_00008-1-0 loss: 0.602157  [   32/  130]
train() client id: f_00008-1-1 loss: 0.741342  [   64/  130]
train() client id: f_00008-1-2 loss: 0.665998  [   96/  130]
train() client id: f_00008-1-3 loss: 0.598714  [  128/  130]
train() client id: f_00008-2-0 loss: 0.665535  [   32/  130]
train() client id: f_00008-2-1 loss: 0.709312  [   64/  130]
train() client id: f_00008-2-2 loss: 0.582823  [   96/  130]
train() client id: f_00008-2-3 loss: 0.671133  [  128/  130]
train() client id: f_00008-3-0 loss: 0.724217  [   32/  130]
train() client id: f_00008-3-1 loss: 0.661280  [   64/  130]
train() client id: f_00008-3-2 loss: 0.622780  [   96/  130]
train() client id: f_00008-3-3 loss: 0.633074  [  128/  130]
train() client id: f_00008-4-0 loss: 0.679378  [   32/  130]
train() client id: f_00008-4-1 loss: 0.590547  [   64/  130]
train() client id: f_00008-4-2 loss: 0.599820  [   96/  130]
train() client id: f_00008-4-3 loss: 0.730307  [  128/  130]
train() client id: f_00008-5-0 loss: 0.629364  [   32/  130]
train() client id: f_00008-5-1 loss: 0.646248  [   64/  130]
train() client id: f_00008-5-2 loss: 0.631490  [   96/  130]
train() client id: f_00008-5-3 loss: 0.737119  [  128/  130]
train() client id: f_00008-6-0 loss: 0.653459  [   32/  130]
train() client id: f_00008-6-1 loss: 0.717199  [   64/  130]
train() client id: f_00008-6-2 loss: 0.663405  [   96/  130]
train() client id: f_00008-6-3 loss: 0.601233  [  128/  130]
train() client id: f_00008-7-0 loss: 0.688913  [   32/  130]
train() client id: f_00008-7-1 loss: 0.734289  [   64/  130]
train() client id: f_00008-7-2 loss: 0.590254  [   96/  130]
train() client id: f_00008-7-3 loss: 0.614729  [  128/  130]
train() client id: f_00008-8-0 loss: 0.705338  [   32/  130]
train() client id: f_00008-8-1 loss: 0.586921  [   64/  130]
train() client id: f_00008-8-2 loss: 0.728854  [   96/  130]
train() client id: f_00008-8-3 loss: 0.599898  [  128/  130]
train() client id: f_00008-9-0 loss: 0.767311  [   32/  130]
train() client id: f_00008-9-1 loss: 0.632375  [   64/  130]
train() client id: f_00008-9-2 loss: 0.591851  [   96/  130]
train() client id: f_00008-9-3 loss: 0.629110  [  128/  130]
train() client id: f_00008-10-0 loss: 0.610844  [   32/  130]
train() client id: f_00008-10-1 loss: 0.667347  [   64/  130]
train() client id: f_00008-10-2 loss: 0.692434  [   96/  130]
train() client id: f_00008-10-3 loss: 0.654779  [  128/  130]
train() client id: f_00008-11-0 loss: 0.708362  [   32/  130]
train() client id: f_00008-11-1 loss: 0.619015  [   64/  130]
train() client id: f_00008-11-2 loss: 0.580989  [   96/  130]
train() client id: f_00008-11-3 loss: 0.730811  [  128/  130]
train() client id: f_00008-12-0 loss: 0.713116  [   32/  130]
train() client id: f_00008-12-1 loss: 0.602897  [   64/  130]
train() client id: f_00008-12-2 loss: 0.677586  [   96/  130]
train() client id: f_00008-12-3 loss: 0.638762  [  128/  130]
train() client id: f_00008-13-0 loss: 0.539909  [   32/  130]
train() client id: f_00008-13-1 loss: 0.703564  [   64/  130]
train() client id: f_00008-13-2 loss: 0.754257  [   96/  130]
train() client id: f_00008-13-3 loss: 0.628094  [  128/  130]
train() client id: f_00008-14-0 loss: 0.594345  [   32/  130]
train() client id: f_00008-14-1 loss: 0.663356  [   64/  130]
train() client id: f_00008-14-2 loss: 0.717953  [   96/  130]
train() client id: f_00008-14-3 loss: 0.604863  [  128/  130]
train() client id: f_00008-15-0 loss: 0.652129  [   32/  130]
train() client id: f_00008-15-1 loss: 0.737132  [   64/  130]
train() client id: f_00008-15-2 loss: 0.619112  [   96/  130]
train() client id: f_00008-15-3 loss: 0.637643  [  128/  130]
train() client id: f_00008-16-0 loss: 0.636158  [   32/  130]
train() client id: f_00008-16-1 loss: 0.700403  [   64/  130]
train() client id: f_00008-16-2 loss: 0.704905  [   96/  130]
train() client id: f_00008-16-3 loss: 0.602313  [  128/  130]
train() client id: f_00009-0-0 loss: 0.936715  [   32/  118]
train() client id: f_00009-0-1 loss: 1.121006  [   64/  118]
train() client id: f_00009-0-2 loss: 1.018636  [   96/  118]
train() client id: f_00009-1-0 loss: 0.900697  [   32/  118]
train() client id: f_00009-1-1 loss: 0.927163  [   64/  118]
train() client id: f_00009-1-2 loss: 0.948941  [   96/  118]
train() client id: f_00009-2-0 loss: 0.788283  [   32/  118]
train() client id: f_00009-2-1 loss: 0.814246  [   64/  118]
train() client id: f_00009-2-2 loss: 1.000286  [   96/  118]
train() client id: f_00009-3-0 loss: 0.776881  [   32/  118]
train() client id: f_00009-3-1 loss: 0.852276  [   64/  118]
train() client id: f_00009-3-2 loss: 0.940016  [   96/  118]
train() client id: f_00009-4-0 loss: 0.762316  [   32/  118]
train() client id: f_00009-4-1 loss: 0.873367  [   64/  118]
train() client id: f_00009-4-2 loss: 0.776600  [   96/  118]
train() client id: f_00009-5-0 loss: 0.869443  [   32/  118]
train() client id: f_00009-5-1 loss: 0.718959  [   64/  118]
train() client id: f_00009-5-2 loss: 0.838261  [   96/  118]
train() client id: f_00009-6-0 loss: 0.685448  [   32/  118]
train() client id: f_00009-6-1 loss: 0.777792  [   64/  118]
train() client id: f_00009-6-2 loss: 0.875436  [   96/  118]
train() client id: f_00009-7-0 loss: 0.878486  [   32/  118]
train() client id: f_00009-7-1 loss: 0.561895  [   64/  118]
train() client id: f_00009-7-2 loss: 0.699057  [   96/  118]
train() client id: f_00009-8-0 loss: 0.642356  [   32/  118]
train() client id: f_00009-8-1 loss: 0.726296  [   64/  118]
train() client id: f_00009-8-2 loss: 0.699310  [   96/  118]
train() client id: f_00009-9-0 loss: 0.658274  [   32/  118]
train() client id: f_00009-9-1 loss: 0.745573  [   64/  118]
train() client id: f_00009-9-2 loss: 0.692141  [   96/  118]
train() client id: f_00009-10-0 loss: 0.693249  [   32/  118]
train() client id: f_00009-10-1 loss: 0.628855  [   64/  118]
train() client id: f_00009-10-2 loss: 0.687423  [   96/  118]
train() client id: f_00009-11-0 loss: 0.715369  [   32/  118]
train() client id: f_00009-11-1 loss: 0.843448  [   64/  118]
train() client id: f_00009-11-2 loss: 0.461035  [   96/  118]
train() client id: f_00009-12-0 loss: 0.502259  [   32/  118]
train() client id: f_00009-12-1 loss: 0.678581  [   64/  118]
train() client id: f_00009-12-2 loss: 0.883387  [   96/  118]
train() client id: f_00009-13-0 loss: 0.599428  [   32/  118]
train() client id: f_00009-13-1 loss: 0.791002  [   64/  118]
train() client id: f_00009-13-2 loss: 0.544639  [   96/  118]
train() client id: f_00009-14-0 loss: 0.606539  [   32/  118]
train() client id: f_00009-14-1 loss: 0.708634  [   64/  118]
train() client id: f_00009-14-2 loss: 0.685874  [   96/  118]
train() client id: f_00009-15-0 loss: 0.775363  [   32/  118]
train() client id: f_00009-15-1 loss: 0.528684  [   64/  118]
train() client id: f_00009-15-2 loss: 0.546533  [   96/  118]
train() client id: f_00009-16-0 loss: 0.862677  [   32/  118]
train() client id: f_00009-16-1 loss: 0.590866  [   64/  118]
train() client id: f_00009-16-2 loss: 0.391568  [   96/  118]
At round 51 accuracy: 0.6472148541114059
At round 51 training accuracy: 0.5888665325285044
At round 51 training loss: 0.8470558724217259
update_location
xs = 8.927491 376.223621 5.882650 0.934260 -292.581990 -140.230757 -100.849135 -5.143845 -315.120581 20.134486 
ys = -367.390647 7.291448 265.684448 -87.290817 -9.642386 0.794442 -1.381692 261.628436 25.881276 -802.232496 
xs mean: -44.182379970521225
ys mean: -70.66579882624052
dists_uav = 380.861638 389.355080 283.941599 132.742456 309.349634 172.236165 142.029775 280.135499 331.618487 808.691768 
uav_gains = -120.651459 -120.997867 -114.335282 -103.076120 -116.486245 -105.929868 -103.811681 -113.992254 -118.077615 -129.662617 
uav_gains_db_mean: -114.70210074412574
dists_bs = 572.727628 576.171536 202.341795 315.831184 218.902570 177.641427 191.334399 190.671444 204.622020 996.524369 
bs_gains = -116.789607 -116.862510 -104.137456 -109.551799 -105.094083 -102.554298 -103.457265 -103.415057 -104.273725 -123.524721 
bs_gains_db_mean: -108.9660521838565
Round 52
-------------------------------
ene_coms = [0.03033526 0.03182569 0.00760494 0.00722297 0.00798462 0.0083478
 0.00748356 0.0073408  0.00765686 0.04662661]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.29832862 8.70520202 3.99704555 1.43410807 9.75073164 4.69984674
 1.78434761 5.73182813 4.18714025 4.21624552]
obj_prev = 48.804824142729814
eta_min = 6.744110494936172e-23	eta_max = 0.8217781400998344
af = 10.126086272682958	bf = 1.6844252975996807	zeta = 11.138694899951254	eta = 0.9090909090909091
af = 10.126086272682958	bf = 1.6844252975996807	zeta = 29.144988410595193	eta = 0.34743833588219175
af = 10.126086272682958	bf = 1.6844252975996807	zeta = 18.98579270884875	eta = 0.5333507232470452
af = 10.126086272682958	bf = 1.6844252975996807	zeta = 17.24944417356952	eta = 0.5870384095157493
af = 10.126086272682958	bf = 1.6844252975996807	zeta = 17.14018321729607	eta = 0.5907805152552148
af = 10.126086272682958	bf = 1.6844252975996807	zeta = 17.139684707271215	eta = 0.5907976981856113
af = 10.126086272682958	bf = 1.6844252975996807	zeta = 17.13968469680478	eta = 0.590797698546385
eta = 0.590797698546385
ene_coms = [0.03033526 0.03182569 0.00760494 0.00722297 0.00798462 0.0083478
 0.00748356 0.0073408  0.00765686 0.04662661]
ene_comp = [0.0434275  0.09133561 0.04273817 0.0148205  0.10546678 0.05032075
 0.01861178 0.06169462 0.04480615 0.0406702 ]
ene_total = [1.86933448 3.12121819 1.2758214  0.55863697 2.8751448  1.48680903
 0.6613218  1.74953175 1.32954491 2.21232136]
ti_comp = [0.77591407 0.76100977 1.00321734 1.00703702 0.99942048 0.99578874
 1.00443113 1.00585869 1.00269814 0.61300059]
ti_coms = [0.30335263 0.31825692 0.07604936 0.07222968 0.07984621 0.08347796
 0.07483557 0.073408   0.07656856 0.46626611]
t_total = [27.35134125 27.35134125 27.35134125 27.35134125 27.35134125 27.35134125
 27.35134125 27.35134125 27.35134125 27.35134125]
ene_coms = [0.03033526 0.03182569 0.00760494 0.00722297 0.00798462 0.0083478
 0.00748356 0.0073408  0.00765686 0.04662661]
ene_comp = [8.50251087e-06 8.22280454e-05 4.84772319e-06 2.00621424e-07
 7.34058384e-05 8.03132057e-06 3.99396051e-07 1.45060096e-05
 5.59181063e-06 1.11888912e-05]
ene_total = [0.76898757 0.80862721 0.19285112 0.18305331 0.20421074 0.21175774
 0.18966233 0.18640202 0.19418576 1.18191952]
optimize_network iter = 0 obj = 4.121657314231502
eta = 0.590797698546385
freqs = [27984734.32660743 60009486.35035945 21300555.36396774  7358466.03754416
 52763968.3757668  25266780.96354323  9264838.77627468 30667637.97865387
 22342790.55133308 33173048.6406446 ]
eta_min = 0.5907976985463856	eta_max = 0.590797698546384
af = 0.004116353467282495	bf = 1.6844252975996807	zeta = 0.004527988814010745	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03033526 0.03182569 0.00760494 0.00722297 0.00798462 0.0083478
 0.00748356 0.0073408  0.00765686 0.04662661]
ene_comp = [1.53981405e-06 1.48915893e-05 8.77927991e-07 3.63327602e-08
 1.32938779e-05 1.45448097e-06 7.23310633e-08 2.62705427e-06
 1.01268304e-06 2.02632049e-06]
ene_total = [3.46059207 3.63213203 0.86761349 0.82394544 0.91234145 0.95241907
 0.85367557 0.83768244 0.87355149 5.31904085]
ti_comp = [0.77591407 0.76100977 1.00321734 1.00703702 0.99942048 0.99578874
 1.00443113 1.00585869 1.00269814 0.61300059]
ti_coms = [0.30335263 0.31825692 0.07604936 0.07222968 0.07984621 0.08347796
 0.07483557 0.073408   0.07656856 0.46626611]
t_total = [27.35134125 27.35134125 27.35134125 27.35134125 27.35134125 27.35134125
 27.35134125 27.35134125 27.35134125 27.35134125]
ene_coms = [0.03033526 0.03182569 0.00760494 0.00722297 0.00798462 0.0083478
 0.00748356 0.0073408  0.00765686 0.04662661]
ene_comp = [8.50251087e-06 8.22280454e-05 4.84772319e-06 2.00621424e-07
 7.34058384e-05 8.03132057e-06 3.99396051e-07 1.45060096e-05
 5.59181063e-06 1.11888912e-05]
ene_total = [0.76898757 0.80862721 0.19285112 0.18305331 0.20421074 0.21175774
 0.18966233 0.18640202 0.19418576 1.18191952]
optimize_network iter = 1 obj = 4.121657314231492
eta = 0.590797698546384
freqs = [27984734.32660743 60009486.35035944 21300555.36396775  7358466.03754417
 52763968.37576684 25266780.96354325  9264838.77627468 30667637.97865389
 22342790.5513331  33173048.64064456]
Done!
ene_coms = [0.03033526 0.03182569 0.00760494 0.00722297 0.00798462 0.0083478
 0.00748356 0.0073408  0.00765686 0.04662661]
ene_comp = [8.38748676e-06 8.11156436e-05 4.78214196e-06 1.97907367e-07
 7.24127857e-05 7.92267083e-06 3.93992920e-07 1.43097687e-05
 5.51616320e-06 1.10375251e-05]
ene_total = [0.03034365 0.03190681 0.00760972 0.00722317 0.00805703 0.00835572
 0.00748395 0.00735511 0.00766237 0.04663765]
At round 52 energy consumption: 0.16263517501216887
At round 52 eta: 0.590797698546384
At round 52 a_n: 10.370218813849878
At round 52 local rounds: 17.233134195147468
At round 52 global rounds: 25.342523189658465
gradient difference: 0.2983430325984955
train() client id: f_00000-0-0 loss: 1.380100  [   32/  126]
train() client id: f_00000-0-1 loss: 1.145648  [   64/  126]
train() client id: f_00000-0-2 loss: 1.321717  [   96/  126]
train() client id: f_00000-1-0 loss: 1.429035  [   32/  126]
train() client id: f_00000-1-1 loss: 1.119847  [   64/  126]
train() client id: f_00000-1-2 loss: 1.072583  [   96/  126]
train() client id: f_00000-2-0 loss: 1.019655  [   32/  126]
train() client id: f_00000-2-1 loss: 1.128623  [   64/  126]
train() client id: f_00000-2-2 loss: 1.003785  [   96/  126]
train() client id: f_00000-3-0 loss: 0.903419  [   32/  126]
train() client id: f_00000-3-1 loss: 1.171730  [   64/  126]
train() client id: f_00000-3-2 loss: 1.069365  [   96/  126]
train() client id: f_00000-4-0 loss: 0.906172  [   32/  126]
train() client id: f_00000-4-1 loss: 1.078514  [   64/  126]
train() client id: f_00000-4-2 loss: 0.810873  [   96/  126]
train() client id: f_00000-5-0 loss: 0.935680  [   32/  126]
train() client id: f_00000-5-1 loss: 0.955157  [   64/  126]
train() client id: f_00000-5-2 loss: 0.719212  [   96/  126]
train() client id: f_00000-6-0 loss: 0.887383  [   32/  126]
train() client id: f_00000-6-1 loss: 0.931776  [   64/  126]
train() client id: f_00000-6-2 loss: 0.868121  [   96/  126]
train() client id: f_00000-7-0 loss: 0.836267  [   32/  126]
train() client id: f_00000-7-1 loss: 0.807883  [   64/  126]
train() client id: f_00000-7-2 loss: 0.933430  [   96/  126]
train() client id: f_00000-8-0 loss: 0.872241  [   32/  126]
train() client id: f_00000-8-1 loss: 0.831276  [   64/  126]
train() client id: f_00000-8-2 loss: 0.834397  [   96/  126]
train() client id: f_00000-9-0 loss: 0.775418  [   32/  126]
train() client id: f_00000-9-1 loss: 0.798304  [   64/  126]
train() client id: f_00000-9-2 loss: 0.902610  [   96/  126]
train() client id: f_00000-10-0 loss: 0.884075  [   32/  126]
train() client id: f_00000-10-1 loss: 0.730952  [   64/  126]
train() client id: f_00000-10-2 loss: 0.907894  [   96/  126]
train() client id: f_00000-11-0 loss: 0.857538  [   32/  126]
train() client id: f_00000-11-1 loss: 0.808743  [   64/  126]
train() client id: f_00000-11-2 loss: 0.807283  [   96/  126]
train() client id: f_00000-12-0 loss: 0.742681  [   32/  126]
train() client id: f_00000-12-1 loss: 0.877050  [   64/  126]
train() client id: f_00000-12-2 loss: 0.775707  [   96/  126]
train() client id: f_00000-13-0 loss: 0.828213  [   32/  126]
train() client id: f_00000-13-1 loss: 0.803587  [   64/  126]
train() client id: f_00000-13-2 loss: 0.811756  [   96/  126]
train() client id: f_00000-14-0 loss: 0.796115  [   32/  126]
train() client id: f_00000-14-1 loss: 0.801568  [   64/  126]
train() client id: f_00000-14-2 loss: 0.708940  [   96/  126]
train() client id: f_00000-15-0 loss: 0.706479  [   32/  126]
train() client id: f_00000-15-1 loss: 0.782094  [   64/  126]
train() client id: f_00000-15-2 loss: 0.754201  [   96/  126]
train() client id: f_00000-16-0 loss: 0.809564  [   32/  126]
train() client id: f_00000-16-1 loss: 0.701115  [   64/  126]
train() client id: f_00000-16-2 loss: 0.853979  [   96/  126]
train() client id: f_00001-0-0 loss: 0.348664  [   32/  265]
train() client id: f_00001-0-1 loss: 0.361316  [   64/  265]
train() client id: f_00001-0-2 loss: 0.399327  [   96/  265]
train() client id: f_00001-0-3 loss: 0.446031  [  128/  265]
train() client id: f_00001-0-4 loss: 0.657732  [  160/  265]
train() client id: f_00001-0-5 loss: 0.373536  [  192/  265]
train() client id: f_00001-0-6 loss: 0.390176  [  224/  265]
train() client id: f_00001-0-7 loss: 0.399083  [  256/  265]
train() client id: f_00001-1-0 loss: 0.521953  [   32/  265]
train() client id: f_00001-1-1 loss: 0.485484  [   64/  265]
train() client id: f_00001-1-2 loss: 0.463433  [   96/  265]
train() client id: f_00001-1-3 loss: 0.301957  [  128/  265]
train() client id: f_00001-1-4 loss: 0.390022  [  160/  265]
train() client id: f_00001-1-5 loss: 0.438818  [  192/  265]
train() client id: f_00001-1-6 loss: 0.463606  [  224/  265]
train() client id: f_00001-1-7 loss: 0.344611  [  256/  265]
train() client id: f_00001-2-0 loss: 0.416070  [   32/  265]
train() client id: f_00001-2-1 loss: 0.444012  [   64/  265]
train() client id: f_00001-2-2 loss: 0.303715  [   96/  265]
train() client id: f_00001-2-3 loss: 0.474126  [  128/  265]
train() client id: f_00001-2-4 loss: 0.318231  [  160/  265]
train() client id: f_00001-2-5 loss: 0.378462  [  192/  265]
train() client id: f_00001-2-6 loss: 0.441520  [  224/  265]
train() client id: f_00001-2-7 loss: 0.550199  [  256/  265]
train() client id: f_00001-3-0 loss: 0.341236  [   32/  265]
train() client id: f_00001-3-1 loss: 0.348972  [   64/  265]
train() client id: f_00001-3-2 loss: 0.375825  [   96/  265]
train() client id: f_00001-3-3 loss: 0.397385  [  128/  265]
train() client id: f_00001-3-4 loss: 0.482890  [  160/  265]
train() client id: f_00001-3-5 loss: 0.454922  [  192/  265]
train() client id: f_00001-3-6 loss: 0.395764  [  224/  265]
train() client id: f_00001-3-7 loss: 0.498575  [  256/  265]
train() client id: f_00001-4-0 loss: 0.486994  [   32/  265]
train() client id: f_00001-4-1 loss: 0.328655  [   64/  265]
train() client id: f_00001-4-2 loss: 0.415282  [   96/  265]
train() client id: f_00001-4-3 loss: 0.344119  [  128/  265]
train() client id: f_00001-4-4 loss: 0.499392  [  160/  265]
train() client id: f_00001-4-5 loss: 0.327670  [  192/  265]
train() client id: f_00001-4-6 loss: 0.396112  [  224/  265]
train() client id: f_00001-4-7 loss: 0.442864  [  256/  265]
train() client id: f_00001-5-0 loss: 0.411006  [   32/  265]
train() client id: f_00001-5-1 loss: 0.331294  [   64/  265]
train() client id: f_00001-5-2 loss: 0.375957  [   96/  265]
train() client id: f_00001-5-3 loss: 0.486669  [  128/  265]
train() client id: f_00001-5-4 loss: 0.369942  [  160/  265]
train() client id: f_00001-5-5 loss: 0.487805  [  192/  265]
train() client id: f_00001-5-6 loss: 0.429471  [  224/  265]
train() client id: f_00001-5-7 loss: 0.350695  [  256/  265]
train() client id: f_00001-6-0 loss: 0.467649  [   32/  265]
train() client id: f_00001-6-1 loss: 0.538234  [   64/  265]
train() client id: f_00001-6-2 loss: 0.300522  [   96/  265]
train() client id: f_00001-6-3 loss: 0.326266  [  128/  265]
train() client id: f_00001-6-4 loss: 0.350906  [  160/  265]
train() client id: f_00001-6-5 loss: 0.407261  [  192/  265]
train() client id: f_00001-6-6 loss: 0.387006  [  224/  265]
train() client id: f_00001-6-7 loss: 0.439232  [  256/  265]
train() client id: f_00001-7-0 loss: 0.346315  [   32/  265]
train() client id: f_00001-7-1 loss: 0.308317  [   64/  265]
train() client id: f_00001-7-2 loss: 0.449479  [   96/  265]
train() client id: f_00001-7-3 loss: 0.445120  [  128/  265]
train() client id: f_00001-7-4 loss: 0.481391  [  160/  265]
train() client id: f_00001-7-5 loss: 0.341940  [  192/  265]
train() client id: f_00001-7-6 loss: 0.397796  [  224/  265]
train() client id: f_00001-7-7 loss: 0.401224  [  256/  265]
train() client id: f_00001-8-0 loss: 0.296016  [   32/  265]
train() client id: f_00001-8-1 loss: 0.359229  [   64/  265]
train() client id: f_00001-8-2 loss: 0.353648  [   96/  265]
train() client id: f_00001-8-3 loss: 0.518942  [  128/  265]
train() client id: f_00001-8-4 loss: 0.436243  [  160/  265]
train() client id: f_00001-8-5 loss: 0.407579  [  192/  265]
train() client id: f_00001-8-6 loss: 0.449175  [  224/  265]
train() client id: f_00001-8-7 loss: 0.380009  [  256/  265]
train() client id: f_00001-9-0 loss: 0.294488  [   32/  265]
train() client id: f_00001-9-1 loss: 0.341065  [   64/  265]
train() client id: f_00001-9-2 loss: 0.395544  [   96/  265]
train() client id: f_00001-9-3 loss: 0.411554  [  128/  265]
train() client id: f_00001-9-4 loss: 0.462664  [  160/  265]
train() client id: f_00001-9-5 loss: 0.555056  [  192/  265]
train() client id: f_00001-9-6 loss: 0.289324  [  224/  265]
train() client id: f_00001-9-7 loss: 0.363223  [  256/  265]
train() client id: f_00001-10-0 loss: 0.370974  [   32/  265]
train() client id: f_00001-10-1 loss: 0.357868  [   64/  265]
train() client id: f_00001-10-2 loss: 0.395655  [   96/  265]
train() client id: f_00001-10-3 loss: 0.370651  [  128/  265]
train() client id: f_00001-10-4 loss: 0.295524  [  160/  265]
train() client id: f_00001-10-5 loss: 0.466210  [  192/  265]
train() client id: f_00001-10-6 loss: 0.411498  [  224/  265]
train() client id: f_00001-10-7 loss: 0.395709  [  256/  265]
train() client id: f_00001-11-0 loss: 0.319987  [   32/  265]
train() client id: f_00001-11-1 loss: 0.418683  [   64/  265]
train() client id: f_00001-11-2 loss: 0.399027  [   96/  265]
train() client id: f_00001-11-3 loss: 0.345812  [  128/  265]
train() client id: f_00001-11-4 loss: 0.374885  [  160/  265]
train() client id: f_00001-11-5 loss: 0.434909  [  192/  265]
train() client id: f_00001-11-6 loss: 0.458842  [  224/  265]
train() client id: f_00001-11-7 loss: 0.368016  [  256/  265]
train() client id: f_00001-12-0 loss: 0.298606  [   32/  265]
train() client id: f_00001-12-1 loss: 0.321744  [   64/  265]
train() client id: f_00001-12-2 loss: 0.492815  [   96/  265]
train() client id: f_00001-12-3 loss: 0.340935  [  128/  265]
train() client id: f_00001-12-4 loss: 0.336345  [  160/  265]
train() client id: f_00001-12-5 loss: 0.420852  [  192/  265]
train() client id: f_00001-12-6 loss: 0.544644  [  224/  265]
train() client id: f_00001-12-7 loss: 0.427162  [  256/  265]
train() client id: f_00001-13-0 loss: 0.290647  [   32/  265]
train() client id: f_00001-13-1 loss: 0.344679  [   64/  265]
train() client id: f_00001-13-2 loss: 0.476846  [   96/  265]
train() client id: f_00001-13-3 loss: 0.471251  [  128/  265]
train() client id: f_00001-13-4 loss: 0.292995  [  160/  265]
train() client id: f_00001-13-5 loss: 0.374319  [  192/  265]
train() client id: f_00001-13-6 loss: 0.478095  [  224/  265]
train() client id: f_00001-13-7 loss: 0.456117  [  256/  265]
train() client id: f_00001-14-0 loss: 0.359996  [   32/  265]
train() client id: f_00001-14-1 loss: 0.398014  [   64/  265]
train() client id: f_00001-14-2 loss: 0.571338  [   96/  265]
train() client id: f_00001-14-3 loss: 0.425072  [  128/  265]
train() client id: f_00001-14-4 loss: 0.290160  [  160/  265]
train() client id: f_00001-14-5 loss: 0.376745  [  192/  265]
train() client id: f_00001-14-6 loss: 0.283439  [  224/  265]
train() client id: f_00001-14-7 loss: 0.482036  [  256/  265]
train() client id: f_00001-15-0 loss: 0.288869  [   32/  265]
train() client id: f_00001-15-1 loss: 0.295884  [   64/  265]
train() client id: f_00001-15-2 loss: 0.320919  [   96/  265]
train() client id: f_00001-15-3 loss: 0.385033  [  128/  265]
train() client id: f_00001-15-4 loss: 0.456803  [  160/  265]
train() client id: f_00001-15-5 loss: 0.513227  [  192/  265]
train() client id: f_00001-15-6 loss: 0.290961  [  224/  265]
train() client id: f_00001-15-7 loss: 0.618729  [  256/  265]
train() client id: f_00001-16-0 loss: 0.423539  [   32/  265]
train() client id: f_00001-16-1 loss: 0.380240  [   64/  265]
train() client id: f_00001-16-2 loss: 0.303375  [   96/  265]
train() client id: f_00001-16-3 loss: 0.475178  [  128/  265]
train() client id: f_00001-16-4 loss: 0.413019  [  160/  265]
train() client id: f_00001-16-5 loss: 0.435109  [  192/  265]
train() client id: f_00001-16-6 loss: 0.304556  [  224/  265]
train() client id: f_00001-16-7 loss: 0.397301  [  256/  265]
train() client id: f_00002-0-0 loss: 1.114896  [   32/  124]
train() client id: f_00002-0-1 loss: 1.238046  [   64/  124]
train() client id: f_00002-0-2 loss: 1.305486  [   96/  124]
train() client id: f_00002-1-0 loss: 1.260104  [   32/  124]
train() client id: f_00002-1-1 loss: 1.013766  [   64/  124]
train() client id: f_00002-1-2 loss: 1.069594  [   96/  124]
train() client id: f_00002-2-0 loss: 1.036544  [   32/  124]
train() client id: f_00002-2-1 loss: 1.115238  [   64/  124]
train() client id: f_00002-2-2 loss: 0.948854  [   96/  124]
train() client id: f_00002-3-0 loss: 0.951747  [   32/  124]
train() client id: f_00002-3-1 loss: 1.117663  [   64/  124]
train() client id: f_00002-3-2 loss: 0.966287  [   96/  124]
train() client id: f_00002-4-0 loss: 1.070911  [   32/  124]
train() client id: f_00002-4-1 loss: 1.243404  [   64/  124]
train() client id: f_00002-4-2 loss: 0.805113  [   96/  124]
train() client id: f_00002-5-0 loss: 0.945979  [   32/  124]
train() client id: f_00002-5-1 loss: 0.820562  [   64/  124]
train() client id: f_00002-5-2 loss: 0.980228  [   96/  124]
train() client id: f_00002-6-0 loss: 0.934897  [   32/  124]
train() client id: f_00002-6-1 loss: 0.923569  [   64/  124]
train() client id: f_00002-6-2 loss: 0.940494  [   96/  124]
train() client id: f_00002-7-0 loss: 1.062171  [   32/  124]
train() client id: f_00002-7-1 loss: 0.987664  [   64/  124]
train() client id: f_00002-7-2 loss: 0.813997  [   96/  124]
train() client id: f_00002-8-0 loss: 0.865688  [   32/  124]
train() client id: f_00002-8-1 loss: 1.036686  [   64/  124]
train() client id: f_00002-8-2 loss: 0.859807  [   96/  124]
train() client id: f_00002-9-0 loss: 0.955899  [   32/  124]
train() client id: f_00002-9-1 loss: 0.955266  [   64/  124]
train() client id: f_00002-9-2 loss: 0.803210  [   96/  124]
train() client id: f_00002-10-0 loss: 0.884653  [   32/  124]
train() client id: f_00002-10-1 loss: 0.977715  [   64/  124]
train() client id: f_00002-10-2 loss: 0.995726  [   96/  124]
train() client id: f_00002-11-0 loss: 0.854698  [   32/  124]
train() client id: f_00002-11-1 loss: 0.969529  [   64/  124]
train() client id: f_00002-11-2 loss: 0.754249  [   96/  124]
train() client id: f_00002-12-0 loss: 0.831213  [   32/  124]
train() client id: f_00002-12-1 loss: 0.742309  [   64/  124]
train() client id: f_00002-12-2 loss: 0.951849  [   96/  124]
train() client id: f_00002-13-0 loss: 0.863989  [   32/  124]
train() client id: f_00002-13-1 loss: 1.083726  [   64/  124]
train() client id: f_00002-13-2 loss: 0.800813  [   96/  124]
train() client id: f_00002-14-0 loss: 0.717019  [   32/  124]
train() client id: f_00002-14-1 loss: 0.736529  [   64/  124]
train() client id: f_00002-14-2 loss: 0.768070  [   96/  124]
train() client id: f_00002-15-0 loss: 0.766162  [   32/  124]
train() client id: f_00002-15-1 loss: 0.763685  [   64/  124]
train() client id: f_00002-15-2 loss: 0.949738  [   96/  124]
train() client id: f_00002-16-0 loss: 0.955842  [   32/  124]
train() client id: f_00002-16-1 loss: 0.704104  [   64/  124]
train() client id: f_00002-16-2 loss: 0.921931  [   96/  124]
train() client id: f_00003-0-0 loss: 0.774167  [   32/   43]
train() client id: f_00003-1-0 loss: 0.929968  [   32/   43]
train() client id: f_00003-2-0 loss: 0.738569  [   32/   43]
train() client id: f_00003-3-0 loss: 0.883943  [   32/   43]
train() client id: f_00003-4-0 loss: 0.863502  [   32/   43]
train() client id: f_00003-5-0 loss: 0.827282  [   32/   43]
train() client id: f_00003-6-0 loss: 0.783196  [   32/   43]
train() client id: f_00003-7-0 loss: 0.683762  [   32/   43]
train() client id: f_00003-8-0 loss: 0.572552  [   32/   43]
train() client id: f_00003-9-0 loss: 0.728493  [   32/   43]
train() client id: f_00003-10-0 loss: 0.772741  [   32/   43]
train() client id: f_00003-11-0 loss: 0.927611  [   32/   43]
train() client id: f_00003-12-0 loss: 0.472061  [   32/   43]
train() client id: f_00003-13-0 loss: 0.494820  [   32/   43]
train() client id: f_00003-14-0 loss: 0.705063  [   32/   43]
train() client id: f_00003-15-0 loss: 0.772951  [   32/   43]
train() client id: f_00003-16-0 loss: 0.580526  [   32/   43]
train() client id: f_00004-0-0 loss: 1.052598  [   32/  306]
train() client id: f_00004-0-1 loss: 0.850643  [   64/  306]
train() client id: f_00004-0-2 loss: 0.753005  [   96/  306]
train() client id: f_00004-0-3 loss: 0.825924  [  128/  306]
train() client id: f_00004-0-4 loss: 0.936399  [  160/  306]
train() client id: f_00004-0-5 loss: 0.765816  [  192/  306]
train() client id: f_00004-0-6 loss: 0.959551  [  224/  306]
train() client id: f_00004-0-7 loss: 0.914591  [  256/  306]
train() client id: f_00004-0-8 loss: 1.008832  [  288/  306]
train() client id: f_00004-1-0 loss: 0.983749  [   32/  306]
train() client id: f_00004-1-1 loss: 0.823078  [   64/  306]
train() client id: f_00004-1-2 loss: 0.863763  [   96/  306]
train() client id: f_00004-1-3 loss: 0.939673  [  128/  306]
train() client id: f_00004-1-4 loss: 0.835038  [  160/  306]
train() client id: f_00004-1-5 loss: 0.948247  [  192/  306]
train() client id: f_00004-1-6 loss: 0.863321  [  224/  306]
train() client id: f_00004-1-7 loss: 0.884197  [  256/  306]
train() client id: f_00004-1-8 loss: 0.753938  [  288/  306]
train() client id: f_00004-2-0 loss: 0.844016  [   32/  306]
train() client id: f_00004-2-1 loss: 0.900414  [   64/  306]
train() client id: f_00004-2-2 loss: 0.872280  [   96/  306]
train() client id: f_00004-2-3 loss: 0.884273  [  128/  306]
train() client id: f_00004-2-4 loss: 0.845334  [  160/  306]
train() client id: f_00004-2-5 loss: 0.857144  [  192/  306]
train() client id: f_00004-2-6 loss: 0.928166  [  224/  306]
train() client id: f_00004-2-7 loss: 0.878955  [  256/  306]
train() client id: f_00004-2-8 loss: 0.800650  [  288/  306]
train() client id: f_00004-3-0 loss: 0.861362  [   32/  306]
train() client id: f_00004-3-1 loss: 0.885217  [   64/  306]
train() client id: f_00004-3-2 loss: 1.058454  [   96/  306]
train() client id: f_00004-3-3 loss: 1.016888  [  128/  306]
train() client id: f_00004-3-4 loss: 0.868919  [  160/  306]
train() client id: f_00004-3-5 loss: 0.744631  [  192/  306]
train() client id: f_00004-3-6 loss: 0.799376  [  224/  306]
train() client id: f_00004-3-7 loss: 0.864795  [  256/  306]
train() client id: f_00004-3-8 loss: 0.754920  [  288/  306]
train() client id: f_00004-4-0 loss: 0.923347  [   32/  306]
train() client id: f_00004-4-1 loss: 0.882519  [   64/  306]
train() client id: f_00004-4-2 loss: 0.915441  [   96/  306]
train() client id: f_00004-4-3 loss: 0.794483  [  128/  306]
train() client id: f_00004-4-4 loss: 0.941510  [  160/  306]
train() client id: f_00004-4-5 loss: 0.883222  [  192/  306]
train() client id: f_00004-4-6 loss: 0.854244  [  224/  306]
train() client id: f_00004-4-7 loss: 0.730401  [  256/  306]
train() client id: f_00004-4-8 loss: 0.827714  [  288/  306]
train() client id: f_00004-5-0 loss: 0.876467  [   32/  306]
train() client id: f_00004-5-1 loss: 0.836616  [   64/  306]
train() client id: f_00004-5-2 loss: 0.806338  [   96/  306]
train() client id: f_00004-5-3 loss: 0.996699  [  128/  306]
train() client id: f_00004-5-4 loss: 0.780600  [  160/  306]
train() client id: f_00004-5-5 loss: 0.925061  [  192/  306]
train() client id: f_00004-5-6 loss: 0.789300  [  224/  306]
train() client id: f_00004-5-7 loss: 0.834508  [  256/  306]
train() client id: f_00004-5-8 loss: 0.845788  [  288/  306]
train() client id: f_00004-6-0 loss: 0.881176  [   32/  306]
train() client id: f_00004-6-1 loss: 1.001755  [   64/  306]
train() client id: f_00004-6-2 loss: 0.783967  [   96/  306]
train() client id: f_00004-6-3 loss: 0.765638  [  128/  306]
train() client id: f_00004-6-4 loss: 0.742470  [  160/  306]
train() client id: f_00004-6-5 loss: 1.015717  [  192/  306]
train() client id: f_00004-6-6 loss: 0.824104  [  224/  306]
train() client id: f_00004-6-7 loss: 0.825025  [  256/  306]
train() client id: f_00004-6-8 loss: 0.899652  [  288/  306]
train() client id: f_00004-7-0 loss: 0.825865  [   32/  306]
train() client id: f_00004-7-1 loss: 0.899889  [   64/  306]
train() client id: f_00004-7-2 loss: 0.877438  [   96/  306]
train() client id: f_00004-7-3 loss: 0.890052  [  128/  306]
train() client id: f_00004-7-4 loss: 0.758399  [  160/  306]
train() client id: f_00004-7-5 loss: 0.940024  [  192/  306]
train() client id: f_00004-7-6 loss: 0.886609  [  224/  306]
train() client id: f_00004-7-7 loss: 0.938280  [  256/  306]
train() client id: f_00004-7-8 loss: 0.729576  [  288/  306]
train() client id: f_00004-8-0 loss: 0.931126  [   32/  306]
train() client id: f_00004-8-1 loss: 0.875732  [   64/  306]
train() client id: f_00004-8-2 loss: 0.795713  [   96/  306]
train() client id: f_00004-8-3 loss: 0.922817  [  128/  306]
train() client id: f_00004-8-4 loss: 0.854085  [  160/  306]
train() client id: f_00004-8-5 loss: 0.859240  [  192/  306]
train() client id: f_00004-8-6 loss: 0.831796  [  224/  306]
train() client id: f_00004-8-7 loss: 0.664122  [  256/  306]
train() client id: f_00004-8-8 loss: 0.989784  [  288/  306]
train() client id: f_00004-9-0 loss: 0.812997  [   32/  306]
train() client id: f_00004-9-1 loss: 0.685378  [   64/  306]
train() client id: f_00004-9-2 loss: 0.962903  [   96/  306]
train() client id: f_00004-9-3 loss: 0.813312  [  128/  306]
train() client id: f_00004-9-4 loss: 0.920361  [  160/  306]
train() client id: f_00004-9-5 loss: 0.902917  [  192/  306]
train() client id: f_00004-9-6 loss: 0.863708  [  224/  306]
train() client id: f_00004-9-7 loss: 0.913803  [  256/  306]
train() client id: f_00004-9-8 loss: 0.798859  [  288/  306]
train() client id: f_00004-10-0 loss: 0.838353  [   32/  306]
train() client id: f_00004-10-1 loss: 0.858759  [   64/  306]
train() client id: f_00004-10-2 loss: 0.962181  [   96/  306]
train() client id: f_00004-10-3 loss: 0.827446  [  128/  306]
train() client id: f_00004-10-4 loss: 0.741431  [  160/  306]
train() client id: f_00004-10-5 loss: 0.866928  [  192/  306]
train() client id: f_00004-10-6 loss: 0.856050  [  224/  306]
train() client id: f_00004-10-7 loss: 0.762020  [  256/  306]
train() client id: f_00004-10-8 loss: 0.962748  [  288/  306]
train() client id: f_00004-11-0 loss: 0.984165  [   32/  306]
train() client id: f_00004-11-1 loss: 0.771992  [   64/  306]
train() client id: f_00004-11-2 loss: 0.964009  [   96/  306]
train() client id: f_00004-11-3 loss: 0.703742  [  128/  306]
train() client id: f_00004-11-4 loss: 0.918093  [  160/  306]
train() client id: f_00004-11-5 loss: 0.837952  [  192/  306]
train() client id: f_00004-11-6 loss: 0.804074  [  224/  306]
train() client id: f_00004-11-7 loss: 0.887401  [  256/  306]
train() client id: f_00004-11-8 loss: 0.815831  [  288/  306]
train() client id: f_00004-12-0 loss: 0.797826  [   32/  306]
train() client id: f_00004-12-1 loss: 0.841087  [   64/  306]
train() client id: f_00004-12-2 loss: 0.929801  [   96/  306]
train() client id: f_00004-12-3 loss: 0.926291  [  128/  306]
train() client id: f_00004-12-4 loss: 0.825083  [  160/  306]
train() client id: f_00004-12-5 loss: 0.814303  [  192/  306]
train() client id: f_00004-12-6 loss: 0.782004  [  224/  306]
train() client id: f_00004-12-7 loss: 1.013447  [  256/  306]
train() client id: f_00004-12-8 loss: 0.814767  [  288/  306]
train() client id: f_00004-13-0 loss: 0.789650  [   32/  306]
train() client id: f_00004-13-1 loss: 0.875386  [   64/  306]
train() client id: f_00004-13-2 loss: 0.919507  [   96/  306]
train() client id: f_00004-13-3 loss: 0.839983  [  128/  306]
train() client id: f_00004-13-4 loss: 0.770743  [  160/  306]
train() client id: f_00004-13-5 loss: 0.834597  [  192/  306]
train() client id: f_00004-13-6 loss: 0.914814  [  224/  306]
train() client id: f_00004-13-7 loss: 0.891516  [  256/  306]
train() client id: f_00004-13-8 loss: 0.821957  [  288/  306]
train() client id: f_00004-14-0 loss: 0.815872  [   32/  306]
train() client id: f_00004-14-1 loss: 0.954877  [   64/  306]
train() client id: f_00004-14-2 loss: 0.724960  [   96/  306]
train() client id: f_00004-14-3 loss: 1.016982  [  128/  306]
train() client id: f_00004-14-4 loss: 0.834701  [  160/  306]
train() client id: f_00004-14-5 loss: 0.775523  [  192/  306]
train() client id: f_00004-14-6 loss: 0.802138  [  224/  306]
train() client id: f_00004-14-7 loss: 0.852938  [  256/  306]
train() client id: f_00004-14-8 loss: 0.923574  [  288/  306]
train() client id: f_00004-15-0 loss: 0.794942  [   32/  306]
train() client id: f_00004-15-1 loss: 0.803794  [   64/  306]
train() client id: f_00004-15-2 loss: 0.872465  [   96/  306]
train() client id: f_00004-15-3 loss: 0.927833  [  128/  306]
train() client id: f_00004-15-4 loss: 0.882357  [  160/  306]
train() client id: f_00004-15-5 loss: 0.780168  [  192/  306]
train() client id: f_00004-15-6 loss: 0.977622  [  224/  306]
train() client id: f_00004-15-7 loss: 0.913474  [  256/  306]
train() client id: f_00004-15-8 loss: 0.787722  [  288/  306]
train() client id: f_00004-16-0 loss: 0.860773  [   32/  306]
train() client id: f_00004-16-1 loss: 0.803849  [   64/  306]
train() client id: f_00004-16-2 loss: 0.863480  [   96/  306]
train() client id: f_00004-16-3 loss: 0.948649  [  128/  306]
train() client id: f_00004-16-4 loss: 0.896911  [  160/  306]
train() client id: f_00004-16-5 loss: 0.821797  [  192/  306]
train() client id: f_00004-16-6 loss: 0.871189  [  224/  306]
train() client id: f_00004-16-7 loss: 0.859766  [  256/  306]
train() client id: f_00004-16-8 loss: 0.776225  [  288/  306]
train() client id: f_00005-0-0 loss: 0.410198  [   32/  146]
train() client id: f_00005-0-1 loss: 0.809416  [   64/  146]
train() client id: f_00005-0-2 loss: 0.565432  [   96/  146]
train() client id: f_00005-0-3 loss: 0.804581  [  128/  146]
train() client id: f_00005-1-0 loss: 0.436215  [   32/  146]
train() client id: f_00005-1-1 loss: 0.950263  [   64/  146]
train() client id: f_00005-1-2 loss: 0.563877  [   96/  146]
train() client id: f_00005-1-3 loss: 0.623158  [  128/  146]
train() client id: f_00005-2-0 loss: 0.637194  [   32/  146]
train() client id: f_00005-2-1 loss: 0.704370  [   64/  146]
train() client id: f_00005-2-2 loss: 0.420236  [   96/  146]
train() client id: f_00005-2-3 loss: 0.743555  [  128/  146]
train() client id: f_00005-3-0 loss: 0.599531  [   32/  146]
train() client id: f_00005-3-1 loss: 0.661727  [   64/  146]
train() client id: f_00005-3-2 loss: 0.697698  [   96/  146]
train() client id: f_00005-3-3 loss: 0.348279  [  128/  146]
train() client id: f_00005-4-0 loss: 0.590849  [   32/  146]
train() client id: f_00005-4-1 loss: 0.624181  [   64/  146]
train() client id: f_00005-4-2 loss: 0.615223  [   96/  146]
train() client id: f_00005-4-3 loss: 0.590174  [  128/  146]
train() client id: f_00005-5-0 loss: 0.519244  [   32/  146]
train() client id: f_00005-5-1 loss: 0.618898  [   64/  146]
train() client id: f_00005-5-2 loss: 0.904968  [   96/  146]
train() client id: f_00005-5-3 loss: 0.306289  [  128/  146]
train() client id: f_00005-6-0 loss: 0.737837  [   32/  146]
train() client id: f_00005-6-1 loss: 0.707997  [   64/  146]
train() client id: f_00005-6-2 loss: 0.665773  [   96/  146]
train() client id: f_00005-6-3 loss: 0.371379  [  128/  146]
train() client id: f_00005-7-0 loss: 0.489196  [   32/  146]
train() client id: f_00005-7-1 loss: 0.629114  [   64/  146]
train() client id: f_00005-7-2 loss: 0.481813  [   96/  146]
train() client id: f_00005-7-3 loss: 0.803351  [  128/  146]
train() client id: f_00005-8-0 loss: 0.521150  [   32/  146]
train() client id: f_00005-8-1 loss: 0.516559  [   64/  146]
train() client id: f_00005-8-2 loss: 0.756053  [   96/  146]
train() client id: f_00005-8-3 loss: 0.722336  [  128/  146]
train() client id: f_00005-9-0 loss: 0.455830  [   32/  146]
train() client id: f_00005-9-1 loss: 0.267102  [   64/  146]
train() client id: f_00005-9-2 loss: 0.844355  [   96/  146]
train() client id: f_00005-9-3 loss: 0.812221  [  128/  146]
train() client id: f_00005-10-0 loss: 0.581516  [   32/  146]
train() client id: f_00005-10-1 loss: 0.579182  [   64/  146]
train() client id: f_00005-10-2 loss: 0.791584  [   96/  146]
train() client id: f_00005-10-3 loss: 0.594047  [  128/  146]
train() client id: f_00005-11-0 loss: 0.631352  [   32/  146]
train() client id: f_00005-11-1 loss: 0.661365  [   64/  146]
train() client id: f_00005-11-2 loss: 0.688718  [   96/  146]
train() client id: f_00005-11-3 loss: 0.474290  [  128/  146]
train() client id: f_00005-12-0 loss: 0.522905  [   32/  146]
train() client id: f_00005-12-1 loss: 0.586707  [   64/  146]
train() client id: f_00005-12-2 loss: 0.566241  [   96/  146]
train() client id: f_00005-12-3 loss: 0.756017  [  128/  146]
train() client id: f_00005-13-0 loss: 0.493562  [   32/  146]
train() client id: f_00005-13-1 loss: 0.703949  [   64/  146]
train() client id: f_00005-13-2 loss: 0.479708  [   96/  146]
train() client id: f_00005-13-3 loss: 0.642806  [  128/  146]
train() client id: f_00005-14-0 loss: 0.703075  [   32/  146]
train() client id: f_00005-14-1 loss: 0.586890  [   64/  146]
train() client id: f_00005-14-2 loss: 0.461763  [   96/  146]
train() client id: f_00005-14-3 loss: 0.587221  [  128/  146]
train() client id: f_00005-15-0 loss: 0.594400  [   32/  146]
train() client id: f_00005-15-1 loss: 0.557901  [   64/  146]
train() client id: f_00005-15-2 loss: 0.425587  [   96/  146]
train() client id: f_00005-15-3 loss: 0.678455  [  128/  146]
train() client id: f_00005-16-0 loss: 0.608252  [   32/  146]
train() client id: f_00005-16-1 loss: 0.870492  [   64/  146]
train() client id: f_00005-16-2 loss: 0.488021  [   96/  146]
train() client id: f_00005-16-3 loss: 0.538203  [  128/  146]
train() client id: f_00006-0-0 loss: 0.500370  [   32/   54]
train() client id: f_00006-1-0 loss: 0.508768  [   32/   54]
train() client id: f_00006-2-0 loss: 0.452126  [   32/   54]
train() client id: f_00006-3-0 loss: 0.466411  [   32/   54]
train() client id: f_00006-4-0 loss: 0.435257  [   32/   54]
train() client id: f_00006-5-0 loss: 0.492962  [   32/   54]
train() client id: f_00006-6-0 loss: 0.408292  [   32/   54]
train() client id: f_00006-7-0 loss: 0.474267  [   32/   54]
train() client id: f_00006-8-0 loss: 0.386256  [   32/   54]
train() client id: f_00006-9-0 loss: 0.458438  [   32/   54]
train() client id: f_00006-10-0 loss: 0.439280  [   32/   54]
train() client id: f_00006-11-0 loss: 0.488319  [   32/   54]
train() client id: f_00006-12-0 loss: 0.443190  [   32/   54]
train() client id: f_00006-13-0 loss: 0.403436  [   32/   54]
train() client id: f_00006-14-0 loss: 0.451171  [   32/   54]
train() client id: f_00006-15-0 loss: 0.429997  [   32/   54]
train() client id: f_00006-16-0 loss: 0.442149  [   32/   54]
train() client id: f_00007-0-0 loss: 0.725887  [   32/  179]
train() client id: f_00007-0-1 loss: 0.635085  [   64/  179]
train() client id: f_00007-0-2 loss: 0.565631  [   96/  179]
train() client id: f_00007-0-3 loss: 0.596933  [  128/  179]
train() client id: f_00007-0-4 loss: 0.607495  [  160/  179]
train() client id: f_00007-1-0 loss: 0.554926  [   32/  179]
train() client id: f_00007-1-1 loss: 0.606030  [   64/  179]
train() client id: f_00007-1-2 loss: 0.811115  [   96/  179]
train() client id: f_00007-1-3 loss: 0.527936  [  128/  179]
train() client id: f_00007-1-4 loss: 0.669948  [  160/  179]
train() client id: f_00007-2-0 loss: 0.584627  [   32/  179]
train() client id: f_00007-2-1 loss: 0.784608  [   64/  179]
train() client id: f_00007-2-2 loss: 0.653128  [   96/  179]
train() client id: f_00007-2-3 loss: 0.477757  [  128/  179]
train() client id: f_00007-2-4 loss: 0.602756  [  160/  179]
train() client id: f_00007-3-0 loss: 0.648284  [   32/  179]
train() client id: f_00007-3-1 loss: 0.590598  [   64/  179]
train() client id: f_00007-3-2 loss: 0.631208  [   96/  179]
train() client id: f_00007-3-3 loss: 0.618919  [  128/  179]
train() client id: f_00007-3-4 loss: 0.572159  [  160/  179]
train() client id: f_00007-4-0 loss: 0.559780  [   32/  179]
train() client id: f_00007-4-1 loss: 0.625453  [   64/  179]
train() client id: f_00007-4-2 loss: 0.476559  [   96/  179]
train() client id: f_00007-4-3 loss: 0.713662  [  128/  179]
train() client id: f_00007-4-4 loss: 0.614776  [  160/  179]
train() client id: f_00007-5-0 loss: 0.502078  [   32/  179]
train() client id: f_00007-5-1 loss: 0.642963  [   64/  179]
train() client id: f_00007-5-2 loss: 0.647087  [   96/  179]
train() client id: f_00007-5-3 loss: 0.443189  [  128/  179]
train() client id: f_00007-5-4 loss: 0.501804  [  160/  179]
train() client id: f_00007-6-0 loss: 0.876809  [   32/  179]
train() client id: f_00007-6-1 loss: 0.504240  [   64/  179]
train() client id: f_00007-6-2 loss: 0.419568  [   96/  179]
train() client id: f_00007-6-3 loss: 0.550192  [  128/  179]
train() client id: f_00007-6-4 loss: 0.410112  [  160/  179]
train() client id: f_00007-7-0 loss: 0.575457  [   32/  179]
train() client id: f_00007-7-1 loss: 0.479017  [   64/  179]
train() client id: f_00007-7-2 loss: 0.515490  [   96/  179]
train() client id: f_00007-7-3 loss: 0.773623  [  128/  179]
train() client id: f_00007-7-4 loss: 0.578809  [  160/  179]
train() client id: f_00007-8-0 loss: 0.665827  [   32/  179]
train() client id: f_00007-8-1 loss: 0.494138  [   64/  179]
train() client id: f_00007-8-2 loss: 0.521195  [   96/  179]
train() client id: f_00007-8-3 loss: 0.548776  [  128/  179]
train() client id: f_00007-8-4 loss: 0.415070  [  160/  179]
train() client id: f_00007-9-0 loss: 0.645721  [   32/  179]
train() client id: f_00007-9-1 loss: 0.500947  [   64/  179]
train() client id: f_00007-9-2 loss: 0.468857  [   96/  179]
train() client id: f_00007-9-3 loss: 0.856192  [  128/  179]
train() client id: f_00007-9-4 loss: 0.405994  [  160/  179]
train() client id: f_00007-10-0 loss: 0.578703  [   32/  179]
train() client id: f_00007-10-1 loss: 0.581310  [   64/  179]
train() client id: f_00007-10-2 loss: 0.443213  [   96/  179]
train() client id: f_00007-10-3 loss: 0.602165  [  128/  179]
train() client id: f_00007-10-4 loss: 0.544671  [  160/  179]
train() client id: f_00007-11-0 loss: 0.626057  [   32/  179]
train() client id: f_00007-11-1 loss: 0.548457  [   64/  179]
train() client id: f_00007-11-2 loss: 0.489460  [   96/  179]
train() client id: f_00007-11-3 loss: 0.387317  [  128/  179]
train() client id: f_00007-11-4 loss: 0.594632  [  160/  179]
train() client id: f_00007-12-0 loss: 0.600227  [   32/  179]
train() client id: f_00007-12-1 loss: 0.643029  [   64/  179]
train() client id: f_00007-12-2 loss: 0.387023  [   96/  179]
train() client id: f_00007-12-3 loss: 0.603689  [  128/  179]
train() client id: f_00007-12-4 loss: 0.537215  [  160/  179]
train() client id: f_00007-13-0 loss: 0.504114  [   32/  179]
train() client id: f_00007-13-1 loss: 0.657335  [   64/  179]
train() client id: f_00007-13-2 loss: 0.500533  [   96/  179]
train() client id: f_00007-13-3 loss: 0.546798  [  128/  179]
train() client id: f_00007-13-4 loss: 0.496127  [  160/  179]
train() client id: f_00007-14-0 loss: 0.549893  [   32/  179]
train() client id: f_00007-14-1 loss: 0.603967  [   64/  179]
train() client id: f_00007-14-2 loss: 0.386354  [   96/  179]
train() client id: f_00007-14-3 loss: 0.589215  [  128/  179]
train() client id: f_00007-14-4 loss: 0.530398  [  160/  179]
train() client id: f_00007-15-0 loss: 0.423135  [   32/  179]
train() client id: f_00007-15-1 loss: 0.574043  [   64/  179]
train() client id: f_00007-15-2 loss: 0.557495  [   96/  179]
train() client id: f_00007-15-3 loss: 0.617466  [  128/  179]
train() client id: f_00007-15-4 loss: 0.608659  [  160/  179]
train() client id: f_00007-16-0 loss: 0.652743  [   32/  179]
train() client id: f_00007-16-1 loss: 0.390975  [   64/  179]
train() client id: f_00007-16-2 loss: 0.793704  [   96/  179]
train() client id: f_00007-16-3 loss: 0.413772  [  128/  179]
train() client id: f_00007-16-4 loss: 0.513969  [  160/  179]
train() client id: f_00008-0-0 loss: 0.661632  [   32/  130]
train() client id: f_00008-0-1 loss: 0.765547  [   64/  130]
train() client id: f_00008-0-2 loss: 0.719871  [   96/  130]
train() client id: f_00008-0-3 loss: 0.779327  [  128/  130]
train() client id: f_00008-1-0 loss: 0.799170  [   32/  130]
train() client id: f_00008-1-1 loss: 0.722303  [   64/  130]
train() client id: f_00008-1-2 loss: 0.703983  [   96/  130]
train() client id: f_00008-1-3 loss: 0.707127  [  128/  130]
train() client id: f_00008-2-0 loss: 0.824588  [   32/  130]
train() client id: f_00008-2-1 loss: 0.647127  [   64/  130]
train() client id: f_00008-2-2 loss: 0.635173  [   96/  130]
train() client id: f_00008-2-3 loss: 0.822648  [  128/  130]
train() client id: f_00008-3-0 loss: 0.874091  [   32/  130]
train() client id: f_00008-3-1 loss: 0.624243  [   64/  130]
train() client id: f_00008-3-2 loss: 0.724863  [   96/  130]
train() client id: f_00008-3-3 loss: 0.697424  [  128/  130]
train() client id: f_00008-4-0 loss: 0.779054  [   32/  130]
train() client id: f_00008-4-1 loss: 0.642013  [   64/  130]
train() client id: f_00008-4-2 loss: 0.696601  [   96/  130]
train() client id: f_00008-4-3 loss: 0.786714  [  128/  130]
train() client id: f_00008-5-0 loss: 0.670739  [   32/  130]
train() client id: f_00008-5-1 loss: 0.798515  [   64/  130]
train() client id: f_00008-5-2 loss: 0.879547  [   96/  130]
train() client id: f_00008-5-3 loss: 0.588714  [  128/  130]
train() client id: f_00008-6-0 loss: 0.723589  [   32/  130]
train() client id: f_00008-6-1 loss: 0.816407  [   64/  130]
train() client id: f_00008-6-2 loss: 0.726233  [   96/  130]
train() client id: f_00008-6-3 loss: 0.632695  [  128/  130]
train() client id: f_00008-7-0 loss: 0.786004  [   32/  130]
train() client id: f_00008-7-1 loss: 0.613718  [   64/  130]
train() client id: f_00008-7-2 loss: 0.758967  [   96/  130]
train() client id: f_00008-7-3 loss: 0.771631  [  128/  130]
train() client id: f_00008-8-0 loss: 0.607939  [   32/  130]
train() client id: f_00008-8-1 loss: 0.770277  [   64/  130]
train() client id: f_00008-8-2 loss: 0.730613  [   96/  130]
train() client id: f_00008-8-3 loss: 0.814932  [  128/  130]
train() client id: f_00008-9-0 loss: 0.727630  [   32/  130]
train() client id: f_00008-9-1 loss: 0.787714  [   64/  130]
train() client id: f_00008-9-2 loss: 0.761758  [   96/  130]
train() client id: f_00008-9-3 loss: 0.644396  [  128/  130]
train() client id: f_00008-10-0 loss: 0.659364  [   32/  130]
train() client id: f_00008-10-1 loss: 0.790468  [   64/  130]
train() client id: f_00008-10-2 loss: 0.730509  [   96/  130]
train() client id: f_00008-10-3 loss: 0.727803  [  128/  130]
train() client id: f_00008-11-0 loss: 0.864691  [   32/  130]
train() client id: f_00008-11-1 loss: 0.701012  [   64/  130]
train() client id: f_00008-11-2 loss: 0.615918  [   96/  130]
train() client id: f_00008-11-3 loss: 0.720766  [  128/  130]
train() client id: f_00008-12-0 loss: 0.796473  [   32/  130]
train() client id: f_00008-12-1 loss: 0.683611  [   64/  130]
train() client id: f_00008-12-2 loss: 0.743089  [   96/  130]
train() client id: f_00008-12-3 loss: 0.677297  [  128/  130]
train() client id: f_00008-13-0 loss: 0.759429  [   32/  130]
train() client id: f_00008-13-1 loss: 0.747097  [   64/  130]
train() client id: f_00008-13-2 loss: 0.774963  [   96/  130]
train() client id: f_00008-13-3 loss: 0.627326  [  128/  130]
train() client id: f_00008-14-0 loss: 0.674700  [   32/  130]
train() client id: f_00008-14-1 loss: 0.753050  [   64/  130]
train() client id: f_00008-14-2 loss: 0.852595  [   96/  130]
train() client id: f_00008-14-3 loss: 0.636672  [  128/  130]
train() client id: f_00008-15-0 loss: 0.729854  [   32/  130]
train() client id: f_00008-15-1 loss: 0.727812  [   64/  130]
train() client id: f_00008-15-2 loss: 0.828653  [   96/  130]
train() client id: f_00008-15-3 loss: 0.629603  [  128/  130]
train() client id: f_00008-16-0 loss: 0.776027  [   32/  130]
train() client id: f_00008-16-1 loss: 0.656480  [   64/  130]
train() client id: f_00008-16-2 loss: 0.681661  [   96/  130]
train() client id: f_00008-16-3 loss: 0.804431  [  128/  130]
train() client id: f_00009-0-0 loss: 1.272676  [   32/  118]
train() client id: f_00009-0-1 loss: 1.253610  [   64/  118]
train() client id: f_00009-0-2 loss: 1.344315  [   96/  118]
train() client id: f_00009-1-0 loss: 1.361168  [   32/  118]
train() client id: f_00009-1-1 loss: 1.093552  [   64/  118]
train() client id: f_00009-1-2 loss: 1.217933  [   96/  118]
train() client id: f_00009-2-0 loss: 1.126211  [   32/  118]
train() client id: f_00009-2-1 loss: 1.149168  [   64/  118]
train() client id: f_00009-2-2 loss: 1.105380  [   96/  118]
train() client id: f_00009-3-0 loss: 1.111376  [   32/  118]
train() client id: f_00009-3-1 loss: 1.008839  [   64/  118]
train() client id: f_00009-3-2 loss: 1.142301  [   96/  118]
train() client id: f_00009-4-0 loss: 1.150620  [   32/  118]
train() client id: f_00009-4-1 loss: 1.069725  [   64/  118]
train() client id: f_00009-4-2 loss: 1.188133  [   96/  118]
train() client id: f_00009-5-0 loss: 0.967719  [   32/  118]
train() client id: f_00009-5-1 loss: 1.285228  [   64/  118]
train() client id: f_00009-5-2 loss: 0.954587  [   96/  118]
train() client id: f_00009-6-0 loss: 0.951902  [   32/  118]
train() client id: f_00009-6-1 loss: 1.128536  [   64/  118]
train() client id: f_00009-6-2 loss: 0.969855  [   96/  118]
train() client id: f_00009-7-0 loss: 1.105054  [   32/  118]
train() client id: f_00009-7-1 loss: 0.994795  [   64/  118]
train() client id: f_00009-7-2 loss: 0.958041  [   96/  118]
train() client id: f_00009-8-0 loss: 1.021628  [   32/  118]
train() client id: f_00009-8-1 loss: 0.845565  [   64/  118]
train() client id: f_00009-8-2 loss: 1.104393  [   96/  118]
train() client id: f_00009-9-0 loss: 0.994888  [   32/  118]
train() client id: f_00009-9-1 loss: 1.009550  [   64/  118]
train() client id: f_00009-9-2 loss: 0.920475  [   96/  118]
train() client id: f_00009-10-0 loss: 0.908591  [   32/  118]
train() client id: f_00009-10-1 loss: 0.916597  [   64/  118]
train() client id: f_00009-10-2 loss: 1.098625  [   96/  118]
train() client id: f_00009-11-0 loss: 0.949910  [   32/  118]
train() client id: f_00009-11-1 loss: 0.917470  [   64/  118]
train() client id: f_00009-11-2 loss: 0.928548  [   96/  118]
train() client id: f_00009-12-0 loss: 0.994549  [   32/  118]
train() client id: f_00009-12-1 loss: 1.023522  [   64/  118]
train() client id: f_00009-12-2 loss: 0.843353  [   96/  118]
train() client id: f_00009-13-0 loss: 0.876489  [   32/  118]
train() client id: f_00009-13-1 loss: 0.933642  [   64/  118]
train() client id: f_00009-13-2 loss: 0.915818  [   96/  118]
train() client id: f_00009-14-0 loss: 0.810312  [   32/  118]
train() client id: f_00009-14-1 loss: 1.080994  [   64/  118]
train() client id: f_00009-14-2 loss: 0.868014  [   96/  118]
train() client id: f_00009-15-0 loss: 1.002665  [   32/  118]
train() client id: f_00009-15-1 loss: 0.771956  [   64/  118]
train() client id: f_00009-15-2 loss: 1.078835  [   96/  118]
train() client id: f_00009-16-0 loss: 0.990194  [   32/  118]
train() client id: f_00009-16-1 loss: 0.806116  [   64/  118]
train() client id: f_00009-16-2 loss: 0.957292  [   96/  118]
At round 52 accuracy: 0.6472148541114059
At round 52 training accuracy: 0.5861837692823608
At round 52 training loss: 0.8419195670287214
update_location
xs = 8.927491 381.223621 5.882650 0.934260 -297.581990 -145.230757 -105.849135 -5.143845 -320.120581 20.134486 
ys = -372.390647 7.291448 270.684448 -92.290817 -9.642386 0.794442 -1.381692 266.628436 25.881276 -807.232496 
xs mean: -45.682379970521225
ys mean: -71.16579882624052
dists_uav = 385.687042 394.188552 288.625495 136.082577 314.082817 176.330950 145.622623 284.810783 336.373344 813.652075 
uav_gains = -120.850935 -121.185692 -114.752318 -103.346292 -116.849704 -106.193941 -104.083792 -114.413150 -118.378769 -129.729464 
uav_gains_db_mean: -114.97840553877967
dists_bs = 577.465014 580.956861 204.631490 319.995695 221.628416 176.730824 189.452748 192.994517 208.077814 1001.428052 
bs_gains = -116.889778 -116.963088 -104.274288 -109.711095 -105.244572 -102.491804 -103.337085 -103.562318 -104.477381 -123.584412 
bs_gains_db_mean: -109.05358204565054
Round 53
-------------------------------
ene_coms = [0.03118076 0.03267862 0.00765707 0.00731659 0.00804773 0.00846928
 0.00758468 0.00739318 0.00773575 0.04708099]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.16491159 8.42629379 3.86554446 1.38768535 9.42928774 4.54583338
 1.72643184 5.54302669 4.04963107 4.08157824]
obj_prev = 47.220224160492975
eta_min = 1.2514641618915478e-23	eta_max = 0.8257084428672039
af = 9.791604535988256	bf = 1.6560166367691755	zeta = 10.770764989587082	eta = 0.9090909090909091
af = 9.791604535988256	bf = 1.6560166367691755	zeta = 28.48181848233765	eta = 0.3437843879968653
af = 9.791604535988256	bf = 1.6560166367691755	zeta = 18.4556875667139	eta = 0.5305467217405699
af = 9.791604535988256	bf = 1.6560166367691755	zeta = 16.747988873411384	eta = 0.5846436016883866
af = 9.791604535988256	bf = 1.6560166367691755	zeta = 16.64037982679955	eta = 0.588424341145071
af = 9.791604535988256	bf = 1.6560166367691755	zeta = 16.639886639697664	eta = 0.5884417813657752
af = 9.791604535988256	bf = 1.6560166367691755	zeta = 16.639886629248103	eta = 0.5884417817353064
eta = 0.5884417817353064
ene_coms = [0.03118076 0.03267862 0.00765707 0.00731659 0.00804773 0.00846928
 0.00758468 0.00739318 0.00773575 0.04708099]
ene_comp = [0.04375721 0.09202905 0.04306265 0.01493302 0.10626751 0.0507028
 0.01875309 0.06216302 0.04514633 0.04097897]
ene_total = [1.82587406 3.0385198  1.23579308 0.54211474 2.7853067  1.44173592
 0.64172336 1.69474641 1.28847921 2.14559334]
ti_comp = [0.80866234 0.79368372 1.04389923 1.04730405 1.03999264 1.03577715
 1.04462318 1.04653818 1.04311243 0.64966001]
ti_coms = [0.31180761 0.32678623 0.07657072 0.0731659  0.08047731 0.08469281
 0.07584678 0.07393177 0.07735752 0.47080994]
t_total = [27.3004055 27.3004055 27.3004055 27.3004055 27.3004055 27.3004055
 27.3004055 27.3004055 27.3004055 27.3004055]
ene_coms = [0.03118076 0.03267862 0.00765707 0.00731659 0.00804773 0.00846928
 0.00758468 0.00739318 0.00773575 0.04708099]
ene_comp = [8.00745426e-06 7.73321238e-05 4.57999667e-06 1.89747938e-07
 6.93459357e-05 7.59352073e-06 3.77728045e-07 1.37077509e-05
 5.28548788e-06 1.01903951e-05]
ene_total = [0.75991867 0.79810338 0.1866772  0.17827436 0.19777371 0.20654021
 0.18481092 0.18046978 0.18861146 1.14738324]
optimize_network iter = 0 obj = 4.028562923506365
eta = 0.5884417817353064
freqs = [27055303.81063926 57975898.2608472  20625866.03855719  7129265.02443659
 51090511.11768746 24475727.99063034  8976007.15044978 29699356.64155923
 21640202.42993276 31538785.19996504]
eta_min = 0.588441781735307	eta_max = 0.5884417817353053
af = 0.003717945793418259	bf = 1.6560166367691755	zeta = 0.0040897403727600855	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03118076 0.03267862 0.00765707 0.00731659 0.00804773 0.00846928
 0.00758468 0.00739318 0.00773575 0.04708099]
ene_comp = [1.43923175e-06 1.38994048e-05 8.23192543e-07 3.41046292e-08
 1.24639954e-05 1.36483279e-06 6.78915145e-08 2.46378310e-06
 9.49995061e-07 1.83158588e-06]
ene_total = [3.43953402 3.60612913 0.84469952 0.80705587 0.88907501 0.93434947
 0.83663082 0.81577178 0.85339232 5.193443  ]
ti_comp = [0.80866234 0.79368372 1.04389923 1.04730405 1.03999264 1.03577715
 1.04462318 1.04653818 1.04311243 0.64966001]
ti_coms = [0.31180761 0.32678623 0.07657072 0.0731659  0.08047731 0.08469281
 0.07584678 0.07393177 0.07735752 0.47080994]
t_total = [27.3004055 27.3004055 27.3004055 27.3004055 27.3004055 27.3004055
 27.3004055 27.3004055 27.3004055 27.3004055]
ene_coms = [0.03118076 0.03267862 0.00765707 0.00731659 0.00804773 0.00846928
 0.00758468 0.00739318 0.00773575 0.04708099]
ene_comp = [8.00745426e-06 7.73321238e-05 4.57999667e-06 1.89747938e-07
 6.93459357e-05 7.59352073e-06 3.77728045e-07 1.37077509e-05
 5.28548788e-06 1.01903951e-05]
ene_total = [0.75991867 0.79810338 0.1866772  0.17827436 0.19777371 0.20654021
 0.18481092 0.18046978 0.18861146 1.14738324]
optimize_network iter = 1 obj = 4.028562923506354
eta = 0.5884417817353053
freqs = [27055303.81063926 57975898.26084718 20625866.03855721  7129265.0244366
 51090511.11768749 24475727.99063036  8976007.15044978 29699356.64155925
 21640202.42993278 31538785.19996501]
Done!
ene_coms = [0.03118076 0.03267862 0.00765707 0.00731659 0.00804773 0.00846928
 0.00758468 0.00739318 0.00773575 0.04708099]
ene_comp = [7.83960716e-06 7.57111377e-05 4.48399373e-06 1.85770564e-07
 6.78923509e-05 7.43435024e-06 3.69810353e-07 1.34204179e-05
 5.17469689e-06 9.97679057e-06]
ene_total = [0.0311886  0.03275433 0.00766156 0.00731678 0.00811562 0.00847671
 0.00758505 0.0074066  0.00774093 0.04709097]
At round 53 energy consumption: 0.16533714833384372
At round 53 eta: 0.5884417817353053
At round 53 a_n: 10.027672966880559
At round 53 local rounds: 17.363972400302487
At round 53 global rounds: 24.365138446661355
gradient difference: 0.2856800854206085
train() client id: f_00000-0-0 loss: 1.483568  [   32/  126]
train() client id: f_00000-0-1 loss: 1.182769  [   64/  126]
train() client id: f_00000-0-2 loss: 1.021776  [   96/  126]
train() client id: f_00000-1-0 loss: 1.112758  [   32/  126]
train() client id: f_00000-1-1 loss: 1.151693  [   64/  126]
train() client id: f_00000-1-2 loss: 0.860006  [   96/  126]
train() client id: f_00000-2-0 loss: 1.118951  [   32/  126]
train() client id: f_00000-2-1 loss: 0.881323  [   64/  126]
train() client id: f_00000-2-2 loss: 1.090139  [   96/  126]
train() client id: f_00000-3-0 loss: 0.851823  [   32/  126]
train() client id: f_00000-3-1 loss: 0.943712  [   64/  126]
train() client id: f_00000-3-2 loss: 0.973660  [   96/  126]
train() client id: f_00000-4-0 loss: 0.984658  [   32/  126]
train() client id: f_00000-4-1 loss: 0.982962  [   64/  126]
train() client id: f_00000-4-2 loss: 0.686799  [   96/  126]
train() client id: f_00000-5-0 loss: 0.923902  [   32/  126]
train() client id: f_00000-5-1 loss: 0.787659  [   64/  126]
train() client id: f_00000-5-2 loss: 0.813998  [   96/  126]
train() client id: f_00000-6-0 loss: 0.809539  [   32/  126]
train() client id: f_00000-6-1 loss: 0.882272  [   64/  126]
train() client id: f_00000-6-2 loss: 0.686515  [   96/  126]
train() client id: f_00000-7-0 loss: 0.845001  [   32/  126]
train() client id: f_00000-7-1 loss: 0.669121  [   64/  126]
train() client id: f_00000-7-2 loss: 0.719695  [   96/  126]
train() client id: f_00000-8-0 loss: 0.851945  [   32/  126]
train() client id: f_00000-8-1 loss: 0.625524  [   64/  126]
train() client id: f_00000-8-2 loss: 0.756304  [   96/  126]
train() client id: f_00000-9-0 loss: 0.765695  [   32/  126]
train() client id: f_00000-9-1 loss: 0.727227  [   64/  126]
train() client id: f_00000-9-2 loss: 0.814087  [   96/  126]
train() client id: f_00000-10-0 loss: 0.649334  [   32/  126]
train() client id: f_00000-10-1 loss: 0.748234  [   64/  126]
train() client id: f_00000-10-2 loss: 0.751176  [   96/  126]
train() client id: f_00000-11-0 loss: 0.708853  [   32/  126]
train() client id: f_00000-11-1 loss: 0.740232  [   64/  126]
train() client id: f_00000-11-2 loss: 0.749221  [   96/  126]
train() client id: f_00000-12-0 loss: 0.827294  [   32/  126]
train() client id: f_00000-12-1 loss: 0.722191  [   64/  126]
train() client id: f_00000-12-2 loss: 0.713192  [   96/  126]
train() client id: f_00000-13-0 loss: 0.678389  [   32/  126]
train() client id: f_00000-13-1 loss: 0.751458  [   64/  126]
train() client id: f_00000-13-2 loss: 0.731425  [   96/  126]
train() client id: f_00000-14-0 loss: 0.749182  [   32/  126]
train() client id: f_00000-14-1 loss: 0.657143  [   64/  126]
train() client id: f_00000-14-2 loss: 0.670395  [   96/  126]
train() client id: f_00000-15-0 loss: 0.806382  [   32/  126]
train() client id: f_00000-15-1 loss: 0.709470  [   64/  126]
train() client id: f_00000-15-2 loss: 0.644274  [   96/  126]
train() client id: f_00000-16-0 loss: 0.769663  [   32/  126]
train() client id: f_00000-16-1 loss: 0.882906  [   64/  126]
train() client id: f_00000-16-2 loss: 0.648140  [   96/  126]
train() client id: f_00001-0-0 loss: 0.330023  [   32/  265]
train() client id: f_00001-0-1 loss: 0.651125  [   64/  265]
train() client id: f_00001-0-2 loss: 0.468127  [   96/  265]
train() client id: f_00001-0-3 loss: 0.420657  [  128/  265]
train() client id: f_00001-0-4 loss: 0.309491  [  160/  265]
train() client id: f_00001-0-5 loss: 0.296643  [  192/  265]
train() client id: f_00001-0-6 loss: 0.417260  [  224/  265]
train() client id: f_00001-0-7 loss: 0.424531  [  256/  265]
train() client id: f_00001-1-0 loss: 0.405224  [   32/  265]
train() client id: f_00001-1-1 loss: 0.391586  [   64/  265]
train() client id: f_00001-1-2 loss: 0.475358  [   96/  265]
train() client id: f_00001-1-3 loss: 0.398059  [  128/  265]
train() client id: f_00001-1-4 loss: 0.477370  [  160/  265]
train() client id: f_00001-1-5 loss: 0.366052  [  192/  265]
train() client id: f_00001-1-6 loss: 0.336767  [  224/  265]
train() client id: f_00001-1-7 loss: 0.407131  [  256/  265]
train() client id: f_00001-2-0 loss: 0.325558  [   32/  265]
train() client id: f_00001-2-1 loss: 0.516691  [   64/  265]
train() client id: f_00001-2-2 loss: 0.465684  [   96/  265]
train() client id: f_00001-2-3 loss: 0.343802  [  128/  265]
train() client id: f_00001-2-4 loss: 0.384608  [  160/  265]
train() client id: f_00001-2-5 loss: 0.412187  [  192/  265]
train() client id: f_00001-2-6 loss: 0.355602  [  224/  265]
train() client id: f_00001-2-7 loss: 0.365446  [  256/  265]
train() client id: f_00001-3-0 loss: 0.420184  [   32/  265]
train() client id: f_00001-3-1 loss: 0.310957  [   64/  265]
train() client id: f_00001-3-2 loss: 0.436415  [   96/  265]
train() client id: f_00001-3-3 loss: 0.419040  [  128/  265]
train() client id: f_00001-3-4 loss: 0.399823  [  160/  265]
train() client id: f_00001-3-5 loss: 0.504111  [  192/  265]
train() client id: f_00001-3-6 loss: 0.321417  [  224/  265]
train() client id: f_00001-3-7 loss: 0.324224  [  256/  265]
train() client id: f_00001-4-0 loss: 0.389165  [   32/  265]
train() client id: f_00001-4-1 loss: 0.463479  [   64/  265]
train() client id: f_00001-4-2 loss: 0.436995  [   96/  265]
train() client id: f_00001-4-3 loss: 0.414131  [  128/  265]
train() client id: f_00001-4-4 loss: 0.387537  [  160/  265]
train() client id: f_00001-4-5 loss: 0.286448  [  192/  265]
train() client id: f_00001-4-6 loss: 0.413470  [  224/  265]
train() client id: f_00001-4-7 loss: 0.300492  [  256/  265]
train() client id: f_00001-5-0 loss: 0.380168  [   32/  265]
train() client id: f_00001-5-1 loss: 0.319653  [   64/  265]
train() client id: f_00001-5-2 loss: 0.475540  [   96/  265]
train() client id: f_00001-5-3 loss: 0.410974  [  128/  265]
train() client id: f_00001-5-4 loss: 0.302515  [  160/  265]
train() client id: f_00001-5-5 loss: 0.472665  [  192/  265]
train() client id: f_00001-5-6 loss: 0.292742  [  224/  265]
train() client id: f_00001-5-7 loss: 0.300525  [  256/  265]
train() client id: f_00001-6-0 loss: 0.319166  [   32/  265]
train() client id: f_00001-6-1 loss: 0.353407  [   64/  265]
train() client id: f_00001-6-2 loss: 0.380030  [   96/  265]
train() client id: f_00001-6-3 loss: 0.383481  [  128/  265]
train() client id: f_00001-6-4 loss: 0.517607  [  160/  265]
train() client id: f_00001-6-5 loss: 0.449046  [  192/  265]
train() client id: f_00001-6-6 loss: 0.273665  [  224/  265]
train() client id: f_00001-6-7 loss: 0.375295  [  256/  265]
train() client id: f_00001-7-0 loss: 0.369094  [   32/  265]
train() client id: f_00001-7-1 loss: 0.288847  [   64/  265]
train() client id: f_00001-7-2 loss: 0.492903  [   96/  265]
train() client id: f_00001-7-3 loss: 0.386685  [  128/  265]
train() client id: f_00001-7-4 loss: 0.298874  [  160/  265]
train() client id: f_00001-7-5 loss: 0.283155  [  192/  265]
train() client id: f_00001-7-6 loss: 0.560689  [  224/  265]
train() client id: f_00001-7-7 loss: 0.346448  [  256/  265]
train() client id: f_00001-8-0 loss: 0.367006  [   32/  265]
train() client id: f_00001-8-1 loss: 0.357263  [   64/  265]
train() client id: f_00001-8-2 loss: 0.415060  [   96/  265]
train() client id: f_00001-8-3 loss: 0.388379  [  128/  265]
train() client id: f_00001-8-4 loss: 0.331740  [  160/  265]
train() client id: f_00001-8-5 loss: 0.360505  [  192/  265]
train() client id: f_00001-8-6 loss: 0.277925  [  224/  265]
train() client id: f_00001-8-7 loss: 0.517295  [  256/  265]
train() client id: f_00001-9-0 loss: 0.285861  [   32/  265]
train() client id: f_00001-9-1 loss: 0.457252  [   64/  265]
train() client id: f_00001-9-2 loss: 0.373506  [   96/  265]
train() client id: f_00001-9-3 loss: 0.279457  [  128/  265]
train() client id: f_00001-9-4 loss: 0.447424  [  160/  265]
train() client id: f_00001-9-5 loss: 0.301479  [  192/  265]
train() client id: f_00001-9-6 loss: 0.568050  [  224/  265]
train() client id: f_00001-9-7 loss: 0.290939  [  256/  265]
train() client id: f_00001-10-0 loss: 0.485344  [   32/  265]
train() client id: f_00001-10-1 loss: 0.329361  [   64/  265]
train() client id: f_00001-10-2 loss: 0.371587  [   96/  265]
train() client id: f_00001-10-3 loss: 0.405670  [  128/  265]
train() client id: f_00001-10-4 loss: 0.392353  [  160/  265]
train() client id: f_00001-10-5 loss: 0.282327  [  192/  265]
train() client id: f_00001-10-6 loss: 0.287941  [  224/  265]
train() client id: f_00001-10-7 loss: 0.431610  [  256/  265]
train() client id: f_00001-11-0 loss: 0.340327  [   32/  265]
train() client id: f_00001-11-1 loss: 0.377907  [   64/  265]
train() client id: f_00001-11-2 loss: 0.290416  [   96/  265]
train() client id: f_00001-11-3 loss: 0.366053  [  128/  265]
train() client id: f_00001-11-4 loss: 0.300061  [  160/  265]
train() client id: f_00001-11-5 loss: 0.489280  [  192/  265]
train() client id: f_00001-11-6 loss: 0.385824  [  224/  265]
train() client id: f_00001-11-7 loss: 0.377877  [  256/  265]
train() client id: f_00001-12-0 loss: 0.326921  [   32/  265]
train() client id: f_00001-12-1 loss: 0.424113  [   64/  265]
train() client id: f_00001-12-2 loss: 0.271276  [   96/  265]
train() client id: f_00001-12-3 loss: 0.374244  [  128/  265]
train() client id: f_00001-12-4 loss: 0.530544  [  160/  265]
train() client id: f_00001-12-5 loss: 0.353128  [  192/  265]
train() client id: f_00001-12-6 loss: 0.281155  [  224/  265]
train() client id: f_00001-12-7 loss: 0.414581  [  256/  265]
train() client id: f_00001-13-0 loss: 0.257551  [   32/  265]
train() client id: f_00001-13-1 loss: 0.493984  [   64/  265]
train() client id: f_00001-13-2 loss: 0.392557  [   96/  265]
train() client id: f_00001-13-3 loss: 0.357534  [  128/  265]
train() client id: f_00001-13-4 loss: 0.381545  [  160/  265]
train() client id: f_00001-13-5 loss: 0.356978  [  192/  265]
train() client id: f_00001-13-6 loss: 0.374768  [  224/  265]
train() client id: f_00001-13-7 loss: 0.351647  [  256/  265]
train() client id: f_00001-14-0 loss: 0.519102  [   32/  265]
train() client id: f_00001-14-1 loss: 0.291533  [   64/  265]
train() client id: f_00001-14-2 loss: 0.488231  [   96/  265]
train() client id: f_00001-14-3 loss: 0.281308  [  128/  265]
train() client id: f_00001-14-4 loss: 0.348500  [  160/  265]
train() client id: f_00001-14-5 loss: 0.338834  [  192/  265]
train() client id: f_00001-14-6 loss: 0.342499  [  224/  265]
train() client id: f_00001-14-7 loss: 0.305334  [  256/  265]
train() client id: f_00001-15-0 loss: 0.308563  [   32/  265]
train() client id: f_00001-15-1 loss: 0.332725  [   64/  265]
train() client id: f_00001-15-2 loss: 0.450185  [   96/  265]
train() client id: f_00001-15-3 loss: 0.358032  [  128/  265]
train() client id: f_00001-15-4 loss: 0.411929  [  160/  265]
train() client id: f_00001-15-5 loss: 0.404327  [  192/  265]
train() client id: f_00001-15-6 loss: 0.273956  [  224/  265]
train() client id: f_00001-15-7 loss: 0.403718  [  256/  265]
train() client id: f_00001-16-0 loss: 0.267391  [   32/  265]
train() client id: f_00001-16-1 loss: 0.417204  [   64/  265]
train() client id: f_00001-16-2 loss: 0.413742  [   96/  265]
train() client id: f_00001-16-3 loss: 0.418030  [  128/  265]
train() client id: f_00001-16-4 loss: 0.417884  [  160/  265]
train() client id: f_00001-16-5 loss: 0.266036  [  192/  265]
train() client id: f_00001-16-6 loss: 0.348725  [  224/  265]
train() client id: f_00001-16-7 loss: 0.402114  [  256/  265]
train() client id: f_00002-0-0 loss: 1.290633  [   32/  124]
train() client id: f_00002-0-1 loss: 1.436964  [   64/  124]
train() client id: f_00002-0-2 loss: 1.127029  [   96/  124]
train() client id: f_00002-1-0 loss: 1.211420  [   32/  124]
train() client id: f_00002-1-1 loss: 1.301453  [   64/  124]
train() client id: f_00002-1-2 loss: 1.087636  [   96/  124]
train() client id: f_00002-2-0 loss: 0.977526  [   32/  124]
train() client id: f_00002-2-1 loss: 1.153218  [   64/  124]
train() client id: f_00002-2-2 loss: 1.153521  [   96/  124]
train() client id: f_00002-3-0 loss: 1.238220  [   32/  124]
train() client id: f_00002-3-1 loss: 0.989516  [   64/  124]
train() client id: f_00002-3-2 loss: 1.109936  [   96/  124]
train() client id: f_00002-4-0 loss: 1.051037  [   32/  124]
train() client id: f_00002-4-1 loss: 1.105457  [   64/  124]
train() client id: f_00002-4-2 loss: 0.950800  [   96/  124]
train() client id: f_00002-5-0 loss: 0.974835  [   32/  124]
train() client id: f_00002-5-1 loss: 1.055757  [   64/  124]
train() client id: f_00002-5-2 loss: 0.801381  [   96/  124]
train() client id: f_00002-6-0 loss: 1.075428  [   32/  124]
train() client id: f_00002-6-1 loss: 0.939932  [   64/  124]
train() client id: f_00002-6-2 loss: 0.805502  [   96/  124]
train() client id: f_00002-7-0 loss: 0.786767  [   32/  124]
train() client id: f_00002-7-1 loss: 0.870910  [   64/  124]
train() client id: f_00002-7-2 loss: 1.114732  [   96/  124]
train() client id: f_00002-8-0 loss: 0.842863  [   32/  124]
train() client id: f_00002-8-1 loss: 0.804670  [   64/  124]
train() client id: f_00002-8-2 loss: 1.080752  [   96/  124]
train() client id: f_00002-9-0 loss: 1.019006  [   32/  124]
train() client id: f_00002-9-1 loss: 0.775641  [   64/  124]
train() client id: f_00002-9-2 loss: 0.779297  [   96/  124]
train() client id: f_00002-10-0 loss: 0.914442  [   32/  124]
train() client id: f_00002-10-1 loss: 1.014246  [   64/  124]
train() client id: f_00002-10-2 loss: 0.682943  [   96/  124]
train() client id: f_00002-11-0 loss: 0.887980  [   32/  124]
train() client id: f_00002-11-1 loss: 0.909543  [   64/  124]
train() client id: f_00002-11-2 loss: 0.752787  [   96/  124]
train() client id: f_00002-12-0 loss: 0.898223  [   32/  124]
train() client id: f_00002-12-1 loss: 0.890325  [   64/  124]
train() client id: f_00002-12-2 loss: 0.779154  [   96/  124]
train() client id: f_00002-13-0 loss: 1.049798  [   32/  124]
train() client id: f_00002-13-1 loss: 0.814329  [   64/  124]
train() client id: f_00002-13-2 loss: 0.835033  [   96/  124]
train() client id: f_00002-14-0 loss: 0.959523  [   32/  124]
train() client id: f_00002-14-1 loss: 0.744547  [   64/  124]
train() client id: f_00002-14-2 loss: 0.859846  [   96/  124]
train() client id: f_00002-15-0 loss: 0.803982  [   32/  124]
train() client id: f_00002-15-1 loss: 1.005241  [   64/  124]
train() client id: f_00002-15-2 loss: 0.731599  [   96/  124]
train() client id: f_00002-16-0 loss: 0.933151  [   32/  124]
train() client id: f_00002-16-1 loss: 0.696240  [   64/  124]
train() client id: f_00002-16-2 loss: 0.972129  [   96/  124]
train() client id: f_00003-0-0 loss: 0.985723  [   32/   43]
train() client id: f_00003-1-0 loss: 0.894007  [   32/   43]
train() client id: f_00003-2-0 loss: 1.056380  [   32/   43]
train() client id: f_00003-3-0 loss: 0.841316  [   32/   43]
train() client id: f_00003-4-0 loss: 0.686521  [   32/   43]
train() client id: f_00003-5-0 loss: 0.792723  [   32/   43]
train() client id: f_00003-6-0 loss: 0.925565  [   32/   43]
train() client id: f_00003-7-0 loss: 0.892787  [   32/   43]
train() client id: f_00003-8-0 loss: 0.895141  [   32/   43]
train() client id: f_00003-9-0 loss: 0.887547  [   32/   43]
train() client id: f_00003-10-0 loss: 0.683562  [   32/   43]
train() client id: f_00003-11-0 loss: 0.807006  [   32/   43]
train() client id: f_00003-12-0 loss: 0.804165  [   32/   43]
train() client id: f_00003-13-0 loss: 0.807794  [   32/   43]
train() client id: f_00003-14-0 loss: 0.722668  [   32/   43]
train() client id: f_00003-15-0 loss: 0.950643  [   32/   43]
train() client id: f_00003-16-0 loss: 0.984034  [   32/   43]
train() client id: f_00004-0-0 loss: 0.877295  [   32/  306]
train() client id: f_00004-0-1 loss: 0.862784  [   64/  306]
train() client id: f_00004-0-2 loss: 1.012809  [   96/  306]
train() client id: f_00004-0-3 loss: 0.917486  [  128/  306]
train() client id: f_00004-0-4 loss: 1.101443  [  160/  306]
train() client id: f_00004-0-5 loss: 0.932434  [  192/  306]
train() client id: f_00004-0-6 loss: 0.958706  [  224/  306]
train() client id: f_00004-0-7 loss: 0.933493  [  256/  306]
train() client id: f_00004-0-8 loss: 0.879637  [  288/  306]
train() client id: f_00004-1-0 loss: 0.945070  [   32/  306]
train() client id: f_00004-1-1 loss: 0.703937  [   64/  306]
train() client id: f_00004-1-2 loss: 0.973941  [   96/  306]
train() client id: f_00004-1-3 loss: 0.953364  [  128/  306]
train() client id: f_00004-1-4 loss: 0.986740  [  160/  306]
train() client id: f_00004-1-5 loss: 1.055751  [  192/  306]
train() client id: f_00004-1-6 loss: 1.057045  [  224/  306]
train() client id: f_00004-1-7 loss: 0.789150  [  256/  306]
train() client id: f_00004-1-8 loss: 0.943701  [  288/  306]
train() client id: f_00004-2-0 loss: 0.833433  [   32/  306]
train() client id: f_00004-2-1 loss: 0.894219  [   64/  306]
train() client id: f_00004-2-2 loss: 1.088908  [   96/  306]
train() client id: f_00004-2-3 loss: 0.838710  [  128/  306]
train() client id: f_00004-2-4 loss: 0.984008  [  160/  306]
train() client id: f_00004-2-5 loss: 0.887393  [  192/  306]
train() client id: f_00004-2-6 loss: 1.113233  [  224/  306]
train() client id: f_00004-2-7 loss: 0.798726  [  256/  306]
train() client id: f_00004-2-8 loss: 0.943415  [  288/  306]
train() client id: f_00004-3-0 loss: 1.085887  [   32/  306]
train() client id: f_00004-3-1 loss: 0.906446  [   64/  306]
train() client id: f_00004-3-2 loss: 0.944070  [   96/  306]
train() client id: f_00004-3-3 loss: 0.905052  [  128/  306]
train() client id: f_00004-3-4 loss: 0.899233  [  160/  306]
train() client id: f_00004-3-5 loss: 0.808207  [  192/  306]
train() client id: f_00004-3-6 loss: 0.884343  [  224/  306]
train() client id: f_00004-3-7 loss: 1.051756  [  256/  306]
train() client id: f_00004-3-8 loss: 0.953396  [  288/  306]
train() client id: f_00004-4-0 loss: 0.935029  [   32/  306]
train() client id: f_00004-4-1 loss: 0.905802  [   64/  306]
train() client id: f_00004-4-2 loss: 0.814052  [   96/  306]
train() client id: f_00004-4-3 loss: 1.008015  [  128/  306]
train() client id: f_00004-4-4 loss: 0.818992  [  160/  306]
train() client id: f_00004-4-5 loss: 1.052099  [  192/  306]
train() client id: f_00004-4-6 loss: 0.998362  [  224/  306]
train() client id: f_00004-4-7 loss: 0.936025  [  256/  306]
train() client id: f_00004-4-8 loss: 0.906997  [  288/  306]
train() client id: f_00004-5-0 loss: 0.996757  [   32/  306]
train() client id: f_00004-5-1 loss: 0.949885  [   64/  306]
train() client id: f_00004-5-2 loss: 0.801024  [   96/  306]
train() client id: f_00004-5-3 loss: 0.907126  [  128/  306]
train() client id: f_00004-5-4 loss: 0.933325  [  160/  306]
train() client id: f_00004-5-5 loss: 0.852646  [  192/  306]
train() client id: f_00004-5-6 loss: 0.887842  [  224/  306]
train() client id: f_00004-5-7 loss: 0.965284  [  256/  306]
train() client id: f_00004-5-8 loss: 0.951643  [  288/  306]
train() client id: f_00004-6-0 loss: 0.863229  [   32/  306]
train() client id: f_00004-6-1 loss: 0.932397  [   64/  306]
train() client id: f_00004-6-2 loss: 0.857008  [   96/  306]
train() client id: f_00004-6-3 loss: 1.008893  [  128/  306]
train() client id: f_00004-6-4 loss: 0.825463  [  160/  306]
train() client id: f_00004-6-5 loss: 1.050936  [  192/  306]
train() client id: f_00004-6-6 loss: 0.888220  [  224/  306]
train() client id: f_00004-6-7 loss: 0.983908  [  256/  306]
train() client id: f_00004-6-8 loss: 0.879883  [  288/  306]
train() client id: f_00004-7-0 loss: 0.876130  [   32/  306]
train() client id: f_00004-7-1 loss: 0.987536  [   64/  306]
train() client id: f_00004-7-2 loss: 1.058164  [   96/  306]
train() client id: f_00004-7-3 loss: 0.822341  [  128/  306]
train() client id: f_00004-7-4 loss: 0.816518  [  160/  306]
train() client id: f_00004-7-5 loss: 1.060678  [  192/  306]
train() client id: f_00004-7-6 loss: 0.844568  [  224/  306]
train() client id: f_00004-7-7 loss: 0.957164  [  256/  306]
train() client id: f_00004-7-8 loss: 0.941983  [  288/  306]
train() client id: f_00004-8-0 loss: 0.932709  [   32/  306]
train() client id: f_00004-8-1 loss: 0.854857  [   64/  306]
train() client id: f_00004-8-2 loss: 0.799541  [   96/  306]
train() client id: f_00004-8-3 loss: 1.047632  [  128/  306]
train() client id: f_00004-8-4 loss: 0.864801  [  160/  306]
train() client id: f_00004-8-5 loss: 1.011675  [  192/  306]
train() client id: f_00004-8-6 loss: 0.951012  [  224/  306]
train() client id: f_00004-8-7 loss: 0.947105  [  256/  306]
train() client id: f_00004-8-8 loss: 0.789812  [  288/  306]
train() client id: f_00004-9-0 loss: 0.875962  [   32/  306]
train() client id: f_00004-9-1 loss: 0.955197  [   64/  306]
train() client id: f_00004-9-2 loss: 0.869267  [   96/  306]
train() client id: f_00004-9-3 loss: 0.900751  [  128/  306]
train() client id: f_00004-9-4 loss: 0.931921  [  160/  306]
train() client id: f_00004-9-5 loss: 1.070571  [  192/  306]
train() client id: f_00004-9-6 loss: 0.742726  [  224/  306]
train() client id: f_00004-9-7 loss: 0.907691  [  256/  306]
train() client id: f_00004-9-8 loss: 1.015559  [  288/  306]
train() client id: f_00004-10-0 loss: 0.968391  [   32/  306]
train() client id: f_00004-10-1 loss: 0.893493  [   64/  306]
train() client id: f_00004-10-2 loss: 0.911981  [   96/  306]
train() client id: f_00004-10-3 loss: 0.965778  [  128/  306]
train() client id: f_00004-10-4 loss: 0.908909  [  160/  306]
train() client id: f_00004-10-5 loss: 1.063347  [  192/  306]
train() client id: f_00004-10-6 loss: 0.932376  [  224/  306]
train() client id: f_00004-10-7 loss: 0.702657  [  256/  306]
train() client id: f_00004-10-8 loss: 0.918459  [  288/  306]
train() client id: f_00004-11-0 loss: 1.028060  [   32/  306]
train() client id: f_00004-11-1 loss: 0.793868  [   64/  306]
train() client id: f_00004-11-2 loss: 0.786438  [   96/  306]
train() client id: f_00004-11-3 loss: 0.891350  [  128/  306]
train() client id: f_00004-11-4 loss: 0.892893  [  160/  306]
train() client id: f_00004-11-5 loss: 0.853756  [  192/  306]
train() client id: f_00004-11-6 loss: 0.986349  [  224/  306]
train() client id: f_00004-11-7 loss: 1.040010  [  256/  306]
train() client id: f_00004-11-8 loss: 0.909726  [  288/  306]
train() client id: f_00004-12-0 loss: 0.929790  [   32/  306]
train() client id: f_00004-12-1 loss: 1.019171  [   64/  306]
train() client id: f_00004-12-2 loss: 0.936665  [   96/  306]
train() client id: f_00004-12-3 loss: 0.920119  [  128/  306]
train() client id: f_00004-12-4 loss: 0.849682  [  160/  306]
train() client id: f_00004-12-5 loss: 0.853924  [  192/  306]
train() client id: f_00004-12-6 loss: 0.885966  [  224/  306]
train() client id: f_00004-12-7 loss: 0.967538  [  256/  306]
train() client id: f_00004-12-8 loss: 0.831833  [  288/  306]
train() client id: f_00004-13-0 loss: 0.906866  [   32/  306]
train() client id: f_00004-13-1 loss: 0.987618  [   64/  306]
train() client id: f_00004-13-2 loss: 0.929113  [   96/  306]
train() client id: f_00004-13-3 loss: 0.812168  [  128/  306]
train() client id: f_00004-13-4 loss: 1.008978  [  160/  306]
train() client id: f_00004-13-5 loss: 0.812488  [  192/  306]
train() client id: f_00004-13-6 loss: 1.007953  [  224/  306]
train() client id: f_00004-13-7 loss: 0.898133  [  256/  306]
train() client id: f_00004-13-8 loss: 0.903349  [  288/  306]
train() client id: f_00004-14-0 loss: 0.926251  [   32/  306]
train() client id: f_00004-14-1 loss: 1.010715  [   64/  306]
train() client id: f_00004-14-2 loss: 0.876147  [   96/  306]
train() client id: f_00004-14-3 loss: 0.944833  [  128/  306]
train() client id: f_00004-14-4 loss: 1.025372  [  160/  306]
train() client id: f_00004-14-5 loss: 0.848841  [  192/  306]
train() client id: f_00004-14-6 loss: 0.852117  [  224/  306]
train() client id: f_00004-14-7 loss: 0.892096  [  256/  306]
train() client id: f_00004-14-8 loss: 0.921358  [  288/  306]
train() client id: f_00004-15-0 loss: 0.965961  [   32/  306]
train() client id: f_00004-15-1 loss: 1.009059  [   64/  306]
train() client id: f_00004-15-2 loss: 0.919911  [   96/  306]
train() client id: f_00004-15-3 loss: 0.889484  [  128/  306]
train() client id: f_00004-15-4 loss: 0.911336  [  160/  306]
train() client id: f_00004-15-5 loss: 0.888697  [  192/  306]
train() client id: f_00004-15-6 loss: 0.882006  [  224/  306]
train() client id: f_00004-15-7 loss: 0.914028  [  256/  306]
train() client id: f_00004-15-8 loss: 0.868152  [  288/  306]
train() client id: f_00004-16-0 loss: 0.919122  [   32/  306]
train() client id: f_00004-16-1 loss: 0.909602  [   64/  306]
train() client id: f_00004-16-2 loss: 0.892023  [   96/  306]
train() client id: f_00004-16-3 loss: 1.021273  [  128/  306]
train() client id: f_00004-16-4 loss: 0.936710  [  160/  306]
train() client id: f_00004-16-5 loss: 0.792471  [  192/  306]
train() client id: f_00004-16-6 loss: 0.848576  [  224/  306]
train() client id: f_00004-16-7 loss: 0.901488  [  256/  306]
train() client id: f_00004-16-8 loss: 1.080865  [  288/  306]
train() client id: f_00005-0-0 loss: 0.645071  [   32/  146]
train() client id: f_00005-0-1 loss: 0.710494  [   64/  146]
train() client id: f_00005-0-2 loss: 0.600030  [   96/  146]
train() client id: f_00005-0-3 loss: 0.713291  [  128/  146]
train() client id: f_00005-1-0 loss: 0.742627  [   32/  146]
train() client id: f_00005-1-1 loss: 0.708491  [   64/  146]
train() client id: f_00005-1-2 loss: 0.590142  [   96/  146]
train() client id: f_00005-1-3 loss: 0.566876  [  128/  146]
train() client id: f_00005-2-0 loss: 0.770912  [   32/  146]
train() client id: f_00005-2-1 loss: 0.533522  [   64/  146]
train() client id: f_00005-2-2 loss: 0.644662  [   96/  146]
train() client id: f_00005-2-3 loss: 0.707122  [  128/  146]
train() client id: f_00005-3-0 loss: 0.765551  [   32/  146]
train() client id: f_00005-3-1 loss: 0.720186  [   64/  146]
train() client id: f_00005-3-2 loss: 0.755818  [   96/  146]
train() client id: f_00005-3-3 loss: 0.501509  [  128/  146]
train() client id: f_00005-4-0 loss: 0.710432  [   32/  146]
train() client id: f_00005-4-1 loss: 0.884583  [   64/  146]
train() client id: f_00005-4-2 loss: 0.573261  [   96/  146]
train() client id: f_00005-4-3 loss: 0.613786  [  128/  146]
train() client id: f_00005-5-0 loss: 0.817657  [   32/  146]
train() client id: f_00005-5-1 loss: 0.749500  [   64/  146]
train() client id: f_00005-5-2 loss: 0.543886  [   96/  146]
train() client id: f_00005-5-3 loss: 0.512765  [  128/  146]
train() client id: f_00005-6-0 loss: 0.546046  [   32/  146]
train() client id: f_00005-6-1 loss: 1.005048  [   64/  146]
train() client id: f_00005-6-2 loss: 0.634817  [   96/  146]
train() client id: f_00005-6-3 loss: 0.713923  [  128/  146]
train() client id: f_00005-7-0 loss: 0.486978  [   32/  146]
train() client id: f_00005-7-1 loss: 0.839647  [   64/  146]
train() client id: f_00005-7-2 loss: 0.733631  [   96/  146]
train() client id: f_00005-7-3 loss: 0.449717  [  128/  146]
train() client id: f_00005-8-0 loss: 0.467315  [   32/  146]
train() client id: f_00005-8-1 loss: 0.877023  [   64/  146]
train() client id: f_00005-8-2 loss: 0.534109  [   96/  146]
train() client id: f_00005-8-3 loss: 0.574513  [  128/  146]
train() client id: f_00005-9-0 loss: 0.665786  [   32/  146]
train() client id: f_00005-9-1 loss: 0.889325  [   64/  146]
train() client id: f_00005-9-2 loss: 0.682745  [   96/  146]
train() client id: f_00005-9-3 loss: 0.676471  [  128/  146]
train() client id: f_00005-10-0 loss: 1.017433  [   32/  146]
train() client id: f_00005-10-1 loss: 0.588471  [   64/  146]
train() client id: f_00005-10-2 loss: 0.706459  [   96/  146]
train() client id: f_00005-10-3 loss: 0.459832  [  128/  146]
train() client id: f_00005-11-0 loss: 0.547601  [   32/  146]
train() client id: f_00005-11-1 loss: 0.765840  [   64/  146]
train() client id: f_00005-11-2 loss: 0.693223  [   96/  146]
train() client id: f_00005-11-3 loss: 0.853728  [  128/  146]
train() client id: f_00005-12-0 loss: 0.719484  [   32/  146]
train() client id: f_00005-12-1 loss: 0.699715  [   64/  146]
train() client id: f_00005-12-2 loss: 0.861882  [   96/  146]
train() client id: f_00005-12-3 loss: 0.366753  [  128/  146]
train() client id: f_00005-13-0 loss: 0.687733  [   32/  146]
train() client id: f_00005-13-1 loss: 0.596397  [   64/  146]
train() client id: f_00005-13-2 loss: 0.859437  [   96/  146]
train() client id: f_00005-13-3 loss: 0.712195  [  128/  146]
train() client id: f_00005-14-0 loss: 0.721538  [   32/  146]
train() client id: f_00005-14-1 loss: 0.682987  [   64/  146]
train() client id: f_00005-14-2 loss: 0.481665  [   96/  146]
train() client id: f_00005-14-3 loss: 1.020016  [  128/  146]
train() client id: f_00005-15-0 loss: 0.855096  [   32/  146]
train() client id: f_00005-15-1 loss: 0.468849  [   64/  146]
train() client id: f_00005-15-2 loss: 0.776483  [   96/  146]
train() client id: f_00005-15-3 loss: 0.588842  [  128/  146]
train() client id: f_00005-16-0 loss: 0.687340  [   32/  146]
train() client id: f_00005-16-1 loss: 0.727210  [   64/  146]
train() client id: f_00005-16-2 loss: 0.773204  [   96/  146]
train() client id: f_00005-16-3 loss: 0.539242  [  128/  146]
train() client id: f_00006-0-0 loss: 0.520629  [   32/   54]
train() client id: f_00006-1-0 loss: 0.495862  [   32/   54]
train() client id: f_00006-2-0 loss: 0.496003  [   32/   54]
train() client id: f_00006-3-0 loss: 0.521554  [   32/   54]
train() client id: f_00006-4-0 loss: 0.482252  [   32/   54]
train() client id: f_00006-5-0 loss: 0.496693  [   32/   54]
train() client id: f_00006-6-0 loss: 0.564002  [   32/   54]
train() client id: f_00006-7-0 loss: 0.507026  [   32/   54]
train() client id: f_00006-8-0 loss: 0.463941  [   32/   54]
train() client id: f_00006-9-0 loss: 0.506052  [   32/   54]
train() client id: f_00006-10-0 loss: 0.461842  [   32/   54]
train() client id: f_00006-11-0 loss: 0.518444  [   32/   54]
train() client id: f_00006-12-0 loss: 0.562607  [   32/   54]
train() client id: f_00006-13-0 loss: 0.504696  [   32/   54]
train() client id: f_00006-14-0 loss: 0.511229  [   32/   54]
train() client id: f_00006-15-0 loss: 0.532386  [   32/   54]
train() client id: f_00006-16-0 loss: 0.552142  [   32/   54]
train() client id: f_00007-0-0 loss: 0.516915  [   32/  179]
train() client id: f_00007-0-1 loss: 0.771385  [   64/  179]
train() client id: f_00007-0-2 loss: 0.720267  [   96/  179]
train() client id: f_00007-0-3 loss: 0.400542  [  128/  179]
train() client id: f_00007-0-4 loss: 0.637683  [  160/  179]
train() client id: f_00007-1-0 loss: 0.518392  [   32/  179]
train() client id: f_00007-1-1 loss: 0.504167  [   64/  179]
train() client id: f_00007-1-2 loss: 0.639548  [   96/  179]
train() client id: f_00007-1-3 loss: 0.690594  [  128/  179]
train() client id: f_00007-1-4 loss: 0.518767  [  160/  179]
train() client id: f_00007-2-0 loss: 0.728163  [   32/  179]
train() client id: f_00007-2-1 loss: 0.547623  [   64/  179]
train() client id: f_00007-2-2 loss: 0.411782  [   96/  179]
train() client id: f_00007-2-3 loss: 0.502694  [  128/  179]
train() client id: f_00007-2-4 loss: 0.664718  [  160/  179]
train() client id: f_00007-3-0 loss: 0.597201  [   32/  179]
train() client id: f_00007-3-1 loss: 0.440811  [   64/  179]
train() client id: f_00007-3-2 loss: 0.572963  [   96/  179]
train() client id: f_00007-3-3 loss: 0.373373  [  128/  179]
train() client id: f_00007-3-4 loss: 0.610305  [  160/  179]
train() client id: f_00007-4-0 loss: 0.592269  [   32/  179]
train() client id: f_00007-4-1 loss: 0.553409  [   64/  179]
train() client id: f_00007-4-2 loss: 0.747003  [   96/  179]
train() client id: f_00007-4-3 loss: 0.423556  [  128/  179]
train() client id: f_00007-4-4 loss: 0.459560  [  160/  179]
train() client id: f_00007-5-0 loss: 0.624742  [   32/  179]
train() client id: f_00007-5-1 loss: 0.547472  [   64/  179]
train() client id: f_00007-5-2 loss: 0.656051  [   96/  179]
train() client id: f_00007-5-3 loss: 0.384316  [  128/  179]
train() client id: f_00007-5-4 loss: 0.373241  [  160/  179]
train() client id: f_00007-6-0 loss: 0.454262  [   32/  179]
train() client id: f_00007-6-1 loss: 0.514353  [   64/  179]
train() client id: f_00007-6-2 loss: 0.526923  [   96/  179]
train() client id: f_00007-6-3 loss: 0.566434  [  128/  179]
train() client id: f_00007-6-4 loss: 0.387832  [  160/  179]
train() client id: f_00007-7-0 loss: 0.589568  [   32/  179]
train() client id: f_00007-7-1 loss: 0.608763  [   64/  179]
train() client id: f_00007-7-2 loss: 0.377681  [   96/  179]
train() client id: f_00007-7-3 loss: 0.528347  [  128/  179]
train() client id: f_00007-7-4 loss: 0.556446  [  160/  179]
train() client id: f_00007-8-0 loss: 0.627640  [   32/  179]
train() client id: f_00007-8-1 loss: 0.724699  [   64/  179]
train() client id: f_00007-8-2 loss: 0.351036  [   96/  179]
train() client id: f_00007-8-3 loss: 0.319061  [  128/  179]
train() client id: f_00007-8-4 loss: 0.508858  [  160/  179]
train() client id: f_00007-9-0 loss: 0.551758  [   32/  179]
train() client id: f_00007-9-1 loss: 0.508759  [   64/  179]
train() client id: f_00007-9-2 loss: 0.624787  [   96/  179]
train() client id: f_00007-9-3 loss: 0.446548  [  128/  179]
train() client id: f_00007-9-4 loss: 0.530410  [  160/  179]
train() client id: f_00007-10-0 loss: 0.543778  [   32/  179]
train() client id: f_00007-10-1 loss: 0.434972  [   64/  179]
train() client id: f_00007-10-2 loss: 0.361659  [   96/  179]
train() client id: f_00007-10-3 loss: 0.712532  [  128/  179]
train() client id: f_00007-10-4 loss: 0.465606  [  160/  179]
train() client id: f_00007-11-0 loss: 0.432711  [   32/  179]
train() client id: f_00007-11-1 loss: 0.362305  [   64/  179]
train() client id: f_00007-11-2 loss: 0.742049  [   96/  179]
train() client id: f_00007-11-3 loss: 0.535881  [  128/  179]
train() client id: f_00007-11-4 loss: 0.376638  [  160/  179]
train() client id: f_00007-12-0 loss: 0.462609  [   32/  179]
train() client id: f_00007-12-1 loss: 0.436731  [   64/  179]
train() client id: f_00007-12-2 loss: 0.621379  [   96/  179]
train() client id: f_00007-12-3 loss: 0.517369  [  128/  179]
train() client id: f_00007-12-4 loss: 0.560910  [  160/  179]
train() client id: f_00007-13-0 loss: 0.446429  [   32/  179]
train() client id: f_00007-13-1 loss: 0.447107  [   64/  179]
train() client id: f_00007-13-2 loss: 0.589635  [   96/  179]
train() client id: f_00007-13-3 loss: 0.475688  [  128/  179]
train() client id: f_00007-13-4 loss: 0.661434  [  160/  179]
train() client id: f_00007-14-0 loss: 0.609996  [   32/  179]
train() client id: f_00007-14-1 loss: 0.370601  [   64/  179]
train() client id: f_00007-14-2 loss: 0.739425  [   96/  179]
train() client id: f_00007-14-3 loss: 0.378523  [  128/  179]
train() client id: f_00007-14-4 loss: 0.432789  [  160/  179]
train() client id: f_00007-15-0 loss: 0.543428  [   32/  179]
train() client id: f_00007-15-1 loss: 0.364399  [   64/  179]
train() client id: f_00007-15-2 loss: 0.342045  [   96/  179]
train() client id: f_00007-15-3 loss: 0.528190  [  128/  179]
train() client id: f_00007-15-4 loss: 0.723207  [  160/  179]
train() client id: f_00007-16-0 loss: 0.652513  [   32/  179]
train() client id: f_00007-16-1 loss: 0.594989  [   64/  179]
train() client id: f_00007-16-2 loss: 0.471857  [   96/  179]
train() client id: f_00007-16-3 loss: 0.468306  [  128/  179]
train() client id: f_00007-16-4 loss: 0.316023  [  160/  179]
train() client id: f_00008-0-0 loss: 0.600013  [   32/  130]
train() client id: f_00008-0-1 loss: 0.642111  [   64/  130]
train() client id: f_00008-0-2 loss: 0.494377  [   96/  130]
train() client id: f_00008-0-3 loss: 0.637645  [  128/  130]
train() client id: f_00008-1-0 loss: 0.541048  [   32/  130]
train() client id: f_00008-1-1 loss: 0.581226  [   64/  130]
train() client id: f_00008-1-2 loss: 0.644530  [   96/  130]
train() client id: f_00008-1-3 loss: 0.596002  [  128/  130]
train() client id: f_00008-2-0 loss: 0.690330  [   32/  130]
train() client id: f_00008-2-1 loss: 0.552994  [   64/  130]
train() client id: f_00008-2-2 loss: 0.542257  [   96/  130]
train() client id: f_00008-2-3 loss: 0.573089  [  128/  130]
train() client id: f_00008-3-0 loss: 0.739164  [   32/  130]
train() client id: f_00008-3-1 loss: 0.472742  [   64/  130]
train() client id: f_00008-3-2 loss: 0.606824  [   96/  130]
train() client id: f_00008-3-3 loss: 0.558747  [  128/  130]
train() client id: f_00008-4-0 loss: 0.564402  [   32/  130]
train() client id: f_00008-4-1 loss: 0.596744  [   64/  130]
train() client id: f_00008-4-2 loss: 0.530560  [   96/  130]
train() client id: f_00008-4-3 loss: 0.692248  [  128/  130]
train() client id: f_00008-5-0 loss: 0.548523  [   32/  130]
train() client id: f_00008-5-1 loss: 0.590147  [   64/  130]
train() client id: f_00008-5-2 loss: 0.598980  [   96/  130]
train() client id: f_00008-5-3 loss: 0.650604  [  128/  130]
train() client id: f_00008-6-0 loss: 0.552373  [   32/  130]
train() client id: f_00008-6-1 loss: 0.746043  [   64/  130]
train() client id: f_00008-6-2 loss: 0.529430  [   96/  130]
train() client id: f_00008-6-3 loss: 0.566776  [  128/  130]
train() client id: f_00008-7-0 loss: 0.569388  [   32/  130]
train() client id: f_00008-7-1 loss: 0.528395  [   64/  130]
train() client id: f_00008-7-2 loss: 0.672112  [   96/  130]
train() client id: f_00008-7-3 loss: 0.622090  [  128/  130]
train() client id: f_00008-8-0 loss: 0.539421  [   32/  130]
train() client id: f_00008-8-1 loss: 0.525334  [   64/  130]
train() client id: f_00008-8-2 loss: 0.521940  [   96/  130]
train() client id: f_00008-8-3 loss: 0.746569  [  128/  130]
train() client id: f_00008-9-0 loss: 0.522484  [   32/  130]
train() client id: f_00008-9-1 loss: 0.590012  [   64/  130]
train() client id: f_00008-9-2 loss: 0.703422  [   96/  130]
train() client id: f_00008-9-3 loss: 0.549655  [  128/  130]
train() client id: f_00008-10-0 loss: 0.623010  [   32/  130]
train() client id: f_00008-10-1 loss: 0.513273  [   64/  130]
train() client id: f_00008-10-2 loss: 0.703927  [   96/  130]
train() client id: f_00008-10-3 loss: 0.546972  [  128/  130]
train() client id: f_00008-11-0 loss: 0.693659  [   32/  130]
train() client id: f_00008-11-1 loss: 0.526547  [   64/  130]
train() client id: f_00008-11-2 loss: 0.509828  [   96/  130]
train() client id: f_00008-11-3 loss: 0.635046  [  128/  130]
train() client id: f_00008-12-0 loss: 0.618000  [   32/  130]
train() client id: f_00008-12-1 loss: 0.610001  [   64/  130]
train() client id: f_00008-12-2 loss: 0.578863  [   96/  130]
train() client id: f_00008-12-3 loss: 0.584671  [  128/  130]
train() client id: f_00008-13-0 loss: 0.524454  [   32/  130]
train() client id: f_00008-13-1 loss: 0.656546  [   64/  130]
train() client id: f_00008-13-2 loss: 0.606619  [   96/  130]
train() client id: f_00008-13-3 loss: 0.549168  [  128/  130]
train() client id: f_00008-14-0 loss: 0.599966  [   32/  130]
train() client id: f_00008-14-1 loss: 0.660230  [   64/  130]
train() client id: f_00008-14-2 loss: 0.578126  [   96/  130]
train() client id: f_00008-14-3 loss: 0.560162  [  128/  130]
train() client id: f_00008-15-0 loss: 0.601138  [   32/  130]
train() client id: f_00008-15-1 loss: 0.546292  [   64/  130]
train() client id: f_00008-15-2 loss: 0.625878  [   96/  130]
train() client id: f_00008-15-3 loss: 0.628561  [  128/  130]
train() client id: f_00008-16-0 loss: 0.522902  [   32/  130]
train() client id: f_00008-16-1 loss: 0.663064  [   64/  130]
train() client id: f_00008-16-2 loss: 0.505222  [   96/  130]
train() client id: f_00008-16-3 loss: 0.710163  [  128/  130]
train() client id: f_00009-0-0 loss: 0.967767  [   32/  118]
train() client id: f_00009-0-1 loss: 1.042151  [   64/  118]
train() client id: f_00009-0-2 loss: 1.150999  [   96/  118]
train() client id: f_00009-1-0 loss: 0.977834  [   32/  118]
train() client id: f_00009-1-1 loss: 0.935192  [   64/  118]
train() client id: f_00009-1-2 loss: 1.080342  [   96/  118]
train() client id: f_00009-2-0 loss: 1.065136  [   32/  118]
train() client id: f_00009-2-1 loss: 0.863303  [   64/  118]
train() client id: f_00009-2-2 loss: 1.035652  [   96/  118]
train() client id: f_00009-3-0 loss: 0.914189  [   32/  118]
train() client id: f_00009-3-1 loss: 0.901181  [   64/  118]
train() client id: f_00009-3-2 loss: 1.000203  [   96/  118]
train() client id: f_00009-4-0 loss: 0.813466  [   32/  118]
train() client id: f_00009-4-1 loss: 0.733759  [   64/  118]
train() client id: f_00009-4-2 loss: 0.985538  [   96/  118]
train() client id: f_00009-5-0 loss: 0.830306  [   32/  118]
train() client id: f_00009-5-1 loss: 0.778751  [   64/  118]
train() client id: f_00009-5-2 loss: 0.868773  [   96/  118]
train() client id: f_00009-6-0 loss: 0.873326  [   32/  118]
train() client id: f_00009-6-1 loss: 0.889125  [   64/  118]
train() client id: f_00009-6-2 loss: 0.723221  [   96/  118]
train() client id: f_00009-7-0 loss: 0.684323  [   32/  118]
train() client id: f_00009-7-1 loss: 0.924929  [   64/  118]
train() client id: f_00009-7-2 loss: 0.738622  [   96/  118]
train() client id: f_00009-8-0 loss: 0.869562  [   32/  118]
train() client id: f_00009-8-1 loss: 0.689720  [   64/  118]
train() client id: f_00009-8-2 loss: 0.834813  [   96/  118]
train() client id: f_00009-9-0 loss: 0.806291  [   32/  118]
train() client id: f_00009-9-1 loss: 0.948893  [   64/  118]
train() client id: f_00009-9-2 loss: 0.753365  [   96/  118]
train() client id: f_00009-10-0 loss: 0.722461  [   32/  118]
train() client id: f_00009-10-1 loss: 0.913519  [   64/  118]
train() client id: f_00009-10-2 loss: 0.700364  [   96/  118]
train() client id: f_00009-11-0 loss: 0.753828  [   32/  118]
train() client id: f_00009-11-1 loss: 0.711927  [   64/  118]
train() client id: f_00009-11-2 loss: 0.612219  [   96/  118]
train() client id: f_00009-12-0 loss: 0.721037  [   32/  118]
train() client id: f_00009-12-1 loss: 0.838158  [   64/  118]
train() client id: f_00009-12-2 loss: 0.806103  [   96/  118]
train() client id: f_00009-13-0 loss: 0.903862  [   32/  118]
train() client id: f_00009-13-1 loss: 0.697217  [   64/  118]
train() client id: f_00009-13-2 loss: 0.627672  [   96/  118]
train() client id: f_00009-14-0 loss: 0.709522  [   32/  118]
train() client id: f_00009-14-1 loss: 0.630075  [   64/  118]
train() client id: f_00009-14-2 loss: 0.652394  [   96/  118]
train() client id: f_00009-15-0 loss: 0.719159  [   32/  118]
train() client id: f_00009-15-1 loss: 0.654520  [   64/  118]
train() client id: f_00009-15-2 loss: 0.709718  [   96/  118]
train() client id: f_00009-16-0 loss: 0.880045  [   32/  118]
train() client id: f_00009-16-1 loss: 0.789261  [   64/  118]
train() client id: f_00009-16-2 loss: 0.628169  [   96/  118]
At round 53 accuracy: 0.6472148541114059
At round 53 training accuracy: 0.5881958417169685
At round 53 training loss: 0.8355209830408825
update_location
xs = 8.927491 386.223621 5.882650 0.934260 -302.581990 -150.230757 -110.849135 -5.143845 -325.120581 20.134486 
ys = -377.390647 7.291448 275.684448 -97.290817 -9.642386 0.794442 -1.381692 271.628436 25.881276 -812.232496 
xs mean: -47.182379970521225
ys mean: -71.66579882624052
dists_uav = 390.516838 399.026128 293.319826 139.521955 318.824146 180.471360 149.296483 289.496920 341.135212 818.612866 
uav_gains = -121.043598 -121.367518 -115.162748 -103.617790 -117.200216 -106.457554 -104.355516 -114.829134 -118.667443 -129.795899 
uav_gains_db_mean: -115.24974166334843
dists_bs = 582.206792 585.745773 207.016645 324.183826 224.432562 175.957642 187.685468 195.417932 211.595327 1006.332683 
bs_gains = -116.989223 -117.062916 -104.415207 -109.869217 -105.397463 -102.438487 -103.223117 -103.714063 -104.681229 -123.643823 
bs_gains_db_mean: -109.1434744522521
Round 54
-------------------------------
ene_coms = [0.03203037 0.03353602 0.0077115  0.0074131  0.00811285 0.00859401
 0.00768835 0.00744792 0.00781633 0.04753909]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.03094969 8.14683904 3.73402971 1.34122607 9.1078198  4.39176765
 1.66847108 5.35421214 3.91208371 3.94663283]
obj_prev = 45.63403173614185
eta_min = 2.060416592642643e-24	eta_max = 0.8297529624068343
af = 9.457122799293558	bf = 1.6260314582455018	zeta = 10.402835079222914	eta = 0.9090909090909091
af = 9.457122799293558	bf = 1.6260314582455018	zeta = 27.801306859455256	eta = 0.34016828227185225
af = 9.457122799293558	bf = 1.6260314582455018	zeta = 17.919398174625396	eta = 0.5277589518985761
af = 9.457122799293558	bf = 1.6260314582455018	zeta = 16.24218497113109	eta = 0.5822568094195872
af = 9.457122799293558	bf = 1.6260314582455018	zeta = 16.136364787147286	eta = 0.5860751739342317
af = 9.457122799293558	bf = 1.6260314582455018	zeta = 16.135877766754785	eta = 0.5860928631213569
af = 9.457122799293558	bf = 1.6260314582455018	zeta = 16.135877756347973	eta = 0.5860928634993566
eta = 0.5860928634993566
ene_coms = [0.03203037 0.03353602 0.0077115  0.0074131  0.00811285 0.00859401
 0.00768835 0.00744792 0.00781633 0.04753909]
ene_comp = [0.04408726 0.09272321 0.04338746 0.01504565 0.10706906 0.05108524
 0.01889454 0.0626319  0.04548686 0.04128807]
ene_total = [1.78109745 2.95437445 1.19567866 0.52551847 2.69517321 1.39645108
 0.62202042 1.63981703 1.24725584 2.07849113]
ti_comp = [0.84423959 0.82918309 1.08742833 1.09041234 1.08341483 1.07860324
 1.08765979 1.09006411 1.08638005 0.68915242]
ti_coms = [0.32030372 0.33536022 0.07711498 0.07413097 0.08112848 0.08594007
 0.07688352 0.0744792  0.07816326 0.47539089]
t_total = [27.24946976 27.24946976 27.24946976 27.24946976 27.24946976 27.24946976
 27.24946976 27.24946976 27.24946976 27.24946976]
ene_coms = [0.03203037 0.03353602 0.0077115  0.0074131  0.00811285 0.00859401
 0.00768835 0.00744792 0.00781633 0.04753909]
ene_comp = [7.51428947e-06 7.24676966e-05 4.31689495e-06 1.79032371e-07
 6.53555450e-05 7.16214044e-06 3.56371402e-07 1.29229813e-05
 4.98395606e-06 9.26239195e-06]
ene_total = [0.74966334 0.78641431 0.18054448 0.17346529 0.19136405 0.20126112
 0.17991021 0.17457831 0.18301299 1.11259696]
optimize_network iter = 0 obj = 3.9328110750897487
eta = 0.5860928634993566
freqs = [26110633.05842388 55912383.37370755 19949573.31227921  6899065.48494825
 49412772.3116814  23681199.80050741  8685868.62611697 28728540.090294
 20935056.61805825 29955687.22303228]
eta_min = 0.5860928634993576	eta_max = 0.5860928634993536
af = 0.003344916843624937	bf = 1.6260314582455018	zeta = 0.003679408527987431	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03203037 0.03353602 0.0077115  0.0074131  0.00811285 0.00859401
 0.00768835 0.00744792 0.00781633 0.04753909]
ene_comp = [1.34048113e-06 1.29275802e-05 7.70094931e-07 3.19377522e-08
 1.16588369e-05 1.27766094e-06 6.35734279e-08 2.30534274e-06
 8.89092586e-07 1.65232677e-06]
ene_total = [3.41254333 3.57418425 0.82163729 0.78976809 0.86555575 0.91571063
 0.81909614 0.79372013 0.83281797 5.06481931]
ti_comp = [0.84423959 0.82918309 1.08742833 1.09041234 1.08341483 1.07860324
 1.08765979 1.09006411 1.08638005 0.68915242]
ti_coms = [0.32030372 0.33536022 0.07711498 0.07413097 0.08112848 0.08594007
 0.07688352 0.0744792  0.07816326 0.47539089]
t_total = [27.24946976 27.24946976 27.24946976 27.24946976 27.24946976 27.24946976
 27.24946976 27.24946976 27.24946976 27.24946976]
ene_coms = [0.03203037 0.03353602 0.0077115  0.0074131  0.00811285 0.00859401
 0.00768835 0.00744792 0.00781633 0.04753909]
ene_comp = [7.51428947e-06 7.24676966e-05 4.31689495e-06 1.79032371e-07
 6.53555450e-05 7.16214044e-06 3.56371402e-07 1.29229813e-05
 4.98395606e-06 9.26239195e-06]
ene_total = [0.74966334 0.78641431 0.18054448 0.17346529 0.19136405 0.20126112
 0.17991021 0.17457831 0.18301299 1.11259696]
optimize_network iter = 1 obj = 3.9328110750897203
eta = 0.5860928634993536
freqs = [26110633.05842387 55912383.37370752 19949573.31227924  6899065.48494826
 49412772.31168149 23681199.80050746  8685868.62611699 28728540.09029405
 20935056.61805829 29955687.22303221]
Done!
ene_coms = [0.03203037 0.03353602 0.0077115  0.0074131  0.00811285 0.00859401
 0.00768835 0.00744792 0.00781633 0.04753909]
ene_comp = [7.30170485e-06 7.04175337e-05 4.19476691e-06 1.73967417e-07
 6.35065900e-05 6.95951837e-06 3.46289400e-07 1.25573810e-05
 4.84295638e-06 9.00035227e-06]
ene_total = [0.03203767 0.03360644 0.00771569 0.00741327 0.00817636 0.00860097
 0.0076887  0.00746048 0.00782117 0.04754809]
At round 54 energy consumption: 0.16806883292839547
At round 54 eta: 0.5860928634993536
At round 54 a_n: 9.685127119911243
At round 54 local rounds: 17.494944474790017
At round 54 global rounds: 23.399275503663898
gradient difference: 0.3667154908180237
train() client id: f_00000-0-0 loss: 1.277703  [   32/  126]
train() client id: f_00000-0-1 loss: 1.180878  [   64/  126]
train() client id: f_00000-0-2 loss: 1.362734  [   96/  126]
train() client id: f_00000-1-0 loss: 1.118810  [   32/  126]
train() client id: f_00000-1-1 loss: 1.236462  [   64/  126]
train() client id: f_00000-1-2 loss: 1.180539  [   96/  126]
train() client id: f_00000-2-0 loss: 1.070915  [   32/  126]
train() client id: f_00000-2-1 loss: 1.102236  [   64/  126]
train() client id: f_00000-2-2 loss: 1.336092  [   96/  126]
train() client id: f_00000-3-0 loss: 1.017825  [   32/  126]
train() client id: f_00000-3-1 loss: 1.164258  [   64/  126]
train() client id: f_00000-3-2 loss: 0.914834  [   96/  126]
train() client id: f_00000-4-0 loss: 0.883205  [   32/  126]
train() client id: f_00000-4-1 loss: 1.034889  [   64/  126]
train() client id: f_00000-4-2 loss: 0.994766  [   96/  126]
train() client id: f_00000-5-0 loss: 0.898687  [   32/  126]
train() client id: f_00000-5-1 loss: 0.996080  [   64/  126]
train() client id: f_00000-5-2 loss: 0.923053  [   96/  126]
train() client id: f_00000-6-0 loss: 0.847965  [   32/  126]
train() client id: f_00000-6-1 loss: 0.943680  [   64/  126]
train() client id: f_00000-6-2 loss: 0.984054  [   96/  126]
train() client id: f_00000-7-0 loss: 0.891076  [   32/  126]
train() client id: f_00000-7-1 loss: 0.916888  [   64/  126]
train() client id: f_00000-7-2 loss: 0.947220  [   96/  126]
train() client id: f_00000-8-0 loss: 0.751105  [   32/  126]
train() client id: f_00000-8-1 loss: 0.918659  [   64/  126]
train() client id: f_00000-8-2 loss: 0.914176  [   96/  126]
train() client id: f_00000-9-0 loss: 0.796798  [   32/  126]
train() client id: f_00000-9-1 loss: 0.814833  [   64/  126]
train() client id: f_00000-9-2 loss: 0.853668  [   96/  126]
train() client id: f_00000-10-0 loss: 0.937171  [   32/  126]
train() client id: f_00000-10-1 loss: 0.876157  [   64/  126]
train() client id: f_00000-10-2 loss: 0.889832  [   96/  126]
train() client id: f_00000-11-0 loss: 0.729895  [   32/  126]
train() client id: f_00000-11-1 loss: 0.750777  [   64/  126]
train() client id: f_00000-11-2 loss: 0.980435  [   96/  126]
train() client id: f_00000-12-0 loss: 0.861220  [   32/  126]
train() client id: f_00000-12-1 loss: 0.801938  [   64/  126]
train() client id: f_00000-12-2 loss: 0.934909  [   96/  126]
train() client id: f_00000-13-0 loss: 0.875011  [   32/  126]
train() client id: f_00000-13-1 loss: 0.797225  [   64/  126]
train() client id: f_00000-13-2 loss: 0.928034  [   96/  126]
train() client id: f_00000-14-0 loss: 0.772695  [   32/  126]
train() client id: f_00000-14-1 loss: 0.954064  [   64/  126]
train() client id: f_00000-14-2 loss: 0.893360  [   96/  126]
train() client id: f_00000-15-0 loss: 0.805712  [   32/  126]
train() client id: f_00000-15-1 loss: 0.929163  [   64/  126]
train() client id: f_00000-15-2 loss: 0.821629  [   96/  126]
train() client id: f_00000-16-0 loss: 0.840255  [   32/  126]
train() client id: f_00000-16-1 loss: 0.945531  [   64/  126]
train() client id: f_00000-16-2 loss: 0.803488  [   96/  126]
train() client id: f_00001-0-0 loss: 0.384381  [   32/  265]
train() client id: f_00001-0-1 loss: 0.372986  [   64/  265]
train() client id: f_00001-0-2 loss: 0.362435  [   96/  265]
train() client id: f_00001-0-3 loss: 0.364947  [  128/  265]
train() client id: f_00001-0-4 loss: 0.349290  [  160/  265]
train() client id: f_00001-0-5 loss: 0.394137  [  192/  265]
train() client id: f_00001-0-6 loss: 0.401878  [  224/  265]
train() client id: f_00001-0-7 loss: 0.257767  [  256/  265]
train() client id: f_00001-1-0 loss: 0.482336  [   32/  265]
train() client id: f_00001-1-1 loss: 0.340025  [   64/  265]
train() client id: f_00001-1-2 loss: 0.275628  [   96/  265]
train() client id: f_00001-1-3 loss: 0.429443  [  128/  265]
train() client id: f_00001-1-4 loss: 0.281253  [  160/  265]
train() client id: f_00001-1-5 loss: 0.382587  [  192/  265]
train() client id: f_00001-1-6 loss: 0.360111  [  224/  265]
train() client id: f_00001-1-7 loss: 0.291315  [  256/  265]
train() client id: f_00001-2-0 loss: 0.314802  [   32/  265]
train() client id: f_00001-2-1 loss: 0.433747  [   64/  265]
train() client id: f_00001-2-2 loss: 0.401600  [   96/  265]
train() client id: f_00001-2-3 loss: 0.281979  [  128/  265]
train() client id: f_00001-2-4 loss: 0.345263  [  160/  265]
train() client id: f_00001-2-5 loss: 0.338730  [  192/  265]
train() client id: f_00001-2-6 loss: 0.318916  [  224/  265]
train() client id: f_00001-2-7 loss: 0.342910  [  256/  265]
train() client id: f_00001-3-0 loss: 0.298894  [   32/  265]
train() client id: f_00001-3-1 loss: 0.369113  [   64/  265]
train() client id: f_00001-3-2 loss: 0.424301  [   96/  265]
train() client id: f_00001-3-3 loss: 0.455702  [  128/  265]
train() client id: f_00001-3-4 loss: 0.314834  [  160/  265]
train() client id: f_00001-3-5 loss: 0.277494  [  192/  265]
train() client id: f_00001-3-6 loss: 0.309788  [  224/  265]
train() client id: f_00001-3-7 loss: 0.257779  [  256/  265]
train() client id: f_00001-4-0 loss: 0.347572  [   32/  265]
train() client id: f_00001-4-1 loss: 0.270110  [   64/  265]
train() client id: f_00001-4-2 loss: 0.398426  [   96/  265]
train() client id: f_00001-4-3 loss: 0.313454  [  128/  265]
train() client id: f_00001-4-4 loss: 0.331156  [  160/  265]
train() client id: f_00001-4-5 loss: 0.268501  [  192/  265]
train() client id: f_00001-4-6 loss: 0.381523  [  224/  265]
train() client id: f_00001-4-7 loss: 0.329888  [  256/  265]
train() client id: f_00001-5-0 loss: 0.255465  [   32/  265]
train() client id: f_00001-5-1 loss: 0.341497  [   64/  265]
train() client id: f_00001-5-2 loss: 0.335769  [   96/  265]
train() client id: f_00001-5-3 loss: 0.417063  [  128/  265]
train() client id: f_00001-5-4 loss: 0.283034  [  160/  265]
train() client id: f_00001-5-5 loss: 0.337036  [  192/  265]
train() client id: f_00001-5-6 loss: 0.299277  [  224/  265]
train() client id: f_00001-5-7 loss: 0.305081  [  256/  265]
train() client id: f_00001-6-0 loss: 0.254529  [   32/  265]
train() client id: f_00001-6-1 loss: 0.293076  [   64/  265]
train() client id: f_00001-6-2 loss: 0.483042  [   96/  265]
train() client id: f_00001-6-3 loss: 0.266319  [  128/  265]
train() client id: f_00001-6-4 loss: 0.240812  [  160/  265]
train() client id: f_00001-6-5 loss: 0.230970  [  192/  265]
train() client id: f_00001-6-6 loss: 0.213433  [  224/  265]
train() client id: f_00001-6-7 loss: 0.546886  [  256/  265]
train() client id: f_00001-7-0 loss: 0.419849  [   32/  265]
train() client id: f_00001-7-1 loss: 0.271691  [   64/  265]
train() client id: f_00001-7-2 loss: 0.280098  [   96/  265]
train() client id: f_00001-7-3 loss: 0.465137  [  128/  265]
train() client id: f_00001-7-4 loss: 0.253684  [  160/  265]
train() client id: f_00001-7-5 loss: 0.297151  [  192/  265]
train() client id: f_00001-7-6 loss: 0.226684  [  224/  265]
train() client id: f_00001-7-7 loss: 0.312296  [  256/  265]
train() client id: f_00001-8-0 loss: 0.248402  [   32/  265]
train() client id: f_00001-8-1 loss: 0.343015  [   64/  265]
train() client id: f_00001-8-2 loss: 0.398096  [   96/  265]
train() client id: f_00001-8-3 loss: 0.282331  [  128/  265]
train() client id: f_00001-8-4 loss: 0.377756  [  160/  265]
train() client id: f_00001-8-5 loss: 0.249221  [  192/  265]
train() client id: f_00001-8-6 loss: 0.314894  [  224/  265]
train() client id: f_00001-8-7 loss: 0.281532  [  256/  265]
train() client id: f_00001-9-0 loss: 0.323356  [   32/  265]
train() client id: f_00001-9-1 loss: 0.225117  [   64/  265]
train() client id: f_00001-9-2 loss: 0.330747  [   96/  265]
train() client id: f_00001-9-3 loss: 0.377673  [  128/  265]
train() client id: f_00001-9-4 loss: 0.336300  [  160/  265]
train() client id: f_00001-9-5 loss: 0.455198  [  192/  265]
train() client id: f_00001-9-6 loss: 0.219577  [  224/  265]
train() client id: f_00001-9-7 loss: 0.239440  [  256/  265]
train() client id: f_00001-10-0 loss: 0.230723  [   32/  265]
train() client id: f_00001-10-1 loss: 0.207857  [   64/  265]
train() client id: f_00001-10-2 loss: 0.283130  [   96/  265]
train() client id: f_00001-10-3 loss: 0.322181  [  128/  265]
train() client id: f_00001-10-4 loss: 0.345190  [  160/  265]
train() client id: f_00001-10-5 loss: 0.383626  [  192/  265]
train() client id: f_00001-10-6 loss: 0.379171  [  224/  265]
train() client id: f_00001-10-7 loss: 0.336460  [  256/  265]
train() client id: f_00001-11-0 loss: 0.321683  [   32/  265]
train() client id: f_00001-11-1 loss: 0.297924  [   64/  265]
train() client id: f_00001-11-2 loss: 0.215539  [   96/  265]
train() client id: f_00001-11-3 loss: 0.283592  [  128/  265]
train() client id: f_00001-11-4 loss: 0.299615  [  160/  265]
train() client id: f_00001-11-5 loss: 0.364903  [  192/  265]
train() client id: f_00001-11-6 loss: 0.446696  [  224/  265]
train() client id: f_00001-11-7 loss: 0.209625  [  256/  265]
train() client id: f_00001-12-0 loss: 0.302583  [   32/  265]
train() client id: f_00001-12-1 loss: 0.255070  [   64/  265]
train() client id: f_00001-12-2 loss: 0.260613  [   96/  265]
train() client id: f_00001-12-3 loss: 0.287647  [  128/  265]
train() client id: f_00001-12-4 loss: 0.443370  [  160/  265]
train() client id: f_00001-12-5 loss: 0.390455  [  192/  265]
train() client id: f_00001-12-6 loss: 0.360111  [  224/  265]
train() client id: f_00001-12-7 loss: 0.232389  [  256/  265]
train() client id: f_00001-13-0 loss: 0.420420  [   32/  265]
train() client id: f_00001-13-1 loss: 0.285227  [   64/  265]
train() client id: f_00001-13-2 loss: 0.203695  [   96/  265]
train() client id: f_00001-13-3 loss: 0.395664  [  128/  265]
train() client id: f_00001-13-4 loss: 0.441141  [  160/  265]
train() client id: f_00001-13-5 loss: 0.231027  [  192/  265]
train() client id: f_00001-13-6 loss: 0.206149  [  224/  265]
train() client id: f_00001-13-7 loss: 0.275213  [  256/  265]
train() client id: f_00001-14-0 loss: 0.289613  [   32/  265]
train() client id: f_00001-14-1 loss: 0.442669  [   64/  265]
train() client id: f_00001-14-2 loss: 0.382017  [   96/  265]
train() client id: f_00001-14-3 loss: 0.222927  [  128/  265]
train() client id: f_00001-14-4 loss: 0.337072  [  160/  265]
train() client id: f_00001-14-5 loss: 0.238737  [  192/  265]
train() client id: f_00001-14-6 loss: 0.293761  [  224/  265]
train() client id: f_00001-14-7 loss: 0.299072  [  256/  265]
train() client id: f_00001-15-0 loss: 0.253773  [   32/  265]
train() client id: f_00001-15-1 loss: 0.216349  [   64/  265]
train() client id: f_00001-15-2 loss: 0.339793  [   96/  265]
train() client id: f_00001-15-3 loss: 0.224827  [  128/  265]
train() client id: f_00001-15-4 loss: 0.479682  [  160/  265]
train() client id: f_00001-15-5 loss: 0.355347  [  192/  265]
train() client id: f_00001-15-6 loss: 0.389197  [  224/  265]
train() client id: f_00001-15-7 loss: 0.250162  [  256/  265]
train() client id: f_00001-16-0 loss: 0.266608  [   32/  265]
train() client id: f_00001-16-1 loss: 0.216168  [   64/  265]
train() client id: f_00001-16-2 loss: 0.206312  [   96/  265]
train() client id: f_00001-16-3 loss: 0.444561  [  128/  265]
train() client id: f_00001-16-4 loss: 0.270134  [  160/  265]
train() client id: f_00001-16-5 loss: 0.437222  [  192/  265]
train() client id: f_00001-16-6 loss: 0.220366  [  224/  265]
train() client id: f_00001-16-7 loss: 0.339629  [  256/  265]
train() client id: f_00002-0-0 loss: 1.223610  [   32/  124]
train() client id: f_00002-0-1 loss: 1.283862  [   64/  124]
train() client id: f_00002-0-2 loss: 0.961582  [   96/  124]
train() client id: f_00002-1-0 loss: 1.150985  [   32/  124]
train() client id: f_00002-1-1 loss: 1.049744  [   64/  124]
train() client id: f_00002-1-2 loss: 1.082274  [   96/  124]
train() client id: f_00002-2-0 loss: 1.240986  [   32/  124]
train() client id: f_00002-2-1 loss: 1.001689  [   64/  124]
train() client id: f_00002-2-2 loss: 0.961285  [   96/  124]
train() client id: f_00002-3-0 loss: 1.044683  [   32/  124]
train() client id: f_00002-3-1 loss: 0.989267  [   64/  124]
train() client id: f_00002-3-2 loss: 0.958532  [   96/  124]
train() client id: f_00002-4-0 loss: 0.821257  [   32/  124]
train() client id: f_00002-4-1 loss: 0.919726  [   64/  124]
train() client id: f_00002-4-2 loss: 1.090963  [   96/  124]
train() client id: f_00002-5-0 loss: 0.939538  [   32/  124]
train() client id: f_00002-5-1 loss: 0.898254  [   64/  124]
train() client id: f_00002-5-2 loss: 0.880977  [   96/  124]
train() client id: f_00002-6-0 loss: 0.973924  [   32/  124]
train() client id: f_00002-6-1 loss: 0.926693  [   64/  124]
train() client id: f_00002-6-2 loss: 0.926550  [   96/  124]
train() client id: f_00002-7-0 loss: 0.902726  [   32/  124]
train() client id: f_00002-7-1 loss: 0.835398  [   64/  124]
train() client id: f_00002-7-2 loss: 1.031660  [   96/  124]
train() client id: f_00002-8-0 loss: 0.914967  [   32/  124]
train() client id: f_00002-8-1 loss: 0.849751  [   64/  124]
train() client id: f_00002-8-2 loss: 0.808711  [   96/  124]
train() client id: f_00002-9-0 loss: 0.951140  [   32/  124]
train() client id: f_00002-9-1 loss: 0.862020  [   64/  124]
train() client id: f_00002-9-2 loss: 0.828876  [   96/  124]
train() client id: f_00002-10-0 loss: 0.892999  [   32/  124]
train() client id: f_00002-10-1 loss: 0.582455  [   64/  124]
train() client id: f_00002-10-2 loss: 0.835530  [   96/  124]
train() client id: f_00002-11-0 loss: 0.973809  [   32/  124]
train() client id: f_00002-11-1 loss: 0.686333  [   64/  124]
train() client id: f_00002-11-2 loss: 0.777815  [   96/  124]
train() client id: f_00002-12-0 loss: 0.703154  [   32/  124]
train() client id: f_00002-12-1 loss: 0.840502  [   64/  124]
train() client id: f_00002-12-2 loss: 0.908539  [   96/  124]
train() client id: f_00002-13-0 loss: 0.892597  [   32/  124]
train() client id: f_00002-13-1 loss: 0.756084  [   64/  124]
train() client id: f_00002-13-2 loss: 0.704534  [   96/  124]
train() client id: f_00002-14-0 loss: 0.773213  [   32/  124]
train() client id: f_00002-14-1 loss: 0.862749  [   64/  124]
train() client id: f_00002-14-2 loss: 0.858995  [   96/  124]
train() client id: f_00002-15-0 loss: 0.750273  [   32/  124]
train() client id: f_00002-15-1 loss: 0.677984  [   64/  124]
train() client id: f_00002-15-2 loss: 1.179201  [   96/  124]
train() client id: f_00002-16-0 loss: 0.855542  [   32/  124]
train() client id: f_00002-16-1 loss: 0.705777  [   64/  124]
train() client id: f_00002-16-2 loss: 0.804986  [   96/  124]
train() client id: f_00003-0-0 loss: 0.501926  [   32/   43]
train() client id: f_00003-1-0 loss: 0.508697  [   32/   43]
train() client id: f_00003-2-0 loss: 0.565646  [   32/   43]
train() client id: f_00003-3-0 loss: 0.240584  [   32/   43]
train() client id: f_00003-4-0 loss: 0.357818  [   32/   43]
train() client id: f_00003-5-0 loss: 0.363770  [   32/   43]
train() client id: f_00003-6-0 loss: 0.339267  [   32/   43]
train() client id: f_00003-7-0 loss: 0.380529  [   32/   43]
train() client id: f_00003-8-0 loss: 0.500138  [   32/   43]
train() client id: f_00003-9-0 loss: 0.741852  [   32/   43]
train() client id: f_00003-10-0 loss: 0.396901  [   32/   43]
train() client id: f_00003-11-0 loss: 0.491104  [   32/   43]
train() client id: f_00003-12-0 loss: 0.459988  [   32/   43]
train() client id: f_00003-13-0 loss: 0.380199  [   32/   43]
train() client id: f_00003-14-0 loss: 0.385471  [   32/   43]
train() client id: f_00003-15-0 loss: 0.401910  [   32/   43]
train() client id: f_00003-16-0 loss: 0.440882  [   32/   43]
train() client id: f_00004-0-0 loss: 0.598237  [   32/  306]
train() client id: f_00004-0-1 loss: 0.457707  [   64/  306]
train() client id: f_00004-0-2 loss: 0.488579  [   96/  306]
train() client id: f_00004-0-3 loss: 0.629928  [  128/  306]
train() client id: f_00004-0-4 loss: 0.616987  [  160/  306]
train() client id: f_00004-0-5 loss: 0.750765  [  192/  306]
train() client id: f_00004-0-6 loss: 0.477185  [  224/  306]
train() client id: f_00004-0-7 loss: 0.826420  [  256/  306]
train() client id: f_00004-0-8 loss: 0.548540  [  288/  306]
train() client id: f_00004-1-0 loss: 0.670587  [   32/  306]
train() client id: f_00004-1-1 loss: 0.481341  [   64/  306]
train() client id: f_00004-1-2 loss: 0.671929  [   96/  306]
train() client id: f_00004-1-3 loss: 0.625851  [  128/  306]
train() client id: f_00004-1-4 loss: 0.473013  [  160/  306]
train() client id: f_00004-1-5 loss: 0.611265  [  192/  306]
train() client id: f_00004-1-6 loss: 0.633937  [  224/  306]
train() client id: f_00004-1-7 loss: 0.580932  [  256/  306]
train() client id: f_00004-1-8 loss: 0.587558  [  288/  306]
train() client id: f_00004-2-0 loss: 0.686187  [   32/  306]
train() client id: f_00004-2-1 loss: 0.672152  [   64/  306]
train() client id: f_00004-2-2 loss: 0.525708  [   96/  306]
train() client id: f_00004-2-3 loss: 0.462900  [  128/  306]
train() client id: f_00004-2-4 loss: 0.645272  [  160/  306]
train() client id: f_00004-2-5 loss: 0.551283  [  192/  306]
train() client id: f_00004-2-6 loss: 0.759520  [  224/  306]
train() client id: f_00004-2-7 loss: 0.447244  [  256/  306]
train() client id: f_00004-2-8 loss: 0.680941  [  288/  306]
train() client id: f_00004-3-0 loss: 0.416564  [   32/  306]
train() client id: f_00004-3-1 loss: 0.647337  [   64/  306]
train() client id: f_00004-3-2 loss: 0.723853  [   96/  306]
train() client id: f_00004-3-3 loss: 0.629004  [  128/  306]
train() client id: f_00004-3-4 loss: 0.612756  [  160/  306]
train() client id: f_00004-3-5 loss: 0.498490  [  192/  306]
train() client id: f_00004-3-6 loss: 0.679491  [  224/  306]
train() client id: f_00004-3-7 loss: 0.622039  [  256/  306]
train() client id: f_00004-3-8 loss: 0.687970  [  288/  306]
train() client id: f_00004-4-0 loss: 0.755239  [   32/  306]
train() client id: f_00004-4-1 loss: 0.548507  [   64/  306]
train() client id: f_00004-4-2 loss: 0.523832  [   96/  306]
train() client id: f_00004-4-3 loss: 0.716455  [  128/  306]
train() client id: f_00004-4-4 loss: 0.632809  [  160/  306]
train() client id: f_00004-4-5 loss: 0.477859  [  192/  306]
train() client id: f_00004-4-6 loss: 0.633716  [  224/  306]
train() client id: f_00004-4-7 loss: 0.603539  [  256/  306]
train() client id: f_00004-4-8 loss: 0.608562  [  288/  306]
train() client id: f_00004-5-0 loss: 0.591890  [   32/  306]
train() client id: f_00004-5-1 loss: 0.648304  [   64/  306]
train() client id: f_00004-5-2 loss: 0.630583  [   96/  306]
train() client id: f_00004-5-3 loss: 0.560993  [  128/  306]
train() client id: f_00004-5-4 loss: 0.632920  [  160/  306]
train() client id: f_00004-5-5 loss: 0.675449  [  192/  306]
train() client id: f_00004-5-6 loss: 0.459277  [  224/  306]
train() client id: f_00004-5-7 loss: 0.674401  [  256/  306]
train() client id: f_00004-5-8 loss: 0.727489  [  288/  306]
train() client id: f_00004-6-0 loss: 0.601663  [   32/  306]
train() client id: f_00004-6-1 loss: 0.731430  [   64/  306]
train() client id: f_00004-6-2 loss: 0.697707  [   96/  306]
train() client id: f_00004-6-3 loss: 0.458080  [  128/  306]
train() client id: f_00004-6-4 loss: 0.516485  [  160/  306]
train() client id: f_00004-6-5 loss: 0.770072  [  192/  306]
train() client id: f_00004-6-6 loss: 0.562355  [  224/  306]
train() client id: f_00004-6-7 loss: 0.685841  [  256/  306]
train() client id: f_00004-6-8 loss: 0.563076  [  288/  306]
train() client id: f_00004-7-0 loss: 0.606396  [   32/  306]
train() client id: f_00004-7-1 loss: 0.652666  [   64/  306]
train() client id: f_00004-7-2 loss: 0.573994  [   96/  306]
train() client id: f_00004-7-3 loss: 0.688472  [  128/  306]
train() client id: f_00004-7-4 loss: 0.626750  [  160/  306]
train() client id: f_00004-7-5 loss: 0.451508  [  192/  306]
train() client id: f_00004-7-6 loss: 0.625579  [  224/  306]
train() client id: f_00004-7-7 loss: 0.566975  [  256/  306]
train() client id: f_00004-7-8 loss: 0.694898  [  288/  306]
train() client id: f_00004-8-0 loss: 0.588471  [   32/  306]
train() client id: f_00004-8-1 loss: 0.750277  [   64/  306]
train() client id: f_00004-8-2 loss: 0.686774  [   96/  306]
train() client id: f_00004-8-3 loss: 0.638052  [  128/  306]
train() client id: f_00004-8-4 loss: 0.579992  [  160/  306]
train() client id: f_00004-8-5 loss: 0.601706  [  192/  306]
train() client id: f_00004-8-6 loss: 0.573051  [  224/  306]
train() client id: f_00004-8-7 loss: 0.580250  [  256/  306]
train() client id: f_00004-8-8 loss: 0.546330  [  288/  306]
train() client id: f_00004-9-0 loss: 0.617723  [   32/  306]
train() client id: f_00004-9-1 loss: 0.769074  [   64/  306]
train() client id: f_00004-9-2 loss: 0.562727  [   96/  306]
train() client id: f_00004-9-3 loss: 0.651275  [  128/  306]
train() client id: f_00004-9-4 loss: 0.653459  [  160/  306]
train() client id: f_00004-9-5 loss: 0.545350  [  192/  306]
train() client id: f_00004-9-6 loss: 0.664224  [  224/  306]
train() client id: f_00004-9-7 loss: 0.556591  [  256/  306]
train() client id: f_00004-9-8 loss: 0.601521  [  288/  306]
train() client id: f_00004-10-0 loss: 0.546163  [   32/  306]
train() client id: f_00004-10-1 loss: 0.654151  [   64/  306]
train() client id: f_00004-10-2 loss: 0.609290  [   96/  306]
train() client id: f_00004-10-3 loss: 0.655468  [  128/  306]
train() client id: f_00004-10-4 loss: 0.686182  [  160/  306]
train() client id: f_00004-10-5 loss: 0.601184  [  192/  306]
train() client id: f_00004-10-6 loss: 0.540089  [  224/  306]
train() client id: f_00004-10-7 loss: 0.650488  [  256/  306]
train() client id: f_00004-10-8 loss: 0.634147  [  288/  306]
train() client id: f_00004-11-0 loss: 0.618725  [   32/  306]
train() client id: f_00004-11-1 loss: 0.637699  [   64/  306]
train() client id: f_00004-11-2 loss: 0.632166  [   96/  306]
train() client id: f_00004-11-3 loss: 0.601857  [  128/  306]
train() client id: f_00004-11-4 loss: 0.644507  [  160/  306]
train() client id: f_00004-11-5 loss: 0.591474  [  192/  306]
train() client id: f_00004-11-6 loss: 0.675759  [  224/  306]
train() client id: f_00004-11-7 loss: 0.628781  [  256/  306]
train() client id: f_00004-11-8 loss: 0.684383  [  288/  306]
train() client id: f_00004-12-0 loss: 0.563586  [   32/  306]
train() client id: f_00004-12-1 loss: 0.600313  [   64/  306]
train() client id: f_00004-12-2 loss: 0.677300  [   96/  306]
train() client id: f_00004-12-3 loss: 0.541793  [  128/  306]
train() client id: f_00004-12-4 loss: 0.611265  [  160/  306]
train() client id: f_00004-12-5 loss: 0.723907  [  192/  306]
train() client id: f_00004-12-6 loss: 0.651719  [  224/  306]
train() client id: f_00004-12-7 loss: 0.567922  [  256/  306]
train() client id: f_00004-12-8 loss: 0.634375  [  288/  306]
train() client id: f_00004-13-0 loss: 0.650373  [   32/  306]
train() client id: f_00004-13-1 loss: 0.716821  [   64/  306]
train() client id: f_00004-13-2 loss: 0.664688  [   96/  306]
train() client id: f_00004-13-3 loss: 0.519377  [  128/  306]
train() client id: f_00004-13-4 loss: 0.679581  [  160/  306]
train() client id: f_00004-13-5 loss: 0.674106  [  192/  306]
train() client id: f_00004-13-6 loss: 0.605746  [  224/  306]
train() client id: f_00004-13-7 loss: 0.553651  [  256/  306]
train() client id: f_00004-13-8 loss: 0.697100  [  288/  306]
train() client id: f_00004-14-0 loss: 0.661277  [   32/  306]
train() client id: f_00004-14-1 loss: 0.612635  [   64/  306]
train() client id: f_00004-14-2 loss: 0.606643  [   96/  306]
train() client id: f_00004-14-3 loss: 0.664029  [  128/  306]
train() client id: f_00004-14-4 loss: 0.503431  [  160/  306]
train() client id: f_00004-14-5 loss: 0.734467  [  192/  306]
train() client id: f_00004-14-6 loss: 0.633955  [  224/  306]
train() client id: f_00004-14-7 loss: 0.595449  [  256/  306]
train() client id: f_00004-14-8 loss: 0.654081  [  288/  306]
train() client id: f_00004-15-0 loss: 0.707505  [   32/  306]
train() client id: f_00004-15-1 loss: 0.604842  [   64/  306]
train() client id: f_00004-15-2 loss: 0.628194  [   96/  306]
train() client id: f_00004-15-3 loss: 0.618215  [  128/  306]
train() client id: f_00004-15-4 loss: 0.579953  [  160/  306]
train() client id: f_00004-15-5 loss: 0.690370  [  192/  306]
train() client id: f_00004-15-6 loss: 0.623362  [  224/  306]
train() client id: f_00004-15-7 loss: 0.638108  [  256/  306]
train() client id: f_00004-15-8 loss: 0.687078  [  288/  306]
train() client id: f_00004-16-0 loss: 0.570576  [   32/  306]
train() client id: f_00004-16-1 loss: 0.562433  [   64/  306]
train() client id: f_00004-16-2 loss: 0.655656  [   96/  306]
train() client id: f_00004-16-3 loss: 0.734577  [  128/  306]
train() client id: f_00004-16-4 loss: 0.633026  [  160/  306]
train() client id: f_00004-16-5 loss: 0.639089  [  192/  306]
train() client id: f_00004-16-6 loss: 0.644667  [  224/  306]
train() client id: f_00004-16-7 loss: 0.701794  [  256/  306]
train() client id: f_00004-16-8 loss: 0.619595  [  288/  306]
train() client id: f_00005-0-0 loss: 0.829406  [   32/  146]
train() client id: f_00005-0-1 loss: 0.920400  [   64/  146]
train() client id: f_00005-0-2 loss: 0.699143  [   96/  146]
train() client id: f_00005-0-3 loss: 0.704587  [  128/  146]
train() client id: f_00005-1-0 loss: 1.053235  [   32/  146]
train() client id: f_00005-1-1 loss: 0.683955  [   64/  146]
train() client id: f_00005-1-2 loss: 0.731223  [   96/  146]
train() client id: f_00005-1-3 loss: 0.588617  [  128/  146]
train() client id: f_00005-2-0 loss: 0.555983  [   32/  146]
train() client id: f_00005-2-1 loss: 0.734409  [   64/  146]
train() client id: f_00005-2-2 loss: 0.973898  [   96/  146]
train() client id: f_00005-2-3 loss: 0.950933  [  128/  146]
train() client id: f_00005-3-0 loss: 0.554129  [   32/  146]
train() client id: f_00005-3-1 loss: 0.936310  [   64/  146]
train() client id: f_00005-3-2 loss: 0.748108  [   96/  146]
train() client id: f_00005-3-3 loss: 0.823240  [  128/  146]
train() client id: f_00005-4-0 loss: 0.847719  [   32/  146]
train() client id: f_00005-4-1 loss: 0.948878  [   64/  146]
train() client id: f_00005-4-2 loss: 0.755178  [   96/  146]
train() client id: f_00005-4-3 loss: 0.555112  [  128/  146]
train() client id: f_00005-5-0 loss: 0.684433  [   32/  146]
train() client id: f_00005-5-1 loss: 0.523918  [   64/  146]
train() client id: f_00005-5-2 loss: 0.888400  [   96/  146]
train() client id: f_00005-5-3 loss: 0.987711  [  128/  146]
train() client id: f_00005-6-0 loss: 0.971650  [   32/  146]
train() client id: f_00005-6-1 loss: 0.633584  [   64/  146]
train() client id: f_00005-6-2 loss: 0.700085  [   96/  146]
train() client id: f_00005-6-3 loss: 0.818376  [  128/  146]
train() client id: f_00005-7-0 loss: 0.661309  [   32/  146]
train() client id: f_00005-7-1 loss: 0.726516  [   64/  146]
train() client id: f_00005-7-2 loss: 0.630674  [   96/  146]
train() client id: f_00005-7-3 loss: 0.863925  [  128/  146]
train() client id: f_00005-8-0 loss: 0.564654  [   32/  146]
train() client id: f_00005-8-1 loss: 0.930221  [   64/  146]
train() client id: f_00005-8-2 loss: 0.764692  [   96/  146]
train() client id: f_00005-8-3 loss: 0.715274  [  128/  146]
train() client id: f_00005-9-0 loss: 0.616901  [   32/  146]
train() client id: f_00005-9-1 loss: 0.629717  [   64/  146]
train() client id: f_00005-9-2 loss: 0.917906  [   96/  146]
train() client id: f_00005-9-3 loss: 0.967964  [  128/  146]
train() client id: f_00005-10-0 loss: 1.029747  [   32/  146]
train() client id: f_00005-10-1 loss: 0.729654  [   64/  146]
train() client id: f_00005-10-2 loss: 0.749407  [   96/  146]
train() client id: f_00005-10-3 loss: 0.632412  [  128/  146]
train() client id: f_00005-11-0 loss: 0.761077  [   32/  146]
train() client id: f_00005-11-1 loss: 0.826001  [   64/  146]
train() client id: f_00005-11-2 loss: 0.618673  [   96/  146]
train() client id: f_00005-11-3 loss: 0.825671  [  128/  146]
train() client id: f_00005-12-0 loss: 0.972634  [   32/  146]
train() client id: f_00005-12-1 loss: 0.717358  [   64/  146]
train() client id: f_00005-12-2 loss: 0.703506  [   96/  146]
train() client id: f_00005-12-3 loss: 0.668437  [  128/  146]
train() client id: f_00005-13-0 loss: 0.862620  [   32/  146]
train() client id: f_00005-13-1 loss: 0.856437  [   64/  146]
train() client id: f_00005-13-2 loss: 0.627975  [   96/  146]
train() client id: f_00005-13-3 loss: 0.734630  [  128/  146]
train() client id: f_00005-14-0 loss: 0.771487  [   32/  146]
train() client id: f_00005-14-1 loss: 0.873819  [   64/  146]
train() client id: f_00005-14-2 loss: 0.673658  [   96/  146]
train() client id: f_00005-14-3 loss: 0.753327  [  128/  146]
train() client id: f_00005-15-0 loss: 0.900314  [   32/  146]
train() client id: f_00005-15-1 loss: 0.665565  [   64/  146]
train() client id: f_00005-15-2 loss: 0.804317  [   96/  146]
train() client id: f_00005-15-3 loss: 0.646603  [  128/  146]
train() client id: f_00005-16-0 loss: 0.626615  [   32/  146]
train() client id: f_00005-16-1 loss: 0.675442  [   64/  146]
train() client id: f_00005-16-2 loss: 0.639679  [   96/  146]
train() client id: f_00005-16-3 loss: 0.857539  [  128/  146]
train() client id: f_00006-0-0 loss: 0.500443  [   32/   54]
train() client id: f_00006-1-0 loss: 0.500193  [   32/   54]
train() client id: f_00006-2-0 loss: 0.403238  [   32/   54]
train() client id: f_00006-3-0 loss: 0.421387  [   32/   54]
train() client id: f_00006-4-0 loss: 0.488793  [   32/   54]
train() client id: f_00006-5-0 loss: 0.483123  [   32/   54]
train() client id: f_00006-6-0 loss: 0.502525  [   32/   54]
train() client id: f_00006-7-0 loss: 0.477207  [   32/   54]
train() client id: f_00006-8-0 loss: 0.504629  [   32/   54]
train() client id: f_00006-9-0 loss: 0.493593  [   32/   54]
train() client id: f_00006-10-0 loss: 0.453872  [   32/   54]
train() client id: f_00006-11-0 loss: 0.472759  [   32/   54]
train() client id: f_00006-12-0 loss: 0.376876  [   32/   54]
train() client id: f_00006-13-0 loss: 0.400920  [   32/   54]
train() client id: f_00006-14-0 loss: 0.380644  [   32/   54]
train() client id: f_00006-15-0 loss: 0.433964  [   32/   54]
train() client id: f_00006-16-0 loss: 0.435314  [   32/   54]
train() client id: f_00007-0-0 loss: 0.691852  [   32/  179]
train() client id: f_00007-0-1 loss: 0.700539  [   64/  179]
train() client id: f_00007-0-2 loss: 0.580039  [   96/  179]
train() client id: f_00007-0-3 loss: 0.842188  [  128/  179]
train() client id: f_00007-0-4 loss: 0.703964  [  160/  179]
train() client id: f_00007-1-0 loss: 0.994048  [   32/  179]
train() client id: f_00007-1-1 loss: 0.603654  [   64/  179]
train() client id: f_00007-1-2 loss: 0.603742  [   96/  179]
train() client id: f_00007-1-3 loss: 0.619425  [  128/  179]
train() client id: f_00007-1-4 loss: 0.585631  [  160/  179]
train() client id: f_00007-2-0 loss: 0.582528  [   32/  179]
train() client id: f_00007-2-1 loss: 0.738683  [   64/  179]
train() client id: f_00007-2-2 loss: 0.628633  [   96/  179]
train() client id: f_00007-2-3 loss: 0.709440  [  128/  179]
train() client id: f_00007-2-4 loss: 0.706293  [  160/  179]
train() client id: f_00007-3-0 loss: 0.854443  [   32/  179]
train() client id: f_00007-3-1 loss: 0.616824  [   64/  179]
train() client id: f_00007-3-2 loss: 0.529901  [   96/  179]
train() client id: f_00007-3-3 loss: 0.452979  [  128/  179]
train() client id: f_00007-3-4 loss: 0.686807  [  160/  179]
train() client id: f_00007-4-0 loss: 0.684612  [   32/  179]
train() client id: f_00007-4-1 loss: 0.543480  [   64/  179]
train() client id: f_00007-4-2 loss: 0.810168  [   96/  179]
train() client id: f_00007-4-3 loss: 0.691161  [  128/  179]
train() client id: f_00007-4-4 loss: 0.592276  [  160/  179]
train() client id: f_00007-5-0 loss: 0.663884  [   32/  179]
train() client id: f_00007-5-1 loss: 0.887326  [   64/  179]
train() client id: f_00007-5-2 loss: 0.573082  [   96/  179]
train() client id: f_00007-5-3 loss: 0.538849  [  128/  179]
train() client id: f_00007-5-4 loss: 0.629801  [  160/  179]
train() client id: f_00007-6-0 loss: 0.509444  [   32/  179]
train() client id: f_00007-6-1 loss: 0.453453  [   64/  179]
train() client id: f_00007-6-2 loss: 0.651468  [   96/  179]
train() client id: f_00007-6-3 loss: 0.643761  [  128/  179]
train() client id: f_00007-6-4 loss: 0.965247  [  160/  179]
train() client id: f_00007-7-0 loss: 0.628397  [   32/  179]
train() client id: f_00007-7-1 loss: 0.775812  [   64/  179]
train() client id: f_00007-7-2 loss: 0.559116  [   96/  179]
train() client id: f_00007-7-3 loss: 0.585360  [  128/  179]
train() client id: f_00007-7-4 loss: 0.562067  [  160/  179]
train() client id: f_00007-8-0 loss: 0.457937  [   32/  179]
train() client id: f_00007-8-1 loss: 0.678406  [   64/  179]
train() client id: f_00007-8-2 loss: 0.547983  [   96/  179]
train() client id: f_00007-8-3 loss: 0.742317  [  128/  179]
train() client id: f_00007-8-4 loss: 0.677611  [  160/  179]
train() client id: f_00007-9-0 loss: 0.822913  [   32/  179]
train() client id: f_00007-9-1 loss: 0.691445  [   64/  179]
train() client id: f_00007-9-2 loss: 0.434644  [   96/  179]
train() client id: f_00007-9-3 loss: 0.555002  [  128/  179]
train() client id: f_00007-9-4 loss: 0.558541  [  160/  179]
train() client id: f_00007-10-0 loss: 0.673270  [   32/  179]
train() client id: f_00007-10-1 loss: 0.824797  [   64/  179]
train() client id: f_00007-10-2 loss: 0.520955  [   96/  179]
train() client id: f_00007-10-3 loss: 0.588886  [  128/  179]
train() client id: f_00007-10-4 loss: 0.476108  [  160/  179]
train() client id: f_00007-11-0 loss: 0.694879  [   32/  179]
train() client id: f_00007-11-1 loss: 0.563188  [   64/  179]
train() client id: f_00007-11-2 loss: 0.535731  [   96/  179]
train() client id: f_00007-11-3 loss: 0.815369  [  128/  179]
train() client id: f_00007-11-4 loss: 0.478477  [  160/  179]
train() client id: f_00007-12-0 loss: 0.498201  [   32/  179]
train() client id: f_00007-12-1 loss: 0.569203  [   64/  179]
train() client id: f_00007-12-2 loss: 0.801740  [   96/  179]
train() client id: f_00007-12-3 loss: 0.712365  [  128/  179]
train() client id: f_00007-12-4 loss: 0.506345  [  160/  179]
train() client id: f_00007-13-0 loss: 0.524413  [   32/  179]
train() client id: f_00007-13-1 loss: 0.537422  [   64/  179]
train() client id: f_00007-13-2 loss: 0.738417  [   96/  179]
train() client id: f_00007-13-3 loss: 0.596292  [  128/  179]
train() client id: f_00007-13-4 loss: 0.646302  [  160/  179]
train() client id: f_00007-14-0 loss: 0.521892  [   32/  179]
train() client id: f_00007-14-1 loss: 0.600603  [   64/  179]
train() client id: f_00007-14-2 loss: 0.652680  [   96/  179]
train() client id: f_00007-14-3 loss: 0.545070  [  128/  179]
train() client id: f_00007-14-4 loss: 0.706995  [  160/  179]
train() client id: f_00007-15-0 loss: 0.468100  [   32/  179]
train() client id: f_00007-15-1 loss: 0.548084  [   64/  179]
train() client id: f_00007-15-2 loss: 0.743479  [   96/  179]
train() client id: f_00007-15-3 loss: 0.574941  [  128/  179]
train() client id: f_00007-15-4 loss: 0.725732  [  160/  179]
train() client id: f_00007-16-0 loss: 0.428190  [   32/  179]
train() client id: f_00007-16-1 loss: 0.656675  [   64/  179]
train() client id: f_00007-16-2 loss: 0.446631  [   96/  179]
train() client id: f_00007-16-3 loss: 0.823205  [  128/  179]
train() client id: f_00007-16-4 loss: 0.675819  [  160/  179]
train() client id: f_00008-0-0 loss: 0.579073  [   32/  130]
train() client id: f_00008-0-1 loss: 0.630012  [   64/  130]
train() client id: f_00008-0-2 loss: 0.574711  [   96/  130]
train() client id: f_00008-0-3 loss: 0.572569  [  128/  130]
train() client id: f_00008-1-0 loss: 0.513170  [   32/  130]
train() client id: f_00008-1-1 loss: 0.596361  [   64/  130]
train() client id: f_00008-1-2 loss: 0.563224  [   96/  130]
train() client id: f_00008-1-3 loss: 0.681902  [  128/  130]
train() client id: f_00008-2-0 loss: 0.610584  [   32/  130]
train() client id: f_00008-2-1 loss: 0.664829  [   64/  130]
train() client id: f_00008-2-2 loss: 0.438361  [   96/  130]
train() client id: f_00008-2-3 loss: 0.624078  [  128/  130]
train() client id: f_00008-3-0 loss: 0.621155  [   32/  130]
train() client id: f_00008-3-1 loss: 0.574080  [   64/  130]
train() client id: f_00008-3-2 loss: 0.498404  [   96/  130]
train() client id: f_00008-3-3 loss: 0.645323  [  128/  130]
train() client id: f_00008-4-0 loss: 0.481615  [   32/  130]
train() client id: f_00008-4-1 loss: 0.703414  [   64/  130]
train() client id: f_00008-4-2 loss: 0.489771  [   96/  130]
train() client id: f_00008-4-3 loss: 0.665905  [  128/  130]
train() client id: f_00008-5-0 loss: 0.589353  [   32/  130]
train() client id: f_00008-5-1 loss: 0.617463  [   64/  130]
train() client id: f_00008-5-2 loss: 0.550448  [   96/  130]
train() client id: f_00008-5-3 loss: 0.617173  [  128/  130]
train() client id: f_00008-6-0 loss: 0.546605  [   32/  130]
train() client id: f_00008-6-1 loss: 0.581455  [   64/  130]
train() client id: f_00008-6-2 loss: 0.555225  [   96/  130]
train() client id: f_00008-6-3 loss: 0.683263  [  128/  130]
train() client id: f_00008-7-0 loss: 0.516769  [   32/  130]
train() client id: f_00008-7-1 loss: 0.631153  [   64/  130]
train() client id: f_00008-7-2 loss: 0.644073  [   96/  130]
train() client id: f_00008-7-3 loss: 0.557123  [  128/  130]
train() client id: f_00008-8-0 loss: 0.566458  [   32/  130]
train() client id: f_00008-8-1 loss: 0.577949  [   64/  130]
train() client id: f_00008-8-2 loss: 0.602705  [   96/  130]
train() client id: f_00008-8-3 loss: 0.624329  [  128/  130]
train() client id: f_00008-9-0 loss: 0.519995  [   32/  130]
train() client id: f_00008-9-1 loss: 0.550595  [   64/  130]
train() client id: f_00008-9-2 loss: 0.622156  [   96/  130]
train() client id: f_00008-9-3 loss: 0.679566  [  128/  130]
train() client id: f_00008-10-0 loss: 0.641104  [   32/  130]
train() client id: f_00008-10-1 loss: 0.537323  [   64/  130]
train() client id: f_00008-10-2 loss: 0.550738  [   96/  130]
train() client id: f_00008-10-3 loss: 0.651322  [  128/  130]
train() client id: f_00008-11-0 loss: 0.568893  [   32/  130]
train() client id: f_00008-11-1 loss: 0.611569  [   64/  130]
train() client id: f_00008-11-2 loss: 0.672991  [   96/  130]
train() client id: f_00008-11-3 loss: 0.526222  [  128/  130]
train() client id: f_00008-12-0 loss: 0.565704  [   32/  130]
train() client id: f_00008-12-1 loss: 0.508967  [   64/  130]
train() client id: f_00008-12-2 loss: 0.685506  [   96/  130]
train() client id: f_00008-12-3 loss: 0.629455  [  128/  130]
train() client id: f_00008-13-0 loss: 0.723918  [   32/  130]
train() client id: f_00008-13-1 loss: 0.486413  [   64/  130]
train() client id: f_00008-13-2 loss: 0.603636  [   96/  130]
train() client id: f_00008-13-3 loss: 0.530762  [  128/  130]
train() client id: f_00008-14-0 loss: 0.662956  [   32/  130]
train() client id: f_00008-14-1 loss: 0.583111  [   64/  130]
train() client id: f_00008-14-2 loss: 0.575552  [   96/  130]
train() client id: f_00008-14-3 loss: 0.570994  [  128/  130]
train() client id: f_00008-15-0 loss: 0.547061  [   32/  130]
train() client id: f_00008-15-1 loss: 0.591791  [   64/  130]
train() client id: f_00008-15-2 loss: 0.594389  [   96/  130]
train() client id: f_00008-15-3 loss: 0.644884  [  128/  130]
train() client id: f_00008-16-0 loss: 0.542859  [   32/  130]
train() client id: f_00008-16-1 loss: 0.656170  [   64/  130]
train() client id: f_00008-16-2 loss: 0.528953  [   96/  130]
train() client id: f_00008-16-3 loss: 0.625314  [  128/  130]
train() client id: f_00009-0-0 loss: 1.257809  [   32/  118]
train() client id: f_00009-0-1 loss: 1.189654  [   64/  118]
train() client id: f_00009-0-2 loss: 1.353375  [   96/  118]
train() client id: f_00009-1-0 loss: 1.176585  [   32/  118]
train() client id: f_00009-1-1 loss: 1.181055  [   64/  118]
train() client id: f_00009-1-2 loss: 1.241257  [   96/  118]
train() client id: f_00009-2-0 loss: 1.216314  [   32/  118]
train() client id: f_00009-2-1 loss: 1.030510  [   64/  118]
train() client id: f_00009-2-2 loss: 1.179462  [   96/  118]
train() client id: f_00009-3-0 loss: 1.197443  [   32/  118]
train() client id: f_00009-3-1 loss: 1.088423  [   64/  118]
train() client id: f_00009-3-2 loss: 1.061168  [   96/  118]
train() client id: f_00009-4-0 loss: 1.181914  [   32/  118]
train() client id: f_00009-4-1 loss: 0.912797  [   64/  118]
train() client id: f_00009-4-2 loss: 1.137433  [   96/  118]
train() client id: f_00009-5-0 loss: 1.077800  [   32/  118]
train() client id: f_00009-5-1 loss: 1.085105  [   64/  118]
train() client id: f_00009-5-2 loss: 0.827168  [   96/  118]
train() client id: f_00009-6-0 loss: 0.937659  [   32/  118]
train() client id: f_00009-6-1 loss: 0.940364  [   64/  118]
train() client id: f_00009-6-2 loss: 1.025162  [   96/  118]
train() client id: f_00009-7-0 loss: 1.042610  [   32/  118]
train() client id: f_00009-7-1 loss: 0.944211  [   64/  118]
train() client id: f_00009-7-2 loss: 0.945108  [   96/  118]
train() client id: f_00009-8-0 loss: 0.842818  [   32/  118]
train() client id: f_00009-8-1 loss: 0.921362  [   64/  118]
train() client id: f_00009-8-2 loss: 1.051488  [   96/  118]
train() client id: f_00009-9-0 loss: 0.969487  [   32/  118]
train() client id: f_00009-9-1 loss: 0.970557  [   64/  118]
train() client id: f_00009-9-2 loss: 0.688608  [   96/  118]
train() client id: f_00009-10-0 loss: 0.946991  [   32/  118]
train() client id: f_00009-10-1 loss: 0.831570  [   64/  118]
train() client id: f_00009-10-2 loss: 0.891205  [   96/  118]
train() client id: f_00009-11-0 loss: 0.941497  [   32/  118]
train() client id: f_00009-11-1 loss: 0.846611  [   64/  118]
train() client id: f_00009-11-2 loss: 0.854222  [   96/  118]
train() client id: f_00009-12-0 loss: 0.885863  [   32/  118]
train() client id: f_00009-12-1 loss: 0.694448  [   64/  118]
train() client id: f_00009-12-2 loss: 0.862227  [   96/  118]
train() client id: f_00009-13-0 loss: 0.666877  [   32/  118]
train() client id: f_00009-13-1 loss: 0.888375  [   64/  118]
train() client id: f_00009-13-2 loss: 0.786567  [   96/  118]
train() client id: f_00009-14-0 loss: 0.954378  [   32/  118]
train() client id: f_00009-14-1 loss: 0.656308  [   64/  118]
train() client id: f_00009-14-2 loss: 0.837465  [   96/  118]
train() client id: f_00009-15-0 loss: 0.970579  [   32/  118]
train() client id: f_00009-15-1 loss: 0.577506  [   64/  118]
train() client id: f_00009-15-2 loss: 0.828485  [   96/  118]
train() client id: f_00009-16-0 loss: 0.649178  [   32/  118]
train() client id: f_00009-16-1 loss: 0.889726  [   64/  118]
train() client id: f_00009-16-2 loss: 0.909100  [   96/  118]
At round 54 accuracy: 0.6472148541114059
At round 54 training accuracy: 0.5902079141515761
At round 54 training loss: 0.8344411244597174
update_location
xs = 8.927491 391.223621 5.882650 0.934260 -307.581990 -155.230757 -115.849135 -5.143845 -330.120581 20.134486 
ys = -382.390647 7.291448 280.684448 -102.290817 -9.642386 0.794442 -1.381692 276.628436 25.881276 -817.232496 
xs mean: -48.682379970521225
ys mean: -72.16579882624052
dists_uav = 395.350866 403.867660 298.024101 143.053431 323.573262 184.654324 153.045520 294.193390 345.903799 823.574131 
uav_gains = -121.229924 -121.543763 -115.564732 -103.889876 -117.537513 -106.721257 -104.626445 -115.238147 -118.944096 -129.861928 
uav_gains_db_mean: -115.51576817642024
dists_bs = 586.952856 590.538184 209.493999 328.394673 227.312109 175.323699 186.035820 197.938002 215.171533 1011.238248 
bs_gains = -117.087949 -117.162003 -104.559864 -110.026150 -105.552491 -102.394597 -103.115763 -103.869876 -104.885034 -123.702957 
bs_gains_db_mean: -109.23566835297201
Round 55
-------------------------------
ene_coms = [0.03288428 0.03439817 0.00776816 0.00751234 0.00817993 0.00872237
 0.00779451 0.00750496 0.00789853 0.04800091]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.89644037 7.86683573 3.60249838 1.29472586 8.78632532 4.23764993
 1.61046203 5.16538141 3.774496   3.81140559]
obj_prev = 44.04622061248223
eta_min = 2.9702732107915335e-25	eta_max = 0.8339135989859802
af = 9.122641062598856	bf = 1.5944437674064293	zeta = 10.034905168858742	eta = 0.909090909090909
af = 9.122641062598856	bf = 1.5944437674064293	zeta = 27.103167601103443	eta = 0.33658947901821806
af = 9.122641062598856	bf = 1.5944437674064293	zeta = 17.376884255516764	eta = 0.5249871569871697
af = 9.122641062598856	bf = 1.5944437674064293	zeta = 15.732005613835742	eta = 0.5798778163780857
af = 9.122641062598856	bf = 1.5944437674064293	zeta = 15.628111327866806	eta = 0.5837327922237211
af = 9.122641062598856	bf = 1.5944437674064293	zeta = 15.6276313172202	eta = 0.5837507218734135
af = 9.122641062598856	bf = 1.5944437674064293	zeta = 15.627631306882245	eta = 0.5837507222595749
eta = 0.5837507222595749
ene_coms = [0.03288428 0.03439817 0.00776816 0.00751234 0.00817993 0.00872237
 0.00779451 0.00750496 0.00789853 0.04800091]
ene_comp = [0.04441768 0.09341813 0.04371263 0.01515841 0.1078715  0.0514681
 0.01903615 0.0631013  0.04582776 0.04159751]
ene_total = [1.7350175  2.86879569 1.15546987 0.50883788 2.60473703 1.35095577
 0.60220552 1.58473491 1.20586943 2.01100771]
ti_comp = [0.88296047 0.86782157 1.13412164 1.13667978 1.13000395 1.12457954
 1.13385811 1.13675362 1.13281789 0.73179411]
ti_coms = [0.32884276 0.34398166 0.07768159 0.07512344 0.08179928 0.08722368
 0.07794512 0.07504961 0.07898533 0.48000911]
t_total = [27.19853401 27.19853401 27.19853401 27.19853401 27.19853401 27.19853401
 27.19853401 27.19853401 27.19853401 27.19853401]
ene_coms = [0.03288428 0.03439817 0.00776816 0.00751234 0.00817993 0.00872237
 0.00779451 0.00750496 0.00789853 0.04800091]
ene_comp = [7.02530247e-06 6.76569869e-05 4.05865086e-06 1.68486460e-07
 6.14384522e-05 6.73773801e-06 3.35351126e-07 1.21524072e-05
 4.68754841e-06 8.40048013e-06]
ene_total = [0.73823469 0.77357432 0.17444493 0.16861594 0.18497481 0.19592198
 0.17495284 0.16871919 0.17738525 1.07755358]
optimize_network iter = 0 obj = 3.834377513558774
eta = 0.5837507222595749
freqs = [25152698.66257136 53823351.35052256 19271581.27949193  6667846.93538365
 47730586.00444405 22883264.40558869  8394413.35865643 27755048.32731231
 20227329.70952416 28421591.78337451]
eta_min = 0.5837507222595756	eta_max = 0.5837507222595714
af = 0.0029967665570901723	bf = 1.5944437674064293	zeta = 0.0032964432127991898	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03288428 0.03439817 0.00776816 0.00751234 0.00817993 0.00872237
 0.00779451 0.00750496 0.00789853 0.04800091]
ene_comp = [1.24392753e-06 1.19796106e-05 7.18640592e-07 2.98328714e-08
 1.08785326e-05 1.19301024e-06 5.93785815e-08 2.15175274e-06
 8.29995651e-07 1.48742186e-06]
ene_total = [3.37959207 3.53627538 0.79839507 0.77203463 0.84175599 0.89650639
 0.80103555 0.77149391 0.81180487 4.93312943]
ti_comp = [0.88296047 0.86782157 1.13412164 1.13667978 1.13000395 1.12457954
 1.13385811 1.13675362 1.13281789 0.73179411]
ti_coms = [0.32884276 0.34398166 0.07768159 0.07512344 0.08179928 0.08722368
 0.07794512 0.07504961 0.07898533 0.48000911]
t_total = [27.19853401 27.19853401 27.19853401 27.19853401 27.19853401 27.19853401
 27.19853401 27.19853401 27.19853401 27.19853401]
ene_coms = [0.03288428 0.03439817 0.00776816 0.00751234 0.00817993 0.00872237
 0.00779451 0.00750496 0.00789853 0.04800091]
ene_comp = [7.02530247e-06 6.76569869e-05 4.05865086e-06 1.68486460e-07
 6.14384522e-05 6.73773801e-06 3.35351126e-07 1.21524072e-05
 4.68754841e-06 8.40048013e-06]
ene_total = [0.73823469 0.77357432 0.17444493 0.16861594 0.18497481 0.19592198
 0.17495284 0.16871919 0.17738525 1.07755358]
optimize_network iter = 1 obj = 3.8343775135587412
eta = 0.5837507222595714
freqs = [25152698.66257135 53823351.35052253 19271581.27949198  6667846.93538367
 47730586.00444415 22883264.40558875  8394413.35865645 27755048.32731238
 20227329.70952421 28421591.78337444]
Done!
ene_coms = [0.03288428 0.03439817 0.00776816 0.00751234 0.00817993 0.00872237
 0.00779451 0.00750496 0.00789853 0.04800091]
ene_comp = [6.77576986e-06 6.52538697e-05 3.91449113e-06 1.62501968e-07
 5.92562119e-05 6.49841943e-06 3.23439746e-07 1.17207643e-05
 4.52105078e-06 8.10210240e-06]
ene_total = [0.03289105 0.03446342 0.00777207 0.00751251 0.00823918 0.00872887
 0.00779484 0.00751668 0.00790305 0.04800901]
At round 55 energy consumption: 0.1708306867190141
At round 55 eta: 0.5837507222595714
At round 55 a_n: 9.342581272941924
At round 55 local rounds: 17.626062356020697
At round 55 global rounds: 22.444678639821976
gradient difference: 0.3083716928958893
train() client id: f_00000-0-0 loss: 1.272027  [   32/  126]
train() client id: f_00000-0-1 loss: 1.472118  [   64/  126]
train() client id: f_00000-0-2 loss: 1.410268  [   96/  126]
train() client id: f_00000-1-0 loss: 1.149703  [   32/  126]
train() client id: f_00000-1-1 loss: 1.421356  [   64/  126]
train() client id: f_00000-1-2 loss: 1.267367  [   96/  126]
train() client id: f_00000-2-0 loss: 1.128522  [   32/  126]
train() client id: f_00000-2-1 loss: 1.174026  [   64/  126]
train() client id: f_00000-2-2 loss: 1.297163  [   96/  126]
train() client id: f_00000-3-0 loss: 1.032852  [   32/  126]
train() client id: f_00000-3-1 loss: 1.216594  [   64/  126]
train() client id: f_00000-3-2 loss: 1.072982  [   96/  126]
train() client id: f_00000-4-0 loss: 1.089988  [   32/  126]
train() client id: f_00000-4-1 loss: 1.037554  [   64/  126]
train() client id: f_00000-4-2 loss: 0.944746  [   96/  126]
train() client id: f_00000-5-0 loss: 1.091307  [   32/  126]
train() client id: f_00000-5-1 loss: 0.921722  [   64/  126]
train() client id: f_00000-5-2 loss: 0.871446  [   96/  126]
train() client id: f_00000-6-0 loss: 0.773960  [   32/  126]
train() client id: f_00000-6-1 loss: 0.980437  [   64/  126]
train() client id: f_00000-6-2 loss: 1.088679  [   96/  126]
train() client id: f_00000-7-0 loss: 0.889045  [   32/  126]
train() client id: f_00000-7-1 loss: 0.845019  [   64/  126]
train() client id: f_00000-7-2 loss: 0.979809  [   96/  126]
train() client id: f_00000-8-0 loss: 0.806551  [   32/  126]
train() client id: f_00000-8-1 loss: 0.886300  [   64/  126]
train() client id: f_00000-8-2 loss: 0.891112  [   96/  126]
train() client id: f_00000-9-0 loss: 0.870506  [   32/  126]
train() client id: f_00000-9-1 loss: 0.874180  [   64/  126]
train() client id: f_00000-9-2 loss: 0.813774  [   96/  126]
train() client id: f_00000-10-0 loss: 0.880295  [   32/  126]
train() client id: f_00000-10-1 loss: 0.737312  [   64/  126]
train() client id: f_00000-10-2 loss: 0.773767  [   96/  126]
train() client id: f_00000-11-0 loss: 0.692424  [   32/  126]
train() client id: f_00000-11-1 loss: 1.060062  [   64/  126]
train() client id: f_00000-11-2 loss: 0.808686  [   96/  126]
train() client id: f_00000-12-0 loss: 0.874501  [   32/  126]
train() client id: f_00000-12-1 loss: 0.786804  [   64/  126]
train() client id: f_00000-12-2 loss: 0.763033  [   96/  126]
train() client id: f_00000-13-0 loss: 0.933133  [   32/  126]
train() client id: f_00000-13-1 loss: 0.854752  [   64/  126]
train() client id: f_00000-13-2 loss: 0.694506  [   96/  126]
train() client id: f_00000-14-0 loss: 0.733586  [   32/  126]
train() client id: f_00000-14-1 loss: 0.995136  [   64/  126]
train() client id: f_00000-14-2 loss: 0.785735  [   96/  126]
train() client id: f_00000-15-0 loss: 0.887197  [   32/  126]
train() client id: f_00000-15-1 loss: 0.863091  [   64/  126]
train() client id: f_00000-15-2 loss: 0.692866  [   96/  126]
train() client id: f_00000-16-0 loss: 0.780207  [   32/  126]
train() client id: f_00000-16-1 loss: 0.787284  [   64/  126]
train() client id: f_00000-16-2 loss: 0.904139  [   96/  126]
train() client id: f_00001-0-0 loss: 0.429243  [   32/  265]
train() client id: f_00001-0-1 loss: 0.347072  [   64/  265]
train() client id: f_00001-0-2 loss: 0.636768  [   96/  265]
train() client id: f_00001-0-3 loss: 0.413352  [  128/  265]
train() client id: f_00001-0-4 loss: 0.331177  [  160/  265]
train() client id: f_00001-0-5 loss: 0.321766  [  192/  265]
train() client id: f_00001-0-6 loss: 0.317534  [  224/  265]
train() client id: f_00001-0-7 loss: 0.449601  [  256/  265]
train() client id: f_00001-1-0 loss: 0.415118  [   32/  265]
train() client id: f_00001-1-1 loss: 0.423991  [   64/  265]
train() client id: f_00001-1-2 loss: 0.483325  [   96/  265]
train() client id: f_00001-1-3 loss: 0.305120  [  128/  265]
train() client id: f_00001-1-4 loss: 0.308086  [  160/  265]
train() client id: f_00001-1-5 loss: 0.359938  [  192/  265]
train() client id: f_00001-1-6 loss: 0.405474  [  224/  265]
train() client id: f_00001-1-7 loss: 0.453554  [  256/  265]
train() client id: f_00001-2-0 loss: 0.449448  [   32/  265]
train() client id: f_00001-2-1 loss: 0.328735  [   64/  265]
train() client id: f_00001-2-2 loss: 0.361201  [   96/  265]
train() client id: f_00001-2-3 loss: 0.312996  [  128/  265]
train() client id: f_00001-2-4 loss: 0.434911  [  160/  265]
train() client id: f_00001-2-5 loss: 0.355902  [  192/  265]
train() client id: f_00001-2-6 loss: 0.473651  [  224/  265]
train() client id: f_00001-2-7 loss: 0.389097  [  256/  265]
train() client id: f_00001-3-0 loss: 0.416028  [   32/  265]
train() client id: f_00001-3-1 loss: 0.469248  [   64/  265]
train() client id: f_00001-3-2 loss: 0.352927  [   96/  265]
train() client id: f_00001-3-3 loss: 0.273766  [  128/  265]
train() client id: f_00001-3-4 loss: 0.370505  [  160/  265]
train() client id: f_00001-3-5 loss: 0.357389  [  192/  265]
train() client id: f_00001-3-6 loss: 0.536050  [  224/  265]
train() client id: f_00001-3-7 loss: 0.295682  [  256/  265]
train() client id: f_00001-4-0 loss: 0.298386  [   32/  265]
train() client id: f_00001-4-1 loss: 0.350967  [   64/  265]
train() client id: f_00001-4-2 loss: 0.449326  [   96/  265]
train() client id: f_00001-4-3 loss: 0.365485  [  128/  265]
train() client id: f_00001-4-4 loss: 0.424122  [  160/  265]
train() client id: f_00001-4-5 loss: 0.408646  [  192/  265]
train() client id: f_00001-4-6 loss: 0.375373  [  224/  265]
train() client id: f_00001-4-7 loss: 0.354414  [  256/  265]
train() client id: f_00001-5-0 loss: 0.516816  [   32/  265]
train() client id: f_00001-5-1 loss: 0.448262  [   64/  265]
train() client id: f_00001-5-2 loss: 0.345221  [   96/  265]
train() client id: f_00001-5-3 loss: 0.323274  [  128/  265]
train() client id: f_00001-5-4 loss: 0.390940  [  160/  265]
train() client id: f_00001-5-5 loss: 0.335091  [  192/  265]
train() client id: f_00001-5-6 loss: 0.278218  [  224/  265]
train() client id: f_00001-5-7 loss: 0.346973  [  256/  265]
train() client id: f_00001-6-0 loss: 0.502638  [   32/  265]
train() client id: f_00001-6-1 loss: 0.326258  [   64/  265]
train() client id: f_00001-6-2 loss: 0.296964  [   96/  265]
train() client id: f_00001-6-3 loss: 0.366510  [  128/  265]
train() client id: f_00001-6-4 loss: 0.312829  [  160/  265]
train() client id: f_00001-6-5 loss: 0.353155  [  192/  265]
train() client id: f_00001-6-6 loss: 0.279983  [  224/  265]
train() client id: f_00001-6-7 loss: 0.454816  [  256/  265]
train() client id: f_00001-7-0 loss: 0.592297  [   32/  265]
train() client id: f_00001-7-1 loss: 0.300573  [   64/  265]
train() client id: f_00001-7-2 loss: 0.325758  [   96/  265]
train() client id: f_00001-7-3 loss: 0.320735  [  128/  265]
train() client id: f_00001-7-4 loss: 0.365618  [  160/  265]
train() client id: f_00001-7-5 loss: 0.422166  [  192/  265]
train() client id: f_00001-7-6 loss: 0.338548  [  224/  265]
train() client id: f_00001-7-7 loss: 0.294142  [  256/  265]
train() client id: f_00001-8-0 loss: 0.444318  [   32/  265]
train() client id: f_00001-8-1 loss: 0.323481  [   64/  265]
train() client id: f_00001-8-2 loss: 0.388624  [   96/  265]
train() client id: f_00001-8-3 loss: 0.531826  [  128/  265]
train() client id: f_00001-8-4 loss: 0.290907  [  160/  265]
train() client id: f_00001-8-5 loss: 0.270536  [  192/  265]
train() client id: f_00001-8-6 loss: 0.357670  [  224/  265]
train() client id: f_00001-8-7 loss: 0.327311  [  256/  265]
train() client id: f_00001-9-0 loss: 0.316781  [   32/  265]
train() client id: f_00001-9-1 loss: 0.390091  [   64/  265]
train() client id: f_00001-9-2 loss: 0.441575  [   96/  265]
train() client id: f_00001-9-3 loss: 0.359256  [  128/  265]
train() client id: f_00001-9-4 loss: 0.253202  [  160/  265]
train() client id: f_00001-9-5 loss: 0.463055  [  192/  265]
train() client id: f_00001-9-6 loss: 0.319349  [  224/  265]
train() client id: f_00001-9-7 loss: 0.327876  [  256/  265]
train() client id: f_00001-10-0 loss: 0.352714  [   32/  265]
train() client id: f_00001-10-1 loss: 0.330118  [   64/  265]
train() client id: f_00001-10-2 loss: 0.392527  [   96/  265]
train() client id: f_00001-10-3 loss: 0.369777  [  128/  265]
train() client id: f_00001-10-4 loss: 0.351497  [  160/  265]
train() client id: f_00001-10-5 loss: 0.268187  [  192/  265]
train() client id: f_00001-10-6 loss: 0.433566  [  224/  265]
train() client id: f_00001-10-7 loss: 0.381117  [  256/  265]
train() client id: f_00001-11-0 loss: 0.337242  [   32/  265]
train() client id: f_00001-11-1 loss: 0.291779  [   64/  265]
train() client id: f_00001-11-2 loss: 0.322993  [   96/  265]
train() client id: f_00001-11-3 loss: 0.441717  [  128/  265]
train() client id: f_00001-11-4 loss: 0.321594  [  160/  265]
train() client id: f_00001-11-5 loss: 0.423802  [  192/  265]
train() client id: f_00001-11-6 loss: 0.371821  [  224/  265]
train() client id: f_00001-11-7 loss: 0.404048  [  256/  265]
train() client id: f_00001-12-0 loss: 0.277562  [   32/  265]
train() client id: f_00001-12-1 loss: 0.274047  [   64/  265]
train() client id: f_00001-12-2 loss: 0.331342  [   96/  265]
train() client id: f_00001-12-3 loss: 0.335732  [  128/  265]
train() client id: f_00001-12-4 loss: 0.507699  [  160/  265]
train() client id: f_00001-12-5 loss: 0.490634  [  192/  265]
train() client id: f_00001-12-6 loss: 0.394121  [  224/  265]
train() client id: f_00001-12-7 loss: 0.309658  [  256/  265]
train() client id: f_00001-13-0 loss: 0.396780  [   32/  265]
train() client id: f_00001-13-1 loss: 0.356469  [   64/  265]
train() client id: f_00001-13-2 loss: 0.263620  [   96/  265]
train() client id: f_00001-13-3 loss: 0.528754  [  128/  265]
train() client id: f_00001-13-4 loss: 0.288912  [  160/  265]
train() client id: f_00001-13-5 loss: 0.393093  [  192/  265]
train() client id: f_00001-13-6 loss: 0.370913  [  224/  265]
train() client id: f_00001-13-7 loss: 0.318974  [  256/  265]
train() client id: f_00001-14-0 loss: 0.326839  [   32/  265]
train() client id: f_00001-14-1 loss: 0.361955  [   64/  265]
train() client id: f_00001-14-2 loss: 0.464184  [   96/  265]
train() client id: f_00001-14-3 loss: 0.269098  [  128/  265]
train() client id: f_00001-14-4 loss: 0.371515  [  160/  265]
train() client id: f_00001-14-5 loss: 0.357168  [  192/  265]
train() client id: f_00001-14-6 loss: 0.360749  [  224/  265]
train() client id: f_00001-14-7 loss: 0.407182  [  256/  265]
train() client id: f_00001-15-0 loss: 0.298376  [   32/  265]
train() client id: f_00001-15-1 loss: 0.327898  [   64/  265]
train() client id: f_00001-15-2 loss: 0.424353  [   96/  265]
train() client id: f_00001-15-3 loss: 0.401039  [  128/  265]
train() client id: f_00001-15-4 loss: 0.345713  [  160/  265]
train() client id: f_00001-15-5 loss: 0.460810  [  192/  265]
train() client id: f_00001-15-6 loss: 0.310795  [  224/  265]
train() client id: f_00001-15-7 loss: 0.340771  [  256/  265]
train() client id: f_00001-16-0 loss: 0.419602  [   32/  265]
train() client id: f_00001-16-1 loss: 0.434353  [   64/  265]
train() client id: f_00001-16-2 loss: 0.299506  [   96/  265]
train() client id: f_00001-16-3 loss: 0.279690  [  128/  265]
train() client id: f_00001-16-4 loss: 0.416130  [  160/  265]
train() client id: f_00001-16-5 loss: 0.357534  [  192/  265]
train() client id: f_00001-16-6 loss: 0.262168  [  224/  265]
train() client id: f_00001-16-7 loss: 0.420072  [  256/  265]
train() client id: f_00002-0-0 loss: 1.366190  [   32/  124]
train() client id: f_00002-0-1 loss: 1.093204  [   64/  124]
train() client id: f_00002-0-2 loss: 1.119371  [   96/  124]
train() client id: f_00002-1-0 loss: 1.087750  [   32/  124]
train() client id: f_00002-1-1 loss: 1.217125  [   64/  124]
train() client id: f_00002-1-2 loss: 1.229889  [   96/  124]
train() client id: f_00002-2-0 loss: 1.115848  [   32/  124]
train() client id: f_00002-2-1 loss: 1.286019  [   64/  124]
train() client id: f_00002-2-2 loss: 1.033123  [   96/  124]
train() client id: f_00002-3-0 loss: 1.060663  [   32/  124]
train() client id: f_00002-3-1 loss: 1.068680  [   64/  124]
train() client id: f_00002-3-2 loss: 1.115385  [   96/  124]
train() client id: f_00002-4-0 loss: 1.046982  [   32/  124]
train() client id: f_00002-4-1 loss: 0.954869  [   64/  124]
train() client id: f_00002-4-2 loss: 1.029781  [   96/  124]
train() client id: f_00002-5-0 loss: 0.918990  [   32/  124]
train() client id: f_00002-5-1 loss: 0.932286  [   64/  124]
train() client id: f_00002-5-2 loss: 1.151806  [   96/  124]
train() client id: f_00002-6-0 loss: 1.068319  [   32/  124]
train() client id: f_00002-6-1 loss: 0.798542  [   64/  124]
train() client id: f_00002-6-2 loss: 1.100183  [   96/  124]
train() client id: f_00002-7-0 loss: 0.872538  [   32/  124]
train() client id: f_00002-7-1 loss: 0.974389  [   64/  124]
train() client id: f_00002-7-2 loss: 1.055692  [   96/  124]
train() client id: f_00002-8-0 loss: 0.781859  [   32/  124]
train() client id: f_00002-8-1 loss: 1.094785  [   64/  124]
train() client id: f_00002-8-2 loss: 0.883229  [   96/  124]
train() client id: f_00002-9-0 loss: 0.901981  [   32/  124]
train() client id: f_00002-9-1 loss: 0.917031  [   64/  124]
train() client id: f_00002-9-2 loss: 1.013767  [   96/  124]
train() client id: f_00002-10-0 loss: 0.862602  [   32/  124]
train() client id: f_00002-10-1 loss: 1.057117  [   64/  124]
train() client id: f_00002-10-2 loss: 0.757010  [   96/  124]
train() client id: f_00002-11-0 loss: 0.869404  [   32/  124]
train() client id: f_00002-11-1 loss: 0.906632  [   64/  124]
train() client id: f_00002-11-2 loss: 1.058097  [   96/  124]
train() client id: f_00002-12-0 loss: 0.890261  [   32/  124]
train() client id: f_00002-12-1 loss: 0.924610  [   64/  124]
train() client id: f_00002-12-2 loss: 0.745440  [   96/  124]
train() client id: f_00002-13-0 loss: 0.895370  [   32/  124]
train() client id: f_00002-13-1 loss: 0.945756  [   64/  124]
train() client id: f_00002-13-2 loss: 1.027422  [   96/  124]
train() client id: f_00002-14-0 loss: 0.870435  [   32/  124]
train() client id: f_00002-14-1 loss: 0.972030  [   64/  124]
train() client id: f_00002-14-2 loss: 0.778980  [   96/  124]
train() client id: f_00002-15-0 loss: 0.872272  [   32/  124]
train() client id: f_00002-15-1 loss: 1.018629  [   64/  124]
train() client id: f_00002-15-2 loss: 0.763208  [   96/  124]
train() client id: f_00002-16-0 loss: 0.881055  [   32/  124]
train() client id: f_00002-16-1 loss: 0.751818  [   64/  124]
train() client id: f_00002-16-2 loss: 0.854565  [   96/  124]
train() client id: f_00003-0-0 loss: 0.696703  [   32/   43]
train() client id: f_00003-1-0 loss: 0.734297  [   32/   43]
train() client id: f_00003-2-0 loss: 0.670420  [   32/   43]
train() client id: f_00003-3-0 loss: 0.795732  [   32/   43]
train() client id: f_00003-4-0 loss: 0.851693  [   32/   43]
train() client id: f_00003-5-0 loss: 0.962434  [   32/   43]
train() client id: f_00003-6-0 loss: 0.564346  [   32/   43]
train() client id: f_00003-7-0 loss: 0.889847  [   32/   43]
train() client id: f_00003-8-0 loss: 0.782050  [   32/   43]
train() client id: f_00003-9-0 loss: 0.573255  [   32/   43]
train() client id: f_00003-10-0 loss: 0.597404  [   32/   43]
train() client id: f_00003-11-0 loss: 0.783698  [   32/   43]
train() client id: f_00003-12-0 loss: 0.723884  [   32/   43]
train() client id: f_00003-13-0 loss: 0.800806  [   32/   43]
train() client id: f_00003-14-0 loss: 0.783929  [   32/   43]
train() client id: f_00003-15-0 loss: 0.903453  [   32/   43]
train() client id: f_00003-16-0 loss: 0.634676  [   32/   43]
train() client id: f_00004-0-0 loss: 0.818279  [   32/  306]
train() client id: f_00004-0-1 loss: 1.015909  [   64/  306]
train() client id: f_00004-0-2 loss: 1.028859  [   96/  306]
train() client id: f_00004-0-3 loss: 1.014298  [  128/  306]
train() client id: f_00004-0-4 loss: 1.052670  [  160/  306]
train() client id: f_00004-0-5 loss: 1.016773  [  192/  306]
train() client id: f_00004-0-6 loss: 0.858823  [  224/  306]
train() client id: f_00004-0-7 loss: 0.873326  [  256/  306]
train() client id: f_00004-0-8 loss: 0.800660  [  288/  306]
train() client id: f_00004-1-0 loss: 0.900969  [   32/  306]
train() client id: f_00004-1-1 loss: 0.937108  [   64/  306]
train() client id: f_00004-1-2 loss: 1.102720  [   96/  306]
train() client id: f_00004-1-3 loss: 0.975474  [  128/  306]
train() client id: f_00004-1-4 loss: 0.923871  [  160/  306]
train() client id: f_00004-1-5 loss: 0.961007  [  192/  306]
train() client id: f_00004-1-6 loss: 0.828410  [  224/  306]
train() client id: f_00004-1-7 loss: 0.836159  [  256/  306]
train() client id: f_00004-1-8 loss: 0.874767  [  288/  306]
train() client id: f_00004-2-0 loss: 0.923908  [   32/  306]
train() client id: f_00004-2-1 loss: 0.826480  [   64/  306]
train() client id: f_00004-2-2 loss: 0.938253  [   96/  306]
train() client id: f_00004-2-3 loss: 1.062369  [  128/  306]
train() client id: f_00004-2-4 loss: 1.025739  [  160/  306]
train() client id: f_00004-2-5 loss: 0.946815  [  192/  306]
train() client id: f_00004-2-6 loss: 0.976964  [  224/  306]
train() client id: f_00004-2-7 loss: 0.872787  [  256/  306]
train() client id: f_00004-2-8 loss: 0.811772  [  288/  306]
train() client id: f_00004-3-0 loss: 1.001460  [   32/  306]
train() client id: f_00004-3-1 loss: 0.763588  [   64/  306]
train() client id: f_00004-3-2 loss: 0.849023  [   96/  306]
train() client id: f_00004-3-3 loss: 0.942432  [  128/  306]
train() client id: f_00004-3-4 loss: 0.863150  [  160/  306]
train() client id: f_00004-3-5 loss: 0.846610  [  192/  306]
train() client id: f_00004-3-6 loss: 1.082970  [  224/  306]
train() client id: f_00004-3-7 loss: 1.042397  [  256/  306]
train() client id: f_00004-3-8 loss: 0.921526  [  288/  306]
train() client id: f_00004-4-0 loss: 0.995900  [   32/  306]
train() client id: f_00004-4-1 loss: 0.934356  [   64/  306]
train() client id: f_00004-4-2 loss: 0.845144  [   96/  306]
train() client id: f_00004-4-3 loss: 1.013717  [  128/  306]
train() client id: f_00004-4-4 loss: 0.855440  [  160/  306]
train() client id: f_00004-4-5 loss: 0.892938  [  192/  306]
train() client id: f_00004-4-6 loss: 0.956866  [  224/  306]
train() client id: f_00004-4-7 loss: 0.915596  [  256/  306]
train() client id: f_00004-4-8 loss: 0.847614  [  288/  306]
train() client id: f_00004-5-0 loss: 1.018542  [   32/  306]
train() client id: f_00004-5-1 loss: 0.937504  [   64/  306]
train() client id: f_00004-5-2 loss: 0.850281  [   96/  306]
train() client id: f_00004-5-3 loss: 0.905720  [  128/  306]
train() client id: f_00004-5-4 loss: 0.960120  [  160/  306]
train() client id: f_00004-5-5 loss: 0.848909  [  192/  306]
train() client id: f_00004-5-6 loss: 0.947212  [  224/  306]
train() client id: f_00004-5-7 loss: 0.933567  [  256/  306]
train() client id: f_00004-5-8 loss: 0.821322  [  288/  306]
train() client id: f_00004-6-0 loss: 0.828212  [   32/  306]
train() client id: f_00004-6-1 loss: 1.140917  [   64/  306]
train() client id: f_00004-6-2 loss: 0.840350  [   96/  306]
train() client id: f_00004-6-3 loss: 0.878551  [  128/  306]
train() client id: f_00004-6-4 loss: 0.979412  [  160/  306]
train() client id: f_00004-6-5 loss: 0.976380  [  192/  306]
train() client id: f_00004-6-6 loss: 0.861829  [  224/  306]
train() client id: f_00004-6-7 loss: 0.768702  [  256/  306]
train() client id: f_00004-6-8 loss: 0.861527  [  288/  306]
train() client id: f_00004-7-0 loss: 0.939110  [   32/  306]
train() client id: f_00004-7-1 loss: 0.919829  [   64/  306]
train() client id: f_00004-7-2 loss: 0.739934  [   96/  306]
train() client id: f_00004-7-3 loss: 0.853018  [  128/  306]
train() client id: f_00004-7-4 loss: 1.136122  [  160/  306]
train() client id: f_00004-7-5 loss: 0.887028  [  192/  306]
train() client id: f_00004-7-6 loss: 0.876872  [  224/  306]
train() client id: f_00004-7-7 loss: 0.946343  [  256/  306]
train() client id: f_00004-7-8 loss: 0.864165  [  288/  306]
train() client id: f_00004-8-0 loss: 0.887543  [   32/  306]
train() client id: f_00004-8-1 loss: 0.999057  [   64/  306]
train() client id: f_00004-8-2 loss: 0.987786  [   96/  306]
train() client id: f_00004-8-3 loss: 0.902572  [  128/  306]
train() client id: f_00004-8-4 loss: 0.826770  [  160/  306]
train() client id: f_00004-8-5 loss: 0.951783  [  192/  306]
train() client id: f_00004-8-6 loss: 0.767662  [  224/  306]
train() client id: f_00004-8-7 loss: 0.902620  [  256/  306]
train() client id: f_00004-8-8 loss: 0.990369  [  288/  306]
train() client id: f_00004-9-0 loss: 0.976060  [   32/  306]
train() client id: f_00004-9-1 loss: 0.955758  [   64/  306]
train() client id: f_00004-9-2 loss: 0.983592  [   96/  306]
train() client id: f_00004-9-3 loss: 0.842638  [  128/  306]
train() client id: f_00004-9-4 loss: 0.912873  [  160/  306]
train() client id: f_00004-9-5 loss: 0.899862  [  192/  306]
train() client id: f_00004-9-6 loss: 0.879749  [  224/  306]
train() client id: f_00004-9-7 loss: 0.840232  [  256/  306]
train() client id: f_00004-9-8 loss: 0.868113  [  288/  306]
train() client id: f_00004-10-0 loss: 0.887803  [   32/  306]
train() client id: f_00004-10-1 loss: 0.942359  [   64/  306]
train() client id: f_00004-10-2 loss: 0.850163  [   96/  306]
train() client id: f_00004-10-3 loss: 0.958149  [  128/  306]
train() client id: f_00004-10-4 loss: 0.978885  [  160/  306]
train() client id: f_00004-10-5 loss: 0.930463  [  192/  306]
train() client id: f_00004-10-6 loss: 0.879557  [  224/  306]
train() client id: f_00004-10-7 loss: 0.823400  [  256/  306]
train() client id: f_00004-10-8 loss: 0.891424  [  288/  306]
train() client id: f_00004-11-0 loss: 0.771603  [   32/  306]
train() client id: f_00004-11-1 loss: 0.875496  [   64/  306]
train() client id: f_00004-11-2 loss: 1.051930  [   96/  306]
train() client id: f_00004-11-3 loss: 0.905306  [  128/  306]
train() client id: f_00004-11-4 loss: 0.841877  [  160/  306]
train() client id: f_00004-11-5 loss: 0.890285  [  192/  306]
train() client id: f_00004-11-6 loss: 0.956419  [  224/  306]
train() client id: f_00004-11-7 loss: 0.801993  [  256/  306]
train() client id: f_00004-11-8 loss: 0.937992  [  288/  306]
train() client id: f_00004-12-0 loss: 0.855828  [   32/  306]
train() client id: f_00004-12-1 loss: 0.877970  [   64/  306]
train() client id: f_00004-12-2 loss: 0.918316  [   96/  306]
train() client id: f_00004-12-3 loss: 0.806843  [  128/  306]
train() client id: f_00004-12-4 loss: 0.902165  [  160/  306]
train() client id: f_00004-12-5 loss: 1.021656  [  192/  306]
train() client id: f_00004-12-6 loss: 0.968416  [  224/  306]
train() client id: f_00004-12-7 loss: 0.893341  [  256/  306]
train() client id: f_00004-12-8 loss: 0.757918  [  288/  306]
train() client id: f_00004-13-0 loss: 0.939160  [   32/  306]
train() client id: f_00004-13-1 loss: 0.905085  [   64/  306]
train() client id: f_00004-13-2 loss: 0.890048  [   96/  306]
train() client id: f_00004-13-3 loss: 0.945069  [  128/  306]
train() client id: f_00004-13-4 loss: 0.894099  [  160/  306]
train() client id: f_00004-13-5 loss: 0.856697  [  192/  306]
train() client id: f_00004-13-6 loss: 0.838936  [  224/  306]
train() client id: f_00004-13-7 loss: 0.882620  [  256/  306]
train() client id: f_00004-13-8 loss: 0.832419  [  288/  306]
train() client id: f_00004-14-0 loss: 0.921371  [   32/  306]
train() client id: f_00004-14-1 loss: 0.875937  [   64/  306]
train() client id: f_00004-14-2 loss: 0.786049  [   96/  306]
train() client id: f_00004-14-3 loss: 0.944590  [  128/  306]
train() client id: f_00004-14-4 loss: 0.850884  [  160/  306]
train() client id: f_00004-14-5 loss: 0.748818  [  192/  306]
train() client id: f_00004-14-6 loss: 1.004823  [  224/  306]
train() client id: f_00004-14-7 loss: 0.876675  [  256/  306]
train() client id: f_00004-14-8 loss: 0.892810  [  288/  306]
train() client id: f_00004-15-0 loss: 0.876862  [   32/  306]
train() client id: f_00004-15-1 loss: 0.900655  [   64/  306]
train() client id: f_00004-15-2 loss: 0.848089  [   96/  306]
train() client id: f_00004-15-3 loss: 0.764259  [  128/  306]
train() client id: f_00004-15-4 loss: 0.879484  [  160/  306]
train() client id: f_00004-15-5 loss: 0.827081  [  192/  306]
train() client id: f_00004-15-6 loss: 0.949461  [  224/  306]
train() client id: f_00004-15-7 loss: 0.940716  [  256/  306]
train() client id: f_00004-15-8 loss: 0.846216  [  288/  306]
train() client id: f_00004-16-0 loss: 0.868768  [   32/  306]
train() client id: f_00004-16-1 loss: 0.915104  [   64/  306]
train() client id: f_00004-16-2 loss: 0.805272  [   96/  306]
train() client id: f_00004-16-3 loss: 0.924520  [  128/  306]
train() client id: f_00004-16-4 loss: 0.880925  [  160/  306]
train() client id: f_00004-16-5 loss: 0.850383  [  192/  306]
train() client id: f_00004-16-6 loss: 0.930005  [  224/  306]
train() client id: f_00004-16-7 loss: 0.829144  [  256/  306]
train() client id: f_00004-16-8 loss: 0.846752  [  288/  306]
train() client id: f_00005-0-0 loss: 0.586914  [   32/  146]
train() client id: f_00005-0-1 loss: 0.327350  [   64/  146]
train() client id: f_00005-0-2 loss: 0.448568  [   96/  146]
train() client id: f_00005-0-3 loss: 0.632142  [  128/  146]
train() client id: f_00005-1-0 loss: 0.504272  [   32/  146]
train() client id: f_00005-1-1 loss: 0.252991  [   64/  146]
train() client id: f_00005-1-2 loss: 0.646796  [   96/  146]
train() client id: f_00005-1-3 loss: 0.619752  [  128/  146]
train() client id: f_00005-2-0 loss: 0.417779  [   32/  146]
train() client id: f_00005-2-1 loss: 0.605183  [   64/  146]
train() client id: f_00005-2-2 loss: 0.310294  [   96/  146]
train() client id: f_00005-2-3 loss: 0.679123  [  128/  146]
train() client id: f_00005-3-0 loss: 0.502690  [   32/  146]
train() client id: f_00005-3-1 loss: 0.379972  [   64/  146]
train() client id: f_00005-3-2 loss: 0.553889  [   96/  146]
train() client id: f_00005-3-3 loss: 0.394103  [  128/  146]
train() client id: f_00005-4-0 loss: 0.314064  [   32/  146]
train() client id: f_00005-4-1 loss: 0.399020  [   64/  146]
train() client id: f_00005-4-2 loss: 0.541767  [   96/  146]
train() client id: f_00005-4-3 loss: 0.572248  [  128/  146]
train() client id: f_00005-5-0 loss: 0.526727  [   32/  146]
train() client id: f_00005-5-1 loss: 0.343683  [   64/  146]
train() client id: f_00005-5-2 loss: 0.494285  [   96/  146]
train() client id: f_00005-5-3 loss: 0.569417  [  128/  146]
train() client id: f_00005-6-0 loss: 0.385815  [   32/  146]
train() client id: f_00005-6-1 loss: 0.563420  [   64/  146]
train() client id: f_00005-6-2 loss: 0.481218  [   96/  146]
train() client id: f_00005-6-3 loss: 0.550649  [  128/  146]
train() client id: f_00005-7-0 loss: 0.548803  [   32/  146]
train() client id: f_00005-7-1 loss: 0.643985  [   64/  146]
train() client id: f_00005-7-2 loss: 0.380375  [   96/  146]
train() client id: f_00005-7-3 loss: 0.413254  [  128/  146]
train() client id: f_00005-8-0 loss: 0.702125  [   32/  146]
train() client id: f_00005-8-1 loss: 0.592076  [   64/  146]
train() client id: f_00005-8-2 loss: 0.455517  [   96/  146]
train() client id: f_00005-8-3 loss: 0.201820  [  128/  146]
train() client id: f_00005-9-0 loss: 0.653191  [   32/  146]
train() client id: f_00005-9-1 loss: 0.245443  [   64/  146]
train() client id: f_00005-9-2 loss: 0.577915  [   96/  146]
train() client id: f_00005-9-3 loss: 0.424141  [  128/  146]
train() client id: f_00005-10-0 loss: 0.404301  [   32/  146]
train() client id: f_00005-10-1 loss: 0.412261  [   64/  146]
train() client id: f_00005-10-2 loss: 0.440347  [   96/  146]
train() client id: f_00005-10-3 loss: 0.538824  [  128/  146]
train() client id: f_00005-11-0 loss: 0.474676  [   32/  146]
train() client id: f_00005-11-1 loss: 0.696673  [   64/  146]
train() client id: f_00005-11-2 loss: 0.363214  [   96/  146]
train() client id: f_00005-11-3 loss: 0.452666  [  128/  146]
train() client id: f_00005-12-0 loss: 0.483616  [   32/  146]
train() client id: f_00005-12-1 loss: 0.350414  [   64/  146]
train() client id: f_00005-12-2 loss: 0.698454  [   96/  146]
train() client id: f_00005-12-3 loss: 0.294484  [  128/  146]
train() client id: f_00005-13-0 loss: 0.655132  [   32/  146]
train() client id: f_00005-13-1 loss: 0.471755  [   64/  146]
train() client id: f_00005-13-2 loss: 0.412134  [   96/  146]
train() client id: f_00005-13-3 loss: 0.149359  [  128/  146]
train() client id: f_00005-14-0 loss: 0.322207  [   32/  146]
train() client id: f_00005-14-1 loss: 0.360940  [   64/  146]
train() client id: f_00005-14-2 loss: 0.399068  [   96/  146]
train() client id: f_00005-14-3 loss: 0.476253  [  128/  146]
train() client id: f_00005-15-0 loss: 0.275688  [   32/  146]
train() client id: f_00005-15-1 loss: 0.361433  [   64/  146]
train() client id: f_00005-15-2 loss: 0.577128  [   96/  146]
train() client id: f_00005-15-3 loss: 0.514407  [  128/  146]
train() client id: f_00005-16-0 loss: 0.823196  [   32/  146]
train() client id: f_00005-16-1 loss: 0.395289  [   64/  146]
train() client id: f_00005-16-2 loss: 0.299076  [   96/  146]
train() client id: f_00005-16-3 loss: 0.436732  [  128/  146]
train() client id: f_00006-0-0 loss: 0.490764  [   32/   54]
train() client id: f_00006-1-0 loss: 0.486925  [   32/   54]
train() client id: f_00006-2-0 loss: 0.456368  [   32/   54]
train() client id: f_00006-3-0 loss: 0.524120  [   32/   54]
train() client id: f_00006-4-0 loss: 0.504123  [   32/   54]
train() client id: f_00006-5-0 loss: 0.560319  [   32/   54]
train() client id: f_00006-6-0 loss: 0.526372  [   32/   54]
train() client id: f_00006-7-0 loss: 0.491756  [   32/   54]
train() client id: f_00006-8-0 loss: 0.506966  [   32/   54]
train() client id: f_00006-9-0 loss: 0.515586  [   32/   54]
train() client id: f_00006-10-0 loss: 0.506853  [   32/   54]
train() client id: f_00006-11-0 loss: 0.546461  [   32/   54]
train() client id: f_00006-12-0 loss: 0.543580  [   32/   54]
train() client id: f_00006-13-0 loss: 0.476453  [   32/   54]
train() client id: f_00006-14-0 loss: 0.448592  [   32/   54]
train() client id: f_00006-15-0 loss: 0.553169  [   32/   54]
train() client id: f_00006-16-0 loss: 0.460433  [   32/   54]
train() client id: f_00007-0-0 loss: 0.481273  [   32/  179]
train() client id: f_00007-0-1 loss: 0.656720  [   64/  179]
train() client id: f_00007-0-2 loss: 0.382912  [   96/  179]
train() client id: f_00007-0-3 loss: 0.463051  [  128/  179]
train() client id: f_00007-0-4 loss: 0.631919  [  160/  179]
train() client id: f_00007-1-0 loss: 0.476271  [   32/  179]
train() client id: f_00007-1-1 loss: 0.407088  [   64/  179]
train() client id: f_00007-1-2 loss: 0.563119  [   96/  179]
train() client id: f_00007-1-3 loss: 0.438207  [  128/  179]
train() client id: f_00007-1-4 loss: 0.668491  [  160/  179]
train() client id: f_00007-2-0 loss: 0.435401  [   32/  179]
train() client id: f_00007-2-1 loss: 0.476949  [   64/  179]
train() client id: f_00007-2-2 loss: 0.456061  [   96/  179]
train() client id: f_00007-2-3 loss: 0.587896  [  128/  179]
train() client id: f_00007-2-4 loss: 0.409399  [  160/  179]
train() client id: f_00007-3-0 loss: 0.421692  [   32/  179]
train() client id: f_00007-3-1 loss: 0.354271  [   64/  179]
train() client id: f_00007-3-2 loss: 0.397868  [   96/  179]
train() client id: f_00007-3-3 loss: 0.600660  [  128/  179]
train() client id: f_00007-3-4 loss: 0.597134  [  160/  179]
train() client id: f_00007-4-0 loss: 0.404411  [   32/  179]
train() client id: f_00007-4-1 loss: 0.546048  [   64/  179]
train() client id: f_00007-4-2 loss: 0.634837  [   96/  179]
train() client id: f_00007-4-3 loss: 0.480145  [  128/  179]
train() client id: f_00007-4-4 loss: 0.331211  [  160/  179]
train() client id: f_00007-5-0 loss: 0.660798  [   32/  179]
train() client id: f_00007-5-1 loss: 0.384421  [   64/  179]
train() client id: f_00007-5-2 loss: 0.305802  [   96/  179]
train() client id: f_00007-5-3 loss: 0.526411  [  128/  179]
train() client id: f_00007-5-4 loss: 0.459832  [  160/  179]
train() client id: f_00007-6-0 loss: 0.321233  [   32/  179]
train() client id: f_00007-6-1 loss: 0.454201  [   64/  179]
train() client id: f_00007-6-2 loss: 0.349675  [   96/  179]
train() client id: f_00007-6-3 loss: 0.486538  [  128/  179]
train() client id: f_00007-6-4 loss: 0.513547  [  160/  179]
train() client id: f_00007-7-0 loss: 0.499994  [   32/  179]
train() client id: f_00007-7-1 loss: 0.384213  [   64/  179]
train() client id: f_00007-7-2 loss: 0.342787  [   96/  179]
train() client id: f_00007-7-3 loss: 0.567450  [  128/  179]
train() client id: f_00007-7-4 loss: 0.463668  [  160/  179]
train() client id: f_00007-8-0 loss: 0.464786  [   32/  179]
train() client id: f_00007-8-1 loss: 0.649571  [   64/  179]
train() client id: f_00007-8-2 loss: 0.363686  [   96/  179]
train() client id: f_00007-8-3 loss: 0.466504  [  128/  179]
train() client id: f_00007-8-4 loss: 0.282040  [  160/  179]
train() client id: f_00007-9-0 loss: 0.467819  [   32/  179]
train() client id: f_00007-9-1 loss: 0.641549  [   64/  179]
train() client id: f_00007-9-2 loss: 0.267467  [   96/  179]
train() client id: f_00007-9-3 loss: 0.342152  [  128/  179]
train() client id: f_00007-9-4 loss: 0.432698  [  160/  179]
train() client id: f_00007-10-0 loss: 0.318138  [   32/  179]
train() client id: f_00007-10-1 loss: 0.450257  [   64/  179]
train() client id: f_00007-10-2 loss: 0.437239  [   96/  179]
train() client id: f_00007-10-3 loss: 0.272044  [  128/  179]
train() client id: f_00007-10-4 loss: 0.612122  [  160/  179]
train() client id: f_00007-11-0 loss: 0.374892  [   32/  179]
train() client id: f_00007-11-1 loss: 0.540982  [   64/  179]
train() client id: f_00007-11-2 loss: 0.383151  [   96/  179]
train() client id: f_00007-11-3 loss: 0.255531  [  128/  179]
train() client id: f_00007-11-4 loss: 0.426275  [  160/  179]
train() client id: f_00007-12-0 loss: 0.372493  [   32/  179]
train() client id: f_00007-12-1 loss: 0.436971  [   64/  179]
train() client id: f_00007-12-2 loss: 0.546445  [   96/  179]
train() client id: f_00007-12-3 loss: 0.442982  [  128/  179]
train() client id: f_00007-12-4 loss: 0.255575  [  160/  179]
train() client id: f_00007-13-0 loss: 0.287933  [   32/  179]
train() client id: f_00007-13-1 loss: 0.347761  [   64/  179]
train() client id: f_00007-13-2 loss: 0.540052  [   96/  179]
train() client id: f_00007-13-3 loss: 0.466249  [  128/  179]
train() client id: f_00007-13-4 loss: 0.361915  [  160/  179]
train() client id: f_00007-14-0 loss: 0.555177  [   32/  179]
train() client id: f_00007-14-1 loss: 0.545777  [   64/  179]
train() client id: f_00007-14-2 loss: 0.336459  [   96/  179]
train() client id: f_00007-14-3 loss: 0.383754  [  128/  179]
train() client id: f_00007-14-4 loss: 0.272784  [  160/  179]
train() client id: f_00007-15-0 loss: 0.353558  [   32/  179]
train() client id: f_00007-15-1 loss: 0.365968  [   64/  179]
train() client id: f_00007-15-2 loss: 0.420043  [   96/  179]
train() client id: f_00007-15-3 loss: 0.469942  [  128/  179]
train() client id: f_00007-15-4 loss: 0.557910  [  160/  179]
train() client id: f_00007-16-0 loss: 0.245080  [   32/  179]
train() client id: f_00007-16-1 loss: 0.517734  [   64/  179]
train() client id: f_00007-16-2 loss: 0.519129  [   96/  179]
train() client id: f_00007-16-3 loss: 0.258686  [  128/  179]
train() client id: f_00007-16-4 loss: 0.610510  [  160/  179]
train() client id: f_00008-0-0 loss: 0.637697  [   32/  130]
train() client id: f_00008-0-1 loss: 0.623444  [   64/  130]
train() client id: f_00008-0-2 loss: 0.628495  [   96/  130]
train() client id: f_00008-0-3 loss: 0.628405  [  128/  130]
train() client id: f_00008-1-0 loss: 0.609027  [   32/  130]
train() client id: f_00008-1-1 loss: 0.657071  [   64/  130]
train() client id: f_00008-1-2 loss: 0.595174  [   96/  130]
train() client id: f_00008-1-3 loss: 0.683661  [  128/  130]
train() client id: f_00008-2-0 loss: 0.582000  [   32/  130]
train() client id: f_00008-2-1 loss: 0.625284  [   64/  130]
train() client id: f_00008-2-2 loss: 0.742717  [   96/  130]
train() client id: f_00008-2-3 loss: 0.601143  [  128/  130]
train() client id: f_00008-3-0 loss: 0.642543  [   32/  130]
train() client id: f_00008-3-1 loss: 0.630896  [   64/  130]
train() client id: f_00008-3-2 loss: 0.603173  [   96/  130]
train() client id: f_00008-3-3 loss: 0.678279  [  128/  130]
train() client id: f_00008-4-0 loss: 0.656116  [   32/  130]
train() client id: f_00008-4-1 loss: 0.722423  [   64/  130]
train() client id: f_00008-4-2 loss: 0.585707  [   96/  130]
train() client id: f_00008-4-3 loss: 0.595135  [  128/  130]
train() client id: f_00008-5-0 loss: 0.634400  [   32/  130]
train() client id: f_00008-5-1 loss: 0.690199  [   64/  130]
train() client id: f_00008-5-2 loss: 0.567678  [   96/  130]
train() client id: f_00008-5-3 loss: 0.659717  [  128/  130]
train() client id: f_00008-6-0 loss: 0.583672  [   32/  130]
train() client id: f_00008-6-1 loss: 0.680134  [   64/  130]
train() client id: f_00008-6-2 loss: 0.646504  [   96/  130]
train() client id: f_00008-6-3 loss: 0.615253  [  128/  130]
train() client id: f_00008-7-0 loss: 0.610008  [   32/  130]
train() client id: f_00008-7-1 loss: 0.599449  [   64/  130]
train() client id: f_00008-7-2 loss: 0.680102  [   96/  130]
train() client id: f_00008-7-3 loss: 0.656295  [  128/  130]
train() client id: f_00008-8-0 loss: 0.621201  [   32/  130]
train() client id: f_00008-8-1 loss: 0.591560  [   64/  130]
train() client id: f_00008-8-2 loss: 0.650891  [   96/  130]
train() client id: f_00008-8-3 loss: 0.633473  [  128/  130]
train() client id: f_00008-9-0 loss: 0.641583  [   32/  130]
train() client id: f_00008-9-1 loss: 0.655535  [   64/  130]
train() client id: f_00008-9-2 loss: 0.547026  [   96/  130]
train() client id: f_00008-9-3 loss: 0.705767  [  128/  130]
train() client id: f_00008-10-0 loss: 0.754394  [   32/  130]
train() client id: f_00008-10-1 loss: 0.626600  [   64/  130]
train() client id: f_00008-10-2 loss: 0.541457  [   96/  130]
train() client id: f_00008-10-3 loss: 0.608513  [  128/  130]
train() client id: f_00008-11-0 loss: 0.601080  [   32/  130]
train() client id: f_00008-11-1 loss: 0.582600  [   64/  130]
train() client id: f_00008-11-2 loss: 0.673486  [   96/  130]
train() client id: f_00008-11-3 loss: 0.683928  [  128/  130]
train() client id: f_00008-12-0 loss: 0.758538  [   32/  130]
train() client id: f_00008-12-1 loss: 0.596819  [   64/  130]
train() client id: f_00008-12-2 loss: 0.659351  [   96/  130]
train() client id: f_00008-12-3 loss: 0.524684  [  128/  130]
train() client id: f_00008-13-0 loss: 0.680483  [   32/  130]
train() client id: f_00008-13-1 loss: 0.656213  [   64/  130]
train() client id: f_00008-13-2 loss: 0.734478  [   96/  130]
train() client id: f_00008-13-3 loss: 0.472267  [  128/  130]
train() client id: f_00008-14-0 loss: 0.680854  [   32/  130]
train() client id: f_00008-14-1 loss: 0.592893  [   64/  130]
train() client id: f_00008-14-2 loss: 0.669839  [   96/  130]
train() client id: f_00008-14-3 loss: 0.601612  [  128/  130]
train() client id: f_00008-15-0 loss: 0.650384  [   32/  130]
train() client id: f_00008-15-1 loss: 0.593253  [   64/  130]
train() client id: f_00008-15-2 loss: 0.605519  [   96/  130]
train() client id: f_00008-15-3 loss: 0.690574  [  128/  130]
train() client id: f_00008-16-0 loss: 0.640878  [   32/  130]
train() client id: f_00008-16-1 loss: 0.627182  [   64/  130]
train() client id: f_00008-16-2 loss: 0.623664  [   96/  130]
train() client id: f_00008-16-3 loss: 0.639494  [  128/  130]
train() client id: f_00009-0-0 loss: 1.060382  [   32/  118]
train() client id: f_00009-0-1 loss: 1.209943  [   64/  118]
train() client id: f_00009-0-2 loss: 1.025600  [   96/  118]
train() client id: f_00009-1-0 loss: 1.000668  [   32/  118]
train() client id: f_00009-1-1 loss: 1.014221  [   64/  118]
train() client id: f_00009-1-2 loss: 0.974279  [   96/  118]
train() client id: f_00009-2-0 loss: 0.994948  [   32/  118]
train() client id: f_00009-2-1 loss: 0.865025  [   64/  118]
train() client id: f_00009-2-2 loss: 0.986273  [   96/  118]
train() client id: f_00009-3-0 loss: 0.947522  [   32/  118]
train() client id: f_00009-3-1 loss: 1.009673  [   64/  118]
train() client id: f_00009-3-2 loss: 0.805604  [   96/  118]
train() client id: f_00009-4-0 loss: 0.928896  [   32/  118]
train() client id: f_00009-4-1 loss: 0.646774  [   64/  118]
train() client id: f_00009-4-2 loss: 0.935510  [   96/  118]
train() client id: f_00009-5-0 loss: 0.753964  [   32/  118]
train() client id: f_00009-5-1 loss: 0.940865  [   64/  118]
train() client id: f_00009-5-2 loss: 0.792108  [   96/  118]
train() client id: f_00009-6-0 loss: 0.763013  [   32/  118]
train() client id: f_00009-6-1 loss: 0.758955  [   64/  118]
train() client id: f_00009-6-2 loss: 0.742657  [   96/  118]
train() client id: f_00009-7-0 loss: 0.718917  [   32/  118]
train() client id: f_00009-7-1 loss: 0.769598  [   64/  118]
train() client id: f_00009-7-2 loss: 0.763830  [   96/  118]
train() client id: f_00009-8-0 loss: 0.737614  [   32/  118]
train() client id: f_00009-8-1 loss: 0.690471  [   64/  118]
train() client id: f_00009-8-2 loss: 0.760752  [   96/  118]
train() client id: f_00009-9-0 loss: 0.739007  [   32/  118]
train() client id: f_00009-9-1 loss: 0.651578  [   64/  118]
train() client id: f_00009-9-2 loss: 0.765905  [   96/  118]
train() client id: f_00009-10-0 loss: 0.791840  [   32/  118]
train() client id: f_00009-10-1 loss: 0.850569  [   64/  118]
train() client id: f_00009-10-2 loss: 0.509365  [   96/  118]
train() client id: f_00009-11-0 loss: 0.655094  [   32/  118]
train() client id: f_00009-11-1 loss: 0.658130  [   64/  118]
train() client id: f_00009-11-2 loss: 0.706718  [   96/  118]
train() client id: f_00009-12-0 loss: 0.731357  [   32/  118]
train() client id: f_00009-12-1 loss: 0.671414  [   64/  118]
train() client id: f_00009-12-2 loss: 0.521105  [   96/  118]
train() client id: f_00009-13-0 loss: 0.786164  [   32/  118]
train() client id: f_00009-13-1 loss: 0.633039  [   64/  118]
train() client id: f_00009-13-2 loss: 0.626221  [   96/  118]
train() client id: f_00009-14-0 loss: 0.716838  [   32/  118]
train() client id: f_00009-14-1 loss: 0.522920  [   64/  118]
train() client id: f_00009-14-2 loss: 0.629813  [   96/  118]
train() client id: f_00009-15-0 loss: 0.576733  [   32/  118]
train() client id: f_00009-15-1 loss: 0.747177  [   64/  118]
train() client id: f_00009-15-2 loss: 0.510276  [   96/  118]
train() client id: f_00009-16-0 loss: 0.497582  [   32/  118]
train() client id: f_00009-16-1 loss: 0.598921  [   64/  118]
train() client id: f_00009-16-2 loss: 0.725427  [   96/  118]
At round 55 accuracy: 0.6472148541114059
At round 55 training accuracy: 0.590878604963112
At round 55 training loss: 0.8378603569140523
update_location
xs = 8.927491 396.223621 5.882650 0.934260 -312.581990 -160.230757 -120.849135 -5.143845 -335.120581 20.134486 
ys = -387.390647 7.291448 285.684448 -107.290817 -9.642386 0.794442 -1.381692 281.628436 25.881276 -822.232496 
xs mean: -50.182379970521225
ys mean: -72.66579882624052
dists_uav = 400.188972 408.713008 302.737856 146.670353 328.329828 188.877015 156.864344 298.899708 350.678834 828.535862 
uav_gains = -121.410356 -121.714816 -115.956726 -104.161937 -117.861544 -106.985744 -104.896282 -115.638403 -119.209254 -129.927555 
uav_gains_db_mean: -115.77626178488813
dists_bs = 591.703103 595.334010 212.060322 332.627373 230.264228 174.830509 184.506958 200.551083 218.803552 1016.144733 
bs_gains = -117.185967 -117.260359 -104.707923 -110.181883 -105.709400 -102.360341 -103.015416 -104.029359 -105.088581 -123.761815 
bs_gains_db_mean: -109.33010449204558
Round 56
-------------------------------
ene_coms = [0.03374271 0.03526537 0.007827   0.00761421 0.00824892 0.00885487
 0.00790312 0.00756424 0.00798233 0.04846648]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.76138133 7.5862818  3.47094764 1.24818081 8.46480185 4.08348102
 1.55240176 4.97653151 3.63686588 3.67589283]
obj_prev = 42.456766414291494
eta_min = 3.6925938354187283e-26	eta_max = 0.8381922756976415
af = 8.788159325904157	bf = 1.56122943278155	zeta = 9.666975258494574	eta = 0.909090909090909
af = 8.788159325904157	bf = 1.56122943278155	zeta = 26.38713526110779	eta = 0.3330471170493864
af = 8.788159325904157	bf = 1.56122943278155	zeta = 16.828112951199973	eta = 0.5222308259630201
af = 8.788159325904157	bf = 1.56122943278155	zeta = 15.217428852651427	eta = 0.5775061878717404
af = 8.788159325904157	bf = 1.56122943278155	zeta = 15.115597347834013	eta = 0.5813967601593631
af = 8.788159325904157	bf = 1.56122943278155	zeta = 15.115125186962974	eta = 0.5814149216232809
af = 8.788159325904157	bf = 1.56122943278155	zeta = 15.115125176720083	eta = 0.5814149220172816
eta = 0.5814149220172816
ene_coms = [0.03374271 0.03526537 0.007827   0.00761421 0.00824892 0.00885487
 0.00790312 0.00756424 0.00798233 0.04846648]
ene_comp = [0.04474852 0.09411396 0.04403823 0.01527132 0.10867498 0.05185146
 0.01917794 0.06357131 0.04616911 0.04190735]
ene_total = [1.68764709 2.78179648 1.11515891 0.49206393 2.51399146 1.30525233
 0.58227225 1.52949187 1.16431505 1.9431358 ]
ti_comp = [0.92518786 0.90996131 1.18434499 1.18647285 1.18012573 1.17406627
 1.18358379 1.18697261 1.18279165 0.77795017]
ti_coms = [0.33742712 0.35265367 0.07826998 0.07614213 0.08248924 0.0885487
 0.07903119 0.07564237 0.07982332 0.48466481]
t_total = [27.14759827 27.14759827 27.14759827 27.14759827 27.14759827 27.14759827
 27.14759827 27.14759827 27.14759827 27.14759827]
ene_coms = [0.03374271 0.03526537 0.007827   0.00761421 0.00824892 0.00885487
 0.00790312 0.00756424 0.00798233 0.04846648]
ene_comp = [6.54268888e-06 6.29210931e-05 3.80551190e-06 1.58122708e-07
 5.75986367e-05 6.32088411e-06 3.14693485e-07 1.13967805e-05
 4.39661098e-06 7.60058374e-06]
ene_total = [0.72564575 0.75959671 0.16837082 0.16371727 0.1785993  0.19052527
 0.16993244 0.16288438 0.17172338 1.04224559]
optimize_network iter = 0 obj = 3.7332409135863314
eta = 0.5814149220172816
freqs = [24183479.08746898 51713163.94317754 18591807.63703512  6435596.56915326
 46043815.63850607 22081999.89785708  8101639.47916663 26778761.87455908
 19517008.95674795 26934467.13021833]
eta_min = 0.5814149220172826	eta_max = 0.5814149220172761
af = 0.002672932357743587	bf = 1.56122943278155	zeta = 0.002940225593517946	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03374271 0.03526537 0.007827   0.00761421 0.00824892 0.00885487
 0.00790312 0.00756424 0.00798233 0.04846648]
ene_comp = [1.14990897e-06 1.10586841e-05 6.68836979e-07 2.77908248e-08
 1.01232368e-05 1.11092572e-06 5.53088901e-08 2.00303886e-06
 7.72725479e-07 1.33583907e-06]
ene_total = [3.34065545 3.49237992 0.77494211 0.75381279 0.81764894 0.8767456
 0.78241736 0.74906067 0.79033052 4.79833274]
ti_comp = [0.92518786 0.90996131 1.18434499 1.18647285 1.18012573 1.17406627
 1.18358379 1.18697261 1.18279165 0.77795017]
ti_coms = [0.33742712 0.35265367 0.07826998 0.07614213 0.08248924 0.0885487
 0.07903119 0.07564237 0.07982332 0.48466481]
t_total = [27.14759827 27.14759827 27.14759827 27.14759827 27.14759827 27.14759827
 27.14759827 27.14759827 27.14759827 27.14759827]
ene_coms = [0.03374271 0.03526537 0.007827   0.00761421 0.00824892 0.00885487
 0.00790312 0.00756424 0.00798233 0.04846648]
ene_comp = [6.54268888e-06 6.29210931e-05 3.80551190e-06 1.58122708e-07
 5.75986367e-05 6.32088411e-06 3.14693485e-07 1.13967805e-05
 4.39661098e-06 7.60058374e-06]
ene_total = [0.72564575 0.75959671 0.16837082 0.16371727 0.1785993  0.19052527
 0.16993244 0.16288438 0.17172338 1.04224559]
optimize_network iter = 1 obj = 3.7332409135862816
eta = 0.5814149220172761
freqs = [24183479.08746897 51713163.9431775  18591807.63703519  6435596.56915329
 46043815.63850622 22081999.89785716  8101639.47916666 26778761.87455918
 19517008.95674802 26934467.13021823]
Done!
ene_coms = [0.03374271 0.03526537 0.007827   0.00761421 0.00824892 0.00885487
 0.00790312 0.00756424 0.00798233 0.04846648]
ene_comp = [6.26364348e-06 6.02375111e-05 3.64320698e-06 1.51378781e-07
 5.51420573e-05 6.05129867e-06 3.01271821e-07 1.09107083e-05
 4.20909571e-06 7.27641916e-06]
ene_total = [0.03374898 0.0353256  0.00783064 0.00761436 0.00830407 0.00886092
 0.00790342 0.00757515 0.00798654 0.04847376]
At round 56 energy consumption: 0.17362344075392838
At round 56 eta: 0.5814149220172761
At round 56 a_n: 9.000035425972609
At round 56 local rounds: 17.757350246386498
At round 56 global rounds: 21.501089979954
gradient difference: 0.3019757866859436
train() client id: f_00000-0-0 loss: 1.614599  [   32/  126]
train() client id: f_00000-0-1 loss: 1.072810  [   64/  126]
train() client id: f_00000-0-2 loss: 1.234514  [   96/  126]
train() client id: f_00000-1-0 loss: 1.248008  [   32/  126]
train() client id: f_00000-1-1 loss: 1.047543  [   64/  126]
train() client id: f_00000-1-2 loss: 1.323673  [   96/  126]
train() client id: f_00000-2-0 loss: 0.905713  [   32/  126]
train() client id: f_00000-2-1 loss: 1.180121  [   64/  126]
train() client id: f_00000-2-2 loss: 0.902964  [   96/  126]
train() client id: f_00000-3-0 loss: 1.101438  [   32/  126]
train() client id: f_00000-3-1 loss: 1.097400  [   64/  126]
train() client id: f_00000-3-2 loss: 0.745001  [   96/  126]
train() client id: f_00000-4-0 loss: 0.913394  [   32/  126]
train() client id: f_00000-4-1 loss: 0.869184  [   64/  126]
train() client id: f_00000-4-2 loss: 0.811581  [   96/  126]
train() client id: f_00000-5-0 loss: 0.816090  [   32/  126]
train() client id: f_00000-5-1 loss: 0.852393  [   64/  126]
train() client id: f_00000-5-2 loss: 0.943933  [   96/  126]
train() client id: f_00000-6-0 loss: 0.787305  [   32/  126]
train() client id: f_00000-6-1 loss: 0.795578  [   64/  126]
train() client id: f_00000-6-2 loss: 0.889144  [   96/  126]
train() client id: f_00000-7-0 loss: 0.781785  [   32/  126]
train() client id: f_00000-7-1 loss: 0.819233  [   64/  126]
train() client id: f_00000-7-2 loss: 0.769329  [   96/  126]
train() client id: f_00000-8-0 loss: 0.762124  [   32/  126]
train() client id: f_00000-8-1 loss: 0.791883  [   64/  126]
train() client id: f_00000-8-2 loss: 0.608316  [   96/  126]
train() client id: f_00000-9-0 loss: 0.881068  [   32/  126]
train() client id: f_00000-9-1 loss: 0.688568  [   64/  126]
train() client id: f_00000-9-2 loss: 0.654245  [   96/  126]
train() client id: f_00000-10-0 loss: 0.656039  [   32/  126]
train() client id: f_00000-10-1 loss: 0.675848  [   64/  126]
train() client id: f_00000-10-2 loss: 0.768373  [   96/  126]
train() client id: f_00000-11-0 loss: 0.739527  [   32/  126]
train() client id: f_00000-11-1 loss: 0.634340  [   64/  126]
train() client id: f_00000-11-2 loss: 0.658731  [   96/  126]
train() client id: f_00000-12-0 loss: 0.687981  [   32/  126]
train() client id: f_00000-12-1 loss: 0.678180  [   64/  126]
train() client id: f_00000-12-2 loss: 0.666777  [   96/  126]
train() client id: f_00000-13-0 loss: 0.757432  [   32/  126]
train() client id: f_00000-13-1 loss: 0.570028  [   64/  126]
train() client id: f_00000-13-2 loss: 0.773012  [   96/  126]
train() client id: f_00000-14-0 loss: 0.678570  [   32/  126]
train() client id: f_00000-14-1 loss: 0.697953  [   64/  126]
train() client id: f_00000-14-2 loss: 0.674998  [   96/  126]
train() client id: f_00000-15-0 loss: 0.771628  [   32/  126]
train() client id: f_00000-15-1 loss: 0.642723  [   64/  126]
train() client id: f_00000-15-2 loss: 0.614516  [   96/  126]
train() client id: f_00000-16-0 loss: 0.801874  [   32/  126]
train() client id: f_00000-16-1 loss: 0.568334  [   64/  126]
train() client id: f_00000-16-2 loss: 0.606232  [   96/  126]
train() client id: f_00001-0-0 loss: 0.295586  [   32/  265]
train() client id: f_00001-0-1 loss: 0.366899  [   64/  265]
train() client id: f_00001-0-2 loss: 0.499791  [   96/  265]
train() client id: f_00001-0-3 loss: 0.335280  [  128/  265]
train() client id: f_00001-0-4 loss: 0.334267  [  160/  265]
train() client id: f_00001-0-5 loss: 0.268824  [  192/  265]
train() client id: f_00001-0-6 loss: 0.325583  [  224/  265]
train() client id: f_00001-0-7 loss: 0.427222  [  256/  265]
train() client id: f_00001-1-0 loss: 0.471549  [   32/  265]
train() client id: f_00001-1-1 loss: 0.320147  [   64/  265]
train() client id: f_00001-1-2 loss: 0.362981  [   96/  265]
train() client id: f_00001-1-3 loss: 0.364556  [  128/  265]
train() client id: f_00001-1-4 loss: 0.253323  [  160/  265]
train() client id: f_00001-1-5 loss: 0.447988  [  192/  265]
train() client id: f_00001-1-6 loss: 0.266036  [  224/  265]
train() client id: f_00001-1-7 loss: 0.296211  [  256/  265]
train() client id: f_00001-2-0 loss: 0.342314  [   32/  265]
train() client id: f_00001-2-1 loss: 0.230939  [   64/  265]
train() client id: f_00001-2-2 loss: 0.453548  [   96/  265]
train() client id: f_00001-2-3 loss: 0.351514  [  128/  265]
train() client id: f_00001-2-4 loss: 0.346365  [  160/  265]
train() client id: f_00001-2-5 loss: 0.320862  [  192/  265]
train() client id: f_00001-2-6 loss: 0.267410  [  224/  265]
train() client id: f_00001-2-7 loss: 0.368550  [  256/  265]
train() client id: f_00001-3-0 loss: 0.308507  [   32/  265]
train() client id: f_00001-3-1 loss: 0.305849  [   64/  265]
train() client id: f_00001-3-2 loss: 0.257274  [   96/  265]
train() client id: f_00001-3-3 loss: 0.394590  [  128/  265]
train() client id: f_00001-3-4 loss: 0.347028  [  160/  265]
train() client id: f_00001-3-5 loss: 0.354408  [  192/  265]
train() client id: f_00001-3-6 loss: 0.292809  [  224/  265]
train() client id: f_00001-3-7 loss: 0.388146  [  256/  265]
train() client id: f_00001-4-0 loss: 0.285683  [   32/  265]
train() client id: f_00001-4-1 loss: 0.286792  [   64/  265]
train() client id: f_00001-4-2 loss: 0.417061  [   96/  265]
train() client id: f_00001-4-3 loss: 0.381686  [  128/  265]
train() client id: f_00001-4-4 loss: 0.238684  [  160/  265]
train() client id: f_00001-4-5 loss: 0.317680  [  192/  265]
train() client id: f_00001-4-6 loss: 0.414892  [  224/  265]
train() client id: f_00001-4-7 loss: 0.341009  [  256/  265]
train() client id: f_00001-5-0 loss: 0.360968  [   32/  265]
train() client id: f_00001-5-1 loss: 0.296858  [   64/  265]
train() client id: f_00001-5-2 loss: 0.241329  [   96/  265]
train() client id: f_00001-5-3 loss: 0.351372  [  128/  265]
train() client id: f_00001-5-4 loss: 0.460470  [  160/  265]
train() client id: f_00001-5-5 loss: 0.365021  [  192/  265]
train() client id: f_00001-5-6 loss: 0.288268  [  224/  265]
train() client id: f_00001-5-7 loss: 0.268061  [  256/  265]
train() client id: f_00001-6-0 loss: 0.348964  [   32/  265]
train() client id: f_00001-6-1 loss: 0.399656  [   64/  265]
train() client id: f_00001-6-2 loss: 0.221105  [   96/  265]
train() client id: f_00001-6-3 loss: 0.394480  [  128/  265]
train() client id: f_00001-6-4 loss: 0.312387  [  160/  265]
train() client id: f_00001-6-5 loss: 0.453708  [  192/  265]
train() client id: f_00001-6-6 loss: 0.233251  [  224/  265]
train() client id: f_00001-6-7 loss: 0.248252  [  256/  265]
train() client id: f_00001-7-0 loss: 0.531657  [   32/  265]
train() client id: f_00001-7-1 loss: 0.352867  [   64/  265]
train() client id: f_00001-7-2 loss: 0.342513  [   96/  265]
train() client id: f_00001-7-3 loss: 0.227636  [  128/  265]
train() client id: f_00001-7-4 loss: 0.262468  [  160/  265]
train() client id: f_00001-7-5 loss: 0.364297  [  192/  265]
train() client id: f_00001-7-6 loss: 0.230069  [  224/  265]
train() client id: f_00001-7-7 loss: 0.284411  [  256/  265]
train() client id: f_00001-8-0 loss: 0.285865  [   32/  265]
train() client id: f_00001-8-1 loss: 0.312734  [   64/  265]
train() client id: f_00001-8-2 loss: 0.241824  [   96/  265]
train() client id: f_00001-8-3 loss: 0.229470  [  128/  265]
train() client id: f_00001-8-4 loss: 0.376398  [  160/  265]
train() client id: f_00001-8-5 loss: 0.427465  [  192/  265]
train() client id: f_00001-8-6 loss: 0.379628  [  224/  265]
train() client id: f_00001-8-7 loss: 0.319889  [  256/  265]
train() client id: f_00001-9-0 loss: 0.265541  [   32/  265]
train() client id: f_00001-9-1 loss: 0.365137  [   64/  265]
train() client id: f_00001-9-2 loss: 0.304966  [   96/  265]
train() client id: f_00001-9-3 loss: 0.276606  [  128/  265]
train() client id: f_00001-9-4 loss: 0.294214  [  160/  265]
train() client id: f_00001-9-5 loss: 0.341811  [  192/  265]
train() client id: f_00001-9-6 loss: 0.424602  [  224/  265]
train() client id: f_00001-9-7 loss: 0.294986  [  256/  265]
train() client id: f_00001-10-0 loss: 0.359103  [   32/  265]
train() client id: f_00001-10-1 loss: 0.229757  [   64/  265]
train() client id: f_00001-10-2 loss: 0.329730  [   96/  265]
train() client id: f_00001-10-3 loss: 0.271977  [  128/  265]
train() client id: f_00001-10-4 loss: 0.420912  [  160/  265]
train() client id: f_00001-10-5 loss: 0.389149  [  192/  265]
train() client id: f_00001-10-6 loss: 0.314668  [  224/  265]
train() client id: f_00001-10-7 loss: 0.234563  [  256/  265]
train() client id: f_00001-11-0 loss: 0.339776  [   32/  265]
train() client id: f_00001-11-1 loss: 0.270335  [   64/  265]
train() client id: f_00001-11-2 loss: 0.275345  [   96/  265]
train() client id: f_00001-11-3 loss: 0.303032  [  128/  265]
train() client id: f_00001-11-4 loss: 0.364772  [  160/  265]
train() client id: f_00001-11-5 loss: 0.274574  [  192/  265]
train() client id: f_00001-11-6 loss: 0.316025  [  224/  265]
train() client id: f_00001-11-7 loss: 0.281137  [  256/  265]
train() client id: f_00001-12-0 loss: 0.317663  [   32/  265]
train() client id: f_00001-12-1 loss: 0.300869  [   64/  265]
train() client id: f_00001-12-2 loss: 0.282361  [   96/  265]
train() client id: f_00001-12-3 loss: 0.394623  [  128/  265]
train() client id: f_00001-12-4 loss: 0.218029  [  160/  265]
train() client id: f_00001-12-5 loss: 0.351565  [  192/  265]
train() client id: f_00001-12-6 loss: 0.198531  [  224/  265]
train() client id: f_00001-12-7 loss: 0.485187  [  256/  265]
train() client id: f_00001-13-0 loss: 0.350932  [   32/  265]
train() client id: f_00001-13-1 loss: 0.344147  [   64/  265]
train() client id: f_00001-13-2 loss: 0.197519  [   96/  265]
train() client id: f_00001-13-3 loss: 0.409806  [  128/  265]
train() client id: f_00001-13-4 loss: 0.240365  [  160/  265]
train() client id: f_00001-13-5 loss: 0.490834  [  192/  265]
train() client id: f_00001-13-6 loss: 0.221570  [  224/  265]
train() client id: f_00001-13-7 loss: 0.275036  [  256/  265]
train() client id: f_00001-14-0 loss: 0.225384  [   32/  265]
train() client id: f_00001-14-1 loss: 0.423586  [   64/  265]
train() client id: f_00001-14-2 loss: 0.218801  [   96/  265]
train() client id: f_00001-14-3 loss: 0.270156  [  128/  265]
train() client id: f_00001-14-4 loss: 0.379621  [  160/  265]
train() client id: f_00001-14-5 loss: 0.306635  [  192/  265]
train() client id: f_00001-14-6 loss: 0.329050  [  224/  265]
train() client id: f_00001-14-7 loss: 0.368710  [  256/  265]
train() client id: f_00001-15-0 loss: 0.270425  [   32/  265]
train() client id: f_00001-15-1 loss: 0.216196  [   64/  265]
train() client id: f_00001-15-2 loss: 0.367251  [   96/  265]
train() client id: f_00001-15-3 loss: 0.346823  [  128/  265]
train() client id: f_00001-15-4 loss: 0.389582  [  160/  265]
train() client id: f_00001-15-5 loss: 0.387007  [  192/  265]
train() client id: f_00001-15-6 loss: 0.284819  [  224/  265]
train() client id: f_00001-15-7 loss: 0.263081  [  256/  265]
train() client id: f_00001-16-0 loss: 0.290126  [   32/  265]
train() client id: f_00001-16-1 loss: 0.375323  [   64/  265]
train() client id: f_00001-16-2 loss: 0.256440  [   96/  265]
train() client id: f_00001-16-3 loss: 0.359831  [  128/  265]
train() client id: f_00001-16-4 loss: 0.317374  [  160/  265]
train() client id: f_00001-16-5 loss: 0.244059  [  192/  265]
train() client id: f_00001-16-6 loss: 0.311661  [  224/  265]
train() client id: f_00001-16-7 loss: 0.377697  [  256/  265]
train() client id: f_00002-0-0 loss: 1.417079  [   32/  124]
train() client id: f_00002-0-1 loss: 1.127593  [   64/  124]
train() client id: f_00002-0-2 loss: 1.048333  [   96/  124]
train() client id: f_00002-1-0 loss: 1.373107  [   32/  124]
train() client id: f_00002-1-1 loss: 1.099938  [   64/  124]
train() client id: f_00002-1-2 loss: 1.096767  [   96/  124]
train() client id: f_00002-2-0 loss: 1.170658  [   32/  124]
train() client id: f_00002-2-1 loss: 1.140927  [   64/  124]
train() client id: f_00002-2-2 loss: 1.208927  [   96/  124]
train() client id: f_00002-3-0 loss: 0.966266  [   32/  124]
train() client id: f_00002-3-1 loss: 1.056584  [   64/  124]
train() client id: f_00002-3-2 loss: 1.062719  [   96/  124]
train() client id: f_00002-4-0 loss: 0.965911  [   32/  124]
train() client id: f_00002-4-1 loss: 1.080620  [   64/  124]
train() client id: f_00002-4-2 loss: 1.048440  [   96/  124]
train() client id: f_00002-5-0 loss: 1.088685  [   32/  124]
train() client id: f_00002-5-1 loss: 1.040824  [   64/  124]
train() client id: f_00002-5-2 loss: 0.860457  [   96/  124]
train() client id: f_00002-6-0 loss: 1.064528  [   32/  124]
train() client id: f_00002-6-1 loss: 0.906285  [   64/  124]
train() client id: f_00002-6-2 loss: 1.049740  [   96/  124]
train() client id: f_00002-7-0 loss: 1.015329  [   32/  124]
train() client id: f_00002-7-1 loss: 0.901988  [   64/  124]
train() client id: f_00002-7-2 loss: 1.033347  [   96/  124]
train() client id: f_00002-8-0 loss: 0.822238  [   32/  124]
train() client id: f_00002-8-1 loss: 0.880898  [   64/  124]
train() client id: f_00002-8-2 loss: 1.162621  [   96/  124]
train() client id: f_00002-9-0 loss: 0.782324  [   32/  124]
train() client id: f_00002-9-1 loss: 1.154862  [   64/  124]
train() client id: f_00002-9-2 loss: 0.870421  [   96/  124]
train() client id: f_00002-10-0 loss: 0.910938  [   32/  124]
train() client id: f_00002-10-1 loss: 0.937837  [   64/  124]
train() client id: f_00002-10-2 loss: 0.964763  [   96/  124]
train() client id: f_00002-11-0 loss: 1.015389  [   32/  124]
train() client id: f_00002-11-1 loss: 0.914138  [   64/  124]
train() client id: f_00002-11-2 loss: 0.986613  [   96/  124]
train() client id: f_00002-12-0 loss: 0.936056  [   32/  124]
train() client id: f_00002-12-1 loss: 0.794426  [   64/  124]
train() client id: f_00002-12-2 loss: 0.818054  [   96/  124]
train() client id: f_00002-13-0 loss: 0.774442  [   32/  124]
train() client id: f_00002-13-1 loss: 0.886706  [   64/  124]
train() client id: f_00002-13-2 loss: 1.013849  [   96/  124]
train() client id: f_00002-14-0 loss: 0.790817  [   32/  124]
train() client id: f_00002-14-1 loss: 0.847687  [   64/  124]
train() client id: f_00002-14-2 loss: 0.994135  [   96/  124]
train() client id: f_00002-15-0 loss: 0.768827  [   32/  124]
train() client id: f_00002-15-1 loss: 1.005912  [   64/  124]
train() client id: f_00002-15-2 loss: 1.017963  [   96/  124]
train() client id: f_00002-16-0 loss: 0.799659  [   32/  124]
train() client id: f_00002-16-1 loss: 0.990563  [   64/  124]
train() client id: f_00002-16-2 loss: 0.843725  [   96/  124]
train() client id: f_00003-0-0 loss: 0.531114  [   32/   43]
train() client id: f_00003-1-0 loss: 0.547081  [   32/   43]
train() client id: f_00003-2-0 loss: 0.666463  [   32/   43]
train() client id: f_00003-3-0 loss: 0.823678  [   32/   43]
train() client id: f_00003-4-0 loss: 0.776443  [   32/   43]
train() client id: f_00003-5-0 loss: 0.508524  [   32/   43]
train() client id: f_00003-6-0 loss: 0.690395  [   32/   43]
train() client id: f_00003-7-0 loss: 0.972240  [   32/   43]
train() client id: f_00003-8-0 loss: 0.761751  [   32/   43]
train() client id: f_00003-9-0 loss: 0.688584  [   32/   43]
train() client id: f_00003-10-0 loss: 0.741424  [   32/   43]
train() client id: f_00003-11-0 loss: 0.703014  [   32/   43]
train() client id: f_00003-12-0 loss: 0.691329  [   32/   43]
train() client id: f_00003-13-0 loss: 0.807152  [   32/   43]
train() client id: f_00003-14-0 loss: 0.662276  [   32/   43]
train() client id: f_00003-15-0 loss: 0.651905  [   32/   43]
train() client id: f_00003-16-0 loss: 0.675362  [   32/   43]
train() client id: f_00004-0-0 loss: 0.761922  [   32/  306]
train() client id: f_00004-0-1 loss: 0.800411  [   64/  306]
train() client id: f_00004-0-2 loss: 0.953633  [   96/  306]
train() client id: f_00004-0-3 loss: 0.825695  [  128/  306]
train() client id: f_00004-0-4 loss: 0.864445  [  160/  306]
train() client id: f_00004-0-5 loss: 0.869908  [  192/  306]
train() client id: f_00004-0-6 loss: 0.966409  [  224/  306]
train() client id: f_00004-0-7 loss: 0.925270  [  256/  306]
train() client id: f_00004-0-8 loss: 0.893079  [  288/  306]
train() client id: f_00004-1-0 loss: 0.806628  [   32/  306]
train() client id: f_00004-1-1 loss: 1.012208  [   64/  306]
train() client id: f_00004-1-2 loss: 0.819543  [   96/  306]
train() client id: f_00004-1-3 loss: 0.796317  [  128/  306]
train() client id: f_00004-1-4 loss: 0.910049  [  160/  306]
train() client id: f_00004-1-5 loss: 0.738385  [  192/  306]
train() client id: f_00004-1-6 loss: 0.824685  [  224/  306]
train() client id: f_00004-1-7 loss: 0.936027  [  256/  306]
train() client id: f_00004-1-8 loss: 0.991553  [  288/  306]
train() client id: f_00004-2-0 loss: 0.908563  [   32/  306]
train() client id: f_00004-2-1 loss: 0.738645  [   64/  306]
train() client id: f_00004-2-2 loss: 0.811986  [   96/  306]
train() client id: f_00004-2-3 loss: 0.876082  [  128/  306]
train() client id: f_00004-2-4 loss: 0.894463  [  160/  306]
train() client id: f_00004-2-5 loss: 0.908720  [  192/  306]
train() client id: f_00004-2-6 loss: 0.948156  [  224/  306]
train() client id: f_00004-2-7 loss: 0.910804  [  256/  306]
train() client id: f_00004-2-8 loss: 0.818597  [  288/  306]
train() client id: f_00004-3-0 loss: 0.819192  [   32/  306]
train() client id: f_00004-3-1 loss: 0.952876  [   64/  306]
train() client id: f_00004-3-2 loss: 0.876111  [   96/  306]
train() client id: f_00004-3-3 loss: 0.829971  [  128/  306]
train() client id: f_00004-3-4 loss: 0.913764  [  160/  306]
train() client id: f_00004-3-5 loss: 0.854264  [  192/  306]
train() client id: f_00004-3-6 loss: 0.793847  [  224/  306]
train() client id: f_00004-3-7 loss: 0.953252  [  256/  306]
train() client id: f_00004-3-8 loss: 0.822711  [  288/  306]
train() client id: f_00004-4-0 loss: 0.993269  [   32/  306]
train() client id: f_00004-4-1 loss: 0.770956  [   64/  306]
train() client id: f_00004-4-2 loss: 0.770064  [   96/  306]
train() client id: f_00004-4-3 loss: 0.728982  [  128/  306]
train() client id: f_00004-4-4 loss: 0.876394  [  160/  306]
train() client id: f_00004-4-5 loss: 0.957906  [  192/  306]
train() client id: f_00004-4-6 loss: 0.981565  [  224/  306]
train() client id: f_00004-4-7 loss: 0.838342  [  256/  306]
train() client id: f_00004-4-8 loss: 0.817582  [  288/  306]
train() client id: f_00004-5-0 loss: 0.912512  [   32/  306]
train() client id: f_00004-5-1 loss: 0.849417  [   64/  306]
train() client id: f_00004-5-2 loss: 0.836340  [   96/  306]
train() client id: f_00004-5-3 loss: 0.887807  [  128/  306]
train() client id: f_00004-5-4 loss: 0.947433  [  160/  306]
train() client id: f_00004-5-5 loss: 0.855276  [  192/  306]
train() client id: f_00004-5-6 loss: 0.789602  [  224/  306]
train() client id: f_00004-5-7 loss: 0.830413  [  256/  306]
train() client id: f_00004-5-8 loss: 0.823355  [  288/  306]
train() client id: f_00004-6-0 loss: 0.867598  [   32/  306]
train() client id: f_00004-6-1 loss: 0.821697  [   64/  306]
train() client id: f_00004-6-2 loss: 0.951084  [   96/  306]
train() client id: f_00004-6-3 loss: 0.759601  [  128/  306]
train() client id: f_00004-6-4 loss: 0.796038  [  160/  306]
train() client id: f_00004-6-5 loss: 0.919334  [  192/  306]
train() client id: f_00004-6-6 loss: 0.882823  [  224/  306]
train() client id: f_00004-6-7 loss: 0.904402  [  256/  306]
train() client id: f_00004-6-8 loss: 0.838768  [  288/  306]
train() client id: f_00004-7-0 loss: 0.837067  [   32/  306]
train() client id: f_00004-7-1 loss: 0.809607  [   64/  306]
train() client id: f_00004-7-2 loss: 0.740349  [   96/  306]
train() client id: f_00004-7-3 loss: 0.764655  [  128/  306]
train() client id: f_00004-7-4 loss: 0.856020  [  160/  306]
train() client id: f_00004-7-5 loss: 0.936080  [  192/  306]
train() client id: f_00004-7-6 loss: 0.962594  [  224/  306]
train() client id: f_00004-7-7 loss: 0.926164  [  256/  306]
train() client id: f_00004-7-8 loss: 0.854462  [  288/  306]
train() client id: f_00004-8-0 loss: 0.909315  [   32/  306]
train() client id: f_00004-8-1 loss: 0.851706  [   64/  306]
train() client id: f_00004-8-2 loss: 0.840375  [   96/  306]
train() client id: f_00004-8-3 loss: 0.850888  [  128/  306]
train() client id: f_00004-8-4 loss: 0.911286  [  160/  306]
train() client id: f_00004-8-5 loss: 0.891000  [  192/  306]
train() client id: f_00004-8-6 loss: 0.848895  [  224/  306]
train() client id: f_00004-8-7 loss: 0.742561  [  256/  306]
train() client id: f_00004-8-8 loss: 0.772642  [  288/  306]
train() client id: f_00004-9-0 loss: 0.802430  [   32/  306]
train() client id: f_00004-9-1 loss: 0.937250  [   64/  306]
train() client id: f_00004-9-2 loss: 0.833794  [   96/  306]
train() client id: f_00004-9-3 loss: 0.915389  [  128/  306]
train() client id: f_00004-9-4 loss: 0.774882  [  160/  306]
train() client id: f_00004-9-5 loss: 0.905448  [  192/  306]
train() client id: f_00004-9-6 loss: 0.870083  [  224/  306]
train() client id: f_00004-9-7 loss: 0.754987  [  256/  306]
train() client id: f_00004-9-8 loss: 0.872054  [  288/  306]
train() client id: f_00004-10-0 loss: 0.907866  [   32/  306]
train() client id: f_00004-10-1 loss: 0.790388  [   64/  306]
train() client id: f_00004-10-2 loss: 0.803954  [   96/  306]
train() client id: f_00004-10-3 loss: 0.765038  [  128/  306]
train() client id: f_00004-10-4 loss: 0.823530  [  160/  306]
train() client id: f_00004-10-5 loss: 0.882306  [  192/  306]
train() client id: f_00004-10-6 loss: 0.992041  [  224/  306]
train() client id: f_00004-10-7 loss: 0.777991  [  256/  306]
train() client id: f_00004-10-8 loss: 0.844064  [  288/  306]
train() client id: f_00004-11-0 loss: 0.830171  [   32/  306]
train() client id: f_00004-11-1 loss: 0.957627  [   64/  306]
train() client id: f_00004-11-2 loss: 0.976112  [   96/  306]
train() client id: f_00004-11-3 loss: 0.834816  [  128/  306]
train() client id: f_00004-11-4 loss: 0.777477  [  160/  306]
train() client id: f_00004-11-5 loss: 0.648809  [  192/  306]
train() client id: f_00004-11-6 loss: 0.808708  [  224/  306]
train() client id: f_00004-11-7 loss: 1.039647  [  256/  306]
train() client id: f_00004-11-8 loss: 0.824857  [  288/  306]
train() client id: f_00004-12-0 loss: 0.824850  [   32/  306]
train() client id: f_00004-12-1 loss: 0.824837  [   64/  306]
train() client id: f_00004-12-2 loss: 0.930008  [   96/  306]
train() client id: f_00004-12-3 loss: 0.946893  [  128/  306]
train() client id: f_00004-12-4 loss: 0.908869  [  160/  306]
train() client id: f_00004-12-5 loss: 0.806230  [  192/  306]
train() client id: f_00004-12-6 loss: 0.752660  [  224/  306]
train() client id: f_00004-12-7 loss: 0.833703  [  256/  306]
train() client id: f_00004-12-8 loss: 0.816184  [  288/  306]
train() client id: f_00004-13-0 loss: 0.826024  [   32/  306]
train() client id: f_00004-13-1 loss: 0.891772  [   64/  306]
train() client id: f_00004-13-2 loss: 0.831543  [   96/  306]
train() client id: f_00004-13-3 loss: 0.814044  [  128/  306]
train() client id: f_00004-13-4 loss: 0.832370  [  160/  306]
train() client id: f_00004-13-5 loss: 0.902008  [  192/  306]
train() client id: f_00004-13-6 loss: 0.891480  [  224/  306]
train() client id: f_00004-13-7 loss: 0.788844  [  256/  306]
train() client id: f_00004-13-8 loss: 0.858627  [  288/  306]
train() client id: f_00004-14-0 loss: 0.851715  [   32/  306]
train() client id: f_00004-14-1 loss: 0.836873  [   64/  306]
train() client id: f_00004-14-2 loss: 0.911218  [   96/  306]
train() client id: f_00004-14-3 loss: 0.926231  [  128/  306]
train() client id: f_00004-14-4 loss: 0.789364  [  160/  306]
train() client id: f_00004-14-5 loss: 0.828887  [  192/  306]
train() client id: f_00004-14-6 loss: 0.939432  [  224/  306]
train() client id: f_00004-14-7 loss: 0.828368  [  256/  306]
train() client id: f_00004-14-8 loss: 0.756778  [  288/  306]
train() client id: f_00004-15-0 loss: 0.932802  [   32/  306]
train() client id: f_00004-15-1 loss: 0.850941  [   64/  306]
train() client id: f_00004-15-2 loss: 0.799970  [   96/  306]
train() client id: f_00004-15-3 loss: 0.973428  [  128/  306]
train() client id: f_00004-15-4 loss: 0.844135  [  160/  306]
train() client id: f_00004-15-5 loss: 0.790051  [  192/  306]
train() client id: f_00004-15-6 loss: 0.873362  [  224/  306]
train() client id: f_00004-15-7 loss: 0.807604  [  256/  306]
train() client id: f_00004-15-8 loss: 0.806786  [  288/  306]
train() client id: f_00004-16-0 loss: 0.932709  [   32/  306]
train() client id: f_00004-16-1 loss: 0.881934  [   64/  306]
train() client id: f_00004-16-2 loss: 0.865975  [   96/  306]
train() client id: f_00004-16-3 loss: 0.767833  [  128/  306]
train() client id: f_00004-16-4 loss: 0.833759  [  160/  306]
train() client id: f_00004-16-5 loss: 0.872103  [  192/  306]
train() client id: f_00004-16-6 loss: 0.868701  [  224/  306]
train() client id: f_00004-16-7 loss: 0.820931  [  256/  306]
train() client id: f_00004-16-8 loss: 0.778822  [  288/  306]
train() client id: f_00005-0-0 loss: 0.735034  [   32/  146]
train() client id: f_00005-0-1 loss: 0.583982  [   64/  146]
train() client id: f_00005-0-2 loss: 0.713317  [   96/  146]
train() client id: f_00005-0-3 loss: 0.423407  [  128/  146]
train() client id: f_00005-1-0 loss: 0.736172  [   32/  146]
train() client id: f_00005-1-1 loss: 0.602499  [   64/  146]
train() client id: f_00005-1-2 loss: 0.534135  [   96/  146]
train() client id: f_00005-1-3 loss: 0.675166  [  128/  146]
train() client id: f_00005-2-0 loss: 0.718328  [   32/  146]
train() client id: f_00005-2-1 loss: 0.651911  [   64/  146]
train() client id: f_00005-2-2 loss: 0.545258  [   96/  146]
train() client id: f_00005-2-3 loss: 0.601997  [  128/  146]
train() client id: f_00005-3-0 loss: 0.475646  [   32/  146]
train() client id: f_00005-3-1 loss: 0.877736  [   64/  146]
train() client id: f_00005-3-2 loss: 0.578177  [   96/  146]
train() client id: f_00005-3-3 loss: 0.615219  [  128/  146]
train() client id: f_00005-4-0 loss: 0.664405  [   32/  146]
train() client id: f_00005-4-1 loss: 0.798438  [   64/  146]
train() client id: f_00005-4-2 loss: 0.479965  [   96/  146]
train() client id: f_00005-4-3 loss: 0.628277  [  128/  146]
train() client id: f_00005-5-0 loss: 0.494515  [   32/  146]
train() client id: f_00005-5-1 loss: 0.714275  [   64/  146]
train() client id: f_00005-5-2 loss: 0.746307  [   96/  146]
train() client id: f_00005-5-3 loss: 0.547016  [  128/  146]
train() client id: f_00005-6-0 loss: 0.544916  [   32/  146]
train() client id: f_00005-6-1 loss: 0.647772  [   64/  146]
train() client id: f_00005-6-2 loss: 0.478265  [   96/  146]
train() client id: f_00005-6-3 loss: 0.711012  [  128/  146]
train() client id: f_00005-7-0 loss: 0.701999  [   32/  146]
train() client id: f_00005-7-1 loss: 0.658546  [   64/  146]
train() client id: f_00005-7-2 loss: 0.551679  [   96/  146]
train() client id: f_00005-7-3 loss: 0.560901  [  128/  146]
train() client id: f_00005-8-0 loss: 0.634483  [   32/  146]
train() client id: f_00005-8-1 loss: 0.828654  [   64/  146]
train() client id: f_00005-8-2 loss: 0.575655  [   96/  146]
train() client id: f_00005-8-3 loss: 0.580271  [  128/  146]
train() client id: f_00005-9-0 loss: 0.606732  [   32/  146]
train() client id: f_00005-9-1 loss: 0.384361  [   64/  146]
train() client id: f_00005-9-2 loss: 0.580985  [   96/  146]
train() client id: f_00005-9-3 loss: 0.897054  [  128/  146]
train() client id: f_00005-10-0 loss: 0.400305  [   32/  146]
train() client id: f_00005-10-1 loss: 0.595465  [   64/  146]
train() client id: f_00005-10-2 loss: 0.774436  [   96/  146]
train() client id: f_00005-10-3 loss: 0.827351  [  128/  146]
train() client id: f_00005-11-0 loss: 0.889983  [   32/  146]
train() client id: f_00005-11-1 loss: 0.640209  [   64/  146]
train() client id: f_00005-11-2 loss: 0.570464  [   96/  146]
train() client id: f_00005-11-3 loss: 0.284221  [  128/  146]
train() client id: f_00005-12-0 loss: 0.547686  [   32/  146]
train() client id: f_00005-12-1 loss: 0.693086  [   64/  146]
train() client id: f_00005-12-2 loss: 0.489005  [   96/  146]
train() client id: f_00005-12-3 loss: 0.602691  [  128/  146]
train() client id: f_00005-13-0 loss: 0.619390  [   32/  146]
train() client id: f_00005-13-1 loss: 0.695045  [   64/  146]
train() client id: f_00005-13-2 loss: 0.781548  [   96/  146]
train() client id: f_00005-13-3 loss: 0.494386  [  128/  146]
train() client id: f_00005-14-0 loss: 0.500340  [   32/  146]
train() client id: f_00005-14-1 loss: 0.643119  [   64/  146]
train() client id: f_00005-14-2 loss: 0.578536  [   96/  146]
train() client id: f_00005-14-3 loss: 0.711323  [  128/  146]
train() client id: f_00005-15-0 loss: 0.722558  [   32/  146]
train() client id: f_00005-15-1 loss: 0.630261  [   64/  146]
train() client id: f_00005-15-2 loss: 0.602633  [   96/  146]
train() client id: f_00005-15-3 loss: 0.525311  [  128/  146]
train() client id: f_00005-16-0 loss: 0.491842  [   32/  146]
train() client id: f_00005-16-1 loss: 0.698378  [   64/  146]
train() client id: f_00005-16-2 loss: 0.706083  [   96/  146]
train() client id: f_00005-16-3 loss: 0.663550  [  128/  146]
train() client id: f_00006-0-0 loss: 0.550879  [   32/   54]
train() client id: f_00006-1-0 loss: 0.577497  [   32/   54]
train() client id: f_00006-2-0 loss: 0.549174  [   32/   54]
train() client id: f_00006-3-0 loss: 0.528335  [   32/   54]
train() client id: f_00006-4-0 loss: 0.556134  [   32/   54]
train() client id: f_00006-5-0 loss: 0.551015  [   32/   54]
train() client id: f_00006-6-0 loss: 0.580052  [   32/   54]
train() client id: f_00006-7-0 loss: 0.522552  [   32/   54]
train() client id: f_00006-8-0 loss: 0.508954  [   32/   54]
train() client id: f_00006-9-0 loss: 0.540997  [   32/   54]
train() client id: f_00006-10-0 loss: 0.524271  [   32/   54]
train() client id: f_00006-11-0 loss: 0.531605  [   32/   54]
train() client id: f_00006-12-0 loss: 0.500055  [   32/   54]
train() client id: f_00006-13-0 loss: 0.600181  [   32/   54]
train() client id: f_00006-14-0 loss: 0.492633  [   32/   54]
train() client id: f_00006-15-0 loss: 0.523631  [   32/   54]
train() client id: f_00006-16-0 loss: 0.517840  [   32/   54]
train() client id: f_00007-0-0 loss: 0.331852  [   32/  179]
train() client id: f_00007-0-1 loss: 0.727267  [   64/  179]
train() client id: f_00007-0-2 loss: 0.402485  [   96/  179]
train() client id: f_00007-0-3 loss: 0.319581  [  128/  179]
train() client id: f_00007-0-4 loss: 0.386878  [  160/  179]
train() client id: f_00007-1-0 loss: 0.347643  [   32/  179]
train() client id: f_00007-1-1 loss: 0.389834  [   64/  179]
train() client id: f_00007-1-2 loss: 0.496648  [   96/  179]
train() client id: f_00007-1-3 loss: 0.264909  [  128/  179]
train() client id: f_00007-1-4 loss: 0.600454  [  160/  179]
train() client id: f_00007-2-0 loss: 0.381331  [   32/  179]
train() client id: f_00007-2-1 loss: 0.203213  [   64/  179]
train() client id: f_00007-2-2 loss: 0.426080  [   96/  179]
train() client id: f_00007-2-3 loss: 0.478658  [  128/  179]
train() client id: f_00007-2-4 loss: 0.407422  [  160/  179]
train() client id: f_00007-3-0 loss: 0.325080  [   32/  179]
train() client id: f_00007-3-1 loss: 0.292129  [   64/  179]
train() client id: f_00007-3-2 loss: 0.393068  [   96/  179]
train() client id: f_00007-3-3 loss: 0.302386  [  128/  179]
train() client id: f_00007-3-4 loss: 0.514597  [  160/  179]
train() client id: f_00007-4-0 loss: 0.312020  [   32/  179]
train() client id: f_00007-4-1 loss: 0.200330  [   64/  179]
train() client id: f_00007-4-2 loss: 0.719282  [   96/  179]
train() client id: f_00007-4-3 loss: 0.256351  [  128/  179]
train() client id: f_00007-4-4 loss: 0.217296  [  160/  179]
train() client id: f_00007-5-0 loss: 0.231392  [   32/  179]
train() client id: f_00007-5-1 loss: 0.183668  [   64/  179]
train() client id: f_00007-5-2 loss: 0.351563  [   96/  179]
train() client id: f_00007-5-3 loss: 0.333920  [  128/  179]
train() client id: f_00007-5-4 loss: 0.544790  [  160/  179]
train() client id: f_00007-6-0 loss: 0.352302  [   32/  179]
train() client id: f_00007-6-1 loss: 0.281290  [   64/  179]
train() client id: f_00007-6-2 loss: 0.322425  [   96/  179]
train() client id: f_00007-6-3 loss: 0.561022  [  128/  179]
train() client id: f_00007-6-4 loss: 0.259022  [  160/  179]
train() client id: f_00007-7-0 loss: 0.262770  [   32/  179]
train() client id: f_00007-7-1 loss: 0.397022  [   64/  179]
train() client id: f_00007-7-2 loss: 0.425625  [   96/  179]
train() client id: f_00007-7-3 loss: 0.190674  [  128/  179]
train() client id: f_00007-7-4 loss: 0.290898  [  160/  179]
train() client id: f_00007-8-0 loss: 0.332193  [   32/  179]
train() client id: f_00007-8-1 loss: 0.205176  [   64/  179]
train() client id: f_00007-8-2 loss: 0.343951  [   96/  179]
train() client id: f_00007-8-3 loss: 0.388178  [  128/  179]
train() client id: f_00007-8-4 loss: 0.465145  [  160/  179]
train() client id: f_00007-9-0 loss: 0.332649  [   32/  179]
train() client id: f_00007-9-1 loss: 0.269293  [   64/  179]
train() client id: f_00007-9-2 loss: 0.221682  [   96/  179]
train() client id: f_00007-9-3 loss: 0.389569  [  128/  179]
train() client id: f_00007-9-4 loss: 0.292333  [  160/  179]
train() client id: f_00007-10-0 loss: 0.147575  [   32/  179]
train() client id: f_00007-10-1 loss: 0.215936  [   64/  179]
train() client id: f_00007-10-2 loss: 0.604824  [   96/  179]
train() client id: f_00007-10-3 loss: 0.318396  [  128/  179]
train() client id: f_00007-10-4 loss: 0.259286  [  160/  179]
train() client id: f_00007-11-0 loss: 0.150015  [   32/  179]
train() client id: f_00007-11-1 loss: 0.308753  [   64/  179]
train() client id: f_00007-11-2 loss: 0.541350  [   96/  179]
train() client id: f_00007-11-3 loss: 0.165931  [  128/  179]
train() client id: f_00007-11-4 loss: 0.336831  [  160/  179]
train() client id: f_00007-12-0 loss: 0.322270  [   32/  179]
train() client id: f_00007-12-1 loss: 0.419162  [   64/  179]
train() client id: f_00007-12-2 loss: 0.317344  [   96/  179]
train() client id: f_00007-12-3 loss: 0.267439  [  128/  179]
train() client id: f_00007-12-4 loss: 0.283947  [  160/  179]
train() client id: f_00007-13-0 loss: 0.222385  [   32/  179]
train() client id: f_00007-13-1 loss: 0.512500  [   64/  179]
train() client id: f_00007-13-2 loss: 0.294693  [   96/  179]
train() client id: f_00007-13-3 loss: 0.248066  [  128/  179]
train() client id: f_00007-13-4 loss: 0.300381  [  160/  179]
train() client id: f_00007-14-0 loss: 0.374279  [   32/  179]
train() client id: f_00007-14-1 loss: 0.365475  [   64/  179]
train() client id: f_00007-14-2 loss: 0.164945  [   96/  179]
train() client id: f_00007-14-3 loss: 0.590643  [  128/  179]
train() client id: f_00007-14-4 loss: 0.148141  [  160/  179]
train() client id: f_00007-15-0 loss: 0.441399  [   32/  179]
train() client id: f_00007-15-1 loss: 0.242110  [   64/  179]
train() client id: f_00007-15-2 loss: 0.325555  [   96/  179]
train() client id: f_00007-15-3 loss: 0.447850  [  128/  179]
train() client id: f_00007-15-4 loss: 0.169627  [  160/  179]
train() client id: f_00007-16-0 loss: 0.666426  [   32/  179]
train() client id: f_00007-16-1 loss: 0.210319  [   64/  179]
train() client id: f_00007-16-2 loss: 0.155525  [   96/  179]
train() client id: f_00007-16-3 loss: 0.168783  [  128/  179]
train() client id: f_00007-16-4 loss: 0.404656  [  160/  179]
train() client id: f_00008-0-0 loss: 0.565745  [   32/  130]
train() client id: f_00008-0-1 loss: 0.628109  [   64/  130]
train() client id: f_00008-0-2 loss: 0.668368  [   96/  130]
train() client id: f_00008-0-3 loss: 0.612497  [  128/  130]
train() client id: f_00008-1-0 loss: 0.656300  [   32/  130]
train() client id: f_00008-1-1 loss: 0.473100  [   64/  130]
train() client id: f_00008-1-2 loss: 0.631692  [   96/  130]
train() client id: f_00008-1-3 loss: 0.689066  [  128/  130]
train() client id: f_00008-2-0 loss: 0.711813  [   32/  130]
train() client id: f_00008-2-1 loss: 0.610056  [   64/  130]
train() client id: f_00008-2-2 loss: 0.598901  [   96/  130]
train() client id: f_00008-2-3 loss: 0.541797  [  128/  130]
train() client id: f_00008-3-0 loss: 0.511667  [   32/  130]
train() client id: f_00008-3-1 loss: 0.645242  [   64/  130]
train() client id: f_00008-3-2 loss: 0.620441  [   96/  130]
train() client id: f_00008-3-3 loss: 0.702152  [  128/  130]
train() client id: f_00008-4-0 loss: 0.624052  [   32/  130]
train() client id: f_00008-4-1 loss: 0.633891  [   64/  130]
train() client id: f_00008-4-2 loss: 0.679496  [   96/  130]
train() client id: f_00008-4-3 loss: 0.560443  [  128/  130]
train() client id: f_00008-5-0 loss: 0.662197  [   32/  130]
train() client id: f_00008-5-1 loss: 0.629820  [   64/  130]
train() client id: f_00008-5-2 loss: 0.583672  [   96/  130]
train() client id: f_00008-5-3 loss: 0.607273  [  128/  130]
train() client id: f_00008-6-0 loss: 0.564379  [   32/  130]
train() client id: f_00008-6-1 loss: 0.589884  [   64/  130]
train() client id: f_00008-6-2 loss: 0.768482  [   96/  130]
train() client id: f_00008-6-3 loss: 0.563751  [  128/  130]
train() client id: f_00008-7-0 loss: 0.640447  [   32/  130]
train() client id: f_00008-7-1 loss: 0.628592  [   64/  130]
train() client id: f_00008-7-2 loss: 0.523460  [   96/  130]
train() client id: f_00008-7-3 loss: 0.697450  [  128/  130]
train() client id: f_00008-8-0 loss: 0.569860  [   32/  130]
train() client id: f_00008-8-1 loss: 0.608023  [   64/  130]
train() client id: f_00008-8-2 loss: 0.740879  [   96/  130]
train() client id: f_00008-8-3 loss: 0.519025  [  128/  130]
train() client id: f_00008-9-0 loss: 0.709234  [   32/  130]
train() client id: f_00008-9-1 loss: 0.580152  [   64/  130]
train() client id: f_00008-9-2 loss: 0.577235  [   96/  130]
train() client id: f_00008-9-3 loss: 0.603490  [  128/  130]
train() client id: f_00008-10-0 loss: 0.585366  [   32/  130]
train() client id: f_00008-10-1 loss: 0.718079  [   64/  130]
train() client id: f_00008-10-2 loss: 0.625653  [   96/  130]
train() client id: f_00008-10-3 loss: 0.524735  [  128/  130]
train() client id: f_00008-11-0 loss: 0.606662  [   32/  130]
train() client id: f_00008-11-1 loss: 0.538759  [   64/  130]
train() client id: f_00008-11-2 loss: 0.661143  [   96/  130]
train() client id: f_00008-11-3 loss: 0.679242  [  128/  130]
train() client id: f_00008-12-0 loss: 0.683586  [   32/  130]
train() client id: f_00008-12-1 loss: 0.613130  [   64/  130]
train() client id: f_00008-12-2 loss: 0.591055  [   96/  130]
train() client id: f_00008-12-3 loss: 0.594030  [  128/  130]
train() client id: f_00008-13-0 loss: 0.678504  [   32/  130]
train() client id: f_00008-13-1 loss: 0.615353  [   64/  130]
train() client id: f_00008-13-2 loss: 0.583245  [   96/  130]
train() client id: f_00008-13-3 loss: 0.594757  [  128/  130]
train() client id: f_00008-14-0 loss: 0.697077  [   32/  130]
train() client id: f_00008-14-1 loss: 0.561522  [   64/  130]
train() client id: f_00008-14-2 loss: 0.508119  [   96/  130]
train() client id: f_00008-14-3 loss: 0.691576  [  128/  130]
train() client id: f_00008-15-0 loss: 0.601723  [   32/  130]
train() client id: f_00008-15-1 loss: 0.596216  [   64/  130]
train() client id: f_00008-15-2 loss: 0.667086  [   96/  130]
train() client id: f_00008-15-3 loss: 0.618324  [  128/  130]
train() client id: f_00008-16-0 loss: 0.593327  [   32/  130]
train() client id: f_00008-16-1 loss: 0.602994  [   64/  130]
train() client id: f_00008-16-2 loss: 0.614549  [   96/  130]
train() client id: f_00008-16-3 loss: 0.647172  [  128/  130]
train() client id: f_00009-0-0 loss: 0.946898  [   32/  118]
train() client id: f_00009-0-1 loss: 1.042262  [   64/  118]
train() client id: f_00009-0-2 loss: 1.017078  [   96/  118]
train() client id: f_00009-1-0 loss: 0.904464  [   32/  118]
train() client id: f_00009-1-1 loss: 0.986591  [   64/  118]
train() client id: f_00009-1-2 loss: 1.028961  [   96/  118]
train() client id: f_00009-2-0 loss: 0.859898  [   32/  118]
train() client id: f_00009-2-1 loss: 1.030993  [   64/  118]
train() client id: f_00009-2-2 loss: 0.946153  [   96/  118]
train() client id: f_00009-3-0 loss: 0.968126  [   32/  118]
train() client id: f_00009-3-1 loss: 0.843869  [   64/  118]
train() client id: f_00009-3-2 loss: 0.893492  [   96/  118]
train() client id: f_00009-4-0 loss: 0.816985  [   32/  118]
train() client id: f_00009-4-1 loss: 0.966895  [   64/  118]
train() client id: f_00009-4-2 loss: 0.714837  [   96/  118]
train() client id: f_00009-5-0 loss: 0.796091  [   32/  118]
train() client id: f_00009-5-1 loss: 0.852303  [   64/  118]
train() client id: f_00009-5-2 loss: 0.869608  [   96/  118]
train() client id: f_00009-6-0 loss: 0.771233  [   32/  118]
train() client id: f_00009-6-1 loss: 0.822234  [   64/  118]
train() client id: f_00009-6-2 loss: 0.779395  [   96/  118]
train() client id: f_00009-7-0 loss: 0.849348  [   32/  118]
train() client id: f_00009-7-1 loss: 0.752554  [   64/  118]
train() client id: f_00009-7-2 loss: 0.752245  [   96/  118]
train() client id: f_00009-8-0 loss: 0.621030  [   32/  118]
train() client id: f_00009-8-1 loss: 0.876473  [   64/  118]
train() client id: f_00009-8-2 loss: 0.707713  [   96/  118]
train() client id: f_00009-9-0 loss: 0.697457  [   32/  118]
train() client id: f_00009-9-1 loss: 0.660633  [   64/  118]
train() client id: f_00009-9-2 loss: 0.671218  [   96/  118]
train() client id: f_00009-10-0 loss: 0.713318  [   32/  118]
train() client id: f_00009-10-1 loss: 0.856006  [   64/  118]
train() client id: f_00009-10-2 loss: 0.603513  [   96/  118]
train() client id: f_00009-11-0 loss: 0.578159  [   32/  118]
train() client id: f_00009-11-1 loss: 0.819446  [   64/  118]
train() client id: f_00009-11-2 loss: 0.694032  [   96/  118]
train() client id: f_00009-12-0 loss: 0.729893  [   32/  118]
train() client id: f_00009-12-1 loss: 0.603308  [   64/  118]
train() client id: f_00009-12-2 loss: 0.769324  [   96/  118]
train() client id: f_00009-13-0 loss: 0.660619  [   32/  118]
train() client id: f_00009-13-1 loss: 0.797298  [   64/  118]
train() client id: f_00009-13-2 loss: 0.627084  [   96/  118]
train() client id: f_00009-14-0 loss: 0.691266  [   32/  118]
train() client id: f_00009-14-1 loss: 0.701038  [   64/  118]
train() client id: f_00009-14-2 loss: 0.661588  [   96/  118]
train() client id: f_00009-15-0 loss: 0.864499  [   32/  118]
train() client id: f_00009-15-1 loss: 0.712301  [   64/  118]
train() client id: f_00009-15-2 loss: 0.619725  [   96/  118]
train() client id: f_00009-16-0 loss: 0.732717  [   32/  118]
train() client id: f_00009-16-1 loss: 0.663201  [   64/  118]
train() client id: f_00009-16-2 loss: 0.776836  [   96/  118]
At round 56 accuracy: 0.6472148541114059
At round 56 training accuracy: 0.590878604963112
At round 56 training loss: 0.8409121489334289
update_location
xs = 8.927491 401.223621 5.882650 0.934260 -317.581990 -165.230757 -125.849135 -5.143845 -340.120581 20.134486 
ys = -392.390647 7.291448 290.684448 -112.290817 -9.642386 0.794442 -1.381692 286.628436 25.881276 -827.232496 
xs mean: -51.68237997052123
ys mean: -73.16579882624052
dists_uav = 405.031011 413.562038 307.460654 150.366554 333.093524 193.136828 160.747982 303.615414 355.460054 833.498050 
uav_gains = -121.585304 -121.881033 -116.337498 -104.433481 -118.172446 -107.251852 -105.164846 -116.028414 -119.463490 -129.992786 
uav_gains_db_mean: -116.03111504733484
dists_bs = 596.457432 600.133168 214.712423 336.881103 233.286165 174.479267 183.101909 203.253588 222.488652 1021.052126 
bs_gains = -117.283284 -117.357993 -104.859060 -110.336405 -105.867951 -102.335886 -102.922460 -104.192129 -105.291679 -123.820401 
bs_gains_db_mean: -109.42672479596683
Round 57
-------------------------------
ene_coms = [0.03460597 0.03613797 0.00788796 0.00771861 0.0083198  0.00899214
 0.00801418 0.00762568 0.00806769 0.04893582]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.62577037 7.30517495 3.33937477 1.20158738 8.14324704 3.92926213
 1.49428777 4.78765959 3.49919136 3.54009081]
obj_prev = 40.86564616951571
eta_min = 3.8895915461481004e-27	eta_max = 0.8425909387026026
af = 8.453677589209455	bf = 1.5263657120374796	zeta = 9.299045348130402	eta = 0.909090909090909
af = 8.453677589209455	bf = 1.5263657120374796	zeta = 25.652959673801035	eta = 0.3295400490510677
af = 8.453677589209455	bf = 1.5263657120374796	zeta = 16.273056960508168	eta = 0.5194892151932508
af = 8.453677589209455	bf = 1.5263657120374796	zeta = 14.698436339858324	eta = 0.5751412867153282
af = 8.453677589209455	bf = 1.5263657120374796	zeta = 14.598804156371816	eta = 0.5790664426113115
af = 8.453677589209455	bf = 1.5263657120374796	zeta = 14.598340680340867	eta = 0.5790848271265351
af = 8.453677589209455	bf = 1.5263657120374796	zeta = 14.598340670219239	eta = 0.5790848275280384
eta = 0.5790848275280384
ene_coms = [0.03460597 0.03613797 0.00788796 0.00771861 0.0083198  0.00899214
 0.00801418 0.00762568 0.00806769 0.04893582]
ene_comp = [0.04507989 0.09481087 0.04436433 0.01538441 0.10947973 0.05223542
 0.01931995 0.06404206 0.04651099 0.04221767]
ene_total = [1.63899873 2.69338882 1.07473842 0.4751887  2.42293033 1.25934408
 0.56221525 1.4740803  1.12258809 1.87486795]
ti_comp = [0.97134261 0.9560226  1.23852264 1.24021615 1.23420429 1.22748083
 1.2372605  1.24114542 1.2367254  0.82804411]
ti_coms = [0.34605965 0.36137967 0.07887963 0.07718612 0.08319797 0.08992143
 0.08014177 0.07625684 0.08067686 0.48935815]
t_total = [27.09666252 27.09666252 27.09666252 27.09666252 27.09666252 27.09666252
 27.09666252 27.09666252 27.09666252 27.09666252]
ene_coms = [0.03460597 0.03613797 0.00788796 0.00771861 0.0083198  0.00899214
 0.00801418 0.00762568 0.00806769 0.04893582]
ene_comp = [6.06853118e-06 5.82798121e-05 3.55773826e-06 1.47954248e-07
 5.38402427e-05 5.91215710e-06 2.94425770e-07 1.06568926e-05
 4.11150032e-06 6.85892273e-06]
ene_total = [0.71190905 0.74449354 0.16231479 0.1587614  0.17223109 0.18507428
 0.16484366 0.15706619 0.16602277 1.00666528]
optimize_network iter = 0 obj = 3.6293820752458803
eta = 0.5790848275280384
freqs = [23204936.3148609  49586104.77649828 17910182.60221902  6202308.31740185
 44352351.50633583 21277490.94328826  7807552.0645567  25799580.11378734
 18804091.14161654 25492405.07928325]
eta_min = 0.5790848275280387	eta_max = 0.5790848275280375
af = 0.0023727899879370973	bf = 1.5263657120374796	zeta = 0.0026100689867308074	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03460597 0.03613797 0.00788796 0.00771861 0.0083198  0.00899214
 0.00801418 0.00762568 0.00806769 0.04893582]
ene_comp = [1.05873352e-06 1.01676648e-05 6.20693317e-07 2.58125265e-08
 9.39312463e-06 1.03145205e-06 5.13663723e-08 1.85923234e-06
 7.17304251e-07 1.19662752e-06]
ene_total = [3.29570945 3.44247308 0.75124863 0.73506428 0.79320866 0.8564415
 0.76321405 0.72638916 0.76837331 4.66038838]
ti_comp = [0.97134261 0.9560226  1.23852264 1.24021615 1.23420429 1.22748083
 1.2372605  1.24114542 1.2367254  0.82804411]
ti_coms = [0.34605965 0.36137967 0.07887963 0.07718612 0.08319797 0.08992143
 0.08014177 0.07625684 0.08067686 0.48935815]
t_total = [27.09666252 27.09666252 27.09666252 27.09666252 27.09666252 27.09666252
 27.09666252 27.09666252 27.09666252 27.09666252]
ene_coms = [0.03460597 0.03613797 0.00788796 0.00771861 0.0083198  0.00899214
 0.00801418 0.00762568 0.00806769 0.04893582]
ene_comp = [6.06853118e-06 5.82798121e-05 3.55773826e-06 1.47954248e-07
 5.38402427e-05 5.91215710e-06 2.94425770e-07 1.06568926e-05
 4.11150032e-06 6.85892273e-06]
ene_total = [0.71190905 0.74449354 0.16231479 0.1587614  0.17223109 0.18507428
 0.16484366 0.15706619 0.16602277 1.00666528]
optimize_network iter = 1 obj = 3.6293820752458723
eta = 0.5790848275280375
freqs = [23204936.31486088 49586104.77649827 17910182.60221902  6202308.31740186
 44352351.50633584 21277490.94328826  7807552.06455671 25799580.11378735
 18804091.14161655 25492405.07928323]
Done!
ene_coms = [0.03460597 0.03613797 0.00788796 0.00771861 0.0083198  0.00899214
 0.00801418 0.00762568 0.00806769 0.04893582]
ene_comp = [5.76700373e-06 5.53840598e-05 3.38096471e-06 1.40602837e-07
 5.11650795e-05 5.61839941e-06 2.79796620e-07 1.01273830e-05
 3.90721197e-06 6.51812305e-06]
ene_total = [0.03461173 0.03619335 0.00789134 0.00771875 0.00837096 0.00899776
 0.00801446 0.00763581 0.00807159 0.04894233]
At round 57 energy consumption: 0.17644809853730428
At round 57 eta: 0.5790848275280375
At round 57 a_n: 8.65748957900329
At round 57 local rounds: 17.888844000587703
At round 57 global rounds: 20.568252572506093
gradient difference: 0.31939518451690674
train() client id: f_00000-0-0 loss: 1.209567  [   32/  126]
train() client id: f_00000-0-1 loss: 1.206773  [   64/  126]
train() client id: f_00000-0-2 loss: 1.224335  [   96/  126]
train() client id: f_00000-1-0 loss: 1.078187  [   32/  126]
train() client id: f_00000-1-1 loss: 1.083384  [   64/  126]
train() client id: f_00000-1-2 loss: 0.919781  [   96/  126]
train() client id: f_00000-2-0 loss: 1.107313  [   32/  126]
train() client id: f_00000-2-1 loss: 1.153887  [   64/  126]
train() client id: f_00000-2-2 loss: 0.943134  [   96/  126]
train() client id: f_00000-3-0 loss: 0.958231  [   32/  126]
train() client id: f_00000-3-1 loss: 1.005690  [   64/  126]
train() client id: f_00000-3-2 loss: 0.825066  [   96/  126]
train() client id: f_00000-4-0 loss: 0.860081  [   32/  126]
train() client id: f_00000-4-1 loss: 0.865408  [   64/  126]
train() client id: f_00000-4-2 loss: 0.887844  [   96/  126]
train() client id: f_00000-5-0 loss: 0.906710  [   32/  126]
train() client id: f_00000-5-1 loss: 0.812462  [   64/  126]
train() client id: f_00000-5-2 loss: 0.842948  [   96/  126]
train() client id: f_00000-6-0 loss: 0.855582  [   32/  126]
train() client id: f_00000-6-1 loss: 0.897517  [   64/  126]
train() client id: f_00000-6-2 loss: 0.808393  [   96/  126]
train() client id: f_00000-7-0 loss: 0.837076  [   32/  126]
train() client id: f_00000-7-1 loss: 0.816224  [   64/  126]
train() client id: f_00000-7-2 loss: 0.889233  [   96/  126]
train() client id: f_00000-8-0 loss: 0.809131  [   32/  126]
train() client id: f_00000-8-1 loss: 0.796740  [   64/  126]
train() client id: f_00000-8-2 loss: 0.839566  [   96/  126]
train() client id: f_00000-9-0 loss: 0.718007  [   32/  126]
train() client id: f_00000-9-1 loss: 0.879729  [   64/  126]
train() client id: f_00000-9-2 loss: 0.761580  [   96/  126]
train() client id: f_00000-10-0 loss: 0.822545  [   32/  126]
train() client id: f_00000-10-1 loss: 0.691402  [   64/  126]
train() client id: f_00000-10-2 loss: 0.871847  [   96/  126]
train() client id: f_00000-11-0 loss: 0.831938  [   32/  126]
train() client id: f_00000-11-1 loss: 0.748904  [   64/  126]
train() client id: f_00000-11-2 loss: 0.847011  [   96/  126]
train() client id: f_00000-12-0 loss: 0.771939  [   32/  126]
train() client id: f_00000-12-1 loss: 0.884499  [   64/  126]
train() client id: f_00000-12-2 loss: 0.732549  [   96/  126]
train() client id: f_00000-13-0 loss: 0.789076  [   32/  126]
train() client id: f_00000-13-1 loss: 0.818600  [   64/  126]
train() client id: f_00000-13-2 loss: 0.870542  [   96/  126]
train() client id: f_00000-14-0 loss: 0.856150  [   32/  126]
train() client id: f_00000-14-1 loss: 0.882261  [   64/  126]
train() client id: f_00000-14-2 loss: 0.699150  [   96/  126]
train() client id: f_00000-15-0 loss: 0.833941  [   32/  126]
train() client id: f_00000-15-1 loss: 0.854377  [   64/  126]
train() client id: f_00000-15-2 loss: 0.726524  [   96/  126]
train() client id: f_00000-16-0 loss: 0.886055  [   32/  126]
train() client id: f_00000-16-1 loss: 0.794753  [   64/  126]
train() client id: f_00000-16-2 loss: 0.755687  [   96/  126]
train() client id: f_00001-0-0 loss: 0.419035  [   32/  265]
train() client id: f_00001-0-1 loss: 0.370893  [   64/  265]
train() client id: f_00001-0-2 loss: 0.322534  [   96/  265]
train() client id: f_00001-0-3 loss: 0.304477  [  128/  265]
train() client id: f_00001-0-4 loss: 0.294969  [  160/  265]
train() client id: f_00001-0-5 loss: 0.389252  [  192/  265]
train() client id: f_00001-0-6 loss: 0.341104  [  224/  265]
train() client id: f_00001-0-7 loss: 0.280091  [  256/  265]
train() client id: f_00001-1-0 loss: 0.285370  [   32/  265]
train() client id: f_00001-1-1 loss: 0.287114  [   64/  265]
train() client id: f_00001-1-2 loss: 0.343592  [   96/  265]
train() client id: f_00001-1-3 loss: 0.224250  [  128/  265]
train() client id: f_00001-1-4 loss: 0.456452  [  160/  265]
train() client id: f_00001-1-5 loss: 0.386056  [  192/  265]
train() client id: f_00001-1-6 loss: 0.263740  [  224/  265]
train() client id: f_00001-1-7 loss: 0.281540  [  256/  265]
train() client id: f_00001-2-0 loss: 0.309116  [   32/  265]
train() client id: f_00001-2-1 loss: 0.311321  [   64/  265]
train() client id: f_00001-2-2 loss: 0.351035  [   96/  265]
train() client id: f_00001-2-3 loss: 0.295694  [  128/  265]
train() client id: f_00001-2-4 loss: 0.341787  [  160/  265]
train() client id: f_00001-2-5 loss: 0.236497  [  192/  265]
train() client id: f_00001-2-6 loss: 0.370818  [  224/  265]
train() client id: f_00001-2-7 loss: 0.328870  [  256/  265]
train() client id: f_00001-3-0 loss: 0.245465  [   32/  265]
train() client id: f_00001-3-1 loss: 0.308147  [   64/  265]
train() client id: f_00001-3-2 loss: 0.344452  [   96/  265]
train() client id: f_00001-3-3 loss: 0.332490  [  128/  265]
train() client id: f_00001-3-4 loss: 0.332704  [  160/  265]
train() client id: f_00001-3-5 loss: 0.345360  [  192/  265]
train() client id: f_00001-3-6 loss: 0.267579  [  224/  265]
train() client id: f_00001-3-7 loss: 0.308085  [  256/  265]
train() client id: f_00001-4-0 loss: 0.232141  [   32/  265]
train() client id: f_00001-4-1 loss: 0.288574  [   64/  265]
train() client id: f_00001-4-2 loss: 0.217167  [   96/  265]
train() client id: f_00001-4-3 loss: 0.222849  [  128/  265]
train() client id: f_00001-4-4 loss: 0.249215  [  160/  265]
train() client id: f_00001-4-5 loss: 0.438909  [  192/  265]
train() client id: f_00001-4-6 loss: 0.421732  [  224/  265]
train() client id: f_00001-4-7 loss: 0.278450  [  256/  265]
train() client id: f_00001-5-0 loss: 0.325527  [   32/  265]
train() client id: f_00001-5-1 loss: 0.279526  [   64/  265]
train() client id: f_00001-5-2 loss: 0.247880  [   96/  265]
train() client id: f_00001-5-3 loss: 0.220342  [  128/  265]
train() client id: f_00001-5-4 loss: 0.408752  [  160/  265]
train() client id: f_00001-5-5 loss: 0.306155  [  192/  265]
train() client id: f_00001-5-6 loss: 0.366346  [  224/  265]
train() client id: f_00001-5-7 loss: 0.252437  [  256/  265]
train() client id: f_00001-6-0 loss: 0.406459  [   32/  265]
train() client id: f_00001-6-1 loss: 0.281649  [   64/  265]
train() client id: f_00001-6-2 loss: 0.458184  [   96/  265]
train() client id: f_00001-6-3 loss: 0.206898  [  128/  265]
train() client id: f_00001-6-4 loss: 0.210564  [  160/  265]
train() client id: f_00001-6-5 loss: 0.213920  [  192/  265]
train() client id: f_00001-6-6 loss: 0.270343  [  224/  265]
train() client id: f_00001-6-7 loss: 0.301329  [  256/  265]
train() client id: f_00001-7-0 loss: 0.200081  [   32/  265]
train() client id: f_00001-7-1 loss: 0.279895  [   64/  265]
train() client id: f_00001-7-2 loss: 0.385291  [   96/  265]
train() client id: f_00001-7-3 loss: 0.190485  [  128/  265]
train() client id: f_00001-7-4 loss: 0.276938  [  160/  265]
train() client id: f_00001-7-5 loss: 0.274509  [  192/  265]
train() client id: f_00001-7-6 loss: 0.417999  [  224/  265]
train() client id: f_00001-7-7 loss: 0.311440  [  256/  265]
train() client id: f_00001-8-0 loss: 0.364828  [   32/  265]
train() client id: f_00001-8-1 loss: 0.303412  [   64/  265]
train() client id: f_00001-8-2 loss: 0.270085  [   96/  265]
train() client id: f_00001-8-3 loss: 0.312715  [  128/  265]
train() client id: f_00001-8-4 loss: 0.265735  [  160/  265]
train() client id: f_00001-8-5 loss: 0.297334  [  192/  265]
train() client id: f_00001-8-6 loss: 0.233444  [  224/  265]
train() client id: f_00001-8-7 loss: 0.276171  [  256/  265]
train() client id: f_00001-9-0 loss: 0.204532  [   32/  265]
train() client id: f_00001-9-1 loss: 0.330130  [   64/  265]
train() client id: f_00001-9-2 loss: 0.328522  [   96/  265]
train() client id: f_00001-9-3 loss: 0.363988  [  128/  265]
train() client id: f_00001-9-4 loss: 0.320962  [  160/  265]
train() client id: f_00001-9-5 loss: 0.232182  [  192/  265]
train() client id: f_00001-9-6 loss: 0.239940  [  224/  265]
train() client id: f_00001-9-7 loss: 0.279894  [  256/  265]
train() client id: f_00001-10-0 loss: 0.291887  [   32/  265]
train() client id: f_00001-10-1 loss: 0.288548  [   64/  265]
train() client id: f_00001-10-2 loss: 0.178874  [   96/  265]
train() client id: f_00001-10-3 loss: 0.282809  [  128/  265]
train() client id: f_00001-10-4 loss: 0.512513  [  160/  265]
train() client id: f_00001-10-5 loss: 0.198744  [  192/  265]
train() client id: f_00001-10-6 loss: 0.262743  [  224/  265]
train() client id: f_00001-10-7 loss: 0.252783  [  256/  265]
train() client id: f_00001-11-0 loss: 0.214735  [   32/  265]
train() client id: f_00001-11-1 loss: 0.195095  [   64/  265]
train() client id: f_00001-11-2 loss: 0.275677  [   96/  265]
train() client id: f_00001-11-3 loss: 0.474361  [  128/  265]
train() client id: f_00001-11-4 loss: 0.269302  [  160/  265]
train() client id: f_00001-11-5 loss: 0.184213  [  192/  265]
train() client id: f_00001-11-6 loss: 0.280040  [  224/  265]
train() client id: f_00001-11-7 loss: 0.360182  [  256/  265]
train() client id: f_00001-12-0 loss: 0.239488  [   32/  265]
train() client id: f_00001-12-1 loss: 0.201008  [   64/  265]
train() client id: f_00001-12-2 loss: 0.161851  [   96/  265]
train() client id: f_00001-12-3 loss: 0.290586  [  128/  265]
train() client id: f_00001-12-4 loss: 0.353725  [  160/  265]
train() client id: f_00001-12-5 loss: 0.241028  [  192/  265]
train() client id: f_00001-12-6 loss: 0.253047  [  224/  265]
train() client id: f_00001-12-7 loss: 0.393873  [  256/  265]
train() client id: f_00001-13-0 loss: 0.238136  [   32/  265]
train() client id: f_00001-13-1 loss: 0.261203  [   64/  265]
train() client id: f_00001-13-2 loss: 0.315167  [   96/  265]
train() client id: f_00001-13-3 loss: 0.406985  [  128/  265]
train() client id: f_00001-13-4 loss: 0.201383  [  160/  265]
train() client id: f_00001-13-5 loss: 0.220404  [  192/  265]
train() client id: f_00001-13-6 loss: 0.283304  [  224/  265]
train() client id: f_00001-13-7 loss: 0.172038  [  256/  265]
train() client id: f_00001-14-0 loss: 0.216803  [   32/  265]
train() client id: f_00001-14-1 loss: 0.284485  [   64/  265]
train() client id: f_00001-14-2 loss: 0.183002  [   96/  265]
train() client id: f_00001-14-3 loss: 0.330073  [  128/  265]
train() client id: f_00001-14-4 loss: 0.237618  [  160/  265]
train() client id: f_00001-14-5 loss: 0.194113  [  192/  265]
train() client id: f_00001-14-6 loss: 0.449912  [  224/  265]
train() client id: f_00001-14-7 loss: 0.333023  [  256/  265]
train() client id: f_00001-15-0 loss: 0.286340  [   32/  265]
train() client id: f_00001-15-1 loss: 0.322653  [   64/  265]
train() client id: f_00001-15-2 loss: 0.281780  [   96/  265]
train() client id: f_00001-15-3 loss: 0.331379  [  128/  265]
train() client id: f_00001-15-4 loss: 0.281751  [  160/  265]
train() client id: f_00001-15-5 loss: 0.271804  [  192/  265]
train() client id: f_00001-15-6 loss: 0.183759  [  224/  265]
train() client id: f_00001-15-7 loss: 0.264296  [  256/  265]
train() client id: f_00001-16-0 loss: 0.298857  [   32/  265]
train() client id: f_00001-16-1 loss: 0.463580  [   64/  265]
train() client id: f_00001-16-2 loss: 0.165334  [   96/  265]
train() client id: f_00001-16-3 loss: 0.189084  [  128/  265]
train() client id: f_00001-16-4 loss: 0.215117  [  160/  265]
train() client id: f_00001-16-5 loss: 0.253039  [  192/  265]
train() client id: f_00001-16-6 loss: 0.338911  [  224/  265]
train() client id: f_00001-16-7 loss: 0.179475  [  256/  265]
train() client id: f_00002-0-0 loss: 1.265752  [   32/  124]
train() client id: f_00002-0-1 loss: 1.248434  [   64/  124]
train() client id: f_00002-0-2 loss: 1.264994  [   96/  124]
train() client id: f_00002-1-0 loss: 1.257725  [   32/  124]
train() client id: f_00002-1-1 loss: 1.130547  [   64/  124]
train() client id: f_00002-1-2 loss: 1.228113  [   96/  124]
train() client id: f_00002-2-0 loss: 1.113464  [   32/  124]
train() client id: f_00002-2-1 loss: 1.305261  [   64/  124]
train() client id: f_00002-2-2 loss: 1.197808  [   96/  124]
train() client id: f_00002-3-0 loss: 1.252120  [   32/  124]
train() client id: f_00002-3-1 loss: 1.242947  [   64/  124]
train() client id: f_00002-3-2 loss: 1.046476  [   96/  124]
train() client id: f_00002-4-0 loss: 1.027971  [   32/  124]
train() client id: f_00002-4-1 loss: 1.216426  [   64/  124]
train() client id: f_00002-4-2 loss: 1.110702  [   96/  124]
train() client id: f_00002-5-0 loss: 1.142507  [   32/  124]
train() client id: f_00002-5-1 loss: 1.082403  [   64/  124]
train() client id: f_00002-5-2 loss: 1.076647  [   96/  124]
train() client id: f_00002-6-0 loss: 0.947046  [   32/  124]
train() client id: f_00002-6-1 loss: 1.153123  [   64/  124]
train() client id: f_00002-6-2 loss: 1.083838  [   96/  124]
train() client id: f_00002-7-0 loss: 0.996667  [   32/  124]
train() client id: f_00002-7-1 loss: 1.058059  [   64/  124]
train() client id: f_00002-7-2 loss: 1.078514  [   96/  124]
train() client id: f_00002-8-0 loss: 0.809284  [   32/  124]
train() client id: f_00002-8-1 loss: 1.082018  [   64/  124]
train() client id: f_00002-8-2 loss: 1.137647  [   96/  124]
train() client id: f_00002-9-0 loss: 0.847906  [   32/  124]
train() client id: f_00002-9-1 loss: 1.035931  [   64/  124]
train() client id: f_00002-9-2 loss: 1.072555  [   96/  124]
train() client id: f_00002-10-0 loss: 0.957668  [   32/  124]
train() client id: f_00002-10-1 loss: 0.989368  [   64/  124]
train() client id: f_00002-10-2 loss: 1.081435  [   96/  124]
train() client id: f_00002-11-0 loss: 1.089367  [   32/  124]
train() client id: f_00002-11-1 loss: 0.941750  [   64/  124]
train() client id: f_00002-11-2 loss: 0.974824  [   96/  124]
train() client id: f_00002-12-0 loss: 0.846278  [   32/  124]
train() client id: f_00002-12-1 loss: 0.948911  [   64/  124]
train() client id: f_00002-12-2 loss: 1.075301  [   96/  124]
train() client id: f_00002-13-0 loss: 1.120484  [   32/  124]
train() client id: f_00002-13-1 loss: 0.897075  [   64/  124]
train() client id: f_00002-13-2 loss: 0.862266  [   96/  124]
train() client id: f_00002-14-0 loss: 1.104950  [   32/  124]
train() client id: f_00002-14-1 loss: 0.918664  [   64/  124]
train() client id: f_00002-14-2 loss: 0.989859  [   96/  124]
train() client id: f_00002-15-0 loss: 0.826350  [   32/  124]
train() client id: f_00002-15-1 loss: 1.097603  [   64/  124]
train() client id: f_00002-15-2 loss: 1.002747  [   96/  124]
train() client id: f_00002-16-0 loss: 1.052065  [   32/  124]
train() client id: f_00002-16-1 loss: 1.039089  [   64/  124]
train() client id: f_00002-16-2 loss: 0.808252  [   96/  124]
train() client id: f_00003-0-0 loss: 0.691660  [   32/   43]
train() client id: f_00003-1-0 loss: 0.726621  [   32/   43]
train() client id: f_00003-2-0 loss: 0.725549  [   32/   43]
train() client id: f_00003-3-0 loss: 0.642646  [   32/   43]
train() client id: f_00003-4-0 loss: 0.654990  [   32/   43]
train() client id: f_00003-5-0 loss: 0.740852  [   32/   43]
train() client id: f_00003-6-0 loss: 0.540929  [   32/   43]
train() client id: f_00003-7-0 loss: 0.583834  [   32/   43]
train() client id: f_00003-8-0 loss: 0.633783  [   32/   43]
train() client id: f_00003-9-0 loss: 0.637520  [   32/   43]
train() client id: f_00003-10-0 loss: 0.665758  [   32/   43]
train() client id: f_00003-11-0 loss: 0.554677  [   32/   43]
train() client id: f_00003-12-0 loss: 0.618875  [   32/   43]
train() client id: f_00003-13-0 loss: 0.619717  [   32/   43]
train() client id: f_00003-14-0 loss: 0.443209  [   32/   43]
train() client id: f_00003-15-0 loss: 0.736284  [   32/   43]
train() client id: f_00003-16-0 loss: 0.642167  [   32/   43]
train() client id: f_00004-0-0 loss: 1.079491  [   32/  306]
train() client id: f_00004-0-1 loss: 1.024274  [   64/  306]
train() client id: f_00004-0-2 loss: 0.970039  [   96/  306]
train() client id: f_00004-0-3 loss: 0.720806  [  128/  306]
train() client id: f_00004-0-4 loss: 1.004848  [  160/  306]
train() client id: f_00004-0-5 loss: 0.891447  [  192/  306]
train() client id: f_00004-0-6 loss: 1.124838  [  224/  306]
train() client id: f_00004-0-7 loss: 1.012870  [  256/  306]
train() client id: f_00004-0-8 loss: 0.822295  [  288/  306]
train() client id: f_00004-1-0 loss: 0.826137  [   32/  306]
train() client id: f_00004-1-1 loss: 1.121952  [   64/  306]
train() client id: f_00004-1-2 loss: 0.979863  [   96/  306]
train() client id: f_00004-1-3 loss: 0.856203  [  128/  306]
train() client id: f_00004-1-4 loss: 0.946220  [  160/  306]
train() client id: f_00004-1-5 loss: 1.103496  [  192/  306]
train() client id: f_00004-1-6 loss: 0.961291  [  224/  306]
train() client id: f_00004-1-7 loss: 0.808737  [  256/  306]
train() client id: f_00004-1-8 loss: 1.030950  [  288/  306]
train() client id: f_00004-2-0 loss: 0.947562  [   32/  306]
train() client id: f_00004-2-1 loss: 0.937379  [   64/  306]
train() client id: f_00004-2-2 loss: 0.885624  [   96/  306]
train() client id: f_00004-2-3 loss: 0.815800  [  128/  306]
train() client id: f_00004-2-4 loss: 0.955483  [  160/  306]
train() client id: f_00004-2-5 loss: 1.067556  [  192/  306]
train() client id: f_00004-2-6 loss: 0.940945  [  224/  306]
train() client id: f_00004-2-7 loss: 0.968133  [  256/  306]
train() client id: f_00004-2-8 loss: 0.928816  [  288/  306]
train() client id: f_00004-3-0 loss: 0.916168  [   32/  306]
train() client id: f_00004-3-1 loss: 1.007880  [   64/  306]
train() client id: f_00004-3-2 loss: 1.093613  [   96/  306]
train() client id: f_00004-3-3 loss: 1.066598  [  128/  306]
train() client id: f_00004-3-4 loss: 1.015388  [  160/  306]
train() client id: f_00004-3-5 loss: 0.670779  [  192/  306]
train() client id: f_00004-3-6 loss: 0.909830  [  224/  306]
train() client id: f_00004-3-7 loss: 0.880079  [  256/  306]
train() client id: f_00004-3-8 loss: 0.933876  [  288/  306]
train() client id: f_00004-4-0 loss: 0.888243  [   32/  306]
train() client id: f_00004-4-1 loss: 1.079806  [   64/  306]
train() client id: f_00004-4-2 loss: 0.969649  [   96/  306]
train() client id: f_00004-4-3 loss: 0.866824  [  128/  306]
train() client id: f_00004-4-4 loss: 1.087519  [  160/  306]
train() client id: f_00004-4-5 loss: 0.987833  [  192/  306]
train() client id: f_00004-4-6 loss: 0.941922  [  224/  306]
train() client id: f_00004-4-7 loss: 0.938144  [  256/  306]
train() client id: f_00004-4-8 loss: 0.848271  [  288/  306]
train() client id: f_00004-5-0 loss: 0.884107  [   32/  306]
train() client id: f_00004-5-1 loss: 0.894010  [   64/  306]
train() client id: f_00004-5-2 loss: 0.888241  [   96/  306]
train() client id: f_00004-5-3 loss: 0.969636  [  128/  306]
train() client id: f_00004-5-4 loss: 0.964973  [  160/  306]
train() client id: f_00004-5-5 loss: 0.862221  [  192/  306]
train() client id: f_00004-5-6 loss: 0.985787  [  224/  306]
train() client id: f_00004-5-7 loss: 0.946236  [  256/  306]
train() client id: f_00004-5-8 loss: 1.009354  [  288/  306]
train() client id: f_00004-6-0 loss: 1.057138  [   32/  306]
train() client id: f_00004-6-1 loss: 0.871972  [   64/  306]
train() client id: f_00004-6-2 loss: 1.084553  [   96/  306]
train() client id: f_00004-6-3 loss: 1.081879  [  128/  306]
train() client id: f_00004-6-4 loss: 0.961221  [  160/  306]
train() client id: f_00004-6-5 loss: 0.879017  [  192/  306]
train() client id: f_00004-6-6 loss: 0.912033  [  224/  306]
train() client id: f_00004-6-7 loss: 0.789711  [  256/  306]
train() client id: f_00004-6-8 loss: 0.859598  [  288/  306]
train() client id: f_00004-7-0 loss: 1.145603  [   32/  306]
train() client id: f_00004-7-1 loss: 0.810256  [   64/  306]
train() client id: f_00004-7-2 loss: 0.932355  [   96/  306]
train() client id: f_00004-7-3 loss: 0.976417  [  128/  306]
train() client id: f_00004-7-4 loss: 0.903898  [  160/  306]
train() client id: f_00004-7-5 loss: 0.933611  [  192/  306]
train() client id: f_00004-7-6 loss: 0.876126  [  224/  306]
train() client id: f_00004-7-7 loss: 0.796008  [  256/  306]
train() client id: f_00004-7-8 loss: 0.955426  [  288/  306]
train() client id: f_00004-8-0 loss: 0.896368  [   32/  306]
train() client id: f_00004-8-1 loss: 0.854135  [   64/  306]
train() client id: f_00004-8-2 loss: 0.960931  [   96/  306]
train() client id: f_00004-8-3 loss: 0.917458  [  128/  306]
train() client id: f_00004-8-4 loss: 0.955816  [  160/  306]
train() client id: f_00004-8-5 loss: 1.033288  [  192/  306]
train() client id: f_00004-8-6 loss: 0.822006  [  224/  306]
train() client id: f_00004-8-7 loss: 1.028818  [  256/  306]
train() client id: f_00004-8-8 loss: 0.912870  [  288/  306]
train() client id: f_00004-9-0 loss: 0.997516  [   32/  306]
train() client id: f_00004-9-1 loss: 1.053182  [   64/  306]
train() client id: f_00004-9-2 loss: 0.856611  [   96/  306]
train() client id: f_00004-9-3 loss: 0.864084  [  128/  306]
train() client id: f_00004-9-4 loss: 0.841297  [  160/  306]
train() client id: f_00004-9-5 loss: 0.824700  [  192/  306]
train() client id: f_00004-9-6 loss: 0.907008  [  224/  306]
train() client id: f_00004-9-7 loss: 0.954517  [  256/  306]
train() client id: f_00004-9-8 loss: 0.962515  [  288/  306]
train() client id: f_00004-10-0 loss: 0.938950  [   32/  306]
train() client id: f_00004-10-1 loss: 0.956004  [   64/  306]
train() client id: f_00004-10-2 loss: 0.878273  [   96/  306]
train() client id: f_00004-10-3 loss: 0.949407  [  128/  306]
train() client id: f_00004-10-4 loss: 0.978941  [  160/  306]
train() client id: f_00004-10-5 loss: 0.935456  [  192/  306]
train() client id: f_00004-10-6 loss: 0.923598  [  224/  306]
train() client id: f_00004-10-7 loss: 0.830560  [  256/  306]
train() client id: f_00004-10-8 loss: 1.013743  [  288/  306]
train() client id: f_00004-11-0 loss: 0.965403  [   32/  306]
train() client id: f_00004-11-1 loss: 0.879885  [   64/  306]
train() client id: f_00004-11-2 loss: 0.980176  [   96/  306]
train() client id: f_00004-11-3 loss: 0.980348  [  128/  306]
train() client id: f_00004-11-4 loss: 0.901683  [  160/  306]
train() client id: f_00004-11-5 loss: 0.894405  [  192/  306]
train() client id: f_00004-11-6 loss: 0.911275  [  224/  306]
train() client id: f_00004-11-7 loss: 0.921472  [  256/  306]
train() client id: f_00004-11-8 loss: 0.784484  [  288/  306]
train() client id: f_00004-12-0 loss: 0.996180  [   32/  306]
train() client id: f_00004-12-1 loss: 0.951034  [   64/  306]
train() client id: f_00004-12-2 loss: 0.902194  [   96/  306]
train() client id: f_00004-12-3 loss: 0.880286  [  128/  306]
train() client id: f_00004-12-4 loss: 0.804142  [  160/  306]
train() client id: f_00004-12-5 loss: 0.873150  [  192/  306]
train() client id: f_00004-12-6 loss: 0.991417  [  224/  306]
train() client id: f_00004-12-7 loss: 0.942011  [  256/  306]
train() client id: f_00004-12-8 loss: 0.826162  [  288/  306]
train() client id: f_00004-13-0 loss: 0.840912  [   32/  306]
train() client id: f_00004-13-1 loss: 1.014358  [   64/  306]
train() client id: f_00004-13-2 loss: 0.920937  [   96/  306]
train() client id: f_00004-13-3 loss: 0.824659  [  128/  306]
train() client id: f_00004-13-4 loss: 0.787153  [  160/  306]
train() client id: f_00004-13-5 loss: 0.851066  [  192/  306]
train() client id: f_00004-13-6 loss: 0.899768  [  224/  306]
train() client id: f_00004-13-7 loss: 0.961237  [  256/  306]
train() client id: f_00004-13-8 loss: 1.053642  [  288/  306]
train() client id: f_00004-14-0 loss: 0.860669  [   32/  306]
train() client id: f_00004-14-1 loss: 0.951781  [   64/  306]
train() client id: f_00004-14-2 loss: 0.943186  [   96/  306]
train() client id: f_00004-14-3 loss: 0.705726  [  128/  306]
train() client id: f_00004-14-4 loss: 0.955124  [  160/  306]
train() client id: f_00004-14-5 loss: 1.044299  [  192/  306]
train() client id: f_00004-14-6 loss: 0.812247  [  224/  306]
train() client id: f_00004-14-7 loss: 1.024318  [  256/  306]
train() client id: f_00004-14-8 loss: 0.918014  [  288/  306]
train() client id: f_00004-15-0 loss: 0.849168  [   32/  306]
train() client id: f_00004-15-1 loss: 0.897996  [   64/  306]
train() client id: f_00004-15-2 loss: 0.901755  [   96/  306]
train() client id: f_00004-15-3 loss: 0.934828  [  128/  306]
train() client id: f_00004-15-4 loss: 0.948320  [  160/  306]
train() client id: f_00004-15-5 loss: 0.865817  [  192/  306]
train() client id: f_00004-15-6 loss: 0.918988  [  224/  306]
train() client id: f_00004-15-7 loss: 0.945322  [  256/  306]
train() client id: f_00004-15-8 loss: 0.860679  [  288/  306]
train() client id: f_00004-16-0 loss: 0.945776  [   32/  306]
train() client id: f_00004-16-1 loss: 0.949128  [   64/  306]
train() client id: f_00004-16-2 loss: 0.827936  [   96/  306]
train() client id: f_00004-16-3 loss: 0.920733  [  128/  306]
train() client id: f_00004-16-4 loss: 0.853774  [  160/  306]
train() client id: f_00004-16-5 loss: 0.886938  [  192/  306]
train() client id: f_00004-16-6 loss: 0.907317  [  224/  306]
train() client id: f_00004-16-7 loss: 0.970833  [  256/  306]
train() client id: f_00004-16-8 loss: 0.898250  [  288/  306]
train() client id: f_00005-0-0 loss: 0.411344  [   32/  146]
train() client id: f_00005-0-1 loss: 0.537187  [   64/  146]
train() client id: f_00005-0-2 loss: 0.439983  [   96/  146]
train() client id: f_00005-0-3 loss: 0.956798  [  128/  146]
train() client id: f_00005-1-0 loss: 0.732399  [   32/  146]
train() client id: f_00005-1-1 loss: 0.564636  [   64/  146]
train() client id: f_00005-1-2 loss: 0.661111  [   96/  146]
train() client id: f_00005-1-3 loss: 0.294415  [  128/  146]
train() client id: f_00005-2-0 loss: 0.444904  [   32/  146]
train() client id: f_00005-2-1 loss: 0.390746  [   64/  146]
train() client id: f_00005-2-2 loss: 0.548903  [   96/  146]
train() client id: f_00005-2-3 loss: 0.933141  [  128/  146]
train() client id: f_00005-3-0 loss: 0.505405  [   32/  146]
train() client id: f_00005-3-1 loss: 0.674014  [   64/  146]
train() client id: f_00005-3-2 loss: 0.546843  [   96/  146]
train() client id: f_00005-3-3 loss: 0.570792  [  128/  146]
train() client id: f_00005-4-0 loss: 0.674086  [   32/  146]
train() client id: f_00005-4-1 loss: 0.271753  [   64/  146]
train() client id: f_00005-4-2 loss: 0.654051  [   96/  146]
train() client id: f_00005-4-3 loss: 0.498823  [  128/  146]
train() client id: f_00005-5-0 loss: 0.439923  [   32/  146]
train() client id: f_00005-5-1 loss: 0.562006  [   64/  146]
train() client id: f_00005-5-2 loss: 0.602738  [   96/  146]
train() client id: f_00005-5-3 loss: 0.704527  [  128/  146]
train() client id: f_00005-6-0 loss: 0.626672  [   32/  146]
train() client id: f_00005-6-1 loss: 0.547085  [   64/  146]
train() client id: f_00005-6-2 loss: 0.532593  [   96/  146]
train() client id: f_00005-6-3 loss: 0.374056  [  128/  146]
train() client id: f_00005-7-0 loss: 0.544969  [   32/  146]
train() client id: f_00005-7-1 loss: 0.458918  [   64/  146]
train() client id: f_00005-7-2 loss: 0.694481  [   96/  146]
train() client id: f_00005-7-3 loss: 0.428788  [  128/  146]
train() client id: f_00005-8-0 loss: 0.317740  [   32/  146]
train() client id: f_00005-8-1 loss: 0.524336  [   64/  146]
train() client id: f_00005-8-2 loss: 0.344535  [   96/  146]
train() client id: f_00005-8-3 loss: 0.709128  [  128/  146]
train() client id: f_00005-9-0 loss: 0.748586  [   32/  146]
train() client id: f_00005-9-1 loss: 0.641144  [   64/  146]
train() client id: f_00005-9-2 loss: 0.257998  [   96/  146]
train() client id: f_00005-9-3 loss: 0.449273  [  128/  146]
train() client id: f_00005-10-0 loss: 0.450443  [   32/  146]
train() client id: f_00005-10-1 loss: 0.560647  [   64/  146]
train() client id: f_00005-10-2 loss: 0.720050  [   96/  146]
train() client id: f_00005-10-3 loss: 0.535412  [  128/  146]
train() client id: f_00005-11-0 loss: 0.549606  [   32/  146]
train() client id: f_00005-11-1 loss: 0.384729  [   64/  146]
train() client id: f_00005-11-2 loss: 0.684551  [   96/  146]
train() client id: f_00005-11-3 loss: 0.417233  [  128/  146]
train() client id: f_00005-12-0 loss: 0.393650  [   32/  146]
train() client id: f_00005-12-1 loss: 0.366358  [   64/  146]
train() client id: f_00005-12-2 loss: 0.776124  [   96/  146]
train() client id: f_00005-12-3 loss: 0.567743  [  128/  146]
train() client id: f_00005-13-0 loss: 0.475073  [   32/  146]
train() client id: f_00005-13-1 loss: 0.756881  [   64/  146]
train() client id: f_00005-13-2 loss: 0.586946  [   96/  146]
train() client id: f_00005-13-3 loss: 0.430773  [  128/  146]
train() client id: f_00005-14-0 loss: 0.536615  [   32/  146]
train() client id: f_00005-14-1 loss: 0.515258  [   64/  146]
train() client id: f_00005-14-2 loss: 0.744809  [   96/  146]
train() client id: f_00005-14-3 loss: 0.440375  [  128/  146]
train() client id: f_00005-15-0 loss: 0.686497  [   32/  146]
train() client id: f_00005-15-1 loss: 0.451700  [   64/  146]
train() client id: f_00005-15-2 loss: 0.493512  [   96/  146]
train() client id: f_00005-15-3 loss: 0.549491  [  128/  146]
train() client id: f_00005-16-0 loss: 0.526198  [   32/  146]
train() client id: f_00005-16-1 loss: 0.610764  [   64/  146]
train() client id: f_00005-16-2 loss: 0.441191  [   96/  146]
train() client id: f_00005-16-3 loss: 0.556397  [  128/  146]
train() client id: f_00006-0-0 loss: 0.542867  [   32/   54]
train() client id: f_00006-1-0 loss: 0.508855  [   32/   54]
train() client id: f_00006-2-0 loss: 0.430132  [   32/   54]
train() client id: f_00006-3-0 loss: 0.531684  [   32/   54]
train() client id: f_00006-4-0 loss: 0.533948  [   32/   54]
train() client id: f_00006-5-0 loss: 0.503460  [   32/   54]
train() client id: f_00006-6-0 loss: 0.522051  [   32/   54]
train() client id: f_00006-7-0 loss: 0.494309  [   32/   54]
train() client id: f_00006-8-0 loss: 0.561222  [   32/   54]
train() client id: f_00006-9-0 loss: 0.545155  [   32/   54]
train() client id: f_00006-10-0 loss: 0.522283  [   32/   54]
train() client id: f_00006-11-0 loss: 0.481662  [   32/   54]
train() client id: f_00006-12-0 loss: 0.474108  [   32/   54]
train() client id: f_00006-13-0 loss: 0.503560  [   32/   54]
train() client id: f_00006-14-0 loss: 0.484819  [   32/   54]
train() client id: f_00006-15-0 loss: 0.500687  [   32/   54]
train() client id: f_00006-16-0 loss: 0.481570  [   32/   54]
train() client id: f_00007-0-0 loss: 0.390379  [   32/  179]
train() client id: f_00007-0-1 loss: 0.577682  [   64/  179]
train() client id: f_00007-0-2 loss: 0.560675  [   96/  179]
train() client id: f_00007-0-3 loss: 0.514085  [  128/  179]
train() client id: f_00007-0-4 loss: 0.340654  [  160/  179]
train() client id: f_00007-1-0 loss: 0.315357  [   32/  179]
train() client id: f_00007-1-1 loss: 0.600461  [   64/  179]
train() client id: f_00007-1-2 loss: 0.622141  [   96/  179]
train() client id: f_00007-1-3 loss: 0.480864  [  128/  179]
train() client id: f_00007-1-4 loss: 0.367963  [  160/  179]
train() client id: f_00007-2-0 loss: 0.270916  [   32/  179]
train() client id: f_00007-2-1 loss: 0.367965  [   64/  179]
train() client id: f_00007-2-2 loss: 0.599948  [   96/  179]
train() client id: f_00007-2-3 loss: 0.464091  [  128/  179]
train() client id: f_00007-2-4 loss: 0.528359  [  160/  179]
train() client id: f_00007-3-0 loss: 0.539933  [   32/  179]
train() client id: f_00007-3-1 loss: 0.592402  [   64/  179]
train() client id: f_00007-3-2 loss: 0.420774  [   96/  179]
train() client id: f_00007-3-3 loss: 0.393400  [  128/  179]
train() client id: f_00007-3-4 loss: 0.263831  [  160/  179]
train() client id: f_00007-4-0 loss: 0.325033  [   32/  179]
train() client id: f_00007-4-1 loss: 0.517615  [   64/  179]
train() client id: f_00007-4-2 loss: 0.409269  [   96/  179]
train() client id: f_00007-4-3 loss: 0.404684  [  128/  179]
train() client id: f_00007-4-4 loss: 0.442744  [  160/  179]
train() client id: f_00007-5-0 loss: 0.436251  [   32/  179]
train() client id: f_00007-5-1 loss: 0.716433  [   64/  179]
train() client id: f_00007-5-2 loss: 0.282576  [   96/  179]
train() client id: f_00007-5-3 loss: 0.255979  [  128/  179]
train() client id: f_00007-5-4 loss: 0.308129  [  160/  179]
train() client id: f_00007-6-0 loss: 0.225519  [   32/  179]
train() client id: f_00007-6-1 loss: 0.331022  [   64/  179]
train() client id: f_00007-6-2 loss: 0.486627  [   96/  179]
train() client id: f_00007-6-3 loss: 0.451421  [  128/  179]
train() client id: f_00007-6-4 loss: 0.569951  [  160/  179]
train() client id: f_00007-7-0 loss: 0.480731  [   32/  179]
train() client id: f_00007-7-1 loss: 0.344441  [   64/  179]
train() client id: f_00007-7-2 loss: 0.260556  [   96/  179]
train() client id: f_00007-7-3 loss: 0.618394  [  128/  179]
train() client id: f_00007-7-4 loss: 0.365383  [  160/  179]
train() client id: f_00007-8-0 loss: 0.517280  [   32/  179]
train() client id: f_00007-8-1 loss: 0.261096  [   64/  179]
train() client id: f_00007-8-2 loss: 0.429281  [   96/  179]
train() client id: f_00007-8-3 loss: 0.340293  [  128/  179]
train() client id: f_00007-8-4 loss: 0.284785  [  160/  179]
train() client id: f_00007-9-0 loss: 0.356551  [   32/  179]
train() client id: f_00007-9-1 loss: 0.369152  [   64/  179]
train() client id: f_00007-9-2 loss: 0.772109  [   96/  179]
train() client id: f_00007-9-3 loss: 0.197582  [  128/  179]
train() client id: f_00007-9-4 loss: 0.279955  [  160/  179]
train() client id: f_00007-10-0 loss: 0.496517  [   32/  179]
train() client id: f_00007-10-1 loss: 0.385475  [   64/  179]
train() client id: f_00007-10-2 loss: 0.520080  [   96/  179]
train() client id: f_00007-10-3 loss: 0.214721  [  128/  179]
train() client id: f_00007-10-4 loss: 0.238836  [  160/  179]
train() client id: f_00007-11-0 loss: 0.538693  [   32/  179]
train() client id: f_00007-11-1 loss: 0.207793  [   64/  179]
train() client id: f_00007-11-2 loss: 0.223827  [   96/  179]
train() client id: f_00007-11-3 loss: 0.496380  [  128/  179]
train() client id: f_00007-11-4 loss: 0.418416  [  160/  179]
train() client id: f_00007-12-0 loss: 0.386385  [   32/  179]
train() client id: f_00007-12-1 loss: 0.315830  [   64/  179]
train() client id: f_00007-12-2 loss: 0.509477  [   96/  179]
train() client id: f_00007-12-3 loss: 0.198287  [  128/  179]
train() client id: f_00007-12-4 loss: 0.532694  [  160/  179]
train() client id: f_00007-13-0 loss: 0.413143  [   32/  179]
train() client id: f_00007-13-1 loss: 0.318800  [   64/  179]
train() client id: f_00007-13-2 loss: 0.217911  [   96/  179]
train() client id: f_00007-13-3 loss: 0.395338  [  128/  179]
train() client id: f_00007-13-4 loss: 0.485761  [  160/  179]
train() client id: f_00007-14-0 loss: 0.218908  [   32/  179]
train() client id: f_00007-14-1 loss: 0.302449  [   64/  179]
train() client id: f_00007-14-2 loss: 0.166592  [   96/  179]
train() client id: f_00007-14-3 loss: 0.483341  [  128/  179]
train() client id: f_00007-14-4 loss: 0.480444  [  160/  179]
train() client id: f_00007-15-0 loss: 0.346024  [   32/  179]
train() client id: f_00007-15-1 loss: 0.225034  [   64/  179]
train() client id: f_00007-15-2 loss: 0.425096  [   96/  179]
train() client id: f_00007-15-3 loss: 0.469703  [  128/  179]
train() client id: f_00007-15-4 loss: 0.331664  [  160/  179]
train() client id: f_00007-16-0 loss: 0.340869  [   32/  179]
train() client id: f_00007-16-1 loss: 0.326697  [   64/  179]
train() client id: f_00007-16-2 loss: 0.573603  [   96/  179]
train() client id: f_00007-16-3 loss: 0.284121  [  128/  179]
train() client id: f_00007-16-4 loss: 0.289347  [  160/  179]
train() client id: f_00008-0-0 loss: 0.775238  [   32/  130]
train() client id: f_00008-0-1 loss: 0.588447  [   64/  130]
train() client id: f_00008-0-2 loss: 0.622885  [   96/  130]
train() client id: f_00008-0-3 loss: 0.687621  [  128/  130]
train() client id: f_00008-1-0 loss: 0.707401  [   32/  130]
train() client id: f_00008-1-1 loss: 0.736537  [   64/  130]
train() client id: f_00008-1-2 loss: 0.640002  [   96/  130]
train() client id: f_00008-1-3 loss: 0.581546  [  128/  130]
train() client id: f_00008-2-0 loss: 0.686635  [   32/  130]
train() client id: f_00008-2-1 loss: 0.583183  [   64/  130]
train() client id: f_00008-2-2 loss: 0.654093  [   96/  130]
train() client id: f_00008-2-3 loss: 0.702709  [  128/  130]
train() client id: f_00008-3-0 loss: 0.621983  [   32/  130]
train() client id: f_00008-3-1 loss: 0.745628  [   64/  130]
train() client id: f_00008-3-2 loss: 0.713774  [   96/  130]
train() client id: f_00008-3-3 loss: 0.550198  [  128/  130]
train() client id: f_00008-4-0 loss: 0.698505  [   32/  130]
train() client id: f_00008-4-1 loss: 0.525011  [   64/  130]
train() client id: f_00008-4-2 loss: 0.687192  [   96/  130]
train() client id: f_00008-4-3 loss: 0.735973  [  128/  130]
train() client id: f_00008-5-0 loss: 0.656697  [   32/  130]
train() client id: f_00008-5-1 loss: 0.672285  [   64/  130]
train() client id: f_00008-5-2 loss: 0.729435  [   96/  130]
train() client id: f_00008-5-3 loss: 0.605049  [  128/  130]
train() client id: f_00008-6-0 loss: 0.662619  [   32/  130]
train() client id: f_00008-6-1 loss: 0.579079  [   64/  130]
train() client id: f_00008-6-2 loss: 0.710469  [   96/  130]
train() client id: f_00008-6-3 loss: 0.697731  [  128/  130]
train() client id: f_00008-7-0 loss: 0.562180  [   32/  130]
train() client id: f_00008-7-1 loss: 0.763056  [   64/  130]
train() client id: f_00008-7-2 loss: 0.643330  [   96/  130]
train() client id: f_00008-7-3 loss: 0.687484  [  128/  130]
train() client id: f_00008-8-0 loss: 0.656167  [   32/  130]
train() client id: f_00008-8-1 loss: 0.702453  [   64/  130]
train() client id: f_00008-8-2 loss: 0.742985  [   96/  130]
train() client id: f_00008-8-3 loss: 0.555872  [  128/  130]
train() client id: f_00008-9-0 loss: 0.601388  [   32/  130]
train() client id: f_00008-9-1 loss: 0.644256  [   64/  130]
train() client id: f_00008-9-2 loss: 0.737675  [   96/  130]
train() client id: f_00008-9-3 loss: 0.667705  [  128/  130]
train() client id: f_00008-10-0 loss: 0.686433  [   32/  130]
train() client id: f_00008-10-1 loss: 0.743528  [   64/  130]
train() client id: f_00008-10-2 loss: 0.645751  [   96/  130]
train() client id: f_00008-10-3 loss: 0.536969  [  128/  130]
train() client id: f_00008-11-0 loss: 0.649301  [   32/  130]
train() client id: f_00008-11-1 loss: 0.551544  [   64/  130]
train() client id: f_00008-11-2 loss: 0.666471  [   96/  130]
train() client id: f_00008-11-3 loss: 0.785170  [  128/  130]
train() client id: f_00008-12-0 loss: 0.654247  [   32/  130]
train() client id: f_00008-12-1 loss: 0.747294  [   64/  130]
train() client id: f_00008-12-2 loss: 0.693429  [   96/  130]
train() client id: f_00008-12-3 loss: 0.549879  [  128/  130]
train() client id: f_00008-13-0 loss: 0.562690  [   32/  130]
train() client id: f_00008-13-1 loss: 0.719553  [   64/  130]
train() client id: f_00008-13-2 loss: 0.663609  [   96/  130]
train() client id: f_00008-13-3 loss: 0.671518  [  128/  130]
train() client id: f_00008-14-0 loss: 0.767747  [   32/  130]
train() client id: f_00008-14-1 loss: 0.683672  [   64/  130]
train() client id: f_00008-14-2 loss: 0.612043  [   96/  130]
train() client id: f_00008-14-3 loss: 0.565249  [  128/  130]
train() client id: f_00008-15-0 loss: 0.589474  [   32/  130]
train() client id: f_00008-15-1 loss: 0.643498  [   64/  130]
train() client id: f_00008-15-2 loss: 0.744556  [   96/  130]
train() client id: f_00008-15-3 loss: 0.683649  [  128/  130]
train() client id: f_00008-16-0 loss: 0.655879  [   32/  130]
train() client id: f_00008-16-1 loss: 0.600928  [   64/  130]
train() client id: f_00008-16-2 loss: 0.723353  [   96/  130]
train() client id: f_00008-16-3 loss: 0.644816  [  128/  130]
train() client id: f_00009-0-0 loss: 1.097335  [   32/  118]
train() client id: f_00009-0-1 loss: 1.243530  [   64/  118]
train() client id: f_00009-0-2 loss: 0.901475  [   96/  118]
train() client id: f_00009-1-0 loss: 1.087740  [   32/  118]
train() client id: f_00009-1-1 loss: 0.978909  [   64/  118]
train() client id: f_00009-1-2 loss: 1.039739  [   96/  118]
train() client id: f_00009-2-0 loss: 0.921043  [   32/  118]
train() client id: f_00009-2-1 loss: 0.891438  [   64/  118]
train() client id: f_00009-2-2 loss: 1.039301  [   96/  118]
train() client id: f_00009-3-0 loss: 0.708215  [   32/  118]
train() client id: f_00009-3-1 loss: 1.007426  [   64/  118]
train() client id: f_00009-3-2 loss: 0.969523  [   96/  118]
train() client id: f_00009-4-0 loss: 0.947043  [   32/  118]
train() client id: f_00009-4-1 loss: 0.892206  [   64/  118]
train() client id: f_00009-4-2 loss: 0.886097  [   96/  118]
train() client id: f_00009-5-0 loss: 0.894613  [   32/  118]
train() client id: f_00009-5-1 loss: 0.851335  [   64/  118]
train() client id: f_00009-5-2 loss: 0.895682  [   96/  118]
train() client id: f_00009-6-0 loss: 0.830842  [   32/  118]
train() client id: f_00009-6-1 loss: 0.849317  [   64/  118]
train() client id: f_00009-6-2 loss: 0.694532  [   96/  118]
train() client id: f_00009-7-0 loss: 0.721857  [   32/  118]
train() client id: f_00009-7-1 loss: 0.835252  [   64/  118]
train() client id: f_00009-7-2 loss: 0.781115  [   96/  118]
train() client id: f_00009-8-0 loss: 0.737083  [   32/  118]
train() client id: f_00009-8-1 loss: 0.732120  [   64/  118]
train() client id: f_00009-8-2 loss: 0.745606  [   96/  118]
train() client id: f_00009-9-0 loss: 0.527697  [   32/  118]
train() client id: f_00009-9-1 loss: 0.672926  [   64/  118]
train() client id: f_00009-9-2 loss: 0.925278  [   96/  118]
train() client id: f_00009-10-0 loss: 0.730626  [   32/  118]
train() client id: f_00009-10-1 loss: 0.806883  [   64/  118]
train() client id: f_00009-10-2 loss: 0.645091  [   96/  118]
train() client id: f_00009-11-0 loss: 0.721172  [   32/  118]
train() client id: f_00009-11-1 loss: 0.806391  [   64/  118]
train() client id: f_00009-11-2 loss: 0.628312  [   96/  118]
train() client id: f_00009-12-0 loss: 0.677422  [   32/  118]
train() client id: f_00009-12-1 loss: 0.565973  [   64/  118]
train() client id: f_00009-12-2 loss: 0.845331  [   96/  118]
train() client id: f_00009-13-0 loss: 0.509663  [   32/  118]
train() client id: f_00009-13-1 loss: 0.706411  [   64/  118]
train() client id: f_00009-13-2 loss: 0.733959  [   96/  118]
train() client id: f_00009-14-0 loss: 0.656796  [   32/  118]
train() client id: f_00009-14-1 loss: 0.748364  [   64/  118]
train() client id: f_00009-14-2 loss: 0.709047  [   96/  118]
train() client id: f_00009-15-0 loss: 0.638999  [   32/  118]
train() client id: f_00009-15-1 loss: 0.773083  [   64/  118]
train() client id: f_00009-15-2 loss: 0.722896  [   96/  118]
train() client id: f_00009-16-0 loss: 0.617337  [   32/  118]
train() client id: f_00009-16-1 loss: 0.615948  [   64/  118]
train() client id: f_00009-16-2 loss: 0.671246  [   96/  118]
At round 57 accuracy: 0.6472148541114059
At round 57 training accuracy: 0.5969148222669349
At round 57 training loss: 0.8129331277654644
update_location
xs = 8.927491 406.223621 5.882650 0.934260 -322.581990 -170.230757 -130.849135 -5.143845 -345.120581 20.134486 
ys = -397.390647 7.291448 295.684448 -117.290817 -9.642386 0.794442 -1.381692 291.628436 25.881276 -832.232496 
xs mean: -53.18237997052123
ys mean: -73.66579882624052
dists_uav = 409.876843 418.414621 312.192086 154.136331 337.864049 197.431360 164.691849 308.340078 360.247215 838.460688 
uav_gains = -121.755150 -122.042745 -116.706121 -104.704132 -118.470506 -107.520563 -105.432069 -116.407006 -119.707398 -130.057625 
uav_gains_db_mean: -116.28033139563152
dists_bs = 601.215747 604.935580 217.447164 341.155076 236.375241 174.270830 181.823542 206.042000 226.224238 1025.960413 
bs_gains = -117.379909 -117.454915 -105.012964 -110.489711 -106.027915 -102.321351 -102.837262 -104.357820 -105.494155 -123.878716 
bs_gains_db_mean: -109.52547172811524
Round 58
-------------------------------
ene_coms = [0.03547436 0.03701632 0.007951   0.00782548 0.00839251 0.00913496
 0.00812773 0.00768924 0.00815456 0.04940893]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.4896052  7.0235126  3.20777712 1.15494246 7.8216586  3.77499482
 1.43611787 4.59876288 3.36147054 3.40399578]
obj_prev = 39.27283787826361
eta_min = 3.400919899499592e-28	eta_max = 0.8471115574751592
af = 8.119195852514759	bf = 1.4898308252818566	zeta = 8.931115437766236	eta = 0.909090909090909
af = 8.119195852514759	bf = 1.4898308252818566	zeta = 24.900401260367207	eta = 0.3260668680643994
af = 8.119195852514759	bf = 1.4898308252818566	zeta = 15.711692903508919	eta = 0.5167613638057733
af = 8.119195852514759	bf = 1.4898308252818566	zeta = 14.17501218576751	eta = 0.5727822837899834
af = 8.119195852514759	bf = 1.4898308252818566	zeta = 14.07771536851154	eta = 0.5767410151419488
af = 8.119195852514759	bf = 1.4898308252818566	zeta = 14.077261405727016	eta = 0.5767596138558347
af = 8.119195852514759	bf = 1.4898308252818566	zeta = 14.077261395752739	eta = 0.576759614264491
eta = 0.576759614264491
ene_coms = [0.03547436 0.03701632 0.007951   0.00782548 0.00839251 0.00913496
 0.00812773 0.00768924 0.00815456 0.04940893]
ene_comp = [0.04541189 0.09550913 0.04469107 0.01549771 0.11028602 0.05262013
 0.01946224 0.06451372 0.04685354 0.04252859]
ene_total = [1.58908414 2.60358353 1.03420143 0.45820534 2.33154789 1.21323504
 0.54203009 1.41849303 1.08068426 1.80619665]
ti_comp = [1.02191563 1.00649597 1.2971492  1.29840439 1.29273415 1.2853096
 1.29538191 1.29976677 1.2951136  0.88256988]
ti_coms = [0.35474357 0.37016323 0.07951    0.07825481 0.08392506 0.09134961
 0.08127729 0.07689243 0.0815456  0.49408933]
t_total = [27.04572678 27.04572678 27.04572678 27.04572678 27.04572678 27.04572678
 27.04572678 27.04572678 27.04572678 27.04572678]
ene_coms = [0.03547436 0.03701632 0.007951   0.00782548 0.00839251 0.00913496
 0.00812773 0.00768924 0.00815456 0.04940893]
ene_comp = [5.60478076e-06 5.37515035e-05 3.31560104e-06 1.37994727e-07
 5.01675529e-05 5.51213881e-06 2.74576080e-07 9.93356896e-06
 3.83258156e-06 6.17198554e-06]
ene_total = [0.69703624 0.72827546 0.15626979 0.15374143 0.16586403 0.17957294
 0.15968205 0.15125736 0.16027907 0.97080484]
optimize_network iter = 0 obj = 3.5227832154462835
eta = 0.576759614264491
freqs = [22219000.94225229 47446355.29204865 17226647.88217032  5967981.99131234
 42656108.35563575 20469825.02745852  7512162.18329574 24817419.70333873
 18088581.53914332 24093613.0778753 ]
eta_min = 0.5767596142644921	eta_max = 0.5767596142644905
af = 0.0020956558592831933	bf = 1.4898308252818566	zeta = 0.0023052214452115127	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03547436 0.03701632 0.007951   0.00782548 0.00839251 0.00913496
 0.00812773 0.00768924 0.00815456 0.04940893]
ene_comp = [9.70677487e-07 9.30908390e-06 5.74220371e-07 2.38989500e-08
 8.68838877e-06 9.54633067e-07 4.75531212e-08 1.72036912e-06
 6.63754889e-07 1.06891022e-06]
ene_total = [3.24472892 3.38652653 0.72728583 0.71575498 0.76841004 0.83561086
 0.74340207 0.70344923 0.74591251 4.51925523]
ti_comp = [1.02191563 1.00649597 1.2971492  1.29840439 1.29273415 1.2853096
 1.29538191 1.29976677 1.2951136  0.88256988]
ti_coms = [0.35474357 0.37016323 0.07951    0.07825481 0.08392506 0.09134961
 0.08127729 0.07689243 0.0815456  0.49408933]
t_total = [27.04572678 27.04572678 27.04572678 27.04572678 27.04572678 27.04572678
 27.04572678 27.04572678 27.04572678 27.04572678]
ene_coms = [0.03547436 0.03701632 0.007951   0.00782548 0.00839251 0.00913496
 0.00812773 0.00768924 0.00815456 0.04940893]
ene_comp = [5.60478076e-06 5.37515035e-05 3.31560104e-06 1.37994727e-07
 5.01675529e-05 5.51213881e-06 2.74576080e-07 9.93356896e-06
 3.83258156e-06 6.17198554e-06]
ene_total = [0.69703624 0.72827546 0.15626979 0.15374143 0.16586403 0.17957294
 0.15968205 0.15125736 0.16027907 0.97080484]
optimize_network iter = 1 obj = 3.52278321544628
eta = 0.5767596142644905
freqs = [22219000.94225229 47446355.29204865 17226647.88217033  5967981.99131234
 42656108.35563577 20469825.02745853  7512162.18329574 24817419.70333874
 18088581.53914332 24093613.07787529]
Done!
ene_coms = [0.03547436 0.03701632 0.007951   0.00782548 0.00839251 0.00913496
 0.00812773 0.00768924 0.00815456 0.04940893]
ene_comp = [5.59837659e-06 5.36900856e-05 3.31181255e-06 1.37837051e-07
 5.01102302e-05 5.50584050e-06 2.74262342e-07 9.92221861e-06
 3.82820235e-06 6.16493327e-06]
ene_total = [0.03547996 0.03707001 0.00795431 0.00782562 0.00844262 0.00914047
 0.008128   0.00769917 0.00815839 0.0494151 ]
At round 58 energy consumption: 0.17931363785802312
At round 58 eta: 0.5767596142644905
At round 58 a_n: 8.314943732033974
At round 58 local rounds: 18.020590792294694
At round 58 global rounds: 19.645912848284123
gradient difference: 0.3287805914878845
train() client id: f_00000-0-0 loss: 1.105577  [   32/  126]
train() client id: f_00000-0-1 loss: 1.376892  [   64/  126]
train() client id: f_00000-0-2 loss: 1.241164  [   96/  126]
train() client id: f_00000-1-0 loss: 1.315666  [   32/  126]
train() client id: f_00000-1-1 loss: 1.158379  [   64/  126]
train() client id: f_00000-1-2 loss: 1.202958  [   96/  126]
train() client id: f_00000-2-0 loss: 1.188585  [   32/  126]
train() client id: f_00000-2-1 loss: 1.078878  [   64/  126]
train() client id: f_00000-2-2 loss: 1.199077  [   96/  126]
train() client id: f_00000-3-0 loss: 1.016854  [   32/  126]
train() client id: f_00000-3-1 loss: 0.830936  [   64/  126]
train() client id: f_00000-3-2 loss: 1.137480  [   96/  126]
train() client id: f_00000-4-0 loss: 0.856933  [   32/  126]
train() client id: f_00000-4-1 loss: 0.938446  [   64/  126]
train() client id: f_00000-4-2 loss: 0.986538  [   96/  126]
train() client id: f_00000-5-0 loss: 0.847362  [   32/  126]
train() client id: f_00000-5-1 loss: 0.933960  [   64/  126]
train() client id: f_00000-5-2 loss: 0.887242  [   96/  126]
train() client id: f_00000-6-0 loss: 0.904984  [   32/  126]
train() client id: f_00000-6-1 loss: 0.714226  [   64/  126]
train() client id: f_00000-6-2 loss: 0.903836  [   96/  126]
train() client id: f_00000-7-0 loss: 0.919835  [   32/  126]
train() client id: f_00000-7-1 loss: 0.887662  [   64/  126]
train() client id: f_00000-7-2 loss: 0.785620  [   96/  126]
train() client id: f_00000-8-0 loss: 0.936677  [   32/  126]
train() client id: f_00000-8-1 loss: 0.790018  [   64/  126]
train() client id: f_00000-8-2 loss: 0.779803  [   96/  126]
train() client id: f_00000-9-0 loss: 0.792395  [   32/  126]
train() client id: f_00000-9-1 loss: 0.761084  [   64/  126]
train() client id: f_00000-9-2 loss: 0.831645  [   96/  126]
train() client id: f_00000-10-0 loss: 0.757794  [   32/  126]
train() client id: f_00000-10-1 loss: 0.921261  [   64/  126]
train() client id: f_00000-10-2 loss: 0.752193  [   96/  126]
train() client id: f_00000-11-0 loss: 0.736016  [   32/  126]
train() client id: f_00000-11-1 loss: 0.765779  [   64/  126]
train() client id: f_00000-11-2 loss: 0.897643  [   96/  126]
train() client id: f_00000-12-0 loss: 0.927357  [   32/  126]
train() client id: f_00000-12-1 loss: 0.738287  [   64/  126]
train() client id: f_00000-12-2 loss: 0.758102  [   96/  126]
train() client id: f_00000-13-0 loss: 0.733789  [   32/  126]
train() client id: f_00000-13-1 loss: 0.736324  [   64/  126]
train() client id: f_00000-13-2 loss: 0.838742  [   96/  126]
train() client id: f_00000-14-0 loss: 0.679712  [   32/  126]
train() client id: f_00000-14-1 loss: 0.728148  [   64/  126]
train() client id: f_00000-14-2 loss: 0.833394  [   96/  126]
train() client id: f_00000-15-0 loss: 0.703814  [   32/  126]
train() client id: f_00000-15-1 loss: 0.674905  [   64/  126]
train() client id: f_00000-15-2 loss: 0.827413  [   96/  126]
train() client id: f_00000-16-0 loss: 0.795844  [   32/  126]
train() client id: f_00000-16-1 loss: 0.852198  [   64/  126]
train() client id: f_00000-16-2 loss: 0.616015  [   96/  126]
train() client id: f_00000-17-0 loss: 0.810383  [   32/  126]
train() client id: f_00000-17-1 loss: 0.771344  [   64/  126]
train() client id: f_00000-17-2 loss: 0.761341  [   96/  126]
train() client id: f_00001-0-0 loss: 0.471716  [   32/  265]
train() client id: f_00001-0-1 loss: 0.405430  [   64/  265]
train() client id: f_00001-0-2 loss: 0.543069  [   96/  265]
train() client id: f_00001-0-3 loss: 0.461764  [  128/  265]
train() client id: f_00001-0-4 loss: 0.412495  [  160/  265]
train() client id: f_00001-0-5 loss: 0.473531  [  192/  265]
train() client id: f_00001-0-6 loss: 0.467280  [  224/  265]
train() client id: f_00001-0-7 loss: 0.625640  [  256/  265]
train() client id: f_00001-1-0 loss: 0.482819  [   32/  265]
train() client id: f_00001-1-1 loss: 0.412379  [   64/  265]
train() client id: f_00001-1-2 loss: 0.468948  [   96/  265]
train() client id: f_00001-1-3 loss: 0.406675  [  128/  265]
train() client id: f_00001-1-4 loss: 0.528650  [  160/  265]
train() client id: f_00001-1-5 loss: 0.545055  [  192/  265]
train() client id: f_00001-1-6 loss: 0.482204  [  224/  265]
train() client id: f_00001-1-7 loss: 0.470366  [  256/  265]
train() client id: f_00001-2-0 loss: 0.390196  [   32/  265]
train() client id: f_00001-2-1 loss: 0.450178  [   64/  265]
train() client id: f_00001-2-2 loss: 0.552477  [   96/  265]
train() client id: f_00001-2-3 loss: 0.448242  [  128/  265]
train() client id: f_00001-2-4 loss: 0.556551  [  160/  265]
train() client id: f_00001-2-5 loss: 0.462504  [  192/  265]
train() client id: f_00001-2-6 loss: 0.469632  [  224/  265]
train() client id: f_00001-2-7 loss: 0.413037  [  256/  265]
train() client id: f_00001-3-0 loss: 0.607630  [   32/  265]
train() client id: f_00001-3-1 loss: 0.453080  [   64/  265]
train() client id: f_00001-3-2 loss: 0.463735  [   96/  265]
train() client id: f_00001-3-3 loss: 0.443823  [  128/  265]
train() client id: f_00001-3-4 loss: 0.358296  [  160/  265]
train() client id: f_00001-3-5 loss: 0.487576  [  192/  265]
train() client id: f_00001-3-6 loss: 0.506012  [  224/  265]
train() client id: f_00001-3-7 loss: 0.375019  [  256/  265]
train() client id: f_00001-4-0 loss: 0.552708  [   32/  265]
train() client id: f_00001-4-1 loss: 0.484687  [   64/  265]
train() client id: f_00001-4-2 loss: 0.385331  [   96/  265]
train() client id: f_00001-4-3 loss: 0.525702  [  128/  265]
train() client id: f_00001-4-4 loss: 0.434289  [  160/  265]
train() client id: f_00001-4-5 loss: 0.389317  [  192/  265]
train() client id: f_00001-4-6 loss: 0.452157  [  224/  265]
train() client id: f_00001-4-7 loss: 0.438745  [  256/  265]
train() client id: f_00001-5-0 loss: 0.522590  [   32/  265]
train() client id: f_00001-5-1 loss: 0.465643  [   64/  265]
train() client id: f_00001-5-2 loss: 0.520890  [   96/  265]
train() client id: f_00001-5-3 loss: 0.422796  [  128/  265]
train() client id: f_00001-5-4 loss: 0.380081  [  160/  265]
train() client id: f_00001-5-5 loss: 0.447190  [  192/  265]
train() client id: f_00001-5-6 loss: 0.398399  [  224/  265]
train() client id: f_00001-5-7 loss: 0.476095  [  256/  265]
train() client id: f_00001-6-0 loss: 0.396797  [   32/  265]
train() client id: f_00001-6-1 loss: 0.447099  [   64/  265]
train() client id: f_00001-6-2 loss: 0.482307  [   96/  265]
train() client id: f_00001-6-3 loss: 0.346660  [  128/  265]
train() client id: f_00001-6-4 loss: 0.447196  [  160/  265]
train() client id: f_00001-6-5 loss: 0.421655  [  192/  265]
train() client id: f_00001-6-6 loss: 0.480835  [  224/  265]
train() client id: f_00001-6-7 loss: 0.618161  [  256/  265]
train() client id: f_00001-7-0 loss: 0.448549  [   32/  265]
train() client id: f_00001-7-1 loss: 0.341788  [   64/  265]
train() client id: f_00001-7-2 loss: 0.476852  [   96/  265]
train() client id: f_00001-7-3 loss: 0.429430  [  128/  265]
train() client id: f_00001-7-4 loss: 0.486097  [  160/  265]
train() client id: f_00001-7-5 loss: 0.470408  [  192/  265]
train() client id: f_00001-7-6 loss: 0.515183  [  224/  265]
train() client id: f_00001-7-7 loss: 0.449950  [  256/  265]
train() client id: f_00001-8-0 loss: 0.521100  [   32/  265]
train() client id: f_00001-8-1 loss: 0.456796  [   64/  265]
train() client id: f_00001-8-2 loss: 0.392264  [   96/  265]
train() client id: f_00001-8-3 loss: 0.356906  [  128/  265]
train() client id: f_00001-8-4 loss: 0.524519  [  160/  265]
train() client id: f_00001-8-5 loss: 0.331010  [  192/  265]
train() client id: f_00001-8-6 loss: 0.387932  [  224/  265]
train() client id: f_00001-8-7 loss: 0.516240  [  256/  265]
train() client id: f_00001-9-0 loss: 0.347709  [   32/  265]
train() client id: f_00001-9-1 loss: 0.360393  [   64/  265]
train() client id: f_00001-9-2 loss: 0.671499  [   96/  265]
train() client id: f_00001-9-3 loss: 0.380485  [  128/  265]
train() client id: f_00001-9-4 loss: 0.445235  [  160/  265]
train() client id: f_00001-9-5 loss: 0.484065  [  192/  265]
train() client id: f_00001-9-6 loss: 0.436413  [  224/  265]
train() client id: f_00001-9-7 loss: 0.428444  [  256/  265]
train() client id: f_00001-10-0 loss: 0.359585  [   32/  265]
train() client id: f_00001-10-1 loss: 0.360418  [   64/  265]
train() client id: f_00001-10-2 loss: 0.541662  [   96/  265]
train() client id: f_00001-10-3 loss: 0.534371  [  128/  265]
train() client id: f_00001-10-4 loss: 0.450420  [  160/  265]
train() client id: f_00001-10-5 loss: 0.394857  [  192/  265]
train() client id: f_00001-10-6 loss: 0.529036  [  224/  265]
train() client id: f_00001-10-7 loss: 0.347031  [  256/  265]
train() client id: f_00001-11-0 loss: 0.443073  [   32/  265]
train() client id: f_00001-11-1 loss: 0.431956  [   64/  265]
train() client id: f_00001-11-2 loss: 0.453877  [   96/  265]
train() client id: f_00001-11-3 loss: 0.614612  [  128/  265]
train() client id: f_00001-11-4 loss: 0.488947  [  160/  265]
train() client id: f_00001-11-5 loss: 0.396148  [  192/  265]
train() client id: f_00001-11-6 loss: 0.384100  [  224/  265]
train() client id: f_00001-11-7 loss: 0.386268  [  256/  265]
train() client id: f_00001-12-0 loss: 0.464573  [   32/  265]
train() client id: f_00001-12-1 loss: 0.420308  [   64/  265]
train() client id: f_00001-12-2 loss: 0.414528  [   96/  265]
train() client id: f_00001-12-3 loss: 0.348796  [  128/  265]
train() client id: f_00001-12-4 loss: 0.360394  [  160/  265]
train() client id: f_00001-12-5 loss: 0.520534  [  192/  265]
train() client id: f_00001-12-6 loss: 0.542933  [  224/  265]
train() client id: f_00001-12-7 loss: 0.427628  [  256/  265]
train() client id: f_00001-13-0 loss: 0.473174  [   32/  265]
train() client id: f_00001-13-1 loss: 0.396752  [   64/  265]
train() client id: f_00001-13-2 loss: 0.487965  [   96/  265]
train() client id: f_00001-13-3 loss: 0.451949  [  128/  265]
train() client id: f_00001-13-4 loss: 0.342931  [  160/  265]
train() client id: f_00001-13-5 loss: 0.449034  [  192/  265]
train() client id: f_00001-13-6 loss: 0.481419  [  224/  265]
train() client id: f_00001-13-7 loss: 0.513083  [  256/  265]
train() client id: f_00001-14-0 loss: 0.410641  [   32/  265]
train() client id: f_00001-14-1 loss: 0.416671  [   64/  265]
train() client id: f_00001-14-2 loss: 0.492040  [   96/  265]
train() client id: f_00001-14-3 loss: 0.448776  [  128/  265]
train() client id: f_00001-14-4 loss: 0.617905  [  160/  265]
train() client id: f_00001-14-5 loss: 0.461259  [  192/  265]
train() client id: f_00001-14-6 loss: 0.377444  [  224/  265]
train() client id: f_00001-14-7 loss: 0.376896  [  256/  265]
train() client id: f_00001-15-0 loss: 0.490863  [   32/  265]
train() client id: f_00001-15-1 loss: 0.436952  [   64/  265]
train() client id: f_00001-15-2 loss: 0.465279  [   96/  265]
train() client id: f_00001-15-3 loss: 0.492404  [  128/  265]
train() client id: f_00001-15-4 loss: 0.395744  [  160/  265]
train() client id: f_00001-15-5 loss: 0.448176  [  192/  265]
train() client id: f_00001-15-6 loss: 0.500218  [  224/  265]
train() client id: f_00001-15-7 loss: 0.368892  [  256/  265]
train() client id: f_00001-16-0 loss: 0.450320  [   32/  265]
train() client id: f_00001-16-1 loss: 0.415388  [   64/  265]
train() client id: f_00001-16-2 loss: 0.389013  [   96/  265]
train() client id: f_00001-16-3 loss: 0.341079  [  128/  265]
train() client id: f_00001-16-4 loss: 0.491765  [  160/  265]
train() client id: f_00001-16-5 loss: 0.344603  [  192/  265]
train() client id: f_00001-16-6 loss: 0.458855  [  224/  265]
train() client id: f_00001-16-7 loss: 0.636892  [  256/  265]
train() client id: f_00001-17-0 loss: 0.396043  [   32/  265]
train() client id: f_00001-17-1 loss: 0.500037  [   64/  265]
train() client id: f_00001-17-2 loss: 0.664575  [   96/  265]
train() client id: f_00001-17-3 loss: 0.450668  [  128/  265]
train() client id: f_00001-17-4 loss: 0.353390  [  160/  265]
train() client id: f_00001-17-5 loss: 0.509241  [  192/  265]
train() client id: f_00001-17-6 loss: 0.349864  [  224/  265]
train() client id: f_00001-17-7 loss: 0.389857  [  256/  265]
train() client id: f_00002-0-0 loss: 1.551448  [   32/  124]
train() client id: f_00002-0-1 loss: 1.483039  [   64/  124]
train() client id: f_00002-0-2 loss: 1.393716  [   96/  124]
train() client id: f_00002-1-0 loss: 1.386551  [   32/  124]
train() client id: f_00002-1-1 loss: 1.488679  [   64/  124]
train() client id: f_00002-1-2 loss: 1.179523  [   96/  124]
train() client id: f_00002-2-0 loss: 1.273170  [   32/  124]
train() client id: f_00002-2-1 loss: 1.411085  [   64/  124]
train() client id: f_00002-2-2 loss: 1.254816  [   96/  124]
train() client id: f_00002-3-0 loss: 1.392147  [   32/  124]
train() client id: f_00002-3-1 loss: 1.187812  [   64/  124]
train() client id: f_00002-3-2 loss: 1.278031  [   96/  124]
train() client id: f_00002-4-0 loss: 1.105713  [   32/  124]
train() client id: f_00002-4-1 loss: 1.393250  [   64/  124]
train() client id: f_00002-4-2 loss: 1.243988  [   96/  124]
train() client id: f_00002-5-0 loss: 1.198925  [   32/  124]
train() client id: f_00002-5-1 loss: 1.160202  [   64/  124]
train() client id: f_00002-5-2 loss: 1.136208  [   96/  124]
train() client id: f_00002-6-0 loss: 1.210635  [   32/  124]
train() client id: f_00002-6-1 loss: 1.160711  [   64/  124]
train() client id: f_00002-6-2 loss: 1.112119  [   96/  124]
train() client id: f_00002-7-0 loss: 1.228806  [   32/  124]
train() client id: f_00002-7-1 loss: 1.321472  [   64/  124]
train() client id: f_00002-7-2 loss: 1.014940  [   96/  124]
train() client id: f_00002-8-0 loss: 1.336181  [   32/  124]
train() client id: f_00002-8-1 loss: 1.073354  [   64/  124]
train() client id: f_00002-8-2 loss: 1.166201  [   96/  124]
train() client id: f_00002-9-0 loss: 1.090410  [   32/  124]
train() client id: f_00002-9-1 loss: 1.132686  [   64/  124]
train() client id: f_00002-9-2 loss: 1.221341  [   96/  124]
train() client id: f_00002-10-0 loss: 0.933022  [   32/  124]
train() client id: f_00002-10-1 loss: 1.073116  [   64/  124]
train() client id: f_00002-10-2 loss: 1.263268  [   96/  124]
train() client id: f_00002-11-0 loss: 1.182280  [   32/  124]
train() client id: f_00002-11-1 loss: 0.976408  [   64/  124]
train() client id: f_00002-11-2 loss: 1.334880  [   96/  124]
train() client id: f_00002-12-0 loss: 1.226919  [   32/  124]
train() client id: f_00002-12-1 loss: 0.997320  [   64/  124]
train() client id: f_00002-12-2 loss: 0.972657  [   96/  124]
train() client id: f_00002-13-0 loss: 0.978789  [   32/  124]
train() client id: f_00002-13-1 loss: 1.030580  [   64/  124]
train() client id: f_00002-13-2 loss: 1.113588  [   96/  124]
train() client id: f_00002-14-0 loss: 1.182235  [   32/  124]
train() client id: f_00002-14-1 loss: 1.046317  [   64/  124]
train() client id: f_00002-14-2 loss: 1.019682  [   96/  124]
train() client id: f_00002-15-0 loss: 1.166632  [   32/  124]
train() client id: f_00002-15-1 loss: 1.114904  [   64/  124]
train() client id: f_00002-15-2 loss: 1.087777  [   96/  124]
train() client id: f_00002-16-0 loss: 1.051909  [   32/  124]
train() client id: f_00002-16-1 loss: 0.984163  [   64/  124]
train() client id: f_00002-16-2 loss: 1.104733  [   96/  124]
train() client id: f_00002-17-0 loss: 1.158351  [   32/  124]
train() client id: f_00002-17-1 loss: 1.205525  [   64/  124]
train() client id: f_00002-17-2 loss: 1.018394  [   96/  124]
train() client id: f_00003-0-0 loss: 0.899667  [   32/   43]
train() client id: f_00003-1-0 loss: 0.527868  [   32/   43]
train() client id: f_00003-2-0 loss: 0.811997  [   32/   43]
train() client id: f_00003-3-0 loss: 0.589278  [   32/   43]
train() client id: f_00003-4-0 loss: 0.598869  [   32/   43]
train() client id: f_00003-5-0 loss: 0.792406  [   32/   43]
train() client id: f_00003-6-0 loss: 0.709114  [   32/   43]
train() client id: f_00003-7-0 loss: 0.587818  [   32/   43]
train() client id: f_00003-8-0 loss: 0.694573  [   32/   43]
train() client id: f_00003-9-0 loss: 0.671620  [   32/   43]
train() client id: f_00003-10-0 loss: 0.627345  [   32/   43]
train() client id: f_00003-11-0 loss: 0.593837  [   32/   43]
train() client id: f_00003-12-0 loss: 0.655265  [   32/   43]
train() client id: f_00003-13-0 loss: 0.574882  [   32/   43]
train() client id: f_00003-14-0 loss: 0.545998  [   32/   43]
train() client id: f_00003-15-0 loss: 0.555503  [   32/   43]
train() client id: f_00003-16-0 loss: 0.674190  [   32/   43]
train() client id: f_00003-17-0 loss: 0.768490  [   32/   43]
train() client id: f_00004-0-0 loss: 0.836834  [   32/  306]
train() client id: f_00004-0-1 loss: 1.073266  [   64/  306]
train() client id: f_00004-0-2 loss: 0.948595  [   96/  306]
train() client id: f_00004-0-3 loss: 0.785412  [  128/  306]
train() client id: f_00004-0-4 loss: 0.972320  [  160/  306]
train() client id: f_00004-0-5 loss: 0.905207  [  192/  306]
train() client id: f_00004-0-6 loss: 1.071887  [  224/  306]
train() client id: f_00004-0-7 loss: 0.782172  [  256/  306]
train() client id: f_00004-0-8 loss: 0.746584  [  288/  306]
train() client id: f_00004-1-0 loss: 0.841212  [   32/  306]
train() client id: f_00004-1-1 loss: 0.972422  [   64/  306]
train() client id: f_00004-1-2 loss: 1.045827  [   96/  306]
train() client id: f_00004-1-3 loss: 0.956529  [  128/  306]
train() client id: f_00004-1-4 loss: 0.944850  [  160/  306]
train() client id: f_00004-1-5 loss: 0.926418  [  192/  306]
train() client id: f_00004-1-6 loss: 0.749813  [  224/  306]
train() client id: f_00004-1-7 loss: 0.863344  [  256/  306]
train() client id: f_00004-1-8 loss: 0.800630  [  288/  306]
train() client id: f_00004-2-0 loss: 0.827284  [   32/  306]
train() client id: f_00004-2-1 loss: 1.006661  [   64/  306]
train() client id: f_00004-2-2 loss: 0.767637  [   96/  306]
train() client id: f_00004-2-3 loss: 0.955507  [  128/  306]
train() client id: f_00004-2-4 loss: 0.800888  [  160/  306]
train() client id: f_00004-2-5 loss: 0.911492  [  192/  306]
train() client id: f_00004-2-6 loss: 0.689165  [  224/  306]
train() client id: f_00004-2-7 loss: 1.005001  [  256/  306]
train() client id: f_00004-2-8 loss: 0.935170  [  288/  306]
train() client id: f_00004-3-0 loss: 0.836111  [   32/  306]
train() client id: f_00004-3-1 loss: 0.877504  [   64/  306]
train() client id: f_00004-3-2 loss: 0.863619  [   96/  306]
train() client id: f_00004-3-3 loss: 1.002167  [  128/  306]
train() client id: f_00004-3-4 loss: 0.941659  [  160/  306]
train() client id: f_00004-3-5 loss: 0.783752  [  192/  306]
train() client id: f_00004-3-6 loss: 0.805617  [  224/  306]
train() client id: f_00004-3-7 loss: 0.882496  [  256/  306]
train() client id: f_00004-3-8 loss: 0.985268  [  288/  306]
train() client id: f_00004-4-0 loss: 0.919403  [   32/  306]
train() client id: f_00004-4-1 loss: 0.818100  [   64/  306]
train() client id: f_00004-4-2 loss: 0.887421  [   96/  306]
train() client id: f_00004-4-3 loss: 0.777689  [  128/  306]
train() client id: f_00004-4-4 loss: 0.942911  [  160/  306]
train() client id: f_00004-4-5 loss: 0.907071  [  192/  306]
train() client id: f_00004-4-6 loss: 0.921031  [  224/  306]
train() client id: f_00004-4-7 loss: 0.918758  [  256/  306]
train() client id: f_00004-4-8 loss: 0.828188  [  288/  306]
train() client id: f_00004-5-0 loss: 0.898920  [   32/  306]
train() client id: f_00004-5-1 loss: 0.994206  [   64/  306]
train() client id: f_00004-5-2 loss: 0.930075  [   96/  306]
train() client id: f_00004-5-3 loss: 0.842223  [  128/  306]
train() client id: f_00004-5-4 loss: 0.804761  [  160/  306]
train() client id: f_00004-5-5 loss: 1.019130  [  192/  306]
train() client id: f_00004-5-6 loss: 0.892547  [  224/  306]
train() client id: f_00004-5-7 loss: 0.759631  [  256/  306]
train() client id: f_00004-5-8 loss: 0.799685  [  288/  306]
train() client id: f_00004-6-0 loss: 0.863456  [   32/  306]
train() client id: f_00004-6-1 loss: 0.955884  [   64/  306]
train() client id: f_00004-6-2 loss: 0.896041  [   96/  306]
train() client id: f_00004-6-3 loss: 0.911804  [  128/  306]
train() client id: f_00004-6-4 loss: 0.749654  [  160/  306]
train() client id: f_00004-6-5 loss: 0.830129  [  192/  306]
train() client id: f_00004-6-6 loss: 0.834410  [  224/  306]
train() client id: f_00004-6-7 loss: 0.932715  [  256/  306]
train() client id: f_00004-6-8 loss: 0.949419  [  288/  306]
train() client id: f_00004-7-0 loss: 0.920628  [   32/  306]
train() client id: f_00004-7-1 loss: 0.900483  [   64/  306]
train() client id: f_00004-7-2 loss: 0.912850  [   96/  306]
train() client id: f_00004-7-3 loss: 0.960917  [  128/  306]
train() client id: f_00004-7-4 loss: 0.761625  [  160/  306]
train() client id: f_00004-7-5 loss: 0.821376  [  192/  306]
train() client id: f_00004-7-6 loss: 0.874532  [  224/  306]
train() client id: f_00004-7-7 loss: 0.868623  [  256/  306]
train() client id: f_00004-7-8 loss: 0.870993  [  288/  306]
train() client id: f_00004-8-0 loss: 0.808168  [   32/  306]
train() client id: f_00004-8-1 loss: 0.926614  [   64/  306]
train() client id: f_00004-8-2 loss: 0.918636  [   96/  306]
train() client id: f_00004-8-3 loss: 0.833443  [  128/  306]
train() client id: f_00004-8-4 loss: 0.862223  [  160/  306]
train() client id: f_00004-8-5 loss: 0.766897  [  192/  306]
train() client id: f_00004-8-6 loss: 0.838991  [  224/  306]
train() client id: f_00004-8-7 loss: 0.845064  [  256/  306]
train() client id: f_00004-8-8 loss: 1.017365  [  288/  306]
train() client id: f_00004-9-0 loss: 0.742750  [   32/  306]
train() client id: f_00004-9-1 loss: 0.902557  [   64/  306]
train() client id: f_00004-9-2 loss: 0.879452  [   96/  306]
train() client id: f_00004-9-3 loss: 0.934234  [  128/  306]
train() client id: f_00004-9-4 loss: 0.864596  [  160/  306]
train() client id: f_00004-9-5 loss: 0.838847  [  192/  306]
train() client id: f_00004-9-6 loss: 0.837689  [  224/  306]
train() client id: f_00004-9-7 loss: 0.827220  [  256/  306]
train() client id: f_00004-9-8 loss: 0.842529  [  288/  306]
train() client id: f_00004-10-0 loss: 0.924312  [   32/  306]
train() client id: f_00004-10-1 loss: 0.932549  [   64/  306]
train() client id: f_00004-10-2 loss: 0.937594  [   96/  306]
train() client id: f_00004-10-3 loss: 0.802804  [  128/  306]
train() client id: f_00004-10-4 loss: 0.814488  [  160/  306]
train() client id: f_00004-10-5 loss: 0.822948  [  192/  306]
train() client id: f_00004-10-6 loss: 0.907328  [  224/  306]
train() client id: f_00004-10-7 loss: 0.811343  [  256/  306]
train() client id: f_00004-10-8 loss: 0.866949  [  288/  306]
train() client id: f_00004-11-0 loss: 0.901060  [   32/  306]
train() client id: f_00004-11-1 loss: 0.903474  [   64/  306]
train() client id: f_00004-11-2 loss: 0.813692  [   96/  306]
train() client id: f_00004-11-3 loss: 0.922059  [  128/  306]
train() client id: f_00004-11-4 loss: 0.831057  [  160/  306]
train() client id: f_00004-11-5 loss: 0.879325  [  192/  306]
train() client id: f_00004-11-6 loss: 0.836754  [  224/  306]
train() client id: f_00004-11-7 loss: 0.747358  [  256/  306]
train() client id: f_00004-11-8 loss: 0.892727  [  288/  306]
train() client id: f_00004-12-0 loss: 0.855066  [   32/  306]
train() client id: f_00004-12-1 loss: 0.996741  [   64/  306]
train() client id: f_00004-12-2 loss: 0.681038  [   96/  306]
train() client id: f_00004-12-3 loss: 0.820461  [  128/  306]
train() client id: f_00004-12-4 loss: 1.014663  [  160/  306]
train() client id: f_00004-12-5 loss: 0.738075  [  192/  306]
train() client id: f_00004-12-6 loss: 0.846582  [  224/  306]
train() client id: f_00004-12-7 loss: 0.859297  [  256/  306]
train() client id: f_00004-12-8 loss: 0.811383  [  288/  306]
train() client id: f_00004-13-0 loss: 0.747975  [   32/  306]
train() client id: f_00004-13-1 loss: 0.826279  [   64/  306]
train() client id: f_00004-13-2 loss: 0.980415  [   96/  306]
train() client id: f_00004-13-3 loss: 0.835637  [  128/  306]
train() client id: f_00004-13-4 loss: 0.844102  [  160/  306]
train() client id: f_00004-13-5 loss: 0.919592  [  192/  306]
train() client id: f_00004-13-6 loss: 0.816979  [  224/  306]
train() client id: f_00004-13-7 loss: 0.934580  [  256/  306]
train() client id: f_00004-13-8 loss: 0.792665  [  288/  306]
train() client id: f_00004-14-0 loss: 0.799566  [   32/  306]
train() client id: f_00004-14-1 loss: 0.777831  [   64/  306]
train() client id: f_00004-14-2 loss: 0.856330  [   96/  306]
train() client id: f_00004-14-3 loss: 0.705011  [  128/  306]
train() client id: f_00004-14-4 loss: 0.928334  [  160/  306]
train() client id: f_00004-14-5 loss: 0.881436  [  192/  306]
train() client id: f_00004-14-6 loss: 0.806855  [  224/  306]
train() client id: f_00004-14-7 loss: 0.911133  [  256/  306]
train() client id: f_00004-14-8 loss: 0.984972  [  288/  306]
train() client id: f_00004-15-0 loss: 0.809449  [   32/  306]
train() client id: f_00004-15-1 loss: 0.810679  [   64/  306]
train() client id: f_00004-15-2 loss: 0.904222  [   96/  306]
train() client id: f_00004-15-3 loss: 0.852624  [  128/  306]
train() client id: f_00004-15-4 loss: 0.785696  [  160/  306]
train() client id: f_00004-15-5 loss: 0.874525  [  192/  306]
train() client id: f_00004-15-6 loss: 0.932008  [  224/  306]
train() client id: f_00004-15-7 loss: 0.769888  [  256/  306]
train() client id: f_00004-15-8 loss: 0.867467  [  288/  306]
train() client id: f_00004-16-0 loss: 0.711110  [   32/  306]
train() client id: f_00004-16-1 loss: 0.818616  [   64/  306]
train() client id: f_00004-16-2 loss: 0.865603  [   96/  306]
train() client id: f_00004-16-3 loss: 0.814625  [  128/  306]
train() client id: f_00004-16-4 loss: 0.898839  [  160/  306]
train() client id: f_00004-16-5 loss: 0.856532  [  192/  306]
train() client id: f_00004-16-6 loss: 0.847851  [  224/  306]
train() client id: f_00004-16-7 loss: 0.815027  [  256/  306]
train() client id: f_00004-16-8 loss: 0.900653  [  288/  306]
train() client id: f_00004-17-0 loss: 0.922224  [   32/  306]
train() client id: f_00004-17-1 loss: 0.838210  [   64/  306]
train() client id: f_00004-17-2 loss: 0.699628  [   96/  306]
train() client id: f_00004-17-3 loss: 0.866466  [  128/  306]
train() client id: f_00004-17-4 loss: 0.781775  [  160/  306]
train() client id: f_00004-17-5 loss: 0.771136  [  192/  306]
train() client id: f_00004-17-6 loss: 0.871836  [  224/  306]
train() client id: f_00004-17-7 loss: 0.909164  [  256/  306]
train() client id: f_00004-17-8 loss: 0.902826  [  288/  306]
train() client id: f_00005-0-0 loss: 0.505213  [   32/  146]
train() client id: f_00005-0-1 loss: 0.592039  [   64/  146]
train() client id: f_00005-0-2 loss: 0.654978  [   96/  146]
train() client id: f_00005-0-3 loss: 0.640148  [  128/  146]
train() client id: f_00005-1-0 loss: 0.516549  [   32/  146]
train() client id: f_00005-1-1 loss: 0.775211  [   64/  146]
train() client id: f_00005-1-2 loss: 0.871098  [   96/  146]
train() client id: f_00005-1-3 loss: 0.271352  [  128/  146]
train() client id: f_00005-2-0 loss: 0.624751  [   32/  146]
train() client id: f_00005-2-1 loss: 0.503631  [   64/  146]
train() client id: f_00005-2-2 loss: 0.502827  [   96/  146]
train() client id: f_00005-2-3 loss: 0.478467  [  128/  146]
train() client id: f_00005-3-0 loss: 0.523670  [   32/  146]
train() client id: f_00005-3-1 loss: 0.290135  [   64/  146]
train() client id: f_00005-3-2 loss: 0.636163  [   96/  146]
train() client id: f_00005-3-3 loss: 0.888214  [  128/  146]
train() client id: f_00005-4-0 loss: 0.671709  [   32/  146]
train() client id: f_00005-4-1 loss: 0.369291  [   64/  146]
train() client id: f_00005-4-2 loss: 0.675642  [   96/  146]
train() client id: f_00005-4-3 loss: 0.621697  [  128/  146]
train() client id: f_00005-5-0 loss: 0.616383  [   32/  146]
train() client id: f_00005-5-1 loss: 0.310923  [   64/  146]
train() client id: f_00005-5-2 loss: 0.694495  [   96/  146]
train() client id: f_00005-5-3 loss: 0.408493  [  128/  146]
train() client id: f_00005-6-0 loss: 0.268666  [   32/  146]
train() client id: f_00005-6-1 loss: 0.561287  [   64/  146]
train() client id: f_00005-6-2 loss: 0.337632  [   96/  146]
train() client id: f_00005-6-3 loss: 1.011862  [  128/  146]
train() client id: f_00005-7-0 loss: 0.391995  [   32/  146]
train() client id: f_00005-7-1 loss: 0.694716  [   64/  146]
train() client id: f_00005-7-2 loss: 0.438878  [   96/  146]
train() client id: f_00005-7-3 loss: 0.769687  [  128/  146]
train() client id: f_00005-8-0 loss: 0.885185  [   32/  146]
train() client id: f_00005-8-1 loss: 0.560679  [   64/  146]
train() client id: f_00005-8-2 loss: 0.408056  [   96/  146]
train() client id: f_00005-8-3 loss: 0.455907  [  128/  146]
train() client id: f_00005-9-0 loss: 0.227800  [   32/  146]
train() client id: f_00005-9-1 loss: 0.735918  [   64/  146]
train() client id: f_00005-9-2 loss: 0.551473  [   96/  146]
train() client id: f_00005-9-3 loss: 0.566839  [  128/  146]
train() client id: f_00005-10-0 loss: 0.783795  [   32/  146]
train() client id: f_00005-10-1 loss: 0.564419  [   64/  146]
train() client id: f_00005-10-2 loss: 0.598474  [   96/  146]
train() client id: f_00005-10-3 loss: 0.418126  [  128/  146]
train() client id: f_00005-11-0 loss: 0.648309  [   32/  146]
train() client id: f_00005-11-1 loss: 0.521177  [   64/  146]
train() client id: f_00005-11-2 loss: 0.393018  [   96/  146]
train() client id: f_00005-11-3 loss: 0.434416  [  128/  146]
train() client id: f_00005-12-0 loss: 0.291498  [   32/  146]
train() client id: f_00005-12-1 loss: 0.518414  [   64/  146]
train() client id: f_00005-12-2 loss: 0.867724  [   96/  146]
train() client id: f_00005-12-3 loss: 0.430617  [  128/  146]
train() client id: f_00005-13-0 loss: 0.643367  [   32/  146]
train() client id: f_00005-13-1 loss: 0.578385  [   64/  146]
train() client id: f_00005-13-2 loss: 0.455576  [   96/  146]
train() client id: f_00005-13-3 loss: 0.558660  [  128/  146]
train() client id: f_00005-14-0 loss: 0.503884  [   32/  146]
train() client id: f_00005-14-1 loss: 0.523420  [   64/  146]
train() client id: f_00005-14-2 loss: 0.882227  [   96/  146]
train() client id: f_00005-14-3 loss: 0.460639  [  128/  146]
train() client id: f_00005-15-0 loss: 0.647619  [   32/  146]
train() client id: f_00005-15-1 loss: 0.571244  [   64/  146]
train() client id: f_00005-15-2 loss: 0.536454  [   96/  146]
train() client id: f_00005-15-3 loss: 0.350026  [  128/  146]
train() client id: f_00005-16-0 loss: 0.857942  [   32/  146]
train() client id: f_00005-16-1 loss: 0.643674  [   64/  146]
train() client id: f_00005-16-2 loss: 0.369313  [   96/  146]
train() client id: f_00005-16-3 loss: 0.378163  [  128/  146]
train() client id: f_00005-17-0 loss: 0.563840  [   32/  146]
train() client id: f_00005-17-1 loss: 0.674153  [   64/  146]
train() client id: f_00005-17-2 loss: 0.456133  [   96/  146]
train() client id: f_00005-17-3 loss: 0.285580  [  128/  146]
train() client id: f_00006-0-0 loss: 0.432089  [   32/   54]
train() client id: f_00006-1-0 loss: 0.448178  [   32/   54]
train() client id: f_00006-2-0 loss: 0.547315  [   32/   54]
train() client id: f_00006-3-0 loss: 0.556332  [   32/   54]
train() client id: f_00006-4-0 loss: 0.550970  [   32/   54]
train() client id: f_00006-5-0 loss: 0.544504  [   32/   54]
train() client id: f_00006-6-0 loss: 0.531963  [   32/   54]
train() client id: f_00006-7-0 loss: 0.528642  [   32/   54]
train() client id: f_00006-8-0 loss: 0.538864  [   32/   54]
train() client id: f_00006-9-0 loss: 0.477270  [   32/   54]
train() client id: f_00006-10-0 loss: 0.496068  [   32/   54]
train() client id: f_00006-11-0 loss: 0.520480  [   32/   54]
train() client id: f_00006-12-0 loss: 0.524133  [   32/   54]
train() client id: f_00006-13-0 loss: 0.486264  [   32/   54]
train() client id: f_00006-14-0 loss: 0.437756  [   32/   54]
train() client id: f_00006-15-0 loss: 0.494924  [   32/   54]
train() client id: f_00006-16-0 loss: 0.488792  [   32/   54]
train() client id: f_00006-17-0 loss: 0.478441  [   32/   54]
train() client id: f_00007-0-0 loss: 0.507459  [   32/  179]
train() client id: f_00007-0-1 loss: 0.649957  [   64/  179]
train() client id: f_00007-0-2 loss: 0.582665  [   96/  179]
train() client id: f_00007-0-3 loss: 0.837246  [  128/  179]
train() client id: f_00007-0-4 loss: 0.572145  [  160/  179]
train() client id: f_00007-1-0 loss: 0.674012  [   32/  179]
train() client id: f_00007-1-1 loss: 0.552104  [   64/  179]
train() client id: f_00007-1-2 loss: 0.619982  [   96/  179]
train() client id: f_00007-1-3 loss: 0.676666  [  128/  179]
train() client id: f_00007-1-4 loss: 0.490774  [  160/  179]
train() client id: f_00007-2-0 loss: 0.636315  [   32/  179]
train() client id: f_00007-2-1 loss: 0.586643  [   64/  179]
train() client id: f_00007-2-2 loss: 0.578148  [   96/  179]
train() client id: f_00007-2-3 loss: 0.586372  [  128/  179]
train() client id: f_00007-2-4 loss: 0.452433  [  160/  179]
train() client id: f_00007-3-0 loss: 0.707463  [   32/  179]
train() client id: f_00007-3-1 loss: 0.516400  [   64/  179]
train() client id: f_00007-3-2 loss: 0.552229  [   96/  179]
train() client id: f_00007-3-3 loss: 0.499067  [  128/  179]
train() client id: f_00007-3-4 loss: 0.671236  [  160/  179]
train() client id: f_00007-4-0 loss: 0.562737  [   32/  179]
train() client id: f_00007-4-1 loss: 0.646360  [   64/  179]
train() client id: f_00007-4-2 loss: 0.507865  [   96/  179]
train() client id: f_00007-4-3 loss: 0.496320  [  128/  179]
train() client id: f_00007-4-4 loss: 0.686210  [  160/  179]
train() client id: f_00007-5-0 loss: 0.605044  [   32/  179]
train() client id: f_00007-5-1 loss: 0.503285  [   64/  179]
train() client id: f_00007-5-2 loss: 0.771403  [   96/  179]
train() client id: f_00007-5-3 loss: 0.594687  [  128/  179]
train() client id: f_00007-5-4 loss: 0.421981  [  160/  179]
train() client id: f_00007-6-0 loss: 0.423925  [   32/  179]
train() client id: f_00007-6-1 loss: 0.672211  [   64/  179]
train() client id: f_00007-6-2 loss: 0.473014  [   96/  179]
train() client id: f_00007-6-3 loss: 0.511766  [  128/  179]
train() client id: f_00007-6-4 loss: 0.839342  [  160/  179]
train() client id: f_00007-7-0 loss: 0.452179  [   32/  179]
train() client id: f_00007-7-1 loss: 0.431568  [   64/  179]
train() client id: f_00007-7-2 loss: 0.694408  [   96/  179]
train() client id: f_00007-7-3 loss: 0.516333  [  128/  179]
train() client id: f_00007-7-4 loss: 0.644519  [  160/  179]
train() client id: f_00007-8-0 loss: 0.380792  [   32/  179]
train() client id: f_00007-8-1 loss: 0.878214  [   64/  179]
train() client id: f_00007-8-2 loss: 0.485296  [   96/  179]
train() client id: f_00007-8-3 loss: 0.419864  [  128/  179]
train() client id: f_00007-8-4 loss: 0.468179  [  160/  179]
train() client id: f_00007-9-0 loss: 0.520226  [   32/  179]
train() client id: f_00007-9-1 loss: 0.371912  [   64/  179]
train() client id: f_00007-9-2 loss: 0.600268  [   96/  179]
train() client id: f_00007-9-3 loss: 0.504368  [  128/  179]
train() client id: f_00007-9-4 loss: 0.740410  [  160/  179]
train() client id: f_00007-10-0 loss: 0.760028  [   32/  179]
train() client id: f_00007-10-1 loss: 0.481346  [   64/  179]
train() client id: f_00007-10-2 loss: 0.378104  [   96/  179]
train() client id: f_00007-10-3 loss: 0.582766  [  128/  179]
train() client id: f_00007-10-4 loss: 0.643377  [  160/  179]
train() client id: f_00007-11-0 loss: 0.389066  [   32/  179]
train() client id: f_00007-11-1 loss: 0.530188  [   64/  179]
train() client id: f_00007-11-2 loss: 0.622089  [   96/  179]
train() client id: f_00007-11-3 loss: 0.628523  [  128/  179]
train() client id: f_00007-11-4 loss: 0.610736  [  160/  179]
train() client id: f_00007-12-0 loss: 0.497428  [   32/  179]
train() client id: f_00007-12-1 loss: 0.750563  [   64/  179]
train() client id: f_00007-12-2 loss: 0.674469  [   96/  179]
train() client id: f_00007-12-3 loss: 0.398714  [  128/  179]
train() client id: f_00007-12-4 loss: 0.347062  [  160/  179]
train() client id: f_00007-13-0 loss: 0.464175  [   32/  179]
train() client id: f_00007-13-1 loss: 0.694913  [   64/  179]
train() client id: f_00007-13-2 loss: 0.477369  [   96/  179]
train() client id: f_00007-13-3 loss: 0.535726  [  128/  179]
train() client id: f_00007-13-4 loss: 0.521074  [  160/  179]
train() client id: f_00007-14-0 loss: 0.539421  [   32/  179]
train() client id: f_00007-14-1 loss: 0.776789  [   64/  179]
train() client id: f_00007-14-2 loss: 0.571136  [   96/  179]
train() client id: f_00007-14-3 loss: 0.408667  [  128/  179]
train() client id: f_00007-14-4 loss: 0.493512  [  160/  179]
train() client id: f_00007-15-0 loss: 0.602291  [   32/  179]
train() client id: f_00007-15-1 loss: 0.572412  [   64/  179]
train() client id: f_00007-15-2 loss: 0.349814  [   96/  179]
train() client id: f_00007-15-3 loss: 0.698776  [  128/  179]
train() client id: f_00007-15-4 loss: 0.507323  [  160/  179]
train() client id: f_00007-16-0 loss: 0.411544  [   32/  179]
train() client id: f_00007-16-1 loss: 0.628197  [   64/  179]
train() client id: f_00007-16-2 loss: 0.860687  [   96/  179]
train() client id: f_00007-16-3 loss: 0.389959  [  128/  179]
train() client id: f_00007-16-4 loss: 0.376081  [  160/  179]
train() client id: f_00007-17-0 loss: 0.369864  [   32/  179]
train() client id: f_00007-17-1 loss: 0.412305  [   64/  179]
train() client id: f_00007-17-2 loss: 0.601053  [   96/  179]
train() client id: f_00007-17-3 loss: 0.567126  [  128/  179]
train() client id: f_00007-17-4 loss: 0.658959  [  160/  179]
train() client id: f_00008-0-0 loss: 0.757083  [   32/  130]
train() client id: f_00008-0-1 loss: 0.607058  [   64/  130]
train() client id: f_00008-0-2 loss: 0.611703  [   96/  130]
train() client id: f_00008-0-3 loss: 0.688314  [  128/  130]
train() client id: f_00008-1-0 loss: 0.617673  [   32/  130]
train() client id: f_00008-1-1 loss: 0.721623  [   64/  130]
train() client id: f_00008-1-2 loss: 0.614267  [   96/  130]
train() client id: f_00008-1-3 loss: 0.716954  [  128/  130]
train() client id: f_00008-2-0 loss: 0.771316  [   32/  130]
train() client id: f_00008-2-1 loss: 0.547801  [   64/  130]
train() client id: f_00008-2-2 loss: 0.631789  [   96/  130]
train() client id: f_00008-2-3 loss: 0.697082  [  128/  130]
train() client id: f_00008-3-0 loss: 0.604706  [   32/  130]
train() client id: f_00008-3-1 loss: 0.616432  [   64/  130]
train() client id: f_00008-3-2 loss: 0.764320  [   96/  130]
train() client id: f_00008-3-3 loss: 0.658442  [  128/  130]
train() client id: f_00008-4-0 loss: 0.714511  [   32/  130]
train() client id: f_00008-4-1 loss: 0.582853  [   64/  130]
train() client id: f_00008-4-2 loss: 0.630821  [   96/  130]
train() client id: f_00008-4-3 loss: 0.734728  [  128/  130]
train() client id: f_00008-5-0 loss: 0.680184  [   32/  130]
train() client id: f_00008-5-1 loss: 0.707716  [   64/  130]
train() client id: f_00008-5-2 loss: 0.566881  [   96/  130]
train() client id: f_00008-5-3 loss: 0.683065  [  128/  130]
train() client id: f_00008-6-0 loss: 0.727534  [   32/  130]
train() client id: f_00008-6-1 loss: 0.681072  [   64/  130]
train() client id: f_00008-6-2 loss: 0.619498  [   96/  130]
train() client id: f_00008-6-3 loss: 0.601843  [  128/  130]
train() client id: f_00008-7-0 loss: 0.577592  [   32/  130]
train() client id: f_00008-7-1 loss: 0.690151  [   64/  130]
train() client id: f_00008-7-2 loss: 0.676021  [   96/  130]
train() client id: f_00008-7-3 loss: 0.716505  [  128/  130]
train() client id: f_00008-8-0 loss: 0.638158  [   32/  130]
train() client id: f_00008-8-1 loss: 0.691912  [   64/  130]
train() client id: f_00008-8-2 loss: 0.559164  [   96/  130]
train() client id: f_00008-8-3 loss: 0.776712  [  128/  130]
train() client id: f_00008-9-0 loss: 0.610173  [   32/  130]
train() client id: f_00008-9-1 loss: 0.708579  [   64/  130]
train() client id: f_00008-9-2 loss: 0.700246  [   96/  130]
train() client id: f_00008-9-3 loss: 0.617877  [  128/  130]
train() client id: f_00008-10-0 loss: 0.754917  [   32/  130]
train() client id: f_00008-10-1 loss: 0.647202  [   64/  130]
train() client id: f_00008-10-2 loss: 0.665423  [   96/  130]
train() client id: f_00008-10-3 loss: 0.578741  [  128/  130]
train() client id: f_00008-11-0 loss: 0.636362  [   32/  130]
train() client id: f_00008-11-1 loss: 0.725011  [   64/  130]
train() client id: f_00008-11-2 loss: 0.705211  [   96/  130]
train() client id: f_00008-11-3 loss: 0.599540  [  128/  130]
train() client id: f_00008-12-0 loss: 0.667265  [   32/  130]
train() client id: f_00008-12-1 loss: 0.602175  [   64/  130]
train() client id: f_00008-12-2 loss: 0.741291  [   96/  130]
train() client id: f_00008-12-3 loss: 0.643470  [  128/  130]
train() client id: f_00008-13-0 loss: 0.663059  [   32/  130]
train() client id: f_00008-13-1 loss: 0.725048  [   64/  130]
train() client id: f_00008-13-2 loss: 0.662471  [   96/  130]
train() client id: f_00008-13-3 loss: 0.595558  [  128/  130]
train() client id: f_00008-14-0 loss: 0.705988  [   32/  130]
train() client id: f_00008-14-1 loss: 0.673638  [   64/  130]
train() client id: f_00008-14-2 loss: 0.647704  [   96/  130]
train() client id: f_00008-14-3 loss: 0.607782  [  128/  130]
train() client id: f_00008-15-0 loss: 0.638969  [   32/  130]
train() client id: f_00008-15-1 loss: 0.752067  [   64/  130]
train() client id: f_00008-15-2 loss: 0.573408  [   96/  130]
train() client id: f_00008-15-3 loss: 0.708432  [  128/  130]
train() client id: f_00008-16-0 loss: 0.728317  [   32/  130]
train() client id: f_00008-16-1 loss: 0.750956  [   64/  130]
train() client id: f_00008-16-2 loss: 0.547903  [   96/  130]
train() client id: f_00008-16-3 loss: 0.611568  [  128/  130]
train() client id: f_00008-17-0 loss: 0.673772  [   32/  130]
train() client id: f_00008-17-1 loss: 0.615004  [   64/  130]
train() client id: f_00008-17-2 loss: 0.634888  [   96/  130]
train() client id: f_00008-17-3 loss: 0.728079  [  128/  130]
train() client id: f_00009-0-0 loss: 0.888564  [   32/  118]
train() client id: f_00009-0-1 loss: 1.036936  [   64/  118]
train() client id: f_00009-0-2 loss: 1.159361  [   96/  118]
train() client id: f_00009-1-0 loss: 1.179439  [   32/  118]
train() client id: f_00009-1-1 loss: 0.769069  [   64/  118]
train() client id: f_00009-1-2 loss: 0.927965  [   96/  118]
train() client id: f_00009-2-0 loss: 0.870282  [   32/  118]
train() client id: f_00009-2-1 loss: 0.839743  [   64/  118]
train() client id: f_00009-2-2 loss: 0.987727  [   96/  118]
train() client id: f_00009-3-0 loss: 0.789356  [   32/  118]
train() client id: f_00009-3-1 loss: 0.846115  [   64/  118]
train() client id: f_00009-3-2 loss: 0.925310  [   96/  118]
train() client id: f_00009-4-0 loss: 0.801603  [   32/  118]
train() client id: f_00009-4-1 loss: 0.956350  [   64/  118]
train() client id: f_00009-4-2 loss: 0.832327  [   96/  118]
train() client id: f_00009-5-0 loss: 0.876844  [   32/  118]
train() client id: f_00009-5-1 loss: 0.646236  [   64/  118]
train() client id: f_00009-5-2 loss: 0.913899  [   96/  118]
train() client id: f_00009-6-0 loss: 0.804567  [   32/  118]
train() client id: f_00009-6-1 loss: 0.828609  [   64/  118]
train() client id: f_00009-6-2 loss: 0.709052  [   96/  118]
train() client id: f_00009-7-0 loss: 0.827648  [   32/  118]
train() client id: f_00009-7-1 loss: 0.795724  [   64/  118]
train() client id: f_00009-7-2 loss: 0.743089  [   96/  118]
train() client id: f_00009-8-0 loss: 0.803669  [   32/  118]
train() client id: f_00009-8-1 loss: 0.814180  [   64/  118]
train() client id: f_00009-8-2 loss: 0.676849  [   96/  118]
train() client id: f_00009-9-0 loss: 0.826402  [   32/  118]
train() client id: f_00009-9-1 loss: 0.836660  [   64/  118]
train() client id: f_00009-9-2 loss: 0.662944  [   96/  118]
train() client id: f_00009-10-0 loss: 0.790703  [   32/  118]
train() client id: f_00009-10-1 loss: 0.737024  [   64/  118]
train() client id: f_00009-10-2 loss: 0.844440  [   96/  118]
train() client id: f_00009-11-0 loss: 0.685184  [   32/  118]
train() client id: f_00009-11-1 loss: 0.660334  [   64/  118]
train() client id: f_00009-11-2 loss: 0.796727  [   96/  118]
train() client id: f_00009-12-0 loss: 0.787558  [   32/  118]
train() client id: f_00009-12-1 loss: 0.748563  [   64/  118]
train() client id: f_00009-12-2 loss: 0.774997  [   96/  118]
train() client id: f_00009-13-0 loss: 0.595902  [   32/  118]
train() client id: f_00009-13-1 loss: 0.831877  [   64/  118]
train() client id: f_00009-13-2 loss: 0.850661  [   96/  118]
train() client id: f_00009-14-0 loss: 0.754581  [   32/  118]
train() client id: f_00009-14-1 loss: 0.652760  [   64/  118]
train() client id: f_00009-14-2 loss: 0.616981  [   96/  118]
train() client id: f_00009-15-0 loss: 0.857837  [   32/  118]
train() client id: f_00009-15-1 loss: 0.839583  [   64/  118]
train() client id: f_00009-15-2 loss: 0.567366  [   96/  118]
train() client id: f_00009-16-0 loss: 0.851277  [   32/  118]
train() client id: f_00009-16-1 loss: 0.712323  [   64/  118]
train() client id: f_00009-16-2 loss: 0.783217  [   96/  118]
train() client id: f_00009-17-0 loss: 0.787638  [   32/  118]
train() client id: f_00009-17-1 loss: 0.743717  [   64/  118]
train() client id: f_00009-17-2 loss: 0.665858  [   96/  118]
At round 58 accuracy: 0.649867374005305
At round 58 training accuracy: 0.5895372233400402
At round 58 training loss: 0.835634566800785
update_location
xs = 8.927491 411.223621 5.882650 0.934260 -327.581990 -175.230757 -135.849135 -5.143845 -350.120581 20.134486 
ys = -402.390647 7.291448 300.684448 -122.290817 -9.642386 0.794442 -1.381692 296.628436 25.881276 -837.232496 
xs mean: -54.68237997052123
ys mean: -74.16579882624052
dists_uav = 414.726335 423.270636 316.931764 157.974418 342.641118 201.758394 168.691720 313.073295 365.040082 843.423766 
uav_gains = -121.920243 -122.200253 -117.061963 -104.973627 -118.756126 -107.792994 -105.697999 -116.773308 -119.941584 -130.122078 
uav_gains_db_mean: -116.52401755465445
dists_bs = 605.977955 609.741168 220.261466 345.448540 239.528859 174.205711 180.674546 208.912876 230.007851 1030.869581 
bs_gains = -117.475850 -117.551134 -105.169338 -110.641793 -106.189079 -102.316806 -102.760174 -104.526085 -105.695854 -123.936763 
bs_gains_db_mean: -109.62628763591981
Round 59
-------------------------------
ene_coms = [0.03634823 0.03790081 0.00801606 0.00793479 0.00846701 0.00928425
 0.00824387 0.00775486 0.00824292 0.04988585]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.35288324 6.74129176 3.07615213 1.10824327 7.50003433 3.62068082
 1.37789027 4.40983872 3.22370159 3.26760396]
obj_prev = 37.678320109296976
eta_min = 2.4096811087467978e-29	eta_max = 0.8517561250521261
af = 7.784714115820054	bf = 1.4516035555888505	zeta = 8.56318552740206	eta = 0.909090909090909
af = 7.784714115820054	bf = 1.4516035555888505	zeta = 24.12922663462215	eta = 0.3226259272085435
af = 7.784714115820054	bf = 1.4516035555888505	zeta = 15.143999834306308	eta = 0.5140461041332707
af = 7.784714115820054	bf = 1.4516035555888505	zeta = 13.647141920109881	eta = 0.5704281644751427
af = 7.784714115820054	bf = 1.4516035555888505	zeta = 13.552315900722874	eta = 0.5744194699154571
af = 7.784714115820054	bf = 1.4516035555888505	zeta = 13.551872271479017	eta = 0.5744382739057834
af = 7.784714115820054	bf = 1.4516035555888505	zeta = 13.551872261677957	eta = 0.5744382743212317
eta = 0.5744382743212317
ene_coms = [0.03634823 0.03790081 0.00801606 0.00793479 0.00846701 0.00928425
 0.00824387 0.00775486 0.00824292 0.04988585]
ene_comp = [0.04574468 0.09620904 0.04501857 0.01561128 0.11109421 0.05300574
 0.01960486 0.06498648 0.04719689 0.04284025]
ene_total = [1.53791399 2.51239007 0.99354134 0.44110796 2.23983878 1.1669296
 0.52171324 1.36272335 1.03859958 1.73711437]
ti_comp = [1.07748311 1.06195738 1.36080484 1.36161749 1.35629531 1.34812291
 1.35852671 1.36341689 1.35853621 0.94210693]
ti_coms = [0.36348233 0.37900806 0.0801606  0.07934795 0.08467013 0.09284253
 0.08243873 0.07754856 0.08242923 0.49885852]
t_total = [26.99479103 26.99479103 26.99479103 26.99479103 26.99479103 26.99479103
 26.99479103 26.99479103 26.99479103 26.99479103]
ene_coms = [0.03634823 0.03790081 0.00801606 0.00793479 0.00846701 0.00928425
 0.00824387 0.00775486 0.00824292 0.04988585]
ene_comp = [5.15324422e-06 4.93529913e-05 3.07938080e-06 1.28258205e-07
 4.65849669e-05 5.12141093e-06 2.55173136e-07 9.22766439e-06
 3.56022671e-06 5.53650457e-06]
ene_total = [0.68103789 0.71095155 0.15022914 0.14865144 0.15949223 0.1740255
 0.15444404 0.14545096 0.15448816 0.93465642]
optimize_network iter = 0 obj = 3.413427320739041
eta = 0.5744382743212317
freqs = [21227560.26857638 45297975.86190391 16541155.68048642  5732622.49206679
 40955023.1058506  19659088.46164728  7215485.99165822 23832213.05231223
 17370492.95178737 22736406.23256954]
eta_min = 0.5744382743212324	eta_max = 0.5744382743210986
af = 0.0018407905743814516	bf = 1.4516035555888505	zeta = 0.002024869631819597	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03634823 0.03790081 0.00801606 0.00793479 0.00846701 0.00928425
 0.00824387 0.00775486 0.00824292 0.04988585]
ene_comp = [8.85984384e-07 8.48513630e-06 5.29430236e-07 2.20511122e-08
 8.00923680e-06 8.80511367e-07 4.38712787e-08 1.58648925e-06
 6.12100869e-07 9.51877376e-07]
ene_total = [3.18768603 3.32450749 0.70302585 0.6958547  0.74322873 0.8142726
 0.72296165 0.68021188 0.72292817 4.37489191]
ti_comp = [1.07748311 1.06195738 1.36080484 1.36161749 1.35629531 1.34812291
 1.35852671 1.36341689 1.35853621 0.94210693]
ti_coms = [0.36348233 0.37900806 0.0801606  0.07934795 0.08467013 0.09284253
 0.08243873 0.07754856 0.08242923 0.49885852]
t_total = [26.99479103 26.99479103 26.99479103 26.99479103 26.99479103 26.99479103
 26.99479103 26.99479103 26.99479103 26.99479103]
ene_coms = [0.03634823 0.03790081 0.00801606 0.00793479 0.00846701 0.00928425
 0.00824387 0.00775486 0.00824292 0.04988585]
ene_comp = [5.15324422e-06 4.93529913e-05 3.07938080e-06 1.28258205e-07
 4.65849669e-05 5.12141093e-06 2.55173136e-07 9.22766439e-06
 3.56022671e-06 5.53650457e-06]
ene_total = [0.68103789 0.71095155 0.15022914 0.14865144 0.15949223 0.1740255
 0.15444404 0.14545096 0.15448816 0.93465642]
optimize_network iter = 1 obj = 3.413427320737974
eta = 0.5744382743210986
freqs = [21227560.26857638 45297975.86190362 16541155.68048785  5732622.49206729
 40955023.10585411 19659088.46164892  7215485.99165884 23832213.05231431
 17370492.95178887 22736406.23256816]
Done!
ene_coms = [0.03634823 0.03790081 0.00801606 0.00793479 0.00846701 0.00928425
 0.00824387 0.00775486 0.00824292 0.04988585]
ene_comp = [5.10990963e-06 4.89379728e-05 3.05348572e-06 1.27179658e-07
 4.61932253e-05 5.07834404e-06 2.53027337e-07 9.15006725e-06
 3.53028810e-06 5.48994707e-06]
ene_total = [0.03635334 0.03794974 0.00801911 0.00793492 0.00851321 0.00928933
 0.00824413 0.00776401 0.00824645 0.04989134]
At round 59 energy consumption: 0.18220558742179419
At round 59 eta: 0.5744382743210986
At round 59 a_n: 7.972397885064655
At round 59 local rounds: 18.15264900287079
At round 59 global rounds: 18.73382262548691
gradient difference: 0.3039829730987549
train() client id: f_00000-0-0 loss: 1.277337  [   32/  126]
train() client id: f_00000-0-1 loss: 1.217160  [   64/  126]
train() client id: f_00000-0-2 loss: 1.365900  [   96/  126]
train() client id: f_00000-1-0 loss: 1.225958  [   32/  126]
train() client id: f_00000-1-1 loss: 1.016822  [   64/  126]
train() client id: f_00000-1-2 loss: 1.286841  [   96/  126]
train() client id: f_00000-2-0 loss: 1.301157  [   32/  126]
train() client id: f_00000-2-1 loss: 0.966590  [   64/  126]
train() client id: f_00000-2-2 loss: 1.022788  [   96/  126]
train() client id: f_00000-3-0 loss: 1.164463  [   32/  126]
train() client id: f_00000-3-1 loss: 0.928343  [   64/  126]
train() client id: f_00000-3-2 loss: 0.974573  [   96/  126]
train() client id: f_00000-4-0 loss: 0.973154  [   32/  126]
train() client id: f_00000-4-1 loss: 1.138832  [   64/  126]
train() client id: f_00000-4-2 loss: 0.766644  [   96/  126]
train() client id: f_00000-5-0 loss: 0.934874  [   32/  126]
train() client id: f_00000-5-1 loss: 0.811490  [   64/  126]
train() client id: f_00000-5-2 loss: 1.000519  [   96/  126]
train() client id: f_00000-6-0 loss: 0.749213  [   32/  126]
train() client id: f_00000-6-1 loss: 0.961711  [   64/  126]
train() client id: f_00000-6-2 loss: 0.775637  [   96/  126]
train() client id: f_00000-7-0 loss: 0.802701  [   32/  126]
train() client id: f_00000-7-1 loss: 0.885705  [   64/  126]
train() client id: f_00000-7-2 loss: 0.796856  [   96/  126]
train() client id: f_00000-8-0 loss: 0.710085  [   32/  126]
train() client id: f_00000-8-1 loss: 0.835290  [   64/  126]
train() client id: f_00000-8-2 loss: 0.674501  [   96/  126]
train() client id: f_00000-9-0 loss: 0.844847  [   32/  126]
train() client id: f_00000-9-1 loss: 0.658645  [   64/  126]
train() client id: f_00000-9-2 loss: 0.747306  [   96/  126]
train() client id: f_00000-10-0 loss: 0.792724  [   32/  126]
train() client id: f_00000-10-1 loss: 0.792483  [   64/  126]
train() client id: f_00000-10-2 loss: 0.677809  [   96/  126]
train() client id: f_00000-11-0 loss: 0.799501  [   32/  126]
train() client id: f_00000-11-1 loss: 0.656874  [   64/  126]
train() client id: f_00000-11-2 loss: 0.737494  [   96/  126]
train() client id: f_00000-12-0 loss: 0.636590  [   32/  126]
train() client id: f_00000-12-1 loss: 0.772656  [   64/  126]
train() client id: f_00000-12-2 loss: 0.825357  [   96/  126]
train() client id: f_00000-13-0 loss: 0.778916  [   32/  126]
train() client id: f_00000-13-1 loss: 0.603010  [   64/  126]
train() client id: f_00000-13-2 loss: 0.705618  [   96/  126]
train() client id: f_00000-14-0 loss: 0.718657  [   32/  126]
train() client id: f_00000-14-1 loss: 0.801395  [   64/  126]
train() client id: f_00000-14-2 loss: 0.767528  [   96/  126]
train() client id: f_00000-15-0 loss: 0.688023  [   32/  126]
train() client id: f_00000-15-1 loss: 0.706824  [   64/  126]
train() client id: f_00000-15-2 loss: 0.743997  [   96/  126]
train() client id: f_00000-16-0 loss: 0.859462  [   32/  126]
train() client id: f_00000-16-1 loss: 0.724058  [   64/  126]
train() client id: f_00000-16-2 loss: 0.515898  [   96/  126]
train() client id: f_00000-17-0 loss: 0.675418  [   32/  126]
train() client id: f_00000-17-1 loss: 0.694491  [   64/  126]
train() client id: f_00000-17-2 loss: 0.746130  [   96/  126]
train() client id: f_00001-0-0 loss: 0.489695  [   32/  265]
train() client id: f_00001-0-1 loss: 0.435590  [   64/  265]
train() client id: f_00001-0-2 loss: 0.431682  [   96/  265]
train() client id: f_00001-0-3 loss: 0.520761  [  128/  265]
train() client id: f_00001-0-4 loss: 0.441158  [  160/  265]
train() client id: f_00001-0-5 loss: 0.436054  [  192/  265]
train() client id: f_00001-0-6 loss: 0.354085  [  224/  265]
train() client id: f_00001-0-7 loss: 0.490586  [  256/  265]
train() client id: f_00001-1-0 loss: 0.441664  [   32/  265]
train() client id: f_00001-1-1 loss: 0.443040  [   64/  265]
train() client id: f_00001-1-2 loss: 0.430036  [   96/  265]
train() client id: f_00001-1-3 loss: 0.386311  [  128/  265]
train() client id: f_00001-1-4 loss: 0.450178  [  160/  265]
train() client id: f_00001-1-5 loss: 0.423183  [  192/  265]
train() client id: f_00001-1-6 loss: 0.509600  [  224/  265]
train() client id: f_00001-1-7 loss: 0.432026  [  256/  265]
train() client id: f_00001-2-0 loss: 0.472519  [   32/  265]
train() client id: f_00001-2-1 loss: 0.515337  [   64/  265]
train() client id: f_00001-2-2 loss: 0.398899  [   96/  265]
train() client id: f_00001-2-3 loss: 0.405447  [  128/  265]
train() client id: f_00001-2-4 loss: 0.321017  [  160/  265]
train() client id: f_00001-2-5 loss: 0.513898  [  192/  265]
train() client id: f_00001-2-6 loss: 0.402681  [  224/  265]
train() client id: f_00001-2-7 loss: 0.425949  [  256/  265]
train() client id: f_00001-3-0 loss: 0.446795  [   32/  265]
train() client id: f_00001-3-1 loss: 0.547867  [   64/  265]
train() client id: f_00001-3-2 loss: 0.403190  [   96/  265]
train() client id: f_00001-3-3 loss: 0.376081  [  128/  265]
train() client id: f_00001-3-4 loss: 0.354587  [  160/  265]
train() client id: f_00001-3-5 loss: 0.453920  [  192/  265]
train() client id: f_00001-3-6 loss: 0.394479  [  224/  265]
train() client id: f_00001-3-7 loss: 0.393051  [  256/  265]
train() client id: f_00001-4-0 loss: 0.439696  [   32/  265]
train() client id: f_00001-4-1 loss: 0.472530  [   64/  265]
train() client id: f_00001-4-2 loss: 0.363350  [   96/  265]
train() client id: f_00001-4-3 loss: 0.593984  [  128/  265]
train() client id: f_00001-4-4 loss: 0.360659  [  160/  265]
train() client id: f_00001-4-5 loss: 0.323989  [  192/  265]
train() client id: f_00001-4-6 loss: 0.319995  [  224/  265]
train() client id: f_00001-4-7 loss: 0.472360  [  256/  265]
train() client id: f_00001-5-0 loss: 0.393352  [   32/  265]
train() client id: f_00001-5-1 loss: 0.338094  [   64/  265]
train() client id: f_00001-5-2 loss: 0.443972  [   96/  265]
train() client id: f_00001-5-3 loss: 0.355477  [  128/  265]
train() client id: f_00001-5-4 loss: 0.336766  [  160/  265]
train() client id: f_00001-5-5 loss: 0.309076  [  192/  265]
train() client id: f_00001-5-6 loss: 0.510500  [  224/  265]
train() client id: f_00001-5-7 loss: 0.655592  [  256/  265]
train() client id: f_00001-6-0 loss: 0.381540  [   32/  265]
train() client id: f_00001-6-1 loss: 0.524399  [   64/  265]
train() client id: f_00001-6-2 loss: 0.299486  [   96/  265]
train() client id: f_00001-6-3 loss: 0.450897  [  128/  265]
train() client id: f_00001-6-4 loss: 0.316993  [  160/  265]
train() client id: f_00001-6-5 loss: 0.412685  [  192/  265]
train() client id: f_00001-6-6 loss: 0.489420  [  224/  265]
train() client id: f_00001-6-7 loss: 0.438877  [  256/  265]
train() client id: f_00001-7-0 loss: 0.319577  [   32/  265]
train() client id: f_00001-7-1 loss: 0.456452  [   64/  265]
train() client id: f_00001-7-2 loss: 0.356071  [   96/  265]
train() client id: f_00001-7-3 loss: 0.450949  [  128/  265]
train() client id: f_00001-7-4 loss: 0.494303  [  160/  265]
train() client id: f_00001-7-5 loss: 0.361095  [  192/  265]
train() client id: f_00001-7-6 loss: 0.417969  [  224/  265]
train() client id: f_00001-7-7 loss: 0.451000  [  256/  265]
train() client id: f_00001-8-0 loss: 0.514903  [   32/  265]
train() client id: f_00001-8-1 loss: 0.423009  [   64/  265]
train() client id: f_00001-8-2 loss: 0.454733  [   96/  265]
train() client id: f_00001-8-3 loss: 0.370817  [  128/  265]
train() client id: f_00001-8-4 loss: 0.471676  [  160/  265]
train() client id: f_00001-8-5 loss: 0.387869  [  192/  265]
train() client id: f_00001-8-6 loss: 0.334312  [  224/  265]
train() client id: f_00001-8-7 loss: 0.344751  [  256/  265]
train() client id: f_00001-9-0 loss: 0.342501  [   32/  265]
train() client id: f_00001-9-1 loss: 0.444131  [   64/  265]
train() client id: f_00001-9-2 loss: 0.365070  [   96/  265]
train() client id: f_00001-9-3 loss: 0.429390  [  128/  265]
train() client id: f_00001-9-4 loss: 0.315142  [  160/  265]
train() client id: f_00001-9-5 loss: 0.475648  [  192/  265]
train() client id: f_00001-9-6 loss: 0.553931  [  224/  265]
train() client id: f_00001-9-7 loss: 0.355087  [  256/  265]
train() client id: f_00001-10-0 loss: 0.358073  [   32/  265]
train() client id: f_00001-10-1 loss: 0.527182  [   64/  265]
train() client id: f_00001-10-2 loss: 0.342885  [   96/  265]
train() client id: f_00001-10-3 loss: 0.392249  [  128/  265]
train() client id: f_00001-10-4 loss: 0.301221  [  160/  265]
train() client id: f_00001-10-5 loss: 0.535748  [  192/  265]
train() client id: f_00001-10-6 loss: 0.397613  [  224/  265]
train() client id: f_00001-10-7 loss: 0.424914  [  256/  265]
train() client id: f_00001-11-0 loss: 0.536192  [   32/  265]
train() client id: f_00001-11-1 loss: 0.304588  [   64/  265]
train() client id: f_00001-11-2 loss: 0.492593  [   96/  265]
train() client id: f_00001-11-3 loss: 0.325641  [  128/  265]
train() client id: f_00001-11-4 loss: 0.498737  [  160/  265]
train() client id: f_00001-11-5 loss: 0.307667  [  192/  265]
train() client id: f_00001-11-6 loss: 0.308638  [  224/  265]
train() client id: f_00001-11-7 loss: 0.481794  [  256/  265]
train() client id: f_00001-12-0 loss: 0.351257  [   32/  265]
train() client id: f_00001-12-1 loss: 0.387307  [   64/  265]
train() client id: f_00001-12-2 loss: 0.467667  [   96/  265]
train() client id: f_00001-12-3 loss: 0.413950  [  128/  265]
train() client id: f_00001-12-4 loss: 0.369477  [  160/  265]
train() client id: f_00001-12-5 loss: 0.456447  [  192/  265]
train() client id: f_00001-12-6 loss: 0.442157  [  224/  265]
train() client id: f_00001-12-7 loss: 0.373988  [  256/  265]
train() client id: f_00001-13-0 loss: 0.403120  [   32/  265]
train() client id: f_00001-13-1 loss: 0.318857  [   64/  265]
train() client id: f_00001-13-2 loss: 0.492242  [   96/  265]
train() client id: f_00001-13-3 loss: 0.349721  [  128/  265]
train() client id: f_00001-13-4 loss: 0.393275  [  160/  265]
train() client id: f_00001-13-5 loss: 0.479744  [  192/  265]
train() client id: f_00001-13-6 loss: 0.336830  [  224/  265]
train() client id: f_00001-13-7 loss: 0.371688  [  256/  265]
train() client id: f_00001-14-0 loss: 0.356030  [   32/  265]
train() client id: f_00001-14-1 loss: 0.423798  [   64/  265]
train() client id: f_00001-14-2 loss: 0.388697  [   96/  265]
train() client id: f_00001-14-3 loss: 0.298864  [  128/  265]
train() client id: f_00001-14-4 loss: 0.436584  [  160/  265]
train() client id: f_00001-14-5 loss: 0.592026  [  192/  265]
train() client id: f_00001-14-6 loss: 0.398739  [  224/  265]
train() client id: f_00001-14-7 loss: 0.342058  [  256/  265]
train() client id: f_00001-15-0 loss: 0.296227  [   32/  265]
train() client id: f_00001-15-1 loss: 0.518736  [   64/  265]
train() client id: f_00001-15-2 loss: 0.373957  [   96/  265]
train() client id: f_00001-15-3 loss: 0.415683  [  128/  265]
train() client id: f_00001-15-4 loss: 0.365842  [  160/  265]
train() client id: f_00001-15-5 loss: 0.487436  [  192/  265]
train() client id: f_00001-15-6 loss: 0.418104  [  224/  265]
train() client id: f_00001-15-7 loss: 0.331752  [  256/  265]
train() client id: f_00001-16-0 loss: 0.305246  [   32/  265]
train() client id: f_00001-16-1 loss: 0.557400  [   64/  265]
train() client id: f_00001-16-2 loss: 0.401267  [   96/  265]
train() client id: f_00001-16-3 loss: 0.459333  [  128/  265]
train() client id: f_00001-16-4 loss: 0.318998  [  160/  265]
train() client id: f_00001-16-5 loss: 0.474462  [  192/  265]
train() client id: f_00001-16-6 loss: 0.383086  [  224/  265]
train() client id: f_00001-16-7 loss: 0.309767  [  256/  265]
train() client id: f_00001-17-0 loss: 0.374852  [   32/  265]
train() client id: f_00001-17-1 loss: 0.444433  [   64/  265]
train() client id: f_00001-17-2 loss: 0.491189  [   96/  265]
train() client id: f_00001-17-3 loss: 0.404612  [  128/  265]
train() client id: f_00001-17-4 loss: 0.375161  [  160/  265]
train() client id: f_00001-17-5 loss: 0.379289  [  192/  265]
train() client id: f_00001-17-6 loss: 0.469413  [  224/  265]
train() client id: f_00001-17-7 loss: 0.322528  [  256/  265]
train() client id: f_00002-0-0 loss: 1.367640  [   32/  124]
train() client id: f_00002-0-1 loss: 1.297677  [   64/  124]
train() client id: f_00002-0-2 loss: 1.392427  [   96/  124]
train() client id: f_00002-1-0 loss: 1.185594  [   32/  124]
train() client id: f_00002-1-1 loss: 1.445183  [   64/  124]
train() client id: f_00002-1-2 loss: 1.252029  [   96/  124]
train() client id: f_00002-2-0 loss: 1.109248  [   32/  124]
train() client id: f_00002-2-1 loss: 1.294272  [   64/  124]
train() client id: f_00002-2-2 loss: 1.299147  [   96/  124]
train() client id: f_00002-3-0 loss: 1.186865  [   32/  124]
train() client id: f_00002-3-1 loss: 1.356960  [   64/  124]
train() client id: f_00002-3-2 loss: 1.079496  [   96/  124]
train() client id: f_00002-4-0 loss: 1.059333  [   32/  124]
train() client id: f_00002-4-1 loss: 1.198490  [   64/  124]
train() client id: f_00002-4-2 loss: 1.341074  [   96/  124]
train() client id: f_00002-5-0 loss: 1.103392  [   32/  124]
train() client id: f_00002-5-1 loss: 1.101884  [   64/  124]
train() client id: f_00002-5-2 loss: 1.255717  [   96/  124]
train() client id: f_00002-6-0 loss: 1.075866  [   32/  124]
train() client id: f_00002-6-1 loss: 1.177880  [   64/  124]
train() client id: f_00002-6-2 loss: 1.054916  [   96/  124]
train() client id: f_00002-7-0 loss: 1.166447  [   32/  124]
train() client id: f_00002-7-1 loss: 1.145628  [   64/  124]
train() client id: f_00002-7-2 loss: 0.938636  [   96/  124]
train() client id: f_00002-8-0 loss: 1.150731  [   32/  124]
train() client id: f_00002-8-1 loss: 0.973433  [   64/  124]
train() client id: f_00002-8-2 loss: 1.145831  [   96/  124]
train() client id: f_00002-9-0 loss: 0.930175  [   32/  124]
train() client id: f_00002-9-1 loss: 1.022281  [   64/  124]
train() client id: f_00002-9-2 loss: 1.045002  [   96/  124]
train() client id: f_00002-10-0 loss: 0.929249  [   32/  124]
train() client id: f_00002-10-1 loss: 0.984076  [   64/  124]
train() client id: f_00002-10-2 loss: 1.163263  [   96/  124]
train() client id: f_00002-11-0 loss: 1.106838  [   32/  124]
train() client id: f_00002-11-1 loss: 0.882056  [   64/  124]
train() client id: f_00002-11-2 loss: 1.112238  [   96/  124]
train() client id: f_00002-12-0 loss: 0.966278  [   32/  124]
train() client id: f_00002-12-1 loss: 1.160100  [   64/  124]
train() client id: f_00002-12-2 loss: 1.021638  [   96/  124]
train() client id: f_00002-13-0 loss: 0.988104  [   32/  124]
train() client id: f_00002-13-1 loss: 0.873497  [   64/  124]
train() client id: f_00002-13-2 loss: 1.200028  [   96/  124]
train() client id: f_00002-14-0 loss: 0.886672  [   32/  124]
train() client id: f_00002-14-1 loss: 0.955505  [   64/  124]
train() client id: f_00002-14-2 loss: 1.169187  [   96/  124]
train() client id: f_00002-15-0 loss: 1.068831  [   32/  124]
train() client id: f_00002-15-1 loss: 1.054082  [   64/  124]
train() client id: f_00002-15-2 loss: 0.979839  [   96/  124]
train() client id: f_00002-16-0 loss: 1.103099  [   32/  124]
train() client id: f_00002-16-1 loss: 0.901794  [   64/  124]
train() client id: f_00002-16-2 loss: 1.108103  [   96/  124]
train() client id: f_00002-17-0 loss: 1.015189  [   32/  124]
train() client id: f_00002-17-1 loss: 0.791198  [   64/  124]
train() client id: f_00002-17-2 loss: 1.035594  [   96/  124]
train() client id: f_00003-0-0 loss: 0.447596  [   32/   43]
train() client id: f_00003-1-0 loss: 0.428568  [   32/   43]
train() client id: f_00003-2-0 loss: 0.260981  [   32/   43]
train() client id: f_00003-3-0 loss: 0.493458  [   32/   43]
train() client id: f_00003-4-0 loss: 0.646490  [   32/   43]
train() client id: f_00003-5-0 loss: 0.601015  [   32/   43]
train() client id: f_00003-6-0 loss: 0.535118  [   32/   43]
train() client id: f_00003-7-0 loss: 0.498619  [   32/   43]
train() client id: f_00003-8-0 loss: 0.402191  [   32/   43]
train() client id: f_00003-9-0 loss: 0.484435  [   32/   43]
train() client id: f_00003-10-0 loss: 0.417184  [   32/   43]
train() client id: f_00003-11-0 loss: 0.564287  [   32/   43]
train() client id: f_00003-12-0 loss: 0.522711  [   32/   43]
train() client id: f_00003-13-0 loss: 0.450124  [   32/   43]
train() client id: f_00003-14-0 loss: 0.714160  [   32/   43]
train() client id: f_00003-15-0 loss: 0.770682  [   32/   43]
train() client id: f_00003-16-0 loss: 0.569299  [   32/   43]
train() client id: f_00003-17-0 loss: 0.474925  [   32/   43]
train() client id: f_00004-0-0 loss: 0.739655  [   32/  306]
train() client id: f_00004-0-1 loss: 0.793684  [   64/  306]
train() client id: f_00004-0-2 loss: 0.914814  [   96/  306]
train() client id: f_00004-0-3 loss: 0.688299  [  128/  306]
train() client id: f_00004-0-4 loss: 0.865801  [  160/  306]
train() client id: f_00004-0-5 loss: 0.762766  [  192/  306]
train() client id: f_00004-0-6 loss: 0.911372  [  224/  306]
train() client id: f_00004-0-7 loss: 0.842835  [  256/  306]
train() client id: f_00004-0-8 loss: 0.770076  [  288/  306]
train() client id: f_00004-1-0 loss: 0.858363  [   32/  306]
train() client id: f_00004-1-1 loss: 0.677126  [   64/  306]
train() client id: f_00004-1-2 loss: 0.830222  [   96/  306]
train() client id: f_00004-1-3 loss: 0.613960  [  128/  306]
train() client id: f_00004-1-4 loss: 0.775369  [  160/  306]
train() client id: f_00004-1-5 loss: 0.830141  [  192/  306]
train() client id: f_00004-1-6 loss: 0.879644  [  224/  306]
train() client id: f_00004-1-7 loss: 0.779741  [  256/  306]
train() client id: f_00004-1-8 loss: 0.971446  [  288/  306]
train() client id: f_00004-2-0 loss: 0.818375  [   32/  306]
train() client id: f_00004-2-1 loss: 0.775194  [   64/  306]
train() client id: f_00004-2-2 loss: 0.803111  [   96/  306]
train() client id: f_00004-2-3 loss: 0.908962  [  128/  306]
train() client id: f_00004-2-4 loss: 0.819754  [  160/  306]
train() client id: f_00004-2-5 loss: 0.870834  [  192/  306]
train() client id: f_00004-2-6 loss: 0.817123  [  224/  306]
train() client id: f_00004-2-7 loss: 0.747706  [  256/  306]
train() client id: f_00004-2-8 loss: 0.647673  [  288/  306]
train() client id: f_00004-3-0 loss: 0.817238  [   32/  306]
train() client id: f_00004-3-1 loss: 0.823773  [   64/  306]
train() client id: f_00004-3-2 loss: 0.794245  [   96/  306]
train() client id: f_00004-3-3 loss: 0.800941  [  128/  306]
train() client id: f_00004-3-4 loss: 0.824323  [  160/  306]
train() client id: f_00004-3-5 loss: 0.918423  [  192/  306]
train() client id: f_00004-3-6 loss: 0.796546  [  224/  306]
train() client id: f_00004-3-7 loss: 0.700239  [  256/  306]
train() client id: f_00004-3-8 loss: 0.674284  [  288/  306]
train() client id: f_00004-4-0 loss: 0.797180  [   32/  306]
train() client id: f_00004-4-1 loss: 0.937211  [   64/  306]
train() client id: f_00004-4-2 loss: 0.933168  [   96/  306]
train() client id: f_00004-4-3 loss: 0.746449  [  128/  306]
train() client id: f_00004-4-4 loss: 0.763932  [  160/  306]
train() client id: f_00004-4-5 loss: 0.767067  [  192/  306]
train() client id: f_00004-4-6 loss: 0.722971  [  224/  306]
train() client id: f_00004-4-7 loss: 0.783624  [  256/  306]
train() client id: f_00004-4-8 loss: 0.618725  [  288/  306]
train() client id: f_00004-5-0 loss: 0.787066  [   32/  306]
train() client id: f_00004-5-1 loss: 0.794433  [   64/  306]
train() client id: f_00004-5-2 loss: 0.840182  [   96/  306]
train() client id: f_00004-5-3 loss: 0.753173  [  128/  306]
train() client id: f_00004-5-4 loss: 0.665005  [  160/  306]
train() client id: f_00004-5-5 loss: 0.769239  [  192/  306]
train() client id: f_00004-5-6 loss: 0.749143  [  224/  306]
train() client id: f_00004-5-7 loss: 0.732306  [  256/  306]
train() client id: f_00004-5-8 loss: 0.854378  [  288/  306]
train() client id: f_00004-6-0 loss: 0.809604  [   32/  306]
train() client id: f_00004-6-1 loss: 0.652372  [   64/  306]
train() client id: f_00004-6-2 loss: 0.796219  [   96/  306]
train() client id: f_00004-6-3 loss: 0.780163  [  128/  306]
train() client id: f_00004-6-4 loss: 0.769507  [  160/  306]
train() client id: f_00004-6-5 loss: 0.778713  [  192/  306]
train() client id: f_00004-6-6 loss: 0.939607  [  224/  306]
train() client id: f_00004-6-7 loss: 0.705064  [  256/  306]
train() client id: f_00004-6-8 loss: 0.833631  [  288/  306]
train() client id: f_00004-7-0 loss: 0.787707  [   32/  306]
train() client id: f_00004-7-1 loss: 0.859258  [   64/  306]
train() client id: f_00004-7-2 loss: 0.841113  [   96/  306]
train() client id: f_00004-7-3 loss: 0.740710  [  128/  306]
train() client id: f_00004-7-4 loss: 0.742358  [  160/  306]
train() client id: f_00004-7-5 loss: 0.722183  [  192/  306]
train() client id: f_00004-7-6 loss: 0.710774  [  224/  306]
train() client id: f_00004-7-7 loss: 0.953110  [  256/  306]
train() client id: f_00004-7-8 loss: 0.660717  [  288/  306]
train() client id: f_00004-8-0 loss: 0.825805  [   32/  306]
train() client id: f_00004-8-1 loss: 0.917971  [   64/  306]
train() client id: f_00004-8-2 loss: 0.832956  [   96/  306]
train() client id: f_00004-8-3 loss: 0.714639  [  128/  306]
train() client id: f_00004-8-4 loss: 0.821759  [  160/  306]
train() client id: f_00004-8-5 loss: 0.806378  [  192/  306]
train() client id: f_00004-8-6 loss: 0.719766  [  224/  306]
train() client id: f_00004-8-7 loss: 0.777646  [  256/  306]
train() client id: f_00004-8-8 loss: 0.665399  [  288/  306]
train() client id: f_00004-9-0 loss: 0.745732  [   32/  306]
train() client id: f_00004-9-1 loss: 0.926733  [   64/  306]
train() client id: f_00004-9-2 loss: 0.680525  [   96/  306]
train() client id: f_00004-9-3 loss: 0.782119  [  128/  306]
train() client id: f_00004-9-4 loss: 0.671340  [  160/  306]
train() client id: f_00004-9-5 loss: 0.744234  [  192/  306]
train() client id: f_00004-9-6 loss: 0.859924  [  224/  306]
train() client id: f_00004-9-7 loss: 0.762197  [  256/  306]
train() client id: f_00004-9-8 loss: 0.832692  [  288/  306]
train() client id: f_00004-10-0 loss: 0.835666  [   32/  306]
train() client id: f_00004-10-1 loss: 0.685601  [   64/  306]
train() client id: f_00004-10-2 loss: 0.894347  [   96/  306]
train() client id: f_00004-10-3 loss: 0.833396  [  128/  306]
train() client id: f_00004-10-4 loss: 0.761934  [  160/  306]
train() client id: f_00004-10-5 loss: 0.779782  [  192/  306]
train() client id: f_00004-10-6 loss: 0.721707  [  224/  306]
train() client id: f_00004-10-7 loss: 0.759637  [  256/  306]
train() client id: f_00004-10-8 loss: 0.769705  [  288/  306]
train() client id: f_00004-11-0 loss: 0.806582  [   32/  306]
train() client id: f_00004-11-1 loss: 0.713890  [   64/  306]
train() client id: f_00004-11-2 loss: 0.751552  [   96/  306]
train() client id: f_00004-11-3 loss: 0.716800  [  128/  306]
train() client id: f_00004-11-4 loss: 1.019074  [  160/  306]
train() client id: f_00004-11-5 loss: 0.782659  [  192/  306]
train() client id: f_00004-11-6 loss: 0.749003  [  224/  306]
train() client id: f_00004-11-7 loss: 0.668762  [  256/  306]
train() client id: f_00004-11-8 loss: 0.774871  [  288/  306]
train() client id: f_00004-12-0 loss: 0.788871  [   32/  306]
train() client id: f_00004-12-1 loss: 0.792161  [   64/  306]
train() client id: f_00004-12-2 loss: 0.774881  [   96/  306]
train() client id: f_00004-12-3 loss: 0.771298  [  128/  306]
train() client id: f_00004-12-4 loss: 0.796106  [  160/  306]
train() client id: f_00004-12-5 loss: 0.626666  [  192/  306]
train() client id: f_00004-12-6 loss: 0.899783  [  224/  306]
train() client id: f_00004-12-7 loss: 0.769408  [  256/  306]
train() client id: f_00004-12-8 loss: 0.777043  [  288/  306]
train() client id: f_00004-13-0 loss: 0.828968  [   32/  306]
train() client id: f_00004-13-1 loss: 0.748586  [   64/  306]
train() client id: f_00004-13-2 loss: 0.775863  [   96/  306]
train() client id: f_00004-13-3 loss: 0.719285  [  128/  306]
train() client id: f_00004-13-4 loss: 0.782533  [  160/  306]
train() client id: f_00004-13-5 loss: 0.824069  [  192/  306]
train() client id: f_00004-13-6 loss: 0.740885  [  224/  306]
train() client id: f_00004-13-7 loss: 0.707088  [  256/  306]
train() client id: f_00004-13-8 loss: 0.795899  [  288/  306]
train() client id: f_00004-14-0 loss: 0.735637  [   32/  306]
train() client id: f_00004-14-1 loss: 0.783448  [   64/  306]
train() client id: f_00004-14-2 loss: 0.688660  [   96/  306]
train() client id: f_00004-14-3 loss: 0.801514  [  128/  306]
train() client id: f_00004-14-4 loss: 0.924767  [  160/  306]
train() client id: f_00004-14-5 loss: 0.691622  [  192/  306]
train() client id: f_00004-14-6 loss: 0.893285  [  224/  306]
train() client id: f_00004-14-7 loss: 0.725343  [  256/  306]
train() client id: f_00004-14-8 loss: 0.742500  [  288/  306]
train() client id: f_00004-15-0 loss: 0.705027  [   32/  306]
train() client id: f_00004-15-1 loss: 0.749401  [   64/  306]
train() client id: f_00004-15-2 loss: 0.814167  [   96/  306]
train() client id: f_00004-15-3 loss: 0.748422  [  128/  306]
train() client id: f_00004-15-4 loss: 0.906355  [  160/  306]
train() client id: f_00004-15-5 loss: 0.880103  [  192/  306]
train() client id: f_00004-15-6 loss: 0.752279  [  224/  306]
train() client id: f_00004-15-7 loss: 0.786046  [  256/  306]
train() client id: f_00004-15-8 loss: 0.675296  [  288/  306]
train() client id: f_00004-16-0 loss: 0.822608  [   32/  306]
train() client id: f_00004-16-1 loss: 0.792245  [   64/  306]
train() client id: f_00004-16-2 loss: 0.843078  [   96/  306]
train() client id: f_00004-16-3 loss: 0.830043  [  128/  306]
train() client id: f_00004-16-4 loss: 0.786107  [  160/  306]
train() client id: f_00004-16-5 loss: 0.725374  [  192/  306]
train() client id: f_00004-16-6 loss: 0.754143  [  224/  306]
train() client id: f_00004-16-7 loss: 0.705767  [  256/  306]
train() client id: f_00004-16-8 loss: 0.656744  [  288/  306]
train() client id: f_00004-17-0 loss: 0.806020  [   32/  306]
train() client id: f_00004-17-1 loss: 0.705539  [   64/  306]
train() client id: f_00004-17-2 loss: 0.763426  [   96/  306]
train() client id: f_00004-17-3 loss: 0.847313  [  128/  306]
train() client id: f_00004-17-4 loss: 0.889890  [  160/  306]
train() client id: f_00004-17-5 loss: 0.696167  [  192/  306]
train() client id: f_00004-17-6 loss: 0.682923  [  224/  306]
train() client id: f_00004-17-7 loss: 0.779814  [  256/  306]
train() client id: f_00004-17-8 loss: 0.798197  [  288/  306]
train() client id: f_00005-0-0 loss: 0.623703  [   32/  146]
train() client id: f_00005-0-1 loss: 0.831226  [   64/  146]
train() client id: f_00005-0-2 loss: 0.771745  [   96/  146]
train() client id: f_00005-0-3 loss: 0.953447  [  128/  146]
train() client id: f_00005-1-0 loss: 0.768726  [   32/  146]
train() client id: f_00005-1-1 loss: 0.931867  [   64/  146]
train() client id: f_00005-1-2 loss: 0.555955  [   96/  146]
train() client id: f_00005-1-3 loss: 0.936339  [  128/  146]
train() client id: f_00005-2-0 loss: 0.724753  [   32/  146]
train() client id: f_00005-2-1 loss: 0.927626  [   64/  146]
train() client id: f_00005-2-2 loss: 0.626439  [   96/  146]
train() client id: f_00005-2-3 loss: 0.810202  [  128/  146]
train() client id: f_00005-3-0 loss: 1.223199  [   32/  146]
train() client id: f_00005-3-1 loss: 0.813518  [   64/  146]
train() client id: f_00005-3-2 loss: 0.676556  [   96/  146]
train() client id: f_00005-3-3 loss: 0.690827  [  128/  146]
train() client id: f_00005-4-0 loss: 0.966083  [   32/  146]
train() client id: f_00005-4-1 loss: 0.740450  [   64/  146]
train() client id: f_00005-4-2 loss: 0.839197  [   96/  146]
train() client id: f_00005-4-3 loss: 0.794278  [  128/  146]
train() client id: f_00005-5-0 loss: 0.969075  [   32/  146]
train() client id: f_00005-5-1 loss: 0.755918  [   64/  146]
train() client id: f_00005-5-2 loss: 0.926789  [   96/  146]
train() client id: f_00005-5-3 loss: 0.729354  [  128/  146]
train() client id: f_00005-6-0 loss: 1.016189  [   32/  146]
train() client id: f_00005-6-1 loss: 0.531570  [   64/  146]
train() client id: f_00005-6-2 loss: 0.762447  [   96/  146]
train() client id: f_00005-6-3 loss: 0.940884  [  128/  146]
train() client id: f_00005-7-0 loss: 0.754950  [   32/  146]
train() client id: f_00005-7-1 loss: 1.027264  [   64/  146]
train() client id: f_00005-7-2 loss: 0.667620  [   96/  146]
train() client id: f_00005-7-3 loss: 0.980204  [  128/  146]
train() client id: f_00005-8-0 loss: 0.799726  [   32/  146]
train() client id: f_00005-8-1 loss: 1.115007  [   64/  146]
train() client id: f_00005-8-2 loss: 0.596825  [   96/  146]
train() client id: f_00005-8-3 loss: 0.746658  [  128/  146]
train() client id: f_00005-9-0 loss: 0.756215  [   32/  146]
train() client id: f_00005-9-1 loss: 0.906644  [   64/  146]
train() client id: f_00005-9-2 loss: 0.907572  [   96/  146]
train() client id: f_00005-9-3 loss: 0.840089  [  128/  146]
train() client id: f_00005-10-0 loss: 0.673665  [   32/  146]
train() client id: f_00005-10-1 loss: 0.968362  [   64/  146]
train() client id: f_00005-10-2 loss: 0.879080  [   96/  146]
train() client id: f_00005-10-3 loss: 0.610482  [  128/  146]
train() client id: f_00005-11-0 loss: 0.890439  [   32/  146]
train() client id: f_00005-11-1 loss: 0.743221  [   64/  146]
train() client id: f_00005-11-2 loss: 0.783912  [   96/  146]
train() client id: f_00005-11-3 loss: 0.740832  [  128/  146]
train() client id: f_00005-12-0 loss: 0.818257  [   32/  146]
train() client id: f_00005-12-1 loss: 1.228177  [   64/  146]
train() client id: f_00005-12-2 loss: 0.530593  [   96/  146]
train() client id: f_00005-12-3 loss: 0.674344  [  128/  146]
train() client id: f_00005-13-0 loss: 0.666210  [   32/  146]
train() client id: f_00005-13-1 loss: 0.746684  [   64/  146]
train() client id: f_00005-13-2 loss: 0.744702  [   96/  146]
train() client id: f_00005-13-3 loss: 0.654912  [  128/  146]
train() client id: f_00005-14-0 loss: 0.874410  [   32/  146]
train() client id: f_00005-14-1 loss: 0.626989  [   64/  146]
train() client id: f_00005-14-2 loss: 0.904726  [   96/  146]
train() client id: f_00005-14-3 loss: 0.853902  [  128/  146]
train() client id: f_00005-15-0 loss: 0.763163  [   32/  146]
train() client id: f_00005-15-1 loss: 0.979518  [   64/  146]
train() client id: f_00005-15-2 loss: 0.704355  [   96/  146]
train() client id: f_00005-15-3 loss: 0.729348  [  128/  146]
train() client id: f_00005-16-0 loss: 1.182351  [   32/  146]
train() client id: f_00005-16-1 loss: 0.601643  [   64/  146]
train() client id: f_00005-16-2 loss: 0.721701  [   96/  146]
train() client id: f_00005-16-3 loss: 0.717682  [  128/  146]
train() client id: f_00005-17-0 loss: 0.709893  [   32/  146]
train() client id: f_00005-17-1 loss: 0.834970  [   64/  146]
train() client id: f_00005-17-2 loss: 0.999941  [   96/  146]
train() client id: f_00005-17-3 loss: 0.747751  [  128/  146]
train() client id: f_00006-0-0 loss: 0.437128  [   32/   54]
train() client id: f_00006-1-0 loss: 0.473058  [   32/   54]
train() client id: f_00006-2-0 loss: 0.512198  [   32/   54]
train() client id: f_00006-3-0 loss: 0.483341  [   32/   54]
train() client id: f_00006-4-0 loss: 0.491043  [   32/   54]
train() client id: f_00006-5-0 loss: 0.545746  [   32/   54]
train() client id: f_00006-6-0 loss: 0.499884  [   32/   54]
train() client id: f_00006-7-0 loss: 0.480453  [   32/   54]
train() client id: f_00006-8-0 loss: 0.532241  [   32/   54]
train() client id: f_00006-9-0 loss: 0.498837  [   32/   54]
train() client id: f_00006-10-0 loss: 0.434232  [   32/   54]
train() client id: f_00006-11-0 loss: 0.554489  [   32/   54]
train() client id: f_00006-12-0 loss: 0.538799  [   32/   54]
train() client id: f_00006-13-0 loss: 0.500277  [   32/   54]
train() client id: f_00006-14-0 loss: 0.475546  [   32/   54]
train() client id: f_00006-15-0 loss: 0.480148  [   32/   54]
train() client id: f_00006-16-0 loss: 0.556958  [   32/   54]
train() client id: f_00006-17-0 loss: 0.497792  [   32/   54]
train() client id: f_00007-0-0 loss: 0.497913  [   32/  179]
train() client id: f_00007-0-1 loss: 0.480941  [   64/  179]
train() client id: f_00007-0-2 loss: 0.518515  [   96/  179]
train() client id: f_00007-0-3 loss: 0.319661  [  128/  179]
train() client id: f_00007-0-4 loss: 0.450840  [  160/  179]
train() client id: f_00007-1-0 loss: 0.405025  [   32/  179]
train() client id: f_00007-1-1 loss: 0.370258  [   64/  179]
train() client id: f_00007-1-2 loss: 0.726363  [   96/  179]
train() client id: f_00007-1-3 loss: 0.349523  [  128/  179]
train() client id: f_00007-1-4 loss: 0.275823  [  160/  179]
train() client id: f_00007-2-0 loss: 0.311491  [   32/  179]
train() client id: f_00007-2-1 loss: 0.363331  [   64/  179]
train() client id: f_00007-2-2 loss: 0.251794  [   96/  179]
train() client id: f_00007-2-3 loss: 0.817594  [  128/  179]
train() client id: f_00007-2-4 loss: 0.243936  [  160/  179]
train() client id: f_00007-3-0 loss: 0.496146  [   32/  179]
train() client id: f_00007-3-1 loss: 0.224908  [   64/  179]
train() client id: f_00007-3-2 loss: 0.630363  [   96/  179]
train() client id: f_00007-3-3 loss: 0.352822  [  128/  179]
train() client id: f_00007-3-4 loss: 0.294591  [  160/  179]
train() client id: f_00007-4-0 loss: 0.235016  [   32/  179]
train() client id: f_00007-4-1 loss: 0.422352  [   64/  179]
train() client id: f_00007-4-2 loss: 0.346506  [   96/  179]
train() client id: f_00007-4-3 loss: 0.628711  [  128/  179]
train() client id: f_00007-4-4 loss: 0.237349  [  160/  179]
train() client id: f_00007-5-0 loss: 0.531618  [   32/  179]
train() client id: f_00007-5-1 loss: 0.535843  [   64/  179]
train() client id: f_00007-5-2 loss: 0.312575  [   96/  179]
train() client id: f_00007-5-3 loss: 0.248201  [  128/  179]
train() client id: f_00007-5-4 loss: 0.194549  [  160/  179]
train() client id: f_00007-6-0 loss: 0.311442  [   32/  179]
train() client id: f_00007-6-1 loss: 0.350232  [   64/  179]
train() client id: f_00007-6-2 loss: 0.490974  [   96/  179]
train() client id: f_00007-6-3 loss: 0.259939  [  128/  179]
train() client id: f_00007-6-4 loss: 0.388039  [  160/  179]
train() client id: f_00007-7-0 loss: 0.487445  [   32/  179]
train() client id: f_00007-7-1 loss: 0.299608  [   64/  179]
train() client id: f_00007-7-2 loss: 0.357806  [   96/  179]
train() client id: f_00007-7-3 loss: 0.308808  [  128/  179]
train() client id: f_00007-7-4 loss: 0.238105  [  160/  179]
train() client id: f_00007-8-0 loss: 0.334066  [   32/  179]
train() client id: f_00007-8-1 loss: 0.258317  [   64/  179]
train() client id: f_00007-8-2 loss: 0.684085  [   96/  179]
train() client id: f_00007-8-3 loss: 0.261093  [  128/  179]
train() client id: f_00007-8-4 loss: 0.189926  [  160/  179]
train() client id: f_00007-9-0 loss: 0.227286  [   32/  179]
train() client id: f_00007-9-1 loss: 0.398588  [   64/  179]
train() client id: f_00007-9-2 loss: 0.477243  [   96/  179]
train() client id: f_00007-9-3 loss: 0.335622  [  128/  179]
train() client id: f_00007-9-4 loss: 0.180559  [  160/  179]
train() client id: f_00007-10-0 loss: 0.253190  [   32/  179]
train() client id: f_00007-10-1 loss: 0.228552  [   64/  179]
train() client id: f_00007-10-2 loss: 0.253109  [   96/  179]
train() client id: f_00007-10-3 loss: 0.556963  [  128/  179]
train() client id: f_00007-10-4 loss: 0.394677  [  160/  179]
train() client id: f_00007-11-0 loss: 0.295098  [   32/  179]
train() client id: f_00007-11-1 loss: 0.224613  [   64/  179]
train() client id: f_00007-11-2 loss: 0.764556  [   96/  179]
train() client id: f_00007-11-3 loss: 0.144831  [  128/  179]
train() client id: f_00007-11-4 loss: 0.259357  [  160/  179]
train() client id: f_00007-12-0 loss: 0.563361  [   32/  179]
train() client id: f_00007-12-1 loss: 0.342843  [   64/  179]
train() client id: f_00007-12-2 loss: 0.283543  [   96/  179]
train() client id: f_00007-12-3 loss: 0.268130  [  128/  179]
train() client id: f_00007-12-4 loss: 0.203000  [  160/  179]
train() client id: f_00007-13-0 loss: 0.241971  [   32/  179]
train() client id: f_00007-13-1 loss: 0.329163  [   64/  179]
train() client id: f_00007-13-2 loss: 0.255837  [   96/  179]
train() client id: f_00007-13-3 loss: 0.626250  [  128/  179]
train() client id: f_00007-13-4 loss: 0.194324  [  160/  179]
train() client id: f_00007-14-0 loss: 0.394031  [   32/  179]
train() client id: f_00007-14-1 loss: 0.160304  [   64/  179]
train() client id: f_00007-14-2 loss: 0.189142  [   96/  179]
train() client id: f_00007-14-3 loss: 0.546041  [  128/  179]
train() client id: f_00007-14-4 loss: 0.443442  [  160/  179]
train() client id: f_00007-15-0 loss: 0.622707  [   32/  179]
train() client id: f_00007-15-1 loss: 0.186046  [   64/  179]
train() client id: f_00007-15-2 loss: 0.178453  [   96/  179]
train() client id: f_00007-15-3 loss: 0.402010  [  128/  179]
train() client id: f_00007-15-4 loss: 0.334648  [  160/  179]
train() client id: f_00007-16-0 loss: 0.284917  [   32/  179]
train() client id: f_00007-16-1 loss: 0.280458  [   64/  179]
train() client id: f_00007-16-2 loss: 0.401226  [   96/  179]
train() client id: f_00007-16-3 loss: 0.450735  [  128/  179]
train() client id: f_00007-16-4 loss: 0.281075  [  160/  179]
train() client id: f_00007-17-0 loss: 0.363810  [   32/  179]
train() client id: f_00007-17-1 loss: 0.485639  [   64/  179]
train() client id: f_00007-17-2 loss: 0.425929  [   96/  179]
train() client id: f_00007-17-3 loss: 0.232755  [  128/  179]
train() client id: f_00007-17-4 loss: 0.260932  [  160/  179]
train() client id: f_00008-0-0 loss: 0.619546  [   32/  130]
train() client id: f_00008-0-1 loss: 0.664506  [   64/  130]
train() client id: f_00008-0-2 loss: 0.621704  [   96/  130]
train() client id: f_00008-0-3 loss: 0.640662  [  128/  130]
train() client id: f_00008-1-0 loss: 0.600662  [   32/  130]
train() client id: f_00008-1-1 loss: 0.720284  [   64/  130]
train() client id: f_00008-1-2 loss: 0.615835  [   96/  130]
train() client id: f_00008-1-3 loss: 0.598924  [  128/  130]
train() client id: f_00008-2-0 loss: 0.664152  [   32/  130]
train() client id: f_00008-2-1 loss: 0.547431  [   64/  130]
train() client id: f_00008-2-2 loss: 0.606842  [   96/  130]
train() client id: f_00008-2-3 loss: 0.721228  [  128/  130]
train() client id: f_00008-3-0 loss: 0.554085  [   32/  130]
train() client id: f_00008-3-1 loss: 0.723540  [   64/  130]
train() client id: f_00008-3-2 loss: 0.670730  [   96/  130]
train() client id: f_00008-3-3 loss: 0.585824  [  128/  130]
train() client id: f_00008-4-0 loss: 0.597450  [   32/  130]
train() client id: f_00008-4-1 loss: 0.679109  [   64/  130]
train() client id: f_00008-4-2 loss: 0.663279  [   96/  130]
train() client id: f_00008-4-3 loss: 0.609369  [  128/  130]
train() client id: f_00008-5-0 loss: 0.618945  [   32/  130]
train() client id: f_00008-5-1 loss: 0.636396  [   64/  130]
train() client id: f_00008-5-2 loss: 0.683314  [   96/  130]
train() client id: f_00008-5-3 loss: 0.602177  [  128/  130]
train() client id: f_00008-6-0 loss: 0.582223  [   32/  130]
train() client id: f_00008-6-1 loss: 0.582827  [   64/  130]
train() client id: f_00008-6-2 loss: 0.657357  [   96/  130]
train() client id: f_00008-6-3 loss: 0.710304  [  128/  130]
train() client id: f_00008-7-0 loss: 0.575112  [   32/  130]
train() client id: f_00008-7-1 loss: 0.609821  [   64/  130]
train() client id: f_00008-7-2 loss: 0.612606  [   96/  130]
train() client id: f_00008-7-3 loss: 0.733883  [  128/  130]
train() client id: f_00008-8-0 loss: 0.628133  [   32/  130]
train() client id: f_00008-8-1 loss: 0.665117  [   64/  130]
train() client id: f_00008-8-2 loss: 0.654846  [   96/  130]
train() client id: f_00008-8-3 loss: 0.558026  [  128/  130]
train() client id: f_00008-9-0 loss: 0.647682  [   32/  130]
train() client id: f_00008-9-1 loss: 0.597982  [   64/  130]
train() client id: f_00008-9-2 loss: 0.558785  [   96/  130]
train() client id: f_00008-9-3 loss: 0.699406  [  128/  130]
train() client id: f_00008-10-0 loss: 0.517061  [   32/  130]
train() client id: f_00008-10-1 loss: 0.673768  [   64/  130]
train() client id: f_00008-10-2 loss: 0.678128  [   96/  130]
train() client id: f_00008-10-3 loss: 0.648257  [  128/  130]
train() client id: f_00008-11-0 loss: 0.663055  [   32/  130]
train() client id: f_00008-11-1 loss: 0.628323  [   64/  130]
train() client id: f_00008-11-2 loss: 0.625205  [   96/  130]
train() client id: f_00008-11-3 loss: 0.611556  [  128/  130]
train() client id: f_00008-12-0 loss: 0.672278  [   32/  130]
train() client id: f_00008-12-1 loss: 0.676002  [   64/  130]
train() client id: f_00008-12-2 loss: 0.560217  [   96/  130]
train() client id: f_00008-12-3 loss: 0.636483  [  128/  130]
train() client id: f_00008-13-0 loss: 0.562730  [   32/  130]
train() client id: f_00008-13-1 loss: 0.654644  [   64/  130]
train() client id: f_00008-13-2 loss: 0.682513  [   96/  130]
train() client id: f_00008-13-3 loss: 0.639224  [  128/  130]
train() client id: f_00008-14-0 loss: 0.526706  [   32/  130]
train() client id: f_00008-14-1 loss: 0.597296  [   64/  130]
train() client id: f_00008-14-2 loss: 0.716435  [   96/  130]
train() client id: f_00008-14-3 loss: 0.699593  [  128/  130]
train() client id: f_00008-15-0 loss: 0.653227  [   32/  130]
train() client id: f_00008-15-1 loss: 0.608433  [   64/  130]
train() client id: f_00008-15-2 loss: 0.677025  [   96/  130]
train() client id: f_00008-15-3 loss: 0.590282  [  128/  130]
train() client id: f_00008-16-0 loss: 0.617747  [   32/  130]
train() client id: f_00008-16-1 loss: 0.728582  [   64/  130]
train() client id: f_00008-16-2 loss: 0.548560  [   96/  130]
train() client id: f_00008-16-3 loss: 0.615037  [  128/  130]
train() client id: f_00008-17-0 loss: 0.574936  [   32/  130]
train() client id: f_00008-17-1 loss: 0.638548  [   64/  130]
train() client id: f_00008-17-2 loss: 0.646754  [   96/  130]
train() client id: f_00008-17-3 loss: 0.675004  [  128/  130]
train() client id: f_00009-0-0 loss: 1.097625  [   32/  118]
train() client id: f_00009-0-1 loss: 1.310585  [   64/  118]
train() client id: f_00009-0-2 loss: 1.161810  [   96/  118]
train() client id: f_00009-1-0 loss: 0.986646  [   32/  118]
train() client id: f_00009-1-1 loss: 1.129516  [   64/  118]
train() client id: f_00009-1-2 loss: 1.191219  [   96/  118]
train() client id: f_00009-2-0 loss: 0.946711  [   32/  118]
train() client id: f_00009-2-1 loss: 1.012658  [   64/  118]
train() client id: f_00009-2-2 loss: 1.122957  [   96/  118]
train() client id: f_00009-3-0 loss: 0.963934  [   32/  118]
train() client id: f_00009-3-1 loss: 1.041188  [   64/  118]
train() client id: f_00009-3-2 loss: 0.977133  [   96/  118]
train() client id: f_00009-4-0 loss: 0.953517  [   32/  118]
train() client id: f_00009-4-1 loss: 0.992628  [   64/  118]
train() client id: f_00009-4-2 loss: 0.925834  [   96/  118]
train() client id: f_00009-5-0 loss: 0.910407  [   32/  118]
train() client id: f_00009-5-1 loss: 1.033402  [   64/  118]
train() client id: f_00009-5-2 loss: 0.804821  [   96/  118]
train() client id: f_00009-6-0 loss: 0.844510  [   32/  118]
train() client id: f_00009-6-1 loss: 0.962464  [   64/  118]
train() client id: f_00009-6-2 loss: 0.903525  [   96/  118]
train() client id: f_00009-7-0 loss: 1.007816  [   32/  118]
train() client id: f_00009-7-1 loss: 0.777882  [   64/  118]
train() client id: f_00009-7-2 loss: 0.750890  [   96/  118]
train() client id: f_00009-8-0 loss: 0.813437  [   32/  118]
train() client id: f_00009-8-1 loss: 0.682712  [   64/  118]
train() client id: f_00009-8-2 loss: 0.903250  [   96/  118]
train() client id: f_00009-9-0 loss: 0.753999  [   32/  118]
train() client id: f_00009-9-1 loss: 0.737991  [   64/  118]
train() client id: f_00009-9-2 loss: 0.968555  [   96/  118]
train() client id: f_00009-10-0 loss: 0.816058  [   32/  118]
train() client id: f_00009-10-1 loss: 0.887285  [   64/  118]
train() client id: f_00009-10-2 loss: 0.729320  [   96/  118]
train() client id: f_00009-11-0 loss: 0.851341  [   32/  118]
train() client id: f_00009-11-1 loss: 0.703106  [   64/  118]
train() client id: f_00009-11-2 loss: 0.858909  [   96/  118]
train() client id: f_00009-12-0 loss: 0.763511  [   32/  118]
train() client id: f_00009-12-1 loss: 0.849348  [   64/  118]
train() client id: f_00009-12-2 loss: 0.696023  [   96/  118]
train() client id: f_00009-13-0 loss: 0.887802  [   32/  118]
train() client id: f_00009-13-1 loss: 0.710532  [   64/  118]
train() client id: f_00009-13-2 loss: 0.818263  [   96/  118]
train() client id: f_00009-14-0 loss: 0.694338  [   32/  118]
train() client id: f_00009-14-1 loss: 0.897611  [   64/  118]
train() client id: f_00009-14-2 loss: 0.792008  [   96/  118]
train() client id: f_00009-15-0 loss: 0.873478  [   32/  118]
train() client id: f_00009-15-1 loss: 0.843501  [   64/  118]
train() client id: f_00009-15-2 loss: 0.770788  [   96/  118]
train() client id: f_00009-16-0 loss: 0.858833  [   32/  118]
train() client id: f_00009-16-1 loss: 0.699600  [   64/  118]
train() client id: f_00009-16-2 loss: 0.698141  [   96/  118]
train() client id: f_00009-17-0 loss: 0.768783  [   32/  118]
train() client id: f_00009-17-1 loss: 0.712784  [   64/  118]
train() client id: f_00009-17-2 loss: 0.820662  [   96/  118]
At round 59 accuracy: 0.649867374005305
At round 59 training accuracy: 0.5881958417169685
At round 59 training loss: 0.8435218022468739
update_location
xs = 8.927491 416.223621 5.882650 0.934260 -332.581990 -180.230757 -140.849135 -5.143845 -355.120581 20.134486 
ys = -407.390647 7.291448 305.684448 -127.290817 -9.642386 0.794442 -1.381692 301.628436 25.881276 -842.232496 
xs mean: -56.18237997052123
ys mean: -74.66579882624052
dists_uav = 419.579360 428.129966 321.679324 161.875956 347.424460 206.115883 172.743706 317.814683 369.838434 848.387279 
uav_gains = -122.080906 -122.353837 -117.404656 -105.241816 -119.029798 -108.070381 -105.962808 -117.126740 -120.166648 -130.186149 
uav_gains_db_mean: -116.76237390796898
dists_bs = 610.743963 614.549858 223.152320 349.760778 242.744504 174.284070 179.657405 211.862867 233.837160 1035.779619 
bs_gains = -117.571116 -117.646659 -105.327899 -110.792650 -106.351243 -102.322275 -102.691523 -104.696595 -105.896638 -123.994545 
bs_gains_db_mean: -109.72911411482016
Round 60
-------------------------------
ene_coms = [0.03722795 0.03879179 0.00808309 0.00804657 0.00854328 0.00944112
 0.00836276 0.00782247 0.00833275 0.05036659]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.2156016  6.45850896 2.94449734 1.06148738 7.17837209 3.4663219
 1.31960347 4.22088456 3.08588276 3.13091154]
obj_prev = 36.082071603009695
eta_min = 1.3448337700164167e-30	eta_max = 0.8565266582855782
af = 7.4502323791253575	bf = 1.4116628559483775	zeta = 8.195255617037894	eta = 0.909090909090909
af = 7.4502323791253575	bf = 1.4116628559483775	zeta = 23.33920427945497	eta = 0.3192153549846447
af = 7.4502323791253575	bf = 1.4116628559483775	zeta = 14.56995782605732	eta = 0.5113420689386727
af = 7.4502323791253575	bf = 1.4116628559483775	zeta = 13.114811503542734	eta = 0.5680777323496269
af = 7.4502323791253575	bf = 1.4116628559483775	zeta = 13.022591014179117	eta = 0.572100618917808
af = 7.4502323791253575	bf = 1.4116628559483775	zeta = 13.02215852942398	eta = 0.5721196192083917
af = 7.4502323791253575	bf = 1.4116628559483775	zeta = 13.022158519821712	eta = 0.5721196196302608
eta = 0.5721196196302608
ene_coms = [0.03722795 0.03879179 0.00808309 0.00804657 0.00854328 0.00944112
 0.00836276 0.00782247 0.00833275 0.05036659]
ene_comp = [0.04607842 0.09691097 0.04534702 0.01572518 0.11190474 0.05339246
 0.0197479  0.06546062 0.04754123 0.04315281]
ene_total = [1.48549771 2.41981643 0.95275193 0.42389155 2.14779792 1.12043212
 0.50126201 1.30676492 0.99633026 1.66761366]
ti_comp = [1.13872578 1.12308741 1.43017438 1.43053966 1.4255725  1.4165941
 1.42737767 1.43278066 1.42767786 1.00733943]
ti_coms = [0.37227954 0.38791792 0.08083094 0.08046567 0.08543282 0.09441123
 0.08362765 0.07822466 0.08332747 0.5036659 ]
t_total = [26.94385529 26.94385529 26.94385529 26.94385529 26.94385529 26.94385529
 26.94385529 26.94385529 26.94385529 26.94385529]
ene_coms = [0.03722795 0.03879179 0.00808309 0.00804657 0.00854328 0.00944112
 0.00836276 0.00782247 0.00833275 0.05036659]
ene_comp = [4.71557346e-06 4.50994966e-05 2.84936610e-06 1.18759060e-07
 4.30969810e-05 4.74055183e-06 2.36246111e-07 8.54005872e-06
 3.29481319e-06 4.94943407e-06]
ene_total = [0.66392332 0.69252938 0.14418645 0.14348641 0.15311009 0.16843619
 0.14912687 0.13964047 0.14864613 0.89821223]
optimize_network iter = 0 obj = 3.3012975268681504
eta = 0.5721196196302608
freqs = [20232448.90198144 43144891.15037671 15853667.72453333  5496239.0717523
 39249052.62983756 18845362.21486477  6917543.85480377 22843906.82906492
 16649844.79040946 21419199.48587523]
eta_min = 0.5721196196302614	eta_max = 0.5721196196302614
af = 0.001607403319622668	bf = 1.4116628559483775	zeta = 0.001768143651584935	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03722795 0.03879179 0.00808309 0.00804657 0.00854328 0.00944112
 0.00836276 0.00782247 0.00833275 0.05036659]
ene_comp = [8.04864563e-07 7.69768236e-06 4.86336142e-07 2.02700604e-08
 7.35588855e-06 8.09127927e-07 4.03230114e-08 1.45763621e-06
 5.62366042e-07 8.44780413e-07]
ene_total = [3.12454915 3.256378   0.67844175 0.67533695 0.71764115 0.79244597
 0.70187663 0.65664918 0.69940106 4.22725679]
ti_comp = [1.13872578 1.12308741 1.43017438 1.43053966 1.4255725  1.4165941
 1.42737767 1.43278066 1.42767786 1.00733943]
ti_coms = [0.37227954 0.38791792 0.08083094 0.08046567 0.08543282 0.09441123
 0.08362765 0.07822466 0.08332747 0.5036659 ]
t_total = [26.94385529 26.94385529 26.94385529 26.94385529 26.94385529 26.94385529
 26.94385529 26.94385529 26.94385529 26.94385529]
ene_coms = [0.03722795 0.03879179 0.00808309 0.00804657 0.00854328 0.00944112
 0.00836276 0.00782247 0.00833275 0.05036659]
ene_comp = [4.71557346e-06 4.50994966e-05 2.84936610e-06 1.18759060e-07
 4.30969810e-05 4.74055183e-06 2.36246111e-07 8.54005872e-06
 3.29481319e-06 4.94943407e-06]
ene_total = [0.66392332 0.69252938 0.14418645 0.14348641 0.15311009 0.16843619
 0.14912687 0.13964047 0.14864613 0.89821223]
optimize_network iter = 1 obj = 3.301297526868154
eta = 0.5721196196302614
freqs = [20232448.90198144 43144891.15037671 15853667.72453333  5496239.0717523
 39249052.62983754 18845362.21486477  6917543.85480377 22843906.82906492
 16649844.79040945 21419199.48587524]
Done!
ene_coms = [0.03722795 0.03879179 0.00808309 0.00804657 0.00854328 0.00944112
 0.00836276 0.00782247 0.00833275 0.05036659]
ene_comp = [4.64205155e-06 4.43963369e-05 2.80494079e-06 1.16907452e-07
 4.24250432e-05 4.66664048e-06 2.32562727e-07 8.40690812e-06
 3.24344278e-06 4.87226597e-06]
ene_total = [0.0372326  0.03883619 0.0080859  0.00804668 0.00858571 0.00944579
 0.008363   0.00783087 0.00833599 0.05037146]
At round 60 energy consumption: 0.1851341860803158
At round 60 eta: 0.5721196196302614
At round 60 a_n: 7.62985203809534
At round 60 local rounds: 18.28508826068327
At round 60 global rounds: 17.831740804526387
gradient difference: 0.27196913957595825
train() client id: f_00000-0-0 loss: 1.285759  [   32/  126]
train() client id: f_00000-0-1 loss: 1.133993  [   64/  126]
train() client id: f_00000-0-2 loss: 1.327784  [   96/  126]
train() client id: f_00000-1-0 loss: 1.215479  [   32/  126]
train() client id: f_00000-1-1 loss: 1.041977  [   64/  126]
train() client id: f_00000-1-2 loss: 1.143231  [   96/  126]
train() client id: f_00000-2-0 loss: 1.044478  [   32/  126]
train() client id: f_00000-2-1 loss: 1.097160  [   64/  126]
train() client id: f_00000-2-2 loss: 0.928423  [   96/  126]
train() client id: f_00000-3-0 loss: 0.972251  [   32/  126]
train() client id: f_00000-3-1 loss: 0.950869  [   64/  126]
train() client id: f_00000-3-2 loss: 0.994569  [   96/  126]
train() client id: f_00000-4-0 loss: 0.868394  [   32/  126]
train() client id: f_00000-4-1 loss: 1.031645  [   64/  126]
train() client id: f_00000-4-2 loss: 0.782938  [   96/  126]
train() client id: f_00000-5-0 loss: 0.754009  [   32/  126]
train() client id: f_00000-5-1 loss: 0.916066  [   64/  126]
train() client id: f_00000-5-2 loss: 0.965856  [   96/  126]
train() client id: f_00000-6-0 loss: 0.829437  [   32/  126]
train() client id: f_00000-6-1 loss: 0.833927  [   64/  126]
train() client id: f_00000-6-2 loss: 0.746596  [   96/  126]
train() client id: f_00000-7-0 loss: 0.829998  [   32/  126]
train() client id: f_00000-7-1 loss: 0.873743  [   64/  126]
train() client id: f_00000-7-2 loss: 0.675793  [   96/  126]
train() client id: f_00000-8-0 loss: 0.736393  [   32/  126]
train() client id: f_00000-8-1 loss: 0.744031  [   64/  126]
train() client id: f_00000-8-2 loss: 0.880488  [   96/  126]
train() client id: f_00000-9-0 loss: 0.746972  [   32/  126]
train() client id: f_00000-9-1 loss: 0.823600  [   64/  126]
train() client id: f_00000-9-2 loss: 0.804788  [   96/  126]
train() client id: f_00000-10-0 loss: 0.791323  [   32/  126]
train() client id: f_00000-10-1 loss: 0.801545  [   64/  126]
train() client id: f_00000-10-2 loss: 0.682161  [   96/  126]
train() client id: f_00000-11-0 loss: 0.770806  [   32/  126]
train() client id: f_00000-11-1 loss: 0.742406  [   64/  126]
train() client id: f_00000-11-2 loss: 0.815740  [   96/  126]
train() client id: f_00000-12-0 loss: 0.714241  [   32/  126]
train() client id: f_00000-12-1 loss: 0.762810  [   64/  126]
train() client id: f_00000-12-2 loss: 0.838351  [   96/  126]
train() client id: f_00000-13-0 loss: 0.629851  [   32/  126]
train() client id: f_00000-13-1 loss: 0.785586  [   64/  126]
train() client id: f_00000-13-2 loss: 0.771330  [   96/  126]
train() client id: f_00000-14-0 loss: 0.548545  [   32/  126]
train() client id: f_00000-14-1 loss: 0.856481  [   64/  126]
train() client id: f_00000-14-2 loss: 0.873504  [   96/  126]
train() client id: f_00000-15-0 loss: 0.667474  [   32/  126]
train() client id: f_00000-15-1 loss: 0.889204  [   64/  126]
train() client id: f_00000-15-2 loss: 0.720563  [   96/  126]
train() client id: f_00000-16-0 loss: 0.680174  [   32/  126]
train() client id: f_00000-16-1 loss: 0.659359  [   64/  126]
train() client id: f_00000-16-2 loss: 0.911632  [   96/  126]
train() client id: f_00000-17-0 loss: 0.603348  [   32/  126]
train() client id: f_00000-17-1 loss: 0.678064  [   64/  126]
train() client id: f_00000-17-2 loss: 0.925534  [   96/  126]
train() client id: f_00001-0-0 loss: 0.625156  [   32/  265]
train() client id: f_00001-0-1 loss: 0.462284  [   64/  265]
train() client id: f_00001-0-2 loss: 0.478160  [   96/  265]
train() client id: f_00001-0-3 loss: 0.454121  [  128/  265]
train() client id: f_00001-0-4 loss: 0.422297  [  160/  265]
train() client id: f_00001-0-5 loss: 0.494792  [  192/  265]
train() client id: f_00001-0-6 loss: 0.405684  [  224/  265]
train() client id: f_00001-0-7 loss: 0.419769  [  256/  265]
train() client id: f_00001-1-0 loss: 0.395994  [   32/  265]
train() client id: f_00001-1-1 loss: 0.374187  [   64/  265]
train() client id: f_00001-1-2 loss: 0.581054  [   96/  265]
train() client id: f_00001-1-3 loss: 0.432447  [  128/  265]
train() client id: f_00001-1-4 loss: 0.523143  [  160/  265]
train() client id: f_00001-1-5 loss: 0.431195  [  192/  265]
train() client id: f_00001-1-6 loss: 0.570208  [  224/  265]
train() client id: f_00001-1-7 loss: 0.397178  [  256/  265]
train() client id: f_00001-2-0 loss: 0.481110  [   32/  265]
train() client id: f_00001-2-1 loss: 0.523298  [   64/  265]
train() client id: f_00001-2-2 loss: 0.407477  [   96/  265]
train() client id: f_00001-2-3 loss: 0.367908  [  128/  265]
train() client id: f_00001-2-4 loss: 0.412488  [  160/  265]
train() client id: f_00001-2-5 loss: 0.442412  [  192/  265]
train() client id: f_00001-2-6 loss: 0.489128  [  224/  265]
train() client id: f_00001-2-7 loss: 0.482251  [  256/  265]
train() client id: f_00001-3-0 loss: 0.380082  [   32/  265]
train() client id: f_00001-3-1 loss: 0.440321  [   64/  265]
train() client id: f_00001-3-2 loss: 0.369336  [   96/  265]
train() client id: f_00001-3-3 loss: 0.572815  [  128/  265]
train() client id: f_00001-3-4 loss: 0.563434  [  160/  265]
train() client id: f_00001-3-5 loss: 0.414123  [  192/  265]
train() client id: f_00001-3-6 loss: 0.441315  [  224/  265]
train() client id: f_00001-3-7 loss: 0.363669  [  256/  265]
train() client id: f_00001-4-0 loss: 0.418631  [   32/  265]
train() client id: f_00001-4-1 loss: 0.336554  [   64/  265]
train() client id: f_00001-4-2 loss: 0.371749  [   96/  265]
train() client id: f_00001-4-3 loss: 0.621244  [  128/  265]
train() client id: f_00001-4-4 loss: 0.388254  [  160/  265]
train() client id: f_00001-4-5 loss: 0.425731  [  192/  265]
train() client id: f_00001-4-6 loss: 0.432513  [  224/  265]
train() client id: f_00001-4-7 loss: 0.558087  [  256/  265]
train() client id: f_00001-5-0 loss: 0.337665  [   32/  265]
train() client id: f_00001-5-1 loss: 0.517101  [   64/  265]
train() client id: f_00001-5-2 loss: 0.470846  [   96/  265]
train() client id: f_00001-5-3 loss: 0.515942  [  128/  265]
train() client id: f_00001-5-4 loss: 0.432273  [  160/  265]
train() client id: f_00001-5-5 loss: 0.498414  [  192/  265]
train() client id: f_00001-5-6 loss: 0.352099  [  224/  265]
train() client id: f_00001-5-7 loss: 0.432686  [  256/  265]
train() client id: f_00001-6-0 loss: 0.439254  [   32/  265]
train() client id: f_00001-6-1 loss: 0.394521  [   64/  265]
train() client id: f_00001-6-2 loss: 0.374641  [   96/  265]
train() client id: f_00001-6-3 loss: 0.456323  [  128/  265]
train() client id: f_00001-6-4 loss: 0.387727  [  160/  265]
train() client id: f_00001-6-5 loss: 0.388935  [  192/  265]
train() client id: f_00001-6-6 loss: 0.579219  [  224/  265]
train() client id: f_00001-6-7 loss: 0.541709  [  256/  265]
train() client id: f_00001-7-0 loss: 0.484596  [   32/  265]
train() client id: f_00001-7-1 loss: 0.539296  [   64/  265]
train() client id: f_00001-7-2 loss: 0.427359  [   96/  265]
train() client id: f_00001-7-3 loss: 0.468232  [  128/  265]
train() client id: f_00001-7-4 loss: 0.366642  [  160/  265]
train() client id: f_00001-7-5 loss: 0.398055  [  192/  265]
train() client id: f_00001-7-6 loss: 0.484585  [  224/  265]
train() client id: f_00001-7-7 loss: 0.344628  [  256/  265]
train() client id: f_00001-8-0 loss: 0.560512  [   32/  265]
train() client id: f_00001-8-1 loss: 0.547107  [   64/  265]
train() client id: f_00001-8-2 loss: 0.363522  [   96/  265]
train() client id: f_00001-8-3 loss: 0.418704  [  128/  265]
train() client id: f_00001-8-4 loss: 0.502523  [  160/  265]
train() client id: f_00001-8-5 loss: 0.429435  [  192/  265]
train() client id: f_00001-8-6 loss: 0.359912  [  224/  265]
train() client id: f_00001-8-7 loss: 0.380079  [  256/  265]
train() client id: f_00001-9-0 loss: 0.377040  [   32/  265]
train() client id: f_00001-9-1 loss: 0.567808  [   64/  265]
train() client id: f_00001-9-2 loss: 0.500874  [   96/  265]
train() client id: f_00001-9-3 loss: 0.528109  [  128/  265]
train() client id: f_00001-9-4 loss: 0.412102  [  160/  265]
train() client id: f_00001-9-5 loss: 0.350354  [  192/  265]
train() client id: f_00001-9-6 loss: 0.466072  [  224/  265]
train() client id: f_00001-9-7 loss: 0.346909  [  256/  265]
train() client id: f_00001-10-0 loss: 0.435989  [   32/  265]
train() client id: f_00001-10-1 loss: 0.349981  [   64/  265]
train() client id: f_00001-10-2 loss: 0.521319  [   96/  265]
train() client id: f_00001-10-3 loss: 0.448484  [  128/  265]
train() client id: f_00001-10-4 loss: 0.502147  [  160/  265]
train() client id: f_00001-10-5 loss: 0.366089  [  192/  265]
train() client id: f_00001-10-6 loss: 0.334189  [  224/  265]
train() client id: f_00001-10-7 loss: 0.588994  [  256/  265]
train() client id: f_00001-11-0 loss: 0.421406  [   32/  265]
train() client id: f_00001-11-1 loss: 0.430337  [   64/  265]
train() client id: f_00001-11-2 loss: 0.380183  [   96/  265]
train() client id: f_00001-11-3 loss: 0.498253  [  128/  265]
train() client id: f_00001-11-4 loss: 0.472946  [  160/  265]
train() client id: f_00001-11-5 loss: 0.333777  [  192/  265]
train() client id: f_00001-11-6 loss: 0.480176  [  224/  265]
train() client id: f_00001-11-7 loss: 0.524394  [  256/  265]
train() client id: f_00001-12-0 loss: 0.449509  [   32/  265]
train() client id: f_00001-12-1 loss: 0.668799  [   64/  265]
train() client id: f_00001-12-2 loss: 0.400374  [   96/  265]
train() client id: f_00001-12-3 loss: 0.429213  [  128/  265]
train() client id: f_00001-12-4 loss: 0.355315  [  160/  265]
train() client id: f_00001-12-5 loss: 0.385814  [  192/  265]
train() client id: f_00001-12-6 loss: 0.513717  [  224/  265]
train() client id: f_00001-12-7 loss: 0.341309  [  256/  265]
train() client id: f_00001-13-0 loss: 0.403677  [   32/  265]
train() client id: f_00001-13-1 loss: 0.348147  [   64/  265]
train() client id: f_00001-13-2 loss: 0.469164  [   96/  265]
train() client id: f_00001-13-3 loss: 0.448936  [  128/  265]
train() client id: f_00001-13-4 loss: 0.458453  [  160/  265]
train() client id: f_00001-13-5 loss: 0.467411  [  192/  265]
train() client id: f_00001-13-6 loss: 0.431481  [  224/  265]
train() client id: f_00001-13-7 loss: 0.532262  [  256/  265]
train() client id: f_00001-14-0 loss: 0.487898  [   32/  265]
train() client id: f_00001-14-1 loss: 0.350609  [   64/  265]
train() client id: f_00001-14-2 loss: 0.454711  [   96/  265]
train() client id: f_00001-14-3 loss: 0.511891  [  128/  265]
train() client id: f_00001-14-4 loss: 0.502656  [  160/  265]
train() client id: f_00001-14-5 loss: 0.484382  [  192/  265]
train() client id: f_00001-14-6 loss: 0.381081  [  224/  265]
train() client id: f_00001-14-7 loss: 0.388217  [  256/  265]
train() client id: f_00001-15-0 loss: 0.524535  [   32/  265]
train() client id: f_00001-15-1 loss: 0.549074  [   64/  265]
train() client id: f_00001-15-2 loss: 0.408377  [   96/  265]
train() client id: f_00001-15-3 loss: 0.414154  [  128/  265]
train() client id: f_00001-15-4 loss: 0.446764  [  160/  265]
train() client id: f_00001-15-5 loss: 0.411266  [  192/  265]
train() client id: f_00001-15-6 loss: 0.392111  [  224/  265]
train() client id: f_00001-15-7 loss: 0.416840  [  256/  265]
train() client id: f_00001-16-0 loss: 0.345051  [   32/  265]
train() client id: f_00001-16-1 loss: 0.355724  [   64/  265]
train() client id: f_00001-16-2 loss: 0.422294  [   96/  265]
train() client id: f_00001-16-3 loss: 0.526087  [  128/  265]
train() client id: f_00001-16-4 loss: 0.337336  [  160/  265]
train() client id: f_00001-16-5 loss: 0.447403  [  192/  265]
train() client id: f_00001-16-6 loss: 0.577409  [  224/  265]
train() client id: f_00001-16-7 loss: 0.553060  [  256/  265]
train() client id: f_00001-17-0 loss: 0.354494  [   32/  265]
train() client id: f_00001-17-1 loss: 0.449735  [   64/  265]
train() client id: f_00001-17-2 loss: 0.479487  [   96/  265]
train() client id: f_00001-17-3 loss: 0.479043  [  128/  265]
train() client id: f_00001-17-4 loss: 0.373414  [  160/  265]
train() client id: f_00001-17-5 loss: 0.477323  [  192/  265]
train() client id: f_00001-17-6 loss: 0.601179  [  224/  265]
train() client id: f_00001-17-7 loss: 0.358795  [  256/  265]
train() client id: f_00002-0-0 loss: 1.466196  [   32/  124]
train() client id: f_00002-0-1 loss: 1.102458  [   64/  124]
train() client id: f_00002-0-2 loss: 1.199424  [   96/  124]
train() client id: f_00002-1-0 loss: 1.042515  [   32/  124]
train() client id: f_00002-1-1 loss: 1.262190  [   64/  124]
train() client id: f_00002-1-2 loss: 1.182225  [   96/  124]
train() client id: f_00002-2-0 loss: 1.194208  [   32/  124]
train() client id: f_00002-2-1 loss: 0.956503  [   64/  124]
train() client id: f_00002-2-2 loss: 1.241408  [   96/  124]
train() client id: f_00002-3-0 loss: 1.094311  [   32/  124]
train() client id: f_00002-3-1 loss: 0.952843  [   64/  124]
train() client id: f_00002-3-2 loss: 1.221116  [   96/  124]
train() client id: f_00002-4-0 loss: 0.980089  [   32/  124]
train() client id: f_00002-4-1 loss: 0.952321  [   64/  124]
train() client id: f_00002-4-2 loss: 1.127602  [   96/  124]
train() client id: f_00002-5-0 loss: 1.060341  [   32/  124]
train() client id: f_00002-5-1 loss: 1.123140  [   64/  124]
train() client id: f_00002-5-2 loss: 0.942882  [   96/  124]
train() client id: f_00002-6-0 loss: 0.791347  [   32/  124]
train() client id: f_00002-6-1 loss: 1.196844  [   64/  124]
train() client id: f_00002-6-2 loss: 0.911934  [   96/  124]
train() client id: f_00002-7-0 loss: 0.982360  [   32/  124]
train() client id: f_00002-7-1 loss: 1.187303  [   64/  124]
train() client id: f_00002-7-2 loss: 0.657035  [   96/  124]
train() client id: f_00002-8-0 loss: 0.998520  [   32/  124]
train() client id: f_00002-8-1 loss: 0.832677  [   64/  124]
train() client id: f_00002-8-2 loss: 0.953830  [   96/  124]
train() client id: f_00002-9-0 loss: 1.022943  [   32/  124]
train() client id: f_00002-9-1 loss: 0.853996  [   64/  124]
train() client id: f_00002-9-2 loss: 0.801204  [   96/  124]
train() client id: f_00002-10-0 loss: 0.809085  [   32/  124]
train() client id: f_00002-10-1 loss: 0.825700  [   64/  124]
train() client id: f_00002-10-2 loss: 0.914485  [   96/  124]
train() client id: f_00002-11-0 loss: 0.788851  [   32/  124]
train() client id: f_00002-11-1 loss: 0.877229  [   64/  124]
train() client id: f_00002-11-2 loss: 1.054299  [   96/  124]
train() client id: f_00002-12-0 loss: 0.667036  [   32/  124]
train() client id: f_00002-12-1 loss: 0.948083  [   64/  124]
train() client id: f_00002-12-2 loss: 0.777272  [   96/  124]
train() client id: f_00002-13-0 loss: 0.994064  [   32/  124]
train() client id: f_00002-13-1 loss: 0.853637  [   64/  124]
train() client id: f_00002-13-2 loss: 0.820050  [   96/  124]
train() client id: f_00002-14-0 loss: 0.730951  [   32/  124]
train() client id: f_00002-14-1 loss: 0.988349  [   64/  124]
train() client id: f_00002-14-2 loss: 0.810590  [   96/  124]
train() client id: f_00002-15-0 loss: 0.688689  [   32/  124]
train() client id: f_00002-15-1 loss: 0.861960  [   64/  124]
train() client id: f_00002-15-2 loss: 1.029776  [   96/  124]
train() client id: f_00002-16-0 loss: 0.896720  [   32/  124]
train() client id: f_00002-16-1 loss: 0.794410  [   64/  124]
train() client id: f_00002-16-2 loss: 0.866663  [   96/  124]
train() client id: f_00002-17-0 loss: 0.691130  [   32/  124]
train() client id: f_00002-17-1 loss: 0.887525  [   64/  124]
train() client id: f_00002-17-2 loss: 0.757691  [   96/  124]
train() client id: f_00003-0-0 loss: 0.848936  [   32/   43]
train() client id: f_00003-1-0 loss: 0.691212  [   32/   43]
train() client id: f_00003-2-0 loss: 0.753819  [   32/   43]
train() client id: f_00003-3-0 loss: 0.807627  [   32/   43]
train() client id: f_00003-4-0 loss: 0.712848  [   32/   43]
train() client id: f_00003-5-0 loss: 0.730328  [   32/   43]
train() client id: f_00003-6-0 loss: 0.811779  [   32/   43]
train() client id: f_00003-7-0 loss: 0.831784  [   32/   43]
train() client id: f_00003-8-0 loss: 0.751445  [   32/   43]
train() client id: f_00003-9-0 loss: 0.916056  [   32/   43]
train() client id: f_00003-10-0 loss: 0.643555  [   32/   43]
train() client id: f_00003-11-0 loss: 0.721987  [   32/   43]
train() client id: f_00003-12-0 loss: 0.793256  [   32/   43]
train() client id: f_00003-13-0 loss: 0.833715  [   32/   43]
train() client id: f_00003-14-0 loss: 0.715457  [   32/   43]
train() client id: f_00003-15-0 loss: 0.764479  [   32/   43]
train() client id: f_00003-16-0 loss: 0.775529  [   32/   43]
train() client id: f_00003-17-0 loss: 0.856009  [   32/   43]
train() client id: f_00004-0-0 loss: 0.727534  [   32/  306]
train() client id: f_00004-0-1 loss: 0.871639  [   64/  306]
train() client id: f_00004-0-2 loss: 0.723180  [   96/  306]
train() client id: f_00004-0-3 loss: 0.686806  [  128/  306]
train() client id: f_00004-0-4 loss: 0.806651  [  160/  306]
train() client id: f_00004-0-5 loss: 0.789222  [  192/  306]
train() client id: f_00004-0-6 loss: 0.777469  [  224/  306]
train() client id: f_00004-0-7 loss: 0.877658  [  256/  306]
train() client id: f_00004-0-8 loss: 0.903609  [  288/  306]
train() client id: f_00004-1-0 loss: 0.867000  [   32/  306]
train() client id: f_00004-1-1 loss: 0.862875  [   64/  306]
train() client id: f_00004-1-2 loss: 0.776585  [   96/  306]
train() client id: f_00004-1-3 loss: 0.805552  [  128/  306]
train() client id: f_00004-1-4 loss: 0.784940  [  160/  306]
train() client id: f_00004-1-5 loss: 0.752580  [  192/  306]
train() client id: f_00004-1-6 loss: 0.898519  [  224/  306]
train() client id: f_00004-1-7 loss: 0.701081  [  256/  306]
train() client id: f_00004-1-8 loss: 0.784931  [  288/  306]
train() client id: f_00004-2-0 loss: 0.698450  [   32/  306]
train() client id: f_00004-2-1 loss: 0.744363  [   64/  306]
train() client id: f_00004-2-2 loss: 0.747520  [   96/  306]
train() client id: f_00004-2-3 loss: 0.884536  [  128/  306]
train() client id: f_00004-2-4 loss: 0.740919  [  160/  306]
train() client id: f_00004-2-5 loss: 0.685833  [  192/  306]
train() client id: f_00004-2-6 loss: 0.868595  [  224/  306]
train() client id: f_00004-2-7 loss: 0.898543  [  256/  306]
train() client id: f_00004-2-8 loss: 0.786566  [  288/  306]
train() client id: f_00004-3-0 loss: 0.850764  [   32/  306]
train() client id: f_00004-3-1 loss: 0.719253  [   64/  306]
train() client id: f_00004-3-2 loss: 0.626565  [   96/  306]
train() client id: f_00004-3-3 loss: 0.733797  [  128/  306]
train() client id: f_00004-3-4 loss: 0.864514  [  160/  306]
train() client id: f_00004-3-5 loss: 0.758707  [  192/  306]
train() client id: f_00004-3-6 loss: 0.751589  [  224/  306]
train() client id: f_00004-3-7 loss: 0.875133  [  256/  306]
train() client id: f_00004-3-8 loss: 1.065176  [  288/  306]
train() client id: f_00004-4-0 loss: 0.803147  [   32/  306]
train() client id: f_00004-4-1 loss: 0.817045  [   64/  306]
train() client id: f_00004-4-2 loss: 0.788669  [   96/  306]
train() client id: f_00004-4-3 loss: 0.875893  [  128/  306]
train() client id: f_00004-4-4 loss: 0.750169  [  160/  306]
train() client id: f_00004-4-5 loss: 0.760511  [  192/  306]
train() client id: f_00004-4-6 loss: 0.662567  [  224/  306]
train() client id: f_00004-4-7 loss: 0.785990  [  256/  306]
train() client id: f_00004-4-8 loss: 0.850269  [  288/  306]
train() client id: f_00004-5-0 loss: 0.760995  [   32/  306]
train() client id: f_00004-5-1 loss: 0.707636  [   64/  306]
train() client id: f_00004-5-2 loss: 0.805084  [   96/  306]
train() client id: f_00004-5-3 loss: 0.923580  [  128/  306]
train() client id: f_00004-5-4 loss: 0.845987  [  160/  306]
train() client id: f_00004-5-5 loss: 0.691556  [  192/  306]
train() client id: f_00004-5-6 loss: 0.798365  [  224/  306]
train() client id: f_00004-5-7 loss: 0.700841  [  256/  306]
train() client id: f_00004-5-8 loss: 0.871338  [  288/  306]
train() client id: f_00004-6-0 loss: 0.719159  [   32/  306]
train() client id: f_00004-6-1 loss: 0.800150  [   64/  306]
train() client id: f_00004-6-2 loss: 0.636516  [   96/  306]
train() client id: f_00004-6-3 loss: 0.883679  [  128/  306]
train() client id: f_00004-6-4 loss: 0.873927  [  160/  306]
train() client id: f_00004-6-5 loss: 0.778452  [  192/  306]
train() client id: f_00004-6-6 loss: 0.857637  [  224/  306]
train() client id: f_00004-6-7 loss: 0.634286  [  256/  306]
train() client id: f_00004-6-8 loss: 0.871151  [  288/  306]
train() client id: f_00004-7-0 loss: 0.768689  [   32/  306]
train() client id: f_00004-7-1 loss: 0.642784  [   64/  306]
train() client id: f_00004-7-2 loss: 0.763410  [   96/  306]
train() client id: f_00004-7-3 loss: 0.964188  [  128/  306]
train() client id: f_00004-7-4 loss: 0.850892  [  160/  306]
train() client id: f_00004-7-5 loss: 0.745467  [  192/  306]
train() client id: f_00004-7-6 loss: 0.838053  [  224/  306]
train() client id: f_00004-7-7 loss: 0.773831  [  256/  306]
train() client id: f_00004-7-8 loss: 0.771747  [  288/  306]
train() client id: f_00004-8-0 loss: 0.804075  [   32/  306]
train() client id: f_00004-8-1 loss: 0.749372  [   64/  306]
train() client id: f_00004-8-2 loss: 0.797102  [   96/  306]
train() client id: f_00004-8-3 loss: 0.845897  [  128/  306]
train() client id: f_00004-8-4 loss: 0.807981  [  160/  306]
train() client id: f_00004-8-5 loss: 0.708054  [  192/  306]
train() client id: f_00004-8-6 loss: 0.763400  [  224/  306]
train() client id: f_00004-8-7 loss: 0.794282  [  256/  306]
train() client id: f_00004-8-8 loss: 0.722124  [  288/  306]
train() client id: f_00004-9-0 loss: 0.674776  [   32/  306]
train() client id: f_00004-9-1 loss: 0.839240  [   64/  306]
train() client id: f_00004-9-2 loss: 0.813028  [   96/  306]
train() client id: f_00004-9-3 loss: 0.869419  [  128/  306]
train() client id: f_00004-9-4 loss: 0.790279  [  160/  306]
train() client id: f_00004-9-5 loss: 0.928842  [  192/  306]
train() client id: f_00004-9-6 loss: 0.765198  [  224/  306]
train() client id: f_00004-9-7 loss: 0.628508  [  256/  306]
train() client id: f_00004-9-8 loss: 0.808109  [  288/  306]
train() client id: f_00004-10-0 loss: 0.861429  [   32/  306]
train() client id: f_00004-10-1 loss: 0.688024  [   64/  306]
train() client id: f_00004-10-2 loss: 0.861893  [   96/  306]
train() client id: f_00004-10-3 loss: 0.681288  [  128/  306]
train() client id: f_00004-10-4 loss: 0.844183  [  160/  306]
train() client id: f_00004-10-5 loss: 0.844734  [  192/  306]
train() client id: f_00004-10-6 loss: 0.843040  [  224/  306]
train() client id: f_00004-10-7 loss: 0.684671  [  256/  306]
train() client id: f_00004-10-8 loss: 0.778600  [  288/  306]
train() client id: f_00004-11-0 loss: 0.738696  [   32/  306]
train() client id: f_00004-11-1 loss: 0.907662  [   64/  306]
train() client id: f_00004-11-2 loss: 0.882236  [   96/  306]
train() client id: f_00004-11-3 loss: 0.974718  [  128/  306]
train() client id: f_00004-11-4 loss: 0.743218  [  160/  306]
train() client id: f_00004-11-5 loss: 0.769381  [  192/  306]
train() client id: f_00004-11-6 loss: 0.789581  [  224/  306]
train() client id: f_00004-11-7 loss: 0.709225  [  256/  306]
train() client id: f_00004-11-8 loss: 0.704471  [  288/  306]
train() client id: f_00004-12-0 loss: 0.782845  [   32/  306]
train() client id: f_00004-12-1 loss: 0.732489  [   64/  306]
train() client id: f_00004-12-2 loss: 0.829875  [   96/  306]
train() client id: f_00004-12-3 loss: 0.714540  [  128/  306]
train() client id: f_00004-12-4 loss: 0.900158  [  160/  306]
train() client id: f_00004-12-5 loss: 0.676582  [  192/  306]
train() client id: f_00004-12-6 loss: 0.728555  [  224/  306]
train() client id: f_00004-12-7 loss: 0.823129  [  256/  306]
train() client id: f_00004-12-8 loss: 0.926833  [  288/  306]
train() client id: f_00004-13-0 loss: 0.745771  [   32/  306]
train() client id: f_00004-13-1 loss: 0.752510  [   64/  306]
train() client id: f_00004-13-2 loss: 0.927547  [   96/  306]
train() client id: f_00004-13-3 loss: 0.739863  [  128/  306]
train() client id: f_00004-13-4 loss: 0.859033  [  160/  306]
train() client id: f_00004-13-5 loss: 0.849099  [  192/  306]
train() client id: f_00004-13-6 loss: 0.794263  [  224/  306]
train() client id: f_00004-13-7 loss: 0.754847  [  256/  306]
train() client id: f_00004-13-8 loss: 0.727845  [  288/  306]
train() client id: f_00004-14-0 loss: 0.759959  [   32/  306]
train() client id: f_00004-14-1 loss: 0.821266  [   64/  306]
train() client id: f_00004-14-2 loss: 0.797287  [   96/  306]
train() client id: f_00004-14-3 loss: 0.691340  [  128/  306]
train() client id: f_00004-14-4 loss: 0.760257  [  160/  306]
train() client id: f_00004-14-5 loss: 0.781922  [  192/  306]
train() client id: f_00004-14-6 loss: 0.764600  [  224/  306]
train() client id: f_00004-14-7 loss: 0.723826  [  256/  306]
train() client id: f_00004-14-8 loss: 0.913772  [  288/  306]
train() client id: f_00004-15-0 loss: 0.740604  [   32/  306]
train() client id: f_00004-15-1 loss: 0.814834  [   64/  306]
train() client id: f_00004-15-2 loss: 0.783986  [   96/  306]
train() client id: f_00004-15-3 loss: 0.832484  [  128/  306]
train() client id: f_00004-15-4 loss: 0.680132  [  160/  306]
train() client id: f_00004-15-5 loss: 0.796509  [  192/  306]
train() client id: f_00004-15-6 loss: 0.851267  [  224/  306]
train() client id: f_00004-15-7 loss: 0.762335  [  256/  306]
train() client id: f_00004-15-8 loss: 0.871828  [  288/  306]
train() client id: f_00004-16-0 loss: 0.680101  [   32/  306]
train() client id: f_00004-16-1 loss: 0.821323  [   64/  306]
train() client id: f_00004-16-2 loss: 0.814658  [   96/  306]
train() client id: f_00004-16-3 loss: 0.678413  [  128/  306]
train() client id: f_00004-16-4 loss: 0.921853  [  160/  306]
train() client id: f_00004-16-5 loss: 0.757563  [  192/  306]
train() client id: f_00004-16-6 loss: 0.885692  [  224/  306]
train() client id: f_00004-16-7 loss: 0.843755  [  256/  306]
train() client id: f_00004-16-8 loss: 0.695084  [  288/  306]
train() client id: f_00004-17-0 loss: 0.740250  [   32/  306]
train() client id: f_00004-17-1 loss: 0.808465  [   64/  306]
train() client id: f_00004-17-2 loss: 0.867948  [   96/  306]
train() client id: f_00004-17-3 loss: 0.817255  [  128/  306]
train() client id: f_00004-17-4 loss: 0.809600  [  160/  306]
train() client id: f_00004-17-5 loss: 0.840595  [  192/  306]
train() client id: f_00004-17-6 loss: 0.784110  [  224/  306]
train() client id: f_00004-17-7 loss: 0.695984  [  256/  306]
train() client id: f_00004-17-8 loss: 0.873091  [  288/  306]
train() client id: f_00005-0-0 loss: 0.665212  [   32/  146]
train() client id: f_00005-0-1 loss: 0.574091  [   64/  146]
train() client id: f_00005-0-2 loss: 0.590197  [   96/  146]
train() client id: f_00005-0-3 loss: 0.637257  [  128/  146]
train() client id: f_00005-1-0 loss: 0.475905  [   32/  146]
train() client id: f_00005-1-1 loss: 0.556307  [   64/  146]
train() client id: f_00005-1-2 loss: 0.557658  [   96/  146]
train() client id: f_00005-1-3 loss: 0.877135  [  128/  146]
train() client id: f_00005-2-0 loss: 0.734151  [   32/  146]
train() client id: f_00005-2-1 loss: 0.578890  [   64/  146]
train() client id: f_00005-2-2 loss: 0.507574  [   96/  146]
train() client id: f_00005-2-3 loss: 0.652966  [  128/  146]
train() client id: f_00005-3-0 loss: 0.586675  [   32/  146]
train() client id: f_00005-3-1 loss: 0.687511  [   64/  146]
train() client id: f_00005-3-2 loss: 0.718723  [   96/  146]
train() client id: f_00005-3-3 loss: 0.506951  [  128/  146]
train() client id: f_00005-4-0 loss: 0.720368  [   32/  146]
train() client id: f_00005-4-1 loss: 0.642929  [   64/  146]
train() client id: f_00005-4-2 loss: 0.475461  [   96/  146]
train() client id: f_00005-4-3 loss: 0.591784  [  128/  146]
train() client id: f_00005-5-0 loss: 0.702768  [   32/  146]
train() client id: f_00005-5-1 loss: 0.742571  [   64/  146]
train() client id: f_00005-5-2 loss: 0.485600  [   96/  146]
train() client id: f_00005-5-3 loss: 0.804710  [  128/  146]
train() client id: f_00005-6-0 loss: 0.798786  [   32/  146]
train() client id: f_00005-6-1 loss: 0.733813  [   64/  146]
train() client id: f_00005-6-2 loss: 0.652140  [   96/  146]
train() client id: f_00005-6-3 loss: 0.414607  [  128/  146]
train() client id: f_00005-7-0 loss: 0.646865  [   32/  146]
train() client id: f_00005-7-1 loss: 0.860106  [   64/  146]
train() client id: f_00005-7-2 loss: 0.538155  [   96/  146]
train() client id: f_00005-7-3 loss: 0.407604  [  128/  146]
train() client id: f_00005-8-0 loss: 0.385490  [   32/  146]
train() client id: f_00005-8-1 loss: 0.745928  [   64/  146]
train() client id: f_00005-8-2 loss: 0.591590  [   96/  146]
train() client id: f_00005-8-3 loss: 0.792829  [  128/  146]
train() client id: f_00005-9-0 loss: 0.700228  [   32/  146]
train() client id: f_00005-9-1 loss: 0.634727  [   64/  146]
train() client id: f_00005-9-2 loss: 0.437400  [   96/  146]
train() client id: f_00005-9-3 loss: 0.780312  [  128/  146]
train() client id: f_00005-10-0 loss: 0.387928  [   32/  146]
train() client id: f_00005-10-1 loss: 0.489974  [   64/  146]
train() client id: f_00005-10-2 loss: 0.939920  [   96/  146]
train() client id: f_00005-10-3 loss: 0.566946  [  128/  146]
train() client id: f_00005-11-0 loss: 0.426020  [   32/  146]
train() client id: f_00005-11-1 loss: 0.661194  [   64/  146]
train() client id: f_00005-11-2 loss: 0.599920  [   96/  146]
train() client id: f_00005-11-3 loss: 0.703650  [  128/  146]
train() client id: f_00005-12-0 loss: 0.709725  [   32/  146]
train() client id: f_00005-12-1 loss: 0.824264  [   64/  146]
train() client id: f_00005-12-2 loss: 0.842684  [   96/  146]
train() client id: f_00005-12-3 loss: 0.357034  [  128/  146]
train() client id: f_00005-13-0 loss: 1.065083  [   32/  146]
train() client id: f_00005-13-1 loss: 0.483667  [   64/  146]
train() client id: f_00005-13-2 loss: 0.550899  [   96/  146]
train() client id: f_00005-13-3 loss: 0.462700  [  128/  146]
train() client id: f_00005-14-0 loss: 0.926139  [   32/  146]
train() client id: f_00005-14-1 loss: 0.450585  [   64/  146]
train() client id: f_00005-14-2 loss: 0.495155  [   96/  146]
train() client id: f_00005-14-3 loss: 0.844466  [  128/  146]
train() client id: f_00005-15-0 loss: 0.727087  [   32/  146]
train() client id: f_00005-15-1 loss: 0.513771  [   64/  146]
train() client id: f_00005-15-2 loss: 0.413517  [   96/  146]
train() client id: f_00005-15-3 loss: 0.766466  [  128/  146]
train() client id: f_00005-16-0 loss: 0.709336  [   32/  146]
train() client id: f_00005-16-1 loss: 0.482521  [   64/  146]
train() client id: f_00005-16-2 loss: 0.450752  [   96/  146]
train() client id: f_00005-16-3 loss: 0.678174  [  128/  146]
train() client id: f_00005-17-0 loss: 0.544638  [   32/  146]
train() client id: f_00005-17-1 loss: 0.560134  [   64/  146]
train() client id: f_00005-17-2 loss: 0.679346  [   96/  146]
train() client id: f_00005-17-3 loss: 0.652114  [  128/  146]
train() client id: f_00006-0-0 loss: 0.490151  [   32/   54]
train() client id: f_00006-1-0 loss: 0.504982  [   32/   54]
train() client id: f_00006-2-0 loss: 0.544434  [   32/   54]
train() client id: f_00006-3-0 loss: 0.566397  [   32/   54]
train() client id: f_00006-4-0 loss: 0.535575  [   32/   54]
train() client id: f_00006-5-0 loss: 0.517387  [   32/   54]
train() client id: f_00006-6-0 loss: 0.520283  [   32/   54]
train() client id: f_00006-7-0 loss: 0.487552  [   32/   54]
train() client id: f_00006-8-0 loss: 0.524571  [   32/   54]
train() client id: f_00006-9-0 loss: 0.564551  [   32/   54]
train() client id: f_00006-10-0 loss: 0.548556  [   32/   54]
train() client id: f_00006-11-0 loss: 0.513793  [   32/   54]
train() client id: f_00006-12-0 loss: 0.515298  [   32/   54]
train() client id: f_00006-13-0 loss: 0.479632  [   32/   54]
train() client id: f_00006-14-0 loss: 0.549517  [   32/   54]
train() client id: f_00006-15-0 loss: 0.445398  [   32/   54]
train() client id: f_00006-16-0 loss: 0.528089  [   32/   54]
train() client id: f_00006-17-0 loss: 0.556951  [   32/   54]
train() client id: f_00007-0-0 loss: 0.730230  [   32/  179]
train() client id: f_00007-0-1 loss: 0.756972  [   64/  179]
train() client id: f_00007-0-2 loss: 0.549477  [   96/  179]
train() client id: f_00007-0-3 loss: 0.552269  [  128/  179]
train() client id: f_00007-0-4 loss: 0.605306  [  160/  179]
train() client id: f_00007-1-0 loss: 0.625963  [   32/  179]
train() client id: f_00007-1-1 loss: 0.630263  [   64/  179]
train() client id: f_00007-1-2 loss: 0.434004  [   96/  179]
train() client id: f_00007-1-3 loss: 0.577848  [  128/  179]
train() client id: f_00007-1-4 loss: 0.706517  [  160/  179]
train() client id: f_00007-2-0 loss: 0.450200  [   32/  179]
train() client id: f_00007-2-1 loss: 0.740333  [   64/  179]
train() client id: f_00007-2-2 loss: 0.635799  [   96/  179]
train() client id: f_00007-2-3 loss: 0.482849  [  128/  179]
train() client id: f_00007-2-4 loss: 0.686931  [  160/  179]
train() client id: f_00007-3-0 loss: 0.556047  [   32/  179]
train() client id: f_00007-3-1 loss: 0.614955  [   64/  179]
train() client id: f_00007-3-2 loss: 0.554144  [   96/  179]
train() client id: f_00007-3-3 loss: 0.648818  [  128/  179]
train() client id: f_00007-3-4 loss: 0.452370  [  160/  179]
train() client id: f_00007-4-0 loss: 0.406502  [   32/  179]
train() client id: f_00007-4-1 loss: 0.566336  [   64/  179]
train() client id: f_00007-4-2 loss: 0.560230  [   96/  179]
train() client id: f_00007-4-3 loss: 0.527775  [  128/  179]
train() client id: f_00007-4-4 loss: 0.892260  [  160/  179]
train() client id: f_00007-5-0 loss: 0.526938  [   32/  179]
train() client id: f_00007-5-1 loss: 0.406309  [   64/  179]
train() client id: f_00007-5-2 loss: 0.476335  [   96/  179]
train() client id: f_00007-5-3 loss: 0.725713  [  128/  179]
train() client id: f_00007-5-4 loss: 0.674847  [  160/  179]
train() client id: f_00007-6-0 loss: 0.634910  [   32/  179]
train() client id: f_00007-6-1 loss: 0.493128  [   64/  179]
train() client id: f_00007-6-2 loss: 0.628145  [   96/  179]
train() client id: f_00007-6-3 loss: 0.369497  [  128/  179]
train() client id: f_00007-6-4 loss: 0.703590  [  160/  179]
train() client id: f_00007-7-0 loss: 0.492539  [   32/  179]
train() client id: f_00007-7-1 loss: 0.533619  [   64/  179]
train() client id: f_00007-7-2 loss: 0.651173  [   96/  179]
train() client id: f_00007-7-3 loss: 0.504346  [  128/  179]
train() client id: f_00007-7-4 loss: 0.679309  [  160/  179]
train() client id: f_00007-8-0 loss: 0.469931  [   32/  179]
train() client id: f_00007-8-1 loss: 0.857411  [   64/  179]
train() client id: f_00007-8-2 loss: 0.547008  [   96/  179]
train() client id: f_00007-8-3 loss: 0.591472  [  128/  179]
train() client id: f_00007-8-4 loss: 0.404784  [  160/  179]
train() client id: f_00007-9-0 loss: 0.498065  [   32/  179]
train() client id: f_00007-9-1 loss: 0.515073  [   64/  179]
train() client id: f_00007-9-2 loss: 0.557905  [   96/  179]
train() client id: f_00007-9-3 loss: 0.575055  [  128/  179]
train() client id: f_00007-9-4 loss: 0.573652  [  160/  179]
train() client id: f_00007-10-0 loss: 0.583514  [   32/  179]
train() client id: f_00007-10-1 loss: 0.491673  [   64/  179]
train() client id: f_00007-10-2 loss: 0.379816  [   96/  179]
train() client id: f_00007-10-3 loss: 0.642500  [  128/  179]
train() client id: f_00007-10-4 loss: 0.456112  [  160/  179]
train() client id: f_00007-11-0 loss: 0.399365  [   32/  179]
train() client id: f_00007-11-1 loss: 0.508816  [   64/  179]
train() client id: f_00007-11-2 loss: 0.737930  [   96/  179]
train() client id: f_00007-11-3 loss: 0.399526  [  128/  179]
train() client id: f_00007-11-4 loss: 0.592102  [  160/  179]
train() client id: f_00007-12-0 loss: 0.666496  [   32/  179]
train() client id: f_00007-12-1 loss: 0.507588  [   64/  179]
train() client id: f_00007-12-2 loss: 0.491877  [   96/  179]
train() client id: f_00007-12-3 loss: 0.477307  [  128/  179]
train() client id: f_00007-12-4 loss: 0.385401  [  160/  179]
train() client id: f_00007-13-0 loss: 0.515093  [   32/  179]
train() client id: f_00007-13-1 loss: 0.368857  [   64/  179]
train() client id: f_00007-13-2 loss: 0.556608  [   96/  179]
train() client id: f_00007-13-3 loss: 0.525463  [  128/  179]
train() client id: f_00007-13-4 loss: 0.492216  [  160/  179]
train() client id: f_00007-14-0 loss: 0.474557  [   32/  179]
train() client id: f_00007-14-1 loss: 0.396231  [   64/  179]
train() client id: f_00007-14-2 loss: 0.589851  [   96/  179]
train() client id: f_00007-14-3 loss: 0.383421  [  128/  179]
train() client id: f_00007-14-4 loss: 0.784456  [  160/  179]
train() client id: f_00007-15-0 loss: 0.760021  [   32/  179]
train() client id: f_00007-15-1 loss: 0.383074  [   64/  179]
train() client id: f_00007-15-2 loss: 0.513125  [   96/  179]
train() client id: f_00007-15-3 loss: 0.374450  [  128/  179]
train() client id: f_00007-15-4 loss: 0.675939  [  160/  179]
train() client id: f_00007-16-0 loss: 0.374799  [   32/  179]
train() client id: f_00007-16-1 loss: 0.374378  [   64/  179]
train() client id: f_00007-16-2 loss: 0.677593  [   96/  179]
train() client id: f_00007-16-3 loss: 0.663519  [  128/  179]
train() client id: f_00007-16-4 loss: 0.564415  [  160/  179]
train() client id: f_00007-17-0 loss: 0.480696  [   32/  179]
train() client id: f_00007-17-1 loss: 0.683509  [   64/  179]
train() client id: f_00007-17-2 loss: 0.377858  [   96/  179]
train() client id: f_00007-17-3 loss: 0.469763  [  128/  179]
train() client id: f_00007-17-4 loss: 0.417657  [  160/  179]
train() client id: f_00008-0-0 loss: 0.719488  [   32/  130]
train() client id: f_00008-0-1 loss: 0.590895  [   64/  130]
train() client id: f_00008-0-2 loss: 0.562636  [   96/  130]
train() client id: f_00008-0-3 loss: 0.728324  [  128/  130]
train() client id: f_00008-1-0 loss: 0.587908  [   32/  130]
train() client id: f_00008-1-1 loss: 0.709259  [   64/  130]
train() client id: f_00008-1-2 loss: 0.558236  [   96/  130]
train() client id: f_00008-1-3 loss: 0.763270  [  128/  130]
train() client id: f_00008-2-0 loss: 0.573979  [   32/  130]
train() client id: f_00008-2-1 loss: 0.772378  [   64/  130]
train() client id: f_00008-2-2 loss: 0.671004  [   96/  130]
train() client id: f_00008-2-3 loss: 0.606886  [  128/  130]
train() client id: f_00008-3-0 loss: 0.640703  [   32/  130]
train() client id: f_00008-3-1 loss: 0.517389  [   64/  130]
train() client id: f_00008-3-2 loss: 0.672434  [   96/  130]
train() client id: f_00008-3-3 loss: 0.786171  [  128/  130]
train() client id: f_00008-4-0 loss: 0.643918  [   32/  130]
train() client id: f_00008-4-1 loss: 0.690788  [   64/  130]
train() client id: f_00008-4-2 loss: 0.528457  [   96/  130]
train() client id: f_00008-4-3 loss: 0.750913  [  128/  130]
train() client id: f_00008-5-0 loss: 0.650734  [   32/  130]
train() client id: f_00008-5-1 loss: 0.644025  [   64/  130]
train() client id: f_00008-5-2 loss: 0.671047  [   96/  130]
train() client id: f_00008-5-3 loss: 0.651107  [  128/  130]
train() client id: f_00008-6-0 loss: 0.494650  [   32/  130]
train() client id: f_00008-6-1 loss: 0.759700  [   64/  130]
train() client id: f_00008-6-2 loss: 0.723397  [   96/  130]
train() client id: f_00008-6-3 loss: 0.636751  [  128/  130]
train() client id: f_00008-7-0 loss: 0.565677  [   32/  130]
train() client id: f_00008-7-1 loss: 0.681029  [   64/  130]
train() client id: f_00008-7-2 loss: 0.651000  [   96/  130]
train() client id: f_00008-7-3 loss: 0.643860  [  128/  130]
train() client id: f_00008-8-0 loss: 0.609147  [   32/  130]
train() client id: f_00008-8-1 loss: 0.650568  [   64/  130]
train() client id: f_00008-8-2 loss: 0.753798  [   96/  130]
train() client id: f_00008-8-3 loss: 0.585949  [  128/  130]
train() client id: f_00008-9-0 loss: 0.673685  [   32/  130]
train() client id: f_00008-9-1 loss: 0.626198  [   64/  130]
train() client id: f_00008-9-2 loss: 0.697868  [   96/  130]
train() client id: f_00008-9-3 loss: 0.594031  [  128/  130]
train() client id: f_00008-10-0 loss: 0.571456  [   32/  130]
train() client id: f_00008-10-1 loss: 0.631181  [   64/  130]
train() client id: f_00008-10-2 loss: 0.716044  [   96/  130]
train() client id: f_00008-10-3 loss: 0.589396  [  128/  130]
train() client id: f_00008-11-0 loss: 0.649540  [   32/  130]
train() client id: f_00008-11-1 loss: 0.668134  [   64/  130]
train() client id: f_00008-11-2 loss: 0.759136  [   96/  130]
train() client id: f_00008-11-3 loss: 0.510393  [  128/  130]
train() client id: f_00008-12-0 loss: 0.705618  [   32/  130]
train() client id: f_00008-12-1 loss: 0.615234  [   64/  130]
train() client id: f_00008-12-2 loss: 0.652906  [   96/  130]
train() client id: f_00008-12-3 loss: 0.629572  [  128/  130]
train() client id: f_00008-13-0 loss: 0.669867  [   32/  130]
train() client id: f_00008-13-1 loss: 0.663563  [   64/  130]
train() client id: f_00008-13-2 loss: 0.607603  [   96/  130]
train() client id: f_00008-13-3 loss: 0.635972  [  128/  130]
train() client id: f_00008-14-0 loss: 0.632551  [   32/  130]
train() client id: f_00008-14-1 loss: 0.628197  [   64/  130]
train() client id: f_00008-14-2 loss: 0.580054  [   96/  130]
train() client id: f_00008-14-3 loss: 0.683008  [  128/  130]
train() client id: f_00008-15-0 loss: 0.613687  [   32/  130]
train() client id: f_00008-15-1 loss: 0.618902  [   64/  130]
train() client id: f_00008-15-2 loss: 0.701365  [   96/  130]
train() client id: f_00008-15-3 loss: 0.652138  [  128/  130]
train() client id: f_00008-16-0 loss: 0.638578  [   32/  130]
train() client id: f_00008-16-1 loss: 0.544490  [   64/  130]
train() client id: f_00008-16-2 loss: 0.747034  [   96/  130]
train() client id: f_00008-16-3 loss: 0.654192  [  128/  130]
train() client id: f_00008-17-0 loss: 0.597568  [   32/  130]
train() client id: f_00008-17-1 loss: 0.665250  [   64/  130]
train() client id: f_00008-17-2 loss: 0.706896  [   96/  130]
train() client id: f_00008-17-3 loss: 0.588923  [  128/  130]
train() client id: f_00009-0-0 loss: 1.078222  [   32/  118]
train() client id: f_00009-0-1 loss: 1.182421  [   64/  118]
train() client id: f_00009-0-2 loss: 1.197716  [   96/  118]
train() client id: f_00009-1-0 loss: 1.069953  [   32/  118]
train() client id: f_00009-1-1 loss: 1.144486  [   64/  118]
train() client id: f_00009-1-2 loss: 1.003205  [   96/  118]
train() client id: f_00009-2-0 loss: 1.024688  [   32/  118]
train() client id: f_00009-2-1 loss: 1.082349  [   64/  118]
train() client id: f_00009-2-2 loss: 0.956163  [   96/  118]
train() client id: f_00009-3-0 loss: 0.973483  [   32/  118]
train() client id: f_00009-3-1 loss: 0.900207  [   64/  118]
train() client id: f_00009-3-2 loss: 0.967411  [   96/  118]
train() client id: f_00009-4-0 loss: 0.865823  [   32/  118]
train() client id: f_00009-4-1 loss: 0.967550  [   64/  118]
train() client id: f_00009-4-2 loss: 0.815849  [   96/  118]
train() client id: f_00009-5-0 loss: 0.968780  [   32/  118]
train() client id: f_00009-5-1 loss: 0.967965  [   64/  118]
train() client id: f_00009-5-2 loss: 0.725355  [   96/  118]
train() client id: f_00009-6-0 loss: 0.920423  [   32/  118]
train() client id: f_00009-6-1 loss: 0.914031  [   64/  118]
train() client id: f_00009-6-2 loss: 0.680153  [   96/  118]
train() client id: f_00009-7-0 loss: 1.022242  [   32/  118]
train() client id: f_00009-7-1 loss: 0.833234  [   64/  118]
train() client id: f_00009-7-2 loss: 0.641872  [   96/  118]
train() client id: f_00009-8-0 loss: 0.777076  [   32/  118]
train() client id: f_00009-8-1 loss: 0.749338  [   64/  118]
train() client id: f_00009-8-2 loss: 0.679089  [   96/  118]
train() client id: f_00009-9-0 loss: 0.688701  [   32/  118]
train() client id: f_00009-9-1 loss: 0.856124  [   64/  118]
train() client id: f_00009-9-2 loss: 0.856480  [   96/  118]
train() client id: f_00009-10-0 loss: 1.046182  [   32/  118]
train() client id: f_00009-10-1 loss: 0.665816  [   64/  118]
train() client id: f_00009-10-2 loss: 0.607963  [   96/  118]
train() client id: f_00009-11-0 loss: 0.728355  [   32/  118]
train() client id: f_00009-11-1 loss: 0.835073  [   64/  118]
train() client id: f_00009-11-2 loss: 0.758889  [   96/  118]
train() client id: f_00009-12-0 loss: 0.857403  [   32/  118]
train() client id: f_00009-12-1 loss: 0.766483  [   64/  118]
train() client id: f_00009-12-2 loss: 0.726790  [   96/  118]
train() client id: f_00009-13-0 loss: 0.736825  [   32/  118]
train() client id: f_00009-13-1 loss: 0.673602  [   64/  118]
train() client id: f_00009-13-2 loss: 0.818311  [   96/  118]
train() client id: f_00009-14-0 loss: 0.778058  [   32/  118]
train() client id: f_00009-14-1 loss: 0.697431  [   64/  118]
train() client id: f_00009-14-2 loss: 0.740857  [   96/  118]
train() client id: f_00009-15-0 loss: 0.931540  [   32/  118]
train() client id: f_00009-15-1 loss: 0.664699  [   64/  118]
train() client id: f_00009-15-2 loss: 0.671986  [   96/  118]
train() client id: f_00009-16-0 loss: 0.614142  [   32/  118]
train() client id: f_00009-16-1 loss: 0.754221  [   64/  118]
train() client id: f_00009-16-2 loss: 0.620649  [   96/  118]
train() client id: f_00009-17-0 loss: 0.723854  [   32/  118]
train() client id: f_00009-17-1 loss: 0.826546  [   64/  118]
train() client id: f_00009-17-2 loss: 0.670469  [   96/  118]
At round 60 accuracy: 0.649867374005305
At round 60 training accuracy: 0.5922199865861838
At round 60 training loss: 0.8320546589231275
update_location
xs = 8.927491 421.223621 5.882650 0.934260 -337.581990 -185.230757 -145.849135 -5.143845 -360.120581 20.134486 
ys = -412.390647 7.291448 310.684448 -132.290817 -9.642386 0.794442 -1.381692 306.628436 25.881276 -847.232496 
xs mean: -57.68237997052123
ys mean: -75.16579882624052
dists_uav = 424.435797 432.992499 326.434422 165.836465 352.213820 210.501935 176.844223 322.563880 374.642060 853.351217 
uav_gains = -122.237437 -122.503749 -117.734068 -105.508665 -119.292073 -108.354051 -106.226789 -117.466980 -120.383178 -130.249843 
uav_gains_db_mean: -116.99568327600483
dists_bs = 615.513683 619.361578 226.116790 354.091104 246.019743 174.505716 178.774367 214.888712 237.709956 1040.690513 
bs_gains = -117.665715 -117.741498 -105.488378 -110.942280 -106.514218 -102.337730 -102.631606 -104.869040 -106.096386 -124.052064 
bs_gains_db_mean: -109.83389141150931
Round 61
-------------------------------
ene_coms = [0.03811389 0.03968966 0.00815206 0.00816086 0.00862128 0.00960685
 0.00848463 0.00789202 0.008424   0.05085116]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.07775695 6.17516023 2.81281035 1.01467267 6.85666979 3.31191964
 1.26125629 4.03189792 2.94801232 2.99391469]
obj_prev = 34.48407086054696
eta_min = 5.71586543434269e-32	eta_max = 0.861425198099103
af = 7.115750642430655	bf = 1.3699874424942295	zeta = 7.8273257066737205	eta = 0.9090909090909091
af = 7.115750642430655	bf = 1.3699874424942295	zeta = 22.530100072337387	eta = 0.31583306863192423
af = 7.115750642430655	bf = 1.3699874424942295	zeta = 13.989546557574899	eta = 0.5086476972749128
af = 7.115750642430655	bf = 1.3699874424942295	zeta = 12.578006338920751	eta = 0.5657296117280552
af = 7.115750642430655	bf = 1.3699874424942295	zeta = 12.488525356489472	eta = 0.569783096026871
af = 7.115750642430655	bf = 1.3699874424942295	zeta = 12.488104816797039	eta = 0.5698022835986821
af = 7.115750642430655	bf = 1.3699874424942295	zeta = 12.48810480741874	eta = 0.5698022840265914
eta = 0.5698022840265914
ene_coms = [0.03811389 0.03968966 0.00815206 0.00816086 0.00862128 0.00960685
 0.00848463 0.00789202 0.008424   0.05085116]
ene_comp = [0.04641333 0.09761534 0.04567661 0.01583947 0.1127181  0.05378053
 0.01989143 0.0659364  0.04788677 0.04346646]
ene_total = [1.43184344 2.32586913 0.91182726 0.40655197 2.05542047 1.0737464
 0.48067442 1.25061177 0.95387275 1.59768719]
ti_comp = [1.20645356 1.19069592 1.50607192 1.50598393 1.50137968 1.49152398
 1.50274617 1.50867228 1.50335244 1.07908084]
ti_coms = [0.38113893 0.39689657 0.08152056 0.08160856 0.08621281 0.09606851
 0.08484632 0.07892021 0.08424005 0.50851165]
t_total = [26.89291954 26.89291954 26.89291954 26.89291954 26.89291954 26.89291954
 26.89291954 26.89291954 26.89291954 26.89291954]
ene_coms = [0.03811389 0.03968966 0.00815206 0.00816086 0.00862128 0.00960685
 0.00848463 0.00789202 0.008424   0.05085116]
ene_comp = [4.29325898e-06 4.10045982e-05 2.62585234e-06 1.09511912e-07
 3.97081709e-05 4.37013406e-06 2.17824482e-07 7.87165306e-06
 3.03672256e-06 4.40793043e-06]
ene_total = [0.64570053 0.67301495 0.13813571 0.13824214 0.14671226 0.16280865
 0.14372855 0.13381972 0.14274932 0.86146466]
optimize_network iter = 0 obj = 3.1863764926202274
eta = 0.5698022840265914
freqs = [19235441.44744102 40990878.89128798 15164154.29595247  5258844.62750045
 37538171.55313161 18028717.70735724  6618359.46883524 21852460.47967418
 15926662.17781421 20140500.05100073]
eta_min = 0.5698022840265915	eta_max = 0.5698022840265928
af = 0.0013946568747361695	bf = 1.3699874424942295	zeta = 0.0015341225622097867	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03811389 0.03968966 0.00815206 0.00816086 0.00862128 0.00960685
 0.00848463 0.00789202 0.008424   0.05085116]
ene_comp = [7.27495343e-07 6.94825408e-06 4.44952275e-07 1.85568600e-08
 6.72857369e-06 7.40521872e-07 3.69104908e-08 1.33385639e-06
 5.14574482e-07 7.46926491e-07]
ene_total = [3.05528198 3.18209446 0.6535075  0.65417868 0.6916244  0.77014807
 0.68013415 0.63273424 0.67531259 4.07630798]
ti_comp = [1.20645356 1.19069592 1.50607192 1.50598393 1.50137968 1.49152398
 1.50274617 1.50867228 1.50335244 1.07908084]
ti_coms = [0.38113893 0.39689657 0.08152056 0.08160856 0.08621281 0.09606851
 0.08484632 0.07892021 0.08424005 0.50851165]
t_total = [26.89291954 26.89291954 26.89291954 26.89291954 26.89291954 26.89291954
 26.89291954 26.89291954 26.89291954 26.89291954]
ene_coms = [0.03811389 0.03968966 0.00815206 0.00816086 0.00862128 0.00960685
 0.00848463 0.00789202 0.008424   0.05085116]
ene_comp = [4.29325898e-06 4.10045982e-05 2.62585234e-06 1.09511912e-07
 3.97081709e-05 4.37013406e-06 2.17824482e-07 7.87165306e-06
 3.03672256e-06 4.40793043e-06]
ene_total = [0.64570053 0.67301495 0.13813571 0.13824214 0.14671226 0.16280865
 0.14372855 0.13381972 0.14274932 0.86146466]
optimize_network iter = 1 obj = 3.186376492620239
eta = 0.5698022840265928
freqs = [19235441.44744102 40990878.89128798 15164154.29595246  5258844.62750044
 37538171.55313157 18028717.70735722  6618359.46883524 21852460.47967416
 15926662.17781419 20140500.05100074]
Done!
ene_coms = [0.03811389 0.03968966 0.00815206 0.00816086 0.00862128 0.00960685
 0.00848463 0.00789202 0.008424   0.05085116]
ene_comp = [4.19582504e-06 4.00740138e-05 2.56625958e-06 1.07026579e-07
 3.88070085e-05 4.27095546e-06 2.12881035e-07 7.69300877e-06
 2.96780525e-06 4.30789406e-06]
ene_total = [0.03811809 0.03972973 0.00815462 0.00816096 0.00866009 0.00961112
 0.00848484 0.00789971 0.00842697 0.05085547]
At round 61 energy consumption: 0.1881016181659395
At round 61 eta: 0.5698022840265928
At round 61 a_n: 7.287306191126021
At round 61 local rounds: 18.417989547649423
At round 61 global rounds: 16.939434870399193
gradient difference: 0.37953177094459534
train() client id: f_00000-0-0 loss: 1.269380  [   32/  126]
train() client id: f_00000-0-1 loss: 1.473568  [   64/  126]
train() client id: f_00000-0-2 loss: 1.214177  [   96/  126]
train() client id: f_00000-1-0 loss: 1.287901  [   32/  126]
train() client id: f_00000-1-1 loss: 1.219964  [   64/  126]
train() client id: f_00000-1-2 loss: 1.054515  [   96/  126]
train() client id: f_00000-2-0 loss: 1.252609  [   32/  126]
train() client id: f_00000-2-1 loss: 0.969265  [   64/  126]
train() client id: f_00000-2-2 loss: 1.052465  [   96/  126]
train() client id: f_00000-3-0 loss: 0.969718  [   32/  126]
train() client id: f_00000-3-1 loss: 1.005812  [   64/  126]
train() client id: f_00000-3-2 loss: 0.866427  [   96/  126]
train() client id: f_00000-4-0 loss: 0.899272  [   32/  126]
train() client id: f_00000-4-1 loss: 1.108316  [   64/  126]
train() client id: f_00000-4-2 loss: 0.775711  [   96/  126]
train() client id: f_00000-5-0 loss: 0.967470  [   32/  126]
train() client id: f_00000-5-1 loss: 0.772591  [   64/  126]
train() client id: f_00000-5-2 loss: 0.726497  [   96/  126]
train() client id: f_00000-6-0 loss: 0.710593  [   32/  126]
train() client id: f_00000-6-1 loss: 0.847113  [   64/  126]
train() client id: f_00000-6-2 loss: 0.744791  [   96/  126]
train() client id: f_00000-7-0 loss: 0.761872  [   32/  126]
train() client id: f_00000-7-1 loss: 0.768969  [   64/  126]
train() client id: f_00000-7-2 loss: 0.777961  [   96/  126]
train() client id: f_00000-8-0 loss: 0.705435  [   32/  126]
train() client id: f_00000-8-1 loss: 0.639004  [   64/  126]
train() client id: f_00000-8-2 loss: 0.767665  [   96/  126]
train() client id: f_00000-9-0 loss: 0.692515  [   32/  126]
train() client id: f_00000-9-1 loss: 0.750438  [   64/  126]
train() client id: f_00000-9-2 loss: 0.493160  [   96/  126]
train() client id: f_00000-10-0 loss: 0.696860  [   32/  126]
train() client id: f_00000-10-1 loss: 0.554893  [   64/  126]
train() client id: f_00000-10-2 loss: 0.637478  [   96/  126]
train() client id: f_00000-11-0 loss: 0.562914  [   32/  126]
train() client id: f_00000-11-1 loss: 0.537713  [   64/  126]
train() client id: f_00000-11-2 loss: 0.795618  [   96/  126]
train() client id: f_00000-12-0 loss: 0.605139  [   32/  126]
train() client id: f_00000-12-1 loss: 0.641186  [   64/  126]
train() client id: f_00000-12-2 loss: 0.600071  [   96/  126]
train() client id: f_00000-13-0 loss: 0.576388  [   32/  126]
train() client id: f_00000-13-1 loss: 0.743079  [   64/  126]
train() client id: f_00000-13-2 loss: 0.504703  [   96/  126]
train() client id: f_00000-14-0 loss: 0.721718  [   32/  126]
train() client id: f_00000-14-1 loss: 0.531337  [   64/  126]
train() client id: f_00000-14-2 loss: 0.536452  [   96/  126]
train() client id: f_00000-15-0 loss: 0.491750  [   32/  126]
train() client id: f_00000-15-1 loss: 0.777450  [   64/  126]
train() client id: f_00000-15-2 loss: 0.586382  [   96/  126]
train() client id: f_00000-16-0 loss: 0.611760  [   32/  126]
train() client id: f_00000-16-1 loss: 0.577441  [   64/  126]
train() client id: f_00000-16-2 loss: 0.601617  [   96/  126]
train() client id: f_00000-17-0 loss: 0.670222  [   32/  126]
train() client id: f_00000-17-1 loss: 0.518893  [   64/  126]
train() client id: f_00000-17-2 loss: 0.577668  [   96/  126]
train() client id: f_00001-0-0 loss: 0.554145  [   32/  265]
train() client id: f_00001-0-1 loss: 0.632844  [   64/  265]
train() client id: f_00001-0-2 loss: 0.441394  [   96/  265]
train() client id: f_00001-0-3 loss: 0.415046  [  128/  265]
train() client id: f_00001-0-4 loss: 0.436132  [  160/  265]
train() client id: f_00001-0-5 loss: 0.442582  [  192/  265]
train() client id: f_00001-0-6 loss: 0.416386  [  224/  265]
train() client id: f_00001-0-7 loss: 0.476705  [  256/  265]
train() client id: f_00001-1-0 loss: 0.442552  [   32/  265]
train() client id: f_00001-1-1 loss: 0.503037  [   64/  265]
train() client id: f_00001-1-2 loss: 0.505571  [   96/  265]
train() client id: f_00001-1-3 loss: 0.482911  [  128/  265]
train() client id: f_00001-1-4 loss: 0.501120  [  160/  265]
train() client id: f_00001-1-5 loss: 0.389573  [  192/  265]
train() client id: f_00001-1-6 loss: 0.553722  [  224/  265]
train() client id: f_00001-1-7 loss: 0.397565  [  256/  265]
train() client id: f_00001-2-0 loss: 0.479775  [   32/  265]
train() client id: f_00001-2-1 loss: 0.493357  [   64/  265]
train() client id: f_00001-2-2 loss: 0.579170  [   96/  265]
train() client id: f_00001-2-3 loss: 0.424577  [  128/  265]
train() client id: f_00001-2-4 loss: 0.417556  [  160/  265]
train() client id: f_00001-2-5 loss: 0.381133  [  192/  265]
train() client id: f_00001-2-6 loss: 0.513035  [  224/  265]
train() client id: f_00001-2-7 loss: 0.434012  [  256/  265]
train() client id: f_00001-3-0 loss: 0.492526  [   32/  265]
train() client id: f_00001-3-1 loss: 0.389155  [   64/  265]
train() client id: f_00001-3-2 loss: 0.626248  [   96/  265]
train() client id: f_00001-3-3 loss: 0.351524  [  128/  265]
train() client id: f_00001-3-4 loss: 0.405353  [  160/  265]
train() client id: f_00001-3-5 loss: 0.480090  [  192/  265]
train() client id: f_00001-3-6 loss: 0.366555  [  224/  265]
train() client id: f_00001-3-7 loss: 0.585393  [  256/  265]
train() client id: f_00001-4-0 loss: 0.532770  [   32/  265]
train() client id: f_00001-4-1 loss: 0.532494  [   64/  265]
train() client id: f_00001-4-2 loss: 0.348324  [   96/  265]
train() client id: f_00001-4-3 loss: 0.399533  [  128/  265]
train() client id: f_00001-4-4 loss: 0.445539  [  160/  265]
train() client id: f_00001-4-5 loss: 0.473987  [  192/  265]
train() client id: f_00001-4-6 loss: 0.483890  [  224/  265]
train() client id: f_00001-4-7 loss: 0.439106  [  256/  265]
train() client id: f_00001-5-0 loss: 0.428293  [   32/  265]
train() client id: f_00001-5-1 loss: 0.459069  [   64/  265]
train() client id: f_00001-5-2 loss: 0.369943  [   96/  265]
train() client id: f_00001-5-3 loss: 0.426384  [  128/  265]
train() client id: f_00001-5-4 loss: 0.478904  [  160/  265]
train() client id: f_00001-5-5 loss: 0.355064  [  192/  265]
train() client id: f_00001-5-6 loss: 0.489976  [  224/  265]
train() client id: f_00001-5-7 loss: 0.634171  [  256/  265]
train() client id: f_00001-6-0 loss: 0.420942  [   32/  265]
train() client id: f_00001-6-1 loss: 0.373348  [   64/  265]
train() client id: f_00001-6-2 loss: 0.513698  [   96/  265]
train() client id: f_00001-6-3 loss: 0.375031  [  128/  265]
train() client id: f_00001-6-4 loss: 0.443838  [  160/  265]
train() client id: f_00001-6-5 loss: 0.501456  [  192/  265]
train() client id: f_00001-6-6 loss: 0.457431  [  224/  265]
train() client id: f_00001-6-7 loss: 0.447203  [  256/  265]
train() client id: f_00001-7-0 loss: 0.393181  [   32/  265]
train() client id: f_00001-7-1 loss: 0.545900  [   64/  265]
train() client id: f_00001-7-2 loss: 0.356962  [   96/  265]
train() client id: f_00001-7-3 loss: 0.454309  [  128/  265]
train() client id: f_00001-7-4 loss: 0.382312  [  160/  265]
train() client id: f_00001-7-5 loss: 0.414803  [  192/  265]
train() client id: f_00001-7-6 loss: 0.452433  [  224/  265]
train() client id: f_00001-7-7 loss: 0.592428  [  256/  265]
train() client id: f_00001-8-0 loss: 0.462621  [   32/  265]
train() client id: f_00001-8-1 loss: 0.452083  [   64/  265]
train() client id: f_00001-8-2 loss: 0.478746  [   96/  265]
train() client id: f_00001-8-3 loss: 0.433804  [  128/  265]
train() client id: f_00001-8-4 loss: 0.484349  [  160/  265]
train() client id: f_00001-8-5 loss: 0.346764  [  192/  265]
train() client id: f_00001-8-6 loss: 0.440075  [  224/  265]
train() client id: f_00001-8-7 loss: 0.359552  [  256/  265]
train() client id: f_00001-9-0 loss: 0.415253  [   32/  265]
train() client id: f_00001-9-1 loss: 0.352858  [   64/  265]
train() client id: f_00001-9-2 loss: 0.490063  [   96/  265]
train() client id: f_00001-9-3 loss: 0.437047  [  128/  265]
train() client id: f_00001-9-4 loss: 0.639869  [  160/  265]
train() client id: f_00001-9-5 loss: 0.354033  [  192/  265]
train() client id: f_00001-9-6 loss: 0.391587  [  224/  265]
train() client id: f_00001-9-7 loss: 0.429978  [  256/  265]
train() client id: f_00001-10-0 loss: 0.342143  [   32/  265]
train() client id: f_00001-10-1 loss: 0.368810  [   64/  265]
train() client id: f_00001-10-2 loss: 0.396177  [   96/  265]
train() client id: f_00001-10-3 loss: 0.419437  [  128/  265]
train() client id: f_00001-10-4 loss: 0.686007  [  160/  265]
train() client id: f_00001-10-5 loss: 0.546448  [  192/  265]
train() client id: f_00001-10-6 loss: 0.364559  [  224/  265]
train() client id: f_00001-10-7 loss: 0.460384  [  256/  265]
train() client id: f_00001-11-0 loss: 0.373176  [   32/  265]
train() client id: f_00001-11-1 loss: 0.370250  [   64/  265]
train() client id: f_00001-11-2 loss: 0.524657  [   96/  265]
train() client id: f_00001-11-3 loss: 0.457806  [  128/  265]
train() client id: f_00001-11-4 loss: 0.421435  [  160/  265]
train() client id: f_00001-11-5 loss: 0.435849  [  192/  265]
train() client id: f_00001-11-6 loss: 0.481690  [  224/  265]
train() client id: f_00001-11-7 loss: 0.508305  [  256/  265]
train() client id: f_00001-12-0 loss: 0.413239  [   32/  265]
train() client id: f_00001-12-1 loss: 0.424973  [   64/  265]
train() client id: f_00001-12-2 loss: 0.385596  [   96/  265]
train() client id: f_00001-12-3 loss: 0.640442  [  128/  265]
train() client id: f_00001-12-4 loss: 0.449617  [  160/  265]
train() client id: f_00001-12-5 loss: 0.431679  [  192/  265]
train() client id: f_00001-12-6 loss: 0.482094  [  224/  265]
train() client id: f_00001-12-7 loss: 0.359624  [  256/  265]
train() client id: f_00001-13-0 loss: 0.449266  [   32/  265]
train() client id: f_00001-13-1 loss: 0.398478  [   64/  265]
train() client id: f_00001-13-2 loss: 0.390981  [   96/  265]
train() client id: f_00001-13-3 loss: 0.412776  [  128/  265]
train() client id: f_00001-13-4 loss: 0.523288  [  160/  265]
train() client id: f_00001-13-5 loss: 0.488536  [  192/  265]
train() client id: f_00001-13-6 loss: 0.512259  [  224/  265]
train() client id: f_00001-13-7 loss: 0.347464  [  256/  265]
train() client id: f_00001-14-0 loss: 0.383413  [   32/  265]
train() client id: f_00001-14-1 loss: 0.490681  [   64/  265]
train() client id: f_00001-14-2 loss: 0.346580  [   96/  265]
train() client id: f_00001-14-3 loss: 0.368655  [  128/  265]
train() client id: f_00001-14-4 loss: 0.481609  [  160/  265]
train() client id: f_00001-14-5 loss: 0.491288  [  192/  265]
train() client id: f_00001-14-6 loss: 0.541938  [  224/  265]
train() client id: f_00001-14-7 loss: 0.427740  [  256/  265]
train() client id: f_00001-15-0 loss: 0.514325  [   32/  265]
train() client id: f_00001-15-1 loss: 0.441286  [   64/  265]
train() client id: f_00001-15-2 loss: 0.422345  [   96/  265]
train() client id: f_00001-15-3 loss: 0.429913  [  128/  265]
train() client id: f_00001-15-4 loss: 0.433483  [  160/  265]
train() client id: f_00001-15-5 loss: 0.583966  [  192/  265]
train() client id: f_00001-15-6 loss: 0.349669  [  224/  265]
train() client id: f_00001-15-7 loss: 0.417015  [  256/  265]
train() client id: f_00001-16-0 loss: 0.503868  [   32/  265]
train() client id: f_00001-16-1 loss: 0.453273  [   64/  265]
train() client id: f_00001-16-2 loss: 0.418861  [   96/  265]
train() client id: f_00001-16-3 loss: 0.354006  [  128/  265]
train() client id: f_00001-16-4 loss: 0.489695  [  160/  265]
train() client id: f_00001-16-5 loss: 0.456578  [  192/  265]
train() client id: f_00001-16-6 loss: 0.438890  [  224/  265]
train() client id: f_00001-16-7 loss: 0.483339  [  256/  265]
train() client id: f_00001-17-0 loss: 0.349106  [   32/  265]
train() client id: f_00001-17-1 loss: 0.494270  [   64/  265]
train() client id: f_00001-17-2 loss: 0.591295  [   96/  265]
train() client id: f_00001-17-3 loss: 0.359271  [  128/  265]
train() client id: f_00001-17-4 loss: 0.421211  [  160/  265]
train() client id: f_00001-17-5 loss: 0.540728  [  192/  265]
train() client id: f_00001-17-6 loss: 0.352412  [  224/  265]
train() client id: f_00001-17-7 loss: 0.426460  [  256/  265]
train() client id: f_00002-0-0 loss: 1.488131  [   32/  124]
train() client id: f_00002-0-1 loss: 1.118122  [   64/  124]
train() client id: f_00002-0-2 loss: 1.036192  [   96/  124]
train() client id: f_00002-1-0 loss: 1.351293  [   32/  124]
train() client id: f_00002-1-1 loss: 0.958284  [   64/  124]
train() client id: f_00002-1-2 loss: 1.074709  [   96/  124]
train() client id: f_00002-2-0 loss: 0.971024  [   32/  124]
train() client id: f_00002-2-1 loss: 0.946947  [   64/  124]
train() client id: f_00002-2-2 loss: 1.139528  [   96/  124]
train() client id: f_00002-3-0 loss: 1.204927  [   32/  124]
train() client id: f_00002-3-1 loss: 0.956590  [   64/  124]
train() client id: f_00002-3-2 loss: 1.061666  [   96/  124]
train() client id: f_00002-4-0 loss: 0.985008  [   32/  124]
train() client id: f_00002-4-1 loss: 1.123593  [   64/  124]
train() client id: f_00002-4-2 loss: 1.018086  [   96/  124]
train() client id: f_00002-5-0 loss: 0.996245  [   32/  124]
train() client id: f_00002-5-1 loss: 0.918566  [   64/  124]
train() client id: f_00002-5-2 loss: 1.058855  [   96/  124]
train() client id: f_00002-6-0 loss: 0.891869  [   32/  124]
train() client id: f_00002-6-1 loss: 1.014944  [   64/  124]
train() client id: f_00002-6-2 loss: 0.952117  [   96/  124]
train() client id: f_00002-7-0 loss: 1.058384  [   32/  124]
train() client id: f_00002-7-1 loss: 0.949367  [   64/  124]
train() client id: f_00002-7-2 loss: 0.868550  [   96/  124]
train() client id: f_00002-8-0 loss: 1.011900  [   32/  124]
train() client id: f_00002-8-1 loss: 0.929151  [   64/  124]
train() client id: f_00002-8-2 loss: 0.854815  [   96/  124]
train() client id: f_00002-9-0 loss: 0.928297  [   32/  124]
train() client id: f_00002-9-1 loss: 1.060144  [   64/  124]
train() client id: f_00002-9-2 loss: 0.853665  [   96/  124]
train() client id: f_00002-10-0 loss: 0.880728  [   32/  124]
train() client id: f_00002-10-1 loss: 1.038145  [   64/  124]
train() client id: f_00002-10-2 loss: 1.000643  [   96/  124]
train() client id: f_00002-11-0 loss: 0.871085  [   32/  124]
train() client id: f_00002-11-1 loss: 0.928314  [   64/  124]
train() client id: f_00002-11-2 loss: 0.785178  [   96/  124]
train() client id: f_00002-12-0 loss: 0.883653  [   32/  124]
train() client id: f_00002-12-1 loss: 1.014444  [   64/  124]
train() client id: f_00002-12-2 loss: 0.903090  [   96/  124]
train() client id: f_00002-13-0 loss: 1.034040  [   32/  124]
train() client id: f_00002-13-1 loss: 1.093123  [   64/  124]
train() client id: f_00002-13-2 loss: 0.706094  [   96/  124]
train() client id: f_00002-14-0 loss: 0.849723  [   32/  124]
train() client id: f_00002-14-1 loss: 0.822573  [   64/  124]
train() client id: f_00002-14-2 loss: 1.044857  [   96/  124]
train() client id: f_00002-15-0 loss: 0.785383  [   32/  124]
train() client id: f_00002-15-1 loss: 0.794760  [   64/  124]
train() client id: f_00002-15-2 loss: 1.132954  [   96/  124]
train() client id: f_00002-16-0 loss: 0.890841  [   32/  124]
train() client id: f_00002-16-1 loss: 0.766777  [   64/  124]
train() client id: f_00002-16-2 loss: 1.057454  [   96/  124]
train() client id: f_00002-17-0 loss: 1.164250  [   32/  124]
train() client id: f_00002-17-1 loss: 0.703758  [   64/  124]
train() client id: f_00002-17-2 loss: 0.827459  [   96/  124]
train() client id: f_00003-0-0 loss: 0.299592  [   32/   43]
train() client id: f_00003-1-0 loss: 0.636682  [   32/   43]
train() client id: f_00003-2-0 loss: 0.552014  [   32/   43]
train() client id: f_00003-3-0 loss: 0.430106  [   32/   43]
train() client id: f_00003-4-0 loss: 0.672466  [   32/   43]
train() client id: f_00003-5-0 loss: 0.520189  [   32/   43]
train() client id: f_00003-6-0 loss: 0.612510  [   32/   43]
train() client id: f_00003-7-0 loss: 0.435849  [   32/   43]
train() client id: f_00003-8-0 loss: 0.526964  [   32/   43]
train() client id: f_00003-9-0 loss: 0.447951  [   32/   43]
train() client id: f_00003-10-0 loss: 0.484349  [   32/   43]
train() client id: f_00003-11-0 loss: 0.542817  [   32/   43]
train() client id: f_00003-12-0 loss: 0.413172  [   32/   43]
train() client id: f_00003-13-0 loss: 0.586251  [   32/   43]
train() client id: f_00003-14-0 loss: 0.498028  [   32/   43]
train() client id: f_00003-15-0 loss: 0.462908  [   32/   43]
train() client id: f_00003-16-0 loss: 0.513495  [   32/   43]
train() client id: f_00003-17-0 loss: 0.704214  [   32/   43]
train() client id: f_00004-0-0 loss: 0.708079  [   32/  306]
train() client id: f_00004-0-1 loss: 0.741520  [   64/  306]
train() client id: f_00004-0-2 loss: 0.817327  [   96/  306]
train() client id: f_00004-0-3 loss: 0.804393  [  128/  306]
train() client id: f_00004-0-4 loss: 0.602400  [  160/  306]
train() client id: f_00004-0-5 loss: 0.778474  [  192/  306]
train() client id: f_00004-0-6 loss: 0.745853  [  224/  306]
train() client id: f_00004-0-7 loss: 0.663651  [  256/  306]
train() client id: f_00004-0-8 loss: 0.671287  [  288/  306]
train() client id: f_00004-1-0 loss: 0.742753  [   32/  306]
train() client id: f_00004-1-1 loss: 0.776025  [   64/  306]
train() client id: f_00004-1-2 loss: 0.724736  [   96/  306]
train() client id: f_00004-1-3 loss: 0.750083  [  128/  306]
train() client id: f_00004-1-4 loss: 0.612993  [  160/  306]
train() client id: f_00004-1-5 loss: 0.756137  [  192/  306]
train() client id: f_00004-1-6 loss: 0.569932  [  224/  306]
train() client id: f_00004-1-7 loss: 0.826864  [  256/  306]
train() client id: f_00004-1-8 loss: 0.968628  [  288/  306]
train() client id: f_00004-2-0 loss: 0.678317  [   32/  306]
train() client id: f_00004-2-1 loss: 0.754915  [   64/  306]
train() client id: f_00004-2-2 loss: 0.705264  [   96/  306]
train() client id: f_00004-2-3 loss: 0.783759  [  128/  306]
train() client id: f_00004-2-4 loss: 0.730879  [  160/  306]
train() client id: f_00004-2-5 loss: 0.683889  [  192/  306]
train() client id: f_00004-2-6 loss: 0.675772  [  224/  306]
train() client id: f_00004-2-7 loss: 0.764471  [  256/  306]
train() client id: f_00004-2-8 loss: 0.831197  [  288/  306]
train() client id: f_00004-3-0 loss: 0.812414  [   32/  306]
train() client id: f_00004-3-1 loss: 0.675246  [   64/  306]
train() client id: f_00004-3-2 loss: 0.893921  [   96/  306]
train() client id: f_00004-3-3 loss: 0.865460  [  128/  306]
train() client id: f_00004-3-4 loss: 0.629861  [  160/  306]
train() client id: f_00004-3-5 loss: 0.653546  [  192/  306]
train() client id: f_00004-3-6 loss: 0.701436  [  224/  306]
train() client id: f_00004-3-7 loss: 0.675870  [  256/  306]
train() client id: f_00004-3-8 loss: 0.682728  [  288/  306]
train() client id: f_00004-4-0 loss: 0.663286  [   32/  306]
train() client id: f_00004-4-1 loss: 0.704511  [   64/  306]
train() client id: f_00004-4-2 loss: 0.746297  [   96/  306]
train() client id: f_00004-4-3 loss: 0.795257  [  128/  306]
train() client id: f_00004-4-4 loss: 0.862412  [  160/  306]
train() client id: f_00004-4-5 loss: 0.755946  [  192/  306]
train() client id: f_00004-4-6 loss: 0.691401  [  224/  306]
train() client id: f_00004-4-7 loss: 0.669304  [  256/  306]
train() client id: f_00004-4-8 loss: 0.737865  [  288/  306]
train() client id: f_00004-5-0 loss: 0.721511  [   32/  306]
train() client id: f_00004-5-1 loss: 0.831819  [   64/  306]
train() client id: f_00004-5-2 loss: 0.720996  [   96/  306]
train() client id: f_00004-5-3 loss: 0.745397  [  128/  306]
train() client id: f_00004-5-4 loss: 0.724435  [  160/  306]
train() client id: f_00004-5-5 loss: 0.637425  [  192/  306]
train() client id: f_00004-5-6 loss: 0.692751  [  224/  306]
train() client id: f_00004-5-7 loss: 0.858187  [  256/  306]
train() client id: f_00004-5-8 loss: 0.769848  [  288/  306]
train() client id: f_00004-6-0 loss: 0.818850  [   32/  306]
train() client id: f_00004-6-1 loss: 0.776208  [   64/  306]
train() client id: f_00004-6-2 loss: 0.750244  [   96/  306]
train() client id: f_00004-6-3 loss: 0.781115  [  128/  306]
train() client id: f_00004-6-4 loss: 0.820106  [  160/  306]
train() client id: f_00004-6-5 loss: 0.633057  [  192/  306]
train() client id: f_00004-6-6 loss: 0.701889  [  224/  306]
train() client id: f_00004-6-7 loss: 0.673011  [  256/  306]
train() client id: f_00004-6-8 loss: 0.742425  [  288/  306]
train() client id: f_00004-7-0 loss: 0.681403  [   32/  306]
train() client id: f_00004-7-1 loss: 0.786600  [   64/  306]
train() client id: f_00004-7-2 loss: 0.700526  [   96/  306]
train() client id: f_00004-7-3 loss: 0.730357  [  128/  306]
train() client id: f_00004-7-4 loss: 0.726715  [  160/  306]
train() client id: f_00004-7-5 loss: 0.961999  [  192/  306]
train() client id: f_00004-7-6 loss: 0.712041  [  224/  306]
train() client id: f_00004-7-7 loss: 0.695699  [  256/  306]
train() client id: f_00004-7-8 loss: 0.683439  [  288/  306]
train() client id: f_00004-8-0 loss: 0.681392  [   32/  306]
train() client id: f_00004-8-1 loss: 0.778672  [   64/  306]
train() client id: f_00004-8-2 loss: 0.691910  [   96/  306]
train() client id: f_00004-8-3 loss: 0.702562  [  128/  306]
train() client id: f_00004-8-4 loss: 0.810730  [  160/  306]
train() client id: f_00004-8-5 loss: 0.761222  [  192/  306]
train() client id: f_00004-8-6 loss: 0.684716  [  224/  306]
train() client id: f_00004-8-7 loss: 0.711550  [  256/  306]
train() client id: f_00004-8-8 loss: 0.841314  [  288/  306]
train() client id: f_00004-9-0 loss: 0.677416  [   32/  306]
train() client id: f_00004-9-1 loss: 0.807094  [   64/  306]
train() client id: f_00004-9-2 loss: 0.880631  [   96/  306]
train() client id: f_00004-9-3 loss: 0.777705  [  128/  306]
train() client id: f_00004-9-4 loss: 0.679850  [  160/  306]
train() client id: f_00004-9-5 loss: 0.621984  [  192/  306]
train() client id: f_00004-9-6 loss: 0.737090  [  224/  306]
train() client id: f_00004-9-7 loss: 0.795146  [  256/  306]
train() client id: f_00004-9-8 loss: 0.724133  [  288/  306]
train() client id: f_00004-10-0 loss: 0.809930  [   32/  306]
train() client id: f_00004-10-1 loss: 0.796523  [   64/  306]
train() client id: f_00004-10-2 loss: 0.682124  [   96/  306]
train() client id: f_00004-10-3 loss: 0.720643  [  128/  306]
train() client id: f_00004-10-4 loss: 0.647858  [  160/  306]
train() client id: f_00004-10-5 loss: 0.777853  [  192/  306]
train() client id: f_00004-10-6 loss: 0.704312  [  224/  306]
train() client id: f_00004-10-7 loss: 0.822639  [  256/  306]
train() client id: f_00004-10-8 loss: 0.702346  [  288/  306]
train() client id: f_00004-11-0 loss: 0.855579  [   32/  306]
train() client id: f_00004-11-1 loss: 0.653454  [   64/  306]
train() client id: f_00004-11-2 loss: 0.810260  [   96/  306]
train() client id: f_00004-11-3 loss: 0.705105  [  128/  306]
train() client id: f_00004-11-4 loss: 0.703045  [  160/  306]
train() client id: f_00004-11-5 loss: 0.707084  [  192/  306]
train() client id: f_00004-11-6 loss: 0.744886  [  224/  306]
train() client id: f_00004-11-7 loss: 0.673681  [  256/  306]
train() client id: f_00004-11-8 loss: 0.733895  [  288/  306]
train() client id: f_00004-12-0 loss: 0.905284  [   32/  306]
train() client id: f_00004-12-1 loss: 0.680051  [   64/  306]
train() client id: f_00004-12-2 loss: 0.830670  [   96/  306]
train() client id: f_00004-12-3 loss: 0.816719  [  128/  306]
train() client id: f_00004-12-4 loss: 0.685567  [  160/  306]
train() client id: f_00004-12-5 loss: 0.686151  [  192/  306]
train() client id: f_00004-12-6 loss: 0.789557  [  224/  306]
train() client id: f_00004-12-7 loss: 0.596689  [  256/  306]
train() client id: f_00004-12-8 loss: 0.725824  [  288/  306]
train() client id: f_00004-13-0 loss: 0.780451  [   32/  306]
train() client id: f_00004-13-1 loss: 0.766139  [   64/  306]
train() client id: f_00004-13-2 loss: 0.779359  [   96/  306]
train() client id: f_00004-13-3 loss: 0.739969  [  128/  306]
train() client id: f_00004-13-4 loss: 0.642217  [  160/  306]
train() client id: f_00004-13-5 loss: 0.707755  [  192/  306]
train() client id: f_00004-13-6 loss: 0.906864  [  224/  306]
train() client id: f_00004-13-7 loss: 0.665210  [  256/  306]
train() client id: f_00004-13-8 loss: 0.725895  [  288/  306]
train() client id: f_00004-14-0 loss: 0.652267  [   32/  306]
train() client id: f_00004-14-1 loss: 0.857602  [   64/  306]
train() client id: f_00004-14-2 loss: 0.769925  [   96/  306]
train() client id: f_00004-14-3 loss: 0.652897  [  128/  306]
train() client id: f_00004-14-4 loss: 0.818287  [  160/  306]
train() client id: f_00004-14-5 loss: 0.867799  [  192/  306]
train() client id: f_00004-14-6 loss: 0.642062  [  224/  306]
train() client id: f_00004-14-7 loss: 0.734931  [  256/  306]
train() client id: f_00004-14-8 loss: 0.640894  [  288/  306]
train() client id: f_00004-15-0 loss: 0.733405  [   32/  306]
train() client id: f_00004-15-1 loss: 0.709539  [   64/  306]
train() client id: f_00004-15-2 loss: 0.868184  [   96/  306]
train() client id: f_00004-15-3 loss: 0.705129  [  128/  306]
train() client id: f_00004-15-4 loss: 0.781432  [  160/  306]
train() client id: f_00004-15-5 loss: 0.776463  [  192/  306]
train() client id: f_00004-15-6 loss: 0.703300  [  224/  306]
train() client id: f_00004-15-7 loss: 0.785583  [  256/  306]
train() client id: f_00004-15-8 loss: 0.679097  [  288/  306]
train() client id: f_00004-16-0 loss: 0.894736  [   32/  306]
train() client id: f_00004-16-1 loss: 0.759728  [   64/  306]
train() client id: f_00004-16-2 loss: 0.779869  [   96/  306]
train() client id: f_00004-16-3 loss: 0.729458  [  128/  306]
train() client id: f_00004-16-4 loss: 0.673279  [  160/  306]
train() client id: f_00004-16-5 loss: 0.693408  [  192/  306]
train() client id: f_00004-16-6 loss: 0.765905  [  224/  306]
train() client id: f_00004-16-7 loss: 0.751703  [  256/  306]
train() client id: f_00004-16-8 loss: 0.717295  [  288/  306]
train() client id: f_00004-17-0 loss: 0.764462  [   32/  306]
train() client id: f_00004-17-1 loss: 0.747257  [   64/  306]
train() client id: f_00004-17-2 loss: 0.734708  [   96/  306]
train() client id: f_00004-17-3 loss: 0.713704  [  128/  306]
train() client id: f_00004-17-4 loss: 0.685904  [  160/  306]
train() client id: f_00004-17-5 loss: 0.737044  [  192/  306]
train() client id: f_00004-17-6 loss: 0.752321  [  224/  306]
train() client id: f_00004-17-7 loss: 0.805503  [  256/  306]
train() client id: f_00004-17-8 loss: 0.789354  [  288/  306]
train() client id: f_00005-0-0 loss: 0.584838  [   32/  146]
train() client id: f_00005-0-1 loss: 0.635130  [   64/  146]
train() client id: f_00005-0-2 loss: 0.472621  [   96/  146]
train() client id: f_00005-0-3 loss: 0.561822  [  128/  146]
train() client id: f_00005-1-0 loss: 0.679855  [   32/  146]
train() client id: f_00005-1-1 loss: 0.379504  [   64/  146]
train() client id: f_00005-1-2 loss: 0.675406  [   96/  146]
train() client id: f_00005-1-3 loss: 0.598458  [  128/  146]
train() client id: f_00005-2-0 loss: 0.577224  [   32/  146]
train() client id: f_00005-2-1 loss: 0.335133  [   64/  146]
train() client id: f_00005-2-2 loss: 0.626459  [   96/  146]
train() client id: f_00005-2-3 loss: 0.652139  [  128/  146]
train() client id: f_00005-3-0 loss: 0.553331  [   32/  146]
train() client id: f_00005-3-1 loss: 0.537168  [   64/  146]
train() client id: f_00005-3-2 loss: 0.565756  [   96/  146]
train() client id: f_00005-3-3 loss: 0.550949  [  128/  146]
train() client id: f_00005-4-0 loss: 0.777500  [   32/  146]
train() client id: f_00005-4-1 loss: 0.361718  [   64/  146]
train() client id: f_00005-4-2 loss: 0.488280  [   96/  146]
train() client id: f_00005-4-3 loss: 0.513648  [  128/  146]
train() client id: f_00005-5-0 loss: 0.475705  [   32/  146]
train() client id: f_00005-5-1 loss: 0.376640  [   64/  146]
train() client id: f_00005-5-2 loss: 0.668073  [   96/  146]
train() client id: f_00005-5-3 loss: 0.544500  [  128/  146]
train() client id: f_00005-6-0 loss: 0.404405  [   32/  146]
train() client id: f_00005-6-1 loss: 0.766321  [   64/  146]
train() client id: f_00005-6-2 loss: 0.372486  [   96/  146]
train() client id: f_00005-6-3 loss: 0.680916  [  128/  146]
train() client id: f_00005-7-0 loss: 0.375316  [   32/  146]
train() client id: f_00005-7-1 loss: 0.618473  [   64/  146]
train() client id: f_00005-7-2 loss: 0.473191  [   96/  146]
train() client id: f_00005-7-3 loss: 0.734764  [  128/  146]
train() client id: f_00005-8-0 loss: 0.923770  [   32/  146]
train() client id: f_00005-8-1 loss: 0.368887  [   64/  146]
train() client id: f_00005-8-2 loss: 0.575866  [   96/  146]
train() client id: f_00005-8-3 loss: 0.447311  [  128/  146]
train() client id: f_00005-9-0 loss: 0.495363  [   32/  146]
train() client id: f_00005-9-1 loss: 0.943381  [   64/  146]
train() client id: f_00005-9-2 loss: 0.395550  [   96/  146]
train() client id: f_00005-9-3 loss: 0.537477  [  128/  146]
train() client id: f_00005-10-0 loss: 0.499511  [   32/  146]
train() client id: f_00005-10-1 loss: 0.538895  [   64/  146]
train() client id: f_00005-10-2 loss: 0.734950  [   96/  146]
train() client id: f_00005-10-3 loss: 0.670734  [  128/  146]
train() client id: f_00005-11-0 loss: 0.700336  [   32/  146]
train() client id: f_00005-11-1 loss: 0.567292  [   64/  146]
train() client id: f_00005-11-2 loss: 0.319880  [   96/  146]
train() client id: f_00005-11-3 loss: 0.547133  [  128/  146]
train() client id: f_00005-12-0 loss: 0.694963  [   32/  146]
train() client id: f_00005-12-1 loss: 0.656465  [   64/  146]
train() client id: f_00005-12-2 loss: 0.418276  [   96/  146]
train() client id: f_00005-12-3 loss: 0.557481  [  128/  146]
train() client id: f_00005-13-0 loss: 0.576663  [   32/  146]
train() client id: f_00005-13-1 loss: 0.511727  [   64/  146]
train() client id: f_00005-13-2 loss: 0.680325  [   96/  146]
train() client id: f_00005-13-3 loss: 0.339827  [  128/  146]
train() client id: f_00005-14-0 loss: 0.364621  [   32/  146]
train() client id: f_00005-14-1 loss: 0.640348  [   64/  146]
train() client id: f_00005-14-2 loss: 0.674793  [   96/  146]
train() client id: f_00005-14-3 loss: 0.603154  [  128/  146]
train() client id: f_00005-15-0 loss: 0.734405  [   32/  146]
train() client id: f_00005-15-1 loss: 0.484869  [   64/  146]
train() client id: f_00005-15-2 loss: 0.453822  [   96/  146]
train() client id: f_00005-15-3 loss: 0.554207  [  128/  146]
train() client id: f_00005-16-0 loss: 0.641519  [   32/  146]
train() client id: f_00005-16-1 loss: 0.519413  [   64/  146]
train() client id: f_00005-16-2 loss: 0.573703  [   96/  146]
train() client id: f_00005-16-3 loss: 0.501956  [  128/  146]
train() client id: f_00005-17-0 loss: 0.450273  [   32/  146]
train() client id: f_00005-17-1 loss: 0.789029  [   64/  146]
train() client id: f_00005-17-2 loss: 0.790577  [   96/  146]
train() client id: f_00005-17-3 loss: 0.308611  [  128/  146]
train() client id: f_00006-0-0 loss: 0.378092  [   32/   54]
train() client id: f_00006-1-0 loss: 0.371228  [   32/   54]
train() client id: f_00006-2-0 loss: 0.414512  [   32/   54]
train() client id: f_00006-3-0 loss: 0.499174  [   32/   54]
train() client id: f_00006-4-0 loss: 0.423495  [   32/   54]
train() client id: f_00006-5-0 loss: 0.478440  [   32/   54]
train() client id: f_00006-6-0 loss: 0.436973  [   32/   54]
train() client id: f_00006-7-0 loss: 0.384157  [   32/   54]
train() client id: f_00006-8-0 loss: 0.441951  [   32/   54]
train() client id: f_00006-9-0 loss: 0.439833  [   32/   54]
train() client id: f_00006-10-0 loss: 0.460815  [   32/   54]
train() client id: f_00006-11-0 loss: 0.468218  [   32/   54]
train() client id: f_00006-12-0 loss: 0.475322  [   32/   54]
train() client id: f_00006-13-0 loss: 0.441175  [   32/   54]
train() client id: f_00006-14-0 loss: 0.456286  [   32/   54]
train() client id: f_00006-15-0 loss: 0.481377  [   32/   54]
train() client id: f_00006-16-0 loss: 0.427877  [   32/   54]
train() client id: f_00006-17-0 loss: 0.403154  [   32/   54]
train() client id: f_00007-0-0 loss: 0.642812  [   32/  179]
train() client id: f_00007-0-1 loss: 0.628690  [   64/  179]
train() client id: f_00007-0-2 loss: 0.690452  [   96/  179]
train() client id: f_00007-0-3 loss: 0.976493  [  128/  179]
train() client id: f_00007-0-4 loss: 0.527623  [  160/  179]
train() client id: f_00007-1-0 loss: 0.583927  [   32/  179]
train() client id: f_00007-1-1 loss: 0.535363  [   64/  179]
train() client id: f_00007-1-2 loss: 0.677456  [   96/  179]
train() client id: f_00007-1-3 loss: 0.825404  [  128/  179]
train() client id: f_00007-1-4 loss: 0.687438  [  160/  179]
train() client id: f_00007-2-0 loss: 0.808861  [   32/  179]
train() client id: f_00007-2-1 loss: 0.613068  [   64/  179]
train() client id: f_00007-2-2 loss: 0.652856  [   96/  179]
train() client id: f_00007-2-3 loss: 0.550126  [  128/  179]
train() client id: f_00007-2-4 loss: 0.713595  [  160/  179]
train() client id: f_00007-3-0 loss: 0.565869  [   32/  179]
train() client id: f_00007-3-1 loss: 0.671122  [   64/  179]
train() client id: f_00007-3-2 loss: 0.546102  [   96/  179]
train() client id: f_00007-3-3 loss: 0.672728  [  128/  179]
train() client id: f_00007-3-4 loss: 0.641867  [  160/  179]
train() client id: f_00007-4-0 loss: 0.690693  [   32/  179]
train() client id: f_00007-4-1 loss: 0.627854  [   64/  179]
train() client id: f_00007-4-2 loss: 0.434915  [   96/  179]
train() client id: f_00007-4-3 loss: 0.775793  [  128/  179]
train() client id: f_00007-4-4 loss: 0.624789  [  160/  179]
train() client id: f_00007-5-0 loss: 0.735509  [   32/  179]
train() client id: f_00007-5-1 loss: 0.643712  [   64/  179]
train() client id: f_00007-5-2 loss: 0.691714  [   96/  179]
train() client id: f_00007-5-3 loss: 0.571816  [  128/  179]
train() client id: f_00007-5-4 loss: 0.504223  [  160/  179]
train() client id: f_00007-6-0 loss: 0.760096  [   32/  179]
train() client id: f_00007-6-1 loss: 0.574543  [   64/  179]
train() client id: f_00007-6-2 loss: 0.557502  [   96/  179]
train() client id: f_00007-6-3 loss: 0.754589  [  128/  179]
train() client id: f_00007-6-4 loss: 0.639184  [  160/  179]
train() client id: f_00007-7-0 loss: 0.707905  [   32/  179]
train() client id: f_00007-7-1 loss: 0.913037  [   64/  179]
train() client id: f_00007-7-2 loss: 0.460354  [   96/  179]
train() client id: f_00007-7-3 loss: 0.579325  [  128/  179]
train() client id: f_00007-7-4 loss: 0.521691  [  160/  179]
train() client id: f_00007-8-0 loss: 0.611451  [   32/  179]
train() client id: f_00007-8-1 loss: 0.711082  [   64/  179]
train() client id: f_00007-8-2 loss: 0.663309  [   96/  179]
train() client id: f_00007-8-3 loss: 0.583209  [  128/  179]
train() client id: f_00007-8-4 loss: 0.541131  [  160/  179]
train() client id: f_00007-9-0 loss: 0.768099  [   32/  179]
train() client id: f_00007-9-1 loss: 0.509642  [   64/  179]
train() client id: f_00007-9-2 loss: 0.600701  [   96/  179]
train() client id: f_00007-9-3 loss: 0.725376  [  128/  179]
train() client id: f_00007-9-4 loss: 0.595056  [  160/  179]
train() client id: f_00007-10-0 loss: 0.590814  [   32/  179]
train() client id: f_00007-10-1 loss: 0.592729  [   64/  179]
train() client id: f_00007-10-2 loss: 0.691919  [   96/  179]
train() client id: f_00007-10-3 loss: 0.585757  [  128/  179]
train() client id: f_00007-10-4 loss: 0.525617  [  160/  179]
train() client id: f_00007-11-0 loss: 0.678874  [   32/  179]
train() client id: f_00007-11-1 loss: 0.717921  [   64/  179]
train() client id: f_00007-11-2 loss: 0.706778  [   96/  179]
train() client id: f_00007-11-3 loss: 0.609104  [  128/  179]
train() client id: f_00007-11-4 loss: 0.521130  [  160/  179]
train() client id: f_00007-12-0 loss: 0.492105  [   32/  179]
train() client id: f_00007-12-1 loss: 0.576840  [   64/  179]
train() client id: f_00007-12-2 loss: 0.554666  [   96/  179]
train() client id: f_00007-12-3 loss: 0.788276  [  128/  179]
train() client id: f_00007-12-4 loss: 0.652617  [  160/  179]
train() client id: f_00007-13-0 loss: 0.467705  [   32/  179]
train() client id: f_00007-13-1 loss: 0.687339  [   64/  179]
train() client id: f_00007-13-2 loss: 0.744602  [   96/  179]
train() client id: f_00007-13-3 loss: 0.587782  [  128/  179]
train() client id: f_00007-13-4 loss: 0.632353  [  160/  179]
train() client id: f_00007-14-0 loss: 0.562681  [   32/  179]
train() client id: f_00007-14-1 loss: 0.743008  [   64/  179]
train() client id: f_00007-14-2 loss: 0.560660  [   96/  179]
train() client id: f_00007-14-3 loss: 0.653170  [  128/  179]
train() client id: f_00007-14-4 loss: 0.725652  [  160/  179]
train() client id: f_00007-15-0 loss: 0.608935  [   32/  179]
train() client id: f_00007-15-1 loss: 0.595641  [   64/  179]
train() client id: f_00007-15-2 loss: 0.842243  [   96/  179]
train() client id: f_00007-15-3 loss: 0.668555  [  128/  179]
train() client id: f_00007-15-4 loss: 0.539203  [  160/  179]
train() client id: f_00007-16-0 loss: 0.675340  [   32/  179]
train() client id: f_00007-16-1 loss: 0.565851  [   64/  179]
train() client id: f_00007-16-2 loss: 0.603433  [   96/  179]
train() client id: f_00007-16-3 loss: 0.669628  [  128/  179]
train() client id: f_00007-16-4 loss: 0.556738  [  160/  179]
train() client id: f_00007-17-0 loss: 0.682766  [   32/  179]
train() client id: f_00007-17-1 loss: 0.544404  [   64/  179]
train() client id: f_00007-17-2 loss: 0.771760  [   96/  179]
train() client id: f_00007-17-3 loss: 0.558661  [  128/  179]
train() client id: f_00007-17-4 loss: 0.639639  [  160/  179]
train() client id: f_00008-0-0 loss: 0.788159  [   32/  130]
train() client id: f_00008-0-1 loss: 0.783001  [   64/  130]
train() client id: f_00008-0-2 loss: 0.808453  [   96/  130]
train() client id: f_00008-0-3 loss: 0.619205  [  128/  130]
train() client id: f_00008-1-0 loss: 0.745356  [   32/  130]
train() client id: f_00008-1-1 loss: 0.829570  [   64/  130]
train() client id: f_00008-1-2 loss: 0.709195  [   96/  130]
train() client id: f_00008-1-3 loss: 0.701394  [  128/  130]
train() client id: f_00008-2-0 loss: 0.703083  [   32/  130]
train() client id: f_00008-2-1 loss: 0.703863  [   64/  130]
train() client id: f_00008-2-2 loss: 0.769151  [   96/  130]
train() client id: f_00008-2-3 loss: 0.810177  [  128/  130]
train() client id: f_00008-3-0 loss: 0.711126  [   32/  130]
train() client id: f_00008-3-1 loss: 0.681510  [   64/  130]
train() client id: f_00008-3-2 loss: 0.736443  [   96/  130]
train() client id: f_00008-3-3 loss: 0.825480  [  128/  130]
train() client id: f_00008-4-0 loss: 0.840723  [   32/  130]
train() client id: f_00008-4-1 loss: 0.703548  [   64/  130]
train() client id: f_00008-4-2 loss: 0.755723  [   96/  130]
train() client id: f_00008-4-3 loss: 0.664406  [  128/  130]
train() client id: f_00008-5-0 loss: 0.884858  [   32/  130]
train() client id: f_00008-5-1 loss: 0.656201  [   64/  130]
train() client id: f_00008-5-2 loss: 0.690522  [   96/  130]
train() client id: f_00008-5-3 loss: 0.735845  [  128/  130]
train() client id: f_00008-6-0 loss: 0.771150  [   32/  130]
train() client id: f_00008-6-1 loss: 0.657613  [   64/  130]
train() client id: f_00008-6-2 loss: 0.789711  [   96/  130]
train() client id: f_00008-6-3 loss: 0.741996  [  128/  130]
train() client id: f_00008-7-0 loss: 0.608379  [   32/  130]
train() client id: f_00008-7-1 loss: 0.708376  [   64/  130]
train() client id: f_00008-7-2 loss: 0.771674  [   96/  130]
train() client id: f_00008-7-3 loss: 0.876928  [  128/  130]
train() client id: f_00008-8-0 loss: 0.748451  [   32/  130]
train() client id: f_00008-8-1 loss: 0.764958  [   64/  130]
train() client id: f_00008-8-2 loss: 0.725049  [   96/  130]
train() client id: f_00008-8-3 loss: 0.745704  [  128/  130]
train() client id: f_00008-9-0 loss: 0.676782  [   32/  130]
train() client id: f_00008-9-1 loss: 0.756581  [   64/  130]
train() client id: f_00008-9-2 loss: 0.865421  [   96/  130]
train() client id: f_00008-9-3 loss: 0.685744  [  128/  130]
train() client id: f_00008-10-0 loss: 0.779830  [   32/  130]
train() client id: f_00008-10-1 loss: 0.674038  [   64/  130]
train() client id: f_00008-10-2 loss: 0.775653  [   96/  130]
train() client id: f_00008-10-3 loss: 0.754931  [  128/  130]
train() client id: f_00008-11-0 loss: 0.714162  [   32/  130]
train() client id: f_00008-11-1 loss: 0.691901  [   64/  130]
train() client id: f_00008-11-2 loss: 0.833233  [   96/  130]
train() client id: f_00008-11-3 loss: 0.688942  [  128/  130]
train() client id: f_00008-12-0 loss: 0.743907  [   32/  130]
train() client id: f_00008-12-1 loss: 0.799003  [   64/  130]
train() client id: f_00008-12-2 loss: 0.713833  [   96/  130]
train() client id: f_00008-12-3 loss: 0.674664  [  128/  130]
train() client id: f_00008-13-0 loss: 0.794655  [   32/  130]
train() client id: f_00008-13-1 loss: 0.733871  [   64/  130]
train() client id: f_00008-13-2 loss: 0.656260  [   96/  130]
train() client id: f_00008-13-3 loss: 0.784202  [  128/  130]
train() client id: f_00008-14-0 loss: 0.746023  [   32/  130]
train() client id: f_00008-14-1 loss: 0.749273  [   64/  130]
train() client id: f_00008-14-2 loss: 0.800058  [   96/  130]
train() client id: f_00008-14-3 loss: 0.681798  [  128/  130]
train() client id: f_00008-15-0 loss: 0.692882  [   32/  130]
train() client id: f_00008-15-1 loss: 0.707880  [   64/  130]
train() client id: f_00008-15-2 loss: 0.769767  [   96/  130]
train() client id: f_00008-15-3 loss: 0.799282  [  128/  130]
train() client id: f_00008-16-0 loss: 0.685818  [   32/  130]
train() client id: f_00008-16-1 loss: 0.748209  [   64/  130]
train() client id: f_00008-16-2 loss: 0.820359  [   96/  130]
train() client id: f_00008-16-3 loss: 0.724255  [  128/  130]
train() client id: f_00008-17-0 loss: 0.670561  [   32/  130]
train() client id: f_00008-17-1 loss: 0.710251  [   64/  130]
train() client id: f_00008-17-2 loss: 0.736550  [   96/  130]
train() client id: f_00008-17-3 loss: 0.862548  [  128/  130]
train() client id: f_00009-0-0 loss: 1.254414  [   32/  118]
train() client id: f_00009-0-1 loss: 0.994956  [   64/  118]
train() client id: f_00009-0-2 loss: 1.209437  [   96/  118]
train() client id: f_00009-1-0 loss: 1.070272  [   32/  118]
train() client id: f_00009-1-1 loss: 0.983577  [   64/  118]
train() client id: f_00009-1-2 loss: 1.181973  [   96/  118]
train() client id: f_00009-2-0 loss: 1.171708  [   32/  118]
train() client id: f_00009-2-1 loss: 0.962523  [   64/  118]
train() client id: f_00009-2-2 loss: 0.978277  [   96/  118]
train() client id: f_00009-3-0 loss: 1.040150  [   32/  118]
train() client id: f_00009-3-1 loss: 1.135941  [   64/  118]
train() client id: f_00009-3-2 loss: 0.832565  [   96/  118]
train() client id: f_00009-4-0 loss: 1.092880  [   32/  118]
train() client id: f_00009-4-1 loss: 0.820940  [   64/  118]
train() client id: f_00009-4-2 loss: 0.898387  [   96/  118]
train() client id: f_00009-5-0 loss: 0.933985  [   32/  118]
train() client id: f_00009-5-1 loss: 0.878389  [   64/  118]
train() client id: f_00009-5-2 loss: 0.944210  [   96/  118]
train() client id: f_00009-6-0 loss: 0.947813  [   32/  118]
train() client id: f_00009-6-1 loss: 0.919635  [   64/  118]
train() client id: f_00009-6-2 loss: 0.848719  [   96/  118]
train() client id: f_00009-7-0 loss: 0.825077  [   32/  118]
train() client id: f_00009-7-1 loss: 0.875075  [   64/  118]
train() client id: f_00009-7-2 loss: 0.806280  [   96/  118]
train() client id: f_00009-8-0 loss: 0.840195  [   32/  118]
train() client id: f_00009-8-1 loss: 0.740470  [   64/  118]
train() client id: f_00009-8-2 loss: 1.008274  [   96/  118]
train() client id: f_00009-9-0 loss: 0.778827  [   32/  118]
train() client id: f_00009-9-1 loss: 0.947511  [   64/  118]
train() client id: f_00009-9-2 loss: 0.782781  [   96/  118]
train() client id: f_00009-10-0 loss: 0.702093  [   32/  118]
train() client id: f_00009-10-1 loss: 0.701516  [   64/  118]
train() client id: f_00009-10-2 loss: 1.064081  [   96/  118]
train() client id: f_00009-11-0 loss: 0.634742  [   32/  118]
train() client id: f_00009-11-1 loss: 0.802732  [   64/  118]
train() client id: f_00009-11-2 loss: 1.009518  [   96/  118]
train() client id: f_00009-12-0 loss: 0.994250  [   32/  118]
train() client id: f_00009-12-1 loss: 0.799269  [   64/  118]
train() client id: f_00009-12-2 loss: 0.558452  [   96/  118]
train() client id: f_00009-13-0 loss: 0.724622  [   32/  118]
train() client id: f_00009-13-1 loss: 0.733472  [   64/  118]
train() client id: f_00009-13-2 loss: 0.918717  [   96/  118]
train() client id: f_00009-14-0 loss: 0.947041  [   32/  118]
train() client id: f_00009-14-1 loss: 0.667078  [   64/  118]
train() client id: f_00009-14-2 loss: 0.649392  [   96/  118]
train() client id: f_00009-15-0 loss: 0.967703  [   32/  118]
train() client id: f_00009-15-1 loss: 0.643101  [   64/  118]
train() client id: f_00009-15-2 loss: 0.648765  [   96/  118]
train() client id: f_00009-16-0 loss: 0.751172  [   32/  118]
train() client id: f_00009-16-1 loss: 0.979098  [   64/  118]
train() client id: f_00009-16-2 loss: 0.674465  [   96/  118]
train() client id: f_00009-17-0 loss: 0.672164  [   32/  118]
train() client id: f_00009-17-1 loss: 0.824327  [   64/  118]
train() client id: f_00009-17-2 loss: 0.801817  [   96/  118]
At round 61 accuracy: 0.649867374005305
At round 61 training accuracy: 0.590878604963112
At round 61 training loss: 0.8388175337366541
update_location
xs = 8.927491 426.223621 5.882650 0.934260 -342.581990 -190.230757 -150.849135 -5.143845 -365.120581 20.134486 
ys = -417.390647 7.291448 315.684448 -137.290817 -9.642386 0.794442 -1.381692 311.628436 25.881276 -852.232496 
xs mean: -59.18237997052123
ys mean: -75.66579882624052
dists_uav = 429.295530 437.858128 331.196734 169.851822 357.008957 214.914802 180.989974 327.320548 379.450760 858.315574 
uav_gains = -122.390106 -122.650225 -118.050265 -105.774258 -119.543541 -108.645381 -106.490373 -117.793935 -120.591739 -130.313164 
uav_gains_db_mean: -117.22429847732266
dists_bs = 620.287031 624.176258 229.152017 358.438863 249.352229 174.870101 178.027430 217.987254 241.624148 1045.602251 
bs_gains = -117.759655 -117.835662 -105.650522 -111.090682 -106.677830 -102.363095 -102.580693 -105.043130 -106.294989 -124.109321 
bs_gains_db_mean: -109.94055788647782
Round 62
-------------------------------
ene_coms = [0.03900642 0.04059477 0.0082229  0.00827777 0.00870098 0.0097829
 0.00860978 0.00796347 0.00851668 0.0513396 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.93934548 5.89124104 2.68108886 0.96779732 6.53492542 3.15747512
 1.20284782 3.84287644 2.81008864 2.85660956]
obj_prev = 32.88429570088656
eta_min = 1.776929743681163e-33	eta_max = 0.8664538097474143
af = 6.781268905735956	bf = 1.3265553560143797	zeta = 7.459395796309552	eta = 0.9090909090909091
af = 6.781268905735956	bf = 1.3265553560143797	zeta = 21.701672461937058	eta = 0.31247678802773116
af = 6.781268905735956	bf = 1.3265553560143797	zeta = 13.402743841488508	eta = 0.5059612409172799
af = 6.781268905735956	bf = 1.3265553560143797	zeta = 12.03671023939001	eta = 0.5633822507037117
af = 6.781268905735956	bf = 1.3265553560143797	zeta = 11.950101959968777	eta = 0.5674653595803859
af = 6.781268905735956	bf = 1.3265553560143797	zeta = 11.949694154707485	eta = 0.5674847253780575
af = 6.781268905735956	bf = 1.3265553560143797	zeta = 11.949694145577883	eta = 0.5674847258116176
eta = 0.5674847258116176
ene_coms = [0.03900642 0.04059477 0.0082229  0.00827777 0.00870098 0.0097829
 0.00860978 0.00796347 0.00851668 0.0513396 ]
ene_comp = [0.04674964 0.09832266 0.04600758 0.01595424 0.11353485 0.05417022
 0.02003556 0.06641417 0.04823376 0.04378141]
ene_total = [1.37695792 2.23055312 0.8707617  0.3890858  1.96270175 1.02687501
 0.45994919 1.19425821 0.91122367 1.52732778]
ti_comp = [1.2816375  1.265754   1.58947272 1.588924   1.58469198 1.5738727
 1.58560391 1.59206704 1.58653499 1.15830579]
ti_coms = [0.39006425 0.40594775 0.08222902 0.08277774 0.08700976 0.09782905
 0.08609783 0.07963471 0.08516675 0.51339596]
t_total = [26.8419838 26.8419838 26.8419838 26.8419838 26.8419838 26.8419838
 26.8419838 26.8419838 26.8419838 26.8419838]
ene_coms = [0.03900642 0.04059477 0.0082229  0.00827777 0.00870098 0.0097829
 0.00860978 0.00796347 0.00851668 0.0513396 ]
ene_comp = [3.88762601e-06 3.70802165e-05 2.40914063e-06 1.00531548e-07
 3.64231759e-05 4.01072254e-06 1.99937895e-07 7.22336641e-06
 2.78633929e-06 3.90933441e-06]
ene_total = [0.62637619 0.65241279 0.13207122 0.1329152  0.14029365 0.15714539
 0.13824776 0.1279829  0.13679428 0.82440635]
optimize_network iter = 0 obj = 3.0686457399171783
eta = 0.5674847258116176
freqs = [18238246.86926623 38839561.34829397 14472593.24814237  5020455.01189726
 35822370.02659089 17209212.78163807  6317958.96436362 20857844.73353443
 15200975.05280364 18898900.15795727]
eta_min = 0.5674847258116256	eta_max = 0.5674847258116172
af = 0.0012016730269059335	bf = 1.3265553560143797	zeta = 0.001321840329596527	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03900642 0.04059477 0.0082229  0.00827777 0.00870098 0.0097829
 0.00860978 0.00796347 0.00851668 0.0513396 ]
ene_comp = [6.54021585e-07 6.23806454e-06 4.05293609e-07 1.69125842e-08
 6.12752956e-06 6.74730313e-07 3.36358741e-08 1.21519857e-06
 4.68750347e-07 6.57673625e-07]
ene_total = [2.97984296 3.10160734 0.62819792 0.63236003 0.66515624 0.74739077
 0.65772426 0.60844119 0.65064475 3.92200331]
ti_comp = [1.2816375  1.265754   1.58947272 1.588924   1.58469198 1.5738727
 1.58560391 1.59206704 1.58653499 1.15830579]
ti_coms = [0.39006425 0.40594775 0.08222902 0.08277774 0.08700976 0.09782905
 0.08609783 0.07963471 0.08516675 0.51339596]
t_total = [26.8419838 26.8419838 26.8419838 26.8419838 26.8419838 26.8419838
 26.8419838 26.8419838 26.8419838 26.8419838]
ene_coms = [0.03900642 0.04059477 0.0082229  0.00827777 0.00870098 0.0097829
 0.00860978 0.00796347 0.00851668 0.0513396 ]
ene_comp = [3.88762601e-06 3.70802165e-05 2.40914063e-06 1.00531548e-07
 3.64231759e-05 4.01072254e-06 1.99937895e-07 7.22336641e-06
 2.78633929e-06 3.90933441e-06]
ene_total = [0.62637619 0.65241279 0.13207122 0.1329152  0.14029365 0.15714539
 0.13824776 0.1279829  0.13679428 0.82440635]
optimize_network iter = 1 obj = 3.0686457399171756
eta = 0.5674847258116172
freqs = [18238246.86926623 38839561.34829396 14472593.24814237  5020455.01189726
 35822370.02659089 17209212.78163807  6317958.96436362 20857844.73353443
 15200975.05280364 18898900.15795727]
Done!
ene_coms = [0.03900642 0.04059477 0.0082229  0.00827777 0.00870098 0.0097829
 0.00860978 0.00796347 0.00851668 0.0513396 ]
ene_comp = [3.77206558e-06 3.59779999e-05 2.33752846e-06 9.75432282e-08
 3.53404900e-05 3.89150304e-06 1.93994703e-07 7.00865046e-06
 2.70351482e-06 3.79312870e-06]
ene_total = [0.0390102  0.04063075 0.00822524 0.00827787 0.00873632 0.0097868
 0.00860998 0.00797048 0.00851938 0.05134339]
At round 62 energy consumption: 0.19111039848411177
At round 62 eta: 0.5674847258116172
At round 62 a_n: 6.944760344156705
At round 62 local rounds: 18.551445280380303
At round 62 global rounds: 16.056682292177047
gradient difference: 0.3713594079017639
train() client id: f_00000-0-0 loss: 1.516639  [   32/  126]
train() client id: f_00000-0-1 loss: 1.261684  [   64/  126]
train() client id: f_00000-0-2 loss: 1.184932  [   96/  126]
train() client id: f_00000-1-0 loss: 1.374012  [   32/  126]
train() client id: f_00000-1-1 loss: 0.821442  [   64/  126]
train() client id: f_00000-1-2 loss: 1.200675  [   96/  126]
train() client id: f_00000-2-0 loss: 1.021468  [   32/  126]
train() client id: f_00000-2-1 loss: 1.023936  [   64/  126]
train() client id: f_00000-2-2 loss: 0.887007  [   96/  126]
train() client id: f_00000-3-0 loss: 0.857548  [   32/  126]
train() client id: f_00000-3-1 loss: 0.843665  [   64/  126]
train() client id: f_00000-3-2 loss: 1.231566  [   96/  126]
train() client id: f_00000-4-0 loss: 1.047340  [   32/  126]
train() client id: f_00000-4-1 loss: 0.901567  [   64/  126]
train() client id: f_00000-4-2 loss: 0.845295  [   96/  126]
train() client id: f_00000-5-0 loss: 0.932320  [   32/  126]
train() client id: f_00000-5-1 loss: 0.908419  [   64/  126]
train() client id: f_00000-5-2 loss: 0.802315  [   96/  126]
train() client id: f_00000-6-0 loss: 0.869290  [   32/  126]
train() client id: f_00000-6-1 loss: 0.818743  [   64/  126]
train() client id: f_00000-6-2 loss: 0.949116  [   96/  126]
train() client id: f_00000-7-0 loss: 0.826754  [   32/  126]
train() client id: f_00000-7-1 loss: 0.711393  [   64/  126]
train() client id: f_00000-7-2 loss: 0.874500  [   96/  126]
train() client id: f_00000-8-0 loss: 0.842353  [   32/  126]
train() client id: f_00000-8-1 loss: 0.815614  [   64/  126]
train() client id: f_00000-8-2 loss: 0.781816  [   96/  126]
train() client id: f_00000-9-0 loss: 0.867425  [   32/  126]
train() client id: f_00000-9-1 loss: 0.617779  [   64/  126]
train() client id: f_00000-9-2 loss: 0.783906  [   96/  126]
train() client id: f_00000-10-0 loss: 0.670455  [   32/  126]
train() client id: f_00000-10-1 loss: 0.852859  [   64/  126]
train() client id: f_00000-10-2 loss: 0.750126  [   96/  126]
train() client id: f_00000-11-0 loss: 0.759121  [   32/  126]
train() client id: f_00000-11-1 loss: 0.823495  [   64/  126]
train() client id: f_00000-11-2 loss: 0.747656  [   96/  126]
train() client id: f_00000-12-0 loss: 0.687601  [   32/  126]
train() client id: f_00000-12-1 loss: 0.832301  [   64/  126]
train() client id: f_00000-12-2 loss: 0.744274  [   96/  126]
train() client id: f_00000-13-0 loss: 0.583319  [   32/  126]
train() client id: f_00000-13-1 loss: 0.836959  [   64/  126]
train() client id: f_00000-13-2 loss: 0.711154  [   96/  126]
train() client id: f_00000-14-0 loss: 0.853185  [   32/  126]
train() client id: f_00000-14-1 loss: 0.617786  [   64/  126]
train() client id: f_00000-14-2 loss: 0.736348  [   96/  126]
train() client id: f_00000-15-0 loss: 0.787031  [   32/  126]
train() client id: f_00000-15-1 loss: 0.824531  [   64/  126]
train() client id: f_00000-15-2 loss: 0.653685  [   96/  126]
train() client id: f_00000-16-0 loss: 0.843025  [   32/  126]
train() client id: f_00000-16-1 loss: 0.617059  [   64/  126]
train() client id: f_00000-16-2 loss: 0.723124  [   96/  126]
train() client id: f_00000-17-0 loss: 0.835606  [   32/  126]
train() client id: f_00000-17-1 loss: 0.842476  [   64/  126]
train() client id: f_00000-17-2 loss: 0.677866  [   96/  126]
train() client id: f_00001-0-0 loss: 0.527975  [   32/  265]
train() client id: f_00001-0-1 loss: 0.392667  [   64/  265]
train() client id: f_00001-0-2 loss: 0.386020  [   96/  265]
train() client id: f_00001-0-3 loss: 0.323425  [  128/  265]
train() client id: f_00001-0-4 loss: 0.357462  [  160/  265]
train() client id: f_00001-0-5 loss: 0.396292  [  192/  265]
train() client id: f_00001-0-6 loss: 0.368016  [  224/  265]
train() client id: f_00001-0-7 loss: 0.328684  [  256/  265]
train() client id: f_00001-1-0 loss: 0.450039  [   32/  265]
train() client id: f_00001-1-1 loss: 0.324264  [   64/  265]
train() client id: f_00001-1-2 loss: 0.382352  [   96/  265]
train() client id: f_00001-1-3 loss: 0.320848  [  128/  265]
train() client id: f_00001-1-4 loss: 0.558150  [  160/  265]
train() client id: f_00001-1-5 loss: 0.376795  [  192/  265]
train() client id: f_00001-1-6 loss: 0.370201  [  224/  265]
train() client id: f_00001-1-7 loss: 0.302851  [  256/  265]
train() client id: f_00001-2-0 loss: 0.334943  [   32/  265]
train() client id: f_00001-2-1 loss: 0.384980  [   64/  265]
train() client id: f_00001-2-2 loss: 0.449353  [   96/  265]
train() client id: f_00001-2-3 loss: 0.418934  [  128/  265]
train() client id: f_00001-2-4 loss: 0.314352  [  160/  265]
train() client id: f_00001-2-5 loss: 0.413552  [  192/  265]
train() client id: f_00001-2-6 loss: 0.261179  [  224/  265]
train() client id: f_00001-2-7 loss: 0.451143  [  256/  265]
train() client id: f_00001-3-0 loss: 0.289699  [   32/  265]
train() client id: f_00001-3-1 loss: 0.288566  [   64/  265]
train() client id: f_00001-3-2 loss: 0.313776  [   96/  265]
train() client id: f_00001-3-3 loss: 0.413618  [  128/  265]
train() client id: f_00001-3-4 loss: 0.380571  [  160/  265]
train() client id: f_00001-3-5 loss: 0.380901  [  192/  265]
train() client id: f_00001-3-6 loss: 0.599001  [  224/  265]
train() client id: f_00001-3-7 loss: 0.331559  [  256/  265]
train() client id: f_00001-4-0 loss: 0.278508  [   32/  265]
train() client id: f_00001-4-1 loss: 0.362210  [   64/  265]
train() client id: f_00001-4-2 loss: 0.459356  [   96/  265]
train() client id: f_00001-4-3 loss: 0.334266  [  128/  265]
train() client id: f_00001-4-4 loss: 0.338106  [  160/  265]
train() client id: f_00001-4-5 loss: 0.474711  [  192/  265]
train() client id: f_00001-4-6 loss: 0.319028  [  224/  265]
train() client id: f_00001-4-7 loss: 0.368329  [  256/  265]
train() client id: f_00001-5-0 loss: 0.315375  [   32/  265]
train() client id: f_00001-5-1 loss: 0.395802  [   64/  265]
train() client id: f_00001-5-2 loss: 0.261485  [   96/  265]
train() client id: f_00001-5-3 loss: 0.314403  [  128/  265]
train() client id: f_00001-5-4 loss: 0.354382  [  160/  265]
train() client id: f_00001-5-5 loss: 0.437487  [  192/  265]
train() client id: f_00001-5-6 loss: 0.415092  [  224/  265]
train() client id: f_00001-5-7 loss: 0.427176  [  256/  265]
train() client id: f_00001-6-0 loss: 0.281736  [   32/  265]
train() client id: f_00001-6-1 loss: 0.337296  [   64/  265]
train() client id: f_00001-6-2 loss: 0.462190  [   96/  265]
train() client id: f_00001-6-3 loss: 0.268510  [  128/  265]
train() client id: f_00001-6-4 loss: 0.309733  [  160/  265]
train() client id: f_00001-6-5 loss: 0.405231  [  192/  265]
train() client id: f_00001-6-6 loss: 0.378971  [  224/  265]
train() client id: f_00001-6-7 loss: 0.455045  [  256/  265]
train() client id: f_00001-7-0 loss: 0.311914  [   32/  265]
train() client id: f_00001-7-1 loss: 0.355698  [   64/  265]
train() client id: f_00001-7-2 loss: 0.403703  [   96/  265]
train() client id: f_00001-7-3 loss: 0.321313  [  128/  265]
train() client id: f_00001-7-4 loss: 0.264832  [  160/  265]
train() client id: f_00001-7-5 loss: 0.423757  [  192/  265]
train() client id: f_00001-7-6 loss: 0.416339  [  224/  265]
train() client id: f_00001-7-7 loss: 0.365990  [  256/  265]
train() client id: f_00001-8-0 loss: 0.278253  [   32/  265]
train() client id: f_00001-8-1 loss: 0.490615  [   64/  265]
train() client id: f_00001-8-2 loss: 0.470000  [   96/  265]
train() client id: f_00001-8-3 loss: 0.261129  [  128/  265]
train() client id: f_00001-8-4 loss: 0.323609  [  160/  265]
train() client id: f_00001-8-5 loss: 0.342603  [  192/  265]
train() client id: f_00001-8-6 loss: 0.240193  [  224/  265]
train() client id: f_00001-8-7 loss: 0.374977  [  256/  265]
train() client id: f_00001-9-0 loss: 0.348027  [   32/  265]
train() client id: f_00001-9-1 loss: 0.439505  [   64/  265]
train() client id: f_00001-9-2 loss: 0.375112  [   96/  265]
train() client id: f_00001-9-3 loss: 0.280893  [  128/  265]
train() client id: f_00001-9-4 loss: 0.345974  [  160/  265]
train() client id: f_00001-9-5 loss: 0.277712  [  192/  265]
train() client id: f_00001-9-6 loss: 0.317565  [  224/  265]
train() client id: f_00001-9-7 loss: 0.399653  [  256/  265]
train() client id: f_00001-10-0 loss: 0.244729  [   32/  265]
train() client id: f_00001-10-1 loss: 0.288457  [   64/  265]
train() client id: f_00001-10-2 loss: 0.335542  [   96/  265]
train() client id: f_00001-10-3 loss: 0.335177  [  128/  265]
train() client id: f_00001-10-4 loss: 0.329814  [  160/  265]
train() client id: f_00001-10-5 loss: 0.468560  [  192/  265]
train() client id: f_00001-10-6 loss: 0.339987  [  224/  265]
train() client id: f_00001-10-7 loss: 0.464316  [  256/  265]
train() client id: f_00001-11-0 loss: 0.381010  [   32/  265]
train() client id: f_00001-11-1 loss: 0.326888  [   64/  265]
train() client id: f_00001-11-2 loss: 0.443085  [   96/  265]
train() client id: f_00001-11-3 loss: 0.388343  [  128/  265]
train() client id: f_00001-11-4 loss: 0.264225  [  160/  265]
train() client id: f_00001-11-5 loss: 0.309358  [  192/  265]
train() client id: f_00001-11-6 loss: 0.321108  [  224/  265]
train() client id: f_00001-11-7 loss: 0.410393  [  256/  265]
train() client id: f_00001-12-0 loss: 0.277177  [   32/  265]
train() client id: f_00001-12-1 loss: 0.304468  [   64/  265]
train() client id: f_00001-12-2 loss: 0.436175  [   96/  265]
train() client id: f_00001-12-3 loss: 0.340024  [  128/  265]
train() client id: f_00001-12-4 loss: 0.241688  [  160/  265]
train() client id: f_00001-12-5 loss: 0.444810  [  192/  265]
train() client id: f_00001-12-6 loss: 0.403522  [  224/  265]
train() client id: f_00001-12-7 loss: 0.388161  [  256/  265]
train() client id: f_00001-13-0 loss: 0.313526  [   32/  265]
train() client id: f_00001-13-1 loss: 0.386831  [   64/  265]
train() client id: f_00001-13-2 loss: 0.272175  [   96/  265]
train() client id: f_00001-13-3 loss: 0.247593  [  128/  265]
train() client id: f_00001-13-4 loss: 0.519554  [  160/  265]
train() client id: f_00001-13-5 loss: 0.303992  [  192/  265]
train() client id: f_00001-13-6 loss: 0.350600  [  224/  265]
train() client id: f_00001-13-7 loss: 0.443053  [  256/  265]
train() client id: f_00001-14-0 loss: 0.337980  [   32/  265]
train() client id: f_00001-14-1 loss: 0.301359  [   64/  265]
train() client id: f_00001-14-2 loss: 0.300771  [   96/  265]
train() client id: f_00001-14-3 loss: 0.449841  [  128/  265]
train() client id: f_00001-14-4 loss: 0.380559  [  160/  265]
train() client id: f_00001-14-5 loss: 0.341028  [  192/  265]
train() client id: f_00001-14-6 loss: 0.405302  [  224/  265]
train() client id: f_00001-14-7 loss: 0.318406  [  256/  265]
train() client id: f_00001-15-0 loss: 0.473574  [   32/  265]
train() client id: f_00001-15-1 loss: 0.341061  [   64/  265]
train() client id: f_00001-15-2 loss: 0.365357  [   96/  265]
train() client id: f_00001-15-3 loss: 0.309808  [  128/  265]
train() client id: f_00001-15-4 loss: 0.435021  [  160/  265]
train() client id: f_00001-15-5 loss: 0.248675  [  192/  265]
train() client id: f_00001-15-6 loss: 0.355443  [  224/  265]
train() client id: f_00001-15-7 loss: 0.308264  [  256/  265]
train() client id: f_00001-16-0 loss: 0.272830  [   32/  265]
train() client id: f_00001-16-1 loss: 0.287791  [   64/  265]
train() client id: f_00001-16-2 loss: 0.473939  [   96/  265]
train() client id: f_00001-16-3 loss: 0.264133  [  128/  265]
train() client id: f_00001-16-4 loss: 0.364826  [  160/  265]
train() client id: f_00001-16-5 loss: 0.345323  [  192/  265]
train() client id: f_00001-16-6 loss: 0.297838  [  224/  265]
train() client id: f_00001-16-7 loss: 0.528065  [  256/  265]
train() client id: f_00001-17-0 loss: 0.255999  [   32/  265]
train() client id: f_00001-17-1 loss: 0.272623  [   64/  265]
train() client id: f_00001-17-2 loss: 0.345860  [   96/  265]
train() client id: f_00001-17-3 loss: 0.254590  [  128/  265]
train() client id: f_00001-17-4 loss: 0.384370  [  160/  265]
train() client id: f_00001-17-5 loss: 0.417848  [  192/  265]
train() client id: f_00001-17-6 loss: 0.320321  [  224/  265]
train() client id: f_00001-17-7 loss: 0.505338  [  256/  265]
train() client id: f_00002-0-0 loss: 1.407243  [   32/  124]
train() client id: f_00002-0-1 loss: 1.332136  [   64/  124]
train() client id: f_00002-0-2 loss: 1.202861  [   96/  124]
train() client id: f_00002-1-0 loss: 1.198462  [   32/  124]
train() client id: f_00002-1-1 loss: 1.228124  [   64/  124]
train() client id: f_00002-1-2 loss: 1.347041  [   96/  124]
train() client id: f_00002-2-0 loss: 1.215554  [   32/  124]
train() client id: f_00002-2-1 loss: 1.212753  [   64/  124]
train() client id: f_00002-2-2 loss: 1.124338  [   96/  124]
train() client id: f_00002-3-0 loss: 1.214426  [   32/  124]
train() client id: f_00002-3-1 loss: 1.210567  [   64/  124]
train() client id: f_00002-3-2 loss: 1.190731  [   96/  124]
train() client id: f_00002-4-0 loss: 1.204009  [   32/  124]
train() client id: f_00002-4-1 loss: 1.234579  [   64/  124]
train() client id: f_00002-4-2 loss: 1.216164  [   96/  124]
train() client id: f_00002-5-0 loss: 1.027337  [   32/  124]
train() client id: f_00002-5-1 loss: 1.210387  [   64/  124]
train() client id: f_00002-5-2 loss: 1.182425  [   96/  124]
train() client id: f_00002-6-0 loss: 1.172643  [   32/  124]
train() client id: f_00002-6-1 loss: 1.008889  [   64/  124]
train() client id: f_00002-6-2 loss: 1.202033  [   96/  124]
train() client id: f_00002-7-0 loss: 1.100746  [   32/  124]
train() client id: f_00002-7-1 loss: 1.102037  [   64/  124]
train() client id: f_00002-7-2 loss: 1.133832  [   96/  124]
train() client id: f_00002-8-0 loss: 1.090795  [   32/  124]
train() client id: f_00002-8-1 loss: 1.047220  [   64/  124]
train() client id: f_00002-8-2 loss: 1.164389  [   96/  124]
train() client id: f_00002-9-0 loss: 1.179183  [   32/  124]
train() client id: f_00002-9-1 loss: 1.186553  [   64/  124]
train() client id: f_00002-9-2 loss: 0.873526  [   96/  124]
train() client id: f_00002-10-0 loss: 1.161992  [   32/  124]
train() client id: f_00002-10-1 loss: 1.050853  [   64/  124]
train() client id: f_00002-10-2 loss: 1.040903  [   96/  124]
train() client id: f_00002-11-0 loss: 0.959818  [   32/  124]
train() client id: f_00002-11-1 loss: 0.955590  [   64/  124]
train() client id: f_00002-11-2 loss: 1.025838  [   96/  124]
train() client id: f_00002-12-0 loss: 1.115309  [   32/  124]
train() client id: f_00002-12-1 loss: 0.775864  [   64/  124]
train() client id: f_00002-12-2 loss: 1.212377  [   96/  124]
train() client id: f_00002-13-0 loss: 0.979503  [   32/  124]
train() client id: f_00002-13-1 loss: 0.949269  [   64/  124]
train() client id: f_00002-13-2 loss: 1.044849  [   96/  124]
train() client id: f_00002-14-0 loss: 0.943467  [   32/  124]
train() client id: f_00002-14-1 loss: 1.134648  [   64/  124]
train() client id: f_00002-14-2 loss: 0.954552  [   96/  124]
train() client id: f_00002-15-0 loss: 1.038722  [   32/  124]
train() client id: f_00002-15-1 loss: 1.104319  [   64/  124]
train() client id: f_00002-15-2 loss: 1.098223  [   96/  124]
train() client id: f_00002-16-0 loss: 1.105327  [   32/  124]
train() client id: f_00002-16-1 loss: 1.114708  [   64/  124]
train() client id: f_00002-16-2 loss: 0.788660  [   96/  124]
train() client id: f_00002-17-0 loss: 1.055268  [   32/  124]
train() client id: f_00002-17-1 loss: 0.814023  [   64/  124]
train() client id: f_00002-17-2 loss: 1.162380  [   96/  124]
train() client id: f_00003-0-0 loss: 0.527070  [   32/   43]
train() client id: f_00003-1-0 loss: 0.368499  [   32/   43]
train() client id: f_00003-2-0 loss: 0.487801  [   32/   43]
train() client id: f_00003-3-0 loss: 0.738262  [   32/   43]
train() client id: f_00003-4-0 loss: 0.302883  [   32/   43]
train() client id: f_00003-5-0 loss: 0.432131  [   32/   43]
train() client id: f_00003-6-0 loss: 0.490342  [   32/   43]
train() client id: f_00003-7-0 loss: 0.500898  [   32/   43]
train() client id: f_00003-8-0 loss: 0.674158  [   32/   43]
train() client id: f_00003-9-0 loss: 0.504409  [   32/   43]
train() client id: f_00003-10-0 loss: 0.551042  [   32/   43]
train() client id: f_00003-11-0 loss: 0.670903  [   32/   43]
train() client id: f_00003-12-0 loss: 0.547721  [   32/   43]
train() client id: f_00003-13-0 loss: 0.506643  [   32/   43]
train() client id: f_00003-14-0 loss: 0.325482  [   32/   43]
train() client id: f_00003-15-0 loss: 0.599649  [   32/   43]
train() client id: f_00003-16-0 loss: 0.472704  [   32/   43]
train() client id: f_00003-17-0 loss: 0.442183  [   32/   43]
train() client id: f_00004-0-0 loss: 0.800006  [   32/  306]
train() client id: f_00004-0-1 loss: 0.816144  [   64/  306]
train() client id: f_00004-0-2 loss: 0.737693  [   96/  306]
train() client id: f_00004-0-3 loss: 0.751485  [  128/  306]
train() client id: f_00004-0-4 loss: 0.875088  [  160/  306]
train() client id: f_00004-0-5 loss: 0.766905  [  192/  306]
train() client id: f_00004-0-6 loss: 0.683544  [  224/  306]
train() client id: f_00004-0-7 loss: 0.749299  [  256/  306]
train() client id: f_00004-0-8 loss: 0.553784  [  288/  306]
train() client id: f_00004-1-0 loss: 0.749324  [   32/  306]
train() client id: f_00004-1-1 loss: 0.731308  [   64/  306]
train() client id: f_00004-1-2 loss: 0.815518  [   96/  306]
train() client id: f_00004-1-3 loss: 0.785579  [  128/  306]
train() client id: f_00004-1-4 loss: 0.705644  [  160/  306]
train() client id: f_00004-1-5 loss: 0.738943  [  192/  306]
train() client id: f_00004-1-6 loss: 0.571353  [  224/  306]
train() client id: f_00004-1-7 loss: 0.907434  [  256/  306]
train() client id: f_00004-1-8 loss: 0.749202  [  288/  306]
train() client id: f_00004-2-0 loss: 0.752583  [   32/  306]
train() client id: f_00004-2-1 loss: 0.737193  [   64/  306]
train() client id: f_00004-2-2 loss: 0.782948  [   96/  306]
train() client id: f_00004-2-3 loss: 0.740994  [  128/  306]
train() client id: f_00004-2-4 loss: 0.784200  [  160/  306]
train() client id: f_00004-2-5 loss: 0.626715  [  192/  306]
train() client id: f_00004-2-6 loss: 0.916346  [  224/  306]
train() client id: f_00004-2-7 loss: 0.741631  [  256/  306]
train() client id: f_00004-2-8 loss: 0.676227  [  288/  306]
train() client id: f_00004-3-0 loss: 0.668537  [   32/  306]
train() client id: f_00004-3-1 loss: 0.628219  [   64/  306]
train() client id: f_00004-3-2 loss: 0.663904  [   96/  306]
train() client id: f_00004-3-3 loss: 0.717462  [  128/  306]
train() client id: f_00004-3-4 loss: 0.893502  [  160/  306]
train() client id: f_00004-3-5 loss: 0.919091  [  192/  306]
train() client id: f_00004-3-6 loss: 0.935733  [  224/  306]
train() client id: f_00004-3-7 loss: 0.761934  [  256/  306]
train() client id: f_00004-3-8 loss: 0.665261  [  288/  306]
train() client id: f_00004-4-0 loss: 0.661205  [   32/  306]
train() client id: f_00004-4-1 loss: 0.729583  [   64/  306]
train() client id: f_00004-4-2 loss: 0.752806  [   96/  306]
train() client id: f_00004-4-3 loss: 0.704721  [  128/  306]
train() client id: f_00004-4-4 loss: 0.821887  [  160/  306]
train() client id: f_00004-4-5 loss: 0.707019  [  192/  306]
train() client id: f_00004-4-6 loss: 0.761259  [  224/  306]
train() client id: f_00004-4-7 loss: 0.798730  [  256/  306]
train() client id: f_00004-4-8 loss: 0.868903  [  288/  306]
train() client id: f_00004-5-0 loss: 0.822736  [   32/  306]
train() client id: f_00004-5-1 loss: 0.678942  [   64/  306]
train() client id: f_00004-5-2 loss: 0.647138  [   96/  306]
train() client id: f_00004-5-3 loss: 0.669001  [  128/  306]
train() client id: f_00004-5-4 loss: 0.730232  [  160/  306]
train() client id: f_00004-5-5 loss: 0.932431  [  192/  306]
train() client id: f_00004-5-6 loss: 0.779951  [  224/  306]
train() client id: f_00004-5-7 loss: 0.786910  [  256/  306]
train() client id: f_00004-5-8 loss: 0.819354  [  288/  306]
train() client id: f_00004-6-0 loss: 0.575759  [   32/  306]
train() client id: f_00004-6-1 loss: 0.872515  [   64/  306]
train() client id: f_00004-6-2 loss: 0.810061  [   96/  306]
train() client id: f_00004-6-3 loss: 0.865263  [  128/  306]
train() client id: f_00004-6-4 loss: 0.763816  [  160/  306]
train() client id: f_00004-6-5 loss: 0.794801  [  192/  306]
train() client id: f_00004-6-6 loss: 0.676886  [  224/  306]
train() client id: f_00004-6-7 loss: 0.752548  [  256/  306]
train() client id: f_00004-6-8 loss: 0.788761  [  288/  306]
train() client id: f_00004-7-0 loss: 0.675352  [   32/  306]
train() client id: f_00004-7-1 loss: 0.836220  [   64/  306]
train() client id: f_00004-7-2 loss: 0.907242  [   96/  306]
train() client id: f_00004-7-3 loss: 0.723694  [  128/  306]
train() client id: f_00004-7-4 loss: 0.719355  [  160/  306]
train() client id: f_00004-7-5 loss: 0.713636  [  192/  306]
train() client id: f_00004-7-6 loss: 0.682195  [  224/  306]
train() client id: f_00004-7-7 loss: 0.707175  [  256/  306]
train() client id: f_00004-7-8 loss: 0.862281  [  288/  306]
train() client id: f_00004-8-0 loss: 0.733403  [   32/  306]
train() client id: f_00004-8-1 loss: 0.799146  [   64/  306]
train() client id: f_00004-8-2 loss: 0.757694  [   96/  306]
train() client id: f_00004-8-3 loss: 0.817370  [  128/  306]
train() client id: f_00004-8-4 loss: 0.748793  [  160/  306]
train() client id: f_00004-8-5 loss: 0.885077  [  192/  306]
train() client id: f_00004-8-6 loss: 0.609570  [  224/  306]
train() client id: f_00004-8-7 loss: 0.842584  [  256/  306]
train() client id: f_00004-8-8 loss: 0.645441  [  288/  306]
train() client id: f_00004-9-0 loss: 0.808814  [   32/  306]
train() client id: f_00004-9-1 loss: 0.695543  [   64/  306]
train() client id: f_00004-9-2 loss: 0.876193  [   96/  306]
train() client id: f_00004-9-3 loss: 0.725527  [  128/  306]
train() client id: f_00004-9-4 loss: 0.966902  [  160/  306]
train() client id: f_00004-9-5 loss: 0.730753  [  192/  306]
train() client id: f_00004-9-6 loss: 0.705393  [  224/  306]
train() client id: f_00004-9-7 loss: 0.737987  [  256/  306]
train() client id: f_00004-9-8 loss: 0.650444  [  288/  306]
train() client id: f_00004-10-0 loss: 0.679126  [   32/  306]
train() client id: f_00004-10-1 loss: 0.743122  [   64/  306]
train() client id: f_00004-10-2 loss: 0.674446  [   96/  306]
train() client id: f_00004-10-3 loss: 0.886182  [  128/  306]
train() client id: f_00004-10-4 loss: 0.813357  [  160/  306]
train() client id: f_00004-10-5 loss: 0.744905  [  192/  306]
train() client id: f_00004-10-6 loss: 0.764633  [  224/  306]
train() client id: f_00004-10-7 loss: 0.861595  [  256/  306]
train() client id: f_00004-10-8 loss: 0.777558  [  288/  306]
train() client id: f_00004-11-0 loss: 0.776674  [   32/  306]
train() client id: f_00004-11-1 loss: 0.771890  [   64/  306]
train() client id: f_00004-11-2 loss: 0.760948  [   96/  306]
train() client id: f_00004-11-3 loss: 0.718815  [  128/  306]
train() client id: f_00004-11-4 loss: 0.767608  [  160/  306]
train() client id: f_00004-11-5 loss: 0.752766  [  192/  306]
train() client id: f_00004-11-6 loss: 0.906994  [  224/  306]
train() client id: f_00004-11-7 loss: 0.695711  [  256/  306]
train() client id: f_00004-11-8 loss: 0.818493  [  288/  306]
train() client id: f_00004-12-0 loss: 0.772844  [   32/  306]
train() client id: f_00004-12-1 loss: 0.811221  [   64/  306]
train() client id: f_00004-12-2 loss: 0.852299  [   96/  306]
train() client id: f_00004-12-3 loss: 0.884979  [  128/  306]
train() client id: f_00004-12-4 loss: 0.775420  [  160/  306]
train() client id: f_00004-12-5 loss: 0.656363  [  192/  306]
train() client id: f_00004-12-6 loss: 0.721151  [  224/  306]
train() client id: f_00004-12-7 loss: 0.767752  [  256/  306]
train() client id: f_00004-12-8 loss: 0.705623  [  288/  306]
train() client id: f_00004-13-0 loss: 0.864998  [   32/  306]
train() client id: f_00004-13-1 loss: 0.801128  [   64/  306]
train() client id: f_00004-13-2 loss: 0.891377  [   96/  306]
train() client id: f_00004-13-3 loss: 0.816864  [  128/  306]
train() client id: f_00004-13-4 loss: 0.739455  [  160/  306]
train() client id: f_00004-13-5 loss: 0.725054  [  192/  306]
train() client id: f_00004-13-6 loss: 0.685945  [  224/  306]
train() client id: f_00004-13-7 loss: 0.602842  [  256/  306]
train() client id: f_00004-13-8 loss: 0.775696  [  288/  306]
train() client id: f_00004-14-0 loss: 0.713515  [   32/  306]
train() client id: f_00004-14-1 loss: 0.852598  [   64/  306]
train() client id: f_00004-14-2 loss: 0.648144  [   96/  306]
train() client id: f_00004-14-3 loss: 0.683440  [  128/  306]
train() client id: f_00004-14-4 loss: 0.926942  [  160/  306]
train() client id: f_00004-14-5 loss: 0.802861  [  192/  306]
train() client id: f_00004-14-6 loss: 0.738865  [  224/  306]
train() client id: f_00004-14-7 loss: 0.692380  [  256/  306]
train() client id: f_00004-14-8 loss: 0.899086  [  288/  306]
train() client id: f_00004-15-0 loss: 0.767346  [   32/  306]
train() client id: f_00004-15-1 loss: 0.749016  [   64/  306]
train() client id: f_00004-15-2 loss: 0.691028  [   96/  306]
train() client id: f_00004-15-3 loss: 0.673976  [  128/  306]
train() client id: f_00004-15-4 loss: 0.747649  [  160/  306]
train() client id: f_00004-15-5 loss: 0.756496  [  192/  306]
train() client id: f_00004-15-6 loss: 0.899214  [  224/  306]
train() client id: f_00004-15-7 loss: 0.785334  [  256/  306]
train() client id: f_00004-15-8 loss: 0.910139  [  288/  306]
train() client id: f_00004-16-0 loss: 0.751366  [   32/  306]
train() client id: f_00004-16-1 loss: 0.686100  [   64/  306]
train() client id: f_00004-16-2 loss: 0.745379  [   96/  306]
train() client id: f_00004-16-3 loss: 0.797659  [  128/  306]
train() client id: f_00004-16-4 loss: 0.713273  [  160/  306]
train() client id: f_00004-16-5 loss: 0.643159  [  192/  306]
train() client id: f_00004-16-6 loss: 0.798705  [  224/  306]
train() client id: f_00004-16-7 loss: 0.848163  [  256/  306]
train() client id: f_00004-16-8 loss: 0.919678  [  288/  306]
train() client id: f_00004-17-0 loss: 0.766047  [   32/  306]
train() client id: f_00004-17-1 loss: 0.721828  [   64/  306]
train() client id: f_00004-17-2 loss: 0.699485  [   96/  306]
train() client id: f_00004-17-3 loss: 0.892130  [  128/  306]
train() client id: f_00004-17-4 loss: 0.778835  [  160/  306]
train() client id: f_00004-17-5 loss: 0.761074  [  192/  306]
train() client id: f_00004-17-6 loss: 0.732266  [  224/  306]
train() client id: f_00004-17-7 loss: 0.788090  [  256/  306]
train() client id: f_00004-17-8 loss: 0.785618  [  288/  306]
train() client id: f_00005-0-0 loss: 1.184198  [   32/  146]
train() client id: f_00005-0-1 loss: 0.935944  [   64/  146]
train() client id: f_00005-0-2 loss: 0.627734  [   96/  146]
train() client id: f_00005-0-3 loss: 0.532096  [  128/  146]
train() client id: f_00005-1-0 loss: 0.907757  [   32/  146]
train() client id: f_00005-1-1 loss: 0.723137  [   64/  146]
train() client id: f_00005-1-2 loss: 0.815228  [   96/  146]
train() client id: f_00005-1-3 loss: 0.853493  [  128/  146]
train() client id: f_00005-2-0 loss: 0.978896  [   32/  146]
train() client id: f_00005-2-1 loss: 0.679939  [   64/  146]
train() client id: f_00005-2-2 loss: 0.565501  [   96/  146]
train() client id: f_00005-2-3 loss: 0.901578  [  128/  146]
train() client id: f_00005-3-0 loss: 0.646263  [   32/  146]
train() client id: f_00005-3-1 loss: 0.706697  [   64/  146]
train() client id: f_00005-3-2 loss: 1.017832  [   96/  146]
train() client id: f_00005-3-3 loss: 0.720715  [  128/  146]
train() client id: f_00005-4-0 loss: 1.080321  [   32/  146]
train() client id: f_00005-4-1 loss: 0.794407  [   64/  146]
train() client id: f_00005-4-2 loss: 0.759352  [   96/  146]
train() client id: f_00005-4-3 loss: 0.622316  [  128/  146]
train() client id: f_00005-5-0 loss: 0.901296  [   32/  146]
train() client id: f_00005-5-1 loss: 0.694913  [   64/  146]
train() client id: f_00005-5-2 loss: 0.812765  [   96/  146]
train() client id: f_00005-5-3 loss: 0.635303  [  128/  146]
train() client id: f_00005-6-0 loss: 0.832232  [   32/  146]
train() client id: f_00005-6-1 loss: 0.628810  [   64/  146]
train() client id: f_00005-6-2 loss: 0.726948  [   96/  146]
train() client id: f_00005-6-3 loss: 0.893979  [  128/  146]
train() client id: f_00005-7-0 loss: 0.722424  [   32/  146]
train() client id: f_00005-7-1 loss: 1.101265  [   64/  146]
train() client id: f_00005-7-2 loss: 0.685914  [   96/  146]
train() client id: f_00005-7-3 loss: 0.805570  [  128/  146]
train() client id: f_00005-8-0 loss: 0.594852  [   32/  146]
train() client id: f_00005-8-1 loss: 0.883721  [   64/  146]
train() client id: f_00005-8-2 loss: 0.947400  [   96/  146]
train() client id: f_00005-8-3 loss: 0.614686  [  128/  146]
train() client id: f_00005-9-0 loss: 0.815800  [   32/  146]
train() client id: f_00005-9-1 loss: 0.748081  [   64/  146]
train() client id: f_00005-9-2 loss: 0.660081  [   96/  146]
train() client id: f_00005-9-3 loss: 1.062753  [  128/  146]
train() client id: f_00005-10-0 loss: 0.781795  [   32/  146]
train() client id: f_00005-10-1 loss: 0.496719  [   64/  146]
train() client id: f_00005-10-2 loss: 1.027827  [   96/  146]
train() client id: f_00005-10-3 loss: 0.784997  [  128/  146]
train() client id: f_00005-11-0 loss: 0.641051  [   32/  146]
train() client id: f_00005-11-1 loss: 0.861302  [   64/  146]
train() client id: f_00005-11-2 loss: 0.588225  [   96/  146]
train() client id: f_00005-11-3 loss: 0.951579  [  128/  146]
train() client id: f_00005-12-0 loss: 0.704624  [   32/  146]
train() client id: f_00005-12-1 loss: 1.096588  [   64/  146]
train() client id: f_00005-12-2 loss: 0.763449  [   96/  146]
train() client id: f_00005-12-3 loss: 0.494333  [  128/  146]
train() client id: f_00005-13-0 loss: 1.009698  [   32/  146]
train() client id: f_00005-13-1 loss: 0.610788  [   64/  146]
train() client id: f_00005-13-2 loss: 0.832282  [   96/  146]
train() client id: f_00005-13-3 loss: 0.909274  [  128/  146]
train() client id: f_00005-14-0 loss: 0.688802  [   32/  146]
train() client id: f_00005-14-1 loss: 0.715665  [   64/  146]
train() client id: f_00005-14-2 loss: 1.073067  [   96/  146]
train() client id: f_00005-14-3 loss: 0.635256  [  128/  146]
train() client id: f_00005-15-0 loss: 1.019586  [   32/  146]
train() client id: f_00005-15-1 loss: 0.723453  [   64/  146]
train() client id: f_00005-15-2 loss: 0.745819  [   96/  146]
train() client id: f_00005-15-3 loss: 0.766426  [  128/  146]
train() client id: f_00005-16-0 loss: 0.612993  [   32/  146]
train() client id: f_00005-16-1 loss: 0.804524  [   64/  146]
train() client id: f_00005-16-2 loss: 0.699119  [   96/  146]
train() client id: f_00005-16-3 loss: 1.093226  [  128/  146]
train() client id: f_00005-17-0 loss: 0.977045  [   32/  146]
train() client id: f_00005-17-1 loss: 0.704172  [   64/  146]
train() client id: f_00005-17-2 loss: 0.745738  [   96/  146]
train() client id: f_00005-17-3 loss: 0.882387  [  128/  146]
train() client id: f_00006-0-0 loss: 0.546195  [   32/   54]
train() client id: f_00006-1-0 loss: 0.552104  [   32/   54]
train() client id: f_00006-2-0 loss: 0.537077  [   32/   54]
train() client id: f_00006-3-0 loss: 0.537797  [   32/   54]
train() client id: f_00006-4-0 loss: 0.540609  [   32/   54]
train() client id: f_00006-5-0 loss: 0.486538  [   32/   54]
train() client id: f_00006-6-0 loss: 0.546124  [   32/   54]
train() client id: f_00006-7-0 loss: 0.506464  [   32/   54]
train() client id: f_00006-8-0 loss: 0.547619  [   32/   54]
train() client id: f_00006-9-0 loss: 0.489857  [   32/   54]
train() client id: f_00006-10-0 loss: 0.470601  [   32/   54]
train() client id: f_00006-11-0 loss: 0.537473  [   32/   54]
train() client id: f_00006-12-0 loss: 0.490746  [   32/   54]
train() client id: f_00006-13-0 loss: 0.482217  [   32/   54]
train() client id: f_00006-14-0 loss: 0.544774  [   32/   54]
train() client id: f_00006-15-0 loss: 0.484040  [   32/   54]
train() client id: f_00006-16-0 loss: 0.498411  [   32/   54]
train() client id: f_00006-17-0 loss: 0.499543  [   32/   54]
train() client id: f_00007-0-0 loss: 0.793303  [   32/  179]
train() client id: f_00007-0-1 loss: 0.581603  [   64/  179]
train() client id: f_00007-0-2 loss: 0.589863  [   96/  179]
train() client id: f_00007-0-3 loss: 0.443544  [  128/  179]
train() client id: f_00007-0-4 loss: 0.547585  [  160/  179]
train() client id: f_00007-1-0 loss: 0.490986  [   32/  179]
train() client id: f_00007-1-1 loss: 0.501867  [   64/  179]
train() client id: f_00007-1-2 loss: 0.687740  [   96/  179]
train() client id: f_00007-1-3 loss: 0.609287  [  128/  179]
train() client id: f_00007-1-4 loss: 0.694447  [  160/  179]
train() client id: f_00007-2-0 loss: 0.607663  [   32/  179]
train() client id: f_00007-2-1 loss: 0.402935  [   64/  179]
train() client id: f_00007-2-2 loss: 0.596035  [   96/  179]
train() client id: f_00007-2-3 loss: 0.565647  [  128/  179]
train() client id: f_00007-2-4 loss: 0.482918  [  160/  179]
train() client id: f_00007-3-0 loss: 0.600007  [   32/  179]
train() client id: f_00007-3-1 loss: 0.654198  [   64/  179]
train() client id: f_00007-3-2 loss: 0.456135  [   96/  179]
train() client id: f_00007-3-3 loss: 0.490236  [  128/  179]
train() client id: f_00007-3-4 loss: 0.588864  [  160/  179]
train() client id: f_00007-4-0 loss: 0.691999  [   32/  179]
train() client id: f_00007-4-1 loss: 0.599432  [   64/  179]
train() client id: f_00007-4-2 loss: 0.531505  [   96/  179]
train() client id: f_00007-4-3 loss: 0.455143  [  128/  179]
train() client id: f_00007-4-4 loss: 0.533749  [  160/  179]
train() client id: f_00007-5-0 loss: 0.577743  [   32/  179]
train() client id: f_00007-5-1 loss: 0.433547  [   64/  179]
train() client id: f_00007-5-2 loss: 0.636083  [   96/  179]
train() client id: f_00007-5-3 loss: 0.521860  [  128/  179]
train() client id: f_00007-5-4 loss: 0.531691  [  160/  179]
train() client id: f_00007-6-0 loss: 0.367727  [   32/  179]
train() client id: f_00007-6-1 loss: 0.713926  [   64/  179]
train() client id: f_00007-6-2 loss: 0.587606  [   96/  179]
train() client id: f_00007-6-3 loss: 0.506689  [  128/  179]
train() client id: f_00007-6-4 loss: 0.611624  [  160/  179]
train() client id: f_00007-7-0 loss: 0.465407  [   32/  179]
train() client id: f_00007-7-1 loss: 0.391688  [   64/  179]
train() client id: f_00007-7-2 loss: 0.469502  [   96/  179]
train() client id: f_00007-7-3 loss: 0.502881  [  128/  179]
train() client id: f_00007-7-4 loss: 0.581468  [  160/  179]
train() client id: f_00007-8-0 loss: 0.541380  [   32/  179]
train() client id: f_00007-8-1 loss: 0.484169  [   64/  179]
train() client id: f_00007-8-2 loss: 0.576567  [   96/  179]
train() client id: f_00007-8-3 loss: 0.545883  [  128/  179]
train() client id: f_00007-8-4 loss: 0.427516  [  160/  179]
train() client id: f_00007-9-0 loss: 0.437342  [   32/  179]
train() client id: f_00007-9-1 loss: 0.692705  [   64/  179]
train() client id: f_00007-9-2 loss: 0.421821  [   96/  179]
train() client id: f_00007-9-3 loss: 0.658785  [  128/  179]
train() client id: f_00007-9-4 loss: 0.412388  [  160/  179]
train() client id: f_00007-10-0 loss: 0.384167  [   32/  179]
train() client id: f_00007-10-1 loss: 0.700496  [   64/  179]
train() client id: f_00007-10-2 loss: 0.593215  [   96/  179]
train() client id: f_00007-10-3 loss: 0.487320  [  128/  179]
train() client id: f_00007-10-4 loss: 0.396957  [  160/  179]
train() client id: f_00007-11-0 loss: 0.440425  [   32/  179]
train() client id: f_00007-11-1 loss: 0.354069  [   64/  179]
train() client id: f_00007-11-2 loss: 0.602202  [   96/  179]
train() client id: f_00007-11-3 loss: 0.525195  [  128/  179]
train() client id: f_00007-11-4 loss: 0.552450  [  160/  179]
train() client id: f_00007-12-0 loss: 0.676187  [   32/  179]
train() client id: f_00007-12-1 loss: 0.493978  [   64/  179]
train() client id: f_00007-12-2 loss: 0.700473  [   96/  179]
train() client id: f_00007-12-3 loss: 0.425965  [  128/  179]
train() client id: f_00007-12-4 loss: 0.455503  [  160/  179]
train() client id: f_00007-13-0 loss: 0.361026  [   32/  179]
train() client id: f_00007-13-1 loss: 0.742069  [   64/  179]
train() client id: f_00007-13-2 loss: 0.403728  [   96/  179]
train() client id: f_00007-13-3 loss: 0.669350  [  128/  179]
train() client id: f_00007-13-4 loss: 0.501133  [  160/  179]
train() client id: f_00007-14-0 loss: 0.495806  [   32/  179]
train() client id: f_00007-14-1 loss: 0.466046  [   64/  179]
train() client id: f_00007-14-2 loss: 0.405856  [   96/  179]
train() client id: f_00007-14-3 loss: 0.921671  [  128/  179]
train() client id: f_00007-14-4 loss: 0.459021  [  160/  179]
train() client id: f_00007-15-0 loss: 0.679628  [   32/  179]
train() client id: f_00007-15-1 loss: 0.504097  [   64/  179]
train() client id: f_00007-15-2 loss: 0.568723  [   96/  179]
train() client id: f_00007-15-3 loss: 0.572416  [  128/  179]
train() client id: f_00007-15-4 loss: 0.471887  [  160/  179]
train() client id: f_00007-16-0 loss: 0.594701  [   32/  179]
train() client id: f_00007-16-1 loss: 0.779360  [   64/  179]
train() client id: f_00007-16-2 loss: 0.489719  [   96/  179]
train() client id: f_00007-16-3 loss: 0.368588  [  128/  179]
train() client id: f_00007-16-4 loss: 0.350157  [  160/  179]
train() client id: f_00007-17-0 loss: 0.376679  [   32/  179]
train() client id: f_00007-17-1 loss: 0.644544  [   64/  179]
train() client id: f_00007-17-2 loss: 0.366491  [   96/  179]
train() client id: f_00007-17-3 loss: 0.552399  [  128/  179]
train() client id: f_00007-17-4 loss: 0.715516  [  160/  179]
train() client id: f_00008-0-0 loss: 0.529924  [   32/  130]
train() client id: f_00008-0-1 loss: 0.659889  [   64/  130]
train() client id: f_00008-0-2 loss: 0.527840  [   96/  130]
train() client id: f_00008-0-3 loss: 0.530256  [  128/  130]
train() client id: f_00008-1-0 loss: 0.662960  [   32/  130]
train() client id: f_00008-1-1 loss: 0.477238  [   64/  130]
train() client id: f_00008-1-2 loss: 0.517295  [   96/  130]
train() client id: f_00008-1-3 loss: 0.630233  [  128/  130]
train() client id: f_00008-2-0 loss: 0.591162  [   32/  130]
train() client id: f_00008-2-1 loss: 0.516344  [   64/  130]
train() client id: f_00008-2-2 loss: 0.609082  [   96/  130]
train() client id: f_00008-2-3 loss: 0.572736  [  128/  130]
train() client id: f_00008-3-0 loss: 0.635858  [   32/  130]
train() client id: f_00008-3-1 loss: 0.466032  [   64/  130]
train() client id: f_00008-3-2 loss: 0.658895  [   96/  130]
train() client id: f_00008-3-3 loss: 0.506648  [  128/  130]
train() client id: f_00008-4-0 loss: 0.548212  [   32/  130]
train() client id: f_00008-4-1 loss: 0.608842  [   64/  130]
train() client id: f_00008-4-2 loss: 0.569794  [   96/  130]
train() client id: f_00008-4-3 loss: 0.555839  [  128/  130]
train() client id: f_00008-5-0 loss: 0.608488  [   32/  130]
train() client id: f_00008-5-1 loss: 0.489490  [   64/  130]
train() client id: f_00008-5-2 loss: 0.568985  [   96/  130]
train() client id: f_00008-5-3 loss: 0.590136  [  128/  130]
train() client id: f_00008-6-0 loss: 0.597824  [   32/  130]
train() client id: f_00008-6-1 loss: 0.553818  [   64/  130]
train() client id: f_00008-6-2 loss: 0.522962  [   96/  130]
train() client id: f_00008-6-3 loss: 0.626179  [  128/  130]
train() client id: f_00008-7-0 loss: 0.494983  [   32/  130]
train() client id: f_00008-7-1 loss: 0.616365  [   64/  130]
train() client id: f_00008-7-2 loss: 0.609219  [   96/  130]
train() client id: f_00008-7-3 loss: 0.569099  [  128/  130]
train() client id: f_00008-8-0 loss: 0.648392  [   32/  130]
train() client id: f_00008-8-1 loss: 0.612992  [   64/  130]
train() client id: f_00008-8-2 loss: 0.539793  [   96/  130]
train() client id: f_00008-8-3 loss: 0.500130  [  128/  130]
train() client id: f_00008-9-0 loss: 0.534334  [   32/  130]
train() client id: f_00008-9-1 loss: 0.551601  [   64/  130]
train() client id: f_00008-9-2 loss: 0.564685  [   96/  130]
train() client id: f_00008-9-3 loss: 0.627446  [  128/  130]
train() client id: f_00008-10-0 loss: 0.590921  [   32/  130]
train() client id: f_00008-10-1 loss: 0.506121  [   64/  130]
train() client id: f_00008-10-2 loss: 0.635778  [   96/  130]
train() client id: f_00008-10-3 loss: 0.555996  [  128/  130]
train() client id: f_00008-11-0 loss: 0.628450  [   32/  130]
train() client id: f_00008-11-1 loss: 0.733247  [   64/  130]
train() client id: f_00008-11-2 loss: 0.506067  [   96/  130]
train() client id: f_00008-11-3 loss: 0.430866  [  128/  130]
train() client id: f_00008-12-0 loss: 0.586259  [   32/  130]
train() client id: f_00008-12-1 loss: 0.611533  [   64/  130]
train() client id: f_00008-12-2 loss: 0.493128  [   96/  130]
train() client id: f_00008-12-3 loss: 0.612152  [  128/  130]
train() client id: f_00008-13-0 loss: 0.555581  [   32/  130]
train() client id: f_00008-13-1 loss: 0.629839  [   64/  130]
train() client id: f_00008-13-2 loss: 0.539390  [   96/  130]
train() client id: f_00008-13-3 loss: 0.586437  [  128/  130]
train() client id: f_00008-14-0 loss: 0.534813  [   32/  130]
train() client id: f_00008-14-1 loss: 0.633676  [   64/  130]
train() client id: f_00008-14-2 loss: 0.619506  [   96/  130]
train() client id: f_00008-14-3 loss: 0.492379  [  128/  130]
train() client id: f_00008-15-0 loss: 0.562763  [   32/  130]
train() client id: f_00008-15-1 loss: 0.616870  [   64/  130]
train() client id: f_00008-15-2 loss: 0.575705  [   96/  130]
train() client id: f_00008-15-3 loss: 0.560579  [  128/  130]
train() client id: f_00008-16-0 loss: 0.540807  [   32/  130]
train() client id: f_00008-16-1 loss: 0.665065  [   64/  130]
train() client id: f_00008-16-2 loss: 0.504906  [   96/  130]
train() client id: f_00008-16-3 loss: 0.603182  [  128/  130]
train() client id: f_00008-17-0 loss: 0.578138  [   32/  130]
train() client id: f_00008-17-1 loss: 0.623743  [   64/  130]
train() client id: f_00008-17-2 loss: 0.570353  [   96/  130]
train() client id: f_00008-17-3 loss: 0.541307  [  128/  130]
train() client id: f_00009-0-0 loss: 0.960450  [   32/  118]
train() client id: f_00009-0-1 loss: 1.251551  [   64/  118]
train() client id: f_00009-0-2 loss: 1.080220  [   96/  118]
train() client id: f_00009-1-0 loss: 1.048858  [   32/  118]
train() client id: f_00009-1-1 loss: 0.949033  [   64/  118]
train() client id: f_00009-1-2 loss: 0.951635  [   96/  118]
train() client id: f_00009-2-0 loss: 0.912434  [   32/  118]
train() client id: f_00009-2-1 loss: 0.886326  [   64/  118]
train() client id: f_00009-2-2 loss: 1.014447  [   96/  118]
train() client id: f_00009-3-0 loss: 0.840616  [   32/  118]
train() client id: f_00009-3-1 loss: 0.890110  [   64/  118]
train() client id: f_00009-3-2 loss: 1.051588  [   96/  118]
train() client id: f_00009-4-0 loss: 0.980426  [   32/  118]
train() client id: f_00009-4-1 loss: 0.963798  [   64/  118]
train() client id: f_00009-4-2 loss: 0.750655  [   96/  118]
train() client id: f_00009-5-0 loss: 0.921396  [   32/  118]
train() client id: f_00009-5-1 loss: 0.637624  [   64/  118]
train() client id: f_00009-5-2 loss: 0.877196  [   96/  118]
train() client id: f_00009-6-0 loss: 0.807342  [   32/  118]
train() client id: f_00009-6-1 loss: 0.843592  [   64/  118]
train() client id: f_00009-6-2 loss: 0.907268  [   96/  118]
train() client id: f_00009-7-0 loss: 0.937983  [   32/  118]
train() client id: f_00009-7-1 loss: 0.809913  [   64/  118]
train() client id: f_00009-7-2 loss: 0.811520  [   96/  118]
train() client id: f_00009-8-0 loss: 0.793465  [   32/  118]
train() client id: f_00009-8-1 loss: 0.711810  [   64/  118]
train() client id: f_00009-8-2 loss: 0.729553  [   96/  118]
train() client id: f_00009-9-0 loss: 0.808864  [   32/  118]
train() client id: f_00009-9-1 loss: 0.642211  [   64/  118]
train() client id: f_00009-9-2 loss: 0.790413  [   96/  118]
train() client id: f_00009-10-0 loss: 0.759216  [   32/  118]
train() client id: f_00009-10-1 loss: 0.706386  [   64/  118]
train() client id: f_00009-10-2 loss: 0.817391  [   96/  118]
train() client id: f_00009-11-0 loss: 0.830632  [   32/  118]
train() client id: f_00009-11-1 loss: 0.820866  [   64/  118]
train() client id: f_00009-11-2 loss: 0.647110  [   96/  118]
train() client id: f_00009-12-0 loss: 0.637444  [   32/  118]
train() client id: f_00009-12-1 loss: 0.648811  [   64/  118]
train() client id: f_00009-12-2 loss: 0.745390  [   96/  118]
train() client id: f_00009-13-0 loss: 0.852005  [   32/  118]
train() client id: f_00009-13-1 loss: 0.789972  [   64/  118]
train() client id: f_00009-13-2 loss: 0.574109  [   96/  118]
train() client id: f_00009-14-0 loss: 0.696649  [   32/  118]
train() client id: f_00009-14-1 loss: 0.798100  [   64/  118]
train() client id: f_00009-14-2 loss: 0.779343  [   96/  118]
train() client id: f_00009-15-0 loss: 0.678996  [   32/  118]
train() client id: f_00009-15-1 loss: 0.706582  [   64/  118]
train() client id: f_00009-15-2 loss: 0.665348  [   96/  118]
train() client id: f_00009-16-0 loss: 0.621604  [   32/  118]
train() client id: f_00009-16-1 loss: 0.722955  [   64/  118]
train() client id: f_00009-16-2 loss: 0.805508  [   96/  118]
train() client id: f_00009-17-0 loss: 0.920566  [   32/  118]
train() client id: f_00009-17-1 loss: 0.547795  [   64/  118]
train() client id: f_00009-17-2 loss: 0.638826  [   96/  118]
At round 62 accuracy: 0.649867374005305
At round 62 training accuracy: 0.5855130784708249
At round 62 training loss: 0.8621552180061597
update_location
xs = 8.927491 431.223621 5.882650 0.934260 -347.581990 -195.230757 -155.849135 -5.143845 -370.120581 20.134486 
ys = -422.390647 7.291448 320.684448 -142.290817 -9.642386 0.794442 -1.381692 316.628436 25.881276 -857.232496 
xs mean: -60.68237997052123
ys mean: -76.16579882624052
dists_uav = 434.158449 442.726751 335.965952 173.918227 361.809639 219.352866 185.177920 332.084365 384.264343 863.280342 
uav_gains = -122.539163 -122.793476 -118.353478 -106.038799 -119.784809 -108.945745 -106.754126 -118.107705 -120.792872 -130.376116 
uav_gains_db_mean: -117.44862896459841
dists_bs = 625.063923 628.993829 232.255229 362.803427 252.739696 175.376338 177.418311 221.155437 245.577757 1050.514823 
bs_gains = -117.852943 -117.929158 -105.814093 -111.237858 -106.841916 -102.398247 -102.539015 -105.218593 -106.492352 -124.166320 
bs_gains_db_mean: -110.04904955284277
Round 63
-------------------------------
ene_coms = [0.03990593 0.04150751 0.00829559 0.0083975  0.00878234 0.00997093
 0.00873863 0.00803677 0.00861074 0.0518319 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.80036284 5.60674633 2.54933062 0.92085976 6.21313702 3.00298862
 1.14437737 3.65381784 2.67211011 2.71899225]
obj_prev = 31.28272277150635
eta_min = 3.84828019337051e-35	eta_max = 0.8716145830794693
af = 6.446787169041255	bf = 1.28134347751193	zeta = 7.091465885945381	eta = 0.9090909090909091
af = 6.446787169041255	bf = 1.28134347751193	zeta = 20.853667139288127	eta = 0.3091440525055454
af = 6.446787169041255	bf = 1.28134347751193	zeta = 12.809524049510804	eta = 0.5032807732842703
af = 6.446787169041255	bf = 1.28134347751193	zeta = 11.490904321255824	eta = 0.5610339263826274
af = 6.446787169041255	bf = 1.28134347751193	zeta = 11.40730116504493	eta = 0.565145697108091
af = 6.446787169041255	bf = 1.28134347751193	zeta = 11.406906871734394	eta = 0.5651652320416495
af = 6.446787169041255	bf = 1.28134347751193	zeta = 11.406906862877689	eta = 0.5651652324804628
eta = 0.5651652324804628
ene_coms = [0.03990593 0.04150751 0.00829559 0.0083975  0.00878234 0.00997093
 0.00873863 0.00803677 0.00861074 0.0518319 ]
ene_comp = [0.04708761 0.09903346 0.04634019 0.01606958 0.11435562 0.05456183
 0.0201804  0.0668943  0.04858245 0.04409792]
ene_total = [1.32084651 2.13387189 0.82954987 0.37149031 1.86963716 0.97981852
 0.43908551 1.1376988  0.86837976 1.45652853]
ti_comp = [1.36545159 1.34943571 1.68155495 1.68053588 1.67668746 1.66480153
 1.67712459 1.68414318 1.67840349 1.24619186]
ti_coms = [0.39905927 0.41507515 0.08295591 0.08397498 0.08782339 0.09970932
 0.08738627 0.08036767 0.08610736 0.518319  ]
t_total = [26.79104805 26.79104805 26.79104805 26.79104805 26.79104805 26.79104805
 26.79104805 26.79104805 26.79104805 26.79104805]
ene_coms = [0.03990593 0.04150751 0.00829559 0.0083975  0.00878234 0.00997093
 0.00873863 0.00803677 0.00861074 0.0518319 ]
ene_comp = [3.49983307e-06 3.33366176e-05 2.19953682e-06 9.18328575e-08
 3.32466854e-05 3.66287367e-06 1.82616046e-07 6.59613260e-06
 2.54404984e-06 3.45115536e-06]
ene_total = [0.60595568 0.63072604 0.12598761 0.12750289 0.13384944 0.15144699
 0.13268372 0.12212457 0.13077778 0.78703023]
optimize_network iter = 0 obj = 2.9480849387837313
eta = 0.5651652324804628
freqs = [17242504.16984098 36694398.82988861 13778968.99775699  4781088.34529742
 34101651.43667793 16386888.15060011  6016369.97937577 19860040.07781282
 14472817.25674908 17693070.12685838]
eta_min = 0.5651652324804638	eta_max = 0.5651652324804586
af = 0.0010275382147731716	bf = 1.28134347751193	zeta = 0.001130292036250489	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03990593 0.04150751 0.00829559 0.0083975  0.00878234 0.00997093
 0.00873863 0.00803677 0.00861074 0.0518319 ]
ene_comp = [5.84556618e-07 5.56801998e-06 3.67375752e-07 1.53383043e-08
 5.55299913e-06 6.11788335e-07 3.05012886e-08 1.10171339e-06
 4.24917743e-07 5.76426266e-07]
ene_total = [2.89818483 3.01486105 0.60248865 0.60986403 0.63821506 0.72417699
 0.63463941 0.58374509 0.62538006 3.76430038]
ti_comp = [1.36545159 1.34943571 1.68155495 1.68053588 1.67668746 1.66480153
 1.67712459 1.68414318 1.67840349 1.24619186]
ti_coms = [0.39905927 0.41507515 0.08295591 0.08397498 0.08782339 0.09970932
 0.08738627 0.08036767 0.08610736 0.518319  ]
t_total = [26.79104805 26.79104805 26.79104805 26.79104805 26.79104805 26.79104805
 26.79104805 26.79104805 26.79104805 26.79104805]
ene_coms = [0.03990593 0.04150751 0.00829559 0.0083975  0.00878234 0.00997093
 0.00873863 0.00803677 0.00861074 0.0518319 ]
ene_comp = [3.49983307e-06 3.33366176e-05 2.19953682e-06 9.18328575e-08
 3.32466854e-05 3.66287367e-06 1.82616046e-07 6.59613260e-06
 2.54404984e-06 3.45115536e-06]
ene_total = [0.60595568 0.63072604 0.12598761 0.12750289 0.13384944 0.15144699
 0.13268372 0.12212457 0.13077778 0.78703023]
optimize_network iter = 1 obj = 2.948084938783703
eta = 0.5651652324804586
freqs = [17242504.16984099 36694398.82988863 13778968.99775703  4781088.34529744
 34101651.43667802 16386888.15060016  6016369.97937578 19860040.07781288
 14472817.25674911 17693070.12685836]
Done!
ene_coms = [0.03990593 0.04150751 0.00829559 0.0083975  0.00878234 0.00997093
 0.00873863 0.00803677 0.00861074 0.0518319 ]
ene_comp = [3.37142679e-06 3.21135219e-05 2.11883745e-06 8.84635783e-08
 3.20268892e-05 3.52848556e-06 1.75916000e-07 6.35412540e-06
 2.45071054e-06 3.32453504e-06]
ene_total = [0.0399093  0.04153963 0.00829771 0.00839759 0.00881437 0.00997446
 0.0087388  0.00804312 0.00861319 0.05183522]
At round 63 energy consumption: 0.1941633851385241
At round 63 eta: 0.5651652324804586
At round 63 a_n: 6.602214497187386
At round 63 local rounds: 18.685559269823173
At round 63 global rounds: 15.183271877841928
gradient difference: 0.31855884194374084
train() client id: f_00000-0-0 loss: 1.212010  [   32/  126]
train() client id: f_00000-0-1 loss: 1.203042  [   64/  126]
train() client id: f_00000-0-2 loss: 1.266927  [   96/  126]
train() client id: f_00000-1-0 loss: 1.055780  [   32/  126]
train() client id: f_00000-1-1 loss: 1.252816  [   64/  126]
train() client id: f_00000-1-2 loss: 1.091949  [   96/  126]
train() client id: f_00000-2-0 loss: 1.070060  [   32/  126]
train() client id: f_00000-2-1 loss: 1.029743  [   64/  126]
train() client id: f_00000-2-2 loss: 1.038466  [   96/  126]
train() client id: f_00000-3-0 loss: 0.991695  [   32/  126]
train() client id: f_00000-3-1 loss: 0.982027  [   64/  126]
train() client id: f_00000-3-2 loss: 0.979142  [   96/  126]
train() client id: f_00000-4-0 loss: 0.926204  [   32/  126]
train() client id: f_00000-4-1 loss: 0.876090  [   64/  126]
train() client id: f_00000-4-2 loss: 0.929037  [   96/  126]
train() client id: f_00000-5-0 loss: 1.042515  [   32/  126]
train() client id: f_00000-5-1 loss: 0.970555  [   64/  126]
train() client id: f_00000-5-2 loss: 0.741643  [   96/  126]
train() client id: f_00000-6-0 loss: 0.885246  [   32/  126]
train() client id: f_00000-6-1 loss: 0.732384  [   64/  126]
train() client id: f_00000-6-2 loss: 0.866961  [   96/  126]
train() client id: f_00000-7-0 loss: 0.843847  [   32/  126]
train() client id: f_00000-7-1 loss: 0.802741  [   64/  126]
train() client id: f_00000-7-2 loss: 0.861876  [   96/  126]
train() client id: f_00000-8-0 loss: 0.832287  [   32/  126]
train() client id: f_00000-8-1 loss: 0.877477  [   64/  126]
train() client id: f_00000-8-2 loss: 0.868658  [   96/  126]
train() client id: f_00000-9-0 loss: 0.825081  [   32/  126]
train() client id: f_00000-9-1 loss: 0.781056  [   64/  126]
train() client id: f_00000-9-2 loss: 0.884361  [   96/  126]
train() client id: f_00000-10-0 loss: 0.763764  [   32/  126]
train() client id: f_00000-10-1 loss: 0.762016  [   64/  126]
train() client id: f_00000-10-2 loss: 0.877290  [   96/  126]
train() client id: f_00000-11-0 loss: 0.797075  [   32/  126]
train() client id: f_00000-11-1 loss: 0.944575  [   64/  126]
train() client id: f_00000-11-2 loss: 0.836970  [   96/  126]
train() client id: f_00000-12-0 loss: 0.743534  [   32/  126]
train() client id: f_00000-12-1 loss: 0.694449  [   64/  126]
train() client id: f_00000-12-2 loss: 0.955670  [   96/  126]
train() client id: f_00000-13-0 loss: 0.855374  [   32/  126]
train() client id: f_00000-13-1 loss: 0.801174  [   64/  126]
train() client id: f_00000-13-2 loss: 0.768408  [   96/  126]
train() client id: f_00000-14-0 loss: 0.834330  [   32/  126]
train() client id: f_00000-14-1 loss: 0.853783  [   64/  126]
train() client id: f_00000-14-2 loss: 0.899804  [   96/  126]
train() client id: f_00000-15-0 loss: 0.787622  [   32/  126]
train() client id: f_00000-15-1 loss: 0.704288  [   64/  126]
train() client id: f_00000-15-2 loss: 0.964651  [   96/  126]
train() client id: f_00000-16-0 loss: 0.797312  [   32/  126]
train() client id: f_00000-16-1 loss: 0.921892  [   64/  126]
train() client id: f_00000-16-2 loss: 0.809477  [   96/  126]
train() client id: f_00000-17-0 loss: 0.913773  [   32/  126]
train() client id: f_00000-17-1 loss: 0.906020  [   64/  126]
train() client id: f_00000-17-2 loss: 0.790498  [   96/  126]
train() client id: f_00001-0-0 loss: 0.276396  [   32/  265]
train() client id: f_00001-0-1 loss: 0.299641  [   64/  265]
train() client id: f_00001-0-2 loss: 0.241774  [   96/  265]
train() client id: f_00001-0-3 loss: 0.424783  [  128/  265]
train() client id: f_00001-0-4 loss: 0.255592  [  160/  265]
train() client id: f_00001-0-5 loss: 0.209218  [  192/  265]
train() client id: f_00001-0-6 loss: 0.398844  [  224/  265]
train() client id: f_00001-0-7 loss: 0.256386  [  256/  265]
train() client id: f_00001-1-0 loss: 0.356599  [   32/  265]
train() client id: f_00001-1-1 loss: 0.406022  [   64/  265]
train() client id: f_00001-1-2 loss: 0.311387  [   96/  265]
train() client id: f_00001-1-3 loss: 0.209217  [  128/  265]
train() client id: f_00001-1-4 loss: 0.262489  [  160/  265]
train() client id: f_00001-1-5 loss: 0.321809  [  192/  265]
train() client id: f_00001-1-6 loss: 0.194160  [  224/  265]
train() client id: f_00001-1-7 loss: 0.285284  [  256/  265]
train() client id: f_00001-2-0 loss: 0.224985  [   32/  265]
train() client id: f_00001-2-1 loss: 0.228256  [   64/  265]
train() client id: f_00001-2-2 loss: 0.272271  [   96/  265]
train() client id: f_00001-2-3 loss: 0.262313  [  128/  265]
train() client id: f_00001-2-4 loss: 0.344700  [  160/  265]
train() client id: f_00001-2-5 loss: 0.365263  [  192/  265]
train() client id: f_00001-2-6 loss: 0.278062  [  224/  265]
train() client id: f_00001-2-7 loss: 0.300909  [  256/  265]
train() client id: f_00001-3-0 loss: 0.227757  [   32/  265]
train() client id: f_00001-3-1 loss: 0.180337  [   64/  265]
train() client id: f_00001-3-2 loss: 0.336778  [   96/  265]
train() client id: f_00001-3-3 loss: 0.297700  [  128/  265]
train() client id: f_00001-3-4 loss: 0.324548  [  160/  265]
train() client id: f_00001-3-5 loss: 0.324528  [  192/  265]
train() client id: f_00001-3-6 loss: 0.305524  [  224/  265]
train() client id: f_00001-3-7 loss: 0.201432  [  256/  265]
train() client id: f_00001-4-0 loss: 0.232356  [   32/  265]
train() client id: f_00001-4-1 loss: 0.219406  [   64/  265]
train() client id: f_00001-4-2 loss: 0.243037  [   96/  265]
train() client id: f_00001-4-3 loss: 0.352905  [  128/  265]
train() client id: f_00001-4-4 loss: 0.168994  [  160/  265]
train() client id: f_00001-4-5 loss: 0.165598  [  192/  265]
train() client id: f_00001-4-6 loss: 0.216774  [  224/  265]
train() client id: f_00001-4-7 loss: 0.441429  [  256/  265]
train() client id: f_00001-5-0 loss: 0.228136  [   32/  265]
train() client id: f_00001-5-1 loss: 0.273088  [   64/  265]
train() client id: f_00001-5-2 loss: 0.317520  [   96/  265]
train() client id: f_00001-5-3 loss: 0.431106  [  128/  265]
train() client id: f_00001-5-4 loss: 0.250957  [  160/  265]
train() client id: f_00001-5-5 loss: 0.175371  [  192/  265]
train() client id: f_00001-5-6 loss: 0.220505  [  224/  265]
train() client id: f_00001-5-7 loss: 0.187645  [  256/  265]
train() client id: f_00001-6-0 loss: 0.252916  [   32/  265]
train() client id: f_00001-6-1 loss: 0.225647  [   64/  265]
train() client id: f_00001-6-2 loss: 0.404371  [   96/  265]
train() client id: f_00001-6-3 loss: 0.217669  [  128/  265]
train() client id: f_00001-6-4 loss: 0.343657  [  160/  265]
train() client id: f_00001-6-5 loss: 0.145101  [  192/  265]
train() client id: f_00001-6-6 loss: 0.240175  [  224/  265]
train() client id: f_00001-6-7 loss: 0.203380  [  256/  265]
train() client id: f_00001-7-0 loss: 0.188071  [   32/  265]
train() client id: f_00001-7-1 loss: 0.229706  [   64/  265]
train() client id: f_00001-7-2 loss: 0.268098  [   96/  265]
train() client id: f_00001-7-3 loss: 0.273235  [  128/  265]
train() client id: f_00001-7-4 loss: 0.244454  [  160/  265]
train() client id: f_00001-7-5 loss: 0.355991  [  192/  265]
train() client id: f_00001-7-6 loss: 0.157695  [  224/  265]
train() client id: f_00001-7-7 loss: 0.296527  [  256/  265]
train() client id: f_00001-8-0 loss: 0.178044  [   32/  265]
train() client id: f_00001-8-1 loss: 0.281913  [   64/  265]
train() client id: f_00001-8-2 loss: 0.335238  [   96/  265]
train() client id: f_00001-8-3 loss: 0.170667  [  128/  265]
train() client id: f_00001-8-4 loss: 0.246676  [  160/  265]
train() client id: f_00001-8-5 loss: 0.326340  [  192/  265]
train() client id: f_00001-8-6 loss: 0.166347  [  224/  265]
train() client id: f_00001-8-7 loss: 0.206298  [  256/  265]
train() client id: f_00001-9-0 loss: 0.175250  [   32/  265]
train() client id: f_00001-9-1 loss: 0.237513  [   64/  265]
train() client id: f_00001-9-2 loss: 0.241441  [   96/  265]
train() client id: f_00001-9-3 loss: 0.228220  [  128/  265]
train() client id: f_00001-9-4 loss: 0.259132  [  160/  265]
train() client id: f_00001-9-5 loss: 0.167882  [  192/  265]
train() client id: f_00001-9-6 loss: 0.356088  [  224/  265]
train() client id: f_00001-9-7 loss: 0.270251  [  256/  265]
train() client id: f_00001-10-0 loss: 0.286429  [   32/  265]
train() client id: f_00001-10-1 loss: 0.236557  [   64/  265]
train() client id: f_00001-10-2 loss: 0.130979  [   96/  265]
train() client id: f_00001-10-3 loss: 0.302915  [  128/  265]
train() client id: f_00001-10-4 loss: 0.177950  [  160/  265]
train() client id: f_00001-10-5 loss: 0.294139  [  192/  265]
train() client id: f_00001-10-6 loss: 0.135245  [  224/  265]
train() client id: f_00001-10-7 loss: 0.292058  [  256/  265]
train() client id: f_00001-11-0 loss: 0.230627  [   32/  265]
train() client id: f_00001-11-1 loss: 0.249957  [   64/  265]
train() client id: f_00001-11-2 loss: 0.296310  [   96/  265]
train() client id: f_00001-11-3 loss: 0.278686  [  128/  265]
train() client id: f_00001-11-4 loss: 0.158835  [  160/  265]
train() client id: f_00001-11-5 loss: 0.279245  [  192/  265]
train() client id: f_00001-11-6 loss: 0.185907  [  224/  265]
train() client id: f_00001-11-7 loss: 0.166393  [  256/  265]
train() client id: f_00001-12-0 loss: 0.293661  [   32/  265]
train() client id: f_00001-12-1 loss: 0.153968  [   64/  265]
train() client id: f_00001-12-2 loss: 0.248625  [   96/  265]
train() client id: f_00001-12-3 loss: 0.285145  [  128/  265]
train() client id: f_00001-12-4 loss: 0.150490  [  160/  265]
train() client id: f_00001-12-5 loss: 0.212053  [  192/  265]
train() client id: f_00001-12-6 loss: 0.322864  [  224/  265]
train() client id: f_00001-12-7 loss: 0.236117  [  256/  265]
train() client id: f_00001-13-0 loss: 0.308233  [   32/  265]
train() client id: f_00001-13-1 loss: 0.157533  [   64/  265]
train() client id: f_00001-13-2 loss: 0.133089  [   96/  265]
train() client id: f_00001-13-3 loss: 0.247813  [  128/  265]
train() client id: f_00001-13-4 loss: 0.204149  [  160/  265]
train() client id: f_00001-13-5 loss: 0.127719  [  192/  265]
train() client id: f_00001-13-6 loss: 0.318901  [  224/  265]
train() client id: f_00001-13-7 loss: 0.337463  [  256/  265]
train() client id: f_00001-14-0 loss: 0.211950  [   32/  265]
train() client id: f_00001-14-1 loss: 0.291402  [   64/  265]
train() client id: f_00001-14-2 loss: 0.162065  [   96/  265]
train() client id: f_00001-14-3 loss: 0.302432  [  128/  265]
train() client id: f_00001-14-4 loss: 0.368783  [  160/  265]
train() client id: f_00001-14-5 loss: 0.254612  [  192/  265]
train() client id: f_00001-14-6 loss: 0.167080  [  224/  265]
train() client id: f_00001-14-7 loss: 0.124605  [  256/  265]
train() client id: f_00001-15-0 loss: 0.252923  [   32/  265]
train() client id: f_00001-15-1 loss: 0.150285  [   64/  265]
train() client id: f_00001-15-2 loss: 0.174425  [   96/  265]
train() client id: f_00001-15-3 loss: 0.202809  [  128/  265]
train() client id: f_00001-15-4 loss: 0.176293  [  160/  265]
train() client id: f_00001-15-5 loss: 0.363750  [  192/  265]
train() client id: f_00001-15-6 loss: 0.144012  [  224/  265]
train() client id: f_00001-15-7 loss: 0.417023  [  256/  265]
train() client id: f_00001-16-0 loss: 0.270866  [   32/  265]
train() client id: f_00001-16-1 loss: 0.282679  [   64/  265]
train() client id: f_00001-16-2 loss: 0.132849  [   96/  265]
train() client id: f_00001-16-3 loss: 0.249653  [  128/  265]
train() client id: f_00001-16-4 loss: 0.251748  [  160/  265]
train() client id: f_00001-16-5 loss: 0.133579  [  192/  265]
train() client id: f_00001-16-6 loss: 0.241766  [  224/  265]
train() client id: f_00001-16-7 loss: 0.312995  [  256/  265]
train() client id: f_00001-17-0 loss: 0.329502  [   32/  265]
train() client id: f_00001-17-1 loss: 0.258469  [   64/  265]
train() client id: f_00001-17-2 loss: 0.370499  [   96/  265]
train() client id: f_00001-17-3 loss: 0.113932  [  128/  265]
train() client id: f_00001-17-4 loss: 0.138446  [  160/  265]
train() client id: f_00001-17-5 loss: 0.202146  [  192/  265]
train() client id: f_00001-17-6 loss: 0.185302  [  224/  265]
train() client id: f_00001-17-7 loss: 0.191242  [  256/  265]
train() client id: f_00002-0-0 loss: 1.147292  [   32/  124]
train() client id: f_00002-0-1 loss: 1.105839  [   64/  124]
train() client id: f_00002-0-2 loss: 1.129947  [   96/  124]
train() client id: f_00002-1-0 loss: 1.282448  [   32/  124]
train() client id: f_00002-1-1 loss: 0.954364  [   64/  124]
train() client id: f_00002-1-2 loss: 1.044064  [   96/  124]
train() client id: f_00002-2-0 loss: 1.085793  [   32/  124]
train() client id: f_00002-2-1 loss: 0.965555  [   64/  124]
train() client id: f_00002-2-2 loss: 1.013676  [   96/  124]
train() client id: f_00002-3-0 loss: 1.091819  [   32/  124]
train() client id: f_00002-3-1 loss: 0.912936  [   64/  124]
train() client id: f_00002-3-2 loss: 1.027620  [   96/  124]
train() client id: f_00002-4-0 loss: 1.022867  [   32/  124]
train() client id: f_00002-4-1 loss: 1.120470  [   64/  124]
train() client id: f_00002-4-2 loss: 0.814290  [   96/  124]
train() client id: f_00002-5-0 loss: 1.135112  [   32/  124]
train() client id: f_00002-5-1 loss: 0.902556  [   64/  124]
train() client id: f_00002-5-2 loss: 0.781973  [   96/  124]
train() client id: f_00002-6-0 loss: 0.908387  [   32/  124]
train() client id: f_00002-6-1 loss: 0.881054  [   64/  124]
train() client id: f_00002-6-2 loss: 0.918038  [   96/  124]
train() client id: f_00002-7-0 loss: 0.850829  [   32/  124]
train() client id: f_00002-7-1 loss: 0.911782  [   64/  124]
train() client id: f_00002-7-2 loss: 0.969333  [   96/  124]
train() client id: f_00002-8-0 loss: 0.897217  [   32/  124]
train() client id: f_00002-8-1 loss: 0.758528  [   64/  124]
train() client id: f_00002-8-2 loss: 0.878769  [   96/  124]
train() client id: f_00002-9-0 loss: 0.786353  [   32/  124]
train() client id: f_00002-9-1 loss: 0.805921  [   64/  124]
train() client id: f_00002-9-2 loss: 0.886368  [   96/  124]
train() client id: f_00002-10-0 loss: 0.806531  [   32/  124]
train() client id: f_00002-10-1 loss: 0.888348  [   64/  124]
train() client id: f_00002-10-2 loss: 0.821056  [   96/  124]
train() client id: f_00002-11-0 loss: 0.830768  [   32/  124]
train() client id: f_00002-11-1 loss: 0.728591  [   64/  124]
train() client id: f_00002-11-2 loss: 0.790790  [   96/  124]
train() client id: f_00002-12-0 loss: 0.886506  [   32/  124]
train() client id: f_00002-12-1 loss: 0.876066  [   64/  124]
train() client id: f_00002-12-2 loss: 0.711354  [   96/  124]
train() client id: f_00002-13-0 loss: 0.636568  [   32/  124]
train() client id: f_00002-13-1 loss: 0.841091  [   64/  124]
train() client id: f_00002-13-2 loss: 0.755031  [   96/  124]
train() client id: f_00002-14-0 loss: 0.572265  [   32/  124]
train() client id: f_00002-14-1 loss: 0.857543  [   64/  124]
train() client id: f_00002-14-2 loss: 0.944799  [   96/  124]
train() client id: f_00002-15-0 loss: 0.655991  [   32/  124]
train() client id: f_00002-15-1 loss: 0.886619  [   64/  124]
train() client id: f_00002-15-2 loss: 0.785078  [   96/  124]
train() client id: f_00002-16-0 loss: 0.692754  [   32/  124]
train() client id: f_00002-16-1 loss: 1.030926  [   64/  124]
train() client id: f_00002-16-2 loss: 0.595167  [   96/  124]
train() client id: f_00002-17-0 loss: 0.736319  [   32/  124]
train() client id: f_00002-17-1 loss: 0.656937  [   64/  124]
train() client id: f_00002-17-2 loss: 0.718348  [   96/  124]
train() client id: f_00003-0-0 loss: 0.642306  [   32/   43]
train() client id: f_00003-1-0 loss: 0.756806  [   32/   43]
train() client id: f_00003-2-0 loss: 0.708544  [   32/   43]
train() client id: f_00003-3-0 loss: 0.755928  [   32/   43]
train() client id: f_00003-4-0 loss: 0.527314  [   32/   43]
train() client id: f_00003-5-0 loss: 0.674988  [   32/   43]
train() client id: f_00003-6-0 loss: 0.712364  [   32/   43]
train() client id: f_00003-7-0 loss: 0.692182  [   32/   43]
train() client id: f_00003-8-0 loss: 0.798130  [   32/   43]
train() client id: f_00003-9-0 loss: 0.686288  [   32/   43]
train() client id: f_00003-10-0 loss: 0.552190  [   32/   43]
train() client id: f_00003-11-0 loss: 0.771510  [   32/   43]
train() client id: f_00003-12-0 loss: 0.741200  [   32/   43]
train() client id: f_00003-13-0 loss: 0.819139  [   32/   43]
train() client id: f_00003-14-0 loss: 0.827322  [   32/   43]
train() client id: f_00003-15-0 loss: 0.667907  [   32/   43]
train() client id: f_00003-16-0 loss: 0.813070  [   32/   43]
train() client id: f_00003-17-0 loss: 0.716535  [   32/   43]
train() client id: f_00004-0-0 loss: 0.680392  [   32/  306]
train() client id: f_00004-0-1 loss: 0.726150  [   64/  306]
train() client id: f_00004-0-2 loss: 0.807588  [   96/  306]
train() client id: f_00004-0-3 loss: 0.903360  [  128/  306]
train() client id: f_00004-0-4 loss: 0.639853  [  160/  306]
train() client id: f_00004-0-5 loss: 0.846391  [  192/  306]
train() client id: f_00004-0-6 loss: 0.728575  [  224/  306]
train() client id: f_00004-0-7 loss: 0.722329  [  256/  306]
train() client id: f_00004-0-8 loss: 0.906774  [  288/  306]
train() client id: f_00004-1-0 loss: 0.907248  [   32/  306]
train() client id: f_00004-1-1 loss: 0.572848  [   64/  306]
train() client id: f_00004-1-2 loss: 0.994584  [   96/  306]
train() client id: f_00004-1-3 loss: 0.675762  [  128/  306]
train() client id: f_00004-1-4 loss: 0.847112  [  160/  306]
train() client id: f_00004-1-5 loss: 0.731044  [  192/  306]
train() client id: f_00004-1-6 loss: 0.711128  [  224/  306]
train() client id: f_00004-1-7 loss: 0.700670  [  256/  306]
train() client id: f_00004-1-8 loss: 0.762774  [  288/  306]
train() client id: f_00004-2-0 loss: 0.664423  [   32/  306]
train() client id: f_00004-2-1 loss: 0.760435  [   64/  306]
train() client id: f_00004-2-2 loss: 0.845220  [   96/  306]
train() client id: f_00004-2-3 loss: 0.690503  [  128/  306]
train() client id: f_00004-2-4 loss: 0.606076  [  160/  306]
train() client id: f_00004-2-5 loss: 0.790553  [  192/  306]
train() client id: f_00004-2-6 loss: 0.788956  [  224/  306]
train() client id: f_00004-2-7 loss: 0.858625  [  256/  306]
train() client id: f_00004-2-8 loss: 0.835559  [  288/  306]
train() client id: f_00004-3-0 loss: 0.812601  [   32/  306]
train() client id: f_00004-3-1 loss: 0.762264  [   64/  306]
train() client id: f_00004-3-2 loss: 0.723200  [   96/  306]
train() client id: f_00004-3-3 loss: 0.674675  [  128/  306]
train() client id: f_00004-3-4 loss: 0.791306  [  160/  306]
train() client id: f_00004-3-5 loss: 0.781687  [  192/  306]
train() client id: f_00004-3-6 loss: 0.861529  [  224/  306]
train() client id: f_00004-3-7 loss: 0.752637  [  256/  306]
train() client id: f_00004-3-8 loss: 0.757355  [  288/  306]
train() client id: f_00004-4-0 loss: 0.646409  [   32/  306]
train() client id: f_00004-4-1 loss: 0.617392  [   64/  306]
train() client id: f_00004-4-2 loss: 0.748919  [   96/  306]
train() client id: f_00004-4-3 loss: 0.820862  [  128/  306]
train() client id: f_00004-4-4 loss: 0.846012  [  160/  306]
train() client id: f_00004-4-5 loss: 0.962761  [  192/  306]
train() client id: f_00004-4-6 loss: 0.748001  [  224/  306]
train() client id: f_00004-4-7 loss: 0.627435  [  256/  306]
train() client id: f_00004-4-8 loss: 0.899799  [  288/  306]
train() client id: f_00004-5-0 loss: 1.021151  [   32/  306]
train() client id: f_00004-5-1 loss: 0.752476  [   64/  306]
train() client id: f_00004-5-2 loss: 0.835358  [   96/  306]
train() client id: f_00004-5-3 loss: 0.615244  [  128/  306]
train() client id: f_00004-5-4 loss: 0.692952  [  160/  306]
train() client id: f_00004-5-5 loss: 0.863725  [  192/  306]
train() client id: f_00004-5-6 loss: 0.644243  [  224/  306]
train() client id: f_00004-5-7 loss: 0.621546  [  256/  306]
train() client id: f_00004-5-8 loss: 0.828386  [  288/  306]
train() client id: f_00004-6-0 loss: 0.778042  [   32/  306]
train() client id: f_00004-6-1 loss: 0.805264  [   64/  306]
train() client id: f_00004-6-2 loss: 0.770604  [   96/  306]
train() client id: f_00004-6-3 loss: 0.626577  [  128/  306]
train() client id: f_00004-6-4 loss: 0.806531  [  160/  306]
train() client id: f_00004-6-5 loss: 0.745381  [  192/  306]
train() client id: f_00004-6-6 loss: 0.799082  [  224/  306]
train() client id: f_00004-6-7 loss: 0.724775  [  256/  306]
train() client id: f_00004-6-8 loss: 0.778478  [  288/  306]
train() client id: f_00004-7-0 loss: 0.700995  [   32/  306]
train() client id: f_00004-7-1 loss: 0.766188  [   64/  306]
train() client id: f_00004-7-2 loss: 0.652177  [   96/  306]
train() client id: f_00004-7-3 loss: 0.801395  [  128/  306]
train() client id: f_00004-7-4 loss: 0.907295  [  160/  306]
train() client id: f_00004-7-5 loss: 0.759716  [  192/  306]
train() client id: f_00004-7-6 loss: 0.799119  [  224/  306]
train() client id: f_00004-7-7 loss: 0.644831  [  256/  306]
train() client id: f_00004-7-8 loss: 0.718773  [  288/  306]
train() client id: f_00004-8-0 loss: 0.670838  [   32/  306]
train() client id: f_00004-8-1 loss: 0.637812  [   64/  306]
train() client id: f_00004-8-2 loss: 0.731827  [   96/  306]
train() client id: f_00004-8-3 loss: 0.718511  [  128/  306]
train() client id: f_00004-8-4 loss: 0.745590  [  160/  306]
train() client id: f_00004-8-5 loss: 0.892359  [  192/  306]
train() client id: f_00004-8-6 loss: 0.815475  [  224/  306]
train() client id: f_00004-8-7 loss: 0.776681  [  256/  306]
train() client id: f_00004-8-8 loss: 0.760466  [  288/  306]
train() client id: f_00004-9-0 loss: 0.671817  [   32/  306]
train() client id: f_00004-9-1 loss: 0.812382  [   64/  306]
train() client id: f_00004-9-2 loss: 0.804790  [   96/  306]
train() client id: f_00004-9-3 loss: 0.751862  [  128/  306]
train() client id: f_00004-9-4 loss: 0.696987  [  160/  306]
train() client id: f_00004-9-5 loss: 0.662731  [  192/  306]
train() client id: f_00004-9-6 loss: 0.756718  [  224/  306]
train() client id: f_00004-9-7 loss: 0.813811  [  256/  306]
train() client id: f_00004-9-8 loss: 0.825087  [  288/  306]
train() client id: f_00004-10-0 loss: 0.771672  [   32/  306]
train() client id: f_00004-10-1 loss: 0.686216  [   64/  306]
train() client id: f_00004-10-2 loss: 0.740128  [   96/  306]
train() client id: f_00004-10-3 loss: 0.885189  [  128/  306]
train() client id: f_00004-10-4 loss: 0.729948  [  160/  306]
train() client id: f_00004-10-5 loss: 0.645051  [  192/  306]
train() client id: f_00004-10-6 loss: 0.800548  [  224/  306]
train() client id: f_00004-10-7 loss: 0.736777  [  256/  306]
train() client id: f_00004-10-8 loss: 0.864792  [  288/  306]
train() client id: f_00004-11-0 loss: 0.779229  [   32/  306]
train() client id: f_00004-11-1 loss: 0.718994  [   64/  306]
train() client id: f_00004-11-2 loss: 0.782862  [   96/  306]
train() client id: f_00004-11-3 loss: 0.854569  [  128/  306]
train() client id: f_00004-11-4 loss: 0.675225  [  160/  306]
train() client id: f_00004-11-5 loss: 0.730785  [  192/  306]
train() client id: f_00004-11-6 loss: 0.740530  [  224/  306]
train() client id: f_00004-11-7 loss: 0.737613  [  256/  306]
train() client id: f_00004-11-8 loss: 0.803129  [  288/  306]
train() client id: f_00004-12-0 loss: 0.738565  [   32/  306]
train() client id: f_00004-12-1 loss: 0.772148  [   64/  306]
train() client id: f_00004-12-2 loss: 0.686921  [   96/  306]
train() client id: f_00004-12-3 loss: 0.662404  [  128/  306]
train() client id: f_00004-12-4 loss: 0.768768  [  160/  306]
train() client id: f_00004-12-5 loss: 0.760924  [  192/  306]
train() client id: f_00004-12-6 loss: 0.718562  [  224/  306]
train() client id: f_00004-12-7 loss: 0.914443  [  256/  306]
train() client id: f_00004-12-8 loss: 0.816580  [  288/  306]
train() client id: f_00004-13-0 loss: 0.738766  [   32/  306]
train() client id: f_00004-13-1 loss: 0.740262  [   64/  306]
train() client id: f_00004-13-2 loss: 0.732818  [   96/  306]
train() client id: f_00004-13-3 loss: 0.641272  [  128/  306]
train() client id: f_00004-13-4 loss: 1.074552  [  160/  306]
train() client id: f_00004-13-5 loss: 0.737278  [  192/  306]
train() client id: f_00004-13-6 loss: 0.705570  [  224/  306]
train() client id: f_00004-13-7 loss: 0.785976  [  256/  306]
train() client id: f_00004-13-8 loss: 0.710907  [  288/  306]
train() client id: f_00004-14-0 loss: 0.798931  [   32/  306]
train() client id: f_00004-14-1 loss: 0.798801  [   64/  306]
train() client id: f_00004-14-2 loss: 0.823640  [   96/  306]
train() client id: f_00004-14-3 loss: 0.729139  [  128/  306]
train() client id: f_00004-14-4 loss: 0.756819  [  160/  306]
train() client id: f_00004-14-5 loss: 0.825277  [  192/  306]
train() client id: f_00004-14-6 loss: 0.761733  [  224/  306]
train() client id: f_00004-14-7 loss: 0.700477  [  256/  306]
train() client id: f_00004-14-8 loss: 0.712609  [  288/  306]
train() client id: f_00004-15-0 loss: 0.754818  [   32/  306]
train() client id: f_00004-15-1 loss: 0.742108  [   64/  306]
train() client id: f_00004-15-2 loss: 0.866091  [   96/  306]
train() client id: f_00004-15-3 loss: 0.617368  [  128/  306]
train() client id: f_00004-15-4 loss: 0.738176  [  160/  306]
train() client id: f_00004-15-5 loss: 0.797701  [  192/  306]
train() client id: f_00004-15-6 loss: 0.741144  [  224/  306]
train() client id: f_00004-15-7 loss: 0.733916  [  256/  306]
train() client id: f_00004-15-8 loss: 0.815701  [  288/  306]
train() client id: f_00004-16-0 loss: 0.852118  [   32/  306]
train() client id: f_00004-16-1 loss: 0.815094  [   64/  306]
train() client id: f_00004-16-2 loss: 0.747190  [   96/  306]
train() client id: f_00004-16-3 loss: 0.642268  [  128/  306]
train() client id: f_00004-16-4 loss: 0.771063  [  160/  306]
train() client id: f_00004-16-5 loss: 0.713017  [  192/  306]
train() client id: f_00004-16-6 loss: 0.788629  [  224/  306]
train() client id: f_00004-16-7 loss: 0.800300  [  256/  306]
train() client id: f_00004-16-8 loss: 0.727970  [  288/  306]
train() client id: f_00004-17-0 loss: 0.770081  [   32/  306]
train() client id: f_00004-17-1 loss: 0.705281  [   64/  306]
train() client id: f_00004-17-2 loss: 0.839673  [   96/  306]
train() client id: f_00004-17-3 loss: 0.680955  [  128/  306]
train() client id: f_00004-17-4 loss: 0.821445  [  160/  306]
train() client id: f_00004-17-5 loss: 0.738514  [  192/  306]
train() client id: f_00004-17-6 loss: 0.743560  [  224/  306]
train() client id: f_00004-17-7 loss: 0.650068  [  256/  306]
train() client id: f_00004-17-8 loss: 0.928368  [  288/  306]
train() client id: f_00005-0-0 loss: 0.583573  [   32/  146]
train() client id: f_00005-0-1 loss: 0.642592  [   64/  146]
train() client id: f_00005-0-2 loss: 0.385327  [   96/  146]
train() client id: f_00005-0-3 loss: 0.459324  [  128/  146]
train() client id: f_00005-1-0 loss: 0.394453  [   32/  146]
train() client id: f_00005-1-1 loss: 0.400532  [   64/  146]
train() client id: f_00005-1-2 loss: 0.669857  [   96/  146]
train() client id: f_00005-1-3 loss: 0.429859  [  128/  146]
train() client id: f_00005-2-0 loss: 0.410296  [   32/  146]
train() client id: f_00005-2-1 loss: 0.373285  [   64/  146]
train() client id: f_00005-2-2 loss: 0.639150  [   96/  146]
train() client id: f_00005-2-3 loss: 0.605689  [  128/  146]
train() client id: f_00005-3-0 loss: 0.585683  [   32/  146]
train() client id: f_00005-3-1 loss: 0.440920  [   64/  146]
train() client id: f_00005-3-2 loss: 0.502865  [   96/  146]
train() client id: f_00005-3-3 loss: 0.630350  [  128/  146]
train() client id: f_00005-4-0 loss: 0.531625  [   32/  146]
train() client id: f_00005-4-1 loss: 0.463579  [   64/  146]
train() client id: f_00005-4-2 loss: 0.602898  [   96/  146]
train() client id: f_00005-4-3 loss: 0.516378  [  128/  146]
train() client id: f_00005-5-0 loss: 0.673465  [   32/  146]
train() client id: f_00005-5-1 loss: 0.312822  [   64/  146]
train() client id: f_00005-5-2 loss: 0.490470  [   96/  146]
train() client id: f_00005-5-3 loss: 0.485444  [  128/  146]
train() client id: f_00005-6-0 loss: 0.529779  [   32/  146]
train() client id: f_00005-6-1 loss: 0.733365  [   64/  146]
train() client id: f_00005-6-2 loss: 0.339982  [   96/  146]
train() client id: f_00005-6-3 loss: 0.367272  [  128/  146]
train() client id: f_00005-7-0 loss: 0.398231  [   32/  146]
train() client id: f_00005-7-1 loss: 0.624393  [   64/  146]
train() client id: f_00005-7-2 loss: 0.443451  [   96/  146]
train() client id: f_00005-7-3 loss: 0.754471  [  128/  146]
train() client id: f_00005-8-0 loss: 0.629507  [   32/  146]
train() client id: f_00005-8-1 loss: 0.207762  [   64/  146]
train() client id: f_00005-8-2 loss: 0.620451  [   96/  146]
train() client id: f_00005-8-3 loss: 0.562646  [  128/  146]
train() client id: f_00005-9-0 loss: 0.389370  [   32/  146]
train() client id: f_00005-9-1 loss: 0.515449  [   64/  146]
train() client id: f_00005-9-2 loss: 0.353229  [   96/  146]
train() client id: f_00005-9-3 loss: 0.790577  [  128/  146]
train() client id: f_00005-10-0 loss: 0.550650  [   32/  146]
train() client id: f_00005-10-1 loss: 0.708943  [   64/  146]
train() client id: f_00005-10-2 loss: 0.663225  [   96/  146]
train() client id: f_00005-10-3 loss: 0.235396  [  128/  146]
train() client id: f_00005-11-0 loss: 0.698538  [   32/  146]
train() client id: f_00005-11-1 loss: 0.405389  [   64/  146]
train() client id: f_00005-11-2 loss: 0.381261  [   96/  146]
train() client id: f_00005-11-3 loss: 0.663109  [  128/  146]
train() client id: f_00005-12-0 loss: 0.825021  [   32/  146]
train() client id: f_00005-12-1 loss: 0.544575  [   64/  146]
train() client id: f_00005-12-2 loss: 0.194634  [   96/  146]
train() client id: f_00005-12-3 loss: 0.532794  [  128/  146]
train() client id: f_00005-13-0 loss: 0.234312  [   32/  146]
train() client id: f_00005-13-1 loss: 0.834216  [   64/  146]
train() client id: f_00005-13-2 loss: 0.721143  [   96/  146]
train() client id: f_00005-13-3 loss: 0.492295  [  128/  146]
train() client id: f_00005-14-0 loss: 0.554031  [   32/  146]
train() client id: f_00005-14-1 loss: 0.503744  [   64/  146]
train() client id: f_00005-14-2 loss: 0.521573  [   96/  146]
train() client id: f_00005-14-3 loss: 0.394925  [  128/  146]
train() client id: f_00005-15-0 loss: 0.717639  [   32/  146]
train() client id: f_00005-15-1 loss: 0.213275  [   64/  146]
train() client id: f_00005-15-2 loss: 0.672534  [   96/  146]
train() client id: f_00005-15-3 loss: 0.363123  [  128/  146]
train() client id: f_00005-16-0 loss: 0.325009  [   32/  146]
train() client id: f_00005-16-1 loss: 0.656133  [   64/  146]
train() client id: f_00005-16-2 loss: 0.850601  [   96/  146]
train() client id: f_00005-16-3 loss: 0.458454  [  128/  146]
train() client id: f_00005-17-0 loss: 0.440612  [   32/  146]
train() client id: f_00005-17-1 loss: 0.593818  [   64/  146]
train() client id: f_00005-17-2 loss: 0.373143  [   96/  146]
train() client id: f_00005-17-3 loss: 0.641486  [  128/  146]
train() client id: f_00006-0-0 loss: 0.419036  [   32/   54]
train() client id: f_00006-1-0 loss: 0.503428  [   32/   54]
train() client id: f_00006-2-0 loss: 0.543173  [   32/   54]
train() client id: f_00006-3-0 loss: 0.444002  [   32/   54]
train() client id: f_00006-4-0 loss: 0.554445  [   32/   54]
train() client id: f_00006-5-0 loss: 0.466164  [   32/   54]
train() client id: f_00006-6-0 loss: 0.527165  [   32/   54]
train() client id: f_00006-7-0 loss: 0.555337  [   32/   54]
train() client id: f_00006-8-0 loss: 0.475051  [   32/   54]
train() client id: f_00006-9-0 loss: 0.495850  [   32/   54]
train() client id: f_00006-10-0 loss: 0.547085  [   32/   54]
train() client id: f_00006-11-0 loss: 0.534737  [   32/   54]
train() client id: f_00006-12-0 loss: 0.535811  [   32/   54]
train() client id: f_00006-13-0 loss: 0.522998  [   32/   54]
train() client id: f_00006-14-0 loss: 0.490249  [   32/   54]
train() client id: f_00006-15-0 loss: 0.463255  [   32/   54]
train() client id: f_00006-16-0 loss: 0.525436  [   32/   54]
train() client id: f_00006-17-0 loss: 0.540189  [   32/   54]
train() client id: f_00007-0-0 loss: 0.312010  [   32/  179]
train() client id: f_00007-0-1 loss: 0.707241  [   64/  179]
train() client id: f_00007-0-2 loss: 0.452928  [   96/  179]
train() client id: f_00007-0-3 loss: 0.460225  [  128/  179]
train() client id: f_00007-0-4 loss: 0.258818  [  160/  179]
train() client id: f_00007-1-0 loss: 0.411410  [   32/  179]
train() client id: f_00007-1-1 loss: 0.579294  [   64/  179]
train() client id: f_00007-1-2 loss: 0.228669  [   96/  179]
train() client id: f_00007-1-3 loss: 0.425731  [  128/  179]
train() client id: f_00007-1-4 loss: 0.352605  [  160/  179]
train() client id: f_00007-2-0 loss: 0.564692  [   32/  179]
train() client id: f_00007-2-1 loss: 0.300822  [   64/  179]
train() client id: f_00007-2-2 loss: 0.325866  [   96/  179]
train() client id: f_00007-2-3 loss: 0.264853  [  128/  179]
train() client id: f_00007-2-4 loss: 0.364503  [  160/  179]
train() client id: f_00007-3-0 loss: 0.448180  [   32/  179]
train() client id: f_00007-3-1 loss: 0.204585  [   64/  179]
train() client id: f_00007-3-2 loss: 0.316230  [   96/  179]
train() client id: f_00007-3-3 loss: 0.496700  [  128/  179]
train() client id: f_00007-3-4 loss: 0.448090  [  160/  179]
train() client id: f_00007-4-0 loss: 0.265769  [   32/  179]
train() client id: f_00007-4-1 loss: 0.376484  [   64/  179]
train() client id: f_00007-4-2 loss: 0.440092  [   96/  179]
train() client id: f_00007-4-3 loss: 0.627240  [  128/  179]
train() client id: f_00007-4-4 loss: 0.199664  [  160/  179]
train() client id: f_00007-5-0 loss: 0.267510  [   32/  179]
train() client id: f_00007-5-1 loss: 0.277439  [   64/  179]
train() client id: f_00007-5-2 loss: 0.530831  [   96/  179]
train() client id: f_00007-5-3 loss: 0.373729  [  128/  179]
train() client id: f_00007-5-4 loss: 0.392053  [  160/  179]
train() client id: f_00007-6-0 loss: 0.534749  [   32/  179]
train() client id: f_00007-6-1 loss: 0.298553  [   64/  179]
train() client id: f_00007-6-2 loss: 0.386006  [   96/  179]
train() client id: f_00007-6-3 loss: 0.298563  [  128/  179]
train() client id: f_00007-6-4 loss: 0.272071  [  160/  179]
train() client id: f_00007-7-0 loss: 0.419758  [   32/  179]
train() client id: f_00007-7-1 loss: 0.212959  [   64/  179]
train() client id: f_00007-7-2 loss: 0.217953  [   96/  179]
train() client id: f_00007-7-3 loss: 0.383126  [  128/  179]
train() client id: f_00007-7-4 loss: 0.342785  [  160/  179]
train() client id: f_00007-8-0 loss: 0.163726  [   32/  179]
train() client id: f_00007-8-1 loss: 0.285104  [   64/  179]
train() client id: f_00007-8-2 loss: 0.273555  [   96/  179]
train() client id: f_00007-8-3 loss: 0.480963  [  128/  179]
train() client id: f_00007-8-4 loss: 0.468556  [  160/  179]
train() client id: f_00007-9-0 loss: 0.427322  [   32/  179]
train() client id: f_00007-9-1 loss: 0.260338  [   64/  179]
train() client id: f_00007-9-2 loss: 0.302695  [   96/  179]
train() client id: f_00007-9-3 loss: 0.392836  [  128/  179]
train() client id: f_00007-9-4 loss: 0.352213  [  160/  179]
train() client id: f_00007-10-0 loss: 0.253381  [   32/  179]
train() client id: f_00007-10-1 loss: 0.535289  [   64/  179]
train() client id: f_00007-10-2 loss: 0.392044  [   96/  179]
train() client id: f_00007-10-3 loss: 0.212239  [  128/  179]
train() client id: f_00007-10-4 loss: 0.298977  [  160/  179]
train() client id: f_00007-11-0 loss: 0.223153  [   32/  179]
train() client id: f_00007-11-1 loss: 0.541527  [   64/  179]
train() client id: f_00007-11-2 loss: 0.270056  [   96/  179]
train() client id: f_00007-11-3 loss: 0.287241  [  128/  179]
train() client id: f_00007-11-4 loss: 0.368802  [  160/  179]
train() client id: f_00007-12-0 loss: 0.465432  [   32/  179]
train() client id: f_00007-12-1 loss: 0.427154  [   64/  179]
train() client id: f_00007-12-2 loss: 0.152094  [   96/  179]
train() client id: f_00007-12-3 loss: 0.326198  [  128/  179]
train() client id: f_00007-12-4 loss: 0.160806  [  160/  179]
train() client id: f_00007-13-0 loss: 0.266167  [   32/  179]
train() client id: f_00007-13-1 loss: 0.251659  [   64/  179]
train() client id: f_00007-13-2 loss: 0.317985  [   96/  179]
train() client id: f_00007-13-3 loss: 0.169003  [  128/  179]
train() client id: f_00007-13-4 loss: 0.557006  [  160/  179]
train() client id: f_00007-14-0 loss: 0.439698  [   32/  179]
train() client id: f_00007-14-1 loss: 0.370949  [   64/  179]
train() client id: f_00007-14-2 loss: 0.332612  [   96/  179]
train() client id: f_00007-14-3 loss: 0.183005  [  128/  179]
train() client id: f_00007-14-4 loss: 0.240845  [  160/  179]
train() client id: f_00007-15-0 loss: 0.111061  [   32/  179]
train() client id: f_00007-15-1 loss: 0.176627  [   64/  179]
train() client id: f_00007-15-2 loss: 0.442433  [   96/  179]
train() client id: f_00007-15-3 loss: 0.338625  [  128/  179]
train() client id: f_00007-15-4 loss: 0.274372  [  160/  179]
train() client id: f_00007-16-0 loss: 0.260109  [   32/  179]
train() client id: f_00007-16-1 loss: 0.162189  [   64/  179]
train() client id: f_00007-16-2 loss: 0.397427  [   96/  179]
train() client id: f_00007-16-3 loss: 0.363457  [  128/  179]
train() client id: f_00007-16-4 loss: 0.390445  [  160/  179]
train() client id: f_00007-17-0 loss: 0.278717  [   32/  179]
train() client id: f_00007-17-1 loss: 0.225147  [   64/  179]
train() client id: f_00007-17-2 loss: 0.165319  [   96/  179]
train() client id: f_00007-17-3 loss: 0.433319  [  128/  179]
train() client id: f_00007-17-4 loss: 0.550372  [  160/  179]
train() client id: f_00008-0-0 loss: 0.721834  [   32/  130]
train() client id: f_00008-0-1 loss: 0.816311  [   64/  130]
train() client id: f_00008-0-2 loss: 0.755608  [   96/  130]
train() client id: f_00008-0-3 loss: 0.631767  [  128/  130]
train() client id: f_00008-1-0 loss: 0.690843  [   32/  130]
train() client id: f_00008-1-1 loss: 0.789012  [   64/  130]
train() client id: f_00008-1-2 loss: 0.731562  [   96/  130]
train() client id: f_00008-1-3 loss: 0.716580  [  128/  130]
train() client id: f_00008-2-0 loss: 0.724146  [   32/  130]
train() client id: f_00008-2-1 loss: 0.791447  [   64/  130]
train() client id: f_00008-2-2 loss: 0.774507  [   96/  130]
train() client id: f_00008-2-3 loss: 0.635825  [  128/  130]
train() client id: f_00008-3-0 loss: 0.635163  [   32/  130]
train() client id: f_00008-3-1 loss: 0.879261  [   64/  130]
train() client id: f_00008-3-2 loss: 0.653800  [   96/  130]
train() client id: f_00008-3-3 loss: 0.715640  [  128/  130]
train() client id: f_00008-4-0 loss: 0.654667  [   32/  130]
train() client id: f_00008-4-1 loss: 0.709584  [   64/  130]
train() client id: f_00008-4-2 loss: 0.822818  [   96/  130]
train() client id: f_00008-4-3 loss: 0.714715  [  128/  130]
train() client id: f_00008-5-0 loss: 0.603711  [   32/  130]
train() client id: f_00008-5-1 loss: 0.783596  [   64/  130]
train() client id: f_00008-5-2 loss: 0.794972  [   96/  130]
train() client id: f_00008-5-3 loss: 0.736162  [  128/  130]
train() client id: f_00008-6-0 loss: 0.630189  [   32/  130]
train() client id: f_00008-6-1 loss: 0.864448  [   64/  130]
train() client id: f_00008-6-2 loss: 0.704923  [   96/  130]
train() client id: f_00008-6-3 loss: 0.678106  [  128/  130]
train() client id: f_00008-7-0 loss: 0.666320  [   32/  130]
train() client id: f_00008-7-1 loss: 0.842753  [   64/  130]
train() client id: f_00008-7-2 loss: 0.700425  [   96/  130]
train() client id: f_00008-7-3 loss: 0.698135  [  128/  130]
train() client id: f_00008-8-0 loss: 0.727804  [   32/  130]
train() client id: f_00008-8-1 loss: 0.699362  [   64/  130]
train() client id: f_00008-8-2 loss: 0.783198  [   96/  130]
train() client id: f_00008-8-3 loss: 0.671035  [  128/  130]
train() client id: f_00008-9-0 loss: 0.684500  [   32/  130]
train() client id: f_00008-9-1 loss: 0.702191  [   64/  130]
train() client id: f_00008-9-2 loss: 0.602960  [   96/  130]
train() client id: f_00008-9-3 loss: 0.893929  [  128/  130]
train() client id: f_00008-10-0 loss: 0.776564  [   32/  130]
train() client id: f_00008-10-1 loss: 0.644620  [   64/  130]
train() client id: f_00008-10-2 loss: 0.649347  [   96/  130]
train() client id: f_00008-10-3 loss: 0.833144  [  128/  130]
train() client id: f_00008-11-0 loss: 0.742643  [   32/  130]
train() client id: f_00008-11-1 loss: 0.792108  [   64/  130]
train() client id: f_00008-11-2 loss: 0.704257  [   96/  130]
train() client id: f_00008-11-3 loss: 0.666872  [  128/  130]
train() client id: f_00008-12-0 loss: 0.727338  [   32/  130]
train() client id: f_00008-12-1 loss: 0.715565  [   64/  130]
train() client id: f_00008-12-2 loss: 0.707489  [   96/  130]
train() client id: f_00008-12-3 loss: 0.708515  [  128/  130]
train() client id: f_00008-13-0 loss: 0.808025  [   32/  130]
train() client id: f_00008-13-1 loss: 0.602302  [   64/  130]
train() client id: f_00008-13-2 loss: 0.685001  [   96/  130]
train() client id: f_00008-13-3 loss: 0.776423  [  128/  130]
train() client id: f_00008-14-0 loss: 0.742750  [   32/  130]
train() client id: f_00008-14-1 loss: 0.705465  [   64/  130]
train() client id: f_00008-14-2 loss: 0.733880  [   96/  130]
train() client id: f_00008-14-3 loss: 0.708926  [  128/  130]
train() client id: f_00008-15-0 loss: 0.713072  [   32/  130]
train() client id: f_00008-15-1 loss: 0.744868  [   64/  130]
train() client id: f_00008-15-2 loss: 0.689422  [   96/  130]
train() client id: f_00008-15-3 loss: 0.709786  [  128/  130]
train() client id: f_00008-16-0 loss: 0.665798  [   32/  130]
train() client id: f_00008-16-1 loss: 0.783088  [   64/  130]
train() client id: f_00008-16-2 loss: 0.794564  [   96/  130]
train() client id: f_00008-16-3 loss: 0.644861  [  128/  130]
train() client id: f_00008-17-0 loss: 0.710887  [   32/  130]
train() client id: f_00008-17-1 loss: 0.806304  [   64/  130]
train() client id: f_00008-17-2 loss: 0.616195  [   96/  130]
train() client id: f_00008-17-3 loss: 0.724103  [  128/  130]
train() client id: f_00009-0-0 loss: 1.163255  [   32/  118]
train() client id: f_00009-0-1 loss: 1.139628  [   64/  118]
train() client id: f_00009-0-2 loss: 1.193174  [   96/  118]
train() client id: f_00009-1-0 loss: 0.947417  [   32/  118]
train() client id: f_00009-1-1 loss: 1.142663  [   64/  118]
train() client id: f_00009-1-2 loss: 1.145344  [   96/  118]
train() client id: f_00009-2-0 loss: 1.088974  [   32/  118]
train() client id: f_00009-2-1 loss: 0.970103  [   64/  118]
train() client id: f_00009-2-2 loss: 0.981758  [   96/  118]
train() client id: f_00009-3-0 loss: 0.909485  [   32/  118]
train() client id: f_00009-3-1 loss: 0.981150  [   64/  118]
train() client id: f_00009-3-2 loss: 1.097564  [   96/  118]
train() client id: f_00009-4-0 loss: 0.949494  [   32/  118]
train() client id: f_00009-4-1 loss: 0.856124  [   64/  118]
train() client id: f_00009-4-2 loss: 1.032426  [   96/  118]
train() client id: f_00009-5-0 loss: 0.976321  [   32/  118]
train() client id: f_00009-5-1 loss: 0.975478  [   64/  118]
train() client id: f_00009-5-2 loss: 0.925881  [   96/  118]
train() client id: f_00009-6-0 loss: 0.963246  [   32/  118]
train() client id: f_00009-6-1 loss: 0.850295  [   64/  118]
train() client id: f_00009-6-2 loss: 0.826512  [   96/  118]
train() client id: f_00009-7-0 loss: 0.932007  [   32/  118]
train() client id: f_00009-7-1 loss: 0.894377  [   64/  118]
train() client id: f_00009-7-2 loss: 0.732852  [   96/  118]
train() client id: f_00009-8-0 loss: 0.684659  [   32/  118]
train() client id: f_00009-8-1 loss: 0.909894  [   64/  118]
train() client id: f_00009-8-2 loss: 0.990714  [   96/  118]
train() client id: f_00009-9-0 loss: 0.791369  [   32/  118]
train() client id: f_00009-9-1 loss: 0.898172  [   64/  118]
train() client id: f_00009-9-2 loss: 0.866192  [   96/  118]
train() client id: f_00009-10-0 loss: 0.936950  [   32/  118]
train() client id: f_00009-10-1 loss: 0.777768  [   64/  118]
train() client id: f_00009-10-2 loss: 0.843726  [   96/  118]
train() client id: f_00009-11-0 loss: 0.903341  [   32/  118]
train() client id: f_00009-11-1 loss: 0.856613  [   64/  118]
train() client id: f_00009-11-2 loss: 0.759317  [   96/  118]
train() client id: f_00009-12-0 loss: 0.890888  [   32/  118]
train() client id: f_00009-12-1 loss: 0.874211  [   64/  118]
train() client id: f_00009-12-2 loss: 0.751227  [   96/  118]
train() client id: f_00009-13-0 loss: 0.835507  [   32/  118]
train() client id: f_00009-13-1 loss: 0.899927  [   64/  118]
train() client id: f_00009-13-2 loss: 0.694667  [   96/  118]
train() client id: f_00009-14-0 loss: 0.776424  [   32/  118]
train() client id: f_00009-14-1 loss: 0.735889  [   64/  118]
train() client id: f_00009-14-2 loss: 0.802211  [   96/  118]
train() client id: f_00009-15-0 loss: 0.758057  [   32/  118]
train() client id: f_00009-15-1 loss: 0.906196  [   64/  118]
train() client id: f_00009-15-2 loss: 0.895285  [   96/  118]
train() client id: f_00009-16-0 loss: 0.806933  [   32/  118]
train() client id: f_00009-16-1 loss: 0.729709  [   64/  118]
train() client id: f_00009-16-2 loss: 1.003296  [   96/  118]
train() client id: f_00009-17-0 loss: 0.825154  [   32/  118]
train() client id: f_00009-17-1 loss: 0.848740  [   64/  118]
train() client id: f_00009-17-2 loss: 0.822720  [   96/  118]
At round 63 accuracy: 0.649867374005305
At round 63 training accuracy: 0.5915492957746479
At round 63 training loss: 0.837705797353281
update_location
xs = 8.927491 436.223621 5.882650 0.934260 -352.581990 -200.230757 -160.849135 -5.143845 -375.120581 20.134486 
ys = -427.390647 7.291448 325.684448 -147.290817 -9.642386 0.794442 -1.381692 321.628436 25.881276 -862.232496 
xs mean: -62.18237997052123
ys mean: -76.66579882624052
dists_uav = 439.024447 447.598271 340.741787 178.032182 366.615650 223.814627 189.405262 336.855028 389.082627 868.245515 
uav_gains = -122.684836 -122.933700 -118.644071 -106.302623 -120.016490 -109.256448 -107.018759 -118.408549 -120.987087 -130.438705 
uav_gains_db_mean: -117.669126787989
dists_bs = 629.844277 633.814226 235.423737 367.184197 256.179964 176.023202 176.948434 224.390311 249.568910 1055.428216 
bs_gains = -117.945588 -118.021995 -105.978866 -111.383811 -107.006324 -102.443017 -102.506767 -105.395174 -106.688393 -124.223063 
bs_gains_db_mean: -110.15929970497271
Round 64
-------------------------------
ene_coms = [0.04081277 0.04242824 0.00837008 0.00852028 0.00886534 0.01017275
 0.00887168 0.00811187 0.00870617 0.0523281 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.66080417 5.32167047 2.41753348 0.87385867 5.89130268 2.8484592
 1.08584439 3.4647199  2.53407516 2.58105887]
obj_prev = 29.679327003698795
eta_min = 5.471068037173868e-37	eta_max = 0.8769096328051732
af = 6.112305432346556	bf = 1.2343269889663975	zeta = 6.723535975581212	eta = 0.909090909090909
af = 6.112305432346556	bf = 1.2343269889663975	zeta = 19.98581110616527	eta = 0.30583224267845793
af = 6.112305432346556	bf = 1.2343269889663975	zeta = 12.209856410626267	eta = 0.5006042026036442
af = 6.112305432346556	bf = 1.2343269889663975	zeta = 10.940565803767527	eta = 0.5586827538884418
af = 6.112305432346556	bf = 1.2343269889663975	zeta = 10.860099451160776	eta = 0.562822233795773
af = 6.112305432346556	bf = 1.2343269889663975	zeta = 10.859719435007547	eta = 0.562841928737389
af = 6.112305432346556	bf = 1.2343269889663975	zeta = 10.85971942644736	eta = 0.5628419291810498
eta = 0.5628419291810498
ene_coms = [0.04081277 0.04242824 0.00837008 0.00852028 0.00886534 0.01017275
 0.00887168 0.00811187 0.00870617 0.0523281 ]
ene_comp = [0.04742753 0.09974837 0.04667471 0.01618558 0.11518113 0.0549557
 0.02032608 0.0673772  0.04893316 0.04441625]
ene_total = [1.26351326 2.03582751 0.78818661 0.35376335 1.77622211 0.93257465
 0.41808294 1.0809283  0.82533787 1.38528282]
ti_comp = [1.4593283  1.44317365 1.78375521 1.78225325 1.77880259 1.76572853
 1.7787392  1.78633736 1.78039432 1.34417507]
ti_coms = [0.40812772 0.42428237 0.08370081 0.08520277 0.08865343 0.10172749
 0.08871682 0.08111866 0.0870617  0.52328095]
t_total = [26.7401123 26.7401123 26.7401123 26.7401123 26.7401123 26.7401123
 26.7401123 26.7401123 26.7401123 26.7401123]
ene_coms = [0.04081277 0.04242824 0.00837008 0.00852028 0.00886534 0.01017275
 0.00887168 0.00811187 0.00870617 0.0523281 ]
ene_comp = [3.13087258e-06 2.97824356e-05 1.99735066e-06 8.34307801e-08
 3.01834265e-05 3.32713536e-06 1.65888583e-07 5.99089763e-06
 2.31024174e-06 3.03105711e-06]
ene_total = [0.58444312 0.6079566  0.11987984 0.12200308 0.12737508 0.14571129
 0.12703604 0.11623963 0.12469677 0.74932966]
optimize_network iter = 0 obj = 2.8246711247374297
eta = 0.5628419291810498
freqs = [16249779.07720727 34558684.73583352 13083271.48257668  4540764.32022793
 32376030.03114335 15561764.69705248  5713620.69959509 18859035.18999602
 13742225.59084607 16521751.76277467]
eta_min = 0.5628419291810518	eta_max = 0.562841929181054
af = 0.000871309259831323	bf = 1.2343269889663975	zeta = 0.0009584401858144554	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.04081277 0.04242824 0.00837008 0.00852028 0.00886534 0.01017275
 0.00887168 0.00811187 0.00870617 0.0523281 ]
ene_comp = [5.19183431e-07 4.93873407e-06 3.31214811e-07 1.38350819e-08
 5.00522923e-06 5.51729114e-07 2.75088180e-08 9.93453010e-07
 3.83100622e-07 5.02631323e-07]
ene_total = [2.81025445 2.92179383 0.57635611 0.58667616 0.61077983 0.70049644
 0.61087361 0.55862192 0.59950151 3.60315651]
ti_comp = [1.4593283  1.44317365 1.78375521 1.78225325 1.77880259 1.76572853
 1.7787392  1.78633736 1.78039432 1.34417507]
ti_coms = [0.40812772 0.42428237 0.08370081 0.08520277 0.08865343 0.10172749
 0.08871682 0.08111866 0.0870617  0.52328095]
t_total = [26.7401123 26.7401123 26.7401123 26.7401123 26.7401123 26.7401123
 26.7401123 26.7401123 26.7401123 26.7401123]
ene_coms = [0.04081277 0.04242824 0.00837008 0.00852028 0.00886534 0.01017275
 0.00887168 0.00811187 0.00870617 0.0523281 ]
ene_comp = [3.13087258e-06 2.97824356e-05 1.99735066e-06 8.34307801e-08
 3.01834265e-05 3.32713536e-06 1.65888583e-07 5.99089763e-06
 2.31024174e-06 3.03105711e-06]
ene_total = [0.58444312 0.6079566  0.11987984 0.12200308 0.12737508 0.14571129
 0.12703604 0.11623963 0.12469677 0.74932966]
optimize_network iter = 1 obj = 2.8246711247374563
eta = 0.562841929181054
freqs = [16249779.07720726 34558684.7358335  13083271.48257664  4540764.32022792
 32376030.03114326 15561764.69705244  5713620.69959507 18859035.18999596
 13742225.59084603 16521751.76277467]
Done!
ene_coms = [0.04081277 0.04242824 0.00837008 0.00852028 0.00886534 0.01017275
 0.00887168 0.00811187 0.00870617 0.0523281 ]
ene_comp = [2.99438733e-06 2.84841192e-05 1.91027944e-06 7.97937522e-08
 2.88676296e-05 3.18209436e-06 1.58656943e-07 5.72973429e-06
 2.20953054e-06 2.89892315e-06]
ene_total = [0.04081577 0.04245672 0.00837199 0.00852036 0.00889421 0.01017593
 0.00887184 0.0081176  0.00870838 0.05233099]
At round 64 energy consumption: 0.19726378782049386
At round 64 eta: 0.562841929181054
At round 64 a_n: 6.259668650218071
At round 64 local rounds: 18.82044646554608
At round 64 global rounds: 14.319005110649288
gradient difference: 0.3383064866065979
train() client id: f_00000-0-0 loss: 1.515219  [   32/  126]
train() client id: f_00000-0-1 loss: 1.177655  [   64/  126]
train() client id: f_00000-0-2 loss: 1.064107  [   96/  126]
train() client id: f_00000-1-0 loss: 1.225807  [   32/  126]
train() client id: f_00000-1-1 loss: 1.108903  [   64/  126]
train() client id: f_00000-1-2 loss: 1.045995  [   96/  126]
train() client id: f_00000-2-0 loss: 0.960721  [   32/  126]
train() client id: f_00000-2-1 loss: 0.927181  [   64/  126]
train() client id: f_00000-2-2 loss: 1.235679  [   96/  126]
train() client id: f_00000-3-0 loss: 1.098680  [   32/  126]
train() client id: f_00000-3-1 loss: 1.072974  [   64/  126]
train() client id: f_00000-3-2 loss: 0.946214  [   96/  126]
train() client id: f_00000-4-0 loss: 0.894028  [   32/  126]
train() client id: f_00000-4-1 loss: 0.935522  [   64/  126]
train() client id: f_00000-4-2 loss: 0.914257  [   96/  126]
train() client id: f_00000-5-0 loss: 0.848000  [   32/  126]
train() client id: f_00000-5-1 loss: 0.883094  [   64/  126]
train() client id: f_00000-5-2 loss: 0.982442  [   96/  126]
train() client id: f_00000-6-0 loss: 0.811172  [   32/  126]
train() client id: f_00000-6-1 loss: 0.968151  [   64/  126]
train() client id: f_00000-6-2 loss: 0.951851  [   96/  126]
train() client id: f_00000-7-0 loss: 0.836383  [   32/  126]
train() client id: f_00000-7-1 loss: 0.835772  [   64/  126]
train() client id: f_00000-7-2 loss: 0.867741  [   96/  126]
train() client id: f_00000-8-0 loss: 0.896316  [   32/  126]
train() client id: f_00000-8-1 loss: 0.774802  [   64/  126]
train() client id: f_00000-8-2 loss: 0.898246  [   96/  126]
train() client id: f_00000-9-0 loss: 0.731155  [   32/  126]
train() client id: f_00000-9-1 loss: 0.775984  [   64/  126]
train() client id: f_00000-9-2 loss: 0.870220  [   96/  126]
train() client id: f_00000-10-0 loss: 0.674957  [   32/  126]
train() client id: f_00000-10-1 loss: 0.926792  [   64/  126]
train() client id: f_00000-10-2 loss: 0.892173  [   96/  126]
train() client id: f_00000-11-0 loss: 0.881793  [   32/  126]
train() client id: f_00000-11-1 loss: 0.689701  [   64/  126]
train() client id: f_00000-11-2 loss: 0.746693  [   96/  126]
train() client id: f_00000-12-0 loss: 0.856338  [   32/  126]
train() client id: f_00000-12-1 loss: 0.761033  [   64/  126]
train() client id: f_00000-12-2 loss: 0.816889  [   96/  126]
train() client id: f_00000-13-0 loss: 0.684521  [   32/  126]
train() client id: f_00000-13-1 loss: 0.834571  [   64/  126]
train() client id: f_00000-13-2 loss: 0.860256  [   96/  126]
train() client id: f_00000-14-0 loss: 0.868150  [   32/  126]
train() client id: f_00000-14-1 loss: 0.764920  [   64/  126]
train() client id: f_00000-14-2 loss: 0.730597  [   96/  126]
train() client id: f_00000-15-0 loss: 0.718658  [   32/  126]
train() client id: f_00000-15-1 loss: 0.866482  [   64/  126]
train() client id: f_00000-15-2 loss: 0.786646  [   96/  126]
train() client id: f_00000-16-0 loss: 0.814661  [   32/  126]
train() client id: f_00000-16-1 loss: 0.801155  [   64/  126]
train() client id: f_00000-16-2 loss: 0.758236  [   96/  126]
train() client id: f_00000-17-0 loss: 0.850030  [   32/  126]
train() client id: f_00000-17-1 loss: 0.614207  [   64/  126]
train() client id: f_00000-17-2 loss: 0.798924  [   96/  126]
train() client id: f_00001-0-0 loss: 0.442924  [   32/  265]
train() client id: f_00001-0-1 loss: 0.443670  [   64/  265]
train() client id: f_00001-0-2 loss: 0.413702  [   96/  265]
train() client id: f_00001-0-3 loss: 0.558680  [  128/  265]
train() client id: f_00001-0-4 loss: 0.541757  [  160/  265]
train() client id: f_00001-0-5 loss: 0.499069  [  192/  265]
train() client id: f_00001-0-6 loss: 0.371766  [  224/  265]
train() client id: f_00001-0-7 loss: 0.348056  [  256/  265]
train() client id: f_00001-1-0 loss: 0.557064  [   32/  265]
train() client id: f_00001-1-1 loss: 0.452781  [   64/  265]
train() client id: f_00001-1-2 loss: 0.396498  [   96/  265]
train() client id: f_00001-1-3 loss: 0.506516  [  128/  265]
train() client id: f_00001-1-4 loss: 0.399343  [  160/  265]
train() client id: f_00001-1-5 loss: 0.432585  [  192/  265]
train() client id: f_00001-1-6 loss: 0.403659  [  224/  265]
train() client id: f_00001-1-7 loss: 0.404674  [  256/  265]
train() client id: f_00001-2-0 loss: 0.503289  [   32/  265]
train() client id: f_00001-2-1 loss: 0.516897  [   64/  265]
train() client id: f_00001-2-2 loss: 0.495984  [   96/  265]
train() client id: f_00001-2-3 loss: 0.338443  [  128/  265]
train() client id: f_00001-2-4 loss: 0.405182  [  160/  265]
train() client id: f_00001-2-5 loss: 0.428597  [  192/  265]
train() client id: f_00001-2-6 loss: 0.448192  [  224/  265]
train() client id: f_00001-2-7 loss: 0.351382  [  256/  265]
train() client id: f_00001-3-0 loss: 0.410661  [   32/  265]
train() client id: f_00001-3-1 loss: 0.427698  [   64/  265]
train() client id: f_00001-3-2 loss: 0.465296  [   96/  265]
train() client id: f_00001-3-3 loss: 0.538044  [  128/  265]
train() client id: f_00001-3-4 loss: 0.342223  [  160/  265]
train() client id: f_00001-3-5 loss: 0.389793  [  192/  265]
train() client id: f_00001-3-6 loss: 0.452044  [  224/  265]
train() client id: f_00001-3-7 loss: 0.416203  [  256/  265]
train() client id: f_00001-4-0 loss: 0.402869  [   32/  265]
train() client id: f_00001-4-1 loss: 0.368247  [   64/  265]
train() client id: f_00001-4-2 loss: 0.479248  [   96/  265]
train() client id: f_00001-4-3 loss: 0.487530  [  128/  265]
train() client id: f_00001-4-4 loss: 0.470377  [  160/  265]
train() client id: f_00001-4-5 loss: 0.418251  [  192/  265]
train() client id: f_00001-4-6 loss: 0.336996  [  224/  265]
train() client id: f_00001-4-7 loss: 0.407529  [  256/  265]
train() client id: f_00001-5-0 loss: 0.568990  [   32/  265]
train() client id: f_00001-5-1 loss: 0.467830  [   64/  265]
train() client id: f_00001-5-2 loss: 0.338189  [   96/  265]
train() client id: f_00001-5-3 loss: 0.376683  [  128/  265]
train() client id: f_00001-5-4 loss: 0.408350  [  160/  265]
train() client id: f_00001-5-5 loss: 0.328834  [  192/  265]
train() client id: f_00001-5-6 loss: 0.430697  [  224/  265]
train() client id: f_00001-5-7 loss: 0.429320  [  256/  265]
train() client id: f_00001-6-0 loss: 0.438493  [   32/  265]
train() client id: f_00001-6-1 loss: 0.375938  [   64/  265]
train() client id: f_00001-6-2 loss: 0.334778  [   96/  265]
train() client id: f_00001-6-3 loss: 0.479157  [  128/  265]
train() client id: f_00001-6-4 loss: 0.532269  [  160/  265]
train() client id: f_00001-6-5 loss: 0.305481  [  192/  265]
train() client id: f_00001-6-6 loss: 0.423146  [  224/  265]
train() client id: f_00001-6-7 loss: 0.405975  [  256/  265]
train() client id: f_00001-7-0 loss: 0.453362  [   32/  265]
train() client id: f_00001-7-1 loss: 0.422336  [   64/  265]
train() client id: f_00001-7-2 loss: 0.316971  [   96/  265]
train() client id: f_00001-7-3 loss: 0.400424  [  128/  265]
train() client id: f_00001-7-4 loss: 0.385629  [  160/  265]
train() client id: f_00001-7-5 loss: 0.309458  [  192/  265]
train() client id: f_00001-7-6 loss: 0.567425  [  224/  265]
train() client id: f_00001-7-7 loss: 0.416425  [  256/  265]
train() client id: f_00001-8-0 loss: 0.501987  [   32/  265]
train() client id: f_00001-8-1 loss: 0.323979  [   64/  265]
train() client id: f_00001-8-2 loss: 0.546547  [   96/  265]
train() client id: f_00001-8-3 loss: 0.378713  [  128/  265]
train() client id: f_00001-8-4 loss: 0.324745  [  160/  265]
train() client id: f_00001-8-5 loss: 0.424032  [  192/  265]
train() client id: f_00001-8-6 loss: 0.450789  [  224/  265]
train() client id: f_00001-8-7 loss: 0.381560  [  256/  265]
train() client id: f_00001-9-0 loss: 0.380985  [   32/  265]
train() client id: f_00001-9-1 loss: 0.409926  [   64/  265]
train() client id: f_00001-9-2 loss: 0.443779  [   96/  265]
train() client id: f_00001-9-3 loss: 0.329944  [  128/  265]
train() client id: f_00001-9-4 loss: 0.417698  [  160/  265]
train() client id: f_00001-9-5 loss: 0.629013  [  192/  265]
train() client id: f_00001-9-6 loss: 0.342039  [  224/  265]
train() client id: f_00001-9-7 loss: 0.320872  [  256/  265]
train() client id: f_00001-10-0 loss: 0.406693  [   32/  265]
train() client id: f_00001-10-1 loss: 0.419149  [   64/  265]
train() client id: f_00001-10-2 loss: 0.346164  [   96/  265]
train() client id: f_00001-10-3 loss: 0.443159  [  128/  265]
train() client id: f_00001-10-4 loss: 0.412476  [  160/  265]
train() client id: f_00001-10-5 loss: 0.413438  [  192/  265]
train() client id: f_00001-10-6 loss: 0.522059  [  224/  265]
train() client id: f_00001-10-7 loss: 0.342150  [  256/  265]
train() client id: f_00001-11-0 loss: 0.392499  [   32/  265]
train() client id: f_00001-11-1 loss: 0.397980  [   64/  265]
train() client id: f_00001-11-2 loss: 0.448025  [   96/  265]
train() client id: f_00001-11-3 loss: 0.415655  [  128/  265]
train() client id: f_00001-11-4 loss: 0.404658  [  160/  265]
train() client id: f_00001-11-5 loss: 0.352890  [  192/  265]
train() client id: f_00001-11-6 loss: 0.446658  [  224/  265]
train() client id: f_00001-11-7 loss: 0.438046  [  256/  265]
train() client id: f_00001-12-0 loss: 0.312691  [   32/  265]
train() client id: f_00001-12-1 loss: 0.324492  [   64/  265]
train() client id: f_00001-12-2 loss: 0.500668  [   96/  265]
train() client id: f_00001-12-3 loss: 0.399726  [  128/  265]
train() client id: f_00001-12-4 loss: 0.485727  [  160/  265]
train() client id: f_00001-12-5 loss: 0.495500  [  192/  265]
train() client id: f_00001-12-6 loss: 0.336655  [  224/  265]
train() client id: f_00001-12-7 loss: 0.442749  [  256/  265]
train() client id: f_00001-13-0 loss: 0.647363  [   32/  265]
train() client id: f_00001-13-1 loss: 0.397557  [   64/  265]
train() client id: f_00001-13-2 loss: 0.520838  [   96/  265]
train() client id: f_00001-13-3 loss: 0.318138  [  128/  265]
train() client id: f_00001-13-4 loss: 0.306151  [  160/  265]
train() client id: f_00001-13-5 loss: 0.325416  [  192/  265]
train() client id: f_00001-13-6 loss: 0.324513  [  224/  265]
train() client id: f_00001-13-7 loss: 0.329097  [  256/  265]
train() client id: f_00001-14-0 loss: 0.452593  [   32/  265]
train() client id: f_00001-14-1 loss: 0.409102  [   64/  265]
train() client id: f_00001-14-2 loss: 0.500299  [   96/  265]
train() client id: f_00001-14-3 loss: 0.376756  [  128/  265]
train() client id: f_00001-14-4 loss: 0.401763  [  160/  265]
train() client id: f_00001-14-5 loss: 0.361277  [  192/  265]
train() client id: f_00001-14-6 loss: 0.369543  [  224/  265]
train() client id: f_00001-14-7 loss: 0.420515  [  256/  265]
train() client id: f_00001-15-0 loss: 0.458117  [   32/  265]
train() client id: f_00001-15-1 loss: 0.362263  [   64/  265]
train() client id: f_00001-15-2 loss: 0.472862  [   96/  265]
train() client id: f_00001-15-3 loss: 0.383151  [  128/  265]
train() client id: f_00001-15-4 loss: 0.455226  [  160/  265]
train() client id: f_00001-15-5 loss: 0.464115  [  192/  265]
train() client id: f_00001-15-6 loss: 0.343426  [  224/  265]
train() client id: f_00001-15-7 loss: 0.335578  [  256/  265]
train() client id: f_00001-16-0 loss: 0.462442  [   32/  265]
train() client id: f_00001-16-1 loss: 0.371249  [   64/  265]
train() client id: f_00001-16-2 loss: 0.319809  [   96/  265]
train() client id: f_00001-16-3 loss: 0.467937  [  128/  265]
train() client id: f_00001-16-4 loss: 0.392732  [  160/  265]
train() client id: f_00001-16-5 loss: 0.406148  [  192/  265]
train() client id: f_00001-16-6 loss: 0.451164  [  224/  265]
train() client id: f_00001-16-7 loss: 0.420137  [  256/  265]
train() client id: f_00001-17-0 loss: 0.348043  [   32/  265]
train() client id: f_00001-17-1 loss: 0.377374  [   64/  265]
train() client id: f_00001-17-2 loss: 0.376232  [   96/  265]
train() client id: f_00001-17-3 loss: 0.423443  [  128/  265]
train() client id: f_00001-17-4 loss: 0.411653  [  160/  265]
train() client id: f_00001-17-5 loss: 0.417546  [  192/  265]
train() client id: f_00001-17-6 loss: 0.423002  [  224/  265]
train() client id: f_00001-17-7 loss: 0.461408  [  256/  265]
train() client id: f_00002-0-0 loss: 1.261781  [   32/  124]
train() client id: f_00002-0-1 loss: 1.423147  [   64/  124]
train() client id: f_00002-0-2 loss: 1.358831  [   96/  124]
train() client id: f_00002-1-0 loss: 1.221740  [   32/  124]
train() client id: f_00002-1-1 loss: 1.347102  [   64/  124]
train() client id: f_00002-1-2 loss: 1.151942  [   96/  124]
train() client id: f_00002-2-0 loss: 1.243447  [   32/  124]
train() client id: f_00002-2-1 loss: 1.192668  [   64/  124]
train() client id: f_00002-2-2 loss: 1.035982  [   96/  124]
train() client id: f_00002-3-0 loss: 1.297117  [   32/  124]
train() client id: f_00002-3-1 loss: 1.154700  [   64/  124]
train() client id: f_00002-3-2 loss: 0.999929  [   96/  124]
train() client id: f_00002-4-0 loss: 1.013141  [   32/  124]
train() client id: f_00002-4-1 loss: 1.003890  [   64/  124]
train() client id: f_00002-4-2 loss: 1.221131  [   96/  124]
train() client id: f_00002-5-0 loss: 0.975782  [   32/  124]
train() client id: f_00002-5-1 loss: 1.008291  [   64/  124]
train() client id: f_00002-5-2 loss: 1.159286  [   96/  124]
train() client id: f_00002-6-0 loss: 1.146560  [   32/  124]
train() client id: f_00002-6-1 loss: 1.206793  [   64/  124]
train() client id: f_00002-6-2 loss: 0.955958  [   96/  124]
train() client id: f_00002-7-0 loss: 0.990236  [   32/  124]
train() client id: f_00002-7-1 loss: 0.983192  [   64/  124]
train() client id: f_00002-7-2 loss: 1.100596  [   96/  124]
train() client id: f_00002-8-0 loss: 1.026408  [   32/  124]
train() client id: f_00002-8-1 loss: 0.967367  [   64/  124]
train() client id: f_00002-8-2 loss: 0.947886  [   96/  124]
train() client id: f_00002-9-0 loss: 0.908558  [   32/  124]
train() client id: f_00002-9-1 loss: 1.107394  [   64/  124]
train() client id: f_00002-9-2 loss: 0.960848  [   96/  124]
train() client id: f_00002-10-0 loss: 0.974185  [   32/  124]
train() client id: f_00002-10-1 loss: 1.127905  [   64/  124]
train() client id: f_00002-10-2 loss: 0.905465  [   96/  124]
train() client id: f_00002-11-0 loss: 1.015285  [   32/  124]
train() client id: f_00002-11-1 loss: 1.107984  [   64/  124]
train() client id: f_00002-11-2 loss: 0.879401  [   96/  124]
train() client id: f_00002-12-0 loss: 0.910378  [   32/  124]
train() client id: f_00002-12-1 loss: 0.901920  [   64/  124]
train() client id: f_00002-12-2 loss: 1.159457  [   96/  124]
train() client id: f_00002-13-0 loss: 0.872703  [   32/  124]
train() client id: f_00002-13-1 loss: 0.994564  [   64/  124]
train() client id: f_00002-13-2 loss: 0.958798  [   96/  124]
train() client id: f_00002-14-0 loss: 1.041290  [   32/  124]
train() client id: f_00002-14-1 loss: 0.831169  [   64/  124]
train() client id: f_00002-14-2 loss: 1.083068  [   96/  124]
train() client id: f_00002-15-0 loss: 1.056650  [   32/  124]
train() client id: f_00002-15-1 loss: 0.865900  [   64/  124]
train() client id: f_00002-15-2 loss: 0.855295  [   96/  124]
train() client id: f_00002-16-0 loss: 1.116187  [   32/  124]
train() client id: f_00002-16-1 loss: 0.992309  [   64/  124]
train() client id: f_00002-16-2 loss: 0.877061  [   96/  124]
train() client id: f_00002-17-0 loss: 0.872445  [   32/  124]
train() client id: f_00002-17-1 loss: 1.029862  [   64/  124]
train() client id: f_00002-17-2 loss: 0.863820  [   96/  124]
train() client id: f_00003-0-0 loss: 0.720107  [   32/   43]
train() client id: f_00003-1-0 loss: 0.662929  [   32/   43]
train() client id: f_00003-2-0 loss: 0.605270  [   32/   43]
train() client id: f_00003-3-0 loss: 0.686010  [   32/   43]
train() client id: f_00003-4-0 loss: 0.645775  [   32/   43]
train() client id: f_00003-5-0 loss: 0.692589  [   32/   43]
train() client id: f_00003-6-0 loss: 0.567785  [   32/   43]
train() client id: f_00003-7-0 loss: 0.671786  [   32/   43]
train() client id: f_00003-8-0 loss: 0.578254  [   32/   43]
train() client id: f_00003-9-0 loss: 0.561594  [   32/   43]
train() client id: f_00003-10-0 loss: 0.704704  [   32/   43]
train() client id: f_00003-11-0 loss: 0.669171  [   32/   43]
train() client id: f_00003-12-0 loss: 0.618674  [   32/   43]
train() client id: f_00003-13-0 loss: 0.724263  [   32/   43]
train() client id: f_00003-14-0 loss: 0.728426  [   32/   43]
train() client id: f_00003-15-0 loss: 0.761248  [   32/   43]
train() client id: f_00003-16-0 loss: 0.427647  [   32/   43]
train() client id: f_00003-17-0 loss: 0.536297  [   32/   43]
train() client id: f_00004-0-0 loss: 0.620441  [   32/  306]
train() client id: f_00004-0-1 loss: 0.638236  [   64/  306]
train() client id: f_00004-0-2 loss: 0.505653  [   96/  306]
train() client id: f_00004-0-3 loss: 0.610257  [  128/  306]
train() client id: f_00004-0-4 loss: 0.782391  [  160/  306]
train() client id: f_00004-0-5 loss: 0.594317  [  192/  306]
train() client id: f_00004-0-6 loss: 0.726247  [  224/  306]
train() client id: f_00004-0-7 loss: 0.740304  [  256/  306]
train() client id: f_00004-0-8 loss: 0.624868  [  288/  306]
train() client id: f_00004-1-0 loss: 0.612954  [   32/  306]
train() client id: f_00004-1-1 loss: 0.615857  [   64/  306]
train() client id: f_00004-1-2 loss: 0.655005  [   96/  306]
train() client id: f_00004-1-3 loss: 0.671898  [  128/  306]
train() client id: f_00004-1-4 loss: 0.691221  [  160/  306]
train() client id: f_00004-1-5 loss: 0.587325  [  192/  306]
train() client id: f_00004-1-6 loss: 0.757449  [  224/  306]
train() client id: f_00004-1-7 loss: 0.540446  [  256/  306]
train() client id: f_00004-1-8 loss: 0.610900  [  288/  306]
train() client id: f_00004-2-0 loss: 0.687136  [   32/  306]
train() client id: f_00004-2-1 loss: 0.610153  [   64/  306]
train() client id: f_00004-2-2 loss: 0.652399  [   96/  306]
train() client id: f_00004-2-3 loss: 0.673588  [  128/  306]
train() client id: f_00004-2-4 loss: 0.568312  [  160/  306]
train() client id: f_00004-2-5 loss: 0.731439  [  192/  306]
train() client id: f_00004-2-6 loss: 0.690551  [  224/  306]
train() client id: f_00004-2-7 loss: 0.646803  [  256/  306]
train() client id: f_00004-2-8 loss: 0.641965  [  288/  306]
train() client id: f_00004-3-0 loss: 0.674611  [   32/  306]
train() client id: f_00004-3-1 loss: 0.795770  [   64/  306]
train() client id: f_00004-3-2 loss: 0.674519  [   96/  306]
train() client id: f_00004-3-3 loss: 0.652829  [  128/  306]
train() client id: f_00004-3-4 loss: 0.658670  [  160/  306]
train() client id: f_00004-3-5 loss: 0.661285  [  192/  306]
train() client id: f_00004-3-6 loss: 0.636233  [  224/  306]
train() client id: f_00004-3-7 loss: 0.464378  [  256/  306]
train() client id: f_00004-3-8 loss: 0.699000  [  288/  306]
train() client id: f_00004-4-0 loss: 0.676338  [   32/  306]
train() client id: f_00004-4-1 loss: 0.697721  [   64/  306]
train() client id: f_00004-4-2 loss: 0.556303  [   96/  306]
train() client id: f_00004-4-3 loss: 0.735910  [  128/  306]
train() client id: f_00004-4-4 loss: 0.646279  [  160/  306]
train() client id: f_00004-4-5 loss: 0.629795  [  192/  306]
train() client id: f_00004-4-6 loss: 0.641027  [  224/  306]
train() client id: f_00004-4-7 loss: 0.569487  [  256/  306]
train() client id: f_00004-4-8 loss: 0.777667  [  288/  306]
train() client id: f_00004-5-0 loss: 0.639824  [   32/  306]
train() client id: f_00004-5-1 loss: 0.570092  [   64/  306]
train() client id: f_00004-5-2 loss: 0.757276  [   96/  306]
train() client id: f_00004-5-3 loss: 0.583539  [  128/  306]
train() client id: f_00004-5-4 loss: 0.790141  [  160/  306]
train() client id: f_00004-5-5 loss: 0.736347  [  192/  306]
train() client id: f_00004-5-6 loss: 0.584039  [  224/  306]
train() client id: f_00004-5-7 loss: 0.670938  [  256/  306]
train() client id: f_00004-5-8 loss: 0.598493  [  288/  306]
train() client id: f_00004-6-0 loss: 0.615808  [   32/  306]
train() client id: f_00004-6-1 loss: 0.659194  [   64/  306]
train() client id: f_00004-6-2 loss: 0.752706  [   96/  306]
train() client id: f_00004-6-3 loss: 0.661847  [  128/  306]
train() client id: f_00004-6-4 loss: 0.751306  [  160/  306]
train() client id: f_00004-6-5 loss: 0.605326  [  192/  306]
train() client id: f_00004-6-6 loss: 0.583179  [  224/  306]
train() client id: f_00004-6-7 loss: 0.607455  [  256/  306]
train() client id: f_00004-6-8 loss: 0.689787  [  288/  306]
train() client id: f_00004-7-0 loss: 0.710245  [   32/  306]
train() client id: f_00004-7-1 loss: 0.635265  [   64/  306]
train() client id: f_00004-7-2 loss: 0.687859  [   96/  306]
train() client id: f_00004-7-3 loss: 0.637052  [  128/  306]
train() client id: f_00004-7-4 loss: 0.612627  [  160/  306]
train() client id: f_00004-7-5 loss: 0.663991  [  192/  306]
train() client id: f_00004-7-6 loss: 0.572706  [  224/  306]
train() client id: f_00004-7-7 loss: 0.628501  [  256/  306]
train() client id: f_00004-7-8 loss: 0.706163  [  288/  306]
train() client id: f_00004-8-0 loss: 0.773961  [   32/  306]
train() client id: f_00004-8-1 loss: 0.701811  [   64/  306]
train() client id: f_00004-8-2 loss: 0.572182  [   96/  306]
train() client id: f_00004-8-3 loss: 0.674602  [  128/  306]
train() client id: f_00004-8-4 loss: 0.672856  [  160/  306]
train() client id: f_00004-8-5 loss: 0.654906  [  192/  306]
train() client id: f_00004-8-6 loss: 0.624847  [  224/  306]
train() client id: f_00004-8-7 loss: 0.552287  [  256/  306]
train() client id: f_00004-8-8 loss: 0.627187  [  288/  306]
train() client id: f_00004-9-0 loss: 0.547848  [   32/  306]
train() client id: f_00004-9-1 loss: 0.634502  [   64/  306]
train() client id: f_00004-9-2 loss: 0.669599  [   96/  306]
train() client id: f_00004-9-3 loss: 0.713334  [  128/  306]
train() client id: f_00004-9-4 loss: 0.639017  [  160/  306]
train() client id: f_00004-9-5 loss: 0.723501  [  192/  306]
train() client id: f_00004-9-6 loss: 0.496155  [  224/  306]
train() client id: f_00004-9-7 loss: 0.624084  [  256/  306]
train() client id: f_00004-9-8 loss: 0.728814  [  288/  306]
train() client id: f_00004-10-0 loss: 0.591235  [   32/  306]
train() client id: f_00004-10-1 loss: 0.665173  [   64/  306]
train() client id: f_00004-10-2 loss: 0.651911  [   96/  306]
train() client id: f_00004-10-3 loss: 0.630498  [  128/  306]
train() client id: f_00004-10-4 loss: 0.765066  [  160/  306]
train() client id: f_00004-10-5 loss: 0.641211  [  192/  306]
train() client id: f_00004-10-6 loss: 0.687017  [  224/  306]
train() client id: f_00004-10-7 loss: 0.699031  [  256/  306]
train() client id: f_00004-10-8 loss: 0.564852  [  288/  306]
train() client id: f_00004-11-0 loss: 0.652175  [   32/  306]
train() client id: f_00004-11-1 loss: 0.559195  [   64/  306]
train() client id: f_00004-11-2 loss: 0.577750  [   96/  306]
train() client id: f_00004-11-3 loss: 0.638937  [  128/  306]
train() client id: f_00004-11-4 loss: 0.665627  [  160/  306]
train() client id: f_00004-11-5 loss: 0.636093  [  192/  306]
train() client id: f_00004-11-6 loss: 0.673797  [  224/  306]
train() client id: f_00004-11-7 loss: 0.711161  [  256/  306]
train() client id: f_00004-11-8 loss: 0.714713  [  288/  306]
train() client id: f_00004-12-0 loss: 0.752792  [   32/  306]
train() client id: f_00004-12-1 loss: 0.622017  [   64/  306]
train() client id: f_00004-12-2 loss: 0.777768  [   96/  306]
train() client id: f_00004-12-3 loss: 0.674963  [  128/  306]
train() client id: f_00004-12-4 loss: 0.653071  [  160/  306]
train() client id: f_00004-12-5 loss: 0.662591  [  192/  306]
train() client id: f_00004-12-6 loss: 0.677828  [  224/  306]
train() client id: f_00004-12-7 loss: 0.624674  [  256/  306]
train() client id: f_00004-12-8 loss: 0.469086  [  288/  306]
train() client id: f_00004-13-0 loss: 0.742497  [   32/  306]
train() client id: f_00004-13-1 loss: 0.715871  [   64/  306]
train() client id: f_00004-13-2 loss: 0.617267  [   96/  306]
train() client id: f_00004-13-3 loss: 0.752107  [  128/  306]
train() client id: f_00004-13-4 loss: 0.754521  [  160/  306]
train() client id: f_00004-13-5 loss: 0.649732  [  192/  306]
train() client id: f_00004-13-6 loss: 0.550407  [  224/  306]
train() client id: f_00004-13-7 loss: 0.527624  [  256/  306]
train() client id: f_00004-13-8 loss: 0.635757  [  288/  306]
train() client id: f_00004-14-0 loss: 0.583114  [   32/  306]
train() client id: f_00004-14-1 loss: 0.694671  [   64/  306]
train() client id: f_00004-14-2 loss: 0.636772  [   96/  306]
train() client id: f_00004-14-3 loss: 0.691727  [  128/  306]
train() client id: f_00004-14-4 loss: 0.616128  [  160/  306]
train() client id: f_00004-14-5 loss: 0.613400  [  192/  306]
train() client id: f_00004-14-6 loss: 0.709587  [  224/  306]
train() client id: f_00004-14-7 loss: 0.644947  [  256/  306]
train() client id: f_00004-14-8 loss: 0.719776  [  288/  306]
train() client id: f_00004-15-0 loss: 0.616843  [   32/  306]
train() client id: f_00004-15-1 loss: 0.763087  [   64/  306]
train() client id: f_00004-15-2 loss: 0.609646  [   96/  306]
train() client id: f_00004-15-3 loss: 0.577541  [  128/  306]
train() client id: f_00004-15-4 loss: 0.680528  [  160/  306]
train() client id: f_00004-15-5 loss: 0.625373  [  192/  306]
train() client id: f_00004-15-6 loss: 0.873018  [  224/  306]
train() client id: f_00004-15-7 loss: 0.670261  [  256/  306]
train() client id: f_00004-15-8 loss: 0.631487  [  288/  306]
train() client id: f_00004-16-0 loss: 0.639244  [   32/  306]
train() client id: f_00004-16-1 loss: 0.708526  [   64/  306]
train() client id: f_00004-16-2 loss: 0.667711  [   96/  306]
train() client id: f_00004-16-3 loss: 0.649918  [  128/  306]
train() client id: f_00004-16-4 loss: 0.753686  [  160/  306]
train() client id: f_00004-16-5 loss: 0.707071  [  192/  306]
train() client id: f_00004-16-6 loss: 0.625132  [  224/  306]
train() client id: f_00004-16-7 loss: 0.664487  [  256/  306]
train() client id: f_00004-16-8 loss: 0.583155  [  288/  306]
train() client id: f_00004-17-0 loss: 0.674729  [   32/  306]
train() client id: f_00004-17-1 loss: 0.767856  [   64/  306]
train() client id: f_00004-17-2 loss: 0.687947  [   96/  306]
train() client id: f_00004-17-3 loss: 0.793075  [  128/  306]
train() client id: f_00004-17-4 loss: 0.634217  [  160/  306]
train() client id: f_00004-17-5 loss: 0.530471  [  192/  306]
train() client id: f_00004-17-6 loss: 0.654350  [  224/  306]
train() client id: f_00004-17-7 loss: 0.724609  [  256/  306]
train() client id: f_00004-17-8 loss: 0.522579  [  288/  306]
train() client id: f_00005-0-0 loss: 0.296663  [   32/  146]
train() client id: f_00005-0-1 loss: 0.447313  [   64/  146]
train() client id: f_00005-0-2 loss: 0.429228  [   96/  146]
train() client id: f_00005-0-3 loss: 0.679562  [  128/  146]
train() client id: f_00005-1-0 loss: 0.334277  [   32/  146]
train() client id: f_00005-1-1 loss: 0.442128  [   64/  146]
train() client id: f_00005-1-2 loss: 0.678155  [   96/  146]
train() client id: f_00005-1-3 loss: 0.619599  [  128/  146]
train() client id: f_00005-2-0 loss: 0.575743  [   32/  146]
train() client id: f_00005-2-1 loss: 0.512858  [   64/  146]
train() client id: f_00005-2-2 loss: 0.577604  [   96/  146]
train() client id: f_00005-2-3 loss: 0.411110  [  128/  146]
train() client id: f_00005-3-0 loss: 0.268220  [   32/  146]
train() client id: f_00005-3-1 loss: 0.598368  [   64/  146]
train() client id: f_00005-3-2 loss: 0.599137  [   96/  146]
train() client id: f_00005-3-3 loss: 0.613088  [  128/  146]
train() client id: f_00005-4-0 loss: 0.486865  [   32/  146]
train() client id: f_00005-4-1 loss: 0.433063  [   64/  146]
train() client id: f_00005-4-2 loss: 0.390185  [   96/  146]
train() client id: f_00005-4-3 loss: 0.707140  [  128/  146]
train() client id: f_00005-5-0 loss: 0.387789  [   32/  146]
train() client id: f_00005-5-1 loss: 0.610155  [   64/  146]
train() client id: f_00005-5-2 loss: 0.280286  [   96/  146]
train() client id: f_00005-5-3 loss: 0.731616  [  128/  146]
train() client id: f_00005-6-0 loss: 0.612528  [   32/  146]
train() client id: f_00005-6-1 loss: 0.395407  [   64/  146]
train() client id: f_00005-6-2 loss: 0.697000  [   96/  146]
train() client id: f_00005-6-3 loss: 0.316664  [  128/  146]
train() client id: f_00005-7-0 loss: 0.491812  [   32/  146]
train() client id: f_00005-7-1 loss: 0.301141  [   64/  146]
train() client id: f_00005-7-2 loss: 0.490659  [   96/  146]
train() client id: f_00005-7-3 loss: 0.603209  [  128/  146]
train() client id: f_00005-8-0 loss: 0.409685  [   32/  146]
train() client id: f_00005-8-1 loss: 0.608845  [   64/  146]
train() client id: f_00005-8-2 loss: 0.332093  [   96/  146]
train() client id: f_00005-8-3 loss: 0.703505  [  128/  146]
train() client id: f_00005-9-0 loss: 0.373237  [   32/  146]
train() client id: f_00005-9-1 loss: 0.484612  [   64/  146]
train() client id: f_00005-9-2 loss: 0.445296  [   96/  146]
train() client id: f_00005-9-3 loss: 0.413692  [  128/  146]
train() client id: f_00005-10-0 loss: 0.651383  [   32/  146]
train() client id: f_00005-10-1 loss: 0.429519  [   64/  146]
train() client id: f_00005-10-2 loss: 0.710780  [   96/  146]
train() client id: f_00005-10-3 loss: 0.263434  [  128/  146]
train() client id: f_00005-11-0 loss: 0.670714  [   32/  146]
train() client id: f_00005-11-1 loss: 0.302774  [   64/  146]
train() client id: f_00005-11-2 loss: 0.540110  [   96/  146]
train() client id: f_00005-11-3 loss: 0.640377  [  128/  146]
train() client id: f_00005-12-0 loss: 0.369128  [   32/  146]
train() client id: f_00005-12-1 loss: 0.515778  [   64/  146]
train() client id: f_00005-12-2 loss: 0.626620  [   96/  146]
train() client id: f_00005-12-3 loss: 0.652930  [  128/  146]
train() client id: f_00005-13-0 loss: 0.552849  [   32/  146]
train() client id: f_00005-13-1 loss: 0.440138  [   64/  146]
train() client id: f_00005-13-2 loss: 0.595812  [   96/  146]
train() client id: f_00005-13-3 loss: 0.493804  [  128/  146]
train() client id: f_00005-14-0 loss: 0.377201  [   32/  146]
train() client id: f_00005-14-1 loss: 0.555199  [   64/  146]
train() client id: f_00005-14-2 loss: 0.589586  [   96/  146]
train() client id: f_00005-14-3 loss: 0.647407  [  128/  146]
train() client id: f_00005-15-0 loss: 0.515799  [   32/  146]
train() client id: f_00005-15-1 loss: 0.373639  [   64/  146]
train() client id: f_00005-15-2 loss: 0.533809  [   96/  146]
train() client id: f_00005-15-3 loss: 0.456826  [  128/  146]
train() client id: f_00005-16-0 loss: 0.689583  [   32/  146]
train() client id: f_00005-16-1 loss: 0.418657  [   64/  146]
train() client id: f_00005-16-2 loss: 0.521351  [   96/  146]
train() client id: f_00005-16-3 loss: 0.446709  [  128/  146]
train() client id: f_00005-17-0 loss: 0.404825  [   32/  146]
train() client id: f_00005-17-1 loss: 0.472478  [   64/  146]
train() client id: f_00005-17-2 loss: 0.393624  [   96/  146]
train() client id: f_00005-17-3 loss: 0.876800  [  128/  146]
train() client id: f_00006-0-0 loss: 0.481958  [   32/   54]
train() client id: f_00006-1-0 loss: 0.420287  [   32/   54]
train() client id: f_00006-2-0 loss: 0.449888  [   32/   54]
train() client id: f_00006-3-0 loss: 0.495318  [   32/   54]
train() client id: f_00006-4-0 loss: 0.463241  [   32/   54]
train() client id: f_00006-5-0 loss: 0.486443  [   32/   54]
train() client id: f_00006-6-0 loss: 0.434967  [   32/   54]
train() client id: f_00006-7-0 loss: 0.498786  [   32/   54]
train() client id: f_00006-8-0 loss: 0.448784  [   32/   54]
train() client id: f_00006-9-0 loss: 0.494731  [   32/   54]
train() client id: f_00006-10-0 loss: 0.423750  [   32/   54]
train() client id: f_00006-11-0 loss: 0.422157  [   32/   54]
train() client id: f_00006-12-0 loss: 0.474211  [   32/   54]
train() client id: f_00006-13-0 loss: 0.426635  [   32/   54]
train() client id: f_00006-14-0 loss: 0.477515  [   32/   54]
train() client id: f_00006-15-0 loss: 0.394557  [   32/   54]
train() client id: f_00006-16-0 loss: 0.473105  [   32/   54]
train() client id: f_00006-17-0 loss: 0.483928  [   32/   54]
train() client id: f_00007-0-0 loss: 0.486036  [   32/  179]
train() client id: f_00007-0-1 loss: 0.436206  [   64/  179]
train() client id: f_00007-0-2 loss: 0.403498  [   96/  179]
train() client id: f_00007-0-3 loss: 0.392550  [  128/  179]
train() client id: f_00007-0-4 loss: 0.196996  [  160/  179]
train() client id: f_00007-1-0 loss: 0.480133  [   32/  179]
train() client id: f_00007-1-1 loss: 0.129955  [   64/  179]
train() client id: f_00007-1-2 loss: 0.230091  [   96/  179]
train() client id: f_00007-1-3 loss: 0.291422  [  128/  179]
train() client id: f_00007-1-4 loss: 0.562736  [  160/  179]
train() client id: f_00007-2-0 loss: 0.356893  [   32/  179]
train() client id: f_00007-2-1 loss: 0.162342  [   64/  179]
train() client id: f_00007-2-2 loss: 0.348586  [   96/  179]
train() client id: f_00007-2-3 loss: 0.377832  [  128/  179]
train() client id: f_00007-2-4 loss: 0.443060  [  160/  179]
train() client id: f_00007-3-0 loss: 0.321319  [   32/  179]
train() client id: f_00007-3-1 loss: 0.199233  [   64/  179]
train() client id: f_00007-3-2 loss: 0.314231  [   96/  179]
train() client id: f_00007-3-3 loss: 0.252862  [  128/  179]
train() client id: f_00007-3-4 loss: 0.431728  [  160/  179]
train() client id: f_00007-4-0 loss: 0.375511  [   32/  179]
train() client id: f_00007-4-1 loss: 0.221461  [   64/  179]
train() client id: f_00007-4-2 loss: 0.090048  [   96/  179]
train() client id: f_00007-4-3 loss: 0.336331  [  128/  179]
train() client id: f_00007-4-4 loss: 0.502014  [  160/  179]
train() client id: f_00007-5-0 loss: 0.176632  [   32/  179]
train() client id: f_00007-5-1 loss: 0.313574  [   64/  179]
train() client id: f_00007-5-2 loss: 0.267489  [   96/  179]
train() client id: f_00007-5-3 loss: 0.406913  [  128/  179]
train() client id: f_00007-5-4 loss: 0.313624  [  160/  179]
train() client id: f_00007-6-0 loss: 0.309768  [   32/  179]
train() client id: f_00007-6-1 loss: 0.365336  [   64/  179]
train() client id: f_00007-6-2 loss: 0.144349  [   96/  179]
train() client id: f_00007-6-3 loss: 0.494427  [  128/  179]
train() client id: f_00007-6-4 loss: 0.106799  [  160/  179]
train() client id: f_00007-7-0 loss: 0.181683  [   32/  179]
train() client id: f_00007-7-1 loss: 0.207054  [   64/  179]
train() client id: f_00007-7-2 loss: 0.346848  [   96/  179]
train() client id: f_00007-7-3 loss: 0.222940  [  128/  179]
train() client id: f_00007-7-4 loss: 0.434976  [  160/  179]
train() client id: f_00007-8-0 loss: 0.305986  [   32/  179]
train() client id: f_00007-8-1 loss: 0.157124  [   64/  179]
train() client id: f_00007-8-2 loss: 0.303146  [   96/  179]
train() client id: f_00007-8-3 loss: 0.364835  [  128/  179]
train() client id: f_00007-8-4 loss: 0.266448  [  160/  179]
train() client id: f_00007-9-0 loss: 0.439039  [   32/  179]
train() client id: f_00007-9-1 loss: 0.300185  [   64/  179]
train() client id: f_00007-9-2 loss: 0.370613  [   96/  179]
train() client id: f_00007-9-3 loss: 0.102449  [  128/  179]
train() client id: f_00007-9-4 loss: 0.225959  [  160/  179]
train() client id: f_00007-10-0 loss: 0.343705  [   32/  179]
train() client id: f_00007-10-1 loss: 0.254932  [   64/  179]
train() client id: f_00007-10-2 loss: 0.221470  [   96/  179]
train() client id: f_00007-10-3 loss: 0.494588  [  128/  179]
train() client id: f_00007-10-4 loss: 0.104599  [  160/  179]
train() client id: f_00007-11-0 loss: 0.086409  [   32/  179]
train() client id: f_00007-11-1 loss: 0.268925  [   64/  179]
train() client id: f_00007-11-2 loss: 0.079933  [   96/  179]
train() client id: f_00007-11-3 loss: 0.135458  [  128/  179]
train() client id: f_00007-11-4 loss: 0.684270  [  160/  179]
train() client id: f_00007-12-0 loss: 0.262800  [   32/  179]
train() client id: f_00007-12-1 loss: 0.342826  [   64/  179]
train() client id: f_00007-12-2 loss: 0.203741  [   96/  179]
train() client id: f_00007-12-3 loss: 0.371316  [  128/  179]
train() client id: f_00007-12-4 loss: 0.064015  [  160/  179]
train() client id: f_00007-13-0 loss: 0.412047  [   32/  179]
train() client id: f_00007-13-1 loss: 0.386431  [   64/  179]
train() client id: f_00007-13-2 loss: 0.277701  [   96/  179]
train() client id: f_00007-13-3 loss: 0.091130  [  128/  179]
train() client id: f_00007-13-4 loss: 0.221348  [  160/  179]
train() client id: f_00007-14-0 loss: 0.513984  [   32/  179]
train() client id: f_00007-14-1 loss: 0.123515  [   64/  179]
train() client id: f_00007-14-2 loss: 0.208333  [   96/  179]
train() client id: f_00007-14-3 loss: 0.177719  [  128/  179]
train() client id: f_00007-14-4 loss: 0.258224  [  160/  179]
train() client id: f_00007-15-0 loss: 0.082365  [   32/  179]
train() client id: f_00007-15-1 loss: 0.315689  [   64/  179]
train() client id: f_00007-15-2 loss: 0.246356  [   96/  179]
train() client id: f_00007-15-3 loss: 0.359272  [  128/  179]
train() client id: f_00007-15-4 loss: 0.377663  [  160/  179]
train() client id: f_00007-16-0 loss: 0.394166  [   32/  179]
train() client id: f_00007-16-1 loss: 0.282327  [   64/  179]
train() client id: f_00007-16-2 loss: 0.234851  [   96/  179]
train() client id: f_00007-16-3 loss: 0.213904  [  128/  179]
train() client id: f_00007-16-4 loss: 0.192705  [  160/  179]
train() client id: f_00007-17-0 loss: 0.251331  [   32/  179]
train() client id: f_00007-17-1 loss: 0.169591  [   64/  179]
train() client id: f_00007-17-2 loss: 0.103280  [   96/  179]
train() client id: f_00007-17-3 loss: 0.365453  [  128/  179]
train() client id: f_00007-17-4 loss: 0.318687  [  160/  179]
train() client id: f_00008-0-0 loss: 0.731826  [   32/  130]
train() client id: f_00008-0-1 loss: 0.658973  [   64/  130]
train() client id: f_00008-0-2 loss: 0.751789  [   96/  130]
train() client id: f_00008-0-3 loss: 0.677484  [  128/  130]
train() client id: f_00008-1-0 loss: 0.723174  [   32/  130]
train() client id: f_00008-1-1 loss: 0.847862  [   64/  130]
train() client id: f_00008-1-2 loss: 0.684475  [   96/  130]
train() client id: f_00008-1-3 loss: 0.563890  [  128/  130]
train() client id: f_00008-2-0 loss: 0.698197  [   32/  130]
train() client id: f_00008-2-1 loss: 0.708384  [   64/  130]
train() client id: f_00008-2-2 loss: 0.645464  [   96/  130]
train() client id: f_00008-2-3 loss: 0.748308  [  128/  130]
train() client id: f_00008-3-0 loss: 0.744430  [   32/  130]
train() client id: f_00008-3-1 loss: 0.714057  [   64/  130]
train() client id: f_00008-3-2 loss: 0.658003  [   96/  130]
train() client id: f_00008-3-3 loss: 0.710701  [  128/  130]
train() client id: f_00008-4-0 loss: 0.730117  [   32/  130]
train() client id: f_00008-4-1 loss: 0.730804  [   64/  130]
train() client id: f_00008-4-2 loss: 0.764079  [   96/  130]
train() client id: f_00008-4-3 loss: 0.602703  [  128/  130]
train() client id: f_00008-5-0 loss: 0.862962  [   32/  130]
train() client id: f_00008-5-1 loss: 0.710396  [   64/  130]
train() client id: f_00008-5-2 loss: 0.708794  [   96/  130]
train() client id: f_00008-5-3 loss: 0.565577  [  128/  130]
train() client id: f_00008-6-0 loss: 0.725850  [   32/  130]
train() client id: f_00008-6-1 loss: 0.765977  [   64/  130]
train() client id: f_00008-6-2 loss: 0.664304  [   96/  130]
train() client id: f_00008-6-3 loss: 0.682193  [  128/  130]
train() client id: f_00008-7-0 loss: 0.805302  [   32/  130]
train() client id: f_00008-7-1 loss: 0.672979  [   64/  130]
train() client id: f_00008-7-2 loss: 0.693814  [   96/  130]
train() client id: f_00008-7-3 loss: 0.681499  [  128/  130]
train() client id: f_00008-8-0 loss: 0.646280  [   32/  130]
train() client id: f_00008-8-1 loss: 0.648261  [   64/  130]
train() client id: f_00008-8-2 loss: 0.793108  [   96/  130]
train() client id: f_00008-8-3 loss: 0.750163  [  128/  130]
train() client id: f_00008-9-0 loss: 0.736924  [   32/  130]
train() client id: f_00008-9-1 loss: 0.797347  [   64/  130]
train() client id: f_00008-9-2 loss: 0.700812  [   96/  130]
train() client id: f_00008-9-3 loss: 0.605754  [  128/  130]
train() client id: f_00008-10-0 loss: 0.749192  [   32/  130]
train() client id: f_00008-10-1 loss: 0.785225  [   64/  130]
train() client id: f_00008-10-2 loss: 0.609681  [   96/  130]
train() client id: f_00008-10-3 loss: 0.695940  [  128/  130]
train() client id: f_00008-11-0 loss: 0.725185  [   32/  130]
train() client id: f_00008-11-1 loss: 0.695575  [   64/  130]
train() client id: f_00008-11-2 loss: 0.713710  [   96/  130]
train() client id: f_00008-11-3 loss: 0.703156  [  128/  130]
train() client id: f_00008-12-0 loss: 0.680206  [   32/  130]
train() client id: f_00008-12-1 loss: 0.658575  [   64/  130]
train() client id: f_00008-12-2 loss: 0.782089  [   96/  130]
train() client id: f_00008-12-3 loss: 0.714946  [  128/  130]
train() client id: f_00008-13-0 loss: 0.759822  [   32/  130]
train() client id: f_00008-13-1 loss: 0.672481  [   64/  130]
train() client id: f_00008-13-2 loss: 0.647113  [   96/  130]
train() client id: f_00008-13-3 loss: 0.715827  [  128/  130]
train() client id: f_00008-14-0 loss: 0.749616  [   32/  130]
train() client id: f_00008-14-1 loss: 0.706131  [   64/  130]
train() client id: f_00008-14-2 loss: 0.732796  [   96/  130]
train() client id: f_00008-14-3 loss: 0.625275  [  128/  130]
train() client id: f_00008-15-0 loss: 0.663524  [   32/  130]
train() client id: f_00008-15-1 loss: 0.651820  [   64/  130]
train() client id: f_00008-15-2 loss: 0.809520  [   96/  130]
train() client id: f_00008-15-3 loss: 0.695379  [  128/  130]
train() client id: f_00008-16-0 loss: 0.697685  [   32/  130]
train() client id: f_00008-16-1 loss: 0.600331  [   64/  130]
train() client id: f_00008-16-2 loss: 0.694530  [   96/  130]
train() client id: f_00008-16-3 loss: 0.808211  [  128/  130]
train() client id: f_00008-17-0 loss: 0.674719  [   32/  130]
train() client id: f_00008-17-1 loss: 0.734675  [   64/  130]
train() client id: f_00008-17-2 loss: 0.796025  [   96/  130]
train() client id: f_00008-17-3 loss: 0.620085  [  128/  130]
train() client id: f_00009-0-0 loss: 1.182828  [   32/  118]
train() client id: f_00009-0-1 loss: 1.090966  [   64/  118]
train() client id: f_00009-0-2 loss: 0.972430  [   96/  118]
train() client id: f_00009-1-0 loss: 1.063395  [   32/  118]
train() client id: f_00009-1-1 loss: 1.123324  [   64/  118]
train() client id: f_00009-1-2 loss: 0.979390  [   96/  118]
train() client id: f_00009-2-0 loss: 0.982478  [   32/  118]
train() client id: f_00009-2-1 loss: 0.932819  [   64/  118]
train() client id: f_00009-2-2 loss: 0.874268  [   96/  118]
train() client id: f_00009-3-0 loss: 0.992050  [   32/  118]
train() client id: f_00009-3-1 loss: 0.874514  [   64/  118]
train() client id: f_00009-3-2 loss: 0.835069  [   96/  118]
train() client id: f_00009-4-0 loss: 0.931197  [   32/  118]
train() client id: f_00009-4-1 loss: 0.757757  [   64/  118]
train() client id: f_00009-4-2 loss: 0.980349  [   96/  118]
train() client id: f_00009-5-0 loss: 0.942357  [   32/  118]
train() client id: f_00009-5-1 loss: 0.889349  [   64/  118]
train() client id: f_00009-5-2 loss: 0.706268  [   96/  118]
train() client id: f_00009-6-0 loss: 0.827805  [   32/  118]
train() client id: f_00009-6-1 loss: 0.777504  [   64/  118]
train() client id: f_00009-6-2 loss: 0.784286  [   96/  118]
train() client id: f_00009-7-0 loss: 0.922764  [   32/  118]
train() client id: f_00009-7-1 loss: 0.758162  [   64/  118]
train() client id: f_00009-7-2 loss: 0.927045  [   96/  118]
train() client id: f_00009-8-0 loss: 0.696067  [   32/  118]
train() client id: f_00009-8-1 loss: 0.609854  [   64/  118]
train() client id: f_00009-8-2 loss: 0.986636  [   96/  118]
train() client id: f_00009-9-0 loss: 0.823937  [   32/  118]
train() client id: f_00009-9-1 loss: 0.808162  [   64/  118]
train() client id: f_00009-9-2 loss: 0.666238  [   96/  118]
train() client id: f_00009-10-0 loss: 0.884638  [   32/  118]
train() client id: f_00009-10-1 loss: 0.744788  [   64/  118]
train() client id: f_00009-10-2 loss: 0.661872  [   96/  118]
train() client id: f_00009-11-0 loss: 0.979517  [   32/  118]
train() client id: f_00009-11-1 loss: 0.571026  [   64/  118]
train() client id: f_00009-11-2 loss: 0.694527  [   96/  118]
train() client id: f_00009-12-0 loss: 0.817232  [   32/  118]
train() client id: f_00009-12-1 loss: 0.665729  [   64/  118]
train() client id: f_00009-12-2 loss: 0.733506  [   96/  118]
train() client id: f_00009-13-0 loss: 0.737168  [   32/  118]
train() client id: f_00009-13-1 loss: 0.774642  [   64/  118]
train() client id: f_00009-13-2 loss: 0.802078  [   96/  118]
train() client id: f_00009-14-0 loss: 0.564599  [   32/  118]
train() client id: f_00009-14-1 loss: 1.073236  [   64/  118]
train() client id: f_00009-14-2 loss: 0.619446  [   96/  118]
train() client id: f_00009-15-0 loss: 0.714775  [   32/  118]
train() client id: f_00009-15-1 loss: 0.627915  [   64/  118]
train() client id: f_00009-15-2 loss: 0.775758  [   96/  118]
train() client id: f_00009-16-0 loss: 0.800166  [   32/  118]
train() client id: f_00009-16-1 loss: 0.590644  [   64/  118]
train() client id: f_00009-16-2 loss: 0.903425  [   96/  118]
train() client id: f_00009-17-0 loss: 0.688816  [   32/  118]
train() client id: f_00009-17-1 loss: 0.891913  [   64/  118]
train() client id: f_00009-17-2 loss: 0.646435  [   96/  118]
At round 64 accuracy: 0.649867374005305
At round 64 training accuracy: 0.5868544600938967
At round 64 training loss: 0.855309212915541
update_location
xs = 8.927491 441.223621 5.882650 0.934260 -357.581990 -205.230757 -165.849135 -5.143845 -380.120581 20.134486 
ys = -432.390647 7.291448 330.684448 -152.290817 -9.642386 0.794442 -1.381692 326.628436 25.881276 -867.232496 
xs mean: -63.68237997052123
ys mean: -77.16579882624052
dists_uav = 443.893424 452.472595 345.523964 182.190466 371.426783 228.298697 193.669421 341.632250 393.905441 873.211085 
uav_gains = -122.827334 -123.071074 -118.922502 -106.566198 -120.239187 -109.578649 -107.285128 -118.696850 -121.174865 -130.500935 
uav_gains_db_mean: -117.88627214900114
dists_bs = 634.628017 638.637385 238.654940 371.580601 259.670934 176.809149 176.618911 227.689034 253.595833 1060.342418 
bs_gains = -118.037598 -118.114181 -106.144631 -111.528544 -107.170913 -102.497192 -102.484101 -105.572638 -106.883038 -124.279551 
bs_gains_db_mean: -110.2712386466664
Round 65
-------------------------------
ene_coms = [0.04172733 0.04335729 0.00844634 0.00864645 0.00894996 0.01039032
 0.0090096  0.00818872 0.00880296 0.0528282 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.52066402 5.03600727 2.28569534 0.82679289 5.56942056 2.69388427
 1.02724838 3.27558051 2.39598228 2.44280548]
obj_prev = 28.074081010425125
eta_min = 4.745849761670094e-39	eta_max = 0.8823410987652313
af = 5.777823695651855	bf = 1.185478777209423	zeta = 6.3556060652170405	eta = 0.9090909090909091
af = 5.777823695651855	bf = 1.185478777209423	zeta = 19.097806117716587	eta = 0.30253860888722206
af = 5.777823695651855	bf = 1.185478777209423	zeta = 11.60370318191857	eta = 0.49792929076772047
af = 5.777823695651855	bf = 1.185478777209423	zeta = 10.38566671473688	eta = 0.556326700475891
af = 5.777823695651855	bf = 1.185478777209423	zeta = 10.308468173822837	eta = 0.5604929460153906
af = 5.777823695651855	bf = 1.185478777209423	zeta = 10.308103187422498	eta = 0.5605127917909966
af = 5.777823695651855	bf = 1.185478777209423	zeta = 10.30810317918181	eta = 0.5605127922390918
eta = 0.5605127922390918
ene_coms = [0.04172733 0.04335729 0.00844634 0.00864645 0.00894996 0.01039032
 0.0090096  0.00818872 0.00880296 0.0528282 ]
ene_comp = [0.04776971 0.10046803 0.04701146 0.01630236 0.11601214 0.0553522
 0.02047273 0.06786331 0.0492862  0.04473671]
ene_total = [1.20496096 1.93642067 0.74666695 0.33590319 1.68245195 0.88513738
 0.39694115 1.02394161 0.78209493 1.31358441]
ti_comp = [1.56503336 1.54873368 1.89784324 1.89584215 1.89280701 1.87840342
 1.89221065 1.90041938 1.89427702 1.45402462]
ti_coms = [0.41727326 0.43357294 0.08446337 0.08646447 0.0894996  0.1039032
 0.09009597 0.08188724 0.0880296  0.528282  ]
t_total = [26.68917656 26.68917656 26.68917656 26.68917656 26.68917656 26.68917656
 26.68917656 26.68917656 26.68917656 26.68917656]
ene_coms = [0.04172733 0.04335729 0.00844634 0.00864645 0.00894996 0.01039032
 0.0090096  0.00818872 0.00880296 0.0528282 ]
ene_comp = [2.78157334e-06 2.64247090e-05 1.80289509e-06 7.53402569e-08
 2.72381549e-05 3.00404825e-06 1.49785024e-07 5.40861753e-06
 2.08530294e-06 2.64684543e-06]
ene_total = [0.56184154 0.58410526 0.1137432  0.11641416 0.12086629 0.13993256
 0.12130451 0.11032332 0.11854847 0.71129853]
optimize_network iter = 0 obj = 2.6983778468621304
eta = 0.5605127922390918
freqs = [15261561.48798536 32435541.71643906 12385495.08516292  4299503.49461348
 30645528.45570698 14733842.04938499  5409738.8780994  17854825.32779439
 13009238.84014773 15383752.05836442]
eta_min = 0.5605127922390928	eta_max = 0.5605127922391011
af = 0.0007320190694867889	bf = 1.185478777209423	zeta = 0.0008052209764354679	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.04172733 0.04335729 0.00844634 0.00864645 0.00894996 0.01039032
 0.0090096  0.00818872 0.00880296 0.0528282 ]
ene_comp = [4.57956095e-07 4.35054377e-06 2.96827261e-07 1.24039619e-08
 4.48446887e-06 4.94584193e-07 2.46604912e-08 8.90470630e-07
 3.43322672e-07 4.35774595e-07]
ene_total = [2.71599263 2.82233791 0.54977748 0.56278379 0.58283005 0.67632096
 0.58642142 0.53304851 0.5729925  3.43852876]
ti_comp = [1.56503336 1.54873368 1.89784324 1.89584215 1.89280701 1.87840342
 1.89221065 1.90041938 1.89427702 1.45402462]
ti_coms = [0.41727326 0.43357294 0.08446337 0.08646447 0.0894996  0.1039032
 0.09009597 0.08188724 0.0880296  0.528282  ]
t_total = [26.68917656 26.68917656 26.68917656 26.68917656 26.68917656 26.68917656
 26.68917656 26.68917656 26.68917656 26.68917656]
ene_coms = [0.04172733 0.04335729 0.00844634 0.00864645 0.00894996 0.01039032
 0.0090096  0.00818872 0.00880296 0.0528282 ]
ene_comp = [2.78157334e-06 2.64247090e-05 1.80289509e-06 7.53402569e-08
 2.72381549e-05 3.00404825e-06 1.49785024e-07 5.40861753e-06
 2.08530294e-06 2.64684543e-06]
ene_total = [0.56184154 0.58410526 0.1137432  0.11641416 0.12086629 0.13993256
 0.12130451 0.11032332 0.11854847 0.71129853]
optimize_network iter = 1 obj = 2.698377846862188
eta = 0.5605127922391011
freqs = [15261561.48798533 32435541.71643902 12385495.08516284  4299503.49461345
 30645528.45570678 14733842.0493849   5409738.87809937 17854825.32779428
 13009238.84014765 15383752.05836442]
Done!
ene_coms = [0.04172733 0.04335729 0.00844634 0.00864645 0.00894996 0.01039032
 0.0090096  0.00818872 0.00880296 0.0528282 ]
ene_comp = [2.64125904e-06 2.50917351e-05 1.71194945e-06 7.15397763e-08
 2.58641473e-05 2.85251137e-06 1.42229235e-07 5.13578403e-06
 1.98011145e-06 2.51332737e-06]
ene_total = [0.04172997 0.04338239 0.00844805 0.00864652 0.00897582 0.01039317
 0.00900974 0.00819386 0.00880494 0.05283071]
At round 65 energy consumption: 0.20041516936125528
At round 65 eta: 0.5605127922391011
At round 65 a_n: 5.917122803248752
At round 65 local rounds: 18.956232401634136
At round 65 global rounds: 13.463697460946205
gradient difference: 0.32208824157714844
train() client id: f_00000-0-0 loss: 1.330248  [   32/  126]
train() client id: f_00000-0-1 loss: 1.300322  [   64/  126]
train() client id: f_00000-0-2 loss: 1.180620  [   96/  126]
train() client id: f_00000-1-0 loss: 1.426991  [   32/  126]
train() client id: f_00000-1-1 loss: 1.101924  [   64/  126]
train() client id: f_00000-1-2 loss: 1.088980  [   96/  126]
train() client id: f_00000-2-0 loss: 1.110519  [   32/  126]
train() client id: f_00000-2-1 loss: 0.911578  [   64/  126]
train() client id: f_00000-2-2 loss: 1.136045  [   96/  126]
train() client id: f_00000-3-0 loss: 1.008147  [   32/  126]
train() client id: f_00000-3-1 loss: 0.953187  [   64/  126]
train() client id: f_00000-3-2 loss: 1.033579  [   96/  126]
train() client id: f_00000-4-0 loss: 1.028508  [   32/  126]
train() client id: f_00000-4-1 loss: 1.020349  [   64/  126]
train() client id: f_00000-4-2 loss: 0.735198  [   96/  126]
train() client id: f_00000-5-0 loss: 0.813966  [   32/  126]
train() client id: f_00000-5-1 loss: 0.796564  [   64/  126]
train() client id: f_00000-5-2 loss: 0.973997  [   96/  126]
train() client id: f_00000-6-0 loss: 0.975496  [   32/  126]
train() client id: f_00000-6-1 loss: 0.793935  [   64/  126]
train() client id: f_00000-6-2 loss: 0.795128  [   96/  126]
train() client id: f_00000-7-0 loss: 0.800668  [   32/  126]
train() client id: f_00000-7-1 loss: 0.843801  [   64/  126]
train() client id: f_00000-7-2 loss: 0.869279  [   96/  126]
train() client id: f_00000-8-0 loss: 0.732287  [   32/  126]
train() client id: f_00000-8-1 loss: 0.791615  [   64/  126]
train() client id: f_00000-8-2 loss: 0.837855  [   96/  126]
train() client id: f_00000-9-0 loss: 0.821604  [   32/  126]
train() client id: f_00000-9-1 loss: 0.786431  [   64/  126]
train() client id: f_00000-9-2 loss: 0.794671  [   96/  126]
train() client id: f_00000-10-0 loss: 0.868828  [   32/  126]
train() client id: f_00000-10-1 loss: 0.663237  [   64/  126]
train() client id: f_00000-10-2 loss: 0.795844  [   96/  126]
train() client id: f_00000-11-0 loss: 0.687881  [   32/  126]
train() client id: f_00000-11-1 loss: 0.828610  [   64/  126]
train() client id: f_00000-11-2 loss: 0.936189  [   96/  126]
train() client id: f_00000-12-0 loss: 0.857767  [   32/  126]
train() client id: f_00000-12-1 loss: 0.663569  [   64/  126]
train() client id: f_00000-12-2 loss: 0.908333  [   96/  126]
train() client id: f_00000-13-0 loss: 0.700944  [   32/  126]
train() client id: f_00000-13-1 loss: 0.778874  [   64/  126]
train() client id: f_00000-13-2 loss: 0.791503  [   96/  126]
train() client id: f_00000-14-0 loss: 0.777585  [   32/  126]
train() client id: f_00000-14-1 loss: 0.844466  [   64/  126]
train() client id: f_00000-14-2 loss: 0.713478  [   96/  126]
train() client id: f_00000-15-0 loss: 0.772954  [   32/  126]
train() client id: f_00000-15-1 loss: 0.770620  [   64/  126]
train() client id: f_00000-15-2 loss: 0.733987  [   96/  126]
train() client id: f_00000-16-0 loss: 0.665442  [   32/  126]
train() client id: f_00000-16-1 loss: 0.791662  [   64/  126]
train() client id: f_00000-16-2 loss: 0.850725  [   96/  126]
train() client id: f_00000-17-0 loss: 0.794446  [   32/  126]
train() client id: f_00000-17-1 loss: 0.680988  [   64/  126]
train() client id: f_00000-17-2 loss: 0.679893  [   96/  126]
train() client id: f_00001-0-0 loss: 0.395546  [   32/  265]
train() client id: f_00001-0-1 loss: 0.398726  [   64/  265]
train() client id: f_00001-0-2 loss: 0.324488  [   96/  265]
train() client id: f_00001-0-3 loss: 0.269934  [  128/  265]
train() client id: f_00001-0-4 loss: 0.343541  [  160/  265]
train() client id: f_00001-0-5 loss: 0.290686  [  192/  265]
train() client id: f_00001-0-6 loss: 0.359507  [  224/  265]
train() client id: f_00001-0-7 loss: 0.429841  [  256/  265]
train() client id: f_00001-1-0 loss: 0.353682  [   32/  265]
train() client id: f_00001-1-1 loss: 0.323164  [   64/  265]
train() client id: f_00001-1-2 loss: 0.295961  [   96/  265]
train() client id: f_00001-1-3 loss: 0.304176  [  128/  265]
train() client id: f_00001-1-4 loss: 0.286575  [  160/  265]
train() client id: f_00001-1-5 loss: 0.411427  [  192/  265]
train() client id: f_00001-1-6 loss: 0.372021  [  224/  265]
train() client id: f_00001-1-7 loss: 0.352048  [  256/  265]
train() client id: f_00001-2-0 loss: 0.281803  [   32/  265]
train() client id: f_00001-2-1 loss: 0.305885  [   64/  265]
train() client id: f_00001-2-2 loss: 0.473703  [   96/  265]
train() client id: f_00001-2-3 loss: 0.224927  [  128/  265]
train() client id: f_00001-2-4 loss: 0.246206  [  160/  265]
train() client id: f_00001-2-5 loss: 0.449501  [  192/  265]
train() client id: f_00001-2-6 loss: 0.364124  [  224/  265]
train() client id: f_00001-2-7 loss: 0.318904  [  256/  265]
train() client id: f_00001-3-0 loss: 0.266062  [   32/  265]
train() client id: f_00001-3-1 loss: 0.297348  [   64/  265]
train() client id: f_00001-3-2 loss: 0.275297  [   96/  265]
train() client id: f_00001-3-3 loss: 0.339466  [  128/  265]
train() client id: f_00001-3-4 loss: 0.366569  [  160/  265]
train() client id: f_00001-3-5 loss: 0.331146  [  192/  265]
train() client id: f_00001-3-6 loss: 0.432752  [  224/  265]
train() client id: f_00001-3-7 loss: 0.250322  [  256/  265]
train() client id: f_00001-4-0 loss: 0.306874  [   32/  265]
train() client id: f_00001-4-1 loss: 0.305853  [   64/  265]
train() client id: f_00001-4-2 loss: 0.335148  [   96/  265]
train() client id: f_00001-4-3 loss: 0.286160  [  128/  265]
train() client id: f_00001-4-4 loss: 0.369518  [  160/  265]
train() client id: f_00001-4-5 loss: 0.353554  [  192/  265]
train() client id: f_00001-4-6 loss: 0.320246  [  224/  265]
train() client id: f_00001-4-7 loss: 0.319284  [  256/  265]
train() client id: f_00001-5-0 loss: 0.465445  [   32/  265]
train() client id: f_00001-5-1 loss: 0.407710  [   64/  265]
train() client id: f_00001-5-2 loss: 0.297415  [   96/  265]
train() client id: f_00001-5-3 loss: 0.237522  [  128/  265]
train() client id: f_00001-5-4 loss: 0.221442  [  160/  265]
train() client id: f_00001-5-5 loss: 0.268332  [  192/  265]
train() client id: f_00001-5-6 loss: 0.365338  [  224/  265]
train() client id: f_00001-5-7 loss: 0.292277  [  256/  265]
train() client id: f_00001-6-0 loss: 0.246583  [   32/  265]
train() client id: f_00001-6-1 loss: 0.497915  [   64/  265]
train() client id: f_00001-6-2 loss: 0.293342  [   96/  265]
train() client id: f_00001-6-3 loss: 0.326273  [  128/  265]
train() client id: f_00001-6-4 loss: 0.255614  [  160/  265]
train() client id: f_00001-6-5 loss: 0.305681  [  192/  265]
train() client id: f_00001-6-6 loss: 0.287387  [  224/  265]
train() client id: f_00001-6-7 loss: 0.323949  [  256/  265]
train() client id: f_00001-7-0 loss: 0.409750  [   32/  265]
train() client id: f_00001-7-1 loss: 0.207772  [   64/  265]
train() client id: f_00001-7-2 loss: 0.327400  [   96/  265]
train() client id: f_00001-7-3 loss: 0.321049  [  128/  265]
train() client id: f_00001-7-4 loss: 0.417459  [  160/  265]
train() client id: f_00001-7-5 loss: 0.300879  [  192/  265]
train() client id: f_00001-7-6 loss: 0.322505  [  224/  265]
train() client id: f_00001-7-7 loss: 0.230413  [  256/  265]
train() client id: f_00001-8-0 loss: 0.273062  [   32/  265]
train() client id: f_00001-8-1 loss: 0.387699  [   64/  265]
train() client id: f_00001-8-2 loss: 0.334445  [   96/  265]
train() client id: f_00001-8-3 loss: 0.328637  [  128/  265]
train() client id: f_00001-8-4 loss: 0.367336  [  160/  265]
train() client id: f_00001-8-5 loss: 0.357666  [  192/  265]
train() client id: f_00001-8-6 loss: 0.250126  [  224/  265]
train() client id: f_00001-8-7 loss: 0.206622  [  256/  265]
train() client id: f_00001-9-0 loss: 0.270361  [   32/  265]
train() client id: f_00001-9-1 loss: 0.217372  [   64/  265]
train() client id: f_00001-9-2 loss: 0.409579  [   96/  265]
train() client id: f_00001-9-3 loss: 0.218910  [  128/  265]
train() client id: f_00001-9-4 loss: 0.355480  [  160/  265]
train() client id: f_00001-9-5 loss: 0.277818  [  192/  265]
train() client id: f_00001-9-6 loss: 0.381812  [  224/  265]
train() client id: f_00001-9-7 loss: 0.254274  [  256/  265]
train() client id: f_00001-10-0 loss: 0.269141  [   32/  265]
train() client id: f_00001-10-1 loss: 0.296522  [   64/  265]
train() client id: f_00001-10-2 loss: 0.264993  [   96/  265]
train() client id: f_00001-10-3 loss: 0.221184  [  128/  265]
train() client id: f_00001-10-4 loss: 0.393437  [  160/  265]
train() client id: f_00001-10-5 loss: 0.428977  [  192/  265]
train() client id: f_00001-10-6 loss: 0.217092  [  224/  265]
train() client id: f_00001-10-7 loss: 0.314705  [  256/  265]
train() client id: f_00001-11-0 loss: 0.303756  [   32/  265]
train() client id: f_00001-11-1 loss: 0.254868  [   64/  265]
train() client id: f_00001-11-2 loss: 0.210519  [   96/  265]
train() client id: f_00001-11-3 loss: 0.218666  [  128/  265]
train() client id: f_00001-11-4 loss: 0.319758  [  160/  265]
train() client id: f_00001-11-5 loss: 0.432003  [  192/  265]
train() client id: f_00001-11-6 loss: 0.392344  [  224/  265]
train() client id: f_00001-11-7 loss: 0.360483  [  256/  265]
train() client id: f_00001-12-0 loss: 0.272619  [   32/  265]
train() client id: f_00001-12-1 loss: 0.210467  [   64/  265]
train() client id: f_00001-12-2 loss: 0.302597  [   96/  265]
train() client id: f_00001-12-3 loss: 0.291321  [  128/  265]
train() client id: f_00001-12-4 loss: 0.254411  [  160/  265]
train() client id: f_00001-12-5 loss: 0.460459  [  192/  265]
train() client id: f_00001-12-6 loss: 0.388727  [  224/  265]
train() client id: f_00001-12-7 loss: 0.322495  [  256/  265]
train() client id: f_00001-13-0 loss: 0.349603  [   32/  265]
train() client id: f_00001-13-1 loss: 0.259720  [   64/  265]
train() client id: f_00001-13-2 loss: 0.323462  [   96/  265]
train() client id: f_00001-13-3 loss: 0.300966  [  128/  265]
train() client id: f_00001-13-4 loss: 0.218099  [  160/  265]
train() client id: f_00001-13-5 loss: 0.409168  [  192/  265]
train() client id: f_00001-13-6 loss: 0.352172  [  224/  265]
train() client id: f_00001-13-7 loss: 0.286679  [  256/  265]
train() client id: f_00001-14-0 loss: 0.216452  [   32/  265]
train() client id: f_00001-14-1 loss: 0.383979  [   64/  265]
train() client id: f_00001-14-2 loss: 0.270428  [   96/  265]
train() client id: f_00001-14-3 loss: 0.257472  [  128/  265]
train() client id: f_00001-14-4 loss: 0.318485  [  160/  265]
train() client id: f_00001-14-5 loss: 0.276967  [  192/  265]
train() client id: f_00001-14-6 loss: 0.534959  [  224/  265]
train() client id: f_00001-14-7 loss: 0.235920  [  256/  265]
train() client id: f_00001-15-0 loss: 0.253520  [   32/  265]
train() client id: f_00001-15-1 loss: 0.319122  [   64/  265]
train() client id: f_00001-15-2 loss: 0.305395  [   96/  265]
train() client id: f_00001-15-3 loss: 0.288019  [  128/  265]
train() client id: f_00001-15-4 loss: 0.272236  [  160/  265]
train() client id: f_00001-15-5 loss: 0.387145  [  192/  265]
train() client id: f_00001-15-6 loss: 0.283694  [  224/  265]
train() client id: f_00001-15-7 loss: 0.319475  [  256/  265]
train() client id: f_00001-16-0 loss: 0.396646  [   32/  265]
train() client id: f_00001-16-1 loss: 0.292536  [   64/  265]
train() client id: f_00001-16-2 loss: 0.236283  [   96/  265]
train() client id: f_00001-16-3 loss: 0.305042  [  128/  265]
train() client id: f_00001-16-4 loss: 0.451008  [  160/  265]
train() client id: f_00001-16-5 loss: 0.248214  [  192/  265]
train() client id: f_00001-16-6 loss: 0.320232  [  224/  265]
train() client id: f_00001-16-7 loss: 0.254607  [  256/  265]
train() client id: f_00001-17-0 loss: 0.262956  [   32/  265]
train() client id: f_00001-17-1 loss: 0.309090  [   64/  265]
train() client id: f_00001-17-2 loss: 0.220217  [   96/  265]
train() client id: f_00001-17-3 loss: 0.379315  [  128/  265]
train() client id: f_00001-17-4 loss: 0.336266  [  160/  265]
train() client id: f_00001-17-5 loss: 0.304807  [  192/  265]
train() client id: f_00001-17-6 loss: 0.298137  [  224/  265]
train() client id: f_00001-17-7 loss: 0.402928  [  256/  265]
train() client id: f_00002-0-0 loss: 1.437350  [   32/  124]
train() client id: f_00002-0-1 loss: 1.059233  [   64/  124]
train() client id: f_00002-0-2 loss: 1.418104  [   96/  124]
train() client id: f_00002-1-0 loss: 1.169505  [   32/  124]
train() client id: f_00002-1-1 loss: 1.131530  [   64/  124]
train() client id: f_00002-1-2 loss: 1.376320  [   96/  124]
train() client id: f_00002-2-0 loss: 1.464704  [   32/  124]
train() client id: f_00002-2-1 loss: 0.949468  [   64/  124]
train() client id: f_00002-2-2 loss: 1.227029  [   96/  124]
train() client id: f_00002-3-0 loss: 1.246748  [   32/  124]
train() client id: f_00002-3-1 loss: 1.060789  [   64/  124]
train() client id: f_00002-3-2 loss: 1.234507  [   96/  124]
train() client id: f_00002-4-0 loss: 1.072111  [   32/  124]
train() client id: f_00002-4-1 loss: 1.168474  [   64/  124]
train() client id: f_00002-4-2 loss: 1.131713  [   96/  124]
train() client id: f_00002-5-0 loss: 1.163797  [   32/  124]
train() client id: f_00002-5-1 loss: 1.213853  [   64/  124]
train() client id: f_00002-5-2 loss: 1.035188  [   96/  124]
train() client id: f_00002-6-0 loss: 1.183782  [   32/  124]
train() client id: f_00002-6-1 loss: 1.099287  [   64/  124]
train() client id: f_00002-6-2 loss: 1.028079  [   96/  124]
train() client id: f_00002-7-0 loss: 1.176899  [   32/  124]
train() client id: f_00002-7-1 loss: 1.064843  [   64/  124]
train() client id: f_00002-7-2 loss: 0.940866  [   96/  124]
train() client id: f_00002-8-0 loss: 1.104344  [   32/  124]
train() client id: f_00002-8-1 loss: 1.181024  [   64/  124]
train() client id: f_00002-8-2 loss: 0.818169  [   96/  124]
train() client id: f_00002-9-0 loss: 1.041673  [   32/  124]
train() client id: f_00002-9-1 loss: 0.824105  [   64/  124]
train() client id: f_00002-9-2 loss: 1.044608  [   96/  124]
train() client id: f_00002-10-0 loss: 1.066609  [   32/  124]
train() client id: f_00002-10-1 loss: 0.946880  [   64/  124]
train() client id: f_00002-10-2 loss: 0.815742  [   96/  124]
train() client id: f_00002-11-0 loss: 1.049941  [   32/  124]
train() client id: f_00002-11-1 loss: 0.884912  [   64/  124]
train() client id: f_00002-11-2 loss: 0.861099  [   96/  124]
train() client id: f_00002-12-0 loss: 0.927171  [   32/  124]
train() client id: f_00002-12-1 loss: 0.994573  [   64/  124]
train() client id: f_00002-12-2 loss: 1.060670  [   96/  124]
train() client id: f_00002-13-0 loss: 0.832996  [   32/  124]
train() client id: f_00002-13-1 loss: 0.988980  [   64/  124]
train() client id: f_00002-13-2 loss: 1.088785  [   96/  124]
train() client id: f_00002-14-0 loss: 0.965003  [   32/  124]
train() client id: f_00002-14-1 loss: 0.825461  [   64/  124]
train() client id: f_00002-14-2 loss: 0.969788  [   96/  124]
train() client id: f_00002-15-0 loss: 1.076772  [   32/  124]
train() client id: f_00002-15-1 loss: 0.951852  [   64/  124]
train() client id: f_00002-15-2 loss: 0.946900  [   96/  124]
train() client id: f_00002-16-0 loss: 0.947796  [   32/  124]
train() client id: f_00002-16-1 loss: 1.016048  [   64/  124]
train() client id: f_00002-16-2 loss: 0.978465  [   96/  124]
train() client id: f_00002-17-0 loss: 0.929783  [   32/  124]
train() client id: f_00002-17-1 loss: 1.007830  [   64/  124]
train() client id: f_00002-17-2 loss: 0.943324  [   96/  124]
train() client id: f_00003-0-0 loss: 0.717516  [   32/   43]
train() client id: f_00003-1-0 loss: 0.724451  [   32/   43]
train() client id: f_00003-2-0 loss: 0.752910  [   32/   43]
train() client id: f_00003-3-0 loss: 0.964501  [   32/   43]
train() client id: f_00003-4-0 loss: 0.693478  [   32/   43]
train() client id: f_00003-5-0 loss: 0.738890  [   32/   43]
train() client id: f_00003-6-0 loss: 0.800183  [   32/   43]
train() client id: f_00003-7-0 loss: 0.885732  [   32/   43]
train() client id: f_00003-8-0 loss: 0.700601  [   32/   43]
train() client id: f_00003-9-0 loss: 0.742440  [   32/   43]
train() client id: f_00003-10-0 loss: 0.595039  [   32/   43]
train() client id: f_00003-11-0 loss: 0.642191  [   32/   43]
train() client id: f_00003-12-0 loss: 0.898754  [   32/   43]
train() client id: f_00003-13-0 loss: 0.517012  [   32/   43]
train() client id: f_00003-14-0 loss: 0.744494  [   32/   43]
train() client id: f_00003-15-0 loss: 0.699224  [   32/   43]
train() client id: f_00003-16-0 loss: 0.896383  [   32/   43]
train() client id: f_00003-17-0 loss: 0.611852  [   32/   43]
train() client id: f_00004-0-0 loss: 1.032274  [   32/  306]
train() client id: f_00004-0-1 loss: 1.174180  [   64/  306]
train() client id: f_00004-0-2 loss: 0.884759  [   96/  306]
train() client id: f_00004-0-3 loss: 0.998477  [  128/  306]
train() client id: f_00004-0-4 loss: 1.071058  [  160/  306]
train() client id: f_00004-0-5 loss: 1.009948  [  192/  306]
train() client id: f_00004-0-6 loss: 1.027562  [  224/  306]
train() client id: f_00004-0-7 loss: 0.814391  [  256/  306]
train() client id: f_00004-0-8 loss: 0.821182  [  288/  306]
train() client id: f_00004-1-0 loss: 0.853516  [   32/  306]
train() client id: f_00004-1-1 loss: 0.875333  [   64/  306]
train() client id: f_00004-1-2 loss: 0.916673  [   96/  306]
train() client id: f_00004-1-3 loss: 1.022013  [  128/  306]
train() client id: f_00004-1-4 loss: 0.910543  [  160/  306]
train() client id: f_00004-1-5 loss: 0.897385  [  192/  306]
train() client id: f_00004-1-6 loss: 1.087012  [  224/  306]
train() client id: f_00004-1-7 loss: 1.022282  [  256/  306]
train() client id: f_00004-1-8 loss: 0.963446  [  288/  306]
train() client id: f_00004-2-0 loss: 1.033265  [   32/  306]
train() client id: f_00004-2-1 loss: 0.968013  [   64/  306]
train() client id: f_00004-2-2 loss: 1.001981  [   96/  306]
train() client id: f_00004-2-3 loss: 0.941559  [  128/  306]
train() client id: f_00004-2-4 loss: 0.900480  [  160/  306]
train() client id: f_00004-2-5 loss: 0.955646  [  192/  306]
train() client id: f_00004-2-6 loss: 0.828140  [  224/  306]
train() client id: f_00004-2-7 loss: 1.020983  [  256/  306]
train() client id: f_00004-2-8 loss: 0.958269  [  288/  306]
train() client id: f_00004-3-0 loss: 0.928469  [   32/  306]
train() client id: f_00004-3-1 loss: 0.964932  [   64/  306]
train() client id: f_00004-3-2 loss: 1.020193  [   96/  306]
train() client id: f_00004-3-3 loss: 0.977605  [  128/  306]
train() client id: f_00004-3-4 loss: 1.017154  [  160/  306]
train() client id: f_00004-3-5 loss: 0.849258  [  192/  306]
train() client id: f_00004-3-6 loss: 0.994811  [  224/  306]
train() client id: f_00004-3-7 loss: 0.938138  [  256/  306]
train() client id: f_00004-3-8 loss: 0.865817  [  288/  306]
train() client id: f_00004-4-0 loss: 0.849580  [   32/  306]
train() client id: f_00004-4-1 loss: 1.095975  [   64/  306]
train() client id: f_00004-4-2 loss: 0.937491  [   96/  306]
train() client id: f_00004-4-3 loss: 1.029571  [  128/  306]
train() client id: f_00004-4-4 loss: 0.913354  [  160/  306]
train() client id: f_00004-4-5 loss: 0.912885  [  192/  306]
train() client id: f_00004-4-6 loss: 0.924497  [  224/  306]
train() client id: f_00004-4-7 loss: 0.884006  [  256/  306]
train() client id: f_00004-4-8 loss: 0.978364  [  288/  306]
train() client id: f_00004-5-0 loss: 1.080017  [   32/  306]
train() client id: f_00004-5-1 loss: 0.886337  [   64/  306]
train() client id: f_00004-5-2 loss: 0.829409  [   96/  306]
train() client id: f_00004-5-3 loss: 0.945635  [  128/  306]
train() client id: f_00004-5-4 loss: 0.940231  [  160/  306]
train() client id: f_00004-5-5 loss: 0.921800  [  192/  306]
train() client id: f_00004-5-6 loss: 0.962195  [  224/  306]
train() client id: f_00004-5-7 loss: 0.986985  [  256/  306]
train() client id: f_00004-5-8 loss: 0.904609  [  288/  306]
train() client id: f_00004-6-0 loss: 0.926303  [   32/  306]
train() client id: f_00004-6-1 loss: 0.943754  [   64/  306]
train() client id: f_00004-6-2 loss: 0.903737  [   96/  306]
train() client id: f_00004-6-3 loss: 1.123115  [  128/  306]
train() client id: f_00004-6-4 loss: 1.043559  [  160/  306]
train() client id: f_00004-6-5 loss: 0.845017  [  192/  306]
train() client id: f_00004-6-6 loss: 0.982925  [  224/  306]
train() client id: f_00004-6-7 loss: 0.824422  [  256/  306]
train() client id: f_00004-6-8 loss: 0.949177  [  288/  306]
train() client id: f_00004-7-0 loss: 0.832467  [   32/  306]
train() client id: f_00004-7-1 loss: 0.897698  [   64/  306]
train() client id: f_00004-7-2 loss: 0.935367  [   96/  306]
train() client id: f_00004-7-3 loss: 1.081851  [  128/  306]
train() client id: f_00004-7-4 loss: 0.901294  [  160/  306]
train() client id: f_00004-7-5 loss: 0.936892  [  192/  306]
train() client id: f_00004-7-6 loss: 0.964205  [  224/  306]
train() client id: f_00004-7-7 loss: 0.993320  [  256/  306]
train() client id: f_00004-7-8 loss: 0.867264  [  288/  306]
train() client id: f_00004-8-0 loss: 0.913790  [   32/  306]
train() client id: f_00004-8-1 loss: 0.911725  [   64/  306]
train() client id: f_00004-8-2 loss: 0.906990  [   96/  306]
train() client id: f_00004-8-3 loss: 0.925686  [  128/  306]
train() client id: f_00004-8-4 loss: 1.006731  [  160/  306]
train() client id: f_00004-8-5 loss: 1.077724  [  192/  306]
train() client id: f_00004-8-6 loss: 0.890864  [  224/  306]
train() client id: f_00004-8-7 loss: 0.826861  [  256/  306]
train() client id: f_00004-8-8 loss: 0.972302  [  288/  306]
train() client id: f_00004-9-0 loss: 1.003780  [   32/  306]
train() client id: f_00004-9-1 loss: 0.855708  [   64/  306]
train() client id: f_00004-9-2 loss: 0.976056  [   96/  306]
train() client id: f_00004-9-3 loss: 0.881423  [  128/  306]
train() client id: f_00004-9-4 loss: 1.025536  [  160/  306]
train() client id: f_00004-9-5 loss: 0.926963  [  192/  306]
train() client id: f_00004-9-6 loss: 0.862165  [  224/  306]
train() client id: f_00004-9-7 loss: 0.921506  [  256/  306]
train() client id: f_00004-9-8 loss: 0.955324  [  288/  306]
train() client id: f_00004-10-0 loss: 0.920238  [   32/  306]
train() client id: f_00004-10-1 loss: 1.001699  [   64/  306]
train() client id: f_00004-10-2 loss: 0.846755  [   96/  306]
train() client id: f_00004-10-3 loss: 0.900355  [  128/  306]
train() client id: f_00004-10-4 loss: 0.902242  [  160/  306]
train() client id: f_00004-10-5 loss: 0.902423  [  192/  306]
train() client id: f_00004-10-6 loss: 0.917746  [  224/  306]
train() client id: f_00004-10-7 loss: 0.938533  [  256/  306]
train() client id: f_00004-10-8 loss: 0.959751  [  288/  306]
train() client id: f_00004-11-0 loss: 1.091360  [   32/  306]
train() client id: f_00004-11-1 loss: 0.906843  [   64/  306]
train() client id: f_00004-11-2 loss: 0.980924  [   96/  306]
train() client id: f_00004-11-3 loss: 0.869564  [  128/  306]
train() client id: f_00004-11-4 loss: 0.857208  [  160/  306]
train() client id: f_00004-11-5 loss: 0.959845  [  192/  306]
train() client id: f_00004-11-6 loss: 0.844110  [  224/  306]
train() client id: f_00004-11-7 loss: 0.950934  [  256/  306]
train() client id: f_00004-11-8 loss: 0.926528  [  288/  306]
train() client id: f_00004-12-0 loss: 0.842046  [   32/  306]
train() client id: f_00004-12-1 loss: 0.910166  [   64/  306]
train() client id: f_00004-12-2 loss: 1.013186  [   96/  306]
train() client id: f_00004-12-3 loss: 0.798718  [  128/  306]
train() client id: f_00004-12-4 loss: 0.987223  [  160/  306]
train() client id: f_00004-12-5 loss: 0.940938  [  192/  306]
train() client id: f_00004-12-6 loss: 0.855264  [  224/  306]
train() client id: f_00004-12-7 loss: 0.931464  [  256/  306]
train() client id: f_00004-12-8 loss: 1.014337  [  288/  306]
train() client id: f_00004-13-0 loss: 0.915273  [   32/  306]
train() client id: f_00004-13-1 loss: 0.931635  [   64/  306]
train() client id: f_00004-13-2 loss: 1.216480  [   96/  306]
train() client id: f_00004-13-3 loss: 0.856122  [  128/  306]
train() client id: f_00004-13-4 loss: 0.944367  [  160/  306]
train() client id: f_00004-13-5 loss: 0.914311  [  192/  306]
train() client id: f_00004-13-6 loss: 0.835646  [  224/  306]
train() client id: f_00004-13-7 loss: 0.952012  [  256/  306]
train() client id: f_00004-13-8 loss: 0.872500  [  288/  306]
train() client id: f_00004-14-0 loss: 0.947590  [   32/  306]
train() client id: f_00004-14-1 loss: 0.966375  [   64/  306]
train() client id: f_00004-14-2 loss: 0.878146  [   96/  306]
train() client id: f_00004-14-3 loss: 0.930431  [  128/  306]
train() client id: f_00004-14-4 loss: 0.883946  [  160/  306]
train() client id: f_00004-14-5 loss: 0.984728  [  192/  306]
train() client id: f_00004-14-6 loss: 0.956422  [  224/  306]
train() client id: f_00004-14-7 loss: 0.881264  [  256/  306]
train() client id: f_00004-14-8 loss: 0.905641  [  288/  306]
train() client id: f_00004-15-0 loss: 0.879871  [   32/  306]
train() client id: f_00004-15-1 loss: 0.943176  [   64/  306]
train() client id: f_00004-15-2 loss: 0.883422  [   96/  306]
train() client id: f_00004-15-3 loss: 0.952944  [  128/  306]
train() client id: f_00004-15-4 loss: 0.972022  [  160/  306]
train() client id: f_00004-15-5 loss: 0.987838  [  192/  306]
train() client id: f_00004-15-6 loss: 0.958606  [  224/  306]
train() client id: f_00004-15-7 loss: 0.910917  [  256/  306]
train() client id: f_00004-15-8 loss: 0.863633  [  288/  306]
train() client id: f_00004-16-0 loss: 0.946470  [   32/  306]
train() client id: f_00004-16-1 loss: 0.849366  [   64/  306]
train() client id: f_00004-16-2 loss: 0.800113  [   96/  306]
train() client id: f_00004-16-3 loss: 0.898350  [  128/  306]
train() client id: f_00004-16-4 loss: 0.870282  [  160/  306]
train() client id: f_00004-16-5 loss: 0.933593  [  192/  306]
train() client id: f_00004-16-6 loss: 0.985103  [  224/  306]
train() client id: f_00004-16-7 loss: 1.032687  [  256/  306]
train() client id: f_00004-16-8 loss: 0.877941  [  288/  306]
train() client id: f_00004-17-0 loss: 0.971919  [   32/  306]
train() client id: f_00004-17-1 loss: 0.871302  [   64/  306]
train() client id: f_00004-17-2 loss: 0.846402  [   96/  306]
train() client id: f_00004-17-3 loss: 1.027914  [  128/  306]
train() client id: f_00004-17-4 loss: 0.940120  [  160/  306]
train() client id: f_00004-17-5 loss: 0.850824  [  192/  306]
train() client id: f_00004-17-6 loss: 0.944729  [  224/  306]
train() client id: f_00004-17-7 loss: 0.913154  [  256/  306]
train() client id: f_00004-17-8 loss: 1.016507  [  288/  306]
train() client id: f_00005-0-0 loss: 0.449389  [   32/  146]
train() client id: f_00005-0-1 loss: 0.125931  [   64/  146]
train() client id: f_00005-0-2 loss: 0.393853  [   96/  146]
train() client id: f_00005-0-3 loss: 0.118420  [  128/  146]
train() client id: f_00005-1-0 loss: 0.202952  [   32/  146]
train() client id: f_00005-1-1 loss: 0.404749  [   64/  146]
train() client id: f_00005-1-2 loss: 0.150237  [   96/  146]
train() client id: f_00005-1-3 loss: 0.176556  [  128/  146]
train() client id: f_00005-2-0 loss: 0.499194  [   32/  146]
train() client id: f_00005-2-1 loss: 0.437737  [   64/  146]
train() client id: f_00005-2-2 loss: 0.118564  [   96/  146]
train() client id: f_00005-2-3 loss: 0.123603  [  128/  146]
train() client id: f_00005-3-0 loss: 0.261805  [   32/  146]
train() client id: f_00005-3-1 loss: 0.223107  [   64/  146]
train() client id: f_00005-3-2 loss: 0.369459  [   96/  146]
train() client id: f_00005-3-3 loss: 0.052360  [  128/  146]
train() client id: f_00005-4-0 loss: 0.217900  [   32/  146]
train() client id: f_00005-4-1 loss: 0.421852  [   64/  146]
train() client id: f_00005-4-2 loss: 0.148524  [   96/  146]
train() client id: f_00005-4-3 loss: 0.081723  [  128/  146]
train() client id: f_00005-5-0 loss: 0.104429  [   32/  146]
train() client id: f_00005-5-1 loss: 0.436880  [   64/  146]
train() client id: f_00005-5-2 loss: 0.184065  [   96/  146]
train() client id: f_00005-5-3 loss: 0.341459  [  128/  146]
train() client id: f_00005-6-0 loss: 0.180341  [   32/  146]
train() client id: f_00005-6-1 loss: 0.143346  [   64/  146]
train() client id: f_00005-6-2 loss: 0.491711  [   96/  146]
train() client id: f_00005-6-3 loss: 0.351871  [  128/  146]
train() client id: f_00005-7-0 loss: 0.043208  [   32/  146]
train() client id: f_00005-7-1 loss: 0.433050  [   64/  146]
train() client id: f_00005-7-2 loss: 0.019828  [   96/  146]
train() client id: f_00005-7-3 loss: 0.536140  [  128/  146]
train() client id: f_00005-8-0 loss: 0.281801  [   32/  146]
train() client id: f_00005-8-1 loss: 0.198108  [   64/  146]
train() client id: f_00005-8-2 loss: 0.173200  [   96/  146]
train() client id: f_00005-8-3 loss: 0.340924  [  128/  146]
train() client id: f_00005-9-0 loss: 0.107439  [   32/  146]
train() client id: f_00005-9-1 loss: 0.361604  [   64/  146]
train() client id: f_00005-9-2 loss: 0.325017  [   96/  146]
train() client id: f_00005-9-3 loss: 0.228909  [  128/  146]
train() client id: f_00005-10-0 loss: 0.180869  [   32/  146]
train() client id: f_00005-10-1 loss: 0.332263  [   64/  146]
train() client id: f_00005-10-2 loss: 0.151721  [   96/  146]
train() client id: f_00005-10-3 loss: 0.369441  [  128/  146]
train() client id: f_00005-11-0 loss: 0.222179  [   32/  146]
train() client id: f_00005-11-1 loss: 0.100822  [   64/  146]
train() client id: f_00005-11-2 loss: 0.375707  [   96/  146]
train() client id: f_00005-11-3 loss: 0.228342  [  128/  146]
train() client id: f_00005-12-0 loss: 0.459312  [   32/  146]
train() client id: f_00005-12-1 loss: -0.118687  [   64/  146]
train() client id: f_00005-12-2 loss: 0.285915  [   96/  146]
train() client id: f_00005-12-3 loss: 0.186581  [  128/  146]
train() client id: f_00005-13-0 loss: 0.288691  [   32/  146]
train() client id: f_00005-13-1 loss: 0.577598  [   64/  146]
train() client id: f_00005-13-2 loss: -0.014429  [   96/  146]
train() client id: f_00005-13-3 loss: 0.270891  [  128/  146]
train() client id: f_00005-14-0 loss: 0.272557  [   32/  146]
train() client id: f_00005-14-1 loss: 0.415188  [   64/  146]
train() client id: f_00005-14-2 loss: -0.044648  [   96/  146]
train() client id: f_00005-14-3 loss: 0.425835  [  128/  146]
train() client id: f_00005-15-0 loss: 0.247284  [   32/  146]
train() client id: f_00005-15-1 loss: 0.213896  [   64/  146]
train() client id: f_00005-15-2 loss: 0.039789  [   96/  146]
train() client id: f_00005-15-3 loss: 0.494368  [  128/  146]
train() client id: f_00005-16-0 loss: 0.438945  [   32/  146]
train() client id: f_00005-16-1 loss: 0.085254  [   64/  146]
train() client id: f_00005-16-2 loss: 0.192244  [   96/  146]
train() client id: f_00005-16-3 loss: 0.255268  [  128/  146]
train() client id: f_00005-17-0 loss: 0.286892  [   32/  146]
train() client id: f_00005-17-1 loss: 0.380095  [   64/  146]
train() client id: f_00005-17-2 loss: 0.047160  [   96/  146]
train() client id: f_00005-17-3 loss: 0.248437  [  128/  146]
train() client id: f_00006-0-0 loss: 0.396719  [   32/   54]
train() client id: f_00006-1-0 loss: 0.402241  [   32/   54]
train() client id: f_00006-2-0 loss: 0.448645  [   32/   54]
train() client id: f_00006-3-0 loss: 0.456807  [   32/   54]
train() client id: f_00006-4-0 loss: 0.441521  [   32/   54]
train() client id: f_00006-5-0 loss: 0.452659  [   32/   54]
train() client id: f_00006-6-0 loss: 0.405056  [   32/   54]
train() client id: f_00006-7-0 loss: 0.390135  [   32/   54]
train() client id: f_00006-8-0 loss: 0.481117  [   32/   54]
train() client id: f_00006-9-0 loss: 0.499589  [   32/   54]
train() client id: f_00006-10-0 loss: 0.421976  [   32/   54]
train() client id: f_00006-11-0 loss: 0.421264  [   32/   54]
train() client id: f_00006-12-0 loss: 0.480176  [   32/   54]
train() client id: f_00006-13-0 loss: 0.483114  [   32/   54]
train() client id: f_00006-14-0 loss: 0.427635  [   32/   54]
train() client id: f_00006-15-0 loss: 0.426464  [   32/   54]
train() client id: f_00006-16-0 loss: 0.366752  [   32/   54]
train() client id: f_00006-17-0 loss: 0.362205  [   32/   54]
train() client id: f_00007-0-0 loss: 0.553442  [   32/  179]
train() client id: f_00007-0-1 loss: 0.395873  [   64/  179]
train() client id: f_00007-0-2 loss: 0.592018  [   96/  179]
train() client id: f_00007-0-3 loss: 0.425789  [  128/  179]
train() client id: f_00007-0-4 loss: 0.374631  [  160/  179]
train() client id: f_00007-1-0 loss: 0.409336  [   32/  179]
train() client id: f_00007-1-1 loss: 0.533388  [   64/  179]
train() client id: f_00007-1-2 loss: 0.258486  [   96/  179]
train() client id: f_00007-1-3 loss: 0.490458  [  128/  179]
train() client id: f_00007-1-4 loss: 0.548161  [  160/  179]
train() client id: f_00007-2-0 loss: 0.492565  [   32/  179]
train() client id: f_00007-2-1 loss: 0.381148  [   64/  179]
train() client id: f_00007-2-2 loss: 0.431946  [   96/  179]
train() client id: f_00007-2-3 loss: 0.408425  [  128/  179]
train() client id: f_00007-2-4 loss: 0.563247  [  160/  179]
train() client id: f_00007-3-0 loss: 0.326399  [   32/  179]
train() client id: f_00007-3-1 loss: 0.445690  [   64/  179]
train() client id: f_00007-3-2 loss: 0.351008  [   96/  179]
train() client id: f_00007-3-3 loss: 0.363072  [  128/  179]
train() client id: f_00007-3-4 loss: 0.577634  [  160/  179]
train() client id: f_00007-4-0 loss: 0.431231  [   32/  179]
train() client id: f_00007-4-1 loss: 0.515433  [   64/  179]
train() client id: f_00007-4-2 loss: 0.565626  [   96/  179]
train() client id: f_00007-4-3 loss: 0.262000  [  128/  179]
train() client id: f_00007-4-4 loss: 0.337021  [  160/  179]
train() client id: f_00007-5-0 loss: 0.710306  [   32/  179]
train() client id: f_00007-5-1 loss: 0.352142  [   64/  179]
train() client id: f_00007-5-2 loss: 0.296047  [   96/  179]
train() client id: f_00007-5-3 loss: 0.300997  [  128/  179]
train() client id: f_00007-5-4 loss: 0.344167  [  160/  179]
train() client id: f_00007-6-0 loss: 0.265112  [   32/  179]
train() client id: f_00007-6-1 loss: 0.339655  [   64/  179]
train() client id: f_00007-6-2 loss: 0.596056  [   96/  179]
train() client id: f_00007-6-3 loss: 0.301025  [  128/  179]
train() client id: f_00007-6-4 loss: 0.439111  [  160/  179]
train() client id: f_00007-7-0 loss: 0.283704  [   32/  179]
train() client id: f_00007-7-1 loss: 0.509789  [   64/  179]
train() client id: f_00007-7-2 loss: 0.247429  [   96/  179]
train() client id: f_00007-7-3 loss: 0.563951  [  128/  179]
train() client id: f_00007-7-4 loss: 0.463331  [  160/  179]
train() client id: f_00007-8-0 loss: 0.457106  [   32/  179]
train() client id: f_00007-8-1 loss: 0.290912  [   64/  179]
train() client id: f_00007-8-2 loss: 0.336111  [   96/  179]
train() client id: f_00007-8-3 loss: 0.526137  [  128/  179]
train() client id: f_00007-8-4 loss: 0.437743  [  160/  179]
train() client id: f_00007-9-0 loss: 0.363120  [   32/  179]
train() client id: f_00007-9-1 loss: 0.646458  [   64/  179]
train() client id: f_00007-9-2 loss: 0.441766  [   96/  179]
train() client id: f_00007-9-3 loss: 0.198115  [  128/  179]
train() client id: f_00007-9-4 loss: 0.356815  [  160/  179]
train() client id: f_00007-10-0 loss: 0.239390  [   32/  179]
train() client id: f_00007-10-1 loss: 0.699493  [   64/  179]
train() client id: f_00007-10-2 loss: 0.436137  [   96/  179]
train() client id: f_00007-10-3 loss: 0.266968  [  128/  179]
train() client id: f_00007-10-4 loss: 0.254347  [  160/  179]
train() client id: f_00007-11-0 loss: 0.411974  [   32/  179]
train() client id: f_00007-11-1 loss: 0.233096  [   64/  179]
train() client id: f_00007-11-2 loss: 0.342678  [   96/  179]
train() client id: f_00007-11-3 loss: 0.410752  [  128/  179]
train() client id: f_00007-11-4 loss: 0.400868  [  160/  179]
train() client id: f_00007-12-0 loss: 0.389572  [   32/  179]
train() client id: f_00007-12-1 loss: 0.258602  [   64/  179]
train() client id: f_00007-12-2 loss: 0.441438  [   96/  179]
train() client id: f_00007-12-3 loss: 0.433197  [  128/  179]
train() client id: f_00007-12-4 loss: 0.329574  [  160/  179]
train() client id: f_00007-13-0 loss: 0.400317  [   32/  179]
train() client id: f_00007-13-1 loss: 0.232101  [   64/  179]
train() client id: f_00007-13-2 loss: 0.369616  [   96/  179]
train() client id: f_00007-13-3 loss: 0.686878  [  128/  179]
train() client id: f_00007-13-4 loss: 0.206154  [  160/  179]
train() client id: f_00007-14-0 loss: 0.403021  [   32/  179]
train() client id: f_00007-14-1 loss: 0.316409  [   64/  179]
train() client id: f_00007-14-2 loss: 0.414525  [   96/  179]
train() client id: f_00007-14-3 loss: 0.368620  [  128/  179]
train() client id: f_00007-14-4 loss: 0.454487  [  160/  179]
train() client id: f_00007-15-0 loss: 0.234695  [   32/  179]
train() client id: f_00007-15-1 loss: 0.199999  [   64/  179]
train() client id: f_00007-15-2 loss: 0.582202  [   96/  179]
train() client id: f_00007-15-3 loss: 0.352878  [  128/  179]
train() client id: f_00007-15-4 loss: 0.584883  [  160/  179]
train() client id: f_00007-16-0 loss: 0.419680  [   32/  179]
train() client id: f_00007-16-1 loss: 0.307152  [   64/  179]
train() client id: f_00007-16-2 loss: 0.209486  [   96/  179]
train() client id: f_00007-16-3 loss: 0.436634  [  128/  179]
train() client id: f_00007-16-4 loss: 0.554207  [  160/  179]
train() client id: f_00007-17-0 loss: 0.510080  [   32/  179]
train() client id: f_00007-17-1 loss: 0.407036  [   64/  179]
train() client id: f_00007-17-2 loss: 0.337973  [   96/  179]
train() client id: f_00007-17-3 loss: 0.269458  [  128/  179]
train() client id: f_00007-17-4 loss: 0.393795  [  160/  179]
train() client id: f_00008-0-0 loss: 0.781366  [   32/  130]
train() client id: f_00008-0-1 loss: 0.702532  [   64/  130]
train() client id: f_00008-0-2 loss: 0.694718  [   96/  130]
train() client id: f_00008-0-3 loss: 0.807397  [  128/  130]
train() client id: f_00008-1-0 loss: 0.816107  [   32/  130]
train() client id: f_00008-1-1 loss: 0.720875  [   64/  130]
train() client id: f_00008-1-2 loss: 0.585672  [   96/  130]
train() client id: f_00008-1-3 loss: 0.869127  [  128/  130]
train() client id: f_00008-2-0 loss: 0.881343  [   32/  130]
train() client id: f_00008-2-1 loss: 0.683294  [   64/  130]
train() client id: f_00008-2-2 loss: 0.705296  [   96/  130]
train() client id: f_00008-2-3 loss: 0.682291  [  128/  130]
train() client id: f_00008-3-0 loss: 0.750081  [   32/  130]
train() client id: f_00008-3-1 loss: 0.778269  [   64/  130]
train() client id: f_00008-3-2 loss: 0.677504  [   96/  130]
train() client id: f_00008-3-3 loss: 0.765230  [  128/  130]
train() client id: f_00008-4-0 loss: 0.690516  [   32/  130]
train() client id: f_00008-4-1 loss: 0.859686  [   64/  130]
train() client id: f_00008-4-2 loss: 0.748333  [   96/  130]
train() client id: f_00008-4-3 loss: 0.674586  [  128/  130]
train() client id: f_00008-5-0 loss: 0.669364  [   32/  130]
train() client id: f_00008-5-1 loss: 0.697784  [   64/  130]
train() client id: f_00008-5-2 loss: 0.767603  [   96/  130]
train() client id: f_00008-5-3 loss: 0.806901  [  128/  130]
train() client id: f_00008-6-0 loss: 0.622834  [   32/  130]
train() client id: f_00008-6-1 loss: 0.819812  [   64/  130]
train() client id: f_00008-6-2 loss: 0.767502  [   96/  130]
train() client id: f_00008-6-3 loss: 0.769581  [  128/  130]
train() client id: f_00008-7-0 loss: 0.727429  [   32/  130]
train() client id: f_00008-7-1 loss: 0.692438  [   64/  130]
train() client id: f_00008-7-2 loss: 0.718750  [   96/  130]
train() client id: f_00008-7-3 loss: 0.788719  [  128/  130]
train() client id: f_00008-8-0 loss: 0.739860  [   32/  130]
train() client id: f_00008-8-1 loss: 0.712301  [   64/  130]
train() client id: f_00008-8-2 loss: 0.787391  [   96/  130]
train() client id: f_00008-8-3 loss: 0.701403  [  128/  130]
train() client id: f_00008-9-0 loss: 0.791033  [   32/  130]
train() client id: f_00008-9-1 loss: 0.814282  [   64/  130]
train() client id: f_00008-9-2 loss: 0.673560  [   96/  130]
train() client id: f_00008-9-3 loss: 0.684358  [  128/  130]
train() client id: f_00008-10-0 loss: 0.771199  [   32/  130]
train() client id: f_00008-10-1 loss: 0.744556  [   64/  130]
train() client id: f_00008-10-2 loss: 0.650716  [   96/  130]
train() client id: f_00008-10-3 loss: 0.785818  [  128/  130]
train() client id: f_00008-11-0 loss: 0.824032  [   32/  130]
train() client id: f_00008-11-1 loss: 0.665635  [   64/  130]
train() client id: f_00008-11-2 loss: 0.772355  [   96/  130]
train() client id: f_00008-11-3 loss: 0.702055  [  128/  130]
train() client id: f_00008-12-0 loss: 0.880112  [   32/  130]
train() client id: f_00008-12-1 loss: 0.636135  [   64/  130]
train() client id: f_00008-12-2 loss: 0.678387  [   96/  130]
train() client id: f_00008-12-3 loss: 0.723000  [  128/  130]
train() client id: f_00008-13-0 loss: 0.779221  [   32/  130]
train() client id: f_00008-13-1 loss: 0.699059  [   64/  130]
train() client id: f_00008-13-2 loss: 0.690099  [   96/  130]
train() client id: f_00008-13-3 loss: 0.778373  [  128/  130]
train() client id: f_00008-14-0 loss: 0.745220  [   32/  130]
train() client id: f_00008-14-1 loss: 0.698423  [   64/  130]
train() client id: f_00008-14-2 loss: 0.691088  [   96/  130]
train() client id: f_00008-14-3 loss: 0.791411  [  128/  130]
train() client id: f_00008-15-0 loss: 0.719668  [   32/  130]
train() client id: f_00008-15-1 loss: 0.712691  [   64/  130]
train() client id: f_00008-15-2 loss: 0.736048  [   96/  130]
train() client id: f_00008-15-3 loss: 0.789891  [  128/  130]
train() client id: f_00008-16-0 loss: 0.601994  [   32/  130]
train() client id: f_00008-16-1 loss: 0.779740  [   64/  130]
train() client id: f_00008-16-2 loss: 0.774158  [   96/  130]
train() client id: f_00008-16-3 loss: 0.790802  [  128/  130]
train() client id: f_00008-17-0 loss: 0.729026  [   32/  130]
train() client id: f_00008-17-1 loss: 0.628053  [   64/  130]
train() client id: f_00008-17-2 loss: 0.765621  [   96/  130]
train() client id: f_00008-17-3 loss: 0.822210  [  128/  130]
train() client id: f_00009-0-0 loss: 1.197017  [   32/  118]
train() client id: f_00009-0-1 loss: 1.197963  [   64/  118]
train() client id: f_00009-0-2 loss: 1.194067  [   96/  118]
train() client id: f_00009-1-0 loss: 1.216444  [   32/  118]
train() client id: f_00009-1-1 loss: 1.199382  [   64/  118]
train() client id: f_00009-1-2 loss: 1.017416  [   96/  118]
train() client id: f_00009-2-0 loss: 0.980175  [   32/  118]
train() client id: f_00009-2-1 loss: 1.184705  [   64/  118]
train() client id: f_00009-2-2 loss: 1.091736  [   96/  118]
train() client id: f_00009-3-0 loss: 1.052201  [   32/  118]
train() client id: f_00009-3-1 loss: 1.055124  [   64/  118]
train() client id: f_00009-3-2 loss: 1.073641  [   96/  118]
train() client id: f_00009-4-0 loss: 1.020144  [   32/  118]
train() client id: f_00009-4-1 loss: 0.999002  [   64/  118]
train() client id: f_00009-4-2 loss: 1.013629  [   96/  118]
train() client id: f_00009-5-0 loss: 1.013371  [   32/  118]
train() client id: f_00009-5-1 loss: 0.790812  [   64/  118]
train() client id: f_00009-5-2 loss: 1.079244  [   96/  118]
train() client id: f_00009-6-0 loss: 1.055713  [   32/  118]
train() client id: f_00009-6-1 loss: 0.971964  [   64/  118]
train() client id: f_00009-6-2 loss: 0.867905  [   96/  118]
train() client id: f_00009-7-0 loss: 1.020939  [   32/  118]
train() client id: f_00009-7-1 loss: 0.883757  [   64/  118]
train() client id: f_00009-7-2 loss: 0.875703  [   96/  118]
train() client id: f_00009-8-0 loss: 0.979935  [   32/  118]
train() client id: f_00009-8-1 loss: 0.792648  [   64/  118]
train() client id: f_00009-8-2 loss: 1.079127  [   96/  118]
train() client id: f_00009-9-0 loss: 0.849513  [   32/  118]
train() client id: f_00009-9-1 loss: 0.920613  [   64/  118]
train() client id: f_00009-9-2 loss: 0.896911  [   96/  118]
train() client id: f_00009-10-0 loss: 0.760774  [   32/  118]
train() client id: f_00009-10-1 loss: 1.035332  [   64/  118]
train() client id: f_00009-10-2 loss: 0.945005  [   96/  118]
train() client id: f_00009-11-0 loss: 0.897460  [   32/  118]
train() client id: f_00009-11-1 loss: 0.954714  [   64/  118]
train() client id: f_00009-11-2 loss: 0.967276  [   96/  118]
train() client id: f_00009-12-0 loss: 0.998324  [   32/  118]
train() client id: f_00009-12-1 loss: 0.927220  [   64/  118]
train() client id: f_00009-12-2 loss: 0.815331  [   96/  118]
train() client id: f_00009-13-0 loss: 1.098026  [   32/  118]
train() client id: f_00009-13-1 loss: 0.821948  [   64/  118]
train() client id: f_00009-13-2 loss: 0.878331  [   96/  118]
train() client id: f_00009-14-0 loss: 1.026966  [   32/  118]
train() client id: f_00009-14-1 loss: 0.792251  [   64/  118]
train() client id: f_00009-14-2 loss: 0.920092  [   96/  118]
train() client id: f_00009-15-0 loss: 0.928086  [   32/  118]
train() client id: f_00009-15-1 loss: 0.930736  [   64/  118]
train() client id: f_00009-15-2 loss: 0.826920  [   96/  118]
train() client id: f_00009-16-0 loss: 0.883553  [   32/  118]
train() client id: f_00009-16-1 loss: 0.855657  [   64/  118]
train() client id: f_00009-16-2 loss: 0.950326  [   96/  118]
train() client id: f_00009-17-0 loss: 1.013342  [   32/  118]
train() client id: f_00009-17-1 loss: 0.844731  [   64/  118]
train() client id: f_00009-17-2 loss: 0.852349  [   96/  118]
At round 65 accuracy: 0.6472148541114059
At round 65 training accuracy: 0.596244131455399
At round 65 training loss: 0.8260071809142293
update_location
xs = 8.927491 446.223621 5.882650 0.934260 -362.581990 -210.230757 -170.849135 -5.143845 -385.120581 20.134486 
ys = -437.390647 7.291448 335.684448 -157.290817 -9.642386 0.794442 -1.381692 331.628436 25.881276 -872.232496 
xs mean: -65.18237997052123
ys mean: -77.66579882624052
dists_uav = 448.765282 457.349631 350.312224 186.390113 376.242840 232.803785 197.968018 346.415760 398.732620 878.177046 
uav_gains = -122.966850 -123.205764 -119.189303 -106.830130 -120.453488 -109.913275 -107.554233 -118.973083 -121.356654 -130.562809 
uav_gains_db_mean: -118.10055885968829
dists_bs = 639.415066 643.463243 241.946327 375.992089 263.210588 177.732334 176.430528 231.048870 257.656850 1065.257419 
bs_gains = -118.128979 -118.205724 -106.311192 -111.672063 -107.335553 -102.560520 -102.471124 -105.750767 -107.076227 -124.335787 
bs_gains_db_mean: -110.38479352475693
Round 66
-------------------------------
ene_coms = [0.04264995 0.04429503 0.00852432 0.00877645 0.00903617 0.01062572
 0.00915316 0.0082673  0.00890109 0.05333223]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.37993642 4.74975    2.15381417 0.77966136 5.24748884 2.53925916
 0.96858874 3.08639763 2.25782998 2.30422813]
obj_prev = 26.466954432293278
eta_min = 2.2930367776230523e-41	eta_max = 0.8879111462045848
af = 5.443341958957153	bf = 1.134768786442856	zeta = 5.987676154852869	eta = 0.9090909090909091
af = 5.443341958957153	bf = 1.134768786442856	zeta = 18.18932156016237	eta = 0.29926030726066083
af = 5.443341958957153	bf = 1.134768786442856	zeta = 10.991017717296868	eta = 0.49525367886458915
af = 5.443341958957153	bf = 1.134768786442856	zeta = 9.826172518883583	eta = 0.5539636057169092
af = 5.443341958957153	bf = 1.134768786442856	zeta = 9.752372223951095	eta = 0.5581556808905133
af = 5.443341958957153	bf = 1.134768786442856	zeta = 9.75202300713804	eta = 0.5581756682662533
af = 5.443341958957153	bf = 1.134768786442856	zeta = 9.75202299923915	eta = 0.5581756687183614
eta = 0.5581756687183614
ene_coms = [0.04264995 0.04429503 0.00852432 0.00877645 0.00903617 0.01062572
 0.00915316 0.0082673  0.00890109 0.05333223]
ene_comp = [0.04811449 0.10119318 0.04735077 0.01642002 0.11684948 0.05575171
 0.0206205  0.06835313 0.04964194 0.0450596 ]
ene_total = [1.14519124 1.83565078 0.70498608 0.31790841 1.58832184 0.83749595
 0.37565961 0.96673372 0.73864791 1.24142745]
ti_comp = [1.68476798 1.66831721 2.02602423 2.02350299 2.02090577 2.00501029
 2.01973583 2.02859444 2.02225654 1.57794515]
ti_coms = [0.42649948 0.44295025 0.08524323 0.08776447 0.09036169 0.10625717
 0.09153163 0.08267302 0.08901092 0.53332231]
t_total = [26.63824081 26.63824081 26.63824081 26.63824081 26.63824081 26.63824081
 26.63824081 26.63824081 26.63824081 26.63824081]
ene_coms = [0.04264995 0.04429503 0.00852432 0.00877645 0.00903617 0.01062572
 0.00915316 0.0082673  0.00890109 0.05333223]
ene_comp = [2.45260452e-06 2.32689321e-05 1.61648572e-06 6.75761980e-08
 2.44156472e-05 2.69414797e-06 1.34334706e-07 4.85025668e-06
 1.86962132e-06 2.29645695e-06]
ene_total = [0.538153   0.55917188 0.10757331 0.11073486 0.11431902 0.13410057
 0.11548879 0.10437124 0.11233027 0.67293132]
optimize_network iter = 0 obj = 2.569174256473425
eta = 0.5581756687183614
freqs = [14279263.46670403 30327918.62929681 11685637.52987511  4057326.57901788
 28910175.21793158 13903098.86627749  5104750.86244017 16847410.68731777
 12273896.76952745 14277937.19300261]
eta_min = 0.5581756687183649	eta_max = 0.5745618121086892
af = 0.0006086822176830232	bf = 1.134768786442856	zeta = 0.0006695504394513256	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.04264995 0.04429503 0.00852432 0.00877645 0.00903617 0.01062572
 0.00915316 0.0082673  0.00890109 0.05333223]
ene_comp = [4.00901347e-07 3.80352647e-06 2.64229842e-07 1.10459671e-08
 3.99096787e-06 4.40383902e-07 2.19582750e-08 7.92820212e-07
 3.05607242e-07 3.75377553e-07]
ene_total = [2.61533417 2.71641953 0.52273062 0.53817543 0.55434573 0.65159964
 0.56127646 0.50700247 0.54583681 3.27037393]
ti_comp = [1.60646643 1.59001566 1.94772268 1.94520144 1.94260422 1.92670874
 1.94143428 1.95029288 1.94395498 1.49964359]
ti_coms = [0.42649948 0.44295025 0.08524323 0.08776447 0.09036169 0.10625717
 0.09153163 0.08267302 0.08901092 0.53332231]
t_total = [26.63824081 26.63824081 26.63824081 26.63824081 26.63824081 26.63824081
 26.63824081 26.63824081 26.63824081 26.63824081]
ene_coms = [0.04264995 0.04429503 0.00852432 0.00877645 0.00903617 0.01062572
 0.00915316 0.0082673  0.00890109 0.05333223]
ene_comp = [2.31554316e-06 2.19897035e-05 1.50139638e-06 6.27712468e-08
 2.26819390e-05 2.50444204e-06 1.24801684e-07 4.50447286e-06
 1.73676958e-06 2.18250106e-06]
ene_total = [0.55887866 0.58069214 0.11171508 0.11499985 0.11869941 0.13926309
 0.11993683 0.10838666 0.11665503 0.6988484 ]
optimize_network iter = 1 obj = 2.6680751463616907
eta = 0.5745618121086892
freqs = [14232146.56502149 30242381.65714838 11552236.72810741  4011209.90738082
 28583053.03533312 13750177.50764632  5047107.82094586 16654235.87789492
 12134689.41387316 14277937.19300261]
eta_min = 0.5745618121086914	eta_max = 0.5745618121086891
af = 0.0005996037632245018	bf = 1.134768786442856	zeta = 0.0006595641395469521	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.04264995 0.04429503 0.00852432 0.00877645 0.00903617 0.01062572
 0.00915316 0.0082673  0.00890109 0.05333223]
ene_comp = [3.98260026e-07 3.78210177e-06 2.58231490e-07 1.07962912e-08
 3.90116226e-06 4.30749540e-07 2.14651675e-08 7.74743267e-07
 2.98714318e-07 3.75377553e-07]
ene_total = [2.61533401 2.71641822 0.52273025 0.53817541 0.55434022 0.65159905
 0.56127643 0.50700136 0.54583639 3.27037393]
ti_comp = [1.60646643 1.59001566 1.94772268 1.94520144 1.94260422 1.92670874
 1.94143428 1.95029288 1.94395498 1.49964359]
ti_coms = [0.42649948 0.44295025 0.08524323 0.08776447 0.09036169 0.10625717
 0.09153163 0.08267302 0.08901092 0.53332231]
t_total = [26.63824081 26.63824081 26.63824081 26.63824081 26.63824081 26.63824081
 26.63824081 26.63824081 26.63824081 26.63824081]
ene_coms = [0.04264995 0.04429503 0.00852432 0.00877645 0.00903617 0.01062572
 0.00915316 0.0082673  0.00890109 0.05333223]
ene_comp = [2.31554316e-06 2.19897035e-05 1.50139638e-06 6.27712468e-08
 2.26819390e-05 2.50444204e-06 1.24801684e-07 4.50447286e-06
 1.73676958e-06 2.18250106e-06]
ene_total = [0.55887866 0.58069214 0.11171508 0.11499985 0.11869941 0.13926309
 0.11993683 0.10838666 0.11665503 0.6988484 ]
optimize_network iter = 2 obj = 2.66807514636169
eta = 0.5745618121086891
freqs = [14232146.56502149 30242381.65714838 11552236.72810741  4011209.90738082
 28583053.03533312 13750177.50764632  5047107.82094586 16654235.87789492
 12134689.41387316 14277937.19300261]
Done!
ene_coms = [0.04264995 0.04429503 0.00852432 0.00877645 0.00903617 0.01062572
 0.00915316 0.0082673  0.00890109 0.05333223]
ene_comp = [2.29696231e-06 2.18132493e-05 1.48934858e-06 6.22675450e-08
 2.24999300e-05 2.48434539e-06 1.23800225e-07 4.46832716e-06
 1.72283304e-06 2.16498779e-06]
ene_total = [0.04265224 0.04431684 0.00852581 0.00877651 0.00905867 0.0106282
 0.00915329 0.00827177 0.00890282 0.0533344 ]
At round 66 energy consumption: 0.20362054341852215
At round 66 eta: 0.5745618121086891
At round 66 a_n: 5.574576956279433
At round 66 local rounds: 18.14560765469979
At round 66 global rounds: 13.103141925058221
gradient difference: 0.3015972375869751
train() client id: f_00000-0-0 loss: 1.399712  [   32/  126]
train() client id: f_00000-0-1 loss: 0.929981  [   64/  126]
train() client id: f_00000-0-2 loss: 1.065820  [   96/  126]
train() client id: f_00000-1-0 loss: 1.168551  [   32/  126]
train() client id: f_00000-1-1 loss: 0.918817  [   64/  126]
train() client id: f_00000-1-2 loss: 1.216635  [   96/  126]
train() client id: f_00000-2-0 loss: 1.050239  [   32/  126]
train() client id: f_00000-2-1 loss: 1.056742  [   64/  126]
train() client id: f_00000-2-2 loss: 1.084382  [   96/  126]
train() client id: f_00000-3-0 loss: 1.124877  [   32/  126]
train() client id: f_00000-3-1 loss: 0.873543  [   64/  126]
train() client id: f_00000-3-2 loss: 0.867271  [   96/  126]
train() client id: f_00000-4-0 loss: 0.954698  [   32/  126]
train() client id: f_00000-4-1 loss: 0.954928  [   64/  126]
train() client id: f_00000-4-2 loss: 0.898653  [   96/  126]
train() client id: f_00000-5-0 loss: 0.950915  [   32/  126]
train() client id: f_00000-5-1 loss: 0.790367  [   64/  126]
train() client id: f_00000-5-2 loss: 0.943451  [   96/  126]
train() client id: f_00000-6-0 loss: 0.763731  [   32/  126]
train() client id: f_00000-6-1 loss: 0.808857  [   64/  126]
train() client id: f_00000-6-2 loss: 0.819261  [   96/  126]
train() client id: f_00000-7-0 loss: 0.761905  [   32/  126]
train() client id: f_00000-7-1 loss: 0.943089  [   64/  126]
train() client id: f_00000-7-2 loss: 0.870591  [   96/  126]
train() client id: f_00000-8-0 loss: 0.853668  [   32/  126]
train() client id: f_00000-8-1 loss: 0.871269  [   64/  126]
train() client id: f_00000-8-2 loss: 0.743313  [   96/  126]
train() client id: f_00000-9-0 loss: 0.839083  [   32/  126]
train() client id: f_00000-9-1 loss: 0.822500  [   64/  126]
train() client id: f_00000-9-2 loss: 0.832002  [   96/  126]
train() client id: f_00000-10-0 loss: 0.803129  [   32/  126]
train() client id: f_00000-10-1 loss: 0.813976  [   64/  126]
train() client id: f_00000-10-2 loss: 0.724958  [   96/  126]
train() client id: f_00000-11-0 loss: 0.909928  [   32/  126]
train() client id: f_00000-11-1 loss: 0.808394  [   64/  126]
train() client id: f_00000-11-2 loss: 0.731963  [   96/  126]
train() client id: f_00000-12-0 loss: 0.731726  [   32/  126]
train() client id: f_00000-12-1 loss: 0.895456  [   64/  126]
train() client id: f_00000-12-2 loss: 0.814704  [   96/  126]
train() client id: f_00000-13-0 loss: 0.989318  [   32/  126]
train() client id: f_00000-13-1 loss: 0.783152  [   64/  126]
train() client id: f_00000-13-2 loss: 0.757589  [   96/  126]
train() client id: f_00000-14-0 loss: 0.995553  [   32/  126]
train() client id: f_00000-14-1 loss: 0.701902  [   64/  126]
train() client id: f_00000-14-2 loss: 0.691616  [   96/  126]
train() client id: f_00000-15-0 loss: 0.858307  [   32/  126]
train() client id: f_00000-15-1 loss: 0.784255  [   64/  126]
train() client id: f_00000-15-2 loss: 0.672961  [   96/  126]
train() client id: f_00000-16-0 loss: 0.720724  [   32/  126]
train() client id: f_00000-16-1 loss: 0.865302  [   64/  126]
train() client id: f_00000-16-2 loss: 0.789159  [   96/  126]
train() client id: f_00000-17-0 loss: 0.667925  [   32/  126]
train() client id: f_00000-17-1 loss: 0.792265  [   64/  126]
train() client id: f_00000-17-2 loss: 0.789877  [   96/  126]
train() client id: f_00001-0-0 loss: 0.272420  [   32/  265]
train() client id: f_00001-0-1 loss: 0.363055  [   64/  265]
train() client id: f_00001-0-2 loss: 0.293344  [   96/  265]
train() client id: f_00001-0-3 loss: 0.361845  [  128/  265]
train() client id: f_00001-0-4 loss: 0.298028  [  160/  265]
train() client id: f_00001-0-5 loss: 0.256300  [  192/  265]
train() client id: f_00001-0-6 loss: 0.291391  [  224/  265]
train() client id: f_00001-0-7 loss: 0.361775  [  256/  265]
train() client id: f_00001-1-0 loss: 0.232512  [   32/  265]
train() client id: f_00001-1-1 loss: 0.274683  [   64/  265]
train() client id: f_00001-1-2 loss: 0.305319  [   96/  265]
train() client id: f_00001-1-3 loss: 0.234408  [  128/  265]
train() client id: f_00001-1-4 loss: 0.298233  [  160/  265]
train() client id: f_00001-1-5 loss: 0.343477  [  192/  265]
train() client id: f_00001-1-6 loss: 0.332962  [  224/  265]
train() client id: f_00001-1-7 loss: 0.299414  [  256/  265]
train() client id: f_00001-2-0 loss: 0.289605  [   32/  265]
train() client id: f_00001-2-1 loss: 0.295169  [   64/  265]
train() client id: f_00001-2-2 loss: 0.412422  [   96/  265]
train() client id: f_00001-2-3 loss: 0.250126  [  128/  265]
train() client id: f_00001-2-4 loss: 0.255832  [  160/  265]
train() client id: f_00001-2-5 loss: 0.347576  [  192/  265]
train() client id: f_00001-2-6 loss: 0.248433  [  224/  265]
train() client id: f_00001-2-7 loss: 0.271484  [  256/  265]
train() client id: f_00001-3-0 loss: 0.395796  [   32/  265]
train() client id: f_00001-3-1 loss: 0.244055  [   64/  265]
train() client id: f_00001-3-2 loss: 0.332926  [   96/  265]
train() client id: f_00001-3-3 loss: 0.188120  [  128/  265]
train() client id: f_00001-3-4 loss: 0.264441  [  160/  265]
train() client id: f_00001-3-5 loss: 0.279668  [  192/  265]
train() client id: f_00001-3-6 loss: 0.261156  [  224/  265]
train() client id: f_00001-3-7 loss: 0.355816  [  256/  265]
train() client id: f_00001-4-0 loss: 0.327484  [   32/  265]
train() client id: f_00001-4-1 loss: 0.199528  [   64/  265]
train() client id: f_00001-4-2 loss: 0.353245  [   96/  265]
train() client id: f_00001-4-3 loss: 0.262483  [  128/  265]
train() client id: f_00001-4-4 loss: 0.208966  [  160/  265]
train() client id: f_00001-4-5 loss: 0.179410  [  192/  265]
train() client id: f_00001-4-6 loss: 0.346225  [  224/  265]
train() client id: f_00001-4-7 loss: 0.421196  [  256/  265]
train() client id: f_00001-5-0 loss: 0.227969  [   32/  265]
train() client id: f_00001-5-1 loss: 0.224128  [   64/  265]
train() client id: f_00001-5-2 loss: 0.254015  [   96/  265]
train() client id: f_00001-5-3 loss: 0.313373  [  128/  265]
train() client id: f_00001-5-4 loss: 0.199414  [  160/  265]
train() client id: f_00001-5-5 loss: 0.242586  [  192/  265]
train() client id: f_00001-5-6 loss: 0.443048  [  224/  265]
train() client id: f_00001-5-7 loss: 0.345077  [  256/  265]
train() client id: f_00001-6-0 loss: 0.271977  [   32/  265]
train() client id: f_00001-6-1 loss: 0.413971  [   64/  265]
train() client id: f_00001-6-2 loss: 0.212759  [   96/  265]
train() client id: f_00001-6-3 loss: 0.291500  [  128/  265]
train() client id: f_00001-6-4 loss: 0.235381  [  160/  265]
train() client id: f_00001-6-5 loss: 0.323462  [  192/  265]
train() client id: f_00001-6-6 loss: 0.208135  [  224/  265]
train() client id: f_00001-6-7 loss: 0.273610  [  256/  265]
train() client id: f_00001-7-0 loss: 0.278718  [   32/  265]
train() client id: f_00001-7-1 loss: 0.298117  [   64/  265]
train() client id: f_00001-7-2 loss: 0.305708  [   96/  265]
train() client id: f_00001-7-3 loss: 0.459829  [  128/  265]
train() client id: f_00001-7-4 loss: 0.266047  [  160/  265]
train() client id: f_00001-7-5 loss: 0.197152  [  192/  265]
train() client id: f_00001-7-6 loss: 0.163147  [  224/  265]
train() client id: f_00001-7-7 loss: 0.252501  [  256/  265]
train() client id: f_00001-8-0 loss: 0.153606  [   32/  265]
train() client id: f_00001-8-1 loss: 0.177784  [   64/  265]
train() client id: f_00001-8-2 loss: 0.301076  [   96/  265]
train() client id: f_00001-8-3 loss: 0.482611  [  128/  265]
train() client id: f_00001-8-4 loss: 0.241648  [  160/  265]
train() client id: f_00001-8-5 loss: 0.175912  [  192/  265]
train() client id: f_00001-8-6 loss: 0.301776  [  224/  265]
train() client id: f_00001-8-7 loss: 0.351089  [  256/  265]
train() client id: f_00001-9-0 loss: 0.306366  [   32/  265]
train() client id: f_00001-9-1 loss: 0.171078  [   64/  265]
train() client id: f_00001-9-2 loss: 0.176613  [   96/  265]
train() client id: f_00001-9-3 loss: 0.377759  [  128/  265]
train() client id: f_00001-9-4 loss: 0.377771  [  160/  265]
train() client id: f_00001-9-5 loss: 0.256852  [  192/  265]
train() client id: f_00001-9-6 loss: 0.210494  [  224/  265]
train() client id: f_00001-9-7 loss: 0.300401  [  256/  265]
train() client id: f_00001-10-0 loss: 0.193888  [   32/  265]
train() client id: f_00001-10-1 loss: 0.281382  [   64/  265]
train() client id: f_00001-10-2 loss: 0.234851  [   96/  265]
train() client id: f_00001-10-3 loss: 0.209912  [  128/  265]
train() client id: f_00001-10-4 loss: 0.239239  [  160/  265]
train() client id: f_00001-10-5 loss: 0.256046  [  192/  265]
train() client id: f_00001-10-6 loss: 0.303770  [  224/  265]
train() client id: f_00001-10-7 loss: 0.458355  [  256/  265]
train() client id: f_00001-11-0 loss: 0.164249  [   32/  265]
train() client id: f_00001-11-1 loss: 0.274021  [   64/  265]
train() client id: f_00001-11-2 loss: 0.263578  [   96/  265]
train() client id: f_00001-11-3 loss: 0.251027  [  128/  265]
train() client id: f_00001-11-4 loss: 0.179256  [  160/  265]
train() client id: f_00001-11-5 loss: 0.249047  [  192/  265]
train() client id: f_00001-11-6 loss: 0.517790  [  224/  265]
train() client id: f_00001-11-7 loss: 0.269167  [  256/  265]
train() client id: f_00001-12-0 loss: 0.285875  [   32/  265]
train() client id: f_00001-12-1 loss: 0.272396  [   64/  265]
train() client id: f_00001-12-2 loss: 0.176087  [   96/  265]
train() client id: f_00001-12-3 loss: 0.413442  [  128/  265]
train() client id: f_00001-12-4 loss: 0.193625  [  160/  265]
train() client id: f_00001-12-5 loss: 0.286298  [  192/  265]
train() client id: f_00001-12-6 loss: 0.219136  [  224/  265]
train() client id: f_00001-12-7 loss: 0.311244  [  256/  265]
train() client id: f_00001-13-0 loss: 0.251791  [   32/  265]
train() client id: f_00001-13-1 loss: 0.232627  [   64/  265]
train() client id: f_00001-13-2 loss: 0.356980  [   96/  265]
train() client id: f_00001-13-3 loss: 0.350563  [  128/  265]
train() client id: f_00001-13-4 loss: 0.177622  [  160/  265]
train() client id: f_00001-13-5 loss: 0.434676  [  192/  265]
train() client id: f_00001-13-6 loss: 0.184676  [  224/  265]
train() client id: f_00001-13-7 loss: 0.164819  [  256/  265]
train() client id: f_00001-14-0 loss: 0.296886  [   32/  265]
train() client id: f_00001-14-1 loss: 0.245500  [   64/  265]
train() client id: f_00001-14-2 loss: 0.253264  [   96/  265]
train() client id: f_00001-14-3 loss: 0.233207  [  128/  265]
train() client id: f_00001-14-4 loss: 0.190594  [  160/  265]
train() client id: f_00001-14-5 loss: 0.396399  [  192/  265]
train() client id: f_00001-14-6 loss: 0.259058  [  224/  265]
train() client id: f_00001-14-7 loss: 0.292745  [  256/  265]
train() client id: f_00001-15-0 loss: 0.250679  [   32/  265]
train() client id: f_00001-15-1 loss: 0.315482  [   64/  265]
train() client id: f_00001-15-2 loss: 0.256305  [   96/  265]
train() client id: f_00001-15-3 loss: 0.278557  [  128/  265]
train() client id: f_00001-15-4 loss: 0.284699  [  160/  265]
train() client id: f_00001-15-5 loss: 0.217019  [  192/  265]
train() client id: f_00001-15-6 loss: 0.232276  [  224/  265]
train() client id: f_00001-15-7 loss: 0.235802  [  256/  265]
train() client id: f_00001-16-0 loss: 0.214467  [   32/  265]
train() client id: f_00001-16-1 loss: 0.172113  [   64/  265]
train() client id: f_00001-16-2 loss: 0.335267  [   96/  265]
train() client id: f_00001-16-3 loss: 0.214002  [  128/  265]
train() client id: f_00001-16-4 loss: 0.314463  [  160/  265]
train() client id: f_00001-16-5 loss: 0.168577  [  192/  265]
train() client id: f_00001-16-6 loss: 0.346587  [  224/  265]
train() client id: f_00001-16-7 loss: 0.389925  [  256/  265]
train() client id: f_00001-17-0 loss: 0.173425  [   32/  265]
train() client id: f_00001-17-1 loss: 0.174220  [   64/  265]
train() client id: f_00001-17-2 loss: 0.297669  [   96/  265]
train() client id: f_00001-17-3 loss: 0.325241  [  128/  265]
train() client id: f_00001-17-4 loss: 0.218895  [  160/  265]
train() client id: f_00001-17-5 loss: 0.219319  [  192/  265]
train() client id: f_00001-17-6 loss: 0.271242  [  224/  265]
train() client id: f_00001-17-7 loss: 0.416869  [  256/  265]
train() client id: f_00002-0-0 loss: 1.538491  [   32/  124]
train() client id: f_00002-0-1 loss: 1.437605  [   64/  124]
train() client id: f_00002-0-2 loss: 1.165342  [   96/  124]
train() client id: f_00002-1-0 loss: 1.364780  [   32/  124]
train() client id: f_00002-1-1 loss: 1.624413  [   64/  124]
train() client id: f_00002-1-2 loss: 1.073192  [   96/  124]
train() client id: f_00002-2-0 loss: 1.402658  [   32/  124]
train() client id: f_00002-2-1 loss: 1.232556  [   64/  124]
train() client id: f_00002-2-2 loss: 1.440531  [   96/  124]
train() client id: f_00002-3-0 loss: 1.323456  [   32/  124]
train() client id: f_00002-3-1 loss: 1.358994  [   64/  124]
train() client id: f_00002-3-2 loss: 1.211376  [   96/  124]
train() client id: f_00002-4-0 loss: 1.237718  [   32/  124]
train() client id: f_00002-4-1 loss: 1.249041  [   64/  124]
train() client id: f_00002-4-2 loss: 1.177810  [   96/  124]
train() client id: f_00002-5-0 loss: 1.091281  [   32/  124]
train() client id: f_00002-5-1 loss: 1.153526  [   64/  124]
train() client id: f_00002-5-2 loss: 1.270712  [   96/  124]
train() client id: f_00002-6-0 loss: 1.122607  [   32/  124]
train() client id: f_00002-6-1 loss: 1.149181  [   64/  124]
train() client id: f_00002-6-2 loss: 1.241953  [   96/  124]
train() client id: f_00002-7-0 loss: 1.142067  [   32/  124]
train() client id: f_00002-7-1 loss: 1.088578  [   64/  124]
train() client id: f_00002-7-2 loss: 1.064992  [   96/  124]
train() client id: f_00002-8-0 loss: 0.948045  [   32/  124]
train() client id: f_00002-8-1 loss: 1.193641  [   64/  124]
train() client id: f_00002-8-2 loss: 1.023257  [   96/  124]
train() client id: f_00002-9-0 loss: 0.837533  [   32/  124]
train() client id: f_00002-9-1 loss: 0.994976  [   64/  124]
train() client id: f_00002-9-2 loss: 1.088582  [   96/  124]
train() client id: f_00002-10-0 loss: 1.228504  [   32/  124]
train() client id: f_00002-10-1 loss: 1.068617  [   64/  124]
train() client id: f_00002-10-2 loss: 0.903956  [   96/  124]
train() client id: f_00002-11-0 loss: 1.087618  [   32/  124]
train() client id: f_00002-11-1 loss: 1.038964  [   64/  124]
train() client id: f_00002-11-2 loss: 1.021692  [   96/  124]
train() client id: f_00002-12-0 loss: 0.962713  [   32/  124]
train() client id: f_00002-12-1 loss: 1.113699  [   64/  124]
train() client id: f_00002-12-2 loss: 0.926417  [   96/  124]
train() client id: f_00002-13-0 loss: 0.839483  [   32/  124]
train() client id: f_00002-13-1 loss: 1.048889  [   64/  124]
train() client id: f_00002-13-2 loss: 0.924620  [   96/  124]
train() client id: f_00002-14-0 loss: 0.991786  [   32/  124]
train() client id: f_00002-14-1 loss: 0.895892  [   64/  124]
train() client id: f_00002-14-2 loss: 1.048027  [   96/  124]
train() client id: f_00002-15-0 loss: 0.913408  [   32/  124]
train() client id: f_00002-15-1 loss: 0.992581  [   64/  124]
train() client id: f_00002-15-2 loss: 0.887535  [   96/  124]
train() client id: f_00002-16-0 loss: 1.057330  [   32/  124]
train() client id: f_00002-16-1 loss: 0.827220  [   64/  124]
train() client id: f_00002-16-2 loss: 0.906203  [   96/  124]
train() client id: f_00002-17-0 loss: 0.893843  [   32/  124]
train() client id: f_00002-17-1 loss: 1.057862  [   64/  124]
train() client id: f_00002-17-2 loss: 0.890917  [   96/  124]
train() client id: f_00003-0-0 loss: 0.746450  [   32/   43]
train() client id: f_00003-1-0 loss: 0.878940  [   32/   43]
train() client id: f_00003-2-0 loss: 0.646079  [   32/   43]
train() client id: f_00003-3-0 loss: 0.888440  [   32/   43]
train() client id: f_00003-4-0 loss: 0.543091  [   32/   43]
train() client id: f_00003-5-0 loss: 0.741979  [   32/   43]
train() client id: f_00003-6-0 loss: 0.739901  [   32/   43]
train() client id: f_00003-7-0 loss: 0.616452  [   32/   43]
train() client id: f_00003-8-0 loss: 0.733986  [   32/   43]
train() client id: f_00003-9-0 loss: 0.694035  [   32/   43]
train() client id: f_00003-10-0 loss: 0.699627  [   32/   43]
train() client id: f_00003-11-0 loss: 0.722719  [   32/   43]
train() client id: f_00003-12-0 loss: 0.798628  [   32/   43]
train() client id: f_00003-13-0 loss: 0.898122  [   32/   43]
train() client id: f_00003-14-0 loss: 0.690199  [   32/   43]
train() client id: f_00003-15-0 loss: 0.942664  [   32/   43]
train() client id: f_00003-16-0 loss: 0.619086  [   32/   43]
train() client id: f_00003-17-0 loss: 0.824208  [   32/   43]
train() client id: f_00004-0-0 loss: 0.680408  [   32/  306]
train() client id: f_00004-0-1 loss: 0.738247  [   64/  306]
train() client id: f_00004-0-2 loss: 0.731155  [   96/  306]
train() client id: f_00004-0-3 loss: 0.750092  [  128/  306]
train() client id: f_00004-0-4 loss: 0.701982  [  160/  306]
train() client id: f_00004-0-5 loss: 0.806925  [  192/  306]
train() client id: f_00004-0-6 loss: 0.599282  [  224/  306]
train() client id: f_00004-0-7 loss: 0.647106  [  256/  306]
train() client id: f_00004-0-8 loss: 0.709151  [  288/  306]
train() client id: f_00004-1-0 loss: 0.631348  [   32/  306]
train() client id: f_00004-1-1 loss: 0.784882  [   64/  306]
train() client id: f_00004-1-2 loss: 0.662039  [   96/  306]
train() client id: f_00004-1-3 loss: 0.685668  [  128/  306]
train() client id: f_00004-1-4 loss: 0.655294  [  160/  306]
train() client id: f_00004-1-5 loss: 0.751564  [  192/  306]
train() client id: f_00004-1-6 loss: 0.766183  [  224/  306]
train() client id: f_00004-1-7 loss: 0.698354  [  256/  306]
train() client id: f_00004-1-8 loss: 0.691625  [  288/  306]
train() client id: f_00004-2-0 loss: 0.563000  [   32/  306]
train() client id: f_00004-2-1 loss: 0.676066  [   64/  306]
train() client id: f_00004-2-2 loss: 0.633898  [   96/  306]
train() client id: f_00004-2-3 loss: 0.690595  [  128/  306]
train() client id: f_00004-2-4 loss: 0.808536  [  160/  306]
train() client id: f_00004-2-5 loss: 0.662633  [  192/  306]
train() client id: f_00004-2-6 loss: 0.642790  [  224/  306]
train() client id: f_00004-2-7 loss: 0.854522  [  256/  306]
train() client id: f_00004-2-8 loss: 0.764693  [  288/  306]
train() client id: f_00004-3-0 loss: 0.833178  [   32/  306]
train() client id: f_00004-3-1 loss: 0.475351  [   64/  306]
train() client id: f_00004-3-2 loss: 0.662483  [   96/  306]
train() client id: f_00004-3-3 loss: 0.750235  [  128/  306]
train() client id: f_00004-3-4 loss: 0.761645  [  160/  306]
train() client id: f_00004-3-5 loss: 0.776765  [  192/  306]
train() client id: f_00004-3-6 loss: 0.912265  [  224/  306]
train() client id: f_00004-3-7 loss: 0.625872  [  256/  306]
train() client id: f_00004-3-8 loss: 0.651970  [  288/  306]
train() client id: f_00004-4-0 loss: 0.557433  [   32/  306]
train() client id: f_00004-4-1 loss: 0.690619  [   64/  306]
train() client id: f_00004-4-2 loss: 0.711244  [   96/  306]
train() client id: f_00004-4-3 loss: 0.580711  [  128/  306]
train() client id: f_00004-4-4 loss: 0.716145  [  160/  306]
train() client id: f_00004-4-5 loss: 0.722839  [  192/  306]
train() client id: f_00004-4-6 loss: 0.893639  [  224/  306]
train() client id: f_00004-4-7 loss: 0.783734  [  256/  306]
train() client id: f_00004-4-8 loss: 0.729916  [  288/  306]
train() client id: f_00004-5-0 loss: 0.797616  [   32/  306]
train() client id: f_00004-5-1 loss: 0.751295  [   64/  306]
train() client id: f_00004-5-2 loss: 0.675394  [   96/  306]
train() client id: f_00004-5-3 loss: 0.579996  [  128/  306]
train() client id: f_00004-5-4 loss: 0.867889  [  160/  306]
train() client id: f_00004-5-5 loss: 0.664686  [  192/  306]
train() client id: f_00004-5-6 loss: 0.807318  [  224/  306]
train() client id: f_00004-5-7 loss: 0.667906  [  256/  306]
train() client id: f_00004-5-8 loss: 0.570361  [  288/  306]
train() client id: f_00004-6-0 loss: 0.895184  [   32/  306]
train() client id: f_00004-6-1 loss: 0.783309  [   64/  306]
train() client id: f_00004-6-2 loss: 0.762250  [   96/  306]
train() client id: f_00004-6-3 loss: 0.677919  [  128/  306]
train() client id: f_00004-6-4 loss: 0.641336  [  160/  306]
train() client id: f_00004-6-5 loss: 0.652593  [  192/  306]
train() client id: f_00004-6-6 loss: 0.720685  [  224/  306]
train() client id: f_00004-6-7 loss: 0.630782  [  256/  306]
train() client id: f_00004-6-8 loss: 0.696413  [  288/  306]
train() client id: f_00004-7-0 loss: 0.746198  [   32/  306]
train() client id: f_00004-7-1 loss: 0.789840  [   64/  306]
train() client id: f_00004-7-2 loss: 0.690785  [   96/  306]
train() client id: f_00004-7-3 loss: 0.628398  [  128/  306]
train() client id: f_00004-7-4 loss: 0.690885  [  160/  306]
train() client id: f_00004-7-5 loss: 0.583122  [  192/  306]
train() client id: f_00004-7-6 loss: 0.780446  [  224/  306]
train() client id: f_00004-7-7 loss: 0.826170  [  256/  306]
train() client id: f_00004-7-8 loss: 0.766581  [  288/  306]
train() client id: f_00004-8-0 loss: 0.770751  [   32/  306]
train() client id: f_00004-8-1 loss: 0.676758  [   64/  306]
train() client id: f_00004-8-2 loss: 0.724142  [   96/  306]
train() client id: f_00004-8-3 loss: 0.669549  [  128/  306]
train() client id: f_00004-8-4 loss: 0.859915  [  160/  306]
train() client id: f_00004-8-5 loss: 0.711801  [  192/  306]
train() client id: f_00004-8-6 loss: 0.590834  [  224/  306]
train() client id: f_00004-8-7 loss: 0.682629  [  256/  306]
train() client id: f_00004-8-8 loss: 0.707329  [  288/  306]
train() client id: f_00004-9-0 loss: 0.632886  [   32/  306]
train() client id: f_00004-9-1 loss: 0.684007  [   64/  306]
train() client id: f_00004-9-2 loss: 0.749674  [   96/  306]
train() client id: f_00004-9-3 loss: 0.714506  [  128/  306]
train() client id: f_00004-9-4 loss: 0.647245  [  160/  306]
train() client id: f_00004-9-5 loss: 0.690559  [  192/  306]
train() client id: f_00004-9-6 loss: 0.691985  [  224/  306]
train() client id: f_00004-9-7 loss: 0.778898  [  256/  306]
train() client id: f_00004-9-8 loss: 0.781021  [  288/  306]
train() client id: f_00004-10-0 loss: 0.586314  [   32/  306]
train() client id: f_00004-10-1 loss: 0.764120  [   64/  306]
train() client id: f_00004-10-2 loss: 0.725937  [   96/  306]
train() client id: f_00004-10-3 loss: 0.781758  [  128/  306]
train() client id: f_00004-10-4 loss: 0.682642  [  160/  306]
train() client id: f_00004-10-5 loss: 0.765448  [  192/  306]
train() client id: f_00004-10-6 loss: 0.648437  [  224/  306]
train() client id: f_00004-10-7 loss: 0.701748  [  256/  306]
train() client id: f_00004-10-8 loss: 0.747758  [  288/  306]
train() client id: f_00004-11-0 loss: 0.636361  [   32/  306]
train() client id: f_00004-11-1 loss: 0.703574  [   64/  306]
train() client id: f_00004-11-2 loss: 0.715439  [   96/  306]
train() client id: f_00004-11-3 loss: 0.778077  [  128/  306]
train() client id: f_00004-11-4 loss: 0.777435  [  160/  306]
train() client id: f_00004-11-5 loss: 0.725860  [  192/  306]
train() client id: f_00004-11-6 loss: 0.733358  [  224/  306]
train() client id: f_00004-11-7 loss: 0.743422  [  256/  306]
train() client id: f_00004-11-8 loss: 0.647834  [  288/  306]
train() client id: f_00004-12-0 loss: 0.731189  [   32/  306]
train() client id: f_00004-12-1 loss: 0.678946  [   64/  306]
train() client id: f_00004-12-2 loss: 0.800637  [   96/  306]
train() client id: f_00004-12-3 loss: 0.800363  [  128/  306]
train() client id: f_00004-12-4 loss: 0.686269  [  160/  306]
train() client id: f_00004-12-5 loss: 0.567421  [  192/  306]
train() client id: f_00004-12-6 loss: 0.668264  [  224/  306]
train() client id: f_00004-12-7 loss: 0.782461  [  256/  306]
train() client id: f_00004-12-8 loss: 0.684049  [  288/  306]
train() client id: f_00004-13-0 loss: 0.642487  [   32/  306]
train() client id: f_00004-13-1 loss: 0.785233  [   64/  306]
train() client id: f_00004-13-2 loss: 0.653711  [   96/  306]
train() client id: f_00004-13-3 loss: 0.803145  [  128/  306]
train() client id: f_00004-13-4 loss: 0.691381  [  160/  306]
train() client id: f_00004-13-5 loss: 0.752398  [  192/  306]
train() client id: f_00004-13-6 loss: 0.679624  [  224/  306]
train() client id: f_00004-13-7 loss: 0.748123  [  256/  306]
train() client id: f_00004-13-8 loss: 0.739784  [  288/  306]
train() client id: f_00004-14-0 loss: 0.750128  [   32/  306]
train() client id: f_00004-14-1 loss: 0.613410  [   64/  306]
train() client id: f_00004-14-2 loss: 0.680733  [   96/  306]
train() client id: f_00004-14-3 loss: 0.717867  [  128/  306]
train() client id: f_00004-14-4 loss: 0.698938  [  160/  306]
train() client id: f_00004-14-5 loss: 0.797969  [  192/  306]
train() client id: f_00004-14-6 loss: 0.708372  [  224/  306]
train() client id: f_00004-14-7 loss: 0.863676  [  256/  306]
train() client id: f_00004-14-8 loss: 0.749104  [  288/  306]
train() client id: f_00004-15-0 loss: 0.856707  [   32/  306]
train() client id: f_00004-15-1 loss: 0.712727  [   64/  306]
train() client id: f_00004-15-2 loss: 0.791446  [   96/  306]
train() client id: f_00004-15-3 loss: 0.739394  [  128/  306]
train() client id: f_00004-15-4 loss: 0.799352  [  160/  306]
train() client id: f_00004-15-5 loss: 0.700277  [  192/  306]
train() client id: f_00004-15-6 loss: 0.711476  [  224/  306]
train() client id: f_00004-15-7 loss: 0.615464  [  256/  306]
train() client id: f_00004-15-8 loss: 0.679113  [  288/  306]
train() client id: f_00004-16-0 loss: 0.772897  [   32/  306]
train() client id: f_00004-16-1 loss: 0.674135  [   64/  306]
train() client id: f_00004-16-2 loss: 0.730864  [   96/  306]
train() client id: f_00004-16-3 loss: 0.693419  [  128/  306]
train() client id: f_00004-16-4 loss: 0.614066  [  160/  306]
train() client id: f_00004-16-5 loss: 0.661334  [  192/  306]
train() client id: f_00004-16-6 loss: 0.723443  [  224/  306]
train() client id: f_00004-16-7 loss: 0.833518  [  256/  306]
train() client id: f_00004-16-8 loss: 0.774313  [  288/  306]
train() client id: f_00004-17-0 loss: 0.883210  [   32/  306]
train() client id: f_00004-17-1 loss: 0.662804  [   64/  306]
train() client id: f_00004-17-2 loss: 0.670645  [   96/  306]
train() client id: f_00004-17-3 loss: 0.757670  [  128/  306]
train() client id: f_00004-17-4 loss: 0.698256  [  160/  306]
train() client id: f_00004-17-5 loss: 0.625187  [  192/  306]
train() client id: f_00004-17-6 loss: 0.740274  [  224/  306]
train() client id: f_00004-17-7 loss: 0.801781  [  256/  306]
train() client id: f_00004-17-8 loss: 0.609183  [  288/  306]
train() client id: f_00005-0-0 loss: 0.664404  [   32/  146]
train() client id: f_00005-0-1 loss: 0.587104  [   64/  146]
train() client id: f_00005-0-2 loss: 0.814101  [   96/  146]
train() client id: f_00005-0-3 loss: 0.520597  [  128/  146]
train() client id: f_00005-1-0 loss: 0.644525  [   32/  146]
train() client id: f_00005-1-1 loss: 0.680790  [   64/  146]
train() client id: f_00005-1-2 loss: 0.455636  [   96/  146]
train() client id: f_00005-1-3 loss: 0.582040  [  128/  146]
train() client id: f_00005-2-0 loss: 0.516949  [   32/  146]
train() client id: f_00005-2-1 loss: 0.519487  [   64/  146]
train() client id: f_00005-2-2 loss: 0.541111  [   96/  146]
train() client id: f_00005-2-3 loss: 0.821334  [  128/  146]
train() client id: f_00005-3-0 loss: 0.282346  [   32/  146]
train() client id: f_00005-3-1 loss: 0.560774  [   64/  146]
train() client id: f_00005-3-2 loss: 0.590627  [   96/  146]
train() client id: f_00005-3-3 loss: 0.626477  [  128/  146]
train() client id: f_00005-4-0 loss: 0.277393  [   32/  146]
train() client id: f_00005-4-1 loss: 0.646504  [   64/  146]
train() client id: f_00005-4-2 loss: 0.638365  [   96/  146]
train() client id: f_00005-4-3 loss: 0.526956  [  128/  146]
train() client id: f_00005-5-0 loss: 0.664544  [   32/  146]
train() client id: f_00005-5-1 loss: 0.360611  [   64/  146]
train() client id: f_00005-5-2 loss: 0.592727  [   96/  146]
train() client id: f_00005-5-3 loss: 0.772160  [  128/  146]
train() client id: f_00005-6-0 loss: 0.527858  [   32/  146]
train() client id: f_00005-6-1 loss: 0.615359  [   64/  146]
train() client id: f_00005-6-2 loss: 0.792291  [   96/  146]
train() client id: f_00005-6-3 loss: 0.421776  [  128/  146]
train() client id: f_00005-7-0 loss: 0.649885  [   32/  146]
train() client id: f_00005-7-1 loss: 0.447758  [   64/  146]
train() client id: f_00005-7-2 loss: 0.810781  [   96/  146]
train() client id: f_00005-7-3 loss: 0.498804  [  128/  146]
train() client id: f_00005-8-0 loss: 0.635269  [   32/  146]
train() client id: f_00005-8-1 loss: 0.498398  [   64/  146]
train() client id: f_00005-8-2 loss: 0.720424  [   96/  146]
train() client id: f_00005-8-3 loss: 0.565835  [  128/  146]
train() client id: f_00005-9-0 loss: 0.615064  [   32/  146]
train() client id: f_00005-9-1 loss: 0.685993  [   64/  146]
train() client id: f_00005-9-2 loss: 0.456193  [   96/  146]
train() client id: f_00005-9-3 loss: 0.690928  [  128/  146]
train() client id: f_00005-10-0 loss: 0.604040  [   32/  146]
train() client id: f_00005-10-1 loss: 0.420520  [   64/  146]
train() client id: f_00005-10-2 loss: 0.677898  [   96/  146]
train() client id: f_00005-10-3 loss: 0.677333  [  128/  146]
train() client id: f_00005-11-0 loss: 0.748075  [   32/  146]
train() client id: f_00005-11-1 loss: 0.290379  [   64/  146]
train() client id: f_00005-11-2 loss: 0.743378  [   96/  146]
train() client id: f_00005-11-3 loss: 0.458511  [  128/  146]
train() client id: f_00005-12-0 loss: 0.466405  [   32/  146]
train() client id: f_00005-12-1 loss: 0.690865  [   64/  146]
train() client id: f_00005-12-2 loss: 0.288225  [   96/  146]
train() client id: f_00005-12-3 loss: 0.834163  [  128/  146]
train() client id: f_00005-13-0 loss: 0.773010  [   32/  146]
train() client id: f_00005-13-1 loss: 0.742225  [   64/  146]
train() client id: f_00005-13-2 loss: 0.351078  [   96/  146]
train() client id: f_00005-13-3 loss: 0.519495  [  128/  146]
train() client id: f_00005-14-0 loss: 0.519704  [   32/  146]
train() client id: f_00005-14-1 loss: 0.615030  [   64/  146]
train() client id: f_00005-14-2 loss: 0.537593  [   96/  146]
train() client id: f_00005-14-3 loss: 0.569521  [  128/  146]
train() client id: f_00005-15-0 loss: 0.835895  [   32/  146]
train() client id: f_00005-15-1 loss: 0.702380  [   64/  146]
train() client id: f_00005-15-2 loss: 0.346481  [   96/  146]
train() client id: f_00005-15-3 loss: 0.344011  [  128/  146]
train() client id: f_00005-16-0 loss: 0.430707  [   32/  146]
train() client id: f_00005-16-1 loss: 0.640228  [   64/  146]
train() client id: f_00005-16-2 loss: 0.698785  [   96/  146]
train() client id: f_00005-16-3 loss: 0.332229  [  128/  146]
train() client id: f_00005-17-0 loss: 0.467157  [   32/  146]
train() client id: f_00005-17-1 loss: 0.678978  [   64/  146]
train() client id: f_00005-17-2 loss: 0.585881  [   96/  146]
train() client id: f_00005-17-3 loss: 0.634038  [  128/  146]
train() client id: f_00006-0-0 loss: 0.566919  [   32/   54]
train() client id: f_00006-1-0 loss: 0.560004  [   32/   54]
train() client id: f_00006-2-0 loss: 0.564042  [   32/   54]
train() client id: f_00006-3-0 loss: 0.565358  [   32/   54]
train() client id: f_00006-4-0 loss: 0.593775  [   32/   54]
train() client id: f_00006-5-0 loss: 0.480887  [   32/   54]
train() client id: f_00006-6-0 loss: 0.544333  [   32/   54]
train() client id: f_00006-7-0 loss: 0.572957  [   32/   54]
train() client id: f_00006-8-0 loss: 0.535095  [   32/   54]
train() client id: f_00006-9-0 loss: 0.580267  [   32/   54]
train() client id: f_00006-10-0 loss: 0.520020  [   32/   54]
train() client id: f_00006-11-0 loss: 0.579673  [   32/   54]
train() client id: f_00006-12-0 loss: 0.524807  [   32/   54]
train() client id: f_00006-13-0 loss: 0.525324  [   32/   54]
train() client id: f_00006-14-0 loss: 0.503351  [   32/   54]
train() client id: f_00006-15-0 loss: 0.478927  [   32/   54]
train() client id: f_00006-16-0 loss: 0.482216  [   32/   54]
train() client id: f_00006-17-0 loss: 0.483594  [   32/   54]
train() client id: f_00007-0-0 loss: 0.360361  [   32/  179]
train() client id: f_00007-0-1 loss: 0.419083  [   64/  179]
train() client id: f_00007-0-2 loss: 0.497954  [   96/  179]
train() client id: f_00007-0-3 loss: 0.472071  [  128/  179]
train() client id: f_00007-0-4 loss: 0.872422  [  160/  179]
train() client id: f_00007-1-0 loss: 0.631257  [   32/  179]
train() client id: f_00007-1-1 loss: 0.560744  [   64/  179]
train() client id: f_00007-1-2 loss: 0.446517  [   96/  179]
train() client id: f_00007-1-3 loss: 0.502281  [  128/  179]
train() client id: f_00007-1-4 loss: 0.487335  [  160/  179]
train() client id: f_00007-2-0 loss: 0.462497  [   32/  179]
train() client id: f_00007-2-1 loss: 0.305022  [   64/  179]
train() client id: f_00007-2-2 loss: 0.651757  [   96/  179]
train() client id: f_00007-2-3 loss: 0.333201  [  128/  179]
train() client id: f_00007-2-4 loss: 0.576384  [  160/  179]
train() client id: f_00007-3-0 loss: 0.312807  [   32/  179]
train() client id: f_00007-3-1 loss: 0.413762  [   64/  179]
train() client id: f_00007-3-2 loss: 0.372570  [   96/  179]
train() client id: f_00007-3-3 loss: 0.600883  [  128/  179]
train() client id: f_00007-3-4 loss: 0.622942  [  160/  179]
train() client id: f_00007-4-0 loss: 0.344907  [   32/  179]
train() client id: f_00007-4-1 loss: 0.649764  [   64/  179]
train() client id: f_00007-4-2 loss: 0.381790  [   96/  179]
train() client id: f_00007-4-3 loss: 0.624131  [  128/  179]
train() client id: f_00007-4-4 loss: 0.300143  [  160/  179]
train() client id: f_00007-5-0 loss: 0.268722  [   32/  179]
train() client id: f_00007-5-1 loss: 0.504221  [   64/  179]
train() client id: f_00007-5-2 loss: 0.317231  [   96/  179]
train() client id: f_00007-5-3 loss: 0.411417  [  128/  179]
train() client id: f_00007-5-4 loss: 0.773483  [  160/  179]
train() client id: f_00007-6-0 loss: 0.370312  [   32/  179]
train() client id: f_00007-6-1 loss: 0.554732  [   64/  179]
train() client id: f_00007-6-2 loss: 0.426197  [   96/  179]
train() client id: f_00007-6-3 loss: 0.376389  [  128/  179]
train() client id: f_00007-6-4 loss: 0.511738  [  160/  179]
train() client id: f_00007-7-0 loss: 0.260643  [   32/  179]
train() client id: f_00007-7-1 loss: 0.536992  [   64/  179]
train() client id: f_00007-7-2 loss: 0.275942  [   96/  179]
train() client id: f_00007-7-3 loss: 0.487744  [  128/  179]
train() client id: f_00007-7-4 loss: 0.588563  [  160/  179]
train() client id: f_00007-8-0 loss: 0.461130  [   32/  179]
train() client id: f_00007-8-1 loss: 0.261905  [   64/  179]
train() client id: f_00007-8-2 loss: 0.335719  [   96/  179]
train() client id: f_00007-8-3 loss: 0.474491  [  128/  179]
train() client id: f_00007-8-4 loss: 0.379580  [  160/  179]
train() client id: f_00007-9-0 loss: 0.459088  [   32/  179]
train() client id: f_00007-9-1 loss: 0.346126  [   64/  179]
train() client id: f_00007-9-2 loss: 0.332172  [   96/  179]
train() client id: f_00007-9-3 loss: 0.608405  [  128/  179]
train() client id: f_00007-9-4 loss: 0.383415  [  160/  179]
train() client id: f_00007-10-0 loss: 0.658705  [   32/  179]
train() client id: f_00007-10-1 loss: 0.443522  [   64/  179]
train() client id: f_00007-10-2 loss: 0.396731  [   96/  179]
train() client id: f_00007-10-3 loss: 0.294024  [  128/  179]
train() client id: f_00007-10-4 loss: 0.240368  [  160/  179]
train() client id: f_00007-11-0 loss: 0.460313  [   32/  179]
train() client id: f_00007-11-1 loss: 0.451238  [   64/  179]
train() client id: f_00007-11-2 loss: 0.277426  [   96/  179]
train() client id: f_00007-11-3 loss: 0.563677  [  128/  179]
train() client id: f_00007-11-4 loss: 0.453898  [  160/  179]
train() client id: f_00007-12-0 loss: 0.351417  [   32/  179]
train() client id: f_00007-12-1 loss: 0.366894  [   64/  179]
train() client id: f_00007-12-2 loss: 0.282732  [   96/  179]
train() client id: f_00007-12-3 loss: 0.263479  [  128/  179]
train() client id: f_00007-12-4 loss: 0.495096  [  160/  179]
train() client id: f_00007-13-0 loss: 0.397258  [   32/  179]
train() client id: f_00007-13-1 loss: 0.410543  [   64/  179]
train() client id: f_00007-13-2 loss: 0.380813  [   96/  179]
train() client id: f_00007-13-3 loss: 0.397942  [  128/  179]
train() client id: f_00007-13-4 loss: 0.440872  [  160/  179]
train() client id: f_00007-14-0 loss: 0.541761  [   32/  179]
train() client id: f_00007-14-1 loss: 0.365043  [   64/  179]
train() client id: f_00007-14-2 loss: 0.500544  [   96/  179]
train() client id: f_00007-14-3 loss: 0.374924  [  128/  179]
train() client id: f_00007-14-4 loss: 0.373777  [  160/  179]
train() client id: f_00007-15-0 loss: 0.495869  [   32/  179]
train() client id: f_00007-15-1 loss: 0.364997  [   64/  179]
train() client id: f_00007-15-2 loss: 0.347284  [   96/  179]
train() client id: f_00007-15-3 loss: 0.513613  [  128/  179]
train() client id: f_00007-15-4 loss: 0.484417  [  160/  179]
train() client id: f_00007-16-0 loss: 0.267323  [   32/  179]
train() client id: f_00007-16-1 loss: 0.438918  [   64/  179]
train() client id: f_00007-16-2 loss: 0.518059  [   96/  179]
train() client id: f_00007-16-3 loss: 0.349762  [  128/  179]
train() client id: f_00007-16-4 loss: 0.468413  [  160/  179]
train() client id: f_00007-17-0 loss: 0.535608  [   32/  179]
train() client id: f_00007-17-1 loss: 0.649111  [   64/  179]
train() client id: f_00007-17-2 loss: 0.242178  [   96/  179]
train() client id: f_00007-17-3 loss: 0.515310  [  128/  179]
train() client id: f_00007-17-4 loss: 0.241467  [  160/  179]
train() client id: f_00008-0-0 loss: 0.569099  [   32/  130]
train() client id: f_00008-0-1 loss: 0.588939  [   64/  130]
train() client id: f_00008-0-2 loss: 0.708870  [   96/  130]
train() client id: f_00008-0-3 loss: 0.605191  [  128/  130]
train() client id: f_00008-1-0 loss: 0.677114  [   32/  130]
train() client id: f_00008-1-1 loss: 0.638691  [   64/  130]
train() client id: f_00008-1-2 loss: 0.652611  [   96/  130]
train() client id: f_00008-1-3 loss: 0.554142  [  128/  130]
train() client id: f_00008-2-0 loss: 0.617049  [   32/  130]
train() client id: f_00008-2-1 loss: 0.577504  [   64/  130]
train() client id: f_00008-2-2 loss: 0.617928  [   96/  130]
train() client id: f_00008-2-3 loss: 0.689529  [  128/  130]
train() client id: f_00008-3-0 loss: 0.653113  [   32/  130]
train() client id: f_00008-3-1 loss: 0.580658  [   64/  130]
train() client id: f_00008-3-2 loss: 0.707400  [   96/  130]
train() client id: f_00008-3-3 loss: 0.582718  [  128/  130]
train() client id: f_00008-4-0 loss: 0.651487  [   32/  130]
train() client id: f_00008-4-1 loss: 0.649054  [   64/  130]
train() client id: f_00008-4-2 loss: 0.622352  [   96/  130]
train() client id: f_00008-4-3 loss: 0.599224  [  128/  130]
train() client id: f_00008-5-0 loss: 0.503999  [   32/  130]
train() client id: f_00008-5-1 loss: 0.683840  [   64/  130]
train() client id: f_00008-5-2 loss: 0.583821  [   96/  130]
train() client id: f_00008-5-3 loss: 0.720405  [  128/  130]
train() client id: f_00008-6-0 loss: 0.786969  [   32/  130]
train() client id: f_00008-6-1 loss: 0.449425  [   64/  130]
train() client id: f_00008-6-2 loss: 0.559526  [   96/  130]
train() client id: f_00008-6-3 loss: 0.728577  [  128/  130]
train() client id: f_00008-7-0 loss: 0.593854  [   32/  130]
train() client id: f_00008-7-1 loss: 0.695173  [   64/  130]
train() client id: f_00008-7-2 loss: 0.584378  [   96/  130]
train() client id: f_00008-7-3 loss: 0.651144  [  128/  130]
train() client id: f_00008-8-0 loss: 0.547604  [   32/  130]
train() client id: f_00008-8-1 loss: 0.590514  [   64/  130]
train() client id: f_00008-8-2 loss: 0.635826  [   96/  130]
train() client id: f_00008-8-3 loss: 0.714621  [  128/  130]
train() client id: f_00008-9-0 loss: 0.618205  [   32/  130]
train() client id: f_00008-9-1 loss: 0.636224  [   64/  130]
train() client id: f_00008-9-2 loss: 0.610307  [   96/  130]
train() client id: f_00008-9-3 loss: 0.662401  [  128/  130]
train() client id: f_00008-10-0 loss: 0.527946  [   32/  130]
train() client id: f_00008-10-1 loss: 0.619517  [   64/  130]
train() client id: f_00008-10-2 loss: 0.648262  [   96/  130]
train() client id: f_00008-10-3 loss: 0.720687  [  128/  130]
train() client id: f_00008-11-0 loss: 0.564064  [   32/  130]
train() client id: f_00008-11-1 loss: 0.481105  [   64/  130]
train() client id: f_00008-11-2 loss: 0.799686  [   96/  130]
train() client id: f_00008-11-3 loss: 0.683415  [  128/  130]
train() client id: f_00008-12-0 loss: 0.625786  [   32/  130]
train() client id: f_00008-12-1 loss: 0.626863  [   64/  130]
train() client id: f_00008-12-2 loss: 0.677597  [   96/  130]
train() client id: f_00008-12-3 loss: 0.599535  [  128/  130]
train() client id: f_00008-13-0 loss: 0.537901  [   32/  130]
train() client id: f_00008-13-1 loss: 0.680887  [   64/  130]
train() client id: f_00008-13-2 loss: 0.719591  [   96/  130]
train() client id: f_00008-13-3 loss: 0.582092  [  128/  130]
train() client id: f_00008-14-0 loss: 0.731747  [   32/  130]
train() client id: f_00008-14-1 loss: 0.562510  [   64/  130]
train() client id: f_00008-14-2 loss: 0.539861  [   96/  130]
train() client id: f_00008-14-3 loss: 0.697001  [  128/  130]
train() client id: f_00008-15-0 loss: 0.604349  [   32/  130]
train() client id: f_00008-15-1 loss: 0.765167  [   64/  130]
train() client id: f_00008-15-2 loss: 0.474764  [   96/  130]
train() client id: f_00008-15-3 loss: 0.684204  [  128/  130]
train() client id: f_00008-16-0 loss: 0.723277  [   32/  130]
train() client id: f_00008-16-1 loss: 0.673931  [   64/  130]
train() client id: f_00008-16-2 loss: 0.610987  [   96/  130]
train() client id: f_00008-16-3 loss: 0.517287  [  128/  130]
train() client id: f_00008-17-0 loss: 0.653498  [   32/  130]
train() client id: f_00008-17-1 loss: 0.581064  [   64/  130]
train() client id: f_00008-17-2 loss: 0.650449  [   96/  130]
train() client id: f_00008-17-3 loss: 0.615334  [  128/  130]
train() client id: f_00009-0-0 loss: 1.257691  [   32/  118]
train() client id: f_00009-0-1 loss: 1.017633  [   64/  118]
train() client id: f_00009-0-2 loss: 1.060541  [   96/  118]
train() client id: f_00009-1-0 loss: 1.105621  [   32/  118]
train() client id: f_00009-1-1 loss: 1.037222  [   64/  118]
train() client id: f_00009-1-2 loss: 1.008154  [   96/  118]
train() client id: f_00009-2-0 loss: 0.957960  [   32/  118]
train() client id: f_00009-2-1 loss: 1.061407  [   64/  118]
train() client id: f_00009-2-2 loss: 1.028737  [   96/  118]
train() client id: f_00009-3-0 loss: 1.114942  [   32/  118]
train() client id: f_00009-3-1 loss: 0.823741  [   64/  118]
train() client id: f_00009-3-2 loss: 0.965747  [   96/  118]
train() client id: f_00009-4-0 loss: 0.924000  [   32/  118]
train() client id: f_00009-4-1 loss: 1.001102  [   64/  118]
train() client id: f_00009-4-2 loss: 1.071323  [   96/  118]
train() client id: f_00009-5-0 loss: 1.124031  [   32/  118]
train() client id: f_00009-5-1 loss: 0.979783  [   64/  118]
train() client id: f_00009-5-2 loss: 0.752197  [   96/  118]
train() client id: f_00009-6-0 loss: 0.852107  [   32/  118]
train() client id: f_00009-6-1 loss: 0.999583  [   64/  118]
train() client id: f_00009-6-2 loss: 1.035864  [   96/  118]
train() client id: f_00009-7-0 loss: 0.968101  [   32/  118]
train() client id: f_00009-7-1 loss: 0.849800  [   64/  118]
train() client id: f_00009-7-2 loss: 0.982608  [   96/  118]
train() client id: f_00009-8-0 loss: 0.962776  [   32/  118]
train() client id: f_00009-8-1 loss: 0.902569  [   64/  118]
train() client id: f_00009-8-2 loss: 0.811873  [   96/  118]
train() client id: f_00009-9-0 loss: 0.980152  [   32/  118]
train() client id: f_00009-9-1 loss: 0.857467  [   64/  118]
train() client id: f_00009-9-2 loss: 0.923265  [   96/  118]
train() client id: f_00009-10-0 loss: 0.823845  [   32/  118]
train() client id: f_00009-10-1 loss: 0.976049  [   64/  118]
train() client id: f_00009-10-2 loss: 0.862321  [   96/  118]
train() client id: f_00009-11-0 loss: 0.857081  [   32/  118]
train() client id: f_00009-11-1 loss: 0.925633  [   64/  118]
train() client id: f_00009-11-2 loss: 0.744161  [   96/  118]
train() client id: f_00009-12-0 loss: 0.763706  [   32/  118]
train() client id: f_00009-12-1 loss: 1.106337  [   64/  118]
train() client id: f_00009-12-2 loss: 0.782047  [   96/  118]
train() client id: f_00009-13-0 loss: 0.858109  [   32/  118]
train() client id: f_00009-13-1 loss: 0.823006  [   64/  118]
train() client id: f_00009-13-2 loss: 0.934845  [   96/  118]
train() client id: f_00009-14-0 loss: 0.895254  [   32/  118]
train() client id: f_00009-14-1 loss: 0.830793  [   64/  118]
train() client id: f_00009-14-2 loss: 0.784947  [   96/  118]
train() client id: f_00009-15-0 loss: 0.891352  [   32/  118]
train() client id: f_00009-15-1 loss: 0.824749  [   64/  118]
train() client id: f_00009-15-2 loss: 0.844351  [   96/  118]
train() client id: f_00009-16-0 loss: 0.837087  [   32/  118]
train() client id: f_00009-16-1 loss: 0.943736  [   64/  118]
train() client id: f_00009-16-2 loss: 0.732093  [   96/  118]
train() client id: f_00009-17-0 loss: 0.880453  [   32/  118]
train() client id: f_00009-17-1 loss: 0.847933  [   64/  118]
train() client id: f_00009-17-2 loss: 0.889581  [   96/  118]
At round 66 accuracy: 0.6472148541114059
At round 66 training accuracy: 0.5942320590207915
At round 66 training loss: 0.8321185213814952
update_location
xs = 8.927491 451.223621 5.882650 0.934260 -367.581990 -215.230757 -175.849135 -5.143845 -390.120581 20.134486 
ys = -442.390647 7.291448 340.684448 -162.290817 -9.642386 0.794442 -1.381692 336.628436 25.881276 -877.232496 
xs mean: -66.68237997052123
ys mean: -78.16579882624052
dists_uav = 453.639928 462.229295 355.106320 190.628388 381.063636 237.328696 202.298857 351.205301 403.564008 883.143391 
uav_gains = -123.103558 -123.337920 -119.445052 -107.095174 -120.659956 -110.260944 -107.827205 -119.237789 -121.532870 -130.624332 
uav_gains_db_mean: -118.31248008874667
dists_bs = 644.205350 648.291741 245.295474 380.418137 266.796989 178.790632 176.383736 234.467193 261.750374 1070.173207 
bs_gains = -118.219740 -118.296633 -106.478366 -111.814373 -107.500125 -102.632712 -102.467898 -105.929358 -107.267904 -124.391773 
bs_gains_db_mean: -110.49988827015127
Round 67
-------------------------------
ene_coms = [0.04358099 0.04524176 0.008604   0.00891083 0.00912395 0.01088108
 0.00930333 0.00834756 0.00900055 0.05384021]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.23861484 4.46289139 2.021888   0.73246303 4.92550575 2.38457669
 0.90986466 2.89716926 2.11961679 2.16532283]
obj_prev = 24.85791324496809
eta_min = 5.50068725509309e-44	eta_max = 0.8936219660491825
af = 5.108860222262455	bf = 1.0821633325748563	zeta = 5.6197462444887005	eta = 0.9090909090909091
af = 5.108860222262455	bf = 1.0821633325748563	zeta = 17.25998690849239	eta = 0.29599444364287175
af = 5.108860222262455	bf = 1.0821633325748563	zeta = 10.371742483756805	eta = 0.4925749198134687
af = 5.108860222262455	bf = 1.0821633325748563	zeta = 9.262040703018792	eta = 0.5515912082525523
af = 5.108860222262455	bf = 1.0821633325748563	zeta = 9.191768642487485	eta = 0.5558081824043702
af = 5.108860222262455	bf = 1.0821633325748563	zeta = 9.191435922306146	eta = 0.5558283020680226
af = 5.108860222262455	bf = 1.0821633325748563	zeta = 9.19143591477062	eta = 0.5558283025237141
eta = 0.5558283025237141
ene_coms = [0.04358099 0.04524176 0.008604   0.00891083 0.00912395 0.01088108
 0.00930333 0.00834756 0.00900055 0.05384021]
ene_comp = [0.04846225 0.10192456 0.047693   0.0165387  0.11769402 0.05615466
 0.02076953 0.06884716 0.05000073 0.04538528]
ene_total = [1.08420473 1.7335161  0.66313927 0.29977765 1.49382675 0.78963409
 0.35423726 0.90929969 0.69499375 1.16880663]
ti_comp = [1.82131135 1.8047036  2.17108117 2.16801291 2.16588176 2.1483104
 2.1640879  2.17364558 2.16711567 1.71871913]
ti_coms = [0.43580987 0.45241762 0.08604005 0.0891083  0.09123946 0.10881082
 0.09303331 0.08347564 0.09000555 0.53840208]
t_total = [26.58730507 26.58730507 26.58730507 26.58730507 26.58730507 26.58730507
 26.58730507 26.58730507 26.58730507 26.58730507]
ene_coms = [0.04358099 0.04524176 0.008604   0.00891083 0.00912395 0.01088108
 0.00930333 0.00834756 0.00900055 0.05384021]
ene_comp = [2.14448109e-06 2.03191167e-05 1.43844051e-06 6.01534595e-08
 2.17206974e-05 2.39796852e-06 1.19566759e-07 4.31678681e-06
 1.66358442e-06 1.97794936e-06]
ene_total = [0.51337873 0.53315557 0.10136609 0.10496404 0.10772954 0.12819976
 0.10958813 0.0983793  0.10603982 0.63422319]
optimize_network iter = 0 obj = 2.4370241576081297
eta = 0.5558283025237141
freqs = [13304217.65625659 28238588.07811458 10983698.76915256  3814253.73322628
 27170002.11551629 13069495.21348306  4798680.67614893 15836794.75232085
 11536239.10591691 13203226.82714988]
eta_min = 0.5558283025237161	eta_max = 0.5964407909346106
af = 0.0005003003260597781	bf = 1.0821633325748563	zeta = 0.000550330358665756	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.04358099 0.04524176 0.008604   0.00891083 0.00912395 0.01088108
 0.00930333 0.00834756 0.00900055 0.05384021]
ene_comp = [3.48020306e-07 3.29751810e-06 2.33439460e-07 9.76209372e-09
 3.52497570e-06 3.89157891e-07 1.94040694e-08 7.00556173e-07
 2.69977274e-07 3.20994457e-07]
ene_total = [2.50820792 2.60395913 0.49519406 0.51283972 0.52530733 0.62625413
 0.53542962 0.48046218 0.51801854 3.09864856]
ti_comp = [1.61493325 1.5983255  1.96470307 1.96163482 1.95950366 1.9419323
 1.95770981 1.96726748 1.96073757 1.51234104]
ti_coms = [0.43580987 0.45241762 0.08604005 0.0891083  0.09123946 0.10881082
 0.09303331 0.08347564 0.09000555 0.53840208]
t_total = [26.58730507 26.58730507 26.58730507 26.58730507 26.58730507 26.58730507
 26.58730507 26.58730507 26.58730507 26.58730507]
ene_coms = [0.04358099 0.04524176 0.008604   0.00891083 0.00912395 0.01088108
 0.00930333 0.00834756 0.00900055 0.05384021]
ene_comp = [1.85830044e-06 1.76490206e-05 1.19669824e-06 5.00590371e-08
 1.80794647e-05 1.99941913e-06 9.95401047e-08 3.59042380e-06
 1.38453698e-06 1.74044376e-06]
ene_total = [0.56503928 0.58677547 0.11156401 0.11552705 0.11852378 0.14109607
 0.12061635 0.10827036 0.1167076  0.69804565]
optimize_network iter = 1 obj = 2.6821655994728477
eta = 0.5964407909346106
freqs = [13202731.25718854 28056174.80540314 10680032.51829947  3709352.51939139
 26425496.84384509 12722327.87514432  4667595.98216775 15397046.88116509
 11219453.29884088 13203226.82714988]
eta_min = 0.5964407909346143	eta_max = 0.5964407909346142
af = 0.00048240565923720973	bf = 1.0821633325748563	zeta = 0.0005306462251609308	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.04358099 0.04524176 0.008604   0.00891083 0.00912395 0.01088108
 0.00930333 0.00834756 0.00900055 0.05384021]
ene_comp = [3.42731064e-07 3.25505363e-06 2.20710092e-07 9.23251517e-09
 3.33444153e-06 3.68757940e-07 1.83584340e-08 6.62190967e-07
 2.55353667e-07 3.20994457e-07]
ene_total = [2.50820761 2.60395668 0.49519333 0.51283969 0.52529636 0.62625296
 0.53542956 0.48045997 0.5180177  3.09864856]
ti_comp = [1.61493325 1.5983255  1.96470307 1.96163482 1.95950366 1.9419323
 1.95770981 1.96726748 1.96073757 1.51234104]
ti_coms = [0.43580987 0.45241762 0.08604005 0.0891083  0.09123946 0.10881082
 0.09303331 0.08347564 0.09000555 0.53840208]
t_total = [26.58730507 26.58730507 26.58730507 26.58730507 26.58730507 26.58730507
 26.58730507 26.58730507 26.58730507 26.58730507]
ene_coms = [0.04358099 0.04524176 0.008604   0.00891083 0.00912395 0.01088108
 0.00930333 0.00834756 0.00900055 0.05384021]
ene_comp = [1.85830044e-06 1.76490206e-05 1.19669824e-06 5.00590371e-08
 1.80794647e-05 1.99941913e-06 9.95401047e-08 3.59042380e-06
 1.38453698e-06 1.74044376e-06]
ene_total = [0.56503928 0.58677547 0.11156401 0.11552705 0.11852378 0.14109607
 0.12061635 0.10827036 0.1167076  0.69804565]
optimize_network iter = 2 obj = 2.682165599472872
eta = 0.5964407909346142
freqs = [13202731.25718853 28056174.80540314 10680032.51829945  3709352.51939138
 26425496.84384502 12722327.87514429  4667595.98216774 15397046.88116505
 11219453.29884086 13203226.82714989]
Done!
ene_coms = [0.04358099 0.04524176 0.008604   0.00891083 0.00912395 0.01088108
 0.00930333 0.00834756 0.00900055 0.05384021]
ene_comp = [1.75706610e-06 1.66875576e-05 1.13150590e-06 4.73319786e-08
 1.70945525e-05 1.89049708e-06 9.41174737e-08 3.39482883e-06
 1.30911178e-06 1.64562988e-06]
ene_total = [0.04358274 0.04525845 0.00860514 0.00891088 0.00914104 0.01088297
 0.00930343 0.00835096 0.00900186 0.05384185]
At round 67 energy consumption: 0.20687932171432488
At round 67 eta: 0.5964407909346142
At round 67 a_n: 5.2320311093101175
At round 67 local rounds: 16.921848959115742
At round 67 global rounds: 12.964717423812795
gradient difference: 0.31779688596725464
train() client id: f_00000-0-0 loss: 1.323485  [   32/  126]
train() client id: f_00000-0-1 loss: 1.324497  [   64/  126]
train() client id: f_00000-0-2 loss: 1.287776  [   96/  126]
train() client id: f_00000-1-0 loss: 1.361651  [   32/  126]
train() client id: f_00000-1-1 loss: 1.239391  [   64/  126]
train() client id: f_00000-1-2 loss: 1.201903  [   96/  126]
train() client id: f_00000-2-0 loss: 0.968704  [   32/  126]
train() client id: f_00000-2-1 loss: 1.209390  [   64/  126]
train() client id: f_00000-2-2 loss: 1.071647  [   96/  126]
train() client id: f_00000-3-0 loss: 0.903048  [   32/  126]
train() client id: f_00000-3-1 loss: 1.117429  [   64/  126]
train() client id: f_00000-3-2 loss: 1.041378  [   96/  126]
train() client id: f_00000-4-0 loss: 0.975396  [   32/  126]
train() client id: f_00000-4-1 loss: 0.933109  [   64/  126]
train() client id: f_00000-4-2 loss: 0.823021  [   96/  126]
train() client id: f_00000-5-0 loss: 0.981397  [   32/  126]
train() client id: f_00000-5-1 loss: 0.971383  [   64/  126]
train() client id: f_00000-5-2 loss: 0.712600  [   96/  126]
train() client id: f_00000-6-0 loss: 0.880536  [   32/  126]
train() client id: f_00000-6-1 loss: 0.848961  [   64/  126]
train() client id: f_00000-6-2 loss: 0.942200  [   96/  126]
train() client id: f_00000-7-0 loss: 0.782669  [   32/  126]
train() client id: f_00000-7-1 loss: 0.929798  [   64/  126]
train() client id: f_00000-7-2 loss: 0.730069  [   96/  126]
train() client id: f_00000-8-0 loss: 0.893105  [   32/  126]
train() client id: f_00000-8-1 loss: 0.763590  [   64/  126]
train() client id: f_00000-8-2 loss: 0.803037  [   96/  126]
train() client id: f_00000-9-0 loss: 0.882030  [   32/  126]
train() client id: f_00000-9-1 loss: 0.760782  [   64/  126]
train() client id: f_00000-9-2 loss: 0.819821  [   96/  126]
train() client id: f_00000-10-0 loss: 0.706100  [   32/  126]
train() client id: f_00000-10-1 loss: 0.736310  [   64/  126]
train() client id: f_00000-10-2 loss: 0.694573  [   96/  126]
train() client id: f_00000-11-0 loss: 0.836197  [   32/  126]
train() client id: f_00000-11-1 loss: 0.662354  [   64/  126]
train() client id: f_00000-11-2 loss: 0.782679  [   96/  126]
train() client id: f_00000-12-0 loss: 0.915382  [   32/  126]
train() client id: f_00000-12-1 loss: 0.669753  [   64/  126]
train() client id: f_00000-12-2 loss: 0.734182  [   96/  126]
train() client id: f_00000-13-0 loss: 0.789548  [   32/  126]
train() client id: f_00000-13-1 loss: 0.881626  [   64/  126]
train() client id: f_00000-13-2 loss: 0.756611  [   96/  126]
train() client id: f_00000-14-0 loss: 0.792488  [   32/  126]
train() client id: f_00000-14-1 loss: 0.809356  [   64/  126]
train() client id: f_00000-14-2 loss: 0.694163  [   96/  126]
train() client id: f_00000-15-0 loss: 0.770192  [   32/  126]
train() client id: f_00000-15-1 loss: 0.805737  [   64/  126]
train() client id: f_00000-15-2 loss: 0.731384  [   96/  126]
train() client id: f_00001-0-0 loss: 0.245415  [   32/  265]
train() client id: f_00001-0-1 loss: 0.308626  [   64/  265]
train() client id: f_00001-0-2 loss: 0.200566  [   96/  265]
train() client id: f_00001-0-3 loss: 0.278345  [  128/  265]
train() client id: f_00001-0-4 loss: 0.354998  [  160/  265]
train() client id: f_00001-0-5 loss: 0.358299  [  192/  265]
train() client id: f_00001-0-6 loss: 0.426518  [  224/  265]
train() client id: f_00001-0-7 loss: 0.224618  [  256/  265]
train() client id: f_00001-1-0 loss: 0.376696  [   32/  265]
train() client id: f_00001-1-1 loss: 0.311337  [   64/  265]
train() client id: f_00001-1-2 loss: 0.348279  [   96/  265]
train() client id: f_00001-1-3 loss: 0.206578  [  128/  265]
train() client id: f_00001-1-4 loss: 0.239650  [  160/  265]
train() client id: f_00001-1-5 loss: 0.258597  [  192/  265]
train() client id: f_00001-1-6 loss: 0.303770  [  224/  265]
train() client id: f_00001-1-7 loss: 0.290104  [  256/  265]
train() client id: f_00001-2-0 loss: 0.342301  [   32/  265]
train() client id: f_00001-2-1 loss: 0.272792  [   64/  265]
train() client id: f_00001-2-2 loss: 0.195846  [   96/  265]
train() client id: f_00001-2-3 loss: 0.373653  [  128/  265]
train() client id: f_00001-2-4 loss: 0.243411  [  160/  265]
train() client id: f_00001-2-5 loss: 0.239657  [  192/  265]
train() client id: f_00001-2-6 loss: 0.332174  [  224/  265]
train() client id: f_00001-2-7 loss: 0.241462  [  256/  265]
train() client id: f_00001-3-0 loss: 0.277173  [   32/  265]
train() client id: f_00001-3-1 loss: 0.276234  [   64/  265]
train() client id: f_00001-3-2 loss: 0.305791  [   96/  265]
train() client id: f_00001-3-3 loss: 0.264988  [  128/  265]
train() client id: f_00001-3-4 loss: 0.268123  [  160/  265]
train() client id: f_00001-3-5 loss: 0.179785  [  192/  265]
train() client id: f_00001-3-6 loss: 0.249536  [  224/  265]
train() client id: f_00001-3-7 loss: 0.360558  [  256/  265]
train() client id: f_00001-4-0 loss: 0.341714  [   32/  265]
train() client id: f_00001-4-1 loss: 0.191675  [   64/  265]
train() client id: f_00001-4-2 loss: 0.257418  [   96/  265]
train() client id: f_00001-4-3 loss: 0.219491  [  128/  265]
train() client id: f_00001-4-4 loss: 0.280548  [  160/  265]
train() client id: f_00001-4-5 loss: 0.198050  [  192/  265]
train() client id: f_00001-4-6 loss: 0.235981  [  224/  265]
train() client id: f_00001-4-7 loss: 0.402534  [  256/  265]
train() client id: f_00001-5-0 loss: 0.180086  [   32/  265]
train() client id: f_00001-5-1 loss: 0.163446  [   64/  265]
train() client id: f_00001-5-2 loss: 0.335736  [   96/  265]
train() client id: f_00001-5-3 loss: 0.159711  [  128/  265]
train() client id: f_00001-5-4 loss: 0.334046  [  160/  265]
train() client id: f_00001-5-5 loss: 0.279070  [  192/  265]
train() client id: f_00001-5-6 loss: 0.273317  [  224/  265]
train() client id: f_00001-5-7 loss: 0.294254  [  256/  265]
train() client id: f_00001-6-0 loss: 0.141748  [   32/  265]
train() client id: f_00001-6-1 loss: 0.144330  [   64/  265]
train() client id: f_00001-6-2 loss: 0.305900  [   96/  265]
train() client id: f_00001-6-3 loss: 0.274090  [  128/  265]
train() client id: f_00001-6-4 loss: 0.393833  [  160/  265]
train() client id: f_00001-6-5 loss: 0.232234  [  192/  265]
train() client id: f_00001-6-6 loss: 0.166153  [  224/  265]
train() client id: f_00001-6-7 loss: 0.341755  [  256/  265]
train() client id: f_00001-7-0 loss: 0.240343  [   32/  265]
train() client id: f_00001-7-1 loss: 0.279916  [   64/  265]
train() client id: f_00001-7-2 loss: 0.155030  [   96/  265]
train() client id: f_00001-7-3 loss: 0.372111  [  128/  265]
train() client id: f_00001-7-4 loss: 0.281220  [  160/  265]
train() client id: f_00001-7-5 loss: 0.173432  [  192/  265]
train() client id: f_00001-7-6 loss: 0.159334  [  224/  265]
train() client id: f_00001-7-7 loss: 0.335000  [  256/  265]
train() client id: f_00001-8-0 loss: 0.456335  [   32/  265]
train() client id: f_00001-8-1 loss: 0.168696  [   64/  265]
train() client id: f_00001-8-2 loss: 0.210180  [   96/  265]
train() client id: f_00001-8-3 loss: 0.227953  [  128/  265]
train() client id: f_00001-8-4 loss: 0.185794  [  160/  265]
train() client id: f_00001-8-5 loss: 0.250336  [  192/  265]
train() client id: f_00001-8-6 loss: 0.345260  [  224/  265]
train() client id: f_00001-8-7 loss: 0.130127  [  256/  265]
train() client id: f_00001-9-0 loss: 0.143305  [   32/  265]
train() client id: f_00001-9-1 loss: 0.202150  [   64/  265]
train() client id: f_00001-9-2 loss: 0.307341  [   96/  265]
train() client id: f_00001-9-3 loss: 0.280064  [  128/  265]
train() client id: f_00001-9-4 loss: 0.185291  [  160/  265]
train() client id: f_00001-9-5 loss: 0.256692  [  192/  265]
train() client id: f_00001-9-6 loss: 0.230939  [  224/  265]
train() client id: f_00001-9-7 loss: 0.344804  [  256/  265]
train() client id: f_00001-10-0 loss: 0.198868  [   32/  265]
train() client id: f_00001-10-1 loss: 0.368251  [   64/  265]
train() client id: f_00001-10-2 loss: 0.153175  [   96/  265]
train() client id: f_00001-10-3 loss: 0.259537  [  128/  265]
train() client id: f_00001-10-4 loss: 0.361465  [  160/  265]
train() client id: f_00001-10-5 loss: 0.225567  [  192/  265]
train() client id: f_00001-10-6 loss: 0.154140  [  224/  265]
train() client id: f_00001-10-7 loss: 0.161399  [  256/  265]
train() client id: f_00001-11-0 loss: 0.256578  [   32/  265]
train() client id: f_00001-11-1 loss: 0.224753  [   64/  265]
train() client id: f_00001-11-2 loss: 0.140719  [   96/  265]
train() client id: f_00001-11-3 loss: 0.159018  [  128/  265]
train() client id: f_00001-11-4 loss: 0.369382  [  160/  265]
train() client id: f_00001-11-5 loss: 0.378009  [  192/  265]
train() client id: f_00001-11-6 loss: 0.196607  [  224/  265]
train() client id: f_00001-11-7 loss: 0.175142  [  256/  265]
train() client id: f_00001-12-0 loss: 0.295883  [   32/  265]
train() client id: f_00001-12-1 loss: 0.268197  [   64/  265]
train() client id: f_00001-12-2 loss: 0.244743  [   96/  265]
train() client id: f_00001-12-3 loss: 0.144841  [  128/  265]
train() client id: f_00001-12-4 loss: 0.234268  [  160/  265]
train() client id: f_00001-12-5 loss: 0.196518  [  192/  265]
train() client id: f_00001-12-6 loss: 0.282033  [  224/  265]
train() client id: f_00001-12-7 loss: 0.216688  [  256/  265]
train() client id: f_00001-13-0 loss: 0.129931  [   32/  265]
train() client id: f_00001-13-1 loss: 0.257439  [   64/  265]
train() client id: f_00001-13-2 loss: 0.335559  [   96/  265]
train() client id: f_00001-13-3 loss: 0.244610  [  128/  265]
train() client id: f_00001-13-4 loss: 0.226286  [  160/  265]
train() client id: f_00001-13-5 loss: 0.321718  [  192/  265]
train() client id: f_00001-13-6 loss: 0.239761  [  224/  265]
train() client id: f_00001-13-7 loss: 0.121150  [  256/  265]
train() client id: f_00001-14-0 loss: 0.191351  [   32/  265]
train() client id: f_00001-14-1 loss: 0.229743  [   64/  265]
train() client id: f_00001-14-2 loss: 0.272613  [   96/  265]
train() client id: f_00001-14-3 loss: 0.180850  [  128/  265]
train() client id: f_00001-14-4 loss: 0.295661  [  160/  265]
train() client id: f_00001-14-5 loss: 0.148946  [  192/  265]
train() client id: f_00001-14-6 loss: 0.342382  [  224/  265]
train() client id: f_00001-14-7 loss: 0.197506  [  256/  265]
train() client id: f_00001-15-0 loss: 0.126019  [   32/  265]
train() client id: f_00001-15-1 loss: 0.206756  [   64/  265]
train() client id: f_00001-15-2 loss: 0.163779  [   96/  265]
train() client id: f_00001-15-3 loss: 0.176716  [  128/  265]
train() client id: f_00001-15-4 loss: 0.252741  [  160/  265]
train() client id: f_00001-15-5 loss: 0.390353  [  192/  265]
train() client id: f_00001-15-6 loss: 0.258615  [  224/  265]
train() client id: f_00001-15-7 loss: 0.267423  [  256/  265]
train() client id: f_00002-0-0 loss: 1.317029  [   32/  124]
train() client id: f_00002-0-1 loss: 1.144283  [   64/  124]
train() client id: f_00002-0-2 loss: 1.281195  [   96/  124]
train() client id: f_00002-1-0 loss: 1.127245  [   32/  124]
train() client id: f_00002-1-1 loss: 1.267444  [   64/  124]
train() client id: f_00002-1-2 loss: 1.208617  [   96/  124]
train() client id: f_00002-2-0 loss: 1.004387  [   32/  124]
train() client id: f_00002-2-1 loss: 1.217001  [   64/  124]
train() client id: f_00002-2-2 loss: 1.067683  [   96/  124]
train() client id: f_00002-3-0 loss: 1.092726  [   32/  124]
train() client id: f_00002-3-1 loss: 1.147115  [   64/  124]
train() client id: f_00002-3-2 loss: 1.089687  [   96/  124]
train() client id: f_00002-4-0 loss: 1.012382  [   32/  124]
train() client id: f_00002-4-1 loss: 1.034979  [   64/  124]
train() client id: f_00002-4-2 loss: 1.281284  [   96/  124]
train() client id: f_00002-5-0 loss: 0.945123  [   32/  124]
train() client id: f_00002-5-1 loss: 1.149454  [   64/  124]
train() client id: f_00002-5-2 loss: 1.199148  [   96/  124]
train() client id: f_00002-6-0 loss: 1.035044  [   32/  124]
train() client id: f_00002-6-1 loss: 1.176664  [   64/  124]
train() client id: f_00002-6-2 loss: 0.818445  [   96/  124]
train() client id: f_00002-7-0 loss: 0.893067  [   32/  124]
train() client id: f_00002-7-1 loss: 1.248729  [   64/  124]
train() client id: f_00002-7-2 loss: 0.832782  [   96/  124]
train() client id: f_00002-8-0 loss: 0.857310  [   32/  124]
train() client id: f_00002-8-1 loss: 0.970498  [   64/  124]
train() client id: f_00002-8-2 loss: 0.977956  [   96/  124]
train() client id: f_00002-9-0 loss: 0.984734  [   32/  124]
train() client id: f_00002-9-1 loss: 1.120212  [   64/  124]
train() client id: f_00002-9-2 loss: 0.831173  [   96/  124]
train() client id: f_00002-10-0 loss: 1.038276  [   32/  124]
train() client id: f_00002-10-1 loss: 0.929020  [   64/  124]
train() client id: f_00002-10-2 loss: 0.825176  [   96/  124]
train() client id: f_00002-11-0 loss: 1.099065  [   32/  124]
train() client id: f_00002-11-1 loss: 0.807832  [   64/  124]
train() client id: f_00002-11-2 loss: 0.957536  [   96/  124]
train() client id: f_00002-12-0 loss: 1.033986  [   32/  124]
train() client id: f_00002-12-1 loss: 0.898987  [   64/  124]
train() client id: f_00002-12-2 loss: 0.895059  [   96/  124]
train() client id: f_00002-13-0 loss: 0.791893  [   32/  124]
train() client id: f_00002-13-1 loss: 0.944748  [   64/  124]
train() client id: f_00002-13-2 loss: 1.018733  [   96/  124]
train() client id: f_00002-14-0 loss: 0.850225  [   32/  124]
train() client id: f_00002-14-1 loss: 1.015101  [   64/  124]
train() client id: f_00002-14-2 loss: 0.948287  [   96/  124]
train() client id: f_00002-15-0 loss: 0.959100  [   32/  124]
train() client id: f_00002-15-1 loss: 0.953989  [   64/  124]
train() client id: f_00002-15-2 loss: 0.917143  [   96/  124]
train() client id: f_00003-0-0 loss: 0.503991  [   32/   43]
train() client id: f_00003-1-0 loss: 0.578670  [   32/   43]
train() client id: f_00003-2-0 loss: 0.525756  [   32/   43]
train() client id: f_00003-3-0 loss: 0.423491  [   32/   43]
train() client id: f_00003-4-0 loss: 0.635337  [   32/   43]
train() client id: f_00003-5-0 loss: 0.643600  [   32/   43]
train() client id: f_00003-6-0 loss: 0.618488  [   32/   43]
train() client id: f_00003-7-0 loss: 0.529444  [   32/   43]
train() client id: f_00003-8-0 loss: 0.542590  [   32/   43]
train() client id: f_00003-9-0 loss: 0.497532  [   32/   43]
train() client id: f_00003-10-0 loss: 0.478160  [   32/   43]
train() client id: f_00003-11-0 loss: 0.496894  [   32/   43]
train() client id: f_00003-12-0 loss: 0.600152  [   32/   43]
train() client id: f_00003-13-0 loss: 0.662553  [   32/   43]
train() client id: f_00003-14-0 loss: 0.566360  [   32/   43]
train() client id: f_00003-15-0 loss: 0.502387  [   32/   43]
train() client id: f_00004-0-0 loss: 0.847639  [   32/  306]
train() client id: f_00004-0-1 loss: 1.000256  [   64/  306]
train() client id: f_00004-0-2 loss: 0.970963  [   96/  306]
train() client id: f_00004-0-3 loss: 0.917225  [  128/  306]
train() client id: f_00004-0-4 loss: 1.109419  [  160/  306]
train() client id: f_00004-0-5 loss: 0.748694  [  192/  306]
train() client id: f_00004-0-6 loss: 1.028966  [  224/  306]
train() client id: f_00004-0-7 loss: 0.776067  [  256/  306]
train() client id: f_00004-0-8 loss: 0.821488  [  288/  306]
train() client id: f_00004-1-0 loss: 0.949582  [   32/  306]
train() client id: f_00004-1-1 loss: 0.769615  [   64/  306]
train() client id: f_00004-1-2 loss: 0.908878  [   96/  306]
train() client id: f_00004-1-3 loss: 0.845829  [  128/  306]
train() client id: f_00004-1-4 loss: 0.948180  [  160/  306]
train() client id: f_00004-1-5 loss: 0.851166  [  192/  306]
train() client id: f_00004-1-6 loss: 0.945976  [  224/  306]
train() client id: f_00004-1-7 loss: 1.066556  [  256/  306]
train() client id: f_00004-1-8 loss: 0.912449  [  288/  306]
train() client id: f_00004-2-0 loss: 0.907932  [   32/  306]
train() client id: f_00004-2-1 loss: 0.933195  [   64/  306]
train() client id: f_00004-2-2 loss: 1.067714  [   96/  306]
train() client id: f_00004-2-3 loss: 0.868704  [  128/  306]
train() client id: f_00004-2-4 loss: 0.827279  [  160/  306]
train() client id: f_00004-2-5 loss: 0.773765  [  192/  306]
train() client id: f_00004-2-6 loss: 0.841237  [  224/  306]
train() client id: f_00004-2-7 loss: 1.010179  [  256/  306]
train() client id: f_00004-2-8 loss: 0.964183  [  288/  306]
train() client id: f_00004-3-0 loss: 0.876726  [   32/  306]
train() client id: f_00004-3-1 loss: 0.940563  [   64/  306]
train() client id: f_00004-3-2 loss: 1.160695  [   96/  306]
train() client id: f_00004-3-3 loss: 0.815372  [  128/  306]
train() client id: f_00004-3-4 loss: 0.805794  [  160/  306]
train() client id: f_00004-3-5 loss: 0.791415  [  192/  306]
train() client id: f_00004-3-6 loss: 0.978516  [  224/  306]
train() client id: f_00004-3-7 loss: 0.933855  [  256/  306]
train() client id: f_00004-3-8 loss: 0.843830  [  288/  306]
train() client id: f_00004-4-0 loss: 0.788233  [   32/  306]
train() client id: f_00004-4-1 loss: 0.863755  [   64/  306]
train() client id: f_00004-4-2 loss: 1.008078  [   96/  306]
train() client id: f_00004-4-3 loss: 0.830526  [  128/  306]
train() client id: f_00004-4-4 loss: 1.043694  [  160/  306]
train() client id: f_00004-4-5 loss: 0.988185  [  192/  306]
train() client id: f_00004-4-6 loss: 0.896747  [  224/  306]
train() client id: f_00004-4-7 loss: 0.875045  [  256/  306]
train() client id: f_00004-4-8 loss: 0.933814  [  288/  306]
train() client id: f_00004-5-0 loss: 0.830264  [   32/  306]
train() client id: f_00004-5-1 loss: 0.902583  [   64/  306]
train() client id: f_00004-5-2 loss: 0.857602  [   96/  306]
train() client id: f_00004-5-3 loss: 0.955707  [  128/  306]
train() client id: f_00004-5-4 loss: 0.833642  [  160/  306]
train() client id: f_00004-5-5 loss: 0.855601  [  192/  306]
train() client id: f_00004-5-6 loss: 0.889580  [  224/  306]
train() client id: f_00004-5-7 loss: 1.006555  [  256/  306]
train() client id: f_00004-5-8 loss: 0.933685  [  288/  306]
train() client id: f_00004-6-0 loss: 1.007037  [   32/  306]
train() client id: f_00004-6-1 loss: 0.791167  [   64/  306]
train() client id: f_00004-6-2 loss: 0.895905  [   96/  306]
train() client id: f_00004-6-3 loss: 0.850212  [  128/  306]
train() client id: f_00004-6-4 loss: 0.836166  [  160/  306]
train() client id: f_00004-6-5 loss: 0.853353  [  192/  306]
train() client id: f_00004-6-6 loss: 0.989943  [  224/  306]
train() client id: f_00004-6-7 loss: 0.851084  [  256/  306]
train() client id: f_00004-6-8 loss: 1.099848  [  288/  306]
train() client id: f_00004-7-0 loss: 0.930178  [   32/  306]
train() client id: f_00004-7-1 loss: 0.909048  [   64/  306]
train() client id: f_00004-7-2 loss: 0.891720  [   96/  306]
train() client id: f_00004-7-3 loss: 0.893677  [  128/  306]
train() client id: f_00004-7-4 loss: 0.896258  [  160/  306]
train() client id: f_00004-7-5 loss: 0.966019  [  192/  306]
train() client id: f_00004-7-6 loss: 0.883877  [  224/  306]
train() client id: f_00004-7-7 loss: 0.959947  [  256/  306]
train() client id: f_00004-7-8 loss: 0.768443  [  288/  306]
train() client id: f_00004-8-0 loss: 0.978790  [   32/  306]
train() client id: f_00004-8-1 loss: 0.851011  [   64/  306]
train() client id: f_00004-8-2 loss: 0.852487  [   96/  306]
train() client id: f_00004-8-3 loss: 0.831374  [  128/  306]
train() client id: f_00004-8-4 loss: 1.005798  [  160/  306]
train() client id: f_00004-8-5 loss: 0.874259  [  192/  306]
train() client id: f_00004-8-6 loss: 0.896018  [  224/  306]
train() client id: f_00004-8-7 loss: 0.893281  [  256/  306]
train() client id: f_00004-8-8 loss: 0.846307  [  288/  306]
train() client id: f_00004-9-0 loss: 0.898455  [   32/  306]
train() client id: f_00004-9-1 loss: 0.815489  [   64/  306]
train() client id: f_00004-9-2 loss: 0.805447  [   96/  306]
train() client id: f_00004-9-3 loss: 0.831355  [  128/  306]
train() client id: f_00004-9-4 loss: 0.926625  [  160/  306]
train() client id: f_00004-9-5 loss: 0.785296  [  192/  306]
train() client id: f_00004-9-6 loss: 1.041475  [  224/  306]
train() client id: f_00004-9-7 loss: 0.964269  [  256/  306]
train() client id: f_00004-9-8 loss: 0.942615  [  288/  306]
train() client id: f_00004-10-0 loss: 0.810743  [   32/  306]
train() client id: f_00004-10-1 loss: 1.027829  [   64/  306]
train() client id: f_00004-10-2 loss: 0.928040  [   96/  306]
train() client id: f_00004-10-3 loss: 0.888007  [  128/  306]
train() client id: f_00004-10-4 loss: 0.942633  [  160/  306]
train() client id: f_00004-10-5 loss: 0.910445  [  192/  306]
train() client id: f_00004-10-6 loss: 0.891360  [  224/  306]
train() client id: f_00004-10-7 loss: 0.887888  [  256/  306]
train() client id: f_00004-10-8 loss: 0.813181  [  288/  306]
train() client id: f_00004-11-0 loss: 0.887475  [   32/  306]
train() client id: f_00004-11-1 loss: 0.866135  [   64/  306]
train() client id: f_00004-11-2 loss: 0.894418  [   96/  306]
train() client id: f_00004-11-3 loss: 0.934781  [  128/  306]
train() client id: f_00004-11-4 loss: 0.792529  [  160/  306]
train() client id: f_00004-11-5 loss: 0.875341  [  192/  306]
train() client id: f_00004-11-6 loss: 0.946476  [  224/  306]
train() client id: f_00004-11-7 loss: 0.852786  [  256/  306]
train() client id: f_00004-11-8 loss: 1.045238  [  288/  306]
train() client id: f_00004-12-0 loss: 0.905666  [   32/  306]
train() client id: f_00004-12-1 loss: 0.967951  [   64/  306]
train() client id: f_00004-12-2 loss: 0.921731  [   96/  306]
train() client id: f_00004-12-3 loss: 0.763629  [  128/  306]
train() client id: f_00004-12-4 loss: 0.824404  [  160/  306]
train() client id: f_00004-12-5 loss: 0.963418  [  192/  306]
train() client id: f_00004-12-6 loss: 0.931938  [  224/  306]
train() client id: f_00004-12-7 loss: 0.949936  [  256/  306]
train() client id: f_00004-12-8 loss: 0.860546  [  288/  306]
train() client id: f_00004-13-0 loss: 1.006614  [   32/  306]
train() client id: f_00004-13-1 loss: 0.869341  [   64/  306]
train() client id: f_00004-13-2 loss: 0.765582  [   96/  306]
train() client id: f_00004-13-3 loss: 0.930609  [  128/  306]
train() client id: f_00004-13-4 loss: 1.021209  [  160/  306]
train() client id: f_00004-13-5 loss: 0.850890  [  192/  306]
train() client id: f_00004-13-6 loss: 0.792176  [  224/  306]
train() client id: f_00004-13-7 loss: 0.895558  [  256/  306]
train() client id: f_00004-13-8 loss: 0.910123  [  288/  306]
train() client id: f_00004-14-0 loss: 0.853285  [   32/  306]
train() client id: f_00004-14-1 loss: 0.834354  [   64/  306]
train() client id: f_00004-14-2 loss: 0.950056  [   96/  306]
train() client id: f_00004-14-3 loss: 0.880783  [  128/  306]
train() client id: f_00004-14-4 loss: 0.830668  [  160/  306]
train() client id: f_00004-14-5 loss: 0.972081  [  192/  306]
train() client id: f_00004-14-6 loss: 0.837917  [  224/  306]
train() client id: f_00004-14-7 loss: 0.992298  [  256/  306]
train() client id: f_00004-14-8 loss: 0.998565  [  288/  306]
train() client id: f_00004-15-0 loss: 0.875823  [   32/  306]
train() client id: f_00004-15-1 loss: 0.904270  [   64/  306]
train() client id: f_00004-15-2 loss: 0.751491  [   96/  306]
train() client id: f_00004-15-3 loss: 1.017118  [  128/  306]
train() client id: f_00004-15-4 loss: 0.920580  [  160/  306]
train() client id: f_00004-15-5 loss: 0.913878  [  192/  306]
train() client id: f_00004-15-6 loss: 0.976662  [  224/  306]
train() client id: f_00004-15-7 loss: 0.959519  [  256/  306]
train() client id: f_00004-15-8 loss: 0.750733  [  288/  306]
train() client id: f_00005-0-0 loss: 0.696975  [   32/  146]
train() client id: f_00005-0-1 loss: 0.524369  [   64/  146]
train() client id: f_00005-0-2 loss: 0.658730  [   96/  146]
train() client id: f_00005-0-3 loss: 0.558795  [  128/  146]
train() client id: f_00005-1-0 loss: 0.702298  [   32/  146]
train() client id: f_00005-1-1 loss: 0.316698  [   64/  146]
train() client id: f_00005-1-2 loss: 0.502658  [   96/  146]
train() client id: f_00005-1-3 loss: 0.784189  [  128/  146]
train() client id: f_00005-2-0 loss: 0.464054  [   32/  146]
train() client id: f_00005-2-1 loss: 0.597722  [   64/  146]
train() client id: f_00005-2-2 loss: 0.689761  [   96/  146]
train() client id: f_00005-2-3 loss: 0.466536  [  128/  146]
train() client id: f_00005-3-0 loss: 0.634862  [   32/  146]
train() client id: f_00005-3-1 loss: 0.516477  [   64/  146]
train() client id: f_00005-3-2 loss: 0.825004  [   96/  146]
train() client id: f_00005-3-3 loss: 0.310189  [  128/  146]
train() client id: f_00005-4-0 loss: 0.568693  [   32/  146]
train() client id: f_00005-4-1 loss: 0.751031  [   64/  146]
train() client id: f_00005-4-2 loss: 0.401192  [   96/  146]
train() client id: f_00005-4-3 loss: 0.651353  [  128/  146]
train() client id: f_00005-5-0 loss: 0.472035  [   32/  146]
train() client id: f_00005-5-1 loss: 0.810323  [   64/  146]
train() client id: f_00005-5-2 loss: 0.445462  [   96/  146]
train() client id: f_00005-5-3 loss: 0.454852  [  128/  146]
train() client id: f_00005-6-0 loss: 0.578386  [   32/  146]
train() client id: f_00005-6-1 loss: 0.476421  [   64/  146]
train() client id: f_00005-6-2 loss: 0.623524  [   96/  146]
train() client id: f_00005-6-3 loss: 0.489618  [  128/  146]
train() client id: f_00005-7-0 loss: 0.916114  [   32/  146]
train() client id: f_00005-7-1 loss: 0.445278  [   64/  146]
train() client id: f_00005-7-2 loss: 0.455640  [   96/  146]
train() client id: f_00005-7-3 loss: 0.523884  [  128/  146]
train() client id: f_00005-8-0 loss: 0.514572  [   32/  146]
train() client id: f_00005-8-1 loss: 0.647720  [   64/  146]
train() client id: f_00005-8-2 loss: 0.443750  [   96/  146]
train() client id: f_00005-8-3 loss: 0.551926  [  128/  146]
train() client id: f_00005-9-0 loss: 0.874263  [   32/  146]
train() client id: f_00005-9-1 loss: 0.378895  [   64/  146]
train() client id: f_00005-9-2 loss: 0.659549  [   96/  146]
train() client id: f_00005-9-3 loss: 0.410343  [  128/  146]
train() client id: f_00005-10-0 loss: 0.841838  [   32/  146]
train() client id: f_00005-10-1 loss: 0.345766  [   64/  146]
train() client id: f_00005-10-2 loss: 0.683940  [   96/  146]
train() client id: f_00005-10-3 loss: 0.388030  [  128/  146]
train() client id: f_00005-11-0 loss: 0.503381  [   32/  146]
train() client id: f_00005-11-1 loss: 0.503590  [   64/  146]
train() client id: f_00005-11-2 loss: 0.423281  [   96/  146]
train() client id: f_00005-11-3 loss: 0.703999  [  128/  146]
train() client id: f_00005-12-0 loss: 0.373173  [   32/  146]
train() client id: f_00005-12-1 loss: 0.772695  [   64/  146]
train() client id: f_00005-12-2 loss: 0.375947  [   96/  146]
train() client id: f_00005-12-3 loss: 0.874040  [  128/  146]
train() client id: f_00005-13-0 loss: 0.772262  [   32/  146]
train() client id: f_00005-13-1 loss: 0.648529  [   64/  146]
train() client id: f_00005-13-2 loss: 0.274819  [   96/  146]
train() client id: f_00005-13-3 loss: 0.507163  [  128/  146]
train() client id: f_00005-14-0 loss: 0.498896  [   32/  146]
train() client id: f_00005-14-1 loss: 0.616388  [   64/  146]
train() client id: f_00005-14-2 loss: 0.343280  [   96/  146]
train() client id: f_00005-14-3 loss: 0.801921  [  128/  146]
train() client id: f_00005-15-0 loss: 0.373812  [   32/  146]
train() client id: f_00005-15-1 loss: 0.934722  [   64/  146]
train() client id: f_00005-15-2 loss: 0.512483  [   96/  146]
train() client id: f_00005-15-3 loss: 0.490014  [  128/  146]
train() client id: f_00006-0-0 loss: 0.488344  [   32/   54]
train() client id: f_00006-1-0 loss: 0.574687  [   32/   54]
train() client id: f_00006-2-0 loss: 0.469226  [   32/   54]
train() client id: f_00006-3-0 loss: 0.532885  [   32/   54]
train() client id: f_00006-4-0 loss: 0.543295  [   32/   54]
train() client id: f_00006-5-0 loss: 0.519638  [   32/   54]
train() client id: f_00006-6-0 loss: 0.517341  [   32/   54]
train() client id: f_00006-7-0 loss: 0.578531  [   32/   54]
train() client id: f_00006-8-0 loss: 0.529126  [   32/   54]
train() client id: f_00006-9-0 loss: 0.519385  [   32/   54]
train() client id: f_00006-10-0 loss: 0.517099  [   32/   54]
train() client id: f_00006-11-0 loss: 0.543425  [   32/   54]
train() client id: f_00006-12-0 loss: 0.584794  [   32/   54]
train() client id: f_00006-13-0 loss: 0.563239  [   32/   54]
train() client id: f_00006-14-0 loss: 0.536859  [   32/   54]
train() client id: f_00006-15-0 loss: 0.488084  [   32/   54]
train() client id: f_00007-0-0 loss: 0.696029  [   32/  179]
train() client id: f_00007-0-1 loss: 0.603342  [   64/  179]
train() client id: f_00007-0-2 loss: 0.650134  [   96/  179]
train() client id: f_00007-0-3 loss: 0.891530  [  128/  179]
train() client id: f_00007-0-4 loss: 0.660937  [  160/  179]
train() client id: f_00007-1-0 loss: 0.707643  [   32/  179]
train() client id: f_00007-1-1 loss: 0.717615  [   64/  179]
train() client id: f_00007-1-2 loss: 0.767716  [   96/  179]
train() client id: f_00007-1-3 loss: 0.532821  [  128/  179]
train() client id: f_00007-1-4 loss: 0.561227  [  160/  179]
train() client id: f_00007-2-0 loss: 0.669341  [   32/  179]
train() client id: f_00007-2-1 loss: 0.715645  [   64/  179]
train() client id: f_00007-2-2 loss: 0.566468  [   96/  179]
train() client id: f_00007-2-3 loss: 0.659182  [  128/  179]
train() client id: f_00007-2-4 loss: 0.556396  [  160/  179]
train() client id: f_00007-3-0 loss: 0.504178  [   32/  179]
train() client id: f_00007-3-1 loss: 0.671730  [   64/  179]
train() client id: f_00007-3-2 loss: 0.626474  [   96/  179]
train() client id: f_00007-3-3 loss: 0.558190  [  128/  179]
train() client id: f_00007-3-4 loss: 0.544287  [  160/  179]
train() client id: f_00007-4-0 loss: 0.645257  [   32/  179]
train() client id: f_00007-4-1 loss: 0.469353  [   64/  179]
train() client id: f_00007-4-2 loss: 0.647370  [   96/  179]
train() client id: f_00007-4-3 loss: 0.762718  [  128/  179]
train() client id: f_00007-4-4 loss: 0.764180  [  160/  179]
train() client id: f_00007-5-0 loss: 0.778107  [   32/  179]
train() client id: f_00007-5-1 loss: 0.634497  [   64/  179]
train() client id: f_00007-5-2 loss: 0.466522  [   96/  179]
train() client id: f_00007-5-3 loss: 0.593500  [  128/  179]
train() client id: f_00007-5-4 loss: 0.713739  [  160/  179]
train() client id: f_00007-6-0 loss: 0.594143  [   32/  179]
train() client id: f_00007-6-1 loss: 0.758897  [   64/  179]
train() client id: f_00007-6-2 loss: 0.474966  [   96/  179]
train() client id: f_00007-6-3 loss: 0.480784  [  128/  179]
train() client id: f_00007-6-4 loss: 0.654701  [  160/  179]
train() client id: f_00007-7-0 loss: 0.506624  [   32/  179]
train() client id: f_00007-7-1 loss: 0.552221  [   64/  179]
train() client id: f_00007-7-2 loss: 0.735399  [   96/  179]
train() client id: f_00007-7-3 loss: 0.564607  [  128/  179]
train() client id: f_00007-7-4 loss: 0.695103  [  160/  179]
train() client id: f_00007-8-0 loss: 0.642862  [   32/  179]
train() client id: f_00007-8-1 loss: 0.528152  [   64/  179]
train() client id: f_00007-8-2 loss: 0.593022  [   96/  179]
train() client id: f_00007-8-3 loss: 0.547026  [  128/  179]
train() client id: f_00007-8-4 loss: 0.817338  [  160/  179]
train() client id: f_00007-9-0 loss: 0.608872  [   32/  179]
train() client id: f_00007-9-1 loss: 0.556333  [   64/  179]
train() client id: f_00007-9-2 loss: 0.790177  [   96/  179]
train() client id: f_00007-9-3 loss: 0.685087  [  128/  179]
train() client id: f_00007-9-4 loss: 0.434287  [  160/  179]
train() client id: f_00007-10-0 loss: 0.563999  [   32/  179]
train() client id: f_00007-10-1 loss: 0.538633  [   64/  179]
train() client id: f_00007-10-2 loss: 0.861493  [   96/  179]
train() client id: f_00007-10-3 loss: 0.486143  [  128/  179]
train() client id: f_00007-10-4 loss: 0.505023  [  160/  179]
train() client id: f_00007-11-0 loss: 0.671952  [   32/  179]
train() client id: f_00007-11-1 loss: 0.579445  [   64/  179]
train() client id: f_00007-11-2 loss: 0.567367  [   96/  179]
train() client id: f_00007-11-3 loss: 0.464094  [  128/  179]
train() client id: f_00007-11-4 loss: 0.671035  [  160/  179]
train() client id: f_00007-12-0 loss: 0.721027  [   32/  179]
train() client id: f_00007-12-1 loss: 0.459805  [   64/  179]
train() client id: f_00007-12-2 loss: 0.563549  [   96/  179]
train() client id: f_00007-12-3 loss: 0.790251  [  128/  179]
train() client id: f_00007-12-4 loss: 0.478327  [  160/  179]
train() client id: f_00007-13-0 loss: 0.636358  [   32/  179]
train() client id: f_00007-13-1 loss: 0.605991  [   64/  179]
train() client id: f_00007-13-2 loss: 0.584864  [   96/  179]
train() client id: f_00007-13-3 loss: 0.709338  [  128/  179]
train() client id: f_00007-13-4 loss: 0.466935  [  160/  179]
train() client id: f_00007-14-0 loss: 0.565819  [   32/  179]
train() client id: f_00007-14-1 loss: 0.533836  [   64/  179]
train() client id: f_00007-14-2 loss: 0.532744  [   96/  179]
train() client id: f_00007-14-3 loss: 0.517269  [  128/  179]
train() client id: f_00007-14-4 loss: 0.659164  [  160/  179]
train() client id: f_00007-15-0 loss: 0.419319  [   32/  179]
train() client id: f_00007-15-1 loss: 0.781367  [   64/  179]
train() client id: f_00007-15-2 loss: 0.618972  [   96/  179]
train() client id: f_00007-15-3 loss: 0.693965  [  128/  179]
train() client id: f_00007-15-4 loss: 0.474172  [  160/  179]
train() client id: f_00008-0-0 loss: 0.747443  [   32/  130]
train() client id: f_00008-0-1 loss: 0.804603  [   64/  130]
train() client id: f_00008-0-2 loss: 0.640808  [   96/  130]
train() client id: f_00008-0-3 loss: 0.624172  [  128/  130]
train() client id: f_00008-1-0 loss: 0.654421  [   32/  130]
train() client id: f_00008-1-1 loss: 0.748005  [   64/  130]
train() client id: f_00008-1-2 loss: 0.714282  [   96/  130]
train() client id: f_00008-1-3 loss: 0.722154  [  128/  130]
train() client id: f_00008-2-0 loss: 0.693500  [   32/  130]
train() client id: f_00008-2-1 loss: 0.569523  [   64/  130]
train() client id: f_00008-2-2 loss: 0.851770  [   96/  130]
train() client id: f_00008-2-3 loss: 0.708073  [  128/  130]
train() client id: f_00008-3-0 loss: 0.785219  [   32/  130]
train() client id: f_00008-3-1 loss: 0.641811  [   64/  130]
train() client id: f_00008-3-2 loss: 0.752314  [   96/  130]
train() client id: f_00008-3-3 loss: 0.651484  [  128/  130]
train() client id: f_00008-4-0 loss: 0.680152  [   32/  130]
train() client id: f_00008-4-1 loss: 0.707462  [   64/  130]
train() client id: f_00008-4-2 loss: 0.779212  [   96/  130]
train() client id: f_00008-4-3 loss: 0.645103  [  128/  130]
train() client id: f_00008-5-0 loss: 0.673856  [   32/  130]
train() client id: f_00008-5-1 loss: 0.686932  [   64/  130]
train() client id: f_00008-5-2 loss: 0.676221  [   96/  130]
train() client id: f_00008-5-3 loss: 0.763246  [  128/  130]
train() client id: f_00008-6-0 loss: 0.718285  [   32/  130]
train() client id: f_00008-6-1 loss: 0.672483  [   64/  130]
train() client id: f_00008-6-2 loss: 0.650912  [   96/  130]
train() client id: f_00008-6-3 loss: 0.776063  [  128/  130]
train() client id: f_00008-7-0 loss: 0.672916  [   32/  130]
train() client id: f_00008-7-1 loss: 0.749092  [   64/  130]
train() client id: f_00008-7-2 loss: 0.688920  [   96/  130]
train() client id: f_00008-7-3 loss: 0.688918  [  128/  130]
train() client id: f_00008-8-0 loss: 0.621736  [   32/  130]
train() client id: f_00008-8-1 loss: 0.655982  [   64/  130]
train() client id: f_00008-8-2 loss: 0.831579  [   96/  130]
train() client id: f_00008-8-3 loss: 0.678581  [  128/  130]
train() client id: f_00008-9-0 loss: 0.636877  [   32/  130]
train() client id: f_00008-9-1 loss: 0.748362  [   64/  130]
train() client id: f_00008-9-2 loss: 0.712652  [   96/  130]
train() client id: f_00008-9-3 loss: 0.697928  [  128/  130]
train() client id: f_00008-10-0 loss: 0.728751  [   32/  130]
train() client id: f_00008-10-1 loss: 0.687009  [   64/  130]
train() client id: f_00008-10-2 loss: 0.795415  [   96/  130]
train() client id: f_00008-10-3 loss: 0.631012  [  128/  130]
train() client id: f_00008-11-0 loss: 0.733516  [   32/  130]
train() client id: f_00008-11-1 loss: 0.705932  [   64/  130]
train() client id: f_00008-11-2 loss: 0.648334  [   96/  130]
train() client id: f_00008-11-3 loss: 0.735529  [  128/  130]
train() client id: f_00008-12-0 loss: 0.795196  [   32/  130]
train() client id: f_00008-12-1 loss: 0.650383  [   64/  130]
train() client id: f_00008-12-2 loss: 0.726637  [   96/  130]
train() client id: f_00008-12-3 loss: 0.663343  [  128/  130]
train() client id: f_00008-13-0 loss: 0.784570  [   32/  130]
train() client id: f_00008-13-1 loss: 0.687760  [   64/  130]
train() client id: f_00008-13-2 loss: 0.756684  [   96/  130]
train() client id: f_00008-13-3 loss: 0.615857  [  128/  130]
train() client id: f_00008-14-0 loss: 0.586345  [   32/  130]
train() client id: f_00008-14-1 loss: 0.668733  [   64/  130]
train() client id: f_00008-14-2 loss: 0.759264  [   96/  130]
train() client id: f_00008-14-3 loss: 0.788984  [  128/  130]
train() client id: f_00008-15-0 loss: 0.768095  [   32/  130]
train() client id: f_00008-15-1 loss: 0.671289  [   64/  130]
train() client id: f_00008-15-2 loss: 0.738414  [   96/  130]
train() client id: f_00008-15-3 loss: 0.657124  [  128/  130]
train() client id: f_00009-0-0 loss: 1.102699  [   32/  118]
train() client id: f_00009-0-1 loss: 1.243807  [   64/  118]
train() client id: f_00009-0-2 loss: 1.043422  [   96/  118]
train() client id: f_00009-1-0 loss: 1.062314  [   32/  118]
train() client id: f_00009-1-1 loss: 1.077433  [   64/  118]
train() client id: f_00009-1-2 loss: 0.998291  [   96/  118]
train() client id: f_00009-2-0 loss: 0.940814  [   32/  118]
train() client id: f_00009-2-1 loss: 0.954177  [   64/  118]
train() client id: f_00009-2-2 loss: 1.117932  [   96/  118]
train() client id: f_00009-3-0 loss: 0.905585  [   32/  118]
train() client id: f_00009-3-1 loss: 0.991377  [   64/  118]
train() client id: f_00009-3-2 loss: 0.944113  [   96/  118]
train() client id: f_00009-4-0 loss: 0.851570  [   32/  118]
train() client id: f_00009-4-1 loss: 0.966931  [   64/  118]
train() client id: f_00009-4-2 loss: 1.038135  [   96/  118]
train() client id: f_00009-5-0 loss: 0.934119  [   32/  118]
train() client id: f_00009-5-1 loss: 0.839568  [   64/  118]
train() client id: f_00009-5-2 loss: 0.935646  [   96/  118]
train() client id: f_00009-6-0 loss: 0.947799  [   32/  118]
train() client id: f_00009-6-1 loss: 0.689112  [   64/  118]
train() client id: f_00009-6-2 loss: 0.858003  [   96/  118]
train() client id: f_00009-7-0 loss: 0.946100  [   32/  118]
train() client id: f_00009-7-1 loss: 0.768695  [   64/  118]
train() client id: f_00009-7-2 loss: 0.718858  [   96/  118]
train() client id: f_00009-8-0 loss: 0.961702  [   32/  118]
train() client id: f_00009-8-1 loss: 0.813833  [   64/  118]
train() client id: f_00009-8-2 loss: 0.621419  [   96/  118]
train() client id: f_00009-9-0 loss: 0.837132  [   32/  118]
train() client id: f_00009-9-1 loss: 0.783370  [   64/  118]
train() client id: f_00009-9-2 loss: 0.801684  [   96/  118]
train() client id: f_00009-10-0 loss: 0.956155  [   32/  118]
train() client id: f_00009-10-1 loss: 0.779579  [   64/  118]
train() client id: f_00009-10-2 loss: 0.678688  [   96/  118]
train() client id: f_00009-11-0 loss: 0.821388  [   32/  118]
train() client id: f_00009-11-1 loss: 0.807648  [   64/  118]
train() client id: f_00009-11-2 loss: 0.644635  [   96/  118]
train() client id: f_00009-12-0 loss: 0.793955  [   32/  118]
train() client id: f_00009-12-1 loss: 0.689665  [   64/  118]
train() client id: f_00009-12-2 loss: 0.793801  [   96/  118]
train() client id: f_00009-13-0 loss: 0.690221  [   32/  118]
train() client id: f_00009-13-1 loss: 0.759210  [   64/  118]
train() client id: f_00009-13-2 loss: 0.923766  [   96/  118]
train() client id: f_00009-14-0 loss: 0.935145  [   32/  118]
train() client id: f_00009-14-1 loss: 0.670481  [   64/  118]
train() client id: f_00009-14-2 loss: 0.760034  [   96/  118]
train() client id: f_00009-15-0 loss: 0.731372  [   32/  118]
train() client id: f_00009-15-1 loss: 0.658330  [   64/  118]
train() client id: f_00009-15-2 loss: 0.927251  [   96/  118]
At round 67 accuracy: 0.6472148541114059
At round 67 training accuracy: 0.5895372233400402
At round 67 training loss: 0.8455421491886894
update_location
xs = 8.927491 456.223621 5.882650 0.934260 -372.581990 -220.230757 -180.849135 -5.143845 -395.120581 20.134486 
ys = -447.390647 7.291448 345.684448 -167.290817 -9.642386 0.794442 -1.381692 341.628436 25.881276 -882.232496 
xs mean: -68.18237997052123
ys mean: -78.66579882624052
dists_uav = 458.517275 467.111504 359.906020 194.902772 385.888993 241.872317 206.659911 356.000629 408.399454 888.110114 
uav_gains = -123.237619 -123.467680 -119.690352 -107.362227 -120.859127 -110.621887 -108.105295 -119.491553 -121.703901 -130.685507 
uav_gains_db_mean: -118.52251481919508
dists_bs = 648.998798 653.122820 248.700048 384.858243 270.428278 179.981660 176.478650 237.941483 265.874903 1075.089772 
bs_gains = -118.309888 -118.386915 -106.645983 -111.955481 -107.664518 -102.713450 -102.474440 -106.108224 -107.458025 -124.447512 
bs_gains_db_mean: -110.61644364461351
Round 68
-------------------------------
ene_coms = [0.04452078 0.04619782 0.00868535 0.00905029 0.00921327 0.01115857
 0.00946123 0.00842947 0.00910134 0.05435215]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.09669222 4.17542368 1.88991492 0.68519676 4.60346957 2.22982673
 0.85107481 2.70789349 1.9813413  2.02608558]
obj_prev = 23.246919048118578
eta_min = 5.654441742720265e-47	eta_max = 0.8994757751859145
af = 4.774378485567752	bf = 1.027624399277806	zeta = 5.251816334124527	eta = 0.9090909090909091
af = 4.774378485567752	bf = 1.027624399277806	zeta = 16.30938398310285	eta = 0.29273812490491313
af = 4.774378485567752	bf = 1.027624399277806	zeta = 9.745807094920362	eta = 0.48989051794963373
af = 4.774378485567752	bf = 1.027624399277806	zeta = 8.693219366461966	eta = 0.5492071791018045
af = 4.774378485567752	bf = 1.027624399277806	zeta = 8.626605237208999	eta = 0.5534481240632759
af = 4.774378485567752	bf = 1.027624399277806	zeta = 8.626289727971864	eta = 0.5534683666010208
af = 4.774378485567752	bf = 1.027624399277806	zeta = 8.626289720820488	eta = 0.5534683670598578
eta = 0.5534683670598578
ene_coms = [0.04452078 0.04619782 0.00868535 0.00905029 0.00921327 0.01115857
 0.00946123 0.00842947 0.00910134 0.05435215]
ene_comp = [0.04881334 0.10266299 0.04803853 0.01665852 0.11854669 0.05656149
 0.02092    0.06934594 0.05036297 0.04571408]
ene_total = [1.02200113 1.6300138  0.62112192 0.28150937 1.39896128 0.74152918
 0.33267202 0.85163451 0.65112939 1.09571715]
ti_comp = [1.97822281 1.96145241 2.3365771  2.33292777 2.33129791 2.31184497
 2.32881837 2.33913588 2.33241726 1.87990915]
ti_coms = [0.44520782 0.46197821 0.08685353 0.09050286 0.09213272 0.11158566
 0.09461226 0.08429474 0.09101336 0.54352148]
t_total = [26.53636932 26.53636932 26.53636932 26.53636932 26.53636932 26.53636932
 26.53636932 26.53636932 26.53636932 26.53636932]
ene_coms = [0.04452078 0.04619782 0.00868535 0.00905029 0.00921327 0.01115857
 0.00946123 0.00842947 0.00910134 0.05435215]
ene_comp = [1.85757045e-06 1.75778655e-05 1.26907968e-06 5.30868369e-08
 1.91581160e-05 2.11604657e-06 1.05510119e-07 3.80918669e-06
 1.46757948e-06 1.68949289e-06]
ene_total = [0.4875193  0.50605488 0.09511781 0.09910048 0.10109436 0.12220856
 0.10360081 0.09234377 0.09967496 0.59517012]
optimize_network iter = 0 obj = 2.3018850501522476
eta = 0.5534683670598578
freqs = [12337676.0050239  26170144.41247511 10279679.88234374  3570303.89886503
 25425041.68497075 12232977.29859088  4491549.22100435 14822982.66784324
 10796304.52935106 12158588.70472346]
eta_min = 0.5534683670598585	eta_max = 0.6188551967841464
af = 0.0004058671819906019	bf = 1.027624399277806	zeta = 0.00044645390018966215	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.04452078 0.04619782 0.00868535 0.00905029 0.00921327 0.01115857
 0.00946123 0.00842947 0.00910134 0.05435215]
ene_comp = [2.99290288e-07 2.83213186e-06 2.04473120e-07 8.55330938e-09
 3.08674058e-06 3.40935757e-07 1.69997073e-08 6.13733161e-07
 2.36455250e-07 2.72209764e-07]
ene_total = [2.39453686 2.48487155 0.46714694 0.48676408 0.49569573 0.6001744
 0.50886668 0.45340669 0.48952205 2.92330893]
ti_comp = [1.62335334 1.60658294 1.98170763 1.9780583  1.97642843 1.9569755
 1.97394889 1.98426641 1.97754779 1.52503967]
ti_coms = [0.44520782 0.46197821 0.08685353 0.09050286 0.09213272 0.11158566
 0.09461226 0.08429474 0.09101336 0.54352148]
t_total = [26.53636932 26.53636932 26.53636932 26.53636932 26.53636932 26.53636932
 26.53636932 26.53636932 26.53636932 26.53636932]
ene_coms = [0.04452078 0.04619782 0.00868535 0.00905029 0.00921327 0.01115857
 0.00946123 0.00842947 0.00910134 0.05435215]
ene_comp = [1.47265979e-06 1.39877619e-05 9.41895680e-07 3.94224947e-08
 1.42304621e-05 1.57653894e-06 7.84018627e-08 2.82602528e-06
 1.08991591e-06 1.37056819e-06]
ene_total = [0.57115014 0.59282445 0.11143143 0.11610136 0.11837426 0.14316698
 0.12137357 0.10817308 0.11676974 0.69726971]
optimize_network iter = 1 obj = 2.696634722561165
eta = 0.6188551967841464
freqs = [12196628.32009669 25919404.85763576  9832510.22502391  3415951.30414004
 24328909.05976168 11723297.06921468  4298729.89223191 14175400.96753533
 10329960.63532737 12158588.70472346]
eta_min = 0.6188551967841811	eta_max = 0.6188551967841464
af = 0.0003832413904092479	bf = 1.027624399277806	zeta = 0.0004215655294501727	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.04452078 0.04619782 0.00868535 0.00905029 0.00921327 0.01115857
 0.00946123 0.00842947 0.00910134 0.05435215]
ene_comp = [2.92486268e-07 2.77812180e-06 1.87070736e-07 7.82973664e-09
 2.82632470e-06 3.13117797e-07 1.55714635e-08 5.61279386e-07
 2.16469165e-07 2.72209764e-07]
ene_total = [2.39453649 2.48486865 0.467146   0.48676404 0.49568173 0.6001729
 0.5088666  0.45340387 0.48952098 2.92330893]
ti_comp = [1.62335334 1.60658294 1.98170763 1.9780583  1.97642843 1.9569755
 1.97394889 1.98426641 1.97754779 1.52503967]
ti_coms = [0.44520782 0.46197821 0.08685353 0.09050286 0.09213272 0.11158566
 0.09461226 0.08429474 0.09101336 0.54352148]
t_total = [26.53636932 26.53636932 26.53636932 26.53636932 26.53636932 26.53636932
 26.53636932 26.53636932 26.53636932 26.53636932]
ene_coms = [0.04452078 0.04619782 0.00868535 0.00905029 0.00921327 0.01115857
 0.00946123 0.00842947 0.00910134 0.05435215]
ene_comp = [1.47265979e-06 1.39877619e-05 9.41895680e-07 3.94224947e-08
 1.42304621e-05 1.57653894e-06 7.84018627e-08 2.82602528e-06
 1.08991591e-06 1.37056819e-06]
ene_total = [0.57115014 0.59282445 0.11143143 0.11610136 0.11837426 0.14316698
 0.12137357 0.10817308 0.11676974 0.69726971]
optimize_network iter = 2 obj = 2.696634722561165
eta = 0.6188551967841464
freqs = [12196628.32009669 25919404.85763576  9832510.22502391  3415951.30414004
 24328909.05976168 11723297.06921468  4298729.89223191 14175400.96753533
 10329960.63532737 12158588.70472346]
Done!
ene_coms = [0.04452078 0.04619782 0.00868535 0.00905029 0.00921327 0.01115857
 0.00946123 0.00842947 0.00910134 0.05435215]
ene_comp = [1.40576067e-06 1.33523340e-05 8.99107793e-07 3.76316327e-08
 1.35840090e-05 1.50492085e-06 7.48402687e-08 2.69764625e-06
 1.04040385e-06 1.30830682e-06]
ene_total = [0.04452219 0.04621117 0.00868625 0.00905032 0.00922686 0.01116007
 0.0094613  0.00843217 0.00910238 0.05435346]
At round 68 energy consumption: 0.21020616709334983
At round 68 eta: 0.6188551967841464
At round 68 a_n: 4.8894852623407985
At round 68 local rounds: 15.713839102143082
At round 68 global rounds: 12.82841907087931
gradient difference: 0.30734992027282715
train() client id: f_00000-0-0 loss: 1.224450  [   32/  126]
train() client id: f_00000-0-1 loss: 1.248374  [   64/  126]
train() client id: f_00000-0-2 loss: 1.425565  [   96/  126]
train() client id: f_00000-1-0 loss: 1.231700  [   32/  126]
train() client id: f_00000-1-1 loss: 1.333944  [   64/  126]
train() client id: f_00000-1-2 loss: 1.158106  [   96/  126]
train() client id: f_00000-2-0 loss: 1.479473  [   32/  126]
train() client id: f_00000-2-1 loss: 1.106309  [   64/  126]
train() client id: f_00000-2-2 loss: 0.864552  [   96/  126]
train() client id: f_00000-3-0 loss: 0.975252  [   32/  126]
train() client id: f_00000-3-1 loss: 0.921307  [   64/  126]
train() client id: f_00000-3-2 loss: 1.023152  [   96/  126]
train() client id: f_00000-4-0 loss: 0.896237  [   32/  126]
train() client id: f_00000-4-1 loss: 0.894605  [   64/  126]
train() client id: f_00000-4-2 loss: 0.945877  [   96/  126]
train() client id: f_00000-5-0 loss: 0.779419  [   32/  126]
train() client id: f_00000-5-1 loss: 0.960906  [   64/  126]
train() client id: f_00000-5-2 loss: 0.863673  [   96/  126]
train() client id: f_00000-6-0 loss: 0.821228  [   32/  126]
train() client id: f_00000-6-1 loss: 0.795543  [   64/  126]
train() client id: f_00000-6-2 loss: 0.996967  [   96/  126]
train() client id: f_00000-7-0 loss: 0.851605  [   32/  126]
train() client id: f_00000-7-1 loss: 0.935941  [   64/  126]
train() client id: f_00000-7-2 loss: 0.813040  [   96/  126]
train() client id: f_00000-8-0 loss: 0.824571  [   32/  126]
train() client id: f_00000-8-1 loss: 0.815065  [   64/  126]
train() client id: f_00000-8-2 loss: 0.740387  [   96/  126]
train() client id: f_00000-9-0 loss: 0.906141  [   32/  126]
train() client id: f_00000-9-1 loss: 0.884211  [   64/  126]
train() client id: f_00000-9-2 loss: 0.720579  [   96/  126]
train() client id: f_00000-10-0 loss: 0.669277  [   32/  126]
train() client id: f_00000-10-1 loss: 0.996158  [   64/  126]
train() client id: f_00000-10-2 loss: 0.786835  [   96/  126]
train() client id: f_00000-11-0 loss: 0.787069  [   32/  126]
train() client id: f_00000-11-1 loss: 0.693606  [   64/  126]
train() client id: f_00000-11-2 loss: 0.874401  [   96/  126]
train() client id: f_00000-12-0 loss: 0.765786  [   32/  126]
train() client id: f_00000-12-1 loss: 0.755558  [   64/  126]
train() client id: f_00000-12-2 loss: 0.907413  [   96/  126]
train() client id: f_00000-13-0 loss: 0.773655  [   32/  126]
train() client id: f_00000-13-1 loss: 0.668745  [   64/  126]
train() client id: f_00000-13-2 loss: 0.902220  [   96/  126]
train() client id: f_00000-14-0 loss: 0.710375  [   32/  126]
train() client id: f_00000-14-1 loss: 0.826753  [   64/  126]
train() client id: f_00000-14-2 loss: 0.799872  [   96/  126]
train() client id: f_00001-0-0 loss: 0.416599  [   32/  265]
train() client id: f_00001-0-1 loss: 0.381441  [   64/  265]
train() client id: f_00001-0-2 loss: 0.330446  [   96/  265]
train() client id: f_00001-0-3 loss: 0.358316  [  128/  265]
train() client id: f_00001-0-4 loss: 0.491664  [  160/  265]
train() client id: f_00001-0-5 loss: 0.364906  [  192/  265]
train() client id: f_00001-0-6 loss: 0.389164  [  224/  265]
train() client id: f_00001-0-7 loss: 0.445149  [  256/  265]
train() client id: f_00001-1-0 loss: 0.365645  [   32/  265]
train() client id: f_00001-1-1 loss: 0.341208  [   64/  265]
train() client id: f_00001-1-2 loss: 0.400701  [   96/  265]
train() client id: f_00001-1-3 loss: 0.372978  [  128/  265]
train() client id: f_00001-1-4 loss: 0.406487  [  160/  265]
train() client id: f_00001-1-5 loss: 0.425900  [  192/  265]
train() client id: f_00001-1-6 loss: 0.454783  [  224/  265]
train() client id: f_00001-1-7 loss: 0.353380  [  256/  265]
train() client id: f_00001-2-0 loss: 0.300474  [   32/  265]
train() client id: f_00001-2-1 loss: 0.355382  [   64/  265]
train() client id: f_00001-2-2 loss: 0.309458  [   96/  265]
train() client id: f_00001-2-3 loss: 0.369922  [  128/  265]
train() client id: f_00001-2-4 loss: 0.342936  [  160/  265]
train() client id: f_00001-2-5 loss: 0.427101  [  192/  265]
train() client id: f_00001-2-6 loss: 0.385750  [  224/  265]
train() client id: f_00001-2-7 loss: 0.581717  [  256/  265]
train() client id: f_00001-3-0 loss: 0.371168  [   32/  265]
train() client id: f_00001-3-1 loss: 0.377641  [   64/  265]
train() client id: f_00001-3-2 loss: 0.288469  [   96/  265]
train() client id: f_00001-3-3 loss: 0.522670  [  128/  265]
train() client id: f_00001-3-4 loss: 0.383802  [  160/  265]
train() client id: f_00001-3-5 loss: 0.408652  [  192/  265]
train() client id: f_00001-3-6 loss: 0.359895  [  224/  265]
train() client id: f_00001-3-7 loss: 0.308152  [  256/  265]
train() client id: f_00001-4-0 loss: 0.343075  [   32/  265]
train() client id: f_00001-4-1 loss: 0.370673  [   64/  265]
train() client id: f_00001-4-2 loss: 0.324170  [   96/  265]
train() client id: f_00001-4-3 loss: 0.356190  [  128/  265]
train() client id: f_00001-4-4 loss: 0.397231  [  160/  265]
train() client id: f_00001-4-5 loss: 0.335535  [  192/  265]
train() client id: f_00001-4-6 loss: 0.295979  [  224/  265]
train() client id: f_00001-4-7 loss: 0.570107  [  256/  265]
train() client id: f_00001-5-0 loss: 0.282932  [   32/  265]
train() client id: f_00001-5-1 loss: 0.509381  [   64/  265]
train() client id: f_00001-5-2 loss: 0.488339  [   96/  265]
train() client id: f_00001-5-3 loss: 0.351118  [  128/  265]
train() client id: f_00001-5-4 loss: 0.323904  [  160/  265]
train() client id: f_00001-5-5 loss: 0.404536  [  192/  265]
train() client id: f_00001-5-6 loss: 0.264267  [  224/  265]
train() client id: f_00001-5-7 loss: 0.282284  [  256/  265]
train() client id: f_00001-6-0 loss: 0.342754  [   32/  265]
train() client id: f_00001-6-1 loss: 0.337543  [   64/  265]
train() client id: f_00001-6-2 loss: 0.501077  [   96/  265]
train() client id: f_00001-6-3 loss: 0.359338  [  128/  265]
train() client id: f_00001-6-4 loss: 0.280601  [  160/  265]
train() client id: f_00001-6-5 loss: 0.406245  [  192/  265]
train() client id: f_00001-6-6 loss: 0.293311  [  224/  265]
train() client id: f_00001-6-7 loss: 0.363889  [  256/  265]
train() client id: f_00001-7-0 loss: 0.308928  [   32/  265]
train() client id: f_00001-7-1 loss: 0.392237  [   64/  265]
train() client id: f_00001-7-2 loss: 0.438929  [   96/  265]
train() client id: f_00001-7-3 loss: 0.404822  [  128/  265]
train() client id: f_00001-7-4 loss: 0.279489  [  160/  265]
train() client id: f_00001-7-5 loss: 0.336279  [  192/  265]
train() client id: f_00001-7-6 loss: 0.357105  [  224/  265]
train() client id: f_00001-7-7 loss: 0.295188  [  256/  265]
train() client id: f_00001-8-0 loss: 0.324438  [   32/  265]
train() client id: f_00001-8-1 loss: 0.323359  [   64/  265]
train() client id: f_00001-8-2 loss: 0.437812  [   96/  265]
train() client id: f_00001-8-3 loss: 0.336242  [  128/  265]
train() client id: f_00001-8-4 loss: 0.461426  [  160/  265]
train() client id: f_00001-8-5 loss: 0.297683  [  192/  265]
train() client id: f_00001-8-6 loss: 0.300989  [  224/  265]
train() client id: f_00001-8-7 loss: 0.426145  [  256/  265]
train() client id: f_00001-9-0 loss: 0.409091  [   32/  265]
train() client id: f_00001-9-1 loss: 0.278007  [   64/  265]
train() client id: f_00001-9-2 loss: 0.381274  [   96/  265]
train() client id: f_00001-9-3 loss: 0.329343  [  128/  265]
train() client id: f_00001-9-4 loss: 0.331166  [  160/  265]
train() client id: f_00001-9-5 loss: 0.446047  [  192/  265]
train() client id: f_00001-9-6 loss: 0.318410  [  224/  265]
train() client id: f_00001-9-7 loss: 0.413188  [  256/  265]
train() client id: f_00001-10-0 loss: 0.263019  [   32/  265]
train() client id: f_00001-10-1 loss: 0.453771  [   64/  265]
train() client id: f_00001-10-2 loss: 0.475410  [   96/  265]
train() client id: f_00001-10-3 loss: 0.301124  [  128/  265]
train() client id: f_00001-10-4 loss: 0.407879  [  160/  265]
train() client id: f_00001-10-5 loss: 0.260256  [  192/  265]
train() client id: f_00001-10-6 loss: 0.362324  [  224/  265]
train() client id: f_00001-10-7 loss: 0.390771  [  256/  265]
train() client id: f_00001-11-0 loss: 0.302437  [   32/  265]
train() client id: f_00001-11-1 loss: 0.251469  [   64/  265]
train() client id: f_00001-11-2 loss: 0.379202  [   96/  265]
train() client id: f_00001-11-3 loss: 0.540769  [  128/  265]
train() client id: f_00001-11-4 loss: 0.397198  [  160/  265]
train() client id: f_00001-11-5 loss: 0.269377  [  192/  265]
train() client id: f_00001-11-6 loss: 0.329420  [  224/  265]
train() client id: f_00001-11-7 loss: 0.358693  [  256/  265]
train() client id: f_00001-12-0 loss: 0.287525  [   32/  265]
train() client id: f_00001-12-1 loss: 0.483522  [   64/  265]
train() client id: f_00001-12-2 loss: 0.319953  [   96/  265]
train() client id: f_00001-12-3 loss: 0.359183  [  128/  265]
train() client id: f_00001-12-4 loss: 0.271801  [  160/  265]
train() client id: f_00001-12-5 loss: 0.333970  [  192/  265]
train() client id: f_00001-12-6 loss: 0.481254  [  224/  265]
train() client id: f_00001-12-7 loss: 0.352194  [  256/  265]
train() client id: f_00001-13-0 loss: 0.492787  [   32/  265]
train() client id: f_00001-13-1 loss: 0.326493  [   64/  265]
train() client id: f_00001-13-2 loss: 0.316033  [   96/  265]
train() client id: f_00001-13-3 loss: 0.375903  [  128/  265]
train() client id: f_00001-13-4 loss: 0.293240  [  160/  265]
train() client id: f_00001-13-5 loss: 0.266381  [  192/  265]
train() client id: f_00001-13-6 loss: 0.351503  [  224/  265]
train() client id: f_00001-13-7 loss: 0.476814  [  256/  265]
train() client id: f_00001-14-0 loss: 0.463106  [   32/  265]
train() client id: f_00001-14-1 loss: 0.293367  [   64/  265]
train() client id: f_00001-14-2 loss: 0.496507  [   96/  265]
train() client id: f_00001-14-3 loss: 0.431335  [  128/  265]
train() client id: f_00001-14-4 loss: 0.412201  [  160/  265]
train() client id: f_00001-14-5 loss: 0.268333  [  192/  265]
train() client id: f_00001-14-6 loss: 0.270217  [  224/  265]
train() client id: f_00001-14-7 loss: 0.263222  [  256/  265]
train() client id: f_00002-0-0 loss: 1.200778  [   32/  124]
train() client id: f_00002-0-1 loss: 1.278878  [   64/  124]
train() client id: f_00002-0-2 loss: 1.223372  [   96/  124]
train() client id: f_00002-1-0 loss: 1.123294  [   32/  124]
train() client id: f_00002-1-1 loss: 1.102514  [   64/  124]
train() client id: f_00002-1-2 loss: 1.343917  [   96/  124]
train() client id: f_00002-2-0 loss: 1.094719  [   32/  124]
train() client id: f_00002-2-1 loss: 1.224178  [   64/  124]
train() client id: f_00002-2-2 loss: 1.109022  [   96/  124]
train() client id: f_00002-3-0 loss: 1.155684  [   32/  124]
train() client id: f_00002-3-1 loss: 0.925855  [   64/  124]
train() client id: f_00002-3-2 loss: 1.170893  [   96/  124]
train() client id: f_00002-4-0 loss: 1.087006  [   32/  124]
train() client id: f_00002-4-1 loss: 1.050979  [   64/  124]
train() client id: f_00002-4-2 loss: 0.897308  [   96/  124]
train() client id: f_00002-5-0 loss: 1.021405  [   32/  124]
train() client id: f_00002-5-1 loss: 1.137285  [   64/  124]
train() client id: f_00002-5-2 loss: 0.846316  [   96/  124]
train() client id: f_00002-6-0 loss: 0.913584  [   32/  124]
train() client id: f_00002-6-1 loss: 0.868780  [   64/  124]
train() client id: f_00002-6-2 loss: 1.099678  [   96/  124]
train() client id: f_00002-7-0 loss: 1.045111  [   32/  124]
train() client id: f_00002-7-1 loss: 1.087967  [   64/  124]
train() client id: f_00002-7-2 loss: 0.827720  [   96/  124]
train() client id: f_00002-8-0 loss: 0.866047  [   32/  124]
train() client id: f_00002-8-1 loss: 0.685919  [   64/  124]
train() client id: f_00002-8-2 loss: 1.231768  [   96/  124]
train() client id: f_00002-9-0 loss: 0.896778  [   32/  124]
train() client id: f_00002-9-1 loss: 0.942895  [   64/  124]
train() client id: f_00002-9-2 loss: 1.001101  [   96/  124]
train() client id: f_00002-10-0 loss: 0.869568  [   32/  124]
train() client id: f_00002-10-1 loss: 0.915110  [   64/  124]
train() client id: f_00002-10-2 loss: 1.098981  [   96/  124]
train() client id: f_00002-11-0 loss: 1.014768  [   32/  124]
train() client id: f_00002-11-1 loss: 0.802034  [   64/  124]
train() client id: f_00002-11-2 loss: 0.772957  [   96/  124]
train() client id: f_00002-12-0 loss: 1.035771  [   32/  124]
train() client id: f_00002-12-1 loss: 0.762012  [   64/  124]
train() client id: f_00002-12-2 loss: 0.753381  [   96/  124]
train() client id: f_00002-13-0 loss: 1.122455  [   32/  124]
train() client id: f_00002-13-1 loss: 0.907415  [   64/  124]
train() client id: f_00002-13-2 loss: 0.818326  [   96/  124]
train() client id: f_00002-14-0 loss: 0.740447  [   32/  124]
train() client id: f_00002-14-1 loss: 0.702447  [   64/  124]
train() client id: f_00002-14-2 loss: 0.973281  [   96/  124]
train() client id: f_00003-0-0 loss: 0.884421  [   32/   43]
train() client id: f_00003-1-0 loss: 0.806078  [   32/   43]
train() client id: f_00003-2-0 loss: 0.608067  [   32/   43]
train() client id: f_00003-3-0 loss: 0.596060  [   32/   43]
train() client id: f_00003-4-0 loss: 0.964630  [   32/   43]
train() client id: f_00003-5-0 loss: 0.679882  [   32/   43]
train() client id: f_00003-6-0 loss: 0.897221  [   32/   43]
train() client id: f_00003-7-0 loss: 0.761744  [   32/   43]
train() client id: f_00003-8-0 loss: 0.723926  [   32/   43]
train() client id: f_00003-9-0 loss: 0.688576  [   32/   43]
train() client id: f_00003-10-0 loss: 0.972028  [   32/   43]
train() client id: f_00003-11-0 loss: 0.800378  [   32/   43]
train() client id: f_00003-12-0 loss: 0.637110  [   32/   43]
train() client id: f_00003-13-0 loss: 0.863937  [   32/   43]
train() client id: f_00003-14-0 loss: 0.719638  [   32/   43]
train() client id: f_00004-0-0 loss: 1.000712  [   32/  306]
train() client id: f_00004-0-1 loss: 1.055527  [   64/  306]
train() client id: f_00004-0-2 loss: 0.960979  [   96/  306]
train() client id: f_00004-0-3 loss: 1.113423  [  128/  306]
train() client id: f_00004-0-4 loss: 1.023170  [  160/  306]
train() client id: f_00004-0-5 loss: 0.984412  [  192/  306]
train() client id: f_00004-0-6 loss: 0.913240  [  224/  306]
train() client id: f_00004-0-7 loss: 1.092345  [  256/  306]
train() client id: f_00004-0-8 loss: 0.999570  [  288/  306]
train() client id: f_00004-1-0 loss: 1.093496  [   32/  306]
train() client id: f_00004-1-1 loss: 1.137375  [   64/  306]
train() client id: f_00004-1-2 loss: 0.971199  [   96/  306]
train() client id: f_00004-1-3 loss: 1.004172  [  128/  306]
train() client id: f_00004-1-4 loss: 0.907126  [  160/  306]
train() client id: f_00004-1-5 loss: 0.977784  [  192/  306]
train() client id: f_00004-1-6 loss: 1.084235  [  224/  306]
train() client id: f_00004-1-7 loss: 0.871080  [  256/  306]
train() client id: f_00004-1-8 loss: 1.023667  [  288/  306]
train() client id: f_00004-2-0 loss: 0.919078  [   32/  306]
train() client id: f_00004-2-1 loss: 1.179903  [   64/  306]
train() client id: f_00004-2-2 loss: 0.779074  [   96/  306]
train() client id: f_00004-2-3 loss: 1.113966  [  128/  306]
train() client id: f_00004-2-4 loss: 0.999233  [  160/  306]
train() client id: f_00004-2-5 loss: 0.933440  [  192/  306]
train() client id: f_00004-2-6 loss: 0.995647  [  224/  306]
train() client id: f_00004-2-7 loss: 1.154761  [  256/  306]
train() client id: f_00004-2-8 loss: 0.894204  [  288/  306]
train() client id: f_00004-3-0 loss: 1.021793  [   32/  306]
train() client id: f_00004-3-1 loss: 1.108172  [   64/  306]
train() client id: f_00004-3-2 loss: 0.956218  [   96/  306]
train() client id: f_00004-3-3 loss: 1.031994  [  128/  306]
train() client id: f_00004-3-4 loss: 0.879157  [  160/  306]
train() client id: f_00004-3-5 loss: 0.965031  [  192/  306]
train() client id: f_00004-3-6 loss: 1.061005  [  224/  306]
train() client id: f_00004-3-7 loss: 1.003506  [  256/  306]
train() client id: f_00004-3-8 loss: 0.970700  [  288/  306]
train() client id: f_00004-4-0 loss: 0.945283  [   32/  306]
train() client id: f_00004-4-1 loss: 1.082747  [   64/  306]
train() client id: f_00004-4-2 loss: 1.002725  [   96/  306]
train() client id: f_00004-4-3 loss: 0.868822  [  128/  306]
train() client id: f_00004-4-4 loss: 0.948761  [  160/  306]
train() client id: f_00004-4-5 loss: 1.149653  [  192/  306]
train() client id: f_00004-4-6 loss: 1.027197  [  224/  306]
train() client id: f_00004-4-7 loss: 1.029963  [  256/  306]
train() client id: f_00004-4-8 loss: 0.917890  [  288/  306]
train() client id: f_00004-5-0 loss: 0.974032  [   32/  306]
train() client id: f_00004-5-1 loss: 0.970940  [   64/  306]
train() client id: f_00004-5-2 loss: 1.030140  [   96/  306]
train() client id: f_00004-5-3 loss: 0.893338  [  128/  306]
train() client id: f_00004-5-4 loss: 0.895133  [  160/  306]
train() client id: f_00004-5-5 loss: 1.050948  [  192/  306]
train() client id: f_00004-5-6 loss: 1.104298  [  224/  306]
train() client id: f_00004-5-7 loss: 0.970985  [  256/  306]
train() client id: f_00004-5-8 loss: 1.113707  [  288/  306]
train() client id: f_00004-6-0 loss: 0.900520  [   32/  306]
train() client id: f_00004-6-1 loss: 0.961315  [   64/  306]
train() client id: f_00004-6-2 loss: 1.118474  [   96/  306]
train() client id: f_00004-6-3 loss: 0.907343  [  128/  306]
train() client id: f_00004-6-4 loss: 0.938784  [  160/  306]
train() client id: f_00004-6-5 loss: 1.048724  [  192/  306]
train() client id: f_00004-6-6 loss: 0.992479  [  224/  306]
train() client id: f_00004-6-7 loss: 1.072890  [  256/  306]
train() client id: f_00004-6-8 loss: 0.969135  [  288/  306]
train() client id: f_00004-7-0 loss: 1.014036  [   32/  306]
train() client id: f_00004-7-1 loss: 0.954801  [   64/  306]
train() client id: f_00004-7-2 loss: 0.961657  [   96/  306]
train() client id: f_00004-7-3 loss: 0.969585  [  128/  306]
train() client id: f_00004-7-4 loss: 0.900920  [  160/  306]
train() client id: f_00004-7-5 loss: 1.090609  [  192/  306]
train() client id: f_00004-7-6 loss: 0.912503  [  224/  306]
train() client id: f_00004-7-7 loss: 1.011082  [  256/  306]
train() client id: f_00004-7-8 loss: 0.921643  [  288/  306]
train() client id: f_00004-8-0 loss: 0.962453  [   32/  306]
train() client id: f_00004-8-1 loss: 0.996360  [   64/  306]
train() client id: f_00004-8-2 loss: 1.016214  [   96/  306]
train() client id: f_00004-8-3 loss: 0.941661  [  128/  306]
train() client id: f_00004-8-4 loss: 0.944740  [  160/  306]
train() client id: f_00004-8-5 loss: 0.986091  [  192/  306]
train() client id: f_00004-8-6 loss: 1.055192  [  224/  306]
train() client id: f_00004-8-7 loss: 0.948420  [  256/  306]
train() client id: f_00004-8-8 loss: 1.044859  [  288/  306]
train() client id: f_00004-9-0 loss: 0.855733  [   32/  306]
train() client id: f_00004-9-1 loss: 1.142878  [   64/  306]
train() client id: f_00004-9-2 loss: 0.944645  [   96/  306]
train() client id: f_00004-9-3 loss: 0.853974  [  128/  306]
train() client id: f_00004-9-4 loss: 0.896929  [  160/  306]
train() client id: f_00004-9-5 loss: 0.909045  [  192/  306]
train() client id: f_00004-9-6 loss: 0.926583  [  224/  306]
train() client id: f_00004-9-7 loss: 1.057882  [  256/  306]
train() client id: f_00004-9-8 loss: 1.086569  [  288/  306]
train() client id: f_00004-10-0 loss: 0.935555  [   32/  306]
train() client id: f_00004-10-1 loss: 0.977439  [   64/  306]
train() client id: f_00004-10-2 loss: 1.108924  [   96/  306]
train() client id: f_00004-10-3 loss: 0.949503  [  128/  306]
train() client id: f_00004-10-4 loss: 0.971929  [  160/  306]
train() client id: f_00004-10-5 loss: 1.089440  [  192/  306]
train() client id: f_00004-10-6 loss: 0.862704  [  224/  306]
train() client id: f_00004-10-7 loss: 0.915814  [  256/  306]
train() client id: f_00004-10-8 loss: 0.898106  [  288/  306]
train() client id: f_00004-11-0 loss: 0.919144  [   32/  306]
train() client id: f_00004-11-1 loss: 0.958510  [   64/  306]
train() client id: f_00004-11-2 loss: 1.001009  [   96/  306]
train() client id: f_00004-11-3 loss: 0.973143  [  128/  306]
train() client id: f_00004-11-4 loss: 0.959216  [  160/  306]
train() client id: f_00004-11-5 loss: 0.985440  [  192/  306]
train() client id: f_00004-11-6 loss: 0.975706  [  224/  306]
train() client id: f_00004-11-7 loss: 0.986778  [  256/  306]
train() client id: f_00004-11-8 loss: 1.019127  [  288/  306]
train() client id: f_00004-12-0 loss: 1.016766  [   32/  306]
train() client id: f_00004-12-1 loss: 0.857567  [   64/  306]
train() client id: f_00004-12-2 loss: 0.946260  [   96/  306]
train() client id: f_00004-12-3 loss: 1.078495  [  128/  306]
train() client id: f_00004-12-4 loss: 0.917292  [  160/  306]
train() client id: f_00004-12-5 loss: 0.975118  [  192/  306]
train() client id: f_00004-12-6 loss: 1.053669  [  224/  306]
train() client id: f_00004-12-7 loss: 0.898305  [  256/  306]
train() client id: f_00004-12-8 loss: 0.952053  [  288/  306]
train() client id: f_00004-13-0 loss: 0.987895  [   32/  306]
train() client id: f_00004-13-1 loss: 1.064291  [   64/  306]
train() client id: f_00004-13-2 loss: 0.942937  [   96/  306]
train() client id: f_00004-13-3 loss: 0.880089  [  128/  306]
train() client id: f_00004-13-4 loss: 0.970465  [  160/  306]
train() client id: f_00004-13-5 loss: 0.974792  [  192/  306]
train() client id: f_00004-13-6 loss: 1.023704  [  224/  306]
train() client id: f_00004-13-7 loss: 0.868895  [  256/  306]
train() client id: f_00004-13-8 loss: 0.919309  [  288/  306]
train() client id: f_00004-14-0 loss: 0.931390  [   32/  306]
train() client id: f_00004-14-1 loss: 0.974938  [   64/  306]
train() client id: f_00004-14-2 loss: 0.970366  [   96/  306]
train() client id: f_00004-14-3 loss: 0.997222  [  128/  306]
train() client id: f_00004-14-4 loss: 0.953250  [  160/  306]
train() client id: f_00004-14-5 loss: 1.027820  [  192/  306]
train() client id: f_00004-14-6 loss: 0.927931  [  224/  306]
train() client id: f_00004-14-7 loss: 0.928596  [  256/  306]
train() client id: f_00004-14-8 loss: 0.902413  [  288/  306]
train() client id: f_00005-0-0 loss: 0.882816  [   32/  146]
train() client id: f_00005-0-1 loss: 0.567995  [   64/  146]
train() client id: f_00005-0-2 loss: 0.537500  [   96/  146]
train() client id: f_00005-0-3 loss: 0.748814  [  128/  146]
train() client id: f_00005-1-0 loss: 0.440626  [   32/  146]
train() client id: f_00005-1-1 loss: 0.594624  [   64/  146]
train() client id: f_00005-1-2 loss: 0.696588  [   96/  146]
train() client id: f_00005-1-3 loss: 1.021598  [  128/  146]
train() client id: f_00005-2-0 loss: 0.794696  [   32/  146]
train() client id: f_00005-2-1 loss: 0.454172  [   64/  146]
train() client id: f_00005-2-2 loss: 0.839377  [   96/  146]
train() client id: f_00005-2-3 loss: 0.641791  [  128/  146]
train() client id: f_00005-3-0 loss: 0.506958  [   32/  146]
train() client id: f_00005-3-1 loss: 0.853884  [   64/  146]
train() client id: f_00005-3-2 loss: 0.774989  [   96/  146]
train() client id: f_00005-3-3 loss: 0.382085  [  128/  146]
train() client id: f_00005-4-0 loss: 0.713935  [   32/  146]
train() client id: f_00005-4-1 loss: 0.629279  [   64/  146]
train() client id: f_00005-4-2 loss: 0.894771  [   96/  146]
train() client id: f_00005-4-3 loss: 0.595041  [  128/  146]
train() client id: f_00005-5-0 loss: 0.458760  [   32/  146]
train() client id: f_00005-5-1 loss: 0.908310  [   64/  146]
train() client id: f_00005-5-2 loss: 0.892015  [   96/  146]
train() client id: f_00005-5-3 loss: 0.498299  [  128/  146]
train() client id: f_00005-6-0 loss: 0.695157  [   32/  146]
train() client id: f_00005-6-1 loss: 0.613698  [   64/  146]
train() client id: f_00005-6-2 loss: 0.785815  [   96/  146]
train() client id: f_00005-6-3 loss: 0.714460  [  128/  146]
train() client id: f_00005-7-0 loss: 0.783652  [   32/  146]
train() client id: f_00005-7-1 loss: 0.524774  [   64/  146]
train() client id: f_00005-7-2 loss: 0.774972  [   96/  146]
train() client id: f_00005-7-3 loss: 0.499820  [  128/  146]
train() client id: f_00005-8-0 loss: 0.583772  [   32/  146]
train() client id: f_00005-8-1 loss: 0.820709  [   64/  146]
train() client id: f_00005-8-2 loss: 0.529483  [   96/  146]
train() client id: f_00005-8-3 loss: 0.816861  [  128/  146]
train() client id: f_00005-9-0 loss: 0.672306  [   32/  146]
train() client id: f_00005-9-1 loss: 0.504493  [   64/  146]
train() client id: f_00005-9-2 loss: 0.782244  [   96/  146]
train() client id: f_00005-9-3 loss: 0.805419  [  128/  146]
train() client id: f_00005-10-0 loss: 0.820260  [   32/  146]
train() client id: f_00005-10-1 loss: 0.737431  [   64/  146]
train() client id: f_00005-10-2 loss: 0.450500  [   96/  146]
train() client id: f_00005-10-3 loss: 0.716998  [  128/  146]
train() client id: f_00005-11-0 loss: 0.739318  [   32/  146]
train() client id: f_00005-11-1 loss: 0.448075  [   64/  146]
train() client id: f_00005-11-2 loss: 0.471299  [   96/  146]
train() client id: f_00005-11-3 loss: 0.821864  [  128/  146]
train() client id: f_00005-12-0 loss: 0.842803  [   32/  146]
train() client id: f_00005-12-1 loss: 0.789666  [   64/  146]
train() client id: f_00005-12-2 loss: 0.443778  [   96/  146]
train() client id: f_00005-12-3 loss: 0.626179  [  128/  146]
train() client id: f_00005-13-0 loss: 0.644083  [   32/  146]
train() client id: f_00005-13-1 loss: 0.810259  [   64/  146]
train() client id: f_00005-13-2 loss: 0.651147  [   96/  146]
train() client id: f_00005-13-3 loss: 0.694611  [  128/  146]
train() client id: f_00005-14-0 loss: 0.629657  [   32/  146]
train() client id: f_00005-14-1 loss: 0.749283  [   64/  146]
train() client id: f_00005-14-2 loss: 0.509189  [   96/  146]
train() client id: f_00005-14-3 loss: 0.648314  [  128/  146]
train() client id: f_00006-0-0 loss: 0.471678  [   32/   54]
train() client id: f_00006-1-0 loss: 0.525773  [   32/   54]
train() client id: f_00006-2-0 loss: 0.547486  [   32/   54]
train() client id: f_00006-3-0 loss: 0.546526  [   32/   54]
train() client id: f_00006-4-0 loss: 0.529124  [   32/   54]
train() client id: f_00006-5-0 loss: 0.546059  [   32/   54]
train() client id: f_00006-6-0 loss: 0.481500  [   32/   54]
train() client id: f_00006-7-0 loss: 0.520310  [   32/   54]
train() client id: f_00006-8-0 loss: 0.576136  [   32/   54]
train() client id: f_00006-9-0 loss: 0.557887  [   32/   54]
train() client id: f_00006-10-0 loss: 0.531878  [   32/   54]
train() client id: f_00006-11-0 loss: 0.569392  [   32/   54]
train() client id: f_00006-12-0 loss: 0.543367  [   32/   54]
train() client id: f_00006-13-0 loss: 0.608373  [   32/   54]
train() client id: f_00006-14-0 loss: 0.563258  [   32/   54]
train() client id: f_00007-0-0 loss: 0.896211  [   32/  179]
train() client id: f_00007-0-1 loss: 0.604165  [   64/  179]
train() client id: f_00007-0-2 loss: 0.607961  [   96/  179]
train() client id: f_00007-0-3 loss: 0.608722  [  128/  179]
train() client id: f_00007-0-4 loss: 0.762991  [  160/  179]
train() client id: f_00007-1-0 loss: 0.613997  [   32/  179]
train() client id: f_00007-1-1 loss: 0.801156  [   64/  179]
train() client id: f_00007-1-2 loss: 0.691466  [   96/  179]
train() client id: f_00007-1-3 loss: 0.700638  [  128/  179]
train() client id: f_00007-1-4 loss: 0.596295  [  160/  179]
train() client id: f_00007-2-0 loss: 0.630529  [   32/  179]
train() client id: f_00007-2-1 loss: 0.747649  [   64/  179]
train() client id: f_00007-2-2 loss: 0.546666  [   96/  179]
train() client id: f_00007-2-3 loss: 0.787482  [  128/  179]
train() client id: f_00007-2-4 loss: 0.678506  [  160/  179]
train() client id: f_00007-3-0 loss: 0.683407  [   32/  179]
train() client id: f_00007-3-1 loss: 0.553218  [   64/  179]
train() client id: f_00007-3-2 loss: 0.659200  [   96/  179]
train() client id: f_00007-3-3 loss: 0.579429  [  128/  179]
train() client id: f_00007-3-4 loss: 0.579110  [  160/  179]
train() client id: f_00007-4-0 loss: 0.869825  [   32/  179]
train() client id: f_00007-4-1 loss: 0.464990  [   64/  179]
train() client id: f_00007-4-2 loss: 0.648167  [   96/  179]
train() client id: f_00007-4-3 loss: 0.625958  [  128/  179]
train() client id: f_00007-4-4 loss: 0.648883  [  160/  179]
train() client id: f_00007-5-0 loss: 0.538246  [   32/  179]
train() client id: f_00007-5-1 loss: 0.677096  [   64/  179]
train() client id: f_00007-5-2 loss: 0.681476  [   96/  179]
train() client id: f_00007-5-3 loss: 0.511054  [  128/  179]
train() client id: f_00007-5-4 loss: 0.828252  [  160/  179]
train() client id: f_00007-6-0 loss: 0.631670  [   32/  179]
train() client id: f_00007-6-1 loss: 0.834365  [   64/  179]
train() client id: f_00007-6-2 loss: 0.521410  [   96/  179]
train() client id: f_00007-6-3 loss: 0.644507  [  128/  179]
train() client id: f_00007-6-4 loss: 0.552153  [  160/  179]
train() client id: f_00007-7-0 loss: 0.745759  [   32/  179]
train() client id: f_00007-7-1 loss: 0.854856  [   64/  179]
train() client id: f_00007-7-2 loss: 0.561374  [   96/  179]
train() client id: f_00007-7-3 loss: 0.495774  [  128/  179]
train() client id: f_00007-7-4 loss: 0.538299  [  160/  179]
train() client id: f_00007-8-0 loss: 0.674206  [   32/  179]
train() client id: f_00007-8-1 loss: 0.561812  [   64/  179]
train() client id: f_00007-8-2 loss: 0.696044  [   96/  179]
train() client id: f_00007-8-3 loss: 0.676268  [  128/  179]
train() client id: f_00007-8-4 loss: 0.653226  [  160/  179]
train() client id: f_00007-9-0 loss: 0.609954  [   32/  179]
train() client id: f_00007-9-1 loss: 0.685252  [   64/  179]
train() client id: f_00007-9-2 loss: 0.857659  [   96/  179]
train() client id: f_00007-9-3 loss: 0.495476  [  128/  179]
train() client id: f_00007-9-4 loss: 0.629136  [  160/  179]
train() client id: f_00007-10-0 loss: 0.704665  [   32/  179]
train() client id: f_00007-10-1 loss: 0.639735  [   64/  179]
train() client id: f_00007-10-2 loss: 0.664981  [   96/  179]
train() client id: f_00007-10-3 loss: 0.612581  [  128/  179]
train() client id: f_00007-10-4 loss: 0.565645  [  160/  179]
train() client id: f_00007-11-0 loss: 0.820536  [   32/  179]
train() client id: f_00007-11-1 loss: 0.759673  [   64/  179]
train() client id: f_00007-11-2 loss: 0.501444  [   96/  179]
train() client id: f_00007-11-3 loss: 0.549388  [  128/  179]
train() client id: f_00007-11-4 loss: 0.623087  [  160/  179]
train() client id: f_00007-12-0 loss: 0.517025  [   32/  179]
train() client id: f_00007-12-1 loss: 0.585531  [   64/  179]
train() client id: f_00007-12-2 loss: 0.652663  [   96/  179]
train() client id: f_00007-12-3 loss: 0.592200  [  128/  179]
train() client id: f_00007-12-4 loss: 0.577763  [  160/  179]
train() client id: f_00007-13-0 loss: 0.584147  [   32/  179]
train() client id: f_00007-13-1 loss: 0.631971  [   64/  179]
train() client id: f_00007-13-2 loss: 0.589324  [   96/  179]
train() client id: f_00007-13-3 loss: 0.814988  [  128/  179]
train() client id: f_00007-13-4 loss: 0.568157  [  160/  179]
train() client id: f_00007-14-0 loss: 0.559193  [   32/  179]
train() client id: f_00007-14-1 loss: 0.556477  [   64/  179]
train() client id: f_00007-14-2 loss: 0.595190  [   96/  179]
train() client id: f_00007-14-3 loss: 0.593511  [  128/  179]
train() client id: f_00007-14-4 loss: 1.009344  [  160/  179]
train() client id: f_00008-0-0 loss: 0.748364  [   32/  130]
train() client id: f_00008-0-1 loss: 0.596912  [   64/  130]
train() client id: f_00008-0-2 loss: 0.654721  [   96/  130]
train() client id: f_00008-0-3 loss: 0.642327  [  128/  130]
train() client id: f_00008-1-0 loss: 0.700558  [   32/  130]
train() client id: f_00008-1-1 loss: 0.641046  [   64/  130]
train() client id: f_00008-1-2 loss: 0.729528  [   96/  130]
train() client id: f_00008-1-3 loss: 0.605552  [  128/  130]
train() client id: f_00008-2-0 loss: 0.673800  [   32/  130]
train() client id: f_00008-2-1 loss: 0.632065  [   64/  130]
train() client id: f_00008-2-2 loss: 0.627540  [   96/  130]
train() client id: f_00008-2-3 loss: 0.735014  [  128/  130]
train() client id: f_00008-3-0 loss: 0.638551  [   32/  130]
train() client id: f_00008-3-1 loss: 0.628042  [   64/  130]
train() client id: f_00008-3-2 loss: 0.665437  [   96/  130]
train() client id: f_00008-3-3 loss: 0.737853  [  128/  130]
train() client id: f_00008-4-0 loss: 0.668357  [   32/  130]
train() client id: f_00008-4-1 loss: 0.583710  [   64/  130]
train() client id: f_00008-4-2 loss: 0.660198  [   96/  130]
train() client id: f_00008-4-3 loss: 0.731162  [  128/  130]
train() client id: f_00008-5-0 loss: 0.738768  [   32/  130]
train() client id: f_00008-5-1 loss: 0.691969  [   64/  130]
train() client id: f_00008-5-2 loss: 0.589443  [   96/  130]
train() client id: f_00008-5-3 loss: 0.652669  [  128/  130]
train() client id: f_00008-6-0 loss: 0.533678  [   32/  130]
train() client id: f_00008-6-1 loss: 0.665178  [   64/  130]
train() client id: f_00008-6-2 loss: 0.696434  [   96/  130]
train() client id: f_00008-6-3 loss: 0.779019  [  128/  130]
train() client id: f_00008-7-0 loss: 0.663713  [   32/  130]
train() client id: f_00008-7-1 loss: 0.704416  [   64/  130]
train() client id: f_00008-7-2 loss: 0.639943  [   96/  130]
train() client id: f_00008-7-3 loss: 0.657816  [  128/  130]
train() client id: f_00008-8-0 loss: 0.729478  [   32/  130]
train() client id: f_00008-8-1 loss: 0.596147  [   64/  130]
train() client id: f_00008-8-2 loss: 0.680552  [   96/  130]
train() client id: f_00008-8-3 loss: 0.663986  [  128/  130]
train() client id: f_00008-9-0 loss: 0.597601  [   32/  130]
train() client id: f_00008-9-1 loss: 0.622059  [   64/  130]
train() client id: f_00008-9-2 loss: 0.706946  [   96/  130]
train() client id: f_00008-9-3 loss: 0.685571  [  128/  130]
train() client id: f_00008-10-0 loss: 0.783298  [   32/  130]
train() client id: f_00008-10-1 loss: 0.610727  [   64/  130]
train() client id: f_00008-10-2 loss: 0.720039  [   96/  130]
train() client id: f_00008-10-3 loss: 0.553577  [  128/  130]
train() client id: f_00008-11-0 loss: 0.669529  [   32/  130]
train() client id: f_00008-11-1 loss: 0.538874  [   64/  130]
train() client id: f_00008-11-2 loss: 0.753610  [   96/  130]
train() client id: f_00008-11-3 loss: 0.705336  [  128/  130]
train() client id: f_00008-12-0 loss: 0.632239  [   32/  130]
train() client id: f_00008-12-1 loss: 0.745650  [   64/  130]
train() client id: f_00008-12-2 loss: 0.673888  [   96/  130]
train() client id: f_00008-12-3 loss: 0.594293  [  128/  130]
train() client id: f_00008-13-0 loss: 0.676610  [   32/  130]
train() client id: f_00008-13-1 loss: 0.704460  [   64/  130]
train() client id: f_00008-13-2 loss: 0.698489  [   96/  130]
train() client id: f_00008-13-3 loss: 0.596060  [  128/  130]
train() client id: f_00008-14-0 loss: 0.714745  [   32/  130]
train() client id: f_00008-14-1 loss: 0.674129  [   64/  130]
train() client id: f_00008-14-2 loss: 0.604062  [   96/  130]
train() client id: f_00008-14-3 loss: 0.666037  [  128/  130]
train() client id: f_00009-0-0 loss: 1.027169  [   32/  118]
train() client id: f_00009-0-1 loss: 0.933443  [   64/  118]
train() client id: f_00009-0-2 loss: 1.004804  [   96/  118]
train() client id: f_00009-1-0 loss: 0.898537  [   32/  118]
train() client id: f_00009-1-1 loss: 0.882130  [   64/  118]
train() client id: f_00009-1-2 loss: 0.961086  [   96/  118]
train() client id: f_00009-2-0 loss: 1.063587  [   32/  118]
train() client id: f_00009-2-1 loss: 0.896871  [   64/  118]
train() client id: f_00009-2-2 loss: 0.794417  [   96/  118]
train() client id: f_00009-3-0 loss: 0.790537  [   32/  118]
train() client id: f_00009-3-1 loss: 0.701333  [   64/  118]
train() client id: f_00009-3-2 loss: 0.879577  [   96/  118]
train() client id: f_00009-4-0 loss: 0.864120  [   32/  118]
train() client id: f_00009-4-1 loss: 0.826627  [   64/  118]
train() client id: f_00009-4-2 loss: 0.700528  [   96/  118]
train() client id: f_00009-5-0 loss: 0.694270  [   32/  118]
train() client id: f_00009-5-1 loss: 0.628210  [   64/  118]
train() client id: f_00009-5-2 loss: 0.965650  [   96/  118]
train() client id: f_00009-6-0 loss: 0.745205  [   32/  118]
train() client id: f_00009-6-1 loss: 0.639720  [   64/  118]
train() client id: f_00009-6-2 loss: 0.850741  [   96/  118]
train() client id: f_00009-7-0 loss: 0.597033  [   32/  118]
train() client id: f_00009-7-1 loss: 0.792087  [   64/  118]
train() client id: f_00009-7-2 loss: 0.722781  [   96/  118]
train() client id: f_00009-8-0 loss: 0.807006  [   32/  118]
train() client id: f_00009-8-1 loss: 0.625263  [   64/  118]
train() client id: f_00009-8-2 loss: 0.566595  [   96/  118]
train() client id: f_00009-9-0 loss: 0.674819  [   32/  118]
train() client id: f_00009-9-1 loss: 0.654433  [   64/  118]
train() client id: f_00009-9-2 loss: 0.742816  [   96/  118]
train() client id: f_00009-10-0 loss: 0.619341  [   32/  118]
train() client id: f_00009-10-1 loss: 0.554114  [   64/  118]
train() client id: f_00009-10-2 loss: 0.731923  [   96/  118]
train() client id: f_00009-11-0 loss: 0.804559  [   32/  118]
train() client id: f_00009-11-1 loss: 0.641817  [   64/  118]
train() client id: f_00009-11-2 loss: 0.420288  [   96/  118]
train() client id: f_00009-12-0 loss: 0.604676  [   32/  118]
train() client id: f_00009-12-1 loss: 0.681472  [   64/  118]
train() client id: f_00009-12-2 loss: 0.611677  [   96/  118]
train() client id: f_00009-13-0 loss: 0.554550  [   32/  118]
train() client id: f_00009-13-1 loss: 0.575914  [   64/  118]
train() client id: f_00009-13-2 loss: 0.778804  [   96/  118]
train() client id: f_00009-14-0 loss: 0.658629  [   32/  118]
train() client id: f_00009-14-1 loss: 0.581406  [   64/  118]
train() client id: f_00009-14-2 loss: 0.441753  [   96/  118]
At round 68 accuracy: 0.6472148541114059
At round 68 training accuracy: 0.5922199865861838
At round 68 training loss: 0.8336668788471457
update_location
xs = 8.927491 461.223621 5.882650 0.934260 -377.581990 -225.230757 -185.849135 -5.143845 -400.120581 20.134486 
ys = -452.390647 7.291448 350.684448 -172.290817 -9.642386 0.794442 -1.381692 346.628436 25.881276 -887.232496 
xs mean: -69.68237997052123
ys mean: -79.16579882624052
dists_uav = 463.397235 471.996180 364.711102 199.210940 390.718741 246.433612 211.049307 360.801513 413.238817 893.077208 
uav_gains = -123.369181 -123.595170 -119.925816 -107.632327 -121.051507 -110.995893 -108.389836 -119.734981 -121.870101 -130.746340 
uav_gains_db_mean: -118.73111542319164
dists_bs = 653.795340 657.956423 252.157805 389.311926 274.102669 181.302800 176.715039 241.469322 270.029017 1080.007103 
bs_gains = -118.399430 -118.476579 -106.813887 -112.095395 -107.828631 -102.802385 -102.490717 -106.287194 -107.646551 -124.503004 
bs_gains_db_mean: -110.73437738819635
Round 69
-------------------------------
ene_coms = [0.04546966 0.04716351 0.00876834 0.00919565 0.00930413 0.01146027
 0.00962815 0.008513   0.00920343 0.05486807]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.95416095 3.8873386  1.75789306 0.63786112 4.2813786  2.07499599
 0.79221718 2.51856846 1.84300208 1.88651234]
obj_prev = 21.63392836103094
eta_min = 2.0564566798057582e-50	eta_max = 0.9054748167460231
af = 4.439896748873053	bf = 0.9711089406449835	zeta = 4.883886423760359	eta = 0.9090909090909091
af = 4.439896748873053	bf = 0.9711089406449835	zeta = 15.337039279019827	eta = 0.2894885165317776
af = 4.439896748873053	bf = 0.9711089406449835	zeta = 9.11312644490464	eta = 0.4871979748898911
af = 4.439896748873053	bf = 0.9711089406449835	zeta = 8.119645874648127	eta = 0.5468091610664562
af = 4.439896748873053	bf = 0.9711089406449835	zeta = 8.056819258116448	eta = 0.5510731476817352
af = 4.439896748873053	bf = 0.9711089406449835	zeta = 8.056521661505968	eta = 0.5510935035509014
af = 4.439896748873053	bf = 0.9711089406449835	zeta = 8.056521654758726	eta = 0.5510935040124357
eta = 0.5510935040124357
ene_coms = [0.04546966 0.04716351 0.00876834 0.00919565 0.00930413 0.01146027
 0.00962815 0.008513   0.00920343 0.05486807]
ene_comp = [0.04916818 0.10340926 0.04838773 0.01677962 0.11940843 0.05697265
 0.02107208 0.06985003 0.05072907 0.04604639]
ene_total = [0.95857938 1.52514007 0.57892943 0.26310147 1.30371965 0.69315178
 0.31096025 0.79373314 0.6070517  1.02215479]
ti_comp = [2.16013553 2.143197   2.52714874 2.52287562 2.52179083 2.50022944
 2.51855059 2.5297021  2.52279783 2.06615143]
ti_coms = [0.45469658 0.47163511 0.08768337 0.09195649 0.09304128 0.11460267
 0.09628151 0.08513001 0.09203428 0.54868068]
t_total = [26.48543358 26.48543358 26.48543358 26.48543358 26.48543358 26.48543358
 26.48543358 26.48543358 26.48543358 26.48543358]
ene_coms = [0.04546966 0.04716351 0.00876834 0.00919565 0.00930413 0.01146027
 0.00962815 0.008513   0.00920343 0.05486807]
ene_comp = [1.59210014e-06 1.50464546e-05 1.10872590e-06 4.63910744e-08
 1.67327337e-05 1.84892623e-06 9.21935768e-08 3.32844262e-06
 1.28199374e-06 1.42936279e-06]
ene_total = [0.4605748  0.47786798 0.08882505 0.09314249 0.09441028 0.11609889
 0.09752374 0.08626125 0.0932338  0.55576893]
optimize_network iter = 0 obj = 2.1637072030715276
eta = 0.5510935040124357
freqs = [11380808.76214261 24125002.15185529  9573582.01581491  3325494.20676815
 23675324.74062835 11393484.64786778  4183373.68481425 13805979.67932191
 10054129.70150102 11143033.59064256]
eta_min = 0.5510935040124377	eta_max = 0.6418119912459638
af = 0.00032437354052540106	bf = 0.9711089406449835	zeta = 0.0003568108945779412	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.04546966 0.04716351 0.00876834 0.00919565 0.00930413 0.01146027
 0.00962815 0.008513   0.00920343 0.05486807]
ene_comp = [2.54666696e-07 2.40677755e-06 1.77347866e-07 7.42055185e-09
 2.67650878e-06 2.95747686e-07 1.47469578e-08 5.32405885e-07
 2.05063175e-07 2.28635805e-07]
ene_total = [2.27423834 2.35906626 0.43856896 0.45993301 0.46549222 0.57321533
 0.48156557 0.42581572 0.46033197 2.74431104]
ti_comp = [1.63171008 1.61477156 1.99872329 1.99445018 1.99336539 1.971804
 1.99012515 2.00127666 1.99437239 1.53772599]
ti_coms = [0.45469658 0.47163511 0.08768337 0.09195649 0.09304128 0.11460267
 0.09628151 0.08513001 0.09203428 0.54868068]
t_total = [26.48543358 26.48543358 26.48543358 26.48543358 26.48543358 26.48543358
 26.48543358 26.48543358 26.48543358 26.48543358]
ene_coms = [0.04546966 0.04716351 0.00876834 0.00919565 0.00930413 0.01146027
 0.00962815 0.008513   0.00920343 0.05486807]
ene_comp = [1.15026208e-06 1.09266391e-05 7.30686141e-07 3.06006143e-08
 1.10398104e-05 1.22546876e-06 6.08683689e-08 2.19238083e-06
 8.45646222e-07 1.06379826e-06]
ene_total = [0.57721924 0.59884559 0.11131702 0.11673254 0.11824936 0.14549541
 0.12222324 0.10809426 0.11684164 0.69652421]
optimize_network iter = 1 obj = 2.711542509430748
eta = 0.6418119912459638
freqs = [11213157.78899957 23830610.46558484  9008850.85965204  3130730.24878866
 22291274.42324039 10752006.02784285  3940159.08631173 12988119.83398787
  9465367.60525531 11143033.59064257]
eta_min = 0.6418119912459641	eta_max = 0.6418119912459627
af = 0.0003000755421825744	bf = 0.9711089406449835	zeta = 0.0003300830964008319	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.04546966 0.04716351 0.00876834 0.00919565 0.00930413 0.01146027
 0.00962815 0.008513   0.00920343 0.05486807]
ene_comp = [2.47218957e-07 2.34839727e-06 1.57042007e-07 6.57680721e-09
 2.37272049e-06 2.63382679e-07 1.30820749e-08 4.71195313e-07
 1.81749690e-07 2.28635805e-07]
ene_total = [2.27423796 2.35906334 0.43856794 0.45993297 0.46547702 0.57321372
 0.48156549 0.42581266 0.46033081 2.74431104]
ti_comp = [1.63171008 1.61477156 1.99872329 1.99445018 1.99336539 1.971804
 1.99012515 2.00127666 1.99437239 1.53772599]
ti_coms = [0.45469658 0.47163511 0.08768337 0.09195649 0.09304128 0.11460267
 0.09628151 0.08513001 0.09203428 0.54868068]
t_total = [26.48543358 26.48543358 26.48543358 26.48543358 26.48543358 26.48543358
 26.48543358 26.48543358 26.48543358 26.48543358]
ene_coms = [0.04546966 0.04716351 0.00876834 0.00919565 0.00930413 0.01146027
 0.00962815 0.008513   0.00920343 0.05486807]
ene_comp = [1.15026208e-06 1.09266391e-05 7.30686141e-07 3.06006143e-08
 1.10398104e-05 1.22546876e-06 6.08683689e-08 2.19238083e-06
 8.45646222e-07 1.06379826e-06]
ene_total = [0.57721924 0.59884559 0.11131702 0.11673254 0.11824936 0.14549541
 0.12222324 0.10809426 0.11684164 0.69652421]
optimize_network iter = 2 obj = 2.7115425094307395
eta = 0.6418119912459627
freqs = [11213157.78899957 23830610.46558484  9008850.85965205  3130730.24878867
 22291274.42324041 10752006.02784286  3940159.08631174 12988119.83398788
  9465367.60525531 11143033.59064257]
Done!
ene_coms = [0.04546966 0.04716351 0.00876834 0.00919565 0.00930413 0.01146027
 0.00962815 0.008513   0.00920343 0.05486807]
ene_comp = [1.10898189e-06 1.05345078e-05 7.04463538e-07 2.95024304e-08
 1.06436176e-05 1.18148958e-06 5.86839467e-08 2.11370145e-06
 8.15297974e-07 1.02562105e-06]
ene_total = [0.04547077 0.04717405 0.00876904 0.00919568 0.00931477 0.01146145
 0.00962821 0.00851511 0.00920424 0.05486909]
At round 69 energy consumption: 0.21360241423068407
At round 69 eta: 0.6418119912459627
At round 69 a_n: 4.546939415371483
At round 69 local rounds: 14.521129082309157
At round 69 global rounds: 12.694281506486172
gradient difference: 0.3660350739955902
train() client id: f_00000-0-0 loss: 1.269701  [   32/  126]
train() client id: f_00000-0-1 loss: 1.341058  [   64/  126]
train() client id: f_00000-0-2 loss: 1.219859  [   96/  126]
train() client id: f_00000-1-0 loss: 1.349311  [   32/  126]
train() client id: f_00000-1-1 loss: 1.065836  [   64/  126]
train() client id: f_00000-1-2 loss: 1.077005  [   96/  126]
train() client id: f_00000-2-0 loss: 0.933462  [   32/  126]
train() client id: f_00000-2-1 loss: 0.995672  [   64/  126]
train() client id: f_00000-2-2 loss: 1.167096  [   96/  126]
train() client id: f_00000-3-0 loss: 1.138590  [   32/  126]
train() client id: f_00000-3-1 loss: 0.927768  [   64/  126]
train() client id: f_00000-3-2 loss: 0.926548  [   96/  126]
train() client id: f_00000-4-0 loss: 0.775112  [   32/  126]
train() client id: f_00000-4-1 loss: 0.958591  [   64/  126]
train() client id: f_00000-4-2 loss: 0.777019  [   96/  126]
train() client id: f_00000-5-0 loss: 0.880480  [   32/  126]
train() client id: f_00000-5-1 loss: 1.045672  [   64/  126]
train() client id: f_00000-5-2 loss: 0.795607  [   96/  126]
train() client id: f_00000-6-0 loss: 0.932596  [   32/  126]
train() client id: f_00000-6-1 loss: 0.750995  [   64/  126]
train() client id: f_00000-6-2 loss: 0.872631  [   96/  126]
train() client id: f_00000-7-0 loss: 0.849869  [   32/  126]
train() client id: f_00000-7-1 loss: 0.765027  [   64/  126]
train() client id: f_00000-7-2 loss: 1.001480  [   96/  126]
train() client id: f_00000-8-0 loss: 0.768559  [   32/  126]
train() client id: f_00000-8-1 loss: 0.886834  [   64/  126]
train() client id: f_00000-8-2 loss: 0.839267  [   96/  126]
train() client id: f_00000-9-0 loss: 0.737190  [   32/  126]
train() client id: f_00000-9-1 loss: 0.843990  [   64/  126]
train() client id: f_00000-9-2 loss: 0.768664  [   96/  126]
train() client id: f_00000-10-0 loss: 0.804211  [   32/  126]
train() client id: f_00000-10-1 loss: 0.821012  [   64/  126]
train() client id: f_00000-10-2 loss: 0.722026  [   96/  126]
train() client id: f_00000-11-0 loss: 0.906063  [   32/  126]
train() client id: f_00000-11-1 loss: 0.859766  [   64/  126]
train() client id: f_00000-11-2 loss: 0.637808  [   96/  126]
train() client id: f_00000-12-0 loss: 0.617883  [   32/  126]
train() client id: f_00000-12-1 loss: 0.794031  [   64/  126]
train() client id: f_00000-12-2 loss: 0.849593  [   96/  126]
train() client id: f_00000-13-0 loss: 0.734006  [   32/  126]
train() client id: f_00000-13-1 loss: 0.723791  [   64/  126]
train() client id: f_00000-13-2 loss: 0.766739  [   96/  126]
train() client id: f_00001-0-0 loss: 0.552445  [   32/  265]
train() client id: f_00001-0-1 loss: 0.443700  [   64/  265]
train() client id: f_00001-0-2 loss: 0.494483  [   96/  265]
train() client id: f_00001-0-3 loss: 0.503660  [  128/  265]
train() client id: f_00001-0-4 loss: 0.602806  [  160/  265]
train() client id: f_00001-0-5 loss: 0.429815  [  192/  265]
train() client id: f_00001-0-6 loss: 0.411601  [  224/  265]
train() client id: f_00001-0-7 loss: 0.460027  [  256/  265]
train() client id: f_00001-1-0 loss: 0.465460  [   32/  265]
train() client id: f_00001-1-1 loss: 0.549520  [   64/  265]
train() client id: f_00001-1-2 loss: 0.371911  [   96/  265]
train() client id: f_00001-1-3 loss: 0.563752  [  128/  265]
train() client id: f_00001-1-4 loss: 0.511362  [  160/  265]
train() client id: f_00001-1-5 loss: 0.488876  [  192/  265]
train() client id: f_00001-1-6 loss: 0.444587  [  224/  265]
train() client id: f_00001-1-7 loss: 0.444513  [  256/  265]
train() client id: f_00001-2-0 loss: 0.440753  [   32/  265]
train() client id: f_00001-2-1 loss: 0.571974  [   64/  265]
train() client id: f_00001-2-2 loss: 0.422206  [   96/  265]
train() client id: f_00001-2-3 loss: 0.469599  [  128/  265]
train() client id: f_00001-2-4 loss: 0.413401  [  160/  265]
train() client id: f_00001-2-5 loss: 0.412778  [  192/  265]
train() client id: f_00001-2-6 loss: 0.386739  [  224/  265]
train() client id: f_00001-2-7 loss: 0.508240  [  256/  265]
train() client id: f_00001-3-0 loss: 0.574490  [   32/  265]
train() client id: f_00001-3-1 loss: 0.461528  [   64/  265]
train() client id: f_00001-3-2 loss: 0.392183  [   96/  265]
train() client id: f_00001-3-3 loss: 0.522237  [  128/  265]
train() client id: f_00001-3-4 loss: 0.374917  [  160/  265]
train() client id: f_00001-3-5 loss: 0.371635  [  192/  265]
train() client id: f_00001-3-6 loss: 0.506660  [  224/  265]
train() client id: f_00001-3-7 loss: 0.430476  [  256/  265]
train() client id: f_00001-4-0 loss: 0.479279  [   32/  265]
train() client id: f_00001-4-1 loss: 0.455415  [   64/  265]
train() client id: f_00001-4-2 loss: 0.439539  [   96/  265]
train() client id: f_00001-4-3 loss: 0.457420  [  128/  265]
train() client id: f_00001-4-4 loss: 0.444160  [  160/  265]
train() client id: f_00001-4-5 loss: 0.482422  [  192/  265]
train() client id: f_00001-4-6 loss: 0.456684  [  224/  265]
train() client id: f_00001-4-7 loss: 0.549391  [  256/  265]
train() client id: f_00001-5-0 loss: 0.543679  [   32/  265]
train() client id: f_00001-5-1 loss: 0.547816  [   64/  265]
train() client id: f_00001-5-2 loss: 0.509745  [   96/  265]
train() client id: f_00001-5-3 loss: 0.377288  [  128/  265]
train() client id: f_00001-5-4 loss: 0.383009  [  160/  265]
train() client id: f_00001-5-5 loss: 0.389585  [  192/  265]
train() client id: f_00001-5-6 loss: 0.455914  [  224/  265]
train() client id: f_00001-5-7 loss: 0.534982  [  256/  265]
train() client id: f_00001-6-0 loss: 0.372418  [   32/  265]
train() client id: f_00001-6-1 loss: 0.414725  [   64/  265]
train() client id: f_00001-6-2 loss: 0.435542  [   96/  265]
train() client id: f_00001-6-3 loss: 0.501393  [  128/  265]
train() client id: f_00001-6-4 loss: 0.455873  [  160/  265]
train() client id: f_00001-6-5 loss: 0.586382  [  192/  265]
train() client id: f_00001-6-6 loss: 0.474820  [  224/  265]
train() client id: f_00001-6-7 loss: 0.410168  [  256/  265]
train() client id: f_00001-7-0 loss: 0.498018  [   32/  265]
train() client id: f_00001-7-1 loss: 0.380673  [   64/  265]
train() client id: f_00001-7-2 loss: 0.378437  [   96/  265]
train() client id: f_00001-7-3 loss: 0.580697  [  128/  265]
train() client id: f_00001-7-4 loss: 0.427886  [  160/  265]
train() client id: f_00001-7-5 loss: 0.477966  [  192/  265]
train() client id: f_00001-7-6 loss: 0.443070  [  224/  265]
train() client id: f_00001-7-7 loss: 0.488416  [  256/  265]
train() client id: f_00001-8-0 loss: 0.367636  [   32/  265]
train() client id: f_00001-8-1 loss: 0.435488  [   64/  265]
train() client id: f_00001-8-2 loss: 0.359204  [   96/  265]
train() client id: f_00001-8-3 loss: 0.521919  [  128/  265]
train() client id: f_00001-8-4 loss: 0.555138  [  160/  265]
train() client id: f_00001-8-5 loss: 0.435939  [  192/  265]
train() client id: f_00001-8-6 loss: 0.547269  [  224/  265]
train() client id: f_00001-8-7 loss: 0.431551  [  256/  265]
train() client id: f_00001-9-0 loss: 0.380935  [   32/  265]
train() client id: f_00001-9-1 loss: 0.686528  [   64/  265]
train() client id: f_00001-9-2 loss: 0.437440  [   96/  265]
train() client id: f_00001-9-3 loss: 0.471871  [  128/  265]
train() client id: f_00001-9-4 loss: 0.368188  [  160/  265]
train() client id: f_00001-9-5 loss: 0.387493  [  192/  265]
train() client id: f_00001-9-6 loss: 0.434164  [  224/  265]
train() client id: f_00001-9-7 loss: 0.547217  [  256/  265]
train() client id: f_00001-10-0 loss: 0.498827  [   32/  265]
train() client id: f_00001-10-1 loss: 0.537190  [   64/  265]
train() client id: f_00001-10-2 loss: 0.423627  [   96/  265]
train() client id: f_00001-10-3 loss: 0.468385  [  128/  265]
train() client id: f_00001-10-4 loss: 0.569765  [  160/  265]
train() client id: f_00001-10-5 loss: 0.371799  [  192/  265]
train() client id: f_00001-10-6 loss: 0.479394  [  224/  265]
train() client id: f_00001-10-7 loss: 0.357827  [  256/  265]
train() client id: f_00001-11-0 loss: 0.354117  [   32/  265]
train() client id: f_00001-11-1 loss: 0.565855  [   64/  265]
train() client id: f_00001-11-2 loss: 0.362537  [   96/  265]
train() client id: f_00001-11-3 loss: 0.456422  [  128/  265]
train() client id: f_00001-11-4 loss: 0.578233  [  160/  265]
train() client id: f_00001-11-5 loss: 0.356196  [  192/  265]
train() client id: f_00001-11-6 loss: 0.562576  [  224/  265]
train() client id: f_00001-11-7 loss: 0.406207  [  256/  265]
train() client id: f_00001-12-0 loss: 0.415795  [   32/  265]
train() client id: f_00001-12-1 loss: 0.479403  [   64/  265]
train() client id: f_00001-12-2 loss: 0.360663  [   96/  265]
train() client id: f_00001-12-3 loss: 0.549141  [  128/  265]
train() client id: f_00001-12-4 loss: 0.550894  [  160/  265]
train() client id: f_00001-12-5 loss: 0.383524  [  192/  265]
train() client id: f_00001-12-6 loss: 0.473327  [  224/  265]
train() client id: f_00001-12-7 loss: 0.485696  [  256/  265]
train() client id: f_00001-13-0 loss: 0.362329  [   32/  265]
train() client id: f_00001-13-1 loss: 0.638043  [   64/  265]
train() client id: f_00001-13-2 loss: 0.585586  [   96/  265]
train() client id: f_00001-13-3 loss: 0.371100  [  128/  265]
train() client id: f_00001-13-4 loss: 0.410086  [  160/  265]
train() client id: f_00001-13-5 loss: 0.401100  [  192/  265]
train() client id: f_00001-13-6 loss: 0.489493  [  224/  265]
train() client id: f_00001-13-7 loss: 0.451085  [  256/  265]
train() client id: f_00002-0-0 loss: 0.967720  [   32/  124]
train() client id: f_00002-0-1 loss: 1.018833  [   64/  124]
train() client id: f_00002-0-2 loss: 1.252785  [   96/  124]
train() client id: f_00002-1-0 loss: 1.034377  [   32/  124]
train() client id: f_00002-1-1 loss: 1.033426  [   64/  124]
train() client id: f_00002-1-2 loss: 1.231367  [   96/  124]
train() client id: f_00002-2-0 loss: 1.162375  [   32/  124]
train() client id: f_00002-2-1 loss: 1.062861  [   64/  124]
train() client id: f_00002-2-2 loss: 0.944315  [   96/  124]
train() client id: f_00002-3-0 loss: 1.054415  [   32/  124]
train() client id: f_00002-3-1 loss: 0.972960  [   64/  124]
train() client id: f_00002-3-2 loss: 0.902341  [   96/  124]
train() client id: f_00002-4-0 loss: 1.107816  [   32/  124]
train() client id: f_00002-4-1 loss: 1.038543  [   64/  124]
train() client id: f_00002-4-2 loss: 0.955196  [   96/  124]
train() client id: f_00002-5-0 loss: 0.883458  [   32/  124]
train() client id: f_00002-5-1 loss: 1.175141  [   64/  124]
train() client id: f_00002-5-2 loss: 0.851374  [   96/  124]
train() client id: f_00002-6-0 loss: 0.946372  [   32/  124]
train() client id: f_00002-6-1 loss: 1.064989  [   64/  124]
train() client id: f_00002-6-2 loss: 0.813499  [   96/  124]
train() client id: f_00002-7-0 loss: 0.989964  [   32/  124]
train() client id: f_00002-7-1 loss: 0.874905  [   64/  124]
train() client id: f_00002-7-2 loss: 0.885484  [   96/  124]
train() client id: f_00002-8-0 loss: 0.802303  [   32/  124]
train() client id: f_00002-8-1 loss: 0.669378  [   64/  124]
train() client id: f_00002-8-2 loss: 0.972198  [   96/  124]
train() client id: f_00002-9-0 loss: 0.809235  [   32/  124]
train() client id: f_00002-9-1 loss: 0.985307  [   64/  124]
train() client id: f_00002-9-2 loss: 0.824547  [   96/  124]
train() client id: f_00002-10-0 loss: 0.854793  [   32/  124]
train() client id: f_00002-10-1 loss: 0.724132  [   64/  124]
train() client id: f_00002-10-2 loss: 0.847242  [   96/  124]
train() client id: f_00002-11-0 loss: 0.665609  [   32/  124]
train() client id: f_00002-11-1 loss: 0.952091  [   64/  124]
train() client id: f_00002-11-2 loss: 1.014622  [   96/  124]
train() client id: f_00002-12-0 loss: 0.675047  [   32/  124]
train() client id: f_00002-12-1 loss: 0.775936  [   64/  124]
train() client id: f_00002-12-2 loss: 0.835948  [   96/  124]
train() client id: f_00002-13-0 loss: 0.925247  [   32/  124]
train() client id: f_00002-13-1 loss: 0.886775  [   64/  124]
train() client id: f_00002-13-2 loss: 0.719591  [   96/  124]
train() client id: f_00003-0-0 loss: 0.746205  [   32/   43]
train() client id: f_00003-1-0 loss: 0.522312  [   32/   43]
train() client id: f_00003-2-0 loss: 0.500433  [   32/   43]
train() client id: f_00003-3-0 loss: 0.555617  [   32/   43]
train() client id: f_00003-4-0 loss: 0.715577  [   32/   43]
train() client id: f_00003-5-0 loss: 0.801519  [   32/   43]
train() client id: f_00003-6-0 loss: 0.631536  [   32/   43]
train() client id: f_00003-7-0 loss: 0.876767  [   32/   43]
train() client id: f_00003-8-0 loss: 0.655825  [   32/   43]
train() client id: f_00003-9-0 loss: 0.593782  [   32/   43]
train() client id: f_00003-10-0 loss: 0.507797  [   32/   43]
train() client id: f_00003-11-0 loss: 0.685408  [   32/   43]
train() client id: f_00003-12-0 loss: 0.585215  [   32/   43]
train() client id: f_00003-13-0 loss: 0.584744  [   32/   43]
train() client id: f_00004-0-0 loss: 0.945561  [   32/  306]
train() client id: f_00004-0-1 loss: 0.935057  [   64/  306]
train() client id: f_00004-0-2 loss: 0.857409  [   96/  306]
train() client id: f_00004-0-3 loss: 0.734144  [  128/  306]
train() client id: f_00004-0-4 loss: 1.038854  [  160/  306]
train() client id: f_00004-0-5 loss: 0.793509  [  192/  306]
train() client id: f_00004-0-6 loss: 0.917880  [  224/  306]
train() client id: f_00004-0-7 loss: 0.878650  [  256/  306]
train() client id: f_00004-0-8 loss: 0.801341  [  288/  306]
train() client id: f_00004-1-0 loss: 0.898083  [   32/  306]
train() client id: f_00004-1-1 loss: 0.817354  [   64/  306]
train() client id: f_00004-1-2 loss: 0.804501  [   96/  306]
train() client id: f_00004-1-3 loss: 0.963035  [  128/  306]
train() client id: f_00004-1-4 loss: 0.953253  [  160/  306]
train() client id: f_00004-1-5 loss: 0.909376  [  192/  306]
train() client id: f_00004-1-6 loss: 0.748425  [  224/  306]
train() client id: f_00004-1-7 loss: 0.897652  [  256/  306]
train() client id: f_00004-1-8 loss: 0.926122  [  288/  306]
train() client id: f_00004-2-0 loss: 0.944929  [   32/  306]
train() client id: f_00004-2-1 loss: 0.907442  [   64/  306]
train() client id: f_00004-2-2 loss: 0.974470  [   96/  306]
train() client id: f_00004-2-3 loss: 0.846289  [  128/  306]
train() client id: f_00004-2-4 loss: 0.767912  [  160/  306]
train() client id: f_00004-2-5 loss: 0.796585  [  192/  306]
train() client id: f_00004-2-6 loss: 0.871816  [  224/  306]
train() client id: f_00004-2-7 loss: 0.977340  [  256/  306]
train() client id: f_00004-2-8 loss: 0.781037  [  288/  306]
train() client id: f_00004-3-0 loss: 0.806532  [   32/  306]
train() client id: f_00004-3-1 loss: 0.890622  [   64/  306]
train() client id: f_00004-3-2 loss: 0.909830  [   96/  306]
train() client id: f_00004-3-3 loss: 0.802204  [  128/  306]
train() client id: f_00004-3-4 loss: 0.916484  [  160/  306]
train() client id: f_00004-3-5 loss: 0.923873  [  192/  306]
train() client id: f_00004-3-6 loss: 0.755058  [  224/  306]
train() client id: f_00004-3-7 loss: 0.855756  [  256/  306]
train() client id: f_00004-3-8 loss: 0.899518  [  288/  306]
train() client id: f_00004-4-0 loss: 0.898145  [   32/  306]
train() client id: f_00004-4-1 loss: 0.797152  [   64/  306]
train() client id: f_00004-4-2 loss: 0.868097  [   96/  306]
train() client id: f_00004-4-3 loss: 0.777023  [  128/  306]
train() client id: f_00004-4-4 loss: 0.833981  [  160/  306]
train() client id: f_00004-4-5 loss: 0.909860  [  192/  306]
train() client id: f_00004-4-6 loss: 0.892957  [  224/  306]
train() client id: f_00004-4-7 loss: 0.920372  [  256/  306]
train() client id: f_00004-4-8 loss: 0.847250  [  288/  306]
train() client id: f_00004-5-0 loss: 0.862630  [   32/  306]
train() client id: f_00004-5-1 loss: 0.785439  [   64/  306]
train() client id: f_00004-5-2 loss: 1.002652  [   96/  306]
train() client id: f_00004-5-3 loss: 0.863914  [  128/  306]
train() client id: f_00004-5-4 loss: 0.952733  [  160/  306]
train() client id: f_00004-5-5 loss: 0.929401  [  192/  306]
train() client id: f_00004-5-6 loss: 0.827704  [  224/  306]
train() client id: f_00004-5-7 loss: 0.766348  [  256/  306]
train() client id: f_00004-5-8 loss: 0.831028  [  288/  306]
train() client id: f_00004-6-0 loss: 0.846124  [   32/  306]
train() client id: f_00004-6-1 loss: 0.900956  [   64/  306]
train() client id: f_00004-6-2 loss: 0.953594  [   96/  306]
train() client id: f_00004-6-3 loss: 0.881141  [  128/  306]
train() client id: f_00004-6-4 loss: 0.780742  [  160/  306]
train() client id: f_00004-6-5 loss: 0.807873  [  192/  306]
train() client id: f_00004-6-6 loss: 0.893701  [  224/  306]
train() client id: f_00004-6-7 loss: 0.760763  [  256/  306]
train() client id: f_00004-6-8 loss: 0.999215  [  288/  306]
train() client id: f_00004-7-0 loss: 0.925496  [   32/  306]
train() client id: f_00004-7-1 loss: 0.803230  [   64/  306]
train() client id: f_00004-7-2 loss: 0.830238  [   96/  306]
train() client id: f_00004-7-3 loss: 0.924521  [  128/  306]
train() client id: f_00004-7-4 loss: 0.781113  [  160/  306]
train() client id: f_00004-7-5 loss: 0.811191  [  192/  306]
train() client id: f_00004-7-6 loss: 0.813994  [  224/  306]
train() client id: f_00004-7-7 loss: 0.872343  [  256/  306]
train() client id: f_00004-7-8 loss: 0.988354  [  288/  306]
train() client id: f_00004-8-0 loss: 0.850544  [   32/  306]
train() client id: f_00004-8-1 loss: 0.797253  [   64/  306]
train() client id: f_00004-8-2 loss: 0.899667  [   96/  306]
train() client id: f_00004-8-3 loss: 0.964804  [  128/  306]
train() client id: f_00004-8-4 loss: 0.815002  [  160/  306]
train() client id: f_00004-8-5 loss: 0.777552  [  192/  306]
train() client id: f_00004-8-6 loss: 0.959865  [  224/  306]
train() client id: f_00004-8-7 loss: 0.845962  [  256/  306]
train() client id: f_00004-8-8 loss: 0.890474  [  288/  306]
train() client id: f_00004-9-0 loss: 0.794649  [   32/  306]
train() client id: f_00004-9-1 loss: 0.865239  [   64/  306]
train() client id: f_00004-9-2 loss: 0.814718  [   96/  306]
train() client id: f_00004-9-3 loss: 0.754544  [  128/  306]
train() client id: f_00004-9-4 loss: 0.905287  [  160/  306]
train() client id: f_00004-9-5 loss: 0.887624  [  192/  306]
train() client id: f_00004-9-6 loss: 1.011216  [  224/  306]
train() client id: f_00004-9-7 loss: 0.868104  [  256/  306]
train() client id: f_00004-9-8 loss: 0.819087  [  288/  306]
train() client id: f_00004-10-0 loss: 0.761505  [   32/  306]
train() client id: f_00004-10-1 loss: 0.837918  [   64/  306]
train() client id: f_00004-10-2 loss: 1.104192  [   96/  306]
train() client id: f_00004-10-3 loss: 0.881286  [  128/  306]
train() client id: f_00004-10-4 loss: 0.776880  [  160/  306]
train() client id: f_00004-10-5 loss: 0.903022  [  192/  306]
train() client id: f_00004-10-6 loss: 0.870537  [  224/  306]
train() client id: f_00004-10-7 loss: 0.880121  [  256/  306]
train() client id: f_00004-10-8 loss: 0.732591  [  288/  306]
train() client id: f_00004-11-0 loss: 0.997917  [   32/  306]
train() client id: f_00004-11-1 loss: 0.799710  [   64/  306]
train() client id: f_00004-11-2 loss: 0.852783  [   96/  306]
train() client id: f_00004-11-3 loss: 0.819056  [  128/  306]
train() client id: f_00004-11-4 loss: 0.879341  [  160/  306]
train() client id: f_00004-11-5 loss: 0.833935  [  192/  306]
train() client id: f_00004-11-6 loss: 0.891230  [  224/  306]
train() client id: f_00004-11-7 loss: 0.889077  [  256/  306]
train() client id: f_00004-11-8 loss: 0.861860  [  288/  306]
train() client id: f_00004-12-0 loss: 0.863894  [   32/  306]
train() client id: f_00004-12-1 loss: 0.933771  [   64/  306]
train() client id: f_00004-12-2 loss: 0.832547  [   96/  306]
train() client id: f_00004-12-3 loss: 0.790845  [  128/  306]
train() client id: f_00004-12-4 loss: 0.791261  [  160/  306]
train() client id: f_00004-12-5 loss: 0.769459  [  192/  306]
train() client id: f_00004-12-6 loss: 0.789291  [  224/  306]
train() client id: f_00004-12-7 loss: 0.944982  [  256/  306]
train() client id: f_00004-12-8 loss: 0.974909  [  288/  306]
train() client id: f_00004-13-0 loss: 0.987836  [   32/  306]
train() client id: f_00004-13-1 loss: 0.914694  [   64/  306]
train() client id: f_00004-13-2 loss: 0.925055  [   96/  306]
train() client id: f_00004-13-3 loss: 0.731440  [  128/  306]
train() client id: f_00004-13-4 loss: 0.826644  [  160/  306]
train() client id: f_00004-13-5 loss: 0.824282  [  192/  306]
train() client id: f_00004-13-6 loss: 0.835831  [  224/  306]
train() client id: f_00004-13-7 loss: 0.824351  [  256/  306]
train() client id: f_00004-13-8 loss: 0.915654  [  288/  306]
train() client id: f_00005-0-0 loss: 0.590241  [   32/  146]
train() client id: f_00005-0-1 loss: 0.874746  [   64/  146]
train() client id: f_00005-0-2 loss: 0.565320  [   96/  146]
train() client id: f_00005-0-3 loss: 0.612528  [  128/  146]
train() client id: f_00005-1-0 loss: 0.625048  [   32/  146]
train() client id: f_00005-1-1 loss: 0.541935  [   64/  146]
train() client id: f_00005-1-2 loss: 0.540242  [   96/  146]
train() client id: f_00005-1-3 loss: 0.583542  [  128/  146]
train() client id: f_00005-2-0 loss: 0.692293  [   32/  146]
train() client id: f_00005-2-1 loss: 0.615955  [   64/  146]
train() client id: f_00005-2-2 loss: 0.689634  [   96/  146]
train() client id: f_00005-2-3 loss: 0.425498  [  128/  146]
train() client id: f_00005-3-0 loss: 0.510785  [   32/  146]
train() client id: f_00005-3-1 loss: 0.716253  [   64/  146]
train() client id: f_00005-3-2 loss: 0.549160  [   96/  146]
train() client id: f_00005-3-3 loss: 0.473593  [  128/  146]
train() client id: f_00005-4-0 loss: 0.514748  [   32/  146]
train() client id: f_00005-4-1 loss: 0.773046  [   64/  146]
train() client id: f_00005-4-2 loss: 0.608068  [   96/  146]
train() client id: f_00005-4-3 loss: 0.628818  [  128/  146]
train() client id: f_00005-5-0 loss: 0.307516  [   32/  146]
train() client id: f_00005-5-1 loss: 0.629137  [   64/  146]
train() client id: f_00005-5-2 loss: 0.523344  [   96/  146]
train() client id: f_00005-5-3 loss: 0.884915  [  128/  146]
train() client id: f_00005-6-0 loss: 0.769465  [   32/  146]
train() client id: f_00005-6-1 loss: 0.401954  [   64/  146]
train() client id: f_00005-6-2 loss: 0.669288  [   96/  146]
train() client id: f_00005-6-3 loss: 0.673956  [  128/  146]
train() client id: f_00005-7-0 loss: 0.561900  [   32/  146]
train() client id: f_00005-7-1 loss: 0.702148  [   64/  146]
train() client id: f_00005-7-2 loss: 0.672891  [   96/  146]
train() client id: f_00005-7-3 loss: 0.508590  [  128/  146]
train() client id: f_00005-8-0 loss: 0.431920  [   32/  146]
train() client id: f_00005-8-1 loss: 0.779974  [   64/  146]
train() client id: f_00005-8-2 loss: 0.675384  [   96/  146]
train() client id: f_00005-8-3 loss: 0.614444  [  128/  146]
train() client id: f_00005-9-0 loss: 0.640035  [   32/  146]
train() client id: f_00005-9-1 loss: 0.725331  [   64/  146]
train() client id: f_00005-9-2 loss: 0.391189  [   96/  146]
train() client id: f_00005-9-3 loss: 0.388803  [  128/  146]
train() client id: f_00005-10-0 loss: 0.792211  [   32/  146]
train() client id: f_00005-10-1 loss: 0.437151  [   64/  146]
train() client id: f_00005-10-2 loss: 0.560690  [   96/  146]
train() client id: f_00005-10-3 loss: 0.607127  [  128/  146]
train() client id: f_00005-11-0 loss: 0.394695  [   32/  146]
train() client id: f_00005-11-1 loss: 0.968336  [   64/  146]
train() client id: f_00005-11-2 loss: 0.589136  [   96/  146]
train() client id: f_00005-11-3 loss: 0.568687  [  128/  146]
train() client id: f_00005-12-0 loss: 0.694614  [   32/  146]
train() client id: f_00005-12-1 loss: 0.731798  [   64/  146]
train() client id: f_00005-12-2 loss: 0.641042  [   96/  146]
train() client id: f_00005-12-3 loss: 0.426154  [  128/  146]
train() client id: f_00005-13-0 loss: 0.531951  [   32/  146]
train() client id: f_00005-13-1 loss: 0.553546  [   64/  146]
train() client id: f_00005-13-2 loss: 0.486433  [   96/  146]
train() client id: f_00005-13-3 loss: 0.711410  [  128/  146]
train() client id: f_00006-0-0 loss: 0.515788  [   32/   54]
train() client id: f_00006-1-0 loss: 0.511069  [   32/   54]
train() client id: f_00006-2-0 loss: 0.449867  [   32/   54]
train() client id: f_00006-3-0 loss: 0.477260  [   32/   54]
train() client id: f_00006-4-0 loss: 0.447899  [   32/   54]
train() client id: f_00006-5-0 loss: 0.479721  [   32/   54]
train() client id: f_00006-6-0 loss: 0.482329  [   32/   54]
train() client id: f_00006-7-0 loss: 0.433995  [   32/   54]
train() client id: f_00006-8-0 loss: 0.425570  [   32/   54]
train() client id: f_00006-9-0 loss: 0.491687  [   32/   54]
train() client id: f_00006-10-0 loss: 0.487078  [   32/   54]
train() client id: f_00006-11-0 loss: 0.466480  [   32/   54]
train() client id: f_00006-12-0 loss: 0.523926  [   32/   54]
train() client id: f_00006-13-0 loss: 0.517245  [   32/   54]
train() client id: f_00007-0-0 loss: 0.268232  [   32/  179]
train() client id: f_00007-0-1 loss: 0.316159  [   64/  179]
train() client id: f_00007-0-2 loss: 0.284366  [   96/  179]
train() client id: f_00007-0-3 loss: 0.142729  [  128/  179]
train() client id: f_00007-0-4 loss: 0.200890  [  160/  179]
train() client id: f_00007-1-0 loss: 0.191569  [   32/  179]
train() client id: f_00007-1-1 loss: 0.533750  [   64/  179]
train() client id: f_00007-1-2 loss: 0.259290  [   96/  179]
train() client id: f_00007-1-3 loss: 0.025941  [  128/  179]
train() client id: f_00007-1-4 loss: 0.166179  [  160/  179]
train() client id: f_00007-2-0 loss: 0.248044  [   32/  179]
train() client id: f_00007-2-1 loss: 0.103224  [   64/  179]
train() client id: f_00007-2-2 loss: 0.115311  [   96/  179]
train() client id: f_00007-2-3 loss: 0.226332  [  128/  179]
train() client id: f_00007-2-4 loss: 0.130839  [  160/  179]
train() client id: f_00007-3-0 loss: 0.152155  [   32/  179]
train() client id: f_00007-3-1 loss: 0.076696  [   64/  179]
train() client id: f_00007-3-2 loss: 0.202654  [   96/  179]
train() client id: f_00007-3-3 loss: 0.175759  [  128/  179]
train() client id: f_00007-3-4 loss: 0.408238  [  160/  179]
train() client id: f_00007-4-0 loss: 0.136821  [   32/  179]
train() client id: f_00007-4-1 loss: 0.471362  [   64/  179]
train() client id: f_00007-4-2 loss: 0.058281  [   96/  179]
train() client id: f_00007-4-3 loss: 0.115820  [  128/  179]
train() client id: f_00007-4-4 loss: 0.233379  [  160/  179]
train() client id: f_00007-5-0 loss: -0.006290  [   32/  179]
train() client id: f_00007-5-1 loss: 0.212533  [   64/  179]
train() client id: f_00007-5-2 loss: 0.242239  [   96/  179]
train() client id: f_00007-5-3 loss: 0.280413  [  128/  179]
train() client id: f_00007-5-4 loss: 0.156172  [  160/  179]
train() client id: f_00007-6-0 loss: 0.273799  [   32/  179]
train() client id: f_00007-6-1 loss: 0.072265  [   64/  179]
train() client id: f_00007-6-2 loss: 0.084217  [   96/  179]
train() client id: f_00007-6-3 loss: 0.239880  [  128/  179]
train() client id: f_00007-6-4 loss: 0.188740  [  160/  179]
train() client id: f_00007-7-0 loss: 0.032865  [   32/  179]
train() client id: f_00007-7-1 loss: -0.016534  [   64/  179]
train() client id: f_00007-7-2 loss: 0.242460  [   96/  179]
train() client id: f_00007-7-3 loss: 0.265745  [  128/  179]
train() client id: f_00007-7-4 loss: 0.284471  [  160/  179]
train() client id: f_00007-8-0 loss: -0.009728  [   32/  179]
train() client id: f_00007-8-1 loss: 0.134776  [   64/  179]
train() client id: f_00007-8-2 loss: 0.174985  [   96/  179]
train() client id: f_00007-8-3 loss: 0.324812  [  128/  179]
train() client id: f_00007-8-4 loss: 0.032598  [  160/  179]
train() client id: f_00007-9-0 loss: 0.107723  [   32/  179]
train() client id: f_00007-9-1 loss: 0.081433  [   64/  179]
train() client id: f_00007-9-2 loss: 0.118719  [   96/  179]
train() client id: f_00007-9-3 loss: 0.112446  [  128/  179]
train() client id: f_00007-9-4 loss: 0.388259  [  160/  179]
train() client id: f_00007-10-0 loss: -0.024096  [   32/  179]
train() client id: f_00007-10-1 loss: 0.311891  [   64/  179]
train() client id: f_00007-10-2 loss: 0.187150  [   96/  179]
train() client id: f_00007-10-3 loss: 0.021773  [  128/  179]
train() client id: f_00007-10-4 loss: 0.228865  [  160/  179]
train() client id: f_00007-11-0 loss: 0.350452  [   32/  179]
train() client id: f_00007-11-1 loss: 0.138907  [   64/  179]
train() client id: f_00007-11-2 loss: 0.304133  [   96/  179]
train() client id: f_00007-11-3 loss: -0.004115  [  128/  179]
train() client id: f_00007-11-4 loss: 0.010461  [  160/  179]
train() client id: f_00007-12-0 loss: 0.061644  [   32/  179]
train() client id: f_00007-12-1 loss: 0.214574  [   64/  179]
train() client id: f_00007-12-2 loss: 0.117199  [   96/  179]
train() client id: f_00007-12-3 loss: 0.232464  [  128/  179]
train() client id: f_00007-12-4 loss: 0.267470  [  160/  179]
train() client id: f_00007-13-0 loss: 0.096050  [   32/  179]
train() client id: f_00007-13-1 loss: 0.117625  [   64/  179]
train() client id: f_00007-13-2 loss: 0.290618  [   96/  179]
train() client id: f_00007-13-3 loss: 0.194685  [  128/  179]
train() client id: f_00007-13-4 loss: 0.102984  [  160/  179]
train() client id: f_00008-0-0 loss: 0.739175  [   32/  130]
train() client id: f_00008-0-1 loss: 0.767164  [   64/  130]
train() client id: f_00008-0-2 loss: 0.661920  [   96/  130]
train() client id: f_00008-0-3 loss: 0.604476  [  128/  130]
train() client id: f_00008-1-0 loss: 0.808532  [   32/  130]
train() client id: f_00008-1-1 loss: 0.665293  [   64/  130]
train() client id: f_00008-1-2 loss: 0.585882  [   96/  130]
train() client id: f_00008-1-3 loss: 0.675770  [  128/  130]
train() client id: f_00008-2-0 loss: 0.872202  [   32/  130]
train() client id: f_00008-2-1 loss: 0.704047  [   64/  130]
train() client id: f_00008-2-2 loss: 0.536100  [   96/  130]
train() client id: f_00008-2-3 loss: 0.648364  [  128/  130]
train() client id: f_00008-3-0 loss: 0.702375  [   32/  130]
train() client id: f_00008-3-1 loss: 0.798688  [   64/  130]
train() client id: f_00008-3-2 loss: 0.613992  [   96/  130]
train() client id: f_00008-3-3 loss: 0.622147  [  128/  130]
train() client id: f_00008-4-0 loss: 0.731474  [   32/  130]
train() client id: f_00008-4-1 loss: 0.727120  [   64/  130]
train() client id: f_00008-4-2 loss: 0.622257  [   96/  130]
train() client id: f_00008-4-3 loss: 0.677104  [  128/  130]
train() client id: f_00008-5-0 loss: 0.599878  [   32/  130]
train() client id: f_00008-5-1 loss: 0.757356  [   64/  130]
train() client id: f_00008-5-2 loss: 0.657708  [   96/  130]
train() client id: f_00008-5-3 loss: 0.752708  [  128/  130]
train() client id: f_00008-6-0 loss: 0.594391  [   32/  130]
train() client id: f_00008-6-1 loss: 0.646726  [   64/  130]
train() client id: f_00008-6-2 loss: 0.873150  [   96/  130]
train() client id: f_00008-6-3 loss: 0.647501  [  128/  130]
train() client id: f_00008-7-0 loss: 0.641943  [   32/  130]
train() client id: f_00008-7-1 loss: 0.774495  [   64/  130]
train() client id: f_00008-7-2 loss: 0.649129  [   96/  130]
train() client id: f_00008-7-3 loss: 0.684691  [  128/  130]
train() client id: f_00008-8-0 loss: 0.626493  [   32/  130]
train() client id: f_00008-8-1 loss: 0.620784  [   64/  130]
train() client id: f_00008-8-2 loss: 0.811492  [   96/  130]
train() client id: f_00008-8-3 loss: 0.698680  [  128/  130]
train() client id: f_00008-9-0 loss: 0.715980  [   32/  130]
train() client id: f_00008-9-1 loss: 0.701827  [   64/  130]
train() client id: f_00008-9-2 loss: 0.625095  [   96/  130]
train() client id: f_00008-9-3 loss: 0.711320  [  128/  130]
train() client id: f_00008-10-0 loss: 0.706844  [   32/  130]
train() client id: f_00008-10-1 loss: 0.693776  [   64/  130]
train() client id: f_00008-10-2 loss: 0.677390  [   96/  130]
train() client id: f_00008-10-3 loss: 0.685737  [  128/  130]
train() client id: f_00008-11-0 loss: 0.695573  [   32/  130]
train() client id: f_00008-11-1 loss: 0.695822  [   64/  130]
train() client id: f_00008-11-2 loss: 0.626098  [   96/  130]
train() client id: f_00008-11-3 loss: 0.677552  [  128/  130]
train() client id: f_00008-12-0 loss: 0.728018  [   32/  130]
train() client id: f_00008-12-1 loss: 0.749251  [   64/  130]
train() client id: f_00008-12-2 loss: 0.674693  [   96/  130]
train() client id: f_00008-12-3 loss: 0.608453  [  128/  130]
train() client id: f_00008-13-0 loss: 0.629182  [   32/  130]
train() client id: f_00008-13-1 loss: 0.637810  [   64/  130]
train() client id: f_00008-13-2 loss: 0.692012  [   96/  130]
train() client id: f_00008-13-3 loss: 0.808597  [  128/  130]
train() client id: f_00009-0-0 loss: 1.196180  [   32/  118]
train() client id: f_00009-0-1 loss: 1.264439  [   64/  118]
train() client id: f_00009-0-2 loss: 1.051450  [   96/  118]
train() client id: f_00009-1-0 loss: 1.143859  [   32/  118]
train() client id: f_00009-1-1 loss: 0.986758  [   64/  118]
train() client id: f_00009-1-2 loss: 1.107746  [   96/  118]
train() client id: f_00009-2-0 loss: 1.068713  [   32/  118]
train() client id: f_00009-2-1 loss: 1.045983  [   64/  118]
train() client id: f_00009-2-2 loss: 1.012404  [   96/  118]
train() client id: f_00009-3-0 loss: 1.073038  [   32/  118]
train() client id: f_00009-3-1 loss: 0.835947  [   64/  118]
train() client id: f_00009-3-2 loss: 0.997563  [   96/  118]
train() client id: f_00009-4-0 loss: 0.955541  [   32/  118]
train() client id: f_00009-4-1 loss: 0.990436  [   64/  118]
train() client id: f_00009-4-2 loss: 1.046503  [   96/  118]
train() client id: f_00009-5-0 loss: 0.908491  [   32/  118]
train() client id: f_00009-5-1 loss: 0.893973  [   64/  118]
train() client id: f_00009-5-2 loss: 1.027786  [   96/  118]
train() client id: f_00009-6-0 loss: 0.914153  [   32/  118]
train() client id: f_00009-6-1 loss: 1.029386  [   64/  118]
train() client id: f_00009-6-2 loss: 0.816306  [   96/  118]
train() client id: f_00009-7-0 loss: 1.018735  [   32/  118]
train() client id: f_00009-7-1 loss: 0.827015  [   64/  118]
train() client id: f_00009-7-2 loss: 0.869330  [   96/  118]
train() client id: f_00009-8-0 loss: 0.940999  [   32/  118]
train() client id: f_00009-8-1 loss: 0.898114  [   64/  118]
train() client id: f_00009-8-2 loss: 0.931842  [   96/  118]
train() client id: f_00009-9-0 loss: 0.830326  [   32/  118]
train() client id: f_00009-9-1 loss: 0.927917  [   64/  118]
train() client id: f_00009-9-2 loss: 0.761958  [   96/  118]
train() client id: f_00009-10-0 loss: 0.915036  [   32/  118]
train() client id: f_00009-10-1 loss: 1.107064  [   64/  118]
train() client id: f_00009-10-2 loss: 0.840970  [   96/  118]
train() client id: f_00009-11-0 loss: 0.958315  [   32/  118]
train() client id: f_00009-11-1 loss: 0.804798  [   64/  118]
train() client id: f_00009-11-2 loss: 0.891503  [   96/  118]
train() client id: f_00009-12-0 loss: 0.926792  [   32/  118]
train() client id: f_00009-12-1 loss: 0.901488  [   64/  118]
train() client id: f_00009-12-2 loss: 0.795628  [   96/  118]
train() client id: f_00009-13-0 loss: 0.880357  [   32/  118]
train() client id: f_00009-13-1 loss: 0.769216  [   64/  118]
train() client id: f_00009-13-2 loss: 0.946796  [   96/  118]
At round 69 accuracy: 0.6472148541114059
At round 69 training accuracy: 0.5982562038900067
At round 69 training loss: 0.8222837729512373
update_location
xs = 8.927491 466.223621 5.882650 0.934260 -382.581990 -230.230757 -190.849135 -5.143845 -405.120581 20.134486 
ys = -457.390647 7.291448 355.684448 -177.290817 -9.642386 0.794442 -1.381692 351.628436 25.881276 -892.232496 
xs mean: -71.18237997052123
ys mean: -79.66579882624052
dists_uav = 468.279729 476.883246 369.521356 203.550747 395.552720 251.011619 215.465314 365.607735 418.081960 898.044667 
uav_gains = -123.498378 -123.720508 -120.152053 -107.906644 -121.237570 -111.382282 -108.682210 -119.968687 -122.031800 -130.806833 
uav_gains_db_mean: -118.93869665667431
dists_bs = 658.594908 662.792494 255.666585 393.778724 277.818454 182.751232 177.092339 245.048399 274.211370 1084.925190 
bs_gains = -118.488373 -118.565631 -106.981930 -112.234122 -107.992370 -102.899148 -102.516653 -106.466111 -107.833452 -124.558253 
bs_gains_db_mean: -110.85360445922271
Round 70
-------------------------------
ene_coms = [0.04642793 0.04813912 0.00885293 0.00934792 0.0093965  0.01178816
 0.0098056  0.00859811 0.00930682 0.05538799]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.81101297 3.59862739 1.62582061 0.59045423 3.95923118 1.92006771
 0.73328871 2.32919237 1.70459774 1.74659905]
obj_prev = 20.018891952829907
eta_min = 2.052484125300071e-54	eta_max = 0.9116213603915557
af = 4.105415012178352	bf = 0.9125682181099254	zeta = 4.515956513396188	eta = 0.9090909090909091
af = 4.105415012178352	bf = 0.9125682181099254	zeta = 14.342416672012206	eta = 0.2862429049484847
af = 4.105415012178352	bf = 0.9125682181099254	zeta = 8.473599030988803	eta = 0.4844948406414367
af = 4.105415012178352	bf = 0.9125682181099254	zeta = 7.541245637949439	eta = 0.5443948134402193
af = 4.105415012178352	bf = 0.9125682181099254	zeta = 7.482336191868942	eta = 0.5486809075272117
af = 4.105415012178352	bf = 0.9125682181099254	zeta = 7.482057197033706	eta = 0.5487013670258979
af = 4.105415012178352	bf = 0.9125682181099254	zeta = 7.482057190709709	eta = 0.5487013674896722
eta = 0.5487013674896722
ene_coms = [0.04642793 0.04813912 0.00885293 0.00934792 0.0093965  0.01178816
 0.0098056  0.00859811 0.00930682 0.05538799]
ene_comp = [0.04952714 0.10416423 0.048741   0.01690212 0.1202802  0.05738859
 0.02122592 0.07035999 0.05109943 0.04638256]
ene_total = [0.8939378  1.41889028 0.53655724 0.24455093 1.2080956  0.64446532
 0.28909617 0.73559041 0.56275747 0.94811598]
ti_comp = [2.37319355 2.35608163 2.74894356 2.74399368 2.74350791 2.71959124
 2.7394169  2.75149173 2.74440465 2.28359301]
ti_coms = [0.46427933 0.48139125 0.08852932 0.0934792  0.09396497 0.11788164
 0.09805598 0.08598115 0.09306823 0.55387987]
t_total = [26.43449783 26.43449783 26.43449783 26.43449783 26.43449783 26.43449783
 26.43449783 26.43449783 26.43449783 26.43449783]
ene_coms = [0.04642793 0.04813912 0.00885293 0.00934792 0.0093965  0.01178816
 0.0098056  0.00859811 0.00930682 0.05538799]
ene_comp = [1.34816650e-06 1.27249232e-05 9.57704716e-07 4.00808912e-08
 1.44494082e-05 1.59716401e-06 7.96458650e-08 2.87554975e-06
 1.10721505e-06 1.19593294e-06]
ene_total = [0.43254501 0.44859282 0.08248471 0.08708757 0.08767437 0.10983591
 0.09135177 0.08012865 0.08671464 0.51601735]
optimize_network iter = 0 obj = 2.022432798378467
eta = 0.5487013674896722
freqs = [10434703.73146137 22105394.86813818  8865405.39605486  3079839.50967419
 21920878.08186992 10550959.57960648  3874167.25416814 12785789.68246569
  9309748.36392613 10155610.58239084]
eta_min = 0.548701367489673	eta_max = 0.6653175710301316
af = 0.00025481156632005115	bf = 0.9125682181099254	zeta = 0.0002802927229520563	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.04642793 0.04813912 0.00885293 0.00934792 0.0093965  0.01178816
 0.0098056  0.00859811 0.00930682 0.05538799]
ene_comp = [2.14084955e-07 2.02068113e-06 1.52080749e-07 6.36472999e-09
 2.29452438e-06 2.53625042e-07 1.26475338e-08 4.56629012e-07
 1.75822558e-07 1.89910704e-07]
ene_total = [2.14722423 2.22644753 0.40944036 0.43232598 0.43467839 0.54519462
 0.4534931  0.39766957 0.43043313 2.56161064]
ti_comp = [1.63998655 1.62287464 2.01573656 2.01078668 2.01030092 1.98638424
 2.00620991 2.01828473 2.01119766 1.55038601]
ti_coms = [0.46427933 0.48139125 0.08852932 0.0934792  0.09396497 0.11788164
 0.09805598 0.08598115 0.09306823 0.55387987]
t_total = [26.43449783 26.43449783 26.43449783 26.43449783 26.43449783 26.43449783
 26.43449783 26.43449783 26.43449783 26.43449783]
ene_coms = [0.04642793 0.04813912 0.00885293 0.00934792 0.0093965  0.01178816
 0.0098056  0.00859811 0.00930682 0.05538799]
ene_comp = [8.83471790e-07 8.39323343e-06 5.57389953e-07 2.33579644e-08
 8.42177753e-06 9.36900930e-07 4.64718718e-08 1.67246042e-06
 6.45181899e-07 8.11947529e-07]
ene_total = [0.58325445 0.60484536 0.11122052 0.11743202 0.11814776 0.14809866
 0.12318181 0.10803343 0.11692355 0.69581278]
optimize_network iter = 1 obj = 2.726950340693432
eta = 0.6653175710301316
freqs = [10251658.52460331 21788371.65918409  8208272.89079226  2853424.12792685
 20310669.06873193  9807390.50404622  3591544.59654425 11834079.15417326
  8624868.29579761 10155610.58239084]
eta_min = 0.6653175710301327	eta_max = 0.6653175710301316
af = 0.00023103734642485876	bf = 0.9125682181099254	zeta = 0.00025414108106734467	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.04642793 0.04813912 0.00885293 0.00934792 0.0093965  0.01178816
 0.0098056  0.00859811 0.00930682 0.05538799]
ene_comp = [2.06639892e-07 1.96313778e-06 1.30370885e-07 5.46331789e-09
 1.96981411e-06 2.19136716e-07 1.08695520e-08 3.91180617e-07
 1.50905008e-07 1.89910704e-07]
ene_total = [2.14722388 2.22644487 0.40943936 0.43232593 0.43466338 0.54519302
 0.45349302 0.39766654 0.43043198 2.56161064]
ti_comp = [1.63998655 1.62287464 2.01573656 2.01078668 2.01030092 1.98638424
 2.00620991 2.01828473 2.01119766 1.55038601]
ti_coms = [0.46427933 0.48139125 0.08852932 0.0934792  0.09396497 0.11788164
 0.09805598 0.08598115 0.09306823 0.55387987]
t_total = [26.43449783 26.43449783 26.43449783 26.43449783 26.43449783 26.43449783
 26.43449783 26.43449783 26.43449783 26.43449783]
ene_coms = [0.04642793 0.04813912 0.00885293 0.00934792 0.0093965  0.01178816
 0.0098056  0.00859811 0.00930682 0.05538799]
ene_comp = [8.83471790e-07 8.39323343e-06 5.57389953e-07 2.33579644e-08
 8.42177753e-06 9.36900930e-07 4.64718718e-08 1.67246042e-06
 6.45181899e-07 8.11947529e-07]
ene_total = [0.58325445 0.60484536 0.11122052 0.11743202 0.11814776 0.14809866
 0.12318181 0.10803343 0.11692355 0.69581278]
optimize_network iter = 2 obj = 2.726950340693432
eta = 0.6653175710301316
freqs = [10251658.52460331 21788371.65918409  8208272.89079226  2853424.12792685
 20310669.06873193  9807390.50404622  3591544.59654425 11834079.15417326
  8624868.29579761 10155610.58239084]
Done!
ene_coms = [0.04642793 0.04813912 0.00885293 0.00934792 0.0093965  0.01178816
 0.0098056  0.00859811 0.00930682 0.05538799]
ene_comp = [8.60740356e-07 8.17727833e-06 5.43048495e-07 2.27569718e-08
 8.20508800e-06 9.12794782e-07 4.52761660e-08 1.62942857e-06
 6.28581584e-07 7.91056390e-07]
ene_total = [0.04642879 0.0481473  0.00885348 0.00934794 0.0094047  0.01178908
 0.00980564 0.00859974 0.00930745 0.05538878]
At round 70 energy consumption: 0.2170729087371294
At round 70 eta: 0.6653175710301316
At round 70 a_n: 4.204393568402164
At round 70 local rounds: 13.343319155623588
At round 70 global rounds: 12.562337321809885
gradient difference: 0.37721630930900574
train() client id: f_00000-0-0 loss: 1.319917  [   32/  126]
train() client id: f_00000-0-1 loss: 1.074088  [   64/  126]
train() client id: f_00000-0-2 loss: 1.258537  [   96/  126]
train() client id: f_00000-1-0 loss: 1.168789  [   32/  126]
train() client id: f_00000-1-1 loss: 1.156489  [   64/  126]
train() client id: f_00000-1-2 loss: 0.976302  [   96/  126]
train() client id: f_00000-2-0 loss: 1.135143  [   32/  126]
train() client id: f_00000-2-1 loss: 1.132774  [   64/  126]
train() client id: f_00000-2-2 loss: 0.933158  [   96/  126]
train() client id: f_00000-3-0 loss: 0.814704  [   32/  126]
train() client id: f_00000-3-1 loss: 0.892458  [   64/  126]
train() client id: f_00000-3-2 loss: 1.083781  [   96/  126]
train() client id: f_00000-4-0 loss: 0.951100  [   32/  126]
train() client id: f_00000-4-1 loss: 0.915031  [   64/  126]
train() client id: f_00000-4-2 loss: 0.760078  [   96/  126]
train() client id: f_00000-5-0 loss: 0.714275  [   32/  126]
train() client id: f_00000-5-1 loss: 0.884916  [   64/  126]
train() client id: f_00000-5-2 loss: 0.888219  [   96/  126]
train() client id: f_00000-6-0 loss: 0.992429  [   32/  126]
train() client id: f_00000-6-1 loss: 0.748048  [   64/  126]
train() client id: f_00000-6-2 loss: 0.781021  [   96/  126]
train() client id: f_00000-7-0 loss: 0.712355  [   32/  126]
train() client id: f_00000-7-1 loss: 0.749977  [   64/  126]
train() client id: f_00000-7-2 loss: 0.934949  [   96/  126]
train() client id: f_00000-8-0 loss: 0.791146  [   32/  126]
train() client id: f_00000-8-1 loss: 0.681941  [   64/  126]
train() client id: f_00000-8-2 loss: 0.926008  [   96/  126]
train() client id: f_00000-9-0 loss: 0.816851  [   32/  126]
train() client id: f_00000-9-1 loss: 0.898097  [   64/  126]
train() client id: f_00000-9-2 loss: 0.709716  [   96/  126]
train() client id: f_00000-10-0 loss: 0.775355  [   32/  126]
train() client id: f_00000-10-1 loss: 0.656615  [   64/  126]
train() client id: f_00000-10-2 loss: 0.631189  [   96/  126]
train() client id: f_00000-11-0 loss: 0.717689  [   32/  126]
train() client id: f_00000-11-1 loss: 0.735566  [   64/  126]
train() client id: f_00000-11-2 loss: 0.753013  [   96/  126]
train() client id: f_00000-12-0 loss: 0.634573  [   32/  126]
train() client id: f_00000-12-1 loss: 0.656046  [   64/  126]
train() client id: f_00000-12-2 loss: 0.901770  [   96/  126]
train() client id: f_00001-0-0 loss: 0.408440  [   32/  265]
train() client id: f_00001-0-1 loss: 0.399278  [   64/  265]
train() client id: f_00001-0-2 loss: 0.376434  [   96/  265]
train() client id: f_00001-0-3 loss: 0.427895  [  128/  265]
train() client id: f_00001-0-4 loss: 0.445655  [  160/  265]
train() client id: f_00001-0-5 loss: 0.429280  [  192/  265]
train() client id: f_00001-0-6 loss: 0.535256  [  224/  265]
train() client id: f_00001-0-7 loss: 0.343209  [  256/  265]
train() client id: f_00001-1-0 loss: 0.374409  [   32/  265]
train() client id: f_00001-1-1 loss: 0.434566  [   64/  265]
train() client id: f_00001-1-2 loss: 0.370056  [   96/  265]
train() client id: f_00001-1-3 loss: 0.292646  [  128/  265]
train() client id: f_00001-1-4 loss: 0.406628  [  160/  265]
train() client id: f_00001-1-5 loss: 0.455333  [  192/  265]
train() client id: f_00001-1-6 loss: 0.475232  [  224/  265]
train() client id: f_00001-1-7 loss: 0.410405  [  256/  265]
train() client id: f_00001-2-0 loss: 0.427798  [   32/  265]
train() client id: f_00001-2-1 loss: 0.439215  [   64/  265]
train() client id: f_00001-2-2 loss: 0.578826  [   96/  265]
train() client id: f_00001-2-3 loss: 0.339866  [  128/  265]
train() client id: f_00001-2-4 loss: 0.451959  [  160/  265]
train() client id: f_00001-2-5 loss: 0.334321  [  192/  265]
train() client id: f_00001-2-6 loss: 0.367856  [  224/  265]
train() client id: f_00001-2-7 loss: 0.315086  [  256/  265]
train() client id: f_00001-3-0 loss: 0.448986  [   32/  265]
train() client id: f_00001-3-1 loss: 0.422520  [   64/  265]
train() client id: f_00001-3-2 loss: 0.331943  [   96/  265]
train() client id: f_00001-3-3 loss: 0.441092  [  128/  265]
train() client id: f_00001-3-4 loss: 0.290103  [  160/  265]
train() client id: f_00001-3-5 loss: 0.488206  [  192/  265]
train() client id: f_00001-3-6 loss: 0.361904  [  224/  265]
train() client id: f_00001-3-7 loss: 0.393229  [  256/  265]
train() client id: f_00001-4-0 loss: 0.348853  [   32/  265]
train() client id: f_00001-4-1 loss: 0.394942  [   64/  265]
train() client id: f_00001-4-2 loss: 0.420483  [   96/  265]
train() client id: f_00001-4-3 loss: 0.303690  [  128/  265]
train() client id: f_00001-4-4 loss: 0.495880  [  160/  265]
train() client id: f_00001-4-5 loss: 0.318907  [  192/  265]
train() client id: f_00001-4-6 loss: 0.479503  [  224/  265]
train() client id: f_00001-4-7 loss: 0.377847  [  256/  265]
train() client id: f_00001-5-0 loss: 0.391359  [   32/  265]
train() client id: f_00001-5-1 loss: 0.404598  [   64/  265]
train() client id: f_00001-5-2 loss: 0.363413  [   96/  265]
train() client id: f_00001-5-3 loss: 0.399787  [  128/  265]
train() client id: f_00001-5-4 loss: 0.349675  [  160/  265]
train() client id: f_00001-5-5 loss: 0.328166  [  192/  265]
train() client id: f_00001-5-6 loss: 0.379632  [  224/  265]
train() client id: f_00001-5-7 loss: 0.464209  [  256/  265]
train() client id: f_00001-6-0 loss: 0.396248  [   32/  265]
train() client id: f_00001-6-1 loss: 0.399246  [   64/  265]
train() client id: f_00001-6-2 loss: 0.339963  [   96/  265]
train() client id: f_00001-6-3 loss: 0.388942  [  128/  265]
train() client id: f_00001-6-4 loss: 0.388189  [  160/  265]
train() client id: f_00001-6-5 loss: 0.382788  [  192/  265]
train() client id: f_00001-6-6 loss: 0.455573  [  224/  265]
train() client id: f_00001-6-7 loss: 0.425866  [  256/  265]
train() client id: f_00001-7-0 loss: 0.444181  [   32/  265]
train() client id: f_00001-7-1 loss: 0.296669  [   64/  265]
train() client id: f_00001-7-2 loss: 0.463375  [   96/  265]
train() client id: f_00001-7-3 loss: 0.308911  [  128/  265]
train() client id: f_00001-7-4 loss: 0.371536  [  160/  265]
train() client id: f_00001-7-5 loss: 0.413439  [  192/  265]
train() client id: f_00001-7-6 loss: 0.458834  [  224/  265]
train() client id: f_00001-7-7 loss: 0.367929  [  256/  265]
train() client id: f_00001-8-0 loss: 0.391427  [   32/  265]
train() client id: f_00001-8-1 loss: 0.426099  [   64/  265]
train() client id: f_00001-8-2 loss: 0.354600  [   96/  265]
train() client id: f_00001-8-3 loss: 0.434176  [  128/  265]
train() client id: f_00001-8-4 loss: 0.355238  [  160/  265]
train() client id: f_00001-8-5 loss: 0.422298  [  192/  265]
train() client id: f_00001-8-6 loss: 0.424572  [  224/  265]
train() client id: f_00001-8-7 loss: 0.348537  [  256/  265]
train() client id: f_00001-9-0 loss: 0.331650  [   32/  265]
train() client id: f_00001-9-1 loss: 0.306318  [   64/  265]
train() client id: f_00001-9-2 loss: 0.408798  [   96/  265]
train() client id: f_00001-9-3 loss: 0.343043  [  128/  265]
train() client id: f_00001-9-4 loss: 0.518953  [  160/  265]
train() client id: f_00001-9-5 loss: 0.494341  [  192/  265]
train() client id: f_00001-9-6 loss: 0.312786  [  224/  265]
train() client id: f_00001-9-7 loss: 0.418875  [  256/  265]
train() client id: f_00001-10-0 loss: 0.483325  [   32/  265]
train() client id: f_00001-10-1 loss: 0.482039  [   64/  265]
train() client id: f_00001-10-2 loss: 0.304421  [   96/  265]
train() client id: f_00001-10-3 loss: 0.290433  [  128/  265]
train() client id: f_00001-10-4 loss: 0.374796  [  160/  265]
train() client id: f_00001-10-5 loss: 0.433906  [  192/  265]
train() client id: f_00001-10-6 loss: 0.315593  [  224/  265]
train() client id: f_00001-10-7 loss: 0.371218  [  256/  265]
train() client id: f_00001-11-0 loss: 0.467972  [   32/  265]
train() client id: f_00001-11-1 loss: 0.392512  [   64/  265]
train() client id: f_00001-11-2 loss: 0.292709  [   96/  265]
train() client id: f_00001-11-3 loss: 0.390482  [  128/  265]
train() client id: f_00001-11-4 loss: 0.412591  [  160/  265]
train() client id: f_00001-11-5 loss: 0.387158  [  192/  265]
train() client id: f_00001-11-6 loss: 0.458581  [  224/  265]
train() client id: f_00001-11-7 loss: 0.284608  [  256/  265]
train() client id: f_00001-12-0 loss: 0.286620  [   32/  265]
train() client id: f_00001-12-1 loss: 0.351661  [   64/  265]
train() client id: f_00001-12-2 loss: 0.428210  [   96/  265]
train() client id: f_00001-12-3 loss: 0.443179  [  128/  265]
train() client id: f_00001-12-4 loss: 0.393963  [  160/  265]
train() client id: f_00001-12-5 loss: 0.450302  [  192/  265]
train() client id: f_00001-12-6 loss: 0.377882  [  224/  265]
train() client id: f_00001-12-7 loss: 0.364355  [  256/  265]
train() client id: f_00002-0-0 loss: 1.440928  [   32/  124]
train() client id: f_00002-0-1 loss: 1.250928  [   64/  124]
train() client id: f_00002-0-2 loss: 1.298453  [   96/  124]
train() client id: f_00002-1-0 loss: 1.227936  [   32/  124]
train() client id: f_00002-1-1 loss: 1.307792  [   64/  124]
train() client id: f_00002-1-2 loss: 1.293154  [   96/  124]
train() client id: f_00002-2-0 loss: 1.035740  [   32/  124]
train() client id: f_00002-2-1 loss: 1.074702  [   64/  124]
train() client id: f_00002-2-2 loss: 1.470697  [   96/  124]
train() client id: f_00002-3-0 loss: 1.101698  [   32/  124]
train() client id: f_00002-3-1 loss: 1.195019  [   64/  124]
train() client id: f_00002-3-2 loss: 1.274157  [   96/  124]
train() client id: f_00002-4-0 loss: 1.183990  [   32/  124]
train() client id: f_00002-4-1 loss: 1.124135  [   64/  124]
train() client id: f_00002-4-2 loss: 1.160468  [   96/  124]
train() client id: f_00002-5-0 loss: 1.165637  [   32/  124]
train() client id: f_00002-5-1 loss: 1.071347  [   64/  124]
train() client id: f_00002-5-2 loss: 1.107063  [   96/  124]
train() client id: f_00002-6-0 loss: 1.345842  [   32/  124]
train() client id: f_00002-6-1 loss: 1.114520  [   64/  124]
train() client id: f_00002-6-2 loss: 1.070314  [   96/  124]
train() client id: f_00002-7-0 loss: 1.008148  [   32/  124]
train() client id: f_00002-7-1 loss: 1.064501  [   64/  124]
train() client id: f_00002-7-2 loss: 1.172934  [   96/  124]
train() client id: f_00002-8-0 loss: 1.051613  [   32/  124]
train() client id: f_00002-8-1 loss: 1.210685  [   64/  124]
train() client id: f_00002-8-2 loss: 1.072122  [   96/  124]
train() client id: f_00002-9-0 loss: 1.118358  [   32/  124]
train() client id: f_00002-9-1 loss: 1.118630  [   64/  124]
train() client id: f_00002-9-2 loss: 1.058816  [   96/  124]
train() client id: f_00002-10-0 loss: 1.063211  [   32/  124]
train() client id: f_00002-10-1 loss: 1.131601  [   64/  124]
train() client id: f_00002-10-2 loss: 1.185696  [   96/  124]
train() client id: f_00002-11-0 loss: 1.253598  [   32/  124]
train() client id: f_00002-11-1 loss: 0.854244  [   64/  124]
train() client id: f_00002-11-2 loss: 0.963323  [   96/  124]
train() client id: f_00002-12-0 loss: 1.174481  [   32/  124]
train() client id: f_00002-12-1 loss: 0.919582  [   64/  124]
train() client id: f_00002-12-2 loss: 1.092781  [   96/  124]
train() client id: f_00003-0-0 loss: 0.836132  [   32/   43]
train() client id: f_00003-1-0 loss: 0.553741  [   32/   43]
train() client id: f_00003-2-0 loss: 0.641984  [   32/   43]
train() client id: f_00003-3-0 loss: 0.753112  [   32/   43]
train() client id: f_00003-4-0 loss: 0.876188  [   32/   43]
train() client id: f_00003-5-0 loss: 0.671684  [   32/   43]
train() client id: f_00003-6-0 loss: 0.789735  [   32/   43]
train() client id: f_00003-7-0 loss: 0.733187  [   32/   43]
train() client id: f_00003-8-0 loss: 0.724232  [   32/   43]
train() client id: f_00003-9-0 loss: 0.739845  [   32/   43]
train() client id: f_00003-10-0 loss: 0.739040  [   32/   43]
train() client id: f_00003-11-0 loss: 0.684229  [   32/   43]
train() client id: f_00003-12-0 loss: 0.617320  [   32/   43]
train() client id: f_00004-0-0 loss: 0.776252  [   32/  306]
train() client id: f_00004-0-1 loss: 0.810252  [   64/  306]
train() client id: f_00004-0-2 loss: 0.672398  [   96/  306]
train() client id: f_00004-0-3 loss: 0.731090  [  128/  306]
train() client id: f_00004-0-4 loss: 0.715812  [  160/  306]
train() client id: f_00004-0-5 loss: 0.849077  [  192/  306]
train() client id: f_00004-0-6 loss: 0.864228  [  224/  306]
train() client id: f_00004-0-7 loss: 0.833031  [  256/  306]
train() client id: f_00004-0-8 loss: 0.670160  [  288/  306]
train() client id: f_00004-1-0 loss: 0.845899  [   32/  306]
train() client id: f_00004-1-1 loss: 0.718843  [   64/  306]
train() client id: f_00004-1-2 loss: 0.652191  [   96/  306]
train() client id: f_00004-1-3 loss: 0.708143  [  128/  306]
train() client id: f_00004-1-4 loss: 0.701983  [  160/  306]
train() client id: f_00004-1-5 loss: 0.774916  [  192/  306]
train() client id: f_00004-1-6 loss: 0.915746  [  224/  306]
train() client id: f_00004-1-7 loss: 0.887597  [  256/  306]
train() client id: f_00004-1-8 loss: 0.682272  [  288/  306]
train() client id: f_00004-2-0 loss: 0.802244  [   32/  306]
train() client id: f_00004-2-1 loss: 0.709518  [   64/  306]
train() client id: f_00004-2-2 loss: 0.773633  [   96/  306]
train() client id: f_00004-2-3 loss: 0.795607  [  128/  306]
train() client id: f_00004-2-4 loss: 0.850887  [  160/  306]
train() client id: f_00004-2-5 loss: 0.726591  [  192/  306]
train() client id: f_00004-2-6 loss: 0.767658  [  224/  306]
train() client id: f_00004-2-7 loss: 0.745026  [  256/  306]
train() client id: f_00004-2-8 loss: 0.840137  [  288/  306]
train() client id: f_00004-3-0 loss: 0.758996  [   32/  306]
train() client id: f_00004-3-1 loss: 0.694896  [   64/  306]
train() client id: f_00004-3-2 loss: 0.921336  [   96/  306]
train() client id: f_00004-3-3 loss: 0.747136  [  128/  306]
train() client id: f_00004-3-4 loss: 0.755136  [  160/  306]
train() client id: f_00004-3-5 loss: 0.729389  [  192/  306]
train() client id: f_00004-3-6 loss: 0.847185  [  224/  306]
train() client id: f_00004-3-7 loss: 0.817685  [  256/  306]
train() client id: f_00004-3-8 loss: 0.731902  [  288/  306]
train() client id: f_00004-4-0 loss: 0.739640  [   32/  306]
train() client id: f_00004-4-1 loss: 0.722786  [   64/  306]
train() client id: f_00004-4-2 loss: 0.750917  [   96/  306]
train() client id: f_00004-4-3 loss: 0.708323  [  128/  306]
train() client id: f_00004-4-4 loss: 0.799311  [  160/  306]
train() client id: f_00004-4-5 loss: 0.714229  [  192/  306]
train() client id: f_00004-4-6 loss: 0.944842  [  224/  306]
train() client id: f_00004-4-7 loss: 0.781949  [  256/  306]
train() client id: f_00004-4-8 loss: 0.786982  [  288/  306]
train() client id: f_00004-5-0 loss: 0.937812  [   32/  306]
train() client id: f_00004-5-1 loss: 0.721274  [   64/  306]
train() client id: f_00004-5-2 loss: 0.769589  [   96/  306]
train() client id: f_00004-5-3 loss: 0.761365  [  128/  306]
train() client id: f_00004-5-4 loss: 0.801877  [  160/  306]
train() client id: f_00004-5-5 loss: 0.801124  [  192/  306]
train() client id: f_00004-5-6 loss: 0.700446  [  224/  306]
train() client id: f_00004-5-7 loss: 0.824869  [  256/  306]
train() client id: f_00004-5-8 loss: 0.714255  [  288/  306]
train() client id: f_00004-6-0 loss: 0.763464  [   32/  306]
train() client id: f_00004-6-1 loss: 0.773670  [   64/  306]
train() client id: f_00004-6-2 loss: 0.749893  [   96/  306]
train() client id: f_00004-6-3 loss: 0.878355  [  128/  306]
train() client id: f_00004-6-4 loss: 0.826332  [  160/  306]
train() client id: f_00004-6-5 loss: 0.742496  [  192/  306]
train() client id: f_00004-6-6 loss: 0.728496  [  224/  306]
train() client id: f_00004-6-7 loss: 0.611474  [  256/  306]
train() client id: f_00004-6-8 loss: 0.889695  [  288/  306]
train() client id: f_00004-7-0 loss: 0.802719  [   32/  306]
train() client id: f_00004-7-1 loss: 0.812804  [   64/  306]
train() client id: f_00004-7-2 loss: 0.710604  [   96/  306]
train() client id: f_00004-7-3 loss: 0.652008  [  128/  306]
train() client id: f_00004-7-4 loss: 0.759534  [  160/  306]
train() client id: f_00004-7-5 loss: 0.775081  [  192/  306]
train() client id: f_00004-7-6 loss: 0.702693  [  224/  306]
train() client id: f_00004-7-7 loss: 0.734871  [  256/  306]
train() client id: f_00004-7-8 loss: 0.917700  [  288/  306]
train() client id: f_00004-8-0 loss: 0.823682  [   32/  306]
train() client id: f_00004-8-1 loss: 0.718056  [   64/  306]
train() client id: f_00004-8-2 loss: 0.812646  [   96/  306]
train() client id: f_00004-8-3 loss: 0.757675  [  128/  306]
train() client id: f_00004-8-4 loss: 0.924806  [  160/  306]
train() client id: f_00004-8-5 loss: 0.711451  [  192/  306]
train() client id: f_00004-8-6 loss: 0.709543  [  224/  306]
train() client id: f_00004-8-7 loss: 0.779592  [  256/  306]
train() client id: f_00004-8-8 loss: 0.811983  [  288/  306]
train() client id: f_00004-9-0 loss: 0.775500  [   32/  306]
train() client id: f_00004-9-1 loss: 0.642139  [   64/  306]
train() client id: f_00004-9-2 loss: 0.908975  [   96/  306]
train() client id: f_00004-9-3 loss: 0.804424  [  128/  306]
train() client id: f_00004-9-4 loss: 0.777785  [  160/  306]
train() client id: f_00004-9-5 loss: 0.759900  [  192/  306]
train() client id: f_00004-9-6 loss: 0.781846  [  224/  306]
train() client id: f_00004-9-7 loss: 0.712710  [  256/  306]
train() client id: f_00004-9-8 loss: 0.790618  [  288/  306]
train() client id: f_00004-10-0 loss: 0.844925  [   32/  306]
train() client id: f_00004-10-1 loss: 0.762575  [   64/  306]
train() client id: f_00004-10-2 loss: 0.842282  [   96/  306]
train() client id: f_00004-10-3 loss: 0.835545  [  128/  306]
train() client id: f_00004-10-4 loss: 0.788450  [  160/  306]
train() client id: f_00004-10-5 loss: 0.714310  [  192/  306]
train() client id: f_00004-10-6 loss: 0.762155  [  224/  306]
train() client id: f_00004-10-7 loss: 0.684851  [  256/  306]
train() client id: f_00004-10-8 loss: 0.677440  [  288/  306]
train() client id: f_00004-11-0 loss: 0.791853  [   32/  306]
train() client id: f_00004-11-1 loss: 0.779796  [   64/  306]
train() client id: f_00004-11-2 loss: 0.801539  [   96/  306]
train() client id: f_00004-11-3 loss: 0.725163  [  128/  306]
train() client id: f_00004-11-4 loss: 0.750342  [  160/  306]
train() client id: f_00004-11-5 loss: 0.774909  [  192/  306]
train() client id: f_00004-11-6 loss: 0.739612  [  224/  306]
train() client id: f_00004-11-7 loss: 0.798796  [  256/  306]
train() client id: f_00004-11-8 loss: 0.831571  [  288/  306]
train() client id: f_00004-12-0 loss: 0.871998  [   32/  306]
train() client id: f_00004-12-1 loss: 0.793103  [   64/  306]
train() client id: f_00004-12-2 loss: 0.677943  [   96/  306]
train() client id: f_00004-12-3 loss: 0.842621  [  128/  306]
train() client id: f_00004-12-4 loss: 0.754957  [  160/  306]
train() client id: f_00004-12-5 loss: 0.692612  [  192/  306]
train() client id: f_00004-12-6 loss: 0.764073  [  224/  306]
train() client id: f_00004-12-7 loss: 0.673883  [  256/  306]
train() client id: f_00004-12-8 loss: 0.858864  [  288/  306]
train() client id: f_00005-0-0 loss: 0.585204  [   32/  146]
train() client id: f_00005-0-1 loss: 0.422625  [   64/  146]
train() client id: f_00005-0-2 loss: 0.374615  [   96/  146]
train() client id: f_00005-0-3 loss: 0.813468  [  128/  146]
train() client id: f_00005-1-0 loss: 0.381286  [   32/  146]
train() client id: f_00005-1-1 loss: 0.575663  [   64/  146]
train() client id: f_00005-1-2 loss: 0.613253  [   96/  146]
train() client id: f_00005-1-3 loss: 0.897176  [  128/  146]
train() client id: f_00005-2-0 loss: 0.476369  [   32/  146]
train() client id: f_00005-2-1 loss: 0.482386  [   64/  146]
train() client id: f_00005-2-2 loss: 0.545260  [   96/  146]
train() client id: f_00005-2-3 loss: 0.417723  [  128/  146]
train() client id: f_00005-3-0 loss: 0.220901  [   32/  146]
train() client id: f_00005-3-1 loss: 0.596904  [   64/  146]
train() client id: f_00005-3-2 loss: 0.866873  [   96/  146]
train() client id: f_00005-3-3 loss: 0.665979  [  128/  146]
train() client id: f_00005-4-0 loss: 0.527207  [   32/  146]
train() client id: f_00005-4-1 loss: 0.362738  [   64/  146]
train() client id: f_00005-4-2 loss: 0.542467  [   96/  146]
train() client id: f_00005-4-3 loss: 0.863432  [  128/  146]
train() client id: f_00005-5-0 loss: 0.559486  [   32/  146]
train() client id: f_00005-5-1 loss: 0.346123  [   64/  146]
train() client id: f_00005-5-2 loss: 0.728888  [   96/  146]
train() client id: f_00005-5-3 loss: 0.567103  [  128/  146]
train() client id: f_00005-6-0 loss: 0.496164  [   32/  146]
train() client id: f_00005-6-1 loss: 0.509502  [   64/  146]
train() client id: f_00005-6-2 loss: 0.523934  [   96/  146]
train() client id: f_00005-6-3 loss: 0.706851  [  128/  146]
train() client id: f_00005-7-0 loss: 0.466085  [   32/  146]
train() client id: f_00005-7-1 loss: 0.463488  [   64/  146]
train() client id: f_00005-7-2 loss: 0.638808  [   96/  146]
train() client id: f_00005-7-3 loss: 0.687339  [  128/  146]
train() client id: f_00005-8-0 loss: 0.839561  [   32/  146]
train() client id: f_00005-8-1 loss: 0.604409  [   64/  146]
train() client id: f_00005-8-2 loss: 0.224479  [   96/  146]
train() client id: f_00005-8-3 loss: 0.797491  [  128/  146]
train() client id: f_00005-9-0 loss: 0.491894  [   32/  146]
train() client id: f_00005-9-1 loss: 0.643973  [   64/  146]
train() client id: f_00005-9-2 loss: 0.616358  [   96/  146]
train() client id: f_00005-9-3 loss: 0.515914  [  128/  146]
train() client id: f_00005-10-0 loss: 0.593517  [   32/  146]
train() client id: f_00005-10-1 loss: 0.609573  [   64/  146]
train() client id: f_00005-10-2 loss: 0.649117  [   96/  146]
train() client id: f_00005-10-3 loss: 0.366555  [  128/  146]
train() client id: f_00005-11-0 loss: 0.604048  [   32/  146]
train() client id: f_00005-11-1 loss: 0.680057  [   64/  146]
train() client id: f_00005-11-2 loss: 0.448361  [   96/  146]
train() client id: f_00005-11-3 loss: 0.418058  [  128/  146]
train() client id: f_00005-12-0 loss: 0.769318  [   32/  146]
train() client id: f_00005-12-1 loss: 0.430286  [   64/  146]
train() client id: f_00005-12-2 loss: 0.447525  [   96/  146]
train() client id: f_00005-12-3 loss: 0.575795  [  128/  146]
train() client id: f_00006-0-0 loss: 0.470214  [   32/   54]
train() client id: f_00006-1-0 loss: 0.389138  [   32/   54]
train() client id: f_00006-2-0 loss: 0.492584  [   32/   54]
train() client id: f_00006-3-0 loss: 0.393921  [   32/   54]
train() client id: f_00006-4-0 loss: 0.427735  [   32/   54]
train() client id: f_00006-5-0 loss: 0.425164  [   32/   54]
train() client id: f_00006-6-0 loss: 0.372225  [   32/   54]
train() client id: f_00006-7-0 loss: 0.475562  [   32/   54]
train() client id: f_00006-8-0 loss: 0.385733  [   32/   54]
train() client id: f_00006-9-0 loss: 0.481458  [   32/   54]
train() client id: f_00006-10-0 loss: 0.427364  [   32/   54]
train() client id: f_00006-11-0 loss: 0.452639  [   32/   54]
train() client id: f_00006-12-0 loss: 0.378923  [   32/   54]
train() client id: f_00007-0-0 loss: 0.987024  [   32/  179]
train() client id: f_00007-0-1 loss: 0.561726  [   64/  179]
train() client id: f_00007-0-2 loss: 0.549289  [   96/  179]
train() client id: f_00007-0-3 loss: 0.594513  [  128/  179]
train() client id: f_00007-0-4 loss: 0.639394  [  160/  179]
train() client id: f_00007-1-0 loss: 0.532397  [   32/  179]
train() client id: f_00007-1-1 loss: 0.565749  [   64/  179]
train() client id: f_00007-1-2 loss: 0.612340  [   96/  179]
train() client id: f_00007-1-3 loss: 0.708511  [  128/  179]
train() client id: f_00007-1-4 loss: 0.724311  [  160/  179]
train() client id: f_00007-2-0 loss: 0.559612  [   32/  179]
train() client id: f_00007-2-1 loss: 0.506638  [   64/  179]
train() client id: f_00007-2-2 loss: 0.638625  [   96/  179]
train() client id: f_00007-2-3 loss: 0.744169  [  128/  179]
train() client id: f_00007-2-4 loss: 0.673184  [  160/  179]
train() client id: f_00007-3-0 loss: 0.828086  [   32/  179]
train() client id: f_00007-3-1 loss: 0.570002  [   64/  179]
train() client id: f_00007-3-2 loss: 0.496265  [   96/  179]
train() client id: f_00007-3-3 loss: 0.555084  [  128/  179]
train() client id: f_00007-3-4 loss: 0.492787  [  160/  179]
train() client id: f_00007-4-0 loss: 0.577586  [   32/  179]
train() client id: f_00007-4-1 loss: 0.445185  [   64/  179]
train() client id: f_00007-4-2 loss: 0.506792  [   96/  179]
train() client id: f_00007-4-3 loss: 0.730794  [  128/  179]
train() client id: f_00007-4-4 loss: 0.555463  [  160/  179]
train() client id: f_00007-5-0 loss: 0.474798  [   32/  179]
train() client id: f_00007-5-1 loss: 0.513349  [   64/  179]
train() client id: f_00007-5-2 loss: 0.628337  [   96/  179]
train() client id: f_00007-5-3 loss: 0.620080  [  128/  179]
train() client id: f_00007-5-4 loss: 0.734684  [  160/  179]
train() client id: f_00007-6-0 loss: 0.548795  [   32/  179]
train() client id: f_00007-6-1 loss: 0.670864  [   64/  179]
train() client id: f_00007-6-2 loss: 0.656422  [   96/  179]
train() client id: f_00007-6-3 loss: 0.587153  [  128/  179]
train() client id: f_00007-6-4 loss: 0.453740  [  160/  179]
train() client id: f_00007-7-0 loss: 0.503699  [   32/  179]
train() client id: f_00007-7-1 loss: 0.472311  [   64/  179]
train() client id: f_00007-7-2 loss: 0.563034  [   96/  179]
train() client id: f_00007-7-3 loss: 0.605438  [  128/  179]
train() client id: f_00007-7-4 loss: 0.569916  [  160/  179]
train() client id: f_00007-8-0 loss: 0.573729  [   32/  179]
train() client id: f_00007-8-1 loss: 0.454772  [   64/  179]
train() client id: f_00007-8-2 loss: 0.639194  [   96/  179]
train() client id: f_00007-8-3 loss: 0.519029  [  128/  179]
train() client id: f_00007-8-4 loss: 0.688582  [  160/  179]
train() client id: f_00007-9-0 loss: 0.441698  [   32/  179]
train() client id: f_00007-9-1 loss: 0.517075  [   64/  179]
train() client id: f_00007-9-2 loss: 0.603936  [   96/  179]
train() client id: f_00007-9-3 loss: 0.564468  [  128/  179]
train() client id: f_00007-9-4 loss: 0.499938  [  160/  179]
train() client id: f_00007-10-0 loss: 0.473107  [   32/  179]
train() client id: f_00007-10-1 loss: 0.581578  [   64/  179]
train() client id: f_00007-10-2 loss: 0.488686  [   96/  179]
train() client id: f_00007-10-3 loss: 0.671237  [  128/  179]
train() client id: f_00007-10-4 loss: 0.773995  [  160/  179]
train() client id: f_00007-11-0 loss: 0.489580  [   32/  179]
train() client id: f_00007-11-1 loss: 0.495191  [   64/  179]
train() client id: f_00007-11-2 loss: 0.459003  [   96/  179]
train() client id: f_00007-11-3 loss: 0.438178  [  128/  179]
train() client id: f_00007-11-4 loss: 0.606683  [  160/  179]
train() client id: f_00007-12-0 loss: 0.775805  [   32/  179]
train() client id: f_00007-12-1 loss: 0.550311  [   64/  179]
train() client id: f_00007-12-2 loss: 0.433601  [   96/  179]
train() client id: f_00007-12-3 loss: 0.612351  [  128/  179]
train() client id: f_00007-12-4 loss: 0.514784  [  160/  179]
train() client id: f_00008-0-0 loss: 0.600960  [   32/  130]
train() client id: f_00008-0-1 loss: 0.574004  [   64/  130]
train() client id: f_00008-0-2 loss: 0.573166  [   96/  130]
train() client id: f_00008-0-3 loss: 0.549580  [  128/  130]
train() client id: f_00008-1-0 loss: 0.595924  [   32/  130]
train() client id: f_00008-1-1 loss: 0.519443  [   64/  130]
train() client id: f_00008-1-2 loss: 0.622180  [   96/  130]
train() client id: f_00008-1-3 loss: 0.573216  [  128/  130]
train() client id: f_00008-2-0 loss: 0.663922  [   32/  130]
train() client id: f_00008-2-1 loss: 0.566375  [   64/  130]
train() client id: f_00008-2-2 loss: 0.577935  [   96/  130]
train() client id: f_00008-2-3 loss: 0.495732  [  128/  130]
train() client id: f_00008-3-0 loss: 0.452948  [   32/  130]
train() client id: f_00008-3-1 loss: 0.714149  [   64/  130]
train() client id: f_00008-3-2 loss: 0.504662  [   96/  130]
train() client id: f_00008-3-3 loss: 0.582839  [  128/  130]
train() client id: f_00008-4-0 loss: 0.635306  [   32/  130]
train() client id: f_00008-4-1 loss: 0.548776  [   64/  130]
train() client id: f_00008-4-2 loss: 0.486005  [   96/  130]
train() client id: f_00008-4-3 loss: 0.612250  [  128/  130]
train() client id: f_00008-5-0 loss: 0.642435  [   32/  130]
train() client id: f_00008-5-1 loss: 0.577101  [   64/  130]
train() client id: f_00008-5-2 loss: 0.514319  [   96/  130]
train() client id: f_00008-5-3 loss: 0.515591  [  128/  130]
train() client id: f_00008-6-0 loss: 0.527766  [   32/  130]
train() client id: f_00008-6-1 loss: 0.648273  [   64/  130]
train() client id: f_00008-6-2 loss: 0.552937  [   96/  130]
train() client id: f_00008-6-3 loss: 0.573877  [  128/  130]
train() client id: f_00008-7-0 loss: 0.594367  [   32/  130]
train() client id: f_00008-7-1 loss: 0.551273  [   64/  130]
train() client id: f_00008-7-2 loss: 0.601282  [   96/  130]
train() client id: f_00008-7-3 loss: 0.530707  [  128/  130]
train() client id: f_00008-8-0 loss: 0.544053  [   32/  130]
train() client id: f_00008-8-1 loss: 0.505845  [   64/  130]
train() client id: f_00008-8-2 loss: 0.616776  [   96/  130]
train() client id: f_00008-8-3 loss: 0.608289  [  128/  130]
train() client id: f_00008-9-0 loss: 0.597493  [   32/  130]
train() client id: f_00008-9-1 loss: 0.478065  [   64/  130]
train() client id: f_00008-9-2 loss: 0.509494  [   96/  130]
train() client id: f_00008-9-3 loss: 0.686660  [  128/  130]
train() client id: f_00008-10-0 loss: 0.547010  [   32/  130]
train() client id: f_00008-10-1 loss: 0.501493  [   64/  130]
train() client id: f_00008-10-2 loss: 0.618090  [   96/  130]
train() client id: f_00008-10-3 loss: 0.623501  [  128/  130]
train() client id: f_00008-11-0 loss: 0.497052  [   32/  130]
train() client id: f_00008-11-1 loss: 0.627140  [   64/  130]
train() client id: f_00008-11-2 loss: 0.584996  [   96/  130]
train() client id: f_00008-11-3 loss: 0.566852  [  128/  130]
train() client id: f_00008-12-0 loss: 0.538726  [   32/  130]
train() client id: f_00008-12-1 loss: 0.637842  [   64/  130]
train() client id: f_00008-12-2 loss: 0.561557  [   96/  130]
train() client id: f_00008-12-3 loss: 0.545067  [  128/  130]
train() client id: f_00009-0-0 loss: 0.783928  [   32/  118]
train() client id: f_00009-0-1 loss: 0.938787  [   64/  118]
train() client id: f_00009-0-2 loss: 1.045232  [   96/  118]
train() client id: f_00009-1-0 loss: 0.954044  [   32/  118]
train() client id: f_00009-1-1 loss: 0.802560  [   64/  118]
train() client id: f_00009-1-2 loss: 0.985042  [   96/  118]
train() client id: f_00009-2-0 loss: 0.829125  [   32/  118]
train() client id: f_00009-2-1 loss: 0.817064  [   64/  118]
train() client id: f_00009-2-2 loss: 1.004335  [   96/  118]
train() client id: f_00009-3-0 loss: 0.950007  [   32/  118]
train() client id: f_00009-3-1 loss: 0.827443  [   64/  118]
train() client id: f_00009-3-2 loss: 0.620318  [   96/  118]
train() client id: f_00009-4-0 loss: 0.695857  [   32/  118]
train() client id: f_00009-4-1 loss: 0.873040  [   64/  118]
train() client id: f_00009-4-2 loss: 0.791859  [   96/  118]
train() client id: f_00009-5-0 loss: 0.830534  [   32/  118]
train() client id: f_00009-5-1 loss: 0.811468  [   64/  118]
train() client id: f_00009-5-2 loss: 0.745525  [   96/  118]
train() client id: f_00009-6-0 loss: 0.986705  [   32/  118]
train() client id: f_00009-6-1 loss: 0.650237  [   64/  118]
train() client id: f_00009-6-2 loss: 0.710149  [   96/  118]
train() client id: f_00009-7-0 loss: 0.835414  [   32/  118]
train() client id: f_00009-7-1 loss: 0.794057  [   64/  118]
train() client id: f_00009-7-2 loss: 0.626126  [   96/  118]
train() client id: f_00009-8-0 loss: 0.872877  [   32/  118]
train() client id: f_00009-8-1 loss: 0.748998  [   64/  118]
train() client id: f_00009-8-2 loss: 0.635183  [   96/  118]
train() client id: f_00009-9-0 loss: 0.728495  [   32/  118]
train() client id: f_00009-9-1 loss: 0.687222  [   64/  118]
train() client id: f_00009-9-2 loss: 0.816777  [   96/  118]
train() client id: f_00009-10-0 loss: 0.754080  [   32/  118]
train() client id: f_00009-10-1 loss: 0.818686  [   64/  118]
train() client id: f_00009-10-2 loss: 0.648868  [   96/  118]
train() client id: f_00009-11-0 loss: 0.763942  [   32/  118]
train() client id: f_00009-11-1 loss: 0.647976  [   64/  118]
train() client id: f_00009-11-2 loss: 0.649772  [   96/  118]
train() client id: f_00009-12-0 loss: 0.864404  [   32/  118]
train() client id: f_00009-12-1 loss: 0.661336  [   64/  118]
train() client id: f_00009-12-2 loss: 0.641731  [   96/  118]
At round 70 accuracy: 0.6472148541114059
At round 70 training accuracy: 0.5888665325285044
At round 70 training loss: 0.8446410598571892
update_location
xs = 8.927491 471.223621 5.882650 0.934260 -387.581990 -235.230757 -195.849135 -5.143845 -410.120581 20.134486 
ys = -462.390647 7.291448 360.684448 -182.290817 -9.642386 0.794442 -1.381692 356.628436 25.881276 -897.232496 
xs mean: -72.68237997052123
ys mean: -80.16579882624052
dists_uav = 473.164676 481.772629 374.336582 207.920213 400.390777 255.605439 219.906327 370.419088 422.928755 903.012486 
uav_gains = -123.625336 -123.843801 -120.369658 -108.186455 -121.417757 -111.779903 -108.983786 -120.193276 -122.189299 -130.866991 
uav_gains_db_mean: -119.14562620058956
dists_bs = 663.397437 667.630981 259.224319 398.258198 281.573992 184.323955 177.609650 248.676501 278.420691 1089.844022 
bs_gains = -118.576725 -118.654081 -107.149980 -112.371671 -108.155650 -103.003349 -102.552123 -106.644832 -108.018701 -124.613261 
bs_gains_db_mean: -110.97403735621324
Round 71
-------------------------------
ene_coms = [0.04739591 0.04912495 0.00893911 0.00950827 0.00949036 0.01214404
 0.00999523 0.00868479 0.00941151 0.05591192]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.66723969 3.30928086 1.49369581 0.54297349 3.63702569 1.76502168
 0.674285   2.13976347 1.56612691 1.60634165]
obj_prev = 18.40175423601202
eta_min = 3.984745458874617e-59	eta_max = 0.9179177026053706
af = 3.7709332754836535	bf = 0.8519472000441574	zeta = 4.14802660303202	eta = 0.909090909090909
af = 3.7709332754836535	bf = 0.8519472000441574	zeta = 13.32491081416676	eta = 0.28299876284909
af = 3.7709332754836535	bf = 0.8519472000441574	zeta = 7.827105551932599	eta = 0.48177876872409986
af = 3.7709332754836535	bf = 0.8519472000441574	zeta = 6.957931076893842	eta = 0.5419618610489417
af = 3.7709332754836535	bf = 0.8519472000441574	zeta = 6.903068735068643	eta = 0.5462691188813951
af = 3.7709332754836535	bf = 0.8519472000441574	zeta = 6.902809018659397	eta = 0.5462896721161222
af = 3.7709332754836535	bf = 0.8519472000441574	zeta = 6.902809012776849	eta = 0.5462896725816683
eta = 0.5462896725816683
ene_coms = [0.04739591 0.04912495 0.00893911 0.00950827 0.00949036 0.01214404
 0.00999523 0.00868479 0.00941151 0.05591192]
ene_comp = [0.04989063 0.10492871 0.04909872 0.01702617 0.12116296 0.05780978
 0.0213817  0.07087637 0.05147446 0.04672297]
ene_total = [0.82807416 1.31125899 0.49400078 0.2258533  1.11208233 0.59542618
 0.26707115 0.677201   0.51824335 0.87359777]
ti_comp = [2.62572155 2.60843117 3.01028951 3.00459789 3.00477699 2.97824019
 2.99972829 3.01283275 3.00556549 2.54056141]
ti_coms = [0.47395909 0.49124946 0.08939113 0.09508274 0.09490364 0.12144044
 0.09995234 0.08684789 0.09411515 0.55911922]
t_total = [26.38356209 26.38356209 26.38356209 26.38356209 26.38356209 26.38356209
 26.38356209 26.38356209 26.38356209 26.38356209]
ene_coms = [0.04739591 0.04912495 0.00893911 0.00950827 0.00949036 0.01214404
 0.00999523 0.00868479 0.00941151 0.05591192]
ene_comp = [1.12574404e-06 1.06121704e-05 8.16345272e-07 3.41710250e-08
 1.23130359e-05 1.36133334e-06 6.78957824e-08 2.45151414e-06
 9.43632787e-07 9.87670272e-07]
ene_total = [0.4034295  0.4182273  0.07609402 0.0809319  0.08088397 0.10337809
 0.08507705 0.07394321 0.08011605 0.4759141 ]
optimize_network iter = 0 obj = 1.8779951864935618
eta = 0.5462896725816683
freqs = [ 9500365.80548897 20113374.61510869  8155148.44812186  2833352.10157694
 20161722.44849138  9705358.58044865  3563939.23954238 11762413.93001535
  8563190.5393301   9195402.8413313 ]
eta_min = 0.5462896725816692	eta_max = 0.689377695566324
af = 0.0001961788800053978	bf = 0.8519472000441574	zeta = 0.00021579676800593758	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.04739591 0.04912495 0.00893911 0.00950827 0.00949036 0.01214404
 0.00999523 0.00868479 0.00941151 0.05591192]
ene_comp = [1.77462485e-07 1.67290437e-06 1.28688810e-07 5.38672628e-09
 1.94102910e-06 2.14600824e-07 1.07031029e-08 3.86457113e-07
 1.48754436e-07 1.55696512e-07]
ene_total = [2.01340115 2.08691473 0.37974188 0.4039148  0.40323621 0.51589205
 0.42460125 0.36894907 0.39981052 2.37516323]
ti_comp = [1.64816572 1.63087535 2.03273368 2.02704207 2.02722117 2.00068437
 2.02217247 2.03527692 2.02800966 1.56300559]
ti_coms = [0.47395909 0.49124946 0.08939113 0.09508274 0.09490364 0.12144044
 0.09995234 0.08684789 0.09411515 0.55911922]
t_total = [26.38356209 26.38356209 26.38356209 26.38356209 26.38356209 26.38356209
 26.38356209 26.38356209 26.38356209 26.38356209]
ene_coms = [0.04739591 0.04912495 0.00893911 0.00950827 0.00949036 0.01214404
 0.00999523 0.00868479 0.00941151 0.05591192]
ene_comp = [6.65315646e-07 6.32140973e-06 4.16890448e-07 1.74822650e-08
 6.29911983e-06 7.02456224e-07 3.47906666e-08 1.25092114e-06
 4.82622842e-07 6.07635047e-07]
ene_total = [0.58926339 0.61083016 0.11114174 0.11821293 0.11806836 0.15099096
 0.12426734 0.10799019 0.11701574 0.69513899]
optimize_network iter = 1 obj = 2.7429198151175647
eta = 0.689377695566324
freqs = [ 9311485.08018276 19791303.07019867  7430028.60088422  2583776.70000257
 18385251.62087849  8888399.3799377   3252556.51054203 10712203.31352771
  7807690.97955605  9195402.8413313 ]
eta_min = 0.6893776955663258	eta_max = 0.689377695566324
af = 0.0001744078978900331	bf = 0.8519472000441574	zeta = 0.0001918486876790364	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.04739591 0.04912495 0.00893911 0.00950827 0.00949036 0.01214404
 0.00999523 0.00868479 0.00941151 0.05591192]
ene_comp = [1.70476219e-07 1.61975754e-06 1.06821338e-07 4.47954360e-09
 1.61404612e-06 1.79992883e-07 8.91453755e-09 3.20528021e-07
 1.23664186e-07 1.55696512e-07]
ene_total = [2.01340085 2.08691247 0.37974095 0.40391476 0.40322232 0.51589058
 0.42460118 0.36894627 0.39980945 2.37516323]
ti_comp = [1.64816572 1.63087535 2.03273368 2.02704207 2.02722117 2.00068437
 2.02217247 2.03527692 2.02800966 1.56300559]
ti_coms = [0.47395909 0.49124946 0.08939113 0.09508274 0.09490364 0.12144044
 0.09995234 0.08684789 0.09411515 0.55911922]
t_total = [26.38356209 26.38356209 26.38356209 26.38356209 26.38356209 26.38356209
 26.38356209 26.38356209 26.38356209 26.38356209]
ene_coms = [0.04739591 0.04912495 0.00893911 0.00950827 0.00949036 0.01214404
 0.00999523 0.00868479 0.00941151 0.05591192]
ene_comp = [6.65315646e-07 6.32140973e-06 4.16890448e-07 1.74822650e-08
 6.29911983e-06 7.02456224e-07 3.47906666e-08 1.25092114e-06
 4.82622842e-07 6.07635047e-07]
ene_total = [0.58926339 0.61083016 0.11114174 0.11821293 0.11806836 0.15099096
 0.12426734 0.10799019 0.11701574 0.69513899]
optimize_network iter = 2 obj = 2.7429198151175647
eta = 0.689377695566324
freqs = [ 9311485.08018276 19791303.07019867  7430028.60088422  2583776.70000257
 18385251.62087849  8888399.3799377   3252556.51054203 10712203.31352771
  7807690.97955605  9195402.8413313 ]
Done!
ene_coms = [0.04739591 0.04912495 0.00893911 0.00950827 0.00949036 0.01214404
 0.00999523 0.00868479 0.00941151 0.05591192]
ene_comp = [6.55480383e-07 6.22796127e-06 4.10727618e-07 1.72238273e-08
 6.20600088e-06 6.92071917e-07 3.42763613e-08 1.23242896e-06
 4.75488300e-07 5.98652469e-07]
ene_total = [0.04739656 0.04913117 0.00893952 0.00950829 0.00949657 0.01214474
 0.00999527 0.00868602 0.00941199 0.05591252]
At round 71 energy consumption: 0.22062265948504667
At round 71 eta: 0.689377695566324
At round 71 a_n: 3.8618477214328486
At round 71 local rounds: 12.18005596044799
At round 71 global rounds: 12.43261564385641
gradient difference: 0.3667824864387512
train() client id: f_00000-0-0 loss: 1.167731  [   32/  126]
train() client id: f_00000-0-1 loss: 1.105384  [   64/  126]
train() client id: f_00000-0-2 loss: 1.167072  [   96/  126]
train() client id: f_00000-1-0 loss: 0.849542  [   32/  126]
train() client id: f_00000-1-1 loss: 1.063673  [   64/  126]
train() client id: f_00000-1-2 loss: 0.892418  [   96/  126]
train() client id: f_00000-2-0 loss: 0.889088  [   32/  126]
train() client id: f_00000-2-1 loss: 0.894494  [   64/  126]
train() client id: f_00000-2-2 loss: 0.875646  [   96/  126]
train() client id: f_00000-3-0 loss: 0.787051  [   32/  126]
train() client id: f_00000-3-1 loss: 0.744523  [   64/  126]
train() client id: f_00000-3-2 loss: 0.887103  [   96/  126]
train() client id: f_00000-4-0 loss: 0.884834  [   32/  126]
train() client id: f_00000-4-1 loss: 0.688891  [   64/  126]
train() client id: f_00000-4-2 loss: 0.782828  [   96/  126]
train() client id: f_00000-5-0 loss: 0.769737  [   32/  126]
train() client id: f_00000-5-1 loss: 0.870703  [   64/  126]
train() client id: f_00000-5-2 loss: 0.515541  [   96/  126]
train() client id: f_00000-6-0 loss: 0.704670  [   32/  126]
train() client id: f_00000-6-1 loss: 0.755999  [   64/  126]
train() client id: f_00000-6-2 loss: 0.631720  [   96/  126]
train() client id: f_00000-7-0 loss: 0.549033  [   32/  126]
train() client id: f_00000-7-1 loss: 0.730395  [   64/  126]
train() client id: f_00000-7-2 loss: 0.658225  [   96/  126]
train() client id: f_00000-8-0 loss: 0.652076  [   32/  126]
train() client id: f_00000-8-1 loss: 0.641950  [   64/  126]
train() client id: f_00000-8-2 loss: 0.721008  [   96/  126]
train() client id: f_00000-9-0 loss: 0.704140  [   32/  126]
train() client id: f_00000-9-1 loss: 0.606954  [   64/  126]
train() client id: f_00000-9-2 loss: 0.670764  [   96/  126]
train() client id: f_00000-10-0 loss: 0.669770  [   32/  126]
train() client id: f_00000-10-1 loss: 0.573332  [   64/  126]
train() client id: f_00000-10-2 loss: 0.681447  [   96/  126]
train() client id: f_00000-11-0 loss: 0.666274  [   32/  126]
train() client id: f_00000-11-1 loss: 0.578946  [   64/  126]
train() client id: f_00000-11-2 loss: 0.607545  [   96/  126]
train() client id: f_00001-0-0 loss: 0.448360  [   32/  265]
train() client id: f_00001-0-1 loss: 0.413698  [   64/  265]
train() client id: f_00001-0-2 loss: 0.506386  [   96/  265]
train() client id: f_00001-0-3 loss: 0.608115  [  128/  265]
train() client id: f_00001-0-4 loss: 0.367667  [  160/  265]
train() client id: f_00001-0-5 loss: 0.409841  [  192/  265]
train() client id: f_00001-0-6 loss: 0.348948  [  224/  265]
train() client id: f_00001-0-7 loss: 0.518700  [  256/  265]
train() client id: f_00001-1-0 loss: 0.389205  [   32/  265]
train() client id: f_00001-1-1 loss: 0.377596  [   64/  265]
train() client id: f_00001-1-2 loss: 0.612875  [   96/  265]
train() client id: f_00001-1-3 loss: 0.379788  [  128/  265]
train() client id: f_00001-1-4 loss: 0.400524  [  160/  265]
train() client id: f_00001-1-5 loss: 0.521929  [  192/  265]
train() client id: f_00001-1-6 loss: 0.360677  [  224/  265]
train() client id: f_00001-1-7 loss: 0.524211  [  256/  265]
train() client id: f_00001-2-0 loss: 0.365766  [   32/  265]
train() client id: f_00001-2-1 loss: 0.406989  [   64/  265]
train() client id: f_00001-2-2 loss: 0.425931  [   96/  265]
train() client id: f_00001-2-3 loss: 0.357884  [  128/  265]
train() client id: f_00001-2-4 loss: 0.403501  [  160/  265]
train() client id: f_00001-2-5 loss: 0.548747  [  192/  265]
train() client id: f_00001-2-6 loss: 0.543387  [  224/  265]
train() client id: f_00001-2-7 loss: 0.428045  [  256/  265]
train() client id: f_00001-3-0 loss: 0.384400  [   32/  265]
train() client id: f_00001-3-1 loss: 0.334064  [   64/  265]
train() client id: f_00001-3-2 loss: 0.412252  [   96/  265]
train() client id: f_00001-3-3 loss: 0.429587  [  128/  265]
train() client id: f_00001-3-4 loss: 0.453018  [  160/  265]
train() client id: f_00001-3-5 loss: 0.359425  [  192/  265]
train() client id: f_00001-3-6 loss: 0.477668  [  224/  265]
train() client id: f_00001-3-7 loss: 0.627160  [  256/  265]
train() client id: f_00001-4-0 loss: 0.411221  [   32/  265]
train() client id: f_00001-4-1 loss: 0.403885  [   64/  265]
train() client id: f_00001-4-2 loss: 0.484455  [   96/  265]
train() client id: f_00001-4-3 loss: 0.407791  [  128/  265]
train() client id: f_00001-4-4 loss: 0.504248  [  160/  265]
train() client id: f_00001-4-5 loss: 0.433312  [  192/  265]
train() client id: f_00001-4-6 loss: 0.473002  [  224/  265]
train() client id: f_00001-4-7 loss: 0.345912  [  256/  265]
train() client id: f_00001-5-0 loss: 0.410802  [   32/  265]
train() client id: f_00001-5-1 loss: 0.441605  [   64/  265]
train() client id: f_00001-5-2 loss: 0.484227  [   96/  265]
train() client id: f_00001-5-3 loss: 0.308134  [  128/  265]
train() client id: f_00001-5-4 loss: 0.465541  [  160/  265]
train() client id: f_00001-5-5 loss: 0.538252  [  192/  265]
train() client id: f_00001-5-6 loss: 0.363852  [  224/  265]
train() client id: f_00001-5-7 loss: 0.432875  [  256/  265]
train() client id: f_00001-6-0 loss: 0.388476  [   32/  265]
train() client id: f_00001-6-1 loss: 0.381414  [   64/  265]
train() client id: f_00001-6-2 loss: 0.512279  [   96/  265]
train() client id: f_00001-6-3 loss: 0.466032  [  128/  265]
train() client id: f_00001-6-4 loss: 0.428754  [  160/  265]
train() client id: f_00001-6-5 loss: 0.323241  [  192/  265]
train() client id: f_00001-6-6 loss: 0.471143  [  224/  265]
train() client id: f_00001-6-7 loss: 0.464343  [  256/  265]
train() client id: f_00001-7-0 loss: 0.454975  [   32/  265]
train() client id: f_00001-7-1 loss: 0.494398  [   64/  265]
train() client id: f_00001-7-2 loss: 0.390581  [   96/  265]
train() client id: f_00001-7-3 loss: 0.387899  [  128/  265]
train() client id: f_00001-7-4 loss: 0.337301  [  160/  265]
train() client id: f_00001-7-5 loss: 0.407431  [  192/  265]
train() client id: f_00001-7-6 loss: 0.566565  [  224/  265]
train() client id: f_00001-7-7 loss: 0.384484  [  256/  265]
train() client id: f_00001-8-0 loss: 0.480388  [   32/  265]
train() client id: f_00001-8-1 loss: 0.437198  [   64/  265]
train() client id: f_00001-8-2 loss: 0.508404  [   96/  265]
train() client id: f_00001-8-3 loss: 0.408558  [  128/  265]
train() client id: f_00001-8-4 loss: 0.337270  [  160/  265]
train() client id: f_00001-8-5 loss: 0.419394  [  192/  265]
train() client id: f_00001-8-6 loss: 0.422899  [  224/  265]
train() client id: f_00001-8-7 loss: 0.398058  [  256/  265]
train() client id: f_00001-9-0 loss: 0.370100  [   32/  265]
train() client id: f_00001-9-1 loss: 0.345094  [   64/  265]
train() client id: f_00001-9-2 loss: 0.383540  [   96/  265]
train() client id: f_00001-9-3 loss: 0.459350  [  128/  265]
train() client id: f_00001-9-4 loss: 0.545898  [  160/  265]
train() client id: f_00001-9-5 loss: 0.550196  [  192/  265]
train() client id: f_00001-9-6 loss: 0.413146  [  224/  265]
train() client id: f_00001-9-7 loss: 0.336429  [  256/  265]
train() client id: f_00001-10-0 loss: 0.392729  [   32/  265]
train() client id: f_00001-10-1 loss: 0.400242  [   64/  265]
train() client id: f_00001-10-2 loss: 0.600487  [   96/  265]
train() client id: f_00001-10-3 loss: 0.413931  [  128/  265]
train() client id: f_00001-10-4 loss: 0.488332  [  160/  265]
train() client id: f_00001-10-5 loss: 0.415751  [  192/  265]
train() client id: f_00001-10-6 loss: 0.375928  [  224/  265]
train() client id: f_00001-10-7 loss: 0.316141  [  256/  265]
train() client id: f_00001-11-0 loss: 0.372855  [   32/  265]
train() client id: f_00001-11-1 loss: 0.448382  [   64/  265]
train() client id: f_00001-11-2 loss: 0.382403  [   96/  265]
train() client id: f_00001-11-3 loss: 0.565013  [  128/  265]
train() client id: f_00001-11-4 loss: 0.333527  [  160/  265]
train() client id: f_00001-11-5 loss: 0.382925  [  192/  265]
train() client id: f_00001-11-6 loss: 0.470209  [  224/  265]
train() client id: f_00001-11-7 loss: 0.455222  [  256/  265]
train() client id: f_00002-0-0 loss: 1.140698  [   32/  124]
train() client id: f_00002-0-1 loss: 1.328532  [   64/  124]
train() client id: f_00002-0-2 loss: 1.101405  [   96/  124]
train() client id: f_00002-1-0 loss: 1.101027  [   32/  124]
train() client id: f_00002-1-1 loss: 1.105961  [   64/  124]
train() client id: f_00002-1-2 loss: 1.070679  [   96/  124]
train() client id: f_00002-2-0 loss: 1.167311  [   32/  124]
train() client id: f_00002-2-1 loss: 0.997247  [   64/  124]
train() client id: f_00002-2-2 loss: 1.171315  [   96/  124]
train() client id: f_00002-3-0 loss: 1.039390  [   32/  124]
train() client id: f_00002-3-1 loss: 1.147605  [   64/  124]
train() client id: f_00002-3-2 loss: 1.017821  [   96/  124]
train() client id: f_00002-4-0 loss: 1.058467  [   32/  124]
train() client id: f_00002-4-1 loss: 0.976703  [   64/  124]
train() client id: f_00002-4-2 loss: 1.065877  [   96/  124]
train() client id: f_00002-5-0 loss: 1.139053  [   32/  124]
train() client id: f_00002-5-1 loss: 0.993436  [   64/  124]
train() client id: f_00002-5-2 loss: 0.877689  [   96/  124]
train() client id: f_00002-6-0 loss: 0.978872  [   32/  124]
train() client id: f_00002-6-1 loss: 1.120801  [   64/  124]
train() client id: f_00002-6-2 loss: 0.882762  [   96/  124]
train() client id: f_00002-7-0 loss: 1.076877  [   32/  124]
train() client id: f_00002-7-1 loss: 0.785296  [   64/  124]
train() client id: f_00002-7-2 loss: 1.071614  [   96/  124]
train() client id: f_00002-8-0 loss: 0.919687  [   32/  124]
train() client id: f_00002-8-1 loss: 0.774585  [   64/  124]
train() client id: f_00002-8-2 loss: 1.072491  [   96/  124]
train() client id: f_00002-9-0 loss: 0.960981  [   32/  124]
train() client id: f_00002-9-1 loss: 0.804501  [   64/  124]
train() client id: f_00002-9-2 loss: 0.908612  [   96/  124]
train() client id: f_00002-10-0 loss: 0.766111  [   32/  124]
train() client id: f_00002-10-1 loss: 1.068711  [   64/  124]
train() client id: f_00002-10-2 loss: 1.072909  [   96/  124]
train() client id: f_00002-11-0 loss: 0.969235  [   32/  124]
train() client id: f_00002-11-1 loss: 0.783652  [   64/  124]
train() client id: f_00002-11-2 loss: 1.039406  [   96/  124]
train() client id: f_00003-0-0 loss: 0.656001  [   32/   43]
train() client id: f_00003-1-0 loss: 0.836359  [   32/   43]
train() client id: f_00003-2-0 loss: 0.779780  [   32/   43]
train() client id: f_00003-3-0 loss: 0.668512  [   32/   43]
train() client id: f_00003-4-0 loss: 0.495395  [   32/   43]
train() client id: f_00003-5-0 loss: 0.717664  [   32/   43]
train() client id: f_00003-6-0 loss: 0.653702  [   32/   43]
train() client id: f_00003-7-0 loss: 0.777304  [   32/   43]
train() client id: f_00003-8-0 loss: 0.478785  [   32/   43]
train() client id: f_00003-9-0 loss: 0.801347  [   32/   43]
train() client id: f_00003-10-0 loss: 0.768681  [   32/   43]
train() client id: f_00003-11-0 loss: 0.607930  [   32/   43]
train() client id: f_00004-0-0 loss: 0.686328  [   32/  306]
train() client id: f_00004-0-1 loss: 0.869444  [   64/  306]
train() client id: f_00004-0-2 loss: 0.832223  [   96/  306]
train() client id: f_00004-0-3 loss: 0.732850  [  128/  306]
train() client id: f_00004-0-4 loss: 0.839957  [  160/  306]
train() client id: f_00004-0-5 loss: 0.732829  [  192/  306]
train() client id: f_00004-0-6 loss: 0.842045  [  224/  306]
train() client id: f_00004-0-7 loss: 0.660178  [  256/  306]
train() client id: f_00004-0-8 loss: 0.925278  [  288/  306]
train() client id: f_00004-1-0 loss: 0.848763  [   32/  306]
train() client id: f_00004-1-1 loss: 0.853178  [   64/  306]
train() client id: f_00004-1-2 loss: 0.759718  [   96/  306]
train() client id: f_00004-1-3 loss: 0.874279  [  128/  306]
train() client id: f_00004-1-4 loss: 0.825585  [  160/  306]
train() client id: f_00004-1-5 loss: 0.818423  [  192/  306]
train() client id: f_00004-1-6 loss: 0.761078  [  224/  306]
train() client id: f_00004-1-7 loss: 0.718606  [  256/  306]
train() client id: f_00004-1-8 loss: 0.714005  [  288/  306]
train() client id: f_00004-2-0 loss: 0.719534  [   32/  306]
train() client id: f_00004-2-1 loss: 0.845596  [   64/  306]
train() client id: f_00004-2-2 loss: 0.781926  [   96/  306]
train() client id: f_00004-2-3 loss: 0.832592  [  128/  306]
train() client id: f_00004-2-4 loss: 0.925479  [  160/  306]
train() client id: f_00004-2-5 loss: 0.649958  [  192/  306]
train() client id: f_00004-2-6 loss: 0.783834  [  224/  306]
train() client id: f_00004-2-7 loss: 0.961634  [  256/  306]
train() client id: f_00004-2-8 loss: 0.703056  [  288/  306]
train() client id: f_00004-3-0 loss: 0.734280  [   32/  306]
train() client id: f_00004-3-1 loss: 0.734026  [   64/  306]
train() client id: f_00004-3-2 loss: 0.795397  [   96/  306]
train() client id: f_00004-3-3 loss: 0.876840  [  128/  306]
train() client id: f_00004-3-4 loss: 0.799601  [  160/  306]
train() client id: f_00004-3-5 loss: 0.860530  [  192/  306]
train() client id: f_00004-3-6 loss: 0.714434  [  224/  306]
train() client id: f_00004-3-7 loss: 0.822484  [  256/  306]
train() client id: f_00004-3-8 loss: 0.738001  [  288/  306]
train() client id: f_00004-4-0 loss: 0.800558  [   32/  306]
train() client id: f_00004-4-1 loss: 0.914781  [   64/  306]
train() client id: f_00004-4-2 loss: 0.741389  [   96/  306]
train() client id: f_00004-4-3 loss: 0.764920  [  128/  306]
train() client id: f_00004-4-4 loss: 0.806087  [  160/  306]
train() client id: f_00004-4-5 loss: 0.752882  [  192/  306]
train() client id: f_00004-4-6 loss: 0.813159  [  224/  306]
train() client id: f_00004-4-7 loss: 0.871771  [  256/  306]
train() client id: f_00004-4-8 loss: 0.686024  [  288/  306]
train() client id: f_00004-5-0 loss: 0.839135  [   32/  306]
train() client id: f_00004-5-1 loss: 0.748169  [   64/  306]
train() client id: f_00004-5-2 loss: 0.598127  [   96/  306]
train() client id: f_00004-5-3 loss: 0.746094  [  128/  306]
train() client id: f_00004-5-4 loss: 0.812569  [  160/  306]
train() client id: f_00004-5-5 loss: 1.012055  [  192/  306]
train() client id: f_00004-5-6 loss: 0.803960  [  224/  306]
train() client id: f_00004-5-7 loss: 0.783312  [  256/  306]
train() client id: f_00004-5-8 loss: 0.723276  [  288/  306]
train() client id: f_00004-6-0 loss: 0.856005  [   32/  306]
train() client id: f_00004-6-1 loss: 0.781993  [   64/  306]
train() client id: f_00004-6-2 loss: 0.831082  [   96/  306]
train() client id: f_00004-6-3 loss: 0.766434  [  128/  306]
train() client id: f_00004-6-4 loss: 0.672459  [  160/  306]
train() client id: f_00004-6-5 loss: 0.900611  [  192/  306]
train() client id: f_00004-6-6 loss: 0.797682  [  224/  306]
train() client id: f_00004-6-7 loss: 0.801232  [  256/  306]
train() client id: f_00004-6-8 loss: 0.755684  [  288/  306]
train() client id: f_00004-7-0 loss: 0.748338  [   32/  306]
train() client id: f_00004-7-1 loss: 0.831382  [   64/  306]
train() client id: f_00004-7-2 loss: 0.749802  [   96/  306]
train() client id: f_00004-7-3 loss: 0.812791  [  128/  306]
train() client id: f_00004-7-4 loss: 0.755432  [  160/  306]
train() client id: f_00004-7-5 loss: 0.842710  [  192/  306]
train() client id: f_00004-7-6 loss: 0.864981  [  224/  306]
train() client id: f_00004-7-7 loss: 0.745901  [  256/  306]
train() client id: f_00004-7-8 loss: 0.729520  [  288/  306]
train() client id: f_00004-8-0 loss: 0.770096  [   32/  306]
train() client id: f_00004-8-1 loss: 0.661425  [   64/  306]
train() client id: f_00004-8-2 loss: 0.734748  [   96/  306]
train() client id: f_00004-8-3 loss: 0.691270  [  128/  306]
train() client id: f_00004-8-4 loss: 0.708154  [  160/  306]
train() client id: f_00004-8-5 loss: 0.881289  [  192/  306]
train() client id: f_00004-8-6 loss: 0.768450  [  224/  306]
train() client id: f_00004-8-7 loss: 0.884547  [  256/  306]
train() client id: f_00004-8-8 loss: 0.847306  [  288/  306]
train() client id: f_00004-9-0 loss: 0.787139  [   32/  306]
train() client id: f_00004-9-1 loss: 0.792510  [   64/  306]
train() client id: f_00004-9-2 loss: 0.846804  [   96/  306]
train() client id: f_00004-9-3 loss: 0.722608  [  128/  306]
train() client id: f_00004-9-4 loss: 0.686180  [  160/  306]
train() client id: f_00004-9-5 loss: 0.863747  [  192/  306]
train() client id: f_00004-9-6 loss: 0.767599  [  224/  306]
train() client id: f_00004-9-7 loss: 0.761481  [  256/  306]
train() client id: f_00004-9-8 loss: 0.812296  [  288/  306]
train() client id: f_00004-10-0 loss: 0.857707  [   32/  306]
train() client id: f_00004-10-1 loss: 0.802446  [   64/  306]
train() client id: f_00004-10-2 loss: 0.762433  [   96/  306]
train() client id: f_00004-10-3 loss: 0.734946  [  128/  306]
train() client id: f_00004-10-4 loss: 0.799197  [  160/  306]
train() client id: f_00004-10-5 loss: 0.760826  [  192/  306]
train() client id: f_00004-10-6 loss: 0.747435  [  224/  306]
train() client id: f_00004-10-7 loss: 0.822748  [  256/  306]
train() client id: f_00004-10-8 loss: 0.829675  [  288/  306]
train() client id: f_00004-11-0 loss: 0.803261  [   32/  306]
train() client id: f_00004-11-1 loss: 0.815392  [   64/  306]
train() client id: f_00004-11-2 loss: 0.785818  [   96/  306]
train() client id: f_00004-11-3 loss: 0.785814  [  128/  306]
train() client id: f_00004-11-4 loss: 0.818569  [  160/  306]
train() client id: f_00004-11-5 loss: 0.808396  [  192/  306]
train() client id: f_00004-11-6 loss: 0.828718  [  224/  306]
train() client id: f_00004-11-7 loss: 0.707651  [  256/  306]
train() client id: f_00004-11-8 loss: 0.768618  [  288/  306]
train() client id: f_00005-0-0 loss: 0.276946  [   32/  146]
train() client id: f_00005-0-1 loss: 0.520024  [   64/  146]
train() client id: f_00005-0-2 loss: 0.446302  [   96/  146]
train() client id: f_00005-0-3 loss: 0.513532  [  128/  146]
train() client id: f_00005-1-0 loss: 0.425986  [   32/  146]
train() client id: f_00005-1-1 loss: 0.437365  [   64/  146]
train() client id: f_00005-1-2 loss: 0.559248  [   96/  146]
train() client id: f_00005-1-3 loss: 0.588033  [  128/  146]
train() client id: f_00005-2-0 loss: 0.550153  [   32/  146]
train() client id: f_00005-2-1 loss: 0.676441  [   64/  146]
train() client id: f_00005-2-2 loss: 0.639149  [   96/  146]
train() client id: f_00005-2-3 loss: 0.436274  [  128/  146]
train() client id: f_00005-3-0 loss: 0.595548  [   32/  146]
train() client id: f_00005-3-1 loss: 0.345206  [   64/  146]
train() client id: f_00005-3-2 loss: 0.505449  [   96/  146]
train() client id: f_00005-3-3 loss: 0.624878  [  128/  146]
train() client id: f_00005-4-0 loss: 0.733678  [   32/  146]
train() client id: f_00005-4-1 loss: 0.480374  [   64/  146]
train() client id: f_00005-4-2 loss: 0.449408  [   96/  146]
train() client id: f_00005-4-3 loss: 0.520735  [  128/  146]
train() client id: f_00005-5-0 loss: 0.721477  [   32/  146]
train() client id: f_00005-5-1 loss: 0.430438  [   64/  146]
train() client id: f_00005-5-2 loss: 0.421408  [   96/  146]
train() client id: f_00005-5-3 loss: 0.840226  [  128/  146]
train() client id: f_00005-6-0 loss: 0.483746  [   32/  146]
train() client id: f_00005-6-1 loss: 0.677214  [   64/  146]
train() client id: f_00005-6-2 loss: 0.425679  [   96/  146]
train() client id: f_00005-6-3 loss: 0.500027  [  128/  146]
train() client id: f_00005-7-0 loss: 0.505808  [   32/  146]
train() client id: f_00005-7-1 loss: 0.375238  [   64/  146]
train() client id: f_00005-7-2 loss: 0.473722  [   96/  146]
train() client id: f_00005-7-3 loss: 0.672207  [  128/  146]
train() client id: f_00005-8-0 loss: 0.543188  [   32/  146]
train() client id: f_00005-8-1 loss: 0.578021  [   64/  146]
train() client id: f_00005-8-2 loss: 0.319354  [   96/  146]
train() client id: f_00005-8-3 loss: 0.780864  [  128/  146]
train() client id: f_00005-9-0 loss: 0.362839  [   32/  146]
train() client id: f_00005-9-1 loss: 0.580129  [   64/  146]
train() client id: f_00005-9-2 loss: 0.397514  [   96/  146]
train() client id: f_00005-9-3 loss: 0.772249  [  128/  146]
train() client id: f_00005-10-0 loss: 0.562347  [   32/  146]
train() client id: f_00005-10-1 loss: 0.599317  [   64/  146]
train() client id: f_00005-10-2 loss: 0.558761  [   96/  146]
train() client id: f_00005-10-3 loss: 0.409427  [  128/  146]
train() client id: f_00005-11-0 loss: 0.533364  [   32/  146]
train() client id: f_00005-11-1 loss: 0.619557  [   64/  146]
train() client id: f_00005-11-2 loss: 0.684193  [   96/  146]
train() client id: f_00005-11-3 loss: 0.395555  [  128/  146]
train() client id: f_00006-0-0 loss: 0.538262  [   32/   54]
train() client id: f_00006-1-0 loss: 0.519325  [   32/   54]
train() client id: f_00006-2-0 loss: 0.523512  [   32/   54]
train() client id: f_00006-3-0 loss: 0.533499  [   32/   54]
train() client id: f_00006-4-0 loss: 0.539425  [   32/   54]
train() client id: f_00006-5-0 loss: 0.563407  [   32/   54]
train() client id: f_00006-6-0 loss: 0.610566  [   32/   54]
train() client id: f_00006-7-0 loss: 0.589011  [   32/   54]
train() client id: f_00006-8-0 loss: 0.553704  [   32/   54]
train() client id: f_00006-9-0 loss: 0.501229  [   32/   54]
train() client id: f_00006-10-0 loss: 0.538309  [   32/   54]
train() client id: f_00006-11-0 loss: 0.588030  [   32/   54]
train() client id: f_00007-0-0 loss: 0.573951  [   32/  179]
train() client id: f_00007-0-1 loss: 0.720793  [   64/  179]
train() client id: f_00007-0-2 loss: 0.598455  [   96/  179]
train() client id: f_00007-0-3 loss: 0.767017  [  128/  179]
train() client id: f_00007-0-4 loss: 0.800189  [  160/  179]
train() client id: f_00007-1-0 loss: 0.752748  [   32/  179]
train() client id: f_00007-1-1 loss: 0.601662  [   64/  179]
train() client id: f_00007-1-2 loss: 0.544384  [   96/  179]
train() client id: f_00007-1-3 loss: 0.681743  [  128/  179]
train() client id: f_00007-1-4 loss: 0.754272  [  160/  179]
train() client id: f_00007-2-0 loss: 0.567269  [   32/  179]
train() client id: f_00007-2-1 loss: 0.580239  [   64/  179]
train() client id: f_00007-2-2 loss: 0.781293  [   96/  179]
train() client id: f_00007-2-3 loss: 0.600358  [  128/  179]
train() client id: f_00007-2-4 loss: 0.517611  [  160/  179]
train() client id: f_00007-3-0 loss: 0.674306  [   32/  179]
train() client id: f_00007-3-1 loss: 0.697188  [   64/  179]
train() client id: f_00007-3-2 loss: 0.636745  [   96/  179]
train() client id: f_00007-3-3 loss: 0.580027  [  128/  179]
train() client id: f_00007-3-4 loss: 0.519252  [  160/  179]
train() client id: f_00007-4-0 loss: 0.563972  [   32/  179]
train() client id: f_00007-4-1 loss: 0.599411  [   64/  179]
train() client id: f_00007-4-2 loss: 0.770686  [   96/  179]
train() client id: f_00007-4-3 loss: 0.507872  [  128/  179]
train() client id: f_00007-4-4 loss: 0.600803  [  160/  179]
train() client id: f_00007-5-0 loss: 0.587238  [   32/  179]
train() client id: f_00007-5-1 loss: 0.802087  [   64/  179]
train() client id: f_00007-5-2 loss: 0.507402  [   96/  179]
train() client id: f_00007-5-3 loss: 0.638165  [  128/  179]
train() client id: f_00007-5-4 loss: 0.681502  [  160/  179]
train() client id: f_00007-6-0 loss: 0.720465  [   32/  179]
train() client id: f_00007-6-1 loss: 0.502206  [   64/  179]
train() client id: f_00007-6-2 loss: 0.632356  [   96/  179]
train() client id: f_00007-6-3 loss: 0.545279  [  128/  179]
train() client id: f_00007-6-4 loss: 0.648977  [  160/  179]
train() client id: f_00007-7-0 loss: 0.480496  [   32/  179]
train() client id: f_00007-7-1 loss: 0.549860  [   64/  179]
train() client id: f_00007-7-2 loss: 0.623406  [   96/  179]
train() client id: f_00007-7-3 loss: 0.590458  [  128/  179]
train() client id: f_00007-7-4 loss: 0.735926  [  160/  179]
train() client id: f_00007-8-0 loss: 0.847084  [   32/  179]
train() client id: f_00007-8-1 loss: 0.632590  [   64/  179]
train() client id: f_00007-8-2 loss: 0.575482  [   96/  179]
train() client id: f_00007-8-3 loss: 0.546543  [  128/  179]
train() client id: f_00007-8-4 loss: 0.463547  [  160/  179]
train() client id: f_00007-9-0 loss: 0.758298  [   32/  179]
train() client id: f_00007-9-1 loss: 0.494156  [   64/  179]
train() client id: f_00007-9-2 loss: 0.802109  [   96/  179]
train() client id: f_00007-9-3 loss: 0.552403  [  128/  179]
train() client id: f_00007-9-4 loss: 0.581842  [  160/  179]
train() client id: f_00007-10-0 loss: 0.786824  [   32/  179]
train() client id: f_00007-10-1 loss: 0.661883  [   64/  179]
train() client id: f_00007-10-2 loss: 0.450175  [   96/  179]
train() client id: f_00007-10-3 loss: 0.628983  [  128/  179]
train() client id: f_00007-10-4 loss: 0.681667  [  160/  179]
train() client id: f_00007-11-0 loss: 0.644747  [   32/  179]
train() client id: f_00007-11-1 loss: 0.700879  [   64/  179]
train() client id: f_00007-11-2 loss: 0.696981  [   96/  179]
train() client id: f_00007-11-3 loss: 0.666919  [  128/  179]
train() client id: f_00007-11-4 loss: 0.480950  [  160/  179]
train() client id: f_00008-0-0 loss: 0.698397  [   32/  130]
train() client id: f_00008-0-1 loss: 0.624225  [   64/  130]
train() client id: f_00008-0-2 loss: 0.689427  [   96/  130]
train() client id: f_00008-0-3 loss: 0.736420  [  128/  130]
train() client id: f_00008-1-0 loss: 0.631698  [   32/  130]
train() client id: f_00008-1-1 loss: 0.645392  [   64/  130]
train() client id: f_00008-1-2 loss: 0.734279  [   96/  130]
train() client id: f_00008-1-3 loss: 0.697554  [  128/  130]
train() client id: f_00008-2-0 loss: 0.785060  [   32/  130]
train() client id: f_00008-2-1 loss: 0.580896  [   64/  130]
train() client id: f_00008-2-2 loss: 0.563400  [   96/  130]
train() client id: f_00008-2-3 loss: 0.791792  [  128/  130]
train() client id: f_00008-3-0 loss: 0.594020  [   32/  130]
train() client id: f_00008-3-1 loss: 0.551103  [   64/  130]
train() client id: f_00008-3-2 loss: 0.768166  [   96/  130]
train() client id: f_00008-3-3 loss: 0.809842  [  128/  130]
train() client id: f_00008-4-0 loss: 0.672047  [   32/  130]
train() client id: f_00008-4-1 loss: 0.774866  [   64/  130]
train() client id: f_00008-4-2 loss: 0.614650  [   96/  130]
train() client id: f_00008-4-3 loss: 0.658585  [  128/  130]
train() client id: f_00008-5-0 loss: 0.808931  [   32/  130]
train() client id: f_00008-5-1 loss: 0.626724  [   64/  130]
train() client id: f_00008-5-2 loss: 0.621193  [   96/  130]
train() client id: f_00008-5-3 loss: 0.687345  [  128/  130]
train() client id: f_00008-6-0 loss: 0.696474  [   32/  130]
train() client id: f_00008-6-1 loss: 0.618462  [   64/  130]
train() client id: f_00008-6-2 loss: 0.735641  [   96/  130]
train() client id: f_00008-6-3 loss: 0.690403  [  128/  130]
train() client id: f_00008-7-0 loss: 0.671720  [   32/  130]
train() client id: f_00008-7-1 loss: 0.756548  [   64/  130]
train() client id: f_00008-7-2 loss: 0.603226  [   96/  130]
train() client id: f_00008-7-3 loss: 0.670055  [  128/  130]
train() client id: f_00008-8-0 loss: 0.767953  [   32/  130]
train() client id: f_00008-8-1 loss: 0.624226  [   64/  130]
train() client id: f_00008-8-2 loss: 0.725129  [   96/  130]
train() client id: f_00008-8-3 loss: 0.605756  [  128/  130]
train() client id: f_00008-9-0 loss: 0.672818  [   32/  130]
train() client id: f_00008-9-1 loss: 0.604801  [   64/  130]
train() client id: f_00008-9-2 loss: 0.694692  [   96/  130]
train() client id: f_00008-9-3 loss: 0.726179  [  128/  130]
train() client id: f_00008-10-0 loss: 0.736265  [   32/  130]
train() client id: f_00008-10-1 loss: 0.642656  [   64/  130]
train() client id: f_00008-10-2 loss: 0.761219  [   96/  130]
train() client id: f_00008-10-3 loss: 0.586198  [  128/  130]
train() client id: f_00008-11-0 loss: 0.597180  [   32/  130]
train() client id: f_00008-11-1 loss: 0.742410  [   64/  130]
train() client id: f_00008-11-2 loss: 0.701315  [   96/  130]
train() client id: f_00008-11-3 loss: 0.689119  [  128/  130]
train() client id: f_00009-0-0 loss: 1.188313  [   32/  118]
train() client id: f_00009-0-1 loss: 1.127450  [   64/  118]
train() client id: f_00009-0-2 loss: 1.153925  [   96/  118]
train() client id: f_00009-1-0 loss: 1.094460  [   32/  118]
train() client id: f_00009-1-1 loss: 1.333114  [   64/  118]
train() client id: f_00009-1-2 loss: 0.972990  [   96/  118]
train() client id: f_00009-2-0 loss: 0.973101  [   32/  118]
train() client id: f_00009-2-1 loss: 1.184129  [   64/  118]
train() client id: f_00009-2-2 loss: 0.961673  [   96/  118]
train() client id: f_00009-3-0 loss: 0.958838  [   32/  118]
train() client id: f_00009-3-1 loss: 1.112347  [   64/  118]
train() client id: f_00009-3-2 loss: 0.983580  [   96/  118]
train() client id: f_00009-4-0 loss: 0.975013  [   32/  118]
train() client id: f_00009-4-1 loss: 1.088944  [   64/  118]
train() client id: f_00009-4-2 loss: 0.933662  [   96/  118]
train() client id: f_00009-5-0 loss: 1.064635  [   32/  118]
train() client id: f_00009-5-1 loss: 1.102896  [   64/  118]
train() client id: f_00009-5-2 loss: 0.780380  [   96/  118]
train() client id: f_00009-6-0 loss: 0.836531  [   32/  118]
train() client id: f_00009-6-1 loss: 1.134454  [   64/  118]
train() client id: f_00009-6-2 loss: 0.868287  [   96/  118]
train() client id: f_00009-7-0 loss: 0.915231  [   32/  118]
train() client id: f_00009-7-1 loss: 1.098994  [   64/  118]
train() client id: f_00009-7-2 loss: 0.896384  [   96/  118]
train() client id: f_00009-8-0 loss: 0.762237  [   32/  118]
train() client id: f_00009-8-1 loss: 0.986388  [   64/  118]
train() client id: f_00009-8-2 loss: 1.015859  [   96/  118]
train() client id: f_00009-9-0 loss: 0.881455  [   32/  118]
train() client id: f_00009-9-1 loss: 0.772105  [   64/  118]
train() client id: f_00009-9-2 loss: 0.842423  [   96/  118]
train() client id: f_00009-10-0 loss: 0.938484  [   32/  118]
train() client id: f_00009-10-1 loss: 1.048738  [   64/  118]
train() client id: f_00009-10-2 loss: 0.676667  [   96/  118]
train() client id: f_00009-11-0 loss: 0.684078  [   32/  118]
train() client id: f_00009-11-1 loss: 1.029068  [   64/  118]
train() client id: f_00009-11-2 loss: 0.898089  [   96/  118]
At round 71 accuracy: 0.6445623342175066
At round 71 training accuracy: 0.5955734406438632
At round 71 training loss: 0.8283542052646752
update_location
xs = 8.927491 476.223621 5.882650 0.934260 -392.581990 -240.230757 -200.849135 -5.143845 -415.120581 20.134486 
ys = -467.390647 7.291448 365.684448 -187.290817 -9.642386 0.794442 -1.381692 361.628436 25.881276 -902.232496 
xs mean: -74.18237997052123
ys mean: -80.66579882624052
dists_uav = 478.052002 486.664260 379.156592 212.317505 405.232765 260.214235 224.370863 375.235373 427.779075 907.980657 
uav_gains = -123.750168 -123.965147 -120.579205 -108.473113 -121.592478 -112.187167 -109.295854 -120.409342 -122.342876 -130.926818 
uav_gains_db_mean: -119.35221667385446
dists_bs = 668.202863 672.471830 262.829016 402.749923 285.367715 186.017817 178.265754 252.351514 282.655775 1094.763590 
bs_gains = -118.664492 -118.741934 -107.317912 -112.508052 -108.318395 -103.114586 -102.596961 -106.823225 -108.202279 -124.668029 
bs_gains_db_mean: -111.09558650920762
Round 72
-------------------------------
ene_coms = [0.04837388 0.05012125 0.00902686 0.00967807 0.00958572 0.01252944
 0.0101989  0.008773   0.0095175  0.05643989]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.52283208 3.01928939 1.36151692 0.49541534 3.31476051 1.6098343
 0.61519992 1.95028005 1.42758823 1.46573601]
obj_prev = 16.782452751377175
eta_min = 9.3286540129937e-65	eta_max = 0.9243661669837216
af = 3.436451538788952	bf = 0.7891840518396214	zeta = 3.7800966926678474	eta = 0.9090909090909091
af = 3.436451538788952	bf = 0.7891840518396214	zeta = 12.283841524794896	eta = 0.27975381576296676
af = 3.436451538788952	bf = 0.7891840518396214	zeta = 7.1735078626146045	eta = 0.4790475740186102
af = 3.436451538788952	bf = 0.7891840518396214	zeta = 6.369600830945931	eta = 0.5395081465848488
af = 3.436451538788952	bf = 0.7891840518396214	zeta = 6.318916002448529	eta = 0.5438356100092729
af = 3.436451538788952	bf = 0.7891840518396214	zeta = 6.318676228537661	eta = 0.5438562468620511
af = 3.436451538788952	bf = 0.7891840518396214	zeta = 6.318676223113805	eta = 0.543856247328889
eta = 0.543856247328889
ene_coms = [0.04837388 0.05012125 0.00902686 0.00967807 0.00958572 0.01252944
 0.0101989  0.008773   0.0095175  0.05643989]
ene_comp = [0.05025902 0.1057035  0.04946126 0.01715189 0.12205763 0.05823665
 0.02153958 0.07139972 0.05185455 0.04706797]
ene_total = [0.76098588 1.20224013 0.45125541 0.20700211 1.01567249 0.54598405
 0.24487299 0.6185594  0.4735059  0.79859785]
ti_comp = [2.92928557 2.91181188 3.3227558  3.31624366 3.31716719 3.28772996
 3.31103538 3.32529438 3.31784936 2.84862544]
ti_coms = [0.48373878 0.50121247 0.09026856 0.09678069 0.09585716 0.12529439
 0.10198897 0.08772997 0.09517499 0.56439891]
t_total = [26.33262634 26.33262634 26.33262634 26.33262634 26.33262634 26.33262634
 26.33262634 26.33262634 26.33262634 26.33262634]
ene_coms = [0.04837388 0.05012125 0.00902686 0.00967807 0.00958572 0.01252944
 0.0101989  0.008773   0.0095175  0.05643989]
ene_comp = [9.24695435e-07 8.70605680e-06 6.84981291e-07 2.86762927e-08
 1.03285667e-05 1.14202810e-06 5.69723508e-08 2.05735577e-06
 7.91639092e-07 8.03129959e-07]
ene_total = [0.37322781 0.38676938 0.0696505  0.07466977 0.0740367  0.09667763
 0.07868835 0.06770249 0.0734368  0.43545886]
optimize_network iter = 0 obj = 1.7303182916288269
eta = 0.543856247328889
freqs = [ 8578716.82257516 18150812.02989145  7442807.05073848  2586041.69333132
 18397870.80202533  8856664.96129383  3252695.71791774 10735849.93996383
  7814481.86837391  8261523.79102458]
eta_min = 0.5438562473288898	eta_max = 0.7139974291917122
af = 0.0001474821818127634	bf = 0.7891840518396214	zeta = 0.00016223039999403976	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.04837388 0.05012125 0.00902686 0.00967807 0.00958572 0.01252944
 0.0101989  0.008773   0.0095175  0.05643989]
ene_comp = [1.44700678e-07 1.36236460e-06 1.07189085e-07 4.48740076e-09
 1.61626256e-06 1.78709914e-07 8.91530064e-09 3.21944680e-07
 1.23879398e-07 1.25677543e-07]
ene_total = [1.87267068 1.94036251 0.34945467 0.37466069 0.37114788 0.48505058
 0.39482332 0.33963555 0.36844927 2.18492402]
ti_comp = [1.65623076 1.63875708 2.04970099 2.04318885 2.04411238 2.01467515
 2.03798057 2.05223957 2.04479455 1.57557064]
ti_coms = [0.48373878 0.50121247 0.09026856 0.09678069 0.09585716 0.12529439
 0.10198897 0.08772997 0.09517499 0.56439891]
t_total = [26.33262634 26.33262634 26.33262634 26.33262634 26.33262634 26.33262634
 26.33262634 26.33262634 26.33262634 26.33262634]
ene_coms = [0.04837388 0.05012125 0.00902686 0.00967807 0.00958572 0.01252944
 0.0101989  0.008773   0.0095175  0.05643989]
ene_comp = [4.89427121e-07 4.65078810e-06 3.04580521e-07 1.27822244e-08
 4.60227782e-06 5.14596227e-07 2.54448146e-08 9.13947122e-07
 3.52653979e-07 4.44210026e-07]
ene_total = [0.5952534  0.61680622 0.11108047 0.11909016 0.11801022 0.15418284
 0.12549918 0.10796421 0.1171185  0.69450626]
optimize_network iter = 1 obj = 2.759511478189543
eta = 0.7139974291917122
freqs = [ 8392004.86093385 17838047.76617316  6673400.71014724  2321538.94748285
 16513254.94588677  7993994.66972779  2922871.67704395  9621460.53124465
  7013094.66891734  8261523.79102458]
eta_min = 0.7139974291917135	eta_max = 0.7139974291917114
af = 0.00012860847377773118	bf = 0.7891840518396214	zeta = 0.00014146932115550431	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.04837388 0.05012125 0.00902686 0.00967807 0.00958572 0.01252944
 0.0101989  0.008773   0.0095175  0.05643989]
ene_comp = [1.38470530e-07 1.31581816e-06 8.61730468e-08 3.61639417e-09
 1.30209345e-06 1.45591467e-07 7.19894101e-09 2.58577298e-07
 9.97741674e-08 1.25677543e-07]
ene_total = [1.87267044 1.94036071 0.34945386 0.37466066 0.37113572 0.4850493
 0.39482326 0.3396331  0.36844834 2.18492402]
ti_comp = [1.65623076 1.63875708 2.04970099 2.04318885 2.04411238 2.01467515
 2.03798057 2.05223957 2.04479455 1.57557064]
ti_coms = [0.48373878 0.50121247 0.09026856 0.09678069 0.09585716 0.12529439
 0.10198897 0.08772997 0.09517499 0.56439891]
t_total = [26.33262634 26.33262634 26.33262634 26.33262634 26.33262634 26.33262634
 26.33262634 26.33262634 26.33262634 26.33262634]
ene_coms = [0.04837388 0.05012125 0.00902686 0.00967807 0.00958572 0.01252944
 0.0101989  0.008773   0.0095175  0.05643989]
ene_comp = [4.89427121e-07 4.65078810e-06 3.04580521e-07 1.27822244e-08
 4.60227782e-06 5.14596227e-07 2.54448146e-08 9.13947122e-07
 3.52653979e-07 4.44210026e-07]
ene_total = [0.5952534  0.61680622 0.11108047 0.11909016 0.11801022 0.15418284
 0.12549918 0.10796421 0.1171185  0.69450626]
optimize_network iter = 2 obj = 2.7595114781895353
eta = 0.7139974291917114
freqs = [ 8392004.86093385 17838047.76617315  6673400.71014724  2321538.94748285
 16513254.94588678  7993994.66972779  2922871.67704395  9621460.53124465
  7013094.66891734  8261523.79102458]
Done!
ene_coms = [0.04837388 0.05012125 0.00902686 0.00967807 0.00958572 0.01252944
 0.0101989  0.008773   0.0095175  0.05643989]
ene_comp = [4.88050417e-07 4.63770594e-06 3.03723769e-07 1.27462694e-08
 4.58933212e-06 5.13148725e-07 2.53732412e-08 9.11376290e-07
 3.51662002e-07 4.42960512e-07]
ene_total = [0.04837437 0.05012588 0.00902716 0.00967808 0.00959031 0.01252995
 0.01019892 0.00877391 0.00951785 0.05644033]
At round 72 energy consumption: 0.22425676546681214
At round 72 eta: 0.7139974291917114
At round 72 a_n: 3.5193018744635296
At round 72 local rounds: 11.03102906472303
At round 72 global rounds: 12.305140700370016
gradient difference: 0.417706161737442
train() client id: f_00000-0-0 loss: 1.287710  [   32/  126]
train() client id: f_00000-0-1 loss: 1.170221  [   64/  126]
train() client id: f_00000-0-2 loss: 1.080609  [   96/  126]
train() client id: f_00000-1-0 loss: 1.174677  [   32/  126]
train() client id: f_00000-1-1 loss: 0.963573  [   64/  126]
train() client id: f_00000-1-2 loss: 0.970311  [   96/  126]
train() client id: f_00000-2-0 loss: 1.057700  [   32/  126]
train() client id: f_00000-2-1 loss: 1.033199  [   64/  126]
train() client id: f_00000-2-2 loss: 0.826598  [   96/  126]
train() client id: f_00000-3-0 loss: 0.775873  [   32/  126]
train() client id: f_00000-3-1 loss: 0.934155  [   64/  126]
train() client id: f_00000-3-2 loss: 1.007741  [   96/  126]
train() client id: f_00000-4-0 loss: 0.872421  [   32/  126]
train() client id: f_00000-4-1 loss: 0.902846  [   64/  126]
train() client id: f_00000-4-2 loss: 0.843530  [   96/  126]
train() client id: f_00000-5-0 loss: 0.710513  [   32/  126]
train() client id: f_00000-5-1 loss: 1.003781  [   64/  126]
train() client id: f_00000-5-2 loss: 0.924778  [   96/  126]
train() client id: f_00000-6-0 loss: 0.906998  [   32/  126]
train() client id: f_00000-6-1 loss: 0.867664  [   64/  126]
train() client id: f_00000-6-2 loss: 0.727666  [   96/  126]
train() client id: f_00000-7-0 loss: 0.816034  [   32/  126]
train() client id: f_00000-7-1 loss: 0.787973  [   64/  126]
train() client id: f_00000-7-2 loss: 0.829198  [   96/  126]
train() client id: f_00000-8-0 loss: 0.866346  [   32/  126]
train() client id: f_00000-8-1 loss: 0.824668  [   64/  126]
train() client id: f_00000-8-2 loss: 0.816358  [   96/  126]
train() client id: f_00000-9-0 loss: 0.893669  [   32/  126]
train() client id: f_00000-9-1 loss: 0.770910  [   64/  126]
train() client id: f_00000-9-2 loss: 0.808154  [   96/  126]
train() client id: f_00000-10-0 loss: 0.748789  [   32/  126]
train() client id: f_00000-10-1 loss: 0.914472  [   64/  126]
train() client id: f_00000-10-2 loss: 0.812478  [   96/  126]
train() client id: f_00001-0-0 loss: 0.291199  [   32/  265]
train() client id: f_00001-0-1 loss: 0.458854  [   64/  265]
train() client id: f_00001-0-2 loss: 0.379702  [   96/  265]
train() client id: f_00001-0-3 loss: 0.332388  [  128/  265]
train() client id: f_00001-0-4 loss: 0.426755  [  160/  265]
train() client id: f_00001-0-5 loss: 0.290720  [  192/  265]
train() client id: f_00001-0-6 loss: 0.394850  [  224/  265]
train() client id: f_00001-0-7 loss: 0.481619  [  256/  265]
train() client id: f_00001-1-0 loss: 0.458976  [   32/  265]
train() client id: f_00001-1-1 loss: 0.349772  [   64/  265]
train() client id: f_00001-1-2 loss: 0.312599  [   96/  265]
train() client id: f_00001-1-3 loss: 0.453804  [  128/  265]
train() client id: f_00001-1-4 loss: 0.386373  [  160/  265]
train() client id: f_00001-1-5 loss: 0.337867  [  192/  265]
train() client id: f_00001-1-6 loss: 0.344355  [  224/  265]
train() client id: f_00001-1-7 loss: 0.289469  [  256/  265]
train() client id: f_00001-2-0 loss: 0.429468  [   32/  265]
train() client id: f_00001-2-1 loss: 0.263379  [   64/  265]
train() client id: f_00001-2-2 loss: 0.339482  [   96/  265]
train() client id: f_00001-2-3 loss: 0.298356  [  128/  265]
train() client id: f_00001-2-4 loss: 0.366219  [  160/  265]
train() client id: f_00001-2-5 loss: 0.500657  [  192/  265]
train() client id: f_00001-2-6 loss: 0.359495  [  224/  265]
train() client id: f_00001-2-7 loss: 0.303941  [  256/  265]
train() client id: f_00001-3-0 loss: 0.302910  [   32/  265]
train() client id: f_00001-3-1 loss: 0.389900  [   64/  265]
train() client id: f_00001-3-2 loss: 0.313117  [   96/  265]
train() client id: f_00001-3-3 loss: 0.508831  [  128/  265]
train() client id: f_00001-3-4 loss: 0.284117  [  160/  265]
train() client id: f_00001-3-5 loss: 0.290364  [  192/  265]
train() client id: f_00001-3-6 loss: 0.437178  [  224/  265]
train() client id: f_00001-3-7 loss: 0.261929  [  256/  265]
train() client id: f_00001-4-0 loss: 0.471219  [   32/  265]
train() client id: f_00001-4-1 loss: 0.276069  [   64/  265]
train() client id: f_00001-4-2 loss: 0.359126  [   96/  265]
train() client id: f_00001-4-3 loss: 0.362041  [  128/  265]
train() client id: f_00001-4-4 loss: 0.439148  [  160/  265]
train() client id: f_00001-4-5 loss: 0.376082  [  192/  265]
train() client id: f_00001-4-6 loss: 0.257111  [  224/  265]
train() client id: f_00001-4-7 loss: 0.323043  [  256/  265]
train() client id: f_00001-5-0 loss: 0.370667  [   32/  265]
train() client id: f_00001-5-1 loss: 0.382876  [   64/  265]
train() client id: f_00001-5-2 loss: 0.388113  [   96/  265]
train() client id: f_00001-5-3 loss: 0.437430  [  128/  265]
train() client id: f_00001-5-4 loss: 0.330824  [  160/  265]
train() client id: f_00001-5-5 loss: 0.352288  [  192/  265]
train() client id: f_00001-5-6 loss: 0.283044  [  224/  265]
train() client id: f_00001-5-7 loss: 0.296962  [  256/  265]
train() client id: f_00001-6-0 loss: 0.334703  [   32/  265]
train() client id: f_00001-6-1 loss: 0.327716  [   64/  265]
train() client id: f_00001-6-2 loss: 0.415687  [   96/  265]
train() client id: f_00001-6-3 loss: 0.318566  [  128/  265]
train() client id: f_00001-6-4 loss: 0.375007  [  160/  265]
train() client id: f_00001-6-5 loss: 0.329120  [  192/  265]
train() client id: f_00001-6-6 loss: 0.316377  [  224/  265]
train() client id: f_00001-6-7 loss: 0.349157  [  256/  265]
train() client id: f_00001-7-0 loss: 0.254110  [   32/  265]
train() client id: f_00001-7-1 loss: 0.296328  [   64/  265]
train() client id: f_00001-7-2 loss: 0.498977  [   96/  265]
train() client id: f_00001-7-3 loss: 0.273734  [  128/  265]
train() client id: f_00001-7-4 loss: 0.468893  [  160/  265]
train() client id: f_00001-7-5 loss: 0.253268  [  192/  265]
train() client id: f_00001-7-6 loss: 0.522943  [  224/  265]
train() client id: f_00001-7-7 loss: 0.238499  [  256/  265]
train() client id: f_00001-8-0 loss: 0.257338  [   32/  265]
train() client id: f_00001-8-1 loss: 0.395747  [   64/  265]
train() client id: f_00001-8-2 loss: 0.288206  [   96/  265]
train() client id: f_00001-8-3 loss: 0.350270  [  128/  265]
train() client id: f_00001-8-4 loss: 0.364770  [  160/  265]
train() client id: f_00001-8-5 loss: 0.366278  [  192/  265]
train() client id: f_00001-8-6 loss: 0.402857  [  224/  265]
train() client id: f_00001-8-7 loss: 0.366942  [  256/  265]
train() client id: f_00001-9-0 loss: 0.483121  [   32/  265]
train() client id: f_00001-9-1 loss: 0.303648  [   64/  265]
train() client id: f_00001-9-2 loss: 0.387102  [   96/  265]
train() client id: f_00001-9-3 loss: 0.273342  [  128/  265]
train() client id: f_00001-9-4 loss: 0.412740  [  160/  265]
train() client id: f_00001-9-5 loss: 0.255238  [  192/  265]
train() client id: f_00001-9-6 loss: 0.418753  [  224/  265]
train() client id: f_00001-9-7 loss: 0.243556  [  256/  265]
train() client id: f_00001-10-0 loss: 0.243809  [   32/  265]
train() client id: f_00001-10-1 loss: 0.316599  [   64/  265]
train() client id: f_00001-10-2 loss: 0.519341  [   96/  265]
train() client id: f_00001-10-3 loss: 0.341811  [  128/  265]
train() client id: f_00001-10-4 loss: 0.264747  [  160/  265]
train() client id: f_00001-10-5 loss: 0.317578  [  192/  265]
train() client id: f_00001-10-6 loss: 0.418282  [  224/  265]
train() client id: f_00001-10-7 loss: 0.365143  [  256/  265]
train() client id: f_00002-0-0 loss: 1.225173  [   32/  124]
train() client id: f_00002-0-1 loss: 0.956434  [   64/  124]
train() client id: f_00002-0-2 loss: 0.968898  [   96/  124]
train() client id: f_00002-1-0 loss: 0.998186  [   32/  124]
train() client id: f_00002-1-1 loss: 0.949071  [   64/  124]
train() client id: f_00002-1-2 loss: 1.077108  [   96/  124]
train() client id: f_00002-2-0 loss: 1.087447  [   32/  124]
train() client id: f_00002-2-1 loss: 0.820307  [   64/  124]
train() client id: f_00002-2-2 loss: 1.052950  [   96/  124]
train() client id: f_00002-3-0 loss: 0.850603  [   32/  124]
train() client id: f_00002-3-1 loss: 1.006247  [   64/  124]
train() client id: f_00002-3-2 loss: 0.922984  [   96/  124]
train() client id: f_00002-4-0 loss: 0.759028  [   32/  124]
train() client id: f_00002-4-1 loss: 0.954054  [   64/  124]
train() client id: f_00002-4-2 loss: 0.941213  [   96/  124]
train() client id: f_00002-5-0 loss: 0.979006  [   32/  124]
train() client id: f_00002-5-1 loss: 0.863177  [   64/  124]
train() client id: f_00002-5-2 loss: 0.722737  [   96/  124]
train() client id: f_00002-6-0 loss: 0.912354  [   32/  124]
train() client id: f_00002-6-1 loss: 0.805168  [   64/  124]
train() client id: f_00002-6-2 loss: 0.695350  [   96/  124]
train() client id: f_00002-7-0 loss: 0.929673  [   32/  124]
train() client id: f_00002-7-1 loss: 0.970425  [   64/  124]
train() client id: f_00002-7-2 loss: 0.738673  [   96/  124]
train() client id: f_00002-8-0 loss: 0.878025  [   32/  124]
train() client id: f_00002-8-1 loss: 0.825788  [   64/  124]
train() client id: f_00002-8-2 loss: 0.686035  [   96/  124]
train() client id: f_00002-9-0 loss: 0.880009  [   32/  124]
train() client id: f_00002-9-1 loss: 0.639408  [   64/  124]
train() client id: f_00002-9-2 loss: 0.929886  [   96/  124]
train() client id: f_00002-10-0 loss: 0.889912  [   32/  124]
train() client id: f_00002-10-1 loss: 0.706284  [   64/  124]
train() client id: f_00002-10-2 loss: 0.676477  [   96/  124]
train() client id: f_00003-0-0 loss: 0.736577  [   32/   43]
train() client id: f_00003-1-0 loss: 0.620523  [   32/   43]
train() client id: f_00003-2-0 loss: 0.562608  [   32/   43]
train() client id: f_00003-3-0 loss: 0.531444  [   32/   43]
train() client id: f_00003-4-0 loss: 0.661067  [   32/   43]
train() client id: f_00003-5-0 loss: 0.896187  [   32/   43]
train() client id: f_00003-6-0 loss: 0.772498  [   32/   43]
train() client id: f_00003-7-0 loss: 0.595538  [   32/   43]
train() client id: f_00003-8-0 loss: 0.738685  [   32/   43]
train() client id: f_00003-9-0 loss: 0.818167  [   32/   43]
train() client id: f_00003-10-0 loss: 0.640835  [   32/   43]
train() client id: f_00004-0-0 loss: 0.827933  [   32/  306]
train() client id: f_00004-0-1 loss: 0.809625  [   64/  306]
train() client id: f_00004-0-2 loss: 0.904458  [   96/  306]
train() client id: f_00004-0-3 loss: 0.846022  [  128/  306]
train() client id: f_00004-0-4 loss: 0.853307  [  160/  306]
train() client id: f_00004-0-5 loss: 0.915237  [  192/  306]
train() client id: f_00004-0-6 loss: 0.799184  [  224/  306]
train() client id: f_00004-0-7 loss: 0.798987  [  256/  306]
train() client id: f_00004-0-8 loss: 0.769085  [  288/  306]
train() client id: f_00004-1-0 loss: 0.729775  [   32/  306]
train() client id: f_00004-1-1 loss: 0.848574  [   64/  306]
train() client id: f_00004-1-2 loss: 0.845654  [   96/  306]
train() client id: f_00004-1-3 loss: 0.958925  [  128/  306]
train() client id: f_00004-1-4 loss: 0.887463  [  160/  306]
train() client id: f_00004-1-5 loss: 0.684849  [  192/  306]
train() client id: f_00004-1-6 loss: 0.871709  [  224/  306]
train() client id: f_00004-1-7 loss: 0.887761  [  256/  306]
train() client id: f_00004-1-8 loss: 0.763349  [  288/  306]
train() client id: f_00004-2-0 loss: 0.850640  [   32/  306]
train() client id: f_00004-2-1 loss: 0.829525  [   64/  306]
train() client id: f_00004-2-2 loss: 0.954897  [   96/  306]
train() client id: f_00004-2-3 loss: 0.831757  [  128/  306]
train() client id: f_00004-2-4 loss: 0.694833  [  160/  306]
train() client id: f_00004-2-5 loss: 0.924196  [  192/  306]
train() client id: f_00004-2-6 loss: 0.821563  [  224/  306]
train() client id: f_00004-2-7 loss: 0.881452  [  256/  306]
train() client id: f_00004-2-8 loss: 0.837988  [  288/  306]
train() client id: f_00004-3-0 loss: 0.824311  [   32/  306]
train() client id: f_00004-3-1 loss: 0.866877  [   64/  306]
train() client id: f_00004-3-2 loss: 0.888149  [   96/  306]
train() client id: f_00004-3-3 loss: 0.738195  [  128/  306]
train() client id: f_00004-3-4 loss: 0.773705  [  160/  306]
train() client id: f_00004-3-5 loss: 0.768522  [  192/  306]
train() client id: f_00004-3-6 loss: 0.733273  [  224/  306]
train() client id: f_00004-3-7 loss: 1.008384  [  256/  306]
train() client id: f_00004-3-8 loss: 0.872746  [  288/  306]
train() client id: f_00004-4-0 loss: 0.753649  [   32/  306]
train() client id: f_00004-4-1 loss: 0.933479  [   64/  306]
train() client id: f_00004-4-2 loss: 0.822408  [   96/  306]
train() client id: f_00004-4-3 loss: 0.844023  [  128/  306]
train() client id: f_00004-4-4 loss: 0.888370  [  160/  306]
train() client id: f_00004-4-5 loss: 0.721579  [  192/  306]
train() client id: f_00004-4-6 loss: 0.797322  [  224/  306]
train() client id: f_00004-4-7 loss: 0.938174  [  256/  306]
train() client id: f_00004-4-8 loss: 0.818681  [  288/  306]
train() client id: f_00004-5-0 loss: 0.810314  [   32/  306]
train() client id: f_00004-5-1 loss: 0.742822  [   64/  306]
train() client id: f_00004-5-2 loss: 0.963417  [   96/  306]
train() client id: f_00004-5-3 loss: 0.674883  [  128/  306]
train() client id: f_00004-5-4 loss: 0.826590  [  160/  306]
train() client id: f_00004-5-5 loss: 0.818149  [  192/  306]
train() client id: f_00004-5-6 loss: 0.973637  [  224/  306]
train() client id: f_00004-5-7 loss: 0.846407  [  256/  306]
train() client id: f_00004-5-8 loss: 0.874209  [  288/  306]
train() client id: f_00004-6-0 loss: 0.955042  [   32/  306]
train() client id: f_00004-6-1 loss: 0.810094  [   64/  306]
train() client id: f_00004-6-2 loss: 0.854554  [   96/  306]
train() client id: f_00004-6-3 loss: 0.772051  [  128/  306]
train() client id: f_00004-6-4 loss: 0.761426  [  160/  306]
train() client id: f_00004-6-5 loss: 1.094974  [  192/  306]
train() client id: f_00004-6-6 loss: 0.698202  [  224/  306]
train() client id: f_00004-6-7 loss: 0.800212  [  256/  306]
train() client id: f_00004-6-8 loss: 0.751244  [  288/  306]
train() client id: f_00004-7-0 loss: 0.780928  [   32/  306]
train() client id: f_00004-7-1 loss: 0.784551  [   64/  306]
train() client id: f_00004-7-2 loss: 0.902889  [   96/  306]
train() client id: f_00004-7-3 loss: 0.820494  [  128/  306]
train() client id: f_00004-7-4 loss: 0.904868  [  160/  306]
train() client id: f_00004-7-5 loss: 0.892451  [  192/  306]
train() client id: f_00004-7-6 loss: 0.751153  [  224/  306]
train() client id: f_00004-7-7 loss: 0.808757  [  256/  306]
train() client id: f_00004-7-8 loss: 0.824098  [  288/  306]
train() client id: f_00004-8-0 loss: 0.882458  [   32/  306]
train() client id: f_00004-8-1 loss: 0.861237  [   64/  306]
train() client id: f_00004-8-2 loss: 0.802778  [   96/  306]
train() client id: f_00004-8-3 loss: 0.730724  [  128/  306]
train() client id: f_00004-8-4 loss: 0.771624  [  160/  306]
train() client id: f_00004-8-5 loss: 0.854911  [  192/  306]
train() client id: f_00004-8-6 loss: 0.849971  [  224/  306]
train() client id: f_00004-8-7 loss: 0.897536  [  256/  306]
train() client id: f_00004-8-8 loss: 0.877851  [  288/  306]
train() client id: f_00004-9-0 loss: 0.680761  [   32/  306]
train() client id: f_00004-9-1 loss: 0.867990  [   64/  306]
train() client id: f_00004-9-2 loss: 0.784999  [   96/  306]
train() client id: f_00004-9-3 loss: 0.963043  [  128/  306]
train() client id: f_00004-9-4 loss: 0.696851  [  160/  306]
train() client id: f_00004-9-5 loss: 0.946868  [  192/  306]
train() client id: f_00004-9-6 loss: 0.793500  [  224/  306]
train() client id: f_00004-9-7 loss: 0.910343  [  256/  306]
train() client id: f_00004-9-8 loss: 0.923246  [  288/  306]
train() client id: f_00004-10-0 loss: 0.924013  [   32/  306]
train() client id: f_00004-10-1 loss: 0.927659  [   64/  306]
train() client id: f_00004-10-2 loss: 0.904370  [   96/  306]
train() client id: f_00004-10-3 loss: 0.762368  [  128/  306]
train() client id: f_00004-10-4 loss: 0.820681  [  160/  306]
train() client id: f_00004-10-5 loss: 0.794608  [  192/  306]
train() client id: f_00004-10-6 loss: 0.732079  [  224/  306]
train() client id: f_00004-10-7 loss: 0.789382  [  256/  306]
train() client id: f_00004-10-8 loss: 0.756851  [  288/  306]
train() client id: f_00005-0-0 loss: 0.654905  [   32/  146]
train() client id: f_00005-0-1 loss: 0.914310  [   64/  146]
train() client id: f_00005-0-2 loss: 0.939556  [   96/  146]
train() client id: f_00005-0-3 loss: 0.663989  [  128/  146]
train() client id: f_00005-1-0 loss: 0.751598  [   32/  146]
train() client id: f_00005-1-1 loss: 0.723160  [   64/  146]
train() client id: f_00005-1-2 loss: 0.784135  [   96/  146]
train() client id: f_00005-1-3 loss: 0.746924  [  128/  146]
train() client id: f_00005-2-0 loss: 0.585234  [   32/  146]
train() client id: f_00005-2-1 loss: 0.732189  [   64/  146]
train() client id: f_00005-2-2 loss: 0.527796  [   96/  146]
train() client id: f_00005-2-3 loss: 1.102374  [  128/  146]
train() client id: f_00005-3-0 loss: 0.865168  [   32/  146]
train() client id: f_00005-3-1 loss: 0.911998  [   64/  146]
train() client id: f_00005-3-2 loss: 0.479191  [   96/  146]
train() client id: f_00005-3-3 loss: 0.628940  [  128/  146]
train() client id: f_00005-4-0 loss: 0.629842  [   32/  146]
train() client id: f_00005-4-1 loss: 0.606824  [   64/  146]
train() client id: f_00005-4-2 loss: 0.687098  [   96/  146]
train() client id: f_00005-4-3 loss: 0.894949  [  128/  146]
train() client id: f_00005-5-0 loss: 0.752109  [   32/  146]
train() client id: f_00005-5-1 loss: 0.848466  [   64/  146]
train() client id: f_00005-5-2 loss: 0.563620  [   96/  146]
train() client id: f_00005-5-3 loss: 0.639083  [  128/  146]
train() client id: f_00005-6-0 loss: 0.532397  [   32/  146]
train() client id: f_00005-6-1 loss: 0.896605  [   64/  146]
train() client id: f_00005-6-2 loss: 0.921380  [   96/  146]
train() client id: f_00005-6-3 loss: 0.641175  [  128/  146]
train() client id: f_00005-7-0 loss: 0.811168  [   32/  146]
train() client id: f_00005-7-1 loss: 0.552408  [   64/  146]
train() client id: f_00005-7-2 loss: 0.575329  [   96/  146]
train() client id: f_00005-7-3 loss: 1.016334  [  128/  146]
train() client id: f_00005-8-0 loss: 0.603863  [   32/  146]
train() client id: f_00005-8-1 loss: 0.904246  [   64/  146]
train() client id: f_00005-8-2 loss: 0.633485  [   96/  146]
train() client id: f_00005-8-3 loss: 0.785555  [  128/  146]
train() client id: f_00005-9-0 loss: 0.609337  [   32/  146]
train() client id: f_00005-9-1 loss: 0.867293  [   64/  146]
train() client id: f_00005-9-2 loss: 0.723870  [   96/  146]
train() client id: f_00005-9-3 loss: 0.695527  [  128/  146]
train() client id: f_00005-10-0 loss: 0.937356  [   32/  146]
train() client id: f_00005-10-1 loss: 0.815091  [   64/  146]
train() client id: f_00005-10-2 loss: 0.496291  [   96/  146]
train() client id: f_00005-10-3 loss: 0.692894  [  128/  146]
train() client id: f_00006-0-0 loss: 0.429547  [   32/   54]
train() client id: f_00006-1-0 loss: 0.490815  [   32/   54]
train() client id: f_00006-2-0 loss: 0.400511  [   32/   54]
train() client id: f_00006-3-0 loss: 0.487578  [   32/   54]
train() client id: f_00006-4-0 loss: 0.494729  [   32/   54]
train() client id: f_00006-5-0 loss: 0.442269  [   32/   54]
train() client id: f_00006-6-0 loss: 0.440330  [   32/   54]
train() client id: f_00006-7-0 loss: 0.389051  [   32/   54]
train() client id: f_00006-8-0 loss: 0.441206  [   32/   54]
train() client id: f_00006-9-0 loss: 0.456624  [   32/   54]
train() client id: f_00006-10-0 loss: 0.427971  [   32/   54]
train() client id: f_00007-0-0 loss: 0.661201  [   32/  179]
train() client id: f_00007-0-1 loss: 0.646005  [   64/  179]
train() client id: f_00007-0-2 loss: 0.804465  [   96/  179]
train() client id: f_00007-0-3 loss: 0.696830  [  128/  179]
train() client id: f_00007-0-4 loss: 0.775919  [  160/  179]
train() client id: f_00007-1-0 loss: 0.575583  [   32/  179]
train() client id: f_00007-1-1 loss: 0.685035  [   64/  179]
train() client id: f_00007-1-2 loss: 0.730391  [   96/  179]
train() client id: f_00007-1-3 loss: 0.558819  [  128/  179]
train() client id: f_00007-1-4 loss: 1.035100  [  160/  179]
train() client id: f_00007-2-0 loss: 0.598612  [   32/  179]
train() client id: f_00007-2-1 loss: 0.863881  [   64/  179]
train() client id: f_00007-2-2 loss: 0.747213  [   96/  179]
train() client id: f_00007-2-3 loss: 0.637732  [  128/  179]
train() client id: f_00007-2-4 loss: 0.668881  [  160/  179]
train() client id: f_00007-3-0 loss: 0.520320  [   32/  179]
train() client id: f_00007-3-1 loss: 0.809787  [   64/  179]
train() client id: f_00007-3-2 loss: 0.545798  [   96/  179]
train() client id: f_00007-3-3 loss: 0.598283  [  128/  179]
train() client id: f_00007-3-4 loss: 0.899860  [  160/  179]
train() client id: f_00007-4-0 loss: 0.723671  [   32/  179]
train() client id: f_00007-4-1 loss: 0.573675  [   64/  179]
train() client id: f_00007-4-2 loss: 0.624389  [   96/  179]
train() client id: f_00007-4-3 loss: 0.818274  [  128/  179]
train() client id: f_00007-4-4 loss: 0.677854  [  160/  179]
train() client id: f_00007-5-0 loss: 0.721309  [   32/  179]
train() client id: f_00007-5-1 loss: 0.774413  [   64/  179]
train() client id: f_00007-5-2 loss: 0.677351  [   96/  179]
train() client id: f_00007-5-3 loss: 0.637129  [  128/  179]
train() client id: f_00007-5-4 loss: 0.593591  [  160/  179]
train() client id: f_00007-6-0 loss: 0.621840  [   32/  179]
train() client id: f_00007-6-1 loss: 0.853755  [   64/  179]
train() client id: f_00007-6-2 loss: 0.579262  [   96/  179]
train() client id: f_00007-6-3 loss: 0.535639  [  128/  179]
train() client id: f_00007-6-4 loss: 0.812434  [  160/  179]
train() client id: f_00007-7-0 loss: 0.473436  [   32/  179]
train() client id: f_00007-7-1 loss: 0.600593  [   64/  179]
train() client id: f_00007-7-2 loss: 0.740409  [   96/  179]
train() client id: f_00007-7-3 loss: 0.773928  [  128/  179]
train() client id: f_00007-7-4 loss: 0.621615  [  160/  179]
train() client id: f_00007-8-0 loss: 0.833935  [   32/  179]
train() client id: f_00007-8-1 loss: 0.736377  [   64/  179]
train() client id: f_00007-8-2 loss: 0.638849  [   96/  179]
train() client id: f_00007-8-3 loss: 0.535824  [  128/  179]
train() client id: f_00007-8-4 loss: 0.674740  [  160/  179]
train() client id: f_00007-9-0 loss: 0.645842  [   32/  179]
train() client id: f_00007-9-1 loss: 0.685142  [   64/  179]
train() client id: f_00007-9-2 loss: 0.652179  [   96/  179]
train() client id: f_00007-9-3 loss: 0.584347  [  128/  179]
train() client id: f_00007-9-4 loss: 0.674889  [  160/  179]
train() client id: f_00007-10-0 loss: 0.530985  [   32/  179]
train() client id: f_00007-10-1 loss: 0.793265  [   64/  179]
train() client id: f_00007-10-2 loss: 0.725620  [   96/  179]
train() client id: f_00007-10-3 loss: 0.792508  [  128/  179]
train() client id: f_00007-10-4 loss: 0.601363  [  160/  179]
train() client id: f_00008-0-0 loss: 0.750826  [   32/  130]
train() client id: f_00008-0-1 loss: 0.569665  [   64/  130]
train() client id: f_00008-0-2 loss: 0.720733  [   96/  130]
train() client id: f_00008-0-3 loss: 0.718514  [  128/  130]
train() client id: f_00008-1-0 loss: 0.729960  [   32/  130]
train() client id: f_00008-1-1 loss: 0.824391  [   64/  130]
train() client id: f_00008-1-2 loss: 0.746555  [   96/  130]
train() client id: f_00008-1-3 loss: 0.500562  [  128/  130]
train() client id: f_00008-2-0 loss: 0.754468  [   32/  130]
train() client id: f_00008-2-1 loss: 0.665931  [   64/  130]
train() client id: f_00008-2-2 loss: 0.754990  [   96/  130]
train() client id: f_00008-2-3 loss: 0.607735  [  128/  130]
train() client id: f_00008-3-0 loss: 0.604606  [   32/  130]
train() client id: f_00008-3-1 loss: 0.726942  [   64/  130]
train() client id: f_00008-3-2 loss: 0.727029  [   96/  130]
train() client id: f_00008-3-3 loss: 0.724187  [  128/  130]
train() client id: f_00008-4-0 loss: 0.645675  [   32/  130]
train() client id: f_00008-4-1 loss: 0.715269  [   64/  130]
train() client id: f_00008-4-2 loss: 0.686633  [   96/  130]
train() client id: f_00008-4-3 loss: 0.732098  [  128/  130]
train() client id: f_00008-5-0 loss: 0.746067  [   32/  130]
train() client id: f_00008-5-1 loss: 0.700569  [   64/  130]
train() client id: f_00008-5-2 loss: 0.665331  [   96/  130]
train() client id: f_00008-5-3 loss: 0.668866  [  128/  130]
train() client id: f_00008-6-0 loss: 0.743681  [   32/  130]
train() client id: f_00008-6-1 loss: 0.717207  [   64/  130]
train() client id: f_00008-6-2 loss: 0.688334  [   96/  130]
train() client id: f_00008-6-3 loss: 0.618698  [  128/  130]
train() client id: f_00008-7-0 loss: 0.733452  [   32/  130]
train() client id: f_00008-7-1 loss: 0.774438  [   64/  130]
train() client id: f_00008-7-2 loss: 0.704325  [   96/  130]
train() client id: f_00008-7-3 loss: 0.584947  [  128/  130]
train() client id: f_00008-8-0 loss: 0.618874  [   32/  130]
train() client id: f_00008-8-1 loss: 0.762738  [   64/  130]
train() client id: f_00008-8-2 loss: 0.690446  [   96/  130]
train() client id: f_00008-8-3 loss: 0.716656  [  128/  130]
train() client id: f_00008-9-0 loss: 0.691306  [   32/  130]
train() client id: f_00008-9-1 loss: 0.627607  [   64/  130]
train() client id: f_00008-9-2 loss: 0.813906  [   96/  130]
train() client id: f_00008-9-3 loss: 0.648145  [  128/  130]
train() client id: f_00008-10-0 loss: 0.685380  [   32/  130]
train() client id: f_00008-10-1 loss: 0.785557  [   64/  130]
train() client id: f_00008-10-2 loss: 0.604587  [   96/  130]
train() client id: f_00008-10-3 loss: 0.681709  [  128/  130]
train() client id: f_00009-0-0 loss: 0.986610  [   32/  118]
train() client id: f_00009-0-1 loss: 1.194675  [   64/  118]
train() client id: f_00009-0-2 loss: 0.912758  [   96/  118]
train() client id: f_00009-1-0 loss: 1.036162  [   32/  118]
train() client id: f_00009-1-1 loss: 1.068300  [   64/  118]
train() client id: f_00009-1-2 loss: 1.038370  [   96/  118]
train() client id: f_00009-2-0 loss: 1.105006  [   32/  118]
train() client id: f_00009-2-1 loss: 0.865687  [   64/  118]
train() client id: f_00009-2-2 loss: 0.926257  [   96/  118]
train() client id: f_00009-3-0 loss: 0.817918  [   32/  118]
train() client id: f_00009-3-1 loss: 0.922134  [   64/  118]
train() client id: f_00009-3-2 loss: 0.987341  [   96/  118]
train() client id: f_00009-4-0 loss: 1.013209  [   32/  118]
train() client id: f_00009-4-1 loss: 0.755222  [   64/  118]
train() client id: f_00009-4-2 loss: 0.889688  [   96/  118]
train() client id: f_00009-5-0 loss: 0.884837  [   32/  118]
train() client id: f_00009-5-1 loss: 0.711491  [   64/  118]
train() client id: f_00009-5-2 loss: 0.928649  [   96/  118]
train() client id: f_00009-6-0 loss: 0.897119  [   32/  118]
train() client id: f_00009-6-1 loss: 0.740823  [   64/  118]
train() client id: f_00009-6-2 loss: 0.870921  [   96/  118]
train() client id: f_00009-7-0 loss: 0.862732  [   32/  118]
train() client id: f_00009-7-1 loss: 0.765962  [   64/  118]
train() client id: f_00009-7-2 loss: 0.815318  [   96/  118]
train() client id: f_00009-8-0 loss: 0.821335  [   32/  118]
train() client id: f_00009-8-1 loss: 0.834177  [   64/  118]
train() client id: f_00009-8-2 loss: 0.719323  [   96/  118]
train() client id: f_00009-9-0 loss: 0.750861  [   32/  118]
train() client id: f_00009-9-1 loss: 0.811023  [   64/  118]
train() client id: f_00009-9-2 loss: 0.788323  [   96/  118]
train() client id: f_00009-10-0 loss: 0.719412  [   32/  118]
train() client id: f_00009-10-1 loss: 0.988326  [   64/  118]
train() client id: f_00009-10-2 loss: 0.748664  [   96/  118]
At round 72 accuracy: 0.6445623342175066
At round 72 training accuracy: 0.5922199865861838
At round 72 training loss: 0.8289723122127233
update_location
xs = 8.927491 481.223621 5.882650 0.934260 -397.581990 -245.230757 -205.849135 -5.143845 -420.120581 20.134486 
ys = -472.390647 7.291448 370.684448 -192.290817 -9.642386 0.794442 -1.381692 366.628436 25.881276 -907.232496 
xs mean: -75.68237997052123
ys mean: -81.16579882624052
dists_uav = 482.941636 491.558072 383.981205 216.740931 410.078546 264.837225 228.857544 380.056403 432.632804 912.949177 
uav_gains = -123.872979 -124.084638 -120.781243 -108.768001 -121.762111 -112.602114 -109.619549 -120.617454 -122.492784 -130.986316 
uav_gains_db_mean: -119.5587188880543
dists_bs = 673.011123 677.314992 266.478773 407.253494 289.198120 187.829539 179.059126 256.071418 286.915481 1099.683883 
bs_gains = -118.751682 -118.829198 -107.485612 -112.643274 -108.480532 -103.232448 -102.650960 -107.001170 -108.384171 -124.722559 
bs_gains_db_mean: -111.21816072657703
Round 73
-------------------------------
ene_coms = [0.04936212 0.05112829 0.00911614 0.00985885 0.00968254 0.01294557
 0.01041857 0.00886272 0.00962477 0.05697191]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.37778067 2.72864293 1.22928226 0.44777494 2.99243408 1.45447884
 0.55602526 1.76074043 1.28898035 1.32477802]
obj_prev = 15.160917771333834
eta_min = 1.3283256296723616e-71	eta_max = 0.930969104532801
af = 3.101969802094253	bf = 0.7242097431806587	zeta = 3.4121667823036788	eta = 0.9090909090909091
af = 3.101969802094253	bf = 0.7242097431806587	zeta = 11.218449470424325	eta = 0.2765061081098692
af = 3.101969802094253	bf = 0.7242097431806587	zeta = 6.512648357844445	eta = 0.476299292031935
af = 3.101969802094253	bf = 0.7242097431806587	zeta = 5.776139262896152	eta = 0.5370316851638595
af = 3.101969802094253	bf = 0.7242097431806587	zeta = 5.729763021196084	eta = 0.5413783764911658
af = 3.101969802094253	bf = 0.7242097431806587	zeta = 5.729543841024935	eta = 0.5413990865875554
af = 3.101969802094253	bf = 0.7242097431806587	zeta = 5.729543836075988	eta = 0.5413990870551939
eta = 0.5413990870551939
ene_coms = [0.04936212 0.05112829 0.00911614 0.00985885 0.00968254 0.01294557
 0.01041857 0.00886272 0.00962477 0.05697191]
ene_comp = [0.05063268 0.10648938 0.04982899 0.01727941 0.12296509 0.05866962
 0.02169972 0.07193056 0.05224007 0.04741791]
ene_total = [0.69267002 1.09182701 0.40831646 0.18798833 0.9188581  0.49608269
 0.2224853  0.55965988 0.42854149 0.72311456]
ti_comp = [3.30044015 3.28277849 3.70289996 3.69547289 3.69723596 3.6646057
 3.68987571 3.70543418 3.69781363 3.22434225]
ti_coms = [0.49362121 0.51128288 0.0911614  0.09858848 0.0968254  0.12945567
 0.10418565 0.08862718 0.09624773 0.56971911]
t_total = [26.2816906 26.2816906 26.2816906 26.2816906 26.2816906 26.2816906
 26.2816906 26.2816906 26.2816906 26.2816906]
ene_coms = [0.04936212 0.05112829 0.00911614 0.00985885 0.00968254 0.01294557
 0.01041857 0.00886272 0.00962477 0.05697191]
ene_comp = [7.44781983e-07 7.00350939e-06 5.63952268e-07 2.36116630e-08
 8.50102304e-06 9.39864646e-07 4.69049908e-08 1.69411207e-06
 6.51630299e-07 6.40951259e-07]
ene_total = [0.34193953 0.35421723 0.06315195 0.06829299 0.06713042 0.08968123
 0.07217035 0.06140431 0.06667589 0.39465229]
optimize_network iter = 0 obj = 1.5793162030752113
eta = 0.5413990870551939
freqs = [ 7670595.80408756 16219397.25080864  6728373.95754177  2337915.71815347
 16629327.00674385  8004901.99229506  2940440.78249832  9706090.6471967
  7063643.11302342  7353113.8266265 ]
eta_min = 0.5413990870551944	eta_max = 0.7391811017629206
af = 0.00010774043396314635	bf = 0.7242097431806587	zeta = 0.000118514477359461	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.04936212 0.05112829 0.00911614 0.00985885 0.00968254 0.01294557
 0.01041857 0.00886272 0.00962477 0.05697191]
ene_comp = [1.15686878e-07 1.08785410e-06 8.75986241e-08 3.66759620e-09
 1.32046268e-06 1.45989039e-07 7.28574542e-09 2.63146182e-07
 1.01217640e-07 9.95588662e-08]
ene_total = [1.72492961 1.78668107 0.31856034 0.34451082 0.33839589 0.45237908
 0.36406989 0.30971082 0.33633465 1.99084797]
ti_comp = [1.66416539 1.64650372 2.0666252  2.05919812 2.0609612  2.02833093
 2.05360095 2.06915942 2.06153887 1.58806749]
ti_coms = [0.49362121 0.51128288 0.0911614  0.09858848 0.0968254  0.12945567
 0.10418565 0.08862718 0.09624773 0.56971911]
t_total = [26.2816906 26.2816906 26.2816906 26.2816906 26.2816906 26.2816906
 26.2816906 26.2816906 26.2816906 26.2816906]
ene_coms = [0.04936212 0.05112829 0.00911614 0.00985885 0.00968254 0.01294557
 0.01041857 0.00886272 0.00962477 0.05697191]
ene_comp = [3.49996237e-07 3.32626540e-06 2.16314845e-07 9.08560413e-09
 3.26865708e-06 3.66543608e-07 1.80923041e-08 6.49106412e-07
 2.50490761e-07 3.15684185e-07]
ene_total = [0.60123146 0.6227795  0.11103658 0.12008019 0.11797248 0.15768057
 0.12689762 0.10795519 0.11723212 0.69391776]
optimize_network iter = 1 obj = 2.7767834653753245
eta = 0.7391811017629206
freqs = [ 7492595.63790751 15927271.83791702  5937699.29705214  2066467.39498926
 14692978.63121096  7123153.07902081  2602171.63939029  8560858.41094713
  6240365.97583618  7353113.8266265 ]
eta_min = 0.7391811017629221	eta_max = 0.7391811017629208
af = 9.21898254588428e-05	bf = 0.7242097431806587	zeta = 0.00010140880800472708	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.04936212 0.05112829 0.00911614 0.00985885 0.00968254 0.01294557
 0.01041857 0.00886272 0.00962477 0.05697191]
ene_comp = [1.10380026e-07 1.04902060e-06 6.82202713e-08 2.86537144e-09
 1.03085238e-06 1.15598652e-07 5.70585847e-09 2.04711866e-07
 7.89984972e-08 9.95588662e-08]
ene_total = [1.72492943 1.78667971 0.31855966 0.34451079 0.33838577 0.45237802
 0.36406983 0.30970878 0.33633387 1.99084797]
ti_comp = [1.66416539 1.64650372 2.0666252  2.05919812 2.0609612  2.02833093
 2.05360095 2.06915942 2.06153887 1.58806749]
ti_coms = [0.49362121 0.51128288 0.0911614  0.09858848 0.0968254  0.12945567
 0.10418565 0.08862718 0.09624773 0.56971911]
t_total = [26.2816906 26.2816906 26.2816906 26.2816906 26.2816906 26.2816906
 26.2816906 26.2816906 26.2816906 26.2816906]
ene_coms = [0.04936212 0.05112829 0.00911614 0.00985885 0.00968254 0.01294557
 0.01041857 0.00886272 0.00962477 0.05697191]
ene_comp = [3.49996237e-07 3.32626540e-06 2.16314845e-07 9.08560413e-09
 3.26865708e-06 3.66543608e-07 1.80923041e-08 6.49106412e-07
 2.50490761e-07 3.15684185e-07]
ene_total = [0.60123146 0.6227795  0.11103658 0.12008019 0.11797248 0.15768057
 0.12689762 0.10795519 0.11723212 0.69391776]
optimize_network iter = 2 obj = 2.776783465375327
eta = 0.7391811017629208
freqs = [ 7492595.63790751 15927271.83791702  5937699.29705214  2066467.39498926
 14692978.63121096  7123153.07902081  2602171.63939029  8560858.41094713
  6240365.97583617  7353113.8266265 ]
Done!
ene_coms = [0.04936212 0.05112829 0.00911614 0.00985885 0.00968254 0.01294557
 0.01041857 0.00886272 0.00962477 0.05697191]
ene_comp = [3.18308070e-07 3.02511001e-06 1.96730003e-07 8.26300630e-09
 2.97271746e-06 3.33357265e-07 1.64542523e-08 5.90337230e-07
 2.27811680e-07 2.87102582e-07]
ene_total = [0.04936244 0.05113131 0.00911634 0.00985886 0.00968551 0.0129459
 0.01041858 0.00886331 0.009625   0.0569722 ]
At round 73 energy consumption: 0.22797944674603887
At round 73 eta: 0.7391811017629208
At round 73 a_n: 3.176756027494214
At round 73 local rounds: 9.895966935986738
At round 73 global rounds: 12.17993039985395
gradient difference: 0.5134314298629761
train() client id: f_00000-0-0 loss: 1.248036  [   32/  126]
train() client id: f_00000-0-1 loss: 0.885521  [   64/  126]
train() client id: f_00000-0-2 loss: 1.167899  [   96/  126]
train() client id: f_00000-1-0 loss: 0.988321  [   32/  126]
train() client id: f_00000-1-1 loss: 1.000364  [   64/  126]
train() client id: f_00000-1-2 loss: 1.103140  [   96/  126]
train() client id: f_00000-2-0 loss: 0.895859  [   32/  126]
train() client id: f_00000-2-1 loss: 0.927403  [   64/  126]
train() client id: f_00000-2-2 loss: 1.145530  [   96/  126]
train() client id: f_00000-3-0 loss: 1.030700  [   32/  126]
train() client id: f_00000-3-1 loss: 0.913826  [   64/  126]
train() client id: f_00000-3-2 loss: 0.841679  [   96/  126]
train() client id: f_00000-4-0 loss: 0.903033  [   32/  126]
train() client id: f_00000-4-1 loss: 0.934343  [   64/  126]
train() client id: f_00000-4-2 loss: 0.866092  [   96/  126]
train() client id: f_00000-5-0 loss: 0.726017  [   32/  126]
train() client id: f_00000-5-1 loss: 0.982652  [   64/  126]
train() client id: f_00000-5-2 loss: 0.969767  [   96/  126]
train() client id: f_00000-6-0 loss: 1.007952  [   32/  126]
train() client id: f_00000-6-1 loss: 0.883747  [   64/  126]
train() client id: f_00000-6-2 loss: 0.763488  [   96/  126]
train() client id: f_00000-7-0 loss: 0.839825  [   32/  126]
train() client id: f_00000-7-1 loss: 0.849807  [   64/  126]
train() client id: f_00000-7-2 loss: 0.802066  [   96/  126]
train() client id: f_00000-8-0 loss: 0.901742  [   32/  126]
train() client id: f_00000-8-1 loss: 0.893509  [   64/  126]
train() client id: f_00000-8-2 loss: 0.785870  [   96/  126]
train() client id: f_00001-0-0 loss: 0.356687  [   32/  265]
train() client id: f_00001-0-1 loss: 0.592149  [   64/  265]
train() client id: f_00001-0-2 loss: 0.423123  [   96/  265]
train() client id: f_00001-0-3 loss: 0.357405  [  128/  265]
train() client id: f_00001-0-4 loss: 0.560464  [  160/  265]
train() client id: f_00001-0-5 loss: 0.424175  [  192/  265]
train() client id: f_00001-0-6 loss: 0.426926  [  224/  265]
train() client id: f_00001-0-7 loss: 0.411222  [  256/  265]
train() client id: f_00001-1-0 loss: 0.335039  [   32/  265]
train() client id: f_00001-1-1 loss: 0.365057  [   64/  265]
train() client id: f_00001-1-2 loss: 0.670859  [   96/  265]
train() client id: f_00001-1-3 loss: 0.403810  [  128/  265]
train() client id: f_00001-1-4 loss: 0.472798  [  160/  265]
train() client id: f_00001-1-5 loss: 0.434258  [  192/  265]
train() client id: f_00001-1-6 loss: 0.412913  [  224/  265]
train() client id: f_00001-1-7 loss: 0.367857  [  256/  265]
train() client id: f_00001-2-0 loss: 0.387045  [   32/  265]
train() client id: f_00001-2-1 loss: 0.431631  [   64/  265]
train() client id: f_00001-2-2 loss: 0.449053  [   96/  265]
train() client id: f_00001-2-3 loss: 0.348981  [  128/  265]
train() client id: f_00001-2-4 loss: 0.395735  [  160/  265]
train() client id: f_00001-2-5 loss: 0.393124  [  192/  265]
train() client id: f_00001-2-6 loss: 0.472311  [  224/  265]
train() client id: f_00001-2-7 loss: 0.430430  [  256/  265]
train() client id: f_00001-3-0 loss: 0.409413  [   32/  265]
train() client id: f_00001-3-1 loss: 0.375741  [   64/  265]
train() client id: f_00001-3-2 loss: 0.398151  [   96/  265]
train() client id: f_00001-3-3 loss: 0.441219  [  128/  265]
train() client id: f_00001-3-4 loss: 0.535992  [  160/  265]
train() client id: f_00001-3-5 loss: 0.357017  [  192/  265]
train() client id: f_00001-3-6 loss: 0.393678  [  224/  265]
train() client id: f_00001-3-7 loss: 0.427564  [  256/  265]
train() client id: f_00001-4-0 loss: 0.347437  [   32/  265]
train() client id: f_00001-4-1 loss: 0.391950  [   64/  265]
train() client id: f_00001-4-2 loss: 0.343798  [   96/  265]
train() client id: f_00001-4-3 loss: 0.492255  [  128/  265]
train() client id: f_00001-4-4 loss: 0.430231  [  160/  265]
train() client id: f_00001-4-5 loss: 0.484276  [  192/  265]
train() client id: f_00001-4-6 loss: 0.445658  [  224/  265]
train() client id: f_00001-4-7 loss: 0.462593  [  256/  265]
train() client id: f_00001-5-0 loss: 0.423816  [   32/  265]
train() client id: f_00001-5-1 loss: 0.353450  [   64/  265]
train() client id: f_00001-5-2 loss: 0.506330  [   96/  265]
train() client id: f_00001-5-3 loss: 0.445597  [  128/  265]
train() client id: f_00001-5-4 loss: 0.393079  [  160/  265]
train() client id: f_00001-5-5 loss: 0.424352  [  192/  265]
train() client id: f_00001-5-6 loss: 0.419446  [  224/  265]
train() client id: f_00001-5-7 loss: 0.400606  [  256/  265]
train() client id: f_00001-6-0 loss: 0.364375  [   32/  265]
train() client id: f_00001-6-1 loss: 0.534740  [   64/  265]
train() client id: f_00001-6-2 loss: 0.413240  [   96/  265]
train() client id: f_00001-6-3 loss: 0.356449  [  128/  265]
train() client id: f_00001-6-4 loss: 0.493678  [  160/  265]
train() client id: f_00001-6-5 loss: 0.413853  [  192/  265]
train() client id: f_00001-6-6 loss: 0.373809  [  224/  265]
train() client id: f_00001-6-7 loss: 0.398136  [  256/  265]
train() client id: f_00001-7-0 loss: 0.365578  [   32/  265]
train() client id: f_00001-7-1 loss: 0.341685  [   64/  265]
train() client id: f_00001-7-2 loss: 0.412981  [   96/  265]
train() client id: f_00001-7-3 loss: 0.449044  [  128/  265]
train() client id: f_00001-7-4 loss: 0.507151  [  160/  265]
train() client id: f_00001-7-5 loss: 0.449238  [  192/  265]
train() client id: f_00001-7-6 loss: 0.483991  [  224/  265]
train() client id: f_00001-7-7 loss: 0.334192  [  256/  265]
train() client id: f_00001-8-0 loss: 0.322519  [   32/  265]
train() client id: f_00001-8-1 loss: 0.436961  [   64/  265]
train() client id: f_00001-8-2 loss: 0.543903  [   96/  265]
train() client id: f_00001-8-3 loss: 0.493774  [  128/  265]
train() client id: f_00001-8-4 loss: 0.411462  [  160/  265]
train() client id: f_00001-8-5 loss: 0.317797  [  192/  265]
train() client id: f_00001-8-6 loss: 0.424992  [  224/  265]
train() client id: f_00001-8-7 loss: 0.379475  [  256/  265]
train() client id: f_00002-0-0 loss: 1.201307  [   32/  124]
train() client id: f_00002-0-1 loss: 1.200104  [   64/  124]
train() client id: f_00002-0-2 loss: 1.017235  [   96/  124]
train() client id: f_00002-1-0 loss: 1.266014  [   32/  124]
train() client id: f_00002-1-1 loss: 1.141413  [   64/  124]
train() client id: f_00002-1-2 loss: 1.049465  [   96/  124]
train() client id: f_00002-2-0 loss: 1.125903  [   32/  124]
train() client id: f_00002-2-1 loss: 0.963868  [   64/  124]
train() client id: f_00002-2-2 loss: 1.117805  [   96/  124]
train() client id: f_00002-3-0 loss: 1.103800  [   32/  124]
train() client id: f_00002-3-1 loss: 0.994672  [   64/  124]
train() client id: f_00002-3-2 loss: 1.069263  [   96/  124]
train() client id: f_00002-4-0 loss: 1.117476  [   32/  124]
train() client id: f_00002-4-1 loss: 1.108843  [   64/  124]
train() client id: f_00002-4-2 loss: 0.886863  [   96/  124]
train() client id: f_00002-5-0 loss: 1.050534  [   32/  124]
train() client id: f_00002-5-1 loss: 0.985350  [   64/  124]
train() client id: f_00002-5-2 loss: 1.132025  [   96/  124]
train() client id: f_00002-6-0 loss: 1.120513  [   32/  124]
train() client id: f_00002-6-1 loss: 0.993642  [   64/  124]
train() client id: f_00002-6-2 loss: 1.042993  [   96/  124]
train() client id: f_00002-7-0 loss: 1.062722  [   32/  124]
train() client id: f_00002-7-1 loss: 0.968973  [   64/  124]
train() client id: f_00002-7-2 loss: 1.056836  [   96/  124]
train() client id: f_00002-8-0 loss: 1.022193  [   32/  124]
train() client id: f_00002-8-1 loss: 1.163439  [   64/  124]
train() client id: f_00002-8-2 loss: 0.909926  [   96/  124]
train() client id: f_00003-0-0 loss: 0.616713  [   32/   43]
train() client id: f_00003-1-0 loss: 0.450407  [   32/   43]
train() client id: f_00003-2-0 loss: 0.833341  [   32/   43]
train() client id: f_00003-3-0 loss: 0.614022  [   32/   43]
train() client id: f_00003-4-0 loss: 0.767733  [   32/   43]
train() client id: f_00003-5-0 loss: 0.658386  [   32/   43]
train() client id: f_00003-6-0 loss: 0.697568  [   32/   43]
train() client id: f_00003-7-0 loss: 0.700893  [   32/   43]
train() client id: f_00003-8-0 loss: 0.621759  [   32/   43]
train() client id: f_00004-0-0 loss: 0.919281  [   32/  306]
train() client id: f_00004-0-1 loss: 1.022173  [   64/  306]
train() client id: f_00004-0-2 loss: 0.818817  [   96/  306]
train() client id: f_00004-0-3 loss: 0.816215  [  128/  306]
train() client id: f_00004-0-4 loss: 0.881949  [  160/  306]
train() client id: f_00004-0-5 loss: 0.847299  [  192/  306]
train() client id: f_00004-0-6 loss: 1.038952  [  224/  306]
train() client id: f_00004-0-7 loss: 0.700683  [  256/  306]
train() client id: f_00004-0-8 loss: 0.828500  [  288/  306]
train() client id: f_00004-1-0 loss: 0.880590  [   32/  306]
train() client id: f_00004-1-1 loss: 0.773663  [   64/  306]
train() client id: f_00004-1-2 loss: 0.737648  [   96/  306]
train() client id: f_00004-1-3 loss: 0.726133  [  128/  306]
train() client id: f_00004-1-4 loss: 1.190167  [  160/  306]
train() client id: f_00004-1-5 loss: 0.914542  [  192/  306]
train() client id: f_00004-1-6 loss: 0.854838  [  224/  306]
train() client id: f_00004-1-7 loss: 0.885877  [  256/  306]
train() client id: f_00004-1-8 loss: 0.896609  [  288/  306]
train() client id: f_00004-2-0 loss: 0.791805  [   32/  306]
train() client id: f_00004-2-1 loss: 0.956996  [   64/  306]
train() client id: f_00004-2-2 loss: 0.886008  [   96/  306]
train() client id: f_00004-2-3 loss: 1.048884  [  128/  306]
train() client id: f_00004-2-4 loss: 0.716285  [  160/  306]
train() client id: f_00004-2-5 loss: 0.828064  [  192/  306]
train() client id: f_00004-2-6 loss: 1.049403  [  224/  306]
train() client id: f_00004-2-7 loss: 0.834680  [  256/  306]
train() client id: f_00004-2-8 loss: 0.750475  [  288/  306]
train() client id: f_00004-3-0 loss: 0.910213  [   32/  306]
train() client id: f_00004-3-1 loss: 0.873410  [   64/  306]
train() client id: f_00004-3-2 loss: 0.895597  [   96/  306]
train() client id: f_00004-3-3 loss: 1.034557  [  128/  306]
train() client id: f_00004-3-4 loss: 0.680243  [  160/  306]
train() client id: f_00004-3-5 loss: 1.046938  [  192/  306]
train() client id: f_00004-3-6 loss: 0.833073  [  224/  306]
train() client id: f_00004-3-7 loss: 0.720359  [  256/  306]
train() client id: f_00004-3-8 loss: 0.849393  [  288/  306]
train() client id: f_00004-4-0 loss: 0.695399  [   32/  306]
train() client id: f_00004-4-1 loss: 0.842667  [   64/  306]
train() client id: f_00004-4-2 loss: 0.955655  [   96/  306]
train() client id: f_00004-4-3 loss: 0.851462  [  128/  306]
train() client id: f_00004-4-4 loss: 0.882776  [  160/  306]
train() client id: f_00004-4-5 loss: 0.800901  [  192/  306]
train() client id: f_00004-4-6 loss: 0.879359  [  224/  306]
train() client id: f_00004-4-7 loss: 0.911212  [  256/  306]
train() client id: f_00004-4-8 loss: 0.879639  [  288/  306]
train() client id: f_00004-5-0 loss: 0.921056  [   32/  306]
train() client id: f_00004-5-1 loss: 0.894632  [   64/  306]
train() client id: f_00004-5-2 loss: 0.763227  [   96/  306]
train() client id: f_00004-5-3 loss: 0.814245  [  128/  306]
train() client id: f_00004-5-4 loss: 0.886105  [  160/  306]
train() client id: f_00004-5-5 loss: 0.853611  [  192/  306]
train() client id: f_00004-5-6 loss: 0.842900  [  224/  306]
train() client id: f_00004-5-7 loss: 0.921549  [  256/  306]
train() client id: f_00004-5-8 loss: 0.774589  [  288/  306]
train() client id: f_00004-6-0 loss: 0.877167  [   32/  306]
train() client id: f_00004-6-1 loss: 0.864439  [   64/  306]
train() client id: f_00004-6-2 loss: 0.791460  [   96/  306]
train() client id: f_00004-6-3 loss: 0.918248  [  128/  306]
train() client id: f_00004-6-4 loss: 0.791758  [  160/  306]
train() client id: f_00004-6-5 loss: 0.869042  [  192/  306]
train() client id: f_00004-6-6 loss: 0.871692  [  224/  306]
train() client id: f_00004-6-7 loss: 0.840963  [  256/  306]
train() client id: f_00004-6-8 loss: 0.934897  [  288/  306]
train() client id: f_00004-7-0 loss: 0.824484  [   32/  306]
train() client id: f_00004-7-1 loss: 0.768682  [   64/  306]
train() client id: f_00004-7-2 loss: 0.883785  [   96/  306]
train() client id: f_00004-7-3 loss: 0.893493  [  128/  306]
train() client id: f_00004-7-4 loss: 0.823802  [  160/  306]
train() client id: f_00004-7-5 loss: 0.926730  [  192/  306]
train() client id: f_00004-7-6 loss: 0.932071  [  224/  306]
train() client id: f_00004-7-7 loss: 0.824096  [  256/  306]
train() client id: f_00004-7-8 loss: 0.822188  [  288/  306]
train() client id: f_00004-8-0 loss: 0.843968  [   32/  306]
train() client id: f_00004-8-1 loss: 0.905258  [   64/  306]
train() client id: f_00004-8-2 loss: 0.781545  [   96/  306]
train() client id: f_00004-8-3 loss: 0.811277  [  128/  306]
train() client id: f_00004-8-4 loss: 0.866017  [  160/  306]
train() client id: f_00004-8-5 loss: 0.962030  [  192/  306]
train() client id: f_00004-8-6 loss: 0.792830  [  224/  306]
train() client id: f_00004-8-7 loss: 0.779097  [  256/  306]
train() client id: f_00004-8-8 loss: 0.958519  [  288/  306]
train() client id: f_00005-0-0 loss: 0.527364  [   32/  146]
train() client id: f_00005-0-1 loss: 0.593825  [   64/  146]
train() client id: f_00005-0-2 loss: 0.579051  [   96/  146]
train() client id: f_00005-0-3 loss: 1.059655  [  128/  146]
train() client id: f_00005-1-0 loss: 0.802965  [   32/  146]
train() client id: f_00005-1-1 loss: 0.645261  [   64/  146]
train() client id: f_00005-1-2 loss: 0.679299  [   96/  146]
train() client id: f_00005-1-3 loss: 0.510956  [  128/  146]
train() client id: f_00005-2-0 loss: 0.500341  [   32/  146]
train() client id: f_00005-2-1 loss: 0.910358  [   64/  146]
train() client id: f_00005-2-2 loss: 0.564545  [   96/  146]
train() client id: f_00005-2-3 loss: 0.784667  [  128/  146]
train() client id: f_00005-3-0 loss: 1.139063  [   32/  146]
train() client id: f_00005-3-1 loss: 0.351853  [   64/  146]
train() client id: f_00005-3-2 loss: 0.763572  [   96/  146]
train() client id: f_00005-3-3 loss: 0.543827  [  128/  146]
train() client id: f_00005-4-0 loss: 0.781073  [   32/  146]
train() client id: f_00005-4-1 loss: 0.647382  [   64/  146]
train() client id: f_00005-4-2 loss: 0.484902  [   96/  146]
train() client id: f_00005-4-3 loss: 0.947704  [  128/  146]
train() client id: f_00005-5-0 loss: 0.736696  [   32/  146]
train() client id: f_00005-5-1 loss: 0.650435  [   64/  146]
train() client id: f_00005-5-2 loss: 0.574248  [   96/  146]
train() client id: f_00005-5-3 loss: 0.459613  [  128/  146]
train() client id: f_00005-6-0 loss: 0.566998  [   32/  146]
train() client id: f_00005-6-1 loss: 0.854103  [   64/  146]
train() client id: f_00005-6-2 loss: 0.806460  [   96/  146]
train() client id: f_00005-6-3 loss: 0.708076  [  128/  146]
train() client id: f_00005-7-0 loss: 0.604534  [   32/  146]
train() client id: f_00005-7-1 loss: 0.896429  [   64/  146]
train() client id: f_00005-7-2 loss: 0.562171  [   96/  146]
train() client id: f_00005-7-3 loss: 0.706619  [  128/  146]
train() client id: f_00005-8-0 loss: 0.740267  [   32/  146]
train() client id: f_00005-8-1 loss: 0.528052  [   64/  146]
train() client id: f_00005-8-2 loss: 0.673914  [   96/  146]
train() client id: f_00005-8-3 loss: 0.631112  [  128/  146]
train() client id: f_00006-0-0 loss: 0.497806  [   32/   54]
train() client id: f_00006-1-0 loss: 0.519225  [   32/   54]
train() client id: f_00006-2-0 loss: 0.516791  [   32/   54]
train() client id: f_00006-3-0 loss: 0.461967  [   32/   54]
train() client id: f_00006-4-0 loss: 0.483924  [   32/   54]
train() client id: f_00006-5-0 loss: 0.511830  [   32/   54]
train() client id: f_00006-6-0 loss: 0.521710  [   32/   54]
train() client id: f_00006-7-0 loss: 0.561211  [   32/   54]
train() client id: f_00006-8-0 loss: 0.439486  [   32/   54]
train() client id: f_00007-0-0 loss: 0.777072  [   32/  179]
train() client id: f_00007-0-1 loss: 0.466378  [   64/  179]
train() client id: f_00007-0-2 loss: 0.566459  [   96/  179]
train() client id: f_00007-0-3 loss: 0.630269  [  128/  179]
train() client id: f_00007-0-4 loss: 0.661232  [  160/  179]
train() client id: f_00007-1-0 loss: 0.578099  [   32/  179]
train() client id: f_00007-1-1 loss: 0.834014  [   64/  179]
train() client id: f_00007-1-2 loss: 0.609650  [   96/  179]
train() client id: f_00007-1-3 loss: 0.463124  [  128/  179]
train() client id: f_00007-1-4 loss: 0.578956  [  160/  179]
train() client id: f_00007-2-0 loss: 0.693545  [   32/  179]
train() client id: f_00007-2-1 loss: 0.407218  [   64/  179]
train() client id: f_00007-2-2 loss: 0.664461  [   96/  179]
train() client id: f_00007-2-3 loss: 0.724971  [  128/  179]
train() client id: f_00007-2-4 loss: 0.657608  [  160/  179]
train() client id: f_00007-3-0 loss: 0.638259  [   32/  179]
train() client id: f_00007-3-1 loss: 0.462214  [   64/  179]
train() client id: f_00007-3-2 loss: 0.436835  [   96/  179]
train() client id: f_00007-3-3 loss: 0.665113  [  128/  179]
train() client id: f_00007-3-4 loss: 0.540934  [  160/  179]
train() client id: f_00007-4-0 loss: 0.709851  [   32/  179]
train() client id: f_00007-4-1 loss: 0.631816  [   64/  179]
train() client id: f_00007-4-2 loss: 0.543878  [   96/  179]
train() client id: f_00007-4-3 loss: 0.615765  [  128/  179]
train() client id: f_00007-4-4 loss: 0.560844  [  160/  179]
train() client id: f_00007-5-0 loss: 0.532445  [   32/  179]
train() client id: f_00007-5-1 loss: 0.426951  [   64/  179]
train() client id: f_00007-5-2 loss: 0.458828  [   96/  179]
train() client id: f_00007-5-3 loss: 0.761013  [  128/  179]
train() client id: f_00007-5-4 loss: 0.598232  [  160/  179]
train() client id: f_00007-6-0 loss: 0.735537  [   32/  179]
train() client id: f_00007-6-1 loss: 0.524617  [   64/  179]
train() client id: f_00007-6-2 loss: 0.517425  [   96/  179]
train() client id: f_00007-6-3 loss: 0.553706  [  128/  179]
train() client id: f_00007-6-4 loss: 0.572582  [  160/  179]
train() client id: f_00007-7-0 loss: 0.506970  [   32/  179]
train() client id: f_00007-7-1 loss: 0.489109  [   64/  179]
train() client id: f_00007-7-2 loss: 0.555524  [   96/  179]
train() client id: f_00007-7-3 loss: 0.688540  [  128/  179]
train() client id: f_00007-7-4 loss: 0.505766  [  160/  179]
train() client id: f_00007-8-0 loss: 0.527748  [   32/  179]
train() client id: f_00007-8-1 loss: 0.783296  [   64/  179]
train() client id: f_00007-8-2 loss: 0.436483  [   96/  179]
train() client id: f_00007-8-3 loss: 0.663919  [  128/  179]
train() client id: f_00007-8-4 loss: 0.529127  [  160/  179]
train() client id: f_00008-0-0 loss: 0.664934  [   32/  130]
train() client id: f_00008-0-1 loss: 0.673548  [   64/  130]
train() client id: f_00008-0-2 loss: 0.861616  [   96/  130]
train() client id: f_00008-0-3 loss: 0.690205  [  128/  130]
train() client id: f_00008-1-0 loss: 0.651340  [   32/  130]
train() client id: f_00008-1-1 loss: 0.743924  [   64/  130]
train() client id: f_00008-1-2 loss: 0.695129  [   96/  130]
train() client id: f_00008-1-3 loss: 0.802204  [  128/  130]
train() client id: f_00008-2-0 loss: 0.722254  [   32/  130]
train() client id: f_00008-2-1 loss: 0.659729  [   64/  130]
train() client id: f_00008-2-2 loss: 0.806963  [   96/  130]
train() client id: f_00008-2-3 loss: 0.713144  [  128/  130]
train() client id: f_00008-3-0 loss: 0.711273  [   32/  130]
train() client id: f_00008-3-1 loss: 0.668493  [   64/  130]
train() client id: f_00008-3-2 loss: 0.838094  [   96/  130]
train() client id: f_00008-3-3 loss: 0.688762  [  128/  130]
train() client id: f_00008-4-0 loss: 0.863194  [   32/  130]
train() client id: f_00008-4-1 loss: 0.740261  [   64/  130]
train() client id: f_00008-4-2 loss: 0.611107  [   96/  130]
train() client id: f_00008-4-3 loss: 0.693891  [  128/  130]
train() client id: f_00008-5-0 loss: 0.743801  [   32/  130]
train() client id: f_00008-5-1 loss: 0.689248  [   64/  130]
train() client id: f_00008-5-2 loss: 0.699384  [   96/  130]
train() client id: f_00008-5-3 loss: 0.758402  [  128/  130]
train() client id: f_00008-6-0 loss: 0.683514  [   32/  130]
train() client id: f_00008-6-1 loss: 0.752429  [   64/  130]
train() client id: f_00008-6-2 loss: 0.691541  [   96/  130]
train() client id: f_00008-6-3 loss: 0.777950  [  128/  130]
train() client id: f_00008-7-0 loss: 0.767019  [   32/  130]
train() client id: f_00008-7-1 loss: 0.820264  [   64/  130]
train() client id: f_00008-7-2 loss: 0.674128  [   96/  130]
train() client id: f_00008-7-3 loss: 0.631922  [  128/  130]
train() client id: f_00008-8-0 loss: 0.796807  [   32/  130]
train() client id: f_00008-8-1 loss: 0.695795  [   64/  130]
train() client id: f_00008-8-2 loss: 0.818649  [   96/  130]
train() client id: f_00008-8-3 loss: 0.583138  [  128/  130]
train() client id: f_00009-0-0 loss: 1.001673  [   32/  118]
train() client id: f_00009-0-1 loss: 0.961648  [   64/  118]
train() client id: f_00009-0-2 loss: 1.021845  [   96/  118]
train() client id: f_00009-1-0 loss: 0.946310  [   32/  118]
train() client id: f_00009-1-1 loss: 0.857891  [   64/  118]
train() client id: f_00009-1-2 loss: 1.083735  [   96/  118]
train() client id: f_00009-2-0 loss: 0.901680  [   32/  118]
train() client id: f_00009-2-1 loss: 0.858806  [   64/  118]
train() client id: f_00009-2-2 loss: 0.965702  [   96/  118]
train() client id: f_00009-3-0 loss: 0.826249  [   32/  118]
train() client id: f_00009-3-1 loss: 1.110868  [   64/  118]
train() client id: f_00009-3-2 loss: 0.807908  [   96/  118]
train() client id: f_00009-4-0 loss: 0.934938  [   32/  118]
train() client id: f_00009-4-1 loss: 0.863034  [   64/  118]
train() client id: f_00009-4-2 loss: 0.914573  [   96/  118]
train() client id: f_00009-5-0 loss: 1.061511  [   32/  118]
train() client id: f_00009-5-1 loss: 0.789475  [   64/  118]
train() client id: f_00009-5-2 loss: 0.896827  [   96/  118]
train() client id: f_00009-6-0 loss: 1.027095  [   32/  118]
train() client id: f_00009-6-1 loss: 0.759441  [   64/  118]
train() client id: f_00009-6-2 loss: 0.814974  [   96/  118]
train() client id: f_00009-7-0 loss: 0.941639  [   32/  118]
train() client id: f_00009-7-1 loss: 0.840440  [   64/  118]
train() client id: f_00009-7-2 loss: 0.895917  [   96/  118]
train() client id: f_00009-8-0 loss: 0.926703  [   32/  118]
train() client id: f_00009-8-1 loss: 0.715472  [   64/  118]
train() client id: f_00009-8-2 loss: 0.865428  [   96/  118]
At round 73 accuracy: 0.6472148541114059
At round 73 training accuracy: 0.5902079141515761
At round 73 training loss: 0.8382909672452639
update_location
xs = 8.927491 486.223621 5.882650 0.934260 -402.581990 -250.230757 -210.849135 -5.143845 -425.120581 20.134486 
ys = -477.390647 7.291448 375.684448 -197.290817 -9.642386 0.794442 -1.381692 371.628436 25.881276 -912.232496 
xs mean: -77.18237997052123
ys mean: -81.66579882624052
dists_uav = 487.833506 496.454001 388.810250 221.188923 414.927986 269.473678 233.365093 384.882000 437.489827 917.918038 
uav_gains = -123.993866 -124.202357 -120.976289 -109.072474 -121.927007 -113.022501 -109.955766 -120.818155 -122.639256 -131.045490 
uav_gains_db_mean: -119.76531607543205
dists_bs = 677.822158 682.160417 270.171762 411.768523 293.063769 189.755746 179.987949 259.834284 291.198727 1104.604892 
bs_gains = -118.838300 -118.915882 -107.652978 -112.777347 -108.641999 -103.356517 -102.713875 -107.178560 -108.564364 -124.776854 
bs_gains_db_mean: -111.34166768268187
Round 74
-------------------------------
ene_coms = [0.05036091 0.05214632 0.00920695 0.01005233 0.00978083 0.01339328
 0.01065632 0.00895393 0.00973333 0.057508  ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.23207557 2.43733105 1.09699017 0.4000458  2.67004483 1.29892584
 0.49675037 1.571143   1.15030192 1.18346351]
obj_prev = 13.537072047532698
eta_min = 4.1680509536471436e-80	eta_max = 0.9377288939674
af = 2.7674880653995517	bf = 0.6569477982014961	zeta = 3.044236871939507	eta = 0.9090909090909091
af = 2.7674880653995517	bf = 0.6569477982014961	zeta = 10.127893416531554	eta = 0.2732540669200011
af = 2.7674880653995517	bf = 0.6569477982014961	zeta = 5.844349850635345	eta = 0.4735322381665252
af = 2.7674880653995517	bf = 0.6569477982014961	zeta = 5.177416306012859	eta = 0.5345307199240467
af = 2.7674880653995517	bf = 0.6569477982014961	zeta = 5.135480558053928	eta = 0.5388956367597041
af = 2.7674880653995517	bf = 0.6569477982014961	zeta = 5.135282609555891	eta = 0.5389164094396139
eta = 0.5389164094396139
ene_coms = [0.05036091 0.05214632 0.00920695 0.01005233 0.00978083 0.01339328
 0.01065632 0.00895393 0.00973333 0.057508  ]
ene_comp = [0.05101195 0.10728705 0.05020224 0.01740884 0.12388617 0.05910909
 0.02186227 0.07246936 0.05263138 0.0477731 ]
ene_total = [0.62312344 0.98001246 0.36517917 0.1687996  0.82163056 0.44566095
 0.19988678 0.5004965  0.38334635 0.64714679]
ti_comp = [3.76374228 3.74588816 4.17528188 4.16682806 4.16954309 4.13341855
 4.16078811 4.17781205 4.170018   3.69227133]
ti_coms = [0.50360907 0.52146319 0.09206947 0.10052329 0.09780826 0.1339328
 0.10656323 0.0895393  0.09733334 0.57508001]
t_total = [26.23075485 26.23075485 26.23075485 26.23075485 26.23075485 26.23075485
 26.23075485 26.23075485 26.23075485 26.23075485]
ene_coms = [0.05036091 0.05214632 0.00920695 0.01005233 0.00978083 0.01339328
 0.01065632 0.00895393 0.00973333 0.057508  ]
ene_comp = [5.85674247e-07 5.50062777e-06 4.53604864e-07 1.89923391e-08
 6.83552059e-06 7.55481873e-07 3.77237040e-08 1.36284211e-06
 5.24008637e-07 4.99853864e-07]
ene_total = [0.30956438 0.32056924 0.05659648 0.06179024 0.06016326 0.08233108
 0.06550302 0.05504681 0.05983254 0.35349594]
optimize_network iter = 0 obj = 1.4248929842370612
eta = 0.5389164094396139
freqs = [ 6776759.63609422 14320641.80424647  6011838.41425183  2088980.03895294
 14856084.98494919  7150145.61960219  2627178.45885226  8673123.84089148
  6310689.85793033  6469337.57435814]
eta_min = 0.5389164094396147	eta_max = 0.7649322914835356
af = 7.598759239011215e-05	bf = 0.6569477982014961	zeta = 8.358635162912337e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.05036091 0.05214632 0.00920695 0.01005233 0.00978083 0.01339328
 0.01065632 0.00895393 0.00973333 0.057508  ]
ene_comp = [9.02963231e-08 8.48059249e-07 6.99345268e-08 2.92814375e-09
 1.05386634e-06 1.16476413e-07 5.81605180e-09 2.10116174e-07
 8.07890281e-08 7.70649660e-08]
ene_total = [1.57007015 1.62575634 0.28704082 0.31339462 0.30496294 0.41755671
 0.33222504 0.27915706 0.30345199 1.79288979]
ti_comp = [1.67195416 1.65410004 2.08349376 2.07503994 2.07775496 2.04163042
 2.06899999 2.08602393 2.07822988 1.60048321]
ti_coms = [0.50360907 0.52146319 0.09206947 0.10052329 0.09780826 0.1339328
 0.10656323 0.0895393  0.09733334 0.57508001]
t_total = [26.23075485 26.23075485 26.23075485 26.23075485 26.23075485 26.23075485
 26.23075485 26.23075485 26.23075485 26.23075485]
ene_coms = [0.05036091 0.05214632 0.00920695 0.01005233 0.00978083 0.01339328
 0.01065632 0.00895393 0.00973333 0.057508  ]
ene_comp = [2.41723957e-07 2.29758121e-06 1.48367042e-07 6.23748104e-09
 2.24198351e-06 2.52209303e-07 1.24255921e-08 4.45222715e-07
 1.71830272e-07 2.16670890e-07]
ene_total = [0.60720411 0.6287556  0.1110099  0.12120095 0.1179544  0.16148575
 0.12848339 0.10796285 0.11735683 0.69337629]
optimize_network iter = 1 obj = 2.7947900827639236
eta = 0.7649322914835356
freqs = [ 6612643.47908461 14057659.88199669  5222259.04289873  1818322.54995853
 12922782.26241294  6274868.8913931   2290140.88551946  7529439.95965174
  5488816.3066339   6469337.57435813]
eta_min = 0.7649322914835367	eta_max = 0.7649322914835359
af = 6.382232620236144e-05	bf = 0.6569477982014961	zeta = 7.020455882259759e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.05036091 0.05214632 0.00920695 0.01005233 0.00978083 0.01339328
 0.01065632 0.00895393 0.00973333 0.057508  ]
ene_comp = [8.59757789e-08 8.17197998e-07 5.27708225e-08 2.21853182e-09
 7.97423146e-07 8.97051806e-08 4.41950386e-09 1.58355713e-07
 6.11161661e-08 7.70649660e-08]
ene_total = [1.57007001 1.62575538 0.28704028 0.3133946  0.30495494 0.41755587
 0.332225   0.27915544 0.30345137 1.79288979]
ti_comp = [1.67195416 1.65410004 2.08349376 2.07503994 2.07775496 2.04163042
 2.06899999 2.08602393 2.07822988 1.60048321]
ti_coms = [0.50360907 0.52146319 0.09206947 0.10052329 0.09780826 0.1339328
 0.10656323 0.0895393  0.09733334 0.57508001]
t_total = [26.23075485 26.23075485 26.23075485 26.23075485 26.23075485 26.23075485
 26.23075485 26.23075485 26.23075485 26.23075485]
ene_coms = [0.05036091 0.05214632 0.00920695 0.01005233 0.00978083 0.01339328
 0.01065632 0.00895393 0.00973333 0.057508  ]
ene_comp = [2.41723957e-07 2.29758121e-06 1.48367042e-07 6.23748104e-09
 2.24198351e-06 2.52209303e-07 1.24255921e-08 4.45222715e-07
 1.71830272e-07 2.16670890e-07]
ene_total = [0.60720411 0.6287556  0.1110099  0.12120095 0.1179544  0.16148575
 0.12848339 0.10796285 0.11735683 0.69337629]
optimize_network iter = 2 obj = 2.7947900827639276
eta = 0.7649322914835359
freqs = [ 6612643.47908461 14057659.88199669  5222259.04289873  1818322.54995852
 12922782.26241294  6274868.89139309  2290140.88551946  7529439.95965173
  5488816.30663389  6469337.57435813]
Done!
ene_coms = [0.05036091 0.05214632 0.00920695 0.01005233 0.00978083 0.01339328
 0.01065632 0.00895393 0.00973333 0.057508  ]
ene_comp = [2.20384351e-07 2.09474869e-06 1.35269068e-07 5.68683066e-09
 2.04405921e-06 2.29944041e-07 1.13286498e-08 4.05918057e-07
 1.56660943e-07 1.97542991e-07]
ene_total = [0.05036113 0.05214841 0.00920708 0.01005233 0.00978287 0.01339351
 0.01065633 0.00895434 0.00973349 0.0575082 ]
At round 74 energy consumption: 0.23179769631924046
At round 74 eta: 0.7649322914835359
At round 74 a_n: 2.834210180524895
At round 74 local rounds: 8.774632350616828
At round 74 global rounds: 12.056994975668417
gradient difference: 0.5571250319480896
train() client id: f_00000-0-0 loss: 1.289787  [   32/  126]
train() client id: f_00000-0-1 loss: 0.977450  [   64/  126]
train() client id: f_00000-0-2 loss: 1.162920  [   96/  126]
train() client id: f_00000-1-0 loss: 1.094773  [   32/  126]
train() client id: f_00000-1-1 loss: 1.155218  [   64/  126]
train() client id: f_00000-1-2 loss: 1.062449  [   96/  126]
train() client id: f_00000-2-0 loss: 1.021878  [   32/  126]
train() client id: f_00000-2-1 loss: 1.096246  [   64/  126]
train() client id: f_00000-2-2 loss: 0.937692  [   96/  126]
train() client id: f_00000-3-0 loss: 0.990113  [   32/  126]
train() client id: f_00000-3-1 loss: 1.087813  [   64/  126]
train() client id: f_00000-3-2 loss: 0.914002  [   96/  126]
train() client id: f_00000-4-0 loss: 0.965408  [   32/  126]
train() client id: f_00000-4-1 loss: 0.993028  [   64/  126]
train() client id: f_00000-4-2 loss: 0.805451  [   96/  126]
train() client id: f_00000-5-0 loss: 1.011986  [   32/  126]
train() client id: f_00000-5-1 loss: 0.842079  [   64/  126]
train() client id: f_00000-5-2 loss: 0.926773  [   96/  126]
train() client id: f_00000-6-0 loss: 0.984062  [   32/  126]
train() client id: f_00000-6-1 loss: 1.011252  [   64/  126]
train() client id: f_00000-6-2 loss: 0.857789  [   96/  126]
train() client id: f_00000-7-0 loss: 0.855109  [   32/  126]
train() client id: f_00000-7-1 loss: 0.934945  [   64/  126]
train() client id: f_00000-7-2 loss: 0.940337  [   96/  126]
train() client id: f_00001-0-0 loss: 0.605454  [   32/  265]
train() client id: f_00001-0-1 loss: 0.507754  [   64/  265]
train() client id: f_00001-0-2 loss: 0.406933  [   96/  265]
train() client id: f_00001-0-3 loss: 0.393641  [  128/  265]
train() client id: f_00001-0-4 loss: 0.475911  [  160/  265]
train() client id: f_00001-0-5 loss: 0.465479  [  192/  265]
train() client id: f_00001-0-6 loss: 0.451848  [  224/  265]
train() client id: f_00001-0-7 loss: 0.683645  [  256/  265]
train() client id: f_00001-1-0 loss: 0.485149  [   32/  265]
train() client id: f_00001-1-1 loss: 0.373116  [   64/  265]
train() client id: f_00001-1-2 loss: 0.488326  [   96/  265]
train() client id: f_00001-1-3 loss: 0.531676  [  128/  265]
train() client id: f_00001-1-4 loss: 0.511782  [  160/  265]
train() client id: f_00001-1-5 loss: 0.477465  [  192/  265]
train() client id: f_00001-1-6 loss: 0.584443  [  224/  265]
train() client id: f_00001-1-7 loss: 0.415900  [  256/  265]
train() client id: f_00001-2-0 loss: 0.453461  [   32/  265]
train() client id: f_00001-2-1 loss: 0.456170  [   64/  265]
train() client id: f_00001-2-2 loss: 0.546192  [   96/  265]
train() client id: f_00001-2-3 loss: 0.446044  [  128/  265]
train() client id: f_00001-2-4 loss: 0.412845  [  160/  265]
train() client id: f_00001-2-5 loss: 0.566619  [  192/  265]
train() client id: f_00001-2-6 loss: 0.545175  [  224/  265]
train() client id: f_00001-2-7 loss: 0.499320  [  256/  265]
train() client id: f_00001-3-0 loss: 0.497655  [   32/  265]
train() client id: f_00001-3-1 loss: 0.437015  [   64/  265]
train() client id: f_00001-3-2 loss: 0.606430  [   96/  265]
train() client id: f_00001-3-3 loss: 0.445315  [  128/  265]
train() client id: f_00001-3-4 loss: 0.499225  [  160/  265]
train() client id: f_00001-3-5 loss: 0.400237  [  192/  265]
train() client id: f_00001-3-6 loss: 0.407056  [  224/  265]
train() client id: f_00001-3-7 loss: 0.534895  [  256/  265]
train() client id: f_00001-4-0 loss: 0.462608  [   32/  265]
train() client id: f_00001-4-1 loss: 0.537803  [   64/  265]
train() client id: f_00001-4-2 loss: 0.702303  [   96/  265]
train() client id: f_00001-4-3 loss: 0.416717  [  128/  265]
train() client id: f_00001-4-4 loss: 0.390954  [  160/  265]
train() client id: f_00001-4-5 loss: 0.555416  [  192/  265]
train() client id: f_00001-4-6 loss: 0.430787  [  224/  265]
train() client id: f_00001-4-7 loss: 0.408826  [  256/  265]
train() client id: f_00001-5-0 loss: 0.513725  [   32/  265]
train() client id: f_00001-5-1 loss: 0.497064  [   64/  265]
train() client id: f_00001-5-2 loss: 0.518309  [   96/  265]
train() client id: f_00001-5-3 loss: 0.498650  [  128/  265]
train() client id: f_00001-5-4 loss: 0.392944  [  160/  265]
train() client id: f_00001-5-5 loss: 0.600017  [  192/  265]
train() client id: f_00001-5-6 loss: 0.420110  [  224/  265]
train() client id: f_00001-5-7 loss: 0.413424  [  256/  265]
train() client id: f_00001-6-0 loss: 0.445925  [   32/  265]
train() client id: f_00001-6-1 loss: 0.462622  [   64/  265]
train() client id: f_00001-6-2 loss: 0.524768  [   96/  265]
train() client id: f_00001-6-3 loss: 0.462136  [  128/  265]
train() client id: f_00001-6-4 loss: 0.520084  [  160/  265]
train() client id: f_00001-6-5 loss: 0.593791  [  192/  265]
train() client id: f_00001-6-6 loss: 0.470342  [  224/  265]
train() client id: f_00001-6-7 loss: 0.399741  [  256/  265]
train() client id: f_00001-7-0 loss: 0.478729  [   32/  265]
train() client id: f_00001-7-1 loss: 0.375138  [   64/  265]
train() client id: f_00001-7-2 loss: 0.467297  [   96/  265]
train() client id: f_00001-7-3 loss: 0.556157  [  128/  265]
train() client id: f_00001-7-4 loss: 0.429644  [  160/  265]
train() client id: f_00001-7-5 loss: 0.620371  [  192/  265]
train() client id: f_00001-7-6 loss: 0.492078  [  224/  265]
train() client id: f_00001-7-7 loss: 0.375902  [  256/  265]
train() client id: f_00002-0-0 loss: 1.467033  [   32/  124]
train() client id: f_00002-0-1 loss: 1.170732  [   64/  124]
train() client id: f_00002-0-2 loss: 1.187888  [   96/  124]
train() client id: f_00002-1-0 loss: 1.312489  [   32/  124]
train() client id: f_00002-1-1 loss: 1.052862  [   64/  124]
train() client id: f_00002-1-2 loss: 1.340570  [   96/  124]
train() client id: f_00002-2-0 loss: 1.257624  [   32/  124]
train() client id: f_00002-2-1 loss: 1.292582  [   64/  124]
train() client id: f_00002-2-2 loss: 1.176446  [   96/  124]
train() client id: f_00002-3-0 loss: 1.085697  [   32/  124]
train() client id: f_00002-3-1 loss: 1.370560  [   64/  124]
train() client id: f_00002-3-2 loss: 1.173521  [   96/  124]
train() client id: f_00002-4-0 loss: 1.246016  [   32/  124]
train() client id: f_00002-4-1 loss: 1.148603  [   64/  124]
train() client id: f_00002-4-2 loss: 1.217910  [   96/  124]
train() client id: f_00002-5-0 loss: 0.984953  [   32/  124]
train() client id: f_00002-5-1 loss: 1.138796  [   64/  124]
train() client id: f_00002-5-2 loss: 1.260725  [   96/  124]
train() client id: f_00002-6-0 loss: 1.042866  [   32/  124]
train() client id: f_00002-6-1 loss: 1.017499  [   64/  124]
train() client id: f_00002-6-2 loss: 1.072280  [   96/  124]
train() client id: f_00002-7-0 loss: 0.967527  [   32/  124]
train() client id: f_00002-7-1 loss: 1.164217  [   64/  124]
train() client id: f_00002-7-2 loss: 1.162871  [   96/  124]
train() client id: f_00003-0-0 loss: 0.788382  [   32/   43]
train() client id: f_00003-1-0 loss: 0.511352  [   32/   43]
train() client id: f_00003-2-0 loss: 0.544894  [   32/   43]
train() client id: f_00003-3-0 loss: 0.712512  [   32/   43]
train() client id: f_00003-4-0 loss: 0.721538  [   32/   43]
train() client id: f_00003-5-0 loss: 0.785323  [   32/   43]
train() client id: f_00003-6-0 loss: 0.760487  [   32/   43]
train() client id: f_00003-7-0 loss: 0.754889  [   32/   43]
train() client id: f_00004-0-0 loss: 0.766622  [   32/  306]
train() client id: f_00004-0-1 loss: 0.675183  [   64/  306]
train() client id: f_00004-0-2 loss: 0.836364  [   96/  306]
train() client id: f_00004-0-3 loss: 0.757863  [  128/  306]
train() client id: f_00004-0-4 loss: 0.837719  [  160/  306]
train() client id: f_00004-0-5 loss: 0.754987  [  192/  306]
train() client id: f_00004-0-6 loss: 0.688454  [  224/  306]
train() client id: f_00004-0-7 loss: 0.715827  [  256/  306]
train() client id: f_00004-0-8 loss: 0.765870  [  288/  306]
train() client id: f_00004-1-0 loss: 0.866538  [   32/  306]
train() client id: f_00004-1-1 loss: 0.807389  [   64/  306]
train() client id: f_00004-1-2 loss: 0.739923  [   96/  306]
train() client id: f_00004-1-3 loss: 0.763141  [  128/  306]
train() client id: f_00004-1-4 loss: 0.783543  [  160/  306]
train() client id: f_00004-1-5 loss: 0.704845  [  192/  306]
train() client id: f_00004-1-6 loss: 0.644618  [  224/  306]
train() client id: f_00004-1-7 loss: 0.766813  [  256/  306]
train() client id: f_00004-1-8 loss: 0.748004  [  288/  306]
train() client id: f_00004-2-0 loss: 0.763736  [   32/  306]
train() client id: f_00004-2-1 loss: 0.755635  [   64/  306]
train() client id: f_00004-2-2 loss: 0.592586  [   96/  306]
train() client id: f_00004-2-3 loss: 0.802897  [  128/  306]
train() client id: f_00004-2-4 loss: 0.802531  [  160/  306]
train() client id: f_00004-2-5 loss: 0.763179  [  192/  306]
train() client id: f_00004-2-6 loss: 0.692253  [  224/  306]
train() client id: f_00004-2-7 loss: 0.836738  [  256/  306]
train() client id: f_00004-2-8 loss: 0.677217  [  288/  306]
train() client id: f_00004-3-0 loss: 0.754623  [   32/  306]
train() client id: f_00004-3-1 loss: 0.653912  [   64/  306]
train() client id: f_00004-3-2 loss: 0.752938  [   96/  306]
train() client id: f_00004-3-3 loss: 0.787044  [  128/  306]
train() client id: f_00004-3-4 loss: 0.814970  [  160/  306]
train() client id: f_00004-3-5 loss: 0.733856  [  192/  306]
train() client id: f_00004-3-6 loss: 0.732444  [  224/  306]
train() client id: f_00004-3-7 loss: 0.764522  [  256/  306]
train() client id: f_00004-3-8 loss: 0.755417  [  288/  306]
train() client id: f_00004-4-0 loss: 0.752934  [   32/  306]
train() client id: f_00004-4-1 loss: 0.753156  [   64/  306]
train() client id: f_00004-4-2 loss: 0.888589  [   96/  306]
train() client id: f_00004-4-3 loss: 0.831187  [  128/  306]
train() client id: f_00004-4-4 loss: 0.815991  [  160/  306]
train() client id: f_00004-4-5 loss: 0.813168  [  192/  306]
train() client id: f_00004-4-6 loss: 0.558047  [  224/  306]
train() client id: f_00004-4-7 loss: 0.774074  [  256/  306]
train() client id: f_00004-4-8 loss: 0.615509  [  288/  306]
train() client id: f_00004-5-0 loss: 0.804887  [   32/  306]
train() client id: f_00004-5-1 loss: 0.689862  [   64/  306]
train() client id: f_00004-5-2 loss: 0.811526  [   96/  306]
train() client id: f_00004-5-3 loss: 0.778618  [  128/  306]
train() client id: f_00004-5-4 loss: 0.778187  [  160/  306]
train() client id: f_00004-5-5 loss: 0.766276  [  192/  306]
train() client id: f_00004-5-6 loss: 0.664342  [  224/  306]
train() client id: f_00004-5-7 loss: 0.801367  [  256/  306]
train() client id: f_00004-5-8 loss: 0.795931  [  288/  306]
train() client id: f_00004-6-0 loss: 0.604591  [   32/  306]
train() client id: f_00004-6-1 loss: 0.785443  [   64/  306]
train() client id: f_00004-6-2 loss: 0.743318  [   96/  306]
train() client id: f_00004-6-3 loss: 0.726915  [  128/  306]
train() client id: f_00004-6-4 loss: 0.766352  [  160/  306]
train() client id: f_00004-6-5 loss: 0.793545  [  192/  306]
train() client id: f_00004-6-6 loss: 0.867400  [  224/  306]
train() client id: f_00004-6-7 loss: 0.737226  [  256/  306]
train() client id: f_00004-6-8 loss: 0.724491  [  288/  306]
train() client id: f_00004-7-0 loss: 0.701609  [   32/  306]
train() client id: f_00004-7-1 loss: 0.676008  [   64/  306]
train() client id: f_00004-7-2 loss: 0.900048  [   96/  306]
train() client id: f_00004-7-3 loss: 0.779888  [  128/  306]
train() client id: f_00004-7-4 loss: 0.763025  [  160/  306]
train() client id: f_00004-7-5 loss: 0.901339  [  192/  306]
train() client id: f_00004-7-6 loss: 0.795537  [  224/  306]
train() client id: f_00004-7-7 loss: 0.641517  [  256/  306]
train() client id: f_00004-7-8 loss: 0.695901  [  288/  306]
train() client id: f_00005-0-0 loss: 0.790343  [   32/  146]
train() client id: f_00005-0-1 loss: 0.640109  [   64/  146]
train() client id: f_00005-0-2 loss: 0.445124  [   96/  146]
train() client id: f_00005-0-3 loss: 0.865468  [  128/  146]
train() client id: f_00005-1-0 loss: 0.526197  [   32/  146]
train() client id: f_00005-1-1 loss: 0.737057  [   64/  146]
train() client id: f_00005-1-2 loss: 0.765933  [   96/  146]
train() client id: f_00005-1-3 loss: 0.566508  [  128/  146]
train() client id: f_00005-2-0 loss: 0.564649  [   32/  146]
train() client id: f_00005-2-1 loss: 0.852769  [   64/  146]
train() client id: f_00005-2-2 loss: 0.424898  [   96/  146]
train() client id: f_00005-2-3 loss: 0.568032  [  128/  146]
train() client id: f_00005-3-0 loss: 0.672379  [   32/  146]
train() client id: f_00005-3-1 loss: 0.478682  [   64/  146]
train() client id: f_00005-3-2 loss: 0.651471  [   96/  146]
train() client id: f_00005-3-3 loss: 0.812679  [  128/  146]
train() client id: f_00005-4-0 loss: 0.834330  [   32/  146]
train() client id: f_00005-4-1 loss: 0.468814  [   64/  146]
train() client id: f_00005-4-2 loss: 0.575292  [   96/  146]
train() client id: f_00005-4-3 loss: 0.812227  [  128/  146]
train() client id: f_00005-5-0 loss: 0.420193  [   32/  146]
train() client id: f_00005-5-1 loss: 0.739992  [   64/  146]
train() client id: f_00005-5-2 loss: 0.721148  [   96/  146]
train() client id: f_00005-5-3 loss: 0.806914  [  128/  146]
train() client id: f_00005-6-0 loss: 0.584577  [   32/  146]
train() client id: f_00005-6-1 loss: 0.730934  [   64/  146]
train() client id: f_00005-6-2 loss: 0.901501  [   96/  146]
train() client id: f_00005-6-3 loss: 0.504398  [  128/  146]
train() client id: f_00005-7-0 loss: 0.691600  [   32/  146]
train() client id: f_00005-7-1 loss: 0.601659  [   64/  146]
train() client id: f_00005-7-2 loss: 0.760416  [   96/  146]
train() client id: f_00005-7-3 loss: 0.771369  [  128/  146]
train() client id: f_00006-0-0 loss: 0.524843  [   32/   54]
train() client id: f_00006-1-0 loss: 0.557823  [   32/   54]
train() client id: f_00006-2-0 loss: 0.582445  [   32/   54]
train() client id: f_00006-3-0 loss: 0.549897  [   32/   54]
train() client id: f_00006-4-0 loss: 0.581158  [   32/   54]
train() client id: f_00006-5-0 loss: 0.580493  [   32/   54]
train() client id: f_00006-6-0 loss: 0.519136  [   32/   54]
train() client id: f_00006-7-0 loss: 0.533507  [   32/   54]
train() client id: f_00007-0-0 loss: 0.633037  [   32/  179]
train() client id: f_00007-0-1 loss: 0.608926  [   64/  179]
train() client id: f_00007-0-2 loss: 0.753899  [   96/  179]
train() client id: f_00007-0-3 loss: 0.707661  [  128/  179]
train() client id: f_00007-0-4 loss: 0.763981  [  160/  179]
train() client id: f_00007-1-0 loss: 0.532148  [   32/  179]
train() client id: f_00007-1-1 loss: 0.814320  [   64/  179]
train() client id: f_00007-1-2 loss: 0.808331  [   96/  179]
train() client id: f_00007-1-3 loss: 0.481839  [  128/  179]
train() client id: f_00007-1-4 loss: 0.606420  [  160/  179]
train() client id: f_00007-2-0 loss: 0.571802  [   32/  179]
train() client id: f_00007-2-1 loss: 0.785474  [   64/  179]
train() client id: f_00007-2-2 loss: 0.471934  [   96/  179]
train() client id: f_00007-2-3 loss: 0.657270  [  128/  179]
train() client id: f_00007-2-4 loss: 0.570983  [  160/  179]
train() client id: f_00007-3-0 loss: 0.727096  [   32/  179]
train() client id: f_00007-3-1 loss: 0.586690  [   64/  179]
train() client id: f_00007-3-2 loss: 0.776037  [   96/  179]
train() client id: f_00007-3-3 loss: 0.559079  [  128/  179]
train() client id: f_00007-3-4 loss: 0.646086  [  160/  179]
train() client id: f_00007-4-0 loss: 0.681767  [   32/  179]
train() client id: f_00007-4-1 loss: 0.839475  [   64/  179]
train() client id: f_00007-4-2 loss: 0.688687  [   96/  179]
train() client id: f_00007-4-3 loss: 0.472163  [  128/  179]
train() client id: f_00007-4-4 loss: 0.651313  [  160/  179]
train() client id: f_00007-5-0 loss: 0.669835  [   32/  179]
train() client id: f_00007-5-1 loss: 0.580475  [   64/  179]
train() client id: f_00007-5-2 loss: 0.521305  [   96/  179]
train() client id: f_00007-5-3 loss: 0.915388  [  128/  179]
train() client id: f_00007-5-4 loss: 0.487370  [  160/  179]
train() client id: f_00007-6-0 loss: 0.539181  [   32/  179]
train() client id: f_00007-6-1 loss: 0.703719  [   64/  179]
train() client id: f_00007-6-2 loss: 0.484487  [   96/  179]
train() client id: f_00007-6-3 loss: 0.505767  [  128/  179]
train() client id: f_00007-6-4 loss: 0.742497  [  160/  179]
train() client id: f_00007-7-0 loss: 0.670525  [   32/  179]
train() client id: f_00007-7-1 loss: 0.520966  [   64/  179]
train() client id: f_00007-7-2 loss: 0.676461  [   96/  179]
train() client id: f_00007-7-3 loss: 0.961302  [  128/  179]
train() client id: f_00007-7-4 loss: 0.467180  [  160/  179]
train() client id: f_00008-0-0 loss: 0.849907  [   32/  130]
train() client id: f_00008-0-1 loss: 0.733452  [   64/  130]
train() client id: f_00008-0-2 loss: 0.916116  [   96/  130]
train() client id: f_00008-0-3 loss: 0.721327  [  128/  130]
train() client id: f_00008-1-0 loss: 0.809355  [   32/  130]
train() client id: f_00008-1-1 loss: 0.843167  [   64/  130]
train() client id: f_00008-1-2 loss: 0.729146  [   96/  130]
train() client id: f_00008-1-3 loss: 0.838678  [  128/  130]
train() client id: f_00008-2-0 loss: 0.855366  [   32/  130]
train() client id: f_00008-2-1 loss: 0.773482  [   64/  130]
train() client id: f_00008-2-2 loss: 0.720435  [   96/  130]
train() client id: f_00008-2-3 loss: 0.861082  [  128/  130]
train() client id: f_00008-3-0 loss: 0.825201  [   32/  130]
train() client id: f_00008-3-1 loss: 0.843309  [   64/  130]
train() client id: f_00008-3-2 loss: 0.738846  [   96/  130]
train() client id: f_00008-3-3 loss: 0.805382  [  128/  130]
train() client id: f_00008-4-0 loss: 0.819113  [   32/  130]
train() client id: f_00008-4-1 loss: 0.747090  [   64/  130]
train() client id: f_00008-4-2 loss: 0.752249  [   96/  130]
train() client id: f_00008-4-3 loss: 0.848880  [  128/  130]
train() client id: f_00008-5-0 loss: 0.887900  [   32/  130]
train() client id: f_00008-5-1 loss: 0.857438  [   64/  130]
train() client id: f_00008-5-2 loss: 0.772893  [   96/  130]
train() client id: f_00008-5-3 loss: 0.699539  [  128/  130]
train() client id: f_00008-6-0 loss: 0.803242  [   32/  130]
train() client id: f_00008-6-1 loss: 0.733384  [   64/  130]
train() client id: f_00008-6-2 loss: 0.849880  [   96/  130]
train() client id: f_00008-6-3 loss: 0.824330  [  128/  130]
train() client id: f_00008-7-0 loss: 0.777956  [   32/  130]
train() client id: f_00008-7-1 loss: 0.764787  [   64/  130]
train() client id: f_00008-7-2 loss: 0.769311  [   96/  130]
train() client id: f_00008-7-3 loss: 0.884055  [  128/  130]
train() client id: f_00009-0-0 loss: 0.725481  [   32/  118]
train() client id: f_00009-0-1 loss: 0.916974  [   64/  118]
train() client id: f_00009-0-2 loss: 0.933334  [   96/  118]
train() client id: f_00009-1-0 loss: 0.959170  [   32/  118]
train() client id: f_00009-1-1 loss: 0.739548  [   64/  118]
train() client id: f_00009-1-2 loss: 0.848011  [   96/  118]
train() client id: f_00009-2-0 loss: 0.956965  [   32/  118]
train() client id: f_00009-2-1 loss: 0.801190  [   64/  118]
train() client id: f_00009-2-2 loss: 0.788088  [   96/  118]
train() client id: f_00009-3-0 loss: 0.750331  [   32/  118]
train() client id: f_00009-3-1 loss: 0.829653  [   64/  118]
train() client id: f_00009-3-2 loss: 0.878247  [   96/  118]
train() client id: f_00009-4-0 loss: 0.799204  [   32/  118]
train() client id: f_00009-4-1 loss: 0.831179  [   64/  118]
train() client id: f_00009-4-2 loss: 0.872352  [   96/  118]
train() client id: f_00009-5-0 loss: 0.682628  [   32/  118]
train() client id: f_00009-5-1 loss: 1.008354  [   64/  118]
train() client id: f_00009-5-2 loss: 0.774130  [   96/  118]
train() client id: f_00009-6-0 loss: 0.786314  [   32/  118]
train() client id: f_00009-6-1 loss: 0.941723  [   64/  118]
train() client id: f_00009-6-2 loss: 0.719915  [   96/  118]
train() client id: f_00009-7-0 loss: 0.659361  [   32/  118]
train() client id: f_00009-7-1 loss: 0.831148  [   64/  118]
train() client id: f_00009-7-2 loss: 0.761465  [   96/  118]
At round 74 accuracy: 0.6472148541114059
At round 74 training accuracy: 0.5935613682092555
At round 74 training loss: 0.8211106992350206
update_location
xs = 8.927491 491.223621 5.882650 0.934260 -407.581990 -255.230757 -215.849135 -5.143845 -430.120581 20.134486 
ys = -482.390647 7.291448 380.684448 -202.290817 -9.642386 0.794442 -1.381692 376.628436 25.881276 -917.232496 
xs mean: -78.68237997052123
ys mean: -82.16579882624052
dists_uav = 492.727548 501.351983 393.643563 225.660027 419.780959 274.122912 237.892325 389.711994 442.350036 922.887235 
uav_gains = -124.112920 -124.318383 -121.164830 -109.387785 -122.087487 -113.445903 -110.305075 -121.011957 -122.782506 -131.104344 
uav_gains_db_mean: -119.97211893659431
dists_bs = 682.635910 687.008058 273.906235 416.294637 296.963285 191.792990 181.050140 263.638274 295.504492 1109.526607 
bs_gains = -118.924355 -119.001990 -107.819913 -112.910282 -108.802736 -103.486376 -102.785427 -107.355296 -108.742854 -124.830915 
bs_gains_db_mean: -111.46601443157604
Round 75
-------------------------------
ene_coms = [0.05137049 0.05317558 0.00929926 0.01026039 0.00988056 0.01387303
 0.01091432 0.00904661 0.00984318 0.05804818]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.08570649 2.14534294 0.96463903 0.35221947 2.34759121 1.14314365
 0.43736189 1.38148615 1.01155159 1.04178831]
obj_prev = 11.910830727766989
eta_min = 5.974663683582329e-91	eta_max = 0.9446479420137616
af = 2.433006328704853	bf = 0.5873142132173877	zeta = 2.6763069615753383	eta = 0.9090909090909091
af = 2.433006328704853	bf = 0.5873142132173877	zeta = 9.011249322584382	eta = 0.2699965611435417
af = 2.433006328704853	bf = 0.5873142132173877	zeta = 5.1684160025710355	eta = 0.47074506531489546
af = 2.433006328704853	bf = 0.5873142132173877	zeta = 4.573287696219227	eta = 0.5320037772205406
af = 2.433006328704853	bf = 0.5873142132173877	zeta = 4.535925321251492	eta = 0.5363858874187925
af = 2.433006328704853	bf = 0.5873142132173877	zeta = 4.5357492283052006	eta = 0.5364067117118718
eta = 0.5364067117118718
ene_coms = [0.05137049 0.05317558 0.00929926 0.01026039 0.00988056 0.01387303
 0.01091432 0.00904661 0.00984318 0.05804818]
ene_comp = [0.05139713 0.10809714 0.0505813  0.01754029 0.1248216  0.0595554
 0.02202734 0.07301656 0.05302879 0.04813382]
ene_total = [0.55234281 0.86678882 0.32183869 0.1494197  0.72398067 0.39465416
 0.17705078 0.44106307 0.33791654 0.57069398]
ti_comp = [4.35725151 4.33920064 4.77796386 4.76835251 4.7721508  4.7322261
 4.76181327 4.7804903  4.77252462 4.29047466]
ti_coms = [0.51370493 0.5317558  0.09299258 0.10260392 0.09880564 0.13873033
 0.10914317 0.09046614 0.09843182 0.58048178]
t_total = [26.17981911 26.17981911 26.17981911 26.17981911 26.17981911 26.17981911
 26.17981911 26.17981911 26.17981911 26.17981911]
ene_coms = [0.05137049 0.05317558 0.00929926 0.01026039 0.00988056 0.01387303
 0.01091432 0.00904661 0.00984318 0.05804818]
ene_comp = [4.46962788e-07 4.19279036e-06 3.54294431e-07 1.48338420e-08
 5.33729186e-06 5.89539059e-07 2.94592406e-08 1.06463123e-06
 4.09184047e-07 3.78634628e-07]
ene_total = [0.27610222 0.28582412 0.04998242 0.05514638 0.05313353 0.07456625
 0.05866109 0.04862835 0.05290613 0.31199226]
optimize_network iter = 0 obj = 1.2669427389609533
eta = 0.5364067117118718
freqs = [ 5897884.24723004 12455881.57934062  5293185.98359827  1839240.11252458
 13078128.37720139  6292535.83782484  2312915.2926595   7636931.90414116
  5555632.4227978   5609381.72051506]
eta_min = 0.5364067117118723	eta_max = 0.7912538315436678
af = 5.127488712907882e-05	bf = 0.5873142132173877	zeta = 5.64023758419867e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.05137049 0.05317558 0.00929926 0.01026039 0.00988056 0.01387303
 0.01091432 0.00904661 0.00984318 0.05804818]
ene_comp = [6.83940611e-08 6.41579047e-07 5.42139873e-08 2.26986837e-09
 8.16710193e-07 9.02110231e-08 4.50784082e-09 1.62909430e-07
 6.26131736e-08 5.79385143e-08]
ene_total = [1.40798015 1.45747023 0.25487842 0.28122006 0.27083191 0.38023884
 0.29914308 0.24795683 0.26978668 1.59100391]
ti_comp = [1.67958285 1.66153198 2.1002952  2.09068385 2.09448214 2.05455744
 2.08414461 2.10282164 2.09485596 1.612806  ]
ti_coms = [0.51370493 0.5317558  0.09299258 0.10260392 0.09880564 0.13873033
 0.10914317 0.09046614 0.09843182 0.58048178]
t_total = [26.17981911 26.17981911 26.17981911 26.17981911 26.17981911 26.17981911
 26.17981911 26.17981911 26.17981911 26.17981911]
ene_coms = [0.05137049 0.05317558 0.00929926 0.01026039 0.00988056 0.01387303
 0.01091432 0.00904661 0.00984318 0.05804818]
ene_comp = [1.59780630e-07 1.51892234e-06 9.73912039e-08 4.09867982e-09
 1.47172380e-06 1.66125900e-07 8.16847412e-09 2.92260130e-07
 1.12807268e-07 1.42330219e-07]
ene_total = [0.61317735 0.6347397  0.11100023 0.12247154 0.11795529 0.16559517
 0.13027705 0.1079869  0.11749287 0.69288424]
optimize_network iter = 1 obj = 2.8135803328010716
eta = 0.7912538315436678
freqs = [ 5751541.1475389  12227911.48940454  4526436.82883065  1576867.52089468
 11201079.56754822  5448157.88848526  1986465.50466435  6526280.1158068
  4757779.42205269  5609381.72051506]
eta_min = 0.7912538315436682	eta_max = 0.7912538315436681
af = 4.228687319782684e-05	bf = 0.5873142132173877	zeta = 4.6515560517609525e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.05137049 0.05317558 0.00929926 0.01026039 0.00988056 0.01387303
 0.01091432 0.00904661 0.00984318 0.05804818]
ene_comp = [6.50420715e-08 6.18309336e-07 3.96451413e-08 1.66845397e-09
 5.99096177e-07 6.76250475e-08 3.32514947e-09 1.18970643e-07
 4.59205749e-08 5.79385143e-08]
ene_total = [1.40798005 1.4574696  0.25487802 0.28122004 0.27082595 0.38023822
 0.29914304 0.24795563 0.26978622 1.59100391]
ti_comp = [1.67958285 1.66153198 2.1002952  2.09068385 2.09448214 2.05455744
 2.08414461 2.10282164 2.09485596 1.612806  ]
ti_coms = [0.51370493 0.5317558  0.09299258 0.10260392 0.09880564 0.13873033
 0.10914317 0.09046614 0.09843182 0.58048178]
t_total = [26.17981911 26.17981911 26.17981911 26.17981911 26.17981911 26.17981911
 26.17981911 26.17981911 26.17981911 26.17981911]
ene_coms = [0.05137049 0.05317558 0.00929926 0.01026039 0.00988056 0.01387303
 0.01091432 0.00904661 0.00984318 0.05804818]
ene_comp = [1.59780630e-07 1.51892234e-06 9.73912039e-08 4.09867982e-09
 1.47172380e-06 1.66125900e-07 8.16847412e-09 2.92260130e-07
 1.12807268e-07 1.42330219e-07]
ene_total = [0.61317735 0.6347397  0.11100023 0.12247154 0.11795529 0.16559517
 0.13027705 0.1079869  0.11749287 0.69288424]
optimize_network iter = 2 obj = 2.8135803328010756
eta = 0.7912538315436681
freqs = [ 5751541.1475389  12227911.48940454  4526436.82883065  1576867.52089468
 11201079.56754822  5448157.88848526  1986465.50466435  6526280.1158068
  4757779.42205269  5609381.72051506]
Done!
ene_coms = [0.05137049 0.05317558 0.00929926 0.01026039 0.00988056 0.01387303
 0.01091432 0.00904661 0.00984318 0.05804818]
ene_comp = [1.45883795e-07 1.38681487e-06 8.89206558e-08 3.74219932e-09
 1.34372141e-06 1.51677189e-07 7.45802543e-09 2.66840961e-07
 1.02995916e-07 1.29951124e-07]
ene_total = [0.05137064 0.05317697 0.00929935 0.0102604  0.00988191 0.01387319
 0.01091432 0.00904688 0.00984329 0.05804831]
At round 75 energy consumption: 0.23571523863842314
At round 75 eta: 0.7912538315436681
At round 75 a_n: 2.4916643335555797
At round 75 local rounds: 7.66681735916392
At round 75 global rounds: 11.936335655793448
gradient difference: 0.7010382413864136
train() client id: f_00000-0-0 loss: 0.787735  [   32/  126]
train() client id: f_00000-0-1 loss: 1.054558  [   64/  126]
train() client id: f_00000-0-2 loss: 1.061111  [   96/  126]
train() client id: f_00000-1-0 loss: 0.794687  [   32/  126]
train() client id: f_00000-1-1 loss: 0.925661  [   64/  126]
train() client id: f_00000-1-2 loss: 0.962868  [   96/  126]
train() client id: f_00000-2-0 loss: 0.853515  [   32/  126]
train() client id: f_00000-2-1 loss: 0.944763  [   64/  126]
train() client id: f_00000-2-2 loss: 0.841216  [   96/  126]
train() client id: f_00000-3-0 loss: 0.793154  [   32/  126]
train() client id: f_00000-3-1 loss: 0.817796  [   64/  126]
train() client id: f_00000-3-2 loss: 0.845946  [   96/  126]
train() client id: f_00000-4-0 loss: 0.842147  [   32/  126]
train() client id: f_00000-4-1 loss: 0.818658  [   64/  126]
train() client id: f_00000-4-2 loss: 0.882527  [   96/  126]
train() client id: f_00000-5-0 loss: 0.794189  [   32/  126]
train() client id: f_00000-5-1 loss: 0.768759  [   64/  126]
train() client id: f_00000-5-2 loss: 0.779885  [   96/  126]
train() client id: f_00000-6-0 loss: 0.741627  [   32/  126]
train() client id: f_00000-6-1 loss: 0.744409  [   64/  126]
train() client id: f_00000-6-2 loss: 0.855113  [   96/  126]
train() client id: f_00001-0-0 loss: 0.547606  [   32/  265]
train() client id: f_00001-0-1 loss: 0.490568  [   64/  265]
train() client id: f_00001-0-2 loss: 0.547155  [   96/  265]
train() client id: f_00001-0-3 loss: 0.555430  [  128/  265]
train() client id: f_00001-0-4 loss: 0.582618  [  160/  265]
train() client id: f_00001-0-5 loss: 0.393217  [  192/  265]
train() client id: f_00001-0-6 loss: 0.527318  [  224/  265]
train() client id: f_00001-0-7 loss: 0.610900  [  256/  265]
train() client id: f_00001-1-0 loss: 0.460686  [   32/  265]
train() client id: f_00001-1-1 loss: 0.453431  [   64/  265]
train() client id: f_00001-1-2 loss: 0.630629  [   96/  265]
train() client id: f_00001-1-3 loss: 0.511387  [  128/  265]
train() client id: f_00001-1-4 loss: 0.645667  [  160/  265]
train() client id: f_00001-1-5 loss: 0.500413  [  192/  265]
train() client id: f_00001-1-6 loss: 0.514978  [  224/  265]
train() client id: f_00001-1-7 loss: 0.446927  [  256/  265]
train() client id: f_00001-2-0 loss: 0.535534  [   32/  265]
train() client id: f_00001-2-1 loss: 0.572223  [   64/  265]
train() client id: f_00001-2-2 loss: 0.553263  [   96/  265]
train() client id: f_00001-2-3 loss: 0.438711  [  128/  265]
train() client id: f_00001-2-4 loss: 0.549616  [  160/  265]
train() client id: f_00001-2-5 loss: 0.422844  [  192/  265]
train() client id: f_00001-2-6 loss: 0.535473  [  224/  265]
train() client id: f_00001-2-7 loss: 0.497096  [  256/  265]
train() client id: f_00001-3-0 loss: 0.567634  [   32/  265]
train() client id: f_00001-3-1 loss: 0.522371  [   64/  265]
train() client id: f_00001-3-2 loss: 0.411841  [   96/  265]
train() client id: f_00001-3-3 loss: 0.708686  [  128/  265]
train() client id: f_00001-3-4 loss: 0.459608  [  160/  265]
train() client id: f_00001-3-5 loss: 0.612071  [  192/  265]
train() client id: f_00001-3-6 loss: 0.416521  [  224/  265]
train() client id: f_00001-3-7 loss: 0.463665  [  256/  265]
train() client id: f_00001-4-0 loss: 0.514238  [   32/  265]
train() client id: f_00001-4-1 loss: 0.508865  [   64/  265]
train() client id: f_00001-4-2 loss: 0.497365  [   96/  265]
train() client id: f_00001-4-3 loss: 0.571049  [  128/  265]
train() client id: f_00001-4-4 loss: 0.454147  [  160/  265]
train() client id: f_00001-4-5 loss: 0.523962  [  192/  265]
train() client id: f_00001-4-6 loss: 0.429165  [  224/  265]
train() client id: f_00001-4-7 loss: 0.566945  [  256/  265]
train() client id: f_00001-5-0 loss: 0.423655  [   32/  265]
train() client id: f_00001-5-1 loss: 0.645871  [   64/  265]
train() client id: f_00001-5-2 loss: 0.489780  [   96/  265]
train() client id: f_00001-5-3 loss: 0.669793  [  128/  265]
train() client id: f_00001-5-4 loss: 0.430718  [  160/  265]
train() client id: f_00001-5-5 loss: 0.453297  [  192/  265]
train() client id: f_00001-5-6 loss: 0.538924  [  224/  265]
train() client id: f_00001-5-7 loss: 0.506402  [  256/  265]
train() client id: f_00001-6-0 loss: 0.561368  [   32/  265]
train() client id: f_00001-6-1 loss: 0.467089  [   64/  265]
train() client id: f_00001-6-2 loss: 0.501729  [   96/  265]
train() client id: f_00001-6-3 loss: 0.655694  [  128/  265]
train() client id: f_00001-6-4 loss: 0.506352  [  160/  265]
train() client id: f_00001-6-5 loss: 0.561861  [  192/  265]
train() client id: f_00001-6-6 loss: 0.463356  [  224/  265]
train() client id: f_00001-6-7 loss: 0.432874  [  256/  265]
train() client id: f_00002-0-0 loss: 1.169732  [   32/  124]
train() client id: f_00002-0-1 loss: 0.944659  [   64/  124]
train() client id: f_00002-0-2 loss: 1.091051  [   96/  124]
train() client id: f_00002-1-0 loss: 1.095093  [   32/  124]
train() client id: f_00002-1-1 loss: 0.973396  [   64/  124]
train() client id: f_00002-1-2 loss: 1.019631  [   96/  124]
train() client id: f_00002-2-0 loss: 0.936538  [   32/  124]
train() client id: f_00002-2-1 loss: 0.929853  [   64/  124]
train() client id: f_00002-2-2 loss: 0.940840  [   96/  124]
train() client id: f_00002-3-0 loss: 0.925672  [   32/  124]
train() client id: f_00002-3-1 loss: 1.136454  [   64/  124]
train() client id: f_00002-3-2 loss: 0.838803  [   96/  124]
train() client id: f_00002-4-0 loss: 0.909371  [   32/  124]
train() client id: f_00002-4-1 loss: 0.748331  [   64/  124]
train() client id: f_00002-4-2 loss: 1.142726  [   96/  124]
train() client id: f_00002-5-0 loss: 1.038664  [   32/  124]
train() client id: f_00002-5-1 loss: 0.878640  [   64/  124]
train() client id: f_00002-5-2 loss: 0.879738  [   96/  124]
train() client id: f_00002-6-0 loss: 0.824030  [   32/  124]
train() client id: f_00002-6-1 loss: 0.931579  [   64/  124]
train() client id: f_00002-6-2 loss: 0.914538  [   96/  124]
train() client id: f_00003-0-0 loss: 0.485420  [   32/   43]
train() client id: f_00003-1-0 loss: 0.426253  [   32/   43]
train() client id: f_00003-2-0 loss: 0.380326  [   32/   43]
train() client id: f_00003-3-0 loss: 0.597474  [   32/   43]
train() client id: f_00003-4-0 loss: 0.495609  [   32/   43]
train() client id: f_00003-5-0 loss: 0.457316  [   32/   43]
train() client id: f_00003-6-0 loss: 0.356728  [   32/   43]
train() client id: f_00004-0-0 loss: 0.793872  [   32/  306]
train() client id: f_00004-0-1 loss: 0.951320  [   64/  306]
train() client id: f_00004-0-2 loss: 0.903375  [   96/  306]
train() client id: f_00004-0-3 loss: 0.791784  [  128/  306]
train() client id: f_00004-0-4 loss: 0.804283  [  160/  306]
train() client id: f_00004-0-5 loss: 0.790991  [  192/  306]
train() client id: f_00004-0-6 loss: 1.064338  [  224/  306]
train() client id: f_00004-0-7 loss: 0.728286  [  256/  306]
train() client id: f_00004-0-8 loss: 0.837292  [  288/  306]
train() client id: f_00004-1-0 loss: 0.978174  [   32/  306]
train() client id: f_00004-1-1 loss: 0.922912  [   64/  306]
train() client id: f_00004-1-2 loss: 0.701571  [   96/  306]
train() client id: f_00004-1-3 loss: 0.907475  [  128/  306]
train() client id: f_00004-1-4 loss: 0.951217  [  160/  306]
train() client id: f_00004-1-5 loss: 0.789531  [  192/  306]
train() client id: f_00004-1-6 loss: 0.900958  [  224/  306]
train() client id: f_00004-1-7 loss: 0.705582  [  256/  306]
train() client id: f_00004-1-8 loss: 0.712860  [  288/  306]
train() client id: f_00004-2-0 loss: 0.769447  [   32/  306]
train() client id: f_00004-2-1 loss: 0.825191  [   64/  306]
train() client id: f_00004-2-2 loss: 0.776471  [   96/  306]
train() client id: f_00004-2-3 loss: 0.975836  [  128/  306]
train() client id: f_00004-2-4 loss: 0.772278  [  160/  306]
train() client id: f_00004-2-5 loss: 0.741423  [  192/  306]
train() client id: f_00004-2-6 loss: 0.837041  [  224/  306]
train() client id: f_00004-2-7 loss: 1.010005  [  256/  306]
train() client id: f_00004-2-8 loss: 0.878045  [  288/  306]
train() client id: f_00004-3-0 loss: 1.033834  [   32/  306]
train() client id: f_00004-3-1 loss: 0.712086  [   64/  306]
train() client id: f_00004-3-2 loss: 1.001850  [   96/  306]
train() client id: f_00004-3-3 loss: 0.807481  [  128/  306]
train() client id: f_00004-3-4 loss: 0.872304  [  160/  306]
train() client id: f_00004-3-5 loss: 0.956797  [  192/  306]
train() client id: f_00004-3-6 loss: 0.864871  [  224/  306]
train() client id: f_00004-3-7 loss: 0.670933  [  256/  306]
train() client id: f_00004-3-8 loss: 0.722950  [  288/  306]
train() client id: f_00004-4-0 loss: 0.840774  [   32/  306]
train() client id: f_00004-4-1 loss: 0.791385  [   64/  306]
train() client id: f_00004-4-2 loss: 0.766025  [   96/  306]
train() client id: f_00004-4-3 loss: 1.028905  [  128/  306]
train() client id: f_00004-4-4 loss: 0.733043  [  160/  306]
train() client id: f_00004-4-5 loss: 0.823337  [  192/  306]
train() client id: f_00004-4-6 loss: 0.845962  [  224/  306]
train() client id: f_00004-4-7 loss: 0.983488  [  256/  306]
train() client id: f_00004-4-8 loss: 0.836083  [  288/  306]
train() client id: f_00004-5-0 loss: 1.011845  [   32/  306]
train() client id: f_00004-5-1 loss: 0.757057  [   64/  306]
train() client id: f_00004-5-2 loss: 0.964982  [   96/  306]
train() client id: f_00004-5-3 loss: 0.882022  [  128/  306]
train() client id: f_00004-5-4 loss: 0.867939  [  160/  306]
train() client id: f_00004-5-5 loss: 0.719619  [  192/  306]
train() client id: f_00004-5-6 loss: 0.869131  [  224/  306]
train() client id: f_00004-5-7 loss: 0.814729  [  256/  306]
train() client id: f_00004-5-8 loss: 0.768916  [  288/  306]
train() client id: f_00004-6-0 loss: 0.794809  [   32/  306]
train() client id: f_00004-6-1 loss: 0.765841  [   64/  306]
train() client id: f_00004-6-2 loss: 0.932343  [   96/  306]
train() client id: f_00004-6-3 loss: 0.737667  [  128/  306]
train() client id: f_00004-6-4 loss: 0.891540  [  160/  306]
train() client id: f_00004-6-5 loss: 0.909783  [  192/  306]
train() client id: f_00004-6-6 loss: 1.099164  [  224/  306]
train() client id: f_00004-6-7 loss: 0.761728  [  256/  306]
train() client id: f_00004-6-8 loss: 0.745741  [  288/  306]
train() client id: f_00005-0-0 loss: 0.421516  [   32/  146]
train() client id: f_00005-0-1 loss: 0.787066  [   64/  146]
train() client id: f_00005-0-2 loss: 0.380551  [   96/  146]
train() client id: f_00005-0-3 loss: 0.439132  [  128/  146]
train() client id: f_00005-1-0 loss: 0.805405  [   32/  146]
train() client id: f_00005-1-1 loss: 0.327589  [   64/  146]
train() client id: f_00005-1-2 loss: 0.641564  [   96/  146]
train() client id: f_00005-1-3 loss: 0.408163  [  128/  146]
train() client id: f_00005-2-0 loss: 0.824881  [   32/  146]
train() client id: f_00005-2-1 loss: 0.267347  [   64/  146]
train() client id: f_00005-2-2 loss: 0.363331  [   96/  146]
train() client id: f_00005-2-3 loss: 0.692002  [  128/  146]
train() client id: f_00005-3-0 loss: 0.344310  [   32/  146]
train() client id: f_00005-3-1 loss: 0.537023  [   64/  146]
train() client id: f_00005-3-2 loss: 0.714052  [   96/  146]
train() client id: f_00005-3-3 loss: 0.593745  [  128/  146]
train() client id: f_00005-4-0 loss: 0.522062  [   32/  146]
train() client id: f_00005-4-1 loss: 0.398923  [   64/  146]
train() client id: f_00005-4-2 loss: 0.377793  [   96/  146]
train() client id: f_00005-4-3 loss: 0.662896  [  128/  146]
train() client id: f_00005-5-0 loss: 0.419327  [   32/  146]
train() client id: f_00005-5-1 loss: 0.777696  [   64/  146]
train() client id: f_00005-5-2 loss: 0.482563  [   96/  146]
train() client id: f_00005-5-3 loss: 0.417464  [  128/  146]
train() client id: f_00005-6-0 loss: 0.580581  [   32/  146]
train() client id: f_00005-6-1 loss: 0.418490  [   64/  146]
train() client id: f_00005-6-2 loss: 0.411469  [   96/  146]
train() client id: f_00005-6-3 loss: 0.466592  [  128/  146]
train() client id: f_00006-0-0 loss: 0.510168  [   32/   54]
train() client id: f_00006-1-0 loss: 0.418819  [   32/   54]
train() client id: f_00006-2-0 loss: 0.529120  [   32/   54]
train() client id: f_00006-3-0 loss: 0.550109  [   32/   54]
train() client id: f_00006-4-0 loss: 0.499240  [   32/   54]
train() client id: f_00006-5-0 loss: 0.455285  [   32/   54]
train() client id: f_00006-6-0 loss: 0.532395  [   32/   54]
train() client id: f_00007-0-0 loss: 0.787131  [   32/  179]
train() client id: f_00007-0-1 loss: 1.045421  [   64/  179]
train() client id: f_00007-0-2 loss: 0.717979  [   96/  179]
train() client id: f_00007-0-3 loss: 0.576566  [  128/  179]
train() client id: f_00007-0-4 loss: 0.652074  [  160/  179]
train() client id: f_00007-1-0 loss: 0.721166  [   32/  179]
train() client id: f_00007-1-1 loss: 0.949189  [   64/  179]
train() client id: f_00007-1-2 loss: 0.574676  [   96/  179]
train() client id: f_00007-1-3 loss: 0.757384  [  128/  179]
train() client id: f_00007-1-4 loss: 0.768958  [  160/  179]
train() client id: f_00007-2-0 loss: 0.672333  [   32/  179]
train() client id: f_00007-2-1 loss: 0.846449  [   64/  179]
train() client id: f_00007-2-2 loss: 0.559189  [   96/  179]
train() client id: f_00007-2-3 loss: 0.755616  [  128/  179]
train() client id: f_00007-2-4 loss: 0.724824  [  160/  179]
train() client id: f_00007-3-0 loss: 0.771881  [   32/  179]
train() client id: f_00007-3-1 loss: 0.757055  [   64/  179]
train() client id: f_00007-3-2 loss: 0.775698  [   96/  179]
train() client id: f_00007-3-3 loss: 0.844178  [  128/  179]
train() client id: f_00007-3-4 loss: 0.544049  [  160/  179]
train() client id: f_00007-4-0 loss: 0.695323  [   32/  179]
train() client id: f_00007-4-1 loss: 0.624145  [   64/  179]
train() client id: f_00007-4-2 loss: 0.757094  [   96/  179]
train() client id: f_00007-4-3 loss: 0.682672  [  128/  179]
train() client id: f_00007-4-4 loss: 0.695805  [  160/  179]
train() client id: f_00007-5-0 loss: 0.668957  [   32/  179]
train() client id: f_00007-5-1 loss: 0.744863  [   64/  179]
train() client id: f_00007-5-2 loss: 0.744856  [   96/  179]
train() client id: f_00007-5-3 loss: 0.632460  [  128/  179]
train() client id: f_00007-5-4 loss: 0.830781  [  160/  179]
train() client id: f_00007-6-0 loss: 0.807285  [   32/  179]
train() client id: f_00007-6-1 loss: 0.770652  [   64/  179]
train() client id: f_00007-6-2 loss: 0.570962  [   96/  179]
train() client id: f_00007-6-3 loss: 0.745490  [  128/  179]
train() client id: f_00007-6-4 loss: 0.677999  [  160/  179]
train() client id: f_00008-0-0 loss: 0.595397  [   32/  130]
train() client id: f_00008-0-1 loss: 0.831907  [   64/  130]
train() client id: f_00008-0-2 loss: 0.616691  [   96/  130]
train() client id: f_00008-0-3 loss: 0.622653  [  128/  130]
train() client id: f_00008-1-0 loss: 0.555718  [   32/  130]
train() client id: f_00008-1-1 loss: 0.617468  [   64/  130]
train() client id: f_00008-1-2 loss: 0.740025  [   96/  130]
train() client id: f_00008-1-3 loss: 0.727792  [  128/  130]
train() client id: f_00008-2-0 loss: 0.742709  [   32/  130]
train() client id: f_00008-2-1 loss: 0.651428  [   64/  130]
train() client id: f_00008-2-2 loss: 0.662611  [   96/  130]
train() client id: f_00008-2-3 loss: 0.604357  [  128/  130]
train() client id: f_00008-3-0 loss: 0.540026  [   32/  130]
train() client id: f_00008-3-1 loss: 0.637419  [   64/  130]
train() client id: f_00008-3-2 loss: 0.658517  [   96/  130]
train() client id: f_00008-3-3 loss: 0.823309  [  128/  130]
train() client id: f_00008-4-0 loss: 0.640615  [   32/  130]
train() client id: f_00008-4-1 loss: 0.796371  [   64/  130]
train() client id: f_00008-4-2 loss: 0.557278  [   96/  130]
train() client id: f_00008-4-3 loss: 0.607016  [  128/  130]
train() client id: f_00008-5-0 loss: 0.669021  [   32/  130]
train() client id: f_00008-5-1 loss: 0.689661  [   64/  130]
train() client id: f_00008-5-2 loss: 0.618001  [   96/  130]
train() client id: f_00008-5-3 loss: 0.637731  [  128/  130]
train() client id: f_00008-6-0 loss: 0.583007  [   32/  130]
train() client id: f_00008-6-1 loss: 0.645602  [   64/  130]
train() client id: f_00008-6-2 loss: 0.708025  [   96/  130]
train() client id: f_00008-6-3 loss: 0.659860  [  128/  130]
train() client id: f_00009-0-0 loss: 0.803338  [   32/  118]
train() client id: f_00009-0-1 loss: 0.863800  [   64/  118]
train() client id: f_00009-0-2 loss: 0.815436  [   96/  118]
train() client id: f_00009-1-0 loss: 0.688988  [   32/  118]
train() client id: f_00009-1-1 loss: 0.917921  [   64/  118]
train() client id: f_00009-1-2 loss: 0.909322  [   96/  118]
train() client id: f_00009-2-0 loss: 0.906508  [   32/  118]
train() client id: f_00009-2-1 loss: 0.639733  [   64/  118]
train() client id: f_00009-2-2 loss: 0.854046  [   96/  118]
train() client id: f_00009-3-0 loss: 0.725683  [   32/  118]
train() client id: f_00009-3-1 loss: 0.644160  [   64/  118]
train() client id: f_00009-3-2 loss: 1.008259  [   96/  118]
train() client id: f_00009-4-0 loss: 0.679126  [   32/  118]
train() client id: f_00009-4-1 loss: 0.801858  [   64/  118]
train() client id: f_00009-4-2 loss: 0.753246  [   96/  118]
train() client id: f_00009-5-0 loss: 0.931466  [   32/  118]
train() client id: f_00009-5-1 loss: 0.848713  [   64/  118]
train() client id: f_00009-5-2 loss: 0.727546  [   96/  118]
train() client id: f_00009-6-0 loss: 0.713680  [   32/  118]
train() client id: f_00009-6-1 loss: 0.818319  [   64/  118]
train() client id: f_00009-6-2 loss: 0.878493  [   96/  118]
At round 75 accuracy: 0.6472148541114059
At round 75 training accuracy: 0.5928906773977196
At round 75 training loss: 0.8206880384247868
update_location
xs = 8.927491 496.223621 5.882650 0.934260 -412.581990 -260.230757 -220.849135 -5.143845 -435.120581 20.134486 
ys = -487.390647 7.291448 385.684448 -207.290817 -9.642386 0.794442 -1.381692 381.628436 25.881276 -922.232496 
xs mean: -80.18237997052123
ys mean: -82.66579882624052
dists_uav = 497.623696 506.251960 398.480990 230.152897 424.637344 278.784286 242.438135 394.546223 447.213328 927.856764 
uav_gains = -124.230221 -124.432787 -121.347322 -109.715009 -122.243845 -113.869824 -110.667657 -121.199341 -122.922728 -131.162880 
uav_gains_db_mean: -120.17916160357217
dists_bs = 687.452319 691.857867 277.680517 420.831478 300.895352 193.937769 182.243366 267.481633 299.831804 1114.449020 
bs_gains = -119.009851 -119.087532 -107.986331 -113.042089 -108.962693 -103.621606 -102.865308 -107.531290 -108.919635 -124.884745 
bs_gains_db_mean: -111.59110793235163
Round 76
-------------------------------
ene_coms = [0.05239113 0.0542163  0.00939306 0.01048505 0.00998175 0.01438486
 0.01119469 0.00914075 0.00995432 0.05859246]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.93866274 1.85266745 0.83222724 0.30428519 2.02507171 0.9870991
 0.37784346 1.19176832 0.87272805 0.8997482 ]
obj_prev = 10.28210146520904
eta_min = 2.974100299540942e-105	eta_max = 0.9517286837142237
af = 2.098524592010151	bf = 0.515217564868979	zeta = 2.308377051211166	eta = 0.9090909090909091
af = 2.098524592010151	bf = 0.515217564868979	zeta = 7.867511531629902	eta = 0.26673295406983694
af = 2.098524592010151	bf = 0.515217564868979	zeta = 4.484632353566693	eta = 0.4679368176838764
af = 2.098524592010151	bf = 0.515217564868979	zeta = 3.963595624817038	eta = 0.5294497195603853
af = 2.098524592010151	bf = 0.515217564868979	zeta = 3.9309405729335722	eta = 0.533847956506773
af = 2.098524592010151	bf = 0.515217564868979	zeta = 3.9307869443097645	eta = 0.5338688211143039
eta = 0.5338688211143039
ene_coms = [0.05239113 0.0542163  0.00939306 0.01048505 0.00998175 0.01438486
 0.01119469 0.00914075 0.00995432 0.05859246]
ene_comp = [0.05178847 0.1089202  0.05096643 0.01767384 0.125772   0.06000886
 0.02219506 0.07357251 0.05343255 0.04850031]
ene_total = [0.48032466 0.75214806 0.27829011 0.12982784 0.62589867 0.34299557
 0.15394494 0.38135317 0.29224796 0.49375598]
ti_comp = [5.14329008 5.12503835 5.57327077 5.56235083 5.56738391 5.52335274
 5.55525442 5.57579383 5.56765819 5.08127676]
ti_coms = [0.52391127 0.54216301 0.09393059 0.10485053 0.09981745 0.14384862
 0.11194694 0.09140753 0.09954316 0.58592459]
t_total = [26.12888336 26.12888336 26.12888336 26.12888336 26.12888336 26.12888336
 26.12888336 26.12888336 26.12888336 26.12888336]
ene_coms = [0.05239113 0.0542163  0.00939306 0.01048505 0.00998175 0.01438486
 0.01119469 0.00914075 0.00995432 0.05859246]
ene_comp = [3.28168758e-07 3.07475857e-06 2.66386619e-07 1.11520896e-08
 4.01171043e-06 4.42711339e-07 2.21432320e-08 8.00595815e-07
 3.07576084e-07 2.76164568e-07]
ene_total = [0.24155313 0.24998084 0.04330834 0.04834185 0.04603978 0.06632409
 0.05161374 0.04214754 0.04589624 0.27014441]
optimize_network iter = 0 obj = 1.105349939538375
eta = 0.5338688211143039
freqs = [ 5034566.36404027 10626281.06390148  4572398.63425761  1588702.65943885
 11295430.84752373  5432285.92358237  1997663.57378958  6597491.93650141
  4798476.04527923  4772453.45011758]
eta_min = 0.5338688211143044	eta_max = 0.8181478471673905
af = 3.267266073436558e-05	bf = 0.515217564868979	zeta = 3.593992680780214e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.05239113 0.0542163  0.00939306 0.01048505 0.00998175 0.01438486
 0.01119469 0.00914075 0.00995432 0.05859246]
ene_comp = [4.98367878e-08 4.66942956e-07 4.04543488e-08 1.69359303e-09
 6.09231550e-07 6.72316011e-08 3.36274409e-09 1.21581116e-07
 4.67095165e-08 4.19392603e-08]
ene_total = [1.23854332 1.28170084 0.22205572 0.24786987 0.23598589 0.34006409
 0.26464604 0.21609304 0.23532416 1.3851445 ]
ti_comp = [1.68703881 1.66878707 2.11701949 2.10609955 2.11113263 2.06710146
 2.09900314 2.11954255 2.11140692 1.62502549]
ti_coms = [0.52391127 0.54216301 0.09393059 0.10485053 0.09981745 0.14384862
 0.11194694 0.09140753 0.09954316 0.58592459]
t_total = [26.12888336 26.12888336 26.12888336 26.12888336 26.12888336 26.12888336
 26.12888336 26.12888336 26.12888336 26.12888336]
ene_coms = [0.05239113 0.0542163  0.00939306 0.01048505 0.00998175 0.01438486
 0.01119469 0.00914075 0.00995432 0.05859246]
ene_comp = [9.97679633e-08 9.48561263e-07 6.03872092e-08 2.54435199e-09
 9.12563008e-07 1.03386682e-07 5.07323195e-09 1.81219292e-07
 6.99545221e-08 8.83192321e-08]
ene_total = [0.61915657 0.6407364  0.11100735 0.12391179 0.11797449 0.17000071
 0.13229832 0.10802704 0.11764038 0.69244342]
optimize_network iter = 1 obj = 2.833196467486135
eta = 0.8181478471673905
freqs = [ 4908687.03154809 10436738.88630514  3849609.69506243  1341866.86997742
  9526333.45429143  4642061.95764786  1690832.32499556  5550482.80024161
  4046609.37360004  4772453.45011758]
eta_min = 0.8181478471673921	eta_max = 0.8181478471673905
af = 2.6466456366737888e-05	bf = 0.515217564868979	zeta = 2.911310200341168e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.05239113 0.0542163  0.00939306 0.01048505 0.00998175 0.01438486
 0.01119469 0.00914075 0.00995432 0.05859246]
ene_comp = [4.73758035e-08 4.50433691e-07 2.86754631e-08 1.20821069e-09
 4.33339564e-07 4.90941876e-08 2.40907433e-09 8.60537718e-08
 3.32185962e-08 4.19392603e-08]
ene_total = [1.23854326 1.28170045 0.22205544 0.24786986 0.23598173 0.34006366
 0.26464602 0.2160922  0.23532384 1.3851445 ]
ti_comp = [1.68703881 1.66878707 2.11701949 2.10609955 2.11113263 2.06710146
 2.09900314 2.11954255 2.11140692 1.62502549]
ti_coms = [0.52391127 0.54216301 0.09393059 0.10485053 0.09981745 0.14384862
 0.11194694 0.09140753 0.09954316 0.58592459]
t_total = [26.12888336 26.12888336 26.12888336 26.12888336 26.12888336 26.12888336
 26.12888336 26.12888336 26.12888336 26.12888336]
ene_coms = [0.05239113 0.0542163  0.00939306 0.01048505 0.00998175 0.01438486
 0.01119469 0.00914075 0.00995432 0.05859246]
ene_comp = [9.97679633e-08 9.48561263e-07 6.03872092e-08 2.54435199e-09
 9.12563008e-07 1.03386682e-07 5.07323195e-09 1.81219292e-07
 6.99545221e-08 8.83192321e-08]
ene_total = [0.61915657 0.6407364  0.11100735 0.12391179 0.11797449 0.17000071
 0.13229832 0.10802704 0.11764038 0.69244342]
optimize_network iter = 2 obj = 2.833196467486135
eta = 0.8181478471673905
freqs = [ 4908687.03154809 10436738.88630514  3849609.69506243  1341866.86997742
  9526333.45429143  4642061.95764786  1690832.32499556  5550482.80024161
  4046609.37360004  4772453.45011758]
Done!
ene_coms = [0.05239113 0.0542163  0.00939306 0.01048505 0.00998175 0.01438486
 0.01119469 0.00914075 0.00995432 0.05859246]
ene_comp = [9.10798877e-08 8.65957873e-07 5.51285207e-08 2.32278264e-09
 8.33094447e-07 9.43834778e-08 4.63144060e-09 1.65438205e-07
 6.38626849e-08 8.06281442e-08]
ene_total = [0.05239122 0.05421717 0.00939311 0.01048506 0.00998258 0.01438496
 0.0111947  0.00914092 0.00995438 0.05859254]
At round 76 energy consumption: 0.23973662579780003
At round 76 eta: 0.8181478471673905
At round 76 a_n: 2.1491184865862607
At round 76 local rounds: 6.572337707017178
At round 76 global rounds: 11.817943604794564
gradient difference: 0.8290277719497681
train() client id: f_00000-0-0 loss: 0.816089  [   32/  126]
train() client id: f_00000-0-1 loss: 0.859384  [   64/  126]
train() client id: f_00000-0-2 loss: 1.060003  [   96/  126]
train() client id: f_00000-1-0 loss: 0.829895  [   32/  126]
train() client id: f_00000-1-1 loss: 0.834428  [   64/  126]
train() client id: f_00000-1-2 loss: 0.932676  [   96/  126]
train() client id: f_00000-2-0 loss: 0.968084  [   32/  126]
train() client id: f_00000-2-1 loss: 0.864517  [   64/  126]
train() client id: f_00000-2-2 loss: 0.876702  [   96/  126]
train() client id: f_00000-3-0 loss: 0.714197  [   32/  126]
train() client id: f_00000-3-1 loss: 0.877011  [   64/  126]
train() client id: f_00000-3-2 loss: 1.002145  [   96/  126]
train() client id: f_00000-4-0 loss: 0.919742  [   32/  126]
train() client id: f_00000-4-1 loss: 0.775942  [   64/  126]
train() client id: f_00000-4-2 loss: 0.957281  [   96/  126]
train() client id: f_00000-5-0 loss: 0.905091  [   32/  126]
train() client id: f_00000-5-1 loss: 1.041594  [   64/  126]
train() client id: f_00000-5-2 loss: 0.758815  [   96/  126]
train() client id: f_00001-0-0 loss: 0.498432  [   32/  265]
train() client id: f_00001-0-1 loss: 0.533461  [   64/  265]
train() client id: f_00001-0-2 loss: 0.577501  [   96/  265]
train() client id: f_00001-0-3 loss: 0.479114  [  128/  265]
train() client id: f_00001-0-4 loss: 0.591676  [  160/  265]
train() client id: f_00001-0-5 loss: 0.456555  [  192/  265]
train() client id: f_00001-0-6 loss: 0.696104  [  224/  265]
train() client id: f_00001-0-7 loss: 0.571207  [  256/  265]
train() client id: f_00001-1-0 loss: 0.474234  [   32/  265]
train() client id: f_00001-1-1 loss: 0.624939  [   64/  265]
train() client id: f_00001-1-2 loss: 0.554818  [   96/  265]
train() client id: f_00001-1-3 loss: 0.472612  [  128/  265]
train() client id: f_00001-1-4 loss: 0.594547  [  160/  265]
train() client id: f_00001-1-5 loss: 0.633643  [  192/  265]
train() client id: f_00001-1-6 loss: 0.472346  [  224/  265]
train() client id: f_00001-1-7 loss: 0.554994  [  256/  265]
train() client id: f_00001-2-0 loss: 0.470825  [   32/  265]
train() client id: f_00001-2-1 loss: 0.446655  [   64/  265]
train() client id: f_00001-2-2 loss: 0.487882  [   96/  265]
train() client id: f_00001-2-3 loss: 0.740802  [  128/  265]
train() client id: f_00001-2-4 loss: 0.579586  [  160/  265]
train() client id: f_00001-2-5 loss: 0.508021  [  192/  265]
train() client id: f_00001-2-6 loss: 0.572344  [  224/  265]
train() client id: f_00001-2-7 loss: 0.497833  [  256/  265]
train() client id: f_00001-3-0 loss: 0.735658  [   32/  265]
train() client id: f_00001-3-1 loss: 0.541901  [   64/  265]
train() client id: f_00001-3-2 loss: 0.527204  [   96/  265]
train() client id: f_00001-3-3 loss: 0.488808  [  128/  265]
train() client id: f_00001-3-4 loss: 0.493321  [  160/  265]
train() client id: f_00001-3-5 loss: 0.531803  [  192/  265]
train() client id: f_00001-3-6 loss: 0.544426  [  224/  265]
train() client id: f_00001-3-7 loss: 0.521931  [  256/  265]
train() client id: f_00001-4-0 loss: 0.674525  [   32/  265]
train() client id: f_00001-4-1 loss: 0.473720  [   64/  265]
train() client id: f_00001-4-2 loss: 0.589168  [   96/  265]
train() client id: f_00001-4-3 loss: 0.445973  [  128/  265]
train() client id: f_00001-4-4 loss: 0.529906  [  160/  265]
train() client id: f_00001-4-5 loss: 0.488778  [  192/  265]
train() client id: f_00001-4-6 loss: 0.485262  [  224/  265]
train() client id: f_00001-4-7 loss: 0.612185  [  256/  265]
train() client id: f_00001-5-0 loss: 0.599983  [   32/  265]
train() client id: f_00001-5-1 loss: 0.547824  [   64/  265]
train() client id: f_00001-5-2 loss: 0.492222  [   96/  265]
train() client id: f_00001-5-3 loss: 0.475172  [  128/  265]
train() client id: f_00001-5-4 loss: 0.523378  [  160/  265]
train() client id: f_00001-5-5 loss: 0.521351  [  192/  265]
train() client id: f_00001-5-6 loss: 0.599135  [  224/  265]
train() client id: f_00001-5-7 loss: 0.620268  [  256/  265]
train() client id: f_00002-0-0 loss: 0.932536  [   32/  124]
train() client id: f_00002-0-1 loss: 0.879890  [   64/  124]
train() client id: f_00002-0-2 loss: 1.058264  [   96/  124]
train() client id: f_00002-1-0 loss: 0.905260  [   32/  124]
train() client id: f_00002-1-1 loss: 0.896824  [   64/  124]
train() client id: f_00002-1-2 loss: 0.963375  [   96/  124]
train() client id: f_00002-2-0 loss: 0.827454  [   32/  124]
train() client id: f_00002-2-1 loss: 0.871630  [   64/  124]
train() client id: f_00002-2-2 loss: 0.970451  [   96/  124]
train() client id: f_00002-3-0 loss: 0.852548  [   32/  124]
train() client id: f_00002-3-1 loss: 0.891199  [   64/  124]
train() client id: f_00002-3-2 loss: 0.989703  [   96/  124]
train() client id: f_00002-4-0 loss: 0.813411  [   32/  124]
train() client id: f_00002-4-1 loss: 0.975020  [   64/  124]
train() client id: f_00002-4-2 loss: 0.887056  [   96/  124]
train() client id: f_00002-5-0 loss: 0.796667  [   32/  124]
train() client id: f_00002-5-1 loss: 0.888852  [   64/  124]
train() client id: f_00002-5-2 loss: 0.745184  [   96/  124]
train() client id: f_00003-0-0 loss: 0.683758  [   32/   43]
train() client id: f_00003-1-0 loss: 0.796974  [   32/   43]
train() client id: f_00003-2-0 loss: 0.832117  [   32/   43]
train() client id: f_00003-3-0 loss: 0.648103  [   32/   43]
train() client id: f_00003-4-0 loss: 0.782391  [   32/   43]
train() client id: f_00003-5-0 loss: 0.701076  [   32/   43]
train() client id: f_00004-0-0 loss: 1.033657  [   32/  306]
train() client id: f_00004-0-1 loss: 0.949329  [   64/  306]
train() client id: f_00004-0-2 loss: 0.942190  [   96/  306]
train() client id: f_00004-0-3 loss: 0.944930  [  128/  306]
train() client id: f_00004-0-4 loss: 0.866269  [  160/  306]
train() client id: f_00004-0-5 loss: 0.950927  [  192/  306]
train() client id: f_00004-0-6 loss: 1.149916  [  224/  306]
train() client id: f_00004-0-7 loss: 0.877956  [  256/  306]
train() client id: f_00004-0-8 loss: 0.779118  [  288/  306]
train() client id: f_00004-1-0 loss: 1.010748  [   32/  306]
train() client id: f_00004-1-1 loss: 0.998285  [   64/  306]
train() client id: f_00004-1-2 loss: 0.886181  [   96/  306]
train() client id: f_00004-1-3 loss: 0.853270  [  128/  306]
train() client id: f_00004-1-4 loss: 0.967269  [  160/  306]
train() client id: f_00004-1-5 loss: 0.929023  [  192/  306]
train() client id: f_00004-1-6 loss: 0.881971  [  224/  306]
train() client id: f_00004-1-7 loss: 0.869796  [  256/  306]
train() client id: f_00004-1-8 loss: 1.029379  [  288/  306]
train() client id: f_00004-2-0 loss: 0.871263  [   32/  306]
train() client id: f_00004-2-1 loss: 1.113535  [   64/  306]
train() client id: f_00004-2-2 loss: 1.086255  [   96/  306]
train() client id: f_00004-2-3 loss: 1.028810  [  128/  306]
train() client id: f_00004-2-4 loss: 0.824716  [  160/  306]
train() client id: f_00004-2-5 loss: 0.927495  [  192/  306]
train() client id: f_00004-2-6 loss: 0.863195  [  224/  306]
train() client id: f_00004-2-7 loss: 0.898171  [  256/  306]
train() client id: f_00004-2-8 loss: 0.856208  [  288/  306]
train() client id: f_00004-3-0 loss: 0.930128  [   32/  306]
train() client id: f_00004-3-1 loss: 1.024377  [   64/  306]
train() client id: f_00004-3-2 loss: 1.000512  [   96/  306]
train() client id: f_00004-3-3 loss: 0.915507  [  128/  306]
train() client id: f_00004-3-4 loss: 0.831341  [  160/  306]
train() client id: f_00004-3-5 loss: 0.982814  [  192/  306]
train() client id: f_00004-3-6 loss: 0.800199  [  224/  306]
train() client id: f_00004-3-7 loss: 0.965928  [  256/  306]
train() client id: f_00004-3-8 loss: 0.798208  [  288/  306]
train() client id: f_00004-4-0 loss: 0.889024  [   32/  306]
train() client id: f_00004-4-1 loss: 0.799610  [   64/  306]
train() client id: f_00004-4-2 loss: 0.918079  [   96/  306]
train() client id: f_00004-4-3 loss: 0.880320  [  128/  306]
train() client id: f_00004-4-4 loss: 0.965401  [  160/  306]
train() client id: f_00004-4-5 loss: 0.979300  [  192/  306]
train() client id: f_00004-4-6 loss: 1.094704  [  224/  306]
train() client id: f_00004-4-7 loss: 0.998031  [  256/  306]
train() client id: f_00004-4-8 loss: 0.944132  [  288/  306]
train() client id: f_00004-5-0 loss: 0.890345  [   32/  306]
train() client id: f_00004-5-1 loss: 1.035704  [   64/  306]
train() client id: f_00004-5-2 loss: 0.900371  [   96/  306]
train() client id: f_00004-5-3 loss: 1.055660  [  128/  306]
train() client id: f_00004-5-4 loss: 0.951001  [  160/  306]
train() client id: f_00004-5-5 loss: 0.851354  [  192/  306]
train() client id: f_00004-5-6 loss: 0.796865  [  224/  306]
train() client id: f_00004-5-7 loss: 1.009057  [  256/  306]
train() client id: f_00004-5-8 loss: 0.899797  [  288/  306]
train() client id: f_00005-0-0 loss: 0.686808  [   32/  146]
train() client id: f_00005-0-1 loss: 0.533628  [   64/  146]
train() client id: f_00005-0-2 loss: 0.777257  [   96/  146]
train() client id: f_00005-0-3 loss: 0.890139  [  128/  146]
train() client id: f_00005-1-0 loss: 0.821005  [   32/  146]
train() client id: f_00005-1-1 loss: 0.811289  [   64/  146]
train() client id: f_00005-1-2 loss: 0.584225  [   96/  146]
train() client id: f_00005-1-3 loss: 0.695571  [  128/  146]
train() client id: f_00005-2-0 loss: 0.723532  [   32/  146]
train() client id: f_00005-2-1 loss: 0.865427  [   64/  146]
train() client id: f_00005-2-2 loss: 0.623327  [   96/  146]
train() client id: f_00005-2-3 loss: 0.728356  [  128/  146]
train() client id: f_00005-3-0 loss: 0.508666  [   32/  146]
train() client id: f_00005-3-1 loss: 0.739738  [   64/  146]
train() client id: f_00005-3-2 loss: 0.664452  [   96/  146]
train() client id: f_00005-3-3 loss: 0.693016  [  128/  146]
train() client id: f_00005-4-0 loss: 0.884268  [   32/  146]
train() client id: f_00005-4-1 loss: 0.765539  [   64/  146]
train() client id: f_00005-4-2 loss: 0.765840  [   96/  146]
train() client id: f_00005-4-3 loss: 0.472954  [  128/  146]
train() client id: f_00005-5-0 loss: 1.066631  [   32/  146]
train() client id: f_00005-5-1 loss: 0.775135  [   64/  146]
train() client id: f_00005-5-2 loss: 0.484556  [   96/  146]
train() client id: f_00005-5-3 loss: 0.495115  [  128/  146]
train() client id: f_00006-0-0 loss: 0.427883  [   32/   54]
train() client id: f_00006-1-0 loss: 0.461440  [   32/   54]
train() client id: f_00006-2-0 loss: 0.440925  [   32/   54]
train() client id: f_00006-3-0 loss: 0.513518  [   32/   54]
train() client id: f_00006-4-0 loss: 0.501528  [   32/   54]
train() client id: f_00006-5-0 loss: 0.504401  [   32/   54]
train() client id: f_00007-0-0 loss: 0.623556  [   32/  179]
train() client id: f_00007-0-1 loss: 0.536783  [   64/  179]
train() client id: f_00007-0-2 loss: 0.588406  [   96/  179]
train() client id: f_00007-0-3 loss: 0.767711  [  128/  179]
train() client id: f_00007-0-4 loss: 0.670781  [  160/  179]
train() client id: f_00007-1-0 loss: 0.744622  [   32/  179]
train() client id: f_00007-1-1 loss: 0.696050  [   64/  179]
train() client id: f_00007-1-2 loss: 0.744132  [   96/  179]
train() client id: f_00007-1-3 loss: 0.563129  [  128/  179]
train() client id: f_00007-1-4 loss: 0.574410  [  160/  179]
train() client id: f_00007-2-0 loss: 0.753105  [   32/  179]
train() client id: f_00007-2-1 loss: 0.635236  [   64/  179]
train() client id: f_00007-2-2 loss: 0.480702  [   96/  179]
train() client id: f_00007-2-3 loss: 0.906927  [  128/  179]
train() client id: f_00007-2-4 loss: 0.601732  [  160/  179]
train() client id: f_00007-3-0 loss: 0.713598  [   32/  179]
train() client id: f_00007-3-1 loss: 0.974459  [   64/  179]
train() client id: f_00007-3-2 loss: 0.548274  [   96/  179]
train() client id: f_00007-3-3 loss: 0.627859  [  128/  179]
train() client id: f_00007-3-4 loss: 0.535977  [  160/  179]
train() client id: f_00007-4-0 loss: 0.728440  [   32/  179]
train() client id: f_00007-4-1 loss: 0.641615  [   64/  179]
train() client id: f_00007-4-2 loss: 0.588839  [   96/  179]
train() client id: f_00007-4-3 loss: 0.712494  [  128/  179]
train() client id: f_00007-4-4 loss: 0.636275  [  160/  179]
train() client id: f_00007-5-0 loss: 0.516846  [   32/  179]
train() client id: f_00007-5-1 loss: 0.643393  [   64/  179]
train() client id: f_00007-5-2 loss: 0.815033  [   96/  179]
train() client id: f_00007-5-3 loss: 0.539628  [  128/  179]
train() client id: f_00007-5-4 loss: 0.783390  [  160/  179]
train() client id: f_00008-0-0 loss: 0.804717  [   32/  130]
train() client id: f_00008-0-1 loss: 0.747295  [   64/  130]
train() client id: f_00008-0-2 loss: 0.648676  [   96/  130]
train() client id: f_00008-0-3 loss: 0.698687  [  128/  130]
train() client id: f_00008-1-0 loss: 0.634817  [   32/  130]
train() client id: f_00008-1-1 loss: 0.861370  [   64/  130]
train() client id: f_00008-1-2 loss: 0.619315  [   96/  130]
train() client id: f_00008-1-3 loss: 0.799225  [  128/  130]
train() client id: f_00008-2-0 loss: 0.889493  [   32/  130]
train() client id: f_00008-2-1 loss: 0.623693  [   64/  130]
train() client id: f_00008-2-2 loss: 0.740017  [   96/  130]
train() client id: f_00008-2-3 loss: 0.693999  [  128/  130]
train() client id: f_00008-3-0 loss: 0.877044  [   32/  130]
train() client id: f_00008-3-1 loss: 0.701609  [   64/  130]
train() client id: f_00008-3-2 loss: 0.653253  [   96/  130]
train() client id: f_00008-3-3 loss: 0.715516  [  128/  130]
train() client id: f_00008-4-0 loss: 0.711022  [   32/  130]
train() client id: f_00008-4-1 loss: 0.860818  [   64/  130]
train() client id: f_00008-4-2 loss: 0.642022  [   96/  130]
train() client id: f_00008-4-3 loss: 0.700758  [  128/  130]
train() client id: f_00008-5-0 loss: 0.640264  [   32/  130]
train() client id: f_00008-5-1 loss: 0.787560  [   64/  130]
train() client id: f_00008-5-2 loss: 0.805781  [   96/  130]
train() client id: f_00008-5-3 loss: 0.700070  [  128/  130]
train() client id: f_00009-0-0 loss: 0.819846  [   32/  118]
train() client id: f_00009-0-1 loss: 0.871181  [   64/  118]
train() client id: f_00009-0-2 loss: 0.890234  [   96/  118]
train() client id: f_00009-1-0 loss: 0.969074  [   32/  118]
train() client id: f_00009-1-1 loss: 0.819920  [   64/  118]
train() client id: f_00009-1-2 loss: 0.771146  [   96/  118]
train() client id: f_00009-2-0 loss: 0.729086  [   32/  118]
train() client id: f_00009-2-1 loss: 0.777294  [   64/  118]
train() client id: f_00009-2-2 loss: 0.902969  [   96/  118]
train() client id: f_00009-3-0 loss: 0.855870  [   32/  118]
train() client id: f_00009-3-1 loss: 0.812146  [   64/  118]
train() client id: f_00009-3-2 loss: 0.843875  [   96/  118]
train() client id: f_00009-4-0 loss: 0.934558  [   32/  118]
train() client id: f_00009-4-1 loss: 0.702087  [   64/  118]
train() client id: f_00009-4-2 loss: 0.606329  [   96/  118]
train() client id: f_00009-5-0 loss: 0.717555  [   32/  118]
train() client id: f_00009-5-1 loss: 0.775315  [   64/  118]
train() client id: f_00009-5-2 loss: 0.816919  [   96/  118]
At round 76 accuracy: 0.6472148541114059
At round 76 training accuracy: 0.5975855130784709
At round 76 training loss: 0.8164182397606268
update_location
xs = 8.927491 501.223621 5.882650 0.934260 -417.581990 -265.230757 -225.849135 -5.143845 -440.120581 20.134486 
ys = -492.390647 7.291448 390.684448 -212.290817 -9.642386 0.794442 -1.381692 386.628436 25.881276 -927.232496 
xs mean: -81.68237997052123
ys mean: -83.16579882624052
dists_uav = 502.521890 511.153874 403.322382 234.666282 429.497024 283.457203 247.001500 399.384535 452.079602 932.826618 
uav_gains = -124.345847 -124.545637 -121.524187 -110.054956 -122.396354 -114.291803 -111.043244 -121.380756 -123.060103 -131.221102 
uav_gains_db_mean: -120.38639892213237
dists_bs = 692.271333 696.709799 281.493009 425.378703 304.858709 196.186559 183.565073 271.362688 304.179743 1119.372120 
bs_gains = -119.094797 -119.172513 -108.152152 -113.172780 -109.121820 -103.761798 -102.953181 -107.706463 -109.094707 -124.938345 
bs_gains_db_mean: -111.71685557259357
Round 77
-------------------------------
ene_coms = [0.05342305 0.0552687  0.00948833 0.01072842 0.01008436 0.01492838
 0.01149954 0.00923633 0.01006674 0.05914086]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.79093329 1.55929308 0.69975324 0.25622958 1.70248482 0.83075818
 0.31817566 1.00198797 0.73382995 0.75733898]
obj_prev = 8.6507847390635
eta_min = 5.536088406187705e-125	eta_max = 0.9589735827367118
af = 1.7640428553154524	bf = 0.4405593275688379	zeta = 1.9404471408469977	eta = 0.9090909090909091
af = 1.7640428553154524	bf = 0.4405593275688379	zeta = 6.6955962622063705	eta = 0.263463145959484
af = 1.7640428553154524	bf = 0.4405593275688379	zeta = 3.79276798164852	eta = 0.46510697829417824
af = 1.7640428553154524	bf = 0.4405593275688379	zeta = 3.348169836070516	eta = 0.5268677939544939
af = 1.7640428553154524	bf = 0.4405593275688379	zeta = 3.3203571769891513	eta = 0.5312810523942063
af = 1.7640428553154524	bf = 0.4405593275688379	zeta = 3.3202266049671154	eta = 0.5313019456793745
eta = 0.5313019456793745
ene_coms = [0.05342305 0.0552687  0.00948833 0.01072842 0.01008436 0.01492838
 0.01149954 0.00923633 0.01006674 0.05914086]
ene_comp = [0.05218618 0.10975664 0.05135783 0.01780957 0.12673786 0.0604697
 0.0223655  0.07413751 0.05384288 0.04887277]
ene_total = [0.40706535 0.63608174 0.23452841 0.1099982  0.52737425 0.29061802
 0.13053109 0.32136018 0.24633636 0.41633301]
ti_comp = [6.2314445  6.21298795 6.67079164 6.65839081 6.66483134 6.61639121
 6.65067956 6.67331167 6.6650076  6.17426635]
ti_coms = [0.53423048 0.55268703 0.09488335 0.10728417 0.10084364 0.14928377
 0.11499542 0.09236331 0.10066739 0.59140863]
t_total = [26.07794762 26.07794762 26.07794762 26.07794762 26.07794762 26.07794762
 26.07794762 26.07794762 26.07794762 26.07794762]
ene_coms = [0.05342305 0.0552687  0.00948833 0.01072842 0.01008436 0.01492838
 0.01149954 0.00923633 0.01006674 0.05914086]
ene_comp = [2.28754165e-07 2.14077703e-06 1.90258971e-07 7.96345702e-09
 2.86431508e-06 3.15682897e-07 1.58082600e-08 5.71888172e-07
 2.19615808e-07 1.91385988e-07]
ene_total = [0.20591728 0.21303864 0.03657303 0.04135217 0.0388807  0.05754188
 0.04432446 0.03560317 0.03880257 0.22795616]
optimize_network iter = 0 obj = 0.9399900670787457
eta = 0.5313019456793745
freqs = [4187325.84916477 8832838.85240935 3849455.06326713 1337377.82560494
 9507956.9600085  4569688.79362028 1681445.05705614 5554776.2147933
 4039221.30328008 3957779.46779306]
eta_min = 0.5313019456793753	eta_max = 0.8456158194050047
af = 1.9271780619542702e-05	bf = 0.4405593275688379	zeta = 2.1198958681496973e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.05342305 0.0552687  0.00948833 0.01072842 0.01008436 0.01492838
 0.01149954 0.00923633 0.01006674 0.05914086]
ene_comp = [3.44746145e-08 3.22627844e-07 2.86731595e-08 1.20014038e-09
 4.31669335e-07 4.75752920e-08 2.38239889e-09 8.61869521e-08
 3.30974097e-08 2.88430077e-08]
ene_total = [1.06163947 1.09832261 0.18855559 0.21319834 0.20040807 0.29666213
 0.22852238 0.18354885 0.2000499  1.17526548]
ti_comp = [1.69431129 1.67585473 2.13365842 2.12125759 2.12769813 2.07925799
 2.11354634 2.13617845 2.12787438 1.63713314]
ti_coms = [0.53423048 0.55268703 0.09488335 0.10728417 0.10084364 0.14928377
 0.11499542 0.09236331 0.10066739 0.59140863]
t_total = [26.07794762 26.07794762 26.07794762 26.07794762 26.07794762 26.07794762
 26.07794762 26.07794762 26.07794762 26.07794762]
ene_coms = [0.05342305 0.0552687  0.00948833 0.01072842 0.01008436 0.01492838
 0.01149954 0.00923633 0.01006674 0.05914086]
ene_comp = [5.76840813e-08 5.48523999e-07 3.46693757e-08 1.46267992e-09
 5.23932102e-07 5.95898871e-08 2.91802146e-09 1.04043460e-07
 4.01669485e-08 5.07468134e-08]
ene_total = [0.62514644 0.64674967 0.11103099 0.1255418  0.11801133 0.17468957
 0.13456536 0.10808291 0.11779942 0.69205508]
optimize_network iter = 1 obj = 2.853672558651371
eta = 0.8456158194050047
freqs = [4083484.62054937 8682865.7549733  3191173.18874918 1113085.76590459
 7897052.00460128 3855654.05079661 1402928.59493841 4601178.52830198
 3354678.84407623 3957779.46779307]
eta_min = 0.845615819405014	eta_max = 0.8456158194050042
af = 1.5338319999830444e-05	bf = 0.4405593275688379	zeta = 1.687215199981349e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.05342305 0.0552687  0.00948833 0.01072842 0.01008436 0.01492838
 0.01149954 0.00923633 0.01006674 0.05914086]
ene_comp = [3.27859484e-08 3.11765033e-07 1.97050613e-08 8.31344582e-10
 2.97787717e-07 3.38691527e-08 1.65851824e-09 5.91352663e-08
 2.28297213e-08 2.88430077e-08]
ene_total = [1.06163943 1.09832239 0.18855541 0.21319833 0.20040541 0.29666185
 0.22852237 0.18354832 0.20004969 1.17526548]
ti_comp = [1.69431129 1.67585473 2.13365842 2.12125759 2.12769813 2.07925799
 2.11354634 2.13617845 2.12787438 1.63713314]
ti_coms = [0.53423048 0.55268703 0.09488335 0.10728417 0.10084364 0.14928377
 0.11499542 0.09236331 0.10066739 0.59140863]
t_total = [26.07794762 26.07794762 26.07794762 26.07794762 26.07794762 26.07794762
 26.07794762 26.07794762 26.07794762 26.07794762]
ene_coms = [0.05342305 0.0552687  0.00948833 0.01072842 0.01008436 0.01492838
 0.01149954 0.00923633 0.01006674 0.05914086]
ene_comp = [5.76840813e-08 5.48523999e-07 3.46693757e-08 1.46267992e-09
 5.23932102e-07 5.95898871e-08 2.91802146e-09 1.04043460e-07
 4.01669485e-08 5.07468134e-08]
ene_total = [0.62514644 0.64674967 0.11103099 0.1255418  0.11801133 0.17468957
 0.13456536 0.10808291 0.11779942 0.69205508]
optimize_network iter = 2 obj = 2.8536725586513603
eta = 0.8456158194050042
freqs = [4083484.62054937 8682865.7549733  3191173.18874918 1113085.76590459
 7897052.00460128 3855654.05079661 1402928.59493841 4601178.52830199
 3354678.84407623 3957779.46779307]
Done!
ene_coms = [0.05342305 0.0552687  0.00948833 0.01072842 0.01008436 0.01492838
 0.01149954 0.00923633 0.01006674 0.05914086]
ene_comp = [5.25257669e-08 4.99473045e-07 3.15691176e-08 1.33188192e-09
 4.77080242e-07 5.42611488e-08 2.65708167e-09 9.47395262e-08
 3.65750780e-08 4.62088540e-08]
ene_total = [0.0534231  0.0552692  0.00948837 0.01072842 0.01008484 0.01492843
 0.01149954 0.00923643 0.01006678 0.05914091]
At round 77 energy consumption: 0.24386601456684354
At round 77 eta: 0.8456158194050042
At round 77 a_n: 1.8065726396169453
At round 77 local rounds: 5.491027043474501
At round 77 global rounds: 11.701798932082445
gradient difference: 0.6686480045318604
train() client id: f_00000-0-0 loss: 0.984812  [   32/  126]
train() client id: f_00000-0-1 loss: 1.396351  [   64/  126]
train() client id: f_00000-0-2 loss: 1.124600  [   96/  126]
train() client id: f_00000-1-0 loss: 1.082025  [   32/  126]
train() client id: f_00000-1-1 loss: 0.924132  [   64/  126]
train() client id: f_00000-1-2 loss: 1.288081  [   96/  126]
train() client id: f_00000-2-0 loss: 1.066959  [   32/  126]
train() client id: f_00000-2-1 loss: 1.016415  [   64/  126]
train() client id: f_00000-2-2 loss: 1.129382  [   96/  126]
train() client id: f_00000-3-0 loss: 1.037391  [   32/  126]
train() client id: f_00000-3-1 loss: 1.000308  [   64/  126]
train() client id: f_00000-3-2 loss: 1.149208  [   96/  126]
train() client id: f_00000-4-0 loss: 1.042120  [   32/  126]
train() client id: f_00000-4-1 loss: 1.009815  [   64/  126]
train() client id: f_00000-4-2 loss: 1.065654  [   96/  126]
train() client id: f_00001-0-0 loss: 0.489961  [   32/  265]
train() client id: f_00001-0-1 loss: 0.410545  [   64/  265]
train() client id: f_00001-0-2 loss: 0.421573  [   96/  265]
train() client id: f_00001-0-3 loss: 0.376984  [  128/  265]
train() client id: f_00001-0-4 loss: 0.391043  [  160/  265]
train() client id: f_00001-0-5 loss: 0.579556  [  192/  265]
train() client id: f_00001-0-6 loss: 0.473190  [  224/  265]
train() client id: f_00001-0-7 loss: 0.310497  [  256/  265]
train() client id: f_00001-1-0 loss: 0.409092  [   32/  265]
train() client id: f_00001-1-1 loss: 0.455233  [   64/  265]
train() client id: f_00001-1-2 loss: 0.400981  [   96/  265]
train() client id: f_00001-1-3 loss: 0.405842  [  128/  265]
train() client id: f_00001-1-4 loss: 0.358078  [  160/  265]
train() client id: f_00001-1-5 loss: 0.486483  [  192/  265]
train() client id: f_00001-1-6 loss: 0.450551  [  224/  265]
train() client id: f_00001-1-7 loss: 0.342736  [  256/  265]
train() client id: f_00001-2-0 loss: 0.434604  [   32/  265]
train() client id: f_00001-2-1 loss: 0.318407  [   64/  265]
train() client id: f_00001-2-2 loss: 0.364226  [   96/  265]
train() client id: f_00001-2-3 loss: 0.443649  [  128/  265]
train() client id: f_00001-2-4 loss: 0.373399  [  160/  265]
train() client id: f_00001-2-5 loss: 0.440090  [  192/  265]
train() client id: f_00001-2-6 loss: 0.475263  [  224/  265]
train() client id: f_00001-2-7 loss: 0.428335  [  256/  265]
train() client id: f_00001-3-0 loss: 0.519166  [   32/  265]
train() client id: f_00001-3-1 loss: 0.430421  [   64/  265]
train() client id: f_00001-3-2 loss: 0.499146  [   96/  265]
train() client id: f_00001-3-3 loss: 0.375702  [  128/  265]
train() client id: f_00001-3-4 loss: 0.430484  [  160/  265]
train() client id: f_00001-3-5 loss: 0.353016  [  192/  265]
train() client id: f_00001-3-6 loss: 0.366675  [  224/  265]
train() client id: f_00001-3-7 loss: 0.371322  [  256/  265]
train() client id: f_00001-4-0 loss: 0.409435  [   32/  265]
train() client id: f_00001-4-1 loss: 0.427210  [   64/  265]
train() client id: f_00001-4-2 loss: 0.465430  [   96/  265]
train() client id: f_00001-4-3 loss: 0.474322  [  128/  265]
train() client id: f_00001-4-4 loss: 0.289714  [  160/  265]
train() client id: f_00001-4-5 loss: 0.418403  [  192/  265]
train() client id: f_00001-4-6 loss: 0.367664  [  224/  265]
train() client id: f_00001-4-7 loss: 0.442632  [  256/  265]
train() client id: f_00002-0-0 loss: 1.478947  [   32/  124]
train() client id: f_00002-0-1 loss: 1.358484  [   64/  124]
train() client id: f_00002-0-2 loss: 1.061016  [   96/  124]
train() client id: f_00002-1-0 loss: 1.141670  [   32/  124]
train() client id: f_00002-1-1 loss: 1.149870  [   64/  124]
train() client id: f_00002-1-2 loss: 0.986601  [   96/  124]
train() client id: f_00002-2-0 loss: 1.124058  [   32/  124]
train() client id: f_00002-2-1 loss: 1.131018  [   64/  124]
train() client id: f_00002-2-2 loss: 1.346016  [   96/  124]
train() client id: f_00002-3-0 loss: 0.917507  [   32/  124]
train() client id: f_00002-3-1 loss: 1.115784  [   64/  124]
train() client id: f_00002-3-2 loss: 1.141444  [   96/  124]
train() client id: f_00002-4-0 loss: 1.192303  [   32/  124]
train() client id: f_00002-4-1 loss: 1.152017  [   64/  124]
train() client id: f_00002-4-2 loss: 1.033754  [   96/  124]
train() client id: f_00003-0-0 loss: 0.627019  [   32/   43]
train() client id: f_00003-1-0 loss: 0.772489  [   32/   43]
train() client id: f_00003-2-0 loss: 0.766449  [   32/   43]
train() client id: f_00003-3-0 loss: 0.698624  [   32/   43]
train() client id: f_00003-4-0 loss: 0.680915  [   32/   43]
train() client id: f_00004-0-0 loss: 0.955878  [   32/  306]
train() client id: f_00004-0-1 loss: 0.822965  [   64/  306]
train() client id: f_00004-0-2 loss: 0.902827  [   96/  306]
train() client id: f_00004-0-3 loss: 0.949370  [  128/  306]
train() client id: f_00004-0-4 loss: 0.658811  [  160/  306]
train() client id: f_00004-0-5 loss: 0.836416  [  192/  306]
train() client id: f_00004-0-6 loss: 0.889695  [  224/  306]
train() client id: f_00004-0-7 loss: 0.751295  [  256/  306]
train() client id: f_00004-0-8 loss: 0.754671  [  288/  306]
train() client id: f_00004-1-0 loss: 0.744066  [   32/  306]
train() client id: f_00004-1-1 loss: 1.060574  [   64/  306]
train() client id: f_00004-1-2 loss: 0.784796  [   96/  306]
train() client id: f_00004-1-3 loss: 0.764190  [  128/  306]
train() client id: f_00004-1-4 loss: 0.845085  [  160/  306]
train() client id: f_00004-1-5 loss: 0.783848  [  192/  306]
train() client id: f_00004-1-6 loss: 0.846959  [  224/  306]
train() client id: f_00004-1-7 loss: 0.882672  [  256/  306]
train() client id: f_00004-1-8 loss: 0.772758  [  288/  306]
train() client id: f_00004-2-0 loss: 0.885733  [   32/  306]
train() client id: f_00004-2-1 loss: 0.892224  [   64/  306]
train() client id: f_00004-2-2 loss: 0.836869  [   96/  306]
train() client id: f_00004-2-3 loss: 0.812358  [  128/  306]
train() client id: f_00004-2-4 loss: 0.906741  [  160/  306]
train() client id: f_00004-2-5 loss: 0.926495  [  192/  306]
train() client id: f_00004-2-6 loss: 0.826387  [  224/  306]
train() client id: f_00004-2-7 loss: 0.808837  [  256/  306]
train() client id: f_00004-2-8 loss: 0.755713  [  288/  306]
train() client id: f_00004-3-0 loss: 0.922235  [   32/  306]
train() client id: f_00004-3-1 loss: 0.840035  [   64/  306]
train() client id: f_00004-3-2 loss: 1.023161  [   96/  306]
train() client id: f_00004-3-3 loss: 0.924057  [  128/  306]
train() client id: f_00004-3-4 loss: 0.747799  [  160/  306]
train() client id: f_00004-3-5 loss: 0.650417  [  192/  306]
train() client id: f_00004-3-6 loss: 0.872174  [  224/  306]
train() client id: f_00004-3-7 loss: 0.848040  [  256/  306]
train() client id: f_00004-3-8 loss: 0.746896  [  288/  306]
train() client id: f_00004-4-0 loss: 0.730764  [   32/  306]
train() client id: f_00004-4-1 loss: 0.746101  [   64/  306]
train() client id: f_00004-4-2 loss: 0.708319  [   96/  306]
train() client id: f_00004-4-3 loss: 0.854715  [  128/  306]
train() client id: f_00004-4-4 loss: 0.956928  [  160/  306]
train() client id: f_00004-4-5 loss: 0.931829  [  192/  306]
train() client id: f_00004-4-6 loss: 0.903591  [  224/  306]
train() client id: f_00004-4-7 loss: 0.825157  [  256/  306]
train() client id: f_00004-4-8 loss: 0.899562  [  288/  306]
train() client id: f_00005-0-0 loss: 0.845065  [   32/  146]
train() client id: f_00005-0-1 loss: 0.849770  [   64/  146]
train() client id: f_00005-0-2 loss: 0.869872  [   96/  146]
train() client id: f_00005-0-3 loss: 0.728985  [  128/  146]
train() client id: f_00005-1-0 loss: 0.721355  [   32/  146]
train() client id: f_00005-1-1 loss: 0.892532  [   64/  146]
train() client id: f_00005-1-2 loss: 0.843478  [   96/  146]
train() client id: f_00005-1-3 loss: 0.688243  [  128/  146]
train() client id: f_00005-2-0 loss: 1.148145  [   32/  146]
train() client id: f_00005-2-1 loss: 0.492725  [   64/  146]
train() client id: f_00005-2-2 loss: 0.988120  [   96/  146]
train() client id: f_00005-2-3 loss: 0.803850  [  128/  146]
train() client id: f_00005-3-0 loss: 0.928482  [   32/  146]
train() client id: f_00005-3-1 loss: 0.867777  [   64/  146]
train() client id: f_00005-3-2 loss: 0.973324  [   96/  146]
train() client id: f_00005-3-3 loss: 0.565624  [  128/  146]
train() client id: f_00005-4-0 loss: 0.852310  [   32/  146]
train() client id: f_00005-4-1 loss: 0.619652  [   64/  146]
train() client id: f_00005-4-2 loss: 0.795239  [   96/  146]
train() client id: f_00005-4-3 loss: 1.016009  [  128/  146]
train() client id: f_00006-0-0 loss: 0.456593  [   32/   54]
train() client id: f_00006-1-0 loss: 0.393928  [   32/   54]
train() client id: f_00006-2-0 loss: 0.409249  [   32/   54]
train() client id: f_00006-3-0 loss: 0.369784  [   32/   54]
train() client id: f_00006-4-0 loss: 0.393168  [   32/   54]
train() client id: f_00007-0-0 loss: 0.416459  [   32/  179]
train() client id: f_00007-0-1 loss: 0.369328  [   64/  179]
train() client id: f_00007-0-2 loss: 0.539789  [   96/  179]
train() client id: f_00007-0-3 loss: 0.600120  [  128/  179]
train() client id: f_00007-0-4 loss: 0.581630  [  160/  179]
train() client id: f_00007-1-0 loss: 0.554801  [   32/  179]
train() client id: f_00007-1-1 loss: 0.513880  [   64/  179]
train() client id: f_00007-1-2 loss: 0.678846  [   96/  179]
train() client id: f_00007-1-3 loss: 0.288699  [  128/  179]
train() client id: f_00007-1-4 loss: 0.387891  [  160/  179]
train() client id: f_00007-2-0 loss: 0.393648  [   32/  179]
train() client id: f_00007-2-1 loss: 0.419783  [   64/  179]
train() client id: f_00007-2-2 loss: 0.322670  [   96/  179]
train() client id: f_00007-2-3 loss: 0.506549  [  128/  179]
train() client id: f_00007-2-4 loss: 0.459447  [  160/  179]
train() client id: f_00007-3-0 loss: 0.390022  [   32/  179]
train() client id: f_00007-3-1 loss: 0.462567  [   64/  179]
train() client id: f_00007-3-2 loss: 0.767088  [   96/  179]
train() client id: f_00007-3-3 loss: 0.313093  [  128/  179]
train() client id: f_00007-3-4 loss: 0.393603  [  160/  179]
train() client id: f_00007-4-0 loss: 0.462859  [   32/  179]
train() client id: f_00007-4-1 loss: 0.332385  [   64/  179]
train() client id: f_00007-4-2 loss: 0.711988  [   96/  179]
train() client id: f_00007-4-3 loss: 0.313965  [  128/  179]
train() client id: f_00007-4-4 loss: 0.454622  [  160/  179]
train() client id: f_00008-0-0 loss: 0.794725  [   32/  130]
train() client id: f_00008-0-1 loss: 0.668502  [   64/  130]
train() client id: f_00008-0-2 loss: 0.721434  [   96/  130]
train() client id: f_00008-0-3 loss: 0.739707  [  128/  130]
train() client id: f_00008-1-0 loss: 0.785846  [   32/  130]
train() client id: f_00008-1-1 loss: 0.689950  [   64/  130]
train() client id: f_00008-1-2 loss: 0.703699  [   96/  130]
train() client id: f_00008-1-3 loss: 0.740646  [  128/  130]
train() client id: f_00008-2-0 loss: 0.700246  [   32/  130]
train() client id: f_00008-2-1 loss: 0.722780  [   64/  130]
train() client id: f_00008-2-2 loss: 0.695156  [   96/  130]
train() client id: f_00008-2-3 loss: 0.754323  [  128/  130]
train() client id: f_00008-3-0 loss: 0.730643  [   32/  130]
train() client id: f_00008-3-1 loss: 0.740907  [   64/  130]
train() client id: f_00008-3-2 loss: 0.692212  [   96/  130]
train() client id: f_00008-3-3 loss: 0.760320  [  128/  130]
train() client id: f_00008-4-0 loss: 0.694544  [   32/  130]
train() client id: f_00008-4-1 loss: 0.702782  [   64/  130]
train() client id: f_00008-4-2 loss: 0.755209  [   96/  130]
train() client id: f_00008-4-3 loss: 0.779268  [  128/  130]
train() client id: f_00009-0-0 loss: 0.738822  [   32/  118]
train() client id: f_00009-0-1 loss: 0.843791  [   64/  118]
train() client id: f_00009-0-2 loss: 0.853694  [   96/  118]
train() client id: f_00009-1-0 loss: 0.737956  [   32/  118]
train() client id: f_00009-1-1 loss: 0.755203  [   64/  118]
train() client id: f_00009-1-2 loss: 0.854258  [   96/  118]
train() client id: f_00009-2-0 loss: 0.743756  [   32/  118]
train() client id: f_00009-2-1 loss: 0.664269  [   64/  118]
train() client id: f_00009-2-2 loss: 0.883847  [   96/  118]
train() client id: f_00009-3-0 loss: 0.779105  [   32/  118]
train() client id: f_00009-3-1 loss: 0.647023  [   64/  118]
train() client id: f_00009-3-2 loss: 0.854018  [   96/  118]
train() client id: f_00009-4-0 loss: 0.659898  [   32/  118]
train() client id: f_00009-4-1 loss: 0.723941  [   64/  118]
train() client id: f_00009-4-2 loss: 0.801607  [   96/  118]
At round 77 accuracy: 0.6472148541114059
At round 77 training accuracy: 0.5928906773977196
At round 77 training loss: 0.8217822534904774
update_location
xs = 8.927491 506.223621 5.882650 0.934260 -422.581990 -270.230757 -230.849135 -5.143845 -445.120581 20.134486 
ys = -497.390647 7.291448 395.684448 -217.290817 -9.642386 0.794442 -1.381692 391.628436 25.881276 -932.232496 
xs mean: -83.18237997052123
ys mean: -83.66579882624052
dists_uav = 507.422069 516.057671 408.167598 239.199022 434.359889 288.141100 251.581462 404.226782 456.948763 937.796792 
uav_gains = -124.459868 -124.656993 -121.695816 -110.408091 -122.545261 -114.709505 -111.431096 -121.556620 -123.194794 -131.279015 
uav_gains_db_mean: -120.59370571572734
dists_bs = 697.092895 701.563810 285.342178 429.935983 308.852153 198.535824 185.012506 275.279844 308.547439 1124.295899 
bs_gains = -119.179198 -119.256940 -108.317306 -113.302365 -109.280077 -103.906548 -103.048690 -107.880743 -109.268074 -124.991717 
bs_gains_db_mean: -111.84316567768494
Round 78
-------------------------------
ene_coms = [0.05446648 0.056333   0.00958507 0.01099264 0.01018841 0.01550278
 0.01183082 0.00933334 0.01018045 0.05969341]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.64250677 1.26520803 0.56721546 0.20803639 1.37982903 0.67408682
 0.25833599 0.81214359 0.59485596 0.61455637]
obj_prev = 7.01677439851595
eta_min = nan	eta_max = nan
af = 1.4295611186207509	bf = 0.36323441201072304	zeta = 1.572517230482826	eta = 0.909090909090909
af = 1.4295611186207509	bf = 0.36323441201072304	zeta = 5.494347531945117	eta = 0.2601876037707894
af = 1.4295611186207509	bf = 0.36323441201072304	zeta = 3.0925777975932456	eta = 0.4622555072772256
af = 1.4295611186207509	bf = 0.36323441201072304	zeta = 2.726829176263561	eta = 0.5242576730015804
af = 1.4295611186207509	bf = 0.36323441201072304	zeta = 2.7039950900473695	eta = 0.5286848056353931
af = 1.4295611186207509	bf = 0.36323441201072304	zeta = 2.703888148692408	eta = 0.5287057156236593
eta = 0.5287057156236593
ene_coms = [0.05446648 0.056333   0.00958507 0.01099264 0.01018841 0.01550278
 0.01183082 0.00933334 0.01018045 0.05969341]
ene_comp = [0.05259039 0.11060678 0.05175562 0.01794751 0.12771952 0.06093807
 0.02253874 0.07471175 0.05425993 0.04925132]
ene_total = [0.33256107 0.51858109 0.19054851 0.08989958 0.42839669 0.23745558
 0.10676546 0.26107733 0.20017735 0.33842548]
ti_comp = [7.83385803 7.81519287 8.28267213 8.26859647 8.27663872 8.22349503
 8.26021467 8.28518948 8.27671834 7.78158879]
ti_coms = [0.54466482 0.56332998 0.09585072 0.10992638 0.10188413 0.15502782
 0.11830818 0.09333337 0.1018045  0.59693406]
t_total = [26.02701187 26.02701187 26.02701187 26.02701187 26.02701187 26.02701187
 26.02701187 26.02701187 26.02701187 26.02701187]
ene_coms = [0.05446648 0.056333   0.00958507 0.01099264 0.01018841 0.01550278
 0.01183082 0.00933334 0.01018045 0.05969341]
ene_comp = [1.48131642e-07 1.38466851e-06 1.26302431e-07 5.28480986e-09
 1.90083263e-06 2.09138165e-07 1.04878437e-08 3.79701050e-07
 1.45747562e-07 1.23309649e-07]
ene_total = [0.16919493 0.17499692 0.02977543 0.03414751 0.03165515 0.04815843
 0.03675124 0.02899422 0.03162497 0.18543175]
optimize_network iter = 0 obj = 0.7707305498238058
eta = 0.5287057156236593
freqs = [3356608.6684844  7076394.51574089 3124331.29397533 1085281.80387401
 7715663.73178624 3705120.05631123 1364295.00637701 4508753.05249556
 3277864.82147838 3164605.60887004]
eta_min = 0.5287057156236598	eta_max = 0.8736586802319519
af = 1.0184651054378497e-05	bf = 0.36323441201072304	zeta = 1.1203116159816349e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.05446648 0.056333   0.00958507 0.01099264 0.01018841 0.01550278
 0.01183082 0.00933334 0.01018045 0.05969341]
ene_comp = [2.21527336e-08 2.07073873e-07 1.88882273e-08 7.90330707e-10
 2.84264985e-07 3.12761137e-08 1.56843201e-09 5.67833861e-08
 2.17962002e-08 1.84406639e-08]
ene_total = [0.87714463 0.90720654 0.15436114 0.1770287  0.16408179 0.24966187
 0.19052701 0.15030772 0.16394933 0.96132049]
ti_comp = [1.70139178 1.68272661 2.15020587 2.13613021 2.14417246 2.09102878
 2.12774841 2.15272323 2.14425209 1.64912254]
ti_coms = [0.54466482 0.56332998 0.09585072 0.10992638 0.10188413 0.15502782
 0.11830818 0.09333337 0.1018045  0.59693406]
t_total = [26.02701187 26.02701187 26.02701187 26.02701187 26.02701187 26.02701187
 26.02701187 26.02701187 26.02701187 26.02701187]
ene_coms = [0.05446648 0.056333   0.00958507 0.01099264 0.01018841 0.01550278
 0.01183082 0.00933334 0.01018045 0.05969341]
ene_comp = [2.98912805e-08 2.84283752e-07 1.78380145e-08 7.53687764e-10
 2.69579209e-07 3.07879017e-08 1.50446564e-09 5.35333280e-08
 2.06690215e-08 2.61325453e-08]
ene_total = [0.63115086 0.65278276 0.11107078 0.12738127 0.11806513 0.17964458
 0.137094   0.10815411 0.11796998 0.69171973]
optimize_network iter = 1 obj = 2.8750331934006796
eta = 0.8736586802319519
freqs = [3275342.56150428 6965027.31347106 2550540.09432145  890289.48921729
 6311785.41238982 3088043.14147604 1122442.2305688  3677522.57408356
 2681377.88639713 3164605.60887004]
eta_min = 0.8736586802319576	eta_max = 0.87365868023188
af = 7.966656322854386e-06	bf = 0.36323441201072304	zeta = 8.763321955139825e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.05446648 0.056333   0.00958507 0.01099264 0.01018841 0.01550278
 0.01183082 0.00933334 0.01018045 0.05969341]
ene_comp = [2.10930489e-08 2.00607367e-07 1.25875542e-08 5.31846499e-10
 1.90230975e-07 2.17257577e-08 1.06163961e-09 3.77762708e-08
 1.45852795e-08 1.84406639e-08]
ene_total = [0.87714461 0.90720644 0.15436103 0.17702869 0.16408027 0.24966171
 0.190527   0.15030741 0.16394921 0.96132049]
ti_comp = [1.70139178 1.68272661 2.15020587 2.13613021 2.14417246 2.09102878
 2.12774841 2.15272323 2.14425209 1.64912254]
ti_coms = [0.54466482 0.56332998 0.09585072 0.10992638 0.10188413 0.15502782
 0.11830818 0.09333337 0.1018045  0.59693406]
t_total = [26.02701187 26.02701187 26.02701187 26.02701187 26.02701187 26.02701187
 26.02701187 26.02701187 26.02701187 26.02701187]
ene_coms = [0.05446648 0.056333   0.00958507 0.01099264 0.01018841 0.01550278
 0.01183082 0.00933334 0.01018045 0.05969341]
ene_comp = [2.98912805e-08 2.84283752e-07 1.78380145e-08 7.53687764e-10
 2.69579209e-07 3.07879017e-08 1.50446564e-09 5.35333280e-08
 2.06690215e-08 2.61325453e-08]
ene_total = [0.63115086 0.65278276 0.11107078 0.12738127 0.11806513 0.17964458
 0.137094   0.10815411 0.11796998 0.69171973]
optimize_network iter = 2 obj = 2.8750331933990427
eta = 0.87365868023188
freqs = [3275342.56150382 6965027.31347001 2550540.09432149  890289.4892173
 6311785.41238991 3088043.14147604 1122442.23056881 3677522.57408362
 2681377.88639717 3164605.60886952]
Done!
ene_coms = [0.05446648 0.056333   0.00958507 0.01099264 0.01018841 0.01550278
 0.01183082 0.00933334 0.01018045 0.05969341]
ene_comp = [2.70342296e-08 2.57111509e-07 1.61330318e-08 6.81649222e-10
 2.43812447e-07 2.78451505e-08 1.36066669e-09 4.84165368e-08
 1.86934472e-08 2.36347596e-08]
ene_total = [0.05446651 0.05633326 0.00958509 0.01099264 0.01018866 0.01550281
 0.01183082 0.00933339 0.01018047 0.05969343]
At round 78 energy consumption: 0.24810706129683285
At round 78 eta: 0.87365868023188
At round 78 a_n: 1.4640267926476263
At round 78 local rounds: 4.422730870115972
At round 78 global rounds: 11.58787002806858
gradient difference: 0.9347447156906128
train() client id: f_00000-0-0 loss: 0.860162  [   32/  126]
train() client id: f_00000-0-1 loss: 0.952169  [   64/  126]
train() client id: f_00000-0-2 loss: 0.679224  [   96/  126]
train() client id: f_00000-1-0 loss: 0.886580  [   32/  126]
train() client id: f_00000-1-1 loss: 0.937629  [   64/  126]
train() client id: f_00000-1-2 loss: 0.794827  [   96/  126]
train() client id: f_00000-2-0 loss: 0.796938  [   32/  126]
train() client id: f_00000-2-1 loss: 0.857176  [   64/  126]
train() client id: f_00000-2-2 loss: 0.833954  [   96/  126]
train() client id: f_00000-3-0 loss: 0.823379  [   32/  126]
train() client id: f_00000-3-1 loss: 0.824659  [   64/  126]
train() client id: f_00000-3-2 loss: 0.857300  [   96/  126]
train() client id: f_00001-0-0 loss: 0.527278  [   32/  265]
train() client id: f_00001-0-1 loss: 0.368390  [   64/  265]
train() client id: f_00001-0-2 loss: 0.486897  [   96/  265]
train() client id: f_00001-0-3 loss: 0.443091  [  128/  265]
train() client id: f_00001-0-4 loss: 0.358514  [  160/  265]
train() client id: f_00001-0-5 loss: 0.504928  [  192/  265]
train() client id: f_00001-0-6 loss: 0.439464  [  224/  265]
train() client id: f_00001-0-7 loss: 0.611060  [  256/  265]
train() client id: f_00001-1-0 loss: 0.456217  [   32/  265]
train() client id: f_00001-1-1 loss: 0.442018  [   64/  265]
train() client id: f_00001-1-2 loss: 0.410190  [   96/  265]
train() client id: f_00001-1-3 loss: 0.367723  [  128/  265]
train() client id: f_00001-1-4 loss: 0.609273  [  160/  265]
train() client id: f_00001-1-5 loss: 0.489131  [  192/  265]
train() client id: f_00001-1-6 loss: 0.396824  [  224/  265]
train() client id: f_00001-1-7 loss: 0.541080  [  256/  265]
train() client id: f_00001-2-0 loss: 0.426071  [   32/  265]
train() client id: f_00001-2-1 loss: 0.552790  [   64/  265]
train() client id: f_00001-2-2 loss: 0.499304  [   96/  265]
train() client id: f_00001-2-3 loss: 0.588319  [  128/  265]
train() client id: f_00001-2-4 loss: 0.376860  [  160/  265]
train() client id: f_00001-2-5 loss: 0.387119  [  192/  265]
train() client id: f_00001-2-6 loss: 0.492130  [  224/  265]
train() client id: f_00001-2-7 loss: 0.374197  [  256/  265]
train() client id: f_00001-3-0 loss: 0.484255  [   32/  265]
train() client id: f_00001-3-1 loss: 0.473196  [   64/  265]
train() client id: f_00001-3-2 loss: 0.513825  [   96/  265]
train() client id: f_00001-3-3 loss: 0.351038  [  128/  265]
train() client id: f_00001-3-4 loss: 0.386799  [  160/  265]
train() client id: f_00001-3-5 loss: 0.518251  [  192/  265]
train() client id: f_00001-3-6 loss: 0.541343  [  224/  265]
train() client id: f_00001-3-7 loss: 0.418605  [  256/  265]
train() client id: f_00002-0-0 loss: 1.175738  [   32/  124]
train() client id: f_00002-0-1 loss: 1.059049  [   64/  124]
train() client id: f_00002-0-2 loss: 1.004205  [   96/  124]
train() client id: f_00002-1-0 loss: 0.878102  [   32/  124]
train() client id: f_00002-1-1 loss: 0.958487  [   64/  124]
train() client id: f_00002-1-2 loss: 1.155265  [   96/  124]
train() client id: f_00002-2-0 loss: 1.120425  [   32/  124]
train() client id: f_00002-2-1 loss: 0.955269  [   64/  124]
train() client id: f_00002-2-2 loss: 0.970387  [   96/  124]
train() client id: f_00002-3-0 loss: 0.883407  [   32/  124]
train() client id: f_00002-3-1 loss: 1.026459  [   64/  124]
train() client id: f_00002-3-2 loss: 1.133323  [   96/  124]
train() client id: f_00003-0-0 loss: 0.682455  [   32/   43]
train() client id: f_00003-1-0 loss: 0.690382  [   32/   43]
train() client id: f_00003-2-0 loss: 0.611506  [   32/   43]
train() client id: f_00003-3-0 loss: 0.730744  [   32/   43]
train() client id: f_00004-0-0 loss: 0.746864  [   32/  306]
train() client id: f_00004-0-1 loss: 0.850829  [   64/  306]
train() client id: f_00004-0-2 loss: 0.571980  [   96/  306]
train() client id: f_00004-0-3 loss: 0.825687  [  128/  306]
train() client id: f_00004-0-4 loss: 0.765466  [  160/  306]
train() client id: f_00004-0-5 loss: 0.750070  [  192/  306]
train() client id: f_00004-0-6 loss: 0.706369  [  224/  306]
train() client id: f_00004-0-7 loss: 0.808452  [  256/  306]
train() client id: f_00004-0-8 loss: 0.697384  [  288/  306]
train() client id: f_00004-1-0 loss: 0.692475  [   32/  306]
train() client id: f_00004-1-1 loss: 0.782887  [   64/  306]
train() client id: f_00004-1-2 loss: 0.689715  [   96/  306]
train() client id: f_00004-1-3 loss: 0.751689  [  128/  306]
train() client id: f_00004-1-4 loss: 0.673219  [  160/  306]
train() client id: f_00004-1-5 loss: 0.857061  [  192/  306]
train() client id: f_00004-1-6 loss: 0.752403  [  224/  306]
train() client id: f_00004-1-7 loss: 0.629300  [  256/  306]
train() client id: f_00004-1-8 loss: 0.788980  [  288/  306]
train() client id: f_00004-2-0 loss: 0.735837  [   32/  306]
train() client id: f_00004-2-1 loss: 0.775052  [   64/  306]
train() client id: f_00004-2-2 loss: 0.771468  [   96/  306]
train() client id: f_00004-2-3 loss: 0.832664  [  128/  306]
train() client id: f_00004-2-4 loss: 0.803385  [  160/  306]
train() client id: f_00004-2-5 loss: 0.659655  [  192/  306]
train() client id: f_00004-2-6 loss: 0.711463  [  224/  306]
train() client id: f_00004-2-7 loss: 0.546587  [  256/  306]
train() client id: f_00004-2-8 loss: 0.791208  [  288/  306]
train() client id: f_00004-3-0 loss: 0.833580  [   32/  306]
train() client id: f_00004-3-1 loss: 0.763536  [   64/  306]
train() client id: f_00004-3-2 loss: 0.789762  [   96/  306]
train() client id: f_00004-3-3 loss: 0.778865  [  128/  306]
train() client id: f_00004-3-4 loss: 0.687319  [  160/  306]
train() client id: f_00004-3-5 loss: 0.799849  [  192/  306]
train() client id: f_00004-3-6 loss: 0.672689  [  224/  306]
train() client id: f_00004-3-7 loss: 0.675020  [  256/  306]
train() client id: f_00004-3-8 loss: 0.695745  [  288/  306]
train() client id: f_00005-0-0 loss: 0.343042  [   32/  146]
train() client id: f_00005-0-1 loss: 0.684399  [   64/  146]
train() client id: f_00005-0-2 loss: 0.399684  [   96/  146]
train() client id: f_00005-0-3 loss: 0.385756  [  128/  146]
train() client id: f_00005-1-0 loss: 0.406707  [   32/  146]
train() client id: f_00005-1-1 loss: 0.372146  [   64/  146]
train() client id: f_00005-1-2 loss: 0.627021  [   96/  146]
train() client id: f_00005-1-3 loss: 0.452498  [  128/  146]
train() client id: f_00005-2-0 loss: 0.125198  [   32/  146]
train() client id: f_00005-2-1 loss: 0.295569  [   64/  146]
train() client id: f_00005-2-2 loss: 0.474847  [   96/  146]
train() client id: f_00005-2-3 loss: 0.823773  [  128/  146]
train() client id: f_00005-3-0 loss: 0.632198  [   32/  146]
train() client id: f_00005-3-1 loss: 0.494226  [   64/  146]
train() client id: f_00005-3-2 loss: 0.263818  [   96/  146]
train() client id: f_00005-3-3 loss: 0.287553  [  128/  146]
train() client id: f_00006-0-0 loss: 0.529428  [   32/   54]
train() client id: f_00006-1-0 loss: 0.490577  [   32/   54]
train() client id: f_00006-2-0 loss: 0.588185  [   32/   54]
train() client id: f_00006-3-0 loss: 0.496694  [   32/   54]
train() client id: f_00007-0-0 loss: 0.485234  [   32/  179]
train() client id: f_00007-0-1 loss: 0.682501  [   64/  179]
train() client id: f_00007-0-2 loss: 0.608516  [   96/  179]
train() client id: f_00007-0-3 loss: 0.820085  [  128/  179]
train() client id: f_00007-0-4 loss: 0.687958  [  160/  179]
train() client id: f_00007-1-0 loss: 0.563551  [   32/  179]
train() client id: f_00007-1-1 loss: 0.479499  [   64/  179]
train() client id: f_00007-1-2 loss: 0.771148  [   96/  179]
train() client id: f_00007-1-3 loss: 0.622909  [  128/  179]
train() client id: f_00007-1-4 loss: 0.805869  [  160/  179]
train() client id: f_00007-2-0 loss: 0.650190  [   32/  179]
train() client id: f_00007-2-1 loss: 0.551439  [   64/  179]
train() client id: f_00007-2-2 loss: 0.484052  [   96/  179]
train() client id: f_00007-2-3 loss: 0.670927  [  128/  179]
train() client id: f_00007-2-4 loss: 0.697938  [  160/  179]
train() client id: f_00007-3-0 loss: 0.742139  [   32/  179]
train() client id: f_00007-3-1 loss: 0.743144  [   64/  179]
train() client id: f_00007-3-2 loss: 0.544618  [   96/  179]
train() client id: f_00007-3-3 loss: 0.637952  [  128/  179]
train() client id: f_00007-3-4 loss: 0.471101  [  160/  179]
train() client id: f_00008-0-0 loss: 0.669426  [   32/  130]
train() client id: f_00008-0-1 loss: 0.716868  [   64/  130]
train() client id: f_00008-0-2 loss: 0.657197  [   96/  130]
train() client id: f_00008-0-3 loss: 0.620756  [  128/  130]
train() client id: f_00008-1-0 loss: 0.612161  [   32/  130]
train() client id: f_00008-1-1 loss: 0.701246  [   64/  130]
train() client id: f_00008-1-2 loss: 0.751901  [   96/  130]
train() client id: f_00008-1-3 loss: 0.638092  [  128/  130]
train() client id: f_00008-2-0 loss: 0.721457  [   32/  130]
train() client id: f_00008-2-1 loss: 0.697966  [   64/  130]
train() client id: f_00008-2-2 loss: 0.676885  [   96/  130]
train() client id: f_00008-2-3 loss: 0.574961  [  128/  130]
train() client id: f_00008-3-0 loss: 0.651784  [   32/  130]
train() client id: f_00008-3-1 loss: 0.721882  [   64/  130]
train() client id: f_00008-3-2 loss: 0.705266  [   96/  130]
train() client id: f_00008-3-3 loss: 0.634002  [  128/  130]
train() client id: f_00009-0-0 loss: 0.956796  [   32/  118]
train() client id: f_00009-0-1 loss: 0.763388  [   64/  118]
train() client id: f_00009-0-2 loss: 0.672813  [   96/  118]
train() client id: f_00009-1-0 loss: 0.779259  [   32/  118]
train() client id: f_00009-1-1 loss: 0.675770  [   64/  118]
train() client id: f_00009-1-2 loss: 0.739728  [   96/  118]
train() client id: f_00009-2-0 loss: 0.575690  [   32/  118]
train() client id: f_00009-2-1 loss: 0.681048  [   64/  118]
train() client id: f_00009-2-2 loss: 0.950180  [   96/  118]
train() client id: f_00009-3-0 loss: 0.609380  [   32/  118]
train() client id: f_00009-3-1 loss: 0.837827  [   64/  118]
train() client id: f_00009-3-2 loss: 0.733547  [   96/  118]
At round 78 accuracy: 0.6472148541114059
At round 78 training accuracy: 0.5875251509054326
At round 78 training loss: 0.8270338437590136
update_location
xs = 8.927491 511.223621 5.882650 0.934260 -427.581990 -275.230757 -235.849135 -5.143845 -450.120581 20.134486 
ys = -502.390647 7.291448 400.684448 -222.290817 -9.642386 0.794442 -1.381692 396.628436 25.881276 -937.232496 
xs mean: -84.68237997052123
ys mean: -84.16579882624052
dists_uav = 512.324177 520.963296 413.016504 243.750037 439.225834 292.835450 256.177133 409.072824 461.820721 942.767282 
uav_gains = -124.572349 -124.766915 -121.862570 -110.774468 -122.690793 -115.120801 -111.830004 -121.727316 -123.326951 -131.336620 
uav_gains_db_mean: -120.8008787978392
dists_bs = 701.916954 706.419858 289.226560 434.503000 312.874531 200.982042 186.582739 279.231583 312.934063 1129.220347 
bs_gains = -119.263060 -119.340820 -108.481728 -113.430857 -109.437425 -104.055462 -103.151460 -108.054066 -109.439739 -125.044863 
bs_gains_db_mean: -111.96994799528093
Round 79
-------------------------------
ene_coms = [0.05552165 0.05740939 0.00968326 0.01127985 0.01029389 0.0161069
 0.01219028 0.00943176 0.01029546 0.06025011]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.49337147 0.97040017 0.43461238 0.15968641 1.05710285 0.51705162
 0.198299   0.62223369 0.45580474 0.4713961 ]
obj_prev = 5.379958431148795
eta_min = 1.2841795960048521e-200	eta_max = 0.9751671909011576
af = 1.0950793819260525	bf = 0.28313192590118325	zeta = 1.2045873201186579	eta = 0.909090909090909
af = 1.0950793819260525	bf = 0.28313192590118325	zeta = 4.262545525618201	eta = 0.25690737502850064
af = 1.0950793819260525	bf = 0.28313192590118325	zeta = 2.383805444263409	eta = 0.45938286807815804
af = 1.0950793819260525	bf = 0.28313192590118325	zeta = 2.099383576487769	eta = 0.5216194859245782
af = 1.0950793819260525	bf = 0.28313192590118325	zeta = 2.0816652797029414	eta = 0.526059300985374
af = 1.0950793819260525	bf = 0.28313192590118325	zeta = 2.081582522814668	eta = 0.5260802153763817
eta = 0.5260802153763817
ene_coms = [0.05552165 0.05740939 0.00968326 0.01127985 0.01029389 0.0161069
 0.01219028 0.00943176 0.01029546 0.06025011]
ene_comp = [0.05300119 0.11147075 0.0521599  0.01808771 0.12871717 0.06141407
 0.02271479 0.07529534 0.05468376 0.04963603]
ene_total = [0.25680779 0.39963695 0.14634528 0.06949522 0.32895493 0.18344516
 0.08259917 0.20049769 0.15376644 0.2600339 ]
ti_comp = [10.42185491 10.40297749 10.88023878 10.86427286 10.8741325  10.81600243
 10.85516856 10.88275383 10.87411684 10.37457033]
ti_coms = [0.55521648 0.5740939  0.09683261 0.11279853 0.10293889 0.16106896
 0.12190283 0.09431756 0.10295455 0.60250106]
t_total = [25.97607613 25.97607613 25.97607613 25.97607613 25.97607613 25.97607613
 25.97607613 25.97607613 25.97607613 25.97607613]
ene_coms = [0.05552165 0.05740939 0.00968326 0.01127985 0.01029389 0.0161069
 0.01219028 0.00943176 0.01029546 0.06025011]
ene_comp = [8.56735477e-08 7.99921676e-07 7.49226903e-08 3.13349639e-09
 1.12719797e-06 1.23751416e-07 6.21631907e-09 2.25271704e-07
 8.64305404e-08 7.10118692e-08]
ene_total = [0.1313863  0.13585513 0.02291459 0.02669258 0.02436206 0.03811556
 0.02884703 0.02231979 0.02436331 0.14257566]
optimize_network iter = 0 obj = 0.5974320019931546
eta = 0.5260802153763817
freqs = [2542790.46730605 5357636.78149362 2397001.52852473  832439.81376172
 5918502.81188551 2839037.51882645 1046266.3086613  3459388.02701173
 2514400.24088596 2392196.9943955 ]
eta_min = 0.5260802153763824	eta_max = 0.9022769360023067
af = 4.545856297961583e-06	bf = 0.28313192590118325	zeta = 5.000441927757742e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.05552165 0.05740939 0.00968326 0.01127985 0.01029389 0.0161069
 0.01219028 0.00943176 0.01029546 0.06025011]
ene_comp = [1.27129709e-08 1.18699193e-07 1.11176671e-08 4.64974889e-10
 1.67263238e-07 1.83632893e-08 9.22430380e-10 3.34277345e-08
 1.28252999e-08 1.05373461e-08]
ene_total = [0.68493133 0.70822037 0.11945566 0.13915155 0.12699047 0.19869957
 0.15038289 0.1163533  0.12700788 0.74326293]
ti_comp = [1.70827427 1.68939685 2.16665814 2.15069222 2.16055186 2.10242179
 2.14158792 2.16917319 2.1605362  1.66098969]
ti_coms = [0.55521648 0.5740939  0.09683261 0.11279853 0.10293889 0.16106896
 0.12190283 0.09431756 0.10295455 0.60250106]
t_total = [25.97607613 25.97607613 25.97607613 25.97607613 25.97607613 25.97607613
 25.97607613 25.97607613 25.97607613 25.97607613]
ene_coms = [0.05552165 0.05740939 0.00968326 0.01127985 0.01029389 0.0161069
 0.01219028 0.00943176 0.01029546 0.06025011]
ene_comp = [1.30861547e-08 1.24477219e-07 7.75353741e-09 3.28143990e-10
 1.17179194e-07 1.34410899e-08 6.55428134e-10 2.32694573e-08
 8.98509305e-09 1.13691439e-08]
ene_total = [0.63717287 0.65883809 0.11112629 0.12944887 0.11813518 0.18484472
 0.13989708 0.10824017 0.11815191 0.69143717]
optimize_network iter = 1 obj = 2.897292346383777
eta = 0.9022769360023067
freqs = [2483675.28427274 5281971.62837259 1927139.54494418  673243.33407369
 4769123.86049679 2338378.86506497  849062.6214326  2778693.68493252
 2026113.0594273  2392196.9943955 ]
eta_min = 0.9022769360023385	eta_max = 0.9022769360023055
af = 3.49578221960663e-06	bf = 0.28313192590118325	zeta = 3.845360441567293e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.05552165 0.05740939 0.00968326 0.01127985 0.01029389 0.0161069
 0.01219028 0.00943176 0.01029546 0.06025011]
ene_comp = [1.21287357e-08 1.15370124e-07 7.18626733e-09 3.04136075e-10
 1.08606042e-07 1.24577028e-08 6.07475212e-10 2.15669999e-08
 8.32771897e-09 1.05373461e-08]
ene_total = [0.68493132 0.70822033 0.11945561 0.13915155 0.12698974 0.1986995
 0.15038288 0.11635315 0.12700783 0.74326293]
ti_comp = [1.70827427 1.68939685 2.16665814 2.15069222 2.16055186 2.10242179
 2.14158792 2.16917319 2.1605362  1.66098969]
ti_coms = [0.55521648 0.5740939  0.09683261 0.11279853 0.10293889 0.16106896
 0.12190283 0.09431756 0.10295455 0.60250106]
t_total = [25.97607613 25.97607613 25.97607613 25.97607613 25.97607613 25.97607613
 25.97607613 25.97607613 25.97607613 25.97607613]
ene_coms = [0.05552165 0.05740939 0.00968326 0.01127985 0.01029389 0.0161069
 0.01219028 0.00943176 0.01029546 0.06025011]
ene_comp = [1.30861547e-08 1.24477219e-07 7.75353741e-09 3.28143990e-10
 1.17179194e-07 1.34410899e-08 6.55428134e-10 2.32694573e-08
 8.98509305e-09 1.13691439e-08]
ene_total = [0.63717287 0.65883809 0.11112629 0.12944887 0.11813518 0.18484472
 0.13989708 0.10824017 0.11815191 0.69143717]
optimize_network iter = 2 obj = 2.8972923463837406
eta = 0.9022769360023055
freqs = [2483675.28427274 5281971.62837258 1927139.54494418  673243.33407369
 4769123.86049679 2338378.86506497  849062.6214326  2778693.68493252
 2026113.0594273  2392196.99439549]
Done!
ene_coms = [0.05552165 0.05740939 0.00968326 0.01127985 0.01029389 0.0161069
 0.01219028 0.00943176 0.01029546 0.06025011]
ene_comp = [1.16587351e-08 1.10899417e-07 6.90779230e-09 2.92350499e-10
 1.04397450e-07 1.19749544e-08 5.83934941e-10 2.07312571e-08
 8.00501155e-09 1.01290134e-08]
ene_total = [0.05552166 0.0574095  0.00968327 0.01127985 0.01029399 0.01610691
 0.01219028 0.00943178 0.01029546 0.06025012]
At round 79 energy consumption: 0.252462823612548
At round 79 eta: 0.9022769360023055
At round 79 a_n: 1.1214809456783108
At round 79 local rounds: 3.3673004694556665
At round 79 global rounds: 11.476113210129895
gradient difference: 0.8830113410949707
train() client id: f_00000-0-0 loss: 0.759674  [   32/  126]
train() client id: f_00000-0-1 loss: 1.121473  [   64/  126]
train() client id: f_00000-0-2 loss: 0.838309  [   96/  126]
train() client id: f_00000-1-0 loss: 1.063638  [   32/  126]
train() client id: f_00000-1-1 loss: 0.778891  [   64/  126]
train() client id: f_00000-1-2 loss: 0.822853  [   96/  126]
train() client id: f_00000-2-0 loss: 0.968418  [   32/  126]
train() client id: f_00000-2-1 loss: 0.913229  [   64/  126]
train() client id: f_00000-2-2 loss: 0.821847  [   96/  126]
train() client id: f_00001-0-0 loss: 0.651872  [   32/  265]
train() client id: f_00001-0-1 loss: 0.493424  [   64/  265]
train() client id: f_00001-0-2 loss: 0.543253  [   96/  265]
train() client id: f_00001-0-3 loss: 0.518897  [  128/  265]
train() client id: f_00001-0-4 loss: 0.698231  [  160/  265]
train() client id: f_00001-0-5 loss: 0.554446  [  192/  265]
train() client id: f_00001-0-6 loss: 0.492932  [  224/  265]
train() client id: f_00001-0-7 loss: 0.598140  [  256/  265]
train() client id: f_00001-1-0 loss: 0.463117  [   32/  265]
train() client id: f_00001-1-1 loss: 0.502657  [   64/  265]
train() client id: f_00001-1-2 loss: 0.650392  [   96/  265]
train() client id: f_00001-1-3 loss: 0.723168  [  128/  265]
train() client id: f_00001-1-4 loss: 0.511653  [  160/  265]
train() client id: f_00001-1-5 loss: 0.676234  [  192/  265]
train() client id: f_00001-1-6 loss: 0.620329  [  224/  265]
train() client id: f_00001-1-7 loss: 0.482398  [  256/  265]
train() client id: f_00001-2-0 loss: 0.522442  [   32/  265]
train() client id: f_00001-2-1 loss: 0.628654  [   64/  265]
train() client id: f_00001-2-2 loss: 0.621348  [   96/  265]
train() client id: f_00001-2-3 loss: 0.570117  [  128/  265]
train() client id: f_00001-2-4 loss: 0.617960  [  160/  265]
train() client id: f_00001-2-5 loss: 0.467535  [  192/  265]
train() client id: f_00001-2-6 loss: 0.597943  [  224/  265]
train() client id: f_00001-2-7 loss: 0.638769  [  256/  265]
train() client id: f_00002-0-0 loss: 0.906373  [   32/  124]
train() client id: f_00002-0-1 loss: 0.856880  [   64/  124]
train() client id: f_00002-0-2 loss: 0.908848  [   96/  124]
train() client id: f_00002-1-0 loss: 0.889350  [   32/  124]
train() client id: f_00002-1-1 loss: 0.739347  [   64/  124]
train() client id: f_00002-1-2 loss: 0.905669  [   96/  124]
train() client id: f_00002-2-0 loss: 0.949865  [   32/  124]
train() client id: f_00002-2-1 loss: 0.787937  [   64/  124]
train() client id: f_00002-2-2 loss: 0.904077  [   96/  124]
train() client id: f_00003-0-0 loss: 0.534072  [   32/   43]
train() client id: f_00003-1-0 loss: 0.361178  [   32/   43]
train() client id: f_00003-2-0 loss: 0.428145  [   32/   43]
train() client id: f_00004-0-0 loss: 0.962858  [   32/  306]
train() client id: f_00004-0-1 loss: 0.869143  [   64/  306]
train() client id: f_00004-0-2 loss: 0.862243  [   96/  306]
train() client id: f_00004-0-3 loss: 0.927375  [  128/  306]
train() client id: f_00004-0-4 loss: 1.012783  [  160/  306]
train() client id: f_00004-0-5 loss: 1.016057  [  192/  306]
train() client id: f_00004-0-6 loss: 0.847853  [  224/  306]
train() client id: f_00004-0-7 loss: 0.831644  [  256/  306]
train() client id: f_00004-0-8 loss: 0.709084  [  288/  306]
train() client id: f_00004-1-0 loss: 0.924886  [   32/  306]
train() client id: f_00004-1-1 loss: 0.871605  [   64/  306]
train() client id: f_00004-1-2 loss: 0.772027  [   96/  306]
train() client id: f_00004-1-3 loss: 0.820436  [  128/  306]
train() client id: f_00004-1-4 loss: 0.945351  [  160/  306]
train() client id: f_00004-1-5 loss: 0.853017  [  192/  306]
train() client id: f_00004-1-6 loss: 1.041981  [  224/  306]
train() client id: f_00004-1-7 loss: 0.898528  [  256/  306]
train() client id: f_00004-1-8 loss: 0.831629  [  288/  306]
train() client id: f_00004-2-0 loss: 0.875700  [   32/  306]
train() client id: f_00004-2-1 loss: 0.936607  [   64/  306]
train() client id: f_00004-2-2 loss: 0.721850  [   96/  306]
train() client id: f_00004-2-3 loss: 0.746396  [  128/  306]
train() client id: f_00004-2-4 loss: 0.984150  [  160/  306]
train() client id: f_00004-2-5 loss: 0.947444  [  192/  306]
train() client id: f_00004-2-6 loss: 0.767106  [  224/  306]
train() client id: f_00004-2-7 loss: 0.872911  [  256/  306]
train() client id: f_00004-2-8 loss: 1.021555  [  288/  306]
train() client id: f_00005-0-0 loss: 0.915439  [   32/  146]
train() client id: f_00005-0-1 loss: 0.829324  [   64/  146]
train() client id: f_00005-0-2 loss: 0.479050  [   96/  146]
train() client id: f_00005-0-3 loss: 0.927979  [  128/  146]
train() client id: f_00005-1-0 loss: 0.795618  [   32/  146]
train() client id: f_00005-1-1 loss: 0.861136  [   64/  146]
train() client id: f_00005-1-2 loss: 0.588687  [   96/  146]
train() client id: f_00005-1-3 loss: 0.775328  [  128/  146]
train() client id: f_00005-2-0 loss: 1.075699  [   32/  146]
train() client id: f_00005-2-1 loss: 0.619257  [   64/  146]
train() client id: f_00005-2-2 loss: 0.637569  [   96/  146]
train() client id: f_00005-2-3 loss: 0.675081  [  128/  146]
train() client id: f_00006-0-0 loss: 0.515522  [   32/   54]
train() client id: f_00006-1-0 loss: 0.556725  [   32/   54]
train() client id: f_00006-2-0 loss: 0.553313  [   32/   54]
train() client id: f_00007-0-0 loss: 0.645895  [   32/  179]
train() client id: f_00007-0-1 loss: 0.728786  [   64/  179]
train() client id: f_00007-0-2 loss: 0.822361  [   96/  179]
train() client id: f_00007-0-3 loss: 0.677125  [  128/  179]
train() client id: f_00007-0-4 loss: 1.097270  [  160/  179]
train() client id: f_00007-1-0 loss: 0.748677  [   32/  179]
train() client id: f_00007-1-1 loss: 1.062695  [   64/  179]
train() client id: f_00007-1-2 loss: 0.668906  [   96/  179]
train() client id: f_00007-1-3 loss: 0.793227  [  128/  179]
train() client id: f_00007-1-4 loss: 0.673790  [  160/  179]
train() client id: f_00007-2-0 loss: 0.791689  [   32/  179]
train() client id: f_00007-2-1 loss: 0.877475  [   64/  179]
train() client id: f_00007-2-2 loss: 0.761093  [   96/  179]
train() client id: f_00007-2-3 loss: 0.674662  [  128/  179]
train() client id: f_00007-2-4 loss: 0.766880  [  160/  179]
train() client id: f_00008-0-0 loss: 0.817032  [   32/  130]
train() client id: f_00008-0-1 loss: 0.788281  [   64/  130]
train() client id: f_00008-0-2 loss: 0.738396  [   96/  130]
train() client id: f_00008-0-3 loss: 0.719863  [  128/  130]
train() client id: f_00008-1-0 loss: 0.742063  [   32/  130]
train() client id: f_00008-1-1 loss: 0.768768  [   64/  130]
train() client id: f_00008-1-2 loss: 0.777764  [   96/  130]
train() client id: f_00008-1-3 loss: 0.788442  [  128/  130]
train() client id: f_00008-2-0 loss: 0.774317  [   32/  130]
train() client id: f_00008-2-1 loss: 0.730570  [   64/  130]
train() client id: f_00008-2-2 loss: 0.732442  [   96/  130]
train() client id: f_00008-2-3 loss: 0.830506  [  128/  130]
train() client id: f_00009-0-0 loss: 0.715671  [   32/  118]
train() client id: f_00009-0-1 loss: 0.943642  [   64/  118]
train() client id: f_00009-0-2 loss: 0.823099  [   96/  118]
train() client id: f_00009-1-0 loss: 0.856134  [   32/  118]
train() client id: f_00009-1-1 loss: 0.748562  [   64/  118]
train() client id: f_00009-1-2 loss: 0.643188  [   96/  118]
train() client id: f_00009-2-0 loss: 0.821753  [   32/  118]
train() client id: f_00009-2-1 loss: 0.755797  [   64/  118]
train() client id: f_00009-2-2 loss: 0.683528  [   96/  118]
At round 79 accuracy: 0.6472148541114059
At round 79 training accuracy: 0.5922199865861838
At round 79 training loss: 0.8323454259396126
update_location
xs = 8.927491 516.223621 5.882650 0.934260 -432.581990 -280.230757 -240.849135 -5.143845 -455.120581 20.134486 
ys = -507.390647 7.291448 405.684448 -227.290817 -9.642386 0.794442 -1.381692 401.628436 25.881276 -942.232496 
xs mean: -86.18237997052123
ys mean: -84.66579882624052
dists_uav = 517.228159 525.870699 417.868971 248.318321 444.094757 297.539760 260.787682 413.922529 466.695386 947.738083 
uav_gains = -124.683353 -124.875456 -122.024782 -111.153687 -122.833159 -115.523820 -112.238331 -121.893202 -123.456713 -131.393922 
uav_gains_db_mean: -121.00764238767559
dists_bs = 706.743459 711.277901 293.144756 439.079452 316.924742 203.521716 188.272701 283.216457 317.338831 1134.145457 
bs_gains = -119.346389 -119.424160 -108.645359 -113.558266 -109.593831 -104.208160 -103.261105 -108.226377 -109.609709 -125.097784 
bs_gains_db_mean: -112.09711414604627
Round 80
-------------------------------
ene_coms = [0.05658876 0.05849807 0.00978289 0.01159212 0.01040079 0.01673921
 0.01257943 0.00953158 0.01041176 0.06081098]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.34351538 0.67485711 0.30194249 0.11115739 0.73430479 0.35962055
 0.13803661 0.43225678 0.31667496 0.32785388]
obj_prev = 3.740219943561181
eta_min = 2.4420381972638337e-288	eta_max = 0.9968703076673727
af = 0.7605976452313506	bf = 0.20013614477340588	zeta = 0.8366574097544858	eta = 0.9090909090909091
af = 0.7605976452313506	bf = 0.20013614477340588	zeta = 2.9989172740906715	eta = 0.253624083532607
af = 0.7605976452313506	bf = 0.20013614477340588	zeta = 1.6661867294496426	eta = 0.45649003907418184
af = 0.7605976452313506	bf = 0.20013614477340588	zeta = 1.4656364225857599	eta = 0.5189538370569835
af = 0.7605976452313506	bf = 0.20013614477340588	zeta = 1.4531720258568177	eta = 0.5234050970551046
af = 0.7605976452313506	bf = 0.20013614477340588	zeta = 1.453113984618476	eta = 0.5234260032471233
eta = 0.5234260032471233
ene_coms = [0.05658876 0.05849807 0.00978289 0.01159212 0.01040079 0.01673921
 0.01257943 0.00953158 0.01041176 0.06081098]
ene_comp = [0.05341856 0.11234857 0.05257065 0.01823014 0.1297308  0.0618977
 0.02289367 0.07588828 0.05511439 0.05002691]
ene_total = [0.17980117 0.27923984 0.10191358 0.04874292 0.22903769 0.12852788
 0.05797892 0.13961426 0.10709904 0.18115869]
ti_comp = [15.29582901 15.27673583 15.76388766 15.74579539 15.7577087  15.69432452
 15.73592226 15.76640078 15.75759901 15.25360677]
ti_coms = [0.56588756 0.58498074 0.09782891 0.11592119 0.10400788 0.16739206
 0.12579432 0.0953158  0.10411757 0.60810981]
t_total = [25.92514038 25.92514038 25.92514038 25.92514038 25.92514038 25.92514038
 25.92514038 25.92514038 25.92514038 25.92514038]
ene_coms = [0.05658876 0.05849807 0.00978289 0.01159212 0.01040079 0.01673921
 0.01257943 0.00953158 0.01041176 0.06081098]
ene_comp = [4.07202642e-08 3.79770718e-07 3.65412487e-08 1.52728875e-09
 5.49570150e-07 6.01753053e-08 3.02859966e-09 1.09885139e-07
 4.21400415e-08 3.36314755e-08]
ene_total = [0.09249141 0.09561264 0.01598968 0.01894671 0.01700044 0.02735945
 0.02056042 0.01557905 0.01701754 0.0993924 ]
optimize_network iter = 0 obj = 0.4199497355097916
eta = 0.5234260032471233
freqs = [1746180.71468129 3677112.92653205 1667439.23496812  578889.28961994
 4116423.23787601 1971977.20240916  727433.3720328  2406645.54581658
 1748819.43131435 1639838.66958218]
eta_min = 0.5234260032471237	eta_max = 0.9314708162779777
af = 1.5124728139073356e-06	bf = 0.20013614477340588	zeta = 1.6637200952980694e-06	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.05658876 0.05849807 0.00978289 0.01159212 0.01040079 0.01673921
 0.01257943 0.00953158 0.01041176 0.06081098]
ene_comp = [5.99520830e-09 5.59133053e-08 5.37993557e-09 2.24861364e-10
 8.09127247e-08 8.85955671e-09 4.45898036e-10 1.61782913e-08
 6.20424086e-09 4.95153224e-09]
ene_total = [0.4848687  0.50122871 0.08382266 0.0993246  0.08911762 0.14342638
 0.10778418 0.08166944 0.08921096 0.52104592]
ti_comp = [2.45982144 2.44072826 2.92788009 2.90978782 2.92170113 2.85831695
 2.89991468 2.93039321 2.92159144 2.4175992 ]
ti_coms = [0.56588756 0.58498074 0.09782891 0.11592119 0.10400788 0.16739206
 0.12579432 0.0953158  0.10411757 0.60810981]
t_total = [25.92514038 25.92514038 25.92514038 25.92514038 25.92514038 25.92514038
 25.92514038 25.92514038 25.92514038 25.92514038]
ene_coms = [0.05658876 0.05849807 0.00978289 0.01159212 0.01040079 0.01673921
 0.01257943 0.00953158 0.01041176 0.06081098]
ene_comp = [5.02493596e-09 4.74815430e-08 3.38052805e-09 1.42727871e-10
 5.10174521e-08 5.78980400e-09 2.84601313e-10 1.01515734e-08
 3.91216490e-09 4.27271060e-09]
ene_total = [0.4848687  0.50122864 0.08382264 0.09932459 0.08911736 0.14342636
 0.10778418 0.08166939 0.08921094 0.52104592]
optimize_network iter = 1 obj = 2.2014987188775663
eta = 0.909090909090909
freqs = [1598646.03452686 3388533.59948498 1321763.48798684  461203.38631832
 3268669.35104322 1594145.01145806  581157.5595784  1906393.22095085
 1388702.51335055 1523291.63325063]
eta_min = 0.909090909090944	eta_max = 0.9090909090909075
af = 1.0743582948292162e-06	bf = 0.20013614477340588	zeta = 1.181794124312138e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.05658876 0.05849807 0.00978289 0.01159212 0.01040079 0.01673921
 0.01257943 0.00953158 0.01041176 0.06081098]
ene_comp = [5.02493596e-09 4.74815430e-08 3.38052805e-09 1.42727871e-10
 5.10174521e-08 5.78980400e-09 2.84601313e-10 1.01515734e-08
 3.91216490e-09 4.27271060e-09]
ene_total = [0.4848687  0.50122864 0.08382264 0.09932459 0.08911736 0.14342636
 0.10778418 0.08166939 0.08921094 0.52104592]
ti_comp = [2.45982144 2.44072826 2.92788009 2.90978782 2.92170113 2.85831695
 2.89991468 2.93039321 2.92159144 2.4175992 ]
ti_coms = [0.56588756 0.58498074 0.09782891 0.11592119 0.10400788 0.16739206
 0.12579432 0.0953158  0.10411757 0.60810981]
t_total = [25.92514038 25.92514038 25.92514038 25.92514038 25.92514038 25.92514038
 25.92514038 25.92514038 25.92514038 25.92514038]
ene_coms = [0.05658876 0.05849807 0.00978289 0.01159212 0.01040079 0.01673921
 0.01257943 0.00953158 0.01041176 0.06081098]
ene_comp = [5.02493596e-09 4.74815430e-08 3.38052805e-09 1.42727871e-10
 5.10174521e-08 5.78980400e-09 2.84601313e-10 1.01515734e-08
 3.91216490e-09 4.27271060e-09]
ene_total = [0.4848687  0.50122864 0.08382264 0.09932459 0.08911736 0.14342636
 0.10778418 0.08166939 0.08921094 0.52104592]
optimize_network iter = 2 obj = 2.201498718877531
eta = 0.9090909090909075
freqs = [1598646.03452685 3388533.59948497 1321763.48798684  461203.38631832
 3268669.35104322 1594145.01145806  581157.5595784  1906393.22095085
 1388702.51335055 1523291.63325063]
Done!
ene_coms = [0.05658876 0.05849807 0.00978289 0.01159212 0.01040079 0.01673921
 0.01257943 0.00953158 0.01041176 0.06081098]
ene_comp = [4.83021468e-09 4.56415858e-08 3.24952922e-09 1.37197023e-10
 4.90404749e-08 5.56544332e-09 2.73572728e-10 9.75818978e-09
 3.76056461e-09 4.10713880e-09]
ene_total = [0.05658876 0.05849812 0.00978289 0.01159212 0.01040084 0.01673921
 0.01257943 0.00953159 0.01041176 0.06081098]
At round 80 energy consumption: 0.25693571073430554
At round 80 eta: 0.9090909090909075
At round 80 a_n: 0.7789350987089918
At round 80 local rounds: 3.1209395205778097
At round 80 global rounds: 8.56828608579876
gradient difference: 0.9697467088699341
train() client id: f_00000-0-0 loss: 0.899429  [   32/  126]
train() client id: f_00000-0-1 loss: 1.167758  [   64/  126]
train() client id: f_00000-0-2 loss: 1.106212  [   96/  126]
train() client id: f_00000-1-0 loss: 0.831672  [   32/  126]
train() client id: f_00000-1-1 loss: 1.140783  [   64/  126]
train() client id: f_00000-1-2 loss: 1.049863  [   96/  126]
train() client id: f_00000-2-0 loss: 0.871058  [   32/  126]
train() client id: f_00000-2-1 loss: 1.035009  [   64/  126]
train() client id: f_00000-2-2 loss: 0.927071  [   96/  126]
train() client id: f_00001-0-0 loss: 0.490562  [   32/  265]
train() client id: f_00001-0-1 loss: 0.530711  [   64/  265]
train() client id: f_00001-0-2 loss: 0.482581  [   96/  265]
train() client id: f_00001-0-3 loss: 0.420182  [  128/  265]
train() client id: f_00001-0-4 loss: 0.413407  [  160/  265]
train() client id: f_00001-0-5 loss: 0.540916  [  192/  265]
train() client id: f_00001-0-6 loss: 0.475850  [  224/  265]
train() client id: f_00001-0-7 loss: 0.431993  [  256/  265]
train() client id: f_00001-1-0 loss: 0.384632  [   32/  265]
train() client id: f_00001-1-1 loss: 0.529346  [   64/  265]
train() client id: f_00001-1-2 loss: 0.532931  [   96/  265]
train() client id: f_00001-1-3 loss: 0.371249  [  128/  265]
train() client id: f_00001-1-4 loss: 0.570751  [  160/  265]
train() client id: f_00001-1-5 loss: 0.487284  [  192/  265]
train() client id: f_00001-1-6 loss: 0.434716  [  224/  265]
train() client id: f_00001-1-7 loss: 0.503671  [  256/  265]
train() client id: f_00001-2-0 loss: 0.425223  [   32/  265]
train() client id: f_00001-2-1 loss: 0.581970  [   64/  265]
train() client id: f_00001-2-2 loss: 0.537317  [   96/  265]
train() client id: f_00001-2-3 loss: 0.465696  [  128/  265]
train() client id: f_00001-2-4 loss: 0.382488  [  160/  265]
train() client id: f_00001-2-5 loss: 0.380664  [  192/  265]
train() client id: f_00001-2-6 loss: 0.551710  [  224/  265]
train() client id: f_00001-2-7 loss: 0.476195  [  256/  265]
train() client id: f_00002-0-0 loss: 0.634539  [   32/  124]
train() client id: f_00002-0-1 loss: 0.917404  [   64/  124]
train() client id: f_00002-0-2 loss: 0.889118  [   96/  124]
train() client id: f_00002-1-0 loss: 0.614243  [   32/  124]
train() client id: f_00002-1-1 loss: 0.879504  [   64/  124]
train() client id: f_00002-1-2 loss: 0.871956  [   96/  124]
train() client id: f_00002-2-0 loss: 0.806618  [   32/  124]
train() client id: f_00002-2-1 loss: 0.842291  [   64/  124]
train() client id: f_00002-2-2 loss: 0.847763  [   96/  124]
train() client id: f_00003-0-0 loss: 0.758176  [   32/   43]
train() client id: f_00003-1-0 loss: 0.710113  [   32/   43]
train() client id: f_00003-2-0 loss: 0.568059  [   32/   43]
train() client id: f_00004-0-0 loss: 0.655134  [   32/  306]
train() client id: f_00004-0-1 loss: 0.757283  [   64/  306]
train() client id: f_00004-0-2 loss: 0.715758  [   96/  306]
train() client id: f_00004-0-3 loss: 0.587378  [  128/  306]
train() client id: f_00004-0-4 loss: 0.701628  [  160/  306]
train() client id: f_00004-0-5 loss: 0.677322  [  192/  306]
train() client id: f_00004-0-6 loss: 0.691328  [  224/  306]
train() client id: f_00004-0-7 loss: 0.706721  [  256/  306]
train() client id: f_00004-0-8 loss: 0.626365  [  288/  306]
train() client id: f_00004-1-0 loss: 0.669898  [   32/  306]
train() client id: f_00004-1-1 loss: 0.753741  [   64/  306]
train() client id: f_00004-1-2 loss: 0.592715  [   96/  306]
train() client id: f_00004-1-3 loss: 0.627842  [  128/  306]
train() client id: f_00004-1-4 loss: 0.722342  [  160/  306]
train() client id: f_00004-1-5 loss: 0.742406  [  192/  306]
train() client id: f_00004-1-6 loss: 0.701166  [  224/  306]
train() client id: f_00004-1-7 loss: 0.616236  [  256/  306]
train() client id: f_00004-1-8 loss: 0.738364  [  288/  306]
train() client id: f_00004-2-0 loss: 0.727724  [   32/  306]
train() client id: f_00004-2-1 loss: 0.686726  [   64/  306]
train() client id: f_00004-2-2 loss: 0.712819  [   96/  306]
train() client id: f_00004-2-3 loss: 0.619071  [  128/  306]
train() client id: f_00004-2-4 loss: 0.627448  [  160/  306]
train() client id: f_00004-2-5 loss: 0.764356  [  192/  306]
train() client id: f_00004-2-6 loss: 0.732601  [  224/  306]
train() client id: f_00004-2-7 loss: 0.686788  [  256/  306]
train() client id: f_00004-2-8 loss: 0.722348  [  288/  306]
train() client id: f_00005-0-0 loss: 0.849595  [   32/  146]
train() client id: f_00005-0-1 loss: 0.600668  [   64/  146]
train() client id: f_00005-0-2 loss: 0.978349  [   96/  146]
train() client id: f_00005-0-3 loss: 0.405506  [  128/  146]
train() client id: f_00005-1-0 loss: 0.625743  [   32/  146]
train() client id: f_00005-1-1 loss: 0.514599  [   64/  146]
train() client id: f_00005-1-2 loss: 0.862766  [   96/  146]
train() client id: f_00005-1-3 loss: 0.901942  [  128/  146]
train() client id: f_00005-2-0 loss: 0.586467  [   32/  146]
train() client id: f_00005-2-1 loss: 0.701411  [   64/  146]
train() client id: f_00005-2-2 loss: 0.935381  [   96/  146]
train() client id: f_00005-2-3 loss: 0.740809  [  128/  146]
train() client id: f_00006-0-0 loss: 0.482207  [   32/   54]
train() client id: f_00006-1-0 loss: 0.479640  [   32/   54]
train() client id: f_00006-2-0 loss: 0.448841  [   32/   54]
train() client id: f_00007-0-0 loss: 0.650645  [   32/  179]
train() client id: f_00007-0-1 loss: 0.645689  [   64/  179]
train() client id: f_00007-0-2 loss: 0.694690  [   96/  179]
train() client id: f_00007-0-3 loss: 0.655450  [  128/  179]
train() client id: f_00007-0-4 loss: 0.945248  [  160/  179]
train() client id: f_00007-1-0 loss: 0.826416  [   32/  179]
train() client id: f_00007-1-1 loss: 0.671460  [   64/  179]
train() client id: f_00007-1-2 loss: 0.586128  [   96/  179]
train() client id: f_00007-1-3 loss: 0.719158  [  128/  179]
train() client id: f_00007-1-4 loss: 0.811808  [  160/  179]
train() client id: f_00007-2-0 loss: 0.665690  [   32/  179]
train() client id: f_00007-2-1 loss: 0.774472  [   64/  179]
train() client id: f_00007-2-2 loss: 0.851551  [   96/  179]
train() client id: f_00007-2-3 loss: 0.758460  [  128/  179]
train() client id: f_00007-2-4 loss: 0.547583  [  160/  179]
train() client id: f_00008-0-0 loss: 0.685162  [   32/  130]
train() client id: f_00008-0-1 loss: 0.688773  [   64/  130]
train() client id: f_00008-0-2 loss: 0.635292  [   96/  130]
train() client id: f_00008-0-3 loss: 0.719300  [  128/  130]
train() client id: f_00008-1-0 loss: 0.644315  [   32/  130]
train() client id: f_00008-1-1 loss: 0.704521  [   64/  130]
train() client id: f_00008-1-2 loss: 0.669916  [   96/  130]
train() client id: f_00008-1-3 loss: 0.708801  [  128/  130]
train() client id: f_00008-2-0 loss: 0.892575  [   32/  130]
train() client id: f_00008-2-1 loss: 0.654492  [   64/  130]
train() client id: f_00008-2-2 loss: 0.635570  [   96/  130]
train() client id: f_00008-2-3 loss: 0.546279  [  128/  130]
train() client id: f_00009-0-0 loss: 0.716874  [   32/  118]
train() client id: f_00009-0-1 loss: 1.066658  [   64/  118]
train() client id: f_00009-0-2 loss: 0.885331  [   96/  118]
train() client id: f_00009-1-0 loss: 0.884265  [   32/  118]
train() client id: f_00009-1-1 loss: 0.902525  [   64/  118]
train() client id: f_00009-1-2 loss: 0.983583  [   96/  118]
train() client id: f_00009-2-0 loss: 0.840548  [   32/  118]
train() client id: f_00009-2-1 loss: 0.995788  [   64/  118]
train() client id: f_00009-2-2 loss: 0.912403  [   96/  118]
At round 80 accuracy: 0.6445623342175066
At round 80 training accuracy: 0.5942320590207915
At round 80 training loss: 0.8186798364363765
update_location
xs = 8.927491 521.223621 5.882650 0.934260 -437.581990 -285.230757 -245.849135 -5.143845 -460.120581 20.134486 
ys = -512.390647 7.291448 410.684448 -232.290817 -9.642386 0.794442 -1.381692 406.628436 25.881276 -947.232496 
xs mean: -87.68237997052123
ys mean: -85.16579882624052
dists_uav = 522.133963 530.779830 422.724877 252.902939 448.966561 302.253563 265.412333 418.775768 471.572677 952.709189 
uav_gains = -124.792935 -124.982665 -122.182757 -111.544869 -122.972551 -115.916986 -112.654077 -122.054604 -123.584206 -131.450923 
uav_gains_db_mean: -121.2136572593493
dists_bs = 711.572360 716.137898 297.095426 443.665047 321.001732 206.151392 190.079198 287.233086 321.760997 1139.071219 
bs_gains = -119.429193 -119.506966 -108.808146 -113.684605 -109.749266 -104.364275 -103.377228 -108.397624 -109.777994 -125.150484 
bs_gains_db_mean: -112.22457803363599
Round 81
-------------------------------
ene_coms = [0.05766801 0.05959924 0.00988395 0.01193134 0.01050911 0.01739792
 0.01299944 0.0096328  0.01052936 0.06137605]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.1929262  0.37856617 0.16920429 0.06242415 0.41143339 0.20176355
 0.07751853 0.24221141 0.17746527 0.18392537]
obj_prev = 2.09743832746785
eta_min = 0.0	eta_max = inf
af = 0.4261159085366522	bf = 0.11412766642495258	zeta = 0.4687274993903175	eta = 0.909090909090909
af = 0.4261159085366522	bf = 0.11412766642495258	zeta = 1.7021493531357033	eta = 0.25033990569138986
af = 0.4261159085366522	bf = 0.11412766642495258	zeta = 0.9394534797388211	eta = 0.4535785089168198
af = 0.4261159085366522	bf = 0.11412766642495258	zeta = 0.8253872363684	eta = 0.5162618099251312
af = 0.4261159085366522	bf = 0.11412766642495258	zeta = 0.8183155322989955	eta = 0.5207232317093039
af = 0.4261159085366522	bf = 0.11412766642495258	zeta = 0.8182827126788732	eta = 0.5207441168366429
eta = 0.5207441168366429
ene_coms = [0.05766801 0.05959924 0.00988395 0.01193134 0.01050911 0.01739792
 0.01299944 0.0096328  0.01052936 0.06137605]
ene_comp = [0.05384245 0.11324007 0.0529878  0.0183748  0.13076023 0.06238887
 0.02307533 0.07649046 0.05555173 0.05042388]
ene_total = [0.1015365  0.15737984 0.05724825 0.02759544 0.12863362 0.07265033
 0.0328481  0.07842004 0.06017052 0.10180008]
ti_comp = [27.8391545  27.8198422  28.31699505 28.29652115 28.31074352 28.2418554
 28.28584019 28.3195066  28.310541   27.80207411]
ti_coms = [0.57668009 0.59599239 0.09883954 0.11931344 0.10509107 0.17397919
 0.1299944  0.09632799 0.10529359 0.61376048]
t_total = [25.87420464 25.87420464 25.87420464 25.87420464 25.87420464 25.87420464
 25.87420464 25.87420464 25.87420464 25.87420464]
ene_coms = [0.05766801 0.05959924 0.00988395 0.01193134 0.01050911 0.01739792
 0.01299944 0.0096328  0.01052936 0.06137605]
ene_comp = [1.25875817e-08 1.17265712e-07 1.15961406e-08 4.84263820e-10
 1.74342985e-07 1.90289518e-08 9.59811305e-10 3.48763057e-08
 1.33683088e-08 1.03665531e-08]
ene_total = [0.05250995 0.05426854 0.0089999  0.01086416 0.00956929 0.01584179
 0.01183672 0.00877123 0.00958758 0.05588633]
optimize_network iter = 0 obj = 0.23813549870594172
eta = 0.5207441168366429
freqs = [ 967027.33791517 2035239.20393204  935618.43035271  324683.08071381
 2309374.67555765 1104546.18164567  407895.50962111 1350490.69549219
  981113.90680799  906836.63870167]
eta_min = 0.5207441168366433	eta_max = 0.9612404427882244
af = 2.6409323415185946e-07	bf = 0.11412766642495258	zeta = 2.9050255756704544e-07	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.05766801 0.05959924 0.00988395 0.01193134 0.01050911 0.01739792
 0.01299944 0.0096328  0.01052936 0.06137605]
ene_comp = [1.83866837e-09 1.71290056e-08 1.69384856e-09 7.07364286e-11
 2.54662842e-08 2.77955947e-09 1.40199662e-10 5.09438286e-09
 1.95270920e-09 1.51424267e-09]
ene_total = [0.2768227  0.28609322 0.04744577 0.05727381 0.0504468  0.08351493
 0.06240097 0.04624017 0.0505439  0.29462233]
ti_comp = [4.8134628  4.79415049 5.29130335 5.27082945 5.28505182 5.2161637
 5.26014849 5.2938149  5.2848493  4.77638241]
ti_coms = [0.57668009 0.59599239 0.09883954 0.11931344 0.10509107 0.17397919
 0.1299944  0.09632799 0.10529359 0.61376048]
t_total = [25.87420464 25.87420464 25.87420464 25.87420464 25.87420464 25.87420464
 25.87420464 25.87420464 25.87420464 25.87420464]
ene_coms = [0.05766801 0.05959924 0.00988395 0.01193134 0.01050911 0.01739792
 0.01299944 0.0096328  0.01052936 0.06137605]
ene_comp = [1.31226776e-09 1.23066644e-08 1.03506065e-09 4.34984220e-11
 1.55916416e-08 1.73853006e-09 8.64990492e-11 3.11062495e-09
 1.19561376e-09 1.09464653e-09]
ene_total = [0.2768227  0.2860932  0.04744577 0.05727381 0.05044675 0.08351492
 0.06240097 0.04624016 0.0505439  0.29462233]
optimize_network iter = 1 obj = 1.2554045107572747
eta = 0.9090909090909091
freqs = [ 816955.26874053 1725121.0071637   731382.18523701  254609.64114433
 1806997.40639345  873548.44771375  320391.59043102 1055284.67624282
  767708.05536703  771024.66183158]
eta_min = 0.9090909090909156	eta_max = 0.9090909090909084
af = 1.717672067352012e-07	bf = 0.11412766642495258	zeta = 1.8894392740872135e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.05766801 0.05959924 0.00988395 0.01193134 0.01050911 0.01739792
 0.01299944 0.0096328  0.01052936 0.06137605]
ene_comp = [1.31226776e-09 1.23066644e-08 1.03506065e-09 4.34984220e-11
 1.55916416e-08 1.73853006e-09 8.64990492e-11 3.11062495e-09
 1.19561376e-09 1.09464653e-09]
ene_total = [0.2768227  0.2860932  0.04744577 0.05727381 0.05044675 0.08351492
 0.06240097 0.04624016 0.0505439  0.29462233]
ti_comp = [4.8134628  4.79415049 5.29130335 5.27082945 5.28505182 5.2161637
 5.26014849 5.2938149  5.2848493  4.77638241]
ti_coms = [0.57668009 0.59599239 0.09883954 0.11931344 0.10509107 0.17397919
 0.1299944  0.09632799 0.10529359 0.61376048]
t_total = [25.87420464 25.87420464 25.87420464 25.87420464 25.87420464 25.87420464
 25.87420464 25.87420464 25.87420464 25.87420464]
ene_coms = [0.05766801 0.05959924 0.00988395 0.01193134 0.01050911 0.01739792
 0.01299944 0.0096328  0.01052936 0.06137605]
ene_comp = [1.31226776e-09 1.23066644e-08 1.03506065e-09 4.34984220e-11
 1.55916416e-08 1.73853006e-09 8.64990492e-11 3.11062495e-09
 1.19561376e-09 1.09464653e-09]
ene_total = [0.2768227  0.2860932  0.04744577 0.05727381 0.05044675 0.08351492
 0.06240097 0.04624016 0.0505439  0.29462233]
optimize_network iter = 2 obj = 1.2554045107572658
eta = 0.9090909090909084
freqs = [ 816955.26874053 1725121.0071637   731382.18523701  254609.64114433
 1806997.40639345  873548.44771375  320391.59043102 1055284.67624282
  767708.05536702  771024.66183158]
Done!
ene_coms = [0.05766801 0.05959924 0.00988395 0.01193134 0.01050911 0.01739792
 0.01299944 0.0096328  0.01052936 0.06137605]
ene_comp = [1.26141607e-09 1.18297689e-08 9.94951016e-10 4.18128147e-11
 1.49874499e-08 1.67116029e-09 8.31471247e-11 2.99008513e-09
 1.14928253e-09 1.05222788e-09]
ene_total = [0.05766801 0.05959925 0.00988395 0.01193134 0.01050912 0.01739792
 0.01299944 0.0096328  0.01052936 0.06137605]
At round 81 energy consumption: 0.26152725280639755
At round 81 eta: 0.9090909090909084
At round 81 a_n: 0.43638925173967635
At round 81 local rounds: 3.120939520577777
At round 81 global rounds: 4.8002817691364035
gradient difference: 0.9630878567695618
train() client id: f_00000-0-0 loss: 0.972161  [   32/  126]
train() client id: f_00000-0-1 loss: 1.272200  [   64/  126]
train() client id: f_00000-0-2 loss: 0.855096  [   96/  126]
train() client id: f_00000-1-0 loss: 1.132081  [   32/  126]
train() client id: f_00000-1-1 loss: 0.801247  [   64/  126]
train() client id: f_00000-1-2 loss: 0.983994  [   96/  126]
train() client id: f_00000-2-0 loss: 0.939196  [   32/  126]
train() client id: f_00000-2-1 loss: 1.014802  [   64/  126]
train() client id: f_00000-2-2 loss: 0.815832  [   96/  126]
train() client id: f_00001-0-0 loss: 0.486659  [   32/  265]
train() client id: f_00001-0-1 loss: 0.473637  [   64/  265]
train() client id: f_00001-0-2 loss: 0.445706  [   96/  265]
train() client id: f_00001-0-3 loss: 0.547510  [  128/  265]
train() client id: f_00001-0-4 loss: 0.552939  [  160/  265]
train() client id: f_00001-0-5 loss: 0.554668  [  192/  265]
train() client id: f_00001-0-6 loss: 0.482417  [  224/  265]
train() client id: f_00001-0-7 loss: 0.465790  [  256/  265]
train() client id: f_00001-1-0 loss: 0.401602  [   32/  265]
train() client id: f_00001-1-1 loss: 0.430072  [   64/  265]
train() client id: f_00001-1-2 loss: 0.491052  [   96/  265]
train() client id: f_00001-1-3 loss: 0.612894  [  128/  265]
train() client id: f_00001-1-4 loss: 0.397557  [  160/  265]
train() client id: f_00001-1-5 loss: 0.423762  [  192/  265]
train() client id: f_00001-1-6 loss: 0.500148  [  224/  265]
train() client id: f_00001-1-7 loss: 0.661560  [  256/  265]
train() client id: f_00001-2-0 loss: 0.482036  [   32/  265]
train() client id: f_00001-2-1 loss: 0.536255  [   64/  265]
train() client id: f_00001-2-2 loss: 0.410140  [   96/  265]
train() client id: f_00001-2-3 loss: 0.551221  [  128/  265]
train() client id: f_00001-2-4 loss: 0.511249  [  160/  265]
train() client id: f_00001-2-5 loss: 0.463326  [  192/  265]
train() client id: f_00001-2-6 loss: 0.516446  [  224/  265]
train() client id: f_00001-2-7 loss: 0.522535  [  256/  265]
train() client id: f_00002-0-0 loss: 0.705298  [   32/  124]
train() client id: f_00002-0-1 loss: 0.894593  [   64/  124]
train() client id: f_00002-0-2 loss: 1.119611  [   96/  124]
train() client id: f_00002-1-0 loss: 0.759673  [   32/  124]
train() client id: f_00002-1-1 loss: 0.857263  [   64/  124]
train() client id: f_00002-1-2 loss: 0.958492  [   96/  124]
train() client id: f_00002-2-0 loss: 0.842309  [   32/  124]
train() client id: f_00002-2-1 loss: 0.972846  [   64/  124]
train() client id: f_00002-2-2 loss: 0.968644  [   96/  124]
train() client id: f_00003-0-0 loss: 0.725690  [   32/   43]
train() client id: f_00003-1-0 loss: 0.471671  [   32/   43]
train() client id: f_00003-2-0 loss: 0.698120  [   32/   43]
train() client id: f_00004-0-0 loss: 0.708052  [   32/  306]
train() client id: f_00004-0-1 loss: 0.895866  [   64/  306]
train() client id: f_00004-0-2 loss: 0.794250  [   96/  306]
train() client id: f_00004-0-3 loss: 0.897941  [  128/  306]
train() client id: f_00004-0-4 loss: 0.707892  [  160/  306]
train() client id: f_00004-0-5 loss: 0.992720  [  192/  306]
train() client id: f_00004-0-6 loss: 0.786074  [  224/  306]
train() client id: f_00004-0-7 loss: 0.846395  [  256/  306]
train() client id: f_00004-0-8 loss: 0.814973  [  288/  306]
train() client id: f_00004-1-0 loss: 0.719486  [   32/  306]
train() client id: f_00004-1-1 loss: 0.770702  [   64/  306]
train() client id: f_00004-1-2 loss: 0.847685  [   96/  306]
train() client id: f_00004-1-3 loss: 0.745219  [  128/  306]
train() client id: f_00004-1-4 loss: 0.784968  [  160/  306]
train() client id: f_00004-1-5 loss: 0.662165  [  192/  306]
train() client id: f_00004-1-6 loss: 0.896154  [  224/  306]
train() client id: f_00004-1-7 loss: 0.952998  [  256/  306]
train() client id: f_00004-1-8 loss: 0.847412  [  288/  306]
train() client id: f_00004-2-0 loss: 0.674666  [   32/  306]
train() client id: f_00004-2-1 loss: 0.912370  [   64/  306]
train() client id: f_00004-2-2 loss: 0.815951  [   96/  306]
train() client id: f_00004-2-3 loss: 0.853239  [  128/  306]
train() client id: f_00004-2-4 loss: 0.789520  [  160/  306]
train() client id: f_00004-2-5 loss: 0.865630  [  192/  306]
train() client id: f_00004-2-6 loss: 0.705382  [  224/  306]
train() client id: f_00004-2-7 loss: 0.814763  [  256/  306]
train() client id: f_00004-2-8 loss: 0.749422  [  288/  306]
train() client id: f_00005-0-0 loss: 0.755185  [   32/  146]
train() client id: f_00005-0-1 loss: 0.432100  [   64/  146]
train() client id: f_00005-0-2 loss: 0.979148  [   96/  146]
train() client id: f_00005-0-3 loss: 0.503878  [  128/  146]
train() client id: f_00005-1-0 loss: 0.489005  [   32/  146]
train() client id: f_00005-1-1 loss: 0.599809  [   64/  146]
train() client id: f_00005-1-2 loss: 0.785285  [   96/  146]
train() client id: f_00005-1-3 loss: 0.829761  [  128/  146]
train() client id: f_00005-2-0 loss: 0.719233  [   32/  146]
train() client id: f_00005-2-1 loss: 0.764677  [   64/  146]
train() client id: f_00005-2-2 loss: 0.600924  [   96/  146]
train() client id: f_00005-2-3 loss: 0.451653  [  128/  146]
train() client id: f_00006-0-0 loss: 0.506731  [   32/   54]
train() client id: f_00006-1-0 loss: 0.544246  [   32/   54]
train() client id: f_00006-2-0 loss: 0.564686  [   32/   54]
train() client id: f_00007-0-0 loss: 0.765516  [   32/  179]
train() client id: f_00007-0-1 loss: 0.924437  [   64/  179]
train() client id: f_00007-0-2 loss: 0.806389  [   96/  179]
train() client id: f_00007-0-3 loss: 0.618357  [  128/  179]
train() client id: f_00007-0-4 loss: 0.958505  [  160/  179]
train() client id: f_00007-1-0 loss: 0.868547  [   32/  179]
train() client id: f_00007-1-1 loss: 0.688421  [   64/  179]
train() client id: f_00007-1-2 loss: 1.110511  [   96/  179]
train() client id: f_00007-1-3 loss: 0.744312  [  128/  179]
train() client id: f_00007-1-4 loss: 0.628027  [  160/  179]
train() client id: f_00007-2-0 loss: 0.863844  [   32/  179]
train() client id: f_00007-2-1 loss: 0.692238  [   64/  179]
train() client id: f_00007-2-2 loss: 0.973220  [   96/  179]
train() client id: f_00007-2-3 loss: 0.666539  [  128/  179]
train() client id: f_00007-2-4 loss: 0.851947  [  160/  179]
train() client id: f_00008-0-0 loss: 0.801904  [   32/  130]
train() client id: f_00008-0-1 loss: 0.746033  [   64/  130]
train() client id: f_00008-0-2 loss: 0.882092  [   96/  130]
train() client id: f_00008-0-3 loss: 0.929835  [  128/  130]
train() client id: f_00008-1-0 loss: 0.871329  [   32/  130]
train() client id: f_00008-1-1 loss: 0.919392  [   64/  130]
train() client id: f_00008-1-2 loss: 0.771760  [   96/  130]
train() client id: f_00008-1-3 loss: 0.804570  [  128/  130]
train() client id: f_00008-2-0 loss: 0.854230  [   32/  130]
train() client id: f_00008-2-1 loss: 0.844970  [   64/  130]
train() client id: f_00008-2-2 loss: 0.934997  [   96/  130]
train() client id: f_00008-2-3 loss: 0.728363  [  128/  130]
train() client id: f_00009-0-0 loss: 0.672271  [   32/  118]
train() client id: f_00009-0-1 loss: 0.836093  [   64/  118]
train() client id: f_00009-0-2 loss: 0.511267  [   96/  118]
train() client id: f_00009-1-0 loss: 0.858768  [   32/  118]
train() client id: f_00009-1-1 loss: 0.577086  [   64/  118]
train() client id: f_00009-1-2 loss: 0.747277  [   96/  118]
train() client id: f_00009-2-0 loss: 0.646459  [   32/  118]
train() client id: f_00009-2-1 loss: 0.801598  [   64/  118]
train() client id: f_00009-2-2 loss: 0.806337  [   96/  118]
At round 81 accuracy: 0.6472148541114059
At round 81 training accuracy: 0.5915492957746479
At round 81 training loss: 0.8269855474796262
update_location
xs = 8.927491 526.223621 5.882650 0.934260 -442.581990 -290.230757 -250.849135 -5.143845 -465.120581 20.134486 
ys = -517.390647 7.291448 415.684448 -237.290817 -9.642386 0.794442 -1.381692 411.628436 25.881276 -952.232496 
xs mean: -89.18237997052123
ys mean: -85.66579882624052
dists_uav = 527.041537 535.690642 427.584104 257.503019 453.841154 306.976422 270.050361 423.632422 476.452511 957.680596 
uav_gains = -124.901148 -125.088590 -122.336777 -111.946678 -123.109142 -116.299035 -113.074968 -122.211824 -123.709547 -131.507627 
uav_gains_db_mean: -121.41853348463837
dists_bs = 716.403608 720.999809 301.077294 448.259503 325.104494 208.867671 191.998939 291.280158 326.199855 1143.997626 
bs_gains = -119.511477 -119.589243 -108.970043 -113.809885 -109.903702 -104.523453 -103.499426 -108.567764 -109.944605 -125.202963 
bs_gains_db_mean: -112.35225620879277
Round 82
-------------------------------
ene_coms = [0.0587596  0.06071307 0.00998644 0.01229922 0.01061884 0.01808103
 0.01345111 0.00973541 0.01064827 0.06194532]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.04159132 0.08151441 0.03639628 0.01345888 0.08848717 0.04345303
 0.0167128  0.05209612 0.03817432 0.03960623]
obj_prev = 0.4514905711271736
eta_min = 0.0	eta_max = inf
af = 0.09163417184195034	bf = 0.024984709231642768	zeta = 0.10079758902614538	eta = 0.9090909090909091
af = 0.09163417184195034	bf = 0.024984709231642768	zeta = 0.370902164887314	eta = 0.247057527609175
af = 0.09163417184195034	bf = 0.024984709231642768	zeta = 0.20333766780339438	eta = 0.4506502549766171
af = 0.09163417184195034	bf = 0.024984709231642768	zeta = 0.17843456697103854	eta = 0.5135449559883952
af = 0.09163417184195034	bf = 0.024984709231642768	zeta = 0.17689475073627384	eta = 0.5180152122126253
af = 0.09163417184195034	bf = 0.024984709231642768	zeta = 0.1768876307117571	eta = 0.5180360631957955
eta = 0.5180360631957955
ene_coms = [0.0587596  0.06071307 0.00998644 0.01229922 0.01061884 0.01808103
 0.01345111 0.00973541 0.01064827 0.06194532]
ene_comp = [0.05427269 0.11414494 0.05341122 0.01852163 0.1318051  0.0628874
 0.02325972 0.07710167 0.05599563 0.0508268 ]
ene_total = [0.02200856 0.03404668 0.01234419 0.00600114 0.02773143 0.0157654
 0.00714798 0.01690809 0.01297626 0.02195791]
ti_comp = [132.03636801 132.01683333 132.52409958 132.50097182 132.51777556
 132.4431537  132.48945289 132.52660995 132.51748132 132.00451076]
ti_coms = [0.58759599 0.60713066 0.09986442 0.12299218 0.10618844 0.1808103
 0.13451111 0.09735405 0.10648268 0.61945324]
t_total = [25.82326889 25.82326889 25.82326889 25.82326889 25.82326889 25.82326889
 25.82326889 25.82326889 25.82326889 25.82326889]
ene_coms = [0.0587596  0.06071307 0.00998644 0.01229922 0.01061884 0.01808103
 0.01345111 0.00973541 0.01064827 0.06194532]
ene_comp = [5.73108511e-10 5.33324286e-09 5.42235182e-10 2.26193229e-11
 8.14945125e-09 8.86159508e-10 4.48054690e-11 1.63104250e-09
 6.24879205e-10 4.70956548e-10]
ene_total = [0.01144111 0.01182147 0.00194446 0.00239479 0.0020676  0.00352057
 0.00261907 0.00189559 0.00207333 0.0120614 ]
optimize_network iter = 0 obj = 0.05183937850750313
eta = 0.5180360631957955
freqs = [205521.7367697  432312.05739778 201515.10662082  69892.43603228
 497311.0108123  237412.79885087  87779.52749705 290891.29685888
 211276.38873707 192519.18793249]
eta_min = 0.518036063195796	eta_max = 0.9915860115950911
af = 2.6080120638628553e-09	bf = 0.024984709231642768	zeta = 2.868813270249141e-09	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0587596  0.06071307 0.00998644 0.01229922 0.01061884 0.01808103
 0.01345111 0.00973541 0.01064827 0.06194532]
ene_comp = [8.30503419e-11 7.72851275e-10 7.85764239e-11 3.27781296e-12
 1.18095387e-09 1.28415211e-10 6.49285336e-12 2.36357748e-10
 9.05525406e-11 6.82472893e-11]
ene_total = [0.06065621 0.06267273 0.01030878 0.01269621 0.01096159 0.01866464
 0.01388528 0.01004964 0.01099197 0.06394476]
ti_comp = [24.4282259  24.40869122 24.91595747 24.89282971 24.90963345 24.83501159
 24.88131078 24.91846784 24.90933921 24.39636865]
ti_coms = [0.58759599 0.60713066 0.09986442 0.12299218 0.10618844 0.1808103
 0.13451111 0.09735405 0.10648268 0.61945324]
t_total = [25.82326889 25.82326889 25.82326889 25.82326889 25.82326889 25.82326889
 25.82326889 25.82326889 25.82326889 25.82326889]
ene_coms = [0.0587596  0.06071307 0.00998644 0.01229922 0.01061884 0.01808103
 0.01345111 0.00973541 0.01064827 0.06194532]
ene_comp = [5.09511327e-11 4.74760630e-10 4.66805491e-11 1.95021680e-12
 7.01868365e-10 7.66930299e-11 3.86599093e-12 1.40392040e-10
 5.38185145e-11 4.19586696e-11]
ene_total = [0.06065621 0.06267273 0.01030878 0.01269621 0.01096159 0.01866464
 0.01388528 0.01004964 0.01099197 0.06394476]
optimize_network iter = 1 obj = 0.27483180319242545
eta = 0.9090909090909091
freqs = [160977.05220567 338833.80528192 155320.74215869  53911.267206
 383388.81835063 183473.70943651  67733.8648319  224190.41885546
 162879.52653544 150953.14746174]
eta_min = 0.9090909090909108	eta_max = 0.9516424998254961
af = 1.568424618315848e-09	bf = 0.024984709231642768	zeta = 1.725267080147433e-09	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0587596  0.06071307 0.00998644 0.01229922 0.01061884 0.01808103
 0.01345111 0.00973541 0.01064827 0.06194532]
ene_comp = [5.09511327e-11 4.74760630e-10 4.66805491e-11 1.95021680e-12
 7.01868365e-10 7.66930299e-11 3.86599093e-12 1.40392040e-10
 5.38185145e-11 4.19586696e-11]
ene_total = [0.06065621 0.06267273 0.01030878 0.01269621 0.01096159 0.01866464
 0.01388528 0.01004964 0.01099197 0.06394476]
ti_comp = [24.4282259  24.40869122 24.91595747 24.89282971 24.90963345 24.83501159
 24.88131078 24.91846784 24.90933921 24.39636865]
ti_coms = [0.58759599 0.60713066 0.09986442 0.12299218 0.10618844 0.1808103
 0.13451111 0.09735405 0.10648268 0.61945324]
t_total = [25.82326889 25.82326889 25.82326889 25.82326889 25.82326889 25.82326889
 25.82326889 25.82326889 25.82326889 25.82326889]
ene_coms = [0.0587596  0.06071307 0.00998644 0.01229922 0.01061884 0.01808103
 0.01345111 0.00973541 0.01064827 0.06194532]
ene_comp = [5.09511327e-11 4.74760630e-10 4.66805491e-11 1.95021680e-12
 7.01868365e-10 7.66930299e-11 3.86599093e-12 1.40392040e-10
 5.38185145e-11 4.19586696e-11]
ene_total = [0.06065621 0.06267273 0.01030878 0.01269621 0.01096159 0.01866464
 0.01388528 0.01004964 0.01099197 0.06394476]
optimize_network iter = 2 obj = 0.2748318031924309
eta = 0.9090909090909108
freqs = [160977.05220567 338833.80528192 155320.74215869  53911.267206
 383388.81835063 183473.70943651  67733.8648319  224190.41885546
 162879.52653544 150953.14746174]
Done!
ene_coms = [0.0587596  0.06071307 0.00998644 0.01229922 0.01061884 0.01808103
 0.01345111 0.00973541 0.01064827 0.06194532]
ene_comp = [4.89767254e-11 4.56363182e-10 4.48716313e-11 1.87464395e-12
 6.74670266e-10 7.37210985e-11 3.71617992e-12 1.34951708e-10
 5.17329933e-11 4.03327293e-11]
ene_total = [0.0587596  0.06071307 0.00998644 0.01229922 0.01061884 0.01808103
 0.01345111 0.00973541 0.01064827 0.06194532]
At round 82 energy consumption: 0.2662383087706217
At round 82 eta: 0.9090909090909108
At round 82 a_n: 0.09384340477035735
At round 82 local rounds: 3.1209395205776906
At round 82 global rounds: 1.0322774524739506
gradient difference: 0.8806955218315125
train() client id: f_00000-0-0 loss: 1.147452  [   32/  126]
train() client id: f_00000-0-1 loss: 1.207022  [   64/  126]
train() client id: f_00000-0-2 loss: 1.131637  [   96/  126]
train() client id: f_00000-1-0 loss: 1.174939  [   32/  126]
train() client id: f_00000-1-1 loss: 1.085412  [   64/  126]
train() client id: f_00000-1-2 loss: 0.944102  [   96/  126]
train() client id: f_00000-2-0 loss: 0.888158  [   32/  126]
train() client id: f_00000-2-1 loss: 1.245318  [   64/  126]
train() client id: f_00000-2-2 loss: 1.125933  [   96/  126]
train() client id: f_00001-0-0 loss: 0.460851  [   32/  265]
train() client id: f_00001-0-1 loss: 0.368717  [   64/  265]
train() client id: f_00001-0-2 loss: 0.387451  [   96/  265]
train() client id: f_00001-0-3 loss: 0.497591  [  128/  265]
train() client id: f_00001-0-4 loss: 0.463960  [  160/  265]
train() client id: f_00001-0-5 loss: 0.375310  [  192/  265]
train() client id: f_00001-0-6 loss: 0.475084  [  224/  265]
train() client id: f_00001-0-7 loss: 0.485697  [  256/  265]
train() client id: f_00001-1-0 loss: 0.423103  [   32/  265]
train() client id: f_00001-1-1 loss: 0.420217  [   64/  265]
train() client id: f_00001-1-2 loss: 0.421677  [   96/  265]
train() client id: f_00001-1-3 loss: 0.419593  [  128/  265]
train() client id: f_00001-1-4 loss: 0.492387  [  160/  265]
train() client id: f_00001-1-5 loss: 0.528052  [  192/  265]
train() client id: f_00001-1-6 loss: 0.395056  [  224/  265]
train() client id: f_00001-1-7 loss: 0.445098  [  256/  265]
train() client id: f_00001-2-0 loss: 0.427099  [   32/  265]
train() client id: f_00001-2-1 loss: 0.386088  [   64/  265]
train() client id: f_00001-2-2 loss: 0.331264  [   96/  265]
train() client id: f_00001-2-3 loss: 0.499535  [  128/  265]
train() client id: f_00001-2-4 loss: 0.532701  [  160/  265]
train() client id: f_00001-2-5 loss: 0.369348  [  192/  265]
train() client id: f_00001-2-6 loss: 0.437352  [  224/  265]
train() client id: f_00001-2-7 loss: 0.562790  [  256/  265]
train() client id: f_00002-0-0 loss: 0.829055  [   32/  124]
train() client id: f_00002-0-1 loss: 0.929544  [   64/  124]
train() client id: f_00002-0-2 loss: 1.125028  [   96/  124]
train() client id: f_00002-1-0 loss: 0.826352  [   32/  124]
train() client id: f_00002-1-1 loss: 0.892047  [   64/  124]
train() client id: f_00002-1-2 loss: 0.822881  [   96/  124]
train() client id: f_00002-2-0 loss: 0.829870  [   32/  124]
train() client id: f_00002-2-1 loss: 0.838278  [   64/  124]
train() client id: f_00002-2-2 loss: 0.810169  [   96/  124]
train() client id: f_00003-0-0 loss: 0.377342  [   32/   43]
train() client id: f_00003-1-0 loss: 0.291732  [   32/   43]
train() client id: f_00003-2-0 loss: 0.383837  [   32/   43]
train() client id: f_00004-0-0 loss: 0.900913  [   32/  306]
train() client id: f_00004-0-1 loss: 0.964738  [   64/  306]
train() client id: f_00004-0-2 loss: 1.018907  [   96/  306]
train() client id: f_00004-0-3 loss: 0.957031  [  128/  306]
train() client id: f_00004-0-4 loss: 1.027858  [  160/  306]
train() client id: f_00004-0-5 loss: 0.787927  [  192/  306]
train() client id: f_00004-0-6 loss: 0.754567  [  224/  306]
train() client id: f_00004-0-7 loss: 0.854804  [  256/  306]
train() client id: f_00004-0-8 loss: 0.856918  [  288/  306]
train() client id: f_00004-1-0 loss: 1.088570  [   32/  306]
train() client id: f_00004-1-1 loss: 0.880855  [   64/  306]
train() client id: f_00004-1-2 loss: 0.769161  [   96/  306]
train() client id: f_00004-1-3 loss: 0.909683  [  128/  306]
train() client id: f_00004-1-4 loss: 0.911234  [  160/  306]
train() client id: f_00004-1-5 loss: 0.907423  [  192/  306]
train() client id: f_00004-1-6 loss: 0.880978  [  224/  306]
train() client id: f_00004-1-7 loss: 0.995519  [  256/  306]
train() client id: f_00004-1-8 loss: 0.781198  [  288/  306]
train() client id: f_00004-2-0 loss: 1.042352  [   32/  306]
train() client id: f_00004-2-1 loss: 0.857964  [   64/  306]
train() client id: f_00004-2-2 loss: 0.947621  [   96/  306]
train() client id: f_00004-2-3 loss: 0.823325  [  128/  306]
train() client id: f_00004-2-4 loss: 0.915621  [  160/  306]
train() client id: f_00004-2-5 loss: 0.914889  [  192/  306]
train() client id: f_00004-2-6 loss: 0.858123  [  224/  306]
train() client id: f_00004-2-7 loss: 0.909613  [  256/  306]
train() client id: f_00004-2-8 loss: 0.982340  [  288/  306]
train() client id: f_00005-0-0 loss: 0.909446  [   32/  146]
train() client id: f_00005-0-1 loss: 0.884226  [   64/  146]
train() client id: f_00005-0-2 loss: 0.859174  [   96/  146]
train() client id: f_00005-0-3 loss: 0.932316  [  128/  146]
train() client id: f_00005-1-0 loss: 0.862613  [   32/  146]
train() client id: f_00005-1-1 loss: 0.907837  [   64/  146]
train() client id: f_00005-1-2 loss: 0.965957  [   96/  146]
train() client id: f_00005-1-3 loss: 0.711723  [  128/  146]
train() client id: f_00005-2-0 loss: 0.787715  [   32/  146]
train() client id: f_00005-2-1 loss: 0.843245  [   64/  146]
train() client id: f_00005-2-2 loss: 0.948162  [   96/  146]
train() client id: f_00005-2-3 loss: 0.854899  [  128/  146]
train() client id: f_00006-0-0 loss: 0.402773  [   32/   54]
train() client id: f_00006-1-0 loss: 0.493404  [   32/   54]
train() client id: f_00006-2-0 loss: 0.409521  [   32/   54]
train() client id: f_00007-0-0 loss: 0.617730  [   32/  179]
train() client id: f_00007-0-1 loss: 0.717457  [   64/  179]
train() client id: f_00007-0-2 loss: 0.493768  [   96/  179]
train() client id: f_00007-0-3 loss: 0.631467  [  128/  179]
train() client id: f_00007-0-4 loss: 0.605492  [  160/  179]
train() client id: f_00007-1-0 loss: 0.431061  [   32/  179]
train() client id: f_00007-1-1 loss: 0.727615  [   64/  179]
train() client id: f_00007-1-2 loss: 0.695037  [   96/  179]
train() client id: f_00007-1-3 loss: 0.758378  [  128/  179]
train() client id: f_00007-1-4 loss: 0.542085  [  160/  179]
train() client id: f_00007-2-0 loss: 0.538966  [   32/  179]
train() client id: f_00007-2-1 loss: 0.487164  [   64/  179]
train() client id: f_00007-2-2 loss: 0.776261  [   96/  179]
train() client id: f_00007-2-3 loss: 0.661696  [  128/  179]
train() client id: f_00007-2-4 loss: 0.569038  [  160/  179]
train() client id: f_00008-0-0 loss: 0.792553  [   32/  130]
train() client id: f_00008-0-1 loss: 0.812854  [   64/  130]
train() client id: f_00008-0-2 loss: 0.848985  [   96/  130]
train() client id: f_00008-0-3 loss: 0.771791  [  128/  130]
train() client id: f_00008-1-0 loss: 0.765405  [   32/  130]
train() client id: f_00008-1-1 loss: 0.830207  [   64/  130]
train() client id: f_00008-1-2 loss: 0.765726  [   96/  130]
train() client id: f_00008-1-3 loss: 0.845850  [  128/  130]
train() client id: f_00008-2-0 loss: 0.824934  [   32/  130]
train() client id: f_00008-2-1 loss: 0.857149  [   64/  130]
train() client id: f_00008-2-2 loss: 0.746153  [   96/  130]
train() client id: f_00008-2-3 loss: 0.773893  [  128/  130]
train() client id: f_00009-0-0 loss: 0.605800  [   32/  118]
train() client id: f_00009-0-1 loss: 0.661604  [   64/  118]
train() client id: f_00009-0-2 loss: 0.734664  [   96/  118]
train() client id: f_00009-1-0 loss: 0.627779  [   32/  118]
train() client id: f_00009-1-1 loss: 0.807918  [   64/  118]
train() client id: f_00009-1-2 loss: 0.507128  [   96/  118]
train() client id: f_00009-2-0 loss: 0.641222  [   32/  118]
train() client id: f_00009-2-1 loss: 0.800255  [   64/  118]
train() client id: f_00009-2-2 loss: 0.560141  [   96/  118]
At round 82 accuracy: 0.6472148541114059
At round 82 training accuracy: 0.5881958417169685
At round 82 training loss: 0.8281574766077693
Done!
