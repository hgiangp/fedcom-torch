v = 22.697160301531774
a_0 = 27.840057009285058	a_alpha = -0.3425458469693173
['f_00000', 'f_00001', 'f_00002', 'f_00003', 'f_00004', 'f_00005', 'f_00006', 'f_00007', 'f_00008', 'f_00009']
10
dict_keys(['x', 'y'])
id = f_00000, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 126
id = f_00001, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 265
id = f_00002, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 124
id = f_00003, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 43
id = f_00004, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 306
id = f_00005, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 146
id = f_00006, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 54
id = f_00007, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 179
id = f_00008, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 130
id = f_00009, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 118
BaseFederated generated!
num_samples = [126 265 124  43 306 146  54 179 130 118]
msize = 502400
xs = [  6.0943416  -20.79968212  15.00902392  18.81129433 -39.02070377
 -26.04359014   2.55680806  -6.32485185  -0.33602315 -17.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  22.54482414   9.35018685
 -17.18584926   7.37501568 -19.17765202  17.56900603  -0.99851822]
dists_uav = [101.71763524 103.31800857 101.12870423 104.22156154 107.7499017
 104.75505717 100.304178   102.01855757 101.5321766  101.44984286]
dists_bs = [239.94522511 221.81113273 257.42563646 246.58750669 214.31339109
 243.15306433 244.18139867 257.20860434 235.14255819 236.47461703]
uav_gains = [9.58312886e-11 9.21631668e-11 9.72326274e-11 9.01785356e-11
 8.29761259e-11 8.90347214e-11 9.92432040e-11 9.51261523e-11
 9.62695134e-11 9.64649612e-11]
bs_gains = [2.39320649e-11 2.98222725e-11 1.96548195e-11 2.21704798e-11
 3.28364367e-11 2.30584863e-11 2.27876148e-11 1.97012919e-11
 2.53260008e-11 2.49285719e-11]
SystemModel __init__!
t_co_uav = [0.06351163 0.06396501 0.06334461 0.06422068 0.06521719 0.06437154
 0.0631106  0.06359693 0.06345904 0.0634357 ]
t_co_bs = [0.08476871 0.08051968 0.08895491 0.08634835 0.0787878  0.08552995
 0.08577462 0.08890235 0.0836346  0.08394849]
difference = [-0.02125708 -0.01655467 -0.0256103  -0.02212767 -0.01357061 -0.0211584
 -0.02266402 -0.02530541 -0.02017555 -0.0205128 ]
decs_opt = [1 0 1 1 0 0 1 1 0 0]
af = 6.796163711028166	bf = 20.32894796997589	zeta = 7.475780082130983	eta = 0.9090909090909091
af = 6.796163711028166	bf = 20.32894796997589	zeta = 230.74360710778967	eta = 0.02945331312192499
af = 6.796163711028166	bf = 20.32894796997589	zeta = 45.629002909774904	eta = 0.1489439452461113
af = 6.796163711028166	bf = 20.32894796997589	zeta = 39.092727771299245	eta = 0.173847262610764
af = 6.796163711028166	bf = 20.32894796997589	zeta = 38.999286881910415	eta = 0.17426379440236703
af = 6.796163711028166	bf = 20.32894796997589	zeta = 38.99926333319505	eta = 0.17426389962713645
eta_opt = 0.17426389962713645
initialize_feasible_solution eta = 0.17426389962713645, tau = 10.000839233398438
ti_comp = [0.03604337 0.0758055  0.03547125 0.01230051 0.0875339  0.04176454
 0.01544716 0.05120447 0.0371876  0.0337549 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [3.35654071 5.27057237 3.33162034 2.5799458  5.60760928 4.29178968
 2.64860954 3.87057992 4.07357378 3.96842191]
system_model train() tau = 30	t0 = 0.050004196166992185	t_min = 10.000839233398438
Round 0
-------------------------------
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86489914 22.7015787  10.6948057   3.82750737 26.17399534 12.62305496
  4.75731633 15.36016845 11.26073498 10.24387519]
obj_prev = 128.5079361631131
eta_min = 2.1972428731623064e-09	eta_max = 0.9187482009227209
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 50.862560519193266	eta = 0.5344727942639534
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 40.950101804889684	eta = 0.6638482847646219
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.18020327767913	eta = 0.6938364931761265
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.094800455198204	eta = 0.6953521830931377
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.094588160812094	eta = 0.6953559590470941
eta = 0.6953559590470941
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.02998124 0.06305577 0.02950534 0.01023169 0.07281157 0.03474016
 0.0128491  0.04259239 0.03093302 0.02807767]
ene_total = [3.3202555  6.49821862 3.27523947 1.52191262 7.37393057 3.95636804
 1.75096148 4.4735121  3.5911297  3.33306006]
ti_comp = [0.26476791 0.24775986 0.26493494 0.26405887 0.24949175 0.2427496
 0.26516894 0.26468261 0.24464495 0.24433105]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.40269086e-05 2.55265831e-04 2.28719695e-05 9.60109636e-07
 3.87586715e-04 4.44691157e-05 1.88561461e-06 6.89326925e-05
 3.09082577e-05 2.31742490e-05]
ene_total = [0.58260011 0.75916097 0.58096823 0.58697181 0.75542628 0.78568377
 0.5769119  0.5874834  0.76712376 0.76928555]
optimize_network_iter = 0 obj = 6.7516157792990965
eta = 0.6953559590470941
freqs = [5.66179550e+07 1.27251793e+08 5.56841306e+07 1.93738838e+07
 1.45919802e+08 7.15555497e+07 2.42281409e+07 8.04593666e+07
 6.32202327e+07 5.74582420e+07]
eta_min = 0.6595211103034527	eta_max = 0.6953559590470929
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 0.07249356690995133	eta = 0.909090909090909
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 22.43093651594189	eta = 0.002938051320263521
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.424233800512977	eta = 0.027185184296772417
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.333929777668074	eta = 0.028237028926918685
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.333881523150504	eta = 0.02823761274584649
eta = 0.02823761274584649
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.35890421e-04 2.50613865e-03 2.24551507e-04 9.42612594e-06
 3.80523333e-03 4.36587104e-04 1.85125116e-05 6.76764628e-04
 3.03449855e-04 2.27519213e-04]
ene_total = [0.18871274 0.3024796  0.18790939 0.18425611 0.33473573 0.25754286
 0.18133617 0.20158775 0.24829862 0.24702256]
ti_comp = [0.30338297 0.28637492 0.30355    0.30267393 0.28810681 0.28136466
 0.303784   0.30329767 0.28326001 0.28294611]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.75152158e-05 2.87283940e-04 2.61968029e-05 1.09875040e-06
 4.37019626e-04 4.97694879e-05 2.16020605e-06 7.89340962e-05
 3.46659600e-05 2.59825795e-05]
ene_total = [0.52156762 0.68187855 0.52009413 0.52520532 0.6799609  0.70342523
 0.51621531 0.52646951 0.68669249 0.68854913]
optimize_network_iter = 1 obj = 6.050058200424808
eta = 0.6595211103034527
freqs = [5.66070564e+07 1.26125273e+08 5.56778794e+07 1.93635364e+07
 1.44763522e+08 7.07252580e+07 2.42281409e+07 8.04405789e+07
 6.25531698e+07 5.68420211e+07]
eta_min = 0.6595211103034622	eta_max = 0.6595211103034516
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 0.07141177968017555	eta = 0.909090909090909
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 22.429905462588092	eta = 0.0028943412096646293
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.4193533145705937	eta = 0.026833534117679286
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.3303118142037405	eta = 0.02785884674898469
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.3302654452731733	eta = 0.02785940109996355
eta = 0.02785940109996355
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.36690987e-04 2.47126971e-03 2.25349755e-04 9.45165461e-06
 3.75932384e-03 4.28126361e-04 1.85824929e-05 6.79005729e-04
 2.98203014e-04 2.23506966e-04]
ene_total = [0.18866225 0.30136334 0.18785915 0.18418516 0.33329075 0.25720037
 0.18126762 0.2015735  0.24805176 0.24681156]
ti_comp = [0.30338297 0.28637492 0.30355    0.30267393 0.28810681 0.28136466
 0.303784   0.30329767 0.28326001 0.28294611]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.75152158e-05 2.87283940e-04 2.61968029e-05 1.09875040e-06
 4.37019626e-04 4.97694879e-05 2.16020605e-06 7.89340962e-05
 3.46659600e-05 2.59825795e-05]
ene_total = [0.52156762 0.68187855 0.52009413 0.52520532 0.6799609  0.70342523
 0.51621531 0.52646951 0.68669249 0.68854913]
optimize_network_iter = 2 obj = 6.0500582004249726
eta = 0.6595211103034622
freqs = [5.66070564e+07 1.26125273e+08 5.56778794e+07 1.93635364e+07
 1.44763522e+08 7.07252580e+07 2.42281409e+07 8.04405789e+07
 6.25531698e+07 5.68420211e+07]
Done!
At round 0 eta: 0.6595211103034622
At round 0 local rounds: 13.62985484868938
At round 0 global rounds: 81.76735137411414
At round 0 a_n: 27.840057009285058
gradient difference: 1.1982429027557373
train() client id: f_00000-0-0 loss: 1.007282  [   32/  126]
train() client id: f_00000-0-1 loss: 0.959461  [   64/  126]
train() client id: f_00000-0-2 loss: 1.078616  [   96/  126]
train() client id: f_00000-1-0 loss: 0.986652  [   32/  126]
train() client id: f_00000-1-1 loss: 0.965955  [   64/  126]
train() client id: f_00000-1-2 loss: 1.065109  [   96/  126]
train() client id: f_00000-2-0 loss: 1.019296  [   32/  126]
train() client id: f_00000-2-1 loss: 1.014938  [   64/  126]
train() client id: f_00000-2-2 loss: 0.979127  [   96/  126]
train() client id: f_00000-3-0 loss: 1.099108  [   32/  126]
train() client id: f_00000-3-1 loss: 1.009150  [   64/  126]
train() client id: f_00000-3-2 loss: 1.016304  [   96/  126]
train() client id: f_00000-4-0 loss: 0.936078  [   32/  126]
train() client id: f_00000-4-1 loss: 0.936502  [   64/  126]
train() client id: f_00000-4-2 loss: 1.090265  [   96/  126]
train() client id: f_00000-5-0 loss: 0.925539  [   32/  126]
train() client id: f_00000-5-1 loss: 1.106037  [   64/  126]
train() client id: f_00000-5-2 loss: 1.006239  [   96/  126]
train() client id: f_00000-6-0 loss: 1.234461  [   32/  126]
train() client id: f_00000-6-1 loss: 0.952775  [   64/  126]
train() client id: f_00000-6-2 loss: 0.896306  [   96/  126]
train() client id: f_00000-7-0 loss: 0.996483  [   32/  126]
train() client id: f_00000-7-1 loss: 1.119300  [   64/  126]
train() client id: f_00000-7-2 loss: 0.918805  [   96/  126]
train() client id: f_00000-8-0 loss: 1.204257  [   32/  126]
train() client id: f_00000-8-1 loss: 1.045380  [   64/  126]
train() client id: f_00000-8-2 loss: 0.960969  [   96/  126]
train() client id: f_00000-9-0 loss: 1.041540  [   32/  126]
train() client id: f_00000-9-1 loss: 1.061073  [   64/  126]
train() client id: f_00000-9-2 loss: 0.934301  [   96/  126]
train() client id: f_00000-10-0 loss: 1.138818  [   32/  126]
train() client id: f_00000-10-1 loss: 1.082683  [   64/  126]
train() client id: f_00000-10-2 loss: 1.098267  [   96/  126]
train() client id: f_00000-11-0 loss: 1.067302  [   32/  126]
train() client id: f_00000-11-1 loss: 1.087061  [   64/  126]
train() client id: f_00000-11-2 loss: 0.944938  [   96/  126]
train() client id: f_00000-12-0 loss: 1.114037  [   32/  126]
train() client id: f_00000-12-1 loss: 1.039788  [   64/  126]
train() client id: f_00000-12-2 loss: 1.106570  [   96/  126]
train() client id: f_00001-0-0 loss: 0.897851  [   32/  265]
train() client id: f_00001-0-1 loss: 0.949323  [   64/  265]
train() client id: f_00001-0-2 loss: 1.020675  [   96/  265]
train() client id: f_00001-0-3 loss: 1.013598  [  128/  265]
train() client id: f_00001-0-4 loss: 0.948781  [  160/  265]
train() client id: f_00001-0-5 loss: 0.972561  [  192/  265]
train() client id: f_00001-0-6 loss: 0.851142  [  224/  265]
train() client id: f_00001-0-7 loss: 0.876700  [  256/  265]
train() client id: f_00001-1-0 loss: 0.948573  [   32/  265]
train() client id: f_00001-1-1 loss: 0.931657  [   64/  265]
train() client id: f_00001-1-2 loss: 0.872528  [   96/  265]
train() client id: f_00001-1-3 loss: 0.896909  [  128/  265]
train() client id: f_00001-1-4 loss: 0.885145  [  160/  265]
train() client id: f_00001-1-5 loss: 0.975281  [  192/  265]
train() client id: f_00001-1-6 loss: 1.055993  [  224/  265]
train() client id: f_00001-1-7 loss: 0.968882  [  256/  265]
train() client id: f_00001-2-0 loss: 0.934341  [   32/  265]
train() client id: f_00001-2-1 loss: 0.833560  [   64/  265]
train() client id: f_00001-2-2 loss: 1.003289  [   96/  265]
train() client id: f_00001-2-3 loss: 0.976012  [  128/  265]
train() client id: f_00001-2-4 loss: 0.978033  [  160/  265]
train() client id: f_00001-2-5 loss: 0.975776  [  192/  265]
train() client id: f_00001-2-6 loss: 0.963674  [  224/  265]
train() client id: f_00001-2-7 loss: 0.917238  [  256/  265]
train() client id: f_00001-3-0 loss: 0.976408  [   32/  265]
train() client id: f_00001-3-1 loss: 0.934797  [   64/  265]
train() client id: f_00001-3-2 loss: 1.008761  [   96/  265]
train() client id: f_00001-3-3 loss: 1.101448  [  128/  265]
train() client id: f_00001-3-4 loss: 0.980617  [  160/  265]
train() client id: f_00001-3-5 loss: 0.856326  [  192/  265]
train() client id: f_00001-3-6 loss: 0.922106  [  224/  265]
train() client id: f_00001-3-7 loss: 0.859835  [  256/  265]
train() client id: f_00001-4-0 loss: 0.900352  [   32/  265]
train() client id: f_00001-4-1 loss: 0.876503  [   64/  265]
train() client id: f_00001-4-2 loss: 0.981173  [   96/  265]
train() client id: f_00001-4-3 loss: 0.882903  [  128/  265]
train() client id: f_00001-4-4 loss: 0.972041  [  160/  265]
train() client id: f_00001-4-5 loss: 1.009240  [  192/  265]
train() client id: f_00001-4-6 loss: 1.022410  [  224/  265]
train() client id: f_00001-4-7 loss: 1.059229  [  256/  265]
train() client id: f_00001-5-0 loss: 1.046628  [   32/  265]
train() client id: f_00001-5-1 loss: 0.856390  [   64/  265]
train() client id: f_00001-5-2 loss: 0.978308  [   96/  265]
train() client id: f_00001-5-3 loss: 1.082973  [  128/  265]
train() client id: f_00001-5-4 loss: 0.938348  [  160/  265]
train() client id: f_00001-5-5 loss: 1.057768  [  192/  265]
train() client id: f_00001-5-6 loss: 0.871620  [  224/  265]
train() client id: f_00001-5-7 loss: 0.927535  [  256/  265]
train() client id: f_00001-6-0 loss: 0.970260  [   32/  265]
train() client id: f_00001-6-1 loss: 1.036579  [   64/  265]
train() client id: f_00001-6-2 loss: 1.084525  [   96/  265]
train() client id: f_00001-6-3 loss: 0.971401  [  128/  265]
train() client id: f_00001-6-4 loss: 1.049248  [  160/  265]
train() client id: f_00001-6-5 loss: 0.921647  [  192/  265]
train() client id: f_00001-6-6 loss: 0.987449  [  224/  265]
train() client id: f_00001-6-7 loss: 0.910236  [  256/  265]
train() client id: f_00001-7-0 loss: 0.894784  [   32/  265]
train() client id: f_00001-7-1 loss: 1.067799  [   64/  265]
train() client id: f_00001-7-2 loss: 0.998574  [   96/  265]
train() client id: f_00001-7-3 loss: 0.981390  [  128/  265]
train() client id: f_00001-7-4 loss: 0.937767  [  160/  265]
train() client id: f_00001-7-5 loss: 1.001083  [  192/  265]
train() client id: f_00001-7-6 loss: 1.186639  [  224/  265]
train() client id: f_00001-7-7 loss: 0.970263  [  256/  265]
train() client id: f_00001-8-0 loss: 1.008954  [   32/  265]
train() client id: f_00001-8-1 loss: 1.073993  [   64/  265]
train() client id: f_00001-8-2 loss: 0.962666  [   96/  265]
train() client id: f_00001-8-3 loss: 0.968923  [  128/  265]
train() client id: f_00001-8-4 loss: 1.104014  [  160/  265]
train() client id: f_00001-8-5 loss: 0.995127  [  192/  265]
train() client id: f_00001-8-6 loss: 0.976068  [  224/  265]
train() client id: f_00001-8-7 loss: 1.072419  [  256/  265]
train() client id: f_00001-9-0 loss: 0.957753  [   32/  265]
train() client id: f_00001-9-1 loss: 1.164462  [   64/  265]
train() client id: f_00001-9-2 loss: 1.037690  [   96/  265]
train() client id: f_00001-9-3 loss: 1.009058  [  128/  265]
train() client id: f_00001-9-4 loss: 0.974350  [  160/  265]
train() client id: f_00001-9-5 loss: 0.951299  [  192/  265]
train() client id: f_00001-9-6 loss: 0.993447  [  224/  265]
train() client id: f_00001-9-7 loss: 1.107662  [  256/  265]
train() client id: f_00001-10-0 loss: 0.921628  [   32/  265]
train() client id: f_00001-10-1 loss: 0.971940  [   64/  265]
train() client id: f_00001-10-2 loss: 1.051153  [   96/  265]
train() client id: f_00001-10-3 loss: 1.059439  [  128/  265]
train() client id: f_00001-10-4 loss: 1.005092  [  160/  265]
train() client id: f_00001-10-5 loss: 0.970915  [  192/  265]
train() client id: f_00001-10-6 loss: 1.101277  [  224/  265]
train() client id: f_00001-10-7 loss: 1.154712  [  256/  265]
train() client id: f_00001-11-0 loss: 0.946184  [   32/  265]
train() client id: f_00001-11-1 loss: 1.049615  [   64/  265]
train() client id: f_00001-11-2 loss: 1.082219  [   96/  265]
train() client id: f_00001-11-3 loss: 1.000567  [  128/  265]
train() client id: f_00001-11-4 loss: 1.063272  [  160/  265]
train() client id: f_00001-11-5 loss: 1.100759  [  192/  265]
train() client id: f_00001-11-6 loss: 1.094658  [  224/  265]
train() client id: f_00001-11-7 loss: 1.136439  [  256/  265]
train() client id: f_00001-12-0 loss: 1.127620  [   32/  265]
train() client id: f_00001-12-1 loss: 1.015514  [   64/  265]
train() client id: f_00001-12-2 loss: 1.060841  [   96/  265]
train() client id: f_00001-12-3 loss: 1.246228  [  128/  265]
train() client id: f_00001-12-4 loss: 0.981683  [  160/  265]
train() client id: f_00001-12-5 loss: 1.001167  [  192/  265]
train() client id: f_00001-12-6 loss: 1.082487  [  224/  265]
train() client id: f_00001-12-7 loss: 1.039856  [  256/  265]
train() client id: f_00002-0-0 loss: 0.957074  [   32/  124]
train() client id: f_00002-0-1 loss: 1.101836  [   64/  124]
train() client id: f_00002-0-2 loss: 1.009525  [   96/  124]
train() client id: f_00002-1-0 loss: 1.027783  [   32/  124]
train() client id: f_00002-1-1 loss: 1.034908  [   64/  124]
train() client id: f_00002-1-2 loss: 1.029508  [   96/  124]
train() client id: f_00002-2-0 loss: 1.025279  [   32/  124]
train() client id: f_00002-2-1 loss: 0.965612  [   64/  124]
train() client id: f_00002-2-2 loss: 1.023143  [   96/  124]
train() client id: f_00002-3-0 loss: 0.979208  [   32/  124]
train() client id: f_00002-3-1 loss: 1.033348  [   64/  124]
train() client id: f_00002-3-2 loss: 1.026570  [   96/  124]
train() client id: f_00002-4-0 loss: 1.039538  [   32/  124]
train() client id: f_00002-4-1 loss: 0.961196  [   64/  124]
train() client id: f_00002-4-2 loss: 0.985857  [   96/  124]
train() client id: f_00002-5-0 loss: 0.954840  [   32/  124]
train() client id: f_00002-5-1 loss: 0.986798  [   64/  124]
train() client id: f_00002-5-2 loss: 1.084885  [   96/  124]
train() client id: f_00002-6-0 loss: 1.072482  [   32/  124]
train() client id: f_00002-6-1 loss: 1.087045  [   64/  124]
train() client id: f_00002-6-2 loss: 0.912455  [   96/  124]
train() client id: f_00002-7-0 loss: 1.037217  [   32/  124]
train() client id: f_00002-7-1 loss: 0.996188  [   64/  124]
train() client id: f_00002-7-2 loss: 1.058878  [   96/  124]
train() client id: f_00002-8-0 loss: 1.005400  [   32/  124]
train() client id: f_00002-8-1 loss: 1.093044  [   64/  124]
train() client id: f_00002-8-2 loss: 0.969064  [   96/  124]
train() client id: f_00002-9-0 loss: 0.983840  [   32/  124]
train() client id: f_00002-9-1 loss: 1.096435  [   64/  124]
train() client id: f_00002-9-2 loss: 0.986249  [   96/  124]
train() client id: f_00002-10-0 loss: 1.055115  [   32/  124]
train() client id: f_00002-10-1 loss: 0.976195  [   64/  124]
train() client id: f_00002-10-2 loss: 1.032489  [   96/  124]
train() client id: f_00002-11-0 loss: 0.966531  [   32/  124]
train() client id: f_00002-11-1 loss: 1.053378  [   64/  124]
train() client id: f_00002-11-2 loss: 1.016752  [   96/  124]
train() client id: f_00002-12-0 loss: 1.125536  [   32/  124]
train() client id: f_00002-12-1 loss: 0.976132  [   64/  124]
train() client id: f_00002-12-2 loss: 1.021323  [   96/  124]
train() client id: f_00003-0-0 loss: 1.150915  [   32/   43]
train() client id: f_00003-1-0 loss: 1.131621  [   32/   43]
train() client id: f_00003-2-0 loss: 1.183449  [   32/   43]
train() client id: f_00003-3-0 loss: 1.097940  [   32/   43]
train() client id: f_00003-4-0 loss: 1.179662  [   32/   43]
train() client id: f_00003-5-0 loss: 1.154276  [   32/   43]
train() client id: f_00003-6-0 loss: 1.160179  [   32/   43]
train() client id: f_00003-7-0 loss: 1.153054  [   32/   43]
train() client id: f_00003-8-0 loss: 1.143676  [   32/   43]
train() client id: f_00003-9-0 loss: 1.138801  [   32/   43]
train() client id: f_00003-10-0 loss: 1.201066  [   32/   43]
train() client id: f_00003-11-0 loss: 1.124115  [   32/   43]
train() client id: f_00003-12-0 loss: 1.159031  [   32/   43]
train() client id: f_00004-0-0 loss: 1.134718  [   32/  306]
train() client id: f_00004-0-1 loss: 1.074897  [   64/  306]
train() client id: f_00004-0-2 loss: 1.086820  [   96/  306]
train() client id: f_00004-0-3 loss: 1.099391  [  128/  306]
train() client id: f_00004-0-4 loss: 1.105118  [  160/  306]
train() client id: f_00004-0-5 loss: 1.157999  [  192/  306]
train() client id: f_00004-0-6 loss: 1.074616  [  224/  306]
train() client id: f_00004-0-7 loss: 1.043726  [  256/  306]
train() client id: f_00004-0-8 loss: 1.106253  [  288/  306]
train() client id: f_00004-1-0 loss: 1.053238  [   32/  306]
train() client id: f_00004-1-1 loss: 1.039517  [   64/  306]
train() client id: f_00004-1-2 loss: 1.125893  [   96/  306]
train() client id: f_00004-1-3 loss: 1.116565  [  128/  306]
train() client id: f_00004-1-4 loss: 1.133003  [  160/  306]
train() client id: f_00004-1-5 loss: 1.101603  [  192/  306]
train() client id: f_00004-1-6 loss: 1.120971  [  224/  306]
train() client id: f_00004-1-7 loss: 1.086517  [  256/  306]
train() client id: f_00004-1-8 loss: 1.079976  [  288/  306]
train() client id: f_00004-2-0 loss: 1.077388  [   32/  306]
train() client id: f_00004-2-1 loss: 1.117297  [   64/  306]
train() client id: f_00004-2-2 loss: 1.088353  [   96/  306]
train() client id: f_00004-2-3 loss: 1.089726  [  128/  306]
train() client id: f_00004-2-4 loss: 1.097960  [  160/  306]
train() client id: f_00004-2-5 loss: 1.094836  [  192/  306]
train() client id: f_00004-2-6 loss: 1.092804  [  224/  306]
train() client id: f_00004-2-7 loss: 1.009287  [  256/  306]
train() client id: f_00004-2-8 loss: 1.174800  [  288/  306]
train() client id: f_00004-3-0 loss: 1.150330  [   32/  306]
train() client id: f_00004-3-1 loss: 1.022163  [   64/  306]
train() client id: f_00004-3-2 loss: 1.131327  [   96/  306]
train() client id: f_00004-3-3 loss: 1.052436  [  128/  306]
train() client id: f_00004-3-4 loss: 1.100084  [  160/  306]
train() client id: f_00004-3-5 loss: 1.107449  [  192/  306]
train() client id: f_00004-3-6 loss: 1.172620  [  224/  306]
train() client id: f_00004-3-7 loss: 1.072287  [  256/  306]
train() client id: f_00004-3-8 loss: 1.140736  [  288/  306]
train() client id: f_00004-4-0 loss: 1.122008  [   32/  306]
train() client id: f_00004-4-1 loss: 1.119744  [   64/  306]
train() client id: f_00004-4-2 loss: 1.086738  [   96/  306]
train() client id: f_00004-4-3 loss: 1.084070  [  128/  306]
train() client id: f_00004-4-4 loss: 1.088115  [  160/  306]
train() client id: f_00004-4-5 loss: 1.118763  [  192/  306]
train() client id: f_00004-4-6 loss: 1.126454  [  224/  306]
train() client id: f_00004-4-7 loss: 1.127654  [  256/  306]
train() client id: f_00004-4-8 loss: 1.104678  [  288/  306]
train() client id: f_00004-5-0 loss: 1.098675  [   32/  306]
train() client id: f_00004-5-1 loss: 1.111997  [   64/  306]
train() client id: f_00004-5-2 loss: 1.095348  [   96/  306]
train() client id: f_00004-5-3 loss: 1.110256  [  128/  306]
train() client id: f_00004-5-4 loss: 1.096460  [  160/  306]
train() client id: f_00004-5-5 loss: 1.144315  [  192/  306]
train() client id: f_00004-5-6 loss: 1.099917  [  224/  306]
train() client id: f_00004-5-7 loss: 1.124673  [  256/  306]
train() client id: f_00004-5-8 loss: 1.136007  [  288/  306]
train() client id: f_00004-6-0 loss: 1.115019  [   32/  306]
train() client id: f_00004-6-1 loss: 1.131166  [   64/  306]
train() client id: f_00004-6-2 loss: 1.068216  [   96/  306]
train() client id: f_00004-6-3 loss: 1.129267  [  128/  306]
train() client id: f_00004-6-4 loss: 1.138329  [  160/  306]
train() client id: f_00004-6-5 loss: 1.205151  [  192/  306]
train() client id: f_00004-6-6 loss: 1.199812  [  224/  306]
train() client id: f_00004-6-7 loss: 1.106295  [  256/  306]
train() client id: f_00004-6-8 loss: 1.064952  [  288/  306]
train() client id: f_00004-7-0 loss: 1.076999  [   32/  306]
train() client id: f_00004-7-1 loss: 1.110594  [   64/  306]
train() client id: f_00004-7-2 loss: 1.184993  [   96/  306]
train() client id: f_00004-7-3 loss: 1.111606  [  128/  306]
train() client id: f_00004-7-4 loss: 1.110318  [  160/  306]
train() client id: f_00004-7-5 loss: 1.120906  [  192/  306]
train() client id: f_00004-7-6 loss: 1.191175  [  224/  306]
train() client id: f_00004-7-7 loss: 1.147675  [  256/  306]
train() client id: f_00004-7-8 loss: 1.170875  [  288/  306]
train() client id: f_00004-8-0 loss: 1.140930  [   32/  306]
train() client id: f_00004-8-1 loss: 1.151076  [   64/  306]
train() client id: f_00004-8-2 loss: 1.188956  [   96/  306]
train() client id: f_00004-8-3 loss: 1.109269  [  128/  306]
train() client id: f_00004-8-4 loss: 1.088848  [  160/  306]
train() client id: f_00004-8-5 loss: 1.180329  [  192/  306]
train() client id: f_00004-8-6 loss: 1.199640  [  224/  306]
train() client id: f_00004-8-7 loss: 1.078637  [  256/  306]
train() client id: f_00004-8-8 loss: 1.174771  [  288/  306]
train() client id: f_00004-9-0 loss: 1.192376  [   32/  306]
train() client id: f_00004-9-1 loss: 1.205119  [   64/  306]
train() client id: f_00004-9-2 loss: 1.138408  [   96/  306]
train() client id: f_00004-9-3 loss: 1.112791  [  128/  306]
train() client id: f_00004-9-4 loss: 1.150616  [  160/  306]
train() client id: f_00004-9-5 loss: 1.097343  [  192/  306]
train() client id: f_00004-9-6 loss: 1.206917  [  224/  306]
train() client id: f_00004-9-7 loss: 1.254404  [  256/  306]
train() client id: f_00004-9-8 loss: 1.106146  [  288/  306]
train() client id: f_00004-10-0 loss: 1.116737  [   32/  306]
train() client id: f_00004-10-1 loss: 1.225536  [   64/  306]
train() client id: f_00004-10-2 loss: 1.079319  [   96/  306]
train() client id: f_00004-10-3 loss: 1.111595  [  128/  306]
train() client id: f_00004-10-4 loss: 1.214358  [  160/  306]
train() client id: f_00004-10-5 loss: 1.161482  [  192/  306]
train() client id: f_00004-10-6 loss: 1.202619  [  224/  306]
train() client id: f_00004-10-7 loss: 1.212731  [  256/  306]
train() client id: f_00004-10-8 loss: 1.181177  [  288/  306]
train() client id: f_00004-11-0 loss: 1.211723  [   32/  306]
train() client id: f_00004-11-1 loss: 1.235338  [   64/  306]
train() client id: f_00004-11-2 loss: 1.246267  [   96/  306]
train() client id: f_00004-11-3 loss: 1.103406  [  128/  306]
train() client id: f_00004-11-4 loss: 1.192678  [  160/  306]
train() client id: f_00004-11-5 loss: 1.206867  [  192/  306]
train() client id: f_00004-11-6 loss: 1.163589  [  224/  306]
train() client id: f_00004-11-7 loss: 1.075319  [  256/  306]
train() client id: f_00004-11-8 loss: 1.161853  [  288/  306]
train() client id: f_00004-12-0 loss: 1.147503  [   32/  306]
train() client id: f_00004-12-1 loss: 1.207248  [   64/  306]
train() client id: f_00004-12-2 loss: 1.162758  [   96/  306]
train() client id: f_00004-12-3 loss: 1.290437  [  128/  306]
train() client id: f_00004-12-4 loss: 1.221695  [  160/  306]
train() client id: f_00004-12-5 loss: 1.194729  [  192/  306]
train() client id: f_00004-12-6 loss: 1.087800  [  224/  306]
train() client id: f_00004-12-7 loss: 1.214241  [  256/  306]
train() client id: f_00004-12-8 loss: 1.141942  [  288/  306]
train() client id: f_00005-0-0 loss: 1.131440  [   32/  146]
train() client id: f_00005-0-1 loss: 1.048885  [   64/  146]
train() client id: f_00005-0-2 loss: 1.066544  [   96/  146]
train() client id: f_00005-0-3 loss: 1.134499  [  128/  146]
train() client id: f_00005-1-0 loss: 1.043662  [   32/  146]
train() client id: f_00005-1-1 loss: 1.023424  [   64/  146]
train() client id: f_00005-1-2 loss: 1.108262  [   96/  146]
train() client id: f_00005-1-3 loss: 1.112531  [  128/  146]
train() client id: f_00005-2-0 loss: 1.096082  [   32/  146]
train() client id: f_00005-2-1 loss: 1.023648  [   64/  146]
train() client id: f_00005-2-2 loss: 1.135705  [   96/  146]
train() client id: f_00005-2-3 loss: 1.066395  [  128/  146]
train() client id: f_00005-3-0 loss: 1.053701  [   32/  146]
train() client id: f_00005-3-1 loss: 1.112590  [   64/  146]
train() client id: f_00005-3-2 loss: 1.140535  [   96/  146]
train() client id: f_00005-3-3 loss: 1.051352  [  128/  146]
train() client id: f_00005-4-0 loss: 1.094418  [   32/  146]
train() client id: f_00005-4-1 loss: 1.179819  [   64/  146]
train() client id: f_00005-4-2 loss: 1.105672  [   96/  146]
train() client id: f_00005-4-3 loss: 0.996139  [  128/  146]
train() client id: f_00005-5-0 loss: 1.103741  [   32/  146]
train() client id: f_00005-5-1 loss: 1.161794  [   64/  146]
train() client id: f_00005-5-2 loss: 1.052048  [   96/  146]
train() client id: f_00005-5-3 loss: 1.101102  [  128/  146]
train() client id: f_00005-6-0 loss: 1.110066  [   32/  146]
train() client id: f_00005-6-1 loss: 1.152359  [   64/  146]
train() client id: f_00005-6-2 loss: 1.109836  [   96/  146]
train() client id: f_00005-6-3 loss: 1.060848  [  128/  146]
train() client id: f_00005-7-0 loss: 1.177389  [   32/  146]
train() client id: f_00005-7-1 loss: 1.089942  [   64/  146]
train() client id: f_00005-7-2 loss: 1.052222  [   96/  146]
train() client id: f_00005-7-3 loss: 1.038211  [  128/  146]
train() client id: f_00005-8-0 loss: 1.189523  [   32/  146]
train() client id: f_00005-8-1 loss: 1.128846  [   64/  146]
train() client id: f_00005-8-2 loss: 1.105255  [   96/  146]
train() client id: f_00005-8-3 loss: 1.087159  [  128/  146]
train() client id: f_00005-9-0 loss: 1.186093  [   32/  146]
train() client id: f_00005-9-1 loss: 1.087453  [   64/  146]
train() client id: f_00005-9-2 loss: 1.099911  [   96/  146]
train() client id: f_00005-9-3 loss: 1.161739  [  128/  146]
train() client id: f_00005-10-0 loss: 1.276834  [   32/  146]
train() client id: f_00005-10-1 loss: 1.093357  [   64/  146]
train() client id: f_00005-10-2 loss: 1.261318  [   96/  146]
train() client id: f_00005-10-3 loss: 1.050261  [  128/  146]
train() client id: f_00005-11-0 loss: 1.043605  [   32/  146]
train() client id: f_00005-11-1 loss: 1.211928  [   64/  146]
train() client id: f_00005-11-2 loss: 1.197039  [   96/  146]
train() client id: f_00005-11-3 loss: 1.206471  [  128/  146]
train() client id: f_00005-12-0 loss: 1.192259  [   32/  146]
train() client id: f_00005-12-1 loss: 1.177072  [   64/  146]
train() client id: f_00005-12-2 loss: 1.230134  [   96/  146]
train() client id: f_00005-12-3 loss: 1.121497  [  128/  146]
train() client id: f_00006-0-0 loss: 1.047826  [   32/   54]
train() client id: f_00006-1-0 loss: 1.065811  [   32/   54]
train() client id: f_00006-2-0 loss: 1.086334  [   32/   54]
train() client id: f_00006-3-0 loss: 1.058381  [   32/   54]
train() client id: f_00006-4-0 loss: 1.062488  [   32/   54]
train() client id: f_00006-5-0 loss: 1.061833  [   32/   54]
train() client id: f_00006-6-0 loss: 1.054613  [   32/   54]
train() client id: f_00006-7-0 loss: 1.096008  [   32/   54]
train() client id: f_00006-8-0 loss: 1.054097  [   32/   54]
train() client id: f_00006-9-0 loss: 1.071773  [   32/   54]
train() client id: f_00006-10-0 loss: 1.073275  [   32/   54]
train() client id: f_00006-11-0 loss: 1.101050  [   32/   54]
train() client id: f_00006-12-0 loss: 1.046964  [   32/   54]
train() client id: f_00007-0-0 loss: 1.037643  [   32/  179]
train() client id: f_00007-0-1 loss: 1.083662  [   64/  179]
train() client id: f_00007-0-2 loss: 0.982559  [   96/  179]
train() client id: f_00007-0-3 loss: 1.060309  [  128/  179]
train() client id: f_00007-0-4 loss: 1.121166  [  160/  179]
train() client id: f_00007-1-0 loss: 1.025699  [   32/  179]
train() client id: f_00007-1-1 loss: 1.015652  [   64/  179]
train() client id: f_00007-1-2 loss: 1.061456  [   96/  179]
train() client id: f_00007-1-3 loss: 1.128761  [  128/  179]
train() client id: f_00007-1-4 loss: 1.061415  [  160/  179]
train() client id: f_00007-2-0 loss: 1.028451  [   32/  179]
train() client id: f_00007-2-1 loss: 1.037027  [   64/  179]
train() client id: f_00007-2-2 loss: 1.102590  [   96/  179]
train() client id: f_00007-2-3 loss: 1.115571  [  128/  179]
train() client id: f_00007-2-4 loss: 1.127967  [  160/  179]
train() client id: f_00007-3-0 loss: 1.165799  [   32/  179]
train() client id: f_00007-3-1 loss: 1.126069  [   64/  179]
train() client id: f_00007-3-2 loss: 1.064443  [   96/  179]
train() client id: f_00007-3-3 loss: 1.111532  [  128/  179]
train() client id: f_00007-3-4 loss: 1.157849  [  160/  179]
train() client id: f_00007-4-0 loss: 1.091400  [   32/  179]
train() client id: f_00007-4-1 loss: 1.111288  [   64/  179]
train() client id: f_00007-4-2 loss: 1.210783  [   96/  179]
train() client id: f_00007-4-3 loss: 1.178125  [  128/  179]
train() client id: f_00007-4-4 loss: 1.165240  [  160/  179]
train() client id: f_00007-5-0 loss: 1.189327  [   32/  179]
train() client id: f_00007-5-1 loss: 1.159294  [   64/  179]
train() client id: f_00007-5-2 loss: 1.234599  [   96/  179]
train() client id: f_00007-5-3 loss: 1.155989  [  128/  179]
train() client id: f_00007-5-4 loss: 1.205790  [  160/  179]
train() client id: f_00007-6-0 loss: 1.230125  [   32/  179]
train() client id: f_00007-6-1 loss: 1.245926  [   64/  179]
train() client id: f_00007-6-2 loss: 1.164574  [   96/  179]
train() client id: f_00007-6-3 loss: 1.263000  [  128/  179]
train() client id: f_00007-6-4 loss: 1.201651  [  160/  179]
train() client id: f_00007-7-0 loss: 1.253970  [   32/  179]
train() client id: f_00007-7-1 loss: 1.275485  [   64/  179]
train() client id: f_00007-7-2 loss: 1.303576  [   96/  179]
train() client id: f_00007-7-3 loss: 1.232671  [  128/  179]
train() client id: f_00007-7-4 loss: 1.328608  [  160/  179]
train() client id: f_00007-8-0 loss: 1.290023  [   32/  179]
train() client id: f_00007-8-1 loss: 1.286362  [   64/  179]
train() client id: f_00007-8-2 loss: 1.294894  [   96/  179]
train() client id: f_00007-8-3 loss: 1.348153  [  128/  179]
train() client id: f_00007-8-4 loss: 1.346260  [  160/  179]
train() client id: f_00007-9-0 loss: 1.373627  [   32/  179]
train() client id: f_00007-9-1 loss: 1.364825  [   64/  179]
train() client id: f_00007-9-2 loss: 1.368969  [   96/  179]
train() client id: f_00007-9-3 loss: 1.351166  [  128/  179]
train() client id: f_00007-9-4 loss: 1.366347  [  160/  179]
train() client id: f_00007-10-0 loss: 1.385906  [   32/  179]
train() client id: f_00007-10-1 loss: 1.312757  [   64/  179]
train() client id: f_00007-10-2 loss: 1.482382  [   96/  179]
train() client id: f_00007-10-3 loss: 1.452970  [  128/  179]
train() client id: f_00007-10-4 loss: 1.390112  [  160/  179]
train() client id: f_00007-11-0 loss: 1.361531  [   32/  179]
train() client id: f_00007-11-1 loss: 1.463947  [   64/  179]
train() client id: f_00007-11-2 loss: 1.418858  [   96/  179]
train() client id: f_00007-11-3 loss: 1.478050  [  128/  179]
train() client id: f_00007-11-4 loss: 1.408865  [  160/  179]
train() client id: f_00007-12-0 loss: 1.472005  [   32/  179]
train() client id: f_00007-12-1 loss: 1.449445  [   64/  179]
train() client id: f_00007-12-2 loss: 1.475825  [   96/  179]
train() client id: f_00007-12-3 loss: 1.501620  [  128/  179]
train() client id: f_00007-12-4 loss: 1.487618  [  160/  179]
train() client id: f_00008-0-0 loss: 1.129773  [   32/  130]
train() client id: f_00008-0-1 loss: 1.045158  [   64/  130]
train() client id: f_00008-0-2 loss: 0.956255  [   96/  130]
train() client id: f_00008-0-3 loss: 0.998370  [  128/  130]
train() client id: f_00008-1-0 loss: 1.054477  [   32/  130]
train() client id: f_00008-1-1 loss: 1.096374  [   64/  130]
train() client id: f_00008-1-2 loss: 1.021158  [   96/  130]
train() client id: f_00008-1-3 loss: 0.970104  [  128/  130]
train() client id: f_00008-2-0 loss: 1.066708  [   32/  130]
train() client id: f_00008-2-1 loss: 1.066128  [   64/  130]
train() client id: f_00008-2-2 loss: 1.016914  [   96/  130]
train() client id: f_00008-2-3 loss: 1.045515  [  128/  130]
train() client id: f_00008-3-0 loss: 1.073813  [   32/  130]
train() client id: f_00008-3-1 loss: 1.074500  [   64/  130]
train() client id: f_00008-3-2 loss: 0.974767  [   96/  130]
train() client id: f_00008-3-3 loss: 1.110281  [  128/  130]
train() client id: f_00008-4-0 loss: 1.138804  [   32/  130]
train() client id: f_00008-4-1 loss: 0.981087  [   64/  130]
train() client id: f_00008-4-2 loss: 1.103023  [   96/  130]
train() client id: f_00008-4-3 loss: 1.057064  [  128/  130]
train() client id: f_00008-5-0 loss: 1.011271  [   32/  130]
train() client id: f_00008-5-1 loss: 1.021615  [   64/  130]
train() client id: f_00008-5-2 loss: 1.094307  [   96/  130]
train() client id: f_00008-5-3 loss: 1.167991  [  128/  130]
train() client id: f_00008-6-0 loss: 1.013115  [   32/  130]
train() client id: f_00008-6-1 loss: 1.072810  [   64/  130]
train() client id: f_00008-6-2 loss: 1.153575  [   96/  130]
train() client id: f_00008-6-3 loss: 1.134865  [  128/  130]
train() client id: f_00008-7-0 loss: 1.046619  [   32/  130]
train() client id: f_00008-7-1 loss: 1.061813  [   64/  130]
train() client id: f_00008-7-2 loss: 1.017844  [   96/  130]
train() client id: f_00008-7-3 loss: 1.253465  [  128/  130]
train() client id: f_00008-8-0 loss: 1.112793  [   32/  130]
train() client id: f_00008-8-1 loss: 1.035002  [   64/  130]
train() client id: f_00008-8-2 loss: 1.190897  [   96/  130]
train() client id: f_00008-8-3 loss: 1.095645  [  128/  130]
train() client id: f_00008-9-0 loss: 1.164824  [   32/  130]
train() client id: f_00008-9-1 loss: 1.169114  [   64/  130]
train() client id: f_00008-9-2 loss: 1.011578  [   96/  130]
train() client id: f_00008-9-3 loss: 1.165921  [  128/  130]
train() client id: f_00008-10-0 loss: 1.103179  [   32/  130]
train() client id: f_00008-10-1 loss: 1.165440  [   64/  130]
train() client id: f_00008-10-2 loss: 1.128997  [   96/  130]
train() client id: f_00008-10-3 loss: 1.136889  [  128/  130]
train() client id: f_00008-11-0 loss: 1.072373  [   32/  130]
train() client id: f_00008-11-1 loss: 1.233355  [   64/  130]
train() client id: f_00008-11-2 loss: 1.112750  [   96/  130]
train() client id: f_00008-11-3 loss: 1.174244  [  128/  130]
train() client id: f_00008-12-0 loss: 1.160162  [   32/  130]
train() client id: f_00008-12-1 loss: 1.244723  [   64/  130]
train() client id: f_00008-12-2 loss: 1.047675  [   96/  130]
train() client id: f_00008-12-3 loss: 1.164316  [  128/  130]
train() client id: f_00009-0-0 loss: 1.101818  [   32/  118]
train() client id: f_00009-0-1 loss: 1.101500  [   64/  118]
train() client id: f_00009-0-2 loss: 1.156857  [   96/  118]
train() client id: f_00009-1-0 loss: 1.134131  [   32/  118]
train() client id: f_00009-1-1 loss: 1.128936  [   64/  118]
train() client id: f_00009-1-2 loss: 1.145267  [   96/  118]
train() client id: f_00009-2-0 loss: 1.108390  [   32/  118]
train() client id: f_00009-2-1 loss: 1.159699  [   64/  118]
train() client id: f_00009-2-2 loss: 1.155066  [   96/  118]
train() client id: f_00009-3-0 loss: 1.141574  [   32/  118]
train() client id: f_00009-3-1 loss: 1.137530  [   64/  118]
train() client id: f_00009-3-2 loss: 1.125095  [   96/  118]
train() client id: f_00009-4-0 loss: 1.140385  [   32/  118]
train() client id: f_00009-4-1 loss: 1.186602  [   64/  118]
train() client id: f_00009-4-2 loss: 1.149018  [   96/  118]
train() client id: f_00009-5-0 loss: 1.154785  [   32/  118]
train() client id: f_00009-5-1 loss: 1.140821  [   64/  118]
train() client id: f_00009-5-2 loss: 1.162505  [   96/  118]
train() client id: f_00009-6-0 loss: 1.126875  [   32/  118]
train() client id: f_00009-6-1 loss: 1.228244  [   64/  118]
train() client id: f_00009-6-2 loss: 1.128330  [   96/  118]
train() client id: f_00009-7-0 loss: 1.140365  [   32/  118]
train() client id: f_00009-7-1 loss: 1.202415  [   64/  118]
train() client id: f_00009-7-2 loss: 1.143489  [   96/  118]
train() client id: f_00009-8-0 loss: 1.205754  [   32/  118]
train() client id: f_00009-8-1 loss: 1.206496  [   64/  118]
train() client id: f_00009-8-2 loss: 1.183305  [   96/  118]
train() client id: f_00009-9-0 loss: 1.200966  [   32/  118]
train() client id: f_00009-9-1 loss: 1.244417  [   64/  118]
train() client id: f_00009-9-2 loss: 1.145408  [   96/  118]
train() client id: f_00009-10-0 loss: 1.190958  [   32/  118]
train() client id: f_00009-10-1 loss: 1.255290  [   64/  118]
train() client id: f_00009-10-2 loss: 1.172829  [   96/  118]
train() client id: f_00009-11-0 loss: 1.286342  [   32/  118]
train() client id: f_00009-11-1 loss: 1.206020  [   64/  118]
train() client id: f_00009-11-2 loss: 1.188273  [   96/  118]
train() client id: f_00009-12-0 loss: 1.218297  [   32/  118]
train() client id: f_00009-12-1 loss: 1.249783  [   64/  118]
train() client id: f_00009-12-2 loss: 1.261999  [   96/  118]
At round 0 accuracy: 0.29442970822281167
At round 0 training accuracy: 0.2702883970489604
At round 0 training loss: 1.2177354676829284
update_location
xs = [  1.0943416  -15.79968212  20.00902392  18.81129433 -34.02070377
 -21.04359014   2.55680806  -6.32485185   4.66397685 -17.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  17.54482414   9.35018685
 -17.18584926   2.37501568 -14.17765202  17.56900603   4.00148178]
dists_uav = [101.5407992  102.42858035 101.99071065 103.25543883 106.04166294
 103.62521942 100.06087131 101.19787334 101.63868679 101.52381707]
dists_bs = [236.19434294 225.31573798 261.13798756 249.70973192 217.52016588
 246.24779551 247.64047589 253.45510377 238.88001683 232.77727406]
uav_gains = [9.62490767e-11 9.41770035e-11 9.51910990e-11 9.23028553e-11
 8.63584788e-11 9.14815864e-11 9.98476140e-11 9.70665613e-11
 9.60174958e-11 9.62893323e-11]
bs_gains = [2.50114868e-11 2.85415667e-11 1.88824323e-11 2.14030060e-11
 3.14989018e-11 2.22562249e-11 2.19075354e-11 2.05291596e-11
 2.42320740e-11 2.60531636e-11]
Round 1
-------------------------------
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86475814 22.70386864 10.69549308  3.82673858 26.17607366 12.62512831
  4.75712203 15.35951406 11.26321529 10.24142851]
obj_prev = 128.51334030366516
eta_min = 2.311289869413348e-09	eta_max = 0.9179068358889176
af = 27.184654844112664	bf = 2.038244896144109	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.038244896144109	zeta = 50.92141160980498	eta = 0.5338550913006942
af = 27.184654844112664	bf = 2.038244896144109	zeta = 40.97475341706718	eta = 0.6634488941863763
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.19821510905128	eta = 0.693517670855259
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.11235513228073	eta = 0.6950400903288041
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.11214097403846	eta = 0.6950438960157429
eta = 0.6950438960157429
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [0.03001828 0.06313368 0.0295418  0.01024433 0.07290153 0.03478308
 0.01286498 0.04264501 0.03097124 0.02811235]
ene_total = [3.31978164 6.5061149  3.27744721 1.51901348 7.38134405 3.96296862
 1.74998819 4.47161385 3.59899574 3.32487331]
ti_comp = [0.26460659 0.24673409 0.26447904 0.26412078 0.24854123 0.24180084
 0.26502657 0.26470385 0.24355148 0.24498963]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.41454867e-05 2.58347910e-04 2.30360589e-05 9.63220790e-07
 3.92006301e-04 4.49850892e-05 1.89464606e-06 6.91773975e-05
 3.13021490e-05 2.31353594e-05]
ene_total = [0.58155701 0.76609949 0.58262017 0.58387567 0.76180371 0.79165776
 0.57569161 0.58478017 0.77442671 0.76055195]
optimize_network_iter = 0 obj = 6.763064260913875
eta = 0.6950438960157429
freqs = [5.67224657e+07 1.27938699e+08 5.58490306e+07 1.93932720e+07
 1.46658825e+08 7.19250640e+07 2.42711055e+07 8.05523076e+07
 6.35825283e+07 5.73745792e+07]
eta_min = 0.6596845988039914	eta_max = 0.6950438960157337
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 0.07315436494122368	eta = 0.9090909090909091
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 22.49041741434538	eta = 0.0029569912778037635
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.432729834459089	eta = 0.02733717784291988
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.3416402766936963	eta = 0.028400591154114474
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.3415910621216036	eta = 0.028401188065745816
eta = 0.028401188065745816
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.36378699e-04 2.52916596e-03 2.25517659e-04 9.42970754e-06
 3.83765052e-03 4.40393562e-04 1.85481443e-05 6.77230634e-04
 3.06440760e-04 2.26489788e-04]
ene_total = [0.18861483 0.30552365 0.1886691  0.18350388 0.33783864 0.25980791
 0.18116974 0.20096826 0.2509534  0.24454163]
ti_comp = [0.30264569 0.28477319 0.30251814 0.30215989 0.28658033 0.27983995
 0.30306567 0.30274295 0.28159058 0.28302874]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.76000760e-05 2.90006048e-04 2.63287260e-05 1.10052366e-06
 4.40899051e-04 5.02234785e-05 2.16658346e-06 7.90821828e-05
 3.50155897e-05 2.59210505e-05]
ene_total = [0.52141495 0.68909041 0.52235439 0.52322131 0.68665088 0.70983183
 0.51589861 0.52483088 0.69426636 0.68175732]
optimize_network_iter = 1 obj = 6.069316940771649
eta = 0.6596845988039914
freqs = [5.67111680e+07 1.26759149e+08 5.58345224e+07 1.93849279e+07
 1.45447947e+08 7.10682616e+07 2.42711055e+07 8.05399831e+07
 6.28865503e+07 5.67915891e+07]
eta_min = 0.659684598803993	eta_max = 0.6596845988039878
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 0.072022402521563	eta = 0.9090909090909091
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 22.489338538927885	eta = 0.002911375595591902
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.427629129214501	eta = 0.026970722420201213
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.3378597367303393	eta = 0.028006347153576604
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.337812514296872	eta = 0.028006912865265216
eta = 0.028006912865265216
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.37212058e-04 2.49249066e-03 2.26285293e-04 9.45857837e-06
 3.78935809e-03 4.31651518e-04 1.86209531e-05 6.79681002e-04
 3.00945552e-04 2.22781478e-04]
ene_total = [0.18856219 0.30434926 0.18861456 0.18343028 0.3363184  0.25945213
 0.18109834 0.20095692 0.25069421 0.24433622]
ti_comp = [0.30264569 0.28477319 0.30251814 0.30215989 0.28658033 0.27983995
 0.30306567 0.30274295 0.28159058 0.28302874]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.76000760e-05 2.90006048e-04 2.63287260e-05 1.10052366e-06
 4.40899051e-04 5.02234785e-05 2.16658346e-06 7.90821828e-05
 3.50155897e-05 2.59210505e-05]
ene_total = [0.52141495 0.68909041 0.52235439 0.52322131 0.68665088 0.70983183
 0.51589861 0.52483088 0.69426636 0.68175732]
optimize_network_iter = 2 obj = 6.069316940771676
eta = 0.659684598803993
freqs = [5.67111680e+07 1.26759149e+08 5.58345224e+07 1.93849279e+07
 1.45447947e+08 7.10682616e+07 2.42711055e+07 8.05399831e+07
 6.28865503e+07 5.67915891e+07]
Done!
At round 1 eta: 0.659684598803993
At round 1 local rounds: 13.621738685881681
At round 1 global rounds: 81.80663264560978
At round 1 a_n: 27.49751116231574
gradient difference: 0.47880473732948303
train() client id: f_00000-0-0 loss: 1.090940  [   32/  126]
train() client id: f_00000-0-1 loss: 1.068534  [   64/  126]
train() client id: f_00000-0-2 loss: 1.091184  [   96/  126]
train() client id: f_00000-1-0 loss: 1.054379  [   32/  126]
train() client id: f_00000-1-1 loss: 0.999810  [   64/  126]
train() client id: f_00000-1-2 loss: 1.033349  [   96/  126]
train() client id: f_00000-2-0 loss: 1.016404  [   32/  126]
train() client id: f_00000-2-1 loss: 0.998739  [   64/  126]
train() client id: f_00000-2-2 loss: 0.920891  [   96/  126]
train() client id: f_00000-3-0 loss: 0.911779  [   32/  126]
train() client id: f_00000-3-1 loss: 0.927673  [   64/  126]
train() client id: f_00000-3-2 loss: 1.039156  [   96/  126]
train() client id: f_00000-4-0 loss: 0.918706  [   32/  126]
train() client id: f_00000-4-1 loss: 0.978284  [   64/  126]
train() client id: f_00000-4-2 loss: 0.877710  [   96/  126]
train() client id: f_00000-5-0 loss: 0.922417  [   32/  126]
train() client id: f_00000-5-1 loss: 0.790944  [   64/  126]
train() client id: f_00000-5-2 loss: 0.917665  [   96/  126]
train() client id: f_00000-6-0 loss: 0.815575  [   32/  126]
train() client id: f_00000-6-1 loss: 0.816550  [   64/  126]
train() client id: f_00000-6-2 loss: 0.910778  [   96/  126]
train() client id: f_00000-7-0 loss: 0.758038  [   32/  126]
train() client id: f_00000-7-1 loss: 0.840037  [   64/  126]
train() client id: f_00000-7-2 loss: 0.948001  [   96/  126]
train() client id: f_00000-8-0 loss: 0.924391  [   32/  126]
train() client id: f_00000-8-1 loss: 0.863343  [   64/  126]
train() client id: f_00000-8-2 loss: 0.864594  [   96/  126]
train() client id: f_00000-9-0 loss: 0.738401  [   32/  126]
train() client id: f_00000-9-1 loss: 0.908283  [   64/  126]
train() client id: f_00000-9-2 loss: 0.878255  [   96/  126]
train() client id: f_00000-10-0 loss: 0.863851  [   32/  126]
train() client id: f_00000-10-1 loss: 0.864509  [   64/  126]
train() client id: f_00000-10-2 loss: 0.847834  [   96/  126]
train() client id: f_00000-11-0 loss: 0.831348  [   32/  126]
train() client id: f_00000-11-1 loss: 0.883608  [   64/  126]
train() client id: f_00000-11-2 loss: 0.795958  [   96/  126]
train() client id: f_00000-12-0 loss: 0.867235  [   32/  126]
train() client id: f_00000-12-1 loss: 0.806260  [   64/  126]
train() client id: f_00000-12-2 loss: 0.823664  [   96/  126]
train() client id: f_00001-0-0 loss: 0.856221  [   32/  265]
train() client id: f_00001-0-1 loss: 0.857312  [   64/  265]
train() client id: f_00001-0-2 loss: 0.859773  [   96/  265]
train() client id: f_00001-0-3 loss: 0.879303  [  128/  265]
train() client id: f_00001-0-4 loss: 0.859197  [  160/  265]
train() client id: f_00001-0-5 loss: 0.829446  [  192/  265]
train() client id: f_00001-0-6 loss: 0.843855  [  224/  265]
train() client id: f_00001-0-7 loss: 0.952744  [  256/  265]
train() client id: f_00001-1-0 loss: 0.767409  [   32/  265]
train() client id: f_00001-1-1 loss: 0.849297  [   64/  265]
train() client id: f_00001-1-2 loss: 0.831364  [   96/  265]
train() client id: f_00001-1-3 loss: 0.778748  [  128/  265]
train() client id: f_00001-1-4 loss: 0.782169  [  160/  265]
train() client id: f_00001-1-5 loss: 0.721251  [  192/  265]
train() client id: f_00001-1-6 loss: 0.776959  [  224/  265]
train() client id: f_00001-1-7 loss: 0.747408  [  256/  265]
train() client id: f_00001-2-0 loss: 0.711387  [   32/  265]
train() client id: f_00001-2-1 loss: 0.840767  [   64/  265]
train() client id: f_00001-2-2 loss: 0.722957  [   96/  265]
train() client id: f_00001-2-3 loss: 0.723198  [  128/  265]
train() client id: f_00001-2-4 loss: 0.707555  [  160/  265]
train() client id: f_00001-2-5 loss: 0.734700  [  192/  265]
train() client id: f_00001-2-6 loss: 0.656468  [  224/  265]
train() client id: f_00001-2-7 loss: 0.646726  [  256/  265]
train() client id: f_00001-3-0 loss: 0.675473  [   32/  265]
train() client id: f_00001-3-1 loss: 0.692385  [   64/  265]
train() client id: f_00001-3-2 loss: 0.574404  [   96/  265]
train() client id: f_00001-3-3 loss: 0.712328  [  128/  265]
train() client id: f_00001-3-4 loss: 0.679756  [  160/  265]
train() client id: f_00001-3-5 loss: 0.634750  [  192/  265]
train() client id: f_00001-3-6 loss: 0.735315  [  224/  265]
train() client id: f_00001-3-7 loss: 0.664858  [  256/  265]
train() client id: f_00001-4-0 loss: 0.604462  [   32/  265]
train() client id: f_00001-4-1 loss: 0.699185  [   64/  265]
train() client id: f_00001-4-2 loss: 0.662753  [   96/  265]
train() client id: f_00001-4-3 loss: 0.658035  [  128/  265]
train() client id: f_00001-4-4 loss: 0.651889  [  160/  265]
train() client id: f_00001-4-5 loss: 0.571450  [  192/  265]
train() client id: f_00001-4-6 loss: 0.651212  [  224/  265]
train() client id: f_00001-4-7 loss: 0.632191  [  256/  265]
train() client id: f_00001-5-0 loss: 0.648149  [   32/  265]
train() client id: f_00001-5-1 loss: 0.626297  [   64/  265]
train() client id: f_00001-5-2 loss: 0.589449  [   96/  265]
train() client id: f_00001-5-3 loss: 0.663532  [  128/  265]
train() client id: f_00001-5-4 loss: 0.554937  [  160/  265]
train() client id: f_00001-5-5 loss: 0.535419  [  192/  265]
train() client id: f_00001-5-6 loss: 0.687509  [  224/  265]
train() client id: f_00001-5-7 loss: 0.574865  [  256/  265]
train() client id: f_00001-6-0 loss: 0.613520  [   32/  265]
train() client id: f_00001-6-1 loss: 0.544696  [   64/  265]
train() client id: f_00001-6-2 loss: 0.564125  [   96/  265]
train() client id: f_00001-6-3 loss: 0.697203  [  128/  265]
train() client id: f_00001-6-4 loss: 0.578773  [  160/  265]
train() client id: f_00001-6-5 loss: 0.528161  [  192/  265]
train() client id: f_00001-6-6 loss: 0.682716  [  224/  265]
train() client id: f_00001-6-7 loss: 0.541168  [  256/  265]
train() client id: f_00001-7-0 loss: 0.547082  [   32/  265]
train() client id: f_00001-7-1 loss: 0.509865  [   64/  265]
train() client id: f_00001-7-2 loss: 0.661341  [   96/  265]
train() client id: f_00001-7-3 loss: 0.566050  [  128/  265]
train() client id: f_00001-7-4 loss: 0.565865  [  160/  265]
train() client id: f_00001-7-5 loss: 0.618976  [  192/  265]
train() client id: f_00001-7-6 loss: 0.625556  [  224/  265]
train() client id: f_00001-7-7 loss: 0.536199  [  256/  265]
train() client id: f_00001-8-0 loss: 0.554548  [   32/  265]
train() client id: f_00001-8-1 loss: 0.554573  [   64/  265]
train() client id: f_00001-8-2 loss: 0.661342  [   96/  265]
train() client id: f_00001-8-3 loss: 0.602649  [  128/  265]
train() client id: f_00001-8-4 loss: 0.577154  [  160/  265]
train() client id: f_00001-8-5 loss: 0.551569  [  192/  265]
train() client id: f_00001-8-6 loss: 0.484772  [  224/  265]
train() client id: f_00001-8-7 loss: 0.625857  [  256/  265]
train() client id: f_00001-9-0 loss: 0.599770  [   32/  265]
train() client id: f_00001-9-1 loss: 0.568211  [   64/  265]
train() client id: f_00001-9-2 loss: 0.576783  [   96/  265]
train() client id: f_00001-9-3 loss: 0.552469  [  128/  265]
train() client id: f_00001-9-4 loss: 0.489060  [  160/  265]
train() client id: f_00001-9-5 loss: 0.507304  [  192/  265]
train() client id: f_00001-9-6 loss: 0.609642  [  224/  265]
train() client id: f_00001-9-7 loss: 0.606945  [  256/  265]
train() client id: f_00001-10-0 loss: 0.562794  [   32/  265]
train() client id: f_00001-10-1 loss: 0.557011  [   64/  265]
train() client id: f_00001-10-2 loss: 0.542593  [   96/  265]
train() client id: f_00001-10-3 loss: 0.533188  [  128/  265]
train() client id: f_00001-10-4 loss: 0.583258  [  160/  265]
train() client id: f_00001-10-5 loss: 0.619331  [  192/  265]
train() client id: f_00001-10-6 loss: 0.514728  [  224/  265]
train() client id: f_00001-10-7 loss: 0.596607  [  256/  265]
train() client id: f_00001-11-0 loss: 0.592872  [   32/  265]
train() client id: f_00001-11-1 loss: 0.522256  [   64/  265]
train() client id: f_00001-11-2 loss: 0.584463  [   96/  265]
train() client id: f_00001-11-3 loss: 0.556588  [  128/  265]
train() client id: f_00001-11-4 loss: 0.606695  [  160/  265]
train() client id: f_00001-11-5 loss: 0.531540  [  192/  265]
train() client id: f_00001-11-6 loss: 0.513359  [  224/  265]
train() client id: f_00001-11-7 loss: 0.540701  [  256/  265]
train() client id: f_00001-12-0 loss: 0.554199  [   32/  265]
train() client id: f_00001-12-1 loss: 0.615620  [   64/  265]
train() client id: f_00001-12-2 loss: 0.530540  [   96/  265]
train() client id: f_00001-12-3 loss: 0.460715  [  128/  265]
train() client id: f_00001-12-4 loss: 0.508112  [  160/  265]
train() client id: f_00001-12-5 loss: 0.643230  [  192/  265]
train() client id: f_00001-12-6 loss: 0.582827  [  224/  265]
train() client id: f_00001-12-7 loss: 0.547375  [  256/  265]
train() client id: f_00002-0-0 loss: 1.104743  [   32/  124]
train() client id: f_00002-0-1 loss: 1.096057  [   64/  124]
train() client id: f_00002-0-2 loss: 1.061919  [   96/  124]
train() client id: f_00002-1-0 loss: 1.076105  [   32/  124]
train() client id: f_00002-1-1 loss: 1.027119  [   64/  124]
train() client id: f_00002-1-2 loss: 1.075118  [   96/  124]
train() client id: f_00002-2-0 loss: 1.048803  [   32/  124]
train() client id: f_00002-2-1 loss: 1.022119  [   64/  124]
train() client id: f_00002-2-2 loss: 0.999806  [   96/  124]
train() client id: f_00002-3-0 loss: 1.025867  [   32/  124]
train() client id: f_00002-3-1 loss: 1.010551  [   64/  124]
train() client id: f_00002-3-2 loss: 0.985620  [   96/  124]
train() client id: f_00002-4-0 loss: 0.959388  [   32/  124]
train() client id: f_00002-4-1 loss: 0.986420  [   64/  124]
train() client id: f_00002-4-2 loss: 0.953019  [   96/  124]
train() client id: f_00002-5-0 loss: 0.955574  [   32/  124]
train() client id: f_00002-5-1 loss: 0.971647  [   64/  124]
train() client id: f_00002-5-2 loss: 0.949654  [   96/  124]
train() client id: f_00002-6-0 loss: 0.968331  [   32/  124]
train() client id: f_00002-6-1 loss: 0.984653  [   64/  124]
train() client id: f_00002-6-2 loss: 0.912084  [   96/  124]
train() client id: f_00002-7-0 loss: 0.958417  [   32/  124]
train() client id: f_00002-7-1 loss: 0.963414  [   64/  124]
train() client id: f_00002-7-2 loss: 0.891413  [   96/  124]
train() client id: f_00002-8-0 loss: 0.991842  [   32/  124]
train() client id: f_00002-8-1 loss: 0.868386  [   64/  124]
train() client id: f_00002-8-2 loss: 0.919817  [   96/  124]
train() client id: f_00002-9-0 loss: 0.944567  [   32/  124]
train() client id: f_00002-9-1 loss: 0.917490  [   64/  124]
train() client id: f_00002-9-2 loss: 0.867260  [   96/  124]
train() client id: f_00002-10-0 loss: 1.009177  [   32/  124]
train() client id: f_00002-10-1 loss: 0.885111  [   64/  124]
train() client id: f_00002-10-2 loss: 0.855224  [   96/  124]
train() client id: f_00002-11-0 loss: 0.938384  [   32/  124]
train() client id: f_00002-11-1 loss: 0.853580  [   64/  124]
train() client id: f_00002-11-2 loss: 0.874939  [   96/  124]
train() client id: f_00002-12-0 loss: 0.845057  [   32/  124]
train() client id: f_00002-12-1 loss: 0.970487  [   64/  124]
train() client id: f_00002-12-2 loss: 0.853303  [   96/  124]
train() client id: f_00003-0-0 loss: 1.102637  [   32/   43]
train() client id: f_00003-1-0 loss: 1.118602  [   32/   43]
train() client id: f_00003-2-0 loss: 1.102769  [   32/   43]
train() client id: f_00003-3-0 loss: 1.104208  [   32/   43]
train() client id: f_00003-4-0 loss: 1.124972  [   32/   43]
train() client id: f_00003-5-0 loss: 1.097277  [   32/   43]
train() client id: f_00003-6-0 loss: 1.131252  [   32/   43]
train() client id: f_00003-7-0 loss: 1.124740  [   32/   43]
train() client id: f_00003-8-0 loss: 1.083851  [   32/   43]
train() client id: f_00003-9-0 loss: 1.101127  [   32/   43]
train() client id: f_00003-10-0 loss: 1.083720  [   32/   43]
train() client id: f_00003-11-0 loss: 1.126312  [   32/   43]
train() client id: f_00003-12-0 loss: 1.088365  [   32/   43]
train() client id: f_00004-0-0 loss: 1.040707  [   32/  306]
train() client id: f_00004-0-1 loss: 1.085140  [   64/  306]
train() client id: f_00004-0-2 loss: 1.121263  [   96/  306]
train() client id: f_00004-0-3 loss: 1.126138  [  128/  306]
train() client id: f_00004-0-4 loss: 1.070109  [  160/  306]
train() client id: f_00004-0-5 loss: 1.101221  [  192/  306]
train() client id: f_00004-0-6 loss: 1.115061  [  224/  306]
train() client id: f_00004-0-7 loss: 1.025180  [  256/  306]
train() client id: f_00004-0-8 loss: 1.031309  [  288/  306]
train() client id: f_00004-1-0 loss: 1.055809  [   32/  306]
train() client id: f_00004-1-1 loss: 1.066420  [   64/  306]
train() client id: f_00004-1-2 loss: 1.072283  [   96/  306]
train() client id: f_00004-1-3 loss: 1.034315  [  128/  306]
train() client id: f_00004-1-4 loss: 1.025831  [  160/  306]
train() client id: f_00004-1-5 loss: 1.054864  [  192/  306]
train() client id: f_00004-1-6 loss: 0.993683  [  224/  306]
train() client id: f_00004-1-7 loss: 1.132304  [  256/  306]
train() client id: f_00004-1-8 loss: 1.065082  [  288/  306]
train() client id: f_00004-2-0 loss: 1.077623  [   32/  306]
train() client id: f_00004-2-1 loss: 1.034356  [   64/  306]
train() client id: f_00004-2-2 loss: 1.045566  [   96/  306]
train() client id: f_00004-2-3 loss: 1.045311  [  128/  306]
train() client id: f_00004-2-4 loss: 1.068007  [  160/  306]
train() client id: f_00004-2-5 loss: 1.043114  [  192/  306]
train() client id: f_00004-2-6 loss: 1.000547  [  224/  306]
train() client id: f_00004-2-7 loss: 1.079817  [  256/  306]
train() client id: f_00004-2-8 loss: 1.039839  [  288/  306]
train() client id: f_00004-3-0 loss: 1.042855  [   32/  306]
train() client id: f_00004-3-1 loss: 1.043677  [   64/  306]
train() client id: f_00004-3-2 loss: 1.028419  [   96/  306]
train() client id: f_00004-3-3 loss: 1.011773  [  128/  306]
train() client id: f_00004-3-4 loss: 1.054891  [  160/  306]
train() client id: f_00004-3-5 loss: 1.038155  [  192/  306]
train() client id: f_00004-3-6 loss: 1.053589  [  224/  306]
train() client id: f_00004-3-7 loss: 1.047069  [  256/  306]
train() client id: f_00004-3-8 loss: 1.009433  [  288/  306]
train() client id: f_00004-4-0 loss: 1.029155  [   32/  306]
train() client id: f_00004-4-1 loss: 1.060303  [   64/  306]
train() client id: f_00004-4-2 loss: 1.049883  [   96/  306]
train() client id: f_00004-4-3 loss: 1.044448  [  128/  306]
train() client id: f_00004-4-4 loss: 1.017777  [  160/  306]
train() client id: f_00004-4-5 loss: 1.050692  [  192/  306]
train() client id: f_00004-4-6 loss: 0.999156  [  224/  306]
train() client id: f_00004-4-7 loss: 1.029468  [  256/  306]
train() client id: f_00004-4-8 loss: 1.036270  [  288/  306]
train() client id: f_00004-5-0 loss: 1.034942  [   32/  306]
train() client id: f_00004-5-1 loss: 1.015810  [   64/  306]
train() client id: f_00004-5-2 loss: 1.013748  [   96/  306]
train() client id: f_00004-5-3 loss: 1.031668  [  128/  306]
train() client id: f_00004-5-4 loss: 1.030492  [  160/  306]
train() client id: f_00004-5-5 loss: 1.044939  [  192/  306]
train() client id: f_00004-5-6 loss: 1.039626  [  224/  306]
train() client id: f_00004-5-7 loss: 1.057618  [  256/  306]
train() client id: f_00004-5-8 loss: 1.031439  [  288/  306]
train() client id: f_00004-6-0 loss: 1.055296  [   32/  306]
train() client id: f_00004-6-1 loss: 1.045079  [   64/  306]
train() client id: f_00004-6-2 loss: 1.050285  [   96/  306]
train() client id: f_00004-6-3 loss: 1.047881  [  128/  306]
train() client id: f_00004-6-4 loss: 1.002371  [  160/  306]
train() client id: f_00004-6-5 loss: 0.993542  [  192/  306]
train() client id: f_00004-6-6 loss: 1.019540  [  224/  306]
train() client id: f_00004-6-7 loss: 1.051772  [  256/  306]
train() client id: f_00004-6-8 loss: 1.063294  [  288/  306]
train() client id: f_00004-7-0 loss: 1.043318  [   32/  306]
train() client id: f_00004-7-1 loss: 0.995425  [   64/  306]
train() client id: f_00004-7-2 loss: 1.017210  [   96/  306]
train() client id: f_00004-7-3 loss: 1.071006  [  128/  306]
train() client id: f_00004-7-4 loss: 1.055434  [  160/  306]
train() client id: f_00004-7-5 loss: 1.040223  [  192/  306]
train() client id: f_00004-7-6 loss: 1.057714  [  224/  306]
train() client id: f_00004-7-7 loss: 0.976116  [  256/  306]
train() client id: f_00004-7-8 loss: 1.064775  [  288/  306]
train() client id: f_00004-8-0 loss: 1.076837  [   32/  306]
train() client id: f_00004-8-1 loss: 1.041480  [   64/  306]
train() client id: f_00004-8-2 loss: 0.961994  [   96/  306]
train() client id: f_00004-8-3 loss: 1.073859  [  128/  306]
train() client id: f_00004-8-4 loss: 1.000236  [  160/  306]
train() client id: f_00004-8-5 loss: 1.067685  [  192/  306]
train() client id: f_00004-8-6 loss: 1.022377  [  224/  306]
train() client id: f_00004-8-7 loss: 1.092242  [  256/  306]
train() client id: f_00004-8-8 loss: 1.063222  [  288/  306]
train() client id: f_00004-9-0 loss: 0.972948  [   32/  306]
train() client id: f_00004-9-1 loss: 1.083717  [   64/  306]
train() client id: f_00004-9-2 loss: 1.072720  [   96/  306]
train() client id: f_00004-9-3 loss: 1.025248  [  128/  306]
train() client id: f_00004-9-4 loss: 1.022538  [  160/  306]
train() client id: f_00004-9-5 loss: 1.069958  [  192/  306]
train() client id: f_00004-9-6 loss: 1.015654  [  224/  306]
train() client id: f_00004-9-7 loss: 1.132672  [  256/  306]
train() client id: f_00004-9-8 loss: 1.041740  [  288/  306]
train() client id: f_00004-10-0 loss: 1.015159  [   32/  306]
train() client id: f_00004-10-1 loss: 0.980072  [   64/  306]
train() client id: f_00004-10-2 loss: 1.043743  [   96/  306]
train() client id: f_00004-10-3 loss: 1.082346  [  128/  306]
train() client id: f_00004-10-4 loss: 1.103722  [  160/  306]
train() client id: f_00004-10-5 loss: 1.020045  [  192/  306]
train() client id: f_00004-10-6 loss: 1.104639  [  224/  306]
train() client id: f_00004-10-7 loss: 1.027475  [  256/  306]
train() client id: f_00004-10-8 loss: 1.087153  [  288/  306]
train() client id: f_00004-11-0 loss: 1.056213  [   32/  306]
train() client id: f_00004-11-1 loss: 1.095471  [   64/  306]
train() client id: f_00004-11-2 loss: 1.056223  [   96/  306]
train() client id: f_00004-11-3 loss: 1.026585  [  128/  306]
train() client id: f_00004-11-4 loss: 1.020123  [  160/  306]
train() client id: f_00004-11-5 loss: 1.060948  [  192/  306]
train() client id: f_00004-11-6 loss: 1.069816  [  224/  306]
train() client id: f_00004-11-7 loss: 1.059894  [  256/  306]
train() client id: f_00004-11-8 loss: 1.141984  [  288/  306]
train() client id: f_00004-12-0 loss: 1.060036  [   32/  306]
train() client id: f_00004-12-1 loss: 1.064609  [   64/  306]
train() client id: f_00004-12-2 loss: 1.025037  [   96/  306]
train() client id: f_00004-12-3 loss: 1.120423  [  128/  306]
train() client id: f_00004-12-4 loss: 1.096930  [  160/  306]
train() client id: f_00004-12-5 loss: 1.050148  [  192/  306]
train() client id: f_00004-12-6 loss: 1.046990  [  224/  306]
train() client id: f_00004-12-7 loss: 1.102811  [  256/  306]
train() client id: f_00004-12-8 loss: 0.964740  [  288/  306]
train() client id: f_00005-0-0 loss: 1.199049  [   32/  146]
train() client id: f_00005-0-1 loss: 1.168684  [   64/  146]
train() client id: f_00005-0-2 loss: 1.194028  [   96/  146]
train() client id: f_00005-0-3 loss: 1.142275  [  128/  146]
train() client id: f_00005-1-0 loss: 1.174661  [   32/  146]
train() client id: f_00005-1-1 loss: 1.164358  [   64/  146]
train() client id: f_00005-1-2 loss: 1.135624  [   96/  146]
train() client id: f_00005-1-3 loss: 1.044859  [  128/  146]
train() client id: f_00005-2-0 loss: 1.094772  [   32/  146]
train() client id: f_00005-2-1 loss: 1.064645  [   64/  146]
train() client id: f_00005-2-2 loss: 1.094563  [   96/  146]
train() client id: f_00005-2-3 loss: 1.105552  [  128/  146]
train() client id: f_00005-3-0 loss: 1.121399  [   32/  146]
train() client id: f_00005-3-1 loss: 1.061172  [   64/  146]
train() client id: f_00005-3-2 loss: 1.064719  [   96/  146]
train() client id: f_00005-3-3 loss: 1.030454  [  128/  146]
train() client id: f_00005-4-0 loss: 1.096488  [   32/  146]
train() client id: f_00005-4-1 loss: 1.033168  [   64/  146]
train() client id: f_00005-4-2 loss: 1.044047  [   96/  146]
train() client id: f_00005-4-3 loss: 0.996709  [  128/  146]
train() client id: f_00005-5-0 loss: 1.026406  [   32/  146]
train() client id: f_00005-5-1 loss: 1.044176  [   64/  146]
train() client id: f_00005-5-2 loss: 1.028902  [   96/  146]
train() client id: f_00005-5-3 loss: 1.022027  [  128/  146]
train() client id: f_00005-6-0 loss: 1.018113  [   32/  146]
train() client id: f_00005-6-1 loss: 0.928748  [   64/  146]
train() client id: f_00005-6-2 loss: 1.039230  [   96/  146]
train() client id: f_00005-6-3 loss: 1.042603  [  128/  146]
train() client id: f_00005-7-0 loss: 0.972313  [   32/  146]
train() client id: f_00005-7-1 loss: 1.029054  [   64/  146]
train() client id: f_00005-7-2 loss: 0.922692  [   96/  146]
train() client id: f_00005-7-3 loss: 1.084168  [  128/  146]
train() client id: f_00005-8-0 loss: 0.954854  [   32/  146]
train() client id: f_00005-8-1 loss: 0.994179  [   64/  146]
train() client id: f_00005-8-2 loss: 1.012348  [   96/  146]
train() client id: f_00005-8-3 loss: 0.970267  [  128/  146]
train() client id: f_00005-9-0 loss: 0.931784  [   32/  146]
train() client id: f_00005-9-1 loss: 1.008900  [   64/  146]
train() client id: f_00005-9-2 loss: 0.978848  [   96/  146]
train() client id: f_00005-9-3 loss: 1.063215  [  128/  146]
train() client id: f_00005-10-0 loss: 1.058126  [   32/  146]
train() client id: f_00005-10-1 loss: 0.999836  [   64/  146]
train() client id: f_00005-10-2 loss: 1.037259  [   96/  146]
train() client id: f_00005-10-3 loss: 0.899976  [  128/  146]
train() client id: f_00005-11-0 loss: 0.933906  [   32/  146]
train() client id: f_00005-11-1 loss: 0.950853  [   64/  146]
train() client id: f_00005-11-2 loss: 1.009610  [   96/  146]
train() client id: f_00005-11-3 loss: 1.118040  [  128/  146]
train() client id: f_00005-12-0 loss: 0.980488  [   32/  146]
train() client id: f_00005-12-1 loss: 0.928709  [   64/  146]
train() client id: f_00005-12-2 loss: 1.008812  [   96/  146]
train() client id: f_00005-12-3 loss: 1.022444  [  128/  146]
train() client id: f_00006-0-0 loss: 1.066431  [   32/   54]
train() client id: f_00006-1-0 loss: 1.090785  [   32/   54]
train() client id: f_00006-2-0 loss: 1.109038  [   32/   54]
train() client id: f_00006-3-0 loss: 1.096479  [   32/   54]
train() client id: f_00006-4-0 loss: 1.109206  [   32/   54]
train() client id: f_00006-5-0 loss: 1.119791  [   32/   54]
train() client id: f_00006-6-0 loss: 1.136813  [   32/   54]
train() client id: f_00006-7-0 loss: 1.136376  [   32/   54]
train() client id: f_00006-8-0 loss: 1.104559  [   32/   54]
train() client id: f_00006-9-0 loss: 1.137138  [   32/   54]
train() client id: f_00006-10-0 loss: 1.108148  [   32/   54]
train() client id: f_00006-11-0 loss: 1.111619  [   32/   54]
train() client id: f_00006-12-0 loss: 1.115629  [   32/   54]
train() client id: f_00007-0-0 loss: 1.503976  [   32/  179]
train() client id: f_00007-0-1 loss: 1.504250  [   64/  179]
train() client id: f_00007-0-2 loss: 1.545391  [   96/  179]
train() client id: f_00007-0-3 loss: 1.438383  [  128/  179]
train() client id: f_00007-0-4 loss: 1.446478  [  160/  179]
train() client id: f_00007-1-0 loss: 1.315254  [   32/  179]
train() client id: f_00007-1-1 loss: 1.377577  [   64/  179]
train() client id: f_00007-1-2 loss: 1.314582  [   96/  179]
train() client id: f_00007-1-3 loss: 1.299711  [  128/  179]
train() client id: f_00007-1-4 loss: 1.322996  [  160/  179]
train() client id: f_00007-2-0 loss: 1.273709  [   32/  179]
train() client id: f_00007-2-1 loss: 1.220786  [   64/  179]
train() client id: f_00007-2-2 loss: 1.182110  [   96/  179]
train() client id: f_00007-2-3 loss: 1.233970  [  128/  179]
train() client id: f_00007-2-4 loss: 1.151045  [  160/  179]
train() client id: f_00007-3-0 loss: 1.163573  [   32/  179]
train() client id: f_00007-3-1 loss: 1.150778  [   64/  179]
train() client id: f_00007-3-2 loss: 1.089247  [   96/  179]
train() client id: f_00007-3-3 loss: 1.082468  [  128/  179]
train() client id: f_00007-3-4 loss: 1.025103  [  160/  179]
train() client id: f_00007-4-0 loss: 1.019419  [   32/  179]
train() client id: f_00007-4-1 loss: 1.032720  [   64/  179]
train() client id: f_00007-4-2 loss: 1.010682  [   96/  179]
train() client id: f_00007-4-3 loss: 1.032065  [  128/  179]
train() client id: f_00007-4-4 loss: 1.010165  [  160/  179]
train() client id: f_00007-5-0 loss: 1.013552  [   32/  179]
train() client id: f_00007-5-1 loss: 0.919513  [   64/  179]
train() client id: f_00007-5-2 loss: 0.984182  [   96/  179]
train() client id: f_00007-5-3 loss: 0.918626  [  128/  179]
train() client id: f_00007-5-4 loss: 0.925515  [  160/  179]
train() client id: f_00007-6-0 loss: 0.899723  [   32/  179]
train() client id: f_00007-6-1 loss: 0.897319  [   64/  179]
train() client id: f_00007-6-2 loss: 0.863410  [   96/  179]
train() client id: f_00007-6-3 loss: 0.870527  [  128/  179]
train() client id: f_00007-6-4 loss: 0.936000  [  160/  179]
train() client id: f_00007-7-0 loss: 0.905965  [   32/  179]
train() client id: f_00007-7-1 loss: 0.872286  [   64/  179]
train() client id: f_00007-7-2 loss: 0.857026  [   96/  179]
train() client id: f_00007-7-3 loss: 0.825761  [  128/  179]
train() client id: f_00007-7-4 loss: 0.826002  [  160/  179]
train() client id: f_00007-8-0 loss: 0.801294  [   32/  179]
train() client id: f_00007-8-1 loss: 0.827124  [   64/  179]
train() client id: f_00007-8-2 loss: 0.795804  [   96/  179]
train() client id: f_00007-8-3 loss: 0.836973  [  128/  179]
train() client id: f_00007-8-4 loss: 0.834122  [  160/  179]
train() client id: f_00007-9-0 loss: 0.830248  [   32/  179]
train() client id: f_00007-9-1 loss: 0.746873  [   64/  179]
train() client id: f_00007-9-2 loss: 0.780854  [   96/  179]
train() client id: f_00007-9-3 loss: 0.775239  [  128/  179]
train() client id: f_00007-9-4 loss: 0.843608  [  160/  179]
train() client id: f_00007-10-0 loss: 0.747704  [   32/  179]
train() client id: f_00007-10-1 loss: 0.804970  [   64/  179]
train() client id: f_00007-10-2 loss: 0.760759  [   96/  179]
train() client id: f_00007-10-3 loss: 0.824662  [  128/  179]
train() client id: f_00007-10-4 loss: 0.755699  [  160/  179]
train() client id: f_00007-11-0 loss: 0.708290  [   32/  179]
train() client id: f_00007-11-1 loss: 0.745452  [   64/  179]
train() client id: f_00007-11-2 loss: 0.752589  [   96/  179]
train() client id: f_00007-11-3 loss: 0.822974  [  128/  179]
train() client id: f_00007-11-4 loss: 0.708174  [  160/  179]
train() client id: f_00007-12-0 loss: 0.714250  [   32/  179]
train() client id: f_00007-12-1 loss: 0.732342  [   64/  179]
train() client id: f_00007-12-2 loss: 0.734275  [   96/  179]
train() client id: f_00007-12-3 loss: 0.707855  [  128/  179]
train() client id: f_00007-12-4 loss: 0.818123  [  160/  179]
train() client id: f_00008-0-0 loss: 1.088215  [   32/  130]
train() client id: f_00008-0-1 loss: 1.090463  [   64/  130]
train() client id: f_00008-0-2 loss: 1.041262  [   96/  130]
train() client id: f_00008-0-3 loss: 1.038428  [  128/  130]
train() client id: f_00008-1-0 loss: 1.013959  [   32/  130]
train() client id: f_00008-1-1 loss: 1.084660  [   64/  130]
train() client id: f_00008-1-2 loss: 1.036656  [   96/  130]
train() client id: f_00008-1-3 loss: 1.043289  [  128/  130]
train() client id: f_00008-2-0 loss: 1.000071  [   32/  130]
train() client id: f_00008-2-1 loss: 1.018286  [   64/  130]
train() client id: f_00008-2-2 loss: 1.090350  [   96/  130]
train() client id: f_00008-2-3 loss: 0.988127  [  128/  130]
train() client id: f_00008-3-0 loss: 0.981655  [   32/  130]
train() client id: f_00008-3-1 loss: 1.028892  [   64/  130]
train() client id: f_00008-3-2 loss: 1.004398  [   96/  130]
train() client id: f_00008-3-3 loss: 1.021304  [  128/  130]
train() client id: f_00008-4-0 loss: 0.930218  [   32/  130]
train() client id: f_00008-4-1 loss: 1.002604  [   64/  130]
train() client id: f_00008-4-2 loss: 1.031943  [   96/  130]
train() client id: f_00008-4-3 loss: 1.032349  [  128/  130]
train() client id: f_00008-5-0 loss: 0.852524  [   32/  130]
train() client id: f_00008-5-1 loss: 1.067186  [   64/  130]
train() client id: f_00008-5-2 loss: 0.933027  [   96/  130]
train() client id: f_00008-5-3 loss: 1.088036  [  128/  130]
train() client id: f_00008-6-0 loss: 1.063715  [   32/  130]
train() client id: f_00008-6-1 loss: 0.952673  [   64/  130]
train() client id: f_00008-6-2 loss: 0.936712  [   96/  130]
train() client id: f_00008-6-3 loss: 0.945666  [  128/  130]
train() client id: f_00008-7-0 loss: 0.969535  [   32/  130]
train() client id: f_00008-7-1 loss: 1.022897  [   64/  130]
train() client id: f_00008-7-2 loss: 0.971426  [   96/  130]
train() client id: f_00008-7-3 loss: 0.905514  [  128/  130]
train() client id: f_00008-8-0 loss: 0.954933  [   32/  130]
train() client id: f_00008-8-1 loss: 0.991638  [   64/  130]
train() client id: f_00008-8-2 loss: 0.951201  [   96/  130]
train() client id: f_00008-8-3 loss: 0.932485  [  128/  130]
train() client id: f_00008-9-0 loss: 0.918036  [   32/  130]
train() client id: f_00008-9-1 loss: 1.022337  [   64/  130]
train() client id: f_00008-9-2 loss: 0.925567  [   96/  130]
train() client id: f_00008-9-3 loss: 0.918080  [  128/  130]
train() client id: f_00008-10-0 loss: 0.916366  [   32/  130]
train() client id: f_00008-10-1 loss: 0.962640  [   64/  130]
train() client id: f_00008-10-2 loss: 0.833062  [   96/  130]
train() client id: f_00008-10-3 loss: 1.112756  [  128/  130]
train() client id: f_00008-11-0 loss: 0.951581  [   32/  130]
train() client id: f_00008-11-1 loss: 0.870592  [   64/  130]
train() client id: f_00008-11-2 loss: 0.963554  [   96/  130]
train() client id: f_00008-11-3 loss: 1.028303  [  128/  130]
train() client id: f_00008-12-0 loss: 1.079616  [   32/  130]
train() client id: f_00008-12-1 loss: 0.878511  [   64/  130]
train() client id: f_00008-12-2 loss: 0.863172  [   96/  130]
train() client id: f_00008-12-3 loss: 0.966603  [  128/  130]
train() client id: f_00009-0-0 loss: 1.259821  [   32/  118]
train() client id: f_00009-0-1 loss: 1.197310  [   64/  118]
train() client id: f_00009-0-2 loss: 1.201893  [   96/  118]
train() client id: f_00009-1-0 loss: 1.156019  [   32/  118]
train() client id: f_00009-1-1 loss: 1.195762  [   64/  118]
train() client id: f_00009-1-2 loss: 1.179953  [   96/  118]
train() client id: f_00009-2-0 loss: 1.159282  [   32/  118]
train() client id: f_00009-2-1 loss: 1.172291  [   64/  118]
train() client id: f_00009-2-2 loss: 1.131096  [   96/  118]
train() client id: f_00009-3-0 loss: 1.127037  [   32/  118]
train() client id: f_00009-3-1 loss: 1.096143  [   64/  118]
train() client id: f_00009-3-2 loss: 1.105453  [   96/  118]
train() client id: f_00009-4-0 loss: 1.044735  [   32/  118]
train() client id: f_00009-4-1 loss: 1.082730  [   64/  118]
train() client id: f_00009-4-2 loss: 1.112613  [   96/  118]
train() client id: f_00009-5-0 loss: 1.086029  [   32/  118]
train() client id: f_00009-5-1 loss: 1.046465  [   64/  118]
train() client id: f_00009-5-2 loss: 1.058792  [   96/  118]
train() client id: f_00009-6-0 loss: 1.071552  [   32/  118]
train() client id: f_00009-6-1 loss: 1.042235  [   64/  118]
train() client id: f_00009-6-2 loss: 1.016677  [   96/  118]
train() client id: f_00009-7-0 loss: 1.010485  [   32/  118]
train() client id: f_00009-7-1 loss: 0.995764  [   64/  118]
train() client id: f_00009-7-2 loss: 1.026932  [   96/  118]
train() client id: f_00009-8-0 loss: 1.026638  [   32/  118]
train() client id: f_00009-8-1 loss: 0.997446  [   64/  118]
train() client id: f_00009-8-2 loss: 0.990892  [   96/  118]
train() client id: f_00009-9-0 loss: 1.024500  [   32/  118]
train() client id: f_00009-9-1 loss: 0.950462  [   64/  118]
train() client id: f_00009-9-2 loss: 0.986295  [   96/  118]
train() client id: f_00009-10-0 loss: 0.991665  [   32/  118]
train() client id: f_00009-10-1 loss: 0.939122  [   64/  118]
train() client id: f_00009-10-2 loss: 0.947559  [   96/  118]
train() client id: f_00009-11-0 loss: 0.934709  [   32/  118]
train() client id: f_00009-11-1 loss: 0.987267  [   64/  118]
train() client id: f_00009-11-2 loss: 1.001059  [   96/  118]
train() client id: f_00009-12-0 loss: 0.995970  [   32/  118]
train() client id: f_00009-12-1 loss: 0.891961  [   64/  118]
train() client id: f_00009-12-2 loss: 1.037813  [   96/  118]
At round 1 accuracy: 0.3793103448275862
At round 1 training accuracy: 0.36284372904091217
At round 1 training loss: 1.100084507473691
update_location
xs = [ -3.9056584  -10.79968212  25.00902392  18.81129433 -29.02070377
 -16.04359014   2.55680806  -6.32485185   9.66397685 -12.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  12.54482414   9.35018685
 -17.18584926  -2.62498432  -9.17765202  17.56900603   4.00148178]
dists_uav = [101.60999206 101.77729242 103.08828885 102.52432593 104.54485756
 102.72657981 100.06711653 100.6192479  101.99050162 100.80414995]
dists_bs = [232.490482   228.87591608 264.89269296 252.89227345 220.7936039
 249.40437224 251.15145857 249.74529647 242.66293951 236.19832881]
uav_gains = [9.60853004e-11 9.56909168e-11 9.26774783e-11 9.39572756e-11
 8.94829513e-11 9.34954732e-11 9.98320356e-11 9.84681076e-11
 9.51915868e-11 9.80171753e-11]
bs_gains = [2.61432507e-11 2.73157933e-11 1.81425423e-11 2.06573456e-11
 3.02086918e-11 2.14764606e-11 2.10607635e-11 2.13944731e-11
 2.31891278e-11 2.50103050e-11]
Round 2
-------------------------------
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.73113195 22.42682549 10.56475866  3.77907907 25.85610411 12.47188502
  4.69859503 15.17007359 11.12712296 10.11765294]
obj_prev = 126.94322881666007
eta_min = 1.8621250529904804e-09	eta_max = 0.9180913765363099
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 29.535190418159765	eta = 0.9090909090909091
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 50.41602139462105	eta = 0.5325722332838158
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 40.52128855107027	eta = 0.6626189360582114
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.752941123532246	eta = 0.6928551054183996
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.66719520504412	eta = 0.6943915369355615
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.6669798224976	eta = 0.694395404830551
eta = 0.694395404830551
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [0.0300953  0.06329568 0.0296176  0.01027062 0.07308859 0.03487234
 0.01289799 0.04275444 0.03105071 0.02818449]
ene_total = [3.27908415 6.43447607 3.23987046 1.49764204 7.29869581 3.92072634
 1.72777469 4.41559147 3.56239059 3.2907282 ]
ti_comp = [0.26882441 0.25014105 0.26840555 0.26856527 0.25202169 0.24528326
 0.26926224 0.26910548 0.24689208 0.24842218]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.35742980e-05 2.53298284e-04 2.25396212e-05 9.38793768e-07
 3.84196142e-04 4.40541962e-05 1.84967284e-06 6.74494733e-05
 3.06958933e-05 2.26740888e-05]
ene_total = [0.57330776 0.76208585 0.57698348 0.57360274 0.75694218 0.78696767
 0.5674136  0.57472659 0.77128999 0.75680081]
optimize_network_iter = 0 obj = 6.700120669142431
eta = 0.694395404830551
freqs = [5.59757626e+07 1.26519974e+08 5.51732252e+07 1.91212720e+07
 1.45004571e+08 7.10858434e+07 2.39506050e+07 7.94380680e+07
 6.28831617e+07 5.67270011e+07]
eta_min = 0.6637595643039359	eta_max = 0.6943954048305468
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 0.07059948298279546	eta = 0.909090909090909
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 22.33326679569504	eta = 0.0028738002708385358
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.406693151543204	eta = 0.026667856733220745
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.3186248233022884	eta = 0.02768078195366155
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.3185795201975767	eta = 0.02768132281299047
eta = 0.02768132281299047
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.31855663e-04 2.49121488e-03 2.21679510e-04 9.23313400e-06
 3.77860887e-03 4.33277584e-04 1.81917241e-05 6.63372559e-04
 3.01897292e-04 2.23002013e-04]
ene_total = [0.18608375 0.30281643 0.18698052 0.18052076 0.33390587 0.25835522
 0.17880307 0.19749234 0.25008995 0.2435316 ]
ti_comp = [0.30213693 0.28345356 0.30171807 0.30187779 0.28533421 0.27859578
 0.30257476 0.302418   0.2802046  0.2817347 ]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.64813595e-05 2.79904609e-04 2.53103811e-05 1.05433666e-06
 4.25297547e-04 4.84557995e-05 2.07851021e-06 7.57844495e-05
 3.38154992e-05 2.50150645e-05]
ene_total = [0.52130977 0.69482585 0.52463943 0.52134959 0.69133623 0.71562472
 0.51573361 0.52304321 0.70127064 0.68803794]
optimize_network_iter = 1 obj = 6.097170992212053
eta = 0.6637595643039359
freqs = [5.59657273e+07 1.25464071e+08 5.51538439e+07 1.91158103e+07
 1.43920617e+08 7.03288863e+07 2.39506050e+07 7.94329736e+07
 6.22620637e+07 5.62078659e+07]
eta_min = 0.6637595643039447	eta_max = 0.6637595643039276
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 0.06961124159514927	eta = 0.909090909090909
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 22.33232490105157	eta = 0.0028336927384439636
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.4022158004882264	eta = 0.026343572834638552
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.3153036098319895	eta = 0.027332461555343614
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.315259971692997	eta = 0.02733297671898439
eta = 0.02733297671898439
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.32590817e-04 2.45845542e-03 2.22305891e-04 9.26043939e-06
 3.73546925e-03 4.25596504e-04 1.82559506e-05 6.65629235e-04
 2.97007961e-04 2.19712069e-04]
ene_total = [0.18603789 0.30178187 0.18693126 0.18045688 0.33256672 0.25804555
 0.17874085 0.19748541 0.24986216 0.24335138]
ti_comp = [0.30213693 0.28345356 0.30171807 0.30187779 0.28533421 0.27859578
 0.30257476 0.302418   0.2802046  0.2817347 ]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.64813595e-05 2.79904609e-04 2.53103811e-05 1.05433666e-06
 4.25297547e-04 4.84557995e-05 2.07851021e-06 7.57844495e-05
 3.38154992e-05 2.50150645e-05]
ene_total = [0.52130977 0.69482585 0.52463943 0.52134959 0.69133623 0.71562472
 0.51573361 0.52304321 0.70127064 0.68803794]
optimize_network_iter = 2 obj = 6.09717099221221
eta = 0.6637595643039447
freqs = [5.59657273e+07 1.25464071e+08 5.51538439e+07 1.91158103e+07
 1.43920617e+08 7.03288863e+07 2.39506050e+07 7.94329736e+07
 6.22620637e+07 5.62078659e+07]
Done!
At round 2 eta: 0.6637595643039447
At round 2 local rounds: 13.420089837852764
At round 2 global rounds: 81.77931100223806
At round 2 a_n: 27.154965315346423
gradient difference: 0.4845411777496338
train() client id: f_00000-0-0 loss: 1.240997  [   32/  126]
train() client id: f_00000-0-1 loss: 1.125689  [   64/  126]
train() client id: f_00000-0-2 loss: 1.089682  [   96/  126]
train() client id: f_00000-1-0 loss: 1.042863  [   32/  126]
train() client id: f_00000-1-1 loss: 1.078121  [   64/  126]
train() client id: f_00000-1-2 loss: 1.064893  [   96/  126]
train() client id: f_00000-2-0 loss: 0.995094  [   32/  126]
train() client id: f_00000-2-1 loss: 1.029474  [   64/  126]
train() client id: f_00000-2-2 loss: 1.013634  [   96/  126]
train() client id: f_00000-3-0 loss: 1.009518  [   32/  126]
train() client id: f_00000-3-1 loss: 0.958473  [   64/  126]
train() client id: f_00000-3-2 loss: 0.965354  [   96/  126]
train() client id: f_00000-4-0 loss: 0.973179  [   32/  126]
train() client id: f_00000-4-1 loss: 0.967117  [   64/  126]
train() client id: f_00000-4-2 loss: 0.914687  [   96/  126]
train() client id: f_00000-5-0 loss: 0.899933  [   32/  126]
train() client id: f_00000-5-1 loss: 0.945627  [   64/  126]
train() client id: f_00000-5-2 loss: 0.868394  [   96/  126]
train() client id: f_00000-6-0 loss: 0.961769  [   32/  126]
train() client id: f_00000-6-1 loss: 0.864875  [   64/  126]
train() client id: f_00000-6-2 loss: 0.866823  [   96/  126]
train() client id: f_00000-7-0 loss: 0.907044  [   32/  126]
train() client id: f_00000-7-1 loss: 0.828077  [   64/  126]
train() client id: f_00000-7-2 loss: 0.920357  [   96/  126]
train() client id: f_00000-8-0 loss: 0.871046  [   32/  126]
train() client id: f_00000-8-1 loss: 0.916367  [   64/  126]
train() client id: f_00000-8-2 loss: 0.918236  [   96/  126]
train() client id: f_00000-9-0 loss: 0.879923  [   32/  126]
train() client id: f_00000-9-1 loss: 0.869460  [   64/  126]
train() client id: f_00000-9-2 loss: 0.845159  [   96/  126]
train() client id: f_00000-10-0 loss: 0.831204  [   32/  126]
train() client id: f_00000-10-1 loss: 0.939915  [   64/  126]
train() client id: f_00000-10-2 loss: 0.865594  [   96/  126]
train() client id: f_00000-11-0 loss: 1.021981  [   32/  126]
train() client id: f_00000-11-1 loss: 0.858386  [   64/  126]
train() client id: f_00000-11-2 loss: 0.756124  [   96/  126]
train() client id: f_00000-12-0 loss: 0.920979  [   32/  126]
train() client id: f_00000-12-1 loss: 0.745475  [   64/  126]
train() client id: f_00000-12-2 loss: 1.046230  [   96/  126]
train() client id: f_00001-0-0 loss: 0.806278  [   32/  265]
train() client id: f_00001-0-1 loss: 0.783214  [   64/  265]
train() client id: f_00001-0-2 loss: 0.740691  [   96/  265]
train() client id: f_00001-0-3 loss: 0.728780  [  128/  265]
train() client id: f_00001-0-4 loss: 0.779725  [  160/  265]
train() client id: f_00001-0-5 loss: 0.766579  [  192/  265]
train() client id: f_00001-0-6 loss: 0.793701  [  224/  265]
train() client id: f_00001-0-7 loss: 0.715732  [  256/  265]
train() client id: f_00001-1-0 loss: 0.614354  [   32/  265]
train() client id: f_00001-1-1 loss: 0.782795  [   64/  265]
train() client id: f_00001-1-2 loss: 0.683369  [   96/  265]
train() client id: f_00001-1-3 loss: 0.784740  [  128/  265]
train() client id: f_00001-1-4 loss: 0.730008  [  160/  265]
train() client id: f_00001-1-5 loss: 0.805638  [  192/  265]
train() client id: f_00001-1-6 loss: 0.686231  [  224/  265]
train() client id: f_00001-1-7 loss: 0.621920  [  256/  265]
train() client id: f_00001-2-0 loss: 0.644503  [   32/  265]
train() client id: f_00001-2-1 loss: 0.624137  [   64/  265]
train() client id: f_00001-2-2 loss: 0.721023  [   96/  265]
train() client id: f_00001-2-3 loss: 0.691518  [  128/  265]
train() client id: f_00001-2-4 loss: 0.629123  [  160/  265]
train() client id: f_00001-2-5 loss: 0.821476  [  192/  265]
train() client id: f_00001-2-6 loss: 0.655923  [  224/  265]
train() client id: f_00001-2-7 loss: 0.589248  [  256/  265]
train() client id: f_00001-3-0 loss: 0.611032  [   32/  265]
train() client id: f_00001-3-1 loss: 0.723504  [   64/  265]
train() client id: f_00001-3-2 loss: 0.608864  [   96/  265]
train() client id: f_00001-3-3 loss: 0.604973  [  128/  265]
train() client id: f_00001-3-4 loss: 0.668850  [  160/  265]
train() client id: f_00001-3-5 loss: 0.667812  [  192/  265]
train() client id: f_00001-3-6 loss: 0.604241  [  224/  265]
train() client id: f_00001-3-7 loss: 0.686964  [  256/  265]
train() client id: f_00001-4-0 loss: 0.680719  [   32/  265]
train() client id: f_00001-4-1 loss: 0.688043  [   64/  265]
train() client id: f_00001-4-2 loss: 0.597145  [   96/  265]
train() client id: f_00001-4-3 loss: 0.618835  [  128/  265]
train() client id: f_00001-4-4 loss: 0.608665  [  160/  265]
train() client id: f_00001-4-5 loss: 0.589756  [  192/  265]
train() client id: f_00001-4-6 loss: 0.553298  [  224/  265]
train() client id: f_00001-4-7 loss: 0.689810  [  256/  265]
train() client id: f_00001-5-0 loss: 0.533314  [   32/  265]
train() client id: f_00001-5-1 loss: 0.615222  [   64/  265]
train() client id: f_00001-5-2 loss: 0.593419  [   96/  265]
train() client id: f_00001-5-3 loss: 0.599937  [  128/  265]
train() client id: f_00001-5-4 loss: 0.556803  [  160/  265]
train() client id: f_00001-5-5 loss: 0.695494  [  192/  265]
train() client id: f_00001-5-6 loss: 0.661608  [  224/  265]
train() client id: f_00001-5-7 loss: 0.650162  [  256/  265]
train() client id: f_00001-6-0 loss: 0.602745  [   32/  265]
train() client id: f_00001-6-1 loss: 0.648590  [   64/  265]
train() client id: f_00001-6-2 loss: 0.600167  [   96/  265]
train() client id: f_00001-6-3 loss: 0.605671  [  128/  265]
train() client id: f_00001-6-4 loss: 0.596308  [  160/  265]
train() client id: f_00001-6-5 loss: 0.549637  [  192/  265]
train() client id: f_00001-6-6 loss: 0.589572  [  224/  265]
train() client id: f_00001-6-7 loss: 0.647186  [  256/  265]
train() client id: f_00001-7-0 loss: 0.694398  [   32/  265]
train() client id: f_00001-7-1 loss: 0.648833  [   64/  265]
train() client id: f_00001-7-2 loss: 0.486756  [   96/  265]
train() client id: f_00001-7-3 loss: 0.566097  [  128/  265]
train() client id: f_00001-7-4 loss: 0.542812  [  160/  265]
train() client id: f_00001-7-5 loss: 0.602867  [  192/  265]
train() client id: f_00001-7-6 loss: 0.519795  [  224/  265]
train() client id: f_00001-7-7 loss: 0.634022  [  256/  265]
train() client id: f_00001-8-0 loss: 0.542074  [   32/  265]
train() client id: f_00001-8-1 loss: 0.616792  [   64/  265]
train() client id: f_00001-8-2 loss: 0.571163  [   96/  265]
train() client id: f_00001-8-3 loss: 0.640039  [  128/  265]
train() client id: f_00001-8-4 loss: 0.589903  [  160/  265]
train() client id: f_00001-8-5 loss: 0.661287  [  192/  265]
train() client id: f_00001-8-6 loss: 0.605319  [  224/  265]
train() client id: f_00001-8-7 loss: 0.459117  [  256/  265]
train() client id: f_00001-9-0 loss: 0.574519  [   32/  265]
train() client id: f_00001-9-1 loss: 0.627427  [   64/  265]
train() client id: f_00001-9-2 loss: 0.685439  [   96/  265]
train() client id: f_00001-9-3 loss: 0.605925  [  128/  265]
train() client id: f_00001-9-4 loss: 0.526296  [  160/  265]
train() client id: f_00001-9-5 loss: 0.518055  [  192/  265]
train() client id: f_00001-9-6 loss: 0.569323  [  224/  265]
train() client id: f_00001-9-7 loss: 0.579548  [  256/  265]
train() client id: f_00001-10-0 loss: 0.605033  [   32/  265]
train() client id: f_00001-10-1 loss: 0.539139  [   64/  265]
train() client id: f_00001-10-2 loss: 0.530043  [   96/  265]
train() client id: f_00001-10-3 loss: 0.608464  [  128/  265]
train() client id: f_00001-10-4 loss: 0.618494  [  160/  265]
train() client id: f_00001-10-5 loss: 0.545459  [  192/  265]
train() client id: f_00001-10-6 loss: 0.619448  [  224/  265]
train() client id: f_00001-10-7 loss: 0.605070  [  256/  265]
train() client id: f_00001-11-0 loss: 0.550182  [   32/  265]
train() client id: f_00001-11-1 loss: 0.490896  [   64/  265]
train() client id: f_00001-11-2 loss: 0.582409  [   96/  265]
train() client id: f_00001-11-3 loss: 0.643678  [  128/  265]
train() client id: f_00001-11-4 loss: 0.667341  [  160/  265]
train() client id: f_00001-11-5 loss: 0.614130  [  192/  265]
train() client id: f_00001-11-6 loss: 0.545780  [  224/  265]
train() client id: f_00001-11-7 loss: 0.572938  [  256/  265]
train() client id: f_00001-12-0 loss: 0.523015  [   32/  265]
train() client id: f_00001-12-1 loss: 0.629662  [   64/  265]
train() client id: f_00001-12-2 loss: 0.608928  [   96/  265]
train() client id: f_00001-12-3 loss: 0.560506  [  128/  265]
train() client id: f_00001-12-4 loss: 0.687480  [  160/  265]
train() client id: f_00001-12-5 loss: 0.546193  [  192/  265]
train() client id: f_00001-12-6 loss: 0.569127  [  224/  265]
train() client id: f_00001-12-7 loss: 0.552530  [  256/  265]
train() client id: f_00002-0-0 loss: 1.188271  [   32/  124]
train() client id: f_00002-0-1 loss: 1.171639  [   64/  124]
train() client id: f_00002-0-2 loss: 1.112454  [   96/  124]
train() client id: f_00002-1-0 loss: 1.128387  [   32/  124]
train() client id: f_00002-1-1 loss: 1.109814  [   64/  124]
train() client id: f_00002-1-2 loss: 1.131486  [   96/  124]
train() client id: f_00002-2-0 loss: 1.114128  [   32/  124]
train() client id: f_00002-2-1 loss: 1.108522  [   64/  124]
train() client id: f_00002-2-2 loss: 1.053940  [   96/  124]
train() client id: f_00002-3-0 loss: 1.061617  [   32/  124]
train() client id: f_00002-3-1 loss: 1.092892  [   64/  124]
train() client id: f_00002-3-2 loss: 1.101296  [   96/  124]
train() client id: f_00002-4-0 loss: 1.056123  [   32/  124]
train() client id: f_00002-4-1 loss: 1.050381  [   64/  124]
train() client id: f_00002-4-2 loss: 1.058050  [   96/  124]
train() client id: f_00002-5-0 loss: 1.066909  [   32/  124]
train() client id: f_00002-5-1 loss: 1.021217  [   64/  124]
train() client id: f_00002-5-2 loss: 1.019664  [   96/  124]
train() client id: f_00002-6-0 loss: 1.048587  [   32/  124]
train() client id: f_00002-6-1 loss: 1.015549  [   64/  124]
train() client id: f_00002-6-2 loss: 1.020835  [   96/  124]
train() client id: f_00002-7-0 loss: 1.026494  [   32/  124]
train() client id: f_00002-7-1 loss: 0.997507  [   64/  124]
train() client id: f_00002-7-2 loss: 1.018759  [   96/  124]
train() client id: f_00002-8-0 loss: 0.966963  [   32/  124]
train() client id: f_00002-8-1 loss: 1.022530  [   64/  124]
train() client id: f_00002-8-2 loss: 1.065421  [   96/  124]
train() client id: f_00002-9-0 loss: 0.970795  [   32/  124]
train() client id: f_00002-9-1 loss: 1.011757  [   64/  124]
train() client id: f_00002-9-2 loss: 1.025363  [   96/  124]
train() client id: f_00002-10-0 loss: 0.981020  [   32/  124]
train() client id: f_00002-10-1 loss: 1.035808  [   64/  124]
train() client id: f_00002-10-2 loss: 0.941768  [   96/  124]
train() client id: f_00002-11-0 loss: 0.999289  [   32/  124]
train() client id: f_00002-11-1 loss: 0.989137  [   64/  124]
train() client id: f_00002-11-2 loss: 1.021703  [   96/  124]
train() client id: f_00002-12-0 loss: 1.033437  [   32/  124]
train() client id: f_00002-12-1 loss: 0.953619  [   64/  124]
train() client id: f_00002-12-2 loss: 1.029351  [   96/  124]
train() client id: f_00003-0-0 loss: 1.137414  [   32/   43]
train() client id: f_00003-1-0 loss: 1.143674  [   32/   43]
train() client id: f_00003-2-0 loss: 1.118352  [   32/   43]
train() client id: f_00003-3-0 loss: 1.130923  [   32/   43]
train() client id: f_00003-4-0 loss: 1.125345  [   32/   43]
train() client id: f_00003-5-0 loss: 1.149962  [   32/   43]
train() client id: f_00003-6-0 loss: 1.108755  [   32/   43]
train() client id: f_00003-7-0 loss: 1.115145  [   32/   43]
train() client id: f_00003-8-0 loss: 1.119631  [   32/   43]
train() client id: f_00003-9-0 loss: 1.120455  [   32/   43]
train() client id: f_00003-10-0 loss: 1.101920  [   32/   43]
train() client id: f_00003-11-0 loss: 1.107579  [   32/   43]
train() client id: f_00003-12-0 loss: 1.110480  [   32/   43]
train() client id: f_00004-0-0 loss: 1.130038  [   32/  306]
train() client id: f_00004-0-1 loss: 1.068316  [   64/  306]
train() client id: f_00004-0-2 loss: 1.068676  [   96/  306]
train() client id: f_00004-0-3 loss: 1.109219  [  128/  306]
train() client id: f_00004-0-4 loss: 1.056238  [  160/  306]
train() client id: f_00004-0-5 loss: 1.086218  [  192/  306]
train() client id: f_00004-0-6 loss: 1.125632  [  224/  306]
train() client id: f_00004-0-7 loss: 1.098726  [  256/  306]
train() client id: f_00004-0-8 loss: 1.078959  [  288/  306]
train() client id: f_00004-1-0 loss: 1.120700  [   32/  306]
train() client id: f_00004-1-1 loss: 1.101477  [   64/  306]
train() client id: f_00004-1-2 loss: 1.053193  [   96/  306]
train() client id: f_00004-1-3 loss: 1.051297  [  128/  306]
train() client id: f_00004-1-4 loss: 1.097790  [  160/  306]
train() client id: f_00004-1-5 loss: 1.078347  [  192/  306]
train() client id: f_00004-1-6 loss: 1.082984  [  224/  306]
train() client id: f_00004-1-7 loss: 1.063606  [  256/  306]
train() client id: f_00004-1-8 loss: 1.096303  [  288/  306]
train() client id: f_00004-2-0 loss: 1.079787  [   32/  306]
train() client id: f_00004-2-1 loss: 1.088869  [   64/  306]
train() client id: f_00004-2-2 loss: 1.083587  [   96/  306]
train() client id: f_00004-2-3 loss: 1.081657  [  128/  306]
train() client id: f_00004-2-4 loss: 1.083969  [  160/  306]
train() client id: f_00004-2-5 loss: 1.094017  [  192/  306]
train() client id: f_00004-2-6 loss: 1.058692  [  224/  306]
train() client id: f_00004-2-7 loss: 1.052620  [  256/  306]
train() client id: f_00004-2-8 loss: 1.095045  [  288/  306]
train() client id: f_00004-3-0 loss: 1.058359  [   32/  306]
train() client id: f_00004-3-1 loss: 1.038138  [   64/  306]
train() client id: f_00004-3-2 loss: 1.075696  [   96/  306]
train() client id: f_00004-3-3 loss: 1.092408  [  128/  306]
train() client id: f_00004-3-4 loss: 1.079320  [  160/  306]
train() client id: f_00004-3-5 loss: 1.141262  [  192/  306]
train() client id: f_00004-3-6 loss: 1.035719  [  224/  306]
train() client id: f_00004-3-7 loss: 1.092716  [  256/  306]
train() client id: f_00004-3-8 loss: 1.094987  [  288/  306]
train() client id: f_00004-4-0 loss: 1.069324  [   32/  306]
train() client id: f_00004-4-1 loss: 1.094699  [   64/  306]
train() client id: f_00004-4-2 loss: 1.081227  [   96/  306]
train() client id: f_00004-4-3 loss: 1.056566  [  128/  306]
train() client id: f_00004-4-4 loss: 1.108234  [  160/  306]
train() client id: f_00004-4-5 loss: 1.105443  [  192/  306]
train() client id: f_00004-4-6 loss: 1.039104  [  224/  306]
train() client id: f_00004-4-7 loss: 1.103405  [  256/  306]
train() client id: f_00004-4-8 loss: 1.040562  [  288/  306]
train() client id: f_00004-5-0 loss: 1.080584  [   32/  306]
train() client id: f_00004-5-1 loss: 1.079820  [   64/  306]
train() client id: f_00004-5-2 loss: 1.085287  [   96/  306]
train() client id: f_00004-5-3 loss: 1.048685  [  128/  306]
train() client id: f_00004-5-4 loss: 1.041695  [  160/  306]
train() client id: f_00004-5-5 loss: 1.089604  [  192/  306]
train() client id: f_00004-5-6 loss: 1.109258  [  224/  306]
train() client id: f_00004-5-7 loss: 1.071703  [  256/  306]
train() client id: f_00004-5-8 loss: 1.075223  [  288/  306]
train() client id: f_00004-6-0 loss: 1.083703  [   32/  306]
train() client id: f_00004-6-1 loss: 1.001740  [   64/  306]
train() client id: f_00004-6-2 loss: 1.095096  [   96/  306]
train() client id: f_00004-6-3 loss: 1.137231  [  128/  306]
train() client id: f_00004-6-4 loss: 1.119431  [  160/  306]
train() client id: f_00004-6-5 loss: 1.142655  [  192/  306]
train() client id: f_00004-6-6 loss: 1.057170  [  224/  306]
train() client id: f_00004-6-7 loss: 1.042196  [  256/  306]
train() client id: f_00004-6-8 loss: 1.041641  [  288/  306]
train() client id: f_00004-7-0 loss: 1.086231  [   32/  306]
train() client id: f_00004-7-1 loss: 1.066037  [   64/  306]
train() client id: f_00004-7-2 loss: 1.080118  [   96/  306]
train() client id: f_00004-7-3 loss: 1.114420  [  128/  306]
train() client id: f_00004-7-4 loss: 1.057770  [  160/  306]
train() client id: f_00004-7-5 loss: 1.146915  [  192/  306]
train() client id: f_00004-7-6 loss: 1.072878  [  224/  306]
train() client id: f_00004-7-7 loss: 1.024537  [  256/  306]
train() client id: f_00004-7-8 loss: 1.090443  [  288/  306]
train() client id: f_00004-8-0 loss: 1.057388  [   32/  306]
train() client id: f_00004-8-1 loss: 1.098946  [   64/  306]
train() client id: f_00004-8-2 loss: 1.106163  [   96/  306]
train() client id: f_00004-8-3 loss: 1.112920  [  128/  306]
train() client id: f_00004-8-4 loss: 1.125911  [  160/  306]
train() client id: f_00004-8-5 loss: 1.086242  [  192/  306]
train() client id: f_00004-8-6 loss: 1.025425  [  224/  306]
train() client id: f_00004-8-7 loss: 1.033176  [  256/  306]
train() client id: f_00004-8-8 loss: 1.120889  [  288/  306]
train() client id: f_00004-9-0 loss: 1.058539  [   32/  306]
train() client id: f_00004-9-1 loss: 1.090587  [   64/  306]
train() client id: f_00004-9-2 loss: 1.024601  [   96/  306]
train() client id: f_00004-9-3 loss: 1.107366  [  128/  306]
train() client id: f_00004-9-4 loss: 1.116743  [  160/  306]
train() client id: f_00004-9-5 loss: 1.117343  [  192/  306]
train() client id: f_00004-9-6 loss: 1.095893  [  224/  306]
train() client id: f_00004-9-7 loss: 1.140079  [  256/  306]
train() client id: f_00004-9-8 loss: 1.073747  [  288/  306]
train() client id: f_00004-10-0 loss: 1.078964  [   32/  306]
train() client id: f_00004-10-1 loss: 1.190395  [   64/  306]
train() client id: f_00004-10-2 loss: 1.103131  [   96/  306]
train() client id: f_00004-10-3 loss: 1.045214  [  128/  306]
train() client id: f_00004-10-4 loss: 1.032522  [  160/  306]
train() client id: f_00004-10-5 loss: 1.139937  [  192/  306]
train() client id: f_00004-10-6 loss: 1.093868  [  224/  306]
train() client id: f_00004-10-7 loss: 1.056084  [  256/  306]
train() client id: f_00004-10-8 loss: 1.116844  [  288/  306]
train() client id: f_00004-11-0 loss: 1.148060  [   32/  306]
train() client id: f_00004-11-1 loss: 1.065674  [   64/  306]
train() client id: f_00004-11-2 loss: 1.142833  [   96/  306]
train() client id: f_00004-11-3 loss: 1.022431  [  128/  306]
train() client id: f_00004-11-4 loss: 1.123713  [  160/  306]
train() client id: f_00004-11-5 loss: 1.105037  [  192/  306]
train() client id: f_00004-11-6 loss: 1.101142  [  224/  306]
train() client id: f_00004-11-7 loss: 1.075496  [  256/  306]
train() client id: f_00004-11-8 loss: 1.043051  [  288/  306]
train() client id: f_00004-12-0 loss: 1.101385  [   32/  306]
train() client id: f_00004-12-1 loss: 1.147207  [   64/  306]
train() client id: f_00004-12-2 loss: 1.113512  [   96/  306]
train() client id: f_00004-12-3 loss: 1.126672  [  128/  306]
train() client id: f_00004-12-4 loss: 1.058102  [  160/  306]
train() client id: f_00004-12-5 loss: 1.133423  [  192/  306]
train() client id: f_00004-12-6 loss: 1.147960  [  224/  306]
train() client id: f_00004-12-7 loss: 0.997817  [  256/  306]
train() client id: f_00004-12-8 loss: 1.056863  [  288/  306]
train() client id: f_00005-0-0 loss: 1.124925  [   32/  146]
train() client id: f_00005-0-1 loss: 1.121013  [   64/  146]
train() client id: f_00005-0-2 loss: 1.110807  [   96/  146]
train() client id: f_00005-0-3 loss: 1.171852  [  128/  146]
train() client id: f_00005-1-0 loss: 1.132183  [   32/  146]
train() client id: f_00005-1-1 loss: 1.107545  [   64/  146]
train() client id: f_00005-1-2 loss: 1.066659  [   96/  146]
train() client id: f_00005-1-3 loss: 1.049294  [  128/  146]
train() client id: f_00005-2-0 loss: 1.065628  [   32/  146]
train() client id: f_00005-2-1 loss: 1.053024  [   64/  146]
train() client id: f_00005-2-2 loss: 1.024903  [   96/  146]
train() client id: f_00005-2-3 loss: 1.057972  [  128/  146]
train() client id: f_00005-3-0 loss: 1.042236  [   32/  146]
train() client id: f_00005-3-1 loss: 1.000429  [   64/  146]
train() client id: f_00005-3-2 loss: 0.999666  [   96/  146]
train() client id: f_00005-3-3 loss: 0.995968  [  128/  146]
train() client id: f_00005-4-0 loss: 1.034178  [   32/  146]
train() client id: f_00005-4-1 loss: 0.978907  [   64/  146]
train() client id: f_00005-4-2 loss: 0.995696  [   96/  146]
train() client id: f_00005-4-3 loss: 0.960485  [  128/  146]
train() client id: f_00005-5-0 loss: 0.965146  [   32/  146]
train() client id: f_00005-5-1 loss: 0.974229  [   64/  146]
train() client id: f_00005-5-2 loss: 0.996661  [   96/  146]
train() client id: f_00005-5-3 loss: 0.930957  [  128/  146]
train() client id: f_00005-6-0 loss: 0.970184  [   32/  146]
train() client id: f_00005-6-1 loss: 0.943605  [   64/  146]
train() client id: f_00005-6-2 loss: 0.953758  [   96/  146]
train() client id: f_00005-6-3 loss: 0.977464  [  128/  146]
train() client id: f_00005-7-0 loss: 0.917257  [   32/  146]
train() client id: f_00005-7-1 loss: 0.946554  [   64/  146]
train() client id: f_00005-7-2 loss: 0.975713  [   96/  146]
train() client id: f_00005-7-3 loss: 0.975270  [  128/  146]
train() client id: f_00005-8-0 loss: 0.937051  [   32/  146]
train() client id: f_00005-8-1 loss: 0.931621  [   64/  146]
train() client id: f_00005-8-2 loss: 0.960980  [   96/  146]
train() client id: f_00005-8-3 loss: 0.974429  [  128/  146]
train() client id: f_00005-9-0 loss: 0.956839  [   32/  146]
train() client id: f_00005-9-1 loss: 0.962068  [   64/  146]
train() client id: f_00005-9-2 loss: 0.831908  [   96/  146]
train() client id: f_00005-9-3 loss: 0.955750  [  128/  146]
train() client id: f_00005-10-0 loss: 0.965858  [   32/  146]
train() client id: f_00005-10-1 loss: 0.988458  [   64/  146]
train() client id: f_00005-10-2 loss: 0.855455  [   96/  146]
train() client id: f_00005-10-3 loss: 0.983751  [  128/  146]
train() client id: f_00005-11-0 loss: 1.016780  [   32/  146]
train() client id: f_00005-11-1 loss: 0.924765  [   64/  146]
train() client id: f_00005-11-2 loss: 0.952212  [   96/  146]
train() client id: f_00005-11-3 loss: 0.870407  [  128/  146]
train() client id: f_00005-12-0 loss: 0.875253  [   32/  146]
train() client id: f_00005-12-1 loss: 0.924615  [   64/  146]
train() client id: f_00005-12-2 loss: 0.977074  [   96/  146]
train() client id: f_00005-12-3 loss: 0.982354  [  128/  146]
train() client id: f_00006-0-0 loss: 1.085808  [   32/   54]
train() client id: f_00006-1-0 loss: 1.096770  [   32/   54]
train() client id: f_00006-2-0 loss: 1.126392  [   32/   54]
train() client id: f_00006-3-0 loss: 1.139479  [   32/   54]
train() client id: f_00006-4-0 loss: 1.116554  [   32/   54]
train() client id: f_00006-5-0 loss: 1.100047  [   32/   54]
train() client id: f_00006-6-0 loss: 1.116462  [   32/   54]
train() client id: f_00006-7-0 loss: 1.115620  [   32/   54]
train() client id: f_00006-8-0 loss: 1.117023  [   32/   54]
train() client id: f_00006-9-0 loss: 1.148704  [   32/   54]
train() client id: f_00006-10-0 loss: 1.140576  [   32/   54]
train() client id: f_00006-11-0 loss: 1.126662  [   32/   54]
train() client id: f_00006-12-0 loss: 1.116948  [   32/   54]
train() client id: f_00007-0-0 loss: 1.172578  [   32/  179]
train() client id: f_00007-0-1 loss: 1.176217  [   64/  179]
train() client id: f_00007-0-2 loss: 1.261609  [   96/  179]
train() client id: f_00007-0-3 loss: 1.150068  [  128/  179]
train() client id: f_00007-0-4 loss: 1.190778  [  160/  179]
train() client id: f_00007-1-0 loss: 1.138089  [   32/  179]
train() client id: f_00007-1-1 loss: 1.106580  [   64/  179]
train() client id: f_00007-1-2 loss: 1.057815  [   96/  179]
train() client id: f_00007-1-3 loss: 1.083254  [  128/  179]
train() client id: f_00007-1-4 loss: 1.026345  [  160/  179]
train() client id: f_00007-2-0 loss: 1.048384  [   32/  179]
train() client id: f_00007-2-1 loss: 1.051688  [   64/  179]
train() client id: f_00007-2-2 loss: 0.974716  [   96/  179]
train() client id: f_00007-2-3 loss: 0.992259  [  128/  179]
train() client id: f_00007-2-4 loss: 1.000304  [  160/  179]
train() client id: f_00007-3-0 loss: 0.964358  [   32/  179]
train() client id: f_00007-3-1 loss: 0.993628  [   64/  179]
train() client id: f_00007-3-2 loss: 0.973432  [   96/  179]
train() client id: f_00007-3-3 loss: 0.955121  [  128/  179]
train() client id: f_00007-3-4 loss: 0.884290  [  160/  179]
train() client id: f_00007-4-0 loss: 0.912661  [   32/  179]
train() client id: f_00007-4-1 loss: 0.908654  [   64/  179]
train() client id: f_00007-4-2 loss: 0.889230  [   96/  179]
train() client id: f_00007-4-3 loss: 0.923231  [  128/  179]
train() client id: f_00007-4-4 loss: 0.870842  [  160/  179]
train() client id: f_00007-5-0 loss: 0.889668  [   32/  179]
train() client id: f_00007-5-1 loss: 0.876685  [   64/  179]
train() client id: f_00007-5-2 loss: 0.860532  [   96/  179]
train() client id: f_00007-5-3 loss: 0.846201  [  128/  179]
train() client id: f_00007-5-4 loss: 0.878794  [  160/  179]
train() client id: f_00007-6-0 loss: 0.826932  [   32/  179]
train() client id: f_00007-6-1 loss: 0.827299  [   64/  179]
train() client id: f_00007-6-2 loss: 0.815890  [   96/  179]
train() client id: f_00007-6-3 loss: 0.845827  [  128/  179]
train() client id: f_00007-6-4 loss: 0.873362  [  160/  179]
train() client id: f_00007-7-0 loss: 0.842133  [   32/  179]
train() client id: f_00007-7-1 loss: 0.793928  [   64/  179]
train() client id: f_00007-7-2 loss: 0.796957  [   96/  179]
train() client id: f_00007-7-3 loss: 0.834394  [  128/  179]
train() client id: f_00007-7-4 loss: 0.861639  [  160/  179]
train() client id: f_00007-8-0 loss: 0.772885  [   32/  179]
train() client id: f_00007-8-1 loss: 0.873999  [   64/  179]
train() client id: f_00007-8-2 loss: 0.798636  [   96/  179]
train() client id: f_00007-8-3 loss: 0.758198  [  128/  179]
train() client id: f_00007-8-4 loss: 0.865133  [  160/  179]
train() client id: f_00007-9-0 loss: 0.850065  [   32/  179]
train() client id: f_00007-9-1 loss: 0.862864  [   64/  179]
train() client id: f_00007-9-2 loss: 0.720964  [   96/  179]
train() client id: f_00007-9-3 loss: 0.727413  [  128/  179]
train() client id: f_00007-9-4 loss: 0.832565  [  160/  179]
train() client id: f_00007-10-0 loss: 0.799907  [   32/  179]
train() client id: f_00007-10-1 loss: 0.718717  [   64/  179]
train() client id: f_00007-10-2 loss: 0.743856  [   96/  179]
train() client id: f_00007-10-3 loss: 0.797649  [  128/  179]
train() client id: f_00007-10-4 loss: 0.854857  [  160/  179]
train() client id: f_00007-11-0 loss: 0.764050  [   32/  179]
train() client id: f_00007-11-1 loss: 0.751344  [   64/  179]
train() client id: f_00007-11-2 loss: 0.804914  [   96/  179]
train() client id: f_00007-11-3 loss: 0.856516  [  128/  179]
train() client id: f_00007-11-4 loss: 0.804184  [  160/  179]
train() client id: f_00007-12-0 loss: 0.865195  [   32/  179]
train() client id: f_00007-12-1 loss: 0.775062  [   64/  179]
train() client id: f_00007-12-2 loss: 0.730700  [   96/  179]
train() client id: f_00007-12-3 loss: 0.796507  [  128/  179]
train() client id: f_00007-12-4 loss: 0.777910  [  160/  179]
train() client id: f_00008-0-0 loss: 0.996732  [   32/  130]
train() client id: f_00008-0-1 loss: 0.987466  [   64/  130]
train() client id: f_00008-0-2 loss: 1.043164  [   96/  130]
train() client id: f_00008-0-3 loss: 0.995679  [  128/  130]
train() client id: f_00008-1-0 loss: 0.969716  [   32/  130]
train() client id: f_00008-1-1 loss: 1.033727  [   64/  130]
train() client id: f_00008-1-2 loss: 0.985419  [   96/  130]
train() client id: f_00008-1-3 loss: 1.011134  [  128/  130]
train() client id: f_00008-2-0 loss: 0.994758  [   32/  130]
train() client id: f_00008-2-1 loss: 0.954244  [   64/  130]
train() client id: f_00008-2-2 loss: 1.113786  [   96/  130]
train() client id: f_00008-2-3 loss: 0.890388  [  128/  130]
train() client id: f_00008-3-0 loss: 0.905828  [   32/  130]
train() client id: f_00008-3-1 loss: 1.053391  [   64/  130]
train() client id: f_00008-3-2 loss: 0.988171  [   96/  130]
train() client id: f_00008-3-3 loss: 0.969776  [  128/  130]
train() client id: f_00008-4-0 loss: 0.939451  [   32/  130]
train() client id: f_00008-4-1 loss: 0.961389  [   64/  130]
train() client id: f_00008-4-2 loss: 1.007058  [   96/  130]
train() client id: f_00008-4-3 loss: 0.980772  [  128/  130]
train() client id: f_00008-5-0 loss: 1.020063  [   32/  130]
train() client id: f_00008-5-1 loss: 0.971910  [   64/  130]
train() client id: f_00008-5-2 loss: 0.962069  [   96/  130]
train() client id: f_00008-5-3 loss: 0.931897  [  128/  130]
train() client id: f_00008-6-0 loss: 0.968039  [   32/  130]
train() client id: f_00008-6-1 loss: 0.920702  [   64/  130]
train() client id: f_00008-6-2 loss: 1.018709  [   96/  130]
train() client id: f_00008-6-3 loss: 0.945308  [  128/  130]
train() client id: f_00008-7-0 loss: 1.040258  [   32/  130]
train() client id: f_00008-7-1 loss: 0.836642  [   64/  130]
train() client id: f_00008-7-2 loss: 1.086902  [   96/  130]
train() client id: f_00008-7-3 loss: 0.851671  [  128/  130]
train() client id: f_00008-8-0 loss: 0.859913  [   32/  130]
train() client id: f_00008-8-1 loss: 1.053542  [   64/  130]
train() client id: f_00008-8-2 loss: 0.906039  [   96/  130]
train() client id: f_00008-8-3 loss: 1.030968  [  128/  130]
train() client id: f_00008-9-0 loss: 1.102157  [   32/  130]
train() client id: f_00008-9-1 loss: 0.901157  [   64/  130]
train() client id: f_00008-9-2 loss: 0.930008  [   96/  130]
train() client id: f_00008-9-3 loss: 0.891648  [  128/  130]
train() client id: f_00008-10-0 loss: 1.095095  [   32/  130]
train() client id: f_00008-10-1 loss: 0.957879  [   64/  130]
train() client id: f_00008-10-2 loss: 0.903090  [   96/  130]
train() client id: f_00008-10-3 loss: 0.910352  [  128/  130]
train() client id: f_00008-11-0 loss: 0.906828  [   32/  130]
train() client id: f_00008-11-1 loss: 0.952108  [   64/  130]
train() client id: f_00008-11-2 loss: 0.993122  [   96/  130]
train() client id: f_00008-11-3 loss: 1.014945  [  128/  130]
train() client id: f_00008-12-0 loss: 0.928135  [   32/  130]
train() client id: f_00008-12-1 loss: 0.953595  [   64/  130]
train() client id: f_00008-12-2 loss: 1.059644  [   96/  130]
train() client id: f_00008-12-3 loss: 0.927936  [  128/  130]
train() client id: f_00009-0-0 loss: 1.218018  [   32/  118]
train() client id: f_00009-0-1 loss: 1.193038  [   64/  118]
train() client id: f_00009-0-2 loss: 1.182360  [   96/  118]
train() client id: f_00009-1-0 loss: 1.137083  [   32/  118]
train() client id: f_00009-1-1 loss: 1.189716  [   64/  118]
train() client id: f_00009-1-2 loss: 1.093851  [   96/  118]
train() client id: f_00009-2-0 loss: 1.147873  [   32/  118]
train() client id: f_00009-2-1 loss: 1.113083  [   64/  118]
train() client id: f_00009-2-2 loss: 1.057624  [   96/  118]
train() client id: f_00009-3-0 loss: 1.083083  [   32/  118]
train() client id: f_00009-3-1 loss: 1.102556  [   64/  118]
train() client id: f_00009-3-2 loss: 1.038289  [   96/  118]
train() client id: f_00009-4-0 loss: 1.066041  [   32/  118]
train() client id: f_00009-4-1 loss: 1.029141  [   64/  118]
train() client id: f_00009-4-2 loss: 1.049902  [   96/  118]
train() client id: f_00009-5-0 loss: 1.036531  [   32/  118]
train() client id: f_00009-5-1 loss: 1.004676  [   64/  118]
train() client id: f_00009-5-2 loss: 1.023819  [   96/  118]
train() client id: f_00009-6-0 loss: 1.047378  [   32/  118]
train() client id: f_00009-6-1 loss: 0.937176  [   64/  118]
train() client id: f_00009-6-2 loss: 0.976487  [   96/  118]
train() client id: f_00009-7-0 loss: 0.998179  [   32/  118]
train() client id: f_00009-7-1 loss: 0.968134  [   64/  118]
train() client id: f_00009-7-2 loss: 1.009528  [   96/  118]
train() client id: f_00009-8-0 loss: 0.967587  [   32/  118]
train() client id: f_00009-8-1 loss: 0.987435  [   64/  118]
train() client id: f_00009-8-2 loss: 0.965877  [   96/  118]
train() client id: f_00009-9-0 loss: 0.952969  [   32/  118]
train() client id: f_00009-9-1 loss: 0.957659  [   64/  118]
train() client id: f_00009-9-2 loss: 0.938393  [   96/  118]
train() client id: f_00009-10-0 loss: 0.945298  [   32/  118]
train() client id: f_00009-10-1 loss: 0.963954  [   64/  118]
train() client id: f_00009-10-2 loss: 0.876449  [   96/  118]
train() client id: f_00009-11-0 loss: 0.914607  [   32/  118]
train() client id: f_00009-11-1 loss: 0.935375  [   64/  118]
train() client id: f_00009-11-2 loss: 0.892769  [   96/  118]
train() client id: f_00009-12-0 loss: 0.995563  [   32/  118]
train() client id: f_00009-12-1 loss: 0.861042  [   64/  118]
train() client id: f_00009-12-2 loss: 0.923706  [   96/  118]
At round 2 accuracy: 0.5092838196286472
At round 2 training accuracy: 0.48088531187122735
At round 2 training loss: 1.020129870329691
update_location
xs = [ -3.9056584   -5.79968212  30.00902392  18.81129433 -24.02070377
 -11.04359014   2.55680806  -6.32485185  14.66397685  -7.06087855]
ys = [ 22.5879595   15.55583871   1.32061395   7.54482414   9.35018685
 -17.18584926   2.37501568  -4.17765202  17.56900603   4.00148178]
dists_uav = [102.59371366 101.36873498 104.41401026 102.0332748  103.26867968
 102.0652453  100.06087131 100.28687116 102.58509731 100.32879877]
dists_bs = [229.13468488 232.48911402 268.68797708 256.13288296 224.13078434
 252.62047619 247.64047589 246.08115854 246.48923299 239.67486674]
uav_gains = [9.37984848e-11 9.66580426e-11 8.97635667e-11 9.50918527e-11
 9.22732701e-11 9.50174026e-11 9.98476140e-11 9.92860273e-11
 9.38181825e-11 9.91823280e-11]
bs_gains = [2.72295051e-11 2.61436814e-11 1.74340799e-11 1.99338479e-11
 2.89660886e-11 2.07196372e-11 2.19075354e-11 2.22984497e-11
 2.21952385e-11 2.40077301e-11]
Round 3
-------------------------------
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.59821531 22.14976829 10.43417884  3.73162013 25.53613015 12.31863725
  4.64005817 14.98083601 10.99100861  9.99386679]
obj_prev = 125.37431955255585
eta_min = 1.4920442712219943e-09	eta_max = 0.9182779578592448
af = 26.51569137072327	bf = 2.011304995713791	zeta = 29.1672605077956	eta = 0.9090909090909091
af = 26.51569137072327	bf = 2.011304995713791	zeta = 49.923723386827525	eta = 0.531124074325744
af = 26.51569137072327	bf = 2.011304995713791	zeta = 40.07321095605609	eta = 0.6616812263883753
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.31157890911957	eta = 0.692106462999664
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.22584319738169	eta = 0.6936587699009733
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.22562613755516	eta = 0.6936627087626083
eta = 0.6936627087626083
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [0.03018242 0.0634789  0.02970333 0.01030035 0.07330016 0.03497328
 0.01293532 0.0428782  0.03114059 0.02826607]
ene_total = [3.24068386 6.36286468 3.20278272 1.47684991 7.21615316 3.87841894
 1.70546401 4.36028936 3.52564504 3.25647446]
ti_comp = [0.27298014 0.25372925 0.27246494 0.27313894 0.25568174 0.24894535
 0.27369854 0.27363436 0.25041517 0.25203535]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.30610989e-05 2.48328765e-04 2.20634935e-05 9.15521600e-07
 3.76526319e-04 4.31400861e-05 1.80578817e-06 6.58033963e-05
 3.00981129e-05 2.22204868e-05]
ene_total = [0.56723758 0.75785384 0.57171611 0.56386684 0.75191015 0.7820715
 0.55898529 0.56522719 0.76788634 0.75282607]
optimize_network_iter = 0 obj = 6.639580919785389
eta = 0.6936627087626083
freqs = [5.52831753e+07 1.25091797e+08 5.45085400e+07 1.88555117e+07
 1.43342577e+08 7.02428831e+07 2.36306014e+07 7.83494392e+07
 6.21779230e+07 5.60756135e+07]
eta_min = 0.6679443667432773	eta_max = 0.6936627087626027
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 0.06810536589768883	eta = 0.9090909090909091
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 22.189266299545174	eta = 0.002790266616394072
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.3821060289713065	eta = 0.025991273371083312
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.2969923954010767	eta = 0.026954363942109684
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.2969507848071644	eta = 0.02695485223602507
eta = 0.02695485223602507
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.27830406e-04 2.45334550e-03 2.17974637e-04 9.04482732e-06
 3.71986367e-03 4.26199261e-04 1.78401495e-05 6.50099743e-04
 2.97352060e-04 2.19525641e-04]
ene_total = [0.18429415 0.3001259  0.18545689 0.17774529 0.33002202 0.25690445
 0.17642907 0.19425277 0.24920683 0.24251343]
ti_comp = [0.30125093 0.28200003 0.30073572 0.30140973 0.28395252 0.27721613
 0.30196932 0.30190514 0.27868595 0.28030614]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.54305861e-05 2.69985736e-04 2.43218458e-05 1.00970000e-06
 4.09991014e-04 4.67221780e-05 1.99231109e-06 7.25971523e-05
 3.26363375e-05 2.41258334e-05]
ene_total = [0.52349772 0.70092769 0.52762029 0.52020198 0.69640997 0.72179151
 0.51570607 0.52200485 0.70861965 0.69467407]
optimize_network_iter = 1 obj = 6.13145379564688
eta = 0.6679443667432773
freqs = [5.52695580e+07 1.24176825e+08 5.44854450e+07 1.88518958e+07
 1.42403130e+08 6.95950244e+07 2.36306014e+07 7.83477189e+07
 6.16413456e+07 5.56279733e+07]
eta_min = 0.6679443667432772	eta_max = 0.6679443667432751
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 0.06726955400122223	eta = 0.9090909090909091
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 22.188469685723824	eta = 0.002756122475650405
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.378298126804526	eta = 0.025713403762074868
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.294165378733591	eta = 0.026656378205336263
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.2941251141107752	eta = 0.02665684605646063
eta = 0.02665684605646063
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.28418763e-04 2.42502502e-03 2.18460004e-04 9.06917459e-06
 3.68255925e-03 4.19660878e-04 1.78950352e-05 6.52071157e-04
 2.93141173e-04 2.16699410e-04]
ene_total = [0.18425414 0.2992439  0.18541365 0.17769155 0.32888024 0.25664338
 0.17637659 0.19424829 0.24901305 0.24236033]
ti_comp = [0.30125093 0.28200003 0.30073572 0.30140973 0.28395252 0.27721613
 0.30196932 0.30190514 0.27868595 0.28030614]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.54305861e-05 2.69985736e-04 2.43218458e-05 1.00970000e-06
 4.09991014e-04 4.67221780e-05 1.99231109e-06 7.25971523e-05
 3.26363375e-05 2.41258334e-05]
ene_total = [0.52349772 0.70092769 0.52762029 0.52020198 0.69640997 0.72179151
 0.51570607 0.52200485 0.70861965 0.69467407]
optimize_network_iter = 2 obj = 6.1314537956468795
eta = 0.6679443667432772
freqs = [5.52695580e+07 1.24176825e+08 5.44854450e+07 1.88518958e+07
 1.42403130e+08 6.95950244e+07 2.36306014e+07 7.83477189e+07
 6.16413456e+07 5.56279733e+07]
Done!
At round 3 eta: 0.6679443667432772
At round 3 local rounds: 13.214290123550223
At round 3 global rounds: 81.77836059884595
At round 3 a_n: 26.812419468377104
gradient difference: 0.4552827775478363
train() client id: f_00000-0-0 loss: 1.214104  [   32/  126]
train() client id: f_00000-0-1 loss: 1.196520  [   64/  126]
train() client id: f_00000-0-2 loss: 1.237561  [   96/  126]
train() client id: f_00000-1-0 loss: 1.140270  [   32/  126]
train() client id: f_00000-1-1 loss: 1.113614  [   64/  126]
train() client id: f_00000-1-2 loss: 1.075514  [   96/  126]
train() client id: f_00000-2-0 loss: 1.067740  [   32/  126]
train() client id: f_00000-2-1 loss: 1.031825  [   64/  126]
train() client id: f_00000-2-2 loss: 1.033834  [   96/  126]
train() client id: f_00000-3-0 loss: 1.009581  [   32/  126]
train() client id: f_00000-3-1 loss: 0.960346  [   64/  126]
train() client id: f_00000-3-2 loss: 0.945780  [   96/  126]
train() client id: f_00000-4-0 loss: 0.941820  [   32/  126]
train() client id: f_00000-4-1 loss: 0.909670  [   64/  126]
train() client id: f_00000-4-2 loss: 0.848509  [   96/  126]
train() client id: f_00000-5-0 loss: 0.868140  [   32/  126]
train() client id: f_00000-5-1 loss: 0.890069  [   64/  126]
train() client id: f_00000-5-2 loss: 0.739745  [   96/  126]
train() client id: f_00000-6-0 loss: 0.840665  [   32/  126]
train() client id: f_00000-6-1 loss: 0.778636  [   64/  126]
train() client id: f_00000-6-2 loss: 0.763492  [   96/  126]
train() client id: f_00000-7-0 loss: 0.804909  [   32/  126]
train() client id: f_00000-7-1 loss: 0.830234  [   64/  126]
train() client id: f_00000-7-2 loss: 0.717963  [   96/  126]
train() client id: f_00000-8-0 loss: 0.768290  [   32/  126]
train() client id: f_00000-8-1 loss: 0.726783  [   64/  126]
train() client id: f_00000-8-2 loss: 0.727290  [   96/  126]
train() client id: f_00000-9-0 loss: 0.694675  [   32/  126]
train() client id: f_00000-9-1 loss: 0.797062  [   64/  126]
train() client id: f_00000-9-2 loss: 0.673542  [   96/  126]
train() client id: f_00000-10-0 loss: 0.780975  [   32/  126]
train() client id: f_00000-10-1 loss: 0.582646  [   64/  126]
train() client id: f_00000-10-2 loss: 0.677442  [   96/  126]
train() client id: f_00000-11-0 loss: 0.701889  [   32/  126]
train() client id: f_00000-11-1 loss: 0.730276  [   64/  126]
train() client id: f_00000-11-2 loss: 0.645494  [   96/  126]
train() client id: f_00000-12-0 loss: 0.666595  [   32/  126]
train() client id: f_00000-12-1 loss: 0.751167  [   64/  126]
train() client id: f_00000-12-2 loss: 0.665432  [   96/  126]
train() client id: f_00001-0-0 loss: 0.754118  [   32/  265]
train() client id: f_00001-0-1 loss: 0.736395  [   64/  265]
train() client id: f_00001-0-2 loss: 0.714686  [   96/  265]
train() client id: f_00001-0-3 loss: 0.754907  [  128/  265]
train() client id: f_00001-0-4 loss: 0.680911  [  160/  265]
train() client id: f_00001-0-5 loss: 0.681869  [  192/  265]
train() client id: f_00001-0-6 loss: 0.689523  [  224/  265]
train() client id: f_00001-0-7 loss: 0.697684  [  256/  265]
train() client id: f_00001-1-0 loss: 0.711444  [   32/  265]
train() client id: f_00001-1-1 loss: 0.668308  [   64/  265]
train() client id: f_00001-1-2 loss: 0.668769  [   96/  265]
train() client id: f_00001-1-3 loss: 0.728589  [  128/  265]
train() client id: f_00001-1-4 loss: 0.690945  [  160/  265]
train() client id: f_00001-1-5 loss: 0.645024  [  192/  265]
train() client id: f_00001-1-6 loss: 0.634538  [  224/  265]
train() client id: f_00001-1-7 loss: 0.638726  [  256/  265]
train() client id: f_00001-2-0 loss: 0.640327  [   32/  265]
train() client id: f_00001-2-1 loss: 0.661520  [   64/  265]
train() client id: f_00001-2-2 loss: 0.644729  [   96/  265]
train() client id: f_00001-2-3 loss: 0.697802  [  128/  265]
train() client id: f_00001-2-4 loss: 0.609967  [  160/  265]
train() client id: f_00001-2-5 loss: 0.622113  [  192/  265]
train() client id: f_00001-2-6 loss: 0.647503  [  224/  265]
train() client id: f_00001-2-7 loss: 0.648744  [  256/  265]
train() client id: f_00001-3-0 loss: 0.595121  [   32/  265]
train() client id: f_00001-3-1 loss: 0.698302  [   64/  265]
train() client id: f_00001-3-2 loss: 0.710147  [   96/  265]
train() client id: f_00001-3-3 loss: 0.602545  [  128/  265]
train() client id: f_00001-3-4 loss: 0.565777  [  160/  265]
train() client id: f_00001-3-5 loss: 0.661609  [  192/  265]
train() client id: f_00001-3-6 loss: 0.579164  [  224/  265]
train() client id: f_00001-3-7 loss: 0.594849  [  256/  265]
train() client id: f_00001-4-0 loss: 0.586065  [   32/  265]
train() client id: f_00001-4-1 loss: 0.639207  [   64/  265]
train() client id: f_00001-4-2 loss: 0.629497  [   96/  265]
train() client id: f_00001-4-3 loss: 0.648859  [  128/  265]
train() client id: f_00001-4-4 loss: 0.655357  [  160/  265]
train() client id: f_00001-4-5 loss: 0.557871  [  192/  265]
train() client id: f_00001-4-6 loss: 0.577357  [  224/  265]
train() client id: f_00001-4-7 loss: 0.569011  [  256/  265]
train() client id: f_00001-5-0 loss: 0.653296  [   32/  265]
train() client id: f_00001-5-1 loss: 0.579067  [   64/  265]
train() client id: f_00001-5-2 loss: 0.581352  [   96/  265]
train() client id: f_00001-5-3 loss: 0.587705  [  128/  265]
train() client id: f_00001-5-4 loss: 0.560804  [  160/  265]
train() client id: f_00001-5-5 loss: 0.568591  [  192/  265]
train() client id: f_00001-5-6 loss: 0.615499  [  224/  265]
train() client id: f_00001-5-7 loss: 0.672133  [  256/  265]
train() client id: f_00001-6-0 loss: 0.673627  [   32/  265]
train() client id: f_00001-6-1 loss: 0.613330  [   64/  265]
train() client id: f_00001-6-2 loss: 0.580707  [   96/  265]
train() client id: f_00001-6-3 loss: 0.618304  [  128/  265]
train() client id: f_00001-6-4 loss: 0.620582  [  160/  265]
train() client id: f_00001-6-5 loss: 0.546430  [  192/  265]
train() client id: f_00001-6-6 loss: 0.566117  [  224/  265]
train() client id: f_00001-6-7 loss: 0.546189  [  256/  265]
train() client id: f_00001-7-0 loss: 0.630007  [   32/  265]
train() client id: f_00001-7-1 loss: 0.509237  [   64/  265]
train() client id: f_00001-7-2 loss: 0.669466  [   96/  265]
train() client id: f_00001-7-3 loss: 0.638552  [  128/  265]
train() client id: f_00001-7-4 loss: 0.638949  [  160/  265]
train() client id: f_00001-7-5 loss: 0.521739  [  192/  265]
train() client id: f_00001-7-6 loss: 0.570335  [  224/  265]
train() client id: f_00001-7-7 loss: 0.546275  [  256/  265]
train() client id: f_00001-8-0 loss: 0.636011  [   32/  265]
train() client id: f_00001-8-1 loss: 0.522943  [   64/  265]
train() client id: f_00001-8-2 loss: 0.552101  [   96/  265]
train() client id: f_00001-8-3 loss: 0.540798  [  128/  265]
train() client id: f_00001-8-4 loss: 0.550868  [  160/  265]
train() client id: f_00001-8-5 loss: 0.588386  [  192/  265]
train() client id: f_00001-8-6 loss: 0.535011  [  224/  265]
train() client id: f_00001-8-7 loss: 0.732829  [  256/  265]
train() client id: f_00001-9-0 loss: 0.587108  [   32/  265]
train() client id: f_00001-9-1 loss: 0.575483  [   64/  265]
train() client id: f_00001-9-2 loss: 0.713176  [   96/  265]
train() client id: f_00001-9-3 loss: 0.500653  [  128/  265]
train() client id: f_00001-9-4 loss: 0.585885  [  160/  265]
train() client id: f_00001-9-5 loss: 0.528360  [  192/  265]
train() client id: f_00001-9-6 loss: 0.524572  [  224/  265]
train() client id: f_00001-9-7 loss: 0.657989  [  256/  265]
train() client id: f_00001-10-0 loss: 0.585321  [   32/  265]
train() client id: f_00001-10-1 loss: 0.553564  [   64/  265]
train() client id: f_00001-10-2 loss: 0.596043  [   96/  265]
train() client id: f_00001-10-3 loss: 0.595562  [  128/  265]
train() client id: f_00001-10-4 loss: 0.537850  [  160/  265]
train() client id: f_00001-10-5 loss: 0.599618  [  192/  265]
train() client id: f_00001-10-6 loss: 0.526927  [  224/  265]
train() client id: f_00001-10-7 loss: 0.674786  [  256/  265]
train() client id: f_00001-11-0 loss: 0.536278  [   32/  265]
train() client id: f_00001-11-1 loss: 0.664132  [   64/  265]
train() client id: f_00001-11-2 loss: 0.626994  [   96/  265]
train() client id: f_00001-11-3 loss: 0.476972  [  128/  265]
train() client id: f_00001-11-4 loss: 0.649086  [  160/  265]
train() client id: f_00001-11-5 loss: 0.559832  [  192/  265]
train() client id: f_00001-11-6 loss: 0.624466  [  224/  265]
train() client id: f_00001-11-7 loss: 0.512808  [  256/  265]
train() client id: f_00001-12-0 loss: 0.565701  [   32/  265]
train() client id: f_00001-12-1 loss: 0.674623  [   64/  265]
train() client id: f_00001-12-2 loss: 0.615565  [   96/  265]
train() client id: f_00001-12-3 loss: 0.510420  [  128/  265]
train() client id: f_00001-12-4 loss: 0.661438  [  160/  265]
train() client id: f_00001-12-5 loss: 0.629930  [  192/  265]
train() client id: f_00001-12-6 loss: 0.491901  [  224/  265]
train() client id: f_00001-12-7 loss: 0.510857  [  256/  265]
train() client id: f_00002-0-0 loss: 1.200248  [   32/  124]
train() client id: f_00002-0-1 loss: 1.169631  [   64/  124]
train() client id: f_00002-0-2 loss: 1.155189  [   96/  124]
train() client id: f_00002-1-0 loss: 1.165268  [   32/  124]
train() client id: f_00002-1-1 loss: 1.152554  [   64/  124]
train() client id: f_00002-1-2 loss: 1.120008  [   96/  124]
train() client id: f_00002-2-0 loss: 1.108773  [   32/  124]
train() client id: f_00002-2-1 loss: 1.100103  [   64/  124]
train() client id: f_00002-2-2 loss: 1.125841  [   96/  124]
train() client id: f_00002-3-0 loss: 1.107870  [   32/  124]
train() client id: f_00002-3-1 loss: 1.063939  [   64/  124]
train() client id: f_00002-3-2 loss: 1.084705  [   96/  124]
train() client id: f_00002-4-0 loss: 1.055858  [   32/  124]
train() client id: f_00002-4-1 loss: 1.042161  [   64/  124]
train() client id: f_00002-4-2 loss: 1.068426  [   96/  124]
train() client id: f_00002-5-0 loss: 1.026601  [   32/  124]
train() client id: f_00002-5-1 loss: 1.003966  [   64/  124]
train() client id: f_00002-5-2 loss: 1.074295  [   96/  124]
train() client id: f_00002-6-0 loss: 1.012897  [   32/  124]
train() client id: f_00002-6-1 loss: 0.998214  [   64/  124]
train() client id: f_00002-6-2 loss: 1.066174  [   96/  124]
train() client id: f_00002-7-0 loss: 0.997050  [   32/  124]
train() client id: f_00002-7-1 loss: 0.989847  [   64/  124]
train() client id: f_00002-7-2 loss: 1.009496  [   96/  124]
train() client id: f_00002-8-0 loss: 1.006387  [   32/  124]
train() client id: f_00002-8-1 loss: 1.004263  [   64/  124]
train() client id: f_00002-8-2 loss: 1.021215  [   96/  124]
train() client id: f_00002-9-0 loss: 0.974762  [   32/  124]
train() client id: f_00002-9-1 loss: 1.023490  [   64/  124]
train() client id: f_00002-9-2 loss: 0.971277  [   96/  124]
train() client id: f_00002-10-0 loss: 1.053285  [   32/  124]
train() client id: f_00002-10-1 loss: 0.926827  [   64/  124]
train() client id: f_00002-10-2 loss: 0.885569  [   96/  124]
train() client id: f_00002-11-0 loss: 0.924170  [   32/  124]
train() client id: f_00002-11-1 loss: 0.923551  [   64/  124]
train() client id: f_00002-11-2 loss: 0.946257  [   96/  124]
train() client id: f_00002-12-0 loss: 1.017314  [   32/  124]
train() client id: f_00002-12-1 loss: 0.874935  [   64/  124]
train() client id: f_00002-12-2 loss: 0.941669  [   96/  124]
train() client id: f_00003-0-0 loss: 1.119399  [   32/   43]
train() client id: f_00003-1-0 loss: 1.119539  [   32/   43]
train() client id: f_00003-2-0 loss: 1.128087  [   32/   43]
train() client id: f_00003-3-0 loss: 1.135948  [   32/   43]
train() client id: f_00003-4-0 loss: 1.139725  [   32/   43]
train() client id: f_00003-5-0 loss: 1.130887  [   32/   43]
train() client id: f_00003-6-0 loss: 1.082080  [   32/   43]
train() client id: f_00003-7-0 loss: 1.133198  [   32/   43]
train() client id: f_00003-8-0 loss: 1.122078  [   32/   43]
train() client id: f_00003-9-0 loss: 1.111100  [   32/   43]
train() client id: f_00003-10-0 loss: 1.088360  [   32/   43]
train() client id: f_00003-11-0 loss: 1.122568  [   32/   43]
train() client id: f_00003-12-0 loss: 1.117475  [   32/   43]
train() client id: f_00004-0-0 loss: 1.038625  [   32/  306]
train() client id: f_00004-0-1 loss: 1.029742  [   64/  306]
train() client id: f_00004-0-2 loss: 1.089472  [   96/  306]
train() client id: f_00004-0-3 loss: 1.034898  [  128/  306]
train() client id: f_00004-0-4 loss: 1.076143  [  160/  306]
train() client id: f_00004-0-5 loss: 1.049638  [  192/  306]
train() client id: f_00004-0-6 loss: 1.063775  [  224/  306]
train() client id: f_00004-0-7 loss: 1.048761  [  256/  306]
train() client id: f_00004-0-8 loss: 1.072018  [  288/  306]
train() client id: f_00004-1-0 loss: 1.037834  [   32/  306]
train() client id: f_00004-1-1 loss: 1.030486  [   64/  306]
train() client id: f_00004-1-2 loss: 1.009179  [   96/  306]
train() client id: f_00004-1-3 loss: 1.061525  [  128/  306]
train() client id: f_00004-1-4 loss: 1.058695  [  160/  306]
train() client id: f_00004-1-5 loss: 1.069705  [  192/  306]
train() client id: f_00004-1-6 loss: 1.023366  [  224/  306]
train() client id: f_00004-1-7 loss: 1.028005  [  256/  306]
train() client id: f_00004-1-8 loss: 1.051896  [  288/  306]
train() client id: f_00004-2-0 loss: 1.051918  [   32/  306]
train() client id: f_00004-2-1 loss: 0.965221  [   64/  306]
train() client id: f_00004-2-2 loss: 1.036681  [   96/  306]
train() client id: f_00004-2-3 loss: 1.020746  [  128/  306]
train() client id: f_00004-2-4 loss: 1.030757  [  160/  306]
train() client id: f_00004-2-5 loss: 1.018891  [  192/  306]
train() client id: f_00004-2-6 loss: 1.036991  [  224/  306]
train() client id: f_00004-2-7 loss: 1.063339  [  256/  306]
train() client id: f_00004-2-8 loss: 0.995376  [  288/  306]
train() client id: f_00004-3-0 loss: 0.979983  [   32/  306]
train() client id: f_00004-3-1 loss: 1.044352  [   64/  306]
train() client id: f_00004-3-2 loss: 0.987146  [   96/  306]
train() client id: f_00004-3-3 loss: 1.010925  [  128/  306]
train() client id: f_00004-3-4 loss: 1.027635  [  160/  306]
train() client id: f_00004-3-5 loss: 1.065869  [  192/  306]
train() client id: f_00004-3-6 loss: 0.993813  [  224/  306]
train() client id: f_00004-3-7 loss: 1.061131  [  256/  306]
train() client id: f_00004-3-8 loss: 1.051283  [  288/  306]
train() client id: f_00004-4-0 loss: 1.002320  [   32/  306]
train() client id: f_00004-4-1 loss: 1.038549  [   64/  306]
train() client id: f_00004-4-2 loss: 1.016817  [   96/  306]
train() client id: f_00004-4-3 loss: 1.077167  [  128/  306]
train() client id: f_00004-4-4 loss: 0.992817  [  160/  306]
train() client id: f_00004-4-5 loss: 0.953571  [  192/  306]
train() client id: f_00004-4-6 loss: 1.000676  [  224/  306]
train() client id: f_00004-4-7 loss: 0.996474  [  256/  306]
train() client id: f_00004-4-8 loss: 1.026127  [  288/  306]
train() client id: f_00004-5-0 loss: 0.989365  [   32/  306]
train() client id: f_00004-5-1 loss: 1.061261  [   64/  306]
train() client id: f_00004-5-2 loss: 0.973952  [   96/  306]
train() client id: f_00004-5-3 loss: 1.025144  [  128/  306]
train() client id: f_00004-5-4 loss: 1.009246  [  160/  306]
train() client id: f_00004-5-5 loss: 0.992833  [  192/  306]
train() client id: f_00004-5-6 loss: 1.056215  [  224/  306]
train() client id: f_00004-5-7 loss: 0.979654  [  256/  306]
train() client id: f_00004-5-8 loss: 0.974531  [  288/  306]
train() client id: f_00004-6-0 loss: 0.968414  [   32/  306]
train() client id: f_00004-6-1 loss: 0.998260  [   64/  306]
train() client id: f_00004-6-2 loss: 1.012170  [   96/  306]
train() client id: f_00004-6-3 loss: 1.057749  [  128/  306]
train() client id: f_00004-6-4 loss: 0.987150  [  160/  306]
train() client id: f_00004-6-5 loss: 0.953204  [  192/  306]
train() client id: f_00004-6-6 loss: 0.983475  [  224/  306]
train() client id: f_00004-6-7 loss: 1.004234  [  256/  306]
train() client id: f_00004-6-8 loss: 1.030443  [  288/  306]
train() client id: f_00004-7-0 loss: 0.944795  [   32/  306]
train() client id: f_00004-7-1 loss: 0.973789  [   64/  306]
train() client id: f_00004-7-2 loss: 1.044997  [   96/  306]
train() client id: f_00004-7-3 loss: 1.003370  [  128/  306]
train() client id: f_00004-7-4 loss: 1.082735  [  160/  306]
train() client id: f_00004-7-5 loss: 0.953326  [  192/  306]
train() client id: f_00004-7-6 loss: 0.963527  [  224/  306]
train() client id: f_00004-7-7 loss: 1.007161  [  256/  306]
train() client id: f_00004-7-8 loss: 0.993042  [  288/  306]
train() client id: f_00004-8-0 loss: 1.012886  [   32/  306]
train() client id: f_00004-8-1 loss: 1.044714  [   64/  306]
train() client id: f_00004-8-2 loss: 0.963433  [   96/  306]
train() client id: f_00004-8-3 loss: 0.988931  [  128/  306]
train() client id: f_00004-8-4 loss: 0.953425  [  160/  306]
train() client id: f_00004-8-5 loss: 1.043142  [  192/  306]
train() client id: f_00004-8-6 loss: 0.964584  [  224/  306]
train() client id: f_00004-8-7 loss: 0.926078  [  256/  306]
train() client id: f_00004-8-8 loss: 0.997553  [  288/  306]
train() client id: f_00004-9-0 loss: 0.954979  [   32/  306]
train() client id: f_00004-9-1 loss: 1.066365  [   64/  306]
train() client id: f_00004-9-2 loss: 1.011320  [   96/  306]
train() client id: f_00004-9-3 loss: 1.036750  [  128/  306]
train() client id: f_00004-9-4 loss: 0.966709  [  160/  306]
train() client id: f_00004-9-5 loss: 1.035531  [  192/  306]
train() client id: f_00004-9-6 loss: 0.916906  [  224/  306]
train() client id: f_00004-9-7 loss: 0.965422  [  256/  306]
train() client id: f_00004-9-8 loss: 0.998056  [  288/  306]
train() client id: f_00004-10-0 loss: 1.014615  [   32/  306]
train() client id: f_00004-10-1 loss: 0.963873  [   64/  306]
train() client id: f_00004-10-2 loss: 0.934401  [   96/  306]
train() client id: f_00004-10-3 loss: 0.960424  [  128/  306]
train() client id: f_00004-10-4 loss: 1.017037  [  160/  306]
train() client id: f_00004-10-5 loss: 1.001060  [  192/  306]
train() client id: f_00004-10-6 loss: 1.028430  [  224/  306]
train() client id: f_00004-10-7 loss: 1.061495  [  256/  306]
train() client id: f_00004-10-8 loss: 0.975279  [  288/  306]
train() client id: f_00004-11-0 loss: 0.970289  [   32/  306]
train() client id: f_00004-11-1 loss: 0.912571  [   64/  306]
train() client id: f_00004-11-2 loss: 0.969217  [   96/  306]
train() client id: f_00004-11-3 loss: 1.009433  [  128/  306]
train() client id: f_00004-11-4 loss: 1.029728  [  160/  306]
train() client id: f_00004-11-5 loss: 1.004408  [  192/  306]
train() client id: f_00004-11-6 loss: 0.951469  [  224/  306]
train() client id: f_00004-11-7 loss: 0.964825  [  256/  306]
train() client id: f_00004-11-8 loss: 0.998690  [  288/  306]
train() client id: f_00004-12-0 loss: 1.026248  [   32/  306]
train() client id: f_00004-12-1 loss: 1.017014  [   64/  306]
train() client id: f_00004-12-2 loss: 1.048726  [   96/  306]
train() client id: f_00004-12-3 loss: 1.009390  [  128/  306]
train() client id: f_00004-12-4 loss: 0.875915  [  160/  306]
train() client id: f_00004-12-5 loss: 1.021897  [  192/  306]
train() client id: f_00004-12-6 loss: 1.017324  [  224/  306]
train() client id: f_00004-12-7 loss: 0.978384  [  256/  306]
train() client id: f_00004-12-8 loss: 0.920428  [  288/  306]
train() client id: f_00005-0-0 loss: 1.082225  [   32/  146]
train() client id: f_00005-0-1 loss: 1.045981  [   64/  146]
train() client id: f_00005-0-2 loss: 1.040169  [   96/  146]
train() client id: f_00005-0-3 loss: 0.984622  [  128/  146]
train() client id: f_00005-1-0 loss: 1.000554  [   32/  146]
train() client id: f_00005-1-1 loss: 1.049041  [   64/  146]
train() client id: f_00005-1-2 loss: 1.018059  [   96/  146]
train() client id: f_00005-1-3 loss: 0.945293  [  128/  146]
train() client id: f_00005-2-0 loss: 1.018446  [   32/  146]
train() client id: f_00005-2-1 loss: 0.928738  [   64/  146]
train() client id: f_00005-2-2 loss: 0.966502  [   96/  146]
train() client id: f_00005-2-3 loss: 1.007159  [  128/  146]
train() client id: f_00005-3-0 loss: 0.997536  [   32/  146]
train() client id: f_00005-3-1 loss: 0.927573  [   64/  146]
train() client id: f_00005-3-2 loss: 0.923132  [   96/  146]
train() client id: f_00005-3-3 loss: 0.957890  [  128/  146]
train() client id: f_00005-4-0 loss: 0.963908  [   32/  146]
train() client id: f_00005-4-1 loss: 0.967121  [   64/  146]
train() client id: f_00005-4-2 loss: 1.000369  [   96/  146]
train() client id: f_00005-4-3 loss: 0.830745  [  128/  146]
train() client id: f_00005-5-0 loss: 0.851212  [   32/  146]
train() client id: f_00005-5-1 loss: 0.897773  [   64/  146]
train() client id: f_00005-5-2 loss: 1.008061  [   96/  146]
train() client id: f_00005-5-3 loss: 0.932325  [  128/  146]
train() client id: f_00005-6-0 loss: 0.901189  [   32/  146]
train() client id: f_00005-6-1 loss: 0.819207  [   64/  146]
train() client id: f_00005-6-2 loss: 0.897433  [   96/  146]
train() client id: f_00005-6-3 loss: 0.973623  [  128/  146]
train() client id: f_00005-7-0 loss: 0.773544  [   32/  146]
train() client id: f_00005-7-1 loss: 0.986936  [   64/  146]
train() client id: f_00005-7-2 loss: 0.870591  [   96/  146]
train() client id: f_00005-7-3 loss: 0.922211  [  128/  146]
train() client id: f_00005-8-0 loss: 0.981492  [   32/  146]
train() client id: f_00005-8-1 loss: 0.854667  [   64/  146]
train() client id: f_00005-8-2 loss: 0.938433  [   96/  146]
train() client id: f_00005-8-3 loss: 0.834972  [  128/  146]
train() client id: f_00005-9-0 loss: 0.836749  [   32/  146]
train() client id: f_00005-9-1 loss: 0.985874  [   64/  146]
train() client id: f_00005-9-2 loss: 0.815100  [   96/  146]
train() client id: f_00005-9-3 loss: 0.937673  [  128/  146]
train() client id: f_00005-10-0 loss: 0.806940  [   32/  146]
train() client id: f_00005-10-1 loss: 0.942796  [   64/  146]
train() client id: f_00005-10-2 loss: 0.771051  [   96/  146]
train() client id: f_00005-10-3 loss: 1.060340  [  128/  146]
train() client id: f_00005-11-0 loss: 0.758024  [   32/  146]
train() client id: f_00005-11-1 loss: 0.791240  [   64/  146]
train() client id: f_00005-11-2 loss: 0.968232  [   96/  146]
train() client id: f_00005-11-3 loss: 1.013869  [  128/  146]
train() client id: f_00005-12-0 loss: 0.797818  [   32/  146]
train() client id: f_00005-12-1 loss: 0.867952  [   64/  146]
train() client id: f_00005-12-2 loss: 0.867133  [   96/  146]
train() client id: f_00005-12-3 loss: 0.854897  [  128/  146]
train() client id: f_00006-0-0 loss: 1.073660  [   32/   54]
train() client id: f_00006-1-0 loss: 1.069016  [   32/   54]
train() client id: f_00006-2-0 loss: 1.053574  [   32/   54]
train() client id: f_00006-3-0 loss: 1.071729  [   32/   54]
train() client id: f_00006-4-0 loss: 1.099636  [   32/   54]
train() client id: f_00006-5-0 loss: 1.049176  [   32/   54]
train() client id: f_00006-6-0 loss: 1.052886  [   32/   54]
train() client id: f_00006-7-0 loss: 1.063498  [   32/   54]
train() client id: f_00006-8-0 loss: 1.101713  [   32/   54]
train() client id: f_00006-9-0 loss: 1.059337  [   32/   54]
train() client id: f_00006-10-0 loss: 1.084697  [   32/   54]
train() client id: f_00006-11-0 loss: 1.093361  [   32/   54]
train() client id: f_00006-12-0 loss: 1.063325  [   32/   54]
train() client id: f_00007-0-0 loss: 1.153180  [   32/  179]
train() client id: f_00007-0-1 loss: 1.135917  [   64/  179]
train() client id: f_00007-0-2 loss: 1.116486  [   96/  179]
train() client id: f_00007-0-3 loss: 1.024341  [  128/  179]
train() client id: f_00007-0-4 loss: 1.047537  [  160/  179]
train() client id: f_00007-1-0 loss: 1.050231  [   32/  179]
train() client id: f_00007-1-1 loss: 1.028989  [   64/  179]
train() client id: f_00007-1-2 loss: 1.037183  [   96/  179]
train() client id: f_00007-1-3 loss: 0.997678  [  128/  179]
train() client id: f_00007-1-4 loss: 0.999703  [  160/  179]
train() client id: f_00007-2-0 loss: 1.003110  [   32/  179]
train() client id: f_00007-2-1 loss: 0.991766  [   64/  179]
train() client id: f_00007-2-2 loss: 0.954811  [   96/  179]
train() client id: f_00007-2-3 loss: 0.942902  [  128/  179]
train() client id: f_00007-2-4 loss: 0.937551  [  160/  179]
train() client id: f_00007-3-0 loss: 0.930337  [   32/  179]
train() client id: f_00007-3-1 loss: 0.918405  [   64/  179]
train() client id: f_00007-3-2 loss: 0.906163  [   96/  179]
train() client id: f_00007-3-3 loss: 0.864129  [  128/  179]
train() client id: f_00007-3-4 loss: 0.957488  [  160/  179]
train() client id: f_00007-4-0 loss: 0.876474  [   32/  179]
train() client id: f_00007-4-1 loss: 0.940403  [   64/  179]
train() client id: f_00007-4-2 loss: 0.880242  [   96/  179]
train() client id: f_00007-4-3 loss: 0.850239  [  128/  179]
train() client id: f_00007-4-4 loss: 0.841179  [  160/  179]
train() client id: f_00007-5-0 loss: 0.924297  [   32/  179]
train() client id: f_00007-5-1 loss: 0.868853  [   64/  179]
train() client id: f_00007-5-2 loss: 0.785084  [   96/  179]
train() client id: f_00007-5-3 loss: 0.840039  [  128/  179]
train() client id: f_00007-5-4 loss: 0.833863  [  160/  179]
train() client id: f_00007-6-0 loss: 0.875514  [   32/  179]
train() client id: f_00007-6-1 loss: 0.775823  [   64/  179]
train() client id: f_00007-6-2 loss: 0.818292  [   96/  179]
train() client id: f_00007-6-3 loss: 0.910881  [  128/  179]
train() client id: f_00007-6-4 loss: 0.816444  [  160/  179]
train() client id: f_00007-7-0 loss: 0.889791  [   32/  179]
train() client id: f_00007-7-1 loss: 0.766888  [   64/  179]
train() client id: f_00007-7-2 loss: 0.813557  [   96/  179]
train() client id: f_00007-7-3 loss: 0.889352  [  128/  179]
train() client id: f_00007-7-4 loss: 0.777678  [  160/  179]
train() client id: f_00007-8-0 loss: 0.726428  [   32/  179]
train() client id: f_00007-8-1 loss: 0.908024  [   64/  179]
train() client id: f_00007-8-2 loss: 0.841432  [   96/  179]
train() client id: f_00007-8-3 loss: 0.831753  [  128/  179]
train() client id: f_00007-8-4 loss: 0.697418  [  160/  179]
train() client id: f_00007-9-0 loss: 0.802897  [   32/  179]
train() client id: f_00007-9-1 loss: 0.806441  [   64/  179]
train() client id: f_00007-9-2 loss: 0.765983  [   96/  179]
train() client id: f_00007-9-3 loss: 0.810526  [  128/  179]
train() client id: f_00007-9-4 loss: 0.823912  [  160/  179]
train() client id: f_00007-10-0 loss: 0.897162  [   32/  179]
train() client id: f_00007-10-1 loss: 0.721174  [   64/  179]
train() client id: f_00007-10-2 loss: 0.876561  [   96/  179]
train() client id: f_00007-10-3 loss: 0.788139  [  128/  179]
train() client id: f_00007-10-4 loss: 0.765561  [  160/  179]
train() client id: f_00007-11-0 loss: 0.779458  [   32/  179]
train() client id: f_00007-11-1 loss: 0.759760  [   64/  179]
train() client id: f_00007-11-2 loss: 0.818482  [   96/  179]
train() client id: f_00007-11-3 loss: 0.892321  [  128/  179]
train() client id: f_00007-11-4 loss: 0.769027  [  160/  179]
train() client id: f_00007-12-0 loss: 0.914823  [   32/  179]
train() client id: f_00007-12-1 loss: 0.723014  [   64/  179]
train() client id: f_00007-12-2 loss: 0.825983  [   96/  179]
train() client id: f_00007-12-3 loss: 0.748637  [  128/  179]
train() client id: f_00007-12-4 loss: 0.840908  [  160/  179]
train() client id: f_00008-0-0 loss: 0.992765  [   32/  130]
train() client id: f_00008-0-1 loss: 1.026297  [   64/  130]
train() client id: f_00008-0-2 loss: 0.946286  [   96/  130]
train() client id: f_00008-0-3 loss: 1.013217  [  128/  130]
train() client id: f_00008-1-0 loss: 1.032709  [   32/  130]
train() client id: f_00008-1-1 loss: 0.944042  [   64/  130]
train() client id: f_00008-1-2 loss: 0.934030  [   96/  130]
train() client id: f_00008-1-3 loss: 1.006772  [  128/  130]
train() client id: f_00008-2-0 loss: 0.985893  [   32/  130]
train() client id: f_00008-2-1 loss: 0.950982  [   64/  130]
train() client id: f_00008-2-2 loss: 0.987682  [   96/  130]
train() client id: f_00008-2-3 loss: 0.983274  [  128/  130]
train() client id: f_00008-3-0 loss: 0.992973  [   32/  130]
train() client id: f_00008-3-1 loss: 0.924689  [   64/  130]
train() client id: f_00008-3-2 loss: 1.003919  [   96/  130]
train() client id: f_00008-3-3 loss: 0.933475  [  128/  130]
train() client id: f_00008-4-0 loss: 0.978231  [   32/  130]
train() client id: f_00008-4-1 loss: 0.907339  [   64/  130]
train() client id: f_00008-4-2 loss: 0.994704  [   96/  130]
train() client id: f_00008-4-3 loss: 1.021452  [  128/  130]
train() client id: f_00008-5-0 loss: 0.953891  [   32/  130]
train() client id: f_00008-5-1 loss: 1.033845  [   64/  130]
train() client id: f_00008-5-2 loss: 0.957717  [   96/  130]
train() client id: f_00008-5-3 loss: 0.923275  [  128/  130]
train() client id: f_00008-6-0 loss: 1.012432  [   32/  130]
train() client id: f_00008-6-1 loss: 0.881516  [   64/  130]
train() client id: f_00008-6-2 loss: 1.007211  [   96/  130]
train() client id: f_00008-6-3 loss: 0.987696  [  128/  130]
train() client id: f_00008-7-0 loss: 0.885940  [   32/  130]
train() client id: f_00008-7-1 loss: 0.935132  [   64/  130]
train() client id: f_00008-7-2 loss: 1.101441  [   96/  130]
train() client id: f_00008-7-3 loss: 0.960308  [  128/  130]
train() client id: f_00008-8-0 loss: 0.966107  [   32/  130]
train() client id: f_00008-8-1 loss: 0.863825  [   64/  130]
train() client id: f_00008-8-2 loss: 1.049919  [   96/  130]
train() client id: f_00008-8-3 loss: 0.976691  [  128/  130]
train() client id: f_00008-9-0 loss: 1.062031  [   32/  130]
train() client id: f_00008-9-1 loss: 1.027927  [   64/  130]
train() client id: f_00008-9-2 loss: 0.933918  [   96/  130]
train() client id: f_00008-9-3 loss: 0.859175  [  128/  130]
train() client id: f_00008-10-0 loss: 1.033995  [   32/  130]
train() client id: f_00008-10-1 loss: 0.936416  [   64/  130]
train() client id: f_00008-10-2 loss: 0.978488  [   96/  130]
train() client id: f_00008-10-3 loss: 0.942374  [  128/  130]
train() client id: f_00008-11-0 loss: 0.903589  [   32/  130]
train() client id: f_00008-11-1 loss: 0.988841  [   64/  130]
train() client id: f_00008-11-2 loss: 1.009388  [   96/  130]
train() client id: f_00008-11-3 loss: 0.978406  [  128/  130]
train() client id: f_00008-12-0 loss: 0.973327  [   32/  130]
train() client id: f_00008-12-1 loss: 0.905552  [   64/  130]
train() client id: f_00008-12-2 loss: 0.982728  [   96/  130]
train() client id: f_00008-12-3 loss: 1.026563  [  128/  130]
train() client id: f_00009-0-0 loss: 1.213574  [   32/  118]
train() client id: f_00009-0-1 loss: 1.278705  [   64/  118]
train() client id: f_00009-0-2 loss: 1.143212  [   96/  118]
train() client id: f_00009-1-0 loss: 1.224471  [   32/  118]
train() client id: f_00009-1-1 loss: 1.240600  [   64/  118]
train() client id: f_00009-1-2 loss: 1.109598  [   96/  118]
train() client id: f_00009-2-0 loss: 1.100360  [   32/  118]
train() client id: f_00009-2-1 loss: 1.217329  [   64/  118]
train() client id: f_00009-2-2 loss: 1.125452  [   96/  118]
train() client id: f_00009-3-0 loss: 1.022423  [   32/  118]
train() client id: f_00009-3-1 loss: 1.157062  [   64/  118]
train() client id: f_00009-3-2 loss: 1.133293  [   96/  118]
train() client id: f_00009-4-0 loss: 1.112502  [   32/  118]
train() client id: f_00009-4-1 loss: 1.072021  [   64/  118]
train() client id: f_00009-4-2 loss: 1.073889  [   96/  118]
train() client id: f_00009-5-0 loss: 1.076160  [   32/  118]
train() client id: f_00009-5-1 loss: 1.022269  [   64/  118]
train() client id: f_00009-5-2 loss: 1.042679  [   96/  118]
train() client id: f_00009-6-0 loss: 1.110421  [   32/  118]
train() client id: f_00009-6-1 loss: 0.994462  [   64/  118]
train() client id: f_00009-6-2 loss: 1.051639  [   96/  118]
train() client id: f_00009-7-0 loss: 1.104686  [   32/  118]
train() client id: f_00009-7-1 loss: 0.973808  [   64/  118]
train() client id: f_00009-7-2 loss: 0.995105  [   96/  118]
train() client id: f_00009-8-0 loss: 1.075972  [   32/  118]
train() client id: f_00009-8-1 loss: 0.973020  [   64/  118]
train() client id: f_00009-8-2 loss: 0.961764  [   96/  118]
train() client id: f_00009-9-0 loss: 1.023667  [   32/  118]
train() client id: f_00009-9-1 loss: 0.990485  [   64/  118]
train() client id: f_00009-9-2 loss: 1.034248  [   96/  118]
train() client id: f_00009-10-0 loss: 0.987437  [   32/  118]
train() client id: f_00009-10-1 loss: 1.019546  [   64/  118]
train() client id: f_00009-10-2 loss: 1.053479  [   96/  118]
train() client id: f_00009-11-0 loss: 1.024335  [   32/  118]
train() client id: f_00009-11-1 loss: 0.988132  [   64/  118]
train() client id: f_00009-11-2 loss: 0.919346  [   96/  118]
train() client id: f_00009-12-0 loss: 1.016266  [   32/  118]
train() client id: f_00009-12-1 loss: 0.953160  [   64/  118]
train() client id: f_00009-12-2 loss: 1.033786  [   96/  118]
At round 3 accuracy: 0.5915119363395226
At round 3 training accuracy: 0.5499664654594232
At round 3 training loss: 0.9755121407414507
update_location
xs = [ -3.9056584   -0.79968212  35.00902392  18.81129433 -19.02070377
  -6.04359014   2.55680806  -6.32485185  19.66397685  -2.06087855]
ys = [ 27.5879595   15.55583871   1.32061395   2.54482414   9.35018685
 -17.18584926  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [103.80919842 101.2058477  105.95931189 101.78575993 102.2213929
 101.64584791 100.06711653 100.2031936  103.41828638 100.10124413]
dists_bs = [225.83972948 236.15289818 272.52214454 259.4293844  227.52890245
 255.89386294 251.15145857 242.46476046 250.35690873 243.20450851]
uav_gains = [9.10767832e-11 9.70474414e-11 8.65263782e-11 9.56710163e-11
 9.46549499e-11 9.60005847e-11 9.98320356e-11 9.94934409e-11
 9.19399105e-11 9.97469659e-11]
bs_gains = [2.83565323e-11 2.50237793e-11 1.67559498e-11 1.92327073e-11
 2.77710106e-11 1.99860260e-11 2.10607635e-11 2.32422371e-11
 2.12484502e-11 2.30448319e-11]
Round 4
-------------------------------
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.46545628 21.87269455 10.30373985  3.68435765 25.21614886 12.16538239
  4.58153103 14.79179615 10.85487015  9.87006751]
obj_prev = 123.8060444419656
eta_min = 1.1887003308068872e-09	eta_max = 0.9184674772800265
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 28.799330597431425	eta = 0.9090909090909091
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 49.438331310801296	eta = 0.5295730850913346
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 39.62791236585137	eta = 0.660675974861339
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.87221877758381	eta = 0.6913038232004766
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.78643716203915	eta = 0.6928732000256066
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.78621816141451	eta = 0.6928772157665561
eta = 0.6928772157665561
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [0.03027591 0.06367553 0.02979534 0.01033226 0.07352722 0.03508161
 0.01297539 0.04301102 0.03123705 0.02835363]
ene_total = [3.202787   6.29122282 3.16613857 1.4566637  7.13363507 3.83605127
 1.68315721 4.30566359 3.48877145 3.22212748]
ti_comp = [0.2772398  0.25747119 0.276632   0.27781288 0.25949396 0.25275957
 0.27830054 0.2782619  0.25409312 0.25580165]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.25663118e-05 2.43411145e-04 2.16033125e-05 8.93224701e-07
 3.68952017e-04 4.22380117e-05 1.76284161e-06 6.42261255e-05
 2.95055929e-05 2.17720570e-05]
ene_total = [0.56161072 0.75347478 0.56683292 0.55471554 0.74677556 0.77704532
 0.5505341  0.55632463 0.76429157 0.74870068]
optimize_network_iter = 0 obj = 6.580305820795663
eta = 0.6928772157665561
freqs = [5.46023917e+07 1.23655643e+08 5.38537534e+07 1.85957107e+07
 1.41674235e+08 6.93972005e+07 2.33118325e+07 7.72851390e+07
 6.14677254e+07 5.54211297e+07]
eta_min = 0.6721669962857192	eta_max = 0.692877215766553
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 0.06566942832741904	eta = 0.9090909090909091
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 22.05222718616278	eta = 0.002707185981428379
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.358376629903991	eta = 0.02531380252868433
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.276155607764052	eta = 0.02622820693542063
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.276117460225635	eta = 0.02622864651797696
eta = 0.02622864651797696
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.23933498e-04 2.41545493e-03 2.14377315e-04 8.86378481e-06
 3.66124143e-03 4.19142738e-04 1.74933012e-05 6.37338573e-04
 2.92794440e-04 2.16051827e-04]
ene_total = [0.18267388 0.2974487  0.18408433 0.17517407 0.32618133 0.25545427
 0.17406893 0.19124253 0.24830343 0.241486  ]
ti_comp = [0.30025765 0.28048904 0.29964984 0.30083073 0.28251181 0.27577742
 0.30131839 0.30127974 0.27711097 0.27881949]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44184640e-05 2.60315910e-04 2.33685526e-05 9.66842755e-07
 3.95080819e-04 4.50335349e-05 1.90864377e-06 6.95367030e-05
 3.14859996e-05 2.32591919e-05]
ene_total = [0.52628353 0.70725804 0.53116872 0.51967847 0.70173644 0.72818562
 0.51576709 0.52161422 0.71617089 0.70152458]
optimize_network_iter = 1 obj = 6.169387607456715
eta = 0.6721669962857192
freqs = [5.45864375e+07 1.22896148e+08 5.38289512e+07 1.85932175e+07
 1.40894199e+08 6.88656172e+07 2.33118325e+07 7.72843192e+07
 6.10236134e+07 5.50512470e+07]
eta_min = 0.6721669962857293	eta_max = 0.6721669962857161
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 0.06499207998561081	eta = 0.909090909090909
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 22.05158160424028	eta = 0.0026793411075088985
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.3552731564287246	eta = 0.02508571412048698
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.273849552272878	eta = 0.0259840010165886
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.2738124460945865	eta = 0.025984425047592612
eta = 0.025984425047592612
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.24377690e-04 2.39200478e-03 2.14730208e-04 8.88417650e-06
 3.63033979e-03 4.13806558e-04 1.75382481e-05 6.38962582e-04
 2.89320240e-04 2.13725309e-04]
ene_total = [0.1826403  0.29672859 0.18404788 0.17513071 0.32524889 0.25524333
 0.17402652 0.19123929 0.24814553 0.24136141]
ti_comp = [0.30025765 0.28048904 0.29964984 0.30083073 0.28251181 0.27577742
 0.30131839 0.30127974 0.27711097 0.27881949]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44184640e-05 2.60315910e-04 2.33685526e-05 9.66842755e-07
 3.95080819e-04 4.50335349e-05 1.90864377e-06 6.95367030e-05
 3.14859996e-05 2.32591919e-05]
ene_total = [0.52628353 0.70725804 0.53116872 0.51967847 0.70173644 0.72818562
 0.51576709 0.52161422 0.71617089 0.70152458]
optimize_network_iter = 2 obj = 6.169387607456903
eta = 0.6721669962857293
freqs = [5.45864375e+07 1.22896148e+08 5.38289512e+07 1.85932175e+07
 1.40894199e+08 6.88656172e+07 2.33118325e+07 7.72843192e+07
 6.10236134e+07 5.50512470e+07]
Done!
At round 4 eta: 0.6721669962857293
At round 4 local rounds: 13.007932941342311
At round 4 global rounds: 81.78682184099438
At round 4 a_n: 26.46987362140779
gradient difference: 0.4468924105167389
train() client id: f_00000-0-0 loss: 1.233819  [   32/  126]
train() client id: f_00000-0-1 loss: 1.243277  [   64/  126]
train() client id: f_00000-0-2 loss: 1.021293  [   96/  126]
train() client id: f_00000-1-0 loss: 1.051259  [   32/  126]
train() client id: f_00000-1-1 loss: 1.112645  [   64/  126]
train() client id: f_00000-1-2 loss: 1.100675  [   96/  126]
train() client id: f_00000-2-0 loss: 1.051686  [   32/  126]
train() client id: f_00000-2-1 loss: 1.024945  [   64/  126]
train() client id: f_00000-2-2 loss: 1.032365  [   96/  126]
train() client id: f_00000-3-0 loss: 1.054791  [   32/  126]
train() client id: f_00000-3-1 loss: 0.967607  [   64/  126]
train() client id: f_00000-3-2 loss: 0.994166  [   96/  126]
train() client id: f_00000-4-0 loss: 0.960187  [   32/  126]
train() client id: f_00000-4-1 loss: 0.928013  [   64/  126]
train() client id: f_00000-4-2 loss: 1.032807  [   96/  126]
train() client id: f_00000-5-0 loss: 0.950542  [   32/  126]
train() client id: f_00000-5-1 loss: 0.943286  [   64/  126]
train() client id: f_00000-5-2 loss: 0.945898  [   96/  126]
train() client id: f_00000-6-0 loss: 0.876994  [   32/  126]
train() client id: f_00000-6-1 loss: 0.976710  [   64/  126]
train() client id: f_00000-6-2 loss: 0.964904  [   96/  126]
train() client id: f_00000-7-0 loss: 0.935574  [   32/  126]
train() client id: f_00000-7-1 loss: 0.905842  [   64/  126]
train() client id: f_00000-7-2 loss: 0.940008  [   96/  126]
train() client id: f_00000-8-0 loss: 0.849355  [   32/  126]
train() client id: f_00000-8-1 loss: 0.923293  [   64/  126]
train() client id: f_00000-8-2 loss: 0.885848  [   96/  126]
train() client id: f_00000-9-0 loss: 0.887237  [   32/  126]
train() client id: f_00000-9-1 loss: 0.988088  [   64/  126]
train() client id: f_00000-9-2 loss: 0.917499  [   96/  126]
train() client id: f_00000-10-0 loss: 0.898337  [   32/  126]
train() client id: f_00000-10-1 loss: 0.892440  [   64/  126]
train() client id: f_00000-10-2 loss: 0.994331  [   96/  126]
train() client id: f_00000-11-0 loss: 0.867615  [   32/  126]
train() client id: f_00000-11-1 loss: 0.839444  [   64/  126]
train() client id: f_00000-11-2 loss: 1.036801  [   96/  126]
train() client id: f_00000-12-0 loss: 0.966060  [   32/  126]
train() client id: f_00000-12-1 loss: 0.812051  [   64/  126]
train() client id: f_00000-12-2 loss: 0.921597  [   96/  126]
train() client id: f_00001-0-0 loss: 0.679679  [   32/  265]
train() client id: f_00001-0-1 loss: 0.630630  [   64/  265]
train() client id: f_00001-0-2 loss: 0.701034  [   96/  265]
train() client id: f_00001-0-3 loss: 0.596609  [  128/  265]
train() client id: f_00001-0-4 loss: 0.646873  [  160/  265]
train() client id: f_00001-0-5 loss: 0.580900  [  192/  265]
train() client id: f_00001-0-6 loss: 0.629514  [  224/  265]
train() client id: f_00001-0-7 loss: 0.687213  [  256/  265]
train() client id: f_00001-1-0 loss: 0.660250  [   32/  265]
train() client id: f_00001-1-1 loss: 0.646640  [   64/  265]
train() client id: f_00001-1-2 loss: 0.604779  [   96/  265]
train() client id: f_00001-1-3 loss: 0.597233  [  128/  265]
train() client id: f_00001-1-4 loss: 0.653012  [  160/  265]
train() client id: f_00001-1-5 loss: 0.566668  [  192/  265]
train() client id: f_00001-1-6 loss: 0.562691  [  224/  265]
train() client id: f_00001-1-7 loss: 0.655737  [  256/  265]
train() client id: f_00001-2-0 loss: 0.617646  [   32/  265]
train() client id: f_00001-2-1 loss: 0.636714  [   64/  265]
train() client id: f_00001-2-2 loss: 0.560039  [   96/  265]
train() client id: f_00001-2-3 loss: 0.583334  [  128/  265]
train() client id: f_00001-2-4 loss: 0.535931  [  160/  265]
train() client id: f_00001-2-5 loss: 0.607942  [  192/  265]
train() client id: f_00001-2-6 loss: 0.612626  [  224/  265]
train() client id: f_00001-2-7 loss: 0.581651  [  256/  265]
train() client id: f_00001-3-0 loss: 0.570687  [   32/  265]
train() client id: f_00001-3-1 loss: 0.695875  [   64/  265]
train() client id: f_00001-3-2 loss: 0.522215  [   96/  265]
train() client id: f_00001-3-3 loss: 0.571735  [  128/  265]
train() client id: f_00001-3-4 loss: 0.539965  [  160/  265]
train() client id: f_00001-3-5 loss: 0.522443  [  192/  265]
train() client id: f_00001-3-6 loss: 0.557093  [  224/  265]
train() client id: f_00001-3-7 loss: 0.634475  [  256/  265]
train() client id: f_00001-4-0 loss: 0.552477  [   32/  265]
train() client id: f_00001-4-1 loss: 0.551618  [   64/  265]
train() client id: f_00001-4-2 loss: 0.512844  [   96/  265]
train() client id: f_00001-4-3 loss: 0.602170  [  128/  265]
train() client id: f_00001-4-4 loss: 0.546169  [  160/  265]
train() client id: f_00001-4-5 loss: 0.680771  [  192/  265]
train() client id: f_00001-4-6 loss: 0.577406  [  224/  265]
train() client id: f_00001-4-7 loss: 0.496847  [  256/  265]
train() client id: f_00001-5-0 loss: 0.527616  [   32/  265]
train() client id: f_00001-5-1 loss: 0.511721  [   64/  265]
train() client id: f_00001-5-2 loss: 0.527828  [   96/  265]
train() client id: f_00001-5-3 loss: 0.618677  [  128/  265]
train() client id: f_00001-5-4 loss: 0.592946  [  160/  265]
train() client id: f_00001-5-5 loss: 0.556416  [  192/  265]
train() client id: f_00001-5-6 loss: 0.539465  [  224/  265]
train() client id: f_00001-5-7 loss: 0.563714  [  256/  265]
train() client id: f_00001-6-0 loss: 0.528633  [   32/  265]
train() client id: f_00001-6-1 loss: 0.492872  [   64/  265]
train() client id: f_00001-6-2 loss: 0.525690  [   96/  265]
train() client id: f_00001-6-3 loss: 0.506759  [  128/  265]
train() client id: f_00001-6-4 loss: 0.526662  [  160/  265]
train() client id: f_00001-6-5 loss: 0.489897  [  192/  265]
train() client id: f_00001-6-6 loss: 0.654260  [  224/  265]
train() client id: f_00001-6-7 loss: 0.701115  [  256/  265]
train() client id: f_00001-7-0 loss: 0.498757  [   32/  265]
train() client id: f_00001-7-1 loss: 0.459070  [   64/  265]
train() client id: f_00001-7-2 loss: 0.552560  [   96/  265]
train() client id: f_00001-7-3 loss: 0.603001  [  128/  265]
train() client id: f_00001-7-4 loss: 0.527246  [  160/  265]
train() client id: f_00001-7-5 loss: 0.626557  [  192/  265]
train() client id: f_00001-7-6 loss: 0.576949  [  224/  265]
train() client id: f_00001-7-7 loss: 0.584716  [  256/  265]
train() client id: f_00001-8-0 loss: 0.535749  [   32/  265]
train() client id: f_00001-8-1 loss: 0.545882  [   64/  265]
train() client id: f_00001-8-2 loss: 0.476822  [   96/  265]
train() client id: f_00001-8-3 loss: 0.550300  [  128/  265]
train() client id: f_00001-8-4 loss: 0.743916  [  160/  265]
train() client id: f_00001-8-5 loss: 0.498031  [  192/  265]
train() client id: f_00001-8-6 loss: 0.511137  [  224/  265]
train() client id: f_00001-8-7 loss: 0.521279  [  256/  265]
train() client id: f_00001-9-0 loss: 0.558832  [   32/  265]
train() client id: f_00001-9-1 loss: 0.511628  [   64/  265]
train() client id: f_00001-9-2 loss: 0.455449  [   96/  265]
train() client id: f_00001-9-3 loss: 0.487158  [  128/  265]
train() client id: f_00001-9-4 loss: 0.547491  [  160/  265]
train() client id: f_00001-9-5 loss: 0.451921  [  192/  265]
train() client id: f_00001-9-6 loss: 0.657268  [  224/  265]
train() client id: f_00001-9-7 loss: 0.646513  [  256/  265]
train() client id: f_00001-10-0 loss: 0.486103  [   32/  265]
train() client id: f_00001-10-1 loss: 0.534971  [   64/  265]
train() client id: f_00001-10-2 loss: 0.458736  [   96/  265]
train() client id: f_00001-10-3 loss: 0.527330  [  128/  265]
train() client id: f_00001-10-4 loss: 0.600487  [  160/  265]
train() client id: f_00001-10-5 loss: 0.551449  [  192/  265]
train() client id: f_00001-10-6 loss: 0.640099  [  224/  265]
train() client id: f_00001-10-7 loss: 0.520400  [  256/  265]
train() client id: f_00001-11-0 loss: 0.518190  [   32/  265]
train() client id: f_00001-11-1 loss: 0.601564  [   64/  265]
train() client id: f_00001-11-2 loss: 0.520063  [   96/  265]
train() client id: f_00001-11-3 loss: 0.467667  [  128/  265]
train() client id: f_00001-11-4 loss: 0.605828  [  160/  265]
train() client id: f_00001-11-5 loss: 0.603538  [  192/  265]
train() client id: f_00001-11-6 loss: 0.602400  [  224/  265]
train() client id: f_00001-11-7 loss: 0.448427  [  256/  265]
train() client id: f_00001-12-0 loss: 0.504851  [   32/  265]
train() client id: f_00001-12-1 loss: 0.474550  [   64/  265]
train() client id: f_00001-12-2 loss: 0.563722  [   96/  265]
train() client id: f_00001-12-3 loss: 0.636146  [  128/  265]
train() client id: f_00001-12-4 loss: 0.628649  [  160/  265]
train() client id: f_00001-12-5 loss: 0.473064  [  192/  265]
train() client id: f_00001-12-6 loss: 0.593280  [  224/  265]
train() client id: f_00001-12-7 loss: 0.503126  [  256/  265]
train() client id: f_00002-0-0 loss: 1.272951  [   32/  124]
train() client id: f_00002-0-1 loss: 1.193330  [   64/  124]
train() client id: f_00002-0-2 loss: 1.278649  [   96/  124]
train() client id: f_00002-1-0 loss: 1.244562  [   32/  124]
train() client id: f_00002-1-1 loss: 1.187060  [   64/  124]
train() client id: f_00002-1-2 loss: 1.237055  [   96/  124]
train() client id: f_00002-2-0 loss: 1.161348  [   32/  124]
train() client id: f_00002-2-1 loss: 1.215351  [   64/  124]
train() client id: f_00002-2-2 loss: 1.171942  [   96/  124]
train() client id: f_00002-3-0 loss: 1.141003  [   32/  124]
train() client id: f_00002-3-1 loss: 1.177929  [   64/  124]
train() client id: f_00002-3-2 loss: 1.107019  [   96/  124]
train() client id: f_00002-4-0 loss: 1.134422  [   32/  124]
train() client id: f_00002-4-1 loss: 1.119490  [   64/  124]
train() client id: f_00002-4-2 loss: 1.096738  [   96/  124]
train() client id: f_00002-5-0 loss: 1.129980  [   32/  124]
train() client id: f_00002-5-1 loss: 1.110816  [   64/  124]
train() client id: f_00002-5-2 loss: 1.103738  [   96/  124]
train() client id: f_00002-6-0 loss: 1.085877  [   32/  124]
train() client id: f_00002-6-1 loss: 1.076706  [   64/  124]
train() client id: f_00002-6-2 loss: 1.141827  [   96/  124]
train() client id: f_00002-7-0 loss: 1.124872  [   32/  124]
train() client id: f_00002-7-1 loss: 1.082031  [   64/  124]
train() client id: f_00002-7-2 loss: 1.000537  [   96/  124]
train() client id: f_00002-8-0 loss: 1.004652  [   32/  124]
train() client id: f_00002-8-1 loss: 1.117650  [   64/  124]
train() client id: f_00002-8-2 loss: 1.054444  [   96/  124]
train() client id: f_00002-9-0 loss: 1.032194  [   32/  124]
train() client id: f_00002-9-1 loss: 1.036288  [   64/  124]
train() client id: f_00002-9-2 loss: 1.058775  [   96/  124]
train() client id: f_00002-10-0 loss: 1.100881  [   32/  124]
train() client id: f_00002-10-1 loss: 1.085476  [   64/  124]
train() client id: f_00002-10-2 loss: 0.952234  [   96/  124]
train() client id: f_00002-11-0 loss: 1.018924  [   32/  124]
train() client id: f_00002-11-1 loss: 1.081702  [   64/  124]
train() client id: f_00002-11-2 loss: 1.057768  [   96/  124]
train() client id: f_00002-12-0 loss: 0.969374  [   32/  124]
train() client id: f_00002-12-1 loss: 1.060162  [   64/  124]
train() client id: f_00002-12-2 loss: 1.017897  [   96/  124]
train() client id: f_00003-0-0 loss: 1.078373  [   32/   43]
train() client id: f_00003-1-0 loss: 1.079756  [   32/   43]
train() client id: f_00003-2-0 loss: 1.042278  [   32/   43]
train() client id: f_00003-3-0 loss: 1.122010  [   32/   43]
train() client id: f_00003-4-0 loss: 1.112183  [   32/   43]
train() client id: f_00003-5-0 loss: 1.054696  [   32/   43]
train() client id: f_00003-6-0 loss: 1.058814  [   32/   43]
train() client id: f_00003-7-0 loss: 1.105911  [   32/   43]
train() client id: f_00003-8-0 loss: 1.103969  [   32/   43]
train() client id: f_00003-9-0 loss: 1.063534  [   32/   43]
train() client id: f_00003-10-0 loss: 1.004400  [   32/   43]
train() client id: f_00003-11-0 loss: 1.104347  [   32/   43]
train() client id: f_00003-12-0 loss: 1.058425  [   32/   43]
train() client id: f_00004-0-0 loss: 1.071519  [   32/  306]
train() client id: f_00004-0-1 loss: 1.104358  [   64/  306]
train() client id: f_00004-0-2 loss: 1.034195  [   96/  306]
train() client id: f_00004-0-3 loss: 1.041572  [  128/  306]
train() client id: f_00004-0-4 loss: 1.043923  [  160/  306]
train() client id: f_00004-0-5 loss: 0.987445  [  192/  306]
train() client id: f_00004-0-6 loss: 1.051204  [  224/  306]
train() client id: f_00004-0-7 loss: 1.044263  [  256/  306]
train() client id: f_00004-0-8 loss: 1.064457  [  288/  306]
train() client id: f_00004-1-0 loss: 1.059974  [   32/  306]
train() client id: f_00004-1-1 loss: 1.185527  [   64/  306]
train() client id: f_00004-1-2 loss: 1.063736  [   96/  306]
train() client id: f_00004-1-3 loss: 0.982912  [  128/  306]
train() client id: f_00004-1-4 loss: 1.023393  [  160/  306]
train() client id: f_00004-1-5 loss: 0.988694  [  192/  306]
train() client id: f_00004-1-6 loss: 1.063463  [  224/  306]
train() client id: f_00004-1-7 loss: 0.984853  [  256/  306]
train() client id: f_00004-1-8 loss: 1.045864  [  288/  306]
train() client id: f_00004-2-0 loss: 1.146370  [   32/  306]
train() client id: f_00004-2-1 loss: 1.050269  [   64/  306]
train() client id: f_00004-2-2 loss: 1.087861  [   96/  306]
train() client id: f_00004-2-3 loss: 1.002314  [  128/  306]
train() client id: f_00004-2-4 loss: 1.016298  [  160/  306]
train() client id: f_00004-2-5 loss: 1.038378  [  192/  306]
train() client id: f_00004-2-6 loss: 1.023958  [  224/  306]
train() client id: f_00004-2-7 loss: 1.037213  [  256/  306]
train() client id: f_00004-2-8 loss: 1.050561  [  288/  306]
train() client id: f_00004-3-0 loss: 1.086298  [   32/  306]
train() client id: f_00004-3-1 loss: 1.026978  [   64/  306]
train() client id: f_00004-3-2 loss: 1.038025  [   96/  306]
train() client id: f_00004-3-3 loss: 1.058610  [  128/  306]
train() client id: f_00004-3-4 loss: 1.032362  [  160/  306]
train() client id: f_00004-3-5 loss: 1.107868  [  192/  306]
train() client id: f_00004-3-6 loss: 0.939618  [  224/  306]
train() client id: f_00004-3-7 loss: 1.079196  [  256/  306]
train() client id: f_00004-3-8 loss: 1.074699  [  288/  306]
train() client id: f_00004-4-0 loss: 1.012248  [   32/  306]
train() client id: f_00004-4-1 loss: 1.016132  [   64/  306]
train() client id: f_00004-4-2 loss: 1.082978  [   96/  306]
train() client id: f_00004-4-3 loss: 1.028764  [  128/  306]
train() client id: f_00004-4-4 loss: 0.997161  [  160/  306]
train() client id: f_00004-4-5 loss: 1.147102  [  192/  306]
train() client id: f_00004-4-6 loss: 1.061073  [  224/  306]
train() client id: f_00004-4-7 loss: 0.974342  [  256/  306]
train() client id: f_00004-4-8 loss: 1.084437  [  288/  306]
train() client id: f_00004-5-0 loss: 1.059457  [   32/  306]
train() client id: f_00004-5-1 loss: 1.028854  [   64/  306]
train() client id: f_00004-5-2 loss: 1.019147  [   96/  306]
train() client id: f_00004-5-3 loss: 1.094773  [  128/  306]
train() client id: f_00004-5-4 loss: 0.977172  [  160/  306]
train() client id: f_00004-5-5 loss: 1.054033  [  192/  306]
train() client id: f_00004-5-6 loss: 1.013822  [  224/  306]
train() client id: f_00004-5-7 loss: 1.052630  [  256/  306]
train() client id: f_00004-5-8 loss: 1.145179  [  288/  306]
train() client id: f_00004-6-0 loss: 1.066224  [   32/  306]
train() client id: f_00004-6-1 loss: 1.019130  [   64/  306]
train() client id: f_00004-6-2 loss: 1.038497  [   96/  306]
train() client id: f_00004-6-3 loss: 1.019221  [  128/  306]
train() client id: f_00004-6-4 loss: 0.996123  [  160/  306]
train() client id: f_00004-6-5 loss: 1.057287  [  192/  306]
train() client id: f_00004-6-6 loss: 1.111570  [  224/  306]
train() client id: f_00004-6-7 loss: 1.010924  [  256/  306]
train() client id: f_00004-6-8 loss: 1.049488  [  288/  306]
train() client id: f_00004-7-0 loss: 1.046036  [   32/  306]
train() client id: f_00004-7-1 loss: 1.026813  [   64/  306]
train() client id: f_00004-7-2 loss: 1.044287  [   96/  306]
train() client id: f_00004-7-3 loss: 1.083798  [  128/  306]
train() client id: f_00004-7-4 loss: 0.992630  [  160/  306]
train() client id: f_00004-7-5 loss: 1.067766  [  192/  306]
train() client id: f_00004-7-6 loss: 1.044188  [  224/  306]
train() client id: f_00004-7-7 loss: 1.051129  [  256/  306]
train() client id: f_00004-7-8 loss: 1.039964  [  288/  306]
train() client id: f_00004-8-0 loss: 1.087440  [   32/  306]
train() client id: f_00004-8-1 loss: 1.113570  [   64/  306]
train() client id: f_00004-8-2 loss: 1.026972  [   96/  306]
train() client id: f_00004-8-3 loss: 1.135478  [  128/  306]
train() client id: f_00004-8-4 loss: 1.056376  [  160/  306]
train() client id: f_00004-8-5 loss: 0.985734  [  192/  306]
train() client id: f_00004-8-6 loss: 0.947977  [  224/  306]
train() client id: f_00004-8-7 loss: 1.061287  [  256/  306]
train() client id: f_00004-8-8 loss: 1.008941  [  288/  306]
train() client id: f_00004-9-0 loss: 1.138674  [   32/  306]
train() client id: f_00004-9-1 loss: 0.966877  [   64/  306]
train() client id: f_00004-9-2 loss: 1.057776  [   96/  306]
train() client id: f_00004-9-3 loss: 1.010546  [  128/  306]
train() client id: f_00004-9-4 loss: 1.038078  [  160/  306]
train() client id: f_00004-9-5 loss: 1.060614  [  192/  306]
train() client id: f_00004-9-6 loss: 1.011962  [  224/  306]
train() client id: f_00004-9-7 loss: 1.068245  [  256/  306]
train() client id: f_00004-9-8 loss: 1.006035  [  288/  306]
train() client id: f_00004-10-0 loss: 1.162367  [   32/  306]
train() client id: f_00004-10-1 loss: 1.045892  [   64/  306]
train() client id: f_00004-10-2 loss: 1.055872  [   96/  306]
train() client id: f_00004-10-3 loss: 1.100767  [  128/  306]
train() client id: f_00004-10-4 loss: 1.048024  [  160/  306]
train() client id: f_00004-10-5 loss: 0.943670  [  192/  306]
train() client id: f_00004-10-6 loss: 1.039360  [  224/  306]
train() client id: f_00004-10-7 loss: 0.881725  [  256/  306]
train() client id: f_00004-10-8 loss: 1.091855  [  288/  306]
train() client id: f_00004-11-0 loss: 0.962885  [   32/  306]
train() client id: f_00004-11-1 loss: 1.053869  [   64/  306]
train() client id: f_00004-11-2 loss: 1.045926  [   96/  306]
train() client id: f_00004-11-3 loss: 1.125900  [  128/  306]
train() client id: f_00004-11-4 loss: 0.995053  [  160/  306]
train() client id: f_00004-11-5 loss: 1.073351  [  192/  306]
train() client id: f_00004-11-6 loss: 0.992325  [  224/  306]
train() client id: f_00004-11-7 loss: 1.092742  [  256/  306]
train() client id: f_00004-11-8 loss: 1.079885  [  288/  306]
train() client id: f_00004-12-0 loss: 1.060860  [   32/  306]
train() client id: f_00004-12-1 loss: 1.051685  [   64/  306]
train() client id: f_00004-12-2 loss: 1.036304  [   96/  306]
train() client id: f_00004-12-3 loss: 1.051153  [  128/  306]
train() client id: f_00004-12-4 loss: 1.004557  [  160/  306]
train() client id: f_00004-12-5 loss: 1.115379  [  192/  306]
train() client id: f_00004-12-6 loss: 1.071858  [  224/  306]
train() client id: f_00004-12-7 loss: 1.052787  [  256/  306]
train() client id: f_00004-12-8 loss: 0.925565  [  288/  306]
train() client id: f_00005-0-0 loss: 1.000870  [   32/  146]
train() client id: f_00005-0-1 loss: 1.020821  [   64/  146]
train() client id: f_00005-0-2 loss: 0.976672  [   96/  146]
train() client id: f_00005-0-3 loss: 0.944248  [  128/  146]
train() client id: f_00005-1-0 loss: 0.985185  [   32/  146]
train() client id: f_00005-1-1 loss: 0.931635  [   64/  146]
train() client id: f_00005-1-2 loss: 1.001937  [   96/  146]
train() client id: f_00005-1-3 loss: 0.940468  [  128/  146]
train() client id: f_00005-2-0 loss: 0.979561  [   32/  146]
train() client id: f_00005-2-1 loss: 0.928183  [   64/  146]
train() client id: f_00005-2-2 loss: 0.951918  [   96/  146]
train() client id: f_00005-2-3 loss: 0.901239  [  128/  146]
train() client id: f_00005-3-0 loss: 0.901491  [   32/  146]
train() client id: f_00005-3-1 loss: 0.958111  [   64/  146]
train() client id: f_00005-3-2 loss: 0.898930  [   96/  146]
train() client id: f_00005-3-3 loss: 0.987733  [  128/  146]
train() client id: f_00005-4-0 loss: 0.969221  [   32/  146]
train() client id: f_00005-4-1 loss: 0.967779  [   64/  146]
train() client id: f_00005-4-2 loss: 0.917336  [   96/  146]
train() client id: f_00005-4-3 loss: 0.806779  [  128/  146]
train() client id: f_00005-5-0 loss: 0.916878  [   32/  146]
train() client id: f_00005-5-1 loss: 0.799133  [   64/  146]
train() client id: f_00005-5-2 loss: 0.931577  [   96/  146]
train() client id: f_00005-5-3 loss: 1.012928  [  128/  146]
train() client id: f_00005-6-0 loss: 0.936733  [   32/  146]
train() client id: f_00005-6-1 loss: 0.930997  [   64/  146]
train() client id: f_00005-6-2 loss: 0.781937  [   96/  146]
train() client id: f_00005-6-3 loss: 0.977351  [  128/  146]
train() client id: f_00005-7-0 loss: 0.907531  [   32/  146]
train() client id: f_00005-7-1 loss: 0.843858  [   64/  146]
train() client id: f_00005-7-2 loss: 1.023800  [   96/  146]
train() client id: f_00005-7-3 loss: 0.834102  [  128/  146]
train() client id: f_00005-8-0 loss: 0.944101  [   32/  146]
train() client id: f_00005-8-1 loss: 0.857756  [   64/  146]
train() client id: f_00005-8-2 loss: 0.886066  [   96/  146]
train() client id: f_00005-8-3 loss: 0.915517  [  128/  146]
train() client id: f_00005-9-0 loss: 0.921968  [   32/  146]
train() client id: f_00005-9-1 loss: 0.898304  [   64/  146]
train() client id: f_00005-9-2 loss: 0.979888  [   96/  146]
train() client id: f_00005-9-3 loss: 0.739285  [  128/  146]
train() client id: f_00005-10-0 loss: 0.894593  [   32/  146]
train() client id: f_00005-10-1 loss: 0.818192  [   64/  146]
train() client id: f_00005-10-2 loss: 0.834173  [   96/  146]
train() client id: f_00005-10-3 loss: 0.974189  [  128/  146]
train() client id: f_00005-11-0 loss: 0.899020  [   32/  146]
train() client id: f_00005-11-1 loss: 0.912697  [   64/  146]
train() client id: f_00005-11-2 loss: 0.852754  [   96/  146]
train() client id: f_00005-11-3 loss: 0.858907  [  128/  146]
train() client id: f_00005-12-0 loss: 0.896949  [   32/  146]
train() client id: f_00005-12-1 loss: 0.804539  [   64/  146]
train() client id: f_00005-12-2 loss: 0.805252  [   96/  146]
train() client id: f_00005-12-3 loss: 1.115199  [  128/  146]
train() client id: f_00006-0-0 loss: 1.023914  [   32/   54]
train() client id: f_00006-1-0 loss: 0.995880  [   32/   54]
train() client id: f_00006-2-0 loss: 1.009310  [   32/   54]
train() client id: f_00006-3-0 loss: 1.002189  [   32/   54]
train() client id: f_00006-4-0 loss: 1.010427  [   32/   54]
train() client id: f_00006-5-0 loss: 1.042975  [   32/   54]
train() client id: f_00006-6-0 loss: 1.002032  [   32/   54]
train() client id: f_00006-7-0 loss: 0.997246  [   32/   54]
train() client id: f_00006-8-0 loss: 1.030714  [   32/   54]
train() client id: f_00006-9-0 loss: 1.018761  [   32/   54]
train() client id: f_00006-10-0 loss: 1.017811  [   32/   54]
train() client id: f_00006-11-0 loss: 1.012202  [   32/   54]
train() client id: f_00006-12-0 loss: 1.025278  [   32/   54]
train() client id: f_00007-0-0 loss: 0.986712  [   32/  179]
train() client id: f_00007-0-1 loss: 0.993661  [   64/  179]
train() client id: f_00007-0-2 loss: 0.941187  [   96/  179]
train() client id: f_00007-0-3 loss: 0.925734  [  128/  179]
train() client id: f_00007-0-4 loss: 0.872785  [  160/  179]
train() client id: f_00007-1-0 loss: 0.924591  [   32/  179]
train() client id: f_00007-1-1 loss: 0.917700  [   64/  179]
train() client id: f_00007-1-2 loss: 0.874910  [   96/  179]
train() client id: f_00007-1-3 loss: 0.823232  [  128/  179]
train() client id: f_00007-1-4 loss: 0.848176  [  160/  179]
train() client id: f_00007-2-0 loss: 0.867492  [   32/  179]
train() client id: f_00007-2-1 loss: 0.829168  [   64/  179]
train() client id: f_00007-2-2 loss: 0.817842  [   96/  179]
train() client id: f_00007-2-3 loss: 0.842868  [  128/  179]
train() client id: f_00007-2-4 loss: 0.825349  [  160/  179]
train() client id: f_00007-3-0 loss: 0.809774  [   32/  179]
train() client id: f_00007-3-1 loss: 0.788277  [   64/  179]
train() client id: f_00007-3-2 loss: 0.721331  [   96/  179]
train() client id: f_00007-3-3 loss: 0.818819  [  128/  179]
train() client id: f_00007-3-4 loss: 0.809728  [  160/  179]
train() client id: f_00007-4-0 loss: 0.828837  [   32/  179]
train() client id: f_00007-4-1 loss: 0.803054  [   64/  179]
train() client id: f_00007-4-2 loss: 0.722905  [   96/  179]
train() client id: f_00007-4-3 loss: 0.791943  [  128/  179]
train() client id: f_00007-4-4 loss: 0.742864  [  160/  179]
train() client id: f_00007-5-0 loss: 0.775750  [   32/  179]
train() client id: f_00007-5-1 loss: 0.822126  [   64/  179]
train() client id: f_00007-5-2 loss: 0.711989  [   96/  179]
train() client id: f_00007-5-3 loss: 0.787727  [  128/  179]
train() client id: f_00007-5-4 loss: 0.677323  [  160/  179]
train() client id: f_00007-6-0 loss: 0.648680  [   32/  179]
train() client id: f_00007-6-1 loss: 0.666854  [   64/  179]
train() client id: f_00007-6-2 loss: 0.825138  [   96/  179]
train() client id: f_00007-6-3 loss: 0.746739  [  128/  179]
train() client id: f_00007-6-4 loss: 0.809867  [  160/  179]
train() client id: f_00007-7-0 loss: 0.711039  [   32/  179]
train() client id: f_00007-7-1 loss: 0.793682  [   64/  179]
train() client id: f_00007-7-2 loss: 0.824925  [   96/  179]
train() client id: f_00007-7-3 loss: 0.627336  [  128/  179]
train() client id: f_00007-7-4 loss: 0.663895  [  160/  179]
train() client id: f_00007-8-0 loss: 0.792387  [   32/  179]
train() client id: f_00007-8-1 loss: 0.658833  [   64/  179]
train() client id: f_00007-8-2 loss: 0.718408  [   96/  179]
train() client id: f_00007-8-3 loss: 0.775139  [  128/  179]
train() client id: f_00007-8-4 loss: 0.664496  [  160/  179]
train() client id: f_00007-9-0 loss: 0.685850  [   32/  179]
train() client id: f_00007-9-1 loss: 0.655353  [   64/  179]
train() client id: f_00007-9-2 loss: 0.757782  [   96/  179]
train() client id: f_00007-9-3 loss: 0.752161  [  128/  179]
train() client id: f_00007-9-4 loss: 0.754567  [  160/  179]
train() client id: f_00007-10-0 loss: 0.623582  [   32/  179]
train() client id: f_00007-10-1 loss: 0.646065  [   64/  179]
train() client id: f_00007-10-2 loss: 0.726198  [   96/  179]
train() client id: f_00007-10-3 loss: 0.730094  [  128/  179]
train() client id: f_00007-10-4 loss: 0.853162  [  160/  179]
train() client id: f_00007-11-0 loss: 0.650564  [   32/  179]
train() client id: f_00007-11-1 loss: 0.611868  [   64/  179]
train() client id: f_00007-11-2 loss: 0.742859  [   96/  179]
train() client id: f_00007-11-3 loss: 0.698948  [  128/  179]
train() client id: f_00007-11-4 loss: 0.745081  [  160/  179]
train() client id: f_00007-12-0 loss: 0.787198  [   32/  179]
train() client id: f_00007-12-1 loss: 0.691761  [   64/  179]
train() client id: f_00007-12-2 loss: 0.820993  [   96/  179]
train() client id: f_00007-12-3 loss: 0.611198  [  128/  179]
train() client id: f_00007-12-4 loss: 0.582709  [  160/  179]
train() client id: f_00008-0-0 loss: 0.878575  [   32/  130]
train() client id: f_00008-0-1 loss: 0.870727  [   64/  130]
train() client id: f_00008-0-2 loss: 0.960178  [   96/  130]
train() client id: f_00008-0-3 loss: 0.787650  [  128/  130]
train() client id: f_00008-1-0 loss: 0.914956  [   32/  130]
train() client id: f_00008-1-1 loss: 0.879900  [   64/  130]
train() client id: f_00008-1-2 loss: 0.875090  [   96/  130]
train() client id: f_00008-1-3 loss: 0.857892  [  128/  130]
train() client id: f_00008-2-0 loss: 0.849241  [   32/  130]
train() client id: f_00008-2-1 loss: 0.931136  [   64/  130]
train() client id: f_00008-2-2 loss: 0.818691  [   96/  130]
train() client id: f_00008-2-3 loss: 0.906154  [  128/  130]
train() client id: f_00008-3-0 loss: 0.873026  [   32/  130]
train() client id: f_00008-3-1 loss: 0.917201  [   64/  130]
train() client id: f_00008-3-2 loss: 0.830591  [   96/  130]
train() client id: f_00008-3-3 loss: 0.853496  [  128/  130]
train() client id: f_00008-4-0 loss: 0.984349  [   32/  130]
train() client id: f_00008-4-1 loss: 0.848526  [   64/  130]
train() client id: f_00008-4-2 loss: 0.807109  [   96/  130]
train() client id: f_00008-4-3 loss: 0.819496  [  128/  130]
train() client id: f_00008-5-0 loss: 0.771931  [   32/  130]
train() client id: f_00008-5-1 loss: 0.769619  [   64/  130]
train() client id: f_00008-5-2 loss: 0.964641  [   96/  130]
train() client id: f_00008-5-3 loss: 0.918374  [  128/  130]
train() client id: f_00008-6-0 loss: 0.770825  [   32/  130]
train() client id: f_00008-6-1 loss: 0.815428  [   64/  130]
train() client id: f_00008-6-2 loss: 0.959281  [   96/  130]
train() client id: f_00008-6-3 loss: 0.856259  [  128/  130]
train() client id: f_00008-7-0 loss: 0.825763  [   32/  130]
train() client id: f_00008-7-1 loss: 0.948810  [   64/  130]
train() client id: f_00008-7-2 loss: 0.857797  [   96/  130]
train() client id: f_00008-7-3 loss: 0.781678  [  128/  130]
train() client id: f_00008-8-0 loss: 0.841564  [   32/  130]
train() client id: f_00008-8-1 loss: 0.859216  [   64/  130]
train() client id: f_00008-8-2 loss: 0.923408  [   96/  130]
train() client id: f_00008-8-3 loss: 0.783759  [  128/  130]
train() client id: f_00008-9-0 loss: 0.721662  [   32/  130]
train() client id: f_00008-9-1 loss: 0.829328  [   64/  130]
train() client id: f_00008-9-2 loss: 0.899806  [   96/  130]
train() client id: f_00008-9-3 loss: 0.950488  [  128/  130]
train() client id: f_00008-10-0 loss: 0.850075  [   32/  130]
train() client id: f_00008-10-1 loss: 0.809230  [   64/  130]
train() client id: f_00008-10-2 loss: 0.833939  [   96/  130]
train() client id: f_00008-10-3 loss: 0.897875  [  128/  130]
train() client id: f_00008-11-0 loss: 0.779217  [   32/  130]
train() client id: f_00008-11-1 loss: 0.958657  [   64/  130]
train() client id: f_00008-11-2 loss: 0.941121  [   96/  130]
train() client id: f_00008-11-3 loss: 0.679450  [  128/  130]
train() client id: f_00008-12-0 loss: 0.955608  [   32/  130]
train() client id: f_00008-12-1 loss: 0.742236  [   64/  130]
train() client id: f_00008-12-2 loss: 0.822934  [   96/  130]
train() client id: f_00008-12-3 loss: 0.855329  [  128/  130]
train() client id: f_00009-0-0 loss: 1.077972  [   32/  118]
train() client id: f_00009-0-1 loss: 1.105169  [   64/  118]
train() client id: f_00009-0-2 loss: 1.151914  [   96/  118]
train() client id: f_00009-1-0 loss: 1.185148  [   32/  118]
train() client id: f_00009-1-1 loss: 1.070652  [   64/  118]
train() client id: f_00009-1-2 loss: 1.021994  [   96/  118]
train() client id: f_00009-2-0 loss: 1.092637  [   32/  118]
train() client id: f_00009-2-1 loss: 1.005620  [   64/  118]
train() client id: f_00009-2-2 loss: 1.063937  [   96/  118]
train() client id: f_00009-3-0 loss: 1.056372  [   32/  118]
train() client id: f_00009-3-1 loss: 1.015484  [   64/  118]
train() client id: f_00009-3-2 loss: 0.974740  [   96/  118]
train() client id: f_00009-4-0 loss: 0.919250  [   32/  118]
train() client id: f_00009-4-1 loss: 1.099534  [   64/  118]
train() client id: f_00009-4-2 loss: 0.972990  [   96/  118]
train() client id: f_00009-5-0 loss: 0.966597  [   32/  118]
train() client id: f_00009-5-1 loss: 0.936116  [   64/  118]
train() client id: f_00009-5-2 loss: 1.054037  [   96/  118]
train() client id: f_00009-6-0 loss: 0.980590  [   32/  118]
train() client id: f_00009-6-1 loss: 0.977921  [   64/  118]
train() client id: f_00009-6-2 loss: 0.945005  [   96/  118]
train() client id: f_00009-7-0 loss: 0.955648  [   32/  118]
train() client id: f_00009-7-1 loss: 0.970437  [   64/  118]
train() client id: f_00009-7-2 loss: 0.945177  [   96/  118]
train() client id: f_00009-8-0 loss: 0.941132  [   32/  118]
train() client id: f_00009-8-1 loss: 0.940241  [   64/  118]
train() client id: f_00009-8-2 loss: 0.917893  [   96/  118]
train() client id: f_00009-9-0 loss: 0.859888  [   32/  118]
train() client id: f_00009-9-1 loss: 0.918130  [   64/  118]
train() client id: f_00009-9-2 loss: 1.018122  [   96/  118]
train() client id: f_00009-10-0 loss: 0.985576  [   32/  118]
train() client id: f_00009-10-1 loss: 0.904682  [   64/  118]
train() client id: f_00009-10-2 loss: 0.944866  [   96/  118]
train() client id: f_00009-11-0 loss: 0.870362  [   32/  118]
train() client id: f_00009-11-1 loss: 0.895730  [   64/  118]
train() client id: f_00009-11-2 loss: 1.008282  [   96/  118]
train() client id: f_00009-12-0 loss: 0.969660  [   32/  118]
train() client id: f_00009-12-1 loss: 0.903482  [   64/  118]
train() client id: f_00009-12-2 loss: 0.944530  [   96/  118]
At round 4 accuracy: 0.6153846153846154
At round 4 training accuracy: 0.5667337357478203
At round 4 training loss: 0.939052946055539
update_location
xs = [ -3.9056584    4.20031788  40.00902392  18.81129433 -14.02070377
  -1.04359014  -2.44319194  -6.32485185  24.66397685   2.93912145]
ys = [ 32.5879595   15.55583871   1.32061395  -2.45517586   9.35018685
 -17.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [105.24841696 101.28981582 107.71474373 101.78355802 101.41008889
 101.47138757 100.06427799 100.28687116 104.48436116 100.1231756 ]
dists_bs = [222.60831747 239.86495054 276.39357718 262.77967434 230.98526882
 259.22236244 247.64185241 246.08115854 254.2640783  246.78497559]
uav_gains = [8.79949509e-11 9.68464333e-11 8.30438549e-11 9.56761907e-11
 9.65595300e-11 9.64137637e-11 9.98391157e-11 9.92860273e-11
 8.96125391e-11 9.96923511e-11]
bs_gains = [2.95242043e-11 2.39544975e-11 1.61070436e-11 1.85539819e-11
 2.66230674e-11 1.92757456e-11 2.19071944e-11 2.22984497e-11
 2.03467974e-11 2.21208434e-11]
Round 5
-------------------------------
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.33284149 21.59560189 10.17342765  3.63728595 24.89615745 12.01211791
  4.52299688 14.602885   10.71870557  9.74625265]
obj_prev = 122.23827245410807
eta_min = 9.414027209246992e-10	eta_max = 0.9186608067299383
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 28.431400687067253	eta = 0.909090909090909
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 48.95841824133416	eta = 0.5279322499743717
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 39.18478015448109	eta = 0.6596114051281231
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.434409794184106	eta = 0.6904537306569067
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.34853700150222	eta = 0.6920412410342677
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.348315836191034	eta = 0.6920453390909805
eta = 0.6920453390909805
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [0.03037504 0.06388402 0.0298929  0.01036609 0.07376796 0.03519648
 0.01301788 0.04315185 0.03133933 0.02844647]
ene_total = [3.16534696 6.21953031 3.1298897  1.43707177 7.05111483 3.79361781
 1.66080998 4.25148272 3.45176733 3.18768443]
ti_comp = [0.28160493 0.26136622 0.28090865 0.2825856  0.2634578  0.25672527
 0.28307344 0.28301023 0.25792521 0.25972041]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20876875e-05 2.38538142e-04 2.11569937e-05 8.71815183e-07
 3.61460861e-04 4.13466617e-05 1.72068806e-06 6.27009232e-05
 2.89175573e-05 2.13281397e-05]
ene_total = [0.55639501 0.74895917 0.56229982 0.54614218 0.74154691 0.77190053
 0.54202196 0.54780676 0.76051829 0.74443548]
optimize_network_iter = 0 obj = 6.522026104246858
eta = 0.6920453390909805
freqs = [5.39320171e+07 1.22211702e+08 5.32075115e+07 1.83414978e+07
 1.39999582e+08 6.85489188e+07 2.29938135e+07 7.62372573e+07
 6.07527495e+07 5.47636387e+07]
eta_min = 0.6764484296472902	eta_max = 0.6920453390909633
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 0.06328991478194519	eta = 0.9090909090909091
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 21.92072085717903	eta = 0.0026247442563715508
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.335363763739976	eta = 0.024636969648472683
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.25597545648311	eta = 0.025503950408706712
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.255940548856704	eta = 0.025504345047817744
eta = 0.025504345047817744
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20148930e-04 2.37751991e-03 2.10872665e-04 8.68941936e-06
 3.60269593e-03 4.12103953e-04 1.71501718e-05 6.24942794e-04
 2.88222535e-04 2.12578485e-04]
ene_total = [0.1812087  0.29478227 0.18284802 0.17280114 0.32238    0.25400243
 0.17170585 0.1883868  0.24737814 0.2404472 ]
ti_comp = [0.29913458 0.27889587 0.2984383  0.30011526 0.28098745 0.27425492
 0.3006031  0.30053989 0.27545486 0.27725007]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.34412342e-05 2.50873933e-04 2.24469855e-05 9.25616981e-07
 3.80533165e-04 4.33862178e-05 1.82724368e-06 6.65820168e-05
 3.03620624e-05 2.24132480e-05]
ene_total = [0.52968454 0.71386454 0.53529951 0.51981962 0.70736071 0.73485767
 0.51590233 0.52171707 0.72397543 0.70863849]
optimize_network_iter = 1 obj = 6.211119913967213
eta = 0.6764484296472902
freqs = [5.39156214e+07 1.21622656e+08 5.31836109e+07 1.83396515e+07
 1.39394357e+08 6.81410966e+07 2.29938135e+07 7.62362643e+07
 6.04092728e+07 5.44779861e+07]
eta_min = 0.6764484296473243	eta_max = 0.6764484296472864
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 0.06277694578067981	eta = 0.909090909090909
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 21.92023194550156	eta = 0.002603528596394288
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.332999905489733	eta = 0.024462045872963308
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.2542173952325193	eta = 0.02531696846560011
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.2541832396466193	eta = 0.02531735207056884
eta = 0.02531735207056884
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20456386e-04 2.35937920e-03 2.11105834e-04 8.70509514e-06
 3.57877769e-03 4.08031791e-04 1.71845703e-05 6.26179946e-04
 2.85544288e-04 2.10788545e-04]
ene_total = [0.18118229 0.29423306 0.18281927 0.17276841 0.32166859 0.25384311
 0.17167384 0.18838425 0.24725795 0.24035246]
ti_comp = [0.29913458 0.27889587 0.2984383  0.30011526 0.28098745 0.27425492
 0.3006031  0.30053989 0.27545486 0.27725007]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.34412342e-05 2.50873933e-04 2.24469855e-05 9.25616981e-07
 3.80533165e-04 4.33862178e-05 1.82724368e-06 6.65820168e-05
 3.03620624e-05 2.24132480e-05]
ene_total = [0.52968454 0.71386454 0.53529951 0.51981962 0.70736071 0.73485767
 0.51590233 0.52171707 0.72397543 0.70863849]
optimize_network_iter = 2 obj = 6.2111199139678615
eta = 0.6764484296473243
freqs = [5.39156214e+07 1.21622656e+08 5.31836109e+07 1.83396515e+07
 1.39394357e+08 6.81410966e+07 2.29938135e+07 7.62362643e+07
 6.04092728e+07 5.44779861e+07]
Done!
At round 5 eta: 0.6764484296473243
At round 5 local rounds: 12.80002140651856
At round 5 global rounds: 81.81036980458869
At round 5 a_n: 26.12732777443847
gradient difference: 0.4239371418952942
train() client id: f_00000-0-0 loss: 1.251902  [   32/  126]
train() client id: f_00000-0-1 loss: 1.122722  [   64/  126]
train() client id: f_00000-0-2 loss: 1.233035  [   96/  126]
train() client id: f_00000-1-0 loss: 1.082313  [   32/  126]
train() client id: f_00000-1-1 loss: 1.081054  [   64/  126]
train() client id: f_00000-1-2 loss: 1.102321  [   96/  126]
train() client id: f_00000-2-0 loss: 1.082464  [   32/  126]
train() client id: f_00000-2-1 loss: 1.071098  [   64/  126]
train() client id: f_00000-2-2 loss: 1.061997  [   96/  126]
train() client id: f_00000-3-0 loss: 1.004570  [   32/  126]
train() client id: f_00000-3-1 loss: 0.981956  [   64/  126]
train() client id: f_00000-3-2 loss: 0.981241  [   96/  126]
train() client id: f_00000-4-0 loss: 0.966188  [   32/  126]
train() client id: f_00000-4-1 loss: 0.994734  [   64/  126]
train() client id: f_00000-4-2 loss: 0.943544  [   96/  126]
train() client id: f_00000-5-0 loss: 0.925092  [   32/  126]
train() client id: f_00000-5-1 loss: 0.921358  [   64/  126]
train() client id: f_00000-5-2 loss: 0.955929  [   96/  126]
train() client id: f_00000-6-0 loss: 0.921066  [   32/  126]
train() client id: f_00000-6-1 loss: 0.921010  [   64/  126]
train() client id: f_00000-6-2 loss: 0.903954  [   96/  126]
train() client id: f_00000-7-0 loss: 0.886890  [   32/  126]
train() client id: f_00000-7-1 loss: 0.904153  [   64/  126]
train() client id: f_00000-7-2 loss: 0.869062  [   96/  126]
train() client id: f_00000-8-0 loss: 0.928812  [   32/  126]
train() client id: f_00000-8-1 loss: 0.930416  [   64/  126]
train() client id: f_00000-8-2 loss: 0.864269  [   96/  126]
train() client id: f_00000-9-0 loss: 0.914397  [   32/  126]
train() client id: f_00000-9-1 loss: 0.882889  [   64/  126]
train() client id: f_00000-9-2 loss: 0.903246  [   96/  126]
train() client id: f_00000-10-0 loss: 0.891476  [   32/  126]
train() client id: f_00000-10-1 loss: 0.944086  [   64/  126]
train() client id: f_00000-10-2 loss: 0.911577  [   96/  126]
train() client id: f_00000-11-0 loss: 0.825651  [   32/  126]
train() client id: f_00000-11-1 loss: 0.950256  [   64/  126]
train() client id: f_00000-11-2 loss: 0.859233  [   96/  126]
train() client id: f_00001-0-0 loss: 0.585561  [   32/  265]
train() client id: f_00001-0-1 loss: 0.667235  [   64/  265]
train() client id: f_00001-0-2 loss: 0.596901  [   96/  265]
train() client id: f_00001-0-3 loss: 0.607789  [  128/  265]
train() client id: f_00001-0-4 loss: 0.752662  [  160/  265]
train() client id: f_00001-0-5 loss: 0.544020  [  192/  265]
train() client id: f_00001-0-6 loss: 0.714158  [  224/  265]
train() client id: f_00001-0-7 loss: 0.574439  [  256/  265]
train() client id: f_00001-1-0 loss: 0.595543  [   32/  265]
train() client id: f_00001-1-1 loss: 0.585164  [   64/  265]
train() client id: f_00001-1-2 loss: 0.597246  [   96/  265]
train() client id: f_00001-1-3 loss: 0.566209  [  128/  265]
train() client id: f_00001-1-4 loss: 0.572888  [  160/  265]
train() client id: f_00001-1-5 loss: 0.676804  [  192/  265]
train() client id: f_00001-1-6 loss: 0.627653  [  224/  265]
train() client id: f_00001-1-7 loss: 0.672361  [  256/  265]
train() client id: f_00001-2-0 loss: 0.666168  [   32/  265]
train() client id: f_00001-2-1 loss: 0.524962  [   64/  265]
train() client id: f_00001-2-2 loss: 0.571734  [   96/  265]
train() client id: f_00001-2-3 loss: 0.586970  [  128/  265]
train() client id: f_00001-2-4 loss: 0.513603  [  160/  265]
train() client id: f_00001-2-5 loss: 0.579217  [  192/  265]
train() client id: f_00001-2-6 loss: 0.628905  [  224/  265]
train() client id: f_00001-2-7 loss: 0.599337  [  256/  265]
train() client id: f_00001-3-0 loss: 0.579836  [   32/  265]
train() client id: f_00001-3-1 loss: 0.564216  [   64/  265]
train() client id: f_00001-3-2 loss: 0.644541  [   96/  265]
train() client id: f_00001-3-3 loss: 0.542789  [  128/  265]
train() client id: f_00001-3-4 loss: 0.502988  [  160/  265]
train() client id: f_00001-3-5 loss: 0.598514  [  192/  265]
train() client id: f_00001-3-6 loss: 0.661231  [  224/  265]
train() client id: f_00001-3-7 loss: 0.570905  [  256/  265]
train() client id: f_00001-4-0 loss: 0.531039  [   32/  265]
train() client id: f_00001-4-1 loss: 0.689199  [   64/  265]
train() client id: f_00001-4-2 loss: 0.505756  [   96/  265]
train() client id: f_00001-4-3 loss: 0.647156  [  128/  265]
train() client id: f_00001-4-4 loss: 0.495285  [  160/  265]
train() client id: f_00001-4-5 loss: 0.608546  [  192/  265]
train() client id: f_00001-4-6 loss: 0.574967  [  224/  265]
train() client id: f_00001-4-7 loss: 0.503879  [  256/  265]
train() client id: f_00001-5-0 loss: 0.504009  [   32/  265]
train() client id: f_00001-5-1 loss: 0.568622  [   64/  265]
train() client id: f_00001-5-2 loss: 0.548386  [   96/  265]
train() client id: f_00001-5-3 loss: 0.514059  [  128/  265]
train() client id: f_00001-5-4 loss: 0.534445  [  160/  265]
train() client id: f_00001-5-5 loss: 0.681578  [  192/  265]
train() client id: f_00001-5-6 loss: 0.595264  [  224/  265]
train() client id: f_00001-5-7 loss: 0.547171  [  256/  265]
train() client id: f_00001-6-0 loss: 0.482891  [   32/  265]
train() client id: f_00001-6-1 loss: 0.603422  [   64/  265]
train() client id: f_00001-6-2 loss: 0.478767  [   96/  265]
train() client id: f_00001-6-3 loss: 0.492007  [  128/  265]
train() client id: f_00001-6-4 loss: 0.545666  [  160/  265]
train() client id: f_00001-6-5 loss: 0.643596  [  192/  265]
train() client id: f_00001-6-6 loss: 0.584958  [  224/  265]
train() client id: f_00001-6-7 loss: 0.686292  [  256/  265]
train() client id: f_00001-7-0 loss: 0.470768  [   32/  265]
train() client id: f_00001-7-1 loss: 0.518938  [   64/  265]
train() client id: f_00001-7-2 loss: 0.530920  [   96/  265]
train() client id: f_00001-7-3 loss: 0.558274  [  128/  265]
train() client id: f_00001-7-4 loss: 0.618590  [  160/  265]
train() client id: f_00001-7-5 loss: 0.640826  [  192/  265]
train() client id: f_00001-7-6 loss: 0.511825  [  224/  265]
train() client id: f_00001-7-7 loss: 0.633736  [  256/  265]
train() client id: f_00001-8-0 loss: 0.600094  [   32/  265]
train() client id: f_00001-8-1 loss: 0.647537  [   64/  265]
train() client id: f_00001-8-2 loss: 0.529688  [   96/  265]
train() client id: f_00001-8-3 loss: 0.538595  [  128/  265]
train() client id: f_00001-8-4 loss: 0.577958  [  160/  265]
train() client id: f_00001-8-5 loss: 0.486922  [  192/  265]
train() client id: f_00001-8-6 loss: 0.575046  [  224/  265]
train() client id: f_00001-8-7 loss: 0.520479  [  256/  265]
train() client id: f_00001-9-0 loss: 0.561265  [   32/  265]
train() client id: f_00001-9-1 loss: 0.728504  [   64/  265]
train() client id: f_00001-9-2 loss: 0.516173  [   96/  265]
train() client id: f_00001-9-3 loss: 0.525322  [  128/  265]
train() client id: f_00001-9-4 loss: 0.677368  [  160/  265]
train() client id: f_00001-9-5 loss: 0.471025  [  192/  265]
train() client id: f_00001-9-6 loss: 0.462484  [  224/  265]
train() client id: f_00001-9-7 loss: 0.515566  [  256/  265]
train() client id: f_00001-10-0 loss: 0.475654  [   32/  265]
train() client id: f_00001-10-1 loss: 0.456402  [   64/  265]
train() client id: f_00001-10-2 loss: 0.632521  [   96/  265]
train() client id: f_00001-10-3 loss: 0.574268  [  128/  265]
train() client id: f_00001-10-4 loss: 0.548932  [  160/  265]
train() client id: f_00001-10-5 loss: 0.482166  [  192/  265]
train() client id: f_00001-10-6 loss: 0.698571  [  224/  265]
train() client id: f_00001-10-7 loss: 0.526337  [  256/  265]
train() client id: f_00001-11-0 loss: 0.665805  [   32/  265]
train() client id: f_00001-11-1 loss: 0.538010  [   64/  265]
train() client id: f_00001-11-2 loss: 0.458470  [   96/  265]
train() client id: f_00001-11-3 loss: 0.527406  [  128/  265]
train() client id: f_00001-11-4 loss: 0.649520  [  160/  265]
train() client id: f_00001-11-5 loss: 0.515633  [  192/  265]
train() client id: f_00001-11-6 loss: 0.545748  [  224/  265]
train() client id: f_00001-11-7 loss: 0.514130  [  256/  265]
train() client id: f_00002-0-0 loss: 1.242087  [   32/  124]
train() client id: f_00002-0-1 loss: 1.175345  [   64/  124]
train() client id: f_00002-0-2 loss: 1.238209  [   96/  124]
train() client id: f_00002-1-0 loss: 1.115333  [   32/  124]
train() client id: f_00002-1-1 loss: 1.211336  [   64/  124]
train() client id: f_00002-1-2 loss: 1.197947  [   96/  124]
train() client id: f_00002-2-0 loss: 1.124830  [   32/  124]
train() client id: f_00002-2-1 loss: 1.156277  [   64/  124]
train() client id: f_00002-2-2 loss: 1.140173  [   96/  124]
train() client id: f_00002-3-0 loss: 1.081743  [   32/  124]
train() client id: f_00002-3-1 loss: 1.179895  [   64/  124]
train() client id: f_00002-3-2 loss: 1.093047  [   96/  124]
train() client id: f_00002-4-0 loss: 1.098642  [   32/  124]
train() client id: f_00002-4-1 loss: 1.108122  [   64/  124]
train() client id: f_00002-4-2 loss: 1.095944  [   96/  124]
train() client id: f_00002-5-0 loss: 1.050147  [   32/  124]
train() client id: f_00002-5-1 loss: 1.052234  [   64/  124]
train() client id: f_00002-5-2 loss: 1.094881  [   96/  124]
train() client id: f_00002-6-0 loss: 1.017591  [   32/  124]
train() client id: f_00002-6-1 loss: 1.119028  [   64/  124]
train() client id: f_00002-6-2 loss: 1.066643  [   96/  124]
train() client id: f_00002-7-0 loss: 1.016411  [   32/  124]
train() client id: f_00002-7-1 loss: 1.092439  [   64/  124]
train() client id: f_00002-7-2 loss: 1.060315  [   96/  124]
train() client id: f_00002-8-0 loss: 0.953459  [   32/  124]
train() client id: f_00002-8-1 loss: 0.987602  [   64/  124]
train() client id: f_00002-8-2 loss: 1.112627  [   96/  124]
train() client id: f_00002-9-0 loss: 1.087933  [   32/  124]
train() client id: f_00002-9-1 loss: 1.058340  [   64/  124]
train() client id: f_00002-9-2 loss: 0.950099  [   96/  124]
train() client id: f_00002-10-0 loss: 0.983103  [   32/  124]
train() client id: f_00002-10-1 loss: 0.979388  [   64/  124]
train() client id: f_00002-10-2 loss: 1.039376  [   96/  124]
train() client id: f_00002-11-0 loss: 1.023598  [   32/  124]
train() client id: f_00002-11-1 loss: 0.969104  [   64/  124]
train() client id: f_00002-11-2 loss: 1.094695  [   96/  124]
train() client id: f_00003-0-0 loss: 1.054150  [   32/   43]
train() client id: f_00003-1-0 loss: 1.042059  [   32/   43]
train() client id: f_00003-2-0 loss: 1.016371  [   32/   43]
train() client id: f_00003-3-0 loss: 1.054321  [   32/   43]
train() client id: f_00003-4-0 loss: 1.090999  [   32/   43]
train() client id: f_00003-5-0 loss: 1.059950  [   32/   43]
train() client id: f_00003-6-0 loss: 1.060264  [   32/   43]
train() client id: f_00003-7-0 loss: 1.084704  [   32/   43]
train() client id: f_00003-8-0 loss: 1.053101  [   32/   43]
train() client id: f_00003-9-0 loss: 1.018247  [   32/   43]
train() client id: f_00003-10-0 loss: 1.011030  [   32/   43]
train() client id: f_00003-11-0 loss: 1.028603  [   32/   43]
train() client id: f_00004-0-0 loss: 1.047908  [   32/  306]
train() client id: f_00004-0-1 loss: 1.116844  [   64/  306]
train() client id: f_00004-0-2 loss: 1.111833  [   96/  306]
train() client id: f_00004-0-3 loss: 1.019475  [  128/  306]
train() client id: f_00004-0-4 loss: 1.068509  [  160/  306]
train() client id: f_00004-0-5 loss: 0.973632  [  192/  306]
train() client id: f_00004-0-6 loss: 1.014733  [  224/  306]
train() client id: f_00004-0-7 loss: 1.112354  [  256/  306]
train() client id: f_00004-0-8 loss: 1.128421  [  288/  306]
train() client id: f_00004-1-0 loss: 1.114224  [   32/  306]
train() client id: f_00004-1-1 loss: 1.011938  [   64/  306]
train() client id: f_00004-1-2 loss: 1.013196  [   96/  306]
train() client id: f_00004-1-3 loss: 1.114721  [  128/  306]
train() client id: f_00004-1-4 loss: 1.044719  [  160/  306]
train() client id: f_00004-1-5 loss: 1.000008  [  192/  306]
train() client id: f_00004-1-6 loss: 1.083703  [  224/  306]
train() client id: f_00004-1-7 loss: 1.008237  [  256/  306]
train() client id: f_00004-1-8 loss: 1.165198  [  288/  306]
train() client id: f_00004-2-0 loss: 1.019609  [   32/  306]
train() client id: f_00004-2-1 loss: 1.073133  [   64/  306]
train() client id: f_00004-2-2 loss: 1.117863  [   96/  306]
train() client id: f_00004-2-3 loss: 1.046670  [  128/  306]
train() client id: f_00004-2-4 loss: 1.080807  [  160/  306]
train() client id: f_00004-2-5 loss: 0.998150  [  192/  306]
train() client id: f_00004-2-6 loss: 1.100471  [  224/  306]
train() client id: f_00004-2-7 loss: 1.117131  [  256/  306]
train() client id: f_00004-2-8 loss: 1.060987  [  288/  306]
train() client id: f_00004-3-0 loss: 1.111836  [   32/  306]
train() client id: f_00004-3-1 loss: 1.028020  [   64/  306]
train() client id: f_00004-3-2 loss: 1.094676  [   96/  306]
train() client id: f_00004-3-3 loss: 1.012246  [  128/  306]
train() client id: f_00004-3-4 loss: 1.102604  [  160/  306]
train() client id: f_00004-3-5 loss: 1.058187  [  192/  306]
train() client id: f_00004-3-6 loss: 1.067412  [  224/  306]
train() client id: f_00004-3-7 loss: 1.076085  [  256/  306]
train() client id: f_00004-3-8 loss: 1.041518  [  288/  306]
train() client id: f_00004-4-0 loss: 1.098169  [   32/  306]
train() client id: f_00004-4-1 loss: 1.124724  [   64/  306]
train() client id: f_00004-4-2 loss: 1.112960  [   96/  306]
train() client id: f_00004-4-3 loss: 1.038829  [  128/  306]
train() client id: f_00004-4-4 loss: 0.967854  [  160/  306]
train() client id: f_00004-4-5 loss: 1.058508  [  192/  306]
train() client id: f_00004-4-6 loss: 1.071734  [  224/  306]
train() client id: f_00004-4-7 loss: 1.023703  [  256/  306]
train() client id: f_00004-4-8 loss: 1.048457  [  288/  306]
train() client id: f_00004-5-0 loss: 1.141632  [   32/  306]
train() client id: f_00004-5-1 loss: 1.115909  [   64/  306]
train() client id: f_00004-5-2 loss: 1.063740  [   96/  306]
train() client id: f_00004-5-3 loss: 1.007820  [  128/  306]
train() client id: f_00004-5-4 loss: 0.993318  [  160/  306]
train() client id: f_00004-5-5 loss: 1.085103  [  192/  306]
train() client id: f_00004-5-6 loss: 1.082720  [  224/  306]
train() client id: f_00004-5-7 loss: 1.036661  [  256/  306]
train() client id: f_00004-5-8 loss: 1.039231  [  288/  306]
train() client id: f_00004-6-0 loss: 0.955385  [   32/  306]
train() client id: f_00004-6-1 loss: 1.054960  [   64/  306]
train() client id: f_00004-6-2 loss: 0.986252  [   96/  306]
train() client id: f_00004-6-3 loss: 0.984315  [  128/  306]
train() client id: f_00004-6-4 loss: 1.030269  [  160/  306]
train() client id: f_00004-6-5 loss: 1.204491  [  192/  306]
train() client id: f_00004-6-6 loss: 1.094513  [  224/  306]
train() client id: f_00004-6-7 loss: 1.176340  [  256/  306]
train() client id: f_00004-6-8 loss: 1.107210  [  288/  306]
train() client id: f_00004-7-0 loss: 1.145304  [   32/  306]
train() client id: f_00004-7-1 loss: 1.028387  [   64/  306]
train() client id: f_00004-7-2 loss: 1.040871  [   96/  306]
train() client id: f_00004-7-3 loss: 0.964523  [  128/  306]
train() client id: f_00004-7-4 loss: 1.096169  [  160/  306]
train() client id: f_00004-7-5 loss: 1.052533  [  192/  306]
train() client id: f_00004-7-6 loss: 1.042730  [  224/  306]
train() client id: f_00004-7-7 loss: 1.135794  [  256/  306]
train() client id: f_00004-7-8 loss: 1.063763  [  288/  306]
train() client id: f_00004-8-0 loss: 1.093877  [   32/  306]
train() client id: f_00004-8-1 loss: 1.098480  [   64/  306]
train() client id: f_00004-8-2 loss: 1.091949  [   96/  306]
train() client id: f_00004-8-3 loss: 1.099337  [  128/  306]
train() client id: f_00004-8-4 loss: 0.935330  [  160/  306]
train() client id: f_00004-8-5 loss: 1.123321  [  192/  306]
train() client id: f_00004-8-6 loss: 1.008912  [  224/  306]
train() client id: f_00004-8-7 loss: 1.104332  [  256/  306]
train() client id: f_00004-8-8 loss: 1.052234  [  288/  306]
train() client id: f_00004-9-0 loss: 1.068041  [   32/  306]
train() client id: f_00004-9-1 loss: 1.115163  [   64/  306]
train() client id: f_00004-9-2 loss: 1.042889  [   96/  306]
train() client id: f_00004-9-3 loss: 1.057635  [  128/  306]
train() client id: f_00004-9-4 loss: 1.106937  [  160/  306]
train() client id: f_00004-9-5 loss: 1.029873  [  192/  306]
train() client id: f_00004-9-6 loss: 1.016222  [  224/  306]
train() client id: f_00004-9-7 loss: 1.084555  [  256/  306]
train() client id: f_00004-9-8 loss: 1.153301  [  288/  306]
train() client id: f_00004-10-0 loss: 1.012429  [   32/  306]
train() client id: f_00004-10-1 loss: 1.080755  [   64/  306]
train() client id: f_00004-10-2 loss: 1.075231  [   96/  306]
train() client id: f_00004-10-3 loss: 1.148508  [  128/  306]
train() client id: f_00004-10-4 loss: 1.065818  [  160/  306]
train() client id: f_00004-10-5 loss: 1.113914  [  192/  306]
train() client id: f_00004-10-6 loss: 1.050294  [  224/  306]
train() client id: f_00004-10-7 loss: 1.064837  [  256/  306]
train() client id: f_00004-10-8 loss: 1.063647  [  288/  306]
train() client id: f_00004-11-0 loss: 1.036651  [   32/  306]
train() client id: f_00004-11-1 loss: 1.063433  [   64/  306]
train() client id: f_00004-11-2 loss: 1.070639  [   96/  306]
train() client id: f_00004-11-3 loss: 1.121844  [  128/  306]
train() client id: f_00004-11-4 loss: 1.190200  [  160/  306]
train() client id: f_00004-11-5 loss: 0.978300  [  192/  306]
train() client id: f_00004-11-6 loss: 1.059114  [  224/  306]
train() client id: f_00004-11-7 loss: 1.005303  [  256/  306]
train() client id: f_00004-11-8 loss: 1.100313  [  288/  306]
train() client id: f_00005-0-0 loss: 0.887849  [   32/  146]
train() client id: f_00005-0-1 loss: 0.844113  [   64/  146]
train() client id: f_00005-0-2 loss: 0.867036  [   96/  146]
train() client id: f_00005-0-3 loss: 0.821547  [  128/  146]
train() client id: f_00005-1-0 loss: 0.849480  [   32/  146]
train() client id: f_00005-1-1 loss: 0.802206  [   64/  146]
train() client id: f_00005-1-2 loss: 0.819102  [   96/  146]
train() client id: f_00005-1-3 loss: 0.738076  [  128/  146]
train() client id: f_00005-2-0 loss: 0.765770  [   32/  146]
train() client id: f_00005-2-1 loss: 0.892058  [   64/  146]
train() client id: f_00005-2-2 loss: 0.732455  [   96/  146]
train() client id: f_00005-2-3 loss: 0.730494  [  128/  146]
train() client id: f_00005-3-0 loss: 0.599231  [   32/  146]
train() client id: f_00005-3-1 loss: 0.832654  [   64/  146]
train() client id: f_00005-3-2 loss: 0.698440  [   96/  146]
train() client id: f_00005-3-3 loss: 0.732378  [  128/  146]
train() client id: f_00005-4-0 loss: 0.789127  [   32/  146]
train() client id: f_00005-4-1 loss: 0.832233  [   64/  146]
train() client id: f_00005-4-2 loss: 0.668798  [   96/  146]
train() client id: f_00005-4-3 loss: 0.625666  [  128/  146]
train() client id: f_00005-5-0 loss: 0.718884  [   32/  146]
train() client id: f_00005-5-1 loss: 0.605511  [   64/  146]
train() client id: f_00005-5-2 loss: 0.730229  [   96/  146]
train() client id: f_00005-5-3 loss: 0.874662  [  128/  146]
train() client id: f_00005-6-0 loss: 0.842782  [   32/  146]
train() client id: f_00005-6-1 loss: 0.657136  [   64/  146]
train() client id: f_00005-6-2 loss: 0.726154  [   96/  146]
train() client id: f_00005-6-3 loss: 0.555102  [  128/  146]
train() client id: f_00005-7-0 loss: 0.817575  [   32/  146]
train() client id: f_00005-7-1 loss: 0.654165  [   64/  146]
train() client id: f_00005-7-2 loss: 0.787544  [   96/  146]
train() client id: f_00005-7-3 loss: 0.555224  [  128/  146]
train() client id: f_00005-8-0 loss: 0.618921  [   32/  146]
train() client id: f_00005-8-1 loss: 0.700207  [   64/  146]
train() client id: f_00005-8-2 loss: 0.774529  [   96/  146]
train() client id: f_00005-8-3 loss: 0.613691  [  128/  146]
train() client id: f_00005-9-0 loss: 0.717624  [   32/  146]
train() client id: f_00005-9-1 loss: 0.555626  [   64/  146]
train() client id: f_00005-9-2 loss: 0.675598  [   96/  146]
train() client id: f_00005-9-3 loss: 0.690978  [  128/  146]
train() client id: f_00005-10-0 loss: 0.575216  [   32/  146]
train() client id: f_00005-10-1 loss: 0.724228  [   64/  146]
train() client id: f_00005-10-2 loss: 0.724543  [   96/  146]
train() client id: f_00005-10-3 loss: 0.607741  [  128/  146]
train() client id: f_00005-11-0 loss: 0.676840  [   32/  146]
train() client id: f_00005-11-1 loss: 0.602764  [   64/  146]
train() client id: f_00005-11-2 loss: 0.702706  [   96/  146]
train() client id: f_00005-11-3 loss: 0.568518  [  128/  146]
train() client id: f_00006-0-0 loss: 0.939391  [   32/   54]
train() client id: f_00006-1-0 loss: 0.942293  [   32/   54]
train() client id: f_00006-2-0 loss: 0.965387  [   32/   54]
train() client id: f_00006-3-0 loss: 0.931612  [   32/   54]
train() client id: f_00006-4-0 loss: 0.942888  [   32/   54]
train() client id: f_00006-5-0 loss: 0.943079  [   32/   54]
train() client id: f_00006-6-0 loss: 0.958374  [   32/   54]
train() client id: f_00006-7-0 loss: 0.942076  [   32/   54]
train() client id: f_00006-8-0 loss: 0.924428  [   32/   54]
train() client id: f_00006-9-0 loss: 0.946944  [   32/   54]
train() client id: f_00006-10-0 loss: 0.920062  [   32/   54]
train() client id: f_00006-11-0 loss: 0.929814  [   32/   54]
train() client id: f_00007-0-0 loss: 1.025823  [   32/  179]
train() client id: f_00007-0-1 loss: 0.991546  [   64/  179]
train() client id: f_00007-0-2 loss: 0.969356  [   96/  179]
train() client id: f_00007-0-3 loss: 0.903914  [  128/  179]
train() client id: f_00007-0-4 loss: 0.893499  [  160/  179]
train() client id: f_00007-1-0 loss: 0.951575  [   32/  179]
train() client id: f_00007-1-1 loss: 0.901224  [   64/  179]
train() client id: f_00007-1-2 loss: 0.873801  [   96/  179]
train() client id: f_00007-1-3 loss: 0.942874  [  128/  179]
train() client id: f_00007-1-4 loss: 0.878778  [  160/  179]
train() client id: f_00007-2-0 loss: 0.857213  [   32/  179]
train() client id: f_00007-2-1 loss: 0.915413  [   64/  179]
train() client id: f_00007-2-2 loss: 0.863895  [   96/  179]
train() client id: f_00007-2-3 loss: 0.921254  [  128/  179]
train() client id: f_00007-2-4 loss: 0.900937  [  160/  179]
train() client id: f_00007-3-0 loss: 0.801722  [   32/  179]
train() client id: f_00007-3-1 loss: 0.843559  [   64/  179]
train() client id: f_00007-3-2 loss: 0.936226  [   96/  179]
train() client id: f_00007-3-3 loss: 0.781806  [  128/  179]
train() client id: f_00007-3-4 loss: 0.945151  [  160/  179]
train() client id: f_00007-4-0 loss: 0.854815  [   32/  179]
train() client id: f_00007-4-1 loss: 0.854305  [   64/  179]
train() client id: f_00007-4-2 loss: 0.800735  [   96/  179]
train() client id: f_00007-4-3 loss: 0.818211  [  128/  179]
train() client id: f_00007-4-4 loss: 0.877042  [  160/  179]
train() client id: f_00007-5-0 loss: 0.776272  [   32/  179]
train() client id: f_00007-5-1 loss: 0.837843  [   64/  179]
train() client id: f_00007-5-2 loss: 0.738381  [   96/  179]
train() client id: f_00007-5-3 loss: 0.953794  [  128/  179]
train() client id: f_00007-5-4 loss: 0.810456  [  160/  179]
train() client id: f_00007-6-0 loss: 0.778668  [   32/  179]
train() client id: f_00007-6-1 loss: 0.824865  [   64/  179]
train() client id: f_00007-6-2 loss: 0.802438  [   96/  179]
train() client id: f_00007-6-3 loss: 0.869971  [  128/  179]
train() client id: f_00007-6-4 loss: 0.832965  [  160/  179]
train() client id: f_00007-7-0 loss: 0.790550  [   32/  179]
train() client id: f_00007-7-1 loss: 0.781608  [   64/  179]
train() client id: f_00007-7-2 loss: 0.916477  [   96/  179]
train() client id: f_00007-7-3 loss: 0.795030  [  128/  179]
train() client id: f_00007-7-4 loss: 0.755057  [  160/  179]
train() client id: f_00007-8-0 loss: 0.849680  [   32/  179]
train() client id: f_00007-8-1 loss: 0.899036  [   64/  179]
train() client id: f_00007-8-2 loss: 0.860435  [   96/  179]
train() client id: f_00007-8-3 loss: 0.715217  [  128/  179]
train() client id: f_00007-8-4 loss: 0.746033  [  160/  179]
train() client id: f_00007-9-0 loss: 0.841793  [   32/  179]
train() client id: f_00007-9-1 loss: 0.766005  [   64/  179]
train() client id: f_00007-9-2 loss: 0.762415  [   96/  179]
train() client id: f_00007-9-3 loss: 0.761008  [  128/  179]
train() client id: f_00007-9-4 loss: 0.826052  [  160/  179]
train() client id: f_00007-10-0 loss: 0.821692  [   32/  179]
train() client id: f_00007-10-1 loss: 0.823375  [   64/  179]
train() client id: f_00007-10-2 loss: 0.712242  [   96/  179]
train() client id: f_00007-10-3 loss: 0.841490  [  128/  179]
train() client id: f_00007-10-4 loss: 0.853234  [  160/  179]
train() client id: f_00007-11-0 loss: 0.709353  [   32/  179]
train() client id: f_00007-11-1 loss: 0.958937  [   64/  179]
train() client id: f_00007-11-2 loss: 0.711222  [   96/  179]
train() client id: f_00007-11-3 loss: 0.959857  [  128/  179]
train() client id: f_00007-11-4 loss: 0.723935  [  160/  179]
train() client id: f_00008-0-0 loss: 0.882858  [   32/  130]
train() client id: f_00008-0-1 loss: 0.947507  [   64/  130]
train() client id: f_00008-0-2 loss: 0.850267  [   96/  130]
train() client id: f_00008-0-3 loss: 0.878512  [  128/  130]
train() client id: f_00008-1-0 loss: 0.818938  [   32/  130]
train() client id: f_00008-1-1 loss: 0.977762  [   64/  130]
train() client id: f_00008-1-2 loss: 0.823650  [   96/  130]
train() client id: f_00008-1-3 loss: 0.917286  [  128/  130]
train() client id: f_00008-2-0 loss: 0.919694  [   32/  130]
train() client id: f_00008-2-1 loss: 0.859817  [   64/  130]
train() client id: f_00008-2-2 loss: 0.871345  [   96/  130]
train() client id: f_00008-2-3 loss: 0.851278  [  128/  130]
train() client id: f_00008-3-0 loss: 0.963686  [   32/  130]
train() client id: f_00008-3-1 loss: 0.848706  [   64/  130]
train() client id: f_00008-3-2 loss: 0.903806  [   96/  130]
train() client id: f_00008-3-3 loss: 0.768926  [  128/  130]
train() client id: f_00008-4-0 loss: 0.943577  [   32/  130]
train() client id: f_00008-4-1 loss: 0.832598  [   64/  130]
train() client id: f_00008-4-2 loss: 0.856699  [   96/  130]
train() client id: f_00008-4-3 loss: 0.886416  [  128/  130]
train() client id: f_00008-5-0 loss: 1.008847  [   32/  130]
train() client id: f_00008-5-1 loss: 0.829475  [   64/  130]
train() client id: f_00008-5-2 loss: 0.760529  [   96/  130]
train() client id: f_00008-5-3 loss: 0.922625  [  128/  130]
train() client id: f_00008-6-0 loss: 0.889116  [   32/  130]
train() client id: f_00008-6-1 loss: 0.949169  [   64/  130]
train() client id: f_00008-6-2 loss: 0.839046  [   96/  130]
train() client id: f_00008-6-3 loss: 0.825535  [  128/  130]
train() client id: f_00008-7-0 loss: 0.864213  [   32/  130]
train() client id: f_00008-7-1 loss: 0.950372  [   64/  130]
train() client id: f_00008-7-2 loss: 0.766283  [   96/  130]
train() client id: f_00008-7-3 loss: 0.891503  [  128/  130]
train() client id: f_00008-8-0 loss: 0.809047  [   32/  130]
train() client id: f_00008-8-1 loss: 0.976953  [   64/  130]
train() client id: f_00008-8-2 loss: 0.823711  [   96/  130]
train() client id: f_00008-8-3 loss: 0.852587  [  128/  130]
train() client id: f_00008-9-0 loss: 0.791353  [   32/  130]
train() client id: f_00008-9-1 loss: 1.054034  [   64/  130]
train() client id: f_00008-9-2 loss: 0.809143  [   96/  130]
train() client id: f_00008-9-3 loss: 0.848083  [  128/  130]
train() client id: f_00008-10-0 loss: 0.794977  [   32/  130]
train() client id: f_00008-10-1 loss: 0.934249  [   64/  130]
train() client id: f_00008-10-2 loss: 0.897523  [   96/  130]
train() client id: f_00008-10-3 loss: 0.884170  [  128/  130]
train() client id: f_00008-11-0 loss: 0.843146  [   32/  130]
train() client id: f_00008-11-1 loss: 0.870009  [   64/  130]
train() client id: f_00008-11-2 loss: 0.897621  [   96/  130]
train() client id: f_00008-11-3 loss: 0.871615  [  128/  130]
train() client id: f_00009-0-0 loss: 1.160087  [   32/  118]
train() client id: f_00009-0-1 loss: 1.197017  [   64/  118]
train() client id: f_00009-0-2 loss: 1.226547  [   96/  118]
train() client id: f_00009-1-0 loss: 1.159560  [   32/  118]
train() client id: f_00009-1-1 loss: 1.137192  [   64/  118]
train() client id: f_00009-1-2 loss: 1.107757  [   96/  118]
train() client id: f_00009-2-0 loss: 1.178047  [   32/  118]
train() client id: f_00009-2-1 loss: 1.078744  [   64/  118]
train() client id: f_00009-2-2 loss: 1.125501  [   96/  118]
train() client id: f_00009-3-0 loss: 1.047246  [   32/  118]
train() client id: f_00009-3-1 loss: 1.089418  [   64/  118]
train() client id: f_00009-3-2 loss: 1.078793  [   96/  118]
train() client id: f_00009-4-0 loss: 1.050888  [   32/  118]
train() client id: f_00009-4-1 loss: 1.046558  [   64/  118]
train() client id: f_00009-4-2 loss: 1.024073  [   96/  118]
train() client id: f_00009-5-0 loss: 1.049678  [   32/  118]
train() client id: f_00009-5-1 loss: 0.958898  [   64/  118]
train() client id: f_00009-5-2 loss: 1.038873  [   96/  118]
train() client id: f_00009-6-0 loss: 1.041521  [   32/  118]
train() client id: f_00009-6-1 loss: 1.003970  [   64/  118]
train() client id: f_00009-6-2 loss: 0.997267  [   96/  118]
train() client id: f_00009-7-0 loss: 1.020568  [   32/  118]
train() client id: f_00009-7-1 loss: 1.015649  [   64/  118]
train() client id: f_00009-7-2 loss: 0.984593  [   96/  118]
train() client id: f_00009-8-0 loss: 0.992957  [   32/  118]
train() client id: f_00009-8-1 loss: 0.948768  [   64/  118]
train() client id: f_00009-8-2 loss: 0.982494  [   96/  118]
train() client id: f_00009-9-0 loss: 0.880719  [   32/  118]
train() client id: f_00009-9-1 loss: 0.937660  [   64/  118]
train() client id: f_00009-9-2 loss: 1.019257  [   96/  118]
train() client id: f_00009-10-0 loss: 0.939897  [   32/  118]
train() client id: f_00009-10-1 loss: 0.897721  [   64/  118]
train() client id: f_00009-10-2 loss: 1.081805  [   96/  118]
train() client id: f_00009-11-0 loss: 0.994428  [   32/  118]
train() client id: f_00009-11-1 loss: 0.889146  [   64/  118]
train() client id: f_00009-11-2 loss: 0.971831  [   96/  118]
At round 5 accuracy: 0.623342175066313
At round 5 training accuracy: 0.568075117370892
At round 5 training loss: 0.919882986060284
update_location
xs = [-3.9056584   4.20031788 45.00902392 18.81129433 -9.02070377  3.95640986
 -7.44319194 -1.32485185 29.66397685 -2.06087855]
ys = [ 37.5879595   20.55583871   1.32061395  -7.45517586   9.35018685
 -17.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [106.90233331 102.17722435 109.6702159  102.02668495 100.84046356
 101.54312677 100.31097472 100.09599397 105.7762804  100.10124413]
dists_bs = [219.44325599 236.57082002 280.3007309  266.18172178 234.49730782
 262.60387904 244.18419069 249.53494358 258.20894888 243.20450851]
uav_gains = [8.46306925e-11 9.47572782e-11 7.93911850e-11 9.51072087e-11
 9.79289548e-11 9.62435611e-11 9.92263936e-11 9.97600464e-11
 8.69011873e-11 9.97469659e-11]
bs_gains = [3.07320703e-11 2.49001977e-11 1.54862491e-11 1.78976100e-11
 2.55216104e-11 1.85887806e-11 2.27868853e-11 2.14450097e-11
 1.94883232e-11 2.30448319e-11]
Round 6
-------------------------------
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.20035717 21.31407924 10.04322819  3.59039783 24.57615325 11.85884139
  4.4646498  14.41376646 10.58251293  9.61787463]
obj_prev = 120.66186087163511
eta_min = 7.409354424179371e-10	eta_max = 0.91885879376386
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 28.063470776703085	eta = 0.909090909090909
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 48.384419986505044	eta = 0.5272822567213747
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 38.70245321762515	eta = 0.6591893805073021
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.96801720495558	eta = 0.6901167032896544
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.88278882583632	eta = 0.6917114180576249
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.88256856080987	eta = 0.6917155490018552
eta = 0.6917155490018552
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [0.03041438 0.06396674 0.02993161 0.01037951 0.07386349 0.03524206
 0.01303473 0.04320773 0.03137991 0.0284833 ]
ene_total = [3.12833392 6.13288912 3.09403261 1.41867893 6.96750532 3.75135634
 1.63958615 4.19625148 3.41497814 3.13895655]
ti_comp = [0.28546116 0.26646804 0.2846808  0.28683999 0.2669565  0.2602256
 0.28732669 0.28738774 0.26129447 0.26489704]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.15785733e-05 2.30384101e-04 2.06801576e-05 8.49436889e-07
 3.53417801e-04 4.03983548e-05 1.67661934e-06 6.10418604e-05
 2.82862087e-05 2.05824434e-05]
ene_total = [0.55252256 0.73118701 0.55906002 0.53908008 0.73747448 0.76799073
 0.53502532 0.53953919 0.7579054  0.72672045]
optimize_network_iter = 0 obj = 6.446505230599096
eta = 0.6917155490018552
freqs = [5.32723555e+07 1.20027048e+08 5.25704736e+07 1.80928566e+07
 1.38343675e+08 6.77144291e+07 2.26827740e+07 7.51732237e+07
 6.00470262e+07 5.37629723e+07]
eta_min = 0.6801612059033121	eta_max = 0.6917155490018475
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 0.06073437862890649	eta = 0.9090909090909091
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 21.69496157536947	eta = 0.002544972079761971
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.3026899172665374	eta = 0.023977640700475235
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.226369694175787	eta = 0.024799597131268097
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.2263378919337997	eta = 0.024799951382431865
eta = 0.024799951382431865
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.16436107e-04 2.31078474e-03 2.07424872e-04 8.51997075e-06
 3.54482996e-03 4.05201146e-04 1.68167264e-05 6.12258394e-04
 2.83714628e-04 2.06444785e-04]
ene_total = [0.17988639 0.28688359 0.18173569 0.17062183 0.31863717 0.25255426
 0.16954014 0.18532952 0.24643571 0.23471359]
ti_comp = [0.29859544 0.27960232 0.29781508 0.29997427 0.28009078 0.27335988
 0.30046097 0.30052202 0.27442876 0.27803133]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.25514581e-05 2.39268435e-04 2.16073258e-05 8.88109865e-07
 3.67109805e-04 4.18618440e-05 1.75321215e-06 6.38316142e-05
 2.93224432e-05 2.13642617e-05]
ene_total = [0.53264186 0.70549825 0.53893943 0.51960869 0.71195131 0.74036622
 0.5157035  0.52027595 0.73061032 0.70053117]
optimize_network_iter = 1 obj = 6.216126697450631
eta = 0.6801612059033121
freqs = [5.32566467e+07 1.19616625e+08 5.25486356e+07 1.80913467e+07
 1.37882470e+08 6.74069266e+07 2.26825634e+07 7.51732237e+07
 5.97860936e+07 5.35642116e+07]
eta_min = 0.6801612059033234	eta_max = 0.680161205903311
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 0.06036643683170785	eta = 0.909090909090909
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 21.694610889380957	eta = 0.0025295949864110306
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.300984648578189	eta = 0.023850041316801688
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.2251002879015918	eta = 0.024663418200206223
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.225068999906738	eta = 0.02466376500693525
eta = 0.02466376500693525
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.16630671e-04 2.29842707e-03 2.07561279e-04 8.53123712e-06
 3.52647900e-03 4.02127407e-04 1.68414620e-05 6.13170349e-04
 2.81673164e-04 2.05226390e-04]
ene_total = [0.17986649 0.2865125  0.18171397 0.17059831 0.31810109 0.25243665
 0.16951713 0.18532807 0.24634661 0.23464818]
ti_comp = [0.29859544 0.27960232 0.29781508 0.29997427 0.28009078 0.27335988
 0.30046097 0.30052202 0.27442876 0.27803133]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.25514581e-05 2.39268435e-04 2.16073258e-05 8.88109865e-07
 3.67109805e-04 4.18618440e-05 1.75321215e-06 6.38316142e-05
 2.93224432e-05 2.13642617e-05]
ene_total = [0.53264186 0.70549825 0.53893943 0.51960869 0.71195131 0.74036622
 0.5157035  0.52027595 0.73061032 0.70053117]
optimize_network_iter = 2 obj = 6.216126697450847
eta = 0.6801612059033234
freqs = [5.32566467e+07 1.19616625e+08 5.25486356e+07 1.80913467e+07
 1.37882470e+08 6.74069266e+07 2.26825634e+07 7.51732237e+07
 5.97860936e+07 5.35642116e+07]
Done!
At round 6 eta: 0.6801612059033234
At round 6 local rounds: 12.620787147614399
At round 6 global rounds: 81.68905166188517
At round 6 a_n: 25.784781927469155
gradient difference: 0.44851502776145935
train() client id: f_00000-0-0 loss: 1.385311  [   32/  126]
train() client id: f_00000-0-1 loss: 1.178715  [   64/  126]
train() client id: f_00000-0-2 loss: 1.191897  [   96/  126]
train() client id: f_00000-1-0 loss: 1.144251  [   32/  126]
train() client id: f_00000-1-1 loss: 1.135344  [   64/  126]
train() client id: f_00000-1-2 loss: 1.091116  [   96/  126]
train() client id: f_00000-2-0 loss: 1.091332  [   32/  126]
train() client id: f_00000-2-1 loss: 1.145362  [   64/  126]
train() client id: f_00000-2-2 loss: 1.050187  [   96/  126]
train() client id: f_00000-3-0 loss: 0.998722  [   32/  126]
train() client id: f_00000-3-1 loss: 1.012761  [   64/  126]
train() client id: f_00000-3-2 loss: 1.078692  [   96/  126]
train() client id: f_00000-4-0 loss: 1.003211  [   32/  126]
train() client id: f_00000-4-1 loss: 0.995054  [   64/  126]
train() client id: f_00000-4-2 loss: 1.002203  [   96/  126]
train() client id: f_00000-5-0 loss: 1.031858  [   32/  126]
train() client id: f_00000-5-1 loss: 0.974550  [   64/  126]
train() client id: f_00000-5-2 loss: 0.939777  [   96/  126]
train() client id: f_00000-6-0 loss: 0.959654  [   32/  126]
train() client id: f_00000-6-1 loss: 0.956051  [   64/  126]
train() client id: f_00000-6-2 loss: 0.971289  [   96/  126]
train() client id: f_00000-7-0 loss: 0.967410  [   32/  126]
train() client id: f_00000-7-1 loss: 0.993281  [   64/  126]
train() client id: f_00000-7-2 loss: 0.866901  [   96/  126]
train() client id: f_00000-8-0 loss: 0.938228  [   32/  126]
train() client id: f_00000-8-1 loss: 0.980224  [   64/  126]
train() client id: f_00000-8-2 loss: 0.932980  [   96/  126]
train() client id: f_00000-9-0 loss: 0.891755  [   32/  126]
train() client id: f_00000-9-1 loss: 0.955151  [   64/  126]
train() client id: f_00000-9-2 loss: 0.945577  [   96/  126]
train() client id: f_00000-10-0 loss: 0.932766  [   32/  126]
train() client id: f_00000-10-1 loss: 0.905185  [   64/  126]
train() client id: f_00000-10-2 loss: 0.962936  [   96/  126]
train() client id: f_00000-11-0 loss: 0.854152  [   32/  126]
train() client id: f_00000-11-1 loss: 0.946660  [   64/  126]
train() client id: f_00000-11-2 loss: 1.000480  [   96/  126]
train() client id: f_00001-0-0 loss: 0.552543  [   32/  265]
train() client id: f_00001-0-1 loss: 0.569964  [   64/  265]
train() client id: f_00001-0-2 loss: 0.686996  [   96/  265]
train() client id: f_00001-0-3 loss: 0.594214  [  128/  265]
train() client id: f_00001-0-4 loss: 0.708785  [  160/  265]
train() client id: f_00001-0-5 loss: 0.636754  [  192/  265]
train() client id: f_00001-0-6 loss: 0.679512  [  224/  265]
train() client id: f_00001-0-7 loss: 0.537939  [  256/  265]
train() client id: f_00001-1-0 loss: 0.611007  [   32/  265]
train() client id: f_00001-1-1 loss: 0.551779  [   64/  265]
train() client id: f_00001-1-2 loss: 0.556211  [   96/  265]
train() client id: f_00001-1-3 loss: 0.687036  [  128/  265]
train() client id: f_00001-1-4 loss: 0.706725  [  160/  265]
train() client id: f_00001-1-5 loss: 0.543880  [  192/  265]
train() client id: f_00001-1-6 loss: 0.564474  [  224/  265]
train() client id: f_00001-1-7 loss: 0.581443  [  256/  265]
train() client id: f_00001-2-0 loss: 0.575949  [   32/  265]
train() client id: f_00001-2-1 loss: 0.564275  [   64/  265]
train() client id: f_00001-2-2 loss: 0.549470  [   96/  265]
train() client id: f_00001-2-3 loss: 0.626787  [  128/  265]
train() client id: f_00001-2-4 loss: 0.576855  [  160/  265]
train() client id: f_00001-2-5 loss: 0.563287  [  192/  265]
train() client id: f_00001-2-6 loss: 0.667281  [  224/  265]
train() client id: f_00001-2-7 loss: 0.579076  [  256/  265]
train() client id: f_00001-3-0 loss: 0.590949  [   32/  265]
train() client id: f_00001-3-1 loss: 0.605357  [   64/  265]
train() client id: f_00001-3-2 loss: 0.558056  [   96/  265]
train() client id: f_00001-3-3 loss: 0.607179  [  128/  265]
train() client id: f_00001-3-4 loss: 0.620439  [  160/  265]
train() client id: f_00001-3-5 loss: 0.602743  [  192/  265]
train() client id: f_00001-3-6 loss: 0.574890  [  224/  265]
train() client id: f_00001-3-7 loss: 0.484939  [  256/  265]
train() client id: f_00001-4-0 loss: 0.544244  [   32/  265]
train() client id: f_00001-4-1 loss: 0.533367  [   64/  265]
train() client id: f_00001-4-2 loss: 0.628603  [   96/  265]
train() client id: f_00001-4-3 loss: 0.585618  [  128/  265]
train() client id: f_00001-4-4 loss: 0.610642  [  160/  265]
train() client id: f_00001-4-5 loss: 0.612277  [  192/  265]
train() client id: f_00001-4-6 loss: 0.510560  [  224/  265]
train() client id: f_00001-4-7 loss: 0.522611  [  256/  265]
train() client id: f_00001-5-0 loss: 0.619909  [   32/  265]
train() client id: f_00001-5-1 loss: 0.585752  [   64/  265]
train() client id: f_00001-5-2 loss: 0.556328  [   96/  265]
train() client id: f_00001-5-3 loss: 0.609760  [  128/  265]
train() client id: f_00001-5-4 loss: 0.542546  [  160/  265]
train() client id: f_00001-5-5 loss: 0.532274  [  192/  265]
train() client id: f_00001-5-6 loss: 0.538023  [  224/  265]
train() client id: f_00001-5-7 loss: 0.568938  [  256/  265]
train() client id: f_00001-6-0 loss: 0.621579  [   32/  265]
train() client id: f_00001-6-1 loss: 0.508616  [   64/  265]
train() client id: f_00001-6-2 loss: 0.482541  [   96/  265]
train() client id: f_00001-6-3 loss: 0.679574  [  128/  265]
train() client id: f_00001-6-4 loss: 0.666166  [  160/  265]
train() client id: f_00001-6-5 loss: 0.496152  [  192/  265]
train() client id: f_00001-6-6 loss: 0.515310  [  224/  265]
train() client id: f_00001-6-7 loss: 0.485566  [  256/  265]
train() client id: f_00001-7-0 loss: 0.548253  [   32/  265]
train() client id: f_00001-7-1 loss: 0.527056  [   64/  265]
train() client id: f_00001-7-2 loss: 0.590904  [   96/  265]
train() client id: f_00001-7-3 loss: 0.580290  [  128/  265]
train() client id: f_00001-7-4 loss: 0.606475  [  160/  265]
train() client id: f_00001-7-5 loss: 0.564515  [  192/  265]
train() client id: f_00001-7-6 loss: 0.583686  [  224/  265]
train() client id: f_00001-7-7 loss: 0.524792  [  256/  265]
train() client id: f_00001-8-0 loss: 0.700639  [   32/  265]
train() client id: f_00001-8-1 loss: 0.585124  [   64/  265]
train() client id: f_00001-8-2 loss: 0.499701  [   96/  265]
train() client id: f_00001-8-3 loss: 0.501797  [  128/  265]
train() client id: f_00001-8-4 loss: 0.641448  [  160/  265]
train() client id: f_00001-8-5 loss: 0.529859  [  192/  265]
train() client id: f_00001-8-6 loss: 0.526536  [  224/  265]
train() client id: f_00001-8-7 loss: 0.478118  [  256/  265]
train() client id: f_00001-9-0 loss: 0.507866  [   32/  265]
train() client id: f_00001-9-1 loss: 0.526639  [   64/  265]
train() client id: f_00001-9-2 loss: 0.557210  [   96/  265]
train() client id: f_00001-9-3 loss: 0.582090  [  128/  265]
train() client id: f_00001-9-4 loss: 0.574811  [  160/  265]
train() client id: f_00001-9-5 loss: 0.631126  [  192/  265]
train() client id: f_00001-9-6 loss: 0.467842  [  224/  265]
train() client id: f_00001-9-7 loss: 0.572773  [  256/  265]
train() client id: f_00001-10-0 loss: 0.520617  [   32/  265]
train() client id: f_00001-10-1 loss: 0.672643  [   64/  265]
train() client id: f_00001-10-2 loss: 0.602135  [   96/  265]
train() client id: f_00001-10-3 loss: 0.478668  [  128/  265]
train() client id: f_00001-10-4 loss: 0.578212  [  160/  265]
train() client id: f_00001-10-5 loss: 0.559677  [  192/  265]
train() client id: f_00001-10-6 loss: 0.555222  [  224/  265]
train() client id: f_00001-10-7 loss: 0.490391  [  256/  265]
train() client id: f_00001-11-0 loss: 0.749091  [   32/  265]
train() client id: f_00001-11-1 loss: 0.577421  [   64/  265]
train() client id: f_00001-11-2 loss: 0.530167  [   96/  265]
train() client id: f_00001-11-3 loss: 0.477967  [  128/  265]
train() client id: f_00001-11-4 loss: 0.618106  [  160/  265]
train() client id: f_00001-11-5 loss: 0.486120  [  192/  265]
train() client id: f_00001-11-6 loss: 0.471509  [  224/  265]
train() client id: f_00001-11-7 loss: 0.597603  [  256/  265]
train() client id: f_00002-0-0 loss: 1.161458  [   32/  124]
train() client id: f_00002-0-1 loss: 1.195780  [   64/  124]
train() client id: f_00002-0-2 loss: 1.210952  [   96/  124]
train() client id: f_00002-1-0 loss: 1.160171  [   32/  124]
train() client id: f_00002-1-1 loss: 1.153934  [   64/  124]
train() client id: f_00002-1-2 loss: 1.142275  [   96/  124]
train() client id: f_00002-2-0 loss: 1.134857  [   32/  124]
train() client id: f_00002-2-1 loss: 1.114858  [   64/  124]
train() client id: f_00002-2-2 loss: 1.092657  [   96/  124]
train() client id: f_00002-3-0 loss: 1.076969  [   32/  124]
train() client id: f_00002-3-1 loss: 1.079925  [   64/  124]
train() client id: f_00002-3-2 loss: 1.059134  [   96/  124]
train() client id: f_00002-4-0 loss: 1.062502  [   32/  124]
train() client id: f_00002-4-1 loss: 1.043442  [   64/  124]
train() client id: f_00002-4-2 loss: 1.029986  [   96/  124]
train() client id: f_00002-5-0 loss: 0.977102  [   32/  124]
train() client id: f_00002-5-1 loss: 1.074357  [   64/  124]
train() client id: f_00002-5-2 loss: 0.988699  [   96/  124]
train() client id: f_00002-6-0 loss: 1.057516  [   32/  124]
train() client id: f_00002-6-1 loss: 1.029363  [   64/  124]
train() client id: f_00002-6-2 loss: 0.933389  [   96/  124]
train() client id: f_00002-7-0 loss: 0.962684  [   32/  124]
train() client id: f_00002-7-1 loss: 0.973535  [   64/  124]
train() client id: f_00002-7-2 loss: 1.034869  [   96/  124]
train() client id: f_00002-8-0 loss: 0.956051  [   32/  124]
train() client id: f_00002-8-1 loss: 0.997056  [   64/  124]
train() client id: f_00002-8-2 loss: 0.977540  [   96/  124]
train() client id: f_00002-9-0 loss: 0.985019  [   32/  124]
train() client id: f_00002-9-1 loss: 0.933467  [   64/  124]
train() client id: f_00002-9-2 loss: 0.924574  [   96/  124]
train() client id: f_00002-10-0 loss: 0.974234  [   32/  124]
train() client id: f_00002-10-1 loss: 0.844865  [   64/  124]
train() client id: f_00002-10-2 loss: 0.973413  [   96/  124]
train() client id: f_00002-11-0 loss: 0.949983  [   32/  124]
train() client id: f_00002-11-1 loss: 1.013889  [   64/  124]
train() client id: f_00002-11-2 loss: 0.925143  [   96/  124]
train() client id: f_00003-0-0 loss: 1.044809  [   32/   43]
train() client id: f_00003-1-0 loss: 1.021822  [   32/   43]
train() client id: f_00003-2-0 loss: 1.068835  [   32/   43]
train() client id: f_00003-3-0 loss: 1.012697  [   32/   43]
train() client id: f_00003-4-0 loss: 0.985329  [   32/   43]
train() client id: f_00003-5-0 loss: 1.031420  [   32/   43]
train() client id: f_00003-6-0 loss: 1.067399  [   32/   43]
train() client id: f_00003-7-0 loss: 1.041609  [   32/   43]
train() client id: f_00003-8-0 loss: 1.084237  [   32/   43]
train() client id: f_00003-9-0 loss: 1.075224  [   32/   43]
train() client id: f_00003-10-0 loss: 1.128266  [   32/   43]
train() client id: f_00003-11-0 loss: 0.981825  [   32/   43]
train() client id: f_00004-0-0 loss: 0.942698  [   32/  306]
train() client id: f_00004-0-1 loss: 0.920741  [   64/  306]
train() client id: f_00004-0-2 loss: 0.944605  [   96/  306]
train() client id: f_00004-0-3 loss: 0.858701  [  128/  306]
train() client id: f_00004-0-4 loss: 0.923873  [  160/  306]
train() client id: f_00004-0-5 loss: 0.968134  [  192/  306]
train() client id: f_00004-0-6 loss: 1.026113  [  224/  306]
train() client id: f_00004-0-7 loss: 0.858400  [  256/  306]
train() client id: f_00004-0-8 loss: 0.971837  [  288/  306]
train() client id: f_00004-1-0 loss: 0.904725  [   32/  306]
train() client id: f_00004-1-1 loss: 0.934105  [   64/  306]
train() client id: f_00004-1-2 loss: 0.936613  [   96/  306]
train() client id: f_00004-1-3 loss: 0.872833  [  128/  306]
train() client id: f_00004-1-4 loss: 0.897743  [  160/  306]
train() client id: f_00004-1-5 loss: 0.929429  [  192/  306]
train() client id: f_00004-1-6 loss: 0.901305  [  224/  306]
train() client id: f_00004-1-7 loss: 0.951850  [  256/  306]
train() client id: f_00004-1-8 loss: 1.014103  [  288/  306]
train() client id: f_00004-2-0 loss: 0.836621  [   32/  306]
train() client id: f_00004-2-1 loss: 0.945233  [   64/  306]
train() client id: f_00004-2-2 loss: 0.998253  [   96/  306]
train() client id: f_00004-2-3 loss: 0.900798  [  128/  306]
train() client id: f_00004-2-4 loss: 0.944659  [  160/  306]
train() client id: f_00004-2-5 loss: 0.936107  [  192/  306]
train() client id: f_00004-2-6 loss: 0.874224  [  224/  306]
train() client id: f_00004-2-7 loss: 0.943813  [  256/  306]
train() client id: f_00004-2-8 loss: 0.993898  [  288/  306]
train() client id: f_00004-3-0 loss: 0.810820  [   32/  306]
train() client id: f_00004-3-1 loss: 0.892696  [   64/  306]
train() client id: f_00004-3-2 loss: 0.869690  [   96/  306]
train() client id: f_00004-3-3 loss: 0.975823  [  128/  306]
train() client id: f_00004-3-4 loss: 0.987648  [  160/  306]
train() client id: f_00004-3-5 loss: 0.884825  [  192/  306]
train() client id: f_00004-3-6 loss: 0.903804  [  224/  306]
train() client id: f_00004-3-7 loss: 0.939835  [  256/  306]
train() client id: f_00004-3-8 loss: 0.973679  [  288/  306]
train() client id: f_00004-4-0 loss: 0.992437  [   32/  306]
train() client id: f_00004-4-1 loss: 0.946088  [   64/  306]
train() client id: f_00004-4-2 loss: 0.906102  [   96/  306]
train() client id: f_00004-4-3 loss: 0.912702  [  128/  306]
train() client id: f_00004-4-4 loss: 0.840297  [  160/  306]
train() client id: f_00004-4-5 loss: 0.880589  [  192/  306]
train() client id: f_00004-4-6 loss: 0.932736  [  224/  306]
train() client id: f_00004-4-7 loss: 0.937358  [  256/  306]
train() client id: f_00004-4-8 loss: 0.931217  [  288/  306]
train() client id: f_00004-5-0 loss: 1.004757  [   32/  306]
train() client id: f_00004-5-1 loss: 0.958636  [   64/  306]
train() client id: f_00004-5-2 loss: 0.945027  [   96/  306]
train() client id: f_00004-5-3 loss: 1.035593  [  128/  306]
train() client id: f_00004-5-4 loss: 0.835613  [  160/  306]
train() client id: f_00004-5-5 loss: 0.945530  [  192/  306]
train() client id: f_00004-5-6 loss: 0.822621  [  224/  306]
train() client id: f_00004-5-7 loss: 0.917851  [  256/  306]
train() client id: f_00004-5-8 loss: 0.908372  [  288/  306]
train() client id: f_00004-6-0 loss: 0.950918  [   32/  306]
train() client id: f_00004-6-1 loss: 0.917684  [   64/  306]
train() client id: f_00004-6-2 loss: 0.868463  [   96/  306]
train() client id: f_00004-6-3 loss: 0.966677  [  128/  306]
train() client id: f_00004-6-4 loss: 0.893088  [  160/  306]
train() client id: f_00004-6-5 loss: 0.865997  [  192/  306]
train() client id: f_00004-6-6 loss: 0.954423  [  224/  306]
train() client id: f_00004-6-7 loss: 0.921259  [  256/  306]
train() client id: f_00004-6-8 loss: 0.896855  [  288/  306]
train() client id: f_00004-7-0 loss: 1.057943  [   32/  306]
train() client id: f_00004-7-1 loss: 0.902632  [   64/  306]
train() client id: f_00004-7-2 loss: 0.856546  [   96/  306]
train() client id: f_00004-7-3 loss: 0.900051  [  128/  306]
train() client id: f_00004-7-4 loss: 0.919845  [  160/  306]
train() client id: f_00004-7-5 loss: 0.894974  [  192/  306]
train() client id: f_00004-7-6 loss: 0.989115  [  224/  306]
train() client id: f_00004-7-7 loss: 0.926428  [  256/  306]
train() client id: f_00004-7-8 loss: 0.843764  [  288/  306]
train() client id: f_00004-8-0 loss: 0.864074  [   32/  306]
train() client id: f_00004-8-1 loss: 0.995468  [   64/  306]
train() client id: f_00004-8-2 loss: 0.928845  [   96/  306]
train() client id: f_00004-8-3 loss: 0.878514  [  128/  306]
train() client id: f_00004-8-4 loss: 0.886027  [  160/  306]
train() client id: f_00004-8-5 loss: 0.911495  [  192/  306]
train() client id: f_00004-8-6 loss: 0.828988  [  224/  306]
train() client id: f_00004-8-7 loss: 0.876956  [  256/  306]
train() client id: f_00004-8-8 loss: 0.985124  [  288/  306]
train() client id: f_00004-9-0 loss: 0.873113  [   32/  306]
train() client id: f_00004-9-1 loss: 0.906554  [   64/  306]
train() client id: f_00004-9-2 loss: 0.967406  [   96/  306]
train() client id: f_00004-9-3 loss: 0.826825  [  128/  306]
train() client id: f_00004-9-4 loss: 1.003954  [  160/  306]
train() client id: f_00004-9-5 loss: 0.929326  [  192/  306]
train() client id: f_00004-9-6 loss: 0.889154  [  224/  306]
train() client id: f_00004-9-7 loss: 0.871254  [  256/  306]
train() client id: f_00004-9-8 loss: 0.891961  [  288/  306]
train() client id: f_00004-10-0 loss: 0.813542  [   32/  306]
train() client id: f_00004-10-1 loss: 0.928202  [   64/  306]
train() client id: f_00004-10-2 loss: 0.924150  [   96/  306]
train() client id: f_00004-10-3 loss: 0.928522  [  128/  306]
train() client id: f_00004-10-4 loss: 0.895768  [  160/  306]
train() client id: f_00004-10-5 loss: 0.859486  [  192/  306]
train() client id: f_00004-10-6 loss: 0.867945  [  224/  306]
train() client id: f_00004-10-7 loss: 0.970204  [  256/  306]
train() client id: f_00004-10-8 loss: 0.992302  [  288/  306]
train() client id: f_00004-11-0 loss: 0.907514  [   32/  306]
train() client id: f_00004-11-1 loss: 0.885868  [   64/  306]
train() client id: f_00004-11-2 loss: 0.871804  [   96/  306]
train() client id: f_00004-11-3 loss: 0.872061  [  128/  306]
train() client id: f_00004-11-4 loss: 0.920860  [  160/  306]
train() client id: f_00004-11-5 loss: 0.919937  [  192/  306]
train() client id: f_00004-11-6 loss: 0.991964  [  224/  306]
train() client id: f_00004-11-7 loss: 0.896006  [  256/  306]
train() client id: f_00004-11-8 loss: 0.897829  [  288/  306]
train() client id: f_00005-0-0 loss: 0.947995  [   32/  146]
train() client id: f_00005-0-1 loss: 1.019685  [   64/  146]
train() client id: f_00005-0-2 loss: 0.901064  [   96/  146]
train() client id: f_00005-0-3 loss: 0.927402  [  128/  146]
train() client id: f_00005-1-0 loss: 0.938841  [   32/  146]
train() client id: f_00005-1-1 loss: 0.954568  [   64/  146]
train() client id: f_00005-1-2 loss: 1.060040  [   96/  146]
train() client id: f_00005-1-3 loss: 0.968488  [  128/  146]
train() client id: f_00005-2-0 loss: 1.014677  [   32/  146]
train() client id: f_00005-2-1 loss: 0.927820  [   64/  146]
train() client id: f_00005-2-2 loss: 0.953479  [   96/  146]
train() client id: f_00005-2-3 loss: 0.881562  [  128/  146]
train() client id: f_00005-3-0 loss: 0.894438  [   32/  146]
train() client id: f_00005-3-1 loss: 0.966777  [   64/  146]
train() client id: f_00005-3-2 loss: 1.006877  [   96/  146]
train() client id: f_00005-3-3 loss: 0.941014  [  128/  146]
train() client id: f_00005-4-0 loss: 1.031958  [   32/  146]
train() client id: f_00005-4-1 loss: 0.902138  [   64/  146]
train() client id: f_00005-4-2 loss: 0.915372  [   96/  146]
train() client id: f_00005-4-3 loss: 0.911934  [  128/  146]
train() client id: f_00005-5-0 loss: 1.094477  [   32/  146]
train() client id: f_00005-5-1 loss: 0.947292  [   64/  146]
train() client id: f_00005-5-2 loss: 0.806147  [   96/  146]
train() client id: f_00005-5-3 loss: 0.968989  [  128/  146]
train() client id: f_00005-6-0 loss: 0.844349  [   32/  146]
train() client id: f_00005-6-1 loss: 0.978689  [   64/  146]
train() client id: f_00005-6-2 loss: 0.820181  [   96/  146]
train() client id: f_00005-6-3 loss: 1.096876  [  128/  146]
train() client id: f_00005-7-0 loss: 1.083057  [   32/  146]
train() client id: f_00005-7-1 loss: 0.884812  [   64/  146]
train() client id: f_00005-7-2 loss: 0.914296  [   96/  146]
train() client id: f_00005-7-3 loss: 0.921369  [  128/  146]
train() client id: f_00005-8-0 loss: 0.808896  [   32/  146]
train() client id: f_00005-8-1 loss: 1.015580  [   64/  146]
train() client id: f_00005-8-2 loss: 0.940185  [   96/  146]
train() client id: f_00005-8-3 loss: 0.909059  [  128/  146]
train() client id: f_00005-9-0 loss: 1.018448  [   32/  146]
train() client id: f_00005-9-1 loss: 1.071360  [   64/  146]
train() client id: f_00005-9-2 loss: 0.808429  [   96/  146]
train() client id: f_00005-9-3 loss: 0.898378  [  128/  146]
train() client id: f_00005-10-0 loss: 1.002450  [   32/  146]
train() client id: f_00005-10-1 loss: 0.870871  [   64/  146]
train() client id: f_00005-10-2 loss: 0.973056  [   96/  146]
train() client id: f_00005-10-3 loss: 0.931522  [  128/  146]
train() client id: f_00005-11-0 loss: 1.089051  [   32/  146]
train() client id: f_00005-11-1 loss: 0.999754  [   64/  146]
train() client id: f_00005-11-2 loss: 0.779707  [   96/  146]
train() client id: f_00005-11-3 loss: 0.978320  [  128/  146]
train() client id: f_00006-0-0 loss: 0.927614  [   32/   54]
train() client id: f_00006-1-0 loss: 0.927224  [   32/   54]
train() client id: f_00006-2-0 loss: 0.923931  [   32/   54]
train() client id: f_00006-3-0 loss: 0.899347  [   32/   54]
train() client id: f_00006-4-0 loss: 0.928185  [   32/   54]
train() client id: f_00006-5-0 loss: 0.892136  [   32/   54]
train() client id: f_00006-6-0 loss: 0.936931  [   32/   54]
train() client id: f_00006-7-0 loss: 0.931534  [   32/   54]
train() client id: f_00006-8-0 loss: 0.883775  [   32/   54]
train() client id: f_00006-9-0 loss: 0.891746  [   32/   54]
train() client id: f_00006-10-0 loss: 0.910193  [   32/   54]
train() client id: f_00006-11-0 loss: 0.910851  [   32/   54]
train() client id: f_00007-0-0 loss: 0.925480  [   32/  179]
train() client id: f_00007-0-1 loss: 0.934552  [   64/  179]
train() client id: f_00007-0-2 loss: 0.846961  [   96/  179]
train() client id: f_00007-0-3 loss: 0.942496  [  128/  179]
train() client id: f_00007-0-4 loss: 0.932981  [  160/  179]
train() client id: f_00007-1-0 loss: 0.961730  [   32/  179]
train() client id: f_00007-1-1 loss: 0.962773  [   64/  179]
train() client id: f_00007-1-2 loss: 0.879161  [   96/  179]
train() client id: f_00007-1-3 loss: 0.817766  [  128/  179]
train() client id: f_00007-1-4 loss: 0.854396  [  160/  179]
train() client id: f_00007-2-0 loss: 0.784256  [   32/  179]
train() client id: f_00007-2-1 loss: 0.975478  [   64/  179]
train() client id: f_00007-2-2 loss: 0.914802  [   96/  179]
train() client id: f_00007-2-3 loss: 0.864271  [  128/  179]
train() client id: f_00007-2-4 loss: 0.826801  [  160/  179]
train() client id: f_00007-3-0 loss: 0.913991  [   32/  179]
train() client id: f_00007-3-1 loss: 0.799636  [   64/  179]
train() client id: f_00007-3-2 loss: 0.933860  [   96/  179]
train() client id: f_00007-3-3 loss: 0.836221  [  128/  179]
train() client id: f_00007-3-4 loss: 0.825525  [  160/  179]
train() client id: f_00007-4-0 loss: 0.885216  [   32/  179]
train() client id: f_00007-4-1 loss: 0.778590  [   64/  179]
train() client id: f_00007-4-2 loss: 0.881040  [   96/  179]
train() client id: f_00007-4-3 loss: 0.846528  [  128/  179]
train() client id: f_00007-4-4 loss: 0.847552  [  160/  179]
train() client id: f_00007-5-0 loss: 0.919027  [   32/  179]
train() client id: f_00007-5-1 loss: 0.881778  [   64/  179]
train() client id: f_00007-5-2 loss: 0.759146  [   96/  179]
train() client id: f_00007-5-3 loss: 0.731230  [  128/  179]
train() client id: f_00007-5-4 loss: 0.746178  [  160/  179]
train() client id: f_00007-6-0 loss: 0.889862  [   32/  179]
train() client id: f_00007-6-1 loss: 0.831657  [   64/  179]
train() client id: f_00007-6-2 loss: 0.749480  [   96/  179]
train() client id: f_00007-6-3 loss: 0.890860  [  128/  179]
train() client id: f_00007-6-4 loss: 0.745490  [  160/  179]
train() client id: f_00007-7-0 loss: 0.800479  [   32/  179]
train() client id: f_00007-7-1 loss: 0.775637  [   64/  179]
train() client id: f_00007-7-2 loss: 0.798204  [   96/  179]
train() client id: f_00007-7-3 loss: 0.828417  [  128/  179]
train() client id: f_00007-7-4 loss: 0.809495  [  160/  179]
train() client id: f_00007-8-0 loss: 0.709995  [   32/  179]
train() client id: f_00007-8-1 loss: 0.852387  [   64/  179]
train() client id: f_00007-8-2 loss: 0.739051  [   96/  179]
train() client id: f_00007-8-3 loss: 0.885350  [  128/  179]
train() client id: f_00007-8-4 loss: 0.927111  [  160/  179]
train() client id: f_00007-9-0 loss: 0.925962  [   32/  179]
train() client id: f_00007-9-1 loss: 0.708914  [   64/  179]
train() client id: f_00007-9-2 loss: 0.815938  [   96/  179]
train() client id: f_00007-9-3 loss: 0.845785  [  128/  179]
train() client id: f_00007-9-4 loss: 0.713888  [  160/  179]
train() client id: f_00007-10-0 loss: 0.863271  [   32/  179]
train() client id: f_00007-10-1 loss: 0.712695  [   64/  179]
train() client id: f_00007-10-2 loss: 0.981875  [   96/  179]
train() client id: f_00007-10-3 loss: 0.825286  [  128/  179]
train() client id: f_00007-10-4 loss: 0.791032  [  160/  179]
train() client id: f_00007-11-0 loss: 0.780427  [   32/  179]
train() client id: f_00007-11-1 loss: 0.827729  [   64/  179]
train() client id: f_00007-11-2 loss: 0.930745  [   96/  179]
train() client id: f_00007-11-3 loss: 0.794192  [  128/  179]
train() client id: f_00007-11-4 loss: 0.856674  [  160/  179]
train() client id: f_00008-0-0 loss: 0.980948  [   32/  130]
train() client id: f_00008-0-1 loss: 0.913290  [   64/  130]
train() client id: f_00008-0-2 loss: 0.889780  [   96/  130]
train() client id: f_00008-0-3 loss: 0.796066  [  128/  130]
train() client id: f_00008-1-0 loss: 0.934287  [   32/  130]
train() client id: f_00008-1-1 loss: 0.874164  [   64/  130]
train() client id: f_00008-1-2 loss: 0.978743  [   96/  130]
train() client id: f_00008-1-3 loss: 0.829677  [  128/  130]
train() client id: f_00008-2-0 loss: 0.871408  [   32/  130]
train() client id: f_00008-2-1 loss: 0.874185  [   64/  130]
train() client id: f_00008-2-2 loss: 0.974877  [   96/  130]
train() client id: f_00008-2-3 loss: 0.849633  [  128/  130]
train() client id: f_00008-3-0 loss: 0.787310  [   32/  130]
train() client id: f_00008-3-1 loss: 0.913893  [   64/  130]
train() client id: f_00008-3-2 loss: 0.925763  [   96/  130]
train() client id: f_00008-3-3 loss: 0.960007  [  128/  130]
train() client id: f_00008-4-0 loss: 0.931003  [   32/  130]
train() client id: f_00008-4-1 loss: 0.849294  [   64/  130]
train() client id: f_00008-4-2 loss: 0.812276  [   96/  130]
train() client id: f_00008-4-3 loss: 0.927375  [  128/  130]
train() client id: f_00008-5-0 loss: 0.997696  [   32/  130]
train() client id: f_00008-5-1 loss: 0.854218  [   64/  130]
train() client id: f_00008-5-2 loss: 0.840008  [   96/  130]
train() client id: f_00008-5-3 loss: 0.871963  [  128/  130]
train() client id: f_00008-6-0 loss: 0.901219  [   32/  130]
train() client id: f_00008-6-1 loss: 0.914904  [   64/  130]
train() client id: f_00008-6-2 loss: 0.853722  [   96/  130]
train() client id: f_00008-6-3 loss: 0.903190  [  128/  130]
train() client id: f_00008-7-0 loss: 0.886566  [   32/  130]
train() client id: f_00008-7-1 loss: 0.849028  [   64/  130]
train() client id: f_00008-7-2 loss: 0.814312  [   96/  130]
train() client id: f_00008-7-3 loss: 0.988629  [  128/  130]
train() client id: f_00008-8-0 loss: 0.747831  [   32/  130]
train() client id: f_00008-8-1 loss: 0.990520  [   64/  130]
train() client id: f_00008-8-2 loss: 0.843804  [   96/  130]
train() client id: f_00008-8-3 loss: 0.983558  [  128/  130]
train() client id: f_00008-9-0 loss: 0.812571  [   32/  130]
train() client id: f_00008-9-1 loss: 0.936227  [   64/  130]
train() client id: f_00008-9-2 loss: 0.880596  [   96/  130]
train() client id: f_00008-9-3 loss: 0.873286  [  128/  130]
train() client id: f_00008-10-0 loss: 0.907713  [   32/  130]
train() client id: f_00008-10-1 loss: 0.833816  [   64/  130]
train() client id: f_00008-10-2 loss: 0.931617  [   96/  130]
train() client id: f_00008-10-3 loss: 0.857428  [  128/  130]
train() client id: f_00008-11-0 loss: 0.958661  [   32/  130]
train() client id: f_00008-11-1 loss: 0.934667  [   64/  130]
train() client id: f_00008-11-2 loss: 0.873371  [   96/  130]
train() client id: f_00008-11-3 loss: 0.792362  [  128/  130]
train() client id: f_00009-0-0 loss: 1.172819  [   32/  118]
train() client id: f_00009-0-1 loss: 1.296993  [   64/  118]
train() client id: f_00009-0-2 loss: 1.191706  [   96/  118]
train() client id: f_00009-1-0 loss: 1.208892  [   32/  118]
train() client id: f_00009-1-1 loss: 1.172758  [   64/  118]
train() client id: f_00009-1-2 loss: 1.165008  [   96/  118]
train() client id: f_00009-2-0 loss: 1.175763  [   32/  118]
train() client id: f_00009-2-1 loss: 1.165398  [   64/  118]
train() client id: f_00009-2-2 loss: 1.091480  [   96/  118]
train() client id: f_00009-3-0 loss: 1.123734  [   32/  118]
train() client id: f_00009-3-1 loss: 1.016713  [   64/  118]
train() client id: f_00009-3-2 loss: 1.097118  [   96/  118]
train() client id: f_00009-4-0 loss: 1.122850  [   32/  118]
train() client id: f_00009-4-1 loss: 1.045914  [   64/  118]
train() client id: f_00009-4-2 loss: 1.024908  [   96/  118]
train() client id: f_00009-5-0 loss: 0.955522  [   32/  118]
train() client id: f_00009-5-1 loss: 1.043351  [   64/  118]
train() client id: f_00009-5-2 loss: 1.051427  [   96/  118]
train() client id: f_00009-6-0 loss: 0.960776  [   32/  118]
train() client id: f_00009-6-1 loss: 1.015530  [   64/  118]
train() client id: f_00009-6-2 loss: 1.050911  [   96/  118]
train() client id: f_00009-7-0 loss: 0.987739  [   32/  118]
train() client id: f_00009-7-1 loss: 1.019878  [   64/  118]
train() client id: f_00009-7-2 loss: 1.025731  [   96/  118]
train() client id: f_00009-8-0 loss: 0.916735  [   32/  118]
train() client id: f_00009-8-1 loss: 1.053536  [   64/  118]
train() client id: f_00009-8-2 loss: 1.003202  [   96/  118]
train() client id: f_00009-9-0 loss: 1.084514  [   32/  118]
train() client id: f_00009-9-1 loss: 0.990076  [   64/  118]
train() client id: f_00009-9-2 loss: 0.963031  [   96/  118]
train() client id: f_00009-10-0 loss: 1.018267  [   32/  118]
train() client id: f_00009-10-1 loss: 1.014349  [   64/  118]
train() client id: f_00009-10-2 loss: 0.919609  [   96/  118]
train() client id: f_00009-11-0 loss: 0.945188  [   32/  118]
train() client id: f_00009-11-1 loss: 0.997430  [   64/  118]
train() client id: f_00009-11-2 loss: 0.954456  [   96/  118]
At round 6 accuracy: 0.6286472148541115
At round 6 training accuracy: 0.5767940979208585
At round 6 training loss: 0.9003989119380014
update_location
xs = [ -3.9056584    4.20031788  50.00902392  18.81129433  -4.02070377
   3.95640986 -12.44319194   3.67514815  34.66397685   2.93912145]
ys = [ 42.5879595   25.55583871   1.32061395 -12.45517586   9.35018685
 -12.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [108.76115328 103.2992912  111.81523373 102.51339522 100.51662575
 100.81739979 100.8053747  100.15467783 107.28588567 100.1231756 ]
dists_bs = [216.34745711 233.33733364 284.24213267 269.63356758 238.06255551
 258.96706121 240.78071123 253.04039114 262.18981874 246.78497559]
uav_gains = [8.10606699e-11 9.22049228e-11 7.56379401e-11 9.39823243e-11
 9.87196326e-11 9.79849732e-11 9.80141981e-11 9.96139759e-11
 8.38762801e-11 9.96923511e-11]
bs_gains = [3.19793078e-11 2.58784487e-11 1.48924591e-11 1.72634266e-11
 2.44657779e-11 1.93290009e-11 2.37002728e-11 2.06235063e-11
 1.86710946e-11 2.21208434e-11]
Round 7
-------------------------------
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.06798939 21.0326556   9.91312762  3.54368464 24.2561337  11.70105956
  4.40648092 14.22483624 10.44629039  9.49400071]
obj_prev = 119.08625877018483
eta_min = 5.793901722590025e-10	eta_max = 0.9200929959618319
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 27.695540866338916	eta = 0.909090909090909
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 47.81923698001299	eta = 0.5265195769323551
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 38.22376664223015	eta = 0.6586939654483899
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.504271658621605	eta = 0.6897210457833629
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.41961812780936	eta = 0.6913242290346582
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.41939846351146	eta = 0.6913283987699586
eta = 0.6913283987699586
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [0.03046057 0.06406391 0.02997707 0.01039528 0.07397568 0.03529559
 0.01305453 0.04327336 0.03142758 0.02852657]
ene_total = [3.09168549 6.04664955 3.05847947 1.40079217 6.88392674 3.69460946
 1.61888686 4.14166343 3.37804549 3.10465979]
ti_comp = [0.28943977 0.27173208 0.28857988 0.29120492 0.27061872 0.2656134
 0.2916892  0.29187392 0.26482942 0.26854656]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.10852063e-05 2.22555556e-04 2.02169483e-05 8.27924223e-07
 3.45486619e-04 3.89530298e-05 1.63426776e-06 5.94499024e-05
 2.76617870e-05 2.01182160e-05]
ene_total = [0.54893267 0.71368309 0.55604324 0.53249539 0.73325245 0.74945808
 0.52851731 0.53180388 0.75506382 0.72338265]
optimize_network_iter = 0 obj = 6.3726325729552675
eta = 0.6913283987699586
freqs = [5.26198833e+07 1.17880646e+08 5.19389521e+07 1.78487289e+07
 1.36678791e+08 6.64416515e+07 2.23774684e+07 7.41302190e+07
 5.93355081e+07 5.31128927e+07]
eta_min = 0.6839075543866936	eta_max = 0.6913283987699287
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 0.058240867111970324	eta = 0.9090909090909091
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 21.478076657689165	eta = 0.0024651296143925966
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.271073836819586	eta = 0.023313307551114187
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.1977536965308997	eta = 0.02409107213089359
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.1977247993541873	eta = 0.0240913888966546
eta = 0.0240913888966546
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.12822560e-04 2.24635427e-03 2.04058838e-04 8.35661507e-06
 3.48715330e-03 3.93170615e-04 1.64954064e-05 6.00054856e-04
 2.79202975e-04 2.03062288e-04]
ene_total = [0.17868869 0.2792033  0.18072909 0.16862269 0.31492846 0.24640612
 0.16755818 0.18248854 0.24546632 0.23363341]
ti_comp = [0.29797302 0.28026532 0.29711312 0.29973816 0.27915197 0.27414664
 0.30022245 0.30040716 0.27336266 0.27707981]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.16912899e-05 2.28100645e-04 2.07945251e-05 8.52018260e-07
 3.54005908e-04 3.98676209e-05 1.68198679e-06 6.11879833e-05
 2.83060505e-05 2.06045860e-05]
ene_total = [0.53609492 0.69738041 0.54303623 0.51999606 0.71673297 0.7319378
 0.5161133  0.5194606  0.73738988 0.7064396 ]
optimize_network_iter = 1 obj = 6.224581758881931
eta = 0.6839075543866936
freqs = [5.26073161e+07 1.17632966e+08 5.19221168e+07 1.78475642e+07
 1.36374531e+08 6.62555801e+07 2.23770658e+07 7.41302190e+07
 5.91638860e+07 5.29821614e+07]
eta_min = 0.6839075543866936	eta_max = 0.6839075543866819
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 0.05801261670554691	eta = 0.9090909090909091
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 21.4778591118164	eta = 0.0024554934542136467
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.270009752860956	eta = 0.023232826375798606
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.196960837665713	eta = 0.024005317507445147
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.196932244064364	eta = 0.024005629942423928
eta = 0.024005629942423928
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.12924512e-04 2.23906549e-03 2.04121753e-04 8.36352164e-06
 3.47496787e-03 3.91345733e-04 1.65106003e-05 6.00629173e-04
 2.77855860e-04 2.02257286e-04]
ene_total = [0.17867568 0.27898621 0.18071488 0.16860805 0.31457886 0.24633625
 0.16754386 0.18248768 0.24540916 0.23359161]
ti_comp = [0.29797302 0.28026532 0.29711312 0.29973816 0.27915197 0.27414664
 0.30022245 0.30040716 0.27336266 0.27707981]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.16912899e-05 2.28100645e-04 2.07945251e-05 8.52018260e-07
 3.54005908e-04 3.98676209e-05 1.68198679e-06 6.11879833e-05
 2.83060505e-05 2.06045860e-05]
ene_total = [0.53609492 0.69738041 0.54303623 0.51999606 0.71673297 0.7319378
 0.5161133  0.5194606  0.73738988 0.7064396 ]
optimize_network_iter = 2 obj = 6.224581758881931
eta = 0.6839075543866936
freqs = [5.26073161e+07 1.17632966e+08 5.19221168e+07 1.78475642e+07
 1.36374531e+08 6.62555801e+07 2.23770658e+07 7.41302190e+07
 5.91638860e+07 5.29821614e+07]
Done!
At round 7 eta: 0.6839075543866936
At round 7 local rounds: 12.440921153911754
At round 7 global rounds: 81.57354686993413
At round 7 a_n: 25.442236080499836
gradient difference: 0.41801610589027405
train() client id: f_00000-0-0 loss: 1.265895  [   32/  126]
train() client id: f_00000-0-1 loss: 1.147008  [   64/  126]
train() client id: f_00000-0-2 loss: 1.192918  [   96/  126]
train() client id: f_00000-1-0 loss: 1.025927  [   32/  126]
train() client id: f_00000-1-1 loss: 1.194353  [   64/  126]
train() client id: f_00000-1-2 loss: 1.146583  [   96/  126]
train() client id: f_00000-2-0 loss: 1.058790  [   32/  126]
train() client id: f_00000-2-1 loss: 1.145222  [   64/  126]
train() client id: f_00000-2-2 loss: 0.987338  [   96/  126]
train() client id: f_00000-3-0 loss: 1.024516  [   32/  126]
train() client id: f_00000-3-1 loss: 0.989518  [   64/  126]
train() client id: f_00000-3-2 loss: 0.964109  [   96/  126]
train() client id: f_00000-4-0 loss: 0.977112  [   32/  126]
train() client id: f_00000-4-1 loss: 0.925452  [   64/  126]
train() client id: f_00000-4-2 loss: 0.897694  [   96/  126]
train() client id: f_00000-5-0 loss: 0.909982  [   32/  126]
train() client id: f_00000-5-1 loss: 0.887726  [   64/  126]
train() client id: f_00000-5-2 loss: 0.906050  [   96/  126]
train() client id: f_00000-6-0 loss: 0.805307  [   32/  126]
train() client id: f_00000-6-1 loss: 0.884861  [   64/  126]
train() client id: f_00000-6-2 loss: 0.924526  [   96/  126]
train() client id: f_00000-7-0 loss: 0.879447  [   32/  126]
train() client id: f_00000-7-1 loss: 0.896982  [   64/  126]
train() client id: f_00000-7-2 loss: 0.873238  [   96/  126]
train() client id: f_00000-8-0 loss: 0.931557  [   32/  126]
train() client id: f_00000-8-1 loss: 0.821305  [   64/  126]
train() client id: f_00000-8-2 loss: 0.791371  [   96/  126]
train() client id: f_00000-9-0 loss: 0.858345  [   32/  126]
train() client id: f_00000-9-1 loss: 0.812942  [   64/  126]
train() client id: f_00000-9-2 loss: 0.829443  [   96/  126]
train() client id: f_00000-10-0 loss: 0.905855  [   32/  126]
train() client id: f_00000-10-1 loss: 0.810915  [   64/  126]
train() client id: f_00000-10-2 loss: 0.741506  [   96/  126]
train() client id: f_00000-11-0 loss: 0.749687  [   32/  126]
train() client id: f_00000-11-1 loss: 0.941232  [   64/  126]
train() client id: f_00000-11-2 loss: 0.859753  [   96/  126]
train() client id: f_00001-0-0 loss: 0.534297  [   32/  265]
train() client id: f_00001-0-1 loss: 0.595908  [   64/  265]
train() client id: f_00001-0-2 loss: 0.645042  [   96/  265]
train() client id: f_00001-0-3 loss: 0.585574  [  128/  265]
train() client id: f_00001-0-4 loss: 0.494215  [  160/  265]
train() client id: f_00001-0-5 loss: 0.568947  [  192/  265]
train() client id: f_00001-0-6 loss: 0.554378  [  224/  265]
train() client id: f_00001-0-7 loss: 0.569035  [  256/  265]
train() client id: f_00001-1-0 loss: 0.588186  [   32/  265]
train() client id: f_00001-1-1 loss: 0.543047  [   64/  265]
train() client id: f_00001-1-2 loss: 0.574027  [   96/  265]
train() client id: f_00001-1-3 loss: 0.557937  [  128/  265]
train() client id: f_00001-1-4 loss: 0.517307  [  160/  265]
train() client id: f_00001-1-5 loss: 0.555846  [  192/  265]
train() client id: f_00001-1-6 loss: 0.599679  [  224/  265]
train() client id: f_00001-1-7 loss: 0.483205  [  256/  265]
train() client id: f_00001-2-0 loss: 0.590610  [   32/  265]
train() client id: f_00001-2-1 loss: 0.477947  [   64/  265]
train() client id: f_00001-2-2 loss: 0.563565  [   96/  265]
train() client id: f_00001-2-3 loss: 0.495951  [  128/  265]
train() client id: f_00001-2-4 loss: 0.509469  [  160/  265]
train() client id: f_00001-2-5 loss: 0.481316  [  192/  265]
train() client id: f_00001-2-6 loss: 0.566811  [  224/  265]
train() client id: f_00001-2-7 loss: 0.546052  [  256/  265]
train() client id: f_00001-3-0 loss: 0.539404  [   32/  265]
train() client id: f_00001-3-1 loss: 0.641421  [   64/  265]
train() client id: f_00001-3-2 loss: 0.512478  [   96/  265]
train() client id: f_00001-3-3 loss: 0.566137  [  128/  265]
train() client id: f_00001-3-4 loss: 0.444476  [  160/  265]
train() client id: f_00001-3-5 loss: 0.512602  [  192/  265]
train() client id: f_00001-3-6 loss: 0.486880  [  224/  265]
train() client id: f_00001-3-7 loss: 0.500003  [  256/  265]
train() client id: f_00001-4-0 loss: 0.447387  [   32/  265]
train() client id: f_00001-4-1 loss: 0.481688  [   64/  265]
train() client id: f_00001-4-2 loss: 0.473499  [   96/  265]
train() client id: f_00001-4-3 loss: 0.569291  [  128/  265]
train() client id: f_00001-4-4 loss: 0.580801  [  160/  265]
train() client id: f_00001-4-5 loss: 0.445008  [  192/  265]
train() client id: f_00001-4-6 loss: 0.616816  [  224/  265]
train() client id: f_00001-4-7 loss: 0.574263  [  256/  265]
train() client id: f_00001-5-0 loss: 0.627077  [   32/  265]
train() client id: f_00001-5-1 loss: 0.518817  [   64/  265]
train() client id: f_00001-5-2 loss: 0.503458  [   96/  265]
train() client id: f_00001-5-3 loss: 0.431792  [  128/  265]
train() client id: f_00001-5-4 loss: 0.649701  [  160/  265]
train() client id: f_00001-5-5 loss: 0.448030  [  192/  265]
train() client id: f_00001-5-6 loss: 0.468400  [  224/  265]
train() client id: f_00001-5-7 loss: 0.518935  [  256/  265]
train() client id: f_00001-6-0 loss: 0.557561  [   32/  265]
train() client id: f_00001-6-1 loss: 0.446432  [   64/  265]
train() client id: f_00001-6-2 loss: 0.539425  [   96/  265]
train() client id: f_00001-6-3 loss: 0.463499  [  128/  265]
train() client id: f_00001-6-4 loss: 0.550868  [  160/  265]
train() client id: f_00001-6-5 loss: 0.553250  [  192/  265]
train() client id: f_00001-6-6 loss: 0.548013  [  224/  265]
train() client id: f_00001-6-7 loss: 0.410786  [  256/  265]
train() client id: f_00001-7-0 loss: 0.491537  [   32/  265]
train() client id: f_00001-7-1 loss: 0.526340  [   64/  265]
train() client id: f_00001-7-2 loss: 0.536047  [   96/  265]
train() client id: f_00001-7-3 loss: 0.442182  [  128/  265]
train() client id: f_00001-7-4 loss: 0.594317  [  160/  265]
train() client id: f_00001-7-5 loss: 0.479776  [  192/  265]
train() client id: f_00001-7-6 loss: 0.426188  [  224/  265]
train() client id: f_00001-7-7 loss: 0.547199  [  256/  265]
train() client id: f_00001-8-0 loss: 0.407430  [   32/  265]
train() client id: f_00001-8-1 loss: 0.505782  [   64/  265]
train() client id: f_00001-8-2 loss: 0.500546  [   96/  265]
train() client id: f_00001-8-3 loss: 0.444025  [  128/  265]
train() client id: f_00001-8-4 loss: 0.472965  [  160/  265]
train() client id: f_00001-8-5 loss: 0.657591  [  192/  265]
train() client id: f_00001-8-6 loss: 0.553665  [  224/  265]
train() client id: f_00001-8-7 loss: 0.489088  [  256/  265]
train() client id: f_00001-9-0 loss: 0.498168  [   32/  265]
train() client id: f_00001-9-1 loss: 0.537875  [   64/  265]
train() client id: f_00001-9-2 loss: 0.404149  [   96/  265]
train() client id: f_00001-9-3 loss: 0.572969  [  128/  265]
train() client id: f_00001-9-4 loss: 0.507285  [  160/  265]
train() client id: f_00001-9-5 loss: 0.501274  [  192/  265]
train() client id: f_00001-9-6 loss: 0.465730  [  224/  265]
train() client id: f_00001-9-7 loss: 0.569186  [  256/  265]
train() client id: f_00001-10-0 loss: 0.541833  [   32/  265]
train() client id: f_00001-10-1 loss: 0.430766  [   64/  265]
train() client id: f_00001-10-2 loss: 0.453532  [   96/  265]
train() client id: f_00001-10-3 loss: 0.503681  [  128/  265]
train() client id: f_00001-10-4 loss: 0.535348  [  160/  265]
train() client id: f_00001-10-5 loss: 0.436242  [  192/  265]
train() client id: f_00001-10-6 loss: 0.568438  [  224/  265]
train() client id: f_00001-10-7 loss: 0.520426  [  256/  265]
train() client id: f_00001-11-0 loss: 0.467615  [   32/  265]
train() client id: f_00001-11-1 loss: 0.623126  [   64/  265]
train() client id: f_00001-11-2 loss: 0.544529  [   96/  265]
train() client id: f_00001-11-3 loss: 0.538419  [  128/  265]
train() client id: f_00001-11-4 loss: 0.405948  [  160/  265]
train() client id: f_00001-11-5 loss: 0.574154  [  192/  265]
train() client id: f_00001-11-6 loss: 0.479044  [  224/  265]
train() client id: f_00001-11-7 loss: 0.464707  [  256/  265]
train() client id: f_00002-0-0 loss: 1.248844  [   32/  124]
train() client id: f_00002-0-1 loss: 1.288358  [   64/  124]
train() client id: f_00002-0-2 loss: 1.194083  [   96/  124]
train() client id: f_00002-1-0 loss: 1.195717  [   32/  124]
train() client id: f_00002-1-1 loss: 1.202233  [   64/  124]
train() client id: f_00002-1-2 loss: 1.220047  [   96/  124]
train() client id: f_00002-2-0 loss: 1.246338  [   32/  124]
train() client id: f_00002-2-1 loss: 1.144660  [   64/  124]
train() client id: f_00002-2-2 loss: 1.142729  [   96/  124]
train() client id: f_00002-3-0 loss: 1.177112  [   32/  124]
train() client id: f_00002-3-1 loss: 1.221367  [   64/  124]
train() client id: f_00002-3-2 loss: 1.083592  [   96/  124]
train() client id: f_00002-4-0 loss: 1.084265  [   32/  124]
train() client id: f_00002-4-1 loss: 1.083786  [   64/  124]
train() client id: f_00002-4-2 loss: 1.133165  [   96/  124]
train() client id: f_00002-5-0 loss: 1.061218  [   32/  124]
train() client id: f_00002-5-1 loss: 1.079116  [   64/  124]
train() client id: f_00002-5-2 loss: 1.076296  [   96/  124]
train() client id: f_00002-6-0 loss: 1.166629  [   32/  124]
train() client id: f_00002-6-1 loss: 1.082538  [   64/  124]
train() client id: f_00002-6-2 loss: 1.057695  [   96/  124]
train() client id: f_00002-7-0 loss: 1.162983  [   32/  124]
train() client id: f_00002-7-1 loss: 1.015976  [   64/  124]
train() client id: f_00002-7-2 loss: 0.997811  [   96/  124]
train() client id: f_00002-8-0 loss: 1.121757  [   32/  124]
train() client id: f_00002-8-1 loss: 0.981059  [   64/  124]
train() client id: f_00002-8-2 loss: 1.028467  [   96/  124]
train() client id: f_00002-9-0 loss: 1.004296  [   32/  124]
train() client id: f_00002-9-1 loss: 1.033169  [   64/  124]
train() client id: f_00002-9-2 loss: 1.114201  [   96/  124]
train() client id: f_00002-10-0 loss: 1.052960  [   32/  124]
train() client id: f_00002-10-1 loss: 1.060351  [   64/  124]
train() client id: f_00002-10-2 loss: 0.963394  [   96/  124]
train() client id: f_00002-11-0 loss: 1.083494  [   32/  124]
train() client id: f_00002-11-1 loss: 0.987219  [   64/  124]
train() client id: f_00002-11-2 loss: 1.032481  [   96/  124]
train() client id: f_00003-0-0 loss: 1.152029  [   32/   43]
train() client id: f_00003-1-0 loss: 1.049397  [   32/   43]
train() client id: f_00003-2-0 loss: 1.109883  [   32/   43]
train() client id: f_00003-3-0 loss: 1.059808  [   32/   43]
train() client id: f_00003-4-0 loss: 1.144047  [   32/   43]
train() client id: f_00003-5-0 loss: 1.074024  [   32/   43]
train() client id: f_00003-6-0 loss: 1.036448  [   32/   43]
train() client id: f_00003-7-0 loss: 1.021877  [   32/   43]
train() client id: f_00003-8-0 loss: 1.110141  [   32/   43]
train() client id: f_00003-9-0 loss: 1.039507  [   32/   43]
train() client id: f_00003-10-0 loss: 1.104479  [   32/   43]
train() client id: f_00003-11-0 loss: 1.011592  [   32/   43]
train() client id: f_00004-0-0 loss: 0.991746  [   32/  306]
train() client id: f_00004-0-1 loss: 0.975886  [   64/  306]
train() client id: f_00004-0-2 loss: 0.850768  [   96/  306]
train() client id: f_00004-0-3 loss: 0.965388  [  128/  306]
train() client id: f_00004-0-4 loss: 0.947577  [  160/  306]
train() client id: f_00004-0-5 loss: 0.960539  [  192/  306]
train() client id: f_00004-0-6 loss: 0.938609  [  224/  306]
train() client id: f_00004-0-7 loss: 1.032213  [  256/  306]
train() client id: f_00004-0-8 loss: 1.050799  [  288/  306]
train() client id: f_00004-1-0 loss: 0.964064  [   32/  306]
train() client id: f_00004-1-1 loss: 0.944637  [   64/  306]
train() client id: f_00004-1-2 loss: 0.917569  [   96/  306]
train() client id: f_00004-1-3 loss: 0.922686  [  128/  306]
train() client id: f_00004-1-4 loss: 0.983450  [  160/  306]
train() client id: f_00004-1-5 loss: 0.936527  [  192/  306]
train() client id: f_00004-1-6 loss: 1.109657  [  224/  306]
train() client id: f_00004-1-7 loss: 1.004890  [  256/  306]
train() client id: f_00004-1-8 loss: 0.857787  [  288/  306]
train() client id: f_00004-2-0 loss: 1.014241  [   32/  306]
train() client id: f_00004-2-1 loss: 0.917890  [   64/  306]
train() client id: f_00004-2-2 loss: 0.876863  [   96/  306]
train() client id: f_00004-2-3 loss: 0.940522  [  128/  306]
train() client id: f_00004-2-4 loss: 0.953736  [  160/  306]
train() client id: f_00004-2-5 loss: 0.970671  [  192/  306]
train() client id: f_00004-2-6 loss: 1.005903  [  224/  306]
train() client id: f_00004-2-7 loss: 0.985371  [  256/  306]
train() client id: f_00004-2-8 loss: 0.939670  [  288/  306]
train() client id: f_00004-3-0 loss: 0.938564  [   32/  306]
train() client id: f_00004-3-1 loss: 0.971974  [   64/  306]
train() client id: f_00004-3-2 loss: 0.906750  [   96/  306]
train() client id: f_00004-3-3 loss: 1.009918  [  128/  306]
train() client id: f_00004-3-4 loss: 0.933323  [  160/  306]
train() client id: f_00004-3-5 loss: 0.896032  [  192/  306]
train() client id: f_00004-3-6 loss: 0.996757  [  224/  306]
train() client id: f_00004-3-7 loss: 0.921567  [  256/  306]
train() client id: f_00004-3-8 loss: 1.072119  [  288/  306]
train() client id: f_00004-4-0 loss: 0.972552  [   32/  306]
train() client id: f_00004-4-1 loss: 1.051628  [   64/  306]
train() client id: f_00004-4-2 loss: 0.925569  [   96/  306]
train() client id: f_00004-4-3 loss: 0.855516  [  128/  306]
train() client id: f_00004-4-4 loss: 0.908701  [  160/  306]
train() client id: f_00004-4-5 loss: 0.870742  [  192/  306]
train() client id: f_00004-4-6 loss: 1.047082  [  224/  306]
train() client id: f_00004-4-7 loss: 0.979535  [  256/  306]
train() client id: f_00004-4-8 loss: 0.980945  [  288/  306]
train() client id: f_00004-5-0 loss: 1.053078  [   32/  306]
train() client id: f_00004-5-1 loss: 0.941517  [   64/  306]
train() client id: f_00004-5-2 loss: 0.960671  [   96/  306]
train() client id: f_00004-5-3 loss: 0.912569  [  128/  306]
train() client id: f_00004-5-4 loss: 0.891817  [  160/  306]
train() client id: f_00004-5-5 loss: 0.914855  [  192/  306]
train() client id: f_00004-5-6 loss: 1.014556  [  224/  306]
train() client id: f_00004-5-7 loss: 0.959887  [  256/  306]
train() client id: f_00004-5-8 loss: 0.883392  [  288/  306]
train() client id: f_00004-6-0 loss: 1.006479  [   32/  306]
train() client id: f_00004-6-1 loss: 0.821439  [   64/  306]
train() client id: f_00004-6-2 loss: 0.880364  [   96/  306]
train() client id: f_00004-6-3 loss: 1.020525  [  128/  306]
train() client id: f_00004-6-4 loss: 0.952935  [  160/  306]
train() client id: f_00004-6-5 loss: 0.976475  [  192/  306]
train() client id: f_00004-6-6 loss: 0.989282  [  224/  306]
train() client id: f_00004-6-7 loss: 0.980417  [  256/  306]
train() client id: f_00004-6-8 loss: 0.959207  [  288/  306]
train() client id: f_00004-7-0 loss: 0.912120  [   32/  306]
train() client id: f_00004-7-1 loss: 0.877883  [   64/  306]
train() client id: f_00004-7-2 loss: 0.912839  [   96/  306]
train() client id: f_00004-7-3 loss: 0.989977  [  128/  306]
train() client id: f_00004-7-4 loss: 0.945018  [  160/  306]
train() client id: f_00004-7-5 loss: 0.932544  [  192/  306]
train() client id: f_00004-7-6 loss: 0.931970  [  224/  306]
train() client id: f_00004-7-7 loss: 1.065095  [  256/  306]
train() client id: f_00004-7-8 loss: 0.938626  [  288/  306]
train() client id: f_00004-8-0 loss: 0.900158  [   32/  306]
train() client id: f_00004-8-1 loss: 0.992727  [   64/  306]
train() client id: f_00004-8-2 loss: 0.972857  [   96/  306]
train() client id: f_00004-8-3 loss: 0.956121  [  128/  306]
train() client id: f_00004-8-4 loss: 0.868449  [  160/  306]
train() client id: f_00004-8-5 loss: 0.907510  [  192/  306]
train() client id: f_00004-8-6 loss: 0.872582  [  224/  306]
train() client id: f_00004-8-7 loss: 0.950173  [  256/  306]
train() client id: f_00004-8-8 loss: 1.023037  [  288/  306]
train() client id: f_00004-9-0 loss: 1.092298  [   32/  306]
train() client id: f_00004-9-1 loss: 0.986381  [   64/  306]
train() client id: f_00004-9-2 loss: 0.913069  [   96/  306]
train() client id: f_00004-9-3 loss: 0.870005  [  128/  306]
train() client id: f_00004-9-4 loss: 0.952565  [  160/  306]
train() client id: f_00004-9-5 loss: 0.888203  [  192/  306]
train() client id: f_00004-9-6 loss: 0.887464  [  224/  306]
train() client id: f_00004-9-7 loss: 1.022164  [  256/  306]
train() client id: f_00004-9-8 loss: 0.943714  [  288/  306]
train() client id: f_00004-10-0 loss: 0.958319  [   32/  306]
train() client id: f_00004-10-1 loss: 1.003503  [   64/  306]
train() client id: f_00004-10-2 loss: 0.870874  [   96/  306]
train() client id: f_00004-10-3 loss: 0.904487  [  128/  306]
train() client id: f_00004-10-4 loss: 0.902093  [  160/  306]
train() client id: f_00004-10-5 loss: 0.979684  [  192/  306]
train() client id: f_00004-10-6 loss: 0.999563  [  224/  306]
train() client id: f_00004-10-7 loss: 0.943166  [  256/  306]
train() client id: f_00004-10-8 loss: 0.966464  [  288/  306]
train() client id: f_00004-11-0 loss: 0.926713  [   32/  306]
train() client id: f_00004-11-1 loss: 0.825447  [   64/  306]
train() client id: f_00004-11-2 loss: 0.994163  [   96/  306]
train() client id: f_00004-11-3 loss: 0.954601  [  128/  306]
train() client id: f_00004-11-4 loss: 0.908738  [  160/  306]
train() client id: f_00004-11-5 loss: 1.024338  [  192/  306]
train() client id: f_00004-11-6 loss: 0.993534  [  224/  306]
train() client id: f_00004-11-7 loss: 0.878680  [  256/  306]
train() client id: f_00004-11-8 loss: 0.977691  [  288/  306]
train() client id: f_00005-0-0 loss: 1.004693  [   32/  146]
train() client id: f_00005-0-1 loss: 0.774012  [   64/  146]
train() client id: f_00005-0-2 loss: 0.772755  [   96/  146]
train() client id: f_00005-0-3 loss: 0.882547  [  128/  146]
train() client id: f_00005-1-0 loss: 0.864090  [   32/  146]
train() client id: f_00005-1-1 loss: 0.859721  [   64/  146]
train() client id: f_00005-1-2 loss: 0.790342  [   96/  146]
train() client id: f_00005-1-3 loss: 0.943130  [  128/  146]
train() client id: f_00005-2-0 loss: 0.756201  [   32/  146]
train() client id: f_00005-2-1 loss: 0.820950  [   64/  146]
train() client id: f_00005-2-2 loss: 0.955272  [   96/  146]
train() client id: f_00005-2-3 loss: 0.822798  [  128/  146]
train() client id: f_00005-3-0 loss: 0.751763  [   32/  146]
train() client id: f_00005-3-1 loss: 0.944240  [   64/  146]
train() client id: f_00005-3-2 loss: 0.876449  [   96/  146]
train() client id: f_00005-3-3 loss: 0.784556  [  128/  146]
train() client id: f_00005-4-0 loss: 0.782027  [   32/  146]
train() client id: f_00005-4-1 loss: 0.820582  [   64/  146]
train() client id: f_00005-4-2 loss: 0.776087  [   96/  146]
train() client id: f_00005-4-3 loss: 0.829168  [  128/  146]
train() client id: f_00005-5-0 loss: 0.686876  [   32/  146]
train() client id: f_00005-5-1 loss: 0.977100  [   64/  146]
train() client id: f_00005-5-2 loss: 0.789546  [   96/  146]
train() client id: f_00005-5-3 loss: 0.820785  [  128/  146]
train() client id: f_00005-6-0 loss: 0.853784  [   32/  146]
train() client id: f_00005-6-1 loss: 0.806092  [   64/  146]
train() client id: f_00005-6-2 loss: 0.796538  [   96/  146]
train() client id: f_00005-6-3 loss: 0.745409  [  128/  146]
train() client id: f_00005-7-0 loss: 0.749200  [   32/  146]
train() client id: f_00005-7-1 loss: 0.840837  [   64/  146]
train() client id: f_00005-7-2 loss: 0.778596  [   96/  146]
train() client id: f_00005-7-3 loss: 0.828875  [  128/  146]
train() client id: f_00005-8-0 loss: 0.751179  [   32/  146]
train() client id: f_00005-8-1 loss: 0.754059  [   64/  146]
train() client id: f_00005-8-2 loss: 0.816709  [   96/  146]
train() client id: f_00005-8-3 loss: 0.790863  [  128/  146]
train() client id: f_00005-9-0 loss: 0.889398  [   32/  146]
train() client id: f_00005-9-1 loss: 0.653297  [   64/  146]
train() client id: f_00005-9-2 loss: 0.822735  [   96/  146]
train() client id: f_00005-9-3 loss: 0.858726  [  128/  146]
train() client id: f_00005-10-0 loss: 0.871258  [   32/  146]
train() client id: f_00005-10-1 loss: 0.650942  [   64/  146]
train() client id: f_00005-10-2 loss: 1.047052  [   96/  146]
train() client id: f_00005-10-3 loss: 0.668814  [  128/  146]
train() client id: f_00005-11-0 loss: 0.834400  [   32/  146]
train() client id: f_00005-11-1 loss: 0.733253  [   64/  146]
train() client id: f_00005-11-2 loss: 0.741134  [   96/  146]
train() client id: f_00005-11-3 loss: 0.784878  [  128/  146]
train() client id: f_00006-0-0 loss: 0.814851  [   32/   54]
train() client id: f_00006-1-0 loss: 0.800318  [   32/   54]
train() client id: f_00006-2-0 loss: 0.841493  [   32/   54]
train() client id: f_00006-3-0 loss: 0.838995  [   32/   54]
train() client id: f_00006-4-0 loss: 0.811613  [   32/   54]
train() client id: f_00006-5-0 loss: 0.816364  [   32/   54]
train() client id: f_00006-6-0 loss: 0.821631  [   32/   54]
train() client id: f_00006-7-0 loss: 0.783585  [   32/   54]
train() client id: f_00006-8-0 loss: 0.798825  [   32/   54]
train() client id: f_00006-9-0 loss: 0.854979  [   32/   54]
train() client id: f_00006-10-0 loss: 0.793876  [   32/   54]
train() client id: f_00006-11-0 loss: 0.850524  [   32/   54]
train() client id: f_00007-0-0 loss: 0.853877  [   32/  179]
train() client id: f_00007-0-1 loss: 0.788105  [   64/  179]
train() client id: f_00007-0-2 loss: 0.810052  [   96/  179]
train() client id: f_00007-0-3 loss: 0.774771  [  128/  179]
train() client id: f_00007-0-4 loss: 0.725955  [  160/  179]
train() client id: f_00007-1-0 loss: 0.724937  [   32/  179]
train() client id: f_00007-1-1 loss: 0.780927  [   64/  179]
train() client id: f_00007-1-2 loss: 0.742873  [   96/  179]
train() client id: f_00007-1-3 loss: 0.734641  [  128/  179]
train() client id: f_00007-1-4 loss: 0.802216  [  160/  179]
train() client id: f_00007-2-0 loss: 0.665514  [   32/  179]
train() client id: f_00007-2-1 loss: 0.705143  [   64/  179]
train() client id: f_00007-2-2 loss: 0.688483  [   96/  179]
train() client id: f_00007-2-3 loss: 0.750848  [  128/  179]
train() client id: f_00007-2-4 loss: 0.816373  [  160/  179]
train() client id: f_00007-3-0 loss: 0.669995  [   32/  179]
train() client id: f_00007-3-1 loss: 0.784843  [   64/  179]
train() client id: f_00007-3-2 loss: 0.779905  [   96/  179]
train() client id: f_00007-3-3 loss: 0.667477  [  128/  179]
train() client id: f_00007-3-4 loss: 0.673501  [  160/  179]
train() client id: f_00007-4-0 loss: 0.756602  [   32/  179]
train() client id: f_00007-4-1 loss: 0.749237  [   64/  179]
train() client id: f_00007-4-2 loss: 0.693462  [   96/  179]
train() client id: f_00007-4-3 loss: 0.689894  [  128/  179]
train() client id: f_00007-4-4 loss: 0.663827  [  160/  179]
train() client id: f_00007-5-0 loss: 0.795193  [   32/  179]
train() client id: f_00007-5-1 loss: 0.698350  [   64/  179]
train() client id: f_00007-5-2 loss: 0.647060  [   96/  179]
train() client id: f_00007-5-3 loss: 0.651760  [  128/  179]
train() client id: f_00007-5-4 loss: 0.682618  [  160/  179]
train() client id: f_00007-6-0 loss: 0.827950  [   32/  179]
train() client id: f_00007-6-1 loss: 0.843551  [   64/  179]
train() client id: f_00007-6-2 loss: 0.557680  [   96/  179]
train() client id: f_00007-6-3 loss: 0.618545  [  128/  179]
train() client id: f_00007-6-4 loss: 0.561287  [  160/  179]
train() client id: f_00007-7-0 loss: 0.726090  [   32/  179]
train() client id: f_00007-7-1 loss: 0.551032  [   64/  179]
train() client id: f_00007-7-2 loss: 0.600404  [   96/  179]
train() client id: f_00007-7-3 loss: 0.683712  [  128/  179]
train() client id: f_00007-7-4 loss: 0.667572  [  160/  179]
train() client id: f_00007-8-0 loss: 0.733373  [   32/  179]
train() client id: f_00007-8-1 loss: 0.623017  [   64/  179]
train() client id: f_00007-8-2 loss: 0.782740  [   96/  179]
train() client id: f_00007-8-3 loss: 0.565817  [  128/  179]
train() client id: f_00007-8-4 loss: 0.655168  [  160/  179]
train() client id: f_00007-9-0 loss: 0.686820  [   32/  179]
train() client id: f_00007-9-1 loss: 0.694085  [   64/  179]
train() client id: f_00007-9-2 loss: 0.697646  [   96/  179]
train() client id: f_00007-9-3 loss: 0.572720  [  128/  179]
train() client id: f_00007-9-4 loss: 0.687949  [  160/  179]
train() client id: f_00007-10-0 loss: 0.627848  [   32/  179]
train() client id: f_00007-10-1 loss: 0.856048  [   64/  179]
train() client id: f_00007-10-2 loss: 0.549706  [   96/  179]
train() client id: f_00007-10-3 loss: 0.625552  [  128/  179]
train() client id: f_00007-10-4 loss: 0.678009  [  160/  179]
train() client id: f_00007-11-0 loss: 0.646157  [   32/  179]
train() client id: f_00007-11-1 loss: 0.562125  [   64/  179]
train() client id: f_00007-11-2 loss: 0.628328  [   96/  179]
train() client id: f_00007-11-3 loss: 0.627954  [  128/  179]
train() client id: f_00007-11-4 loss: 0.735522  [  160/  179]
train() client id: f_00008-0-0 loss: 0.958967  [   32/  130]
train() client id: f_00008-0-1 loss: 1.033283  [   64/  130]
train() client id: f_00008-0-2 loss: 0.850915  [   96/  130]
train() client id: f_00008-0-3 loss: 0.911969  [  128/  130]
train() client id: f_00008-1-0 loss: 0.997817  [   32/  130]
train() client id: f_00008-1-1 loss: 0.913502  [   64/  130]
train() client id: f_00008-1-2 loss: 0.935927  [   96/  130]
train() client id: f_00008-1-3 loss: 0.946643  [  128/  130]
train() client id: f_00008-2-0 loss: 0.778858  [   32/  130]
train() client id: f_00008-2-1 loss: 0.912934  [   64/  130]
train() client id: f_00008-2-2 loss: 0.995303  [   96/  130]
train() client id: f_00008-2-3 loss: 1.065582  [  128/  130]
train() client id: f_00008-3-0 loss: 0.850468  [   32/  130]
train() client id: f_00008-3-1 loss: 1.015849  [   64/  130]
train() client id: f_00008-3-2 loss: 0.986351  [   96/  130]
train() client id: f_00008-3-3 loss: 0.904526  [  128/  130]
train() client id: f_00008-4-0 loss: 1.001907  [   32/  130]
train() client id: f_00008-4-1 loss: 0.939866  [   64/  130]
train() client id: f_00008-4-2 loss: 0.927328  [   96/  130]
train() client id: f_00008-4-3 loss: 0.903634  [  128/  130]
train() client id: f_00008-5-0 loss: 1.082033  [   32/  130]
train() client id: f_00008-5-1 loss: 0.901241  [   64/  130]
train() client id: f_00008-5-2 loss: 0.907316  [   96/  130]
train() client id: f_00008-5-3 loss: 0.917637  [  128/  130]
train() client id: f_00008-6-0 loss: 1.010062  [   32/  130]
train() client id: f_00008-6-1 loss: 0.935561  [   64/  130]
train() client id: f_00008-6-2 loss: 0.869839  [   96/  130]
train() client id: f_00008-6-3 loss: 0.984919  [  128/  130]
train() client id: f_00008-7-0 loss: 0.921599  [   32/  130]
train() client id: f_00008-7-1 loss: 0.980953  [   64/  130]
train() client id: f_00008-7-2 loss: 0.988546  [   96/  130]
train() client id: f_00008-7-3 loss: 0.898040  [  128/  130]
train() client id: f_00008-8-0 loss: 0.964752  [   32/  130]
train() client id: f_00008-8-1 loss: 0.998747  [   64/  130]
train() client id: f_00008-8-2 loss: 0.853476  [   96/  130]
train() client id: f_00008-8-3 loss: 0.944552  [  128/  130]
train() client id: f_00008-9-0 loss: 0.977641  [   32/  130]
train() client id: f_00008-9-1 loss: 1.032753  [   64/  130]
train() client id: f_00008-9-2 loss: 0.757347  [   96/  130]
train() client id: f_00008-9-3 loss: 0.992920  [  128/  130]
train() client id: f_00008-10-0 loss: 0.877824  [   32/  130]
train() client id: f_00008-10-1 loss: 0.934743  [   64/  130]
train() client id: f_00008-10-2 loss: 0.907401  [   96/  130]
train() client id: f_00008-10-3 loss: 1.080678  [  128/  130]
train() client id: f_00008-11-0 loss: 1.052484  [   32/  130]
train() client id: f_00008-11-1 loss: 0.870466  [   64/  130]
train() client id: f_00008-11-2 loss: 0.876000  [   96/  130]
train() client id: f_00008-11-3 loss: 1.029270  [  128/  130]
train() client id: f_00009-0-0 loss: 1.268269  [   32/  118]
train() client id: f_00009-0-1 loss: 1.173263  [   64/  118]
train() client id: f_00009-0-2 loss: 1.144312  [   96/  118]
train() client id: f_00009-1-0 loss: 1.178846  [   32/  118]
train() client id: f_00009-1-1 loss: 1.171932  [   64/  118]
train() client id: f_00009-1-2 loss: 1.064733  [   96/  118]
train() client id: f_00009-2-0 loss: 1.128660  [   32/  118]
train() client id: f_00009-2-1 loss: 1.071311  [   64/  118]
train() client id: f_00009-2-2 loss: 1.103990  [   96/  118]
train() client id: f_00009-3-0 loss: 1.027530  [   32/  118]
train() client id: f_00009-3-1 loss: 1.023320  [   64/  118]
train() client id: f_00009-3-2 loss: 1.085973  [   96/  118]
train() client id: f_00009-4-0 loss: 1.050071  [   32/  118]
train() client id: f_00009-4-1 loss: 1.038331  [   64/  118]
train() client id: f_00009-4-2 loss: 1.072702  [   96/  118]
train() client id: f_00009-5-0 loss: 0.995783  [   32/  118]
train() client id: f_00009-5-1 loss: 1.101416  [   64/  118]
train() client id: f_00009-5-2 loss: 0.991564  [   96/  118]
train() client id: f_00009-6-0 loss: 0.977258  [   32/  118]
train() client id: f_00009-6-1 loss: 1.009531  [   64/  118]
train() client id: f_00009-6-2 loss: 1.020629  [   96/  118]
train() client id: f_00009-7-0 loss: 1.035683  [   32/  118]
train() client id: f_00009-7-1 loss: 1.022608  [   64/  118]
train() client id: f_00009-7-2 loss: 0.935689  [   96/  118]
train() client id: f_00009-8-0 loss: 1.073774  [   32/  118]
train() client id: f_00009-8-1 loss: 0.956392  [   64/  118]
train() client id: f_00009-8-2 loss: 0.981538  [   96/  118]
train() client id: f_00009-9-0 loss: 1.017081  [   32/  118]
train() client id: f_00009-9-1 loss: 0.934285  [   64/  118]
train() client id: f_00009-9-2 loss: 0.959550  [   96/  118]
train() client id: f_00009-10-0 loss: 0.983678  [   32/  118]
train() client id: f_00009-10-1 loss: 1.106071  [   64/  118]
train() client id: f_00009-10-2 loss: 0.904752  [   96/  118]
train() client id: f_00009-11-0 loss: 1.041100  [   32/  118]
train() client id: f_00009-11-1 loss: 0.877633  [   64/  118]
train() client id: f_00009-11-2 loss: 0.981309  [   96/  118]
At round 7 accuracy: 0.6286472148541115
At round 7 training accuracy: 0.5734406438631791
At round 7 training loss: 0.8950309354537902
update_location
xs = [ -3.9056584    4.20031788  55.00902392  18.81129433   0.97929623
   3.95640986 -17.44319194  -1.32485185  39.66397685  -2.06087855]
ys = [ 47.5879595   30.55583871   1.32061395 -17.45517586   9.35018685
  -7.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [110.81456609 104.64846845 114.13911132 103.24024389 100.44095288
 100.33588395 101.54385992 100.09599397 109.0041331  100.10124413]
dists_bs = [213.32393628 230.16704729 288.21637743 273.13332372 241.6786571
 255.37635032 237.43374407 249.53494358 266.20507287 243.20450851]
uav_gains = [7.73572375e-11 8.92616175e-11 7.18461074e-11 9.23368231e-11
 9.89056819e-11 9.91648192e-11 9.62418239e-11 9.97600464e-11
 8.06096645e-11 9.97469659e-11]
bs_gains = [3.32646713e-11 2.68889148e-11 1.43245785e-11 1.66511777e-11
 2.34545361e-11 2.00996360e-11 2.46476348e-11 2.14450097e-11
 1.78932145e-11 2.30448319e-11]
Round 8
-------------------------------
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.93572436 20.7513301   9.78311252  3.49713647 23.93609638 11.54337877
  4.34848004 14.03581921 10.31003617  9.36568175]
obj_prev = 117.50679575560986
eta_min = 4.500137844098843e-10	eta_max = 0.9201793140385103
af = 24.843282687249765	bf = 1.924183044657018	zeta = 27.327610955974745	eta = 0.909090909090909
af = 24.843282687249765	bf = 1.924183044657018	zeta = 47.21200862959305	eta = 0.5262068572883742
af = 24.843282687249765	bf = 1.924183044657018	zeta = 37.72761010314322	eta = 0.6584907609925703
af = 24.843282687249765	bf = 1.924183044657018	zeta = 36.02779691629381	eta = 0.689558752231509
af = 24.843282687249765	bf = 1.924183044657018	zeta = 35.944047891502834	eta = 0.6911654124832922
af = 24.843282687249765	bf = 1.924183044657018	zeta = 35.94383021535565	eta = 0.6911695981870181
eta = 0.6911695981870181
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [0.03047953 0.06410378 0.02999573 0.01040174 0.07402172 0.03531755
 0.01306266 0.04330029 0.03144714 0.02854432]
ene_total = [3.05537056 5.9604125  3.0232152  1.38370094 6.79982988 3.63829942
 1.59895401 4.0866222  3.34115314 3.05627237]
ti_comp = [0.29321835 0.27683249 0.29228351 0.29535599 0.2741193  0.27083978
 0.29583662 0.2962475  0.2682047  0.2737568 ]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.05836579e-05 2.14830890e-04 1.97446596e-05 8.06319238e-07
 3.37348113e-04 3.75341750e-05 1.59173447e-06 5.78153199e-05
 2.70203220e-05 1.93958759e-05]
ene_total = [0.54608481 0.69707832 0.55371718 0.52684511 0.72952358 0.73184161
 0.52295022 0.52419719 0.75268396 0.70631616]
optimize_network_iter = 0 obj = 6.291238143797367
eta = 0.6911695981870181
freqs = [5.19741187e+07 1.15780803e+08 5.13127289e+07 1.76088264e+07
 1.35017343e+08 6.52000814e+07 2.20774830e+07 7.30812690e+07
 5.86252507e+07 5.21344532e+07]
eta_min = 0.6875884070281042	eta_max = 0.6911695981870177
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 0.05581615321591653	eta = 0.909090909090909
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 21.219211967217113	eta = 0.002391321484860486
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.2358034144344674	eta = 0.022695178449689362
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.1654164527348314	eta = 0.02343288627225956
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.1653901507160183	eta = 0.02343317090097433
eta = 0.02343317090097433
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.09174714e-04 2.18314889e-03 2.00648668e-04 8.19395643e-06
 3.42819024e-03 3.81428819e-04 1.61754827e-05 5.87529343e-04
 2.74585215e-04 1.97104266e-04]
ene_total = [0.17760768 0.27172476 0.17982108 0.16680246 0.3112301  0.24039767
 0.16575821 0.17957311 0.24447922 0.22799587]
ti_comp = [0.29738478 0.28099891 0.29644993 0.29952241 0.27828572 0.27500621
 0.30000305 0.30041392 0.27237112 0.27792323]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.08671664e-05 2.17429093e-04 2.00148185e-05 8.17590855e-07
 3.41327869e-04 3.79632066e-05 1.61405844e-06 5.86284344e-05
 2.73210417e-05 1.96239144e-05]
ene_total = [0.5398481  0.68929927 0.54739189 0.52080677 0.72148512 0.72348741
 0.51695743 0.51825451 0.74408039 0.69823819]
optimize_network_iter = 1 obj = 6.219849079279121
eta = 0.6875884070281042
freqs = [5.19666731e+07 1.15668296e+08 5.13030792e+07 1.76080893e+07
 1.34866351e+08 6.51153626e+07 2.20770578e+07 7.30812690e+07
 5.85403612e+07 5.20751181e+07]
eta_min = 0.6875884070281044	eta_max = 0.6875884070280862
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 0.055709725023689025	eta = 0.909090909090909
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 21.21911053031574	eta = 0.0023867732106222467
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.23530448131582	eta = 0.022656960154787298
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.1650443530624273	eta = 0.023392224965439173
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.1650181860209075	eta = 0.023392507690695646
eta = 0.023392507690695646
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.09211545e-04 2.17991631e-03 2.00666013e-04 8.19706147e-06
 3.42210961e-03 3.80614259e-04 1.61823437e-05 5.87801193e-04
 2.73917274e-04 1.96746859e-04]
ene_total = [0.17760125 0.27162923 0.17981404 0.16679559 0.31105873 0.24036644
 0.16575149 0.17957271 0.24445164 0.22797706]
ti_comp = [0.29738478 0.28099891 0.29644993 0.29952241 0.27828572 0.27500621
 0.30000305 0.30041392 0.27237112 0.27792323]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.08671664e-05 2.17429093e-04 2.00148185e-05 8.17590855e-07
 3.41327869e-04 3.79632066e-05 1.61405844e-06 5.86284344e-05
 2.73210417e-05 1.96239144e-05]
ene_total = [0.5398481  0.68929927 0.54739189 0.52080677 0.72148512 0.72348741
 0.51695743 0.51825451 0.74408039 0.69823819]
optimize_network_iter = 2 obj = 6.219849079279126
eta = 0.6875884070281044
freqs = [5.19666731e+07 1.15668296e+08 5.13030792e+07 1.76080893e+07
 1.34866351e+08 6.51153626e+07 2.20770578e+07 7.30812690e+07
 5.85403612e+07 5.20751181e+07]
Done!
At round 8 eta: 0.6875884070281044
At round 8 local rounds: 12.265156719696218
At round 8 global rounds: 81.4381945256065
At round 8 a_n: 25.09969023353052
gradient difference: 0.41428136825561523
train() client id: f_00000-0-0 loss: 1.464861  [   32/  126]
train() client id: f_00000-0-1 loss: 1.351374  [   64/  126]
train() client id: f_00000-0-2 loss: 1.352576  [   96/  126]
train() client id: f_00000-1-0 loss: 1.299896  [   32/  126]
train() client id: f_00000-1-1 loss: 1.256290  [   64/  126]
train() client id: f_00000-1-2 loss: 1.205422  [   96/  126]
train() client id: f_00000-2-0 loss: 1.232809  [   32/  126]
train() client id: f_00000-2-1 loss: 1.130755  [   64/  126]
train() client id: f_00000-2-2 loss: 1.106113  [   96/  126]
train() client id: f_00000-3-0 loss: 1.093169  [   32/  126]
train() client id: f_00000-3-1 loss: 1.077278  [   64/  126]
train() client id: f_00000-3-2 loss: 1.034748  [   96/  126]
train() client id: f_00000-4-0 loss: 1.030284  [   32/  126]
train() client id: f_00000-4-1 loss: 1.044130  [   64/  126]
train() client id: f_00000-4-2 loss: 0.995570  [   96/  126]
train() client id: f_00000-5-0 loss: 0.989656  [   32/  126]
train() client id: f_00000-5-1 loss: 0.960329  [   64/  126]
train() client id: f_00000-5-2 loss: 0.972197  [   96/  126]
train() client id: f_00000-6-0 loss: 0.939349  [   32/  126]
train() client id: f_00000-6-1 loss: 0.954140  [   64/  126]
train() client id: f_00000-6-2 loss: 0.946061  [   96/  126]
train() client id: f_00000-7-0 loss: 0.951358  [   32/  126]
train() client id: f_00000-7-1 loss: 0.924933  [   64/  126]
train() client id: f_00000-7-2 loss: 0.900398  [   96/  126]
train() client id: f_00000-8-0 loss: 0.970174  [   32/  126]
train() client id: f_00000-8-1 loss: 0.921471  [   64/  126]
train() client id: f_00000-8-2 loss: 0.834275  [   96/  126]
train() client id: f_00000-9-0 loss: 0.924231  [   32/  126]
train() client id: f_00000-9-1 loss: 1.009804  [   64/  126]
train() client id: f_00000-9-2 loss: 0.896335  [   96/  126]
train() client id: f_00000-10-0 loss: 0.908236  [   32/  126]
train() client id: f_00000-10-1 loss: 0.865685  [   64/  126]
train() client id: f_00000-10-2 loss: 0.861049  [   96/  126]
train() client id: f_00000-11-0 loss: 0.845186  [   32/  126]
train() client id: f_00000-11-1 loss: 0.913128  [   64/  126]
train() client id: f_00000-11-2 loss: 0.901598  [   96/  126]
train() client id: f_00001-0-0 loss: 0.606879  [   32/  265]
train() client id: f_00001-0-1 loss: 0.513202  [   64/  265]
train() client id: f_00001-0-2 loss: 0.569414  [   96/  265]
train() client id: f_00001-0-3 loss: 0.575715  [  128/  265]
train() client id: f_00001-0-4 loss: 0.639583  [  160/  265]
train() client id: f_00001-0-5 loss: 0.497395  [  192/  265]
train() client id: f_00001-0-6 loss: 0.519795  [  224/  265]
train() client id: f_00001-0-7 loss: 0.615975  [  256/  265]
train() client id: f_00001-1-0 loss: 0.486961  [   32/  265]
train() client id: f_00001-1-1 loss: 0.490895  [   64/  265]
train() client id: f_00001-1-2 loss: 0.575253  [   96/  265]
train() client id: f_00001-1-3 loss: 0.490724  [  128/  265]
train() client id: f_00001-1-4 loss: 0.573231  [  160/  265]
train() client id: f_00001-1-5 loss: 0.636279  [  192/  265]
train() client id: f_00001-1-6 loss: 0.536815  [  224/  265]
train() client id: f_00001-1-7 loss: 0.645379  [  256/  265]
train() client id: f_00001-2-0 loss: 0.538139  [   32/  265]
train() client id: f_00001-2-1 loss: 0.510051  [   64/  265]
train() client id: f_00001-2-2 loss: 0.581014  [   96/  265]
train() client id: f_00001-2-3 loss: 0.644624  [  128/  265]
train() client id: f_00001-2-4 loss: 0.507871  [  160/  265]
train() client id: f_00001-2-5 loss: 0.576821  [  192/  265]
train() client id: f_00001-2-6 loss: 0.463991  [  224/  265]
train() client id: f_00001-2-7 loss: 0.536713  [  256/  265]
train() client id: f_00001-3-0 loss: 0.547461  [   32/  265]
train() client id: f_00001-3-1 loss: 0.486609  [   64/  265]
train() client id: f_00001-3-2 loss: 0.553649  [   96/  265]
train() client id: f_00001-3-3 loss: 0.524888  [  128/  265]
train() client id: f_00001-3-4 loss: 0.588603  [  160/  265]
train() client id: f_00001-3-5 loss: 0.476399  [  192/  265]
train() client id: f_00001-3-6 loss: 0.558632  [  224/  265]
train() client id: f_00001-3-7 loss: 0.482811  [  256/  265]
train() client id: f_00001-4-0 loss: 0.556800  [   32/  265]
train() client id: f_00001-4-1 loss: 0.549980  [   64/  265]
train() client id: f_00001-4-2 loss: 0.593113  [   96/  265]
train() client id: f_00001-4-3 loss: 0.541394  [  128/  265]
train() client id: f_00001-4-4 loss: 0.468815  [  160/  265]
train() client id: f_00001-4-5 loss: 0.544279  [  192/  265]
train() client id: f_00001-4-6 loss: 0.434841  [  224/  265]
train() client id: f_00001-4-7 loss: 0.562953  [  256/  265]
train() client id: f_00001-5-0 loss: 0.544154  [   32/  265]
train() client id: f_00001-5-1 loss: 0.432425  [   64/  265]
train() client id: f_00001-5-2 loss: 0.559748  [   96/  265]
train() client id: f_00001-5-3 loss: 0.643611  [  128/  265]
train() client id: f_00001-5-4 loss: 0.460072  [  160/  265]
train() client id: f_00001-5-5 loss: 0.526926  [  192/  265]
train() client id: f_00001-5-6 loss: 0.573347  [  224/  265]
train() client id: f_00001-5-7 loss: 0.488590  [  256/  265]
train() client id: f_00001-6-0 loss: 0.520102  [   32/  265]
train() client id: f_00001-6-1 loss: 0.649373  [   64/  265]
train() client id: f_00001-6-2 loss: 0.601899  [   96/  265]
train() client id: f_00001-6-3 loss: 0.491498  [  128/  265]
train() client id: f_00001-6-4 loss: 0.429345  [  160/  265]
train() client id: f_00001-6-5 loss: 0.434230  [  192/  265]
train() client id: f_00001-6-6 loss: 0.556126  [  224/  265]
train() client id: f_00001-6-7 loss: 0.525461  [  256/  265]
train() client id: f_00001-7-0 loss: 0.471876  [   32/  265]
train() client id: f_00001-7-1 loss: 0.488833  [   64/  265]
train() client id: f_00001-7-2 loss: 0.548170  [   96/  265]
train() client id: f_00001-7-3 loss: 0.449124  [  128/  265]
train() client id: f_00001-7-4 loss: 0.633556  [  160/  265]
train() client id: f_00001-7-5 loss: 0.574391  [  192/  265]
train() client id: f_00001-7-6 loss: 0.592696  [  224/  265]
train() client id: f_00001-7-7 loss: 0.448484  [  256/  265]
train() client id: f_00001-8-0 loss: 0.451099  [   32/  265]
train() client id: f_00001-8-1 loss: 0.526081  [   64/  265]
train() client id: f_00001-8-2 loss: 0.468036  [   96/  265]
train() client id: f_00001-8-3 loss: 0.510662  [  128/  265]
train() client id: f_00001-8-4 loss: 0.628362  [  160/  265]
train() client id: f_00001-8-5 loss: 0.448466  [  192/  265]
train() client id: f_00001-8-6 loss: 0.643942  [  224/  265]
train() client id: f_00001-8-7 loss: 0.526395  [  256/  265]
train() client id: f_00001-9-0 loss: 0.553312  [   32/  265]
train() client id: f_00001-9-1 loss: 0.498476  [   64/  265]
train() client id: f_00001-9-2 loss: 0.498177  [   96/  265]
train() client id: f_00001-9-3 loss: 0.428744  [  128/  265]
train() client id: f_00001-9-4 loss: 0.436860  [  160/  265]
train() client id: f_00001-9-5 loss: 0.488245  [  192/  265]
train() client id: f_00001-9-6 loss: 0.526350  [  224/  265]
train() client id: f_00001-9-7 loss: 0.726382  [  256/  265]
train() client id: f_00001-10-0 loss: 0.533577  [   32/  265]
train() client id: f_00001-10-1 loss: 0.433048  [   64/  265]
train() client id: f_00001-10-2 loss: 0.507037  [   96/  265]
train() client id: f_00001-10-3 loss: 0.695236  [  128/  265]
train() client id: f_00001-10-4 loss: 0.548553  [  160/  265]
train() client id: f_00001-10-5 loss: 0.588340  [  192/  265]
train() client id: f_00001-10-6 loss: 0.431130  [  224/  265]
train() client id: f_00001-10-7 loss: 0.414374  [  256/  265]
train() client id: f_00001-11-0 loss: 0.675516  [   32/  265]
train() client id: f_00001-11-1 loss: 0.444610  [   64/  265]
train() client id: f_00001-11-2 loss: 0.491314  [   96/  265]
train() client id: f_00001-11-3 loss: 0.494208  [  128/  265]
train() client id: f_00001-11-4 loss: 0.443102  [  160/  265]
train() client id: f_00001-11-5 loss: 0.578116  [  192/  265]
train() client id: f_00001-11-6 loss: 0.536724  [  224/  265]
train() client id: f_00001-11-7 loss: 0.484546  [  256/  265]
train() client id: f_00002-0-0 loss: 1.265339  [   32/  124]
train() client id: f_00002-0-1 loss: 1.179966  [   64/  124]
train() client id: f_00002-0-2 loss: 1.185104  [   96/  124]
train() client id: f_00002-1-0 loss: 1.161658  [   32/  124]
train() client id: f_00002-1-1 loss: 1.154558  [   64/  124]
train() client id: f_00002-1-2 loss: 1.145157  [   96/  124]
train() client id: f_00002-2-0 loss: 1.182620  [   32/  124]
train() client id: f_00002-2-1 loss: 1.155211  [   64/  124]
train() client id: f_00002-2-2 loss: 1.101930  [   96/  124]
train() client id: f_00002-3-0 loss: 1.171223  [   32/  124]
train() client id: f_00002-3-1 loss: 1.066355  [   64/  124]
train() client id: f_00002-3-2 loss: 1.111131  [   96/  124]
train() client id: f_00002-4-0 loss: 1.072344  [   32/  124]
train() client id: f_00002-4-1 loss: 1.084384  [   64/  124]
train() client id: f_00002-4-2 loss: 1.087780  [   96/  124]
train() client id: f_00002-5-0 loss: 1.058615  [   32/  124]
train() client id: f_00002-5-1 loss: 1.029569  [   64/  124]
train() client id: f_00002-5-2 loss: 1.055774  [   96/  124]
train() client id: f_00002-6-0 loss: 1.071112  [   32/  124]
train() client id: f_00002-6-1 loss: 1.078841  [   64/  124]
train() client id: f_00002-6-2 loss: 0.982261  [   96/  124]
train() client id: f_00002-7-0 loss: 1.090562  [   32/  124]
train() client id: f_00002-7-1 loss: 1.077243  [   64/  124]
train() client id: f_00002-7-2 loss: 1.010149  [   96/  124]
train() client id: f_00002-8-0 loss: 0.944897  [   32/  124]
train() client id: f_00002-8-1 loss: 1.030236  [   64/  124]
train() client id: f_00002-8-2 loss: 1.115709  [   96/  124]
train() client id: f_00002-9-0 loss: 1.061801  [   32/  124]
train() client id: f_00002-9-1 loss: 1.082198  [   64/  124]
train() client id: f_00002-9-2 loss: 0.997160  [   96/  124]
train() client id: f_00002-10-0 loss: 1.030216  [   32/  124]
train() client id: f_00002-10-1 loss: 0.994869  [   64/  124]
train() client id: f_00002-10-2 loss: 0.968728  [   96/  124]
train() client id: f_00002-11-0 loss: 1.140674  [   32/  124]
train() client id: f_00002-11-1 loss: 0.953487  [   64/  124]
train() client id: f_00002-11-2 loss: 1.031671  [   96/  124]
train() client id: f_00003-0-0 loss: 1.031960  [   32/   43]
train() client id: f_00003-1-0 loss: 0.963348  [   32/   43]
train() client id: f_00003-2-0 loss: 0.966849  [   32/   43]
train() client id: f_00003-3-0 loss: 1.010316  [   32/   43]
train() client id: f_00003-4-0 loss: 1.047864  [   32/   43]
train() client id: f_00003-5-0 loss: 0.941819  [   32/   43]
train() client id: f_00003-6-0 loss: 1.074537  [   32/   43]
train() client id: f_00003-7-0 loss: 1.031819  [   32/   43]
train() client id: f_00003-8-0 loss: 0.991936  [   32/   43]
train() client id: f_00003-9-0 loss: 1.059105  [   32/   43]
train() client id: f_00003-10-0 loss: 0.954373  [   32/   43]
train() client id: f_00003-11-0 loss: 1.120890  [   32/   43]
train() client id: f_00004-0-0 loss: 0.999993  [   32/  306]
train() client id: f_00004-0-1 loss: 0.888440  [   64/  306]
train() client id: f_00004-0-2 loss: 0.920218  [   96/  306]
train() client id: f_00004-0-3 loss: 1.058876  [  128/  306]
train() client id: f_00004-0-4 loss: 1.040264  [  160/  306]
train() client id: f_00004-0-5 loss: 0.940124  [  192/  306]
train() client id: f_00004-0-6 loss: 0.931416  [  224/  306]
train() client id: f_00004-0-7 loss: 0.801638  [  256/  306]
train() client id: f_00004-0-8 loss: 0.971427  [  288/  306]
train() client id: f_00004-1-0 loss: 0.821836  [   32/  306]
train() client id: f_00004-1-1 loss: 0.937569  [   64/  306]
train() client id: f_00004-1-2 loss: 0.852012  [   96/  306]
train() client id: f_00004-1-3 loss: 0.999529  [  128/  306]
train() client id: f_00004-1-4 loss: 1.095752  [  160/  306]
train() client id: f_00004-1-5 loss: 0.981338  [  192/  306]
train() client id: f_00004-1-6 loss: 0.951338  [  224/  306]
train() client id: f_00004-1-7 loss: 0.861934  [  256/  306]
train() client id: f_00004-1-8 loss: 1.000387  [  288/  306]
train() client id: f_00004-2-0 loss: 0.836715  [   32/  306]
train() client id: f_00004-2-1 loss: 0.932643  [   64/  306]
train() client id: f_00004-2-2 loss: 0.944625  [   96/  306]
train() client id: f_00004-2-3 loss: 0.911591  [  128/  306]
train() client id: f_00004-2-4 loss: 0.997496  [  160/  306]
train() client id: f_00004-2-5 loss: 0.961490  [  192/  306]
train() client id: f_00004-2-6 loss: 0.884500  [  224/  306]
train() client id: f_00004-2-7 loss: 1.039666  [  256/  306]
train() client id: f_00004-2-8 loss: 0.924425  [  288/  306]
train() client id: f_00004-3-0 loss: 0.915188  [   32/  306]
train() client id: f_00004-3-1 loss: 0.823254  [   64/  306]
train() client id: f_00004-3-2 loss: 0.895816  [   96/  306]
train() client id: f_00004-3-3 loss: 0.944352  [  128/  306]
train() client id: f_00004-3-4 loss: 0.966626  [  160/  306]
train() client id: f_00004-3-5 loss: 0.975050  [  192/  306]
train() client id: f_00004-3-6 loss: 0.933556  [  224/  306]
train() client id: f_00004-3-7 loss: 0.959012  [  256/  306]
train() client id: f_00004-3-8 loss: 0.934302  [  288/  306]
train() client id: f_00004-4-0 loss: 0.945772  [   32/  306]
train() client id: f_00004-4-1 loss: 0.982643  [   64/  306]
train() client id: f_00004-4-2 loss: 0.895468  [   96/  306]
train() client id: f_00004-4-3 loss: 0.880326  [  128/  306]
train() client id: f_00004-4-4 loss: 0.889170  [  160/  306]
train() client id: f_00004-4-5 loss: 0.930175  [  192/  306]
train() client id: f_00004-4-6 loss: 0.951154  [  224/  306]
train() client id: f_00004-4-7 loss: 1.033222  [  256/  306]
train() client id: f_00004-4-8 loss: 0.846664  [  288/  306]
train() client id: f_00004-5-0 loss: 1.011486  [   32/  306]
train() client id: f_00004-5-1 loss: 0.931678  [   64/  306]
train() client id: f_00004-5-2 loss: 0.944424  [   96/  306]
train() client id: f_00004-5-3 loss: 0.872099  [  128/  306]
train() client id: f_00004-5-4 loss: 0.910390  [  160/  306]
train() client id: f_00004-5-5 loss: 0.938191  [  192/  306]
train() client id: f_00004-5-6 loss: 0.930213  [  224/  306]
train() client id: f_00004-5-7 loss: 0.874466  [  256/  306]
train() client id: f_00004-5-8 loss: 0.997913  [  288/  306]
train() client id: f_00004-6-0 loss: 0.966531  [   32/  306]
train() client id: f_00004-6-1 loss: 0.941981  [   64/  306]
train() client id: f_00004-6-2 loss: 0.818657  [   96/  306]
train() client id: f_00004-6-3 loss: 0.940133  [  128/  306]
train() client id: f_00004-6-4 loss: 0.922034  [  160/  306]
train() client id: f_00004-6-5 loss: 0.800924  [  192/  306]
train() client id: f_00004-6-6 loss: 0.942047  [  224/  306]
train() client id: f_00004-6-7 loss: 1.074589  [  256/  306]
train() client id: f_00004-6-8 loss: 0.930881  [  288/  306]
train() client id: f_00004-7-0 loss: 0.995500  [   32/  306]
train() client id: f_00004-7-1 loss: 0.840449  [   64/  306]
train() client id: f_00004-7-2 loss: 0.988655  [   96/  306]
train() client id: f_00004-7-3 loss: 0.937083  [  128/  306]
train() client id: f_00004-7-4 loss: 0.944063  [  160/  306]
train() client id: f_00004-7-5 loss: 0.871514  [  192/  306]
train() client id: f_00004-7-6 loss: 0.859704  [  224/  306]
train() client id: f_00004-7-7 loss: 0.935084  [  256/  306]
train() client id: f_00004-7-8 loss: 0.958929  [  288/  306]
train() client id: f_00004-8-0 loss: 0.913797  [   32/  306]
train() client id: f_00004-8-1 loss: 0.894605  [   64/  306]
train() client id: f_00004-8-2 loss: 0.935624  [   96/  306]
train() client id: f_00004-8-3 loss: 0.905753  [  128/  306]
train() client id: f_00004-8-4 loss: 0.948418  [  160/  306]
train() client id: f_00004-8-5 loss: 0.913892  [  192/  306]
train() client id: f_00004-8-6 loss: 0.915101  [  224/  306]
train() client id: f_00004-8-7 loss: 0.967808  [  256/  306]
train() client id: f_00004-8-8 loss: 0.927972  [  288/  306]
train() client id: f_00004-9-0 loss: 0.970669  [   32/  306]
train() client id: f_00004-9-1 loss: 0.897140  [   64/  306]
train() client id: f_00004-9-2 loss: 0.933662  [   96/  306]
train() client id: f_00004-9-3 loss: 0.876251  [  128/  306]
train() client id: f_00004-9-4 loss: 1.005550  [  160/  306]
train() client id: f_00004-9-5 loss: 0.960954  [  192/  306]
train() client id: f_00004-9-6 loss: 1.008641  [  224/  306]
train() client id: f_00004-9-7 loss: 0.917763  [  256/  306]
train() client id: f_00004-9-8 loss: 0.811252  [  288/  306]
train() client id: f_00004-10-0 loss: 0.845531  [   32/  306]
train() client id: f_00004-10-1 loss: 0.927633  [   64/  306]
train() client id: f_00004-10-2 loss: 0.852494  [   96/  306]
train() client id: f_00004-10-3 loss: 0.963951  [  128/  306]
train() client id: f_00004-10-4 loss: 0.871848  [  160/  306]
train() client id: f_00004-10-5 loss: 0.926435  [  192/  306]
train() client id: f_00004-10-6 loss: 0.918936  [  224/  306]
train() client id: f_00004-10-7 loss: 0.972227  [  256/  306]
train() client id: f_00004-10-8 loss: 0.978254  [  288/  306]
train() client id: f_00004-11-0 loss: 0.942621  [   32/  306]
train() client id: f_00004-11-1 loss: 0.921451  [   64/  306]
train() client id: f_00004-11-2 loss: 0.957115  [   96/  306]
train() client id: f_00004-11-3 loss: 0.848807  [  128/  306]
train() client id: f_00004-11-4 loss: 0.959332  [  160/  306]
train() client id: f_00004-11-5 loss: 1.016034  [  192/  306]
train() client id: f_00004-11-6 loss: 0.906382  [  224/  306]
train() client id: f_00004-11-7 loss: 0.849313  [  256/  306]
train() client id: f_00004-11-8 loss: 0.899120  [  288/  306]
train() client id: f_00005-0-0 loss: 0.807738  [   32/  146]
train() client id: f_00005-0-1 loss: 0.856432  [   64/  146]
train() client id: f_00005-0-2 loss: 0.917757  [   96/  146]
train() client id: f_00005-0-3 loss: 0.874583  [  128/  146]
train() client id: f_00005-1-0 loss: 0.796390  [   32/  146]
train() client id: f_00005-1-1 loss: 0.861726  [   64/  146]
train() client id: f_00005-1-2 loss: 0.875889  [   96/  146]
train() client id: f_00005-1-3 loss: 0.868033  [  128/  146]
train() client id: f_00005-2-0 loss: 0.765074  [   32/  146]
train() client id: f_00005-2-1 loss: 0.742475  [   64/  146]
train() client id: f_00005-2-2 loss: 0.928180  [   96/  146]
train() client id: f_00005-2-3 loss: 1.014148  [  128/  146]
train() client id: f_00005-3-0 loss: 0.837665  [   32/  146]
train() client id: f_00005-3-1 loss: 0.846114  [   64/  146]
train() client id: f_00005-3-2 loss: 0.751636  [   96/  146]
train() client id: f_00005-3-3 loss: 0.816972  [  128/  146]
train() client id: f_00005-4-0 loss: 0.915586  [   32/  146]
train() client id: f_00005-4-1 loss: 0.902781  [   64/  146]
train() client id: f_00005-4-2 loss: 0.851548  [   96/  146]
train() client id: f_00005-4-3 loss: 0.769655  [  128/  146]
train() client id: f_00005-5-0 loss: 0.809031  [   32/  146]
train() client id: f_00005-5-1 loss: 0.683606  [   64/  146]
train() client id: f_00005-5-2 loss: 0.865591  [   96/  146]
train() client id: f_00005-5-3 loss: 1.007161  [  128/  146]
train() client id: f_00005-6-0 loss: 0.751392  [   32/  146]
train() client id: f_00005-6-1 loss: 0.925007  [   64/  146]
train() client id: f_00005-6-2 loss: 0.927258  [   96/  146]
train() client id: f_00005-6-3 loss: 0.743606  [  128/  146]
train() client id: f_00005-7-0 loss: 0.831052  [   32/  146]
train() client id: f_00005-7-1 loss: 0.793063  [   64/  146]
train() client id: f_00005-7-2 loss: 0.763800  [   96/  146]
train() client id: f_00005-7-3 loss: 0.844634  [  128/  146]
train() client id: f_00005-8-0 loss: 0.664928  [   32/  146]
train() client id: f_00005-8-1 loss: 0.944904  [   64/  146]
train() client id: f_00005-8-2 loss: 0.979459  [   96/  146]
train() client id: f_00005-8-3 loss: 0.841906  [  128/  146]
train() client id: f_00005-9-0 loss: 0.869594  [   32/  146]
train() client id: f_00005-9-1 loss: 0.840242  [   64/  146]
train() client id: f_00005-9-2 loss: 0.893774  [   96/  146]
train() client id: f_00005-9-3 loss: 0.758930  [  128/  146]
train() client id: f_00005-10-0 loss: 0.849436  [   32/  146]
train() client id: f_00005-10-1 loss: 0.894495  [   64/  146]
train() client id: f_00005-10-2 loss: 0.918162  [   96/  146]
train() client id: f_00005-10-3 loss: 0.719213  [  128/  146]
train() client id: f_00005-11-0 loss: 0.800647  [   32/  146]
train() client id: f_00005-11-1 loss: 0.897658  [   64/  146]
train() client id: f_00005-11-2 loss: 0.772991  [   96/  146]
train() client id: f_00005-11-3 loss: 0.863420  [  128/  146]
train() client id: f_00006-0-0 loss: 0.809807  [   32/   54]
train() client id: f_00006-1-0 loss: 0.776972  [   32/   54]
train() client id: f_00006-2-0 loss: 0.765694  [   32/   54]
train() client id: f_00006-3-0 loss: 0.774533  [   32/   54]
train() client id: f_00006-4-0 loss: 0.815177  [   32/   54]
train() client id: f_00006-5-0 loss: 0.790878  [   32/   54]
train() client id: f_00006-6-0 loss: 0.794960  [   32/   54]
train() client id: f_00006-7-0 loss: 0.720454  [   32/   54]
train() client id: f_00006-8-0 loss: 0.808257  [   32/   54]
train() client id: f_00006-9-0 loss: 0.798367  [   32/   54]
train() client id: f_00006-10-0 loss: 0.773195  [   32/   54]
train() client id: f_00006-11-0 loss: 0.777742  [   32/   54]
train() client id: f_00007-0-0 loss: 0.686941  [   32/  179]
train() client id: f_00007-0-1 loss: 0.707566  [   64/  179]
train() client id: f_00007-0-2 loss: 0.626401  [   96/  179]
train() client id: f_00007-0-3 loss: 0.797632  [  128/  179]
train() client id: f_00007-0-4 loss: 0.643838  [  160/  179]
train() client id: f_00007-1-0 loss: 0.636616  [   32/  179]
train() client id: f_00007-1-1 loss: 0.692497  [   64/  179]
train() client id: f_00007-1-2 loss: 0.581493  [   96/  179]
train() client id: f_00007-1-3 loss: 0.753723  [  128/  179]
train() client id: f_00007-1-4 loss: 0.666277  [  160/  179]
train() client id: f_00007-2-0 loss: 0.801609  [   32/  179]
train() client id: f_00007-2-1 loss: 0.554914  [   64/  179]
train() client id: f_00007-2-2 loss: 0.581549  [   96/  179]
train() client id: f_00007-2-3 loss: 0.722958  [  128/  179]
train() client id: f_00007-2-4 loss: 0.595804  [  160/  179]
train() client id: f_00007-3-0 loss: 0.593048  [   32/  179]
train() client id: f_00007-3-1 loss: 0.558408  [   64/  179]
train() client id: f_00007-3-2 loss: 0.632905  [   96/  179]
train() client id: f_00007-3-3 loss: 0.529645  [  128/  179]
train() client id: f_00007-3-4 loss: 0.796239  [  160/  179]
train() client id: f_00007-4-0 loss: 0.668259  [   32/  179]
train() client id: f_00007-4-1 loss: 0.691236  [   64/  179]
train() client id: f_00007-4-2 loss: 0.665830  [   96/  179]
train() client id: f_00007-4-3 loss: 0.497467  [  128/  179]
train() client id: f_00007-4-4 loss: 0.518753  [  160/  179]
train() client id: f_00007-5-0 loss: 0.571656  [   32/  179]
train() client id: f_00007-5-1 loss: 0.608890  [   64/  179]
train() client id: f_00007-5-2 loss: 0.686899  [   96/  179]
train() client id: f_00007-5-3 loss: 0.566492  [  128/  179]
train() client id: f_00007-5-4 loss: 0.569063  [  160/  179]
train() client id: f_00007-6-0 loss: 0.613181  [   32/  179]
train() client id: f_00007-6-1 loss: 0.512012  [   64/  179]
train() client id: f_00007-6-2 loss: 0.532943  [   96/  179]
train() client id: f_00007-6-3 loss: 0.766960  [  128/  179]
train() client id: f_00007-6-4 loss: 0.546951  [  160/  179]
train() client id: f_00007-7-0 loss: 0.629787  [   32/  179]
train() client id: f_00007-7-1 loss: 0.541149  [   64/  179]
train() client id: f_00007-7-2 loss: 0.623353  [   96/  179]
train() client id: f_00007-7-3 loss: 0.632928  [  128/  179]
train() client id: f_00007-7-4 loss: 0.509947  [  160/  179]
train() client id: f_00007-8-0 loss: 0.482229  [   32/  179]
train() client id: f_00007-8-1 loss: 0.788667  [   64/  179]
train() client id: f_00007-8-2 loss: 0.569920  [   96/  179]
train() client id: f_00007-8-3 loss: 0.458017  [  128/  179]
train() client id: f_00007-8-4 loss: 0.626969  [  160/  179]
train() client id: f_00007-9-0 loss: 0.633750  [   32/  179]
train() client id: f_00007-9-1 loss: 0.556527  [   64/  179]
train() client id: f_00007-9-2 loss: 0.473106  [   96/  179]
train() client id: f_00007-9-3 loss: 0.612899  [  128/  179]
train() client id: f_00007-9-4 loss: 0.614813  [  160/  179]
train() client id: f_00007-10-0 loss: 0.465024  [   32/  179]
train() client id: f_00007-10-1 loss: 0.644033  [   64/  179]
train() client id: f_00007-10-2 loss: 0.615673  [   96/  179]
train() client id: f_00007-10-3 loss: 0.465506  [  128/  179]
train() client id: f_00007-10-4 loss: 0.620997  [  160/  179]
train() client id: f_00007-11-0 loss: 0.471642  [   32/  179]
train() client id: f_00007-11-1 loss: 0.634296  [   64/  179]
train() client id: f_00007-11-2 loss: 0.553100  [   96/  179]
train() client id: f_00007-11-3 loss: 0.459827  [  128/  179]
train() client id: f_00007-11-4 loss: 0.732829  [  160/  179]
train() client id: f_00008-0-0 loss: 0.764677  [   32/  130]
train() client id: f_00008-0-1 loss: 0.739046  [   64/  130]
train() client id: f_00008-0-2 loss: 0.866692  [   96/  130]
train() client id: f_00008-0-3 loss: 0.806610  [  128/  130]
train() client id: f_00008-1-0 loss: 0.827758  [   32/  130]
train() client id: f_00008-1-1 loss: 0.710649  [   64/  130]
train() client id: f_00008-1-2 loss: 0.843904  [   96/  130]
train() client id: f_00008-1-3 loss: 0.785639  [  128/  130]
train() client id: f_00008-2-0 loss: 0.804235  [   32/  130]
train() client id: f_00008-2-1 loss: 0.741399  [   64/  130]
train() client id: f_00008-2-2 loss: 0.860798  [   96/  130]
train() client id: f_00008-2-3 loss: 0.729671  [  128/  130]
train() client id: f_00008-3-0 loss: 0.876652  [   32/  130]
train() client id: f_00008-3-1 loss: 0.683566  [   64/  130]
train() client id: f_00008-3-2 loss: 0.751352  [   96/  130]
train() client id: f_00008-3-3 loss: 0.829603  [  128/  130]
train() client id: f_00008-4-0 loss: 0.735273  [   32/  130]
train() client id: f_00008-4-1 loss: 0.769321  [   64/  130]
train() client id: f_00008-4-2 loss: 0.817254  [   96/  130]
train() client id: f_00008-4-3 loss: 0.808564  [  128/  130]
train() client id: f_00008-5-0 loss: 0.794137  [   32/  130]
train() client id: f_00008-5-1 loss: 0.747129  [   64/  130]
train() client id: f_00008-5-2 loss: 0.840851  [   96/  130]
train() client id: f_00008-5-3 loss: 0.747234  [  128/  130]
train() client id: f_00008-6-0 loss: 0.780468  [   32/  130]
train() client id: f_00008-6-1 loss: 0.800416  [   64/  130]
train() client id: f_00008-6-2 loss: 0.841102  [   96/  130]
train() client id: f_00008-6-3 loss: 0.683854  [  128/  130]
train() client id: f_00008-7-0 loss: 0.625704  [   32/  130]
train() client id: f_00008-7-1 loss: 0.848225  [   64/  130]
train() client id: f_00008-7-2 loss: 0.790817  [   96/  130]
train() client id: f_00008-7-3 loss: 0.818895  [  128/  130]
train() client id: f_00008-8-0 loss: 0.747275  [   32/  130]
train() client id: f_00008-8-1 loss: 0.889151  [   64/  130]
train() client id: f_00008-8-2 loss: 0.780439  [   96/  130]
train() client id: f_00008-8-3 loss: 0.681396  [  128/  130]
train() client id: f_00008-9-0 loss: 0.869044  [   32/  130]
train() client id: f_00008-9-1 loss: 0.706291  [   64/  130]
train() client id: f_00008-9-2 loss: 0.759240  [   96/  130]
train() client id: f_00008-9-3 loss: 0.763857  [  128/  130]
train() client id: f_00008-10-0 loss: 0.754286  [   32/  130]
train() client id: f_00008-10-1 loss: 0.794869  [   64/  130]
train() client id: f_00008-10-2 loss: 0.706002  [   96/  130]
train() client id: f_00008-10-3 loss: 0.809645  [  128/  130]
train() client id: f_00008-11-0 loss: 0.756686  [   32/  130]
train() client id: f_00008-11-1 loss: 0.767545  [   64/  130]
train() client id: f_00008-11-2 loss: 0.765532  [   96/  130]
train() client id: f_00008-11-3 loss: 0.815216  [  128/  130]
train() client id: f_00009-0-0 loss: 1.141949  [   32/  118]
train() client id: f_00009-0-1 loss: 1.176450  [   64/  118]
train() client id: f_00009-0-2 loss: 1.263187  [   96/  118]
train() client id: f_00009-1-0 loss: 1.226137  [   32/  118]
train() client id: f_00009-1-1 loss: 1.053016  [   64/  118]
train() client id: f_00009-1-2 loss: 1.241197  [   96/  118]
train() client id: f_00009-2-0 loss: 1.139230  [   32/  118]
train() client id: f_00009-2-1 loss: 1.110255  [   64/  118]
train() client id: f_00009-2-2 loss: 1.110018  [   96/  118]
train() client id: f_00009-3-0 loss: 1.088736  [   32/  118]
train() client id: f_00009-3-1 loss: 1.077824  [   64/  118]
train() client id: f_00009-3-2 loss: 0.989700  [   96/  118]
train() client id: f_00009-4-0 loss: 1.039276  [   32/  118]
train() client id: f_00009-4-1 loss: 1.054166  [   64/  118]
train() client id: f_00009-4-2 loss: 1.092692  [   96/  118]
train() client id: f_00009-5-0 loss: 1.026839  [   32/  118]
train() client id: f_00009-5-1 loss: 0.898816  [   64/  118]
train() client id: f_00009-5-2 loss: 1.163960  [   96/  118]
train() client id: f_00009-6-0 loss: 0.993378  [   32/  118]
train() client id: f_00009-6-1 loss: 1.023676  [   64/  118]
train() client id: f_00009-6-2 loss: 1.041205  [   96/  118]
train() client id: f_00009-7-0 loss: 1.036789  [   32/  118]
train() client id: f_00009-7-1 loss: 0.918414  [   64/  118]
train() client id: f_00009-7-2 loss: 0.995368  [   96/  118]
train() client id: f_00009-8-0 loss: 1.039769  [   32/  118]
train() client id: f_00009-8-1 loss: 0.917954  [   64/  118]
train() client id: f_00009-8-2 loss: 0.985911  [   96/  118]
train() client id: f_00009-9-0 loss: 1.010358  [   32/  118]
train() client id: f_00009-9-1 loss: 0.938088  [   64/  118]
train() client id: f_00009-9-2 loss: 0.974729  [   96/  118]
train() client id: f_00009-10-0 loss: 0.965914  [   32/  118]
train() client id: f_00009-10-1 loss: 1.081830  [   64/  118]
train() client id: f_00009-10-2 loss: 0.925477  [   96/  118]
train() client id: f_00009-11-0 loss: 0.945176  [   32/  118]
train() client id: f_00009-11-1 loss: 1.047635  [   64/  118]
train() client id: f_00009-11-2 loss: 0.920240  [   96/  118]
At round 8 accuracy: 0.6286472148541115
At round 8 training accuracy: 0.5774647887323944
At round 8 training loss: 0.8771560070693506
update_location
xs = [ -3.9056584    4.20031788  60.00902392  18.81129433   0.97929623
   3.95640986 -22.44319194  -1.32485185  44.66397685   2.93912145]
ys = [ 52.5879595   35.55583871   1.32061395 -22.45517586  14.35018685
  -2.18584926  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [113.05196881 106.21610206 116.63115781 104.20220591 101.02913878
 100.10210345 102.5211559  100.0121567  110.92132708 100.1231756 ]
dists_bs = [210.37580989 227.0626082  292.22212521 276.67917212 238.27940567
 251.83371857 234.14571262 245.9693305  270.25317868 246.78497559]
uav_gains = [7.35860472e-11 8.60043309e-11 6.80689758e-11 9.02204198e-11
 9.74723702e-11 9.97448252e-11 9.39645390e-11 9.99692471e-11
 7.71712154e-11 9.96923511e-11]
bs_gains = [3.45864385e-11 2.79309909e-11 1.37815295e-11 1.60605338e-11
 2.44034856e-11 2.09013928e-11 2.56290612e-11 2.23268472e-11
 1.71528313e-11 2.21208434e-11]
Round 9
-------------------------------
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.80354861 20.47010187  9.65317002  3.45074232 23.61178753 11.38579797
  4.29063581 13.84678522 10.17374854  9.24174878]
obj_prev = 115.9280666591941
eta_min = 3.3564500217132474e-10	eta_max = 0.9202772988105504
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 26.95968104561057	eta = 0.909090909090909
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 46.612772647926946	eta = 0.5257958185768866
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 37.23476446800202	eta = 0.6582236063723965
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.55373220830825	eta = 0.6893453774967627
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.470824829161245	eta = 0.690956612049402
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.47060887359621	eta = 0.690960818797758
eta = 0.690960818797758
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [0.03050446 0.06415621 0.03002026 0.01041025 0.07408226 0.03534644
 0.01307334 0.0433357  0.03147286 0.02856767]
ene_total = [3.01933378 5.87455757 2.98817258 1.36705087 6.70212565 3.5823017
 1.57947799 4.03156066 3.30411446 3.0219136 ]
ti_comp = [0.29712306 0.28209195 0.2961178  0.29961782 0.27945843 0.27622759
 0.30009367 0.30080535 0.27174348 0.27743751]
ti_coms = [0.06670996 0.08174107 0.06771522 0.0642152  0.08437459 0.08760544
 0.06373935 0.06302768 0.09208954 0.08639551]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.00954316e-05 2.07402819e-04 1.92838817e-05 7.85468046e-07
 3.25378074e-04 3.61728644e-05 1.55069375e-06 5.62142497e-05
 2.63857967e-05 1.89310232e-05]
ene_total = [0.54344022 0.68073341 0.5515389  0.52160987 0.71170427 0.71445587
 0.51780721 0.51646678 0.75008018 0.70322868]
optimize_network_iter = 0 obj = 6.211065406906146
eta = 0.690960818797758
freqs = [5.13330407e+07 1.13715060e+08 5.06897303e+07 1.73725523e+07
 1.32546121e+08 6.39806453e+07 2.17820992e+07 7.20327985e+07
 5.79091277e+07 5.14848716e+07]
eta_min = 0.6909608187976134	eta_max = 0.6909608187977386
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 0.05319201668668125	eta = 0.9090909090909091
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 20.968149575428694	eta = 0.002306182461743776
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.200288580777995	eta = 0.021977289355824242
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.1330781928113622	eta = 0.022669763803801746
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.1330546285065273	eta = 0.022670014241468774
eta = 0.022670014241468774
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.05846218e-04 2.12451699e-03 1.97533160e-04 8.04588971e-06
 3.33298867e-03 3.70534332e-04 1.58844283e-05 5.75826930e-04
 2.70281154e-04 1.93918678e-04]
ene_total = [0.17661037 0.26448822 0.17897858 0.16512347 0.30228746 0.23450342
 0.1641027  0.17665537 0.24344476 0.22686028]
ti_comp = [0.29712306 0.28209195 0.2961178  0.29961782 0.27945843 0.27622759
 0.30009367 0.30080535 0.27174348 0.27743751]
ti_coms = [0.06670996 0.08174107 0.06771522 0.0642152  0.08437459 0.08760544
 0.06373935 0.06302768 0.09208954 0.08639551]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.00954316e-05 2.07402819e-04 1.92838817e-05 7.85468046e-07
 3.25378074e-04 3.61728644e-05 1.55069375e-06 5.62142497e-05
 2.63857967e-05 1.89310232e-05]
ene_total = [0.54344022 0.68073341 0.5515389  0.52160987 0.71170427 0.71445587
 0.51780721 0.51646678 0.75008018 0.70322868]
optimize_network_iter = 1 obj = 6.211065406903268
eta = 0.6909608187976134
freqs = [5.13330407e+07 1.13715060e+08 5.06897303e+07 1.73725523e+07
 1.32546121e+08 6.39806453e+07 2.17820992e+07 7.20327985e+07
 5.79091277e+07 5.14848716e+07]
Done!
At round 9 eta: 0.6909608187976134
At round 9 local rounds: 12.104944645341616
At round 9 global rounds: 81.21847247936174
At round 9 a_n: 24.7571443865612
gradient difference: 0.4049440920352936
train() client id: f_00000-0-0 loss: 1.503703  [   32/  126]
train() client id: f_00000-0-1 loss: 1.168631  [   64/  126]
train() client id: f_00000-0-2 loss: 1.366232  [   96/  126]
train() client id: f_00000-1-0 loss: 1.252643  [   32/  126]
train() client id: f_00000-1-1 loss: 1.134797  [   64/  126]
train() client id: f_00000-1-2 loss: 1.221097  [   96/  126]
train() client id: f_00000-2-0 loss: 1.130050  [   32/  126]
train() client id: f_00000-2-1 loss: 1.091983  [   64/  126]
train() client id: f_00000-2-2 loss: 1.122394  [   96/  126]
train() client id: f_00000-3-0 loss: 1.076564  [   32/  126]
train() client id: f_00000-3-1 loss: 1.112239  [   64/  126]
train() client id: f_00000-3-2 loss: 0.934748  [   96/  126]
train() client id: f_00000-4-0 loss: 0.946682  [   32/  126]
train() client id: f_00000-4-1 loss: 1.011040  [   64/  126]
train() client id: f_00000-4-2 loss: 0.962885  [   96/  126]
train() client id: f_00000-5-0 loss: 0.955745  [   32/  126]
train() client id: f_00000-5-1 loss: 0.989482  [   64/  126]
train() client id: f_00000-5-2 loss: 0.898280  [   96/  126]
train() client id: f_00000-6-0 loss: 0.936875  [   32/  126]
train() client id: f_00000-6-1 loss: 0.823043  [   64/  126]
train() client id: f_00000-6-2 loss: 0.951162  [   96/  126]
train() client id: f_00000-7-0 loss: 0.961151  [   32/  126]
train() client id: f_00000-7-1 loss: 0.857730  [   64/  126]
train() client id: f_00000-7-2 loss: 0.934308  [   96/  126]
train() client id: f_00000-8-0 loss: 0.885099  [   32/  126]
train() client id: f_00000-8-1 loss: 0.836383  [   64/  126]
train() client id: f_00000-8-2 loss: 0.977014  [   96/  126]
train() client id: f_00000-9-0 loss: 0.811054  [   32/  126]
train() client id: f_00000-9-1 loss: 0.816743  [   64/  126]
train() client id: f_00000-9-2 loss: 0.955888  [   96/  126]
train() client id: f_00000-10-0 loss: 0.899304  [   32/  126]
train() client id: f_00000-10-1 loss: 0.871436  [   64/  126]
train() client id: f_00000-10-2 loss: 0.857215  [   96/  126]
train() client id: f_00000-11-0 loss: 0.923952  [   32/  126]
train() client id: f_00000-11-1 loss: 0.778986  [   64/  126]
train() client id: f_00000-11-2 loss: 0.853558  [   96/  126]
train() client id: f_00001-0-0 loss: 0.497615  [   32/  265]
train() client id: f_00001-0-1 loss: 0.559804  [   64/  265]
train() client id: f_00001-0-2 loss: 0.668971  [   96/  265]
train() client id: f_00001-0-3 loss: 0.567294  [  128/  265]
train() client id: f_00001-0-4 loss: 0.566307  [  160/  265]
train() client id: f_00001-0-5 loss: 0.499671  [  192/  265]
train() client id: f_00001-0-6 loss: 0.519361  [  224/  265]
train() client id: f_00001-0-7 loss: 0.577305  [  256/  265]
train() client id: f_00001-1-0 loss: 0.487596  [   32/  265]
train() client id: f_00001-1-1 loss: 0.551723  [   64/  265]
train() client id: f_00001-1-2 loss: 0.546568  [   96/  265]
train() client id: f_00001-1-3 loss: 0.612586  [  128/  265]
train() client id: f_00001-1-4 loss: 0.490170  [  160/  265]
train() client id: f_00001-1-5 loss: 0.612345  [  192/  265]
train() client id: f_00001-1-6 loss: 0.570626  [  224/  265]
train() client id: f_00001-1-7 loss: 0.455141  [  256/  265]
train() client id: f_00001-2-0 loss: 0.517679  [   32/  265]
train() client id: f_00001-2-1 loss: 0.446209  [   64/  265]
train() client id: f_00001-2-2 loss: 0.492369  [   96/  265]
train() client id: f_00001-2-3 loss: 0.597974  [  128/  265]
train() client id: f_00001-2-4 loss: 0.467716  [  160/  265]
train() client id: f_00001-2-5 loss: 0.528687  [  192/  265]
train() client id: f_00001-2-6 loss: 0.551556  [  224/  265]
train() client id: f_00001-2-7 loss: 0.650330  [  256/  265]
train() client id: f_00001-3-0 loss: 0.559687  [   32/  265]
train() client id: f_00001-3-1 loss: 0.477053  [   64/  265]
train() client id: f_00001-3-2 loss: 0.461880  [   96/  265]
train() client id: f_00001-3-3 loss: 0.478365  [  128/  265]
train() client id: f_00001-3-4 loss: 0.585068  [  160/  265]
train() client id: f_00001-3-5 loss: 0.513555  [  192/  265]
train() client id: f_00001-3-6 loss: 0.615403  [  224/  265]
train() client id: f_00001-3-7 loss: 0.571977  [  256/  265]
train() client id: f_00001-4-0 loss: 0.481281  [   32/  265]
train() client id: f_00001-4-1 loss: 0.551977  [   64/  265]
train() client id: f_00001-4-2 loss: 0.528324  [   96/  265]
train() client id: f_00001-4-3 loss: 0.489389  [  128/  265]
train() client id: f_00001-4-4 loss: 0.564795  [  160/  265]
train() client id: f_00001-4-5 loss: 0.551981  [  192/  265]
train() client id: f_00001-4-6 loss: 0.564797  [  224/  265]
train() client id: f_00001-4-7 loss: 0.495955  [  256/  265]
train() client id: f_00001-5-0 loss: 0.470507  [   32/  265]
train() client id: f_00001-5-1 loss: 0.499042  [   64/  265]
train() client id: f_00001-5-2 loss: 0.495840  [   96/  265]
train() client id: f_00001-5-3 loss: 0.442032  [  128/  265]
train() client id: f_00001-5-4 loss: 0.515100  [  160/  265]
train() client id: f_00001-5-5 loss: 0.680001  [  192/  265]
train() client id: f_00001-5-6 loss: 0.498802  [  224/  265]
train() client id: f_00001-5-7 loss: 0.555612  [  256/  265]
train() client id: f_00001-6-0 loss: 0.495855  [   32/  265]
train() client id: f_00001-6-1 loss: 0.502102  [   64/  265]
train() client id: f_00001-6-2 loss: 0.591148  [   96/  265]
train() client id: f_00001-6-3 loss: 0.486645  [  128/  265]
train() client id: f_00001-6-4 loss: 0.509796  [  160/  265]
train() client id: f_00001-6-5 loss: 0.539176  [  192/  265]
train() client id: f_00001-6-6 loss: 0.638812  [  224/  265]
train() client id: f_00001-6-7 loss: 0.439404  [  256/  265]
train() client id: f_00001-7-0 loss: 0.539219  [   32/  265]
train() client id: f_00001-7-1 loss: 0.490679  [   64/  265]
train() client id: f_00001-7-2 loss: 0.569031  [   96/  265]
train() client id: f_00001-7-3 loss: 0.483735  [  128/  265]
train() client id: f_00001-7-4 loss: 0.494585  [  160/  265]
train() client id: f_00001-7-5 loss: 0.467988  [  192/  265]
train() client id: f_00001-7-6 loss: 0.459071  [  224/  265]
train() client id: f_00001-7-7 loss: 0.604707  [  256/  265]
train() client id: f_00001-8-0 loss: 0.590496  [   32/  265]
train() client id: f_00001-8-1 loss: 0.511813  [   64/  265]
train() client id: f_00001-8-2 loss: 0.506686  [   96/  265]
train() client id: f_00001-8-3 loss: 0.555376  [  128/  265]
train() client id: f_00001-8-4 loss: 0.478830  [  160/  265]
train() client id: f_00001-8-5 loss: 0.583675  [  192/  265]
train() client id: f_00001-8-6 loss: 0.515785  [  224/  265]
train() client id: f_00001-8-7 loss: 0.440308  [  256/  265]
train() client id: f_00001-9-0 loss: 0.479407  [   32/  265]
train() client id: f_00001-9-1 loss: 0.464694  [   64/  265]
train() client id: f_00001-9-2 loss: 0.594221  [   96/  265]
train() client id: f_00001-9-3 loss: 0.621140  [  128/  265]
train() client id: f_00001-9-4 loss: 0.586262  [  160/  265]
train() client id: f_00001-9-5 loss: 0.452259  [  192/  265]
train() client id: f_00001-9-6 loss: 0.485287  [  224/  265]
train() client id: f_00001-9-7 loss: 0.423452  [  256/  265]
train() client id: f_00001-10-0 loss: 0.607658  [   32/  265]
train() client id: f_00001-10-1 loss: 0.446536  [   64/  265]
train() client id: f_00001-10-2 loss: 0.544834  [   96/  265]
train() client id: f_00001-10-3 loss: 0.485037  [  128/  265]
train() client id: f_00001-10-4 loss: 0.599821  [  160/  265]
train() client id: f_00001-10-5 loss: 0.436182  [  192/  265]
train() client id: f_00001-10-6 loss: 0.513900  [  224/  265]
train() client id: f_00001-10-7 loss: 0.541777  [  256/  265]
train() client id: f_00001-11-0 loss: 0.431221  [   32/  265]
train() client id: f_00001-11-1 loss: 0.475388  [   64/  265]
train() client id: f_00001-11-2 loss: 0.541522  [   96/  265]
train() client id: f_00001-11-3 loss: 0.544553  [  128/  265]
train() client id: f_00001-11-4 loss: 0.646318  [  160/  265]
train() client id: f_00001-11-5 loss: 0.537245  [  192/  265]
train() client id: f_00001-11-6 loss: 0.519636  [  224/  265]
train() client id: f_00001-11-7 loss: 0.487618  [  256/  265]
train() client id: f_00002-0-0 loss: 1.189898  [   32/  124]
train() client id: f_00002-0-1 loss: 1.234678  [   64/  124]
train() client id: f_00002-0-2 loss: 1.274494  [   96/  124]
train() client id: f_00002-1-0 loss: 1.141968  [   32/  124]
train() client id: f_00002-1-1 loss: 1.220513  [   64/  124]
train() client id: f_00002-1-2 loss: 1.228635  [   96/  124]
train() client id: f_00002-2-0 loss: 1.140059  [   32/  124]
train() client id: f_00002-2-1 loss: 1.179734  [   64/  124]
train() client id: f_00002-2-2 loss: 1.216075  [   96/  124]
train() client id: f_00002-3-0 loss: 1.069293  [   32/  124]
train() client id: f_00002-3-1 loss: 1.244436  [   64/  124]
train() client id: f_00002-3-2 loss: 1.158493  [   96/  124]
train() client id: f_00002-4-0 loss: 1.066707  [   32/  124]
train() client id: f_00002-4-1 loss: 1.129440  [   64/  124]
train() client id: f_00002-4-2 loss: 1.186011  [   96/  124]
train() client id: f_00002-5-0 loss: 1.101000  [   32/  124]
train() client id: f_00002-5-1 loss: 1.071915  [   64/  124]
train() client id: f_00002-5-2 loss: 1.107597  [   96/  124]
train() client id: f_00002-6-0 loss: 1.059081  [   32/  124]
train() client id: f_00002-6-1 loss: 1.080977  [   64/  124]
train() client id: f_00002-6-2 loss: 1.074280  [   96/  124]
train() client id: f_00002-7-0 loss: 1.013799  [   32/  124]
train() client id: f_00002-7-1 loss: 1.041556  [   64/  124]
train() client id: f_00002-7-2 loss: 1.096942  [   96/  124]
train() client id: f_00002-8-0 loss: 1.017786  [   32/  124]
train() client id: f_00002-8-1 loss: 1.005809  [   64/  124]
train() client id: f_00002-8-2 loss: 1.053036  [   96/  124]
train() client id: f_00002-9-0 loss: 0.979542  [   32/  124]
train() client id: f_00002-9-1 loss: 1.033773  [   64/  124]
train() client id: f_00002-9-2 loss: 1.014333  [   96/  124]
train() client id: f_00002-10-0 loss: 1.012314  [   32/  124]
train() client id: f_00002-10-1 loss: 0.987512  [   64/  124]
train() client id: f_00002-10-2 loss: 1.056938  [   96/  124]
train() client id: f_00002-11-0 loss: 1.046898  [   32/  124]
train() client id: f_00002-11-1 loss: 1.004583  [   64/  124]
train() client id: f_00002-11-2 loss: 0.974504  [   96/  124]
train() client id: f_00003-0-0 loss: 0.878925  [   32/   43]
train() client id: f_00003-1-0 loss: 0.807980  [   32/   43]
train() client id: f_00003-2-0 loss: 0.874137  [   32/   43]
train() client id: f_00003-3-0 loss: 0.873523  [   32/   43]
train() client id: f_00003-4-0 loss: 0.781356  [   32/   43]
train() client id: f_00003-5-0 loss: 0.832233  [   32/   43]
train() client id: f_00003-6-0 loss: 0.883084  [   32/   43]
train() client id: f_00003-7-0 loss: 0.798139  [   32/   43]
train() client id: f_00003-8-0 loss: 0.904193  [   32/   43]
train() client id: f_00003-9-0 loss: 0.812926  [   32/   43]
train() client id: f_00003-10-0 loss: 0.890509  [   32/   43]
train() client id: f_00003-11-0 loss: 0.841215  [   32/   43]
train() client id: f_00004-0-0 loss: 0.891929  [   32/  306]
train() client id: f_00004-0-1 loss: 0.899078  [   64/  306]
train() client id: f_00004-0-2 loss: 0.888494  [   96/  306]
train() client id: f_00004-0-3 loss: 0.947685  [  128/  306]
train() client id: f_00004-0-4 loss: 0.860009  [  160/  306]
train() client id: f_00004-0-5 loss: 1.021938  [  192/  306]
train() client id: f_00004-0-6 loss: 0.962671  [  224/  306]
train() client id: f_00004-0-7 loss: 0.954638  [  256/  306]
train() client id: f_00004-0-8 loss: 0.938468  [  288/  306]
train() client id: f_00004-1-0 loss: 0.976677  [   32/  306]
train() client id: f_00004-1-1 loss: 0.861683  [   64/  306]
train() client id: f_00004-1-2 loss: 0.993980  [   96/  306]
train() client id: f_00004-1-3 loss: 0.769396  [  128/  306]
train() client id: f_00004-1-4 loss: 0.953334  [  160/  306]
train() client id: f_00004-1-5 loss: 0.919707  [  192/  306]
train() client id: f_00004-1-6 loss: 1.096313  [  224/  306]
train() client id: f_00004-1-7 loss: 0.788669  [  256/  306]
train() client id: f_00004-1-8 loss: 0.907476  [  288/  306]
train() client id: f_00004-2-0 loss: 0.850516  [   32/  306]
train() client id: f_00004-2-1 loss: 1.018111  [   64/  306]
train() client id: f_00004-2-2 loss: 0.984660  [   96/  306]
train() client id: f_00004-2-3 loss: 0.962150  [  128/  306]
train() client id: f_00004-2-4 loss: 0.926717  [  160/  306]
train() client id: f_00004-2-5 loss: 0.900234  [  192/  306]
train() client id: f_00004-2-6 loss: 0.907994  [  224/  306]
train() client id: f_00004-2-7 loss: 0.980818  [  256/  306]
train() client id: f_00004-2-8 loss: 0.854661  [  288/  306]
train() client id: f_00004-3-0 loss: 0.811933  [   32/  306]
train() client id: f_00004-3-1 loss: 0.953338  [   64/  306]
train() client id: f_00004-3-2 loss: 1.033102  [   96/  306]
train() client id: f_00004-3-3 loss: 0.916849  [  128/  306]
train() client id: f_00004-3-4 loss: 0.992195  [  160/  306]
train() client id: f_00004-3-5 loss: 0.888901  [  192/  306]
train() client id: f_00004-3-6 loss: 0.879415  [  224/  306]
train() client id: f_00004-3-7 loss: 0.894102  [  256/  306]
train() client id: f_00004-3-8 loss: 0.847986  [  288/  306]
train() client id: f_00004-4-0 loss: 0.925548  [   32/  306]
train() client id: f_00004-4-1 loss: 0.941590  [   64/  306]
train() client id: f_00004-4-2 loss: 0.938247  [   96/  306]
train() client id: f_00004-4-3 loss: 0.912974  [  128/  306]
train() client id: f_00004-4-4 loss: 0.909140  [  160/  306]
train() client id: f_00004-4-5 loss: 0.935514  [  192/  306]
train() client id: f_00004-4-6 loss: 0.945090  [  224/  306]
train() client id: f_00004-4-7 loss: 0.862120  [  256/  306]
train() client id: f_00004-4-8 loss: 0.968262  [  288/  306]
train() client id: f_00004-5-0 loss: 0.880909  [   32/  306]
train() client id: f_00004-5-1 loss: 1.002810  [   64/  306]
train() client id: f_00004-5-2 loss: 0.888038  [   96/  306]
train() client id: f_00004-5-3 loss: 0.873537  [  128/  306]
train() client id: f_00004-5-4 loss: 0.801370  [  160/  306]
train() client id: f_00004-5-5 loss: 0.872306  [  192/  306]
train() client id: f_00004-5-6 loss: 1.000392  [  224/  306]
train() client id: f_00004-5-7 loss: 0.947398  [  256/  306]
train() client id: f_00004-5-8 loss: 0.910602  [  288/  306]
train() client id: f_00004-6-0 loss: 0.933703  [   32/  306]
train() client id: f_00004-6-1 loss: 0.889584  [   64/  306]
train() client id: f_00004-6-2 loss: 0.924018  [   96/  306]
train() client id: f_00004-6-3 loss: 0.894742  [  128/  306]
train() client id: f_00004-6-4 loss: 0.942284  [  160/  306]
train() client id: f_00004-6-5 loss: 0.916028  [  192/  306]
train() client id: f_00004-6-6 loss: 0.942073  [  224/  306]
train() client id: f_00004-6-7 loss: 0.842722  [  256/  306]
train() client id: f_00004-6-8 loss: 1.026426  [  288/  306]
train() client id: f_00004-7-0 loss: 0.928840  [   32/  306]
train() client id: f_00004-7-1 loss: 0.888943  [   64/  306]
train() client id: f_00004-7-2 loss: 0.794789  [   96/  306]
train() client id: f_00004-7-3 loss: 0.910736  [  128/  306]
train() client id: f_00004-7-4 loss: 0.910860  [  160/  306]
train() client id: f_00004-7-5 loss: 0.905155  [  192/  306]
train() client id: f_00004-7-6 loss: 0.907054  [  224/  306]
train() client id: f_00004-7-7 loss: 1.037260  [  256/  306]
train() client id: f_00004-7-8 loss: 0.936579  [  288/  306]
train() client id: f_00004-8-0 loss: 0.827935  [   32/  306]
train() client id: f_00004-8-1 loss: 0.981290  [   64/  306]
train() client id: f_00004-8-2 loss: 0.965241  [   96/  306]
train() client id: f_00004-8-3 loss: 0.904356  [  128/  306]
train() client id: f_00004-8-4 loss: 0.891114  [  160/  306]
train() client id: f_00004-8-5 loss: 0.923433  [  192/  306]
train() client id: f_00004-8-6 loss: 0.884744  [  224/  306]
train() client id: f_00004-8-7 loss: 1.021721  [  256/  306]
train() client id: f_00004-8-8 loss: 0.945076  [  288/  306]
train() client id: f_00004-9-0 loss: 0.827051  [   32/  306]
train() client id: f_00004-9-1 loss: 0.885670  [   64/  306]
train() client id: f_00004-9-2 loss: 1.008850  [   96/  306]
train() client id: f_00004-9-3 loss: 0.873251  [  128/  306]
train() client id: f_00004-9-4 loss: 0.964042  [  160/  306]
train() client id: f_00004-9-5 loss: 0.843307  [  192/  306]
train() client id: f_00004-9-6 loss: 0.897058  [  224/  306]
train() client id: f_00004-9-7 loss: 0.943691  [  256/  306]
train() client id: f_00004-9-8 loss: 0.992894  [  288/  306]
train() client id: f_00004-10-0 loss: 0.819356  [   32/  306]
train() client id: f_00004-10-1 loss: 0.978284  [   64/  306]
train() client id: f_00004-10-2 loss: 0.992894  [   96/  306]
train() client id: f_00004-10-3 loss: 0.950124  [  128/  306]
train() client id: f_00004-10-4 loss: 0.897852  [  160/  306]
train() client id: f_00004-10-5 loss: 0.883052  [  192/  306]
train() client id: f_00004-10-6 loss: 0.904789  [  224/  306]
train() client id: f_00004-10-7 loss: 0.947888  [  256/  306]
train() client id: f_00004-10-8 loss: 0.899088  [  288/  306]
train() client id: f_00004-11-0 loss: 0.909904  [   32/  306]
train() client id: f_00004-11-1 loss: 0.925417  [   64/  306]
train() client id: f_00004-11-2 loss: 0.872269  [   96/  306]
train() client id: f_00004-11-3 loss: 1.002881  [  128/  306]
train() client id: f_00004-11-4 loss: 0.939887  [  160/  306]
train() client id: f_00004-11-5 loss: 0.931245  [  192/  306]
train() client id: f_00004-11-6 loss: 0.896915  [  224/  306]
train() client id: f_00004-11-7 loss: 0.926459  [  256/  306]
train() client id: f_00004-11-8 loss: 0.916811  [  288/  306]
train() client id: f_00005-0-0 loss: 0.797352  [   32/  146]
train() client id: f_00005-0-1 loss: 0.821843  [   64/  146]
train() client id: f_00005-0-2 loss: 0.901219  [   96/  146]
train() client id: f_00005-0-3 loss: 0.801724  [  128/  146]
train() client id: f_00005-1-0 loss: 0.786562  [   32/  146]
train() client id: f_00005-1-1 loss: 0.644621  [   64/  146]
train() client id: f_00005-1-2 loss: 0.939630  [   96/  146]
train() client id: f_00005-1-3 loss: 0.830296  [  128/  146]
train() client id: f_00005-2-0 loss: 0.728662  [   32/  146]
train() client id: f_00005-2-1 loss: 0.752665  [   64/  146]
train() client id: f_00005-2-2 loss: 0.927678  [   96/  146]
train() client id: f_00005-2-3 loss: 0.784296  [  128/  146]
train() client id: f_00005-3-0 loss: 0.791670  [   32/  146]
train() client id: f_00005-3-1 loss: 0.837551  [   64/  146]
train() client id: f_00005-3-2 loss: 0.680698  [   96/  146]
train() client id: f_00005-3-3 loss: 0.801950  [  128/  146]
train() client id: f_00005-4-0 loss: 0.769102  [   32/  146]
train() client id: f_00005-4-1 loss: 0.767418  [   64/  146]
train() client id: f_00005-4-2 loss: 0.739973  [   96/  146]
train() client id: f_00005-4-3 loss: 0.720747  [  128/  146]
train() client id: f_00005-5-0 loss: 0.774097  [   32/  146]
train() client id: f_00005-5-1 loss: 0.711506  [   64/  146]
train() client id: f_00005-5-2 loss: 0.844060  [   96/  146]
train() client id: f_00005-5-3 loss: 0.661240  [  128/  146]
train() client id: f_00005-6-0 loss: 0.714357  [   32/  146]
train() client id: f_00005-6-1 loss: 0.794286  [   64/  146]
train() client id: f_00005-6-2 loss: 0.769388  [   96/  146]
train() client id: f_00005-6-3 loss: 0.784833  [  128/  146]
train() client id: f_00005-7-0 loss: 0.706125  [   32/  146]
train() client id: f_00005-7-1 loss: 0.722152  [   64/  146]
train() client id: f_00005-7-2 loss: 0.800612  [   96/  146]
train() client id: f_00005-7-3 loss: 0.777204  [  128/  146]
train() client id: f_00005-8-0 loss: 0.713856  [   32/  146]
train() client id: f_00005-8-1 loss: 0.842535  [   64/  146]
train() client id: f_00005-8-2 loss: 0.713650  [   96/  146]
train() client id: f_00005-8-3 loss: 0.710670  [  128/  146]
train() client id: f_00005-9-0 loss: 0.813340  [   32/  146]
train() client id: f_00005-9-1 loss: 0.691540  [   64/  146]
train() client id: f_00005-9-2 loss: 0.880542  [   96/  146]
train() client id: f_00005-9-3 loss: 0.711008  [  128/  146]
train() client id: f_00005-10-0 loss: 0.894616  [   32/  146]
train() client id: f_00005-10-1 loss: 0.734496  [   64/  146]
train() client id: f_00005-10-2 loss: 0.723512  [   96/  146]
train() client id: f_00005-10-3 loss: 0.849015  [  128/  146]
train() client id: f_00005-11-0 loss: 0.615012  [   32/  146]
train() client id: f_00005-11-1 loss: 0.722635  [   64/  146]
train() client id: f_00005-11-2 loss: 0.880671  [   96/  146]
train() client id: f_00005-11-3 loss: 0.794460  [  128/  146]
train() client id: f_00006-0-0 loss: 0.799287  [   32/   54]
train() client id: f_00006-1-0 loss: 0.767275  [   32/   54]
train() client id: f_00006-2-0 loss: 0.746116  [   32/   54]
train() client id: f_00006-3-0 loss: 0.758323  [   32/   54]
train() client id: f_00006-4-0 loss: 0.763313  [   32/   54]
train() client id: f_00006-5-0 loss: 0.752397  [   32/   54]
train() client id: f_00006-6-0 loss: 0.767696  [   32/   54]
train() client id: f_00006-7-0 loss: 0.727339  [   32/   54]
train() client id: f_00006-8-0 loss: 0.753002  [   32/   54]
train() client id: f_00006-9-0 loss: 0.757809  [   32/   54]
train() client id: f_00006-10-0 loss: 0.775059  [   32/   54]
train() client id: f_00006-11-0 loss: 0.803747  [   32/   54]
train() client id: f_00007-0-0 loss: 0.655851  [   32/  179]
train() client id: f_00007-0-1 loss: 0.725308  [   64/  179]
train() client id: f_00007-0-2 loss: 0.681069  [   96/  179]
train() client id: f_00007-0-3 loss: 0.637732  [  128/  179]
train() client id: f_00007-0-4 loss: 0.556902  [  160/  179]
train() client id: f_00007-1-0 loss: 0.638489  [   32/  179]
train() client id: f_00007-1-1 loss: 0.538256  [   64/  179]
train() client id: f_00007-1-2 loss: 0.515893  [   96/  179]
train() client id: f_00007-1-3 loss: 0.644769  [  128/  179]
train() client id: f_00007-1-4 loss: 0.612728  [  160/  179]
train() client id: f_00007-2-0 loss: 0.580830  [   32/  179]
train() client id: f_00007-2-1 loss: 0.632839  [   64/  179]
train() client id: f_00007-2-2 loss: 0.648720  [   96/  179]
train() client id: f_00007-2-3 loss: 0.594509  [  128/  179]
train() client id: f_00007-2-4 loss: 0.525048  [  160/  179]
train() client id: f_00007-3-0 loss: 0.539613  [   32/  179]
train() client id: f_00007-3-1 loss: 0.526220  [   64/  179]
train() client id: f_00007-3-2 loss: 0.448418  [   96/  179]
train() client id: f_00007-3-3 loss: 0.660950  [  128/  179]
train() client id: f_00007-3-4 loss: 0.563788  [  160/  179]
train() client id: f_00007-4-0 loss: 0.596258  [   32/  179]
train() client id: f_00007-4-1 loss: 0.699142  [   64/  179]
train() client id: f_00007-4-2 loss: 0.525545  [   96/  179]
train() client id: f_00007-4-3 loss: 0.449903  [  128/  179]
train() client id: f_00007-4-4 loss: 0.446277  [  160/  179]
train() client id: f_00007-5-0 loss: 0.547779  [   32/  179]
train() client id: f_00007-5-1 loss: 0.533297  [   64/  179]
train() client id: f_00007-5-2 loss: 0.645457  [   96/  179]
train() client id: f_00007-5-3 loss: 0.473052  [  128/  179]
train() client id: f_00007-5-4 loss: 0.427052  [  160/  179]
train() client id: f_00007-6-0 loss: 0.500819  [   32/  179]
train() client id: f_00007-6-1 loss: 0.614628  [   64/  179]
train() client id: f_00007-6-2 loss: 0.575380  [   96/  179]
train() client id: f_00007-6-3 loss: 0.481417  [  128/  179]
train() client id: f_00007-6-4 loss: 0.396024  [  160/  179]
train() client id: f_00007-7-0 loss: 0.485331  [   32/  179]
train() client id: f_00007-7-1 loss: 0.480805  [   64/  179]
train() client id: f_00007-7-2 loss: 0.537276  [   96/  179]
train() client id: f_00007-7-3 loss: 0.524377  [  128/  179]
train() client id: f_00007-7-4 loss: 0.554825  [  160/  179]
train() client id: f_00007-8-0 loss: 0.427003  [   32/  179]
train() client id: f_00007-8-1 loss: 0.543061  [   64/  179]
train() client id: f_00007-8-2 loss: 0.608432  [   96/  179]
train() client id: f_00007-8-3 loss: 0.481813  [  128/  179]
train() client id: f_00007-8-4 loss: 0.401246  [  160/  179]
train() client id: f_00007-9-0 loss: 0.379610  [   32/  179]
train() client id: f_00007-9-1 loss: 0.447712  [   64/  179]
train() client id: f_00007-9-2 loss: 0.380625  [   96/  179]
train() client id: f_00007-9-3 loss: 0.505518  [  128/  179]
train() client id: f_00007-9-4 loss: 0.638122  [  160/  179]
train() client id: f_00007-10-0 loss: 0.471395  [   32/  179]
train() client id: f_00007-10-1 loss: 0.499008  [   64/  179]
train() client id: f_00007-10-2 loss: 0.429637  [   96/  179]
train() client id: f_00007-10-3 loss: 0.434239  [  128/  179]
train() client id: f_00007-10-4 loss: 0.502864  [  160/  179]
train() client id: f_00007-11-0 loss: 0.470053  [   32/  179]
train() client id: f_00007-11-1 loss: 0.537901  [   64/  179]
train() client id: f_00007-11-2 loss: 0.552714  [   96/  179]
train() client id: f_00007-11-3 loss: 0.438469  [  128/  179]
train() client id: f_00007-11-4 loss: 0.447308  [  160/  179]
train() client id: f_00008-0-0 loss: 0.819691  [   32/  130]
train() client id: f_00008-0-1 loss: 0.764733  [   64/  130]
train() client id: f_00008-0-2 loss: 0.698926  [   96/  130]
train() client id: f_00008-0-3 loss: 0.859695  [  128/  130]
train() client id: f_00008-1-0 loss: 0.794223  [   32/  130]
train() client id: f_00008-1-1 loss: 0.786555  [   64/  130]
train() client id: f_00008-1-2 loss: 0.774603  [   96/  130]
train() client id: f_00008-1-3 loss: 0.784120  [  128/  130]
train() client id: f_00008-2-0 loss: 0.870911  [   32/  130]
train() client id: f_00008-2-1 loss: 0.772884  [   64/  130]
train() client id: f_00008-2-2 loss: 0.747294  [   96/  130]
train() client id: f_00008-2-3 loss: 0.742024  [  128/  130]
train() client id: f_00008-3-0 loss: 0.739995  [   32/  130]
train() client id: f_00008-3-1 loss: 0.849483  [   64/  130]
train() client id: f_00008-3-2 loss: 0.715301  [   96/  130]
train() client id: f_00008-3-3 loss: 0.829006  [  128/  130]
train() client id: f_00008-4-0 loss: 0.859552  [   32/  130]
train() client id: f_00008-4-1 loss: 0.724918  [   64/  130]
train() client id: f_00008-4-2 loss: 0.751006  [   96/  130]
train() client id: f_00008-4-3 loss: 0.784390  [  128/  130]
train() client id: f_00008-5-0 loss: 0.814791  [   32/  130]
train() client id: f_00008-5-1 loss: 0.769683  [   64/  130]
train() client id: f_00008-5-2 loss: 0.792675  [   96/  130]
train() client id: f_00008-5-3 loss: 0.708961  [  128/  130]
train() client id: f_00008-6-0 loss: 0.782525  [   32/  130]
train() client id: f_00008-6-1 loss: 0.772029  [   64/  130]
train() client id: f_00008-6-2 loss: 0.787545  [   96/  130]
train() client id: f_00008-6-3 loss: 0.732601  [  128/  130]
train() client id: f_00008-7-0 loss: 0.807686  [   32/  130]
train() client id: f_00008-7-1 loss: 0.766416  [   64/  130]
train() client id: f_00008-7-2 loss: 0.698020  [   96/  130]
train() client id: f_00008-7-3 loss: 0.817346  [  128/  130]
train() client id: f_00008-8-0 loss: 0.766502  [   32/  130]
train() client id: f_00008-8-1 loss: 0.748206  [   64/  130]
train() client id: f_00008-8-2 loss: 0.877692  [   96/  130]
train() client id: f_00008-8-3 loss: 0.698846  [  128/  130]
train() client id: f_00008-9-0 loss: 0.793167  [   32/  130]
train() client id: f_00008-9-1 loss: 0.737952  [   64/  130]
train() client id: f_00008-9-2 loss: 0.900801  [   96/  130]
train() client id: f_00008-9-3 loss: 0.658544  [  128/  130]
train() client id: f_00008-10-0 loss: 0.706332  [   32/  130]
train() client id: f_00008-10-1 loss: 0.796408  [   64/  130]
train() client id: f_00008-10-2 loss: 0.791160  [   96/  130]
train() client id: f_00008-10-3 loss: 0.789429  [  128/  130]
train() client id: f_00008-11-0 loss: 0.668211  [   32/  130]
train() client id: f_00008-11-1 loss: 0.851937  [   64/  130]
train() client id: f_00008-11-2 loss: 0.833353  [   96/  130]
train() client id: f_00008-11-3 loss: 0.731051  [  128/  130]
train() client id: f_00009-0-0 loss: 1.161159  [   32/  118]
train() client id: f_00009-0-1 loss: 1.312787  [   64/  118]
train() client id: f_00009-0-2 loss: 1.266629  [   96/  118]
train() client id: f_00009-1-0 loss: 1.163004  [   32/  118]
train() client id: f_00009-1-1 loss: 1.262515  [   64/  118]
train() client id: f_00009-1-2 loss: 1.123988  [   96/  118]
train() client id: f_00009-2-0 loss: 1.228379  [   32/  118]
train() client id: f_00009-2-1 loss: 1.189466  [   64/  118]
train() client id: f_00009-2-2 loss: 1.043322  [   96/  118]
train() client id: f_00009-3-0 loss: 1.121357  [   32/  118]
train() client id: f_00009-3-1 loss: 1.031805  [   64/  118]
train() client id: f_00009-3-2 loss: 1.114685  [   96/  118]
train() client id: f_00009-4-0 loss: 1.106257  [   32/  118]
train() client id: f_00009-4-1 loss: 1.113068  [   64/  118]
train() client id: f_00009-4-2 loss: 1.010139  [   96/  118]
train() client id: f_00009-5-0 loss: 1.030685  [   32/  118]
train() client id: f_00009-5-1 loss: 0.990760  [   64/  118]
train() client id: f_00009-5-2 loss: 1.114435  [   96/  118]
train() client id: f_00009-6-0 loss: 1.091153  [   32/  118]
train() client id: f_00009-6-1 loss: 1.075250  [   64/  118]
train() client id: f_00009-6-2 loss: 0.984779  [   96/  118]
train() client id: f_00009-7-0 loss: 0.982851  [   32/  118]
train() client id: f_00009-7-1 loss: 1.035473  [   64/  118]
train() client id: f_00009-7-2 loss: 0.998614  [   96/  118]
train() client id: f_00009-8-0 loss: 1.065310  [   32/  118]
train() client id: f_00009-8-1 loss: 1.015516  [   64/  118]
train() client id: f_00009-8-2 loss: 0.936777  [   96/  118]
train() client id: f_00009-9-0 loss: 1.053890  [   32/  118]
train() client id: f_00009-9-1 loss: 0.968621  [   64/  118]
train() client id: f_00009-9-2 loss: 1.071180  [   96/  118]
train() client id: f_00009-10-0 loss: 0.945792  [   32/  118]
train() client id: f_00009-10-1 loss: 1.043818  [   64/  118]
train() client id: f_00009-10-2 loss: 1.051749  [   96/  118]
train() client id: f_00009-11-0 loss: 1.114141  [   32/  118]
train() client id: f_00009-11-1 loss: 0.999007  [   64/  118]
train() client id: f_00009-11-2 loss: 1.025122  [   96/  118]
At round 9 accuracy: 0.6286472148541115
At round 9 training accuracy: 0.5700871898054997
At round 9 training loss: 0.8825963764195138
update_location
xs = [ -3.9056584    4.20031788  65.00902392  18.81129433   0.97929623
   3.95640986 -27.44319194  -6.32485185  49.66397685   2.93912145]
ys = [ 57.5879595   40.55583871   1.32061395 -27.45517586  19.35018685
   2.81415074  -2.62498432   0.82234798  17.56900603  -0.99851822]
dists_uav = [115.46266603 107.99267903 119.28083338 105.39284357 101.85965223
 100.11779374 103.730513   100.2031936  113.02734434 100.04816577]
dists_bs = [207.50629142 224.02675383 296.25809812 280.26936337 234.93738961
 248.34122355 230.91913446 242.46476046 274.3326819  250.27546695]
uav_gains = [6.98045343e-11 8.25105385e-11 6.43507590e-11 8.76937845e-11
 9.54975988e-11 9.97057494e-11 9.12496048e-11 9.94934409e-11
 7.36261378e-11 9.98793178e-11]
bs_gains = [3.59423553e-11 2.90037668e-11 1.32622570e-11 1.54911026e-11
 2.53879770e-11 2.17348876e-11 2.66444222e-11 2.32422371e-11
 1.64481458e-11 2.12678163e-11]
Round 10
-------------------------------
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.67144915 20.18897006  9.52328793  3.40449035 23.28757639 11.22831614
  4.23293599 13.65794809 10.03742584  9.11771254]
obj_prev = 114.35011248363482
eta_min = 2.485131023397881e-10	eta_max = 0.9203875705067922
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 26.5917511352464	eta = 0.909090909090909
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 46.02197555438805	eta = 0.5252777379209097
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 36.745409717816976	eta = 0.6578867782263103
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 35.08220734696584	eta = 0.6890763450194114
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 35.00007514490989	eta = 0.6906933517648766
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 34.99986062549486	eta = 0.690697585128414
eta = 0.690697585128414
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [0.0305359  0.06422234 0.03005121 0.01042098 0.07415862 0.03538287
 0.01308682 0.04338037 0.0315053  0.02859712]
ene_total = [2.98353072 5.7890883  2.95330941 1.35079969 6.60483527 3.5266156
 1.5604156  3.97716561 3.26691924 2.98718119]
ti_comp = [0.30116936 0.28752245 0.30009808 0.30400477 0.28497028 0.28178895
 0.30447482 0.30547464 0.27545826 0.28132541]
ti_coms = [0.06738721 0.08103412 0.06845849 0.0645518  0.08358629 0.08676762
 0.06408176 0.06308193 0.09309831 0.08723116]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.96196272e-05 2.00260269e-04 1.88338590e-05 7.65323874e-07
 3.13881583e-04 3.48667396e-05 1.51105132e-06 5.46776000e-05
 2.57584149e-05 1.84683789e-05]
ene_total = [0.54095026 0.66464151 0.5494621  0.51674598 0.69416402 0.6972951
 0.51304332 0.50929606 0.74723805 0.69969276]
optimize_network_iter = 0 obj = 6.132529161591935
eta = 0.690697585128414
freqs = [5.06955673e+07 1.11682299e+08 5.00689748e+07 1.71395055e+07
 1.30116415e+08 6.27825752e+07 2.14908014e+07 7.10048625e+07
 5.71870607e+07 5.08256882e+07]
eta_min = 0.6906975851284028	eta_max = 0.6906975851284034
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 0.050678425323249086	eta = 0.9090909090909091
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 20.72563143256374	eta = 0.002222913974819694
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.1660187348301907	eta = 0.02127003566846808
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.101859368191834	eta = 0.021919304614581324
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.101838286505997	eta = 0.02191952446779107
eta = 0.02191952446779107
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [2.02550289e-04 2.06745903e-03 1.94438128e-04 7.90109671e-06
 3.24046959e-03 3.59959346e-04 1.55998826e-05 5.64483901e-04
 2.65926276e-04 1.90664962e-04]
ene_total = [0.17569726 0.2574448  0.17820355 0.16359333 0.29359605 0.22873723
 0.16259842 0.17396099 0.24238129 0.22562536]
ti_comp = [0.30116936 0.28752245 0.30009808 0.30400477 0.28497028 0.28178895
 0.30447482 0.30547464 0.27545826 0.28132541]
ti_coms = [0.06738721 0.08103412 0.06845849 0.0645518  0.08358629 0.08676762
 0.06408176 0.06308193 0.09309831 0.08723116]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.96196272e-05 2.00260269e-04 1.88338590e-05 7.65323874e-07
 3.13881583e-04 3.48667396e-05 1.51105132e-06 5.46776000e-05
 2.57584149e-05 1.84683789e-05]
ene_total = [0.54095026 0.66464151 0.5494621  0.51674598 0.69416402 0.6972951
 0.51304332 0.50929606 0.74723805 0.69969276]
optimize_network_iter = 1 obj = 6.132529161591719
eta = 0.6906975851284028
freqs = [5.06955673e+07 1.11682299e+08 5.00689748e+07 1.71395055e+07
 1.30116415e+08 6.27825752e+07 2.14908014e+07 7.10048625e+07
 5.71870607e+07 5.08256882e+07]
Done!
At round 10 eta: 0.6906975851284028
At round 10 local rounds: 12.11742183622639
At round 10 global rounds: 80.0418722784263
At round 10 a_n: 24.414598539591886
gradient difference: 0.3932175040245056
train() client id: f_00000-0-0 loss: 1.476048  [   32/  126]
train() client id: f_00000-0-1 loss: 1.323210  [   64/  126]
train() client id: f_00000-0-2 loss: 1.296299  [   96/  126]
train() client id: f_00000-1-0 loss: 1.234278  [   32/  126]
train() client id: f_00000-1-1 loss: 1.371569  [   64/  126]
train() client id: f_00000-1-2 loss: 1.171290  [   96/  126]
train() client id: f_00000-2-0 loss: 1.231874  [   32/  126]
train() client id: f_00000-2-1 loss: 1.093689  [   64/  126]
train() client id: f_00000-2-2 loss: 1.157176  [   96/  126]
train() client id: f_00000-3-0 loss: 1.129484  [   32/  126]
train() client id: f_00000-3-1 loss: 1.047396  [   64/  126]
train() client id: f_00000-3-2 loss: 1.092104  [   96/  126]
train() client id: f_00000-4-0 loss: 1.020028  [   32/  126]
train() client id: f_00000-4-1 loss: 1.039546  [   64/  126]
train() client id: f_00000-4-2 loss: 0.997653  [   96/  126]
train() client id: f_00000-5-0 loss: 0.944128  [   32/  126]
train() client id: f_00000-5-1 loss: 1.097984  [   64/  126]
train() client id: f_00000-5-2 loss: 0.984446  [   96/  126]
train() client id: f_00000-6-0 loss: 0.930879  [   32/  126]
train() client id: f_00000-6-1 loss: 0.973971  [   64/  126]
train() client id: f_00000-6-2 loss: 0.979765  [   96/  126]
train() client id: f_00000-7-0 loss: 0.951843  [   32/  126]
train() client id: f_00000-7-1 loss: 0.921490  [   64/  126]
train() client id: f_00000-7-2 loss: 0.926906  [   96/  126]
train() client id: f_00000-8-0 loss: 0.943327  [   32/  126]
train() client id: f_00000-8-1 loss: 0.876692  [   64/  126]
train() client id: f_00000-8-2 loss: 0.979812  [   96/  126]
train() client id: f_00000-9-0 loss: 0.912293  [   32/  126]
train() client id: f_00000-9-1 loss: 0.876428  [   64/  126]
train() client id: f_00000-9-2 loss: 0.821325  [   96/  126]
train() client id: f_00000-10-0 loss: 0.809193  [   32/  126]
train() client id: f_00000-10-1 loss: 0.994688  [   64/  126]
train() client id: f_00000-10-2 loss: 0.910005  [   96/  126]
train() client id: f_00000-11-0 loss: 0.980702  [   32/  126]
train() client id: f_00000-11-1 loss: 0.903003  [   64/  126]
train() client id: f_00000-11-2 loss: 0.845943  [   96/  126]
train() client id: f_00001-0-0 loss: 0.581609  [   32/  265]
train() client id: f_00001-0-1 loss: 0.527123  [   64/  265]
train() client id: f_00001-0-2 loss: 0.582274  [   96/  265]
train() client id: f_00001-0-3 loss: 0.495592  [  128/  265]
train() client id: f_00001-0-4 loss: 0.563351  [  160/  265]
train() client id: f_00001-0-5 loss: 0.563884  [  192/  265]
train() client id: f_00001-0-6 loss: 0.461456  [  224/  265]
train() client id: f_00001-0-7 loss: 0.579428  [  256/  265]
train() client id: f_00001-1-0 loss: 0.432587  [   32/  265]
train() client id: f_00001-1-1 loss: 0.628298  [   64/  265]
train() client id: f_00001-1-2 loss: 0.513803  [   96/  265]
train() client id: f_00001-1-3 loss: 0.558762  [  128/  265]
train() client id: f_00001-1-4 loss: 0.516177  [  160/  265]
train() client id: f_00001-1-5 loss: 0.505337  [  192/  265]
train() client id: f_00001-1-6 loss: 0.565328  [  224/  265]
train() client id: f_00001-1-7 loss: 0.568554  [  256/  265]
train() client id: f_00001-2-0 loss: 0.429840  [   32/  265]
train() client id: f_00001-2-1 loss: 0.507561  [   64/  265]
train() client id: f_00001-2-2 loss: 0.612171  [   96/  265]
train() client id: f_00001-2-3 loss: 0.478532  [  128/  265]
train() client id: f_00001-2-4 loss: 0.546887  [  160/  265]
train() client id: f_00001-2-5 loss: 0.520179  [  192/  265]
train() client id: f_00001-2-6 loss: 0.581978  [  224/  265]
train() client id: f_00001-2-7 loss: 0.541284  [  256/  265]
train() client id: f_00001-3-0 loss: 0.550294  [   32/  265]
train() client id: f_00001-3-1 loss: 0.446773  [   64/  265]
train() client id: f_00001-3-2 loss: 0.524443  [   96/  265]
train() client id: f_00001-3-3 loss: 0.571832  [  128/  265]
train() client id: f_00001-3-4 loss: 0.490699  [  160/  265]
train() client id: f_00001-3-5 loss: 0.515715  [  192/  265]
train() client id: f_00001-3-6 loss: 0.504134  [  224/  265]
train() client id: f_00001-3-7 loss: 0.550361  [  256/  265]
train() client id: f_00001-4-0 loss: 0.425945  [   32/  265]
train() client id: f_00001-4-1 loss: 0.435313  [   64/  265]
train() client id: f_00001-4-2 loss: 0.604467  [   96/  265]
train() client id: f_00001-4-3 loss: 0.419523  [  128/  265]
train() client id: f_00001-4-4 loss: 0.620976  [  160/  265]
train() client id: f_00001-4-5 loss: 0.600893  [  192/  265]
train() client id: f_00001-4-6 loss: 0.502200  [  224/  265]
train() client id: f_00001-4-7 loss: 0.483034  [  256/  265]
train() client id: f_00001-5-0 loss: 0.483632  [   32/  265]
train() client id: f_00001-5-1 loss: 0.665266  [   64/  265]
train() client id: f_00001-5-2 loss: 0.458235  [   96/  265]
train() client id: f_00001-5-3 loss: 0.624000  [  128/  265]
train() client id: f_00001-5-4 loss: 0.426350  [  160/  265]
train() client id: f_00001-5-5 loss: 0.519876  [  192/  265]
train() client id: f_00001-5-6 loss: 0.426251  [  224/  265]
train() client id: f_00001-5-7 loss: 0.430457  [  256/  265]
train() client id: f_00001-6-0 loss: 0.469348  [   32/  265]
train() client id: f_00001-6-1 loss: 0.418759  [   64/  265]
train() client id: f_00001-6-2 loss: 0.436409  [   96/  265]
train() client id: f_00001-6-3 loss: 0.426638  [  128/  265]
train() client id: f_00001-6-4 loss: 0.578834  [  160/  265]
train() client id: f_00001-6-5 loss: 0.716930  [  192/  265]
train() client id: f_00001-6-6 loss: 0.596210  [  224/  265]
train() client id: f_00001-6-7 loss: 0.419049  [  256/  265]
train() client id: f_00001-7-0 loss: 0.449183  [   32/  265]
train() client id: f_00001-7-1 loss: 0.420059  [   64/  265]
train() client id: f_00001-7-2 loss: 0.496914  [   96/  265]
train() client id: f_00001-7-3 loss: 0.553322  [  128/  265]
train() client id: f_00001-7-4 loss: 0.601622  [  160/  265]
train() client id: f_00001-7-5 loss: 0.422605  [  192/  265]
train() client id: f_00001-7-6 loss: 0.543277  [  224/  265]
train() client id: f_00001-7-7 loss: 0.499721  [  256/  265]
train() client id: f_00001-8-0 loss: 0.432075  [   32/  265]
train() client id: f_00001-8-1 loss: 0.466736  [   64/  265]
train() client id: f_00001-8-2 loss: 0.647654  [   96/  265]
train() client id: f_00001-8-3 loss: 0.463706  [  128/  265]
train() client id: f_00001-8-4 loss: 0.570804  [  160/  265]
train() client id: f_00001-8-5 loss: 0.473907  [  192/  265]
train() client id: f_00001-8-6 loss: 0.418894  [  224/  265]
train() client id: f_00001-8-7 loss: 0.566785  [  256/  265]
train() client id: f_00001-9-0 loss: 0.437231  [   32/  265]
train() client id: f_00001-9-1 loss: 0.514571  [   64/  265]
train() client id: f_00001-9-2 loss: 0.516991  [   96/  265]
train() client id: f_00001-9-3 loss: 0.439667  [  128/  265]
train() client id: f_00001-9-4 loss: 0.474458  [  160/  265]
train() client id: f_00001-9-5 loss: 0.634843  [  192/  265]
train() client id: f_00001-9-6 loss: 0.553952  [  224/  265]
train() client id: f_00001-9-7 loss: 0.473260  [  256/  265]
train() client id: f_00001-10-0 loss: 0.456283  [   32/  265]
train() client id: f_00001-10-1 loss: 0.424894  [   64/  265]
train() client id: f_00001-10-2 loss: 0.661100  [   96/  265]
train() client id: f_00001-10-3 loss: 0.513412  [  128/  265]
train() client id: f_00001-10-4 loss: 0.480177  [  160/  265]
train() client id: f_00001-10-5 loss: 0.429042  [  192/  265]
train() client id: f_00001-10-6 loss: 0.474229  [  224/  265]
train() client id: f_00001-10-7 loss: 0.536273  [  256/  265]
train() client id: f_00001-11-0 loss: 0.420407  [   32/  265]
train() client id: f_00001-11-1 loss: 0.772906  [   64/  265]
train() client id: f_00001-11-2 loss: 0.495929  [   96/  265]
train() client id: f_00001-11-3 loss: 0.397582  [  128/  265]
train() client id: f_00001-11-4 loss: 0.501127  [  160/  265]
train() client id: f_00001-11-5 loss: 0.535344  [  192/  265]
train() client id: f_00001-11-6 loss: 0.408850  [  224/  265]
train() client id: f_00001-11-7 loss: 0.450224  [  256/  265]
train() client id: f_00002-0-0 loss: 1.375196  [   32/  124]
train() client id: f_00002-0-1 loss: 1.332865  [   64/  124]
train() client id: f_00002-0-2 loss: 1.200833  [   96/  124]
train() client id: f_00002-1-0 loss: 1.339613  [   32/  124]
train() client id: f_00002-1-1 loss: 1.171214  [   64/  124]
train() client id: f_00002-1-2 loss: 1.293554  [   96/  124]
train() client id: f_00002-2-0 loss: 1.207846  [   32/  124]
train() client id: f_00002-2-1 loss: 1.180790  [   64/  124]
train() client id: f_00002-2-2 loss: 1.291163  [   96/  124]
train() client id: f_00002-3-0 loss: 1.259860  [   32/  124]
train() client id: f_00002-3-1 loss: 1.194009  [   64/  124]
train() client id: f_00002-3-2 loss: 1.131894  [   96/  124]
train() client id: f_00002-4-0 loss: 1.195907  [   32/  124]
train() client id: f_00002-4-1 loss: 1.163152  [   64/  124]
train() client id: f_00002-4-2 loss: 1.149298  [   96/  124]
train() client id: f_00002-5-0 loss: 1.140085  [   32/  124]
train() client id: f_00002-5-1 loss: 1.140207  [   64/  124]
train() client id: f_00002-5-2 loss: 1.155564  [   96/  124]
train() client id: f_00002-6-0 loss: 1.082384  [   32/  124]
train() client id: f_00002-6-1 loss: 1.083593  [   64/  124]
train() client id: f_00002-6-2 loss: 1.216674  [   96/  124]
train() client id: f_00002-7-0 loss: 1.080262  [   32/  124]
train() client id: f_00002-7-1 loss: 1.070913  [   64/  124]
train() client id: f_00002-7-2 loss: 1.049082  [   96/  124]
train() client id: f_00002-8-0 loss: 1.040942  [   32/  124]
train() client id: f_00002-8-1 loss: 1.126686  [   64/  124]
train() client id: f_00002-8-2 loss: 1.060994  [   96/  124]
train() client id: f_00002-9-0 loss: 1.190344  [   32/  124]
train() client id: f_00002-9-1 loss: 1.088892  [   64/  124]
train() client id: f_00002-9-2 loss: 0.979234  [   96/  124]
train() client id: f_00002-10-0 loss: 0.936345  [   32/  124]
train() client id: f_00002-10-1 loss: 1.127556  [   64/  124]
train() client id: f_00002-10-2 loss: 1.154072  [   96/  124]
train() client id: f_00002-11-0 loss: 1.110850  [   32/  124]
train() client id: f_00002-11-1 loss: 0.932391  [   64/  124]
train() client id: f_00002-11-2 loss: 1.134162  [   96/  124]
train() client id: f_00003-0-0 loss: 0.982922  [   32/   43]
train() client id: f_00003-1-0 loss: 0.882677  [   32/   43]
train() client id: f_00003-2-0 loss: 0.927947  [   32/   43]
train() client id: f_00003-3-0 loss: 0.962950  [   32/   43]
train() client id: f_00003-4-0 loss: 0.849507  [   32/   43]
train() client id: f_00003-5-0 loss: 0.933749  [   32/   43]
train() client id: f_00003-6-0 loss: 0.966447  [   32/   43]
train() client id: f_00003-7-0 loss: 0.913777  [   32/   43]
train() client id: f_00003-8-0 loss: 0.912993  [   32/   43]
train() client id: f_00003-9-0 loss: 0.928266  [   32/   43]
train() client id: f_00003-10-0 loss: 0.966129  [   32/   43]
train() client id: f_00003-11-0 loss: 0.860910  [   32/   43]
train() client id: f_00004-0-0 loss: 0.890896  [   32/  306]
train() client id: f_00004-0-1 loss: 0.884987  [   64/  306]
train() client id: f_00004-0-2 loss: 0.988252  [   96/  306]
train() client id: f_00004-0-3 loss: 0.838434  [  128/  306]
train() client id: f_00004-0-4 loss: 0.950006  [  160/  306]
train() client id: f_00004-0-5 loss: 0.884987  [  192/  306]
train() client id: f_00004-0-6 loss: 0.924242  [  224/  306]
train() client id: f_00004-0-7 loss: 0.964276  [  256/  306]
train() client id: f_00004-0-8 loss: 0.870150  [  288/  306]
train() client id: f_00004-1-0 loss: 0.918819  [   32/  306]
train() client id: f_00004-1-1 loss: 0.886440  [   64/  306]
train() client id: f_00004-1-2 loss: 0.872787  [   96/  306]
train() client id: f_00004-1-3 loss: 1.007782  [  128/  306]
train() client id: f_00004-1-4 loss: 0.924560  [  160/  306]
train() client id: f_00004-1-5 loss: 0.886564  [  192/  306]
train() client id: f_00004-1-6 loss: 1.019458  [  224/  306]
train() client id: f_00004-1-7 loss: 0.932415  [  256/  306]
train() client id: f_00004-1-8 loss: 0.928651  [  288/  306]
train() client id: f_00004-2-0 loss: 0.945260  [   32/  306]
train() client id: f_00004-2-1 loss: 0.837828  [   64/  306]
train() client id: f_00004-2-2 loss: 0.943303  [   96/  306]
train() client id: f_00004-2-3 loss: 0.783062  [  128/  306]
train() client id: f_00004-2-4 loss: 1.018370  [  160/  306]
train() client id: f_00004-2-5 loss: 0.920971  [  192/  306]
train() client id: f_00004-2-6 loss: 0.977151  [  224/  306]
train() client id: f_00004-2-7 loss: 0.866771  [  256/  306]
train() client id: f_00004-2-8 loss: 0.971345  [  288/  306]
train() client id: f_00004-3-0 loss: 0.869856  [   32/  306]
train() client id: f_00004-3-1 loss: 0.925640  [   64/  306]
train() client id: f_00004-3-2 loss: 0.953916  [   96/  306]
train() client id: f_00004-3-3 loss: 0.962600  [  128/  306]
train() client id: f_00004-3-4 loss: 0.890305  [  160/  306]
train() client id: f_00004-3-5 loss: 0.847729  [  192/  306]
train() client id: f_00004-3-6 loss: 0.828879  [  224/  306]
train() client id: f_00004-3-7 loss: 0.961415  [  256/  306]
train() client id: f_00004-3-8 loss: 0.974418  [  288/  306]
train() client id: f_00004-4-0 loss: 0.938538  [   32/  306]
train() client id: f_00004-4-1 loss: 0.998293  [   64/  306]
train() client id: f_00004-4-2 loss: 0.946409  [   96/  306]
train() client id: f_00004-4-3 loss: 0.875311  [  128/  306]
train() client id: f_00004-4-4 loss: 0.831981  [  160/  306]
train() client id: f_00004-4-5 loss: 0.912319  [  192/  306]
train() client id: f_00004-4-6 loss: 0.957888  [  224/  306]
train() client id: f_00004-4-7 loss: 0.961600  [  256/  306]
train() client id: f_00004-4-8 loss: 0.859939  [  288/  306]
train() client id: f_00004-5-0 loss: 0.901698  [   32/  306]
train() client id: f_00004-5-1 loss: 1.015745  [   64/  306]
train() client id: f_00004-5-2 loss: 0.806062  [   96/  306]
train() client id: f_00004-5-3 loss: 0.909011  [  128/  306]
train() client id: f_00004-5-4 loss: 0.948444  [  160/  306]
train() client id: f_00004-5-5 loss: 0.911256  [  192/  306]
train() client id: f_00004-5-6 loss: 1.000706  [  224/  306]
train() client id: f_00004-5-7 loss: 0.871540  [  256/  306]
train() client id: f_00004-5-8 loss: 0.895911  [  288/  306]
train() client id: f_00004-6-0 loss: 0.932376  [   32/  306]
train() client id: f_00004-6-1 loss: 0.915522  [   64/  306]
train() client id: f_00004-6-2 loss: 0.936623  [   96/  306]
train() client id: f_00004-6-3 loss: 0.953687  [  128/  306]
train() client id: f_00004-6-4 loss: 0.943061  [  160/  306]
train() client id: f_00004-6-5 loss: 0.789622  [  192/  306]
train() client id: f_00004-6-6 loss: 0.863261  [  224/  306]
train() client id: f_00004-6-7 loss: 1.003811  [  256/  306]
train() client id: f_00004-6-8 loss: 0.901754  [  288/  306]
train() client id: f_00004-7-0 loss: 0.862138  [   32/  306]
train() client id: f_00004-7-1 loss: 0.947006  [   64/  306]
train() client id: f_00004-7-2 loss: 0.987570  [   96/  306]
train() client id: f_00004-7-3 loss: 0.970978  [  128/  306]
train() client id: f_00004-7-4 loss: 0.884310  [  160/  306]
train() client id: f_00004-7-5 loss: 0.903052  [  192/  306]
train() client id: f_00004-7-6 loss: 0.943957  [  224/  306]
train() client id: f_00004-7-7 loss: 0.930778  [  256/  306]
train() client id: f_00004-7-8 loss: 0.887707  [  288/  306]
train() client id: f_00004-8-0 loss: 0.979485  [   32/  306]
train() client id: f_00004-8-1 loss: 0.867655  [   64/  306]
train() client id: f_00004-8-2 loss: 0.978906  [   96/  306]
train() client id: f_00004-8-3 loss: 0.877717  [  128/  306]
train() client id: f_00004-8-4 loss: 0.912358  [  160/  306]
train() client id: f_00004-8-5 loss: 0.879607  [  192/  306]
train() client id: f_00004-8-6 loss: 1.007111  [  224/  306]
train() client id: f_00004-8-7 loss: 0.855861  [  256/  306]
train() client id: f_00004-8-8 loss: 0.938392  [  288/  306]
train() client id: f_00004-9-0 loss: 0.800246  [   32/  306]
train() client id: f_00004-9-1 loss: 0.979120  [   64/  306]
train() client id: f_00004-9-2 loss: 0.978062  [   96/  306]
train() client id: f_00004-9-3 loss: 0.965414  [  128/  306]
train() client id: f_00004-9-4 loss: 0.932873  [  160/  306]
train() client id: f_00004-9-5 loss: 0.959375  [  192/  306]
train() client id: f_00004-9-6 loss: 0.963088  [  224/  306]
train() client id: f_00004-9-7 loss: 0.878200  [  256/  306]
train() client id: f_00004-9-8 loss: 0.830661  [  288/  306]
train() client id: f_00004-10-0 loss: 0.929339  [   32/  306]
train() client id: f_00004-10-1 loss: 0.901816  [   64/  306]
train() client id: f_00004-10-2 loss: 0.926888  [   96/  306]
train() client id: f_00004-10-3 loss: 0.989198  [  128/  306]
train() client id: f_00004-10-4 loss: 0.915012  [  160/  306]
train() client id: f_00004-10-5 loss: 0.974965  [  192/  306]
train() client id: f_00004-10-6 loss: 0.814980  [  224/  306]
train() client id: f_00004-10-7 loss: 0.835636  [  256/  306]
train() client id: f_00004-10-8 loss: 1.018298  [  288/  306]
train() client id: f_00004-11-0 loss: 0.876812  [   32/  306]
train() client id: f_00004-11-1 loss: 0.914852  [   64/  306]
train() client id: f_00004-11-2 loss: 0.896106  [   96/  306]
train() client id: f_00004-11-3 loss: 0.942666  [  128/  306]
train() client id: f_00004-11-4 loss: 0.862970  [  160/  306]
train() client id: f_00004-11-5 loss: 0.977089  [  192/  306]
train() client id: f_00004-11-6 loss: 0.988609  [  224/  306]
train() client id: f_00004-11-7 loss: 0.937845  [  256/  306]
train() client id: f_00004-11-8 loss: 0.882607  [  288/  306]
train() client id: f_00005-0-0 loss: 0.876027  [   32/  146]
train() client id: f_00005-0-1 loss: 0.693867  [   64/  146]
train() client id: f_00005-0-2 loss: 0.708761  [   96/  146]
train() client id: f_00005-0-3 loss: 0.752127  [  128/  146]
train() client id: f_00005-1-0 loss: 0.867108  [   32/  146]
train() client id: f_00005-1-1 loss: 0.754873  [   64/  146]
train() client id: f_00005-1-2 loss: 0.667540  [   96/  146]
train() client id: f_00005-1-3 loss: 0.723017  [  128/  146]
train() client id: f_00005-2-0 loss: 0.636181  [   32/  146]
train() client id: f_00005-2-1 loss: 0.752522  [   64/  146]
train() client id: f_00005-2-2 loss: 0.765132  [   96/  146]
train() client id: f_00005-2-3 loss: 0.668345  [  128/  146]
train() client id: f_00005-3-0 loss: 0.673470  [   32/  146]
train() client id: f_00005-3-1 loss: 0.764754  [   64/  146]
train() client id: f_00005-3-2 loss: 0.650544  [   96/  146]
train() client id: f_00005-3-3 loss: 0.913312  [  128/  146]
train() client id: f_00005-4-0 loss: 0.822777  [   32/  146]
train() client id: f_00005-4-1 loss: 0.517479  [   64/  146]
train() client id: f_00005-4-2 loss: 0.788522  [   96/  146]
train() client id: f_00005-4-3 loss: 0.881444  [  128/  146]
train() client id: f_00005-5-0 loss: 0.674458  [   32/  146]
train() client id: f_00005-5-1 loss: 0.775713  [   64/  146]
train() client id: f_00005-5-2 loss: 0.670929  [   96/  146]
train() client id: f_00005-5-3 loss: 0.757512  [  128/  146]
train() client id: f_00005-6-0 loss: 0.842411  [   32/  146]
train() client id: f_00005-6-1 loss: 0.575282  [   64/  146]
train() client id: f_00005-6-2 loss: 0.694202  [   96/  146]
train() client id: f_00005-6-3 loss: 0.827963  [  128/  146]
train() client id: f_00005-7-0 loss: 0.608229  [   32/  146]
train() client id: f_00005-7-1 loss: 0.716952  [   64/  146]
train() client id: f_00005-7-2 loss: 0.913352  [   96/  146]
train() client id: f_00005-7-3 loss: 0.692466  [  128/  146]
train() client id: f_00005-8-0 loss: 0.616348  [   32/  146]
train() client id: f_00005-8-1 loss: 0.845953  [   64/  146]
train() client id: f_00005-8-2 loss: 0.780748  [   96/  146]
train() client id: f_00005-8-3 loss: 0.659313  [  128/  146]
train() client id: f_00005-9-0 loss: 0.687964  [   32/  146]
train() client id: f_00005-9-1 loss: 0.517637  [   64/  146]
train() client id: f_00005-9-2 loss: 0.788477  [   96/  146]
train() client id: f_00005-9-3 loss: 0.902425  [  128/  146]
train() client id: f_00005-10-0 loss: 0.645799  [   32/  146]
train() client id: f_00005-10-1 loss: 0.770374  [   64/  146]
train() client id: f_00005-10-2 loss: 0.870433  [   96/  146]
train() client id: f_00005-10-3 loss: 0.677973  [  128/  146]
train() client id: f_00005-11-0 loss: 0.665217  [   32/  146]
train() client id: f_00005-11-1 loss: 0.682478  [   64/  146]
train() client id: f_00005-11-2 loss: 0.677536  [   96/  146]
train() client id: f_00005-11-3 loss: 0.865145  [  128/  146]
train() client id: f_00006-0-0 loss: 0.736086  [   32/   54]
train() client id: f_00006-1-0 loss: 0.715333  [   32/   54]
train() client id: f_00006-2-0 loss: 0.689384  [   32/   54]
train() client id: f_00006-3-0 loss: 0.692177  [   32/   54]
train() client id: f_00006-4-0 loss: 0.728097  [   32/   54]
train() client id: f_00006-5-0 loss: 0.680025  [   32/   54]
train() client id: f_00006-6-0 loss: 0.732527  [   32/   54]
train() client id: f_00006-7-0 loss: 0.692300  [   32/   54]
train() client id: f_00006-8-0 loss: 0.654017  [   32/   54]
train() client id: f_00006-9-0 loss: 0.736640  [   32/   54]
train() client id: f_00006-10-0 loss: 0.704573  [   32/   54]
train() client id: f_00006-11-0 loss: 0.714981  [   32/   54]
train() client id: f_00007-0-0 loss: 0.745820  [   32/  179]
train() client id: f_00007-0-1 loss: 0.676265  [   64/  179]
train() client id: f_00007-0-2 loss: 0.778880  [   96/  179]
train() client id: f_00007-0-3 loss: 0.872658  [  128/  179]
train() client id: f_00007-0-4 loss: 0.778881  [  160/  179]
train() client id: f_00007-1-0 loss: 0.786981  [   32/  179]
train() client id: f_00007-1-1 loss: 0.726932  [   64/  179]
train() client id: f_00007-1-2 loss: 0.675163  [   96/  179]
train() client id: f_00007-1-3 loss: 0.678210  [  128/  179]
train() client id: f_00007-1-4 loss: 0.766977  [  160/  179]
train() client id: f_00007-2-0 loss: 0.608236  [   32/  179]
train() client id: f_00007-2-1 loss: 0.888094  [   64/  179]
train() client id: f_00007-2-2 loss: 0.749470  [   96/  179]
train() client id: f_00007-2-3 loss: 0.740512  [  128/  179]
train() client id: f_00007-2-4 loss: 0.624715  [  160/  179]
train() client id: f_00007-3-0 loss: 0.791041  [   32/  179]
train() client id: f_00007-3-1 loss: 0.628091  [   64/  179]
train() client id: f_00007-3-2 loss: 0.732823  [   96/  179]
train() client id: f_00007-3-3 loss: 0.655470  [  128/  179]
train() client id: f_00007-3-4 loss: 0.767485  [  160/  179]
train() client id: f_00007-4-0 loss: 0.691371  [   32/  179]
train() client id: f_00007-4-1 loss: 0.740009  [   64/  179]
train() client id: f_00007-4-2 loss: 0.605869  [   96/  179]
train() client id: f_00007-4-3 loss: 0.659383  [  128/  179]
train() client id: f_00007-4-4 loss: 0.756769  [  160/  179]
train() client id: f_00007-5-0 loss: 0.743576  [   32/  179]
train() client id: f_00007-5-1 loss: 0.592699  [   64/  179]
train() client id: f_00007-5-2 loss: 0.557368  [   96/  179]
train() client id: f_00007-5-3 loss: 0.840384  [  128/  179]
train() client id: f_00007-5-4 loss: 0.753036  [  160/  179]
train() client id: f_00007-6-0 loss: 0.537304  [   32/  179]
train() client id: f_00007-6-1 loss: 0.832493  [   64/  179]
train() client id: f_00007-6-2 loss: 0.660815  [   96/  179]
train() client id: f_00007-6-3 loss: 0.633389  [  128/  179]
train() client id: f_00007-6-4 loss: 0.712360  [  160/  179]
train() client id: f_00007-7-0 loss: 0.827556  [   32/  179]
train() client id: f_00007-7-1 loss: 0.801770  [   64/  179]
train() client id: f_00007-7-2 loss: 0.669054  [   96/  179]
train() client id: f_00007-7-3 loss: 0.576695  [  128/  179]
train() client id: f_00007-7-4 loss: 0.583915  [  160/  179]
train() client id: f_00007-8-0 loss: 0.665266  [   32/  179]
train() client id: f_00007-8-1 loss: 0.783787  [   64/  179]
train() client id: f_00007-8-2 loss: 0.640296  [   96/  179]
train() client id: f_00007-8-3 loss: 0.703073  [  128/  179]
train() client id: f_00007-8-4 loss: 0.643849  [  160/  179]
train() client id: f_00007-9-0 loss: 0.572665  [   32/  179]
train() client id: f_00007-9-1 loss: 0.917466  [   64/  179]
train() client id: f_00007-9-2 loss: 0.627771  [   96/  179]
train() client id: f_00007-9-3 loss: 0.557142  [  128/  179]
train() client id: f_00007-9-4 loss: 0.739706  [  160/  179]
train() client id: f_00007-10-0 loss: 0.617896  [   32/  179]
train() client id: f_00007-10-1 loss: 0.614106  [   64/  179]
train() client id: f_00007-10-2 loss: 0.820077  [   96/  179]
train() client id: f_00007-10-3 loss: 0.717461  [  128/  179]
train() client id: f_00007-10-4 loss: 0.634850  [  160/  179]
train() client id: f_00007-11-0 loss: 0.570831  [   32/  179]
train() client id: f_00007-11-1 loss: 0.767816  [   64/  179]
train() client id: f_00007-11-2 loss: 0.891582  [   96/  179]
train() client id: f_00007-11-3 loss: 0.548092  [  128/  179]
train() client id: f_00007-11-4 loss: 0.641847  [  160/  179]
train() client id: f_00008-0-0 loss: 0.869059  [   32/  130]
train() client id: f_00008-0-1 loss: 0.889722  [   64/  130]
train() client id: f_00008-0-2 loss: 0.913586  [   96/  130]
train() client id: f_00008-0-3 loss: 0.814516  [  128/  130]
train() client id: f_00008-1-0 loss: 0.887084  [   32/  130]
train() client id: f_00008-1-1 loss: 0.868538  [   64/  130]
train() client id: f_00008-1-2 loss: 0.923056  [   96/  130]
train() client id: f_00008-1-3 loss: 0.798681  [  128/  130]
train() client id: f_00008-2-0 loss: 0.886617  [   32/  130]
train() client id: f_00008-2-1 loss: 0.870512  [   64/  130]
train() client id: f_00008-2-2 loss: 0.978367  [   96/  130]
train() client id: f_00008-2-3 loss: 0.765116  [  128/  130]
train() client id: f_00008-3-0 loss: 0.981620  [   32/  130]
train() client id: f_00008-3-1 loss: 0.934999  [   64/  130]
train() client id: f_00008-3-2 loss: 0.863909  [   96/  130]
train() client id: f_00008-3-3 loss: 0.710945  [  128/  130]
train() client id: f_00008-4-0 loss: 0.822552  [   32/  130]
train() client id: f_00008-4-1 loss: 0.806973  [   64/  130]
train() client id: f_00008-4-2 loss: 0.836467  [   96/  130]
train() client id: f_00008-4-3 loss: 1.014707  [  128/  130]
train() client id: f_00008-5-0 loss: 0.867171  [   32/  130]
train() client id: f_00008-5-1 loss: 0.914540  [   64/  130]
train() client id: f_00008-5-2 loss: 0.882814  [   96/  130]
train() client id: f_00008-5-3 loss: 0.814171  [  128/  130]
train() client id: f_00008-6-0 loss: 1.036527  [   32/  130]
train() client id: f_00008-6-1 loss: 0.886135  [   64/  130]
train() client id: f_00008-6-2 loss: 0.735899  [   96/  130]
train() client id: f_00008-6-3 loss: 0.825915  [  128/  130]
train() client id: f_00008-7-0 loss: 0.996520  [   32/  130]
train() client id: f_00008-7-1 loss: 0.862988  [   64/  130]
train() client id: f_00008-7-2 loss: 0.799542  [   96/  130]
train() client id: f_00008-7-3 loss: 0.828388  [  128/  130]
train() client id: f_00008-8-0 loss: 0.883145  [   32/  130]
train() client id: f_00008-8-1 loss: 0.877406  [   64/  130]
train() client id: f_00008-8-2 loss: 0.869871  [   96/  130]
train() client id: f_00008-8-3 loss: 0.802298  [  128/  130]
train() client id: f_00008-9-0 loss: 0.718978  [   32/  130]
train() client id: f_00008-9-1 loss: 0.833219  [   64/  130]
train() client id: f_00008-9-2 loss: 0.831392  [   96/  130]
train() client id: f_00008-9-3 loss: 1.089694  [  128/  130]
train() client id: f_00008-10-0 loss: 0.897784  [   32/  130]
train() client id: f_00008-10-1 loss: 0.845643  [   64/  130]
train() client id: f_00008-10-2 loss: 0.791620  [   96/  130]
train() client id: f_00008-10-3 loss: 0.888398  [  128/  130]
train() client id: f_00008-11-0 loss: 0.907585  [   32/  130]
train() client id: f_00008-11-1 loss: 0.781124  [   64/  130]
train() client id: f_00008-11-2 loss: 0.880722  [   96/  130]
train() client id: f_00008-11-3 loss: 0.911500  [  128/  130]
train() client id: f_00009-0-0 loss: 1.087498  [   32/  118]
train() client id: f_00009-0-1 loss: 1.209487  [   64/  118]
train() client id: f_00009-0-2 loss: 1.125619  [   96/  118]
train() client id: f_00009-1-0 loss: 1.174755  [   32/  118]
train() client id: f_00009-1-1 loss: 1.241916  [   64/  118]
train() client id: f_00009-1-2 loss: 1.038536  [   96/  118]
train() client id: f_00009-2-0 loss: 1.108114  [   32/  118]
train() client id: f_00009-2-1 loss: 1.097885  [   64/  118]
train() client id: f_00009-2-2 loss: 1.051249  [   96/  118]
train() client id: f_00009-3-0 loss: 1.005461  [   32/  118]
train() client id: f_00009-3-1 loss: 0.952499  [   64/  118]
train() client id: f_00009-3-2 loss: 1.118629  [   96/  118]
train() client id: f_00009-4-0 loss: 0.986622  [   32/  118]
train() client id: f_00009-4-1 loss: 0.965316  [   64/  118]
train() client id: f_00009-4-2 loss: 1.019228  [   96/  118]
train() client id: f_00009-5-0 loss: 1.071344  [   32/  118]
train() client id: f_00009-5-1 loss: 0.952263  [   64/  118]
train() client id: f_00009-5-2 loss: 0.997824  [   96/  118]
train() client id: f_00009-6-0 loss: 1.044413  [   32/  118]
train() client id: f_00009-6-1 loss: 0.970772  [   64/  118]
train() client id: f_00009-6-2 loss: 0.889762  [   96/  118]
train() client id: f_00009-7-0 loss: 0.919712  [   32/  118]
train() client id: f_00009-7-1 loss: 0.950618  [   64/  118]
train() client id: f_00009-7-2 loss: 0.930940  [   96/  118]
train() client id: f_00009-8-0 loss: 0.981622  [   32/  118]
train() client id: f_00009-8-1 loss: 0.967552  [   64/  118]
train() client id: f_00009-8-2 loss: 0.949116  [   96/  118]
train() client id: f_00009-9-0 loss: 0.887780  [   32/  118]
train() client id: f_00009-9-1 loss: 0.976533  [   64/  118]
train() client id: f_00009-9-2 loss: 0.881749  [   96/  118]
train() client id: f_00009-10-0 loss: 0.882197  [   32/  118]
train() client id: f_00009-10-1 loss: 0.939083  [   64/  118]
train() client id: f_00009-10-2 loss: 0.961024  [   96/  118]
train() client id: f_00009-11-0 loss: 0.916170  [   32/  118]
train() client id: f_00009-11-1 loss: 1.014447  [   64/  118]
train() client id: f_00009-11-2 loss: 0.901304  [   96/  118]
At round 10 accuracy: 0.6286472148541115
At round 10 training accuracy: 0.5707578806170356
At round 10 training loss: 0.8801922603895299
update_location
xs = [ -3.9056584    4.20031788  70.00902392  18.81129433   0.97929623
   3.95640986 -32.44319194 -11.32485185  54.66397685  -2.06087855]
ys = [ 62.5879595   45.55583871   1.32061395 -32.45517586  24.35018685
   7.81415074  -2.62498432   0.82234798  17.56900603  -0.99851822]
dists_uav = [118.03604044 109.96807314 122.07787453 106.80450943 102.92662736
 100.38283783 105.16392559 100.64257809 115.31183954 100.02621786]
dists_bs = [204.71868644 221.06230981 300.3230776  283.90221521 231.65508607
 244.90101025 227.75662137 239.01340671 278.44220249 246.74565476]
uav_gains = [6.60612004e-11 7.88546405e-11 6.07267938e-11 8.48246225e-11
 9.30418280e-11 9.90488970e-11 8.81718085e-11 9.84110509e-11
 7.00330537e-11 9.99341172e-11]
bs_gains = [3.73295817e-11 3.01059902e-11 1.27657316e-11 1.49424392e-11
 2.64080865e-11 2.26006261e-11 2.76933346e-11 2.41942270e-11
 1.57774174e-11 2.21307151e-11]
Round 11
-------------------------------
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.53941363 19.90793381  9.39345483  3.35836813 22.96346205 11.07093228
  4.17536772 13.46928103  9.90106646  8.98947369]
obj_prev = 112.76875362310263
eta_min = 1.8259794231621306e-10	eta_max = 0.9205107351620903
af = 23.839837477165666	bf = 1.854557298226717	zeta = 26.223821224882236	eta = 0.909090909090909
af = 23.839837477165666	bf = 1.854557298226717	zeta = 45.394101441493795	eta = 0.5251747852723034
af = 23.839837477165666	bf = 1.854557298226717	zeta = 36.24067924685511	eta = 0.6578198304391449
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.59948643670541	eta = 0.6890228709254712
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.51842070411875	eta = 0.6906410255994443
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.51820885585075	eta = 0.6906452642639038
eta = 0.6906452642639038
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [0.03054215 0.06423548 0.03005736 0.01042312 0.0741738  0.03539011
 0.01308949 0.04338925 0.03151175 0.02860297]
ene_total = [2.94794512 5.70363107 2.91862696 1.33519898 6.50745954 3.47131979
 1.5419756  3.92315854 3.2297488  2.93914446]
ti_comp = [0.30504778 0.29281109 0.30391469 0.30820672 0.29034205 0.28721115
 0.30867006 0.30995051 0.27903667 0.28677106]
ti_coms = [0.0681094  0.08034609 0.06924248 0.06495046 0.08281513 0.08594602
 0.06448711 0.06320667 0.09412051 0.08638612]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.91356519e-05 1.93210162e-04 1.83750713e-05 7.45055334e-07
 3.02560183e-04 3.35832973e-05 1.47115905e-06 5.31424719e-05
 2.51174115e-05 1.77845446e-05]
ene_total = [0.53903666 0.6493481  0.54791909 0.51265458 0.677464   0.68094541
 0.50905514 0.50302767 0.7447912  0.68317183]
optimize_network_iter = 0 obj = 6.04741367166706
eta = 0.6906452642639038
freqs = [5.00612622e+07 1.09687587e+08 4.94503204e+07 1.69092939e+07
 1.27735205e+08 6.16099244e+07 2.12030516e+07 6.99938362e+07
 5.64652429e+07 4.98707394e+07]
eta_min = 0.6906452642638832	eta_max = 0.6906452642639023
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 0.04826057494319857	eta = 0.909090909090909
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 20.44612752124682	eta = 0.0021457975307437047
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.1286929945616637	eta = 0.020610416842846153
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.067483129885282	eta = 0.02122060843649869
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.0674642356817103	eta = 0.021220802368025433
eta = 0.021220802368025433
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.99188226e-04 2.01117734e-03 1.91271134e-04 7.75548441e-06
 3.14943156e-03 3.49577711e-04 1.53136962e-05 5.53174501e-04
 2.61453995e-04 1.85124181e-04]
ene_total = [0.17486013 0.25058139 0.17748901 0.16220541 0.28513266 0.22310299
 0.16123818 0.17146061 0.24129522 0.22009864]
ti_comp = [0.30504778 0.29281109 0.30391469 0.30820672 0.29034205 0.28721115
 0.30867006 0.30995051 0.27903667 0.28677106]
ti_coms = [0.0681094  0.08034609 0.06924248 0.06495046 0.08281513 0.08594602
 0.06448711 0.06320667 0.09412051 0.08638612]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.91356519e-05 1.93210162e-04 1.83750713e-05 7.45055334e-07
 3.02560183e-04 3.35832973e-05 1.47115905e-06 5.31424719e-05
 2.51174115e-05 1.77845446e-05]
ene_total = [0.53903666 0.6493481  0.54791909 0.51265458 0.677464   0.68094541
 0.50905514 0.50302767 0.7447912  0.68317183]
optimize_network_iter = 1 obj = 6.047413671666661
eta = 0.6906452642638832
freqs = [5.00612622e+07 1.09687587e+08 4.94503204e+07 1.69092939e+07
 1.27735205e+08 6.16099244e+07 2.12030516e+07 6.99938362e+07
 5.64652429e+07 4.98707394e+07]
Done!
At round 11 eta: 0.6906452642638832
At round 11 local rounds: 12.119902394804324
At round 11 global rounds: 78.921043447086
At round 11 a_n: 24.072052692622567
gradient difference: 0.3712407052516937
train() client id: f_00000-0-0 loss: 1.321262  [   32/  126]
train() client id: f_00000-0-1 loss: 1.264651  [   64/  126]
train() client id: f_00000-0-2 loss: 1.106454  [   96/  126]
train() client id: f_00000-1-0 loss: 1.312612  [   32/  126]
train() client id: f_00000-1-1 loss: 1.246528  [   64/  126]
train() client id: f_00000-1-2 loss: 1.032560  [   96/  126]
train() client id: f_00000-2-0 loss: 1.166228  [   32/  126]
train() client id: f_00000-2-1 loss: 1.173822  [   64/  126]
train() client id: f_00000-2-2 loss: 0.961963  [   96/  126]
train() client id: f_00000-3-0 loss: 0.988914  [   32/  126]
train() client id: f_00000-3-1 loss: 1.060649  [   64/  126]
train() client id: f_00000-3-2 loss: 1.028339  [   96/  126]
train() client id: f_00000-4-0 loss: 1.009120  [   32/  126]
train() client id: f_00000-4-1 loss: 1.026493  [   64/  126]
train() client id: f_00000-4-2 loss: 0.971087  [   96/  126]
train() client id: f_00000-5-0 loss: 0.955827  [   32/  126]
train() client id: f_00000-5-1 loss: 0.990792  [   64/  126]
train() client id: f_00000-5-2 loss: 0.978781  [   96/  126]
train() client id: f_00000-6-0 loss: 0.957726  [   32/  126]
train() client id: f_00000-6-1 loss: 0.917548  [   64/  126]
train() client id: f_00000-6-2 loss: 0.975931  [   96/  126]
train() client id: f_00000-7-0 loss: 0.981680  [   32/  126]
train() client id: f_00000-7-1 loss: 0.958096  [   64/  126]
train() client id: f_00000-7-2 loss: 0.908066  [   96/  126]
train() client id: f_00000-8-0 loss: 0.940073  [   32/  126]
train() client id: f_00000-8-1 loss: 0.954530  [   64/  126]
train() client id: f_00000-8-2 loss: 0.962267  [   96/  126]
train() client id: f_00000-9-0 loss: 0.979696  [   32/  126]
train() client id: f_00000-9-1 loss: 0.945841  [   64/  126]
train() client id: f_00000-9-2 loss: 0.958583  [   96/  126]
train() client id: f_00000-10-0 loss: 0.963747  [   32/  126]
train() client id: f_00000-10-1 loss: 0.917305  [   64/  126]
train() client id: f_00000-10-2 loss: 0.878898  [   96/  126]
train() client id: f_00000-11-0 loss: 1.002628  [   32/  126]
train() client id: f_00000-11-1 loss: 0.914149  [   64/  126]
train() client id: f_00000-11-2 loss: 0.900510  [   96/  126]
train() client id: f_00001-0-0 loss: 0.469332  [   32/  265]
train() client id: f_00001-0-1 loss: 0.474487  [   64/  265]
train() client id: f_00001-0-2 loss: 0.545349  [   96/  265]
train() client id: f_00001-0-3 loss: 0.552087  [  128/  265]
train() client id: f_00001-0-4 loss: 0.498865  [  160/  265]
train() client id: f_00001-0-5 loss: 0.626510  [  192/  265]
train() client id: f_00001-0-6 loss: 0.514595  [  224/  265]
train() client id: f_00001-0-7 loss: 0.678126  [  256/  265]
train() client id: f_00001-1-0 loss: 0.526542  [   32/  265]
train() client id: f_00001-1-1 loss: 0.590574  [   64/  265]
train() client id: f_00001-1-2 loss: 0.503118  [   96/  265]
train() client id: f_00001-1-3 loss: 0.472424  [  128/  265]
train() client id: f_00001-1-4 loss: 0.553976  [  160/  265]
train() client id: f_00001-1-5 loss: 0.573976  [  192/  265]
train() client id: f_00001-1-6 loss: 0.525300  [  224/  265]
train() client id: f_00001-1-7 loss: 0.467748  [  256/  265]
train() client id: f_00001-2-0 loss: 0.470007  [   32/  265]
train() client id: f_00001-2-1 loss: 0.571749  [   64/  265]
train() client id: f_00001-2-2 loss: 0.503743  [   96/  265]
train() client id: f_00001-2-3 loss: 0.505160  [  128/  265]
train() client id: f_00001-2-4 loss: 0.547832  [  160/  265]
train() client id: f_00001-2-5 loss: 0.494487  [  192/  265]
train() client id: f_00001-2-6 loss: 0.603382  [  224/  265]
train() client id: f_00001-2-7 loss: 0.505995  [  256/  265]
train() client id: f_00001-3-0 loss: 0.503960  [   32/  265]
train() client id: f_00001-3-1 loss: 0.599252  [   64/  265]
train() client id: f_00001-3-2 loss: 0.561226  [   96/  265]
train() client id: f_00001-3-3 loss: 0.452990  [  128/  265]
train() client id: f_00001-3-4 loss: 0.596362  [  160/  265]
train() client id: f_00001-3-5 loss: 0.511437  [  192/  265]
train() client id: f_00001-3-6 loss: 0.495701  [  224/  265]
train() client id: f_00001-3-7 loss: 0.442746  [  256/  265]
train() client id: f_00001-4-0 loss: 0.488256  [   32/  265]
train() client id: f_00001-4-1 loss: 0.503047  [   64/  265]
train() client id: f_00001-4-2 loss: 0.531383  [   96/  265]
train() client id: f_00001-4-3 loss: 0.480237  [  128/  265]
train() client id: f_00001-4-4 loss: 0.528300  [  160/  265]
train() client id: f_00001-4-5 loss: 0.642549  [  192/  265]
train() client id: f_00001-4-6 loss: 0.518559  [  224/  265]
train() client id: f_00001-4-7 loss: 0.439742  [  256/  265]
train() client id: f_00001-5-0 loss: 0.418619  [   32/  265]
train() client id: f_00001-5-1 loss: 0.460166  [   64/  265]
train() client id: f_00001-5-2 loss: 0.511914  [   96/  265]
train() client id: f_00001-5-3 loss: 0.548221  [  128/  265]
train() client id: f_00001-5-4 loss: 0.483877  [  160/  265]
train() client id: f_00001-5-5 loss: 0.615355  [  192/  265]
train() client id: f_00001-5-6 loss: 0.572825  [  224/  265]
train() client id: f_00001-5-7 loss: 0.492406  [  256/  265]
train() client id: f_00001-6-0 loss: 0.535367  [   32/  265]
train() client id: f_00001-6-1 loss: 0.546934  [   64/  265]
train() client id: f_00001-6-2 loss: 0.490397  [   96/  265]
train() client id: f_00001-6-3 loss: 0.469052  [  128/  265]
train() client id: f_00001-6-4 loss: 0.425102  [  160/  265]
train() client id: f_00001-6-5 loss: 0.561040  [  192/  265]
train() client id: f_00001-6-6 loss: 0.602884  [  224/  265]
train() client id: f_00001-6-7 loss: 0.427643  [  256/  265]
train() client id: f_00001-7-0 loss: 0.433199  [   32/  265]
train() client id: f_00001-7-1 loss: 0.476066  [   64/  265]
train() client id: f_00001-7-2 loss: 0.592225  [   96/  265]
train() client id: f_00001-7-3 loss: 0.558540  [  128/  265]
train() client id: f_00001-7-4 loss: 0.537852  [  160/  265]
train() client id: f_00001-7-5 loss: 0.516746  [  192/  265]
train() client id: f_00001-7-6 loss: 0.476020  [  224/  265]
train() client id: f_00001-7-7 loss: 0.501944  [  256/  265]
train() client id: f_00001-8-0 loss: 0.474298  [   32/  265]
train() client id: f_00001-8-1 loss: 0.436182  [   64/  265]
train() client id: f_00001-8-2 loss: 0.560318  [   96/  265]
train() client id: f_00001-8-3 loss: 0.476931  [  128/  265]
train() client id: f_00001-8-4 loss: 0.617484  [  160/  265]
train() client id: f_00001-8-5 loss: 0.537462  [  192/  265]
train() client id: f_00001-8-6 loss: 0.465189  [  224/  265]
train() client id: f_00001-8-7 loss: 0.437958  [  256/  265]
train() client id: f_00001-9-0 loss: 0.638653  [   32/  265]
train() client id: f_00001-9-1 loss: 0.427398  [   64/  265]
train() client id: f_00001-9-2 loss: 0.474484  [   96/  265]
train() client id: f_00001-9-3 loss: 0.415184  [  128/  265]
train() client id: f_00001-9-4 loss: 0.612942  [  160/  265]
train() client id: f_00001-9-5 loss: 0.552236  [  192/  265]
train() client id: f_00001-9-6 loss: 0.437853  [  224/  265]
train() client id: f_00001-9-7 loss: 0.522473  [  256/  265]
train() client id: f_00001-10-0 loss: 0.425654  [   32/  265]
train() client id: f_00001-10-1 loss: 0.672330  [   64/  265]
train() client id: f_00001-10-2 loss: 0.424900  [   96/  265]
train() client id: f_00001-10-3 loss: 0.489125  [  128/  265]
train() client id: f_00001-10-4 loss: 0.413968  [  160/  265]
train() client id: f_00001-10-5 loss: 0.511233  [  192/  265]
train() client id: f_00001-10-6 loss: 0.543688  [  224/  265]
train() client id: f_00001-10-7 loss: 0.566125  [  256/  265]
train() client id: f_00001-11-0 loss: 0.578063  [   32/  265]
train() client id: f_00001-11-1 loss: 0.469922  [   64/  265]
train() client id: f_00001-11-2 loss: 0.406801  [   96/  265]
train() client id: f_00001-11-3 loss: 0.490487  [  128/  265]
train() client id: f_00001-11-4 loss: 0.479548  [  160/  265]
train() client id: f_00001-11-5 loss: 0.586216  [  192/  265]
train() client id: f_00001-11-6 loss: 0.454931  [  224/  265]
train() client id: f_00001-11-7 loss: 0.520717  [  256/  265]
train() client id: f_00002-0-0 loss: 1.335591  [   32/  124]
train() client id: f_00002-0-1 loss: 1.271772  [   64/  124]
train() client id: f_00002-0-2 loss: 1.173054  [   96/  124]
train() client id: f_00002-1-0 loss: 1.181708  [   32/  124]
train() client id: f_00002-1-1 loss: 1.105871  [   64/  124]
train() client id: f_00002-1-2 loss: 1.154339  [   96/  124]
train() client id: f_00002-2-0 loss: 1.128070  [   32/  124]
train() client id: f_00002-2-1 loss: 1.082459  [   64/  124]
train() client id: f_00002-2-2 loss: 1.130247  [   96/  124]
train() client id: f_00002-3-0 loss: 1.060326  [   32/  124]
train() client id: f_00002-3-1 loss: 1.111304  [   64/  124]
train() client id: f_00002-3-2 loss: 1.145117  [   96/  124]
train() client id: f_00002-4-0 loss: 1.030347  [   32/  124]
train() client id: f_00002-4-1 loss: 1.080148  [   64/  124]
train() client id: f_00002-4-2 loss: 1.034254  [   96/  124]
train() client id: f_00002-5-0 loss: 1.013873  [   32/  124]
train() client id: f_00002-5-1 loss: 1.070089  [   64/  124]
train() client id: f_00002-5-2 loss: 1.022530  [   96/  124]
train() client id: f_00002-6-0 loss: 1.002205  [   32/  124]
train() client id: f_00002-6-1 loss: 1.006722  [   64/  124]
train() client id: f_00002-6-2 loss: 0.975485  [   96/  124]
train() client id: f_00002-7-0 loss: 1.038246  [   32/  124]
train() client id: f_00002-7-1 loss: 1.000391  [   64/  124]
train() client id: f_00002-7-2 loss: 0.951742  [   96/  124]
train() client id: f_00002-8-0 loss: 0.944983  [   32/  124]
train() client id: f_00002-8-1 loss: 0.961945  [   64/  124]
train() client id: f_00002-8-2 loss: 1.002167  [   96/  124]
train() client id: f_00002-9-0 loss: 0.941520  [   32/  124]
train() client id: f_00002-9-1 loss: 0.889044  [   64/  124]
train() client id: f_00002-9-2 loss: 1.048176  [   96/  124]
train() client id: f_00002-10-0 loss: 0.949923  [   32/  124]
train() client id: f_00002-10-1 loss: 0.989627  [   64/  124]
train() client id: f_00002-10-2 loss: 0.890838  [   96/  124]
train() client id: f_00002-11-0 loss: 0.984061  [   32/  124]
train() client id: f_00002-11-1 loss: 0.792427  [   64/  124]
train() client id: f_00002-11-2 loss: 0.996833  [   96/  124]
train() client id: f_00003-0-0 loss: 0.923498  [   32/   43]
train() client id: f_00003-1-0 loss: 0.907935  [   32/   43]
train() client id: f_00003-2-0 loss: 0.878385  [   32/   43]
train() client id: f_00003-3-0 loss: 1.037889  [   32/   43]
train() client id: f_00003-4-0 loss: 1.122722  [   32/   43]
train() client id: f_00003-5-0 loss: 1.131875  [   32/   43]
train() client id: f_00003-6-0 loss: 0.994569  [   32/   43]
train() client id: f_00003-7-0 loss: 0.884559  [   32/   43]
train() client id: f_00003-8-0 loss: 1.090246  [   32/   43]
train() client id: f_00003-9-0 loss: 0.980933  [   32/   43]
train() client id: f_00003-10-0 loss: 0.983658  [   32/   43]
train() client id: f_00003-11-0 loss: 1.017772  [   32/   43]
train() client id: f_00004-0-0 loss: 1.053119  [   32/  306]
train() client id: f_00004-0-1 loss: 1.032045  [   64/  306]
train() client id: f_00004-0-2 loss: 0.875094  [   96/  306]
train() client id: f_00004-0-3 loss: 0.901501  [  128/  306]
train() client id: f_00004-0-4 loss: 1.009593  [  160/  306]
train() client id: f_00004-0-5 loss: 0.853603  [  192/  306]
train() client id: f_00004-0-6 loss: 0.920540  [  224/  306]
train() client id: f_00004-0-7 loss: 0.970985  [  256/  306]
train() client id: f_00004-0-8 loss: 0.890444  [  288/  306]
train() client id: f_00004-1-0 loss: 0.998712  [   32/  306]
train() client id: f_00004-1-1 loss: 1.123993  [   64/  306]
train() client id: f_00004-1-2 loss: 0.870130  [   96/  306]
train() client id: f_00004-1-3 loss: 1.016455  [  128/  306]
train() client id: f_00004-1-4 loss: 0.965913  [  160/  306]
train() client id: f_00004-1-5 loss: 0.970606  [  192/  306]
train() client id: f_00004-1-6 loss: 0.878578  [  224/  306]
train() client id: f_00004-1-7 loss: 0.865102  [  256/  306]
train() client id: f_00004-1-8 loss: 0.914884  [  288/  306]
train() client id: f_00004-2-0 loss: 0.757166  [   32/  306]
train() client id: f_00004-2-1 loss: 0.936100  [   64/  306]
train() client id: f_00004-2-2 loss: 0.965875  [   96/  306]
train() client id: f_00004-2-3 loss: 1.018643  [  128/  306]
train() client id: f_00004-2-4 loss: 1.040342  [  160/  306]
train() client id: f_00004-2-5 loss: 0.946571  [  192/  306]
train() client id: f_00004-2-6 loss: 0.975700  [  224/  306]
train() client id: f_00004-2-7 loss: 0.929336  [  256/  306]
train() client id: f_00004-2-8 loss: 0.986585  [  288/  306]
train() client id: f_00004-3-0 loss: 0.976952  [   32/  306]
train() client id: f_00004-3-1 loss: 1.085571  [   64/  306]
train() client id: f_00004-3-2 loss: 0.959388  [   96/  306]
train() client id: f_00004-3-3 loss: 0.861909  [  128/  306]
train() client id: f_00004-3-4 loss: 0.990548  [  160/  306]
train() client id: f_00004-3-5 loss: 0.875882  [  192/  306]
train() client id: f_00004-3-6 loss: 1.012187  [  224/  306]
train() client id: f_00004-3-7 loss: 0.852731  [  256/  306]
train() client id: f_00004-3-8 loss: 0.915886  [  288/  306]
train() client id: f_00004-4-0 loss: 1.062940  [   32/  306]
train() client id: f_00004-4-1 loss: 0.975998  [   64/  306]
train() client id: f_00004-4-2 loss: 0.929429  [   96/  306]
train() client id: f_00004-4-3 loss: 1.058475  [  128/  306]
train() client id: f_00004-4-4 loss: 0.835309  [  160/  306]
train() client id: f_00004-4-5 loss: 0.909971  [  192/  306]
train() client id: f_00004-4-6 loss: 0.949952  [  224/  306]
train() client id: f_00004-4-7 loss: 0.962927  [  256/  306]
train() client id: f_00004-4-8 loss: 0.841595  [  288/  306]
train() client id: f_00004-5-0 loss: 1.000560  [   32/  306]
train() client id: f_00004-5-1 loss: 1.026783  [   64/  306]
train() client id: f_00004-5-2 loss: 0.939318  [   96/  306]
train() client id: f_00004-5-3 loss: 0.915470  [  128/  306]
train() client id: f_00004-5-4 loss: 0.945847  [  160/  306]
train() client id: f_00004-5-5 loss: 0.815270  [  192/  306]
train() client id: f_00004-5-6 loss: 0.900182  [  224/  306]
train() client id: f_00004-5-7 loss: 1.017931  [  256/  306]
train() client id: f_00004-5-8 loss: 1.012495  [  288/  306]
train() client id: f_00004-6-0 loss: 1.013577  [   32/  306]
train() client id: f_00004-6-1 loss: 0.955442  [   64/  306]
train() client id: f_00004-6-2 loss: 0.922472  [   96/  306]
train() client id: f_00004-6-3 loss: 0.863176  [  128/  306]
train() client id: f_00004-6-4 loss: 0.866300  [  160/  306]
train() client id: f_00004-6-5 loss: 1.076072  [  192/  306]
train() client id: f_00004-6-6 loss: 0.972334  [  224/  306]
train() client id: f_00004-6-7 loss: 0.996454  [  256/  306]
train() client id: f_00004-6-8 loss: 0.918237  [  288/  306]
train() client id: f_00004-7-0 loss: 0.996188  [   32/  306]
train() client id: f_00004-7-1 loss: 0.865277  [   64/  306]
train() client id: f_00004-7-2 loss: 0.864135  [   96/  306]
train() client id: f_00004-7-3 loss: 1.030071  [  128/  306]
train() client id: f_00004-7-4 loss: 0.887869  [  160/  306]
train() client id: f_00004-7-5 loss: 1.069224  [  192/  306]
train() client id: f_00004-7-6 loss: 0.847022  [  224/  306]
train() client id: f_00004-7-7 loss: 0.877234  [  256/  306]
train() client id: f_00004-7-8 loss: 1.066506  [  288/  306]
train() client id: f_00004-8-0 loss: 0.904136  [   32/  306]
train() client id: f_00004-8-1 loss: 0.926780  [   64/  306]
train() client id: f_00004-8-2 loss: 1.077894  [   96/  306]
train() client id: f_00004-8-3 loss: 0.929426  [  128/  306]
train() client id: f_00004-8-4 loss: 0.959639  [  160/  306]
train() client id: f_00004-8-5 loss: 0.993353  [  192/  306]
train() client id: f_00004-8-6 loss: 0.947182  [  224/  306]
train() client id: f_00004-8-7 loss: 0.954296  [  256/  306]
train() client id: f_00004-8-8 loss: 0.863594  [  288/  306]
train() client id: f_00004-9-0 loss: 0.954637  [   32/  306]
train() client id: f_00004-9-1 loss: 0.780318  [   64/  306]
train() client id: f_00004-9-2 loss: 0.959025  [   96/  306]
train() client id: f_00004-9-3 loss: 1.014655  [  128/  306]
train() client id: f_00004-9-4 loss: 1.000463  [  160/  306]
train() client id: f_00004-9-5 loss: 1.066396  [  192/  306]
train() client id: f_00004-9-6 loss: 0.889984  [  224/  306]
train() client id: f_00004-9-7 loss: 1.062235  [  256/  306]
train() client id: f_00004-9-8 loss: 0.859662  [  288/  306]
train() client id: f_00004-10-0 loss: 1.079693  [   32/  306]
train() client id: f_00004-10-1 loss: 0.945563  [   64/  306]
train() client id: f_00004-10-2 loss: 0.934686  [   96/  306]
train() client id: f_00004-10-3 loss: 0.960984  [  128/  306]
train() client id: f_00004-10-4 loss: 0.900099  [  160/  306]
train() client id: f_00004-10-5 loss: 0.974612  [  192/  306]
train() client id: f_00004-10-6 loss: 0.942486  [  224/  306]
train() client id: f_00004-10-7 loss: 0.943930  [  256/  306]
train() client id: f_00004-10-8 loss: 0.887047  [  288/  306]
train() client id: f_00004-11-0 loss: 0.828201  [   32/  306]
train() client id: f_00004-11-1 loss: 0.907417  [   64/  306]
train() client id: f_00004-11-2 loss: 1.019376  [   96/  306]
train() client id: f_00004-11-3 loss: 1.011664  [  128/  306]
train() client id: f_00004-11-4 loss: 0.930923  [  160/  306]
train() client id: f_00004-11-5 loss: 0.901831  [  192/  306]
train() client id: f_00004-11-6 loss: 0.912580  [  224/  306]
train() client id: f_00004-11-7 loss: 0.996976  [  256/  306]
train() client id: f_00004-11-8 loss: 1.068565  [  288/  306]
train() client id: f_00005-0-0 loss: 0.808884  [   32/  146]
train() client id: f_00005-0-1 loss: 0.599558  [   64/  146]
train() client id: f_00005-0-2 loss: 0.590354  [   96/  146]
train() client id: f_00005-0-3 loss: 0.608818  [  128/  146]
train() client id: f_00005-1-0 loss: 0.702944  [   32/  146]
train() client id: f_00005-1-1 loss: 0.643285  [   64/  146]
train() client id: f_00005-1-2 loss: 0.513044  [   96/  146]
train() client id: f_00005-1-3 loss: 0.607934  [  128/  146]
train() client id: f_00005-2-0 loss: 0.586066  [   32/  146]
train() client id: f_00005-2-1 loss: 0.644378  [   64/  146]
train() client id: f_00005-2-2 loss: 0.497057  [   96/  146]
train() client id: f_00005-2-3 loss: 0.659572  [  128/  146]
train() client id: f_00005-3-0 loss: 0.547447  [   32/  146]
train() client id: f_00005-3-1 loss: 0.651197  [   64/  146]
train() client id: f_00005-3-2 loss: 0.793700  [   96/  146]
train() client id: f_00005-3-3 loss: 0.491429  [  128/  146]
train() client id: f_00005-4-0 loss: 0.704804  [   32/  146]
train() client id: f_00005-4-1 loss: 0.491609  [   64/  146]
train() client id: f_00005-4-2 loss: 0.689126  [   96/  146]
train() client id: f_00005-4-3 loss: 0.610020  [  128/  146]
train() client id: f_00005-5-0 loss: 0.657902  [   32/  146]
train() client id: f_00005-5-1 loss: 0.709516  [   64/  146]
train() client id: f_00005-5-2 loss: 0.649588  [   96/  146]
train() client id: f_00005-5-3 loss: 0.581261  [  128/  146]
train() client id: f_00005-6-0 loss: 0.640446  [   32/  146]
train() client id: f_00005-6-1 loss: 0.597244  [   64/  146]
train() client id: f_00005-6-2 loss: 0.475149  [   96/  146]
train() client id: f_00005-6-3 loss: 0.726368  [  128/  146]
train() client id: f_00005-7-0 loss: 0.607759  [   32/  146]
train() client id: f_00005-7-1 loss: 0.874709  [   64/  146]
train() client id: f_00005-7-2 loss: 0.453394  [   96/  146]
train() client id: f_00005-7-3 loss: 0.610905  [  128/  146]
train() client id: f_00005-8-0 loss: 0.577005  [   32/  146]
train() client id: f_00005-8-1 loss: 0.663276  [   64/  146]
train() client id: f_00005-8-2 loss: 0.577282  [   96/  146]
train() client id: f_00005-8-3 loss: 0.668780  [  128/  146]
train() client id: f_00005-9-0 loss: 0.475327  [   32/  146]
train() client id: f_00005-9-1 loss: 0.746315  [   64/  146]
train() client id: f_00005-9-2 loss: 0.556407  [   96/  146]
train() client id: f_00005-9-3 loss: 0.567766  [  128/  146]
train() client id: f_00005-10-0 loss: 0.417840  [   32/  146]
train() client id: f_00005-10-1 loss: 0.775821  [   64/  146]
train() client id: f_00005-10-2 loss: 0.601291  [   96/  146]
train() client id: f_00005-10-3 loss: 0.539340  [  128/  146]
train() client id: f_00005-11-0 loss: 0.596510  [   32/  146]
train() client id: f_00005-11-1 loss: 0.628313  [   64/  146]
train() client id: f_00005-11-2 loss: 0.578750  [   96/  146]
train() client id: f_00005-11-3 loss: 0.604786  [  128/  146]
train() client id: f_00006-0-0 loss: 0.653506  [   32/   54]
train() client id: f_00006-1-0 loss: 0.672886  [   32/   54]
train() client id: f_00006-2-0 loss: 0.650732  [   32/   54]
train() client id: f_00006-3-0 loss: 0.650146  [   32/   54]
train() client id: f_00006-4-0 loss: 0.665335  [   32/   54]
train() client id: f_00006-5-0 loss: 0.613986  [   32/   54]
train() client id: f_00006-6-0 loss: 0.630686  [   32/   54]
train() client id: f_00006-7-0 loss: 0.674076  [   32/   54]
train() client id: f_00006-8-0 loss: 0.687955  [   32/   54]
train() client id: f_00006-9-0 loss: 0.683474  [   32/   54]
train() client id: f_00006-10-0 loss: 0.645616  [   32/   54]
train() client id: f_00006-11-0 loss: 0.695361  [   32/   54]
train() client id: f_00007-0-0 loss: 0.782408  [   32/  179]
train() client id: f_00007-0-1 loss: 0.721902  [   64/  179]
train() client id: f_00007-0-2 loss: 0.743900  [   96/  179]
train() client id: f_00007-0-3 loss: 0.742454  [  128/  179]
train() client id: f_00007-0-4 loss: 0.792952  [  160/  179]
train() client id: f_00007-1-0 loss: 0.645005  [   32/  179]
train() client id: f_00007-1-1 loss: 0.706585  [   64/  179]
train() client id: f_00007-1-2 loss: 0.738923  [   96/  179]
train() client id: f_00007-1-3 loss: 0.839237  [  128/  179]
train() client id: f_00007-1-4 loss: 0.681238  [  160/  179]
train() client id: f_00007-2-0 loss: 0.797061  [   32/  179]
train() client id: f_00007-2-1 loss: 0.760808  [   64/  179]
train() client id: f_00007-2-2 loss: 0.629985  [   96/  179]
train() client id: f_00007-2-3 loss: 0.603706  [  128/  179]
train() client id: f_00007-2-4 loss: 0.834695  [  160/  179]
train() client id: f_00007-3-0 loss: 0.643191  [   32/  179]
train() client id: f_00007-3-1 loss: 0.822937  [   64/  179]
train() client id: f_00007-3-2 loss: 0.563828  [   96/  179]
train() client id: f_00007-3-3 loss: 0.670132  [  128/  179]
train() client id: f_00007-3-4 loss: 0.733854  [  160/  179]
train() client id: f_00007-4-0 loss: 0.718765  [   32/  179]
train() client id: f_00007-4-1 loss: 0.725485  [   64/  179]
train() client id: f_00007-4-2 loss: 0.684357  [   96/  179]
train() client id: f_00007-4-3 loss: 0.604732  [  128/  179]
train() client id: f_00007-4-4 loss: 0.593474  [  160/  179]
train() client id: f_00007-5-0 loss: 0.765553  [   32/  179]
train() client id: f_00007-5-1 loss: 0.686819  [   64/  179]
train() client id: f_00007-5-2 loss: 0.644545  [   96/  179]
train() client id: f_00007-5-3 loss: 0.577507  [  128/  179]
train() client id: f_00007-5-4 loss: 0.810647  [  160/  179]
train() client id: f_00007-6-0 loss: 0.563599  [   32/  179]
train() client id: f_00007-6-1 loss: 0.644393  [   64/  179]
train() client id: f_00007-6-2 loss: 0.660818  [   96/  179]
train() client id: f_00007-6-3 loss: 0.724959  [  128/  179]
train() client id: f_00007-6-4 loss: 0.869162  [  160/  179]
train() client id: f_00007-7-0 loss: 0.690366  [   32/  179]
train() client id: f_00007-7-1 loss: 0.710857  [   64/  179]
train() client id: f_00007-7-2 loss: 0.564158  [   96/  179]
train() client id: f_00007-7-3 loss: 0.555927  [  128/  179]
train() client id: f_00007-7-4 loss: 0.753090  [  160/  179]
train() client id: f_00007-8-0 loss: 0.751421  [   32/  179]
train() client id: f_00007-8-1 loss: 0.573949  [   64/  179]
train() client id: f_00007-8-2 loss: 0.534788  [   96/  179]
train() client id: f_00007-8-3 loss: 0.721742  [  128/  179]
train() client id: f_00007-8-4 loss: 0.837936  [  160/  179]
train() client id: f_00007-9-0 loss: 0.651865  [   32/  179]
train() client id: f_00007-9-1 loss: 0.687278  [   64/  179]
train() client id: f_00007-9-2 loss: 0.688003  [   96/  179]
train() client id: f_00007-9-3 loss: 0.741537  [  128/  179]
train() client id: f_00007-9-4 loss: 0.560381  [  160/  179]
train() client id: f_00007-10-0 loss: 0.565903  [   32/  179]
train() client id: f_00007-10-1 loss: 0.538165  [   64/  179]
train() client id: f_00007-10-2 loss: 0.694915  [   96/  179]
train() client id: f_00007-10-3 loss: 0.728421  [  128/  179]
train() client id: f_00007-10-4 loss: 0.719423  [  160/  179]
train() client id: f_00007-11-0 loss: 0.627295  [   32/  179]
train() client id: f_00007-11-1 loss: 0.635320  [   64/  179]
train() client id: f_00007-11-2 loss: 0.685744  [   96/  179]
train() client id: f_00007-11-3 loss: 0.828606  [  128/  179]
train() client id: f_00007-11-4 loss: 0.631364  [  160/  179]
train() client id: f_00008-0-0 loss: 0.900338  [   32/  130]
train() client id: f_00008-0-1 loss: 0.737613  [   64/  130]
train() client id: f_00008-0-2 loss: 0.707258  [   96/  130]
train() client id: f_00008-0-3 loss: 0.850999  [  128/  130]
train() client id: f_00008-1-0 loss: 0.752983  [   32/  130]
train() client id: f_00008-1-1 loss: 0.825437  [   64/  130]
train() client id: f_00008-1-2 loss: 0.882781  [   96/  130]
train() client id: f_00008-1-3 loss: 0.742369  [  128/  130]
train() client id: f_00008-2-0 loss: 0.850261  [   32/  130]
train() client id: f_00008-2-1 loss: 0.862280  [   64/  130]
train() client id: f_00008-2-2 loss: 0.755065  [   96/  130]
train() client id: f_00008-2-3 loss: 0.719744  [  128/  130]
train() client id: f_00008-3-0 loss: 0.841806  [   32/  130]
train() client id: f_00008-3-1 loss: 0.768207  [   64/  130]
train() client id: f_00008-3-2 loss: 0.877183  [   96/  130]
train() client id: f_00008-3-3 loss: 0.647391  [  128/  130]
train() client id: f_00008-4-0 loss: 0.858216  [   32/  130]
train() client id: f_00008-4-1 loss: 0.870614  [   64/  130]
train() client id: f_00008-4-2 loss: 0.747646  [   96/  130]
train() client id: f_00008-4-3 loss: 0.701666  [  128/  130]
train() client id: f_00008-5-0 loss: 0.782809  [   32/  130]
train() client id: f_00008-5-1 loss: 0.739932  [   64/  130]
train() client id: f_00008-5-2 loss: 0.786879  [   96/  130]
train() client id: f_00008-5-3 loss: 0.849481  [  128/  130]
train() client id: f_00008-6-0 loss: 0.698853  [   32/  130]
train() client id: f_00008-6-1 loss: 0.778316  [   64/  130]
train() client id: f_00008-6-2 loss: 0.846139  [   96/  130]
train() client id: f_00008-6-3 loss: 0.851665  [  128/  130]
train() client id: f_00008-7-0 loss: 0.791973  [   32/  130]
train() client id: f_00008-7-1 loss: 0.712552  [   64/  130]
train() client id: f_00008-7-2 loss: 0.926935  [   96/  130]
train() client id: f_00008-7-3 loss: 0.740149  [  128/  130]
train() client id: f_00008-8-0 loss: 0.765595  [   32/  130]
train() client id: f_00008-8-1 loss: 0.863401  [   64/  130]
train() client id: f_00008-8-2 loss: 0.846713  [   96/  130]
train() client id: f_00008-8-3 loss: 0.649400  [  128/  130]
train() client id: f_00008-9-0 loss: 0.720151  [   32/  130]
train() client id: f_00008-9-1 loss: 0.810319  [   64/  130]
train() client id: f_00008-9-2 loss: 0.910644  [   96/  130]
train() client id: f_00008-9-3 loss: 0.726874  [  128/  130]
train() client id: f_00008-10-0 loss: 0.823083  [   32/  130]
train() client id: f_00008-10-1 loss: 0.765951  [   64/  130]
train() client id: f_00008-10-2 loss: 0.743054  [   96/  130]
train() client id: f_00008-10-3 loss: 0.832297  [  128/  130]
train() client id: f_00008-11-0 loss: 0.774153  [   32/  130]
train() client id: f_00008-11-1 loss: 0.867209  [   64/  130]
train() client id: f_00008-11-2 loss: 0.738613  [   96/  130]
train() client id: f_00008-11-3 loss: 0.776640  [  128/  130]
train() client id: f_00009-0-0 loss: 1.275701  [   32/  118]
train() client id: f_00009-0-1 loss: 1.169738  [   64/  118]
train() client id: f_00009-0-2 loss: 1.219409  [   96/  118]
train() client id: f_00009-1-0 loss: 1.157748  [   32/  118]
train() client id: f_00009-1-1 loss: 1.216371  [   64/  118]
train() client id: f_00009-1-2 loss: 1.192075  [   96/  118]
train() client id: f_00009-2-0 loss: 1.126102  [   32/  118]
train() client id: f_00009-2-1 loss: 1.140890  [   64/  118]
train() client id: f_00009-2-2 loss: 1.170626  [   96/  118]
train() client id: f_00009-3-0 loss: 1.067826  [   32/  118]
train() client id: f_00009-3-1 loss: 1.094826  [   64/  118]
train() client id: f_00009-3-2 loss: 1.093435  [   96/  118]
train() client id: f_00009-4-0 loss: 1.075430  [   32/  118]
train() client id: f_00009-4-1 loss: 1.015631  [   64/  118]
train() client id: f_00009-4-2 loss: 1.076398  [   96/  118]
train() client id: f_00009-5-0 loss: 0.983202  [   32/  118]
train() client id: f_00009-5-1 loss: 0.998039  [   64/  118]
train() client id: f_00009-5-2 loss: 1.062955  [   96/  118]
train() client id: f_00009-6-0 loss: 1.063593  [   32/  118]
train() client id: f_00009-6-1 loss: 0.947637  [   64/  118]
train() client id: f_00009-6-2 loss: 1.044283  [   96/  118]
train() client id: f_00009-7-0 loss: 0.989643  [   32/  118]
train() client id: f_00009-7-1 loss: 1.055136  [   64/  118]
train() client id: f_00009-7-2 loss: 0.932057  [   96/  118]
train() client id: f_00009-8-0 loss: 1.033539  [   32/  118]
train() client id: f_00009-8-1 loss: 0.941472  [   64/  118]
train() client id: f_00009-8-2 loss: 0.937716  [   96/  118]
train() client id: f_00009-9-0 loss: 0.937937  [   32/  118]
train() client id: f_00009-9-1 loss: 0.908637  [   64/  118]
train() client id: f_00009-9-2 loss: 1.063797  [   96/  118]
train() client id: f_00009-10-0 loss: 0.954953  [   32/  118]
train() client id: f_00009-10-1 loss: 0.936628  [   64/  118]
train() client id: f_00009-10-2 loss: 0.959186  [   96/  118]
train() client id: f_00009-11-0 loss: 0.886557  [   32/  118]
train() client id: f_00009-11-1 loss: 0.936813  [   64/  118]
train() client id: f_00009-11-2 loss: 1.033044  [   96/  118]
At round 11 accuracy: 0.6312997347480106
At round 11 training accuracy: 0.5727699530516432
At round 11 training loss: 0.8664413899841216
update_location
xs = [ -3.9056584    4.20031788  75.00902392  18.81129433   0.97929623
   3.95640986 -37.44319194 -16.32485185  59.66397685  -2.06087855]
ys = [ 67.5879595   50.55583871   1.32061395 -37.45517586  29.35018685
  12.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [120.76169275 112.13177738 125.01239015 108.42857093 104.22280215
 100.89527064 106.81237365 101.32707952 117.76442632 100.10124413]
dists_bs = [202.0163859  218.17218706 304.41590165 287.5761109  228.43506905
 241.51531283 224.66087888 235.6176078  282.58043084 243.20450851]
uav_gains = [6.23955507e-11 7.51052065e-11 5.72241493e-11 8.16837356e-11
 9.01758519e-11 9.77960164e-11 8.48090092e-11 9.67574158e-11
 6.64428547e-11 9.97469659e-11]
bs_gains = [3.87446385e-11 3.12360282e-11 1.22909527e-11 1.44140568e-11
 2.74636534e-11 2.34989796e-11 2.87751248e-11 2.51832865e-11
 1.51389674e-11 2.30448319e-11]
Round 12
-------------------------------
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.40743039 19.62699225  9.26366007  3.31236285 22.63944362 10.91364539
  4.11791779 13.28077427  9.76466884  8.86129599]
obj_prev = 111.18819146740302
eta_min = 1.3309903798194387e-10	eta_max = 0.9206473861818304
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 25.855891314518058	eta = 0.9090909090909091
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 44.77490344422677	eta = 0.5249672010962586
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 35.73954375539702	eta = 0.6576848294802706
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.11938254858583	eta = 0.6889150384535667
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.039315064938776	eta = 0.6905355085914164
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.039105597683566	eta = 0.6905397579562299
eta = 0.6905397579562299
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [0.03055476 0.064262   0.03006976 0.01042742 0.07420442 0.03540472
 0.0130949  0.04340716 0.03152475 0.02861478]
ene_total = [2.91251884 5.6185508  2.88405621 1.31991488 6.41048895 3.41632264
 1.52386453 3.86970026 3.19241569 2.89127279]
ti_comp = [0.30907982 0.29827603 0.3078889  0.3125449  0.295892   0.29281253
 0.31300078 0.31455258 0.28279738 0.29241128]
ti_coms = [0.06887363 0.07967742 0.07006456 0.06540855 0.08206146 0.08514092
 0.06495268 0.06340088 0.09515608 0.08554218]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.86627385e-05 1.86425773e-04 1.79259442e-05 7.25413394e-07
 2.91677471e-04 3.23507450e-05 1.43250282e-06 5.16627645e-05
 2.44841111e-05 1.71262716e-05]
ene_total = [0.53720059 0.6342901  0.54640715 0.5088514  0.66102208 0.66480406
 0.50536026 0.49719653 0.74209725 0.66674105]
optimize_network_iter = 0 obj = 5.96397047673148
eta = 0.6905397579562299
freqs = [4.94285918e+07 1.07722361e+08 4.88321676e+07 1.66814724e+07
 1.25391054e+08 6.04562965e+07 2.09183144e+07 6.89982586e+07
 5.57373503e+07 4.89289881e+07]
eta_min = 0.6905397579563017	eta_max = 0.6905397579562269
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 0.04594401078389732	eta = 0.909090909090909
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 20.175396261636216	eta = 0.002070208782478131
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.0926044394311156	eta = 0.019959473345172857
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.034227485351597	eta = 0.020532257494100595
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.0342105771628316	eta = 0.02053242815651352
eta = 0.02053242815651352
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.95847449e-04 1.95635877e-03 1.88115503e-04 7.61251424e-06
 3.06087387e-03 3.39489881e-04 1.50327361e-05 5.42150909e-04
 2.56937143e-04 1.79723710e-04]
ene_total = [0.17408175 0.24390138 0.17681862 0.16093954 0.27690584 0.21759158
 0.16000151 0.16914252 0.24017663 0.21465121]
ti_comp = [0.30907982 0.29827603 0.3078889  0.3125449  0.295892   0.29281253
 0.31300078 0.31455258 0.28279738 0.29241128]
ti_coms = [0.06887363 0.07967742 0.07006456 0.06540855 0.08206146 0.08514092
 0.06495268 0.06340088 0.09515608 0.08554218]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.86627385e-05 1.86425773e-04 1.79259442e-05 7.25413394e-07
 2.91677471e-04 3.23507450e-05 1.43250282e-06 5.16627645e-05
 2.44841111e-05 1.71262716e-05]
ene_total = [0.53720059 0.6342901  0.54640715 0.5088514  0.66102208 0.66480406
 0.50536026 0.49719653 0.74209725 0.66674105]
optimize_network_iter = 1 obj = 5.963970476732851
eta = 0.6905397579563017
freqs = [4.94285918e+07 1.07722361e+08 4.88321676e+07 1.66814724e+07
 1.25391054e+08 6.04562965e+07 2.09183144e+07 6.89982586e+07
 5.57373503e+07 4.89289881e+07]
Done!
At round 12 eta: 0.6905397579563017
At round 12 local rounds: 12.124905073660063
At round 12 global rounds: 77.78722246725123
At round 12 a_n: 23.72950684565325
gradient difference: 0.4210842251777649
train() client id: f_00000-0-0 loss: 1.324293  [   32/  126]
train() client id: f_00000-0-1 loss: 1.323596  [   64/  126]
train() client id: f_00000-0-2 loss: 1.217027  [   96/  126]
train() client id: f_00000-1-0 loss: 1.291332  [   32/  126]
train() client id: f_00000-1-1 loss: 1.142136  [   64/  126]
train() client id: f_00000-1-2 loss: 1.125569  [   96/  126]
train() client id: f_00000-2-0 loss: 1.051350  [   32/  126]
train() client id: f_00000-2-1 loss: 1.119275  [   64/  126]
train() client id: f_00000-2-2 loss: 1.101688  [   96/  126]
train() client id: f_00000-3-0 loss: 1.024387  [   32/  126]
train() client id: f_00000-3-1 loss: 1.073883  [   64/  126]
train() client id: f_00000-3-2 loss: 1.117607  [   96/  126]
train() client id: f_00000-4-0 loss: 1.000096  [   32/  126]
train() client id: f_00000-4-1 loss: 0.996189  [   64/  126]
train() client id: f_00000-4-2 loss: 1.063964  [   96/  126]
train() client id: f_00000-5-0 loss: 1.065777  [   32/  126]
train() client id: f_00000-5-1 loss: 1.043349  [   64/  126]
train() client id: f_00000-5-2 loss: 1.035055  [   96/  126]
train() client id: f_00000-6-0 loss: 1.010506  [   32/  126]
train() client id: f_00000-6-1 loss: 0.914744  [   64/  126]
train() client id: f_00000-6-2 loss: 1.002638  [   96/  126]
train() client id: f_00000-7-0 loss: 0.979070  [   32/  126]
train() client id: f_00000-7-1 loss: 0.995112  [   64/  126]
train() client id: f_00000-7-2 loss: 0.908278  [   96/  126]
train() client id: f_00000-8-0 loss: 1.023361  [   32/  126]
train() client id: f_00000-8-1 loss: 0.822951  [   64/  126]
train() client id: f_00000-8-2 loss: 0.937450  [   96/  126]
train() client id: f_00000-9-0 loss: 0.941439  [   32/  126]
train() client id: f_00000-9-1 loss: 0.956974  [   64/  126]
train() client id: f_00000-9-2 loss: 0.959925  [   96/  126]
train() client id: f_00000-10-0 loss: 1.001522  [   32/  126]
train() client id: f_00000-10-1 loss: 0.890822  [   64/  126]
train() client id: f_00000-10-2 loss: 0.966955  [   96/  126]
train() client id: f_00000-11-0 loss: 0.986517  [   32/  126]
train() client id: f_00000-11-1 loss: 0.931755  [   64/  126]
train() client id: f_00000-11-2 loss: 0.888913  [   96/  126]
train() client id: f_00001-0-0 loss: 0.524284  [   32/  265]
train() client id: f_00001-0-1 loss: 0.635847  [   64/  265]
train() client id: f_00001-0-2 loss: 0.545161  [   96/  265]
train() client id: f_00001-0-3 loss: 0.522493  [  128/  265]
train() client id: f_00001-0-4 loss: 0.480723  [  160/  265]
train() client id: f_00001-0-5 loss: 0.535853  [  192/  265]
train() client id: f_00001-0-6 loss: 0.463713  [  224/  265]
train() client id: f_00001-0-7 loss: 0.527951  [  256/  265]
train() client id: f_00001-1-0 loss: 0.548536  [   32/  265]
train() client id: f_00001-1-1 loss: 0.627928  [   64/  265]
train() client id: f_00001-1-2 loss: 0.446167  [   96/  265]
train() client id: f_00001-1-3 loss: 0.463242  [  128/  265]
train() client id: f_00001-1-4 loss: 0.425641  [  160/  265]
train() client id: f_00001-1-5 loss: 0.470116  [  192/  265]
train() client id: f_00001-1-6 loss: 0.611999  [  224/  265]
train() client id: f_00001-1-7 loss: 0.573347  [  256/  265]
train() client id: f_00001-2-0 loss: 0.429328  [   32/  265]
train() client id: f_00001-2-1 loss: 0.450899  [   64/  265]
train() client id: f_00001-2-2 loss: 0.606048  [   96/  265]
train() client id: f_00001-2-3 loss: 0.656110  [  128/  265]
train() client id: f_00001-2-4 loss: 0.557602  [  160/  265]
train() client id: f_00001-2-5 loss: 0.426744  [  192/  265]
train() client id: f_00001-2-6 loss: 0.503370  [  224/  265]
train() client id: f_00001-2-7 loss: 0.461162  [  256/  265]
train() client id: f_00001-3-0 loss: 0.567982  [   32/  265]
train() client id: f_00001-3-1 loss: 0.474885  [   64/  265]
train() client id: f_00001-3-2 loss: 0.569039  [   96/  265]
train() client id: f_00001-3-3 loss: 0.512711  [  128/  265]
train() client id: f_00001-3-4 loss: 0.421069  [  160/  265]
train() client id: f_00001-3-5 loss: 0.462120  [  192/  265]
train() client id: f_00001-3-6 loss: 0.473850  [  224/  265]
train() client id: f_00001-3-7 loss: 0.572513  [  256/  265]
train() client id: f_00001-4-0 loss: 0.511895  [   32/  265]
train() client id: f_00001-4-1 loss: 0.432772  [   64/  265]
train() client id: f_00001-4-2 loss: 0.430205  [   96/  265]
train() client id: f_00001-4-3 loss: 0.526179  [  128/  265]
train() client id: f_00001-4-4 loss: 0.532394  [  160/  265]
train() client id: f_00001-4-5 loss: 0.460921  [  192/  265]
train() client id: f_00001-4-6 loss: 0.448240  [  224/  265]
train() client id: f_00001-4-7 loss: 0.567009  [  256/  265]
train() client id: f_00001-5-0 loss: 0.566096  [   32/  265]
train() client id: f_00001-5-1 loss: 0.443281  [   64/  265]
train() client id: f_00001-5-2 loss: 0.459541  [   96/  265]
train() client id: f_00001-5-3 loss: 0.421767  [  128/  265]
train() client id: f_00001-5-4 loss: 0.553685  [  160/  265]
train() client id: f_00001-5-5 loss: 0.577955  [  192/  265]
train() client id: f_00001-5-6 loss: 0.432134  [  224/  265]
train() client id: f_00001-5-7 loss: 0.427872  [  256/  265]
train() client id: f_00001-6-0 loss: 0.462172  [   32/  265]
train() client id: f_00001-6-1 loss: 0.508815  [   64/  265]
train() client id: f_00001-6-2 loss: 0.468169  [   96/  265]
train() client id: f_00001-6-3 loss: 0.474026  [  128/  265]
train() client id: f_00001-6-4 loss: 0.532693  [  160/  265]
train() client id: f_00001-6-5 loss: 0.537004  [  192/  265]
train() client id: f_00001-6-6 loss: 0.514403  [  224/  265]
train() client id: f_00001-6-7 loss: 0.409328  [  256/  265]
train() client id: f_00001-7-0 loss: 0.409212  [   32/  265]
train() client id: f_00001-7-1 loss: 0.665382  [   64/  265]
train() client id: f_00001-7-2 loss: 0.493089  [   96/  265]
train() client id: f_00001-7-3 loss: 0.444968  [  128/  265]
train() client id: f_00001-7-4 loss: 0.406793  [  160/  265]
train() client id: f_00001-7-5 loss: 0.562956  [  192/  265]
train() client id: f_00001-7-6 loss: 0.471396  [  224/  265]
train() client id: f_00001-7-7 loss: 0.440712  [  256/  265]
train() client id: f_00001-8-0 loss: 0.479181  [   32/  265]
train() client id: f_00001-8-1 loss: 0.483495  [   64/  265]
train() client id: f_00001-8-2 loss: 0.734028  [   96/  265]
train() client id: f_00001-8-3 loss: 0.370237  [  128/  265]
train() client id: f_00001-8-4 loss: 0.455518  [  160/  265]
train() client id: f_00001-8-5 loss: 0.466184  [  192/  265]
train() client id: f_00001-8-6 loss: 0.506994  [  224/  265]
train() client id: f_00001-8-7 loss: 0.441748  [  256/  265]
train() client id: f_00001-9-0 loss: 0.483225  [   32/  265]
train() client id: f_00001-9-1 loss: 0.526681  [   64/  265]
train() client id: f_00001-9-2 loss: 0.512633  [   96/  265]
train() client id: f_00001-9-3 loss: 0.414597  [  128/  265]
train() client id: f_00001-9-4 loss: 0.390744  [  160/  265]
train() client id: f_00001-9-5 loss: 0.621900  [  192/  265]
train() client id: f_00001-9-6 loss: 0.443605  [  224/  265]
train() client id: f_00001-9-7 loss: 0.533845  [  256/  265]
train() client id: f_00001-10-0 loss: 0.486579  [   32/  265]
train() client id: f_00001-10-1 loss: 0.487648  [   64/  265]
train() client id: f_00001-10-2 loss: 0.464374  [   96/  265]
train() client id: f_00001-10-3 loss: 0.534665  [  128/  265]
train() client id: f_00001-10-4 loss: 0.458293  [  160/  265]
train() client id: f_00001-10-5 loss: 0.456356  [  192/  265]
train() client id: f_00001-10-6 loss: 0.599417  [  224/  265]
train() client id: f_00001-10-7 loss: 0.450868  [  256/  265]
train() client id: f_00001-11-0 loss: 0.408415  [   32/  265]
train() client id: f_00001-11-1 loss: 0.552151  [   64/  265]
train() client id: f_00001-11-2 loss: 0.462596  [   96/  265]
train() client id: f_00001-11-3 loss: 0.437934  [  128/  265]
train() client id: f_00001-11-4 loss: 0.502037  [  160/  265]
train() client id: f_00001-11-5 loss: 0.391857  [  192/  265]
train() client id: f_00001-11-6 loss: 0.544805  [  224/  265]
train() client id: f_00001-11-7 loss: 0.640675  [  256/  265]
train() client id: f_00002-0-0 loss: 1.205714  [   32/  124]
train() client id: f_00002-0-1 loss: 1.251575  [   64/  124]
train() client id: f_00002-0-2 loss: 1.097161  [   96/  124]
train() client id: f_00002-1-0 loss: 1.067235  [   32/  124]
train() client id: f_00002-1-1 loss: 1.179545  [   64/  124]
train() client id: f_00002-1-2 loss: 1.121990  [   96/  124]
train() client id: f_00002-2-0 loss: 1.127799  [   32/  124]
train() client id: f_00002-2-1 loss: 1.109699  [   64/  124]
train() client id: f_00002-2-2 loss: 1.003463  [   96/  124]
train() client id: f_00002-3-0 loss: 0.980967  [   32/  124]
train() client id: f_00002-3-1 loss: 0.990869  [   64/  124]
train() client id: f_00002-3-2 loss: 1.040713  [   96/  124]
train() client id: f_00002-4-0 loss: 0.963296  [   32/  124]
train() client id: f_00002-4-1 loss: 1.021533  [   64/  124]
train() client id: f_00002-4-2 loss: 1.021783  [   96/  124]
train() client id: f_00002-5-0 loss: 0.889977  [   32/  124]
train() client id: f_00002-5-1 loss: 1.040903  [   64/  124]
train() client id: f_00002-5-2 loss: 1.029344  [   96/  124]
train() client id: f_00002-6-0 loss: 0.967752  [   32/  124]
train() client id: f_00002-6-1 loss: 1.076322  [   64/  124]
train() client id: f_00002-6-2 loss: 0.887979  [   96/  124]
train() client id: f_00002-7-0 loss: 0.891514  [   32/  124]
train() client id: f_00002-7-1 loss: 0.999860  [   64/  124]
train() client id: f_00002-7-2 loss: 0.945887  [   96/  124]
train() client id: f_00002-8-0 loss: 0.925290  [   32/  124]
train() client id: f_00002-8-1 loss: 0.938014  [   64/  124]
train() client id: f_00002-8-2 loss: 0.980297  [   96/  124]
train() client id: f_00002-9-0 loss: 1.039538  [   32/  124]
train() client id: f_00002-9-1 loss: 0.966032  [   64/  124]
train() client id: f_00002-9-2 loss: 0.926916  [   96/  124]
train() client id: f_00002-10-0 loss: 0.953677  [   32/  124]
train() client id: f_00002-10-1 loss: 0.926150  [   64/  124]
train() client id: f_00002-10-2 loss: 0.925362  [   96/  124]
train() client id: f_00002-11-0 loss: 1.060007  [   32/  124]
train() client id: f_00002-11-1 loss: 0.978777  [   64/  124]
train() client id: f_00002-11-2 loss: 0.866438  [   96/  124]
train() client id: f_00003-0-0 loss: 0.795829  [   32/   43]
train() client id: f_00003-1-0 loss: 0.882702  [   32/   43]
train() client id: f_00003-2-0 loss: 0.923124  [   32/   43]
train() client id: f_00003-3-0 loss: 0.828516  [   32/   43]
train() client id: f_00003-4-0 loss: 0.837873  [   32/   43]
train() client id: f_00003-5-0 loss: 0.796048  [   32/   43]
train() client id: f_00003-6-0 loss: 0.946576  [   32/   43]
train() client id: f_00003-7-0 loss: 0.867114  [   32/   43]
train() client id: f_00003-8-0 loss: 0.852083  [   32/   43]
train() client id: f_00003-9-0 loss: 0.914422  [   32/   43]
train() client id: f_00003-10-0 loss: 0.795345  [   32/   43]
train() client id: f_00003-11-0 loss: 0.923677  [   32/   43]
train() client id: f_00004-0-0 loss: 0.764413  [   32/  306]
train() client id: f_00004-0-1 loss: 0.766000  [   64/  306]
train() client id: f_00004-0-2 loss: 0.792175  [   96/  306]
train() client id: f_00004-0-3 loss: 0.765836  [  128/  306]
train() client id: f_00004-0-4 loss: 0.706478  [  160/  306]
train() client id: f_00004-0-5 loss: 0.753998  [  192/  306]
train() client id: f_00004-0-6 loss: 0.849973  [  224/  306]
train() client id: f_00004-0-7 loss: 0.779085  [  256/  306]
train() client id: f_00004-0-8 loss: 0.735113  [  288/  306]
train() client id: f_00004-1-0 loss: 0.846714  [   32/  306]
train() client id: f_00004-1-1 loss: 0.757222  [   64/  306]
train() client id: f_00004-1-2 loss: 0.714320  [   96/  306]
train() client id: f_00004-1-3 loss: 0.765167  [  128/  306]
train() client id: f_00004-1-4 loss: 0.802691  [  160/  306]
train() client id: f_00004-1-5 loss: 0.761532  [  192/  306]
train() client id: f_00004-1-6 loss: 0.825992  [  224/  306]
train() client id: f_00004-1-7 loss: 0.712202  [  256/  306]
train() client id: f_00004-1-8 loss: 0.787500  [  288/  306]
train() client id: f_00004-2-0 loss: 0.796950  [   32/  306]
train() client id: f_00004-2-1 loss: 0.808131  [   64/  306]
train() client id: f_00004-2-2 loss: 0.894318  [   96/  306]
train() client id: f_00004-2-3 loss: 0.743665  [  128/  306]
train() client id: f_00004-2-4 loss: 0.671150  [  160/  306]
train() client id: f_00004-2-5 loss: 0.791855  [  192/  306]
train() client id: f_00004-2-6 loss: 0.715135  [  224/  306]
train() client id: f_00004-2-7 loss: 0.858176  [  256/  306]
train() client id: f_00004-2-8 loss: 0.789662  [  288/  306]
train() client id: f_00004-3-0 loss: 0.796191  [   32/  306]
train() client id: f_00004-3-1 loss: 0.809331  [   64/  306]
train() client id: f_00004-3-2 loss: 0.732018  [   96/  306]
train() client id: f_00004-3-3 loss: 0.758614  [  128/  306]
train() client id: f_00004-3-4 loss: 0.890009  [  160/  306]
train() client id: f_00004-3-5 loss: 0.792163  [  192/  306]
train() client id: f_00004-3-6 loss: 0.619638  [  224/  306]
train() client id: f_00004-3-7 loss: 0.902165  [  256/  306]
train() client id: f_00004-3-8 loss: 0.790722  [  288/  306]
train() client id: f_00004-4-0 loss: 0.819994  [   32/  306]
train() client id: f_00004-4-1 loss: 0.799470  [   64/  306]
train() client id: f_00004-4-2 loss: 0.745071  [   96/  306]
train() client id: f_00004-4-3 loss: 0.716765  [  128/  306]
train() client id: f_00004-4-4 loss: 0.824225  [  160/  306]
train() client id: f_00004-4-5 loss: 0.854776  [  192/  306]
train() client id: f_00004-4-6 loss: 0.772256  [  224/  306]
train() client id: f_00004-4-7 loss: 0.822006  [  256/  306]
train() client id: f_00004-4-8 loss: 0.805878  [  288/  306]
train() client id: f_00004-5-0 loss: 0.770234  [   32/  306]
train() client id: f_00004-5-1 loss: 0.831994  [   64/  306]
train() client id: f_00004-5-2 loss: 0.862922  [   96/  306]
train() client id: f_00004-5-3 loss: 0.810686  [  128/  306]
train() client id: f_00004-5-4 loss: 0.693978  [  160/  306]
train() client id: f_00004-5-5 loss: 0.778851  [  192/  306]
train() client id: f_00004-5-6 loss: 0.772115  [  224/  306]
train() client id: f_00004-5-7 loss: 0.716944  [  256/  306]
train() client id: f_00004-5-8 loss: 0.791807  [  288/  306]
train() client id: f_00004-6-0 loss: 0.779201  [   32/  306]
train() client id: f_00004-6-1 loss: 0.762198  [   64/  306]
train() client id: f_00004-6-2 loss: 0.798340  [   96/  306]
train() client id: f_00004-6-3 loss: 0.797254  [  128/  306]
train() client id: f_00004-6-4 loss: 0.900004  [  160/  306]
train() client id: f_00004-6-5 loss: 0.762969  [  192/  306]
train() client id: f_00004-6-6 loss: 0.686941  [  224/  306]
train() client id: f_00004-6-7 loss: 0.882498  [  256/  306]
train() client id: f_00004-6-8 loss: 0.764390  [  288/  306]
train() client id: f_00004-7-0 loss: 0.866006  [   32/  306]
train() client id: f_00004-7-1 loss: 0.799479  [   64/  306]
train() client id: f_00004-7-2 loss: 0.834987  [   96/  306]
train() client id: f_00004-7-3 loss: 0.688404  [  128/  306]
train() client id: f_00004-7-4 loss: 0.768690  [  160/  306]
train() client id: f_00004-7-5 loss: 0.764058  [  192/  306]
train() client id: f_00004-7-6 loss: 0.775107  [  224/  306]
train() client id: f_00004-7-7 loss: 0.788684  [  256/  306]
train() client id: f_00004-7-8 loss: 0.834905  [  288/  306]
train() client id: f_00004-8-0 loss: 0.766837  [   32/  306]
train() client id: f_00004-8-1 loss: 0.756560  [   64/  306]
train() client id: f_00004-8-2 loss: 0.767147  [   96/  306]
train() client id: f_00004-8-3 loss: 0.677186  [  128/  306]
train() client id: f_00004-8-4 loss: 0.787761  [  160/  306]
train() client id: f_00004-8-5 loss: 0.832943  [  192/  306]
train() client id: f_00004-8-6 loss: 0.961570  [  224/  306]
train() client id: f_00004-8-7 loss: 0.807335  [  256/  306]
train() client id: f_00004-8-8 loss: 0.788533  [  288/  306]
train() client id: f_00004-9-0 loss: 0.791306  [   32/  306]
train() client id: f_00004-9-1 loss: 0.799419  [   64/  306]
train() client id: f_00004-9-2 loss: 0.723757  [   96/  306]
train() client id: f_00004-9-3 loss: 0.745962  [  128/  306]
train() client id: f_00004-9-4 loss: 0.707657  [  160/  306]
train() client id: f_00004-9-5 loss: 0.782992  [  192/  306]
train() client id: f_00004-9-6 loss: 0.813545  [  224/  306]
train() client id: f_00004-9-7 loss: 0.911804  [  256/  306]
train() client id: f_00004-9-8 loss: 0.750219  [  288/  306]
train() client id: f_00004-10-0 loss: 0.779182  [   32/  306]
train() client id: f_00004-10-1 loss: 0.865272  [   64/  306]
train() client id: f_00004-10-2 loss: 0.836334  [   96/  306]
train() client id: f_00004-10-3 loss: 0.757794  [  128/  306]
train() client id: f_00004-10-4 loss: 0.793970  [  160/  306]
train() client id: f_00004-10-5 loss: 0.849087  [  192/  306]
train() client id: f_00004-10-6 loss: 0.836590  [  224/  306]
train() client id: f_00004-10-7 loss: 0.800069  [  256/  306]
train() client id: f_00004-10-8 loss: 0.668107  [  288/  306]
train() client id: f_00004-11-0 loss: 0.703946  [   32/  306]
train() client id: f_00004-11-1 loss: 0.854625  [   64/  306]
train() client id: f_00004-11-2 loss: 0.814460  [   96/  306]
train() client id: f_00004-11-3 loss: 0.757858  [  128/  306]
train() client id: f_00004-11-4 loss: 0.793825  [  160/  306]
train() client id: f_00004-11-5 loss: 0.827716  [  192/  306]
train() client id: f_00004-11-6 loss: 0.774663  [  224/  306]
train() client id: f_00004-11-7 loss: 0.741792  [  256/  306]
train() client id: f_00004-11-8 loss: 0.825526  [  288/  306]
train() client id: f_00005-0-0 loss: 0.781672  [   32/  146]
train() client id: f_00005-0-1 loss: 0.650118  [   64/  146]
train() client id: f_00005-0-2 loss: 0.797116  [   96/  146]
train() client id: f_00005-0-3 loss: 0.928249  [  128/  146]
train() client id: f_00005-1-0 loss: 0.873449  [   32/  146]
train() client id: f_00005-1-1 loss: 0.693810  [   64/  146]
train() client id: f_00005-1-2 loss: 0.553160  [   96/  146]
train() client id: f_00005-1-3 loss: 0.924577  [  128/  146]
train() client id: f_00005-2-0 loss: 0.835898  [   32/  146]
train() client id: f_00005-2-1 loss: 0.613922  [   64/  146]
train() client id: f_00005-2-2 loss: 0.754986  [   96/  146]
train() client id: f_00005-2-3 loss: 0.830395  [  128/  146]
train() client id: f_00005-3-0 loss: 0.575785  [   32/  146]
train() client id: f_00005-3-1 loss: 0.912789  [   64/  146]
train() client id: f_00005-3-2 loss: 0.821332  [   96/  146]
train() client id: f_00005-3-3 loss: 0.655797  [  128/  146]
train() client id: f_00005-4-0 loss: 0.817920  [   32/  146]
train() client id: f_00005-4-1 loss: 0.782455  [   64/  146]
train() client id: f_00005-4-2 loss: 0.608882  [   96/  146]
train() client id: f_00005-4-3 loss: 0.797947  [  128/  146]
train() client id: f_00005-5-0 loss: 0.761668  [   32/  146]
train() client id: f_00005-5-1 loss: 0.824034  [   64/  146]
train() client id: f_00005-5-2 loss: 0.680773  [   96/  146]
train() client id: f_00005-5-3 loss: 0.672547  [  128/  146]
train() client id: f_00005-6-0 loss: 0.704486  [   32/  146]
train() client id: f_00005-6-1 loss: 0.791397  [   64/  146]
train() client id: f_00005-6-2 loss: 0.779792  [   96/  146]
train() client id: f_00005-6-3 loss: 0.648491  [  128/  146]
train() client id: f_00005-7-0 loss: 0.801926  [   32/  146]
train() client id: f_00005-7-1 loss: 0.794029  [   64/  146]
train() client id: f_00005-7-2 loss: 0.632958  [   96/  146]
train() client id: f_00005-7-3 loss: 0.538387  [  128/  146]
train() client id: f_00005-8-0 loss: 0.653208  [   32/  146]
train() client id: f_00005-8-1 loss: 0.949747  [   64/  146]
train() client id: f_00005-8-2 loss: 0.556973  [   96/  146]
train() client id: f_00005-8-3 loss: 0.610289  [  128/  146]
train() client id: f_00005-9-0 loss: 0.745015  [   32/  146]
train() client id: f_00005-9-1 loss: 0.801440  [   64/  146]
train() client id: f_00005-9-2 loss: 0.851606  [   96/  146]
train() client id: f_00005-9-3 loss: 0.596832  [  128/  146]
train() client id: f_00005-10-0 loss: 0.698390  [   32/  146]
train() client id: f_00005-10-1 loss: 0.629296  [   64/  146]
train() client id: f_00005-10-2 loss: 0.816083  [   96/  146]
train() client id: f_00005-10-3 loss: 0.633662  [  128/  146]
train() client id: f_00005-11-0 loss: 0.760337  [   32/  146]
train() client id: f_00005-11-1 loss: 0.698260  [   64/  146]
train() client id: f_00005-11-2 loss: 0.864961  [   96/  146]
train() client id: f_00005-11-3 loss: 0.638763  [  128/  146]
train() client id: f_00006-0-0 loss: 0.609578  [   32/   54]
train() client id: f_00006-1-0 loss: 0.595317  [   32/   54]
train() client id: f_00006-2-0 loss: 0.691910  [   32/   54]
train() client id: f_00006-3-0 loss: 0.693368  [   32/   54]
train() client id: f_00006-4-0 loss: 0.684768  [   32/   54]
train() client id: f_00006-5-0 loss: 0.685127  [   32/   54]
train() client id: f_00006-6-0 loss: 0.596326  [   32/   54]
train() client id: f_00006-7-0 loss: 0.611097  [   32/   54]
train() client id: f_00006-8-0 loss: 0.666133  [   32/   54]
train() client id: f_00006-9-0 loss: 0.652415  [   32/   54]
train() client id: f_00006-10-0 loss: 0.686298  [   32/   54]
train() client id: f_00006-11-0 loss: 0.620985  [   32/   54]
train() client id: f_00007-0-0 loss: 0.701753  [   32/  179]
train() client id: f_00007-0-1 loss: 0.742401  [   64/  179]
train() client id: f_00007-0-2 loss: 0.850912  [   96/  179]
train() client id: f_00007-0-3 loss: 0.620438  [  128/  179]
train() client id: f_00007-0-4 loss: 0.767646  [  160/  179]
train() client id: f_00007-1-0 loss: 0.771957  [   32/  179]
train() client id: f_00007-1-1 loss: 0.684677  [   64/  179]
train() client id: f_00007-1-2 loss: 0.796122  [   96/  179]
train() client id: f_00007-1-3 loss: 0.661710  [  128/  179]
train() client id: f_00007-1-4 loss: 0.721886  [  160/  179]
train() client id: f_00007-2-0 loss: 0.680869  [   32/  179]
train() client id: f_00007-2-1 loss: 0.646810  [   64/  179]
train() client id: f_00007-2-2 loss: 0.668586  [   96/  179]
train() client id: f_00007-2-3 loss: 0.766374  [  128/  179]
train() client id: f_00007-2-4 loss: 0.748546  [  160/  179]
train() client id: f_00007-3-0 loss: 0.667976  [   32/  179]
train() client id: f_00007-3-1 loss: 0.878832  [   64/  179]
train() client id: f_00007-3-2 loss: 0.574267  [   96/  179]
train() client id: f_00007-3-3 loss: 0.630851  [  128/  179]
train() client id: f_00007-3-4 loss: 0.750041  [  160/  179]
train() client id: f_00007-4-0 loss: 0.581573  [   32/  179]
train() client id: f_00007-4-1 loss: 0.728115  [   64/  179]
train() client id: f_00007-4-2 loss: 0.873290  [   96/  179]
train() client id: f_00007-4-3 loss: 0.591207  [  128/  179]
train() client id: f_00007-4-4 loss: 0.641185  [  160/  179]
train() client id: f_00007-5-0 loss: 0.769939  [   32/  179]
train() client id: f_00007-5-1 loss: 0.554337  [   64/  179]
train() client id: f_00007-5-2 loss: 0.669209  [   96/  179]
train() client id: f_00007-5-3 loss: 0.655215  [  128/  179]
train() client id: f_00007-5-4 loss: 0.736345  [  160/  179]
train() client id: f_00007-6-0 loss: 0.852973  [   32/  179]
train() client id: f_00007-6-1 loss: 0.557159  [   64/  179]
train() client id: f_00007-6-2 loss: 0.721187  [   96/  179]
train() client id: f_00007-6-3 loss: 0.692316  [  128/  179]
train() client id: f_00007-6-4 loss: 0.561159  [  160/  179]
train() client id: f_00007-7-0 loss: 0.710649  [   32/  179]
train() client id: f_00007-7-1 loss: 0.719434  [   64/  179]
train() client id: f_00007-7-2 loss: 0.700025  [   96/  179]
train() client id: f_00007-7-3 loss: 0.606677  [  128/  179]
train() client id: f_00007-7-4 loss: 0.556380  [  160/  179]
train() client id: f_00007-8-0 loss: 0.741989  [   32/  179]
train() client id: f_00007-8-1 loss: 0.551423  [   64/  179]
train() client id: f_00007-8-2 loss: 0.535520  [   96/  179]
train() client id: f_00007-8-3 loss: 0.629340  [  128/  179]
train() client id: f_00007-8-4 loss: 0.915159  [  160/  179]
train() client id: f_00007-9-0 loss: 0.865127  [   32/  179]
train() client id: f_00007-9-1 loss: 0.598494  [   64/  179]
train() client id: f_00007-9-2 loss: 0.635572  [   96/  179]
train() client id: f_00007-9-3 loss: 0.649649  [  128/  179]
train() client id: f_00007-9-4 loss: 0.547061  [  160/  179]
train() client id: f_00007-10-0 loss: 0.762658  [   32/  179]
train() client id: f_00007-10-1 loss: 0.539573  [   64/  179]
train() client id: f_00007-10-2 loss: 0.556485  [   96/  179]
train() client id: f_00007-10-3 loss: 0.708725  [  128/  179]
train() client id: f_00007-10-4 loss: 0.704449  [  160/  179]
train() client id: f_00007-11-0 loss: 0.650477  [   32/  179]
train() client id: f_00007-11-1 loss: 0.531826  [   64/  179]
train() client id: f_00007-11-2 loss: 0.701764  [   96/  179]
train() client id: f_00007-11-3 loss: 0.534622  [  128/  179]
train() client id: f_00007-11-4 loss: 0.670485  [  160/  179]
train() client id: f_00008-0-0 loss: 0.865012  [   32/  130]
train() client id: f_00008-0-1 loss: 0.896493  [   64/  130]
train() client id: f_00008-0-2 loss: 0.797327  [   96/  130]
train() client id: f_00008-0-3 loss: 0.790530  [  128/  130]
train() client id: f_00008-1-0 loss: 0.836052  [   32/  130]
train() client id: f_00008-1-1 loss: 0.890802  [   64/  130]
train() client id: f_00008-1-2 loss: 0.862123  [   96/  130]
train() client id: f_00008-1-3 loss: 0.749142  [  128/  130]
train() client id: f_00008-2-0 loss: 0.735012  [   32/  130]
train() client id: f_00008-2-1 loss: 0.879599  [   64/  130]
train() client id: f_00008-2-2 loss: 0.937770  [   96/  130]
train() client id: f_00008-2-3 loss: 0.794613  [  128/  130]
train() client id: f_00008-3-0 loss: 0.958151  [   32/  130]
train() client id: f_00008-3-1 loss: 0.686500  [   64/  130]
train() client id: f_00008-3-2 loss: 0.881674  [   96/  130]
train() client id: f_00008-3-3 loss: 0.819124  [  128/  130]
train() client id: f_00008-4-0 loss: 0.798549  [   32/  130]
train() client id: f_00008-4-1 loss: 0.763423  [   64/  130]
train() client id: f_00008-4-2 loss: 0.755307  [   96/  130]
train() client id: f_00008-4-3 loss: 0.976098  [  128/  130]
train() client id: f_00008-5-0 loss: 0.903489  [   32/  130]
train() client id: f_00008-5-1 loss: 0.742243  [   64/  130]
train() client id: f_00008-5-2 loss: 0.811635  [   96/  130]
train() client id: f_00008-5-3 loss: 0.858057  [  128/  130]
train() client id: f_00008-6-0 loss: 0.894195  [   32/  130]
train() client id: f_00008-6-1 loss: 0.905608  [   64/  130]
train() client id: f_00008-6-2 loss: 0.757802  [   96/  130]
train() client id: f_00008-6-3 loss: 0.784891  [  128/  130]
train() client id: f_00008-7-0 loss: 0.804697  [   32/  130]
train() client id: f_00008-7-1 loss: 0.815143  [   64/  130]
train() client id: f_00008-7-2 loss: 0.871718  [   96/  130]
train() client id: f_00008-7-3 loss: 0.846632  [  128/  130]
train() client id: f_00008-8-0 loss: 0.738038  [   32/  130]
train() client id: f_00008-8-1 loss: 0.921948  [   64/  130]
train() client id: f_00008-8-2 loss: 0.800908  [   96/  130]
train() client id: f_00008-8-3 loss: 0.868198  [  128/  130]
train() client id: f_00008-9-0 loss: 0.881420  [   32/  130]
train() client id: f_00008-9-1 loss: 0.775048  [   64/  130]
train() client id: f_00008-9-2 loss: 0.863091  [   96/  130]
train() client id: f_00008-9-3 loss: 0.810601  [  128/  130]
train() client id: f_00008-10-0 loss: 0.846684  [   32/  130]
train() client id: f_00008-10-1 loss: 0.790016  [   64/  130]
train() client id: f_00008-10-2 loss: 0.724335  [   96/  130]
train() client id: f_00008-10-3 loss: 0.963706  [  128/  130]
train() client id: f_00008-11-0 loss: 0.847624  [   32/  130]
train() client id: f_00008-11-1 loss: 0.907711  [   64/  130]
train() client id: f_00008-11-2 loss: 0.775704  [   96/  130]
train() client id: f_00008-11-3 loss: 0.785101  [  128/  130]
train() client id: f_00009-0-0 loss: 1.013497  [   32/  118]
train() client id: f_00009-0-1 loss: 1.297719  [   64/  118]
train() client id: f_00009-0-2 loss: 1.089788  [   96/  118]
train() client id: f_00009-1-0 loss: 1.066468  [   32/  118]
train() client id: f_00009-1-1 loss: 1.134441  [   64/  118]
train() client id: f_00009-1-2 loss: 1.022178  [   96/  118]
train() client id: f_00009-2-0 loss: 1.025662  [   32/  118]
train() client id: f_00009-2-1 loss: 1.094950  [   64/  118]
train() client id: f_00009-2-2 loss: 1.044094  [   96/  118]
train() client id: f_00009-3-0 loss: 0.880261  [   32/  118]
train() client id: f_00009-3-1 loss: 1.040342  [   64/  118]
train() client id: f_00009-3-2 loss: 1.090282  [   96/  118]
train() client id: f_00009-4-0 loss: 1.018252  [   32/  118]
train() client id: f_00009-4-1 loss: 0.942341  [   64/  118]
train() client id: f_00009-4-2 loss: 0.932322  [   96/  118]
train() client id: f_00009-5-0 loss: 0.921119  [   32/  118]
train() client id: f_00009-5-1 loss: 1.039336  [   64/  118]
train() client id: f_00009-5-2 loss: 0.905263  [   96/  118]
train() client id: f_00009-6-0 loss: 0.827772  [   32/  118]
train() client id: f_00009-6-1 loss: 1.043032  [   64/  118]
train() client id: f_00009-6-2 loss: 0.889752  [   96/  118]
train() client id: f_00009-7-0 loss: 0.939989  [   32/  118]
train() client id: f_00009-7-1 loss: 0.977642  [   64/  118]
train() client id: f_00009-7-2 loss: 0.859284  [   96/  118]
train() client id: f_00009-8-0 loss: 0.893870  [   32/  118]
train() client id: f_00009-8-1 loss: 0.872090  [   64/  118]
train() client id: f_00009-8-2 loss: 0.956959  [   96/  118]
train() client id: f_00009-9-0 loss: 0.827593  [   32/  118]
train() client id: f_00009-9-1 loss: 0.860756  [   64/  118]
train() client id: f_00009-9-2 loss: 0.940283  [   96/  118]
train() client id: f_00009-10-0 loss: 0.895719  [   32/  118]
train() client id: f_00009-10-1 loss: 0.789046  [   64/  118]
train() client id: f_00009-10-2 loss: 0.825034  [   96/  118]
train() client id: f_00009-11-0 loss: 0.819996  [   32/  118]
train() client id: f_00009-11-1 loss: 0.951789  [   64/  118]
train() client id: f_00009-11-2 loss: 0.898227  [   96/  118]
At round 12 accuracy: 0.6312997347480106
At round 12 training accuracy: 0.5734406438631791
At round 12 training loss: 0.8673828187754442
update_location
xs = [ -3.9056584    4.20031788  80.00902392  18.81129433   0.97929623
   3.95640986 -42.44319194 -21.32485185  64.66397685  -7.06087855]
ys = [ 72.5879595   55.55583871   1.32061395 -42.45517586  34.35018685
  17.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [123.62955161 114.47311424 128.07493092 110.25564272 105.73974824
 101.65135093 108.66607145 102.25177535 120.37483074 100.32879877]
dists_bs = [199.40285797 215.35937777 308.53546217 291.28949744 225.28000941
 238.18645603 221.6347049  232.27980029 286.74612406 239.67486674]
uav_gains = [5.88385192e-11 7.13231094e-11 5.38624893e-11 7.83414303e-11
 8.69762693e-11 9.59875921e-11 8.12381170e-11 9.45846506e-11
 6.28982282e-11 9.91823280e-11]
bs_gains = [4.01833579e-11 3.23918296e-11 1.18369507e-11 1.39054348e-11
 2.85542429e-11 2.44301599e-11 2.98887905e-11 2.62096995e-11
 1.45311815e-11 2.40077301e-11]
Round 13
-------------------------------
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.27548856 19.34614452  9.13389385  3.26646163 22.31552022 10.75645448
  4.0605729  13.0924169   9.62823145  8.73319217]
obj_prev = 109.60837667978073
eta_min = 9.621283683779976e-11	eta_max = 0.9207981058157783
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 25.487961404153893	eta = 0.9090909090909091
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 44.16384428532155	eta = 0.5246570894979226
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 35.24177796016038	eta = 0.6574831164866354
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.64173104871985	eta = 0.6887539160877387
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.56259776728117	eta = 0.6903778475206297
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.56239040785321	eta = 0.6903821129008305
eta = 0.6903821129008305
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [0.0305736  0.06430162 0.03008831 0.01043385 0.07425017 0.03542655
 0.01310297 0.04343393 0.03154419 0.02863242]
ene_total = [2.87721447 5.53384282 2.84956312 1.30490774 6.3139176  3.36162512
 1.50604088 3.81675253 3.15491369 2.84361242]
ti_comp = [0.31327504 0.30392366 0.31202998 0.31702896 0.30162658 0.29859963
 0.31747675 0.31928923 0.2867473  0.29824756]
ti_coms = [0.06967721 0.0790286  0.07092227 0.06592329 0.08132567 0.08435263
 0.0654755  0.06366303 0.09620495 0.08470469]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.81998670e-05 1.79894040e-04 1.74856014e-05 7.06343005e-07
 2.81211599e-04 3.11665129e-05 1.39497231e-06 5.02341124e-05
 2.38583025e-05 1.64930279e-05]
ene_total = [0.53540984 0.61947247 0.54489739 0.50529856 0.64484267 0.64887784
 0.50191943 0.49177151 0.73915545 0.65045154]
optimize_network_iter = 0 obj = 5.882096701797337
eta = 0.6903821129008305
freqs = [4.87967396e+07 1.05785811e+08 4.82138059e+07 1.64556699e+07
 1.23082943e+08 5.93211620e+07 2.06361124e+07 6.80165848e+07
 5.50034670e+07 4.80010972e+07]
eta_min = 0.6903821129008529	eta_max = 0.6903821129008285
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 0.04372443773870311	eta = 0.909090909090909
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 19.912896282792712	eta = 0.0019961681258650003
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.05768598057864	eta = 0.019317568000433452
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.002030029818999	eta = 0.019854591720065342
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.0020149221690455	eta = 0.019854741547231138
eta = 0.019854741547231138
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.92521000e-04 1.90294690e-03 1.84965388e-04 7.47180525e-06
 2.97469966e-03 3.29684180e-04 1.47562323e-05 5.31384189e-04
 2.52376804e-04 1.74465793e-04]
ene_total = [0.17335085 0.23740048 0.17618224 0.15978245 0.26890909 0.21220106
 0.15887471 0.16699434 0.23902414 0.20929556]
ti_comp = [0.31327504 0.30392366 0.31202998 0.31702896 0.30162658 0.29859963
 0.31747675 0.31928923 0.2867473  0.29824756]
ti_coms = [0.06967721 0.0790286  0.07092227 0.06592329 0.08132567 0.08435263
 0.0654755  0.06366303 0.09620495 0.08470469]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.81998670e-05 1.79894040e-04 1.74856014e-05 7.06343005e-07
 2.81211599e-04 3.11665129e-05 1.39497231e-06 5.02341124e-05
 2.38583025e-05 1.64930279e-05]
ene_total = [0.53540984 0.61947247 0.54489739 0.50529856 0.64484267 0.64887784
 0.50191943 0.49177151 0.73915545 0.65045154]
optimize_network_iter = 1 obj = 5.882096701797758
eta = 0.6903821129008529
freqs = [4.87967396e+07 1.05785811e+08 4.82138059e+07 1.64556699e+07
 1.23082943e+08 5.93211620e+07 2.06361124e+07 6.80165848e+07
 5.50034670e+07 4.80010972e+07]
Done!
At round 13 eta: 0.6903821129008529
At round 13 local rounds: 12.132381383768099
At round 13 global rounds: 76.641266006878
At round 13 a_n: 23.386960998683932
gradient difference: 0.49451571702957153
train() client id: f_00000-0-0 loss: 0.991246  [   32/  126]
train() client id: f_00000-0-1 loss: 1.172359  [   64/  126]
train() client id: f_00000-0-2 loss: 1.216543  [   96/  126]
train() client id: f_00000-1-0 loss: 1.178327  [   32/  126]
train() client id: f_00000-1-1 loss: 1.026904  [   64/  126]
train() client id: f_00000-1-2 loss: 1.037534  [   96/  126]
train() client id: f_00000-2-0 loss: 1.072795  [   32/  126]
train() client id: f_00000-2-1 loss: 1.010560  [   64/  126]
train() client id: f_00000-2-2 loss: 0.985664  [   96/  126]
train() client id: f_00000-3-0 loss: 1.055140  [   32/  126]
train() client id: f_00000-3-1 loss: 0.933178  [   64/  126]
train() client id: f_00000-3-2 loss: 0.961863  [   96/  126]
train() client id: f_00000-4-0 loss: 0.921139  [   32/  126]
train() client id: f_00000-4-1 loss: 0.937395  [   64/  126]
train() client id: f_00000-4-2 loss: 0.966685  [   96/  126]
train() client id: f_00000-5-0 loss: 0.940596  [   32/  126]
train() client id: f_00000-5-1 loss: 0.895338  [   64/  126]
train() client id: f_00000-5-2 loss: 1.025186  [   96/  126]
train() client id: f_00000-6-0 loss: 0.959503  [   32/  126]
train() client id: f_00000-6-1 loss: 0.889083  [   64/  126]
train() client id: f_00000-6-2 loss: 0.917298  [   96/  126]
train() client id: f_00000-7-0 loss: 0.925151  [   32/  126]
train() client id: f_00000-7-1 loss: 0.891021  [   64/  126]
train() client id: f_00000-7-2 loss: 0.999235  [   96/  126]
train() client id: f_00000-8-0 loss: 0.981440  [   32/  126]
train() client id: f_00000-8-1 loss: 0.892417  [   64/  126]
train() client id: f_00000-8-2 loss: 0.950019  [   96/  126]
train() client id: f_00000-9-0 loss: 0.912170  [   32/  126]
train() client id: f_00000-9-1 loss: 0.933393  [   64/  126]
train() client id: f_00000-9-2 loss: 1.000064  [   96/  126]
train() client id: f_00000-10-0 loss: 1.022802  [   32/  126]
train() client id: f_00000-10-1 loss: 0.948209  [   64/  126]
train() client id: f_00000-10-2 loss: 0.941022  [   96/  126]
train() client id: f_00000-11-0 loss: 0.993787  [   32/  126]
train() client id: f_00000-11-1 loss: 0.893802  [   64/  126]
train() client id: f_00000-11-2 loss: 0.999109  [   96/  126]
train() client id: f_00001-0-0 loss: 0.530404  [   32/  265]
train() client id: f_00001-0-1 loss: 0.452247  [   64/  265]
train() client id: f_00001-0-2 loss: 0.331102  [   96/  265]
train() client id: f_00001-0-3 loss: 0.378143  [  128/  265]
train() client id: f_00001-0-4 loss: 0.363422  [  160/  265]
train() client id: f_00001-0-5 loss: 0.397136  [  192/  265]
train() client id: f_00001-0-6 loss: 0.431241  [  224/  265]
train() client id: f_00001-0-7 loss: 0.325623  [  256/  265]
train() client id: f_00001-1-0 loss: 0.299509  [   32/  265]
train() client id: f_00001-1-1 loss: 0.532230  [   64/  265]
train() client id: f_00001-1-2 loss: 0.420624  [   96/  265]
train() client id: f_00001-1-3 loss: 0.414619  [  128/  265]
train() client id: f_00001-1-4 loss: 0.334611  [  160/  265]
train() client id: f_00001-1-5 loss: 0.390864  [  192/  265]
train() client id: f_00001-1-6 loss: 0.351343  [  224/  265]
train() client id: f_00001-1-7 loss: 0.307056  [  256/  265]
train() client id: f_00001-2-0 loss: 0.289313  [   32/  265]
train() client id: f_00001-2-1 loss: 0.471204  [   64/  265]
train() client id: f_00001-2-2 loss: 0.417955  [   96/  265]
train() client id: f_00001-2-3 loss: 0.366924  [  128/  265]
train() client id: f_00001-2-4 loss: 0.390839  [  160/  265]
train() client id: f_00001-2-5 loss: 0.376588  [  192/  265]
train() client id: f_00001-2-6 loss: 0.346295  [  224/  265]
train() client id: f_00001-2-7 loss: 0.299419  [  256/  265]
train() client id: f_00001-3-0 loss: 0.335174  [   32/  265]
train() client id: f_00001-3-1 loss: 0.516753  [   64/  265]
train() client id: f_00001-3-2 loss: 0.271243  [   96/  265]
train() client id: f_00001-3-3 loss: 0.410948  [  128/  265]
train() client id: f_00001-3-4 loss: 0.306004  [  160/  265]
train() client id: f_00001-3-5 loss: 0.283684  [  192/  265]
train() client id: f_00001-3-6 loss: 0.290637  [  224/  265]
train() client id: f_00001-3-7 loss: 0.543638  [  256/  265]
train() client id: f_00001-4-0 loss: 0.443576  [   32/  265]
train() client id: f_00001-4-1 loss: 0.339821  [   64/  265]
train() client id: f_00001-4-2 loss: 0.351768  [   96/  265]
train() client id: f_00001-4-3 loss: 0.252584  [  128/  265]
train() client id: f_00001-4-4 loss: 0.316145  [  160/  265]
train() client id: f_00001-4-5 loss: 0.310008  [  192/  265]
train() client id: f_00001-4-6 loss: 0.391114  [  224/  265]
train() client id: f_00001-4-7 loss: 0.432463  [  256/  265]
train() client id: f_00001-5-0 loss: 0.324794  [   32/  265]
train() client id: f_00001-5-1 loss: 0.277109  [   64/  265]
train() client id: f_00001-5-2 loss: 0.323001  [   96/  265]
train() client id: f_00001-5-3 loss: 0.465305  [  128/  265]
train() client id: f_00001-5-4 loss: 0.414516  [  160/  265]
train() client id: f_00001-5-5 loss: 0.311436  [  192/  265]
train() client id: f_00001-5-6 loss: 0.372158  [  224/  265]
train() client id: f_00001-5-7 loss: 0.353435  [  256/  265]
train() client id: f_00001-6-0 loss: 0.392231  [   32/  265]
train() client id: f_00001-6-1 loss: 0.295862  [   64/  265]
train() client id: f_00001-6-2 loss: 0.410722  [   96/  265]
train() client id: f_00001-6-3 loss: 0.316180  [  128/  265]
train() client id: f_00001-6-4 loss: 0.268902  [  160/  265]
train() client id: f_00001-6-5 loss: 0.372323  [  192/  265]
train() client id: f_00001-6-6 loss: 0.333319  [  224/  265]
train() client id: f_00001-6-7 loss: 0.354567  [  256/  265]
train() client id: f_00001-7-0 loss: 0.322509  [   32/  265]
train() client id: f_00001-7-1 loss: 0.411235  [   64/  265]
train() client id: f_00001-7-2 loss: 0.310467  [   96/  265]
train() client id: f_00001-7-3 loss: 0.312850  [  128/  265]
train() client id: f_00001-7-4 loss: 0.307046  [  160/  265]
train() client id: f_00001-7-5 loss: 0.390575  [  192/  265]
train() client id: f_00001-7-6 loss: 0.472819  [  224/  265]
train() client id: f_00001-7-7 loss: 0.275922  [  256/  265]
train() client id: f_00001-8-0 loss: 0.325677  [   32/  265]
train() client id: f_00001-8-1 loss: 0.383470  [   64/  265]
train() client id: f_00001-8-2 loss: 0.315116  [   96/  265]
train() client id: f_00001-8-3 loss: 0.452185  [  128/  265]
train() client id: f_00001-8-4 loss: 0.328938  [  160/  265]
train() client id: f_00001-8-5 loss: 0.289189  [  192/  265]
train() client id: f_00001-8-6 loss: 0.325148  [  224/  265]
train() client id: f_00001-8-7 loss: 0.360286  [  256/  265]
train() client id: f_00001-9-0 loss: 0.350807  [   32/  265]
train() client id: f_00001-9-1 loss: 0.326083  [   64/  265]
train() client id: f_00001-9-2 loss: 0.324699  [   96/  265]
train() client id: f_00001-9-3 loss: 0.432585  [  128/  265]
train() client id: f_00001-9-4 loss: 0.337958  [  160/  265]
train() client id: f_00001-9-5 loss: 0.359491  [  192/  265]
train() client id: f_00001-9-6 loss: 0.344629  [  224/  265]
train() client id: f_00001-9-7 loss: 0.242085  [  256/  265]
train() client id: f_00001-10-0 loss: 0.303324  [   32/  265]
train() client id: f_00001-10-1 loss: 0.394456  [   64/  265]
train() client id: f_00001-10-2 loss: 0.333108  [   96/  265]
train() client id: f_00001-10-3 loss: 0.361594  [  128/  265]
train() client id: f_00001-10-4 loss: 0.244376  [  160/  265]
train() client id: f_00001-10-5 loss: 0.410594  [  192/  265]
train() client id: f_00001-10-6 loss: 0.310427  [  224/  265]
train() client id: f_00001-10-7 loss: 0.399313  [  256/  265]
train() client id: f_00001-11-0 loss: 0.308831  [   32/  265]
train() client id: f_00001-11-1 loss: 0.233749  [   64/  265]
train() client id: f_00001-11-2 loss: 0.273711  [   96/  265]
train() client id: f_00001-11-3 loss: 0.379944  [  128/  265]
train() client id: f_00001-11-4 loss: 0.363874  [  160/  265]
train() client id: f_00001-11-5 loss: 0.374335  [  192/  265]
train() client id: f_00001-11-6 loss: 0.445005  [  224/  265]
train() client id: f_00001-11-7 loss: 0.372358  [  256/  265]
train() client id: f_00002-0-0 loss: 1.268099  [   32/  124]
train() client id: f_00002-0-1 loss: 1.284946  [   64/  124]
train() client id: f_00002-0-2 loss: 1.268825  [   96/  124]
train() client id: f_00002-1-0 loss: 1.249706  [   32/  124]
train() client id: f_00002-1-1 loss: 1.220962  [   64/  124]
train() client id: f_00002-1-2 loss: 1.225893  [   96/  124]
train() client id: f_00002-2-0 loss: 1.259060  [   32/  124]
train() client id: f_00002-2-1 loss: 1.175900  [   64/  124]
train() client id: f_00002-2-2 loss: 1.126321  [   96/  124]
train() client id: f_00002-3-0 loss: 1.188917  [   32/  124]
train() client id: f_00002-3-1 loss: 1.201596  [   64/  124]
train() client id: f_00002-3-2 loss: 1.163545  [   96/  124]
train() client id: f_00002-4-0 loss: 1.200138  [   32/  124]
train() client id: f_00002-4-1 loss: 1.181545  [   64/  124]
train() client id: f_00002-4-2 loss: 1.110214  [   96/  124]
train() client id: f_00002-5-0 loss: 1.056241  [   32/  124]
train() client id: f_00002-5-1 loss: 1.097648  [   64/  124]
train() client id: f_00002-5-2 loss: 1.108901  [   96/  124]
train() client id: f_00002-6-0 loss: 1.208383  [   32/  124]
train() client id: f_00002-6-1 loss: 1.025007  [   64/  124]
train() client id: f_00002-6-2 loss: 1.139022  [   96/  124]
train() client id: f_00002-7-0 loss: 0.996006  [   32/  124]
train() client id: f_00002-7-1 loss: 1.005999  [   64/  124]
train() client id: f_00002-7-2 loss: 1.248739  [   96/  124]
train() client id: f_00002-8-0 loss: 1.045813  [   32/  124]
train() client id: f_00002-8-1 loss: 1.127947  [   64/  124]
train() client id: f_00002-8-2 loss: 1.074426  [   96/  124]
train() client id: f_00002-9-0 loss: 1.105917  [   32/  124]
train() client id: f_00002-9-1 loss: 1.103801  [   64/  124]
train() client id: f_00002-9-2 loss: 1.037241  [   96/  124]
train() client id: f_00002-10-0 loss: 1.179050  [   32/  124]
train() client id: f_00002-10-1 loss: 0.986734  [   64/  124]
train() client id: f_00002-10-2 loss: 1.076485  [   96/  124]
train() client id: f_00002-11-0 loss: 1.070393  [   32/  124]
train() client id: f_00002-11-1 loss: 1.057884  [   64/  124]
train() client id: f_00002-11-2 loss: 1.196352  [   96/  124]
train() client id: f_00003-0-0 loss: 0.747311  [   32/   43]
train() client id: f_00003-1-0 loss: 0.847912  [   32/   43]
train() client id: f_00003-2-0 loss: 0.804233  [   32/   43]
train() client id: f_00003-3-0 loss: 0.815168  [   32/   43]
train() client id: f_00003-4-0 loss: 0.816791  [   32/   43]
train() client id: f_00003-5-0 loss: 0.750684  [   32/   43]
train() client id: f_00003-6-0 loss: 0.845799  [   32/   43]
train() client id: f_00003-7-0 loss: 0.847933  [   32/   43]
train() client id: f_00003-8-0 loss: 0.788166  [   32/   43]
train() client id: f_00003-9-0 loss: 0.828615  [   32/   43]
train() client id: f_00003-10-0 loss: 0.764905  [   32/   43]
train() client id: f_00003-11-0 loss: 0.804476  [   32/   43]
train() client id: f_00004-0-0 loss: 0.867068  [   32/  306]
train() client id: f_00004-0-1 loss: 1.038782  [   64/  306]
train() client id: f_00004-0-2 loss: 0.855972  [   96/  306]
train() client id: f_00004-0-3 loss: 0.821732  [  128/  306]
train() client id: f_00004-0-4 loss: 1.001403  [  160/  306]
train() client id: f_00004-0-5 loss: 1.049451  [  192/  306]
train() client id: f_00004-0-6 loss: 0.843281  [  224/  306]
train() client id: f_00004-0-7 loss: 1.015769  [  256/  306]
train() client id: f_00004-0-8 loss: 0.815393  [  288/  306]
train() client id: f_00004-1-0 loss: 1.032644  [   32/  306]
train() client id: f_00004-1-1 loss: 0.949384  [   64/  306]
train() client id: f_00004-1-2 loss: 0.792543  [   96/  306]
train() client id: f_00004-1-3 loss: 0.836672  [  128/  306]
train() client id: f_00004-1-4 loss: 0.974402  [  160/  306]
train() client id: f_00004-1-5 loss: 0.875224  [  192/  306]
train() client id: f_00004-1-6 loss: 0.969082  [  224/  306]
train() client id: f_00004-1-7 loss: 0.899603  [  256/  306]
train() client id: f_00004-1-8 loss: 0.898812  [  288/  306]
train() client id: f_00004-2-0 loss: 0.959259  [   32/  306]
train() client id: f_00004-2-1 loss: 0.964144  [   64/  306]
train() client id: f_00004-2-2 loss: 0.864664  [   96/  306]
train() client id: f_00004-2-3 loss: 0.954599  [  128/  306]
train() client id: f_00004-2-4 loss: 0.995373  [  160/  306]
train() client id: f_00004-2-5 loss: 0.723652  [  192/  306]
train() client id: f_00004-2-6 loss: 0.896639  [  224/  306]
train() client id: f_00004-2-7 loss: 0.991974  [  256/  306]
train() client id: f_00004-2-8 loss: 0.895610  [  288/  306]
train() client id: f_00004-3-0 loss: 1.005422  [   32/  306]
train() client id: f_00004-3-1 loss: 0.953832  [   64/  306]
train() client id: f_00004-3-2 loss: 0.883029  [   96/  306]
train() client id: f_00004-3-3 loss: 0.770298  [  128/  306]
train() client id: f_00004-3-4 loss: 0.863580  [  160/  306]
train() client id: f_00004-3-5 loss: 0.951914  [  192/  306]
train() client id: f_00004-3-6 loss: 1.040793  [  224/  306]
train() client id: f_00004-3-7 loss: 0.898085  [  256/  306]
train() client id: f_00004-3-8 loss: 0.840904  [  288/  306]
train() client id: f_00004-4-0 loss: 0.928839  [   32/  306]
train() client id: f_00004-4-1 loss: 0.882401  [   64/  306]
train() client id: f_00004-4-2 loss: 0.843574  [   96/  306]
train() client id: f_00004-4-3 loss: 0.864531  [  128/  306]
train() client id: f_00004-4-4 loss: 1.046397  [  160/  306]
train() client id: f_00004-4-5 loss: 0.769391  [  192/  306]
train() client id: f_00004-4-6 loss: 0.945459  [  224/  306]
train() client id: f_00004-4-7 loss: 1.002540  [  256/  306]
train() client id: f_00004-4-8 loss: 0.912817  [  288/  306]
train() client id: f_00004-5-0 loss: 0.989368  [   32/  306]
train() client id: f_00004-5-1 loss: 0.904111  [   64/  306]
train() client id: f_00004-5-2 loss: 0.877111  [   96/  306]
train() client id: f_00004-5-3 loss: 0.933219  [  128/  306]
train() client id: f_00004-5-4 loss: 0.891133  [  160/  306]
train() client id: f_00004-5-5 loss: 0.844272  [  192/  306]
train() client id: f_00004-5-6 loss: 0.836525  [  224/  306]
train() client id: f_00004-5-7 loss: 0.914976  [  256/  306]
train() client id: f_00004-5-8 loss: 0.932281  [  288/  306]
train() client id: f_00004-6-0 loss: 0.976170  [   32/  306]
train() client id: f_00004-6-1 loss: 0.851926  [   64/  306]
train() client id: f_00004-6-2 loss: 0.850184  [   96/  306]
train() client id: f_00004-6-3 loss: 0.922462  [  128/  306]
train() client id: f_00004-6-4 loss: 0.830643  [  160/  306]
train() client id: f_00004-6-5 loss: 0.862875  [  192/  306]
train() client id: f_00004-6-6 loss: 0.937814  [  224/  306]
train() client id: f_00004-6-7 loss: 0.904677  [  256/  306]
train() client id: f_00004-6-8 loss: 0.909608  [  288/  306]
train() client id: f_00004-7-0 loss: 0.855762  [   32/  306]
train() client id: f_00004-7-1 loss: 0.942678  [   64/  306]
train() client id: f_00004-7-2 loss: 0.871221  [   96/  306]
train() client id: f_00004-7-3 loss: 0.879666  [  128/  306]
train() client id: f_00004-7-4 loss: 0.869758  [  160/  306]
train() client id: f_00004-7-5 loss: 0.849797  [  192/  306]
train() client id: f_00004-7-6 loss: 1.017728  [  224/  306]
train() client id: f_00004-7-7 loss: 0.837930  [  256/  306]
train() client id: f_00004-7-8 loss: 0.918714  [  288/  306]
train() client id: f_00004-8-0 loss: 0.809282  [   32/  306]
train() client id: f_00004-8-1 loss: 0.870587  [   64/  306]
train() client id: f_00004-8-2 loss: 0.822758  [   96/  306]
train() client id: f_00004-8-3 loss: 0.899252  [  128/  306]
train() client id: f_00004-8-4 loss: 0.947162  [  160/  306]
train() client id: f_00004-8-5 loss: 0.937620  [  192/  306]
train() client id: f_00004-8-6 loss: 0.922294  [  224/  306]
train() client id: f_00004-8-7 loss: 0.938078  [  256/  306]
train() client id: f_00004-8-8 loss: 0.888974  [  288/  306]
train() client id: f_00004-9-0 loss: 0.989373  [   32/  306]
train() client id: f_00004-9-1 loss: 0.941372  [   64/  306]
train() client id: f_00004-9-2 loss: 0.903106  [   96/  306]
train() client id: f_00004-9-3 loss: 0.787737  [  128/  306]
train() client id: f_00004-9-4 loss: 0.904521  [  160/  306]
train() client id: f_00004-9-5 loss: 0.851113  [  192/  306]
train() client id: f_00004-9-6 loss: 0.876925  [  224/  306]
train() client id: f_00004-9-7 loss: 0.884957  [  256/  306]
train() client id: f_00004-9-8 loss: 0.892481  [  288/  306]
train() client id: f_00004-10-0 loss: 1.003092  [   32/  306]
train() client id: f_00004-10-1 loss: 0.854205  [   64/  306]
train() client id: f_00004-10-2 loss: 0.859862  [   96/  306]
train() client id: f_00004-10-3 loss: 0.869723  [  128/  306]
train() client id: f_00004-10-4 loss: 0.837403  [  160/  306]
train() client id: f_00004-10-5 loss: 0.892619  [  192/  306]
train() client id: f_00004-10-6 loss: 0.910471  [  224/  306]
train() client id: f_00004-10-7 loss: 0.874053  [  256/  306]
train() client id: f_00004-10-8 loss: 0.905978  [  288/  306]
train() client id: f_00004-11-0 loss: 0.957090  [   32/  306]
train() client id: f_00004-11-1 loss: 0.776448  [   64/  306]
train() client id: f_00004-11-2 loss: 0.979421  [   96/  306]
train() client id: f_00004-11-3 loss: 0.933215  [  128/  306]
train() client id: f_00004-11-4 loss: 0.865518  [  160/  306]
train() client id: f_00004-11-5 loss: 0.928201  [  192/  306]
train() client id: f_00004-11-6 loss: 0.841080  [  224/  306]
train() client id: f_00004-11-7 loss: 0.813421  [  256/  306]
train() client id: f_00004-11-8 loss: 0.878511  [  288/  306]
train() client id: f_00005-0-0 loss: 0.883019  [   32/  146]
train() client id: f_00005-0-1 loss: 0.834482  [   64/  146]
train() client id: f_00005-0-2 loss: 0.857888  [   96/  146]
train() client id: f_00005-0-3 loss: 0.757613  [  128/  146]
train() client id: f_00005-1-0 loss: 0.766519  [   32/  146]
train() client id: f_00005-1-1 loss: 0.648300  [   64/  146]
train() client id: f_00005-1-2 loss: 0.897899  [   96/  146]
train() client id: f_00005-1-3 loss: 1.109095  [  128/  146]
train() client id: f_00005-2-0 loss: 0.835968  [   32/  146]
train() client id: f_00005-2-1 loss: 0.812625  [   64/  146]
train() client id: f_00005-2-2 loss: 0.832334  [   96/  146]
train() client id: f_00005-2-3 loss: 0.891492  [  128/  146]
train() client id: f_00005-3-0 loss: 0.956439  [   32/  146]
train() client id: f_00005-3-1 loss: 0.848431  [   64/  146]
train() client id: f_00005-3-2 loss: 0.873721  [   96/  146]
train() client id: f_00005-3-3 loss: 0.699302  [  128/  146]
train() client id: f_00005-4-0 loss: 0.747787  [   32/  146]
train() client id: f_00005-4-1 loss: 0.929732  [   64/  146]
train() client id: f_00005-4-2 loss: 0.827991  [   96/  146]
train() client id: f_00005-4-3 loss: 0.923576  [  128/  146]
train() client id: f_00005-5-0 loss: 0.869977  [   32/  146]
train() client id: f_00005-5-1 loss: 0.863135  [   64/  146]
train() client id: f_00005-5-2 loss: 0.799479  [   96/  146]
train() client id: f_00005-5-3 loss: 0.832863  [  128/  146]
train() client id: f_00005-6-0 loss: 1.001324  [   32/  146]
train() client id: f_00005-6-1 loss: 0.791861  [   64/  146]
train() client id: f_00005-6-2 loss: 0.758550  [   96/  146]
train() client id: f_00005-6-3 loss: 0.755385  [  128/  146]
train() client id: f_00005-7-0 loss: 0.930314  [   32/  146]
train() client id: f_00005-7-1 loss: 0.688453  [   64/  146]
train() client id: f_00005-7-2 loss: 0.876167  [   96/  146]
train() client id: f_00005-7-3 loss: 0.968971  [  128/  146]
train() client id: f_00005-8-0 loss: 0.877123  [   32/  146]
train() client id: f_00005-8-1 loss: 0.831449  [   64/  146]
train() client id: f_00005-8-2 loss: 0.727015  [   96/  146]
train() client id: f_00005-8-3 loss: 0.867273  [  128/  146]
train() client id: f_00005-9-0 loss: 0.973158  [   32/  146]
train() client id: f_00005-9-1 loss: 0.632502  [   64/  146]
train() client id: f_00005-9-2 loss: 0.923579  [   96/  146]
train() client id: f_00005-9-3 loss: 0.647281  [  128/  146]
train() client id: f_00005-10-0 loss: 0.768771  [   32/  146]
train() client id: f_00005-10-1 loss: 0.897661  [   64/  146]
train() client id: f_00005-10-2 loss: 0.820121  [   96/  146]
train() client id: f_00005-10-3 loss: 0.938660  [  128/  146]
train() client id: f_00005-11-0 loss: 0.875429  [   32/  146]
train() client id: f_00005-11-1 loss: 0.862861  [   64/  146]
train() client id: f_00005-11-2 loss: 0.887244  [   96/  146]
train() client id: f_00005-11-3 loss: 0.744762  [  128/  146]
train() client id: f_00006-0-0 loss: 0.708457  [   32/   54]
train() client id: f_00006-1-0 loss: 0.665637  [   32/   54]
train() client id: f_00006-2-0 loss: 0.737000  [   32/   54]
train() client id: f_00006-3-0 loss: 0.690585  [   32/   54]
train() client id: f_00006-4-0 loss: 0.705751  [   32/   54]
train() client id: f_00006-5-0 loss: 0.747094  [   32/   54]
train() client id: f_00006-6-0 loss: 0.750227  [   32/   54]
train() client id: f_00006-7-0 loss: 0.664581  [   32/   54]
train() client id: f_00006-8-0 loss: 0.657022  [   32/   54]
train() client id: f_00006-9-0 loss: 0.710142  [   32/   54]
train() client id: f_00006-10-0 loss: 0.761158  [   32/   54]
train() client id: f_00006-11-0 loss: 0.709433  [   32/   54]
train() client id: f_00007-0-0 loss: 0.635063  [   32/  179]
train() client id: f_00007-0-1 loss: 0.657715  [   64/  179]
train() client id: f_00007-0-2 loss: 0.713189  [   96/  179]
train() client id: f_00007-0-3 loss: 0.770800  [  128/  179]
train() client id: f_00007-0-4 loss: 0.614913  [  160/  179]
train() client id: f_00007-1-0 loss: 0.743308  [   32/  179]
train() client id: f_00007-1-1 loss: 0.684723  [   64/  179]
train() client id: f_00007-1-2 loss: 0.623902  [   96/  179]
train() client id: f_00007-1-3 loss: 0.503836  [  128/  179]
train() client id: f_00007-1-4 loss: 0.640290  [  160/  179]
train() client id: f_00007-2-0 loss: 0.618508  [   32/  179]
train() client id: f_00007-2-1 loss: 0.736697  [   64/  179]
train() client id: f_00007-2-2 loss: 0.661534  [   96/  179]
train() client id: f_00007-2-3 loss: 0.546708  [  128/  179]
train() client id: f_00007-2-4 loss: 0.563602  [  160/  179]
train() client id: f_00007-3-0 loss: 0.602027  [   32/  179]
train() client id: f_00007-3-1 loss: 0.647520  [   64/  179]
train() client id: f_00007-3-2 loss: 0.539573  [   96/  179]
train() client id: f_00007-3-3 loss: 0.684766  [  128/  179]
train() client id: f_00007-3-4 loss: 0.693841  [  160/  179]
train() client id: f_00007-4-0 loss: 0.652760  [   32/  179]
train() client id: f_00007-4-1 loss: 0.655985  [   64/  179]
train() client id: f_00007-4-2 loss: 0.585017  [   96/  179]
train() client id: f_00007-4-3 loss: 0.588445  [  128/  179]
train() client id: f_00007-4-4 loss: 0.639944  [  160/  179]
train() client id: f_00007-5-0 loss: 0.631884  [   32/  179]
train() client id: f_00007-5-1 loss: 0.510476  [   64/  179]
train() client id: f_00007-5-2 loss: 0.575358  [   96/  179]
train() client id: f_00007-5-3 loss: 0.548193  [  128/  179]
train() client id: f_00007-5-4 loss: 0.630023  [  160/  179]
train() client id: f_00007-6-0 loss: 0.676477  [   32/  179]
train() client id: f_00007-6-1 loss: 0.549730  [   64/  179]
train() client id: f_00007-6-2 loss: 0.549236  [   96/  179]
train() client id: f_00007-6-3 loss: 0.636874  [  128/  179]
train() client id: f_00007-6-4 loss: 0.549776  [  160/  179]
train() client id: f_00007-7-0 loss: 0.557827  [   32/  179]
train() client id: f_00007-7-1 loss: 0.775814  [   64/  179]
train() client id: f_00007-7-2 loss: 0.575736  [   96/  179]
train() client id: f_00007-7-3 loss: 0.469870  [  128/  179]
train() client id: f_00007-7-4 loss: 0.557017  [  160/  179]
train() client id: f_00007-8-0 loss: 0.495595  [   32/  179]
train() client id: f_00007-8-1 loss: 0.532135  [   64/  179]
train() client id: f_00007-8-2 loss: 0.568192  [   96/  179]
train() client id: f_00007-8-3 loss: 0.741892  [  128/  179]
train() client id: f_00007-8-4 loss: 0.618704  [  160/  179]
train() client id: f_00007-9-0 loss: 0.654950  [   32/  179]
train() client id: f_00007-9-1 loss: 0.430956  [   64/  179]
train() client id: f_00007-9-2 loss: 0.553081  [   96/  179]
train() client id: f_00007-9-3 loss: 0.452582  [  128/  179]
train() client id: f_00007-9-4 loss: 0.749026  [  160/  179]
train() client id: f_00007-10-0 loss: 0.626219  [   32/  179]
train() client id: f_00007-10-1 loss: 0.631240  [   64/  179]
train() client id: f_00007-10-2 loss: 0.523657  [   96/  179]
train() client id: f_00007-10-3 loss: 0.597955  [  128/  179]
train() client id: f_00007-10-4 loss: 0.476506  [  160/  179]
train() client id: f_00007-11-0 loss: 0.418777  [   32/  179]
train() client id: f_00007-11-1 loss: 0.684540  [   64/  179]
train() client id: f_00007-11-2 loss: 0.477453  [   96/  179]
train() client id: f_00007-11-3 loss: 0.632428  [  128/  179]
train() client id: f_00007-11-4 loss: 0.721039  [  160/  179]
train() client id: f_00008-0-0 loss: 0.840613  [   32/  130]
train() client id: f_00008-0-1 loss: 0.737782  [   64/  130]
train() client id: f_00008-0-2 loss: 0.912978  [   96/  130]
train() client id: f_00008-0-3 loss: 0.942387  [  128/  130]
train() client id: f_00008-1-0 loss: 0.890978  [   32/  130]
train() client id: f_00008-1-1 loss: 0.887928  [   64/  130]
train() client id: f_00008-1-2 loss: 0.794424  [   96/  130]
train() client id: f_00008-1-3 loss: 0.807352  [  128/  130]
train() client id: f_00008-2-0 loss: 0.814243  [   32/  130]
train() client id: f_00008-2-1 loss: 0.872635  [   64/  130]
train() client id: f_00008-2-2 loss: 0.881371  [   96/  130]
train() client id: f_00008-2-3 loss: 0.825131  [  128/  130]
train() client id: f_00008-3-0 loss: 0.795766  [   32/  130]
train() client id: f_00008-3-1 loss: 0.821908  [   64/  130]
train() client id: f_00008-3-2 loss: 0.873422  [   96/  130]
train() client id: f_00008-3-3 loss: 0.893862  [  128/  130]
train() client id: f_00008-4-0 loss: 0.890332  [   32/  130]
train() client id: f_00008-4-1 loss: 0.863941  [   64/  130]
train() client id: f_00008-4-2 loss: 0.798736  [   96/  130]
train() client id: f_00008-4-3 loss: 0.852429  [  128/  130]
train() client id: f_00008-5-0 loss: 0.855621  [   32/  130]
train() client id: f_00008-5-1 loss: 0.782176  [   64/  130]
train() client id: f_00008-5-2 loss: 0.851795  [   96/  130]
train() client id: f_00008-5-3 loss: 0.925529  [  128/  130]
train() client id: f_00008-6-0 loss: 0.815964  [   32/  130]
train() client id: f_00008-6-1 loss: 0.903481  [   64/  130]
train() client id: f_00008-6-2 loss: 0.873608  [   96/  130]
train() client id: f_00008-6-3 loss: 0.818208  [  128/  130]
train() client id: f_00008-7-0 loss: 0.847692  [   32/  130]
train() client id: f_00008-7-1 loss: 0.802684  [   64/  130]
train() client id: f_00008-7-2 loss: 0.944170  [   96/  130]
train() client id: f_00008-7-3 loss: 0.816066  [  128/  130]
train() client id: f_00008-8-0 loss: 0.870456  [   32/  130]
train() client id: f_00008-8-1 loss: 0.842749  [   64/  130]
train() client id: f_00008-8-2 loss: 0.810192  [   96/  130]
train() client id: f_00008-8-3 loss: 0.867692  [  128/  130]
train() client id: f_00008-9-0 loss: 0.780646  [   32/  130]
train() client id: f_00008-9-1 loss: 0.920710  [   64/  130]
train() client id: f_00008-9-2 loss: 0.843914  [   96/  130]
train() client id: f_00008-9-3 loss: 0.866531  [  128/  130]
train() client id: f_00008-10-0 loss: 0.836476  [   32/  130]
train() client id: f_00008-10-1 loss: 0.916149  [   64/  130]
train() client id: f_00008-10-2 loss: 0.725830  [   96/  130]
train() client id: f_00008-10-3 loss: 0.884901  [  128/  130]
train() client id: f_00008-11-0 loss: 0.906420  [   32/  130]
train() client id: f_00008-11-1 loss: 0.772301  [   64/  130]
train() client id: f_00008-11-2 loss: 0.832823  [   96/  130]
train() client id: f_00008-11-3 loss: 0.869039  [  128/  130]
train() client id: f_00009-0-0 loss: 1.209799  [   32/  118]
train() client id: f_00009-0-1 loss: 1.224572  [   64/  118]
train() client id: f_00009-0-2 loss: 1.029339  [   96/  118]
train() client id: f_00009-1-0 loss: 1.186881  [   32/  118]
train() client id: f_00009-1-1 loss: 1.134263  [   64/  118]
train() client id: f_00009-1-2 loss: 1.011704  [   96/  118]
train() client id: f_00009-2-0 loss: 1.185568  [   32/  118]
train() client id: f_00009-2-1 loss: 1.016151  [   64/  118]
train() client id: f_00009-2-2 loss: 1.016941  [   96/  118]
train() client id: f_00009-3-0 loss: 1.110007  [   32/  118]
train() client id: f_00009-3-1 loss: 1.032590  [   64/  118]
train() client id: f_00009-3-2 loss: 1.006891  [   96/  118]
train() client id: f_00009-4-0 loss: 0.990432  [   32/  118]
train() client id: f_00009-4-1 loss: 0.950929  [   64/  118]
train() client id: f_00009-4-2 loss: 1.040721  [   96/  118]
train() client id: f_00009-5-0 loss: 1.023405  [   32/  118]
train() client id: f_00009-5-1 loss: 0.896926  [   64/  118]
train() client id: f_00009-5-2 loss: 0.964152  [   96/  118]
train() client id: f_00009-6-0 loss: 0.946024  [   32/  118]
train() client id: f_00009-6-1 loss: 0.980744  [   64/  118]
train() client id: f_00009-6-2 loss: 0.867044  [   96/  118]
train() client id: f_00009-7-0 loss: 0.859491  [   32/  118]
train() client id: f_00009-7-1 loss: 0.946690  [   64/  118]
train() client id: f_00009-7-2 loss: 0.979982  [   96/  118]
train() client id: f_00009-8-0 loss: 0.869744  [   32/  118]
train() client id: f_00009-8-1 loss: 0.849614  [   64/  118]
train() client id: f_00009-8-2 loss: 0.916656  [   96/  118]
train() client id: f_00009-9-0 loss: 0.949257  [   32/  118]
train() client id: f_00009-9-1 loss: 0.876360  [   64/  118]
train() client id: f_00009-9-2 loss: 0.932128  [   96/  118]
train() client id: f_00009-10-0 loss: 0.941950  [   32/  118]
train() client id: f_00009-10-1 loss: 0.853866  [   64/  118]
train() client id: f_00009-10-2 loss: 0.870260  [   96/  118]
train() client id: f_00009-11-0 loss: 0.929514  [   32/  118]
train() client id: f_00009-11-1 loss: 0.862647  [   64/  118]
train() client id: f_00009-11-2 loss: 0.843799  [   96/  118]
At round 13 accuracy: 0.6312997347480106
At round 13 training accuracy: 0.5767940979208585
At round 13 training loss: 0.8650267529144148
update_location
xs = [ -3.9056584    4.20031788  85.00902392  18.81129433   0.97929623
   3.95640986 -47.44319194 -26.32485185  69.66397685 -12.06087855]
ys = [ 77.5879595   60.55583871   1.32061395 -47.45517586  39.35018685
  22.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [126.62995549 116.98141849 131.25653572 112.27581445 107.46811725
 102.64569476 110.71470997 103.41022232 123.13301606 100.80414995]
dists_bs = [196.88163795 212.62695027 312.6807024  295.04088374 222.19267429
 234.91685624 218.6809876  229.00251994 290.93810241 236.19832881]
uav_gains = [5.54132201e-11 6.75605000e-11 5.06550612e-11 7.48645336e-11
 8.35211431e-11 9.36797744e-11 7.75317969e-11 9.19578362e-11
 5.94337263e-11 9.80171753e-11]
bs_gains = [4.16408405e-11 3.35708871e-11 1.14027884e-11 1.34160270e-11
 2.96791066e-11 2.53941909e-11 3.10329607e-11 2.72735301e-11
 1.39525118e-11 2.50103050e-11]
Round 14
-------------------------------
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.14357806 19.06538973  9.00414718  3.22065168 21.99169094 10.5993586
  4.00331986 12.90419715  9.4917528   8.60518442]
obj_prev = 108.02927042739734
eta_min = 6.894580279323513e-11	eta_max = 0.920963466545226
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 25.120031493789718	eta = 0.9090909090909091
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 43.56050107516789	eta = 0.5242453990066629
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 34.74720289395531	eta = 0.6572152681404528
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.166400765192755	eta = 0.6885399603277947
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.08814089362885	eta = 0.6901684908951391
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.08793538155577	eta = 0.6901727775922599
eta = 0.6901727775922599
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [0.03059863 0.06435425 0.03011293 0.01044239 0.07431095 0.03545555
 0.0131137  0.04346948 0.03157001 0.02865586]
ene_total = [2.84199716 5.44950306 2.81511635 1.29013739 6.21774054 3.30722781
 1.48846279 3.76427446 3.1172366  2.79623923]
ti_comp = [0.31764397 0.30976147 0.31634818 0.32166983 0.30755338 0.30458011
 0.32210904 0.32417046 0.29089448 0.30427822]
ti_coms = [0.07051759 0.0784001  0.07181339 0.06649173 0.08060818 0.08358146
 0.06605252 0.06399111 0.09726708 0.08388335]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.77461660e-05 1.73602902e-04 1.70533033e-05 6.87794728e-07
 2.71142377e-04 3.00282236e-05 1.35846854e-06 4.88524522e-05
 2.32398452e-05 1.58846785e-05]
ene_total = [0.53363372 0.6048986  0.54336256 0.50195733 0.62892872 0.63317194
 0.49869262 0.4867173  0.73596393 0.63438309]
optimize_network_iter = 0 obj = 5.8017097996113725
eta = 0.6901727775922599
freqs = [4.81649713e+07 1.03877111e+08 4.75946046e+07 1.62315316e+07
 1.20809836e+08 5.82039824e+07 2.03559896e+07 6.70472512e+07
 5.42636806e+07 4.70882466e+07]
eta_min = 0.6901727775922659	eta_max = 0.6901727775922564
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 0.041597769241550876	eta = 0.9090909090909091
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 19.658200800192578	eta = 0.0019236833645317572
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 2.023882036535733	eta = 0.01868495948542783
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 1.9708396260580614	eta = 0.01918783921124654
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 1.9708261485414031	eta = 0.01918797042749958
eta = 0.01918797042749958
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.89202933e-04 1.85088870e-03 1.81815891e-04 7.33300812e-06
 2.89081782e-03 3.20149601e-04 1.44834795e-05 5.20846430e-04
 2.47774470e-04 1.69356454e-04]
ene_total = [0.17265704 0.23107452 0.17557065 0.15872099 0.26113616 0.20692952
 0.15784421 0.16500284 0.23783644 0.20405377]
ti_comp = [0.31764397 0.30976147 0.31634818 0.32166983 0.30755338 0.30458011
 0.32210904 0.32417046 0.29089448 0.30427822]
ti_coms = [0.07051759 0.0784001  0.07181339 0.06649173 0.08060818 0.08358146
 0.06605252 0.06399111 0.09726708 0.08388335]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.77461660e-05 1.73602902e-04 1.70533033e-05 6.87794728e-07
 2.71142377e-04 3.00282236e-05 1.35846854e-06 4.88524522e-05
 2.32398452e-05 1.58846785e-05]
ene_total = [0.53363372 0.6048986  0.54336256 0.50195733 0.62892872 0.63317194
 0.49869262 0.4867173  0.73596393 0.63438309]
optimize_network_iter = 1 obj = 5.801709799611484
eta = 0.6901727775922659
freqs = [4.81649713e+07 1.03877111e+08 4.75946046e+07 1.62315316e+07
 1.20809836e+08 5.82039824e+07 2.03559896e+07 6.70472512e+07
 5.42636806e+07 4.70882466e+07]
Done!
At round 14 eta: 0.6901727775922659
At round 14 local rounds: 12.142311740979952
At round 14 global rounds: 75.48388039288098
At round 14 a_n: 23.044415151714617
gradient difference: 0.4645424783229828
train() client id: f_00000-0-0 loss: 1.298193  [   32/  126]
train() client id: f_00000-0-1 loss: 1.301503  [   64/  126]
train() client id: f_00000-0-2 loss: 1.349512  [   96/  126]
train() client id: f_00000-1-0 loss: 1.336199  [   32/  126]
train() client id: f_00000-1-1 loss: 1.173053  [   64/  126]
train() client id: f_00000-1-2 loss: 1.267907  [   96/  126]
train() client id: f_00000-2-0 loss: 1.248902  [   32/  126]
train() client id: f_00000-2-1 loss: 1.114491  [   64/  126]
train() client id: f_00000-2-2 loss: 1.038890  [   96/  126]
train() client id: f_00000-3-0 loss: 1.114010  [   32/  126]
train() client id: f_00000-3-1 loss: 1.048561  [   64/  126]
train() client id: f_00000-3-2 loss: 1.002899  [   96/  126]
train() client id: f_00000-4-0 loss: 0.984987  [   32/  126]
train() client id: f_00000-4-1 loss: 0.930215  [   64/  126]
train() client id: f_00000-4-2 loss: 1.013270  [   96/  126]
train() client id: f_00000-5-0 loss: 0.985169  [   32/  126]
train() client id: f_00000-5-1 loss: 0.926472  [   64/  126]
train() client id: f_00000-5-2 loss: 0.984705  [   96/  126]
train() client id: f_00000-6-0 loss: 0.898996  [   32/  126]
train() client id: f_00000-6-1 loss: 0.968852  [   64/  126]
train() client id: f_00000-6-2 loss: 0.873652  [   96/  126]
train() client id: f_00000-7-0 loss: 0.807934  [   32/  126]
train() client id: f_00000-7-1 loss: 0.939128  [   64/  126]
train() client id: f_00000-7-2 loss: 0.946615  [   96/  126]
train() client id: f_00000-8-0 loss: 0.857414  [   32/  126]
train() client id: f_00000-8-1 loss: 0.797146  [   64/  126]
train() client id: f_00000-8-2 loss: 0.808874  [   96/  126]
train() client id: f_00000-9-0 loss: 0.872954  [   32/  126]
train() client id: f_00000-9-1 loss: 0.759622  [   64/  126]
train() client id: f_00000-9-2 loss: 0.951847  [   96/  126]
train() client id: f_00000-10-0 loss: 0.873706  [   32/  126]
train() client id: f_00000-10-1 loss: 0.781265  [   64/  126]
train() client id: f_00000-10-2 loss: 0.826001  [   96/  126]
train() client id: f_00000-11-0 loss: 0.820010  [   32/  126]
train() client id: f_00000-11-1 loss: 0.766288  [   64/  126]
train() client id: f_00000-11-2 loss: 0.850908  [   96/  126]
train() client id: f_00001-0-0 loss: 0.551080  [   32/  265]
train() client id: f_00001-0-1 loss: 0.597919  [   64/  265]
train() client id: f_00001-0-2 loss: 0.428579  [   96/  265]
train() client id: f_00001-0-3 loss: 0.418896  [  128/  265]
train() client id: f_00001-0-4 loss: 0.495568  [  160/  265]
train() client id: f_00001-0-5 loss: 0.460879  [  192/  265]
train() client id: f_00001-0-6 loss: 0.574380  [  224/  265]
train() client id: f_00001-0-7 loss: 0.495885  [  256/  265]
train() client id: f_00001-1-0 loss: 0.498015  [   32/  265]
train() client id: f_00001-1-1 loss: 0.392616  [   64/  265]
train() client id: f_00001-1-2 loss: 0.529615  [   96/  265]
train() client id: f_00001-1-3 loss: 0.579067  [  128/  265]
train() client id: f_00001-1-4 loss: 0.413618  [  160/  265]
train() client id: f_00001-1-5 loss: 0.571582  [  192/  265]
train() client id: f_00001-1-6 loss: 0.489478  [  224/  265]
train() client id: f_00001-1-7 loss: 0.469512  [  256/  265]
train() client id: f_00001-2-0 loss: 0.416776  [   32/  265]
train() client id: f_00001-2-1 loss: 0.462572  [   64/  265]
train() client id: f_00001-2-2 loss: 0.455014  [   96/  265]
train() client id: f_00001-2-3 loss: 0.455975  [  128/  265]
train() client id: f_00001-2-4 loss: 0.554299  [  160/  265]
train() client id: f_00001-2-5 loss: 0.517173  [  192/  265]
train() client id: f_00001-2-6 loss: 0.425587  [  224/  265]
train() client id: f_00001-2-7 loss: 0.553073  [  256/  265]
train() client id: f_00001-3-0 loss: 0.608459  [   32/  265]
train() client id: f_00001-3-1 loss: 0.472573  [   64/  265]
train() client id: f_00001-3-2 loss: 0.473593  [   96/  265]
train() client id: f_00001-3-3 loss: 0.520256  [  128/  265]
train() client id: f_00001-3-4 loss: 0.465486  [  160/  265]
train() client id: f_00001-3-5 loss: 0.386774  [  192/  265]
train() client id: f_00001-3-6 loss: 0.445135  [  224/  265]
train() client id: f_00001-3-7 loss: 0.446892  [  256/  265]
train() client id: f_00001-4-0 loss: 0.483523  [   32/  265]
train() client id: f_00001-4-1 loss: 0.504339  [   64/  265]
train() client id: f_00001-4-2 loss: 0.568098  [   96/  265]
train() client id: f_00001-4-3 loss: 0.480761  [  128/  265]
train() client id: f_00001-4-4 loss: 0.445303  [  160/  265]
train() client id: f_00001-4-5 loss: 0.417965  [  192/  265]
train() client id: f_00001-4-6 loss: 0.524084  [  224/  265]
train() client id: f_00001-4-7 loss: 0.372198  [  256/  265]
train() client id: f_00001-5-0 loss: 0.459776  [   32/  265]
train() client id: f_00001-5-1 loss: 0.496293  [   64/  265]
train() client id: f_00001-5-2 loss: 0.412113  [   96/  265]
train() client id: f_00001-5-3 loss: 0.550695  [  128/  265]
train() client id: f_00001-5-4 loss: 0.403113  [  160/  265]
train() client id: f_00001-5-5 loss: 0.499128  [  192/  265]
train() client id: f_00001-5-6 loss: 0.485813  [  224/  265]
train() client id: f_00001-5-7 loss: 0.377030  [  256/  265]
train() client id: f_00001-6-0 loss: 0.472225  [   32/  265]
train() client id: f_00001-6-1 loss: 0.454260  [   64/  265]
train() client id: f_00001-6-2 loss: 0.386028  [   96/  265]
train() client id: f_00001-6-3 loss: 0.564034  [  128/  265]
train() client id: f_00001-6-4 loss: 0.489483  [  160/  265]
train() client id: f_00001-6-5 loss: 0.514378  [  192/  265]
train() client id: f_00001-6-6 loss: 0.463968  [  224/  265]
train() client id: f_00001-6-7 loss: 0.375013  [  256/  265]
train() client id: f_00001-7-0 loss: 0.471595  [   32/  265]
train() client id: f_00001-7-1 loss: 0.397177  [   64/  265]
train() client id: f_00001-7-2 loss: 0.497260  [   96/  265]
train() client id: f_00001-7-3 loss: 0.599858  [  128/  265]
train() client id: f_00001-7-4 loss: 0.372742  [  160/  265]
train() client id: f_00001-7-5 loss: 0.432520  [  192/  265]
train() client id: f_00001-7-6 loss: 0.498227  [  224/  265]
train() client id: f_00001-7-7 loss: 0.389075  [  256/  265]
train() client id: f_00001-8-0 loss: 0.430899  [   32/  265]
train() client id: f_00001-8-1 loss: 0.477503  [   64/  265]
train() client id: f_00001-8-2 loss: 0.613166  [   96/  265]
train() client id: f_00001-8-3 loss: 0.372963  [  128/  265]
train() client id: f_00001-8-4 loss: 0.550722  [  160/  265]
train() client id: f_00001-8-5 loss: 0.366520  [  192/  265]
train() client id: f_00001-8-6 loss: 0.404535  [  224/  265]
train() client id: f_00001-8-7 loss: 0.504627  [  256/  265]
train() client id: f_00001-9-0 loss: 0.455171  [   32/  265]
train() client id: f_00001-9-1 loss: 0.414242  [   64/  265]
train() client id: f_00001-9-2 loss: 0.452109  [   96/  265]
train() client id: f_00001-9-3 loss: 0.453374  [  128/  265]
train() client id: f_00001-9-4 loss: 0.471361  [  160/  265]
train() client id: f_00001-9-5 loss: 0.577338  [  192/  265]
train() client id: f_00001-9-6 loss: 0.395949  [  224/  265]
train() client id: f_00001-9-7 loss: 0.496525  [  256/  265]
train() client id: f_00001-10-0 loss: 0.462036  [   32/  265]
train() client id: f_00001-10-1 loss: 0.461969  [   64/  265]
train() client id: f_00001-10-2 loss: 0.422495  [   96/  265]
train() client id: f_00001-10-3 loss: 0.552944  [  128/  265]
train() client id: f_00001-10-4 loss: 0.445745  [  160/  265]
train() client id: f_00001-10-5 loss: 0.554670  [  192/  265]
train() client id: f_00001-10-6 loss: 0.441255  [  224/  265]
train() client id: f_00001-10-7 loss: 0.354288  [  256/  265]
train() client id: f_00001-11-0 loss: 0.536632  [   32/  265]
train() client id: f_00001-11-1 loss: 0.517530  [   64/  265]
train() client id: f_00001-11-2 loss: 0.431473  [   96/  265]
train() client id: f_00001-11-3 loss: 0.372468  [  128/  265]
train() client id: f_00001-11-4 loss: 0.478283  [  160/  265]
train() client id: f_00001-11-5 loss: 0.486725  [  192/  265]
train() client id: f_00001-11-6 loss: 0.502552  [  224/  265]
train() client id: f_00001-11-7 loss: 0.370515  [  256/  265]
train() client id: f_00002-0-0 loss: 1.046029  [   32/  124]
train() client id: f_00002-0-1 loss: 1.264018  [   64/  124]
train() client id: f_00002-0-2 loss: 1.174316  [   96/  124]
train() client id: f_00002-1-0 loss: 1.187729  [   32/  124]
train() client id: f_00002-1-1 loss: 0.991373  [   64/  124]
train() client id: f_00002-1-2 loss: 1.125874  [   96/  124]
train() client id: f_00002-2-0 loss: 1.081076  [   32/  124]
train() client id: f_00002-2-1 loss: 1.072546  [   64/  124]
train() client id: f_00002-2-2 loss: 1.002527  [   96/  124]
train() client id: f_00002-3-0 loss: 1.004974  [   32/  124]
train() client id: f_00002-3-1 loss: 1.048349  [   64/  124]
train() client id: f_00002-3-2 loss: 0.980269  [   96/  124]
train() client id: f_00002-4-0 loss: 1.033724  [   32/  124]
train() client id: f_00002-4-1 loss: 0.950950  [   64/  124]
train() client id: f_00002-4-2 loss: 0.928968  [   96/  124]
train() client id: f_00002-5-0 loss: 1.072437  [   32/  124]
train() client id: f_00002-5-1 loss: 0.896583  [   64/  124]
train() client id: f_00002-5-2 loss: 0.946115  [   96/  124]
train() client id: f_00002-6-0 loss: 0.931600  [   32/  124]
train() client id: f_00002-6-1 loss: 1.001957  [   64/  124]
train() client id: f_00002-6-2 loss: 0.828411  [   96/  124]
train() client id: f_00002-7-0 loss: 0.991555  [   32/  124]
train() client id: f_00002-7-1 loss: 0.822010  [   64/  124]
train() client id: f_00002-7-2 loss: 0.972284  [   96/  124]
train() client id: f_00002-8-0 loss: 1.025059  [   32/  124]
train() client id: f_00002-8-1 loss: 0.937132  [   64/  124]
train() client id: f_00002-8-2 loss: 0.796966  [   96/  124]
train() client id: f_00002-9-0 loss: 0.946098  [   32/  124]
train() client id: f_00002-9-1 loss: 0.860085  [   64/  124]
train() client id: f_00002-9-2 loss: 0.958963  [   96/  124]
train() client id: f_00002-10-0 loss: 0.906049  [   32/  124]
train() client id: f_00002-10-1 loss: 0.862072  [   64/  124]
train() client id: f_00002-10-2 loss: 0.878193  [   96/  124]
train() client id: f_00002-11-0 loss: 0.877510  [   32/  124]
train() client id: f_00002-11-1 loss: 0.828418  [   64/  124]
train() client id: f_00002-11-2 loss: 0.907637  [   96/  124]
train() client id: f_00003-0-0 loss: 0.836423  [   32/   43]
train() client id: f_00003-1-0 loss: 0.891401  [   32/   43]
train() client id: f_00003-2-0 loss: 0.847354  [   32/   43]
train() client id: f_00003-3-0 loss: 0.850850  [   32/   43]
train() client id: f_00003-4-0 loss: 0.828140  [   32/   43]
train() client id: f_00003-5-0 loss: 0.850183  [   32/   43]
train() client id: f_00003-6-0 loss: 0.873097  [   32/   43]
train() client id: f_00003-7-0 loss: 0.926926  [   32/   43]
train() client id: f_00003-8-0 loss: 0.955458  [   32/   43]
train() client id: f_00003-9-0 loss: 0.966075  [   32/   43]
train() client id: f_00003-10-0 loss: 0.964442  [   32/   43]
train() client id: f_00003-11-0 loss: 0.867643  [   32/   43]
train() client id: f_00004-0-0 loss: 0.978628  [   32/  306]
train() client id: f_00004-0-1 loss: 0.954353  [   64/  306]
train() client id: f_00004-0-2 loss: 0.825882  [   96/  306]
train() client id: f_00004-0-3 loss: 0.927795  [  128/  306]
train() client id: f_00004-0-4 loss: 0.993091  [  160/  306]
train() client id: f_00004-0-5 loss: 0.895946  [  192/  306]
train() client id: f_00004-0-6 loss: 0.795922  [  224/  306]
train() client id: f_00004-0-7 loss: 0.870214  [  256/  306]
train() client id: f_00004-0-8 loss: 0.850917  [  288/  306]
train() client id: f_00004-1-0 loss: 0.862336  [   32/  306]
train() client id: f_00004-1-1 loss: 0.859775  [   64/  306]
train() client id: f_00004-1-2 loss: 0.817179  [   96/  306]
train() client id: f_00004-1-3 loss: 0.873672  [  128/  306]
train() client id: f_00004-1-4 loss: 0.892486  [  160/  306]
train() client id: f_00004-1-5 loss: 0.865764  [  192/  306]
train() client id: f_00004-1-6 loss: 0.983200  [  224/  306]
train() client id: f_00004-1-7 loss: 0.943415  [  256/  306]
train() client id: f_00004-1-8 loss: 0.939453  [  288/  306]
train() client id: f_00004-2-0 loss: 0.980732  [   32/  306]
train() client id: f_00004-2-1 loss: 0.794695  [   64/  306]
train() client id: f_00004-2-2 loss: 0.924742  [   96/  306]
train() client id: f_00004-2-3 loss: 0.818784  [  128/  306]
train() client id: f_00004-2-4 loss: 0.939926  [  160/  306]
train() client id: f_00004-2-5 loss: 1.030099  [  192/  306]
train() client id: f_00004-2-6 loss: 0.923523  [  224/  306]
train() client id: f_00004-2-7 loss: 0.776906  [  256/  306]
train() client id: f_00004-2-8 loss: 0.846461  [  288/  306]
train() client id: f_00004-3-0 loss: 0.804723  [   32/  306]
train() client id: f_00004-3-1 loss: 0.911639  [   64/  306]
train() client id: f_00004-3-2 loss: 0.972684  [   96/  306]
train() client id: f_00004-3-3 loss: 0.911114  [  128/  306]
train() client id: f_00004-3-4 loss: 0.863386  [  160/  306]
train() client id: f_00004-3-5 loss: 0.932998  [  192/  306]
train() client id: f_00004-3-6 loss: 0.878475  [  224/  306]
train() client id: f_00004-3-7 loss: 0.821284  [  256/  306]
train() client id: f_00004-3-8 loss: 1.022686  [  288/  306]
train() client id: f_00004-4-0 loss: 0.885960  [   32/  306]
train() client id: f_00004-4-1 loss: 0.958222  [   64/  306]
train() client id: f_00004-4-2 loss: 0.909283  [   96/  306]
train() client id: f_00004-4-3 loss: 0.904667  [  128/  306]
train() client id: f_00004-4-4 loss: 0.827664  [  160/  306]
train() client id: f_00004-4-5 loss: 0.947528  [  192/  306]
train() client id: f_00004-4-6 loss: 0.759254  [  224/  306]
train() client id: f_00004-4-7 loss: 0.822122  [  256/  306]
train() client id: f_00004-4-8 loss: 1.020026  [  288/  306]
train() client id: f_00004-5-0 loss: 0.864073  [   32/  306]
train() client id: f_00004-5-1 loss: 0.994676  [   64/  306]
train() client id: f_00004-5-2 loss: 0.919159  [   96/  306]
train() client id: f_00004-5-3 loss: 0.920593  [  128/  306]
train() client id: f_00004-5-4 loss: 0.929006  [  160/  306]
train() client id: f_00004-5-5 loss: 0.878946  [  192/  306]
train() client id: f_00004-5-6 loss: 0.769997  [  224/  306]
train() client id: f_00004-5-7 loss: 0.890855  [  256/  306]
train() client id: f_00004-5-8 loss: 0.930272  [  288/  306]
train() client id: f_00004-6-0 loss: 0.981230  [   32/  306]
train() client id: f_00004-6-1 loss: 0.811743  [   64/  306]
train() client id: f_00004-6-2 loss: 0.911229  [   96/  306]
train() client id: f_00004-6-3 loss: 0.793752  [  128/  306]
train() client id: f_00004-6-4 loss: 0.990971  [  160/  306]
train() client id: f_00004-6-5 loss: 0.898285  [  192/  306]
train() client id: f_00004-6-6 loss: 0.861255  [  224/  306]
train() client id: f_00004-6-7 loss: 0.959378  [  256/  306]
train() client id: f_00004-6-8 loss: 0.895142  [  288/  306]
train() client id: f_00004-7-0 loss: 0.882401  [   32/  306]
train() client id: f_00004-7-1 loss: 0.846731  [   64/  306]
train() client id: f_00004-7-2 loss: 0.849632  [   96/  306]
train() client id: f_00004-7-3 loss: 0.797814  [  128/  306]
train() client id: f_00004-7-4 loss: 0.941436  [  160/  306]
train() client id: f_00004-7-5 loss: 0.835742  [  192/  306]
train() client id: f_00004-7-6 loss: 0.879234  [  224/  306]
train() client id: f_00004-7-7 loss: 0.947973  [  256/  306]
train() client id: f_00004-7-8 loss: 1.032133  [  288/  306]
train() client id: f_00004-8-0 loss: 0.810742  [   32/  306]
train() client id: f_00004-8-1 loss: 0.997749  [   64/  306]
train() client id: f_00004-8-2 loss: 0.916449  [   96/  306]
train() client id: f_00004-8-3 loss: 0.924150  [  128/  306]
train() client id: f_00004-8-4 loss: 0.875512  [  160/  306]
train() client id: f_00004-8-5 loss: 0.822380  [  192/  306]
train() client id: f_00004-8-6 loss: 0.859557  [  224/  306]
train() client id: f_00004-8-7 loss: 0.927378  [  256/  306]
train() client id: f_00004-8-8 loss: 0.946364  [  288/  306]
train() client id: f_00004-9-0 loss: 0.836742  [   32/  306]
train() client id: f_00004-9-1 loss: 0.902535  [   64/  306]
train() client id: f_00004-9-2 loss: 0.915919  [   96/  306]
train() client id: f_00004-9-3 loss: 0.889726  [  128/  306]
train() client id: f_00004-9-4 loss: 0.844006  [  160/  306]
train() client id: f_00004-9-5 loss: 0.917201  [  192/  306]
train() client id: f_00004-9-6 loss: 1.000763  [  224/  306]
train() client id: f_00004-9-7 loss: 0.831915  [  256/  306]
train() client id: f_00004-9-8 loss: 0.873071  [  288/  306]
train() client id: f_00004-10-0 loss: 0.967331  [   32/  306]
train() client id: f_00004-10-1 loss: 0.918191  [   64/  306]
train() client id: f_00004-10-2 loss: 0.873367  [   96/  306]
train() client id: f_00004-10-3 loss: 0.910800  [  128/  306]
train() client id: f_00004-10-4 loss: 0.961740  [  160/  306]
train() client id: f_00004-10-5 loss: 0.802617  [  192/  306]
train() client id: f_00004-10-6 loss: 0.851375  [  224/  306]
train() client id: f_00004-10-7 loss: 0.844360  [  256/  306]
train() client id: f_00004-10-8 loss: 0.839051  [  288/  306]
train() client id: f_00004-11-0 loss: 0.862159  [   32/  306]
train() client id: f_00004-11-1 loss: 0.897972  [   64/  306]
train() client id: f_00004-11-2 loss: 1.038986  [   96/  306]
train() client id: f_00004-11-3 loss: 0.826176  [  128/  306]
train() client id: f_00004-11-4 loss: 0.780885  [  160/  306]
train() client id: f_00004-11-5 loss: 0.942424  [  192/  306]
train() client id: f_00004-11-6 loss: 0.856514  [  224/  306]
train() client id: f_00004-11-7 loss: 0.906574  [  256/  306]
train() client id: f_00004-11-8 loss: 0.960554  [  288/  306]
train() client id: f_00005-0-0 loss: 0.717204  [   32/  146]
train() client id: f_00005-0-1 loss: 0.713696  [   64/  146]
train() client id: f_00005-0-2 loss: 0.903204  [   96/  146]
train() client id: f_00005-0-3 loss: 0.897196  [  128/  146]
train() client id: f_00005-1-0 loss: 0.844552  [   32/  146]
train() client id: f_00005-1-1 loss: 0.989542  [   64/  146]
train() client id: f_00005-1-2 loss: 0.732393  [   96/  146]
train() client id: f_00005-1-3 loss: 0.705126  [  128/  146]
train() client id: f_00005-2-0 loss: 0.951945  [   32/  146]
train() client id: f_00005-2-1 loss: 0.687827  [   64/  146]
train() client id: f_00005-2-2 loss: 0.793299  [   96/  146]
train() client id: f_00005-2-3 loss: 0.898172  [  128/  146]
train() client id: f_00005-3-0 loss: 0.708639  [   32/  146]
train() client id: f_00005-3-1 loss: 0.730937  [   64/  146]
train() client id: f_00005-3-2 loss: 0.880926  [   96/  146]
train() client id: f_00005-3-3 loss: 0.936525  [  128/  146]
train() client id: f_00005-4-0 loss: 0.787867  [   32/  146]
train() client id: f_00005-4-1 loss: 0.788169  [   64/  146]
train() client id: f_00005-4-2 loss: 0.838732  [   96/  146]
train() client id: f_00005-4-3 loss: 0.847094  [  128/  146]
train() client id: f_00005-5-0 loss: 0.766316  [   32/  146]
train() client id: f_00005-5-1 loss: 0.965032  [   64/  146]
train() client id: f_00005-5-2 loss: 0.750358  [   96/  146]
train() client id: f_00005-5-3 loss: 0.701167  [  128/  146]
train() client id: f_00005-6-0 loss: 0.859308  [   32/  146]
train() client id: f_00005-6-1 loss: 0.859796  [   64/  146]
train() client id: f_00005-6-2 loss: 0.607096  [   96/  146]
train() client id: f_00005-6-3 loss: 0.880650  [  128/  146]
train() client id: f_00005-7-0 loss: 0.786371  [   32/  146]
train() client id: f_00005-7-1 loss: 0.828165  [   64/  146]
train() client id: f_00005-7-2 loss: 0.867800  [   96/  146]
train() client id: f_00005-7-3 loss: 0.904981  [  128/  146]
train() client id: f_00005-8-0 loss: 1.073611  [   32/  146]
train() client id: f_00005-8-1 loss: 0.840505  [   64/  146]
train() client id: f_00005-8-2 loss: 0.689066  [   96/  146]
train() client id: f_00005-8-3 loss: 0.717286  [  128/  146]
train() client id: f_00005-9-0 loss: 0.817332  [   32/  146]
train() client id: f_00005-9-1 loss: 1.012914  [   64/  146]
train() client id: f_00005-9-2 loss: 0.769323  [   96/  146]
train() client id: f_00005-9-3 loss: 0.826830  [  128/  146]
train() client id: f_00005-10-0 loss: 0.767407  [   32/  146]
train() client id: f_00005-10-1 loss: 0.724511  [   64/  146]
train() client id: f_00005-10-2 loss: 0.846992  [   96/  146]
train() client id: f_00005-10-3 loss: 0.780686  [  128/  146]
train() client id: f_00005-11-0 loss: 0.738860  [   32/  146]
train() client id: f_00005-11-1 loss: 0.893122  [   64/  146]
train() client id: f_00005-11-2 loss: 0.935535  [   96/  146]
train() client id: f_00005-11-3 loss: 0.684550  [  128/  146]
train() client id: f_00006-0-0 loss: 0.654416  [   32/   54]
train() client id: f_00006-1-0 loss: 0.631104  [   32/   54]
train() client id: f_00006-2-0 loss: 0.682535  [   32/   54]
train() client id: f_00006-3-0 loss: 0.690181  [   32/   54]
train() client id: f_00006-4-0 loss: 0.676810  [   32/   54]
train() client id: f_00006-5-0 loss: 0.635354  [   32/   54]
train() client id: f_00006-6-0 loss: 0.629012  [   32/   54]
train() client id: f_00006-7-0 loss: 0.683564  [   32/   54]
train() client id: f_00006-8-0 loss: 0.637478  [   32/   54]
train() client id: f_00006-9-0 loss: 0.649360  [   32/   54]
train() client id: f_00006-10-0 loss: 0.638253  [   32/   54]
train() client id: f_00006-11-0 loss: 0.637583  [   32/   54]
train() client id: f_00007-0-0 loss: 0.538384  [   32/  179]
train() client id: f_00007-0-1 loss: 0.618759  [   64/  179]
train() client id: f_00007-0-2 loss: 0.548853  [   96/  179]
train() client id: f_00007-0-3 loss: 0.566144  [  128/  179]
train() client id: f_00007-0-4 loss: 0.432208  [  160/  179]
train() client id: f_00007-1-0 loss: 0.535386  [   32/  179]
train() client id: f_00007-1-1 loss: 0.429759  [   64/  179]
train() client id: f_00007-1-2 loss: 0.475205  [   96/  179]
train() client id: f_00007-1-3 loss: 0.728915  [  128/  179]
train() client id: f_00007-1-4 loss: 0.444909  [  160/  179]
train() client id: f_00007-2-0 loss: 0.583435  [   32/  179]
train() client id: f_00007-2-1 loss: 0.406215  [   64/  179]
train() client id: f_00007-2-2 loss: 0.405513  [   96/  179]
train() client id: f_00007-2-3 loss: 0.558864  [  128/  179]
train() client id: f_00007-2-4 loss: 0.436427  [  160/  179]
train() client id: f_00007-3-0 loss: 0.454959  [   32/  179]
train() client id: f_00007-3-1 loss: 0.624137  [   64/  179]
train() client id: f_00007-3-2 loss: 0.451853  [   96/  179]
train() client id: f_00007-3-3 loss: 0.369919  [  128/  179]
train() client id: f_00007-3-4 loss: 0.491656  [  160/  179]
train() client id: f_00007-4-0 loss: 0.434632  [   32/  179]
train() client id: f_00007-4-1 loss: 0.484359  [   64/  179]
train() client id: f_00007-4-2 loss: 0.472976  [   96/  179]
train() client id: f_00007-4-3 loss: 0.468107  [  128/  179]
train() client id: f_00007-4-4 loss: 0.452412  [  160/  179]
train() client id: f_00007-5-0 loss: 0.463848  [   32/  179]
train() client id: f_00007-5-1 loss: 0.553721  [   64/  179]
train() client id: f_00007-5-2 loss: 0.438464  [   96/  179]
train() client id: f_00007-5-3 loss: 0.424127  [  128/  179]
train() client id: f_00007-5-4 loss: 0.475016  [  160/  179]
train() client id: f_00007-6-0 loss: 0.423726  [   32/  179]
train() client id: f_00007-6-1 loss: 0.420848  [   64/  179]
train() client id: f_00007-6-2 loss: 0.376504  [   96/  179]
train() client id: f_00007-6-3 loss: 0.499374  [  128/  179]
train() client id: f_00007-6-4 loss: 0.519710  [  160/  179]
train() client id: f_00007-7-0 loss: 0.534654  [   32/  179]
train() client id: f_00007-7-1 loss: 0.361634  [   64/  179]
train() client id: f_00007-7-2 loss: 0.325333  [   96/  179]
train() client id: f_00007-7-3 loss: 0.444261  [  128/  179]
train() client id: f_00007-7-4 loss: 0.529342  [  160/  179]
train() client id: f_00007-8-0 loss: 0.581559  [   32/  179]
train() client id: f_00007-8-1 loss: 0.347914  [   64/  179]
train() client id: f_00007-8-2 loss: 0.401785  [   96/  179]
train() client id: f_00007-8-3 loss: 0.400393  [  128/  179]
train() client id: f_00007-8-4 loss: 0.374894  [  160/  179]
train() client id: f_00007-9-0 loss: 0.694640  [   32/  179]
train() client id: f_00007-9-1 loss: 0.400991  [   64/  179]
train() client id: f_00007-9-2 loss: 0.356883  [   96/  179]
train() client id: f_00007-9-3 loss: 0.305276  [  128/  179]
train() client id: f_00007-9-4 loss: 0.469719  [  160/  179]
train() client id: f_00007-10-0 loss: 0.652342  [   32/  179]
train() client id: f_00007-10-1 loss: 0.432408  [   64/  179]
train() client id: f_00007-10-2 loss: 0.383615  [   96/  179]
train() client id: f_00007-10-3 loss: 0.381504  [  128/  179]
train() client id: f_00007-10-4 loss: 0.311368  [  160/  179]
train() client id: f_00007-11-0 loss: 0.454046  [   32/  179]
train() client id: f_00007-11-1 loss: 0.422967  [   64/  179]
train() client id: f_00007-11-2 loss: 0.497822  [   96/  179]
train() client id: f_00007-11-3 loss: 0.376953  [  128/  179]
train() client id: f_00007-11-4 loss: 0.506434  [  160/  179]
train() client id: f_00008-0-0 loss: 0.777359  [   32/  130]
train() client id: f_00008-0-1 loss: 0.728244  [   64/  130]
train() client id: f_00008-0-2 loss: 0.868717  [   96/  130]
train() client id: f_00008-0-3 loss: 0.772260  [  128/  130]
train() client id: f_00008-1-0 loss: 0.813604  [   32/  130]
train() client id: f_00008-1-1 loss: 0.710675  [   64/  130]
train() client id: f_00008-1-2 loss: 0.786720  [   96/  130]
train() client id: f_00008-1-3 loss: 0.825088  [  128/  130]
train() client id: f_00008-2-0 loss: 0.845532  [   32/  130]
train() client id: f_00008-2-1 loss: 0.787442  [   64/  130]
train() client id: f_00008-2-2 loss: 0.776279  [   96/  130]
train() client id: f_00008-2-3 loss: 0.720023  [  128/  130]
train() client id: f_00008-3-0 loss: 0.730875  [   32/  130]
train() client id: f_00008-3-1 loss: 0.880761  [   64/  130]
train() client id: f_00008-3-2 loss: 0.801033  [   96/  130]
train() client id: f_00008-3-3 loss: 0.708233  [  128/  130]
train() client id: f_00008-4-0 loss: 0.727187  [   32/  130]
train() client id: f_00008-4-1 loss: 0.803322  [   64/  130]
train() client id: f_00008-4-2 loss: 0.888594  [   96/  130]
train() client id: f_00008-4-3 loss: 0.684684  [  128/  130]
train() client id: f_00008-5-0 loss: 0.632501  [   32/  130]
train() client id: f_00008-5-1 loss: 0.837711  [   64/  130]
train() client id: f_00008-5-2 loss: 0.809509  [   96/  130]
train() client id: f_00008-5-3 loss: 0.803098  [  128/  130]
train() client id: f_00008-6-0 loss: 0.762836  [   32/  130]
train() client id: f_00008-6-1 loss: 0.791854  [   64/  130]
train() client id: f_00008-6-2 loss: 0.742717  [   96/  130]
train() client id: f_00008-6-3 loss: 0.793284  [  128/  130]
train() client id: f_00008-7-0 loss: 0.762118  [   32/  130]
train() client id: f_00008-7-1 loss: 0.835204  [   64/  130]
train() client id: f_00008-7-2 loss: 0.715885  [   96/  130]
train() client id: f_00008-7-3 loss: 0.784538  [  128/  130]
train() client id: f_00008-8-0 loss: 0.758145  [   32/  130]
train() client id: f_00008-8-1 loss: 0.722282  [   64/  130]
train() client id: f_00008-8-2 loss: 0.764143  [   96/  130]
train() client id: f_00008-8-3 loss: 0.850941  [  128/  130]
train() client id: f_00008-9-0 loss: 0.772650  [   32/  130]
train() client id: f_00008-9-1 loss: 0.748204  [   64/  130]
train() client id: f_00008-9-2 loss: 0.807202  [   96/  130]
train() client id: f_00008-9-3 loss: 0.759201  [  128/  130]
train() client id: f_00008-10-0 loss: 0.726257  [   32/  130]
train() client id: f_00008-10-1 loss: 0.817892  [   64/  130]
train() client id: f_00008-10-2 loss: 0.721228  [   96/  130]
train() client id: f_00008-10-3 loss: 0.820261  [  128/  130]
train() client id: f_00008-11-0 loss: 0.710701  [   32/  130]
train() client id: f_00008-11-1 loss: 0.785293  [   64/  130]
train() client id: f_00008-11-2 loss: 0.719600  [   96/  130]
train() client id: f_00008-11-3 loss: 0.853652  [  128/  130]
train() client id: f_00009-0-0 loss: 1.189176  [   32/  118]
train() client id: f_00009-0-1 loss: 1.120564  [   64/  118]
train() client id: f_00009-0-2 loss: 0.981754  [   96/  118]
train() client id: f_00009-1-0 loss: 1.130846  [   32/  118]
train() client id: f_00009-1-1 loss: 0.948327  [   64/  118]
train() client id: f_00009-1-2 loss: 1.065133  [   96/  118]
train() client id: f_00009-2-0 loss: 1.102576  [   32/  118]
train() client id: f_00009-2-1 loss: 0.978504  [   64/  118]
train() client id: f_00009-2-2 loss: 0.957319  [   96/  118]
train() client id: f_00009-3-0 loss: 1.053181  [   32/  118]
train() client id: f_00009-3-1 loss: 1.021068  [   64/  118]
train() client id: f_00009-3-2 loss: 0.951994  [   96/  118]
train() client id: f_00009-4-0 loss: 1.092925  [   32/  118]
train() client id: f_00009-4-1 loss: 0.890634  [   64/  118]
train() client id: f_00009-4-2 loss: 0.920699  [   96/  118]
train() client id: f_00009-5-0 loss: 1.013026  [   32/  118]
train() client id: f_00009-5-1 loss: 0.921559  [   64/  118]
train() client id: f_00009-5-2 loss: 0.956379  [   96/  118]
train() client id: f_00009-6-0 loss: 1.063966  [   32/  118]
train() client id: f_00009-6-1 loss: 0.964713  [   64/  118]
train() client id: f_00009-6-2 loss: 0.897666  [   96/  118]
train() client id: f_00009-7-0 loss: 0.877698  [   32/  118]
train() client id: f_00009-7-1 loss: 1.071542  [   64/  118]
train() client id: f_00009-7-2 loss: 1.003703  [   96/  118]
train() client id: f_00009-8-0 loss: 0.932681  [   32/  118]
train() client id: f_00009-8-1 loss: 0.924084  [   64/  118]
train() client id: f_00009-8-2 loss: 0.967235  [   96/  118]
train() client id: f_00009-9-0 loss: 0.942405  [   32/  118]
train() client id: f_00009-9-1 loss: 1.043207  [   64/  118]
train() client id: f_00009-9-2 loss: 0.910685  [   96/  118]
train() client id: f_00009-10-0 loss: 0.881608  [   32/  118]
train() client id: f_00009-10-1 loss: 0.985430  [   64/  118]
train() client id: f_00009-10-2 loss: 0.907399  [   96/  118]
train() client id: f_00009-11-0 loss: 1.019769  [   32/  118]
train() client id: f_00009-11-1 loss: 0.854382  [   64/  118]
train() client id: f_00009-11-2 loss: 0.953570  [   96/  118]
At round 14 accuracy: 0.6312997347480106
At round 14 training accuracy: 0.5720992622401073
At round 14 training loss: 0.8719509209610526
update_location
xs = [ -3.9056584    4.20031788  90.00902392  18.81129433   0.97929623
   3.95640986 -52.44319194 -31.32485185  74.66397685 -17.06087855]
ys = [ 82.5879595   65.55583871   1.32061395 -52.45517586  44.35018685
  27.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [129.75370986 119.64618949 134.54875848 114.47886385 109.39788889
 103.8714598  112.94768224 104.79466875 126.02927998 101.52381707]
dists_bs = [194.45631632 209.97804258 316.85061448 298.82883869 219.17592563
 231.70902195 215.80270215 225.7884024  295.15524593 232.77727406]
uav_gains = [5.21358841e-11 6.38604963e-11 4.76097128e-11 7.13141531e-11
 7.98862224e-11 9.09403593e-11 7.37560435e-11 8.89506057e-11
 5.60762293e-11 9.62893323e-11]
bs_gains = [4.31114205e-11 3.47702020e-11 1.09875619e-11 1.29452686e-11
 3.08371410e-11 2.63908782e-11 3.22058545e-11 2.83745851e-11
 1.34014770e-11 2.60531636e-11]
Round 15
-------------------------------
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.01168963 18.78472696  8.87441189  3.17492049 21.66795492 10.44235678
  3.94614583 12.71610262  9.35523145  8.4772718 ]
obj_prev = 106.45081237176021
eta_min = 4.89584141592709e-11	eta_max = 0.9211440323876784
af = 22.501910530386866	bf = 1.761176745733534	zeta = 24.752101583425553	eta = 0.9090909090909091
af = 22.501910530386866	bf = 1.761176745733534	zeta = 42.96421672758091	eta = 0.5237360818902523
af = 22.501910530386866	bf = 1.761176745733534	zeta = 34.25554170777676	eta = 0.6568838035709252
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.69318907763576	eta = 0.6882751779568552
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.6157469047972	eta = 0.6899094046832094
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.61554299983585	eta = 0.6899137178399916
eta = 0.6899137178399916
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [0.0306296  0.06441941 0.03014342 0.01045296 0.07438618 0.03549145
 0.01312697 0.04351349 0.03160197 0.02868487]
ene_total = [2.80683503 5.36552507 2.7806876  1.27556609 6.12194985 3.25313134
 1.47109077 3.71222193 3.07937974 2.74915557]
ti_comp = [0.32219551 0.3157955  0.32085207 0.32647704 0.31367854 0.31076018
 0.32690731 0.32920522 0.2952455  0.31050951]
ti_coms = [0.07139244 0.07779246 0.07273588 0.06711092 0.07990942 0.08282778
 0.06668064 0.06438274 0.09834245 0.08307845]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.73007617e-05 1.67539923e-04 1.66283024e-05 6.69719050e-07
 2.61449152e-04 2.89334621e-05 1.32289255e-06 4.75136331e-05
 2.26285138e-05 1.52999065e-05]
ene_total = [0.5318468  0.59057441 0.54178073 0.49879221 0.61328584 0.61769428
 0.4956431  0.48199869 0.73252479 0.61854397]
optimize_network_iter = 0 obj = 5.7226848172315465
eta = 0.6899137178399916
freqs = [4.75326373e+07 1.01995448e+08 4.69740154e+07 1.60087223e+07
 1.18570726e+08 5.71042387e+07 2.00775158e+07 6.60886950e+07
 5.35181277e+07 4.61899994e+07]
eta_min = 0.6899137178400166	eta_max = 0.6899137178399891
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 0.03956004889123434	eta = 0.9090909090909091
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 19.410648956797782	eta = 0.0018527809600934662
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9911157923761436	eta = 0.018062074012930685
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9405838032428373	eta = 0.018532402852232132
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9405717992779243	eta = 0.01853251748973932
eta = 0.01853251748973932
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.85887615e-04 1.80012864e-03 1.78662392e-04 7.19578012e-06
 2.80913409e-03 3.10874884e-04 1.42137870e-05 5.10509081e-04
 2.43131519e-04 1.64389475e-04]
ene_total = [0.1719908  0.2249194  0.17497549 0.15774238 0.25358093 0.2017751
 0.15689688 0.1631543  0.23661227 0.19892425]
ti_comp = [0.32219551 0.3157955  0.32085207 0.32647704 0.31367854 0.31076018
 0.32690731 0.32920522 0.2952455  0.31050951]
ti_coms = [0.07139244 0.07779246 0.07273588 0.06711092 0.07990942 0.08282778
 0.06668064 0.06438274 0.09834245 0.08307845]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.73007617e-05 1.67539923e-04 1.66283024e-05 6.69719050e-07
 2.61449152e-04 2.89334621e-05 1.32289255e-06 4.75136331e-05
 2.26285138e-05 1.52999065e-05]
ene_total = [0.5318468  0.59057441 0.54178073 0.49879221 0.61328584 0.61769428
 0.4956431  0.48199869 0.73252479 0.61854397]
optimize_network_iter = 1 obj = 5.722684817232005
eta = 0.6899137178400166
freqs = [4.75326373e+07 1.01995448e+08 4.69740154e+07 1.60087223e+07
 1.18570726e+08 5.71042387e+07 2.00775158e+07 6.60886950e+07
 5.35181277e+07 4.61899994e+07]
Done!
At round 15 eta: 0.6899137178400166
At round 15 local rounds: 12.154605075633594
At round 15 global rounds: 74.31613869273092
At round 15 a_n: 22.701869304745298
gradient difference: 0.4515308439731598
train() client id: f_00000-0-0 loss: 1.225911  [   32/  126]
train() client id: f_00000-0-1 loss: 1.210751  [   64/  126]
train() client id: f_00000-0-2 loss: 1.273505  [   96/  126]
train() client id: f_00000-1-0 loss: 0.998327  [   32/  126]
train() client id: f_00000-1-1 loss: 1.271613  [   64/  126]
train() client id: f_00000-1-2 loss: 1.152838  [   96/  126]
train() client id: f_00000-2-0 loss: 0.990937  [   32/  126]
train() client id: f_00000-2-1 loss: 1.031484  [   64/  126]
train() client id: f_00000-2-2 loss: 0.958103  [   96/  126]
train() client id: f_00000-3-0 loss: 1.050020  [   32/  126]
train() client id: f_00000-3-1 loss: 0.938872  [   64/  126]
train() client id: f_00000-3-2 loss: 0.988090  [   96/  126]
train() client id: f_00000-4-0 loss: 0.964515  [   32/  126]
train() client id: f_00000-4-1 loss: 1.036967  [   64/  126]
train() client id: f_00000-4-2 loss: 0.937511  [   96/  126]
train() client id: f_00000-5-0 loss: 0.911527  [   32/  126]
train() client id: f_00000-5-1 loss: 0.898762  [   64/  126]
train() client id: f_00000-5-2 loss: 0.883473  [   96/  126]
train() client id: f_00000-6-0 loss: 0.898704  [   32/  126]
train() client id: f_00000-6-1 loss: 0.868068  [   64/  126]
train() client id: f_00000-6-2 loss: 0.924774  [   96/  126]
train() client id: f_00000-7-0 loss: 0.861673  [   32/  126]
train() client id: f_00000-7-1 loss: 0.805849  [   64/  126]
train() client id: f_00000-7-2 loss: 1.012958  [   96/  126]
train() client id: f_00000-8-0 loss: 1.026077  [   32/  126]
train() client id: f_00000-8-1 loss: 0.835413  [   64/  126]
train() client id: f_00000-8-2 loss: 0.862167  [   96/  126]
train() client id: f_00000-9-0 loss: 0.837425  [   32/  126]
train() client id: f_00000-9-1 loss: 0.863850  [   64/  126]
train() client id: f_00000-9-2 loss: 0.947945  [   96/  126]
train() client id: f_00000-10-0 loss: 0.828477  [   32/  126]
train() client id: f_00000-10-1 loss: 0.971415  [   64/  126]
train() client id: f_00000-10-2 loss: 0.873977  [   96/  126]
train() client id: f_00000-11-0 loss: 0.869628  [   32/  126]
train() client id: f_00000-11-1 loss: 0.817827  [   64/  126]
train() client id: f_00000-11-2 loss: 0.932210  [   96/  126]
train() client id: f_00001-0-0 loss: 0.498529  [   32/  265]
train() client id: f_00001-0-1 loss: 0.530849  [   64/  265]
train() client id: f_00001-0-2 loss: 0.620013  [   96/  265]
train() client id: f_00001-0-3 loss: 0.489618  [  128/  265]
train() client id: f_00001-0-4 loss: 0.578868  [  160/  265]
train() client id: f_00001-0-5 loss: 0.629354  [  192/  265]
train() client id: f_00001-0-6 loss: 0.456593  [  224/  265]
train() client id: f_00001-0-7 loss: 0.583438  [  256/  265]
train() client id: f_00001-1-0 loss: 0.492584  [   32/  265]
train() client id: f_00001-1-1 loss: 0.555628  [   64/  265]
train() client id: f_00001-1-2 loss: 0.511178  [   96/  265]
train() client id: f_00001-1-3 loss: 0.490039  [  128/  265]
train() client id: f_00001-1-4 loss: 0.645318  [  160/  265]
train() client id: f_00001-1-5 loss: 0.531433  [  192/  265]
train() client id: f_00001-1-6 loss: 0.500533  [  224/  265]
train() client id: f_00001-1-7 loss: 0.532443  [  256/  265]
train() client id: f_00001-2-0 loss: 0.497960  [   32/  265]
train() client id: f_00001-2-1 loss: 0.481357  [   64/  265]
train() client id: f_00001-2-2 loss: 0.575379  [   96/  265]
train() client id: f_00001-2-3 loss: 0.498980  [  128/  265]
train() client id: f_00001-2-4 loss: 0.600503  [  160/  265]
train() client id: f_00001-2-5 loss: 0.592365  [  192/  265]
train() client id: f_00001-2-6 loss: 0.518411  [  224/  265]
train() client id: f_00001-2-7 loss: 0.516740  [  256/  265]
train() client id: f_00001-3-0 loss: 0.534056  [   32/  265]
train() client id: f_00001-3-1 loss: 0.450888  [   64/  265]
train() client id: f_00001-3-2 loss: 0.537945  [   96/  265]
train() client id: f_00001-3-3 loss: 0.666889  [  128/  265]
train() client id: f_00001-3-4 loss: 0.521573  [  160/  265]
train() client id: f_00001-3-5 loss: 0.445075  [  192/  265]
train() client id: f_00001-3-6 loss: 0.487444  [  224/  265]
train() client id: f_00001-3-7 loss: 0.611548  [  256/  265]
train() client id: f_00001-4-0 loss: 0.466913  [   32/  265]
train() client id: f_00001-4-1 loss: 0.495505  [   64/  265]
train() client id: f_00001-4-2 loss: 0.608455  [   96/  265]
train() client id: f_00001-4-3 loss: 0.507047  [  128/  265]
train() client id: f_00001-4-4 loss: 0.550153  [  160/  265]
train() client id: f_00001-4-5 loss: 0.534418  [  192/  265]
train() client id: f_00001-4-6 loss: 0.545252  [  224/  265]
train() client id: f_00001-4-7 loss: 0.525785  [  256/  265]
train() client id: f_00001-5-0 loss: 0.524867  [   32/  265]
train() client id: f_00001-5-1 loss: 0.485120  [   64/  265]
train() client id: f_00001-5-2 loss: 0.511059  [   96/  265]
train() client id: f_00001-5-3 loss: 0.496328  [  128/  265]
train() client id: f_00001-5-4 loss: 0.516129  [  160/  265]
train() client id: f_00001-5-5 loss: 0.567998  [  192/  265]
train() client id: f_00001-5-6 loss: 0.474594  [  224/  265]
train() client id: f_00001-5-7 loss: 0.600227  [  256/  265]
train() client id: f_00001-6-0 loss: 0.503474  [   32/  265]
train() client id: f_00001-6-1 loss: 0.569923  [   64/  265]
train() client id: f_00001-6-2 loss: 0.643017  [   96/  265]
train() client id: f_00001-6-3 loss: 0.519951  [  128/  265]
train() client id: f_00001-6-4 loss: 0.468266  [  160/  265]
train() client id: f_00001-6-5 loss: 0.421456  [  192/  265]
train() client id: f_00001-6-6 loss: 0.529628  [  224/  265]
train() client id: f_00001-6-7 loss: 0.553433  [  256/  265]
train() client id: f_00001-7-0 loss: 0.609123  [   32/  265]
train() client id: f_00001-7-1 loss: 0.584522  [   64/  265]
train() client id: f_00001-7-2 loss: 0.498013  [   96/  265]
train() client id: f_00001-7-3 loss: 0.598884  [  128/  265]
train() client id: f_00001-7-4 loss: 0.501926  [  160/  265]
train() client id: f_00001-7-5 loss: 0.466324  [  192/  265]
train() client id: f_00001-7-6 loss: 0.488321  [  224/  265]
train() client id: f_00001-7-7 loss: 0.450192  [  256/  265]
train() client id: f_00001-8-0 loss: 0.647376  [   32/  265]
train() client id: f_00001-8-1 loss: 0.525228  [   64/  265]
train() client id: f_00001-8-2 loss: 0.537085  [   96/  265]
train() client id: f_00001-8-3 loss: 0.479763  [  128/  265]
train() client id: f_00001-8-4 loss: 0.422911  [  160/  265]
train() client id: f_00001-8-5 loss: 0.599965  [  192/  265]
train() client id: f_00001-8-6 loss: 0.553038  [  224/  265]
train() client id: f_00001-8-7 loss: 0.436327  [  256/  265]
train() client id: f_00001-9-0 loss: 0.624280  [   32/  265]
train() client id: f_00001-9-1 loss: 0.604236  [   64/  265]
train() client id: f_00001-9-2 loss: 0.693017  [   96/  265]
train() client id: f_00001-9-3 loss: 0.452739  [  128/  265]
train() client id: f_00001-9-4 loss: 0.486982  [  160/  265]
train() client id: f_00001-9-5 loss: 0.458067  [  192/  265]
train() client id: f_00001-9-6 loss: 0.447654  [  224/  265]
train() client id: f_00001-9-7 loss: 0.446388  [  256/  265]
train() client id: f_00001-10-0 loss: 0.508677  [   32/  265]
train() client id: f_00001-10-1 loss: 0.603653  [   64/  265]
train() client id: f_00001-10-2 loss: 0.442571  [   96/  265]
train() client id: f_00001-10-3 loss: 0.434214  [  128/  265]
train() client id: f_00001-10-4 loss: 0.693595  [  160/  265]
train() client id: f_00001-10-5 loss: 0.599588  [  192/  265]
train() client id: f_00001-10-6 loss: 0.434759  [  224/  265]
train() client id: f_00001-10-7 loss: 0.506310  [  256/  265]
train() client id: f_00001-11-0 loss: 0.643361  [   32/  265]
train() client id: f_00001-11-1 loss: 0.540598  [   64/  265]
train() client id: f_00001-11-2 loss: 0.533962  [   96/  265]
train() client id: f_00001-11-3 loss: 0.511107  [  128/  265]
train() client id: f_00001-11-4 loss: 0.434506  [  160/  265]
train() client id: f_00001-11-5 loss: 0.514019  [  192/  265]
train() client id: f_00001-11-6 loss: 0.542618  [  224/  265]
train() client id: f_00001-11-7 loss: 0.514413  [  256/  265]
train() client id: f_00002-0-0 loss: 1.171987  [   32/  124]
train() client id: f_00002-0-1 loss: 1.167842  [   64/  124]
train() client id: f_00002-0-2 loss: 1.257576  [   96/  124]
train() client id: f_00002-1-0 loss: 1.179826  [   32/  124]
train() client id: f_00002-1-1 loss: 1.093486  [   64/  124]
train() client id: f_00002-1-2 loss: 1.194845  [   96/  124]
train() client id: f_00002-2-0 loss: 1.067490  [   32/  124]
train() client id: f_00002-2-1 loss: 1.208055  [   64/  124]
train() client id: f_00002-2-2 loss: 1.123520  [   96/  124]
train() client id: f_00002-3-0 loss: 1.069111  [   32/  124]
train() client id: f_00002-3-1 loss: 1.090429  [   64/  124]
train() client id: f_00002-3-2 loss: 1.051680  [   96/  124]
train() client id: f_00002-4-0 loss: 1.072751  [   32/  124]
train() client id: f_00002-4-1 loss: 0.997669  [   64/  124]
train() client id: f_00002-4-2 loss: 1.160009  [   96/  124]
train() client id: f_00002-5-0 loss: 0.964723  [   32/  124]
train() client id: f_00002-5-1 loss: 1.048912  [   64/  124]
train() client id: f_00002-5-2 loss: 1.033176  [   96/  124]
train() client id: f_00002-6-0 loss: 0.955600  [   32/  124]
train() client id: f_00002-6-1 loss: 1.026481  [   64/  124]
train() client id: f_00002-6-2 loss: 1.077434  [   96/  124]
train() client id: f_00002-7-0 loss: 0.964277  [   32/  124]
train() client id: f_00002-7-1 loss: 1.058338  [   64/  124]
train() client id: f_00002-7-2 loss: 1.012152  [   96/  124]
train() client id: f_00002-8-0 loss: 0.901409  [   32/  124]
train() client id: f_00002-8-1 loss: 1.031543  [   64/  124]
train() client id: f_00002-8-2 loss: 1.060970  [   96/  124]
train() client id: f_00002-9-0 loss: 1.064933  [   32/  124]
train() client id: f_00002-9-1 loss: 1.005384  [   64/  124]
train() client id: f_00002-9-2 loss: 0.915858  [   96/  124]
train() client id: f_00002-10-0 loss: 0.966837  [   32/  124]
train() client id: f_00002-10-1 loss: 0.960174  [   64/  124]
train() client id: f_00002-10-2 loss: 0.858298  [   96/  124]
train() client id: f_00002-11-0 loss: 0.937461  [   32/  124]
train() client id: f_00002-11-1 loss: 1.002185  [   64/  124]
train() client id: f_00002-11-2 loss: 1.000777  [   96/  124]
train() client id: f_00003-0-0 loss: 0.904457  [   32/   43]
train() client id: f_00003-1-0 loss: 0.839077  [   32/   43]
train() client id: f_00003-2-0 loss: 0.879749  [   32/   43]
train() client id: f_00003-3-0 loss: 1.028590  [   32/   43]
train() client id: f_00003-4-0 loss: 0.738308  [   32/   43]
train() client id: f_00003-5-0 loss: 0.921573  [   32/   43]
train() client id: f_00003-6-0 loss: 0.907300  [   32/   43]
train() client id: f_00003-7-0 loss: 0.894016  [   32/   43]
train() client id: f_00003-8-0 loss: 0.934519  [   32/   43]
train() client id: f_00003-9-0 loss: 0.834713  [   32/   43]
train() client id: f_00003-10-0 loss: 0.772954  [   32/   43]
train() client id: f_00003-11-0 loss: 0.923650  [   32/   43]
train() client id: f_00004-0-0 loss: 0.753557  [   32/  306]
train() client id: f_00004-0-1 loss: 0.739663  [   64/  306]
train() client id: f_00004-0-2 loss: 0.779777  [   96/  306]
train() client id: f_00004-0-3 loss: 0.922573  [  128/  306]
train() client id: f_00004-0-4 loss: 0.864154  [  160/  306]
train() client id: f_00004-0-5 loss: 0.827836  [  192/  306]
train() client id: f_00004-0-6 loss: 0.836800  [  224/  306]
train() client id: f_00004-0-7 loss: 0.760732  [  256/  306]
train() client id: f_00004-0-8 loss: 0.953325  [  288/  306]
train() client id: f_00004-1-0 loss: 0.844832  [   32/  306]
train() client id: f_00004-1-1 loss: 0.876213  [   64/  306]
train() client id: f_00004-1-2 loss: 0.905159  [   96/  306]
train() client id: f_00004-1-3 loss: 0.877863  [  128/  306]
train() client id: f_00004-1-4 loss: 0.792844  [  160/  306]
train() client id: f_00004-1-5 loss: 0.725444  [  192/  306]
train() client id: f_00004-1-6 loss: 0.779609  [  224/  306]
train() client id: f_00004-1-7 loss: 0.859559  [  256/  306]
train() client id: f_00004-1-8 loss: 0.777722  [  288/  306]
train() client id: f_00004-2-0 loss: 0.889478  [   32/  306]
train() client id: f_00004-2-1 loss: 0.745891  [   64/  306]
train() client id: f_00004-2-2 loss: 0.955032  [   96/  306]
train() client id: f_00004-2-3 loss: 0.779473  [  128/  306]
train() client id: f_00004-2-4 loss: 0.812198  [  160/  306]
train() client id: f_00004-2-5 loss: 0.930942  [  192/  306]
train() client id: f_00004-2-6 loss: 0.843255  [  224/  306]
train() client id: f_00004-2-7 loss: 0.749671  [  256/  306]
train() client id: f_00004-2-8 loss: 0.828380  [  288/  306]
train() client id: f_00004-3-0 loss: 0.832048  [   32/  306]
train() client id: f_00004-3-1 loss: 0.855525  [   64/  306]
train() client id: f_00004-3-2 loss: 0.804980  [   96/  306]
train() client id: f_00004-3-3 loss: 0.759965  [  128/  306]
train() client id: f_00004-3-4 loss: 0.799477  [  160/  306]
train() client id: f_00004-3-5 loss: 0.795653  [  192/  306]
train() client id: f_00004-3-6 loss: 0.875057  [  224/  306]
train() client id: f_00004-3-7 loss: 1.016098  [  256/  306]
train() client id: f_00004-3-8 loss: 0.707974  [  288/  306]
train() client id: f_00004-4-0 loss: 0.765967  [   32/  306]
train() client id: f_00004-4-1 loss: 0.733736  [   64/  306]
train() client id: f_00004-4-2 loss: 0.878514  [   96/  306]
train() client id: f_00004-4-3 loss: 0.816280  [  128/  306]
train() client id: f_00004-4-4 loss: 0.771401  [  160/  306]
train() client id: f_00004-4-5 loss: 0.862674  [  192/  306]
train() client id: f_00004-4-6 loss: 0.827833  [  224/  306]
train() client id: f_00004-4-7 loss: 0.927215  [  256/  306]
train() client id: f_00004-4-8 loss: 0.855218  [  288/  306]
train() client id: f_00004-5-0 loss: 0.832501  [   32/  306]
train() client id: f_00004-5-1 loss: 0.782028  [   64/  306]
train() client id: f_00004-5-2 loss: 0.765272  [   96/  306]
train() client id: f_00004-5-3 loss: 0.921726  [  128/  306]
train() client id: f_00004-5-4 loss: 0.740064  [  160/  306]
train() client id: f_00004-5-5 loss: 0.788050  [  192/  306]
train() client id: f_00004-5-6 loss: 0.953803  [  224/  306]
train() client id: f_00004-5-7 loss: 0.854712  [  256/  306]
train() client id: f_00004-5-8 loss: 0.853794  [  288/  306]
train() client id: f_00004-6-0 loss: 0.709327  [   32/  306]
train() client id: f_00004-6-1 loss: 0.867290  [   64/  306]
train() client id: f_00004-6-2 loss: 0.918504  [   96/  306]
train() client id: f_00004-6-3 loss: 0.878186  [  128/  306]
train() client id: f_00004-6-4 loss: 0.980268  [  160/  306]
train() client id: f_00004-6-5 loss: 0.732329  [  192/  306]
train() client id: f_00004-6-6 loss: 0.873245  [  224/  306]
train() client id: f_00004-6-7 loss: 0.782269  [  256/  306]
train() client id: f_00004-6-8 loss: 0.791844  [  288/  306]
train() client id: f_00004-7-0 loss: 0.816297  [   32/  306]
train() client id: f_00004-7-1 loss: 0.947014  [   64/  306]
train() client id: f_00004-7-2 loss: 0.794922  [   96/  306]
train() client id: f_00004-7-3 loss: 0.916465  [  128/  306]
train() client id: f_00004-7-4 loss: 0.796465  [  160/  306]
train() client id: f_00004-7-5 loss: 0.795033  [  192/  306]
train() client id: f_00004-7-6 loss: 0.782278  [  224/  306]
train() client id: f_00004-7-7 loss: 0.864265  [  256/  306]
train() client id: f_00004-7-8 loss: 0.841452  [  288/  306]
train() client id: f_00004-8-0 loss: 0.878728  [   32/  306]
train() client id: f_00004-8-1 loss: 0.837758  [   64/  306]
train() client id: f_00004-8-2 loss: 0.889653  [   96/  306]
train() client id: f_00004-8-3 loss: 0.847144  [  128/  306]
train() client id: f_00004-8-4 loss: 0.690519  [  160/  306]
train() client id: f_00004-8-5 loss: 0.839438  [  192/  306]
train() client id: f_00004-8-6 loss: 0.832507  [  224/  306]
train() client id: f_00004-8-7 loss: 0.831312  [  256/  306]
train() client id: f_00004-8-8 loss: 0.831273  [  288/  306]
train() client id: f_00004-9-0 loss: 0.822235  [   32/  306]
train() client id: f_00004-9-1 loss: 0.865190  [   64/  306]
train() client id: f_00004-9-2 loss: 0.782913  [   96/  306]
train() client id: f_00004-9-3 loss: 0.754362  [  128/  306]
train() client id: f_00004-9-4 loss: 0.847514  [  160/  306]
train() client id: f_00004-9-5 loss: 0.821271  [  192/  306]
train() client id: f_00004-9-6 loss: 0.724613  [  224/  306]
train() client id: f_00004-9-7 loss: 0.920583  [  256/  306]
train() client id: f_00004-9-8 loss: 0.948721  [  288/  306]
train() client id: f_00004-10-0 loss: 0.799158  [   32/  306]
train() client id: f_00004-10-1 loss: 0.829145  [   64/  306]
train() client id: f_00004-10-2 loss: 0.923015  [   96/  306]
train() client id: f_00004-10-3 loss: 0.736826  [  128/  306]
train() client id: f_00004-10-4 loss: 0.849613  [  160/  306]
train() client id: f_00004-10-5 loss: 0.800352  [  192/  306]
train() client id: f_00004-10-6 loss: 0.854669  [  224/  306]
train() client id: f_00004-10-7 loss: 0.830911  [  256/  306]
train() client id: f_00004-10-8 loss: 0.818187  [  288/  306]
train() client id: f_00004-11-0 loss: 0.918985  [   32/  306]
train() client id: f_00004-11-1 loss: 0.933022  [   64/  306]
train() client id: f_00004-11-2 loss: 0.738127  [   96/  306]
train() client id: f_00004-11-3 loss: 0.760954  [  128/  306]
train() client id: f_00004-11-4 loss: 0.876705  [  160/  306]
train() client id: f_00004-11-5 loss: 0.774668  [  192/  306]
train() client id: f_00004-11-6 loss: 0.756990  [  224/  306]
train() client id: f_00004-11-7 loss: 0.884391  [  256/  306]
train() client id: f_00004-11-8 loss: 0.835334  [  288/  306]
train() client id: f_00005-0-0 loss: 0.767359  [   32/  146]
train() client id: f_00005-0-1 loss: 0.793116  [   64/  146]
train() client id: f_00005-0-2 loss: 0.725552  [   96/  146]
train() client id: f_00005-0-3 loss: 0.853819  [  128/  146]
train() client id: f_00005-1-0 loss: 0.885567  [   32/  146]
train() client id: f_00005-1-1 loss: 0.834376  [   64/  146]
train() client id: f_00005-1-2 loss: 0.855379  [   96/  146]
train() client id: f_00005-1-3 loss: 0.786479  [  128/  146]
train() client id: f_00005-2-0 loss: 0.851799  [   32/  146]
train() client id: f_00005-2-1 loss: 0.855409  [   64/  146]
train() client id: f_00005-2-2 loss: 0.862357  [   96/  146]
train() client id: f_00005-2-3 loss: 0.832903  [  128/  146]
train() client id: f_00005-3-0 loss: 0.915522  [   32/  146]
train() client id: f_00005-3-1 loss: 0.737437  [   64/  146]
train() client id: f_00005-3-2 loss: 0.725219  [   96/  146]
train() client id: f_00005-3-3 loss: 0.935402  [  128/  146]
train() client id: f_00005-4-0 loss: 0.851383  [   32/  146]
train() client id: f_00005-4-1 loss: 0.906889  [   64/  146]
train() client id: f_00005-4-2 loss: 0.842870  [   96/  146]
train() client id: f_00005-4-3 loss: 0.684101  [  128/  146]
train() client id: f_00005-5-0 loss: 0.774036  [   32/  146]
train() client id: f_00005-5-1 loss: 0.753145  [   64/  146]
train() client id: f_00005-5-2 loss: 0.750542  [   96/  146]
train() client id: f_00005-5-3 loss: 1.068194  [  128/  146]
train() client id: f_00005-6-0 loss: 0.720900  [   32/  146]
train() client id: f_00005-6-1 loss: 0.808195  [   64/  146]
train() client id: f_00005-6-2 loss: 0.958754  [   96/  146]
train() client id: f_00005-6-3 loss: 0.904846  [  128/  146]
train() client id: f_00005-7-0 loss: 0.870194  [   32/  146]
train() client id: f_00005-7-1 loss: 0.704918  [   64/  146]
train() client id: f_00005-7-2 loss: 0.812000  [   96/  146]
train() client id: f_00005-7-3 loss: 0.939296  [  128/  146]
train() client id: f_00005-8-0 loss: 0.866886  [   32/  146]
train() client id: f_00005-8-1 loss: 0.936416  [   64/  146]
train() client id: f_00005-8-2 loss: 0.744527  [   96/  146]
train() client id: f_00005-8-3 loss: 0.791544  [  128/  146]
train() client id: f_00005-9-0 loss: 0.683229  [   32/  146]
train() client id: f_00005-9-1 loss: 0.710627  [   64/  146]
train() client id: f_00005-9-2 loss: 1.025792  [   96/  146]
train() client id: f_00005-9-3 loss: 0.996452  [  128/  146]
train() client id: f_00005-10-0 loss: 0.786298  [   32/  146]
train() client id: f_00005-10-1 loss: 0.812899  [   64/  146]
train() client id: f_00005-10-2 loss: 0.822766  [   96/  146]
train() client id: f_00005-10-3 loss: 0.939057  [  128/  146]
train() client id: f_00005-11-0 loss: 0.762663  [   32/  146]
train() client id: f_00005-11-1 loss: 0.773372  [   64/  146]
train() client id: f_00005-11-2 loss: 0.805107  [   96/  146]
train() client id: f_00005-11-3 loss: 1.051008  [  128/  146]
train() client id: f_00006-0-0 loss: 0.660234  [   32/   54]
train() client id: f_00006-1-0 loss: 0.656244  [   32/   54]
train() client id: f_00006-2-0 loss: 0.671196  [   32/   54]
train() client id: f_00006-3-0 loss: 0.582307  [   32/   54]
train() client id: f_00006-4-0 loss: 0.619551  [   32/   54]
train() client id: f_00006-5-0 loss: 0.616081  [   32/   54]
train() client id: f_00006-6-0 loss: 0.622421  [   32/   54]
train() client id: f_00006-7-0 loss: 0.659741  [   32/   54]
train() client id: f_00006-8-0 loss: 0.611052  [   32/   54]
train() client id: f_00006-9-0 loss: 0.617252  [   32/   54]
train() client id: f_00006-10-0 loss: 0.659624  [   32/   54]
train() client id: f_00006-11-0 loss: 0.660824  [   32/   54]
train() client id: f_00007-0-0 loss: 0.624415  [   32/  179]
train() client id: f_00007-0-1 loss: 0.674216  [   64/  179]
train() client id: f_00007-0-2 loss: 0.638054  [   96/  179]
train() client id: f_00007-0-3 loss: 0.662056  [  128/  179]
train() client id: f_00007-0-4 loss: 0.544200  [  160/  179]
train() client id: f_00007-1-0 loss: 0.785738  [   32/  179]
train() client id: f_00007-1-1 loss: 0.733304  [   64/  179]
train() client id: f_00007-1-2 loss: 0.494530  [   96/  179]
train() client id: f_00007-1-3 loss: 0.598229  [  128/  179]
train() client id: f_00007-1-4 loss: 0.502396  [  160/  179]
train() client id: f_00007-2-0 loss: 0.630576  [   32/  179]
train() client id: f_00007-2-1 loss: 0.447356  [   64/  179]
train() client id: f_00007-2-2 loss: 0.572787  [   96/  179]
train() client id: f_00007-2-3 loss: 0.553679  [  128/  179]
train() client id: f_00007-2-4 loss: 0.696650  [  160/  179]
train() client id: f_00007-3-0 loss: 0.616846  [   32/  179]
train() client id: f_00007-3-1 loss: 0.640943  [   64/  179]
train() client id: f_00007-3-2 loss: 0.536258  [   96/  179]
train() client id: f_00007-3-3 loss: 0.464451  [  128/  179]
train() client id: f_00007-3-4 loss: 0.619659  [  160/  179]
train() client id: f_00007-4-0 loss: 0.454832  [   32/  179]
train() client id: f_00007-4-1 loss: 0.647466  [   64/  179]
train() client id: f_00007-4-2 loss: 0.667738  [   96/  179]
train() client id: f_00007-4-3 loss: 0.594407  [  128/  179]
train() client id: f_00007-4-4 loss: 0.601534  [  160/  179]
train() client id: f_00007-5-0 loss: 0.505421  [   32/  179]
train() client id: f_00007-5-1 loss: 0.539222  [   64/  179]
train() client id: f_00007-5-2 loss: 0.529489  [   96/  179]
train() client id: f_00007-5-3 loss: 0.467846  [  128/  179]
train() client id: f_00007-5-4 loss: 0.628591  [  160/  179]
train() client id: f_00007-6-0 loss: 0.517950  [   32/  179]
train() client id: f_00007-6-1 loss: 0.737707  [   64/  179]
train() client id: f_00007-6-2 loss: 0.444531  [   96/  179]
train() client id: f_00007-6-3 loss: 0.535159  [  128/  179]
train() client id: f_00007-6-4 loss: 0.685698  [  160/  179]
train() client id: f_00007-7-0 loss: 0.635079  [   32/  179]
train() client id: f_00007-7-1 loss: 0.448236  [   64/  179]
train() client id: f_00007-7-2 loss: 0.615036  [   96/  179]
train() client id: f_00007-7-3 loss: 0.506446  [  128/  179]
train() client id: f_00007-7-4 loss: 0.609333  [  160/  179]
train() client id: f_00007-8-0 loss: 0.612756  [   32/  179]
train() client id: f_00007-8-1 loss: 0.426473  [   64/  179]
train() client id: f_00007-8-2 loss: 0.528419  [   96/  179]
train() client id: f_00007-8-3 loss: 0.535901  [  128/  179]
train() client id: f_00007-8-4 loss: 0.522662  [  160/  179]
train() client id: f_00007-9-0 loss: 0.497961  [   32/  179]
train() client id: f_00007-9-1 loss: 0.685689  [   64/  179]
train() client id: f_00007-9-2 loss: 0.617021  [   96/  179]
train() client id: f_00007-9-3 loss: 0.439572  [  128/  179]
train() client id: f_00007-9-4 loss: 0.636685  [  160/  179]
train() client id: f_00007-10-0 loss: 0.485903  [   32/  179]
train() client id: f_00007-10-1 loss: 0.416023  [   64/  179]
train() client id: f_00007-10-2 loss: 0.631235  [   96/  179]
train() client id: f_00007-10-3 loss: 0.751427  [  128/  179]
train() client id: f_00007-10-4 loss: 0.503492  [  160/  179]
train() client id: f_00007-11-0 loss: 0.438166  [   32/  179]
train() client id: f_00007-11-1 loss: 0.762539  [   64/  179]
train() client id: f_00007-11-2 loss: 0.566546  [   96/  179]
train() client id: f_00007-11-3 loss: 0.524869  [  128/  179]
train() client id: f_00007-11-4 loss: 0.424610  [  160/  179]
train() client id: f_00008-0-0 loss: 0.714801  [   32/  130]
train() client id: f_00008-0-1 loss: 0.815112  [   64/  130]
train() client id: f_00008-0-2 loss: 0.774050  [   96/  130]
train() client id: f_00008-0-3 loss: 0.810122  [  128/  130]
train() client id: f_00008-1-0 loss: 0.714148  [   32/  130]
train() client id: f_00008-1-1 loss: 0.770206  [   64/  130]
train() client id: f_00008-1-2 loss: 0.842102  [   96/  130]
train() client id: f_00008-1-3 loss: 0.764254  [  128/  130]
train() client id: f_00008-2-0 loss: 0.813150  [   32/  130]
train() client id: f_00008-2-1 loss: 0.817362  [   64/  130]
train() client id: f_00008-2-2 loss: 0.712042  [   96/  130]
train() client id: f_00008-2-3 loss: 0.750968  [  128/  130]
train() client id: f_00008-3-0 loss: 0.758278  [   32/  130]
train() client id: f_00008-3-1 loss: 0.741733  [   64/  130]
train() client id: f_00008-3-2 loss: 0.879570  [   96/  130]
train() client id: f_00008-3-3 loss: 0.718764  [  128/  130]
train() client id: f_00008-4-0 loss: 0.749174  [   32/  130]
train() client id: f_00008-4-1 loss: 0.778319  [   64/  130]
train() client id: f_00008-4-2 loss: 0.829720  [   96/  130]
train() client id: f_00008-4-3 loss: 0.734316  [  128/  130]
train() client id: f_00008-5-0 loss: 0.800044  [   32/  130]
train() client id: f_00008-5-1 loss: 0.812988  [   64/  130]
train() client id: f_00008-5-2 loss: 0.718599  [   96/  130]
train() client id: f_00008-5-3 loss: 0.743549  [  128/  130]
train() client id: f_00008-6-0 loss: 0.764950  [   32/  130]
train() client id: f_00008-6-1 loss: 0.761157  [   64/  130]
train() client id: f_00008-6-2 loss: 0.826936  [   96/  130]
train() client id: f_00008-6-3 loss: 0.742979  [  128/  130]
train() client id: f_00008-7-0 loss: 0.714484  [   32/  130]
train() client id: f_00008-7-1 loss: 0.697865  [   64/  130]
train() client id: f_00008-7-2 loss: 0.829257  [   96/  130]
train() client id: f_00008-7-3 loss: 0.812720  [  128/  130]
train() client id: f_00008-8-0 loss: 0.822310  [   32/  130]
train() client id: f_00008-8-1 loss: 0.705685  [   64/  130]
train() client id: f_00008-8-2 loss: 0.757196  [   96/  130]
train() client id: f_00008-8-3 loss: 0.784344  [  128/  130]
train() client id: f_00008-9-0 loss: 0.801146  [   32/  130]
train() client id: f_00008-9-1 loss: 0.706498  [   64/  130]
train() client id: f_00008-9-2 loss: 0.800837  [   96/  130]
train() client id: f_00008-9-3 loss: 0.783473  [  128/  130]
train() client id: f_00008-10-0 loss: 0.800833  [   32/  130]
train() client id: f_00008-10-1 loss: 0.708186  [   64/  130]
train() client id: f_00008-10-2 loss: 0.787117  [   96/  130]
train() client id: f_00008-10-3 loss: 0.790342  [  128/  130]
train() client id: f_00008-11-0 loss: 0.772185  [   32/  130]
train() client id: f_00008-11-1 loss: 0.645454  [   64/  130]
train() client id: f_00008-11-2 loss: 0.836356  [   96/  130]
train() client id: f_00008-11-3 loss: 0.836715  [  128/  130]
train() client id: f_00009-0-0 loss: 1.125050  [   32/  118]
train() client id: f_00009-0-1 loss: 1.089176  [   64/  118]
train() client id: f_00009-0-2 loss: 1.070719  [   96/  118]
train() client id: f_00009-1-0 loss: 1.166464  [   32/  118]
train() client id: f_00009-1-1 loss: 1.032843  [   64/  118]
train() client id: f_00009-1-2 loss: 1.121634  [   96/  118]
train() client id: f_00009-2-0 loss: 1.076321  [   32/  118]
train() client id: f_00009-2-1 loss: 1.015443  [   64/  118]
train() client id: f_00009-2-2 loss: 1.005366  [   96/  118]
train() client id: f_00009-3-0 loss: 0.964311  [   32/  118]
train() client id: f_00009-3-1 loss: 0.962804  [   64/  118]
train() client id: f_00009-3-2 loss: 1.083120  [   96/  118]
train() client id: f_00009-4-0 loss: 0.955913  [   32/  118]
train() client id: f_00009-4-1 loss: 0.946830  [   64/  118]
train() client id: f_00009-4-2 loss: 0.955800  [   96/  118]
train() client id: f_00009-5-0 loss: 1.003573  [   32/  118]
train() client id: f_00009-5-1 loss: 0.908445  [   64/  118]
train() client id: f_00009-5-2 loss: 0.898861  [   96/  118]
train() client id: f_00009-6-0 loss: 0.931355  [   32/  118]
train() client id: f_00009-6-1 loss: 0.935598  [   64/  118]
train() client id: f_00009-6-2 loss: 0.963636  [   96/  118]
train() client id: f_00009-7-0 loss: 0.968062  [   32/  118]
train() client id: f_00009-7-1 loss: 0.946804  [   64/  118]
train() client id: f_00009-7-2 loss: 0.941133  [   96/  118]
train() client id: f_00009-8-0 loss: 0.986059  [   32/  118]
train() client id: f_00009-8-1 loss: 0.904331  [   64/  118]
train() client id: f_00009-8-2 loss: 0.841685  [   96/  118]
train() client id: f_00009-9-0 loss: 0.874685  [   32/  118]
train() client id: f_00009-9-1 loss: 0.834326  [   64/  118]
train() client id: f_00009-9-2 loss: 0.868993  [   96/  118]
train() client id: f_00009-10-0 loss: 0.932470  [   32/  118]
train() client id: f_00009-10-1 loss: 1.101595  [   64/  118]
train() client id: f_00009-10-2 loss: 0.776673  [   96/  118]
train() client id: f_00009-11-0 loss: 0.972141  [   32/  118]
train() client id: f_00009-11-1 loss: 0.745659  [   64/  118]
train() client id: f_00009-11-2 loss: 0.894454  [   96/  118]
At round 15 accuracy: 0.6339522546419099
At round 15 training accuracy: 0.5774647887323944
At round 15 training loss: 0.8482254635508484
update_location
xs = [ -3.9056584    4.20031788  95.00902392  18.81129433   0.97929623
   3.95640986 -57.44319194 -36.32485185  79.66397685 -22.06087855]
ys = [ 87.5879595   70.55583871   1.32061395 -57.45517586  49.35018685
  32.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [132.99212314 122.45721312 137.94367926 116.85444804 111.51860815
 105.32056622 115.35428402 106.39629278 129.05432647 102.48265326]
dists_bs = [192.13052478 207.41585464 321.04423704 302.65198924 216.2327178
 228.56555375 213.0029065  222.6401832  299.39649124 229.41418462]
uav_gains = [4.90168661e-11 6.02574305e-11 4.47298682e-11 6.77442080e-11
 7.61419638e-11 8.78443206e-11 6.99686398e-11 8.56406368e-11
 5.28456671e-11 9.40528225e-11]
bs_gains = [4.45886427e-11 3.59862526e-11 1.05904015e-11 1.24925816e-11
 3.20268441e-11 2.74197759e-11 3.34052405e-11 2.95123738e-11
 1.28766623e-11 2.71367191e-11]
Round 16
-------------------------------
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.87981483 18.50415528  8.7446806   3.12925602 21.34431125 10.28544805
  3.88903844 12.52812053  9.21866598  8.34945337]
obj_prev = 104.87294435893298
eta_min = 3.443571724151839e-11	eta_max = 0.9213403601232922
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 24.38417167306138	eta = 0.9090909090909091
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 42.374357946194124	eta = 0.5231330896350052
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 33.76652681787571	eta = 0.6564912320788918
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.22189998010197	eta = 0.6879615667412922
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.14522467977385	eta = 0.6896025463975112
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.14502216089842	eta = 0.6896068910058766
eta = 0.6896068910058766
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [0.03066631 0.06449661 0.03017954 0.01046549 0.07447533 0.03553398
 0.0131427  0.04356563 0.03163984 0.02871924]
ene_total = [2.771699   5.28190218 2.74625142 1.26115748 6.02653747 3.19933593
 1.45388682 3.66054961 3.04133893 2.70236334]
ti_comp = [0.32693877 0.3220322  0.32555044 0.33146051 0.32000855 0.31714645
 0.33188162 0.33440318 0.29980737 0.31694807]
ti_coms = [0.07229963 0.0772062  0.07368796 0.06777789 0.07922985 0.08209195
 0.06735678 0.06483522 0.09943103 0.08229033]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.68628817e-05 1.61693297e-04 1.62099429e-05 6.52070823e-07
 2.52112341e-04 2.78799409e-05 1.28815427e-06 4.62137600e-05
 2.20241139e-05 1.47374657e-05]
ene_total = [0.53002626 0.57650519 0.54013261 0.49576897 0.59791914 0.60245213
 0.49273557 0.47757895 0.72884031 0.60294189]
optimize_network_iter = 0 obj = 5.644901027509804
eta = 0.6896068910058766
freqs = [4.68991664e+07 1.00139996e+08 4.63515646e+07 1.57869290e+07
 1.16364588e+08 5.60214049e+07 1.98002903e+07 6.51393823e+07
 5.27669565e+07 4.53059144e+07]
eta_min = 0.6896068910059152	eta_max = 0.6896068910058611
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 0.03760750116688597	eta = 0.909090909090909
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 19.169603857786115	eta = 0.0017834816868453646
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9593134418751286	eta = 0.01744929458133157
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9111928987478155	eta = 0.017888637743914634
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9111822247051171	eta = 0.01788873765279858
eta = 0.01788873765279858
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.82570215e-04 1.75061300e-03 1.75501009e-04 7.05980817e-06
 2.72955744e-03 3.01849167e-04 1.39465251e-05 5.00344854e-04
 2.38449589e-04 1.59558866e-04]
ene_total = [0.17134348 0.21893109 0.17438925 0.15683431 0.24623749 0.19673596
 0.1560201  0.1614347  0.23535039 0.19390545]
ti_comp = [0.32693877 0.3220322  0.32555044 0.33146051 0.32000855 0.31714645
 0.33188162 0.33440318 0.29980737 0.31694807]
ti_coms = [0.07229963 0.0772062  0.07368796 0.06777789 0.07922985 0.08209195
 0.06735678 0.06483522 0.09943103 0.08229033]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.68628817e-05 1.61693297e-04 1.62099429e-05 6.52070823e-07
 2.52112341e-04 2.78799409e-05 1.28815427e-06 4.62137600e-05
 2.20241139e-05 1.47374657e-05]
ene_total = [0.53002626 0.57650519 0.54013261 0.49576897 0.59791914 0.60245213
 0.49273557 0.47757895 0.72884031 0.60294189]
optimize_network_iter = 1 obj = 5.644901027510502
eta = 0.6896068910059152
freqs = [4.68991664e+07 1.00139996e+08 4.63515646e+07 1.57869290e+07
 1.16364588e+08 5.60214049e+07 1.98002903e+07 6.51393823e+07
 5.27669565e+07 4.53059144e+07]
Done!
At round 16 eta: 0.6896068910059152
At round 16 local rounds: 12.169171106060379
At round 16 global rounds: 73.13908926108901
At round 16 a_n: 22.359323457775982
gradient difference: 0.48396340012550354
train() client id: f_00000-0-0 loss: 1.286356  [   32/  126]
train() client id: f_00000-0-1 loss: 1.291101  [   64/  126]
train() client id: f_00000-0-2 loss: 1.221493  [   96/  126]
train() client id: f_00000-1-0 loss: 1.137265  [   32/  126]
train() client id: f_00000-1-1 loss: 1.190621  [   64/  126]
train() client id: f_00000-1-2 loss: 1.063158  [   96/  126]
train() client id: f_00000-2-0 loss: 1.096692  [   32/  126]
train() client id: f_00000-2-1 loss: 1.172675  [   64/  126]
train() client id: f_00000-2-2 loss: 1.027548  [   96/  126]
train() client id: f_00000-3-0 loss: 1.024416  [   32/  126]
train() client id: f_00000-3-1 loss: 1.070833  [   64/  126]
train() client id: f_00000-3-2 loss: 0.949800  [   96/  126]
train() client id: f_00000-4-0 loss: 0.959577  [   32/  126]
train() client id: f_00000-4-1 loss: 0.964702  [   64/  126]
train() client id: f_00000-4-2 loss: 1.027287  [   96/  126]
train() client id: f_00000-5-0 loss: 0.950891  [   32/  126]
train() client id: f_00000-5-1 loss: 0.961261  [   64/  126]
train() client id: f_00000-5-2 loss: 0.905395  [   96/  126]
train() client id: f_00000-6-0 loss: 0.848152  [   32/  126]
train() client id: f_00000-6-1 loss: 0.932801  [   64/  126]
train() client id: f_00000-6-2 loss: 0.866843  [   96/  126]
train() client id: f_00000-7-0 loss: 0.821890  [   32/  126]
train() client id: f_00000-7-1 loss: 0.820414  [   64/  126]
train() client id: f_00000-7-2 loss: 0.860952  [   96/  126]
train() client id: f_00000-8-0 loss: 0.867440  [   32/  126]
train() client id: f_00000-8-1 loss: 0.820456  [   64/  126]
train() client id: f_00000-8-2 loss: 0.803356  [   96/  126]
train() client id: f_00000-9-0 loss: 0.848210  [   32/  126]
train() client id: f_00000-9-1 loss: 0.778769  [   64/  126]
train() client id: f_00000-9-2 loss: 0.882287  [   96/  126]
train() client id: f_00000-10-0 loss: 0.869056  [   32/  126]
train() client id: f_00000-10-1 loss: 0.827247  [   64/  126]
train() client id: f_00000-10-2 loss: 0.867004  [   96/  126]
train() client id: f_00000-11-0 loss: 0.897845  [   32/  126]
train() client id: f_00000-11-1 loss: 0.846328  [   64/  126]
train() client id: f_00000-11-2 loss: 0.868643  [   96/  126]
train() client id: f_00001-0-0 loss: 0.582197  [   32/  265]
train() client id: f_00001-0-1 loss: 0.501487  [   64/  265]
train() client id: f_00001-0-2 loss: 0.517847  [   96/  265]
train() client id: f_00001-0-3 loss: 0.498724  [  128/  265]
train() client id: f_00001-0-4 loss: 0.490065  [  160/  265]
train() client id: f_00001-0-5 loss: 0.515484  [  192/  265]
train() client id: f_00001-0-6 loss: 0.490746  [  224/  265]
train() client id: f_00001-0-7 loss: 0.431462  [  256/  265]
train() client id: f_00001-1-0 loss: 0.550122  [   32/  265]
train() client id: f_00001-1-1 loss: 0.506361  [   64/  265]
train() client id: f_00001-1-2 loss: 0.460037  [   96/  265]
train() client id: f_00001-1-3 loss: 0.426610  [  128/  265]
train() client id: f_00001-1-4 loss: 0.536702  [  160/  265]
train() client id: f_00001-1-5 loss: 0.467387  [  192/  265]
train() client id: f_00001-1-6 loss: 0.463093  [  224/  265]
train() client id: f_00001-1-7 loss: 0.503011  [  256/  265]
train() client id: f_00001-2-0 loss: 0.485582  [   32/  265]
train() client id: f_00001-2-1 loss: 0.465110  [   64/  265]
train() client id: f_00001-2-2 loss: 0.581541  [   96/  265]
train() client id: f_00001-2-3 loss: 0.414399  [  128/  265]
train() client id: f_00001-2-4 loss: 0.470194  [  160/  265]
train() client id: f_00001-2-5 loss: 0.481499  [  192/  265]
train() client id: f_00001-2-6 loss: 0.478907  [  224/  265]
train() client id: f_00001-2-7 loss: 0.474000  [  256/  265]
train() client id: f_00001-3-0 loss: 0.432653  [   32/  265]
train() client id: f_00001-3-1 loss: 0.471339  [   64/  265]
train() client id: f_00001-3-2 loss: 0.510560  [   96/  265]
train() client id: f_00001-3-3 loss: 0.430994  [  128/  265]
train() client id: f_00001-3-4 loss: 0.507051  [  160/  265]
train() client id: f_00001-3-5 loss: 0.492258  [  192/  265]
train() client id: f_00001-3-6 loss: 0.552957  [  224/  265]
train() client id: f_00001-3-7 loss: 0.457397  [  256/  265]
train() client id: f_00001-4-0 loss: 0.506211  [   32/  265]
train() client id: f_00001-4-1 loss: 0.436682  [   64/  265]
train() client id: f_00001-4-2 loss: 0.477787  [   96/  265]
train() client id: f_00001-4-3 loss: 0.426935  [  128/  265]
train() client id: f_00001-4-4 loss: 0.422043  [  160/  265]
train() client id: f_00001-4-5 loss: 0.495546  [  192/  265]
train() client id: f_00001-4-6 loss: 0.564512  [  224/  265]
train() client id: f_00001-4-7 loss: 0.478875  [  256/  265]
train() client id: f_00001-5-0 loss: 0.555192  [   32/  265]
train() client id: f_00001-5-1 loss: 0.436427  [   64/  265]
train() client id: f_00001-5-2 loss: 0.592254  [   96/  265]
train() client id: f_00001-5-3 loss: 0.396796  [  128/  265]
train() client id: f_00001-5-4 loss: 0.456566  [  160/  265]
train() client id: f_00001-5-5 loss: 0.366316  [  192/  265]
train() client id: f_00001-5-6 loss: 0.543027  [  224/  265]
train() client id: f_00001-5-7 loss: 0.451923  [  256/  265]
train() client id: f_00001-6-0 loss: 0.454352  [   32/  265]
train() client id: f_00001-6-1 loss: 0.461361  [   64/  265]
train() client id: f_00001-6-2 loss: 0.446904  [   96/  265]
train() client id: f_00001-6-3 loss: 0.558299  [  128/  265]
train() client id: f_00001-6-4 loss: 0.518631  [  160/  265]
train() client id: f_00001-6-5 loss: 0.468663  [  192/  265]
train() client id: f_00001-6-6 loss: 0.378138  [  224/  265]
train() client id: f_00001-6-7 loss: 0.499702  [  256/  265]
train() client id: f_00001-7-0 loss: 0.456874  [   32/  265]
train() client id: f_00001-7-1 loss: 0.539945  [   64/  265]
train() client id: f_00001-7-2 loss: 0.392893  [   96/  265]
train() client id: f_00001-7-3 loss: 0.466559  [  128/  265]
train() client id: f_00001-7-4 loss: 0.456767  [  160/  265]
train() client id: f_00001-7-5 loss: 0.467645  [  192/  265]
train() client id: f_00001-7-6 loss: 0.443642  [  224/  265]
train() client id: f_00001-7-7 loss: 0.497627  [  256/  265]
train() client id: f_00001-8-0 loss: 0.437287  [   32/  265]
train() client id: f_00001-8-1 loss: 0.530359  [   64/  265]
train() client id: f_00001-8-2 loss: 0.438148  [   96/  265]
train() client id: f_00001-8-3 loss: 0.477783  [  128/  265]
train() client id: f_00001-8-4 loss: 0.484632  [  160/  265]
train() client id: f_00001-8-5 loss: 0.582594  [  192/  265]
train() client id: f_00001-8-6 loss: 0.446351  [  224/  265]
train() client id: f_00001-8-7 loss: 0.374266  [  256/  265]
train() client id: f_00001-9-0 loss: 0.481256  [   32/  265]
train() client id: f_00001-9-1 loss: 0.498454  [   64/  265]
train() client id: f_00001-9-2 loss: 0.547527  [   96/  265]
train() client id: f_00001-9-3 loss: 0.375159  [  128/  265]
train() client id: f_00001-9-4 loss: 0.469518  [  160/  265]
train() client id: f_00001-9-5 loss: 0.416042  [  192/  265]
train() client id: f_00001-9-6 loss: 0.488394  [  224/  265]
train() client id: f_00001-9-7 loss: 0.488701  [  256/  265]
train() client id: f_00001-10-0 loss: 0.397322  [   32/  265]
train() client id: f_00001-10-1 loss: 0.383946  [   64/  265]
train() client id: f_00001-10-2 loss: 0.433042  [   96/  265]
train() client id: f_00001-10-3 loss: 0.545675  [  128/  265]
train() client id: f_00001-10-4 loss: 0.564416  [  160/  265]
train() client id: f_00001-10-5 loss: 0.373018  [  192/  265]
train() client id: f_00001-10-6 loss: 0.436143  [  224/  265]
train() client id: f_00001-10-7 loss: 0.537249  [  256/  265]
train() client id: f_00001-11-0 loss: 0.488190  [   32/  265]
train() client id: f_00001-11-1 loss: 0.364632  [   64/  265]
train() client id: f_00001-11-2 loss: 0.472448  [   96/  265]
train() client id: f_00001-11-3 loss: 0.437186  [  128/  265]
train() client id: f_00001-11-4 loss: 0.530742  [  160/  265]
train() client id: f_00001-11-5 loss: 0.497036  [  192/  265]
train() client id: f_00001-11-6 loss: 0.423627  [  224/  265]
train() client id: f_00001-11-7 loss: 0.552084  [  256/  265]
train() client id: f_00002-0-0 loss: 1.196058  [   32/  124]
train() client id: f_00002-0-1 loss: 1.131442  [   64/  124]
train() client id: f_00002-0-2 loss: 0.994240  [   96/  124]
train() client id: f_00002-1-0 loss: 1.102087  [   32/  124]
train() client id: f_00002-1-1 loss: 1.085662  [   64/  124]
train() client id: f_00002-1-2 loss: 1.115480  [   96/  124]
train() client id: f_00002-2-0 loss: 1.037067  [   32/  124]
train() client id: f_00002-2-1 loss: 1.160817  [   64/  124]
train() client id: f_00002-2-2 loss: 1.030433  [   96/  124]
train() client id: f_00002-3-0 loss: 1.116458  [   32/  124]
train() client id: f_00002-3-1 loss: 1.159148  [   64/  124]
train() client id: f_00002-3-2 loss: 0.944375  [   96/  124]
train() client id: f_00002-4-0 loss: 0.992053  [   32/  124]
train() client id: f_00002-4-1 loss: 1.010778  [   64/  124]
train() client id: f_00002-4-2 loss: 1.024747  [   96/  124]
train() client id: f_00002-5-0 loss: 1.212123  [   32/  124]
train() client id: f_00002-5-1 loss: 0.934049  [   64/  124]
train() client id: f_00002-5-2 loss: 0.873355  [   96/  124]
train() client id: f_00002-6-0 loss: 0.948264  [   32/  124]
train() client id: f_00002-6-1 loss: 0.996565  [   64/  124]
train() client id: f_00002-6-2 loss: 1.056525  [   96/  124]
train() client id: f_00002-7-0 loss: 1.148664  [   32/  124]
train() client id: f_00002-7-1 loss: 0.931437  [   64/  124]
train() client id: f_00002-7-2 loss: 0.919656  [   96/  124]
train() client id: f_00002-8-0 loss: 1.062465  [   32/  124]
train() client id: f_00002-8-1 loss: 0.899487  [   64/  124]
train() client id: f_00002-8-2 loss: 1.023965  [   96/  124]
train() client id: f_00002-9-0 loss: 1.027770  [   32/  124]
train() client id: f_00002-9-1 loss: 0.989336  [   64/  124]
train() client id: f_00002-9-2 loss: 0.970230  [   96/  124]
train() client id: f_00002-10-0 loss: 0.881349  [   32/  124]
train() client id: f_00002-10-1 loss: 1.013112  [   64/  124]
train() client id: f_00002-10-2 loss: 1.013382  [   96/  124]
train() client id: f_00002-11-0 loss: 0.884042  [   32/  124]
train() client id: f_00002-11-1 loss: 0.881093  [   64/  124]
train() client id: f_00002-11-2 loss: 1.089272  [   96/  124]
train() client id: f_00003-0-0 loss: 0.643264  [   32/   43]
train() client id: f_00003-1-0 loss: 0.842265  [   32/   43]
train() client id: f_00003-2-0 loss: 0.732346  [   32/   43]
train() client id: f_00003-3-0 loss: 0.863723  [   32/   43]
train() client id: f_00003-4-0 loss: 0.725538  [   32/   43]
train() client id: f_00003-5-0 loss: 0.804935  [   32/   43]
train() client id: f_00003-6-0 loss: 0.731524  [   32/   43]
train() client id: f_00003-7-0 loss: 0.717988  [   32/   43]
train() client id: f_00003-8-0 loss: 0.874811  [   32/   43]
train() client id: f_00003-9-0 loss: 0.798761  [   32/   43]
train() client id: f_00003-10-0 loss: 0.637932  [   32/   43]
train() client id: f_00003-11-0 loss: 0.811238  [   32/   43]
train() client id: f_00004-0-0 loss: 0.824471  [   32/  306]
train() client id: f_00004-0-1 loss: 1.065835  [   64/  306]
train() client id: f_00004-0-2 loss: 0.899770  [   96/  306]
train() client id: f_00004-0-3 loss: 0.855874  [  128/  306]
train() client id: f_00004-0-4 loss: 0.894949  [  160/  306]
train() client id: f_00004-0-5 loss: 0.850929  [  192/  306]
train() client id: f_00004-0-6 loss: 0.790470  [  224/  306]
train() client id: f_00004-0-7 loss: 0.955324  [  256/  306]
train() client id: f_00004-0-8 loss: 1.006196  [  288/  306]
train() client id: f_00004-1-0 loss: 0.902701  [   32/  306]
train() client id: f_00004-1-1 loss: 0.950640  [   64/  306]
train() client id: f_00004-1-2 loss: 0.919237  [   96/  306]
train() client id: f_00004-1-3 loss: 0.979897  [  128/  306]
train() client id: f_00004-1-4 loss: 0.831579  [  160/  306]
train() client id: f_00004-1-5 loss: 0.820575  [  192/  306]
train() client id: f_00004-1-6 loss: 0.721087  [  224/  306]
train() client id: f_00004-1-7 loss: 1.016567  [  256/  306]
train() client id: f_00004-1-8 loss: 0.895735  [  288/  306]
train() client id: f_00004-2-0 loss: 0.881175  [   32/  306]
train() client id: f_00004-2-1 loss: 0.869510  [   64/  306]
train() client id: f_00004-2-2 loss: 0.938741  [   96/  306]
train() client id: f_00004-2-3 loss: 0.834787  [  128/  306]
train() client id: f_00004-2-4 loss: 0.829972  [  160/  306]
train() client id: f_00004-2-5 loss: 0.941861  [  192/  306]
train() client id: f_00004-2-6 loss: 0.937696  [  224/  306]
train() client id: f_00004-2-7 loss: 0.924777  [  256/  306]
train() client id: f_00004-2-8 loss: 0.939747  [  288/  306]
train() client id: f_00004-3-0 loss: 0.833380  [   32/  306]
train() client id: f_00004-3-1 loss: 0.934003  [   64/  306]
train() client id: f_00004-3-2 loss: 0.962523  [   96/  306]
train() client id: f_00004-3-3 loss: 0.741369  [  128/  306]
train() client id: f_00004-3-4 loss: 0.950247  [  160/  306]
train() client id: f_00004-3-5 loss: 0.928126  [  192/  306]
train() client id: f_00004-3-6 loss: 0.838737  [  224/  306]
train() client id: f_00004-3-7 loss: 0.962620  [  256/  306]
train() client id: f_00004-3-8 loss: 0.839422  [  288/  306]
train() client id: f_00004-4-0 loss: 0.888544  [   32/  306]
train() client id: f_00004-4-1 loss: 0.886379  [   64/  306]
train() client id: f_00004-4-2 loss: 0.875551  [   96/  306]
train() client id: f_00004-4-3 loss: 0.751956  [  128/  306]
train() client id: f_00004-4-4 loss: 0.921739  [  160/  306]
train() client id: f_00004-4-5 loss: 0.950562  [  192/  306]
train() client id: f_00004-4-6 loss: 0.919407  [  224/  306]
train() client id: f_00004-4-7 loss: 0.971852  [  256/  306]
train() client id: f_00004-4-8 loss: 0.844396  [  288/  306]
train() client id: f_00004-5-0 loss: 0.796086  [   32/  306]
train() client id: f_00004-5-1 loss: 0.919161  [   64/  306]
train() client id: f_00004-5-2 loss: 0.875047  [   96/  306]
train() client id: f_00004-5-3 loss: 0.882061  [  128/  306]
train() client id: f_00004-5-4 loss: 0.903672  [  160/  306]
train() client id: f_00004-5-5 loss: 0.894538  [  192/  306]
train() client id: f_00004-5-6 loss: 0.906176  [  224/  306]
train() client id: f_00004-5-7 loss: 0.810627  [  256/  306]
train() client id: f_00004-5-8 loss: 0.937645  [  288/  306]
train() client id: f_00004-6-0 loss: 0.834217  [   32/  306]
train() client id: f_00004-6-1 loss: 0.984134  [   64/  306]
train() client id: f_00004-6-2 loss: 0.882107  [   96/  306]
train() client id: f_00004-6-3 loss: 0.915697  [  128/  306]
train() client id: f_00004-6-4 loss: 0.788140  [  160/  306]
train() client id: f_00004-6-5 loss: 0.959574  [  192/  306]
train() client id: f_00004-6-6 loss: 0.858012  [  224/  306]
train() client id: f_00004-6-7 loss: 0.877164  [  256/  306]
train() client id: f_00004-6-8 loss: 0.921651  [  288/  306]
train() client id: f_00004-7-0 loss: 0.924371  [   32/  306]
train() client id: f_00004-7-1 loss: 0.834793  [   64/  306]
train() client id: f_00004-7-2 loss: 0.906569  [   96/  306]
train() client id: f_00004-7-3 loss: 0.821739  [  128/  306]
train() client id: f_00004-7-4 loss: 0.889515  [  160/  306]
train() client id: f_00004-7-5 loss: 0.931139  [  192/  306]
train() client id: f_00004-7-6 loss: 0.954278  [  224/  306]
train() client id: f_00004-7-7 loss: 0.892073  [  256/  306]
train() client id: f_00004-7-8 loss: 0.861833  [  288/  306]
train() client id: f_00004-8-0 loss: 1.028141  [   32/  306]
train() client id: f_00004-8-1 loss: 0.980010  [   64/  306]
train() client id: f_00004-8-2 loss: 0.833610  [   96/  306]
train() client id: f_00004-8-3 loss: 0.831317  [  128/  306]
train() client id: f_00004-8-4 loss: 0.826236  [  160/  306]
train() client id: f_00004-8-5 loss: 0.879751  [  192/  306]
train() client id: f_00004-8-6 loss: 0.955435  [  224/  306]
train() client id: f_00004-8-7 loss: 0.798526  [  256/  306]
train() client id: f_00004-8-8 loss: 0.856296  [  288/  306]
train() client id: f_00004-9-0 loss: 0.906410  [   32/  306]
train() client id: f_00004-9-1 loss: 0.902603  [   64/  306]
train() client id: f_00004-9-2 loss: 0.900974  [   96/  306]
train() client id: f_00004-9-3 loss: 0.850613  [  128/  306]
train() client id: f_00004-9-4 loss: 0.833207  [  160/  306]
train() client id: f_00004-9-5 loss: 0.932253  [  192/  306]
train() client id: f_00004-9-6 loss: 0.864417  [  224/  306]
train() client id: f_00004-9-7 loss: 0.889235  [  256/  306]
train() client id: f_00004-9-8 loss: 0.890025  [  288/  306]
train() client id: f_00004-10-0 loss: 0.890083  [   32/  306]
train() client id: f_00004-10-1 loss: 0.910276  [   64/  306]
train() client id: f_00004-10-2 loss: 0.857728  [   96/  306]
train() client id: f_00004-10-3 loss: 0.845070  [  128/  306]
train() client id: f_00004-10-4 loss: 0.932677  [  160/  306]
train() client id: f_00004-10-5 loss: 0.919322  [  192/  306]
train() client id: f_00004-10-6 loss: 0.832084  [  224/  306]
train() client id: f_00004-10-7 loss: 0.844471  [  256/  306]
train() client id: f_00004-10-8 loss: 0.935092  [  288/  306]
train() client id: f_00004-11-0 loss: 1.020489  [   32/  306]
train() client id: f_00004-11-1 loss: 0.832205  [   64/  306]
train() client id: f_00004-11-2 loss: 0.847180  [   96/  306]
train() client id: f_00004-11-3 loss: 0.836500  [  128/  306]
train() client id: f_00004-11-4 loss: 0.919655  [  160/  306]
train() client id: f_00004-11-5 loss: 0.848511  [  192/  306]
train() client id: f_00004-11-6 loss: 1.053392  [  224/  306]
train() client id: f_00004-11-7 loss: 0.765579  [  256/  306]
train() client id: f_00004-11-8 loss: 0.796608  [  288/  306]
train() client id: f_00005-0-0 loss: 0.457772  [   32/  146]
train() client id: f_00005-0-1 loss: 0.471043  [   64/  146]
train() client id: f_00005-0-2 loss: 0.521233  [   96/  146]
train() client id: f_00005-0-3 loss: 0.797133  [  128/  146]
train() client id: f_00005-1-0 loss: 0.461345  [   32/  146]
train() client id: f_00005-1-1 loss: 0.634063  [   64/  146]
train() client id: f_00005-1-2 loss: 0.538177  [   96/  146]
train() client id: f_00005-1-3 loss: 0.477221  [  128/  146]
train() client id: f_00005-2-0 loss: 0.694606  [   32/  146]
train() client id: f_00005-2-1 loss: 0.490029  [   64/  146]
train() client id: f_00005-2-2 loss: 0.648554  [   96/  146]
train() client id: f_00005-2-3 loss: 0.395774  [  128/  146]
train() client id: f_00005-3-0 loss: 0.479480  [   32/  146]
train() client id: f_00005-3-1 loss: 0.704042  [   64/  146]
train() client id: f_00005-3-2 loss: 0.448219  [   96/  146]
train() client id: f_00005-3-3 loss: 0.569854  [  128/  146]
train() client id: f_00005-4-0 loss: 0.624229  [   32/  146]
train() client id: f_00005-4-1 loss: 0.561171  [   64/  146]
train() client id: f_00005-4-2 loss: 0.570309  [   96/  146]
train() client id: f_00005-4-3 loss: 0.491678  [  128/  146]
train() client id: f_00005-5-0 loss: 0.576411  [   32/  146]
train() client id: f_00005-5-1 loss: 0.655191  [   64/  146]
train() client id: f_00005-5-2 loss: 0.297357  [   96/  146]
train() client id: f_00005-5-3 loss: 0.591320  [  128/  146]
train() client id: f_00005-6-0 loss: 0.506264  [   32/  146]
train() client id: f_00005-6-1 loss: 0.638611  [   64/  146]
train() client id: f_00005-6-2 loss: 0.490736  [   96/  146]
train() client id: f_00005-6-3 loss: 0.645705  [  128/  146]
train() client id: f_00005-7-0 loss: 0.542107  [   32/  146]
train() client id: f_00005-7-1 loss: 0.616662  [   64/  146]
train() client id: f_00005-7-2 loss: 0.499213  [   96/  146]
train() client id: f_00005-7-3 loss: 0.501282  [  128/  146]
train() client id: f_00005-8-0 loss: 0.517773  [   32/  146]
train() client id: f_00005-8-1 loss: 0.567437  [   64/  146]
train() client id: f_00005-8-2 loss: 0.415399  [   96/  146]
train() client id: f_00005-8-3 loss: 0.508961  [  128/  146]
train() client id: f_00005-9-0 loss: 0.800916  [   32/  146]
train() client id: f_00005-9-1 loss: 0.384618  [   64/  146]
train() client id: f_00005-9-2 loss: 0.461325  [   96/  146]
train() client id: f_00005-9-3 loss: 0.382348  [  128/  146]
train() client id: f_00005-10-0 loss: 0.431991  [   32/  146]
train() client id: f_00005-10-1 loss: 0.538631  [   64/  146]
train() client id: f_00005-10-2 loss: 0.509273  [   96/  146]
train() client id: f_00005-10-3 loss: 0.487072  [  128/  146]
train() client id: f_00005-11-0 loss: 0.344497  [   32/  146]
train() client id: f_00005-11-1 loss: 0.765328  [   64/  146]
train() client id: f_00005-11-2 loss: 0.344373  [   96/  146]
train() client id: f_00005-11-3 loss: 0.666755  [  128/  146]
train() client id: f_00006-0-0 loss: 0.540349  [   32/   54]
train() client id: f_00006-1-0 loss: 0.630107  [   32/   54]
train() client id: f_00006-2-0 loss: 0.544273  [   32/   54]
train() client id: f_00006-3-0 loss: 0.539609  [   32/   54]
train() client id: f_00006-4-0 loss: 0.523900  [   32/   54]
train() client id: f_00006-5-0 loss: 0.575922  [   32/   54]
train() client id: f_00006-6-0 loss: 0.624601  [   32/   54]
train() client id: f_00006-7-0 loss: 0.614236  [   32/   54]
train() client id: f_00006-8-0 loss: 0.579111  [   32/   54]
train() client id: f_00006-9-0 loss: 0.572038  [   32/   54]
train() client id: f_00006-10-0 loss: 0.569772  [   32/   54]
train() client id: f_00006-11-0 loss: 0.622563  [   32/   54]
train() client id: f_00007-0-0 loss: 0.830347  [   32/  179]
train() client id: f_00007-0-1 loss: 0.705700  [   64/  179]
train() client id: f_00007-0-2 loss: 0.659373  [   96/  179]
train() client id: f_00007-0-3 loss: 0.944363  [  128/  179]
train() client id: f_00007-0-4 loss: 0.809366  [  160/  179]
train() client id: f_00007-1-0 loss: 0.708919  [   32/  179]
train() client id: f_00007-1-1 loss: 0.803306  [   64/  179]
train() client id: f_00007-1-2 loss: 0.712385  [   96/  179]
train() client id: f_00007-1-3 loss: 0.839587  [  128/  179]
train() client id: f_00007-1-4 loss: 0.815415  [  160/  179]
train() client id: f_00007-2-0 loss: 1.024065  [   32/  179]
train() client id: f_00007-2-1 loss: 0.638899  [   64/  179]
train() client id: f_00007-2-2 loss: 0.717990  [   96/  179]
train() client id: f_00007-2-3 loss: 0.672965  [  128/  179]
train() client id: f_00007-2-4 loss: 0.780363  [  160/  179]
train() client id: f_00007-3-0 loss: 0.834820  [   32/  179]
train() client id: f_00007-3-1 loss: 0.642098  [   64/  179]
train() client id: f_00007-3-2 loss: 0.702007  [   96/  179]
train() client id: f_00007-3-3 loss: 0.858642  [  128/  179]
train() client id: f_00007-3-4 loss: 0.705903  [  160/  179]
train() client id: f_00007-4-0 loss: 0.867739  [   32/  179]
train() client id: f_00007-4-1 loss: 0.773457  [   64/  179]
train() client id: f_00007-4-2 loss: 0.696147  [   96/  179]
train() client id: f_00007-4-3 loss: 0.687759  [  128/  179]
train() client id: f_00007-4-4 loss: 0.703674  [  160/  179]
train() client id: f_00007-5-0 loss: 0.720875  [   32/  179]
train() client id: f_00007-5-1 loss: 0.667292  [   64/  179]
train() client id: f_00007-5-2 loss: 0.801101  [   96/  179]
train() client id: f_00007-5-3 loss: 0.641307  [  128/  179]
train() client id: f_00007-5-4 loss: 0.773393  [  160/  179]
train() client id: f_00007-6-0 loss: 0.693098  [   32/  179]
train() client id: f_00007-6-1 loss: 0.814403  [   64/  179]
train() client id: f_00007-6-2 loss: 0.697376  [   96/  179]
train() client id: f_00007-6-3 loss: 0.709931  [  128/  179]
train() client id: f_00007-6-4 loss: 0.690243  [  160/  179]
train() client id: f_00007-7-0 loss: 0.576812  [   32/  179]
train() client id: f_00007-7-1 loss: 0.676843  [   64/  179]
train() client id: f_00007-7-2 loss: 0.814162  [   96/  179]
train() client id: f_00007-7-3 loss: 0.856291  [  128/  179]
train() client id: f_00007-7-4 loss: 0.752540  [  160/  179]
train() client id: f_00007-8-0 loss: 0.771227  [   32/  179]
train() client id: f_00007-8-1 loss: 0.780507  [   64/  179]
train() client id: f_00007-8-2 loss: 0.599952  [   96/  179]
train() client id: f_00007-8-3 loss: 0.665780  [  128/  179]
train() client id: f_00007-8-4 loss: 0.803023  [  160/  179]
train() client id: f_00007-9-0 loss: 0.777582  [   32/  179]
train() client id: f_00007-9-1 loss: 0.601718  [   64/  179]
train() client id: f_00007-9-2 loss: 0.669099  [   96/  179]
train() client id: f_00007-9-3 loss: 0.611855  [  128/  179]
train() client id: f_00007-9-4 loss: 0.943845  [  160/  179]
train() client id: f_00007-10-0 loss: 0.682489  [   32/  179]
train() client id: f_00007-10-1 loss: 0.749800  [   64/  179]
train() client id: f_00007-10-2 loss: 0.706379  [   96/  179]
train() client id: f_00007-10-3 loss: 0.681535  [  128/  179]
train() client id: f_00007-10-4 loss: 0.866386  [  160/  179]
train() client id: f_00007-11-0 loss: 0.762217  [   32/  179]
train() client id: f_00007-11-1 loss: 0.786414  [   64/  179]
train() client id: f_00007-11-2 loss: 0.680198  [   96/  179]
train() client id: f_00007-11-3 loss: 0.743290  [  128/  179]
train() client id: f_00007-11-4 loss: 0.714201  [  160/  179]
train() client id: f_00008-0-0 loss: 0.750916  [   32/  130]
train() client id: f_00008-0-1 loss: 0.670755  [   64/  130]
train() client id: f_00008-0-2 loss: 0.794900  [   96/  130]
train() client id: f_00008-0-3 loss: 0.741584  [  128/  130]
train() client id: f_00008-1-0 loss: 0.729940  [   32/  130]
train() client id: f_00008-1-1 loss: 0.778474  [   64/  130]
train() client id: f_00008-1-2 loss: 0.703875  [   96/  130]
train() client id: f_00008-1-3 loss: 0.731323  [  128/  130]
train() client id: f_00008-2-0 loss: 0.680847  [   32/  130]
train() client id: f_00008-2-1 loss: 0.792861  [   64/  130]
train() client id: f_00008-2-2 loss: 0.681304  [   96/  130]
train() client id: f_00008-2-3 loss: 0.778659  [  128/  130]
train() client id: f_00008-3-0 loss: 0.717674  [   32/  130]
train() client id: f_00008-3-1 loss: 0.883265  [   64/  130]
train() client id: f_00008-3-2 loss: 0.685177  [   96/  130]
train() client id: f_00008-3-3 loss: 0.633845  [  128/  130]
train() client id: f_00008-4-0 loss: 0.745406  [   32/  130]
train() client id: f_00008-4-1 loss: 0.701451  [   64/  130]
train() client id: f_00008-4-2 loss: 0.726071  [   96/  130]
train() client id: f_00008-4-3 loss: 0.745821  [  128/  130]
train() client id: f_00008-5-0 loss: 0.760918  [   32/  130]
train() client id: f_00008-5-1 loss: 0.817424  [   64/  130]
train() client id: f_00008-5-2 loss: 0.721488  [   96/  130]
train() client id: f_00008-5-3 loss: 0.615211  [  128/  130]
train() client id: f_00008-6-0 loss: 0.770792  [   32/  130]
train() client id: f_00008-6-1 loss: 0.673269  [   64/  130]
train() client id: f_00008-6-2 loss: 0.808490  [   96/  130]
train() client id: f_00008-6-3 loss: 0.667734  [  128/  130]
train() client id: f_00008-7-0 loss: 0.761880  [   32/  130]
train() client id: f_00008-7-1 loss: 0.711659  [   64/  130]
train() client id: f_00008-7-2 loss: 0.729864  [   96/  130]
train() client id: f_00008-7-3 loss: 0.728526  [  128/  130]
train() client id: f_00008-8-0 loss: 0.719720  [   32/  130]
train() client id: f_00008-8-1 loss: 0.680745  [   64/  130]
train() client id: f_00008-8-2 loss: 0.793537  [   96/  130]
train() client id: f_00008-8-3 loss: 0.704715  [  128/  130]
train() client id: f_00008-9-0 loss: 0.831491  [   32/  130]
train() client id: f_00008-9-1 loss: 0.592640  [   64/  130]
train() client id: f_00008-9-2 loss: 0.750414  [   96/  130]
train() client id: f_00008-9-3 loss: 0.718468  [  128/  130]
train() client id: f_00008-10-0 loss: 0.795146  [   32/  130]
train() client id: f_00008-10-1 loss: 0.676633  [   64/  130]
train() client id: f_00008-10-2 loss: 0.765421  [   96/  130]
train() client id: f_00008-10-3 loss: 0.678000  [  128/  130]
train() client id: f_00008-11-0 loss: 0.647601  [   32/  130]
train() client id: f_00008-11-1 loss: 0.800913  [   64/  130]
train() client id: f_00008-11-2 loss: 0.774536  [   96/  130]
train() client id: f_00008-11-3 loss: 0.665933  [  128/  130]
train() client id: f_00009-0-0 loss: 0.999658  [   32/  118]
train() client id: f_00009-0-1 loss: 1.207072  [   64/  118]
train() client id: f_00009-0-2 loss: 0.998063  [   96/  118]
train() client id: f_00009-1-0 loss: 1.056220  [   32/  118]
train() client id: f_00009-1-1 loss: 1.072914  [   64/  118]
train() client id: f_00009-1-2 loss: 0.994023  [   96/  118]
train() client id: f_00009-2-0 loss: 1.034433  [   32/  118]
train() client id: f_00009-2-1 loss: 1.030831  [   64/  118]
train() client id: f_00009-2-2 loss: 0.932445  [   96/  118]
train() client id: f_00009-3-0 loss: 1.028930  [   32/  118]
train() client id: f_00009-3-1 loss: 0.879274  [   64/  118]
train() client id: f_00009-3-2 loss: 1.044194  [   96/  118]
train() client id: f_00009-4-0 loss: 0.911915  [   32/  118]
train() client id: f_00009-4-1 loss: 0.975178  [   64/  118]
train() client id: f_00009-4-2 loss: 0.938364  [   96/  118]
train() client id: f_00009-5-0 loss: 0.965040  [   32/  118]
train() client id: f_00009-5-1 loss: 0.960389  [   64/  118]
train() client id: f_00009-5-2 loss: 0.935058  [   96/  118]
train() client id: f_00009-6-0 loss: 0.930509  [   32/  118]
train() client id: f_00009-6-1 loss: 1.002912  [   64/  118]
train() client id: f_00009-6-2 loss: 0.856078  [   96/  118]
train() client id: f_00009-7-0 loss: 0.806748  [   32/  118]
train() client id: f_00009-7-1 loss: 0.905001  [   64/  118]
train() client id: f_00009-7-2 loss: 0.982318  [   96/  118]
train() client id: f_00009-8-0 loss: 0.888673  [   32/  118]
train() client id: f_00009-8-1 loss: 0.849357  [   64/  118]
train() client id: f_00009-8-2 loss: 0.965009  [   96/  118]
train() client id: f_00009-9-0 loss: 0.934404  [   32/  118]
train() client id: f_00009-9-1 loss: 0.845160  [   64/  118]
train() client id: f_00009-9-2 loss: 0.975249  [   96/  118]
train() client id: f_00009-10-0 loss: 0.953922  [   32/  118]
train() client id: f_00009-10-1 loss: 0.897338  [   64/  118]
train() client id: f_00009-10-2 loss: 0.917933  [   96/  118]
train() client id: f_00009-11-0 loss: 0.929307  [   32/  118]
train() client id: f_00009-11-1 loss: 1.031386  [   64/  118]
train() client id: f_00009-11-2 loss: 0.833856  [   96/  118]
At round 16 accuracy: 0.6339522546419099
At round 16 training accuracy: 0.5747820254862508
At round 16 training loss: 0.8521413370663757
update_location
xs = [ -3.9056584    4.20031788 100.00902392  18.81129433   0.97929623
   3.95640986 -62.44319194 -41.32485185  84.66397685 -27.06087855]
ys = [ 92.5879595   75.55583871   1.32061395 -62.45517586  54.35018685
  37.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [136.33702509 125.40465475 141.43390289 119.39226854 113.81960214
 106.98393887 117.92388546 108.20545105 132.19931524 103.6740228 ]
dists_bs = [189.90792018 204.94363894 325.26065297 306.50901839 213.36609411
 225.48914357 210.28473576 219.56069707 303.66082845 226.11164695]
uav_gains = [4.60616434e-11 5.67774848e-11 4.20154197e-11 6.42006706e-11
 7.23514457e-11 8.44693888e-11 6.62184205e-11 8.21054949e-11
 4.97558800e-11 9.13739608e-11]
bs_gains = [4.60652538e-11 3.72149675e-11 1.02104716e-11 1.20573805e-11
 3.32462714e-11 2.84801519e-11 3.46283970e-11 3.06860651e-11
 1.23767188e-11 2.82611527e-11]
Round 17
-------------------------------
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.74794599 18.22367373  8.61494673  3.08364678 21.02075903 10.12863146
  3.83198593 12.340238    9.082055    8.2217282 ]
obj_prev = 103.29561085136496
eta_min = 2.3980574083554427e-11	eta_max = 0.9215530004474818
af = 21.83294705699746	bf = 1.718217883710131	zeta = 24.01624176269721	eta = 0.9090909090909091
af = 21.83294705699746	bf = 1.718217883710131	zeta = 41.790319927079516	eta = 0.5224402946685754
af = 21.83294705699746	bf = 1.718217883710131	zeta = 33.27990204277066	eta = 0.6560400036315671
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.752345685579005	eta = 0.6876010759391976
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.67639108760098	eta = 0.6892498263649575
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.676189751909728	eta = 0.6892542072766557
eta = 0.6892542072766557
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [0.03070852 0.06458539 0.03022109 0.01047989 0.07457784 0.03558289
 0.0131608  0.0436256  0.0316834  0.02875878]
ene_total = [2.73656278 5.19862747 2.71178509 1.24687687 5.93149526 3.1458414
 1.43681478 3.6092118  3.0031103  2.655864  ]
ti_comp = [0.33188313 0.32847851 0.33045231 0.33663064 0.32655044 0.32374602
 0.33704245 0.33977473 0.30458757 0.32360102]
ti_coms = [0.07323726 0.07664188 0.07466807 0.06848974 0.07856994 0.08137436
 0.06807793 0.06534565 0.10053281 0.08151936]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.64318511e-05 1.56051844e-04 1.57976562e-05 6.34809284e-07
 2.43113417e-04 2.68654979e-05 1.25417250e-06 4.49492172e-05
 2.14264829e-05 1.41961788e-05]
ene_total = [0.52815183 0.56269552 0.53840142 0.492855   0.58283307 0.5874521
 0.48993648 0.47342069 0.72491292 0.58758383]
optimize_network_iter = 0 obj = 5.56824286220838
eta = 0.6892542072766557
freqs = [4.62640631e+07 9.83099137e+07 4.57268501e+07 1.55658631e+07
 1.14190388e+08 5.49549501e+07 1.95239441e+07 6.41978302e+07
 5.20103250e+07 4.44355465e+07]
eta_min = 0.6892542072766561	eta_max = 0.6892542072766527
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 0.03573652736322809	eta = 0.9090909090909091
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 18.93445726929715	eta = 0.0017158005474531947
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.9284046028512083	eta = 0.016846958413371853
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.882600477836571	eta = 0.017256848986739414
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.8825910021965795	eta = 0.01725693584558884
eta = 0.01725693584558884
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.79246673e-04 1.70228989e-03 1.72328565e-04 6.92481033e-06
 2.65200012e-03 2.93061998e-04 1.36811274e-05 4.90328058e-04
 2.33730561e-04 1.54858866e-04]
ene_total = [0.17070727 0.21310565 0.17380525 0.15598508 0.23910014 0.1918103
 0.15520187 0.15983003 0.23404959 0.18899581]
ti_comp = [0.33188313 0.32847851 0.33045231 0.33663064 0.32655044 0.32374602
 0.33704245 0.33977473 0.30458757 0.32360102]
ti_coms = [0.07323726 0.07664188 0.07466807 0.06848974 0.07856994 0.08137436
 0.06807793 0.06534565 0.10053281 0.08151936]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.64318511e-05 1.56051844e-04 1.57976562e-05 6.34809284e-07
 2.43113417e-04 2.68654979e-05 1.25417250e-06 4.49492172e-05
 2.14264829e-05 1.41961788e-05]
ene_total = [0.52815183 0.56269552 0.53840142 0.492855   0.58283307 0.5874521
 0.48993648 0.47342069 0.72491292 0.58758383]
optimize_network_iter = 1 obj = 5.56824286220839
eta = 0.6892542072766561
freqs = [4.62640631e+07 9.83099137e+07 4.57268501e+07 1.55658631e+07
 1.14190388e+08 5.49549501e+07 1.95239441e+07 6.41978302e+07
 5.20103250e+07 4.44355465e+07]
Done!
At round 17 eta: 0.6892542072766561
At round 17 local rounds: 12.185922115151007
At round 17 global rounds: 71.9537447693859
At round 17 a_n: 22.016777610806663
gradient difference: 0.41422876715660095
train() client id: f_00000-0-0 loss: 1.462394  [   32/  126]
train() client id: f_00000-0-1 loss: 1.183218  [   64/  126]
train() client id: f_00000-0-2 loss: 1.327662  [   96/  126]
train() client id: f_00000-1-0 loss: 1.154174  [   32/  126]
train() client id: f_00000-1-1 loss: 1.217086  [   64/  126]
train() client id: f_00000-1-2 loss: 1.259662  [   96/  126]
train() client id: f_00000-2-0 loss: 1.092725  [   32/  126]
train() client id: f_00000-2-1 loss: 1.125116  [   64/  126]
train() client id: f_00000-2-2 loss: 1.116829  [   96/  126]
train() client id: f_00000-3-0 loss: 0.943766  [   32/  126]
train() client id: f_00000-3-1 loss: 1.074583  [   64/  126]
train() client id: f_00000-3-2 loss: 0.961610  [   96/  126]
train() client id: f_00000-4-0 loss: 0.962318  [   32/  126]
train() client id: f_00000-4-1 loss: 0.968268  [   64/  126]
train() client id: f_00000-4-2 loss: 0.972701  [   96/  126]
train() client id: f_00000-5-0 loss: 1.067242  [   32/  126]
train() client id: f_00000-5-1 loss: 0.885564  [   64/  126]
train() client id: f_00000-5-2 loss: 0.944534  [   96/  126]
train() client id: f_00000-6-0 loss: 0.967729  [   32/  126]
train() client id: f_00000-6-1 loss: 0.940917  [   64/  126]
train() client id: f_00000-6-2 loss: 0.907473  [   96/  126]
train() client id: f_00000-7-0 loss: 0.896266  [   32/  126]
train() client id: f_00000-7-1 loss: 0.929030  [   64/  126]
train() client id: f_00000-7-2 loss: 0.811987  [   96/  126]
train() client id: f_00000-8-0 loss: 0.939635  [   32/  126]
train() client id: f_00000-8-1 loss: 0.879366  [   64/  126]
train() client id: f_00000-8-2 loss: 0.861941  [   96/  126]
train() client id: f_00000-9-0 loss: 0.830035  [   32/  126]
train() client id: f_00000-9-1 loss: 0.767506  [   64/  126]
train() client id: f_00000-9-2 loss: 1.025036  [   96/  126]
train() client id: f_00000-10-0 loss: 0.900919  [   32/  126]
train() client id: f_00000-10-1 loss: 0.890488  [   64/  126]
train() client id: f_00000-10-2 loss: 0.810816  [   96/  126]
train() client id: f_00000-11-0 loss: 0.850971  [   32/  126]
train() client id: f_00000-11-1 loss: 0.934601  [   64/  126]
train() client id: f_00000-11-2 loss: 0.712596  [   96/  126]
train() client id: f_00001-0-0 loss: 0.416625  [   32/  265]
train() client id: f_00001-0-1 loss: 0.403894  [   64/  265]
train() client id: f_00001-0-2 loss: 0.448249  [   96/  265]
train() client id: f_00001-0-3 loss: 0.317315  [  128/  265]
train() client id: f_00001-0-4 loss: 0.531062  [  160/  265]
train() client id: f_00001-0-5 loss: 0.436670  [  192/  265]
train() client id: f_00001-0-6 loss: 0.333706  [  224/  265]
train() client id: f_00001-0-7 loss: 0.440774  [  256/  265]
train() client id: f_00001-1-0 loss: 0.526481  [   32/  265]
train() client id: f_00001-1-1 loss: 0.385891  [   64/  265]
train() client id: f_00001-1-2 loss: 0.497461  [   96/  265]
train() client id: f_00001-1-3 loss: 0.356405  [  128/  265]
train() client id: f_00001-1-4 loss: 0.392943  [  160/  265]
train() client id: f_00001-1-5 loss: 0.340282  [  192/  265]
train() client id: f_00001-1-6 loss: 0.303727  [  224/  265]
train() client id: f_00001-1-7 loss: 0.405930  [  256/  265]
train() client id: f_00001-2-0 loss: 0.302543  [   32/  265]
train() client id: f_00001-2-1 loss: 0.361452  [   64/  265]
train() client id: f_00001-2-2 loss: 0.446813  [   96/  265]
train() client id: f_00001-2-3 loss: 0.389432  [  128/  265]
train() client id: f_00001-2-4 loss: 0.419627  [  160/  265]
train() client id: f_00001-2-5 loss: 0.420329  [  192/  265]
train() client id: f_00001-2-6 loss: 0.446746  [  224/  265]
train() client id: f_00001-2-7 loss: 0.291382  [  256/  265]
train() client id: f_00001-3-0 loss: 0.364253  [   32/  265]
train() client id: f_00001-3-1 loss: 0.384378  [   64/  265]
train() client id: f_00001-3-2 loss: 0.489593  [   96/  265]
train() client id: f_00001-3-3 loss: 0.356175  [  128/  265]
train() client id: f_00001-3-4 loss: 0.361331  [  160/  265]
train() client id: f_00001-3-5 loss: 0.353942  [  192/  265]
train() client id: f_00001-3-6 loss: 0.440339  [  224/  265]
train() client id: f_00001-3-7 loss: 0.309845  [  256/  265]
train() client id: f_00001-4-0 loss: 0.396841  [   32/  265]
train() client id: f_00001-4-1 loss: 0.301366  [   64/  265]
train() client id: f_00001-4-2 loss: 0.363216  [   96/  265]
train() client id: f_00001-4-3 loss: 0.368774  [  128/  265]
train() client id: f_00001-4-4 loss: 0.445703  [  160/  265]
train() client id: f_00001-4-5 loss: 0.407390  [  192/  265]
train() client id: f_00001-4-6 loss: 0.387733  [  224/  265]
train() client id: f_00001-4-7 loss: 0.284278  [  256/  265]
train() client id: f_00001-5-0 loss: 0.300000  [   32/  265]
train() client id: f_00001-5-1 loss: 0.433800  [   64/  265]
train() client id: f_00001-5-2 loss: 0.329940  [   96/  265]
train() client id: f_00001-5-3 loss: 0.376483  [  128/  265]
train() client id: f_00001-5-4 loss: 0.250191  [  160/  265]
train() client id: f_00001-5-5 loss: 0.459381  [  192/  265]
train() client id: f_00001-5-6 loss: 0.411066  [  224/  265]
train() client id: f_00001-5-7 loss: 0.398028  [  256/  265]
train() client id: f_00001-6-0 loss: 0.482285  [   32/  265]
train() client id: f_00001-6-1 loss: 0.375787  [   64/  265]
train() client id: f_00001-6-2 loss: 0.279850  [   96/  265]
train() client id: f_00001-6-3 loss: 0.277918  [  128/  265]
train() client id: f_00001-6-4 loss: 0.367763  [  160/  265]
train() client id: f_00001-6-5 loss: 0.395124  [  192/  265]
train() client id: f_00001-6-6 loss: 0.402484  [  224/  265]
train() client id: f_00001-6-7 loss: 0.343158  [  256/  265]
train() client id: f_00001-7-0 loss: 0.275411  [   32/  265]
train() client id: f_00001-7-1 loss: 0.309368  [   64/  265]
train() client id: f_00001-7-2 loss: 0.278570  [   96/  265]
train() client id: f_00001-7-3 loss: 0.275260  [  128/  265]
train() client id: f_00001-7-4 loss: 0.366338  [  160/  265]
train() client id: f_00001-7-5 loss: 0.489935  [  192/  265]
train() client id: f_00001-7-6 loss: 0.337482  [  224/  265]
train() client id: f_00001-7-7 loss: 0.541674  [  256/  265]
train() client id: f_00001-8-0 loss: 0.308924  [   32/  265]
train() client id: f_00001-8-1 loss: 0.357665  [   64/  265]
train() client id: f_00001-8-2 loss: 0.522783  [   96/  265]
train() client id: f_00001-8-3 loss: 0.279598  [  128/  265]
train() client id: f_00001-8-4 loss: 0.319831  [  160/  265]
train() client id: f_00001-8-5 loss: 0.319944  [  192/  265]
train() client id: f_00001-8-6 loss: 0.407116  [  224/  265]
train() client id: f_00001-8-7 loss: 0.258076  [  256/  265]
train() client id: f_00001-9-0 loss: 0.266355  [   32/  265]
train() client id: f_00001-9-1 loss: 0.323913  [   64/  265]
train() client id: f_00001-9-2 loss: 0.261533  [   96/  265]
train() client id: f_00001-9-3 loss: 0.370272  [  128/  265]
train() client id: f_00001-9-4 loss: 0.506095  [  160/  265]
train() client id: f_00001-9-5 loss: 0.252553  [  192/  265]
train() client id: f_00001-9-6 loss: 0.334581  [  224/  265]
train() client id: f_00001-9-7 loss: 0.520063  [  256/  265]
train() client id: f_00001-10-0 loss: 0.312648  [   32/  265]
train() client id: f_00001-10-1 loss: 0.444357  [   64/  265]
train() client id: f_00001-10-2 loss: 0.246875  [   96/  265]
train() client id: f_00001-10-3 loss: 0.404005  [  128/  265]
train() client id: f_00001-10-4 loss: 0.316290  [  160/  265]
train() client id: f_00001-10-5 loss: 0.464759  [  192/  265]
train() client id: f_00001-10-6 loss: 0.309573  [  224/  265]
train() client id: f_00001-10-7 loss: 0.325875  [  256/  265]
train() client id: f_00001-11-0 loss: 0.275947  [   32/  265]
train() client id: f_00001-11-1 loss: 0.502371  [   64/  265]
train() client id: f_00001-11-2 loss: 0.384768  [   96/  265]
train() client id: f_00001-11-3 loss: 0.322286  [  128/  265]
train() client id: f_00001-11-4 loss: 0.247428  [  160/  265]
train() client id: f_00001-11-5 loss: 0.328736  [  192/  265]
train() client id: f_00001-11-6 loss: 0.431276  [  224/  265]
train() client id: f_00001-11-7 loss: 0.303747  [  256/  265]
train() client id: f_00002-0-0 loss: 1.319281  [   32/  124]
train() client id: f_00002-0-1 loss: 1.064597  [   64/  124]
train() client id: f_00002-0-2 loss: 0.974671  [   96/  124]
train() client id: f_00002-1-0 loss: 1.088246  [   32/  124]
train() client id: f_00002-1-1 loss: 1.170148  [   64/  124]
train() client id: f_00002-1-2 loss: 1.029881  [   96/  124]
train() client id: f_00002-2-0 loss: 0.929376  [   32/  124]
train() client id: f_00002-2-1 loss: 1.113326  [   64/  124]
train() client id: f_00002-2-2 loss: 1.039300  [   96/  124]
train() client id: f_00002-3-0 loss: 0.921790  [   32/  124]
train() client id: f_00002-3-1 loss: 0.918961  [   64/  124]
train() client id: f_00002-3-2 loss: 1.096342  [   96/  124]
train() client id: f_00002-4-0 loss: 0.831406  [   32/  124]
train() client id: f_00002-4-1 loss: 1.145678  [   64/  124]
train() client id: f_00002-4-2 loss: 0.914227  [   96/  124]
train() client id: f_00002-5-0 loss: 0.926186  [   32/  124]
train() client id: f_00002-5-1 loss: 0.955629  [   64/  124]
train() client id: f_00002-5-2 loss: 0.999428  [   96/  124]
train() client id: f_00002-6-0 loss: 0.911890  [   32/  124]
train() client id: f_00002-6-1 loss: 0.912194  [   64/  124]
train() client id: f_00002-6-2 loss: 0.967270  [   96/  124]
train() client id: f_00002-7-0 loss: 0.965860  [   32/  124]
train() client id: f_00002-7-1 loss: 0.799939  [   64/  124]
train() client id: f_00002-7-2 loss: 0.934871  [   96/  124]
train() client id: f_00002-8-0 loss: 0.796278  [   32/  124]
train() client id: f_00002-8-1 loss: 0.853342  [   64/  124]
train() client id: f_00002-8-2 loss: 1.022481  [   96/  124]
train() client id: f_00002-9-0 loss: 0.959701  [   32/  124]
train() client id: f_00002-9-1 loss: 0.758028  [   64/  124]
train() client id: f_00002-9-2 loss: 0.824516  [   96/  124]
train() client id: f_00002-10-0 loss: 0.831500  [   32/  124]
train() client id: f_00002-10-1 loss: 0.794290  [   64/  124]
train() client id: f_00002-10-2 loss: 0.959279  [   96/  124]
train() client id: f_00002-11-0 loss: 0.789364  [   32/  124]
train() client id: f_00002-11-1 loss: 0.774412  [   64/  124]
train() client id: f_00002-11-2 loss: 1.000375  [   96/  124]
train() client id: f_00003-0-0 loss: 0.751298  [   32/   43]
train() client id: f_00003-1-0 loss: 0.881776  [   32/   43]
train() client id: f_00003-2-0 loss: 0.822172  [   32/   43]
train() client id: f_00003-3-0 loss: 0.726782  [   32/   43]
train() client id: f_00003-4-0 loss: 0.758272  [   32/   43]
train() client id: f_00003-5-0 loss: 0.599889  [   32/   43]
train() client id: f_00003-6-0 loss: 0.940122  [   32/   43]
train() client id: f_00003-7-0 loss: 0.781197  [   32/   43]
train() client id: f_00003-8-0 loss: 0.834245  [   32/   43]
train() client id: f_00003-9-0 loss: 0.865052  [   32/   43]
train() client id: f_00003-10-0 loss: 0.919167  [   32/   43]
train() client id: f_00003-11-0 loss: 0.879741  [   32/   43]
train() client id: f_00004-0-0 loss: 1.026574  [   32/  306]
train() client id: f_00004-0-1 loss: 1.140841  [   64/  306]
train() client id: f_00004-0-2 loss: 1.076712  [   96/  306]
train() client id: f_00004-0-3 loss: 0.957036  [  128/  306]
train() client id: f_00004-0-4 loss: 0.939133  [  160/  306]
train() client id: f_00004-0-5 loss: 0.950317  [  192/  306]
train() client id: f_00004-0-6 loss: 0.906089  [  224/  306]
train() client id: f_00004-0-7 loss: 0.946127  [  256/  306]
train() client id: f_00004-0-8 loss: 0.997589  [  288/  306]
train() client id: f_00004-1-0 loss: 1.107358  [   32/  306]
train() client id: f_00004-1-1 loss: 0.942839  [   64/  306]
train() client id: f_00004-1-2 loss: 0.946191  [   96/  306]
train() client id: f_00004-1-3 loss: 0.938904  [  128/  306]
train() client id: f_00004-1-4 loss: 1.056624  [  160/  306]
train() client id: f_00004-1-5 loss: 0.964927  [  192/  306]
train() client id: f_00004-1-6 loss: 1.039624  [  224/  306]
train() client id: f_00004-1-7 loss: 1.061718  [  256/  306]
train() client id: f_00004-1-8 loss: 0.831970  [  288/  306]
train() client id: f_00004-2-0 loss: 0.938920  [   32/  306]
train() client id: f_00004-2-1 loss: 0.945648  [   64/  306]
train() client id: f_00004-2-2 loss: 1.092965  [   96/  306]
train() client id: f_00004-2-3 loss: 0.907975  [  128/  306]
train() client id: f_00004-2-4 loss: 1.017431  [  160/  306]
train() client id: f_00004-2-5 loss: 0.999780  [  192/  306]
train() client id: f_00004-2-6 loss: 0.955972  [  224/  306]
train() client id: f_00004-2-7 loss: 1.133716  [  256/  306]
train() client id: f_00004-2-8 loss: 0.855466  [  288/  306]
train() client id: f_00004-3-0 loss: 1.037159  [   32/  306]
train() client id: f_00004-3-1 loss: 0.939026  [   64/  306]
train() client id: f_00004-3-2 loss: 1.049014  [   96/  306]
train() client id: f_00004-3-3 loss: 1.032331  [  128/  306]
train() client id: f_00004-3-4 loss: 1.112329  [  160/  306]
train() client id: f_00004-3-5 loss: 0.948100  [  192/  306]
train() client id: f_00004-3-6 loss: 0.900648  [  224/  306]
train() client id: f_00004-3-7 loss: 0.914499  [  256/  306]
train() client id: f_00004-3-8 loss: 1.013121  [  288/  306]
train() client id: f_00004-4-0 loss: 1.025085  [   32/  306]
train() client id: f_00004-4-1 loss: 1.102460  [   64/  306]
train() client id: f_00004-4-2 loss: 1.046047  [   96/  306]
train() client id: f_00004-4-3 loss: 0.935708  [  128/  306]
train() client id: f_00004-4-4 loss: 0.999439  [  160/  306]
train() client id: f_00004-4-5 loss: 0.889318  [  192/  306]
train() client id: f_00004-4-6 loss: 0.987589  [  224/  306]
train() client id: f_00004-4-7 loss: 0.912921  [  256/  306]
train() client id: f_00004-4-8 loss: 0.966866  [  288/  306]
train() client id: f_00004-5-0 loss: 1.072719  [   32/  306]
train() client id: f_00004-5-1 loss: 0.963022  [   64/  306]
train() client id: f_00004-5-2 loss: 0.932298  [   96/  306]
train() client id: f_00004-5-3 loss: 0.990577  [  128/  306]
train() client id: f_00004-5-4 loss: 0.951728  [  160/  306]
train() client id: f_00004-5-5 loss: 1.049221  [  192/  306]
train() client id: f_00004-5-6 loss: 0.916887  [  224/  306]
train() client id: f_00004-5-7 loss: 1.082647  [  256/  306]
train() client id: f_00004-5-8 loss: 0.940396  [  288/  306]
train() client id: f_00004-6-0 loss: 0.927025  [   32/  306]
train() client id: f_00004-6-1 loss: 1.054796  [   64/  306]
train() client id: f_00004-6-2 loss: 1.005856  [   96/  306]
train() client id: f_00004-6-3 loss: 1.020046  [  128/  306]
train() client id: f_00004-6-4 loss: 0.997818  [  160/  306]
train() client id: f_00004-6-5 loss: 0.943899  [  192/  306]
train() client id: f_00004-6-6 loss: 0.940259  [  224/  306]
train() client id: f_00004-6-7 loss: 1.012215  [  256/  306]
train() client id: f_00004-6-8 loss: 0.916337  [  288/  306]
train() client id: f_00004-7-0 loss: 0.963699  [   32/  306]
train() client id: f_00004-7-1 loss: 1.036440  [   64/  306]
train() client id: f_00004-7-2 loss: 0.927192  [   96/  306]
train() client id: f_00004-7-3 loss: 0.971832  [  128/  306]
train() client id: f_00004-7-4 loss: 0.943947  [  160/  306]
train() client id: f_00004-7-5 loss: 0.955530  [  192/  306]
train() client id: f_00004-7-6 loss: 1.001116  [  224/  306]
train() client id: f_00004-7-7 loss: 1.020828  [  256/  306]
train() client id: f_00004-7-8 loss: 1.016457  [  288/  306]
train() client id: f_00004-8-0 loss: 1.089876  [   32/  306]
train() client id: f_00004-8-1 loss: 1.059520  [   64/  306]
train() client id: f_00004-8-2 loss: 0.977154  [   96/  306]
train() client id: f_00004-8-3 loss: 0.886731  [  128/  306]
train() client id: f_00004-8-4 loss: 0.948888  [  160/  306]
train() client id: f_00004-8-5 loss: 1.016007  [  192/  306]
train() client id: f_00004-8-6 loss: 1.009417  [  224/  306]
train() client id: f_00004-8-7 loss: 0.943893  [  256/  306]
train() client id: f_00004-8-8 loss: 0.952166  [  288/  306]
train() client id: f_00004-9-0 loss: 1.012887  [   32/  306]
train() client id: f_00004-9-1 loss: 0.945349  [   64/  306]
train() client id: f_00004-9-2 loss: 0.884530  [   96/  306]
train() client id: f_00004-9-3 loss: 1.057393  [  128/  306]
train() client id: f_00004-9-4 loss: 1.003223  [  160/  306]
train() client id: f_00004-9-5 loss: 0.975424  [  192/  306]
train() client id: f_00004-9-6 loss: 1.014912  [  224/  306]
train() client id: f_00004-9-7 loss: 0.895578  [  256/  306]
train() client id: f_00004-9-8 loss: 0.945948  [  288/  306]
train() client id: f_00004-10-0 loss: 0.930920  [   32/  306]
train() client id: f_00004-10-1 loss: 0.905382  [   64/  306]
train() client id: f_00004-10-2 loss: 1.104041  [   96/  306]
train() client id: f_00004-10-3 loss: 0.964493  [  128/  306]
train() client id: f_00004-10-4 loss: 0.949005  [  160/  306]
train() client id: f_00004-10-5 loss: 0.977626  [  192/  306]
train() client id: f_00004-10-6 loss: 0.922343  [  224/  306]
train() client id: f_00004-10-7 loss: 1.009309  [  256/  306]
train() client id: f_00004-10-8 loss: 0.968948  [  288/  306]
train() client id: f_00004-11-0 loss: 0.939929  [   32/  306]
train() client id: f_00004-11-1 loss: 0.986329  [   64/  306]
train() client id: f_00004-11-2 loss: 0.878851  [   96/  306]
train() client id: f_00004-11-3 loss: 0.975975  [  128/  306]
train() client id: f_00004-11-4 loss: 0.994359  [  160/  306]
train() client id: f_00004-11-5 loss: 0.925298  [  192/  306]
train() client id: f_00004-11-6 loss: 1.022286  [  224/  306]
train() client id: f_00004-11-7 loss: 1.134033  [  256/  306]
train() client id: f_00004-11-8 loss: 0.935452  [  288/  306]
train() client id: f_00005-0-0 loss: 0.515926  [   32/  146]
train() client id: f_00005-0-1 loss: 0.667641  [   64/  146]
train() client id: f_00005-0-2 loss: 0.623577  [   96/  146]
train() client id: f_00005-0-3 loss: 0.515507  [  128/  146]
train() client id: f_00005-1-0 loss: 0.418219  [   32/  146]
train() client id: f_00005-1-1 loss: 0.515841  [   64/  146]
train() client id: f_00005-1-2 loss: 0.864364  [   96/  146]
train() client id: f_00005-1-3 loss: 0.577363  [  128/  146]
train() client id: f_00005-2-0 loss: 0.455589  [   32/  146]
train() client id: f_00005-2-1 loss: 0.648100  [   64/  146]
train() client id: f_00005-2-2 loss: 0.744565  [   96/  146]
train() client id: f_00005-2-3 loss: 0.442287  [  128/  146]
train() client id: f_00005-3-0 loss: 0.762487  [   32/  146]
train() client id: f_00005-3-1 loss: 0.425335  [   64/  146]
train() client id: f_00005-3-2 loss: 0.649156  [   96/  146]
train() client id: f_00005-3-3 loss: 0.462791  [  128/  146]
train() client id: f_00005-4-0 loss: 0.324228  [   32/  146]
train() client id: f_00005-4-1 loss: 0.572737  [   64/  146]
train() client id: f_00005-4-2 loss: 0.735257  [   96/  146]
train() client id: f_00005-4-3 loss: 0.668012  [  128/  146]
train() client id: f_00005-5-0 loss: 0.581322  [   32/  146]
train() client id: f_00005-5-1 loss: 0.666376  [   64/  146]
train() client id: f_00005-5-2 loss: 0.648670  [   96/  146]
train() client id: f_00005-5-3 loss: 0.457770  [  128/  146]
train() client id: f_00005-6-0 loss: 0.608428  [   32/  146]
train() client id: f_00005-6-1 loss: 0.565801  [   64/  146]
train() client id: f_00005-6-2 loss: 0.502554  [   96/  146]
train() client id: f_00005-6-3 loss: 0.475653  [  128/  146]
train() client id: f_00005-7-0 loss: 0.395643  [   32/  146]
train() client id: f_00005-7-1 loss: 0.480346  [   64/  146]
train() client id: f_00005-7-2 loss: 0.500091  [   96/  146]
train() client id: f_00005-7-3 loss: 0.655441  [  128/  146]
train() client id: f_00005-8-0 loss: 0.484797  [   32/  146]
train() client id: f_00005-8-1 loss: 0.457488  [   64/  146]
train() client id: f_00005-8-2 loss: 0.474026  [   96/  146]
train() client id: f_00005-8-3 loss: 0.576151  [  128/  146]
train() client id: f_00005-9-0 loss: 0.521326  [   32/  146]
train() client id: f_00005-9-1 loss: 0.543813  [   64/  146]
train() client id: f_00005-9-2 loss: 0.475358  [   96/  146]
train() client id: f_00005-9-3 loss: 0.591489  [  128/  146]
train() client id: f_00005-10-0 loss: 0.374906  [   32/  146]
train() client id: f_00005-10-1 loss: 0.536911  [   64/  146]
train() client id: f_00005-10-2 loss: 0.559956  [   96/  146]
train() client id: f_00005-10-3 loss: 0.547184  [  128/  146]
train() client id: f_00005-11-0 loss: 0.530331  [   32/  146]
train() client id: f_00005-11-1 loss: 0.466162  [   64/  146]
train() client id: f_00005-11-2 loss: 0.603351  [   96/  146]
train() client id: f_00005-11-3 loss: 0.525649  [  128/  146]
train() client id: f_00006-0-0 loss: 0.596502  [   32/   54]
train() client id: f_00006-1-0 loss: 0.606944  [   32/   54]
train() client id: f_00006-2-0 loss: 0.583326  [   32/   54]
train() client id: f_00006-3-0 loss: 0.554700  [   32/   54]
train() client id: f_00006-4-0 loss: 0.508237  [   32/   54]
train() client id: f_00006-5-0 loss: 0.564272  [   32/   54]
train() client id: f_00006-6-0 loss: 0.541618  [   32/   54]
train() client id: f_00006-7-0 loss: 0.570337  [   32/   54]
train() client id: f_00006-8-0 loss: 0.607737  [   32/   54]
train() client id: f_00006-9-0 loss: 0.608505  [   32/   54]
train() client id: f_00006-10-0 loss: 0.556493  [   32/   54]
train() client id: f_00006-11-0 loss: 0.572966  [   32/   54]
train() client id: f_00007-0-0 loss: 0.612848  [   32/  179]
train() client id: f_00007-0-1 loss: 0.529225  [   64/  179]
train() client id: f_00007-0-2 loss: 0.667511  [   96/  179]
train() client id: f_00007-0-3 loss: 0.590128  [  128/  179]
train() client id: f_00007-0-4 loss: 0.604031  [  160/  179]
train() client id: f_00007-1-0 loss: 0.728149  [   32/  179]
train() client id: f_00007-1-1 loss: 0.527204  [   64/  179]
train() client id: f_00007-1-2 loss: 0.765647  [   96/  179]
train() client id: f_00007-1-3 loss: 0.682214  [  128/  179]
train() client id: f_00007-1-4 loss: 0.506343  [  160/  179]
train() client id: f_00007-2-0 loss: 0.505343  [   32/  179]
train() client id: f_00007-2-1 loss: 0.690223  [   64/  179]
train() client id: f_00007-2-2 loss: 0.501942  [   96/  179]
train() client id: f_00007-2-3 loss: 0.820118  [  128/  179]
train() client id: f_00007-2-4 loss: 0.580484  [  160/  179]
train() client id: f_00007-3-0 loss: 0.509229  [   32/  179]
train() client id: f_00007-3-1 loss: 0.603453  [   64/  179]
train() client id: f_00007-3-2 loss: 0.653961  [   96/  179]
train() client id: f_00007-3-3 loss: 0.561329  [  128/  179]
train() client id: f_00007-3-4 loss: 0.786224  [  160/  179]
train() client id: f_00007-4-0 loss: 0.772210  [   32/  179]
train() client id: f_00007-4-1 loss: 0.636042  [   64/  179]
train() client id: f_00007-4-2 loss: 0.537589  [   96/  179]
train() client id: f_00007-4-3 loss: 0.648388  [  128/  179]
train() client id: f_00007-4-4 loss: 0.481781  [  160/  179]
train() client id: f_00007-5-0 loss: 0.493637  [   32/  179]
train() client id: f_00007-5-1 loss: 0.679367  [   64/  179]
train() client id: f_00007-5-2 loss: 0.561189  [   96/  179]
train() client id: f_00007-5-3 loss: 0.455894  [  128/  179]
train() client id: f_00007-5-4 loss: 0.716460  [  160/  179]
train() client id: f_00007-6-0 loss: 0.673856  [   32/  179]
train() client id: f_00007-6-1 loss: 0.633049  [   64/  179]
train() client id: f_00007-6-2 loss: 0.477104  [   96/  179]
train() client id: f_00007-6-3 loss: 0.529753  [  128/  179]
train() client id: f_00007-6-4 loss: 0.728969  [  160/  179]
train() client id: f_00007-7-0 loss: 0.461806  [   32/  179]
train() client id: f_00007-7-1 loss: 0.639391  [   64/  179]
train() client id: f_00007-7-2 loss: 0.716556  [   96/  179]
train() client id: f_00007-7-3 loss: 0.449259  [  128/  179]
train() client id: f_00007-7-4 loss: 0.685980  [  160/  179]
train() client id: f_00007-8-0 loss: 0.640483  [   32/  179]
train() client id: f_00007-8-1 loss: 0.470509  [   64/  179]
train() client id: f_00007-8-2 loss: 0.549318  [   96/  179]
train() client id: f_00007-8-3 loss: 0.623585  [  128/  179]
train() client id: f_00007-8-4 loss: 0.778476  [  160/  179]
train() client id: f_00007-9-0 loss: 0.475951  [   32/  179]
train() client id: f_00007-9-1 loss: 0.477726  [   64/  179]
train() client id: f_00007-9-2 loss: 0.636562  [   96/  179]
train() client id: f_00007-9-3 loss: 0.635491  [  128/  179]
train() client id: f_00007-9-4 loss: 0.623724  [  160/  179]
train() client id: f_00007-10-0 loss: 0.759737  [   32/  179]
train() client id: f_00007-10-1 loss: 0.542976  [   64/  179]
train() client id: f_00007-10-2 loss: 0.528987  [   96/  179]
train() client id: f_00007-10-3 loss: 0.630074  [  128/  179]
train() client id: f_00007-10-4 loss: 0.581518  [  160/  179]
train() client id: f_00007-11-0 loss: 0.715630  [   32/  179]
train() client id: f_00007-11-1 loss: 0.550142  [   64/  179]
train() client id: f_00007-11-2 loss: 0.772238  [   96/  179]
train() client id: f_00007-11-3 loss: 0.453281  [  128/  179]
train() client id: f_00007-11-4 loss: 0.459537  [  160/  179]
train() client id: f_00008-0-0 loss: 0.845119  [   32/  130]
train() client id: f_00008-0-1 loss: 0.907005  [   64/  130]
train() client id: f_00008-0-2 loss: 0.771040  [   96/  130]
train() client id: f_00008-0-3 loss: 0.831503  [  128/  130]
train() client id: f_00008-1-0 loss: 0.926282  [   32/  130]
train() client id: f_00008-1-1 loss: 0.755812  [   64/  130]
train() client id: f_00008-1-2 loss: 0.869442  [   96/  130]
train() client id: f_00008-1-3 loss: 0.790123  [  128/  130]
train() client id: f_00008-2-0 loss: 0.953092  [   32/  130]
train() client id: f_00008-2-1 loss: 0.847984  [   64/  130]
train() client id: f_00008-2-2 loss: 0.761037  [   96/  130]
train() client id: f_00008-2-3 loss: 0.825613  [  128/  130]
train() client id: f_00008-3-0 loss: 0.817281  [   32/  130]
train() client id: f_00008-3-1 loss: 0.897639  [   64/  130]
train() client id: f_00008-3-2 loss: 0.711074  [   96/  130]
train() client id: f_00008-3-3 loss: 0.932354  [  128/  130]
train() client id: f_00008-4-0 loss: 0.884151  [   32/  130]
train() client id: f_00008-4-1 loss: 0.907344  [   64/  130]
train() client id: f_00008-4-2 loss: 0.857850  [   96/  130]
train() client id: f_00008-4-3 loss: 0.700835  [  128/  130]
train() client id: f_00008-5-0 loss: 0.763862  [   32/  130]
train() client id: f_00008-5-1 loss: 0.844265  [   64/  130]
train() client id: f_00008-5-2 loss: 0.935367  [   96/  130]
train() client id: f_00008-5-3 loss: 0.842042  [  128/  130]
train() client id: f_00008-6-0 loss: 0.748030  [   32/  130]
train() client id: f_00008-6-1 loss: 0.867860  [   64/  130]
train() client id: f_00008-6-2 loss: 0.890307  [   96/  130]
train() client id: f_00008-6-3 loss: 0.883969  [  128/  130]
train() client id: f_00008-7-0 loss: 0.884029  [   32/  130]
train() client id: f_00008-7-1 loss: 0.823676  [   64/  130]
train() client id: f_00008-7-2 loss: 0.798155  [   96/  130]
train() client id: f_00008-7-3 loss: 0.876352  [  128/  130]
train() client id: f_00008-8-0 loss: 0.875127  [   32/  130]
train() client id: f_00008-8-1 loss: 0.866377  [   64/  130]
train() client id: f_00008-8-2 loss: 0.725962  [   96/  130]
train() client id: f_00008-8-3 loss: 0.926241  [  128/  130]
train() client id: f_00008-9-0 loss: 0.871871  [   32/  130]
train() client id: f_00008-9-1 loss: 0.774751  [   64/  130]
train() client id: f_00008-9-2 loss: 0.930331  [   96/  130]
train() client id: f_00008-9-3 loss: 0.776734  [  128/  130]
train() client id: f_00008-10-0 loss: 0.783307  [   32/  130]
train() client id: f_00008-10-1 loss: 0.882215  [   64/  130]
train() client id: f_00008-10-2 loss: 0.836463  [   96/  130]
train() client id: f_00008-10-3 loss: 0.857596  [  128/  130]
train() client id: f_00008-11-0 loss: 0.700563  [   32/  130]
train() client id: f_00008-11-1 loss: 0.806191  [   64/  130]
train() client id: f_00008-11-2 loss: 0.855124  [   96/  130]
train() client id: f_00008-11-3 loss: 1.005631  [  128/  130]
train() client id: f_00009-0-0 loss: 1.157967  [   32/  118]
train() client id: f_00009-0-1 loss: 1.086084  [   64/  118]
train() client id: f_00009-0-2 loss: 1.238679  [   96/  118]
train() client id: f_00009-1-0 loss: 1.227890  [   32/  118]
train() client id: f_00009-1-1 loss: 1.069511  [   64/  118]
train() client id: f_00009-1-2 loss: 1.098776  [   96/  118]
train() client id: f_00009-2-0 loss: 1.072464  [   32/  118]
train() client id: f_00009-2-1 loss: 1.064116  [   64/  118]
train() client id: f_00009-2-2 loss: 1.169649  [   96/  118]
train() client id: f_00009-3-0 loss: 1.090272  [   32/  118]
train() client id: f_00009-3-1 loss: 0.972879  [   64/  118]
train() client id: f_00009-3-2 loss: 1.111930  [   96/  118]
train() client id: f_00009-4-0 loss: 1.078431  [   32/  118]
train() client id: f_00009-4-1 loss: 1.022675  [   64/  118]
train() client id: f_00009-4-2 loss: 1.044364  [   96/  118]
train() client id: f_00009-5-0 loss: 1.017745  [   32/  118]
train() client id: f_00009-5-1 loss: 0.923836  [   64/  118]
train() client id: f_00009-5-2 loss: 1.024306  [   96/  118]
train() client id: f_00009-6-0 loss: 0.964549  [   32/  118]
train() client id: f_00009-6-1 loss: 0.936717  [   64/  118]
train() client id: f_00009-6-2 loss: 1.058235  [   96/  118]
train() client id: f_00009-7-0 loss: 1.028791  [   32/  118]
train() client id: f_00009-7-1 loss: 0.914447  [   64/  118]
train() client id: f_00009-7-2 loss: 0.935387  [   96/  118]
train() client id: f_00009-8-0 loss: 1.018038  [   32/  118]
train() client id: f_00009-8-1 loss: 1.007013  [   64/  118]
train() client id: f_00009-8-2 loss: 0.880489  [   96/  118]
train() client id: f_00009-9-0 loss: 0.904843  [   32/  118]
train() client id: f_00009-9-1 loss: 0.940509  [   64/  118]
train() client id: f_00009-9-2 loss: 0.948920  [   96/  118]
train() client id: f_00009-10-0 loss: 0.993394  [   32/  118]
train() client id: f_00009-10-1 loss: 1.097413  [   64/  118]
train() client id: f_00009-10-2 loss: 0.912462  [   96/  118]
train() client id: f_00009-11-0 loss: 0.905785  [   32/  118]
train() client id: f_00009-11-1 loss: 1.045033  [   64/  118]
train() client id: f_00009-11-2 loss: 0.935950  [   96/  118]
At round 17 accuracy: 0.6339522546419099
At round 17 training accuracy: 0.5781354795439303
At round 17 training loss: 0.8494290696972973
update_location
xs = [ -3.9056584    4.20031788 105.00902392  18.81129433   0.97929623
   3.95640986 -67.44319194 -46.32485185  89.66397685 -32.06087855]
ys = [ 97.5879595   80.55583871   1.32061395 -67.45517586  59.35018685
  42.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [139.78077123 128.47912601 145.01254816 122.08220814 116.29017026
 108.85175553 120.64607197 110.21192383 135.45589215 105.09001756]
dists_bs = [187.79216635 202.56468974 329.49898727 310.39866319 210.57918222
 222.48257319 207.65139541 216.55287625 307.94729826 222.87235287]
uav_gains = [4.32717486e-11 5.34395670e-11 3.94635132e-11 6.07214029e-11
 6.85691473e-11 8.08920872e-11 6.25451947e-11 7.84191512e-11
 4.68155255e-11 8.83269213e-11]
bs_gains = [4.75332122e-11 3.84517052e-11 9.84697079e-12 1.16390766e-11
 3.44929920e-11 2.95709508e-11 3.58720746e-11 3.18944419e-11
 1.19003619e-11 2.94263727e-11]
Round 18
-------------------------------
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.61607623 17.94328127  8.48520447  3.03808191 20.69729735  9.97190604
  3.77497723 12.15244223  8.94539714  8.09409538]
obj_prev = 101.71875926654823
eta_min = 1.6526197130074664e-11	eta_max = 0.921782499053972
af = 21.498465320302763	bf = 1.69748013632556	zeta = 23.64831185233304	eta = 0.9090909090909091
af = 21.498465320302763	bf = 1.69748013632556	zeta = 41.211530046727255	eta = 0.5216614208675814
af = 21.498465320302763	bf = 1.69748013632556	zeta = 32.795424289084025	eta = 0.6555324648584754
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.284347894451706	eta = 0.6871955711793989
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.2090722305542	eta = 0.6888530732821788
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.208871892116978	eta = 0.6888574952218328
eta = 0.6888574952218328
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [0.03075603 0.06468531 0.03026784 0.01049611 0.07469322 0.03563794
 0.01318116 0.04369309 0.03173241 0.02880327]
ene_total = [2.70140281 5.11569384 2.67726863 1.23269158 5.83681499 3.09264715
 1.41984069 3.55816331 2.96469034 2.60965854]
ti_comp = [0.33703834 0.33514192 0.33556709 0.34199829 0.33331177 0.33056654
 0.34240076 0.34533101 0.3095942  0.33047606]
ti_coms = [0.07420365 0.07610006 0.0756749  0.0692437  0.07793021 0.08067545
 0.06884123 0.06591098 0.10164779 0.08076593]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.60070873e-05 1.50605001e-04 1.53909571e-05 6.17897996e-07
 2.34434896e-04 2.58880941e-05 1.22087479e-06 4.37166851e-05
 2.08354902e-05 1.36749361e-05]
ene_total = [0.52620567 0.54914919 0.53657281 0.49001957 0.56803143 0.57270002
 0.4872143  0.4694866  0.72074503 0.57247604]
optimize_network_iter = 0 obj = 5.492600646974247
eta = 0.6888574952218328
freqs = [4.56269040e+07 9.65043471e+07 4.50995376e+07 1.53452611e+07
 1.12047077e+08 5.39043395e+07 1.92481414e+07 6.32626258e+07
 5.12483990e+07 4.35784487e+07]
eta_min = 0.6888574952218411	eta_max = 0.6888574952218237
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 0.033943701105718926	eta = 0.909090909090909
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 18.704633302137236	eta = 0.001649746862055949
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8983226351773008	eta = 0.016255355925430464
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8547436567121627	eta = 0.016637291080325793
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8547352592532886	eta = 0.016637366406961725
eta = 0.016637366406961725
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.75913676e-04 1.65510932e-03 1.69142568e-04 6.79053635e-06
 2.57637780e-03 2.84503340e-04 1.34170925e-05 4.80434863e-04
 2.28976553e-04 1.50283948e-04]
ene_total = [0.17007518 0.20743924 0.17321761 0.15518366 0.23216334 0.18699632
 0.15443092 0.1583265  0.23270867 0.18419382]
ti_comp = [0.33703834 0.33514192 0.33556709 0.34199829 0.33331177 0.33056654
 0.34240076 0.34533101 0.3095942  0.33047606]
ti_coms = [0.07420365 0.07610006 0.0756749  0.0692437  0.07793021 0.08067545
 0.06884123 0.06591098 0.10164779 0.08076593]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.60070873e-05 1.50605001e-04 1.53909571e-05 6.17897996e-07
 2.34434896e-04 2.58880941e-05 1.22087479e-06 4.37166851e-05
 2.08354902e-05 1.36749361e-05]
ene_total = [0.52620567 0.54914919 0.53657281 0.49001957 0.56803143 0.57270002
 0.4872143  0.4694866  0.72074503 0.57247604]
optimize_network_iter = 1 obj = 5.492600646974393
eta = 0.6888574952218411
freqs = [4.56269040e+07 9.65043471e+07 4.50995376e+07 1.53452611e+07
 1.12047077e+08 5.39043395e+07 1.92481414e+07 6.32626258e+07
 5.12483990e+07 4.35784487e+07]
Done!
At round 18 eta: 0.6888574952218411
At round 18 local rounds: 12.204774532627562
At round 18 global rounds: 70.7610733753795
At round 18 a_n: 21.674231763837348
gradient difference: 0.46485471725463867
train() client id: f_00000-0-0 loss: 1.299643  [   32/  126]
train() client id: f_00000-0-1 loss: 1.230041  [   64/  126]
train() client id: f_00000-0-2 loss: 1.317457  [   96/  126]
train() client id: f_00000-1-0 loss: 1.054404  [   32/  126]
train() client id: f_00000-1-1 loss: 1.370390  [   64/  126]
train() client id: f_00000-1-2 loss: 1.145209  [   96/  126]
train() client id: f_00000-2-0 loss: 1.135901  [   32/  126]
train() client id: f_00000-2-1 loss: 1.153546  [   64/  126]
train() client id: f_00000-2-2 loss: 1.092676  [   96/  126]
train() client id: f_00000-3-0 loss: 1.110192  [   32/  126]
train() client id: f_00000-3-1 loss: 1.075973  [   64/  126]
train() client id: f_00000-3-2 loss: 1.084966  [   96/  126]
train() client id: f_00000-4-0 loss: 1.046287  [   32/  126]
train() client id: f_00000-4-1 loss: 1.133505  [   64/  126]
train() client id: f_00000-4-2 loss: 0.907897  [   96/  126]
train() client id: f_00000-5-0 loss: 1.066302  [   32/  126]
train() client id: f_00000-5-1 loss: 1.026138  [   64/  126]
train() client id: f_00000-5-2 loss: 0.955495  [   96/  126]
train() client id: f_00000-6-0 loss: 1.071859  [   32/  126]
train() client id: f_00000-6-1 loss: 1.115363  [   64/  126]
train() client id: f_00000-6-2 loss: 0.970085  [   96/  126]
train() client id: f_00000-7-0 loss: 0.992066  [   32/  126]
train() client id: f_00000-7-1 loss: 1.079902  [   64/  126]
train() client id: f_00000-7-2 loss: 0.933913  [   96/  126]
train() client id: f_00000-8-0 loss: 1.079468  [   32/  126]
train() client id: f_00000-8-1 loss: 0.933091  [   64/  126]
train() client id: f_00000-8-2 loss: 0.947924  [   96/  126]
train() client id: f_00000-9-0 loss: 0.946201  [   32/  126]
train() client id: f_00000-9-1 loss: 0.973692  [   64/  126]
train() client id: f_00000-9-2 loss: 1.030042  [   96/  126]
train() client id: f_00000-10-0 loss: 0.942894  [   32/  126]
train() client id: f_00000-10-1 loss: 0.997979  [   64/  126]
train() client id: f_00000-10-2 loss: 0.999798  [   96/  126]
train() client id: f_00000-11-0 loss: 0.974441  [   32/  126]
train() client id: f_00000-11-1 loss: 1.020598  [   64/  126]
train() client id: f_00000-11-2 loss: 1.005180  [   96/  126]
train() client id: f_00001-0-0 loss: 0.534116  [   32/  265]
train() client id: f_00001-0-1 loss: 0.406320  [   64/  265]
train() client id: f_00001-0-2 loss: 0.416419  [   96/  265]
train() client id: f_00001-0-3 loss: 0.536992  [  128/  265]
train() client id: f_00001-0-4 loss: 0.468363  [  160/  265]
train() client id: f_00001-0-5 loss: 0.509066  [  192/  265]
train() client id: f_00001-0-6 loss: 0.480247  [  224/  265]
train() client id: f_00001-0-7 loss: 0.528482  [  256/  265]
train() client id: f_00001-1-0 loss: 0.392342  [   32/  265]
train() client id: f_00001-1-1 loss: 0.494334  [   64/  265]
train() client id: f_00001-1-2 loss: 0.488725  [   96/  265]
train() client id: f_00001-1-3 loss: 0.449147  [  128/  265]
train() client id: f_00001-1-4 loss: 0.391489  [  160/  265]
train() client id: f_00001-1-5 loss: 0.476927  [  192/  265]
train() client id: f_00001-1-6 loss: 0.591404  [  224/  265]
train() client id: f_00001-1-7 loss: 0.480873  [  256/  265]
train() client id: f_00001-2-0 loss: 0.481639  [   32/  265]
train() client id: f_00001-2-1 loss: 0.515251  [   64/  265]
train() client id: f_00001-2-2 loss: 0.453866  [   96/  265]
train() client id: f_00001-2-3 loss: 0.493343  [  128/  265]
train() client id: f_00001-2-4 loss: 0.368479  [  160/  265]
train() client id: f_00001-2-5 loss: 0.446945  [  192/  265]
train() client id: f_00001-2-6 loss: 0.374803  [  224/  265]
train() client id: f_00001-2-7 loss: 0.598590  [  256/  265]
train() client id: f_00001-3-0 loss: 0.447324  [   32/  265]
train() client id: f_00001-3-1 loss: 0.458016  [   64/  265]
train() client id: f_00001-3-2 loss: 0.506154  [   96/  265]
train() client id: f_00001-3-3 loss: 0.443437  [  128/  265]
train() client id: f_00001-3-4 loss: 0.427046  [  160/  265]
train() client id: f_00001-3-5 loss: 0.509761  [  192/  265]
train() client id: f_00001-3-6 loss: 0.523168  [  224/  265]
train() client id: f_00001-3-7 loss: 0.406297  [  256/  265]
train() client id: f_00001-4-0 loss: 0.536222  [   32/  265]
train() client id: f_00001-4-1 loss: 0.368608  [   64/  265]
train() client id: f_00001-4-2 loss: 0.439046  [   96/  265]
train() client id: f_00001-4-3 loss: 0.486691  [  128/  265]
train() client id: f_00001-4-4 loss: 0.491040  [  160/  265]
train() client id: f_00001-4-5 loss: 0.418086  [  192/  265]
train() client id: f_00001-4-6 loss: 0.462952  [  224/  265]
train() client id: f_00001-4-7 loss: 0.480970  [  256/  265]
train() client id: f_00001-5-0 loss: 0.377072  [   32/  265]
train() client id: f_00001-5-1 loss: 0.374328  [   64/  265]
train() client id: f_00001-5-2 loss: 0.570122  [   96/  265]
train() client id: f_00001-5-3 loss: 0.372508  [  128/  265]
train() client id: f_00001-5-4 loss: 0.488396  [  160/  265]
train() client id: f_00001-5-5 loss: 0.393180  [  192/  265]
train() client id: f_00001-5-6 loss: 0.519939  [  224/  265]
train() client id: f_00001-5-7 loss: 0.552907  [  256/  265]
train() client id: f_00001-6-0 loss: 0.416199  [   32/  265]
train() client id: f_00001-6-1 loss: 0.399514  [   64/  265]
train() client id: f_00001-6-2 loss: 0.485396  [   96/  265]
train() client id: f_00001-6-3 loss: 0.560141  [  128/  265]
train() client id: f_00001-6-4 loss: 0.595501  [  160/  265]
train() client id: f_00001-6-5 loss: 0.357635  [  192/  265]
train() client id: f_00001-6-6 loss: 0.386388  [  224/  265]
train() client id: f_00001-6-7 loss: 0.410207  [  256/  265]
train() client id: f_00001-7-0 loss: 0.419449  [   32/  265]
train() client id: f_00001-7-1 loss: 0.422403  [   64/  265]
train() client id: f_00001-7-2 loss: 0.457982  [   96/  265]
train() client id: f_00001-7-3 loss: 0.559074  [  128/  265]
train() client id: f_00001-7-4 loss: 0.538984  [  160/  265]
train() client id: f_00001-7-5 loss: 0.371654  [  192/  265]
train() client id: f_00001-7-6 loss: 0.402491  [  224/  265]
train() client id: f_00001-7-7 loss: 0.440618  [  256/  265]
train() client id: f_00001-8-0 loss: 0.368524  [   32/  265]
train() client id: f_00001-8-1 loss: 0.345395  [   64/  265]
train() client id: f_00001-8-2 loss: 0.403722  [   96/  265]
train() client id: f_00001-8-3 loss: 0.576882  [  128/  265]
train() client id: f_00001-8-4 loss: 0.587122  [  160/  265]
train() client id: f_00001-8-5 loss: 0.374432  [  192/  265]
train() client id: f_00001-8-6 loss: 0.509141  [  224/  265]
train() client id: f_00001-8-7 loss: 0.441539  [  256/  265]
train() client id: f_00001-9-0 loss: 0.566692  [   32/  265]
train() client id: f_00001-9-1 loss: 0.476368  [   64/  265]
train() client id: f_00001-9-2 loss: 0.481367  [   96/  265]
train() client id: f_00001-9-3 loss: 0.403188  [  128/  265]
train() client id: f_00001-9-4 loss: 0.377485  [  160/  265]
train() client id: f_00001-9-5 loss: 0.353794  [  192/  265]
train() client id: f_00001-9-6 loss: 0.424724  [  224/  265]
train() client id: f_00001-9-7 loss: 0.512321  [  256/  265]
train() client id: f_00001-10-0 loss: 0.434182  [   32/  265]
train() client id: f_00001-10-1 loss: 0.538941  [   64/  265]
train() client id: f_00001-10-2 loss: 0.432843  [   96/  265]
train() client id: f_00001-10-3 loss: 0.335467  [  128/  265]
train() client id: f_00001-10-4 loss: 0.415749  [  160/  265]
train() client id: f_00001-10-5 loss: 0.365044  [  192/  265]
train() client id: f_00001-10-6 loss: 0.531066  [  224/  265]
train() client id: f_00001-10-7 loss: 0.537740  [  256/  265]
train() client id: f_00001-11-0 loss: 0.441747  [   32/  265]
train() client id: f_00001-11-1 loss: 0.543491  [   64/  265]
train() client id: f_00001-11-2 loss: 0.400207  [   96/  265]
train() client id: f_00001-11-3 loss: 0.464409  [  128/  265]
train() client id: f_00001-11-4 loss: 0.363502  [  160/  265]
train() client id: f_00001-11-5 loss: 0.347423  [  192/  265]
train() client id: f_00001-11-6 loss: 0.490229  [  224/  265]
train() client id: f_00001-11-7 loss: 0.492239  [  256/  265]
train() client id: f_00002-0-0 loss: 0.922345  [   32/  124]
train() client id: f_00002-0-1 loss: 1.223036  [   64/  124]
train() client id: f_00002-0-2 loss: 1.053756  [   96/  124]
train() client id: f_00002-1-0 loss: 1.005665  [   32/  124]
train() client id: f_00002-1-1 loss: 1.093755  [   64/  124]
train() client id: f_00002-1-2 loss: 1.023203  [   96/  124]
train() client id: f_00002-2-0 loss: 0.995262  [   32/  124]
train() client id: f_00002-2-1 loss: 0.975360  [   64/  124]
train() client id: f_00002-2-2 loss: 1.027843  [   96/  124]
train() client id: f_00002-3-0 loss: 0.894382  [   32/  124]
train() client id: f_00002-3-1 loss: 0.938907  [   64/  124]
train() client id: f_00002-3-2 loss: 1.093148  [   96/  124]
train() client id: f_00002-4-0 loss: 0.973121  [   32/  124]
train() client id: f_00002-4-1 loss: 0.915071  [   64/  124]
train() client id: f_00002-4-2 loss: 0.944202  [   96/  124]
train() client id: f_00002-5-0 loss: 0.878031  [   32/  124]
train() client id: f_00002-5-1 loss: 0.787258  [   64/  124]
train() client id: f_00002-5-2 loss: 1.030718  [   96/  124]
train() client id: f_00002-6-0 loss: 0.851079  [   32/  124]
train() client id: f_00002-6-1 loss: 0.863995  [   64/  124]
train() client id: f_00002-6-2 loss: 0.889638  [   96/  124]
train() client id: f_00002-7-0 loss: 0.832702  [   32/  124]
train() client id: f_00002-7-1 loss: 0.928564  [   64/  124]
train() client id: f_00002-7-2 loss: 0.777709  [   96/  124]
train() client id: f_00002-8-0 loss: 0.987395  [   32/  124]
train() client id: f_00002-8-1 loss: 0.792869  [   64/  124]
train() client id: f_00002-8-2 loss: 0.907528  [   96/  124]
train() client id: f_00002-9-0 loss: 0.836541  [   32/  124]
train() client id: f_00002-9-1 loss: 0.826313  [   64/  124]
train() client id: f_00002-9-2 loss: 0.837186  [   96/  124]
train() client id: f_00002-10-0 loss: 0.834800  [   32/  124]
train() client id: f_00002-10-1 loss: 0.823779  [   64/  124]
train() client id: f_00002-10-2 loss: 0.787091  [   96/  124]
train() client id: f_00002-11-0 loss: 0.948429  [   32/  124]
train() client id: f_00002-11-1 loss: 0.810126  [   64/  124]
train() client id: f_00002-11-2 loss: 0.726568  [   96/  124]
train() client id: f_00003-0-0 loss: 0.729660  [   32/   43]
train() client id: f_00003-1-0 loss: 0.571593  [   32/   43]
train() client id: f_00003-2-0 loss: 0.550599  [   32/   43]
train() client id: f_00003-3-0 loss: 0.596139  [   32/   43]
train() client id: f_00003-4-0 loss: 0.724301  [   32/   43]
train() client id: f_00003-5-0 loss: 0.889286  [   32/   43]
train() client id: f_00003-6-0 loss: 0.580195  [   32/   43]
train() client id: f_00003-7-0 loss: 0.699165  [   32/   43]
train() client id: f_00003-8-0 loss: 0.640562  [   32/   43]
train() client id: f_00003-9-0 loss: 0.730870  [   32/   43]
train() client id: f_00003-10-0 loss: 0.809748  [   32/   43]
train() client id: f_00003-11-0 loss: 0.819229  [   32/   43]
train() client id: f_00004-0-0 loss: 0.778326  [   32/  306]
train() client id: f_00004-0-1 loss: 0.843241  [   64/  306]
train() client id: f_00004-0-2 loss: 1.002394  [   96/  306]
train() client id: f_00004-0-3 loss: 0.791545  [  128/  306]
train() client id: f_00004-0-4 loss: 0.927956  [  160/  306]
train() client id: f_00004-0-5 loss: 0.884261  [  192/  306]
train() client id: f_00004-0-6 loss: 0.829147  [  224/  306]
train() client id: f_00004-0-7 loss: 0.903604  [  256/  306]
train() client id: f_00004-0-8 loss: 0.842139  [  288/  306]
train() client id: f_00004-1-0 loss: 0.918330  [   32/  306]
train() client id: f_00004-1-1 loss: 0.965685  [   64/  306]
train() client id: f_00004-1-2 loss: 0.843914  [   96/  306]
train() client id: f_00004-1-3 loss: 0.701003  [  128/  306]
train() client id: f_00004-1-4 loss: 0.875226  [  160/  306]
train() client id: f_00004-1-5 loss: 0.923417  [  192/  306]
train() client id: f_00004-1-6 loss: 0.842654  [  224/  306]
train() client id: f_00004-1-7 loss: 0.940994  [  256/  306]
train() client id: f_00004-1-8 loss: 0.800100  [  288/  306]
train() client id: f_00004-2-0 loss: 0.751375  [   32/  306]
train() client id: f_00004-2-1 loss: 0.882659  [   64/  306]
train() client id: f_00004-2-2 loss: 0.910050  [   96/  306]
train() client id: f_00004-2-3 loss: 0.875013  [  128/  306]
train() client id: f_00004-2-4 loss: 0.787724  [  160/  306]
train() client id: f_00004-2-5 loss: 0.952089  [  192/  306]
train() client id: f_00004-2-6 loss: 0.883313  [  224/  306]
train() client id: f_00004-2-7 loss: 0.944775  [  256/  306]
train() client id: f_00004-2-8 loss: 0.891653  [  288/  306]
train() client id: f_00004-3-0 loss: 1.041761  [   32/  306]
train() client id: f_00004-3-1 loss: 0.773289  [   64/  306]
train() client id: f_00004-3-2 loss: 0.855716  [   96/  306]
train() client id: f_00004-3-3 loss: 0.942733  [  128/  306]
train() client id: f_00004-3-4 loss: 0.929188  [  160/  306]
train() client id: f_00004-3-5 loss: 0.828401  [  192/  306]
train() client id: f_00004-3-6 loss: 0.811622  [  224/  306]
train() client id: f_00004-3-7 loss: 0.835105  [  256/  306]
train() client id: f_00004-3-8 loss: 0.769411  [  288/  306]
train() client id: f_00004-4-0 loss: 0.852596  [   32/  306]
train() client id: f_00004-4-1 loss: 0.873783  [   64/  306]
train() client id: f_00004-4-2 loss: 0.959680  [   96/  306]
train() client id: f_00004-4-3 loss: 0.772280  [  128/  306]
train() client id: f_00004-4-4 loss: 0.863751  [  160/  306]
train() client id: f_00004-4-5 loss: 0.860744  [  192/  306]
train() client id: f_00004-4-6 loss: 0.977656  [  224/  306]
train() client id: f_00004-4-7 loss: 0.837841  [  256/  306]
train() client id: f_00004-4-8 loss: 0.830415  [  288/  306]
train() client id: f_00004-5-0 loss: 0.887917  [   32/  306]
train() client id: f_00004-5-1 loss: 0.789868  [   64/  306]
train() client id: f_00004-5-2 loss: 0.844173  [   96/  306]
train() client id: f_00004-5-3 loss: 0.863222  [  128/  306]
train() client id: f_00004-5-4 loss: 0.899460  [  160/  306]
train() client id: f_00004-5-5 loss: 0.892312  [  192/  306]
train() client id: f_00004-5-6 loss: 0.872924  [  224/  306]
train() client id: f_00004-5-7 loss: 0.847776  [  256/  306]
train() client id: f_00004-5-8 loss: 0.807187  [  288/  306]
train() client id: f_00004-6-0 loss: 0.799695  [   32/  306]
train() client id: f_00004-6-1 loss: 0.817264  [   64/  306]
train() client id: f_00004-6-2 loss: 0.851785  [   96/  306]
train() client id: f_00004-6-3 loss: 0.853295  [  128/  306]
train() client id: f_00004-6-4 loss: 0.864608  [  160/  306]
train() client id: f_00004-6-5 loss: 0.887635  [  192/  306]
train() client id: f_00004-6-6 loss: 0.861048  [  224/  306]
train() client id: f_00004-6-7 loss: 0.804142  [  256/  306]
train() client id: f_00004-6-8 loss: 0.993793  [  288/  306]
train() client id: f_00004-7-0 loss: 0.844504  [   32/  306]
train() client id: f_00004-7-1 loss: 0.835114  [   64/  306]
train() client id: f_00004-7-2 loss: 0.987836  [   96/  306]
train() client id: f_00004-7-3 loss: 0.873161  [  128/  306]
train() client id: f_00004-7-4 loss: 0.755061  [  160/  306]
train() client id: f_00004-7-5 loss: 0.848359  [  192/  306]
train() client id: f_00004-7-6 loss: 0.816696  [  224/  306]
train() client id: f_00004-7-7 loss: 0.893249  [  256/  306]
train() client id: f_00004-7-8 loss: 0.937768  [  288/  306]
train() client id: f_00004-8-0 loss: 0.886323  [   32/  306]
train() client id: f_00004-8-1 loss: 0.844226  [   64/  306]
train() client id: f_00004-8-2 loss: 0.868843  [   96/  306]
train() client id: f_00004-8-3 loss: 0.806654  [  128/  306]
train() client id: f_00004-8-4 loss: 0.855406  [  160/  306]
train() client id: f_00004-8-5 loss: 0.922862  [  192/  306]
train() client id: f_00004-8-6 loss: 0.858457  [  224/  306]
train() client id: f_00004-8-7 loss: 0.849738  [  256/  306]
train() client id: f_00004-8-8 loss: 0.823089  [  288/  306]
train() client id: f_00004-9-0 loss: 0.839124  [   32/  306]
train() client id: f_00004-9-1 loss: 0.836431  [   64/  306]
train() client id: f_00004-9-2 loss: 0.861505  [   96/  306]
train() client id: f_00004-9-3 loss: 0.874614  [  128/  306]
train() client id: f_00004-9-4 loss: 0.862473  [  160/  306]
train() client id: f_00004-9-5 loss: 0.860808  [  192/  306]
train() client id: f_00004-9-6 loss: 0.949995  [  224/  306]
train() client id: f_00004-9-7 loss: 0.722174  [  256/  306]
train() client id: f_00004-9-8 loss: 0.951044  [  288/  306]
train() client id: f_00004-10-0 loss: 0.989537  [   32/  306]
train() client id: f_00004-10-1 loss: 0.987150  [   64/  306]
train() client id: f_00004-10-2 loss: 0.863469  [   96/  306]
train() client id: f_00004-10-3 loss: 0.809920  [  128/  306]
train() client id: f_00004-10-4 loss: 0.816168  [  160/  306]
train() client id: f_00004-10-5 loss: 0.773585  [  192/  306]
train() client id: f_00004-10-6 loss: 0.826278  [  224/  306]
train() client id: f_00004-10-7 loss: 0.776574  [  256/  306]
train() client id: f_00004-10-8 loss: 0.838746  [  288/  306]
train() client id: f_00004-11-0 loss: 0.909495  [   32/  306]
train() client id: f_00004-11-1 loss: 0.826620  [   64/  306]
train() client id: f_00004-11-2 loss: 0.835711  [   96/  306]
train() client id: f_00004-11-3 loss: 0.798877  [  128/  306]
train() client id: f_00004-11-4 loss: 0.894827  [  160/  306]
train() client id: f_00004-11-5 loss: 0.869635  [  192/  306]
train() client id: f_00004-11-6 loss: 0.818986  [  224/  306]
train() client id: f_00004-11-7 loss: 0.939096  [  256/  306]
train() client id: f_00004-11-8 loss: 0.826996  [  288/  306]
train() client id: f_00005-0-0 loss: 0.802478  [   32/  146]
train() client id: f_00005-0-1 loss: 0.577702  [   64/  146]
train() client id: f_00005-0-2 loss: 0.617842  [   96/  146]
train() client id: f_00005-0-3 loss: 0.799809  [  128/  146]
train() client id: f_00005-1-0 loss: 0.757986  [   32/  146]
train() client id: f_00005-1-1 loss: 0.565914  [   64/  146]
train() client id: f_00005-1-2 loss: 0.633261  [   96/  146]
train() client id: f_00005-1-3 loss: 0.915122  [  128/  146]
train() client id: f_00005-2-0 loss: 0.673043  [   32/  146]
train() client id: f_00005-2-1 loss: 0.551041  [   64/  146]
train() client id: f_00005-2-2 loss: 0.768744  [   96/  146]
train() client id: f_00005-2-3 loss: 0.814150  [  128/  146]
train() client id: f_00005-3-0 loss: 0.606372  [   32/  146]
train() client id: f_00005-3-1 loss: 0.732089  [   64/  146]
train() client id: f_00005-3-2 loss: 0.759170  [   96/  146]
train() client id: f_00005-3-3 loss: 0.674014  [  128/  146]
train() client id: f_00005-4-0 loss: 0.842469  [   32/  146]
train() client id: f_00005-4-1 loss: 0.686714  [   64/  146]
train() client id: f_00005-4-2 loss: 0.567653  [   96/  146]
train() client id: f_00005-4-3 loss: 0.729902  [  128/  146]
train() client id: f_00005-5-0 loss: 0.651320  [   32/  146]
train() client id: f_00005-5-1 loss: 0.607840  [   64/  146]
train() client id: f_00005-5-2 loss: 0.739386  [   96/  146]
train() client id: f_00005-5-3 loss: 0.722401  [  128/  146]
train() client id: f_00005-6-0 loss: 0.758000  [   32/  146]
train() client id: f_00005-6-1 loss: 0.612935  [   64/  146]
train() client id: f_00005-6-2 loss: 0.651909  [   96/  146]
train() client id: f_00005-6-3 loss: 0.778591  [  128/  146]
train() client id: f_00005-7-0 loss: 0.756831  [   32/  146]
train() client id: f_00005-7-1 loss: 0.583883  [   64/  146]
train() client id: f_00005-7-2 loss: 0.827126  [   96/  146]
train() client id: f_00005-7-3 loss: 0.517689  [  128/  146]
train() client id: f_00005-8-0 loss: 0.542443  [   32/  146]
train() client id: f_00005-8-1 loss: 0.701851  [   64/  146]
train() client id: f_00005-8-2 loss: 0.604388  [   96/  146]
train() client id: f_00005-8-3 loss: 0.887318  [  128/  146]
train() client id: f_00005-9-0 loss: 0.777677  [   32/  146]
train() client id: f_00005-9-1 loss: 0.752592  [   64/  146]
train() client id: f_00005-9-2 loss: 0.700966  [   96/  146]
train() client id: f_00005-9-3 loss: 0.558378  [  128/  146]
train() client id: f_00005-10-0 loss: 0.844699  [   32/  146]
train() client id: f_00005-10-1 loss: 0.692819  [   64/  146]
train() client id: f_00005-10-2 loss: 0.614565  [   96/  146]
train() client id: f_00005-10-3 loss: 0.667073  [  128/  146]
train() client id: f_00005-11-0 loss: 0.722178  [   32/  146]
train() client id: f_00005-11-1 loss: 0.694948  [   64/  146]
train() client id: f_00005-11-2 loss: 0.640565  [   96/  146]
train() client id: f_00005-11-3 loss: 0.686783  [  128/  146]
train() client id: f_00006-0-0 loss: 0.555201  [   32/   54]
train() client id: f_00006-1-0 loss: 0.643308  [   32/   54]
train() client id: f_00006-2-0 loss: 0.644423  [   32/   54]
train() client id: f_00006-3-0 loss: 0.640931  [   32/   54]
train() client id: f_00006-4-0 loss: 0.631650  [   32/   54]
train() client id: f_00006-5-0 loss: 0.633825  [   32/   54]
train() client id: f_00006-6-0 loss: 0.572061  [   32/   54]
train() client id: f_00006-7-0 loss: 0.554330  [   32/   54]
train() client id: f_00006-8-0 loss: 0.593230  [   32/   54]
train() client id: f_00006-9-0 loss: 0.593324  [   32/   54]
train() client id: f_00006-10-0 loss: 0.633975  [   32/   54]
train() client id: f_00006-11-0 loss: 0.638598  [   32/   54]
train() client id: f_00007-0-0 loss: 0.833477  [   32/  179]
train() client id: f_00007-0-1 loss: 0.711209  [   64/  179]
train() client id: f_00007-0-2 loss: 0.795739  [   96/  179]
train() client id: f_00007-0-3 loss: 0.686391  [  128/  179]
train() client id: f_00007-0-4 loss: 0.700385  [  160/  179]
train() client id: f_00007-1-0 loss: 0.680923  [   32/  179]
train() client id: f_00007-1-1 loss: 0.642850  [   64/  179]
train() client id: f_00007-1-2 loss: 0.697497  [   96/  179]
train() client id: f_00007-1-3 loss: 0.953524  [  128/  179]
train() client id: f_00007-1-4 loss: 0.689018  [  160/  179]
train() client id: f_00007-2-0 loss: 0.705448  [   32/  179]
train() client id: f_00007-2-1 loss: 0.780668  [   64/  179]
train() client id: f_00007-2-2 loss: 0.678298  [   96/  179]
train() client id: f_00007-2-3 loss: 0.828962  [  128/  179]
train() client id: f_00007-2-4 loss: 0.636152  [  160/  179]
train() client id: f_00007-3-0 loss: 0.790088  [   32/  179]
train() client id: f_00007-3-1 loss: 0.693835  [   64/  179]
train() client id: f_00007-3-2 loss: 0.846515  [   96/  179]
train() client id: f_00007-3-3 loss: 0.711335  [  128/  179]
train() client id: f_00007-3-4 loss: 0.669358  [  160/  179]
train() client id: f_00007-4-0 loss: 0.685721  [   32/  179]
train() client id: f_00007-4-1 loss: 0.857655  [   64/  179]
train() client id: f_00007-4-2 loss: 0.640668  [   96/  179]
train() client id: f_00007-4-3 loss: 0.607137  [  128/  179]
train() client id: f_00007-4-4 loss: 0.772209  [  160/  179]
train() client id: f_00007-5-0 loss: 0.755956  [   32/  179]
train() client id: f_00007-5-1 loss: 0.652989  [   64/  179]
train() client id: f_00007-5-2 loss: 0.826047  [   96/  179]
train() client id: f_00007-5-3 loss: 0.694366  [  128/  179]
train() client id: f_00007-5-4 loss: 0.770515  [  160/  179]
train() client id: f_00007-6-0 loss: 0.586064  [   32/  179]
train() client id: f_00007-6-1 loss: 0.761371  [   64/  179]
train() client id: f_00007-6-2 loss: 0.648262  [   96/  179]
train() client id: f_00007-6-3 loss: 0.741986  [  128/  179]
train() client id: f_00007-6-4 loss: 0.752643  [  160/  179]
train() client id: f_00007-7-0 loss: 0.728631  [   32/  179]
train() client id: f_00007-7-1 loss: 0.573236  [   64/  179]
train() client id: f_00007-7-2 loss: 0.924202  [   96/  179]
train() client id: f_00007-7-3 loss: 0.575329  [  128/  179]
train() client id: f_00007-7-4 loss: 0.842781  [  160/  179]
train() client id: f_00007-8-0 loss: 0.571632  [   32/  179]
train() client id: f_00007-8-1 loss: 0.847558  [   64/  179]
train() client id: f_00007-8-2 loss: 0.708999  [   96/  179]
train() client id: f_00007-8-3 loss: 0.707304  [  128/  179]
train() client id: f_00007-8-4 loss: 0.806681  [  160/  179]
train() client id: f_00007-9-0 loss: 0.601362  [   32/  179]
train() client id: f_00007-9-1 loss: 0.843299  [   64/  179]
train() client id: f_00007-9-2 loss: 0.554373  [   96/  179]
train() client id: f_00007-9-3 loss: 0.866906  [  128/  179]
train() client id: f_00007-9-4 loss: 0.592829  [  160/  179]
train() client id: f_00007-10-0 loss: 0.759468  [   32/  179]
train() client id: f_00007-10-1 loss: 0.723652  [   64/  179]
train() client id: f_00007-10-2 loss: 0.681965  [   96/  179]
train() client id: f_00007-10-3 loss: 0.703682  [  128/  179]
train() client id: f_00007-10-4 loss: 0.766017  [  160/  179]
train() client id: f_00007-11-0 loss: 0.738266  [   32/  179]
train() client id: f_00007-11-1 loss: 0.846540  [   64/  179]
train() client id: f_00007-11-2 loss: 0.616184  [   96/  179]
train() client id: f_00007-11-3 loss: 0.694676  [  128/  179]
train() client id: f_00007-11-4 loss: 0.685607  [  160/  179]
train() client id: f_00008-0-0 loss: 0.598538  [   32/  130]
train() client id: f_00008-0-1 loss: 0.648806  [   64/  130]
train() client id: f_00008-0-2 loss: 0.693570  [   96/  130]
train() client id: f_00008-0-3 loss: 0.625435  [  128/  130]
train() client id: f_00008-1-0 loss: 0.717885  [   32/  130]
train() client id: f_00008-1-1 loss: 0.650650  [   64/  130]
train() client id: f_00008-1-2 loss: 0.589877  [   96/  130]
train() client id: f_00008-1-3 loss: 0.612239  [  128/  130]
train() client id: f_00008-2-0 loss: 0.727803  [   32/  130]
train() client id: f_00008-2-1 loss: 0.657473  [   64/  130]
train() client id: f_00008-2-2 loss: 0.631207  [   96/  130]
train() client id: f_00008-2-3 loss: 0.553601  [  128/  130]
train() client id: f_00008-3-0 loss: 0.616285  [   32/  130]
train() client id: f_00008-3-1 loss: 0.727287  [   64/  130]
train() client id: f_00008-3-2 loss: 0.685880  [   96/  130]
train() client id: f_00008-3-3 loss: 0.527505  [  128/  130]
train() client id: f_00008-4-0 loss: 0.535281  [   32/  130]
train() client id: f_00008-4-1 loss: 0.746724  [   64/  130]
train() client id: f_00008-4-2 loss: 0.601071  [   96/  130]
train() client id: f_00008-4-3 loss: 0.645946  [  128/  130]
train() client id: f_00008-5-0 loss: 0.618424  [   32/  130]
train() client id: f_00008-5-1 loss: 0.572794  [   64/  130]
train() client id: f_00008-5-2 loss: 0.679156  [   96/  130]
train() client id: f_00008-5-3 loss: 0.673300  [  128/  130]
train() client id: f_00008-6-0 loss: 0.607301  [   32/  130]
train() client id: f_00008-6-1 loss: 0.638812  [   64/  130]
train() client id: f_00008-6-2 loss: 0.649279  [   96/  130]
train() client id: f_00008-6-3 loss: 0.652677  [  128/  130]
train() client id: f_00008-7-0 loss: 0.528615  [   32/  130]
train() client id: f_00008-7-1 loss: 0.715488  [   64/  130]
train() client id: f_00008-7-2 loss: 0.639932  [   96/  130]
train() client id: f_00008-7-3 loss: 0.619298  [  128/  130]
train() client id: f_00008-8-0 loss: 0.800748  [   32/  130]
train() client id: f_00008-8-1 loss: 0.651369  [   64/  130]
train() client id: f_00008-8-2 loss: 0.565911  [   96/  130]
train() client id: f_00008-8-3 loss: 0.526682  [  128/  130]
train() client id: f_00008-9-0 loss: 0.613119  [   32/  130]
train() client id: f_00008-9-1 loss: 0.522978  [   64/  130]
train() client id: f_00008-9-2 loss: 0.714537  [   96/  130]
train() client id: f_00008-9-3 loss: 0.684044  [  128/  130]
train() client id: f_00008-10-0 loss: 0.684645  [   32/  130]
train() client id: f_00008-10-1 loss: 0.672148  [   64/  130]
train() client id: f_00008-10-2 loss: 0.644694  [   96/  130]
train() client id: f_00008-10-3 loss: 0.541436  [  128/  130]
train() client id: f_00008-11-0 loss: 0.624720  [   32/  130]
train() client id: f_00008-11-1 loss: 0.661600  [   64/  130]
train() client id: f_00008-11-2 loss: 0.594221  [   96/  130]
train() client id: f_00008-11-3 loss: 0.617024  [  128/  130]
train() client id: f_00009-0-0 loss: 1.222299  [   32/  118]
train() client id: f_00009-0-1 loss: 1.107572  [   64/  118]
train() client id: f_00009-0-2 loss: 1.137733  [   96/  118]
train() client id: f_00009-1-0 loss: 1.214442  [   32/  118]
train() client id: f_00009-1-1 loss: 1.026644  [   64/  118]
train() client id: f_00009-1-2 loss: 1.106668  [   96/  118]
train() client id: f_00009-2-0 loss: 1.124810  [   32/  118]
train() client id: f_00009-2-1 loss: 1.146539  [   64/  118]
train() client id: f_00009-2-2 loss: 0.952220  [   96/  118]
train() client id: f_00009-3-0 loss: 1.064676  [   32/  118]
train() client id: f_00009-3-1 loss: 1.191273  [   64/  118]
train() client id: f_00009-3-2 loss: 0.974397  [   96/  118]
train() client id: f_00009-4-0 loss: 1.004550  [   32/  118]
train() client id: f_00009-4-1 loss: 1.093122  [   64/  118]
train() client id: f_00009-4-2 loss: 0.990089  [   96/  118]
train() client id: f_00009-5-0 loss: 1.048494  [   32/  118]
train() client id: f_00009-5-1 loss: 0.990213  [   64/  118]
train() client id: f_00009-5-2 loss: 0.980401  [   96/  118]
train() client id: f_00009-6-0 loss: 0.987246  [   32/  118]
train() client id: f_00009-6-1 loss: 0.917508  [   64/  118]
train() client id: f_00009-6-2 loss: 1.093124  [   96/  118]
train() client id: f_00009-7-0 loss: 1.006893  [   32/  118]
train() client id: f_00009-7-1 loss: 1.004464  [   64/  118]
train() client id: f_00009-7-2 loss: 0.889315  [   96/  118]
train() client id: f_00009-8-0 loss: 0.881611  [   32/  118]
train() client id: f_00009-8-1 loss: 0.988485  [   64/  118]
train() client id: f_00009-8-2 loss: 0.919796  [   96/  118]
train() client id: f_00009-9-0 loss: 0.851711  [   32/  118]
train() client id: f_00009-9-1 loss: 0.944836  [   64/  118]
train() client id: f_00009-9-2 loss: 0.983076  [   96/  118]
train() client id: f_00009-10-0 loss: 0.904585  [   32/  118]
train() client id: f_00009-10-1 loss: 0.914700  [   64/  118]
train() client id: f_00009-10-2 loss: 1.123075  [   96/  118]
train() client id: f_00009-11-0 loss: 1.012465  [   32/  118]
train() client id: f_00009-11-1 loss: 0.919331  [   64/  118]
train() client id: f_00009-11-2 loss: 0.893218  [   96/  118]
At round 18 accuracy: 0.636604774535809
At round 18 training accuracy: 0.5814889336016097
At round 18 training loss: 0.838103676357313
update_location
xs = [ -3.9056584    4.20031788 110.00902392  18.81129433   0.97929623
   3.95640986 -72.44319194 -51.32485185  94.66397685 -37.06087855]
ys = [102.5879595   85.55583871   1.32061395 -72.45517586  64.35018685
  47.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [143.31623635 131.67172896 148.67323015 124.91443993 118.91974423
 110.91368802 123.510755   112.40514523 138.81620397 106.72169684]
dists_bs = [185.78691379 200.28233051 333.75840492 314.31971282 207.87518816
 219.54871187 205.10615284 213.61974799 312.25498919 219.69909981]
uav_gains = [4.06456051e-11 5.02562998e-11 3.70692179e-11 5.73364491e-11
 6.48404832e-11 7.71845050e-11 5.89801623e-11 7.46493522e-11
 4.40289659e-11 8.49892801e-11]
bs_gains = [4.89837182e-11 3.96912434e-11 9.49913137e-12 1.12370821e-11
 3.57640465e-11 3.06907558e-11 3.71324630e-11 3.31358541e-11
 1.14463706e-11 3.06319686e-11]
Round 19
-------------------------------
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.48419946 17.66297686  8.35544877  2.99255128 20.37392527  9.81527082
  3.718002   11.96472075  8.80869105  7.96655398]
obj_prev = 100.14234023128081
eta_min = 1.126497835179761e-11	eta_max = 0.9220293976525566
af = 21.163983583608065	bf = 1.67717061299475	zeta = 23.280381941968873	eta = 0.9090909090909091
af = 21.163983583608065	bf = 1.67717061299475	zeta = 40.637450630966356	eta = 0.5207999826514902
af = 21.163983583608065	bf = 1.67717061299475	zeta = 32.312864825181585	eta = 0.6549708203871438
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.81773875501804	eta = 0.6867468035811661
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.743104385934746	eta = 0.6884140039315868
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.74290487459993	eta = 0.6884184715119078
eta = 0.6884184715119078
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [0.03080864 0.06479595 0.03031961 0.01051406 0.07482098 0.0356989
 0.0132037  0.04376783 0.03178669 0.02885253]
ene_total = [2.66619823 5.03309398 2.64268475 1.21857109 5.74248846 3.03975216
 1.40293291 3.50736005 2.92607578 2.56374747]
ti_comp = [0.34241459 0.34203063 0.34090455 0.34757484 0.34030078 0.33761632
 0.34796803 0.35108386 0.31483598 0.33758153]
ti_coms = [0.07519738 0.07558134 0.07670742 0.07003712 0.07731119 0.07999565
 0.06964393 0.0665281  0.10277599 0.08003044]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.55880946e-05 1.45342807e-04 1.49894390e-05 6.01304716e-07
 2.26060309e-04 2.49458102e-05 1.18819712e-06 4.25131485e-05
 2.02510366e-05 1.31726936e-05]
ene_total = [0.52417228 0.53586919 0.53463478 0.48723401 0.55351726 0.55820092
 0.48453973 0.46574004 0.71633895 0.557624  ]
optimize_network_iter = 0 obj = 5.417871154985847
eta = 0.6884184715119078
freqs = [4.49873339e+07 9.47224336e+07 4.44693580e+07 1.51248853e+07
 1.09933602e+08 5.28690360e+07 1.89725795e+07 6.23324407e+07
 5.04813506e+07 4.27341730e+07]
eta_min = 0.6884184715119183	eta_max = 0.6884184715118975
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 0.03222576338096967	eta = 0.909090909090909
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 18.47959117596394	eta = 0.0015853244938805354
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.8690048705094464	eta = 0.015674730970695012
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.8275633379854408	eta = 0.016030168650925096
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.827555909002531	eta = 0.016030233813281204
eta = 0.016030233813281204
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.72568617e-04 1.60902329e-03 1.65941177e-04 6.65676762e-06
 2.50260959e-03 2.76163578e-04 1.31539832e-05 4.70643491e-04
 2.24189901e-04 1.45828825e-04]
ene_total = [0.16944101 0.20192804 0.17262124 0.15441975 0.22542177 0.18229224
 0.15369677 0.1569107  0.23132642 0.17949795]
ti_comp = [0.34241459 0.34203063 0.34090455 0.34757484 0.34030078 0.33761632
 0.34796803 0.35108386 0.31483598 0.33758153]
ti_coms = [0.07519738 0.07558134 0.07670742 0.07003712 0.07731119 0.07999565
 0.06964393 0.0665281  0.10277599 0.08003044]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.55880946e-05 1.45342807e-04 1.49894390e-05 6.01304716e-07
 2.26060309e-04 2.49458102e-05 1.18819712e-06 4.25131485e-05
 2.02510366e-05 1.31726936e-05]
ene_total = [0.52417228 0.53586919 0.53463478 0.48723401 0.55351726 0.55820092
 0.48453973 0.46574004 0.71633895 0.557624  ]
optimize_network_iter = 1 obj = 5.4178711549860274
eta = 0.6884184715119183
freqs = [4.49873339e+07 9.47224336e+07 4.44693580e+07 1.51248853e+07
 1.09933602e+08 5.28690360e+07 1.89725795e+07 6.23324407e+07
 5.04813506e+07 4.27341730e+07]
Done!
At round 19 eta: 0.6884184715119183
At round 19 local rounds: 12.225650330368824
At round 19 global rounds: 69.56199190949924
At round 19 a_n: 21.33168591686803
gradient difference: 0.49045881628990173
train() client id: f_00000-0-0 loss: 1.222404  [   32/  126]
train() client id: f_00000-0-1 loss: 1.155581  [   64/  126]
train() client id: f_00000-0-2 loss: 1.203344  [   96/  126]
train() client id: f_00000-1-0 loss: 1.052922  [   32/  126]
train() client id: f_00000-1-1 loss: 1.056637  [   64/  126]
train() client id: f_00000-1-2 loss: 1.035384  [   96/  126]
train() client id: f_00000-2-0 loss: 1.066961  [   32/  126]
train() client id: f_00000-2-1 loss: 0.962664  [   64/  126]
train() client id: f_00000-2-2 loss: 0.952349  [   96/  126]
train() client id: f_00000-3-0 loss: 1.022591  [   32/  126]
train() client id: f_00000-3-1 loss: 1.025257  [   64/  126]
train() client id: f_00000-3-2 loss: 0.884062  [   96/  126]
train() client id: f_00000-4-0 loss: 0.962379  [   32/  126]
train() client id: f_00000-4-1 loss: 0.933396  [   64/  126]
train() client id: f_00000-4-2 loss: 0.927784  [   96/  126]
train() client id: f_00000-5-0 loss: 0.957160  [   32/  126]
train() client id: f_00000-5-1 loss: 0.881550  [   64/  126]
train() client id: f_00000-5-2 loss: 0.908432  [   96/  126]
train() client id: f_00000-6-0 loss: 0.873709  [   32/  126]
train() client id: f_00000-6-1 loss: 0.876130  [   64/  126]
train() client id: f_00000-6-2 loss: 0.892311  [   96/  126]
train() client id: f_00000-7-0 loss: 0.942504  [   32/  126]
train() client id: f_00000-7-1 loss: 0.885165  [   64/  126]
train() client id: f_00000-7-2 loss: 0.872545  [   96/  126]
train() client id: f_00000-8-0 loss: 0.957841  [   32/  126]
train() client id: f_00000-8-1 loss: 0.883318  [   64/  126]
train() client id: f_00000-8-2 loss: 0.859959  [   96/  126]
train() client id: f_00000-9-0 loss: 0.927612  [   32/  126]
train() client id: f_00000-9-1 loss: 0.891649  [   64/  126]
train() client id: f_00000-9-2 loss: 0.931325  [   96/  126]
train() client id: f_00000-10-0 loss: 0.842951  [   32/  126]
train() client id: f_00000-10-1 loss: 0.840068  [   64/  126]
train() client id: f_00000-10-2 loss: 0.998193  [   96/  126]
train() client id: f_00000-11-0 loss: 0.921864  [   32/  126]
train() client id: f_00000-11-1 loss: 0.988728  [   64/  126]
train() client id: f_00000-11-2 loss: 0.790933  [   96/  126]
train() client id: f_00001-0-0 loss: 0.511583  [   32/  265]
train() client id: f_00001-0-1 loss: 0.513738  [   64/  265]
train() client id: f_00001-0-2 loss: 0.436894  [   96/  265]
train() client id: f_00001-0-3 loss: 0.566506  [  128/  265]
train() client id: f_00001-0-4 loss: 0.441068  [  160/  265]
train() client id: f_00001-0-5 loss: 0.374094  [  192/  265]
train() client id: f_00001-0-6 loss: 0.502566  [  224/  265]
train() client id: f_00001-0-7 loss: 0.393143  [  256/  265]
train() client id: f_00001-1-0 loss: 0.493665  [   32/  265]
train() client id: f_00001-1-1 loss: 0.488532  [   64/  265]
train() client id: f_00001-1-2 loss: 0.424155  [   96/  265]
train() client id: f_00001-1-3 loss: 0.503436  [  128/  265]
train() client id: f_00001-1-4 loss: 0.544086  [  160/  265]
train() client id: f_00001-1-5 loss: 0.364137  [  192/  265]
train() client id: f_00001-1-6 loss: 0.409570  [  224/  265]
train() client id: f_00001-1-7 loss: 0.438220  [  256/  265]
train() client id: f_00001-2-0 loss: 0.409723  [   32/  265]
train() client id: f_00001-2-1 loss: 0.512871  [   64/  265]
train() client id: f_00001-2-2 loss: 0.412745  [   96/  265]
train() client id: f_00001-2-3 loss: 0.496623  [  128/  265]
train() client id: f_00001-2-4 loss: 0.422222  [  160/  265]
train() client id: f_00001-2-5 loss: 0.460560  [  192/  265]
train() client id: f_00001-2-6 loss: 0.422519  [  224/  265]
train() client id: f_00001-2-7 loss: 0.472650  [  256/  265]
train() client id: f_00001-3-0 loss: 0.373857  [   32/  265]
train() client id: f_00001-3-1 loss: 0.462039  [   64/  265]
train() client id: f_00001-3-2 loss: 0.397629  [   96/  265]
train() client id: f_00001-3-3 loss: 0.402460  [  128/  265]
train() client id: f_00001-3-4 loss: 0.389202  [  160/  265]
train() client id: f_00001-3-5 loss: 0.497320  [  192/  265]
train() client id: f_00001-3-6 loss: 0.524816  [  224/  265]
train() client id: f_00001-3-7 loss: 0.467297  [  256/  265]
train() client id: f_00001-4-0 loss: 0.484536  [   32/  265]
train() client id: f_00001-4-1 loss: 0.470778  [   64/  265]
train() client id: f_00001-4-2 loss: 0.472191  [   96/  265]
train() client id: f_00001-4-3 loss: 0.433536  [  128/  265]
train() client id: f_00001-4-4 loss: 0.405978  [  160/  265]
train() client id: f_00001-4-5 loss: 0.327647  [  192/  265]
train() client id: f_00001-4-6 loss: 0.398220  [  224/  265]
train() client id: f_00001-4-7 loss: 0.529953  [  256/  265]
train() client id: f_00001-5-0 loss: 0.379549  [   32/  265]
train() client id: f_00001-5-1 loss: 0.341852  [   64/  265]
train() client id: f_00001-5-2 loss: 0.387413  [   96/  265]
train() client id: f_00001-5-3 loss: 0.571617  [  128/  265]
train() client id: f_00001-5-4 loss: 0.338542  [  160/  265]
train() client id: f_00001-5-5 loss: 0.335780  [  192/  265]
train() client id: f_00001-5-6 loss: 0.526060  [  224/  265]
train() client id: f_00001-5-7 loss: 0.543115  [  256/  265]
train() client id: f_00001-6-0 loss: 0.365417  [   32/  265]
train() client id: f_00001-6-1 loss: 0.612802  [   64/  265]
train() client id: f_00001-6-2 loss: 0.351767  [   96/  265]
train() client id: f_00001-6-3 loss: 0.342931  [  128/  265]
train() client id: f_00001-6-4 loss: 0.339490  [  160/  265]
train() client id: f_00001-6-5 loss: 0.392077  [  192/  265]
train() client id: f_00001-6-6 loss: 0.579560  [  224/  265]
train() client id: f_00001-6-7 loss: 0.493070  [  256/  265]
train() client id: f_00001-7-0 loss: 0.435812  [   32/  265]
train() client id: f_00001-7-1 loss: 0.345498  [   64/  265]
train() client id: f_00001-7-2 loss: 0.538888  [   96/  265]
train() client id: f_00001-7-3 loss: 0.383391  [  128/  265]
train() client id: f_00001-7-4 loss: 0.552143  [  160/  265]
train() client id: f_00001-7-5 loss: 0.475228  [  192/  265]
train() client id: f_00001-7-6 loss: 0.383697  [  224/  265]
train() client id: f_00001-7-7 loss: 0.342594  [  256/  265]
train() client id: f_00001-8-0 loss: 0.331077  [   32/  265]
train() client id: f_00001-8-1 loss: 0.400799  [   64/  265]
train() client id: f_00001-8-2 loss: 0.359227  [   96/  265]
train() client id: f_00001-8-3 loss: 0.532781  [  128/  265]
train() client id: f_00001-8-4 loss: 0.402879  [  160/  265]
train() client id: f_00001-8-5 loss: 0.536805  [  192/  265]
train() client id: f_00001-8-6 loss: 0.484420  [  224/  265]
train() client id: f_00001-8-7 loss: 0.401482  [  256/  265]
train() client id: f_00001-9-0 loss: 0.419986  [   32/  265]
train() client id: f_00001-9-1 loss: 0.354618  [   64/  265]
train() client id: f_00001-9-2 loss: 0.544890  [   96/  265]
train() client id: f_00001-9-3 loss: 0.342949  [  128/  265]
train() client id: f_00001-9-4 loss: 0.508416  [  160/  265]
train() client id: f_00001-9-5 loss: 0.470055  [  192/  265]
train() client id: f_00001-9-6 loss: 0.457282  [  224/  265]
train() client id: f_00001-9-7 loss: 0.337456  [  256/  265]
train() client id: f_00001-10-0 loss: 0.407956  [   32/  265]
train() client id: f_00001-10-1 loss: 0.619148  [   64/  265]
train() client id: f_00001-10-2 loss: 0.341588  [   96/  265]
train() client id: f_00001-10-3 loss: 0.452932  [  128/  265]
train() client id: f_00001-10-4 loss: 0.405146  [  160/  265]
train() client id: f_00001-10-5 loss: 0.401224  [  192/  265]
train() client id: f_00001-10-6 loss: 0.461353  [  224/  265]
train() client id: f_00001-10-7 loss: 0.332467  [  256/  265]
train() client id: f_00001-11-0 loss: 0.394556  [   32/  265]
train() client id: f_00001-11-1 loss: 0.382007  [   64/  265]
train() client id: f_00001-11-2 loss: 0.555021  [   96/  265]
train() client id: f_00001-11-3 loss: 0.443177  [  128/  265]
train() client id: f_00001-11-4 loss: 0.331917  [  160/  265]
train() client id: f_00001-11-5 loss: 0.464915  [  192/  265]
train() client id: f_00001-11-6 loss: 0.436184  [  224/  265]
train() client id: f_00001-11-7 loss: 0.424949  [  256/  265]
train() client id: f_00002-0-0 loss: 1.252568  [   32/  124]
train() client id: f_00002-0-1 loss: 1.224328  [   64/  124]
train() client id: f_00002-0-2 loss: 1.226573  [   96/  124]
train() client id: f_00002-1-0 loss: 1.173818  [   32/  124]
train() client id: f_00002-1-1 loss: 1.251304  [   64/  124]
train() client id: f_00002-1-2 loss: 1.132305  [   96/  124]
train() client id: f_00002-2-0 loss: 1.229942  [   32/  124]
train() client id: f_00002-2-1 loss: 1.138859  [   64/  124]
train() client id: f_00002-2-2 loss: 1.129576  [   96/  124]
train() client id: f_00002-3-0 loss: 1.201174  [   32/  124]
train() client id: f_00002-3-1 loss: 1.065505  [   64/  124]
train() client id: f_00002-3-2 loss: 1.151987  [   96/  124]
train() client id: f_00002-4-0 loss: 1.155261  [   32/  124]
train() client id: f_00002-4-1 loss: 1.155970  [   64/  124]
train() client id: f_00002-4-2 loss: 1.106679  [   96/  124]
train() client id: f_00002-5-0 loss: 1.150195  [   32/  124]
train() client id: f_00002-5-1 loss: 1.072694  [   64/  124]
train() client id: f_00002-5-2 loss: 1.219505  [   96/  124]
train() client id: f_00002-6-0 loss: 1.113111  [   32/  124]
train() client id: f_00002-6-1 loss: 1.120249  [   64/  124]
train() client id: f_00002-6-2 loss: 1.108543  [   96/  124]
train() client id: f_00002-7-0 loss: 1.056414  [   32/  124]
train() client id: f_00002-7-1 loss: 1.101461  [   64/  124]
train() client id: f_00002-7-2 loss: 1.141324  [   96/  124]
train() client id: f_00002-8-0 loss: 1.164289  [   32/  124]
train() client id: f_00002-8-1 loss: 1.231260  [   64/  124]
train() client id: f_00002-8-2 loss: 0.953867  [   96/  124]
train() client id: f_00002-9-0 loss: 1.113691  [   32/  124]
train() client id: f_00002-9-1 loss: 1.159854  [   64/  124]
train() client id: f_00002-9-2 loss: 0.981309  [   96/  124]
train() client id: f_00002-10-0 loss: 1.195599  [   32/  124]
train() client id: f_00002-10-1 loss: 1.094969  [   64/  124]
train() client id: f_00002-10-2 loss: 1.040481  [   96/  124]
train() client id: f_00002-11-0 loss: 1.046028  [   32/  124]
train() client id: f_00002-11-1 loss: 1.119341  [   64/  124]
train() client id: f_00002-11-2 loss: 1.013059  [   96/  124]
train() client id: f_00003-0-0 loss: 0.658358  [   32/   43]
train() client id: f_00003-1-0 loss: 0.521443  [   32/   43]
train() client id: f_00003-2-0 loss: 0.616331  [   32/   43]
train() client id: f_00003-3-0 loss: 0.622183  [   32/   43]
train() client id: f_00003-4-0 loss: 0.706889  [   32/   43]
train() client id: f_00003-5-0 loss: 0.789494  [   32/   43]
train() client id: f_00003-6-0 loss: 0.705047  [   32/   43]
train() client id: f_00003-7-0 loss: 0.829189  [   32/   43]
train() client id: f_00003-8-0 loss: 0.706312  [   32/   43]
train() client id: f_00003-9-0 loss: 0.822458  [   32/   43]
train() client id: f_00003-10-0 loss: 0.583463  [   32/   43]
train() client id: f_00003-11-0 loss: 0.711043  [   32/   43]
train() client id: f_00004-0-0 loss: 0.965107  [   32/  306]
train() client id: f_00004-0-1 loss: 0.939552  [   64/  306]
train() client id: f_00004-0-2 loss: 1.001960  [   96/  306]
train() client id: f_00004-0-3 loss: 1.040030  [  128/  306]
train() client id: f_00004-0-4 loss: 0.814900  [  160/  306]
train() client id: f_00004-0-5 loss: 0.799555  [  192/  306]
train() client id: f_00004-0-6 loss: 0.798657  [  224/  306]
train() client id: f_00004-0-7 loss: 0.909372  [  256/  306]
train() client id: f_00004-0-8 loss: 0.850689  [  288/  306]
train() client id: f_00004-1-0 loss: 0.930530  [   32/  306]
train() client id: f_00004-1-1 loss: 0.757764  [   64/  306]
train() client id: f_00004-1-2 loss: 0.898733  [   96/  306]
train() client id: f_00004-1-3 loss: 0.978134  [  128/  306]
train() client id: f_00004-1-4 loss: 0.864737  [  160/  306]
train() client id: f_00004-1-5 loss: 0.892780  [  192/  306]
train() client id: f_00004-1-6 loss: 0.876308  [  224/  306]
train() client id: f_00004-1-7 loss: 0.790214  [  256/  306]
train() client id: f_00004-1-8 loss: 0.981505  [  288/  306]
train() client id: f_00004-2-0 loss: 0.839829  [   32/  306]
train() client id: f_00004-2-1 loss: 0.924916  [   64/  306]
train() client id: f_00004-2-2 loss: 0.926437  [   96/  306]
train() client id: f_00004-2-3 loss: 0.838044  [  128/  306]
train() client id: f_00004-2-4 loss: 0.814273  [  160/  306]
train() client id: f_00004-2-5 loss: 0.855291  [  192/  306]
train() client id: f_00004-2-6 loss: 1.005430  [  224/  306]
train() client id: f_00004-2-7 loss: 0.944265  [  256/  306]
train() client id: f_00004-2-8 loss: 0.870482  [  288/  306]
train() client id: f_00004-3-0 loss: 0.959379  [   32/  306]
train() client id: f_00004-3-1 loss: 0.874583  [   64/  306]
train() client id: f_00004-3-2 loss: 0.928829  [   96/  306]
train() client id: f_00004-3-3 loss: 0.801067  [  128/  306]
train() client id: f_00004-3-4 loss: 0.731739  [  160/  306]
train() client id: f_00004-3-5 loss: 0.932396  [  192/  306]
train() client id: f_00004-3-6 loss: 0.823310  [  224/  306]
train() client id: f_00004-3-7 loss: 0.996301  [  256/  306]
train() client id: f_00004-3-8 loss: 0.981067  [  288/  306]
train() client id: f_00004-4-0 loss: 0.888138  [   32/  306]
train() client id: f_00004-4-1 loss: 0.904581  [   64/  306]
train() client id: f_00004-4-2 loss: 0.767477  [   96/  306]
train() client id: f_00004-4-3 loss: 0.819172  [  128/  306]
train() client id: f_00004-4-4 loss: 0.991086  [  160/  306]
train() client id: f_00004-4-5 loss: 0.742933  [  192/  306]
train() client id: f_00004-4-6 loss: 0.909765  [  224/  306]
train() client id: f_00004-4-7 loss: 0.989268  [  256/  306]
train() client id: f_00004-4-8 loss: 1.009184  [  288/  306]
train() client id: f_00004-5-0 loss: 0.808039  [   32/  306]
train() client id: f_00004-5-1 loss: 0.962387  [   64/  306]
train() client id: f_00004-5-2 loss: 0.789115  [   96/  306]
train() client id: f_00004-5-3 loss: 0.922517  [  128/  306]
train() client id: f_00004-5-4 loss: 0.938586  [  160/  306]
train() client id: f_00004-5-5 loss: 0.944111  [  192/  306]
train() client id: f_00004-5-6 loss: 0.920362  [  224/  306]
train() client id: f_00004-5-7 loss: 0.792775  [  256/  306]
train() client id: f_00004-5-8 loss: 0.913582  [  288/  306]
train() client id: f_00004-6-0 loss: 0.802358  [   32/  306]
train() client id: f_00004-6-1 loss: 0.865399  [   64/  306]
train() client id: f_00004-6-2 loss: 0.972169  [   96/  306]
train() client id: f_00004-6-3 loss: 1.050335  [  128/  306]
train() client id: f_00004-6-4 loss: 0.873062  [  160/  306]
train() client id: f_00004-6-5 loss: 0.907963  [  192/  306]
train() client id: f_00004-6-6 loss: 0.832471  [  224/  306]
train() client id: f_00004-6-7 loss: 0.793671  [  256/  306]
train() client id: f_00004-6-8 loss: 0.885784  [  288/  306]
train() client id: f_00004-7-0 loss: 0.896750  [   32/  306]
train() client id: f_00004-7-1 loss: 0.768105  [   64/  306]
train() client id: f_00004-7-2 loss: 0.983203  [   96/  306]
train() client id: f_00004-7-3 loss: 0.858936  [  128/  306]
train() client id: f_00004-7-4 loss: 0.933712  [  160/  306]
train() client id: f_00004-7-5 loss: 0.882136  [  192/  306]
train() client id: f_00004-7-6 loss: 0.891383  [  224/  306]
train() client id: f_00004-7-7 loss: 0.983723  [  256/  306]
train() client id: f_00004-7-8 loss: 0.813478  [  288/  306]
train() client id: f_00004-8-0 loss: 0.903979  [   32/  306]
train() client id: f_00004-8-1 loss: 0.796172  [   64/  306]
train() client id: f_00004-8-2 loss: 0.942160  [   96/  306]
train() client id: f_00004-8-3 loss: 0.804225  [  128/  306]
train() client id: f_00004-8-4 loss: 0.862821  [  160/  306]
train() client id: f_00004-8-5 loss: 0.941791  [  192/  306]
train() client id: f_00004-8-6 loss: 0.994839  [  224/  306]
train() client id: f_00004-8-7 loss: 0.852210  [  256/  306]
train() client id: f_00004-8-8 loss: 0.928313  [  288/  306]
train() client id: f_00004-9-0 loss: 1.002724  [   32/  306]
train() client id: f_00004-9-1 loss: 1.016982  [   64/  306]
train() client id: f_00004-9-2 loss: 0.771051  [   96/  306]
train() client id: f_00004-9-3 loss: 0.960017  [  128/  306]
train() client id: f_00004-9-4 loss: 0.914519  [  160/  306]
train() client id: f_00004-9-5 loss: 0.872152  [  192/  306]
train() client id: f_00004-9-6 loss: 0.824090  [  224/  306]
train() client id: f_00004-9-7 loss: 0.853560  [  256/  306]
train() client id: f_00004-9-8 loss: 0.855311  [  288/  306]
train() client id: f_00004-10-0 loss: 0.838632  [   32/  306]
train() client id: f_00004-10-1 loss: 0.889815  [   64/  306]
train() client id: f_00004-10-2 loss: 0.899484  [   96/  306]
train() client id: f_00004-10-3 loss: 0.773571  [  128/  306]
train() client id: f_00004-10-4 loss: 1.003038  [  160/  306]
train() client id: f_00004-10-5 loss: 0.951738  [  192/  306]
train() client id: f_00004-10-6 loss: 0.857344  [  224/  306]
train() client id: f_00004-10-7 loss: 0.818261  [  256/  306]
train() client id: f_00004-10-8 loss: 0.925562  [  288/  306]
train() client id: f_00004-11-0 loss: 0.927176  [   32/  306]
train() client id: f_00004-11-1 loss: 0.795009  [   64/  306]
train() client id: f_00004-11-2 loss: 0.904899  [   96/  306]
train() client id: f_00004-11-3 loss: 0.953348  [  128/  306]
train() client id: f_00004-11-4 loss: 0.888110  [  160/  306]
train() client id: f_00004-11-5 loss: 0.868379  [  192/  306]
train() client id: f_00004-11-6 loss: 0.946048  [  224/  306]
train() client id: f_00004-11-7 loss: 0.879327  [  256/  306]
train() client id: f_00004-11-8 loss: 0.913226  [  288/  306]
train() client id: f_00005-0-0 loss: 0.780035  [   32/  146]
train() client id: f_00005-0-1 loss: 0.745662  [   64/  146]
train() client id: f_00005-0-2 loss: 0.488870  [   96/  146]
train() client id: f_00005-0-3 loss: 0.712100  [  128/  146]
train() client id: f_00005-1-0 loss: 0.645970  [   32/  146]
train() client id: f_00005-1-1 loss: 0.423106  [   64/  146]
train() client id: f_00005-1-2 loss: 0.721070  [   96/  146]
train() client id: f_00005-1-3 loss: 0.772759  [  128/  146]
train() client id: f_00005-2-0 loss: 0.686306  [   32/  146]
train() client id: f_00005-2-1 loss: 0.622119  [   64/  146]
train() client id: f_00005-2-2 loss: 0.845765  [   96/  146]
train() client id: f_00005-2-3 loss: 0.664228  [  128/  146]
train() client id: f_00005-3-0 loss: 0.856114  [   32/  146]
train() client id: f_00005-3-1 loss: 0.634167  [   64/  146]
train() client id: f_00005-3-2 loss: 0.594184  [   96/  146]
train() client id: f_00005-3-3 loss: 0.635084  [  128/  146]
train() client id: f_00005-4-0 loss: 0.560940  [   32/  146]
train() client id: f_00005-4-1 loss: 0.838138  [   64/  146]
train() client id: f_00005-4-2 loss: 0.766500  [   96/  146]
train() client id: f_00005-4-3 loss: 0.585454  [  128/  146]
train() client id: f_00005-5-0 loss: 0.885823  [   32/  146]
train() client id: f_00005-5-1 loss: 0.659473  [   64/  146]
train() client id: f_00005-5-2 loss: 0.627510  [   96/  146]
train() client id: f_00005-5-3 loss: 0.644830  [  128/  146]
train() client id: f_00005-6-0 loss: 0.524687  [   32/  146]
train() client id: f_00005-6-1 loss: 0.761468  [   64/  146]
train() client id: f_00005-6-2 loss: 0.811535  [   96/  146]
train() client id: f_00005-6-3 loss: 0.703516  [  128/  146]
train() client id: f_00005-7-0 loss: 0.705686  [   32/  146]
train() client id: f_00005-7-1 loss: 0.807088  [   64/  146]
train() client id: f_00005-7-2 loss: 0.513547  [   96/  146]
train() client id: f_00005-7-3 loss: 0.673394  [  128/  146]
train() client id: f_00005-8-0 loss: 0.540411  [   32/  146]
train() client id: f_00005-8-1 loss: 0.550633  [   64/  146]
train() client id: f_00005-8-2 loss: 0.869204  [   96/  146]
train() client id: f_00005-8-3 loss: 0.830672  [  128/  146]
train() client id: f_00005-9-0 loss: 0.733395  [   32/  146]
train() client id: f_00005-9-1 loss: 0.586746  [   64/  146]
train() client id: f_00005-9-2 loss: 0.554724  [   96/  146]
train() client id: f_00005-9-3 loss: 0.823521  [  128/  146]
train() client id: f_00005-10-0 loss: 0.573821  [   32/  146]
train() client id: f_00005-10-1 loss: 0.790812  [   64/  146]
train() client id: f_00005-10-2 loss: 0.766737  [   96/  146]
train() client id: f_00005-10-3 loss: 0.698997  [  128/  146]
train() client id: f_00005-11-0 loss: 0.509918  [   32/  146]
train() client id: f_00005-11-1 loss: 0.695155  [   64/  146]
train() client id: f_00005-11-2 loss: 0.669778  [   96/  146]
train() client id: f_00005-11-3 loss: 0.724289  [  128/  146]
train() client id: f_00006-0-0 loss: 0.549664  [   32/   54]
train() client id: f_00006-1-0 loss: 0.503134  [   32/   54]
train() client id: f_00006-2-0 loss: 0.548512  [   32/   54]
train() client id: f_00006-3-0 loss: 0.532989  [   32/   54]
train() client id: f_00006-4-0 loss: 0.549865  [   32/   54]
train() client id: f_00006-5-0 loss: 0.589406  [   32/   54]
train() client id: f_00006-6-0 loss: 0.587074  [   32/   54]
train() client id: f_00006-7-0 loss: 0.549440  [   32/   54]
train() client id: f_00006-8-0 loss: 0.597782  [   32/   54]
train() client id: f_00006-9-0 loss: 0.543571  [   32/   54]
train() client id: f_00006-10-0 loss: 0.589017  [   32/   54]
train() client id: f_00006-11-0 loss: 0.558847  [   32/   54]
train() client id: f_00007-0-0 loss: 0.875105  [   32/  179]
train() client id: f_00007-0-1 loss: 0.667104  [   64/  179]
train() client id: f_00007-0-2 loss: 0.626072  [   96/  179]
train() client id: f_00007-0-3 loss: 0.522006  [  128/  179]
train() client id: f_00007-0-4 loss: 0.526477  [  160/  179]
train() client id: f_00007-1-0 loss: 0.540101  [   32/  179]
train() client id: f_00007-1-1 loss: 0.662998  [   64/  179]
train() client id: f_00007-1-2 loss: 0.694038  [   96/  179]
train() client id: f_00007-1-3 loss: 0.683898  [  128/  179]
train() client id: f_00007-1-4 loss: 0.576817  [  160/  179]
train() client id: f_00007-2-0 loss: 0.542109  [   32/  179]
train() client id: f_00007-2-1 loss: 0.751568  [   64/  179]
train() client id: f_00007-2-2 loss: 0.522291  [   96/  179]
train() client id: f_00007-2-3 loss: 0.670670  [  128/  179]
train() client id: f_00007-2-4 loss: 0.639637  [  160/  179]
train() client id: f_00007-3-0 loss: 0.647897  [   32/  179]
train() client id: f_00007-3-1 loss: 0.642185  [   64/  179]
train() client id: f_00007-3-2 loss: 0.526156  [   96/  179]
train() client id: f_00007-3-3 loss: 0.534829  [  128/  179]
train() client id: f_00007-3-4 loss: 0.582717  [  160/  179]
train() client id: f_00007-4-0 loss: 0.490295  [   32/  179]
train() client id: f_00007-4-1 loss: 0.610923  [   64/  179]
train() client id: f_00007-4-2 loss: 0.694253  [   96/  179]
train() client id: f_00007-4-3 loss: 0.500789  [  128/  179]
train() client id: f_00007-4-4 loss: 0.641341  [  160/  179]
train() client id: f_00007-5-0 loss: 0.649648  [   32/  179]
train() client id: f_00007-5-1 loss: 0.566982  [   64/  179]
train() client id: f_00007-5-2 loss: 0.440345  [   96/  179]
train() client id: f_00007-5-3 loss: 0.651703  [  128/  179]
train() client id: f_00007-5-4 loss: 0.663889  [  160/  179]
train() client id: f_00007-6-0 loss: 0.565618  [   32/  179]
train() client id: f_00007-6-1 loss: 0.461567  [   64/  179]
train() client id: f_00007-6-2 loss: 0.545655  [   96/  179]
train() client id: f_00007-6-3 loss: 0.771425  [  128/  179]
train() client id: f_00007-6-4 loss: 0.573458  [  160/  179]
train() client id: f_00007-7-0 loss: 0.643675  [   32/  179]
train() client id: f_00007-7-1 loss: 0.530089  [   64/  179]
train() client id: f_00007-7-2 loss: 0.621759  [   96/  179]
train() client id: f_00007-7-3 loss: 0.620677  [  128/  179]
train() client id: f_00007-7-4 loss: 0.456748  [  160/  179]
train() client id: f_00007-8-0 loss: 0.892633  [   32/  179]
train() client id: f_00007-8-1 loss: 0.531499  [   64/  179]
train() client id: f_00007-8-2 loss: 0.444980  [   96/  179]
train() client id: f_00007-8-3 loss: 0.528320  [  128/  179]
train() client id: f_00007-8-4 loss: 0.459627  [  160/  179]
train() client id: f_00007-9-0 loss: 0.455902  [   32/  179]
train() client id: f_00007-9-1 loss: 0.441136  [   64/  179]
train() client id: f_00007-9-2 loss: 0.585129  [   96/  179]
train() client id: f_00007-9-3 loss: 0.761287  [  128/  179]
train() client id: f_00007-9-4 loss: 0.603835  [  160/  179]
train() client id: f_00007-10-0 loss: 0.737804  [   32/  179]
train() client id: f_00007-10-1 loss: 0.522092  [   64/  179]
train() client id: f_00007-10-2 loss: 0.533109  [   96/  179]
train() client id: f_00007-10-3 loss: 0.542260  [  128/  179]
train() client id: f_00007-10-4 loss: 0.521698  [  160/  179]
train() client id: f_00007-11-0 loss: 0.527286  [   32/  179]
train() client id: f_00007-11-1 loss: 0.707951  [   64/  179]
train() client id: f_00007-11-2 loss: 0.690838  [   96/  179]
train() client id: f_00007-11-3 loss: 0.552925  [  128/  179]
train() client id: f_00007-11-4 loss: 0.440214  [  160/  179]
train() client id: f_00008-0-0 loss: 0.675400  [   32/  130]
train() client id: f_00008-0-1 loss: 0.755811  [   64/  130]
train() client id: f_00008-0-2 loss: 0.828233  [   96/  130]
train() client id: f_00008-0-3 loss: 0.950276  [  128/  130]
train() client id: f_00008-1-0 loss: 0.772458  [   32/  130]
train() client id: f_00008-1-1 loss: 0.792075  [   64/  130]
train() client id: f_00008-1-2 loss: 0.741829  [   96/  130]
train() client id: f_00008-1-3 loss: 0.922074  [  128/  130]
train() client id: f_00008-2-0 loss: 0.788278  [   32/  130]
train() client id: f_00008-2-1 loss: 0.828934  [   64/  130]
train() client id: f_00008-2-2 loss: 0.846947  [   96/  130]
train() client id: f_00008-2-3 loss: 0.754918  [  128/  130]
train() client id: f_00008-3-0 loss: 0.910096  [   32/  130]
train() client id: f_00008-3-1 loss: 0.772521  [   64/  130]
train() client id: f_00008-3-2 loss: 0.704172  [   96/  130]
train() client id: f_00008-3-3 loss: 0.833688  [  128/  130]
train() client id: f_00008-4-0 loss: 0.833744  [   32/  130]
train() client id: f_00008-4-1 loss: 0.696118  [   64/  130]
train() client id: f_00008-4-2 loss: 0.837706  [   96/  130]
train() client id: f_00008-4-3 loss: 0.823145  [  128/  130]
train() client id: f_00008-5-0 loss: 0.846370  [   32/  130]
train() client id: f_00008-5-1 loss: 0.784125  [   64/  130]
train() client id: f_00008-5-2 loss: 0.849197  [   96/  130]
train() client id: f_00008-5-3 loss: 0.747323  [  128/  130]
train() client id: f_00008-6-0 loss: 0.832397  [   32/  130]
train() client id: f_00008-6-1 loss: 0.813737  [   64/  130]
train() client id: f_00008-6-2 loss: 0.762106  [   96/  130]
train() client id: f_00008-6-3 loss: 0.787281  [  128/  130]
train() client id: f_00008-7-0 loss: 0.728281  [   32/  130]
train() client id: f_00008-7-1 loss: 0.994780  [   64/  130]
train() client id: f_00008-7-2 loss: 0.702293  [   96/  130]
train() client id: f_00008-7-3 loss: 0.764337  [  128/  130]
train() client id: f_00008-8-0 loss: 0.684603  [   32/  130]
train() client id: f_00008-8-1 loss: 0.920467  [   64/  130]
train() client id: f_00008-8-2 loss: 0.757233  [   96/  130]
train() client id: f_00008-8-3 loss: 0.828560  [  128/  130]
train() client id: f_00008-9-0 loss: 0.868635  [   32/  130]
train() client id: f_00008-9-1 loss: 0.921700  [   64/  130]
train() client id: f_00008-9-2 loss: 0.716459  [   96/  130]
train() client id: f_00008-9-3 loss: 0.702224  [  128/  130]
train() client id: f_00008-10-0 loss: 0.799352  [   32/  130]
train() client id: f_00008-10-1 loss: 0.763438  [   64/  130]
train() client id: f_00008-10-2 loss: 0.746034  [   96/  130]
train() client id: f_00008-10-3 loss: 0.898678  [  128/  130]
train() client id: f_00008-11-0 loss: 0.739794  [   32/  130]
train() client id: f_00008-11-1 loss: 0.827038  [   64/  130]
train() client id: f_00008-11-2 loss: 0.847343  [   96/  130]
train() client id: f_00008-11-3 loss: 0.748724  [  128/  130]
train() client id: f_00009-0-0 loss: 1.344889  [   32/  118]
train() client id: f_00009-0-1 loss: 1.263979  [   64/  118]
train() client id: f_00009-0-2 loss: 1.173563  [   96/  118]
train() client id: f_00009-1-0 loss: 1.096647  [   32/  118]
train() client id: f_00009-1-1 loss: 1.191811  [   64/  118]
train() client id: f_00009-1-2 loss: 1.164596  [   96/  118]
train() client id: f_00009-2-0 loss: 1.152611  [   32/  118]
train() client id: f_00009-2-1 loss: 1.138495  [   64/  118]
train() client id: f_00009-2-2 loss: 1.143318  [   96/  118]
train() client id: f_00009-3-0 loss: 1.112832  [   32/  118]
train() client id: f_00009-3-1 loss: 1.090749  [   64/  118]
train() client id: f_00009-3-2 loss: 1.116078  [   96/  118]
train() client id: f_00009-4-0 loss: 1.240807  [   32/  118]
train() client id: f_00009-4-1 loss: 0.998428  [   64/  118]
train() client id: f_00009-4-2 loss: 0.924924  [   96/  118]
train() client id: f_00009-5-0 loss: 0.980841  [   32/  118]
train() client id: f_00009-5-1 loss: 1.110383  [   64/  118]
train() client id: f_00009-5-2 loss: 0.942878  [   96/  118]
train() client id: f_00009-6-0 loss: 0.962020  [   32/  118]
train() client id: f_00009-6-1 loss: 0.963348  [   64/  118]
train() client id: f_00009-6-2 loss: 0.992974  [   96/  118]
train() client id: f_00009-7-0 loss: 1.096585  [   32/  118]
train() client id: f_00009-7-1 loss: 0.902724  [   64/  118]
train() client id: f_00009-7-2 loss: 0.962042  [   96/  118]
train() client id: f_00009-8-0 loss: 0.944958  [   32/  118]
train() client id: f_00009-8-1 loss: 0.871451  [   64/  118]
train() client id: f_00009-8-2 loss: 0.955458  [   96/  118]
train() client id: f_00009-9-0 loss: 0.932634  [   32/  118]
train() client id: f_00009-9-1 loss: 0.920885  [   64/  118]
train() client id: f_00009-9-2 loss: 0.935262  [   96/  118]
train() client id: f_00009-10-0 loss: 1.007204  [   32/  118]
train() client id: f_00009-10-1 loss: 0.961589  [   64/  118]
train() client id: f_00009-10-2 loss: 0.853116  [   96/  118]
train() client id: f_00009-11-0 loss: 0.876285  [   32/  118]
train() client id: f_00009-11-1 loss: 0.998003  [   64/  118]
train() client id: f_00009-11-2 loss: 0.877890  [   96/  118]
At round 19 accuracy: 0.636604774535809
At round 19 training accuracy: 0.5801475519785378
At round 19 training loss: 0.8496018362035628
update_location
xs = [ -3.9056584    4.20031788 115.00902392  18.81129433   0.97929623
   3.95640986 -77.44319194 -56.32485185  99.66397685 -42.06087855]
ys = [107.5879595   90.55583871   1.32061395 -77.45517586  69.35018685
  52.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [146.93680001 134.9740812  152.41003774 127.87950994 121.69801739
 113.15912556 126.50825475 114.77441    142.27290063 108.55933567]
dists_bs = [183.89577736 198.09989981 338.03810893 318.27100658 205.25738895
 216.69051292 202.65232753 210.76443071 316.58303499 216.59479044]
uav_gains = [3.81792511e-11 4.72350303e-11 3.48260844e-11 5.40686414e-11
 6.12019359e-11 7.34119429e-11 5.55466623e-11 7.08558845e-11
 4.13970915e-11 8.14379617e-11]
bs_gains = [5.04072687e-11 4.09277782e-11 9.16621896e-12 1.08508127e-11
 3.70559068e-11 3.18377503e-11 3.84051633e-11 3.44081712e-11
 1.10135848e-11 3.18771632e-11]
Round 20
-------------------------------
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.3523103  17.38275937  8.22567535  2.94704545 20.05064182  9.65872482
  3.66105067 11.77706151  8.67193539  7.83910309]
obj_prev = 98.5663077651687
eta_min = 7.590997972408306e-12	eta_max = 0.9222942349256907
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 22.912452031604698	eta = 0.9090909090909091
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 40.06758095330687	eta = 0.519859231611392
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 31.832010204673995	eta = 0.654357098812908
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.352361561329435	eta = 0.6862563825495306
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.27833469058294	eta = 0.6879341965062457
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.27813585075753	eta = 0.6879387142452571
eta = 0.6879387142452571
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [0.03086617 0.06491693 0.03037623 0.01053369 0.07496069 0.03576556
 0.01322836 0.04384955 0.03184604 0.02890641]
ene_total = [2.63093078 4.95082043 2.60801881 1.20448716 5.64850744 2.98715498
 1.38606222 3.45675966 2.88726353 2.51813083]
ti_comp = [0.34802257 0.34915355 0.34647497 0.3533723  0.34752642 0.34490443
 0.35375633 0.35704591 0.32032243 0.34492651]
ti_coms = [0.07621728 0.0750863  0.07776488 0.07086754 0.07671342 0.07933541
 0.07048351 0.06719393 0.10391742 0.07931333]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.51744596e-05 1.40255892e-04 1.45927700e-05 5.85001217e-07
 2.17974172e-04 2.40368430e-05 1.15608361e-06 4.13358984e-05
 1.96730538e-05 1.26884708e-05]
ene_total = [0.52203848 0.52285762 0.53257769 0.48447181 0.53929285 0.54395903
 0.48188571 0.46214562 0.71169684 0.54303237]
optimize_network_iter = 0 obj = 5.343958006285102
eta = 0.6879387142452571
freqs = [4.43450629e+07 9.29633038e+07 4.38361048e+07 1.49045232e+07
 1.07848904e+08 5.18485023e+07 1.86969890e+07 6.14060420e+07
 4.97093568e+07 4.19022717e+07]
eta_min = 0.6879387142452722	eta_max = 0.6879387142452453
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 0.030579617084646203	eta = 0.9090909090909091
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 18.258827212431594	eta = 0.001522532174257359
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.8403927678865823	eta = 0.015105282079029619
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.8010043726548246	eta = 0.015435638201229906
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.800997812209442	eta = 0.015435694428206238
eta = 0.015435694428206238
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.69209568e-04 1.56398576e-03 1.62723179e-04 6.52331649e-06
 2.43061804e-03 2.68033519e-04 1.28914250e-05 4.60934337e-04
 2.19373145e-04 1.41488442e-04]
ene_total = [0.16879936 0.19656835 0.17201187 0.15368379 0.21887026 0.17769628
 0.15298971 0.15556983 0.22990166 0.17490671]
ti_comp = [0.34802257 0.34915355 0.34647497 0.3533723  0.34752642 0.34490443
 0.35375633 0.35704591 0.32032243 0.34492651]
ti_coms = [0.07621728 0.0750863  0.07776488 0.07086754 0.07671342 0.07933541
 0.07048351 0.06719393 0.10391742 0.07931333]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.51744596e-05 1.40255892e-04 1.45927700e-05 5.85001217e-07
 2.17974172e-04 2.40368430e-05 1.15608361e-06 4.13358984e-05
 1.96730538e-05 1.26884708e-05]
ene_total = [0.52203848 0.52285762 0.53257769 0.48447181 0.53929285 0.54395903
 0.48188571 0.46214562 0.71169684 0.54303237]
optimize_network_iter = 1 obj = 5.343958006285359
eta = 0.6879387142452722
freqs = [4.43450629e+07 9.29633038e+07 4.38361048e+07 1.49045232e+07
 1.07848904e+08 5.18485023e+07 1.86969890e+07 6.14060420e+07
 4.97093568e+07 4.19022717e+07]
Done!
At round 20 eta: 0.6879387142452722
At round 20 local rounds: 12.248478257840025
At round 20 global rounds: 68.35736084749131
At round 20 a_n: 20.989140069898713
gradient difference: 0.5073829293251038
train() client id: f_00000-0-0 loss: 1.383020  [   32/  126]
train() client id: f_00000-0-1 loss: 1.215058  [   64/  126]
train() client id: f_00000-0-2 loss: 1.026132  [   96/  126]
train() client id: f_00000-1-0 loss: 1.173191  [   32/  126]
train() client id: f_00000-1-1 loss: 1.207883  [   64/  126]
train() client id: f_00000-1-2 loss: 0.992111  [   96/  126]
train() client id: f_00000-2-0 loss: 0.950447  [   32/  126]
train() client id: f_00000-2-1 loss: 1.159113  [   64/  126]
train() client id: f_00000-2-2 loss: 1.001040  [   96/  126]
train() client id: f_00000-3-0 loss: 0.944656  [   32/  126]
train() client id: f_00000-3-1 loss: 1.146203  [   64/  126]
train() client id: f_00000-3-2 loss: 0.998519  [   96/  126]
train() client id: f_00000-4-0 loss: 1.069573  [   32/  126]
train() client id: f_00000-4-1 loss: 1.007503  [   64/  126]
train() client id: f_00000-4-2 loss: 0.950010  [   96/  126]
train() client id: f_00000-5-0 loss: 0.950030  [   32/  126]
train() client id: f_00000-5-1 loss: 0.915531  [   64/  126]
train() client id: f_00000-5-2 loss: 1.062249  [   96/  126]
train() client id: f_00000-6-0 loss: 0.905067  [   32/  126]
train() client id: f_00000-6-1 loss: 1.039956  [   64/  126]
train() client id: f_00000-6-2 loss: 1.012442  [   96/  126]
train() client id: f_00000-7-0 loss: 0.966640  [   32/  126]
train() client id: f_00000-7-1 loss: 0.986890  [   64/  126]
train() client id: f_00000-7-2 loss: 1.010575  [   96/  126]
train() client id: f_00000-8-0 loss: 0.966383  [   32/  126]
train() client id: f_00000-8-1 loss: 0.883561  [   64/  126]
train() client id: f_00000-8-2 loss: 1.094938  [   96/  126]
train() client id: f_00000-9-0 loss: 1.046946  [   32/  126]
train() client id: f_00000-9-1 loss: 1.006727  [   64/  126]
train() client id: f_00000-9-2 loss: 0.932519  [   96/  126]
train() client id: f_00000-10-0 loss: 1.027882  [   32/  126]
train() client id: f_00000-10-1 loss: 1.032529  [   64/  126]
train() client id: f_00000-10-2 loss: 0.934247  [   96/  126]
train() client id: f_00000-11-0 loss: 0.974195  [   32/  126]
train() client id: f_00000-11-1 loss: 0.956070  [   64/  126]
train() client id: f_00000-11-2 loss: 1.067145  [   96/  126]
train() client id: f_00001-0-0 loss: 0.525481  [   32/  265]
train() client id: f_00001-0-1 loss: 0.604228  [   64/  265]
train() client id: f_00001-0-2 loss: 0.470472  [   96/  265]
train() client id: f_00001-0-3 loss: 0.389193  [  128/  265]
train() client id: f_00001-0-4 loss: 0.429682  [  160/  265]
train() client id: f_00001-0-5 loss: 0.380134  [  192/  265]
train() client id: f_00001-0-6 loss: 0.506095  [  224/  265]
train() client id: f_00001-0-7 loss: 0.445610  [  256/  265]
train() client id: f_00001-1-0 loss: 0.393195  [   32/  265]
train() client id: f_00001-1-1 loss: 0.488692  [   64/  265]
train() client id: f_00001-1-2 loss: 0.460651  [   96/  265]
train() client id: f_00001-1-3 loss: 0.656481  [  128/  265]
train() client id: f_00001-1-4 loss: 0.429639  [  160/  265]
train() client id: f_00001-1-5 loss: 0.371989  [  192/  265]
train() client id: f_00001-1-6 loss: 0.388676  [  224/  265]
train() client id: f_00001-1-7 loss: 0.442000  [  256/  265]
train() client id: f_00001-2-0 loss: 0.533588  [   32/  265]
train() client id: f_00001-2-1 loss: 0.352068  [   64/  265]
train() client id: f_00001-2-2 loss: 0.422236  [   96/  265]
train() client id: f_00001-2-3 loss: 0.380805  [  128/  265]
train() client id: f_00001-2-4 loss: 0.395740  [  160/  265]
train() client id: f_00001-2-5 loss: 0.488523  [  192/  265]
train() client id: f_00001-2-6 loss: 0.453788  [  224/  265]
train() client id: f_00001-2-7 loss: 0.525621  [  256/  265]
train() client id: f_00001-3-0 loss: 0.577824  [   32/  265]
train() client id: f_00001-3-1 loss: 0.405270  [   64/  265]
train() client id: f_00001-3-2 loss: 0.372751  [   96/  265]
train() client id: f_00001-3-3 loss: 0.477264  [  128/  265]
train() client id: f_00001-3-4 loss: 0.408102  [  160/  265]
train() client id: f_00001-3-5 loss: 0.478701  [  192/  265]
train() client id: f_00001-3-6 loss: 0.429395  [  224/  265]
train() client id: f_00001-3-7 loss: 0.412518  [  256/  265]
train() client id: f_00001-4-0 loss: 0.552338  [   32/  265]
train() client id: f_00001-4-1 loss: 0.410636  [   64/  265]
train() client id: f_00001-4-2 loss: 0.351932  [   96/  265]
train() client id: f_00001-4-3 loss: 0.478026  [  128/  265]
train() client id: f_00001-4-4 loss: 0.396406  [  160/  265]
train() client id: f_00001-4-5 loss: 0.547998  [  192/  265]
train() client id: f_00001-4-6 loss: 0.365900  [  224/  265]
train() client id: f_00001-4-7 loss: 0.426409  [  256/  265]
train() client id: f_00001-5-0 loss: 0.384967  [   32/  265]
train() client id: f_00001-5-1 loss: 0.372301  [   64/  265]
train() client id: f_00001-5-2 loss: 0.343977  [   96/  265]
train() client id: f_00001-5-3 loss: 0.435824  [  128/  265]
train() client id: f_00001-5-4 loss: 0.459351  [  160/  265]
train() client id: f_00001-5-5 loss: 0.422418  [  192/  265]
train() client id: f_00001-5-6 loss: 0.595608  [  224/  265]
train() client id: f_00001-5-7 loss: 0.502985  [  256/  265]
train() client id: f_00001-6-0 loss: 0.434860  [   32/  265]
train() client id: f_00001-6-1 loss: 0.457776  [   64/  265]
train() client id: f_00001-6-2 loss: 0.531145  [   96/  265]
train() client id: f_00001-6-3 loss: 0.513739  [  128/  265]
train() client id: f_00001-6-4 loss: 0.384422  [  160/  265]
train() client id: f_00001-6-5 loss: 0.435152  [  192/  265]
train() client id: f_00001-6-6 loss: 0.330550  [  224/  265]
train() client id: f_00001-6-7 loss: 0.410289  [  256/  265]
train() client id: f_00001-7-0 loss: 0.400447  [   32/  265]
train() client id: f_00001-7-1 loss: 0.399976  [   64/  265]
train() client id: f_00001-7-2 loss: 0.460754  [   96/  265]
train() client id: f_00001-7-3 loss: 0.486357  [  128/  265]
train() client id: f_00001-7-4 loss: 0.379601  [  160/  265]
train() client id: f_00001-7-5 loss: 0.495729  [  192/  265]
train() client id: f_00001-7-6 loss: 0.413739  [  224/  265]
train() client id: f_00001-7-7 loss: 0.455324  [  256/  265]
train() client id: f_00001-8-0 loss: 0.398387  [   32/  265]
train() client id: f_00001-8-1 loss: 0.422732  [   64/  265]
train() client id: f_00001-8-2 loss: 0.343852  [   96/  265]
train() client id: f_00001-8-3 loss: 0.527927  [  128/  265]
train() client id: f_00001-8-4 loss: 0.394994  [  160/  265]
train() client id: f_00001-8-5 loss: 0.333165  [  192/  265]
train() client id: f_00001-8-6 loss: 0.519375  [  224/  265]
train() client id: f_00001-8-7 loss: 0.386005  [  256/  265]
train() client id: f_00001-9-0 loss: 0.457632  [   32/  265]
train() client id: f_00001-9-1 loss: 0.372030  [   64/  265]
train() client id: f_00001-9-2 loss: 0.406586  [   96/  265]
train() client id: f_00001-9-3 loss: 0.517132  [  128/  265]
train() client id: f_00001-9-4 loss: 0.431197  [  160/  265]
train() client id: f_00001-9-5 loss: 0.378318  [  192/  265]
train() client id: f_00001-9-6 loss: 0.407518  [  224/  265]
train() client id: f_00001-9-7 loss: 0.469577  [  256/  265]
train() client id: f_00001-10-0 loss: 0.405694  [   32/  265]
train() client id: f_00001-10-1 loss: 0.455313  [   64/  265]
train() client id: f_00001-10-2 loss: 0.399173  [   96/  265]
train() client id: f_00001-10-3 loss: 0.316710  [  128/  265]
train() client id: f_00001-10-4 loss: 0.449091  [  160/  265]
train() client id: f_00001-10-5 loss: 0.342479  [  192/  265]
train() client id: f_00001-10-6 loss: 0.424075  [  224/  265]
train() client id: f_00001-10-7 loss: 0.620340  [  256/  265]
train() client id: f_00001-11-0 loss: 0.469513  [   32/  265]
train() client id: f_00001-11-1 loss: 0.427633  [   64/  265]
train() client id: f_00001-11-2 loss: 0.323302  [   96/  265]
train() client id: f_00001-11-3 loss: 0.484616  [  128/  265]
train() client id: f_00001-11-4 loss: 0.352547  [  160/  265]
train() client id: f_00001-11-5 loss: 0.436311  [  192/  265]
train() client id: f_00001-11-6 loss: 0.450651  [  224/  265]
train() client id: f_00001-11-7 loss: 0.451232  [  256/  265]
train() client id: f_00002-0-0 loss: 1.219834  [   32/  124]
train() client id: f_00002-0-1 loss: 1.328081  [   64/  124]
train() client id: f_00002-0-2 loss: 1.312143  [   96/  124]
train() client id: f_00002-1-0 loss: 1.125233  [   32/  124]
train() client id: f_00002-1-1 loss: 1.216541  [   64/  124]
train() client id: f_00002-1-2 loss: 1.395206  [   96/  124]
train() client id: f_00002-2-0 loss: 1.212868  [   32/  124]
train() client id: f_00002-2-1 loss: 1.256339  [   64/  124]
train() client id: f_00002-2-2 loss: 1.261618  [   96/  124]
train() client id: f_00002-3-0 loss: 1.144379  [   32/  124]
train() client id: f_00002-3-1 loss: 1.203984  [   64/  124]
train() client id: f_00002-3-2 loss: 1.188244  [   96/  124]
train() client id: f_00002-4-0 loss: 1.189474  [   32/  124]
train() client id: f_00002-4-1 loss: 1.166541  [   64/  124]
train() client id: f_00002-4-2 loss: 1.166564  [   96/  124]
train() client id: f_00002-5-0 loss: 1.063011  [   32/  124]
train() client id: f_00002-5-1 loss: 1.154167  [   64/  124]
train() client id: f_00002-5-2 loss: 1.148518  [   96/  124]
train() client id: f_00002-6-0 loss: 1.148566  [   32/  124]
train() client id: f_00002-6-1 loss: 1.074882  [   64/  124]
train() client id: f_00002-6-2 loss: 1.273242  [   96/  124]
train() client id: f_00002-7-0 loss: 1.016173  [   32/  124]
train() client id: f_00002-7-1 loss: 1.107771  [   64/  124]
train() client id: f_00002-7-2 loss: 1.205496  [   96/  124]
train() client id: f_00002-8-0 loss: 1.269854  [   32/  124]
train() client id: f_00002-8-1 loss: 0.964557  [   64/  124]
train() client id: f_00002-8-2 loss: 1.097121  [   96/  124]
train() client id: f_00002-9-0 loss: 1.034549  [   32/  124]
train() client id: f_00002-9-1 loss: 1.046252  [   64/  124]
train() client id: f_00002-9-2 loss: 1.268401  [   96/  124]
train() client id: f_00002-10-0 loss: 1.009371  [   32/  124]
train() client id: f_00002-10-1 loss: 0.913792  [   64/  124]
train() client id: f_00002-10-2 loss: 1.178358  [   96/  124]
train() client id: f_00002-11-0 loss: 1.140477  [   32/  124]
train() client id: f_00002-11-1 loss: 1.023287  [   64/  124]
train() client id: f_00002-11-2 loss: 1.136491  [   96/  124]
train() client id: f_00003-0-0 loss: 0.778793  [   32/   43]
train() client id: f_00003-1-0 loss: 0.765129  [   32/   43]
train() client id: f_00003-2-0 loss: 0.710627  [   32/   43]
train() client id: f_00003-3-0 loss: 0.839310  [   32/   43]
train() client id: f_00003-4-0 loss: 0.723075  [   32/   43]
train() client id: f_00003-5-0 loss: 0.788231  [   32/   43]
train() client id: f_00003-6-0 loss: 0.768836  [   32/   43]
train() client id: f_00003-7-0 loss: 0.759261  [   32/   43]
train() client id: f_00003-8-0 loss: 0.700775  [   32/   43]
train() client id: f_00003-9-0 loss: 0.805239  [   32/   43]
train() client id: f_00003-10-0 loss: 0.683411  [   32/   43]
train() client id: f_00003-11-0 loss: 0.659578  [   32/   43]
train() client id: f_00004-0-0 loss: 0.937397  [   32/  306]
train() client id: f_00004-0-1 loss: 0.934204  [   64/  306]
train() client id: f_00004-0-2 loss: 0.838320  [   96/  306]
train() client id: f_00004-0-3 loss: 0.956161  [  128/  306]
train() client id: f_00004-0-4 loss: 0.780031  [  160/  306]
train() client id: f_00004-0-5 loss: 0.773832  [  192/  306]
train() client id: f_00004-0-6 loss: 0.852415  [  224/  306]
train() client id: f_00004-0-7 loss: 0.923238  [  256/  306]
train() client id: f_00004-0-8 loss: 0.835682  [  288/  306]
train() client id: f_00004-1-0 loss: 0.833887  [   32/  306]
train() client id: f_00004-1-1 loss: 0.831335  [   64/  306]
train() client id: f_00004-1-2 loss: 0.880986  [   96/  306]
train() client id: f_00004-1-3 loss: 0.871052  [  128/  306]
train() client id: f_00004-1-4 loss: 0.883342  [  160/  306]
train() client id: f_00004-1-5 loss: 0.924083  [  192/  306]
train() client id: f_00004-1-6 loss: 0.954507  [  224/  306]
train() client id: f_00004-1-7 loss: 0.753948  [  256/  306]
train() client id: f_00004-1-8 loss: 0.894263  [  288/  306]
train() client id: f_00004-2-0 loss: 0.897542  [   32/  306]
train() client id: f_00004-2-1 loss: 0.777359  [   64/  306]
train() client id: f_00004-2-2 loss: 0.892560  [   96/  306]
train() client id: f_00004-2-3 loss: 0.805200  [  128/  306]
train() client id: f_00004-2-4 loss: 1.020239  [  160/  306]
train() client id: f_00004-2-5 loss: 0.813511  [  192/  306]
train() client id: f_00004-2-6 loss: 0.823806  [  224/  306]
train() client id: f_00004-2-7 loss: 0.870480  [  256/  306]
train() client id: f_00004-2-8 loss: 0.933464  [  288/  306]
train() client id: f_00004-3-0 loss: 0.808074  [   32/  306]
train() client id: f_00004-3-1 loss: 0.957818  [   64/  306]
train() client id: f_00004-3-2 loss: 0.759028  [   96/  306]
train() client id: f_00004-3-3 loss: 0.816307  [  128/  306]
train() client id: f_00004-3-4 loss: 0.986748  [  160/  306]
train() client id: f_00004-3-5 loss: 0.867163  [  192/  306]
train() client id: f_00004-3-6 loss: 0.748328  [  224/  306]
train() client id: f_00004-3-7 loss: 1.007637  [  256/  306]
train() client id: f_00004-3-8 loss: 0.846420  [  288/  306]
train() client id: f_00004-4-0 loss: 0.839213  [   32/  306]
train() client id: f_00004-4-1 loss: 0.913828  [   64/  306]
train() client id: f_00004-4-2 loss: 0.936804  [   96/  306]
train() client id: f_00004-4-3 loss: 0.777999  [  128/  306]
train() client id: f_00004-4-4 loss: 0.862061  [  160/  306]
train() client id: f_00004-4-5 loss: 0.921356  [  192/  306]
train() client id: f_00004-4-6 loss: 0.906690  [  224/  306]
train() client id: f_00004-4-7 loss: 0.861277  [  256/  306]
train() client id: f_00004-4-8 loss: 0.785935  [  288/  306]
train() client id: f_00004-5-0 loss: 0.800966  [   32/  306]
train() client id: f_00004-5-1 loss: 0.852887  [   64/  306]
train() client id: f_00004-5-2 loss: 0.942662  [   96/  306]
train() client id: f_00004-5-3 loss: 0.904775  [  128/  306]
train() client id: f_00004-5-4 loss: 0.819092  [  160/  306]
train() client id: f_00004-5-5 loss: 0.852482  [  192/  306]
train() client id: f_00004-5-6 loss: 0.931131  [  224/  306]
train() client id: f_00004-5-7 loss: 0.845578  [  256/  306]
train() client id: f_00004-5-8 loss: 0.851571  [  288/  306]
train() client id: f_00004-6-0 loss: 0.767123  [   32/  306]
train() client id: f_00004-6-1 loss: 0.766996  [   64/  306]
train() client id: f_00004-6-2 loss: 0.833569  [   96/  306]
train() client id: f_00004-6-3 loss: 0.769866  [  128/  306]
train() client id: f_00004-6-4 loss: 0.931250  [  160/  306]
train() client id: f_00004-6-5 loss: 0.869067  [  192/  306]
train() client id: f_00004-6-6 loss: 0.923847  [  224/  306]
train() client id: f_00004-6-7 loss: 0.966912  [  256/  306]
train() client id: f_00004-6-8 loss: 0.787247  [  288/  306]
train() client id: f_00004-7-0 loss: 0.832764  [   32/  306]
train() client id: f_00004-7-1 loss: 0.809141  [   64/  306]
train() client id: f_00004-7-2 loss: 0.887753  [   96/  306]
train() client id: f_00004-7-3 loss: 0.970215  [  128/  306]
train() client id: f_00004-7-4 loss: 0.863246  [  160/  306]
train() client id: f_00004-7-5 loss: 0.817407  [  192/  306]
train() client id: f_00004-7-6 loss: 0.849362  [  224/  306]
train() client id: f_00004-7-7 loss: 0.819061  [  256/  306]
train() client id: f_00004-7-8 loss: 0.811838  [  288/  306]
train() client id: f_00004-8-0 loss: 0.882998  [   32/  306]
train() client id: f_00004-8-1 loss: 0.810975  [   64/  306]
train() client id: f_00004-8-2 loss: 0.826127  [   96/  306]
train() client id: f_00004-8-3 loss: 0.945406  [  128/  306]
train() client id: f_00004-8-4 loss: 0.872149  [  160/  306]
train() client id: f_00004-8-5 loss: 0.774508  [  192/  306]
train() client id: f_00004-8-6 loss: 0.877805  [  224/  306]
train() client id: f_00004-8-7 loss: 0.895051  [  256/  306]
train() client id: f_00004-8-8 loss: 0.805231  [  288/  306]
train() client id: f_00004-9-0 loss: 0.844990  [   32/  306]
train() client id: f_00004-9-1 loss: 0.881395  [   64/  306]
train() client id: f_00004-9-2 loss: 0.812328  [   96/  306]
train() client id: f_00004-9-3 loss: 0.813927  [  128/  306]
train() client id: f_00004-9-4 loss: 0.940306  [  160/  306]
train() client id: f_00004-9-5 loss: 0.834548  [  192/  306]
train() client id: f_00004-9-6 loss: 0.788352  [  224/  306]
train() client id: f_00004-9-7 loss: 0.890917  [  256/  306]
train() client id: f_00004-9-8 loss: 0.741573  [  288/  306]
train() client id: f_00004-10-0 loss: 0.791117  [   32/  306]
train() client id: f_00004-10-1 loss: 0.844464  [   64/  306]
train() client id: f_00004-10-2 loss: 1.030531  [   96/  306]
train() client id: f_00004-10-3 loss: 0.875556  [  128/  306]
train() client id: f_00004-10-4 loss: 0.756986  [  160/  306]
train() client id: f_00004-10-5 loss: 0.818547  [  192/  306]
train() client id: f_00004-10-6 loss: 0.940034  [  224/  306]
train() client id: f_00004-10-7 loss: 0.873447  [  256/  306]
train() client id: f_00004-10-8 loss: 0.757573  [  288/  306]
train() client id: f_00004-11-0 loss: 0.877939  [   32/  306]
train() client id: f_00004-11-1 loss: 0.859421  [   64/  306]
train() client id: f_00004-11-2 loss: 0.944357  [   96/  306]
train() client id: f_00004-11-3 loss: 0.858576  [  128/  306]
train() client id: f_00004-11-4 loss: 0.878691  [  160/  306]
train() client id: f_00004-11-5 loss: 0.872847  [  192/  306]
train() client id: f_00004-11-6 loss: 0.820934  [  224/  306]
train() client id: f_00004-11-7 loss: 0.775942  [  256/  306]
train() client id: f_00004-11-8 loss: 0.784721  [  288/  306]
train() client id: f_00005-0-0 loss: 0.765206  [   32/  146]
train() client id: f_00005-0-1 loss: 0.654183  [   64/  146]
train() client id: f_00005-0-2 loss: 0.481551  [   96/  146]
train() client id: f_00005-0-3 loss: 0.650754  [  128/  146]
train() client id: f_00005-1-0 loss: 0.556679  [   32/  146]
train() client id: f_00005-1-1 loss: 0.474491  [   64/  146]
train() client id: f_00005-1-2 loss: 0.623000  [   96/  146]
train() client id: f_00005-1-3 loss: 0.719914  [  128/  146]
train() client id: f_00005-2-0 loss: 0.711991  [   32/  146]
train() client id: f_00005-2-1 loss: 0.595116  [   64/  146]
train() client id: f_00005-2-2 loss: 0.433223  [   96/  146]
train() client id: f_00005-2-3 loss: 0.788339  [  128/  146]
train() client id: f_00005-3-0 loss: 0.637084  [   32/  146]
train() client id: f_00005-3-1 loss: 0.575480  [   64/  146]
train() client id: f_00005-3-2 loss: 0.585006  [   96/  146]
train() client id: f_00005-3-3 loss: 0.831763  [  128/  146]
train() client id: f_00005-4-0 loss: 0.664699  [   32/  146]
train() client id: f_00005-4-1 loss: 0.761931  [   64/  146]
train() client id: f_00005-4-2 loss: 0.512485  [   96/  146]
train() client id: f_00005-4-3 loss: 0.574982  [  128/  146]
train() client id: f_00005-5-0 loss: 0.412357  [   32/  146]
train() client id: f_00005-5-1 loss: 0.747463  [   64/  146]
train() client id: f_00005-5-2 loss: 0.801803  [   96/  146]
train() client id: f_00005-5-3 loss: 0.613709  [  128/  146]
train() client id: f_00005-6-0 loss: 0.676628  [   32/  146]
train() client id: f_00005-6-1 loss: 0.628659  [   64/  146]
train() client id: f_00005-6-2 loss: 0.459362  [   96/  146]
train() client id: f_00005-6-3 loss: 0.737130  [  128/  146]
train() client id: f_00005-7-0 loss: 0.833742  [   32/  146]
train() client id: f_00005-7-1 loss: 0.600902  [   64/  146]
train() client id: f_00005-7-2 loss: 0.476129  [   96/  146]
train() client id: f_00005-7-3 loss: 0.607037  [  128/  146]
train() client id: f_00005-8-0 loss: 0.845325  [   32/  146]
train() client id: f_00005-8-1 loss: 0.737850  [   64/  146]
train() client id: f_00005-8-2 loss: 0.527444  [   96/  146]
train() client id: f_00005-8-3 loss: 0.499470  [  128/  146]
train() client id: f_00005-9-0 loss: 0.536702  [   32/  146]
train() client id: f_00005-9-1 loss: 0.705773  [   64/  146]
train() client id: f_00005-9-2 loss: 0.791990  [   96/  146]
train() client id: f_00005-9-3 loss: 0.614364  [  128/  146]
train() client id: f_00005-10-0 loss: 0.563015  [   32/  146]
train() client id: f_00005-10-1 loss: 0.753158  [   64/  146]
train() client id: f_00005-10-2 loss: 0.636748  [   96/  146]
train() client id: f_00005-10-3 loss: 0.417322  [  128/  146]
train() client id: f_00005-11-0 loss: 0.790988  [   32/  146]
train() client id: f_00005-11-1 loss: 0.611056  [   64/  146]
train() client id: f_00005-11-2 loss: 0.507125  [   96/  146]
train() client id: f_00005-11-3 loss: 0.616836  [  128/  146]
train() client id: f_00006-0-0 loss: 0.628527  [   32/   54]
train() client id: f_00006-1-0 loss: 0.616369  [   32/   54]
train() client id: f_00006-2-0 loss: 0.571114  [   32/   54]
train() client id: f_00006-3-0 loss: 0.529227  [   32/   54]
train() client id: f_00006-4-0 loss: 0.568874  [   32/   54]
train() client id: f_00006-5-0 loss: 0.584405  [   32/   54]
train() client id: f_00006-6-0 loss: 0.571202  [   32/   54]
train() client id: f_00006-7-0 loss: 0.566376  [   32/   54]
train() client id: f_00006-8-0 loss: 0.615109  [   32/   54]
train() client id: f_00006-9-0 loss: 0.617299  [   32/   54]
train() client id: f_00006-10-0 loss: 0.572810  [   32/   54]
train() client id: f_00006-11-0 loss: 0.622770  [   32/   54]
train() client id: f_00007-0-0 loss: 0.809955  [   32/  179]
train() client id: f_00007-0-1 loss: 0.965181  [   64/  179]
train() client id: f_00007-0-2 loss: 0.664485  [   96/  179]
train() client id: f_00007-0-3 loss: 0.615450  [  128/  179]
train() client id: f_00007-0-4 loss: 0.748155  [  160/  179]
train() client id: f_00007-1-0 loss: 0.637755  [   32/  179]
train() client id: f_00007-1-1 loss: 0.832416  [   64/  179]
train() client id: f_00007-1-2 loss: 0.696403  [   96/  179]
train() client id: f_00007-1-3 loss: 0.871283  [  128/  179]
train() client id: f_00007-1-4 loss: 0.747436  [  160/  179]
train() client id: f_00007-2-0 loss: 0.739624  [   32/  179]
train() client id: f_00007-2-1 loss: 0.652436  [   64/  179]
train() client id: f_00007-2-2 loss: 0.936460  [   96/  179]
train() client id: f_00007-2-3 loss: 0.635046  [  128/  179]
train() client id: f_00007-2-4 loss: 0.763000  [  160/  179]
train() client id: f_00007-3-0 loss: 0.785028  [   32/  179]
train() client id: f_00007-3-1 loss: 0.622168  [   64/  179]
train() client id: f_00007-3-2 loss: 0.868866  [   96/  179]
train() client id: f_00007-3-3 loss: 0.683459  [  128/  179]
train() client id: f_00007-3-4 loss: 0.683489  [  160/  179]
train() client id: f_00007-4-0 loss: 0.703043  [   32/  179]
train() client id: f_00007-4-1 loss: 0.766693  [   64/  179]
train() client id: f_00007-4-2 loss: 0.688548  [   96/  179]
train() client id: f_00007-4-3 loss: 0.836900  [  128/  179]
train() client id: f_00007-4-4 loss: 0.728587  [  160/  179]
train() client id: f_00007-5-0 loss: 0.624846  [   32/  179]
train() client id: f_00007-5-1 loss: 0.713643  [   64/  179]
train() client id: f_00007-5-2 loss: 0.751400  [   96/  179]
train() client id: f_00007-5-3 loss: 0.674360  [  128/  179]
train() client id: f_00007-5-4 loss: 0.849027  [  160/  179]
train() client id: f_00007-6-0 loss: 0.588354  [   32/  179]
train() client id: f_00007-6-1 loss: 0.691972  [   64/  179]
train() client id: f_00007-6-2 loss: 0.767712  [   96/  179]
train() client id: f_00007-6-3 loss: 0.805079  [  128/  179]
train() client id: f_00007-6-4 loss: 0.668004  [  160/  179]
train() client id: f_00007-7-0 loss: 0.755277  [   32/  179]
train() client id: f_00007-7-1 loss: 0.693271  [   64/  179]
train() client id: f_00007-7-2 loss: 0.582408  [   96/  179]
train() client id: f_00007-7-3 loss: 0.755170  [  128/  179]
train() client id: f_00007-7-4 loss: 0.726590  [  160/  179]
train() client id: f_00007-8-0 loss: 0.621544  [   32/  179]
train() client id: f_00007-8-1 loss: 0.806796  [   64/  179]
train() client id: f_00007-8-2 loss: 0.590192  [   96/  179]
train() client id: f_00007-8-3 loss: 0.777448  [  128/  179]
train() client id: f_00007-8-4 loss: 0.764117  [  160/  179]
train() client id: f_00007-9-0 loss: 0.751510  [   32/  179]
train() client id: f_00007-9-1 loss: 0.677598  [   64/  179]
train() client id: f_00007-9-2 loss: 0.672122  [   96/  179]
train() client id: f_00007-9-3 loss: 0.593629  [  128/  179]
train() client id: f_00007-9-4 loss: 0.824242  [  160/  179]
train() client id: f_00007-10-0 loss: 0.561061  [   32/  179]
train() client id: f_00007-10-1 loss: 0.693349  [   64/  179]
train() client id: f_00007-10-2 loss: 0.778910  [   96/  179]
train() client id: f_00007-10-3 loss: 0.947697  [  128/  179]
train() client id: f_00007-10-4 loss: 0.582119  [  160/  179]
train() client id: f_00007-11-0 loss: 0.678117  [   32/  179]
train() client id: f_00007-11-1 loss: 0.899234  [   64/  179]
train() client id: f_00007-11-2 loss: 0.582686  [   96/  179]
train() client id: f_00007-11-3 loss: 0.766884  [  128/  179]
train() client id: f_00007-11-4 loss: 0.595586  [  160/  179]
train() client id: f_00008-0-0 loss: 0.885772  [   32/  130]
train() client id: f_00008-0-1 loss: 0.775309  [   64/  130]
train() client id: f_00008-0-2 loss: 0.813676  [   96/  130]
train() client id: f_00008-0-3 loss: 0.787602  [  128/  130]
train() client id: f_00008-1-0 loss: 0.807914  [   32/  130]
train() client id: f_00008-1-1 loss: 0.837534  [   64/  130]
train() client id: f_00008-1-2 loss: 0.820153  [   96/  130]
train() client id: f_00008-1-3 loss: 0.819408  [  128/  130]
train() client id: f_00008-2-0 loss: 0.759929  [   32/  130]
train() client id: f_00008-2-1 loss: 0.826130  [   64/  130]
train() client id: f_00008-2-2 loss: 0.969805  [   96/  130]
train() client id: f_00008-2-3 loss: 0.706290  [  128/  130]
train() client id: f_00008-3-0 loss: 0.838641  [   32/  130]
train() client id: f_00008-3-1 loss: 0.768919  [   64/  130]
train() client id: f_00008-3-2 loss: 0.757815  [   96/  130]
train() client id: f_00008-3-3 loss: 0.915962  [  128/  130]
train() client id: f_00008-4-0 loss: 0.928161  [   32/  130]
train() client id: f_00008-4-1 loss: 0.720589  [   64/  130]
train() client id: f_00008-4-2 loss: 0.777209  [   96/  130]
train() client id: f_00008-4-3 loss: 0.855035  [  128/  130]
train() client id: f_00008-5-0 loss: 0.833584  [   32/  130]
train() client id: f_00008-5-1 loss: 0.692376  [   64/  130]
train() client id: f_00008-5-2 loss: 0.831035  [   96/  130]
train() client id: f_00008-5-3 loss: 0.909335  [  128/  130]
train() client id: f_00008-6-0 loss: 0.710417  [   32/  130]
train() client id: f_00008-6-1 loss: 0.791673  [   64/  130]
train() client id: f_00008-6-2 loss: 0.991598  [   96/  130]
train() client id: f_00008-6-3 loss: 0.761938  [  128/  130]
train() client id: f_00008-7-0 loss: 0.834440  [   32/  130]
train() client id: f_00008-7-1 loss: 0.825041  [   64/  130]
train() client id: f_00008-7-2 loss: 0.682146  [   96/  130]
train() client id: f_00008-7-3 loss: 0.899999  [  128/  130]
train() client id: f_00008-8-0 loss: 0.712585  [   32/  130]
train() client id: f_00008-8-1 loss: 0.864426  [   64/  130]
train() client id: f_00008-8-2 loss: 0.834899  [   96/  130]
train() client id: f_00008-8-3 loss: 0.823778  [  128/  130]
train() client id: f_00008-9-0 loss: 0.785426  [   32/  130]
train() client id: f_00008-9-1 loss: 0.905852  [   64/  130]
train() client id: f_00008-9-2 loss: 0.776540  [   96/  130]
train() client id: f_00008-9-3 loss: 0.797397  [  128/  130]
train() client id: f_00008-10-0 loss: 0.834525  [   32/  130]
train() client id: f_00008-10-1 loss: 0.752741  [   64/  130]
train() client id: f_00008-10-2 loss: 0.909016  [   96/  130]
train() client id: f_00008-10-3 loss: 0.786403  [  128/  130]
train() client id: f_00008-11-0 loss: 0.857559  [   32/  130]
train() client id: f_00008-11-1 loss: 0.805050  [   64/  130]
train() client id: f_00008-11-2 loss: 0.866977  [   96/  130]
train() client id: f_00008-11-3 loss: 0.747392  [  128/  130]
train() client id: f_00009-0-0 loss: 1.171516  [   32/  118]
train() client id: f_00009-0-1 loss: 1.149554  [   64/  118]
train() client id: f_00009-0-2 loss: 1.165397  [   96/  118]
train() client id: f_00009-1-0 loss: 1.031741  [   32/  118]
train() client id: f_00009-1-1 loss: 1.066264  [   64/  118]
train() client id: f_00009-1-2 loss: 1.129941  [   96/  118]
train() client id: f_00009-2-0 loss: 1.056821  [   32/  118]
train() client id: f_00009-2-1 loss: 1.006160  [   64/  118]
train() client id: f_00009-2-2 loss: 1.126271  [   96/  118]
train() client id: f_00009-3-0 loss: 1.078639  [   32/  118]
train() client id: f_00009-3-1 loss: 1.004694  [   64/  118]
train() client id: f_00009-3-2 loss: 1.080547  [   96/  118]
train() client id: f_00009-4-0 loss: 1.079489  [   32/  118]
train() client id: f_00009-4-1 loss: 0.998379  [   64/  118]
train() client id: f_00009-4-2 loss: 0.975985  [   96/  118]
train() client id: f_00009-5-0 loss: 1.064694  [   32/  118]
train() client id: f_00009-5-1 loss: 1.004076  [   64/  118]
train() client id: f_00009-5-2 loss: 0.874205  [   96/  118]
train() client id: f_00009-6-0 loss: 0.958694  [   32/  118]
train() client id: f_00009-6-1 loss: 0.913215  [   64/  118]
train() client id: f_00009-6-2 loss: 0.963219  [   96/  118]
train() client id: f_00009-7-0 loss: 0.898291  [   32/  118]
train() client id: f_00009-7-1 loss: 0.964841  [   64/  118]
train() client id: f_00009-7-2 loss: 0.970000  [   96/  118]
train() client id: f_00009-8-0 loss: 0.912785  [   32/  118]
train() client id: f_00009-8-1 loss: 0.836700  [   64/  118]
train() client id: f_00009-8-2 loss: 0.979769  [   96/  118]
train() client id: f_00009-9-0 loss: 0.990066  [   32/  118]
train() client id: f_00009-9-1 loss: 0.820639  [   64/  118]
train() client id: f_00009-9-2 loss: 0.871160  [   96/  118]
train() client id: f_00009-10-0 loss: 0.890406  [   32/  118]
train() client id: f_00009-10-1 loss: 0.949387  [   64/  118]
train() client id: f_00009-10-2 loss: 0.990695  [   96/  118]
train() client id: f_00009-11-0 loss: 0.890003  [   32/  118]
train() client id: f_00009-11-1 loss: 0.904635  [   64/  118]
train() client id: f_00009-11-2 loss: 0.971206  [   96/  118]
At round 20 accuracy: 0.636604774535809
At round 20 training accuracy: 0.5774647887323944
At round 20 training loss: 0.8520582662238628
update_location
xs = [ -3.9056584    4.20031788 120.00902392  18.81129433   0.97929623
   3.95640986 -82.44319194 -61.32485185 104.66397685 -47.06087855]
ys = [112.5879595   95.55583871   1.32061395 -82.45517586  74.35018685
  57.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [150.63632627 138.37832555 156.21750812 130.96839627 124.61504446
 115.57737324 129.62935794 117.30905212 145.81912777 110.59266769]
dists_bs = [182.12231199 196.02073536 342.3373385  322.25143194 202.72912368
 213.91100929 200.2932794  207.99012902 320.93061214 213.56243122]
uav_gains = [3.58669488e-11 4.43787932e-11 3.27265969e-11 5.09343950e-11
 5.76816207e-11 6.96314375e-11 5.22611087e-11 6.70896685e-11
 3.89180544e-11 7.77458897e-11]
bs_gains = [5.17937375e-11 4.21549371e-11 8.84753174e-12 1.04796913e-11
 3.83644419e-11 3.30096791e-11 3.96851685e-11 3.57087343e-11
 1.06009035e-11 3.31607609e-11]
Round 21
-------------------------------
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.22040415 17.10262761  8.09588073  2.90155574 19.72744601  9.50226703
  3.60411444 11.58945304  8.53512887  7.71174179]
obj_prev = 96.99061941020106
eta_min = 5.053940464590129e-12	eta_max = 0.9225775474279215
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 22.544522121240533	eta = 0.9090909090909091
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 39.50145864601099	eta = 0.5188421089429398
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 31.352662916909924	eta = 0.6536931221610769
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.88807124482688	eta = 0.6857257513318464
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.81462162317344	eta = 0.6874150666493414
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.81442331258453	eta = 0.6874196389895562
eta = 0.6874196389895562
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [0.03092845 0.06504793 0.03043752 0.01055495 0.07511195 0.03583773
 0.01325505 0.04393804 0.03191031 0.02896474]
ene_total = [2.59558483 4.86886559 2.57325892 1.19041384 5.55486377 2.93485376
 1.36920185 3.40632189 2.84825067 2.47280818]
ti_comp = [0.35387355 0.35652047 0.35228915 0.35940333 0.35499852 0.35244077
 0.35977839 0.36323055 0.32606389 0.35252092]
ti_coms = [0.07726245 0.07461553 0.07884685 0.07173267 0.07613748 0.07869523
 0.07135761 0.06790545 0.10507211 0.07861508]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.47658462e-05 1.35335454e-04 1.42006892e-05 5.68963075e-07
 2.10161950e-04 2.31595009e-05 1.12448598e-06 4.01825275e-05
 1.91015032e-05 1.22213485e-05]
ene_total = [0.51979327 0.51011572 0.53039422 0.48170862 0.52535973 0.52997772
 0.47922748 0.45866954 0.70682062 0.52870501]
optimize_network_iter = 0 obj = 5.270771946473119
eta = 0.6874196389895562
freqs = [4.36998634e+07 9.12260848e+07 4.31996322e+07 1.46839871e+07
 1.05791923e+08 5.08422020e+07 1.84211324e+07 6.04822990e+07
 4.89325984e+07 4.10822992e+07]
eta_min = 0.6874196389895763	eta_max = 0.6874196389895456
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 0.029002321139164264	eta = 0.9090909090909091
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 18.041876240655995	eta = 0.0014613638924501724
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.8124320127004978	eta = 0.014547164420730322
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.775015665989668	eta = 0.01485381058620065
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.7750098831933823	eta = 0.01485385897835976
eta = 0.01485385897835976
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.65835243e-04 1.51995270e-03 1.59487963e-04 6.39002520e-06
 2.36032919e-03 2.60104392e-04 1.26291038e-05 4.51290031e-04
 2.14529013e-04 1.37257984e-04]
ene_total = [0.1681456  0.19135646 0.17138602 0.15296698 0.2125038  0.17320668
 0.15230083 0.15429171 0.22843322 0.17041859]
ti_comp = [0.35387355 0.35652047 0.35228915 0.35940333 0.35499852 0.35244077
 0.35977839 0.36323055 0.32606389 0.35252092]
ti_coms = [0.07726245 0.07461553 0.07884685 0.07173267 0.07613748 0.07869523
 0.07135761 0.06790545 0.10507211 0.07861508]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.47658462e-05 1.35335454e-04 1.42006892e-05 5.68963075e-07
 2.10161950e-04 2.31595009e-05 1.12448598e-06 4.01825275e-05
 1.91015032e-05 1.22213485e-05]
ene_total = [0.51979327 0.51011572 0.53039422 0.48170862 0.52535973 0.52997772
 0.47922748 0.45866954 0.70682062 0.52870501]
optimize_network_iter = 1 obj = 5.270771946473455
eta = 0.6874196389895763
freqs = [4.36998634e+07 9.12260848e+07 4.31996322e+07 1.46839871e+07
 1.05791923e+08 5.08422020e+07 1.84211324e+07 6.04822990e+07
 4.89325984e+07 4.10822992e+07]
Done!
At round 21 eta: 0.6874196389895763
At round 21 local rounds: 12.273194960737252
At round 21 global rounds: 67.14798076901184
At round 21 a_n: 20.646594222929394
gradient difference: 0.44357210397720337
train() client id: f_00000-0-0 loss: 1.293913  [   32/  126]
train() client id: f_00000-0-1 loss: 1.145430  [   64/  126]
train() client id: f_00000-0-2 loss: 1.369825  [   96/  126]
train() client id: f_00000-1-0 loss: 1.204471  [   32/  126]
train() client id: f_00000-1-1 loss: 1.041470  [   64/  126]
train() client id: f_00000-1-2 loss: 1.166443  [   96/  126]
train() client id: f_00000-2-0 loss: 1.026175  [   32/  126]
train() client id: f_00000-2-1 loss: 0.964429  [   64/  126]
train() client id: f_00000-2-2 loss: 1.193672  [   96/  126]
train() client id: f_00000-3-0 loss: 0.996555  [   32/  126]
train() client id: f_00000-3-1 loss: 1.042895  [   64/  126]
train() client id: f_00000-3-2 loss: 0.910202  [   96/  126]
train() client id: f_00000-4-0 loss: 0.901542  [   32/  126]
train() client id: f_00000-4-1 loss: 0.930874  [   64/  126]
train() client id: f_00000-4-2 loss: 0.966992  [   96/  126]
train() client id: f_00000-5-0 loss: 0.967088  [   32/  126]
train() client id: f_00000-5-1 loss: 0.848990  [   64/  126]
train() client id: f_00000-5-2 loss: 0.946410  [   96/  126]
train() client id: f_00000-6-0 loss: 0.793290  [   32/  126]
train() client id: f_00000-6-1 loss: 0.850123  [   64/  126]
train() client id: f_00000-6-2 loss: 0.857956  [   96/  126]
train() client id: f_00000-7-0 loss: 0.887413  [   32/  126]
train() client id: f_00000-7-1 loss: 0.751455  [   64/  126]
train() client id: f_00000-7-2 loss: 0.912825  [   96/  126]
train() client id: f_00000-8-0 loss: 0.857170  [   32/  126]
train() client id: f_00000-8-1 loss: 0.771876  [   64/  126]
train() client id: f_00000-8-2 loss: 0.870716  [   96/  126]
train() client id: f_00000-9-0 loss: 0.819860  [   32/  126]
train() client id: f_00000-9-1 loss: 0.792361  [   64/  126]
train() client id: f_00000-9-2 loss: 0.850219  [   96/  126]
train() client id: f_00000-10-0 loss: 0.777331  [   32/  126]
train() client id: f_00000-10-1 loss: 0.783218  [   64/  126]
train() client id: f_00000-10-2 loss: 0.753059  [   96/  126]
train() client id: f_00000-11-0 loss: 0.766142  [   32/  126]
train() client id: f_00000-11-1 loss: 0.752874  [   64/  126]
train() client id: f_00000-11-2 loss: 0.809952  [   96/  126]
train() client id: f_00001-0-0 loss: 0.468738  [   32/  265]
train() client id: f_00001-0-1 loss: 0.420092  [   64/  265]
train() client id: f_00001-0-2 loss: 0.514362  [   96/  265]
train() client id: f_00001-0-3 loss: 0.494614  [  128/  265]
train() client id: f_00001-0-4 loss: 0.423301  [  160/  265]
train() client id: f_00001-0-5 loss: 0.435004  [  192/  265]
train() client id: f_00001-0-6 loss: 0.350488  [  224/  265]
train() client id: f_00001-0-7 loss: 0.394222  [  256/  265]
train() client id: f_00001-1-0 loss: 0.534289  [   32/  265]
train() client id: f_00001-1-1 loss: 0.334909  [   64/  265]
train() client id: f_00001-1-2 loss: 0.380895  [   96/  265]
train() client id: f_00001-1-3 loss: 0.506904  [  128/  265]
train() client id: f_00001-1-4 loss: 0.517105  [  160/  265]
train() client id: f_00001-1-5 loss: 0.343588  [  192/  265]
train() client id: f_00001-1-6 loss: 0.351777  [  224/  265]
train() client id: f_00001-1-7 loss: 0.440520  [  256/  265]
train() client id: f_00001-2-0 loss: 0.381905  [   32/  265]
train() client id: f_00001-2-1 loss: 0.346140  [   64/  265]
train() client id: f_00001-2-2 loss: 0.438535  [   96/  265]
train() client id: f_00001-2-3 loss: 0.371930  [  128/  265]
train() client id: f_00001-2-4 loss: 0.373544  [  160/  265]
train() client id: f_00001-2-5 loss: 0.580908  [  192/  265]
train() client id: f_00001-2-6 loss: 0.392781  [  224/  265]
train() client id: f_00001-2-7 loss: 0.444299  [  256/  265]
train() client id: f_00001-3-0 loss: 0.455987  [   32/  265]
train() client id: f_00001-3-1 loss: 0.319445  [   64/  265]
train() client id: f_00001-3-2 loss: 0.391131  [   96/  265]
train() client id: f_00001-3-3 loss: 0.449983  [  128/  265]
train() client id: f_00001-3-4 loss: 0.501099  [  160/  265]
train() client id: f_00001-3-5 loss: 0.409654  [  192/  265]
train() client id: f_00001-3-6 loss: 0.369083  [  224/  265]
train() client id: f_00001-3-7 loss: 0.383464  [  256/  265]
train() client id: f_00001-4-0 loss: 0.414381  [   32/  265]
train() client id: f_00001-4-1 loss: 0.419796  [   64/  265]
train() client id: f_00001-4-2 loss: 0.462555  [   96/  265]
train() client id: f_00001-4-3 loss: 0.366952  [  128/  265]
train() client id: f_00001-4-4 loss: 0.379556  [  160/  265]
train() client id: f_00001-4-5 loss: 0.519286  [  192/  265]
train() client id: f_00001-4-6 loss: 0.311942  [  224/  265]
train() client id: f_00001-4-7 loss: 0.347588  [  256/  265]
train() client id: f_00001-5-0 loss: 0.404508  [   32/  265]
train() client id: f_00001-5-1 loss: 0.329154  [   64/  265]
train() client id: f_00001-5-2 loss: 0.317570  [   96/  265]
train() client id: f_00001-5-3 loss: 0.489277  [  128/  265]
train() client id: f_00001-5-4 loss: 0.359104  [  160/  265]
train() client id: f_00001-5-5 loss: 0.465558  [  192/  265]
train() client id: f_00001-5-6 loss: 0.409641  [  224/  265]
train() client id: f_00001-5-7 loss: 0.348545  [  256/  265]
train() client id: f_00001-6-0 loss: 0.302230  [   32/  265]
train() client id: f_00001-6-1 loss: 0.351716  [   64/  265]
train() client id: f_00001-6-2 loss: 0.591439  [   96/  265]
train() client id: f_00001-6-3 loss: 0.514797  [  128/  265]
train() client id: f_00001-6-4 loss: 0.357063  [  160/  265]
train() client id: f_00001-6-5 loss: 0.343389  [  192/  265]
train() client id: f_00001-6-6 loss: 0.314988  [  224/  265]
train() client id: f_00001-6-7 loss: 0.378486  [  256/  265]
train() client id: f_00001-7-0 loss: 0.312002  [   32/  265]
train() client id: f_00001-7-1 loss: 0.355620  [   64/  265]
train() client id: f_00001-7-2 loss: 0.449178  [   96/  265]
train() client id: f_00001-7-3 loss: 0.346960  [  128/  265]
train() client id: f_00001-7-4 loss: 0.520039  [  160/  265]
train() client id: f_00001-7-5 loss: 0.382867  [  192/  265]
train() client id: f_00001-7-6 loss: 0.449690  [  224/  265]
train() client id: f_00001-7-7 loss: 0.318065  [  256/  265]
train() client id: f_00001-8-0 loss: 0.444136  [   32/  265]
train() client id: f_00001-8-1 loss: 0.384527  [   64/  265]
train() client id: f_00001-8-2 loss: 0.561156  [   96/  265]
train() client id: f_00001-8-3 loss: 0.288183  [  128/  265]
train() client id: f_00001-8-4 loss: 0.394100  [  160/  265]
train() client id: f_00001-8-5 loss: 0.317473  [  192/  265]
train() client id: f_00001-8-6 loss: 0.351383  [  224/  265]
train() client id: f_00001-8-7 loss: 0.370151  [  256/  265]
train() client id: f_00001-9-0 loss: 0.362459  [   32/  265]
train() client id: f_00001-9-1 loss: 0.281710  [   64/  265]
train() client id: f_00001-9-2 loss: 0.279008  [   96/  265]
train() client id: f_00001-9-3 loss: 0.530902  [  128/  265]
train() client id: f_00001-9-4 loss: 0.337150  [  160/  265]
train() client id: f_00001-9-5 loss: 0.336932  [  192/  265]
train() client id: f_00001-9-6 loss: 0.416097  [  224/  265]
train() client id: f_00001-9-7 loss: 0.511448  [  256/  265]
train() client id: f_00001-10-0 loss: 0.361392  [   32/  265]
train() client id: f_00001-10-1 loss: 0.351967  [   64/  265]
train() client id: f_00001-10-2 loss: 0.458795  [   96/  265]
train() client id: f_00001-10-3 loss: 0.355238  [  128/  265]
train() client id: f_00001-10-4 loss: 0.479813  [  160/  265]
train() client id: f_00001-10-5 loss: 0.284438  [  192/  265]
train() client id: f_00001-10-6 loss: 0.390634  [  224/  265]
train() client id: f_00001-10-7 loss: 0.345948  [  256/  265]
train() client id: f_00001-11-0 loss: 0.276693  [   32/  265]
train() client id: f_00001-11-1 loss: 0.426877  [   64/  265]
train() client id: f_00001-11-2 loss: 0.282063  [   96/  265]
train() client id: f_00001-11-3 loss: 0.354820  [  128/  265]
train() client id: f_00001-11-4 loss: 0.463248  [  160/  265]
train() client id: f_00001-11-5 loss: 0.389492  [  192/  265]
train() client id: f_00001-11-6 loss: 0.297613  [  224/  265]
train() client id: f_00001-11-7 loss: 0.495099  [  256/  265]
train() client id: f_00002-0-0 loss: 1.215487  [   32/  124]
train() client id: f_00002-0-1 loss: 1.083309  [   64/  124]
train() client id: f_00002-0-2 loss: 1.057080  [   96/  124]
train() client id: f_00002-1-0 loss: 1.045270  [   32/  124]
train() client id: f_00002-1-1 loss: 1.155200  [   64/  124]
train() client id: f_00002-1-2 loss: 1.037466  [   96/  124]
train() client id: f_00002-2-0 loss: 0.999219  [   32/  124]
train() client id: f_00002-2-1 loss: 1.047755  [   64/  124]
train() client id: f_00002-2-2 loss: 0.969548  [   96/  124]
train() client id: f_00002-3-0 loss: 0.994185  [   32/  124]
train() client id: f_00002-3-1 loss: 1.159266  [   64/  124]
train() client id: f_00002-3-2 loss: 1.001282  [   96/  124]
train() client id: f_00002-4-0 loss: 1.078020  [   32/  124]
train() client id: f_00002-4-1 loss: 0.959345  [   64/  124]
train() client id: f_00002-4-2 loss: 0.926331  [   96/  124]
train() client id: f_00002-5-0 loss: 0.919780  [   32/  124]
train() client id: f_00002-5-1 loss: 1.065931  [   64/  124]
train() client id: f_00002-5-2 loss: 0.944143  [   96/  124]
train() client id: f_00002-6-0 loss: 1.024278  [   32/  124]
train() client id: f_00002-6-1 loss: 0.952351  [   64/  124]
train() client id: f_00002-6-2 loss: 1.021601  [   96/  124]
train() client id: f_00002-7-0 loss: 0.918719  [   32/  124]
train() client id: f_00002-7-1 loss: 1.139695  [   64/  124]
train() client id: f_00002-7-2 loss: 0.892474  [   96/  124]
train() client id: f_00002-8-0 loss: 0.955859  [   32/  124]
train() client id: f_00002-8-1 loss: 0.889167  [   64/  124]
train() client id: f_00002-8-2 loss: 0.986517  [   96/  124]
train() client id: f_00002-9-0 loss: 0.950993  [   32/  124]
train() client id: f_00002-9-1 loss: 0.811697  [   64/  124]
train() client id: f_00002-9-2 loss: 1.094349  [   96/  124]
train() client id: f_00002-10-0 loss: 0.834173  [   32/  124]
train() client id: f_00002-10-1 loss: 0.911678  [   64/  124]
train() client id: f_00002-10-2 loss: 1.142764  [   96/  124]
train() client id: f_00002-11-0 loss: 0.951657  [   32/  124]
train() client id: f_00002-11-1 loss: 0.988460  [   64/  124]
train() client id: f_00002-11-2 loss: 0.896268  [   96/  124]
train() client id: f_00003-0-0 loss: 0.925787  [   32/   43]
train() client id: f_00003-1-0 loss: 0.902689  [   32/   43]
train() client id: f_00003-2-0 loss: 0.962227  [   32/   43]
train() client id: f_00003-3-0 loss: 0.987149  [   32/   43]
train() client id: f_00003-4-0 loss: 0.934835  [   32/   43]
train() client id: f_00003-5-0 loss: 1.014169  [   32/   43]
train() client id: f_00003-6-0 loss: 0.830630  [   32/   43]
train() client id: f_00003-7-0 loss: 0.887972  [   32/   43]
train() client id: f_00003-8-0 loss: 0.956402  [   32/   43]
train() client id: f_00003-9-0 loss: 0.718875  [   32/   43]
train() client id: f_00003-10-0 loss: 0.912152  [   32/   43]
train() client id: f_00003-11-0 loss: 0.986447  [   32/   43]
train() client id: f_00004-0-0 loss: 0.919344  [   32/  306]
train() client id: f_00004-0-1 loss: 1.001996  [   64/  306]
train() client id: f_00004-0-2 loss: 0.937227  [   96/  306]
train() client id: f_00004-0-3 loss: 0.842387  [  128/  306]
train() client id: f_00004-0-4 loss: 0.830652  [  160/  306]
train() client id: f_00004-0-5 loss: 0.954765  [  192/  306]
train() client id: f_00004-0-6 loss: 1.074925  [  224/  306]
train() client id: f_00004-0-7 loss: 0.903265  [  256/  306]
train() client id: f_00004-0-8 loss: 0.858961  [  288/  306]
train() client id: f_00004-1-0 loss: 0.949063  [   32/  306]
train() client id: f_00004-1-1 loss: 1.066147  [   64/  306]
train() client id: f_00004-1-2 loss: 0.986694  [   96/  306]
train() client id: f_00004-1-3 loss: 1.009707  [  128/  306]
train() client id: f_00004-1-4 loss: 0.765654  [  160/  306]
train() client id: f_00004-1-5 loss: 0.973946  [  192/  306]
train() client id: f_00004-1-6 loss: 0.830810  [  224/  306]
train() client id: f_00004-1-7 loss: 0.818074  [  256/  306]
train() client id: f_00004-1-8 loss: 0.921233  [  288/  306]
train() client id: f_00004-2-0 loss: 0.848144  [   32/  306]
train() client id: f_00004-2-1 loss: 0.940618  [   64/  306]
train() client id: f_00004-2-2 loss: 0.892832  [   96/  306]
train() client id: f_00004-2-3 loss: 0.954066  [  128/  306]
train() client id: f_00004-2-4 loss: 0.931927  [  160/  306]
train() client id: f_00004-2-5 loss: 1.021285  [  192/  306]
train() client id: f_00004-2-6 loss: 0.990362  [  224/  306]
train() client id: f_00004-2-7 loss: 0.815449  [  256/  306]
train() client id: f_00004-2-8 loss: 1.007687  [  288/  306]
train() client id: f_00004-3-0 loss: 1.004829  [   32/  306]
train() client id: f_00004-3-1 loss: 0.983342  [   64/  306]
train() client id: f_00004-3-2 loss: 0.891182  [   96/  306]
train() client id: f_00004-3-3 loss: 0.962936  [  128/  306]
train() client id: f_00004-3-4 loss: 0.950508  [  160/  306]
train() client id: f_00004-3-5 loss: 0.948654  [  192/  306]
train() client id: f_00004-3-6 loss: 0.806931  [  224/  306]
train() client id: f_00004-3-7 loss: 0.959918  [  256/  306]
train() client id: f_00004-3-8 loss: 0.835826  [  288/  306]
train() client id: f_00004-4-0 loss: 0.892961  [   32/  306]
train() client id: f_00004-4-1 loss: 0.848379  [   64/  306]
train() client id: f_00004-4-2 loss: 0.880691  [   96/  306]
train() client id: f_00004-4-3 loss: 0.912776  [  128/  306]
train() client id: f_00004-4-4 loss: 0.991521  [  160/  306]
train() client id: f_00004-4-5 loss: 1.031595  [  192/  306]
train() client id: f_00004-4-6 loss: 0.898641  [  224/  306]
train() client id: f_00004-4-7 loss: 0.926915  [  256/  306]
train() client id: f_00004-4-8 loss: 0.949583  [  288/  306]
train() client id: f_00004-5-0 loss: 0.959389  [   32/  306]
train() client id: f_00004-5-1 loss: 0.890329  [   64/  306]
train() client id: f_00004-5-2 loss: 0.905409  [   96/  306]
train() client id: f_00004-5-3 loss: 1.018528  [  128/  306]
train() client id: f_00004-5-4 loss: 0.983784  [  160/  306]
train() client id: f_00004-5-5 loss: 0.889933  [  192/  306]
train() client id: f_00004-5-6 loss: 0.977260  [  224/  306]
train() client id: f_00004-5-7 loss: 0.882002  [  256/  306]
train() client id: f_00004-5-8 loss: 0.835306  [  288/  306]
train() client id: f_00004-6-0 loss: 0.945106  [   32/  306]
train() client id: f_00004-6-1 loss: 0.886181  [   64/  306]
train() client id: f_00004-6-2 loss: 0.934776  [   96/  306]
train() client id: f_00004-6-3 loss: 0.933433  [  128/  306]
train() client id: f_00004-6-4 loss: 0.820725  [  160/  306]
train() client id: f_00004-6-5 loss: 0.904318  [  192/  306]
train() client id: f_00004-6-6 loss: 0.876369  [  224/  306]
train() client id: f_00004-6-7 loss: 0.902121  [  256/  306]
train() client id: f_00004-6-8 loss: 1.015840  [  288/  306]
train() client id: f_00004-7-0 loss: 1.004835  [   32/  306]
train() client id: f_00004-7-1 loss: 0.909370  [   64/  306]
train() client id: f_00004-7-2 loss: 0.931301  [   96/  306]
train() client id: f_00004-7-3 loss: 0.830585  [  128/  306]
train() client id: f_00004-7-4 loss: 0.949766  [  160/  306]
train() client id: f_00004-7-5 loss: 0.939763  [  192/  306]
train() client id: f_00004-7-6 loss: 0.955643  [  224/  306]
train() client id: f_00004-7-7 loss: 0.812570  [  256/  306]
train() client id: f_00004-7-8 loss: 0.963931  [  288/  306]
train() client id: f_00004-8-0 loss: 0.956121  [   32/  306]
train() client id: f_00004-8-1 loss: 0.894841  [   64/  306]
train() client id: f_00004-8-2 loss: 0.898469  [   96/  306]
train() client id: f_00004-8-3 loss: 1.009951  [  128/  306]
train() client id: f_00004-8-4 loss: 0.835230  [  160/  306]
train() client id: f_00004-8-5 loss: 0.922219  [  192/  306]
train() client id: f_00004-8-6 loss: 0.918848  [  224/  306]
train() client id: f_00004-8-7 loss: 0.838646  [  256/  306]
train() client id: f_00004-8-8 loss: 0.929798  [  288/  306]
train() client id: f_00004-9-0 loss: 0.974353  [   32/  306]
train() client id: f_00004-9-1 loss: 0.869896  [   64/  306]
train() client id: f_00004-9-2 loss: 0.909638  [   96/  306]
train() client id: f_00004-9-3 loss: 0.909190  [  128/  306]
train() client id: f_00004-9-4 loss: 0.962387  [  160/  306]
train() client id: f_00004-9-5 loss: 1.014894  [  192/  306]
train() client id: f_00004-9-6 loss: 0.938729  [  224/  306]
train() client id: f_00004-9-7 loss: 0.924254  [  256/  306]
train() client id: f_00004-9-8 loss: 0.817500  [  288/  306]
train() client id: f_00004-10-0 loss: 0.885172  [   32/  306]
train() client id: f_00004-10-1 loss: 1.020111  [   64/  306]
train() client id: f_00004-10-2 loss: 0.952976  [   96/  306]
train() client id: f_00004-10-3 loss: 0.919186  [  128/  306]
train() client id: f_00004-10-4 loss: 0.921998  [  160/  306]
train() client id: f_00004-10-5 loss: 0.943913  [  192/  306]
train() client id: f_00004-10-6 loss: 0.900954  [  224/  306]
train() client id: f_00004-10-7 loss: 0.969921  [  256/  306]
train() client id: f_00004-10-8 loss: 0.798407  [  288/  306]
train() client id: f_00004-11-0 loss: 0.957888  [   32/  306]
train() client id: f_00004-11-1 loss: 0.950980  [   64/  306]
train() client id: f_00004-11-2 loss: 0.859445  [   96/  306]
train() client id: f_00004-11-3 loss: 0.937127  [  128/  306]
train() client id: f_00004-11-4 loss: 0.971221  [  160/  306]
train() client id: f_00004-11-5 loss: 0.844523  [  192/  306]
train() client id: f_00004-11-6 loss: 0.711456  [  224/  306]
train() client id: f_00004-11-7 loss: 1.034420  [  256/  306]
train() client id: f_00004-11-8 loss: 0.947849  [  288/  306]
train() client id: f_00005-0-0 loss: 0.689969  [   32/  146]
train() client id: f_00005-0-1 loss: 0.636776  [   64/  146]
train() client id: f_00005-0-2 loss: 0.870944  [   96/  146]
train() client id: f_00005-0-3 loss: 0.677793  [  128/  146]
train() client id: f_00005-1-0 loss: 0.794063  [   32/  146]
train() client id: f_00005-1-1 loss: 0.679594  [   64/  146]
train() client id: f_00005-1-2 loss: 0.730112  [   96/  146]
train() client id: f_00005-1-3 loss: 0.480628  [  128/  146]
train() client id: f_00005-2-0 loss: 0.795117  [   32/  146]
train() client id: f_00005-2-1 loss: 0.707370  [   64/  146]
train() client id: f_00005-2-2 loss: 0.489641  [   96/  146]
train() client id: f_00005-2-3 loss: 0.696908  [  128/  146]
train() client id: f_00005-3-0 loss: 0.716918  [   32/  146]
train() client id: f_00005-3-1 loss: 0.709470  [   64/  146]
train() client id: f_00005-3-2 loss: 0.917608  [   96/  146]
train() client id: f_00005-3-3 loss: 0.494527  [  128/  146]
train() client id: f_00005-4-0 loss: 0.727544  [   32/  146]
train() client id: f_00005-4-1 loss: 0.752092  [   64/  146]
train() client id: f_00005-4-2 loss: 0.665735  [   96/  146]
train() client id: f_00005-4-3 loss: 0.763947  [  128/  146]
train() client id: f_00005-5-0 loss: 0.822181  [   32/  146]
train() client id: f_00005-5-1 loss: 0.625707  [   64/  146]
train() client id: f_00005-5-2 loss: 0.595635  [   96/  146]
train() client id: f_00005-5-3 loss: 0.712074  [  128/  146]
train() client id: f_00005-6-0 loss: 0.727816  [   32/  146]
train() client id: f_00005-6-1 loss: 0.501503  [   64/  146]
train() client id: f_00005-6-2 loss: 0.693265  [   96/  146]
train() client id: f_00005-6-3 loss: 0.833363  [  128/  146]
train() client id: f_00005-7-0 loss: 0.636222  [   32/  146]
train() client id: f_00005-7-1 loss: 0.615788  [   64/  146]
train() client id: f_00005-7-2 loss: 0.792870  [   96/  146]
train() client id: f_00005-7-3 loss: 0.653366  [  128/  146]
train() client id: f_00005-8-0 loss: 0.868862  [   32/  146]
train() client id: f_00005-8-1 loss: 0.597268  [   64/  146]
train() client id: f_00005-8-2 loss: 0.453253  [   96/  146]
train() client id: f_00005-8-3 loss: 0.878220  [  128/  146]
train() client id: f_00005-9-0 loss: 0.644815  [   32/  146]
train() client id: f_00005-9-1 loss: 0.738366  [   64/  146]
train() client id: f_00005-9-2 loss: 0.773693  [   96/  146]
train() client id: f_00005-9-3 loss: 0.680629  [  128/  146]
train() client id: f_00005-10-0 loss: 0.505451  [   32/  146]
train() client id: f_00005-10-1 loss: 0.825759  [   64/  146]
train() client id: f_00005-10-2 loss: 0.573074  [   96/  146]
train() client id: f_00005-10-3 loss: 0.634973  [  128/  146]
train() client id: f_00005-11-0 loss: 0.770815  [   32/  146]
train() client id: f_00005-11-1 loss: 0.638661  [   64/  146]
train() client id: f_00005-11-2 loss: 0.730385  [   96/  146]
train() client id: f_00005-11-3 loss: 0.656557  [  128/  146]
train() client id: f_00006-0-0 loss: 0.466483  [   32/   54]
train() client id: f_00006-1-0 loss: 0.525321  [   32/   54]
train() client id: f_00006-2-0 loss: 0.562601  [   32/   54]
train() client id: f_00006-3-0 loss: 0.516756  [   32/   54]
train() client id: f_00006-4-0 loss: 0.559973  [   32/   54]
train() client id: f_00006-5-0 loss: 0.513175  [   32/   54]
train() client id: f_00006-6-0 loss: 0.529191  [   32/   54]
train() client id: f_00006-7-0 loss: 0.513282  [   32/   54]
train() client id: f_00006-8-0 loss: 0.557371  [   32/   54]
train() client id: f_00006-9-0 loss: 0.529529  [   32/   54]
train() client id: f_00006-10-0 loss: 0.526330  [   32/   54]
train() client id: f_00006-11-0 loss: 0.552252  [   32/   54]
train() client id: f_00007-0-0 loss: 0.810632  [   32/  179]
train() client id: f_00007-0-1 loss: 0.808725  [   64/  179]
train() client id: f_00007-0-2 loss: 0.675073  [   96/  179]
train() client id: f_00007-0-3 loss: 0.751195  [  128/  179]
train() client id: f_00007-0-4 loss: 0.610785  [  160/  179]
train() client id: f_00007-1-0 loss: 0.838338  [   32/  179]
train() client id: f_00007-1-1 loss: 0.586672  [   64/  179]
train() client id: f_00007-1-2 loss: 0.749870  [   96/  179]
train() client id: f_00007-1-3 loss: 0.769964  [  128/  179]
train() client id: f_00007-1-4 loss: 0.651487  [  160/  179]
train() client id: f_00007-2-0 loss: 0.675992  [   32/  179]
train() client id: f_00007-2-1 loss: 0.600093  [   64/  179]
train() client id: f_00007-2-2 loss: 0.726660  [   96/  179]
train() client id: f_00007-2-3 loss: 0.585034  [  128/  179]
train() client id: f_00007-2-4 loss: 0.803631  [  160/  179]
train() client id: f_00007-3-0 loss: 0.683222  [   32/  179]
train() client id: f_00007-3-1 loss: 0.764867  [   64/  179]
train() client id: f_00007-3-2 loss: 0.632868  [   96/  179]
train() client id: f_00007-3-3 loss: 0.642417  [  128/  179]
train() client id: f_00007-3-4 loss: 0.917488  [  160/  179]
train() client id: f_00007-4-0 loss: 0.772570  [   32/  179]
train() client id: f_00007-4-1 loss: 0.611615  [   64/  179]
train() client id: f_00007-4-2 loss: 0.582209  [   96/  179]
train() client id: f_00007-4-3 loss: 0.692782  [  128/  179]
train() client id: f_00007-4-4 loss: 0.798623  [  160/  179]
train() client id: f_00007-5-0 loss: 0.787581  [   32/  179]
train() client id: f_00007-5-1 loss: 0.678088  [   64/  179]
train() client id: f_00007-5-2 loss: 0.635486  [   96/  179]
train() client id: f_00007-5-3 loss: 0.696502  [  128/  179]
train() client id: f_00007-5-4 loss: 0.662732  [  160/  179]
train() client id: f_00007-6-0 loss: 0.766386  [   32/  179]
train() client id: f_00007-6-1 loss: 0.648943  [   64/  179]
train() client id: f_00007-6-2 loss: 0.567360  [   96/  179]
train() client id: f_00007-6-3 loss: 0.601196  [  128/  179]
train() client id: f_00007-6-4 loss: 0.821596  [  160/  179]
train() client id: f_00007-7-0 loss: 0.704202  [   32/  179]
train() client id: f_00007-7-1 loss: 0.639761  [   64/  179]
train() client id: f_00007-7-2 loss: 0.764245  [   96/  179]
train() client id: f_00007-7-3 loss: 0.753628  [  128/  179]
train() client id: f_00007-7-4 loss: 0.649377  [  160/  179]
train() client id: f_00007-8-0 loss: 0.567294  [   32/  179]
train() client id: f_00007-8-1 loss: 0.589611  [   64/  179]
train() client id: f_00007-8-2 loss: 0.796811  [   96/  179]
train() client id: f_00007-8-3 loss: 0.719311  [  128/  179]
train() client id: f_00007-8-4 loss: 0.734837  [  160/  179]
train() client id: f_00007-9-0 loss: 0.563472  [   32/  179]
train() client id: f_00007-9-1 loss: 0.658172  [   64/  179]
train() client id: f_00007-9-2 loss: 0.726250  [   96/  179]
train() client id: f_00007-9-3 loss: 0.723135  [  128/  179]
train() client id: f_00007-9-4 loss: 0.733133  [  160/  179]
train() client id: f_00007-10-0 loss: 0.717672  [   32/  179]
train() client id: f_00007-10-1 loss: 0.600586  [   64/  179]
train() client id: f_00007-10-2 loss: 0.824531  [   96/  179]
train() client id: f_00007-10-3 loss: 0.711475  [  128/  179]
train() client id: f_00007-10-4 loss: 0.733539  [  160/  179]
train() client id: f_00007-11-0 loss: 0.558266  [   32/  179]
train() client id: f_00007-11-1 loss: 0.737582  [   64/  179]
train() client id: f_00007-11-2 loss: 0.634027  [   96/  179]
train() client id: f_00007-11-3 loss: 0.733949  [  128/  179]
train() client id: f_00007-11-4 loss: 0.719500  [  160/  179]
train() client id: f_00008-0-0 loss: 0.887098  [   32/  130]
train() client id: f_00008-0-1 loss: 0.836295  [   64/  130]
train() client id: f_00008-0-2 loss: 0.947545  [   96/  130]
train() client id: f_00008-0-3 loss: 0.800948  [  128/  130]
train() client id: f_00008-1-0 loss: 0.872012  [   32/  130]
train() client id: f_00008-1-1 loss: 0.850015  [   64/  130]
train() client id: f_00008-1-2 loss: 0.963957  [   96/  130]
train() client id: f_00008-1-3 loss: 0.813137  [  128/  130]
train() client id: f_00008-2-0 loss: 0.923418  [   32/  130]
train() client id: f_00008-2-1 loss: 0.854423  [   64/  130]
train() client id: f_00008-2-2 loss: 0.927394  [   96/  130]
train() client id: f_00008-2-3 loss: 0.795864  [  128/  130]
train() client id: f_00008-3-0 loss: 0.936003  [   32/  130]
train() client id: f_00008-3-1 loss: 0.847973  [   64/  130]
train() client id: f_00008-3-2 loss: 0.859757  [   96/  130]
train() client id: f_00008-3-3 loss: 0.829153  [  128/  130]
train() client id: f_00008-4-0 loss: 0.820868  [   32/  130]
train() client id: f_00008-4-1 loss: 0.942517  [   64/  130]
train() client id: f_00008-4-2 loss: 0.813024  [   96/  130]
train() client id: f_00008-4-3 loss: 0.929909  [  128/  130]
train() client id: f_00008-5-0 loss: 0.965467  [   32/  130]
train() client id: f_00008-5-1 loss: 0.791211  [   64/  130]
train() client id: f_00008-5-2 loss: 0.831789  [   96/  130]
train() client id: f_00008-5-3 loss: 0.879725  [  128/  130]
train() client id: f_00008-6-0 loss: 0.820230  [   32/  130]
train() client id: f_00008-6-1 loss: 0.919065  [   64/  130]
train() client id: f_00008-6-2 loss: 0.789566  [   96/  130]
train() client id: f_00008-6-3 loss: 0.987652  [  128/  130]
train() client id: f_00008-7-0 loss: 1.014352  [   32/  130]
train() client id: f_00008-7-1 loss: 0.818173  [   64/  130]
train() client id: f_00008-7-2 loss: 0.877908  [   96/  130]
train() client id: f_00008-7-3 loss: 0.802385  [  128/  130]
train() client id: f_00008-8-0 loss: 0.923384  [   32/  130]
train() client id: f_00008-8-1 loss: 0.745977  [   64/  130]
train() client id: f_00008-8-2 loss: 0.824464  [   96/  130]
train() client id: f_00008-8-3 loss: 1.011232  [  128/  130]
train() client id: f_00008-9-0 loss: 0.855511  [   32/  130]
train() client id: f_00008-9-1 loss: 0.962644  [   64/  130]
train() client id: f_00008-9-2 loss: 0.881084  [   96/  130]
train() client id: f_00008-9-3 loss: 0.764650  [  128/  130]
train() client id: f_00008-10-0 loss: 0.882358  [   32/  130]
train() client id: f_00008-10-1 loss: 0.850712  [   64/  130]
train() client id: f_00008-10-2 loss: 0.942517  [   96/  130]
train() client id: f_00008-10-3 loss: 0.808364  [  128/  130]
train() client id: f_00008-11-0 loss: 0.962504  [   32/  130]
train() client id: f_00008-11-1 loss: 0.786032  [   64/  130]
train() client id: f_00008-11-2 loss: 0.819865  [   96/  130]
train() client id: f_00008-11-3 loss: 0.922038  [  128/  130]
train() client id: f_00009-0-0 loss: 1.247412  [   32/  118]
train() client id: f_00009-0-1 loss: 1.031105  [   64/  118]
train() client id: f_00009-0-2 loss: 1.000374  [   96/  118]
train() client id: f_00009-1-0 loss: 1.010466  [   32/  118]
train() client id: f_00009-1-1 loss: 1.006096  [   64/  118]
train() client id: f_00009-1-2 loss: 1.038042  [   96/  118]
train() client id: f_00009-2-0 loss: 0.866397  [   32/  118]
train() client id: f_00009-2-1 loss: 0.992711  [   64/  118]
train() client id: f_00009-2-2 loss: 1.099913  [   96/  118]
train() client id: f_00009-3-0 loss: 0.810972  [   32/  118]
train() client id: f_00009-3-1 loss: 0.900350  [   64/  118]
train() client id: f_00009-3-2 loss: 1.078091  [   96/  118]
train() client id: f_00009-4-0 loss: 0.897757  [   32/  118]
train() client id: f_00009-4-1 loss: 0.926692  [   64/  118]
train() client id: f_00009-4-2 loss: 0.984875  [   96/  118]
train() client id: f_00009-5-0 loss: 1.070907  [   32/  118]
train() client id: f_00009-5-1 loss: 0.908814  [   64/  118]
train() client id: f_00009-5-2 loss: 0.808267  [   96/  118]
train() client id: f_00009-6-0 loss: 0.907605  [   32/  118]
train() client id: f_00009-6-1 loss: 0.777249  [   64/  118]
train() client id: f_00009-6-2 loss: 1.024548  [   96/  118]
train() client id: f_00009-7-0 loss: 0.864868  [   32/  118]
train() client id: f_00009-7-1 loss: 0.861003  [   64/  118]
train() client id: f_00009-7-2 loss: 0.779125  [   96/  118]
train() client id: f_00009-8-0 loss: 0.901894  [   32/  118]
train() client id: f_00009-8-1 loss: 0.751592  [   64/  118]
train() client id: f_00009-8-2 loss: 0.847741  [   96/  118]
train() client id: f_00009-9-0 loss: 0.803711  [   32/  118]
train() client id: f_00009-9-1 loss: 0.913357  [   64/  118]
train() client id: f_00009-9-2 loss: 0.930333  [   96/  118]
train() client id: f_00009-10-0 loss: 0.961536  [   32/  118]
train() client id: f_00009-10-1 loss: 0.795595  [   64/  118]
train() client id: f_00009-10-2 loss: 0.836865  [   96/  118]
train() client id: f_00009-11-0 loss: 0.818770  [   32/  118]
train() client id: f_00009-11-1 loss: 0.823443  [   64/  118]
train() client id: f_00009-11-2 loss: 0.787966  [   96/  118]
At round 21 accuracy: 0.636604774535809
At round 21 training accuracy: 0.5761234071093226
At round 21 training loss: 0.8451854837144311
update_location
xs = [ -3.9056584    4.20031788 125.00902392  18.81129433   0.97929623
   3.95640986 -87.44319194 -66.32485185 109.66397685 -52.06087855]
ys = [117.5879595  100.55583871   1.32061395 -87.45517586  79.35018685
  62.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [154.40913958 141.87712771 160.0905996  134.17254778 127.66131432
 118.15782121 132.86535425 119.99859261 149.44851217 112.81111174]
dists_bs = [180.46998676 194.04815659 346.65536714 326.25992268 200.29378287
 211.21330783 198.03239556 205.30012735 325.29693755 210.60513008]
uav_gains = [3.37016863e-11 4.16871884e-11 3.07625343e-11 4.79445886e-11
 5.43001226e-11 6.58910772e-11 4.91340014e-11 6.33925495e-11
 3.65879032e-11 7.39794975e-11]
bs_gains = [5.31324844e-11 4.33658063e-11 8.54239964e-12 1.01231494e-11
 3.96848892e-11 3.42038117e-11 4.09668536e-11 3.70343113e-11
 1.02072830e-11 3.44810940e-11]
Round 22
-------------------------------
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.08847719 16.82258033  7.96606227  2.85607422 19.40433678  9.34589642
  3.54718528 11.40188453  8.39827017  7.58446914]
obj_prev = 95.4152363247567
eta_min = 3.3224549121166505e-12	eta_max = 0.9228798704318716
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 22.17659221087636	eta = 0.909090909090909
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 38.93866072382321	eta = 0.5177512014733847
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 30.874641848632802	eta = 0.6529804773886541
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.424734721470024	eta = 0.6851561641714188
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.351835344463147	eta = 0.6868578450692009
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.351637432902866	eta = 0.6868624763988199
eta = 0.6868624763988199
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [0.03099536 0.06518865 0.03050337 0.01057778 0.07527445 0.03591526
 0.01328373 0.04403309 0.03197934 0.0290274 ]
ene_total = [2.56014735 4.7872217  2.53839595 1.17632747 5.46154932 2.88284624
 1.35232746 3.3560089  2.80903437 2.42777865]
ti_comp = [0.35997945 0.36414214 0.35835849 0.36568135 0.36272784 0.36023616
 0.36604767 0.36965205 0.33207169 0.36037562]
ti_coms = [0.07833233 0.07416965 0.07995329 0.07263043 0.07558394 0.07807562
 0.07226411 0.06865973 0.1062401  0.07793616]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.43619908e-05 1.30573243e-04 1.38130026e-05 5.53169442e-07
 2.02610025e-04 2.23121998e-05 1.09336312e-06 3.90509212e-05
 1.85363747e-05 1.17704659e-05]
ene_total = [0.51742795 0.49764382 0.5280795  0.47892225 0.51171862 0.51625955
 0.47654255 0.45527992 0.70171191 0.51464497]
optimize_network_iter = 0 obj = 5.19823104157566
eta = 0.6868624763988199
freqs = [4.30515682e+07 8.95099024e+07 4.25598540e+07 1.44631133e+07
 1.03761604e+08 4.98496017e+07 1.81448029e+07 5.95601869e+07
 4.81512593e+07 4.02738129e+07]
eta_min = 0.6868624763988167	eta_max = 0.6868624763987461
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 0.02749108426556509	eta = 0.9090909090909091
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 17.828312615008755	eta = 0.0014018093201842298
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.7850725781134285	eta = 0.014000492245133406
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.749550246261795	eta = 0.014284753947636938
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.7495451585936168	eta = 0.014284795487626767
eta = 0.014284795487626767
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.62444973e-04 1.47688208e-03 1.56235501e-04 6.25676453e-06
 2.29167254e-03 2.52367846e-04 1.23667633e-05 4.41695438e-04
 2.09660410e-04 1.33132867e-04]
ene_total = [0.16747587 0.18628874 0.17074105 0.15226131 0.20631753 0.16882169
 0.151622   0.15306496 0.22691989 0.16603211]
ti_comp = [0.35997945 0.36414214 0.35835849 0.36568135 0.36272784 0.36023616
 0.36604767 0.36965205 0.33207169 0.36037562]
ti_coms = [0.07833233 0.07416965 0.07995329 0.07263043 0.07558394 0.07807562
 0.07226411 0.06865973 0.1062401  0.07793616]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.43619908e-05 1.30573243e-04 1.38130026e-05 5.53169442e-07
 2.02610025e-04 2.23121998e-05 1.09336312e-06 3.90509212e-05
 1.85363747e-05 1.17704659e-05]
ene_total = [0.51742795 0.49764382 0.5280795  0.47892225 0.51171862 0.51625955
 0.47654255 0.45527992 0.70171191 0.51464497]
optimize_network_iter = 1 obj = 5.198231041575606
eta = 0.6868624763988167
freqs = [4.30515682e+07 8.95099024e+07 4.25598540e+07 1.44631133e+07
 1.03761604e+08 4.98496017e+07 1.81448029e+07 5.95601869e+07
 4.81512593e+07 4.02738129e+07]
Done!
At round 22 eta: 0.6868624763988167
At round 22 local rounds: 12.299746037466248
At round 22 global rounds: 65.93458997020494
At round 22 a_n: 20.30404837596008
gradient difference: 0.40073639154434204
train() client id: f_00000-0-0 loss: 1.372140  [   32/  126]
train() client id: f_00000-0-1 loss: 1.267360  [   64/  126]
train() client id: f_00000-0-2 loss: 1.070969  [   96/  126]
train() client id: f_00000-1-0 loss: 1.116838  [   32/  126]
train() client id: f_00000-1-1 loss: 1.240665  [   64/  126]
train() client id: f_00000-1-2 loss: 1.051863  [   96/  126]
train() client id: f_00000-2-0 loss: 1.094053  [   32/  126]
train() client id: f_00000-2-1 loss: 1.140113  [   64/  126]
train() client id: f_00000-2-2 loss: 0.936923  [   96/  126]
train() client id: f_00000-3-0 loss: 1.031235  [   32/  126]
train() client id: f_00000-3-1 loss: 0.902687  [   64/  126]
train() client id: f_00000-3-2 loss: 1.019104  [   96/  126]
train() client id: f_00000-4-0 loss: 1.031127  [   32/  126]
train() client id: f_00000-4-1 loss: 0.885664  [   64/  126]
train() client id: f_00000-4-2 loss: 0.931269  [   96/  126]
train() client id: f_00000-5-0 loss: 0.906357  [   32/  126]
train() client id: f_00000-5-1 loss: 0.971990  [   64/  126]
train() client id: f_00000-5-2 loss: 0.848952  [   96/  126]
train() client id: f_00000-6-0 loss: 0.884372  [   32/  126]
train() client id: f_00000-6-1 loss: 0.922605  [   64/  126]
train() client id: f_00000-6-2 loss: 0.898704  [   96/  126]
train() client id: f_00000-7-0 loss: 0.853502  [   32/  126]
train() client id: f_00000-7-1 loss: 0.902688  [   64/  126]
train() client id: f_00000-7-2 loss: 0.855025  [   96/  126]
train() client id: f_00000-8-0 loss: 0.940385  [   32/  126]
train() client id: f_00000-8-1 loss: 0.758959  [   64/  126]
train() client id: f_00000-8-2 loss: 0.938695  [   96/  126]
train() client id: f_00000-9-0 loss: 0.731196  [   32/  126]
train() client id: f_00000-9-1 loss: 0.859295  [   64/  126]
train() client id: f_00000-9-2 loss: 0.933900  [   96/  126]
train() client id: f_00000-10-0 loss: 0.845064  [   32/  126]
train() client id: f_00000-10-1 loss: 0.757037  [   64/  126]
train() client id: f_00000-10-2 loss: 0.958232  [   96/  126]
train() client id: f_00000-11-0 loss: 0.816310  [   32/  126]
train() client id: f_00000-11-1 loss: 0.807505  [   64/  126]
train() client id: f_00000-11-2 loss: 0.876053  [   96/  126]
train() client id: f_00001-0-0 loss: 0.455972  [   32/  265]
train() client id: f_00001-0-1 loss: 0.387694  [   64/  265]
train() client id: f_00001-0-2 loss: 0.337560  [   96/  265]
train() client id: f_00001-0-3 loss: 0.403676  [  128/  265]
train() client id: f_00001-0-4 loss: 0.371653  [  160/  265]
train() client id: f_00001-0-5 loss: 0.450491  [  192/  265]
train() client id: f_00001-0-6 loss: 0.445431  [  224/  265]
train() client id: f_00001-0-7 loss: 0.490588  [  256/  265]
train() client id: f_00001-1-0 loss: 0.438219  [   32/  265]
train() client id: f_00001-1-1 loss: 0.394670  [   64/  265]
train() client id: f_00001-1-2 loss: 0.494635  [   96/  265]
train() client id: f_00001-1-3 loss: 0.404365  [  128/  265]
train() client id: f_00001-1-4 loss: 0.331766  [  160/  265]
train() client id: f_00001-1-5 loss: 0.390106  [  192/  265]
train() client id: f_00001-1-6 loss: 0.426055  [  224/  265]
train() client id: f_00001-1-7 loss: 0.350847  [  256/  265]
train() client id: f_00001-2-0 loss: 0.360947  [   32/  265]
train() client id: f_00001-2-1 loss: 0.535453  [   64/  265]
train() client id: f_00001-2-2 loss: 0.343267  [   96/  265]
train() client id: f_00001-2-3 loss: 0.455146  [  128/  265]
train() client id: f_00001-2-4 loss: 0.341375  [  160/  265]
train() client id: f_00001-2-5 loss: 0.419391  [  192/  265]
train() client id: f_00001-2-6 loss: 0.306213  [  224/  265]
train() client id: f_00001-2-7 loss: 0.417724  [  256/  265]
train() client id: f_00001-3-0 loss: 0.424219  [   32/  265]
train() client id: f_00001-3-1 loss: 0.302797  [   64/  265]
train() client id: f_00001-3-2 loss: 0.354342  [   96/  265]
train() client id: f_00001-3-3 loss: 0.461285  [  128/  265]
train() client id: f_00001-3-4 loss: 0.446878  [  160/  265]
train() client id: f_00001-3-5 loss: 0.442519  [  192/  265]
train() client id: f_00001-3-6 loss: 0.290399  [  224/  265]
train() client id: f_00001-3-7 loss: 0.340689  [  256/  265]
train() client id: f_00001-4-0 loss: 0.399772  [   32/  265]
train() client id: f_00001-4-1 loss: 0.342758  [   64/  265]
train() client id: f_00001-4-2 loss: 0.304437  [   96/  265]
train() client id: f_00001-4-3 loss: 0.512360  [  128/  265]
train() client id: f_00001-4-4 loss: 0.414330  [  160/  265]
train() client id: f_00001-4-5 loss: 0.344432  [  192/  265]
train() client id: f_00001-4-6 loss: 0.364132  [  224/  265]
train() client id: f_00001-4-7 loss: 0.302459  [  256/  265]
train() client id: f_00001-5-0 loss: 0.341144  [   32/  265]
train() client id: f_00001-5-1 loss: 0.525846  [   64/  265]
train() client id: f_00001-5-2 loss: 0.425673  [   96/  265]
train() client id: f_00001-5-3 loss: 0.310806  [  128/  265]
train() client id: f_00001-5-4 loss: 0.403443  [  160/  265]
train() client id: f_00001-5-5 loss: 0.373572  [  192/  265]
train() client id: f_00001-5-6 loss: 0.361288  [  224/  265]
train() client id: f_00001-5-7 loss: 0.277187  [  256/  265]
train() client id: f_00001-6-0 loss: 0.384453  [   32/  265]
train() client id: f_00001-6-1 loss: 0.450616  [   64/  265]
train() client id: f_00001-6-2 loss: 0.336631  [   96/  265]
train() client id: f_00001-6-3 loss: 0.330136  [  128/  265]
train() client id: f_00001-6-4 loss: 0.339368  [  160/  265]
train() client id: f_00001-6-5 loss: 0.347421  [  192/  265]
train() client id: f_00001-6-6 loss: 0.356013  [  224/  265]
train() client id: f_00001-6-7 loss: 0.357315  [  256/  265]
train() client id: f_00001-7-0 loss: 0.376648  [   32/  265]
train() client id: f_00001-7-1 loss: 0.444848  [   64/  265]
train() client id: f_00001-7-2 loss: 0.330638  [   96/  265]
train() client id: f_00001-7-3 loss: 0.277488  [  128/  265]
train() client id: f_00001-7-4 loss: 0.476146  [  160/  265]
train() client id: f_00001-7-5 loss: 0.409525  [  192/  265]
train() client id: f_00001-7-6 loss: 0.307934  [  224/  265]
train() client id: f_00001-7-7 loss: 0.328587  [  256/  265]
train() client id: f_00001-8-0 loss: 0.330273  [   32/  265]
train() client id: f_00001-8-1 loss: 0.328206  [   64/  265]
train() client id: f_00001-8-2 loss: 0.264047  [   96/  265]
train() client id: f_00001-8-3 loss: 0.456158  [  128/  265]
train() client id: f_00001-8-4 loss: 0.441905  [  160/  265]
train() client id: f_00001-8-5 loss: 0.294269  [  192/  265]
train() client id: f_00001-8-6 loss: 0.456465  [  224/  265]
train() client id: f_00001-8-7 loss: 0.358482  [  256/  265]
train() client id: f_00001-9-0 loss: 0.309374  [   32/  265]
train() client id: f_00001-9-1 loss: 0.320338  [   64/  265]
train() client id: f_00001-9-2 loss: 0.424779  [   96/  265]
train() client id: f_00001-9-3 loss: 0.398644  [  128/  265]
train() client id: f_00001-9-4 loss: 0.319581  [  160/  265]
train() client id: f_00001-9-5 loss: 0.351161  [  192/  265]
train() client id: f_00001-9-6 loss: 0.483030  [  224/  265]
train() client id: f_00001-9-7 loss: 0.326545  [  256/  265]
train() client id: f_00001-10-0 loss: 0.405799  [   32/  265]
train() client id: f_00001-10-1 loss: 0.264726  [   64/  265]
train() client id: f_00001-10-2 loss: 0.325816  [   96/  265]
train() client id: f_00001-10-3 loss: 0.385934  [  128/  265]
train() client id: f_00001-10-4 loss: 0.408660  [  160/  265]
train() client id: f_00001-10-5 loss: 0.425586  [  192/  265]
train() client id: f_00001-10-6 loss: 0.295242  [  224/  265]
train() client id: f_00001-10-7 loss: 0.402938  [  256/  265]
train() client id: f_00001-11-0 loss: 0.265569  [   32/  265]
train() client id: f_00001-11-1 loss: 0.526515  [   64/  265]
train() client id: f_00001-11-2 loss: 0.338135  [   96/  265]
train() client id: f_00001-11-3 loss: 0.409588  [  128/  265]
train() client id: f_00001-11-4 loss: 0.420870  [  160/  265]
train() client id: f_00001-11-5 loss: 0.289465  [  192/  265]
train() client id: f_00001-11-6 loss: 0.316654  [  224/  265]
train() client id: f_00001-11-7 loss: 0.250763  [  256/  265]
train() client id: f_00002-0-0 loss: 1.325587  [   32/  124]
train() client id: f_00002-0-1 loss: 1.323165  [   64/  124]
train() client id: f_00002-0-2 loss: 1.175525  [   96/  124]
train() client id: f_00002-1-0 loss: 1.284863  [   32/  124]
train() client id: f_00002-1-1 loss: 1.238764  [   64/  124]
train() client id: f_00002-1-2 loss: 1.243073  [   96/  124]
train() client id: f_00002-2-0 loss: 1.304844  [   32/  124]
train() client id: f_00002-2-1 loss: 1.147470  [   64/  124]
train() client id: f_00002-2-2 loss: 1.132734  [   96/  124]
train() client id: f_00002-3-0 loss: 1.186746  [   32/  124]
train() client id: f_00002-3-1 loss: 1.074235  [   64/  124]
train() client id: f_00002-3-2 loss: 1.322841  [   96/  124]
train() client id: f_00002-4-0 loss: 1.219790  [   32/  124]
train() client id: f_00002-4-1 loss: 1.155643  [   64/  124]
train() client id: f_00002-4-2 loss: 1.122599  [   96/  124]
train() client id: f_00002-5-0 loss: 1.193681  [   32/  124]
train() client id: f_00002-5-1 loss: 1.068831  [   64/  124]
train() client id: f_00002-5-2 loss: 1.110406  [   96/  124]
train() client id: f_00002-6-0 loss: 1.050472  [   32/  124]
train() client id: f_00002-6-1 loss: 1.195035  [   64/  124]
train() client id: f_00002-6-2 loss: 1.039358  [   96/  124]
train() client id: f_00002-7-0 loss: 1.049941  [   32/  124]
train() client id: f_00002-7-1 loss: 0.931551  [   64/  124]
train() client id: f_00002-7-2 loss: 1.196379  [   96/  124]
train() client id: f_00002-8-0 loss: 1.158059  [   32/  124]
train() client id: f_00002-8-1 loss: 1.138211  [   64/  124]
train() client id: f_00002-8-2 loss: 0.973428  [   96/  124]
train() client id: f_00002-9-0 loss: 0.947367  [   32/  124]
train() client id: f_00002-9-1 loss: 1.041047  [   64/  124]
train() client id: f_00002-9-2 loss: 1.074117  [   96/  124]
train() client id: f_00002-10-0 loss: 1.092688  [   32/  124]
train() client id: f_00002-10-1 loss: 1.007463  [   64/  124]
train() client id: f_00002-10-2 loss: 1.046953  [   96/  124]
train() client id: f_00002-11-0 loss: 1.005756  [   32/  124]
train() client id: f_00002-11-1 loss: 1.082137  [   64/  124]
train() client id: f_00002-11-2 loss: 1.036316  [   96/  124]
train() client id: f_00003-0-0 loss: 0.816810  [   32/   43]
train() client id: f_00003-1-0 loss: 0.683696  [   32/   43]
train() client id: f_00003-2-0 loss: 0.753896  [   32/   43]
train() client id: f_00003-3-0 loss: 0.884931  [   32/   43]
train() client id: f_00003-4-0 loss: 0.841660  [   32/   43]
train() client id: f_00003-5-0 loss: 0.614196  [   32/   43]
train() client id: f_00003-6-0 loss: 0.728798  [   32/   43]
train() client id: f_00003-7-0 loss: 0.781664  [   32/   43]
train() client id: f_00003-8-0 loss: 0.656563  [   32/   43]
train() client id: f_00003-9-0 loss: 0.845813  [   32/   43]
train() client id: f_00003-10-0 loss: 0.791436  [   32/   43]
train() client id: f_00003-11-0 loss: 0.568949  [   32/   43]
train() client id: f_00004-0-0 loss: 0.763309  [   32/  306]
train() client id: f_00004-0-1 loss: 0.940385  [   64/  306]
train() client id: f_00004-0-2 loss: 0.703194  [   96/  306]
train() client id: f_00004-0-3 loss: 0.775542  [  128/  306]
train() client id: f_00004-0-4 loss: 0.913293  [  160/  306]
train() client id: f_00004-0-5 loss: 0.657403  [  192/  306]
train() client id: f_00004-0-6 loss: 0.856711  [  224/  306]
train() client id: f_00004-0-7 loss: 0.862762  [  256/  306]
train() client id: f_00004-0-8 loss: 0.992323  [  288/  306]
train() client id: f_00004-1-0 loss: 0.695072  [   32/  306]
train() client id: f_00004-1-1 loss: 0.742275  [   64/  306]
train() client id: f_00004-1-2 loss: 0.995464  [   96/  306]
train() client id: f_00004-1-3 loss: 0.822910  [  128/  306]
train() client id: f_00004-1-4 loss: 0.831549  [  160/  306]
train() client id: f_00004-1-5 loss: 0.799139  [  192/  306]
train() client id: f_00004-1-6 loss: 0.881135  [  224/  306]
train() client id: f_00004-1-7 loss: 0.690552  [  256/  306]
train() client id: f_00004-1-8 loss: 1.029035  [  288/  306]
train() client id: f_00004-2-0 loss: 0.649878  [   32/  306]
train() client id: f_00004-2-1 loss: 0.956644  [   64/  306]
train() client id: f_00004-2-2 loss: 0.811969  [   96/  306]
train() client id: f_00004-2-3 loss: 0.742609  [  128/  306]
train() client id: f_00004-2-4 loss: 0.825333  [  160/  306]
train() client id: f_00004-2-5 loss: 0.823692  [  192/  306]
train() client id: f_00004-2-6 loss: 0.788002  [  224/  306]
train() client id: f_00004-2-7 loss: 1.002476  [  256/  306]
train() client id: f_00004-2-8 loss: 0.794132  [  288/  306]
train() client id: f_00004-3-0 loss: 0.783044  [   32/  306]
train() client id: f_00004-3-1 loss: 0.854301  [   64/  306]
train() client id: f_00004-3-2 loss: 0.814760  [   96/  306]
train() client id: f_00004-3-3 loss: 0.875998  [  128/  306]
train() client id: f_00004-3-4 loss: 0.789944  [  160/  306]
train() client id: f_00004-3-5 loss: 0.775208  [  192/  306]
train() client id: f_00004-3-6 loss: 0.861475  [  224/  306]
train() client id: f_00004-3-7 loss: 0.857754  [  256/  306]
train() client id: f_00004-3-8 loss: 0.847471  [  288/  306]
train() client id: f_00004-4-0 loss: 0.713105  [   32/  306]
train() client id: f_00004-4-1 loss: 0.795969  [   64/  306]
train() client id: f_00004-4-2 loss: 0.858183  [   96/  306]
train() client id: f_00004-4-3 loss: 0.801026  [  128/  306]
train() client id: f_00004-4-4 loss: 0.829480  [  160/  306]
train() client id: f_00004-4-5 loss: 0.946227  [  192/  306]
train() client id: f_00004-4-6 loss: 0.791684  [  224/  306]
train() client id: f_00004-4-7 loss: 0.845693  [  256/  306]
train() client id: f_00004-4-8 loss: 0.789543  [  288/  306]
train() client id: f_00004-5-0 loss: 0.853370  [   32/  306]
train() client id: f_00004-5-1 loss: 0.825190  [   64/  306]
train() client id: f_00004-5-2 loss: 0.801776  [   96/  306]
train() client id: f_00004-5-3 loss: 0.861002  [  128/  306]
train() client id: f_00004-5-4 loss: 0.735860  [  160/  306]
train() client id: f_00004-5-5 loss: 0.833254  [  192/  306]
train() client id: f_00004-5-6 loss: 0.807285  [  224/  306]
train() client id: f_00004-5-7 loss: 0.846730  [  256/  306]
train() client id: f_00004-5-8 loss: 0.771735  [  288/  306]
train() client id: f_00004-6-0 loss: 0.869657  [   32/  306]
train() client id: f_00004-6-1 loss: 0.866315  [   64/  306]
train() client id: f_00004-6-2 loss: 0.839414  [   96/  306]
train() client id: f_00004-6-3 loss: 0.764080  [  128/  306]
train() client id: f_00004-6-4 loss: 0.782843  [  160/  306]
train() client id: f_00004-6-5 loss: 0.850172  [  192/  306]
train() client id: f_00004-6-6 loss: 0.839297  [  224/  306]
train() client id: f_00004-6-7 loss: 0.802216  [  256/  306]
train() client id: f_00004-6-8 loss: 0.745672  [  288/  306]
train() client id: f_00004-7-0 loss: 0.704866  [   32/  306]
train() client id: f_00004-7-1 loss: 0.776934  [   64/  306]
train() client id: f_00004-7-2 loss: 0.864019  [   96/  306]
train() client id: f_00004-7-3 loss: 1.021577  [  128/  306]
train() client id: f_00004-7-4 loss: 0.840882  [  160/  306]
train() client id: f_00004-7-5 loss: 0.782974  [  192/  306]
train() client id: f_00004-7-6 loss: 0.851517  [  224/  306]
train() client id: f_00004-7-7 loss: 0.756250  [  256/  306]
train() client id: f_00004-7-8 loss: 0.761891  [  288/  306]
train() client id: f_00004-8-0 loss: 0.816286  [   32/  306]
train() client id: f_00004-8-1 loss: 0.906051  [   64/  306]
train() client id: f_00004-8-2 loss: 0.698550  [   96/  306]
train() client id: f_00004-8-3 loss: 0.760532  [  128/  306]
train() client id: f_00004-8-4 loss: 0.889338  [  160/  306]
train() client id: f_00004-8-5 loss: 0.816956  [  192/  306]
train() client id: f_00004-8-6 loss: 0.803963  [  224/  306]
train() client id: f_00004-8-7 loss: 0.860398  [  256/  306]
train() client id: f_00004-8-8 loss: 0.862927  [  288/  306]
train() client id: f_00004-9-0 loss: 0.856408  [   32/  306]
train() client id: f_00004-9-1 loss: 0.803834  [   64/  306]
train() client id: f_00004-9-2 loss: 0.804681  [   96/  306]
train() client id: f_00004-9-3 loss: 0.823324  [  128/  306]
train() client id: f_00004-9-4 loss: 0.721679  [  160/  306]
train() client id: f_00004-9-5 loss: 0.806297  [  192/  306]
train() client id: f_00004-9-6 loss: 0.850194  [  224/  306]
train() client id: f_00004-9-7 loss: 0.853970  [  256/  306]
train() client id: f_00004-9-8 loss: 0.856363  [  288/  306]
train() client id: f_00004-10-0 loss: 0.855387  [   32/  306]
train() client id: f_00004-10-1 loss: 0.820146  [   64/  306]
train() client id: f_00004-10-2 loss: 0.763303  [   96/  306]
train() client id: f_00004-10-3 loss: 0.830175  [  128/  306]
train() client id: f_00004-10-4 loss: 0.803712  [  160/  306]
train() client id: f_00004-10-5 loss: 0.855681  [  192/  306]
train() client id: f_00004-10-6 loss: 0.861558  [  224/  306]
train() client id: f_00004-10-7 loss: 0.662045  [  256/  306]
train() client id: f_00004-10-8 loss: 0.889386  [  288/  306]
train() client id: f_00004-11-0 loss: 0.789553  [   32/  306]
train() client id: f_00004-11-1 loss: 0.889800  [   64/  306]
train() client id: f_00004-11-2 loss: 0.842755  [   96/  306]
train() client id: f_00004-11-3 loss: 0.792773  [  128/  306]
train() client id: f_00004-11-4 loss: 0.719909  [  160/  306]
train() client id: f_00004-11-5 loss: 0.829846  [  192/  306]
train() client id: f_00004-11-6 loss: 0.753569  [  224/  306]
train() client id: f_00004-11-7 loss: 0.796176  [  256/  306]
train() client id: f_00004-11-8 loss: 0.948936  [  288/  306]
train() client id: f_00005-0-0 loss: 0.853390  [   32/  146]
train() client id: f_00005-0-1 loss: 0.791107  [   64/  146]
train() client id: f_00005-0-2 loss: 0.648079  [   96/  146]
train() client id: f_00005-0-3 loss: 0.663728  [  128/  146]
train() client id: f_00005-1-0 loss: 0.676187  [   32/  146]
train() client id: f_00005-1-1 loss: 0.804779  [   64/  146]
train() client id: f_00005-1-2 loss: 0.664641  [   96/  146]
train() client id: f_00005-1-3 loss: 0.695207  [  128/  146]
train() client id: f_00005-2-0 loss: 0.737432  [   32/  146]
train() client id: f_00005-2-1 loss: 0.882216  [   64/  146]
train() client id: f_00005-2-2 loss: 0.738532  [   96/  146]
train() client id: f_00005-2-3 loss: 0.674362  [  128/  146]
train() client id: f_00005-3-0 loss: 0.702166  [   32/  146]
train() client id: f_00005-3-1 loss: 0.606080  [   64/  146]
train() client id: f_00005-3-2 loss: 0.787738  [   96/  146]
train() client id: f_00005-3-3 loss: 0.737298  [  128/  146]
train() client id: f_00005-4-0 loss: 0.756790  [   32/  146]
train() client id: f_00005-4-1 loss: 0.675086  [   64/  146]
train() client id: f_00005-4-2 loss: 0.828735  [   96/  146]
train() client id: f_00005-4-3 loss: 0.731198  [  128/  146]
train() client id: f_00005-5-0 loss: 0.783971  [   32/  146]
train() client id: f_00005-5-1 loss: 0.836018  [   64/  146]
train() client id: f_00005-5-2 loss: 0.770855  [   96/  146]
train() client id: f_00005-5-3 loss: 0.624462  [  128/  146]
train() client id: f_00005-6-0 loss: 0.718996  [   32/  146]
train() client id: f_00005-6-1 loss: 0.873634  [   64/  146]
train() client id: f_00005-6-2 loss: 0.598290  [   96/  146]
train() client id: f_00005-6-3 loss: 0.825501  [  128/  146]
train() client id: f_00005-7-0 loss: 0.651005  [   32/  146]
train() client id: f_00005-7-1 loss: 0.675580  [   64/  146]
train() client id: f_00005-7-2 loss: 0.790860  [   96/  146]
train() client id: f_00005-7-3 loss: 0.853925  [  128/  146]
train() client id: f_00005-8-0 loss: 0.694241  [   32/  146]
train() client id: f_00005-8-1 loss: 0.599664  [   64/  146]
train() client id: f_00005-8-2 loss: 0.758180  [   96/  146]
train() client id: f_00005-8-3 loss: 0.771955  [  128/  146]
train() client id: f_00005-9-0 loss: 0.743957  [   32/  146]
train() client id: f_00005-9-1 loss: 0.683686  [   64/  146]
train() client id: f_00005-9-2 loss: 0.786788  [   96/  146]
train() client id: f_00005-9-3 loss: 0.751071  [  128/  146]
train() client id: f_00005-10-0 loss: 0.837660  [   32/  146]
train() client id: f_00005-10-1 loss: 0.671638  [   64/  146]
train() client id: f_00005-10-2 loss: 0.632062  [   96/  146]
train() client id: f_00005-10-3 loss: 0.872034  [  128/  146]
train() client id: f_00005-11-0 loss: 0.742743  [   32/  146]
train() client id: f_00005-11-1 loss: 0.579083  [   64/  146]
train() client id: f_00005-11-2 loss: 0.876088  [   96/  146]
train() client id: f_00005-11-3 loss: 0.768151  [  128/  146]
train() client id: f_00006-0-0 loss: 0.509324  [   32/   54]
train() client id: f_00006-1-0 loss: 0.574648  [   32/   54]
train() client id: f_00006-2-0 loss: 0.458020  [   32/   54]
train() client id: f_00006-3-0 loss: 0.550983  [   32/   54]
train() client id: f_00006-4-0 loss: 0.511203  [   32/   54]
train() client id: f_00006-5-0 loss: 0.516170  [   32/   54]
train() client id: f_00006-6-0 loss: 0.524309  [   32/   54]
train() client id: f_00006-7-0 loss: 0.575311  [   32/   54]
train() client id: f_00006-8-0 loss: 0.509652  [   32/   54]
train() client id: f_00006-9-0 loss: 0.472781  [   32/   54]
train() client id: f_00006-10-0 loss: 0.464105  [   32/   54]
train() client id: f_00006-11-0 loss: 0.506396  [   32/   54]
train() client id: f_00007-0-0 loss: 0.573605  [   32/  179]
train() client id: f_00007-0-1 loss: 0.564873  [   64/  179]
train() client id: f_00007-0-2 loss: 0.665444  [   96/  179]
train() client id: f_00007-0-3 loss: 0.527539  [  128/  179]
train() client id: f_00007-0-4 loss: 0.659111  [  160/  179]
train() client id: f_00007-1-0 loss: 0.712226  [   32/  179]
train() client id: f_00007-1-1 loss: 0.491672  [   64/  179]
train() client id: f_00007-1-2 loss: 0.537589  [   96/  179]
train() client id: f_00007-1-3 loss: 0.441875  [  128/  179]
train() client id: f_00007-1-4 loss: 0.572847  [  160/  179]
train() client id: f_00007-2-0 loss: 0.608269  [   32/  179]
train() client id: f_00007-2-1 loss: 0.598535  [   64/  179]
train() client id: f_00007-2-2 loss: 0.606737  [   96/  179]
train() client id: f_00007-2-3 loss: 0.484721  [  128/  179]
train() client id: f_00007-2-4 loss: 0.529830  [  160/  179]
train() client id: f_00007-3-0 loss: 0.519495  [   32/  179]
train() client id: f_00007-3-1 loss: 0.528080  [   64/  179]
train() client id: f_00007-3-2 loss: 0.523389  [   96/  179]
train() client id: f_00007-3-3 loss: 0.434656  [  128/  179]
train() client id: f_00007-3-4 loss: 0.781488  [  160/  179]
train() client id: f_00007-4-0 loss: 0.517683  [   32/  179]
train() client id: f_00007-4-1 loss: 0.423860  [   64/  179]
train() client id: f_00007-4-2 loss: 0.636055  [   96/  179]
train() client id: f_00007-4-3 loss: 0.452322  [  128/  179]
train() client id: f_00007-4-4 loss: 0.675747  [  160/  179]
train() client id: f_00007-5-0 loss: 0.475752  [   32/  179]
train() client id: f_00007-5-1 loss: 0.551542  [   64/  179]
train() client id: f_00007-5-2 loss: 0.547267  [   96/  179]
train() client id: f_00007-5-3 loss: 0.358203  [  128/  179]
train() client id: f_00007-5-4 loss: 0.743094  [  160/  179]
train() client id: f_00007-6-0 loss: 0.449739  [   32/  179]
train() client id: f_00007-6-1 loss: 0.564200  [   64/  179]
train() client id: f_00007-6-2 loss: 0.563813  [   96/  179]
train() client id: f_00007-6-3 loss: 0.472238  [  128/  179]
train() client id: f_00007-6-4 loss: 0.590044  [  160/  179]
train() client id: f_00007-7-0 loss: 0.648849  [   32/  179]
train() client id: f_00007-7-1 loss: 0.524305  [   64/  179]
train() client id: f_00007-7-2 loss: 0.389653  [   96/  179]
train() client id: f_00007-7-3 loss: 0.472035  [  128/  179]
train() client id: f_00007-7-4 loss: 0.606077  [  160/  179]
train() client id: f_00007-8-0 loss: 0.376221  [   32/  179]
train() client id: f_00007-8-1 loss: 0.630351  [   64/  179]
train() client id: f_00007-8-2 loss: 0.548094  [   96/  179]
train() client id: f_00007-8-3 loss: 0.500692  [  128/  179]
train() client id: f_00007-8-4 loss: 0.468217  [  160/  179]
train() client id: f_00007-9-0 loss: 0.370663  [   32/  179]
train() client id: f_00007-9-1 loss: 0.467107  [   64/  179]
train() client id: f_00007-9-2 loss: 0.501825  [   96/  179]
train() client id: f_00007-9-3 loss: 0.618745  [  128/  179]
train() client id: f_00007-9-4 loss: 0.611528  [  160/  179]
train() client id: f_00007-10-0 loss: 0.441454  [   32/  179]
train() client id: f_00007-10-1 loss: 0.417588  [   64/  179]
train() client id: f_00007-10-2 loss: 0.469316  [   96/  179]
train() client id: f_00007-10-3 loss: 0.558537  [  128/  179]
train() client id: f_00007-10-4 loss: 0.483504  [  160/  179]
train() client id: f_00007-11-0 loss: 0.761498  [   32/  179]
train() client id: f_00007-11-1 loss: 0.447281  [   64/  179]
train() client id: f_00007-11-2 loss: 0.458910  [   96/  179]
train() client id: f_00007-11-3 loss: 0.428995  [  128/  179]
train() client id: f_00007-11-4 loss: 0.350188  [  160/  179]
train() client id: f_00008-0-0 loss: 0.633970  [   32/  130]
train() client id: f_00008-0-1 loss: 0.696343  [   64/  130]
train() client id: f_00008-0-2 loss: 0.822835  [   96/  130]
train() client id: f_00008-0-3 loss: 0.812099  [  128/  130]
train() client id: f_00008-1-0 loss: 0.674424  [   32/  130]
train() client id: f_00008-1-1 loss: 0.795538  [   64/  130]
train() client id: f_00008-1-2 loss: 0.838301  [   96/  130]
train() client id: f_00008-1-3 loss: 0.632819  [  128/  130]
train() client id: f_00008-2-0 loss: 0.777704  [   32/  130]
train() client id: f_00008-2-1 loss: 0.606208  [   64/  130]
train() client id: f_00008-2-2 loss: 0.755241  [   96/  130]
train() client id: f_00008-2-3 loss: 0.780457  [  128/  130]
train() client id: f_00008-3-0 loss: 0.681768  [   32/  130]
train() client id: f_00008-3-1 loss: 0.837303  [   64/  130]
train() client id: f_00008-3-2 loss: 0.859757  [   96/  130]
train() client id: f_00008-3-3 loss: 0.587979  [  128/  130]
train() client id: f_00008-4-0 loss: 0.742068  [   32/  130]
train() client id: f_00008-4-1 loss: 0.706073  [   64/  130]
train() client id: f_00008-4-2 loss: 0.819254  [   96/  130]
train() client id: f_00008-4-3 loss: 0.653534  [  128/  130]
train() client id: f_00008-5-0 loss: 0.812739  [   32/  130]
train() client id: f_00008-5-1 loss: 0.719019  [   64/  130]
train() client id: f_00008-5-2 loss: 0.667476  [   96/  130]
train() client id: f_00008-5-3 loss: 0.699805  [  128/  130]
train() client id: f_00008-6-0 loss: 0.663932  [   32/  130]
train() client id: f_00008-6-1 loss: 0.791315  [   64/  130]
train() client id: f_00008-6-2 loss: 0.716957  [   96/  130]
train() client id: f_00008-6-3 loss: 0.758381  [  128/  130]
train() client id: f_00008-7-0 loss: 0.662427  [   32/  130]
train() client id: f_00008-7-1 loss: 0.732825  [   64/  130]
train() client id: f_00008-7-2 loss: 0.775326  [   96/  130]
train() client id: f_00008-7-3 loss: 0.771134  [  128/  130]
train() client id: f_00008-8-0 loss: 0.664596  [   32/  130]
train() client id: f_00008-8-1 loss: 0.733324  [   64/  130]
train() client id: f_00008-8-2 loss: 0.724227  [   96/  130]
train() client id: f_00008-8-3 loss: 0.819533  [  128/  130]
train() client id: f_00008-9-0 loss: 0.717252  [   32/  130]
train() client id: f_00008-9-1 loss: 0.725829  [   64/  130]
train() client id: f_00008-9-2 loss: 0.692870  [   96/  130]
train() client id: f_00008-9-3 loss: 0.767277  [  128/  130]
train() client id: f_00008-10-0 loss: 0.713390  [   32/  130]
train() client id: f_00008-10-1 loss: 0.744812  [   64/  130]
train() client id: f_00008-10-2 loss: 0.760615  [   96/  130]
train() client id: f_00008-10-3 loss: 0.734679  [  128/  130]
train() client id: f_00008-11-0 loss: 0.745393  [   32/  130]
train() client id: f_00008-11-1 loss: 0.711717  [   64/  130]
train() client id: f_00008-11-2 loss: 0.745446  [   96/  130]
train() client id: f_00008-11-3 loss: 0.695300  [  128/  130]
train() client id: f_00009-0-0 loss: 1.082557  [   32/  118]
train() client id: f_00009-0-1 loss: 1.198251  [   64/  118]
train() client id: f_00009-0-2 loss: 1.372942  [   96/  118]
train() client id: f_00009-1-0 loss: 1.012785  [   32/  118]
train() client id: f_00009-1-1 loss: 1.182718  [   64/  118]
train() client id: f_00009-1-2 loss: 1.210783  [   96/  118]
train() client id: f_00009-2-0 loss: 1.190230  [   32/  118]
train() client id: f_00009-2-1 loss: 0.973113  [   64/  118]
train() client id: f_00009-2-2 loss: 1.163221  [   96/  118]
train() client id: f_00009-3-0 loss: 1.084240  [   32/  118]
train() client id: f_00009-3-1 loss: 0.976930  [   64/  118]
train() client id: f_00009-3-2 loss: 1.092439  [   96/  118]
train() client id: f_00009-4-0 loss: 1.022276  [   32/  118]
train() client id: f_00009-4-1 loss: 1.086925  [   64/  118]
train() client id: f_00009-4-2 loss: 1.021983  [   96/  118]
train() client id: f_00009-5-0 loss: 0.988386  [   32/  118]
train() client id: f_00009-5-1 loss: 0.947762  [   64/  118]
train() client id: f_00009-5-2 loss: 1.032269  [   96/  118]
train() client id: f_00009-6-0 loss: 0.888821  [   32/  118]
train() client id: f_00009-6-1 loss: 1.019850  [   64/  118]
train() client id: f_00009-6-2 loss: 0.970334  [   96/  118]
train() client id: f_00009-7-0 loss: 1.043264  [   32/  118]
train() client id: f_00009-7-1 loss: 0.857493  [   64/  118]
train() client id: f_00009-7-2 loss: 0.927002  [   96/  118]
train() client id: f_00009-8-0 loss: 1.009571  [   32/  118]
train() client id: f_00009-8-1 loss: 0.895873  [   64/  118]
train() client id: f_00009-8-2 loss: 0.958844  [   96/  118]
train() client id: f_00009-9-0 loss: 0.843401  [   32/  118]
train() client id: f_00009-9-1 loss: 1.158958  [   64/  118]
train() client id: f_00009-9-2 loss: 0.904148  [   96/  118]
train() client id: f_00009-10-0 loss: 0.989035  [   32/  118]
train() client id: f_00009-10-1 loss: 0.815363  [   64/  118]
train() client id: f_00009-10-2 loss: 0.927126  [   96/  118]
train() client id: f_00009-11-0 loss: 0.992283  [   32/  118]
train() client id: f_00009-11-1 loss: 0.970891  [   64/  118]
train() client id: f_00009-11-2 loss: 0.882736  [   96/  118]
At round 22 accuracy: 0.6392572944297082
At round 22 training accuracy: 0.579476861167002
At round 22 training loss: 0.8493612757667636
update_location
xs = [ -3.9056584    4.20031788 130.00902392  18.81129433   0.97929623
   3.95640986 -92.44319194 -71.32485185 114.66397685 -57.06087855]
ys = [122.5879595  105.55583871   1.32061395 -92.45517586  84.35018685
  67.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [158.24999836 145.46366473 164.02466376 137.48390574 130.82779919
 120.89008322 136.20805511 122.83285695 153.15514213 115.2039744 ]
dists_bs = [178.94215746 192.18544551 350.99150105 330.29545698 197.95479617
 208.60058224 195.87307526 202.69778195 329.6812663  207.72609273]
uav_gains = [3.16755799e-11 3.91571505e-11 2.89252519e-11 4.51054531e-11
 5.10714755e-11 6.22299652e-11 4.61709278e-11 5.97976208e-11
 3.44011166e-11 7.01971242e-11]
bs_gains = [5.44124914e-11 4.45529740e-11 8.25018342e-12 9.78062943e-12
 4.10118354e-11 3.54169077e-11 4.22439780e-11 3.83810539e-11
 9.83173423e-12 3.58359679e-11]
Round 23
-------------------------------
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.95652636 16.54261616  7.83621819  2.81059366 19.08131306  9.18961195
  3.4902559  11.21434588  8.261358    7.45728421]
obj_prev = 93.84012336038582
eta_min = 2.1552750513700054e-12	eta_max = 0.9232017387246043
af = 19.826056636829264	bf = 1.599357197144509	zeta = 21.808662300512193	eta = 0.909090909090909
af = 19.826056636829264	bf = 1.599357197144509	zeta = 38.37880442012573	eta = 0.5165886987983539
af = 19.826056636829264	bf = 1.599357197144509	zeta = 30.397782640654498	eta = 0.6522204882902731
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.962231155820284	eta = 0.6845486637463356
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.889857955433257	eta = 0.686263555446129
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.889660323483184	eta = 0.6862682501224668
eta = 0.6862682501224668
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [0.03106678 0.06533886 0.03057366 0.01060215 0.07544789 0.03599801
 0.01331433 0.04413455 0.03205303 0.02909429]
ene_total = [2.52460799 4.70588087 2.50342372 1.16220666 5.36855605 2.83112977
 1.33541705 3.30578544 2.76961187 2.3830409 ]
ti_comp = [0.36635294 0.37203036 0.36469503 0.37222065 0.37072619 0.36830251
 0.37257852 0.37632559 0.33835818 0.3685025 ]
ti_coms = [0.07942667 0.07374925 0.08108457 0.07355896 0.07505341 0.0774771
 0.07320109 0.06945402 0.10742142 0.07727711]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.39626984e-05 1.25961536e-04 1.34295805e-05 5.37602806e-07
 1.95305657e-04 2.14934581e-05 1.06268058e-06 3.79392448e-05
 1.79776854e-05 1.13350184e-05]
ene_total = [0.51493605 0.48544134 0.52563117 0.47609264 0.49836942 0.50280622
 0.47381058 0.45194692 0.69637198 0.5008545 ]
optimize_network_iter = 0 obj = 5.126260825771548
eta = 0.6862682501224668
freqs = [4.24000684e+07 8.78138843e+07 4.19167432e+07 1.42417608e+07
 1.01756896e+08 4.88701724e+07 1.78678231e+07 5.86387868e+07
 4.73655258e+07 3.94763747e+07]
eta_min = 0.686268250122481	eta_max = 0.6862682501224266
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 0.026043258509324534	eta = 0.9090909090909091
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 17.617751045101738	eta = 0.0013438542463973541
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7582687679572628	eta = 0.01346534158224146
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7245653152193903	eta = 0.013728496882659091
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7245608478782077	eta = 0.013728532445265985
eta = 0.013728532445265985
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.59038678e-04 1.43473385e-03 1.52966329e-04 6.12343236e-06
 2.22458099e-03 2.44815942e-04 1.21042015e-05 4.32137624e-04
 2.04770398e-04 1.29108736e-04]
ene_total = [0.16678716 0.18136157 0.17007523 0.15155948 0.20030673 0.16453951
 0.15094588 0.151879   0.22536052 0.16174577]
ti_comp = [0.36635294 0.37203036 0.36469503 0.37222065 0.37072619 0.36830251
 0.37257852 0.37632559 0.33835818 0.3685025 ]
ti_coms = [0.07942667 0.07374925 0.08108457 0.07355896 0.07505341 0.0774771
 0.07320109 0.06945402 0.10742142 0.07727711]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.39626984e-05 1.25961536e-04 1.34295805e-05 5.37602806e-07
 1.95305657e-04 2.14934581e-05 1.06268058e-06 3.79392448e-05
 1.79776854e-05 1.13350184e-05]
ene_total = [0.51493605 0.48544134 0.52563117 0.47609264 0.49836942 0.50280622
 0.47381058 0.45194692 0.69637198 0.5008545 ]
optimize_network_iter = 1 obj = 5.126260825771778
eta = 0.686268250122481
freqs = [4.24000684e+07 8.78138843e+07 4.19167432e+07 1.42417608e+07
 1.01756896e+08 4.88701724e+07 1.78678231e+07 5.86387868e+07
 4.73655258e+07 3.94763747e+07]
Done!
At round 23 eta: 0.686268250122481
At round 23 local rounds: 12.328087095105714
At round 23 global rounds: 64.71786290003097
At round 23 a_n: 19.96150252899076
gradient difference: 0.4007430076599121
train() client id: f_00000-0-0 loss: 1.226663  [   32/  126]
train() client id: f_00000-0-1 loss: 1.282869  [   64/  126]
train() client id: f_00000-0-2 loss: 1.367311  [   96/  126]
train() client id: f_00000-1-0 loss: 1.162379  [   32/  126]
train() client id: f_00000-1-1 loss: 1.308803  [   64/  126]
train() client id: f_00000-1-2 loss: 1.080553  [   96/  126]
train() client id: f_00000-2-0 loss: 1.116021  [   32/  126]
train() client id: f_00000-2-1 loss: 0.996456  [   64/  126]
train() client id: f_00000-2-2 loss: 0.997720  [   96/  126]
train() client id: f_00000-3-0 loss: 1.020166  [   32/  126]
train() client id: f_00000-3-1 loss: 0.974669  [   64/  126]
train() client id: f_00000-3-2 loss: 0.959603  [   96/  126]
train() client id: f_00000-4-0 loss: 0.960601  [   32/  126]
train() client id: f_00000-4-1 loss: 0.997980  [   64/  126]
train() client id: f_00000-4-2 loss: 1.035383  [   96/  126]
train() client id: f_00000-5-0 loss: 0.944338  [   32/  126]
train() client id: f_00000-5-1 loss: 1.000543  [   64/  126]
train() client id: f_00000-5-2 loss: 0.844952  [   96/  126]
train() client id: f_00000-6-0 loss: 0.890626  [   32/  126]
train() client id: f_00000-6-1 loss: 0.863938  [   64/  126]
train() client id: f_00000-6-2 loss: 0.919198  [   96/  126]
train() client id: f_00000-7-0 loss: 0.922329  [   32/  126]
train() client id: f_00000-7-1 loss: 0.852672  [   64/  126]
train() client id: f_00000-7-2 loss: 0.864496  [   96/  126]
train() client id: f_00000-8-0 loss: 0.869622  [   32/  126]
train() client id: f_00000-8-1 loss: 0.806631  [   64/  126]
train() client id: f_00000-8-2 loss: 0.946145  [   96/  126]
train() client id: f_00000-9-0 loss: 0.918185  [   32/  126]
train() client id: f_00000-9-1 loss: 0.892228  [   64/  126]
train() client id: f_00000-9-2 loss: 0.783574  [   96/  126]
train() client id: f_00000-10-0 loss: 0.769711  [   32/  126]
train() client id: f_00000-10-1 loss: 0.880967  [   64/  126]
train() client id: f_00000-10-2 loss: 0.882021  [   96/  126]
train() client id: f_00000-11-0 loss: 0.883253  [   32/  126]
train() client id: f_00000-11-1 loss: 0.835873  [   64/  126]
train() client id: f_00000-11-2 loss: 0.887301  [   96/  126]
train() client id: f_00001-0-0 loss: 0.614165  [   32/  265]
train() client id: f_00001-0-1 loss: 0.498702  [   64/  265]
train() client id: f_00001-0-2 loss: 0.543769  [   96/  265]
train() client id: f_00001-0-3 loss: 0.463943  [  128/  265]
train() client id: f_00001-0-4 loss: 0.443824  [  160/  265]
train() client id: f_00001-0-5 loss: 0.543887  [  192/  265]
train() client id: f_00001-0-6 loss: 0.521422  [  224/  265]
train() client id: f_00001-0-7 loss: 0.516541  [  256/  265]
train() client id: f_00001-1-0 loss: 0.532139  [   32/  265]
train() client id: f_00001-1-1 loss: 0.435934  [   64/  265]
train() client id: f_00001-1-2 loss: 0.574777  [   96/  265]
train() client id: f_00001-1-3 loss: 0.482735  [  128/  265]
train() client id: f_00001-1-4 loss: 0.529052  [  160/  265]
train() client id: f_00001-1-5 loss: 0.610829  [  192/  265]
train() client id: f_00001-1-6 loss: 0.406786  [  224/  265]
train() client id: f_00001-1-7 loss: 0.509246  [  256/  265]
train() client id: f_00001-2-0 loss: 0.444032  [   32/  265]
train() client id: f_00001-2-1 loss: 0.583703  [   64/  265]
train() client id: f_00001-2-2 loss: 0.575074  [   96/  265]
train() client id: f_00001-2-3 loss: 0.498052  [  128/  265]
train() client id: f_00001-2-4 loss: 0.465703  [  160/  265]
train() client id: f_00001-2-5 loss: 0.427035  [  192/  265]
train() client id: f_00001-2-6 loss: 0.466102  [  224/  265]
train() client id: f_00001-2-7 loss: 0.578154  [  256/  265]
train() client id: f_00001-3-0 loss: 0.457137  [   32/  265]
train() client id: f_00001-3-1 loss: 0.466303  [   64/  265]
train() client id: f_00001-3-2 loss: 0.597535  [   96/  265]
train() client id: f_00001-3-3 loss: 0.403912  [  128/  265]
train() client id: f_00001-3-4 loss: 0.515600  [  160/  265]
train() client id: f_00001-3-5 loss: 0.679411  [  192/  265]
train() client id: f_00001-3-6 loss: 0.432611  [  224/  265]
train() client id: f_00001-3-7 loss: 0.447824  [  256/  265]
train() client id: f_00001-4-0 loss: 0.440019  [   32/  265]
train() client id: f_00001-4-1 loss: 0.508183  [   64/  265]
train() client id: f_00001-4-2 loss: 0.476407  [   96/  265]
train() client id: f_00001-4-3 loss: 0.554885  [  128/  265]
train() client id: f_00001-4-4 loss: 0.439798  [  160/  265]
train() client id: f_00001-4-5 loss: 0.471432  [  192/  265]
train() client id: f_00001-4-6 loss: 0.561138  [  224/  265]
train() client id: f_00001-4-7 loss: 0.439483  [  256/  265]
train() client id: f_00001-5-0 loss: 0.557761  [   32/  265]
train() client id: f_00001-5-1 loss: 0.450610  [   64/  265]
train() client id: f_00001-5-2 loss: 0.466163  [   96/  265]
train() client id: f_00001-5-3 loss: 0.485219  [  128/  265]
train() client id: f_00001-5-4 loss: 0.537650  [  160/  265]
train() client id: f_00001-5-5 loss: 0.402110  [  192/  265]
train() client id: f_00001-5-6 loss: 0.531187  [  224/  265]
train() client id: f_00001-5-7 loss: 0.522098  [  256/  265]
train() client id: f_00001-6-0 loss: 0.393275  [   32/  265]
train() client id: f_00001-6-1 loss: 0.554532  [   64/  265]
train() client id: f_00001-6-2 loss: 0.462651  [   96/  265]
train() client id: f_00001-6-3 loss: 0.413812  [  128/  265]
train() client id: f_00001-6-4 loss: 0.403677  [  160/  265]
train() client id: f_00001-6-5 loss: 0.566636  [  192/  265]
train() client id: f_00001-6-6 loss: 0.509163  [  224/  265]
train() client id: f_00001-6-7 loss: 0.578744  [  256/  265]
train() client id: f_00001-7-0 loss: 0.591610  [   32/  265]
train() client id: f_00001-7-1 loss: 0.546547  [   64/  265]
train() client id: f_00001-7-2 loss: 0.465123  [   96/  265]
train() client id: f_00001-7-3 loss: 0.432942  [  128/  265]
train() client id: f_00001-7-4 loss: 0.446177  [  160/  265]
train() client id: f_00001-7-5 loss: 0.514335  [  192/  265]
train() client id: f_00001-7-6 loss: 0.450056  [  224/  265]
train() client id: f_00001-7-7 loss: 0.489087  [  256/  265]
train() client id: f_00001-8-0 loss: 0.476579  [   32/  265]
train() client id: f_00001-8-1 loss: 0.491881  [   64/  265]
train() client id: f_00001-8-2 loss: 0.400561  [   96/  265]
train() client id: f_00001-8-3 loss: 0.577515  [  128/  265]
train() client id: f_00001-8-4 loss: 0.535930  [  160/  265]
train() client id: f_00001-8-5 loss: 0.384283  [  192/  265]
train() client id: f_00001-8-6 loss: 0.551600  [  224/  265]
train() client id: f_00001-8-7 loss: 0.515957  [  256/  265]
train() client id: f_00001-9-0 loss: 0.434329  [   32/  265]
train() client id: f_00001-9-1 loss: 0.503836  [   64/  265]
train() client id: f_00001-9-2 loss: 0.478533  [   96/  265]
train() client id: f_00001-9-3 loss: 0.418401  [  128/  265]
train() client id: f_00001-9-4 loss: 0.543812  [  160/  265]
train() client id: f_00001-9-5 loss: 0.622287  [  192/  265]
train() client id: f_00001-9-6 loss: 0.401570  [  224/  265]
train() client id: f_00001-9-7 loss: 0.527601  [  256/  265]
train() client id: f_00001-10-0 loss: 0.493159  [   32/  265]
train() client id: f_00001-10-1 loss: 0.473389  [   64/  265]
train() client id: f_00001-10-2 loss: 0.389696  [   96/  265]
train() client id: f_00001-10-3 loss: 0.524423  [  128/  265]
train() client id: f_00001-10-4 loss: 0.556238  [  160/  265]
train() client id: f_00001-10-5 loss: 0.497160  [  192/  265]
train() client id: f_00001-10-6 loss: 0.459953  [  224/  265]
train() client id: f_00001-10-7 loss: 0.540579  [  256/  265]
train() client id: f_00001-11-0 loss: 0.599369  [   32/  265]
train() client id: f_00001-11-1 loss: 0.501022  [   64/  265]
train() client id: f_00001-11-2 loss: 0.501307  [   96/  265]
train() client id: f_00001-11-3 loss: 0.442133  [  128/  265]
train() client id: f_00001-11-4 loss: 0.396024  [  160/  265]
train() client id: f_00001-11-5 loss: 0.605818  [  192/  265]
train() client id: f_00001-11-6 loss: 0.416588  [  224/  265]
train() client id: f_00001-11-7 loss: 0.469905  [  256/  265]
train() client id: f_00002-0-0 loss: 1.349626  [   32/  124]
train() client id: f_00002-0-1 loss: 1.210747  [   64/  124]
train() client id: f_00002-0-2 loss: 1.120642  [   96/  124]
train() client id: f_00002-1-0 loss: 1.150590  [   32/  124]
train() client id: f_00002-1-1 loss: 1.209715  [   64/  124]
train() client id: f_00002-1-2 loss: 1.187721  [   96/  124]
train() client id: f_00002-2-0 loss: 1.061202  [   32/  124]
train() client id: f_00002-2-1 loss: 1.036433  [   64/  124]
train() client id: f_00002-2-2 loss: 1.217713  [   96/  124]
train() client id: f_00002-3-0 loss: 1.137241  [   32/  124]
train() client id: f_00002-3-1 loss: 1.095587  [   64/  124]
train() client id: f_00002-3-2 loss: 1.035690  [   96/  124]
train() client id: f_00002-4-0 loss: 0.912888  [   32/  124]
train() client id: f_00002-4-1 loss: 1.268029  [   64/  124]
train() client id: f_00002-4-2 loss: 1.081838  [   96/  124]
train() client id: f_00002-5-0 loss: 1.002399  [   32/  124]
train() client id: f_00002-5-1 loss: 1.069091  [   64/  124]
train() client id: f_00002-5-2 loss: 1.062205  [   96/  124]
train() client id: f_00002-6-0 loss: 1.108303  [   32/  124]
train() client id: f_00002-6-1 loss: 0.956847  [   64/  124]
train() client id: f_00002-6-2 loss: 1.015166  [   96/  124]
train() client id: f_00002-7-0 loss: 1.036886  [   32/  124]
train() client id: f_00002-7-1 loss: 0.924643  [   64/  124]
train() client id: f_00002-7-2 loss: 0.907078  [   96/  124]
train() client id: f_00002-8-0 loss: 0.984438  [   32/  124]
train() client id: f_00002-8-1 loss: 1.112462  [   64/  124]
train() client id: f_00002-8-2 loss: 0.961541  [   96/  124]
train() client id: f_00002-9-0 loss: 1.056739  [   32/  124]
train() client id: f_00002-9-1 loss: 1.004635  [   64/  124]
train() client id: f_00002-9-2 loss: 0.959793  [   96/  124]
train() client id: f_00002-10-0 loss: 0.912534  [   32/  124]
train() client id: f_00002-10-1 loss: 1.023288  [   64/  124]
train() client id: f_00002-10-2 loss: 0.986097  [   96/  124]
train() client id: f_00002-11-0 loss: 0.861539  [   32/  124]
train() client id: f_00002-11-1 loss: 0.989738  [   64/  124]
train() client id: f_00002-11-2 loss: 0.930082  [   96/  124]
train() client id: f_00003-0-0 loss: 0.964273  [   32/   43]
train() client id: f_00003-1-0 loss: 0.839567  [   32/   43]
train() client id: f_00003-2-0 loss: 0.906776  [   32/   43]
train() client id: f_00003-3-0 loss: 0.857534  [   32/   43]
train() client id: f_00003-4-0 loss: 0.933465  [   32/   43]
train() client id: f_00003-5-0 loss: 0.893405  [   32/   43]
train() client id: f_00003-6-0 loss: 1.029836  [   32/   43]
train() client id: f_00003-7-0 loss: 0.918148  [   32/   43]
train() client id: f_00003-8-0 loss: 1.048739  [   32/   43]
train() client id: f_00003-9-0 loss: 0.964147  [   32/   43]
train() client id: f_00003-10-0 loss: 0.750005  [   32/   43]
train() client id: f_00003-11-0 loss: 1.061146  [   32/   43]
train() client id: f_00004-0-0 loss: 0.704446  [   32/  306]
train() client id: f_00004-0-1 loss: 0.759027  [   64/  306]
train() client id: f_00004-0-2 loss: 0.815311  [   96/  306]
train() client id: f_00004-0-3 loss: 0.673473  [  128/  306]
train() client id: f_00004-0-4 loss: 0.780332  [  160/  306]
train() client id: f_00004-0-5 loss: 0.693146  [  192/  306]
train() client id: f_00004-0-6 loss: 0.640317  [  224/  306]
train() client id: f_00004-0-7 loss: 0.807677  [  256/  306]
train() client id: f_00004-0-8 loss: 0.799636  [  288/  306]
train() client id: f_00004-1-0 loss: 0.799584  [   32/  306]
train() client id: f_00004-1-1 loss: 0.801103  [   64/  306]
train() client id: f_00004-1-2 loss: 0.724652  [   96/  306]
train() client id: f_00004-1-3 loss: 0.704970  [  128/  306]
train() client id: f_00004-1-4 loss: 0.783922  [  160/  306]
train() client id: f_00004-1-5 loss: 0.751296  [  192/  306]
train() client id: f_00004-1-6 loss: 0.693405  [  224/  306]
train() client id: f_00004-1-7 loss: 0.783153  [  256/  306]
train() client id: f_00004-1-8 loss: 0.695666  [  288/  306]
train() client id: f_00004-2-0 loss: 0.709134  [   32/  306]
train() client id: f_00004-2-1 loss: 0.644051  [   64/  306]
train() client id: f_00004-2-2 loss: 0.738274  [   96/  306]
train() client id: f_00004-2-3 loss: 0.873152  [  128/  306]
train() client id: f_00004-2-4 loss: 0.616825  [  160/  306]
train() client id: f_00004-2-5 loss: 0.914480  [  192/  306]
train() client id: f_00004-2-6 loss: 0.722571  [  224/  306]
train() client id: f_00004-2-7 loss: 0.740281  [  256/  306]
train() client id: f_00004-2-8 loss: 0.778046  [  288/  306]
train() client id: f_00004-3-0 loss: 0.762269  [   32/  306]
train() client id: f_00004-3-1 loss: 0.859221  [   64/  306]
train() client id: f_00004-3-2 loss: 0.761418  [   96/  306]
train() client id: f_00004-3-3 loss: 0.745670  [  128/  306]
train() client id: f_00004-3-4 loss: 0.712881  [  160/  306]
train() client id: f_00004-3-5 loss: 0.656142  [  192/  306]
train() client id: f_00004-3-6 loss: 0.641158  [  224/  306]
train() client id: f_00004-3-7 loss: 0.777721  [  256/  306]
train() client id: f_00004-3-8 loss: 0.794309  [  288/  306]
train() client id: f_00004-4-0 loss: 0.672736  [   32/  306]
train() client id: f_00004-4-1 loss: 0.831888  [   64/  306]
train() client id: f_00004-4-2 loss: 0.687841  [   96/  306]
train() client id: f_00004-4-3 loss: 0.677163  [  128/  306]
train() client id: f_00004-4-4 loss: 0.847035  [  160/  306]
train() client id: f_00004-4-5 loss: 0.608708  [  192/  306]
train() client id: f_00004-4-6 loss: 0.805294  [  224/  306]
train() client id: f_00004-4-7 loss: 0.789514  [  256/  306]
train() client id: f_00004-4-8 loss: 0.694112  [  288/  306]
train() client id: f_00004-5-0 loss: 0.715859  [   32/  306]
train() client id: f_00004-5-1 loss: 0.645530  [   64/  306]
train() client id: f_00004-5-2 loss: 0.785999  [   96/  306]
train() client id: f_00004-5-3 loss: 0.870390  [  128/  306]
train() client id: f_00004-5-4 loss: 0.638839  [  160/  306]
train() client id: f_00004-5-5 loss: 0.839474  [  192/  306]
train() client id: f_00004-5-6 loss: 0.706390  [  224/  306]
train() client id: f_00004-5-7 loss: 0.862661  [  256/  306]
train() client id: f_00004-5-8 loss: 0.729226  [  288/  306]
train() client id: f_00004-6-0 loss: 0.781199  [   32/  306]
train() client id: f_00004-6-1 loss: 0.713133  [   64/  306]
train() client id: f_00004-6-2 loss: 0.749936  [   96/  306]
train() client id: f_00004-6-3 loss: 0.699365  [  128/  306]
train() client id: f_00004-6-4 loss: 0.888223  [  160/  306]
train() client id: f_00004-6-5 loss: 0.799004  [  192/  306]
train() client id: f_00004-6-6 loss: 0.697592  [  224/  306]
train() client id: f_00004-6-7 loss: 0.585357  [  256/  306]
train() client id: f_00004-6-8 loss: 0.829561  [  288/  306]
train() client id: f_00004-7-0 loss: 0.686829  [   32/  306]
train() client id: f_00004-7-1 loss: 0.612548  [   64/  306]
train() client id: f_00004-7-2 loss: 0.742746  [   96/  306]
train() client id: f_00004-7-3 loss: 0.620546  [  128/  306]
train() client id: f_00004-7-4 loss: 0.776595  [  160/  306]
train() client id: f_00004-7-5 loss: 0.860520  [  192/  306]
train() client id: f_00004-7-6 loss: 0.767304  [  224/  306]
train() client id: f_00004-7-7 loss: 0.814971  [  256/  306]
train() client id: f_00004-7-8 loss: 0.869521  [  288/  306]
train() client id: f_00004-8-0 loss: 0.690913  [   32/  306]
train() client id: f_00004-8-1 loss: 0.746272  [   64/  306]
train() client id: f_00004-8-2 loss: 0.790243  [   96/  306]
train() client id: f_00004-8-3 loss: 0.770860  [  128/  306]
train() client id: f_00004-8-4 loss: 0.771620  [  160/  306]
train() client id: f_00004-8-5 loss: 0.821820  [  192/  306]
train() client id: f_00004-8-6 loss: 0.749773  [  224/  306]
train() client id: f_00004-8-7 loss: 0.793660  [  256/  306]
train() client id: f_00004-8-8 loss: 0.649040  [  288/  306]
train() client id: f_00004-9-0 loss: 0.747859  [   32/  306]
train() client id: f_00004-9-1 loss: 0.719328  [   64/  306]
train() client id: f_00004-9-2 loss: 0.772451  [   96/  306]
train() client id: f_00004-9-3 loss: 0.741997  [  128/  306]
train() client id: f_00004-9-4 loss: 0.820632  [  160/  306]
train() client id: f_00004-9-5 loss: 0.751770  [  192/  306]
train() client id: f_00004-9-6 loss: 0.686597  [  224/  306]
train() client id: f_00004-9-7 loss: 0.646124  [  256/  306]
train() client id: f_00004-9-8 loss: 0.857331  [  288/  306]
train() client id: f_00004-10-0 loss: 0.817841  [   32/  306]
train() client id: f_00004-10-1 loss: 0.650611  [   64/  306]
train() client id: f_00004-10-2 loss: 0.877327  [   96/  306]
train() client id: f_00004-10-3 loss: 0.751517  [  128/  306]
train() client id: f_00004-10-4 loss: 0.771443  [  160/  306]
train() client id: f_00004-10-5 loss: 0.816300  [  192/  306]
train() client id: f_00004-10-6 loss: 0.653171  [  224/  306]
train() client id: f_00004-10-7 loss: 0.679181  [  256/  306]
train() client id: f_00004-10-8 loss: 0.785092  [  288/  306]
train() client id: f_00004-11-0 loss: 0.728165  [   32/  306]
train() client id: f_00004-11-1 loss: 0.687347  [   64/  306]
train() client id: f_00004-11-2 loss: 0.713810  [   96/  306]
train() client id: f_00004-11-3 loss: 0.784222  [  128/  306]
train() client id: f_00004-11-4 loss: 0.812642  [  160/  306]
train() client id: f_00004-11-5 loss: 0.799769  [  192/  306]
train() client id: f_00004-11-6 loss: 0.732397  [  224/  306]
train() client id: f_00004-11-7 loss: 0.789725  [  256/  306]
train() client id: f_00004-11-8 loss: 0.705313  [  288/  306]
train() client id: f_00005-0-0 loss: 0.504140  [   32/  146]
train() client id: f_00005-0-1 loss: 0.791207  [   64/  146]
train() client id: f_00005-0-2 loss: 0.723451  [   96/  146]
train() client id: f_00005-0-3 loss: 0.526521  [  128/  146]
train() client id: f_00005-1-0 loss: 0.613236  [   32/  146]
train() client id: f_00005-1-1 loss: 0.557244  [   64/  146]
train() client id: f_00005-1-2 loss: 0.812852  [   96/  146]
train() client id: f_00005-1-3 loss: 0.595368  [  128/  146]
train() client id: f_00005-2-0 loss: 0.450220  [   32/  146]
train() client id: f_00005-2-1 loss: 0.600933  [   64/  146]
train() client id: f_00005-2-2 loss: 0.460327  [   96/  146]
train() client id: f_00005-2-3 loss: 0.846161  [  128/  146]
train() client id: f_00005-3-0 loss: 0.612806  [   32/  146]
train() client id: f_00005-3-1 loss: 0.623188  [   64/  146]
train() client id: f_00005-3-2 loss: 0.608959  [   96/  146]
train() client id: f_00005-3-3 loss: 0.700486  [  128/  146]
train() client id: f_00005-4-0 loss: 0.474166  [   32/  146]
train() client id: f_00005-4-1 loss: 0.829928  [   64/  146]
train() client id: f_00005-4-2 loss: 0.691285  [   96/  146]
train() client id: f_00005-4-3 loss: 0.647234  [  128/  146]
train() client id: f_00005-5-0 loss: 0.593652  [   32/  146]
train() client id: f_00005-5-1 loss: 0.398509  [   64/  146]
train() client id: f_00005-5-2 loss: 0.663738  [   96/  146]
train() client id: f_00005-5-3 loss: 0.844570  [  128/  146]
train() client id: f_00005-6-0 loss: 0.649723  [   32/  146]
train() client id: f_00005-6-1 loss: 0.583779  [   64/  146]
train() client id: f_00005-6-2 loss: 0.852122  [   96/  146]
train() client id: f_00005-6-3 loss: 0.538510  [  128/  146]
train() client id: f_00005-7-0 loss: 0.466418  [   32/  146]
train() client id: f_00005-7-1 loss: 0.671334  [   64/  146]
train() client id: f_00005-7-2 loss: 0.471504  [   96/  146]
train() client id: f_00005-7-3 loss: 0.817823  [  128/  146]
train() client id: f_00005-8-0 loss: 0.585404  [   32/  146]
train() client id: f_00005-8-1 loss: 0.661272  [   64/  146]
train() client id: f_00005-8-2 loss: 0.695795  [   96/  146]
train() client id: f_00005-8-3 loss: 0.604903  [  128/  146]
train() client id: f_00005-9-0 loss: 0.687757  [   32/  146]
train() client id: f_00005-9-1 loss: 0.647958  [   64/  146]
train() client id: f_00005-9-2 loss: 0.611677  [   96/  146]
train() client id: f_00005-9-3 loss: 0.558918  [  128/  146]
train() client id: f_00005-10-0 loss: 0.648123  [   32/  146]
train() client id: f_00005-10-1 loss: 0.676649  [   64/  146]
train() client id: f_00005-10-2 loss: 0.425796  [   96/  146]
train() client id: f_00005-10-3 loss: 0.618322  [  128/  146]
train() client id: f_00005-11-0 loss: 0.463035  [   32/  146]
train() client id: f_00005-11-1 loss: 0.685950  [   64/  146]
train() client id: f_00005-11-2 loss: 0.853523  [   96/  146]
train() client id: f_00005-11-3 loss: 0.669827  [  128/  146]
train() client id: f_00006-0-0 loss: 0.531964  [   32/   54]
train() client id: f_00006-1-0 loss: 0.573144  [   32/   54]
train() client id: f_00006-2-0 loss: 0.612228  [   32/   54]
train() client id: f_00006-3-0 loss: 0.583183  [   32/   54]
train() client id: f_00006-4-0 loss: 0.556687  [   32/   54]
train() client id: f_00006-5-0 loss: 0.586277  [   32/   54]
train() client id: f_00006-6-0 loss: 0.561181  [   32/   54]
train() client id: f_00006-7-0 loss: 0.522567  [   32/   54]
train() client id: f_00006-8-0 loss: 0.565150  [   32/   54]
train() client id: f_00006-9-0 loss: 0.617590  [   32/   54]
train() client id: f_00006-10-0 loss: 0.626068  [   32/   54]
train() client id: f_00006-11-0 loss: 0.576941  [   32/   54]
train() client id: f_00007-0-0 loss: 0.743134  [   32/  179]
train() client id: f_00007-0-1 loss: 0.632305  [   64/  179]
train() client id: f_00007-0-2 loss: 0.762970  [   96/  179]
train() client id: f_00007-0-3 loss: 0.796300  [  128/  179]
train() client id: f_00007-0-4 loss: 0.753347  [  160/  179]
train() client id: f_00007-1-0 loss: 0.584059  [   32/  179]
train() client id: f_00007-1-1 loss: 0.744382  [   64/  179]
train() client id: f_00007-1-2 loss: 0.848969  [   96/  179]
train() client id: f_00007-1-3 loss: 0.824216  [  128/  179]
train() client id: f_00007-1-4 loss: 0.751233  [  160/  179]
train() client id: f_00007-2-0 loss: 0.856728  [   32/  179]
train() client id: f_00007-2-1 loss: 0.589046  [   64/  179]
train() client id: f_00007-2-2 loss: 0.753530  [   96/  179]
train() client id: f_00007-2-3 loss: 0.782507  [  128/  179]
train() client id: f_00007-2-4 loss: 0.628819  [  160/  179]
train() client id: f_00007-3-0 loss: 0.662015  [   32/  179]
train() client id: f_00007-3-1 loss: 0.711630  [   64/  179]
train() client id: f_00007-3-2 loss: 0.924658  [   96/  179]
train() client id: f_00007-3-3 loss: 0.694362  [  128/  179]
train() client id: f_00007-3-4 loss: 0.640563  [  160/  179]
train() client id: f_00007-4-0 loss: 0.614350  [   32/  179]
train() client id: f_00007-4-1 loss: 0.871751  [   64/  179]
train() client id: f_00007-4-2 loss: 0.664874  [   96/  179]
train() client id: f_00007-4-3 loss: 0.820718  [  128/  179]
train() client id: f_00007-4-4 loss: 0.590857  [  160/  179]
train() client id: f_00007-5-0 loss: 1.030679  [   32/  179]
train() client id: f_00007-5-1 loss: 0.598144  [   64/  179]
train() client id: f_00007-5-2 loss: 0.589408  [   96/  179]
train() client id: f_00007-5-3 loss: 0.592809  [  128/  179]
train() client id: f_00007-5-4 loss: 0.610647  [  160/  179]
train() client id: f_00007-6-0 loss: 0.707894  [   32/  179]
train() client id: f_00007-6-1 loss: 0.614514  [   64/  179]
train() client id: f_00007-6-2 loss: 0.893598  [   96/  179]
train() client id: f_00007-6-3 loss: 0.659501  [  128/  179]
train() client id: f_00007-6-4 loss: 0.699548  [  160/  179]
train() client id: f_00007-7-0 loss: 0.665980  [   32/  179]
train() client id: f_00007-7-1 loss: 0.779436  [   64/  179]
train() client id: f_00007-7-2 loss: 0.770997  [   96/  179]
train() client id: f_00007-7-3 loss: 0.638542  [  128/  179]
train() client id: f_00007-7-4 loss: 0.746921  [  160/  179]
train() client id: f_00007-8-0 loss: 0.571568  [   32/  179]
train() client id: f_00007-8-1 loss: 0.973334  [   64/  179]
train() client id: f_00007-8-2 loss: 0.643541  [   96/  179]
train() client id: f_00007-8-3 loss: 0.635099  [  128/  179]
train() client id: f_00007-8-4 loss: 0.660470  [  160/  179]
train() client id: f_00007-9-0 loss: 0.647786  [   32/  179]
train() client id: f_00007-9-1 loss: 0.544558  [   64/  179]
train() client id: f_00007-9-2 loss: 0.878329  [   96/  179]
train() client id: f_00007-9-3 loss: 0.719363  [  128/  179]
train() client id: f_00007-9-4 loss: 0.755187  [  160/  179]
train() client id: f_00007-10-0 loss: 0.602345  [   32/  179]
train() client id: f_00007-10-1 loss: 0.866844  [   64/  179]
train() client id: f_00007-10-2 loss: 0.772345  [   96/  179]
train() client id: f_00007-10-3 loss: 0.671470  [  128/  179]
train() client id: f_00007-10-4 loss: 0.635565  [  160/  179]
train() client id: f_00007-11-0 loss: 0.624360  [   32/  179]
train() client id: f_00007-11-1 loss: 0.571807  [   64/  179]
train() client id: f_00007-11-2 loss: 0.756968  [   96/  179]
train() client id: f_00007-11-3 loss: 0.877035  [  128/  179]
train() client id: f_00007-11-4 loss: 0.650275  [  160/  179]
train() client id: f_00008-0-0 loss: 0.713642  [   32/  130]
train() client id: f_00008-0-1 loss: 0.811187  [   64/  130]
train() client id: f_00008-0-2 loss: 0.800378  [   96/  130]
train() client id: f_00008-0-3 loss: 0.851619  [  128/  130]
train() client id: f_00008-1-0 loss: 0.864615  [   32/  130]
train() client id: f_00008-1-1 loss: 0.785762  [   64/  130]
train() client id: f_00008-1-2 loss: 0.772459  [   96/  130]
train() client id: f_00008-1-3 loss: 0.802207  [  128/  130]
train() client id: f_00008-2-0 loss: 0.773825  [   32/  130]
train() client id: f_00008-2-1 loss: 0.852634  [   64/  130]
train() client id: f_00008-2-2 loss: 0.852867  [   96/  130]
train() client id: f_00008-2-3 loss: 0.744607  [  128/  130]
train() client id: f_00008-3-0 loss: 0.819005  [   32/  130]
train() client id: f_00008-3-1 loss: 0.758947  [   64/  130]
train() client id: f_00008-3-2 loss: 0.746754  [   96/  130]
train() client id: f_00008-3-3 loss: 0.851394  [  128/  130]
train() client id: f_00008-4-0 loss: 0.798364  [   32/  130]
train() client id: f_00008-4-1 loss: 0.758632  [   64/  130]
train() client id: f_00008-4-2 loss: 0.785499  [   96/  130]
train() client id: f_00008-4-3 loss: 0.871634  [  128/  130]
train() client id: f_00008-5-0 loss: 0.848239  [   32/  130]
train() client id: f_00008-5-1 loss: 0.792293  [   64/  130]
train() client id: f_00008-5-2 loss: 0.762095  [   96/  130]
train() client id: f_00008-5-3 loss: 0.794101  [  128/  130]
train() client id: f_00008-6-0 loss: 0.764939  [   32/  130]
train() client id: f_00008-6-1 loss: 0.866641  [   64/  130]
train() client id: f_00008-6-2 loss: 0.825882  [   96/  130]
train() client id: f_00008-6-3 loss: 0.749084  [  128/  130]
train() client id: f_00008-7-0 loss: 0.841645  [   32/  130]
train() client id: f_00008-7-1 loss: 0.822069  [   64/  130]
train() client id: f_00008-7-2 loss: 0.868460  [   96/  130]
train() client id: f_00008-7-3 loss: 0.689290  [  128/  130]
train() client id: f_00008-8-0 loss: 0.777215  [   32/  130]
train() client id: f_00008-8-1 loss: 0.861431  [   64/  130]
train() client id: f_00008-8-2 loss: 0.800791  [   96/  130]
train() client id: f_00008-8-3 loss: 0.770041  [  128/  130]
train() client id: f_00008-9-0 loss: 1.002163  [   32/  130]
train() client id: f_00008-9-1 loss: 0.727485  [   64/  130]
train() client id: f_00008-9-2 loss: 0.602972  [   96/  130]
train() client id: f_00008-9-3 loss: 0.886321  [  128/  130]
train() client id: f_00008-10-0 loss: 0.922423  [   32/  130]
train() client id: f_00008-10-1 loss: 0.766719  [   64/  130]
train() client id: f_00008-10-2 loss: 0.745097  [   96/  130]
train() client id: f_00008-10-3 loss: 0.795868  [  128/  130]
train() client id: f_00008-11-0 loss: 0.808674  [   32/  130]
train() client id: f_00008-11-1 loss: 0.755814  [   64/  130]
train() client id: f_00008-11-2 loss: 0.842387  [   96/  130]
train() client id: f_00008-11-3 loss: 0.785628  [  128/  130]
train() client id: f_00009-0-0 loss: 1.209857  [   32/  118]
train() client id: f_00009-0-1 loss: 1.228827  [   64/  118]
train() client id: f_00009-0-2 loss: 1.041915  [   96/  118]
train() client id: f_00009-1-0 loss: 1.093892  [   32/  118]
train() client id: f_00009-1-1 loss: 0.946529  [   64/  118]
train() client id: f_00009-1-2 loss: 1.305764  [   96/  118]
train() client id: f_00009-2-0 loss: 1.074086  [   32/  118]
train() client id: f_00009-2-1 loss: 1.107569  [   64/  118]
train() client id: f_00009-2-2 loss: 1.088782  [   96/  118]
train() client id: f_00009-3-0 loss: 1.018583  [   32/  118]
train() client id: f_00009-3-1 loss: 0.960810  [   64/  118]
train() client id: f_00009-3-2 loss: 1.080294  [   96/  118]
train() client id: f_00009-4-0 loss: 0.957372  [   32/  118]
train() client id: f_00009-4-1 loss: 1.080634  [   64/  118]
train() client id: f_00009-4-2 loss: 1.032258  [   96/  118]
train() client id: f_00009-5-0 loss: 1.061776  [   32/  118]
train() client id: f_00009-5-1 loss: 0.912047  [   64/  118]
train() client id: f_00009-5-2 loss: 0.956290  [   96/  118]
train() client id: f_00009-6-0 loss: 0.968910  [   32/  118]
train() client id: f_00009-6-1 loss: 0.987286  [   64/  118]
train() client id: f_00009-6-2 loss: 0.957907  [   96/  118]
train() client id: f_00009-7-0 loss: 0.966338  [   32/  118]
train() client id: f_00009-7-1 loss: 0.913973  [   64/  118]
train() client id: f_00009-7-2 loss: 0.844701  [   96/  118]
train() client id: f_00009-8-0 loss: 1.018556  [   32/  118]
train() client id: f_00009-8-1 loss: 0.866527  [   64/  118]
train() client id: f_00009-8-2 loss: 0.851617  [   96/  118]
train() client id: f_00009-9-0 loss: 0.906246  [   32/  118]
train() client id: f_00009-9-1 loss: 0.989715  [   64/  118]
train() client id: f_00009-9-2 loss: 0.876884  [   96/  118]
train() client id: f_00009-10-0 loss: 1.034994  [   32/  118]
train() client id: f_00009-10-1 loss: 0.840462  [   64/  118]
train() client id: f_00009-10-2 loss: 0.914345  [   96/  118]
train() client id: f_00009-11-0 loss: 0.849419  [   32/  118]
train() client id: f_00009-11-1 loss: 0.926818  [   64/  118]
train() client id: f_00009-11-2 loss: 0.924504  [   96/  118]
At round 23 accuracy: 0.6392572944297082
At round 23 training accuracy: 0.5801475519785378
At round 23 training loss: 0.8423799101560608
update_location
xs = [ -3.9056584    4.20031788 135.00902392  18.81129433   0.97929623
   3.95640986 -97.44319194 -76.32485185 119.66397685 -62.06087855]
ys = [127.5879595  110.55583871   1.32061395 -97.45517586  89.35018685
  72.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [162.15406741 149.13160679 168.01541763 140.89491153 134.10598387
 123.76410517 139.64979842 125.80206384 156.93354431 117.76062374]
dists_bs = [177.54203815 190.43582607 355.34507742 334.35705565 195.71561817
 206.07606464 193.81871306 200.18651135 334.08288959 204.92861778]
uav_gains = [2.97801922e-11 3.67836040e-11 2.72058983e-11 4.24194154e-11
 4.80041803e-11 5.86786623e-11 4.33735009e-11 5.63299134e-11
 3.23510419e-11 6.64482197e-11]
bs_gains = [5.56225278e-11 4.57085931e-11 7.97027370e-12 9.45158637e-12
 4.23392081e-11 3.66451867e-11 4.35097021e-11 3.97444612e-11
 9.47332053e-12 3.72226060e-11]
Round 24
-------------------------------
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.82454946 16.26273367  7.70634767  2.76510758 18.75837368  9.03341251
  3.43331975 11.02682771  8.12439108  7.33018602]
obj_prev = 92.26524913835074
eta_min = 1.3786631317009973e-12	eta_max = 0.9235436873576162
af = 19.491574900134562	bf = 1.580577038913744	zeta = 21.44073239014802	eta = 0.909090909090909
af = 19.491574900134562	bf = 1.580577038913744	zeta = 37.821548020465336	eta = 0.5153563489677266
af = 19.491574900134562	bf = 1.580577038913744	zeta = 29.921938016706985	eta = 0.65141418611493
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.500452198585428	eta = 0.6839040575328834
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.428583728430333	eta = 0.6856329912995908
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.428386266155645	eta = 0.685637753675084
eta = 0.685637753675084
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [0.03114263 0.06549838 0.0306483  0.01062804 0.07563209 0.0360859
 0.01334684 0.0442423  0.03213128 0.02916532]
ene_total = [2.48895914 4.62483507 2.46833909 1.14803217 5.27587599 2.77970132
 1.31845091 3.25561899 2.72998043 2.33859317]
ti_comp = [0.37300745 0.38019815 0.37131151 0.37903648 0.37900657 0.37665288
 0.37938621 0.38326735 0.34493694 0.37691463]
ti_coms = [0.08054563 0.07335493 0.08224157 0.07451661 0.07454651 0.07690021
 0.07416687 0.07028573 0.10861614 0.07663845]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.35678390e-05 1.21493121e-04 1.30503540e-05 5.22248762e-07
 1.88236947e-04 2.07018929e-05 1.03241013e-06 3.68459284e-05
 1.74254783e-05 1.09142550e-05]
ene_total = [0.51231347 0.47350677 0.52304957 0.47320176 0.48531125 0.48961862
 0.47101339 0.44864286 0.69080172 0.48733502]
optimize_network_iter = 0 obj = 5.054794435766291
eta = 0.685637753675084
freqs = [41745313.04826085 86137162.39129016 41270331.48315956 14019810.25468265
 99776755.16786237 47903390.74277508 17590043.05099849 57717284.54230393
 46575586.46414848 38689551.73454573]
eta_min = 0.6856377536750926	eta_max = 0.6856377536750637
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 0.024656332621380766	eta = 0.909090909090909
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 17.409847423005747	eta = 0.0012874810038828607
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.7319792587002336	eta = 0.012941753040645609
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.7000222977574324	eta = 0.013185031671165255
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.700018383049954	eta = 0.013185062032920516
eta = 0.013185062032920516
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.55616847e-04 1.39346998e-03 1.49681533e-04 5.98995210e-06
 2.15899085e-03 2.37441151e-04 1.18412674e-05 4.22605781e-04
 1.99862188e-04 1.25181464e-04]
ene_total = [0.16607727 0.17657133 0.16938779 0.15085494 0.1944668  0.16035839
 0.15026585 0.15072403 0.2237539  0.15755809]
ti_comp = [0.37300745 0.38019815 0.37131151 0.37903648 0.37900657 0.37665288
 0.37938621 0.38326735 0.34493694 0.37691463]
ti_coms = [0.08054563 0.07335493 0.08224157 0.07451661 0.07454651 0.07690021
 0.07416687 0.07028573 0.10861614 0.07663845]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.35678390e-05 1.21493121e-04 1.30503540e-05 5.22248762e-07
 1.88236947e-04 2.07018929e-05 1.03241013e-06 3.68459284e-05
 1.74254783e-05 1.09142550e-05]
ene_total = [0.51231347 0.47350677 0.52304957 0.47320176 0.48531125 0.48961862
 0.47101339 0.44864286 0.69080172 0.48733502]
optimize_network_iter = 1 obj = 5.0547944357664285
eta = 0.6856377536750926
freqs = [41745313.04826085 86137162.3912901  41270331.48315956 14019810.25468264
 99776755.16786231 47903390.74277506 17590043.05099848 57717284.54230388
 46575586.46414861 38689551.73454572]
Done!
At round 24 eta: 0.6856377536750926
At round 24 local rounds: 12.358184869136853
At round 24 global rounds: 63.498409119903215
At round 24 a_n: 19.61895668202144
gradient difference: 0.45966020226478577
train() client id: f_00000-0-0 loss: 1.330003  [   32/  126]
train() client id: f_00000-0-1 loss: 1.154465  [   64/  126]
train() client id: f_00000-0-2 loss: 1.078990  [   96/  126]
train() client id: f_00000-1-0 loss: 1.145863  [   32/  126]
train() client id: f_00000-1-1 loss: 1.299586  [   64/  126]
train() client id: f_00000-1-2 loss: 0.940219  [   96/  126]
train() client id: f_00000-2-0 loss: 1.091084  [   32/  126]
train() client id: f_00000-2-1 loss: 1.054363  [   64/  126]
train() client id: f_00000-2-2 loss: 1.008467  [   96/  126]
train() client id: f_00000-3-0 loss: 0.987886  [   32/  126]
train() client id: f_00000-3-1 loss: 0.904561  [   64/  126]
train() client id: f_00000-3-2 loss: 0.910678  [   96/  126]
train() client id: f_00000-4-0 loss: 0.923767  [   32/  126]
train() client id: f_00000-4-1 loss: 0.886832  [   64/  126]
train() client id: f_00000-4-2 loss: 0.875330  [   96/  126]
train() client id: f_00000-5-0 loss: 0.874285  [   32/  126]
train() client id: f_00000-5-1 loss: 0.917577  [   64/  126]
train() client id: f_00000-5-2 loss: 0.881806  [   96/  126]
train() client id: f_00000-6-0 loss: 0.937462  [   32/  126]
train() client id: f_00000-6-1 loss: 0.862871  [   64/  126]
train() client id: f_00000-6-2 loss: 0.810185  [   96/  126]
train() client id: f_00000-7-0 loss: 0.829471  [   32/  126]
train() client id: f_00000-7-1 loss: 0.960120  [   64/  126]
train() client id: f_00000-7-2 loss: 0.779776  [   96/  126]
train() client id: f_00000-8-0 loss: 0.691225  [   32/  126]
train() client id: f_00000-8-1 loss: 0.891065  [   64/  126]
train() client id: f_00000-8-2 loss: 0.866179  [   96/  126]
train() client id: f_00000-9-0 loss: 0.885698  [   32/  126]
train() client id: f_00000-9-1 loss: 0.843618  [   64/  126]
train() client id: f_00000-9-2 loss: 0.680454  [   96/  126]
train() client id: f_00000-10-0 loss: 0.900162  [   32/  126]
train() client id: f_00000-10-1 loss: 0.727752  [   64/  126]
train() client id: f_00000-10-2 loss: 0.830726  [   96/  126]
train() client id: f_00000-11-0 loss: 0.790858  [   32/  126]
train() client id: f_00000-11-1 loss: 0.741837  [   64/  126]
train() client id: f_00000-11-2 loss: 0.842885  [   96/  126]
train() client id: f_00001-0-0 loss: 0.458269  [   32/  265]
train() client id: f_00001-0-1 loss: 0.574266  [   64/  265]
train() client id: f_00001-0-2 loss: 0.448393  [   96/  265]
train() client id: f_00001-0-3 loss: 0.422361  [  128/  265]
train() client id: f_00001-0-4 loss: 0.433416  [  160/  265]
train() client id: f_00001-0-5 loss: 0.473628  [  192/  265]
train() client id: f_00001-0-6 loss: 0.429880  [  224/  265]
train() client id: f_00001-0-7 loss: 0.432075  [  256/  265]
train() client id: f_00001-1-0 loss: 0.583589  [   32/  265]
train() client id: f_00001-1-1 loss: 0.438619  [   64/  265]
train() client id: f_00001-1-2 loss: 0.397149  [   96/  265]
train() client id: f_00001-1-3 loss: 0.516662  [  128/  265]
train() client id: f_00001-1-4 loss: 0.411199  [  160/  265]
train() client id: f_00001-1-5 loss: 0.444972  [  192/  265]
train() client id: f_00001-1-6 loss: 0.412801  [  224/  265]
train() client id: f_00001-1-7 loss: 0.458212  [  256/  265]
train() client id: f_00001-2-0 loss: 0.465835  [   32/  265]
train() client id: f_00001-2-1 loss: 0.432993  [   64/  265]
train() client id: f_00001-2-2 loss: 0.490165  [   96/  265]
train() client id: f_00001-2-3 loss: 0.366398  [  128/  265]
train() client id: f_00001-2-4 loss: 0.497824  [  160/  265]
train() client id: f_00001-2-5 loss: 0.452432  [  192/  265]
train() client id: f_00001-2-6 loss: 0.443216  [  224/  265]
train() client id: f_00001-2-7 loss: 0.396581  [  256/  265]
train() client id: f_00001-3-0 loss: 0.403258  [   32/  265]
train() client id: f_00001-3-1 loss: 0.414480  [   64/  265]
train() client id: f_00001-3-2 loss: 0.481455  [   96/  265]
train() client id: f_00001-3-3 loss: 0.388804  [  128/  265]
train() client id: f_00001-3-4 loss: 0.560685  [  160/  265]
train() client id: f_00001-3-5 loss: 0.329599  [  192/  265]
train() client id: f_00001-3-6 loss: 0.447853  [  224/  265]
train() client id: f_00001-3-7 loss: 0.516177  [  256/  265]
train() client id: f_00001-4-0 loss: 0.374570  [   32/  265]
train() client id: f_00001-4-1 loss: 0.541734  [   64/  265]
train() client id: f_00001-4-2 loss: 0.333233  [   96/  265]
train() client id: f_00001-4-3 loss: 0.471619  [  128/  265]
train() client id: f_00001-4-4 loss: 0.484052  [  160/  265]
train() client id: f_00001-4-5 loss: 0.470122  [  192/  265]
train() client id: f_00001-4-6 loss: 0.435112  [  224/  265]
train() client id: f_00001-4-7 loss: 0.390091  [  256/  265]
train() client id: f_00001-5-0 loss: 0.363148  [   32/  265]
train() client id: f_00001-5-1 loss: 0.436770  [   64/  265]
train() client id: f_00001-5-2 loss: 0.444339  [   96/  265]
train() client id: f_00001-5-3 loss: 0.338881  [  128/  265]
train() client id: f_00001-5-4 loss: 0.540653  [  160/  265]
train() client id: f_00001-5-5 loss: 0.393127  [  192/  265]
train() client id: f_00001-5-6 loss: 0.415347  [  224/  265]
train() client id: f_00001-5-7 loss: 0.546114  [  256/  265]
train() client id: f_00001-6-0 loss: 0.453590  [   32/  265]
train() client id: f_00001-6-1 loss: 0.352086  [   64/  265]
train() client id: f_00001-6-2 loss: 0.421550  [   96/  265]
train() client id: f_00001-6-3 loss: 0.442753  [  128/  265]
train() client id: f_00001-6-4 loss: 0.435188  [  160/  265]
train() client id: f_00001-6-5 loss: 0.318780  [  192/  265]
train() client id: f_00001-6-6 loss: 0.489189  [  224/  265]
train() client id: f_00001-6-7 loss: 0.471698  [  256/  265]
train() client id: f_00001-7-0 loss: 0.387785  [   32/  265]
train() client id: f_00001-7-1 loss: 0.491912  [   64/  265]
train() client id: f_00001-7-2 loss: 0.486435  [   96/  265]
train() client id: f_00001-7-3 loss: 0.340431  [  128/  265]
train() client id: f_00001-7-4 loss: 0.350315  [  160/  265]
train() client id: f_00001-7-5 loss: 0.398263  [  192/  265]
train() client id: f_00001-7-6 loss: 0.380780  [  224/  265]
train() client id: f_00001-7-7 loss: 0.529554  [  256/  265]
train() client id: f_00001-8-0 loss: 0.433784  [   32/  265]
train() client id: f_00001-8-1 loss: 0.497927  [   64/  265]
train() client id: f_00001-8-2 loss: 0.360160  [   96/  265]
train() client id: f_00001-8-3 loss: 0.321794  [  128/  265]
train() client id: f_00001-8-4 loss: 0.416693  [  160/  265]
train() client id: f_00001-8-5 loss: 0.378254  [  192/  265]
train() client id: f_00001-8-6 loss: 0.428039  [  224/  265]
train() client id: f_00001-8-7 loss: 0.443368  [  256/  265]
train() client id: f_00001-9-0 loss: 0.408650  [   32/  265]
train() client id: f_00001-9-1 loss: 0.432638  [   64/  265]
train() client id: f_00001-9-2 loss: 0.496257  [   96/  265]
train() client id: f_00001-9-3 loss: 0.318853  [  128/  265]
train() client id: f_00001-9-4 loss: 0.441146  [  160/  265]
train() client id: f_00001-9-5 loss: 0.406907  [  192/  265]
train() client id: f_00001-9-6 loss: 0.346615  [  224/  265]
train() client id: f_00001-9-7 loss: 0.470530  [  256/  265]
train() client id: f_00001-10-0 loss: 0.350391  [   32/  265]
train() client id: f_00001-10-1 loss: 0.524788  [   64/  265]
train() client id: f_00001-10-2 loss: 0.331881  [   96/  265]
train() client id: f_00001-10-3 loss: 0.478491  [  128/  265]
train() client id: f_00001-10-4 loss: 0.344543  [  160/  265]
train() client id: f_00001-10-5 loss: 0.470026  [  192/  265]
train() client id: f_00001-10-6 loss: 0.413756  [  224/  265]
train() client id: f_00001-10-7 loss: 0.504495  [  256/  265]
train() client id: f_00001-11-0 loss: 0.347750  [   32/  265]
train() client id: f_00001-11-1 loss: 0.507621  [   64/  265]
train() client id: f_00001-11-2 loss: 0.459510  [   96/  265]
train() client id: f_00001-11-3 loss: 0.322746  [  128/  265]
train() client id: f_00001-11-4 loss: 0.478480  [  160/  265]
train() client id: f_00001-11-5 loss: 0.400848  [  192/  265]
train() client id: f_00001-11-6 loss: 0.412051  [  224/  265]
train() client id: f_00001-11-7 loss: 0.415440  [  256/  265]
train() client id: f_00002-0-0 loss: 1.240750  [   32/  124]
train() client id: f_00002-0-1 loss: 1.157721  [   64/  124]
train() client id: f_00002-0-2 loss: 1.183972  [   96/  124]
train() client id: f_00002-1-0 loss: 1.205421  [   32/  124]
train() client id: f_00002-1-1 loss: 1.193766  [   64/  124]
train() client id: f_00002-1-2 loss: 1.095898  [   96/  124]
train() client id: f_00002-2-0 loss: 1.305766  [   32/  124]
train() client id: f_00002-2-1 loss: 1.018904  [   64/  124]
train() client id: f_00002-2-2 loss: 1.082537  [   96/  124]
train() client id: f_00002-3-0 loss: 1.137827  [   32/  124]
train() client id: f_00002-3-1 loss: 1.110569  [   64/  124]
train() client id: f_00002-3-2 loss: 1.067538  [   96/  124]
train() client id: f_00002-4-0 loss: 0.999537  [   32/  124]
train() client id: f_00002-4-1 loss: 1.118041  [   64/  124]
train() client id: f_00002-4-2 loss: 1.067314  [   96/  124]
train() client id: f_00002-5-0 loss: 1.060682  [   32/  124]
train() client id: f_00002-5-1 loss: 1.188756  [   64/  124]
train() client id: f_00002-5-2 loss: 0.990382  [   96/  124]
train() client id: f_00002-6-0 loss: 1.022586  [   32/  124]
train() client id: f_00002-6-1 loss: 1.138028  [   64/  124]
train() client id: f_00002-6-2 loss: 1.057793  [   96/  124]
train() client id: f_00002-7-0 loss: 1.155085  [   32/  124]
train() client id: f_00002-7-1 loss: 1.039615  [   64/  124]
train() client id: f_00002-7-2 loss: 1.065494  [   96/  124]
train() client id: f_00002-8-0 loss: 1.053299  [   32/  124]
train() client id: f_00002-8-1 loss: 1.198100  [   64/  124]
train() client id: f_00002-8-2 loss: 0.948994  [   96/  124]
train() client id: f_00002-9-0 loss: 0.958899  [   32/  124]
train() client id: f_00002-9-1 loss: 1.027889  [   64/  124]
train() client id: f_00002-9-2 loss: 1.126906  [   96/  124]
train() client id: f_00002-10-0 loss: 0.841873  [   32/  124]
train() client id: f_00002-10-1 loss: 0.976247  [   64/  124]
train() client id: f_00002-10-2 loss: 1.277279  [   96/  124]
train() client id: f_00002-11-0 loss: 1.114155  [   32/  124]
train() client id: f_00002-11-1 loss: 0.987291  [   64/  124]
train() client id: f_00002-11-2 loss: 1.009495  [   96/  124]
train() client id: f_00003-0-0 loss: 0.607002  [   32/   43]
train() client id: f_00003-1-0 loss: 0.506905  [   32/   43]
train() client id: f_00003-2-0 loss: 0.580795  [   32/   43]
train() client id: f_00003-3-0 loss: 0.767519  [   32/   43]
train() client id: f_00003-4-0 loss: 0.546917  [   32/   43]
train() client id: f_00003-5-0 loss: 0.540127  [   32/   43]
train() client id: f_00003-6-0 loss: 0.660680  [   32/   43]
train() client id: f_00003-7-0 loss: 0.587249  [   32/   43]
train() client id: f_00003-8-0 loss: 0.566121  [   32/   43]
train() client id: f_00003-9-0 loss: 0.685618  [   32/   43]
train() client id: f_00003-10-0 loss: 0.626293  [   32/   43]
train() client id: f_00003-11-0 loss: 0.567487  [   32/   43]
train() client id: f_00004-0-0 loss: 0.967537  [   32/  306]
train() client id: f_00004-0-1 loss: 0.879436  [   64/  306]
train() client id: f_00004-0-2 loss: 0.916655  [   96/  306]
train() client id: f_00004-0-3 loss: 0.766055  [  128/  306]
train() client id: f_00004-0-4 loss: 0.934513  [  160/  306]
train() client id: f_00004-0-5 loss: 0.973440  [  192/  306]
train() client id: f_00004-0-6 loss: 1.028549  [  224/  306]
train() client id: f_00004-0-7 loss: 1.025572  [  256/  306]
train() client id: f_00004-0-8 loss: 0.954155  [  288/  306]
train() client id: f_00004-1-0 loss: 0.980024  [   32/  306]
train() client id: f_00004-1-1 loss: 0.881355  [   64/  306]
train() client id: f_00004-1-2 loss: 0.858984  [   96/  306]
train() client id: f_00004-1-3 loss: 0.866644  [  128/  306]
train() client id: f_00004-1-4 loss: 1.076909  [  160/  306]
train() client id: f_00004-1-5 loss: 1.028576  [  192/  306]
train() client id: f_00004-1-6 loss: 0.920959  [  224/  306]
train() client id: f_00004-1-7 loss: 0.978220  [  256/  306]
train() client id: f_00004-1-8 loss: 0.822931  [  288/  306]
train() client id: f_00004-2-0 loss: 0.976294  [   32/  306]
train() client id: f_00004-2-1 loss: 0.976992  [   64/  306]
train() client id: f_00004-2-2 loss: 1.032642  [   96/  306]
train() client id: f_00004-2-3 loss: 0.916282  [  128/  306]
train() client id: f_00004-2-4 loss: 0.792917  [  160/  306]
train() client id: f_00004-2-5 loss: 0.892806  [  192/  306]
train() client id: f_00004-2-6 loss: 0.940229  [  224/  306]
train() client id: f_00004-2-7 loss: 0.836906  [  256/  306]
train() client id: f_00004-2-8 loss: 0.963916  [  288/  306]
train() client id: f_00004-3-0 loss: 0.901669  [   32/  306]
train() client id: f_00004-3-1 loss: 0.950353  [   64/  306]
train() client id: f_00004-3-2 loss: 0.907295  [   96/  306]
train() client id: f_00004-3-3 loss: 0.911992  [  128/  306]
train() client id: f_00004-3-4 loss: 1.048823  [  160/  306]
train() client id: f_00004-3-5 loss: 0.931469  [  192/  306]
train() client id: f_00004-3-6 loss: 0.962275  [  224/  306]
train() client id: f_00004-3-7 loss: 0.903639  [  256/  306]
train() client id: f_00004-3-8 loss: 0.884203  [  288/  306]
train() client id: f_00004-4-0 loss: 0.927684  [   32/  306]
train() client id: f_00004-4-1 loss: 0.971175  [   64/  306]
train() client id: f_00004-4-2 loss: 1.033865  [   96/  306]
train() client id: f_00004-4-3 loss: 0.993734  [  128/  306]
train() client id: f_00004-4-4 loss: 0.837699  [  160/  306]
train() client id: f_00004-4-5 loss: 0.855747  [  192/  306]
train() client id: f_00004-4-6 loss: 0.943320  [  224/  306]
train() client id: f_00004-4-7 loss: 0.987792  [  256/  306]
train() client id: f_00004-4-8 loss: 0.847370  [  288/  306]
train() client id: f_00004-5-0 loss: 0.860654  [   32/  306]
train() client id: f_00004-5-1 loss: 0.812475  [   64/  306]
train() client id: f_00004-5-2 loss: 0.877391  [   96/  306]
train() client id: f_00004-5-3 loss: 0.997154  [  128/  306]
train() client id: f_00004-5-4 loss: 1.059032  [  160/  306]
train() client id: f_00004-5-5 loss: 1.000772  [  192/  306]
train() client id: f_00004-5-6 loss: 0.863880  [  224/  306]
train() client id: f_00004-5-7 loss: 0.940146  [  256/  306]
train() client id: f_00004-5-8 loss: 0.949856  [  288/  306]
train() client id: f_00004-6-0 loss: 1.076905  [   32/  306]
train() client id: f_00004-6-1 loss: 0.890536  [   64/  306]
train() client id: f_00004-6-2 loss: 1.053336  [   96/  306]
train() client id: f_00004-6-3 loss: 0.856841  [  128/  306]
train() client id: f_00004-6-4 loss: 0.890012  [  160/  306]
train() client id: f_00004-6-5 loss: 0.874379  [  192/  306]
train() client id: f_00004-6-6 loss: 0.826647  [  224/  306]
train() client id: f_00004-6-7 loss: 0.952113  [  256/  306]
train() client id: f_00004-6-8 loss: 0.882994  [  288/  306]
train() client id: f_00004-7-0 loss: 1.003662  [   32/  306]
train() client id: f_00004-7-1 loss: 0.739214  [   64/  306]
train() client id: f_00004-7-2 loss: 0.883213  [   96/  306]
train() client id: f_00004-7-3 loss: 0.917144  [  128/  306]
train() client id: f_00004-7-4 loss: 1.010625  [  160/  306]
train() client id: f_00004-7-5 loss: 0.850374  [  192/  306]
train() client id: f_00004-7-6 loss: 0.910032  [  224/  306]
train() client id: f_00004-7-7 loss: 1.078733  [  256/  306]
train() client id: f_00004-7-8 loss: 0.839014  [  288/  306]
train() client id: f_00004-8-0 loss: 0.979671  [   32/  306]
train() client id: f_00004-8-1 loss: 0.981889  [   64/  306]
train() client id: f_00004-8-2 loss: 0.889402  [   96/  306]
train() client id: f_00004-8-3 loss: 0.884856  [  128/  306]
train() client id: f_00004-8-4 loss: 0.958166  [  160/  306]
train() client id: f_00004-8-5 loss: 1.024969  [  192/  306]
train() client id: f_00004-8-6 loss: 0.981720  [  224/  306]
train() client id: f_00004-8-7 loss: 0.834679  [  256/  306]
train() client id: f_00004-8-8 loss: 0.792999  [  288/  306]
train() client id: f_00004-9-0 loss: 0.994731  [   32/  306]
train() client id: f_00004-9-1 loss: 0.975967  [   64/  306]
train() client id: f_00004-9-2 loss: 0.865435  [   96/  306]
train() client id: f_00004-9-3 loss: 0.922364  [  128/  306]
train() client id: f_00004-9-4 loss: 0.858023  [  160/  306]
train() client id: f_00004-9-5 loss: 0.841459  [  192/  306]
train() client id: f_00004-9-6 loss: 0.975242  [  224/  306]
train() client id: f_00004-9-7 loss: 0.917085  [  256/  306]
train() client id: f_00004-9-8 loss: 0.872413  [  288/  306]
train() client id: f_00004-10-0 loss: 0.833455  [   32/  306]
train() client id: f_00004-10-1 loss: 0.939735  [   64/  306]
train() client id: f_00004-10-2 loss: 0.835692  [   96/  306]
train() client id: f_00004-10-3 loss: 0.968329  [  128/  306]
train() client id: f_00004-10-4 loss: 0.884764  [  160/  306]
train() client id: f_00004-10-5 loss: 0.957898  [  192/  306]
train() client id: f_00004-10-6 loss: 0.952971  [  224/  306]
train() client id: f_00004-10-7 loss: 0.949058  [  256/  306]
train() client id: f_00004-10-8 loss: 0.985776  [  288/  306]
train() client id: f_00004-11-0 loss: 0.967182  [   32/  306]
train() client id: f_00004-11-1 loss: 0.960681  [   64/  306]
train() client id: f_00004-11-2 loss: 0.890014  [   96/  306]
train() client id: f_00004-11-3 loss: 0.958359  [  128/  306]
train() client id: f_00004-11-4 loss: 0.914675  [  160/  306]
train() client id: f_00004-11-5 loss: 1.009468  [  192/  306]
train() client id: f_00004-11-6 loss: 0.802352  [  224/  306]
train() client id: f_00004-11-7 loss: 0.799685  [  256/  306]
train() client id: f_00004-11-8 loss: 0.903244  [  288/  306]
train() client id: f_00005-0-0 loss: 0.677345  [   32/  146]
train() client id: f_00005-0-1 loss: 0.554556  [   64/  146]
train() client id: f_00005-0-2 loss: 0.711894  [   96/  146]
train() client id: f_00005-0-3 loss: 0.596394  [  128/  146]
train() client id: f_00005-1-0 loss: 0.787780  [   32/  146]
train() client id: f_00005-1-1 loss: 0.722824  [   64/  146]
train() client id: f_00005-1-2 loss: 0.502939  [   96/  146]
train() client id: f_00005-1-3 loss: 0.503699  [  128/  146]
train() client id: f_00005-2-0 loss: 0.543712  [   32/  146]
train() client id: f_00005-2-1 loss: 0.635738  [   64/  146]
train() client id: f_00005-2-2 loss: 0.800242  [   96/  146]
train() client id: f_00005-2-3 loss: 0.501520  [  128/  146]
train() client id: f_00005-3-0 loss: 0.726554  [   32/  146]
train() client id: f_00005-3-1 loss: 0.497322  [   64/  146]
train() client id: f_00005-3-2 loss: 0.385670  [   96/  146]
train() client id: f_00005-3-3 loss: 0.728305  [  128/  146]
train() client id: f_00005-4-0 loss: 0.423551  [   32/  146]
train() client id: f_00005-4-1 loss: 0.743545  [   64/  146]
train() client id: f_00005-4-2 loss: 0.518287  [   96/  146]
train() client id: f_00005-4-3 loss: 0.753600  [  128/  146]
train() client id: f_00005-5-0 loss: 0.519216  [   32/  146]
train() client id: f_00005-5-1 loss: 0.726078  [   64/  146]
train() client id: f_00005-5-2 loss: 0.674800  [   96/  146]
train() client id: f_00005-5-3 loss: 0.649499  [  128/  146]
train() client id: f_00005-6-0 loss: 0.589705  [   32/  146]
train() client id: f_00005-6-1 loss: 0.877942  [   64/  146]
train() client id: f_00005-6-2 loss: 0.550029  [   96/  146]
train() client id: f_00005-6-3 loss: 0.526618  [  128/  146]
train() client id: f_00005-7-0 loss: 0.445291  [   32/  146]
train() client id: f_00005-7-1 loss: 0.796695  [   64/  146]
train() client id: f_00005-7-2 loss: 0.648005  [   96/  146]
train() client id: f_00005-7-3 loss: 0.761751  [  128/  146]
train() client id: f_00005-8-0 loss: 0.618474  [   32/  146]
train() client id: f_00005-8-1 loss: 0.511814  [   64/  146]
train() client id: f_00005-8-2 loss: 0.605211  [   96/  146]
train() client id: f_00005-8-3 loss: 0.546644  [  128/  146]
train() client id: f_00005-9-0 loss: 0.588958  [   32/  146]
train() client id: f_00005-9-1 loss: 0.648519  [   64/  146]
train() client id: f_00005-9-2 loss: 0.656504  [   96/  146]
train() client id: f_00005-9-3 loss: 0.523414  [  128/  146]
train() client id: f_00005-10-0 loss: 0.388316  [   32/  146]
train() client id: f_00005-10-1 loss: 0.755702  [   64/  146]
train() client id: f_00005-10-2 loss: 0.515659  [   96/  146]
train() client id: f_00005-10-3 loss: 0.757641  [  128/  146]
train() client id: f_00005-11-0 loss: 0.318514  [   32/  146]
train() client id: f_00005-11-1 loss: 0.545251  [   64/  146]
train() client id: f_00005-11-2 loss: 0.713426  [   96/  146]
train() client id: f_00005-11-3 loss: 0.600958  [  128/  146]
train() client id: f_00006-0-0 loss: 0.555681  [   32/   54]
train() client id: f_00006-1-0 loss: 0.606061  [   32/   54]
train() client id: f_00006-2-0 loss: 0.528428  [   32/   54]
train() client id: f_00006-3-0 loss: 0.505773  [   32/   54]
train() client id: f_00006-4-0 loss: 0.552605  [   32/   54]
train() client id: f_00006-5-0 loss: 0.597162  [   32/   54]
train() client id: f_00006-6-0 loss: 0.544331  [   32/   54]
train() client id: f_00006-7-0 loss: 0.573076  [   32/   54]
train() client id: f_00006-8-0 loss: 0.574181  [   32/   54]
train() client id: f_00006-9-0 loss: 0.544626  [   32/   54]
train() client id: f_00006-10-0 loss: 0.568242  [   32/   54]
train() client id: f_00006-11-0 loss: 0.590745  [   32/   54]
train() client id: f_00007-0-0 loss: 0.673823  [   32/  179]
train() client id: f_00007-0-1 loss: 0.799986  [   64/  179]
train() client id: f_00007-0-2 loss: 0.662304  [   96/  179]
train() client id: f_00007-0-3 loss: 0.607550  [  128/  179]
train() client id: f_00007-0-4 loss: 0.626481  [  160/  179]
train() client id: f_00007-1-0 loss: 0.621531  [   32/  179]
train() client id: f_00007-1-1 loss: 0.627885  [   64/  179]
train() client id: f_00007-1-2 loss: 0.705925  [   96/  179]
train() client id: f_00007-1-3 loss: 0.739631  [  128/  179]
train() client id: f_00007-1-4 loss: 0.603245  [  160/  179]
train() client id: f_00007-2-0 loss: 0.804091  [   32/  179]
train() client id: f_00007-2-1 loss: 0.611003  [   64/  179]
train() client id: f_00007-2-2 loss: 0.582623  [   96/  179]
train() client id: f_00007-2-3 loss: 0.619617  [  128/  179]
train() client id: f_00007-2-4 loss: 0.677210  [  160/  179]
train() client id: f_00007-3-0 loss: 0.566237  [   32/  179]
train() client id: f_00007-3-1 loss: 0.763776  [   64/  179]
train() client id: f_00007-3-2 loss: 0.671374  [   96/  179]
train() client id: f_00007-3-3 loss: 0.689940  [  128/  179]
train() client id: f_00007-3-4 loss: 0.599010  [  160/  179]
train() client id: f_00007-4-0 loss: 0.536821  [   32/  179]
train() client id: f_00007-4-1 loss: 0.704461  [   64/  179]
train() client id: f_00007-4-2 loss: 0.835830  [   96/  179]
train() client id: f_00007-4-3 loss: 0.520921  [  128/  179]
train() client id: f_00007-4-4 loss: 0.611399  [  160/  179]
train() client id: f_00007-5-0 loss: 0.630731  [   32/  179]
train() client id: f_00007-5-1 loss: 0.527932  [   64/  179]
train() client id: f_00007-5-2 loss: 0.531052  [   96/  179]
train() client id: f_00007-5-3 loss: 1.000805  [  128/  179]
train() client id: f_00007-5-4 loss: 0.605895  [  160/  179]
train() client id: f_00007-6-0 loss: 0.542842  [   32/  179]
train() client id: f_00007-6-1 loss: 0.728536  [   64/  179]
train() client id: f_00007-6-2 loss: 0.522330  [   96/  179]
train() client id: f_00007-6-3 loss: 0.660543  [  128/  179]
train() client id: f_00007-6-4 loss: 0.729489  [  160/  179]
train() client id: f_00007-7-0 loss: 0.733571  [   32/  179]
train() client id: f_00007-7-1 loss: 0.614946  [   64/  179]
train() client id: f_00007-7-2 loss: 0.508174  [   96/  179]
train() client id: f_00007-7-3 loss: 0.651032  [  128/  179]
train() client id: f_00007-7-4 loss: 0.752844  [  160/  179]
train() client id: f_00007-8-0 loss: 0.685518  [   32/  179]
train() client id: f_00007-8-1 loss: 0.589860  [   64/  179]
train() client id: f_00007-8-2 loss: 0.688120  [   96/  179]
train() client id: f_00007-8-3 loss: 0.794084  [  128/  179]
train() client id: f_00007-8-4 loss: 0.516359  [  160/  179]
train() client id: f_00007-9-0 loss: 0.465968  [   32/  179]
train() client id: f_00007-9-1 loss: 0.501804  [   64/  179]
train() client id: f_00007-9-2 loss: 0.941073  [   96/  179]
train() client id: f_00007-9-3 loss: 0.646916  [  128/  179]
train() client id: f_00007-9-4 loss: 0.623297  [  160/  179]
train() client id: f_00007-10-0 loss: 0.497534  [   32/  179]
train() client id: f_00007-10-1 loss: 0.645603  [   64/  179]
train() client id: f_00007-10-2 loss: 0.745712  [   96/  179]
train() client id: f_00007-10-3 loss: 0.631664  [  128/  179]
train() client id: f_00007-10-4 loss: 0.695121  [  160/  179]
train() client id: f_00007-11-0 loss: 0.671946  [   32/  179]
train() client id: f_00007-11-1 loss: 0.582577  [   64/  179]
train() client id: f_00007-11-2 loss: 0.667155  [   96/  179]
train() client id: f_00007-11-3 loss: 0.576186  [  128/  179]
train() client id: f_00007-11-4 loss: 0.688734  [  160/  179]
train() client id: f_00008-0-0 loss: 0.730453  [   32/  130]
train() client id: f_00008-0-1 loss: 0.722206  [   64/  130]
train() client id: f_00008-0-2 loss: 0.894265  [   96/  130]
train() client id: f_00008-0-3 loss: 0.710036  [  128/  130]
train() client id: f_00008-1-0 loss: 0.731617  [   32/  130]
train() client id: f_00008-1-1 loss: 0.744097  [   64/  130]
train() client id: f_00008-1-2 loss: 0.759284  [   96/  130]
train() client id: f_00008-1-3 loss: 0.769221  [  128/  130]
train() client id: f_00008-2-0 loss: 0.739749  [   32/  130]
train() client id: f_00008-2-1 loss: 0.868977  [   64/  130]
train() client id: f_00008-2-2 loss: 0.680704  [   96/  130]
train() client id: f_00008-2-3 loss: 0.736011  [  128/  130]
train() client id: f_00008-3-0 loss: 0.672576  [   32/  130]
train() client id: f_00008-3-1 loss: 0.965966  [   64/  130]
train() client id: f_00008-3-2 loss: 0.685672  [   96/  130]
train() client id: f_00008-3-3 loss: 0.730448  [  128/  130]
train() client id: f_00008-4-0 loss: 0.815942  [   32/  130]
train() client id: f_00008-4-1 loss: 0.698280  [   64/  130]
train() client id: f_00008-4-2 loss: 0.753256  [   96/  130]
train() client id: f_00008-4-3 loss: 0.711775  [  128/  130]
train() client id: f_00008-5-0 loss: 0.841273  [   32/  130]
train() client id: f_00008-5-1 loss: 0.729851  [   64/  130]
train() client id: f_00008-5-2 loss: 0.783875  [   96/  130]
train() client id: f_00008-5-3 loss: 0.670684  [  128/  130]
train() client id: f_00008-6-0 loss: 0.702014  [   32/  130]
train() client id: f_00008-6-1 loss: 0.745040  [   64/  130]
train() client id: f_00008-6-2 loss: 0.833258  [   96/  130]
train() client id: f_00008-6-3 loss: 0.757779  [  128/  130]
train() client id: f_00008-7-0 loss: 0.842090  [   32/  130]
train() client id: f_00008-7-1 loss: 0.778268  [   64/  130]
train() client id: f_00008-7-2 loss: 0.637076  [   96/  130]
train() client id: f_00008-7-3 loss: 0.769126  [  128/  130]
train() client id: f_00008-8-0 loss: 0.681091  [   32/  130]
train() client id: f_00008-8-1 loss: 0.794795  [   64/  130]
train() client id: f_00008-8-2 loss: 0.767746  [   96/  130]
train() client id: f_00008-8-3 loss: 0.771741  [  128/  130]
train() client id: f_00008-9-0 loss: 0.800950  [   32/  130]
train() client id: f_00008-9-1 loss: 0.708730  [   64/  130]
train() client id: f_00008-9-2 loss: 0.741763  [   96/  130]
train() client id: f_00008-9-3 loss: 0.782366  [  128/  130]
train() client id: f_00008-10-0 loss: 0.795328  [   32/  130]
train() client id: f_00008-10-1 loss: 0.752566  [   64/  130]
train() client id: f_00008-10-2 loss: 0.776325  [   96/  130]
train() client id: f_00008-10-3 loss: 0.676094  [  128/  130]
train() client id: f_00008-11-0 loss: 0.685080  [   32/  130]
train() client id: f_00008-11-1 loss: 0.734958  [   64/  130]
train() client id: f_00008-11-2 loss: 0.838579  [   96/  130]
train() client id: f_00008-11-3 loss: 0.768366  [  128/  130]
train() client id: f_00009-0-0 loss: 1.141249  [   32/  118]
train() client id: f_00009-0-1 loss: 1.201385  [   64/  118]
train() client id: f_00009-0-2 loss: 1.067918  [   96/  118]
train() client id: f_00009-1-0 loss: 1.048202  [   32/  118]
train() client id: f_00009-1-1 loss: 1.063457  [   64/  118]
train() client id: f_00009-1-2 loss: 1.007889  [   96/  118]
train() client id: f_00009-2-0 loss: 0.974324  [   32/  118]
train() client id: f_00009-2-1 loss: 1.070173  [   64/  118]
train() client id: f_00009-2-2 loss: 1.065739  [   96/  118]
train() client id: f_00009-3-0 loss: 0.960221  [   32/  118]
train() client id: f_00009-3-1 loss: 0.946567  [   64/  118]
train() client id: f_00009-3-2 loss: 1.004268  [   96/  118]
train() client id: f_00009-4-0 loss: 0.923914  [   32/  118]
train() client id: f_00009-4-1 loss: 1.046130  [   64/  118]
train() client id: f_00009-4-2 loss: 0.906119  [   96/  118]
train() client id: f_00009-5-0 loss: 0.922830  [   32/  118]
train() client id: f_00009-5-1 loss: 0.869266  [   64/  118]
train() client id: f_00009-5-2 loss: 0.918118  [   96/  118]
train() client id: f_00009-6-0 loss: 1.012976  [   32/  118]
train() client id: f_00009-6-1 loss: 0.893797  [   64/  118]
train() client id: f_00009-6-2 loss: 0.874659  [   96/  118]
train() client id: f_00009-7-0 loss: 0.881437  [   32/  118]
train() client id: f_00009-7-1 loss: 0.920408  [   64/  118]
train() client id: f_00009-7-2 loss: 0.793533  [   96/  118]
train() client id: f_00009-8-0 loss: 0.979223  [   32/  118]
train() client id: f_00009-8-1 loss: 0.819100  [   64/  118]
train() client id: f_00009-8-2 loss: 0.859120  [   96/  118]
train() client id: f_00009-9-0 loss: 0.788148  [   32/  118]
train() client id: f_00009-9-1 loss: 0.966263  [   64/  118]
train() client id: f_00009-9-2 loss: 0.957679  [   96/  118]
train() client id: f_00009-10-0 loss: 0.908187  [   32/  118]
train() client id: f_00009-10-1 loss: 0.902144  [   64/  118]
train() client id: f_00009-10-2 loss: 0.873188  [   96/  118]
train() client id: f_00009-11-0 loss: 0.844053  [   32/  118]
train() client id: f_00009-11-1 loss: 0.796659  [   64/  118]
train() client id: f_00009-11-2 loss: 0.906422  [   96/  118]
At round 24 accuracy: 0.6419098143236074
At round 24 training accuracy: 0.5828303152246814
At round 24 training loss: 0.8294924867716502
update_location
xs = [  -3.9056584     4.20031788  140.00902392   18.81129433    0.97929623
    3.95640986 -102.44319194  -81.32485185  124.66397685  -67.06087855]
ys = [ 132.5879595   115.55583871    1.32061395 -102.45517586   94.35018685
   77.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [166.11689008 152.87509454 172.05891665 144.39850365 137.48787866
 126.77024586 143.18344219 128.89688819 160.77865871 120.47063247]
dists_bs = [176.27267203 188.80244235 359.71546295 338.4437803  193.57971242
 203.64303555 191.87268031 197.76978496 338.50113277 202.21609029]
uav_gains = [2.80067787e-11 3.45600080e-11 2.55955797e-11 3.98858645e-11
 4.51021890e-11 5.52599497e-11 4.07401996e-11 5.30073043e-11
 3.04302492e-11 6.27732201e-11]
bs_gains = [5.67513399e-11 4.68244606e-11 7.70208994e-12 9.13548862e-12
 4.36602816e-11 3.78843039e-11 4.47566205e-11 4.11193508e-11
 9.13115535e-12 3.86375962e-11]
Round 25
-------------------------------
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.69254516 15.98293133  7.57645092  2.71961017 18.43551744  8.87729699
  3.37637101 10.83932141  7.98736814  7.20317358]
obj_prev = 90.69058614039777
eta_min = 8.689629583387306e-13	eta_max = 0.9239062523538892
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 21.07280247978385	eta = 0.909090909090909
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 37.266591851058905	eta = 0.5140554102721235
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 29.4469781495679	eta = 0.65056227726107
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 28.03930224522773	eta = 0.683222891778642
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 27.967919357716607	eta = 0.6849666905290988
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 27.967721963314986	eta = 0.684971524980406
eta = 0.684971524980406
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [0.03122285 0.0656671  0.03072725 0.01065542 0.07582691 0.03617885
 0.01338122 0.04435627 0.03221405 0.02924044]
ene_total = [2.45319609 4.54407619 2.43314225 1.13378689 5.18350126 2.72855748
 1.30141153 3.20547972 2.69013729 2.29443326]
ti_comp = [0.37995732 0.38865987 0.37822142 0.38614518 0.38758329 0.38530164
 0.38648714 0.39049469 0.35182285 0.38562639]
ti_coms = [0.08168984 0.07298729 0.08342574 0.07550198 0.07406386 0.07634552
 0.07516002 0.07115247 0.10982431 0.07602077]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.31773438e-05 1.17161273e-04 1.26753124e-05 5.07095790e-07
 1.81392801e-04 1.99362151e-05 1.00252924e-06 3.57696506e-05
 1.68798211e-05 1.05074756e-05]
ene_total = [0.50955862 0.46183769 0.52033793 0.47023356 0.47254241 0.47669679
 0.46813477 0.44534224 0.68500157 0.47408718]
optimize_network_iter = 0 obj = 4.983772760330714
eta = 0.684971524980406
freqs = [41087307.98029104 84478875.62799275 40620710.39935426 13797163.01214605
 97820151.49584162 46948740.71955757 17311339.16633044 56794967.35235866
 45781631.37350148 37912918.15352243]
eta_min = 0.6849715249804152	eta_max = 0.68497152498039
af = 0.02120720489719426	bf = 1.5620059925242489	zeta = 0.023327925386913688	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [3.31926004e-06 2.95119211e-05 3.19280262e-06 1.27733086e-07
 4.56912927e-05 5.02176180e-06 2.52528528e-07 9.01006858e-06
 4.25188234e-06 2.64674311e-06]
ene_total = [1.76365275 1.58149679 1.80108751 1.6294247  1.6082219  1.64868503
 1.62207175 1.53747541 2.37101979 1.64116416]
ti_comp = [0.37995732 0.38865987 0.37822142 0.38614518 0.38758329 0.38530164
 0.38648714 0.39049469 0.35182285 0.38562639]
ti_coms = [0.08168984 0.07298729 0.08342574 0.07550198 0.07406386 0.07634552
 0.07516002 0.07115247 0.10982431 0.07602077]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.31773438e-05 1.17161273e-04 1.26753124e-05 5.07095790e-07
 1.81392801e-04 1.99362151e-05 1.00252924e-06 3.57696506e-05
 1.68798211e-05 1.05074756e-05]
ene_total = [0.50955862 0.46183769 0.52033793 0.47023356 0.47254241 0.47669679
 0.46813477 0.44534224 0.68500157 0.47408718]
optimize_network_iter = 1 obj = 4.983772760330463
eta = 0.68497152498039
freqs = [41087307.98029105 84478875.62799287 40620710.39935425 13797163.01214607
 97820151.49584176 46948740.71955761 17311339.16633046 56794967.35235876
 45781631.37350126 37912918.15352247]
Done!
At round 25 eta: 0.68497152498039
At round 25 local rounds: 12.390018469521971
At round 25 global rounds: 62.276772538737
At round 25 a_n: 19.276410835052125
gradient difference: 0.43823155760765076
train() client id: f_00000-0-0 loss: 1.123595  [   32/  126]
train() client id: f_00000-0-1 loss: 1.005637  [   64/  126]
train() client id: f_00000-0-2 loss: 1.177769  [   96/  126]
train() client id: f_00000-1-0 loss: 0.829463  [   32/  126]
train() client id: f_00000-1-1 loss: 1.094308  [   64/  126]
train() client id: f_00000-1-2 loss: 1.053637  [   96/  126]
train() client id: f_00000-2-0 loss: 1.003543  [   32/  126]
train() client id: f_00000-2-1 loss: 0.906545  [   64/  126]
train() client id: f_00000-2-2 loss: 0.974725  [   96/  126]
train() client id: f_00000-3-0 loss: 0.875886  [   32/  126]
train() client id: f_00000-3-1 loss: 0.914457  [   64/  126]
train() client id: f_00000-3-2 loss: 0.954450  [   96/  126]
train() client id: f_00000-4-0 loss: 0.841026  [   32/  126]
train() client id: f_00000-4-1 loss: 0.804880  [   64/  126]
train() client id: f_00000-4-2 loss: 0.843381  [   96/  126]
train() client id: f_00000-5-0 loss: 0.782793  [   32/  126]
train() client id: f_00000-5-1 loss: 0.850655  [   64/  126]
train() client id: f_00000-5-2 loss: 0.905238  [   96/  126]
train() client id: f_00000-6-0 loss: 0.878349  [   32/  126]
train() client id: f_00000-6-1 loss: 0.828772  [   64/  126]
train() client id: f_00000-6-2 loss: 0.773711  [   96/  126]
train() client id: f_00000-7-0 loss: 0.826380  [   32/  126]
train() client id: f_00000-7-1 loss: 0.786211  [   64/  126]
train() client id: f_00000-7-2 loss: 0.810267  [   96/  126]
train() client id: f_00000-8-0 loss: 0.696126  [   32/  126]
train() client id: f_00000-8-1 loss: 0.856521  [   64/  126]
train() client id: f_00000-8-2 loss: 0.934196  [   96/  126]
train() client id: f_00000-9-0 loss: 0.898982  [   32/  126]
train() client id: f_00000-9-1 loss: 0.841767  [   64/  126]
train() client id: f_00000-9-2 loss: 0.757367  [   96/  126]
train() client id: f_00000-10-0 loss: 0.835722  [   32/  126]
train() client id: f_00000-10-1 loss: 0.854186  [   64/  126]
train() client id: f_00000-10-2 loss: 0.727598  [   96/  126]
train() client id: f_00000-11-0 loss: 0.725370  [   32/  126]
train() client id: f_00000-11-1 loss: 0.976090  [   64/  126]
train() client id: f_00000-11-2 loss: 0.831509  [   96/  126]
train() client id: f_00001-0-0 loss: 0.385628  [   32/  265]
train() client id: f_00001-0-1 loss: 0.447709  [   64/  265]
train() client id: f_00001-0-2 loss: 0.618430  [   96/  265]
train() client id: f_00001-0-3 loss: 0.411196  [  128/  265]
train() client id: f_00001-0-4 loss: 0.532136  [  160/  265]
train() client id: f_00001-0-5 loss: 0.490584  [  192/  265]
train() client id: f_00001-0-6 loss: 0.510487  [  224/  265]
train() client id: f_00001-0-7 loss: 0.374492  [  256/  265]
train() client id: f_00001-1-0 loss: 0.487619  [   32/  265]
train() client id: f_00001-1-1 loss: 0.492770  [   64/  265]
train() client id: f_00001-1-2 loss: 0.464676  [   96/  265]
train() client id: f_00001-1-3 loss: 0.372380  [  128/  265]
train() client id: f_00001-1-4 loss: 0.412586  [  160/  265]
train() client id: f_00001-1-5 loss: 0.513699  [  192/  265]
train() client id: f_00001-1-6 loss: 0.434964  [  224/  265]
train() client id: f_00001-1-7 loss: 0.542799  [  256/  265]
train() client id: f_00001-2-0 loss: 0.504294  [   32/  265]
train() client id: f_00001-2-1 loss: 0.382798  [   64/  265]
train() client id: f_00001-2-2 loss: 0.487603  [   96/  265]
train() client id: f_00001-2-3 loss: 0.382944  [  128/  265]
train() client id: f_00001-2-4 loss: 0.549785  [  160/  265]
train() client id: f_00001-2-5 loss: 0.440698  [  192/  265]
train() client id: f_00001-2-6 loss: 0.493404  [  224/  265]
train() client id: f_00001-2-7 loss: 0.414146  [  256/  265]
train() client id: f_00001-3-0 loss: 0.361894  [   32/  265]
train() client id: f_00001-3-1 loss: 0.569322  [   64/  265]
train() client id: f_00001-3-2 loss: 0.449684  [   96/  265]
train() client id: f_00001-3-3 loss: 0.483795  [  128/  265]
train() client id: f_00001-3-4 loss: 0.486082  [  160/  265]
train() client id: f_00001-3-5 loss: 0.461069  [  192/  265]
train() client id: f_00001-3-6 loss: 0.368090  [  224/  265]
train() client id: f_00001-3-7 loss: 0.445712  [  256/  265]
train() client id: f_00001-4-0 loss: 0.350935  [   32/  265]
train() client id: f_00001-4-1 loss: 0.442542  [   64/  265]
train() client id: f_00001-4-2 loss: 0.437574  [   96/  265]
train() client id: f_00001-4-3 loss: 0.462039  [  128/  265]
train() client id: f_00001-4-4 loss: 0.465867  [  160/  265]
train() client id: f_00001-4-5 loss: 0.430426  [  192/  265]
train() client id: f_00001-4-6 loss: 0.526701  [  224/  265]
train() client id: f_00001-4-7 loss: 0.446125  [  256/  265]
train() client id: f_00001-5-0 loss: 0.487517  [   32/  265]
train() client id: f_00001-5-1 loss: 0.422897  [   64/  265]
train() client id: f_00001-5-2 loss: 0.402736  [   96/  265]
train() client id: f_00001-5-3 loss: 0.405573  [  128/  265]
train() client id: f_00001-5-4 loss: 0.467085  [  160/  265]
train() client id: f_00001-5-5 loss: 0.534434  [  192/  265]
train() client id: f_00001-5-6 loss: 0.375092  [  224/  265]
train() client id: f_00001-5-7 loss: 0.476531  [  256/  265]
train() client id: f_00001-6-0 loss: 0.520258  [   32/  265]
train() client id: f_00001-6-1 loss: 0.445634  [   64/  265]
train() client id: f_00001-6-2 loss: 0.366804  [   96/  265]
train() client id: f_00001-6-3 loss: 0.587028  [  128/  265]
train() client id: f_00001-6-4 loss: 0.351971  [  160/  265]
train() client id: f_00001-6-5 loss: 0.459170  [  192/  265]
train() client id: f_00001-6-6 loss: 0.416334  [  224/  265]
train() client id: f_00001-6-7 loss: 0.412518  [  256/  265]
train() client id: f_00001-7-0 loss: 0.539969  [   32/  265]
train() client id: f_00001-7-1 loss: 0.404631  [   64/  265]
train() client id: f_00001-7-2 loss: 0.425249  [   96/  265]
train() client id: f_00001-7-3 loss: 0.395910  [  128/  265]
train() client id: f_00001-7-4 loss: 0.362231  [  160/  265]
train() client id: f_00001-7-5 loss: 0.433662  [  192/  265]
train() client id: f_00001-7-6 loss: 0.398572  [  224/  265]
train() client id: f_00001-7-7 loss: 0.597033  [  256/  265]
train() client id: f_00001-8-0 loss: 0.365226  [   32/  265]
train() client id: f_00001-8-1 loss: 0.386279  [   64/  265]
train() client id: f_00001-8-2 loss: 0.434235  [   96/  265]
train() client id: f_00001-8-3 loss: 0.454186  [  128/  265]
train() client id: f_00001-8-4 loss: 0.564404  [  160/  265]
train() client id: f_00001-8-5 loss: 0.546921  [  192/  265]
train() client id: f_00001-8-6 loss: 0.357716  [  224/  265]
train() client id: f_00001-8-7 loss: 0.443440  [  256/  265]
train() client id: f_00001-9-0 loss: 0.409691  [   32/  265]
train() client id: f_00001-9-1 loss: 0.403242  [   64/  265]
train() client id: f_00001-9-2 loss: 0.440030  [   96/  265]
train() client id: f_00001-9-3 loss: 0.418713  [  128/  265]
train() client id: f_00001-9-4 loss: 0.578764  [  160/  265]
train() client id: f_00001-9-5 loss: 0.376238  [  192/  265]
train() client id: f_00001-9-6 loss: 0.430078  [  224/  265]
train() client id: f_00001-9-7 loss: 0.418309  [  256/  265]
train() client id: f_00001-10-0 loss: 0.469951  [   32/  265]
train() client id: f_00001-10-1 loss: 0.415462  [   64/  265]
train() client id: f_00001-10-2 loss: 0.397199  [   96/  265]
train() client id: f_00001-10-3 loss: 0.506167  [  128/  265]
train() client id: f_00001-10-4 loss: 0.490724  [  160/  265]
train() client id: f_00001-10-5 loss: 0.482889  [  192/  265]
train() client id: f_00001-10-6 loss: 0.420693  [  224/  265]
train() client id: f_00001-10-7 loss: 0.362530  [  256/  265]
train() client id: f_00001-11-0 loss: 0.358377  [   32/  265]
train() client id: f_00001-11-1 loss: 0.566721  [   64/  265]
train() client id: f_00001-11-2 loss: 0.386748  [   96/  265]
train() client id: f_00001-11-3 loss: 0.412000  [  128/  265]
train() client id: f_00001-11-4 loss: 0.478504  [  160/  265]
train() client id: f_00001-11-5 loss: 0.509185  [  192/  265]
train() client id: f_00001-11-6 loss: 0.399118  [  224/  265]
train() client id: f_00001-11-7 loss: 0.446375  [  256/  265]
train() client id: f_00002-0-0 loss: 1.232973  [   32/  124]
train() client id: f_00002-0-1 loss: 1.349622  [   64/  124]
train() client id: f_00002-0-2 loss: 1.145936  [   96/  124]
train() client id: f_00002-1-0 loss: 1.252034  [   32/  124]
train() client id: f_00002-1-1 loss: 1.133838  [   64/  124]
train() client id: f_00002-1-2 loss: 1.235847  [   96/  124]
train() client id: f_00002-2-0 loss: 1.064284  [   32/  124]
train() client id: f_00002-2-1 loss: 1.274064  [   64/  124]
train() client id: f_00002-2-2 loss: 1.176931  [   96/  124]
train() client id: f_00002-3-0 loss: 1.046626  [   32/  124]
train() client id: f_00002-3-1 loss: 1.309415  [   64/  124]
train() client id: f_00002-3-2 loss: 1.126164  [   96/  124]
train() client id: f_00002-4-0 loss: 1.114662  [   32/  124]
train() client id: f_00002-4-1 loss: 1.188515  [   64/  124]
train() client id: f_00002-4-2 loss: 0.997195  [   96/  124]
train() client id: f_00002-5-0 loss: 1.037087  [   32/  124]
train() client id: f_00002-5-1 loss: 1.174899  [   64/  124]
train() client id: f_00002-5-2 loss: 0.957579  [   96/  124]
train() client id: f_00002-6-0 loss: 0.939546  [   32/  124]
train() client id: f_00002-6-1 loss: 1.172645  [   64/  124]
train() client id: f_00002-6-2 loss: 1.081851  [   96/  124]
train() client id: f_00002-7-0 loss: 1.048552  [   32/  124]
train() client id: f_00002-7-1 loss: 1.018117  [   64/  124]
train() client id: f_00002-7-2 loss: 1.078910  [   96/  124]
train() client id: f_00002-8-0 loss: 1.044055  [   32/  124]
train() client id: f_00002-8-1 loss: 1.020438  [   64/  124]
train() client id: f_00002-8-2 loss: 0.998330  [   96/  124]
train() client id: f_00002-9-0 loss: 0.924883  [   32/  124]
train() client id: f_00002-9-1 loss: 1.084433  [   64/  124]
train() client id: f_00002-9-2 loss: 1.060845  [   96/  124]
train() client id: f_00002-10-0 loss: 1.071377  [   32/  124]
train() client id: f_00002-10-1 loss: 0.995641  [   64/  124]
train() client id: f_00002-10-2 loss: 0.992761  [   96/  124]
train() client id: f_00002-11-0 loss: 0.971948  [   32/  124]
train() client id: f_00002-11-1 loss: 1.120965  [   64/  124]
train() client id: f_00002-11-2 loss: 0.937637  [   96/  124]
train() client id: f_00003-0-0 loss: 0.939479  [   32/   43]
train() client id: f_00003-1-0 loss: 0.864298  [   32/   43]
train() client id: f_00003-2-0 loss: 0.779532  [   32/   43]
train() client id: f_00003-3-0 loss: 0.788626  [   32/   43]
train() client id: f_00003-4-0 loss: 0.704314  [   32/   43]
train() client id: f_00003-5-0 loss: 0.634655  [   32/   43]
train() client id: f_00003-6-0 loss: 0.842321  [   32/   43]
train() client id: f_00003-7-0 loss: 0.754363  [   32/   43]
train() client id: f_00003-8-0 loss: 0.936993  [   32/   43]
train() client id: f_00003-9-0 loss: 1.011797  [   32/   43]
train() client id: f_00003-10-0 loss: 0.761330  [   32/   43]
train() client id: f_00003-11-0 loss: 0.903928  [   32/   43]
train() client id: f_00004-0-0 loss: 0.774865  [   32/  306]
train() client id: f_00004-0-1 loss: 0.914911  [   64/  306]
train() client id: f_00004-0-2 loss: 0.944554  [   96/  306]
train() client id: f_00004-0-3 loss: 1.041106  [  128/  306]
train() client id: f_00004-0-4 loss: 0.693856  [  160/  306]
train() client id: f_00004-0-5 loss: 0.913449  [  192/  306]
train() client id: f_00004-0-6 loss: 0.893939  [  224/  306]
train() client id: f_00004-0-7 loss: 0.948679  [  256/  306]
train() client id: f_00004-0-8 loss: 0.874050  [  288/  306]
train() client id: f_00004-1-0 loss: 0.811822  [   32/  306]
train() client id: f_00004-1-1 loss: 0.882393  [   64/  306]
train() client id: f_00004-1-2 loss: 0.868858  [   96/  306]
train() client id: f_00004-1-3 loss: 0.838600  [  128/  306]
train() client id: f_00004-1-4 loss: 1.005458  [  160/  306]
train() client id: f_00004-1-5 loss: 0.921092  [  192/  306]
train() client id: f_00004-1-6 loss: 0.923466  [  224/  306]
train() client id: f_00004-1-7 loss: 0.906200  [  256/  306]
train() client id: f_00004-1-8 loss: 0.900370  [  288/  306]
train() client id: f_00004-2-0 loss: 0.780551  [   32/  306]
train() client id: f_00004-2-1 loss: 0.913146  [   64/  306]
train() client id: f_00004-2-2 loss: 0.863516  [   96/  306]
train() client id: f_00004-2-3 loss: 1.127415  [  128/  306]
train() client id: f_00004-2-4 loss: 0.835806  [  160/  306]
train() client id: f_00004-2-5 loss: 0.916095  [  192/  306]
train() client id: f_00004-2-6 loss: 0.943756  [  224/  306]
train() client id: f_00004-2-7 loss: 0.726816  [  256/  306]
train() client id: f_00004-2-8 loss: 0.816493  [  288/  306]
train() client id: f_00004-3-0 loss: 0.808634  [   32/  306]
train() client id: f_00004-3-1 loss: 0.879395  [   64/  306]
train() client id: f_00004-3-2 loss: 0.920315  [   96/  306]
train() client id: f_00004-3-3 loss: 0.876079  [  128/  306]
train() client id: f_00004-3-4 loss: 0.974774  [  160/  306]
train() client id: f_00004-3-5 loss: 0.897871  [  192/  306]
train() client id: f_00004-3-6 loss: 0.823173  [  224/  306]
train() client id: f_00004-3-7 loss: 0.951391  [  256/  306]
train() client id: f_00004-3-8 loss: 0.885758  [  288/  306]
train() client id: f_00004-4-0 loss: 0.934173  [   32/  306]
train() client id: f_00004-4-1 loss: 0.912911  [   64/  306]
train() client id: f_00004-4-2 loss: 0.893093  [   96/  306]
train() client id: f_00004-4-3 loss: 0.895052  [  128/  306]
train() client id: f_00004-4-4 loss: 0.860513  [  160/  306]
train() client id: f_00004-4-5 loss: 0.800634  [  192/  306]
train() client id: f_00004-4-6 loss: 0.889652  [  224/  306]
train() client id: f_00004-4-7 loss: 0.837631  [  256/  306]
train() client id: f_00004-4-8 loss: 0.913635  [  288/  306]
train() client id: f_00004-5-0 loss: 0.889989  [   32/  306]
train() client id: f_00004-5-1 loss: 0.979563  [   64/  306]
train() client id: f_00004-5-2 loss: 1.020371  [   96/  306]
train() client id: f_00004-5-3 loss: 0.851782  [  128/  306]
train() client id: f_00004-5-4 loss: 0.867642  [  160/  306]
train() client id: f_00004-5-5 loss: 0.810919  [  192/  306]
train() client id: f_00004-5-6 loss: 0.915085  [  224/  306]
train() client id: f_00004-5-7 loss: 0.827896  [  256/  306]
train() client id: f_00004-5-8 loss: 0.862111  [  288/  306]
train() client id: f_00004-6-0 loss: 0.886889  [   32/  306]
train() client id: f_00004-6-1 loss: 0.934000  [   64/  306]
train() client id: f_00004-6-2 loss: 1.047206  [   96/  306]
train() client id: f_00004-6-3 loss: 0.870454  [  128/  306]
train() client id: f_00004-6-4 loss: 0.837744  [  160/  306]
train() client id: f_00004-6-5 loss: 0.901758  [  192/  306]
train() client id: f_00004-6-6 loss: 0.867008  [  224/  306]
train() client id: f_00004-6-7 loss: 0.706396  [  256/  306]
train() client id: f_00004-6-8 loss: 0.941589  [  288/  306]
train() client id: f_00004-7-0 loss: 0.950124  [   32/  306]
train() client id: f_00004-7-1 loss: 0.764766  [   64/  306]
train() client id: f_00004-7-2 loss: 0.902513  [   96/  306]
train() client id: f_00004-7-3 loss: 0.873007  [  128/  306]
train() client id: f_00004-7-4 loss: 0.767956  [  160/  306]
train() client id: f_00004-7-5 loss: 0.850095  [  192/  306]
train() client id: f_00004-7-6 loss: 1.165897  [  224/  306]
train() client id: f_00004-7-7 loss: 0.883436  [  256/  306]
train() client id: f_00004-7-8 loss: 0.858890  [  288/  306]
train() client id: f_00004-8-0 loss: 0.909767  [   32/  306]
train() client id: f_00004-8-1 loss: 0.794496  [   64/  306]
train() client id: f_00004-8-2 loss: 0.871605  [   96/  306]
train() client id: f_00004-8-3 loss: 0.949211  [  128/  306]
train() client id: f_00004-8-4 loss: 0.907589  [  160/  306]
train() client id: f_00004-8-5 loss: 0.904609  [  192/  306]
train() client id: f_00004-8-6 loss: 0.800302  [  224/  306]
train() client id: f_00004-8-7 loss: 0.944463  [  256/  306]
train() client id: f_00004-8-8 loss: 0.946198  [  288/  306]
train() client id: f_00004-9-0 loss: 0.872468  [   32/  306]
train() client id: f_00004-9-1 loss: 0.985233  [   64/  306]
train() client id: f_00004-9-2 loss: 0.894978  [   96/  306]
train() client id: f_00004-9-3 loss: 0.862130  [  128/  306]
train() client id: f_00004-9-4 loss: 0.848867  [  160/  306]
train() client id: f_00004-9-5 loss: 0.786845  [  192/  306]
train() client id: f_00004-9-6 loss: 0.929438  [  224/  306]
train() client id: f_00004-9-7 loss: 0.801498  [  256/  306]
train() client id: f_00004-9-8 loss: 0.901874  [  288/  306]
train() client id: f_00004-10-0 loss: 0.999014  [   32/  306]
train() client id: f_00004-10-1 loss: 0.887429  [   64/  306]
train() client id: f_00004-10-2 loss: 0.766683  [   96/  306]
train() client id: f_00004-10-3 loss: 0.874851  [  128/  306]
train() client id: f_00004-10-4 loss: 0.892349  [  160/  306]
train() client id: f_00004-10-5 loss: 0.765925  [  192/  306]
train() client id: f_00004-10-6 loss: 0.936343  [  224/  306]
train() client id: f_00004-10-7 loss: 0.868010  [  256/  306]
train() client id: f_00004-10-8 loss: 0.943496  [  288/  306]
train() client id: f_00004-11-0 loss: 0.868490  [   32/  306]
train() client id: f_00004-11-1 loss: 0.976189  [   64/  306]
train() client id: f_00004-11-2 loss: 0.918931  [   96/  306]
train() client id: f_00004-11-3 loss: 0.838859  [  128/  306]
train() client id: f_00004-11-4 loss: 0.772268  [  160/  306]
train() client id: f_00004-11-5 loss: 0.777346  [  192/  306]
train() client id: f_00004-11-6 loss: 0.930871  [  224/  306]
train() client id: f_00004-11-7 loss: 0.945981  [  256/  306]
train() client id: f_00004-11-8 loss: 0.891767  [  288/  306]
train() client id: f_00005-0-0 loss: 0.634384  [   32/  146]
train() client id: f_00005-0-1 loss: 0.723073  [   64/  146]
train() client id: f_00005-0-2 loss: 0.669523  [   96/  146]
train() client id: f_00005-0-3 loss: 0.850228  [  128/  146]
train() client id: f_00005-1-0 loss: 0.647430  [   32/  146]
train() client id: f_00005-1-1 loss: 0.620529  [   64/  146]
train() client id: f_00005-1-2 loss: 0.904694  [   96/  146]
train() client id: f_00005-1-3 loss: 0.612840  [  128/  146]
train() client id: f_00005-2-0 loss: 0.721627  [   32/  146]
train() client id: f_00005-2-1 loss: 0.572245  [   64/  146]
train() client id: f_00005-2-2 loss: 0.977013  [   96/  146]
train() client id: f_00005-2-3 loss: 0.590993  [  128/  146]
train() client id: f_00005-3-0 loss: 0.750603  [   32/  146]
train() client id: f_00005-3-1 loss: 0.655748  [   64/  146]
train() client id: f_00005-3-2 loss: 0.717689  [   96/  146]
train() client id: f_00005-3-3 loss: 0.687860  [  128/  146]
train() client id: f_00005-4-0 loss: 0.564571  [   32/  146]
train() client id: f_00005-4-1 loss: 0.678687  [   64/  146]
train() client id: f_00005-4-2 loss: 0.785834  [   96/  146]
train() client id: f_00005-4-3 loss: 0.653852  [  128/  146]
train() client id: f_00005-5-0 loss: 1.029800  [   32/  146]
train() client id: f_00005-5-1 loss: 0.655293  [   64/  146]
train() client id: f_00005-5-2 loss: 0.372909  [   96/  146]
train() client id: f_00005-5-3 loss: 0.597938  [  128/  146]
train() client id: f_00005-6-0 loss: 0.722791  [   32/  146]
train() client id: f_00005-6-1 loss: 0.480951  [   64/  146]
train() client id: f_00005-6-2 loss: 0.612667  [   96/  146]
train() client id: f_00005-6-3 loss: 0.928182  [  128/  146]
train() client id: f_00005-7-0 loss: 0.677546  [   32/  146]
train() client id: f_00005-7-1 loss: 0.657796  [   64/  146]
train() client id: f_00005-7-2 loss: 0.501826  [   96/  146]
train() client id: f_00005-7-3 loss: 0.835881  [  128/  146]
train() client id: f_00005-8-0 loss: 0.640652  [   32/  146]
train() client id: f_00005-8-1 loss: 0.768331  [   64/  146]
train() client id: f_00005-8-2 loss: 0.630079  [   96/  146]
train() client id: f_00005-8-3 loss: 0.597172  [  128/  146]
train() client id: f_00005-9-0 loss: 0.543908  [   32/  146]
train() client id: f_00005-9-1 loss: 0.629815  [   64/  146]
train() client id: f_00005-9-2 loss: 0.687786  [   96/  146]
train() client id: f_00005-9-3 loss: 0.860956  [  128/  146]
train() client id: f_00005-10-0 loss: 0.492858  [   32/  146]
train() client id: f_00005-10-1 loss: 0.861192  [   64/  146]
train() client id: f_00005-10-2 loss: 0.531792  [   96/  146]
train() client id: f_00005-10-3 loss: 0.807439  [  128/  146]
train() client id: f_00005-11-0 loss: 0.532171  [   32/  146]
train() client id: f_00005-11-1 loss: 0.728928  [   64/  146]
train() client id: f_00005-11-2 loss: 0.720042  [   96/  146]
train() client id: f_00005-11-3 loss: 0.778090  [  128/  146]
train() client id: f_00006-0-0 loss: 0.560033  [   32/   54]
train() client id: f_00006-1-0 loss: 0.554307  [   32/   54]
train() client id: f_00006-2-0 loss: 0.453157  [   32/   54]
train() client id: f_00006-3-0 loss: 0.534076  [   32/   54]
train() client id: f_00006-4-0 loss: 0.557229  [   32/   54]
train() client id: f_00006-5-0 loss: 0.467956  [   32/   54]
train() client id: f_00006-6-0 loss: 0.458368  [   32/   54]
train() client id: f_00006-7-0 loss: 0.498870  [   32/   54]
train() client id: f_00006-8-0 loss: 0.505666  [   32/   54]
train() client id: f_00006-9-0 loss: 0.551920  [   32/   54]
train() client id: f_00006-10-0 loss: 0.558810  [   32/   54]
train() client id: f_00006-11-0 loss: 0.512077  [   32/   54]
train() client id: f_00007-0-0 loss: 0.588821  [   32/  179]
train() client id: f_00007-0-1 loss: 0.790519  [   64/  179]
train() client id: f_00007-0-2 loss: 0.436594  [   96/  179]
train() client id: f_00007-0-3 loss: 0.516727  [  128/  179]
train() client id: f_00007-0-4 loss: 0.586882  [  160/  179]
train() client id: f_00007-1-0 loss: 0.543721  [   32/  179]
train() client id: f_00007-1-1 loss: 0.452588  [   64/  179]
train() client id: f_00007-1-2 loss: 0.496656  [   96/  179]
train() client id: f_00007-1-3 loss: 0.606514  [  128/  179]
train() client id: f_00007-1-4 loss: 0.747444  [  160/  179]
train() client id: f_00007-2-0 loss: 0.712961  [   32/  179]
train() client id: f_00007-2-1 loss: 0.501770  [   64/  179]
train() client id: f_00007-2-2 loss: 0.479809  [   96/  179]
train() client id: f_00007-2-3 loss: 0.499098  [  128/  179]
train() client id: f_00007-2-4 loss: 0.585813  [  160/  179]
train() client id: f_00007-3-0 loss: 0.689057  [   32/  179]
train() client id: f_00007-3-1 loss: 0.737805  [   64/  179]
train() client id: f_00007-3-2 loss: 0.435561  [   96/  179]
train() client id: f_00007-3-3 loss: 0.526320  [  128/  179]
train() client id: f_00007-3-4 loss: 0.391615  [  160/  179]
train() client id: f_00007-4-0 loss: 0.482048  [   32/  179]
train() client id: f_00007-4-1 loss: 0.600641  [   64/  179]
train() client id: f_00007-4-2 loss: 0.497438  [   96/  179]
train() client id: f_00007-4-3 loss: 0.614426  [  128/  179]
train() client id: f_00007-4-4 loss: 0.561176  [  160/  179]
train() client id: f_00007-5-0 loss: 0.506220  [   32/  179]
train() client id: f_00007-5-1 loss: 0.400033  [   64/  179]
train() client id: f_00007-5-2 loss: 0.501248  [   96/  179]
train() client id: f_00007-5-3 loss: 0.853971  [  128/  179]
train() client id: f_00007-5-4 loss: 0.521820  [  160/  179]
train() client id: f_00007-6-0 loss: 0.395817  [   32/  179]
train() client id: f_00007-6-1 loss: 0.654603  [   64/  179]
train() client id: f_00007-6-2 loss: 0.816637  [   96/  179]
train() client id: f_00007-6-3 loss: 0.476856  [  128/  179]
train() client id: f_00007-6-4 loss: 0.417541  [  160/  179]
train() client id: f_00007-7-0 loss: 0.550177  [   32/  179]
train() client id: f_00007-7-1 loss: 0.401589  [   64/  179]
train() client id: f_00007-7-2 loss: 0.503059  [   96/  179]
train() client id: f_00007-7-3 loss: 0.609155  [  128/  179]
train() client id: f_00007-7-4 loss: 0.480092  [  160/  179]
train() client id: f_00007-8-0 loss: 0.523763  [   32/  179]
train() client id: f_00007-8-1 loss: 0.468002  [   64/  179]
train() client id: f_00007-8-2 loss: 0.732956  [   96/  179]
train() client id: f_00007-8-3 loss: 0.564061  [  128/  179]
train() client id: f_00007-8-4 loss: 0.413070  [  160/  179]
train() client id: f_00007-9-0 loss: 0.399789  [   32/  179]
train() client id: f_00007-9-1 loss: 0.904051  [   64/  179]
train() client id: f_00007-9-2 loss: 0.564885  [   96/  179]
train() client id: f_00007-9-3 loss: 0.414591  [  128/  179]
train() client id: f_00007-9-4 loss: 0.381566  [  160/  179]
train() client id: f_00007-10-0 loss: 0.422848  [   32/  179]
train() client id: f_00007-10-1 loss: 0.776753  [   64/  179]
train() client id: f_00007-10-2 loss: 0.480958  [   96/  179]
train() client id: f_00007-10-3 loss: 0.399116  [  128/  179]
train() client id: f_00007-10-4 loss: 0.605158  [  160/  179]
train() client id: f_00007-11-0 loss: 0.590766  [   32/  179]
train() client id: f_00007-11-1 loss: 0.391813  [   64/  179]
train() client id: f_00007-11-2 loss: 0.817995  [   96/  179]
train() client id: f_00007-11-3 loss: 0.465127  [  128/  179]
train() client id: f_00007-11-4 loss: 0.492191  [  160/  179]
train() client id: f_00008-0-0 loss: 0.651815  [   32/  130]
train() client id: f_00008-0-1 loss: 0.819163  [   64/  130]
train() client id: f_00008-0-2 loss: 0.790564  [   96/  130]
train() client id: f_00008-0-3 loss: 0.742529  [  128/  130]
train() client id: f_00008-1-0 loss: 0.742361  [   32/  130]
train() client id: f_00008-1-1 loss: 0.696835  [   64/  130]
train() client id: f_00008-1-2 loss: 0.799088  [   96/  130]
train() client id: f_00008-1-3 loss: 0.795148  [  128/  130]
train() client id: f_00008-2-0 loss: 0.704238  [   32/  130]
train() client id: f_00008-2-1 loss: 0.719745  [   64/  130]
train() client id: f_00008-2-2 loss: 0.850856  [   96/  130]
train() client id: f_00008-2-3 loss: 0.750959  [  128/  130]
train() client id: f_00008-3-0 loss: 0.750777  [   32/  130]
train() client id: f_00008-3-1 loss: 0.830738  [   64/  130]
train() client id: f_00008-3-2 loss: 0.838631  [   96/  130]
train() client id: f_00008-3-3 loss: 0.604825  [  128/  130]
train() client id: f_00008-4-0 loss: 0.760329  [   32/  130]
train() client id: f_00008-4-1 loss: 0.716225  [   64/  130]
train() client id: f_00008-4-2 loss: 0.851361  [   96/  130]
train() client id: f_00008-4-3 loss: 0.697794  [  128/  130]
train() client id: f_00008-5-0 loss: 0.821196  [   32/  130]
train() client id: f_00008-5-1 loss: 0.754835  [   64/  130]
train() client id: f_00008-5-2 loss: 0.691088  [   96/  130]
train() client id: f_00008-5-3 loss: 0.752512  [  128/  130]
train() client id: f_00008-6-0 loss: 0.721495  [   32/  130]
train() client id: f_00008-6-1 loss: 0.991889  [   64/  130]
train() client id: f_00008-6-2 loss: 0.636502  [   96/  130]
train() client id: f_00008-6-3 loss: 0.649068  [  128/  130]
train() client id: f_00008-7-0 loss: 0.723878  [   32/  130]
train() client id: f_00008-7-1 loss: 0.723172  [   64/  130]
train() client id: f_00008-7-2 loss: 0.768537  [   96/  130]
train() client id: f_00008-7-3 loss: 0.799026  [  128/  130]
train() client id: f_00008-8-0 loss: 0.702417  [   32/  130]
train() client id: f_00008-8-1 loss: 0.770270  [   64/  130]
train() client id: f_00008-8-2 loss: 0.791841  [   96/  130]
train() client id: f_00008-8-3 loss: 0.744454  [  128/  130]
train() client id: f_00008-9-0 loss: 0.717754  [   32/  130]
train() client id: f_00008-9-1 loss: 0.702500  [   64/  130]
train() client id: f_00008-9-2 loss: 0.823474  [   96/  130]
train() client id: f_00008-9-3 loss: 0.778721  [  128/  130]
train() client id: f_00008-10-0 loss: 0.855398  [   32/  130]
train() client id: f_00008-10-1 loss: 0.703743  [   64/  130]
train() client id: f_00008-10-2 loss: 0.819084  [   96/  130]
train() client id: f_00008-10-3 loss: 0.624885  [  128/  130]
train() client id: f_00008-11-0 loss: 0.846227  [   32/  130]
train() client id: f_00008-11-1 loss: 0.707647  [   64/  130]
train() client id: f_00008-11-2 loss: 0.694711  [   96/  130]
train() client id: f_00008-11-3 loss: 0.776016  [  128/  130]
train() client id: f_00009-0-0 loss: 0.909073  [   32/  118]
train() client id: f_00009-0-1 loss: 1.099111  [   64/  118]
train() client id: f_00009-0-2 loss: 1.053384  [   96/  118]
train() client id: f_00009-1-0 loss: 0.926760  [   32/  118]
train() client id: f_00009-1-1 loss: 1.065579  [   64/  118]
train() client id: f_00009-1-2 loss: 0.933367  [   96/  118]
train() client id: f_00009-2-0 loss: 0.917882  [   32/  118]
train() client id: f_00009-2-1 loss: 0.842762  [   64/  118]
train() client id: f_00009-2-2 loss: 0.933574  [   96/  118]
train() client id: f_00009-3-0 loss: 0.984416  [   32/  118]
train() client id: f_00009-3-1 loss: 1.017924  [   64/  118]
train() client id: f_00009-3-2 loss: 0.803219  [   96/  118]
train() client id: f_00009-4-0 loss: 0.907121  [   32/  118]
train() client id: f_00009-4-1 loss: 0.770815  [   64/  118]
train() client id: f_00009-4-2 loss: 1.052733  [   96/  118]
train() client id: f_00009-5-0 loss: 0.925812  [   32/  118]
train() client id: f_00009-5-1 loss: 0.687963  [   64/  118]
train() client id: f_00009-5-2 loss: 0.966891  [   96/  118]
train() client id: f_00009-6-0 loss: 0.885182  [   32/  118]
train() client id: f_00009-6-1 loss: 0.870134  [   64/  118]
train() client id: f_00009-6-2 loss: 0.751462  [   96/  118]
train() client id: f_00009-7-0 loss: 0.717913  [   32/  118]
train() client id: f_00009-7-1 loss: 0.897003  [   64/  118]
train() client id: f_00009-7-2 loss: 0.930832  [   96/  118]
train() client id: f_00009-8-0 loss: 0.855546  [   32/  118]
train() client id: f_00009-8-1 loss: 0.921132  [   64/  118]
train() client id: f_00009-8-2 loss: 0.642449  [   96/  118]
train() client id: f_00009-9-0 loss: 0.813263  [   32/  118]
train() client id: f_00009-9-1 loss: 0.834753  [   64/  118]
train() client id: f_00009-9-2 loss: 0.760848  [   96/  118]
train() client id: f_00009-10-0 loss: 0.780539  [   32/  118]
train() client id: f_00009-10-1 loss: 0.782039  [   64/  118]
train() client id: f_00009-10-2 loss: 0.833853  [   96/  118]
train() client id: f_00009-11-0 loss: 0.740013  [   32/  118]
train() client id: f_00009-11-1 loss: 0.826295  [   64/  118]
train() client id: f_00009-11-2 loss: 0.717388  [   96/  118]
At round 25 accuracy: 0.6419098143236074
At round 25 training accuracy: 0.5814889336016097
At round 25 training loss: 0.8362403916736797
update_location
xs = [  -3.9056584     4.20031788  145.00902392   18.81129433    0.97929623
    3.95640986 -107.44319194  -86.32485185  129.66397685  -72.06087855]
ys = [ 137.5879595   120.55583871    1.32061395 -107.45517586   99.35018685
   82.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [170.13436092 156.68871343 176.15152863 147.98810632 140.96601948
 129.89933311 146.80235024 132.10850201 164.68581258 123.32389093]
dists_bs = [175.13690217 187.28833553 364.10205235 342.55473166 191.55053362
 201.30481225 190.03830501 195.4511099  342.93535346 199.59197368]
uav_gains = [2.63464755e-11 3.24787964e-11 2.40854862e-11 3.75018239e-11
 4.23658097e-11 5.19897705e-11 3.82670974e-11 4.98415206e-11
 2.86308105e-11 5.92039294e-11]
bs_gains = [5.77878620e-11 4.78921177e-11 7.44507933e-12 8.83181915e-12
 4.49676985e-11 3.91293331e-11 4.59768137e-11 4.24998396e-11
 8.80439989e-12 4.00768410e-11]
Round 26
-------------------------------
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.5605131  15.70320748  7.4465293   2.67409633 18.11274305  8.72126419
  3.31940453 10.65181908  7.8502879   7.07624586]
obj_prev = 89.11611082472227
eta_min = 5.392428774203226e-13	eta_max = 0.9242899713750001
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 20.70487256941968	eta = 0.909090909090909
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 36.71367954165409	eta = 0.5126865969778287
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 28.972791112910695	eta = 0.6496651065957375
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.57869875098564	eta = 0.6825054219090905
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.5077842638857	eta = 0.6842649064780151
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.50758684227239	eta = 0.6842698174388544
eta = 0.6842698174388544
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [0.03130742 0.06584498 0.03081048 0.01068428 0.07603231 0.03627686
 0.01341747 0.04447642 0.03230131 0.02931965]
ene_total = [2.4173172  4.46359599 2.39783696 1.11945579 5.09142412 2.67769452
 1.2842835  3.15534052 2.65007964 2.25055859]
ti_comp = [0.3872178  0.39743138 0.38543905 0.39356432 0.39647217 0.39426467
 0.39389891 0.39802621 0.35903228 0.39465362]
ti_coms = [0.08286047 0.07264689 0.08463921 0.07651394 0.0736061  0.07581359
 0.07617935 0.07205206 0.11104599 0.07542464]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.27912028e-05 1.12959740e-04 1.23045006e-05 4.92135047e-07
 1.74762896e-04 1.91952252e-05 9.73020721e-07 3.47093215e-05
 1.63408045e-05 1.01140293e-05]
ene_total = [0.50667253 0.45043075 0.51750266 0.46717392 0.46006034 0.46403994
 0.4651605  0.44202165 0.67897151 0.46111082]
optimize_network_iter = 0 obj = 4.913144626268169
eta = 0.6842698174388544
freqs = [40426116.40412594 82838172.3252855  39968031.32913876 13573739.87970061
 95886067.29898961 46005714.64645749 17031612.51169543 55871219.40930291
 44983852.28187121 37146055.45423073]
eta_min = 0.6842698174388597	eta_max = 0.6842698174388548
af = 0.020050708162629324	bf = 1.5436207515894453	zeta = 0.022055778978892257	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [3.21329018e-06 2.83767235e-05 3.09102528e-06 1.23629712e-07
 4.39023530e-05 4.82205074e-06 2.44433458e-07 8.71936159e-06
 4.10498900e-06 2.54075489e-06]
ene_total = [1.757659   1.54642537 1.79534967 1.62243185 1.57005655 1.60857787
 1.61536282 1.52964443 2.35549529 1.59984681]
ti_comp = [0.3872178  0.39743138 0.38543905 0.39356432 0.39647217 0.39426467
 0.39389891 0.39802621 0.35903228 0.39465362]
ti_coms = [0.08286047 0.07264689 0.08463921 0.07651394 0.0736061  0.07581359
 0.07617935 0.07205206 0.11104599 0.07542464]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.27912028e-05 1.12959740e-04 1.23045006e-05 4.92135047e-07
 1.74762896e-04 1.91952252e-05 9.73020721e-07 3.47093215e-05
 1.63408045e-05 1.01140293e-05]
ene_total = [0.50667253 0.45043075 0.51750266 0.46717392 0.46006034 0.46403994
 0.4651605  0.44202165 0.67897151 0.46111082]
optimize_network_iter = 1 obj = 4.913144626268177
eta = 0.6842698174388548
freqs = [40426116.40412594 82838172.32528551 39968031.32913877 13573739.87970061
 95886067.29898961 46005714.64645751 17031612.51169543 55871219.40930291
 44983852.28187121 37146055.45423073]
Done!
At round 26 eta: 0.6842698174388548
At round 26 local rounds: 12.423580809938418
At round 26 global rounds: 61.05343074483861
At round 26 a_n: 18.93386498808281
gradient difference: 0.4808921217918396
train() client id: f_00000-0-0 loss: 1.328978  [   32/  126]
train() client id: f_00000-0-1 loss: 1.186460  [   64/  126]
train() client id: f_00000-0-2 loss: 1.058958  [   96/  126]
train() client id: f_00000-1-0 loss: 1.046386  [   32/  126]
train() client id: f_00000-1-1 loss: 1.093942  [   64/  126]
train() client id: f_00000-1-2 loss: 1.075644  [   96/  126]
train() client id: f_00000-2-0 loss: 0.952791  [   32/  126]
train() client id: f_00000-2-1 loss: 1.149628  [   64/  126]
train() client id: f_00000-2-2 loss: 1.025699  [   96/  126]
train() client id: f_00000-3-0 loss: 0.987729  [   32/  126]
train() client id: f_00000-3-1 loss: 1.168042  [   64/  126]
train() client id: f_00000-3-2 loss: 0.933386  [   96/  126]
train() client id: f_00000-4-0 loss: 1.038501  [   32/  126]
train() client id: f_00000-4-1 loss: 0.851656  [   64/  126]
train() client id: f_00000-4-2 loss: 0.950793  [   96/  126]
train() client id: f_00000-5-0 loss: 1.000343  [   32/  126]
train() client id: f_00000-5-1 loss: 0.866141  [   64/  126]
train() client id: f_00000-5-2 loss: 0.896108  [   96/  126]
train() client id: f_00000-6-0 loss: 0.957899  [   32/  126]
train() client id: f_00000-6-1 loss: 0.962456  [   64/  126]
train() client id: f_00000-6-2 loss: 0.850289  [   96/  126]
train() client id: f_00000-7-0 loss: 0.914163  [   32/  126]
train() client id: f_00000-7-1 loss: 0.980298  [   64/  126]
train() client id: f_00000-7-2 loss: 0.828839  [   96/  126]
train() client id: f_00000-8-0 loss: 0.895247  [   32/  126]
train() client id: f_00000-8-1 loss: 0.865516  [   64/  126]
train() client id: f_00000-8-2 loss: 0.960482  [   96/  126]
train() client id: f_00000-9-0 loss: 0.865013  [   32/  126]
train() client id: f_00000-9-1 loss: 0.893705  [   64/  126]
train() client id: f_00000-9-2 loss: 0.938660  [   96/  126]
train() client id: f_00000-10-0 loss: 0.899125  [   32/  126]
train() client id: f_00000-10-1 loss: 0.931963  [   64/  126]
train() client id: f_00000-10-2 loss: 0.901736  [   96/  126]
train() client id: f_00000-11-0 loss: 1.015188  [   32/  126]
train() client id: f_00000-11-1 loss: 0.876911  [   64/  126]
train() client id: f_00000-11-2 loss: 0.805235  [   96/  126]
train() client id: f_00001-0-0 loss: 0.391918  [   32/  265]
train() client id: f_00001-0-1 loss: 0.435975  [   64/  265]
train() client id: f_00001-0-2 loss: 0.479186  [   96/  265]
train() client id: f_00001-0-3 loss: 0.492223  [  128/  265]
train() client id: f_00001-0-4 loss: 0.552620  [  160/  265]
train() client id: f_00001-0-5 loss: 0.431065  [  192/  265]
train() client id: f_00001-0-6 loss: 0.396938  [  224/  265]
train() client id: f_00001-0-7 loss: 0.534284  [  256/  265]
train() client id: f_00001-1-0 loss: 0.462664  [   32/  265]
train() client id: f_00001-1-1 loss: 0.491441  [   64/  265]
train() client id: f_00001-1-2 loss: 0.470777  [   96/  265]
train() client id: f_00001-1-3 loss: 0.438895  [  128/  265]
train() client id: f_00001-1-4 loss: 0.363378  [  160/  265]
train() client id: f_00001-1-5 loss: 0.530833  [  192/  265]
train() client id: f_00001-1-6 loss: 0.425011  [  224/  265]
train() client id: f_00001-1-7 loss: 0.462734  [  256/  265]
train() client id: f_00001-2-0 loss: 0.388783  [   32/  265]
train() client id: f_00001-2-1 loss: 0.602564  [   64/  265]
train() client id: f_00001-2-2 loss: 0.430125  [   96/  265]
train() client id: f_00001-2-3 loss: 0.540233  [  128/  265]
train() client id: f_00001-2-4 loss: 0.451100  [  160/  265]
train() client id: f_00001-2-5 loss: 0.405575  [  192/  265]
train() client id: f_00001-2-6 loss: 0.414058  [  224/  265]
train() client id: f_00001-2-7 loss: 0.358649  [  256/  265]
train() client id: f_00001-3-0 loss: 0.432225  [   32/  265]
train() client id: f_00001-3-1 loss: 0.505390  [   64/  265]
train() client id: f_00001-3-2 loss: 0.366498  [   96/  265]
train() client id: f_00001-3-3 loss: 0.592164  [  128/  265]
train() client id: f_00001-3-4 loss: 0.336660  [  160/  265]
train() client id: f_00001-3-5 loss: 0.464479  [  192/  265]
train() client id: f_00001-3-6 loss: 0.427469  [  224/  265]
train() client id: f_00001-3-7 loss: 0.438923  [  256/  265]
train() client id: f_00001-4-0 loss: 0.443514  [   32/  265]
train() client id: f_00001-4-1 loss: 0.458873  [   64/  265]
train() client id: f_00001-4-2 loss: 0.589196  [   96/  265]
train() client id: f_00001-4-3 loss: 0.397098  [  128/  265]
train() client id: f_00001-4-4 loss: 0.385503  [  160/  265]
train() client id: f_00001-4-5 loss: 0.364153  [  192/  265]
train() client id: f_00001-4-6 loss: 0.388776  [  224/  265]
train() client id: f_00001-4-7 loss: 0.499346  [  256/  265]
train() client id: f_00001-5-0 loss: 0.357086  [   32/  265]
train() client id: f_00001-5-1 loss: 0.355272  [   64/  265]
train() client id: f_00001-5-2 loss: 0.524667  [   96/  265]
train() client id: f_00001-5-3 loss: 0.513061  [  128/  265]
train() client id: f_00001-5-4 loss: 0.383353  [  160/  265]
train() client id: f_00001-5-5 loss: 0.549721  [  192/  265]
train() client id: f_00001-5-6 loss: 0.390526  [  224/  265]
train() client id: f_00001-5-7 loss: 0.445774  [  256/  265]
train() client id: f_00001-6-0 loss: 0.415721  [   32/  265]
train() client id: f_00001-6-1 loss: 0.376247  [   64/  265]
train() client id: f_00001-6-2 loss: 0.463228  [   96/  265]
train() client id: f_00001-6-3 loss: 0.463121  [  128/  265]
train() client id: f_00001-6-4 loss: 0.428459  [  160/  265]
train() client id: f_00001-6-5 loss: 0.427941  [  192/  265]
train() client id: f_00001-6-6 loss: 0.450165  [  224/  265]
train() client id: f_00001-6-7 loss: 0.403000  [  256/  265]
train() client id: f_00001-7-0 loss: 0.345216  [   32/  265]
train() client id: f_00001-7-1 loss: 0.370592  [   64/  265]
train() client id: f_00001-7-2 loss: 0.375369  [   96/  265]
train() client id: f_00001-7-3 loss: 0.480657  [  128/  265]
train() client id: f_00001-7-4 loss: 0.421045  [  160/  265]
train() client id: f_00001-7-5 loss: 0.439483  [  192/  265]
train() client id: f_00001-7-6 loss: 0.628591  [  224/  265]
train() client id: f_00001-7-7 loss: 0.417466  [  256/  265]
train() client id: f_00001-8-0 loss: 0.348471  [   32/  265]
train() client id: f_00001-8-1 loss: 0.595299  [   64/  265]
train() client id: f_00001-8-2 loss: 0.413718  [   96/  265]
train() client id: f_00001-8-3 loss: 0.346067  [  128/  265]
train() client id: f_00001-8-4 loss: 0.338793  [  160/  265]
train() client id: f_00001-8-5 loss: 0.467699  [  192/  265]
train() client id: f_00001-8-6 loss: 0.435998  [  224/  265]
train() client id: f_00001-8-7 loss: 0.449273  [  256/  265]
train() client id: f_00001-9-0 loss: 0.427109  [   32/  265]
train() client id: f_00001-9-1 loss: 0.614222  [   64/  265]
train() client id: f_00001-9-2 loss: 0.438324  [   96/  265]
train() client id: f_00001-9-3 loss: 0.489763  [  128/  265]
train() client id: f_00001-9-4 loss: 0.307121  [  160/  265]
train() client id: f_00001-9-5 loss: 0.404916  [  192/  265]
train() client id: f_00001-9-6 loss: 0.371612  [  224/  265]
train() client id: f_00001-9-7 loss: 0.400802  [  256/  265]
train() client id: f_00001-10-0 loss: 0.434907  [   32/  265]
train() client id: f_00001-10-1 loss: 0.435762  [   64/  265]
train() client id: f_00001-10-2 loss: 0.433620  [   96/  265]
train() client id: f_00001-10-3 loss: 0.391935  [  128/  265]
train() client id: f_00001-10-4 loss: 0.393953  [  160/  265]
train() client id: f_00001-10-5 loss: 0.500415  [  192/  265]
train() client id: f_00001-10-6 loss: 0.422298  [  224/  265]
train() client id: f_00001-10-7 loss: 0.345782  [  256/  265]
train() client id: f_00001-11-0 loss: 0.450540  [   32/  265]
train() client id: f_00001-11-1 loss: 0.645409  [   64/  265]
train() client id: f_00001-11-2 loss: 0.345702  [   96/  265]
train() client id: f_00001-11-3 loss: 0.596963  [  128/  265]
train() client id: f_00001-11-4 loss: 0.348671  [  160/  265]
train() client id: f_00001-11-5 loss: 0.359327  [  192/  265]
train() client id: f_00001-11-6 loss: 0.305863  [  224/  265]
train() client id: f_00001-11-7 loss: 0.357926  [  256/  265]
train() client id: f_00002-0-0 loss: 1.259233  [   32/  124]
train() client id: f_00002-0-1 loss: 1.286729  [   64/  124]
train() client id: f_00002-0-2 loss: 1.216652  [   96/  124]
train() client id: f_00002-1-0 loss: 1.208043  [   32/  124]
train() client id: f_00002-1-1 loss: 1.168605  [   64/  124]
train() client id: f_00002-1-2 loss: 1.164715  [   96/  124]
train() client id: f_00002-2-0 loss: 1.177740  [   32/  124]
train() client id: f_00002-2-1 loss: 1.224132  [   64/  124]
train() client id: f_00002-2-2 loss: 1.245572  [   96/  124]
train() client id: f_00002-3-0 loss: 1.223925  [   32/  124]
train() client id: f_00002-3-1 loss: 1.024205  [   64/  124]
train() client id: f_00002-3-2 loss: 1.181519  [   96/  124]
train() client id: f_00002-4-0 loss: 1.208339  [   32/  124]
train() client id: f_00002-4-1 loss: 1.065642  [   64/  124]
train() client id: f_00002-4-2 loss: 1.101121  [   96/  124]
train() client id: f_00002-5-0 loss: 1.201750  [   32/  124]
train() client id: f_00002-5-1 loss: 1.120799  [   64/  124]
train() client id: f_00002-5-2 loss: 1.113261  [   96/  124]
train() client id: f_00002-6-0 loss: 1.121423  [   32/  124]
train() client id: f_00002-6-1 loss: 1.014839  [   64/  124]
train() client id: f_00002-6-2 loss: 1.101106  [   96/  124]
train() client id: f_00002-7-0 loss: 1.116082  [   32/  124]
train() client id: f_00002-7-1 loss: 1.062030  [   64/  124]
train() client id: f_00002-7-2 loss: 1.251341  [   96/  124]
train() client id: f_00002-8-0 loss: 1.037771  [   32/  124]
train() client id: f_00002-8-1 loss: 0.928415  [   64/  124]
train() client id: f_00002-8-2 loss: 1.301085  [   96/  124]
train() client id: f_00002-9-0 loss: 0.987402  [   32/  124]
train() client id: f_00002-9-1 loss: 1.136640  [   64/  124]
train() client id: f_00002-9-2 loss: 1.030382  [   96/  124]
train() client id: f_00002-10-0 loss: 1.098153  [   32/  124]
train() client id: f_00002-10-1 loss: 1.022289  [   64/  124]
train() client id: f_00002-10-2 loss: 1.138877  [   96/  124]
train() client id: f_00002-11-0 loss: 1.201027  [   32/  124]
train() client id: f_00002-11-1 loss: 0.958959  [   64/  124]
train() client id: f_00002-11-2 loss: 1.048929  [   96/  124]
train() client id: f_00003-0-0 loss: 0.725566  [   32/   43]
train() client id: f_00003-1-0 loss: 0.747151  [   32/   43]
train() client id: f_00003-2-0 loss: 0.656452  [   32/   43]
train() client id: f_00003-3-0 loss: 0.624881  [   32/   43]
train() client id: f_00003-4-0 loss: 0.785347  [   32/   43]
train() client id: f_00003-5-0 loss: 0.445589  [   32/   43]
train() client id: f_00003-6-0 loss: 0.717937  [   32/   43]
train() client id: f_00003-7-0 loss: 0.702779  [   32/   43]
train() client id: f_00003-8-0 loss: 0.655240  [   32/   43]
train() client id: f_00003-9-0 loss: 0.623538  [   32/   43]
train() client id: f_00003-10-0 loss: 0.662837  [   32/   43]
train() client id: f_00003-11-0 loss: 0.661917  [   32/   43]
train() client id: f_00004-0-0 loss: 0.836239  [   32/  306]
train() client id: f_00004-0-1 loss: 0.957644  [   64/  306]
train() client id: f_00004-0-2 loss: 1.026215  [   96/  306]
train() client id: f_00004-0-3 loss: 0.815280  [  128/  306]
train() client id: f_00004-0-4 loss: 0.932812  [  160/  306]
train() client id: f_00004-0-5 loss: 0.946367  [  192/  306]
train() client id: f_00004-0-6 loss: 0.865399  [  224/  306]
train() client id: f_00004-0-7 loss: 0.895465  [  256/  306]
train() client id: f_00004-0-8 loss: 0.856840  [  288/  306]
train() client id: f_00004-1-0 loss: 1.018104  [   32/  306]
train() client id: f_00004-1-1 loss: 0.931861  [   64/  306]
train() client id: f_00004-1-2 loss: 0.896243  [   96/  306]
train() client id: f_00004-1-3 loss: 0.929435  [  128/  306]
train() client id: f_00004-1-4 loss: 0.832178  [  160/  306]
train() client id: f_00004-1-5 loss: 0.788996  [  192/  306]
train() client id: f_00004-1-6 loss: 0.781380  [  224/  306]
train() client id: f_00004-1-7 loss: 0.900648  [  256/  306]
train() client id: f_00004-1-8 loss: 0.848544  [  288/  306]
train() client id: f_00004-2-0 loss: 0.880519  [   32/  306]
train() client id: f_00004-2-1 loss: 0.929472  [   64/  306]
train() client id: f_00004-2-2 loss: 0.829207  [   96/  306]
train() client id: f_00004-2-3 loss: 0.865480  [  128/  306]
train() client id: f_00004-2-4 loss: 0.933667  [  160/  306]
train() client id: f_00004-2-5 loss: 0.973951  [  192/  306]
train() client id: f_00004-2-6 loss: 0.832909  [  224/  306]
train() client id: f_00004-2-7 loss: 0.984113  [  256/  306]
train() client id: f_00004-2-8 loss: 0.828949  [  288/  306]
train() client id: f_00004-3-0 loss: 0.800287  [   32/  306]
train() client id: f_00004-3-1 loss: 0.914794  [   64/  306]
train() client id: f_00004-3-2 loss: 0.864640  [   96/  306]
train() client id: f_00004-3-3 loss: 0.807457  [  128/  306]
train() client id: f_00004-3-4 loss: 0.893347  [  160/  306]
train() client id: f_00004-3-5 loss: 0.913132  [  192/  306]
train() client id: f_00004-3-6 loss: 0.998787  [  224/  306]
train() client id: f_00004-3-7 loss: 0.830387  [  256/  306]
train() client id: f_00004-3-8 loss: 0.894705  [  288/  306]
train() client id: f_00004-4-0 loss: 0.933366  [   32/  306]
train() client id: f_00004-4-1 loss: 0.783954  [   64/  306]
train() client id: f_00004-4-2 loss: 0.868030  [   96/  306]
train() client id: f_00004-4-3 loss: 0.946828  [  128/  306]
train() client id: f_00004-4-4 loss: 0.804925  [  160/  306]
train() client id: f_00004-4-5 loss: 1.080061  [  192/  306]
train() client id: f_00004-4-6 loss: 0.916421  [  224/  306]
train() client id: f_00004-4-7 loss: 0.802018  [  256/  306]
train() client id: f_00004-4-8 loss: 0.809685  [  288/  306]
train() client id: f_00004-5-0 loss: 0.798640  [   32/  306]
train() client id: f_00004-5-1 loss: 0.838445  [   64/  306]
train() client id: f_00004-5-2 loss: 0.780822  [   96/  306]
train() client id: f_00004-5-3 loss: 0.898149  [  128/  306]
train() client id: f_00004-5-4 loss: 0.993875  [  160/  306]
train() client id: f_00004-5-5 loss: 0.900852  [  192/  306]
train() client id: f_00004-5-6 loss: 0.810543  [  224/  306]
train() client id: f_00004-5-7 loss: 0.954666  [  256/  306]
train() client id: f_00004-5-8 loss: 0.894693  [  288/  306]
train() client id: f_00004-6-0 loss: 0.810250  [   32/  306]
train() client id: f_00004-6-1 loss: 0.818397  [   64/  306]
train() client id: f_00004-6-2 loss: 0.966259  [   96/  306]
train() client id: f_00004-6-3 loss: 0.876716  [  128/  306]
train() client id: f_00004-6-4 loss: 0.975274  [  160/  306]
train() client id: f_00004-6-5 loss: 0.907367  [  192/  306]
train() client id: f_00004-6-6 loss: 0.796840  [  224/  306]
train() client id: f_00004-6-7 loss: 0.922679  [  256/  306]
train() client id: f_00004-6-8 loss: 0.838678  [  288/  306]
train() client id: f_00004-7-0 loss: 0.897540  [   32/  306]
train() client id: f_00004-7-1 loss: 0.818149  [   64/  306]
train() client id: f_00004-7-2 loss: 0.838163  [   96/  306]
train() client id: f_00004-7-3 loss: 0.906922  [  128/  306]
train() client id: f_00004-7-4 loss: 0.765693  [  160/  306]
train() client id: f_00004-7-5 loss: 0.811168  [  192/  306]
train() client id: f_00004-7-6 loss: 1.000635  [  224/  306]
train() client id: f_00004-7-7 loss: 0.989093  [  256/  306]
train() client id: f_00004-7-8 loss: 0.880687  [  288/  306]
train() client id: f_00004-8-0 loss: 0.860010  [   32/  306]
train() client id: f_00004-8-1 loss: 0.812829  [   64/  306]
train() client id: f_00004-8-2 loss: 0.811384  [   96/  306]
train() client id: f_00004-8-3 loss: 0.884583  [  128/  306]
train() client id: f_00004-8-4 loss: 0.840689  [  160/  306]
train() client id: f_00004-8-5 loss: 0.991885  [  192/  306]
train() client id: f_00004-8-6 loss: 0.864780  [  224/  306]
train() client id: f_00004-8-7 loss: 0.981966  [  256/  306]
train() client id: f_00004-8-8 loss: 0.817126  [  288/  306]
train() client id: f_00004-9-0 loss: 0.823678  [   32/  306]
train() client id: f_00004-9-1 loss: 0.854193  [   64/  306]
train() client id: f_00004-9-2 loss: 0.942339  [   96/  306]
train() client id: f_00004-9-3 loss: 0.813730  [  128/  306]
train() client id: f_00004-9-4 loss: 0.824572  [  160/  306]
train() client id: f_00004-9-5 loss: 0.886045  [  192/  306]
train() client id: f_00004-9-6 loss: 0.970401  [  224/  306]
train() client id: f_00004-9-7 loss: 0.795585  [  256/  306]
train() client id: f_00004-9-8 loss: 0.903247  [  288/  306]
train() client id: f_00004-10-0 loss: 0.967507  [   32/  306]
train() client id: f_00004-10-1 loss: 0.871292  [   64/  306]
train() client id: f_00004-10-2 loss: 0.857860  [   96/  306]
train() client id: f_00004-10-3 loss: 0.839684  [  128/  306]
train() client id: f_00004-10-4 loss: 0.943523  [  160/  306]
train() client id: f_00004-10-5 loss: 0.934063  [  192/  306]
train() client id: f_00004-10-6 loss: 0.945791  [  224/  306]
train() client id: f_00004-10-7 loss: 0.792399  [  256/  306]
train() client id: f_00004-10-8 loss: 0.796764  [  288/  306]
train() client id: f_00004-11-0 loss: 0.861754  [   32/  306]
train() client id: f_00004-11-1 loss: 0.804076  [   64/  306]
train() client id: f_00004-11-2 loss: 0.786280  [   96/  306]
train() client id: f_00004-11-3 loss: 0.860189  [  128/  306]
train() client id: f_00004-11-4 loss: 0.912770  [  160/  306]
train() client id: f_00004-11-5 loss: 0.925787  [  192/  306]
train() client id: f_00004-11-6 loss: 0.851779  [  224/  306]
train() client id: f_00004-11-7 loss: 0.883589  [  256/  306]
train() client id: f_00004-11-8 loss: 0.925434  [  288/  306]
train() client id: f_00005-0-0 loss: 0.414085  [   32/  146]
train() client id: f_00005-0-1 loss: 0.679703  [   64/  146]
train() client id: f_00005-0-2 loss: 0.506148  [   96/  146]
train() client id: f_00005-0-3 loss: 0.721531  [  128/  146]
train() client id: f_00005-1-0 loss: 0.459716  [   32/  146]
train() client id: f_00005-1-1 loss: 0.492438  [   64/  146]
train() client id: f_00005-1-2 loss: 0.863362  [   96/  146]
train() client id: f_00005-1-3 loss: 0.487488  [  128/  146]
train() client id: f_00005-2-0 loss: 0.636757  [   32/  146]
train() client id: f_00005-2-1 loss: 0.559069  [   64/  146]
train() client id: f_00005-2-2 loss: 0.489071  [   96/  146]
train() client id: f_00005-2-3 loss: 0.681701  [  128/  146]
train() client id: f_00005-3-0 loss: 0.800480  [   32/  146]
train() client id: f_00005-3-1 loss: 0.476250  [   64/  146]
train() client id: f_00005-3-2 loss: 0.513467  [   96/  146]
train() client id: f_00005-3-3 loss: 0.678923  [  128/  146]
train() client id: f_00005-4-0 loss: 0.750448  [   32/  146]
train() client id: f_00005-4-1 loss: 0.417452  [   64/  146]
train() client id: f_00005-4-2 loss: 0.553508  [   96/  146]
train() client id: f_00005-4-3 loss: 0.680921  [  128/  146]
train() client id: f_00005-5-0 loss: 0.327578  [   32/  146]
train() client id: f_00005-5-1 loss: 0.657437  [   64/  146]
train() client id: f_00005-5-2 loss: 0.931051  [   96/  146]
train() client id: f_00005-5-3 loss: 0.376383  [  128/  146]
train() client id: f_00005-6-0 loss: 0.676425  [   32/  146]
train() client id: f_00005-6-1 loss: 0.443270  [   64/  146]
train() client id: f_00005-6-2 loss: 0.954615  [   96/  146]
train() client id: f_00005-6-3 loss: 0.344048  [  128/  146]
train() client id: f_00005-7-0 loss: 0.541242  [   32/  146]
train() client id: f_00005-7-1 loss: 0.562767  [   64/  146]
train() client id: f_00005-7-2 loss: 0.636650  [   96/  146]
train() client id: f_00005-7-3 loss: 0.687424  [  128/  146]
train() client id: f_00005-8-0 loss: 0.675468  [   32/  146]
train() client id: f_00005-8-1 loss: 0.733113  [   64/  146]
train() client id: f_00005-8-2 loss: 0.393014  [   96/  146]
train() client id: f_00005-8-3 loss: 0.635614  [  128/  146]
train() client id: f_00005-9-0 loss: 0.677446  [   32/  146]
train() client id: f_00005-9-1 loss: 0.597160  [   64/  146]
train() client id: f_00005-9-2 loss: 0.576132  [   96/  146]
train() client id: f_00005-9-3 loss: 0.661191  [  128/  146]
train() client id: f_00005-10-0 loss: 0.711601  [   32/  146]
train() client id: f_00005-10-1 loss: 0.492913  [   64/  146]
train() client id: f_00005-10-2 loss: 0.638381  [   96/  146]
train() client id: f_00005-10-3 loss: 0.501060  [  128/  146]
train() client id: f_00005-11-0 loss: 0.478006  [   32/  146]
train() client id: f_00005-11-1 loss: 0.733383  [   64/  146]
train() client id: f_00005-11-2 loss: 0.633538  [   96/  146]
train() client id: f_00005-11-3 loss: 0.632355  [  128/  146]
train() client id: f_00006-0-0 loss: 0.570091  [   32/   54]
train() client id: f_00006-1-0 loss: 0.592043  [   32/   54]
train() client id: f_00006-2-0 loss: 0.504167  [   32/   54]
train() client id: f_00006-3-0 loss: 0.540538  [   32/   54]
train() client id: f_00006-4-0 loss: 0.592224  [   32/   54]
train() client id: f_00006-5-0 loss: 0.545339  [   32/   54]
train() client id: f_00006-6-0 loss: 0.530224  [   32/   54]
train() client id: f_00006-7-0 loss: 0.583566  [   32/   54]
train() client id: f_00006-8-0 loss: 0.589712  [   32/   54]
train() client id: f_00006-9-0 loss: 0.537460  [   32/   54]
train() client id: f_00006-10-0 loss: 0.484969  [   32/   54]
train() client id: f_00006-11-0 loss: 0.540860  [   32/   54]
train() client id: f_00007-0-0 loss: 0.757623  [   32/  179]
train() client id: f_00007-0-1 loss: 0.613872  [   64/  179]
train() client id: f_00007-0-2 loss: 0.525388  [   96/  179]
train() client id: f_00007-0-3 loss: 0.765952  [  128/  179]
train() client id: f_00007-0-4 loss: 0.577884  [  160/  179]
train() client id: f_00007-1-0 loss: 0.822206  [   32/  179]
train() client id: f_00007-1-1 loss: 0.633938  [   64/  179]
train() client id: f_00007-1-2 loss: 0.499557  [   96/  179]
train() client id: f_00007-1-3 loss: 0.479373  [  128/  179]
train() client id: f_00007-1-4 loss: 0.690968  [  160/  179]
train() client id: f_00007-2-0 loss: 0.542841  [   32/  179]
train() client id: f_00007-2-1 loss: 0.820391  [   64/  179]
train() client id: f_00007-2-2 loss: 0.524997  [   96/  179]
train() client id: f_00007-2-3 loss: 0.638697  [  128/  179]
train() client id: f_00007-2-4 loss: 0.586954  [  160/  179]
train() client id: f_00007-3-0 loss: 0.914115  [   32/  179]
train() client id: f_00007-3-1 loss: 0.518229  [   64/  179]
train() client id: f_00007-3-2 loss: 0.614084  [   96/  179]
train() client id: f_00007-3-3 loss: 0.558101  [  128/  179]
train() client id: f_00007-3-4 loss: 0.562409  [  160/  179]
train() client id: f_00007-4-0 loss: 0.459455  [   32/  179]
train() client id: f_00007-4-1 loss: 0.806938  [   64/  179]
train() client id: f_00007-4-2 loss: 0.766780  [   96/  179]
train() client id: f_00007-4-3 loss: 0.509855  [  128/  179]
train() client id: f_00007-4-4 loss: 0.550622  [  160/  179]
train() client id: f_00007-5-0 loss: 0.552712  [   32/  179]
train() client id: f_00007-5-1 loss: 0.489923  [   64/  179]
train() client id: f_00007-5-2 loss: 0.494126  [   96/  179]
train() client id: f_00007-5-3 loss: 0.760365  [  128/  179]
train() client id: f_00007-5-4 loss: 0.730238  [  160/  179]
train() client id: f_00007-6-0 loss: 0.665805  [   32/  179]
train() client id: f_00007-6-1 loss: 0.510087  [   64/  179]
train() client id: f_00007-6-2 loss: 0.629021  [   96/  179]
train() client id: f_00007-6-3 loss: 0.495996  [  128/  179]
train() client id: f_00007-6-4 loss: 0.839304  [  160/  179]
train() client id: f_00007-7-0 loss: 0.757706  [   32/  179]
train() client id: f_00007-7-1 loss: 0.550189  [   64/  179]
train() client id: f_00007-7-2 loss: 0.609813  [   96/  179]
train() client id: f_00007-7-3 loss: 0.491467  [  128/  179]
train() client id: f_00007-7-4 loss: 0.589421  [  160/  179]
train() client id: f_00007-8-0 loss: 0.664186  [   32/  179]
train() client id: f_00007-8-1 loss: 0.705878  [   64/  179]
train() client id: f_00007-8-2 loss: 0.643612  [   96/  179]
train() client id: f_00007-8-3 loss: 0.448203  [  128/  179]
train() client id: f_00007-8-4 loss: 0.466793  [  160/  179]
train() client id: f_00007-9-0 loss: 0.617476  [   32/  179]
train() client id: f_00007-9-1 loss: 0.681525  [   64/  179]
train() client id: f_00007-9-2 loss: 0.654442  [   96/  179]
train() client id: f_00007-9-3 loss: 0.582804  [  128/  179]
train() client id: f_00007-9-4 loss: 0.480528  [  160/  179]
train() client id: f_00007-10-0 loss: 0.706409  [   32/  179]
train() client id: f_00007-10-1 loss: 0.562403  [   64/  179]
train() client id: f_00007-10-2 loss: 0.720183  [   96/  179]
train() client id: f_00007-10-3 loss: 0.662404  [  128/  179]
train() client id: f_00007-10-4 loss: 0.464840  [  160/  179]
train() client id: f_00007-11-0 loss: 0.476349  [   32/  179]
train() client id: f_00007-11-1 loss: 0.604302  [   64/  179]
train() client id: f_00007-11-2 loss: 0.770416  [   96/  179]
train() client id: f_00007-11-3 loss: 0.543448  [  128/  179]
train() client id: f_00007-11-4 loss: 0.630408  [  160/  179]
train() client id: f_00008-0-0 loss: 0.650465  [   32/  130]
train() client id: f_00008-0-1 loss: 0.648419  [   64/  130]
train() client id: f_00008-0-2 loss: 0.745993  [   96/  130]
train() client id: f_00008-0-3 loss: 0.683480  [  128/  130]
train() client id: f_00008-1-0 loss: 0.658229  [   32/  130]
train() client id: f_00008-1-1 loss: 0.703131  [   64/  130]
train() client id: f_00008-1-2 loss: 0.721215  [   96/  130]
train() client id: f_00008-1-3 loss: 0.650989  [  128/  130]
train() client id: f_00008-2-0 loss: 0.749594  [   32/  130]
train() client id: f_00008-2-1 loss: 0.621458  [   64/  130]
train() client id: f_00008-2-2 loss: 0.742424  [   96/  130]
train() client id: f_00008-2-3 loss: 0.595899  [  128/  130]
train() client id: f_00008-3-0 loss: 0.655181  [   32/  130]
train() client id: f_00008-3-1 loss: 0.755908  [   64/  130]
train() client id: f_00008-3-2 loss: 0.700889  [   96/  130]
train() client id: f_00008-3-3 loss: 0.605053  [  128/  130]
train() client id: f_00008-4-0 loss: 0.696328  [   32/  130]
train() client id: f_00008-4-1 loss: 0.721923  [   64/  130]
train() client id: f_00008-4-2 loss: 0.635330  [   96/  130]
train() client id: f_00008-4-3 loss: 0.640870  [  128/  130]
train() client id: f_00008-5-0 loss: 0.752279  [   32/  130]
train() client id: f_00008-5-1 loss: 0.826468  [   64/  130]
train() client id: f_00008-5-2 loss: 0.564747  [   96/  130]
train() client id: f_00008-5-3 loss: 0.605814  [  128/  130]
train() client id: f_00008-6-0 loss: 0.697594  [   32/  130]
train() client id: f_00008-6-1 loss: 0.654085  [   64/  130]
train() client id: f_00008-6-2 loss: 0.608433  [   96/  130]
train() client id: f_00008-6-3 loss: 0.782347  [  128/  130]
train() client id: f_00008-7-0 loss: 0.689893  [   32/  130]
train() client id: f_00008-7-1 loss: 0.654776  [   64/  130]
train() client id: f_00008-7-2 loss: 0.609269  [   96/  130]
train() client id: f_00008-7-3 loss: 0.722206  [  128/  130]
train() client id: f_00008-8-0 loss: 0.645698  [   32/  130]
train() client id: f_00008-8-1 loss: 0.732644  [   64/  130]
train() client id: f_00008-8-2 loss: 0.711717  [   96/  130]
train() client id: f_00008-8-3 loss: 0.610692  [  128/  130]
train() client id: f_00008-9-0 loss: 0.693254  [   32/  130]
train() client id: f_00008-9-1 loss: 0.660645  [   64/  130]
train() client id: f_00008-9-2 loss: 0.609569  [   96/  130]
train() client id: f_00008-9-3 loss: 0.768195  [  128/  130]
train() client id: f_00008-10-0 loss: 0.738447  [   32/  130]
train() client id: f_00008-10-1 loss: 0.633310  [   64/  130]
train() client id: f_00008-10-2 loss: 0.633226  [   96/  130]
train() client id: f_00008-10-3 loss: 0.708244  [  128/  130]
train() client id: f_00008-11-0 loss: 0.665664  [   32/  130]
train() client id: f_00008-11-1 loss: 0.722018  [   64/  130]
train() client id: f_00008-11-2 loss: 0.750311  [   96/  130]
train() client id: f_00008-11-3 loss: 0.599346  [  128/  130]
train() client id: f_00009-0-0 loss: 1.118189  [   32/  118]
train() client id: f_00009-0-1 loss: 1.232471  [   64/  118]
train() client id: f_00009-0-2 loss: 1.138558  [   96/  118]
train() client id: f_00009-1-0 loss: 1.024287  [   32/  118]
train() client id: f_00009-1-1 loss: 1.169585  [   64/  118]
train() client id: f_00009-1-2 loss: 1.176666  [   96/  118]
train() client id: f_00009-2-0 loss: 1.134603  [   32/  118]
train() client id: f_00009-2-1 loss: 0.989166  [   64/  118]
train() client id: f_00009-2-2 loss: 1.142686  [   96/  118]
train() client id: f_00009-3-0 loss: 1.075941  [   32/  118]
train() client id: f_00009-3-1 loss: 1.117810  [   64/  118]
train() client id: f_00009-3-2 loss: 0.965970  [   96/  118]
train() client id: f_00009-4-0 loss: 1.037963  [   32/  118]
train() client id: f_00009-4-1 loss: 1.009983  [   64/  118]
train() client id: f_00009-4-2 loss: 0.980192  [   96/  118]
train() client id: f_00009-5-0 loss: 0.980475  [   32/  118]
train() client id: f_00009-5-1 loss: 0.958576  [   64/  118]
train() client id: f_00009-5-2 loss: 0.960475  [   96/  118]
train() client id: f_00009-6-0 loss: 1.016215  [   32/  118]
train() client id: f_00009-6-1 loss: 1.063236  [   64/  118]
train() client id: f_00009-6-2 loss: 0.835901  [   96/  118]
train() client id: f_00009-7-0 loss: 0.861855  [   32/  118]
train() client id: f_00009-7-1 loss: 0.988056  [   64/  118]
train() client id: f_00009-7-2 loss: 1.088602  [   96/  118]
train() client id: f_00009-8-0 loss: 0.909458  [   32/  118]
train() client id: f_00009-8-1 loss: 0.976306  [   64/  118]
train() client id: f_00009-8-2 loss: 0.912477  [   96/  118]
train() client id: f_00009-9-0 loss: 0.948632  [   32/  118]
train() client id: f_00009-9-1 loss: 0.967475  [   64/  118]
train() client id: f_00009-9-2 loss: 0.890532  [   96/  118]
train() client id: f_00009-10-0 loss: 1.029407  [   32/  118]
train() client id: f_00009-10-1 loss: 0.890525  [   64/  118]
train() client id: f_00009-10-2 loss: 0.913711  [   96/  118]
train() client id: f_00009-11-0 loss: 0.986248  [   32/  118]
train() client id: f_00009-11-1 loss: 1.003620  [   64/  118]
train() client id: f_00009-11-2 loss: 0.873472  [   96/  118]
At round 26 accuracy: 0.6419098143236074
At round 26 training accuracy: 0.5841716968477532
At round 26 training loss: 0.8332753382473285
update_location
xs = [  -3.9056584     4.20031788  150.00902392   18.81129433    0.97929623
    3.95640986 -112.44319194  -91.32485185  134.66397685  -77.06087855]
ys = [ 142.5879595   125.55583871    1.32061395 -112.45517586  104.35018685
   87.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [174.20269906 160.56746652 180.28990897 151.65761231 144.53345812
 133.14269882 150.50037194 135.42859676 168.65069414 126.31069179]
dists_bs = [174.13734262 185.89642012 368.50426696 346.68904791 189.63150793
 199.06473556 188.31885006 193.23401585 347.38493983 197.05979992]
uav_gains = [2.47904434e-11 3.05317297e-11 2.26669887e-11 3.52625235e-11
 3.97925071e-11 4.88782381e-11 3.59484763e-11 4.68391522e-11
 2.69445184e-11 5.57642431e-11]
bs_gains = [5.87214444e-11 4.89029672e-11 7.19871577e-12 8.54007614e-12
 4.62535094e-11 4.03747604e-11 4.71619185e-11 4.38793376e-11
 8.49226080e-12 4.15355131e-11]
Round 27
-------------------------------
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.428454   15.42356035  7.31658542  2.62856167 17.79004913  8.56531288
  3.26241587 10.46431357  7.71314909  6.94940181]
obj_prev = 87.54180377366474
eta_min = 3.2918080146347544e-13	eta_max = 0.9246953843511696
af = 18.488129690050464	bf = 1.525402092626451	zeta = 20.336942659055513	eta = 0.909090909090909
af = 18.488129690050464	bf = 1.525402092626451	zeta = 36.16259963393917	eta = 0.5112500173438599
af = 18.488129690050464	bf = 1.525402092626451	zeta = 28.49928344660974	eta = 0.6487226152435002
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.11857262246232	eta = 0.6817515784269833
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.048110971781785	eta = 0.6835275745998821
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.047913433089164	eta = 0.6835325665983885
eta = 0.6835325665983885
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [0.03139638 0.06603207 0.03089802 0.01071464 0.07624835 0.03637993
 0.01345559 0.04460279 0.03239309 0.02940296]
ene_total = [2.38132415 4.38338617 2.36243079 1.10502586 4.99963694 2.62710832
 1.26705354 3.1051769  2.60980459 2.20696617]
ti_comp = [0.39480521 0.40653023 0.39297956 0.40131289 0.40569068 0.40355948
 0.40164054 0.40588199 0.36658327 0.40401382]
ti_coms = [0.08405931 0.07233429 0.08588495 0.07755163 0.07317384 0.07530504
 0.07722398 0.07298253 0.11228124 0.07485069]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.24094611e-05 1.08882722e-04 1.19380162e-05 4.77360184e-07
 1.68337651e-04 1.84778100e-05 9.43872309e-07 3.36640665e-05
 1.58085418e-05 9.73331126e-06]
ene_total = [0.50365913 0.43928163 0.51455356 0.4640106  0.44786169 0.45164645
 0.46207824 0.43865975 0.67271096 0.448405  ]
optimize_network_iter = 0 obj = 4.842867033053818
eta = 0.6835325665983885
freqs = [39761858.95611374 81214212.7965591  39312506.23491908 13349480.34302717
 93973501.61782414 45073814.47997239 16750787.69960297 54945517.04012107
 44182442.51361968 36388553.66776452]
eta_min = 0.6835325665983994	eta_max = 0.6835325665983844
af = 0.018943411267469068	bf = 1.525402092626451	zeta = 0.020837752394215977	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [3.10856006e-06 2.72750345e-05 2.99046350e-06 1.19578344e-07
 4.21684466e-05 4.62867660e-06 2.36439257e-07 8.43282168e-06
 3.96002704e-06 2.43818667e-06]
ene_total = [1.75137172 1.512205   1.78937036 1.61521218 1.53279247 1.56936094
 1.60841252 1.52178181 2.33933449 1.55944202]
ti_comp = [0.39480521 0.40653023 0.39297956 0.40131289 0.40569068 0.40355948
 0.40164054 0.40588199 0.36658327 0.40401382]
ti_coms = [0.08405931 0.07233429 0.08588495 0.07755163 0.07317384 0.07530504
 0.07722398 0.07298253 0.11228124 0.07485069]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.24094611e-05 1.08882722e-04 1.19380162e-05 4.77360184e-07
 1.68337651e-04 1.84778100e-05 9.43872309e-07 3.36640665e-05
 1.58085418e-05 9.73331126e-06]
ene_total = [0.50365913 0.43928163 0.51455356 0.4640106  0.44786169 0.45164645
 0.46207824 0.43865975 0.67271096 0.448405  ]
optimize_network_iter = 1 obj = 4.842867033053756
eta = 0.6835325665983844
freqs = [39761858.95611373 81214212.79655911 39312506.23491906 13349480.34302717
 93973501.61782415 45073814.4799724  16750787.69960297 54945517.04012109
 44182442.51361962 36388553.66776453]
Done!
At round 27 eta: 0.6835325665983844
At round 27 local rounds: 12.458880266672946
At round 27 global rounds: 59.82879433933611
At round 27 a_n: 18.59131914111349
gradient difference: 0.4678993225097656
train() client id: f_00000-0-0 loss: 1.456579  [   32/  126]
train() client id: f_00000-0-1 loss: 1.173483  [   64/  126]
train() client id: f_00000-0-2 loss: 1.252011  [   96/  126]
train() client id: f_00000-1-0 loss: 1.349373  [   32/  126]
train() client id: f_00000-1-1 loss: 1.199651  [   64/  126]
train() client id: f_00000-1-2 loss: 1.137208  [   96/  126]
train() client id: f_00000-2-0 loss: 1.245123  [   32/  126]
train() client id: f_00000-2-1 loss: 1.008216  [   64/  126]
train() client id: f_00000-2-2 loss: 1.064177  [   96/  126]
train() client id: f_00000-3-0 loss: 1.214442  [   32/  126]
train() client id: f_00000-3-1 loss: 0.926536  [   64/  126]
train() client id: f_00000-3-2 loss: 1.008854  [   96/  126]
train() client id: f_00000-4-0 loss: 1.079163  [   32/  126]
train() client id: f_00000-4-1 loss: 1.002915  [   64/  126]
train() client id: f_00000-4-2 loss: 0.989476  [   96/  126]
train() client id: f_00000-5-0 loss: 1.065328  [   32/  126]
train() client id: f_00000-5-1 loss: 0.987275  [   64/  126]
train() client id: f_00000-5-2 loss: 0.978073  [   96/  126]
train() client id: f_00000-6-0 loss: 0.992576  [   32/  126]
train() client id: f_00000-6-1 loss: 0.824477  [   64/  126]
train() client id: f_00000-6-2 loss: 1.160178  [   96/  126]
train() client id: f_00000-7-0 loss: 0.976829  [   32/  126]
train() client id: f_00000-7-1 loss: 0.922515  [   64/  126]
train() client id: f_00000-7-2 loss: 0.987924  [   96/  126]
train() client id: f_00000-8-0 loss: 0.984747  [   32/  126]
train() client id: f_00000-8-1 loss: 0.961746  [   64/  126]
train() client id: f_00000-8-2 loss: 0.943117  [   96/  126]
train() client id: f_00000-9-0 loss: 0.989796  [   32/  126]
train() client id: f_00000-9-1 loss: 0.976065  [   64/  126]
train() client id: f_00000-9-2 loss: 0.849074  [   96/  126]
train() client id: f_00000-10-0 loss: 0.964567  [   32/  126]
train() client id: f_00000-10-1 loss: 0.850551  [   64/  126]
train() client id: f_00000-10-2 loss: 0.948805  [   96/  126]
train() client id: f_00000-11-0 loss: 0.858438  [   32/  126]
train() client id: f_00000-11-1 loss: 0.914510  [   64/  126]
train() client id: f_00000-11-2 loss: 0.929538  [   96/  126]
train() client id: f_00001-0-0 loss: 0.531583  [   32/  265]
train() client id: f_00001-0-1 loss: 0.498001  [   64/  265]
train() client id: f_00001-0-2 loss: 0.516020  [   96/  265]
train() client id: f_00001-0-3 loss: 0.561186  [  128/  265]
train() client id: f_00001-0-4 loss: 0.575920  [  160/  265]
train() client id: f_00001-0-5 loss: 0.533120  [  192/  265]
train() client id: f_00001-0-6 loss: 0.493581  [  224/  265]
train() client id: f_00001-0-7 loss: 0.659089  [  256/  265]
train() client id: f_00001-1-0 loss: 0.448787  [   32/  265]
train() client id: f_00001-1-1 loss: 0.514009  [   64/  265]
train() client id: f_00001-1-2 loss: 0.551315  [   96/  265]
train() client id: f_00001-1-3 loss: 0.475764  [  128/  265]
train() client id: f_00001-1-4 loss: 0.629668  [  160/  265]
train() client id: f_00001-1-5 loss: 0.548496  [  192/  265]
train() client id: f_00001-1-6 loss: 0.510826  [  224/  265]
train() client id: f_00001-1-7 loss: 0.630836  [  256/  265]
train() client id: f_00001-2-0 loss: 0.545556  [   32/  265]
train() client id: f_00001-2-1 loss: 0.442954  [   64/  265]
train() client id: f_00001-2-2 loss: 0.425156  [   96/  265]
train() client id: f_00001-2-3 loss: 0.504144  [  128/  265]
train() client id: f_00001-2-4 loss: 0.572060  [  160/  265]
train() client id: f_00001-2-5 loss: 0.609251  [  192/  265]
train() client id: f_00001-2-6 loss: 0.627187  [  224/  265]
train() client id: f_00001-2-7 loss: 0.482619  [  256/  265]
train() client id: f_00001-3-0 loss: 0.513050  [   32/  265]
train() client id: f_00001-3-1 loss: 0.627919  [   64/  265]
train() client id: f_00001-3-2 loss: 0.547682  [   96/  265]
train() client id: f_00001-3-3 loss: 0.547201  [  128/  265]
train() client id: f_00001-3-4 loss: 0.502439  [  160/  265]
train() client id: f_00001-3-5 loss: 0.444598  [  192/  265]
train() client id: f_00001-3-6 loss: 0.451232  [  224/  265]
train() client id: f_00001-3-7 loss: 0.558858  [  256/  265]
train() client id: f_00001-4-0 loss: 0.554462  [   32/  265]
train() client id: f_00001-4-1 loss: 0.525145  [   64/  265]
train() client id: f_00001-4-2 loss: 0.484441  [   96/  265]
train() client id: f_00001-4-3 loss: 0.491205  [  128/  265]
train() client id: f_00001-4-4 loss: 0.515667  [  160/  265]
train() client id: f_00001-4-5 loss: 0.576163  [  192/  265]
train() client id: f_00001-4-6 loss: 0.521758  [  224/  265]
train() client id: f_00001-4-7 loss: 0.571415  [  256/  265]
train() client id: f_00001-5-0 loss: 0.436061  [   32/  265]
train() client id: f_00001-5-1 loss: 0.539055  [   64/  265]
train() client id: f_00001-5-2 loss: 0.446177  [   96/  265]
train() client id: f_00001-5-3 loss: 0.571186  [  128/  265]
train() client id: f_00001-5-4 loss: 0.499654  [  160/  265]
train() client id: f_00001-5-5 loss: 0.656184  [  192/  265]
train() client id: f_00001-5-6 loss: 0.451152  [  224/  265]
train() client id: f_00001-5-7 loss: 0.555086  [  256/  265]
train() client id: f_00001-6-0 loss: 0.545971  [   32/  265]
train() client id: f_00001-6-1 loss: 0.568819  [   64/  265]
train() client id: f_00001-6-2 loss: 0.455010  [   96/  265]
train() client id: f_00001-6-3 loss: 0.454391  [  128/  265]
train() client id: f_00001-6-4 loss: 0.680927  [  160/  265]
train() client id: f_00001-6-5 loss: 0.550202  [  192/  265]
train() client id: f_00001-6-6 loss: 0.470021  [  224/  265]
train() client id: f_00001-6-7 loss: 0.481171  [  256/  265]
train() client id: f_00001-7-0 loss: 0.556717  [   32/  265]
train() client id: f_00001-7-1 loss: 0.504616  [   64/  265]
train() client id: f_00001-7-2 loss: 0.547012  [   96/  265]
train() client id: f_00001-7-3 loss: 0.531782  [  128/  265]
train() client id: f_00001-7-4 loss: 0.648004  [  160/  265]
train() client id: f_00001-7-5 loss: 0.517798  [  192/  265]
train() client id: f_00001-7-6 loss: 0.422739  [  224/  265]
train() client id: f_00001-7-7 loss: 0.487278  [  256/  265]
train() client id: f_00001-8-0 loss: 0.560378  [   32/  265]
train() client id: f_00001-8-1 loss: 0.508840  [   64/  265]
train() client id: f_00001-8-2 loss: 0.532803  [   96/  265]
train() client id: f_00001-8-3 loss: 0.473382  [  128/  265]
train() client id: f_00001-8-4 loss: 0.724496  [  160/  265]
train() client id: f_00001-8-5 loss: 0.484266  [  192/  265]
train() client id: f_00001-8-6 loss: 0.429605  [  224/  265]
train() client id: f_00001-8-7 loss: 0.486937  [  256/  265]
train() client id: f_00001-9-0 loss: 0.461650  [   32/  265]
train() client id: f_00001-9-1 loss: 0.582741  [   64/  265]
train() client id: f_00001-9-2 loss: 0.690960  [   96/  265]
train() client id: f_00001-9-3 loss: 0.542507  [  128/  265]
train() client id: f_00001-9-4 loss: 0.444344  [  160/  265]
train() client id: f_00001-9-5 loss: 0.427651  [  192/  265]
train() client id: f_00001-9-6 loss: 0.511575  [  224/  265]
train() client id: f_00001-9-7 loss: 0.506873  [  256/  265]
train() client id: f_00001-10-0 loss: 0.480297  [   32/  265]
train() client id: f_00001-10-1 loss: 0.415524  [   64/  265]
train() client id: f_00001-10-2 loss: 0.548121  [   96/  265]
train() client id: f_00001-10-3 loss: 0.446685  [  128/  265]
train() client id: f_00001-10-4 loss: 0.694211  [  160/  265]
train() client id: f_00001-10-5 loss: 0.477285  [  192/  265]
train() client id: f_00001-10-6 loss: 0.600430  [  224/  265]
train() client id: f_00001-10-7 loss: 0.513627  [  256/  265]
train() client id: f_00001-11-0 loss: 0.536956  [   32/  265]
train() client id: f_00001-11-1 loss: 0.574435  [   64/  265]
train() client id: f_00001-11-2 loss: 0.502943  [   96/  265]
train() client id: f_00001-11-3 loss: 0.495183  [  128/  265]
train() client id: f_00001-11-4 loss: 0.486160  [  160/  265]
train() client id: f_00001-11-5 loss: 0.548578  [  192/  265]
train() client id: f_00001-11-6 loss: 0.505755  [  224/  265]
train() client id: f_00001-11-7 loss: 0.568604  [  256/  265]
train() client id: f_00002-0-0 loss: 1.088718  [   32/  124]
train() client id: f_00002-0-1 loss: 1.154488  [   64/  124]
train() client id: f_00002-0-2 loss: 1.224345  [   96/  124]
train() client id: f_00002-1-0 loss: 1.162964  [   32/  124]
train() client id: f_00002-1-1 loss: 1.303542  [   64/  124]
train() client id: f_00002-1-2 loss: 1.050415  [   96/  124]
train() client id: f_00002-2-0 loss: 1.101977  [   32/  124]
train() client id: f_00002-2-1 loss: 1.048139  [   64/  124]
train() client id: f_00002-2-2 loss: 1.168073  [   96/  124]
train() client id: f_00002-3-0 loss: 1.170990  [   32/  124]
train() client id: f_00002-3-1 loss: 1.064808  [   64/  124]
train() client id: f_00002-3-2 loss: 1.077707  [   96/  124]
train() client id: f_00002-4-0 loss: 0.966825  [   32/  124]
train() client id: f_00002-4-1 loss: 1.206783  [   64/  124]
train() client id: f_00002-4-2 loss: 1.053131  [   96/  124]
train() client id: f_00002-5-0 loss: 1.099340  [   32/  124]
train() client id: f_00002-5-1 loss: 1.093602  [   64/  124]
train() client id: f_00002-5-2 loss: 0.922408  [   96/  124]
train() client id: f_00002-6-0 loss: 1.015674  [   32/  124]
train() client id: f_00002-6-1 loss: 1.138474  [   64/  124]
train() client id: f_00002-6-2 loss: 0.862503  [   96/  124]
train() client id: f_00002-7-0 loss: 1.025156  [   32/  124]
train() client id: f_00002-7-1 loss: 0.967628  [   64/  124]
train() client id: f_00002-7-2 loss: 1.063220  [   96/  124]
train() client id: f_00002-8-0 loss: 0.918292  [   32/  124]
train() client id: f_00002-8-1 loss: 1.005753  [   64/  124]
train() client id: f_00002-8-2 loss: 1.057347  [   96/  124]
train() client id: f_00002-9-0 loss: 0.955643  [   32/  124]
train() client id: f_00002-9-1 loss: 1.007641  [   64/  124]
train() client id: f_00002-9-2 loss: 1.012956  [   96/  124]
train() client id: f_00002-10-0 loss: 1.117412  [   32/  124]
train() client id: f_00002-10-1 loss: 0.975040  [   64/  124]
train() client id: f_00002-10-2 loss: 0.825244  [   96/  124]
train() client id: f_00002-11-0 loss: 1.037062  [   32/  124]
train() client id: f_00002-11-1 loss: 1.044994  [   64/  124]
train() client id: f_00002-11-2 loss: 1.108839  [   96/  124]
train() client id: f_00003-0-0 loss: 0.714723  [   32/   43]
train() client id: f_00003-1-0 loss: 0.733372  [   32/   43]
train() client id: f_00003-2-0 loss: 0.687767  [   32/   43]
train() client id: f_00003-3-0 loss: 0.723655  [   32/   43]
train() client id: f_00003-4-0 loss: 0.826036  [   32/   43]
train() client id: f_00003-5-0 loss: 0.882341  [   32/   43]
train() client id: f_00003-6-0 loss: 0.750465  [   32/   43]
train() client id: f_00003-7-0 loss: 0.731990  [   32/   43]
train() client id: f_00003-8-0 loss: 0.804694  [   32/   43]
train() client id: f_00003-9-0 loss: 0.814230  [   32/   43]
train() client id: f_00003-10-0 loss: 0.620673  [   32/   43]
train() client id: f_00003-11-0 loss: 0.767256  [   32/   43]
train() client id: f_00004-0-0 loss: 0.673049  [   32/  306]
train() client id: f_00004-0-1 loss: 0.861086  [   64/  306]
train() client id: f_00004-0-2 loss: 0.678860  [   96/  306]
train() client id: f_00004-0-3 loss: 0.814174  [  128/  306]
train() client id: f_00004-0-4 loss: 0.835373  [  160/  306]
train() client id: f_00004-0-5 loss: 0.862353  [  192/  306]
train() client id: f_00004-0-6 loss: 0.842351  [  224/  306]
train() client id: f_00004-0-7 loss: 0.769966  [  256/  306]
train() client id: f_00004-0-8 loss: 0.708389  [  288/  306]
train() client id: f_00004-1-0 loss: 0.813054  [   32/  306]
train() client id: f_00004-1-1 loss: 0.684627  [   64/  306]
train() client id: f_00004-1-2 loss: 0.839617  [   96/  306]
train() client id: f_00004-1-3 loss: 0.857465  [  128/  306]
train() client id: f_00004-1-4 loss: 0.838302  [  160/  306]
train() client id: f_00004-1-5 loss: 0.680033  [  192/  306]
train() client id: f_00004-1-6 loss: 0.706167  [  224/  306]
train() client id: f_00004-1-7 loss: 0.834596  [  256/  306]
train() client id: f_00004-1-8 loss: 0.734658  [  288/  306]
train() client id: f_00004-2-0 loss: 0.859242  [   32/  306]
train() client id: f_00004-2-1 loss: 0.788791  [   64/  306]
train() client id: f_00004-2-2 loss: 0.733991  [   96/  306]
train() client id: f_00004-2-3 loss: 0.718424  [  128/  306]
train() client id: f_00004-2-4 loss: 0.747871  [  160/  306]
train() client id: f_00004-2-5 loss: 0.894255  [  192/  306]
train() client id: f_00004-2-6 loss: 0.861960  [  224/  306]
train() client id: f_00004-2-7 loss: 0.851895  [  256/  306]
train() client id: f_00004-2-8 loss: 0.728760  [  288/  306]
train() client id: f_00004-3-0 loss: 0.829096  [   32/  306]
train() client id: f_00004-3-1 loss: 0.723424  [   64/  306]
train() client id: f_00004-3-2 loss: 0.974072  [   96/  306]
train() client id: f_00004-3-3 loss: 0.727378  [  128/  306]
train() client id: f_00004-3-4 loss: 0.786145  [  160/  306]
train() client id: f_00004-3-5 loss: 0.763432  [  192/  306]
train() client id: f_00004-3-6 loss: 0.730331  [  224/  306]
train() client id: f_00004-3-7 loss: 0.838856  [  256/  306]
train() client id: f_00004-3-8 loss: 0.780024  [  288/  306]
train() client id: f_00004-4-0 loss: 0.719439  [   32/  306]
train() client id: f_00004-4-1 loss: 0.893778  [   64/  306]
train() client id: f_00004-4-2 loss: 0.818752  [   96/  306]
train() client id: f_00004-4-3 loss: 0.831440  [  128/  306]
train() client id: f_00004-4-4 loss: 0.698472  [  160/  306]
train() client id: f_00004-4-5 loss: 0.661664  [  192/  306]
train() client id: f_00004-4-6 loss: 0.837977  [  224/  306]
train() client id: f_00004-4-7 loss: 0.838710  [  256/  306]
train() client id: f_00004-4-8 loss: 0.827429  [  288/  306]
train() client id: f_00004-5-0 loss: 0.847003  [   32/  306]
train() client id: f_00004-5-1 loss: 0.919711  [   64/  306]
train() client id: f_00004-5-2 loss: 0.752406  [   96/  306]
train() client id: f_00004-5-3 loss: 0.777925  [  128/  306]
train() client id: f_00004-5-4 loss: 0.803805  [  160/  306]
train() client id: f_00004-5-5 loss: 0.748205  [  192/  306]
train() client id: f_00004-5-6 loss: 0.750254  [  224/  306]
train() client id: f_00004-5-7 loss: 0.750837  [  256/  306]
train() client id: f_00004-5-8 loss: 0.765908  [  288/  306]
train() client id: f_00004-6-0 loss: 0.799687  [   32/  306]
train() client id: f_00004-6-1 loss: 0.771704  [   64/  306]
train() client id: f_00004-6-2 loss: 0.782713  [   96/  306]
train() client id: f_00004-6-3 loss: 0.851397  [  128/  306]
train() client id: f_00004-6-4 loss: 0.730536  [  160/  306]
train() client id: f_00004-6-5 loss: 0.749961  [  192/  306]
train() client id: f_00004-6-6 loss: 0.831587  [  224/  306]
train() client id: f_00004-6-7 loss: 0.855954  [  256/  306]
train() client id: f_00004-6-8 loss: 0.733527  [  288/  306]
train() client id: f_00004-7-0 loss: 0.788323  [   32/  306]
train() client id: f_00004-7-1 loss: 0.892179  [   64/  306]
train() client id: f_00004-7-2 loss: 0.692881  [   96/  306]
train() client id: f_00004-7-3 loss: 0.733765  [  128/  306]
train() client id: f_00004-7-4 loss: 0.883412  [  160/  306]
train() client id: f_00004-7-5 loss: 0.751806  [  192/  306]
train() client id: f_00004-7-6 loss: 0.727430  [  224/  306]
train() client id: f_00004-7-7 loss: 0.922629  [  256/  306]
train() client id: f_00004-7-8 loss: 0.768136  [  288/  306]
train() client id: f_00004-8-0 loss: 0.670555  [   32/  306]
train() client id: f_00004-8-1 loss: 0.834142  [   64/  306]
train() client id: f_00004-8-2 loss: 0.787516  [   96/  306]
train() client id: f_00004-8-3 loss: 0.776330  [  128/  306]
train() client id: f_00004-8-4 loss: 0.739540  [  160/  306]
train() client id: f_00004-8-5 loss: 0.988730  [  192/  306]
train() client id: f_00004-8-6 loss: 0.648123  [  224/  306]
train() client id: f_00004-8-7 loss: 0.855767  [  256/  306]
train() client id: f_00004-8-8 loss: 0.855748  [  288/  306]
train() client id: f_00004-9-0 loss: 0.869162  [   32/  306]
train() client id: f_00004-9-1 loss: 0.872517  [   64/  306]
train() client id: f_00004-9-2 loss: 0.718526  [   96/  306]
train() client id: f_00004-9-3 loss: 0.830430  [  128/  306]
train() client id: f_00004-9-4 loss: 0.829693  [  160/  306]
train() client id: f_00004-9-5 loss: 0.788333  [  192/  306]
train() client id: f_00004-9-6 loss: 0.799836  [  224/  306]
train() client id: f_00004-9-7 loss: 0.733538  [  256/  306]
train() client id: f_00004-9-8 loss: 0.786942  [  288/  306]
train() client id: f_00004-10-0 loss: 0.807865  [   32/  306]
train() client id: f_00004-10-1 loss: 0.822136  [   64/  306]
train() client id: f_00004-10-2 loss: 0.801135  [   96/  306]
train() client id: f_00004-10-3 loss: 0.766676  [  128/  306]
train() client id: f_00004-10-4 loss: 0.785630  [  160/  306]
train() client id: f_00004-10-5 loss: 0.795639  [  192/  306]
train() client id: f_00004-10-6 loss: 0.755394  [  224/  306]
train() client id: f_00004-10-7 loss: 0.770678  [  256/  306]
train() client id: f_00004-10-8 loss: 0.871938  [  288/  306]
train() client id: f_00004-11-0 loss: 0.722581  [   32/  306]
train() client id: f_00004-11-1 loss: 0.609158  [   64/  306]
train() client id: f_00004-11-2 loss: 0.723247  [   96/  306]
train() client id: f_00004-11-3 loss: 0.960366  [  128/  306]
train() client id: f_00004-11-4 loss: 0.846704  [  160/  306]
train() client id: f_00004-11-5 loss: 0.875524  [  192/  306]
train() client id: f_00004-11-6 loss: 0.787536  [  224/  306]
train() client id: f_00004-11-7 loss: 0.829006  [  256/  306]
train() client id: f_00004-11-8 loss: 0.805306  [  288/  306]
train() client id: f_00005-0-0 loss: 0.696075  [   32/  146]
train() client id: f_00005-0-1 loss: 0.601276  [   64/  146]
train() client id: f_00005-0-2 loss: 0.651897  [   96/  146]
train() client id: f_00005-0-3 loss: 0.584864  [  128/  146]
train() client id: f_00005-1-0 loss: 0.584619  [   32/  146]
train() client id: f_00005-1-1 loss: 0.489697  [   64/  146]
train() client id: f_00005-1-2 loss: 0.614300  [   96/  146]
train() client id: f_00005-1-3 loss: 0.868986  [  128/  146]
train() client id: f_00005-2-0 loss: 0.753209  [   32/  146]
train() client id: f_00005-2-1 loss: 0.626358  [   64/  146]
train() client id: f_00005-2-2 loss: 0.627261  [   96/  146]
train() client id: f_00005-2-3 loss: 0.709449  [  128/  146]
train() client id: f_00005-3-0 loss: 0.516129  [   32/  146]
train() client id: f_00005-3-1 loss: 0.669267  [   64/  146]
train() client id: f_00005-3-2 loss: 0.896695  [   96/  146]
train() client id: f_00005-3-3 loss: 0.454606  [  128/  146]
train() client id: f_00005-4-0 loss: 0.741744  [   32/  146]
train() client id: f_00005-4-1 loss: 0.653872  [   64/  146]
train() client id: f_00005-4-2 loss: 0.680812  [   96/  146]
train() client id: f_00005-4-3 loss: 0.689884  [  128/  146]
train() client id: f_00005-5-0 loss: 0.795615  [   32/  146]
train() client id: f_00005-5-1 loss: 0.539450  [   64/  146]
train() client id: f_00005-5-2 loss: 0.322189  [   96/  146]
train() client id: f_00005-5-3 loss: 0.977861  [  128/  146]
train() client id: f_00005-6-0 loss: 0.863452  [   32/  146]
train() client id: f_00005-6-1 loss: 0.660825  [   64/  146]
train() client id: f_00005-6-2 loss: 0.594226  [   96/  146]
train() client id: f_00005-6-3 loss: 0.663171  [  128/  146]
train() client id: f_00005-7-0 loss: 0.553355  [   32/  146]
train() client id: f_00005-7-1 loss: 0.771554  [   64/  146]
train() client id: f_00005-7-2 loss: 0.785102  [   96/  146]
train() client id: f_00005-7-3 loss: 0.527138  [  128/  146]
train() client id: f_00005-8-0 loss: 0.676631  [   32/  146]
train() client id: f_00005-8-1 loss: 0.562748  [   64/  146]
train() client id: f_00005-8-2 loss: 0.597573  [   96/  146]
train() client id: f_00005-8-3 loss: 0.848933  [  128/  146]
train() client id: f_00005-9-0 loss: 0.571447  [   32/  146]
train() client id: f_00005-9-1 loss: 0.460097  [   64/  146]
train() client id: f_00005-9-2 loss: 0.639988  [   96/  146]
train() client id: f_00005-9-3 loss: 1.055051  [  128/  146]
train() client id: f_00005-10-0 loss: 0.610048  [   32/  146]
train() client id: f_00005-10-1 loss: 0.684338  [   64/  146]
train() client id: f_00005-10-2 loss: 0.669696  [   96/  146]
train() client id: f_00005-10-3 loss: 0.658308  [  128/  146]
train() client id: f_00005-11-0 loss: 0.681897  [   32/  146]
train() client id: f_00005-11-1 loss: 0.543502  [   64/  146]
train() client id: f_00005-11-2 loss: 0.633469  [   96/  146]
train() client id: f_00005-11-3 loss: 0.809949  [  128/  146]
train() client id: f_00006-0-0 loss: 0.563982  [   32/   54]
train() client id: f_00006-1-0 loss: 0.545944  [   32/   54]
train() client id: f_00006-2-0 loss: 0.546864  [   32/   54]
train() client id: f_00006-3-0 loss: 0.573346  [   32/   54]
train() client id: f_00006-4-0 loss: 0.581271  [   32/   54]
train() client id: f_00006-5-0 loss: 0.542810  [   32/   54]
train() client id: f_00006-6-0 loss: 0.532929  [   32/   54]
train() client id: f_00006-7-0 loss: 0.501381  [   32/   54]
train() client id: f_00006-8-0 loss: 0.528035  [   32/   54]
train() client id: f_00006-9-0 loss: 0.566189  [   32/   54]
train() client id: f_00006-10-0 loss: 0.541760  [   32/   54]
train() client id: f_00006-11-0 loss: 0.585954  [   32/   54]
train() client id: f_00007-0-0 loss: 0.482989  [   32/  179]
train() client id: f_00007-0-1 loss: 0.617255  [   64/  179]
train() client id: f_00007-0-2 loss: 0.695044  [   96/  179]
train() client id: f_00007-0-3 loss: 0.586361  [  128/  179]
train() client id: f_00007-0-4 loss: 0.458566  [  160/  179]
train() client id: f_00007-1-0 loss: 0.508317  [   32/  179]
train() client id: f_00007-1-1 loss: 0.764041  [   64/  179]
train() client id: f_00007-1-2 loss: 0.568697  [   96/  179]
train() client id: f_00007-1-3 loss: 0.463418  [  128/  179]
train() client id: f_00007-1-4 loss: 0.519143  [  160/  179]
train() client id: f_00007-2-0 loss: 0.483701  [   32/  179]
train() client id: f_00007-2-1 loss: 0.488565  [   64/  179]
train() client id: f_00007-2-2 loss: 0.533993  [   96/  179]
train() client id: f_00007-2-3 loss: 0.601021  [  128/  179]
train() client id: f_00007-2-4 loss: 0.567280  [  160/  179]
train() client id: f_00007-3-0 loss: 0.597948  [   32/  179]
train() client id: f_00007-3-1 loss: 0.615688  [   64/  179]
train() client id: f_00007-3-2 loss: 0.482777  [   96/  179]
train() client id: f_00007-3-3 loss: 0.509368  [  128/  179]
train() client id: f_00007-3-4 loss: 0.559324  [  160/  179]
train() client id: f_00007-4-0 loss: 0.406523  [   32/  179]
train() client id: f_00007-4-1 loss: 0.690536  [   64/  179]
train() client id: f_00007-4-2 loss: 0.621936  [   96/  179]
train() client id: f_00007-4-3 loss: 0.541571  [  128/  179]
train() client id: f_00007-4-4 loss: 0.462912  [  160/  179]
train() client id: f_00007-5-0 loss: 0.481068  [   32/  179]
train() client id: f_00007-5-1 loss: 0.619157  [   64/  179]
train() client id: f_00007-5-2 loss: 0.577867  [   96/  179]
train() client id: f_00007-5-3 loss: 0.501144  [  128/  179]
train() client id: f_00007-5-4 loss: 0.426179  [  160/  179]
train() client id: f_00007-6-0 loss: 0.520388  [   32/  179]
train() client id: f_00007-6-1 loss: 0.518120  [   64/  179]
train() client id: f_00007-6-2 loss: 0.512439  [   96/  179]
train() client id: f_00007-6-3 loss: 0.560736  [  128/  179]
train() client id: f_00007-6-4 loss: 0.463574  [  160/  179]
train() client id: f_00007-7-0 loss: 0.869717  [   32/  179]
train() client id: f_00007-7-1 loss: 0.472538  [   64/  179]
train() client id: f_00007-7-2 loss: 0.525143  [   96/  179]
train() client id: f_00007-7-3 loss: 0.398220  [  128/  179]
train() client id: f_00007-7-4 loss: 0.377442  [  160/  179]
train() client id: f_00007-8-0 loss: 0.522909  [   32/  179]
train() client id: f_00007-8-1 loss: 0.373759  [   64/  179]
train() client id: f_00007-8-2 loss: 0.626719  [   96/  179]
train() client id: f_00007-8-3 loss: 0.563354  [  128/  179]
train() client id: f_00007-8-4 loss: 0.550376  [  160/  179]
train() client id: f_00007-9-0 loss: 0.467562  [   32/  179]
train() client id: f_00007-9-1 loss: 0.548004  [   64/  179]
train() client id: f_00007-9-2 loss: 0.696418  [   96/  179]
train() client id: f_00007-9-3 loss: 0.380809  [  128/  179]
train() client id: f_00007-9-4 loss: 0.479966  [  160/  179]
train() client id: f_00007-10-0 loss: 0.506176  [   32/  179]
train() client id: f_00007-10-1 loss: 0.534634  [   64/  179]
train() client id: f_00007-10-2 loss: 0.505207  [   96/  179]
train() client id: f_00007-10-3 loss: 0.596856  [  128/  179]
train() client id: f_00007-10-4 loss: 0.466993  [  160/  179]
train() client id: f_00007-11-0 loss: 0.585298  [   32/  179]
train() client id: f_00007-11-1 loss: 0.480458  [   64/  179]
train() client id: f_00007-11-2 loss: 0.477704  [   96/  179]
train() client id: f_00007-11-3 loss: 0.567235  [  128/  179]
train() client id: f_00007-11-4 loss: 0.340988  [  160/  179]
train() client id: f_00008-0-0 loss: 0.828053  [   32/  130]
train() client id: f_00008-0-1 loss: 0.918326  [   64/  130]
train() client id: f_00008-0-2 loss: 0.872911  [   96/  130]
train() client id: f_00008-0-3 loss: 0.717466  [  128/  130]
train() client id: f_00008-1-0 loss: 0.798562  [   32/  130]
train() client id: f_00008-1-1 loss: 0.828899  [   64/  130]
train() client id: f_00008-1-2 loss: 0.853824  [   96/  130]
train() client id: f_00008-1-3 loss: 0.828639  [  128/  130]
train() client id: f_00008-2-0 loss: 0.714357  [   32/  130]
train() client id: f_00008-2-1 loss: 0.842635  [   64/  130]
train() client id: f_00008-2-2 loss: 0.882167  [   96/  130]
train() client id: f_00008-2-3 loss: 0.887431  [  128/  130]
train() client id: f_00008-3-0 loss: 0.810142  [   32/  130]
train() client id: f_00008-3-1 loss: 0.856798  [   64/  130]
train() client id: f_00008-3-2 loss: 0.903480  [   96/  130]
train() client id: f_00008-3-3 loss: 0.760094  [  128/  130]
train() client id: f_00008-4-0 loss: 0.750802  [   32/  130]
train() client id: f_00008-4-1 loss: 0.934770  [   64/  130]
train() client id: f_00008-4-2 loss: 0.800312  [   96/  130]
train() client id: f_00008-4-3 loss: 0.851228  [  128/  130]
train() client id: f_00008-5-0 loss: 0.771476  [   32/  130]
train() client id: f_00008-5-1 loss: 0.776920  [   64/  130]
train() client id: f_00008-5-2 loss: 0.948988  [   96/  130]
train() client id: f_00008-5-3 loss: 0.798109  [  128/  130]
train() client id: f_00008-6-0 loss: 0.803625  [   32/  130]
train() client id: f_00008-6-1 loss: 1.061434  [   64/  130]
train() client id: f_00008-6-2 loss: 0.737186  [   96/  130]
train() client id: f_00008-6-3 loss: 0.738860  [  128/  130]
train() client id: f_00008-7-0 loss: 0.987860  [   32/  130]
train() client id: f_00008-7-1 loss: 0.781998  [   64/  130]
train() client id: f_00008-7-2 loss: 0.724383  [   96/  130]
train() client id: f_00008-7-3 loss: 0.829718  [  128/  130]
train() client id: f_00008-8-0 loss: 0.802444  [   32/  130]
train() client id: f_00008-8-1 loss: 0.835128  [   64/  130]
train() client id: f_00008-8-2 loss: 0.855046  [   96/  130]
train() client id: f_00008-8-3 loss: 0.812948  [  128/  130]
train() client id: f_00008-9-0 loss: 0.800647  [   32/  130]
train() client id: f_00008-9-1 loss: 0.850814  [   64/  130]
train() client id: f_00008-9-2 loss: 0.871146  [   96/  130]
train() client id: f_00008-9-3 loss: 0.785651  [  128/  130]
train() client id: f_00008-10-0 loss: 0.901137  [   32/  130]
train() client id: f_00008-10-1 loss: 0.859503  [   64/  130]
train() client id: f_00008-10-2 loss: 0.787798  [   96/  130]
train() client id: f_00008-10-3 loss: 0.792712  [  128/  130]
train() client id: f_00008-11-0 loss: 0.822208  [   32/  130]
train() client id: f_00008-11-1 loss: 0.823648  [   64/  130]
train() client id: f_00008-11-2 loss: 0.832574  [   96/  130]
train() client id: f_00008-11-3 loss: 0.862258  [  128/  130]
train() client id: f_00009-0-0 loss: 1.255145  [   32/  118]
train() client id: f_00009-0-1 loss: 1.190809  [   64/  118]
train() client id: f_00009-0-2 loss: 1.052642  [   96/  118]
train() client id: f_00009-1-0 loss: 1.273642  [   32/  118]
train() client id: f_00009-1-1 loss: 1.062937  [   64/  118]
train() client id: f_00009-1-2 loss: 1.046957  [   96/  118]
train() client id: f_00009-2-0 loss: 1.061645  [   32/  118]
train() client id: f_00009-2-1 loss: 1.057466  [   64/  118]
train() client id: f_00009-2-2 loss: 1.025594  [   96/  118]
train() client id: f_00009-3-0 loss: 1.062674  [   32/  118]
train() client id: f_00009-3-1 loss: 0.918530  [   64/  118]
train() client id: f_00009-3-2 loss: 1.101806  [   96/  118]
train() client id: f_00009-4-0 loss: 1.005991  [   32/  118]
train() client id: f_00009-4-1 loss: 1.002050  [   64/  118]
train() client id: f_00009-4-2 loss: 1.035532  [   96/  118]
train() client id: f_00009-5-0 loss: 0.997331  [   32/  118]
train() client id: f_00009-5-1 loss: 0.963283  [   64/  118]
train() client id: f_00009-5-2 loss: 0.903840  [   96/  118]
train() client id: f_00009-6-0 loss: 0.955190  [   32/  118]
train() client id: f_00009-6-1 loss: 1.001414  [   64/  118]
train() client id: f_00009-6-2 loss: 0.973566  [   96/  118]
train() client id: f_00009-7-0 loss: 1.051105  [   32/  118]
train() client id: f_00009-7-1 loss: 0.896387  [   64/  118]
train() client id: f_00009-7-2 loss: 0.952094  [   96/  118]
train() client id: f_00009-8-0 loss: 0.885942  [   32/  118]
train() client id: f_00009-8-1 loss: 0.943228  [   64/  118]
train() client id: f_00009-8-2 loss: 0.903265  [   96/  118]
train() client id: f_00009-9-0 loss: 1.028888  [   32/  118]
train() client id: f_00009-9-1 loss: 0.941802  [   64/  118]
train() client id: f_00009-9-2 loss: 0.891078  [   96/  118]
train() client id: f_00009-10-0 loss: 0.893593  [   32/  118]
train() client id: f_00009-10-1 loss: 0.871514  [   64/  118]
train() client id: f_00009-10-2 loss: 0.903312  [   96/  118]
train() client id: f_00009-11-0 loss: 0.981552  [   32/  118]
train() client id: f_00009-11-1 loss: 0.995064  [   64/  118]
train() client id: f_00009-11-2 loss: 0.810679  [   96/  118]
At round 27 accuracy: 0.6445623342175066
At round 27 training accuracy: 0.5801475519785378
At round 27 training loss: 0.8314188857673398
update_location
xs = [  -3.9056584     4.20031788  155.00902392   18.81129433    0.97929623
    3.95640986 -117.44319194  -96.32485185  139.66397685  -82.06087855]
ys = [ 147.5879595   130.55583871    1.32061395 -117.45517586  109.35018685
   92.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [178.31842293 164.50674664 184.47097744 155.40136142 148.18374534
 136.49219669 154.27181815 138.84939085 172.66932676 129.42178968]
dists_bs = [173.27635064 184.62945973 372.92155342 350.84590306 187.82601169
 196.92615481 186.71749037 191.12203797 351.84930892 194.62315774]
uav_gains = [2.33299769e-11 2.87101678e-11 2.13317186e-11 3.31618763e-11
 3.73775877e-11 4.59306330e-11 3.37773315e-11 4.40026087e-11
 2.53630547e-11 5.24710704e-11]
bs_gains = [5.95420887e-11 4.98484075e-11 6.96249872e-12 8.25977351e-12
 4.75092334e-11 4.16144887e-11 4.83032198e-11 4.52505576e-11
 8.19398791e-12 4.30080195e-11]
Round 28
-------------------------------
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.2963697  15.14398805  7.18662324  2.58300247 17.46743421  8.40944175
  3.20540132 10.27679841  7.57595044  6.8226403 ]
obj_prev = 85.96764987436393
eta_min = 1.97492431522131e-13	eta_max = 0.9251230340770827
af = 18.15364795335576	bf = 1.507335053902618	zeta = 19.96901274869134	eta = 0.909090909090909
af = 18.15364795335576	bf = 1.507335053902618	zeta = 35.613187548855024	eta = 0.509745102946827
af = 18.15364795335576	bf = 1.507335053902618	zeta = 28.02638083867362	eta = 0.6477342921247089
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.65886868799346	eta = 0.6809609277055236
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.58884556407932	eta = 0.682754274141212
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.588647822049044	eta = 0.6827593518426902
eta = 0.6827593518426902
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [0.03148978 0.0662285  0.03098994 0.01074651 0.07647517 0.03648815
 0.01349562 0.04473547 0.03248945 0.02949042]
ene_total = [2.34522213 4.30343838 2.32693545 1.09048608 4.90813228 2.57679446
 1.24971036 3.05496696 2.56930912 2.1636526 ]
ti_comp = [0.402737   0.41597589 0.40085904 0.40941141 0.41525818 0.41320547
 0.40973259 0.41408375 0.37449574 0.41372635]
ti_coms = [0.08528891 0.07205001 0.08716686 0.07861449 0.07276772 0.07482043
 0.07829331 0.07394215 0.11353016 0.07429955]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.20322169e-05 1.04924856e-04 1.15760063e-05 4.62767175e-07
 1.62108192e-04 1.77829383e-05 9.15076330e-07 3.26332096e-05
 1.52831670e-05 9.36476148e-06]
ene_total = [0.50052546 0.42838512 0.51150413 0.46073322 0.43594225 0.43951388
 0.4588775  0.43523713 0.66621878 0.43596797]
optimize_network_iter = 0 obj = 4.772905437718752
eta = 0.6827593518426902
freqs = [39094714.05548728 79606171.90046072 38654407.63725649 13124341.80140644
 92081472.39087442 44152552.9427729  16468811.85493763 54017423.64815775
 43377597.092848   35640012.17367139]
eta_min = 0.6827593518432681	eta_max = 0.6827593518426932
af = 0.017883468190837512	bf = 1.507335053902618	zeta = 0.019671815009921264	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [3.00512114e-06 2.62056364e-05 2.89117971e-06 1.15578985e-07
 4.04875310e-05 4.44139964e-06 2.28546015e-07 8.15034742e-06
 3.81706619e-06 2.33890753e-06]
ene_total = [1.74481114 1.47881446 1.78319283 1.60772537 1.49641265 1.53101993
 1.60118017 1.51381715 2.32252357 1.51993757]
ti_comp = [0.402737   0.41597589 0.40085904 0.40941141 0.41525818 0.41320547
 0.40973259 0.41408375 0.37449574 0.41372635]
ti_coms = [0.08528891 0.07205001 0.08716686 0.07861449 0.07276772 0.07482043
 0.07829331 0.07394215 0.11353016 0.07429955]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.20322169e-05 1.04924856e-04 1.15760063e-05 4.62767175e-07
 1.62108192e-04 1.77829383e-05 9.15076330e-07 3.26332096e-05
 1.52831670e-05 9.36476148e-06]
ene_total = [0.50052546 0.42838512 0.51150413 0.46073322 0.43594225 0.43951388
 0.4588775  0.43523713 0.66621878 0.43596797]
optimize_network_iter = 1 obj = 4.772905437718796
eta = 0.6827593518426932
freqs = [39094714.05548728 79606171.9004607  38654407.6372565  13124341.80140644
 92081472.39087439 44152552.94277289 16468811.85493763 54017423.64815772
 43377597.09284804 35640012.17367138]
Done!
At round 28 eta: 0.6827593518426932
At round 28 local rounds: 12.495942598469853
At round 28 global rounds: 58.60320626975522
At round 28 a_n: 18.248773294144172
gradient difference: 0.4265783727169037
train() client id: f_00000-0-0 loss: 1.335147  [   32/  126]
train() client id: f_00000-0-1 loss: 1.275809  [   64/  126]
train() client id: f_00000-0-2 loss: 1.336825  [   96/  126]
train() client id: f_00000-1-0 loss: 1.207723  [   32/  126]
train() client id: f_00000-1-1 loss: 1.217243  [   64/  126]
train() client id: f_00000-1-2 loss: 1.190276  [   96/  126]
train() client id: f_00000-2-0 loss: 1.083392  [   32/  126]
train() client id: f_00000-2-1 loss: 1.124863  [   64/  126]
train() client id: f_00000-2-2 loss: 1.104724  [   96/  126]
train() client id: f_00000-3-0 loss: 1.084356  [   32/  126]
train() client id: f_00000-3-1 loss: 1.000098  [   64/  126]
train() client id: f_00000-3-2 loss: 0.968110  [   96/  126]
train() client id: f_00000-4-0 loss: 1.095389  [   32/  126]
train() client id: f_00000-4-1 loss: 0.950670  [   64/  126]
train() client id: f_00000-4-2 loss: 0.942541  [   96/  126]
train() client id: f_00000-5-0 loss: 0.873334  [   32/  126]
train() client id: f_00000-5-1 loss: 0.923288  [   64/  126]
train() client id: f_00000-5-2 loss: 1.017823  [   96/  126]
train() client id: f_00000-6-0 loss: 0.888654  [   32/  126]
train() client id: f_00000-6-1 loss: 1.000016  [   64/  126]
train() client id: f_00000-6-2 loss: 0.894696  [   96/  126]
train() client id: f_00000-7-0 loss: 0.853261  [   32/  126]
train() client id: f_00000-7-1 loss: 0.860272  [   64/  126]
train() client id: f_00000-7-2 loss: 0.995562  [   96/  126]
train() client id: f_00000-8-0 loss: 0.831467  [   32/  126]
train() client id: f_00000-8-1 loss: 0.881854  [   64/  126]
train() client id: f_00000-8-2 loss: 0.848912  [   96/  126]
train() client id: f_00000-9-0 loss: 0.833815  [   32/  126]
train() client id: f_00000-9-1 loss: 0.935934  [   64/  126]
train() client id: f_00000-9-2 loss: 0.796119  [   96/  126]
train() client id: f_00000-10-0 loss: 0.869881  [   32/  126]
train() client id: f_00000-10-1 loss: 0.752163  [   64/  126]
train() client id: f_00000-10-2 loss: 0.941940  [   96/  126]
train() client id: f_00000-11-0 loss: 0.864831  [   32/  126]
train() client id: f_00000-11-1 loss: 0.962837  [   64/  126]
train() client id: f_00000-11-2 loss: 0.865870  [   96/  126]
train() client id: f_00001-0-0 loss: 0.441763  [   32/  265]
train() client id: f_00001-0-1 loss: 0.460541  [   64/  265]
train() client id: f_00001-0-2 loss: 0.523405  [   96/  265]
train() client id: f_00001-0-3 loss: 0.382576  [  128/  265]
train() client id: f_00001-0-4 loss: 0.336740  [  160/  265]
train() client id: f_00001-0-5 loss: 0.576829  [  192/  265]
train() client id: f_00001-0-6 loss: 0.361563  [  224/  265]
train() client id: f_00001-0-7 loss: 0.388329  [  256/  265]
train() client id: f_00001-1-0 loss: 0.411894  [   32/  265]
train() client id: f_00001-1-1 loss: 0.450467  [   64/  265]
train() client id: f_00001-1-2 loss: 0.431305  [   96/  265]
train() client id: f_00001-1-3 loss: 0.435276  [  128/  265]
train() client id: f_00001-1-4 loss: 0.419679  [  160/  265]
train() client id: f_00001-1-5 loss: 0.463125  [  192/  265]
train() client id: f_00001-1-6 loss: 0.381861  [  224/  265]
train() client id: f_00001-1-7 loss: 0.406224  [  256/  265]
train() client id: f_00001-2-0 loss: 0.493764  [   32/  265]
train() client id: f_00001-2-1 loss: 0.492966  [   64/  265]
train() client id: f_00001-2-2 loss: 0.354257  [   96/  265]
train() client id: f_00001-2-3 loss: 0.415871  [  128/  265]
train() client id: f_00001-2-4 loss: 0.305787  [  160/  265]
train() client id: f_00001-2-5 loss: 0.386613  [  192/  265]
train() client id: f_00001-2-6 loss: 0.416254  [  224/  265]
train() client id: f_00001-2-7 loss: 0.408824  [  256/  265]
train() client id: f_00001-3-0 loss: 0.379371  [   32/  265]
train() client id: f_00001-3-1 loss: 0.462826  [   64/  265]
train() client id: f_00001-3-2 loss: 0.359436  [   96/  265]
train() client id: f_00001-3-3 loss: 0.349899  [  128/  265]
train() client id: f_00001-3-4 loss: 0.478571  [  160/  265]
train() client id: f_00001-3-5 loss: 0.369939  [  192/  265]
train() client id: f_00001-3-6 loss: 0.320103  [  224/  265]
train() client id: f_00001-3-7 loss: 0.529332  [  256/  265]
train() client id: f_00001-4-0 loss: 0.439217  [   32/  265]
train() client id: f_00001-4-1 loss: 0.323599  [   64/  265]
train() client id: f_00001-4-2 loss: 0.387735  [   96/  265]
train() client id: f_00001-4-3 loss: 0.405611  [  128/  265]
train() client id: f_00001-4-4 loss: 0.327158  [  160/  265]
train() client id: f_00001-4-5 loss: 0.470354  [  192/  265]
train() client id: f_00001-4-6 loss: 0.450158  [  224/  265]
train() client id: f_00001-4-7 loss: 0.402950  [  256/  265]
train() client id: f_00001-5-0 loss: 0.410520  [   32/  265]
train() client id: f_00001-5-1 loss: 0.377575  [   64/  265]
train() client id: f_00001-5-2 loss: 0.401153  [   96/  265]
train() client id: f_00001-5-3 loss: 0.365036  [  128/  265]
train() client id: f_00001-5-4 loss: 0.362991  [  160/  265]
train() client id: f_00001-5-5 loss: 0.436629  [  192/  265]
train() client id: f_00001-5-6 loss: 0.364425  [  224/  265]
train() client id: f_00001-5-7 loss: 0.499031  [  256/  265]
train() client id: f_00001-6-0 loss: 0.432955  [   32/  265]
train() client id: f_00001-6-1 loss: 0.405015  [   64/  265]
train() client id: f_00001-6-2 loss: 0.368898  [   96/  265]
train() client id: f_00001-6-3 loss: 0.435051  [  128/  265]
train() client id: f_00001-6-4 loss: 0.407608  [  160/  265]
train() client id: f_00001-6-5 loss: 0.331147  [  192/  265]
train() client id: f_00001-6-6 loss: 0.427848  [  224/  265]
train() client id: f_00001-6-7 loss: 0.390048  [  256/  265]
train() client id: f_00001-7-0 loss: 0.320530  [   32/  265]
train() client id: f_00001-7-1 loss: 0.368087  [   64/  265]
train() client id: f_00001-7-2 loss: 0.395069  [   96/  265]
train() client id: f_00001-7-3 loss: 0.324861  [  128/  265]
train() client id: f_00001-7-4 loss: 0.381508  [  160/  265]
train() client id: f_00001-7-5 loss: 0.532579  [  192/  265]
train() client id: f_00001-7-6 loss: 0.469226  [  224/  265]
train() client id: f_00001-7-7 loss: 0.388927  [  256/  265]
train() client id: f_00001-8-0 loss: 0.386288  [   32/  265]
train() client id: f_00001-8-1 loss: 0.407755  [   64/  265]
train() client id: f_00001-8-2 loss: 0.386805  [   96/  265]
train() client id: f_00001-8-3 loss: 0.362671  [  128/  265]
train() client id: f_00001-8-4 loss: 0.395083  [  160/  265]
train() client id: f_00001-8-5 loss: 0.298747  [  192/  265]
train() client id: f_00001-8-6 loss: 0.421414  [  224/  265]
train() client id: f_00001-8-7 loss: 0.436699  [  256/  265]
train() client id: f_00001-9-0 loss: 0.307678  [   32/  265]
train() client id: f_00001-9-1 loss: 0.290224  [   64/  265]
train() client id: f_00001-9-2 loss: 0.374830  [   96/  265]
train() client id: f_00001-9-3 loss: 0.450629  [  128/  265]
train() client id: f_00001-9-4 loss: 0.436298  [  160/  265]
train() client id: f_00001-9-5 loss: 0.398837  [  192/  265]
train() client id: f_00001-9-6 loss: 0.359752  [  224/  265]
train() client id: f_00001-9-7 loss: 0.528886  [  256/  265]
train() client id: f_00001-10-0 loss: 0.429924  [   32/  265]
train() client id: f_00001-10-1 loss: 0.363458  [   64/  265]
train() client id: f_00001-10-2 loss: 0.390978  [   96/  265]
train() client id: f_00001-10-3 loss: 0.352752  [  128/  265]
train() client id: f_00001-10-4 loss: 0.404909  [  160/  265]
train() client id: f_00001-10-5 loss: 0.436322  [  192/  265]
train() client id: f_00001-10-6 loss: 0.327641  [  224/  265]
train() client id: f_00001-10-7 loss: 0.434196  [  256/  265]
train() client id: f_00001-11-0 loss: 0.337467  [   32/  265]
train() client id: f_00001-11-1 loss: 0.392648  [   64/  265]
train() client id: f_00001-11-2 loss: 0.344762  [   96/  265]
train() client id: f_00001-11-3 loss: 0.462146  [  128/  265]
train() client id: f_00001-11-4 loss: 0.507004  [  160/  265]
train() client id: f_00001-11-5 loss: 0.416422  [  192/  265]
train() client id: f_00001-11-6 loss: 0.366866  [  224/  265]
train() client id: f_00001-11-7 loss: 0.309223  [  256/  265]
train() client id: f_00002-0-0 loss: 1.215656  [   32/  124]
train() client id: f_00002-0-1 loss: 0.950375  [   64/  124]
train() client id: f_00002-0-2 loss: 1.107784  [   96/  124]
train() client id: f_00002-1-0 loss: 1.190528  [   32/  124]
train() client id: f_00002-1-1 loss: 1.052615  [   64/  124]
train() client id: f_00002-1-2 loss: 1.017713  [   96/  124]
train() client id: f_00002-2-0 loss: 1.059202  [   32/  124]
train() client id: f_00002-2-1 loss: 1.063768  [   64/  124]
train() client id: f_00002-2-2 loss: 1.060995  [   96/  124]
train() client id: f_00002-3-0 loss: 1.080140  [   32/  124]
train() client id: f_00002-3-1 loss: 0.997902  [   64/  124]
train() client id: f_00002-3-2 loss: 1.146741  [   96/  124]
train() client id: f_00002-4-0 loss: 0.964102  [   32/  124]
train() client id: f_00002-4-1 loss: 0.976494  [   64/  124]
train() client id: f_00002-4-2 loss: 1.032697  [   96/  124]
train() client id: f_00002-5-0 loss: 0.976594  [   32/  124]
train() client id: f_00002-5-1 loss: 1.024372  [   64/  124]
train() client id: f_00002-5-2 loss: 0.928973  [   96/  124]
train() client id: f_00002-6-0 loss: 1.033444  [   32/  124]
train() client id: f_00002-6-1 loss: 0.943940  [   64/  124]
train() client id: f_00002-6-2 loss: 0.941751  [   96/  124]
train() client id: f_00002-7-0 loss: 0.907460  [   32/  124]
train() client id: f_00002-7-1 loss: 0.795904  [   64/  124]
train() client id: f_00002-7-2 loss: 1.160427  [   96/  124]
train() client id: f_00002-8-0 loss: 0.947065  [   32/  124]
train() client id: f_00002-8-1 loss: 0.915622  [   64/  124]
train() client id: f_00002-8-2 loss: 0.892631  [   96/  124]
train() client id: f_00002-9-0 loss: 0.844213  [   32/  124]
train() client id: f_00002-9-1 loss: 0.835734  [   64/  124]
train() client id: f_00002-9-2 loss: 1.062679  [   96/  124]
train() client id: f_00002-10-0 loss: 0.964911  [   32/  124]
train() client id: f_00002-10-1 loss: 0.948440  [   64/  124]
train() client id: f_00002-10-2 loss: 0.865661  [   96/  124]
train() client id: f_00002-11-0 loss: 0.793041  [   32/  124]
train() client id: f_00002-11-1 loss: 0.871020  [   64/  124]
train() client id: f_00002-11-2 loss: 0.803412  [   96/  124]
train() client id: f_00003-0-0 loss: 0.724323  [   32/   43]
train() client id: f_00003-1-0 loss: 0.833568  [   32/   43]
train() client id: f_00003-2-0 loss: 0.777865  [   32/   43]
train() client id: f_00003-3-0 loss: 0.837823  [   32/   43]
train() client id: f_00003-4-0 loss: 0.842190  [   32/   43]
train() client id: f_00003-5-0 loss: 0.666925  [   32/   43]
train() client id: f_00003-6-0 loss: 0.758467  [   32/   43]
train() client id: f_00003-7-0 loss: 0.855319  [   32/   43]
train() client id: f_00003-8-0 loss: 0.644958  [   32/   43]
train() client id: f_00003-9-0 loss: 0.959362  [   32/   43]
train() client id: f_00003-10-0 loss: 0.635496  [   32/   43]
train() client id: f_00003-11-0 loss: 0.625335  [   32/   43]
train() client id: f_00004-0-0 loss: 0.928022  [   32/  306]
train() client id: f_00004-0-1 loss: 0.918464  [   64/  306]
train() client id: f_00004-0-2 loss: 0.766990  [   96/  306]
train() client id: f_00004-0-3 loss: 0.867949  [  128/  306]
train() client id: f_00004-0-4 loss: 0.810271  [  160/  306]
train() client id: f_00004-0-5 loss: 0.842787  [  192/  306]
train() client id: f_00004-0-6 loss: 0.984138  [  224/  306]
train() client id: f_00004-0-7 loss: 0.829359  [  256/  306]
train() client id: f_00004-0-8 loss: 0.863671  [  288/  306]
train() client id: f_00004-1-0 loss: 1.046144  [   32/  306]
train() client id: f_00004-1-1 loss: 0.869405  [   64/  306]
train() client id: f_00004-1-2 loss: 0.748633  [   96/  306]
train() client id: f_00004-1-3 loss: 0.854419  [  128/  306]
train() client id: f_00004-1-4 loss: 0.774100  [  160/  306]
train() client id: f_00004-1-5 loss: 0.752810  [  192/  306]
train() client id: f_00004-1-6 loss: 0.874746  [  224/  306]
train() client id: f_00004-1-7 loss: 0.874818  [  256/  306]
train() client id: f_00004-1-8 loss: 0.903223  [  288/  306]
train() client id: f_00004-2-0 loss: 0.959481  [   32/  306]
train() client id: f_00004-2-1 loss: 0.915679  [   64/  306]
train() client id: f_00004-2-2 loss: 0.848293  [   96/  306]
train() client id: f_00004-2-3 loss: 0.776691  [  128/  306]
train() client id: f_00004-2-4 loss: 0.726534  [  160/  306]
train() client id: f_00004-2-5 loss: 0.955094  [  192/  306]
train() client id: f_00004-2-6 loss: 0.814986  [  224/  306]
train() client id: f_00004-2-7 loss: 0.872992  [  256/  306]
train() client id: f_00004-2-8 loss: 0.916794  [  288/  306]
train() client id: f_00004-3-0 loss: 0.996179  [   32/  306]
train() client id: f_00004-3-1 loss: 0.944151  [   64/  306]
train() client id: f_00004-3-2 loss: 0.850148  [   96/  306]
train() client id: f_00004-3-3 loss: 0.856577  [  128/  306]
train() client id: f_00004-3-4 loss: 1.003061  [  160/  306]
train() client id: f_00004-3-5 loss: 0.737311  [  192/  306]
train() client id: f_00004-3-6 loss: 0.698772  [  224/  306]
train() client id: f_00004-3-7 loss: 0.746862  [  256/  306]
train() client id: f_00004-3-8 loss: 0.846925  [  288/  306]
train() client id: f_00004-4-0 loss: 0.861685  [   32/  306]
train() client id: f_00004-4-1 loss: 0.877980  [   64/  306]
train() client id: f_00004-4-2 loss: 0.882129  [   96/  306]
train() client id: f_00004-4-3 loss: 0.697874  [  128/  306]
train() client id: f_00004-4-4 loss: 0.826045  [  160/  306]
train() client id: f_00004-4-5 loss: 0.800031  [  192/  306]
train() client id: f_00004-4-6 loss: 0.813140  [  224/  306]
train() client id: f_00004-4-7 loss: 0.978815  [  256/  306]
train() client id: f_00004-4-8 loss: 0.965721  [  288/  306]
train() client id: f_00004-5-0 loss: 0.788558  [   32/  306]
train() client id: f_00004-5-1 loss: 0.732802  [   64/  306]
train() client id: f_00004-5-2 loss: 0.837913  [   96/  306]
train() client id: f_00004-5-3 loss: 0.821748  [  128/  306]
train() client id: f_00004-5-4 loss: 0.871703  [  160/  306]
train() client id: f_00004-5-5 loss: 0.910782  [  192/  306]
train() client id: f_00004-5-6 loss: 0.950777  [  224/  306]
train() client id: f_00004-5-7 loss: 1.070676  [  256/  306]
train() client id: f_00004-5-8 loss: 0.785441  [  288/  306]
train() client id: f_00004-6-0 loss: 0.880445  [   32/  306]
train() client id: f_00004-6-1 loss: 0.770152  [   64/  306]
train() client id: f_00004-6-2 loss: 0.896581  [   96/  306]
train() client id: f_00004-6-3 loss: 0.814905  [  128/  306]
train() client id: f_00004-6-4 loss: 0.945317  [  160/  306]
train() client id: f_00004-6-5 loss: 0.858800  [  192/  306]
train() client id: f_00004-6-6 loss: 0.827936  [  224/  306]
train() client id: f_00004-6-7 loss: 0.798512  [  256/  306]
train() client id: f_00004-6-8 loss: 0.957924  [  288/  306]
train() client id: f_00004-7-0 loss: 0.940102  [   32/  306]
train() client id: f_00004-7-1 loss: 0.823986  [   64/  306]
train() client id: f_00004-7-2 loss: 0.872460  [   96/  306]
train() client id: f_00004-7-3 loss: 0.758645  [  128/  306]
train() client id: f_00004-7-4 loss: 0.858625  [  160/  306]
train() client id: f_00004-7-5 loss: 0.799391  [  192/  306]
train() client id: f_00004-7-6 loss: 0.815092  [  224/  306]
train() client id: f_00004-7-7 loss: 0.747358  [  256/  306]
train() client id: f_00004-7-8 loss: 1.025650  [  288/  306]
train() client id: f_00004-8-0 loss: 0.821377  [   32/  306]
train() client id: f_00004-8-1 loss: 0.866713  [   64/  306]
train() client id: f_00004-8-2 loss: 0.880398  [   96/  306]
train() client id: f_00004-8-3 loss: 0.761750  [  128/  306]
train() client id: f_00004-8-4 loss: 0.867530  [  160/  306]
train() client id: f_00004-8-5 loss: 0.998590  [  192/  306]
train() client id: f_00004-8-6 loss: 0.934427  [  224/  306]
train() client id: f_00004-8-7 loss: 0.821715  [  256/  306]
train() client id: f_00004-8-8 loss: 0.776520  [  288/  306]
train() client id: f_00004-9-0 loss: 0.891090  [   32/  306]
train() client id: f_00004-9-1 loss: 0.784372  [   64/  306]
train() client id: f_00004-9-2 loss: 0.743689  [   96/  306]
train() client id: f_00004-9-3 loss: 0.872473  [  128/  306]
train() client id: f_00004-9-4 loss: 0.834481  [  160/  306]
train() client id: f_00004-9-5 loss: 0.842018  [  192/  306]
train() client id: f_00004-9-6 loss: 0.856015  [  224/  306]
train() client id: f_00004-9-7 loss: 0.942533  [  256/  306]
train() client id: f_00004-9-8 loss: 0.865068  [  288/  306]
train() client id: f_00004-10-0 loss: 0.784155  [   32/  306]
train() client id: f_00004-10-1 loss: 0.892298  [   64/  306]
train() client id: f_00004-10-2 loss: 0.788208  [   96/  306]
train() client id: f_00004-10-3 loss: 0.865796  [  128/  306]
train() client id: f_00004-10-4 loss: 0.946357  [  160/  306]
train() client id: f_00004-10-5 loss: 0.997806  [  192/  306]
train() client id: f_00004-10-6 loss: 0.810983  [  224/  306]
train() client id: f_00004-10-7 loss: 0.834323  [  256/  306]
train() client id: f_00004-10-8 loss: 0.795724  [  288/  306]
train() client id: f_00004-11-0 loss: 0.894433  [   32/  306]
train() client id: f_00004-11-1 loss: 0.778799  [   64/  306]
train() client id: f_00004-11-2 loss: 0.872198  [   96/  306]
train() client id: f_00004-11-3 loss: 0.957415  [  128/  306]
train() client id: f_00004-11-4 loss: 0.813800  [  160/  306]
train() client id: f_00004-11-5 loss: 0.853076  [  192/  306]
train() client id: f_00004-11-6 loss: 0.808793  [  224/  306]
train() client id: f_00004-11-7 loss: 0.961420  [  256/  306]
train() client id: f_00004-11-8 loss: 0.732018  [  288/  306]
train() client id: f_00005-0-0 loss: 0.347906  [   32/  146]
train() client id: f_00005-0-1 loss: 0.527900  [   64/  146]
train() client id: f_00005-0-2 loss: 0.543630  [   96/  146]
train() client id: f_00005-0-3 loss: 0.562370  [  128/  146]
train() client id: f_00005-1-0 loss: 0.364777  [   32/  146]
train() client id: f_00005-1-1 loss: 0.526876  [   64/  146]
train() client id: f_00005-1-2 loss: 0.356672  [   96/  146]
train() client id: f_00005-1-3 loss: 0.728488  [  128/  146]
train() client id: f_00005-2-0 loss: 0.531752  [   32/  146]
train() client id: f_00005-2-1 loss: 0.522377  [   64/  146]
train() client id: f_00005-2-2 loss: 0.487922  [   96/  146]
train() client id: f_00005-2-3 loss: 0.500484  [  128/  146]
train() client id: f_00005-3-0 loss: 0.589206  [   32/  146]
train() client id: f_00005-3-1 loss: 0.569521  [   64/  146]
train() client id: f_00005-3-2 loss: 0.364420  [   96/  146]
train() client id: f_00005-3-3 loss: 0.500731  [  128/  146]
train() client id: f_00005-4-0 loss: 0.327637  [   32/  146]
train() client id: f_00005-4-1 loss: 0.581843  [   64/  146]
train() client id: f_00005-4-2 loss: 0.466979  [   96/  146]
train() client id: f_00005-4-3 loss: 0.543405  [  128/  146]
train() client id: f_00005-5-0 loss: 0.391042  [   32/  146]
train() client id: f_00005-5-1 loss: 0.507170  [   64/  146]
train() client id: f_00005-5-2 loss: 0.562093  [   96/  146]
train() client id: f_00005-5-3 loss: 0.484852  [  128/  146]
train() client id: f_00005-6-0 loss: 0.443163  [   32/  146]
train() client id: f_00005-6-1 loss: 0.323819  [   64/  146]
train() client id: f_00005-6-2 loss: 0.648166  [   96/  146]
train() client id: f_00005-6-3 loss: 0.430187  [  128/  146]
train() client id: f_00005-7-0 loss: 0.438159  [   32/  146]
train() client id: f_00005-7-1 loss: 0.507869  [   64/  146]
train() client id: f_00005-7-2 loss: 0.614999  [   96/  146]
train() client id: f_00005-7-3 loss: 0.419466  [  128/  146]
train() client id: f_00005-8-0 loss: 0.462725  [   32/  146]
train() client id: f_00005-8-1 loss: 0.438920  [   64/  146]
train() client id: f_00005-8-2 loss: 0.565503  [   96/  146]
train() client id: f_00005-8-3 loss: 0.519917  [  128/  146]
train() client id: f_00005-9-0 loss: 0.424774  [   32/  146]
train() client id: f_00005-9-1 loss: 0.529241  [   64/  146]
train() client id: f_00005-9-2 loss: 0.427246  [   96/  146]
train() client id: f_00005-9-3 loss: 0.557841  [  128/  146]
train() client id: f_00005-10-0 loss: 0.635985  [   32/  146]
train() client id: f_00005-10-1 loss: 0.470314  [   64/  146]
train() client id: f_00005-10-2 loss: 0.385125  [   96/  146]
train() client id: f_00005-10-3 loss: 0.545562  [  128/  146]
train() client id: f_00005-11-0 loss: 0.420158  [   32/  146]
train() client id: f_00005-11-1 loss: 0.534305  [   64/  146]
train() client id: f_00005-11-2 loss: 0.555850  [   96/  146]
train() client id: f_00005-11-3 loss: 0.390971  [  128/  146]
train() client id: f_00006-0-0 loss: 0.457393  [   32/   54]
train() client id: f_00006-1-0 loss: 0.505001  [   32/   54]
train() client id: f_00006-2-0 loss: 0.501540  [   32/   54]
train() client id: f_00006-3-0 loss: 0.433626  [   32/   54]
train() client id: f_00006-4-0 loss: 0.488132  [   32/   54]
train() client id: f_00006-5-0 loss: 0.482878  [   32/   54]
train() client id: f_00006-6-0 loss: 0.552798  [   32/   54]
train() client id: f_00006-7-0 loss: 0.485435  [   32/   54]
train() client id: f_00006-8-0 loss: 0.537395  [   32/   54]
train() client id: f_00006-9-0 loss: 0.512882  [   32/   54]
train() client id: f_00006-10-0 loss: 0.483291  [   32/   54]
train() client id: f_00006-11-0 loss: 0.480578  [   32/   54]
train() client id: f_00007-0-0 loss: 0.659375  [   32/  179]
train() client id: f_00007-0-1 loss: 0.806644  [   64/  179]
train() client id: f_00007-0-2 loss: 0.829670  [   96/  179]
train() client id: f_00007-0-3 loss: 0.808253  [  128/  179]
train() client id: f_00007-0-4 loss: 0.680522  [  160/  179]
train() client id: f_00007-1-0 loss: 0.639748  [   32/  179]
train() client id: f_00007-1-1 loss: 0.794264  [   64/  179]
train() client id: f_00007-1-2 loss: 0.724687  [   96/  179]
train() client id: f_00007-1-3 loss: 0.732799  [  128/  179]
train() client id: f_00007-1-4 loss: 0.588081  [  160/  179]
train() client id: f_00007-2-0 loss: 0.689837  [   32/  179]
train() client id: f_00007-2-1 loss: 0.600773  [   64/  179]
train() client id: f_00007-2-2 loss: 0.736644  [   96/  179]
train() client id: f_00007-2-3 loss: 0.596907  [  128/  179]
train() client id: f_00007-2-4 loss: 0.930047  [  160/  179]
train() client id: f_00007-3-0 loss: 0.558290  [   32/  179]
train() client id: f_00007-3-1 loss: 0.787491  [   64/  179]
train() client id: f_00007-3-2 loss: 0.649082  [   96/  179]
train() client id: f_00007-3-3 loss: 0.847864  [  128/  179]
train() client id: f_00007-3-4 loss: 0.738315  [  160/  179]
train() client id: f_00007-4-0 loss: 0.605851  [   32/  179]
train() client id: f_00007-4-1 loss: 0.579621  [   64/  179]
train() client id: f_00007-4-2 loss: 0.886556  [   96/  179]
train() client id: f_00007-4-3 loss: 0.803440  [  128/  179]
train() client id: f_00007-4-4 loss: 0.670748  [  160/  179]
train() client id: f_00007-5-0 loss: 0.649998  [   32/  179]
train() client id: f_00007-5-1 loss: 0.981432  [   64/  179]
train() client id: f_00007-5-2 loss: 0.580393  [   96/  179]
train() client id: f_00007-5-3 loss: 0.766716  [  128/  179]
train() client id: f_00007-5-4 loss: 0.532467  [  160/  179]
train() client id: f_00007-6-0 loss: 0.577231  [   32/  179]
train() client id: f_00007-6-1 loss: 0.844295  [   64/  179]
train() client id: f_00007-6-2 loss: 0.739247  [   96/  179]
train() client id: f_00007-6-3 loss: 0.725218  [  128/  179]
train() client id: f_00007-6-4 loss: 0.572362  [  160/  179]
train() client id: f_00007-7-0 loss: 0.834704  [   32/  179]
train() client id: f_00007-7-1 loss: 0.705909  [   64/  179]
train() client id: f_00007-7-2 loss: 0.644349  [   96/  179]
train() client id: f_00007-7-3 loss: 0.732557  [  128/  179]
train() client id: f_00007-7-4 loss: 0.655902  [  160/  179]
train() client id: f_00007-8-0 loss: 0.591863  [   32/  179]
train() client id: f_00007-8-1 loss: 0.821563  [   64/  179]
train() client id: f_00007-8-2 loss: 0.827206  [   96/  179]
train() client id: f_00007-8-3 loss: 0.634900  [  128/  179]
train() client id: f_00007-8-4 loss: 0.668417  [  160/  179]
train() client id: f_00007-9-0 loss: 0.663556  [   32/  179]
train() client id: f_00007-9-1 loss: 0.833235  [   64/  179]
train() client id: f_00007-9-2 loss: 0.656074  [   96/  179]
train() client id: f_00007-9-3 loss: 0.633427  [  128/  179]
train() client id: f_00007-9-4 loss: 0.694012  [  160/  179]
train() client id: f_00007-10-0 loss: 0.729914  [   32/  179]
train() client id: f_00007-10-1 loss: 0.651112  [   64/  179]
train() client id: f_00007-10-2 loss: 0.578257  [   96/  179]
train() client id: f_00007-10-3 loss: 0.551147  [  128/  179]
train() client id: f_00007-10-4 loss: 0.846266  [  160/  179]
train() client id: f_00007-11-0 loss: 0.660871  [   32/  179]
train() client id: f_00007-11-1 loss: 0.714163  [   64/  179]
train() client id: f_00007-11-2 loss: 0.719702  [   96/  179]
train() client id: f_00007-11-3 loss: 0.657738  [  128/  179]
train() client id: f_00007-11-4 loss: 0.631818  [  160/  179]
train() client id: f_00008-0-0 loss: 0.640179  [   32/  130]
train() client id: f_00008-0-1 loss: 0.809165  [   64/  130]
train() client id: f_00008-0-2 loss: 0.678371  [   96/  130]
train() client id: f_00008-0-3 loss: 0.800391  [  128/  130]
train() client id: f_00008-1-0 loss: 0.685650  [   32/  130]
train() client id: f_00008-1-1 loss: 0.763769  [   64/  130]
train() client id: f_00008-1-2 loss: 0.800954  [   96/  130]
train() client id: f_00008-1-3 loss: 0.665040  [  128/  130]
train() client id: f_00008-2-0 loss: 0.739646  [   32/  130]
train() client id: f_00008-2-1 loss: 0.639025  [   64/  130]
train() client id: f_00008-2-2 loss: 0.760239  [   96/  130]
train() client id: f_00008-2-3 loss: 0.743504  [  128/  130]
train() client id: f_00008-3-0 loss: 0.662575  [   32/  130]
train() client id: f_00008-3-1 loss: 0.756721  [   64/  130]
train() client id: f_00008-3-2 loss: 0.847122  [   96/  130]
train() client id: f_00008-3-3 loss: 0.660837  [  128/  130]
train() client id: f_00008-4-0 loss: 0.637762  [   32/  130]
train() client id: f_00008-4-1 loss: 0.731459  [   64/  130]
train() client id: f_00008-4-2 loss: 0.746111  [   96/  130]
train() client id: f_00008-4-3 loss: 0.746356  [  128/  130]
train() client id: f_00008-5-0 loss: 0.836585  [   32/  130]
train() client id: f_00008-5-1 loss: 0.785244  [   64/  130]
train() client id: f_00008-5-2 loss: 0.636678  [   96/  130]
train() client id: f_00008-5-3 loss: 0.675373  [  128/  130]
train() client id: f_00008-6-0 loss: 0.688693  [   32/  130]
train() client id: f_00008-6-1 loss: 0.651857  [   64/  130]
train() client id: f_00008-6-2 loss: 0.806797  [   96/  130]
train() client id: f_00008-6-3 loss: 0.777723  [  128/  130]
train() client id: f_00008-7-0 loss: 0.795115  [   32/  130]
train() client id: f_00008-7-1 loss: 0.610628  [   64/  130]
train() client id: f_00008-7-2 loss: 0.866743  [   96/  130]
train() client id: f_00008-7-3 loss: 0.655264  [  128/  130]
train() client id: f_00008-8-0 loss: 0.740135  [   32/  130]
train() client id: f_00008-8-1 loss: 0.667240  [   64/  130]
train() client id: f_00008-8-2 loss: 0.653264  [   96/  130]
train() client id: f_00008-8-3 loss: 0.833767  [  128/  130]
train() client id: f_00008-9-0 loss: 0.756145  [   32/  130]
train() client id: f_00008-9-1 loss: 0.718679  [   64/  130]
train() client id: f_00008-9-2 loss: 0.653671  [   96/  130]
train() client id: f_00008-9-3 loss: 0.791445  [  128/  130]
train() client id: f_00008-10-0 loss: 0.721258  [   32/  130]
train() client id: f_00008-10-1 loss: 0.762958  [   64/  130]
train() client id: f_00008-10-2 loss: 0.768618  [   96/  130]
train() client id: f_00008-10-3 loss: 0.671755  [  128/  130]
train() client id: f_00008-11-0 loss: 0.813497  [   32/  130]
train() client id: f_00008-11-1 loss: 0.690957  [   64/  130]
train() client id: f_00008-11-2 loss: 0.786108  [   96/  130]
train() client id: f_00008-11-3 loss: 0.622567  [  128/  130]
train() client id: f_00009-0-0 loss: 1.250997  [   32/  118]
train() client id: f_00009-0-1 loss: 1.249350  [   64/  118]
train() client id: f_00009-0-2 loss: 1.095667  [   96/  118]
train() client id: f_00009-1-0 loss: 1.251442  [   32/  118]
train() client id: f_00009-1-1 loss: 1.057663  [   64/  118]
train() client id: f_00009-1-2 loss: 0.976258  [   96/  118]
train() client id: f_00009-2-0 loss: 1.130755  [   32/  118]
train() client id: f_00009-2-1 loss: 0.957555  [   64/  118]
train() client id: f_00009-2-2 loss: 1.107514  [   96/  118]
train() client id: f_00009-3-0 loss: 0.871825  [   32/  118]
train() client id: f_00009-3-1 loss: 1.127253  [   64/  118]
train() client id: f_00009-3-2 loss: 0.997984  [   96/  118]
train() client id: f_00009-4-0 loss: 1.063388  [   32/  118]
train() client id: f_00009-4-1 loss: 1.047326  [   64/  118]
train() client id: f_00009-4-2 loss: 0.973871  [   96/  118]
train() client id: f_00009-5-0 loss: 1.149371  [   32/  118]
train() client id: f_00009-5-1 loss: 1.074218  [   64/  118]
train() client id: f_00009-5-2 loss: 0.829175  [   96/  118]
train() client id: f_00009-6-0 loss: 1.038472  [   32/  118]
train() client id: f_00009-6-1 loss: 0.903048  [   64/  118]
train() client id: f_00009-6-2 loss: 0.995675  [   96/  118]
train() client id: f_00009-7-0 loss: 1.109262  [   32/  118]
train() client id: f_00009-7-1 loss: 1.008333  [   64/  118]
train() client id: f_00009-7-2 loss: 0.878558  [   96/  118]
train() client id: f_00009-8-0 loss: 0.942512  [   32/  118]
train() client id: f_00009-8-1 loss: 0.934835  [   64/  118]
train() client id: f_00009-8-2 loss: 1.088660  [   96/  118]
train() client id: f_00009-9-0 loss: 0.961516  [   32/  118]
train() client id: f_00009-9-1 loss: 1.050362  [   64/  118]
train() client id: f_00009-9-2 loss: 0.949348  [   96/  118]
train() client id: f_00009-10-0 loss: 1.010538  [   32/  118]
train() client id: f_00009-10-1 loss: 0.934665  [   64/  118]
train() client id: f_00009-10-2 loss: 0.941536  [   96/  118]
train() client id: f_00009-11-0 loss: 1.143703  [   32/  118]
train() client id: f_00009-11-1 loss: 0.890015  [   64/  118]
train() client id: f_00009-11-2 loss: 0.896872  [   96/  118]
At round 28 accuracy: 0.649867374005305
At round 28 training accuracy: 0.5821596244131455
At round 28 training loss: 0.831706515880018
update_location
xs = [  -3.9056584     4.20031788  160.00902392   18.81129433    0.97929623
    3.95640986 -122.44319194 -101.32485185  144.66397685  -87.06087855]
ys = [ 152.5879595   135.55583871    1.32061395 -122.45517586  114.35018685
   97.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [182.47832625 168.50230882 188.69189637 159.21411649 151.91090893
 139.94020603 158.11143474 142.36362547 176.73804393 132.64843923]
dists_bs = [172.55600043 183.49004274 377.35338245 355.02450543 186.13734858
 194.89241123 185.23728871 189.11869796 356.32790511 192.28567891]
uav_gains = [2.19565925e-11 2.70052809e-11 2.00716392e-11 3.11928652e-11
 3.51147715e-11 4.31483308e-11 3.17457777e-11 4.13309852e-11
 2.38781215e-11 4.93353388e-11]
bs_gains = [6.02406849e-11 5.07199808e-11 6.73595215e-12 7.99044117e-12
 4.87259393e-11 4.28418567e-11 4.93917622e-11 4.66055429e-11
 7.90887203e-12 4.44879766e-11]
Round 29
-------------------------------
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.16426333 14.86448857  7.05664821  2.53741575 17.14489673  8.25364943
  3.14835783 10.08926783  7.43869069  6.69596016]
obj_prev = 84.39363852739328
eta_min = 1.1633239161667213e-13	eta_max = 0.9255734667757686
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 19.601082838327166	eta = 0.9090909090909091
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 35.06532785864593	eta = 0.5081705292616409
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 27.55402889869821	eta = 0.6466991191078749
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.19954622843339	eta = 0.6801326275385099
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.129948192651668	eta = 0.6819441847065052
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.129750162898475	eta = 0.6819493529625255
eta = 0.6819493529625255
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [0.03158773 0.06643451 0.03108634 0.01077994 0.07671306 0.03660165
 0.0135376  0.04487463 0.03259051 0.02958216]
ene_total = [2.30902016 4.22374425 2.291367   1.07582749 4.81690287 2.52674818
 1.23224478 3.00469125 2.52859005 2.12061414]
ti_comp = [0.41103184 0.42578996 0.40909461 0.41788221 0.42519616 0.42322413
 0.41819745 0.42265509 0.38279172 0.42381267]
ti_coms = [0.08655269 0.07179457 0.08848992 0.07970232 0.07238837 0.0743604
 0.07938708 0.07492943 0.11479281 0.07377186]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.16596177e-05 1.01081200e-04 1.12186639e-05 4.48354160e-07
 1.56066329e-04 1.71096574e-05 8.86629391e-07 3.16162585e-05
 1.47648345e-05 9.00786229e-06]
ene_total = [0.49728187 0.41773503 0.50837179 0.45733327 0.42429695 0.4276389
 0.45554967 0.43173622 0.65949319 0.42379718]
optimize_network_iter = 0 obj = 4.703234079287297
eta = 0.6819493529625255
freqs = [38424917.91998439 78013241.65854445 37994067.71862566 12898298.99218661
 90209018.95065838 43241454.64083756 16185653.82662158 53086584.80672698
 42569513.0112679  34900040.52352524]
eta_min = 0.6819493529625276	eta_max = 0.6819493529625246
af = 0.016869127524841664	bf = 1.4894091419856037	zeta = 0.018556040277325832	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [2.90303184e-06 2.51673727e-05 2.79324241e-06 1.11631997e-07
 3.88576653e-05 4.25999219e-06 2.20754524e-07 7.87187088e-06
 3.67617412e-06 2.24279318e-06]
ene_total = [1.73801127 1.44623117 1.77687644 1.59993898 1.46089904 1.49353974
 1.59363288 1.50568747 2.3050487  1.48132066]
ti_comp = [0.41103184 0.42578996 0.40909461 0.41788221 0.42519616 0.42322413
 0.41819745 0.42265509 0.38279172 0.42381267]
ti_coms = [0.08655269 0.07179457 0.08848992 0.07970232 0.07238837 0.0743604
 0.07938708 0.07492943 0.11479281 0.07377186]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.16596177e-05 1.01081200e-04 1.12186639e-05 4.48354160e-07
 1.56066329e-04 1.71096574e-05 8.86629391e-07 3.16162585e-05
 1.47648345e-05 9.00786229e-06]
ene_total = [0.49728187 0.41773503 0.50837179 0.45733327 0.42429695 0.4276389
 0.45554967 0.43173622 0.65949319 0.42379718]
optimize_network_iter = 1 obj = 4.703234079287284
eta = 0.6819493529625246
freqs = [38424917.91998437 78013241.65854444 37994067.71862565 12898298.99218661
 90209018.95065837 43241454.64083755 16185653.82662157 53086584.80672697
 42569513.01126787 34900040.52352523]
Done!
At round 29 eta: 0.6819493529625246
At round 29 local rounds: 12.534813137868813
At round 29 global rounds: 57.376941264307334
At round 29 a_n: 17.906227447174857
gradient difference: 0.41800111532211304
train() client id: f_00000-0-0 loss: 1.525754  [   32/  126]
train() client id: f_00000-0-1 loss: 1.358752  [   64/  126]
train() client id: f_00000-0-2 loss: 1.297786  [   96/  126]
train() client id: f_00000-1-0 loss: 1.054027  [   32/  126]
train() client id: f_00000-1-1 loss: 1.252442  [   64/  126]
train() client id: f_00000-1-2 loss: 1.168772  [   96/  126]
train() client id: f_00000-2-0 loss: 1.099451  [   32/  126]
train() client id: f_00000-2-1 loss: 1.001658  [   64/  126]
train() client id: f_00000-2-2 loss: 1.141671  [   96/  126]
train() client id: f_00000-3-0 loss: 1.103232  [   32/  126]
train() client id: f_00000-3-1 loss: 0.946800  [   64/  126]
train() client id: f_00000-3-2 loss: 1.107333  [   96/  126]
train() client id: f_00000-4-0 loss: 1.042997  [   32/  126]
train() client id: f_00000-4-1 loss: 1.138027  [   64/  126]
train() client id: f_00000-4-2 loss: 0.916533  [   96/  126]
train() client id: f_00000-5-0 loss: 1.030093  [   32/  126]
train() client id: f_00000-5-1 loss: 0.931682  [   64/  126]
train() client id: f_00000-5-2 loss: 0.862011  [   96/  126]
train() client id: f_00000-6-0 loss: 0.964263  [   32/  126]
train() client id: f_00000-6-1 loss: 0.774134  [   64/  126]
train() client id: f_00000-6-2 loss: 1.034340  [   96/  126]
train() client id: f_00000-7-0 loss: 0.843558  [   32/  126]
train() client id: f_00000-7-1 loss: 1.041365  [   64/  126]
train() client id: f_00000-7-2 loss: 0.815520  [   96/  126]
train() client id: f_00000-8-0 loss: 0.781092  [   32/  126]
train() client id: f_00000-8-1 loss: 0.996207  [   64/  126]
train() client id: f_00000-8-2 loss: 0.802266  [   96/  126]
train() client id: f_00000-9-0 loss: 0.822911  [   32/  126]
train() client id: f_00000-9-1 loss: 0.786618  [   64/  126]
train() client id: f_00000-9-2 loss: 0.952037  [   96/  126]
train() client id: f_00000-10-0 loss: 0.841675  [   32/  126]
train() client id: f_00000-10-1 loss: 0.882653  [   64/  126]
train() client id: f_00000-10-2 loss: 0.761048  [   96/  126]
train() client id: f_00000-11-0 loss: 0.779912  [   32/  126]
train() client id: f_00000-11-1 loss: 1.074711  [   64/  126]
train() client id: f_00000-11-2 loss: 0.789752  [   96/  126]
train() client id: f_00001-0-0 loss: 0.550866  [   32/  265]
train() client id: f_00001-0-1 loss: 0.554789  [   64/  265]
train() client id: f_00001-0-2 loss: 0.554909  [   96/  265]
train() client id: f_00001-0-3 loss: 0.459995  [  128/  265]
train() client id: f_00001-0-4 loss: 0.460080  [  160/  265]
train() client id: f_00001-0-5 loss: 0.495731  [  192/  265]
train() client id: f_00001-0-6 loss: 0.506004  [  224/  265]
train() client id: f_00001-0-7 loss: 0.489625  [  256/  265]
train() client id: f_00001-1-0 loss: 0.523674  [   32/  265]
train() client id: f_00001-1-1 loss: 0.548046  [   64/  265]
train() client id: f_00001-1-2 loss: 0.483357  [   96/  265]
train() client id: f_00001-1-3 loss: 0.531152  [  128/  265]
train() client id: f_00001-1-4 loss: 0.499635  [  160/  265]
train() client id: f_00001-1-5 loss: 0.483059  [  192/  265]
train() client id: f_00001-1-6 loss: 0.565311  [  224/  265]
train() client id: f_00001-1-7 loss: 0.401441  [  256/  265]
train() client id: f_00001-2-0 loss: 0.500550  [   32/  265]
train() client id: f_00001-2-1 loss: 0.593888  [   64/  265]
train() client id: f_00001-2-2 loss: 0.477250  [   96/  265]
train() client id: f_00001-2-3 loss: 0.411219  [  128/  265]
train() client id: f_00001-2-4 loss: 0.611569  [  160/  265]
train() client id: f_00001-2-5 loss: 0.405899  [  192/  265]
train() client id: f_00001-2-6 loss: 0.488151  [  224/  265]
train() client id: f_00001-2-7 loss: 0.578754  [  256/  265]
train() client id: f_00001-3-0 loss: 0.502180  [   32/  265]
train() client id: f_00001-3-1 loss: 0.659222  [   64/  265]
train() client id: f_00001-3-2 loss: 0.608940  [   96/  265]
train() client id: f_00001-3-3 loss: 0.433229  [  128/  265]
train() client id: f_00001-3-4 loss: 0.508305  [  160/  265]
train() client id: f_00001-3-5 loss: 0.447488  [  192/  265]
train() client id: f_00001-3-6 loss: 0.457095  [  224/  265]
train() client id: f_00001-3-7 loss: 0.435837  [  256/  265]
train() client id: f_00001-4-0 loss: 0.504191  [   32/  265]
train() client id: f_00001-4-1 loss: 0.460974  [   64/  265]
train() client id: f_00001-4-2 loss: 0.397228  [   96/  265]
train() client id: f_00001-4-3 loss: 0.525218  [  128/  265]
train() client id: f_00001-4-4 loss: 0.473866  [  160/  265]
train() client id: f_00001-4-5 loss: 0.486709  [  192/  265]
train() client id: f_00001-4-6 loss: 0.468427  [  224/  265]
train() client id: f_00001-4-7 loss: 0.647539  [  256/  265]
train() client id: f_00001-5-0 loss: 0.473850  [   32/  265]
train() client id: f_00001-5-1 loss: 0.433679  [   64/  265]
train() client id: f_00001-5-2 loss: 0.617851  [   96/  265]
train() client id: f_00001-5-3 loss: 0.547316  [  128/  265]
train() client id: f_00001-5-4 loss: 0.439234  [  160/  265]
train() client id: f_00001-5-5 loss: 0.522100  [  192/  265]
train() client id: f_00001-5-6 loss: 0.478139  [  224/  265]
train() client id: f_00001-5-7 loss: 0.514408  [  256/  265]
train() client id: f_00001-6-0 loss: 0.414241  [   32/  265]
train() client id: f_00001-6-1 loss: 0.541326  [   64/  265]
train() client id: f_00001-6-2 loss: 0.692783  [   96/  265]
train() client id: f_00001-6-3 loss: 0.568384  [  128/  265]
train() client id: f_00001-6-4 loss: 0.524436  [  160/  265]
train() client id: f_00001-6-5 loss: 0.476541  [  192/  265]
train() client id: f_00001-6-6 loss: 0.391372  [  224/  265]
train() client id: f_00001-6-7 loss: 0.400610  [  256/  265]
train() client id: f_00001-7-0 loss: 0.474571  [   32/  265]
train() client id: f_00001-7-1 loss: 0.553781  [   64/  265]
train() client id: f_00001-7-2 loss: 0.549221  [   96/  265]
train() client id: f_00001-7-3 loss: 0.503793  [  128/  265]
train() client id: f_00001-7-4 loss: 0.582180  [  160/  265]
train() client id: f_00001-7-5 loss: 0.490539  [  192/  265]
train() client id: f_00001-7-6 loss: 0.374478  [  224/  265]
train() client id: f_00001-7-7 loss: 0.491845  [  256/  265]
train() client id: f_00001-8-0 loss: 0.545249  [   32/  265]
train() client id: f_00001-8-1 loss: 0.421240  [   64/  265]
train() client id: f_00001-8-2 loss: 0.593574  [   96/  265]
train() client id: f_00001-8-3 loss: 0.467944  [  128/  265]
train() client id: f_00001-8-4 loss: 0.500078  [  160/  265]
train() client id: f_00001-8-5 loss: 0.418495  [  192/  265]
train() client id: f_00001-8-6 loss: 0.508942  [  224/  265]
train() client id: f_00001-8-7 loss: 0.564456  [  256/  265]
train() client id: f_00001-9-0 loss: 0.547272  [   32/  265]
train() client id: f_00001-9-1 loss: 0.404956  [   64/  265]
train() client id: f_00001-9-2 loss: 0.529191  [   96/  265]
train() client id: f_00001-9-3 loss: 0.511595  [  128/  265]
train() client id: f_00001-9-4 loss: 0.651552  [  160/  265]
train() client id: f_00001-9-5 loss: 0.418174  [  192/  265]
train() client id: f_00001-9-6 loss: 0.464436  [  224/  265]
train() client id: f_00001-9-7 loss: 0.415814  [  256/  265]
train() client id: f_00001-10-0 loss: 0.488784  [   32/  265]
train() client id: f_00001-10-1 loss: 0.412917  [   64/  265]
train() client id: f_00001-10-2 loss: 0.606834  [   96/  265]
train() client id: f_00001-10-3 loss: 0.611719  [  128/  265]
train() client id: f_00001-10-4 loss: 0.407490  [  160/  265]
train() client id: f_00001-10-5 loss: 0.561713  [  192/  265]
train() client id: f_00001-10-6 loss: 0.391459  [  224/  265]
train() client id: f_00001-10-7 loss: 0.560642  [  256/  265]
train() client id: f_00001-11-0 loss: 0.478782  [   32/  265]
train() client id: f_00001-11-1 loss: 0.635411  [   64/  265]
train() client id: f_00001-11-2 loss: 0.455042  [   96/  265]
train() client id: f_00001-11-3 loss: 0.464264  [  128/  265]
train() client id: f_00001-11-4 loss: 0.411268  [  160/  265]
train() client id: f_00001-11-5 loss: 0.412151  [  192/  265]
train() client id: f_00001-11-6 loss: 0.610501  [  224/  265]
train() client id: f_00001-11-7 loss: 0.506835  [  256/  265]
train() client id: f_00002-0-0 loss: 1.194704  [   32/  124]
train() client id: f_00002-0-1 loss: 1.309373  [   64/  124]
train() client id: f_00002-0-2 loss: 1.119525  [   96/  124]
train() client id: f_00002-1-0 loss: 1.135222  [   32/  124]
train() client id: f_00002-1-1 loss: 1.080790  [   64/  124]
train() client id: f_00002-1-2 loss: 1.299657  [   96/  124]
train() client id: f_00002-2-0 loss: 1.130579  [   32/  124]
train() client id: f_00002-2-1 loss: 1.127265  [   64/  124]
train() client id: f_00002-2-2 loss: 1.094967  [   96/  124]
train() client id: f_00002-3-0 loss: 1.267223  [   32/  124]
train() client id: f_00002-3-1 loss: 1.156132  [   64/  124]
train() client id: f_00002-3-2 loss: 1.058089  [   96/  124]
train() client id: f_00002-4-0 loss: 1.042327  [   32/  124]
train() client id: f_00002-4-1 loss: 1.013960  [   64/  124]
train() client id: f_00002-4-2 loss: 1.126427  [   96/  124]
train() client id: f_00002-5-0 loss: 1.075689  [   32/  124]
train() client id: f_00002-5-1 loss: 1.028626  [   64/  124]
train() client id: f_00002-5-2 loss: 1.143772  [   96/  124]
train() client id: f_00002-6-0 loss: 1.040157  [   32/  124]
train() client id: f_00002-6-1 loss: 1.006638  [   64/  124]
train() client id: f_00002-6-2 loss: 1.152066  [   96/  124]
train() client id: f_00002-7-0 loss: 1.110590  [   32/  124]
train() client id: f_00002-7-1 loss: 1.136847  [   64/  124]
train() client id: f_00002-7-2 loss: 0.955489  [   96/  124]
train() client id: f_00002-8-0 loss: 1.010216  [   32/  124]
train() client id: f_00002-8-1 loss: 1.121235  [   64/  124]
train() client id: f_00002-8-2 loss: 1.077018  [   96/  124]
train() client id: f_00002-9-0 loss: 0.900209  [   32/  124]
train() client id: f_00002-9-1 loss: 1.108651  [   64/  124]
train() client id: f_00002-9-2 loss: 1.092385  [   96/  124]
train() client id: f_00002-10-0 loss: 1.105755  [   32/  124]
train() client id: f_00002-10-1 loss: 0.951087  [   64/  124]
train() client id: f_00002-10-2 loss: 1.023903  [   96/  124]
train() client id: f_00002-11-0 loss: 1.152024  [   32/  124]
train() client id: f_00002-11-1 loss: 0.919427  [   64/  124]
train() client id: f_00002-11-2 loss: 1.179044  [   96/  124]
train() client id: f_00003-0-0 loss: 0.755800  [   32/   43]
train() client id: f_00003-1-0 loss: 0.877752  [   32/   43]
train() client id: f_00003-2-0 loss: 0.900600  [   32/   43]
train() client id: f_00003-3-0 loss: 1.094959  [   32/   43]
train() client id: f_00003-4-0 loss: 0.922861  [   32/   43]
train() client id: f_00003-5-0 loss: 0.728773  [   32/   43]
train() client id: f_00003-6-0 loss: 0.826040  [   32/   43]
train() client id: f_00003-7-0 loss: 0.884523  [   32/   43]
train() client id: f_00003-8-0 loss: 0.798827  [   32/   43]
train() client id: f_00003-9-0 loss: 0.798571  [   32/   43]
train() client id: f_00003-10-0 loss: 0.940072  [   32/   43]
train() client id: f_00003-11-0 loss: 0.811274  [   32/   43]
train() client id: f_00004-0-0 loss: 0.954536  [   32/  306]
train() client id: f_00004-0-1 loss: 0.987677  [   64/  306]
train() client id: f_00004-0-2 loss: 1.121544  [   96/  306]
train() client id: f_00004-0-3 loss: 0.988417  [  128/  306]
train() client id: f_00004-0-4 loss: 0.899271  [  160/  306]
train() client id: f_00004-0-5 loss: 0.853563  [  192/  306]
train() client id: f_00004-0-6 loss: 0.943495  [  224/  306]
train() client id: f_00004-0-7 loss: 0.989276  [  256/  306]
train() client id: f_00004-0-8 loss: 0.971468  [  288/  306]
train() client id: f_00004-1-0 loss: 0.842686  [   32/  306]
train() client id: f_00004-1-1 loss: 0.990152  [   64/  306]
train() client id: f_00004-1-2 loss: 0.985468  [   96/  306]
train() client id: f_00004-1-3 loss: 1.036343  [  128/  306]
train() client id: f_00004-1-4 loss: 0.945076  [  160/  306]
train() client id: f_00004-1-5 loss: 0.857096  [  192/  306]
train() client id: f_00004-1-6 loss: 1.082121  [  224/  306]
train() client id: f_00004-1-7 loss: 1.024371  [  256/  306]
train() client id: f_00004-1-8 loss: 1.014458  [  288/  306]
train() client id: f_00004-2-0 loss: 0.903718  [   32/  306]
train() client id: f_00004-2-1 loss: 1.097548  [   64/  306]
train() client id: f_00004-2-2 loss: 0.874896  [   96/  306]
train() client id: f_00004-2-3 loss: 0.928149  [  128/  306]
train() client id: f_00004-2-4 loss: 1.170390  [  160/  306]
train() client id: f_00004-2-5 loss: 0.946291  [  192/  306]
train() client id: f_00004-2-6 loss: 0.826883  [  224/  306]
train() client id: f_00004-2-7 loss: 0.934722  [  256/  306]
train() client id: f_00004-2-8 loss: 1.006055  [  288/  306]
train() client id: f_00004-3-0 loss: 0.919805  [   32/  306]
train() client id: f_00004-3-1 loss: 0.903726  [   64/  306]
train() client id: f_00004-3-2 loss: 0.877813  [   96/  306]
train() client id: f_00004-3-3 loss: 1.046898  [  128/  306]
train() client id: f_00004-3-4 loss: 0.907432  [  160/  306]
train() client id: f_00004-3-5 loss: 1.046113  [  192/  306]
train() client id: f_00004-3-6 loss: 0.933089  [  224/  306]
train() client id: f_00004-3-7 loss: 1.029318  [  256/  306]
train() client id: f_00004-3-8 loss: 0.972717  [  288/  306]
train() client id: f_00004-4-0 loss: 1.090050  [   32/  306]
train() client id: f_00004-4-1 loss: 1.018955  [   64/  306]
train() client id: f_00004-4-2 loss: 0.911587  [   96/  306]
train() client id: f_00004-4-3 loss: 0.959015  [  128/  306]
train() client id: f_00004-4-4 loss: 1.001001  [  160/  306]
train() client id: f_00004-4-5 loss: 0.884226  [  192/  306]
train() client id: f_00004-4-6 loss: 0.865551  [  224/  306]
train() client id: f_00004-4-7 loss: 0.926451  [  256/  306]
train() client id: f_00004-4-8 loss: 0.986611  [  288/  306]
train() client id: f_00004-5-0 loss: 0.999998  [   32/  306]
train() client id: f_00004-5-1 loss: 0.965031  [   64/  306]
train() client id: f_00004-5-2 loss: 0.937881  [   96/  306]
train() client id: f_00004-5-3 loss: 0.911113  [  128/  306]
train() client id: f_00004-5-4 loss: 1.007853  [  160/  306]
train() client id: f_00004-5-5 loss: 0.908889  [  192/  306]
train() client id: f_00004-5-6 loss: 0.975326  [  224/  306]
train() client id: f_00004-5-7 loss: 0.896447  [  256/  306]
train() client id: f_00004-5-8 loss: 0.983166  [  288/  306]
train() client id: f_00004-6-0 loss: 0.983892  [   32/  306]
train() client id: f_00004-6-1 loss: 0.911767  [   64/  306]
train() client id: f_00004-6-2 loss: 0.988235  [   96/  306]
train() client id: f_00004-6-3 loss: 1.031926  [  128/  306]
train() client id: f_00004-6-4 loss: 0.950457  [  160/  306]
train() client id: f_00004-6-5 loss: 0.949447  [  192/  306]
train() client id: f_00004-6-6 loss: 0.797017  [  224/  306]
train() client id: f_00004-6-7 loss: 0.967505  [  256/  306]
train() client id: f_00004-6-8 loss: 1.077364  [  288/  306]
train() client id: f_00004-7-0 loss: 0.974360  [   32/  306]
train() client id: f_00004-7-1 loss: 1.138123  [   64/  306]
train() client id: f_00004-7-2 loss: 0.810214  [   96/  306]
train() client id: f_00004-7-3 loss: 0.830578  [  128/  306]
train() client id: f_00004-7-4 loss: 0.986252  [  160/  306]
train() client id: f_00004-7-5 loss: 0.899286  [  192/  306]
train() client id: f_00004-7-6 loss: 0.960725  [  224/  306]
train() client id: f_00004-7-7 loss: 0.936541  [  256/  306]
train() client id: f_00004-7-8 loss: 1.023917  [  288/  306]
train() client id: f_00004-8-0 loss: 0.933297  [   32/  306]
train() client id: f_00004-8-1 loss: 1.072926  [   64/  306]
train() client id: f_00004-8-2 loss: 0.812476  [   96/  306]
train() client id: f_00004-8-3 loss: 0.910195  [  128/  306]
train() client id: f_00004-8-4 loss: 0.972522  [  160/  306]
train() client id: f_00004-8-5 loss: 0.922289  [  192/  306]
train() client id: f_00004-8-6 loss: 1.055968  [  224/  306]
train() client id: f_00004-8-7 loss: 0.871928  [  256/  306]
train() client id: f_00004-8-8 loss: 1.015267  [  288/  306]
train() client id: f_00004-9-0 loss: 0.943527  [   32/  306]
train() client id: f_00004-9-1 loss: 0.890107  [   64/  306]
train() client id: f_00004-9-2 loss: 0.935540  [   96/  306]
train() client id: f_00004-9-3 loss: 1.045982  [  128/  306]
train() client id: f_00004-9-4 loss: 0.989382  [  160/  306]
train() client id: f_00004-9-5 loss: 0.993086  [  192/  306]
train() client id: f_00004-9-6 loss: 0.985187  [  224/  306]
train() client id: f_00004-9-7 loss: 0.995274  [  256/  306]
train() client id: f_00004-9-8 loss: 0.851506  [  288/  306]
train() client id: f_00004-10-0 loss: 0.965109  [   32/  306]
train() client id: f_00004-10-1 loss: 0.977676  [   64/  306]
train() client id: f_00004-10-2 loss: 0.994602  [   96/  306]
train() client id: f_00004-10-3 loss: 0.995449  [  128/  306]
train() client id: f_00004-10-4 loss: 0.987473  [  160/  306]
train() client id: f_00004-10-5 loss: 0.939325  [  192/  306]
train() client id: f_00004-10-6 loss: 0.926276  [  224/  306]
train() client id: f_00004-10-7 loss: 0.871365  [  256/  306]
train() client id: f_00004-10-8 loss: 0.872953  [  288/  306]
train() client id: f_00004-11-0 loss: 1.026569  [   32/  306]
train() client id: f_00004-11-1 loss: 0.896591  [   64/  306]
train() client id: f_00004-11-2 loss: 0.899558  [   96/  306]
train() client id: f_00004-11-3 loss: 0.981954  [  128/  306]
train() client id: f_00004-11-4 loss: 0.985355  [  160/  306]
train() client id: f_00004-11-5 loss: 0.985432  [  192/  306]
train() client id: f_00004-11-6 loss: 0.918779  [  224/  306]
train() client id: f_00004-11-7 loss: 0.878927  [  256/  306]
train() client id: f_00004-11-8 loss: 1.022331  [  288/  306]
train() client id: f_00005-0-0 loss: 0.739425  [   32/  146]
train() client id: f_00005-0-1 loss: 0.576411  [   64/  146]
train() client id: f_00005-0-2 loss: 0.651803  [   96/  146]
train() client id: f_00005-0-3 loss: 0.878572  [  128/  146]
train() client id: f_00005-1-0 loss: 0.709886  [   32/  146]
train() client id: f_00005-1-1 loss: 0.808372  [   64/  146]
train() client id: f_00005-1-2 loss: 0.844549  [   96/  146]
train() client id: f_00005-1-3 loss: 0.630186  [  128/  146]
train() client id: f_00005-2-0 loss: 0.697947  [   32/  146]
train() client id: f_00005-2-1 loss: 0.624212  [   64/  146]
train() client id: f_00005-2-2 loss: 1.079206  [   96/  146]
train() client id: f_00005-2-3 loss: 0.531389  [  128/  146]
train() client id: f_00005-3-0 loss: 0.799358  [   32/  146]
train() client id: f_00005-3-1 loss: 0.916843  [   64/  146]
train() client id: f_00005-3-2 loss: 0.601004  [   96/  146]
train() client id: f_00005-3-3 loss: 0.697851  [  128/  146]
train() client id: f_00005-4-0 loss: 0.685910  [   32/  146]
train() client id: f_00005-4-1 loss: 0.730383  [   64/  146]
train() client id: f_00005-4-2 loss: 0.662862  [   96/  146]
train() client id: f_00005-4-3 loss: 0.792171  [  128/  146]
train() client id: f_00005-5-0 loss: 0.694397  [   32/  146]
train() client id: f_00005-5-1 loss: 0.888434  [   64/  146]
train() client id: f_00005-5-2 loss: 0.663770  [   96/  146]
train() client id: f_00005-5-3 loss: 0.725717  [  128/  146]
train() client id: f_00005-6-0 loss: 0.745864  [   32/  146]
train() client id: f_00005-6-1 loss: 0.643678  [   64/  146]
train() client id: f_00005-6-2 loss: 0.760478  [   96/  146]
train() client id: f_00005-6-3 loss: 0.703300  [  128/  146]
train() client id: f_00005-7-0 loss: 0.868179  [   32/  146]
train() client id: f_00005-7-1 loss: 0.644153  [   64/  146]
train() client id: f_00005-7-2 loss: 0.644614  [   96/  146]
train() client id: f_00005-7-3 loss: 0.649670  [  128/  146]
train() client id: f_00005-8-0 loss: 0.494033  [   32/  146]
train() client id: f_00005-8-1 loss: 0.725455  [   64/  146]
train() client id: f_00005-8-2 loss: 0.744437  [   96/  146]
train() client id: f_00005-8-3 loss: 0.873818  [  128/  146]
train() client id: f_00005-9-0 loss: 0.697822  [   32/  146]
train() client id: f_00005-9-1 loss: 0.585693  [   64/  146]
train() client id: f_00005-9-2 loss: 0.745759  [   96/  146]
train() client id: f_00005-9-3 loss: 0.623133  [  128/  146]
train() client id: f_00005-10-0 loss: 0.547814  [   32/  146]
train() client id: f_00005-10-1 loss: 0.712426  [   64/  146]
train() client id: f_00005-10-2 loss: 0.919273  [   96/  146]
train() client id: f_00005-10-3 loss: 0.812270  [  128/  146]
train() client id: f_00005-11-0 loss: 0.766083  [   32/  146]
train() client id: f_00005-11-1 loss: 0.503165  [   64/  146]
train() client id: f_00005-11-2 loss: 0.726297  [   96/  146]
train() client id: f_00005-11-3 loss: 0.819281  [  128/  146]
train() client id: f_00006-0-0 loss: 0.560260  [   32/   54]
train() client id: f_00006-1-0 loss: 0.523675  [   32/   54]
train() client id: f_00006-2-0 loss: 0.544469  [   32/   54]
train() client id: f_00006-3-0 loss: 0.497410  [   32/   54]
train() client id: f_00006-4-0 loss: 0.542426  [   32/   54]
train() client id: f_00006-5-0 loss: 0.524111  [   32/   54]
train() client id: f_00006-6-0 loss: 0.491567  [   32/   54]
train() client id: f_00006-7-0 loss: 0.505377  [   32/   54]
train() client id: f_00006-8-0 loss: 0.510386  [   32/   54]
train() client id: f_00006-9-0 loss: 0.482177  [   32/   54]
train() client id: f_00006-10-0 loss: 0.523415  [   32/   54]
train() client id: f_00006-11-0 loss: 0.486980  [   32/   54]
train() client id: f_00007-0-0 loss: 0.692896  [   32/  179]
train() client id: f_00007-0-1 loss: 0.618303  [   64/  179]
train() client id: f_00007-0-2 loss: 0.755021  [   96/  179]
train() client id: f_00007-0-3 loss: 0.726063  [  128/  179]
train() client id: f_00007-0-4 loss: 0.491911  [  160/  179]
train() client id: f_00007-1-0 loss: 0.590203  [   32/  179]
train() client id: f_00007-1-1 loss: 0.514963  [   64/  179]
train() client id: f_00007-1-2 loss: 0.655740  [   96/  179]
train() client id: f_00007-1-3 loss: 0.630301  [  128/  179]
train() client id: f_00007-1-4 loss: 0.835281  [  160/  179]
train() client id: f_00007-2-0 loss: 0.765189  [   32/  179]
train() client id: f_00007-2-1 loss: 0.506307  [   64/  179]
train() client id: f_00007-2-2 loss: 0.844461  [   96/  179]
train() client id: f_00007-2-3 loss: 0.510658  [  128/  179]
train() client id: f_00007-2-4 loss: 0.577327  [  160/  179]
train() client id: f_00007-3-0 loss: 0.579294  [   32/  179]
train() client id: f_00007-3-1 loss: 0.732878  [   64/  179]
train() client id: f_00007-3-2 loss: 0.647241  [   96/  179]
train() client id: f_00007-3-3 loss: 0.614669  [  128/  179]
train() client id: f_00007-3-4 loss: 0.672191  [  160/  179]
train() client id: f_00007-4-0 loss: 0.519125  [   32/  179]
train() client id: f_00007-4-1 loss: 0.630795  [   64/  179]
train() client id: f_00007-4-2 loss: 0.601673  [   96/  179]
train() client id: f_00007-4-3 loss: 0.560158  [  128/  179]
train() client id: f_00007-4-4 loss: 0.635806  [  160/  179]
train() client id: f_00007-5-0 loss: 0.478427  [   32/  179]
train() client id: f_00007-5-1 loss: 0.721538  [   64/  179]
train() client id: f_00007-5-2 loss: 0.734235  [   96/  179]
train() client id: f_00007-5-3 loss: 0.570767  [  128/  179]
train() client id: f_00007-5-4 loss: 0.652967  [  160/  179]
train() client id: f_00007-6-0 loss: 0.585081  [   32/  179]
train() client id: f_00007-6-1 loss: 0.476856  [   64/  179]
train() client id: f_00007-6-2 loss: 0.724347  [   96/  179]
train() client id: f_00007-6-3 loss: 0.662402  [  128/  179]
train() client id: f_00007-6-4 loss: 0.629623  [  160/  179]
train() client id: f_00007-7-0 loss: 0.539193  [   32/  179]
train() client id: f_00007-7-1 loss: 0.579179  [   64/  179]
train() client id: f_00007-7-2 loss: 0.667971  [   96/  179]
train() client id: f_00007-7-3 loss: 0.565496  [  128/  179]
train() client id: f_00007-7-4 loss: 0.679717  [  160/  179]
train() client id: f_00007-8-0 loss: 0.588178  [   32/  179]
train() client id: f_00007-8-1 loss: 0.665493  [   64/  179]
train() client id: f_00007-8-2 loss: 0.535540  [   96/  179]
train() client id: f_00007-8-3 loss: 0.565672  [  128/  179]
train() client id: f_00007-8-4 loss: 0.615652  [  160/  179]
train() client id: f_00007-9-0 loss: 0.466096  [   32/  179]
train() client id: f_00007-9-1 loss: 0.627393  [   64/  179]
train() client id: f_00007-9-2 loss: 0.479552  [   96/  179]
train() client id: f_00007-9-3 loss: 0.605980  [  128/  179]
train() client id: f_00007-9-4 loss: 0.921423  [  160/  179]
train() client id: f_00007-10-0 loss: 0.752314  [   32/  179]
train() client id: f_00007-10-1 loss: 0.579978  [   64/  179]
train() client id: f_00007-10-2 loss: 0.665917  [   96/  179]
train() client id: f_00007-10-3 loss: 0.558170  [  128/  179]
train() client id: f_00007-10-4 loss: 0.531886  [  160/  179]
train() client id: f_00007-11-0 loss: 0.636030  [   32/  179]
train() client id: f_00007-11-1 loss: 0.562764  [   64/  179]
train() client id: f_00007-11-2 loss: 0.656400  [   96/  179]
train() client id: f_00007-11-3 loss: 0.536673  [  128/  179]
train() client id: f_00007-11-4 loss: 0.625940  [  160/  179]
train() client id: f_00008-0-0 loss: 0.856675  [   32/  130]
train() client id: f_00008-0-1 loss: 0.856227  [   64/  130]
train() client id: f_00008-0-2 loss: 0.749626  [   96/  130]
train() client id: f_00008-0-3 loss: 0.770170  [  128/  130]
train() client id: f_00008-1-0 loss: 0.779381  [   32/  130]
train() client id: f_00008-1-1 loss: 0.822188  [   64/  130]
train() client id: f_00008-1-2 loss: 0.871660  [   96/  130]
train() client id: f_00008-1-3 loss: 0.779201  [  128/  130]
train() client id: f_00008-2-0 loss: 0.789457  [   32/  130]
train() client id: f_00008-2-1 loss: 0.842010  [   64/  130]
train() client id: f_00008-2-2 loss: 0.848119  [   96/  130]
train() client id: f_00008-2-3 loss: 0.782026  [  128/  130]
train() client id: f_00008-3-0 loss: 0.925870  [   32/  130]
train() client id: f_00008-3-1 loss: 0.656322  [   64/  130]
train() client id: f_00008-3-2 loss: 0.808758  [   96/  130]
train() client id: f_00008-3-3 loss: 0.847134  [  128/  130]
train() client id: f_00008-4-0 loss: 0.782362  [   32/  130]
train() client id: f_00008-4-1 loss: 0.735336  [   64/  130]
train() client id: f_00008-4-2 loss: 0.810106  [   96/  130]
train() client id: f_00008-4-3 loss: 0.907900  [  128/  130]
train() client id: f_00008-5-0 loss: 0.818383  [   32/  130]
train() client id: f_00008-5-1 loss: 0.778382  [   64/  130]
train() client id: f_00008-5-2 loss: 0.885137  [   96/  130]
train() client id: f_00008-5-3 loss: 0.754295  [  128/  130]
train() client id: f_00008-6-0 loss: 0.918757  [   32/  130]
train() client id: f_00008-6-1 loss: 0.793549  [   64/  130]
train() client id: f_00008-6-2 loss: 0.937298  [   96/  130]
train() client id: f_00008-6-3 loss: 0.630266  [  128/  130]
train() client id: f_00008-7-0 loss: 0.769930  [   32/  130]
train() client id: f_00008-7-1 loss: 0.737050  [   64/  130]
train() client id: f_00008-7-2 loss: 0.776517  [   96/  130]
train() client id: f_00008-7-3 loss: 0.963720  [  128/  130]
train() client id: f_00008-8-0 loss: 0.778315  [   32/  130]
train() client id: f_00008-8-1 loss: 0.854931  [   64/  130]
train() client id: f_00008-8-2 loss: 0.755005  [   96/  130]
train() client id: f_00008-8-3 loss: 0.884238  [  128/  130]
train() client id: f_00008-9-0 loss: 0.843981  [   32/  130]
train() client id: f_00008-9-1 loss: 0.797994  [   64/  130]
train() client id: f_00008-9-2 loss: 0.776479  [   96/  130]
train() client id: f_00008-9-3 loss: 0.819660  [  128/  130]
train() client id: f_00008-10-0 loss: 0.745492  [   32/  130]
train() client id: f_00008-10-1 loss: 0.868632  [   64/  130]
train() client id: f_00008-10-2 loss: 0.879666  [   96/  130]
train() client id: f_00008-10-3 loss: 0.735819  [  128/  130]
train() client id: f_00008-11-0 loss: 0.790779  [   32/  130]
train() client id: f_00008-11-1 loss: 0.861792  [   64/  130]
train() client id: f_00008-11-2 loss: 0.743453  [   96/  130]
train() client id: f_00008-11-3 loss: 0.840590  [  128/  130]
train() client id: f_00009-0-0 loss: 1.123241  [   32/  118]
train() client id: f_00009-0-1 loss: 1.306323  [   64/  118]
train() client id: f_00009-0-2 loss: 1.114539  [   96/  118]
train() client id: f_00009-1-0 loss: 0.933766  [   32/  118]
train() client id: f_00009-1-1 loss: 1.167596  [   64/  118]
train() client id: f_00009-1-2 loss: 1.168307  [   96/  118]
train() client id: f_00009-2-0 loss: 0.975357  [   32/  118]
train() client id: f_00009-2-1 loss: 1.040939  [   64/  118]
train() client id: f_00009-2-2 loss: 1.198425  [   96/  118]
train() client id: f_00009-3-0 loss: 1.075723  [   32/  118]
train() client id: f_00009-3-1 loss: 1.091595  [   64/  118]
train() client id: f_00009-3-2 loss: 0.908221  [   96/  118]
train() client id: f_00009-4-0 loss: 1.020269  [   32/  118]
train() client id: f_00009-4-1 loss: 1.036095  [   64/  118]
train() client id: f_00009-4-2 loss: 1.019905  [   96/  118]
train() client id: f_00009-5-0 loss: 0.948437  [   32/  118]
train() client id: f_00009-5-1 loss: 0.911701  [   64/  118]
train() client id: f_00009-5-2 loss: 0.995143  [   96/  118]
train() client id: f_00009-6-0 loss: 1.034097  [   32/  118]
train() client id: f_00009-6-1 loss: 1.132725  [   64/  118]
train() client id: f_00009-6-2 loss: 0.793599  [   96/  118]
train() client id: f_00009-7-0 loss: 0.972453  [   32/  118]
train() client id: f_00009-7-1 loss: 0.878513  [   64/  118]
train() client id: f_00009-7-2 loss: 0.903377  [   96/  118]
train() client id: f_00009-8-0 loss: 0.867848  [   32/  118]
train() client id: f_00009-8-1 loss: 1.028734  [   64/  118]
train() client id: f_00009-8-2 loss: 0.971304  [   96/  118]
train() client id: f_00009-9-0 loss: 0.985651  [   32/  118]
train() client id: f_00009-9-1 loss: 0.783750  [   64/  118]
train() client id: f_00009-9-2 loss: 0.920110  [   96/  118]
train() client id: f_00009-10-0 loss: 0.804961  [   32/  118]
train() client id: f_00009-10-1 loss: 1.108755  [   64/  118]
train() client id: f_00009-10-2 loss: 0.867061  [   96/  118]
train() client id: f_00009-11-0 loss: 0.896034  [   32/  118]
train() client id: f_00009-11-1 loss: 0.805159  [   64/  118]
train() client id: f_00009-11-2 loss: 0.879081  [   96/  118]
At round 29 accuracy: 0.6525198938992043
At round 29 training accuracy: 0.579476861167002
At round 29 training loss: 0.8435562881861999
update_location
xs = [  -3.9056584     4.20031788  165.00902392   18.81129433    0.97929623
    3.95640986 -127.44319194 -106.32485185  149.66397685  -92.06087855]
ys = [ 157.5879595   140.55583871    1.32061395 -127.45517586  119.35018685
  102.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [186.67945561 172.55024331 192.95005052 163.09103791 155.70942849
 143.47962494 162.01437502 145.96455178 180.85346538 135.9824151 ]
dists_bs = [171.9780593  182.48055834 381.79924762 359.22409609 184.56872542
 192.96681959 183.881171   187.22748312 360.82019861 190.05102236]
uav_gains = [2.06621021e-11 2.54082094e-11 1.88791154e-11 2.93478532e-11
 3.29966588e-11 4.05296333e-11 2.98453693e-11 3.88208176e-11
 2.24815448e-11 4.63629961e-11]
bs_gains = [6.08092382e-11 5.15095318e-11 6.51862344e-12 7.73162525e-12
 4.98943499e-11 4.40496737e-11 5.04184808e-11 4.79357148e-11
 7.63624282e-12 4.59681998e-11]
Round 30
-------------------------------
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.03213939 14.58505974  6.92666737  2.4917993  16.82243501  8.09793447
  3.09128315  9.90171672  7.30136856  6.56936016]
obj_prev = 82.8197638713389
eta_min = 6.720783557496832e-14	eta_max = 0.9260472326332857
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 19.233152927962998	eta = 0.9090909090909091
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 34.518956732515555	eta = 0.5065241286245609
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 27.08219396594083	eta = 0.6456155103960739
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.74057952697607	eta = 0.6792653779081567
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.671393607462342	eta = 0.6810960381552402
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.671195205593733	eta = 0.6811013020600015
eta = 0.6811013020600015
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [0.03169041 0.06665046 0.03118739 0.01081498 0.07696242 0.03672063
 0.0135816  0.0450205  0.03269645 0.02967832]
ene_total = [2.2727313  4.14429542 2.25574611 1.06104318 4.72594168 2.47696442
 1.21464967 2.95433273 2.48764402 2.07784665]
ti_comp = [0.41970979 0.43599649 0.41770452 0.42674957 0.43552852 0.43363939
 0.42705945 0.43162177 0.39149563 0.43429663]
ti_coms = [0.08785512 0.07156842 0.08986039 0.08081534 0.07203639 0.07392552
 0.08050546 0.07594314 0.11606928 0.07326828]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.12918568e-05 9.73472227e-05 1.08662233e-05 4.34121306e-07
 1.50204523e-04 1.64570901e-05 8.58532087e-07 3.06128901e-05
 1.42537176e-05 8.66213655e-06]
ene_total = [0.49394235 0.40732423 0.50517807 0.45380416 0.41291985 0.4160174
 0.45208802 0.42814121 0.65253176 0.41188928]
optimize_network_iter = 0 obj = 4.633836318316938
eta = 0.6811013020600015
freqs = [37752763.88864101 76434633.93405241 37331875.99521733 12671343.57363147
 88355204.53366868 42340057.1595814  15901303.6163569  52152723.73147339
 41758389.65718008 34168259.23522189]
eta_min = 0.6811013020600248	eta_max = 0.6811013020600005
af = 0.015898726867838657	bf = 1.4716185540757498	zeta = 0.017488599554622525	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [2.80235665e-06 2.41591478e-05 2.69672506e-06 1.07738059e-07
 3.72770088e-05 4.08423848e-06 2.13066208e-07 7.59735426e-06
 3.53741648e-06 2.14972581e-06]
ene_total = [1.73102112 1.4144311  1.77049778 1.59182881 1.42623245 1.45690442
 1.58574596 1.49733708 2.28689603 1.44357775]
ti_comp = [0.41970979 0.43599649 0.41770452 0.42674957 0.43552852 0.43363939
 0.42705945 0.43162177 0.39149563 0.43429663]
ti_coms = [0.08785512 0.07156842 0.08986039 0.08081534 0.07203639 0.07392552
 0.08050546 0.07594314 0.11606928 0.07326828]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.12918568e-05 9.73472227e-05 1.08662233e-05 4.34121306e-07
 1.50204523e-04 1.64570901e-05 8.58532087e-07 3.06128901e-05
 1.42537176e-05 8.66213655e-06]
ene_total = [0.49394235 0.40732423 0.50517807 0.45380416 0.41291985 0.4160174
 0.45208802 0.42814121 0.65253176 0.41188928]
optimize_network_iter = 1 obj = 4.633836318316922
eta = 0.6811013020600005
freqs = [37752763.88864101 76434633.93405241 37331875.99521733 12671343.57363147
 88355204.53366868 42340057.1595814  15901303.6163569  52152723.73147339
 41758389.65718006 34168259.23522189]
Done!
At round 30 eta: 0.6811013020600005
At round 30 local rounds: 12.575559237281574
At round 30 global rounds: 56.15020557576531
At round 30 a_n: 17.56368160020554
gradient difference: 0.49263015389442444
train() client id: f_00000-0-0 loss: 1.106534  [   32/  126]
train() client id: f_00000-0-1 loss: 1.296355  [   64/  126]
train() client id: f_00000-0-2 loss: 1.269496  [   96/  126]
train() client id: f_00000-1-0 loss: 1.033139  [   32/  126]
train() client id: f_00000-1-1 loss: 1.188089  [   64/  126]
train() client id: f_00000-1-2 loss: 1.070451  [   96/  126]
train() client id: f_00000-2-0 loss: 1.010236  [   32/  126]
train() client id: f_00000-2-1 loss: 1.027394  [   64/  126]
train() client id: f_00000-2-2 loss: 1.008814  [   96/  126]
train() client id: f_00000-3-0 loss: 0.993331  [   32/  126]
train() client id: f_00000-3-1 loss: 1.018597  [   64/  126]
train() client id: f_00000-3-2 loss: 0.927700  [   96/  126]
train() client id: f_00000-4-0 loss: 0.987085  [   32/  126]
train() client id: f_00000-4-1 loss: 0.944591  [   64/  126]
train() client id: f_00000-4-2 loss: 1.035065  [   96/  126]
train() client id: f_00000-5-0 loss: 0.959998  [   32/  126]
train() client id: f_00000-5-1 loss: 0.919339  [   64/  126]
train() client id: f_00000-5-2 loss: 0.923271  [   96/  126]
train() client id: f_00000-6-0 loss: 0.992532  [   32/  126]
train() client id: f_00000-6-1 loss: 0.898685  [   64/  126]
train() client id: f_00000-6-2 loss: 0.868885  [   96/  126]
train() client id: f_00000-7-0 loss: 0.895843  [   32/  126]
train() client id: f_00000-7-1 loss: 0.871372  [   64/  126]
train() client id: f_00000-7-2 loss: 0.844667  [   96/  126]
train() client id: f_00000-8-0 loss: 0.985039  [   32/  126]
train() client id: f_00000-8-1 loss: 0.839795  [   64/  126]
train() client id: f_00000-8-2 loss: 0.855008  [   96/  126]
train() client id: f_00000-9-0 loss: 0.877935  [   32/  126]
train() client id: f_00000-9-1 loss: 0.913169  [   64/  126]
train() client id: f_00000-9-2 loss: 0.949655  [   96/  126]
train() client id: f_00000-10-0 loss: 0.916498  [   32/  126]
train() client id: f_00000-10-1 loss: 0.964453  [   64/  126]
train() client id: f_00000-10-2 loss: 0.844620  [   96/  126]
train() client id: f_00000-11-0 loss: 0.903826  [   32/  126]
train() client id: f_00000-11-1 loss: 0.910202  [   64/  126]
train() client id: f_00000-11-2 loss: 0.929062  [   96/  126]
train() client id: f_00001-0-0 loss: 0.483654  [   32/  265]
train() client id: f_00001-0-1 loss: 0.413010  [   64/  265]
train() client id: f_00001-0-2 loss: 0.426820  [   96/  265]
train() client id: f_00001-0-3 loss: 0.375769  [  128/  265]
train() client id: f_00001-0-4 loss: 0.498927  [  160/  265]
train() client id: f_00001-0-5 loss: 0.620950  [  192/  265]
train() client id: f_00001-0-6 loss: 0.432161  [  224/  265]
train() client id: f_00001-0-7 loss: 0.486998  [  256/  265]
train() client id: f_00001-1-0 loss: 0.575488  [   32/  265]
train() client id: f_00001-1-1 loss: 0.501340  [   64/  265]
train() client id: f_00001-1-2 loss: 0.409283  [   96/  265]
train() client id: f_00001-1-3 loss: 0.505730  [  128/  265]
train() client id: f_00001-1-4 loss: 0.410323  [  160/  265]
train() client id: f_00001-1-5 loss: 0.366389  [  192/  265]
train() client id: f_00001-1-6 loss: 0.415177  [  224/  265]
train() client id: f_00001-1-7 loss: 0.380833  [  256/  265]
train() client id: f_00001-2-0 loss: 0.402674  [   32/  265]
train() client id: f_00001-2-1 loss: 0.418002  [   64/  265]
train() client id: f_00001-2-2 loss: 0.401742  [   96/  265]
train() client id: f_00001-2-3 loss: 0.468542  [  128/  265]
train() client id: f_00001-2-4 loss: 0.440157  [  160/  265]
train() client id: f_00001-2-5 loss: 0.465749  [  192/  265]
train() client id: f_00001-2-6 loss: 0.451257  [  224/  265]
train() client id: f_00001-2-7 loss: 0.489579  [  256/  265]
train() client id: f_00001-3-0 loss: 0.374695  [   32/  265]
train() client id: f_00001-3-1 loss: 0.593226  [   64/  265]
train() client id: f_00001-3-2 loss: 0.501131  [   96/  265]
train() client id: f_00001-3-3 loss: 0.353030  [  128/  265]
train() client id: f_00001-3-4 loss: 0.423990  [  160/  265]
train() client id: f_00001-3-5 loss: 0.408442  [  192/  265]
train() client id: f_00001-3-6 loss: 0.448325  [  224/  265]
train() client id: f_00001-3-7 loss: 0.433045  [  256/  265]
train() client id: f_00001-4-0 loss: 0.406406  [   32/  265]
train() client id: f_00001-4-1 loss: 0.368554  [   64/  265]
train() client id: f_00001-4-2 loss: 0.393627  [   96/  265]
train() client id: f_00001-4-3 loss: 0.479435  [  128/  265]
train() client id: f_00001-4-4 loss: 0.557341  [  160/  265]
train() client id: f_00001-4-5 loss: 0.496924  [  192/  265]
train() client id: f_00001-4-6 loss: 0.329546  [  224/  265]
train() client id: f_00001-4-7 loss: 0.464681  [  256/  265]
train() client id: f_00001-5-0 loss: 0.415961  [   32/  265]
train() client id: f_00001-5-1 loss: 0.456221  [   64/  265]
train() client id: f_00001-5-2 loss: 0.415705  [   96/  265]
train() client id: f_00001-5-3 loss: 0.468151  [  128/  265]
train() client id: f_00001-5-4 loss: 0.566053  [  160/  265]
train() client id: f_00001-5-5 loss: 0.432270  [  192/  265]
train() client id: f_00001-5-6 loss: 0.369250  [  224/  265]
train() client id: f_00001-5-7 loss: 0.335198  [  256/  265]
train() client id: f_00001-6-0 loss: 0.529701  [   32/  265]
train() client id: f_00001-6-1 loss: 0.430142  [   64/  265]
train() client id: f_00001-6-2 loss: 0.416012  [   96/  265]
train() client id: f_00001-6-3 loss: 0.466245  [  128/  265]
train() client id: f_00001-6-4 loss: 0.350493  [  160/  265]
train() client id: f_00001-6-5 loss: 0.409230  [  192/  265]
train() client id: f_00001-6-6 loss: 0.406269  [  224/  265]
train() client id: f_00001-6-7 loss: 0.361078  [  256/  265]
train() client id: f_00001-7-0 loss: 0.558289  [   32/  265]
train() client id: f_00001-7-1 loss: 0.468904  [   64/  265]
train() client id: f_00001-7-2 loss: 0.411387  [   96/  265]
train() client id: f_00001-7-3 loss: 0.483184  [  128/  265]
train() client id: f_00001-7-4 loss: 0.354667  [  160/  265]
train() client id: f_00001-7-5 loss: 0.349537  [  192/  265]
train() client id: f_00001-7-6 loss: 0.370706  [  224/  265]
train() client id: f_00001-7-7 loss: 0.425124  [  256/  265]
train() client id: f_00001-8-0 loss: 0.335221  [   32/  265]
train() client id: f_00001-8-1 loss: 0.414350  [   64/  265]
train() client id: f_00001-8-2 loss: 0.335094  [   96/  265]
train() client id: f_00001-8-3 loss: 0.338096  [  128/  265]
train() client id: f_00001-8-4 loss: 0.465971  [  160/  265]
train() client id: f_00001-8-5 loss: 0.553411  [  192/  265]
train() client id: f_00001-8-6 loss: 0.479674  [  224/  265]
train() client id: f_00001-8-7 loss: 0.504177  [  256/  265]
train() client id: f_00001-9-0 loss: 0.430346  [   32/  265]
train() client id: f_00001-9-1 loss: 0.471668  [   64/  265]
train() client id: f_00001-9-2 loss: 0.351071  [   96/  265]
train() client id: f_00001-9-3 loss: 0.423412  [  128/  265]
train() client id: f_00001-9-4 loss: 0.387079  [  160/  265]
train() client id: f_00001-9-5 loss: 0.489120  [  192/  265]
train() client id: f_00001-9-6 loss: 0.455793  [  224/  265]
train() client id: f_00001-9-7 loss: 0.390761  [  256/  265]
train() client id: f_00001-10-0 loss: 0.457189  [   32/  265]
train() client id: f_00001-10-1 loss: 0.326029  [   64/  265]
train() client id: f_00001-10-2 loss: 0.428431  [   96/  265]
train() client id: f_00001-10-3 loss: 0.400688  [  128/  265]
train() client id: f_00001-10-4 loss: 0.327989  [  160/  265]
train() client id: f_00001-10-5 loss: 0.482063  [  192/  265]
train() client id: f_00001-10-6 loss: 0.483553  [  224/  265]
train() client id: f_00001-10-7 loss: 0.456784  [  256/  265]
train() client id: f_00001-11-0 loss: 0.466664  [   32/  265]
train() client id: f_00001-11-1 loss: 0.328530  [   64/  265]
train() client id: f_00001-11-2 loss: 0.338915  [   96/  265]
train() client id: f_00001-11-3 loss: 0.380639  [  128/  265]
train() client id: f_00001-11-4 loss: 0.376165  [  160/  265]
train() client id: f_00001-11-5 loss: 0.564773  [  192/  265]
train() client id: f_00001-11-6 loss: 0.421416  [  224/  265]
train() client id: f_00001-11-7 loss: 0.421981  [  256/  265]
train() client id: f_00002-0-0 loss: 0.989355  [   32/  124]
train() client id: f_00002-0-1 loss: 1.115520  [   64/  124]
train() client id: f_00002-0-2 loss: 1.077306  [   96/  124]
train() client id: f_00002-1-0 loss: 1.201617  [   32/  124]
train() client id: f_00002-1-1 loss: 0.923342  [   64/  124]
train() client id: f_00002-1-2 loss: 1.001619  [   96/  124]
train() client id: f_00002-2-0 loss: 1.230013  [   32/  124]
train() client id: f_00002-2-1 loss: 0.891730  [   64/  124]
train() client id: f_00002-2-2 loss: 0.905237  [   96/  124]
train() client id: f_00002-3-0 loss: 0.863124  [   32/  124]
train() client id: f_00002-3-1 loss: 1.121621  [   64/  124]
train() client id: f_00002-3-2 loss: 0.920492  [   96/  124]
train() client id: f_00002-4-0 loss: 0.895728  [   32/  124]
train() client id: f_00002-4-1 loss: 0.993943  [   64/  124]
train() client id: f_00002-4-2 loss: 0.868031  [   96/  124]
train() client id: f_00002-5-0 loss: 0.943839  [   32/  124]
train() client id: f_00002-5-1 loss: 0.814367  [   64/  124]
train() client id: f_00002-5-2 loss: 0.786474  [   96/  124]
train() client id: f_00002-6-0 loss: 0.910747  [   32/  124]
train() client id: f_00002-6-1 loss: 0.800443  [   64/  124]
train() client id: f_00002-6-2 loss: 0.816282  [   96/  124]
train() client id: f_00002-7-0 loss: 0.815506  [   32/  124]
train() client id: f_00002-7-1 loss: 0.852233  [   64/  124]
train() client id: f_00002-7-2 loss: 0.915582  [   96/  124]
train() client id: f_00002-8-0 loss: 0.927777  [   32/  124]
train() client id: f_00002-8-1 loss: 0.877299  [   64/  124]
train() client id: f_00002-8-2 loss: 0.690257  [   96/  124]
train() client id: f_00002-9-0 loss: 0.856852  [   32/  124]
train() client id: f_00002-9-1 loss: 1.031687  [   64/  124]
train() client id: f_00002-9-2 loss: 0.771076  [   96/  124]
train() client id: f_00002-10-0 loss: 0.819903  [   32/  124]
train() client id: f_00002-10-1 loss: 0.885090  [   64/  124]
train() client id: f_00002-10-2 loss: 0.797683  [   96/  124]
train() client id: f_00002-11-0 loss: 0.826643  [   32/  124]
train() client id: f_00002-11-1 loss: 1.015280  [   64/  124]
train() client id: f_00002-11-2 loss: 0.769777  [   96/  124]
train() client id: f_00003-0-0 loss: 0.959365  [   32/   43]
train() client id: f_00003-1-0 loss: 0.595454  [   32/   43]
train() client id: f_00003-2-0 loss: 0.656262  [   32/   43]
train() client id: f_00003-3-0 loss: 0.800521  [   32/   43]
train() client id: f_00003-4-0 loss: 0.662994  [   32/   43]
train() client id: f_00003-5-0 loss: 0.935776  [   32/   43]
train() client id: f_00003-6-0 loss: 0.637943  [   32/   43]
train() client id: f_00003-7-0 loss: 0.648642  [   32/   43]
train() client id: f_00003-8-0 loss: 0.705038  [   32/   43]
train() client id: f_00003-9-0 loss: 0.918154  [   32/   43]
train() client id: f_00003-10-0 loss: 0.779654  [   32/   43]
train() client id: f_00003-11-0 loss: 0.829858  [   32/   43]
train() client id: f_00004-0-0 loss: 0.711704  [   32/  306]
train() client id: f_00004-0-1 loss: 0.841371  [   64/  306]
train() client id: f_00004-0-2 loss: 0.780601  [   96/  306]
train() client id: f_00004-0-3 loss: 0.762141  [  128/  306]
train() client id: f_00004-0-4 loss: 0.658122  [  160/  306]
train() client id: f_00004-0-5 loss: 0.660106  [  192/  306]
train() client id: f_00004-0-6 loss: 0.873848  [  224/  306]
train() client id: f_00004-0-7 loss: 0.771833  [  256/  306]
train() client id: f_00004-0-8 loss: 0.839808  [  288/  306]
train() client id: f_00004-1-0 loss: 0.798299  [   32/  306]
train() client id: f_00004-1-1 loss: 0.662285  [   64/  306]
train() client id: f_00004-1-2 loss: 0.684549  [   96/  306]
train() client id: f_00004-1-3 loss: 0.698420  [  128/  306]
train() client id: f_00004-1-4 loss: 0.808656  [  160/  306]
train() client id: f_00004-1-5 loss: 0.740420  [  192/  306]
train() client id: f_00004-1-6 loss: 0.880492  [  224/  306]
train() client id: f_00004-1-7 loss: 0.771135  [  256/  306]
train() client id: f_00004-1-8 loss: 0.722846  [  288/  306]
train() client id: f_00004-2-0 loss: 0.681635  [   32/  306]
train() client id: f_00004-2-1 loss: 0.710325  [   64/  306]
train() client id: f_00004-2-2 loss: 0.808848  [   96/  306]
train() client id: f_00004-2-3 loss: 0.715023  [  128/  306]
train() client id: f_00004-2-4 loss: 0.827692  [  160/  306]
train() client id: f_00004-2-5 loss: 0.799779  [  192/  306]
train() client id: f_00004-2-6 loss: 0.803121  [  224/  306]
train() client id: f_00004-2-7 loss: 0.791804  [  256/  306]
train() client id: f_00004-2-8 loss: 0.732000  [  288/  306]
train() client id: f_00004-3-0 loss: 0.857557  [   32/  306]
train() client id: f_00004-3-1 loss: 0.696787  [   64/  306]
train() client id: f_00004-3-2 loss: 0.795369  [   96/  306]
train() client id: f_00004-3-3 loss: 0.726789  [  128/  306]
train() client id: f_00004-3-4 loss: 0.723598  [  160/  306]
train() client id: f_00004-3-5 loss: 0.796556  [  192/  306]
train() client id: f_00004-3-6 loss: 0.810089  [  224/  306]
train() client id: f_00004-3-7 loss: 0.817449  [  256/  306]
train() client id: f_00004-3-8 loss: 0.755221  [  288/  306]
train() client id: f_00004-4-0 loss: 0.648297  [   32/  306]
train() client id: f_00004-4-1 loss: 0.758773  [   64/  306]
train() client id: f_00004-4-2 loss: 0.677628  [   96/  306]
train() client id: f_00004-4-3 loss: 0.787385  [  128/  306]
train() client id: f_00004-4-4 loss: 0.947085  [  160/  306]
train() client id: f_00004-4-5 loss: 0.813399  [  192/  306]
train() client id: f_00004-4-6 loss: 0.782490  [  224/  306]
train() client id: f_00004-4-7 loss: 0.853106  [  256/  306]
train() client id: f_00004-4-8 loss: 0.712681  [  288/  306]
train() client id: f_00004-5-0 loss: 0.800402  [   32/  306]
train() client id: f_00004-5-1 loss: 0.792781  [   64/  306]
train() client id: f_00004-5-2 loss: 0.826768  [   96/  306]
train() client id: f_00004-5-3 loss: 0.702225  [  128/  306]
train() client id: f_00004-5-4 loss: 0.800771  [  160/  306]
train() client id: f_00004-5-5 loss: 0.664374  [  192/  306]
train() client id: f_00004-5-6 loss: 0.886585  [  224/  306]
train() client id: f_00004-5-7 loss: 0.633742  [  256/  306]
train() client id: f_00004-5-8 loss: 0.870685  [  288/  306]
train() client id: f_00004-6-0 loss: 0.864886  [   32/  306]
train() client id: f_00004-6-1 loss: 0.749687  [   64/  306]
train() client id: f_00004-6-2 loss: 0.873146  [   96/  306]
train() client id: f_00004-6-3 loss: 0.666762  [  128/  306]
train() client id: f_00004-6-4 loss: 0.800207  [  160/  306]
train() client id: f_00004-6-5 loss: 0.777454  [  192/  306]
train() client id: f_00004-6-6 loss: 0.774885  [  224/  306]
train() client id: f_00004-6-7 loss: 0.679189  [  256/  306]
train() client id: f_00004-6-8 loss: 0.800291  [  288/  306]
train() client id: f_00004-7-0 loss: 0.708574  [   32/  306]
train() client id: f_00004-7-1 loss: 0.792626  [   64/  306]
train() client id: f_00004-7-2 loss: 0.685211  [   96/  306]
train() client id: f_00004-7-3 loss: 0.815109  [  128/  306]
train() client id: f_00004-7-4 loss: 0.837825  [  160/  306]
train() client id: f_00004-7-5 loss: 0.841245  [  192/  306]
train() client id: f_00004-7-6 loss: 0.752700  [  224/  306]
train() client id: f_00004-7-7 loss: 0.757049  [  256/  306]
train() client id: f_00004-7-8 loss: 0.799926  [  288/  306]
train() client id: f_00004-8-0 loss: 0.710939  [   32/  306]
train() client id: f_00004-8-1 loss: 0.829215  [   64/  306]
train() client id: f_00004-8-2 loss: 0.720167  [   96/  306]
train() client id: f_00004-8-3 loss: 0.841687  [  128/  306]
train() client id: f_00004-8-4 loss: 0.827654  [  160/  306]
train() client id: f_00004-8-5 loss: 0.765897  [  192/  306]
train() client id: f_00004-8-6 loss: 0.859376  [  224/  306]
train() client id: f_00004-8-7 loss: 0.667549  [  256/  306]
train() client id: f_00004-8-8 loss: 0.756512  [  288/  306]
train() client id: f_00004-9-0 loss: 0.720961  [   32/  306]
train() client id: f_00004-9-1 loss: 0.767040  [   64/  306]
train() client id: f_00004-9-2 loss: 0.674576  [   96/  306]
train() client id: f_00004-9-3 loss: 0.882689  [  128/  306]
train() client id: f_00004-9-4 loss: 0.728985  [  160/  306]
train() client id: f_00004-9-5 loss: 0.732179  [  192/  306]
train() client id: f_00004-9-6 loss: 0.767390  [  224/  306]
train() client id: f_00004-9-7 loss: 0.820534  [  256/  306]
train() client id: f_00004-9-8 loss: 0.861057  [  288/  306]
train() client id: f_00004-10-0 loss: 0.805726  [   32/  306]
train() client id: f_00004-10-1 loss: 0.852689  [   64/  306]
train() client id: f_00004-10-2 loss: 0.822501  [   96/  306]
train() client id: f_00004-10-3 loss: 0.702959  [  128/  306]
train() client id: f_00004-10-4 loss: 0.807477  [  160/  306]
train() client id: f_00004-10-5 loss: 0.612152  [  192/  306]
train() client id: f_00004-10-6 loss: 0.874801  [  224/  306]
train() client id: f_00004-10-7 loss: 0.711626  [  256/  306]
train() client id: f_00004-10-8 loss: 0.862733  [  288/  306]
train() client id: f_00004-11-0 loss: 0.793036  [   32/  306]
train() client id: f_00004-11-1 loss: 0.695869  [   64/  306]
train() client id: f_00004-11-2 loss: 0.753176  [   96/  306]
train() client id: f_00004-11-3 loss: 0.809963  [  128/  306]
train() client id: f_00004-11-4 loss: 0.782270  [  160/  306]
train() client id: f_00004-11-5 loss: 0.797912  [  192/  306]
train() client id: f_00004-11-6 loss: 0.757293  [  224/  306]
train() client id: f_00004-11-7 loss: 0.809101  [  256/  306]
train() client id: f_00004-11-8 loss: 0.788543  [  288/  306]
train() client id: f_00005-0-0 loss: 0.445001  [   32/  146]
train() client id: f_00005-0-1 loss: 0.612812  [   64/  146]
train() client id: f_00005-0-2 loss: 0.467231  [   96/  146]
train() client id: f_00005-0-3 loss: 0.601795  [  128/  146]
train() client id: f_00005-1-0 loss: 0.509285  [   32/  146]
train() client id: f_00005-1-1 loss: 0.600642  [   64/  146]
train() client id: f_00005-1-2 loss: 0.389003  [   96/  146]
train() client id: f_00005-1-3 loss: 0.580447  [  128/  146]
train() client id: f_00005-2-0 loss: 0.405808  [   32/  146]
train() client id: f_00005-2-1 loss: 0.480643  [   64/  146]
train() client id: f_00005-2-2 loss: 0.289639  [   96/  146]
train() client id: f_00005-2-3 loss: 0.827220  [  128/  146]
train() client id: f_00005-3-0 loss: 0.556270  [   32/  146]
train() client id: f_00005-3-1 loss: 0.544718  [   64/  146]
train() client id: f_00005-3-2 loss: 0.674127  [   96/  146]
train() client id: f_00005-3-3 loss: 0.323727  [  128/  146]
train() client id: f_00005-4-0 loss: 0.344050  [   32/  146]
train() client id: f_00005-4-1 loss: 0.524915  [   64/  146]
train() client id: f_00005-4-2 loss: 0.744082  [   96/  146]
train() client id: f_00005-4-3 loss: 0.391022  [  128/  146]
train() client id: f_00005-5-0 loss: 0.443731  [   32/  146]
train() client id: f_00005-5-1 loss: 0.550166  [   64/  146]
train() client id: f_00005-5-2 loss: 0.560840  [   96/  146]
train() client id: f_00005-5-3 loss: 0.475313  [  128/  146]
train() client id: f_00005-6-0 loss: 0.686216  [   32/  146]
train() client id: f_00005-6-1 loss: 0.383218  [   64/  146]
train() client id: f_00005-6-2 loss: 0.376211  [   96/  146]
train() client id: f_00005-6-3 loss: 0.496583  [  128/  146]
train() client id: f_00005-7-0 loss: 0.463281  [   32/  146]
train() client id: f_00005-7-1 loss: 0.542263  [   64/  146]
train() client id: f_00005-7-2 loss: 0.481809  [   96/  146]
train() client id: f_00005-7-3 loss: 0.641762  [  128/  146]
train() client id: f_00005-8-0 loss: 0.478497  [   32/  146]
train() client id: f_00005-8-1 loss: 0.602892  [   64/  146]
train() client id: f_00005-8-2 loss: 0.361544  [   96/  146]
train() client id: f_00005-8-3 loss: 0.741557  [  128/  146]
train() client id: f_00005-9-0 loss: 0.615349  [   32/  146]
train() client id: f_00005-9-1 loss: 0.376361  [   64/  146]
train() client id: f_00005-9-2 loss: 0.624917  [   96/  146]
train() client id: f_00005-9-3 loss: 0.439857  [  128/  146]
train() client id: f_00005-10-0 loss: 0.663994  [   32/  146]
train() client id: f_00005-10-1 loss: 0.502479  [   64/  146]
train() client id: f_00005-10-2 loss: 0.544824  [   96/  146]
train() client id: f_00005-10-3 loss: 0.442091  [  128/  146]
train() client id: f_00005-11-0 loss: 0.472350  [   32/  146]
train() client id: f_00005-11-1 loss: 0.601768  [   64/  146]
train() client id: f_00005-11-2 loss: 0.527074  [   96/  146]
train() client id: f_00005-11-3 loss: 0.536568  [  128/  146]
train() client id: f_00006-0-0 loss: 0.632180  [   32/   54]
train() client id: f_00006-1-0 loss: 0.636021  [   32/   54]
train() client id: f_00006-2-0 loss: 0.577600  [   32/   54]
train() client id: f_00006-3-0 loss: 0.575686  [   32/   54]
train() client id: f_00006-4-0 loss: 0.529929  [   32/   54]
train() client id: f_00006-5-0 loss: 0.565310  [   32/   54]
train() client id: f_00006-6-0 loss: 0.567611  [   32/   54]
train() client id: f_00006-7-0 loss: 0.550249  [   32/   54]
train() client id: f_00006-8-0 loss: 0.624436  [   32/   54]
train() client id: f_00006-9-0 loss: 0.589888  [   32/   54]
train() client id: f_00006-10-0 loss: 0.563933  [   32/   54]
train() client id: f_00006-11-0 loss: 0.591089  [   32/   54]
train() client id: f_00007-0-0 loss: 0.703179  [   32/  179]
train() client id: f_00007-0-1 loss: 0.505545  [   64/  179]
train() client id: f_00007-0-2 loss: 0.484218  [   96/  179]
train() client id: f_00007-0-3 loss: 0.627119  [  128/  179]
train() client id: f_00007-0-4 loss: 0.586528  [  160/  179]
train() client id: f_00007-1-0 loss: 0.495812  [   32/  179]
train() client id: f_00007-1-1 loss: 0.754291  [   64/  179]
train() client id: f_00007-1-2 loss: 0.705004  [   96/  179]
train() client id: f_00007-1-3 loss: 0.544617  [  128/  179]
train() client id: f_00007-1-4 loss: 0.484375  [  160/  179]
train() client id: f_00007-2-0 loss: 0.487591  [   32/  179]
train() client id: f_00007-2-1 loss: 0.672206  [   64/  179]
train() client id: f_00007-2-2 loss: 0.444228  [   96/  179]
train() client id: f_00007-2-3 loss: 0.628307  [  128/  179]
train() client id: f_00007-2-4 loss: 0.501223  [  160/  179]
train() client id: f_00007-3-0 loss: 0.823153  [   32/  179]
train() client id: f_00007-3-1 loss: 0.445917  [   64/  179]
train() client id: f_00007-3-2 loss: 0.661077  [   96/  179]
train() client id: f_00007-3-3 loss: 0.476512  [  128/  179]
train() client id: f_00007-3-4 loss: 0.527026  [  160/  179]
train() client id: f_00007-4-0 loss: 0.712319  [   32/  179]
train() client id: f_00007-4-1 loss: 0.468872  [   64/  179]
train() client id: f_00007-4-2 loss: 0.529187  [   96/  179]
train() client id: f_00007-4-3 loss: 0.563251  [  128/  179]
train() client id: f_00007-4-4 loss: 0.624109  [  160/  179]
train() client id: f_00007-5-0 loss: 0.487211  [   32/  179]
train() client id: f_00007-5-1 loss: 0.769835  [   64/  179]
train() client id: f_00007-5-2 loss: 0.508743  [   96/  179]
train() client id: f_00007-5-3 loss: 0.589119  [  128/  179]
train() client id: f_00007-5-4 loss: 0.436000  [  160/  179]
train() client id: f_00007-6-0 loss: 0.512612  [   32/  179]
train() client id: f_00007-6-1 loss: 0.844880  [   64/  179]
train() client id: f_00007-6-2 loss: 0.489831  [   96/  179]
train() client id: f_00007-6-3 loss: 0.501866  [  128/  179]
train() client id: f_00007-6-4 loss: 0.503169  [  160/  179]
train() client id: f_00007-7-0 loss: 0.516727  [   32/  179]
train() client id: f_00007-7-1 loss: 0.642459  [   64/  179]
train() client id: f_00007-7-2 loss: 0.499793  [   96/  179]
train() client id: f_00007-7-3 loss: 0.530448  [  128/  179]
train() client id: f_00007-7-4 loss: 0.597752  [  160/  179]
train() client id: f_00007-8-0 loss: 0.693122  [   32/  179]
train() client id: f_00007-8-1 loss: 0.575730  [   64/  179]
train() client id: f_00007-8-2 loss: 0.664540  [   96/  179]
train() client id: f_00007-8-3 loss: 0.409180  [  128/  179]
train() client id: f_00007-8-4 loss: 0.528007  [  160/  179]
train() client id: f_00007-9-0 loss: 0.509484  [   32/  179]
train() client id: f_00007-9-1 loss: 0.501839  [   64/  179]
train() client id: f_00007-9-2 loss: 0.627090  [   96/  179]
train() client id: f_00007-9-3 loss: 0.513244  [  128/  179]
train() client id: f_00007-9-4 loss: 0.699374  [  160/  179]
train() client id: f_00007-10-0 loss: 0.629213  [   32/  179]
train() client id: f_00007-10-1 loss: 0.427578  [   64/  179]
train() client id: f_00007-10-2 loss: 0.410999  [   96/  179]
train() client id: f_00007-10-3 loss: 0.524742  [  128/  179]
train() client id: f_00007-10-4 loss: 0.626616  [  160/  179]
train() client id: f_00007-11-0 loss: 0.514465  [   32/  179]
train() client id: f_00007-11-1 loss: 0.481816  [   64/  179]
train() client id: f_00007-11-2 loss: 0.408383  [   96/  179]
train() client id: f_00007-11-3 loss: 0.512248  [  128/  179]
train() client id: f_00007-11-4 loss: 0.602827  [  160/  179]
train() client id: f_00008-0-0 loss: 0.824941  [   32/  130]
train() client id: f_00008-0-1 loss: 0.677555  [   64/  130]
train() client id: f_00008-0-2 loss: 0.653863  [   96/  130]
train() client id: f_00008-0-3 loss: 0.765791  [  128/  130]
train() client id: f_00008-1-0 loss: 0.717421  [   32/  130]
train() client id: f_00008-1-1 loss: 0.768164  [   64/  130]
train() client id: f_00008-1-2 loss: 0.632328  [   96/  130]
train() client id: f_00008-1-3 loss: 0.802000  [  128/  130]
train() client id: f_00008-2-0 loss: 0.746129  [   32/  130]
train() client id: f_00008-2-1 loss: 0.807131  [   64/  130]
train() client id: f_00008-2-2 loss: 0.672120  [   96/  130]
train() client id: f_00008-2-3 loss: 0.699143  [  128/  130]
train() client id: f_00008-3-0 loss: 0.731907  [   32/  130]
train() client id: f_00008-3-1 loss: 0.749286  [   64/  130]
train() client id: f_00008-3-2 loss: 0.690322  [   96/  130]
train() client id: f_00008-3-3 loss: 0.749716  [  128/  130]
train() client id: f_00008-4-0 loss: 0.852581  [   32/  130]
train() client id: f_00008-4-1 loss: 0.650924  [   64/  130]
train() client id: f_00008-4-2 loss: 0.740238  [   96/  130]
train() client id: f_00008-4-3 loss: 0.706818  [  128/  130]
train() client id: f_00008-5-0 loss: 0.818453  [   32/  130]
train() client id: f_00008-5-1 loss: 0.786301  [   64/  130]
train() client id: f_00008-5-2 loss: 0.587915  [   96/  130]
train() client id: f_00008-5-3 loss: 0.731243  [  128/  130]
train() client id: f_00008-6-0 loss: 0.682369  [   32/  130]
train() client id: f_00008-6-1 loss: 0.757749  [   64/  130]
train() client id: f_00008-6-2 loss: 0.738125  [   96/  130]
train() client id: f_00008-6-3 loss: 0.775954  [  128/  130]
train() client id: f_00008-7-0 loss: 0.707571  [   32/  130]
train() client id: f_00008-7-1 loss: 0.805758  [   64/  130]
train() client id: f_00008-7-2 loss: 0.649470  [   96/  130]
train() client id: f_00008-7-3 loss: 0.790382  [  128/  130]
train() client id: f_00008-8-0 loss: 0.769493  [   32/  130]
train() client id: f_00008-8-1 loss: 0.685911  [   64/  130]
train() client id: f_00008-8-2 loss: 0.695975  [   96/  130]
train() client id: f_00008-8-3 loss: 0.774733  [  128/  130]
train() client id: f_00008-9-0 loss: 0.690180  [   32/  130]
train() client id: f_00008-9-1 loss: 0.726387  [   64/  130]
train() client id: f_00008-9-2 loss: 0.727334  [   96/  130]
train() client id: f_00008-9-3 loss: 0.797925  [  128/  130]
train() client id: f_00008-10-0 loss: 0.733163  [   32/  130]
train() client id: f_00008-10-1 loss: 0.689801  [   64/  130]
train() client id: f_00008-10-2 loss: 0.796023  [   96/  130]
train() client id: f_00008-10-3 loss: 0.744229  [  128/  130]
train() client id: f_00008-11-0 loss: 0.660770  [   32/  130]
train() client id: f_00008-11-1 loss: 0.760018  [   64/  130]
train() client id: f_00008-11-2 loss: 0.633798  [   96/  130]
train() client id: f_00008-11-3 loss: 0.913836  [  128/  130]
train() client id: f_00009-0-0 loss: 1.096564  [   32/  118]
train() client id: f_00009-0-1 loss: 1.034610  [   64/  118]
train() client id: f_00009-0-2 loss: 1.016051  [   96/  118]
train() client id: f_00009-1-0 loss: 0.950811  [   32/  118]
train() client id: f_00009-1-1 loss: 1.113012  [   64/  118]
train() client id: f_00009-1-2 loss: 0.937018  [   96/  118]
train() client id: f_00009-2-0 loss: 0.992159  [   32/  118]
train() client id: f_00009-2-1 loss: 0.864418  [   64/  118]
train() client id: f_00009-2-2 loss: 1.047156  [   96/  118]
train() client id: f_00009-3-0 loss: 0.974358  [   32/  118]
train() client id: f_00009-3-1 loss: 0.893153  [   64/  118]
train() client id: f_00009-3-2 loss: 0.907066  [   96/  118]
train() client id: f_00009-4-0 loss: 0.900328  [   32/  118]
train() client id: f_00009-4-1 loss: 0.852202  [   64/  118]
train() client id: f_00009-4-2 loss: 0.978753  [   96/  118]
train() client id: f_00009-5-0 loss: 0.901533  [   32/  118]
train() client id: f_00009-5-1 loss: 0.796602  [   64/  118]
train() client id: f_00009-5-2 loss: 0.993133  [   96/  118]
train() client id: f_00009-6-0 loss: 0.929302  [   32/  118]
train() client id: f_00009-6-1 loss: 0.883878  [   64/  118]
train() client id: f_00009-6-2 loss: 0.812943  [   96/  118]
train() client id: f_00009-7-0 loss: 0.728786  [   32/  118]
train() client id: f_00009-7-1 loss: 0.963959  [   64/  118]
train() client id: f_00009-7-2 loss: 0.929066  [   96/  118]
train() client id: f_00009-8-0 loss: 0.890958  [   32/  118]
train() client id: f_00009-8-1 loss: 0.913831  [   64/  118]
train() client id: f_00009-8-2 loss: 0.814518  [   96/  118]
train() client id: f_00009-9-0 loss: 0.788382  [   32/  118]
train() client id: f_00009-9-1 loss: 1.027396  [   64/  118]
train() client id: f_00009-9-2 loss: 0.889959  [   96/  118]
train() client id: f_00009-10-0 loss: 0.856495  [   32/  118]
train() client id: f_00009-10-1 loss: 0.892781  [   64/  118]
train() client id: f_00009-10-2 loss: 0.863345  [   96/  118]
train() client id: f_00009-11-0 loss: 0.812428  [   32/  118]
train() client id: f_00009-11-1 loss: 0.811559  [   64/  118]
train() client id: f_00009-11-2 loss: 1.034857  [   96/  118]
At round 30 accuracy: 0.6525198938992043
At round 30 training accuracy: 0.5855130784708249
At round 30 training loss: 0.8339461390112279
update_location
xs = [  -3.9056584     4.20031788  170.00902392   18.81129433    0.97929623
    3.95640986 -132.44319194 -111.32485185  154.66397685  -97.06087855]
ys = [ 162.5879595   145.55583871    1.32061395 -132.45517586  124.35018685
  107.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [190.91908951 176.64694974 197.24302835 167.02765761 159.57420841
 147.10385542 165.97617188 149.64591172 185.01247447 139.41601774]
dists_bs = [171.5439666  181.60317332 386.25866427 363.4439475  183.12322702
 191.15264835 182.65190108 185.45182381 365.32568415 187.92285621]
uav_gains = [1.94386836e-11 2.39101861e-11 1.77469892e-11 2.76188258e-11
 3.10151017e-11 3.80704871e-11 2.80673490e-11 3.64667227e-11
 2.11653608e-11 4.35559512e-11]
bs_gains = [6.12410786e-11 5.22093724e-11 6.31008238e-12 7.48288810e-12
 5.10049673e-11 4.52302711e-11 5.13743493e-11 4.92319435e-11
 7.37546672e-12 4.74407113e-11]
Round 31
-------------------------------
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.90000386 14.30569929  6.7966894   2.4461517  16.50004725  7.94229532
  3.0341758   9.71414062  7.16398278  6.44283898]
obj_prev = 81.24602500345188
eta_min = 3.803699202355281e-14	eta_max = 0.9265448863059464
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 18.86522301759883	eta = 0.9090909090909091
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 33.97406433952712	eta = 0.5048027981544223
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 26.610863860443008	eta = 0.6444812477044536
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.281958371636836	eta = 0.6783573681741374
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.213171638427752	eta = 0.6802080670062467
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.212972778010624	eta = 0.680213431961071
eta = 0.680213431961071
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [0.03179805 0.06687685 0.03129332 0.01085171 0.07722383 0.03684536
 0.01362773 0.04517342 0.03280751 0.02977912]
ene_total = [2.23637289 4.06508358 2.2200981  1.04612844 4.63524192 2.42743785
 1.19692013 2.90387674 2.44646747 2.03534566]
ti_comp = [0.42879245 0.44662227 0.42670833 0.43604005 0.44628188 0.44447784
 0.43634521 0.44101196 0.40063459 0.44520477]
ti_coms = [0.08920181 0.07137199 0.09128593 0.08195421 0.07171238 0.07351642
 0.08164905 0.0769823  0.11735967 0.07278949]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.09291688e-05 9.37187797e-05 1.05189542e-05 4.20070663e-07
 1.44515864e-04 1.58244307e-05 8.30788735e-07 2.96229369e-05
 1.37500070e-05 8.32714570e-06]
ene_total = [0.49052461 0.39714469 0.5019487  0.45014129 0.40180415 0.40464437
 0.44848781 0.42443794 0.64533131 0.40024009]
optimize_network_iter = 0 obj = 4.564704952114099
eta = 0.680213431961071
freqs = [37078600.58661079 74869583.15751041 36668274.98538263 12443483.84142069
 86519118.7997421  41447912.13753933 15615771.9937943  51215637.28090309
 40944429.42168844 33444300.55685099]
eta_min = 0.6802134319610765	eta_max = 0.6802134319610728
af = 0.014970687331339992	bf = 1.4539623964515263	zeta = 0.016467756064473992	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [2.70316510e-06 2.31799269e-05 2.60170470e-06 1.03898144e-07
 3.57438195e-05 3.91393429e-06 2.05483065e-07 7.32678673e-06
 3.40085688e-06 2.05959391e-06]
ene_total = [1.72390573 1.38338877 1.76415135 1.58337944 1.39239245 1.42109708
 1.57750334 1.48871744 2.26805167 1.40669455]
ti_comp = [0.42879245 0.44662227 0.42670833 0.43604005 0.44628188 0.44447784
 0.43634521 0.44101196 0.40063459 0.44520477]
ti_coms = [0.08920181 0.07137199 0.09128593 0.08195421 0.07171238 0.07351642
 0.08164905 0.0769823  0.11735967 0.07278949]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.09291688e-05 9.37187797e-05 1.05189542e-05 4.20070663e-07
 1.44515864e-04 1.58244307e-05 8.30788735e-07 2.96229369e-05
 1.37500070e-05 8.32714570e-06]
ene_total = [0.49052461 0.39714469 0.5019487  0.45014129 0.40180415 0.40464437
 0.44848781 0.42443794 0.64533131 0.40024009]
optimize_network_iter = 1 obj = 4.564704952114124
eta = 0.6802134319610728
freqs = [37078600.58661079 74869583.15751038 36668274.98538262 12443483.84142068
 86519118.79974204 41447912.1375393  15615771.9937943  51215637.28090306
 40944429.42168847 33444300.55685098]
Done!
At round 31 eta: 0.6802134319610728
At round 31 local rounds: 12.618272918924527
At round 31 global rounds: 54.923137353497395
At round 31 a_n: 17.221135753236222
gradient difference: 0.41974586248397827
train() client id: f_00000-0-0 loss: 1.133054  [   32/  126]
train() client id: f_00000-0-1 loss: 1.201771  [   64/  126]
train() client id: f_00000-0-2 loss: 1.098244  [   96/  126]
train() client id: f_00000-1-0 loss: 1.252545  [   32/  126]
train() client id: f_00000-1-1 loss: 1.057777  [   64/  126]
train() client id: f_00000-1-2 loss: 1.112834  [   96/  126]
train() client id: f_00000-2-0 loss: 0.930685  [   32/  126]
train() client id: f_00000-2-1 loss: 1.045913  [   64/  126]
train() client id: f_00000-2-2 loss: 1.029374  [   96/  126]
train() client id: f_00000-3-0 loss: 0.908609  [   32/  126]
train() client id: f_00000-3-1 loss: 0.990556  [   64/  126]
train() client id: f_00000-3-2 loss: 0.846934  [   96/  126]
train() client id: f_00000-4-0 loss: 0.815864  [   32/  126]
train() client id: f_00000-4-1 loss: 0.977231  [   64/  126]
train() client id: f_00000-4-2 loss: 0.897536  [   96/  126]
train() client id: f_00000-5-0 loss: 0.873251  [   32/  126]
train() client id: f_00000-5-1 loss: 0.891754  [   64/  126]
train() client id: f_00000-5-2 loss: 0.816138  [   96/  126]
train() client id: f_00000-6-0 loss: 0.966234  [   32/  126]
train() client id: f_00000-6-1 loss: 0.709649  [   64/  126]
train() client id: f_00000-6-2 loss: 0.789982  [   96/  126]
train() client id: f_00000-7-0 loss: 0.765215  [   32/  126]
train() client id: f_00000-7-1 loss: 0.875355  [   64/  126]
train() client id: f_00000-7-2 loss: 0.878724  [   96/  126]
train() client id: f_00000-8-0 loss: 0.872792  [   32/  126]
train() client id: f_00000-8-1 loss: 0.820083  [   64/  126]
train() client id: f_00000-8-2 loss: 0.758545  [   96/  126]
train() client id: f_00000-9-0 loss: 0.917719  [   32/  126]
train() client id: f_00000-9-1 loss: 0.875895  [   64/  126]
train() client id: f_00000-9-2 loss: 0.719325  [   96/  126]
train() client id: f_00000-10-0 loss: 0.824869  [   32/  126]
train() client id: f_00000-10-1 loss: 0.825730  [   64/  126]
train() client id: f_00000-10-2 loss: 0.789619  [   96/  126]
train() client id: f_00000-11-0 loss: 0.862226  [   32/  126]
train() client id: f_00000-11-1 loss: 0.855638  [   64/  126]
train() client id: f_00000-11-2 loss: 0.791075  [   96/  126]
train() client id: f_00001-0-0 loss: 0.367741  [   32/  265]
train() client id: f_00001-0-1 loss: 0.433802  [   64/  265]
train() client id: f_00001-0-2 loss: 0.478834  [   96/  265]
train() client id: f_00001-0-3 loss: 0.408874  [  128/  265]
train() client id: f_00001-0-4 loss: 0.455792  [  160/  265]
train() client id: f_00001-0-5 loss: 0.536499  [  192/  265]
train() client id: f_00001-0-6 loss: 0.463727  [  224/  265]
train() client id: f_00001-0-7 loss: 0.541073  [  256/  265]
train() client id: f_00001-1-0 loss: 0.435619  [   32/  265]
train() client id: f_00001-1-1 loss: 0.418838  [   64/  265]
train() client id: f_00001-1-2 loss: 0.392405  [   96/  265]
train() client id: f_00001-1-3 loss: 0.392444  [  128/  265]
train() client id: f_00001-1-4 loss: 0.370941  [  160/  265]
train() client id: f_00001-1-5 loss: 0.392469  [  192/  265]
train() client id: f_00001-1-6 loss: 0.589398  [  224/  265]
train() client id: f_00001-1-7 loss: 0.579454  [  256/  265]
train() client id: f_00001-2-0 loss: 0.361887  [   32/  265]
train() client id: f_00001-2-1 loss: 0.362752  [   64/  265]
train() client id: f_00001-2-2 loss: 0.440986  [   96/  265]
train() client id: f_00001-2-3 loss: 0.353231  [  128/  265]
train() client id: f_00001-2-4 loss: 0.476419  [  160/  265]
train() client id: f_00001-2-5 loss: 0.494021  [  192/  265]
train() client id: f_00001-2-6 loss: 0.483021  [  224/  265]
train() client id: f_00001-2-7 loss: 0.633388  [  256/  265]
train() client id: f_00001-3-0 loss: 0.390933  [   32/  265]
train() client id: f_00001-3-1 loss: 0.430053  [   64/  265]
train() client id: f_00001-3-2 loss: 0.401247  [   96/  265]
train() client id: f_00001-3-3 loss: 0.538465  [  128/  265]
train() client id: f_00001-3-4 loss: 0.420715  [  160/  265]
train() client id: f_00001-3-5 loss: 0.460803  [  192/  265]
train() client id: f_00001-3-6 loss: 0.394274  [  224/  265]
train() client id: f_00001-3-7 loss: 0.433217  [  256/  265]
train() client id: f_00001-4-0 loss: 0.422049  [   32/  265]
train() client id: f_00001-4-1 loss: 0.401239  [   64/  265]
train() client id: f_00001-4-2 loss: 0.409095  [   96/  265]
train() client id: f_00001-4-3 loss: 0.389370  [  128/  265]
train() client id: f_00001-4-4 loss: 0.538946  [  160/  265]
train() client id: f_00001-4-5 loss: 0.443788  [  192/  265]
train() client id: f_00001-4-6 loss: 0.412542  [  224/  265]
train() client id: f_00001-4-7 loss: 0.529805  [  256/  265]
train() client id: f_00001-5-0 loss: 0.508250  [   32/  265]
train() client id: f_00001-5-1 loss: 0.424596  [   64/  265]
train() client id: f_00001-5-2 loss: 0.476808  [   96/  265]
train() client id: f_00001-5-3 loss: 0.382124  [  128/  265]
train() client id: f_00001-5-4 loss: 0.428305  [  160/  265]
train() client id: f_00001-5-5 loss: 0.343907  [  192/  265]
train() client id: f_00001-5-6 loss: 0.544080  [  224/  265]
train() client id: f_00001-5-7 loss: 0.421277  [  256/  265]
train() client id: f_00001-6-0 loss: 0.352370  [   32/  265]
train() client id: f_00001-6-1 loss: 0.378704  [   64/  265]
train() client id: f_00001-6-2 loss: 0.463516  [   96/  265]
train() client id: f_00001-6-3 loss: 0.403343  [  128/  265]
train() client id: f_00001-6-4 loss: 0.484194  [  160/  265]
train() client id: f_00001-6-5 loss: 0.383553  [  192/  265]
train() client id: f_00001-6-6 loss: 0.540444  [  224/  265]
train() client id: f_00001-6-7 loss: 0.450661  [  256/  265]
train() client id: f_00001-7-0 loss: 0.497666  [   32/  265]
train() client id: f_00001-7-1 loss: 0.409342  [   64/  265]
train() client id: f_00001-7-2 loss: 0.478868  [   96/  265]
train() client id: f_00001-7-3 loss: 0.345843  [  128/  265]
train() client id: f_00001-7-4 loss: 0.446724  [  160/  265]
train() client id: f_00001-7-5 loss: 0.416175  [  192/  265]
train() client id: f_00001-7-6 loss: 0.526067  [  224/  265]
train() client id: f_00001-7-7 loss: 0.401423  [  256/  265]
train() client id: f_00001-8-0 loss: 0.403867  [   32/  265]
train() client id: f_00001-8-1 loss: 0.362430  [   64/  265]
train() client id: f_00001-8-2 loss: 0.421547  [   96/  265]
train() client id: f_00001-8-3 loss: 0.479663  [  128/  265]
train() client id: f_00001-8-4 loss: 0.419366  [  160/  265]
train() client id: f_00001-8-5 loss: 0.611338  [  192/  265]
train() client id: f_00001-8-6 loss: 0.458379  [  224/  265]
train() client id: f_00001-8-7 loss: 0.344907  [  256/  265]
train() client id: f_00001-9-0 loss: 0.410849  [   32/  265]
train() client id: f_00001-9-1 loss: 0.403962  [   64/  265]
train() client id: f_00001-9-2 loss: 0.401203  [   96/  265]
train() client id: f_00001-9-3 loss: 0.459623  [  128/  265]
train() client id: f_00001-9-4 loss: 0.356046  [  160/  265]
train() client id: f_00001-9-5 loss: 0.575463  [  192/  265]
train() client id: f_00001-9-6 loss: 0.436383  [  224/  265]
train() client id: f_00001-9-7 loss: 0.471856  [  256/  265]
train() client id: f_00001-10-0 loss: 0.402819  [   32/  265]
train() client id: f_00001-10-1 loss: 0.475239  [   64/  265]
train() client id: f_00001-10-2 loss: 0.343608  [   96/  265]
train() client id: f_00001-10-3 loss: 0.501698  [  128/  265]
train() client id: f_00001-10-4 loss: 0.477711  [  160/  265]
train() client id: f_00001-10-5 loss: 0.391799  [  192/  265]
train() client id: f_00001-10-6 loss: 0.476034  [  224/  265]
train() client id: f_00001-10-7 loss: 0.438089  [  256/  265]
train() client id: f_00001-11-0 loss: 0.385990  [   32/  265]
train() client id: f_00001-11-1 loss: 0.438563  [   64/  265]
train() client id: f_00001-11-2 loss: 0.431011  [   96/  265]
train() client id: f_00001-11-3 loss: 0.561707  [  128/  265]
train() client id: f_00001-11-4 loss: 0.387226  [  160/  265]
train() client id: f_00001-11-5 loss: 0.445482  [  192/  265]
train() client id: f_00001-11-6 loss: 0.407246  [  224/  265]
train() client id: f_00001-11-7 loss: 0.396121  [  256/  265]
train() client id: f_00002-0-0 loss: 1.234268  [   32/  124]
train() client id: f_00002-0-1 loss: 1.232719  [   64/  124]
train() client id: f_00002-0-2 loss: 1.286754  [   96/  124]
train() client id: f_00002-1-0 loss: 1.329260  [   32/  124]
train() client id: f_00002-1-1 loss: 1.141314  [   64/  124]
train() client id: f_00002-1-2 loss: 1.123689  [   96/  124]
train() client id: f_00002-2-0 loss: 1.085632  [   32/  124]
train() client id: f_00002-2-1 loss: 1.224547  [   64/  124]
train() client id: f_00002-2-2 loss: 1.313670  [   96/  124]
train() client id: f_00002-3-0 loss: 1.134866  [   32/  124]
train() client id: f_00002-3-1 loss: 1.033615  [   64/  124]
train() client id: f_00002-3-2 loss: 1.228256  [   96/  124]
train() client id: f_00002-4-0 loss: 1.037090  [   32/  124]
train() client id: f_00002-4-1 loss: 1.054249  [   64/  124]
train() client id: f_00002-4-2 loss: 1.213054  [   96/  124]
train() client id: f_00002-5-0 loss: 1.120426  [   32/  124]
train() client id: f_00002-5-1 loss: 1.142104  [   64/  124]
train() client id: f_00002-5-2 loss: 0.994875  [   96/  124]
train() client id: f_00002-6-0 loss: 0.967148  [   32/  124]
train() client id: f_00002-6-1 loss: 1.006481  [   64/  124]
train() client id: f_00002-6-2 loss: 1.190928  [   96/  124]
train() client id: f_00002-7-0 loss: 1.124874  [   32/  124]
train() client id: f_00002-7-1 loss: 1.080440  [   64/  124]
train() client id: f_00002-7-2 loss: 1.111217  [   96/  124]
train() client id: f_00002-8-0 loss: 1.098115  [   32/  124]
train() client id: f_00002-8-1 loss: 1.126428  [   64/  124]
train() client id: f_00002-8-2 loss: 1.044059  [   96/  124]
train() client id: f_00002-9-0 loss: 0.956802  [   32/  124]
train() client id: f_00002-9-1 loss: 1.053285  [   64/  124]
train() client id: f_00002-9-2 loss: 1.087667  [   96/  124]
train() client id: f_00002-10-0 loss: 1.018820  [   32/  124]
train() client id: f_00002-10-1 loss: 1.024084  [   64/  124]
train() client id: f_00002-10-2 loss: 1.016186  [   96/  124]
train() client id: f_00002-11-0 loss: 1.029315  [   32/  124]
train() client id: f_00002-11-1 loss: 0.977833  [   64/  124]
train() client id: f_00002-11-2 loss: 0.993683  [   96/  124]
train() client id: f_00003-0-0 loss: 0.578655  [   32/   43]
train() client id: f_00003-1-0 loss: 0.694587  [   32/   43]
train() client id: f_00003-2-0 loss: 0.659737  [   32/   43]
train() client id: f_00003-3-0 loss: 0.716917  [   32/   43]
train() client id: f_00003-4-0 loss: 0.546660  [   32/   43]
train() client id: f_00003-5-0 loss: 0.796452  [   32/   43]
train() client id: f_00003-6-0 loss: 0.556249  [   32/   43]
train() client id: f_00003-7-0 loss: 0.558403  [   32/   43]
train() client id: f_00003-8-0 loss: 0.609688  [   32/   43]
train() client id: f_00003-9-0 loss: 0.787300  [   32/   43]
train() client id: f_00003-10-0 loss: 0.700216  [   32/   43]
train() client id: f_00003-11-0 loss: 0.668395  [   32/   43]
train() client id: f_00004-0-0 loss: 0.867826  [   32/  306]
train() client id: f_00004-0-1 loss: 0.766277  [   64/  306]
train() client id: f_00004-0-2 loss: 0.737253  [   96/  306]
train() client id: f_00004-0-3 loss: 0.698090  [  128/  306]
train() client id: f_00004-0-4 loss: 0.869684  [  160/  306]
train() client id: f_00004-0-5 loss: 0.773875  [  192/  306]
train() client id: f_00004-0-6 loss: 0.696380  [  224/  306]
train() client id: f_00004-0-7 loss: 0.764422  [  256/  306]
train() client id: f_00004-0-8 loss: 0.836819  [  288/  306]
train() client id: f_00004-1-0 loss: 0.963720  [   32/  306]
train() client id: f_00004-1-1 loss: 0.737112  [   64/  306]
train() client id: f_00004-1-2 loss: 0.806065  [   96/  306]
train() client id: f_00004-1-3 loss: 0.864221  [  128/  306]
train() client id: f_00004-1-4 loss: 0.705456  [  160/  306]
train() client id: f_00004-1-5 loss: 0.749189  [  192/  306]
train() client id: f_00004-1-6 loss: 0.707481  [  224/  306]
train() client id: f_00004-1-7 loss: 0.811614  [  256/  306]
train() client id: f_00004-1-8 loss: 0.802383  [  288/  306]
train() client id: f_00004-2-0 loss: 0.662758  [   32/  306]
train() client id: f_00004-2-1 loss: 0.708566  [   64/  306]
train() client id: f_00004-2-2 loss: 0.794878  [   96/  306]
train() client id: f_00004-2-3 loss: 0.730508  [  128/  306]
train() client id: f_00004-2-4 loss: 0.836424  [  160/  306]
train() client id: f_00004-2-5 loss: 0.944619  [  192/  306]
train() client id: f_00004-2-6 loss: 0.913540  [  224/  306]
train() client id: f_00004-2-7 loss: 0.793036  [  256/  306]
train() client id: f_00004-2-8 loss: 0.747313  [  288/  306]
train() client id: f_00004-3-0 loss: 0.838092  [   32/  306]
train() client id: f_00004-3-1 loss: 0.814184  [   64/  306]
train() client id: f_00004-3-2 loss: 0.867479  [   96/  306]
train() client id: f_00004-3-3 loss: 0.845095  [  128/  306]
train() client id: f_00004-3-4 loss: 0.743423  [  160/  306]
train() client id: f_00004-3-5 loss: 0.846653  [  192/  306]
train() client id: f_00004-3-6 loss: 0.626357  [  224/  306]
train() client id: f_00004-3-7 loss: 0.766794  [  256/  306]
train() client id: f_00004-3-8 loss: 0.796377  [  288/  306]
train() client id: f_00004-4-0 loss: 0.583050  [   32/  306]
train() client id: f_00004-4-1 loss: 0.852488  [   64/  306]
train() client id: f_00004-4-2 loss: 0.795767  [   96/  306]
train() client id: f_00004-4-3 loss: 0.818032  [  128/  306]
train() client id: f_00004-4-4 loss: 0.893811  [  160/  306]
train() client id: f_00004-4-5 loss: 0.750747  [  192/  306]
train() client id: f_00004-4-6 loss: 0.776051  [  224/  306]
train() client id: f_00004-4-7 loss: 0.834888  [  256/  306]
train() client id: f_00004-4-8 loss: 0.841255  [  288/  306]
train() client id: f_00004-5-0 loss: 0.805016  [   32/  306]
train() client id: f_00004-5-1 loss: 0.669214  [   64/  306]
train() client id: f_00004-5-2 loss: 0.840483  [   96/  306]
train() client id: f_00004-5-3 loss: 0.772784  [  128/  306]
train() client id: f_00004-5-4 loss: 0.847192  [  160/  306]
train() client id: f_00004-5-5 loss: 0.844299  [  192/  306]
train() client id: f_00004-5-6 loss: 0.658502  [  224/  306]
train() client id: f_00004-5-7 loss: 0.781813  [  256/  306]
train() client id: f_00004-5-8 loss: 0.896742  [  288/  306]
train() client id: f_00004-6-0 loss: 0.684326  [   32/  306]
train() client id: f_00004-6-1 loss: 0.789653  [   64/  306]
train() client id: f_00004-6-2 loss: 0.751278  [   96/  306]
train() client id: f_00004-6-3 loss: 0.840308  [  128/  306]
train() client id: f_00004-6-4 loss: 0.761793  [  160/  306]
train() client id: f_00004-6-5 loss: 0.770051  [  192/  306]
train() client id: f_00004-6-6 loss: 0.830974  [  224/  306]
train() client id: f_00004-6-7 loss: 0.833184  [  256/  306]
train() client id: f_00004-6-8 loss: 0.868903  [  288/  306]
train() client id: f_00004-7-0 loss: 0.847973  [   32/  306]
train() client id: f_00004-7-1 loss: 0.937695  [   64/  306]
train() client id: f_00004-7-2 loss: 0.769331  [   96/  306]
train() client id: f_00004-7-3 loss: 0.751565  [  128/  306]
train() client id: f_00004-7-4 loss: 0.744192  [  160/  306]
train() client id: f_00004-7-5 loss: 0.861113  [  192/  306]
train() client id: f_00004-7-6 loss: 0.748154  [  224/  306]
train() client id: f_00004-7-7 loss: 0.796125  [  256/  306]
train() client id: f_00004-7-8 loss: 0.788803  [  288/  306]
train() client id: f_00004-8-0 loss: 0.786901  [   32/  306]
train() client id: f_00004-8-1 loss: 0.709624  [   64/  306]
train() client id: f_00004-8-2 loss: 0.721522  [   96/  306]
train() client id: f_00004-8-3 loss: 0.813033  [  128/  306]
train() client id: f_00004-8-4 loss: 0.879708  [  160/  306]
train() client id: f_00004-8-5 loss: 0.812201  [  192/  306]
train() client id: f_00004-8-6 loss: 0.841168  [  224/  306]
train() client id: f_00004-8-7 loss: 0.803743  [  256/  306]
train() client id: f_00004-8-8 loss: 0.778485  [  288/  306]
train() client id: f_00004-9-0 loss: 0.743059  [   32/  306]
train() client id: f_00004-9-1 loss: 0.894390  [   64/  306]
train() client id: f_00004-9-2 loss: 0.875688  [   96/  306]
train() client id: f_00004-9-3 loss: 0.638027  [  128/  306]
train() client id: f_00004-9-4 loss: 0.921810  [  160/  306]
train() client id: f_00004-9-5 loss: 0.644642  [  192/  306]
train() client id: f_00004-9-6 loss: 0.736751  [  224/  306]
train() client id: f_00004-9-7 loss: 0.827930  [  256/  306]
train() client id: f_00004-9-8 loss: 0.863908  [  288/  306]
train() client id: f_00004-10-0 loss: 0.799418  [   32/  306]
train() client id: f_00004-10-1 loss: 0.828418  [   64/  306]
train() client id: f_00004-10-2 loss: 0.728569  [   96/  306]
train() client id: f_00004-10-3 loss: 0.785993  [  128/  306]
train() client id: f_00004-10-4 loss: 0.799813  [  160/  306]
train() client id: f_00004-10-5 loss: 0.674872  [  192/  306]
train() client id: f_00004-10-6 loss: 0.797259  [  224/  306]
train() client id: f_00004-10-7 loss: 0.735076  [  256/  306]
train() client id: f_00004-10-8 loss: 0.902695  [  288/  306]
train() client id: f_00004-11-0 loss: 0.793722  [   32/  306]
train() client id: f_00004-11-1 loss: 0.740414  [   64/  306]
train() client id: f_00004-11-2 loss: 0.946878  [   96/  306]
train() client id: f_00004-11-3 loss: 0.792041  [  128/  306]
train() client id: f_00004-11-4 loss: 0.827899  [  160/  306]
train() client id: f_00004-11-5 loss: 0.721113  [  192/  306]
train() client id: f_00004-11-6 loss: 0.737669  [  224/  306]
train() client id: f_00004-11-7 loss: 0.727603  [  256/  306]
train() client id: f_00004-11-8 loss: 0.909170  [  288/  306]
train() client id: f_00005-0-0 loss: 0.777403  [   32/  146]
train() client id: f_00005-0-1 loss: 0.663493  [   64/  146]
train() client id: f_00005-0-2 loss: 0.719881  [   96/  146]
train() client id: f_00005-0-3 loss: 0.704346  [  128/  146]
train() client id: f_00005-1-0 loss: 0.532421  [   32/  146]
train() client id: f_00005-1-1 loss: 0.861762  [   64/  146]
train() client id: f_00005-1-2 loss: 0.782041  [   96/  146]
train() client id: f_00005-1-3 loss: 0.467719  [  128/  146]
train() client id: f_00005-2-0 loss: 0.723610  [   32/  146]
train() client id: f_00005-2-1 loss: 0.604856  [   64/  146]
train() client id: f_00005-2-2 loss: 0.802592  [   96/  146]
train() client id: f_00005-2-3 loss: 0.708620  [  128/  146]
train() client id: f_00005-3-0 loss: 0.520333  [   32/  146]
train() client id: f_00005-3-1 loss: 0.750487  [   64/  146]
train() client id: f_00005-3-2 loss: 0.715602  [   96/  146]
train() client id: f_00005-3-3 loss: 0.726600  [  128/  146]
train() client id: f_00005-4-0 loss: 0.569777  [   32/  146]
train() client id: f_00005-4-1 loss: 0.910801  [   64/  146]
train() client id: f_00005-4-2 loss: 0.535727  [   96/  146]
train() client id: f_00005-4-3 loss: 0.652994  [  128/  146]
train() client id: f_00005-5-0 loss: 0.565512  [   32/  146]
train() client id: f_00005-5-1 loss: 0.596888  [   64/  146]
train() client id: f_00005-5-2 loss: 0.808229  [   96/  146]
train() client id: f_00005-5-3 loss: 0.680223  [  128/  146]
train() client id: f_00005-6-0 loss: 0.472741  [   32/  146]
train() client id: f_00005-6-1 loss: 0.761186  [   64/  146]
train() client id: f_00005-6-2 loss: 0.718005  [   96/  146]
train() client id: f_00005-6-3 loss: 0.708635  [  128/  146]
train() client id: f_00005-7-0 loss: 0.840772  [   32/  146]
train() client id: f_00005-7-1 loss: 0.541009  [   64/  146]
train() client id: f_00005-7-2 loss: 0.554069  [   96/  146]
train() client id: f_00005-7-3 loss: 0.730028  [  128/  146]
train() client id: f_00005-8-0 loss: 0.650060  [   32/  146]
train() client id: f_00005-8-1 loss: 0.851499  [   64/  146]
train() client id: f_00005-8-2 loss: 0.656969  [   96/  146]
train() client id: f_00005-8-3 loss: 0.547539  [  128/  146]
train() client id: f_00005-9-0 loss: 1.022586  [   32/  146]
train() client id: f_00005-9-1 loss: 0.464597  [   64/  146]
train() client id: f_00005-9-2 loss: 0.539849  [   96/  146]
train() client id: f_00005-9-3 loss: 0.557341  [  128/  146]
train() client id: f_00005-10-0 loss: 0.779210  [   32/  146]
train() client id: f_00005-10-1 loss: 0.441203  [   64/  146]
train() client id: f_00005-10-2 loss: 0.634521  [   96/  146]
train() client id: f_00005-10-3 loss: 0.871887  [  128/  146]
train() client id: f_00005-11-0 loss: 0.629390  [   32/  146]
train() client id: f_00005-11-1 loss: 0.570685  [   64/  146]
train() client id: f_00005-11-2 loss: 0.712832  [   96/  146]
train() client id: f_00005-11-3 loss: 0.734776  [  128/  146]
train() client id: f_00006-0-0 loss: 0.562871  [   32/   54]
train() client id: f_00006-1-0 loss: 0.477786  [   32/   54]
train() client id: f_00006-2-0 loss: 0.532094  [   32/   54]
train() client id: f_00006-3-0 loss: 0.524697  [   32/   54]
train() client id: f_00006-4-0 loss: 0.519771  [   32/   54]
train() client id: f_00006-5-0 loss: 0.478060  [   32/   54]
train() client id: f_00006-6-0 loss: 0.576443  [   32/   54]
train() client id: f_00006-7-0 loss: 0.465144  [   32/   54]
train() client id: f_00006-8-0 loss: 0.533922  [   32/   54]
train() client id: f_00006-9-0 loss: 0.573042  [   32/   54]
train() client id: f_00006-10-0 loss: 0.581334  [   32/   54]
train() client id: f_00006-11-0 loss: 0.514652  [   32/   54]
train() client id: f_00007-0-0 loss: 0.633186  [   32/  179]
train() client id: f_00007-0-1 loss: 0.499591  [   64/  179]
train() client id: f_00007-0-2 loss: 0.674489  [   96/  179]
train() client id: f_00007-0-3 loss: 0.540963  [  128/  179]
train() client id: f_00007-0-4 loss: 0.604985  [  160/  179]
train() client id: f_00007-1-0 loss: 0.430678  [   32/  179]
train() client id: f_00007-1-1 loss: 0.730612  [   64/  179]
train() client id: f_00007-1-2 loss: 0.589213  [   96/  179]
train() client id: f_00007-1-3 loss: 0.633465  [  128/  179]
train() client id: f_00007-1-4 loss: 0.452953  [  160/  179]
train() client id: f_00007-2-0 loss: 0.707737  [   32/  179]
train() client id: f_00007-2-1 loss: 0.581779  [   64/  179]
train() client id: f_00007-2-2 loss: 0.575450  [   96/  179]
train() client id: f_00007-2-3 loss: 0.442922  [  128/  179]
train() client id: f_00007-2-4 loss: 0.563617  [  160/  179]
train() client id: f_00007-3-0 loss: 0.462548  [   32/  179]
train() client id: f_00007-3-1 loss: 0.597011  [   64/  179]
train() client id: f_00007-3-2 loss: 0.541571  [   96/  179]
train() client id: f_00007-3-3 loss: 0.718293  [  128/  179]
train() client id: f_00007-3-4 loss: 0.491304  [  160/  179]
train() client id: f_00007-4-0 loss: 0.359328  [   32/  179]
train() client id: f_00007-4-1 loss: 0.693933  [   64/  179]
train() client id: f_00007-4-2 loss: 0.626573  [   96/  179]
train() client id: f_00007-4-3 loss: 0.566588  [  128/  179]
train() client id: f_00007-4-4 loss: 0.551295  [  160/  179]
train() client id: f_00007-5-0 loss: 0.346964  [   32/  179]
train() client id: f_00007-5-1 loss: 0.714053  [   64/  179]
train() client id: f_00007-5-2 loss: 0.668975  [   96/  179]
train() client id: f_00007-5-3 loss: 0.596851  [  128/  179]
train() client id: f_00007-5-4 loss: 0.405320  [  160/  179]
train() client id: f_00007-6-0 loss: 0.623372  [   32/  179]
train() client id: f_00007-6-1 loss: 0.555257  [   64/  179]
train() client id: f_00007-6-2 loss: 0.667377  [   96/  179]
train() client id: f_00007-6-3 loss: 0.406110  [  128/  179]
train() client id: f_00007-6-4 loss: 0.499531  [  160/  179]
train() client id: f_00007-7-0 loss: 0.634590  [   32/  179]
train() client id: f_00007-7-1 loss: 0.551431  [   64/  179]
train() client id: f_00007-7-2 loss: 0.588413  [   96/  179]
train() client id: f_00007-7-3 loss: 0.389046  [  128/  179]
train() client id: f_00007-7-4 loss: 0.491366  [  160/  179]
train() client id: f_00007-8-0 loss: 0.556307  [   32/  179]
train() client id: f_00007-8-1 loss: 0.472331  [   64/  179]
train() client id: f_00007-8-2 loss: 0.695436  [   96/  179]
train() client id: f_00007-8-3 loss: 0.428802  [  128/  179]
train() client id: f_00007-8-4 loss: 0.445051  [  160/  179]
train() client id: f_00007-9-0 loss: 0.432515  [   32/  179]
train() client id: f_00007-9-1 loss: 0.400163  [   64/  179]
train() client id: f_00007-9-2 loss: 0.657597  [   96/  179]
train() client id: f_00007-9-3 loss: 0.503295  [  128/  179]
train() client id: f_00007-9-4 loss: 0.731186  [  160/  179]
train() client id: f_00007-10-0 loss: 0.596362  [   32/  179]
train() client id: f_00007-10-1 loss: 0.613736  [   64/  179]
train() client id: f_00007-10-2 loss: 0.457487  [   96/  179]
train() client id: f_00007-10-3 loss: 0.390034  [  128/  179]
train() client id: f_00007-10-4 loss: 0.576289  [  160/  179]
train() client id: f_00007-11-0 loss: 0.476931  [   32/  179]
train() client id: f_00007-11-1 loss: 0.596740  [   64/  179]
train() client id: f_00007-11-2 loss: 0.384659  [   96/  179]
train() client id: f_00007-11-3 loss: 0.474436  [  128/  179]
train() client id: f_00007-11-4 loss: 0.550530  [  160/  179]
train() client id: f_00008-0-0 loss: 0.701342  [   32/  130]
train() client id: f_00008-0-1 loss: 0.599767  [   64/  130]
train() client id: f_00008-0-2 loss: 0.670654  [   96/  130]
train() client id: f_00008-0-3 loss: 0.502756  [  128/  130]
train() client id: f_00008-1-0 loss: 0.619891  [   32/  130]
train() client id: f_00008-1-1 loss: 0.564857  [   64/  130]
train() client id: f_00008-1-2 loss: 0.718176  [   96/  130]
train() client id: f_00008-1-3 loss: 0.539294  [  128/  130]
train() client id: f_00008-2-0 loss: 0.548148  [   32/  130]
train() client id: f_00008-2-1 loss: 0.730204  [   64/  130]
train() client id: f_00008-2-2 loss: 0.593857  [   96/  130]
train() client id: f_00008-2-3 loss: 0.523499  [  128/  130]
train() client id: f_00008-3-0 loss: 0.595309  [   32/  130]
train() client id: f_00008-3-1 loss: 0.614483  [   64/  130]
train() client id: f_00008-3-2 loss: 0.609731  [   96/  130]
train() client id: f_00008-3-3 loss: 0.641085  [  128/  130]
train() client id: f_00008-4-0 loss: 0.630993  [   32/  130]
train() client id: f_00008-4-1 loss: 0.670312  [   64/  130]
train() client id: f_00008-4-2 loss: 0.559379  [   96/  130]
train() client id: f_00008-4-3 loss: 0.603592  [  128/  130]
train() client id: f_00008-5-0 loss: 0.615939  [   32/  130]
train() client id: f_00008-5-1 loss: 0.612717  [   64/  130]
train() client id: f_00008-5-2 loss: 0.524408  [   96/  130]
train() client id: f_00008-5-3 loss: 0.715506  [  128/  130]
train() client id: f_00008-6-0 loss: 0.673238  [   32/  130]
train() client id: f_00008-6-1 loss: 0.596284  [   64/  130]
train() client id: f_00008-6-2 loss: 0.538267  [   96/  130]
train() client id: f_00008-6-3 loss: 0.631191  [  128/  130]
train() client id: f_00008-7-0 loss: 0.568921  [   32/  130]
train() client id: f_00008-7-1 loss: 0.686093  [   64/  130]
train() client id: f_00008-7-2 loss: 0.668290  [   96/  130]
train() client id: f_00008-7-3 loss: 0.549898  [  128/  130]
train() client id: f_00008-8-0 loss: 0.644352  [   32/  130]
train() client id: f_00008-8-1 loss: 0.599364  [   64/  130]
train() client id: f_00008-8-2 loss: 0.568872  [   96/  130]
train() client id: f_00008-8-3 loss: 0.659615  [  128/  130]
train() client id: f_00008-9-0 loss: 0.643052  [   32/  130]
train() client id: f_00008-9-1 loss: 0.635245  [   64/  130]
train() client id: f_00008-9-2 loss: 0.660131  [   96/  130]
train() client id: f_00008-9-3 loss: 0.531123  [  128/  130]
train() client id: f_00008-10-0 loss: 0.699995  [   32/  130]
train() client id: f_00008-10-1 loss: 0.694997  [   64/  130]
train() client id: f_00008-10-2 loss: 0.459706  [   96/  130]
train() client id: f_00008-10-3 loss: 0.600111  [  128/  130]
train() client id: f_00008-11-0 loss: 0.570750  [   32/  130]
train() client id: f_00008-11-1 loss: 0.628612  [   64/  130]
train() client id: f_00008-11-2 loss: 0.640847  [   96/  130]
train() client id: f_00008-11-3 loss: 0.623695  [  128/  130]
train() client id: f_00009-0-0 loss: 1.090510  [   32/  118]
train() client id: f_00009-0-1 loss: 1.017662  [   64/  118]
train() client id: f_00009-0-2 loss: 1.084587  [   96/  118]
train() client id: f_00009-1-0 loss: 1.070298  [   32/  118]
train() client id: f_00009-1-1 loss: 0.889451  [   64/  118]
train() client id: f_00009-1-2 loss: 0.979528  [   96/  118]
train() client id: f_00009-2-0 loss: 0.885299  [   32/  118]
train() client id: f_00009-2-1 loss: 0.849684  [   64/  118]
train() client id: f_00009-2-2 loss: 0.942598  [   96/  118]
train() client id: f_00009-3-0 loss: 0.850388  [   32/  118]
train() client id: f_00009-3-1 loss: 0.897152  [   64/  118]
train() client id: f_00009-3-2 loss: 0.908884  [   96/  118]
train() client id: f_00009-4-0 loss: 0.812121  [   32/  118]
train() client id: f_00009-4-1 loss: 0.991010  [   64/  118]
train() client id: f_00009-4-2 loss: 0.636518  [   96/  118]
train() client id: f_00009-5-0 loss: 0.697229  [   32/  118]
train() client id: f_00009-5-1 loss: 0.848800  [   64/  118]
train() client id: f_00009-5-2 loss: 0.878331  [   96/  118]
train() client id: f_00009-6-0 loss: 0.833161  [   32/  118]
train() client id: f_00009-6-1 loss: 0.673072  [   64/  118]
train() client id: f_00009-6-2 loss: 0.788143  [   96/  118]
train() client id: f_00009-7-0 loss: 0.893811  [   32/  118]
train() client id: f_00009-7-1 loss: 0.721327  [   64/  118]
train() client id: f_00009-7-2 loss: 0.690010  [   96/  118]
train() client id: f_00009-8-0 loss: 0.792569  [   32/  118]
train() client id: f_00009-8-1 loss: 0.765225  [   64/  118]
train() client id: f_00009-8-2 loss: 0.672677  [   96/  118]
train() client id: f_00009-9-0 loss: 0.646648  [   32/  118]
train() client id: f_00009-9-1 loss: 0.706493  [   64/  118]
train() client id: f_00009-9-2 loss: 0.854618  [   96/  118]
train() client id: f_00009-10-0 loss: 0.659789  [   32/  118]
train() client id: f_00009-10-1 loss: 0.754758  [   64/  118]
train() client id: f_00009-10-2 loss: 0.787452  [   96/  118]
train() client id: f_00009-11-0 loss: 0.680534  [   32/  118]
train() client id: f_00009-11-1 loss: 0.896984  [   64/  118]
train() client id: f_00009-11-2 loss: 0.632542  [   96/  118]
At round 31 accuracy: 0.6525198938992043
At round 31 training accuracy: 0.5814889336016097
At round 31 training loss: 0.840244425892357
update_location
xs = [  -3.9056584     4.20031788  175.00902392   18.81129433    0.97929623
    3.95640986 -137.44319194 -116.32485185  159.66397685 -102.06087855]
ys = [ 167.5879595   150.55583871    1.32061395 -137.45517586  129.35018685
  112.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [195.19471903 180.78911261 201.56860488 171.01985313 163.50055002
 150.80678296 169.99271029 153.40191463 189.21219695 142.94206794]
dists_bs = [171.2548162  180.85981021 390.7311684  367.68336205 181.80379023
 189.45309836 181.55205558 183.79506923 369.84387958 185.90483768]
uav_gains = [1.82789532e-11 2.25026314e-11 1.66686654e-11 2.59975805e-11
 2.91614959e-11 3.57650883e-11 2.64028374e-11 3.42619275e-11
 1.99218930e-11 4.09129213e-11]
bs_gains = [6.15310409e-11 5.28124466e-11 6.10992006e-12 7.24380826e-12
 5.20482189e-11 4.63755733e-11 5.22505430e-11 5.04846428e-11
 7.12594497e-12 4.88967675e-11]
Round 32
-------------------------------
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.76786425 14.02640477  6.66672465  2.4004724  16.17773155  7.78673034
  2.97703515  9.52653575  7.02653207  6.31639524]
obj_prev = 79.67242616820144
eta_min = 2.1062834512478834e-14	eta_max = 0.9270669874026624
af = 16.815721006576958	bf = 1.436444871137545	zeta = 18.497293107234654	eta = 0.9090909090909091
af = 16.815721006576958	bf = 1.436444871137545	zeta = 33.430696901951336	eta = 0.5030024069164837
af = 16.815721006576958	bf = 1.436444871137545	zeta = 26.140048449463926	eta = 0.643293413900379
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.823688418099213	eta = 0.677406222772134
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.755287540536187	eta = 0.6792779513888344
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.755088130913464	eta = 0.6792834231754543
eta = 0.6792834231754543
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [0.03191095 0.06711429 0.03140442 0.01089024 0.07749801 0.03697618
 0.01367612 0.0453338  0.03292399 0.02988485]
ene_total = [2.19996663 3.98610054 2.18445289 1.03108082 4.54479709 2.37816282
 1.17905352 2.85331091 2.40505659 1.99310631]
ti_comp = [0.43830311 0.4576972  0.43612707 0.44578269 0.45748595 0.45576916
 0.44608386 0.45085659 0.41023879 0.45656667]
ti_coms = [0.09059974 0.07120565 0.09277578 0.08312016 0.07141689 0.07313368
 0.08281899 0.07804626 0.11866406 0.07233618]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.05718244e-05 9.01921040e-05 1.01771547e-05 4.06206033e-07
 1.38994033e-04 1.52109415e-05 8.03407087e-07 2.86463737e-05
 1.32539102e-05 8.00248765e-06]
ene_total = [0.48705029 0.38718743 0.49871352 0.44634221 0.39094219 0.39351399
 0.44474637 0.42061388 0.63788794 0.38884464]
optimize_network_iter = 0 obj = 4.495842450611667
eta = 0.6792834231754543
freqs = [36402828.40748152 73317349.05671008 36003753.27798384 12214744.53533785
 84699880.32505599 40564586.30164858 15329090.24416601 50275192.55652302
 40127838.49551824 32727809.18735842]
eta_min = 0.6792834231754692	eta_max = 0.6792834231754555
af = 0.014083508139639458	bf = 1.436444871137545	zeta = 0.015491858953603405	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [2.60553049e-06 2.22287345e-05 2.50826025e-06 1.00113487e-07
 3.42564517e-05 3.74888670e-06 1.98007609e-07 7.06018162e-06
 3.26655702e-06 1.97229209e-06]
ene_total = [1.71674703 1.35307722 1.75794987 1.57458483 1.35935733 1.38609978
 1.56889819 1.47978715 2.24850168 1.37065582]
ti_comp = [0.43830311 0.4576972  0.43612707 0.44578269 0.45748595 0.45576916
 0.44608386 0.45085659 0.41023879 0.45656667]
ti_coms = [0.09059974 0.07120565 0.09277578 0.08312016 0.07141689 0.07313368
 0.08281899 0.07804626 0.11866406 0.07233618]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.05718244e-05 9.01921040e-05 1.01771547e-05 4.06206033e-07
 1.38994033e-04 1.52109415e-05 8.03407087e-07 2.86463737e-05
 1.32539102e-05 8.00248765e-06]
ene_total = [0.48705029 0.38718743 0.49871352 0.44634221 0.39094219 0.39351399
 0.44474637 0.42061388 0.63788794 0.38884464]
optimize_network_iter = 1 obj = 4.495842450611684
eta = 0.6792834231754555
freqs = [36402828.40748152 73317349.05671008 36003753.27798384 12214744.53533785
 84699880.32505599 40564586.30164856 15329090.24416601 50275192.55652302
 40127838.49551825 32727809.18735842]
Done!
At round 32 eta: 0.6792834231754555
At round 32 local rounds: 12.663073636613152
At round 32 global rounds: 53.69580806749958
At round 32 a_n: 16.878589906266903
gradient difference: 0.3888794183731079
train() client id: f_00000-0-0 loss: 1.332007  [   32/  126]
train() client id: f_00000-0-1 loss: 1.240032  [   64/  126]
train() client id: f_00000-0-2 loss: 1.207017  [   96/  126]
train() client id: f_00000-1-0 loss: 1.136846  [   32/  126]
train() client id: f_00000-1-1 loss: 1.080848  [   64/  126]
train() client id: f_00000-1-2 loss: 1.020030  [   96/  126]
train() client id: f_00000-2-0 loss: 1.002527  [   32/  126]
train() client id: f_00000-2-1 loss: 1.020364  [   64/  126]
train() client id: f_00000-2-2 loss: 1.079888  [   96/  126]
train() client id: f_00000-3-0 loss: 0.993969  [   32/  126]
train() client id: f_00000-3-1 loss: 1.063560  [   64/  126]
train() client id: f_00000-3-2 loss: 0.844530  [   96/  126]
train() client id: f_00000-4-0 loss: 0.845850  [   32/  126]
train() client id: f_00000-4-1 loss: 0.924085  [   64/  126]
train() client id: f_00000-4-2 loss: 1.048210  [   96/  126]
train() client id: f_00000-5-0 loss: 0.829640  [   32/  126]
train() client id: f_00000-5-1 loss: 0.898247  [   64/  126]
train() client id: f_00000-5-2 loss: 0.942633  [   96/  126]
train() client id: f_00000-6-0 loss: 0.853000  [   32/  126]
train() client id: f_00000-6-1 loss: 0.990022  [   64/  126]
train() client id: f_00000-6-2 loss: 0.797147  [   96/  126]
train() client id: f_00000-7-0 loss: 0.897094  [   32/  126]
train() client id: f_00000-7-1 loss: 0.800939  [   64/  126]
train() client id: f_00000-7-2 loss: 0.853810  [   96/  126]
train() client id: f_00000-8-0 loss: 1.014711  [   32/  126]
train() client id: f_00000-8-1 loss: 0.822959  [   64/  126]
train() client id: f_00000-8-2 loss: 0.839691  [   96/  126]
train() client id: f_00000-9-0 loss: 0.955589  [   32/  126]
train() client id: f_00000-9-1 loss: 0.844691  [   64/  126]
train() client id: f_00000-9-2 loss: 0.752775  [   96/  126]
train() client id: f_00000-10-0 loss: 0.853373  [   32/  126]
train() client id: f_00000-10-1 loss: 0.902007  [   64/  126]
train() client id: f_00000-10-2 loss: 0.862054  [   96/  126]
train() client id: f_00000-11-0 loss: 0.773110  [   32/  126]
train() client id: f_00000-11-1 loss: 0.936463  [   64/  126]
train() client id: f_00000-11-2 loss: 0.908200  [   96/  126]
train() client id: f_00001-0-0 loss: 0.484794  [   32/  265]
train() client id: f_00001-0-1 loss: 0.511142  [   64/  265]
train() client id: f_00001-0-2 loss: 0.679943  [   96/  265]
train() client id: f_00001-0-3 loss: 0.484847  [  128/  265]
train() client id: f_00001-0-4 loss: 0.496267  [  160/  265]
train() client id: f_00001-0-5 loss: 0.531912  [  192/  265]
train() client id: f_00001-0-6 loss: 0.472009  [  224/  265]
train() client id: f_00001-0-7 loss: 0.411975  [  256/  265]
train() client id: f_00001-1-0 loss: 0.471678  [   32/  265]
train() client id: f_00001-1-1 loss: 0.526668  [   64/  265]
train() client id: f_00001-1-2 loss: 0.411919  [   96/  265]
train() client id: f_00001-1-3 loss: 0.422350  [  128/  265]
train() client id: f_00001-1-4 loss: 0.463295  [  160/  265]
train() client id: f_00001-1-5 loss: 0.582130  [  192/  265]
train() client id: f_00001-1-6 loss: 0.493896  [  224/  265]
train() client id: f_00001-1-7 loss: 0.480188  [  256/  265]
train() client id: f_00001-2-0 loss: 0.617371  [   32/  265]
train() client id: f_00001-2-1 loss: 0.493953  [   64/  265]
train() client id: f_00001-2-2 loss: 0.508105  [   96/  265]
train() client id: f_00001-2-3 loss: 0.412068  [  128/  265]
train() client id: f_00001-2-4 loss: 0.486650  [  160/  265]
train() client id: f_00001-2-5 loss: 0.477261  [  192/  265]
train() client id: f_00001-2-6 loss: 0.502782  [  224/  265]
train() client id: f_00001-2-7 loss: 0.400976  [  256/  265]
train() client id: f_00001-3-0 loss: 0.444657  [   32/  265]
train() client id: f_00001-3-1 loss: 0.517053  [   64/  265]
train() client id: f_00001-3-2 loss: 0.603792  [   96/  265]
train() client id: f_00001-3-3 loss: 0.521634  [  128/  265]
train() client id: f_00001-3-4 loss: 0.537142  [  160/  265]
train() client id: f_00001-3-5 loss: 0.417315  [  192/  265]
train() client id: f_00001-3-6 loss: 0.482764  [  224/  265]
train() client id: f_00001-3-7 loss: 0.388131  [  256/  265]
train() client id: f_00001-4-0 loss: 0.567310  [   32/  265]
train() client id: f_00001-4-1 loss: 0.574339  [   64/  265]
train() client id: f_00001-4-2 loss: 0.472194  [   96/  265]
train() client id: f_00001-4-3 loss: 0.449834  [  128/  265]
train() client id: f_00001-4-4 loss: 0.399006  [  160/  265]
train() client id: f_00001-4-5 loss: 0.461013  [  192/  265]
train() client id: f_00001-4-6 loss: 0.483306  [  224/  265]
train() client id: f_00001-4-7 loss: 0.524918  [  256/  265]
train() client id: f_00001-5-0 loss: 0.558775  [   32/  265]
train() client id: f_00001-5-1 loss: 0.441158  [   64/  265]
train() client id: f_00001-5-2 loss: 0.490237  [   96/  265]
train() client id: f_00001-5-3 loss: 0.540148  [  128/  265]
train() client id: f_00001-5-4 loss: 0.383460  [  160/  265]
train() client id: f_00001-5-5 loss: 0.465041  [  192/  265]
train() client id: f_00001-5-6 loss: 0.487872  [  224/  265]
train() client id: f_00001-5-7 loss: 0.548358  [  256/  265]
train() client id: f_00001-6-0 loss: 0.462113  [   32/  265]
train() client id: f_00001-6-1 loss: 0.385458  [   64/  265]
train() client id: f_00001-6-2 loss: 0.767067  [   96/  265]
train() client id: f_00001-6-3 loss: 0.409996  [  128/  265]
train() client id: f_00001-6-4 loss: 0.447298  [  160/  265]
train() client id: f_00001-6-5 loss: 0.381355  [  192/  265]
train() client id: f_00001-6-6 loss: 0.548213  [  224/  265]
train() client id: f_00001-6-7 loss: 0.511768  [  256/  265]
train() client id: f_00001-7-0 loss: 0.569658  [   32/  265]
train() client id: f_00001-7-1 loss: 0.374141  [   64/  265]
train() client id: f_00001-7-2 loss: 0.393832  [   96/  265]
train() client id: f_00001-7-3 loss: 0.462311  [  128/  265]
train() client id: f_00001-7-4 loss: 0.475216  [  160/  265]
train() client id: f_00001-7-5 loss: 0.487628  [  192/  265]
train() client id: f_00001-7-6 loss: 0.440994  [  224/  265]
train() client id: f_00001-7-7 loss: 0.703889  [  256/  265]
train() client id: f_00001-8-0 loss: 0.488561  [   32/  265]
train() client id: f_00001-8-1 loss: 0.485995  [   64/  265]
train() client id: f_00001-8-2 loss: 0.497712  [   96/  265]
train() client id: f_00001-8-3 loss: 0.625467  [  128/  265]
train() client id: f_00001-8-4 loss: 0.511322  [  160/  265]
train() client id: f_00001-8-5 loss: 0.403796  [  192/  265]
train() client id: f_00001-8-6 loss: 0.388566  [  224/  265]
train() client id: f_00001-8-7 loss: 0.417929  [  256/  265]
train() client id: f_00001-9-0 loss: 0.471581  [   32/  265]
train() client id: f_00001-9-1 loss: 0.502407  [   64/  265]
train() client id: f_00001-9-2 loss: 0.510562  [   96/  265]
train() client id: f_00001-9-3 loss: 0.494604  [  128/  265]
train() client id: f_00001-9-4 loss: 0.549314  [  160/  265]
train() client id: f_00001-9-5 loss: 0.482145  [  192/  265]
train() client id: f_00001-9-6 loss: 0.389497  [  224/  265]
train() client id: f_00001-9-7 loss: 0.390093  [  256/  265]
train() client id: f_00001-10-0 loss: 0.444882  [   32/  265]
train() client id: f_00001-10-1 loss: 0.507304  [   64/  265]
train() client id: f_00001-10-2 loss: 0.425605  [   96/  265]
train() client id: f_00001-10-3 loss: 0.466319  [  128/  265]
train() client id: f_00001-10-4 loss: 0.432089  [  160/  265]
train() client id: f_00001-10-5 loss: 0.461587  [  192/  265]
train() client id: f_00001-10-6 loss: 0.399942  [  224/  265]
train() client id: f_00001-10-7 loss: 0.777153  [  256/  265]
train() client id: f_00001-11-0 loss: 0.470226  [   32/  265]
train() client id: f_00001-11-1 loss: 0.522333  [   64/  265]
train() client id: f_00001-11-2 loss: 0.526970  [   96/  265]
train() client id: f_00001-11-3 loss: 0.652546  [  128/  265]
train() client id: f_00001-11-4 loss: 0.412167  [  160/  265]
train() client id: f_00001-11-5 loss: 0.420618  [  192/  265]
train() client id: f_00001-11-6 loss: 0.453607  [  224/  265]
train() client id: f_00001-11-7 loss: 0.457690  [  256/  265]
train() client id: f_00002-0-0 loss: 1.302972  [   32/  124]
train() client id: f_00002-0-1 loss: 1.248686  [   64/  124]
train() client id: f_00002-0-2 loss: 1.279610  [   96/  124]
train() client id: f_00002-1-0 loss: 1.234914  [   32/  124]
train() client id: f_00002-1-1 loss: 1.335489  [   64/  124]
train() client id: f_00002-1-2 loss: 1.183380  [   96/  124]
train() client id: f_00002-2-0 loss: 1.091938  [   32/  124]
train() client id: f_00002-2-1 loss: 1.170364  [   64/  124]
train() client id: f_00002-2-2 loss: 1.322615  [   96/  124]
train() client id: f_00002-3-0 loss: 1.097514  [   32/  124]
train() client id: f_00002-3-1 loss: 1.206592  [   64/  124]
train() client id: f_00002-3-2 loss: 1.211134  [   96/  124]
train() client id: f_00002-4-0 loss: 1.254926  [   32/  124]
train() client id: f_00002-4-1 loss: 1.003749  [   64/  124]
train() client id: f_00002-4-2 loss: 1.219661  [   96/  124]
train() client id: f_00002-5-0 loss: 1.201261  [   32/  124]
train() client id: f_00002-5-1 loss: 0.989990  [   64/  124]
train() client id: f_00002-5-2 loss: 0.983746  [   96/  124]
train() client id: f_00002-6-0 loss: 1.153924  [   32/  124]
train() client id: f_00002-6-1 loss: 1.126581  [   64/  124]
train() client id: f_00002-6-2 loss: 1.098010  [   96/  124]
train() client id: f_00002-7-0 loss: 1.263801  [   32/  124]
train() client id: f_00002-7-1 loss: 1.059823  [   64/  124]
train() client id: f_00002-7-2 loss: 0.922333  [   96/  124]
train() client id: f_00002-8-0 loss: 1.050607  [   32/  124]
train() client id: f_00002-8-1 loss: 1.006354  [   64/  124]
train() client id: f_00002-8-2 loss: 0.898126  [   96/  124]
train() client id: f_00002-9-0 loss: 1.109605  [   32/  124]
train() client id: f_00002-9-1 loss: 1.096640  [   64/  124]
train() client id: f_00002-9-2 loss: 0.897630  [   96/  124]
train() client id: f_00002-10-0 loss: 1.176661  [   32/  124]
train() client id: f_00002-10-1 loss: 0.986519  [   64/  124]
train() client id: f_00002-10-2 loss: 1.053963  [   96/  124]
train() client id: f_00002-11-0 loss: 0.897284  [   32/  124]
train() client id: f_00002-11-1 loss: 1.276041  [   64/  124]
train() client id: f_00002-11-2 loss: 0.952684  [   96/  124]
train() client id: f_00003-0-0 loss: 0.719882  [   32/   43]
train() client id: f_00003-1-0 loss: 0.718658  [   32/   43]
train() client id: f_00003-2-0 loss: 0.707763  [   32/   43]
train() client id: f_00003-3-0 loss: 0.729976  [   32/   43]
train() client id: f_00003-4-0 loss: 0.629162  [   32/   43]
train() client id: f_00003-5-0 loss: 0.592107  [   32/   43]
train() client id: f_00003-6-0 loss: 0.805884  [   32/   43]
train() client id: f_00003-7-0 loss: 0.656129  [   32/   43]
train() client id: f_00003-8-0 loss: 0.614951  [   32/   43]
train() client id: f_00003-9-0 loss: 0.699888  [   32/   43]
train() client id: f_00003-10-0 loss: 0.686092  [   32/   43]
train() client id: f_00003-11-0 loss: 0.634597  [   32/   43]
train() client id: f_00004-0-0 loss: 0.755541  [   32/  306]
train() client id: f_00004-0-1 loss: 0.774031  [   64/  306]
train() client id: f_00004-0-2 loss: 0.698174  [   96/  306]
train() client id: f_00004-0-3 loss: 0.699443  [  128/  306]
train() client id: f_00004-0-4 loss: 0.740938  [  160/  306]
train() client id: f_00004-0-5 loss: 0.824718  [  192/  306]
train() client id: f_00004-0-6 loss: 0.680500  [  224/  306]
train() client id: f_00004-0-7 loss: 0.868345  [  256/  306]
train() client id: f_00004-0-8 loss: 0.815401  [  288/  306]
train() client id: f_00004-1-0 loss: 0.731004  [   32/  306]
train() client id: f_00004-1-1 loss: 0.718793  [   64/  306]
train() client id: f_00004-1-2 loss: 0.764660  [   96/  306]
train() client id: f_00004-1-3 loss: 0.742939  [  128/  306]
train() client id: f_00004-1-4 loss: 0.892285  [  160/  306]
train() client id: f_00004-1-5 loss: 0.745417  [  192/  306]
train() client id: f_00004-1-6 loss: 0.766053  [  224/  306]
train() client id: f_00004-1-7 loss: 0.714371  [  256/  306]
train() client id: f_00004-1-8 loss: 0.842183  [  288/  306]
train() client id: f_00004-2-0 loss: 0.734172  [   32/  306]
train() client id: f_00004-2-1 loss: 0.849378  [   64/  306]
train() client id: f_00004-2-2 loss: 0.713855  [   96/  306]
train() client id: f_00004-2-3 loss: 0.803181  [  128/  306]
train() client id: f_00004-2-4 loss: 0.760143  [  160/  306]
train() client id: f_00004-2-5 loss: 0.635216  [  192/  306]
train() client id: f_00004-2-6 loss: 0.882794  [  224/  306]
train() client id: f_00004-2-7 loss: 0.716774  [  256/  306]
train() client id: f_00004-2-8 loss: 0.705934  [  288/  306]
train() client id: f_00004-3-0 loss: 0.678890  [   32/  306]
train() client id: f_00004-3-1 loss: 0.880649  [   64/  306]
train() client id: f_00004-3-2 loss: 0.610030  [   96/  306]
train() client id: f_00004-3-3 loss: 0.803083  [  128/  306]
train() client id: f_00004-3-4 loss: 0.779975  [  160/  306]
train() client id: f_00004-3-5 loss: 0.865478  [  192/  306]
train() client id: f_00004-3-6 loss: 0.763582  [  224/  306]
train() client id: f_00004-3-7 loss: 0.692678  [  256/  306]
train() client id: f_00004-3-8 loss: 0.836400  [  288/  306]
train() client id: f_00004-4-0 loss: 0.801841  [   32/  306]
train() client id: f_00004-4-1 loss: 0.651043  [   64/  306]
train() client id: f_00004-4-2 loss: 0.651109  [   96/  306]
train() client id: f_00004-4-3 loss: 0.661562  [  128/  306]
train() client id: f_00004-4-4 loss: 0.862437  [  160/  306]
train() client id: f_00004-4-5 loss: 0.749806  [  192/  306]
train() client id: f_00004-4-6 loss: 0.685368  [  224/  306]
train() client id: f_00004-4-7 loss: 0.971275  [  256/  306]
train() client id: f_00004-4-8 loss: 0.830004  [  288/  306]
train() client id: f_00004-5-0 loss: 0.828919  [   32/  306]
train() client id: f_00004-5-1 loss: 0.686761  [   64/  306]
train() client id: f_00004-5-2 loss: 0.776084  [   96/  306]
train() client id: f_00004-5-3 loss: 0.826673  [  128/  306]
train() client id: f_00004-5-4 loss: 0.847893  [  160/  306]
train() client id: f_00004-5-5 loss: 0.753369  [  192/  306]
train() client id: f_00004-5-6 loss: 0.716173  [  224/  306]
train() client id: f_00004-5-7 loss: 0.701151  [  256/  306]
train() client id: f_00004-5-8 loss: 0.731624  [  288/  306]
train() client id: f_00004-6-0 loss: 0.719226  [   32/  306]
train() client id: f_00004-6-1 loss: 0.810565  [   64/  306]
train() client id: f_00004-6-2 loss: 0.835980  [   96/  306]
train() client id: f_00004-6-3 loss: 0.799879  [  128/  306]
train() client id: f_00004-6-4 loss: 0.834688  [  160/  306]
train() client id: f_00004-6-5 loss: 0.771122  [  192/  306]
train() client id: f_00004-6-6 loss: 0.636891  [  224/  306]
train() client id: f_00004-6-7 loss: 0.774793  [  256/  306]
train() client id: f_00004-6-8 loss: 0.746758  [  288/  306]
train() client id: f_00004-7-0 loss: 0.713450  [   32/  306]
train() client id: f_00004-7-1 loss: 0.795523  [   64/  306]
train() client id: f_00004-7-2 loss: 0.665214  [   96/  306]
train() client id: f_00004-7-3 loss: 0.822411  [  128/  306]
train() client id: f_00004-7-4 loss: 0.815065  [  160/  306]
train() client id: f_00004-7-5 loss: 0.708272  [  192/  306]
train() client id: f_00004-7-6 loss: 0.788725  [  224/  306]
train() client id: f_00004-7-7 loss: 0.841043  [  256/  306]
train() client id: f_00004-7-8 loss: 0.652028  [  288/  306]
train() client id: f_00004-8-0 loss: 0.878018  [   32/  306]
train() client id: f_00004-8-1 loss: 0.812590  [   64/  306]
train() client id: f_00004-8-2 loss: 0.723888  [   96/  306]
train() client id: f_00004-8-3 loss: 0.739666  [  128/  306]
train() client id: f_00004-8-4 loss: 0.575319  [  160/  306]
train() client id: f_00004-8-5 loss: 0.793178  [  192/  306]
train() client id: f_00004-8-6 loss: 0.850930  [  224/  306]
train() client id: f_00004-8-7 loss: 0.726307  [  256/  306]
train() client id: f_00004-8-8 loss: 0.720655  [  288/  306]
train() client id: f_00004-9-0 loss: 0.824046  [   32/  306]
train() client id: f_00004-9-1 loss: 0.705156  [   64/  306]
train() client id: f_00004-9-2 loss: 0.733491  [   96/  306]
train() client id: f_00004-9-3 loss: 0.776139  [  128/  306]
train() client id: f_00004-9-4 loss: 0.731553  [  160/  306]
train() client id: f_00004-9-5 loss: 0.758677  [  192/  306]
train() client id: f_00004-9-6 loss: 0.833655  [  224/  306]
train() client id: f_00004-9-7 loss: 0.850296  [  256/  306]
train() client id: f_00004-9-8 loss: 0.702674  [  288/  306]
train() client id: f_00004-10-0 loss: 0.709669  [   32/  306]
train() client id: f_00004-10-1 loss: 0.795132  [   64/  306]
train() client id: f_00004-10-2 loss: 0.889756  [   96/  306]
train() client id: f_00004-10-3 loss: 0.727130  [  128/  306]
train() client id: f_00004-10-4 loss: 0.760499  [  160/  306]
train() client id: f_00004-10-5 loss: 0.721079  [  192/  306]
train() client id: f_00004-10-6 loss: 0.849638  [  224/  306]
train() client id: f_00004-10-7 loss: 0.701107  [  256/  306]
train() client id: f_00004-10-8 loss: 0.732769  [  288/  306]
train() client id: f_00004-11-0 loss: 0.740961  [   32/  306]
train() client id: f_00004-11-1 loss: 0.768073  [   64/  306]
train() client id: f_00004-11-2 loss: 0.993496  [   96/  306]
train() client id: f_00004-11-3 loss: 0.742775  [  128/  306]
train() client id: f_00004-11-4 loss: 0.641054  [  160/  306]
train() client id: f_00004-11-5 loss: 0.607617  [  192/  306]
train() client id: f_00004-11-6 loss: 0.826001  [  224/  306]
train() client id: f_00004-11-7 loss: 0.799009  [  256/  306]
train() client id: f_00004-11-8 loss: 0.789814  [  288/  306]
train() client id: f_00005-0-0 loss: 0.569352  [   32/  146]
train() client id: f_00005-0-1 loss: 0.492958  [   64/  146]
train() client id: f_00005-0-2 loss: 0.670621  [   96/  146]
train() client id: f_00005-0-3 loss: 0.643230  [  128/  146]
train() client id: f_00005-1-0 loss: 0.445133  [   32/  146]
train() client id: f_00005-1-1 loss: 0.677054  [   64/  146]
train() client id: f_00005-1-2 loss: 0.752624  [   96/  146]
train() client id: f_00005-1-3 loss: 0.561187  [  128/  146]
train() client id: f_00005-2-0 loss: 0.495637  [   32/  146]
train() client id: f_00005-2-1 loss: 0.742105  [   64/  146]
train() client id: f_00005-2-2 loss: 0.504558  [   96/  146]
train() client id: f_00005-2-3 loss: 0.533225  [  128/  146]
train() client id: f_00005-3-0 loss: 1.000584  [   32/  146]
train() client id: f_00005-3-1 loss: 0.362100  [   64/  146]
train() client id: f_00005-3-2 loss: 0.601592  [   96/  146]
train() client id: f_00005-3-3 loss: 0.453904  [  128/  146]
train() client id: f_00005-4-0 loss: 0.664757  [   32/  146]
train() client id: f_00005-4-1 loss: 0.674459  [   64/  146]
train() client id: f_00005-4-2 loss: 0.597797  [   96/  146]
train() client id: f_00005-4-3 loss: 0.456320  [  128/  146]
train() client id: f_00005-5-0 loss: 0.571056  [   32/  146]
train() client id: f_00005-5-1 loss: 0.625693  [   64/  146]
train() client id: f_00005-5-2 loss: 0.578022  [   96/  146]
train() client id: f_00005-5-3 loss: 0.683562  [  128/  146]
train() client id: f_00005-6-0 loss: 0.334996  [   32/  146]
train() client id: f_00005-6-1 loss: 0.455651  [   64/  146]
train() client id: f_00005-6-2 loss: 0.648018  [   96/  146]
train() client id: f_00005-6-3 loss: 0.794317  [  128/  146]
train() client id: f_00005-7-0 loss: 0.703750  [   32/  146]
train() client id: f_00005-7-1 loss: 0.546732  [   64/  146]
train() client id: f_00005-7-2 loss: 0.450713  [   96/  146]
train() client id: f_00005-7-3 loss: 0.513102  [  128/  146]
train() client id: f_00005-8-0 loss: 0.655127  [   32/  146]
train() client id: f_00005-8-1 loss: 0.599964  [   64/  146]
train() client id: f_00005-8-2 loss: 0.588727  [   96/  146]
train() client id: f_00005-8-3 loss: 0.572591  [  128/  146]
train() client id: f_00005-9-0 loss: 0.651841  [   32/  146]
train() client id: f_00005-9-1 loss: 0.492314  [   64/  146]
train() client id: f_00005-9-2 loss: 0.533846  [   96/  146]
train() client id: f_00005-9-3 loss: 0.555297  [  128/  146]
train() client id: f_00005-10-0 loss: 0.256564  [   32/  146]
train() client id: f_00005-10-1 loss: 0.868604  [   64/  146]
train() client id: f_00005-10-2 loss: 0.485832  [   96/  146]
train() client id: f_00005-10-3 loss: 0.616111  [  128/  146]
train() client id: f_00005-11-0 loss: 0.443644  [   32/  146]
train() client id: f_00005-11-1 loss: 0.714475  [   64/  146]
train() client id: f_00005-11-2 loss: 0.522877  [   96/  146]
train() client id: f_00005-11-3 loss: 0.429776  [  128/  146]
train() client id: f_00006-0-0 loss: 0.570096  [   32/   54]
train() client id: f_00006-1-0 loss: 0.545811  [   32/   54]
train() client id: f_00006-2-0 loss: 0.543634  [   32/   54]
train() client id: f_00006-3-0 loss: 0.525058  [   32/   54]
train() client id: f_00006-4-0 loss: 0.521361  [   32/   54]
train() client id: f_00006-5-0 loss: 0.535248  [   32/   54]
train() client id: f_00006-6-0 loss: 0.566828  [   32/   54]
train() client id: f_00006-7-0 loss: 0.519150  [   32/   54]
train() client id: f_00006-8-0 loss: 0.565412  [   32/   54]
train() client id: f_00006-9-0 loss: 0.498484  [   32/   54]
train() client id: f_00006-10-0 loss: 0.514902  [   32/   54]
train() client id: f_00006-11-0 loss: 0.518759  [   32/   54]
train() client id: f_00007-0-0 loss: 0.571022  [   32/  179]
train() client id: f_00007-0-1 loss: 0.538215  [   64/  179]
train() client id: f_00007-0-2 loss: 0.699422  [   96/  179]
train() client id: f_00007-0-3 loss: 0.434379  [  128/  179]
train() client id: f_00007-0-4 loss: 0.435122  [  160/  179]
train() client id: f_00007-1-0 loss: 0.402814  [   32/  179]
train() client id: f_00007-1-1 loss: 0.610827  [   64/  179]
train() client id: f_00007-1-2 loss: 0.574307  [   96/  179]
train() client id: f_00007-1-3 loss: 0.551683  [  128/  179]
train() client id: f_00007-1-4 loss: 0.548697  [  160/  179]
train() client id: f_00007-2-0 loss: 0.611769  [   32/  179]
train() client id: f_00007-2-1 loss: 0.628834  [   64/  179]
train() client id: f_00007-2-2 loss: 0.569223  [   96/  179]
train() client id: f_00007-2-3 loss: 0.456932  [  128/  179]
train() client id: f_00007-2-4 loss: 0.404464  [  160/  179]
train() client id: f_00007-3-0 loss: 0.482739  [   32/  179]
train() client id: f_00007-3-1 loss: 0.527444  [   64/  179]
train() client id: f_00007-3-2 loss: 0.401867  [   96/  179]
train() client id: f_00007-3-3 loss: 0.645294  [  128/  179]
train() client id: f_00007-3-4 loss: 0.481147  [  160/  179]
train() client id: f_00007-4-0 loss: 0.575105  [   32/  179]
train() client id: f_00007-4-1 loss: 0.527960  [   64/  179]
train() client id: f_00007-4-2 loss: 0.575164  [   96/  179]
train() client id: f_00007-4-3 loss: 0.362580  [  128/  179]
train() client id: f_00007-4-4 loss: 0.549529  [  160/  179]
train() client id: f_00007-5-0 loss: 0.528333  [   32/  179]
train() client id: f_00007-5-1 loss: 0.469462  [   64/  179]
train() client id: f_00007-5-2 loss: 0.581764  [   96/  179]
train() client id: f_00007-5-3 loss: 0.390388  [  128/  179]
train() client id: f_00007-5-4 loss: 0.533222  [  160/  179]
train() client id: f_00007-6-0 loss: 0.463482  [   32/  179]
train() client id: f_00007-6-1 loss: 0.457596  [   64/  179]
train() client id: f_00007-6-2 loss: 0.532721  [   96/  179]
train() client id: f_00007-6-3 loss: 0.442943  [  128/  179]
train() client id: f_00007-6-4 loss: 0.651161  [  160/  179]
train() client id: f_00007-7-0 loss: 0.481200  [   32/  179]
train() client id: f_00007-7-1 loss: 0.567921  [   64/  179]
train() client id: f_00007-7-2 loss: 0.602104  [   96/  179]
train() client id: f_00007-7-3 loss: 0.451416  [  128/  179]
train() client id: f_00007-7-4 loss: 0.402222  [  160/  179]
train() client id: f_00007-8-0 loss: 0.538049  [   32/  179]
train() client id: f_00007-8-1 loss: 0.400789  [   64/  179]
train() client id: f_00007-8-2 loss: 0.481061  [   96/  179]
train() client id: f_00007-8-3 loss: 0.587789  [  128/  179]
train() client id: f_00007-8-4 loss: 0.395772  [  160/  179]
train() client id: f_00007-9-0 loss: 0.598255  [   32/  179]
train() client id: f_00007-9-1 loss: 0.578055  [   64/  179]
train() client id: f_00007-9-2 loss: 0.443283  [   96/  179]
train() client id: f_00007-9-3 loss: 0.450287  [  128/  179]
train() client id: f_00007-9-4 loss: 0.430464  [  160/  179]
train() client id: f_00007-10-0 loss: 0.418635  [   32/  179]
train() client id: f_00007-10-1 loss: 0.535403  [   64/  179]
train() client id: f_00007-10-2 loss: 0.462981  [   96/  179]
train() client id: f_00007-10-3 loss: 0.424357  [  128/  179]
train() client id: f_00007-10-4 loss: 0.558242  [  160/  179]
train() client id: f_00007-11-0 loss: 0.594454  [   32/  179]
train() client id: f_00007-11-1 loss: 0.628992  [   64/  179]
train() client id: f_00007-11-2 loss: 0.360910  [   96/  179]
train() client id: f_00007-11-3 loss: 0.609679  [  128/  179]
train() client id: f_00007-11-4 loss: 0.374941  [  160/  179]
train() client id: f_00008-0-0 loss: 0.756874  [   32/  130]
train() client id: f_00008-0-1 loss: 0.694988  [   64/  130]
train() client id: f_00008-0-2 loss: 0.740895  [   96/  130]
train() client id: f_00008-0-3 loss: 0.639417  [  128/  130]
train() client id: f_00008-1-0 loss: 0.809338  [   32/  130]
train() client id: f_00008-1-1 loss: 0.768378  [   64/  130]
train() client id: f_00008-1-2 loss: 0.630512  [   96/  130]
train() client id: f_00008-1-3 loss: 0.676198  [  128/  130]
train() client id: f_00008-2-0 loss: 0.696980  [   32/  130]
train() client id: f_00008-2-1 loss: 0.705147  [   64/  130]
train() client id: f_00008-2-2 loss: 0.746594  [   96/  130]
train() client id: f_00008-2-3 loss: 0.727795  [  128/  130]
train() client id: f_00008-3-0 loss: 0.744606  [   32/  130]
train() client id: f_00008-3-1 loss: 0.700223  [   64/  130]
train() client id: f_00008-3-2 loss: 0.727914  [   96/  130]
train() client id: f_00008-3-3 loss: 0.700422  [  128/  130]
train() client id: f_00008-4-0 loss: 0.650267  [   32/  130]
train() client id: f_00008-4-1 loss: 0.754166  [   64/  130]
train() client id: f_00008-4-2 loss: 0.744697  [   96/  130]
train() client id: f_00008-4-3 loss: 0.694531  [  128/  130]
train() client id: f_00008-5-0 loss: 0.760980  [   32/  130]
train() client id: f_00008-5-1 loss: 0.657419  [   64/  130]
train() client id: f_00008-5-2 loss: 0.656375  [   96/  130]
train() client id: f_00008-5-3 loss: 0.787579  [  128/  130]
train() client id: f_00008-6-0 loss: 0.755039  [   32/  130]
train() client id: f_00008-6-1 loss: 0.773538  [   64/  130]
train() client id: f_00008-6-2 loss: 0.605553  [   96/  130]
train() client id: f_00008-6-3 loss: 0.711273  [  128/  130]
train() client id: f_00008-7-0 loss: 0.822868  [   32/  130]
train() client id: f_00008-7-1 loss: 0.692454  [   64/  130]
train() client id: f_00008-7-2 loss: 0.666759  [   96/  130]
train() client id: f_00008-7-3 loss: 0.683798  [  128/  130]
train() client id: f_00008-8-0 loss: 0.743150  [   32/  130]
train() client id: f_00008-8-1 loss: 0.669307  [   64/  130]
train() client id: f_00008-8-2 loss: 0.789936  [   96/  130]
train() client id: f_00008-8-3 loss: 0.671488  [  128/  130]
train() client id: f_00008-9-0 loss: 0.687225  [   32/  130]
train() client id: f_00008-9-1 loss: 0.791845  [   64/  130]
train() client id: f_00008-9-2 loss: 0.738778  [   96/  130]
train() client id: f_00008-9-3 loss: 0.652279  [  128/  130]
train() client id: f_00008-10-0 loss: 0.713611  [   32/  130]
train() client id: f_00008-10-1 loss: 0.771955  [   64/  130]
train() client id: f_00008-10-2 loss: 0.594894  [   96/  130]
train() client id: f_00008-10-3 loss: 0.760005  [  128/  130]
train() client id: f_00008-11-0 loss: 0.762683  [   32/  130]
train() client id: f_00008-11-1 loss: 0.677832  [   64/  130]
train() client id: f_00008-11-2 loss: 0.638245  [   96/  130]
train() client id: f_00008-11-3 loss: 0.725378  [  128/  130]
train() client id: f_00009-0-0 loss: 1.278995  [   32/  118]
train() client id: f_00009-0-1 loss: 1.172121  [   64/  118]
train() client id: f_00009-0-2 loss: 1.242534  [   96/  118]
train() client id: f_00009-1-0 loss: 1.245450  [   32/  118]
train() client id: f_00009-1-1 loss: 1.317317  [   64/  118]
train() client id: f_00009-1-2 loss: 1.118965  [   96/  118]
train() client id: f_00009-2-0 loss: 1.082269  [   32/  118]
train() client id: f_00009-2-1 loss: 1.087498  [   64/  118]
train() client id: f_00009-2-2 loss: 1.166852  [   96/  118]
train() client id: f_00009-3-0 loss: 1.151892  [   32/  118]
train() client id: f_00009-3-1 loss: 1.018084  [   64/  118]
train() client id: f_00009-3-2 loss: 1.113077  [   96/  118]
train() client id: f_00009-4-0 loss: 1.190019  [   32/  118]
train() client id: f_00009-4-1 loss: 1.040671  [   64/  118]
train() client id: f_00009-4-2 loss: 0.945962  [   96/  118]
train() client id: f_00009-5-0 loss: 1.046337  [   32/  118]
train() client id: f_00009-5-1 loss: 1.022528  [   64/  118]
train() client id: f_00009-5-2 loss: 1.081096  [   96/  118]
train() client id: f_00009-6-0 loss: 0.855857  [   32/  118]
train() client id: f_00009-6-1 loss: 1.091622  [   64/  118]
train() client id: f_00009-6-2 loss: 0.956464  [   96/  118]
train() client id: f_00009-7-0 loss: 1.054804  [   32/  118]
train() client id: f_00009-7-1 loss: 0.948819  [   64/  118]
train() client id: f_00009-7-2 loss: 0.898741  [   96/  118]
train() client id: f_00009-8-0 loss: 1.086269  [   32/  118]
train() client id: f_00009-8-1 loss: 0.867563  [   64/  118]
train() client id: f_00009-8-2 loss: 0.933280  [   96/  118]
train() client id: f_00009-9-0 loss: 0.981922  [   32/  118]
train() client id: f_00009-9-1 loss: 0.927166  [   64/  118]
train() client id: f_00009-9-2 loss: 1.058089  [   96/  118]
train() client id: f_00009-10-0 loss: 1.046169  [   32/  118]
train() client id: f_00009-10-1 loss: 0.880233  [   64/  118]
train() client id: f_00009-10-2 loss: 0.902136  [   96/  118]
train() client id: f_00009-11-0 loss: 0.821221  [   32/  118]
train() client id: f_00009-11-1 loss: 0.863446  [   64/  118]
train() client id: f_00009-11-2 loss: 0.992195  [   96/  118]
At round 32 accuracy: 0.6525198938992043
At round 32 training accuracy: 0.5774647887323944
At round 32 training loss: 0.8515678294267853
update_location
xs = [  -3.9056584     4.20031788  180.00902392   18.81129433    0.97929623
    3.95640986 -142.44319194 -121.32485185  164.66397685 -107.06087855]
ys = [ 172.5879595   155.55583871    1.32061395 -142.45517586  134.35018685
  117.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [199.50402986 184.9736782  205.92472584 175.06382243 167.4841238
 154.58275225 174.06020071 157.22721117 193.44998125 146.55389307]
dists_bs = [171.11134289 180.25212713 395.21631571 371.94167082 180.61317784
 187.87128037 180.58399931 182.26046195 374.37432475 184.00059091]
uav_gains = [1.71760459e-11 2.11772319e-11 1.56382078e-11 2.44758737e-11
 2.74270048e-11 3.36063783e-11 2.48429772e-11 3.21986963e-11
 1.87438290e-11 3.84301675e-11]
bs_gains = [6.16756090e-11 5.33124903e-11 5.91774794e-12 7.01398027e-12
 5.30146221e-11 4.74771870e-11 5.30386105e-11 5.16838882e-11
 6.88711171e-12 5.03269111e-11]
Round 33
-------------------------------
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.63572969 13.74717363  6.53678508  2.35476176 15.85548586  7.6312378
  2.9198615   9.33889895  6.88901514  6.19002745]
obj_prev = 78.09897687730752
eta_min = 1.1396317579661002e-14	eta_max = 0.9276141009437577
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 18.129363196870486	eta = 0.9090909090909091
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 32.88895800241827	eta = 0.5011177085230375
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 25.66977986634642	eta = 0.642048329034932
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.365791295624447	eta = 0.6764089485097873
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.297762088268055	eta = 0.678302767555703
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.29756203224391	eta = 0.678308352418689
eta = 0.678308352418689
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [0.03202948 0.06736359 0.03152108 0.0109307  0.07778588 0.03711352
 0.01372692 0.0455022  0.03304629 0.02999586]
ene_total = [2.16353862 3.9073382  2.14884473 1.01590035 4.454601   2.32913348
 1.16104971 2.80262517 2.36340733 1.95112344]
ti_comp = [0.44826702 0.46925465 0.4459835  0.45600932 0.4691739  0.46754648
 0.45630733 0.4611897  0.42034182 0.46841538]
ti_coms = [0.09205736 0.07106973 0.09434088 0.08431506 0.07115048 0.0727779
 0.08401706 0.07913468 0.11998256 0.07190901]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [1.02201224e-05 8.67637833e-05 9.84114204e-06 3.92532807e-07
 1.33633277e-04 1.46159488e-05 7.76398040e-07 2.76833041e-05
 1.27656487e-05 7.68779460e-06]
ene_total = [0.48354488 0.3774426  0.49550619 0.44240675 0.38032544 0.3826196
 0.44086332 0.41665811 0.63019699 0.37769716]
optimize_network_iter = 0 obj = 4.427261042788227
eta = 0.678308352418689
freqs = [35725893.75226643 71777219.31597894 35338835.42638909 11985166.67285503
 82896638.99614602 39689662.43089405 15041309.96971984 49331324.0982344
 39308827.86172418 32018442.92741502]
eta_min = 0.6783083524187157	eta_max = 0.6783083524186895
af = 0.013235761297397841	bf = 1.4190753947365375	zeta = 0.014559337427137626	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [2.50952835e-06 2.13046542e-05 2.41647056e-06 9.63855586e-08
 3.28133544e-05 3.58891376e-06 1.90642814e-07 6.79757383e-06
 3.13457667e-06 1.88772088e-06]
ene_total = [1.70964427 1.323468   1.75202375 1.56544914 1.327104   1.35189348
 1.55993376 1.47051205 2.22823206 1.33544537]
ti_comp = [0.44826702 0.46925465 0.4459835  0.45600932 0.4691739  0.46754648
 0.45630733 0.4611897  0.42034182 0.46841538]
ti_coms = [0.09205736 0.07106973 0.09434088 0.08431506 0.07115048 0.0727779
 0.08401706 0.07913468 0.11998256 0.07190901]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [1.02201224e-05 8.67637833e-05 9.84114204e-06 3.92532807e-07
 1.33633277e-04 1.46159488e-05 7.76398040e-07 2.76833041e-05
 1.27656487e-05 7.68779460e-06]
ene_total = [0.48354488 0.3774426  0.49550619 0.44240675 0.38032544 0.3826196
 0.44086332 0.41665811 0.63019699 0.37769716]
optimize_network_iter = 1 obj = 4.427261042788233
eta = 0.6783083524186895
freqs = [35725893.75226644 71777219.31597894 35338835.4263891  11985166.67285503
 82896638.99614602 39689662.43089406 15041309.96971984 49331324.09823441
 39308827.86172419 32018442.92741502]
Done!
At round 33 eta: 0.6783083524186895
At round 33 local rounds: 12.71011101057834
At round 33 global rounds: 52.468225498458686
At round 33 a_n: 16.536044059297588
gradient difference: 0.44557881355285645
train() client id: f_00000-0-0 loss: 1.082879  [   32/  126]
train() client id: f_00000-0-1 loss: 1.141069  [   64/  126]
train() client id: f_00000-0-2 loss: 1.261122  [   96/  126]
train() client id: f_00000-1-0 loss: 1.077010  [   32/  126]
train() client id: f_00000-1-1 loss: 1.051150  [   64/  126]
train() client id: f_00000-1-2 loss: 1.118910  [   96/  126]
train() client id: f_00000-2-0 loss: 0.972057  [   32/  126]
train() client id: f_00000-2-1 loss: 1.059438  [   64/  126]
train() client id: f_00000-2-2 loss: 1.105288  [   96/  126]
train() client id: f_00000-3-0 loss: 1.066777  [   32/  126]
train() client id: f_00000-3-1 loss: 0.890992  [   64/  126]
train() client id: f_00000-3-2 loss: 0.885652  [   96/  126]
train() client id: f_00000-4-0 loss: 0.862275  [   32/  126]
train() client id: f_00000-4-1 loss: 0.832587  [   64/  126]
train() client id: f_00000-4-2 loss: 0.842179  [   96/  126]
train() client id: f_00000-5-0 loss: 0.875576  [   32/  126]
train() client id: f_00000-5-1 loss: 0.817093  [   64/  126]
train() client id: f_00000-5-2 loss: 0.909554  [   96/  126]
train() client id: f_00000-6-0 loss: 0.904116  [   32/  126]
train() client id: f_00000-6-1 loss: 0.767644  [   64/  126]
train() client id: f_00000-6-2 loss: 0.668666  [   96/  126]
train() client id: f_00000-7-0 loss: 0.782508  [   32/  126]
train() client id: f_00000-7-1 loss: 0.789491  [   64/  126]
train() client id: f_00000-7-2 loss: 0.837227  [   96/  126]
train() client id: f_00000-8-0 loss: 0.929610  [   32/  126]
train() client id: f_00000-8-1 loss: 0.726603  [   64/  126]
train() client id: f_00000-8-2 loss: 0.783577  [   96/  126]
train() client id: f_00000-9-0 loss: 0.861237  [   32/  126]
train() client id: f_00000-9-1 loss: 0.778325  [   64/  126]
train() client id: f_00000-9-2 loss: 0.646715  [   96/  126]
train() client id: f_00000-10-0 loss: 0.731698  [   32/  126]
train() client id: f_00000-10-1 loss: 0.757341  [   64/  126]
train() client id: f_00000-10-2 loss: 0.801087  [   96/  126]
train() client id: f_00000-11-0 loss: 0.745100  [   32/  126]
train() client id: f_00000-11-1 loss: 0.805122  [   64/  126]
train() client id: f_00000-11-2 loss: 0.821899  [   96/  126]
train() client id: f_00001-0-0 loss: 0.503616  [   32/  265]
train() client id: f_00001-0-1 loss: 0.549993  [   64/  265]
train() client id: f_00001-0-2 loss: 0.378741  [   96/  265]
train() client id: f_00001-0-3 loss: 0.482560  [  128/  265]
train() client id: f_00001-0-4 loss: 0.461960  [  160/  265]
train() client id: f_00001-0-5 loss: 0.370890  [  192/  265]
train() client id: f_00001-0-6 loss: 0.470233  [  224/  265]
train() client id: f_00001-0-7 loss: 0.477125  [  256/  265]
train() client id: f_00001-1-0 loss: 0.474636  [   32/  265]
train() client id: f_00001-1-1 loss: 0.503581  [   64/  265]
train() client id: f_00001-1-2 loss: 0.562896  [   96/  265]
train() client id: f_00001-1-3 loss: 0.518815  [  128/  265]
train() client id: f_00001-1-4 loss: 0.364241  [  160/  265]
train() client id: f_00001-1-5 loss: 0.389541  [  192/  265]
train() client id: f_00001-1-6 loss: 0.356907  [  224/  265]
train() client id: f_00001-1-7 loss: 0.508907  [  256/  265]
train() client id: f_00001-2-0 loss: 0.428910  [   32/  265]
train() client id: f_00001-2-1 loss: 0.464150  [   64/  265]
train() client id: f_00001-2-2 loss: 0.397618  [   96/  265]
train() client id: f_00001-2-3 loss: 0.430962  [  128/  265]
train() client id: f_00001-2-4 loss: 0.414855  [  160/  265]
train() client id: f_00001-2-5 loss: 0.462186  [  192/  265]
train() client id: f_00001-2-6 loss: 0.681224  [  224/  265]
train() client id: f_00001-2-7 loss: 0.352690  [  256/  265]
train() client id: f_00001-3-0 loss: 0.532320  [   32/  265]
train() client id: f_00001-3-1 loss: 0.429633  [   64/  265]
train() client id: f_00001-3-2 loss: 0.491949  [   96/  265]
train() client id: f_00001-3-3 loss: 0.404530  [  128/  265]
train() client id: f_00001-3-4 loss: 0.462492  [  160/  265]
train() client id: f_00001-3-5 loss: 0.426724  [  192/  265]
train() client id: f_00001-3-6 loss: 0.427681  [  224/  265]
train() client id: f_00001-3-7 loss: 0.415697  [  256/  265]
train() client id: f_00001-4-0 loss: 0.527930  [   32/  265]
train() client id: f_00001-4-1 loss: 0.361915  [   64/  265]
train() client id: f_00001-4-2 loss: 0.562431  [   96/  265]
train() client id: f_00001-4-3 loss: 0.368437  [  128/  265]
train() client id: f_00001-4-4 loss: 0.404733  [  160/  265]
train() client id: f_00001-4-5 loss: 0.489866  [  192/  265]
train() client id: f_00001-4-6 loss: 0.348782  [  224/  265]
train() client id: f_00001-4-7 loss: 0.411474  [  256/  265]
train() client id: f_00001-5-0 loss: 0.462246  [   32/  265]
train() client id: f_00001-5-1 loss: 0.418900  [   64/  265]
train() client id: f_00001-5-2 loss: 0.381464  [   96/  265]
train() client id: f_00001-5-3 loss: 0.515658  [  128/  265]
train() client id: f_00001-5-4 loss: 0.446911  [  160/  265]
train() client id: f_00001-5-5 loss: 0.439334  [  192/  265]
train() client id: f_00001-5-6 loss: 0.407962  [  224/  265]
train() client id: f_00001-5-7 loss: 0.468426  [  256/  265]
train() client id: f_00001-6-0 loss: 0.509268  [   32/  265]
train() client id: f_00001-6-1 loss: 0.445349  [   64/  265]
train() client id: f_00001-6-2 loss: 0.427141  [   96/  265]
train() client id: f_00001-6-3 loss: 0.350522  [  128/  265]
train() client id: f_00001-6-4 loss: 0.382151  [  160/  265]
train() client id: f_00001-6-5 loss: 0.492213  [  192/  265]
train() client id: f_00001-6-6 loss: 0.486115  [  224/  265]
train() client id: f_00001-6-7 loss: 0.339255  [  256/  265]
train() client id: f_00001-7-0 loss: 0.480630  [   32/  265]
train() client id: f_00001-7-1 loss: 0.482719  [   64/  265]
train() client id: f_00001-7-2 loss: 0.453141  [   96/  265]
train() client id: f_00001-7-3 loss: 0.596612  [  128/  265]
train() client id: f_00001-7-4 loss: 0.373237  [  160/  265]
train() client id: f_00001-7-5 loss: 0.378842  [  192/  265]
train() client id: f_00001-7-6 loss: 0.408410  [  224/  265]
train() client id: f_00001-7-7 loss: 0.340599  [  256/  265]
train() client id: f_00001-8-0 loss: 0.486470  [   32/  265]
train() client id: f_00001-8-1 loss: 0.365193  [   64/  265]
train() client id: f_00001-8-2 loss: 0.519483  [   96/  265]
train() client id: f_00001-8-3 loss: 0.482279  [  128/  265]
train() client id: f_00001-8-4 loss: 0.382864  [  160/  265]
train() client id: f_00001-8-5 loss: 0.505740  [  192/  265]
train() client id: f_00001-8-6 loss: 0.358622  [  224/  265]
train() client id: f_00001-8-7 loss: 0.415909  [  256/  265]
train() client id: f_00001-9-0 loss: 0.449080  [   32/  265]
train() client id: f_00001-9-1 loss: 0.523630  [   64/  265]
train() client id: f_00001-9-2 loss: 0.505674  [   96/  265]
train() client id: f_00001-9-3 loss: 0.346283  [  128/  265]
train() client id: f_00001-9-4 loss: 0.548262  [  160/  265]
train() client id: f_00001-9-5 loss: 0.331519  [  192/  265]
train() client id: f_00001-9-6 loss: 0.390779  [  224/  265]
train() client id: f_00001-9-7 loss: 0.408610  [  256/  265]
train() client id: f_00001-10-0 loss: 0.478745  [   32/  265]
train() client id: f_00001-10-1 loss: 0.310818  [   64/  265]
train() client id: f_00001-10-2 loss: 0.381164  [   96/  265]
train() client id: f_00001-10-3 loss: 0.403640  [  128/  265]
train() client id: f_00001-10-4 loss: 0.512276  [  160/  265]
train() client id: f_00001-10-5 loss: 0.566256  [  192/  265]
train() client id: f_00001-10-6 loss: 0.468780  [  224/  265]
train() client id: f_00001-10-7 loss: 0.377312  [  256/  265]
train() client id: f_00001-11-0 loss: 0.584110  [   32/  265]
train() client id: f_00001-11-1 loss: 0.451082  [   64/  265]
train() client id: f_00001-11-2 loss: 0.408946  [   96/  265]
train() client id: f_00001-11-3 loss: 0.592060  [  128/  265]
train() client id: f_00001-11-4 loss: 0.393214  [  160/  265]
train() client id: f_00001-11-5 loss: 0.352830  [  192/  265]
train() client id: f_00001-11-6 loss: 0.385445  [  224/  265]
train() client id: f_00001-11-7 loss: 0.330930  [  256/  265]
train() client id: f_00002-0-0 loss: 1.071039  [   32/  124]
train() client id: f_00002-0-1 loss: 1.309317  [   64/  124]
train() client id: f_00002-0-2 loss: 1.182492  [   96/  124]
train() client id: f_00002-1-0 loss: 1.379234  [   32/  124]
train() client id: f_00002-1-1 loss: 1.062969  [   64/  124]
train() client id: f_00002-1-2 loss: 1.036306  [   96/  124]
train() client id: f_00002-2-0 loss: 1.290738  [   32/  124]
train() client id: f_00002-2-1 loss: 1.121730  [   64/  124]
train() client id: f_00002-2-2 loss: 1.099748  [   96/  124]
train() client id: f_00002-3-0 loss: 0.971583  [   32/  124]
train() client id: f_00002-3-1 loss: 1.110334  [   64/  124]
train() client id: f_00002-3-2 loss: 1.159902  [   96/  124]
train() client id: f_00002-4-0 loss: 1.001469  [   32/  124]
train() client id: f_00002-4-1 loss: 1.200897  [   64/  124]
train() client id: f_00002-4-2 loss: 1.206557  [   96/  124]
train() client id: f_00002-5-0 loss: 1.038616  [   32/  124]
train() client id: f_00002-5-1 loss: 1.326865  [   64/  124]
train() client id: f_00002-5-2 loss: 0.996696  [   96/  124]
train() client id: f_00002-6-0 loss: 0.982250  [   32/  124]
train() client id: f_00002-6-1 loss: 1.316515  [   64/  124]
train() client id: f_00002-6-2 loss: 1.175040  [   96/  124]
train() client id: f_00002-7-0 loss: 1.073619  [   32/  124]
train() client id: f_00002-7-1 loss: 1.087319  [   64/  124]
train() client id: f_00002-7-2 loss: 1.017921  [   96/  124]
train() client id: f_00002-8-0 loss: 0.919063  [   32/  124]
train() client id: f_00002-8-1 loss: 1.151849  [   64/  124]
train() client id: f_00002-8-2 loss: 1.285007  [   96/  124]
train() client id: f_00002-9-0 loss: 1.023757  [   32/  124]
train() client id: f_00002-9-1 loss: 0.941256  [   64/  124]
train() client id: f_00002-9-2 loss: 1.347679  [   96/  124]
train() client id: f_00002-10-0 loss: 0.908425  [   32/  124]
train() client id: f_00002-10-1 loss: 0.953889  [   64/  124]
train() client id: f_00002-10-2 loss: 1.248955  [   96/  124]
train() client id: f_00002-11-0 loss: 1.223926  [   32/  124]
train() client id: f_00002-11-1 loss: 1.151923  [   64/  124]
train() client id: f_00002-11-2 loss: 1.052385  [   96/  124]
train() client id: f_00003-0-0 loss: 0.785639  [   32/   43]
train() client id: f_00003-1-0 loss: 0.789132  [   32/   43]
train() client id: f_00003-2-0 loss: 0.896189  [   32/   43]
train() client id: f_00003-3-0 loss: 0.650869  [   32/   43]
train() client id: f_00003-4-0 loss: 0.789818  [   32/   43]
train() client id: f_00003-5-0 loss: 0.940961  [   32/   43]
train() client id: f_00003-6-0 loss: 0.756078  [   32/   43]
train() client id: f_00003-7-0 loss: 0.796205  [   32/   43]
train() client id: f_00003-8-0 loss: 0.713783  [   32/   43]
train() client id: f_00003-9-0 loss: 0.715647  [   32/   43]
train() client id: f_00003-10-0 loss: 0.875875  [   32/   43]
train() client id: f_00003-11-0 loss: 0.796916  [   32/   43]
train() client id: f_00004-0-0 loss: 0.858434  [   32/  306]
train() client id: f_00004-0-1 loss: 0.929402  [   64/  306]
train() client id: f_00004-0-2 loss: 0.760753  [   96/  306]
train() client id: f_00004-0-3 loss: 0.699509  [  128/  306]
train() client id: f_00004-0-4 loss: 0.834788  [  160/  306]
train() client id: f_00004-0-5 loss: 0.872130  [  192/  306]
train() client id: f_00004-0-6 loss: 0.773938  [  224/  306]
train() client id: f_00004-0-7 loss: 0.777894  [  256/  306]
train() client id: f_00004-0-8 loss: 0.849935  [  288/  306]
train() client id: f_00004-1-0 loss: 0.780889  [   32/  306]
train() client id: f_00004-1-1 loss: 0.775634  [   64/  306]
train() client id: f_00004-1-2 loss: 0.840417  [   96/  306]
train() client id: f_00004-1-3 loss: 0.850508  [  128/  306]
train() client id: f_00004-1-4 loss: 0.857486  [  160/  306]
train() client id: f_00004-1-5 loss: 0.783957  [  192/  306]
train() client id: f_00004-1-6 loss: 0.716131  [  224/  306]
train() client id: f_00004-1-7 loss: 0.884473  [  256/  306]
train() client id: f_00004-1-8 loss: 0.756650  [  288/  306]
train() client id: f_00004-2-0 loss: 0.944084  [   32/  306]
train() client id: f_00004-2-1 loss: 0.827788  [   64/  306]
train() client id: f_00004-2-2 loss: 0.752787  [   96/  306]
train() client id: f_00004-2-3 loss: 0.969990  [  128/  306]
train() client id: f_00004-2-4 loss: 0.701015  [  160/  306]
train() client id: f_00004-2-5 loss: 0.813280  [  192/  306]
train() client id: f_00004-2-6 loss: 0.732435  [  224/  306]
train() client id: f_00004-2-7 loss: 0.735260  [  256/  306]
train() client id: f_00004-2-8 loss: 0.876128  [  288/  306]
train() client id: f_00004-3-0 loss: 0.779426  [   32/  306]
train() client id: f_00004-3-1 loss: 0.825067  [   64/  306]
train() client id: f_00004-3-2 loss: 0.967737  [   96/  306]
train() client id: f_00004-3-3 loss: 0.807265  [  128/  306]
train() client id: f_00004-3-4 loss: 0.780753  [  160/  306]
train() client id: f_00004-3-5 loss: 0.844448  [  192/  306]
train() client id: f_00004-3-6 loss: 0.669818  [  224/  306]
train() client id: f_00004-3-7 loss: 0.832967  [  256/  306]
train() client id: f_00004-3-8 loss: 0.700542  [  288/  306]
train() client id: f_00004-4-0 loss: 0.840310  [   32/  306]
train() client id: f_00004-4-1 loss: 0.789476  [   64/  306]
train() client id: f_00004-4-2 loss: 0.759558  [   96/  306]
train() client id: f_00004-4-3 loss: 1.003383  [  128/  306]
train() client id: f_00004-4-4 loss: 0.850639  [  160/  306]
train() client id: f_00004-4-5 loss: 0.779932  [  192/  306]
train() client id: f_00004-4-6 loss: 0.738569  [  224/  306]
train() client id: f_00004-4-7 loss: 0.778249  [  256/  306]
train() client id: f_00004-4-8 loss: 0.744102  [  288/  306]
train() client id: f_00004-5-0 loss: 0.857843  [   32/  306]
train() client id: f_00004-5-1 loss: 0.849838  [   64/  306]
train() client id: f_00004-5-2 loss: 0.775038  [   96/  306]
train() client id: f_00004-5-3 loss: 0.860277  [  128/  306]
train() client id: f_00004-5-4 loss: 0.880119  [  160/  306]
train() client id: f_00004-5-5 loss: 0.805794  [  192/  306]
train() client id: f_00004-5-6 loss: 0.785789  [  224/  306]
train() client id: f_00004-5-7 loss: 0.712614  [  256/  306]
train() client id: f_00004-5-8 loss: 0.865222  [  288/  306]
train() client id: f_00004-6-0 loss: 0.782371  [   32/  306]
train() client id: f_00004-6-1 loss: 0.804150  [   64/  306]
train() client id: f_00004-6-2 loss: 0.897519  [   96/  306]
train() client id: f_00004-6-3 loss: 0.702248  [  128/  306]
train() client id: f_00004-6-4 loss: 0.876859  [  160/  306]
train() client id: f_00004-6-5 loss: 0.842127  [  192/  306]
train() client id: f_00004-6-6 loss: 0.809029  [  224/  306]
train() client id: f_00004-6-7 loss: 0.755545  [  256/  306]
train() client id: f_00004-6-8 loss: 0.709360  [  288/  306]
train() client id: f_00004-7-0 loss: 0.877338  [   32/  306]
train() client id: f_00004-7-1 loss: 0.875552  [   64/  306]
train() client id: f_00004-7-2 loss: 0.860624  [   96/  306]
train() client id: f_00004-7-3 loss: 0.745978  [  128/  306]
train() client id: f_00004-7-4 loss: 0.932450  [  160/  306]
train() client id: f_00004-7-5 loss: 0.781186  [  192/  306]
train() client id: f_00004-7-6 loss: 0.799924  [  224/  306]
train() client id: f_00004-7-7 loss: 0.716352  [  256/  306]
train() client id: f_00004-7-8 loss: 0.728026  [  288/  306]
train() client id: f_00004-8-0 loss: 0.852519  [   32/  306]
train() client id: f_00004-8-1 loss: 0.934497  [   64/  306]
train() client id: f_00004-8-2 loss: 0.810356  [   96/  306]
train() client id: f_00004-8-3 loss: 0.838652  [  128/  306]
train() client id: f_00004-8-4 loss: 0.767177  [  160/  306]
train() client id: f_00004-8-5 loss: 0.844793  [  192/  306]
train() client id: f_00004-8-6 loss: 0.769652  [  224/  306]
train() client id: f_00004-8-7 loss: 0.732168  [  256/  306]
train() client id: f_00004-8-8 loss: 0.735247  [  288/  306]
train() client id: f_00004-9-0 loss: 0.749503  [   32/  306]
train() client id: f_00004-9-1 loss: 0.906923  [   64/  306]
train() client id: f_00004-9-2 loss: 0.871847  [   96/  306]
train() client id: f_00004-9-3 loss: 0.830172  [  128/  306]
train() client id: f_00004-9-4 loss: 0.866129  [  160/  306]
train() client id: f_00004-9-5 loss: 0.739860  [  192/  306]
train() client id: f_00004-9-6 loss: 0.830182  [  224/  306]
train() client id: f_00004-9-7 loss: 0.740199  [  256/  306]
train() client id: f_00004-9-8 loss: 0.790985  [  288/  306]
train() client id: f_00004-10-0 loss: 0.903809  [   32/  306]
train() client id: f_00004-10-1 loss: 0.972298  [   64/  306]
train() client id: f_00004-10-2 loss: 0.838155  [   96/  306]
train() client id: f_00004-10-3 loss: 0.758837  [  128/  306]
train() client id: f_00004-10-4 loss: 0.734753  [  160/  306]
train() client id: f_00004-10-5 loss: 0.760515  [  192/  306]
train() client id: f_00004-10-6 loss: 0.783624  [  224/  306]
train() client id: f_00004-10-7 loss: 0.900943  [  256/  306]
train() client id: f_00004-10-8 loss: 0.773775  [  288/  306]
train() client id: f_00004-11-0 loss: 0.884961  [   32/  306]
train() client id: f_00004-11-1 loss: 0.769091  [   64/  306]
train() client id: f_00004-11-2 loss: 0.661762  [   96/  306]
train() client id: f_00004-11-3 loss: 0.920019  [  128/  306]
train() client id: f_00004-11-4 loss: 0.777176  [  160/  306]
train() client id: f_00004-11-5 loss: 0.816455  [  192/  306]
train() client id: f_00004-11-6 loss: 0.773407  [  224/  306]
train() client id: f_00004-11-7 loss: 0.760162  [  256/  306]
train() client id: f_00004-11-8 loss: 0.956338  [  288/  306]
train() client id: f_00005-0-0 loss: 0.854273  [   32/  146]
train() client id: f_00005-0-1 loss: 0.590317  [   64/  146]
train() client id: f_00005-0-2 loss: 1.070477  [   96/  146]
train() client id: f_00005-0-3 loss: 0.656139  [  128/  146]
train() client id: f_00005-1-0 loss: 0.696803  [   32/  146]
train() client id: f_00005-1-1 loss: 0.628766  [   64/  146]
train() client id: f_00005-1-2 loss: 0.762267  [   96/  146]
train() client id: f_00005-1-3 loss: 0.895082  [  128/  146]
train() client id: f_00005-2-0 loss: 0.765140  [   32/  146]
train() client id: f_00005-2-1 loss: 0.808897  [   64/  146]
train() client id: f_00005-2-2 loss: 0.751037  [   96/  146]
train() client id: f_00005-2-3 loss: 0.759534  [  128/  146]
train() client id: f_00005-3-0 loss: 0.854488  [   32/  146]
train() client id: f_00005-3-1 loss: 0.725038  [   64/  146]
train() client id: f_00005-3-2 loss: 0.641946  [   96/  146]
train() client id: f_00005-3-3 loss: 0.819005  [  128/  146]
train() client id: f_00005-4-0 loss: 0.935836  [   32/  146]
train() client id: f_00005-4-1 loss: 0.625623  [   64/  146]
train() client id: f_00005-4-2 loss: 0.630794  [   96/  146]
train() client id: f_00005-4-3 loss: 0.700843  [  128/  146]
train() client id: f_00005-5-0 loss: 0.604151  [   32/  146]
train() client id: f_00005-5-1 loss: 1.006055  [   64/  146]
train() client id: f_00005-5-2 loss: 0.551748  [   96/  146]
train() client id: f_00005-5-3 loss: 0.720748  [  128/  146]
train() client id: f_00005-6-0 loss: 0.721049  [   32/  146]
train() client id: f_00005-6-1 loss: 1.157093  [   64/  146]
train() client id: f_00005-6-2 loss: 0.594788  [   96/  146]
train() client id: f_00005-6-3 loss: 0.490224  [  128/  146]
train() client id: f_00005-7-0 loss: 0.655190  [   32/  146]
train() client id: f_00005-7-1 loss: 0.554147  [   64/  146]
train() client id: f_00005-7-2 loss: 1.013608  [   96/  146]
train() client id: f_00005-7-3 loss: 0.645714  [  128/  146]
train() client id: f_00005-8-0 loss: 0.629357  [   32/  146]
train() client id: f_00005-8-1 loss: 0.815899  [   64/  146]
train() client id: f_00005-8-2 loss: 0.528952  [   96/  146]
train() client id: f_00005-8-3 loss: 0.948555  [  128/  146]
train() client id: f_00005-9-0 loss: 0.903124  [   32/  146]
train() client id: f_00005-9-1 loss: 0.640960  [   64/  146]
train() client id: f_00005-9-2 loss: 0.760366  [   96/  146]
train() client id: f_00005-9-3 loss: 0.818504  [  128/  146]
train() client id: f_00005-10-0 loss: 0.894270  [   32/  146]
train() client id: f_00005-10-1 loss: 0.756912  [   64/  146]
train() client id: f_00005-10-2 loss: 0.815414  [   96/  146]
train() client id: f_00005-10-3 loss: 0.509048  [  128/  146]
train() client id: f_00005-11-0 loss: 0.874961  [   32/  146]
train() client id: f_00005-11-1 loss: 0.723005  [   64/  146]
train() client id: f_00005-11-2 loss: 0.619304  [   96/  146]
train() client id: f_00005-11-3 loss: 0.761656  [  128/  146]
train() client id: f_00006-0-0 loss: 0.514212  [   32/   54]
train() client id: f_00006-1-0 loss: 0.513323  [   32/   54]
train() client id: f_00006-2-0 loss: 0.570357  [   32/   54]
train() client id: f_00006-3-0 loss: 0.505953  [   32/   54]
train() client id: f_00006-4-0 loss: 0.485413  [   32/   54]
train() client id: f_00006-5-0 loss: 0.462153  [   32/   54]
train() client id: f_00006-6-0 loss: 0.563831  [   32/   54]
train() client id: f_00006-7-0 loss: 0.452058  [   32/   54]
train() client id: f_00006-8-0 loss: 0.561964  [   32/   54]
train() client id: f_00006-9-0 loss: 0.487575  [   32/   54]
train() client id: f_00006-10-0 loss: 0.501542  [   32/   54]
train() client id: f_00006-11-0 loss: 0.490026  [   32/   54]
train() client id: f_00007-0-0 loss: 0.714354  [   32/  179]
train() client id: f_00007-0-1 loss: 0.688947  [   64/  179]
train() client id: f_00007-0-2 loss: 0.733325  [   96/  179]
train() client id: f_00007-0-3 loss: 0.695365  [  128/  179]
train() client id: f_00007-0-4 loss: 0.758568  [  160/  179]
train() client id: f_00007-1-0 loss: 0.640639  [   32/  179]
train() client id: f_00007-1-1 loss: 0.550149  [   64/  179]
train() client id: f_00007-1-2 loss: 0.858888  [   96/  179]
train() client id: f_00007-1-3 loss: 0.651676  [  128/  179]
train() client id: f_00007-1-4 loss: 0.890170  [  160/  179]
train() client id: f_00007-2-0 loss: 0.657499  [   32/  179]
train() client id: f_00007-2-1 loss: 0.814175  [   64/  179]
train() client id: f_00007-2-2 loss: 0.600712  [   96/  179]
train() client id: f_00007-2-3 loss: 0.798561  [  128/  179]
train() client id: f_00007-2-4 loss: 0.692253  [  160/  179]
train() client id: f_00007-3-0 loss: 0.649308  [   32/  179]
train() client id: f_00007-3-1 loss: 0.616047  [   64/  179]
train() client id: f_00007-3-2 loss: 0.728138  [   96/  179]
train() client id: f_00007-3-3 loss: 0.619078  [  128/  179]
train() client id: f_00007-3-4 loss: 0.876072  [  160/  179]
train() client id: f_00007-4-0 loss: 0.613283  [   32/  179]
train() client id: f_00007-4-1 loss: 0.624144  [   64/  179]
train() client id: f_00007-4-2 loss: 0.683681  [   96/  179]
train() client id: f_00007-4-3 loss: 0.626621  [  128/  179]
train() client id: f_00007-4-4 loss: 0.781525  [  160/  179]
train() client id: f_00007-5-0 loss: 0.588937  [   32/  179]
train() client id: f_00007-5-1 loss: 0.667467  [   64/  179]
train() client id: f_00007-5-2 loss: 0.569046  [   96/  179]
train() client id: f_00007-5-3 loss: 0.969063  [  128/  179]
train() client id: f_00007-5-4 loss: 0.527657  [  160/  179]
train() client id: f_00007-6-0 loss: 0.793682  [   32/  179]
train() client id: f_00007-6-1 loss: 0.637016  [   64/  179]
train() client id: f_00007-6-2 loss: 0.797939  [   96/  179]
train() client id: f_00007-6-3 loss: 0.594362  [  128/  179]
train() client id: f_00007-6-4 loss: 0.618794  [  160/  179]
train() client id: f_00007-7-0 loss: 0.878553  [   32/  179]
train() client id: f_00007-7-1 loss: 0.793562  [   64/  179]
train() client id: f_00007-7-2 loss: 0.536430  [   96/  179]
train() client id: f_00007-7-3 loss: 0.688821  [  128/  179]
train() client id: f_00007-7-4 loss: 0.634883  [  160/  179]
train() client id: f_00007-8-0 loss: 0.595358  [   32/  179]
train() client id: f_00007-8-1 loss: 0.575061  [   64/  179]
train() client id: f_00007-8-2 loss: 0.983701  [   96/  179]
train() client id: f_00007-8-3 loss: 0.537975  [  128/  179]
train() client id: f_00007-8-4 loss: 0.778462  [  160/  179]
train() client id: f_00007-9-0 loss: 0.945130  [   32/  179]
train() client id: f_00007-9-1 loss: 0.516718  [   64/  179]
train() client id: f_00007-9-2 loss: 0.551513  [   96/  179]
train() client id: f_00007-9-3 loss: 0.761447  [  128/  179]
train() client id: f_00007-9-4 loss: 0.741712  [  160/  179]
train() client id: f_00007-10-0 loss: 0.671277  [   32/  179]
train() client id: f_00007-10-1 loss: 0.604002  [   64/  179]
train() client id: f_00007-10-2 loss: 1.033591  [   96/  179]
train() client id: f_00007-10-3 loss: 0.556168  [  128/  179]
train() client id: f_00007-10-4 loss: 0.720523  [  160/  179]
train() client id: f_00007-11-0 loss: 0.629835  [   32/  179]
train() client id: f_00007-11-1 loss: 0.613903  [   64/  179]
train() client id: f_00007-11-2 loss: 0.631894  [   96/  179]
train() client id: f_00007-11-3 loss: 0.668250  [  128/  179]
train() client id: f_00007-11-4 loss: 0.801612  [  160/  179]
train() client id: f_00008-0-0 loss: 0.844284  [   32/  130]
train() client id: f_00008-0-1 loss: 0.753119  [   64/  130]
train() client id: f_00008-0-2 loss: 0.680508  [   96/  130]
train() client id: f_00008-0-3 loss: 0.842154  [  128/  130]
train() client id: f_00008-1-0 loss: 0.833687  [   32/  130]
train() client id: f_00008-1-1 loss: 0.714316  [   64/  130]
train() client id: f_00008-1-2 loss: 0.760407  [   96/  130]
train() client id: f_00008-1-3 loss: 0.770816  [  128/  130]
train() client id: f_00008-2-0 loss: 0.681470  [   32/  130]
train() client id: f_00008-2-1 loss: 0.857222  [   64/  130]
train() client id: f_00008-2-2 loss: 0.706481  [   96/  130]
train() client id: f_00008-2-3 loss: 0.800494  [  128/  130]
train() client id: f_00008-3-0 loss: 0.740540  [   32/  130]
train() client id: f_00008-3-1 loss: 0.832311  [   64/  130]
train() client id: f_00008-3-2 loss: 0.792162  [   96/  130]
train() client id: f_00008-3-3 loss: 0.735292  [  128/  130]
train() client id: f_00008-4-0 loss: 0.918914  [   32/  130]
train() client id: f_00008-4-1 loss: 0.706653  [   64/  130]
train() client id: f_00008-4-2 loss: 0.691952  [   96/  130]
train() client id: f_00008-4-3 loss: 0.775353  [  128/  130]
train() client id: f_00008-5-0 loss: 0.644581  [   32/  130]
train() client id: f_00008-5-1 loss: 0.852273  [   64/  130]
train() client id: f_00008-5-2 loss: 0.814268  [   96/  130]
train() client id: f_00008-5-3 loss: 0.800296  [  128/  130]
train() client id: f_00008-6-0 loss: 0.780888  [   32/  130]
train() client id: f_00008-6-1 loss: 0.774732  [   64/  130]
train() client id: f_00008-6-2 loss: 0.766503  [   96/  130]
train() client id: f_00008-6-3 loss: 0.740282  [  128/  130]
train() client id: f_00008-7-0 loss: 0.765822  [   32/  130]
train() client id: f_00008-7-1 loss: 0.719682  [   64/  130]
train() client id: f_00008-7-2 loss: 0.814965  [   96/  130]
train() client id: f_00008-7-3 loss: 0.774834  [  128/  130]
train() client id: f_00008-8-0 loss: 0.946462  [   32/  130]
train() client id: f_00008-8-1 loss: 0.666473  [   64/  130]
train() client id: f_00008-8-2 loss: 0.741246  [   96/  130]
train() client id: f_00008-8-3 loss: 0.756989  [  128/  130]
train() client id: f_00008-9-0 loss: 0.824940  [   32/  130]
train() client id: f_00008-9-1 loss: 0.763184  [   64/  130]
train() client id: f_00008-9-2 loss: 0.869407  [   96/  130]
train() client id: f_00008-9-3 loss: 0.662102  [  128/  130]
train() client id: f_00008-10-0 loss: 0.849443  [   32/  130]
train() client id: f_00008-10-1 loss: 0.826553  [   64/  130]
train() client id: f_00008-10-2 loss: 0.715633  [   96/  130]
train() client id: f_00008-10-3 loss: 0.719128  [  128/  130]
train() client id: f_00008-11-0 loss: 0.705640  [   32/  130]
train() client id: f_00008-11-1 loss: 0.763581  [   64/  130]
train() client id: f_00008-11-2 loss: 0.792770  [   96/  130]
train() client id: f_00008-11-3 loss: 0.849427  [  128/  130]
train() client id: f_00009-0-0 loss: 1.118901  [   32/  118]
train() client id: f_00009-0-1 loss: 1.063273  [   64/  118]
train() client id: f_00009-0-2 loss: 0.968039  [   96/  118]
train() client id: f_00009-1-0 loss: 1.120826  [   32/  118]
train() client id: f_00009-1-1 loss: 0.966483  [   64/  118]
train() client id: f_00009-1-2 loss: 0.904812  [   96/  118]
train() client id: f_00009-2-0 loss: 1.050397  [   32/  118]
train() client id: f_00009-2-1 loss: 0.878674  [   64/  118]
train() client id: f_00009-2-2 loss: 0.979945  [   96/  118]
train() client id: f_00009-3-0 loss: 0.881554  [   32/  118]
train() client id: f_00009-3-1 loss: 0.894283  [   64/  118]
train() client id: f_00009-3-2 loss: 0.901906  [   96/  118]
train() client id: f_00009-4-0 loss: 0.851441  [   32/  118]
train() client id: f_00009-4-1 loss: 0.826692  [   64/  118]
train() client id: f_00009-4-2 loss: 0.953908  [   96/  118]
train() client id: f_00009-5-0 loss: 0.686630  [   32/  118]
train() client id: f_00009-5-1 loss: 1.055205  [   64/  118]
train() client id: f_00009-5-2 loss: 0.840200  [   96/  118]
train() client id: f_00009-6-0 loss: 0.751502  [   32/  118]
train() client id: f_00009-6-1 loss: 0.803023  [   64/  118]
train() client id: f_00009-6-2 loss: 0.985483  [   96/  118]
train() client id: f_00009-7-0 loss: 0.731477  [   32/  118]
train() client id: f_00009-7-1 loss: 0.815330  [   64/  118]
train() client id: f_00009-7-2 loss: 0.824871  [   96/  118]
train() client id: f_00009-8-0 loss: 0.830552  [   32/  118]
train() client id: f_00009-8-1 loss: 0.945517  [   64/  118]
train() client id: f_00009-8-2 loss: 0.740564  [   96/  118]
train() client id: f_00009-9-0 loss: 0.796577  [   32/  118]
train() client id: f_00009-9-1 loss: 0.848019  [   64/  118]
train() client id: f_00009-9-2 loss: 0.889446  [   96/  118]
train() client id: f_00009-10-0 loss: 0.799581  [   32/  118]
train() client id: f_00009-10-1 loss: 0.779495  [   64/  118]
train() client id: f_00009-10-2 loss: 0.863181  [   96/  118]
train() client id: f_00009-11-0 loss: 0.794361  [   32/  118]
train() client id: f_00009-11-1 loss: 0.760489  [   64/  118]
train() client id: f_00009-11-2 loss: 0.875227  [   96/  118]
At round 33 accuracy: 0.6525198938992043
At round 33 training accuracy: 0.5861837692823608
At round 33 training loss: 0.8245368130364336
update_location
xs = [  -3.9056584     4.20031788  185.00902392   18.81129433    0.97929623
    3.95640986 -147.44319194 -126.32485185  169.66397685 -112.06087855]
ys = [ 177.5879595   160.55583871    1.32061395 -147.45517586  139.35018685
  122.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [203.84488595 189.197833   210.30949325 179.15605957 171.52094215
 158.42654071 178.17515369 161.11686582 197.72338004 150.24530728]
dists_bs = [171.11391311 179.78149994 399.71368058 376.21823221 179.55395256
 186.4101915  179.74986154 180.85111144 378.91658027 182.21368292]
uav_gains = [1.61237068e-11 1.99260114e-11 1.46504448e-11 2.30455348e-11
 2.58027311e-11 3.15864432e-11 2.33790442e-11 3.02686694e-11
 1.76243014e-11 3.61021161e-11]
bs_gains = [6.16730151e-11 5.37041796e-11 5.73319680e-12 6.79301444e-12
 5.38949620e-11 4.85265100e-11 5.37306513e-11 5.28195595e-11
 6.65843217e-12 5.17210489e-11]
Round 34
-------------------------------
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.50361085 13.46800316  6.40688418  2.3090212  15.53330799  7.47581585
  2.86265615  9.15122779  6.75143071  6.06373403]
obj_prev = 76.52569191747648
eta_min = 6.016067092016877e-15	eta_max = 0.9281867977986356
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 17.761433286506318	eta = 0.9090909090909091
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 32.349008668159414	eta = 0.4991422673511644
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 25.200112188715238	eta = 0.6407414940167677
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.908304317469916	eta = 0.6753618876010812
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.840631284811383	eta = 0.6772789419999331
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.84043047624284	eta = 0.6772846467381501
eta = 0.6772846467381501
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [0.03215411 0.06762571 0.03164373 0.01097323 0.07808855 0.03725794
 0.01378033 0.04567925 0.03317488 0.03011258]
ene_total = [2.12711913 3.82878864 2.11331163 1.00058971 4.36464773 2.28034368
 1.14291118 2.75181183 2.3215154  1.90939155]
ti_comp = [0.45871166 0.48133195 0.45630249 0.4667549  0.48138282 0.47984682
 0.46705067 0.47204883 0.43098118 0.48078779]
ti_coms = [0.09358479 0.0709645  0.09599397 0.08554155 0.07091363 0.07244963
 0.08524578 0.08024762 0.12131527 0.07150866]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.87438211e-06 8.34307382e-05 9.51124209e-06 3.79057801e-07
 1.28428366e-04 1.40388386e-05 7.49775295e-07 2.67339474e-05
 1.22854563e-05 7.38273045e-06]
ene_total = [0.48003762 0.36789952 0.49236369 0.43833721 0.36994456 0.37195374
 0.43684068 0.41256138 0.62225307 0.3667911 ]
optimize_network_iter = 0 obj = 4.3589825705706815
eta = 0.6772846467381501
freqs = [35048280.46691417 70248512.05463123 34674068.18486581 11754807.32564252
 81108578.19233961 38822740.19627412 14752502.84041418 48384031.59575192
 38487614.47521663 31315873.21898438]
eta_min = 0.677284646738166	eta_max = 0.6772846467381503
af = 0.012426086302136423	bf = 1.4018686060877306	zeta = 0.013668694932350066	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [2.41523466e-06 2.04068274e-05 2.32641206e-06 9.27160332e-08
 3.14130684e-05 3.43384418e-06 1.83392061e-07 6.53901742e-06
 3.00497385e-06 1.80578656e-06]
ene_total = [1.70271381 1.29453122 1.74651965 1.55598755 1.29560796 1.31845798
 1.55062412 1.46086543 2.20722871 1.30104589]
ti_comp = [0.45871166 0.48133195 0.45630249 0.4667549  0.48138282 0.47984682
 0.46705067 0.47204883 0.43098118 0.48078779]
ti_coms = [0.09358479 0.0709645  0.09599397 0.08554155 0.07091363 0.07244963
 0.08524578 0.08024762 0.12131527 0.07150866]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.87438211e-06 8.34307382e-05 9.51124209e-06 3.79057801e-07
 1.28428366e-04 1.40388386e-05 7.49775295e-07 2.67339474e-05
 1.22854563e-05 7.38273045e-06]
ene_total = [0.48003762 0.36789952 0.49236369 0.43833721 0.36994456 0.37195374
 0.43684068 0.41256138 0.62225307 0.3667911 ]
optimize_network_iter = 1 obj = 4.358982570570683
eta = 0.6772846467381503
freqs = [35048280.46691417 70248512.05463123 34674068.18486582 11754807.32564252
 81108578.19233961 38822740.19627412 14752502.84041418 48384031.59575192
 38487614.47521663 31315873.21898438]
Done!
At round 34 eta: 0.6772846467381503
At round 34 local rounds: 12.759567346547202
At round 34 global rounds: 51.24033886878731
At round 34 a_n: 16.193498212328272
gradient difference: 0.43160128593444824
train() client id: f_00000-0-0 loss: 1.224796  [   32/  126]
train() client id: f_00000-0-1 loss: 1.230700  [   64/  126]
train() client id: f_00000-0-2 loss: 0.979397  [   96/  126]
train() client id: f_00000-1-0 loss: 1.082403  [   32/  126]
train() client id: f_00000-1-1 loss: 1.078203  [   64/  126]
train() client id: f_00000-1-2 loss: 0.915228  [   96/  126]
train() client id: f_00000-2-0 loss: 0.896753  [   32/  126]
train() client id: f_00000-2-1 loss: 0.967287  [   64/  126]
train() client id: f_00000-2-2 loss: 0.962130  [   96/  126]
train() client id: f_00000-3-0 loss: 0.924506  [   32/  126]
train() client id: f_00000-3-1 loss: 1.075688  [   64/  126]
train() client id: f_00000-3-2 loss: 0.792498  [   96/  126]
train() client id: f_00000-4-0 loss: 0.926662  [   32/  126]
train() client id: f_00000-4-1 loss: 0.894593  [   64/  126]
train() client id: f_00000-4-2 loss: 0.879335  [   96/  126]
train() client id: f_00000-5-0 loss: 0.921086  [   32/  126]
train() client id: f_00000-5-1 loss: 0.799252  [   64/  126]
train() client id: f_00000-5-2 loss: 0.842169  [   96/  126]
train() client id: f_00000-6-0 loss: 0.767402  [   32/  126]
train() client id: f_00000-6-1 loss: 0.870902  [   64/  126]
train() client id: f_00000-6-2 loss: 0.771773  [   96/  126]
train() client id: f_00000-7-0 loss: 0.875399  [   32/  126]
train() client id: f_00000-7-1 loss: 0.868327  [   64/  126]
train() client id: f_00000-7-2 loss: 0.755801  [   96/  126]
train() client id: f_00000-8-0 loss: 0.789568  [   32/  126]
train() client id: f_00000-8-1 loss: 0.834505  [   64/  126]
train() client id: f_00000-8-2 loss: 0.836930  [   96/  126]
train() client id: f_00000-9-0 loss: 0.879790  [   32/  126]
train() client id: f_00000-9-1 loss: 0.733964  [   64/  126]
train() client id: f_00000-9-2 loss: 0.829133  [   96/  126]
train() client id: f_00000-10-0 loss: 0.872416  [   32/  126]
train() client id: f_00000-10-1 loss: 0.696124  [   64/  126]
train() client id: f_00000-10-2 loss: 0.773902  [   96/  126]
train() client id: f_00000-11-0 loss: 0.846333  [   32/  126]
train() client id: f_00000-11-1 loss: 0.809075  [   64/  126]
train() client id: f_00000-11-2 loss: 0.694430  [   96/  126]
train() client id: f_00001-0-0 loss: 0.544321  [   32/  265]
train() client id: f_00001-0-1 loss: 0.622705  [   64/  265]
train() client id: f_00001-0-2 loss: 0.490801  [   96/  265]
train() client id: f_00001-0-3 loss: 0.620752  [  128/  265]
train() client id: f_00001-0-4 loss: 0.493906  [  160/  265]
train() client id: f_00001-0-5 loss: 0.460020  [  192/  265]
train() client id: f_00001-0-6 loss: 0.425057  [  224/  265]
train() client id: f_00001-0-7 loss: 0.463770  [  256/  265]
train() client id: f_00001-1-0 loss: 0.482509  [   32/  265]
train() client id: f_00001-1-1 loss: 0.451104  [   64/  265]
train() client id: f_00001-1-2 loss: 0.566465  [   96/  265]
train() client id: f_00001-1-3 loss: 0.456377  [  128/  265]
train() client id: f_00001-1-4 loss: 0.577450  [  160/  265]
train() client id: f_00001-1-5 loss: 0.679768  [  192/  265]
train() client id: f_00001-1-6 loss: 0.487087  [  224/  265]
train() client id: f_00001-1-7 loss: 0.410298  [  256/  265]
train() client id: f_00001-2-0 loss: 0.432545  [   32/  265]
train() client id: f_00001-2-1 loss: 0.588020  [   64/  265]
train() client id: f_00001-2-2 loss: 0.448458  [   96/  265]
train() client id: f_00001-2-3 loss: 0.516550  [  128/  265]
train() client id: f_00001-2-4 loss: 0.458308  [  160/  265]
train() client id: f_00001-2-5 loss: 0.539827  [  192/  265]
train() client id: f_00001-2-6 loss: 0.555716  [  224/  265]
train() client id: f_00001-2-7 loss: 0.481525  [  256/  265]
train() client id: f_00001-3-0 loss: 0.498938  [   32/  265]
train() client id: f_00001-3-1 loss: 0.416513  [   64/  265]
train() client id: f_00001-3-2 loss: 0.571433  [   96/  265]
train() client id: f_00001-3-3 loss: 0.519112  [  128/  265]
train() client id: f_00001-3-4 loss: 0.510378  [  160/  265]
train() client id: f_00001-3-5 loss: 0.414556  [  192/  265]
train() client id: f_00001-3-6 loss: 0.503859  [  224/  265]
train() client id: f_00001-3-7 loss: 0.621664  [  256/  265]
train() client id: f_00001-4-0 loss: 0.497884  [   32/  265]
train() client id: f_00001-4-1 loss: 0.417499  [   64/  265]
train() client id: f_00001-4-2 loss: 0.590488  [   96/  265]
train() client id: f_00001-4-3 loss: 0.568357  [  128/  265]
train() client id: f_00001-4-4 loss: 0.418070  [  160/  265]
train() client id: f_00001-4-5 loss: 0.555035  [  192/  265]
train() client id: f_00001-4-6 loss: 0.510255  [  224/  265]
train() client id: f_00001-4-7 loss: 0.472151  [  256/  265]
train() client id: f_00001-5-0 loss: 0.557443  [   32/  265]
train() client id: f_00001-5-1 loss: 0.417347  [   64/  265]
train() client id: f_00001-5-2 loss: 0.525840  [   96/  265]
train() client id: f_00001-5-3 loss: 0.457586  [  128/  265]
train() client id: f_00001-5-4 loss: 0.558886  [  160/  265]
train() client id: f_00001-5-5 loss: 0.537158  [  192/  265]
train() client id: f_00001-5-6 loss: 0.484742  [  224/  265]
train() client id: f_00001-5-7 loss: 0.462628  [  256/  265]
train() client id: f_00001-6-0 loss: 0.514338  [   32/  265]
train() client id: f_00001-6-1 loss: 0.429036  [   64/  265]
train() client id: f_00001-6-2 loss: 0.527305  [   96/  265]
train() client id: f_00001-6-3 loss: 0.448440  [  128/  265]
train() client id: f_00001-6-4 loss: 0.413319  [  160/  265]
train() client id: f_00001-6-5 loss: 0.516117  [  192/  265]
train() client id: f_00001-6-6 loss: 0.619601  [  224/  265]
train() client id: f_00001-6-7 loss: 0.523807  [  256/  265]
train() client id: f_00001-7-0 loss: 0.475857  [   32/  265]
train() client id: f_00001-7-1 loss: 0.521097  [   64/  265]
train() client id: f_00001-7-2 loss: 0.491658  [   96/  265]
train() client id: f_00001-7-3 loss: 0.488082  [  128/  265]
train() client id: f_00001-7-4 loss: 0.559680  [  160/  265]
train() client id: f_00001-7-5 loss: 0.446528  [  192/  265]
train() client id: f_00001-7-6 loss: 0.463808  [  224/  265]
train() client id: f_00001-7-7 loss: 0.543873  [  256/  265]
train() client id: f_00001-8-0 loss: 0.465293  [   32/  265]
train() client id: f_00001-8-1 loss: 0.570526  [   64/  265]
train() client id: f_00001-8-2 loss: 0.540480  [   96/  265]
train() client id: f_00001-8-3 loss: 0.405970  [  128/  265]
train() client id: f_00001-8-4 loss: 0.474660  [  160/  265]
train() client id: f_00001-8-5 loss: 0.544236  [  192/  265]
train() client id: f_00001-8-6 loss: 0.400444  [  224/  265]
train() client id: f_00001-8-7 loss: 0.576928  [  256/  265]
train() client id: f_00001-9-0 loss: 0.474551  [   32/  265]
train() client id: f_00001-9-1 loss: 0.567601  [   64/  265]
train() client id: f_00001-9-2 loss: 0.448717  [   96/  265]
train() client id: f_00001-9-3 loss: 0.461067  [  128/  265]
train() client id: f_00001-9-4 loss: 0.409500  [  160/  265]
train() client id: f_00001-9-5 loss: 0.619531  [  192/  265]
train() client id: f_00001-9-6 loss: 0.445086  [  224/  265]
train() client id: f_00001-9-7 loss: 0.498597  [  256/  265]
train() client id: f_00001-10-0 loss: 0.437907  [   32/  265]
train() client id: f_00001-10-1 loss: 0.586452  [   64/  265]
train() client id: f_00001-10-2 loss: 0.448067  [   96/  265]
train() client id: f_00001-10-3 loss: 0.599009  [  128/  265]
train() client id: f_00001-10-4 loss: 0.563667  [  160/  265]
train() client id: f_00001-10-5 loss: 0.482827  [  192/  265]
train() client id: f_00001-10-6 loss: 0.416372  [  224/  265]
train() client id: f_00001-10-7 loss: 0.406233  [  256/  265]
train() client id: f_00001-11-0 loss: 0.542917  [   32/  265]
train() client id: f_00001-11-1 loss: 0.558881  [   64/  265]
train() client id: f_00001-11-2 loss: 0.547263  [   96/  265]
train() client id: f_00001-11-3 loss: 0.426729  [  128/  265]
train() client id: f_00001-11-4 loss: 0.461891  [  160/  265]
train() client id: f_00001-11-5 loss: 0.462591  [  192/  265]
train() client id: f_00001-11-6 loss: 0.588665  [  224/  265]
train() client id: f_00001-11-7 loss: 0.394782  [  256/  265]
train() client id: f_00002-0-0 loss: 1.340396  [   32/  124]
train() client id: f_00002-0-1 loss: 1.219740  [   64/  124]
train() client id: f_00002-0-2 loss: 1.231413  [   96/  124]
train() client id: f_00002-1-0 loss: 1.288733  [   32/  124]
train() client id: f_00002-1-1 loss: 1.170283  [   64/  124]
train() client id: f_00002-1-2 loss: 1.134986  [   96/  124]
train() client id: f_00002-2-0 loss: 1.223759  [   32/  124]
train() client id: f_00002-2-1 loss: 1.046815  [   64/  124]
train() client id: f_00002-2-2 loss: 1.252385  [   96/  124]
train() client id: f_00002-3-0 loss: 1.281172  [   32/  124]
train() client id: f_00002-3-1 loss: 1.047114  [   64/  124]
train() client id: f_00002-3-2 loss: 1.025990  [   96/  124]
train() client id: f_00002-4-0 loss: 1.071113  [   32/  124]
train() client id: f_00002-4-1 loss: 1.228595  [   64/  124]
train() client id: f_00002-4-2 loss: 0.968424  [   96/  124]
train() client id: f_00002-5-0 loss: 0.919018  [   32/  124]
train() client id: f_00002-5-1 loss: 1.170077  [   64/  124]
train() client id: f_00002-5-2 loss: 1.098432  [   96/  124]
train() client id: f_00002-6-0 loss: 1.151422  [   32/  124]
train() client id: f_00002-6-1 loss: 0.939628  [   64/  124]
train() client id: f_00002-6-2 loss: 1.018999  [   96/  124]
train() client id: f_00002-7-0 loss: 1.122397  [   32/  124]
train() client id: f_00002-7-1 loss: 0.941245  [   64/  124]
train() client id: f_00002-7-2 loss: 1.046632  [   96/  124]
train() client id: f_00002-8-0 loss: 0.976093  [   32/  124]
train() client id: f_00002-8-1 loss: 1.022211  [   64/  124]
train() client id: f_00002-8-2 loss: 1.037240  [   96/  124]
train() client id: f_00002-9-0 loss: 1.063599  [   32/  124]
train() client id: f_00002-9-1 loss: 0.989891  [   64/  124]
train() client id: f_00002-9-2 loss: 1.051140  [   96/  124]
train() client id: f_00002-10-0 loss: 0.964154  [   32/  124]
train() client id: f_00002-10-1 loss: 1.070099  [   64/  124]
train() client id: f_00002-10-2 loss: 1.081926  [   96/  124]
train() client id: f_00002-11-0 loss: 0.956085  [   32/  124]
train() client id: f_00002-11-1 loss: 1.082727  [   64/  124]
train() client id: f_00002-11-2 loss: 0.931404  [   96/  124]
train() client id: f_00003-0-0 loss: 0.841477  [   32/   43]
train() client id: f_00003-1-0 loss: 0.634214  [   32/   43]
train() client id: f_00003-2-0 loss: 0.580032  [   32/   43]
train() client id: f_00003-3-0 loss: 0.790319  [   32/   43]
train() client id: f_00003-4-0 loss: 0.510723  [   32/   43]
train() client id: f_00003-5-0 loss: 0.529559  [   32/   43]
train() client id: f_00003-6-0 loss: 0.706663  [   32/   43]
train() client id: f_00003-7-0 loss: 0.799085  [   32/   43]
train() client id: f_00003-8-0 loss: 0.724697  [   32/   43]
train() client id: f_00003-9-0 loss: 0.658924  [   32/   43]
train() client id: f_00003-10-0 loss: 0.847066  [   32/   43]
train() client id: f_00003-11-0 loss: 0.804061  [   32/   43]
train() client id: f_00004-0-0 loss: 0.702997  [   32/  306]
train() client id: f_00004-0-1 loss: 0.762551  [   64/  306]
train() client id: f_00004-0-2 loss: 0.614707  [   96/  306]
train() client id: f_00004-0-3 loss: 0.842622  [  128/  306]
train() client id: f_00004-0-4 loss: 0.690205  [  160/  306]
train() client id: f_00004-0-5 loss: 0.729182  [  192/  306]
train() client id: f_00004-0-6 loss: 0.772331  [  224/  306]
train() client id: f_00004-0-7 loss: 0.792196  [  256/  306]
train() client id: f_00004-0-8 loss: 0.718904  [  288/  306]
train() client id: f_00004-1-0 loss: 0.655597  [   32/  306]
train() client id: f_00004-1-1 loss: 0.797623  [   64/  306]
train() client id: f_00004-1-2 loss: 0.792348  [   96/  306]
train() client id: f_00004-1-3 loss: 0.673626  [  128/  306]
train() client id: f_00004-1-4 loss: 0.808412  [  160/  306]
train() client id: f_00004-1-5 loss: 0.648402  [  192/  306]
train() client id: f_00004-1-6 loss: 0.774967  [  224/  306]
train() client id: f_00004-1-7 loss: 0.785757  [  256/  306]
train() client id: f_00004-1-8 loss: 0.738190  [  288/  306]
train() client id: f_00004-2-0 loss: 0.625489  [   32/  306]
train() client id: f_00004-2-1 loss: 0.819847  [   64/  306]
train() client id: f_00004-2-2 loss: 0.691046  [   96/  306]
train() client id: f_00004-2-3 loss: 0.657387  [  128/  306]
train() client id: f_00004-2-4 loss: 0.638641  [  160/  306]
train() client id: f_00004-2-5 loss: 0.897811  [  192/  306]
train() client id: f_00004-2-6 loss: 0.775478  [  224/  306]
train() client id: f_00004-2-7 loss: 0.688022  [  256/  306]
train() client id: f_00004-2-8 loss: 0.811472  [  288/  306]
train() client id: f_00004-3-0 loss: 0.614946  [   32/  306]
train() client id: f_00004-3-1 loss: 0.780846  [   64/  306]
train() client id: f_00004-3-2 loss: 0.769554  [   96/  306]
train() client id: f_00004-3-3 loss: 0.591358  [  128/  306]
train() client id: f_00004-3-4 loss: 0.785469  [  160/  306]
train() client id: f_00004-3-5 loss: 0.752521  [  192/  306]
train() client id: f_00004-3-6 loss: 0.840028  [  224/  306]
train() client id: f_00004-3-7 loss: 0.701987  [  256/  306]
train() client id: f_00004-3-8 loss: 0.794760  [  288/  306]
train() client id: f_00004-4-0 loss: 0.774749  [   32/  306]
train() client id: f_00004-4-1 loss: 0.721650  [   64/  306]
train() client id: f_00004-4-2 loss: 0.710208  [   96/  306]
train() client id: f_00004-4-3 loss: 0.750356  [  128/  306]
train() client id: f_00004-4-4 loss: 0.746643  [  160/  306]
train() client id: f_00004-4-5 loss: 0.556841  [  192/  306]
train() client id: f_00004-4-6 loss: 0.675852  [  224/  306]
train() client id: f_00004-4-7 loss: 0.968770  [  256/  306]
train() client id: f_00004-4-8 loss: 0.715369  [  288/  306]
train() client id: f_00004-5-0 loss: 0.770734  [   32/  306]
train() client id: f_00004-5-1 loss: 0.749135  [   64/  306]
train() client id: f_00004-5-2 loss: 0.613384  [   96/  306]
train() client id: f_00004-5-3 loss: 0.670377  [  128/  306]
train() client id: f_00004-5-4 loss: 0.734749  [  160/  306]
train() client id: f_00004-5-5 loss: 0.725424  [  192/  306]
train() client id: f_00004-5-6 loss: 0.789392  [  224/  306]
train() client id: f_00004-5-7 loss: 0.666868  [  256/  306]
train() client id: f_00004-5-8 loss: 0.884262  [  288/  306]
train() client id: f_00004-6-0 loss: 0.661523  [   32/  306]
train() client id: f_00004-6-1 loss: 0.716790  [   64/  306]
train() client id: f_00004-6-2 loss: 0.663671  [   96/  306]
train() client id: f_00004-6-3 loss: 0.710103  [  128/  306]
train() client id: f_00004-6-4 loss: 0.682444  [  160/  306]
train() client id: f_00004-6-5 loss: 0.799770  [  192/  306]
train() client id: f_00004-6-6 loss: 0.903495  [  224/  306]
train() client id: f_00004-6-7 loss: 0.738430  [  256/  306]
train() client id: f_00004-6-8 loss: 0.819391  [  288/  306]
train() client id: f_00004-7-0 loss: 0.725946  [   32/  306]
train() client id: f_00004-7-1 loss: 0.725580  [   64/  306]
train() client id: f_00004-7-2 loss: 0.670183  [   96/  306]
train() client id: f_00004-7-3 loss: 0.710832  [  128/  306]
train() client id: f_00004-7-4 loss: 0.851749  [  160/  306]
train() client id: f_00004-7-5 loss: 0.779988  [  192/  306]
train() client id: f_00004-7-6 loss: 0.669778  [  224/  306]
train() client id: f_00004-7-7 loss: 0.693668  [  256/  306]
train() client id: f_00004-7-8 loss: 0.753286  [  288/  306]
train() client id: f_00004-8-0 loss: 0.802862  [   32/  306]
train() client id: f_00004-8-1 loss: 0.759387  [   64/  306]
train() client id: f_00004-8-2 loss: 0.796888  [   96/  306]
train() client id: f_00004-8-3 loss: 0.764643  [  128/  306]
train() client id: f_00004-8-4 loss: 0.845246  [  160/  306]
train() client id: f_00004-8-5 loss: 0.850008  [  192/  306]
train() client id: f_00004-8-6 loss: 0.668793  [  224/  306]
train() client id: f_00004-8-7 loss: 0.629926  [  256/  306]
train() client id: f_00004-8-8 loss: 0.659009  [  288/  306]
train() client id: f_00004-9-0 loss: 0.788656  [   32/  306]
train() client id: f_00004-9-1 loss: 0.616515  [   64/  306]
train() client id: f_00004-9-2 loss: 0.704771  [   96/  306]
train() client id: f_00004-9-3 loss: 0.680104  [  128/  306]
train() client id: f_00004-9-4 loss: 0.735234  [  160/  306]
train() client id: f_00004-9-5 loss: 0.903508  [  192/  306]
train() client id: f_00004-9-6 loss: 0.839238  [  224/  306]
train() client id: f_00004-9-7 loss: 0.845449  [  256/  306]
train() client id: f_00004-9-8 loss: 0.665213  [  288/  306]
train() client id: f_00004-10-0 loss: 0.801041  [   32/  306]
train() client id: f_00004-10-1 loss: 0.693750  [   64/  306]
train() client id: f_00004-10-2 loss: 0.784218  [   96/  306]
train() client id: f_00004-10-3 loss: 0.817485  [  128/  306]
train() client id: f_00004-10-4 loss: 0.793002  [  160/  306]
train() client id: f_00004-10-5 loss: 0.750739  [  192/  306]
train() client id: f_00004-10-6 loss: 0.802626  [  224/  306]
train() client id: f_00004-10-7 loss: 0.657277  [  256/  306]
train() client id: f_00004-10-8 loss: 0.698567  [  288/  306]
train() client id: f_00004-11-0 loss: 0.789426  [   32/  306]
train() client id: f_00004-11-1 loss: 0.818276  [   64/  306]
train() client id: f_00004-11-2 loss: 0.820168  [   96/  306]
train() client id: f_00004-11-3 loss: 0.777206  [  128/  306]
train() client id: f_00004-11-4 loss: 0.782692  [  160/  306]
train() client id: f_00004-11-5 loss: 0.725396  [  192/  306]
train() client id: f_00004-11-6 loss: 0.744232  [  224/  306]
train() client id: f_00004-11-7 loss: 0.624833  [  256/  306]
train() client id: f_00004-11-8 loss: 0.719517  [  288/  306]
train() client id: f_00005-0-0 loss: 0.542921  [   32/  146]
train() client id: f_00005-0-1 loss: 0.655285  [   64/  146]
train() client id: f_00005-0-2 loss: 0.665356  [   96/  146]
train() client id: f_00005-0-3 loss: 0.681168  [  128/  146]
train() client id: f_00005-1-0 loss: 0.737557  [   32/  146]
train() client id: f_00005-1-1 loss: 0.536653  [   64/  146]
train() client id: f_00005-1-2 loss: 0.674940  [   96/  146]
train() client id: f_00005-1-3 loss: 0.608746  [  128/  146]
train() client id: f_00005-2-0 loss: 0.793530  [   32/  146]
train() client id: f_00005-2-1 loss: 0.467897  [   64/  146]
train() client id: f_00005-2-2 loss: 0.613783  [   96/  146]
train() client id: f_00005-2-3 loss: 0.434147  [  128/  146]
train() client id: f_00005-3-0 loss: 0.700480  [   32/  146]
train() client id: f_00005-3-1 loss: 0.762038  [   64/  146]
train() client id: f_00005-3-2 loss: 0.451418  [   96/  146]
train() client id: f_00005-3-3 loss: 0.571924  [  128/  146]
train() client id: f_00005-4-0 loss: 0.703372  [   32/  146]
train() client id: f_00005-4-1 loss: 0.631069  [   64/  146]
train() client id: f_00005-4-2 loss: 0.421419  [   96/  146]
train() client id: f_00005-4-3 loss: 0.516476  [  128/  146]
train() client id: f_00005-5-0 loss: 0.746460  [   32/  146]
train() client id: f_00005-5-1 loss: 0.401430  [   64/  146]
train() client id: f_00005-5-2 loss: 0.757242  [   96/  146]
train() client id: f_00005-5-3 loss: 0.588297  [  128/  146]
train() client id: f_00005-6-0 loss: 0.385602  [   32/  146]
train() client id: f_00005-6-1 loss: 0.884763  [   64/  146]
train() client id: f_00005-6-2 loss: 0.533049  [   96/  146]
train() client id: f_00005-6-3 loss: 0.704768  [  128/  146]
train() client id: f_00005-7-0 loss: 0.732347  [   32/  146]
train() client id: f_00005-7-1 loss: 0.494909  [   64/  146]
train() client id: f_00005-7-2 loss: 0.803841  [   96/  146]
train() client id: f_00005-7-3 loss: 0.411297  [  128/  146]
train() client id: f_00005-8-0 loss: 0.509785  [   32/  146]
train() client id: f_00005-8-1 loss: 0.631139  [   64/  146]
train() client id: f_00005-8-2 loss: 0.482732  [   96/  146]
train() client id: f_00005-8-3 loss: 0.697858  [  128/  146]
train() client id: f_00005-9-0 loss: 0.656827  [   32/  146]
train() client id: f_00005-9-1 loss: 0.679911  [   64/  146]
train() client id: f_00005-9-2 loss: 0.703312  [   96/  146]
train() client id: f_00005-9-3 loss: 0.563892  [  128/  146]
train() client id: f_00005-10-0 loss: 0.465045  [   32/  146]
train() client id: f_00005-10-1 loss: 0.630620  [   64/  146]
train() client id: f_00005-10-2 loss: 0.873126  [   96/  146]
train() client id: f_00005-10-3 loss: 0.615145  [  128/  146]
train() client id: f_00005-11-0 loss: 0.732432  [   32/  146]
train() client id: f_00005-11-1 loss: 0.425934  [   64/  146]
train() client id: f_00005-11-2 loss: 0.439458  [   96/  146]
train() client id: f_00005-11-3 loss: 0.753607  [  128/  146]
train() client id: f_00006-0-0 loss: 0.526963  [   32/   54]
train() client id: f_00006-1-0 loss: 0.565958  [   32/   54]
train() client id: f_00006-2-0 loss: 0.577667  [   32/   54]
train() client id: f_00006-3-0 loss: 0.537113  [   32/   54]
train() client id: f_00006-4-0 loss: 0.542212  [   32/   54]
train() client id: f_00006-5-0 loss: 0.529527  [   32/   54]
train() client id: f_00006-6-0 loss: 0.584974  [   32/   54]
train() client id: f_00006-7-0 loss: 0.517771  [   32/   54]
train() client id: f_00006-8-0 loss: 0.531266  [   32/   54]
train() client id: f_00006-9-0 loss: 0.535112  [   32/   54]
train() client id: f_00006-10-0 loss: 0.549034  [   32/   54]
train() client id: f_00006-11-0 loss: 0.540462  [   32/   54]
train() client id: f_00007-0-0 loss: 0.489418  [   32/  179]
train() client id: f_00007-0-1 loss: 0.563910  [   64/  179]
train() client id: f_00007-0-2 loss: 0.528141  [   96/  179]
train() client id: f_00007-0-3 loss: 0.456853  [  128/  179]
train() client id: f_00007-0-4 loss: 0.451927  [  160/  179]
train() client id: f_00007-1-0 loss: 0.492788  [   32/  179]
train() client id: f_00007-1-1 loss: 0.530330  [   64/  179]
train() client id: f_00007-1-2 loss: 0.320442  [   96/  179]
train() client id: f_00007-1-3 loss: 0.354865  [  128/  179]
train() client id: f_00007-1-4 loss: 0.771598  [  160/  179]
train() client id: f_00007-2-0 loss: 0.482549  [   32/  179]
train() client id: f_00007-2-1 loss: 0.409415  [   64/  179]
train() client id: f_00007-2-2 loss: 0.569167  [   96/  179]
train() client id: f_00007-2-3 loss: 0.436227  [  128/  179]
train() client id: f_00007-2-4 loss: 0.340925  [  160/  179]
train() client id: f_00007-3-0 loss: 0.515501  [   32/  179]
train() client id: f_00007-3-1 loss: 0.450070  [   64/  179]
train() client id: f_00007-3-2 loss: 0.684376  [   96/  179]
train() client id: f_00007-3-3 loss: 0.333954  [  128/  179]
train() client id: f_00007-3-4 loss: 0.321096  [  160/  179]
train() client id: f_00007-4-0 loss: 0.387881  [   32/  179]
train() client id: f_00007-4-1 loss: 0.460481  [   64/  179]
train() client id: f_00007-4-2 loss: 0.688356  [   96/  179]
train() client id: f_00007-4-3 loss: 0.483292  [  128/  179]
train() client id: f_00007-4-4 loss: 0.297069  [  160/  179]
train() client id: f_00007-5-0 loss: 0.298604  [   32/  179]
train() client id: f_00007-5-1 loss: 0.442084  [   64/  179]
train() client id: f_00007-5-2 loss: 0.483105  [   96/  179]
train() client id: f_00007-5-3 loss: 0.624774  [  128/  179]
train() client id: f_00007-5-4 loss: 0.449073  [  160/  179]
train() client id: f_00007-6-0 loss: 0.320862  [   32/  179]
train() client id: f_00007-6-1 loss: 0.308910  [   64/  179]
train() client id: f_00007-6-2 loss: 0.666706  [   96/  179]
train() client id: f_00007-6-3 loss: 0.520164  [  128/  179]
train() client id: f_00007-6-4 loss: 0.400140  [  160/  179]
train() client id: f_00007-7-0 loss: 0.318621  [   32/  179]
train() client id: f_00007-7-1 loss: 0.296055  [   64/  179]
train() client id: f_00007-7-2 loss: 0.660114  [   96/  179]
train() client id: f_00007-7-3 loss: 0.417499  [  128/  179]
train() client id: f_00007-7-4 loss: 0.264669  [  160/  179]
train() client id: f_00007-8-0 loss: 0.572172  [   32/  179]
train() client id: f_00007-8-1 loss: 0.401746  [   64/  179]
train() client id: f_00007-8-2 loss: 0.380025  [   96/  179]
train() client id: f_00007-8-3 loss: 0.612702  [  128/  179]
train() client id: f_00007-8-4 loss: 0.280690  [  160/  179]
train() client id: f_00007-9-0 loss: 0.298885  [   32/  179]
train() client id: f_00007-9-1 loss: 0.286622  [   64/  179]
train() client id: f_00007-9-2 loss: 0.652198  [   96/  179]
train() client id: f_00007-9-3 loss: 0.366173  [  128/  179]
train() client id: f_00007-9-4 loss: 0.476461  [  160/  179]
train() client id: f_00007-10-0 loss: 0.520121  [   32/  179]
train() client id: f_00007-10-1 loss: 0.323983  [   64/  179]
train() client id: f_00007-10-2 loss: 0.559054  [   96/  179]
train() client id: f_00007-10-3 loss: 0.473019  [  128/  179]
train() client id: f_00007-10-4 loss: 0.284758  [  160/  179]
train() client id: f_00007-11-0 loss: 0.570602  [   32/  179]
train() client id: f_00007-11-1 loss: 0.372086  [   64/  179]
train() client id: f_00007-11-2 loss: 0.473212  [   96/  179]
train() client id: f_00007-11-3 loss: 0.436221  [  128/  179]
train() client id: f_00007-11-4 loss: 0.369360  [  160/  179]
train() client id: f_00008-0-0 loss: 0.622565  [   32/  130]
train() client id: f_00008-0-1 loss: 0.593820  [   64/  130]
train() client id: f_00008-0-2 loss: 0.795702  [   96/  130]
train() client id: f_00008-0-3 loss: 0.702306  [  128/  130]
train() client id: f_00008-1-0 loss: 0.662301  [   32/  130]
train() client id: f_00008-1-1 loss: 0.672993  [   64/  130]
train() client id: f_00008-1-2 loss: 0.692743  [   96/  130]
train() client id: f_00008-1-3 loss: 0.681634  [  128/  130]
train() client id: f_00008-2-0 loss: 0.627190  [   32/  130]
train() client id: f_00008-2-1 loss: 0.717966  [   64/  130]
train() client id: f_00008-2-2 loss: 0.682896  [   96/  130]
train() client id: f_00008-2-3 loss: 0.644384  [  128/  130]
train() client id: f_00008-3-0 loss: 0.599547  [   32/  130]
train() client id: f_00008-3-1 loss: 0.762469  [   64/  130]
train() client id: f_00008-3-2 loss: 0.651554  [   96/  130]
train() client id: f_00008-3-3 loss: 0.689936  [  128/  130]
train() client id: f_00008-4-0 loss: 0.712438  [   32/  130]
train() client id: f_00008-4-1 loss: 0.767517  [   64/  130]
train() client id: f_00008-4-2 loss: 0.676182  [   96/  130]
train() client id: f_00008-4-3 loss: 0.541518  [  128/  130]
train() client id: f_00008-5-0 loss: 0.761021  [   32/  130]
train() client id: f_00008-5-1 loss: 0.623732  [   64/  130]
train() client id: f_00008-5-2 loss: 0.692549  [   96/  130]
train() client id: f_00008-5-3 loss: 0.615219  [  128/  130]
train() client id: f_00008-6-0 loss: 0.629710  [   32/  130]
train() client id: f_00008-6-1 loss: 0.758121  [   64/  130]
train() client id: f_00008-6-2 loss: 0.618159  [   96/  130]
train() client id: f_00008-6-3 loss: 0.606909  [  128/  130]
train() client id: f_00008-7-0 loss: 0.606308  [   32/  130]
train() client id: f_00008-7-1 loss: 0.717610  [   64/  130]
train() client id: f_00008-7-2 loss: 0.687432  [   96/  130]
train() client id: f_00008-7-3 loss: 0.674950  [  128/  130]
train() client id: f_00008-8-0 loss: 0.700869  [   32/  130]
train() client id: f_00008-8-1 loss: 0.646497  [   64/  130]
train() client id: f_00008-8-2 loss: 0.564558  [   96/  130]
train() client id: f_00008-8-3 loss: 0.761834  [  128/  130]
train() client id: f_00008-9-0 loss: 0.626162  [   32/  130]
train() client id: f_00008-9-1 loss: 0.547416  [   64/  130]
train() client id: f_00008-9-2 loss: 0.826697  [   96/  130]
train() client id: f_00008-9-3 loss: 0.642859  [  128/  130]
train() client id: f_00008-10-0 loss: 0.651581  [   32/  130]
train() client id: f_00008-10-1 loss: 0.675780  [   64/  130]
train() client id: f_00008-10-2 loss: 0.628912  [   96/  130]
train() client id: f_00008-10-3 loss: 0.719947  [  128/  130]
train() client id: f_00008-11-0 loss: 0.627032  [   32/  130]
train() client id: f_00008-11-1 loss: 0.776786  [   64/  130]
train() client id: f_00008-11-2 loss: 0.603798  [   96/  130]
train() client id: f_00008-11-3 loss: 0.643103  [  128/  130]
train() client id: f_00009-0-0 loss: 0.894309  [   32/  118]
train() client id: f_00009-0-1 loss: 1.003982  [   64/  118]
train() client id: f_00009-0-2 loss: 1.038288  [   96/  118]
train() client id: f_00009-1-0 loss: 0.925039  [   32/  118]
train() client id: f_00009-1-1 loss: 1.011340  [   64/  118]
train() client id: f_00009-1-2 loss: 0.745356  [   96/  118]
train() client id: f_00009-2-0 loss: 0.783148  [   32/  118]
train() client id: f_00009-2-1 loss: 1.013077  [   64/  118]
train() client id: f_00009-2-2 loss: 0.939182  [   96/  118]
train() client id: f_00009-3-0 loss: 0.931032  [   32/  118]
train() client id: f_00009-3-1 loss: 0.786229  [   64/  118]
train() client id: f_00009-3-2 loss: 0.783812  [   96/  118]
train() client id: f_00009-4-0 loss: 0.768070  [   32/  118]
train() client id: f_00009-4-1 loss: 0.864037  [   64/  118]
train() client id: f_00009-4-2 loss: 0.895042  [   96/  118]
train() client id: f_00009-5-0 loss: 0.821827  [   32/  118]
train() client id: f_00009-5-1 loss: 0.795169  [   64/  118]
train() client id: f_00009-5-2 loss: 0.744183  [   96/  118]
train() client id: f_00009-6-0 loss: 0.739052  [   32/  118]
train() client id: f_00009-6-1 loss: 0.831050  [   64/  118]
train() client id: f_00009-6-2 loss: 0.748484  [   96/  118]
train() client id: f_00009-7-0 loss: 0.839858  [   32/  118]
train() client id: f_00009-7-1 loss: 0.703268  [   64/  118]
train() client id: f_00009-7-2 loss: 0.707032  [   96/  118]
train() client id: f_00009-8-0 loss: 0.766910  [   32/  118]
train() client id: f_00009-8-1 loss: 0.731498  [   64/  118]
train() client id: f_00009-8-2 loss: 0.833448  [   96/  118]
train() client id: f_00009-9-0 loss: 0.892670  [   32/  118]
train() client id: f_00009-9-1 loss: 0.761605  [   64/  118]
train() client id: f_00009-9-2 loss: 0.690118  [   96/  118]
train() client id: f_00009-10-0 loss: 0.693050  [   32/  118]
train() client id: f_00009-10-1 loss: 0.814547  [   64/  118]
train() client id: f_00009-10-2 loss: 0.861631  [   96/  118]
train() client id: f_00009-11-0 loss: 0.864509  [   32/  118]
train() client id: f_00009-11-1 loss: 0.617570  [   64/  118]
train() client id: f_00009-11-2 loss: 0.818533  [   96/  118]
At round 34 accuracy: 0.6525198938992043
At round 34 training accuracy: 0.5881958417169685
At round 34 training loss: 0.832489502902182
update_location
xs = [  -3.9056584     4.20031788  190.00902392   18.81129433    0.97929623
    3.95640986 -152.44319194 -131.32485185  174.66397685 -117.06087855]
ys = [ 182.5879595   165.55583871    1.32061395 -152.45517586  144.35018685
  127.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [208.21531433 193.45898377 214.72115217 183.29333169 175.60733317
 162.33333086 182.33435582 165.066329   202.03013335 154.01058776]
dists_bs = [171.26252029 179.44900699 404.22285522 380.51243082 178.62845167
 185.07269113 179.05151394 179.56996694 383.47022645 180.54759768]
uav_gains = [1.51163930e-11 1.87414015e-11 1.37010772e-11 2.16985605e-11
 2.42798461e-11 2.96968280e-11 2.20025346e-11 2.84631267e-11
 1.65569782e-11 3.39218710e-11]
bs_gains = [6.15232911e-11 5.39832614e-11 5.55591589e-12 6.58053658e-12
 5.46804790e-11 4.95148575e-11 5.43194911e-11 5.38815050e-11
 6.43940092e-12 5.30685578e-11]
Round 35
-------------------------------
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.3715199  13.18889052  6.27703667  2.2632532  15.21119563  7.32046252
  2.80542146  8.96352054  6.61377748  5.93751329]
obj_prev = 74.95259119677627
eta_min = 3.0936410554122904e-15	eta_max = 0.9287856551033156
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 17.39350337614215	eta = 0.9090909090909091
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 31.811065698035446	eta = 0.4970684084145405
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 24.73112036289277	eta = 0.6393675484357765
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.451279643601314	eta = 0.6742606815832007
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.38394553843602	eta = 0.6762022161958229
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.38374385979249	eta = 0.6762080482621735
eta = 0.6762080482621735
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [0.03228538 0.0679018  0.03177292 0.01101803 0.07840736 0.03741005
 0.01383659 0.04586574 0.03331032 0.03023552]
ene_total = [2.0907422  3.75044408 2.07789445 0.9851544  4.27493166 2.23178708
 1.1246433  2.70086556 2.2793763  1.86790483]
ti_comp = [0.46966707 0.49397079 0.46711134 0.47805782 0.49415416 0.49271153
 0.47835241 0.48347539 0.44219867 0.49372516]
ti_coms = [0.09519389 0.07089017 0.09774963 0.08680315 0.07070681 0.07214943
 0.08650855 0.08138558 0.1226623  0.07113581]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [9.53493184e-06 8.01901942e-05 9.18777644e-06 3.65789049e-07
 1.23374545e-04 1.34790504e-05 7.23554964e-07 2.57986237e-05
 1.18135760e-05 7.08698796e-06]
ene_total = [0.47656101 0.35854674 0.48932539 0.4341386  0.35978946 0.36150821
 0.43268317 0.40831618 0.61405008 0.35611918]
optimize_network_iter = 0 obj = 4.291038018675944
eta = 0.6762080482621735
freqs = [34370497.98105174 68730577.98040815 34010002.78410535 11523739.23427672
 79334916.60468721 37963436.80656631 14462760.16316765 47433377.96218494
 37664422.59882781 30619785.51757285]
eta_min = 0.6762080482621772	eta_max = 0.6762080482621733
af = 0.011653184886415587	bf = 1.3848442141784587	zeta = 0.012818503375057147	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [2.32272370e-06 1.95344516e-05 2.23815612e-06 8.91067609e-08
 3.00542243e-05 3.28351691e-06 1.76259075e-07 6.28458343e-06
 2.87780483e-06 1.72640090e-06]
ene_total = [1.69608812 1.26623554 1.74159793 1.54622714 1.26484323 1.28577181
 1.54099513 1.45082842 2.18547748 1.2674389 ]
ti_comp = [0.46966707 0.49397079 0.46711134 0.47805782 0.49415416 0.49271153
 0.47835241 0.48347539 0.44219867 0.49372516]
ti_coms = [0.09519389 0.07089017 0.09774963 0.08680315 0.07070681 0.07214943
 0.08650855 0.08138558 0.1226623  0.07113581]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [9.53493184e-06 8.01901942e-05 9.18777644e-06 3.65789049e-07
 1.23374545e-04 1.34790504e-05 7.23554964e-07 2.57986237e-05
 1.18135760e-05 7.08698796e-06]
ene_total = [0.47656101 0.35854674 0.48932539 0.4341386  0.35978946 0.36150821
 0.43268317 0.40831618 0.61405008 0.35611918]
optimize_network_iter = 1 obj = 4.291038018675939
eta = 0.6762080482621733
freqs = [34370497.98105174 68730577.98040818 34010002.78410535 11523739.23427672
 79334916.60468724 37963436.80656631 14462760.16316766 47433377.96218494
 37664422.59882782 30619785.51757286]
Done!
At round 35 eta: 0.6762080482621733
At round 35 local rounds: 12.811659702043181
At round 35 global rounds: 50.012046702878195
At round 35 a_n: 15.850952365358953
gradient difference: 0.44332438707351685
train() client id: f_00000-0-0 loss: 1.013276  [   32/  126]
train() client id: f_00000-0-1 loss: 1.313080  [   64/  126]
train() client id: f_00000-0-2 loss: 1.180547  [   96/  126]
train() client id: f_00000-1-0 loss: 1.166467  [   32/  126]
train() client id: f_00000-1-1 loss: 1.061845  [   64/  126]
train() client id: f_00000-1-2 loss: 1.169649  [   96/  126]
train() client id: f_00000-2-0 loss: 1.130772  [   32/  126]
train() client id: f_00000-2-1 loss: 0.937875  [   64/  126]
train() client id: f_00000-2-2 loss: 1.009896  [   96/  126]
train() client id: f_00000-3-0 loss: 1.008219  [   32/  126]
train() client id: f_00000-3-1 loss: 0.875015  [   64/  126]
train() client id: f_00000-3-2 loss: 1.127017  [   96/  126]
train() client id: f_00000-4-0 loss: 0.948694  [   32/  126]
train() client id: f_00000-4-1 loss: 1.038662  [   64/  126]
train() client id: f_00000-4-2 loss: 0.942497  [   96/  126]
train() client id: f_00000-5-0 loss: 0.950999  [   32/  126]
train() client id: f_00000-5-1 loss: 1.004479  [   64/  126]
train() client id: f_00000-5-2 loss: 0.860632  [   96/  126]
train() client id: f_00000-6-0 loss: 0.980639  [   32/  126]
train() client id: f_00000-6-1 loss: 0.936398  [   64/  126]
train() client id: f_00000-6-2 loss: 0.861389  [   96/  126]
train() client id: f_00000-7-0 loss: 0.927950  [   32/  126]
train() client id: f_00000-7-1 loss: 1.017540  [   64/  126]
train() client id: f_00000-7-2 loss: 0.854754  [   96/  126]
train() client id: f_00000-8-0 loss: 0.842217  [   32/  126]
train() client id: f_00000-8-1 loss: 0.983833  [   64/  126]
train() client id: f_00000-8-2 loss: 0.951479  [   96/  126]
train() client id: f_00000-9-0 loss: 0.925235  [   32/  126]
train() client id: f_00000-9-1 loss: 0.987765  [   64/  126]
train() client id: f_00000-9-2 loss: 0.830711  [   96/  126]
train() client id: f_00000-10-0 loss: 0.944410  [   32/  126]
train() client id: f_00000-10-1 loss: 0.957656  [   64/  126]
train() client id: f_00000-10-2 loss: 0.840059  [   96/  126]
train() client id: f_00000-11-0 loss: 0.952558  [   32/  126]
train() client id: f_00000-11-1 loss: 0.888853  [   64/  126]
train() client id: f_00000-11-2 loss: 0.991875  [   96/  126]
train() client id: f_00001-0-0 loss: 0.514476  [   32/  265]
train() client id: f_00001-0-1 loss: 0.506609  [   64/  265]
train() client id: f_00001-0-2 loss: 0.451128  [   96/  265]
train() client id: f_00001-0-3 loss: 0.480613  [  128/  265]
train() client id: f_00001-0-4 loss: 0.595598  [  160/  265]
train() client id: f_00001-0-5 loss: 0.478241  [  192/  265]
train() client id: f_00001-0-6 loss: 0.491610  [  224/  265]
train() client id: f_00001-0-7 loss: 0.512474  [  256/  265]
train() client id: f_00001-1-0 loss: 0.449063  [   32/  265]
train() client id: f_00001-1-1 loss: 0.436360  [   64/  265]
train() client id: f_00001-1-2 loss: 0.521995  [   96/  265]
train() client id: f_00001-1-3 loss: 0.473955  [  128/  265]
train() client id: f_00001-1-4 loss: 0.494687  [  160/  265]
train() client id: f_00001-1-5 loss: 0.500934  [  192/  265]
train() client id: f_00001-1-6 loss: 0.554106  [  224/  265]
train() client id: f_00001-1-7 loss: 0.530700  [  256/  265]
train() client id: f_00001-2-0 loss: 0.496863  [   32/  265]
train() client id: f_00001-2-1 loss: 0.560147  [   64/  265]
train() client id: f_00001-2-2 loss: 0.541362  [   96/  265]
train() client id: f_00001-2-3 loss: 0.422613  [  128/  265]
train() client id: f_00001-2-4 loss: 0.522138  [  160/  265]
train() client id: f_00001-2-5 loss: 0.464069  [  192/  265]
train() client id: f_00001-2-6 loss: 0.417936  [  224/  265]
train() client id: f_00001-2-7 loss: 0.481802  [  256/  265]
train() client id: f_00001-3-0 loss: 0.385819  [   32/  265]
train() client id: f_00001-3-1 loss: 0.434995  [   64/  265]
train() client id: f_00001-3-2 loss: 0.504321  [   96/  265]
train() client id: f_00001-3-3 loss: 0.545575  [  128/  265]
train() client id: f_00001-3-4 loss: 0.429582  [  160/  265]
train() client id: f_00001-3-5 loss: 0.656340  [  192/  265]
train() client id: f_00001-3-6 loss: 0.410425  [  224/  265]
train() client id: f_00001-3-7 loss: 0.432743  [  256/  265]
train() client id: f_00001-4-0 loss: 0.483888  [   32/  265]
train() client id: f_00001-4-1 loss: 0.565716  [   64/  265]
train() client id: f_00001-4-2 loss: 0.521766  [   96/  265]
train() client id: f_00001-4-3 loss: 0.397689  [  128/  265]
train() client id: f_00001-4-4 loss: 0.539410  [  160/  265]
train() client id: f_00001-4-5 loss: 0.458532  [  192/  265]
train() client id: f_00001-4-6 loss: 0.527303  [  224/  265]
train() client id: f_00001-4-7 loss: 0.387379  [  256/  265]
train() client id: f_00001-5-0 loss: 0.575085  [   32/  265]
train() client id: f_00001-5-1 loss: 0.468737  [   64/  265]
train() client id: f_00001-5-2 loss: 0.484392  [   96/  265]
train() client id: f_00001-5-3 loss: 0.475030  [  128/  265]
train() client id: f_00001-5-4 loss: 0.389608  [  160/  265]
train() client id: f_00001-5-5 loss: 0.456416  [  192/  265]
train() client id: f_00001-5-6 loss: 0.363465  [  224/  265]
train() client id: f_00001-5-7 loss: 0.641143  [  256/  265]
train() client id: f_00001-6-0 loss: 0.445094  [   32/  265]
train() client id: f_00001-6-1 loss: 0.529318  [   64/  265]
train() client id: f_00001-6-2 loss: 0.454525  [   96/  265]
train() client id: f_00001-6-3 loss: 0.595684  [  128/  265]
train() client id: f_00001-6-4 loss: 0.506043  [  160/  265]
train() client id: f_00001-6-5 loss: 0.397369  [  192/  265]
train() client id: f_00001-6-6 loss: 0.497788  [  224/  265]
train() client id: f_00001-6-7 loss: 0.422683  [  256/  265]
train() client id: f_00001-7-0 loss: 0.492767  [   32/  265]
train() client id: f_00001-7-1 loss: 0.679975  [   64/  265]
train() client id: f_00001-7-2 loss: 0.456638  [   96/  265]
train() client id: f_00001-7-3 loss: 0.484010  [  128/  265]
train() client id: f_00001-7-4 loss: 0.435362  [  160/  265]
train() client id: f_00001-7-5 loss: 0.423433  [  192/  265]
train() client id: f_00001-7-6 loss: 0.437603  [  224/  265]
train() client id: f_00001-7-7 loss: 0.430269  [  256/  265]
train() client id: f_00001-8-0 loss: 0.612915  [   32/  265]
train() client id: f_00001-8-1 loss: 0.482171  [   64/  265]
train() client id: f_00001-8-2 loss: 0.554254  [   96/  265]
train() client id: f_00001-8-3 loss: 0.438646  [  128/  265]
train() client id: f_00001-8-4 loss: 0.390336  [  160/  265]
train() client id: f_00001-8-5 loss: 0.532010  [  192/  265]
train() client id: f_00001-8-6 loss: 0.423087  [  224/  265]
train() client id: f_00001-8-7 loss: 0.354533  [  256/  265]
train() client id: f_00001-9-0 loss: 0.591742  [   32/  265]
train() client id: f_00001-9-1 loss: 0.429754  [   64/  265]
train() client id: f_00001-9-2 loss: 0.575176  [   96/  265]
train() client id: f_00001-9-3 loss: 0.446181  [  128/  265]
train() client id: f_00001-9-4 loss: 0.392241  [  160/  265]
train() client id: f_00001-9-5 loss: 0.448514  [  192/  265]
train() client id: f_00001-9-6 loss: 0.502656  [  224/  265]
train() client id: f_00001-9-7 loss: 0.402886  [  256/  265]
train() client id: f_00001-10-0 loss: 0.394705  [   32/  265]
train() client id: f_00001-10-1 loss: 0.482042  [   64/  265]
train() client id: f_00001-10-2 loss: 0.624133  [   96/  265]
train() client id: f_00001-10-3 loss: 0.458264  [  128/  265]
train() client id: f_00001-10-4 loss: 0.503878  [  160/  265]
train() client id: f_00001-10-5 loss: 0.516855  [  192/  265]
train() client id: f_00001-10-6 loss: 0.445235  [  224/  265]
train() client id: f_00001-10-7 loss: 0.414197  [  256/  265]
train() client id: f_00001-11-0 loss: 0.466187  [   32/  265]
train() client id: f_00001-11-1 loss: 0.448994  [   64/  265]
train() client id: f_00001-11-2 loss: 0.524386  [   96/  265]
train() client id: f_00001-11-3 loss: 0.453311  [  128/  265]
train() client id: f_00001-11-4 loss: 0.419756  [  160/  265]
train() client id: f_00001-11-5 loss: 0.483717  [  192/  265]
train() client id: f_00001-11-6 loss: 0.522280  [  224/  265]
train() client id: f_00001-11-7 loss: 0.530363  [  256/  265]
train() client id: f_00002-0-0 loss: 1.270030  [   32/  124]
train() client id: f_00002-0-1 loss: 1.292641  [   64/  124]
train() client id: f_00002-0-2 loss: 1.242753  [   96/  124]
train() client id: f_00002-1-0 loss: 1.272637  [   32/  124]
train() client id: f_00002-1-1 loss: 1.261487  [   64/  124]
train() client id: f_00002-1-2 loss: 1.344662  [   96/  124]
train() client id: f_00002-2-0 loss: 1.316047  [   32/  124]
train() client id: f_00002-2-1 loss: 1.061438  [   64/  124]
train() client id: f_00002-2-2 loss: 1.267827  [   96/  124]
train() client id: f_00002-3-0 loss: 1.084528  [   32/  124]
train() client id: f_00002-3-1 loss: 1.301207  [   64/  124]
train() client id: f_00002-3-2 loss: 1.116973  [   96/  124]
train() client id: f_00002-4-0 loss: 1.134593  [   32/  124]
train() client id: f_00002-4-1 loss: 1.152325  [   64/  124]
train() client id: f_00002-4-2 loss: 1.129375  [   96/  124]
train() client id: f_00002-5-0 loss: 0.979228  [   32/  124]
train() client id: f_00002-5-1 loss: 1.208334  [   64/  124]
train() client id: f_00002-5-2 loss: 1.102469  [   96/  124]
train() client id: f_00002-6-0 loss: 1.118124  [   32/  124]
train() client id: f_00002-6-1 loss: 1.065037  [   64/  124]
train() client id: f_00002-6-2 loss: 1.169071  [   96/  124]
train() client id: f_00002-7-0 loss: 1.085637  [   32/  124]
train() client id: f_00002-7-1 loss: 1.122229  [   64/  124]
train() client id: f_00002-7-2 loss: 1.065528  [   96/  124]
train() client id: f_00002-8-0 loss: 0.966496  [   32/  124]
train() client id: f_00002-8-1 loss: 1.302800  [   64/  124]
train() client id: f_00002-8-2 loss: 0.900859  [   96/  124]
train() client id: f_00002-9-0 loss: 1.105447  [   32/  124]
train() client id: f_00002-9-1 loss: 1.087579  [   64/  124]
train() client id: f_00002-9-2 loss: 1.027063  [   96/  124]
train() client id: f_00002-10-0 loss: 1.130196  [   32/  124]
train() client id: f_00002-10-1 loss: 0.932629  [   64/  124]
train() client id: f_00002-10-2 loss: 1.039528  [   96/  124]
train() client id: f_00002-11-0 loss: 0.927584  [   32/  124]
train() client id: f_00002-11-1 loss: 1.072351  [   64/  124]
train() client id: f_00002-11-2 loss: 1.049993  [   96/  124]
train() client id: f_00003-0-0 loss: 0.688038  [   32/   43]
train() client id: f_00003-1-0 loss: 0.647375  [   32/   43]
train() client id: f_00003-2-0 loss: 0.506746  [   32/   43]
train() client id: f_00003-3-0 loss: 0.689496  [   32/   43]
train() client id: f_00003-4-0 loss: 0.668143  [   32/   43]
train() client id: f_00003-5-0 loss: 0.463501  [   32/   43]
train() client id: f_00003-6-0 loss: 0.599028  [   32/   43]
train() client id: f_00003-7-0 loss: 0.540194  [   32/   43]
train() client id: f_00003-8-0 loss: 0.530376  [   32/   43]
train() client id: f_00003-9-0 loss: 0.728500  [   32/   43]
train() client id: f_00003-10-0 loss: 0.585248  [   32/   43]
train() client id: f_00003-11-0 loss: 0.737473  [   32/   43]
train() client id: f_00004-0-0 loss: 0.993359  [   32/  306]
train() client id: f_00004-0-1 loss: 0.966173  [   64/  306]
train() client id: f_00004-0-2 loss: 0.977345  [   96/  306]
train() client id: f_00004-0-3 loss: 0.887689  [  128/  306]
train() client id: f_00004-0-4 loss: 0.972457  [  160/  306]
train() client id: f_00004-0-5 loss: 1.036310  [  192/  306]
train() client id: f_00004-0-6 loss: 0.931787  [  224/  306]
train() client id: f_00004-0-7 loss: 0.825329  [  256/  306]
train() client id: f_00004-0-8 loss: 0.842334  [  288/  306]
train() client id: f_00004-1-0 loss: 0.886089  [   32/  306]
train() client id: f_00004-1-1 loss: 0.940263  [   64/  306]
train() client id: f_00004-1-2 loss: 1.019370  [   96/  306]
train() client id: f_00004-1-3 loss: 0.898183  [  128/  306]
train() client id: f_00004-1-4 loss: 1.013641  [  160/  306]
train() client id: f_00004-1-5 loss: 0.869113  [  192/  306]
train() client id: f_00004-1-6 loss: 0.986251  [  224/  306]
train() client id: f_00004-1-7 loss: 0.943354  [  256/  306]
train() client id: f_00004-1-8 loss: 0.892899  [  288/  306]
train() client id: f_00004-2-0 loss: 1.034813  [   32/  306]
train() client id: f_00004-2-1 loss: 0.964468  [   64/  306]
train() client id: f_00004-2-2 loss: 0.996045  [   96/  306]
train() client id: f_00004-2-3 loss: 0.962663  [  128/  306]
train() client id: f_00004-2-4 loss: 0.792642  [  160/  306]
train() client id: f_00004-2-5 loss: 1.046539  [  192/  306]
train() client id: f_00004-2-6 loss: 0.828116  [  224/  306]
train() client id: f_00004-2-7 loss: 0.972145  [  256/  306]
train() client id: f_00004-2-8 loss: 0.878956  [  288/  306]
train() client id: f_00004-3-0 loss: 0.974490  [   32/  306]
train() client id: f_00004-3-1 loss: 0.869274  [   64/  306]
train() client id: f_00004-3-2 loss: 1.004556  [   96/  306]
train() client id: f_00004-3-3 loss: 0.934303  [  128/  306]
train() client id: f_00004-3-4 loss: 1.078572  [  160/  306]
train() client id: f_00004-3-5 loss: 0.906779  [  192/  306]
train() client id: f_00004-3-6 loss: 0.950500  [  224/  306]
train() client id: f_00004-3-7 loss: 0.797381  [  256/  306]
train() client id: f_00004-3-8 loss: 0.863147  [  288/  306]
train() client id: f_00004-4-0 loss: 0.941215  [   32/  306]
train() client id: f_00004-4-1 loss: 0.878476  [   64/  306]
train() client id: f_00004-4-2 loss: 0.964768  [   96/  306]
train() client id: f_00004-4-3 loss: 0.990832  [  128/  306]
train() client id: f_00004-4-4 loss: 0.834861  [  160/  306]
train() client id: f_00004-4-5 loss: 1.011221  [  192/  306]
train() client id: f_00004-4-6 loss: 1.052174  [  224/  306]
train() client id: f_00004-4-7 loss: 0.977633  [  256/  306]
train() client id: f_00004-4-8 loss: 0.763116  [  288/  306]
train() client id: f_00004-5-0 loss: 1.003943  [   32/  306]
train() client id: f_00004-5-1 loss: 0.872957  [   64/  306]
train() client id: f_00004-5-2 loss: 0.963372  [   96/  306]
train() client id: f_00004-5-3 loss: 0.940203  [  128/  306]
train() client id: f_00004-5-4 loss: 0.931950  [  160/  306]
train() client id: f_00004-5-5 loss: 0.904294  [  192/  306]
train() client id: f_00004-5-6 loss: 0.890868  [  224/  306]
train() client id: f_00004-5-7 loss: 1.023592  [  256/  306]
train() client id: f_00004-5-8 loss: 0.905970  [  288/  306]
train() client id: f_00004-6-0 loss: 0.976635  [   32/  306]
train() client id: f_00004-6-1 loss: 0.826201  [   64/  306]
train() client id: f_00004-6-2 loss: 1.091301  [   96/  306]
train() client id: f_00004-6-3 loss: 0.922218  [  128/  306]
train() client id: f_00004-6-4 loss: 0.896371  [  160/  306]
train() client id: f_00004-6-5 loss: 0.894564  [  192/  306]
train() client id: f_00004-6-6 loss: 0.868878  [  224/  306]
train() client id: f_00004-6-7 loss: 0.930663  [  256/  306]
train() client id: f_00004-6-8 loss: 0.997516  [  288/  306]
train() client id: f_00004-7-0 loss: 0.957814  [   32/  306]
train() client id: f_00004-7-1 loss: 0.907797  [   64/  306]
train() client id: f_00004-7-2 loss: 0.864873  [   96/  306]
train() client id: f_00004-7-3 loss: 0.966272  [  128/  306]
train() client id: f_00004-7-4 loss: 0.967849  [  160/  306]
train() client id: f_00004-7-5 loss: 0.871388  [  192/  306]
train() client id: f_00004-7-6 loss: 0.856226  [  224/  306]
train() client id: f_00004-7-7 loss: 0.932983  [  256/  306]
train() client id: f_00004-7-8 loss: 1.031378  [  288/  306]
train() client id: f_00004-8-0 loss: 0.956168  [   32/  306]
train() client id: f_00004-8-1 loss: 1.001861  [   64/  306]
train() client id: f_00004-8-2 loss: 0.938132  [   96/  306]
train() client id: f_00004-8-3 loss: 0.909318  [  128/  306]
train() client id: f_00004-8-4 loss: 0.855494  [  160/  306]
train() client id: f_00004-8-5 loss: 0.876893  [  192/  306]
train() client id: f_00004-8-6 loss: 0.940753  [  224/  306]
train() client id: f_00004-8-7 loss: 1.016301  [  256/  306]
train() client id: f_00004-8-8 loss: 0.978979  [  288/  306]
train() client id: f_00004-9-0 loss: 0.865617  [   32/  306]
train() client id: f_00004-9-1 loss: 0.857525  [   64/  306]
train() client id: f_00004-9-2 loss: 0.976195  [   96/  306]
train() client id: f_00004-9-3 loss: 0.978338  [  128/  306]
train() client id: f_00004-9-4 loss: 0.925077  [  160/  306]
train() client id: f_00004-9-5 loss: 0.953383  [  192/  306]
train() client id: f_00004-9-6 loss: 0.858492  [  224/  306]
train() client id: f_00004-9-7 loss: 0.982129  [  256/  306]
train() client id: f_00004-9-8 loss: 0.975026  [  288/  306]
train() client id: f_00004-10-0 loss: 1.128736  [   32/  306]
train() client id: f_00004-10-1 loss: 0.909386  [   64/  306]
train() client id: f_00004-10-2 loss: 0.883000  [   96/  306]
train() client id: f_00004-10-3 loss: 0.794742  [  128/  306]
train() client id: f_00004-10-4 loss: 1.002978  [  160/  306]
train() client id: f_00004-10-5 loss: 0.890085  [  192/  306]
train() client id: f_00004-10-6 loss: 0.965640  [  224/  306]
train() client id: f_00004-10-7 loss: 0.877181  [  256/  306]
train() client id: f_00004-10-8 loss: 0.914620  [  288/  306]
train() client id: f_00004-11-0 loss: 0.959283  [   32/  306]
train() client id: f_00004-11-1 loss: 0.938149  [   64/  306]
train() client id: f_00004-11-2 loss: 0.930557  [   96/  306]
train() client id: f_00004-11-3 loss: 0.947189  [  128/  306]
train() client id: f_00004-11-4 loss: 0.951373  [  160/  306]
train() client id: f_00004-11-5 loss: 1.015649  [  192/  306]
train() client id: f_00004-11-6 loss: 0.802984  [  224/  306]
train() client id: f_00004-11-7 loss: 0.936053  [  256/  306]
train() client id: f_00004-11-8 loss: 0.831109  [  288/  306]
train() client id: f_00005-0-0 loss: 0.417178  [   32/  146]
train() client id: f_00005-0-1 loss: 0.493744  [   64/  146]
train() client id: f_00005-0-2 loss: 0.566414  [   96/  146]
train() client id: f_00005-0-3 loss: 0.523037  [  128/  146]
train() client id: f_00005-1-0 loss: 0.248158  [   32/  146]
train() client id: f_00005-1-1 loss: 0.483143  [   64/  146]
train() client id: f_00005-1-2 loss: 0.892370  [   96/  146]
train() client id: f_00005-1-3 loss: 0.357642  [  128/  146]
train() client id: f_00005-2-0 loss: 0.672085  [   32/  146]
train() client id: f_00005-2-1 loss: 0.658649  [   64/  146]
train() client id: f_00005-2-2 loss: 0.195117  [   96/  146]
train() client id: f_00005-2-3 loss: 0.472347  [  128/  146]
train() client id: f_00005-3-0 loss: 0.387618  [   32/  146]
train() client id: f_00005-3-1 loss: 0.440121  [   64/  146]
train() client id: f_00005-3-2 loss: 0.683950  [   96/  146]
train() client id: f_00005-3-3 loss: 0.546964  [  128/  146]
train() client id: f_00005-4-0 loss: 0.508829  [   32/  146]
train() client id: f_00005-4-1 loss: 0.507951  [   64/  146]
train() client id: f_00005-4-2 loss: 0.404403  [   96/  146]
train() client id: f_00005-4-3 loss: 0.456251  [  128/  146]
train() client id: f_00005-5-0 loss: 0.628404  [   32/  146]
train() client id: f_00005-5-1 loss: 0.343482  [   64/  146]
train() client id: f_00005-5-2 loss: 0.407277  [   96/  146]
train() client id: f_00005-5-3 loss: 0.519177  [  128/  146]
train() client id: f_00005-6-0 loss: 0.414607  [   32/  146]
train() client id: f_00005-6-1 loss: 0.605886  [   64/  146]
train() client id: f_00005-6-2 loss: 0.628254  [   96/  146]
train() client id: f_00005-6-3 loss: 0.335133  [  128/  146]
train() client id: f_00005-7-0 loss: 0.458005  [   32/  146]
train() client id: f_00005-7-1 loss: 0.648376  [   64/  146]
train() client id: f_00005-7-2 loss: 0.401995  [   96/  146]
train() client id: f_00005-7-3 loss: 0.431035  [  128/  146]
train() client id: f_00005-8-0 loss: 0.606351  [   32/  146]
train() client id: f_00005-8-1 loss: 0.417776  [   64/  146]
train() client id: f_00005-8-2 loss: 0.573127  [   96/  146]
train() client id: f_00005-8-3 loss: 0.542917  [  128/  146]
train() client id: f_00005-9-0 loss: 0.413462  [   32/  146]
train() client id: f_00005-9-1 loss: 0.445079  [   64/  146]
train() client id: f_00005-9-2 loss: 0.525943  [   96/  146]
train() client id: f_00005-9-3 loss: 0.501291  [  128/  146]
train() client id: f_00005-10-0 loss: 0.497958  [   32/  146]
train() client id: f_00005-10-1 loss: 0.871658  [   64/  146]
train() client id: f_00005-10-2 loss: 0.389154  [   96/  146]
train() client id: f_00005-10-3 loss: 0.340034  [  128/  146]
train() client id: f_00005-11-0 loss: 0.502443  [   32/  146]
train() client id: f_00005-11-1 loss: 0.459222  [   64/  146]
train() client id: f_00005-11-2 loss: 0.563255  [   96/  146]
train() client id: f_00005-11-3 loss: 0.426614  [  128/  146]
train() client id: f_00006-0-0 loss: 0.493453  [   32/   54]
train() client id: f_00006-1-0 loss: 0.514353  [   32/   54]
train() client id: f_00006-2-0 loss: 0.458356  [   32/   54]
train() client id: f_00006-3-0 loss: 0.511519  [   32/   54]
train() client id: f_00006-4-0 loss: 0.517741  [   32/   54]
train() client id: f_00006-5-0 loss: 0.513454  [   32/   54]
train() client id: f_00006-6-0 loss: 0.471605  [   32/   54]
train() client id: f_00006-7-0 loss: 0.557648  [   32/   54]
train() client id: f_00006-8-0 loss: 0.530087  [   32/   54]
train() client id: f_00006-9-0 loss: 0.553178  [   32/   54]
train() client id: f_00006-10-0 loss: 0.446142  [   32/   54]
train() client id: f_00006-11-0 loss: 0.527601  [   32/   54]
train() client id: f_00007-0-0 loss: 0.537449  [   32/  179]
train() client id: f_00007-0-1 loss: 0.573335  [   64/  179]
train() client id: f_00007-0-2 loss: 0.777897  [   96/  179]
train() client id: f_00007-0-3 loss: 0.700216  [  128/  179]
train() client id: f_00007-0-4 loss: 0.655764  [  160/  179]
train() client id: f_00007-1-0 loss: 0.487062  [   32/  179]
train() client id: f_00007-1-1 loss: 0.864860  [   64/  179]
train() client id: f_00007-1-2 loss: 0.751350  [   96/  179]
train() client id: f_00007-1-3 loss: 0.575390  [  128/  179]
train() client id: f_00007-1-4 loss: 0.522147  [  160/  179]
train() client id: f_00007-2-0 loss: 0.685859  [   32/  179]
train() client id: f_00007-2-1 loss: 0.664195  [   64/  179]
train() client id: f_00007-2-2 loss: 0.715299  [   96/  179]
train() client id: f_00007-2-3 loss: 0.497415  [  128/  179]
train() client id: f_00007-2-4 loss: 0.583549  [  160/  179]
train() client id: f_00007-3-0 loss: 0.501268  [   32/  179]
train() client id: f_00007-3-1 loss: 0.875779  [   64/  179]
train() client id: f_00007-3-2 loss: 0.619026  [   96/  179]
train() client id: f_00007-3-3 loss: 0.520062  [  128/  179]
train() client id: f_00007-3-4 loss: 0.631884  [  160/  179]
train() client id: f_00007-4-0 loss: 0.731602  [   32/  179]
train() client id: f_00007-4-1 loss: 0.543885  [   64/  179]
train() client id: f_00007-4-2 loss: 0.683972  [   96/  179]
train() client id: f_00007-4-3 loss: 0.664356  [  128/  179]
train() client id: f_00007-4-4 loss: 0.483934  [  160/  179]
train() client id: f_00007-5-0 loss: 0.719253  [   32/  179]
train() client id: f_00007-5-1 loss: 0.642806  [   64/  179]
train() client id: f_00007-5-2 loss: 0.522920  [   96/  179]
train() client id: f_00007-5-3 loss: 0.533300  [  128/  179]
train() client id: f_00007-5-4 loss: 0.673050  [  160/  179]
train() client id: f_00007-6-0 loss: 0.642374  [   32/  179]
train() client id: f_00007-6-1 loss: 0.959010  [   64/  179]
train() client id: f_00007-6-2 loss: 0.486077  [   96/  179]
train() client id: f_00007-6-3 loss: 0.609619  [  128/  179]
train() client id: f_00007-6-4 loss: 0.433296  [  160/  179]
train() client id: f_00007-7-0 loss: 0.459318  [   32/  179]
train() client id: f_00007-7-1 loss: 0.580582  [   64/  179]
train() client id: f_00007-7-2 loss: 0.636949  [   96/  179]
train() client id: f_00007-7-3 loss: 0.567425  [  128/  179]
train() client id: f_00007-7-4 loss: 0.724779  [  160/  179]
train() client id: f_00007-8-0 loss: 0.470771  [   32/  179]
train() client id: f_00007-8-1 loss: 0.445416  [   64/  179]
train() client id: f_00007-8-2 loss: 0.605447  [   96/  179]
train() client id: f_00007-8-3 loss: 0.701402  [  128/  179]
train() client id: f_00007-8-4 loss: 0.701921  [  160/  179]
train() client id: f_00007-9-0 loss: 0.683490  [   32/  179]
train() client id: f_00007-9-1 loss: 0.557687  [   64/  179]
train() client id: f_00007-9-2 loss: 0.592846  [   96/  179]
train() client id: f_00007-9-3 loss: 0.723375  [  128/  179]
train() client id: f_00007-9-4 loss: 0.568007  [  160/  179]
train() client id: f_00007-10-0 loss: 0.451712  [   32/  179]
train() client id: f_00007-10-1 loss: 0.707626  [   64/  179]
train() client id: f_00007-10-2 loss: 0.438143  [   96/  179]
train() client id: f_00007-10-3 loss: 0.661037  [  128/  179]
train() client id: f_00007-10-4 loss: 0.668761  [  160/  179]
train() client id: f_00007-11-0 loss: 0.616857  [   32/  179]
train() client id: f_00007-11-1 loss: 0.490019  [   64/  179]
train() client id: f_00007-11-2 loss: 0.439679  [   96/  179]
train() client id: f_00007-11-3 loss: 0.900926  [  128/  179]
train() client id: f_00007-11-4 loss: 0.577967  [  160/  179]
train() client id: f_00008-0-0 loss: 0.687284  [   32/  130]
train() client id: f_00008-0-1 loss: 0.739471  [   64/  130]
train() client id: f_00008-0-2 loss: 0.817039  [   96/  130]
train() client id: f_00008-0-3 loss: 0.759136  [  128/  130]
train() client id: f_00008-1-0 loss: 0.804887  [   32/  130]
train() client id: f_00008-1-1 loss: 0.738083  [   64/  130]
train() client id: f_00008-1-2 loss: 0.795853  [   96/  130]
train() client id: f_00008-1-3 loss: 0.666525  [  128/  130]
train() client id: f_00008-2-0 loss: 0.870014  [   32/  130]
train() client id: f_00008-2-1 loss: 0.773731  [   64/  130]
train() client id: f_00008-2-2 loss: 0.742696  [   96/  130]
train() client id: f_00008-2-3 loss: 0.584451  [  128/  130]
train() client id: f_00008-3-0 loss: 0.713964  [   32/  130]
train() client id: f_00008-3-1 loss: 0.760271  [   64/  130]
train() client id: f_00008-3-2 loss: 0.798954  [   96/  130]
train() client id: f_00008-3-3 loss: 0.727745  [  128/  130]
train() client id: f_00008-4-0 loss: 0.732237  [   32/  130]
train() client id: f_00008-4-1 loss: 0.717861  [   64/  130]
train() client id: f_00008-4-2 loss: 0.831253  [   96/  130]
train() client id: f_00008-4-3 loss: 0.685302  [  128/  130]
train() client id: f_00008-5-0 loss: 0.744946  [   32/  130]
train() client id: f_00008-5-1 loss: 0.709267  [   64/  130]
train() client id: f_00008-5-2 loss: 0.732544  [   96/  130]
train() client id: f_00008-5-3 loss: 0.784668  [  128/  130]
train() client id: f_00008-6-0 loss: 0.775779  [   32/  130]
train() client id: f_00008-6-1 loss: 0.843973  [   64/  130]
train() client id: f_00008-6-2 loss: 0.662298  [   96/  130]
train() client id: f_00008-6-3 loss: 0.687358  [  128/  130]
train() client id: f_00008-7-0 loss: 0.623952  [   32/  130]
train() client id: f_00008-7-1 loss: 0.755537  [   64/  130]
train() client id: f_00008-7-2 loss: 0.767998  [   96/  130]
train() client id: f_00008-7-3 loss: 0.828989  [  128/  130]
train() client id: f_00008-8-0 loss: 0.799318  [   32/  130]
train() client id: f_00008-8-1 loss: 0.639499  [   64/  130]
train() client id: f_00008-8-2 loss: 0.738771  [   96/  130]
train() client id: f_00008-8-3 loss: 0.763422  [  128/  130]
train() client id: f_00008-9-0 loss: 0.750163  [   32/  130]
train() client id: f_00008-9-1 loss: 0.779238  [   64/  130]
train() client id: f_00008-9-2 loss: 0.720021  [   96/  130]
train() client id: f_00008-9-3 loss: 0.700218  [  128/  130]
train() client id: f_00008-10-0 loss: 0.742265  [   32/  130]
train() client id: f_00008-10-1 loss: 0.677005  [   64/  130]
train() client id: f_00008-10-2 loss: 0.724230  [   96/  130]
train() client id: f_00008-10-3 loss: 0.821344  [  128/  130]
train() client id: f_00008-11-0 loss: 0.728041  [   32/  130]
train() client id: f_00008-11-1 loss: 0.793501  [   64/  130]
train() client id: f_00008-11-2 loss: 0.699489  [   96/  130]
train() client id: f_00008-11-3 loss: 0.687520  [  128/  130]
train() client id: f_00009-0-0 loss: 1.151306  [   32/  118]
train() client id: f_00009-0-1 loss: 0.801019  [   64/  118]
train() client id: f_00009-0-2 loss: 0.913778  [   96/  118]
train() client id: f_00009-1-0 loss: 0.963927  [   32/  118]
train() client id: f_00009-1-1 loss: 0.925740  [   64/  118]
train() client id: f_00009-1-2 loss: 0.913657  [   96/  118]
train() client id: f_00009-2-0 loss: 0.843189  [   32/  118]
train() client id: f_00009-2-1 loss: 0.882891  [   64/  118]
train() client id: f_00009-2-2 loss: 0.756960  [   96/  118]
train() client id: f_00009-3-0 loss: 0.966785  [   32/  118]
train() client id: f_00009-3-1 loss: 0.862694  [   64/  118]
train() client id: f_00009-3-2 loss: 0.780565  [   96/  118]
train() client id: f_00009-4-0 loss: 0.882496  [   32/  118]
train() client id: f_00009-4-1 loss: 0.797898  [   64/  118]
train() client id: f_00009-4-2 loss: 0.838823  [   96/  118]
train() client id: f_00009-5-0 loss: 0.773955  [   32/  118]
train() client id: f_00009-5-1 loss: 0.847653  [   64/  118]
train() client id: f_00009-5-2 loss: 0.786542  [   96/  118]
train() client id: f_00009-6-0 loss: 0.860434  [   32/  118]
train() client id: f_00009-6-1 loss: 0.792882  [   64/  118]
train() client id: f_00009-6-2 loss: 0.724302  [   96/  118]
train() client id: f_00009-7-0 loss: 0.753434  [   32/  118]
train() client id: f_00009-7-1 loss: 0.907489  [   64/  118]
train() client id: f_00009-7-2 loss: 0.684429  [   96/  118]
train() client id: f_00009-8-0 loss: 0.786419  [   32/  118]
train() client id: f_00009-8-1 loss: 0.640826  [   64/  118]
train() client id: f_00009-8-2 loss: 0.678521  [   96/  118]
train() client id: f_00009-9-0 loss: 0.690989  [   32/  118]
train() client id: f_00009-9-1 loss: 0.920015  [   64/  118]
train() client id: f_00009-9-2 loss: 0.672507  [   96/  118]
train() client id: f_00009-10-0 loss: 0.754149  [   32/  118]
train() client id: f_00009-10-1 loss: 0.735483  [   64/  118]
train() client id: f_00009-10-2 loss: 0.754670  [   96/  118]
train() client id: f_00009-11-0 loss: 0.866061  [   32/  118]
train() client id: f_00009-11-1 loss: 0.714718  [   64/  118]
train() client id: f_00009-11-2 loss: 0.745999  [   96/  118]
At round 35 accuracy: 0.6525198938992043
At round 35 training accuracy: 0.5848423876592891
At round 35 training loss: 0.8348462370639675
update_location
xs = [  -3.9056584     4.20031788  195.00902392   18.81129433    0.97929623
    3.95640986 -157.44319194 -136.32485185  179.66397685 -122.06087855]
ys = [ 187.5879595   170.55583871    1.32061395 -157.45517586  149.35018685
  132.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [212.61349138 197.75473898 219.15807863 187.47265721 179.7399158
 166.29868255 186.53484723 169.07140943 206.36815295 157.84444852]
dists_bs = [171.55678491 179.25541692 408.74344878 384.8236762  177.83876297
 183.86147641 178.49055035 178.41979023 388.03486227 179.00570888]
uav_gains = [1.41493817e-11 1.76163185e-11 1.27867778e-11 2.04271955e-11
 2.28496909e-11 2.79287797e-11 2.07052398e-11 2.67731890e-11
 1.55361629e-11 3.18816260e-11]
bs_gains = [6.12282672e-11 5.41466606e-11 5.38557194e-12 6.37618765e-12
 5.53630598e-11 5.04336040e-11 5.47988501e-11 5.48597238e-11
 6.22954026e-12 5.43584189e-11]
Round 36
-------------------------------
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.2394703  12.90983275  6.14725815  2.21746141 14.88914632  7.16517573
  2.74816098  8.77577622  6.47605413  5.8113634 ]
obj_prev = 73.37969937876653
eta_min = 1.5469793420595954e-15	eta_max = 0.9294112566599255
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 17.025573465777978	eta = 0.9090909090909091
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 31.275397678211032	eta = 0.4948872023642162
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 24.26289815788516	eta = 0.6379202500492736
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.99478274194065	eta = 0.6731002520657826
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.927768157082628	eta = 0.6750676277672019
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.927565477060664	eta = 0.6750735953751348
eta = 0.6750735953751348
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [0.03242394 0.0681932  0.03190927 0.01106531 0.07874384 0.03757059
 0.01389597 0.04606258 0.03345327 0.03036527]
ene_total = [2.05444491 3.67229692 2.04263567 0.969603   4.18544742 2.18345709
 1.10625446 2.64978349 2.23698535 1.82665715]
ti_comp = [0.4811663  0.50721776 0.47844041 0.48996025 0.50753423 0.50618685
 0.49025487 0.4955151  0.45404092 0.50727357]
ti_coms = [0.09689836 0.07084691 0.09962426 0.08810442 0.07053044 0.07187782
 0.08780979 0.08254957 0.12402375 0.0707911 ]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [9.20209687e-06 7.70396484e-05 8.87104805e-06 3.52735562e-07
 1.18467487e-04 1.29360722e-05 6.97755096e-07 2.48777379e-05
 1.13502559e-05 6.80028536e-06]
ene_total = [0.47315021 0.34937218 0.48643187 0.42981882 0.34984932 0.35127413
 0.42839839 0.40391691 0.60558136 0.34567345]
optimize_network_iter = 0 obj = 4.223466627519707
eta = 0.6750735953751348
freqs = [33693065.79010019 67222802.04491435 33347173.22625211 11292050.13707799
 77574909.50601687 37111387.37372465 14172192.11427163 46479487.54198894
 36839485.23777357 29929879.4291095 ]
eta_min = 0.6750735953751431	eta_max = 0.675073595375134
af = 0.01091581578850546	bf = 1.3680266359327833	zeta = 0.012007397367356006	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [2.23206564e-06 1.86867792e-05 2.15176626e-06 8.55597305e-08
 2.87355384e-05 3.13778074e-06 1.69247857e-07 6.03435770e-06
 2.75312427e-06 1.64948093e-06]
ene_total = [1.68991368 1.23854826 1.73742856 1.53620773 1.23478239 1.25381221
 1.53108527 1.44039041 2.16296411 1.23460465]
ti_comp = [0.4811663  0.50721776 0.47844041 0.48996025 0.50753423 0.50618685
 0.49025487 0.4955151  0.45404092 0.50727357]
ti_coms = [0.09689836 0.07084691 0.09962426 0.08810442 0.07053044 0.07187782
 0.08780979 0.08254957 0.12402375 0.0707911 ]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [9.20209687e-06 7.70396484e-05 8.87104805e-06 3.52735562e-07
 1.18467487e-04 1.29360722e-05 6.97755096e-07 2.48777379e-05
 1.13502559e-05 6.80028536e-06]
ene_total = [0.47315021 0.34937218 0.48643187 0.42981882 0.34984932 0.35127413
 0.42839839 0.40391691 0.60558136 0.34567345]
optimize_network_iter = 1 obj = 4.223466627519696
eta = 0.675073595375134
freqs = [33693065.7901002  67222802.04491436 33347173.22625211 11292050.13707799
 77574909.50601688 37111387.37372466 14172192.11427163 46479487.54198895
 36839485.23777357 29929879.42910951]
Done!
At round 36 eta: 0.675073595375134
At round 36 local rounds: 12.866641222936334
At round 36 global rounds: 48.78320795030245
At round 36 a_n: 15.508406518389636
gradient difference: 0.4570787847042084
train() client id: f_00000-0-0 loss: 1.104125  [   32/  126]
train() client id: f_00000-0-1 loss: 1.112256  [   64/  126]
train() client id: f_00000-0-2 loss: 1.472476  [   96/  126]
train() client id: f_00000-1-0 loss: 1.104355  [   32/  126]
train() client id: f_00000-1-1 loss: 1.095741  [   64/  126]
train() client id: f_00000-1-2 loss: 1.160421  [   96/  126]
train() client id: f_00000-2-0 loss: 1.036399  [   32/  126]
train() client id: f_00000-2-1 loss: 1.152457  [   64/  126]
train() client id: f_00000-2-2 loss: 0.999790  [   96/  126]
train() client id: f_00000-3-0 loss: 0.991903  [   32/  126]
train() client id: f_00000-3-1 loss: 0.864987  [   64/  126]
train() client id: f_00000-3-2 loss: 1.023386  [   96/  126]
train() client id: f_00000-4-0 loss: 0.878233  [   32/  126]
train() client id: f_00000-4-1 loss: 0.994212  [   64/  126]
train() client id: f_00000-4-2 loss: 0.957446  [   96/  126]
train() client id: f_00000-5-0 loss: 0.809307  [   32/  126]
train() client id: f_00000-5-1 loss: 0.847758  [   64/  126]
train() client id: f_00000-5-2 loss: 0.981264  [   96/  126]
train() client id: f_00000-6-0 loss: 0.917489  [   32/  126]
train() client id: f_00000-6-1 loss: 0.890640  [   64/  126]
train() client id: f_00000-6-2 loss: 0.869572  [   96/  126]
train() client id: f_00000-7-0 loss: 0.806096  [   32/  126]
train() client id: f_00000-7-1 loss: 0.896351  [   64/  126]
train() client id: f_00000-7-2 loss: 0.741095  [   96/  126]
train() client id: f_00000-8-0 loss: 0.766716  [   32/  126]
train() client id: f_00000-8-1 loss: 0.943670  [   64/  126]
train() client id: f_00000-8-2 loss: 0.747461  [   96/  126]
train() client id: f_00000-9-0 loss: 0.752856  [   32/  126]
train() client id: f_00000-9-1 loss: 0.865504  [   64/  126]
train() client id: f_00000-9-2 loss: 0.846131  [   96/  126]
train() client id: f_00000-10-0 loss: 0.757132  [   32/  126]
train() client id: f_00000-10-1 loss: 0.740753  [   64/  126]
train() client id: f_00000-10-2 loss: 0.882679  [   96/  126]
train() client id: f_00000-11-0 loss: 0.866838  [   32/  126]
train() client id: f_00000-11-1 loss: 0.742504  [   64/  126]
train() client id: f_00000-11-2 loss: 0.714691  [   96/  126]
train() client id: f_00001-0-0 loss: 0.358416  [   32/  265]
train() client id: f_00001-0-1 loss: 0.471589  [   64/  265]
train() client id: f_00001-0-2 loss: 0.524581  [   96/  265]
train() client id: f_00001-0-3 loss: 0.556420  [  128/  265]
train() client id: f_00001-0-4 loss: 0.406377  [  160/  265]
train() client id: f_00001-0-5 loss: 0.441153  [  192/  265]
train() client id: f_00001-0-6 loss: 0.389767  [  224/  265]
train() client id: f_00001-0-7 loss: 0.333690  [  256/  265]
train() client id: f_00001-1-0 loss: 0.405839  [   32/  265]
train() client id: f_00001-1-1 loss: 0.500596  [   64/  265]
train() client id: f_00001-1-2 loss: 0.395821  [   96/  265]
train() client id: f_00001-1-3 loss: 0.423405  [  128/  265]
train() client id: f_00001-1-4 loss: 0.324889  [  160/  265]
train() client id: f_00001-1-5 loss: 0.482852  [  192/  265]
train() client id: f_00001-1-6 loss: 0.433211  [  224/  265]
train() client id: f_00001-1-7 loss: 0.451137  [  256/  265]
train() client id: f_00001-2-0 loss: 0.345281  [   32/  265]
train() client id: f_00001-2-1 loss: 0.428961  [   64/  265]
train() client id: f_00001-2-2 loss: 0.561823  [   96/  265]
train() client id: f_00001-2-3 loss: 0.344527  [  128/  265]
train() client id: f_00001-2-4 loss: 0.386532  [  160/  265]
train() client id: f_00001-2-5 loss: 0.443792  [  192/  265]
train() client id: f_00001-2-6 loss: 0.381695  [  224/  265]
train() client id: f_00001-2-7 loss: 0.381694  [  256/  265]
train() client id: f_00001-3-0 loss: 0.485918  [   32/  265]
train() client id: f_00001-3-1 loss: 0.335830  [   64/  265]
train() client id: f_00001-3-2 loss: 0.388386  [   96/  265]
train() client id: f_00001-3-3 loss: 0.415593  [  128/  265]
train() client id: f_00001-3-4 loss: 0.327445  [  160/  265]
train() client id: f_00001-3-5 loss: 0.528850  [  192/  265]
train() client id: f_00001-3-6 loss: 0.490833  [  224/  265]
train() client id: f_00001-3-7 loss: 0.366158  [  256/  265]
train() client id: f_00001-4-0 loss: 0.314115  [   32/  265]
train() client id: f_00001-4-1 loss: 0.411437  [   64/  265]
train() client id: f_00001-4-2 loss: 0.388374  [   96/  265]
train() client id: f_00001-4-3 loss: 0.484294  [  128/  265]
train() client id: f_00001-4-4 loss: 0.527673  [  160/  265]
train() client id: f_00001-4-5 loss: 0.358300  [  192/  265]
train() client id: f_00001-4-6 loss: 0.350560  [  224/  265]
train() client id: f_00001-4-7 loss: 0.473216  [  256/  265]
train() client id: f_00001-5-0 loss: 0.442527  [   32/  265]
train() client id: f_00001-5-1 loss: 0.449248  [   64/  265]
train() client id: f_00001-5-2 loss: 0.391026  [   96/  265]
train() client id: f_00001-5-3 loss: 0.331798  [  128/  265]
train() client id: f_00001-5-4 loss: 0.396018  [  160/  265]
train() client id: f_00001-5-5 loss: 0.314882  [  192/  265]
train() client id: f_00001-5-6 loss: 0.501723  [  224/  265]
train() client id: f_00001-5-7 loss: 0.344118  [  256/  265]
train() client id: f_00001-6-0 loss: 0.378210  [   32/  265]
train() client id: f_00001-6-1 loss: 0.388747  [   64/  265]
train() client id: f_00001-6-2 loss: 0.354959  [   96/  265]
train() client id: f_00001-6-3 loss: 0.414791  [  128/  265]
train() client id: f_00001-6-4 loss: 0.454590  [  160/  265]
train() client id: f_00001-6-5 loss: 0.427344  [  192/  265]
train() client id: f_00001-6-6 loss: 0.392094  [  224/  265]
train() client id: f_00001-6-7 loss: 0.398890  [  256/  265]
train() client id: f_00001-7-0 loss: 0.482991  [   32/  265]
train() client id: f_00001-7-1 loss: 0.347783  [   64/  265]
train() client id: f_00001-7-2 loss: 0.418918  [   96/  265]
train() client id: f_00001-7-3 loss: 0.403760  [  128/  265]
train() client id: f_00001-7-4 loss: 0.320189  [  160/  265]
train() client id: f_00001-7-5 loss: 0.307852  [  192/  265]
train() client id: f_00001-7-6 loss: 0.464984  [  224/  265]
train() client id: f_00001-7-7 loss: 0.471568  [  256/  265]
train() client id: f_00001-8-0 loss: 0.364767  [   32/  265]
train() client id: f_00001-8-1 loss: 0.367287  [   64/  265]
train() client id: f_00001-8-2 loss: 0.482558  [   96/  265]
train() client id: f_00001-8-3 loss: 0.566180  [  128/  265]
train() client id: f_00001-8-4 loss: 0.420093  [  160/  265]
train() client id: f_00001-8-5 loss: 0.440371  [  192/  265]
train() client id: f_00001-8-6 loss: 0.319129  [  224/  265]
train() client id: f_00001-8-7 loss: 0.311245  [  256/  265]
train() client id: f_00001-9-0 loss: 0.424011  [   32/  265]
train() client id: f_00001-9-1 loss: 0.317129  [   64/  265]
train() client id: f_00001-9-2 loss: 0.435675  [   96/  265]
train() client id: f_00001-9-3 loss: 0.441412  [  128/  265]
train() client id: f_00001-9-4 loss: 0.307542  [  160/  265]
train() client id: f_00001-9-5 loss: 0.385312  [  192/  265]
train() client id: f_00001-9-6 loss: 0.556816  [  224/  265]
train() client id: f_00001-9-7 loss: 0.403159  [  256/  265]
train() client id: f_00001-10-0 loss: 0.411470  [   32/  265]
train() client id: f_00001-10-1 loss: 0.491279  [   64/  265]
train() client id: f_00001-10-2 loss: 0.485081  [   96/  265]
train() client id: f_00001-10-3 loss: 0.421624  [  128/  265]
train() client id: f_00001-10-4 loss: 0.336495  [  160/  265]
train() client id: f_00001-10-5 loss: 0.356808  [  192/  265]
train() client id: f_00001-10-6 loss: 0.346717  [  224/  265]
train() client id: f_00001-10-7 loss: 0.428180  [  256/  265]
train() client id: f_00001-11-0 loss: 0.402244  [   32/  265]
train() client id: f_00001-11-1 loss: 0.423754  [   64/  265]
train() client id: f_00001-11-2 loss: 0.592546  [   96/  265]
train() client id: f_00001-11-3 loss: 0.366090  [  128/  265]
train() client id: f_00001-11-4 loss: 0.414911  [  160/  265]
train() client id: f_00001-11-5 loss: 0.316160  [  192/  265]
train() client id: f_00001-11-6 loss: 0.393190  [  224/  265]
train() client id: f_00001-11-7 loss: 0.304520  [  256/  265]
train() client id: f_00002-0-0 loss: 1.066866  [   32/  124]
train() client id: f_00002-0-1 loss: 1.195548  [   64/  124]
train() client id: f_00002-0-2 loss: 1.162503  [   96/  124]
train() client id: f_00002-1-0 loss: 1.374131  [   32/  124]
train() client id: f_00002-1-1 loss: 0.975813  [   64/  124]
train() client id: f_00002-1-2 loss: 0.964047  [   96/  124]
train() client id: f_00002-2-0 loss: 1.215309  [   32/  124]
train() client id: f_00002-2-1 loss: 1.099190  [   64/  124]
train() client id: f_00002-2-2 loss: 0.963700  [   96/  124]
train() client id: f_00002-3-0 loss: 0.945959  [   32/  124]
train() client id: f_00002-3-1 loss: 1.099751  [   64/  124]
train() client id: f_00002-3-2 loss: 1.108718  [   96/  124]
train() client id: f_00002-4-0 loss: 1.197714  [   32/  124]
train() client id: f_00002-4-1 loss: 0.870658  [   64/  124]
train() client id: f_00002-4-2 loss: 0.992226  [   96/  124]
train() client id: f_00002-5-0 loss: 1.044782  [   32/  124]
train() client id: f_00002-5-1 loss: 1.055486  [   64/  124]
train() client id: f_00002-5-2 loss: 0.836205  [   96/  124]
train() client id: f_00002-6-0 loss: 0.882422  [   32/  124]
train() client id: f_00002-6-1 loss: 1.008029  [   64/  124]
train() client id: f_00002-6-2 loss: 0.972649  [   96/  124]
train() client id: f_00002-7-0 loss: 0.934760  [   32/  124]
train() client id: f_00002-7-1 loss: 0.980466  [   64/  124]
train() client id: f_00002-7-2 loss: 0.890768  [   96/  124]
train() client id: f_00002-8-0 loss: 0.899042  [   32/  124]
train() client id: f_00002-8-1 loss: 0.883926  [   64/  124]
train() client id: f_00002-8-2 loss: 1.049455  [   96/  124]
train() client id: f_00002-9-0 loss: 1.183182  [   32/  124]
train() client id: f_00002-9-1 loss: 0.852861  [   64/  124]
train() client id: f_00002-9-2 loss: 0.750366  [   96/  124]
train() client id: f_00002-10-0 loss: 0.888368  [   32/  124]
train() client id: f_00002-10-1 loss: 0.821828  [   64/  124]
train() client id: f_00002-10-2 loss: 1.060974  [   96/  124]
train() client id: f_00002-11-0 loss: 1.070838  [   32/  124]
train() client id: f_00002-11-1 loss: 0.930651  [   64/  124]
train() client id: f_00002-11-2 loss: 0.863041  [   96/  124]
train() client id: f_00003-0-0 loss: 0.672311  [   32/   43]
train() client id: f_00003-1-0 loss: 0.813702  [   32/   43]
train() client id: f_00003-2-0 loss: 0.646344  [   32/   43]
train() client id: f_00003-3-0 loss: 0.444067  [   32/   43]
train() client id: f_00003-4-0 loss: 0.518103  [   32/   43]
train() client id: f_00003-5-0 loss: 0.472422  [   32/   43]
train() client id: f_00003-6-0 loss: 0.503603  [   32/   43]
train() client id: f_00003-7-0 loss: 0.577032  [   32/   43]
train() client id: f_00003-8-0 loss: 0.620321  [   32/   43]
train() client id: f_00003-9-0 loss: 0.592006  [   32/   43]
train() client id: f_00003-10-0 loss: 0.565593  [   32/   43]
train() client id: f_00003-11-0 loss: 0.639631  [   32/   43]
train() client id: f_00004-0-0 loss: 0.980857  [   32/  306]
train() client id: f_00004-0-1 loss: 0.980319  [   64/  306]
train() client id: f_00004-0-2 loss: 0.948793  [   96/  306]
train() client id: f_00004-0-3 loss: 1.147594  [  128/  306]
train() client id: f_00004-0-4 loss: 0.981886  [  160/  306]
train() client id: f_00004-0-5 loss: 1.005605  [  192/  306]
train() client id: f_00004-0-6 loss: 0.906228  [  224/  306]
train() client id: f_00004-0-7 loss: 0.932930  [  256/  306]
train() client id: f_00004-0-8 loss: 1.102259  [  288/  306]
train() client id: f_00004-1-0 loss: 0.960932  [   32/  306]
train() client id: f_00004-1-1 loss: 1.065026  [   64/  306]
train() client id: f_00004-1-2 loss: 1.039810  [   96/  306]
train() client id: f_00004-1-3 loss: 0.915506  [  128/  306]
train() client id: f_00004-1-4 loss: 0.971792  [  160/  306]
train() client id: f_00004-1-5 loss: 1.021284  [  192/  306]
train() client id: f_00004-1-6 loss: 1.006917  [  224/  306]
train() client id: f_00004-1-7 loss: 1.090623  [  256/  306]
train() client id: f_00004-1-8 loss: 0.870258  [  288/  306]
train() client id: f_00004-2-0 loss: 0.967877  [   32/  306]
train() client id: f_00004-2-1 loss: 1.123039  [   64/  306]
train() client id: f_00004-2-2 loss: 0.877855  [   96/  306]
train() client id: f_00004-2-3 loss: 1.022322  [  128/  306]
train() client id: f_00004-2-4 loss: 0.871990  [  160/  306]
train() client id: f_00004-2-5 loss: 0.961073  [  192/  306]
train() client id: f_00004-2-6 loss: 1.007387  [  224/  306]
train() client id: f_00004-2-7 loss: 0.922208  [  256/  306]
train() client id: f_00004-2-8 loss: 1.175031  [  288/  306]
train() client id: f_00004-3-0 loss: 0.965179  [   32/  306]
train() client id: f_00004-3-1 loss: 0.950839  [   64/  306]
train() client id: f_00004-3-2 loss: 1.119082  [   96/  306]
train() client id: f_00004-3-3 loss: 0.915605  [  128/  306]
train() client id: f_00004-3-4 loss: 0.849531  [  160/  306]
train() client id: f_00004-3-5 loss: 0.993100  [  192/  306]
train() client id: f_00004-3-6 loss: 1.032905  [  224/  306]
train() client id: f_00004-3-7 loss: 1.128770  [  256/  306]
train() client id: f_00004-3-8 loss: 0.825936  [  288/  306]
train() client id: f_00004-4-0 loss: 0.968931  [   32/  306]
train() client id: f_00004-4-1 loss: 1.017781  [   64/  306]
train() client id: f_00004-4-2 loss: 0.929352  [   96/  306]
train() client id: f_00004-4-3 loss: 1.007241  [  128/  306]
train() client id: f_00004-4-4 loss: 1.003673  [  160/  306]
train() client id: f_00004-4-5 loss: 0.875180  [  192/  306]
train() client id: f_00004-4-6 loss: 1.035059  [  224/  306]
train() client id: f_00004-4-7 loss: 0.868658  [  256/  306]
train() client id: f_00004-4-8 loss: 0.949017  [  288/  306]
train() client id: f_00004-5-0 loss: 1.156721  [   32/  306]
train() client id: f_00004-5-1 loss: 0.944909  [   64/  306]
train() client id: f_00004-5-2 loss: 1.018348  [   96/  306]
train() client id: f_00004-5-3 loss: 1.026671  [  128/  306]
train() client id: f_00004-5-4 loss: 0.979975  [  160/  306]
train() client id: f_00004-5-5 loss: 0.905163  [  192/  306]
train() client id: f_00004-5-6 loss: 0.923529  [  224/  306]
train() client id: f_00004-5-7 loss: 0.881636  [  256/  306]
train() client id: f_00004-5-8 loss: 0.856417  [  288/  306]
train() client id: f_00004-6-0 loss: 1.083157  [   32/  306]
train() client id: f_00004-6-1 loss: 1.055490  [   64/  306]
train() client id: f_00004-6-2 loss: 0.968905  [   96/  306]
train() client id: f_00004-6-3 loss: 0.993281  [  128/  306]
train() client id: f_00004-6-4 loss: 0.931580  [  160/  306]
train() client id: f_00004-6-5 loss: 0.946914  [  192/  306]
train() client id: f_00004-6-6 loss: 0.860175  [  224/  306]
train() client id: f_00004-6-7 loss: 0.912503  [  256/  306]
train() client id: f_00004-6-8 loss: 0.878056  [  288/  306]
train() client id: f_00004-7-0 loss: 0.943359  [   32/  306]
train() client id: f_00004-7-1 loss: 1.105526  [   64/  306]
train() client id: f_00004-7-2 loss: 1.040351  [   96/  306]
train() client id: f_00004-7-3 loss: 0.923003  [  128/  306]
train() client id: f_00004-7-4 loss: 0.931973  [  160/  306]
train() client id: f_00004-7-5 loss: 0.835594  [  192/  306]
train() client id: f_00004-7-6 loss: 0.996566  [  224/  306]
train() client id: f_00004-7-7 loss: 1.011241  [  256/  306]
train() client id: f_00004-7-8 loss: 0.829430  [  288/  306]
train() client id: f_00004-8-0 loss: 0.867091  [   32/  306]
train() client id: f_00004-8-1 loss: 1.051727  [   64/  306]
train() client id: f_00004-8-2 loss: 0.981364  [   96/  306]
train() client id: f_00004-8-3 loss: 0.871093  [  128/  306]
train() client id: f_00004-8-4 loss: 0.843409  [  160/  306]
train() client id: f_00004-8-5 loss: 0.978322  [  192/  306]
train() client id: f_00004-8-6 loss: 0.985785  [  224/  306]
train() client id: f_00004-8-7 loss: 0.909371  [  256/  306]
train() client id: f_00004-8-8 loss: 1.007348  [  288/  306]
train() client id: f_00004-9-0 loss: 0.914614  [   32/  306]
train() client id: f_00004-9-1 loss: 0.948691  [   64/  306]
train() client id: f_00004-9-2 loss: 0.956738  [   96/  306]
train() client id: f_00004-9-3 loss: 0.951702  [  128/  306]
train() client id: f_00004-9-4 loss: 0.863456  [  160/  306]
train() client id: f_00004-9-5 loss: 0.979708  [  192/  306]
train() client id: f_00004-9-6 loss: 0.947356  [  224/  306]
train() client id: f_00004-9-7 loss: 1.043364  [  256/  306]
train() client id: f_00004-9-8 loss: 1.021658  [  288/  306]
train() client id: f_00004-10-0 loss: 0.899502  [   32/  306]
train() client id: f_00004-10-1 loss: 1.081941  [   64/  306]
train() client id: f_00004-10-2 loss: 0.768292  [   96/  306]
train() client id: f_00004-10-3 loss: 0.909854  [  128/  306]
train() client id: f_00004-10-4 loss: 0.915567  [  160/  306]
train() client id: f_00004-10-5 loss: 0.951702  [  192/  306]
train() client id: f_00004-10-6 loss: 1.018493  [  224/  306]
train() client id: f_00004-10-7 loss: 1.007562  [  256/  306]
train() client id: f_00004-10-8 loss: 1.003835  [  288/  306]
train() client id: f_00004-11-0 loss: 0.964874  [   32/  306]
train() client id: f_00004-11-1 loss: 1.067343  [   64/  306]
train() client id: f_00004-11-2 loss: 0.900823  [   96/  306]
train() client id: f_00004-11-3 loss: 0.939202  [  128/  306]
train() client id: f_00004-11-4 loss: 0.882281  [  160/  306]
train() client id: f_00004-11-5 loss: 0.882509  [  192/  306]
train() client id: f_00004-11-6 loss: 0.970927  [  224/  306]
train() client id: f_00004-11-7 loss: 0.943092  [  256/  306]
train() client id: f_00004-11-8 loss: 0.935168  [  288/  306]
train() client id: f_00005-0-0 loss: 0.601182  [   32/  146]
train() client id: f_00005-0-1 loss: 0.505178  [   64/  146]
train() client id: f_00005-0-2 loss: 0.871577  [   96/  146]
train() client id: f_00005-0-3 loss: 0.715638  [  128/  146]
train() client id: f_00005-1-0 loss: 0.413404  [   32/  146]
train() client id: f_00005-1-1 loss: 0.748996  [   64/  146]
train() client id: f_00005-1-2 loss: 0.738074  [   96/  146]
train() client id: f_00005-1-3 loss: 0.698864  [  128/  146]
train() client id: f_00005-2-0 loss: 0.891725  [   32/  146]
train() client id: f_00005-2-1 loss: 0.778134  [   64/  146]
train() client id: f_00005-2-2 loss: 0.510317  [   96/  146]
train() client id: f_00005-2-3 loss: 0.650975  [  128/  146]
train() client id: f_00005-3-0 loss: 0.711394  [   32/  146]
train() client id: f_00005-3-1 loss: 0.671272  [   64/  146]
train() client id: f_00005-3-2 loss: 0.778246  [   96/  146]
train() client id: f_00005-3-3 loss: 0.580172  [  128/  146]
train() client id: f_00005-4-0 loss: 0.813229  [   32/  146]
train() client id: f_00005-4-1 loss: 0.763847  [   64/  146]
train() client id: f_00005-4-2 loss: 0.658119  [   96/  146]
train() client id: f_00005-4-3 loss: 0.419828  [  128/  146]
train() client id: f_00005-5-0 loss: 0.651418  [   32/  146]
train() client id: f_00005-5-1 loss: 0.595549  [   64/  146]
train() client id: f_00005-5-2 loss: 0.945731  [   96/  146]
train() client id: f_00005-5-3 loss: 0.581440  [  128/  146]
train() client id: f_00005-6-0 loss: 0.453886  [   32/  146]
train() client id: f_00005-6-1 loss: 0.553408  [   64/  146]
train() client id: f_00005-6-2 loss: 0.628884  [   96/  146]
train() client id: f_00005-6-3 loss: 0.834911  [  128/  146]
train() client id: f_00005-7-0 loss: 0.829347  [   32/  146]
train() client id: f_00005-7-1 loss: 0.662820  [   64/  146]
train() client id: f_00005-7-2 loss: 0.575629  [   96/  146]
train() client id: f_00005-7-3 loss: 0.671571  [  128/  146]
train() client id: f_00005-8-0 loss: 0.730839  [   32/  146]
train() client id: f_00005-8-1 loss: 0.549007  [   64/  146]
train() client id: f_00005-8-2 loss: 0.915910  [   96/  146]
train() client id: f_00005-8-3 loss: 0.614646  [  128/  146]
train() client id: f_00005-9-0 loss: 0.696924  [   32/  146]
train() client id: f_00005-9-1 loss: 0.527142  [   64/  146]
train() client id: f_00005-9-2 loss: 0.882716  [   96/  146]
train() client id: f_00005-9-3 loss: 0.657008  [  128/  146]
train() client id: f_00005-10-0 loss: 0.551424  [   32/  146]
train() client id: f_00005-10-1 loss: 0.945273  [   64/  146]
train() client id: f_00005-10-2 loss: 0.332624  [   96/  146]
train() client id: f_00005-10-3 loss: 0.819135  [  128/  146]
train() client id: f_00005-11-0 loss: 0.835742  [   32/  146]
train() client id: f_00005-11-1 loss: 0.776833  [   64/  146]
train() client id: f_00005-11-2 loss: 0.538853  [   96/  146]
train() client id: f_00005-11-3 loss: 0.649911  [  128/  146]
train() client id: f_00006-0-0 loss: 0.497988  [   32/   54]
train() client id: f_00006-1-0 loss: 0.549978  [   32/   54]
train() client id: f_00006-2-0 loss: 0.552462  [   32/   54]
train() client id: f_00006-3-0 loss: 0.501587  [   32/   54]
train() client id: f_00006-4-0 loss: 0.501380  [   32/   54]
train() client id: f_00006-5-0 loss: 0.562552  [   32/   54]
train() client id: f_00006-6-0 loss: 0.508027  [   32/   54]
train() client id: f_00006-7-0 loss: 0.499135  [   32/   54]
train() client id: f_00006-8-0 loss: 0.499385  [   32/   54]
train() client id: f_00006-9-0 loss: 0.553647  [   32/   54]
train() client id: f_00006-10-0 loss: 0.496246  [   32/   54]
train() client id: f_00006-11-0 loss: 0.463257  [   32/   54]
train() client id: f_00007-0-0 loss: 0.589686  [   32/  179]
train() client id: f_00007-0-1 loss: 0.753256  [   64/  179]
train() client id: f_00007-0-2 loss: 0.598969  [   96/  179]
train() client id: f_00007-0-3 loss: 0.630172  [  128/  179]
train() client id: f_00007-0-4 loss: 0.627702  [  160/  179]
train() client id: f_00007-1-0 loss: 0.800139  [   32/  179]
train() client id: f_00007-1-1 loss: 0.548029  [   64/  179]
train() client id: f_00007-1-2 loss: 0.510848  [   96/  179]
train() client id: f_00007-1-3 loss: 0.600705  [  128/  179]
train() client id: f_00007-1-4 loss: 0.668092  [  160/  179]
train() client id: f_00007-2-0 loss: 0.613331  [   32/  179]
train() client id: f_00007-2-1 loss: 0.631917  [   64/  179]
train() client id: f_00007-2-2 loss: 0.501292  [   96/  179]
train() client id: f_00007-2-3 loss: 0.524908  [  128/  179]
train() client id: f_00007-2-4 loss: 0.620448  [  160/  179]
train() client id: f_00007-3-0 loss: 0.552190  [   32/  179]
train() client id: f_00007-3-1 loss: 0.530070  [   64/  179]
train() client id: f_00007-3-2 loss: 0.480306  [   96/  179]
train() client id: f_00007-3-3 loss: 0.722262  [  128/  179]
train() client id: f_00007-3-4 loss: 0.665741  [  160/  179]
train() client id: f_00007-4-0 loss: 0.502415  [   32/  179]
train() client id: f_00007-4-1 loss: 0.529335  [   64/  179]
train() client id: f_00007-4-2 loss: 0.676729  [   96/  179]
train() client id: f_00007-4-3 loss: 0.580328  [  128/  179]
train() client id: f_00007-4-4 loss: 0.693123  [  160/  179]
train() client id: f_00007-5-0 loss: 0.444833  [   32/  179]
train() client id: f_00007-5-1 loss: 0.523033  [   64/  179]
train() client id: f_00007-5-2 loss: 0.686259  [   96/  179]
train() client id: f_00007-5-3 loss: 0.730584  [  128/  179]
train() client id: f_00007-5-4 loss: 0.483564  [  160/  179]
train() client id: f_00007-6-0 loss: 0.453923  [   32/  179]
train() client id: f_00007-6-1 loss: 0.558623  [   64/  179]
train() client id: f_00007-6-2 loss: 0.567811  [   96/  179]
train() client id: f_00007-6-3 loss: 0.612052  [  128/  179]
train() client id: f_00007-6-4 loss: 0.525495  [  160/  179]
train() client id: f_00007-7-0 loss: 0.600715  [   32/  179]
train() client id: f_00007-7-1 loss: 0.412082  [   64/  179]
train() client id: f_00007-7-2 loss: 0.443832  [   96/  179]
train() client id: f_00007-7-3 loss: 0.704750  [  128/  179]
train() client id: f_00007-7-4 loss: 0.752055  [  160/  179]
train() client id: f_00007-8-0 loss: 0.508659  [   32/  179]
train() client id: f_00007-8-1 loss: 0.533374  [   64/  179]
train() client id: f_00007-8-2 loss: 0.673292  [   96/  179]
train() client id: f_00007-8-3 loss: 0.507034  [  128/  179]
train() client id: f_00007-8-4 loss: 0.562743  [  160/  179]
train() client id: f_00007-9-0 loss: 0.443388  [   32/  179]
train() client id: f_00007-9-1 loss: 0.659434  [   64/  179]
train() client id: f_00007-9-2 loss: 0.499890  [   96/  179]
train() client id: f_00007-9-3 loss: 0.605044  [  128/  179]
train() client id: f_00007-9-4 loss: 0.712024  [  160/  179]
train() client id: f_00007-10-0 loss: 0.479500  [   32/  179]
train() client id: f_00007-10-1 loss: 0.586521  [   64/  179]
train() client id: f_00007-10-2 loss: 0.450634  [   96/  179]
train() client id: f_00007-10-3 loss: 0.646345  [  128/  179]
train() client id: f_00007-10-4 loss: 0.686283  [  160/  179]
train() client id: f_00007-11-0 loss: 0.503701  [   32/  179]
train() client id: f_00007-11-1 loss: 0.594330  [   64/  179]
train() client id: f_00007-11-2 loss: 0.437729  [   96/  179]
train() client id: f_00007-11-3 loss: 0.602153  [  128/  179]
train() client id: f_00007-11-4 loss: 0.595608  [  160/  179]
train() client id: f_00008-0-0 loss: 0.849243  [   32/  130]
train() client id: f_00008-0-1 loss: 0.781570  [   64/  130]
train() client id: f_00008-0-2 loss: 0.776185  [   96/  130]
train() client id: f_00008-0-3 loss: 0.698684  [  128/  130]
train() client id: f_00008-1-0 loss: 0.834025  [   32/  130]
train() client id: f_00008-1-1 loss: 0.808880  [   64/  130]
train() client id: f_00008-1-2 loss: 0.715360  [   96/  130]
train() client id: f_00008-1-3 loss: 0.747698  [  128/  130]
train() client id: f_00008-2-0 loss: 0.685792  [   32/  130]
train() client id: f_00008-2-1 loss: 0.898880  [   64/  130]
train() client id: f_00008-2-2 loss: 0.709316  [   96/  130]
train() client id: f_00008-2-3 loss: 0.773487  [  128/  130]
train() client id: f_00008-3-0 loss: 0.784351  [   32/  130]
train() client id: f_00008-3-1 loss: 0.786089  [   64/  130]
train() client id: f_00008-3-2 loss: 0.708468  [   96/  130]
train() client id: f_00008-3-3 loss: 0.808582  [  128/  130]
train() client id: f_00008-4-0 loss: 0.691976  [   32/  130]
train() client id: f_00008-4-1 loss: 0.671779  [   64/  130]
train() client id: f_00008-4-2 loss: 0.819797  [   96/  130]
train() client id: f_00008-4-3 loss: 0.909261  [  128/  130]
train() client id: f_00008-5-0 loss: 0.742848  [   32/  130]
train() client id: f_00008-5-1 loss: 0.850326  [   64/  130]
train() client id: f_00008-5-2 loss: 0.719482  [   96/  130]
train() client id: f_00008-5-3 loss: 0.773904  [  128/  130]
train() client id: f_00008-6-0 loss: 0.696127  [   32/  130]
train() client id: f_00008-6-1 loss: 0.742603  [   64/  130]
train() client id: f_00008-6-2 loss: 0.764706  [   96/  130]
train() client id: f_00008-6-3 loss: 0.821999  [  128/  130]
train() client id: f_00008-7-0 loss: 0.697988  [   32/  130]
train() client id: f_00008-7-1 loss: 0.848873  [   64/  130]
train() client id: f_00008-7-2 loss: 0.826758  [   96/  130]
train() client id: f_00008-7-3 loss: 0.662632  [  128/  130]
train() client id: f_00008-8-0 loss: 0.839831  [   32/  130]
train() client id: f_00008-8-1 loss: 0.688124  [   64/  130]
train() client id: f_00008-8-2 loss: 0.766032  [   96/  130]
train() client id: f_00008-8-3 loss: 0.784769  [  128/  130]
train() client id: f_00008-9-0 loss: 0.815773  [   32/  130]
train() client id: f_00008-9-1 loss: 0.831409  [   64/  130]
train() client id: f_00008-9-2 loss: 0.685569  [   96/  130]
train() client id: f_00008-9-3 loss: 0.749807  [  128/  130]
train() client id: f_00008-10-0 loss: 0.732392  [   32/  130]
train() client id: f_00008-10-1 loss: 0.794209  [   64/  130]
train() client id: f_00008-10-2 loss: 0.747368  [   96/  130]
train() client id: f_00008-10-3 loss: 0.804476  [  128/  130]
train() client id: f_00008-11-0 loss: 0.670548  [   32/  130]
train() client id: f_00008-11-1 loss: 0.809718  [   64/  130]
train() client id: f_00008-11-2 loss: 0.722573  [   96/  130]
train() client id: f_00008-11-3 loss: 0.874959  [  128/  130]
train() client id: f_00009-0-0 loss: 1.110224  [   32/  118]
train() client id: f_00009-0-1 loss: 1.070140  [   64/  118]
train() client id: f_00009-0-2 loss: 0.976961  [   96/  118]
train() client id: f_00009-1-0 loss: 0.961950  [   32/  118]
train() client id: f_00009-1-1 loss: 0.852538  [   64/  118]
train() client id: f_00009-1-2 loss: 1.070649  [   96/  118]
train() client id: f_00009-2-0 loss: 0.940248  [   32/  118]
train() client id: f_00009-2-1 loss: 0.962418  [   64/  118]
train() client id: f_00009-2-2 loss: 0.872555  [   96/  118]
train() client id: f_00009-3-0 loss: 0.947279  [   32/  118]
train() client id: f_00009-3-1 loss: 0.916866  [   64/  118]
train() client id: f_00009-3-2 loss: 0.951676  [   96/  118]
train() client id: f_00009-4-0 loss: 0.830895  [   32/  118]
train() client id: f_00009-4-1 loss: 0.950781  [   64/  118]
train() client id: f_00009-4-2 loss: 0.819728  [   96/  118]
train() client id: f_00009-5-0 loss: 0.874938  [   32/  118]
train() client id: f_00009-5-1 loss: 0.713982  [   64/  118]
train() client id: f_00009-5-2 loss: 0.800368  [   96/  118]
train() client id: f_00009-6-0 loss: 0.851117  [   32/  118]
train() client id: f_00009-6-1 loss: 0.840192  [   64/  118]
train() client id: f_00009-6-2 loss: 0.859479  [   96/  118]
train() client id: f_00009-7-0 loss: 0.768273  [   32/  118]
train() client id: f_00009-7-1 loss: 0.878569  [   64/  118]
train() client id: f_00009-7-2 loss: 0.774635  [   96/  118]
train() client id: f_00009-8-0 loss: 0.804041  [   32/  118]
train() client id: f_00009-8-1 loss: 0.873075  [   64/  118]
train() client id: f_00009-8-2 loss: 0.750705  [   96/  118]
train() client id: f_00009-9-0 loss: 0.837466  [   32/  118]
train() client id: f_00009-9-1 loss: 0.675162  [   64/  118]
train() client id: f_00009-9-2 loss: 0.945371  [   96/  118]
train() client id: f_00009-10-0 loss: 0.739420  [   32/  118]
train() client id: f_00009-10-1 loss: 0.836838  [   64/  118]
train() client id: f_00009-10-2 loss: 0.834497  [   96/  118]
train() client id: f_00009-11-0 loss: 0.854810  [   32/  118]
train() client id: f_00009-11-1 loss: 0.837647  [   64/  118]
train() client id: f_00009-11-2 loss: 0.672225  [   96/  118]
At round 36 accuracy: 0.6525198938992043
At round 36 training accuracy: 0.5875251509054326
At round 36 training loss: 0.8358401985038234
update_location
xs = [  -3.9056584     4.20031788  200.00902392   18.81129433    0.97929623
    3.95640986 -162.44319194 -141.32485185  184.66397685 -127.06087855]
ys = [ 192.5879595   175.55583871    1.32061395 -162.45517586  154.35018685
  137.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [217.03773016 202.08289184 223.6187686  191.69128556 183.91557628
 170.31850552 190.7739006  173.12824728 210.73550797 161.74201283]
dists_bs = [171.99595939 179.20117992 413.27508654 389.1514018  177.18670233
 182.77905793 178.06826916 177.40312868 392.61010444 177.59125147]
uav_gains = [1.32188752e-11 1.65442503e-11 1.19052653e-11 1.92240094e-11
 2.15038573e-11 2.62734338e-11 1.94793158e-11 2.51899720e-11
 1.45569033e-11 2.99729913e-11]
bs_gains = [6.07915198e-11 5.41925596e-11 5.22184833e-12 6.17962338e-12
 5.59354239e-11 5.12743388e-11 5.51634959e-11 5.57445631e-11
 6.02839862e-12 5.55793797e-11]
Round 37
-------------------------------
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.10747648 12.63082676  6.01756463  2.17165072 14.56715747  7.00995328
  2.69087946  8.58799468  6.33825934  5.68528245]
obj_prev = 71.80704525657299
eta_min = 7.508228350489047e-16	eta_max = 0.9300641933191646
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 16.65764355541381	eta = 0.909090909090909
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 30.742318165825036	eta = 0.4925884977645458
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 23.795554951531557	eta = 0.636392483972255
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.538890009475697	eta = 0.6718748046925549
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.472173027112785	eta = 0.6738695143025546
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.47196919842037	eta = 0.6738756265369006
eta = 0.6738756265369006
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [0.0325705  0.06850145 0.03205351 0.01111533 0.07909978 0.03774042
 0.01395879 0.04627079 0.03360448 0.03050253]
ene_total = [2.01826629 3.59433964 2.00757769 0.95394728 4.09618977 2.13534693
 1.08775634 2.59856536 2.19433777 1.78564212]
ti_comp = [0.49324588 0.52112484 0.49032366 0.50250852 0.52157476 0.52032434
 0.50280454 0.5082184  0.46655987 0.52148445]
ti_coms = [0.09871374 0.07083478 0.10163597 0.08945111 0.07038486 0.07163528
 0.08915509 0.08374122 0.12539975 0.07047517]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.87618492e-06 7.39768305e-05 8.56132548e-06 3.39907040e-07
 1.13703226e-04 1.24094325e-05 6.72395117e-07 2.39717625e-05
 1.08957452e-05 6.52236260e-06]
ene_total = [0.46984203 0.34036317 0.48372332 0.42538889 0.34011276 0.34124206
 0.42399702 0.39936008 0.59683981 0.33544538]
optimize_network_iter = 0 obj = 4.156314504851677
eta = 0.6738756265369006
freqs = [33016494.15795401 65724604.40972588 32686070.96869287 11059841.67344831
 75827849.2653193  36266244.90193002 13880926.45922257 45522544.15430705
 36013045.58118701 29245868.53629994]
eta_min = 0.673875626536905	eta_max = 0.6738756265369027
af = 0.01021278956996407	bf = 1.3514443765451492	zeta = 0.011234068526960479	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [2.14332394e-06 1.78631149e-05 2.06729513e-06 8.20770299e-08
 2.74558099e-05 2.99649386e-06 1.62362610e-07 5.78843872e-06
 2.63098523e-06 1.57494870e-06]
ene_total = [1.68434772 1.21143542 1.73418576 1.52598255 1.2053965  1.2225551
 1.52094637 1.42954967 2.1396742  1.20252205]
ti_comp = [0.49324588 0.52112484 0.49032366 0.50250852 0.52157476 0.52032434
 0.50280454 0.5082184  0.46655987 0.52148445]
ti_coms = [0.09871374 0.07083478 0.10163597 0.08945111 0.07038486 0.07163528
 0.08915509 0.08374122 0.12539975 0.07047517]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.87618492e-06 7.39768305e-05 8.56132548e-06 3.39907040e-07
 1.13703226e-04 1.24094325e-05 6.72395117e-07 2.39717625e-05
 1.08957452e-05 6.52236260e-06]
ene_total = [0.46984203 0.34036317 0.48372332 0.42538889 0.34011276 0.34124206
 0.42399702 0.39936008 0.59683981 0.33544538]
optimize_network_iter = 1 obj = 4.156314504851704
eta = 0.6738756265369027
freqs = [33016494.15795401 65724604.40972584 32686070.96869286 11059841.67344831
 75827849.26531927 36266244.90193    13880926.45922257 45522544.15430704
 36013045.58118702 29245868.53629993]
Done!
At round 37 eta: 0.6738756265369027
At round 37 local rounds: 12.924801449949687
At round 37 global rounds: 47.55365676507614
At round 37 a_n: 15.165860671420319
gradient difference: 0.5399806499481201
train() client id: f_00000-0-0 loss: 1.042660  [   32/  126]
train() client id: f_00000-0-1 loss: 1.025464  [   64/  126]
train() client id: f_00000-0-2 loss: 1.028881  [   96/  126]
train() client id: f_00000-1-0 loss: 1.002046  [   32/  126]
train() client id: f_00000-1-1 loss: 0.862610  [   64/  126]
train() client id: f_00000-1-2 loss: 1.016644  [   96/  126]
train() client id: f_00000-2-0 loss: 0.931273  [   32/  126]
train() client id: f_00000-2-1 loss: 0.956410  [   64/  126]
train() client id: f_00000-2-2 loss: 0.878360  [   96/  126]
train() client id: f_00000-3-0 loss: 0.897056  [   32/  126]
train() client id: f_00000-3-1 loss: 0.850660  [   64/  126]
train() client id: f_00000-3-2 loss: 0.771851  [   96/  126]
train() client id: f_00000-4-0 loss: 0.866647  [   32/  126]
train() client id: f_00000-4-1 loss: 0.901403  [   64/  126]
train() client id: f_00000-4-2 loss: 0.913421  [   96/  126]
train() client id: f_00000-5-0 loss: 0.887737  [   32/  126]
train() client id: f_00000-5-1 loss: 0.948491  [   64/  126]
train() client id: f_00000-5-2 loss: 0.822250  [   96/  126]
train() client id: f_00000-6-0 loss: 0.830758  [   32/  126]
train() client id: f_00000-6-1 loss: 0.785903  [   64/  126]
train() client id: f_00000-6-2 loss: 0.895949  [   96/  126]
train() client id: f_00000-7-0 loss: 0.872342  [   32/  126]
train() client id: f_00000-7-1 loss: 0.741978  [   64/  126]
train() client id: f_00000-7-2 loss: 0.975245  [   96/  126]
train() client id: f_00000-8-0 loss: 0.896194  [   32/  126]
train() client id: f_00000-8-1 loss: 0.855637  [   64/  126]
train() client id: f_00000-8-2 loss: 0.922189  [   96/  126]
train() client id: f_00000-9-0 loss: 0.859043  [   32/  126]
train() client id: f_00000-9-1 loss: 0.991630  [   64/  126]
train() client id: f_00000-9-2 loss: 0.812585  [   96/  126]
train() client id: f_00000-10-0 loss: 0.974091  [   32/  126]
train() client id: f_00000-10-1 loss: 0.904362  [   64/  126]
train() client id: f_00000-10-2 loss: 0.782502  [   96/  126]
train() client id: f_00000-11-0 loss: 0.869181  [   32/  126]
train() client id: f_00000-11-1 loss: 0.976393  [   64/  126]
train() client id: f_00000-11-2 loss: 0.850657  [   96/  126]
train() client id: f_00001-0-0 loss: 0.438952  [   32/  265]
train() client id: f_00001-0-1 loss: 0.504810  [   64/  265]
train() client id: f_00001-0-2 loss: 0.590955  [   96/  265]
train() client id: f_00001-0-3 loss: 0.602673  [  128/  265]
train() client id: f_00001-0-4 loss: 0.425452  [  160/  265]
train() client id: f_00001-0-5 loss: 0.586654  [  192/  265]
train() client id: f_00001-0-6 loss: 0.466883  [  224/  265]
train() client id: f_00001-0-7 loss: 0.422702  [  256/  265]
train() client id: f_00001-1-0 loss: 0.448406  [   32/  265]
train() client id: f_00001-1-1 loss: 0.532304  [   64/  265]
train() client id: f_00001-1-2 loss: 0.415828  [   96/  265]
train() client id: f_00001-1-3 loss: 0.542741  [  128/  265]
train() client id: f_00001-1-4 loss: 0.529372  [  160/  265]
train() client id: f_00001-1-5 loss: 0.486812  [  192/  265]
train() client id: f_00001-1-6 loss: 0.498122  [  224/  265]
train() client id: f_00001-1-7 loss: 0.459831  [  256/  265]
train() client id: f_00001-2-0 loss: 0.403696  [   32/  265]
train() client id: f_00001-2-1 loss: 0.504443  [   64/  265]
train() client id: f_00001-2-2 loss: 0.540015  [   96/  265]
train() client id: f_00001-2-3 loss: 0.530114  [  128/  265]
train() client id: f_00001-2-4 loss: 0.504163  [  160/  265]
train() client id: f_00001-2-5 loss: 0.461378  [  192/  265]
train() client id: f_00001-2-6 loss: 0.540678  [  224/  265]
train() client id: f_00001-2-7 loss: 0.473963  [  256/  265]
train() client id: f_00001-3-0 loss: 0.546289  [   32/  265]
train() client id: f_00001-3-1 loss: 0.538268  [   64/  265]
train() client id: f_00001-3-2 loss: 0.458901  [   96/  265]
train() client id: f_00001-3-3 loss: 0.488419  [  128/  265]
train() client id: f_00001-3-4 loss: 0.483456  [  160/  265]
train() client id: f_00001-3-5 loss: 0.410782  [  192/  265]
train() client id: f_00001-3-6 loss: 0.446154  [  224/  265]
train() client id: f_00001-3-7 loss: 0.539659  [  256/  265]
train() client id: f_00001-4-0 loss: 0.515204  [   32/  265]
train() client id: f_00001-4-1 loss: 0.490477  [   64/  265]
train() client id: f_00001-4-2 loss: 0.497971  [   96/  265]
train() client id: f_00001-4-3 loss: 0.538962  [  128/  265]
train() client id: f_00001-4-4 loss: 0.390189  [  160/  265]
train() client id: f_00001-4-5 loss: 0.530608  [  192/  265]
train() client id: f_00001-4-6 loss: 0.542093  [  224/  265]
train() client id: f_00001-4-7 loss: 0.399270  [  256/  265]
train() client id: f_00001-5-0 loss: 0.398817  [   32/  265]
train() client id: f_00001-5-1 loss: 0.434048  [   64/  265]
train() client id: f_00001-5-2 loss: 0.531819  [   96/  265]
train() client id: f_00001-5-3 loss: 0.465554  [  128/  265]
train() client id: f_00001-5-4 loss: 0.467101  [  160/  265]
train() client id: f_00001-5-5 loss: 0.393511  [  192/  265]
train() client id: f_00001-5-6 loss: 0.450194  [  224/  265]
train() client id: f_00001-5-7 loss: 0.608938  [  256/  265]
train() client id: f_00001-6-0 loss: 0.447405  [   32/  265]
train() client id: f_00001-6-1 loss: 0.534345  [   64/  265]
train() client id: f_00001-6-2 loss: 0.430438  [   96/  265]
train() client id: f_00001-6-3 loss: 0.398857  [  128/  265]
train() client id: f_00001-6-4 loss: 0.639733  [  160/  265]
train() client id: f_00001-6-5 loss: 0.425799  [  192/  265]
train() client id: f_00001-6-6 loss: 0.508676  [  224/  265]
train() client id: f_00001-6-7 loss: 0.492406  [  256/  265]
train() client id: f_00001-7-0 loss: 0.399717  [   32/  265]
train() client id: f_00001-7-1 loss: 0.397237  [   64/  265]
train() client id: f_00001-7-2 loss: 0.422932  [   96/  265]
train() client id: f_00001-7-3 loss: 0.738625  [  128/  265]
train() client id: f_00001-7-4 loss: 0.567833  [  160/  265]
train() client id: f_00001-7-5 loss: 0.414839  [  192/  265]
train() client id: f_00001-7-6 loss: 0.434728  [  224/  265]
train() client id: f_00001-7-7 loss: 0.509890  [  256/  265]
train() client id: f_00001-8-0 loss: 0.473283  [   32/  265]
train() client id: f_00001-8-1 loss: 0.410475  [   64/  265]
train() client id: f_00001-8-2 loss: 0.443997  [   96/  265]
train() client id: f_00001-8-3 loss: 0.456529  [  128/  265]
train() client id: f_00001-8-4 loss: 0.562292  [  160/  265]
train() client id: f_00001-8-5 loss: 0.640847  [  192/  265]
train() client id: f_00001-8-6 loss: 0.412597  [  224/  265]
train() client id: f_00001-8-7 loss: 0.474400  [  256/  265]
train() client id: f_00001-9-0 loss: 0.413328  [   32/  265]
train() client id: f_00001-9-1 loss: 0.434907  [   64/  265]
train() client id: f_00001-9-2 loss: 0.405236  [   96/  265]
train() client id: f_00001-9-3 loss: 0.554407  [  128/  265]
train() client id: f_00001-9-4 loss: 0.583641  [  160/  265]
train() client id: f_00001-9-5 loss: 0.401707  [  192/  265]
train() client id: f_00001-9-6 loss: 0.604861  [  224/  265]
train() client id: f_00001-9-7 loss: 0.383736  [  256/  265]
train() client id: f_00001-10-0 loss: 0.470055  [   32/  265]
train() client id: f_00001-10-1 loss: 0.443111  [   64/  265]
train() client id: f_00001-10-2 loss: 0.512312  [   96/  265]
train() client id: f_00001-10-3 loss: 0.521009  [  128/  265]
train() client id: f_00001-10-4 loss: 0.522733  [  160/  265]
train() client id: f_00001-10-5 loss: 0.527962  [  192/  265]
train() client id: f_00001-10-6 loss: 0.385126  [  224/  265]
train() client id: f_00001-10-7 loss: 0.490169  [  256/  265]
train() client id: f_00001-11-0 loss: 0.376860  [   32/  265]
train() client id: f_00001-11-1 loss: 0.547198  [   64/  265]
train() client id: f_00001-11-2 loss: 0.441769  [   96/  265]
train() client id: f_00001-11-3 loss: 0.397921  [  128/  265]
train() client id: f_00001-11-4 loss: 0.470474  [  160/  265]
train() client id: f_00001-11-5 loss: 0.410005  [  192/  265]
train() client id: f_00001-11-6 loss: 0.469134  [  224/  265]
train() client id: f_00001-11-7 loss: 0.750309  [  256/  265]
train() client id: f_00002-0-0 loss: 1.298834  [   32/  124]
train() client id: f_00002-0-1 loss: 1.124982  [   64/  124]
train() client id: f_00002-0-2 loss: 1.220586  [   96/  124]
train() client id: f_00002-1-0 loss: 1.192032  [   32/  124]
train() client id: f_00002-1-1 loss: 1.120024  [   64/  124]
train() client id: f_00002-1-2 loss: 1.160137  [   96/  124]
train() client id: f_00002-2-0 loss: 1.138126  [   32/  124]
train() client id: f_00002-2-1 loss: 1.065712  [   64/  124]
train() client id: f_00002-2-2 loss: 1.127088  [   96/  124]
train() client id: f_00002-3-0 loss: 1.217279  [   32/  124]
train() client id: f_00002-3-1 loss: 1.139540  [   64/  124]
train() client id: f_00002-3-2 loss: 1.071864  [   96/  124]
train() client id: f_00002-4-0 loss: 1.148143  [   32/  124]
train() client id: f_00002-4-1 loss: 0.978358  [   64/  124]
train() client id: f_00002-4-2 loss: 1.156797  [   96/  124]
train() client id: f_00002-5-0 loss: 1.114571  [   32/  124]
train() client id: f_00002-5-1 loss: 0.993340  [   64/  124]
train() client id: f_00002-5-2 loss: 1.206112  [   96/  124]
train() client id: f_00002-6-0 loss: 1.221342  [   32/  124]
train() client id: f_00002-6-1 loss: 0.991171  [   64/  124]
train() client id: f_00002-6-2 loss: 1.022163  [   96/  124]
train() client id: f_00002-7-0 loss: 0.976630  [   32/  124]
train() client id: f_00002-7-1 loss: 1.204512  [   64/  124]
train() client id: f_00002-7-2 loss: 1.207399  [   96/  124]
train() client id: f_00002-8-0 loss: 1.050621  [   32/  124]
train() client id: f_00002-8-1 loss: 1.064654  [   64/  124]
train() client id: f_00002-8-2 loss: 1.201168  [   96/  124]
train() client id: f_00002-9-0 loss: 1.151058  [   32/  124]
train() client id: f_00002-9-1 loss: 0.928564  [   64/  124]
train() client id: f_00002-9-2 loss: 0.982689  [   96/  124]
train() client id: f_00002-10-0 loss: 1.081876  [   32/  124]
train() client id: f_00002-10-1 loss: 1.016665  [   64/  124]
train() client id: f_00002-10-2 loss: 1.164769  [   96/  124]
train() client id: f_00002-11-0 loss: 1.200849  [   32/  124]
train() client id: f_00002-11-1 loss: 0.854172  [   64/  124]
train() client id: f_00002-11-2 loss: 1.130745  [   96/  124]
train() client id: f_00003-0-0 loss: 0.607355  [   32/   43]
train() client id: f_00003-1-0 loss: 0.749066  [   32/   43]
train() client id: f_00003-2-0 loss: 0.647996  [   32/   43]
train() client id: f_00003-3-0 loss: 0.819757  [   32/   43]
train() client id: f_00003-4-0 loss: 0.497368  [   32/   43]
train() client id: f_00003-5-0 loss: 0.649503  [   32/   43]
train() client id: f_00003-6-0 loss: 0.749654  [   32/   43]
train() client id: f_00003-7-0 loss: 0.699681  [   32/   43]
train() client id: f_00003-8-0 loss: 0.703970  [   32/   43]
train() client id: f_00003-9-0 loss: 0.477942  [   32/   43]
train() client id: f_00003-10-0 loss: 0.754174  [   32/   43]
train() client id: f_00003-11-0 loss: 0.774346  [   32/   43]
train() client id: f_00004-0-0 loss: 0.800907  [   32/  306]
train() client id: f_00004-0-1 loss: 0.805315  [   64/  306]
train() client id: f_00004-0-2 loss: 0.818373  [   96/  306]
train() client id: f_00004-0-3 loss: 0.842117  [  128/  306]
train() client id: f_00004-0-4 loss: 0.825168  [  160/  306]
train() client id: f_00004-0-5 loss: 0.746421  [  192/  306]
train() client id: f_00004-0-6 loss: 0.810844  [  224/  306]
train() client id: f_00004-0-7 loss: 0.796782  [  256/  306]
train() client id: f_00004-0-8 loss: 0.796818  [  288/  306]
train() client id: f_00004-1-0 loss: 0.774045  [   32/  306]
train() client id: f_00004-1-1 loss: 0.584444  [   64/  306]
train() client id: f_00004-1-2 loss: 0.902941  [   96/  306]
train() client id: f_00004-1-3 loss: 0.890418  [  128/  306]
train() client id: f_00004-1-4 loss: 0.961398  [  160/  306]
train() client id: f_00004-1-5 loss: 0.741356  [  192/  306]
train() client id: f_00004-1-6 loss: 0.724411  [  224/  306]
train() client id: f_00004-1-7 loss: 0.811024  [  256/  306]
train() client id: f_00004-1-8 loss: 0.826747  [  288/  306]
train() client id: f_00004-2-0 loss: 0.830504  [   32/  306]
train() client id: f_00004-2-1 loss: 0.820455  [   64/  306]
train() client id: f_00004-2-2 loss: 0.712529  [   96/  306]
train() client id: f_00004-2-3 loss: 0.790272  [  128/  306]
train() client id: f_00004-2-4 loss: 0.750189  [  160/  306]
train() client id: f_00004-2-5 loss: 0.771001  [  192/  306]
train() client id: f_00004-2-6 loss: 0.863077  [  224/  306]
train() client id: f_00004-2-7 loss: 0.861326  [  256/  306]
train() client id: f_00004-2-8 loss: 0.810070  [  288/  306]
train() client id: f_00004-3-0 loss: 0.909950  [   32/  306]
train() client id: f_00004-3-1 loss: 0.838386  [   64/  306]
train() client id: f_00004-3-2 loss: 0.725727  [   96/  306]
train() client id: f_00004-3-3 loss: 0.810565  [  128/  306]
train() client id: f_00004-3-4 loss: 0.807503  [  160/  306]
train() client id: f_00004-3-5 loss: 0.729026  [  192/  306]
train() client id: f_00004-3-6 loss: 0.763262  [  224/  306]
train() client id: f_00004-3-7 loss: 0.793482  [  256/  306]
train() client id: f_00004-3-8 loss: 0.844216  [  288/  306]
train() client id: f_00004-4-0 loss: 0.796176  [   32/  306]
train() client id: f_00004-4-1 loss: 0.753154  [   64/  306]
train() client id: f_00004-4-2 loss: 0.899615  [   96/  306]
train() client id: f_00004-4-3 loss: 0.811043  [  128/  306]
train() client id: f_00004-4-4 loss: 0.710645  [  160/  306]
train() client id: f_00004-4-5 loss: 0.764473  [  192/  306]
train() client id: f_00004-4-6 loss: 0.789715  [  224/  306]
train() client id: f_00004-4-7 loss: 0.930200  [  256/  306]
train() client id: f_00004-4-8 loss: 0.757454  [  288/  306]
train() client id: f_00004-5-0 loss: 0.719359  [   32/  306]
train() client id: f_00004-5-1 loss: 0.752192  [   64/  306]
train() client id: f_00004-5-2 loss: 0.713351  [   96/  306]
train() client id: f_00004-5-3 loss: 0.777078  [  128/  306]
train() client id: f_00004-5-4 loss: 0.771352  [  160/  306]
train() client id: f_00004-5-5 loss: 0.827625  [  192/  306]
train() client id: f_00004-5-6 loss: 0.767916  [  224/  306]
train() client id: f_00004-5-7 loss: 0.853893  [  256/  306]
train() client id: f_00004-5-8 loss: 0.989319  [  288/  306]
train() client id: f_00004-6-0 loss: 0.851264  [   32/  306]
train() client id: f_00004-6-1 loss: 0.823577  [   64/  306]
train() client id: f_00004-6-2 loss: 0.892739  [   96/  306]
train() client id: f_00004-6-3 loss: 0.905341  [  128/  306]
train() client id: f_00004-6-4 loss: 0.713476  [  160/  306]
train() client id: f_00004-6-5 loss: 0.751299  [  192/  306]
train() client id: f_00004-6-6 loss: 0.819414  [  224/  306]
train() client id: f_00004-6-7 loss: 0.678289  [  256/  306]
train() client id: f_00004-6-8 loss: 0.795325  [  288/  306]
train() client id: f_00004-7-0 loss: 0.751872  [   32/  306]
train() client id: f_00004-7-1 loss: 0.924068  [   64/  306]
train() client id: f_00004-7-2 loss: 0.927647  [   96/  306]
train() client id: f_00004-7-3 loss: 0.722077  [  128/  306]
train() client id: f_00004-7-4 loss: 0.774528  [  160/  306]
train() client id: f_00004-7-5 loss: 0.858383  [  192/  306]
train() client id: f_00004-7-6 loss: 0.733725  [  224/  306]
train() client id: f_00004-7-7 loss: 0.889902  [  256/  306]
train() client id: f_00004-7-8 loss: 0.801772  [  288/  306]
train() client id: f_00004-8-0 loss: 0.871194  [   32/  306]
train() client id: f_00004-8-1 loss: 0.734940  [   64/  306]
train() client id: f_00004-8-2 loss: 0.782583  [   96/  306]
train() client id: f_00004-8-3 loss: 0.798811  [  128/  306]
train() client id: f_00004-8-4 loss: 0.814496  [  160/  306]
train() client id: f_00004-8-5 loss: 0.846624  [  192/  306]
train() client id: f_00004-8-6 loss: 0.860297  [  224/  306]
train() client id: f_00004-8-7 loss: 0.783505  [  256/  306]
train() client id: f_00004-8-8 loss: 0.875304  [  288/  306]
train() client id: f_00004-9-0 loss: 0.805337  [   32/  306]
train() client id: f_00004-9-1 loss: 0.880108  [   64/  306]
train() client id: f_00004-9-2 loss: 0.770771  [   96/  306]
train() client id: f_00004-9-3 loss: 0.781556  [  128/  306]
train() client id: f_00004-9-4 loss: 0.923334  [  160/  306]
train() client id: f_00004-9-5 loss: 0.740166  [  192/  306]
train() client id: f_00004-9-6 loss: 0.813931  [  224/  306]
train() client id: f_00004-9-7 loss: 0.790562  [  256/  306]
train() client id: f_00004-9-8 loss: 0.863467  [  288/  306]
train() client id: f_00004-10-0 loss: 0.912343  [   32/  306]
train() client id: f_00004-10-1 loss: 0.939919  [   64/  306]
train() client id: f_00004-10-2 loss: 0.840900  [   96/  306]
train() client id: f_00004-10-3 loss: 0.886627  [  128/  306]
train() client id: f_00004-10-4 loss: 0.673947  [  160/  306]
train() client id: f_00004-10-5 loss: 0.747926  [  192/  306]
train() client id: f_00004-10-6 loss: 0.877024  [  224/  306]
train() client id: f_00004-10-7 loss: 0.695345  [  256/  306]
train() client id: f_00004-10-8 loss: 0.824087  [  288/  306]
train() client id: f_00004-11-0 loss: 0.870109  [   32/  306]
train() client id: f_00004-11-1 loss: 0.778495  [   64/  306]
train() client id: f_00004-11-2 loss: 0.739566  [   96/  306]
train() client id: f_00004-11-3 loss: 0.728572  [  128/  306]
train() client id: f_00004-11-4 loss: 0.773648  [  160/  306]
train() client id: f_00004-11-5 loss: 0.804418  [  192/  306]
train() client id: f_00004-11-6 loss: 0.926858  [  224/  306]
train() client id: f_00004-11-7 loss: 0.830408  [  256/  306]
train() client id: f_00004-11-8 loss: 0.844780  [  288/  306]
train() client id: f_00005-0-0 loss: 1.031408  [   32/  146]
train() client id: f_00005-0-1 loss: 0.558226  [   64/  146]
train() client id: f_00005-0-2 loss: 0.718801  [   96/  146]
train() client id: f_00005-0-3 loss: 0.364399  [  128/  146]
train() client id: f_00005-1-0 loss: 0.696795  [   32/  146]
train() client id: f_00005-1-1 loss: 0.574259  [   64/  146]
train() client id: f_00005-1-2 loss: 0.686724  [   96/  146]
train() client id: f_00005-1-3 loss: 0.724879  [  128/  146]
train() client id: f_00005-2-0 loss: 0.738276  [   32/  146]
train() client id: f_00005-2-1 loss: 0.395665  [   64/  146]
train() client id: f_00005-2-2 loss: 0.573770  [   96/  146]
train() client id: f_00005-2-3 loss: 0.782631  [  128/  146]
train() client id: f_00005-3-0 loss: 0.567693  [   32/  146]
train() client id: f_00005-3-1 loss: 0.696164  [   64/  146]
train() client id: f_00005-3-2 loss: 0.509090  [   96/  146]
train() client id: f_00005-3-3 loss: 0.895908  [  128/  146]
train() client id: f_00005-4-0 loss: 0.502816  [   32/  146]
train() client id: f_00005-4-1 loss: 0.403518  [   64/  146]
train() client id: f_00005-4-2 loss: 0.679869  [   96/  146]
train() client id: f_00005-4-3 loss: 0.712159  [  128/  146]
train() client id: f_00005-5-0 loss: 0.843317  [   32/  146]
train() client id: f_00005-5-1 loss: 0.711862  [   64/  146]
train() client id: f_00005-5-2 loss: 0.607324  [   96/  146]
train() client id: f_00005-5-3 loss: 0.554227  [  128/  146]
train() client id: f_00005-6-0 loss: 0.533774  [   32/  146]
train() client id: f_00005-6-1 loss: 0.446883  [   64/  146]
train() client id: f_00005-6-2 loss: 0.878700  [   96/  146]
train() client id: f_00005-6-3 loss: 0.661743  [  128/  146]
train() client id: f_00005-7-0 loss: 0.531689  [   32/  146]
train() client id: f_00005-7-1 loss: 0.941092  [   64/  146]
train() client id: f_00005-7-2 loss: 0.441883  [   96/  146]
train() client id: f_00005-7-3 loss: 0.771490  [  128/  146]
train() client id: f_00005-8-0 loss: 0.693028  [   32/  146]
train() client id: f_00005-8-1 loss: 0.710220  [   64/  146]
train() client id: f_00005-8-2 loss: 0.534593  [   96/  146]
train() client id: f_00005-8-3 loss: 0.721140  [  128/  146]
train() client id: f_00005-9-0 loss: 0.621383  [   32/  146]
train() client id: f_00005-9-1 loss: 0.649052  [   64/  146]
train() client id: f_00005-9-2 loss: 0.612837  [   96/  146]
train() client id: f_00005-9-3 loss: 0.610100  [  128/  146]
train() client id: f_00005-10-0 loss: 0.486005  [   32/  146]
train() client id: f_00005-10-1 loss: 0.638911  [   64/  146]
train() client id: f_00005-10-2 loss: 0.585557  [   96/  146]
train() client id: f_00005-10-3 loss: 0.913991  [  128/  146]
train() client id: f_00005-11-0 loss: 0.625884  [   32/  146]
train() client id: f_00005-11-1 loss: 0.698941  [   64/  146]
train() client id: f_00005-11-2 loss: 0.604308  [   96/  146]
train() client id: f_00005-11-3 loss: 0.532812  [  128/  146]
train() client id: f_00006-0-0 loss: 0.573975  [   32/   54]
train() client id: f_00006-1-0 loss: 0.566732  [   32/   54]
train() client id: f_00006-2-0 loss: 0.572312  [   32/   54]
train() client id: f_00006-3-0 loss: 0.550092  [   32/   54]
train() client id: f_00006-4-0 loss: 0.621457  [   32/   54]
train() client id: f_00006-5-0 loss: 0.553444  [   32/   54]
train() client id: f_00006-6-0 loss: 0.628471  [   32/   54]
train() client id: f_00006-7-0 loss: 0.584974  [   32/   54]
train() client id: f_00006-8-0 loss: 0.555852  [   32/   54]
train() client id: f_00006-9-0 loss: 0.591311  [   32/   54]
train() client id: f_00006-10-0 loss: 0.556050  [   32/   54]
train() client id: f_00006-11-0 loss: 0.612646  [   32/   54]
train() client id: f_00007-0-0 loss: 0.519478  [   32/  179]
train() client id: f_00007-0-1 loss: 0.657300  [   64/  179]
train() client id: f_00007-0-2 loss: 0.563537  [   96/  179]
train() client id: f_00007-0-3 loss: 0.744211  [  128/  179]
train() client id: f_00007-0-4 loss: 0.511106  [  160/  179]
train() client id: f_00007-1-0 loss: 0.499069  [   32/  179]
train() client id: f_00007-1-1 loss: 0.617967  [   64/  179]
train() client id: f_00007-1-2 loss: 0.439061  [   96/  179]
train() client id: f_00007-1-3 loss: 0.436633  [  128/  179]
train() client id: f_00007-1-4 loss: 0.845547  [  160/  179]
train() client id: f_00007-2-0 loss: 0.479379  [   32/  179]
train() client id: f_00007-2-1 loss: 0.474547  [   64/  179]
train() client id: f_00007-2-2 loss: 0.501890  [   96/  179]
train() client id: f_00007-2-3 loss: 0.580206  [  128/  179]
train() client id: f_00007-2-4 loss: 0.654363  [  160/  179]
train() client id: f_00007-3-0 loss: 0.400169  [   32/  179]
train() client id: f_00007-3-1 loss: 0.684266  [   64/  179]
train() client id: f_00007-3-2 loss: 0.702594  [   96/  179]
train() client id: f_00007-3-3 loss: 0.412528  [  128/  179]
train() client id: f_00007-3-4 loss: 0.587316  [  160/  179]
train() client id: f_00007-4-0 loss: 0.665892  [   32/  179]
train() client id: f_00007-4-1 loss: 0.395017  [   64/  179]
train() client id: f_00007-4-2 loss: 0.575903  [   96/  179]
train() client id: f_00007-4-3 loss: 0.407498  [  128/  179]
train() client id: f_00007-4-4 loss: 0.547997  [  160/  179]
train() client id: f_00007-5-0 loss: 0.479831  [   32/  179]
train() client id: f_00007-5-1 loss: 0.488303  [   64/  179]
train() client id: f_00007-5-2 loss: 0.615085  [   96/  179]
train() client id: f_00007-5-3 loss: 0.375814  [  128/  179]
train() client id: f_00007-5-4 loss: 0.515384  [  160/  179]
train() client id: f_00007-6-0 loss: 0.580070  [   32/  179]
train() client id: f_00007-6-1 loss: 0.536943  [   64/  179]
train() client id: f_00007-6-2 loss: 0.474370  [   96/  179]
train() client id: f_00007-6-3 loss: 0.447373  [  128/  179]
train() client id: f_00007-6-4 loss: 0.451688  [  160/  179]
train() client id: f_00007-7-0 loss: 0.347195  [   32/  179]
train() client id: f_00007-7-1 loss: 0.413330  [   64/  179]
train() client id: f_00007-7-2 loss: 0.706354  [   96/  179]
train() client id: f_00007-7-3 loss: 0.422910  [  128/  179]
train() client id: f_00007-7-4 loss: 0.629183  [  160/  179]
train() client id: f_00007-8-0 loss: 0.382543  [   32/  179]
train() client id: f_00007-8-1 loss: 0.362711  [   64/  179]
train() client id: f_00007-8-2 loss: 0.454195  [   96/  179]
train() client id: f_00007-8-3 loss: 0.852935  [  128/  179]
train() client id: f_00007-8-4 loss: 0.561238  [  160/  179]
train() client id: f_00007-9-0 loss: 0.601365  [   32/  179]
train() client id: f_00007-9-1 loss: 0.350567  [   64/  179]
train() client id: f_00007-9-2 loss: 0.660398  [   96/  179]
train() client id: f_00007-9-3 loss: 0.395033  [  128/  179]
train() client id: f_00007-9-4 loss: 0.462500  [  160/  179]
train() client id: f_00007-10-0 loss: 0.458165  [   32/  179]
train() client id: f_00007-10-1 loss: 0.623378  [   64/  179]
train() client id: f_00007-10-2 loss: 0.527072  [   96/  179]
train() client id: f_00007-10-3 loss: 0.351407  [  128/  179]
train() client id: f_00007-10-4 loss: 0.557541  [  160/  179]
train() client id: f_00007-11-0 loss: 0.418324  [   32/  179]
train() client id: f_00007-11-1 loss: 0.577982  [   64/  179]
train() client id: f_00007-11-2 loss: 0.537107  [   96/  179]
train() client id: f_00007-11-3 loss: 0.527137  [  128/  179]
train() client id: f_00007-11-4 loss: 0.451235  [  160/  179]
train() client id: f_00008-0-0 loss: 0.662989  [   32/  130]
train() client id: f_00008-0-1 loss: 0.605315  [   64/  130]
train() client id: f_00008-0-2 loss: 0.715907  [   96/  130]
train() client id: f_00008-0-3 loss: 0.642972  [  128/  130]
train() client id: f_00008-1-0 loss: 0.570580  [   32/  130]
train() client id: f_00008-1-1 loss: 0.637822  [   64/  130]
train() client id: f_00008-1-2 loss: 0.676619  [   96/  130]
train() client id: f_00008-1-3 loss: 0.718028  [  128/  130]
train() client id: f_00008-2-0 loss: 0.618892  [   32/  130]
train() client id: f_00008-2-1 loss: 0.649265  [   64/  130]
train() client id: f_00008-2-2 loss: 0.642094  [   96/  130]
train() client id: f_00008-2-3 loss: 0.688214  [  128/  130]
train() client id: f_00008-3-0 loss: 0.595707  [   32/  130]
train() client id: f_00008-3-1 loss: 0.714463  [   64/  130]
train() client id: f_00008-3-2 loss: 0.691113  [   96/  130]
train() client id: f_00008-3-3 loss: 0.628398  [  128/  130]
train() client id: f_00008-4-0 loss: 0.589269  [   32/  130]
train() client id: f_00008-4-1 loss: 0.780071  [   64/  130]
train() client id: f_00008-4-2 loss: 0.608763  [   96/  130]
train() client id: f_00008-4-3 loss: 0.646045  [  128/  130]
train() client id: f_00008-5-0 loss: 0.638936  [   32/  130]
train() client id: f_00008-5-1 loss: 0.682818  [   64/  130]
train() client id: f_00008-5-2 loss: 0.675815  [   96/  130]
train() client id: f_00008-5-3 loss: 0.614467  [  128/  130]
train() client id: f_00008-6-0 loss: 0.612633  [   32/  130]
train() client id: f_00008-6-1 loss: 0.655858  [   64/  130]
train() client id: f_00008-6-2 loss: 0.642020  [   96/  130]
train() client id: f_00008-6-3 loss: 0.714112  [  128/  130]
train() client id: f_00008-7-0 loss: 0.613466  [   32/  130]
train() client id: f_00008-7-1 loss: 0.662865  [   64/  130]
train() client id: f_00008-7-2 loss: 0.655773  [   96/  130]
train() client id: f_00008-7-3 loss: 0.675609  [  128/  130]
train() client id: f_00008-8-0 loss: 0.743998  [   32/  130]
train() client id: f_00008-8-1 loss: 0.594343  [   64/  130]
train() client id: f_00008-8-2 loss: 0.605838  [   96/  130]
train() client id: f_00008-8-3 loss: 0.683317  [  128/  130]
train() client id: f_00008-9-0 loss: 0.670116  [   32/  130]
train() client id: f_00008-9-1 loss: 0.571744  [   64/  130]
train() client id: f_00008-9-2 loss: 0.647763  [   96/  130]
train() client id: f_00008-9-3 loss: 0.705275  [  128/  130]
train() client id: f_00008-10-0 loss: 0.624882  [   32/  130]
train() client id: f_00008-10-1 loss: 0.654151  [   64/  130]
train() client id: f_00008-10-2 loss: 0.694531  [   96/  130]
train() client id: f_00008-10-3 loss: 0.648109  [  128/  130]
train() client id: f_00008-11-0 loss: 0.662833  [   32/  130]
train() client id: f_00008-11-1 loss: 0.686399  [   64/  130]
train() client id: f_00008-11-2 loss: 0.612464  [   96/  130]
train() client id: f_00008-11-3 loss: 0.654450  [  128/  130]
train() client id: f_00009-0-0 loss: 1.359681  [   32/  118]
train() client id: f_00009-0-1 loss: 1.080932  [   64/  118]
train() client id: f_00009-0-2 loss: 1.097875  [   96/  118]
train() client id: f_00009-1-0 loss: 1.180195  [   32/  118]
train() client id: f_00009-1-1 loss: 1.063465  [   64/  118]
train() client id: f_00009-1-2 loss: 1.084220  [   96/  118]
train() client id: f_00009-2-0 loss: 1.070824  [   32/  118]
train() client id: f_00009-2-1 loss: 1.209907  [   64/  118]
train() client id: f_00009-2-2 loss: 1.071429  [   96/  118]
train() client id: f_00009-3-0 loss: 1.175446  [   32/  118]
train() client id: f_00009-3-1 loss: 1.015318  [   64/  118]
train() client id: f_00009-3-2 loss: 1.041727  [   96/  118]
train() client id: f_00009-4-0 loss: 1.026155  [   32/  118]
train() client id: f_00009-4-1 loss: 1.002502  [   64/  118]
train() client id: f_00009-4-2 loss: 1.111461  [   96/  118]
train() client id: f_00009-5-0 loss: 0.953425  [   32/  118]
train() client id: f_00009-5-1 loss: 0.931709  [   64/  118]
train() client id: f_00009-5-2 loss: 1.098688  [   96/  118]
train() client id: f_00009-6-0 loss: 0.969060  [   32/  118]
train() client id: f_00009-6-1 loss: 1.062553  [   64/  118]
train() client id: f_00009-6-2 loss: 0.899998  [   96/  118]
train() client id: f_00009-7-0 loss: 0.917046  [   32/  118]
train() client id: f_00009-7-1 loss: 0.841837  [   64/  118]
train() client id: f_00009-7-2 loss: 1.031530  [   96/  118]
train() client id: f_00009-8-0 loss: 0.868356  [   32/  118]
train() client id: f_00009-8-1 loss: 1.040143  [   64/  118]
train() client id: f_00009-8-2 loss: 0.905981  [   96/  118]
train() client id: f_00009-9-0 loss: 0.947630  [   32/  118]
train() client id: f_00009-9-1 loss: 0.881153  [   64/  118]
train() client id: f_00009-9-2 loss: 0.986724  [   96/  118]
train() client id: f_00009-10-0 loss: 0.941937  [   32/  118]
train() client id: f_00009-10-1 loss: 0.841980  [   64/  118]
train() client id: f_00009-10-2 loss: 1.087421  [   96/  118]
train() client id: f_00009-11-0 loss: 1.081530  [   32/  118]
train() client id: f_00009-11-1 loss: 0.839353  [   64/  118]
train() client id: f_00009-11-2 loss: 0.913703  [   96/  118]
At round 37 accuracy: 0.649867374005305
At round 37 training accuracy: 0.5868544600938967
At round 37 training loss: 0.8274757949610818
update_location
xs = [  -3.9056584     4.20031788  205.00902392   18.81129433    0.97929623
    3.95640986 -167.44319194 -146.32485185  189.66397685 -132.06087855]
ys = [ 197.5879595   180.55583871    1.32061395 -167.45517586  159.35018685
  142.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [221.4864689  206.44140467 228.10182794 195.94667825 188.13144625
 174.389033   195.04900171 177.23328842 215.13041181 165.69878545]
dists_bs = [172.57893742 179.28642244 417.81740916 393.49506386 176.67379362
 181.82773585 177.7856586  176.5222892  397.19558643 176.30729249]
uav_gains = [1.23220895e-11 1.55193540e-11 1.10553362e-11 1.80819760e-11
 2.02342599e-11 2.47219556e-11 1.83173556e-11 2.37047028e-11
 1.36151003e-11 2.81872466e-11]
bs_gains = [6.02182696e-11 5.41204454e-11 5.06444427e-12 5.99051385e-12
 5.63913003e-11 5.20290288e-11 5.54093755e-11 5.65269226e-11
 5.83554908e-12 5.67201424e-11]
Round 38
-------------------------------
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.97555343 12.35186937  5.88797184  2.12582729 14.24522634  6.85479284
  2.63358293  8.40017663  6.2003918   5.55926836]
obj_prev = 70.23466082817666
eta_min = 3.529677745883019e-16	eta_max = 0.9307450633473059
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 16.289713645049638	eta = 0.9090909090909091
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 30.212175618890367	eta = 0.49016101234197246
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 23.329211198156166	eta = 0.6347763094355793
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.083685449611664	eta = 0.6705778625672812
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.017241376491434	eta = 0.6726015459057763
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.017036233942203	eta = 0.6726078128344527
eta = 0.6726078128344527
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [0.03272589 0.06882827 0.03220643 0.01116836 0.07947717 0.03792048
 0.01402538 0.04649155 0.03376481 0.03064806]
ene_total = [1.98224589 3.51656478 1.97276077 0.93820235 4.0071535  2.08744961
 1.06916394 2.54721356 2.15142878 1.74485306]
ti_comp = [0.50594645 0.53574993 0.50279946 0.51575345 0.53633338 0.53518151
 0.51605241 0.52164089 0.47981336 0.53641515]
ti_coms = [0.10065732 0.07085384 0.10380431 0.09085032 0.07027039 0.07142226
 0.09055136 0.08496288 0.12679041 0.07018862]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [8.55747057e-06 7.09996571e-05 8.25882640e-06 3.27313532e-07
 1.09078086e-04 1.18986925e-05 6.47495169e-07 2.30812175e-05
 1.04502883e-05 6.25297698e-06]
ene_total = [0.46667354 0.33150672 0.48123759 0.42086309 0.33056794 0.33140208
 0.41949304 0.39464461 0.58781812 0.32542594]
optimize_network_iter = 0 obj = 4.0896326721833045
eta = 0.6726078128344527
freqs = [32341261.25530905 64235440.52933019 32027116.84044752 10827227.71361229
 74093064.8966756  35427679.80345908 13589106.57226709 44562788.60807764
 35185358.32378028 28567479.83964483]
eta_min = 0.6726078128344555	eta_max = 0.6726078128344546
af = 0.00954296352207343	bf = 1.3351291140167194	zeta = 0.010497259874280773	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [2.05655253e-06 1.70628135e-05 1.98478162e-06 7.86607991e-08
 2.62139160e-05 2.85952327e-06 1.55607644e-07 5.54693540e-06
 2.51143918e-06 1.50273092e-06]
ene_total = [1.67955348 1.18486183 1.73204102 1.5156187  1.17665519 1.19197501
 1.51064412 1.41831402 2.11559328 1.17116856]
ti_comp = [0.50594645 0.53574993 0.50279946 0.51575345 0.53633338 0.53518151
 0.51605241 0.52164089 0.47981336 0.53641515]
ti_coms = [0.10065732 0.07085384 0.10380431 0.09085032 0.07027039 0.07142226
 0.09055136 0.08496288 0.12679041 0.07018862]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [8.55747057e-06 7.09996571e-05 8.25882640e-06 3.27313532e-07
 1.09078086e-04 1.18986925e-05 6.47495169e-07 2.30812175e-05
 1.04502883e-05 6.25297698e-06]
ene_total = [0.46667354 0.33150672 0.48123759 0.42086309 0.33056794 0.33140208
 0.41949304 0.39464461 0.58781812 0.32542594]
optimize_network_iter = 1 obj = 4.0896326721833285
eta = 0.6726078128344546
freqs = [32341261.25530905 64235440.52933017 32027116.84044754 10827227.71361229
 74093064.89667559 35427679.80345907 13589106.57226709 44562788.60807764
 35185358.32378031 28567479.83964482]
Done!
At round 38 eta: 0.6726078128344546
At round 38 local rounds: 12.986465296587173
At round 38 global rounds: 46.323221096756726
At round 38 a_n: 14.823314824451002
gradient difference: 0.48051485419273376
train() client id: f_00000-0-0 loss: 1.245041  [   32/  126]
train() client id: f_00000-0-1 loss: 1.102079  [   64/  126]
train() client id: f_00000-0-2 loss: 1.267952  [   96/  126]
train() client id: f_00000-1-0 loss: 1.029002  [   32/  126]
train() client id: f_00000-1-1 loss: 1.024207  [   64/  126]
train() client id: f_00000-1-2 loss: 1.040468  [   96/  126]
train() client id: f_00000-2-0 loss: 1.074339  [   32/  126]
train() client id: f_00000-2-1 loss: 1.127221  [   64/  126]
train() client id: f_00000-2-2 loss: 0.962828  [   96/  126]
train() client id: f_00000-3-0 loss: 1.034628  [   32/  126]
train() client id: f_00000-3-1 loss: 1.029265  [   64/  126]
train() client id: f_00000-3-2 loss: 0.922657  [   96/  126]
train() client id: f_00000-4-0 loss: 0.957502  [   32/  126]
train() client id: f_00000-4-1 loss: 0.853036  [   64/  126]
train() client id: f_00000-4-2 loss: 0.949913  [   96/  126]
train() client id: f_00000-5-0 loss: 1.017811  [   32/  126]
train() client id: f_00000-5-1 loss: 0.889052  [   64/  126]
train() client id: f_00000-5-2 loss: 0.912167  [   96/  126]
train() client id: f_00000-6-0 loss: 0.882285  [   32/  126]
train() client id: f_00000-6-1 loss: 0.811175  [   64/  126]
train() client id: f_00000-6-2 loss: 0.965075  [   96/  126]
train() client id: f_00000-7-0 loss: 0.853767  [   32/  126]
train() client id: f_00000-7-1 loss: 0.858406  [   64/  126]
train() client id: f_00000-7-2 loss: 0.895945  [   96/  126]
train() client id: f_00000-8-0 loss: 0.832935  [   32/  126]
train() client id: f_00000-8-1 loss: 0.836924  [   64/  126]
train() client id: f_00000-8-2 loss: 1.039111  [   96/  126]
train() client id: f_00000-9-0 loss: 0.874281  [   32/  126]
train() client id: f_00000-9-1 loss: 0.945843  [   64/  126]
train() client id: f_00000-9-2 loss: 0.826863  [   96/  126]
train() client id: f_00000-10-0 loss: 1.033065  [   32/  126]
train() client id: f_00000-10-1 loss: 0.898515  [   64/  126]
train() client id: f_00000-10-2 loss: 0.798370  [   96/  126]
train() client id: f_00000-11-0 loss: 0.883243  [   32/  126]
train() client id: f_00000-11-1 loss: 0.979713  [   64/  126]
train() client id: f_00000-11-2 loss: 0.790066  [   96/  126]
train() client id: f_00001-0-0 loss: 0.329709  [   32/  265]
train() client id: f_00001-0-1 loss: 0.298002  [   64/  265]
train() client id: f_00001-0-2 loss: 0.185130  [   96/  265]
train() client id: f_00001-0-3 loss: 0.203204  [  128/  265]
train() client id: f_00001-0-4 loss: 0.283558  [  160/  265]
train() client id: f_00001-0-5 loss: 0.195593  [  192/  265]
train() client id: f_00001-0-6 loss: 0.278965  [  224/  265]
train() client id: f_00001-0-7 loss: 0.215609  [  256/  265]
train() client id: f_00001-1-0 loss: 0.308501  [   32/  265]
train() client id: f_00001-1-1 loss: 0.254225  [   64/  265]
train() client id: f_00001-1-2 loss: 0.143526  [   96/  265]
train() client id: f_00001-1-3 loss: 0.414658  [  128/  265]
train() client id: f_00001-1-4 loss: 0.244864  [  160/  265]
train() client id: f_00001-1-5 loss: 0.210993  [  192/  265]
train() client id: f_00001-1-6 loss: 0.144772  [  224/  265]
train() client id: f_00001-1-7 loss: 0.175385  [  256/  265]
train() client id: f_00001-2-0 loss: 0.190684  [   32/  265]
train() client id: f_00001-2-1 loss: 0.171242  [   64/  265]
train() client id: f_00001-2-2 loss: 0.305146  [   96/  265]
train() client id: f_00001-2-3 loss: 0.183590  [  128/  265]
train() client id: f_00001-2-4 loss: 0.208970  [  160/  265]
train() client id: f_00001-2-5 loss: 0.217156  [  192/  265]
train() client id: f_00001-2-6 loss: 0.246015  [  224/  265]
train() client id: f_00001-2-7 loss: 0.286862  [  256/  265]
train() client id: f_00001-3-0 loss: 0.116836  [   32/  265]
train() client id: f_00001-3-1 loss: 0.129894  [   64/  265]
train() client id: f_00001-3-2 loss: 0.133871  [   96/  265]
train() client id: f_00001-3-3 loss: 0.240643  [  128/  265]
train() client id: f_00001-3-4 loss: 0.222392  [  160/  265]
train() client id: f_00001-3-5 loss: 0.345660  [  192/  265]
train() client id: f_00001-3-6 loss: 0.150916  [  224/  265]
train() client id: f_00001-3-7 loss: 0.390604  [  256/  265]
train() client id: f_00001-4-0 loss: 0.134911  [   32/  265]
train() client id: f_00001-4-1 loss: 0.368042  [   64/  265]
train() client id: f_00001-4-2 loss: 0.152103  [   96/  265]
train() client id: f_00001-4-3 loss: 0.193447  [  128/  265]
train() client id: f_00001-4-4 loss: 0.176223  [  160/  265]
train() client id: f_00001-4-5 loss: 0.131462  [  192/  265]
train() client id: f_00001-4-6 loss: 0.324711  [  224/  265]
train() client id: f_00001-4-7 loss: 0.196464  [  256/  265]
train() client id: f_00001-5-0 loss: 0.174701  [   32/  265]
train() client id: f_00001-5-1 loss: 0.122598  [   64/  265]
train() client id: f_00001-5-2 loss: 0.281916  [   96/  265]
train() client id: f_00001-5-3 loss: 0.155883  [  128/  265]
train() client id: f_00001-5-4 loss: 0.169776  [  160/  265]
train() client id: f_00001-5-5 loss: 0.123976  [  192/  265]
train() client id: f_00001-5-6 loss: 0.186867  [  224/  265]
train() client id: f_00001-5-7 loss: 0.308781  [  256/  265]
train() client id: f_00001-6-0 loss: 0.219574  [   32/  265]
train() client id: f_00001-6-1 loss: 0.131686  [   64/  265]
train() client id: f_00001-6-2 loss: 0.237023  [   96/  265]
train() client id: f_00001-6-3 loss: 0.093330  [  128/  265]
train() client id: f_00001-6-4 loss: 0.244435  [  160/  265]
train() client id: f_00001-6-5 loss: 0.209911  [  192/  265]
train() client id: f_00001-6-6 loss: 0.150603  [  224/  265]
train() client id: f_00001-6-7 loss: 0.286359  [  256/  265]
train() client id: f_00001-7-0 loss: 0.270746  [   32/  265]
train() client id: f_00001-7-1 loss: 0.156632  [   64/  265]
train() client id: f_00001-7-2 loss: 0.175534  [   96/  265]
train() client id: f_00001-7-3 loss: 0.085077  [  128/  265]
train() client id: f_00001-7-4 loss: 0.272773  [  160/  265]
train() client id: f_00001-7-5 loss: 0.332558  [  192/  265]
train() client id: f_00001-7-6 loss: 0.102753  [  224/  265]
train() client id: f_00001-7-7 loss: 0.148460  [  256/  265]
train() client id: f_00001-8-0 loss: 0.080220  [   32/  265]
train() client id: f_00001-8-1 loss: 0.101539  [   64/  265]
train() client id: f_00001-8-2 loss: 0.092080  [   96/  265]
train() client id: f_00001-8-3 loss: 0.166102  [  128/  265]
train() client id: f_00001-8-4 loss: 0.198544  [  160/  265]
train() client id: f_00001-8-5 loss: 0.247111  [  192/  265]
train() client id: f_00001-8-6 loss: 0.442423  [  224/  265]
train() client id: f_00001-8-7 loss: 0.132212  [  256/  265]
train() client id: f_00001-9-0 loss: 0.168844  [   32/  265]
train() client id: f_00001-9-1 loss: 0.111994  [   64/  265]
train() client id: f_00001-9-2 loss: 0.215516  [   96/  265]
train() client id: f_00001-9-3 loss: 0.186482  [  128/  265]
train() client id: f_00001-9-4 loss: 0.266714  [  160/  265]
train() client id: f_00001-9-5 loss: 0.163105  [  192/  265]
train() client id: f_00001-9-6 loss: 0.176601  [  224/  265]
train() client id: f_00001-9-7 loss: 0.194613  [  256/  265]
train() client id: f_00001-10-0 loss: 0.127710  [   32/  265]
train() client id: f_00001-10-1 loss: 0.094894  [   64/  265]
train() client id: f_00001-10-2 loss: 0.352414  [   96/  265]
train() client id: f_00001-10-3 loss: 0.127313  [  128/  265]
train() client id: f_00001-10-4 loss: 0.086341  [  160/  265]
train() client id: f_00001-10-5 loss: 0.142332  [  192/  265]
train() client id: f_00001-10-6 loss: 0.293908  [  224/  265]
train() client id: f_00001-10-7 loss: 0.242139  [  256/  265]
train() client id: f_00001-11-0 loss: 0.224672  [   32/  265]
train() client id: f_00001-11-1 loss: 0.323326  [   64/  265]
train() client id: f_00001-11-2 loss: 0.090220  [   96/  265]
train() client id: f_00001-11-3 loss: 0.150236  [  128/  265]
train() client id: f_00001-11-4 loss: 0.168162  [  160/  265]
train() client id: f_00001-11-5 loss: 0.187224  [  192/  265]
train() client id: f_00001-11-6 loss: 0.142937  [  224/  265]
train() client id: f_00001-11-7 loss: 0.153598  [  256/  265]
train() client id: f_00002-0-0 loss: 1.135016  [   32/  124]
train() client id: f_00002-0-1 loss: 1.171409  [   64/  124]
train() client id: f_00002-0-2 loss: 1.069723  [   96/  124]
train() client id: f_00002-1-0 loss: 1.118223  [   32/  124]
train() client id: f_00002-1-1 loss: 1.038014  [   64/  124]
train() client id: f_00002-1-2 loss: 0.920047  [   96/  124]
train() client id: f_00002-2-0 loss: 1.014617  [   32/  124]
train() client id: f_00002-2-1 loss: 0.887578  [   64/  124]
train() client id: f_00002-2-2 loss: 0.995998  [   96/  124]
train() client id: f_00002-3-0 loss: 0.932555  [   32/  124]
train() client id: f_00002-3-1 loss: 1.047581  [   64/  124]
train() client id: f_00002-3-2 loss: 0.975922  [   96/  124]
train() client id: f_00002-4-0 loss: 0.897166  [   32/  124]
train() client id: f_00002-4-1 loss: 0.966450  [   64/  124]
train() client id: f_00002-4-2 loss: 1.045605  [   96/  124]
train() client id: f_00002-5-0 loss: 1.023813  [   32/  124]
train() client id: f_00002-5-1 loss: 0.878474  [   64/  124]
train() client id: f_00002-5-2 loss: 0.820187  [   96/  124]
train() client id: f_00002-6-0 loss: 0.847339  [   32/  124]
train() client id: f_00002-6-1 loss: 0.895735  [   64/  124]
train() client id: f_00002-6-2 loss: 0.860707  [   96/  124]
train() client id: f_00002-7-0 loss: 0.859908  [   32/  124]
train() client id: f_00002-7-1 loss: 0.928422  [   64/  124]
train() client id: f_00002-7-2 loss: 0.961021  [   96/  124]
train() client id: f_00002-8-0 loss: 0.815713  [   32/  124]
train() client id: f_00002-8-1 loss: 0.879965  [   64/  124]
train() client id: f_00002-8-2 loss: 0.829037  [   96/  124]
train() client id: f_00002-9-0 loss: 0.803646  [   32/  124]
train() client id: f_00002-9-1 loss: 0.883527  [   64/  124]
train() client id: f_00002-9-2 loss: 0.704128  [   96/  124]
train() client id: f_00002-10-0 loss: 0.720545  [   32/  124]
train() client id: f_00002-10-1 loss: 0.943139  [   64/  124]
train() client id: f_00002-10-2 loss: 0.999672  [   96/  124]
train() client id: f_00002-11-0 loss: 0.824940  [   32/  124]
train() client id: f_00002-11-1 loss: 1.070472  [   64/  124]
train() client id: f_00002-11-2 loss: 0.904410  [   96/  124]
train() client id: f_00003-0-0 loss: 0.552033  [   32/   43]
train() client id: f_00003-1-0 loss: 0.523865  [   32/   43]
train() client id: f_00003-2-0 loss: 0.471571  [   32/   43]
train() client id: f_00003-3-0 loss: 0.512609  [   32/   43]
train() client id: f_00003-4-0 loss: 0.322544  [   32/   43]
train() client id: f_00003-5-0 loss: 0.643279  [   32/   43]
train() client id: f_00003-6-0 loss: 0.658864  [   32/   43]
train() client id: f_00003-7-0 loss: 0.518199  [   32/   43]
train() client id: f_00003-8-0 loss: 0.404130  [   32/   43]
train() client id: f_00003-9-0 loss: 0.441306  [   32/   43]
train() client id: f_00003-10-0 loss: 0.557502  [   32/   43]
train() client id: f_00003-11-0 loss: 0.354056  [   32/   43]
train() client id: f_00004-0-0 loss: 0.726299  [   32/  306]
train() client id: f_00004-0-1 loss: 0.826254  [   64/  306]
train() client id: f_00004-0-2 loss: 0.886954  [   96/  306]
train() client id: f_00004-0-3 loss: 0.887611  [  128/  306]
train() client id: f_00004-0-4 loss: 0.730293  [  160/  306]
train() client id: f_00004-0-5 loss: 0.875469  [  192/  306]
train() client id: f_00004-0-6 loss: 0.899707  [  224/  306]
train() client id: f_00004-0-7 loss: 0.712841  [  256/  306]
train() client id: f_00004-0-8 loss: 0.980418  [  288/  306]
train() client id: f_00004-1-0 loss: 0.850511  [   32/  306]
train() client id: f_00004-1-1 loss: 0.958222  [   64/  306]
train() client id: f_00004-1-2 loss: 0.945731  [   96/  306]
train() client id: f_00004-1-3 loss: 1.034973  [  128/  306]
train() client id: f_00004-1-4 loss: 0.757511  [  160/  306]
train() client id: f_00004-1-5 loss: 0.750818  [  192/  306]
train() client id: f_00004-1-6 loss: 0.660256  [  224/  306]
train() client id: f_00004-1-7 loss: 0.847113  [  256/  306]
train() client id: f_00004-1-8 loss: 0.749991  [  288/  306]
train() client id: f_00004-2-0 loss: 0.960516  [   32/  306]
train() client id: f_00004-2-1 loss: 0.822589  [   64/  306]
train() client id: f_00004-2-2 loss: 0.837171  [   96/  306]
train() client id: f_00004-2-3 loss: 0.769448  [  128/  306]
train() client id: f_00004-2-4 loss: 0.921183  [  160/  306]
train() client id: f_00004-2-5 loss: 0.910816  [  192/  306]
train() client id: f_00004-2-6 loss: 0.715452  [  224/  306]
train() client id: f_00004-2-7 loss: 0.844385  [  256/  306]
train() client id: f_00004-2-8 loss: 0.827322  [  288/  306]
train() client id: f_00004-3-0 loss: 0.908132  [   32/  306]
train() client id: f_00004-3-1 loss: 0.787159  [   64/  306]
train() client id: f_00004-3-2 loss: 0.708735  [   96/  306]
train() client id: f_00004-3-3 loss: 0.896535  [  128/  306]
train() client id: f_00004-3-4 loss: 0.866697  [  160/  306]
train() client id: f_00004-3-5 loss: 0.784749  [  192/  306]
train() client id: f_00004-3-6 loss: 0.851095  [  224/  306]
train() client id: f_00004-3-7 loss: 0.816942  [  256/  306]
train() client id: f_00004-3-8 loss: 0.900691  [  288/  306]
train() client id: f_00004-4-0 loss: 0.902827  [   32/  306]
train() client id: f_00004-4-1 loss: 0.832211  [   64/  306]
train() client id: f_00004-4-2 loss: 0.846812  [   96/  306]
train() client id: f_00004-4-3 loss: 0.791084  [  128/  306]
train() client id: f_00004-4-4 loss: 0.751937  [  160/  306]
train() client id: f_00004-4-5 loss: 0.796110  [  192/  306]
train() client id: f_00004-4-6 loss: 0.879439  [  224/  306]
train() client id: f_00004-4-7 loss: 0.840530  [  256/  306]
train() client id: f_00004-4-8 loss: 0.846347  [  288/  306]
train() client id: f_00004-5-0 loss: 0.956090  [   32/  306]
train() client id: f_00004-5-1 loss: 0.726972  [   64/  306]
train() client id: f_00004-5-2 loss: 0.792862  [   96/  306]
train() client id: f_00004-5-3 loss: 0.879245  [  128/  306]
train() client id: f_00004-5-4 loss: 0.842074  [  160/  306]
train() client id: f_00004-5-5 loss: 0.940922  [  192/  306]
train() client id: f_00004-5-6 loss: 0.830281  [  224/  306]
train() client id: f_00004-5-7 loss: 0.812071  [  256/  306]
train() client id: f_00004-5-8 loss: 0.786632  [  288/  306]
train() client id: f_00004-6-0 loss: 0.944901  [   32/  306]
train() client id: f_00004-6-1 loss: 0.765253  [   64/  306]
train() client id: f_00004-6-2 loss: 0.856859  [   96/  306]
train() client id: f_00004-6-3 loss: 0.674218  [  128/  306]
train() client id: f_00004-6-4 loss: 0.884655  [  160/  306]
train() client id: f_00004-6-5 loss: 0.822748  [  192/  306]
train() client id: f_00004-6-6 loss: 0.788061  [  224/  306]
train() client id: f_00004-6-7 loss: 0.885437  [  256/  306]
train() client id: f_00004-6-8 loss: 0.872386  [  288/  306]
train() client id: f_00004-7-0 loss: 0.815854  [   32/  306]
train() client id: f_00004-7-1 loss: 0.651054  [   64/  306]
train() client id: f_00004-7-2 loss: 0.798457  [   96/  306]
train() client id: f_00004-7-3 loss: 0.901419  [  128/  306]
train() client id: f_00004-7-4 loss: 0.916528  [  160/  306]
train() client id: f_00004-7-5 loss: 0.833620  [  192/  306]
train() client id: f_00004-7-6 loss: 0.919962  [  224/  306]
train() client id: f_00004-7-7 loss: 0.815303  [  256/  306]
train() client id: f_00004-7-8 loss: 0.874523  [  288/  306]
train() client id: f_00004-8-0 loss: 0.846494  [   32/  306]
train() client id: f_00004-8-1 loss: 0.831247  [   64/  306]
train() client id: f_00004-8-2 loss: 0.867732  [   96/  306]
train() client id: f_00004-8-3 loss: 0.883157  [  128/  306]
train() client id: f_00004-8-4 loss: 0.926762  [  160/  306]
train() client id: f_00004-8-5 loss: 0.732654  [  192/  306]
train() client id: f_00004-8-6 loss: 0.866526  [  224/  306]
train() client id: f_00004-8-7 loss: 0.823967  [  256/  306]
train() client id: f_00004-8-8 loss: 0.691261  [  288/  306]
train() client id: f_00004-9-0 loss: 0.739756  [   32/  306]
train() client id: f_00004-9-1 loss: 0.866905  [   64/  306]
train() client id: f_00004-9-2 loss: 0.795283  [   96/  306]
train() client id: f_00004-9-3 loss: 0.836575  [  128/  306]
train() client id: f_00004-9-4 loss: 0.920531  [  160/  306]
train() client id: f_00004-9-5 loss: 0.763304  [  192/  306]
train() client id: f_00004-9-6 loss: 0.896136  [  224/  306]
train() client id: f_00004-9-7 loss: 0.904686  [  256/  306]
train() client id: f_00004-9-8 loss: 0.820077  [  288/  306]
train() client id: f_00004-10-0 loss: 0.812732  [   32/  306]
train() client id: f_00004-10-1 loss: 0.873321  [   64/  306]
train() client id: f_00004-10-2 loss: 0.834938  [   96/  306]
train() client id: f_00004-10-3 loss: 0.790104  [  128/  306]
train() client id: f_00004-10-4 loss: 0.852761  [  160/  306]
train() client id: f_00004-10-5 loss: 0.918093  [  192/  306]
train() client id: f_00004-10-6 loss: 0.825373  [  224/  306]
train() client id: f_00004-10-7 loss: 0.772341  [  256/  306]
train() client id: f_00004-10-8 loss: 0.832810  [  288/  306]
train() client id: f_00004-11-0 loss: 0.800039  [   32/  306]
train() client id: f_00004-11-1 loss: 0.874519  [   64/  306]
train() client id: f_00004-11-2 loss: 0.906717  [   96/  306]
train() client id: f_00004-11-3 loss: 0.787941  [  128/  306]
train() client id: f_00004-11-4 loss: 0.803381  [  160/  306]
train() client id: f_00004-11-5 loss: 0.793425  [  192/  306]
train() client id: f_00004-11-6 loss: 0.849609  [  224/  306]
train() client id: f_00004-11-7 loss: 0.946391  [  256/  306]
train() client id: f_00004-11-8 loss: 0.730062  [  288/  306]
train() client id: f_00005-0-0 loss: 0.691159  [   32/  146]
train() client id: f_00005-0-1 loss: 1.058456  [   64/  146]
train() client id: f_00005-0-2 loss: 0.591632  [   96/  146]
train() client id: f_00005-0-3 loss: 0.475665  [  128/  146]
train() client id: f_00005-1-0 loss: 0.686428  [   32/  146]
train() client id: f_00005-1-1 loss: 0.865846  [   64/  146]
train() client id: f_00005-1-2 loss: 0.558405  [   96/  146]
train() client id: f_00005-1-3 loss: 0.688178  [  128/  146]
train() client id: f_00005-2-0 loss: 0.948308  [   32/  146]
train() client id: f_00005-2-1 loss: 0.424526  [   64/  146]
train() client id: f_00005-2-2 loss: 0.455544  [   96/  146]
train() client id: f_00005-2-3 loss: 0.738593  [  128/  146]
train() client id: f_00005-3-0 loss: 0.714038  [   32/  146]
train() client id: f_00005-3-1 loss: 0.537123  [   64/  146]
train() client id: f_00005-3-2 loss: 0.837431  [   96/  146]
train() client id: f_00005-3-3 loss: 0.793220  [  128/  146]
train() client id: f_00005-4-0 loss: 0.643288  [   32/  146]
train() client id: f_00005-4-1 loss: 0.795735  [   64/  146]
train() client id: f_00005-4-2 loss: 0.570787  [   96/  146]
train() client id: f_00005-4-3 loss: 0.703527  [  128/  146]
train() client id: f_00005-5-0 loss: 0.714628  [   32/  146]
train() client id: f_00005-5-1 loss: 0.789031  [   64/  146]
train() client id: f_00005-5-2 loss: 0.707785  [   96/  146]
train() client id: f_00005-5-3 loss: 0.723904  [  128/  146]
train() client id: f_00005-6-0 loss: 0.736331  [   32/  146]
train() client id: f_00005-6-1 loss: 0.656606  [   64/  146]
train() client id: f_00005-6-2 loss: 0.668945  [   96/  146]
train() client id: f_00005-6-3 loss: 0.790907  [  128/  146]
train() client id: f_00005-7-0 loss: 0.458669  [   32/  146]
train() client id: f_00005-7-1 loss: 0.577161  [   64/  146]
train() client id: f_00005-7-2 loss: 0.754076  [   96/  146]
train() client id: f_00005-7-3 loss: 0.884036  [  128/  146]
train() client id: f_00005-8-0 loss: 0.662113  [   32/  146]
train() client id: f_00005-8-1 loss: 0.654242  [   64/  146]
train() client id: f_00005-8-2 loss: 0.922484  [   96/  146]
train() client id: f_00005-8-3 loss: 0.682944  [  128/  146]
train() client id: f_00005-9-0 loss: 0.841011  [   32/  146]
train() client id: f_00005-9-1 loss: 0.732252  [   64/  146]
train() client id: f_00005-9-2 loss: 0.671574  [   96/  146]
train() client id: f_00005-9-3 loss: 0.689764  [  128/  146]
train() client id: f_00005-10-0 loss: 0.637513  [   32/  146]
train() client id: f_00005-10-1 loss: 0.589424  [   64/  146]
train() client id: f_00005-10-2 loss: 0.583709  [   96/  146]
train() client id: f_00005-10-3 loss: 0.884093  [  128/  146]
train() client id: f_00005-11-0 loss: 0.905706  [   32/  146]
train() client id: f_00005-11-1 loss: 0.630078  [   64/  146]
train() client id: f_00005-11-2 loss: 0.715102  [   96/  146]
train() client id: f_00005-11-3 loss: 0.789305  [  128/  146]
train() client id: f_00006-0-0 loss: 0.496980  [   32/   54]
train() client id: f_00006-1-0 loss: 0.451114  [   32/   54]
train() client id: f_00006-2-0 loss: 0.523119  [   32/   54]
train() client id: f_00006-3-0 loss: 0.470016  [   32/   54]
train() client id: f_00006-4-0 loss: 0.515672  [   32/   54]
train() client id: f_00006-5-0 loss: 0.518559  [   32/   54]
train() client id: f_00006-6-0 loss: 0.518913  [   32/   54]
train() client id: f_00006-7-0 loss: 0.533880  [   32/   54]
train() client id: f_00006-8-0 loss: 0.540448  [   32/   54]
train() client id: f_00006-9-0 loss: 0.470293  [   32/   54]
train() client id: f_00006-10-0 loss: 0.524861  [   32/   54]
train() client id: f_00006-11-0 loss: 0.479984  [   32/   54]
train() client id: f_00007-0-0 loss: 0.537805  [   32/  179]
train() client id: f_00007-0-1 loss: 0.752939  [   64/  179]
train() client id: f_00007-0-2 loss: 0.675725  [   96/  179]
train() client id: f_00007-0-3 loss: 0.611723  [  128/  179]
train() client id: f_00007-0-4 loss: 0.574752  [  160/  179]
train() client id: f_00007-1-0 loss: 0.499117  [   32/  179]
train() client id: f_00007-1-1 loss: 0.499532  [   64/  179]
train() client id: f_00007-1-2 loss: 0.858204  [   96/  179]
train() client id: f_00007-1-3 loss: 0.590211  [  128/  179]
train() client id: f_00007-1-4 loss: 0.844124  [  160/  179]
train() client id: f_00007-2-0 loss: 0.487564  [   32/  179]
train() client id: f_00007-2-1 loss: 0.782015  [   64/  179]
train() client id: f_00007-2-2 loss: 0.766827  [   96/  179]
train() client id: f_00007-2-3 loss: 0.517321  [  128/  179]
train() client id: f_00007-2-4 loss: 0.616733  [  160/  179]
train() client id: f_00007-3-0 loss: 0.614805  [   32/  179]
train() client id: f_00007-3-1 loss: 0.593768  [   64/  179]
train() client id: f_00007-3-2 loss: 0.580142  [   96/  179]
train() client id: f_00007-3-3 loss: 0.736783  [  128/  179]
train() client id: f_00007-3-4 loss: 0.589642  [  160/  179]
train() client id: f_00007-4-0 loss: 0.613154  [   32/  179]
train() client id: f_00007-4-1 loss: 0.492834  [   64/  179]
train() client id: f_00007-4-2 loss: 0.501531  [   96/  179]
train() client id: f_00007-4-3 loss: 0.736655  [  128/  179]
train() client id: f_00007-4-4 loss: 0.826046  [  160/  179]
train() client id: f_00007-5-0 loss: 0.661634  [   32/  179]
train() client id: f_00007-5-1 loss: 0.488215  [   64/  179]
train() client id: f_00007-5-2 loss: 0.544754  [   96/  179]
train() client id: f_00007-5-3 loss: 0.692930  [  128/  179]
train() client id: f_00007-5-4 loss: 0.646662  [  160/  179]
train() client id: f_00007-6-0 loss: 0.805238  [   32/  179]
train() client id: f_00007-6-1 loss: 0.572325  [   64/  179]
train() client id: f_00007-6-2 loss: 0.573655  [   96/  179]
train() client id: f_00007-6-3 loss: 0.441577  [  128/  179]
train() client id: f_00007-6-4 loss: 0.582788  [  160/  179]
train() client id: f_00007-7-0 loss: 0.550873  [   32/  179]
train() client id: f_00007-7-1 loss: 0.634688  [   64/  179]
train() client id: f_00007-7-2 loss: 0.465267  [   96/  179]
train() client id: f_00007-7-3 loss: 0.794365  [  128/  179]
train() client id: f_00007-7-4 loss: 0.476321  [  160/  179]
train() client id: f_00007-8-0 loss: 0.836786  [   32/  179]
train() client id: f_00007-8-1 loss: 0.589066  [   64/  179]
train() client id: f_00007-8-2 loss: 0.678041  [   96/  179]
train() client id: f_00007-8-3 loss: 0.463466  [  128/  179]
train() client id: f_00007-8-4 loss: 0.434577  [  160/  179]
train() client id: f_00007-9-0 loss: 0.576465  [   32/  179]
train() client id: f_00007-9-1 loss: 0.523903  [   64/  179]
train() client id: f_00007-9-2 loss: 0.561758  [   96/  179]
train() client id: f_00007-9-3 loss: 0.766471  [  128/  179]
train() client id: f_00007-9-4 loss: 0.453425  [  160/  179]
train() client id: f_00007-10-0 loss: 0.562728  [   32/  179]
train() client id: f_00007-10-1 loss: 0.589705  [   64/  179]
train() client id: f_00007-10-2 loss: 0.520986  [   96/  179]
train() client id: f_00007-10-3 loss: 0.782097  [  128/  179]
train() client id: f_00007-10-4 loss: 0.646849  [  160/  179]
train() client id: f_00007-11-0 loss: 0.764342  [   32/  179]
train() client id: f_00007-11-1 loss: 0.532978  [   64/  179]
train() client id: f_00007-11-2 loss: 0.452675  [   96/  179]
train() client id: f_00007-11-3 loss: 0.565489  [  128/  179]
train() client id: f_00007-11-4 loss: 0.728374  [  160/  179]
train() client id: f_00008-0-0 loss: 0.898037  [   32/  130]
train() client id: f_00008-0-1 loss: 0.726452  [   64/  130]
train() client id: f_00008-0-2 loss: 0.763798  [   96/  130]
train() client id: f_00008-0-3 loss: 0.713725  [  128/  130]
train() client id: f_00008-1-0 loss: 0.833034  [   32/  130]
train() client id: f_00008-1-1 loss: 0.702694  [   64/  130]
train() client id: f_00008-1-2 loss: 0.749898  [   96/  130]
train() client id: f_00008-1-3 loss: 0.855358  [  128/  130]
train() client id: f_00008-2-0 loss: 0.869257  [   32/  130]
train() client id: f_00008-2-1 loss: 0.799243  [   64/  130]
train() client id: f_00008-2-2 loss: 0.743475  [   96/  130]
train() client id: f_00008-2-3 loss: 0.726925  [  128/  130]
train() client id: f_00008-3-0 loss: 0.782256  [   32/  130]
train() client id: f_00008-3-1 loss: 0.900074  [   64/  130]
train() client id: f_00008-3-2 loss: 0.691595  [   96/  130]
train() client id: f_00008-3-3 loss: 0.737975  [  128/  130]
train() client id: f_00008-4-0 loss: 0.762131  [   32/  130]
train() client id: f_00008-4-1 loss: 0.728897  [   64/  130]
train() client id: f_00008-4-2 loss: 0.940540  [   96/  130]
train() client id: f_00008-4-3 loss: 0.709535  [  128/  130]
train() client id: f_00008-5-0 loss: 0.787350  [   32/  130]
train() client id: f_00008-5-1 loss: 0.723495  [   64/  130]
train() client id: f_00008-5-2 loss: 0.849847  [   96/  130]
train() client id: f_00008-5-3 loss: 0.764924  [  128/  130]
train() client id: f_00008-6-0 loss: 0.626213  [   32/  130]
train() client id: f_00008-6-1 loss: 0.702170  [   64/  130]
train() client id: f_00008-6-2 loss: 0.947535  [   96/  130]
train() client id: f_00008-6-3 loss: 0.862550  [  128/  130]
train() client id: f_00008-7-0 loss: 0.763203  [   32/  130]
train() client id: f_00008-7-1 loss: 0.829892  [   64/  130]
train() client id: f_00008-7-2 loss: 0.763861  [   96/  130]
train() client id: f_00008-7-3 loss: 0.742503  [  128/  130]
train() client id: f_00008-8-0 loss: 0.660288  [   32/  130]
train() client id: f_00008-8-1 loss: 0.949025  [   64/  130]
train() client id: f_00008-8-2 loss: 0.821494  [   96/  130]
train() client id: f_00008-8-3 loss: 0.703579  [  128/  130]
train() client id: f_00008-9-0 loss: 0.820462  [   32/  130]
train() client id: f_00008-9-1 loss: 0.865430  [   64/  130]
train() client id: f_00008-9-2 loss: 0.746925  [   96/  130]
train() client id: f_00008-9-3 loss: 0.705106  [  128/  130]
train() client id: f_00008-10-0 loss: 0.719729  [   32/  130]
train() client id: f_00008-10-1 loss: 0.709683  [   64/  130]
train() client id: f_00008-10-2 loss: 0.907172  [   96/  130]
train() client id: f_00008-10-3 loss: 0.799947  [  128/  130]
train() client id: f_00008-11-0 loss: 0.662162  [   32/  130]
train() client id: f_00008-11-1 loss: 0.865288  [   64/  130]
train() client id: f_00008-11-2 loss: 0.739673  [   96/  130]
train() client id: f_00008-11-3 loss: 0.870612  [  128/  130]
train() client id: f_00009-0-0 loss: 1.089506  [   32/  118]
train() client id: f_00009-0-1 loss: 1.044528  [   64/  118]
train() client id: f_00009-0-2 loss: 1.061526  [   96/  118]
train() client id: f_00009-1-0 loss: 0.955757  [   32/  118]
train() client id: f_00009-1-1 loss: 1.105645  [   64/  118]
train() client id: f_00009-1-2 loss: 1.011957  [   96/  118]
train() client id: f_00009-2-0 loss: 0.952966  [   32/  118]
train() client id: f_00009-2-1 loss: 0.962279  [   64/  118]
train() client id: f_00009-2-2 loss: 1.126995  [   96/  118]
train() client id: f_00009-3-0 loss: 0.970620  [   32/  118]
train() client id: f_00009-3-1 loss: 1.029048  [   64/  118]
train() client id: f_00009-3-2 loss: 0.819227  [   96/  118]
train() client id: f_00009-4-0 loss: 0.962359  [   32/  118]
train() client id: f_00009-4-1 loss: 1.034200  [   64/  118]
train() client id: f_00009-4-2 loss: 0.784899  [   96/  118]
train() client id: f_00009-5-0 loss: 0.995767  [   32/  118]
train() client id: f_00009-5-1 loss: 0.811376  [   64/  118]
train() client id: f_00009-5-2 loss: 0.859985  [   96/  118]
train() client id: f_00009-6-0 loss: 1.010353  [   32/  118]
train() client id: f_00009-6-1 loss: 0.907729  [   64/  118]
train() client id: f_00009-6-2 loss: 0.872314  [   96/  118]
train() client id: f_00009-7-0 loss: 0.787370  [   32/  118]
train() client id: f_00009-7-1 loss: 0.888359  [   64/  118]
train() client id: f_00009-7-2 loss: 1.010792  [   96/  118]
train() client id: f_00009-8-0 loss: 0.840481  [   32/  118]
train() client id: f_00009-8-1 loss: 0.917362  [   64/  118]
train() client id: f_00009-8-2 loss: 0.973566  [   96/  118]
train() client id: f_00009-9-0 loss: 0.849356  [   32/  118]
train() client id: f_00009-9-1 loss: 0.820253  [   64/  118]
train() client id: f_00009-9-2 loss: 0.860829  [   96/  118]
train() client id: f_00009-10-0 loss: 0.926141  [   32/  118]
train() client id: f_00009-10-1 loss: 0.835049  [   64/  118]
train() client id: f_00009-10-2 loss: 0.767538  [   96/  118]
train() client id: f_00009-11-0 loss: 0.821565  [   32/  118]
train() client id: f_00009-11-1 loss: 0.842926  [   64/  118]
train() client id: f_00009-11-2 loss: 0.940217  [   96/  118]
At round 38 accuracy: 0.649867374005305
At round 38 training accuracy: 0.5868544600938967
At round 38 training loss: 0.8242333678900531
update_location
xs = [  -3.9056584     4.20031788  210.00902392   18.81129433    0.97929623
    3.95640986 -172.44319194 -151.32485185  194.66397685 -137.06087855]
ys = [ 202.5879595   185.55583871    1.32061395 -172.45517586  164.35018685
  147.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [225.95826053 210.82839455 232.60596327 200.23649137 192.3848823
 178.50679634 199.35783152 181.38326009 219.5512101  169.71062514]
dists_bs = [173.30426779 179.51094579 422.3700719  397.85414041 176.30125133
 181.00957718 177.64338525 175.77931364 401.79095764 175.15670175]
uav_gains = [1.14573089e-11 1.45365625e-11 1.02368353e-11 1.69945603e-11
 1.90332055e-11 2.32656486e-11 1.72124693e-11 2.23088121e-11
 1.27076078e-11 2.65155347e-11]
bs_gains = [5.95152365e-11 5.39311234e-11 4.91307394e-12 5.80854309e-12
 5.67255845e-11 5.26901863e-11 5.55337207e-11 5.71984612e-11
 5.65058798e-12 5.77695734e-11]
Round 39
-------------------------------
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.84371614 12.07295727  5.75849455  2.07999855 13.92335007  6.69969194
  2.5762787   8.21232374  6.06245015  5.43331894]
obj_prev = 68.66258004963703
eta_min = 1.603615121775097e-16	eta_max = 0.9314544727787696
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 15.921783734685466	eta = 0.909090909090909
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 29.685339818895304	eta = 0.4875924930628838
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 22.863992502600777	eta = 0.6330630509114977
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.629256357976853	eta = 0.6692023345673616
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.563057577188566	eta = 0.6712567917560257
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.562850936268624	eta = 0.6712632245380996
eta = 0.6712632245380996
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [0.03289102 0.06917555 0.03236894 0.01122471 0.07987818 0.03811181
 0.01409615 0.04672613 0.03393518 0.0308027 ]
ene_total = [1.94642197 3.43896482 1.93822068 0.92238666 3.91833323 2.03975789
 1.05049566 2.49573328 2.10825374 1.704283  ]
ti_comp = [0.51931345 0.55115743 0.51591145 0.5297508  0.55187418 0.5508223
 0.53005444 0.53584373 0.4938656  0.55212944]
ti_coms = [0.102748   0.07090402 0.10615    0.09231065 0.07018727 0.07123916
 0.09200701 0.08621772 0.12819586 0.06993201]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.24617860e-06 6.81061808e-05 7.96370126e-06 3.14965047e-07
 1.04588604e-04 1.14034376e-05 6.23075349e-07 2.22066489e-05
 1.00141191e-05 5.99189852e-06]
ene_total = [0.4636805  0.32278962 0.47900796 0.41625902 0.32120273 0.32174399
 0.41490377 0.38977208 0.57850904 0.31560581]
optimize_network_iter = 0 obj = 4.023474514862622
eta = 0.6712632245380996
freqs = [31667787.3789384  62754800.17478311 31370631.5489173  10594331.96949695
 72369920.453618   34595378.85521331 13296888.56956221 43600515.27273497
 34356690.70712458 27894452.74815087]
eta_min = 0.6712632245381114	eta_max = 0.6712632245380962
af = 0.008905236727720691	bf = 1.3191144648464428	zeta = 0.009795760400492761	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [1.97179310e-06 1.62852764e-05 1.90424825e-06 7.53131768e-08
 2.50088069e-05 2.72674418e-06 1.48987275e-07 5.30996467e-06
 2.39453594e-06 1.43275870e-06]
ene_total = [1.67569409 1.15879127 1.73115483 1.50519706 1.14852664 1.1620451
 1.50025809 1.40670146 2.09070673 1.14052021]
ti_comp = [0.51931345 0.55115743 0.51591145 0.5297508  0.55187418 0.5508223
 0.53005444 0.53584373 0.4938656  0.55212944]
ti_coms = [0.102748   0.07090402 0.10615    0.09231065 0.07018727 0.07123916
 0.09200701 0.08621772 0.12819586 0.06993201]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.24617860e-06 6.81061808e-05 7.96370126e-06 3.14965047e-07
 1.04588604e-04 1.14034376e-05 6.23075349e-07 2.22066489e-05
 1.00141191e-05 5.99189852e-06]
ene_total = [0.4636805  0.32278962 0.47900796 0.41625902 0.32120273 0.32174399
 0.41490377 0.38977208 0.57850904 0.31560581]
optimize_network_iter = 1 obj = 4.0234745148625795
eta = 0.6712632245380962
freqs = [31667787.37893841 62754800.17478317 31370631.54891731 10594331.96949696
 72369920.4536181  34595378.85521334 13296888.56956222 43600515.27273501
 34356690.70712457 27894452.7481509 ]
Done!
At round 39 eta: 0.6712632245380962
At round 39 local rounds: 13.051990433476096
At round 39 global rounds: 45.091744918477566
At round 39 a_n: 14.480768977481684
gradient difference: 0.44250306487083435
train() client id: f_00000-0-0 loss: 1.331744  [   32/  126]
train() client id: f_00000-0-1 loss: 1.485903  [   64/  126]
train() client id: f_00000-0-2 loss: 1.064316  [   96/  126]
train() client id: f_00000-1-0 loss: 1.244934  [   32/  126]
train() client id: f_00000-1-1 loss: 1.099241  [   64/  126]
train() client id: f_00000-1-2 loss: 1.221676  [   96/  126]
train() client id: f_00000-2-0 loss: 1.302111  [   32/  126]
train() client id: f_00000-2-1 loss: 1.107453  [   64/  126]
train() client id: f_00000-2-2 loss: 1.123164  [   96/  126]
train() client id: f_00000-3-0 loss: 0.981054  [   32/  126]
train() client id: f_00000-3-1 loss: 0.988073  [   64/  126]
train() client id: f_00000-3-2 loss: 1.107672  [   96/  126]
train() client id: f_00000-4-0 loss: 1.073763  [   32/  126]
train() client id: f_00000-4-1 loss: 0.964046  [   64/  126]
train() client id: f_00000-4-2 loss: 0.958710  [   96/  126]
train() client id: f_00000-5-0 loss: 1.078023  [   32/  126]
train() client id: f_00000-5-1 loss: 0.960280  [   64/  126]
train() client id: f_00000-5-2 loss: 0.983155  [   96/  126]
train() client id: f_00000-6-0 loss: 0.949797  [   32/  126]
train() client id: f_00000-6-1 loss: 1.018026  [   64/  126]
train() client id: f_00000-6-2 loss: 0.975357  [   96/  126]
train() client id: f_00000-7-0 loss: 0.990727  [   32/  126]
train() client id: f_00000-7-1 loss: 0.810410  [   64/  126]
train() client id: f_00000-7-2 loss: 0.956423  [   96/  126]
train() client id: f_00000-8-0 loss: 0.857788  [   32/  126]
train() client id: f_00000-8-1 loss: 0.965111  [   64/  126]
train() client id: f_00000-8-2 loss: 0.946471  [   96/  126]
train() client id: f_00000-9-0 loss: 0.913681  [   32/  126]
train() client id: f_00000-9-1 loss: 0.988161  [   64/  126]
train() client id: f_00000-9-2 loss: 0.883232  [   96/  126]
train() client id: f_00000-10-0 loss: 0.964034  [   32/  126]
train() client id: f_00000-10-1 loss: 0.821971  [   64/  126]
train() client id: f_00000-10-2 loss: 0.963256  [   96/  126]
train() client id: f_00000-11-0 loss: 0.831618  [   32/  126]
train() client id: f_00000-11-1 loss: 1.025631  [   64/  126]
train() client id: f_00000-11-2 loss: 0.995595  [   96/  126]
train() client id: f_00000-12-0 loss: 0.894645  [   32/  126]
train() client id: f_00000-12-1 loss: 0.998853  [   64/  126]
train() client id: f_00000-12-2 loss: 0.888449  [   96/  126]
train() client id: f_00001-0-0 loss: 0.447960  [   32/  265]
train() client id: f_00001-0-1 loss: 0.500379  [   64/  265]
train() client id: f_00001-0-2 loss: 0.472358  [   96/  265]
train() client id: f_00001-0-3 loss: 0.474256  [  128/  265]
train() client id: f_00001-0-4 loss: 0.447201  [  160/  265]
train() client id: f_00001-0-5 loss: 0.397151  [  192/  265]
train() client id: f_00001-0-6 loss: 0.348091  [  224/  265]
train() client id: f_00001-0-7 loss: 0.383721  [  256/  265]
train() client id: f_00001-1-0 loss: 0.337146  [   32/  265]
train() client id: f_00001-1-1 loss: 0.378554  [   64/  265]
train() client id: f_00001-1-2 loss: 0.411764  [   96/  265]
train() client id: f_00001-1-3 loss: 0.431948  [  128/  265]
train() client id: f_00001-1-4 loss: 0.455415  [  160/  265]
train() client id: f_00001-1-5 loss: 0.524228  [  192/  265]
train() client id: f_00001-1-6 loss: 0.396412  [  224/  265]
train() client id: f_00001-1-7 loss: 0.470911  [  256/  265]
train() client id: f_00001-2-0 loss: 0.502121  [   32/  265]
train() client id: f_00001-2-1 loss: 0.446595  [   64/  265]
train() client id: f_00001-2-2 loss: 0.340369  [   96/  265]
train() client id: f_00001-2-3 loss: 0.416428  [  128/  265]
train() client id: f_00001-2-4 loss: 0.330702  [  160/  265]
train() client id: f_00001-2-5 loss: 0.507447  [  192/  265]
train() client id: f_00001-2-6 loss: 0.378305  [  224/  265]
train() client id: f_00001-2-7 loss: 0.431129  [  256/  265]
train() client id: f_00001-3-0 loss: 0.383306  [   32/  265]
train() client id: f_00001-3-1 loss: 0.325784  [   64/  265]
train() client id: f_00001-3-2 loss: 0.414855  [   96/  265]
train() client id: f_00001-3-3 loss: 0.443713  [  128/  265]
train() client id: f_00001-3-4 loss: 0.448024  [  160/  265]
train() client id: f_00001-3-5 loss: 0.319336  [  192/  265]
train() client id: f_00001-3-6 loss: 0.350445  [  224/  265]
train() client id: f_00001-3-7 loss: 0.538018  [  256/  265]
train() client id: f_00001-4-0 loss: 0.392094  [   32/  265]
train() client id: f_00001-4-1 loss: 0.333995  [   64/  265]
train() client id: f_00001-4-2 loss: 0.530615  [   96/  265]
train() client id: f_00001-4-3 loss: 0.376294  [  128/  265]
train() client id: f_00001-4-4 loss: 0.374080  [  160/  265]
train() client id: f_00001-4-5 loss: 0.467338  [  192/  265]
train() client id: f_00001-4-6 loss: 0.368929  [  224/  265]
train() client id: f_00001-4-7 loss: 0.453006  [  256/  265]
train() client id: f_00001-5-0 loss: 0.369921  [   32/  265]
train() client id: f_00001-5-1 loss: 0.321984  [   64/  265]
train() client id: f_00001-5-2 loss: 0.396962  [   96/  265]
train() client id: f_00001-5-3 loss: 0.408088  [  128/  265]
train() client id: f_00001-5-4 loss: 0.533233  [  160/  265]
train() client id: f_00001-5-5 loss: 0.513726  [  192/  265]
train() client id: f_00001-5-6 loss: 0.327417  [  224/  265]
train() client id: f_00001-5-7 loss: 0.397787  [  256/  265]
train() client id: f_00001-6-0 loss: 0.310265  [   32/  265]
train() client id: f_00001-6-1 loss: 0.368091  [   64/  265]
train() client id: f_00001-6-2 loss: 0.481331  [   96/  265]
train() client id: f_00001-6-3 loss: 0.443443  [  128/  265]
train() client id: f_00001-6-4 loss: 0.398514  [  160/  265]
train() client id: f_00001-6-5 loss: 0.345183  [  192/  265]
train() client id: f_00001-6-6 loss: 0.464836  [  224/  265]
train() client id: f_00001-6-7 loss: 0.446262  [  256/  265]
train() client id: f_00001-7-0 loss: 0.499333  [   32/  265]
train() client id: f_00001-7-1 loss: 0.328481  [   64/  265]
train() client id: f_00001-7-2 loss: 0.535522  [   96/  265]
train() client id: f_00001-7-3 loss: 0.336510  [  128/  265]
train() client id: f_00001-7-4 loss: 0.375769  [  160/  265]
train() client id: f_00001-7-5 loss: 0.458836  [  192/  265]
train() client id: f_00001-7-6 loss: 0.295138  [  224/  265]
train() client id: f_00001-7-7 loss: 0.423156  [  256/  265]
train() client id: f_00001-8-0 loss: 0.476137  [   32/  265]
train() client id: f_00001-8-1 loss: 0.302419  [   64/  265]
train() client id: f_00001-8-2 loss: 0.384729  [   96/  265]
train() client id: f_00001-8-3 loss: 0.467285  [  128/  265]
train() client id: f_00001-8-4 loss: 0.310013  [  160/  265]
train() client id: f_00001-8-5 loss: 0.409389  [  192/  265]
train() client id: f_00001-8-6 loss: 0.364261  [  224/  265]
train() client id: f_00001-8-7 loss: 0.518302  [  256/  265]
train() client id: f_00001-9-0 loss: 0.567135  [   32/  265]
train() client id: f_00001-9-1 loss: 0.323926  [   64/  265]
train() client id: f_00001-9-2 loss: 0.418055  [   96/  265]
train() client id: f_00001-9-3 loss: 0.380674  [  128/  265]
train() client id: f_00001-9-4 loss: 0.364277  [  160/  265]
train() client id: f_00001-9-5 loss: 0.450168  [  192/  265]
train() client id: f_00001-9-6 loss: 0.316343  [  224/  265]
train() client id: f_00001-9-7 loss: 0.401096  [  256/  265]
train() client id: f_00001-10-0 loss: 0.476212  [   32/  265]
train() client id: f_00001-10-1 loss: 0.384088  [   64/  265]
train() client id: f_00001-10-2 loss: 0.509681  [   96/  265]
train() client id: f_00001-10-3 loss: 0.372180  [  128/  265]
train() client id: f_00001-10-4 loss: 0.299484  [  160/  265]
train() client id: f_00001-10-5 loss: 0.453402  [  192/  265]
train() client id: f_00001-10-6 loss: 0.314861  [  224/  265]
train() client id: f_00001-10-7 loss: 0.411294  [  256/  265]
train() client id: f_00001-11-0 loss: 0.338010  [   32/  265]
train() client id: f_00001-11-1 loss: 0.438365  [   64/  265]
train() client id: f_00001-11-2 loss: 0.340197  [   96/  265]
train() client id: f_00001-11-3 loss: 0.348442  [  128/  265]
train() client id: f_00001-11-4 loss: 0.452783  [  160/  265]
train() client id: f_00001-11-5 loss: 0.411207  [  192/  265]
train() client id: f_00001-11-6 loss: 0.424769  [  224/  265]
train() client id: f_00001-11-7 loss: 0.417470  [  256/  265]
train() client id: f_00001-12-0 loss: 0.312129  [   32/  265]
train() client id: f_00001-12-1 loss: 0.472395  [   64/  265]
train() client id: f_00001-12-2 loss: 0.326221  [   96/  265]
train() client id: f_00001-12-3 loss: 0.316516  [  128/  265]
train() client id: f_00001-12-4 loss: 0.396528  [  160/  265]
train() client id: f_00001-12-5 loss: 0.548973  [  192/  265]
train() client id: f_00001-12-6 loss: 0.405656  [  224/  265]
train() client id: f_00001-12-7 loss: 0.417705  [  256/  265]
train() client id: f_00002-0-0 loss: 1.220753  [   32/  124]
train() client id: f_00002-0-1 loss: 1.025440  [   64/  124]
train() client id: f_00002-0-2 loss: 1.020679  [   96/  124]
train() client id: f_00002-1-0 loss: 1.122072  [   32/  124]
train() client id: f_00002-1-1 loss: 1.006735  [   64/  124]
train() client id: f_00002-1-2 loss: 1.101279  [   96/  124]
train() client id: f_00002-2-0 loss: 0.976964  [   32/  124]
train() client id: f_00002-2-1 loss: 1.155540  [   64/  124]
train() client id: f_00002-2-2 loss: 1.046052  [   96/  124]
train() client id: f_00002-3-0 loss: 1.017772  [   32/  124]
train() client id: f_00002-3-1 loss: 1.074464  [   64/  124]
train() client id: f_00002-3-2 loss: 1.015591  [   96/  124]
train() client id: f_00002-4-0 loss: 1.028707  [   32/  124]
train() client id: f_00002-4-1 loss: 0.998810  [   64/  124]
train() client id: f_00002-4-2 loss: 0.855443  [   96/  124]
train() client id: f_00002-5-0 loss: 1.026463  [   32/  124]
train() client id: f_00002-5-1 loss: 0.922729  [   64/  124]
train() client id: f_00002-5-2 loss: 1.007703  [   96/  124]
train() client id: f_00002-6-0 loss: 1.019404  [   32/  124]
train() client id: f_00002-6-1 loss: 1.055765  [   64/  124]
train() client id: f_00002-6-2 loss: 0.763694  [   96/  124]
train() client id: f_00002-7-0 loss: 0.929288  [   32/  124]
train() client id: f_00002-7-1 loss: 0.827713  [   64/  124]
train() client id: f_00002-7-2 loss: 1.079012  [   96/  124]
train() client id: f_00002-8-0 loss: 1.004347  [   32/  124]
train() client id: f_00002-8-1 loss: 0.909597  [   64/  124]
train() client id: f_00002-8-2 loss: 0.858816  [   96/  124]
train() client id: f_00002-9-0 loss: 0.895357  [   32/  124]
train() client id: f_00002-9-1 loss: 1.050286  [   64/  124]
train() client id: f_00002-9-2 loss: 0.954331  [   96/  124]
train() client id: f_00002-10-0 loss: 0.997596  [   32/  124]
train() client id: f_00002-10-1 loss: 0.949654  [   64/  124]
train() client id: f_00002-10-2 loss: 0.967731  [   96/  124]
train() client id: f_00002-11-0 loss: 0.832660  [   32/  124]
train() client id: f_00002-11-1 loss: 0.953988  [   64/  124]
train() client id: f_00002-11-2 loss: 1.028753  [   96/  124]
train() client id: f_00002-12-0 loss: 1.010466  [   32/  124]
train() client id: f_00002-12-1 loss: 1.016306  [   64/  124]
train() client id: f_00002-12-2 loss: 0.824813  [   96/  124]
train() client id: f_00003-0-0 loss: 1.021321  [   32/   43]
train() client id: f_00003-1-0 loss: 1.000761  [   32/   43]
train() client id: f_00003-2-0 loss: 0.906272  [   32/   43]
train() client id: f_00003-3-0 loss: 1.015827  [   32/   43]
train() client id: f_00003-4-0 loss: 0.692104  [   32/   43]
train() client id: f_00003-5-0 loss: 0.919707  [   32/   43]
train() client id: f_00003-6-0 loss: 1.013036  [   32/   43]
train() client id: f_00003-7-0 loss: 0.922157  [   32/   43]
train() client id: f_00003-8-0 loss: 1.014277  [   32/   43]
train() client id: f_00003-9-0 loss: 1.073160  [   32/   43]
train() client id: f_00003-10-0 loss: 1.023903  [   32/   43]
train() client id: f_00003-11-0 loss: 0.884471  [   32/   43]
train() client id: f_00003-12-0 loss: 0.898929  [   32/   43]
train() client id: f_00004-0-0 loss: 1.086222  [   32/  306]
train() client id: f_00004-0-1 loss: 0.771022  [   64/  306]
train() client id: f_00004-0-2 loss: 0.945042  [   96/  306]
train() client id: f_00004-0-3 loss: 1.026443  [  128/  306]
train() client id: f_00004-0-4 loss: 0.983558  [  160/  306]
train() client id: f_00004-0-5 loss: 0.877470  [  192/  306]
train() client id: f_00004-0-6 loss: 1.021351  [  224/  306]
train() client id: f_00004-0-7 loss: 1.027251  [  256/  306]
train() client id: f_00004-0-8 loss: 1.041957  [  288/  306]
train() client id: f_00004-1-0 loss: 0.857656  [   32/  306]
train() client id: f_00004-1-1 loss: 1.129139  [   64/  306]
train() client id: f_00004-1-2 loss: 0.894701  [   96/  306]
train() client id: f_00004-1-3 loss: 0.873471  [  128/  306]
train() client id: f_00004-1-4 loss: 0.877954  [  160/  306]
train() client id: f_00004-1-5 loss: 1.010976  [  192/  306]
train() client id: f_00004-1-6 loss: 1.034307  [  224/  306]
train() client id: f_00004-1-7 loss: 0.941802  [  256/  306]
train() client id: f_00004-1-8 loss: 0.988202  [  288/  306]
train() client id: f_00004-2-0 loss: 1.130425  [   32/  306]
train() client id: f_00004-2-1 loss: 0.869306  [   64/  306]
train() client id: f_00004-2-2 loss: 0.986462  [   96/  306]
train() client id: f_00004-2-3 loss: 0.953544  [  128/  306]
train() client id: f_00004-2-4 loss: 0.876943  [  160/  306]
train() client id: f_00004-2-5 loss: 0.854846  [  192/  306]
train() client id: f_00004-2-6 loss: 1.011458  [  224/  306]
train() client id: f_00004-2-7 loss: 1.020162  [  256/  306]
train() client id: f_00004-2-8 loss: 0.959507  [  288/  306]
train() client id: f_00004-3-0 loss: 0.886193  [   32/  306]
train() client id: f_00004-3-1 loss: 0.897493  [   64/  306]
train() client id: f_00004-3-2 loss: 1.005373  [   96/  306]
train() client id: f_00004-3-3 loss: 1.050830  [  128/  306]
train() client id: f_00004-3-4 loss: 0.922317  [  160/  306]
train() client id: f_00004-3-5 loss: 0.925544  [  192/  306]
train() client id: f_00004-3-6 loss: 0.835860  [  224/  306]
train() client id: f_00004-3-7 loss: 1.050265  [  256/  306]
train() client id: f_00004-3-8 loss: 0.955326  [  288/  306]
train() client id: f_00004-4-0 loss: 0.878217  [   32/  306]
train() client id: f_00004-4-1 loss: 1.064579  [   64/  306]
train() client id: f_00004-4-2 loss: 0.864277  [   96/  306]
train() client id: f_00004-4-3 loss: 0.945758  [  128/  306]
train() client id: f_00004-4-4 loss: 0.955372  [  160/  306]
train() client id: f_00004-4-5 loss: 0.944643  [  192/  306]
train() client id: f_00004-4-6 loss: 0.975971  [  224/  306]
train() client id: f_00004-4-7 loss: 0.923818  [  256/  306]
train() client id: f_00004-4-8 loss: 1.032211  [  288/  306]
train() client id: f_00004-5-0 loss: 0.941039  [   32/  306]
train() client id: f_00004-5-1 loss: 1.002832  [   64/  306]
train() client id: f_00004-5-2 loss: 0.730889  [   96/  306]
train() client id: f_00004-5-3 loss: 0.852274  [  128/  306]
train() client id: f_00004-5-4 loss: 1.062712  [  160/  306]
train() client id: f_00004-5-5 loss: 1.026082  [  192/  306]
train() client id: f_00004-5-6 loss: 1.000865  [  224/  306]
train() client id: f_00004-5-7 loss: 0.983288  [  256/  306]
train() client id: f_00004-5-8 loss: 0.906195  [  288/  306]
train() client id: f_00004-6-0 loss: 1.016733  [   32/  306]
train() client id: f_00004-6-1 loss: 0.857320  [   64/  306]
train() client id: f_00004-6-2 loss: 1.039021  [   96/  306]
train() client id: f_00004-6-3 loss: 1.085213  [  128/  306]
train() client id: f_00004-6-4 loss: 0.968590  [  160/  306]
train() client id: f_00004-6-5 loss: 0.961269  [  192/  306]
train() client id: f_00004-6-6 loss: 0.876565  [  224/  306]
train() client id: f_00004-6-7 loss: 0.922942  [  256/  306]
train() client id: f_00004-6-8 loss: 0.890387  [  288/  306]
train() client id: f_00004-7-0 loss: 0.848269  [   32/  306]
train() client id: f_00004-7-1 loss: 1.109861  [   64/  306]
train() client id: f_00004-7-2 loss: 0.964393  [   96/  306]
train() client id: f_00004-7-3 loss: 0.983316  [  128/  306]
train() client id: f_00004-7-4 loss: 0.998964  [  160/  306]
train() client id: f_00004-7-5 loss: 0.872104  [  192/  306]
train() client id: f_00004-7-6 loss: 0.915039  [  224/  306]
train() client id: f_00004-7-7 loss: 1.100536  [  256/  306]
train() client id: f_00004-7-8 loss: 0.842470  [  288/  306]
train() client id: f_00004-8-0 loss: 0.906672  [   32/  306]
train() client id: f_00004-8-1 loss: 0.820600  [   64/  306]
train() client id: f_00004-8-2 loss: 1.074058  [   96/  306]
train() client id: f_00004-8-3 loss: 0.965598  [  128/  306]
train() client id: f_00004-8-4 loss: 1.115554  [  160/  306]
train() client id: f_00004-8-5 loss: 0.851911  [  192/  306]
train() client id: f_00004-8-6 loss: 0.882274  [  224/  306]
train() client id: f_00004-8-7 loss: 0.908080  [  256/  306]
train() client id: f_00004-8-8 loss: 1.019984  [  288/  306]
train() client id: f_00004-9-0 loss: 0.873288  [   32/  306]
train() client id: f_00004-9-1 loss: 0.977909  [   64/  306]
train() client id: f_00004-9-2 loss: 1.024730  [   96/  306]
train() client id: f_00004-9-3 loss: 0.891430  [  128/  306]
train() client id: f_00004-9-4 loss: 0.915415  [  160/  306]
train() client id: f_00004-9-5 loss: 0.925116  [  192/  306]
train() client id: f_00004-9-6 loss: 1.033410  [  224/  306]
train() client id: f_00004-9-7 loss: 0.934137  [  256/  306]
train() client id: f_00004-9-8 loss: 0.951059  [  288/  306]
train() client id: f_00004-10-0 loss: 0.922175  [   32/  306]
train() client id: f_00004-10-1 loss: 0.912659  [   64/  306]
train() client id: f_00004-10-2 loss: 0.955409  [   96/  306]
train() client id: f_00004-10-3 loss: 0.757249  [  128/  306]
train() client id: f_00004-10-4 loss: 0.896930  [  160/  306]
train() client id: f_00004-10-5 loss: 1.082787  [  192/  306]
train() client id: f_00004-10-6 loss: 1.045982  [  224/  306]
train() client id: f_00004-10-7 loss: 0.971817  [  256/  306]
train() client id: f_00004-10-8 loss: 0.906903  [  288/  306]
train() client id: f_00004-11-0 loss: 0.918451  [   32/  306]
train() client id: f_00004-11-1 loss: 0.994504  [   64/  306]
train() client id: f_00004-11-2 loss: 0.888868  [   96/  306]
train() client id: f_00004-11-3 loss: 0.910125  [  128/  306]
train() client id: f_00004-11-4 loss: 0.976059  [  160/  306]
train() client id: f_00004-11-5 loss: 0.860841  [  192/  306]
train() client id: f_00004-11-6 loss: 0.920747  [  224/  306]
train() client id: f_00004-11-7 loss: 0.945631  [  256/  306]
train() client id: f_00004-11-8 loss: 1.036855  [  288/  306]
train() client id: f_00004-12-0 loss: 0.933600  [   32/  306]
train() client id: f_00004-12-1 loss: 0.852954  [   64/  306]
train() client id: f_00004-12-2 loss: 1.030761  [   96/  306]
train() client id: f_00004-12-3 loss: 0.934415  [  128/  306]
train() client id: f_00004-12-4 loss: 0.862980  [  160/  306]
train() client id: f_00004-12-5 loss: 0.928554  [  192/  306]
train() client id: f_00004-12-6 loss: 1.029363  [  224/  306]
train() client id: f_00004-12-7 loss: 0.898926  [  256/  306]
train() client id: f_00004-12-8 loss: 0.914677  [  288/  306]
train() client id: f_00005-0-0 loss: 0.594894  [   32/  146]
train() client id: f_00005-0-1 loss: 0.465582  [   64/  146]
train() client id: f_00005-0-2 loss: 0.308416  [   96/  146]
train() client id: f_00005-0-3 loss: 0.690144  [  128/  146]
train() client id: f_00005-1-0 loss: 0.701999  [   32/  146]
train() client id: f_00005-1-1 loss: 0.469643  [   64/  146]
train() client id: f_00005-1-2 loss: 0.563193  [   96/  146]
train() client id: f_00005-1-3 loss: 0.512430  [  128/  146]
train() client id: f_00005-2-0 loss: 0.743740  [   32/  146]
train() client id: f_00005-2-1 loss: 0.375633  [   64/  146]
train() client id: f_00005-2-2 loss: 0.454621  [   96/  146]
train() client id: f_00005-2-3 loss: 0.618098  [  128/  146]
train() client id: f_00005-3-0 loss: 0.325423  [   32/  146]
train() client id: f_00005-3-1 loss: 0.878116  [   64/  146]
train() client id: f_00005-3-2 loss: 0.485395  [   96/  146]
train() client id: f_00005-3-3 loss: 0.738454  [  128/  146]
train() client id: f_00005-4-0 loss: 0.643423  [   32/  146]
train() client id: f_00005-4-1 loss: 0.320109  [   64/  146]
train() client id: f_00005-4-2 loss: 0.752695  [   96/  146]
train() client id: f_00005-4-3 loss: 0.453497  [  128/  146]
train() client id: f_00005-5-0 loss: 0.605062  [   32/  146]
train() client id: f_00005-5-1 loss: 0.582075  [   64/  146]
train() client id: f_00005-5-2 loss: 0.453375  [   96/  146]
train() client id: f_00005-5-3 loss: 0.664658  [  128/  146]
train() client id: f_00005-6-0 loss: 0.653529  [   32/  146]
train() client id: f_00005-6-1 loss: 0.675917  [   64/  146]
train() client id: f_00005-6-2 loss: 0.382200  [   96/  146]
train() client id: f_00005-6-3 loss: 0.618657  [  128/  146]
train() client id: f_00005-7-0 loss: 0.708061  [   32/  146]
train() client id: f_00005-7-1 loss: 0.425918  [   64/  146]
train() client id: f_00005-7-2 loss: 0.475047  [   96/  146]
train() client id: f_00005-7-3 loss: 0.335662  [  128/  146]
train() client id: f_00005-8-0 loss: 0.636156  [   32/  146]
train() client id: f_00005-8-1 loss: 0.399638  [   64/  146]
train() client id: f_00005-8-2 loss: 0.605347  [   96/  146]
train() client id: f_00005-8-3 loss: 0.485859  [  128/  146]
train() client id: f_00005-9-0 loss: 0.737965  [   32/  146]
train() client id: f_00005-9-1 loss: 0.355711  [   64/  146]
train() client id: f_00005-9-2 loss: 0.613528  [   96/  146]
train() client id: f_00005-9-3 loss: 0.505267  [  128/  146]
train() client id: f_00005-10-0 loss: 0.662314  [   32/  146]
train() client id: f_00005-10-1 loss: 0.370599  [   64/  146]
train() client id: f_00005-10-2 loss: 0.624191  [   96/  146]
train() client id: f_00005-10-3 loss: 0.570603  [  128/  146]
train() client id: f_00005-11-0 loss: 0.496290  [   32/  146]
train() client id: f_00005-11-1 loss: 0.415210  [   64/  146]
train() client id: f_00005-11-2 loss: 0.708266  [   96/  146]
train() client id: f_00005-11-3 loss: 0.514752  [  128/  146]
train() client id: f_00005-12-0 loss: 0.541381  [   32/  146]
train() client id: f_00005-12-1 loss: 0.525106  [   64/  146]
train() client id: f_00005-12-2 loss: 0.588836  [   96/  146]
train() client id: f_00005-12-3 loss: 0.447561  [  128/  146]
train() client id: f_00006-0-0 loss: 0.501342  [   32/   54]
train() client id: f_00006-1-0 loss: 0.569571  [   32/   54]
train() client id: f_00006-2-0 loss: 0.553721  [   32/   54]
train() client id: f_00006-3-0 loss: 0.512320  [   32/   54]
train() client id: f_00006-4-0 loss: 0.497063  [   32/   54]
train() client id: f_00006-5-0 loss: 0.505399  [   32/   54]
train() client id: f_00006-6-0 loss: 0.518805  [   32/   54]
train() client id: f_00006-7-0 loss: 0.526192  [   32/   54]
train() client id: f_00006-8-0 loss: 0.511214  [   32/   54]
train() client id: f_00006-9-0 loss: 0.478285  [   32/   54]
train() client id: f_00006-10-0 loss: 0.480285  [   32/   54]
train() client id: f_00006-11-0 loss: 0.552603  [   32/   54]
train() client id: f_00006-12-0 loss: 0.496209  [   32/   54]
train() client id: f_00007-0-0 loss: 0.336253  [   32/  179]
train() client id: f_00007-0-1 loss: 0.474871  [   64/  179]
train() client id: f_00007-0-2 loss: 0.476462  [   96/  179]
train() client id: f_00007-0-3 loss: 0.560960  [  128/  179]
train() client id: f_00007-0-4 loss: 0.312510  [  160/  179]
train() client id: f_00007-1-0 loss: 0.648806  [   32/  179]
train() client id: f_00007-1-1 loss: 0.386734  [   64/  179]
train() client id: f_00007-1-2 loss: 0.466668  [   96/  179]
train() client id: f_00007-1-3 loss: 0.369121  [  128/  179]
train() client id: f_00007-1-4 loss: 0.359313  [  160/  179]
train() client id: f_00007-2-0 loss: 0.514356  [   32/  179]
train() client id: f_00007-2-1 loss: 0.342918  [   64/  179]
train() client id: f_00007-2-2 loss: 0.623740  [   96/  179]
train() client id: f_00007-2-3 loss: 0.357923  [  128/  179]
train() client id: f_00007-2-4 loss: 0.336966  [  160/  179]
train() client id: f_00007-3-0 loss: 0.359607  [   32/  179]
train() client id: f_00007-3-1 loss: 0.498622  [   64/  179]
train() client id: f_00007-3-2 loss: 0.529259  [   96/  179]
train() client id: f_00007-3-3 loss: 0.354978  [  128/  179]
train() client id: f_00007-3-4 loss: 0.394586  [  160/  179]
train() client id: f_00007-4-0 loss: 0.499686  [   32/  179]
train() client id: f_00007-4-1 loss: 0.321973  [   64/  179]
train() client id: f_00007-4-2 loss: 0.426719  [   96/  179]
train() client id: f_00007-4-3 loss: 0.350567  [  128/  179]
train() client id: f_00007-4-4 loss: 0.301742  [  160/  179]
train() client id: f_00007-5-0 loss: 0.446357  [   32/  179]
train() client id: f_00007-5-1 loss: 0.230641  [   64/  179]
train() client id: f_00007-5-2 loss: 0.436874  [   96/  179]
train() client id: f_00007-5-3 loss: 0.474730  [  128/  179]
train() client id: f_00007-5-4 loss: 0.430628  [  160/  179]
train() client id: f_00007-6-0 loss: 0.427297  [   32/  179]
train() client id: f_00007-6-1 loss: 0.416513  [   64/  179]
train() client id: f_00007-6-2 loss: 0.433774  [   96/  179]
train() client id: f_00007-6-3 loss: 0.471439  [  128/  179]
train() client id: f_00007-6-4 loss: 0.222491  [  160/  179]
train() client id: f_00007-7-0 loss: 0.419066  [   32/  179]
train() client id: f_00007-7-1 loss: 0.496084  [   64/  179]
train() client id: f_00007-7-2 loss: 0.360396  [   96/  179]
train() client id: f_00007-7-3 loss: 0.230701  [  128/  179]
train() client id: f_00007-7-4 loss: 0.450346  [  160/  179]
train() client id: f_00007-8-0 loss: 0.473516  [   32/  179]
train() client id: f_00007-8-1 loss: 0.350256  [   64/  179]
train() client id: f_00007-8-2 loss: 0.269696  [   96/  179]
train() client id: f_00007-8-3 loss: 0.226073  [  128/  179]
train() client id: f_00007-8-4 loss: 0.487963  [  160/  179]
train() client id: f_00007-9-0 loss: 0.436868  [   32/  179]
train() client id: f_00007-9-1 loss: 0.296880  [   64/  179]
train() client id: f_00007-9-2 loss: 0.292792  [   96/  179]
train() client id: f_00007-9-3 loss: 0.445045  [  128/  179]
train() client id: f_00007-9-4 loss: 0.383699  [  160/  179]
train() client id: f_00007-10-0 loss: 0.217415  [   32/  179]
train() client id: f_00007-10-1 loss: 0.207829  [   64/  179]
train() client id: f_00007-10-2 loss: 0.319440  [   96/  179]
train() client id: f_00007-10-3 loss: 0.686848  [  128/  179]
train() client id: f_00007-10-4 loss: 0.366571  [  160/  179]
train() client id: f_00007-11-0 loss: 0.232106  [   32/  179]
train() client id: f_00007-11-1 loss: 0.363163  [   64/  179]
train() client id: f_00007-11-2 loss: 0.322474  [   96/  179]
train() client id: f_00007-11-3 loss: 0.526522  [  128/  179]
train() client id: f_00007-11-4 loss: 0.320720  [  160/  179]
train() client id: f_00007-12-0 loss: 0.271453  [   32/  179]
train() client id: f_00007-12-1 loss: 0.433456  [   64/  179]
train() client id: f_00007-12-2 loss: 0.478241  [   96/  179]
train() client id: f_00007-12-3 loss: 0.181831  [  128/  179]
train() client id: f_00007-12-4 loss: 0.363404  [  160/  179]
train() client id: f_00008-0-0 loss: 0.530422  [   32/  130]
train() client id: f_00008-0-1 loss: 0.677292  [   64/  130]
train() client id: f_00008-0-2 loss: 0.813162  [   96/  130]
train() client id: f_00008-0-3 loss: 0.600647  [  128/  130]
train() client id: f_00008-1-0 loss: 0.630655  [   32/  130]
train() client id: f_00008-1-1 loss: 0.692112  [   64/  130]
train() client id: f_00008-1-2 loss: 0.710762  [   96/  130]
train() client id: f_00008-1-3 loss: 0.588105  [  128/  130]
train() client id: f_00008-2-0 loss: 0.698601  [   32/  130]
train() client id: f_00008-2-1 loss: 0.732703  [   64/  130]
train() client id: f_00008-2-2 loss: 0.606296  [   96/  130]
train() client id: f_00008-2-3 loss: 0.595107  [  128/  130]
train() client id: f_00008-3-0 loss: 0.706084  [   32/  130]
train() client id: f_00008-3-1 loss: 0.566462  [   64/  130]
train() client id: f_00008-3-2 loss: 0.729626  [   96/  130]
train() client id: f_00008-3-3 loss: 0.611006  [  128/  130]
train() client id: f_00008-4-0 loss: 0.618616  [   32/  130]
train() client id: f_00008-4-1 loss: 0.694012  [   64/  130]
train() client id: f_00008-4-2 loss: 0.755483  [   96/  130]
train() client id: f_00008-4-3 loss: 0.556959  [  128/  130]
train() client id: f_00008-5-0 loss: 0.635010  [   32/  130]
train() client id: f_00008-5-1 loss: 0.661027  [   64/  130]
train() client id: f_00008-5-2 loss: 0.628514  [   96/  130]
train() client id: f_00008-5-3 loss: 0.656597  [  128/  130]
train() client id: f_00008-6-0 loss: 0.714447  [   32/  130]
train() client id: f_00008-6-1 loss: 0.563848  [   64/  130]
train() client id: f_00008-6-2 loss: 0.761297  [   96/  130]
train() client id: f_00008-6-3 loss: 0.574204  [  128/  130]
train() client id: f_00008-7-0 loss: 0.595810  [   32/  130]
train() client id: f_00008-7-1 loss: 0.654486  [   64/  130]
train() client id: f_00008-7-2 loss: 0.575365  [   96/  130]
train() client id: f_00008-7-3 loss: 0.755286  [  128/  130]
train() client id: f_00008-8-0 loss: 0.587701  [   32/  130]
train() client id: f_00008-8-1 loss: 0.672074  [   64/  130]
train() client id: f_00008-8-2 loss: 0.581133  [   96/  130]
train() client id: f_00008-8-3 loss: 0.774208  [  128/  130]
train() client id: f_00008-9-0 loss: 0.682020  [   32/  130]
train() client id: f_00008-9-1 loss: 0.569090  [   64/  130]
train() client id: f_00008-9-2 loss: 0.687453  [   96/  130]
train() client id: f_00008-9-3 loss: 0.670769  [  128/  130]
train() client id: f_00008-10-0 loss: 0.611054  [   32/  130]
train() client id: f_00008-10-1 loss: 0.650637  [   64/  130]
train() client id: f_00008-10-2 loss: 0.755105  [   96/  130]
train() client id: f_00008-10-3 loss: 0.599678  [  128/  130]
train() client id: f_00008-11-0 loss: 0.638969  [   32/  130]
train() client id: f_00008-11-1 loss: 0.790338  [   64/  130]
train() client id: f_00008-11-2 loss: 0.625759  [   96/  130]
train() client id: f_00008-11-3 loss: 0.572970  [  128/  130]
train() client id: f_00008-12-0 loss: 0.669628  [   32/  130]
train() client id: f_00008-12-1 loss: 0.673002  [   64/  130]
train() client id: f_00008-12-2 loss: 0.654149  [   96/  130]
train() client id: f_00008-12-3 loss: 0.608650  [  128/  130]
train() client id: f_00009-0-0 loss: 0.940054  [   32/  118]
train() client id: f_00009-0-1 loss: 0.943153  [   64/  118]
train() client id: f_00009-0-2 loss: 1.037999  [   96/  118]
train() client id: f_00009-1-0 loss: 0.918140  [   32/  118]
train() client id: f_00009-1-1 loss: 0.952374  [   64/  118]
train() client id: f_00009-1-2 loss: 0.841792  [   96/  118]
train() client id: f_00009-2-0 loss: 0.884550  [   32/  118]
train() client id: f_00009-2-1 loss: 0.874120  [   64/  118]
train() client id: f_00009-2-2 loss: 0.873396  [   96/  118]
train() client id: f_00009-3-0 loss: 0.931459  [   32/  118]
train() client id: f_00009-3-1 loss: 0.748089  [   64/  118]
train() client id: f_00009-3-2 loss: 0.966766  [   96/  118]
train() client id: f_00009-4-0 loss: 0.867244  [   32/  118]
train() client id: f_00009-4-1 loss: 0.802514  [   64/  118]
train() client id: f_00009-4-2 loss: 0.818298  [   96/  118]
train() client id: f_00009-5-0 loss: 0.737902  [   32/  118]
train() client id: f_00009-5-1 loss: 0.827592  [   64/  118]
train() client id: f_00009-5-2 loss: 0.733534  [   96/  118]
train() client id: f_00009-6-0 loss: 0.779556  [   32/  118]
train() client id: f_00009-6-1 loss: 0.795216  [   64/  118]
train() client id: f_00009-6-2 loss: 0.730112  [   96/  118]
train() client id: f_00009-7-0 loss: 0.768558  [   32/  118]
train() client id: f_00009-7-1 loss: 0.831873  [   64/  118]
train() client id: f_00009-7-2 loss: 0.657707  [   96/  118]
train() client id: f_00009-8-0 loss: 0.665006  [   32/  118]
train() client id: f_00009-8-1 loss: 0.928001  [   64/  118]
train() client id: f_00009-8-2 loss: 0.745669  [   96/  118]
train() client id: f_00009-9-0 loss: 0.701766  [   32/  118]
train() client id: f_00009-9-1 loss: 0.787229  [   64/  118]
train() client id: f_00009-9-2 loss: 0.739486  [   96/  118]
train() client id: f_00009-10-0 loss: 0.708048  [   32/  118]
train() client id: f_00009-10-1 loss: 0.791852  [   64/  118]
train() client id: f_00009-10-2 loss: 0.701987  [   96/  118]
train() client id: f_00009-11-0 loss: 0.889445  [   32/  118]
train() client id: f_00009-11-1 loss: 0.649427  [   64/  118]
train() client id: f_00009-11-2 loss: 0.746475  [   96/  118]
train() client id: f_00009-12-0 loss: 0.961498  [   32/  118]
train() client id: f_00009-12-1 loss: 0.756658  [   64/  118]
train() client id: f_00009-12-2 loss: 0.592425  [   96/  118]
At round 39 accuracy: 0.649867374005305
At round 39 training accuracy: 0.5888665325285044
At round 39 training loss: 0.8221179669007089
update_location
xs = [  -3.9056584     4.20031788  215.00902392   18.81129433    0.97929623
    3.95640986 -177.44319194 -156.32485185  199.66397685 -142.06087855]
ys = [ 207.5879595   190.55583871    1.32061395 -177.45517586  169.35018685
  152.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [230.45176306 215.24212027 237.12997361 204.55855942 196.67344713
 182.66860115 203.69824964 185.57514802 223.99636967 173.77371801]
dists_bs = [174.17017204 179.87422841 426.93274397 402.22813029 176.06996646
 180.32639446 177.6417863  175.17595617 406.39588262 174.14212286]
uav_gains = [1.06238871e-11 1.35916922e-11 9.45055194e-12 1.59558146e-11
 1.78934679e-11 2.18960415e-11 1.61583748e-11 2.09940106e-11
 1.18323052e-11 2.49490095e-11]
bs_gains = [5.86904580e-11 5.36266962e-11 4.76746578e-12 5.63340860e-12
 5.69344720e-11 5.32510343e-11 5.55351203e-11 5.77517960e-11
 5.47313357e-12 5.87169311e-11]
Round 40
-------------------------------
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.71197888 11.79408709  5.62914561  2.03417315 13.60152567  6.544648
  2.51897531  8.02443866  5.92443305  5.30743185]
obj_prev = 67.09083726402451
eta_min = 7.023642441479729e-17	eta_max = 0.9321930357555286
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 15.5538538243213	eta = 0.909090909090909
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 29.16218476187658	eta = 0.4848699515649548
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 22.40002232600407	eta = 0.6312434383873119
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.175688042561685	eta = 0.667740622387296
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.10970401340027	eta = 0.6698278243997874
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.109495669486368	eta = 0.6698344353843775
eta = 0.6698344353843775
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [0.03306684 0.06954534 0.03254197 0.01128472 0.08030519 0.03831555
 0.0141715  0.04697591 0.03411658 0.03096736]
ene_total = [1.91082934 3.36153203 1.90398611 0.90652186 3.82972324 1.99226433
 1.03177315 2.44413259 2.06480829 1.66392474]
ti_comp = [0.53339798 0.56741874 0.52970956 0.54456162 0.56826829 0.56731763
 0.54487186 0.55089405 0.50878776 0.5686981 ]
ti_coms = [0.10500598 0.07098523 0.10869441 0.09384235 0.07013567 0.07108634
 0.09353211 0.08750992 0.12961621 0.06970587]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.94246766e-06 6.52945351e-05 7.67601831e-06 3.02871121e-07
 1.00231444e-04 1.09232674e-05 5.99154859e-07 2.13486048e-05
 9.58745418e-06 5.73890489e-06]
ene_total = [0.46089529 0.31419869 0.47706073 0.4115975  0.31200491 0.31225746
 0.41024983 0.3847471  0.56890568 0.30597546]
optimize_network_iter = 0 obj = 3.9578926480979075
eta = 0.6698344353843775
freqs = [30996407.38459397 61282205.26212957 30716806.61521623 10361284.76037134
 70657812.12672363 33769043.53321395 13004437.39099001 42636067.25492053
 33527323.09434476 27226537.57500104]
eta_min = 0.6698344353843826	eta_max = 0.6698344353843708
af = 0.008298545367209708	bf = 1.3034344286740114	zeta = 0.00912839990393068	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [1.88907249e-06 1.55299480e-05 1.82569896e-06 7.20362393e-08
 2.38395007e-05 2.59803940e-06 1.42505706e-07 5.07764887e-06
 2.28032353e-06 1.36496714e-06]
ene_total = [1.67292505 1.13318653 1.73166738 1.49481175 1.1209777  1.13273715
 1.48988129 1.3947409  2.06499979 1.11055146]
ti_comp = [0.53339798 0.56741874 0.52970956 0.54456162 0.56826829 0.56731763
 0.54487186 0.55089405 0.50878776 0.5686981 ]
ti_coms = [0.10500598 0.07098523 0.10869441 0.09384235 0.07013567 0.07108634
 0.09353211 0.08750992 0.12961621 0.06970587]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.94246766e-06 6.52945351e-05 7.67601831e-06 3.02871121e-07
 1.00231444e-04 1.09232674e-05 5.99154859e-07 2.13486048e-05
 9.58745418e-06 5.73890489e-06]
ene_total = [0.46089529 0.31419869 0.47706073 0.4115975  0.31200491 0.31225746
 0.41024983 0.3847471  0.56890568 0.30597546]
optimize_network_iter = 1 obj = 3.9578926480978276
eta = 0.6698344353843708
freqs = [30996407.384594   61282205.26212971 30716806.61521625 10361284.76037136
 70657812.12672378 33769043.53321403 13004437.39099002 42636067.2549206
 33527323.09434475 27226537.5750011 ]
Done!
At round 40 eta: 0.6698344353843708
At round 40 local rounds: 13.121762881988987
At round 40 global rounds: 43.859113515789716
At round 40 a_n: 14.138223130512367
gradient difference: 0.43250805139541626
train() client id: f_00000-0-0 loss: 1.535609  [   32/  126]
train() client id: f_00000-0-1 loss: 1.231828  [   64/  126]
train() client id: f_00000-0-2 loss: 1.363932  [   96/  126]
train() client id: f_00000-1-0 loss: 1.329269  [   32/  126]
train() client id: f_00000-1-1 loss: 1.244205  [   64/  126]
train() client id: f_00000-1-2 loss: 1.216034  [   96/  126]
train() client id: f_00000-2-0 loss: 1.095472  [   32/  126]
train() client id: f_00000-2-1 loss: 1.197275  [   64/  126]
train() client id: f_00000-2-2 loss: 1.129620  [   96/  126]
train() client id: f_00000-3-0 loss: 1.018290  [   32/  126]
train() client id: f_00000-3-1 loss: 1.082947  [   64/  126]
train() client id: f_00000-3-2 loss: 1.020033  [   96/  126]
train() client id: f_00000-4-0 loss: 1.033438  [   32/  126]
train() client id: f_00000-4-1 loss: 0.971521  [   64/  126]
train() client id: f_00000-4-2 loss: 1.031519  [   96/  126]
train() client id: f_00000-5-0 loss: 0.951234  [   32/  126]
train() client id: f_00000-5-1 loss: 1.027527  [   64/  126]
train() client id: f_00000-5-2 loss: 0.899480  [   96/  126]
train() client id: f_00000-6-0 loss: 0.970847  [   32/  126]
train() client id: f_00000-6-1 loss: 0.918362  [   64/  126]
train() client id: f_00000-6-2 loss: 0.937306  [   96/  126]
train() client id: f_00000-7-0 loss: 0.950002  [   32/  126]
train() client id: f_00000-7-1 loss: 0.852691  [   64/  126]
train() client id: f_00000-7-2 loss: 0.888831  [   96/  126]
train() client id: f_00000-8-0 loss: 0.902856  [   32/  126]
train() client id: f_00000-8-1 loss: 0.815859  [   64/  126]
train() client id: f_00000-8-2 loss: 1.025246  [   96/  126]
train() client id: f_00000-9-0 loss: 0.795799  [   32/  126]
train() client id: f_00000-9-1 loss: 0.880123  [   64/  126]
train() client id: f_00000-9-2 loss: 1.008802  [   96/  126]
train() client id: f_00000-10-0 loss: 0.860335  [   32/  126]
train() client id: f_00000-10-1 loss: 0.897622  [   64/  126]
train() client id: f_00000-10-2 loss: 0.824287  [   96/  126]
train() client id: f_00000-11-0 loss: 0.940018  [   32/  126]
train() client id: f_00000-11-1 loss: 0.943996  [   64/  126]
train() client id: f_00000-11-2 loss: 0.814912  [   96/  126]
train() client id: f_00000-12-0 loss: 0.850577  [   32/  126]
train() client id: f_00000-12-1 loss: 0.827557  [   64/  126]
train() client id: f_00000-12-2 loss: 0.751320  [   96/  126]
train() client id: f_00001-0-0 loss: 0.436414  [   32/  265]
train() client id: f_00001-0-1 loss: 0.383222  [   64/  265]
train() client id: f_00001-0-2 loss: 0.390592  [   96/  265]
train() client id: f_00001-0-3 loss: 0.308877  [  128/  265]
train() client id: f_00001-0-4 loss: 0.362892  [  160/  265]
train() client id: f_00001-0-5 loss: 0.416240  [  192/  265]
train() client id: f_00001-0-6 loss: 0.506916  [  224/  265]
train() client id: f_00001-0-7 loss: 0.412129  [  256/  265]
train() client id: f_00001-1-0 loss: 0.335272  [   32/  265]
train() client id: f_00001-1-1 loss: 0.307220  [   64/  265]
train() client id: f_00001-1-2 loss: 0.389530  [   96/  265]
train() client id: f_00001-1-3 loss: 0.417664  [  128/  265]
train() client id: f_00001-1-4 loss: 0.469224  [  160/  265]
train() client id: f_00001-1-5 loss: 0.312650  [  192/  265]
train() client id: f_00001-1-6 loss: 0.477465  [  224/  265]
train() client id: f_00001-1-7 loss: 0.495162  [  256/  265]
train() client id: f_00001-2-0 loss: 0.561120  [   32/  265]
train() client id: f_00001-2-1 loss: 0.499419  [   64/  265]
train() client id: f_00001-2-2 loss: 0.289100  [   96/  265]
train() client id: f_00001-2-3 loss: 0.309432  [  128/  265]
train() client id: f_00001-2-4 loss: 0.296545  [  160/  265]
train() client id: f_00001-2-5 loss: 0.452747  [  192/  265]
train() client id: f_00001-2-6 loss: 0.393516  [  224/  265]
train() client id: f_00001-2-7 loss: 0.319287  [  256/  265]
train() client id: f_00001-3-0 loss: 0.418489  [   32/  265]
train() client id: f_00001-3-1 loss: 0.446693  [   64/  265]
train() client id: f_00001-3-2 loss: 0.399718  [   96/  265]
train() client id: f_00001-3-3 loss: 0.424062  [  128/  265]
train() client id: f_00001-3-4 loss: 0.284648  [  160/  265]
train() client id: f_00001-3-5 loss: 0.342154  [  192/  265]
train() client id: f_00001-3-6 loss: 0.400186  [  224/  265]
train() client id: f_00001-3-7 loss: 0.359124  [  256/  265]
train() client id: f_00001-4-0 loss: 0.424246  [   32/  265]
train() client id: f_00001-4-1 loss: 0.349224  [   64/  265]
train() client id: f_00001-4-2 loss: 0.342182  [   96/  265]
train() client id: f_00001-4-3 loss: 0.378233  [  128/  265]
train() client id: f_00001-4-4 loss: 0.295171  [  160/  265]
train() client id: f_00001-4-5 loss: 0.446379  [  192/  265]
train() client id: f_00001-4-6 loss: 0.536142  [  224/  265]
train() client id: f_00001-4-7 loss: 0.280248  [  256/  265]
train() client id: f_00001-5-0 loss: 0.429801  [   32/  265]
train() client id: f_00001-5-1 loss: 0.327754  [   64/  265]
train() client id: f_00001-5-2 loss: 0.283287  [   96/  265]
train() client id: f_00001-5-3 loss: 0.426495  [  128/  265]
train() client id: f_00001-5-4 loss: 0.473471  [  160/  265]
train() client id: f_00001-5-5 loss: 0.291884  [  192/  265]
train() client id: f_00001-5-6 loss: 0.496323  [  224/  265]
train() client id: f_00001-5-7 loss: 0.292290  [  256/  265]
train() client id: f_00001-6-0 loss: 0.326321  [   32/  265]
train() client id: f_00001-6-1 loss: 0.399144  [   64/  265]
train() client id: f_00001-6-2 loss: 0.271694  [   96/  265]
train() client id: f_00001-6-3 loss: 0.348545  [  128/  265]
train() client id: f_00001-6-4 loss: 0.399227  [  160/  265]
train() client id: f_00001-6-5 loss: 0.345878  [  192/  265]
train() client id: f_00001-6-6 loss: 0.278648  [  224/  265]
train() client id: f_00001-6-7 loss: 0.621215  [  256/  265]
train() client id: f_00001-7-0 loss: 0.293924  [   32/  265]
train() client id: f_00001-7-1 loss: 0.378555  [   64/  265]
train() client id: f_00001-7-2 loss: 0.337658  [   96/  265]
train() client id: f_00001-7-3 loss: 0.332350  [  128/  265]
train() client id: f_00001-7-4 loss: 0.448862  [  160/  265]
train() client id: f_00001-7-5 loss: 0.342041  [  192/  265]
train() client id: f_00001-7-6 loss: 0.392741  [  224/  265]
train() client id: f_00001-7-7 loss: 0.453131  [  256/  265]
train() client id: f_00001-8-0 loss: 0.294387  [   32/  265]
train() client id: f_00001-8-1 loss: 0.282723  [   64/  265]
train() client id: f_00001-8-2 loss: 0.349220  [   96/  265]
train() client id: f_00001-8-3 loss: 0.527606  [  128/  265]
train() client id: f_00001-8-4 loss: 0.391170  [  160/  265]
train() client id: f_00001-8-5 loss: 0.323966  [  192/  265]
train() client id: f_00001-8-6 loss: 0.435248  [  224/  265]
train() client id: f_00001-8-7 loss: 0.354431  [  256/  265]
train() client id: f_00001-9-0 loss: 0.315829  [   32/  265]
train() client id: f_00001-9-1 loss: 0.535039  [   64/  265]
train() client id: f_00001-9-2 loss: 0.388412  [   96/  265]
train() client id: f_00001-9-3 loss: 0.330584  [  128/  265]
train() client id: f_00001-9-4 loss: 0.323674  [  160/  265]
train() client id: f_00001-9-5 loss: 0.331542  [  192/  265]
train() client id: f_00001-9-6 loss: 0.282726  [  224/  265]
train() client id: f_00001-9-7 loss: 0.433378  [  256/  265]
train() client id: f_00001-10-0 loss: 0.402474  [   32/  265]
train() client id: f_00001-10-1 loss: 0.352632  [   64/  265]
train() client id: f_00001-10-2 loss: 0.319440  [   96/  265]
train() client id: f_00001-10-3 loss: 0.403712  [  128/  265]
train() client id: f_00001-10-4 loss: 0.336533  [  160/  265]
train() client id: f_00001-10-5 loss: 0.365725  [  192/  265]
train() client id: f_00001-10-6 loss: 0.342092  [  224/  265]
train() client id: f_00001-10-7 loss: 0.424840  [  256/  265]
train() client id: f_00001-11-0 loss: 0.268392  [   32/  265]
train() client id: f_00001-11-1 loss: 0.434593  [   64/  265]
train() client id: f_00001-11-2 loss: 0.387379  [   96/  265]
train() client id: f_00001-11-3 loss: 0.331164  [  128/  265]
train() client id: f_00001-11-4 loss: 0.347103  [  160/  265]
train() client id: f_00001-11-5 loss: 0.338468  [  192/  265]
train() client id: f_00001-11-6 loss: 0.366345  [  224/  265]
train() client id: f_00001-11-7 loss: 0.374545  [  256/  265]
train() client id: f_00001-12-0 loss: 0.299920  [   32/  265]
train() client id: f_00001-12-1 loss: 0.444666  [   64/  265]
train() client id: f_00001-12-2 loss: 0.413116  [   96/  265]
train() client id: f_00001-12-3 loss: 0.355083  [  128/  265]
train() client id: f_00001-12-4 loss: 0.449316  [  160/  265]
train() client id: f_00001-12-5 loss: 0.333342  [  192/  265]
train() client id: f_00001-12-6 loss: 0.364706  [  224/  265]
train() client id: f_00001-12-7 loss: 0.272678  [  256/  265]
train() client id: f_00002-0-0 loss: 1.127482  [   32/  124]
train() client id: f_00002-0-1 loss: 1.283707  [   64/  124]
train() client id: f_00002-0-2 loss: 1.310768  [   96/  124]
train() client id: f_00002-1-0 loss: 1.105256  [   32/  124]
train() client id: f_00002-1-1 loss: 1.200951  [   64/  124]
train() client id: f_00002-1-2 loss: 1.312718  [   96/  124]
train() client id: f_00002-2-0 loss: 1.227756  [   32/  124]
train() client id: f_00002-2-1 loss: 1.218712  [   64/  124]
train() client id: f_00002-2-2 loss: 1.160246  [   96/  124]
train() client id: f_00002-3-0 loss: 1.235692  [   32/  124]
train() client id: f_00002-3-1 loss: 1.207221  [   64/  124]
train() client id: f_00002-3-2 loss: 1.012072  [   96/  124]
train() client id: f_00002-4-0 loss: 1.156268  [   32/  124]
train() client id: f_00002-4-1 loss: 1.148461  [   64/  124]
train() client id: f_00002-4-2 loss: 1.152645  [   96/  124]
train() client id: f_00002-5-0 loss: 1.160649  [   32/  124]
train() client id: f_00002-5-1 loss: 1.021160  [   64/  124]
train() client id: f_00002-5-2 loss: 1.168990  [   96/  124]
train() client id: f_00002-6-0 loss: 0.962133  [   32/  124]
train() client id: f_00002-6-1 loss: 1.207024  [   64/  124]
train() client id: f_00002-6-2 loss: 1.223804  [   96/  124]
train() client id: f_00002-7-0 loss: 1.207169  [   32/  124]
train() client id: f_00002-7-1 loss: 1.018952  [   64/  124]
train() client id: f_00002-7-2 loss: 1.051416  [   96/  124]
train() client id: f_00002-8-0 loss: 1.127362  [   32/  124]
train() client id: f_00002-8-1 loss: 1.087410  [   64/  124]
train() client id: f_00002-8-2 loss: 0.999380  [   96/  124]
train() client id: f_00002-9-0 loss: 1.192869  [   32/  124]
train() client id: f_00002-9-1 loss: 1.122278  [   64/  124]
train() client id: f_00002-9-2 loss: 1.027406  [   96/  124]
train() client id: f_00002-10-0 loss: 1.125578  [   32/  124]
train() client id: f_00002-10-1 loss: 1.098046  [   64/  124]
train() client id: f_00002-10-2 loss: 1.113955  [   96/  124]
train() client id: f_00002-11-0 loss: 0.999467  [   32/  124]
train() client id: f_00002-11-1 loss: 1.123149  [   64/  124]
train() client id: f_00002-11-2 loss: 1.021781  [   96/  124]
train() client id: f_00002-12-0 loss: 1.091951  [   32/  124]
train() client id: f_00002-12-1 loss: 1.030616  [   64/  124]
train() client id: f_00002-12-2 loss: 1.177829  [   96/  124]
train() client id: f_00003-0-0 loss: 0.646962  [   32/   43]
train() client id: f_00003-1-0 loss: 0.931214  [   32/   43]
train() client id: f_00003-2-0 loss: 0.581524  [   32/   43]
train() client id: f_00003-3-0 loss: 0.700414  [   32/   43]
train() client id: f_00003-4-0 loss: 0.671909  [   32/   43]
train() client id: f_00003-5-0 loss: 0.608293  [   32/   43]
train() client id: f_00003-6-0 loss: 0.558751  [   32/   43]
train() client id: f_00003-7-0 loss: 0.813998  [   32/   43]
train() client id: f_00003-8-0 loss: 0.429343  [   32/   43]
train() client id: f_00003-9-0 loss: 0.559856  [   32/   43]
train() client id: f_00003-10-0 loss: 0.609648  [   32/   43]
train() client id: f_00003-11-0 loss: 0.481791  [   32/   43]
train() client id: f_00003-12-0 loss: 0.807051  [   32/   43]
train() client id: f_00004-0-0 loss: 0.821369  [   32/  306]
train() client id: f_00004-0-1 loss: 0.985244  [   64/  306]
train() client id: f_00004-0-2 loss: 1.141975  [   96/  306]
train() client id: f_00004-0-3 loss: 0.800071  [  128/  306]
train() client id: f_00004-0-4 loss: 0.753995  [  160/  306]
train() client id: f_00004-0-5 loss: 0.791062  [  192/  306]
train() client id: f_00004-0-6 loss: 0.777108  [  224/  306]
train() client id: f_00004-0-7 loss: 0.930997  [  256/  306]
train() client id: f_00004-0-8 loss: 0.826995  [  288/  306]
train() client id: f_00004-1-0 loss: 1.001934  [   32/  306]
train() client id: f_00004-1-1 loss: 0.893619  [   64/  306]
train() client id: f_00004-1-2 loss: 0.783486  [   96/  306]
train() client id: f_00004-1-3 loss: 0.947157  [  128/  306]
train() client id: f_00004-1-4 loss: 0.882251  [  160/  306]
train() client id: f_00004-1-5 loss: 0.834860  [  192/  306]
train() client id: f_00004-1-6 loss: 0.904431  [  224/  306]
train() client id: f_00004-1-7 loss: 0.901291  [  256/  306]
train() client id: f_00004-1-8 loss: 0.867657  [  288/  306]
train() client id: f_00004-2-0 loss: 0.824703  [   32/  306]
train() client id: f_00004-2-1 loss: 0.639621  [   64/  306]
train() client id: f_00004-2-2 loss: 0.925270  [   96/  306]
train() client id: f_00004-2-3 loss: 0.875281  [  128/  306]
train() client id: f_00004-2-4 loss: 0.897273  [  160/  306]
train() client id: f_00004-2-5 loss: 0.887007  [  192/  306]
train() client id: f_00004-2-6 loss: 0.991483  [  224/  306]
train() client id: f_00004-2-7 loss: 0.931689  [  256/  306]
train() client id: f_00004-2-8 loss: 0.939691  [  288/  306]
train() client id: f_00004-3-0 loss: 0.774725  [   32/  306]
train() client id: f_00004-3-1 loss: 0.908631  [   64/  306]
train() client id: f_00004-3-2 loss: 1.034147  [   96/  306]
train() client id: f_00004-3-3 loss: 0.956273  [  128/  306]
train() client id: f_00004-3-4 loss: 0.787828  [  160/  306]
train() client id: f_00004-3-5 loss: 0.789237  [  192/  306]
train() client id: f_00004-3-6 loss: 0.935433  [  224/  306]
train() client id: f_00004-3-7 loss: 0.944196  [  256/  306]
train() client id: f_00004-3-8 loss: 0.782954  [  288/  306]
train() client id: f_00004-4-0 loss: 0.709560  [   32/  306]
train() client id: f_00004-4-1 loss: 0.805237  [   64/  306]
train() client id: f_00004-4-2 loss: 0.998647  [   96/  306]
train() client id: f_00004-4-3 loss: 0.827011  [  128/  306]
train() client id: f_00004-4-4 loss: 0.956093  [  160/  306]
train() client id: f_00004-4-5 loss: 0.981884  [  192/  306]
train() client id: f_00004-4-6 loss: 1.044599  [  224/  306]
train() client id: f_00004-4-7 loss: 0.793361  [  256/  306]
train() client id: f_00004-4-8 loss: 0.766252  [  288/  306]
train() client id: f_00004-5-0 loss: 0.820061  [   32/  306]
train() client id: f_00004-5-1 loss: 0.903575  [   64/  306]
train() client id: f_00004-5-2 loss: 0.937381  [   96/  306]
train() client id: f_00004-5-3 loss: 0.811133  [  128/  306]
train() client id: f_00004-5-4 loss: 0.963648  [  160/  306]
train() client id: f_00004-5-5 loss: 0.946175  [  192/  306]
train() client id: f_00004-5-6 loss: 0.958350  [  224/  306]
train() client id: f_00004-5-7 loss: 0.722113  [  256/  306]
train() client id: f_00004-5-8 loss: 0.849127  [  288/  306]
train() client id: f_00004-6-0 loss: 0.764448  [   32/  306]
train() client id: f_00004-6-1 loss: 1.002621  [   64/  306]
train() client id: f_00004-6-2 loss: 0.859458  [   96/  306]
train() client id: f_00004-6-3 loss: 0.821058  [  128/  306]
train() client id: f_00004-6-4 loss: 0.905517  [  160/  306]
train() client id: f_00004-6-5 loss: 0.849329  [  192/  306]
train() client id: f_00004-6-6 loss: 0.785722  [  224/  306]
train() client id: f_00004-6-7 loss: 0.967758  [  256/  306]
train() client id: f_00004-6-8 loss: 0.954914  [  288/  306]
train() client id: f_00004-7-0 loss: 0.759650  [   32/  306]
train() client id: f_00004-7-1 loss: 0.900774  [   64/  306]
train() client id: f_00004-7-2 loss: 0.801936  [   96/  306]
train() client id: f_00004-7-3 loss: 0.859828  [  128/  306]
train() client id: f_00004-7-4 loss: 0.820961  [  160/  306]
train() client id: f_00004-7-5 loss: 0.783704  [  192/  306]
train() client id: f_00004-7-6 loss: 0.840017  [  224/  306]
train() client id: f_00004-7-7 loss: 1.046839  [  256/  306]
train() client id: f_00004-7-8 loss: 1.004200  [  288/  306]
train() client id: f_00004-8-0 loss: 0.802596  [   32/  306]
train() client id: f_00004-8-1 loss: 0.846585  [   64/  306]
train() client id: f_00004-8-2 loss: 0.904448  [   96/  306]
train() client id: f_00004-8-3 loss: 1.087195  [  128/  306]
train() client id: f_00004-8-4 loss: 0.846628  [  160/  306]
train() client id: f_00004-8-5 loss: 0.782228  [  192/  306]
train() client id: f_00004-8-6 loss: 0.845240  [  224/  306]
train() client id: f_00004-8-7 loss: 0.878526  [  256/  306]
train() client id: f_00004-8-8 loss: 0.859764  [  288/  306]
train() client id: f_00004-9-0 loss: 0.904757  [   32/  306]
train() client id: f_00004-9-1 loss: 0.955238  [   64/  306]
train() client id: f_00004-9-2 loss: 0.751217  [   96/  306]
train() client id: f_00004-9-3 loss: 1.012428  [  128/  306]
train() client id: f_00004-9-4 loss: 0.800213  [  160/  306]
train() client id: f_00004-9-5 loss: 0.843158  [  192/  306]
train() client id: f_00004-9-6 loss: 0.865573  [  224/  306]
train() client id: f_00004-9-7 loss: 0.785496  [  256/  306]
train() client id: f_00004-9-8 loss: 0.902093  [  288/  306]
train() client id: f_00004-10-0 loss: 0.893251  [   32/  306]
train() client id: f_00004-10-1 loss: 0.778271  [   64/  306]
train() client id: f_00004-10-2 loss: 0.755459  [   96/  306]
train() client id: f_00004-10-3 loss: 0.901338  [  128/  306]
train() client id: f_00004-10-4 loss: 0.949415  [  160/  306]
train() client id: f_00004-10-5 loss: 0.908500  [  192/  306]
train() client id: f_00004-10-6 loss: 0.879536  [  224/  306]
train() client id: f_00004-10-7 loss: 0.835651  [  256/  306]
train() client id: f_00004-10-8 loss: 0.878267  [  288/  306]
train() client id: f_00004-11-0 loss: 0.873031  [   32/  306]
train() client id: f_00004-11-1 loss: 0.873413  [   64/  306]
train() client id: f_00004-11-2 loss: 0.804219  [   96/  306]
train() client id: f_00004-11-3 loss: 0.873091  [  128/  306]
train() client id: f_00004-11-4 loss: 0.939294  [  160/  306]
train() client id: f_00004-11-5 loss: 0.834998  [  192/  306]
train() client id: f_00004-11-6 loss: 0.835213  [  224/  306]
train() client id: f_00004-11-7 loss: 0.873563  [  256/  306]
train() client id: f_00004-11-8 loss: 0.823902  [  288/  306]
train() client id: f_00004-12-0 loss: 0.911620  [   32/  306]
train() client id: f_00004-12-1 loss: 0.873595  [   64/  306]
train() client id: f_00004-12-2 loss: 0.846798  [   96/  306]
train() client id: f_00004-12-3 loss: 0.841488  [  128/  306]
train() client id: f_00004-12-4 loss: 0.870666  [  160/  306]
train() client id: f_00004-12-5 loss: 0.885039  [  192/  306]
train() client id: f_00004-12-6 loss: 0.915711  [  224/  306]
train() client id: f_00004-12-7 loss: 0.806560  [  256/  306]
train() client id: f_00004-12-8 loss: 0.903202  [  288/  306]
train() client id: f_00005-0-0 loss: 0.530507  [   32/  146]
train() client id: f_00005-0-1 loss: 0.536961  [   64/  146]
train() client id: f_00005-0-2 loss: 0.903551  [   96/  146]
train() client id: f_00005-0-3 loss: 0.803763  [  128/  146]
train() client id: f_00005-1-0 loss: 0.775689  [   32/  146]
train() client id: f_00005-1-1 loss: 0.781532  [   64/  146]
train() client id: f_00005-1-2 loss: 0.567539  [   96/  146]
train() client id: f_00005-1-3 loss: 0.572425  [  128/  146]
train() client id: f_00005-2-0 loss: 0.616961  [   32/  146]
train() client id: f_00005-2-1 loss: 0.643906  [   64/  146]
train() client id: f_00005-2-2 loss: 0.852184  [   96/  146]
train() client id: f_00005-2-3 loss: 0.613878  [  128/  146]
train() client id: f_00005-3-0 loss: 0.994135  [   32/  146]
train() client id: f_00005-3-1 loss: 0.791200  [   64/  146]
train() client id: f_00005-3-2 loss: 0.508237  [   96/  146]
train() client id: f_00005-3-3 loss: 0.409979  [  128/  146]
train() client id: f_00005-4-0 loss: 0.605513  [   32/  146]
train() client id: f_00005-4-1 loss: 0.584356  [   64/  146]
train() client id: f_00005-4-2 loss: 1.102257  [   96/  146]
train() client id: f_00005-4-3 loss: 0.582896  [  128/  146]
train() client id: f_00005-5-0 loss: 0.661546  [   32/  146]
train() client id: f_00005-5-1 loss: 0.992998  [   64/  146]
train() client id: f_00005-5-2 loss: 0.656506  [   96/  146]
train() client id: f_00005-5-3 loss: 0.556210  [  128/  146]
train() client id: f_00005-6-0 loss: 0.554488  [   32/  146]
train() client id: f_00005-6-1 loss: 0.596928  [   64/  146]
train() client id: f_00005-6-2 loss: 0.649134  [   96/  146]
train() client id: f_00005-6-3 loss: 0.738487  [  128/  146]
train() client id: f_00005-7-0 loss: 0.969976  [   32/  146]
train() client id: f_00005-7-1 loss: 0.545952  [   64/  146]
train() client id: f_00005-7-2 loss: 0.498474  [   96/  146]
train() client id: f_00005-7-3 loss: 0.739837  [  128/  146]
train() client id: f_00005-8-0 loss: 0.683292  [   32/  146]
train() client id: f_00005-8-1 loss: 0.617973  [   64/  146]
train() client id: f_00005-8-2 loss: 0.653492  [   96/  146]
train() client id: f_00005-8-3 loss: 0.506739  [  128/  146]
train() client id: f_00005-9-0 loss: 0.794982  [   32/  146]
train() client id: f_00005-9-1 loss: 0.659353  [   64/  146]
train() client id: f_00005-9-2 loss: 0.511599  [   96/  146]
train() client id: f_00005-9-3 loss: 0.869111  [  128/  146]
train() client id: f_00005-10-0 loss: 0.945787  [   32/  146]
train() client id: f_00005-10-1 loss: 0.478089  [   64/  146]
train() client id: f_00005-10-2 loss: 0.608302  [   96/  146]
train() client id: f_00005-10-3 loss: 0.818497  [  128/  146]
train() client id: f_00005-11-0 loss: 0.819299  [   32/  146]
train() client id: f_00005-11-1 loss: 0.915428  [   64/  146]
train() client id: f_00005-11-2 loss: 0.548515  [   96/  146]
train() client id: f_00005-11-3 loss: 0.671807  [  128/  146]
train() client id: f_00005-12-0 loss: 0.850395  [   32/  146]
train() client id: f_00005-12-1 loss: 0.783265  [   64/  146]
train() client id: f_00005-12-2 loss: 0.633492  [   96/  146]
train() client id: f_00005-12-3 loss: 0.565258  [  128/  146]
train() client id: f_00006-0-0 loss: 0.579737  [   32/   54]
train() client id: f_00006-1-0 loss: 0.560231  [   32/   54]
train() client id: f_00006-2-0 loss: 0.508200  [   32/   54]
train() client id: f_00006-3-0 loss: 0.473941  [   32/   54]
train() client id: f_00006-4-0 loss: 0.574172  [   32/   54]
train() client id: f_00006-5-0 loss: 0.533518  [   32/   54]
train() client id: f_00006-6-0 loss: 0.559810  [   32/   54]
train() client id: f_00006-7-0 loss: 0.539087  [   32/   54]
train() client id: f_00006-8-0 loss: 0.470134  [   32/   54]
train() client id: f_00006-9-0 loss: 0.486503  [   32/   54]
train() client id: f_00006-10-0 loss: 0.584120  [   32/   54]
train() client id: f_00006-11-0 loss: 0.517960  [   32/   54]
train() client id: f_00006-12-0 loss: 0.591228  [   32/   54]
train() client id: f_00007-0-0 loss: 0.487560  [   32/  179]
train() client id: f_00007-0-1 loss: 0.545078  [   64/  179]
train() client id: f_00007-0-2 loss: 0.616675  [   96/  179]
train() client id: f_00007-0-3 loss: 0.611541  [  128/  179]
train() client id: f_00007-0-4 loss: 0.591965  [  160/  179]
train() client id: f_00007-1-0 loss: 0.471233  [   32/  179]
train() client id: f_00007-1-1 loss: 0.475899  [   64/  179]
train() client id: f_00007-1-2 loss: 0.560789  [   96/  179]
train() client id: f_00007-1-3 loss: 0.528905  [  128/  179]
train() client id: f_00007-1-4 loss: 0.665851  [  160/  179]
train() client id: f_00007-2-0 loss: 0.528268  [   32/  179]
train() client id: f_00007-2-1 loss: 0.495724  [   64/  179]
train() client id: f_00007-2-2 loss: 0.416080  [   96/  179]
train() client id: f_00007-2-3 loss: 0.502850  [  128/  179]
train() client id: f_00007-2-4 loss: 0.825535  [  160/  179]
train() client id: f_00007-3-0 loss: 0.821460  [   32/  179]
train() client id: f_00007-3-1 loss: 0.587259  [   64/  179]
train() client id: f_00007-3-2 loss: 0.389166  [   96/  179]
train() client id: f_00007-3-3 loss: 0.410758  [  128/  179]
train() client id: f_00007-3-4 loss: 0.377332  [  160/  179]
train() client id: f_00007-4-0 loss: 0.570291  [   32/  179]
train() client id: f_00007-4-1 loss: 0.543342  [   64/  179]
train() client id: f_00007-4-2 loss: 0.514298  [   96/  179]
train() client id: f_00007-4-3 loss: 0.443438  [  128/  179]
train() client id: f_00007-4-4 loss: 0.507751  [  160/  179]
train() client id: f_00007-5-0 loss: 0.390949  [   32/  179]
train() client id: f_00007-5-1 loss: 0.710256  [   64/  179]
train() client id: f_00007-5-2 loss: 0.746386  [   96/  179]
train() client id: f_00007-5-3 loss: 0.364974  [  128/  179]
train() client id: f_00007-5-4 loss: 0.373772  [  160/  179]
train() client id: f_00007-6-0 loss: 0.391227  [   32/  179]
train() client id: f_00007-6-1 loss: 0.530906  [   64/  179]
train() client id: f_00007-6-2 loss: 0.730120  [   96/  179]
train() client id: f_00007-6-3 loss: 0.478497  [  128/  179]
train() client id: f_00007-6-4 loss: 0.339803  [  160/  179]
train() client id: f_00007-7-0 loss: 0.318893  [   32/  179]
train() client id: f_00007-7-1 loss: 0.825469  [   64/  179]
train() client id: f_00007-7-2 loss: 0.427342  [   96/  179]
train() client id: f_00007-7-3 loss: 0.565913  [  128/  179]
train() client id: f_00007-7-4 loss: 0.349359  [  160/  179]
train() client id: f_00007-8-0 loss: 0.370307  [   32/  179]
train() client id: f_00007-8-1 loss: 0.434952  [   64/  179]
train() client id: f_00007-8-2 loss: 0.615626  [   96/  179]
train() client id: f_00007-8-3 loss: 0.352448  [  128/  179]
train() client id: f_00007-8-4 loss: 0.608982  [  160/  179]
train() client id: f_00007-9-0 loss: 0.444705  [   32/  179]
train() client id: f_00007-9-1 loss: 0.526759  [   64/  179]
train() client id: f_00007-9-2 loss: 0.367390  [   96/  179]
train() client id: f_00007-9-3 loss: 0.602041  [  128/  179]
train() client id: f_00007-9-4 loss: 0.613108  [  160/  179]
train() client id: f_00007-10-0 loss: 0.506833  [   32/  179]
train() client id: f_00007-10-1 loss: 0.432344  [   64/  179]
train() client id: f_00007-10-2 loss: 0.562421  [   96/  179]
train() client id: f_00007-10-3 loss: 0.496110  [  128/  179]
train() client id: f_00007-10-4 loss: 0.450882  [  160/  179]
train() client id: f_00007-11-0 loss: 0.379647  [   32/  179]
train() client id: f_00007-11-1 loss: 0.424251  [   64/  179]
train() client id: f_00007-11-2 loss: 0.415096  [   96/  179]
train() client id: f_00007-11-3 loss: 0.692078  [  128/  179]
train() client id: f_00007-11-4 loss: 0.455294  [  160/  179]
train() client id: f_00007-12-0 loss: 0.696485  [   32/  179]
train() client id: f_00007-12-1 loss: 0.428024  [   64/  179]
train() client id: f_00007-12-2 loss: 0.448914  [   96/  179]
train() client id: f_00007-12-3 loss: 0.499378  [  128/  179]
train() client id: f_00007-12-4 loss: 0.439420  [  160/  179]
train() client id: f_00008-0-0 loss: 0.781976  [   32/  130]
train() client id: f_00008-0-1 loss: 0.737002  [   64/  130]
train() client id: f_00008-0-2 loss: 0.689884  [   96/  130]
train() client id: f_00008-0-3 loss: 0.759607  [  128/  130]
train() client id: f_00008-1-0 loss: 0.739871  [   32/  130]
train() client id: f_00008-1-1 loss: 0.773389  [   64/  130]
train() client id: f_00008-1-2 loss: 0.656587  [   96/  130]
train() client id: f_00008-1-3 loss: 0.803192  [  128/  130]
train() client id: f_00008-2-0 loss: 0.693874  [   32/  130]
train() client id: f_00008-2-1 loss: 0.782777  [   64/  130]
train() client id: f_00008-2-2 loss: 0.715228  [   96/  130]
train() client id: f_00008-2-3 loss: 0.768291  [  128/  130]
train() client id: f_00008-3-0 loss: 0.664939  [   32/  130]
train() client id: f_00008-3-1 loss: 0.867596  [   64/  130]
train() client id: f_00008-3-2 loss: 0.610267  [   96/  130]
train() client id: f_00008-3-3 loss: 0.813026  [  128/  130]
train() client id: f_00008-4-0 loss: 0.662830  [   32/  130]
train() client id: f_00008-4-1 loss: 0.721493  [   64/  130]
train() client id: f_00008-4-2 loss: 0.776418  [   96/  130]
train() client id: f_00008-4-3 loss: 0.811981  [  128/  130]
train() client id: f_00008-5-0 loss: 0.725027  [   32/  130]
train() client id: f_00008-5-1 loss: 0.845810  [   64/  130]
train() client id: f_00008-5-2 loss: 0.551328  [   96/  130]
train() client id: f_00008-5-3 loss: 0.821985  [  128/  130]
train() client id: f_00008-6-0 loss: 0.738027  [   32/  130]
train() client id: f_00008-6-1 loss: 0.759918  [   64/  130]
train() client id: f_00008-6-2 loss: 0.674840  [   96/  130]
train() client id: f_00008-6-3 loss: 0.757942  [  128/  130]
train() client id: f_00008-7-0 loss: 0.740273  [   32/  130]
train() client id: f_00008-7-1 loss: 0.777722  [   64/  130]
train() client id: f_00008-7-2 loss: 0.823268  [   96/  130]
train() client id: f_00008-7-3 loss: 0.629040  [  128/  130]
train() client id: f_00008-8-0 loss: 0.735701  [   32/  130]
train() client id: f_00008-8-1 loss: 0.665057  [   64/  130]
train() client id: f_00008-8-2 loss: 0.819237  [   96/  130]
train() client id: f_00008-8-3 loss: 0.730134  [  128/  130]
train() client id: f_00008-9-0 loss: 0.794285  [   32/  130]
train() client id: f_00008-9-1 loss: 0.740664  [   64/  130]
train() client id: f_00008-9-2 loss: 0.610178  [   96/  130]
train() client id: f_00008-9-3 loss: 0.816149  [  128/  130]
train() client id: f_00008-10-0 loss: 0.739017  [   32/  130]
train() client id: f_00008-10-1 loss: 0.691970  [   64/  130]
train() client id: f_00008-10-2 loss: 0.757441  [   96/  130]
train() client id: f_00008-10-3 loss: 0.782522  [  128/  130]
train() client id: f_00008-11-0 loss: 0.575424  [   32/  130]
train() client id: f_00008-11-1 loss: 0.878634  [   64/  130]
train() client id: f_00008-11-2 loss: 0.738182  [   96/  130]
train() client id: f_00008-11-3 loss: 0.743500  [  128/  130]
train() client id: f_00008-12-0 loss: 0.600188  [   32/  130]
train() client id: f_00008-12-1 loss: 0.752962  [   64/  130]
train() client id: f_00008-12-2 loss: 0.703039  [   96/  130]
train() client id: f_00008-12-3 loss: 0.896290  [  128/  130]
train() client id: f_00009-0-0 loss: 1.082505  [   32/  118]
train() client id: f_00009-0-1 loss: 1.296938  [   64/  118]
train() client id: f_00009-0-2 loss: 1.249519  [   96/  118]
train() client id: f_00009-1-0 loss: 1.068322  [   32/  118]
train() client id: f_00009-1-1 loss: 1.163592  [   64/  118]
train() client id: f_00009-1-2 loss: 1.188199  [   96/  118]
train() client id: f_00009-2-0 loss: 1.167513  [   32/  118]
train() client id: f_00009-2-1 loss: 0.969775  [   64/  118]
train() client id: f_00009-2-2 loss: 1.238141  [   96/  118]
train() client id: f_00009-3-0 loss: 1.104324  [   32/  118]
train() client id: f_00009-3-1 loss: 1.195264  [   64/  118]
train() client id: f_00009-3-2 loss: 0.915177  [   96/  118]
train() client id: f_00009-4-0 loss: 1.007706  [   32/  118]
train() client id: f_00009-4-1 loss: 1.107641  [   64/  118]
train() client id: f_00009-4-2 loss: 0.960635  [   96/  118]
train() client id: f_00009-5-0 loss: 1.004542  [   32/  118]
train() client id: f_00009-5-1 loss: 1.068804  [   64/  118]
train() client id: f_00009-5-2 loss: 1.023320  [   96/  118]
train() client id: f_00009-6-0 loss: 0.905417  [   32/  118]
train() client id: f_00009-6-1 loss: 1.002132  [   64/  118]
train() client id: f_00009-6-2 loss: 1.136935  [   96/  118]
train() client id: f_00009-7-0 loss: 0.974923  [   32/  118]
train() client id: f_00009-7-1 loss: 1.005322  [   64/  118]
train() client id: f_00009-7-2 loss: 0.969532  [   96/  118]
train() client id: f_00009-8-0 loss: 1.020361  [   32/  118]
train() client id: f_00009-8-1 loss: 0.989453  [   64/  118]
train() client id: f_00009-8-2 loss: 0.894922  [   96/  118]
train() client id: f_00009-9-0 loss: 0.998276  [   32/  118]
train() client id: f_00009-9-1 loss: 0.974493  [   64/  118]
train() client id: f_00009-9-2 loss: 0.973490  [   96/  118]
train() client id: f_00009-10-0 loss: 1.025936  [   32/  118]
train() client id: f_00009-10-1 loss: 0.987360  [   64/  118]
train() client id: f_00009-10-2 loss: 0.977633  [   96/  118]
train() client id: f_00009-11-0 loss: 1.009962  [   32/  118]
train() client id: f_00009-11-1 loss: 0.937618  [   64/  118]
train() client id: f_00009-11-2 loss: 0.933375  [   96/  118]
train() client id: f_00009-12-0 loss: 1.086071  [   32/  118]
train() client id: f_00009-12-1 loss: 0.987406  [   64/  118]
train() client id: f_00009-12-2 loss: 0.925989  [   96/  118]
At round 40 accuracy: 0.6472148541114059
At round 40 training accuracy: 0.5915492957746479
At round 40 training loss: 0.8220776979504797
update_location
xs = [  -3.9056584     4.20031788  220.00902392   18.81129433    0.97929623
    3.95640986 -182.44319194 -161.32485185  204.66397685 -147.06087855]
ys = [ 212.5879595   195.55583871    1.32061395 -182.45517586  174.35018685
  157.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [234.96573089 219.68097033 241.67274283 208.9108805  200.99489216
 186.87150493 208.06827924 189.80617503 228.46446856 177.88455205]
dists_bs = [175.17456558 180.3754319  431.50510786 406.61655224 175.98049596
 179.77972646 177.78086556 174.71366329 411.01004024 173.26594513]
uav_gains = [9.82218151e-12 1.26815403e-11 8.69804006e-12 1.49604850e-11
 1.68083716e-11 2.06049621e-11 1.51494992e-11 1.97523587e-11
 1.09881270e-11 2.34789484e-11]
bs_gains = [5.77530797e-11 5.32105086e-11 4.62736173e-12 5.46482092e-12
 5.70155582e-11 5.37056630e-11 5.54135584e-11 5.81806876e-11
 5.30282480e-12 5.95521015e-11]
Round 41
-------------------------------
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.58035445 11.51525536  5.49993513  1.98836081 13.27975002  6.38965832
  2.46168242  7.8365251   5.78633913  5.1816046 ]
obj_prev = 65.51946532847218
eta_min = 2.9576516171485466e-17	eta_max = 0.9329613748543103
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 15.185923913957124	eta = 0.9090909090909091
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 28.643068262013102	eta = 0.4819799767971639
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 21.937413464592197	eta = 0.6293077986886223
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.723057688485657	eta = 0.6661847678972268
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.65725512436301	eta = 0.668306863289048
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.65704485277882	eta = 0.6683136660986402
eta = 0.6683136660986402
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [0.0332544  0.06993981 0.03272655 0.01134872 0.08076069 0.03853288
 0.01425189 0.04724236 0.0343101  0.03114301]
ene_total = [1.87549698 3.28425833 1.87007596 0.89063245 3.74131718 1.94496118
 1.01302102 2.39242243 2.02108855 1.62377077]
ti_comp = [0.54825773 0.58461281 0.54425104 0.56025271 0.5855944  0.58474602
 0.56057165 0.56686532 0.52465853 0.58619944]
ti_coms = [0.10745239 0.0710973  0.11145908 0.09545741 0.07011572 0.0709641
 0.09513846 0.0888448  0.13105159 0.06951068]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.64641487e-06 6.25628775e-05 7.39575090e-06 2.91040345e-07
 9.60033089e-05 1.04577867e-05 5.75751087e-07 2.05076110e-05
 9.17048565e-06 5.49377638e-06]
ene_total = [0.45834474 0.30572102 0.4754127  0.40690231 0.30296238 0.30293225
 0.40555492 0.37957759 0.55900197 0.29652539]
optimize_network_iter = 0 obj = 3.892935261137077
eta = 0.6683136660986402
freqs = [30327342.94858874 59817206.41115162 30065677.93401769 10128218.84644976
 68956163.97393337 32948387.69791197 12711921.71227483 41669829.72547625
 32697548.88052001 26563493.52335148]
eta_min = 0.6683136660986426	eta_max = 0.6683136660986362
af = 0.007721858371378171	bf = 1.2881215340611454	zeta = 0.008494044208515988	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [1.80840054e-06 1.47963121e-05 1.74911774e-06 6.88319331e-08
 2.27050766e-05 2.47329858e-06 1.36166896e-07 4.85011281e-06
 2.16884794e-06 1.29929494e-06]
ene_total = [1.67138562 1.10800963 1.73368868 1.48456874 1.09397392 1.1040216
 1.47961889 1.38247263 2.03845756 1.08123529]
ti_comp = [0.54825773 0.58461281 0.54425104 0.56025271 0.5855944  0.58474602
 0.56057165 0.56686532 0.52465853 0.58619944]
ti_coms = [0.10745239 0.0710973  0.11145908 0.09545741 0.07011572 0.0709641
 0.09513846 0.0888448  0.13105159 0.06951068]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.64641487e-06 6.25628775e-05 7.39575090e-06 2.91040345e-07
 9.60033089e-05 1.04577867e-05 5.75751087e-07 2.05076110e-05
 9.17048565e-06 5.49377638e-06]
ene_total = [0.45834474 0.30572102 0.4754127  0.40690231 0.30296238 0.30293225
 0.40555492 0.37957759 0.55900197 0.29652539]
optimize_network_iter = 1 obj = 3.8929352611370294
eta = 0.6683136660986362
freqs = [30327342.94858875 59817206.41115168 30065677.9340177  10128218.84644976
 68956163.97393346 32948387.697912   12711921.71227484 41669829.72547627
 32697548.88052    26563493.52335151]
Done!
At round 41 eta: 0.6683136660986362
At round 41 local rounds: 13.19619071883817
At round 41 global rounds: 42.625280831488105
At round 41 a_n: 13.79567728354305
gradient difference: 0.4040200710296631
train() client id: f_00000-0-0 loss: 0.997897  [   32/  126]
train() client id: f_00000-0-1 loss: 1.393116  [   64/  126]
train() client id: f_00000-0-2 loss: 1.266255  [   96/  126]
train() client id: f_00000-1-0 loss: 1.217265  [   32/  126]
train() client id: f_00000-1-1 loss: 1.062376  [   64/  126]
train() client id: f_00000-1-2 loss: 1.174801  [   96/  126]
train() client id: f_00000-2-0 loss: 1.017965  [   32/  126]
train() client id: f_00000-2-1 loss: 1.129818  [   64/  126]
train() client id: f_00000-2-2 loss: 1.050748  [   96/  126]
train() client id: f_00000-3-0 loss: 1.154527  [   32/  126]
train() client id: f_00000-3-1 loss: 0.993251  [   64/  126]
train() client id: f_00000-3-2 loss: 0.979680  [   96/  126]
train() client id: f_00000-4-0 loss: 0.884049  [   32/  126]
train() client id: f_00000-4-1 loss: 0.987382  [   64/  126]
train() client id: f_00000-4-2 loss: 0.963292  [   96/  126]
train() client id: f_00000-5-0 loss: 1.001482  [   32/  126]
train() client id: f_00000-5-1 loss: 0.975101  [   64/  126]
train() client id: f_00000-5-2 loss: 0.968564  [   96/  126]
train() client id: f_00000-6-0 loss: 0.926900  [   32/  126]
train() client id: f_00000-6-1 loss: 0.872385  [   64/  126]
train() client id: f_00000-6-2 loss: 0.969534  [   96/  126]
train() client id: f_00000-7-0 loss: 0.929465  [   32/  126]
train() client id: f_00000-7-1 loss: 1.080707  [   64/  126]
train() client id: f_00000-7-2 loss: 0.809689  [   96/  126]
train() client id: f_00000-8-0 loss: 1.056463  [   32/  126]
train() client id: f_00000-8-1 loss: 0.854237  [   64/  126]
train() client id: f_00000-8-2 loss: 0.855699  [   96/  126]
train() client id: f_00000-9-0 loss: 0.964740  [   32/  126]
train() client id: f_00000-9-1 loss: 0.987932  [   64/  126]
train() client id: f_00000-9-2 loss: 0.754431  [   96/  126]
train() client id: f_00000-10-0 loss: 0.907087  [   32/  126]
train() client id: f_00000-10-1 loss: 0.927279  [   64/  126]
train() client id: f_00000-10-2 loss: 0.850502  [   96/  126]
train() client id: f_00000-11-0 loss: 0.864670  [   32/  126]
train() client id: f_00000-11-1 loss: 0.846258  [   64/  126]
train() client id: f_00000-11-2 loss: 0.916838  [   96/  126]
train() client id: f_00000-12-0 loss: 0.893234  [   32/  126]
train() client id: f_00000-12-1 loss: 0.915037  [   64/  126]
train() client id: f_00000-12-2 loss: 0.922349  [   96/  126]
train() client id: f_00001-0-0 loss: 0.566429  [   32/  265]
train() client id: f_00001-0-1 loss: 0.505313  [   64/  265]
train() client id: f_00001-0-2 loss: 0.539087  [   96/  265]
train() client id: f_00001-0-3 loss: 0.461372  [  128/  265]
train() client id: f_00001-0-4 loss: 0.525657  [  160/  265]
train() client id: f_00001-0-5 loss: 0.510086  [  192/  265]
train() client id: f_00001-0-6 loss: 0.529016  [  224/  265]
train() client id: f_00001-0-7 loss: 0.450100  [  256/  265]
train() client id: f_00001-1-0 loss: 0.544107  [   32/  265]
train() client id: f_00001-1-1 loss: 0.433072  [   64/  265]
train() client id: f_00001-1-2 loss: 0.622364  [   96/  265]
train() client id: f_00001-1-3 loss: 0.475014  [  128/  265]
train() client id: f_00001-1-4 loss: 0.451324  [  160/  265]
train() client id: f_00001-1-5 loss: 0.492449  [  192/  265]
train() client id: f_00001-1-6 loss: 0.435126  [  224/  265]
train() client id: f_00001-1-7 loss: 0.527647  [  256/  265]
train() client id: f_00001-2-0 loss: 0.580063  [   32/  265]
train() client id: f_00001-2-1 loss: 0.440086  [   64/  265]
train() client id: f_00001-2-2 loss: 0.429250  [   96/  265]
train() client id: f_00001-2-3 loss: 0.466886  [  128/  265]
train() client id: f_00001-2-4 loss: 0.581907  [  160/  265]
train() client id: f_00001-2-5 loss: 0.477797  [  192/  265]
train() client id: f_00001-2-6 loss: 0.507068  [  224/  265]
train() client id: f_00001-2-7 loss: 0.512456  [  256/  265]
train() client id: f_00001-3-0 loss: 0.435153  [   32/  265]
train() client id: f_00001-3-1 loss: 0.625447  [   64/  265]
train() client id: f_00001-3-2 loss: 0.451088  [   96/  265]
train() client id: f_00001-3-3 loss: 0.440209  [  128/  265]
train() client id: f_00001-3-4 loss: 0.477133  [  160/  265]
train() client id: f_00001-3-5 loss: 0.541378  [  192/  265]
train() client id: f_00001-3-6 loss: 0.506637  [  224/  265]
train() client id: f_00001-3-7 loss: 0.480030  [  256/  265]
train() client id: f_00001-4-0 loss: 0.516704  [   32/  265]
train() client id: f_00001-4-1 loss: 0.528575  [   64/  265]
train() client id: f_00001-4-2 loss: 0.494740  [   96/  265]
train() client id: f_00001-4-3 loss: 0.496016  [  128/  265]
train() client id: f_00001-4-4 loss: 0.507622  [  160/  265]
train() client id: f_00001-4-5 loss: 0.533151  [  192/  265]
train() client id: f_00001-4-6 loss: 0.395529  [  224/  265]
train() client id: f_00001-4-7 loss: 0.478215  [  256/  265]
train() client id: f_00001-5-0 loss: 0.458402  [   32/  265]
train() client id: f_00001-5-1 loss: 0.485199  [   64/  265]
train() client id: f_00001-5-2 loss: 0.430460  [   96/  265]
train() client id: f_00001-5-3 loss: 0.419012  [  128/  265]
train() client id: f_00001-5-4 loss: 0.488336  [  160/  265]
train() client id: f_00001-5-5 loss: 0.423816  [  192/  265]
train() client id: f_00001-5-6 loss: 0.543170  [  224/  265]
train() client id: f_00001-5-7 loss: 0.478778  [  256/  265]
train() client id: f_00001-6-0 loss: 0.671089  [   32/  265]
train() client id: f_00001-6-1 loss: 0.433188  [   64/  265]
train() client id: f_00001-6-2 loss: 0.518681  [   96/  265]
train() client id: f_00001-6-3 loss: 0.405801  [  128/  265]
train() client id: f_00001-6-4 loss: 0.432755  [  160/  265]
train() client id: f_00001-6-5 loss: 0.458994  [  192/  265]
train() client id: f_00001-6-6 loss: 0.540012  [  224/  265]
train() client id: f_00001-6-7 loss: 0.403639  [  256/  265]
train() client id: f_00001-7-0 loss: 0.375180  [   32/  265]
train() client id: f_00001-7-1 loss: 0.607344  [   64/  265]
train() client id: f_00001-7-2 loss: 0.572566  [   96/  265]
train() client id: f_00001-7-3 loss: 0.493756  [  128/  265]
train() client id: f_00001-7-4 loss: 0.379457  [  160/  265]
train() client id: f_00001-7-5 loss: 0.457793  [  192/  265]
train() client id: f_00001-7-6 loss: 0.488842  [  224/  265]
train() client id: f_00001-7-7 loss: 0.528148  [  256/  265]
train() client id: f_00001-8-0 loss: 0.460639  [   32/  265]
train() client id: f_00001-8-1 loss: 0.568951  [   64/  265]
train() client id: f_00001-8-2 loss: 0.403627  [   96/  265]
train() client id: f_00001-8-3 loss: 0.413449  [  128/  265]
train() client id: f_00001-8-4 loss: 0.403240  [  160/  265]
train() client id: f_00001-8-5 loss: 0.497491  [  192/  265]
train() client id: f_00001-8-6 loss: 0.658552  [  224/  265]
train() client id: f_00001-8-7 loss: 0.497348  [  256/  265]
train() client id: f_00001-9-0 loss: 0.441062  [   32/  265]
train() client id: f_00001-9-1 loss: 0.666525  [   64/  265]
train() client id: f_00001-9-2 loss: 0.403353  [   96/  265]
train() client id: f_00001-9-3 loss: 0.525213  [  128/  265]
train() client id: f_00001-9-4 loss: 0.503832  [  160/  265]
train() client id: f_00001-9-5 loss: 0.543673  [  192/  265]
train() client id: f_00001-9-6 loss: 0.424713  [  224/  265]
train() client id: f_00001-9-7 loss: 0.395348  [  256/  265]
train() client id: f_00001-10-0 loss: 0.465707  [   32/  265]
train() client id: f_00001-10-1 loss: 0.466777  [   64/  265]
train() client id: f_00001-10-2 loss: 0.397864  [   96/  265]
train() client id: f_00001-10-3 loss: 0.471823  [  128/  265]
train() client id: f_00001-10-4 loss: 0.606887  [  160/  265]
train() client id: f_00001-10-5 loss: 0.403922  [  192/  265]
train() client id: f_00001-10-6 loss: 0.444743  [  224/  265]
train() client id: f_00001-10-7 loss: 0.595284  [  256/  265]
train() client id: f_00001-11-0 loss: 0.417162  [   32/  265]
train() client id: f_00001-11-1 loss: 0.392604  [   64/  265]
train() client id: f_00001-11-2 loss: 0.525205  [   96/  265]
train() client id: f_00001-11-3 loss: 0.600088  [  128/  265]
train() client id: f_00001-11-4 loss: 0.610256  [  160/  265]
train() client id: f_00001-11-5 loss: 0.400534  [  192/  265]
train() client id: f_00001-11-6 loss: 0.519807  [  224/  265]
train() client id: f_00001-11-7 loss: 0.440185  [  256/  265]
train() client id: f_00001-12-0 loss: 0.555047  [   32/  265]
train() client id: f_00001-12-1 loss: 0.468688  [   64/  265]
train() client id: f_00001-12-2 loss: 0.553454  [   96/  265]
train() client id: f_00001-12-3 loss: 0.396881  [  128/  265]
train() client id: f_00001-12-4 loss: 0.612117  [  160/  265]
train() client id: f_00001-12-5 loss: 0.497520  [  192/  265]
train() client id: f_00001-12-6 loss: 0.411469  [  224/  265]
train() client id: f_00001-12-7 loss: 0.424975  [  256/  265]
train() client id: f_00002-0-0 loss: 1.046783  [   32/  124]
train() client id: f_00002-0-1 loss: 1.155596  [   64/  124]
train() client id: f_00002-0-2 loss: 1.191935  [   96/  124]
train() client id: f_00002-1-0 loss: 1.053956  [   32/  124]
train() client id: f_00002-1-1 loss: 1.033733  [   64/  124]
train() client id: f_00002-1-2 loss: 1.044467  [   96/  124]
train() client id: f_00002-2-0 loss: 1.129845  [   32/  124]
train() client id: f_00002-2-1 loss: 0.925048  [   64/  124]
train() client id: f_00002-2-2 loss: 1.100971  [   96/  124]
train() client id: f_00002-3-0 loss: 1.108820  [   32/  124]
train() client id: f_00002-3-1 loss: 0.948546  [   64/  124]
train() client id: f_00002-3-2 loss: 0.983798  [   96/  124]
train() client id: f_00002-4-0 loss: 0.933855  [   32/  124]
train() client id: f_00002-4-1 loss: 1.020197  [   64/  124]
train() client id: f_00002-4-2 loss: 0.962397  [   96/  124]
train() client id: f_00002-5-0 loss: 0.941141  [   32/  124]
train() client id: f_00002-5-1 loss: 1.031725  [   64/  124]
train() client id: f_00002-5-2 loss: 1.014618  [   96/  124]
train() client id: f_00002-6-0 loss: 0.963003  [   32/  124]
train() client id: f_00002-6-1 loss: 1.023808  [   64/  124]
train() client id: f_00002-6-2 loss: 0.847856  [   96/  124]
train() client id: f_00002-7-0 loss: 0.913226  [   32/  124]
train() client id: f_00002-7-1 loss: 0.869645  [   64/  124]
train() client id: f_00002-7-2 loss: 1.027962  [   96/  124]
train() client id: f_00002-8-0 loss: 1.000808  [   32/  124]
train() client id: f_00002-8-1 loss: 0.931509  [   64/  124]
train() client id: f_00002-8-2 loss: 0.815052  [   96/  124]
train() client id: f_00002-9-0 loss: 0.730784  [   32/  124]
train() client id: f_00002-9-1 loss: 1.108203  [   64/  124]
train() client id: f_00002-9-2 loss: 0.872836  [   96/  124]
train() client id: f_00002-10-0 loss: 0.856035  [   32/  124]
train() client id: f_00002-10-1 loss: 0.897597  [   64/  124]
train() client id: f_00002-10-2 loss: 0.765374  [   96/  124]
train() client id: f_00002-11-0 loss: 0.771568  [   32/  124]
train() client id: f_00002-11-1 loss: 0.854998  [   64/  124]
train() client id: f_00002-11-2 loss: 1.028209  [   96/  124]
train() client id: f_00002-12-0 loss: 0.809759  [   32/  124]
train() client id: f_00002-12-1 loss: 0.824287  [   64/  124]
train() client id: f_00002-12-2 loss: 0.872644  [   96/  124]
train() client id: f_00003-0-0 loss: 0.450156  [   32/   43]
train() client id: f_00003-1-0 loss: 0.529219  [   32/   43]
train() client id: f_00003-2-0 loss: 0.654229  [   32/   43]
train() client id: f_00003-3-0 loss: 0.617310  [   32/   43]
train() client id: f_00003-4-0 loss: 0.511668  [   32/   43]
train() client id: f_00003-5-0 loss: 0.541030  [   32/   43]
train() client id: f_00003-6-0 loss: 0.580180  [   32/   43]
train() client id: f_00003-7-0 loss: 0.538782  [   32/   43]
train() client id: f_00003-8-0 loss: 0.393183  [   32/   43]
train() client id: f_00003-9-0 loss: 0.397894  [   32/   43]
train() client id: f_00003-10-0 loss: 0.560005  [   32/   43]
train() client id: f_00003-11-0 loss: 0.568247  [   32/   43]
train() client id: f_00003-12-0 loss: 0.527815  [   32/   43]
train() client id: f_00004-0-0 loss: 0.808798  [   32/  306]
train() client id: f_00004-0-1 loss: 0.700759  [   64/  306]
train() client id: f_00004-0-2 loss: 0.775886  [   96/  306]
train() client id: f_00004-0-3 loss: 0.744319  [  128/  306]
train() client id: f_00004-0-4 loss: 0.869728  [  160/  306]
train() client id: f_00004-0-5 loss: 0.646345  [  192/  306]
train() client id: f_00004-0-6 loss: 0.738540  [  224/  306]
train() client id: f_00004-0-7 loss: 0.814352  [  256/  306]
train() client id: f_00004-0-8 loss: 0.724189  [  288/  306]
train() client id: f_00004-1-0 loss: 0.768320  [   32/  306]
train() client id: f_00004-1-1 loss: 0.862638  [   64/  306]
train() client id: f_00004-1-2 loss: 0.600076  [   96/  306]
train() client id: f_00004-1-3 loss: 0.824832  [  128/  306]
train() client id: f_00004-1-4 loss: 0.660042  [  160/  306]
train() client id: f_00004-1-5 loss: 0.758923  [  192/  306]
train() client id: f_00004-1-6 loss: 0.726990  [  224/  306]
train() client id: f_00004-1-7 loss: 0.730962  [  256/  306]
train() client id: f_00004-1-8 loss: 0.886731  [  288/  306]
train() client id: f_00004-2-0 loss: 0.713305  [   32/  306]
train() client id: f_00004-2-1 loss: 0.688006  [   64/  306]
train() client id: f_00004-2-2 loss: 0.730554  [   96/  306]
train() client id: f_00004-2-3 loss: 0.752226  [  128/  306]
train() client id: f_00004-2-4 loss: 0.774753  [  160/  306]
train() client id: f_00004-2-5 loss: 0.775404  [  192/  306]
train() client id: f_00004-2-6 loss: 0.732961  [  224/  306]
train() client id: f_00004-2-7 loss: 0.854761  [  256/  306]
train() client id: f_00004-2-8 loss: 0.715916  [  288/  306]
train() client id: f_00004-3-0 loss: 0.821278  [   32/  306]
train() client id: f_00004-3-1 loss: 0.678181  [   64/  306]
train() client id: f_00004-3-2 loss: 0.766259  [   96/  306]
train() client id: f_00004-3-3 loss: 0.811611  [  128/  306]
train() client id: f_00004-3-4 loss: 0.746438  [  160/  306]
train() client id: f_00004-3-5 loss: 0.790468  [  192/  306]
train() client id: f_00004-3-6 loss: 0.658445  [  224/  306]
train() client id: f_00004-3-7 loss: 0.839621  [  256/  306]
train() client id: f_00004-3-8 loss: 0.690225  [  288/  306]
train() client id: f_00004-4-0 loss: 0.819043  [   32/  306]
train() client id: f_00004-4-1 loss: 0.730494  [   64/  306]
train() client id: f_00004-4-2 loss: 0.750255  [   96/  306]
train() client id: f_00004-4-3 loss: 0.787444  [  128/  306]
train() client id: f_00004-4-4 loss: 0.694664  [  160/  306]
train() client id: f_00004-4-5 loss: 0.747136  [  192/  306]
train() client id: f_00004-4-6 loss: 0.767653  [  224/  306]
train() client id: f_00004-4-7 loss: 0.753021  [  256/  306]
train() client id: f_00004-4-8 loss: 0.737201  [  288/  306]
train() client id: f_00004-5-0 loss: 0.714250  [   32/  306]
train() client id: f_00004-5-1 loss: 0.700289  [   64/  306]
train() client id: f_00004-5-2 loss: 0.640919  [   96/  306]
train() client id: f_00004-5-3 loss: 0.704878  [  128/  306]
train() client id: f_00004-5-4 loss: 0.759569  [  160/  306]
train() client id: f_00004-5-5 loss: 0.808795  [  192/  306]
train() client id: f_00004-5-6 loss: 0.893228  [  224/  306]
train() client id: f_00004-5-7 loss: 0.776689  [  256/  306]
train() client id: f_00004-5-8 loss: 0.779822  [  288/  306]
train() client id: f_00004-6-0 loss: 0.755362  [   32/  306]
train() client id: f_00004-6-1 loss: 0.672117  [   64/  306]
train() client id: f_00004-6-2 loss: 0.776553  [   96/  306]
train() client id: f_00004-6-3 loss: 0.713599  [  128/  306]
train() client id: f_00004-6-4 loss: 0.679936  [  160/  306]
train() client id: f_00004-6-5 loss: 0.903147  [  192/  306]
train() client id: f_00004-6-6 loss: 0.754856  [  224/  306]
train() client id: f_00004-6-7 loss: 0.656099  [  256/  306]
train() client id: f_00004-6-8 loss: 0.879019  [  288/  306]
train() client id: f_00004-7-0 loss: 0.686286  [   32/  306]
train() client id: f_00004-7-1 loss: 0.763982  [   64/  306]
train() client id: f_00004-7-2 loss: 0.874772  [   96/  306]
train() client id: f_00004-7-3 loss: 0.785970  [  128/  306]
train() client id: f_00004-7-4 loss: 0.687167  [  160/  306]
train() client id: f_00004-7-5 loss: 0.773943  [  192/  306]
train() client id: f_00004-7-6 loss: 0.745570  [  224/  306]
train() client id: f_00004-7-7 loss: 0.757914  [  256/  306]
train() client id: f_00004-7-8 loss: 0.857962  [  288/  306]
train() client id: f_00004-8-0 loss: 0.730816  [   32/  306]
train() client id: f_00004-8-1 loss: 0.668541  [   64/  306]
train() client id: f_00004-8-2 loss: 0.746615  [   96/  306]
train() client id: f_00004-8-3 loss: 0.815701  [  128/  306]
train() client id: f_00004-8-4 loss: 0.802769  [  160/  306]
train() client id: f_00004-8-5 loss: 0.803192  [  192/  306]
train() client id: f_00004-8-6 loss: 0.789741  [  224/  306]
train() client id: f_00004-8-7 loss: 0.829426  [  256/  306]
train() client id: f_00004-8-8 loss: 0.721746  [  288/  306]
train() client id: f_00004-9-0 loss: 0.714479  [   32/  306]
train() client id: f_00004-9-1 loss: 0.868970  [   64/  306]
train() client id: f_00004-9-2 loss: 0.756377  [   96/  306]
train() client id: f_00004-9-3 loss: 0.795264  [  128/  306]
train() client id: f_00004-9-4 loss: 0.754739  [  160/  306]
train() client id: f_00004-9-5 loss: 0.717149  [  192/  306]
train() client id: f_00004-9-6 loss: 0.714620  [  224/  306]
train() client id: f_00004-9-7 loss: 0.759427  [  256/  306]
train() client id: f_00004-9-8 loss: 0.784949  [  288/  306]
train() client id: f_00004-10-0 loss: 0.744766  [   32/  306]
train() client id: f_00004-10-1 loss: 0.773830  [   64/  306]
train() client id: f_00004-10-2 loss: 0.773727  [   96/  306]
train() client id: f_00004-10-3 loss: 0.708470  [  128/  306]
train() client id: f_00004-10-4 loss: 0.798224  [  160/  306]
train() client id: f_00004-10-5 loss: 0.922590  [  192/  306]
train() client id: f_00004-10-6 loss: 0.736392  [  224/  306]
train() client id: f_00004-10-7 loss: 0.753772  [  256/  306]
train() client id: f_00004-10-8 loss: 0.643249  [  288/  306]
train() client id: f_00004-11-0 loss: 0.738073  [   32/  306]
train() client id: f_00004-11-1 loss: 0.680593  [   64/  306]
train() client id: f_00004-11-2 loss: 0.677897  [   96/  306]
train() client id: f_00004-11-3 loss: 0.721783  [  128/  306]
train() client id: f_00004-11-4 loss: 0.782696  [  160/  306]
train() client id: f_00004-11-5 loss: 0.726861  [  192/  306]
train() client id: f_00004-11-6 loss: 0.880339  [  224/  306]
train() client id: f_00004-11-7 loss: 0.841717  [  256/  306]
train() client id: f_00004-11-8 loss: 0.880116  [  288/  306]
train() client id: f_00004-12-0 loss: 0.727731  [   32/  306]
train() client id: f_00004-12-1 loss: 0.766784  [   64/  306]
train() client id: f_00004-12-2 loss: 0.775198  [   96/  306]
train() client id: f_00004-12-3 loss: 0.880406  [  128/  306]
train() client id: f_00004-12-4 loss: 0.777165  [  160/  306]
train() client id: f_00004-12-5 loss: 0.737910  [  192/  306]
train() client id: f_00004-12-6 loss: 0.760874  [  224/  306]
train() client id: f_00004-12-7 loss: 0.677318  [  256/  306]
train() client id: f_00004-12-8 loss: 0.797574  [  288/  306]
train() client id: f_00005-0-0 loss: 0.918560  [   32/  146]
train() client id: f_00005-0-1 loss: 0.746986  [   64/  146]
train() client id: f_00005-0-2 loss: 0.577089  [   96/  146]
train() client id: f_00005-0-3 loss: 0.837748  [  128/  146]
train() client id: f_00005-1-0 loss: 0.822755  [   32/  146]
train() client id: f_00005-1-1 loss: 0.861423  [   64/  146]
train() client id: f_00005-1-2 loss: 0.664839  [   96/  146]
train() client id: f_00005-1-3 loss: 0.734222  [  128/  146]
train() client id: f_00005-2-0 loss: 0.761180  [   32/  146]
train() client id: f_00005-2-1 loss: 0.794445  [   64/  146]
train() client id: f_00005-2-2 loss: 0.766512  [   96/  146]
train() client id: f_00005-2-3 loss: 0.716307  [  128/  146]
train() client id: f_00005-3-0 loss: 0.736145  [   32/  146]
train() client id: f_00005-3-1 loss: 0.868213  [   64/  146]
train() client id: f_00005-3-2 loss: 0.749111  [   96/  146]
train() client id: f_00005-3-3 loss: 0.808907  [  128/  146]
train() client id: f_00005-4-0 loss: 0.813321  [   32/  146]
train() client id: f_00005-4-1 loss: 0.404229  [   64/  146]
train() client id: f_00005-4-2 loss: 0.850760  [   96/  146]
train() client id: f_00005-4-3 loss: 1.033899  [  128/  146]
train() client id: f_00005-5-0 loss: 0.755043  [   32/  146]
train() client id: f_00005-5-1 loss: 0.815924  [   64/  146]
train() client id: f_00005-5-2 loss: 0.746382  [   96/  146]
train() client id: f_00005-5-3 loss: 0.680841  [  128/  146]
train() client id: f_00005-6-0 loss: 0.738371  [   32/  146]
train() client id: f_00005-6-1 loss: 0.852170  [   64/  146]
train() client id: f_00005-6-2 loss: 0.816845  [   96/  146]
train() client id: f_00005-6-3 loss: 0.533255  [  128/  146]
train() client id: f_00005-7-0 loss: 0.576546  [   32/  146]
train() client id: f_00005-7-1 loss: 0.683526  [   64/  146]
train() client id: f_00005-7-2 loss: 0.864029  [   96/  146]
train() client id: f_00005-7-3 loss: 0.726954  [  128/  146]
train() client id: f_00005-8-0 loss: 0.484907  [   32/  146]
train() client id: f_00005-8-1 loss: 0.716502  [   64/  146]
train() client id: f_00005-8-2 loss: 0.600678  [   96/  146]
train() client id: f_00005-8-3 loss: 0.956805  [  128/  146]
train() client id: f_00005-9-0 loss: 0.578602  [   32/  146]
train() client id: f_00005-9-1 loss: 0.919268  [   64/  146]
train() client id: f_00005-9-2 loss: 1.002039  [   96/  146]
train() client id: f_00005-9-3 loss: 0.606385  [  128/  146]
train() client id: f_00005-10-0 loss: 0.678581  [   32/  146]
train() client id: f_00005-10-1 loss: 0.995624  [   64/  146]
train() client id: f_00005-10-2 loss: 0.636718  [   96/  146]
train() client id: f_00005-10-3 loss: 0.687647  [  128/  146]
train() client id: f_00005-11-0 loss: 1.071104  [   32/  146]
train() client id: f_00005-11-1 loss: 0.490981  [   64/  146]
train() client id: f_00005-11-2 loss: 0.715118  [   96/  146]
train() client id: f_00005-11-3 loss: 0.552844  [  128/  146]
train() client id: f_00005-12-0 loss: 0.834079  [   32/  146]
train() client id: f_00005-12-1 loss: 0.846592  [   64/  146]
train() client id: f_00005-12-2 loss: 0.591931  [   96/  146]
train() client id: f_00005-12-3 loss: 0.688481  [  128/  146]
train() client id: f_00006-0-0 loss: 0.447566  [   32/   54]
train() client id: f_00006-1-0 loss: 0.511189  [   32/   54]
train() client id: f_00006-2-0 loss: 0.510079  [   32/   54]
train() client id: f_00006-3-0 loss: 0.453741  [   32/   54]
train() client id: f_00006-4-0 loss: 0.455332  [   32/   54]
train() client id: f_00006-5-0 loss: 0.426283  [   32/   54]
train() client id: f_00006-6-0 loss: 0.482448  [   32/   54]
train() client id: f_00006-7-0 loss: 0.534748  [   32/   54]
train() client id: f_00006-8-0 loss: 0.471609  [   32/   54]
train() client id: f_00006-9-0 loss: 0.467500  [   32/   54]
train() client id: f_00006-10-0 loss: 0.464069  [   32/   54]
train() client id: f_00006-11-0 loss: 0.503929  [   32/   54]
train() client id: f_00006-12-0 loss: 0.516507  [   32/   54]
train() client id: f_00007-0-0 loss: 0.542167  [   32/  179]
train() client id: f_00007-0-1 loss: 0.771934  [   64/  179]
train() client id: f_00007-0-2 loss: 0.679007  [   96/  179]
train() client id: f_00007-0-3 loss: 0.541039  [  128/  179]
train() client id: f_00007-0-4 loss: 0.782209  [  160/  179]
train() client id: f_00007-1-0 loss: 0.696174  [   32/  179]
train() client id: f_00007-1-1 loss: 0.642147  [   64/  179]
train() client id: f_00007-1-2 loss: 0.614190  [   96/  179]
train() client id: f_00007-1-3 loss: 0.474921  [  128/  179]
train() client id: f_00007-1-4 loss: 0.801296  [  160/  179]
train() client id: f_00007-2-0 loss: 0.672789  [   32/  179]
train() client id: f_00007-2-1 loss: 0.655558  [   64/  179]
train() client id: f_00007-2-2 loss: 0.533483  [   96/  179]
train() client id: f_00007-2-3 loss: 0.484594  [  128/  179]
train() client id: f_00007-2-4 loss: 0.762048  [  160/  179]
train() client id: f_00007-3-0 loss: 0.768558  [   32/  179]
train() client id: f_00007-3-1 loss: 0.480428  [   64/  179]
train() client id: f_00007-3-2 loss: 0.600060  [   96/  179]
train() client id: f_00007-3-3 loss: 0.783584  [  128/  179]
train() client id: f_00007-3-4 loss: 0.429960  [  160/  179]
train() client id: f_00007-4-0 loss: 0.559547  [   32/  179]
train() client id: f_00007-4-1 loss: 0.540597  [   64/  179]
train() client id: f_00007-4-2 loss: 0.651244  [   96/  179]
train() client id: f_00007-4-3 loss: 0.456016  [  128/  179]
train() client id: f_00007-4-4 loss: 0.724616  [  160/  179]
train() client id: f_00007-5-0 loss: 0.570243  [   32/  179]
train() client id: f_00007-5-1 loss: 0.520226  [   64/  179]
train() client id: f_00007-5-2 loss: 0.532073  [   96/  179]
train() client id: f_00007-5-3 loss: 0.614234  [  128/  179]
train() client id: f_00007-5-4 loss: 0.589368  [  160/  179]
train() client id: f_00007-6-0 loss: 0.668157  [   32/  179]
train() client id: f_00007-6-1 loss: 0.697852  [   64/  179]
train() client id: f_00007-6-2 loss: 0.530140  [   96/  179]
train() client id: f_00007-6-3 loss: 0.621206  [  128/  179]
train() client id: f_00007-6-4 loss: 0.483318  [  160/  179]
train() client id: f_00007-7-0 loss: 0.621651  [   32/  179]
train() client id: f_00007-7-1 loss: 0.505453  [   64/  179]
train() client id: f_00007-7-2 loss: 0.657721  [   96/  179]
train() client id: f_00007-7-3 loss: 0.568105  [  128/  179]
train() client id: f_00007-7-4 loss: 0.403881  [  160/  179]
train() client id: f_00007-8-0 loss: 0.624990  [   32/  179]
train() client id: f_00007-8-1 loss: 0.556477  [   64/  179]
train() client id: f_00007-8-2 loss: 0.535093  [   96/  179]
train() client id: f_00007-8-3 loss: 0.510598  [  128/  179]
train() client id: f_00007-8-4 loss: 0.762057  [  160/  179]
train() client id: f_00007-9-0 loss: 0.609841  [   32/  179]
train() client id: f_00007-9-1 loss: 0.587556  [   64/  179]
train() client id: f_00007-9-2 loss: 0.654067  [   96/  179]
train() client id: f_00007-9-3 loss: 0.733038  [  128/  179]
train() client id: f_00007-9-4 loss: 0.409633  [  160/  179]
train() client id: f_00007-10-0 loss: 0.571853  [   32/  179]
train() client id: f_00007-10-1 loss: 0.632478  [   64/  179]
train() client id: f_00007-10-2 loss: 0.474309  [   96/  179]
train() client id: f_00007-10-3 loss: 0.747531  [  128/  179]
train() client id: f_00007-10-4 loss: 0.568982  [  160/  179]
train() client id: f_00007-11-0 loss: 0.486454  [   32/  179]
train() client id: f_00007-11-1 loss: 0.722531  [   64/  179]
train() client id: f_00007-11-2 loss: 0.504660  [   96/  179]
train() client id: f_00007-11-3 loss: 0.454774  [  128/  179]
train() client id: f_00007-11-4 loss: 0.442997  [  160/  179]
train() client id: f_00007-12-0 loss: 0.418399  [   32/  179]
train() client id: f_00007-12-1 loss: 0.520666  [   64/  179]
train() client id: f_00007-12-2 loss: 0.517075  [   96/  179]
train() client id: f_00007-12-3 loss: 0.890783  [  128/  179]
train() client id: f_00007-12-4 loss: 0.503174  [  160/  179]
train() client id: f_00008-0-0 loss: 0.679218  [   32/  130]
train() client id: f_00008-0-1 loss: 0.644284  [   64/  130]
train() client id: f_00008-0-2 loss: 0.632161  [   96/  130]
train() client id: f_00008-0-3 loss: 0.504034  [  128/  130]
train() client id: f_00008-1-0 loss: 0.568831  [   32/  130]
train() client id: f_00008-1-1 loss: 0.587309  [   64/  130]
train() client id: f_00008-1-2 loss: 0.580477  [   96/  130]
train() client id: f_00008-1-3 loss: 0.690204  [  128/  130]
train() client id: f_00008-2-0 loss: 0.567779  [   32/  130]
train() client id: f_00008-2-1 loss: 0.691412  [   64/  130]
train() client id: f_00008-2-2 loss: 0.614447  [   96/  130]
train() client id: f_00008-2-3 loss: 0.558065  [  128/  130]
train() client id: f_00008-3-0 loss: 0.574365  [   32/  130]
train() client id: f_00008-3-1 loss: 0.574508  [   64/  130]
train() client id: f_00008-3-2 loss: 0.587676  [   96/  130]
train() client id: f_00008-3-3 loss: 0.683817  [  128/  130]
train() client id: f_00008-4-0 loss: 0.671575  [   32/  130]
train() client id: f_00008-4-1 loss: 0.592236  [   64/  130]
train() client id: f_00008-4-2 loss: 0.586012  [   96/  130]
train() client id: f_00008-4-3 loss: 0.570155  [  128/  130]
train() client id: f_00008-5-0 loss: 0.659761  [   32/  130]
train() client id: f_00008-5-1 loss: 0.605484  [   64/  130]
train() client id: f_00008-5-2 loss: 0.545532  [   96/  130]
train() client id: f_00008-5-3 loss: 0.656646  [  128/  130]
train() client id: f_00008-6-0 loss: 0.614525  [   32/  130]
train() client id: f_00008-6-1 loss: 0.534818  [   64/  130]
train() client id: f_00008-6-2 loss: 0.649754  [   96/  130]
train() client id: f_00008-6-3 loss: 0.663962  [  128/  130]
train() client id: f_00008-7-0 loss: 0.637577  [   32/  130]
train() client id: f_00008-7-1 loss: 0.553366  [   64/  130]
train() client id: f_00008-7-2 loss: 0.678959  [   96/  130]
train() client id: f_00008-7-3 loss: 0.587187  [  128/  130]
train() client id: f_00008-8-0 loss: 0.599852  [   32/  130]
train() client id: f_00008-8-1 loss: 0.576202  [   64/  130]
train() client id: f_00008-8-2 loss: 0.663164  [   96/  130]
train() client id: f_00008-8-3 loss: 0.597028  [  128/  130]
train() client id: f_00008-9-0 loss: 0.679319  [   32/  130]
train() client id: f_00008-9-1 loss: 0.627403  [   64/  130]
train() client id: f_00008-9-2 loss: 0.583600  [   96/  130]
train() client id: f_00008-9-3 loss: 0.578745  [  128/  130]
train() client id: f_00008-10-0 loss: 0.557282  [   32/  130]
train() client id: f_00008-10-1 loss: 0.547294  [   64/  130]
train() client id: f_00008-10-2 loss: 0.631813  [   96/  130]
train() client id: f_00008-10-3 loss: 0.733381  [  128/  130]
train() client id: f_00008-11-0 loss: 0.811644  [   32/  130]
train() client id: f_00008-11-1 loss: 0.575427  [   64/  130]
train() client id: f_00008-11-2 loss: 0.431371  [   96/  130]
train() client id: f_00008-11-3 loss: 0.662128  [  128/  130]
train() client id: f_00008-12-0 loss: 0.724479  [   32/  130]
train() client id: f_00008-12-1 loss: 0.556850  [   64/  130]
train() client id: f_00008-12-2 loss: 0.605290  [   96/  130]
train() client id: f_00008-12-3 loss: 0.580678  [  128/  130]
train() client id: f_00009-0-0 loss: 1.252635  [   32/  118]
train() client id: f_00009-0-1 loss: 1.113510  [   64/  118]
train() client id: f_00009-0-2 loss: 1.090509  [   96/  118]
train() client id: f_00009-1-0 loss: 1.043125  [   32/  118]
train() client id: f_00009-1-1 loss: 1.185324  [   64/  118]
train() client id: f_00009-1-2 loss: 1.194078  [   96/  118]
train() client id: f_00009-2-0 loss: 1.020432  [   32/  118]
train() client id: f_00009-2-1 loss: 1.220776  [   64/  118]
train() client id: f_00009-2-2 loss: 1.080435  [   96/  118]
train() client id: f_00009-3-0 loss: 0.891742  [   32/  118]
train() client id: f_00009-3-1 loss: 1.107025  [   64/  118]
train() client id: f_00009-3-2 loss: 1.068055  [   96/  118]
train() client id: f_00009-4-0 loss: 0.995853  [   32/  118]
train() client id: f_00009-4-1 loss: 1.104934  [   64/  118]
train() client id: f_00009-4-2 loss: 1.055758  [   96/  118]
train() client id: f_00009-5-0 loss: 1.046198  [   32/  118]
train() client id: f_00009-5-1 loss: 0.914191  [   64/  118]
train() client id: f_00009-5-2 loss: 0.898957  [   96/  118]
train() client id: f_00009-6-0 loss: 0.960858  [   32/  118]
train() client id: f_00009-6-1 loss: 0.987519  [   64/  118]
train() client id: f_00009-6-2 loss: 0.974504  [   96/  118]
train() client id: f_00009-7-0 loss: 1.033998  [   32/  118]
train() client id: f_00009-7-1 loss: 0.973168  [   64/  118]
train() client id: f_00009-7-2 loss: 0.902106  [   96/  118]
train() client id: f_00009-8-0 loss: 0.844197  [   32/  118]
train() client id: f_00009-8-1 loss: 1.015964  [   64/  118]
train() client id: f_00009-8-2 loss: 0.989567  [   96/  118]
train() client id: f_00009-9-0 loss: 0.968948  [   32/  118]
train() client id: f_00009-9-1 loss: 0.841403  [   64/  118]
train() client id: f_00009-9-2 loss: 0.911667  [   96/  118]
train() client id: f_00009-10-0 loss: 0.829447  [   32/  118]
train() client id: f_00009-10-1 loss: 0.828945  [   64/  118]
train() client id: f_00009-10-2 loss: 1.055211  [   96/  118]
train() client id: f_00009-11-0 loss: 1.012176  [   32/  118]
train() client id: f_00009-11-1 loss: 0.873806  [   64/  118]
train() client id: f_00009-11-2 loss: 0.876415  [   96/  118]
train() client id: f_00009-12-0 loss: 0.913609  [   32/  118]
train() client id: f_00009-12-1 loss: 0.797777  [   64/  118]
train() client id: f_00009-12-2 loss: 0.889669  [   96/  118]
At round 41 accuracy: 0.6472148541114059
At round 41 training accuracy: 0.5928906773977196
At round 41 training loss: 0.8317539913503615
update_location
xs = [  -3.9056584     4.20031788  225.00902392   18.81129433    0.97929623
    3.95640986 -187.44319194 -166.32485185  209.66397685 -152.06087855]
ys = [ 217.5879595   200.55583871    1.32061395 -187.45517586  179.35018685
  162.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [239.49900686 224.14345208 246.23323266 213.29160263 205.34714155
 191.11279617 212.46609317 194.07378133 232.95418683 182.039893  ]
dists_bs = [176.31508166 181.0134106  436.08685872 411.01894399 176.03305606
 179.37082135 178.06029338 174.39355681 415.63312302 172.53027713]
uav_gains = [9.05340846e-12 1.18039549e-11 7.98136893e-12 1.40041215e-11
 1.57718869e-11 1.93846067e-11 1.41810871e-11 1.85763382e-11
 1.01750306e-11 2.20968418e-11]
bs_gains = [5.67131265e-11 5.26870620e-11 4.49251656e-12 5.30250316e-12
 5.69679045e-11 5.40491731e-11 5.51704147e-11 5.84802023e-11
 5.13932005e-12 6.02658366e-11]
Round 42
-------------------------------
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.44885328 11.23645857  5.37086954  1.94257213 12.9580199   6.23472006
  2.4044106   7.64858783  5.64816701  5.05583458]
obj_prev = 63.948493486950056
eta_min = 1.1938832184881957e-17	eta_max = 0.9337601214025776
af = 13.47090363962996	bf = 1.273204732538192	zeta = 14.817994003592956	eta = 0.9090909090909091
af = 13.47090363962996	bf = 1.273204732538192	zeta = 28.128308786138636	eta = 0.47890912112953954
af = 13.47090363962996	bf = 1.273204732538192	zeta = 21.476258560043068	eta = 0.6272462962749386
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.271427562519737	eta = 0.6645266396796146
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.2057708129807	eta = 0.666685956418743
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.20555837007598	eta = 0.6666929660097932
eta = 0.6666929660097932
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [0.03345475 0.07036119 0.03292373 0.0114171  0.08124726 0.03876503
 0.01433775 0.04752699 0.03451681 0.03133064]
ene_total = [1.84044551 3.20713505 1.83649691 0.87474524 3.65310778 1.89784042
 0.99426624 2.34061661 1.97709129 1.58381332]
ti_comp = [0.56395803 0.60282675 0.55960175 0.5768971  0.60393932 0.60319406
 0.57722705 0.58383781 0.54156463 0.6047199 ]
ti_coms = [0.11010873 0.07124001 0.11446502 0.09716967 0.07012744 0.0708727
 0.09683971 0.09022895 0.13250214 0.06934686]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.35800290e-06 5.99093333e-05 7.12276845e-06 2.79479884e-07
 9.19008568e-05 1.00065955e-05 5.52878649e-07 1.96841446e-05
 8.76337451e-06 5.25629088e-06]
ene_total = [0.45604785 0.2973442  0.47406892 0.4021997  0.29406337 0.29375838
 0.40084532 0.37427503 0.548793   0.28724627]
optimize_network_iter = 0 obj = 3.828642052039432
eta = 0.6666929660097932
freqs = [29660676.66717937 58359378.23819214 29417104.30254062  9895264.30741952
 67264422.3037599  32133134.64789411 12419507.6458075  40702220.97084436
 31867673.54934066 25905086.18443851]
eta_min = 0.6666929660097993	eta_max = 0.6666929660097998
af = 0.007174173529547853	bf = 1.273204732538192	zeta = 0.007891590882502639	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [1.72976861e-06 1.40838874e-05 1.67446812e-06 6.57020031e-08
 2.16046690e-05 2.35241749e-06 1.29974416e-07 4.62748057e-06
 2.06015278e-06 1.23568407e-06]
ene_total = [1.67118944 1.08322193 1.73728876 1.47458345 1.06747966 1.07586755
 1.46958603 1.36994863 2.01106501 1.0525431 ]
ti_comp = [0.56395803 0.60282675 0.55960175 0.5768971  0.60393932 0.60319406
 0.57722705 0.58383781 0.54156463 0.6047199 ]
ti_coms = [0.11010873 0.07124001 0.11446502 0.09716967 0.07012744 0.0708727
 0.09683971 0.09022895 0.13250214 0.06934686]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.35800290e-06 5.99093333e-05 7.12276845e-06 2.79479884e-07
 9.19008568e-05 1.00065955e-05 5.52878649e-07 1.96841446e-05
 8.76337451e-06 5.25629088e-06]
ene_total = [0.45604785 0.2973442  0.47406892 0.4021997  0.29406337 0.29375838
 0.40084532 0.37427503 0.548793   0.28724627]
optimize_network_iter = 1 obj = 3.8286420520395077
eta = 0.6666929660097998
freqs = [29660676.66717935 58359378.23819202 29417104.30254061  9895264.30741951
 67264422.30375975 32133134.64789404 12419507.64580748 40702220.97084429
 31867673.54934067 25905086.18443845]
Done!
At round 42 eta: 0.6666929660097998
At round 42 local rounds: 13.275695913565364
At round 42 global rounds: 41.390297463535276
At round 42 a_n: 13.453131436573733
gradient difference: 0.4042520523071289
train() client id: f_00000-0-0 loss: 1.284284  [   32/  126]
train() client id: f_00000-0-1 loss: 1.338440  [   64/  126]
train() client id: f_00000-0-2 loss: 1.203938  [   96/  126]
train() client id: f_00000-1-0 loss: 1.304143  [   32/  126]
train() client id: f_00000-1-1 loss: 1.127179  [   64/  126]
train() client id: f_00000-1-2 loss: 1.175886  [   96/  126]
train() client id: f_00000-2-0 loss: 0.961867  [   32/  126]
train() client id: f_00000-2-1 loss: 1.255932  [   64/  126]
train() client id: f_00000-2-2 loss: 1.168774  [   96/  126]
train() client id: f_00000-3-0 loss: 1.134306  [   32/  126]
train() client id: f_00000-3-1 loss: 1.008446  [   64/  126]
train() client id: f_00000-3-2 loss: 0.834309  [   96/  126]
train() client id: f_00000-4-0 loss: 1.103706  [   32/  126]
train() client id: f_00000-4-1 loss: 0.883326  [   64/  126]
train() client id: f_00000-4-2 loss: 0.891769  [   96/  126]
train() client id: f_00000-5-0 loss: 0.912692  [   32/  126]
train() client id: f_00000-5-1 loss: 0.827612  [   64/  126]
train() client id: f_00000-5-2 loss: 0.957715  [   96/  126]
train() client id: f_00000-6-0 loss: 0.890009  [   32/  126]
train() client id: f_00000-6-1 loss: 0.800823  [   64/  126]
train() client id: f_00000-6-2 loss: 0.936265  [   96/  126]
train() client id: f_00000-7-0 loss: 0.906963  [   32/  126]
train() client id: f_00000-7-1 loss: 0.900178  [   64/  126]
train() client id: f_00000-7-2 loss: 0.872980  [   96/  126]
train() client id: f_00000-8-0 loss: 0.805052  [   32/  126]
train() client id: f_00000-8-1 loss: 0.848944  [   64/  126]
train() client id: f_00000-8-2 loss: 0.843880  [   96/  126]
train() client id: f_00000-9-0 loss: 0.820577  [   32/  126]
train() client id: f_00000-9-1 loss: 0.775345  [   64/  126]
train() client id: f_00000-9-2 loss: 0.780834  [   96/  126]
train() client id: f_00000-10-0 loss: 0.801279  [   32/  126]
train() client id: f_00000-10-1 loss: 0.845397  [   64/  126]
train() client id: f_00000-10-2 loss: 0.858956  [   96/  126]
train() client id: f_00000-11-0 loss: 0.690482  [   32/  126]
train() client id: f_00000-11-1 loss: 0.862182  [   64/  126]
train() client id: f_00000-11-2 loss: 0.862640  [   96/  126]
train() client id: f_00000-12-0 loss: 0.784435  [   32/  126]
train() client id: f_00000-12-1 loss: 0.717301  [   64/  126]
train() client id: f_00000-12-2 loss: 0.881847  [   96/  126]
train() client id: f_00001-0-0 loss: 0.528660  [   32/  265]
train() client id: f_00001-0-1 loss: 0.396907  [   64/  265]
train() client id: f_00001-0-2 loss: 0.451770  [   96/  265]
train() client id: f_00001-0-3 loss: 0.524995  [  128/  265]
train() client id: f_00001-0-4 loss: 0.470985  [  160/  265]
train() client id: f_00001-0-5 loss: 0.363331  [  192/  265]
train() client id: f_00001-0-6 loss: 0.617888  [  224/  265]
train() client id: f_00001-0-7 loss: 0.438836  [  256/  265]
train() client id: f_00001-1-0 loss: 0.429554  [   32/  265]
train() client id: f_00001-1-1 loss: 0.396542  [   64/  265]
train() client id: f_00001-1-2 loss: 0.421628  [   96/  265]
train() client id: f_00001-1-3 loss: 0.377905  [  128/  265]
train() client id: f_00001-1-4 loss: 0.443462  [  160/  265]
train() client id: f_00001-1-5 loss: 0.539353  [  192/  265]
train() client id: f_00001-1-6 loss: 0.562230  [  224/  265]
train() client id: f_00001-1-7 loss: 0.506131  [  256/  265]
train() client id: f_00001-2-0 loss: 0.508799  [   32/  265]
train() client id: f_00001-2-1 loss: 0.430888  [   64/  265]
train() client id: f_00001-2-2 loss: 0.518001  [   96/  265]
train() client id: f_00001-2-3 loss: 0.355188  [  128/  265]
train() client id: f_00001-2-4 loss: 0.405969  [  160/  265]
train() client id: f_00001-2-5 loss: 0.519041  [  192/  265]
train() client id: f_00001-2-6 loss: 0.356643  [  224/  265]
train() client id: f_00001-2-7 loss: 0.545973  [  256/  265]
train() client id: f_00001-3-0 loss: 0.386401  [   32/  265]
train() client id: f_00001-3-1 loss: 0.427816  [   64/  265]
train() client id: f_00001-3-2 loss: 0.479062  [   96/  265]
train() client id: f_00001-3-3 loss: 0.506829  [  128/  265]
train() client id: f_00001-3-4 loss: 0.446767  [  160/  265]
train() client id: f_00001-3-5 loss: 0.364884  [  192/  265]
train() client id: f_00001-3-6 loss: 0.500553  [  224/  265]
train() client id: f_00001-3-7 loss: 0.460072  [  256/  265]
train() client id: f_00001-4-0 loss: 0.476317  [   32/  265]
train() client id: f_00001-4-1 loss: 0.345585  [   64/  265]
train() client id: f_00001-4-2 loss: 0.429633  [   96/  265]
train() client id: f_00001-4-3 loss: 0.460090  [  128/  265]
train() client id: f_00001-4-4 loss: 0.462643  [  160/  265]
train() client id: f_00001-4-5 loss: 0.423689  [  192/  265]
train() client id: f_00001-4-6 loss: 0.497926  [  224/  265]
train() client id: f_00001-4-7 loss: 0.511916  [  256/  265]
train() client id: f_00001-5-0 loss: 0.479288  [   32/  265]
train() client id: f_00001-5-1 loss: 0.548507  [   64/  265]
train() client id: f_00001-5-2 loss: 0.370881  [   96/  265]
train() client id: f_00001-5-3 loss: 0.466684  [  128/  265]
train() client id: f_00001-5-4 loss: 0.477895  [  160/  265]
train() client id: f_00001-5-5 loss: 0.362801  [  192/  265]
train() client id: f_00001-5-6 loss: 0.470726  [  224/  265]
train() client id: f_00001-5-7 loss: 0.360126  [  256/  265]
train() client id: f_00001-6-0 loss: 0.460219  [   32/  265]
train() client id: f_00001-6-1 loss: 0.474355  [   64/  265]
train() client id: f_00001-6-2 loss: 0.350382  [   96/  265]
train() client id: f_00001-6-3 loss: 0.513962  [  128/  265]
train() client id: f_00001-6-4 loss: 0.415657  [  160/  265]
train() client id: f_00001-6-5 loss: 0.467769  [  192/  265]
train() client id: f_00001-6-6 loss: 0.491436  [  224/  265]
train() client id: f_00001-6-7 loss: 0.394103  [  256/  265]
train() client id: f_00001-7-0 loss: 0.396397  [   32/  265]
train() client id: f_00001-7-1 loss: 0.501963  [   64/  265]
train() client id: f_00001-7-2 loss: 0.381912  [   96/  265]
train() client id: f_00001-7-3 loss: 0.585307  [  128/  265]
train() client id: f_00001-7-4 loss: 0.392368  [  160/  265]
train() client id: f_00001-7-5 loss: 0.414390  [  192/  265]
train() client id: f_00001-7-6 loss: 0.485028  [  224/  265]
train() client id: f_00001-7-7 loss: 0.418059  [  256/  265]
train() client id: f_00001-8-0 loss: 0.569701  [   32/  265]
train() client id: f_00001-8-1 loss: 0.433237  [   64/  265]
train() client id: f_00001-8-2 loss: 0.498087  [   96/  265]
train() client id: f_00001-8-3 loss: 0.387498  [  128/  265]
train() client id: f_00001-8-4 loss: 0.467900  [  160/  265]
train() client id: f_00001-8-5 loss: 0.413879  [  192/  265]
train() client id: f_00001-8-6 loss: 0.333525  [  224/  265]
train() client id: f_00001-8-7 loss: 0.447636  [  256/  265]
train() client id: f_00001-9-0 loss: 0.563755  [   32/  265]
train() client id: f_00001-9-1 loss: 0.390665  [   64/  265]
train() client id: f_00001-9-2 loss: 0.492545  [   96/  265]
train() client id: f_00001-9-3 loss: 0.467706  [  128/  265]
train() client id: f_00001-9-4 loss: 0.342197  [  160/  265]
train() client id: f_00001-9-5 loss: 0.476226  [  192/  265]
train() client id: f_00001-9-6 loss: 0.337717  [  224/  265]
train() client id: f_00001-9-7 loss: 0.476684  [  256/  265]
train() client id: f_00001-10-0 loss: 0.530279  [   32/  265]
train() client id: f_00001-10-1 loss: 0.342398  [   64/  265]
train() client id: f_00001-10-2 loss: 0.439806  [   96/  265]
train() client id: f_00001-10-3 loss: 0.536741  [  128/  265]
train() client id: f_00001-10-4 loss: 0.536258  [  160/  265]
train() client id: f_00001-10-5 loss: 0.354169  [  192/  265]
train() client id: f_00001-10-6 loss: 0.392964  [  224/  265]
train() client id: f_00001-10-7 loss: 0.347255  [  256/  265]
train() client id: f_00001-11-0 loss: 0.484569  [   32/  265]
train() client id: f_00001-11-1 loss: 0.517722  [   64/  265]
train() client id: f_00001-11-2 loss: 0.461705  [   96/  265]
train() client id: f_00001-11-3 loss: 0.359555  [  128/  265]
train() client id: f_00001-11-4 loss: 0.373394  [  160/  265]
train() client id: f_00001-11-5 loss: 0.452223  [  192/  265]
train() client id: f_00001-11-6 loss: 0.369921  [  224/  265]
train() client id: f_00001-11-7 loss: 0.526970  [  256/  265]
train() client id: f_00001-12-0 loss: 0.354287  [   32/  265]
train() client id: f_00001-12-1 loss: 0.386585  [   64/  265]
train() client id: f_00001-12-2 loss: 0.681552  [   96/  265]
train() client id: f_00001-12-3 loss: 0.343427  [  128/  265]
train() client id: f_00001-12-4 loss: 0.443892  [  160/  265]
train() client id: f_00001-12-5 loss: 0.475602  [  192/  265]
train() client id: f_00001-12-6 loss: 0.528645  [  224/  265]
train() client id: f_00001-12-7 loss: 0.340704  [  256/  265]
train() client id: f_00002-0-0 loss: 1.094833  [   32/  124]
train() client id: f_00002-0-1 loss: 0.991623  [   64/  124]
train() client id: f_00002-0-2 loss: 1.085768  [   96/  124]
train() client id: f_00002-1-0 loss: 1.047997  [   32/  124]
train() client id: f_00002-1-1 loss: 0.901783  [   64/  124]
train() client id: f_00002-1-2 loss: 1.299386  [   96/  124]
train() client id: f_00002-2-0 loss: 1.067623  [   32/  124]
train() client id: f_00002-2-1 loss: 1.002323  [   64/  124]
train() client id: f_00002-2-2 loss: 1.168923  [   96/  124]
train() client id: f_00002-3-0 loss: 1.037009  [   32/  124]
train() client id: f_00002-3-1 loss: 1.134740  [   64/  124]
train() client id: f_00002-3-2 loss: 0.894426  [   96/  124]
train() client id: f_00002-4-0 loss: 1.042567  [   32/  124]
train() client id: f_00002-4-1 loss: 0.807933  [   64/  124]
train() client id: f_00002-4-2 loss: 1.073759  [   96/  124]
train() client id: f_00002-5-0 loss: 1.006000  [   32/  124]
train() client id: f_00002-5-1 loss: 1.089379  [   64/  124]
train() client id: f_00002-5-2 loss: 0.924913  [   96/  124]
train() client id: f_00002-6-0 loss: 0.895173  [   32/  124]
train() client id: f_00002-6-1 loss: 0.789039  [   64/  124]
train() client id: f_00002-6-2 loss: 1.026049  [   96/  124]
train() client id: f_00002-7-0 loss: 0.934159  [   32/  124]
train() client id: f_00002-7-1 loss: 0.812701  [   64/  124]
train() client id: f_00002-7-2 loss: 1.018350  [   96/  124]
train() client id: f_00002-8-0 loss: 0.989477  [   32/  124]
train() client id: f_00002-8-1 loss: 0.994429  [   64/  124]
train() client id: f_00002-8-2 loss: 0.988637  [   96/  124]
train() client id: f_00002-9-0 loss: 0.716701  [   32/  124]
train() client id: f_00002-9-1 loss: 0.990576  [   64/  124]
train() client id: f_00002-9-2 loss: 0.942605  [   96/  124]
train() client id: f_00002-10-0 loss: 0.904893  [   32/  124]
train() client id: f_00002-10-1 loss: 0.835777  [   64/  124]
train() client id: f_00002-10-2 loss: 1.032645  [   96/  124]
train() client id: f_00002-11-0 loss: 0.942127  [   32/  124]
train() client id: f_00002-11-1 loss: 0.944099  [   64/  124]
train() client id: f_00002-11-2 loss: 0.850263  [   96/  124]
train() client id: f_00002-12-0 loss: 1.103426  [   32/  124]
train() client id: f_00002-12-1 loss: 0.765086  [   64/  124]
train() client id: f_00002-12-2 loss: 0.831173  [   96/  124]
train() client id: f_00003-0-0 loss: 0.683228  [   32/   43]
train() client id: f_00003-1-0 loss: 0.699440  [   32/   43]
train() client id: f_00003-2-0 loss: 0.780996  [   32/   43]
train() client id: f_00003-3-0 loss: 0.854807  [   32/   43]
train() client id: f_00003-4-0 loss: 0.642969  [   32/   43]
train() client id: f_00003-5-0 loss: 0.670714  [   32/   43]
train() client id: f_00003-6-0 loss: 0.668175  [   32/   43]
train() client id: f_00003-7-0 loss: 0.853204  [   32/   43]
train() client id: f_00003-8-0 loss: 0.827702  [   32/   43]
train() client id: f_00003-9-0 loss: 0.778415  [   32/   43]
train() client id: f_00003-10-0 loss: 0.674655  [   32/   43]
train() client id: f_00003-11-0 loss: 0.584100  [   32/   43]
train() client id: f_00003-12-0 loss: 0.693244  [   32/   43]
train() client id: f_00004-0-0 loss: 0.683818  [   32/  306]
train() client id: f_00004-0-1 loss: 0.785723  [   64/  306]
train() client id: f_00004-0-2 loss: 0.851237  [   96/  306]
train() client id: f_00004-0-3 loss: 0.888629  [  128/  306]
train() client id: f_00004-0-4 loss: 0.728144  [  160/  306]
train() client id: f_00004-0-5 loss: 0.751722  [  192/  306]
train() client id: f_00004-0-6 loss: 0.794506  [  224/  306]
train() client id: f_00004-0-7 loss: 0.893491  [  256/  306]
train() client id: f_00004-0-8 loss: 0.726162  [  288/  306]
train() client id: f_00004-1-0 loss: 0.654858  [   32/  306]
train() client id: f_00004-1-1 loss: 0.821125  [   64/  306]
train() client id: f_00004-1-2 loss: 0.744779  [   96/  306]
train() client id: f_00004-1-3 loss: 0.886859  [  128/  306]
train() client id: f_00004-1-4 loss: 0.860012  [  160/  306]
train() client id: f_00004-1-5 loss: 0.797769  [  192/  306]
train() client id: f_00004-1-6 loss: 0.819933  [  224/  306]
train() client id: f_00004-1-7 loss: 0.741799  [  256/  306]
train() client id: f_00004-1-8 loss: 0.781347  [  288/  306]
train() client id: f_00004-2-0 loss: 0.722387  [   32/  306]
train() client id: f_00004-2-1 loss: 0.899987  [   64/  306]
train() client id: f_00004-2-2 loss: 0.834163  [   96/  306]
train() client id: f_00004-2-3 loss: 0.900886  [  128/  306]
train() client id: f_00004-2-4 loss: 0.788722  [  160/  306]
train() client id: f_00004-2-5 loss: 0.644796  [  192/  306]
train() client id: f_00004-2-6 loss: 0.820954  [  224/  306]
train() client id: f_00004-2-7 loss: 0.835032  [  256/  306]
train() client id: f_00004-2-8 loss: 0.739542  [  288/  306]
train() client id: f_00004-3-0 loss: 0.886314  [   32/  306]
train() client id: f_00004-3-1 loss: 0.695068  [   64/  306]
train() client id: f_00004-3-2 loss: 0.845245  [   96/  306]
train() client id: f_00004-3-3 loss: 0.849298  [  128/  306]
train() client id: f_00004-3-4 loss: 0.874457  [  160/  306]
train() client id: f_00004-3-5 loss: 0.709744  [  192/  306]
train() client id: f_00004-3-6 loss: 0.672156  [  224/  306]
train() client id: f_00004-3-7 loss: 0.814105  [  256/  306]
train() client id: f_00004-3-8 loss: 0.769455  [  288/  306]
train() client id: f_00004-4-0 loss: 0.739299  [   32/  306]
train() client id: f_00004-4-1 loss: 0.860209  [   64/  306]
train() client id: f_00004-4-2 loss: 0.816214  [   96/  306]
train() client id: f_00004-4-3 loss: 0.808775  [  128/  306]
train() client id: f_00004-4-4 loss: 0.796219  [  160/  306]
train() client id: f_00004-4-5 loss: 0.849658  [  192/  306]
train() client id: f_00004-4-6 loss: 0.782545  [  224/  306]
train() client id: f_00004-4-7 loss: 0.765161  [  256/  306]
train() client id: f_00004-4-8 loss: 0.739669  [  288/  306]
train() client id: f_00004-5-0 loss: 0.893891  [   32/  306]
train() client id: f_00004-5-1 loss: 0.790039  [   64/  306]
train() client id: f_00004-5-2 loss: 0.780676  [   96/  306]
train() client id: f_00004-5-3 loss: 0.762178  [  128/  306]
train() client id: f_00004-5-4 loss: 0.808894  [  160/  306]
train() client id: f_00004-5-5 loss: 0.727808  [  192/  306]
train() client id: f_00004-5-6 loss: 0.803276  [  224/  306]
train() client id: f_00004-5-7 loss: 0.748652  [  256/  306]
train() client id: f_00004-5-8 loss: 0.897693  [  288/  306]
train() client id: f_00004-6-0 loss: 0.896168  [   32/  306]
train() client id: f_00004-6-1 loss: 0.960940  [   64/  306]
train() client id: f_00004-6-2 loss: 0.787895  [   96/  306]
train() client id: f_00004-6-3 loss: 0.828354  [  128/  306]
train() client id: f_00004-6-4 loss: 0.834186  [  160/  306]
train() client id: f_00004-6-5 loss: 0.777762  [  192/  306]
train() client id: f_00004-6-6 loss: 0.644468  [  224/  306]
train() client id: f_00004-6-7 loss: 0.750331  [  256/  306]
train() client id: f_00004-6-8 loss: 0.703163  [  288/  306]
train() client id: f_00004-7-0 loss: 0.701648  [   32/  306]
train() client id: f_00004-7-1 loss: 0.777034  [   64/  306]
train() client id: f_00004-7-2 loss: 0.766801  [   96/  306]
train() client id: f_00004-7-3 loss: 0.881580  [  128/  306]
train() client id: f_00004-7-4 loss: 0.766492  [  160/  306]
train() client id: f_00004-7-5 loss: 0.800530  [  192/  306]
train() client id: f_00004-7-6 loss: 0.812256  [  224/  306]
train() client id: f_00004-7-7 loss: 0.770955  [  256/  306]
train() client id: f_00004-7-8 loss: 0.938284  [  288/  306]
train() client id: f_00004-8-0 loss: 0.734902  [   32/  306]
train() client id: f_00004-8-1 loss: 0.744941  [   64/  306]
train() client id: f_00004-8-2 loss: 0.810294  [   96/  306]
train() client id: f_00004-8-3 loss: 0.683951  [  128/  306]
train() client id: f_00004-8-4 loss: 0.754315  [  160/  306]
train() client id: f_00004-8-5 loss: 0.832021  [  192/  306]
train() client id: f_00004-8-6 loss: 0.857299  [  224/  306]
train() client id: f_00004-8-7 loss: 0.873928  [  256/  306]
train() client id: f_00004-8-8 loss: 0.760228  [  288/  306]
train() client id: f_00004-9-0 loss: 0.795404  [   32/  306]
train() client id: f_00004-9-1 loss: 0.883141  [   64/  306]
train() client id: f_00004-9-2 loss: 0.829803  [   96/  306]
train() client id: f_00004-9-3 loss: 0.761322  [  128/  306]
train() client id: f_00004-9-4 loss: 0.752268  [  160/  306]
train() client id: f_00004-9-5 loss: 0.850751  [  192/  306]
train() client id: f_00004-9-6 loss: 0.738816  [  224/  306]
train() client id: f_00004-9-7 loss: 0.734204  [  256/  306]
train() client id: f_00004-9-8 loss: 0.791422  [  288/  306]
train() client id: f_00004-10-0 loss: 0.860429  [   32/  306]
train() client id: f_00004-10-1 loss: 0.718075  [   64/  306]
train() client id: f_00004-10-2 loss: 0.854379  [   96/  306]
train() client id: f_00004-10-3 loss: 0.787888  [  128/  306]
train() client id: f_00004-10-4 loss: 0.791014  [  160/  306]
train() client id: f_00004-10-5 loss: 0.944109  [  192/  306]
train() client id: f_00004-10-6 loss: 0.748052  [  224/  306]
train() client id: f_00004-10-7 loss: 0.779043  [  256/  306]
train() client id: f_00004-10-8 loss: 0.766778  [  288/  306]
train() client id: f_00004-11-0 loss: 0.879882  [   32/  306]
train() client id: f_00004-11-1 loss: 0.817974  [   64/  306]
train() client id: f_00004-11-2 loss: 0.739662  [   96/  306]
train() client id: f_00004-11-3 loss: 0.781478  [  128/  306]
train() client id: f_00004-11-4 loss: 0.726692  [  160/  306]
train() client id: f_00004-11-5 loss: 0.817258  [  192/  306]
train() client id: f_00004-11-6 loss: 0.906061  [  224/  306]
train() client id: f_00004-11-7 loss: 0.790648  [  256/  306]
train() client id: f_00004-11-8 loss: 0.772767  [  288/  306]
train() client id: f_00004-12-0 loss: 0.687533  [   32/  306]
train() client id: f_00004-12-1 loss: 0.703629  [   64/  306]
train() client id: f_00004-12-2 loss: 0.892723  [   96/  306]
train() client id: f_00004-12-3 loss: 0.929750  [  128/  306]
train() client id: f_00004-12-4 loss: 0.825242  [  160/  306]
train() client id: f_00004-12-5 loss: 0.713628  [  192/  306]
train() client id: f_00004-12-6 loss: 0.859353  [  224/  306]
train() client id: f_00004-12-7 loss: 0.760963  [  256/  306]
train() client id: f_00004-12-8 loss: 0.799036  [  288/  306]
train() client id: f_00005-0-0 loss: 0.720280  [   32/  146]
train() client id: f_00005-0-1 loss: 0.456619  [   64/  146]
train() client id: f_00005-0-2 loss: 0.731834  [   96/  146]
train() client id: f_00005-0-3 loss: 0.921286  [  128/  146]
train() client id: f_00005-1-0 loss: 0.630491  [   32/  146]
train() client id: f_00005-1-1 loss: 0.398015  [   64/  146]
train() client id: f_00005-1-2 loss: 0.772917  [   96/  146]
train() client id: f_00005-1-3 loss: 0.836576  [  128/  146]
train() client id: f_00005-2-0 loss: 0.603817  [   32/  146]
train() client id: f_00005-2-1 loss: 0.581590  [   64/  146]
train() client id: f_00005-2-2 loss: 0.624074  [   96/  146]
train() client id: f_00005-2-3 loss: 0.878752  [  128/  146]
train() client id: f_00005-3-0 loss: 0.760007  [   32/  146]
train() client id: f_00005-3-1 loss: 0.629753  [   64/  146]
train() client id: f_00005-3-2 loss: 0.550965  [   96/  146]
train() client id: f_00005-3-3 loss: 0.729210  [  128/  146]
train() client id: f_00005-4-0 loss: 0.703870  [   32/  146]
train() client id: f_00005-4-1 loss: 0.727328  [   64/  146]
train() client id: f_00005-4-2 loss: 0.517054  [   96/  146]
train() client id: f_00005-4-3 loss: 0.722652  [  128/  146]
train() client id: f_00005-5-0 loss: 0.555927  [   32/  146]
train() client id: f_00005-5-1 loss: 0.578441  [   64/  146]
train() client id: f_00005-5-2 loss: 0.725951  [   96/  146]
train() client id: f_00005-5-3 loss: 0.770321  [  128/  146]
train() client id: f_00005-6-0 loss: 0.687401  [   32/  146]
train() client id: f_00005-6-1 loss: 0.652155  [   64/  146]
train() client id: f_00005-6-2 loss: 0.938116  [   96/  146]
train() client id: f_00005-6-3 loss: 0.505549  [  128/  146]
train() client id: f_00005-7-0 loss: 0.679169  [   32/  146]
train() client id: f_00005-7-1 loss: 0.769167  [   64/  146]
train() client id: f_00005-7-2 loss: 0.658317  [   96/  146]
train() client id: f_00005-7-3 loss: 0.721145  [  128/  146]
train() client id: f_00005-8-0 loss: 0.701548  [   32/  146]
train() client id: f_00005-8-1 loss: 0.643342  [   64/  146]
train() client id: f_00005-8-2 loss: 0.467348  [   96/  146]
train() client id: f_00005-8-3 loss: 1.059616  [  128/  146]
train() client id: f_00005-9-0 loss: 0.697080  [   32/  146]
train() client id: f_00005-9-1 loss: 0.985534  [   64/  146]
train() client id: f_00005-9-2 loss: 0.495129  [   96/  146]
train() client id: f_00005-9-3 loss: 0.631164  [  128/  146]
train() client id: f_00005-10-0 loss: 0.592486  [   32/  146]
train() client id: f_00005-10-1 loss: 0.566857  [   64/  146]
train() client id: f_00005-10-2 loss: 0.779746  [   96/  146]
train() client id: f_00005-10-3 loss: 0.942889  [  128/  146]
train() client id: f_00005-11-0 loss: 0.693708  [   32/  146]
train() client id: f_00005-11-1 loss: 0.740642  [   64/  146]
train() client id: f_00005-11-2 loss: 0.514657  [   96/  146]
train() client id: f_00005-11-3 loss: 0.740006  [  128/  146]
train() client id: f_00005-12-0 loss: 0.520438  [   32/  146]
train() client id: f_00005-12-1 loss: 0.799779  [   64/  146]
train() client id: f_00005-12-2 loss: 0.713900  [   96/  146]
train() client id: f_00005-12-3 loss: 0.805766  [  128/  146]
train() client id: f_00006-0-0 loss: 0.468645  [   32/   54]
train() client id: f_00006-1-0 loss: 0.434271  [   32/   54]
train() client id: f_00006-2-0 loss: 0.443704  [   32/   54]
train() client id: f_00006-3-0 loss: 0.508638  [   32/   54]
train() client id: f_00006-4-0 loss: 0.519962  [   32/   54]
train() client id: f_00006-5-0 loss: 0.410586  [   32/   54]
train() client id: f_00006-6-0 loss: 0.457505  [   32/   54]
train() client id: f_00006-7-0 loss: 0.500577  [   32/   54]
train() client id: f_00006-8-0 loss: 0.459580  [   32/   54]
train() client id: f_00006-9-0 loss: 0.422032  [   32/   54]
train() client id: f_00006-10-0 loss: 0.454700  [   32/   54]
train() client id: f_00006-11-0 loss: 0.487342  [   32/   54]
train() client id: f_00006-12-0 loss: 0.396032  [   32/   54]
train() client id: f_00007-0-0 loss: 0.813107  [   32/  179]
train() client id: f_00007-0-1 loss: 0.532669  [   64/  179]
train() client id: f_00007-0-2 loss: 0.693406  [   96/  179]
train() client id: f_00007-0-3 loss: 0.689937  [  128/  179]
train() client id: f_00007-0-4 loss: 0.562686  [  160/  179]
train() client id: f_00007-1-0 loss: 0.750824  [   32/  179]
train() client id: f_00007-1-1 loss: 0.604443  [   64/  179]
train() client id: f_00007-1-2 loss: 0.549753  [   96/  179]
train() client id: f_00007-1-3 loss: 0.525308  [  128/  179]
train() client id: f_00007-1-4 loss: 0.632868  [  160/  179]
train() client id: f_00007-2-0 loss: 0.767739  [   32/  179]
train() client id: f_00007-2-1 loss: 0.546393  [   64/  179]
train() client id: f_00007-2-2 loss: 0.568263  [   96/  179]
train() client id: f_00007-2-3 loss: 0.617056  [  128/  179]
train() client id: f_00007-2-4 loss: 0.619203  [  160/  179]
train() client id: f_00007-3-0 loss: 0.618119  [   32/  179]
train() client id: f_00007-3-1 loss: 0.582443  [   64/  179]
train() client id: f_00007-3-2 loss: 0.538305  [   96/  179]
train() client id: f_00007-3-3 loss: 0.713787  [  128/  179]
train() client id: f_00007-3-4 loss: 0.545841  [  160/  179]
train() client id: f_00007-4-0 loss: 0.599492  [   32/  179]
train() client id: f_00007-4-1 loss: 0.492247  [   64/  179]
train() client id: f_00007-4-2 loss: 0.610337  [   96/  179]
train() client id: f_00007-4-3 loss: 0.452807  [  128/  179]
train() client id: f_00007-4-4 loss: 0.649974  [  160/  179]
train() client id: f_00007-5-0 loss: 0.616946  [   32/  179]
train() client id: f_00007-5-1 loss: 0.726479  [   64/  179]
train() client id: f_00007-5-2 loss: 0.586412  [   96/  179]
train() client id: f_00007-5-3 loss: 0.653673  [  128/  179]
train() client id: f_00007-5-4 loss: 0.460033  [  160/  179]
train() client id: f_00007-6-0 loss: 0.607237  [   32/  179]
train() client id: f_00007-6-1 loss: 0.521597  [   64/  179]
train() client id: f_00007-6-2 loss: 0.483302  [   96/  179]
train() client id: f_00007-6-3 loss: 0.908793  [  128/  179]
train() client id: f_00007-6-4 loss: 0.449794  [  160/  179]
train() client id: f_00007-7-0 loss: 0.599306  [   32/  179]
train() client id: f_00007-7-1 loss: 0.801512  [   64/  179]
train() client id: f_00007-7-2 loss: 0.566596  [   96/  179]
train() client id: f_00007-7-3 loss: 0.586418  [  128/  179]
train() client id: f_00007-7-4 loss: 0.459879  [  160/  179]
train() client id: f_00007-8-0 loss: 0.608754  [   32/  179]
train() client id: f_00007-8-1 loss: 0.540089  [   64/  179]
train() client id: f_00007-8-2 loss: 0.423691  [   96/  179]
train() client id: f_00007-8-3 loss: 0.559205  [  128/  179]
train() client id: f_00007-8-4 loss: 0.598658  [  160/  179]
train() client id: f_00007-9-0 loss: 0.565860  [   32/  179]
train() client id: f_00007-9-1 loss: 0.750591  [   64/  179]
train() client id: f_00007-9-2 loss: 0.684648  [   96/  179]
train() client id: f_00007-9-3 loss: 0.491898  [  128/  179]
train() client id: f_00007-9-4 loss: 0.508414  [  160/  179]
train() client id: f_00007-10-0 loss: 0.781995  [   32/  179]
train() client id: f_00007-10-1 loss: 0.500711  [   64/  179]
train() client id: f_00007-10-2 loss: 0.449119  [   96/  179]
train() client id: f_00007-10-3 loss: 0.432237  [  128/  179]
train() client id: f_00007-10-4 loss: 0.813764  [  160/  179]
train() client id: f_00007-11-0 loss: 0.621530  [   32/  179]
train() client id: f_00007-11-1 loss: 0.697947  [   64/  179]
train() client id: f_00007-11-2 loss: 0.611825  [   96/  179]
train() client id: f_00007-11-3 loss: 0.549181  [  128/  179]
train() client id: f_00007-11-4 loss: 0.437915  [  160/  179]
train() client id: f_00007-12-0 loss: 0.804420  [   32/  179]
train() client id: f_00007-12-1 loss: 0.534278  [   64/  179]
train() client id: f_00007-12-2 loss: 0.533834  [   96/  179]
train() client id: f_00007-12-3 loss: 0.509497  [  128/  179]
train() client id: f_00007-12-4 loss: 0.498605  [  160/  179]
train() client id: f_00008-0-0 loss: 0.734402  [   32/  130]
train() client id: f_00008-0-1 loss: 0.681311  [   64/  130]
train() client id: f_00008-0-2 loss: 0.873130  [   96/  130]
train() client id: f_00008-0-3 loss: 0.694378  [  128/  130]
train() client id: f_00008-1-0 loss: 0.736839  [   32/  130]
train() client id: f_00008-1-1 loss: 0.797580  [   64/  130]
train() client id: f_00008-1-2 loss: 0.788933  [   96/  130]
train() client id: f_00008-1-3 loss: 0.663219  [  128/  130]
train() client id: f_00008-2-0 loss: 0.705046  [   32/  130]
train() client id: f_00008-2-1 loss: 0.938471  [   64/  130]
train() client id: f_00008-2-2 loss: 0.673387  [   96/  130]
train() client id: f_00008-2-3 loss: 0.700163  [  128/  130]
train() client id: f_00008-3-0 loss: 0.752493  [   32/  130]
train() client id: f_00008-3-1 loss: 0.702234  [   64/  130]
train() client id: f_00008-3-2 loss: 0.903310  [   96/  130]
train() client id: f_00008-3-3 loss: 0.665514  [  128/  130]
train() client id: f_00008-4-0 loss: 0.793307  [   32/  130]
train() client id: f_00008-4-1 loss: 0.695557  [   64/  130]
train() client id: f_00008-4-2 loss: 0.839175  [   96/  130]
train() client id: f_00008-4-3 loss: 0.689120  [  128/  130]
train() client id: f_00008-5-0 loss: 0.771258  [   32/  130]
train() client id: f_00008-5-1 loss: 0.786485  [   64/  130]
train() client id: f_00008-5-2 loss: 0.662477  [   96/  130]
train() client id: f_00008-5-3 loss: 0.795278  [  128/  130]
train() client id: f_00008-6-0 loss: 0.717184  [   32/  130]
train() client id: f_00008-6-1 loss: 0.871648  [   64/  130]
train() client id: f_00008-6-2 loss: 0.692194  [   96/  130]
train() client id: f_00008-6-3 loss: 0.705856  [  128/  130]
train() client id: f_00008-7-0 loss: 0.916306  [   32/  130]
train() client id: f_00008-7-1 loss: 0.640404  [   64/  130]
train() client id: f_00008-7-2 loss: 0.735638  [   96/  130]
train() client id: f_00008-7-3 loss: 0.690253  [  128/  130]
train() client id: f_00008-8-0 loss: 0.837075  [   32/  130]
train() client id: f_00008-8-1 loss: 0.738147  [   64/  130]
train() client id: f_00008-8-2 loss: 0.763102  [   96/  130]
train() client id: f_00008-8-3 loss: 0.676544  [  128/  130]
train() client id: f_00008-9-0 loss: 0.613239  [   32/  130]
train() client id: f_00008-9-1 loss: 0.880803  [   64/  130]
train() client id: f_00008-9-2 loss: 0.876972  [   96/  130]
train() client id: f_00008-9-3 loss: 0.641553  [  128/  130]
train() client id: f_00008-10-0 loss: 0.852135  [   32/  130]
train() client id: f_00008-10-1 loss: 0.701165  [   64/  130]
train() client id: f_00008-10-2 loss: 0.701922  [   96/  130]
train() client id: f_00008-10-3 loss: 0.733169  [  128/  130]
train() client id: f_00008-11-0 loss: 0.837673  [   32/  130]
train() client id: f_00008-11-1 loss: 0.663157  [   64/  130]
train() client id: f_00008-11-2 loss: 0.744808  [   96/  130]
train() client id: f_00008-11-3 loss: 0.765127  [  128/  130]
train() client id: f_00008-12-0 loss: 0.712857  [   32/  130]
train() client id: f_00008-12-1 loss: 0.810571  [   64/  130]
train() client id: f_00008-12-2 loss: 0.703258  [   96/  130]
train() client id: f_00008-12-3 loss: 0.786940  [  128/  130]
train() client id: f_00009-0-0 loss: 0.914376  [   32/  118]
train() client id: f_00009-0-1 loss: 1.048883  [   64/  118]
train() client id: f_00009-0-2 loss: 1.065557  [   96/  118]
train() client id: f_00009-1-0 loss: 1.047754  [   32/  118]
train() client id: f_00009-1-1 loss: 1.053042  [   64/  118]
train() client id: f_00009-1-2 loss: 1.008743  [   96/  118]
train() client id: f_00009-2-0 loss: 1.097533  [   32/  118]
train() client id: f_00009-2-1 loss: 0.824230  [   64/  118]
train() client id: f_00009-2-2 loss: 1.035963  [   96/  118]
train() client id: f_00009-3-0 loss: 0.945669  [   32/  118]
train() client id: f_00009-3-1 loss: 0.988600  [   64/  118]
train() client id: f_00009-3-2 loss: 0.944838  [   96/  118]
train() client id: f_00009-4-0 loss: 0.958100  [   32/  118]
train() client id: f_00009-4-1 loss: 0.899814  [   64/  118]
train() client id: f_00009-4-2 loss: 0.840518  [   96/  118]
train() client id: f_00009-5-0 loss: 0.945320  [   32/  118]
train() client id: f_00009-5-1 loss: 0.773389  [   64/  118]
train() client id: f_00009-5-2 loss: 0.859401  [   96/  118]
train() client id: f_00009-6-0 loss: 0.781363  [   32/  118]
train() client id: f_00009-6-1 loss: 0.862421  [   64/  118]
train() client id: f_00009-6-2 loss: 0.898225  [   96/  118]
train() client id: f_00009-7-0 loss: 1.010993  [   32/  118]
train() client id: f_00009-7-1 loss: 0.870021  [   64/  118]
train() client id: f_00009-7-2 loss: 0.672497  [   96/  118]
train() client id: f_00009-8-0 loss: 0.878886  [   32/  118]
train() client id: f_00009-8-1 loss: 0.813165  [   64/  118]
train() client id: f_00009-8-2 loss: 0.761357  [   96/  118]
train() client id: f_00009-9-0 loss: 0.934112  [   32/  118]
train() client id: f_00009-9-1 loss: 0.881861  [   64/  118]
train() client id: f_00009-9-2 loss: 0.700969  [   96/  118]
train() client id: f_00009-10-0 loss: 0.873555  [   32/  118]
train() client id: f_00009-10-1 loss: 0.863135  [   64/  118]
train() client id: f_00009-10-2 loss: 0.802260  [   96/  118]
train() client id: f_00009-11-0 loss: 0.762784  [   32/  118]
train() client id: f_00009-11-1 loss: 0.891552  [   64/  118]
train() client id: f_00009-11-2 loss: 0.781671  [   96/  118]
train() client id: f_00009-12-0 loss: 0.730114  [   32/  118]
train() client id: f_00009-12-1 loss: 0.836273  [   64/  118]
train() client id: f_00009-12-2 loss: 0.836321  [   96/  118]
At round 42 accuracy: 0.6472148541114059
At round 42 training accuracy: 0.5902079141515761
At round 42 training loss: 0.830242596924215
update_location
xs = [  -3.9056584     4.20031788  230.00902392   18.81129433    0.97929623
    3.95640986 -192.44319194 -171.32485185  214.66397685 -157.06087855]
ys = [ 222.5879595   205.55583871    1.32061395 -192.45517586  184.35018685
  167.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [244.05051502 228.62818177 250.81047646 217.69901127 209.72827757
 195.3899751  216.8900013  198.37560615 237.46429822 186.23676175]
dists_bs = [177.58909768 181.7867245  440.67770376 415.43486141 176.22751969
 179.10062273 178.47941057 174.21642051 420.2648364  171.93692248]
uav_gains = [8.31942576e-12 1.09578607e-11 7.30282910e-12 1.30831832e-11
 1.47787344e-11 1.82276138e-11 1.32493055e-11 1.74589294e-11
 9.39389080e-12 2.07944683e-11]
bs_gains = [5.55812671e-11 5.20619013e-11 4.36269716e-12 5.14619051e-12
 5.67920630e-11 5.42777976e-11 5.48084274e-11 5.86468436e-11
 4.98229610e-12 6.08499840e-11]
Round 43
-------------------------------
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.31748257 10.95769314  5.24195078  1.89681827 12.63633198  6.07983027
  2.34717104  7.46063269  5.5099153   4.93011902]
obj_prev = 62.377945058813665
eta_min = 4.6045000649351174e-18	eta_max = 0.9345899157842099
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 14.450064093228786	eta = 0.9090909090909091
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 27.618160282034836	eta = 0.47564435026761315
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 21.01662000340652	eta = 0.6250492182285267
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.820837826510346	eta = 0.6627581547216592
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.755289483233774	eta = 0.6649571961010281
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.755074608659463	eta = 0.6649644287942615
eta = 0.6649644287942615
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [0.03366898 0.07081173 0.03313455 0.01149021 0.08176751 0.03901326
 0.01442956 0.04783132 0.03473783 0.03153126]
ene_total = [1.80568477 3.13015273 1.80324111 0.85888841 3.56508653 1.85089364
 0.97553732 2.28873162 1.93281417 1.54404431]
ti_comp = [0.58057304 0.62215632 0.57583732 0.59457466 0.62339857 0.62275708
 0.59491807 0.60189896 0.55960141 0.62435462]
ti_coms = [0.11299635 0.07141307 0.11773207 0.09899474 0.07017082 0.07081231
 0.09865133 0.09167044 0.13396798 0.06921478]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [7.07711056e-06 5.73319437e-05 6.85683199e-06 2.68194986e-07
 8.79206213e-05 9.56928073e-06 5.30548435e-07 1.88786089e-05
 8.36624401e-06 5.02621941e-06]
ene_total = [0.45401354 0.2890566  0.47302065 0.39751757 0.28529668 0.2847264
 0.39614917 0.36885463 0.53827549 0.27812918]
optimize_network_iter = 0 obj = 3.765039909361509
eta = 0.6649644287942615
freqs = [28996330.2200803  56908313.47423983 28770753.14460626  9662542.53169571
 65582048.83208953 31323013.60893005 12127351.29718995 39733680.81777115
 31038012.71809572 25251084.61040274]
eta_min = 0.6649644287942639	eta_max = 0.6649644287943239
af = 0.006654514151181828	bf = 1.2587071102671177	zeta = 0.007319965566300011	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [1.65314891e-06 1.33922227e-05 1.60169383e-06 6.26479190e-08
 2.05374606e-05 2.23529729e-06 1.23931308e-07 4.40987202e-06
 1.95427881e-06 1.17407932e-06]
ene_total = [1.6724149  1.05878427 1.74248852 1.46497738 1.04145823 1.04824288
 1.45990451 1.35723249 1.98280693 1.02444478]
ti_comp = [0.58057304 0.62215632 0.57583732 0.59457466 0.62339857 0.62275708
 0.59491807 0.60189896 0.55960141 0.62435462]
ti_coms = [0.11299635 0.07141307 0.11773207 0.09899474 0.07017082 0.07081231
 0.09865133 0.09167044 0.13396798 0.06921478]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [7.07711056e-06 5.73319437e-05 6.85683199e-06 2.68194986e-07
 8.79206213e-05 9.56928073e-06 5.30548435e-07 1.88786089e-05
 8.36624401e-06 5.02621941e-06]
ene_total = [0.45401354 0.2890566  0.47302065 0.39751757 0.28529668 0.2847264
 0.39614917 0.36885463 0.53827549 0.27812918]
optimize_network_iter = 1 obj = 3.765039909362207
eta = 0.6649644287943239
freqs = [28996330.22008007 56908313.47423852 28770753.14460608  9662542.53169558
 65582048.832088   31323013.60892932 12127351.29718979 39733680.81777052
 31038012.71809573 25251084.61040214]
Done!
At round 43 eta: 0.6649644287943239
At round 43 local rounds: 13.360704448743727
At round 43 global rounds: 40.15433760708037
At round 43 a_n: 13.110585589604415
gradient difference: 0.44322362542152405
train() client id: f_00000-0-0 loss: 1.125357  [   32/  126]
train() client id: f_00000-0-1 loss: 1.031334  [   64/  126]
train() client id: f_00000-0-2 loss: 1.206980  [   96/  126]
train() client id: f_00000-1-0 loss: 1.150838  [   32/  126]
train() client id: f_00000-1-1 loss: 1.043923  [   64/  126]
train() client id: f_00000-1-2 loss: 0.974409  [   96/  126]
train() client id: f_00000-2-0 loss: 0.906955  [   32/  126]
train() client id: f_00000-2-1 loss: 1.021725  [   64/  126]
train() client id: f_00000-2-2 loss: 0.924795  [   96/  126]
train() client id: f_00000-3-0 loss: 0.955134  [   32/  126]
train() client id: f_00000-3-1 loss: 0.919299  [   64/  126]
train() client id: f_00000-3-2 loss: 0.851105  [   96/  126]
train() client id: f_00000-4-0 loss: 0.745508  [   32/  126]
train() client id: f_00000-4-1 loss: 0.794500  [   64/  126]
train() client id: f_00000-4-2 loss: 0.828592  [   96/  126]
train() client id: f_00000-5-0 loss: 0.830685  [   32/  126]
train() client id: f_00000-5-1 loss: 0.842634  [   64/  126]
train() client id: f_00000-5-2 loss: 0.810237  [   96/  126]
train() client id: f_00000-6-0 loss: 0.750904  [   32/  126]
train() client id: f_00000-6-1 loss: 0.770746  [   64/  126]
train() client id: f_00000-6-2 loss: 0.877819  [   96/  126]
train() client id: f_00000-7-0 loss: 0.740470  [   32/  126]
train() client id: f_00000-7-1 loss: 0.773964  [   64/  126]
train() client id: f_00000-7-2 loss: 0.808365  [   96/  126]
train() client id: f_00000-8-0 loss: 0.797229  [   32/  126]
train() client id: f_00000-8-1 loss: 0.805858  [   64/  126]
train() client id: f_00000-8-2 loss: 0.677276  [   96/  126]
train() client id: f_00000-9-0 loss: 0.813859  [   32/  126]
train() client id: f_00000-9-1 loss: 0.708001  [   64/  126]
train() client id: f_00000-9-2 loss: 0.681495  [   96/  126]
train() client id: f_00000-10-0 loss: 0.878428  [   32/  126]
train() client id: f_00000-10-1 loss: 0.687153  [   64/  126]
train() client id: f_00000-10-2 loss: 0.640853  [   96/  126]
train() client id: f_00000-11-0 loss: 0.653731  [   32/  126]
train() client id: f_00000-11-1 loss: 0.689525  [   64/  126]
train() client id: f_00000-11-2 loss: 0.800768  [   96/  126]
train() client id: f_00000-12-0 loss: 0.626115  [   32/  126]
train() client id: f_00000-12-1 loss: 0.814964  [   64/  126]
train() client id: f_00000-12-2 loss: 0.665966  [   96/  126]
train() client id: f_00001-0-0 loss: 0.681914  [   32/  265]
train() client id: f_00001-0-1 loss: 0.530051  [   64/  265]
train() client id: f_00001-0-2 loss: 0.476629  [   96/  265]
train() client id: f_00001-0-3 loss: 0.578550  [  128/  265]
train() client id: f_00001-0-4 loss: 0.417992  [  160/  265]
train() client id: f_00001-0-5 loss: 0.492501  [  192/  265]
train() client id: f_00001-0-6 loss: 0.472617  [  224/  265]
train() client id: f_00001-0-7 loss: 0.405394  [  256/  265]
train() client id: f_00001-1-0 loss: 0.647713  [   32/  265]
train() client id: f_00001-1-1 loss: 0.403665  [   64/  265]
train() client id: f_00001-1-2 loss: 0.469329  [   96/  265]
train() client id: f_00001-1-3 loss: 0.568518  [  128/  265]
train() client id: f_00001-1-4 loss: 0.411396  [  160/  265]
train() client id: f_00001-1-5 loss: 0.444954  [  192/  265]
train() client id: f_00001-1-6 loss: 0.531695  [  224/  265]
train() client id: f_00001-1-7 loss: 0.468730  [  256/  265]
train() client id: f_00001-2-0 loss: 0.514207  [   32/  265]
train() client id: f_00001-2-1 loss: 0.502945  [   64/  265]
train() client id: f_00001-2-2 loss: 0.513689  [   96/  265]
train() client id: f_00001-2-3 loss: 0.478035  [  128/  265]
train() client id: f_00001-2-4 loss: 0.440145  [  160/  265]
train() client id: f_00001-2-5 loss: 0.440017  [  192/  265]
train() client id: f_00001-2-6 loss: 0.537992  [  224/  265]
train() client id: f_00001-2-7 loss: 0.424348  [  256/  265]
train() client id: f_00001-3-0 loss: 0.419320  [   32/  265]
train() client id: f_00001-3-1 loss: 0.460149  [   64/  265]
train() client id: f_00001-3-2 loss: 0.531777  [   96/  265]
train() client id: f_00001-3-3 loss: 0.569014  [  128/  265]
train() client id: f_00001-3-4 loss: 0.450429  [  160/  265]
train() client id: f_00001-3-5 loss: 0.394713  [  192/  265]
train() client id: f_00001-3-6 loss: 0.526226  [  224/  265]
train() client id: f_00001-3-7 loss: 0.453528  [  256/  265]
train() client id: f_00001-4-0 loss: 0.491894  [   32/  265]
train() client id: f_00001-4-1 loss: 0.415630  [   64/  265]
train() client id: f_00001-4-2 loss: 0.607003  [   96/  265]
train() client id: f_00001-4-3 loss: 0.491375  [  128/  265]
train() client id: f_00001-4-4 loss: 0.387926  [  160/  265]
train() client id: f_00001-4-5 loss: 0.536788  [  192/  265]
train() client id: f_00001-4-6 loss: 0.440105  [  224/  265]
train() client id: f_00001-4-7 loss: 0.432616  [  256/  265]
train() client id: f_00001-5-0 loss: 0.382572  [   32/  265]
train() client id: f_00001-5-1 loss: 0.481057  [   64/  265]
train() client id: f_00001-5-2 loss: 0.540119  [   96/  265]
train() client id: f_00001-5-3 loss: 0.415428  [  128/  265]
train() client id: f_00001-5-4 loss: 0.530598  [  160/  265]
train() client id: f_00001-5-5 loss: 0.517183  [  192/  265]
train() client id: f_00001-5-6 loss: 0.519502  [  224/  265]
train() client id: f_00001-5-7 loss: 0.474338  [  256/  265]
train() client id: f_00001-6-0 loss: 0.487949  [   32/  265]
train() client id: f_00001-6-1 loss: 0.525117  [   64/  265]
train() client id: f_00001-6-2 loss: 0.589246  [   96/  265]
train() client id: f_00001-6-3 loss: 0.405087  [  128/  265]
train() client id: f_00001-6-4 loss: 0.485656  [  160/  265]
train() client id: f_00001-6-5 loss: 0.461713  [  192/  265]
train() client id: f_00001-6-6 loss: 0.417934  [  224/  265]
train() client id: f_00001-6-7 loss: 0.456218  [  256/  265]
train() client id: f_00001-7-0 loss: 0.528988  [   32/  265]
train() client id: f_00001-7-1 loss: 0.440903  [   64/  265]
train() client id: f_00001-7-2 loss: 0.546723  [   96/  265]
train() client id: f_00001-7-3 loss: 0.435369  [  128/  265]
train() client id: f_00001-7-4 loss: 0.442680  [  160/  265]
train() client id: f_00001-7-5 loss: 0.445063  [  192/  265]
train() client id: f_00001-7-6 loss: 0.561675  [  224/  265]
train() client id: f_00001-7-7 loss: 0.374509  [  256/  265]
train() client id: f_00001-8-0 loss: 0.450323  [   32/  265]
train() client id: f_00001-8-1 loss: 0.508962  [   64/  265]
train() client id: f_00001-8-2 loss: 0.393886  [   96/  265]
train() client id: f_00001-8-3 loss: 0.421552  [  128/  265]
train() client id: f_00001-8-4 loss: 0.472264  [  160/  265]
train() client id: f_00001-8-5 loss: 0.568741  [  192/  265]
train() client id: f_00001-8-6 loss: 0.614991  [  224/  265]
train() client id: f_00001-8-7 loss: 0.397555  [  256/  265]
train() client id: f_00001-9-0 loss: 0.482552  [   32/  265]
train() client id: f_00001-9-1 loss: 0.390397  [   64/  265]
train() client id: f_00001-9-2 loss: 0.488479  [   96/  265]
train() client id: f_00001-9-3 loss: 0.387010  [  128/  265]
train() client id: f_00001-9-4 loss: 0.523901  [  160/  265]
train() client id: f_00001-9-5 loss: 0.374164  [  192/  265]
train() client id: f_00001-9-6 loss: 0.676967  [  224/  265]
train() client id: f_00001-9-7 loss: 0.505692  [  256/  265]
train() client id: f_00001-10-0 loss: 0.480428  [   32/  265]
train() client id: f_00001-10-1 loss: 0.527211  [   64/  265]
train() client id: f_00001-10-2 loss: 0.414124  [   96/  265]
train() client id: f_00001-10-3 loss: 0.470672  [  128/  265]
train() client id: f_00001-10-4 loss: 0.659748  [  160/  265]
train() client id: f_00001-10-5 loss: 0.421487  [  192/  265]
train() client id: f_00001-10-6 loss: 0.408088  [  224/  265]
train() client id: f_00001-10-7 loss: 0.445699  [  256/  265]
train() client id: f_00001-11-0 loss: 0.454325  [   32/  265]
train() client id: f_00001-11-1 loss: 0.433273  [   64/  265]
train() client id: f_00001-11-2 loss: 0.465887  [   96/  265]
train() client id: f_00001-11-3 loss: 0.502889  [  128/  265]
train() client id: f_00001-11-4 loss: 0.532026  [  160/  265]
train() client id: f_00001-11-5 loss: 0.448098  [  192/  265]
train() client id: f_00001-11-6 loss: 0.379343  [  224/  265]
train() client id: f_00001-11-7 loss: 0.603013  [  256/  265]
train() client id: f_00001-12-0 loss: 0.524026  [   32/  265]
train() client id: f_00001-12-1 loss: 0.409890  [   64/  265]
train() client id: f_00001-12-2 loss: 0.552749  [   96/  265]
train() client id: f_00001-12-3 loss: 0.558558  [  128/  265]
train() client id: f_00001-12-4 loss: 0.375012  [  160/  265]
train() client id: f_00001-12-5 loss: 0.539242  [  192/  265]
train() client id: f_00001-12-6 loss: 0.366559  [  224/  265]
train() client id: f_00001-12-7 loss: 0.491804  [  256/  265]
train() client id: f_00002-0-0 loss: 0.982635  [   32/  124]
train() client id: f_00002-0-1 loss: 1.451566  [   64/  124]
train() client id: f_00002-0-2 loss: 1.126555  [   96/  124]
train() client id: f_00002-1-0 loss: 1.226952  [   32/  124]
train() client id: f_00002-1-1 loss: 1.114289  [   64/  124]
train() client id: f_00002-1-2 loss: 1.082126  [   96/  124]
train() client id: f_00002-2-0 loss: 1.302947  [   32/  124]
train() client id: f_00002-2-1 loss: 1.236013  [   64/  124]
train() client id: f_00002-2-2 loss: 0.903643  [   96/  124]
train() client id: f_00002-3-0 loss: 1.187078  [   32/  124]
train() client id: f_00002-3-1 loss: 1.073687  [   64/  124]
train() client id: f_00002-3-2 loss: 1.187794  [   96/  124]
train() client id: f_00002-4-0 loss: 0.976277  [   32/  124]
train() client id: f_00002-4-1 loss: 1.124685  [   64/  124]
train() client id: f_00002-4-2 loss: 1.146226  [   96/  124]
train() client id: f_00002-5-0 loss: 1.132807  [   32/  124]
train() client id: f_00002-5-1 loss: 1.071965  [   64/  124]
train() client id: f_00002-5-2 loss: 1.070623  [   96/  124]
train() client id: f_00002-6-0 loss: 1.020185  [   32/  124]
train() client id: f_00002-6-1 loss: 1.063779  [   64/  124]
train() client id: f_00002-6-2 loss: 0.896935  [   96/  124]
train() client id: f_00002-7-0 loss: 1.071718  [   32/  124]
train() client id: f_00002-7-1 loss: 1.174441  [   64/  124]
train() client id: f_00002-7-2 loss: 1.108802  [   96/  124]
train() client id: f_00002-8-0 loss: 1.046057  [   32/  124]
train() client id: f_00002-8-1 loss: 0.880853  [   64/  124]
train() client id: f_00002-8-2 loss: 1.017581  [   96/  124]
train() client id: f_00002-9-0 loss: 0.982724  [   32/  124]
train() client id: f_00002-9-1 loss: 1.165565  [   64/  124]
train() client id: f_00002-9-2 loss: 1.088266  [   96/  124]
train() client id: f_00002-10-0 loss: 1.084671  [   32/  124]
train() client id: f_00002-10-1 loss: 1.056216  [   64/  124]
train() client id: f_00002-10-2 loss: 1.122422  [   96/  124]
train() client id: f_00002-11-0 loss: 0.978304  [   32/  124]
train() client id: f_00002-11-1 loss: 1.156799  [   64/  124]
train() client id: f_00002-11-2 loss: 1.128690  [   96/  124]
train() client id: f_00002-12-0 loss: 1.010320  [   32/  124]
train() client id: f_00002-12-1 loss: 1.172081  [   64/  124]
train() client id: f_00002-12-2 loss: 1.069371  [   96/  124]
train() client id: f_00003-0-0 loss: 0.861851  [   32/   43]
train() client id: f_00003-1-0 loss: 0.774233  [   32/   43]
train() client id: f_00003-2-0 loss: 0.846289  [   32/   43]
train() client id: f_00003-3-0 loss: 0.711268  [   32/   43]
train() client id: f_00003-4-0 loss: 0.660060  [   32/   43]
train() client id: f_00003-5-0 loss: 0.578886  [   32/   43]
train() client id: f_00003-6-0 loss: 0.659038  [   32/   43]
train() client id: f_00003-7-0 loss: 0.619268  [   32/   43]
train() client id: f_00003-8-0 loss: 0.764485  [   32/   43]
train() client id: f_00003-9-0 loss: 0.786418  [   32/   43]
train() client id: f_00003-10-0 loss: 0.828569  [   32/   43]
train() client id: f_00003-11-0 loss: 0.811562  [   32/   43]
train() client id: f_00003-12-0 loss: 0.674522  [   32/   43]
train() client id: f_00004-0-0 loss: 0.675103  [   32/  306]
train() client id: f_00004-0-1 loss: 0.826292  [   64/  306]
train() client id: f_00004-0-2 loss: 0.788207  [   96/  306]
train() client id: f_00004-0-3 loss: 0.800968  [  128/  306]
train() client id: f_00004-0-4 loss: 0.937392  [  160/  306]
train() client id: f_00004-0-5 loss: 0.792166  [  192/  306]
train() client id: f_00004-0-6 loss: 0.894132  [  224/  306]
train() client id: f_00004-0-7 loss: 0.714138  [  256/  306]
train() client id: f_00004-0-8 loss: 0.890709  [  288/  306]
train() client id: f_00004-1-0 loss: 0.779235  [   32/  306]
train() client id: f_00004-1-1 loss: 0.844436  [   64/  306]
train() client id: f_00004-1-2 loss: 0.721708  [   96/  306]
train() client id: f_00004-1-3 loss: 0.671787  [  128/  306]
train() client id: f_00004-1-4 loss: 0.917239  [  160/  306]
train() client id: f_00004-1-5 loss: 0.894535  [  192/  306]
train() client id: f_00004-1-6 loss: 0.917250  [  224/  306]
train() client id: f_00004-1-7 loss: 0.930299  [  256/  306]
train() client id: f_00004-1-8 loss: 0.794685  [  288/  306]
train() client id: f_00004-2-0 loss: 0.960834  [   32/  306]
train() client id: f_00004-2-1 loss: 0.822787  [   64/  306]
train() client id: f_00004-2-2 loss: 0.757493  [   96/  306]
train() client id: f_00004-2-3 loss: 0.741499  [  128/  306]
train() client id: f_00004-2-4 loss: 0.856311  [  160/  306]
train() client id: f_00004-2-5 loss: 0.757095  [  192/  306]
train() client id: f_00004-2-6 loss: 0.913629  [  224/  306]
train() client id: f_00004-2-7 loss: 0.798161  [  256/  306]
train() client id: f_00004-2-8 loss: 0.726832  [  288/  306]
train() client id: f_00004-3-0 loss: 0.843860  [   32/  306]
train() client id: f_00004-3-1 loss: 0.896106  [   64/  306]
train() client id: f_00004-3-2 loss: 0.874493  [   96/  306]
train() client id: f_00004-3-3 loss: 0.876862  [  128/  306]
train() client id: f_00004-3-4 loss: 0.760014  [  160/  306]
train() client id: f_00004-3-5 loss: 0.837622  [  192/  306]
train() client id: f_00004-3-6 loss: 0.805908  [  224/  306]
train() client id: f_00004-3-7 loss: 0.673028  [  256/  306]
train() client id: f_00004-3-8 loss: 0.706354  [  288/  306]
train() client id: f_00004-4-0 loss: 0.726764  [   32/  306]
train() client id: f_00004-4-1 loss: 0.778584  [   64/  306]
train() client id: f_00004-4-2 loss: 0.791422  [   96/  306]
train() client id: f_00004-4-3 loss: 0.967263  [  128/  306]
train() client id: f_00004-4-4 loss: 0.785130  [  160/  306]
train() client id: f_00004-4-5 loss: 0.763850  [  192/  306]
train() client id: f_00004-4-6 loss: 0.832063  [  224/  306]
train() client id: f_00004-4-7 loss: 0.782574  [  256/  306]
train() client id: f_00004-4-8 loss: 0.851641  [  288/  306]
train() client id: f_00004-5-0 loss: 0.674185  [   32/  306]
train() client id: f_00004-5-1 loss: 0.833460  [   64/  306]
train() client id: f_00004-5-2 loss: 0.779055  [   96/  306]
train() client id: f_00004-5-3 loss: 0.833882  [  128/  306]
train() client id: f_00004-5-4 loss: 0.692691  [  160/  306]
train() client id: f_00004-5-5 loss: 0.925753  [  192/  306]
train() client id: f_00004-5-6 loss: 0.841172  [  224/  306]
train() client id: f_00004-5-7 loss: 0.982015  [  256/  306]
train() client id: f_00004-5-8 loss: 0.727604  [  288/  306]
train() client id: f_00004-6-0 loss: 0.814628  [   32/  306]
train() client id: f_00004-6-1 loss: 0.853005  [   64/  306]
train() client id: f_00004-6-2 loss: 0.786833  [   96/  306]
train() client id: f_00004-6-3 loss: 0.881160  [  128/  306]
train() client id: f_00004-6-4 loss: 0.785469  [  160/  306]
train() client id: f_00004-6-5 loss: 0.861168  [  192/  306]
train() client id: f_00004-6-6 loss: 0.799036  [  224/  306]
train() client id: f_00004-6-7 loss: 0.755918  [  256/  306]
train() client id: f_00004-6-8 loss: 0.819176  [  288/  306]
train() client id: f_00004-7-0 loss: 0.792547  [   32/  306]
train() client id: f_00004-7-1 loss: 0.829233  [   64/  306]
train() client id: f_00004-7-2 loss: 0.820442  [   96/  306]
train() client id: f_00004-7-3 loss: 0.808012  [  128/  306]
train() client id: f_00004-7-4 loss: 0.705330  [  160/  306]
train() client id: f_00004-7-5 loss: 0.899313  [  192/  306]
train() client id: f_00004-7-6 loss: 0.877139  [  224/  306]
train() client id: f_00004-7-7 loss: 0.788136  [  256/  306]
train() client id: f_00004-7-8 loss: 0.875613  [  288/  306]
train() client id: f_00004-8-0 loss: 0.907200  [   32/  306]
train() client id: f_00004-8-1 loss: 0.722970  [   64/  306]
train() client id: f_00004-8-2 loss: 0.811115  [   96/  306]
train() client id: f_00004-8-3 loss: 0.869875  [  128/  306]
train() client id: f_00004-8-4 loss: 0.686293  [  160/  306]
train() client id: f_00004-8-5 loss: 0.833811  [  192/  306]
train() client id: f_00004-8-6 loss: 0.737747  [  224/  306]
train() client id: f_00004-8-7 loss: 0.827105  [  256/  306]
train() client id: f_00004-8-8 loss: 0.931559  [  288/  306]
train() client id: f_00004-9-0 loss: 0.765919  [   32/  306]
train() client id: f_00004-9-1 loss: 0.852926  [   64/  306]
train() client id: f_00004-9-2 loss: 0.813923  [   96/  306]
train() client id: f_00004-9-3 loss: 0.824037  [  128/  306]
train() client id: f_00004-9-4 loss: 0.816142  [  160/  306]
train() client id: f_00004-9-5 loss: 0.791098  [  192/  306]
train() client id: f_00004-9-6 loss: 0.783944  [  224/  306]
train() client id: f_00004-9-7 loss: 0.893741  [  256/  306]
train() client id: f_00004-9-8 loss: 0.784086  [  288/  306]
train() client id: f_00004-10-0 loss: 0.664736  [   32/  306]
train() client id: f_00004-10-1 loss: 0.872039  [   64/  306]
train() client id: f_00004-10-2 loss: 0.858554  [   96/  306]
train() client id: f_00004-10-3 loss: 0.853654  [  128/  306]
train() client id: f_00004-10-4 loss: 0.855457  [  160/  306]
train() client id: f_00004-10-5 loss: 0.987492  [  192/  306]
train() client id: f_00004-10-6 loss: 0.729797  [  224/  306]
train() client id: f_00004-10-7 loss: 0.706135  [  256/  306]
train() client id: f_00004-10-8 loss: 0.781512  [  288/  306]
train() client id: f_00004-11-0 loss: 0.710340  [   32/  306]
train() client id: f_00004-11-1 loss: 0.802787  [   64/  306]
train() client id: f_00004-11-2 loss: 0.737072  [   96/  306]
train() client id: f_00004-11-3 loss: 0.773545  [  128/  306]
train() client id: f_00004-11-4 loss: 0.897483  [  160/  306]
train() client id: f_00004-11-5 loss: 0.811582  [  192/  306]
train() client id: f_00004-11-6 loss: 0.873270  [  224/  306]
train() client id: f_00004-11-7 loss: 0.959939  [  256/  306]
train() client id: f_00004-11-8 loss: 0.823015  [  288/  306]
train() client id: f_00004-12-0 loss: 0.712328  [   32/  306]
train() client id: f_00004-12-1 loss: 0.830112  [   64/  306]
train() client id: f_00004-12-2 loss: 0.770951  [   96/  306]
train() client id: f_00004-12-3 loss: 0.868442  [  128/  306]
train() client id: f_00004-12-4 loss: 0.783945  [  160/  306]
train() client id: f_00004-12-5 loss: 0.815863  [  192/  306]
train() client id: f_00004-12-6 loss: 0.919605  [  224/  306]
train() client id: f_00004-12-7 loss: 0.795507  [  256/  306]
train() client id: f_00004-12-8 loss: 0.866366  [  288/  306]
train() client id: f_00005-0-0 loss: 0.411060  [   32/  146]
train() client id: f_00005-0-1 loss: 0.559295  [   64/  146]
train() client id: f_00005-0-2 loss: 0.534784  [   96/  146]
train() client id: f_00005-0-3 loss: 0.589107  [  128/  146]
train() client id: f_00005-1-0 loss: 0.342192  [   32/  146]
train() client id: f_00005-1-1 loss: 0.734987  [   64/  146]
train() client id: f_00005-1-2 loss: 0.455986  [   96/  146]
train() client id: f_00005-1-3 loss: 0.529082  [  128/  146]
train() client id: f_00005-2-0 loss: 0.536443  [   32/  146]
train() client id: f_00005-2-1 loss: 0.306698  [   64/  146]
train() client id: f_00005-2-2 loss: 0.575755  [   96/  146]
train() client id: f_00005-2-3 loss: 0.645637  [  128/  146]
train() client id: f_00005-3-0 loss: 0.499241  [   32/  146]
train() client id: f_00005-3-1 loss: 0.421989  [   64/  146]
train() client id: f_00005-3-2 loss: 0.380482  [   96/  146]
train() client id: f_00005-3-3 loss: 0.751792  [  128/  146]
train() client id: f_00005-4-0 loss: 0.515084  [   32/  146]
train() client id: f_00005-4-1 loss: 0.416680  [   64/  146]
train() client id: f_00005-4-2 loss: 0.583419  [   96/  146]
train() client id: f_00005-4-3 loss: 0.616324  [  128/  146]
train() client id: f_00005-5-0 loss: 0.310395  [   32/  146]
train() client id: f_00005-5-1 loss: 0.612512  [   64/  146]
train() client id: f_00005-5-2 loss: 0.568437  [   96/  146]
train() client id: f_00005-5-3 loss: 0.490220  [  128/  146]
train() client id: f_00005-6-0 loss: 0.611585  [   32/  146]
train() client id: f_00005-6-1 loss: 0.494444  [   64/  146]
train() client id: f_00005-6-2 loss: 0.597319  [   96/  146]
train() client id: f_00005-6-3 loss: 0.398198  [  128/  146]
train() client id: f_00005-7-0 loss: 0.356296  [   32/  146]
train() client id: f_00005-7-1 loss: 0.672416  [   64/  146]
train() client id: f_00005-7-2 loss: 0.290809  [   96/  146]
train() client id: f_00005-7-3 loss: 0.796871  [  128/  146]
train() client id: f_00005-8-0 loss: 0.517556  [   32/  146]
train() client id: f_00005-8-1 loss: 0.467047  [   64/  146]
train() client id: f_00005-8-2 loss: 0.600199  [   96/  146]
train() client id: f_00005-8-3 loss: 0.576813  [  128/  146]
train() client id: f_00005-9-0 loss: 0.313707  [   32/  146]
train() client id: f_00005-9-1 loss: 0.398164  [   64/  146]
train() client id: f_00005-9-2 loss: 0.484487  [   96/  146]
train() client id: f_00005-9-3 loss: 0.681234  [  128/  146]
train() client id: f_00005-10-0 loss: 0.466144  [   32/  146]
train() client id: f_00005-10-1 loss: 0.661559  [   64/  146]
train() client id: f_00005-10-2 loss: 0.391402  [   96/  146]
train() client id: f_00005-10-3 loss: 0.622226  [  128/  146]
train() client id: f_00005-11-0 loss: 0.594890  [   32/  146]
train() client id: f_00005-11-1 loss: 0.353171  [   64/  146]
train() client id: f_00005-11-2 loss: 0.563537  [   96/  146]
train() client id: f_00005-11-3 loss: 0.604394  [  128/  146]
train() client id: f_00005-12-0 loss: 0.307012  [   32/  146]
train() client id: f_00005-12-1 loss: 0.542639  [   64/  146]
train() client id: f_00005-12-2 loss: 0.774277  [   96/  146]
train() client id: f_00005-12-3 loss: 0.461186  [  128/  146]
train() client id: f_00006-0-0 loss: 0.555614  [   32/   54]
train() client id: f_00006-1-0 loss: 0.558686  [   32/   54]
train() client id: f_00006-2-0 loss: 0.530472  [   32/   54]
train() client id: f_00006-3-0 loss: 0.563272  [   32/   54]
train() client id: f_00006-4-0 loss: 0.509793  [   32/   54]
train() client id: f_00006-5-0 loss: 0.552953  [   32/   54]
train() client id: f_00006-6-0 loss: 0.555009  [   32/   54]
train() client id: f_00006-7-0 loss: 0.605715  [   32/   54]
train() client id: f_00006-8-0 loss: 0.542893  [   32/   54]
train() client id: f_00006-9-0 loss: 0.604834  [   32/   54]
train() client id: f_00006-10-0 loss: 0.550260  [   32/   54]
train() client id: f_00006-11-0 loss: 0.508226  [   32/   54]
train() client id: f_00006-12-0 loss: 0.535906  [   32/   54]
train() client id: f_00007-0-0 loss: 0.686764  [   32/  179]
train() client id: f_00007-0-1 loss: 0.588068  [   64/  179]
train() client id: f_00007-0-2 loss: 0.609461  [   96/  179]
train() client id: f_00007-0-3 loss: 0.629614  [  128/  179]
train() client id: f_00007-0-4 loss: 0.703213  [  160/  179]
train() client id: f_00007-1-0 loss: 0.522981  [   32/  179]
train() client id: f_00007-1-1 loss: 0.483882  [   64/  179]
train() client id: f_00007-1-2 loss: 0.530240  [   96/  179]
train() client id: f_00007-1-3 loss: 0.854859  [  128/  179]
train() client id: f_00007-1-4 loss: 0.736263  [  160/  179]
train() client id: f_00007-2-0 loss: 0.910540  [   32/  179]
train() client id: f_00007-2-1 loss: 0.599421  [   64/  179]
train() client id: f_00007-2-2 loss: 0.634835  [   96/  179]
train() client id: f_00007-2-3 loss: 0.649943  [  128/  179]
train() client id: f_00007-2-4 loss: 0.477526  [  160/  179]
train() client id: f_00007-3-0 loss: 0.821908  [   32/  179]
train() client id: f_00007-3-1 loss: 0.531358  [   64/  179]
train() client id: f_00007-3-2 loss: 0.593459  [   96/  179]
train() client id: f_00007-3-3 loss: 0.718003  [  128/  179]
train() client id: f_00007-3-4 loss: 0.542066  [  160/  179]
train() client id: f_00007-4-0 loss: 0.621326  [   32/  179]
train() client id: f_00007-4-1 loss: 0.575712  [   64/  179]
train() client id: f_00007-4-2 loss: 0.744083  [   96/  179]
train() client id: f_00007-4-3 loss: 0.668092  [  128/  179]
train() client id: f_00007-4-4 loss: 0.566053  [  160/  179]
train() client id: f_00007-5-0 loss: 0.696914  [   32/  179]
train() client id: f_00007-5-1 loss: 0.551403  [   64/  179]
train() client id: f_00007-5-2 loss: 0.726880  [   96/  179]
train() client id: f_00007-5-3 loss: 0.581185  [  128/  179]
train() client id: f_00007-5-4 loss: 0.480868  [  160/  179]
train() client id: f_00007-6-0 loss: 0.460079  [   32/  179]
train() client id: f_00007-6-1 loss: 0.642951  [   64/  179]
train() client id: f_00007-6-2 loss: 0.837979  [   96/  179]
train() client id: f_00007-6-3 loss: 0.530093  [  128/  179]
train() client id: f_00007-6-4 loss: 0.501163  [  160/  179]
train() client id: f_00007-7-0 loss: 0.735358  [   32/  179]
train() client id: f_00007-7-1 loss: 0.621090  [   64/  179]
train() client id: f_00007-7-2 loss: 0.541375  [   96/  179]
train() client id: f_00007-7-3 loss: 0.465164  [  128/  179]
train() client id: f_00007-7-4 loss: 0.533586  [  160/  179]
train() client id: f_00007-8-0 loss: 0.446226  [   32/  179]
train() client id: f_00007-8-1 loss: 0.681548  [   64/  179]
train() client id: f_00007-8-2 loss: 0.777492  [   96/  179]
train() client id: f_00007-8-3 loss: 0.602874  [  128/  179]
train() client id: f_00007-8-4 loss: 0.488845  [  160/  179]
train() client id: f_00007-9-0 loss: 0.533058  [   32/  179]
train() client id: f_00007-9-1 loss: 0.691200  [   64/  179]
train() client id: f_00007-9-2 loss: 0.653111  [   96/  179]
train() client id: f_00007-9-3 loss: 0.424878  [  128/  179]
train() client id: f_00007-9-4 loss: 0.622392  [  160/  179]
train() client id: f_00007-10-0 loss: 0.631655  [   32/  179]
train() client id: f_00007-10-1 loss: 0.686624  [   64/  179]
train() client id: f_00007-10-2 loss: 0.428371  [   96/  179]
train() client id: f_00007-10-3 loss: 0.724611  [  128/  179]
train() client id: f_00007-10-4 loss: 0.579320  [  160/  179]
train() client id: f_00007-11-0 loss: 0.642363  [   32/  179]
train() client id: f_00007-11-1 loss: 0.543249  [   64/  179]
train() client id: f_00007-11-2 loss: 0.469729  [   96/  179]
train() client id: f_00007-11-3 loss: 0.549474  [  128/  179]
train() client id: f_00007-11-4 loss: 0.762824  [  160/  179]
train() client id: f_00007-12-0 loss: 0.703292  [   32/  179]
train() client id: f_00007-12-1 loss: 0.446942  [   64/  179]
train() client id: f_00007-12-2 loss: 0.536059  [   96/  179]
train() client id: f_00007-12-3 loss: 0.555513  [  128/  179]
train() client id: f_00007-12-4 loss: 0.720344  [  160/  179]
train() client id: f_00008-0-0 loss: 0.739558  [   32/  130]
train() client id: f_00008-0-1 loss: 0.740509  [   64/  130]
train() client id: f_00008-0-2 loss: 0.616563  [   96/  130]
train() client id: f_00008-0-3 loss: 0.786080  [  128/  130]
train() client id: f_00008-1-0 loss: 0.594554  [   32/  130]
train() client id: f_00008-1-1 loss: 0.767274  [   64/  130]
train() client id: f_00008-1-2 loss: 0.880704  [   96/  130]
train() client id: f_00008-1-3 loss: 0.662750  [  128/  130]
train() client id: f_00008-2-0 loss: 0.726085  [   32/  130]
train() client id: f_00008-2-1 loss: 0.682684  [   64/  130]
train() client id: f_00008-2-2 loss: 0.795207  [   96/  130]
train() client id: f_00008-2-3 loss: 0.700478  [  128/  130]
train() client id: f_00008-3-0 loss: 0.818244  [   32/  130]
train() client id: f_00008-3-1 loss: 0.754846  [   64/  130]
train() client id: f_00008-3-2 loss: 0.679760  [   96/  130]
train() client id: f_00008-3-3 loss: 0.633637  [  128/  130]
train() client id: f_00008-4-0 loss: 0.649831  [   32/  130]
train() client id: f_00008-4-1 loss: 0.711166  [   64/  130]
train() client id: f_00008-4-2 loss: 0.822257  [   96/  130]
train() client id: f_00008-4-3 loss: 0.730777  [  128/  130]
train() client id: f_00008-5-0 loss: 0.777984  [   32/  130]
train() client id: f_00008-5-1 loss: 0.720053  [   64/  130]
train() client id: f_00008-5-2 loss: 0.678112  [   96/  130]
train() client id: f_00008-5-3 loss: 0.727021  [  128/  130]
train() client id: f_00008-6-0 loss: 0.654245  [   32/  130]
train() client id: f_00008-6-1 loss: 0.753415  [   64/  130]
train() client id: f_00008-6-2 loss: 0.738643  [   96/  130]
train() client id: f_00008-6-3 loss: 0.688465  [  128/  130]
train() client id: f_00008-7-0 loss: 0.762444  [   32/  130]
train() client id: f_00008-7-1 loss: 0.699571  [   64/  130]
train() client id: f_00008-7-2 loss: 0.714902  [   96/  130]
train() client id: f_00008-7-3 loss: 0.724311  [  128/  130]
train() client id: f_00008-8-0 loss: 0.666990  [   32/  130]
train() client id: f_00008-8-1 loss: 0.732033  [   64/  130]
train() client id: f_00008-8-2 loss: 0.745572  [   96/  130]
train() client id: f_00008-8-3 loss: 0.751163  [  128/  130]
train() client id: f_00008-9-0 loss: 0.795371  [   32/  130]
train() client id: f_00008-9-1 loss: 0.813539  [   64/  130]
train() client id: f_00008-9-2 loss: 0.617102  [   96/  130]
train() client id: f_00008-9-3 loss: 0.682246  [  128/  130]
train() client id: f_00008-10-0 loss: 0.689919  [   32/  130]
train() client id: f_00008-10-1 loss: 0.694624  [   64/  130]
train() client id: f_00008-10-2 loss: 0.734685  [   96/  130]
train() client id: f_00008-10-3 loss: 0.768570  [  128/  130]
train() client id: f_00008-11-0 loss: 0.717963  [   32/  130]
train() client id: f_00008-11-1 loss: 0.785550  [   64/  130]
train() client id: f_00008-11-2 loss: 0.722047  [   96/  130]
train() client id: f_00008-11-3 loss: 0.694119  [  128/  130]
train() client id: f_00008-12-0 loss: 0.678798  [   32/  130]
train() client id: f_00008-12-1 loss: 0.744235  [   64/  130]
train() client id: f_00008-12-2 loss: 0.766500  [   96/  130]
train() client id: f_00008-12-3 loss: 0.725814  [  128/  130]
train() client id: f_00009-0-0 loss: 0.957457  [   32/  118]
train() client id: f_00009-0-1 loss: 1.059777  [   64/  118]
train() client id: f_00009-0-2 loss: 1.139039  [   96/  118]
train() client id: f_00009-1-0 loss: 0.935076  [   32/  118]
train() client id: f_00009-1-1 loss: 1.139183  [   64/  118]
train() client id: f_00009-1-2 loss: 0.911819  [   96/  118]
train() client id: f_00009-2-0 loss: 1.106152  [   32/  118]
train() client id: f_00009-2-1 loss: 0.932847  [   64/  118]
train() client id: f_00009-2-2 loss: 0.836296  [   96/  118]
train() client id: f_00009-3-0 loss: 0.897800  [   32/  118]
train() client id: f_00009-3-1 loss: 0.935597  [   64/  118]
train() client id: f_00009-3-2 loss: 0.790353  [   96/  118]
train() client id: f_00009-4-0 loss: 0.854168  [   32/  118]
train() client id: f_00009-4-1 loss: 0.752461  [   64/  118]
train() client id: f_00009-4-2 loss: 0.900396  [   96/  118]
train() client id: f_00009-5-0 loss: 0.955896  [   32/  118]
train() client id: f_00009-5-1 loss: 0.809821  [   64/  118]
train() client id: f_00009-5-2 loss: 0.616849  [   96/  118]
train() client id: f_00009-6-0 loss: 0.801122  [   32/  118]
train() client id: f_00009-6-1 loss: 0.873214  [   64/  118]
train() client id: f_00009-6-2 loss: 0.746550  [   96/  118]
train() client id: f_00009-7-0 loss: 0.800162  [   32/  118]
train() client id: f_00009-7-1 loss: 0.687726  [   64/  118]
train() client id: f_00009-7-2 loss: 0.850168  [   96/  118]
train() client id: f_00009-8-0 loss: 0.784268  [   32/  118]
train() client id: f_00009-8-1 loss: 0.807690  [   64/  118]
train() client id: f_00009-8-2 loss: 0.784367  [   96/  118]
train() client id: f_00009-9-0 loss: 0.809891  [   32/  118]
train() client id: f_00009-9-1 loss: 0.702086  [   64/  118]
train() client id: f_00009-9-2 loss: 0.752021  [   96/  118]
train() client id: f_00009-10-0 loss: 0.797882  [   32/  118]
train() client id: f_00009-10-1 loss: 0.643034  [   64/  118]
train() client id: f_00009-10-2 loss: 0.845463  [   96/  118]
train() client id: f_00009-11-0 loss: 0.578375  [   32/  118]
train() client id: f_00009-11-1 loss: 0.796551  [   64/  118]
train() client id: f_00009-11-2 loss: 0.904294  [   96/  118]
train() client id: f_00009-12-0 loss: 0.809794  [   32/  118]
train() client id: f_00009-12-1 loss: 0.765511  [   64/  118]
train() client id: f_00009-12-2 loss: 0.754229  [   96/  118]
At round 43 accuracy: 0.6472148541114059
At round 43 training accuracy: 0.590878604963112
At round 43 training loss: 0.826332576872051
update_location
xs = [  -3.9056584     4.20031788  235.00902392   18.81129433    0.97929623
    3.95640986 -197.44319194 -176.32485185  219.66397685 -162.06087855]
ys = [ 227.5879595   210.55583871    1.32061395 -197.45517586  189.35018685
  172.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [248.61925403 233.13387545 255.40357347 222.13151795 214.1365272
 199.70073579 221.33843901 202.709471   241.99366252 190.47241326]
dists_bs = [178.99376305 182.69365504 445.27736169 419.86387774 176.56341796
 178.96975881 179.03723612 174.18269057 424.90489817 171.48735842]
uav_gains = [7.62245662e-12 1.01432215e-11 6.66462561e-12 1.21951248e-11
 1.38244935e-11 1.71271443e-11 1.23513336e-11 1.63936996e-11
 8.64631835e-12 1.95639647e-11]
bs_gains = [5.43685774e-11 5.13414801e-11 4.23768198e-12 4.99562983e-12
 5.64900616e-11 5.43889979e-11 5.43316212e-11 5.86786482e-11
 4.83144700e-12 6.12976995e-11]
Round 44
-------------------------------
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.18624546 10.67895549  5.11317553  1.85111052 12.31468286  5.9249859
  2.28997512  7.27266655  5.37158258  4.80445503]
obj_prev = 60.80783503027712
eta_min = 1.6905775058448652e-18	eta_max = 0.9354514077356543
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 14.082134182864616	eta = 0.9090909090909091
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 27.112785949573404	eta = 0.4721735416659381
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 20.558519671827195	eta = 0.622707294620243
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.371299283019138	eta = 0.6608715285021035
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.305821021572196	eta = 0.6631129622477986
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.305603441902093	eta = 0.6631204357204616
eta = 0.6631204357204616
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [0.03389812 0.07129367 0.03336005 0.01156841 0.08232401 0.03927877
 0.01452777 0.04815685 0.03497425 0.03174586]
ene_total = [1.77121153 3.05330088 1.77028457 0.8430904  3.47724339 1.80411205
 0.95686313 2.2367863  1.88825588 1.50445532]
ti_comp = [0.59818702 0.64270661 0.59304455 0.61337277 0.644077   0.64353969
 0.6137322  0.62114382 0.5788735  0.64520803]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.80350798e-06 5.48286200e-05 6.59759497e-06 2.57188538e-07
 8.40589437e-05 9.14540875e-06 5.08766715e-07 1.80913087e-05
 7.97917407e-06 4.80332220e-06]
ene_total = [0.45223865 0.28084755 0.47224396 0.39288439 0.2766519  0.27582751
 0.39149537 0.36333529 0.52744819 0.26916572]
optimize_network_iter = 0 obj = 3.7021385292782734
eta = 0.6631204357204616
freqs = [28334048.79448884 55463616.087887   28126095.25989743  9430159.49291534
 63908512.8336163  30517755.77420575 11835590.3817379  38764656.20847187
 30208889.06833503 24601258.06337142]
eta_min = 0.6631204357204632	eta_max = 0.6660036163441293
af = 0.006161926359054621	bf = 1.2446435035998953	zeta = 0.006778118994960084	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [1.57849488e-06 1.27208928e-05 1.53072061e-06 5.96708039e-08
 1.95026760e-05 2.12184375e-06 1.18039937e-07 4.19739907e-06
 1.85126341e-06 1.11442796e-06]
ene_total = [1.67509603 1.03465718 1.74925201 1.45587345 1.015872   1.02111426
 1.45069832 1.34439885 1.95366795 0.99690874]
ti_comp = [0.59207349 0.63659309 0.58693102 0.60725925 0.63796347 0.63742617
 0.60761867 0.6150303  0.57275998 0.63909451]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.72701829e-06 5.41347322e-05 6.52458862e-06 2.54167082e-07
 8.29917378e-05 9.02944383e-06 5.02783784e-07 1.78742687e-05
 7.89490544e-06 4.74218027e-06]
ene_total = [0.45613954 0.28324469 0.47631768 0.39627579 0.27899817 0.278204
 0.39487466 0.36646322 0.53199801 0.27148686]
optimize_network_iter = 1 obj = 3.734002631064773
eta = 0.6660036163441293
freqs = [28324287.6160973  55404880.64581718 28118923.39438503  9424501.49550652
 63839529.8841224  30485058.77113441 11828419.4116198  38736518.87654436
 30208889.06833503 24574290.61426852]
eta_min = 0.6660036163441323	eta_max = 0.6660036163441293
af = 0.006150348470560822	bf = 1.2446435035998953	zeta = 0.006765383317616905	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [1.57740747e-06 1.26939645e-05 1.52994007e-06 5.95992217e-08
 1.94605963e-05 2.11729946e-06 1.17896944e-07 4.19130791e-06
 1.85126341e-06 1.11198606e-06]
ene_total = [1.67509587 1.0346533  1.74925189 1.45587344 1.01586593 1.0211136
 1.4506983  1.34439798 1.95366795 0.99690839]
ti_comp = [0.59207349 0.63659309 0.58693102 0.60725925 0.63796347 0.63742617
 0.60761867 0.6150303  0.57275998 0.63909451]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.72701829e-06 5.41347322e-05 6.52458862e-06 2.54167082e-07
 8.29917378e-05 9.02944383e-06 5.02783784e-07 1.78742687e-05
 7.89490544e-06 4.74218027e-06]
ene_total = [0.45613954 0.28324469 0.47631768 0.39627579 0.27899817 0.278204
 0.39487466 0.36646322 0.53199801 0.27148686]
optimize_network_iter = 2 obj = 3.734002631064773
eta = 0.6660036163441293
freqs = [28324287.6160973  55404880.64581718 28118923.39438503  9424501.49550652
 63839529.8841224  30485058.77113441 11828419.4116198  38736518.87654436
 30208889.06833503 24574290.61426852]
Done!
At round 44 eta: 0.6660036163441293
At round 44 local rounds: 13.309571309784454
At round 44 global rounds: 39.25367528264245
At round 44 a_n: 12.768039742635098
gradient difference: 0.407795250415802
train() client id: f_00000-0-0 loss: 1.033541  [   32/  126]
train() client id: f_00000-0-1 loss: 1.168745  [   64/  126]
train() client id: f_00000-0-2 loss: 1.208648  [   96/  126]
train() client id: f_00000-1-0 loss: 0.962239  [   32/  126]
train() client id: f_00000-1-1 loss: 0.947799  [   64/  126]
train() client id: f_00000-1-2 loss: 1.086410  [   96/  126]
train() client id: f_00000-2-0 loss: 0.877137  [   32/  126]
train() client id: f_00000-2-1 loss: 1.129562  [   64/  126]
train() client id: f_00000-2-2 loss: 0.994587  [   96/  126]
train() client id: f_00000-3-0 loss: 0.846683  [   32/  126]
train() client id: f_00000-3-1 loss: 1.002880  [   64/  126]
train() client id: f_00000-3-2 loss: 0.987130  [   96/  126]
train() client id: f_00000-4-0 loss: 0.944040  [   32/  126]
train() client id: f_00000-4-1 loss: 0.787061  [   64/  126]
train() client id: f_00000-4-2 loss: 0.907088  [   96/  126]
train() client id: f_00000-5-0 loss: 0.895374  [   32/  126]
train() client id: f_00000-5-1 loss: 0.832940  [   64/  126]
train() client id: f_00000-5-2 loss: 0.888702  [   96/  126]
train() client id: f_00000-6-0 loss: 0.893959  [   32/  126]
train() client id: f_00000-6-1 loss: 0.928305  [   64/  126]
train() client id: f_00000-6-2 loss: 0.795578  [   96/  126]
train() client id: f_00000-7-0 loss: 0.906792  [   32/  126]
train() client id: f_00000-7-1 loss: 0.863639  [   64/  126]
train() client id: f_00000-7-2 loss: 0.795039  [   96/  126]
train() client id: f_00000-8-0 loss: 0.951020  [   32/  126]
train() client id: f_00000-8-1 loss: 0.819542  [   64/  126]
train() client id: f_00000-8-2 loss: 0.748473  [   96/  126]
train() client id: f_00000-9-0 loss: 0.849298  [   32/  126]
train() client id: f_00000-9-1 loss: 0.849867  [   64/  126]
train() client id: f_00000-9-2 loss: 0.802915  [   96/  126]
train() client id: f_00000-10-0 loss: 0.873861  [   32/  126]
train() client id: f_00000-10-1 loss: 0.702102  [   64/  126]
train() client id: f_00000-10-2 loss: 0.868211  [   96/  126]
train() client id: f_00000-11-0 loss: 0.785067  [   32/  126]
train() client id: f_00000-11-1 loss: 0.787115  [   64/  126]
train() client id: f_00000-11-2 loss: 0.896835  [   96/  126]
train() client id: f_00000-12-0 loss: 0.867815  [   32/  126]
train() client id: f_00000-12-1 loss: 0.908612  [   64/  126]
train() client id: f_00000-12-2 loss: 0.804677  [   96/  126]
train() client id: f_00001-0-0 loss: 0.449768  [   32/  265]
train() client id: f_00001-0-1 loss: 0.327059  [   64/  265]
train() client id: f_00001-0-2 loss: 0.280269  [   96/  265]
train() client id: f_00001-0-3 loss: 0.506357  [  128/  265]
train() client id: f_00001-0-4 loss: 0.439956  [  160/  265]
train() client id: f_00001-0-5 loss: 0.379798  [  192/  265]
train() client id: f_00001-0-6 loss: 0.334502  [  224/  265]
train() client id: f_00001-0-7 loss: 0.286688  [  256/  265]
train() client id: f_00001-1-0 loss: 0.468792  [   32/  265]
train() client id: f_00001-1-1 loss: 0.442017  [   64/  265]
train() client id: f_00001-1-2 loss: 0.319941  [   96/  265]
train() client id: f_00001-1-3 loss: 0.292660  [  128/  265]
train() client id: f_00001-1-4 loss: 0.467855  [  160/  265]
train() client id: f_00001-1-5 loss: 0.296009  [  192/  265]
train() client id: f_00001-1-6 loss: 0.321486  [  224/  265]
train() client id: f_00001-1-7 loss: 0.351739  [  256/  265]
train() client id: f_00001-2-0 loss: 0.429842  [   32/  265]
train() client id: f_00001-2-1 loss: 0.444902  [   64/  265]
train() client id: f_00001-2-2 loss: 0.282672  [   96/  265]
train() client id: f_00001-2-3 loss: 0.320993  [  128/  265]
train() client id: f_00001-2-4 loss: 0.317000  [  160/  265]
train() client id: f_00001-2-5 loss: 0.449139  [  192/  265]
train() client id: f_00001-2-6 loss: 0.262851  [  224/  265]
train() client id: f_00001-2-7 loss: 0.387059  [  256/  265]
train() client id: f_00001-3-0 loss: 0.274622  [   32/  265]
train() client id: f_00001-3-1 loss: 0.448094  [   64/  265]
train() client id: f_00001-3-2 loss: 0.460056  [   96/  265]
train() client id: f_00001-3-3 loss: 0.464357  [  128/  265]
train() client id: f_00001-3-4 loss: 0.262652  [  160/  265]
train() client id: f_00001-3-5 loss: 0.324475  [  192/  265]
train() client id: f_00001-3-6 loss: 0.267411  [  224/  265]
train() client id: f_00001-3-7 loss: 0.351030  [  256/  265]
train() client id: f_00001-4-0 loss: 0.256881  [   32/  265]
train() client id: f_00001-4-1 loss: 0.328891  [   64/  265]
train() client id: f_00001-4-2 loss: 0.337069  [   96/  265]
train() client id: f_00001-4-3 loss: 0.349814  [  128/  265]
train() client id: f_00001-4-4 loss: 0.300161  [  160/  265]
train() client id: f_00001-4-5 loss: 0.377827  [  192/  265]
train() client id: f_00001-4-6 loss: 0.330181  [  224/  265]
train() client id: f_00001-4-7 loss: 0.345340  [  256/  265]
train() client id: f_00001-5-0 loss: 0.230220  [   32/  265]
train() client id: f_00001-5-1 loss: 0.348207  [   64/  265]
train() client id: f_00001-5-2 loss: 0.403936  [   96/  265]
train() client id: f_00001-5-3 loss: 0.362460  [  128/  265]
train() client id: f_00001-5-4 loss: 0.567844  [  160/  265]
train() client id: f_00001-5-5 loss: 0.252040  [  192/  265]
train() client id: f_00001-5-6 loss: 0.295203  [  224/  265]
train() client id: f_00001-5-7 loss: 0.293059  [  256/  265]
train() client id: f_00001-6-0 loss: 0.337282  [   32/  265]
train() client id: f_00001-6-1 loss: 0.400792  [   64/  265]
train() client id: f_00001-6-2 loss: 0.318928  [   96/  265]
train() client id: f_00001-6-3 loss: 0.330037  [  128/  265]
train() client id: f_00001-6-4 loss: 0.329596  [  160/  265]
train() client id: f_00001-6-5 loss: 0.265847  [  192/  265]
train() client id: f_00001-6-6 loss: 0.317086  [  224/  265]
train() client id: f_00001-6-7 loss: 0.266479  [  256/  265]
train() client id: f_00001-7-0 loss: 0.466234  [   32/  265]
train() client id: f_00001-7-1 loss: 0.251446  [   64/  265]
train() client id: f_00001-7-2 loss: 0.450539  [   96/  265]
train() client id: f_00001-7-3 loss: 0.351224  [  128/  265]
train() client id: f_00001-7-4 loss: 0.395680  [  160/  265]
train() client id: f_00001-7-5 loss: 0.246841  [  192/  265]
train() client id: f_00001-7-6 loss: 0.257305  [  224/  265]
train() client id: f_00001-7-7 loss: 0.264741  [  256/  265]
train() client id: f_00001-8-0 loss: 0.326334  [   32/  265]
train() client id: f_00001-8-1 loss: 0.378825  [   64/  265]
train() client id: f_00001-8-2 loss: 0.499513  [   96/  265]
train() client id: f_00001-8-3 loss: 0.361304  [  128/  265]
train() client id: f_00001-8-4 loss: 0.238189  [  160/  265]
train() client id: f_00001-8-5 loss: 0.271978  [  192/  265]
train() client id: f_00001-8-6 loss: 0.243372  [  224/  265]
train() client id: f_00001-8-7 loss: 0.357309  [  256/  265]
train() client id: f_00001-9-0 loss: 0.354826  [   32/  265]
train() client id: f_00001-9-1 loss: 0.374199  [   64/  265]
train() client id: f_00001-9-2 loss: 0.371235  [   96/  265]
train() client id: f_00001-9-3 loss: 0.290740  [  128/  265]
train() client id: f_00001-9-4 loss: 0.312679  [  160/  265]
train() client id: f_00001-9-5 loss: 0.234914  [  192/  265]
train() client id: f_00001-9-6 loss: 0.308576  [  224/  265]
train() client id: f_00001-9-7 loss: 0.399100  [  256/  265]
train() client id: f_00001-10-0 loss: 0.429952  [   32/  265]
train() client id: f_00001-10-1 loss: 0.402975  [   64/  265]
train() client id: f_00001-10-2 loss: 0.367623  [   96/  265]
train() client id: f_00001-10-3 loss: 0.348460  [  128/  265]
train() client id: f_00001-10-4 loss: 0.252719  [  160/  265]
train() client id: f_00001-10-5 loss: 0.293040  [  192/  265]
train() client id: f_00001-10-6 loss: 0.210032  [  224/  265]
train() client id: f_00001-10-7 loss: 0.264834  [  256/  265]
train() client id: f_00001-11-0 loss: 0.222670  [   32/  265]
train() client id: f_00001-11-1 loss: 0.286345  [   64/  265]
train() client id: f_00001-11-2 loss: 0.415835  [   96/  265]
train() client id: f_00001-11-3 loss: 0.297399  [  128/  265]
train() client id: f_00001-11-4 loss: 0.300909  [  160/  265]
train() client id: f_00001-11-5 loss: 0.324903  [  192/  265]
train() client id: f_00001-11-6 loss: 0.399156  [  224/  265]
train() client id: f_00001-11-7 loss: 0.297833  [  256/  265]
train() client id: f_00001-12-0 loss: 0.559196  [   32/  265]
train() client id: f_00001-12-1 loss: 0.269519  [   64/  265]
train() client id: f_00001-12-2 loss: 0.415205  [   96/  265]
train() client id: f_00001-12-3 loss: 0.216848  [  128/  265]
train() client id: f_00001-12-4 loss: 0.222697  [  160/  265]
train() client id: f_00001-12-5 loss: 0.254230  [  192/  265]
train() client id: f_00001-12-6 loss: 0.311500  [  224/  265]
train() client id: f_00001-12-7 loss: 0.383710  [  256/  265]
train() client id: f_00002-0-0 loss: 1.218319  [   32/  124]
train() client id: f_00002-0-1 loss: 1.273759  [   64/  124]
train() client id: f_00002-0-2 loss: 1.143709  [   96/  124]
train() client id: f_00002-1-0 loss: 1.197601  [   32/  124]
train() client id: f_00002-1-1 loss: 1.011326  [   64/  124]
train() client id: f_00002-1-2 loss: 1.119910  [   96/  124]
train() client id: f_00002-2-0 loss: 1.105185  [   32/  124]
train() client id: f_00002-2-1 loss: 1.048602  [   64/  124]
train() client id: f_00002-2-2 loss: 1.062741  [   96/  124]
train() client id: f_00002-3-0 loss: 1.026466  [   32/  124]
train() client id: f_00002-3-1 loss: 0.874242  [   64/  124]
train() client id: f_00002-3-2 loss: 1.149474  [   96/  124]
train() client id: f_00002-4-0 loss: 1.019436  [   32/  124]
train() client id: f_00002-4-1 loss: 0.892921  [   64/  124]
train() client id: f_00002-4-2 loss: 1.036324  [   96/  124]
train() client id: f_00002-5-0 loss: 1.052807  [   32/  124]
train() client id: f_00002-5-1 loss: 0.951402  [   64/  124]
train() client id: f_00002-5-2 loss: 0.845616  [   96/  124]
train() client id: f_00002-6-0 loss: 0.984937  [   32/  124]
train() client id: f_00002-6-1 loss: 0.798163  [   64/  124]
train() client id: f_00002-6-2 loss: 1.005604  [   96/  124]
train() client id: f_00002-7-0 loss: 0.857542  [   32/  124]
train() client id: f_00002-7-1 loss: 0.950189  [   64/  124]
train() client id: f_00002-7-2 loss: 1.153315  [   96/  124]
train() client id: f_00002-8-0 loss: 0.842784  [   32/  124]
train() client id: f_00002-8-1 loss: 0.843734  [   64/  124]
train() client id: f_00002-8-2 loss: 1.020299  [   96/  124]
train() client id: f_00002-9-0 loss: 1.074211  [   32/  124]
train() client id: f_00002-9-1 loss: 1.023286  [   64/  124]
train() client id: f_00002-9-2 loss: 0.832391  [   96/  124]
train() client id: f_00002-10-0 loss: 0.948021  [   32/  124]
train() client id: f_00002-10-1 loss: 1.078229  [   64/  124]
train() client id: f_00002-10-2 loss: 0.919443  [   96/  124]
train() client id: f_00002-11-0 loss: 0.848240  [   32/  124]
train() client id: f_00002-11-1 loss: 0.890095  [   64/  124]
train() client id: f_00002-11-2 loss: 0.925681  [   96/  124]
train() client id: f_00002-12-0 loss: 0.768725  [   32/  124]
train() client id: f_00002-12-1 loss: 0.936472  [   64/  124]
train() client id: f_00002-12-2 loss: 1.162322  [   96/  124]
train() client id: f_00003-0-0 loss: 0.793957  [   32/   43]
train() client id: f_00003-1-0 loss: 0.675710  [   32/   43]
train() client id: f_00003-2-0 loss: 0.703641  [   32/   43]
train() client id: f_00003-3-0 loss: 0.855427  [   32/   43]
train() client id: f_00003-4-0 loss: 0.784921  [   32/   43]
train() client id: f_00003-5-0 loss: 0.767314  [   32/   43]
train() client id: f_00003-6-0 loss: 0.747842  [   32/   43]
train() client id: f_00003-7-0 loss: 0.628583  [   32/   43]
train() client id: f_00003-8-0 loss: 0.809445  [   32/   43]
train() client id: f_00003-9-0 loss: 0.613022  [   32/   43]
train() client id: f_00003-10-0 loss: 0.727015  [   32/   43]
train() client id: f_00003-11-0 loss: 0.743949  [   32/   43]
train() client id: f_00003-12-0 loss: 0.793225  [   32/   43]
train() client id: f_00004-0-0 loss: 0.710824  [   32/  306]
train() client id: f_00004-0-1 loss: 0.768450  [   64/  306]
train() client id: f_00004-0-2 loss: 0.894642  [   96/  306]
train() client id: f_00004-0-3 loss: 0.927473  [  128/  306]
train() client id: f_00004-0-4 loss: 0.865043  [  160/  306]
train() client id: f_00004-0-5 loss: 0.762637  [  192/  306]
train() client id: f_00004-0-6 loss: 0.829530  [  224/  306]
train() client id: f_00004-0-7 loss: 1.019029  [  256/  306]
train() client id: f_00004-0-8 loss: 0.803338  [  288/  306]
train() client id: f_00004-1-0 loss: 0.770844  [   32/  306]
train() client id: f_00004-1-1 loss: 0.785112  [   64/  306]
train() client id: f_00004-1-2 loss: 0.851346  [   96/  306]
train() client id: f_00004-1-3 loss: 0.852735  [  128/  306]
train() client id: f_00004-1-4 loss: 0.873539  [  160/  306]
train() client id: f_00004-1-5 loss: 0.819891  [  192/  306]
train() client id: f_00004-1-6 loss: 0.948266  [  224/  306]
train() client id: f_00004-1-7 loss: 0.873604  [  256/  306]
train() client id: f_00004-1-8 loss: 0.806875  [  288/  306]
train() client id: f_00004-2-0 loss: 0.876682  [   32/  306]
train() client id: f_00004-2-1 loss: 0.670180  [   64/  306]
train() client id: f_00004-2-2 loss: 0.856164  [   96/  306]
train() client id: f_00004-2-3 loss: 0.989169  [  128/  306]
train() client id: f_00004-2-4 loss: 1.016437  [  160/  306]
train() client id: f_00004-2-5 loss: 0.743692  [  192/  306]
train() client id: f_00004-2-6 loss: 0.804214  [  224/  306]
train() client id: f_00004-2-7 loss: 0.807929  [  256/  306]
train() client id: f_00004-2-8 loss: 0.884727  [  288/  306]
train() client id: f_00004-3-0 loss: 0.953797  [   32/  306]
train() client id: f_00004-3-1 loss: 0.767612  [   64/  306]
train() client id: f_00004-3-2 loss: 0.978827  [   96/  306]
train() client id: f_00004-3-3 loss: 0.870376  [  128/  306]
train() client id: f_00004-3-4 loss: 0.842935  [  160/  306]
train() client id: f_00004-3-5 loss: 0.797907  [  192/  306]
train() client id: f_00004-3-6 loss: 0.830591  [  224/  306]
train() client id: f_00004-3-7 loss: 0.778308  [  256/  306]
train() client id: f_00004-3-8 loss: 0.731860  [  288/  306]
train() client id: f_00004-4-0 loss: 0.766033  [   32/  306]
train() client id: f_00004-4-1 loss: 0.758851  [   64/  306]
train() client id: f_00004-4-2 loss: 0.623731  [   96/  306]
train() client id: f_00004-4-3 loss: 0.895055  [  128/  306]
train() client id: f_00004-4-4 loss: 0.959801  [  160/  306]
train() client id: f_00004-4-5 loss: 0.902413  [  192/  306]
train() client id: f_00004-4-6 loss: 0.812392  [  224/  306]
train() client id: f_00004-4-7 loss: 0.834398  [  256/  306]
train() client id: f_00004-4-8 loss: 0.849546  [  288/  306]
train() client id: f_00004-5-0 loss: 0.856932  [   32/  306]
train() client id: f_00004-5-1 loss: 0.802312  [   64/  306]
train() client id: f_00004-5-2 loss: 0.787953  [   96/  306]
train() client id: f_00004-5-3 loss: 0.877790  [  128/  306]
train() client id: f_00004-5-4 loss: 0.792234  [  160/  306]
train() client id: f_00004-5-5 loss: 0.916424  [  192/  306]
train() client id: f_00004-5-6 loss: 0.861891  [  224/  306]
train() client id: f_00004-5-7 loss: 0.857056  [  256/  306]
train() client id: f_00004-5-8 loss: 0.811451  [  288/  306]
train() client id: f_00004-6-0 loss: 0.806903  [   32/  306]
train() client id: f_00004-6-1 loss: 0.747526  [   64/  306]
train() client id: f_00004-6-2 loss: 0.747061  [   96/  306]
train() client id: f_00004-6-3 loss: 0.856004  [  128/  306]
train() client id: f_00004-6-4 loss: 0.837968  [  160/  306]
train() client id: f_00004-6-5 loss: 0.683235  [  192/  306]
train() client id: f_00004-6-6 loss: 0.834579  [  224/  306]
train() client id: f_00004-6-7 loss: 0.884006  [  256/  306]
train() client id: f_00004-6-8 loss: 0.954676  [  288/  306]
train() client id: f_00004-7-0 loss: 0.907163  [   32/  306]
train() client id: f_00004-7-1 loss: 0.706598  [   64/  306]
train() client id: f_00004-7-2 loss: 0.833494  [   96/  306]
train() client id: f_00004-7-3 loss: 0.824561  [  128/  306]
train() client id: f_00004-7-4 loss: 0.789124  [  160/  306]
train() client id: f_00004-7-5 loss: 0.831192  [  192/  306]
train() client id: f_00004-7-6 loss: 0.905914  [  224/  306]
train() client id: f_00004-7-7 loss: 0.887248  [  256/  306]
train() client id: f_00004-7-8 loss: 0.792506  [  288/  306]
train() client id: f_00004-8-0 loss: 0.806612  [   32/  306]
train() client id: f_00004-8-1 loss: 0.957148  [   64/  306]
train() client id: f_00004-8-2 loss: 0.761237  [   96/  306]
train() client id: f_00004-8-3 loss: 0.829559  [  128/  306]
train() client id: f_00004-8-4 loss: 0.764796  [  160/  306]
train() client id: f_00004-8-5 loss: 0.890546  [  192/  306]
train() client id: f_00004-8-6 loss: 0.751518  [  224/  306]
train() client id: f_00004-8-7 loss: 0.843750  [  256/  306]
train() client id: f_00004-8-8 loss: 0.873565  [  288/  306]
train() client id: f_00004-9-0 loss: 0.843053  [   32/  306]
train() client id: f_00004-9-1 loss: 0.952304  [   64/  306]
train() client id: f_00004-9-2 loss: 0.823115  [   96/  306]
train() client id: f_00004-9-3 loss: 0.763759  [  128/  306]
train() client id: f_00004-9-4 loss: 0.785508  [  160/  306]
train() client id: f_00004-9-5 loss: 0.752623  [  192/  306]
train() client id: f_00004-9-6 loss: 0.775911  [  224/  306]
train() client id: f_00004-9-7 loss: 0.828366  [  256/  306]
train() client id: f_00004-9-8 loss: 0.894143  [  288/  306]
train() client id: f_00004-10-0 loss: 0.702919  [   32/  306]
train() client id: f_00004-10-1 loss: 0.820253  [   64/  306]
train() client id: f_00004-10-2 loss: 0.796028  [   96/  306]
train() client id: f_00004-10-3 loss: 0.662355  [  128/  306]
train() client id: f_00004-10-4 loss: 0.838770  [  160/  306]
train() client id: f_00004-10-5 loss: 0.853384  [  192/  306]
train() client id: f_00004-10-6 loss: 0.924880  [  224/  306]
train() client id: f_00004-10-7 loss: 0.941627  [  256/  306]
train() client id: f_00004-10-8 loss: 0.851147  [  288/  306]
train() client id: f_00004-11-0 loss: 0.662400  [   32/  306]
train() client id: f_00004-11-1 loss: 0.929299  [   64/  306]
train() client id: f_00004-11-2 loss: 0.903951  [   96/  306]
train() client id: f_00004-11-3 loss: 0.744146  [  128/  306]
train() client id: f_00004-11-4 loss: 0.799441  [  160/  306]
train() client id: f_00004-11-5 loss: 0.852973  [  192/  306]
train() client id: f_00004-11-6 loss: 0.827841  [  224/  306]
train() client id: f_00004-11-7 loss: 0.836664  [  256/  306]
train() client id: f_00004-11-8 loss: 0.843958  [  288/  306]
train() client id: f_00004-12-0 loss: 0.762078  [   32/  306]
train() client id: f_00004-12-1 loss: 0.757758  [   64/  306]
train() client id: f_00004-12-2 loss: 0.971235  [   96/  306]
train() client id: f_00004-12-3 loss: 0.829164  [  128/  306]
train() client id: f_00004-12-4 loss: 0.817111  [  160/  306]
train() client id: f_00004-12-5 loss: 0.850463  [  192/  306]
train() client id: f_00004-12-6 loss: 0.715918  [  224/  306]
train() client id: f_00004-12-7 loss: 0.791895  [  256/  306]
train() client id: f_00004-12-8 loss: 0.884433  [  288/  306]
train() client id: f_00005-0-0 loss: 0.638812  [   32/  146]
train() client id: f_00005-0-1 loss: 0.803499  [   64/  146]
train() client id: f_00005-0-2 loss: 0.740649  [   96/  146]
train() client id: f_00005-0-3 loss: 0.609663  [  128/  146]
train() client id: f_00005-1-0 loss: 0.706212  [   32/  146]
train() client id: f_00005-1-1 loss: 0.593607  [   64/  146]
train() client id: f_00005-1-2 loss: 0.863959  [   96/  146]
train() client id: f_00005-1-3 loss: 0.721363  [  128/  146]
train() client id: f_00005-2-0 loss: 0.734515  [   32/  146]
train() client id: f_00005-2-1 loss: 0.644109  [   64/  146]
train() client id: f_00005-2-2 loss: 0.681052  [   96/  146]
train() client id: f_00005-2-3 loss: 0.639023  [  128/  146]
train() client id: f_00005-3-0 loss: 0.965370  [   32/  146]
train() client id: f_00005-3-1 loss: 0.691205  [   64/  146]
train() client id: f_00005-3-2 loss: 0.584858  [   96/  146]
train() client id: f_00005-3-3 loss: 0.599291  [  128/  146]
train() client id: f_00005-4-0 loss: 0.616069  [   32/  146]
train() client id: f_00005-4-1 loss: 0.790926  [   64/  146]
train() client id: f_00005-4-2 loss: 0.701398  [   96/  146]
train() client id: f_00005-4-3 loss: 0.715580  [  128/  146]
train() client id: f_00005-5-0 loss: 0.895655  [   32/  146]
train() client id: f_00005-5-1 loss: 0.588876  [   64/  146]
train() client id: f_00005-5-2 loss: 0.663362  [   96/  146]
train() client id: f_00005-5-3 loss: 0.668202  [  128/  146]
train() client id: f_00005-6-0 loss: 0.861624  [   32/  146]
train() client id: f_00005-6-1 loss: 0.752620  [   64/  146]
train() client id: f_00005-6-2 loss: 0.592983  [   96/  146]
train() client id: f_00005-6-3 loss: 0.636415  [  128/  146]
train() client id: f_00005-7-0 loss: 0.812403  [   32/  146]
train() client id: f_00005-7-1 loss: 0.705715  [   64/  146]
train() client id: f_00005-7-2 loss: 0.543481  [   96/  146]
train() client id: f_00005-7-3 loss: 0.513034  [  128/  146]
train() client id: f_00005-8-0 loss: 0.682032  [   32/  146]
train() client id: f_00005-8-1 loss: 0.658544  [   64/  146]
train() client id: f_00005-8-2 loss: 0.712092  [   96/  146]
train() client id: f_00005-8-3 loss: 0.665560  [  128/  146]
train() client id: f_00005-9-0 loss: 0.859937  [   32/  146]
train() client id: f_00005-9-1 loss: 0.491705  [   64/  146]
train() client id: f_00005-9-2 loss: 0.561438  [   96/  146]
train() client id: f_00005-9-3 loss: 0.703400  [  128/  146]
train() client id: f_00005-10-0 loss: 0.631517  [   32/  146]
train() client id: f_00005-10-1 loss: 0.524394  [   64/  146]
train() client id: f_00005-10-2 loss: 0.861959  [   96/  146]
train() client id: f_00005-10-3 loss: 0.719755  [  128/  146]
train() client id: f_00005-11-0 loss: 0.961974  [   32/  146]
train() client id: f_00005-11-1 loss: 0.409608  [   64/  146]
train() client id: f_00005-11-2 loss: 0.654010  [   96/  146]
train() client id: f_00005-11-3 loss: 0.532442  [  128/  146]
train() client id: f_00005-12-0 loss: 0.474117  [   32/  146]
train() client id: f_00005-12-1 loss: 0.803395  [   64/  146]
train() client id: f_00005-12-2 loss: 0.964712  [   96/  146]
train() client id: f_00005-12-3 loss: 0.601255  [  128/  146]
train() client id: f_00006-0-0 loss: 0.502248  [   32/   54]
train() client id: f_00006-1-0 loss: 0.525362  [   32/   54]
train() client id: f_00006-2-0 loss: 0.494358  [   32/   54]
train() client id: f_00006-3-0 loss: 0.532796  [   32/   54]
train() client id: f_00006-4-0 loss: 0.565860  [   32/   54]
train() client id: f_00006-5-0 loss: 0.547738  [   32/   54]
train() client id: f_00006-6-0 loss: 0.518486  [   32/   54]
train() client id: f_00006-7-0 loss: 0.453600  [   32/   54]
train() client id: f_00006-8-0 loss: 0.521572  [   32/   54]
train() client id: f_00006-9-0 loss: 0.529794  [   32/   54]
train() client id: f_00006-10-0 loss: 0.518412  [   32/   54]
train() client id: f_00006-11-0 loss: 0.568670  [   32/   54]
train() client id: f_00006-12-0 loss: 0.508235  [   32/   54]
train() client id: f_00007-0-0 loss: 0.337925  [   32/  179]
train() client id: f_00007-0-1 loss: 0.564770  [   64/  179]
train() client id: f_00007-0-2 loss: 0.473427  [   96/  179]
train() client id: f_00007-0-3 loss: 0.388082  [  128/  179]
train() client id: f_00007-0-4 loss: 0.382352  [  160/  179]
train() client id: f_00007-1-0 loss: 0.329343  [   32/  179]
train() client id: f_00007-1-1 loss: 0.275615  [   64/  179]
train() client id: f_00007-1-2 loss: 0.669490  [   96/  179]
train() client id: f_00007-1-3 loss: 0.307383  [  128/  179]
train() client id: f_00007-1-4 loss: 0.623298  [  160/  179]
train() client id: f_00007-2-0 loss: 0.328713  [   32/  179]
train() client id: f_00007-2-1 loss: 0.315224  [   64/  179]
train() client id: f_00007-2-2 loss: 0.567355  [   96/  179]
train() client id: f_00007-2-3 loss: 0.492991  [  128/  179]
train() client id: f_00007-2-4 loss: 0.533161  [  160/  179]
train() client id: f_00007-3-0 loss: 0.367111  [   32/  179]
train() client id: f_00007-3-1 loss: 0.306089  [   64/  179]
train() client id: f_00007-3-2 loss: 0.502192  [   96/  179]
train() client id: f_00007-3-3 loss: 0.557818  [  128/  179]
train() client id: f_00007-3-4 loss: 0.368428  [  160/  179]
train() client id: f_00007-4-0 loss: 0.326687  [   32/  179]
train() client id: f_00007-4-1 loss: 0.418460  [   64/  179]
train() client id: f_00007-4-2 loss: 0.657472  [   96/  179]
train() client id: f_00007-4-3 loss: 0.352163  [  128/  179]
train() client id: f_00007-4-4 loss: 0.321708  [  160/  179]
train() client id: f_00007-5-0 loss: 0.514890  [   32/  179]
train() client id: f_00007-5-1 loss: 0.580282  [   64/  179]
train() client id: f_00007-5-2 loss: 0.293321  [   96/  179]
train() client id: f_00007-5-3 loss: 0.347754  [  128/  179]
train() client id: f_00007-5-4 loss: 0.279983  [  160/  179]
train() client id: f_00007-6-0 loss: 0.496306  [   32/  179]
train() client id: f_00007-6-1 loss: 0.483212  [   64/  179]
train() client id: f_00007-6-2 loss: 0.447336  [   96/  179]
train() client id: f_00007-6-3 loss: 0.417712  [  128/  179]
train() client id: f_00007-6-4 loss: 0.256615  [  160/  179]
train() client id: f_00007-7-0 loss: 0.349871  [   32/  179]
train() client id: f_00007-7-1 loss: 0.439076  [   64/  179]
train() client id: f_00007-7-2 loss: 0.358299  [   96/  179]
train() client id: f_00007-7-3 loss: 0.224601  [  128/  179]
train() client id: f_00007-7-4 loss: 0.621855  [  160/  179]
train() client id: f_00007-8-0 loss: 0.326389  [   32/  179]
train() client id: f_00007-8-1 loss: 0.627781  [   64/  179]
train() client id: f_00007-8-2 loss: 0.335774  [   96/  179]
train() client id: f_00007-8-3 loss: 0.423158  [  128/  179]
train() client id: f_00007-8-4 loss: 0.334223  [  160/  179]
train() client id: f_00007-9-0 loss: 0.338905  [   32/  179]
train() client id: f_00007-9-1 loss: 0.310165  [   64/  179]
train() client id: f_00007-9-2 loss: 0.634854  [   96/  179]
train() client id: f_00007-9-3 loss: 0.474818  [  128/  179]
train() client id: f_00007-9-4 loss: 0.296763  [  160/  179]
train() client id: f_00007-10-0 loss: 0.223965  [   32/  179]
train() client id: f_00007-10-1 loss: 0.629334  [   64/  179]
train() client id: f_00007-10-2 loss: 0.406880  [   96/  179]
train() client id: f_00007-10-3 loss: 0.311523  [  128/  179]
train() client id: f_00007-10-4 loss: 0.423185  [  160/  179]
train() client id: f_00007-11-0 loss: 0.333301  [   32/  179]
train() client id: f_00007-11-1 loss: 0.582103  [   64/  179]
train() client id: f_00007-11-2 loss: 0.429365  [   96/  179]
train() client id: f_00007-11-3 loss: 0.350471  [  128/  179]
train() client id: f_00007-11-4 loss: 0.266288  [  160/  179]
train() client id: f_00007-12-0 loss: 0.396665  [   32/  179]
train() client id: f_00007-12-1 loss: 0.406407  [   64/  179]
train() client id: f_00007-12-2 loss: 0.546853  [   96/  179]
train() client id: f_00007-12-3 loss: 0.264669  [  128/  179]
train() client id: f_00007-12-4 loss: 0.362040  [  160/  179]
train() client id: f_00008-0-0 loss: 0.635362  [   32/  130]
train() client id: f_00008-0-1 loss: 0.752775  [   64/  130]
train() client id: f_00008-0-2 loss: 0.760953  [   96/  130]
train() client id: f_00008-0-3 loss: 0.746968  [  128/  130]
train() client id: f_00008-1-0 loss: 0.754646  [   32/  130]
train() client id: f_00008-1-1 loss: 0.736001  [   64/  130]
train() client id: f_00008-1-2 loss: 0.807625  [   96/  130]
train() client id: f_00008-1-3 loss: 0.640899  [  128/  130]
train() client id: f_00008-2-0 loss: 0.674181  [   32/  130]
train() client id: f_00008-2-1 loss: 0.719965  [   64/  130]
train() client id: f_00008-2-2 loss: 0.784683  [   96/  130]
train() client id: f_00008-2-3 loss: 0.745360  [  128/  130]
train() client id: f_00008-3-0 loss: 0.684819  [   32/  130]
train() client id: f_00008-3-1 loss: 0.754322  [   64/  130]
train() client id: f_00008-3-2 loss: 0.773917  [   96/  130]
train() client id: f_00008-3-3 loss: 0.677330  [  128/  130]
train() client id: f_00008-4-0 loss: 0.790303  [   32/  130]
train() client id: f_00008-4-1 loss: 0.675333  [   64/  130]
train() client id: f_00008-4-2 loss: 0.713169  [   96/  130]
train() client id: f_00008-4-3 loss: 0.724873  [  128/  130]
train() client id: f_00008-5-0 loss: 0.819522  [   32/  130]
train() client id: f_00008-5-1 loss: 0.619420  [   64/  130]
train() client id: f_00008-5-2 loss: 0.692256  [   96/  130]
train() client id: f_00008-5-3 loss: 0.806755  [  128/  130]
train() client id: f_00008-6-0 loss: 0.680027  [   32/  130]
train() client id: f_00008-6-1 loss: 0.734463  [   64/  130]
train() client id: f_00008-6-2 loss: 0.791889  [   96/  130]
train() client id: f_00008-6-3 loss: 0.718578  [  128/  130]
train() client id: f_00008-7-0 loss: 0.756783  [   32/  130]
train() client id: f_00008-7-1 loss: 0.732392  [   64/  130]
train() client id: f_00008-7-2 loss: 0.707763  [   96/  130]
train() client id: f_00008-7-3 loss: 0.755296  [  128/  130]
train() client id: f_00008-8-0 loss: 0.719812  [   32/  130]
train() client id: f_00008-8-1 loss: 0.677528  [   64/  130]
train() client id: f_00008-8-2 loss: 0.840032  [   96/  130]
train() client id: f_00008-8-3 loss: 0.699186  [  128/  130]
train() client id: f_00008-9-0 loss: 0.735422  [   32/  130]
train() client id: f_00008-9-1 loss: 0.816269  [   64/  130]
train() client id: f_00008-9-2 loss: 0.679495  [   96/  130]
train() client id: f_00008-9-3 loss: 0.711303  [  128/  130]
train() client id: f_00008-10-0 loss: 0.629283  [   32/  130]
train() client id: f_00008-10-1 loss: 0.828070  [   64/  130]
train() client id: f_00008-10-2 loss: 0.728757  [   96/  130]
train() client id: f_00008-10-3 loss: 0.759368  [  128/  130]
train() client id: f_00008-11-0 loss: 0.575785  [   32/  130]
train() client id: f_00008-11-1 loss: 0.818384  [   64/  130]
train() client id: f_00008-11-2 loss: 0.810454  [   96/  130]
train() client id: f_00008-11-3 loss: 0.736992  [  128/  130]
train() client id: f_00008-12-0 loss: 0.709994  [   32/  130]
train() client id: f_00008-12-1 loss: 0.764959  [   64/  130]
train() client id: f_00008-12-2 loss: 0.752699  [   96/  130]
train() client id: f_00008-12-3 loss: 0.728442  [  128/  130]
train() client id: f_00009-0-0 loss: 1.049164  [   32/  118]
train() client id: f_00009-0-1 loss: 1.102695  [   64/  118]
train() client id: f_00009-0-2 loss: 0.998876  [   96/  118]
train() client id: f_00009-1-0 loss: 0.909982  [   32/  118]
train() client id: f_00009-1-1 loss: 1.044231  [   64/  118]
train() client id: f_00009-1-2 loss: 1.084552  [   96/  118]
train() client id: f_00009-2-0 loss: 0.931244  [   32/  118]
train() client id: f_00009-2-1 loss: 0.906713  [   64/  118]
train() client id: f_00009-2-2 loss: 0.909346  [   96/  118]
train() client id: f_00009-3-0 loss: 0.947315  [   32/  118]
train() client id: f_00009-3-1 loss: 0.976599  [   64/  118]
train() client id: f_00009-3-2 loss: 1.004913  [   96/  118]
train() client id: f_00009-4-0 loss: 0.784716  [   32/  118]
train() client id: f_00009-4-1 loss: 0.886732  [   64/  118]
train() client id: f_00009-4-2 loss: 1.036234  [   96/  118]
train() client id: f_00009-5-0 loss: 0.954308  [   32/  118]
train() client id: f_00009-5-1 loss: 0.926345  [   64/  118]
train() client id: f_00009-5-2 loss: 0.778053  [   96/  118]
train() client id: f_00009-6-0 loss: 0.908262  [   32/  118]
train() client id: f_00009-6-1 loss: 0.803055  [   64/  118]
train() client id: f_00009-6-2 loss: 1.018492  [   96/  118]
train() client id: f_00009-7-0 loss: 0.868913  [   32/  118]
train() client id: f_00009-7-1 loss: 0.803269  [   64/  118]
train() client id: f_00009-7-2 loss: 0.858374  [   96/  118]
train() client id: f_00009-8-0 loss: 0.891701  [   32/  118]
train() client id: f_00009-8-1 loss: 0.875354  [   64/  118]
train() client id: f_00009-8-2 loss: 0.890955  [   96/  118]
train() client id: f_00009-9-0 loss: 0.901080  [   32/  118]
train() client id: f_00009-9-1 loss: 0.726330  [   64/  118]
train() client id: f_00009-9-2 loss: 0.857674  [   96/  118]
train() client id: f_00009-10-0 loss: 0.949124  [   32/  118]
train() client id: f_00009-10-1 loss: 0.845097  [   64/  118]
train() client id: f_00009-10-2 loss: 0.836527  [   96/  118]
train() client id: f_00009-11-0 loss: 0.847552  [   32/  118]
train() client id: f_00009-11-1 loss: 0.910738  [   64/  118]
train() client id: f_00009-11-2 loss: 0.876158  [   96/  118]
train() client id: f_00009-12-0 loss: 0.877327  [   32/  118]
train() client id: f_00009-12-1 loss: 0.701438  [   64/  118]
train() client id: f_00009-12-2 loss: 0.869721  [   96/  118]
At round 44 accuracy: 0.6445623342175066
At round 44 training accuracy: 0.5928906773977196
At round 44 training loss: 0.8273866359648316
update_location
xs = [  -3.9056584     4.20031788  240.00902392   18.81129433    0.97929623
    3.95640986 -202.44319194 -181.32485185  224.66397685 -167.06087855]
ys = [ 232.5879595   215.55583871    1.32061395 -202.45517586  194.35018685
  177.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [253.20429118 237.6593408  260.01168355 226.58764977 218.57024991
 204.04294985 225.80995661 207.07336418 246.5412186  194.74431699]
dists_bs = [180.52602805 183.73222358 449.88556219 424.3055828  177.03994586
 178.97853524 179.73247853 174.29245025 429.55303777 171.18271783]
uav_gains = [6.96478461e-12 9.36092802e-12 6.06859569e-12 1.13384472e-11
 1.29057031e-11 1.60769736e-11 1.14854179e-11 1.53749023e-11
 7.93441177e-12 1.83978984e-11]
bs_gains = [5.30863138e-11 5.05330089e-11 4.11726034e-12 4.85057916e-12
 5.60653496e-11 5.43815306e-11 5.37452031e-11 5.85752397e-11
 4.68648315e-12 6.16036319e-11]
Round 45
-------------------------------
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.05514024 10.40024199  4.98453475  1.80545975 11.99306906  5.77018381
  2.2328339   7.08469715  5.23316744  4.67883957]
obj_prev = 59.2381676466099
eta_min = 5.885406011502381e-19	eta_max = 0.9363452566333951
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 13.714204272500444	eta = 0.9090909090909091
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 26.6122320197388	eta = 0.4684860112559707
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 20.101928982645475	eta = 0.6202120423522212
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.922786406509566	eta = 0.6588595443458035
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.857340066002127	eta = 0.6611461842396013
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.857119499819667	eta = 0.6611539174721296
eta = 0.6611539174721296
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [0.03414319 0.0718091  0.03360124 0.01165204 0.08291919 0.03956275
 0.0146328  0.04850501 0.0352271  0.03197537]
ene_total = [1.73700778 2.97656775 1.73758614 0.82737842 3.38956642 1.75748632
 0.93827146 2.18480132 1.84341633 1.46503756]
ti_comp = [0.61689568 0.66459269 0.61132264 0.63338719 0.66608943 0.66565651
 0.63376527 0.64167568 0.59949544 0.66739459]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.53685718e-06 5.23971089e-05 6.34460979e-06 2.46460673e-07
 8.03119208e-05 8.73451965e-06 4.87534362e-07 1.73224277e-05
 7.60219691e-06 4.58734577e-06]
ene_total = [0.45070637 0.27270761 0.47169887 0.38832777 0.26811959 0.26705379
 0.38691223 0.35773942 0.51631227 0.26034825]
optimize_network_iter = 0 obj = 3.639926174690373
eta = 0.6611539174721296
freqs = [27673393.660334   54024893.67528107 27482409.80128485  9198198.6140707
 62243282.60071961 29717090.05574555 11544335.26103987 37795583.8923452
 29380628.13581736 23955372.57683051]
eta_min = 0.6611539174721324	eta_max = 0.6721599416563945
af = 0.005695477061159853	bf = 1.2310181153532935	zeta = 0.006265024767275839	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [1.50574262e-06 1.20694943e-05 1.46145910e-06 5.67713705e-08
 1.84995754e-05 2.01196663e-06 1.12301867e-07 3.99016180e-06
 1.75113998e-06 1.05667936e-06]
ene_total = [1.67921435 1.010801   1.75748063 1.44739012 0.99068256 0.99444728
 1.44208793 1.33153211 1.92363253 0.96990197]
ti_comp = [0.5929754  0.64067241 0.58740236 0.60946691 0.64216915 0.64173622
 0.60984499 0.6177554  0.57557516 0.64347431]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.26134418e-06 4.98993487e-05 6.08167179e-06 2.35577818e-07
 7.64706350e-05 8.31715417e-06 4.65984701e-07 1.65407536e-05
 7.29886402e-06 4.36729864e-06]
ene_total = [0.46582645 0.28176549 0.48752418 0.40136402 0.27697112 0.27600289
 0.39990055 0.36971876 0.53363374 0.26907992]
optimize_network_iter = 1 obj = 3.761787113624325
eta = 0.6721599416563945
freqs = [27640992.43136498 53805865.01782712 27460330.69138759  9177789.36033805
 61985734.40435877 29594844.6264841  11518450.63917791 37692613.12413883
 29380628.13581735 23854513.15446249]
eta_min = 0.6721599416563961	eta_max = 0.6721599416563635
af = 0.0056547282477792775	bf = 1.2310181153532935	zeta = 0.006220201072557206	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [1.50221871e-06 1.19718279e-05 1.45911179e-06 5.65197178e-08
 1.83467983e-05 1.99544766e-06 1.11798827e-07 3.96844972e-06
 1.75113998e-06 1.04780021e-06]
ene_total = [1.67921385 1.01078729 1.7574803  1.44739009 0.9906611  0.99444496
 1.44208786 1.33152906 1.92363253 0.96990072]
ti_comp = [0.5929754  0.64067241 0.58740236 0.60946691 0.64216915 0.64173622
 0.60984499 0.6177554  0.57557516 0.64347431]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.26134418e-06 4.98993487e-05 6.08167179e-06 2.35577818e-07
 7.64706350e-05 8.31715417e-06 4.65984701e-07 1.65407536e-05
 7.29886402e-06 4.36729864e-06]
ene_total = [0.46582645 0.28176549 0.48752418 0.40136402 0.27697112 0.27600289
 0.39990055 0.36971876 0.53363374 0.26907992]
optimize_network_iter = 2 obj = 3.761787113623971
eta = 0.6721599416563635
freqs = [27640992.43136505 53805865.01782771 27460330.69138763  9177789.3603381
 61985734.40435947 29594844.62648444 11518450.63917798 37692613.1241391
 29380628.13581733 23854513.15446277]
Done!
At round 45 eta: 0.6721599416563635
At round 45 local rounds: 13.008276614299735
At round 45 global rounds: 38.94594152753551
At round 45 a_n: 12.425493895665781
gradient difference: 0.3721940815448761
train() client id: f_00000-0-0 loss: 1.337603  [   32/  126]
train() client id: f_00000-0-1 loss: 1.302346  [   64/  126]
train() client id: f_00000-0-2 loss: 1.318322  [   96/  126]
train() client id: f_00000-1-0 loss: 1.278402  [   32/  126]
train() client id: f_00000-1-1 loss: 1.326774  [   64/  126]
train() client id: f_00000-1-2 loss: 1.176509  [   96/  126]
train() client id: f_00000-2-0 loss: 1.012165  [   32/  126]
train() client id: f_00000-2-1 loss: 0.999344  [   64/  126]
train() client id: f_00000-2-2 loss: 1.290573  [   96/  126]
train() client id: f_00000-3-0 loss: 1.047262  [   32/  126]
train() client id: f_00000-3-1 loss: 1.131982  [   64/  126]
train() client id: f_00000-3-2 loss: 1.077005  [   96/  126]
train() client id: f_00000-4-0 loss: 1.050596  [   32/  126]
train() client id: f_00000-4-1 loss: 1.016959  [   64/  126]
train() client id: f_00000-4-2 loss: 0.971387  [   96/  126]
train() client id: f_00000-5-0 loss: 0.959264  [   32/  126]
train() client id: f_00000-5-1 loss: 1.005996  [   64/  126]
train() client id: f_00000-5-2 loss: 0.890442  [   96/  126]
train() client id: f_00000-6-0 loss: 0.961536  [   32/  126]
train() client id: f_00000-6-1 loss: 0.855971  [   64/  126]
train() client id: f_00000-6-2 loss: 0.959493  [   96/  126]
train() client id: f_00000-7-0 loss: 0.957257  [   32/  126]
train() client id: f_00000-7-1 loss: 0.926918  [   64/  126]
train() client id: f_00000-7-2 loss: 0.767260  [   96/  126]
train() client id: f_00000-8-0 loss: 0.911239  [   32/  126]
train() client id: f_00000-8-1 loss: 0.884714  [   64/  126]
train() client id: f_00000-8-2 loss: 0.800251  [   96/  126]
train() client id: f_00000-9-0 loss: 0.870557  [   32/  126]
train() client id: f_00000-9-1 loss: 0.928325  [   64/  126]
train() client id: f_00000-9-2 loss: 0.835863  [   96/  126]
train() client id: f_00000-10-0 loss: 0.751474  [   32/  126]
train() client id: f_00000-10-1 loss: 0.880911  [   64/  126]
train() client id: f_00000-10-2 loss: 0.902438  [   96/  126]
train() client id: f_00000-11-0 loss: 0.895157  [   32/  126]
train() client id: f_00000-11-1 loss: 0.782442  [   64/  126]
train() client id: f_00000-11-2 loss: 0.857182  [   96/  126]
train() client id: f_00000-12-0 loss: 0.868285  [   32/  126]
train() client id: f_00000-12-1 loss: 0.864547  [   64/  126]
train() client id: f_00000-12-2 loss: 0.852410  [   96/  126]
train() client id: f_00001-0-0 loss: 0.372537  [   32/  265]
train() client id: f_00001-0-1 loss: 0.280314  [   64/  265]
train() client id: f_00001-0-2 loss: 0.431439  [   96/  265]
train() client id: f_00001-0-3 loss: 0.448211  [  128/  265]
train() client id: f_00001-0-4 loss: 0.418426  [  160/  265]
train() client id: f_00001-0-5 loss: 0.470228  [  192/  265]
train() client id: f_00001-0-6 loss: 0.369066  [  224/  265]
train() client id: f_00001-0-7 loss: 0.298921  [  256/  265]
train() client id: f_00001-1-0 loss: 0.424997  [   32/  265]
train() client id: f_00001-1-1 loss: 0.505216  [   64/  265]
train() client id: f_00001-1-2 loss: 0.361713  [   96/  265]
train() client id: f_00001-1-3 loss: 0.330418  [  128/  265]
train() client id: f_00001-1-4 loss: 0.378429  [  160/  265]
train() client id: f_00001-1-5 loss: 0.290141  [  192/  265]
train() client id: f_00001-1-6 loss: 0.360762  [  224/  265]
train() client id: f_00001-1-7 loss: 0.355711  [  256/  265]
train() client id: f_00001-2-0 loss: 0.322969  [   32/  265]
train() client id: f_00001-2-1 loss: 0.402320  [   64/  265]
train() client id: f_00001-2-2 loss: 0.360222  [   96/  265]
train() client id: f_00001-2-3 loss: 0.347625  [  128/  265]
train() client id: f_00001-2-4 loss: 0.423089  [  160/  265]
train() client id: f_00001-2-5 loss: 0.346674  [  192/  265]
train() client id: f_00001-2-6 loss: 0.358563  [  224/  265]
train() client id: f_00001-2-7 loss: 0.387400  [  256/  265]
train() client id: f_00001-3-0 loss: 0.444008  [   32/  265]
train() client id: f_00001-3-1 loss: 0.324501  [   64/  265]
train() client id: f_00001-3-2 loss: 0.290943  [   96/  265]
train() client id: f_00001-3-3 loss: 0.366344  [  128/  265]
train() client id: f_00001-3-4 loss: 0.400751  [  160/  265]
train() client id: f_00001-3-5 loss: 0.379619  [  192/  265]
train() client id: f_00001-3-6 loss: 0.334461  [  224/  265]
train() client id: f_00001-3-7 loss: 0.343199  [  256/  265]
train() client id: f_00001-4-0 loss: 0.456335  [   32/  265]
train() client id: f_00001-4-1 loss: 0.299175  [   64/  265]
train() client id: f_00001-4-2 loss: 0.427462  [   96/  265]
train() client id: f_00001-4-3 loss: 0.304497  [  128/  265]
train() client id: f_00001-4-4 loss: 0.275739  [  160/  265]
train() client id: f_00001-4-5 loss: 0.368685  [  192/  265]
train() client id: f_00001-4-6 loss: 0.369703  [  224/  265]
train() client id: f_00001-4-7 loss: 0.346212  [  256/  265]
train() client id: f_00001-5-0 loss: 0.452640  [   32/  265]
train() client id: f_00001-5-1 loss: 0.293876  [   64/  265]
train() client id: f_00001-5-2 loss: 0.335622  [   96/  265]
train() client id: f_00001-5-3 loss: 0.346045  [  128/  265]
train() client id: f_00001-5-4 loss: 0.342236  [  160/  265]
train() client id: f_00001-5-5 loss: 0.467292  [  192/  265]
train() client id: f_00001-5-6 loss: 0.266460  [  224/  265]
train() client id: f_00001-5-7 loss: 0.261151  [  256/  265]
train() client id: f_00001-6-0 loss: 0.256262  [   32/  265]
train() client id: f_00001-6-1 loss: 0.438389  [   64/  265]
train() client id: f_00001-6-2 loss: 0.478790  [   96/  265]
train() client id: f_00001-6-3 loss: 0.407554  [  128/  265]
train() client id: f_00001-6-4 loss: 0.283275  [  160/  265]
train() client id: f_00001-6-5 loss: 0.302919  [  192/  265]
train() client id: f_00001-6-6 loss: 0.305070  [  224/  265]
train() client id: f_00001-6-7 loss: 0.262055  [  256/  265]
train() client id: f_00001-7-0 loss: 0.334670  [   32/  265]
train() client id: f_00001-7-1 loss: 0.396215  [   64/  265]
train() client id: f_00001-7-2 loss: 0.272968  [   96/  265]
train() client id: f_00001-7-3 loss: 0.357542  [  128/  265]
train() client id: f_00001-7-4 loss: 0.387671  [  160/  265]
train() client id: f_00001-7-5 loss: 0.316279  [  192/  265]
train() client id: f_00001-7-6 loss: 0.385712  [  224/  265]
train() client id: f_00001-7-7 loss: 0.361014  [  256/  265]
train() client id: f_00001-8-0 loss: 0.397289  [   32/  265]
train() client id: f_00001-8-1 loss: 0.337473  [   64/  265]
train() client id: f_00001-8-2 loss: 0.306697  [   96/  265]
train() client id: f_00001-8-3 loss: 0.251530  [  128/  265]
train() client id: f_00001-8-4 loss: 0.462132  [  160/  265]
train() client id: f_00001-8-5 loss: 0.271828  [  192/  265]
train() client id: f_00001-8-6 loss: 0.428666  [  224/  265]
train() client id: f_00001-8-7 loss: 0.299545  [  256/  265]
train() client id: f_00001-9-0 loss: 0.299670  [   32/  265]
train() client id: f_00001-9-1 loss: 0.382080  [   64/  265]
train() client id: f_00001-9-2 loss: 0.514889  [   96/  265]
train() client id: f_00001-9-3 loss: 0.275295  [  128/  265]
train() client id: f_00001-9-4 loss: 0.263152  [  160/  265]
train() client id: f_00001-9-5 loss: 0.311557  [  192/  265]
train() client id: f_00001-9-6 loss: 0.346892  [  224/  265]
train() client id: f_00001-9-7 loss: 0.406817  [  256/  265]
train() client id: f_00001-10-0 loss: 0.354960  [   32/  265]
train() client id: f_00001-10-1 loss: 0.366219  [   64/  265]
train() client id: f_00001-10-2 loss: 0.382821  [   96/  265]
train() client id: f_00001-10-3 loss: 0.246936  [  128/  265]
train() client id: f_00001-10-4 loss: 0.350762  [  160/  265]
train() client id: f_00001-10-5 loss: 0.351781  [  192/  265]
train() client id: f_00001-10-6 loss: 0.335686  [  224/  265]
train() client id: f_00001-10-7 loss: 0.294486  [  256/  265]
train() client id: f_00001-11-0 loss: 0.367921  [   32/  265]
train() client id: f_00001-11-1 loss: 0.338906  [   64/  265]
train() client id: f_00001-11-2 loss: 0.442496  [   96/  265]
train() client id: f_00001-11-3 loss: 0.273027  [  128/  265]
train() client id: f_00001-11-4 loss: 0.329188  [  160/  265]
train() client id: f_00001-11-5 loss: 0.362923  [  192/  265]
train() client id: f_00001-11-6 loss: 0.347714  [  224/  265]
train() client id: f_00001-11-7 loss: 0.335515  [  256/  265]
train() client id: f_00001-12-0 loss: 0.334432  [   32/  265]
train() client id: f_00001-12-1 loss: 0.350379  [   64/  265]
train() client id: f_00001-12-2 loss: 0.366502  [   96/  265]
train() client id: f_00001-12-3 loss: 0.414502  [  128/  265]
train() client id: f_00001-12-4 loss: 0.334620  [  160/  265]
train() client id: f_00001-12-5 loss: 0.252956  [  192/  265]
train() client id: f_00001-12-6 loss: 0.399834  [  224/  265]
train() client id: f_00001-12-7 loss: 0.335730  [  256/  265]
train() client id: f_00002-0-0 loss: 1.017527  [   32/  124]
train() client id: f_00002-0-1 loss: 1.112576  [   64/  124]
train() client id: f_00002-0-2 loss: 0.804976  [   96/  124]
train() client id: f_00002-1-0 loss: 1.065889  [   32/  124]
train() client id: f_00002-1-1 loss: 0.840996  [   64/  124]
train() client id: f_00002-1-2 loss: 1.113563  [   96/  124]
train() client id: f_00002-2-0 loss: 0.708158  [   32/  124]
train() client id: f_00002-2-1 loss: 0.992343  [   64/  124]
train() client id: f_00002-2-2 loss: 1.015482  [   96/  124]
train() client id: f_00002-3-0 loss: 0.881988  [   32/  124]
train() client id: f_00002-3-1 loss: 0.732090  [   64/  124]
train() client id: f_00002-3-2 loss: 0.825663  [   96/  124]
train() client id: f_00002-4-0 loss: 0.783335  [   32/  124]
train() client id: f_00002-4-1 loss: 0.813731  [   64/  124]
train() client id: f_00002-4-2 loss: 1.055081  [   96/  124]
train() client id: f_00002-5-0 loss: 0.756158  [   32/  124]
train() client id: f_00002-5-1 loss: 0.841143  [   64/  124]
train() client id: f_00002-5-2 loss: 0.856550  [   96/  124]
train() client id: f_00002-6-0 loss: 0.795249  [   32/  124]
train() client id: f_00002-6-1 loss: 0.860930  [   64/  124]
train() client id: f_00002-6-2 loss: 0.683114  [   96/  124]
train() client id: f_00002-7-0 loss: 0.781872  [   32/  124]
train() client id: f_00002-7-1 loss: 0.652102  [   64/  124]
train() client id: f_00002-7-2 loss: 0.843027  [   96/  124]
train() client id: f_00002-8-0 loss: 0.624669  [   32/  124]
train() client id: f_00002-8-1 loss: 0.692520  [   64/  124]
train() client id: f_00002-8-2 loss: 0.784440  [   96/  124]
train() client id: f_00002-9-0 loss: 0.657211  [   32/  124]
train() client id: f_00002-9-1 loss: 0.904114  [   64/  124]
train() client id: f_00002-9-2 loss: 0.578344  [   96/  124]
train() client id: f_00002-10-0 loss: 0.667176  [   32/  124]
train() client id: f_00002-10-1 loss: 0.848081  [   64/  124]
train() client id: f_00002-10-2 loss: 0.665222  [   96/  124]
train() client id: f_00002-11-0 loss: 0.647123  [   32/  124]
train() client id: f_00002-11-1 loss: 0.750132  [   64/  124]
train() client id: f_00002-11-2 loss: 0.681841  [   96/  124]
train() client id: f_00002-12-0 loss: 0.748396  [   32/  124]
train() client id: f_00002-12-1 loss: 0.685121  [   64/  124]
train() client id: f_00002-12-2 loss: 0.737758  [   96/  124]
train() client id: f_00003-0-0 loss: 0.690736  [   32/   43]
train() client id: f_00003-1-0 loss: 0.583284  [   32/   43]
train() client id: f_00003-2-0 loss: 0.494547  [   32/   43]
train() client id: f_00003-3-0 loss: 0.617789  [   32/   43]
train() client id: f_00003-4-0 loss: 0.654668  [   32/   43]
train() client id: f_00003-5-0 loss: 0.670140  [   32/   43]
train() client id: f_00003-6-0 loss: 0.424845  [   32/   43]
train() client id: f_00003-7-0 loss: 0.611523  [   32/   43]
train() client id: f_00003-8-0 loss: 0.789429  [   32/   43]
train() client id: f_00003-9-0 loss: 0.477121  [   32/   43]
train() client id: f_00003-10-0 loss: 0.561935  [   32/   43]
train() client id: f_00003-11-0 loss: 0.661155  [   32/   43]
train() client id: f_00003-12-0 loss: 0.665457  [   32/   43]
train() client id: f_00004-0-0 loss: 0.926835  [   32/  306]
train() client id: f_00004-0-1 loss: 0.682957  [   64/  306]
train() client id: f_00004-0-2 loss: 0.843299  [   96/  306]
train() client id: f_00004-0-3 loss: 0.691476  [  128/  306]
train() client id: f_00004-0-4 loss: 0.800575  [  160/  306]
train() client id: f_00004-0-5 loss: 0.747290  [  192/  306]
train() client id: f_00004-0-6 loss: 0.710395  [  224/  306]
train() client id: f_00004-0-7 loss: 0.755240  [  256/  306]
train() client id: f_00004-0-8 loss: 0.814169  [  288/  306]
train() client id: f_00004-1-0 loss: 0.718711  [   32/  306]
train() client id: f_00004-1-1 loss: 0.744253  [   64/  306]
train() client id: f_00004-1-2 loss: 0.935642  [   96/  306]
train() client id: f_00004-1-3 loss: 0.677906  [  128/  306]
train() client id: f_00004-1-4 loss: 0.875882  [  160/  306]
train() client id: f_00004-1-5 loss: 0.854849  [  192/  306]
train() client id: f_00004-1-6 loss: 0.904989  [  224/  306]
train() client id: f_00004-1-7 loss: 0.604241  [  256/  306]
train() client id: f_00004-1-8 loss: 0.711176  [  288/  306]
train() client id: f_00004-2-0 loss: 0.713621  [   32/  306]
train() client id: f_00004-2-1 loss: 0.809807  [   64/  306]
train() client id: f_00004-2-2 loss: 0.811745  [   96/  306]
train() client id: f_00004-2-3 loss: 0.792191  [  128/  306]
train() client id: f_00004-2-4 loss: 0.727333  [  160/  306]
train() client id: f_00004-2-5 loss: 0.787754  [  192/  306]
train() client id: f_00004-2-6 loss: 0.777943  [  224/  306]
train() client id: f_00004-2-7 loss: 0.853428  [  256/  306]
train() client id: f_00004-2-8 loss: 0.777048  [  288/  306]
train() client id: f_00004-3-0 loss: 0.818006  [   32/  306]
train() client id: f_00004-3-1 loss: 0.705475  [   64/  306]
train() client id: f_00004-3-2 loss: 0.661861  [   96/  306]
train() client id: f_00004-3-3 loss: 0.741588  [  128/  306]
train() client id: f_00004-3-4 loss: 0.776181  [  160/  306]
train() client id: f_00004-3-5 loss: 0.710861  [  192/  306]
train() client id: f_00004-3-6 loss: 0.816364  [  224/  306]
train() client id: f_00004-3-7 loss: 0.879376  [  256/  306]
train() client id: f_00004-3-8 loss: 0.876485  [  288/  306]
train() client id: f_00004-4-0 loss: 0.753619  [   32/  306]
train() client id: f_00004-4-1 loss: 0.815451  [   64/  306]
train() client id: f_00004-4-2 loss: 0.732511  [   96/  306]
train() client id: f_00004-4-3 loss: 0.728273  [  128/  306]
train() client id: f_00004-4-4 loss: 0.826745  [  160/  306]
train() client id: f_00004-4-5 loss: 0.737181  [  192/  306]
train() client id: f_00004-4-6 loss: 0.765752  [  224/  306]
train() client id: f_00004-4-7 loss: 0.752442  [  256/  306]
train() client id: f_00004-4-8 loss: 0.789767  [  288/  306]
train() client id: f_00004-5-0 loss: 0.739909  [   32/  306]
train() client id: f_00004-5-1 loss: 0.798109  [   64/  306]
train() client id: f_00004-5-2 loss: 0.732085  [   96/  306]
train() client id: f_00004-5-3 loss: 0.722054  [  128/  306]
train() client id: f_00004-5-4 loss: 0.815648  [  160/  306]
train() client id: f_00004-5-5 loss: 0.823274  [  192/  306]
train() client id: f_00004-5-6 loss: 0.722243  [  224/  306]
train() client id: f_00004-5-7 loss: 0.770476  [  256/  306]
train() client id: f_00004-5-8 loss: 0.817655  [  288/  306]
train() client id: f_00004-6-0 loss: 0.724386  [   32/  306]
train() client id: f_00004-6-1 loss: 0.669227  [   64/  306]
train() client id: f_00004-6-2 loss: 0.903415  [   96/  306]
train() client id: f_00004-6-3 loss: 0.774028  [  128/  306]
train() client id: f_00004-6-4 loss: 0.723255  [  160/  306]
train() client id: f_00004-6-5 loss: 0.783215  [  192/  306]
train() client id: f_00004-6-6 loss: 0.794396  [  224/  306]
train() client id: f_00004-6-7 loss: 0.760970  [  256/  306]
train() client id: f_00004-6-8 loss: 0.754335  [  288/  306]
train() client id: f_00004-7-0 loss: 0.803109  [   32/  306]
train() client id: f_00004-7-1 loss: 0.741743  [   64/  306]
train() client id: f_00004-7-2 loss: 0.879125  [   96/  306]
train() client id: f_00004-7-3 loss: 0.766453  [  128/  306]
train() client id: f_00004-7-4 loss: 0.820818  [  160/  306]
train() client id: f_00004-7-5 loss: 0.800844  [  192/  306]
train() client id: f_00004-7-6 loss: 0.709191  [  224/  306]
train() client id: f_00004-7-7 loss: 0.864229  [  256/  306]
train() client id: f_00004-7-8 loss: 0.672986  [  288/  306]
train() client id: f_00004-8-0 loss: 0.727912  [   32/  306]
train() client id: f_00004-8-1 loss: 0.673838  [   64/  306]
train() client id: f_00004-8-2 loss: 0.654345  [   96/  306]
train() client id: f_00004-8-3 loss: 0.720014  [  128/  306]
train() client id: f_00004-8-4 loss: 0.842171  [  160/  306]
train() client id: f_00004-8-5 loss: 0.980182  [  192/  306]
train() client id: f_00004-8-6 loss: 0.773428  [  224/  306]
train() client id: f_00004-8-7 loss: 0.842057  [  256/  306]
train() client id: f_00004-8-8 loss: 0.769109  [  288/  306]
train() client id: f_00004-9-0 loss: 0.780621  [   32/  306]
train() client id: f_00004-9-1 loss: 0.898981  [   64/  306]
train() client id: f_00004-9-2 loss: 0.894002  [   96/  306]
train() client id: f_00004-9-3 loss: 0.756907  [  128/  306]
train() client id: f_00004-9-4 loss: 0.695216  [  160/  306]
train() client id: f_00004-9-5 loss: 0.701402  [  192/  306]
train() client id: f_00004-9-6 loss: 0.800013  [  224/  306]
train() client id: f_00004-9-7 loss: 0.853546  [  256/  306]
train() client id: f_00004-9-8 loss: 0.703229  [  288/  306]
train() client id: f_00004-10-0 loss: 0.894889  [   32/  306]
train() client id: f_00004-10-1 loss: 0.791626  [   64/  306]
train() client id: f_00004-10-2 loss: 0.770168  [   96/  306]
train() client id: f_00004-10-3 loss: 0.767219  [  128/  306]
train() client id: f_00004-10-4 loss: 0.717183  [  160/  306]
train() client id: f_00004-10-5 loss: 0.753748  [  192/  306]
train() client id: f_00004-10-6 loss: 0.669089  [  224/  306]
train() client id: f_00004-10-7 loss: 0.803157  [  256/  306]
train() client id: f_00004-10-8 loss: 0.796191  [  288/  306]
train() client id: f_00004-11-0 loss: 0.710732  [   32/  306]
train() client id: f_00004-11-1 loss: 0.759160  [   64/  306]
train() client id: f_00004-11-2 loss: 0.637824  [   96/  306]
train() client id: f_00004-11-3 loss: 0.698392  [  128/  306]
train() client id: f_00004-11-4 loss: 0.809519  [  160/  306]
train() client id: f_00004-11-5 loss: 0.857834  [  192/  306]
train() client id: f_00004-11-6 loss: 0.852473  [  224/  306]
train() client id: f_00004-11-7 loss: 0.799675  [  256/  306]
train() client id: f_00004-11-8 loss: 0.778257  [  288/  306]
train() client id: f_00004-12-0 loss: 0.731011  [   32/  306]
train() client id: f_00004-12-1 loss: 0.715026  [   64/  306]
train() client id: f_00004-12-2 loss: 0.731561  [   96/  306]
train() client id: f_00004-12-3 loss: 0.732953  [  128/  306]
train() client id: f_00004-12-4 loss: 0.726254  [  160/  306]
train() client id: f_00004-12-5 loss: 0.701511  [  192/  306]
train() client id: f_00004-12-6 loss: 1.004773  [  224/  306]
train() client id: f_00004-12-7 loss: 0.836696  [  256/  306]
train() client id: f_00004-12-8 loss: 0.766172  [  288/  306]
train() client id: f_00005-0-0 loss: 0.798475  [   32/  146]
train() client id: f_00005-0-1 loss: 1.071906  [   64/  146]
train() client id: f_00005-0-2 loss: 0.514442  [   96/  146]
train() client id: f_00005-0-3 loss: 0.613004  [  128/  146]
train() client id: f_00005-1-0 loss: 0.807320  [   32/  146]
train() client id: f_00005-1-1 loss: 0.634715  [   64/  146]
train() client id: f_00005-1-2 loss: 0.472035  [   96/  146]
train() client id: f_00005-1-3 loss: 0.929803  [  128/  146]
train() client id: f_00005-2-0 loss: 0.921771  [   32/  146]
train() client id: f_00005-2-1 loss: 0.663232  [   64/  146]
train() client id: f_00005-2-2 loss: 0.791538  [   96/  146]
train() client id: f_00005-2-3 loss: 0.483801  [  128/  146]
train() client id: f_00005-3-0 loss: 0.665647  [   32/  146]
train() client id: f_00005-3-1 loss: 0.527360  [   64/  146]
train() client id: f_00005-3-2 loss: 0.873789  [   96/  146]
train() client id: f_00005-3-3 loss: 0.852962  [  128/  146]
train() client id: f_00005-4-0 loss: 0.776003  [   32/  146]
train() client id: f_00005-4-1 loss: 0.591210  [   64/  146]
train() client id: f_00005-4-2 loss: 1.014155  [   96/  146]
train() client id: f_00005-4-3 loss: 0.556764  [  128/  146]
train() client id: f_00005-5-0 loss: 0.667637  [   32/  146]
train() client id: f_00005-5-1 loss: 0.831950  [   64/  146]
train() client id: f_00005-5-2 loss: 0.636663  [   96/  146]
train() client id: f_00005-5-3 loss: 0.703907  [  128/  146]
train() client id: f_00005-6-0 loss: 0.808595  [   32/  146]
train() client id: f_00005-6-1 loss: 0.519818  [   64/  146]
train() client id: f_00005-6-2 loss: 0.924032  [   96/  146]
train() client id: f_00005-6-3 loss: 0.878835  [  128/  146]
train() client id: f_00005-7-0 loss: 0.823941  [   32/  146]
train() client id: f_00005-7-1 loss: 0.897782  [   64/  146]
train() client id: f_00005-7-2 loss: 0.840432  [   96/  146]
train() client id: f_00005-7-3 loss: 0.553108  [  128/  146]
train() client id: f_00005-8-0 loss: 0.848724  [   32/  146]
train() client id: f_00005-8-1 loss: 0.725278  [   64/  146]
train() client id: f_00005-8-2 loss: 0.613221  [   96/  146]
train() client id: f_00005-8-3 loss: 0.600433  [  128/  146]
train() client id: f_00005-9-0 loss: 0.828705  [   32/  146]
train() client id: f_00005-9-1 loss: 0.818304  [   64/  146]
train() client id: f_00005-9-2 loss: 0.755012  [   96/  146]
train() client id: f_00005-9-3 loss: 0.597469  [  128/  146]
train() client id: f_00005-10-0 loss: 0.850034  [   32/  146]
train() client id: f_00005-10-1 loss: 0.621890  [   64/  146]
train() client id: f_00005-10-2 loss: 0.706649  [   96/  146]
train() client id: f_00005-10-3 loss: 0.586641  [  128/  146]
train() client id: f_00005-11-0 loss: 0.753982  [   32/  146]
train() client id: f_00005-11-1 loss: 0.741780  [   64/  146]
train() client id: f_00005-11-2 loss: 0.731284  [   96/  146]
train() client id: f_00005-11-3 loss: 0.690331  [  128/  146]
train() client id: f_00005-12-0 loss: 0.752313  [   32/  146]
train() client id: f_00005-12-1 loss: 0.632587  [   64/  146]
train() client id: f_00005-12-2 loss: 0.791132  [   96/  146]
train() client id: f_00005-12-3 loss: 0.726820  [  128/  146]
train() client id: f_00006-0-0 loss: 0.523370  [   32/   54]
train() client id: f_00006-1-0 loss: 0.503843  [   32/   54]
train() client id: f_00006-2-0 loss: 0.556895  [   32/   54]
train() client id: f_00006-3-0 loss: 0.514477  [   32/   54]
train() client id: f_00006-4-0 loss: 0.499592  [   32/   54]
train() client id: f_00006-5-0 loss: 0.528579  [   32/   54]
train() client id: f_00006-6-0 loss: 0.527599  [   32/   54]
train() client id: f_00006-7-0 loss: 0.501167  [   32/   54]
train() client id: f_00006-8-0 loss: 0.479555  [   32/   54]
train() client id: f_00006-9-0 loss: 0.568761  [   32/   54]
train() client id: f_00006-10-0 loss: 0.514239  [   32/   54]
train() client id: f_00006-11-0 loss: 0.463883  [   32/   54]
train() client id: f_00006-12-0 loss: 0.506643  [   32/   54]
train() client id: f_00007-0-0 loss: 0.704339  [   32/  179]
train() client id: f_00007-0-1 loss: 0.571431  [   64/  179]
train() client id: f_00007-0-2 loss: 0.769026  [   96/  179]
train() client id: f_00007-0-3 loss: 0.664192  [  128/  179]
train() client id: f_00007-0-4 loss: 0.635098  [  160/  179]
train() client id: f_00007-1-0 loss: 0.548689  [   32/  179]
train() client id: f_00007-1-1 loss: 0.849720  [   64/  179]
train() client id: f_00007-1-2 loss: 0.482303  [   96/  179]
train() client id: f_00007-1-3 loss: 0.535832  [  128/  179]
train() client id: f_00007-1-4 loss: 0.749343  [  160/  179]
train() client id: f_00007-2-0 loss: 0.642797  [   32/  179]
train() client id: f_00007-2-1 loss: 0.640152  [   64/  179]
train() client id: f_00007-2-2 loss: 0.653889  [   96/  179]
train() client id: f_00007-2-3 loss: 0.564680  [  128/  179]
train() client id: f_00007-2-4 loss: 0.729557  [  160/  179]
train() client id: f_00007-3-0 loss: 0.552056  [   32/  179]
train() client id: f_00007-3-1 loss: 0.530454  [   64/  179]
train() client id: f_00007-3-2 loss: 0.819722  [   96/  179]
train() client id: f_00007-3-3 loss: 0.617397  [  128/  179]
train() client id: f_00007-3-4 loss: 0.666193  [  160/  179]
train() client id: f_00007-4-0 loss: 0.459107  [   32/  179]
train() client id: f_00007-4-1 loss: 0.684357  [   64/  179]
train() client id: f_00007-4-2 loss: 0.601797  [   96/  179]
train() client id: f_00007-4-3 loss: 0.704134  [  128/  179]
train() client id: f_00007-4-4 loss: 0.592225  [  160/  179]
train() client id: f_00007-5-0 loss: 0.687234  [   32/  179]
train() client id: f_00007-5-1 loss: 0.647930  [   64/  179]
train() client id: f_00007-5-2 loss: 0.683887  [   96/  179]
train() client id: f_00007-5-3 loss: 0.597785  [  128/  179]
train() client id: f_00007-5-4 loss: 0.550912  [  160/  179]
train() client id: f_00007-6-0 loss: 0.746164  [   32/  179]
train() client id: f_00007-6-1 loss: 0.740722  [   64/  179]
train() client id: f_00007-6-2 loss: 0.564035  [   96/  179]
train() client id: f_00007-6-3 loss: 0.500443  [  128/  179]
train() client id: f_00007-6-4 loss: 0.600558  [  160/  179]
train() client id: f_00007-7-0 loss: 0.534487  [   32/  179]
train() client id: f_00007-7-1 loss: 0.543308  [   64/  179]
train() client id: f_00007-7-2 loss: 0.752716  [   96/  179]
train() client id: f_00007-7-3 loss: 0.456862  [  128/  179]
train() client id: f_00007-7-4 loss: 0.775328  [  160/  179]
train() client id: f_00007-8-0 loss: 0.840136  [   32/  179]
train() client id: f_00007-8-1 loss: 0.491776  [   64/  179]
train() client id: f_00007-8-2 loss: 0.510710  [   96/  179]
train() client id: f_00007-8-3 loss: 0.735586  [  128/  179]
train() client id: f_00007-8-4 loss: 0.529608  [  160/  179]
train() client id: f_00007-9-0 loss: 0.471388  [   32/  179]
train() client id: f_00007-9-1 loss: 0.721711  [   64/  179]
train() client id: f_00007-9-2 loss: 0.505272  [   96/  179]
train() client id: f_00007-9-3 loss: 0.808983  [  128/  179]
train() client id: f_00007-9-4 loss: 0.528522  [  160/  179]
train() client id: f_00007-10-0 loss: 0.664230  [   32/  179]
train() client id: f_00007-10-1 loss: 0.710291  [   64/  179]
train() client id: f_00007-10-2 loss: 0.719105  [   96/  179]
train() client id: f_00007-10-3 loss: 0.541654  [  128/  179]
train() client id: f_00007-10-4 loss: 0.439912  [  160/  179]
train() client id: f_00007-11-0 loss: 0.608330  [   32/  179]
train() client id: f_00007-11-1 loss: 0.521945  [   64/  179]
train() client id: f_00007-11-2 loss: 0.754368  [   96/  179]
train() client id: f_00007-11-3 loss: 0.452892  [  128/  179]
train() client id: f_00007-11-4 loss: 0.577765  [  160/  179]
train() client id: f_00007-12-0 loss: 0.531860  [   32/  179]
train() client id: f_00007-12-1 loss: 0.630975  [   64/  179]
train() client id: f_00007-12-2 loss: 0.646702  [   96/  179]
train() client id: f_00007-12-3 loss: 0.719865  [  128/  179]
train() client id: f_00007-12-4 loss: 0.460777  [  160/  179]
train() client id: f_00008-0-0 loss: 0.729944  [   32/  130]
train() client id: f_00008-0-1 loss: 0.965483  [   64/  130]
train() client id: f_00008-0-2 loss: 0.659785  [   96/  130]
train() client id: f_00008-0-3 loss: 0.659880  [  128/  130]
train() client id: f_00008-1-0 loss: 0.754074  [   32/  130]
train() client id: f_00008-1-1 loss: 0.794630  [   64/  130]
train() client id: f_00008-1-2 loss: 0.677489  [   96/  130]
train() client id: f_00008-1-3 loss: 0.768483  [  128/  130]
train() client id: f_00008-2-0 loss: 0.808140  [   32/  130]
train() client id: f_00008-2-1 loss: 0.709247  [   64/  130]
train() client id: f_00008-2-2 loss: 0.715398  [   96/  130]
train() client id: f_00008-2-3 loss: 0.800956  [  128/  130]
train() client id: f_00008-3-0 loss: 0.733225  [   32/  130]
train() client id: f_00008-3-1 loss: 0.840481  [   64/  130]
train() client id: f_00008-3-2 loss: 0.693771  [   96/  130]
train() client id: f_00008-3-3 loss: 0.764942  [  128/  130]
train() client id: f_00008-4-0 loss: 0.683062  [   32/  130]
train() client id: f_00008-4-1 loss: 0.838906  [   64/  130]
train() client id: f_00008-4-2 loss: 0.687663  [   96/  130]
train() client id: f_00008-4-3 loss: 0.822598  [  128/  130]
train() client id: f_00008-5-0 loss: 0.782508  [   32/  130]
train() client id: f_00008-5-1 loss: 0.758570  [   64/  130]
train() client id: f_00008-5-2 loss: 0.709088  [   96/  130]
train() client id: f_00008-5-3 loss: 0.745869  [  128/  130]
train() client id: f_00008-6-0 loss: 0.785339  [   32/  130]
train() client id: f_00008-6-1 loss: 0.687660  [   64/  130]
train() client id: f_00008-6-2 loss: 0.712680  [   96/  130]
train() client id: f_00008-6-3 loss: 0.796378  [  128/  130]
train() client id: f_00008-7-0 loss: 0.732335  [   32/  130]
train() client id: f_00008-7-1 loss: 0.761261  [   64/  130]
train() client id: f_00008-7-2 loss: 0.838246  [   96/  130]
train() client id: f_00008-7-3 loss: 0.700010  [  128/  130]
train() client id: f_00008-8-0 loss: 0.730079  [   32/  130]
train() client id: f_00008-8-1 loss: 0.688525  [   64/  130]
train() client id: f_00008-8-2 loss: 0.696575  [   96/  130]
train() client id: f_00008-8-3 loss: 0.914752  [  128/  130]
train() client id: f_00008-9-0 loss: 0.750340  [   32/  130]
train() client id: f_00008-9-1 loss: 0.778352  [   64/  130]
train() client id: f_00008-9-2 loss: 0.710839  [   96/  130]
train() client id: f_00008-9-3 loss: 0.781065  [  128/  130]
train() client id: f_00008-10-0 loss: 0.768702  [   32/  130]
train() client id: f_00008-10-1 loss: 0.807409  [   64/  130]
train() client id: f_00008-10-2 loss: 0.696090  [   96/  130]
train() client id: f_00008-10-3 loss: 0.730997  [  128/  130]
train() client id: f_00008-11-0 loss: 0.739113  [   32/  130]
train() client id: f_00008-11-1 loss: 0.710626  [   64/  130]
train() client id: f_00008-11-2 loss: 0.873024  [   96/  130]
train() client id: f_00008-11-3 loss: 0.661895  [  128/  130]
train() client id: f_00008-12-0 loss: 0.742031  [   32/  130]
train() client id: f_00008-12-1 loss: 0.726192  [   64/  130]
train() client id: f_00008-12-2 loss: 0.748481  [   96/  130]
train() client id: f_00008-12-3 loss: 0.768920  [  128/  130]
train() client id: f_00009-0-0 loss: 1.204186  [   32/  118]
train() client id: f_00009-0-1 loss: 1.150493  [   64/  118]
train() client id: f_00009-0-2 loss: 0.943873  [   96/  118]
train() client id: f_00009-1-0 loss: 0.995615  [   32/  118]
train() client id: f_00009-1-1 loss: 1.140670  [   64/  118]
train() client id: f_00009-1-2 loss: 1.033408  [   96/  118]
train() client id: f_00009-2-0 loss: 1.016467  [   32/  118]
train() client id: f_00009-2-1 loss: 0.886952  [   64/  118]
train() client id: f_00009-2-2 loss: 0.999646  [   96/  118]
train() client id: f_00009-3-0 loss: 0.960149  [   32/  118]
train() client id: f_00009-3-1 loss: 0.806494  [   64/  118]
train() client id: f_00009-3-2 loss: 1.029992  [   96/  118]
train() client id: f_00009-4-0 loss: 1.059491  [   32/  118]
train() client id: f_00009-4-1 loss: 0.863515  [   64/  118]
train() client id: f_00009-4-2 loss: 0.885923  [   96/  118]
train() client id: f_00009-5-0 loss: 0.930715  [   32/  118]
train() client id: f_00009-5-1 loss: 0.845298  [   64/  118]
train() client id: f_00009-5-2 loss: 0.867969  [   96/  118]
train() client id: f_00009-6-0 loss: 0.900072  [   32/  118]
train() client id: f_00009-6-1 loss: 0.823917  [   64/  118]
train() client id: f_00009-6-2 loss: 0.900974  [   96/  118]
train() client id: f_00009-7-0 loss: 0.893220  [   32/  118]
train() client id: f_00009-7-1 loss: 0.664077  [   64/  118]
train() client id: f_00009-7-2 loss: 0.982834  [   96/  118]
train() client id: f_00009-8-0 loss: 0.849023  [   32/  118]
train() client id: f_00009-8-1 loss: 0.778488  [   64/  118]
train() client id: f_00009-8-2 loss: 0.815943  [   96/  118]
train() client id: f_00009-9-0 loss: 0.737507  [   32/  118]
train() client id: f_00009-9-1 loss: 0.900430  [   64/  118]
train() client id: f_00009-9-2 loss: 0.825680  [   96/  118]
train() client id: f_00009-10-0 loss: 0.676235  [   32/  118]
train() client id: f_00009-10-1 loss: 0.889959  [   64/  118]
train() client id: f_00009-10-2 loss: 0.883539  [   96/  118]
train() client id: f_00009-11-0 loss: 0.913933  [   32/  118]
train() client id: f_00009-11-1 loss: 0.766351  [   64/  118]
train() client id: f_00009-11-2 loss: 0.862494  [   96/  118]
train() client id: f_00009-12-0 loss: 0.635445  [   32/  118]
train() client id: f_00009-12-1 loss: 0.884681  [   64/  118]
train() client id: f_00009-12-2 loss: 0.777652  [   96/  118]
At round 45 accuracy: 0.6445623342175066
At round 45 training accuracy: 0.5942320590207915
At round 45 training loss: 0.8318415710021693
update_location
xs = [  -3.9056584     4.20031788  245.00902392   18.81129433    0.97929623
    3.95640986 -207.44319194 -186.32485185  229.66397685 -172.06087855]
ys = [ 237.5879595   220.55583871    1.32061395 -207.45517586  199.35018685
  182.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [257.80475687 242.20346954 264.63402242 231.06603988 223.02792654
 208.41465133 230.30320976 211.46542666 251.1059781  199.05013887]
dists_bs = [182.18267316 184.90021192 454.50204544 428.75958223 177.65597175
 179.12693148 180.56355047 174.54542885 434.20899579 171.02377515]
uav_gains = [6.34845528e-12 8.61260857e-12 5.51598422e-12 1.05126925e-11
 1.20199406e-11 1.50715939e-11 1.06508766e-11 1.43975841e-11
 7.26046655e-12 1.72893462e-11]
bs_gains = [5.17457027e-11 4.96442942e-11 4.00123195e-12 4.71080726e-12
 5.55227049e-11 5.42554792e-11 5.30554308e-11 5.83378396e-11
 4.54713029e-12 6.17640715e-11]
Round 46
-------------------------------
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.92415982 10.12154902  4.85601333  1.75987582 11.67148702  5.61542073
  2.1757575   6.89673295  5.09466844  4.55326949]
obj_prev = 57.668934107649186
eta_min = 1.934063127362615e-19	eta_max = 0.9372721317732582
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 13.346274362136274	eta = 0.9090909090909091
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 26.11640265875819	eta = 0.46457304443429304
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 19.646759764324543	eta = 0.617556118077178
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.475231020287687	eta = 0.6567158310241378
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.40977991233988	eta = 0.6590506106332402
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.40955607686498	eta = 0.6590586238034545
eta = 0.6590586238034545
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [0.03440512 0.07235997 0.03385901 0.01174143 0.08355529 0.03986625
 0.01474505 0.04887711 0.03549735 0.03222067]
ene_total = [1.70303953 2.89994011 1.70508731 0.81177669 3.30204155 1.7110066
 0.91978726 2.13279846 1.79829675 1.42578181]
ti_comp = [0.63680768 0.68794036 0.63078466 0.65472313 0.68956148 0.68923291
 0.65512248 0.66360668 0.62159244 0.69103952]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [6.27671845e-06 5.00349702e-05 6.09733991e-06 2.36008472e-07
 7.66753725e-05 8.33612381e-06 4.66846244e-07 1.65720103e-05
 7.23529432e-06 4.37802116e-06]
ene_total = [0.44938525 0.26462869 0.47132941 0.38387283 0.25969147 0.25839833
 0.38242583 0.35209241 0.50487161 0.25166996]
optimize_network_iter = 0 obj = 3.578365787451716
eta = 0.6590586238034545
freqs = [27013744.23254671 52591749.45149608 26838799.91285678  8966713.64201406
 60585816.6007772  28920738.74380525 11253659.91650407 36826870.45355151
 28553553.02519245 23313187.49305714]
eta_min = 0.6590586238034565	eta_max = 0.6785433277513145
af = 0.005254252612989919	bf = 1.2178222333661453	zeta = 0.005779677874288912	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [1.43481355e-06 1.14376411e-05 1.39380888e-06 5.39498714e-08
 1.75274491e-05 1.90557908e-06 1.06717757e-07 3.78824463e-06
 1.65393723e-06 1.00078475e-06]
ene_total = [1.6846925  0.98717606 1.76700979 1.43963457 0.96585086 0.96820657
 1.43418346 1.3187243  1.89268496 0.94339011]
ti_comp = [0.59337097 0.64450365 0.58734795 0.61128643 0.64612477 0.6457962
 0.61168578 0.62016997 0.57815574 0.64760281]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.81719976e-06 4.58713790e-05 5.65886160e-06 2.17856639e-07
 7.02726667e-05 7.64051325e-06 4.30902936e-07 1.52683660e-05
 6.72969396e-06 4.01128493e-06]
ene_total = [0.47660643 0.28050789 0.49988152 0.40714012 0.27518486 0.27403394
 0.40560472 0.37338368 0.53545424 0.26691045]
optimize_network_iter = 1 obj = 3.7947078371699785
eta = 0.6785433277513145
freqs = [26965339.44106933 52213419.54907353 26809447.82144539  8932753.44599161
 60140450.82785589 28709062.73531702 11210552.63537973 36652528.58326961
 28553553.02519245 23138485.42973953]
eta_min = 0.6785433277513172	eta_max = 0.6785433277513145
af = 0.005187942505206725	bf = 1.2178222333661453	zeta = 0.005706736755727398	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [1.42967619e-06 1.12736748e-05 1.39076189e-06 5.35419898e-08
 1.72707080e-05 1.87778663e-06 1.05901756e-07 3.75246172e-06
 1.65393723e-06 9.85841782e-07]
ene_total = [1.6846918  0.98715364 1.76700937 1.43963452 0.96581577 0.96820277
 1.43418335 1.31871941 1.89268496 0.94338807]
ti_comp = [0.59337097 0.64450365 0.58734795 0.61128643 0.64612477 0.6457962
 0.61168578 0.62016997 0.57815574 0.64760281]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.81719976e-06 4.58713790e-05 5.65886160e-06 2.17856639e-07
 7.02726667e-05 7.64051325e-06 4.30902936e-07 1.52683660e-05
 6.72969396e-06 4.01128493e-06]
ene_total = [0.47660643 0.28050789 0.49988152 0.40714012 0.27518486 0.27403394
 0.40560472 0.37338368 0.53545424 0.26691045]
optimize_network_iter = 2 obj = 3.7947078371699785
eta = 0.6785433277513145
freqs = [26965339.44106933 52213419.54907353 26809447.82144539  8932753.44599161
 60140450.82785589 28709062.73531702 11210552.63537973 36652528.58326961
 28553553.02519245 23138485.42973953]
Done!
At round 46 eta: 0.6785433277513145
At round 46 local rounds: 12.69876962905909
At round 46 global rounds: 38.653712827752926
At round 46 a_n: 12.082948048696464
gradient difference: 0.4323503375053406
train() client id: f_00000-0-0 loss: 1.235965  [   32/  126]
train() client id: f_00000-0-1 loss: 1.142566  [   64/  126]
train() client id: f_00000-0-2 loss: 1.184027  [   96/  126]
train() client id: f_00000-1-0 loss: 0.990844  [   32/  126]
train() client id: f_00000-1-1 loss: 1.248275  [   64/  126]
train() client id: f_00000-1-2 loss: 0.919081  [   96/  126]
train() client id: f_00000-2-0 loss: 1.036387  [   32/  126]
train() client id: f_00000-2-1 loss: 1.002361  [   64/  126]
train() client id: f_00000-2-2 loss: 1.044379  [   96/  126]
train() client id: f_00000-3-0 loss: 0.892030  [   32/  126]
train() client id: f_00000-3-1 loss: 0.897050  [   64/  126]
train() client id: f_00000-3-2 loss: 1.011264  [   96/  126]
train() client id: f_00000-4-0 loss: 0.916852  [   32/  126]
train() client id: f_00000-4-1 loss: 0.881737  [   64/  126]
train() client id: f_00000-4-2 loss: 0.780684  [   96/  126]
train() client id: f_00000-5-0 loss: 0.850084  [   32/  126]
train() client id: f_00000-5-1 loss: 0.917588  [   64/  126]
train() client id: f_00000-5-2 loss: 0.822179  [   96/  126]
train() client id: f_00000-6-0 loss: 0.853627  [   32/  126]
train() client id: f_00000-6-1 loss: 0.781507  [   64/  126]
train() client id: f_00000-6-2 loss: 0.823805  [   96/  126]
train() client id: f_00000-7-0 loss: 0.815028  [   32/  126]
train() client id: f_00000-7-1 loss: 0.710875  [   64/  126]
train() client id: f_00000-7-2 loss: 0.853555  [   96/  126]
train() client id: f_00000-8-0 loss: 0.769051  [   32/  126]
train() client id: f_00000-8-1 loss: 0.716802  [   64/  126]
train() client id: f_00000-8-2 loss: 0.775512  [   96/  126]
train() client id: f_00000-9-0 loss: 0.890119  [   32/  126]
train() client id: f_00000-9-1 loss: 0.755803  [   64/  126]
train() client id: f_00000-9-2 loss: 0.732289  [   96/  126]
train() client id: f_00000-10-0 loss: 0.823535  [   32/  126]
train() client id: f_00000-10-1 loss: 0.680190  [   64/  126]
train() client id: f_00000-10-2 loss: 0.826190  [   96/  126]
train() client id: f_00000-11-0 loss: 0.782402  [   32/  126]
train() client id: f_00000-11-1 loss: 0.798480  [   64/  126]
train() client id: f_00000-11-2 loss: 0.763606  [   96/  126]
train() client id: f_00001-0-0 loss: 0.463993  [   32/  265]
train() client id: f_00001-0-1 loss: 0.472537  [   64/  265]
train() client id: f_00001-0-2 loss: 0.603798  [   96/  265]
train() client id: f_00001-0-3 loss: 0.506504  [  128/  265]
train() client id: f_00001-0-4 loss: 0.418846  [  160/  265]
train() client id: f_00001-0-5 loss: 0.493534  [  192/  265]
train() client id: f_00001-0-6 loss: 0.445359  [  224/  265]
train() client id: f_00001-0-7 loss: 0.418275  [  256/  265]
train() client id: f_00001-1-0 loss: 0.495486  [   32/  265]
train() client id: f_00001-1-1 loss: 0.383766  [   64/  265]
train() client id: f_00001-1-2 loss: 0.433161  [   96/  265]
train() client id: f_00001-1-3 loss: 0.537477  [  128/  265]
train() client id: f_00001-1-4 loss: 0.390520  [  160/  265]
train() client id: f_00001-1-5 loss: 0.540457  [  192/  265]
train() client id: f_00001-1-6 loss: 0.455107  [  224/  265]
train() client id: f_00001-1-7 loss: 0.476783  [  256/  265]
train() client id: f_00001-2-0 loss: 0.405236  [   32/  265]
train() client id: f_00001-2-1 loss: 0.489575  [   64/  265]
train() client id: f_00001-2-2 loss: 0.467090  [   96/  265]
train() client id: f_00001-2-3 loss: 0.438833  [  128/  265]
train() client id: f_00001-2-4 loss: 0.528503  [  160/  265]
train() client id: f_00001-2-5 loss: 0.381505  [  192/  265]
train() client id: f_00001-2-6 loss: 0.428117  [  224/  265]
train() client id: f_00001-2-7 loss: 0.528859  [  256/  265]
train() client id: f_00001-3-0 loss: 0.403017  [   32/  265]
train() client id: f_00001-3-1 loss: 0.494954  [   64/  265]
train() client id: f_00001-3-2 loss: 0.558688  [   96/  265]
train() client id: f_00001-3-3 loss: 0.382940  [  128/  265]
train() client id: f_00001-3-4 loss: 0.388792  [  160/  265]
train() client id: f_00001-3-5 loss: 0.437477  [  192/  265]
train() client id: f_00001-3-6 loss: 0.480236  [  224/  265]
train() client id: f_00001-3-7 loss: 0.531020  [  256/  265]
train() client id: f_00001-4-0 loss: 0.477301  [   32/  265]
train() client id: f_00001-4-1 loss: 0.445284  [   64/  265]
train() client id: f_00001-4-2 loss: 0.501958  [   96/  265]
train() client id: f_00001-4-3 loss: 0.382802  [  128/  265]
train() client id: f_00001-4-4 loss: 0.528771  [  160/  265]
train() client id: f_00001-4-5 loss: 0.427085  [  192/  265]
train() client id: f_00001-4-6 loss: 0.440510  [  224/  265]
train() client id: f_00001-4-7 loss: 0.437166  [  256/  265]
train() client id: f_00001-5-0 loss: 0.356774  [   32/  265]
train() client id: f_00001-5-1 loss: 0.512551  [   64/  265]
train() client id: f_00001-5-2 loss: 0.332017  [   96/  265]
train() client id: f_00001-5-3 loss: 0.405394  [  128/  265]
train() client id: f_00001-5-4 loss: 0.449459  [  160/  265]
train() client id: f_00001-5-5 loss: 0.401597  [  192/  265]
train() client id: f_00001-5-6 loss: 0.515019  [  224/  265]
train() client id: f_00001-5-7 loss: 0.648321  [  256/  265]
train() client id: f_00001-6-0 loss: 0.371254  [   32/  265]
train() client id: f_00001-6-1 loss: 0.486838  [   64/  265]
train() client id: f_00001-6-2 loss: 0.483614  [   96/  265]
train() client id: f_00001-6-3 loss: 0.391314  [  128/  265]
train() client id: f_00001-6-4 loss: 0.440636  [  160/  265]
train() client id: f_00001-6-5 loss: 0.385471  [  192/  265]
train() client id: f_00001-6-6 loss: 0.595880  [  224/  265]
train() client id: f_00001-6-7 loss: 0.414903  [  256/  265]
train() client id: f_00001-7-0 loss: 0.449035  [   32/  265]
train() client id: f_00001-7-1 loss: 0.575162  [   64/  265]
train() client id: f_00001-7-2 loss: 0.392926  [   96/  265]
train() client id: f_00001-7-3 loss: 0.413515  [  128/  265]
train() client id: f_00001-7-4 loss: 0.524085  [  160/  265]
train() client id: f_00001-7-5 loss: 0.395720  [  192/  265]
train() client id: f_00001-7-6 loss: 0.372570  [  224/  265]
train() client id: f_00001-7-7 loss: 0.490444  [  256/  265]
train() client id: f_00001-8-0 loss: 0.435474  [   32/  265]
train() client id: f_00001-8-1 loss: 0.413041  [   64/  265]
train() client id: f_00001-8-2 loss: 0.535626  [   96/  265]
train() client id: f_00001-8-3 loss: 0.525789  [  128/  265]
train() client id: f_00001-8-4 loss: 0.486929  [  160/  265]
train() client id: f_00001-8-5 loss: 0.400631  [  192/  265]
train() client id: f_00001-8-6 loss: 0.414579  [  224/  265]
train() client id: f_00001-8-7 loss: 0.345343  [  256/  265]
train() client id: f_00001-9-0 loss: 0.476924  [   32/  265]
train() client id: f_00001-9-1 loss: 0.343456  [   64/  265]
train() client id: f_00001-9-2 loss: 0.410206  [   96/  265]
train() client id: f_00001-9-3 loss: 0.435659  [  128/  265]
train() client id: f_00001-9-4 loss: 0.443796  [  160/  265]
train() client id: f_00001-9-5 loss: 0.431459  [  192/  265]
train() client id: f_00001-9-6 loss: 0.456611  [  224/  265]
train() client id: f_00001-9-7 loss: 0.437479  [  256/  265]
train() client id: f_00001-10-0 loss: 0.377271  [   32/  265]
train() client id: f_00001-10-1 loss: 0.484759  [   64/  265]
train() client id: f_00001-10-2 loss: 0.452411  [   96/  265]
train() client id: f_00001-10-3 loss: 0.487607  [  128/  265]
train() client id: f_00001-10-4 loss: 0.555174  [  160/  265]
train() client id: f_00001-10-5 loss: 0.359031  [  192/  265]
train() client id: f_00001-10-6 loss: 0.431118  [  224/  265]
train() client id: f_00001-10-7 loss: 0.368114  [  256/  265]
train() client id: f_00001-11-0 loss: 0.384271  [   32/  265]
train() client id: f_00001-11-1 loss: 0.442214  [   64/  265]
train() client id: f_00001-11-2 loss: 0.471249  [   96/  265]
train() client id: f_00001-11-3 loss: 0.397152  [  128/  265]
train() client id: f_00001-11-4 loss: 0.450388  [  160/  265]
train() client id: f_00001-11-5 loss: 0.477042  [  192/  265]
train() client id: f_00001-11-6 loss: 0.439285  [  224/  265]
train() client id: f_00001-11-7 loss: 0.535527  [  256/  265]
train() client id: f_00002-0-0 loss: 1.094461  [   32/  124]
train() client id: f_00002-0-1 loss: 1.327328  [   64/  124]
train() client id: f_00002-0-2 loss: 1.212735  [   96/  124]
train() client id: f_00002-1-0 loss: 1.059748  [   32/  124]
train() client id: f_00002-1-1 loss: 1.055465  [   64/  124]
train() client id: f_00002-1-2 loss: 1.203832  [   96/  124]
train() client id: f_00002-2-0 loss: 1.069809  [   32/  124]
train() client id: f_00002-2-1 loss: 0.954669  [   64/  124]
train() client id: f_00002-2-2 loss: 1.172846  [   96/  124]
train() client id: f_00002-3-0 loss: 0.988038  [   32/  124]
train() client id: f_00002-3-1 loss: 1.068066  [   64/  124]
train() client id: f_00002-3-2 loss: 1.114466  [   96/  124]
train() client id: f_00002-4-0 loss: 1.054968  [   32/  124]
train() client id: f_00002-4-1 loss: 0.967102  [   64/  124]
train() client id: f_00002-4-2 loss: 1.066426  [   96/  124]
train() client id: f_00002-5-0 loss: 1.073410  [   32/  124]
train() client id: f_00002-5-1 loss: 1.001649  [   64/  124]
train() client id: f_00002-5-2 loss: 1.041464  [   96/  124]
train() client id: f_00002-6-0 loss: 0.980630  [   32/  124]
train() client id: f_00002-6-1 loss: 1.189253  [   64/  124]
train() client id: f_00002-6-2 loss: 0.804544  [   96/  124]
train() client id: f_00002-7-0 loss: 0.922839  [   32/  124]
train() client id: f_00002-7-1 loss: 0.954845  [   64/  124]
train() client id: f_00002-7-2 loss: 1.030490  [   96/  124]
train() client id: f_00002-8-0 loss: 0.897108  [   32/  124]
train() client id: f_00002-8-1 loss: 1.019442  [   64/  124]
train() client id: f_00002-8-2 loss: 0.809717  [   96/  124]
train() client id: f_00002-9-0 loss: 1.034678  [   32/  124]
train() client id: f_00002-9-1 loss: 0.865248  [   64/  124]
train() client id: f_00002-9-2 loss: 0.931147  [   96/  124]
train() client id: f_00002-10-0 loss: 0.963395  [   32/  124]
train() client id: f_00002-10-1 loss: 0.862905  [   64/  124]
train() client id: f_00002-10-2 loss: 0.976715  [   96/  124]
train() client id: f_00002-11-0 loss: 0.795261  [   32/  124]
train() client id: f_00002-11-1 loss: 1.066454  [   64/  124]
train() client id: f_00002-11-2 loss: 0.903003  [   96/  124]
train() client id: f_00003-0-0 loss: 0.534243  [   32/   43]
train() client id: f_00003-1-0 loss: 0.753252  [   32/   43]
train() client id: f_00003-2-0 loss: 0.663696  [   32/   43]
train() client id: f_00003-3-0 loss: 0.787261  [   32/   43]
train() client id: f_00003-4-0 loss: 0.718635  [   32/   43]
train() client id: f_00003-5-0 loss: 0.649629  [   32/   43]
train() client id: f_00003-6-0 loss: 0.668197  [   32/   43]
train() client id: f_00003-7-0 loss: 0.569364  [   32/   43]
train() client id: f_00003-8-0 loss: 0.652286  [   32/   43]
train() client id: f_00003-9-0 loss: 0.500039  [   32/   43]
train() client id: f_00003-10-0 loss: 0.646239  [   32/   43]
train() client id: f_00003-11-0 loss: 0.816097  [   32/   43]
train() client id: f_00004-0-0 loss: 0.987701  [   32/  306]
train() client id: f_00004-0-1 loss: 0.893332  [   64/  306]
train() client id: f_00004-0-2 loss: 1.027556  [   96/  306]
train() client id: f_00004-0-3 loss: 0.883262  [  128/  306]
train() client id: f_00004-0-4 loss: 0.819426  [  160/  306]
train() client id: f_00004-0-5 loss: 0.769178  [  192/  306]
train() client id: f_00004-0-6 loss: 0.832808  [  224/  306]
train() client id: f_00004-0-7 loss: 0.842551  [  256/  306]
train() client id: f_00004-0-8 loss: 1.022405  [  288/  306]
train() client id: f_00004-1-0 loss: 0.812171  [   32/  306]
train() client id: f_00004-1-1 loss: 0.964786  [   64/  306]
train() client id: f_00004-1-2 loss: 0.865435  [   96/  306]
train() client id: f_00004-1-3 loss: 0.951640  [  128/  306]
train() client id: f_00004-1-4 loss: 0.748960  [  160/  306]
train() client id: f_00004-1-5 loss: 1.084503  [  192/  306]
train() client id: f_00004-1-6 loss: 0.843710  [  224/  306]
train() client id: f_00004-1-7 loss: 0.787988  [  256/  306]
train() client id: f_00004-1-8 loss: 0.891416  [  288/  306]
train() client id: f_00004-2-0 loss: 0.816514  [   32/  306]
train() client id: f_00004-2-1 loss: 0.801446  [   64/  306]
train() client id: f_00004-2-2 loss: 0.930805  [   96/  306]
train() client id: f_00004-2-3 loss: 0.895049  [  128/  306]
train() client id: f_00004-2-4 loss: 0.774845  [  160/  306]
train() client id: f_00004-2-5 loss: 1.040785  [  192/  306]
train() client id: f_00004-2-6 loss: 0.841621  [  224/  306]
train() client id: f_00004-2-7 loss: 0.894429  [  256/  306]
train() client id: f_00004-2-8 loss: 0.922122  [  288/  306]
train() client id: f_00004-3-0 loss: 0.921130  [   32/  306]
train() client id: f_00004-3-1 loss: 1.015888  [   64/  306]
train() client id: f_00004-3-2 loss: 0.826459  [   96/  306]
train() client id: f_00004-3-3 loss: 0.723795  [  128/  306]
train() client id: f_00004-3-4 loss: 0.874691  [  160/  306]
train() client id: f_00004-3-5 loss: 0.948592  [  192/  306]
train() client id: f_00004-3-6 loss: 0.941391  [  224/  306]
train() client id: f_00004-3-7 loss: 0.776968  [  256/  306]
train() client id: f_00004-3-8 loss: 0.919619  [  288/  306]
train() client id: f_00004-4-0 loss: 0.922927  [   32/  306]
train() client id: f_00004-4-1 loss: 0.914477  [   64/  306]
train() client id: f_00004-4-2 loss: 0.823831  [   96/  306]
train() client id: f_00004-4-3 loss: 0.862473  [  128/  306]
train() client id: f_00004-4-4 loss: 0.902840  [  160/  306]
train() client id: f_00004-4-5 loss: 0.994269  [  192/  306]
train() client id: f_00004-4-6 loss: 0.976536  [  224/  306]
train() client id: f_00004-4-7 loss: 0.837811  [  256/  306]
train() client id: f_00004-4-8 loss: 0.730443  [  288/  306]
train() client id: f_00004-5-0 loss: 0.783312  [   32/  306]
train() client id: f_00004-5-1 loss: 0.925264  [   64/  306]
train() client id: f_00004-5-2 loss: 0.914469  [   96/  306]
train() client id: f_00004-5-3 loss: 0.831036  [  128/  306]
train() client id: f_00004-5-4 loss: 0.892397  [  160/  306]
train() client id: f_00004-5-5 loss: 0.903142  [  192/  306]
train() client id: f_00004-5-6 loss: 0.843005  [  224/  306]
train() client id: f_00004-5-7 loss: 0.853520  [  256/  306]
train() client id: f_00004-5-8 loss: 0.954017  [  288/  306]
train() client id: f_00004-6-0 loss: 0.900738  [   32/  306]
train() client id: f_00004-6-1 loss: 0.796839  [   64/  306]
train() client id: f_00004-6-2 loss: 0.879334  [   96/  306]
train() client id: f_00004-6-3 loss: 0.908963  [  128/  306]
train() client id: f_00004-6-4 loss: 0.794362  [  160/  306]
train() client id: f_00004-6-5 loss: 0.795840  [  192/  306]
train() client id: f_00004-6-6 loss: 0.955112  [  224/  306]
train() client id: f_00004-6-7 loss: 0.981988  [  256/  306]
train() client id: f_00004-6-8 loss: 0.769480  [  288/  306]
train() client id: f_00004-7-0 loss: 0.861117  [   32/  306]
train() client id: f_00004-7-1 loss: 0.958080  [   64/  306]
train() client id: f_00004-7-2 loss: 0.894253  [   96/  306]
train() client id: f_00004-7-3 loss: 0.824041  [  128/  306]
train() client id: f_00004-7-4 loss: 0.894017  [  160/  306]
train() client id: f_00004-7-5 loss: 0.743162  [  192/  306]
train() client id: f_00004-7-6 loss: 0.913106  [  224/  306]
train() client id: f_00004-7-7 loss: 0.906120  [  256/  306]
train() client id: f_00004-7-8 loss: 0.836890  [  288/  306]
train() client id: f_00004-8-0 loss: 0.805947  [   32/  306]
train() client id: f_00004-8-1 loss: 0.718177  [   64/  306]
train() client id: f_00004-8-2 loss: 0.907856  [   96/  306]
train() client id: f_00004-8-3 loss: 0.885715  [  128/  306]
train() client id: f_00004-8-4 loss: 0.852193  [  160/  306]
train() client id: f_00004-8-5 loss: 1.029795  [  192/  306]
train() client id: f_00004-8-6 loss: 0.901543  [  224/  306]
train() client id: f_00004-8-7 loss: 0.838025  [  256/  306]
train() client id: f_00004-8-8 loss: 0.904654  [  288/  306]
train() client id: f_00004-9-0 loss: 0.783671  [   32/  306]
train() client id: f_00004-9-1 loss: 0.877169  [   64/  306]
train() client id: f_00004-9-2 loss: 0.868550  [   96/  306]
train() client id: f_00004-9-3 loss: 0.916192  [  128/  306]
train() client id: f_00004-9-4 loss: 0.976945  [  160/  306]
train() client id: f_00004-9-5 loss: 0.888314  [  192/  306]
train() client id: f_00004-9-6 loss: 0.902278  [  224/  306]
train() client id: f_00004-9-7 loss: 0.748699  [  256/  306]
train() client id: f_00004-9-8 loss: 0.890113  [  288/  306]
train() client id: f_00004-10-0 loss: 0.858823  [   32/  306]
train() client id: f_00004-10-1 loss: 0.986431  [   64/  306]
train() client id: f_00004-10-2 loss: 0.948133  [   96/  306]
train() client id: f_00004-10-3 loss: 0.823820  [  128/  306]
train() client id: f_00004-10-4 loss: 0.790754  [  160/  306]
train() client id: f_00004-10-5 loss: 0.773609  [  192/  306]
train() client id: f_00004-10-6 loss: 0.973855  [  224/  306]
train() client id: f_00004-10-7 loss: 0.810652  [  256/  306]
train() client id: f_00004-10-8 loss: 0.758110  [  288/  306]
train() client id: f_00004-11-0 loss: 0.881851  [   32/  306]
train() client id: f_00004-11-1 loss: 0.989158  [   64/  306]
train() client id: f_00004-11-2 loss: 0.760613  [   96/  306]
train() client id: f_00004-11-3 loss: 0.902046  [  128/  306]
train() client id: f_00004-11-4 loss: 0.807282  [  160/  306]
train() client id: f_00004-11-5 loss: 0.948566  [  192/  306]
train() client id: f_00004-11-6 loss: 0.920354  [  224/  306]
train() client id: f_00004-11-7 loss: 0.777317  [  256/  306]
train() client id: f_00004-11-8 loss: 0.849873  [  288/  306]
train() client id: f_00005-0-0 loss: 0.555529  [   32/  146]
train() client id: f_00005-0-1 loss: 0.820613  [   64/  146]
train() client id: f_00005-0-2 loss: 0.666960  [   96/  146]
train() client id: f_00005-0-3 loss: 0.533060  [  128/  146]
train() client id: f_00005-1-0 loss: 1.022149  [   32/  146]
train() client id: f_00005-1-1 loss: 0.574272  [   64/  146]
train() client id: f_00005-1-2 loss: 0.650021  [   96/  146]
train() client id: f_00005-1-3 loss: 0.603197  [  128/  146]
train() client id: f_00005-2-0 loss: 0.561879  [   32/  146]
train() client id: f_00005-2-1 loss: 0.734654  [   64/  146]
train() client id: f_00005-2-2 loss: 0.688338  [   96/  146]
train() client id: f_00005-2-3 loss: 0.682750  [  128/  146]
train() client id: f_00005-3-0 loss: 0.846006  [   32/  146]
train() client id: f_00005-3-1 loss: 0.554203  [   64/  146]
train() client id: f_00005-3-2 loss: 0.700799  [   96/  146]
train() client id: f_00005-3-3 loss: 0.691057  [  128/  146]
train() client id: f_00005-4-0 loss: 0.761114  [   32/  146]
train() client id: f_00005-4-1 loss: 0.712305  [   64/  146]
train() client id: f_00005-4-2 loss: 0.565396  [   96/  146]
train() client id: f_00005-4-3 loss: 0.693024  [  128/  146]
train() client id: f_00005-5-0 loss: 0.727186  [   32/  146]
train() client id: f_00005-5-1 loss: 0.481113  [   64/  146]
train() client id: f_00005-5-2 loss: 0.553483  [   96/  146]
train() client id: f_00005-5-3 loss: 0.683929  [  128/  146]
train() client id: f_00005-6-0 loss: 0.499914  [   32/  146]
train() client id: f_00005-6-1 loss: 0.916032  [   64/  146]
train() client id: f_00005-6-2 loss: 0.634632  [   96/  146]
train() client id: f_00005-6-3 loss: 0.617563  [  128/  146]
train() client id: f_00005-7-0 loss: 0.814950  [   32/  146]
train() client id: f_00005-7-1 loss: 0.716040  [   64/  146]
train() client id: f_00005-7-2 loss: 0.425421  [   96/  146]
train() client id: f_00005-7-3 loss: 0.752572  [  128/  146]
train() client id: f_00005-8-0 loss: 0.566263  [   32/  146]
train() client id: f_00005-8-1 loss: 0.914006  [   64/  146]
train() client id: f_00005-8-2 loss: 0.584782  [   96/  146]
train() client id: f_00005-8-3 loss: 0.620943  [  128/  146]
train() client id: f_00005-9-0 loss: 0.696871  [   32/  146]
train() client id: f_00005-9-1 loss: 0.698661  [   64/  146]
train() client id: f_00005-9-2 loss: 0.586227  [   96/  146]
train() client id: f_00005-9-3 loss: 0.721930  [  128/  146]
train() client id: f_00005-10-0 loss: 0.592585  [   32/  146]
train() client id: f_00005-10-1 loss: 0.616984  [   64/  146]
train() client id: f_00005-10-2 loss: 0.682389  [   96/  146]
train() client id: f_00005-10-3 loss: 0.904888  [  128/  146]
train() client id: f_00005-11-0 loss: 0.712977  [   32/  146]
train() client id: f_00005-11-1 loss: 0.365343  [   64/  146]
train() client id: f_00005-11-2 loss: 0.683564  [   96/  146]
train() client id: f_00005-11-3 loss: 0.689803  [  128/  146]
train() client id: f_00006-0-0 loss: 0.548316  [   32/   54]
train() client id: f_00006-1-0 loss: 0.503341  [   32/   54]
train() client id: f_00006-2-0 loss: 0.492701  [   32/   54]
train() client id: f_00006-3-0 loss: 0.531840  [   32/   54]
train() client id: f_00006-4-0 loss: 0.474623  [   32/   54]
train() client id: f_00006-5-0 loss: 0.520311  [   32/   54]
train() client id: f_00006-6-0 loss: 0.556978  [   32/   54]
train() client id: f_00006-7-0 loss: 0.521796  [   32/   54]
train() client id: f_00006-8-0 loss: 0.516338  [   32/   54]
train() client id: f_00006-9-0 loss: 0.544414  [   32/   54]
train() client id: f_00006-10-0 loss: 0.549069  [   32/   54]
train() client id: f_00006-11-0 loss: 0.560763  [   32/   54]
train() client id: f_00007-0-0 loss: 0.753764  [   32/  179]
train() client id: f_00007-0-1 loss: 0.682889  [   64/  179]
train() client id: f_00007-0-2 loss: 0.752742  [   96/  179]
train() client id: f_00007-0-3 loss: 0.574698  [  128/  179]
train() client id: f_00007-0-4 loss: 0.736658  [  160/  179]
train() client id: f_00007-1-0 loss: 0.580252  [   32/  179]
train() client id: f_00007-1-1 loss: 0.688328  [   64/  179]
train() client id: f_00007-1-2 loss: 0.538689  [   96/  179]
train() client id: f_00007-1-3 loss: 0.764961  [  128/  179]
train() client id: f_00007-1-4 loss: 0.892525  [  160/  179]
train() client id: f_00007-2-0 loss: 0.758791  [   32/  179]
train() client id: f_00007-2-1 loss: 0.618888  [   64/  179]
train() client id: f_00007-2-2 loss: 0.573309  [   96/  179]
train() client id: f_00007-2-3 loss: 0.869890  [  128/  179]
train() client id: f_00007-2-4 loss: 0.744263  [  160/  179]
train() client id: f_00007-3-0 loss: 0.602340  [   32/  179]
train() client id: f_00007-3-1 loss: 0.696075  [   64/  179]
train() client id: f_00007-3-2 loss: 0.685612  [   96/  179]
train() client id: f_00007-3-3 loss: 0.618356  [  128/  179]
train() client id: f_00007-3-4 loss: 0.904629  [  160/  179]
train() client id: f_00007-4-0 loss: 0.543426  [   32/  179]
train() client id: f_00007-4-1 loss: 0.725194  [   64/  179]
train() client id: f_00007-4-2 loss: 0.701049  [   96/  179]
train() client id: f_00007-4-3 loss: 0.710907  [  128/  179]
train() client id: f_00007-4-4 loss: 0.667559  [  160/  179]
train() client id: f_00007-5-0 loss: 0.678644  [   32/  179]
train() client id: f_00007-5-1 loss: 0.635568  [   64/  179]
train() client id: f_00007-5-2 loss: 0.649889  [   96/  179]
train() client id: f_00007-5-3 loss: 0.729377  [  128/  179]
train() client id: f_00007-5-4 loss: 0.776981  [  160/  179]
train() client id: f_00007-6-0 loss: 0.936564  [   32/  179]
train() client id: f_00007-6-1 loss: 0.554175  [   64/  179]
train() client id: f_00007-6-2 loss: 0.634029  [   96/  179]
train() client id: f_00007-6-3 loss: 0.686318  [  128/  179]
train() client id: f_00007-6-4 loss: 0.649829  [  160/  179]
train() client id: f_00007-7-0 loss: 0.604465  [   32/  179]
train() client id: f_00007-7-1 loss: 0.716190  [   64/  179]
train() client id: f_00007-7-2 loss: 0.577469  [   96/  179]
train() client id: f_00007-7-3 loss: 1.033138  [  128/  179]
train() client id: f_00007-7-4 loss: 0.527539  [  160/  179]
train() client id: f_00007-8-0 loss: 0.610225  [   32/  179]
train() client id: f_00007-8-1 loss: 0.686686  [   64/  179]
train() client id: f_00007-8-2 loss: 0.878155  [   96/  179]
train() client id: f_00007-8-3 loss: 0.671726  [  128/  179]
train() client id: f_00007-8-4 loss: 0.625008  [  160/  179]
train() client id: f_00007-9-0 loss: 0.530217  [   32/  179]
train() client id: f_00007-9-1 loss: 0.622664  [   64/  179]
train() client id: f_00007-9-2 loss: 0.944797  [   96/  179]
train() client id: f_00007-9-3 loss: 0.716352  [  128/  179]
train() client id: f_00007-9-4 loss: 0.625981  [  160/  179]
train() client id: f_00007-10-0 loss: 0.681485  [   32/  179]
train() client id: f_00007-10-1 loss: 0.563265  [   64/  179]
train() client id: f_00007-10-2 loss: 0.851893  [   96/  179]
train() client id: f_00007-10-3 loss: 0.733094  [  128/  179]
train() client id: f_00007-10-4 loss: 0.524076  [  160/  179]
train() client id: f_00007-11-0 loss: 0.529601  [   32/  179]
train() client id: f_00007-11-1 loss: 0.679991  [   64/  179]
train() client id: f_00007-11-2 loss: 0.590690  [   96/  179]
train() client id: f_00007-11-3 loss: 0.613310  [  128/  179]
train() client id: f_00007-11-4 loss: 0.821453  [  160/  179]
train() client id: f_00008-0-0 loss: 0.758109  [   32/  130]
train() client id: f_00008-0-1 loss: 0.719273  [   64/  130]
train() client id: f_00008-0-2 loss: 0.672779  [   96/  130]
train() client id: f_00008-0-3 loss: 0.686999  [  128/  130]
train() client id: f_00008-1-0 loss: 0.719060  [   32/  130]
train() client id: f_00008-1-1 loss: 0.908026  [   64/  130]
train() client id: f_00008-1-2 loss: 0.656259  [   96/  130]
train() client id: f_00008-1-3 loss: 0.564180  [  128/  130]
train() client id: f_00008-2-0 loss: 0.668010  [   32/  130]
train() client id: f_00008-2-1 loss: 0.716290  [   64/  130]
train() client id: f_00008-2-2 loss: 0.682820  [   96/  130]
train() client id: f_00008-2-3 loss: 0.755030  [  128/  130]
train() client id: f_00008-3-0 loss: 0.648931  [   32/  130]
train() client id: f_00008-3-1 loss: 0.715169  [   64/  130]
train() client id: f_00008-3-2 loss: 0.663333  [   96/  130]
train() client id: f_00008-3-3 loss: 0.751593  [  128/  130]
train() client id: f_00008-4-0 loss: 0.630772  [   32/  130]
train() client id: f_00008-4-1 loss: 0.725717  [   64/  130]
train() client id: f_00008-4-2 loss: 0.652779  [   96/  130]
train() client id: f_00008-4-3 loss: 0.792993  [  128/  130]
train() client id: f_00008-5-0 loss: 0.639507  [   32/  130]
train() client id: f_00008-5-1 loss: 0.798470  [   64/  130]
train() client id: f_00008-5-2 loss: 0.757433  [   96/  130]
train() client id: f_00008-5-3 loss: 0.613723  [  128/  130]
train() client id: f_00008-6-0 loss: 0.698943  [   32/  130]
train() client id: f_00008-6-1 loss: 0.670366  [   64/  130]
train() client id: f_00008-6-2 loss: 0.743531  [   96/  130]
train() client id: f_00008-6-3 loss: 0.716511  [  128/  130]
train() client id: f_00008-7-0 loss: 0.705446  [   32/  130]
train() client id: f_00008-7-1 loss: 0.826972  [   64/  130]
train() client id: f_00008-7-2 loss: 0.598209  [   96/  130]
train() client id: f_00008-7-3 loss: 0.682834  [  128/  130]
train() client id: f_00008-8-0 loss: 0.683900  [   32/  130]
train() client id: f_00008-8-1 loss: 0.597223  [   64/  130]
train() client id: f_00008-8-2 loss: 0.716092  [   96/  130]
train() client id: f_00008-8-3 loss: 0.825259  [  128/  130]
train() client id: f_00008-9-0 loss: 0.669085  [   32/  130]
train() client id: f_00008-9-1 loss: 0.709003  [   64/  130]
train() client id: f_00008-9-2 loss: 0.816318  [   96/  130]
train() client id: f_00008-9-3 loss: 0.637741  [  128/  130]
train() client id: f_00008-10-0 loss: 0.732790  [   32/  130]
train() client id: f_00008-10-1 loss: 0.700530  [   64/  130]
train() client id: f_00008-10-2 loss: 0.634088  [   96/  130]
train() client id: f_00008-10-3 loss: 0.740673  [  128/  130]
train() client id: f_00008-11-0 loss: 0.741964  [   32/  130]
train() client id: f_00008-11-1 loss: 0.690415  [   64/  130]
train() client id: f_00008-11-2 loss: 0.650261  [   96/  130]
train() client id: f_00008-11-3 loss: 0.751573  [  128/  130]
train() client id: f_00009-0-0 loss: 0.960331  [   32/  118]
train() client id: f_00009-0-1 loss: 1.072395  [   64/  118]
train() client id: f_00009-0-2 loss: 1.024426  [   96/  118]
train() client id: f_00009-1-0 loss: 0.933921  [   32/  118]
train() client id: f_00009-1-1 loss: 1.045691  [   64/  118]
train() client id: f_00009-1-2 loss: 0.830535  [   96/  118]
train() client id: f_00009-2-0 loss: 0.908238  [   32/  118]
train() client id: f_00009-2-1 loss: 0.955491  [   64/  118]
train() client id: f_00009-2-2 loss: 0.857352  [   96/  118]
train() client id: f_00009-3-0 loss: 0.889958  [   32/  118]
train() client id: f_00009-3-1 loss: 0.915208  [   64/  118]
train() client id: f_00009-3-2 loss: 1.002811  [   96/  118]
train() client id: f_00009-4-0 loss: 0.935372  [   32/  118]
train() client id: f_00009-4-1 loss: 0.977421  [   64/  118]
train() client id: f_00009-4-2 loss: 0.759018  [   96/  118]
train() client id: f_00009-5-0 loss: 0.836128  [   32/  118]
train() client id: f_00009-5-1 loss: 1.022728  [   64/  118]
train() client id: f_00009-5-2 loss: 0.758726  [   96/  118]
train() client id: f_00009-6-0 loss: 0.867597  [   32/  118]
train() client id: f_00009-6-1 loss: 0.732455  [   64/  118]
train() client id: f_00009-6-2 loss: 0.755007  [   96/  118]
train() client id: f_00009-7-0 loss: 0.753645  [   32/  118]
train() client id: f_00009-7-1 loss: 0.837510  [   64/  118]
train() client id: f_00009-7-2 loss: 0.869902  [   96/  118]
train() client id: f_00009-8-0 loss: 0.842634  [   32/  118]
train() client id: f_00009-8-1 loss: 0.821225  [   64/  118]
train() client id: f_00009-8-2 loss: 0.763847  [   96/  118]
train() client id: f_00009-9-0 loss: 0.865949  [   32/  118]
train() client id: f_00009-9-1 loss: 0.868518  [   64/  118]
train() client id: f_00009-9-2 loss: 0.707418  [   96/  118]
train() client id: f_00009-10-0 loss: 0.825600  [   32/  118]
train() client id: f_00009-10-1 loss: 0.845686  [   64/  118]
train() client id: f_00009-10-2 loss: 0.936357  [   96/  118]
train() client id: f_00009-11-0 loss: 0.729035  [   32/  118]
train() client id: f_00009-11-1 loss: 0.897019  [   64/  118]
train() client id: f_00009-11-2 loss: 0.781453  [   96/  118]
At round 46 accuracy: 0.6445623342175066
At round 46 training accuracy: 0.5888665325285044
At round 46 training loss: 0.8385371298977067
update_location
xs = [  -3.9056584     4.20031788  250.00902392   18.81129433    0.97929623
    3.95640986 -212.44319194 -191.32485185  234.66397685 -177.06087855]
ys = [ 242.5879595   225.55583871    1.32061395 -212.45517586  204.35018685
  187.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [262.41983969 246.76523063 269.26985732 235.56541882 227.50814905
 212.81402303 234.81695071 215.88393917 255.68701962 203.38772473]
dists_bs = [183.9603381  186.19518456 459.12656158 433.22549684 178.41005064
 179.41460111 181.52858639 174.94100506 438.8725234  171.01093665]
uav_gains = [5.77502052e-12 7.90037418e-12 5.00729947e-12 9.71837173e-12
 1.11658603e-11 1.41063225e-11 9.84803495e-12 1.34576919e-11
 6.62667355e-12 1.62319850e-11]
bs_gains = [5.03577512e-11 4.86835724e-11 3.88940630e-12 4.57609321e-12
 5.48681098e-11 5.40122526e-11 5.22694600e-11 5.79692334e-11
 4.41312865e-12 6.17770556e-11]
Round 47
-------------------------------
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.7932913   9.84287299  4.72759004  1.71436679 11.34993316  5.46069336
  2.11875437  6.70878283  4.95608411  4.42774153]
obj_prev = 56.1001104698108
eta_min = 5.969830954362688e-20	eta_max = 0.9382327126425405
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 12.978344451772108	eta = 0.9090909090909091
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 25.625037120999544	eta = 0.4604284044719558
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 19.192856434060143	eta = 0.6147336638864522
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 18.0285169675076	eta = 0.6544351361468406
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 17.963027394397052	eta = 0.6568210745943969
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 17.962800013672382	eta = 0.6568293889135345
eta = 0.6568293889135345
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [0.0346847  0.07294799 0.03413415 0.01183684 0.08423428 0.04019021
 0.01486487 0.0492743  0.03578581 0.0324825 ]
ene_total = [1.66925633 2.82340307 1.67271292 0.79630461 3.21465228 1.66466235
 0.90143074 2.08079959 1.75289976 1.38667837]
ti_comp = [0.65804615 0.71288711 0.65155892 0.67749658 0.71463044 0.71440598
 0.67791976 0.68705877 0.6453014  0.71627974]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [6.02256253e-06 4.77395682e-05 5.85517725e-06 2.25825789e-07
 7.31448314e-05 7.94970091e-06 4.46690869e-07 1.58399461e-05
 6.87839679e-06 4.17506343e-06]
ene_total = [0.44822894 0.2566042  0.4710644  0.37954037 0.25136048 0.24985529
 0.37805813 0.34642191 0.493133   0.24312494]
optimize_network_iter = 0 obj = 3.5173916587449052
eta = 0.6568293889135345
freqs = [26354310.22875877 51163774.23769676 26194218.65211249  8735722.05600123
 58935554.78523942 28128413.29923534 10963593.51009882 35858870.20763274
 27727978.21502854 22674452.1605431 ]
eta_min = 0.6568293889135383	eta_max = 0.6851522482532448
af = 0.0048373581457900995	bf = 1.2050321534900845	zeta = 0.00532109396036911	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [1.36561793e-06 1.08249620e-05 1.32766327e-06 5.12060679e-08
 1.65856133e-05 1.80259716e-06 1.01287294e-07 3.59171272e-06
 1.55967862e-06 9.46696933e-07]
ene_total = [1.69139003 0.96374279 1.77760827 1.4326949  0.9413374  0.94235589
 1.42707694 1.30607207 1.86080934 0.91733759]
ti_comp = [0.59323403 0.64807499 0.5867468  0.61268446 0.64981832 0.64959386
 0.61310764 0.62224665 0.58048928 0.65146762]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.39431995e-06 4.20498333e-05 5.25582343e-06 2.01005902e-07
 6.43958017e-05 6.99925668e-06 3.97543809e-07 1.40575984e-05
 6.18755144e-06 3.67398631e-06]
ene_total = [0.4885263  0.27946926 0.51341709 0.41368185 0.27363643 0.27229513
 0.41206534 0.3775167  0.53746741 0.26497658]
optimize_network_iter = 1 obj = 3.8330520913049613
eta = 0.6851522482532448
freqs = [26297444.52410097 50627871.73999517 26166161.04026021  8689617.22720056
 58304024.4554098  27827872.27421467 10905010.46336445 35617179.03212754
 27727978.21502853 22426331.23656411]
eta_min = 0.6851522482532453	eta_max = 0.6851522482532441
af = 0.0047490407946122245	bf = 1.2050321534900845	zeta = 0.005223944874073448	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [1.35973100e-06 1.05993827e-05 1.32482057e-06 5.06669901e-08
 1.62320679e-05 1.76428286e-06 1.00207745e-07 3.54345912e-06
 1.55967862e-06 9.26091353e-07]
ene_total = [1.69138925 0.96371281 1.77760789 1.43269482 0.94129041 0.9423508
 1.4270768  1.30606566 1.86080934 0.91733486]
ti_comp = [0.59323403 0.64807499 0.5867468  0.61268446 0.64981832 0.64959386
 0.61310764 0.62224665 0.58048928 0.65146762]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.39431995e-06 4.20498333e-05 5.25582343e-06 2.01005902e-07
 6.43958017e-05 6.99925668e-06 3.97543809e-07 1.40575984e-05
 6.18755144e-06 3.67398631e-06]
ene_total = [0.4885263  0.27946926 0.51341709 0.41368185 0.27363643 0.27229513
 0.41206534 0.3775167  0.53746741 0.26497658]
optimize_network_iter = 2 obj = 3.8330520913049524
eta = 0.6851522482532441
freqs = [26297444.52410097 50627871.73999519 26166161.04026021  8689617.22720056
 58304024.45540981 27827872.27421467 10905010.46336445 35617179.03212754
 27727978.21502853 22426331.23656412]
Done!
At round 47 eta: 0.6851522482532441
At round 47 local rounds: 12.381380128537735
At round 47 global rounds: 38.37711395955986
At round 47 a_n: 11.740402201727147
gradient difference: 0.47827014327049255
train() client id: f_00000-0-0 loss: 1.264235  [   32/  126]
train() client id: f_00000-0-1 loss: 1.231356  [   64/  126]
train() client id: f_00000-0-2 loss: 1.285125  [   96/  126]
train() client id: f_00000-1-0 loss: 1.092836  [   32/  126]
train() client id: f_00000-1-1 loss: 1.078693  [   64/  126]
train() client id: f_00000-1-2 loss: 1.140354  [   96/  126]
train() client id: f_00000-2-0 loss: 1.092107  [   32/  126]
train() client id: f_00000-2-1 loss: 1.204620  [   64/  126]
train() client id: f_00000-2-2 loss: 0.886202  [   96/  126]
train() client id: f_00000-3-0 loss: 1.064045  [   32/  126]
train() client id: f_00000-3-1 loss: 1.055692  [   64/  126]
train() client id: f_00000-3-2 loss: 0.782447  [   96/  126]
train() client id: f_00000-4-0 loss: 0.929739  [   32/  126]
train() client id: f_00000-4-1 loss: 0.883056  [   64/  126]
train() client id: f_00000-4-2 loss: 1.020932  [   96/  126]
train() client id: f_00000-5-0 loss: 0.856919  [   32/  126]
train() client id: f_00000-5-1 loss: 0.818641  [   64/  126]
train() client id: f_00000-5-2 loss: 0.874600  [   96/  126]
train() client id: f_00000-6-0 loss: 0.860309  [   32/  126]
train() client id: f_00000-6-1 loss: 0.891967  [   64/  126]
train() client id: f_00000-6-2 loss: 0.869124  [   96/  126]
train() client id: f_00000-7-0 loss: 0.803722  [   32/  126]
train() client id: f_00000-7-1 loss: 0.744929  [   64/  126]
train() client id: f_00000-7-2 loss: 0.970229  [   96/  126]
train() client id: f_00000-8-0 loss: 0.742188  [   32/  126]
train() client id: f_00000-8-1 loss: 0.758844  [   64/  126]
train() client id: f_00000-8-2 loss: 0.851047  [   96/  126]
train() client id: f_00000-9-0 loss: 0.868925  [   32/  126]
train() client id: f_00000-9-1 loss: 0.877641  [   64/  126]
train() client id: f_00000-9-2 loss: 0.676416  [   96/  126]
train() client id: f_00000-10-0 loss: 0.836225  [   32/  126]
train() client id: f_00000-10-1 loss: 0.763247  [   64/  126]
train() client id: f_00000-10-2 loss: 0.679468  [   96/  126]
train() client id: f_00000-11-0 loss: 0.757154  [   32/  126]
train() client id: f_00000-11-1 loss: 0.757554  [   64/  126]
train() client id: f_00000-11-2 loss: 0.911443  [   96/  126]
train() client id: f_00001-0-0 loss: 0.425981  [   32/  265]
train() client id: f_00001-0-1 loss: 0.287156  [   64/  265]
train() client id: f_00001-0-2 loss: 0.397409  [   96/  265]
train() client id: f_00001-0-3 loss: 0.250027  [  128/  265]
train() client id: f_00001-0-4 loss: 0.431159  [  160/  265]
train() client id: f_00001-0-5 loss: 0.272903  [  192/  265]
train() client id: f_00001-0-6 loss: 0.244422  [  224/  265]
train() client id: f_00001-0-7 loss: 0.438972  [  256/  265]
train() client id: f_00001-1-0 loss: 0.372071  [   32/  265]
train() client id: f_00001-1-1 loss: 0.340114  [   64/  265]
train() client id: f_00001-1-2 loss: 0.362895  [   96/  265]
train() client id: f_00001-1-3 loss: 0.345938  [  128/  265]
train() client id: f_00001-1-4 loss: 0.370805  [  160/  265]
train() client id: f_00001-1-5 loss: 0.237078  [  192/  265]
train() client id: f_00001-1-6 loss: 0.295118  [  224/  265]
train() client id: f_00001-1-7 loss: 0.390690  [  256/  265]
train() client id: f_00001-2-0 loss: 0.379536  [   32/  265]
train() client id: f_00001-2-1 loss: 0.354462  [   64/  265]
train() client id: f_00001-2-2 loss: 0.224512  [   96/  265]
train() client id: f_00001-2-3 loss: 0.304810  [  128/  265]
train() client id: f_00001-2-4 loss: 0.329841  [  160/  265]
train() client id: f_00001-2-5 loss: 0.311580  [  192/  265]
train() client id: f_00001-2-6 loss: 0.435891  [  224/  265]
train() client id: f_00001-2-7 loss: 0.253299  [  256/  265]
train() client id: f_00001-3-0 loss: 0.255388  [   32/  265]
train() client id: f_00001-3-1 loss: 0.396507  [   64/  265]
train() client id: f_00001-3-2 loss: 0.278408  [   96/  265]
train() client id: f_00001-3-3 loss: 0.367481  [  128/  265]
train() client id: f_00001-3-4 loss: 0.287690  [  160/  265]
train() client id: f_00001-3-5 loss: 0.255845  [  192/  265]
train() client id: f_00001-3-6 loss: 0.402708  [  224/  265]
train() client id: f_00001-3-7 loss: 0.304480  [  256/  265]
train() client id: f_00001-4-0 loss: 0.280083  [   32/  265]
train() client id: f_00001-4-1 loss: 0.376793  [   64/  265]
train() client id: f_00001-4-2 loss: 0.377493  [   96/  265]
train() client id: f_00001-4-3 loss: 0.380372  [  128/  265]
train() client id: f_00001-4-4 loss: 0.308735  [  160/  265]
train() client id: f_00001-4-5 loss: 0.230144  [  192/  265]
train() client id: f_00001-4-6 loss: 0.341566  [  224/  265]
train() client id: f_00001-4-7 loss: 0.278158  [  256/  265]
train() client id: f_00001-5-0 loss: 0.241340  [   32/  265]
train() client id: f_00001-5-1 loss: 0.418199  [   64/  265]
train() client id: f_00001-5-2 loss: 0.399274  [   96/  265]
train() client id: f_00001-5-3 loss: 0.274114  [  128/  265]
train() client id: f_00001-5-4 loss: 0.269433  [  160/  265]
train() client id: f_00001-5-5 loss: 0.325044  [  192/  265]
train() client id: f_00001-5-6 loss: 0.299651  [  224/  265]
train() client id: f_00001-5-7 loss: 0.305064  [  256/  265]
train() client id: f_00001-6-0 loss: 0.234122  [   32/  265]
train() client id: f_00001-6-1 loss: 0.227924  [   64/  265]
train() client id: f_00001-6-2 loss: 0.286128  [   96/  265]
train() client id: f_00001-6-3 loss: 0.241215  [  128/  265]
train() client id: f_00001-6-4 loss: 0.378504  [  160/  265]
train() client id: f_00001-6-5 loss: 0.228845  [  192/  265]
train() client id: f_00001-6-6 loss: 0.441624  [  224/  265]
train() client id: f_00001-6-7 loss: 0.422378  [  256/  265]
train() client id: f_00001-7-0 loss: 0.215281  [   32/  265]
train() client id: f_00001-7-1 loss: 0.240923  [   64/  265]
train() client id: f_00001-7-2 loss: 0.215822  [   96/  265]
train() client id: f_00001-7-3 loss: 0.290365  [  128/  265]
train() client id: f_00001-7-4 loss: 0.319217  [  160/  265]
train() client id: f_00001-7-5 loss: 0.466605  [  192/  265]
train() client id: f_00001-7-6 loss: 0.405676  [  224/  265]
train() client id: f_00001-7-7 loss: 0.331518  [  256/  265]
train() client id: f_00001-8-0 loss: 0.211409  [   32/  265]
train() client id: f_00001-8-1 loss: 0.305314  [   64/  265]
train() client id: f_00001-8-2 loss: 0.212813  [   96/  265]
train() client id: f_00001-8-3 loss: 0.239685  [  128/  265]
train() client id: f_00001-8-4 loss: 0.359023  [  160/  265]
train() client id: f_00001-8-5 loss: 0.437222  [  192/  265]
train() client id: f_00001-8-6 loss: 0.383347  [  224/  265]
train() client id: f_00001-8-7 loss: 0.262923  [  256/  265]
train() client id: f_00001-9-0 loss: 0.345172  [   32/  265]
train() client id: f_00001-9-1 loss: 0.357769  [   64/  265]
train() client id: f_00001-9-2 loss: 0.260530  [   96/  265]
train() client id: f_00001-9-3 loss: 0.284223  [  128/  265]
train() client id: f_00001-9-4 loss: 0.329926  [  160/  265]
train() client id: f_00001-9-5 loss: 0.343945  [  192/  265]
train() client id: f_00001-9-6 loss: 0.243856  [  224/  265]
train() client id: f_00001-9-7 loss: 0.314049  [  256/  265]
train() client id: f_00001-10-0 loss: 0.292669  [   32/  265]
train() client id: f_00001-10-1 loss: 0.299596  [   64/  265]
train() client id: f_00001-10-2 loss: 0.298533  [   96/  265]
train() client id: f_00001-10-3 loss: 0.231076  [  128/  265]
train() client id: f_00001-10-4 loss: 0.268984  [  160/  265]
train() client id: f_00001-10-5 loss: 0.369876  [  192/  265]
train() client id: f_00001-10-6 loss: 0.352327  [  224/  265]
train() client id: f_00001-10-7 loss: 0.310054  [  256/  265]
train() client id: f_00001-11-0 loss: 0.503942  [   32/  265]
train() client id: f_00001-11-1 loss: 0.345429  [   64/  265]
train() client id: f_00001-11-2 loss: 0.359011  [   96/  265]
train() client id: f_00001-11-3 loss: 0.254739  [  128/  265]
train() client id: f_00001-11-4 loss: 0.211503  [  160/  265]
train() client id: f_00001-11-5 loss: 0.359840  [  192/  265]
train() client id: f_00001-11-6 loss: 0.212053  [  224/  265]
train() client id: f_00001-11-7 loss: 0.212236  [  256/  265]
train() client id: f_00002-0-0 loss: 1.142046  [   32/  124]
train() client id: f_00002-0-1 loss: 1.071743  [   64/  124]
train() client id: f_00002-0-2 loss: 1.032457  [   96/  124]
train() client id: f_00002-1-0 loss: 1.097227  [   32/  124]
train() client id: f_00002-1-1 loss: 1.250723  [   64/  124]
train() client id: f_00002-1-2 loss: 0.834796  [   96/  124]
train() client id: f_00002-2-0 loss: 1.098912  [   32/  124]
train() client id: f_00002-2-1 loss: 1.002523  [   64/  124]
train() client id: f_00002-2-2 loss: 0.879451  [   96/  124]
train() client id: f_00002-3-0 loss: 1.073536  [   32/  124]
train() client id: f_00002-3-1 loss: 1.099377  [   64/  124]
train() client id: f_00002-3-2 loss: 0.958194  [   96/  124]
train() client id: f_00002-4-0 loss: 1.136969  [   32/  124]
train() client id: f_00002-4-1 loss: 1.018047  [   64/  124]
train() client id: f_00002-4-2 loss: 0.847661  [   96/  124]
train() client id: f_00002-5-0 loss: 1.007737  [   32/  124]
train() client id: f_00002-5-1 loss: 1.057984  [   64/  124]
train() client id: f_00002-5-2 loss: 0.882674  [   96/  124]
train() client id: f_00002-6-0 loss: 0.974927  [   32/  124]
train() client id: f_00002-6-1 loss: 1.047942  [   64/  124]
train() client id: f_00002-6-2 loss: 0.877485  [   96/  124]
train() client id: f_00002-7-0 loss: 0.829650  [   32/  124]
train() client id: f_00002-7-1 loss: 1.043416  [   64/  124]
train() client id: f_00002-7-2 loss: 1.000760  [   96/  124]
train() client id: f_00002-8-0 loss: 1.133989  [   32/  124]
train() client id: f_00002-8-1 loss: 1.063750  [   64/  124]
train() client id: f_00002-8-2 loss: 0.808806  [   96/  124]
train() client id: f_00002-9-0 loss: 0.945312  [   32/  124]
train() client id: f_00002-9-1 loss: 0.831159  [   64/  124]
train() client id: f_00002-9-2 loss: 1.044034  [   96/  124]
train() client id: f_00002-10-0 loss: 1.035635  [   32/  124]
train() client id: f_00002-10-1 loss: 0.932771  [   64/  124]
train() client id: f_00002-10-2 loss: 0.993505  [   96/  124]
train() client id: f_00002-11-0 loss: 0.955615  [   32/  124]
train() client id: f_00002-11-1 loss: 0.800939  [   64/  124]
train() client id: f_00002-11-2 loss: 0.953423  [   96/  124]
train() client id: f_00003-0-0 loss: 0.490595  [   32/   43]
train() client id: f_00003-1-0 loss: 0.390862  [   32/   43]
train() client id: f_00003-2-0 loss: 0.511172  [   32/   43]
train() client id: f_00003-3-0 loss: 0.618475  [   32/   43]
train() client id: f_00003-4-0 loss: 0.552630  [   32/   43]
train() client id: f_00003-5-0 loss: 0.365514  [   32/   43]
train() client id: f_00003-6-0 loss: 0.422842  [   32/   43]
train() client id: f_00003-7-0 loss: 0.683252  [   32/   43]
train() client id: f_00003-8-0 loss: 0.460575  [   32/   43]
train() client id: f_00003-9-0 loss: 0.291747  [   32/   43]
train() client id: f_00003-10-0 loss: 0.512482  [   32/   43]
train() client id: f_00003-11-0 loss: 0.301641  [   32/   43]
train() client id: f_00004-0-0 loss: 0.648507  [   32/  306]
train() client id: f_00004-0-1 loss: 0.959804  [   64/  306]
train() client id: f_00004-0-2 loss: 0.911802  [   96/  306]
train() client id: f_00004-0-3 loss: 0.998402  [  128/  306]
train() client id: f_00004-0-4 loss: 1.025569  [  160/  306]
train() client id: f_00004-0-5 loss: 0.883187  [  192/  306]
train() client id: f_00004-0-6 loss: 0.779545  [  224/  306]
train() client id: f_00004-0-7 loss: 0.982020  [  256/  306]
train() client id: f_00004-0-8 loss: 0.714136  [  288/  306]
train() client id: f_00004-1-0 loss: 0.838090  [   32/  306]
train() client id: f_00004-1-1 loss: 0.927394  [   64/  306]
train() client id: f_00004-1-2 loss: 0.925066  [   96/  306]
train() client id: f_00004-1-3 loss: 1.055435  [  128/  306]
train() client id: f_00004-1-4 loss: 0.976013  [  160/  306]
train() client id: f_00004-1-5 loss: 0.685705  [  192/  306]
train() client id: f_00004-1-6 loss: 0.751746  [  224/  306]
train() client id: f_00004-1-7 loss: 0.919800  [  256/  306]
train() client id: f_00004-1-8 loss: 0.795276  [  288/  306]
train() client id: f_00004-2-0 loss: 0.933061  [   32/  306]
train() client id: f_00004-2-1 loss: 0.779691  [   64/  306]
train() client id: f_00004-2-2 loss: 0.941054  [   96/  306]
train() client id: f_00004-2-3 loss: 0.986443  [  128/  306]
train() client id: f_00004-2-4 loss: 0.923385  [  160/  306]
train() client id: f_00004-2-5 loss: 0.841692  [  192/  306]
train() client id: f_00004-2-6 loss: 0.852088  [  224/  306]
train() client id: f_00004-2-7 loss: 0.855855  [  256/  306]
train() client id: f_00004-2-8 loss: 0.840200  [  288/  306]
train() client id: f_00004-3-0 loss: 0.824275  [   32/  306]
train() client id: f_00004-3-1 loss: 0.864146  [   64/  306]
train() client id: f_00004-3-2 loss: 0.899111  [   96/  306]
train() client id: f_00004-3-3 loss: 0.875754  [  128/  306]
train() client id: f_00004-3-4 loss: 0.768594  [  160/  306]
train() client id: f_00004-3-5 loss: 0.872351  [  192/  306]
train() client id: f_00004-3-6 loss: 1.042871  [  224/  306]
train() client id: f_00004-3-7 loss: 0.852616  [  256/  306]
train() client id: f_00004-3-8 loss: 0.859080  [  288/  306]
train() client id: f_00004-4-0 loss: 0.846536  [   32/  306]
train() client id: f_00004-4-1 loss: 0.917348  [   64/  306]
train() client id: f_00004-4-2 loss: 0.965978  [   96/  306]
train() client id: f_00004-4-3 loss: 0.828292  [  128/  306]
train() client id: f_00004-4-4 loss: 0.824059  [  160/  306]
train() client id: f_00004-4-5 loss: 0.815419  [  192/  306]
train() client id: f_00004-4-6 loss: 0.900173  [  224/  306]
train() client id: f_00004-4-7 loss: 0.928060  [  256/  306]
train() client id: f_00004-4-8 loss: 0.878350  [  288/  306]
train() client id: f_00004-5-0 loss: 0.969550  [   32/  306]
train() client id: f_00004-5-1 loss: 0.910235  [   64/  306]
train() client id: f_00004-5-2 loss: 0.922930  [   96/  306]
train() client id: f_00004-5-3 loss: 0.816754  [  128/  306]
train() client id: f_00004-5-4 loss: 0.874457  [  160/  306]
train() client id: f_00004-5-5 loss: 0.692146  [  192/  306]
train() client id: f_00004-5-6 loss: 0.953502  [  224/  306]
train() client id: f_00004-5-7 loss: 0.901129  [  256/  306]
train() client id: f_00004-5-8 loss: 0.837037  [  288/  306]
train() client id: f_00004-6-0 loss: 0.917877  [   32/  306]
train() client id: f_00004-6-1 loss: 0.932915  [   64/  306]
train() client id: f_00004-6-2 loss: 0.808366  [   96/  306]
train() client id: f_00004-6-3 loss: 0.873309  [  128/  306]
train() client id: f_00004-6-4 loss: 0.975291  [  160/  306]
train() client id: f_00004-6-5 loss: 0.826407  [  192/  306]
train() client id: f_00004-6-6 loss: 0.819202  [  224/  306]
train() client id: f_00004-6-7 loss: 0.783291  [  256/  306]
train() client id: f_00004-6-8 loss: 0.864076  [  288/  306]
train() client id: f_00004-7-0 loss: 0.943059  [   32/  306]
train() client id: f_00004-7-1 loss: 0.966978  [   64/  306]
train() client id: f_00004-7-2 loss: 0.860822  [   96/  306]
train() client id: f_00004-7-3 loss: 0.793960  [  128/  306]
train() client id: f_00004-7-4 loss: 0.824568  [  160/  306]
train() client id: f_00004-7-5 loss: 0.729757  [  192/  306]
train() client id: f_00004-7-6 loss: 0.857792  [  224/  306]
train() client id: f_00004-7-7 loss: 0.857094  [  256/  306]
train() client id: f_00004-7-8 loss: 0.973707  [  288/  306]
train() client id: f_00004-8-0 loss: 0.827700  [   32/  306]
train() client id: f_00004-8-1 loss: 0.836941  [   64/  306]
train() client id: f_00004-8-2 loss: 0.908450  [   96/  306]
train() client id: f_00004-8-3 loss: 0.799270  [  128/  306]
train() client id: f_00004-8-4 loss: 0.956375  [  160/  306]
train() client id: f_00004-8-5 loss: 0.822197  [  192/  306]
train() client id: f_00004-8-6 loss: 0.892939  [  224/  306]
train() client id: f_00004-8-7 loss: 0.983833  [  256/  306]
train() client id: f_00004-8-8 loss: 0.833976  [  288/  306]
train() client id: f_00004-9-0 loss: 0.814014  [   32/  306]
train() client id: f_00004-9-1 loss: 0.745175  [   64/  306]
train() client id: f_00004-9-2 loss: 0.799555  [   96/  306]
train() client id: f_00004-9-3 loss: 0.818163  [  128/  306]
train() client id: f_00004-9-4 loss: 0.868328  [  160/  306]
train() client id: f_00004-9-5 loss: 1.036108  [  192/  306]
train() client id: f_00004-9-6 loss: 0.876722  [  224/  306]
train() client id: f_00004-9-7 loss: 0.982455  [  256/  306]
train() client id: f_00004-9-8 loss: 0.909583  [  288/  306]
train() client id: f_00004-10-0 loss: 0.951560  [   32/  306]
train() client id: f_00004-10-1 loss: 0.748395  [   64/  306]
train() client id: f_00004-10-2 loss: 0.846638  [   96/  306]
train() client id: f_00004-10-3 loss: 0.850592  [  128/  306]
train() client id: f_00004-10-4 loss: 0.812613  [  160/  306]
train() client id: f_00004-10-5 loss: 0.832455  [  192/  306]
train() client id: f_00004-10-6 loss: 0.940026  [  224/  306]
train() client id: f_00004-10-7 loss: 0.844028  [  256/  306]
train() client id: f_00004-10-8 loss: 0.979353  [  288/  306]
train() client id: f_00004-11-0 loss: 0.836682  [   32/  306]
train() client id: f_00004-11-1 loss: 0.844417  [   64/  306]
train() client id: f_00004-11-2 loss: 0.907192  [   96/  306]
train() client id: f_00004-11-3 loss: 0.933638  [  128/  306]
train() client id: f_00004-11-4 loss: 0.842624  [  160/  306]
train() client id: f_00004-11-5 loss: 0.918798  [  192/  306]
train() client id: f_00004-11-6 loss: 0.873426  [  224/  306]
train() client id: f_00004-11-7 loss: 0.918751  [  256/  306]
train() client id: f_00004-11-8 loss: 0.775308  [  288/  306]
train() client id: f_00005-0-0 loss: 0.444018  [   32/  146]
train() client id: f_00005-0-1 loss: 0.741698  [   64/  146]
train() client id: f_00005-0-2 loss: 0.716217  [   96/  146]
train() client id: f_00005-0-3 loss: 0.778244  [  128/  146]
train() client id: f_00005-1-0 loss: 0.538198  [   32/  146]
train() client id: f_00005-1-1 loss: 0.849458  [   64/  146]
train() client id: f_00005-1-2 loss: 0.438134  [   96/  146]
train() client id: f_00005-1-3 loss: 0.746719  [  128/  146]
train() client id: f_00005-2-0 loss: 0.656443  [   32/  146]
train() client id: f_00005-2-1 loss: 0.711429  [   64/  146]
train() client id: f_00005-2-2 loss: 0.490683  [   96/  146]
train() client id: f_00005-2-3 loss: 0.747963  [  128/  146]
train() client id: f_00005-3-0 loss: 0.506973  [   32/  146]
train() client id: f_00005-3-1 loss: 0.821053  [   64/  146]
train() client id: f_00005-3-2 loss: 0.461970  [   96/  146]
train() client id: f_00005-3-3 loss: 0.572321  [  128/  146]
train() client id: f_00005-4-0 loss: 0.359677  [   32/  146]
train() client id: f_00005-4-1 loss: 0.793524  [   64/  146]
train() client id: f_00005-4-2 loss: 0.592601  [   96/  146]
train() client id: f_00005-4-3 loss: 0.751006  [  128/  146]
train() client id: f_00005-5-0 loss: 0.624659  [   32/  146]
train() client id: f_00005-5-1 loss: 0.654435  [   64/  146]
train() client id: f_00005-5-2 loss: 0.740789  [   96/  146]
train() client id: f_00005-5-3 loss: 0.467115  [  128/  146]
train() client id: f_00005-6-0 loss: 0.847719  [   32/  146]
train() client id: f_00005-6-1 loss: 0.654796  [   64/  146]
train() client id: f_00005-6-2 loss: 0.432546  [   96/  146]
train() client id: f_00005-6-3 loss: 0.641177  [  128/  146]
train() client id: f_00005-7-0 loss: 0.866270  [   32/  146]
train() client id: f_00005-7-1 loss: 0.690153  [   64/  146]
train() client id: f_00005-7-2 loss: 0.353131  [   96/  146]
train() client id: f_00005-7-3 loss: 0.670751  [  128/  146]
train() client id: f_00005-8-0 loss: 0.792451  [   32/  146]
train() client id: f_00005-8-1 loss: 0.525434  [   64/  146]
train() client id: f_00005-8-2 loss: 0.519217  [   96/  146]
train() client id: f_00005-8-3 loss: 0.771010  [  128/  146]
train() client id: f_00005-9-0 loss: 0.693768  [   32/  146]
train() client id: f_00005-9-1 loss: 0.520379  [   64/  146]
train() client id: f_00005-9-2 loss: 0.684863  [   96/  146]
train() client id: f_00005-9-3 loss: 0.519007  [  128/  146]
train() client id: f_00005-10-0 loss: 0.838699  [   32/  146]
train() client id: f_00005-10-1 loss: 0.419169  [   64/  146]
train() client id: f_00005-10-2 loss: 0.563211  [   96/  146]
train() client id: f_00005-10-3 loss: 0.831551  [  128/  146]
train() client id: f_00005-11-0 loss: 0.644706  [   32/  146]
train() client id: f_00005-11-1 loss: 0.686581  [   64/  146]
train() client id: f_00005-11-2 loss: 0.758238  [   96/  146]
train() client id: f_00005-11-3 loss: 0.493468  [  128/  146]
train() client id: f_00006-0-0 loss: 0.469134  [   32/   54]
train() client id: f_00006-1-0 loss: 0.512356  [   32/   54]
train() client id: f_00006-2-0 loss: 0.440250  [   32/   54]
train() client id: f_00006-3-0 loss: 0.502506  [   32/   54]
train() client id: f_00006-4-0 loss: 0.434755  [   32/   54]
train() client id: f_00006-5-0 loss: 0.407396  [   32/   54]
train() client id: f_00006-6-0 loss: 0.455213  [   32/   54]
train() client id: f_00006-7-0 loss: 0.445788  [   32/   54]
train() client id: f_00006-8-0 loss: 0.501857  [   32/   54]
train() client id: f_00006-9-0 loss: 0.504498  [   32/   54]
train() client id: f_00006-10-0 loss: 0.518708  [   32/   54]
train() client id: f_00006-11-0 loss: 0.410298  [   32/   54]
train() client id: f_00007-0-0 loss: 0.566838  [   32/  179]
train() client id: f_00007-0-1 loss: 0.643097  [   64/  179]
train() client id: f_00007-0-2 loss: 0.448400  [   96/  179]
train() client id: f_00007-0-3 loss: 0.416776  [  128/  179]
train() client id: f_00007-0-4 loss: 0.485955  [  160/  179]
train() client id: f_00007-1-0 loss: 0.501486  [   32/  179]
train() client id: f_00007-1-1 loss: 0.386987  [   64/  179]
train() client id: f_00007-1-2 loss: 0.675249  [   96/  179]
train() client id: f_00007-1-3 loss: 0.471083  [  128/  179]
train() client id: f_00007-1-4 loss: 0.389965  [  160/  179]
train() client id: f_00007-2-0 loss: 0.575367  [   32/  179]
train() client id: f_00007-2-1 loss: 0.392928  [   64/  179]
train() client id: f_00007-2-2 loss: 0.512848  [   96/  179]
train() client id: f_00007-2-3 loss: 0.419709  [  128/  179]
train() client id: f_00007-2-4 loss: 0.461504  [  160/  179]
train() client id: f_00007-3-0 loss: 0.622699  [   32/  179]
train() client id: f_00007-3-1 loss: 0.421424  [   64/  179]
train() client id: f_00007-3-2 loss: 0.355945  [   96/  179]
train() client id: f_00007-3-3 loss: 0.351209  [  128/  179]
train() client id: f_00007-3-4 loss: 0.471292  [  160/  179]
train() client id: f_00007-4-0 loss: 0.289034  [   32/  179]
train() client id: f_00007-4-1 loss: 0.567098  [   64/  179]
train() client id: f_00007-4-2 loss: 0.496593  [   96/  179]
train() client id: f_00007-4-3 loss: 0.520528  [  128/  179]
train() client id: f_00007-4-4 loss: 0.379471  [  160/  179]
train() client id: f_00007-5-0 loss: 0.385721  [   32/  179]
train() client id: f_00007-5-1 loss: 0.662387  [   64/  179]
train() client id: f_00007-5-2 loss: 0.281468  [   96/  179]
train() client id: f_00007-5-3 loss: 0.575146  [  128/  179]
train() client id: f_00007-5-4 loss: 0.417955  [  160/  179]
train() client id: f_00007-6-0 loss: 0.455057  [   32/  179]
train() client id: f_00007-6-1 loss: 0.488628  [   64/  179]
train() client id: f_00007-6-2 loss: 0.540939  [   96/  179]
train() client id: f_00007-6-3 loss: 0.374576  [  128/  179]
train() client id: f_00007-6-4 loss: 0.398475  [  160/  179]
train() client id: f_00007-7-0 loss: 0.361001  [   32/  179]
train() client id: f_00007-7-1 loss: 0.466067  [   64/  179]
train() client id: f_00007-7-2 loss: 0.330289  [   96/  179]
train() client id: f_00007-7-3 loss: 0.273663  [  128/  179]
train() client id: f_00007-7-4 loss: 0.822024  [  160/  179]
train() client id: f_00007-8-0 loss: 0.546564  [   32/  179]
train() client id: f_00007-8-1 loss: 0.455207  [   64/  179]
train() client id: f_00007-8-2 loss: 0.372642  [   96/  179]
train() client id: f_00007-8-3 loss: 0.390089  [  128/  179]
train() client id: f_00007-8-4 loss: 0.278890  [  160/  179]
train() client id: f_00007-9-0 loss: 0.450546  [   32/  179]
train() client id: f_00007-9-1 loss: 0.300061  [   64/  179]
train() client id: f_00007-9-2 loss: 0.459334  [   96/  179]
train() client id: f_00007-9-3 loss: 0.270240  [  128/  179]
train() client id: f_00007-9-4 loss: 0.496609  [  160/  179]
train() client id: f_00007-10-0 loss: 0.321076  [   32/  179]
train() client id: f_00007-10-1 loss: 0.480774  [   64/  179]
train() client id: f_00007-10-2 loss: 0.560695  [   96/  179]
train() client id: f_00007-10-3 loss: 0.537242  [  128/  179]
train() client id: f_00007-10-4 loss: 0.251280  [  160/  179]
train() client id: f_00007-11-0 loss: 0.456746  [   32/  179]
train() client id: f_00007-11-1 loss: 0.265536  [   64/  179]
train() client id: f_00007-11-2 loss: 0.522100  [   96/  179]
train() client id: f_00007-11-3 loss: 0.399343  [  128/  179]
train() client id: f_00007-11-4 loss: 0.354099  [  160/  179]
train() client id: f_00008-0-0 loss: 0.646989  [   32/  130]
train() client id: f_00008-0-1 loss: 0.714987  [   64/  130]
train() client id: f_00008-0-2 loss: 0.671127  [   96/  130]
train() client id: f_00008-0-3 loss: 0.715993  [  128/  130]
train() client id: f_00008-1-0 loss: 0.662898  [   32/  130]
train() client id: f_00008-1-1 loss: 0.701978  [   64/  130]
train() client id: f_00008-1-2 loss: 0.677622  [   96/  130]
train() client id: f_00008-1-3 loss: 0.710841  [  128/  130]
train() client id: f_00008-2-0 loss: 0.600279  [   32/  130]
train() client id: f_00008-2-1 loss: 0.681281  [   64/  130]
train() client id: f_00008-2-2 loss: 0.710165  [   96/  130]
train() client id: f_00008-2-3 loss: 0.799774  [  128/  130]
train() client id: f_00008-3-0 loss: 0.630872  [   32/  130]
train() client id: f_00008-3-1 loss: 0.812869  [   64/  130]
train() client id: f_00008-3-2 loss: 0.617641  [   96/  130]
train() client id: f_00008-3-3 loss: 0.696945  [  128/  130]
train() client id: f_00008-4-0 loss: 0.702263  [   32/  130]
train() client id: f_00008-4-1 loss: 0.802212  [   64/  130]
train() client id: f_00008-4-2 loss: 0.660324  [   96/  130]
train() client id: f_00008-4-3 loss: 0.613730  [  128/  130]
train() client id: f_00008-5-0 loss: 0.653937  [   32/  130]
train() client id: f_00008-5-1 loss: 0.759426  [   64/  130]
train() client id: f_00008-5-2 loss: 0.660229  [   96/  130]
train() client id: f_00008-5-3 loss: 0.685158  [  128/  130]
train() client id: f_00008-6-0 loss: 0.619676  [   32/  130]
train() client id: f_00008-6-1 loss: 0.725041  [   64/  130]
train() client id: f_00008-6-2 loss: 0.856748  [   96/  130]
train() client id: f_00008-6-3 loss: 0.565227  [  128/  130]
train() client id: f_00008-7-0 loss: 0.726536  [   32/  130]
train() client id: f_00008-7-1 loss: 0.661347  [   64/  130]
train() client id: f_00008-7-2 loss: 0.708620  [   96/  130]
train() client id: f_00008-7-3 loss: 0.694449  [  128/  130]
train() client id: f_00008-8-0 loss: 0.625330  [   32/  130]
train() client id: f_00008-8-1 loss: 0.690281  [   64/  130]
train() client id: f_00008-8-2 loss: 0.691450  [   96/  130]
train() client id: f_00008-8-3 loss: 0.793516  [  128/  130]
train() client id: f_00008-9-0 loss: 0.796078  [   32/  130]
train() client id: f_00008-9-1 loss: 0.707103  [   64/  130]
train() client id: f_00008-9-2 loss: 0.647050  [   96/  130]
train() client id: f_00008-9-3 loss: 0.594784  [  128/  130]
train() client id: f_00008-10-0 loss: 0.698069  [   32/  130]
train() client id: f_00008-10-1 loss: 0.717167  [   64/  130]
train() client id: f_00008-10-2 loss: 0.680103  [   96/  130]
train() client id: f_00008-10-3 loss: 0.676610  [  128/  130]
train() client id: f_00008-11-0 loss: 0.823164  [   32/  130]
train() client id: f_00008-11-1 loss: 0.689976  [   64/  130]
train() client id: f_00008-11-2 loss: 0.676414  [   96/  130]
train() client id: f_00008-11-3 loss: 0.621182  [  128/  130]
train() client id: f_00009-0-0 loss: 0.941642  [   32/  118]
train() client id: f_00009-0-1 loss: 1.091370  [   64/  118]
train() client id: f_00009-0-2 loss: 1.101167  [   96/  118]
train() client id: f_00009-1-0 loss: 0.977727  [   32/  118]
train() client id: f_00009-1-1 loss: 1.058285  [   64/  118]
train() client id: f_00009-1-2 loss: 0.923110  [   96/  118]
train() client id: f_00009-2-0 loss: 1.097174  [   32/  118]
train() client id: f_00009-2-1 loss: 0.869072  [   64/  118]
train() client id: f_00009-2-2 loss: 1.006612  [   96/  118]
train() client id: f_00009-3-0 loss: 0.807551  [   32/  118]
train() client id: f_00009-3-1 loss: 0.942855  [   64/  118]
train() client id: f_00009-3-2 loss: 1.136271  [   96/  118]
train() client id: f_00009-4-0 loss: 0.986963  [   32/  118]
train() client id: f_00009-4-1 loss: 0.883386  [   64/  118]
train() client id: f_00009-4-2 loss: 0.845312  [   96/  118]
train() client id: f_00009-5-0 loss: 0.865914  [   32/  118]
train() client id: f_00009-5-1 loss: 0.806615  [   64/  118]
train() client id: f_00009-5-2 loss: 1.086608  [   96/  118]
train() client id: f_00009-6-0 loss: 0.913796  [   32/  118]
train() client id: f_00009-6-1 loss: 0.746709  [   64/  118]
train() client id: f_00009-6-2 loss: 0.913875  [   96/  118]
train() client id: f_00009-7-0 loss: 0.850583  [   32/  118]
train() client id: f_00009-7-1 loss: 0.831617  [   64/  118]
train() client id: f_00009-7-2 loss: 0.792684  [   96/  118]
train() client id: f_00009-8-0 loss: 0.741852  [   32/  118]
train() client id: f_00009-8-1 loss: 0.927864  [   64/  118]
train() client id: f_00009-8-2 loss: 0.858204  [   96/  118]
train() client id: f_00009-9-0 loss: 0.919239  [   32/  118]
train() client id: f_00009-9-1 loss: 0.742779  [   64/  118]
train() client id: f_00009-9-2 loss: 0.812584  [   96/  118]
train() client id: f_00009-10-0 loss: 0.847903  [   32/  118]
train() client id: f_00009-10-1 loss: 0.747503  [   64/  118]
train() client id: f_00009-10-2 loss: 0.947320  [   96/  118]
train() client id: f_00009-11-0 loss: 0.973663  [   32/  118]
train() client id: f_00009-11-1 loss: 0.734902  [   64/  118]
train() client id: f_00009-11-2 loss: 0.767666  [   96/  118]
At round 47 accuracy: 0.6445623342175066
At round 47 training accuracy: 0.5902079141515761
At round 47 training loss: 0.8322565088384628
update_location
xs = [  -3.9056584     4.20031788  255.00902392   18.81129433    0.97929623
    3.95640986 -217.44319194 -196.32485185  239.66397685 -182.06087855]
ys = [ 247.5879595   230.55583871    1.32061395 -217.45517586  209.35018685
  192.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [267.04878179 251.343664   273.91850303 240.08460655 232.00961134
 217.23938387 239.35002039 220.3273104  260.28348348 207.75508503]
dists_bs = [185.85555033 187.61451208 463.75887031 437.70296191 179.3004407
 179.84087577 182.62546262 175.47821451 443.54338182 171.14423519]
uav_gains = [5.24535404e-12 7.22652115e-12 4.54225750e-12 8.95681473e-12
 1.03431723e-11 1.31774062e-11 9.07808384e-12 1.25521670e-11
 6.03484339e-12 1.52201927e-11]
bs_gains = [4.89330838e-11 4.76593468e-11 3.78160221e-12 4.44622592e-12
 5.41085986e-11 5.36545480e-11 5.13951767e-11 5.74736933e-11
 4.28423215e-12 6.16424250e-11]
Round 48
-------------------------------
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.66251584  9.56421031  4.59923771  1.66893827 11.02840386  5.30599829
  2.06183055  6.52085571  4.81741298  4.30225233]
obj_prev = 54.53165585566928
eta_min = 1.7212465954185627e-20	eta_max = 0.9392276891852349
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 12.610414541407936	eta = 0.909090909090909
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 25.13769025130001	eta = 0.4560487898791294
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 18.739989943316214	eta = 0.6117406281506839
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.582476098023903	eta = 0.6520135819067145
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.516919047777254	eta = 0.6544537420190021
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.516687862363067	eta = 0.6544623795057578
eta = 0.6544623795057578
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [0.03498261 0.07357454 0.03442733 0.01193851 0.08495777 0.0405354
 0.01499255 0.04969752 0.03609317 0.03276149]
ene_total = [1.63559168 2.74693988 1.64037262 0.78097472 3.12737952 1.61844234
 0.88321539 2.02882541 1.7072293  1.347717  ]
ti_comp = [0.68075038 0.73958322 0.67379048 0.70183595 0.74144644 0.74132564
 0.70228536 0.71216487 0.67077194 0.74326503]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [5.77378821e-06 4.55080801e-05 5.61746411e-06 2.15903231e-07
 6.97155574e-05 7.57470152e-06 4.27050309e-07 1.51259614e-05
 6.53138483e-06 3.97817259e-06]
ene_total = [0.4471766  0.2486291  0.47081906 0.37534488 0.24312091 0.24142
 0.37382507 0.34075668 0.48110627 0.23470829]
optimize_network_iter = 0 obj = 3.4569068484449996
eta = 0.6544623795057578
freqs = [25694153.73552614 49740538.88381484 25547504.06148466  8505199.59149158
 57291910.54818947 27339810.52250793 10674113.26419133 34891861.85896511
 26904202.71882671 22038902.98735746]
eta_min = 0.6544623795057585	eta_max = 0.691983191142256
af = 0.004443917503062845	bf = 1.1926074070739463	zeta = 0.00488830925336913	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [1.29805932e-06 1.02310970e-05 1.26291463e-06 4.85392243e-08
 1.56734064e-05 1.70293948e-06 9.60091734e-08 3.40060882e-06
 1.46838170e-06 8.94370180e-07]
ene_total = [1.6991017  0.94046194 1.78898032 1.42663193 0.91710237 0.91685829
 1.42083414 1.2936725  1.8279896  0.89170773]
ti_comp = [0.59254498 0.65137783 0.58558508 0.61363055 0.65324104 0.65312024
 0.61407996 0.62395948 0.58256655 0.65505963]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.99236055e-06 3.84333847e-05 4.87216508e-06 1.85024130e-07
 5.88375125e-05 6.39306671e-06 3.65905026e-07 1.29087227e-05
 5.67251856e-06 3.35522234e-06]
ene_total = [0.50161926 0.27864598 0.52814309 0.42106603 0.27232185 0.27078333
 0.41935993 0.38218116 0.53967909 0.26327534]
optimize_network_iter = 1 obj = 3.877075061802886
eta = 0.691983191142256
freqs = [25637255.917499   49049571.69376329 25530186.95114224  8448581.49608057
 56476825.65917366 26951443.47001215 10602081.72934842 34587483.18659778
 26904202.71882672 21718183.03696004]
eta_min = 0.6919831911422635	eta_max = 0.6919831911422551
af = 0.004337069151020109	bf = 1.1926074070739463	zeta = 0.004770776066122121	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [1.29231677e-06 9.94882221e-06 1.26120311e-06 4.78951357e-08
 1.52306115e-05 1.65490197e-06 9.47177586e-08 3.34153728e-06
 1.46838170e-06 8.68529037e-07]
ene_total = [1.69910096 0.94042549 1.7889801  1.42663185 0.91704519 0.91685209
 1.42083397 1.29366487 1.8279896  0.8917044 ]
ti_comp = [0.59254498 0.65137783 0.58558508 0.61363055 0.65324104 0.65312024
 0.61407996 0.62395948 0.58256655 0.65505963]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.99236055e-06 3.84333847e-05 4.87216508e-06 1.85024130e-07
 5.88375125e-05 6.39306671e-06 3.65905026e-07 1.29087227e-05
 5.67251856e-06 3.35522234e-06]
ene_total = [0.50161926 0.27864598 0.52814309 0.42106603 0.27232185 0.27078333
 0.41935993 0.38218116 0.53967909 0.26327534]
optimize_network_iter = 2 obj = 3.8770750618028753
eta = 0.6919831911422551
freqs = [25637255.91749901 49049571.69376332 25530186.95114225  8448581.49608057
 56476825.6591737  26951443.47001216 10602081.72934843 34587483.18659779
 26904202.71882672 21718183.03696005]
Done!
At round 48 eta: 0.6919831911422551
At round 48 local rounds: 12.056529567513225
At round 48 global rounds: 38.11610880998822
At round 48 a_n: 11.397856354757828
gradient difference: 0.4615103304386139
train() client id: f_00000-0-0 loss: 0.843153  [   32/  126]
train() client id: f_00000-0-1 loss: 1.059452  [   64/  126]
train() client id: f_00000-0-2 loss: 1.003918  [   96/  126]
train() client id: f_00000-1-0 loss: 0.861985  [   32/  126]
train() client id: f_00000-1-1 loss: 0.942445  [   64/  126]
train() client id: f_00000-1-2 loss: 0.774891  [   96/  126]
train() client id: f_00000-2-0 loss: 0.862712  [   32/  126]
train() client id: f_00000-2-1 loss: 0.739391  [   64/  126]
train() client id: f_00000-2-2 loss: 1.029313  [   96/  126]
train() client id: f_00000-3-0 loss: 0.823003  [   32/  126]
train() client id: f_00000-3-1 loss: 0.982266  [   64/  126]
train() client id: f_00000-3-2 loss: 0.748079  [   96/  126]
train() client id: f_00000-4-0 loss: 0.844033  [   32/  126]
train() client id: f_00000-4-1 loss: 0.713112  [   64/  126]
train() client id: f_00000-4-2 loss: 0.798139  [   96/  126]
train() client id: f_00000-5-0 loss: 0.900951  [   32/  126]
train() client id: f_00000-5-1 loss: 0.697654  [   64/  126]
train() client id: f_00000-5-2 loss: 0.655331  [   96/  126]
train() client id: f_00000-6-0 loss: 0.807651  [   32/  126]
train() client id: f_00000-6-1 loss: 0.815307  [   64/  126]
train() client id: f_00000-6-2 loss: 0.698715  [   96/  126]
train() client id: f_00000-7-0 loss: 0.716699  [   32/  126]
train() client id: f_00000-7-1 loss: 0.811674  [   64/  126]
train() client id: f_00000-7-2 loss: 0.822452  [   96/  126]
train() client id: f_00000-8-0 loss: 0.774145  [   32/  126]
train() client id: f_00000-8-1 loss: 0.696030  [   64/  126]
train() client id: f_00000-8-2 loss: 0.819648  [   96/  126]
train() client id: f_00000-9-0 loss: 0.741989  [   32/  126]
train() client id: f_00000-9-1 loss: 0.740486  [   64/  126]
train() client id: f_00000-9-2 loss: 0.784524  [   96/  126]
train() client id: f_00000-10-0 loss: 0.713581  [   32/  126]
train() client id: f_00000-10-1 loss: 0.721767  [   64/  126]
train() client id: f_00000-10-2 loss: 0.802787  [   96/  126]
train() client id: f_00000-11-0 loss: 0.711116  [   32/  126]
train() client id: f_00000-11-1 loss: 0.744801  [   64/  126]
train() client id: f_00000-11-2 loss: 0.719681  [   96/  126]
train() client id: f_00001-0-0 loss: 0.425891  [   32/  265]
train() client id: f_00001-0-1 loss: 0.417792  [   64/  265]
train() client id: f_00001-0-2 loss: 0.493101  [   96/  265]
train() client id: f_00001-0-3 loss: 0.504204  [  128/  265]
train() client id: f_00001-0-4 loss: 0.500601  [  160/  265]
train() client id: f_00001-0-5 loss: 0.568859  [  192/  265]
train() client id: f_00001-0-6 loss: 0.507200  [  224/  265]
train() client id: f_00001-0-7 loss: 0.570300  [  256/  265]
train() client id: f_00001-1-0 loss: 0.421875  [   32/  265]
train() client id: f_00001-1-1 loss: 0.476649  [   64/  265]
train() client id: f_00001-1-2 loss: 0.550048  [   96/  265]
train() client id: f_00001-1-3 loss: 0.556948  [  128/  265]
train() client id: f_00001-1-4 loss: 0.590338  [  160/  265]
train() client id: f_00001-1-5 loss: 0.481870  [  192/  265]
train() client id: f_00001-1-6 loss: 0.425906  [  224/  265]
train() client id: f_00001-1-7 loss: 0.411548  [  256/  265]
train() client id: f_00001-2-0 loss: 0.486830  [   32/  265]
train() client id: f_00001-2-1 loss: 0.679889  [   64/  265]
train() client id: f_00001-2-2 loss: 0.404975  [   96/  265]
train() client id: f_00001-2-3 loss: 0.437151  [  128/  265]
train() client id: f_00001-2-4 loss: 0.387837  [  160/  265]
train() client id: f_00001-2-5 loss: 0.517915  [  192/  265]
train() client id: f_00001-2-6 loss: 0.551029  [  224/  265]
train() client id: f_00001-2-7 loss: 0.471792  [  256/  265]
train() client id: f_00001-3-0 loss: 0.472633  [   32/  265]
train() client id: f_00001-3-1 loss: 0.567424  [   64/  265]
train() client id: f_00001-3-2 loss: 0.414249  [   96/  265]
train() client id: f_00001-3-3 loss: 0.505237  [  128/  265]
train() client id: f_00001-3-4 loss: 0.434816  [  160/  265]
train() client id: f_00001-3-5 loss: 0.562724  [  192/  265]
train() client id: f_00001-3-6 loss: 0.432610  [  224/  265]
train() client id: f_00001-3-7 loss: 0.444288  [  256/  265]
train() client id: f_00001-4-0 loss: 0.385381  [   32/  265]
train() client id: f_00001-4-1 loss: 0.612173  [   64/  265]
train() client id: f_00001-4-2 loss: 0.406538  [   96/  265]
train() client id: f_00001-4-3 loss: 0.587562  [  128/  265]
train() client id: f_00001-4-4 loss: 0.471197  [  160/  265]
train() client id: f_00001-4-5 loss: 0.422023  [  192/  265]
train() client id: f_00001-4-6 loss: 0.504803  [  224/  265]
train() client id: f_00001-4-7 loss: 0.406985  [  256/  265]
train() client id: f_00001-5-0 loss: 0.392490  [   32/  265]
train() client id: f_00001-5-1 loss: 0.432232  [   64/  265]
train() client id: f_00001-5-2 loss: 0.535862  [   96/  265]
train() client id: f_00001-5-3 loss: 0.404919  [  128/  265]
train() client id: f_00001-5-4 loss: 0.545696  [  160/  265]
train() client id: f_00001-5-5 loss: 0.674578  [  192/  265]
train() client id: f_00001-5-6 loss: 0.481113  [  224/  265]
train() client id: f_00001-5-7 loss: 0.384866  [  256/  265]
train() client id: f_00001-6-0 loss: 0.397070  [   32/  265]
train() client id: f_00001-6-1 loss: 0.367143  [   64/  265]
train() client id: f_00001-6-2 loss: 0.570623  [   96/  265]
train() client id: f_00001-6-3 loss: 0.476963  [  128/  265]
train() client id: f_00001-6-4 loss: 0.545268  [  160/  265]
train() client id: f_00001-6-5 loss: 0.553810  [  192/  265]
train() client id: f_00001-6-6 loss: 0.519364  [  224/  265]
train() client id: f_00001-6-7 loss: 0.451923  [  256/  265]
train() client id: f_00001-7-0 loss: 0.533874  [   32/  265]
train() client id: f_00001-7-1 loss: 0.386696  [   64/  265]
train() client id: f_00001-7-2 loss: 0.504547  [   96/  265]
train() client id: f_00001-7-3 loss: 0.465395  [  128/  265]
train() client id: f_00001-7-4 loss: 0.405318  [  160/  265]
train() client id: f_00001-7-5 loss: 0.651560  [  192/  265]
train() client id: f_00001-7-6 loss: 0.451652  [  224/  265]
train() client id: f_00001-7-7 loss: 0.471335  [  256/  265]
train() client id: f_00001-8-0 loss: 0.503326  [   32/  265]
train() client id: f_00001-8-1 loss: 0.623432  [   64/  265]
train() client id: f_00001-8-2 loss: 0.467143  [   96/  265]
train() client id: f_00001-8-3 loss: 0.527882  [  128/  265]
train() client id: f_00001-8-4 loss: 0.372537  [  160/  265]
train() client id: f_00001-8-5 loss: 0.554469  [  192/  265]
train() client id: f_00001-8-6 loss: 0.399757  [  224/  265]
train() client id: f_00001-8-7 loss: 0.432943  [  256/  265]
train() client id: f_00001-9-0 loss: 0.483593  [   32/  265]
train() client id: f_00001-9-1 loss: 0.481799  [   64/  265]
train() client id: f_00001-9-2 loss: 0.391032  [   96/  265]
train() client id: f_00001-9-3 loss: 0.529343  [  128/  265]
train() client id: f_00001-9-4 loss: 0.384357  [  160/  265]
train() client id: f_00001-9-5 loss: 0.460329  [  192/  265]
train() client id: f_00001-9-6 loss: 0.516473  [  224/  265]
train() client id: f_00001-9-7 loss: 0.619350  [  256/  265]
train() client id: f_00001-10-0 loss: 0.398440  [   32/  265]
train() client id: f_00001-10-1 loss: 0.385773  [   64/  265]
train() client id: f_00001-10-2 loss: 0.379262  [   96/  265]
train() client id: f_00001-10-3 loss: 0.474893  [  128/  265]
train() client id: f_00001-10-4 loss: 0.579754  [  160/  265]
train() client id: f_00001-10-5 loss: 0.399880  [  192/  265]
train() client id: f_00001-10-6 loss: 0.713053  [  224/  265]
train() client id: f_00001-10-7 loss: 0.391782  [  256/  265]
train() client id: f_00001-11-0 loss: 0.462902  [   32/  265]
train() client id: f_00001-11-1 loss: 0.472201  [   64/  265]
train() client id: f_00001-11-2 loss: 0.425062  [   96/  265]
train() client id: f_00001-11-3 loss: 0.552962  [  128/  265]
train() client id: f_00001-11-4 loss: 0.545149  [  160/  265]
train() client id: f_00001-11-5 loss: 0.391877  [  192/  265]
train() client id: f_00001-11-6 loss: 0.526710  [  224/  265]
train() client id: f_00001-11-7 loss: 0.462231  [  256/  265]
train() client id: f_00002-0-0 loss: 1.086759  [   32/  124]
train() client id: f_00002-0-1 loss: 1.276580  [   64/  124]
train() client id: f_00002-0-2 loss: 1.022761  [   96/  124]
train() client id: f_00002-1-0 loss: 1.188077  [   32/  124]
train() client id: f_00002-1-1 loss: 0.992566  [   64/  124]
train() client id: f_00002-1-2 loss: 1.070047  [   96/  124]
train() client id: f_00002-2-0 loss: 1.050860  [   32/  124]
train() client id: f_00002-2-1 loss: 0.959744  [   64/  124]
train() client id: f_00002-2-2 loss: 1.115975  [   96/  124]
train() client id: f_00002-3-0 loss: 1.040857  [   32/  124]
train() client id: f_00002-3-1 loss: 1.111592  [   64/  124]
train() client id: f_00002-3-2 loss: 1.042500  [   96/  124]
train() client id: f_00002-4-0 loss: 0.961724  [   32/  124]
train() client id: f_00002-4-1 loss: 1.055451  [   64/  124]
train() client id: f_00002-4-2 loss: 1.001797  [   96/  124]
train() client id: f_00002-5-0 loss: 1.075245  [   32/  124]
train() client id: f_00002-5-1 loss: 0.967976  [   64/  124]
train() client id: f_00002-5-2 loss: 0.772582  [   96/  124]
train() client id: f_00002-6-0 loss: 0.867900  [   32/  124]
train() client id: f_00002-6-1 loss: 1.190326  [   64/  124]
train() client id: f_00002-6-2 loss: 1.028045  [   96/  124]
train() client id: f_00002-7-0 loss: 0.977536  [   32/  124]
train() client id: f_00002-7-1 loss: 1.063373  [   64/  124]
train() client id: f_00002-7-2 loss: 0.840050  [   96/  124]
train() client id: f_00002-8-0 loss: 0.904337  [   32/  124]
train() client id: f_00002-8-1 loss: 0.952141  [   64/  124]
train() client id: f_00002-8-2 loss: 0.959879  [   96/  124]
train() client id: f_00002-9-0 loss: 1.005384  [   32/  124]
train() client id: f_00002-9-1 loss: 0.850693  [   64/  124]
train() client id: f_00002-9-2 loss: 1.054458  [   96/  124]
train() client id: f_00002-10-0 loss: 1.223006  [   32/  124]
train() client id: f_00002-10-1 loss: 0.951488  [   64/  124]
train() client id: f_00002-10-2 loss: 0.782889  [   96/  124]
train() client id: f_00002-11-0 loss: 0.887110  [   32/  124]
train() client id: f_00002-11-1 loss: 0.872664  [   64/  124]
train() client id: f_00002-11-2 loss: 1.093967  [   96/  124]
train() client id: f_00003-0-0 loss: 0.941378  [   32/   43]
train() client id: f_00003-1-0 loss: 0.640881  [   32/   43]
train() client id: f_00003-2-0 loss: 0.727375  [   32/   43]
train() client id: f_00003-3-0 loss: 1.018972  [   32/   43]
train() client id: f_00003-4-0 loss: 0.915040  [   32/   43]
train() client id: f_00003-5-0 loss: 0.846351  [   32/   43]
train() client id: f_00003-6-0 loss: 1.094492  [   32/   43]
train() client id: f_00003-7-0 loss: 0.892616  [   32/   43]
train() client id: f_00003-8-0 loss: 0.976484  [   32/   43]
train() client id: f_00003-9-0 loss: 0.846663  [   32/   43]
train() client id: f_00003-10-0 loss: 0.820421  [   32/   43]
train() client id: f_00003-11-0 loss: 0.836883  [   32/   43]
train() client id: f_00004-0-0 loss: 0.941101  [   32/  306]
train() client id: f_00004-0-1 loss: 1.023503  [   64/  306]
train() client id: f_00004-0-2 loss: 0.963597  [   96/  306]
train() client id: f_00004-0-3 loss: 0.953967  [  128/  306]
train() client id: f_00004-0-4 loss: 0.942858  [  160/  306]
train() client id: f_00004-0-5 loss: 0.940257  [  192/  306]
train() client id: f_00004-0-6 loss: 0.839164  [  224/  306]
train() client id: f_00004-0-7 loss: 1.065994  [  256/  306]
train() client id: f_00004-0-8 loss: 0.955009  [  288/  306]
train() client id: f_00004-1-0 loss: 0.911996  [   32/  306]
train() client id: f_00004-1-1 loss: 1.009954  [   64/  306]
train() client id: f_00004-1-2 loss: 0.972561  [   96/  306]
train() client id: f_00004-1-3 loss: 0.922292  [  128/  306]
train() client id: f_00004-1-4 loss: 0.955778  [  160/  306]
train() client id: f_00004-1-5 loss: 0.900786  [  192/  306]
train() client id: f_00004-1-6 loss: 0.979943  [  224/  306]
train() client id: f_00004-1-7 loss: 0.984911  [  256/  306]
train() client id: f_00004-1-8 loss: 0.936994  [  288/  306]
train() client id: f_00004-2-0 loss: 0.864210  [   32/  306]
train() client id: f_00004-2-1 loss: 0.972515  [   64/  306]
train() client id: f_00004-2-2 loss: 0.845028  [   96/  306]
train() client id: f_00004-2-3 loss: 1.022003  [  128/  306]
train() client id: f_00004-2-4 loss: 1.034692  [  160/  306]
train() client id: f_00004-2-5 loss: 0.881740  [  192/  306]
train() client id: f_00004-2-6 loss: 1.049174  [  224/  306]
train() client id: f_00004-2-7 loss: 1.042242  [  256/  306]
train() client id: f_00004-2-8 loss: 0.893862  [  288/  306]
train() client id: f_00004-3-0 loss: 0.809575  [   32/  306]
train() client id: f_00004-3-1 loss: 0.814849  [   64/  306]
train() client id: f_00004-3-2 loss: 0.855341  [   96/  306]
train() client id: f_00004-3-3 loss: 0.954287  [  128/  306]
train() client id: f_00004-3-4 loss: 0.963652  [  160/  306]
train() client id: f_00004-3-5 loss: 1.037641  [  192/  306]
train() client id: f_00004-3-6 loss: 1.070142  [  224/  306]
train() client id: f_00004-3-7 loss: 1.016647  [  256/  306]
train() client id: f_00004-3-8 loss: 0.999549  [  288/  306]
train() client id: f_00004-4-0 loss: 1.122125  [   32/  306]
train() client id: f_00004-4-1 loss: 0.902997  [   64/  306]
train() client id: f_00004-4-2 loss: 0.897086  [   96/  306]
train() client id: f_00004-4-3 loss: 0.925624  [  128/  306]
train() client id: f_00004-4-4 loss: 0.848417  [  160/  306]
train() client id: f_00004-4-5 loss: 1.014051  [  192/  306]
train() client id: f_00004-4-6 loss: 0.907023  [  224/  306]
train() client id: f_00004-4-7 loss: 1.016534  [  256/  306]
train() client id: f_00004-4-8 loss: 0.861148  [  288/  306]
train() client id: f_00004-5-0 loss: 0.952079  [   32/  306]
train() client id: f_00004-5-1 loss: 0.870527  [   64/  306]
train() client id: f_00004-5-2 loss: 1.044302  [   96/  306]
train() client id: f_00004-5-3 loss: 0.881488  [  128/  306]
train() client id: f_00004-5-4 loss: 0.873116  [  160/  306]
train() client id: f_00004-5-5 loss: 0.888860  [  192/  306]
train() client id: f_00004-5-6 loss: 0.990139  [  224/  306]
train() client id: f_00004-5-7 loss: 0.903821  [  256/  306]
train() client id: f_00004-5-8 loss: 0.993897  [  288/  306]
train() client id: f_00004-6-0 loss: 0.820378  [   32/  306]
train() client id: f_00004-6-1 loss: 0.945611  [   64/  306]
train() client id: f_00004-6-2 loss: 1.003098  [   96/  306]
train() client id: f_00004-6-3 loss: 1.064363  [  128/  306]
train() client id: f_00004-6-4 loss: 0.927950  [  160/  306]
train() client id: f_00004-6-5 loss: 0.875264  [  192/  306]
train() client id: f_00004-6-6 loss: 1.027983  [  224/  306]
train() client id: f_00004-6-7 loss: 0.856934  [  256/  306]
train() client id: f_00004-6-8 loss: 0.847885  [  288/  306]
train() client id: f_00004-7-0 loss: 0.939772  [   32/  306]
train() client id: f_00004-7-1 loss: 0.896576  [   64/  306]
train() client id: f_00004-7-2 loss: 0.962642  [   96/  306]
train() client id: f_00004-7-3 loss: 0.919892  [  128/  306]
train() client id: f_00004-7-4 loss: 0.860079  [  160/  306]
train() client id: f_00004-7-5 loss: 0.961389  [  192/  306]
train() client id: f_00004-7-6 loss: 0.977346  [  224/  306]
train() client id: f_00004-7-7 loss: 0.884450  [  256/  306]
train() client id: f_00004-7-8 loss: 0.968632  [  288/  306]
train() client id: f_00004-8-0 loss: 0.791879  [   32/  306]
train() client id: f_00004-8-1 loss: 0.919926  [   64/  306]
train() client id: f_00004-8-2 loss: 0.995373  [   96/  306]
train() client id: f_00004-8-3 loss: 0.849894  [  128/  306]
train() client id: f_00004-8-4 loss: 0.904313  [  160/  306]
train() client id: f_00004-8-5 loss: 1.034501  [  192/  306]
train() client id: f_00004-8-6 loss: 0.968510  [  224/  306]
train() client id: f_00004-8-7 loss: 0.888287  [  256/  306]
train() client id: f_00004-8-8 loss: 0.963376  [  288/  306]
train() client id: f_00004-9-0 loss: 0.838362  [   32/  306]
train() client id: f_00004-9-1 loss: 1.106880  [   64/  306]
train() client id: f_00004-9-2 loss: 0.858003  [   96/  306]
train() client id: f_00004-9-3 loss: 0.932626  [  128/  306]
train() client id: f_00004-9-4 loss: 0.955938  [  160/  306]
train() client id: f_00004-9-5 loss: 1.013896  [  192/  306]
train() client id: f_00004-9-6 loss: 0.875696  [  224/  306]
train() client id: f_00004-9-7 loss: 0.919070  [  256/  306]
train() client id: f_00004-9-8 loss: 0.876623  [  288/  306]
train() client id: f_00004-10-0 loss: 0.817070  [   32/  306]
train() client id: f_00004-10-1 loss: 0.960567  [   64/  306]
train() client id: f_00004-10-2 loss: 0.924357  [   96/  306]
train() client id: f_00004-10-3 loss: 0.862800  [  128/  306]
train() client id: f_00004-10-4 loss: 0.940453  [  160/  306]
train() client id: f_00004-10-5 loss: 0.948604  [  192/  306]
train() client id: f_00004-10-6 loss: 1.044234  [  224/  306]
train() client id: f_00004-10-7 loss: 1.002216  [  256/  306]
train() client id: f_00004-10-8 loss: 0.833533  [  288/  306]
train() client id: f_00004-11-0 loss: 0.992742  [   32/  306]
train() client id: f_00004-11-1 loss: 0.903398  [   64/  306]
train() client id: f_00004-11-2 loss: 0.831487  [   96/  306]
train() client id: f_00004-11-3 loss: 0.982661  [  128/  306]
train() client id: f_00004-11-4 loss: 0.838690  [  160/  306]
train() client id: f_00004-11-5 loss: 0.861757  [  192/  306]
train() client id: f_00004-11-6 loss: 1.068036  [  224/  306]
train() client id: f_00004-11-7 loss: 0.815606  [  256/  306]
train() client id: f_00004-11-8 loss: 0.975697  [  288/  306]
train() client id: f_00005-0-0 loss: 0.865856  [   32/  146]
train() client id: f_00005-0-1 loss: 0.825126  [   64/  146]
train() client id: f_00005-0-2 loss: 0.848045  [   96/  146]
train() client id: f_00005-0-3 loss: 0.610669  [  128/  146]
train() client id: f_00005-1-0 loss: 0.776457  [   32/  146]
train() client id: f_00005-1-1 loss: 0.785255  [   64/  146]
train() client id: f_00005-1-2 loss: 0.751353  [   96/  146]
train() client id: f_00005-1-3 loss: 0.784464  [  128/  146]
train() client id: f_00005-2-0 loss: 0.927731  [   32/  146]
train() client id: f_00005-2-1 loss: 0.810808  [   64/  146]
train() client id: f_00005-2-2 loss: 0.481564  [   96/  146]
train() client id: f_00005-2-3 loss: 0.959255  [  128/  146]
train() client id: f_00005-3-0 loss: 0.961627  [   32/  146]
train() client id: f_00005-3-1 loss: 0.713922  [   64/  146]
train() client id: f_00005-3-2 loss: 0.658273  [   96/  146]
train() client id: f_00005-3-3 loss: 0.700351  [  128/  146]
train() client id: f_00005-4-0 loss: 0.825313  [   32/  146]
train() client id: f_00005-4-1 loss: 0.703359  [   64/  146]
train() client id: f_00005-4-2 loss: 0.915942  [   96/  146]
train() client id: f_00005-4-3 loss: 0.687511  [  128/  146]
train() client id: f_00005-5-0 loss: 0.839052  [   32/  146]
train() client id: f_00005-5-1 loss: 0.716763  [   64/  146]
train() client id: f_00005-5-2 loss: 0.920317  [   96/  146]
train() client id: f_00005-5-3 loss: 0.667958  [  128/  146]
train() client id: f_00005-6-0 loss: 0.757118  [   32/  146]
train() client id: f_00005-6-1 loss: 0.502640  [   64/  146]
train() client id: f_00005-6-2 loss: 0.779009  [   96/  146]
train() client id: f_00005-6-3 loss: 1.014724  [  128/  146]
train() client id: f_00005-7-0 loss: 0.778580  [   32/  146]
train() client id: f_00005-7-1 loss: 0.520213  [   64/  146]
train() client id: f_00005-7-2 loss: 0.744849  [   96/  146]
train() client id: f_00005-7-3 loss: 1.096013  [  128/  146]
train() client id: f_00005-8-0 loss: 0.519166  [   32/  146]
train() client id: f_00005-8-1 loss: 0.821469  [   64/  146]
train() client id: f_00005-8-2 loss: 0.948185  [   96/  146]
train() client id: f_00005-8-3 loss: 0.744628  [  128/  146]
train() client id: f_00005-9-0 loss: 0.702322  [   32/  146]
train() client id: f_00005-9-1 loss: 0.773926  [   64/  146]
train() client id: f_00005-9-2 loss: 0.829393  [   96/  146]
train() client id: f_00005-9-3 loss: 0.772151  [  128/  146]
train() client id: f_00005-10-0 loss: 0.858731  [   32/  146]
train() client id: f_00005-10-1 loss: 0.633447  [   64/  146]
train() client id: f_00005-10-2 loss: 0.891104  [   96/  146]
train() client id: f_00005-10-3 loss: 0.874429  [  128/  146]
train() client id: f_00005-11-0 loss: 0.779717  [   32/  146]
train() client id: f_00005-11-1 loss: 0.656171  [   64/  146]
train() client id: f_00005-11-2 loss: 0.872980  [   96/  146]
train() client id: f_00005-11-3 loss: 0.884722  [  128/  146]
train() client id: f_00006-0-0 loss: 0.532392  [   32/   54]
train() client id: f_00006-1-0 loss: 0.591117  [   32/   54]
train() client id: f_00006-2-0 loss: 0.575192  [   32/   54]
train() client id: f_00006-3-0 loss: 0.501379  [   32/   54]
train() client id: f_00006-4-0 loss: 0.505379  [   32/   54]
train() client id: f_00006-5-0 loss: 0.527845  [   32/   54]
train() client id: f_00006-6-0 loss: 0.569881  [   32/   54]
train() client id: f_00006-7-0 loss: 0.526208  [   32/   54]
train() client id: f_00006-8-0 loss: 0.581906  [   32/   54]
train() client id: f_00006-9-0 loss: 0.480707  [   32/   54]
train() client id: f_00006-10-0 loss: 0.512417  [   32/   54]
train() client id: f_00006-11-0 loss: 0.579002  [   32/   54]
train() client id: f_00007-0-0 loss: 0.951383  [   32/  179]
train() client id: f_00007-0-1 loss: 0.536420  [   64/  179]
train() client id: f_00007-0-2 loss: 0.634926  [   96/  179]
train() client id: f_00007-0-3 loss: 0.551420  [  128/  179]
train() client id: f_00007-0-4 loss: 0.718803  [  160/  179]
train() client id: f_00007-1-0 loss: 0.570927  [   32/  179]
train() client id: f_00007-1-1 loss: 0.546076  [   64/  179]
train() client id: f_00007-1-2 loss: 0.870973  [   96/  179]
train() client id: f_00007-1-3 loss: 0.626831  [  128/  179]
train() client id: f_00007-1-4 loss: 0.538740  [  160/  179]
train() client id: f_00007-2-0 loss: 0.887731  [   32/  179]
train() client id: f_00007-2-1 loss: 0.507479  [   64/  179]
train() client id: f_00007-2-2 loss: 0.559365  [   96/  179]
train() client id: f_00007-2-3 loss: 0.543993  [  128/  179]
train() client id: f_00007-2-4 loss: 0.515236  [  160/  179]
train() client id: f_00007-3-0 loss: 0.502586  [   32/  179]
train() client id: f_00007-3-1 loss: 0.553168  [   64/  179]
train() client id: f_00007-3-2 loss: 0.848005  [   96/  179]
train() client id: f_00007-3-3 loss: 0.741538  [  128/  179]
train() client id: f_00007-3-4 loss: 0.489143  [  160/  179]
train() client id: f_00007-4-0 loss: 0.479702  [   32/  179]
train() client id: f_00007-4-1 loss: 0.600904  [   64/  179]
train() client id: f_00007-4-2 loss: 0.762503  [   96/  179]
train() client id: f_00007-4-3 loss: 0.594630  [  128/  179]
train() client id: f_00007-4-4 loss: 0.654321  [  160/  179]
train() client id: f_00007-5-0 loss: 0.470611  [   32/  179]
train() client id: f_00007-5-1 loss: 0.494048  [   64/  179]
train() client id: f_00007-5-2 loss: 0.708116  [   96/  179]
train() client id: f_00007-5-3 loss: 0.510664  [  128/  179]
train() client id: f_00007-5-4 loss: 0.762768  [  160/  179]
train() client id: f_00007-6-0 loss: 0.540483  [   32/  179]
train() client id: f_00007-6-1 loss: 0.683291  [   64/  179]
train() client id: f_00007-6-2 loss: 0.436964  [   96/  179]
train() client id: f_00007-6-3 loss: 0.572542  [  128/  179]
train() client id: f_00007-6-4 loss: 0.702619  [  160/  179]
train() client id: f_00007-7-0 loss: 0.527985  [   32/  179]
train() client id: f_00007-7-1 loss: 0.799605  [   64/  179]
train() client id: f_00007-7-2 loss: 0.478635  [   96/  179]
train() client id: f_00007-7-3 loss: 0.577851  [  128/  179]
train() client id: f_00007-7-4 loss: 0.638625  [  160/  179]
train() client id: f_00007-8-0 loss: 0.619649  [   32/  179]
train() client id: f_00007-8-1 loss: 0.581398  [   64/  179]
train() client id: f_00007-8-2 loss: 0.401090  [   96/  179]
train() client id: f_00007-8-3 loss: 0.676860  [  128/  179]
train() client id: f_00007-8-4 loss: 0.691446  [  160/  179]
train() client id: f_00007-9-0 loss: 0.489079  [   32/  179]
train() client id: f_00007-9-1 loss: 0.446255  [   64/  179]
train() client id: f_00007-9-2 loss: 0.679360  [   96/  179]
train() client id: f_00007-9-3 loss: 0.636473  [  128/  179]
train() client id: f_00007-9-4 loss: 0.578749  [  160/  179]
train() client id: f_00007-10-0 loss: 0.520167  [   32/  179]
train() client id: f_00007-10-1 loss: 0.540886  [   64/  179]
train() client id: f_00007-10-2 loss: 0.443612  [   96/  179]
train() client id: f_00007-10-3 loss: 0.629858  [  128/  179]
train() client id: f_00007-10-4 loss: 0.811608  [  160/  179]
train() client id: f_00007-11-0 loss: 0.555669  [   32/  179]
train() client id: f_00007-11-1 loss: 0.560621  [   64/  179]
train() client id: f_00007-11-2 loss: 0.601582  [   96/  179]
train() client id: f_00007-11-3 loss: 0.680790  [  128/  179]
train() client id: f_00007-11-4 loss: 0.421489  [  160/  179]
train() client id: f_00008-0-0 loss: 0.804769  [   32/  130]
train() client id: f_00008-0-1 loss: 0.756066  [   64/  130]
train() client id: f_00008-0-2 loss: 0.769953  [   96/  130]
train() client id: f_00008-0-3 loss: 0.758586  [  128/  130]
train() client id: f_00008-1-0 loss: 0.768853  [   32/  130]
train() client id: f_00008-1-1 loss: 0.761719  [   64/  130]
train() client id: f_00008-1-2 loss: 0.836763  [   96/  130]
train() client id: f_00008-1-3 loss: 0.751870  [  128/  130]
train() client id: f_00008-2-0 loss: 0.741287  [   32/  130]
train() client id: f_00008-2-1 loss: 0.752603  [   64/  130]
train() client id: f_00008-2-2 loss: 0.923139  [   96/  130]
train() client id: f_00008-2-3 loss: 0.690205  [  128/  130]
train() client id: f_00008-3-0 loss: 0.954641  [   32/  130]
train() client id: f_00008-3-1 loss: 0.709893  [   64/  130]
train() client id: f_00008-3-2 loss: 0.742837  [   96/  130]
train() client id: f_00008-3-3 loss: 0.706533  [  128/  130]
train() client id: f_00008-4-0 loss: 0.853969  [   32/  130]
train() client id: f_00008-4-1 loss: 0.730773  [   64/  130]
train() client id: f_00008-4-2 loss: 0.692121  [   96/  130]
train() client id: f_00008-4-3 loss: 0.845073  [  128/  130]
train() client id: f_00008-5-0 loss: 0.819770  [   32/  130]
train() client id: f_00008-5-1 loss: 0.710227  [   64/  130]
train() client id: f_00008-5-2 loss: 0.801685  [   96/  130]
train() client id: f_00008-5-3 loss: 0.782691  [  128/  130]
train() client id: f_00008-6-0 loss: 0.709958  [   32/  130]
train() client id: f_00008-6-1 loss: 0.742699  [   64/  130]
train() client id: f_00008-6-2 loss: 0.902408  [   96/  130]
train() client id: f_00008-6-3 loss: 0.752320  [  128/  130]
train() client id: f_00008-7-0 loss: 0.921275  [   32/  130]
train() client id: f_00008-7-1 loss: 0.737011  [   64/  130]
train() client id: f_00008-7-2 loss: 0.664742  [   96/  130]
train() client id: f_00008-7-3 loss: 0.760722  [  128/  130]
train() client id: f_00008-8-0 loss: 0.712328  [   32/  130]
train() client id: f_00008-8-1 loss: 0.838483  [   64/  130]
train() client id: f_00008-8-2 loss: 0.888140  [   96/  130]
train() client id: f_00008-8-3 loss: 0.673024  [  128/  130]
train() client id: f_00008-9-0 loss: 0.796388  [   32/  130]
train() client id: f_00008-9-1 loss: 0.760826  [   64/  130]
train() client id: f_00008-9-2 loss: 0.833089  [   96/  130]
train() client id: f_00008-9-3 loss: 0.704523  [  128/  130]
train() client id: f_00008-10-0 loss: 0.789197  [   32/  130]
train() client id: f_00008-10-1 loss: 0.758523  [   64/  130]
train() client id: f_00008-10-2 loss: 0.722625  [   96/  130]
train() client id: f_00008-10-3 loss: 0.838966  [  128/  130]
train() client id: f_00008-11-0 loss: 0.774951  [   32/  130]
train() client id: f_00008-11-1 loss: 0.875657  [   64/  130]
train() client id: f_00008-11-2 loss: 0.698601  [   96/  130]
train() client id: f_00008-11-3 loss: 0.754602  [  128/  130]
train() client id: f_00009-0-0 loss: 1.037330  [   32/  118]
train() client id: f_00009-0-1 loss: 1.046652  [   64/  118]
train() client id: f_00009-0-2 loss: 1.033486  [   96/  118]
train() client id: f_00009-1-0 loss: 1.101977  [   32/  118]
train() client id: f_00009-1-1 loss: 0.986033  [   64/  118]
train() client id: f_00009-1-2 loss: 1.022759  [   96/  118]
train() client id: f_00009-2-0 loss: 0.957669  [   32/  118]
train() client id: f_00009-2-1 loss: 1.050020  [   64/  118]
train() client id: f_00009-2-2 loss: 0.830944  [   96/  118]
train() client id: f_00009-3-0 loss: 0.890850  [   32/  118]
train() client id: f_00009-3-1 loss: 1.050891  [   64/  118]
train() client id: f_00009-3-2 loss: 0.917581  [   96/  118]
train() client id: f_00009-4-0 loss: 1.032471  [   32/  118]
train() client id: f_00009-4-1 loss: 0.841651  [   64/  118]
train() client id: f_00009-4-2 loss: 1.060461  [   96/  118]
train() client id: f_00009-5-0 loss: 0.893832  [   32/  118]
train() client id: f_00009-5-1 loss: 0.947686  [   64/  118]
train() client id: f_00009-5-2 loss: 0.738452  [   96/  118]
train() client id: f_00009-6-0 loss: 0.945853  [   32/  118]
train() client id: f_00009-6-1 loss: 0.816558  [   64/  118]
train() client id: f_00009-6-2 loss: 0.969528  [   96/  118]
train() client id: f_00009-7-0 loss: 1.044796  [   32/  118]
train() client id: f_00009-7-1 loss: 0.885390  [   64/  118]
train() client id: f_00009-7-2 loss: 0.769083  [   96/  118]
train() client id: f_00009-8-0 loss: 0.883183  [   32/  118]
train() client id: f_00009-8-1 loss: 0.846629  [   64/  118]
train() client id: f_00009-8-2 loss: 0.915283  [   96/  118]
train() client id: f_00009-9-0 loss: 0.908516  [   32/  118]
train() client id: f_00009-9-1 loss: 0.920328  [   64/  118]
train() client id: f_00009-9-2 loss: 0.872825  [   96/  118]
train() client id: f_00009-10-0 loss: 0.955379  [   32/  118]
train() client id: f_00009-10-1 loss: 0.882704  [   64/  118]
train() client id: f_00009-10-2 loss: 0.738507  [   96/  118]
train() client id: f_00009-11-0 loss: 0.833120  [   32/  118]
train() client id: f_00009-11-1 loss: 0.747523  [   64/  118]
train() client id: f_00009-11-2 loss: 1.101338  [   96/  118]
At round 48 accuracy: 0.6445623342175066
At round 48 training accuracy: 0.590878604963112
At round 48 training loss: 0.8371098423869302
update_location
xs = [  -3.9056584     4.20031788  260.00902392   18.81129433    0.97929623
    3.95640986 -222.44319194 -201.32485185  244.66397685 -187.06087855]
ys = [ 252.5879595   235.55583871    1.32061395 -222.45517586  214.35018685
  197.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [271.6908748  255.93787492 278.57931822 244.62250522 236.53110075
 221.68917748 243.90134108 224.79406626 264.89456684 212.15038096]
dists_bs = [187.86475237 189.15539519 468.39874042 442.19162659 180.32512278
 180.40477296 183.85181945 176.15576144 448.22134189 171.42332987]
uav_gains = [4.75955329e-12 6.59322551e-12 4.11980872e-12 8.22994915e-12
 9.55255008e-12 1.22821086e-11 8.34286394e-12 1.16790101e-11
 5.48618892e-12 1.42491555e-11]
bs_gains = [4.74818064e-11 4.65802305e-11 3.67764727e-12 4.32100377e-12
 5.32520858e-11 5.31862805e-11 5.04410218e-11 5.68568637e-11
 4.16020754e-12 6.13618281e-11]
Round 49
-------------------------------
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.53180875  9.28555743  4.47092374  1.6235926  10.7068955   5.1513321
  2.0049889   6.33296008  4.67865356  4.17679842]
obj_prev = 52.96351106757935
eta_min = 4.606985121778528e-21	eta_max = 0.9400638118405741
af = 11.129531482767057	bf = 1.180489388448874	zeta = 12.242484631043764	eta = 0.909090909090909
af = 11.129531482767057	bf = 1.180489388448874	zeta = 24.65371738730223	eta = 0.45143421204703454
af = 11.129531482767057	bf = 1.180489388448874	zeta = 18.287853905358034	eta = 0.608575043324591
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.136885852504452	eta = 0.6494488892881634
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.071238826404898	eta = 0.6519463289068677
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.07100360444631	eta = 0.6519553120982448
eta = 0.6519553120982448
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [0.03529932 0.07424063 0.03473901 0.01204659 0.08572692 0.04090239
 0.01512828 0.05014745 0.03641993 0.03305809]
ene_total = [1.60196419 2.67053184 1.60796312 0.76579092 3.04020143 1.57233453
 0.86514606 1.976894   1.66129063 1.30888688]
ti_comp = [0.70507767 0.76819322 0.69764277 0.72788409 0.77017386 0.77015604
 0.7283619  0.73907045 0.6981679  0.77215943]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [5.52974468e-06 4.33375200e-05 5.38351868e-06 2.06228308e-07
 6.63825764e-05 7.21055148e-06 4.07900477e-07 1.44296163e-05
 6.19409234e-06 3.78703601e-06]
ene_total = [0.44615416 0.2406999  0.47049736 0.37129277 0.23496834 0.23308889
 0.36973465 0.33512519 0.46880422 0.22641604]
optimize_network_iter = 0 obj = 3.396781522582919
eta = 0.6519553120982448
freqs = [25032220.23451398 48321587.59240132 24897421.60469212  8275076.45541563
 55654263.85367705 26554609.35010472 10385139.39706882 33926025.18152012
 26082502.95796476 21406261.0487775 ]
eta_min = 0.6519553120982468	eta_max = 0.6979914102198819
af = 0.004073073698070901	bf = 1.180489388448874	zeta = 0.004480381067877992	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [1.23203954e-06 9.65569680e-06 1.19946005e-06 4.59481302e-08
 1.47901871e-05 1.60652707e-06 9.08811424e-08 3.21495093e-06
 1.38005769e-06 8.43760128e-07]
ene_total = [1.70755859 0.91729463 1.80077061 1.42147114 0.89310582 0.89167624
 1.41548626 1.28161791 1.79420944 0.86646288]
ti_comp = [0.59380367 0.65691922 0.58636878 0.6166101  0.65889986 0.65888204
 0.6170879  0.62779645 0.5868939  0.66088543]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.62920539e-06 3.51879741e-05 4.52484764e-06 1.70633650e-07
 5.38525197e-05 5.84956806e-06 3.37417651e-07 1.18741342e-05
 5.20464897e-06 3.06954669e-06]
ene_total = [0.51412882 0.27708296 0.54218432 0.42788873 0.2703124  0.26856798
 0.42609178 0.38611291 0.54022815 0.26090226]
optimize_network_iter = 1 obj = 3.9135003002771818
eta = 0.6979914102198819
freqs = [24982229.19460944 47493855.941672   24897421.60469214  8210343.90468077
 54677108.13463603 26088475.84559726 10302681.03588401 33568945.36426646
 26078780.65421337 21021289.55092173]
eta_min = 0.697991410219886	eta_max = 0.6979914102198779
af = 0.003953064498406836	bf = 1.180489388448874	zeta = 0.004348370948247519	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [1.22712352e-06 9.32773270e-06 1.19946005e-06 4.52320749e-08
 1.42753859e-05 1.55062088e-06 8.94436732e-08 3.14763077e-06
 1.37966381e-06 8.13684556e-07]
ene_total = [1.70755797 0.91725351 1.80077061 1.42147105 0.89304128 0.89166923
 1.41548608 1.28160947 1.79420939 0.86645911]
ti_comp = [0.59380367 0.65691922 0.58636878 0.6166101  0.65889986 0.65888204
 0.6170879  0.62779645 0.5868939  0.66088543]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.62920539e-06 3.51879741e-05 4.52484764e-06 1.70633650e-07
 5.38525197e-05 5.84956806e-06 3.37417651e-07 1.18741342e-05
 5.20464897e-06 3.06954669e-06]
ene_total = [0.51412882 0.27708296 0.54218432 0.42788873 0.2703124  0.26856798
 0.42609178 0.38611291 0.54022815 0.26090226]
optimize_network_iter = 2 obj = 3.9135003002771303
eta = 0.6979914102198779
freqs = [24982229.19460944 47493855.94167206 24897421.60469212  8210343.90468077
 54677108.13463611 26088475.8455973  10302681.03588402 33568945.36426648
 26078780.65421337 21021289.55092177]
Done!
At round 49 eta: 0.6979914102198779
At round 49 local rounds: 11.773444044723618
At round 49 global rounds: 37.74017276480797
At round 49 a_n: 11.055310507788512
gradient difference: 0.36857888102531433
train() client id: f_00000-0-0 loss: 1.346077  [   32/  126]
train() client id: f_00000-0-1 loss: 1.081326  [   64/  126]
train() client id: f_00000-0-2 loss: 1.135500  [   96/  126]
train() client id: f_00000-1-0 loss: 1.191128  [   32/  126]
train() client id: f_00000-1-1 loss: 1.188702  [   64/  126]
train() client id: f_00000-1-2 loss: 1.130130  [   96/  126]
train() client id: f_00000-2-0 loss: 1.031739  [   32/  126]
train() client id: f_00000-2-1 loss: 1.199639  [   64/  126]
train() client id: f_00000-2-2 loss: 1.013639  [   96/  126]
train() client id: f_00000-3-0 loss: 0.941145  [   32/  126]
train() client id: f_00000-3-1 loss: 1.006200  [   64/  126]
train() client id: f_00000-3-2 loss: 0.995415  [   96/  126]
train() client id: f_00000-4-0 loss: 1.063997  [   32/  126]
train() client id: f_00000-4-1 loss: 0.822016  [   64/  126]
train() client id: f_00000-4-2 loss: 0.989595  [   96/  126]
train() client id: f_00000-5-0 loss: 0.851384  [   32/  126]
train() client id: f_00000-5-1 loss: 0.934442  [   64/  126]
train() client id: f_00000-5-2 loss: 0.967514  [   96/  126]
train() client id: f_00000-6-0 loss: 0.877223  [   32/  126]
train() client id: f_00000-6-1 loss: 0.849647  [   64/  126]
train() client id: f_00000-6-2 loss: 0.935765  [   96/  126]
train() client id: f_00000-7-0 loss: 0.842687  [   32/  126]
train() client id: f_00000-7-1 loss: 0.890541  [   64/  126]
train() client id: f_00000-7-2 loss: 0.882828  [   96/  126]
train() client id: f_00000-8-0 loss: 0.798551  [   32/  126]
train() client id: f_00000-8-1 loss: 0.932232  [   64/  126]
train() client id: f_00000-8-2 loss: 0.818009  [   96/  126]
train() client id: f_00000-9-0 loss: 0.945333  [   32/  126]
train() client id: f_00000-9-1 loss: 0.878321  [   64/  126]
train() client id: f_00000-9-2 loss: 0.882183  [   96/  126]
train() client id: f_00000-10-0 loss: 0.928054  [   32/  126]
train() client id: f_00000-10-1 loss: 0.924475  [   64/  126]
train() client id: f_00000-10-2 loss: 0.730654  [   96/  126]
train() client id: f_00001-0-0 loss: 0.382612  [   32/  265]
train() client id: f_00001-0-1 loss: 0.477343  [   64/  265]
train() client id: f_00001-0-2 loss: 0.423972  [   96/  265]
train() client id: f_00001-0-3 loss: 0.534973  [  128/  265]
train() client id: f_00001-0-4 loss: 0.425140  [  160/  265]
train() client id: f_00001-0-5 loss: 0.391014  [  192/  265]
train() client id: f_00001-0-6 loss: 0.436690  [  224/  265]
train() client id: f_00001-0-7 loss: 0.635538  [  256/  265]
train() client id: f_00001-1-0 loss: 0.579488  [   32/  265]
train() client id: f_00001-1-1 loss: 0.384117  [   64/  265]
train() client id: f_00001-1-2 loss: 0.389479  [   96/  265]
train() client id: f_00001-1-3 loss: 0.372047  [  128/  265]
train() client id: f_00001-1-4 loss: 0.414214  [  160/  265]
train() client id: f_00001-1-5 loss: 0.471743  [  192/  265]
train() client id: f_00001-1-6 loss: 0.505543  [  224/  265]
train() client id: f_00001-1-7 loss: 0.529709  [  256/  265]
train() client id: f_00001-2-0 loss: 0.423691  [   32/  265]
train() client id: f_00001-2-1 loss: 0.603293  [   64/  265]
train() client id: f_00001-2-2 loss: 0.523782  [   96/  265]
train() client id: f_00001-2-3 loss: 0.435293  [  128/  265]
train() client id: f_00001-2-4 loss: 0.350155  [  160/  265]
train() client id: f_00001-2-5 loss: 0.431341  [  192/  265]
train() client id: f_00001-2-6 loss: 0.377154  [  224/  265]
train() client id: f_00001-2-7 loss: 0.478697  [  256/  265]
train() client id: f_00001-3-0 loss: 0.475368  [   32/  265]
train() client id: f_00001-3-1 loss: 0.396093  [   64/  265]
train() client id: f_00001-3-2 loss: 0.548051  [   96/  265]
train() client id: f_00001-3-3 loss: 0.511789  [  128/  265]
train() client id: f_00001-3-4 loss: 0.397737  [  160/  265]
train() client id: f_00001-3-5 loss: 0.359288  [  192/  265]
train() client id: f_00001-3-6 loss: 0.349481  [  224/  265]
train() client id: f_00001-3-7 loss: 0.554744  [  256/  265]
train() client id: f_00001-4-0 loss: 0.354496  [   32/  265]
train() client id: f_00001-4-1 loss: 0.374377  [   64/  265]
train() client id: f_00001-4-2 loss: 0.554834  [   96/  265]
train() client id: f_00001-4-3 loss: 0.535215  [  128/  265]
train() client id: f_00001-4-4 loss: 0.366785  [  160/  265]
train() client id: f_00001-4-5 loss: 0.379334  [  192/  265]
train() client id: f_00001-4-6 loss: 0.602816  [  224/  265]
train() client id: f_00001-4-7 loss: 0.397565  [  256/  265]
train() client id: f_00001-5-0 loss: 0.522622  [   32/  265]
train() client id: f_00001-5-1 loss: 0.358159  [   64/  265]
train() client id: f_00001-5-2 loss: 0.493679  [   96/  265]
train() client id: f_00001-5-3 loss: 0.468426  [  128/  265]
train() client id: f_00001-5-4 loss: 0.497475  [  160/  265]
train() client id: f_00001-5-5 loss: 0.396741  [  192/  265]
train() client id: f_00001-5-6 loss: 0.466759  [  224/  265]
train() client id: f_00001-5-7 loss: 0.353211  [  256/  265]
train() client id: f_00001-6-0 loss: 0.477525  [   32/  265]
train() client id: f_00001-6-1 loss: 0.415993  [   64/  265]
train() client id: f_00001-6-2 loss: 0.459410  [   96/  265]
train() client id: f_00001-6-3 loss: 0.344765  [  128/  265]
train() client id: f_00001-6-4 loss: 0.515323  [  160/  265]
train() client id: f_00001-6-5 loss: 0.495701  [  192/  265]
train() client id: f_00001-6-6 loss: 0.369684  [  224/  265]
train() client id: f_00001-6-7 loss: 0.406947  [  256/  265]
train() client id: f_00001-7-0 loss: 0.492887  [   32/  265]
train() client id: f_00001-7-1 loss: 0.469654  [   64/  265]
train() client id: f_00001-7-2 loss: 0.463275  [   96/  265]
train() client id: f_00001-7-3 loss: 0.444644  [  128/  265]
train() client id: f_00001-7-4 loss: 0.518267  [  160/  265]
train() client id: f_00001-7-5 loss: 0.357342  [  192/  265]
train() client id: f_00001-7-6 loss: 0.413097  [  224/  265]
train() client id: f_00001-7-7 loss: 0.367327  [  256/  265]
train() client id: f_00001-8-0 loss: 0.492047  [   32/  265]
train() client id: f_00001-8-1 loss: 0.344102  [   64/  265]
train() client id: f_00001-8-2 loss: 0.494913  [   96/  265]
train() client id: f_00001-8-3 loss: 0.423210  [  128/  265]
train() client id: f_00001-8-4 loss: 0.339105  [  160/  265]
train() client id: f_00001-8-5 loss: 0.516646  [  192/  265]
train() client id: f_00001-8-6 loss: 0.351112  [  224/  265]
train() client id: f_00001-8-7 loss: 0.557802  [  256/  265]
train() client id: f_00001-9-0 loss: 0.476348  [   32/  265]
train() client id: f_00001-9-1 loss: 0.354595  [   64/  265]
train() client id: f_00001-9-2 loss: 0.405974  [   96/  265]
train() client id: f_00001-9-3 loss: 0.484548  [  128/  265]
train() client id: f_00001-9-4 loss: 0.365518  [  160/  265]
train() client id: f_00001-9-5 loss: 0.366855  [  192/  265]
train() client id: f_00001-9-6 loss: 0.527467  [  224/  265]
train() client id: f_00001-9-7 loss: 0.549474  [  256/  265]
train() client id: f_00001-10-0 loss: 0.495316  [   32/  265]
train() client id: f_00001-10-1 loss: 0.429324  [   64/  265]
train() client id: f_00001-10-2 loss: 0.612970  [   96/  265]
train() client id: f_00001-10-3 loss: 0.384437  [  128/  265]
train() client id: f_00001-10-4 loss: 0.417096  [  160/  265]
train() client id: f_00001-10-5 loss: 0.354403  [  192/  265]
train() client id: f_00001-10-6 loss: 0.337092  [  224/  265]
train() client id: f_00001-10-7 loss: 0.407327  [  256/  265]
train() client id: f_00002-0-0 loss: 1.100178  [   32/  124]
train() client id: f_00002-0-1 loss: 1.121368  [   64/  124]
train() client id: f_00002-0-2 loss: 1.152707  [   96/  124]
train() client id: f_00002-1-0 loss: 1.013386  [   32/  124]
train() client id: f_00002-1-1 loss: 1.065493  [   64/  124]
train() client id: f_00002-1-2 loss: 1.090511  [   96/  124]
train() client id: f_00002-2-0 loss: 0.992528  [   32/  124]
train() client id: f_00002-2-1 loss: 0.984829  [   64/  124]
train() client id: f_00002-2-2 loss: 1.018667  [   96/  124]
train() client id: f_00002-3-0 loss: 0.929914  [   32/  124]
train() client id: f_00002-3-1 loss: 0.977463  [   64/  124]
train() client id: f_00002-3-2 loss: 1.077127  [   96/  124]
train() client id: f_00002-4-0 loss: 0.902592  [   32/  124]
train() client id: f_00002-4-1 loss: 0.886498  [   64/  124]
train() client id: f_00002-4-2 loss: 0.904446  [   96/  124]
train() client id: f_00002-5-0 loss: 0.882322  [   32/  124]
train() client id: f_00002-5-1 loss: 0.909708  [   64/  124]
train() client id: f_00002-5-2 loss: 0.939193  [   96/  124]
train() client id: f_00002-6-0 loss: 0.787527  [   32/  124]
train() client id: f_00002-6-1 loss: 0.880135  [   64/  124]
train() client id: f_00002-6-2 loss: 0.995661  [   96/  124]
train() client id: f_00002-7-0 loss: 0.812688  [   32/  124]
train() client id: f_00002-7-1 loss: 0.849860  [   64/  124]
train() client id: f_00002-7-2 loss: 0.789014  [   96/  124]
train() client id: f_00002-8-0 loss: 0.968719  [   32/  124]
train() client id: f_00002-8-1 loss: 0.746802  [   64/  124]
train() client id: f_00002-8-2 loss: 0.755342  [   96/  124]
train() client id: f_00002-9-0 loss: 0.678183  [   32/  124]
train() client id: f_00002-9-1 loss: 0.987600  [   64/  124]
train() client id: f_00002-9-2 loss: 0.791250  [   96/  124]
train() client id: f_00002-10-0 loss: 0.627504  [   32/  124]
train() client id: f_00002-10-1 loss: 0.833448  [   64/  124]
train() client id: f_00002-10-2 loss: 0.800257  [   96/  124]
train() client id: f_00003-0-0 loss: 0.531914  [   32/   43]
train() client id: f_00003-1-0 loss: 0.718602  [   32/   43]
train() client id: f_00003-2-0 loss: 0.683762  [   32/   43]
train() client id: f_00003-3-0 loss: 0.659374  [   32/   43]
train() client id: f_00003-4-0 loss: 0.746980  [   32/   43]
train() client id: f_00003-5-0 loss: 0.794245  [   32/   43]
train() client id: f_00003-6-0 loss: 0.788245  [   32/   43]
train() client id: f_00003-7-0 loss: 0.572639  [   32/   43]
train() client id: f_00003-8-0 loss: 0.655989  [   32/   43]
train() client id: f_00003-9-0 loss: 0.678649  [   32/   43]
train() client id: f_00003-10-0 loss: 0.823938  [   32/   43]
train() client id: f_00004-0-0 loss: 0.908336  [   32/  306]
train() client id: f_00004-0-1 loss: 0.669117  [   64/  306]
train() client id: f_00004-0-2 loss: 0.696088  [   96/  306]
train() client id: f_00004-0-3 loss: 0.650392  [  128/  306]
train() client id: f_00004-0-4 loss: 0.801504  [  160/  306]
train() client id: f_00004-0-5 loss: 0.750099  [  192/  306]
train() client id: f_00004-0-6 loss: 0.801375  [  224/  306]
train() client id: f_00004-0-7 loss: 0.702141  [  256/  306]
train() client id: f_00004-0-8 loss: 0.757102  [  288/  306]
train() client id: f_00004-1-0 loss: 0.688396  [   32/  306]
train() client id: f_00004-1-1 loss: 0.725567  [   64/  306]
train() client id: f_00004-1-2 loss: 0.742326  [   96/  306]
train() client id: f_00004-1-3 loss: 0.777771  [  128/  306]
train() client id: f_00004-1-4 loss: 0.867848  [  160/  306]
train() client id: f_00004-1-5 loss: 0.779921  [  192/  306]
train() client id: f_00004-1-6 loss: 0.650481  [  224/  306]
train() client id: f_00004-1-7 loss: 0.655686  [  256/  306]
train() client id: f_00004-1-8 loss: 0.770455  [  288/  306]
train() client id: f_00004-2-0 loss: 0.845629  [   32/  306]
train() client id: f_00004-2-1 loss: 0.856139  [   64/  306]
train() client id: f_00004-2-2 loss: 0.657946  [   96/  306]
train() client id: f_00004-2-3 loss: 0.786057  [  128/  306]
train() client id: f_00004-2-4 loss: 0.734460  [  160/  306]
train() client id: f_00004-2-5 loss: 0.747337  [  192/  306]
train() client id: f_00004-2-6 loss: 0.703455  [  224/  306]
train() client id: f_00004-2-7 loss: 0.775113  [  256/  306]
train() client id: f_00004-2-8 loss: 0.643688  [  288/  306]
train() client id: f_00004-3-0 loss: 0.672722  [   32/  306]
train() client id: f_00004-3-1 loss: 0.834643  [   64/  306]
train() client id: f_00004-3-2 loss: 0.696888  [   96/  306]
train() client id: f_00004-3-3 loss: 0.736629  [  128/  306]
train() client id: f_00004-3-4 loss: 0.668817  [  160/  306]
train() client id: f_00004-3-5 loss: 0.832966  [  192/  306]
train() client id: f_00004-3-6 loss: 0.731116  [  224/  306]
train() client id: f_00004-3-7 loss: 0.821364  [  256/  306]
train() client id: f_00004-3-8 loss: 0.803883  [  288/  306]
train() client id: f_00004-4-0 loss: 0.742206  [   32/  306]
train() client id: f_00004-4-1 loss: 0.799644  [   64/  306]
train() client id: f_00004-4-2 loss: 0.875819  [   96/  306]
train() client id: f_00004-4-3 loss: 0.720003  [  128/  306]
train() client id: f_00004-4-4 loss: 0.749776  [  160/  306]
train() client id: f_00004-4-5 loss: 0.747218  [  192/  306]
train() client id: f_00004-4-6 loss: 0.760862  [  224/  306]
train() client id: f_00004-4-7 loss: 0.671398  [  256/  306]
train() client id: f_00004-4-8 loss: 0.622624  [  288/  306]
train() client id: f_00004-5-0 loss: 0.672092  [   32/  306]
train() client id: f_00004-5-1 loss: 0.764056  [   64/  306]
train() client id: f_00004-5-2 loss: 0.884830  [   96/  306]
train() client id: f_00004-5-3 loss: 0.701744  [  128/  306]
train() client id: f_00004-5-4 loss: 0.762902  [  160/  306]
train() client id: f_00004-5-5 loss: 0.768044  [  192/  306]
train() client id: f_00004-5-6 loss: 0.673433  [  224/  306]
train() client id: f_00004-5-7 loss: 0.767824  [  256/  306]
train() client id: f_00004-5-8 loss: 0.756342  [  288/  306]
train() client id: f_00004-6-0 loss: 0.666456  [   32/  306]
train() client id: f_00004-6-1 loss: 0.688050  [   64/  306]
train() client id: f_00004-6-2 loss: 0.680178  [   96/  306]
train() client id: f_00004-6-3 loss: 0.750958  [  128/  306]
train() client id: f_00004-6-4 loss: 0.803796  [  160/  306]
train() client id: f_00004-6-5 loss: 0.671492  [  192/  306]
train() client id: f_00004-6-6 loss: 0.737276  [  224/  306]
train() client id: f_00004-6-7 loss: 0.743785  [  256/  306]
train() client id: f_00004-6-8 loss: 0.900434  [  288/  306]
train() client id: f_00004-7-0 loss: 0.786497  [   32/  306]
train() client id: f_00004-7-1 loss: 0.782733  [   64/  306]
train() client id: f_00004-7-2 loss: 0.686682  [   96/  306]
train() client id: f_00004-7-3 loss: 0.858147  [  128/  306]
train() client id: f_00004-7-4 loss: 0.688785  [  160/  306]
train() client id: f_00004-7-5 loss: 0.610938  [  192/  306]
train() client id: f_00004-7-6 loss: 0.842382  [  224/  306]
train() client id: f_00004-7-7 loss: 0.779921  [  256/  306]
train() client id: f_00004-7-8 loss: 0.756617  [  288/  306]
train() client id: f_00004-8-0 loss: 0.768612  [   32/  306]
train() client id: f_00004-8-1 loss: 0.832244  [   64/  306]
train() client id: f_00004-8-2 loss: 0.746753  [   96/  306]
train() client id: f_00004-8-3 loss: 0.705549  [  128/  306]
train() client id: f_00004-8-4 loss: 0.694980  [  160/  306]
train() client id: f_00004-8-5 loss: 0.733118  [  192/  306]
train() client id: f_00004-8-6 loss: 0.687121  [  224/  306]
train() client id: f_00004-8-7 loss: 0.930916  [  256/  306]
train() client id: f_00004-8-8 loss: 0.773055  [  288/  306]
train() client id: f_00004-9-0 loss: 0.771776  [   32/  306]
train() client id: f_00004-9-1 loss: 0.725507  [   64/  306]
train() client id: f_00004-9-2 loss: 0.620808  [   96/  306]
train() client id: f_00004-9-3 loss: 0.719111  [  128/  306]
train() client id: f_00004-9-4 loss: 0.787312  [  160/  306]
train() client id: f_00004-9-5 loss: 0.919745  [  192/  306]
train() client id: f_00004-9-6 loss: 0.806056  [  224/  306]
train() client id: f_00004-9-7 loss: 0.796468  [  256/  306]
train() client id: f_00004-9-8 loss: 0.689000  [  288/  306]
train() client id: f_00004-10-0 loss: 0.635253  [   32/  306]
train() client id: f_00004-10-1 loss: 0.749642  [   64/  306]
train() client id: f_00004-10-2 loss: 0.765337  [   96/  306]
train() client id: f_00004-10-3 loss: 0.615729  [  128/  306]
train() client id: f_00004-10-4 loss: 0.836897  [  160/  306]
train() client id: f_00004-10-5 loss: 0.846280  [  192/  306]
train() client id: f_00004-10-6 loss: 0.849102  [  224/  306]
train() client id: f_00004-10-7 loss: 0.792008  [  256/  306]
train() client id: f_00004-10-8 loss: 0.743075  [  288/  306]
train() client id: f_00005-0-0 loss: 0.416509  [   32/  146]
train() client id: f_00005-0-1 loss: 0.482971  [   64/  146]
train() client id: f_00005-0-2 loss: 0.484554  [   96/  146]
train() client id: f_00005-0-3 loss: 0.630540  [  128/  146]
train() client id: f_00005-1-0 loss: 0.329118  [   32/  146]
train() client id: f_00005-1-1 loss: 0.575126  [   64/  146]
train() client id: f_00005-1-2 loss: 0.654883  [   96/  146]
train() client id: f_00005-1-3 loss: 0.529768  [  128/  146]
train() client id: f_00005-2-0 loss: 0.398980  [   32/  146]
train() client id: f_00005-2-1 loss: 0.554710  [   64/  146]
train() client id: f_00005-2-2 loss: 0.453499  [   96/  146]
train() client id: f_00005-2-3 loss: 0.601840  [  128/  146]
train() client id: f_00005-3-0 loss: 0.248527  [   32/  146]
train() client id: f_00005-3-1 loss: 0.594286  [   64/  146]
train() client id: f_00005-3-2 loss: 0.614203  [   96/  146]
train() client id: f_00005-3-3 loss: 0.649083  [  128/  146]
train() client id: f_00005-4-0 loss: 0.573759  [   32/  146]
train() client id: f_00005-4-1 loss: 0.483715  [   64/  146]
train() client id: f_00005-4-2 loss: 0.454163  [   96/  146]
train() client id: f_00005-4-3 loss: 0.466521  [  128/  146]
train() client id: f_00005-5-0 loss: 0.412627  [   32/  146]
train() client id: f_00005-5-1 loss: 0.395943  [   64/  146]
train() client id: f_00005-5-2 loss: 0.598764  [   96/  146]
train() client id: f_00005-5-3 loss: 0.627773  [  128/  146]
train() client id: f_00005-6-0 loss: 0.549938  [   32/  146]
train() client id: f_00005-6-1 loss: 0.377292  [   64/  146]
train() client id: f_00005-6-2 loss: 0.446664  [   96/  146]
train() client id: f_00005-6-3 loss: 0.478170  [  128/  146]
train() client id: f_00005-7-0 loss: 0.453101  [   32/  146]
train() client id: f_00005-7-1 loss: 0.607135  [   64/  146]
train() client id: f_00005-7-2 loss: 0.455314  [   96/  146]
train() client id: f_00005-7-3 loss: 0.505961  [  128/  146]
train() client id: f_00005-8-0 loss: 0.459961  [   32/  146]
train() client id: f_00005-8-1 loss: 0.335678  [   64/  146]
train() client id: f_00005-8-2 loss: 0.650455  [   96/  146]
train() client id: f_00005-8-3 loss: 0.554601  [  128/  146]
train() client id: f_00005-9-0 loss: 0.300290  [   32/  146]
train() client id: f_00005-9-1 loss: 0.374836  [   64/  146]
train() client id: f_00005-9-2 loss: 0.535713  [   96/  146]
train() client id: f_00005-9-3 loss: 0.632983  [  128/  146]
train() client id: f_00005-10-0 loss: 0.387459  [   32/  146]
train() client id: f_00005-10-1 loss: 0.603703  [   64/  146]
train() client id: f_00005-10-2 loss: 0.490551  [   96/  146]
train() client id: f_00005-10-3 loss: 0.538708  [  128/  146]
train() client id: f_00006-0-0 loss: 0.441175  [   32/   54]
train() client id: f_00006-1-0 loss: 0.442799  [   32/   54]
train() client id: f_00006-2-0 loss: 0.448696  [   32/   54]
train() client id: f_00006-3-0 loss: 0.474168  [   32/   54]
train() client id: f_00006-4-0 loss: 0.505056  [   32/   54]
train() client id: f_00006-5-0 loss: 0.433864  [   32/   54]
train() client id: f_00006-6-0 loss: 0.455088  [   32/   54]
train() client id: f_00006-7-0 loss: 0.466401  [   32/   54]
train() client id: f_00006-8-0 loss: 0.499954  [   32/   54]
train() client id: f_00006-9-0 loss: 0.509595  [   32/   54]
train() client id: f_00006-10-0 loss: 0.477914  [   32/   54]
train() client id: f_00007-0-0 loss: 0.822275  [   32/  179]
train() client id: f_00007-0-1 loss: 0.733076  [   64/  179]
train() client id: f_00007-0-2 loss: 0.681764  [   96/  179]
train() client id: f_00007-0-3 loss: 0.596058  [  128/  179]
train() client id: f_00007-0-4 loss: 0.579242  [  160/  179]
train() client id: f_00007-1-0 loss: 0.642209  [   32/  179]
train() client id: f_00007-1-1 loss: 0.696750  [   64/  179]
train() client id: f_00007-1-2 loss: 0.874891  [   96/  179]
train() client id: f_00007-1-3 loss: 0.745123  [  128/  179]
train() client id: f_00007-1-4 loss: 0.716071  [  160/  179]
train() client id: f_00007-2-0 loss: 0.835448  [   32/  179]
train() client id: f_00007-2-1 loss: 0.542964  [   64/  179]
train() client id: f_00007-2-2 loss: 0.767024  [   96/  179]
train() client id: f_00007-2-3 loss: 0.745185  [  128/  179]
train() client id: f_00007-2-4 loss: 0.736289  [  160/  179]
train() client id: f_00007-3-0 loss: 0.615059  [   32/  179]
train() client id: f_00007-3-1 loss: 0.693563  [   64/  179]
train() client id: f_00007-3-2 loss: 0.799858  [   96/  179]
train() client id: f_00007-3-3 loss: 0.699787  [  128/  179]
train() client id: f_00007-3-4 loss: 0.726806  [  160/  179]
train() client id: f_00007-4-0 loss: 0.643882  [   32/  179]
train() client id: f_00007-4-1 loss: 0.977005  [   64/  179]
train() client id: f_00007-4-2 loss: 0.535759  [   96/  179]
train() client id: f_00007-4-3 loss: 0.601603  [  128/  179]
train() client id: f_00007-4-4 loss: 0.606179  [  160/  179]
train() client id: f_00007-5-0 loss: 0.788854  [   32/  179]
train() client id: f_00007-5-1 loss: 0.578674  [   64/  179]
train() client id: f_00007-5-2 loss: 0.770122  [   96/  179]
train() client id: f_00007-5-3 loss: 0.636530  [  128/  179]
train() client id: f_00007-5-4 loss: 0.662959  [  160/  179]
train() client id: f_00007-6-0 loss: 0.779125  [   32/  179]
train() client id: f_00007-6-1 loss: 0.843742  [   64/  179]
train() client id: f_00007-6-2 loss: 0.792022  [   96/  179]
train() client id: f_00007-6-3 loss: 0.567920  [  128/  179]
train() client id: f_00007-6-4 loss: 0.541080  [  160/  179]
train() client id: f_00007-7-0 loss: 0.974520  [   32/  179]
train() client id: f_00007-7-1 loss: 0.611565  [   64/  179]
train() client id: f_00007-7-2 loss: 0.504601  [   96/  179]
train() client id: f_00007-7-3 loss: 0.643897  [  128/  179]
train() client id: f_00007-7-4 loss: 0.624359  [  160/  179]
train() client id: f_00007-8-0 loss: 0.541065  [   32/  179]
train() client id: f_00007-8-1 loss: 0.617097  [   64/  179]
train() client id: f_00007-8-2 loss: 0.643920  [   96/  179]
train() client id: f_00007-8-3 loss: 0.656987  [  128/  179]
train() client id: f_00007-8-4 loss: 0.914797  [  160/  179]
train() client id: f_00007-9-0 loss: 0.547493  [   32/  179]
train() client id: f_00007-9-1 loss: 0.978337  [   64/  179]
train() client id: f_00007-9-2 loss: 0.538994  [   96/  179]
train() client id: f_00007-9-3 loss: 0.793907  [  128/  179]
train() client id: f_00007-9-4 loss: 0.655076  [  160/  179]
train() client id: f_00007-10-0 loss: 0.541290  [   32/  179]
train() client id: f_00007-10-1 loss: 0.686858  [   64/  179]
train() client id: f_00007-10-2 loss: 0.510241  [   96/  179]
train() client id: f_00007-10-3 loss: 0.838596  [  128/  179]
train() client id: f_00007-10-4 loss: 0.739024  [  160/  179]
train() client id: f_00008-0-0 loss: 0.660595  [   32/  130]
train() client id: f_00008-0-1 loss: 0.775243  [   64/  130]
train() client id: f_00008-0-2 loss: 0.726918  [   96/  130]
train() client id: f_00008-0-3 loss: 0.815614  [  128/  130]
train() client id: f_00008-1-0 loss: 0.746893  [   32/  130]
train() client id: f_00008-1-1 loss: 0.590946  [   64/  130]
train() client id: f_00008-1-2 loss: 0.862322  [   96/  130]
train() client id: f_00008-1-3 loss: 0.752227  [  128/  130]
train() client id: f_00008-2-0 loss: 0.786620  [   32/  130]
train() client id: f_00008-2-1 loss: 0.739430  [   64/  130]
train() client id: f_00008-2-2 loss: 0.816102  [   96/  130]
train() client id: f_00008-2-3 loss: 0.666104  [  128/  130]
train() client id: f_00008-3-0 loss: 0.691340  [   32/  130]
train() client id: f_00008-3-1 loss: 0.766691  [   64/  130]
train() client id: f_00008-3-2 loss: 0.744951  [   96/  130]
train() client id: f_00008-3-3 loss: 0.786839  [  128/  130]
train() client id: f_00008-4-0 loss: 0.730022  [   32/  130]
train() client id: f_00008-4-1 loss: 0.824322  [   64/  130]
train() client id: f_00008-4-2 loss: 0.804674  [   96/  130]
train() client id: f_00008-4-3 loss: 0.640377  [  128/  130]
train() client id: f_00008-5-0 loss: 0.700752  [   32/  130]
train() client id: f_00008-5-1 loss: 0.639963  [   64/  130]
train() client id: f_00008-5-2 loss: 0.773447  [   96/  130]
train() client id: f_00008-5-3 loss: 0.838140  [  128/  130]
train() client id: f_00008-6-0 loss: 0.728691  [   32/  130]
train() client id: f_00008-6-1 loss: 0.666164  [   64/  130]
train() client id: f_00008-6-2 loss: 0.761514  [   96/  130]
train() client id: f_00008-6-3 loss: 0.805693  [  128/  130]
train() client id: f_00008-7-0 loss: 0.792063  [   32/  130]
train() client id: f_00008-7-1 loss: 0.780838  [   64/  130]
train() client id: f_00008-7-2 loss: 0.599877  [   96/  130]
train() client id: f_00008-7-3 loss: 0.790433  [  128/  130]
train() client id: f_00008-8-0 loss: 0.662139  [   32/  130]
train() client id: f_00008-8-1 loss: 0.705690  [   64/  130]
train() client id: f_00008-8-2 loss: 0.888503  [   96/  130]
train() client id: f_00008-8-3 loss: 0.739398  [  128/  130]
train() client id: f_00008-9-0 loss: 0.897362  [   32/  130]
train() client id: f_00008-9-1 loss: 0.645226  [   64/  130]
train() client id: f_00008-9-2 loss: 0.748910  [   96/  130]
train() client id: f_00008-9-3 loss: 0.673240  [  128/  130]
train() client id: f_00008-10-0 loss: 0.747247  [   32/  130]
train() client id: f_00008-10-1 loss: 0.750709  [   64/  130]
train() client id: f_00008-10-2 loss: 0.752791  [   96/  130]
train() client id: f_00008-10-3 loss: 0.739518  [  128/  130]
train() client id: f_00009-0-0 loss: 0.998109  [   32/  118]
train() client id: f_00009-0-1 loss: 0.994355  [   64/  118]
train() client id: f_00009-0-2 loss: 0.818007  [   96/  118]
train() client id: f_00009-1-0 loss: 0.790560  [   32/  118]
train() client id: f_00009-1-1 loss: 0.863164  [   64/  118]
train() client id: f_00009-1-2 loss: 0.990972  [   96/  118]
train() client id: f_00009-2-0 loss: 0.897079  [   32/  118]
train() client id: f_00009-2-1 loss: 0.771210  [   64/  118]
train() client id: f_00009-2-2 loss: 0.753203  [   96/  118]
train() client id: f_00009-3-0 loss: 0.718480  [   32/  118]
train() client id: f_00009-3-1 loss: 0.854742  [   64/  118]
train() client id: f_00009-3-2 loss: 0.934613  [   96/  118]
train() client id: f_00009-4-0 loss: 0.776118  [   32/  118]
train() client id: f_00009-4-1 loss: 0.681788  [   64/  118]
train() client id: f_00009-4-2 loss: 0.817905  [   96/  118]
train() client id: f_00009-5-0 loss: 0.764357  [   32/  118]
train() client id: f_00009-5-1 loss: 0.797719  [   64/  118]
train() client id: f_00009-5-2 loss: 0.708113  [   96/  118]
train() client id: f_00009-6-0 loss: 0.801108  [   32/  118]
train() client id: f_00009-6-1 loss: 0.576484  [   64/  118]
train() client id: f_00009-6-2 loss: 0.631635  [   96/  118]
train() client id: f_00009-7-0 loss: 0.584652  [   32/  118]
train() client id: f_00009-7-1 loss: 0.726818  [   64/  118]
train() client id: f_00009-7-2 loss: 0.697584  [   96/  118]
train() client id: f_00009-8-0 loss: 0.657257  [   32/  118]
train() client id: f_00009-8-1 loss: 0.660258  [   64/  118]
train() client id: f_00009-8-2 loss: 0.782383  [   96/  118]
train() client id: f_00009-9-0 loss: 0.718940  [   32/  118]
train() client id: f_00009-9-1 loss: 0.556885  [   64/  118]
train() client id: f_00009-9-2 loss: 0.626323  [   96/  118]
train() client id: f_00009-10-0 loss: 0.541661  [   32/  118]
train() client id: f_00009-10-1 loss: 0.630110  [   64/  118]
train() client id: f_00009-10-2 loss: 0.745721  [   96/  118]
At round 49 accuracy: 0.6445623342175066
At round 49 training accuracy: 0.5902079141515761
At round 49 training loss: 0.8292032104344502
update_location
xs = [  -3.9056584     4.20031788  265.00902392   18.81129433    0.97929623
    3.95640986 -227.44319194 -206.32485185  249.66397685 -192.06087855]
ys = [ 257.5879595   240.55583871    1.32061395 -227.45517586  219.35018685
  202.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [276.34545599 260.54702878 283.25170216 249.17809258 241.07149041
 226.1619617  248.46990985 229.28284006 269.51951935 216.57191167]
dists_bs = [189.98432772 190.81488913 473.04594942 446.69115324 181.48182216
 181.10500714 185.2050848  176.97203396 452.90618355 171.84751034]
uav_gains = [4.31692826e-12 6.00226608e-12 3.73822966e-12 7.54002475e-12
 8.79545935e-12 1.14187632e-11 7.64459119e-12 1.08372987e-11
 4.98118754e-12 1.33149743e-11]
bs_gains = [4.60133985e-11 4.54548008e-11 3.57737746e-12 4.20023415e-12
 5.23071814e-11 5.26124836e-11 4.94158134e-11 5.61256116e-11
 4.04083377e-12 6.09386741e-11]
Round 50
-------------------------------
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.40113982  9.00691085  4.34261068  1.57832816 10.38540444  4.99669129
  1.94822841  6.14510341  4.53980435  4.05137628]
obj_prev = 51.395597693307295
eta_min = 1.1367158248091635e-21	eta_max = 0.9395757044516898
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 11.874554720679596	eta = 0.909090909090909
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 24.172264619780965	eta = 0.44658826617504477
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 17.836063247717608	eta = 0.6052372430028105
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.691468667850707	eta = 0.6467405571604739
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.62571758399257	eta = 0.6492982748886553
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.62547813342434	eta = 0.6493076264898318
eta = 0.6493076264898318
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [0.03563512 0.07494687 0.03506948 0.01216119 0.08654243 0.04129149
 0.01527219 0.05062449 0.03676639 0.03337257]
ene_total = [1.56827942 2.59415826 1.57537116 0.75074681 2.95309342 1.52632602
 0.84721729 1.92501916 1.61509008 1.2701765 ]
ti_comp = [0.73120545 0.79889761 0.72329946 0.75580078 0.8009931  0.80107742
 0.7563088  0.76793556 0.72766908 0.80314305]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [5.28975756e-06 4.12247795e-05 5.15266296e-06 1.96785757e-07
 6.31407441e-05 6.85665898e-06 3.89211744e-07 1.37503103e-05
 5.86631230e-06 3.60133211e-06]
ene_total = [0.44507619 0.2328146  0.46999487 0.36738071 0.22689962 0.22485949
 0.36578528 0.32955399 0.45624248 0.21824512]
optimize_network_iter = 0 obj = 3.3368523467960083
eta = 0.6493076264898318
freqs = [24367376.98591928 46906432.60757383 24242711.66907599  8045235.73109414
 54021956.04098655 25772468.52031963 10096532.76307652 32961419.31716986
 25263125.77228831 20776230.43734453]
eta_min = 0.6493076264898333	eta_max = 0.6979381775060964
af = 0.0037239897789525765	bf = 1.1686004695034842	zeta = 0.004096388756847835	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [1.16746392e-06 9.09842125e-06 1.13720677e-06 4.34311531e-08
 1.39353344e-05 1.51328333e-06 8.59000931e-08 3.03473097e-06
 1.29471112e-06 7.94823816e-07]
ene_total = [1.71643177 0.89420252 1.81257148 1.41719511 0.86930783 0.86677177
 1.41102229 1.26998969 1.7594524  0.84156459]
ti_comp = [0.61023825 0.67793041 0.60233226 0.63483358 0.6800259  0.68011022
 0.6353416  0.64696837 0.60670188 0.68217585]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.38601091e-06 3.30616333e-05 4.29091311e-06 1.61080327e-07
 5.05906534e-05 5.49359804e-06 3.18510394e-07 1.11879346e-05
 4.87344749e-06 2.88277172e-06]
ene_total = [0.51669831 0.2699979  0.54563033 0.42652599 0.26297008 0.26101093
 0.42467242 0.38251687 0.52965907 0.25335527]
optimize_network_iter = 1 obj = 3.8730371713241745
eta = 0.6979381775060964
freqs = [24314579.32952386 46031634.30360291 24242711.66907599  7976349.99245549
 52989718.17503818 25279541.36753655 10008802.12815238 32581091.06223162
 25232695.58126215 20369543.75424322]
eta_min = 0.6979381775061025	eta_max = 0.697938177506091
af = 0.0036043043465234475	bf = 1.1686004695034842	zeta = 0.003964734781175793	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [1.16241023e-06 8.76221731e-06 1.13720677e-06 4.26905959e-08
 1.34078766e-05 1.45595045e-06 8.44137753e-08 2.96510198e-06
 1.29159396e-06 7.64011626e-07]
ene_total = [1.71643115 0.89416163 1.81257148 1.41719502 0.86924368 0.8667648
 1.41102211 1.26998122 1.75945202 0.84156084]
ti_comp = [0.61023825 0.67793041 0.60233226 0.63483358 0.6800259  0.68011022
 0.6353416  0.64696837 0.60670188 0.68217585]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.38601091e-06 3.30616333e-05 4.29091311e-06 1.61080327e-07
 5.05906534e-05 5.49359804e-06 3.18510394e-07 1.11879346e-05
 4.87344749e-06 2.88277172e-06]
ene_total = [0.51669831 0.2699979  0.54563033 0.42652599 0.26297008 0.26101093
 0.42467242 0.38251687 0.52965907 0.25335527]
optimize_network_iter = 2 obj = 3.873037171324106
eta = 0.697938177506091
freqs = [24314579.32952385 46031634.30360299 24242711.66907597  7976349.99245549
 52989718.17503828 25279541.3675366  10008802.12815239 32581091.06223165
 25232695.58126213 20369543.75424326]
Done!
At round 50 eta: 0.697938177506091
At round 50 local rounds: 11.775941462244061
At round 50 global rounds: 36.59949614457299
At round 50 a_n: 10.712764660819193
gradient difference: 0.538799524307251
train() client id: f_00000-0-0 loss: 1.415308  [   32/  126]
train() client id: f_00000-0-1 loss: 0.969647  [   64/  126]
train() client id: f_00000-0-2 loss: 1.036988  [   96/  126]
train() client id: f_00000-1-0 loss: 1.225439  [   32/  126]
train() client id: f_00000-1-1 loss: 1.019464  [   64/  126]
train() client id: f_00000-1-2 loss: 0.933615  [   96/  126]
train() client id: f_00000-2-0 loss: 1.144010  [   32/  126]
train() client id: f_00000-2-1 loss: 0.899733  [   64/  126]
train() client id: f_00000-2-2 loss: 1.043035  [   96/  126]
train() client id: f_00000-3-0 loss: 1.028292  [   32/  126]
train() client id: f_00000-3-1 loss: 0.911921  [   64/  126]
train() client id: f_00000-3-2 loss: 0.989851  [   96/  126]
train() client id: f_00000-4-0 loss: 1.011053  [   32/  126]
train() client id: f_00000-4-1 loss: 0.969164  [   64/  126]
train() client id: f_00000-4-2 loss: 0.992837  [   96/  126]
train() client id: f_00000-5-0 loss: 0.909962  [   32/  126]
train() client id: f_00000-5-1 loss: 0.935997  [   64/  126]
train() client id: f_00000-5-2 loss: 0.933705  [   96/  126]
train() client id: f_00000-6-0 loss: 0.890181  [   32/  126]
train() client id: f_00000-6-1 loss: 0.959948  [   64/  126]
train() client id: f_00000-6-2 loss: 1.083388  [   96/  126]
train() client id: f_00000-7-0 loss: 0.898484  [   32/  126]
train() client id: f_00000-7-1 loss: 0.928335  [   64/  126]
train() client id: f_00000-7-2 loss: 1.051770  [   96/  126]
train() client id: f_00000-8-0 loss: 0.868815  [   32/  126]
train() client id: f_00000-8-1 loss: 0.956934  [   64/  126]
train() client id: f_00000-8-2 loss: 1.021999  [   96/  126]
train() client id: f_00000-9-0 loss: 0.892579  [   32/  126]
train() client id: f_00000-9-1 loss: 0.964699  [   64/  126]
train() client id: f_00000-9-2 loss: 0.882757  [   96/  126]
train() client id: f_00000-10-0 loss: 0.992546  [   32/  126]
train() client id: f_00000-10-1 loss: 0.896933  [   64/  126]
train() client id: f_00000-10-2 loss: 0.990986  [   96/  126]
train() client id: f_00001-0-0 loss: 0.529421  [   32/  265]
train() client id: f_00001-0-1 loss: 0.502864  [   64/  265]
train() client id: f_00001-0-2 loss: 0.490629  [   96/  265]
train() client id: f_00001-0-3 loss: 0.514737  [  128/  265]
train() client id: f_00001-0-4 loss: 0.424248  [  160/  265]
train() client id: f_00001-0-5 loss: 0.391039  [  192/  265]
train() client id: f_00001-0-6 loss: 0.553305  [  224/  265]
train() client id: f_00001-0-7 loss: 0.462491  [  256/  265]
train() client id: f_00001-1-0 loss: 0.435175  [   32/  265]
train() client id: f_00001-1-1 loss: 0.393487  [   64/  265]
train() client id: f_00001-1-2 loss: 0.552955  [   96/  265]
train() client id: f_00001-1-3 loss: 0.443176  [  128/  265]
train() client id: f_00001-1-4 loss: 0.531071  [  160/  265]
train() client id: f_00001-1-5 loss: 0.441663  [  192/  265]
train() client id: f_00001-1-6 loss: 0.471185  [  224/  265]
train() client id: f_00001-1-7 loss: 0.570735  [  256/  265]
train() client id: f_00001-2-0 loss: 0.506160  [   32/  265]
train() client id: f_00001-2-1 loss: 0.391869  [   64/  265]
train() client id: f_00001-2-2 loss: 0.387332  [   96/  265]
train() client id: f_00001-2-3 loss: 0.488838  [  128/  265]
train() client id: f_00001-2-4 loss: 0.489620  [  160/  265]
train() client id: f_00001-2-5 loss: 0.494134  [  192/  265]
train() client id: f_00001-2-6 loss: 0.587085  [  224/  265]
train() client id: f_00001-2-7 loss: 0.419315  [  256/  265]
train() client id: f_00001-3-0 loss: 0.386883  [   32/  265]
train() client id: f_00001-3-1 loss: 0.481838  [   64/  265]
train() client id: f_00001-3-2 loss: 0.669458  [   96/  265]
train() client id: f_00001-3-3 loss: 0.373573  [  128/  265]
train() client id: f_00001-3-4 loss: 0.462799  [  160/  265]
train() client id: f_00001-3-5 loss: 0.504238  [  192/  265]
train() client id: f_00001-3-6 loss: 0.437020  [  224/  265]
train() client id: f_00001-3-7 loss: 0.411144  [  256/  265]
train() client id: f_00001-4-0 loss: 0.382080  [   32/  265]
train() client id: f_00001-4-1 loss: 0.417996  [   64/  265]
train() client id: f_00001-4-2 loss: 0.483466  [   96/  265]
train() client id: f_00001-4-3 loss: 0.406293  [  128/  265]
train() client id: f_00001-4-4 loss: 0.466705  [  160/  265]
train() client id: f_00001-4-5 loss: 0.585498  [  192/  265]
train() client id: f_00001-4-6 loss: 0.478783  [  224/  265]
train() client id: f_00001-4-7 loss: 0.460575  [  256/  265]
train() client id: f_00001-5-0 loss: 0.455773  [   32/  265]
train() client id: f_00001-5-1 loss: 0.414069  [   64/  265]
train() client id: f_00001-5-2 loss: 0.479659  [   96/  265]
train() client id: f_00001-5-3 loss: 0.431997  [  128/  265]
train() client id: f_00001-5-4 loss: 0.375854  [  160/  265]
train() client id: f_00001-5-5 loss: 0.467970  [  192/  265]
train() client id: f_00001-5-6 loss: 0.512793  [  224/  265]
train() client id: f_00001-5-7 loss: 0.605801  [  256/  265]
train() client id: f_00001-6-0 loss: 0.564455  [   32/  265]
train() client id: f_00001-6-1 loss: 0.455736  [   64/  265]
train() client id: f_00001-6-2 loss: 0.363180  [   96/  265]
train() client id: f_00001-6-3 loss: 0.558023  [  128/  265]
train() client id: f_00001-6-4 loss: 0.468468  [  160/  265]
train() client id: f_00001-6-5 loss: 0.364595  [  192/  265]
train() client id: f_00001-6-6 loss: 0.395146  [  224/  265]
train() client id: f_00001-6-7 loss: 0.438797  [  256/  265]
train() client id: f_00001-7-0 loss: 0.503370  [   32/  265]
train() client id: f_00001-7-1 loss: 0.605754  [   64/  265]
train() client id: f_00001-7-2 loss: 0.437237  [   96/  265]
train() client id: f_00001-7-3 loss: 0.438051  [  128/  265]
train() client id: f_00001-7-4 loss: 0.414330  [  160/  265]
train() client id: f_00001-7-5 loss: 0.390471  [  192/  265]
train() client id: f_00001-7-6 loss: 0.404718  [  224/  265]
train() client id: f_00001-7-7 loss: 0.503318  [  256/  265]
train() client id: f_00001-8-0 loss: 0.513460  [   32/  265]
train() client id: f_00001-8-1 loss: 0.517319  [   64/  265]
train() client id: f_00001-8-2 loss: 0.395311  [   96/  265]
train() client id: f_00001-8-3 loss: 0.497418  [  128/  265]
train() client id: f_00001-8-4 loss: 0.405183  [  160/  265]
train() client id: f_00001-8-5 loss: 0.465676  [  192/  265]
train() client id: f_00001-8-6 loss: 0.460567  [  224/  265]
train() client id: f_00001-8-7 loss: 0.482967  [  256/  265]
train() client id: f_00001-9-0 loss: 0.392607  [   32/  265]
train() client id: f_00001-9-1 loss: 0.483068  [   64/  265]
train() client id: f_00001-9-2 loss: 0.373610  [   96/  265]
train() client id: f_00001-9-3 loss: 0.552595  [  128/  265]
train() client id: f_00001-9-4 loss: 0.526331  [  160/  265]
train() client id: f_00001-9-5 loss: 0.440115  [  192/  265]
train() client id: f_00001-9-6 loss: 0.482452  [  224/  265]
train() client id: f_00001-9-7 loss: 0.474522  [  256/  265]
train() client id: f_00001-10-0 loss: 0.471543  [   32/  265]
train() client id: f_00001-10-1 loss: 0.377122  [   64/  265]
train() client id: f_00001-10-2 loss: 0.573750  [   96/  265]
train() client id: f_00001-10-3 loss: 0.519963  [  128/  265]
train() client id: f_00001-10-4 loss: 0.544197  [  160/  265]
train() client id: f_00001-10-5 loss: 0.374834  [  192/  265]
train() client id: f_00001-10-6 loss: 0.374059  [  224/  265]
train() client id: f_00001-10-7 loss: 0.509887  [  256/  265]
train() client id: f_00002-0-0 loss: 1.062582  [   32/  124]
train() client id: f_00002-0-1 loss: 0.941863  [   64/  124]
train() client id: f_00002-0-2 loss: 1.389059  [   96/  124]
train() client id: f_00002-1-0 loss: 1.053566  [   32/  124]
train() client id: f_00002-1-1 loss: 1.076635  [   64/  124]
train() client id: f_00002-1-2 loss: 1.071630  [   96/  124]
train() client id: f_00002-2-0 loss: 0.959779  [   32/  124]
train() client id: f_00002-2-1 loss: 1.079782  [   64/  124]
train() client id: f_00002-2-2 loss: 1.196499  [   96/  124]
train() client id: f_00002-3-0 loss: 0.981188  [   32/  124]
train() client id: f_00002-3-1 loss: 0.909099  [   64/  124]
train() client id: f_00002-3-2 loss: 1.218490  [   96/  124]
train() client id: f_00002-4-0 loss: 1.013206  [   32/  124]
train() client id: f_00002-4-1 loss: 1.019339  [   64/  124]
train() client id: f_00002-4-2 loss: 1.118301  [   96/  124]
train() client id: f_00002-5-0 loss: 0.903636  [   32/  124]
train() client id: f_00002-5-1 loss: 1.120587  [   64/  124]
train() client id: f_00002-5-2 loss: 0.984926  [   96/  124]
train() client id: f_00002-6-0 loss: 1.145311  [   32/  124]
train() client id: f_00002-6-1 loss: 0.863509  [   64/  124]
train() client id: f_00002-6-2 loss: 0.886307  [   96/  124]
train() client id: f_00002-7-0 loss: 0.915247  [   32/  124]
train() client id: f_00002-7-1 loss: 0.879229  [   64/  124]
train() client id: f_00002-7-2 loss: 0.888394  [   96/  124]
train() client id: f_00002-8-0 loss: 0.988577  [   32/  124]
train() client id: f_00002-8-1 loss: 1.185565  [   64/  124]
train() client id: f_00002-8-2 loss: 0.983388  [   96/  124]
train() client id: f_00002-9-0 loss: 0.821726  [   32/  124]
train() client id: f_00002-9-1 loss: 1.033599  [   64/  124]
train() client id: f_00002-9-2 loss: 0.920620  [   96/  124]
train() client id: f_00002-10-0 loss: 1.009109  [   32/  124]
train() client id: f_00002-10-1 loss: 0.949184  [   64/  124]
train() client id: f_00002-10-2 loss: 0.890718  [   96/  124]
train() client id: f_00003-0-0 loss: 0.432464  [   32/   43]
train() client id: f_00003-1-0 loss: 0.506111  [   32/   43]
train() client id: f_00003-2-0 loss: 0.465648  [   32/   43]
train() client id: f_00003-3-0 loss: 0.640797  [   32/   43]
train() client id: f_00003-4-0 loss: 0.725731  [   32/   43]
train() client id: f_00003-5-0 loss: 0.479509  [   32/   43]
train() client id: f_00003-6-0 loss: 0.497164  [   32/   43]
train() client id: f_00003-7-0 loss: 0.540364  [   32/   43]
train() client id: f_00003-8-0 loss: 0.312169  [   32/   43]
train() client id: f_00003-9-0 loss: 0.534355  [   32/   43]
train() client id: f_00003-10-0 loss: 0.588103  [   32/   43]
train() client id: f_00004-0-0 loss: 0.827266  [   32/  306]
train() client id: f_00004-0-1 loss: 0.878990  [   64/  306]
train() client id: f_00004-0-2 loss: 0.819420  [   96/  306]
train() client id: f_00004-0-3 loss: 0.884032  [  128/  306]
train() client id: f_00004-0-4 loss: 0.915049  [  160/  306]
train() client id: f_00004-0-5 loss: 0.925266  [  192/  306]
train() client id: f_00004-0-6 loss: 0.869679  [  224/  306]
train() client id: f_00004-0-7 loss: 0.777233  [  256/  306]
train() client id: f_00004-0-8 loss: 0.841213  [  288/  306]
train() client id: f_00004-1-0 loss: 0.865808  [   32/  306]
train() client id: f_00004-1-1 loss: 0.974935  [   64/  306]
train() client id: f_00004-1-2 loss: 0.863413  [   96/  306]
train() client id: f_00004-1-3 loss: 0.904761  [  128/  306]
train() client id: f_00004-1-4 loss: 0.965464  [  160/  306]
train() client id: f_00004-1-5 loss: 0.780748  [  192/  306]
train() client id: f_00004-1-6 loss: 0.824490  [  224/  306]
train() client id: f_00004-1-7 loss: 0.765483  [  256/  306]
train() client id: f_00004-1-8 loss: 0.859858  [  288/  306]
train() client id: f_00004-2-0 loss: 0.814301  [   32/  306]
train() client id: f_00004-2-1 loss: 0.867016  [   64/  306]
train() client id: f_00004-2-2 loss: 0.939600  [   96/  306]
train() client id: f_00004-2-3 loss: 0.806923  [  128/  306]
train() client id: f_00004-2-4 loss: 0.960272  [  160/  306]
train() client id: f_00004-2-5 loss: 0.839136  [  192/  306]
train() client id: f_00004-2-6 loss: 0.805082  [  224/  306]
train() client id: f_00004-2-7 loss: 0.691562  [  256/  306]
train() client id: f_00004-2-8 loss: 0.940520  [  288/  306]
train() client id: f_00004-3-0 loss: 0.915385  [   32/  306]
train() client id: f_00004-3-1 loss: 0.946012  [   64/  306]
train() client id: f_00004-3-2 loss: 0.750641  [   96/  306]
train() client id: f_00004-3-3 loss: 0.745575  [  128/  306]
train() client id: f_00004-3-4 loss: 0.905377  [  160/  306]
train() client id: f_00004-3-5 loss: 0.864364  [  192/  306]
train() client id: f_00004-3-6 loss: 0.789369  [  224/  306]
train() client id: f_00004-3-7 loss: 0.894228  [  256/  306]
train() client id: f_00004-3-8 loss: 0.961091  [  288/  306]
train() client id: f_00004-4-0 loss: 0.878182  [   32/  306]
train() client id: f_00004-4-1 loss: 0.920959  [   64/  306]
train() client id: f_00004-4-2 loss: 0.934084  [   96/  306]
train() client id: f_00004-4-3 loss: 0.863922  [  128/  306]
train() client id: f_00004-4-4 loss: 0.837149  [  160/  306]
train() client id: f_00004-4-5 loss: 0.778604  [  192/  306]
train() client id: f_00004-4-6 loss: 0.865459  [  224/  306]
train() client id: f_00004-4-7 loss: 0.788557  [  256/  306]
train() client id: f_00004-4-8 loss: 0.952584  [  288/  306]
train() client id: f_00004-5-0 loss: 0.852782  [   32/  306]
train() client id: f_00004-5-1 loss: 0.868083  [   64/  306]
train() client id: f_00004-5-2 loss: 0.912085  [   96/  306]
train() client id: f_00004-5-3 loss: 0.949189  [  128/  306]
train() client id: f_00004-5-4 loss: 0.879998  [  160/  306]
train() client id: f_00004-5-5 loss: 0.887537  [  192/  306]
train() client id: f_00004-5-6 loss: 0.839623  [  224/  306]
train() client id: f_00004-5-7 loss: 0.691305  [  256/  306]
train() client id: f_00004-5-8 loss: 0.931873  [  288/  306]
train() client id: f_00004-6-0 loss: 0.807978  [   32/  306]
train() client id: f_00004-6-1 loss: 0.882722  [   64/  306]
train() client id: f_00004-6-2 loss: 0.792990  [   96/  306]
train() client id: f_00004-6-3 loss: 1.034726  [  128/  306]
train() client id: f_00004-6-4 loss: 0.843200  [  160/  306]
train() client id: f_00004-6-5 loss: 0.845700  [  192/  306]
train() client id: f_00004-6-6 loss: 0.904712  [  224/  306]
train() client id: f_00004-6-7 loss: 0.914269  [  256/  306]
train() client id: f_00004-6-8 loss: 0.798985  [  288/  306]
train() client id: f_00004-7-0 loss: 0.881415  [   32/  306]
train() client id: f_00004-7-1 loss: 0.884593  [   64/  306]
train() client id: f_00004-7-2 loss: 0.889129  [   96/  306]
train() client id: f_00004-7-3 loss: 0.917947  [  128/  306]
train() client id: f_00004-7-4 loss: 0.945918  [  160/  306]
train() client id: f_00004-7-5 loss: 0.994303  [  192/  306]
train() client id: f_00004-7-6 loss: 0.679266  [  224/  306]
train() client id: f_00004-7-7 loss: 0.834488  [  256/  306]
train() client id: f_00004-7-8 loss: 0.790537  [  288/  306]
train() client id: f_00004-8-0 loss: 0.877078  [   32/  306]
train() client id: f_00004-8-1 loss: 0.845783  [   64/  306]
train() client id: f_00004-8-2 loss: 0.742068  [   96/  306]
train() client id: f_00004-8-3 loss: 0.848810  [  128/  306]
train() client id: f_00004-8-4 loss: 0.862084  [  160/  306]
train() client id: f_00004-8-5 loss: 0.884773  [  192/  306]
train() client id: f_00004-8-6 loss: 0.821190  [  224/  306]
train() client id: f_00004-8-7 loss: 0.971376  [  256/  306]
train() client id: f_00004-8-8 loss: 0.916838  [  288/  306]
train() client id: f_00004-9-0 loss: 0.870151  [   32/  306]
train() client id: f_00004-9-1 loss: 0.873247  [   64/  306]
train() client id: f_00004-9-2 loss: 0.697307  [   96/  306]
train() client id: f_00004-9-3 loss: 0.896577  [  128/  306]
train() client id: f_00004-9-4 loss: 0.820189  [  160/  306]
train() client id: f_00004-9-5 loss: 0.906246  [  192/  306]
train() client id: f_00004-9-6 loss: 0.926620  [  224/  306]
train() client id: f_00004-9-7 loss: 0.847146  [  256/  306]
train() client id: f_00004-9-8 loss: 0.907399  [  288/  306]
train() client id: f_00004-10-0 loss: 0.849995  [   32/  306]
train() client id: f_00004-10-1 loss: 0.851594  [   64/  306]
train() client id: f_00004-10-2 loss: 0.841838  [   96/  306]
train() client id: f_00004-10-3 loss: 0.854198  [  128/  306]
train() client id: f_00004-10-4 loss: 0.881283  [  160/  306]
train() client id: f_00004-10-5 loss: 0.897565  [  192/  306]
train() client id: f_00004-10-6 loss: 0.863250  [  224/  306]
train() client id: f_00004-10-7 loss: 0.884210  [  256/  306]
train() client id: f_00004-10-8 loss: 0.911784  [  288/  306]
train() client id: f_00005-0-0 loss: 0.575064  [   32/  146]
train() client id: f_00005-0-1 loss: 0.709570  [   64/  146]
train() client id: f_00005-0-2 loss: 0.398979  [   96/  146]
train() client id: f_00005-0-3 loss: 0.511925  [  128/  146]
train() client id: f_00005-1-0 loss: 0.671016  [   32/  146]
train() client id: f_00005-1-1 loss: 0.444380  [   64/  146]
train() client id: f_00005-1-2 loss: 0.647002  [   96/  146]
train() client id: f_00005-1-3 loss: 0.593897  [  128/  146]
train() client id: f_00005-2-0 loss: 0.676506  [   32/  146]
train() client id: f_00005-2-1 loss: 0.550212  [   64/  146]
train() client id: f_00005-2-2 loss: 0.453626  [   96/  146]
train() client id: f_00005-2-3 loss: 0.500706  [  128/  146]
train() client id: f_00005-3-0 loss: 0.450965  [   32/  146]
train() client id: f_00005-3-1 loss: 0.507955  [   64/  146]
train() client id: f_00005-3-2 loss: 0.608535  [   96/  146]
train() client id: f_00005-3-3 loss: 0.722333  [  128/  146]
train() client id: f_00005-4-0 loss: 0.569013  [   32/  146]
train() client id: f_00005-4-1 loss: 0.654158  [   64/  146]
train() client id: f_00005-4-2 loss: 0.501942  [   96/  146]
train() client id: f_00005-4-3 loss: 0.505118  [  128/  146]
train() client id: f_00005-5-0 loss: 0.616416  [   32/  146]
train() client id: f_00005-5-1 loss: 0.498035  [   64/  146]
train() client id: f_00005-5-2 loss: 0.522128  [   96/  146]
train() client id: f_00005-5-3 loss: 0.613246  [  128/  146]
train() client id: f_00005-6-0 loss: 0.828867  [   32/  146]
train() client id: f_00005-6-1 loss: 0.542113  [   64/  146]
train() client id: f_00005-6-2 loss: 0.452077  [   96/  146]
train() client id: f_00005-6-3 loss: 0.446290  [  128/  146]
train() client id: f_00005-7-0 loss: 0.535713  [   32/  146]
train() client id: f_00005-7-1 loss: 0.687282  [   64/  146]
train() client id: f_00005-7-2 loss: 0.688772  [   96/  146]
train() client id: f_00005-7-3 loss: 0.414733  [  128/  146]
train() client id: f_00005-8-0 loss: 0.567241  [   32/  146]
train() client id: f_00005-8-1 loss: 0.827979  [   64/  146]
train() client id: f_00005-8-2 loss: 0.474392  [   96/  146]
train() client id: f_00005-8-3 loss: 0.457038  [  128/  146]
train() client id: f_00005-9-0 loss: 0.635070  [   32/  146]
train() client id: f_00005-9-1 loss: 0.524307  [   64/  146]
train() client id: f_00005-9-2 loss: 0.577062  [   96/  146]
train() client id: f_00005-9-3 loss: 0.544204  [  128/  146]
train() client id: f_00005-10-0 loss: 0.644036  [   32/  146]
train() client id: f_00005-10-1 loss: 0.464340  [   64/  146]
train() client id: f_00005-10-2 loss: 0.470974  [   96/  146]
train() client id: f_00005-10-3 loss: 0.752097  [  128/  146]
train() client id: f_00006-0-0 loss: 0.496287  [   32/   54]
train() client id: f_00006-1-0 loss: 0.487568  [   32/   54]
train() client id: f_00006-2-0 loss: 0.550489  [   32/   54]
train() client id: f_00006-3-0 loss: 0.550156  [   32/   54]
train() client id: f_00006-4-0 loss: 0.538115  [   32/   54]
train() client id: f_00006-5-0 loss: 0.450022  [   32/   54]
train() client id: f_00006-6-0 loss: 0.535200  [   32/   54]
train() client id: f_00006-7-0 loss: 0.443614  [   32/   54]
train() client id: f_00006-8-0 loss: 0.476872  [   32/   54]
train() client id: f_00006-9-0 loss: 0.502602  [   32/   54]
train() client id: f_00006-10-0 loss: 0.455046  [   32/   54]
train() client id: f_00007-0-0 loss: 0.606210  [   32/  179]
train() client id: f_00007-0-1 loss: 0.437192  [   64/  179]
train() client id: f_00007-0-2 loss: 0.597537  [   96/  179]
train() client id: f_00007-0-3 loss: 0.639453  [  128/  179]
train() client id: f_00007-0-4 loss: 0.440064  [  160/  179]
train() client id: f_00007-1-0 loss: 0.474856  [   32/  179]
train() client id: f_00007-1-1 loss: 0.442649  [   64/  179]
train() client id: f_00007-1-2 loss: 0.493631  [   96/  179]
train() client id: f_00007-1-3 loss: 0.619894  [  128/  179]
train() client id: f_00007-1-4 loss: 0.612833  [  160/  179]
train() client id: f_00007-2-0 loss: 0.714864  [   32/  179]
train() client id: f_00007-2-1 loss: 0.481625  [   64/  179]
train() client id: f_00007-2-2 loss: 0.445192  [   96/  179]
train() client id: f_00007-2-3 loss: 0.649713  [  128/  179]
train() client id: f_00007-2-4 loss: 0.434404  [  160/  179]
train() client id: f_00007-3-0 loss: 0.394791  [   32/  179]
train() client id: f_00007-3-1 loss: 0.462936  [   64/  179]
train() client id: f_00007-3-2 loss: 0.418981  [   96/  179]
train() client id: f_00007-3-3 loss: 0.639926  [  128/  179]
train() client id: f_00007-3-4 loss: 0.592894  [  160/  179]
train() client id: f_00007-4-0 loss: 0.343637  [   32/  179]
train() client id: f_00007-4-1 loss: 0.684938  [   64/  179]
train() client id: f_00007-4-2 loss: 0.492500  [   96/  179]
train() client id: f_00007-4-3 loss: 0.603415  [  128/  179]
train() client id: f_00007-4-4 loss: 0.535150  [  160/  179]
train() client id: f_00007-5-0 loss: 0.314858  [   32/  179]
train() client id: f_00007-5-1 loss: 0.806828  [   64/  179]
train() client id: f_00007-5-2 loss: 0.503393  [   96/  179]
train() client id: f_00007-5-3 loss: 0.475611  [  128/  179]
train() client id: f_00007-5-4 loss: 0.402627  [  160/  179]
train() client id: f_00007-6-0 loss: 0.470036  [   32/  179]
train() client id: f_00007-6-1 loss: 0.453137  [   64/  179]
train() client id: f_00007-6-2 loss: 0.501361  [   96/  179]
train() client id: f_00007-6-3 loss: 0.578656  [  128/  179]
train() client id: f_00007-6-4 loss: 0.570094  [  160/  179]
train() client id: f_00007-7-0 loss: 0.783133  [   32/  179]
train() client id: f_00007-7-1 loss: 0.474828  [   64/  179]
train() client id: f_00007-7-2 loss: 0.367929  [   96/  179]
train() client id: f_00007-7-3 loss: 0.410471  [  128/  179]
train() client id: f_00007-7-4 loss: 0.340681  [  160/  179]
train() client id: f_00007-8-0 loss: 0.628917  [   32/  179]
train() client id: f_00007-8-1 loss: 0.369434  [   64/  179]
train() client id: f_00007-8-2 loss: 0.532869  [   96/  179]
train() client id: f_00007-8-3 loss: 0.435146  [  128/  179]
train() client id: f_00007-8-4 loss: 0.528435  [  160/  179]
train() client id: f_00007-9-0 loss: 0.360163  [   32/  179]
train() client id: f_00007-9-1 loss: 0.559814  [   64/  179]
train() client id: f_00007-9-2 loss: 0.595964  [   96/  179]
train() client id: f_00007-9-3 loss: 0.589020  [  128/  179]
train() client id: f_00007-9-4 loss: 0.376024  [  160/  179]
train() client id: f_00007-10-0 loss: 0.440630  [   32/  179]
train() client id: f_00007-10-1 loss: 0.514849  [   64/  179]
train() client id: f_00007-10-2 loss: 0.537039  [   96/  179]
train() client id: f_00007-10-3 loss: 0.434234  [  128/  179]
train() client id: f_00007-10-4 loss: 0.517092  [  160/  179]
train() client id: f_00008-0-0 loss: 0.796775  [   32/  130]
train() client id: f_00008-0-1 loss: 0.754844  [   64/  130]
train() client id: f_00008-0-2 loss: 0.657817  [   96/  130]
train() client id: f_00008-0-3 loss: 0.835897  [  128/  130]
train() client id: f_00008-1-0 loss: 0.816875  [   32/  130]
train() client id: f_00008-1-1 loss: 0.727437  [   64/  130]
train() client id: f_00008-1-2 loss: 0.673741  [   96/  130]
train() client id: f_00008-1-3 loss: 0.791933  [  128/  130]
train() client id: f_00008-2-0 loss: 0.741956  [   32/  130]
train() client id: f_00008-2-1 loss: 0.815159  [   64/  130]
train() client id: f_00008-2-2 loss: 0.664123  [   96/  130]
train() client id: f_00008-2-3 loss: 0.829266  [  128/  130]
train() client id: f_00008-3-0 loss: 0.747155  [   32/  130]
train() client id: f_00008-3-1 loss: 0.807002  [   64/  130]
train() client id: f_00008-3-2 loss: 0.775675  [   96/  130]
train() client id: f_00008-3-3 loss: 0.714516  [  128/  130]
train() client id: f_00008-4-0 loss: 0.735174  [   32/  130]
train() client id: f_00008-4-1 loss: 0.726898  [   64/  130]
train() client id: f_00008-4-2 loss: 0.828289  [   96/  130]
train() client id: f_00008-4-3 loss: 0.728185  [  128/  130]
train() client id: f_00008-5-0 loss: 0.739262  [   32/  130]
train() client id: f_00008-5-1 loss: 0.678622  [   64/  130]
train() client id: f_00008-5-2 loss: 0.823852  [   96/  130]
train() client id: f_00008-5-3 loss: 0.799242  [  128/  130]
train() client id: f_00008-6-0 loss: 0.780634  [   32/  130]
train() client id: f_00008-6-1 loss: 0.821869  [   64/  130]
train() client id: f_00008-6-2 loss: 0.676150  [   96/  130]
train() client id: f_00008-6-3 loss: 0.721544  [  128/  130]
train() client id: f_00008-7-0 loss: 0.682729  [   32/  130]
train() client id: f_00008-7-1 loss: 0.835140  [   64/  130]
train() client id: f_00008-7-2 loss: 0.785809  [   96/  130]
train() client id: f_00008-7-3 loss: 0.734723  [  128/  130]
train() client id: f_00008-8-0 loss: 0.736871  [   32/  130]
train() client id: f_00008-8-1 loss: 0.799670  [   64/  130]
train() client id: f_00008-8-2 loss: 0.773908  [   96/  130]
train() client id: f_00008-8-3 loss: 0.747131  [  128/  130]
train() client id: f_00008-9-0 loss: 0.855985  [   32/  130]
train() client id: f_00008-9-1 loss: 0.784935  [   64/  130]
train() client id: f_00008-9-2 loss: 0.704978  [   96/  130]
train() client id: f_00008-9-3 loss: 0.690253  [  128/  130]
train() client id: f_00008-10-0 loss: 0.793387  [   32/  130]
train() client id: f_00008-10-1 loss: 0.785606  [   64/  130]
train() client id: f_00008-10-2 loss: 0.803220  [   96/  130]
train() client id: f_00008-10-3 loss: 0.667888  [  128/  130]
train() client id: f_00009-0-0 loss: 0.998692  [   32/  118]
train() client id: f_00009-0-1 loss: 0.940123  [   64/  118]
train() client id: f_00009-0-2 loss: 0.860868  [   96/  118]
train() client id: f_00009-1-0 loss: 0.708452  [   32/  118]
train() client id: f_00009-1-1 loss: 1.065005  [   64/  118]
train() client id: f_00009-1-2 loss: 0.847092  [   96/  118]
train() client id: f_00009-2-0 loss: 0.694198  [   32/  118]
train() client id: f_00009-2-1 loss: 0.764995  [   64/  118]
train() client id: f_00009-2-2 loss: 0.912370  [   96/  118]
train() client id: f_00009-3-0 loss: 0.833910  [   32/  118]
train() client id: f_00009-3-1 loss: 0.688240  [   64/  118]
train() client id: f_00009-3-2 loss: 0.830928  [   96/  118]
train() client id: f_00009-4-0 loss: 0.739735  [   32/  118]
train() client id: f_00009-4-1 loss: 0.743017  [   64/  118]
train() client id: f_00009-4-2 loss: 0.779388  [   96/  118]
train() client id: f_00009-5-0 loss: 0.792933  [   32/  118]
train() client id: f_00009-5-1 loss: 0.735309  [   64/  118]
train() client id: f_00009-5-2 loss: 0.751781  [   96/  118]
train() client id: f_00009-6-0 loss: 0.734928  [   32/  118]
train() client id: f_00009-6-1 loss: 0.850865  [   64/  118]
train() client id: f_00009-6-2 loss: 0.672046  [   96/  118]
train() client id: f_00009-7-0 loss: 0.762064  [   32/  118]
train() client id: f_00009-7-1 loss: 0.726388  [   64/  118]
train() client id: f_00009-7-2 loss: 0.786701  [   96/  118]
train() client id: f_00009-8-0 loss: 0.857399  [   32/  118]
train() client id: f_00009-8-1 loss: 0.618654  [   64/  118]
train() client id: f_00009-8-2 loss: 0.719096  [   96/  118]
train() client id: f_00009-9-0 loss: 0.681024  [   32/  118]
train() client id: f_00009-9-1 loss: 0.774823  [   64/  118]
train() client id: f_00009-9-2 loss: 0.712904  [   96/  118]
train() client id: f_00009-10-0 loss: 0.655697  [   32/  118]
train() client id: f_00009-10-1 loss: 0.746558  [   64/  118]
train() client id: f_00009-10-2 loss: 0.684841  [   96/  118]
At round 50 accuracy: 0.6472148541114059
At round 50 training accuracy: 0.5875251509054326
At round 50 training loss: 0.8293167269041002
update_location
xs = [  -3.9056584     4.20031788  270.00902392   18.81129433    0.97929623
    3.95640986 -232.44319194 -211.32485185  254.66397685 -197.06087855]
ys = [ 262.5879595   245.55583871    1.32061395 -232.45517586  224.35018685
  207.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [281.0119048  265.17034637 287.93509167 253.75041592 245.62973224
 230.65639906 253.05479253 233.79236357 274.1576391  221.01810268]
dists_bs = [192.21062503 192.58992783 477.70028313 451.20121691 182.76803233
 181.94000418 186.68249879 177.9251228  457.59769543 172.41570577]
uav_gains = [3.91606458e-12 5.45480845e-12 3.39525697e-12 6.88931259e-12
 8.07391748e-12 1.05867714e-11 6.98555204e-12 1.00271385e-11
 4.51953049e-12 1.24147554e-11]
bs_gains = [4.45366340e-11 4.42914694e-11 3.48063665e-12 4.08373311e-12
 5.12830001e-11 5.19391842e-11 4.83285750e-11 5.52878520e-11
 3.92590119e-12 6.03780354e-11]
Round 51
-------------------------------
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.27047392  8.72826714  4.21425716  1.53313884 10.06392708  4.84207237
  1.89154359  5.95729153  4.4008638   3.92598233]
obj_prev = 49.827817776908454
eta_min = 2.5652490042097954e-22	eta_max = 0.9391007800776031
af = 10.460568009377656	bf = 1.156843673852587	zeta = 11.506624810315422	eta = 0.9090909090909091
af = 10.460568009377656	bf = 1.156843673852587	zeta = 23.692265208499137	eta = 0.44151827262279386
af = 10.460568009377656	bf = 1.156843673852587	zeta = 17.38415562977596	eta = 0.6017300024316722
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.245893349082294	eta = 0.6438899840474798
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.180034455220028	eta = 0.6465108611683364
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.179790636766835	eta = 0.6465206036478086
eta = 0.6465206036478086
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [0.03599007 0.0756934  0.0354188  0.01228233 0.08740446 0.04170278
 0.01542432 0.05112875 0.03713261 0.03370499]
ene_total = [1.53443253 2.51779645 1.54247681 0.73582445 2.86602817 1.48040303
 0.82941199 1.87320864 1.56863488 1.23157367]
ti_comp = [0.75933363 0.83189525 0.75096662 0.78576566 0.83410293 0.83428835
 0.78630533 0.79893754 0.75947361 0.83641438]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [5.05315743e-06 3.91666828e-05 4.92425175e-06 1.87558063e-07
 5.99848321e-05 6.51242403e-06 3.70949938e-07 1.30872950e-05
 5.54780444e-06 3.42073551e-06]
ene_total = [0.44384841 0.22497254 0.46920207 0.3635944  0.21891275 0.21673023
 0.36196441 0.32406582 0.44343917 0.21019324]
optimize_network_iter = 0 obj = 3.2769230299910435
eta = 0.6465206036478086
freqs = [23698456.65782105 45494550.69374827 23582139.4675787   7815514.32290755
 52394286.76461308 24993025.32151997  9808095.66767819 31997964.51374336
 24446282.03194968 20148497.51725332]
eta_min = 0.6465206036478099	eta_max = 0.6980699554597802
af = 0.0033958499648260683	bf = 1.156843673852587	zeta = 0.003735434961308675	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [1.10424650e-06 8.55894019e-06 1.07607725e-06 4.09863212e-08
 1.31082480e-05 1.42313425e-06 8.10622219e-08 2.85991478e-06
 1.21233975e-06 7.47519793e-07]
ene_total = [1.7253386  0.8711479  1.82393243 1.41373717 0.84566865 0.84210666
 1.4073824  1.2588513  1.7237018  0.81697375]
ti_comp = [0.62724647 0.69980809 0.61887946 0.6536785  0.70201577 0.70220119
 0.65421817 0.66685038 0.62738645 0.70432722]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [4.14484175e-06 3.09779353e-05 4.05812745e-06 1.51687479e-07
 4.73961769e-05 5.14527051e-06 2.99922360e-07 1.05141713e-05
 4.55022169e-06 2.70003775e-06]
ene_total = [0.51959565 0.26309218 0.54927951 0.42567061 0.25584167 0.25368468
 0.42376107 0.37930318 0.51911337 0.24605454]
optimize_network_iter = 1 obj = 3.8353964400634717
eta = 0.6980699554597802
freqs = [23642854.46224663 44569165.40272366 23582139.46757871  7742332.9461439
 51302927.41865983 24471403.78525164  9714909.2256883  31593097.86188618
 24387978.69085684 19718556.51146085]
eta_min = 0.698069955459783	eta_max = 0.6980699554597762
af = 0.0032766452472554383	bf = 1.156843673852587	zeta = 0.003604309771980982	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [1.09907093e-06 8.21429384e-06 1.07607725e-06 4.02223555e-08
 1.25678526e-05 1.36435058e-06 7.95291995e-08 2.78800028e-06
 1.20656389e-06 7.15958094e-07]
ene_total = [1.72533799 0.87110728 1.82393243 1.41373708 0.84560497 0.84209973
 1.40738222 1.25884283 1.72370112 0.81697003]
ti_comp = [0.62724647 0.69980809 0.61887946 0.6536785  0.70201577 0.70220119
 0.65421817 0.66685038 0.62738645 0.70432722]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [4.14484175e-06 3.09779353e-05 4.05812745e-06 1.51687479e-07
 4.73961769e-05 5.14527051e-06 2.99922360e-07 1.05141713e-05
 4.55022169e-06 2.70003775e-06]
ene_total = [0.51959565 0.26309218 0.54927951 0.42567061 0.25584167 0.25368468
 0.42376107 0.37930318 0.51911337 0.24605454]
optimize_network_iter = 2 obj = 3.8353964400634206
eta = 0.6980699554597762
freqs = [23642854.46224662 44569165.40272372 23582139.46757869  7742332.9461439
 51302927.41865991 24471403.78525168  9714909.2256883  31593097.8618862
 24387978.69085682 19718556.51146088]
Done!
At round 51 eta: 0.6980699554597762
At round 51 local rounds: 11.76975943561131
At round 51 global rounds: 35.48094949322612
At round 51 a_n: 10.370218813849878
gradient difference: 0.6163690090179443
train() client id: f_00000-0-0 loss: 1.046301  [   32/  126]
train() client id: f_00000-0-1 loss: 1.098895  [   64/  126]
train() client id: f_00000-0-2 loss: 0.918848  [   96/  126]
train() client id: f_00000-1-0 loss: 0.895385  [   32/  126]
train() client id: f_00000-1-1 loss: 0.874961  [   64/  126]
train() client id: f_00000-1-2 loss: 1.016771  [   96/  126]
train() client id: f_00000-2-0 loss: 0.876207  [   32/  126]
train() client id: f_00000-2-1 loss: 0.786177  [   64/  126]
train() client id: f_00000-2-2 loss: 1.058558  [   96/  126]
train() client id: f_00000-3-0 loss: 0.979941  [   32/  126]
train() client id: f_00000-3-1 loss: 0.805662  [   64/  126]
train() client id: f_00000-3-2 loss: 0.878796  [   96/  126]
train() client id: f_00000-4-0 loss: 0.916278  [   32/  126]
train() client id: f_00000-4-1 loss: 0.801853  [   64/  126]
train() client id: f_00000-4-2 loss: 0.831713  [   96/  126]
train() client id: f_00000-5-0 loss: 0.753265  [   32/  126]
train() client id: f_00000-5-1 loss: 0.968560  [   64/  126]
train() client id: f_00000-5-2 loss: 0.784548  [   96/  126]
train() client id: f_00000-6-0 loss: 0.823351  [   32/  126]
train() client id: f_00000-6-1 loss: 0.809928  [   64/  126]
train() client id: f_00000-6-2 loss: 0.868448  [   96/  126]
train() client id: f_00000-7-0 loss: 0.763066  [   32/  126]
train() client id: f_00000-7-1 loss: 0.812482  [   64/  126]
train() client id: f_00000-7-2 loss: 0.881827  [   96/  126]
train() client id: f_00000-8-0 loss: 0.732171  [   32/  126]
train() client id: f_00000-8-1 loss: 0.774237  [   64/  126]
train() client id: f_00000-8-2 loss: 0.889383  [   96/  126]
train() client id: f_00000-9-0 loss: 0.778181  [   32/  126]
train() client id: f_00000-9-1 loss: 0.812982  [   64/  126]
train() client id: f_00000-9-2 loss: 0.798845  [   96/  126]
train() client id: f_00000-10-0 loss: 0.835920  [   32/  126]
train() client id: f_00000-10-1 loss: 0.742820  [   64/  126]
train() client id: f_00000-10-2 loss: 0.908839  [   96/  126]
train() client id: f_00001-0-0 loss: 0.381855  [   32/  265]
train() client id: f_00001-0-1 loss: 0.337712  [   64/  265]
train() client id: f_00001-0-2 loss: 0.340877  [   96/  265]
train() client id: f_00001-0-3 loss: 0.384928  [  128/  265]
train() client id: f_00001-0-4 loss: 0.442631  [  160/  265]
train() client id: f_00001-0-5 loss: 0.373877  [  192/  265]
train() client id: f_00001-0-6 loss: 0.473284  [  224/  265]
train() client id: f_00001-0-7 loss: 0.449696  [  256/  265]
train() client id: f_00001-1-0 loss: 0.323958  [   32/  265]
train() client id: f_00001-1-1 loss: 0.548364  [   64/  265]
train() client id: f_00001-1-2 loss: 0.368265  [   96/  265]
train() client id: f_00001-1-3 loss: 0.317901  [  128/  265]
train() client id: f_00001-1-4 loss: 0.476551  [  160/  265]
train() client id: f_00001-1-5 loss: 0.391361  [  192/  265]
train() client id: f_00001-1-6 loss: 0.346010  [  224/  265]
train() client id: f_00001-1-7 loss: 0.342816  [  256/  265]
train() client id: f_00001-2-0 loss: 0.413455  [   32/  265]
train() client id: f_00001-2-1 loss: 0.370178  [   64/  265]
train() client id: f_00001-2-2 loss: 0.421895  [   96/  265]
train() client id: f_00001-2-3 loss: 0.515721  [  128/  265]
train() client id: f_00001-2-4 loss: 0.314325  [  160/  265]
train() client id: f_00001-2-5 loss: 0.379532  [  192/  265]
train() client id: f_00001-2-6 loss: 0.355562  [  224/  265]
train() client id: f_00001-2-7 loss: 0.296514  [  256/  265]
train() client id: f_00001-3-0 loss: 0.383110  [   32/  265]
train() client id: f_00001-3-1 loss: 0.468304  [   64/  265]
train() client id: f_00001-3-2 loss: 0.546015  [   96/  265]
train() client id: f_00001-3-3 loss: 0.305105  [  128/  265]
train() client id: f_00001-3-4 loss: 0.336741  [  160/  265]
train() client id: f_00001-3-5 loss: 0.421913  [  192/  265]
train() client id: f_00001-3-6 loss: 0.295301  [  224/  265]
train() client id: f_00001-3-7 loss: 0.311930  [  256/  265]
train() client id: f_00001-4-0 loss: 0.345053  [   32/  265]
train() client id: f_00001-4-1 loss: 0.307983  [   64/  265]
train() client id: f_00001-4-2 loss: 0.370386  [   96/  265]
train() client id: f_00001-4-3 loss: 0.508364  [  128/  265]
train() client id: f_00001-4-4 loss: 0.318316  [  160/  265]
train() client id: f_00001-4-5 loss: 0.505636  [  192/  265]
train() client id: f_00001-4-6 loss: 0.416433  [  224/  265]
train() client id: f_00001-4-7 loss: 0.286199  [  256/  265]
train() client id: f_00001-5-0 loss: 0.359112  [   32/  265]
train() client id: f_00001-5-1 loss: 0.317132  [   64/  265]
train() client id: f_00001-5-2 loss: 0.281053  [   96/  265]
train() client id: f_00001-5-3 loss: 0.473276  [  128/  265]
train() client id: f_00001-5-4 loss: 0.365151  [  160/  265]
train() client id: f_00001-5-5 loss: 0.488121  [  192/  265]
train() client id: f_00001-5-6 loss: 0.364252  [  224/  265]
train() client id: f_00001-5-7 loss: 0.370457  [  256/  265]
train() client id: f_00001-6-0 loss: 0.438370  [   32/  265]
train() client id: f_00001-6-1 loss: 0.435793  [   64/  265]
train() client id: f_00001-6-2 loss: 0.386010  [   96/  265]
train() client id: f_00001-6-3 loss: 0.419494  [  128/  265]
train() client id: f_00001-6-4 loss: 0.296139  [  160/  265]
train() client id: f_00001-6-5 loss: 0.377430  [  192/  265]
train() client id: f_00001-6-6 loss: 0.319075  [  224/  265]
train() client id: f_00001-6-7 loss: 0.344830  [  256/  265]
train() client id: f_00001-7-0 loss: 0.409204  [   32/  265]
train() client id: f_00001-7-1 loss: 0.309196  [   64/  265]
train() client id: f_00001-7-2 loss: 0.343388  [   96/  265]
train() client id: f_00001-7-3 loss: 0.477828  [  128/  265]
train() client id: f_00001-7-4 loss: 0.293421  [  160/  265]
train() client id: f_00001-7-5 loss: 0.279266  [  192/  265]
train() client id: f_00001-7-6 loss: 0.303728  [  224/  265]
train() client id: f_00001-7-7 loss: 0.504215  [  256/  265]
train() client id: f_00001-8-0 loss: 0.409536  [   32/  265]
train() client id: f_00001-8-1 loss: 0.364449  [   64/  265]
train() client id: f_00001-8-2 loss: 0.372143  [   96/  265]
train() client id: f_00001-8-3 loss: 0.266659  [  128/  265]
train() client id: f_00001-8-4 loss: 0.321137  [  160/  265]
train() client id: f_00001-8-5 loss: 0.320603  [  192/  265]
train() client id: f_00001-8-6 loss: 0.495750  [  224/  265]
train() client id: f_00001-8-7 loss: 0.348386  [  256/  265]
train() client id: f_00001-9-0 loss: 0.287684  [   32/  265]
train() client id: f_00001-9-1 loss: 0.466971  [   64/  265]
train() client id: f_00001-9-2 loss: 0.273068  [   96/  265]
train() client id: f_00001-9-3 loss: 0.435722  [  128/  265]
train() client id: f_00001-9-4 loss: 0.444232  [  160/  265]
train() client id: f_00001-9-5 loss: 0.408287  [  192/  265]
train() client id: f_00001-9-6 loss: 0.370670  [  224/  265]
train() client id: f_00001-9-7 loss: 0.305851  [  256/  265]
train() client id: f_00001-10-0 loss: 0.466312  [   32/  265]
train() client id: f_00001-10-1 loss: 0.332644  [   64/  265]
train() client id: f_00001-10-2 loss: 0.345269  [   96/  265]
train() client id: f_00001-10-3 loss: 0.305194  [  128/  265]
train() client id: f_00001-10-4 loss: 0.326444  [  160/  265]
train() client id: f_00001-10-5 loss: 0.394103  [  192/  265]
train() client id: f_00001-10-6 loss: 0.270994  [  224/  265]
train() client id: f_00001-10-7 loss: 0.457637  [  256/  265]
train() client id: f_00002-0-0 loss: 1.138648  [   32/  124]
train() client id: f_00002-0-1 loss: 1.018973  [   64/  124]
train() client id: f_00002-0-2 loss: 1.152532  [   96/  124]
train() client id: f_00002-1-0 loss: 1.033691  [   32/  124]
train() client id: f_00002-1-1 loss: 1.164226  [   64/  124]
train() client id: f_00002-1-2 loss: 1.035054  [   96/  124]
train() client id: f_00002-2-0 loss: 1.158328  [   32/  124]
train() client id: f_00002-2-1 loss: 1.006351  [   64/  124]
train() client id: f_00002-2-2 loss: 1.046226  [   96/  124]
train() client id: f_00002-3-0 loss: 0.931518  [   32/  124]
train() client id: f_00002-3-1 loss: 1.239491  [   64/  124]
train() client id: f_00002-3-2 loss: 1.063906  [   96/  124]
train() client id: f_00002-4-0 loss: 1.077787  [   32/  124]
train() client id: f_00002-4-1 loss: 0.990457  [   64/  124]
train() client id: f_00002-4-2 loss: 1.009454  [   96/  124]
train() client id: f_00002-5-0 loss: 1.044294  [   32/  124]
train() client id: f_00002-5-1 loss: 1.266890  [   64/  124]
train() client id: f_00002-5-2 loss: 0.942550  [   96/  124]
train() client id: f_00002-6-0 loss: 1.097271  [   32/  124]
train() client id: f_00002-6-1 loss: 0.980853  [   64/  124]
train() client id: f_00002-6-2 loss: 1.180890  [   96/  124]
train() client id: f_00002-7-0 loss: 0.915103  [   32/  124]
train() client id: f_00002-7-1 loss: 1.307554  [   64/  124]
train() client id: f_00002-7-2 loss: 0.946726  [   96/  124]
train() client id: f_00002-8-0 loss: 1.120852  [   32/  124]
train() client id: f_00002-8-1 loss: 1.113543  [   64/  124]
train() client id: f_00002-8-2 loss: 0.964163  [   96/  124]
train() client id: f_00002-9-0 loss: 0.993153  [   32/  124]
train() client id: f_00002-9-1 loss: 0.997023  [   64/  124]
train() client id: f_00002-9-2 loss: 1.055844  [   96/  124]
train() client id: f_00002-10-0 loss: 1.056748  [   32/  124]
train() client id: f_00002-10-1 loss: 1.106512  [   64/  124]
train() client id: f_00002-10-2 loss: 1.057089  [   96/  124]
train() client id: f_00003-0-0 loss: 0.688968  [   32/   43]
train() client id: f_00003-1-0 loss: 0.560748  [   32/   43]
train() client id: f_00003-2-0 loss: 0.361225  [   32/   43]
train() client id: f_00003-3-0 loss: 0.720673  [   32/   43]
train() client id: f_00003-4-0 loss: 0.637855  [   32/   43]
train() client id: f_00003-5-0 loss: 0.663220  [   32/   43]
train() client id: f_00003-6-0 loss: 0.498527  [   32/   43]
train() client id: f_00003-7-0 loss: 0.581750  [   32/   43]
train() client id: f_00003-8-0 loss: 0.557133  [   32/   43]
train() client id: f_00003-9-0 loss: 0.765865  [   32/   43]
train() client id: f_00003-10-0 loss: 0.692072  [   32/   43]
train() client id: f_00004-0-0 loss: 0.751851  [   32/  306]
train() client id: f_00004-0-1 loss: 0.746553  [   64/  306]
train() client id: f_00004-0-2 loss: 0.849434  [   96/  306]
train() client id: f_00004-0-3 loss: 0.819245  [  128/  306]
train() client id: f_00004-0-4 loss: 0.841189  [  160/  306]
train() client id: f_00004-0-5 loss: 0.849113  [  192/  306]
train() client id: f_00004-0-6 loss: 0.680813  [  224/  306]
train() client id: f_00004-0-7 loss: 0.890383  [  256/  306]
train() client id: f_00004-0-8 loss: 0.726384  [  288/  306]
train() client id: f_00004-1-0 loss: 0.849892  [   32/  306]
train() client id: f_00004-1-1 loss: 0.758650  [   64/  306]
train() client id: f_00004-1-2 loss: 0.810032  [   96/  306]
train() client id: f_00004-1-3 loss: 0.668545  [  128/  306]
train() client id: f_00004-1-4 loss: 0.677302  [  160/  306]
train() client id: f_00004-1-5 loss: 0.974160  [  192/  306]
train() client id: f_00004-1-6 loss: 0.825859  [  224/  306]
train() client id: f_00004-1-7 loss: 0.802384  [  256/  306]
train() client id: f_00004-1-8 loss: 0.936539  [  288/  306]
train() client id: f_00004-2-0 loss: 0.779556  [   32/  306]
train() client id: f_00004-2-1 loss: 0.821563  [   64/  306]
train() client id: f_00004-2-2 loss: 0.770641  [   96/  306]
train() client id: f_00004-2-3 loss: 0.742909  [  128/  306]
train() client id: f_00004-2-4 loss: 0.789095  [  160/  306]
train() client id: f_00004-2-5 loss: 0.821143  [  192/  306]
train() client id: f_00004-2-6 loss: 0.751497  [  224/  306]
train() client id: f_00004-2-7 loss: 0.830772  [  256/  306]
train() client id: f_00004-2-8 loss: 0.871036  [  288/  306]
train() client id: f_00004-3-0 loss: 0.849417  [   32/  306]
train() client id: f_00004-3-1 loss: 0.890911  [   64/  306]
train() client id: f_00004-3-2 loss: 0.808449  [   96/  306]
train() client id: f_00004-3-3 loss: 0.803232  [  128/  306]
train() client id: f_00004-3-4 loss: 0.727668  [  160/  306]
train() client id: f_00004-3-5 loss: 0.817416  [  192/  306]
train() client id: f_00004-3-6 loss: 0.866917  [  224/  306]
train() client id: f_00004-3-7 loss: 0.692888  [  256/  306]
train() client id: f_00004-3-8 loss: 0.800316  [  288/  306]
train() client id: f_00004-4-0 loss: 0.901440  [   32/  306]
train() client id: f_00004-4-1 loss: 0.689927  [   64/  306]
train() client id: f_00004-4-2 loss: 0.742665  [   96/  306]
train() client id: f_00004-4-3 loss: 0.744251  [  128/  306]
train() client id: f_00004-4-4 loss: 0.849615  [  160/  306]
train() client id: f_00004-4-5 loss: 0.941954  [  192/  306]
train() client id: f_00004-4-6 loss: 0.912851  [  224/  306]
train() client id: f_00004-4-7 loss: 0.746057  [  256/  306]
train() client id: f_00004-4-8 loss: 0.781854  [  288/  306]
train() client id: f_00004-5-0 loss: 0.836873  [   32/  306]
train() client id: f_00004-5-1 loss: 0.670843  [   64/  306]
train() client id: f_00004-5-2 loss: 0.693558  [   96/  306]
train() client id: f_00004-5-3 loss: 0.652516  [  128/  306]
train() client id: f_00004-5-4 loss: 0.851710  [  160/  306]
train() client id: f_00004-5-5 loss: 0.910352  [  192/  306]
train() client id: f_00004-5-6 loss: 0.988927  [  224/  306]
train() client id: f_00004-5-7 loss: 0.908490  [  256/  306]
train() client id: f_00004-5-8 loss: 0.771557  [  288/  306]
train() client id: f_00004-6-0 loss: 0.727369  [   32/  306]
train() client id: f_00004-6-1 loss: 0.818194  [   64/  306]
train() client id: f_00004-6-2 loss: 0.768050  [   96/  306]
train() client id: f_00004-6-3 loss: 0.854293  [  128/  306]
train() client id: f_00004-6-4 loss: 0.852746  [  160/  306]
train() client id: f_00004-6-5 loss: 0.692019  [  192/  306]
train() client id: f_00004-6-6 loss: 0.812853  [  224/  306]
train() client id: f_00004-6-7 loss: 0.846644  [  256/  306]
train() client id: f_00004-6-8 loss: 0.785163  [  288/  306]
train() client id: f_00004-7-0 loss: 0.776701  [   32/  306]
train() client id: f_00004-7-1 loss: 0.800466  [   64/  306]
train() client id: f_00004-7-2 loss: 0.850351  [   96/  306]
train() client id: f_00004-7-3 loss: 0.828286  [  128/  306]
train() client id: f_00004-7-4 loss: 0.757575  [  160/  306]
train() client id: f_00004-7-5 loss: 0.799935  [  192/  306]
train() client id: f_00004-7-6 loss: 0.787213  [  224/  306]
train() client id: f_00004-7-7 loss: 0.803059  [  256/  306]
train() client id: f_00004-7-8 loss: 0.861523  [  288/  306]
train() client id: f_00004-8-0 loss: 0.817550  [   32/  306]
train() client id: f_00004-8-1 loss: 0.825500  [   64/  306]
train() client id: f_00004-8-2 loss: 0.792962  [   96/  306]
train() client id: f_00004-8-3 loss: 0.947726  [  128/  306]
train() client id: f_00004-8-4 loss: 0.740955  [  160/  306]
train() client id: f_00004-8-5 loss: 0.707352  [  192/  306]
train() client id: f_00004-8-6 loss: 0.777698  [  224/  306]
train() client id: f_00004-8-7 loss: 0.750792  [  256/  306]
train() client id: f_00004-8-8 loss: 0.876323  [  288/  306]
train() client id: f_00004-9-0 loss: 0.907682  [   32/  306]
train() client id: f_00004-9-1 loss: 0.749940  [   64/  306]
train() client id: f_00004-9-2 loss: 0.820651  [   96/  306]
train() client id: f_00004-9-3 loss: 0.977445  [  128/  306]
train() client id: f_00004-9-4 loss: 0.748258  [  160/  306]
train() client id: f_00004-9-5 loss: 0.791807  [  192/  306]
train() client id: f_00004-9-6 loss: 0.815543  [  224/  306]
train() client id: f_00004-9-7 loss: 0.763548  [  256/  306]
train() client id: f_00004-9-8 loss: 0.756493  [  288/  306]
train() client id: f_00004-10-0 loss: 0.833872  [   32/  306]
train() client id: f_00004-10-1 loss: 0.850019  [   64/  306]
train() client id: f_00004-10-2 loss: 0.845620  [   96/  306]
train() client id: f_00004-10-3 loss: 0.799308  [  128/  306]
train() client id: f_00004-10-4 loss: 0.819716  [  160/  306]
train() client id: f_00004-10-5 loss: 0.773892  [  192/  306]
train() client id: f_00004-10-6 loss: 0.788771  [  224/  306]
train() client id: f_00004-10-7 loss: 0.824764  [  256/  306]
train() client id: f_00004-10-8 loss: 0.834679  [  288/  306]
train() client id: f_00005-0-0 loss: 0.610580  [   32/  146]
train() client id: f_00005-0-1 loss: 0.604858  [   64/  146]
train() client id: f_00005-0-2 loss: 0.385063  [   96/  146]
train() client id: f_00005-0-3 loss: 0.463961  [  128/  146]
train() client id: f_00005-1-0 loss: 0.755831  [   32/  146]
train() client id: f_00005-1-1 loss: 0.365146  [   64/  146]
train() client id: f_00005-1-2 loss: 0.270946  [   96/  146]
train() client id: f_00005-1-3 loss: 0.472616  [  128/  146]
train() client id: f_00005-2-0 loss: 0.533817  [   32/  146]
train() client id: f_00005-2-1 loss: 0.685363  [   64/  146]
train() client id: f_00005-2-2 loss: 0.278848  [   96/  146]
train() client id: f_00005-2-3 loss: 0.392294  [  128/  146]
train() client id: f_00005-3-0 loss: 0.288069  [   32/  146]
train() client id: f_00005-3-1 loss: 0.618974  [   64/  146]
train() client id: f_00005-3-2 loss: 0.630970  [   96/  146]
train() client id: f_00005-3-3 loss: 0.400584  [  128/  146]
train() client id: f_00005-4-0 loss: 0.270349  [   32/  146]
train() client id: f_00005-4-1 loss: 0.516783  [   64/  146]
train() client id: f_00005-4-2 loss: 0.403003  [   96/  146]
train() client id: f_00005-4-3 loss: 0.784375  [  128/  146]
train() client id: f_00005-5-0 loss: 0.569581  [   32/  146]
train() client id: f_00005-5-1 loss: 0.359801  [   64/  146]
train() client id: f_00005-5-2 loss: 0.238198  [   96/  146]
train() client id: f_00005-5-3 loss: 0.659176  [  128/  146]
train() client id: f_00005-6-0 loss: 0.417401  [   32/  146]
train() client id: f_00005-6-1 loss: 0.773466  [   64/  146]
train() client id: f_00005-6-2 loss: 0.406727  [   96/  146]
train() client id: f_00005-6-3 loss: 0.240760  [  128/  146]
train() client id: f_00005-7-0 loss: 0.520106  [   32/  146]
train() client id: f_00005-7-1 loss: 0.246361  [   64/  146]
train() client id: f_00005-7-2 loss: 0.474775  [   96/  146]
train() client id: f_00005-7-3 loss: 0.338755  [  128/  146]
train() client id: f_00005-8-0 loss: 0.318698  [   32/  146]
train() client id: f_00005-8-1 loss: 0.746363  [   64/  146]
train() client id: f_00005-8-2 loss: 0.439532  [   96/  146]
train() client id: f_00005-8-3 loss: 0.369219  [  128/  146]
train() client id: f_00005-9-0 loss: 0.239599  [   32/  146]
train() client id: f_00005-9-1 loss: 0.845747  [   64/  146]
train() client id: f_00005-9-2 loss: 0.291789  [   96/  146]
train() client id: f_00005-9-3 loss: 0.431586  [  128/  146]
train() client id: f_00005-10-0 loss: 0.528603  [   32/  146]
train() client id: f_00005-10-1 loss: 0.516744  [   64/  146]
train() client id: f_00005-10-2 loss: 0.480940  [   96/  146]
train() client id: f_00005-10-3 loss: 0.492571  [  128/  146]
train() client id: f_00006-0-0 loss: 0.555209  [   32/   54]
train() client id: f_00006-1-0 loss: 0.558572  [   32/   54]
train() client id: f_00006-2-0 loss: 0.608391  [   32/   54]
train() client id: f_00006-3-0 loss: 0.550845  [   32/   54]
train() client id: f_00006-4-0 loss: 0.553414  [   32/   54]
train() client id: f_00006-5-0 loss: 0.600850  [   32/   54]
train() client id: f_00006-6-0 loss: 0.602278  [   32/   54]
train() client id: f_00006-7-0 loss: 0.601345  [   32/   54]
train() client id: f_00006-8-0 loss: 0.505313  [   32/   54]
train() client id: f_00006-9-0 loss: 0.513439  [   32/   54]
train() client id: f_00006-10-0 loss: 0.551929  [   32/   54]
train() client id: f_00007-0-0 loss: 0.501474  [   32/  179]
train() client id: f_00007-0-1 loss: 0.609351  [   64/  179]
train() client id: f_00007-0-2 loss: 0.541310  [   96/  179]
train() client id: f_00007-0-3 loss: 0.521812  [  128/  179]
train() client id: f_00007-0-4 loss: 0.845383  [  160/  179]
train() client id: f_00007-1-0 loss: 0.467126  [   32/  179]
train() client id: f_00007-1-1 loss: 0.662013  [   64/  179]
train() client id: f_00007-1-2 loss: 0.537340  [   96/  179]
train() client id: f_00007-1-3 loss: 0.464867  [  128/  179]
train() client id: f_00007-1-4 loss: 0.694718  [  160/  179]
train() client id: f_00007-2-0 loss: 0.660516  [   32/  179]
train() client id: f_00007-2-1 loss: 0.600398  [   64/  179]
train() client id: f_00007-2-2 loss: 0.563242  [   96/  179]
train() client id: f_00007-2-3 loss: 0.455886  [  128/  179]
train() client id: f_00007-2-4 loss: 0.541551  [  160/  179]
train() client id: f_00007-3-0 loss: 0.540761  [   32/  179]
train() client id: f_00007-3-1 loss: 0.732084  [   64/  179]
train() client id: f_00007-3-2 loss: 0.399301  [   96/  179]
train() client id: f_00007-3-3 loss: 0.665875  [  128/  179]
train() client id: f_00007-3-4 loss: 0.574577  [  160/  179]
train() client id: f_00007-4-0 loss: 0.499442  [   32/  179]
train() client id: f_00007-4-1 loss: 0.383626  [   64/  179]
train() client id: f_00007-4-2 loss: 0.829271  [   96/  179]
train() client id: f_00007-4-3 loss: 0.695700  [  128/  179]
train() client id: f_00007-4-4 loss: 0.372419  [  160/  179]
train() client id: f_00007-5-0 loss: 0.492964  [   32/  179]
train() client id: f_00007-5-1 loss: 0.426742  [   64/  179]
train() client id: f_00007-5-2 loss: 0.586146  [   96/  179]
train() client id: f_00007-5-3 loss: 0.803435  [  128/  179]
train() client id: f_00007-5-4 loss: 0.392632  [  160/  179]
train() client id: f_00007-6-0 loss: 0.470423  [   32/  179]
train() client id: f_00007-6-1 loss: 0.474437  [   64/  179]
train() client id: f_00007-6-2 loss: 0.652631  [   96/  179]
train() client id: f_00007-6-3 loss: 0.738441  [  128/  179]
train() client id: f_00007-6-4 loss: 0.494714  [  160/  179]
train() client id: f_00007-7-0 loss: 0.671380  [   32/  179]
train() client id: f_00007-7-1 loss: 0.499439  [   64/  179]
train() client id: f_00007-7-2 loss: 0.627786  [   96/  179]
train() client id: f_00007-7-3 loss: 0.532221  [  128/  179]
train() client id: f_00007-7-4 loss: 0.468691  [  160/  179]
train() client id: f_00007-8-0 loss: 0.405789  [   32/  179]
train() client id: f_00007-8-1 loss: 0.567121  [   64/  179]
train() client id: f_00007-8-2 loss: 0.381829  [   96/  179]
train() client id: f_00007-8-3 loss: 0.462935  [  128/  179]
train() client id: f_00007-8-4 loss: 0.830815  [  160/  179]
train() client id: f_00007-9-0 loss: 0.399154  [   32/  179]
train() client id: f_00007-9-1 loss: 0.512172  [   64/  179]
train() client id: f_00007-9-2 loss: 0.599752  [   96/  179]
train() client id: f_00007-9-3 loss: 0.448206  [  128/  179]
train() client id: f_00007-9-4 loss: 0.687548  [  160/  179]
train() client id: f_00007-10-0 loss: 0.410744  [   32/  179]
train() client id: f_00007-10-1 loss: 0.382969  [   64/  179]
train() client id: f_00007-10-2 loss: 0.363016  [   96/  179]
train() client id: f_00007-10-3 loss: 0.829515  [  128/  179]
train() client id: f_00007-10-4 loss: 0.669732  [  160/  179]
train() client id: f_00008-0-0 loss: 0.798827  [   32/  130]
train() client id: f_00008-0-1 loss: 0.763706  [   64/  130]
train() client id: f_00008-0-2 loss: 0.741201  [   96/  130]
train() client id: f_00008-0-3 loss: 0.851893  [  128/  130]
train() client id: f_00008-1-0 loss: 0.730166  [   32/  130]
train() client id: f_00008-1-1 loss: 0.788219  [   64/  130]
train() client id: f_00008-1-2 loss: 0.825875  [   96/  130]
train() client id: f_00008-1-3 loss: 0.818564  [  128/  130]
train() client id: f_00008-2-0 loss: 0.833972  [   32/  130]
train() client id: f_00008-2-1 loss: 0.656389  [   64/  130]
train() client id: f_00008-2-2 loss: 0.847141  [   96/  130]
train() client id: f_00008-2-3 loss: 0.828567  [  128/  130]
train() client id: f_00008-3-0 loss: 0.801460  [   32/  130]
train() client id: f_00008-3-1 loss: 0.890950  [   64/  130]
train() client id: f_00008-3-2 loss: 0.776958  [   96/  130]
train() client id: f_00008-3-3 loss: 0.663911  [  128/  130]
train() client id: f_00008-4-0 loss: 0.704593  [   32/  130]
train() client id: f_00008-4-1 loss: 0.894237  [   64/  130]
train() client id: f_00008-4-2 loss: 0.823821  [   96/  130]
train() client id: f_00008-4-3 loss: 0.727111  [  128/  130]
train() client id: f_00008-5-0 loss: 0.829828  [   32/  130]
train() client id: f_00008-5-1 loss: 0.956105  [   64/  130]
train() client id: f_00008-5-2 loss: 0.633635  [   96/  130]
train() client id: f_00008-5-3 loss: 0.684485  [  128/  130]
train() client id: f_00008-6-0 loss: 0.723455  [   32/  130]
train() client id: f_00008-6-1 loss: 0.713608  [   64/  130]
train() client id: f_00008-6-2 loss: 0.856642  [   96/  130]
train() client id: f_00008-6-3 loss: 0.782569  [  128/  130]
train() client id: f_00008-7-0 loss: 0.753484  [   32/  130]
train() client id: f_00008-7-1 loss: 0.737267  [   64/  130]
train() client id: f_00008-7-2 loss: 0.819662  [   96/  130]
train() client id: f_00008-7-3 loss: 0.805582  [  128/  130]
train() client id: f_00008-8-0 loss: 0.813259  [   32/  130]
train() client id: f_00008-8-1 loss: 0.678380  [   64/  130]
train() client id: f_00008-8-2 loss: 0.878560  [   96/  130]
train() client id: f_00008-8-3 loss: 0.747431  [  128/  130]
train() client id: f_00008-9-0 loss: 0.771820  [   32/  130]
train() client id: f_00008-9-1 loss: 0.784314  [   64/  130]
train() client id: f_00008-9-2 loss: 0.767632  [   96/  130]
train() client id: f_00008-9-3 loss: 0.797679  [  128/  130]
train() client id: f_00008-10-0 loss: 0.737096  [   32/  130]
train() client id: f_00008-10-1 loss: 0.948536  [   64/  130]
train() client id: f_00008-10-2 loss: 0.727920  [   96/  130]
train() client id: f_00008-10-3 loss: 0.742642  [  128/  130]
train() client id: f_00009-0-0 loss: 0.982622  [   32/  118]
train() client id: f_00009-0-1 loss: 0.987569  [   64/  118]
train() client id: f_00009-0-2 loss: 0.896168  [   96/  118]
train() client id: f_00009-1-0 loss: 0.934628  [   32/  118]
train() client id: f_00009-1-1 loss: 0.929196  [   64/  118]
train() client id: f_00009-1-2 loss: 0.875281  [   96/  118]
train() client id: f_00009-2-0 loss: 0.968951  [   32/  118]
train() client id: f_00009-2-1 loss: 0.767080  [   64/  118]
train() client id: f_00009-2-2 loss: 0.989008  [   96/  118]
train() client id: f_00009-3-0 loss: 0.866937  [   32/  118]
train() client id: f_00009-3-1 loss: 0.765477  [   64/  118]
train() client id: f_00009-3-2 loss: 0.935931  [   96/  118]
train() client id: f_00009-4-0 loss: 0.832506  [   32/  118]
train() client id: f_00009-4-1 loss: 0.890167  [   64/  118]
train() client id: f_00009-4-2 loss: 0.905993  [   96/  118]
train() client id: f_00009-5-0 loss: 0.700914  [   32/  118]
train() client id: f_00009-5-1 loss: 0.908443  [   64/  118]
train() client id: f_00009-5-2 loss: 0.852583  [   96/  118]
train() client id: f_00009-6-0 loss: 0.813648  [   32/  118]
train() client id: f_00009-6-1 loss: 0.788935  [   64/  118]
train() client id: f_00009-6-2 loss: 0.890952  [   96/  118]
train() client id: f_00009-7-0 loss: 0.788452  [   32/  118]
train() client id: f_00009-7-1 loss: 0.924329  [   64/  118]
train() client id: f_00009-7-2 loss: 0.783262  [   96/  118]
train() client id: f_00009-8-0 loss: 0.928505  [   32/  118]
train() client id: f_00009-8-1 loss: 0.755267  [   64/  118]
train() client id: f_00009-8-2 loss: 0.844901  [   96/  118]
train() client id: f_00009-9-0 loss: 0.858968  [   32/  118]
train() client id: f_00009-9-1 loss: 0.815807  [   64/  118]
train() client id: f_00009-9-2 loss: 0.967719  [   96/  118]
train() client id: f_00009-10-0 loss: 0.751270  [   32/  118]
train() client id: f_00009-10-1 loss: 0.914897  [   64/  118]
train() client id: f_00009-10-2 loss: 0.757780  [   96/  118]
At round 51 accuracy: 0.6472148541114059
At round 51 training accuracy: 0.5915492957746479
At round 51 training loss: 0.8236180744733741
update_location
xs = [  -3.9056584     4.20031788  275.00902392   18.81129433    0.97929623
    3.95640986 -237.44319194 -216.32485185  259.66397685 -202.06087855]
ys = [ 267.5879595   250.55583871    1.32061395 -237.45517586  229.35018685
  212.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [285.68963971 269.80709958 292.62895834 258.33858662 250.20485053
 235.1712481  257.65511821 238.32145892 278.80826897 225.48749521]
dists_bs = [194.53998039 194.4773475  482.36153531 455.72150476 184.18104004
 182.90791844 188.28113892 179.01284267 462.29567447 173.12649821]
uav_gains = [3.55494025e-12 4.95127103e-12 3.08824037e-12 6.27981480e-12
 7.39020348e-12 9.78653336e-12 6.36780404e-12 9.24954034e-12
 4.10015262e-12 1.15466699e-11]
bs_gains = [4.30595271e-11 4.30983677e-11 3.38727622e-12 3.97132493e-12
 5.01889727e-11 5.11732577e-11 4.71883724e-11 5.43523537e-11
 3.81521100e-12 5.96865065e-11]
Round 52
-------------------------------
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.13977177 8.44962292 4.08581891 1.48801359 9.74245982 4.68747181
 1.834924   5.76952797 4.26183039 3.80061293]
obj_prev = 48.26005410532363
eta_min = 5.2478248998997527e-23	eta_max = 0.9386552346452276
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 11.138694899951254	eta = 0.9090909090909091
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 23.212442703241933	eta = 0.43623527270004686
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 16.931595727685714	eta = 0.598058590314986
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.799778450901695	eta = 0.6409005230136667
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.733820172197483	eta = 0.6435872637324471
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.733571912722388	eta = 0.6435974188731335
eta = 0.6435974188731335
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [0.03636401 0.07647987 0.03578681 0.01240994 0.0883126  0.04213608
 0.01558458 0.05165999 0.03751843 0.03405519]
ene_total = [1.50031121 2.44142183 1.50915709 0.72099371 2.77897583 1.43455086
 0.8117007  1.8214624  1.52193281 1.19306548]
ti_comp = [0.78968755 0.8674062  0.78087534 0.81798185 0.86972334 0.87000865
 0.81855413 0.83227438 0.79380079 0.87219318]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [4.81930945e-06 3.71600544e-05 4.69770128e-06 1.78526131e-07
 5.69096341e-05 6.17725006e-06 3.53077655e-07 1.24396969e-05
 5.23830472e-06 3.24492313e-06]
ene_total = [0.44237055 0.21717421 0.46800782 0.35990781 0.2110067  0.20870035
 0.35824773 0.31867769 0.43041446 0.20225874]
optimize_network_iter = 0 obj = 3.2167660550334363
eta = 0.6435974188731335
freqs = [23024303.73895735 44085381.74515043 22914544.46834145  7585706.59027375
 50770513.42376783 24215895.58340987  9519576.07846752 31035429.18235327
 23632141.3100814  19522731.20042242]
eta_min = 0.6435974188731354	eta_max = 0.698454705717541
af = 0.003087860896766924	bf = 1.1451029605675662	zeta = 0.003396646986443617	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [1.04231482e-06 8.03693475e-06 1.01601355e-06 3.86114307e-08
 1.23083516e-05 1.33600869e-06 7.63632379e-08 2.69044364e-06
 1.13293465e-06 7.01808322e-07]
ene_total = [1.7338512  0.84809379 1.83437118 1.41097682 0.82214886 0.81764254
 1.40445301 1.24824095 1.68694076 0.79265082]
ti_comp = [0.6447458  0.72246444 0.63593359 0.6730401  0.72478158 0.7250669
 0.67361237 0.68733263 0.64885904 0.72725142]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.90488746e-06 2.89320090e-05 3.82574001e-06 1.42428669e-07
 4.42612736e-05 4.80370504e-06 2.81601583e-07 9.85142590e-06
 4.23451096e-06 2.52087196e-06]
ene_total = [0.52281539 0.2563997  0.55311808 0.42538119 0.24895817 0.24662
 0.4234179  0.3765627  0.50868118 0.23902886]
optimize_network_iter = 1 obj = 3.8009831688814426
eta = 0.698454705717541
freqs = [22965892.7521965  43105303.11724808 22914544.46834147  7508078.8429111
 49615295.72405244 23663342.1290412   9420739.8305015  30604647.12122058
 23544761.66323882 19067718.62692055]
eta_min = 0.698454705717542	eta_max = 0.6984547057175405
af = 0.002969273754740996	bf = 1.1451029605675662	zeta = 0.003266201130215096	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [1.03703298e-06 7.68356267e-06 1.01601355e-06 3.78252199e-08
 1.17546026e-05 1.27573473e-06 7.47857989e-08 2.61627349e-06
 1.12457210e-06 6.69475724e-07]
ene_total = [1.73385059 0.84805348 1.83437118 1.41097673 0.8220857  0.81763566
 1.40445283 1.24823249 1.68693981 0.79264713]
ti_comp = [0.6447458  0.72246444 0.63593359 0.6730401  0.72478158 0.7250669
 0.67361237 0.68733263 0.64885904 0.72725142]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.90488746e-06 2.89320090e-05 3.82574001e-06 1.42428669e-07
 4.42612736e-05 4.80370504e-06 2.81601583e-07 9.85142590e-06
 4.23451096e-06 2.52087196e-06]
ene_total = [0.52281539 0.2563997  0.55311808 0.42538119 0.24895817 0.24662
 0.4234179  0.3765627  0.50868118 0.23902886]
optimize_network_iter = 2 obj = 3.800983168881437
eta = 0.6984547057175405
freqs = [22965892.7521965  43105303.11724809 22914544.46834146  7508078.8429111
 49615295.72405244 23663342.12904121  9420739.8305015  30604647.12122059
 23544761.66323882 19067718.62692055]
Done!
At round 52 eta: 0.6984547057175405
At round 52 local rounds: 11.751716534008594
At round 52 global rounds: 34.39025251090811
At round 52 a_n: 10.027672966880559
gradient difference: 0.539344310760498
train() client id: f_00000-0-0 loss: 1.292880  [   32/  126]
train() client id: f_00000-0-1 loss: 0.724213  [   64/  126]
train() client id: f_00000-0-2 loss: 0.768495  [   96/  126]
train() client id: f_00000-1-0 loss: 0.924582  [   32/  126]
train() client id: f_00000-1-1 loss: 0.966344  [   64/  126]
train() client id: f_00000-1-2 loss: 0.768392  [   96/  126]
train() client id: f_00000-2-0 loss: 0.792376  [   32/  126]
train() client id: f_00000-2-1 loss: 0.785764  [   64/  126]
train() client id: f_00000-2-2 loss: 0.797656  [   96/  126]
train() client id: f_00000-3-0 loss: 0.784047  [   32/  126]
train() client id: f_00000-3-1 loss: 0.908132  [   64/  126]
train() client id: f_00000-3-2 loss: 0.786613  [   96/  126]
train() client id: f_00000-4-0 loss: 0.787274  [   32/  126]
train() client id: f_00000-4-1 loss: 0.681115  [   64/  126]
train() client id: f_00000-4-2 loss: 0.747054  [   96/  126]
train() client id: f_00000-5-0 loss: 0.727674  [   32/  126]
train() client id: f_00000-5-1 loss: 0.749247  [   64/  126]
train() client id: f_00000-5-2 loss: 0.773280  [   96/  126]
train() client id: f_00000-6-0 loss: 0.750696  [   32/  126]
train() client id: f_00000-6-1 loss: 0.844344  [   64/  126]
train() client id: f_00000-6-2 loss: 0.772054  [   96/  126]
train() client id: f_00000-7-0 loss: 0.716492  [   32/  126]
train() client id: f_00000-7-1 loss: 0.799645  [   64/  126]
train() client id: f_00000-7-2 loss: 0.794964  [   96/  126]
train() client id: f_00000-8-0 loss: 0.839800  [   32/  126]
train() client id: f_00000-8-1 loss: 0.596026  [   64/  126]
train() client id: f_00000-8-2 loss: 0.740886  [   96/  126]
train() client id: f_00000-9-0 loss: 0.835529  [   32/  126]
train() client id: f_00000-9-1 loss: 0.758811  [   64/  126]
train() client id: f_00000-9-2 loss: 0.758209  [   96/  126]
train() client id: f_00000-10-0 loss: 0.845326  [   32/  126]
train() client id: f_00000-10-1 loss: 0.794636  [   64/  126]
train() client id: f_00000-10-2 loss: 0.773706  [   96/  126]
train() client id: f_00001-0-0 loss: 0.394967  [   32/  265]
train() client id: f_00001-0-1 loss: 0.377105  [   64/  265]
train() client id: f_00001-0-2 loss: 0.409761  [   96/  265]
train() client id: f_00001-0-3 loss: 0.296780  [  128/  265]
train() client id: f_00001-0-4 loss: 0.400251  [  160/  265]
train() client id: f_00001-0-5 loss: 0.397206  [  192/  265]
train() client id: f_00001-0-6 loss: 0.289461  [  224/  265]
train() client id: f_00001-0-7 loss: 0.382244  [  256/  265]
train() client id: f_00001-1-0 loss: 0.436558  [   32/  265]
train() client id: f_00001-1-1 loss: 0.316442  [   64/  265]
train() client id: f_00001-1-2 loss: 0.320034  [   96/  265]
train() client id: f_00001-1-3 loss: 0.307074  [  128/  265]
train() client id: f_00001-1-4 loss: 0.379482  [  160/  265]
train() client id: f_00001-1-5 loss: 0.309148  [  192/  265]
train() client id: f_00001-1-6 loss: 0.461755  [  224/  265]
train() client id: f_00001-1-7 loss: 0.284428  [  256/  265]
train() client id: f_00001-2-0 loss: 0.290803  [   32/  265]
train() client id: f_00001-2-1 loss: 0.245980  [   64/  265]
train() client id: f_00001-2-2 loss: 0.332770  [   96/  265]
train() client id: f_00001-2-3 loss: 0.402057  [  128/  265]
train() client id: f_00001-2-4 loss: 0.524201  [  160/  265]
train() client id: f_00001-2-5 loss: 0.344784  [  192/  265]
train() client id: f_00001-2-6 loss: 0.360646  [  224/  265]
train() client id: f_00001-2-7 loss: 0.306679  [  256/  265]
train() client id: f_00001-3-0 loss: 0.430768  [   32/  265]
train() client id: f_00001-3-1 loss: 0.292385  [   64/  265]
train() client id: f_00001-3-2 loss: 0.280386  [   96/  265]
train() client id: f_00001-3-3 loss: 0.440970  [  128/  265]
train() client id: f_00001-3-4 loss: 0.258081  [  160/  265]
train() client id: f_00001-3-5 loss: 0.372874  [  192/  265]
train() client id: f_00001-3-6 loss: 0.328791  [  224/  265]
train() client id: f_00001-3-7 loss: 0.364265  [  256/  265]
train() client id: f_00001-4-0 loss: 0.272686  [   32/  265]
train() client id: f_00001-4-1 loss: 0.421453  [   64/  265]
train() client id: f_00001-4-2 loss: 0.254535  [   96/  265]
train() client id: f_00001-4-3 loss: 0.397556  [  128/  265]
train() client id: f_00001-4-4 loss: 0.471526  [  160/  265]
train() client id: f_00001-4-5 loss: 0.327867  [  192/  265]
train() client id: f_00001-4-6 loss: 0.278950  [  224/  265]
train() client id: f_00001-4-7 loss: 0.273600  [  256/  265]
train() client id: f_00001-5-0 loss: 0.266704  [   32/  265]
train() client id: f_00001-5-1 loss: 0.291629  [   64/  265]
train() client id: f_00001-5-2 loss: 0.413345  [   96/  265]
train() client id: f_00001-5-3 loss: 0.288225  [  128/  265]
train() client id: f_00001-5-4 loss: 0.456477  [  160/  265]
train() client id: f_00001-5-5 loss: 0.314460  [  192/  265]
train() client id: f_00001-5-6 loss: 0.389303  [  224/  265]
train() client id: f_00001-5-7 loss: 0.255224  [  256/  265]
train() client id: f_00001-6-0 loss: 0.278052  [   32/  265]
train() client id: f_00001-6-1 loss: 0.312678  [   64/  265]
train() client id: f_00001-6-2 loss: 0.306005  [   96/  265]
train() client id: f_00001-6-3 loss: 0.460141  [  128/  265]
train() client id: f_00001-6-4 loss: 0.310432  [  160/  265]
train() client id: f_00001-6-5 loss: 0.398098  [  192/  265]
train() client id: f_00001-6-6 loss: 0.247253  [  224/  265]
train() client id: f_00001-6-7 loss: 0.299967  [  256/  265]
train() client id: f_00001-7-0 loss: 0.435008  [   32/  265]
train() client id: f_00001-7-1 loss: 0.393684  [   64/  265]
train() client id: f_00001-7-2 loss: 0.249806  [   96/  265]
train() client id: f_00001-7-3 loss: 0.276510  [  128/  265]
train() client id: f_00001-7-4 loss: 0.275313  [  160/  265]
train() client id: f_00001-7-5 loss: 0.265741  [  192/  265]
train() client id: f_00001-7-6 loss: 0.366782  [  224/  265]
train() client id: f_00001-7-7 loss: 0.424306  [  256/  265]
train() client id: f_00001-8-0 loss: 0.367110  [   32/  265]
train() client id: f_00001-8-1 loss: 0.283545  [   64/  265]
train() client id: f_00001-8-2 loss: 0.289498  [   96/  265]
train() client id: f_00001-8-3 loss: 0.398300  [  128/  265]
train() client id: f_00001-8-4 loss: 0.317129  [  160/  265]
train() client id: f_00001-8-5 loss: 0.304843  [  192/  265]
train() client id: f_00001-8-6 loss: 0.340030  [  224/  265]
train() client id: f_00001-8-7 loss: 0.368116  [  256/  265]
train() client id: f_00001-9-0 loss: 0.347349  [   32/  265]
train() client id: f_00001-9-1 loss: 0.263970  [   64/  265]
train() client id: f_00001-9-2 loss: 0.413425  [   96/  265]
train() client id: f_00001-9-3 loss: 0.344789  [  128/  265]
train() client id: f_00001-9-4 loss: 0.308408  [  160/  265]
train() client id: f_00001-9-5 loss: 0.321379  [  192/  265]
train() client id: f_00001-9-6 loss: 0.378625  [  224/  265]
train() client id: f_00001-9-7 loss: 0.294669  [  256/  265]
train() client id: f_00001-10-0 loss: 0.350632  [   32/  265]
train() client id: f_00001-10-1 loss: 0.403220  [   64/  265]
train() client id: f_00001-10-2 loss: 0.455995  [   96/  265]
train() client id: f_00001-10-3 loss: 0.286620  [  128/  265]
train() client id: f_00001-10-4 loss: 0.307185  [  160/  265]
train() client id: f_00001-10-5 loss: 0.267399  [  192/  265]
train() client id: f_00001-10-6 loss: 0.271528  [  224/  265]
train() client id: f_00001-10-7 loss: 0.325415  [  256/  265]
train() client id: f_00002-0-0 loss: 1.119956  [   32/  124]
train() client id: f_00002-0-1 loss: 1.058199  [   64/  124]
train() client id: f_00002-0-2 loss: 0.965095  [   96/  124]
train() client id: f_00002-1-0 loss: 1.034627  [   32/  124]
train() client id: f_00002-1-1 loss: 1.011257  [   64/  124]
train() client id: f_00002-1-2 loss: 1.034132  [   96/  124]
train() client id: f_00002-2-0 loss: 1.173068  [   32/  124]
train() client id: f_00002-2-1 loss: 0.864699  [   64/  124]
train() client id: f_00002-2-2 loss: 1.030315  [   96/  124]
train() client id: f_00002-3-0 loss: 1.032179  [   32/  124]
train() client id: f_00002-3-1 loss: 0.930028  [   64/  124]
train() client id: f_00002-3-2 loss: 1.051366  [   96/  124]
train() client id: f_00002-4-0 loss: 1.010407  [   32/  124]
train() client id: f_00002-4-1 loss: 0.910979  [   64/  124]
train() client id: f_00002-4-2 loss: 0.948333  [   96/  124]
train() client id: f_00002-5-0 loss: 1.128549  [   32/  124]
train() client id: f_00002-5-1 loss: 1.167491  [   64/  124]
train() client id: f_00002-5-2 loss: 0.789604  [   96/  124]
train() client id: f_00002-6-0 loss: 1.034880  [   32/  124]
train() client id: f_00002-6-1 loss: 0.919992  [   64/  124]
train() client id: f_00002-6-2 loss: 0.866434  [   96/  124]
train() client id: f_00002-7-0 loss: 0.814900  [   32/  124]
train() client id: f_00002-7-1 loss: 0.960338  [   64/  124]
train() client id: f_00002-7-2 loss: 0.948913  [   96/  124]
train() client id: f_00002-8-0 loss: 1.017231  [   32/  124]
train() client id: f_00002-8-1 loss: 0.836039  [   64/  124]
train() client id: f_00002-8-2 loss: 1.167451  [   96/  124]
train() client id: f_00002-9-0 loss: 0.917234  [   32/  124]
train() client id: f_00002-9-1 loss: 0.900782  [   64/  124]
train() client id: f_00002-9-2 loss: 0.946068  [   96/  124]
train() client id: f_00002-10-0 loss: 0.887387  [   32/  124]
train() client id: f_00002-10-1 loss: 0.933958  [   64/  124]
train() client id: f_00002-10-2 loss: 1.137943  [   96/  124]
train() client id: f_00003-0-0 loss: 0.440174  [   32/   43]
train() client id: f_00003-1-0 loss: 0.619595  [   32/   43]
train() client id: f_00003-2-0 loss: 0.476005  [   32/   43]
train() client id: f_00003-3-0 loss: 0.577748  [   32/   43]
train() client id: f_00003-4-0 loss: 0.425866  [   32/   43]
train() client id: f_00003-5-0 loss: 0.509604  [   32/   43]
train() client id: f_00003-6-0 loss: 0.480599  [   32/   43]
train() client id: f_00003-7-0 loss: 0.540794  [   32/   43]
train() client id: f_00003-8-0 loss: 0.624614  [   32/   43]
train() client id: f_00003-9-0 loss: 0.693096  [   32/   43]
train() client id: f_00003-10-0 loss: 0.406702  [   32/   43]
train() client id: f_00004-0-0 loss: 0.987629  [   32/  306]
train() client id: f_00004-0-1 loss: 0.769824  [   64/  306]
train() client id: f_00004-0-2 loss: 0.899747  [   96/  306]
train() client id: f_00004-0-3 loss: 0.647088  [  128/  306]
train() client id: f_00004-0-4 loss: 0.966728  [  160/  306]
train() client id: f_00004-0-5 loss: 0.728076  [  192/  306]
train() client id: f_00004-0-6 loss: 0.857981  [  224/  306]
train() client id: f_00004-0-7 loss: 0.861731  [  256/  306]
train() client id: f_00004-0-8 loss: 0.701339  [  288/  306]
train() client id: f_00004-1-0 loss: 0.793648  [   32/  306]
train() client id: f_00004-1-1 loss: 0.685362  [   64/  306]
train() client id: f_00004-1-2 loss: 0.998161  [   96/  306]
train() client id: f_00004-1-3 loss: 0.873109  [  128/  306]
train() client id: f_00004-1-4 loss: 0.809586  [  160/  306]
train() client id: f_00004-1-5 loss: 0.854919  [  192/  306]
train() client id: f_00004-1-6 loss: 0.757770  [  224/  306]
train() client id: f_00004-1-7 loss: 0.732954  [  256/  306]
train() client id: f_00004-1-8 loss: 0.818165  [  288/  306]
train() client id: f_00004-2-0 loss: 0.959445  [   32/  306]
train() client id: f_00004-2-1 loss: 0.775507  [   64/  306]
train() client id: f_00004-2-2 loss: 0.746445  [   96/  306]
train() client id: f_00004-2-3 loss: 0.874397  [  128/  306]
train() client id: f_00004-2-4 loss: 0.910327  [  160/  306]
train() client id: f_00004-2-5 loss: 0.918473  [  192/  306]
train() client id: f_00004-2-6 loss: 0.671426  [  224/  306]
train() client id: f_00004-2-7 loss: 0.771652  [  256/  306]
train() client id: f_00004-2-8 loss: 0.811112  [  288/  306]
train() client id: f_00004-3-0 loss: 0.717104  [   32/  306]
train() client id: f_00004-3-1 loss: 0.866081  [   64/  306]
train() client id: f_00004-3-2 loss: 0.834516  [   96/  306]
train() client id: f_00004-3-3 loss: 0.719157  [  128/  306]
train() client id: f_00004-3-4 loss: 0.956268  [  160/  306]
train() client id: f_00004-3-5 loss: 0.916640  [  192/  306]
train() client id: f_00004-3-6 loss: 0.767248  [  224/  306]
train() client id: f_00004-3-7 loss: 0.813053  [  256/  306]
train() client id: f_00004-3-8 loss: 0.785444  [  288/  306]
train() client id: f_00004-4-0 loss: 0.711446  [   32/  306]
train() client id: f_00004-4-1 loss: 0.876888  [   64/  306]
train() client id: f_00004-4-2 loss: 0.881920  [   96/  306]
train() client id: f_00004-4-3 loss: 0.940760  [  128/  306]
train() client id: f_00004-4-4 loss: 0.777270  [  160/  306]
train() client id: f_00004-4-5 loss: 0.768699  [  192/  306]
train() client id: f_00004-4-6 loss: 0.716735  [  224/  306]
train() client id: f_00004-4-7 loss: 0.942688  [  256/  306]
train() client id: f_00004-4-8 loss: 0.679428  [  288/  306]
train() client id: f_00004-5-0 loss: 0.869665  [   32/  306]
train() client id: f_00004-5-1 loss: 0.837481  [   64/  306]
train() client id: f_00004-5-2 loss: 0.753307  [   96/  306]
train() client id: f_00004-5-3 loss: 0.900244  [  128/  306]
train() client id: f_00004-5-4 loss: 0.886095  [  160/  306]
train() client id: f_00004-5-5 loss: 0.848355  [  192/  306]
train() client id: f_00004-5-6 loss: 0.756175  [  224/  306]
train() client id: f_00004-5-7 loss: 0.872232  [  256/  306]
train() client id: f_00004-5-8 loss: 0.762022  [  288/  306]
train() client id: f_00004-6-0 loss: 0.812601  [   32/  306]
train() client id: f_00004-6-1 loss: 0.813315  [   64/  306]
train() client id: f_00004-6-2 loss: 0.831574  [   96/  306]
train() client id: f_00004-6-3 loss: 0.827410  [  128/  306]
train() client id: f_00004-6-4 loss: 0.886160  [  160/  306]
train() client id: f_00004-6-5 loss: 0.734190  [  192/  306]
train() client id: f_00004-6-6 loss: 0.964962  [  224/  306]
train() client id: f_00004-6-7 loss: 0.724902  [  256/  306]
train() client id: f_00004-6-8 loss: 0.819283  [  288/  306]
train() client id: f_00004-7-0 loss: 0.743285  [   32/  306]
train() client id: f_00004-7-1 loss: 0.874555  [   64/  306]
train() client id: f_00004-7-2 loss: 1.033697  [   96/  306]
train() client id: f_00004-7-3 loss: 0.730888  [  128/  306]
train() client id: f_00004-7-4 loss: 0.763916  [  160/  306]
train() client id: f_00004-7-5 loss: 0.988846  [  192/  306]
train() client id: f_00004-7-6 loss: 0.853199  [  224/  306]
train() client id: f_00004-7-7 loss: 0.762752  [  256/  306]
train() client id: f_00004-7-8 loss: 0.721628  [  288/  306]
train() client id: f_00004-8-0 loss: 0.715073  [   32/  306]
train() client id: f_00004-8-1 loss: 0.865239  [   64/  306]
train() client id: f_00004-8-2 loss: 0.830108  [   96/  306]
train() client id: f_00004-8-3 loss: 0.728834  [  128/  306]
train() client id: f_00004-8-4 loss: 0.852076  [  160/  306]
train() client id: f_00004-8-5 loss: 1.014454  [  192/  306]
train() client id: f_00004-8-6 loss: 0.804274  [  224/  306]
train() client id: f_00004-8-7 loss: 0.791770  [  256/  306]
train() client id: f_00004-8-8 loss: 0.915613  [  288/  306]
train() client id: f_00004-9-0 loss: 0.877681  [   32/  306]
train() client id: f_00004-9-1 loss: 0.838237  [   64/  306]
train() client id: f_00004-9-2 loss: 0.752423  [   96/  306]
train() client id: f_00004-9-3 loss: 1.058184  [  128/  306]
train() client id: f_00004-9-4 loss: 0.839296  [  160/  306]
train() client id: f_00004-9-5 loss: 0.753596  [  192/  306]
train() client id: f_00004-9-6 loss: 0.745576  [  224/  306]
train() client id: f_00004-9-7 loss: 0.831539  [  256/  306]
train() client id: f_00004-9-8 loss: 0.895753  [  288/  306]
train() client id: f_00004-10-0 loss: 0.756677  [   32/  306]
train() client id: f_00004-10-1 loss: 0.734992  [   64/  306]
train() client id: f_00004-10-2 loss: 1.117614  [   96/  306]
train() client id: f_00004-10-3 loss: 0.915585  [  128/  306]
train() client id: f_00004-10-4 loss: 0.736150  [  160/  306]
train() client id: f_00004-10-5 loss: 0.829942  [  192/  306]
train() client id: f_00004-10-6 loss: 0.913230  [  224/  306]
train() client id: f_00004-10-7 loss: 0.697651  [  256/  306]
train() client id: f_00004-10-8 loss: 0.861753  [  288/  306]
train() client id: f_00005-0-0 loss: 0.588199  [   32/  146]
train() client id: f_00005-0-1 loss: 0.547228  [   64/  146]
train() client id: f_00005-0-2 loss: 0.460963  [   96/  146]
train() client id: f_00005-0-3 loss: 0.657251  [  128/  146]
train() client id: f_00005-1-0 loss: 0.625753  [   32/  146]
train() client id: f_00005-1-1 loss: 0.628128  [   64/  146]
train() client id: f_00005-1-2 loss: 0.494850  [   96/  146]
train() client id: f_00005-1-3 loss: 0.619673  [  128/  146]
train() client id: f_00005-2-0 loss: 0.659269  [   32/  146]
train() client id: f_00005-2-1 loss: 0.536359  [   64/  146]
train() client id: f_00005-2-2 loss: 0.469793  [   96/  146]
train() client id: f_00005-2-3 loss: 0.453766  [  128/  146]
train() client id: f_00005-3-0 loss: 0.560645  [   32/  146]
train() client id: f_00005-3-1 loss: 0.612316  [   64/  146]
train() client id: f_00005-3-2 loss: 0.553143  [   96/  146]
train() client id: f_00005-3-3 loss: 0.650421  [  128/  146]
train() client id: f_00005-4-0 loss: 0.428571  [   32/  146]
train() client id: f_00005-4-1 loss: 0.600522  [   64/  146]
train() client id: f_00005-4-2 loss: 0.766161  [   96/  146]
train() client id: f_00005-4-3 loss: 0.497855  [  128/  146]
train() client id: f_00005-5-0 loss: 0.434029  [   32/  146]
train() client id: f_00005-5-1 loss: 0.603076  [   64/  146]
train() client id: f_00005-5-2 loss: 0.437487  [   96/  146]
train() client id: f_00005-5-3 loss: 0.610537  [  128/  146]
train() client id: f_00005-6-0 loss: 0.625108  [   32/  146]
train() client id: f_00005-6-1 loss: 0.706356  [   64/  146]
train() client id: f_00005-6-2 loss: 0.547332  [   96/  146]
train() client id: f_00005-6-3 loss: 0.512154  [  128/  146]
train() client id: f_00005-7-0 loss: 0.432512  [   32/  146]
train() client id: f_00005-7-1 loss: 0.696429  [   64/  146]
train() client id: f_00005-7-2 loss: 0.393953  [   96/  146]
train() client id: f_00005-7-3 loss: 0.772798  [  128/  146]
train() client id: f_00005-8-0 loss: 0.473843  [   32/  146]
train() client id: f_00005-8-1 loss: 0.601663  [   64/  146]
train() client id: f_00005-8-2 loss: 0.471607  [   96/  146]
train() client id: f_00005-8-3 loss: 0.714070  [  128/  146]
train() client id: f_00005-9-0 loss: 0.303840  [   32/  146]
train() client id: f_00005-9-1 loss: 0.594151  [   64/  146]
train() client id: f_00005-9-2 loss: 0.637036  [   96/  146]
train() client id: f_00005-9-3 loss: 0.530170  [  128/  146]
train() client id: f_00005-10-0 loss: 0.680420  [   32/  146]
train() client id: f_00005-10-1 loss: 0.444390  [   64/  146]
train() client id: f_00005-10-2 loss: 0.763094  [   96/  146]
train() client id: f_00005-10-3 loss: 0.536220  [  128/  146]
train() client id: f_00006-0-0 loss: 0.507332  [   32/   54]
train() client id: f_00006-1-0 loss: 0.463089  [   32/   54]
train() client id: f_00006-2-0 loss: 0.514772  [   32/   54]
train() client id: f_00006-3-0 loss: 0.539635  [   32/   54]
train() client id: f_00006-4-0 loss: 0.591272  [   32/   54]
train() client id: f_00006-5-0 loss: 0.534356  [   32/   54]
train() client id: f_00006-6-0 loss: 0.574603  [   32/   54]
train() client id: f_00006-7-0 loss: 0.549907  [   32/   54]
train() client id: f_00006-8-0 loss: 0.516013  [   32/   54]
train() client id: f_00006-9-0 loss: 0.526690  [   32/   54]
train() client id: f_00006-10-0 loss: 0.574270  [   32/   54]
train() client id: f_00007-0-0 loss: 0.531877  [   32/  179]
train() client id: f_00007-0-1 loss: 0.483261  [   64/  179]
train() client id: f_00007-0-2 loss: 0.553325  [   96/  179]
train() client id: f_00007-0-3 loss: 0.466730  [  128/  179]
train() client id: f_00007-0-4 loss: 0.309581  [  160/  179]
train() client id: f_00007-1-0 loss: 0.408376  [   32/  179]
train() client id: f_00007-1-1 loss: 0.462046  [   64/  179]
train() client id: f_00007-1-2 loss: 0.399529  [   96/  179]
train() client id: f_00007-1-3 loss: 0.588820  [  128/  179]
train() client id: f_00007-1-4 loss: 0.680600  [  160/  179]
train() client id: f_00007-2-0 loss: 0.320170  [   32/  179]
train() client id: f_00007-2-1 loss: 0.497713  [   64/  179]
train() client id: f_00007-2-2 loss: 0.542341  [   96/  179]
train() client id: f_00007-2-3 loss: 0.726922  [  128/  179]
train() client id: f_00007-2-4 loss: 0.373427  [  160/  179]
train() client id: f_00007-3-0 loss: 0.523575  [   32/  179]
train() client id: f_00007-3-1 loss: 0.601756  [   64/  179]
train() client id: f_00007-3-2 loss: 0.310470  [   96/  179]
train() client id: f_00007-3-3 loss: 0.436701  [  128/  179]
train() client id: f_00007-3-4 loss: 0.386002  [  160/  179]
train() client id: f_00007-4-0 loss: 0.571845  [   32/  179]
train() client id: f_00007-4-1 loss: 0.625355  [   64/  179]
train() client id: f_00007-4-2 loss: 0.280868  [   96/  179]
train() client id: f_00007-4-3 loss: 0.331342  [  128/  179]
train() client id: f_00007-4-4 loss: 0.372601  [  160/  179]
train() client id: f_00007-5-0 loss: 0.284839  [   32/  179]
train() client id: f_00007-5-1 loss: 0.634070  [   64/  179]
train() client id: f_00007-5-2 loss: 0.565853  [   96/  179]
train() client id: f_00007-5-3 loss: 0.495433  [  128/  179]
train() client id: f_00007-5-4 loss: 0.359666  [  160/  179]
train() client id: f_00007-6-0 loss: 0.474992  [   32/  179]
train() client id: f_00007-6-1 loss: 0.541478  [   64/  179]
train() client id: f_00007-6-2 loss: 0.286038  [   96/  179]
train() client id: f_00007-6-3 loss: 0.431033  [  128/  179]
train() client id: f_00007-6-4 loss: 0.291175  [  160/  179]
train() client id: f_00007-7-0 loss: 0.609393  [   32/  179]
train() client id: f_00007-7-1 loss: 0.323447  [   64/  179]
train() client id: f_00007-7-2 loss: 0.282567  [   96/  179]
train() client id: f_00007-7-3 loss: 0.593438  [  128/  179]
train() client id: f_00007-7-4 loss: 0.366174  [  160/  179]
train() client id: f_00007-8-0 loss: 0.388591  [   32/  179]
train() client id: f_00007-8-1 loss: 0.265121  [   64/  179]
train() client id: f_00007-8-2 loss: 0.411045  [   96/  179]
train() client id: f_00007-8-3 loss: 0.501203  [  128/  179]
train() client id: f_00007-8-4 loss: 0.313806  [  160/  179]
train() client id: f_00007-9-0 loss: 0.551627  [   32/  179]
train() client id: f_00007-9-1 loss: 0.487676  [   64/  179]
train() client id: f_00007-9-2 loss: 0.259111  [   96/  179]
train() client id: f_00007-9-3 loss: 0.368151  [  128/  179]
train() client id: f_00007-9-4 loss: 0.522033  [  160/  179]
train() client id: f_00007-10-0 loss: 0.447054  [   32/  179]
train() client id: f_00007-10-1 loss: 0.297158  [   64/  179]
train() client id: f_00007-10-2 loss: 0.351585  [   96/  179]
train() client id: f_00007-10-3 loss: 0.513245  [  128/  179]
train() client id: f_00007-10-4 loss: 0.491682  [  160/  179]
train() client id: f_00008-0-0 loss: 0.778804  [   32/  130]
train() client id: f_00008-0-1 loss: 0.746604  [   64/  130]
train() client id: f_00008-0-2 loss: 0.742179  [   96/  130]
train() client id: f_00008-0-3 loss: 0.743138  [  128/  130]
train() client id: f_00008-1-0 loss: 0.917020  [   32/  130]
train() client id: f_00008-1-1 loss: 0.705219  [   64/  130]
train() client id: f_00008-1-2 loss: 0.791995  [   96/  130]
train() client id: f_00008-1-3 loss: 0.630357  [  128/  130]
train() client id: f_00008-2-0 loss: 0.775033  [   32/  130]
train() client id: f_00008-2-1 loss: 0.835390  [   64/  130]
train() client id: f_00008-2-2 loss: 0.757158  [   96/  130]
train() client id: f_00008-2-3 loss: 0.675965  [  128/  130]
train() client id: f_00008-3-0 loss: 0.765963  [   32/  130]
train() client id: f_00008-3-1 loss: 0.864146  [   64/  130]
train() client id: f_00008-3-2 loss: 0.678102  [   96/  130]
train() client id: f_00008-3-3 loss: 0.706152  [  128/  130]
train() client id: f_00008-4-0 loss: 0.749445  [   32/  130]
train() client id: f_00008-4-1 loss: 0.765187  [   64/  130]
train() client id: f_00008-4-2 loss: 0.705428  [   96/  130]
train() client id: f_00008-4-3 loss: 0.780674  [  128/  130]
train() client id: f_00008-5-0 loss: 0.815309  [   32/  130]
train() client id: f_00008-5-1 loss: 0.895465  [   64/  130]
train() client id: f_00008-5-2 loss: 0.598043  [   96/  130]
train() client id: f_00008-5-3 loss: 0.708347  [  128/  130]
train() client id: f_00008-6-0 loss: 0.739045  [   32/  130]
train() client id: f_00008-6-1 loss: 0.748372  [   64/  130]
train() client id: f_00008-6-2 loss: 0.671798  [   96/  130]
train() client id: f_00008-6-3 loss: 0.872679  [  128/  130]
train() client id: f_00008-7-0 loss: 0.657182  [   32/  130]
train() client id: f_00008-7-1 loss: 0.896356  [   64/  130]
train() client id: f_00008-7-2 loss: 0.665401  [   96/  130]
train() client id: f_00008-7-3 loss: 0.831181  [  128/  130]
train() client id: f_00008-8-0 loss: 0.722020  [   32/  130]
train() client id: f_00008-8-1 loss: 0.796248  [   64/  130]
train() client id: f_00008-8-2 loss: 0.736681  [   96/  130]
train() client id: f_00008-8-3 loss: 0.787719  [  128/  130]
train() client id: f_00008-9-0 loss: 0.737749  [   32/  130]
train() client id: f_00008-9-1 loss: 0.670786  [   64/  130]
train() client id: f_00008-9-2 loss: 0.759915  [   96/  130]
train() client id: f_00008-9-3 loss: 0.877347  [  128/  130]
train() client id: f_00008-10-0 loss: 0.763387  [   32/  130]
train() client id: f_00008-10-1 loss: 0.743131  [   64/  130]
train() client id: f_00008-10-2 loss: 0.663331  [   96/  130]
train() client id: f_00008-10-3 loss: 0.866882  [  128/  130]
train() client id: f_00009-0-0 loss: 1.129981  [   32/  118]
train() client id: f_00009-0-1 loss: 0.913747  [   64/  118]
train() client id: f_00009-0-2 loss: 1.194885  [   96/  118]
train() client id: f_00009-1-0 loss: 0.972617  [   32/  118]
train() client id: f_00009-1-1 loss: 1.128728  [   64/  118]
train() client id: f_00009-1-2 loss: 0.991915  [   96/  118]
train() client id: f_00009-2-0 loss: 1.010091  [   32/  118]
train() client id: f_00009-2-1 loss: 1.043836  [   64/  118]
train() client id: f_00009-2-2 loss: 0.974689  [   96/  118]
train() client id: f_00009-3-0 loss: 0.936981  [   32/  118]
train() client id: f_00009-3-1 loss: 0.966962  [   64/  118]
train() client id: f_00009-3-2 loss: 1.004220  [   96/  118]
train() client id: f_00009-4-0 loss: 0.871764  [   32/  118]
train() client id: f_00009-4-1 loss: 0.954263  [   64/  118]
train() client id: f_00009-4-2 loss: 1.050078  [   96/  118]
train() client id: f_00009-5-0 loss: 0.897615  [   32/  118]
train() client id: f_00009-5-1 loss: 0.976561  [   64/  118]
train() client id: f_00009-5-2 loss: 0.839300  [   96/  118]
train() client id: f_00009-6-0 loss: 0.799039  [   32/  118]
train() client id: f_00009-6-1 loss: 1.072481  [   64/  118]
train() client id: f_00009-6-2 loss: 0.863897  [   96/  118]
train() client id: f_00009-7-0 loss: 0.881892  [   32/  118]
train() client id: f_00009-7-1 loss: 0.915064  [   64/  118]
train() client id: f_00009-7-2 loss: 0.821098  [   96/  118]
train() client id: f_00009-8-0 loss: 0.774842  [   32/  118]
train() client id: f_00009-8-1 loss: 0.944196  [   64/  118]
train() client id: f_00009-8-2 loss: 0.956719  [   96/  118]
train() client id: f_00009-9-0 loss: 0.732126  [   32/  118]
train() client id: f_00009-9-1 loss: 1.072711  [   64/  118]
train() client id: f_00009-9-2 loss: 0.886101  [   96/  118]
train() client id: f_00009-10-0 loss: 0.754390  [   32/  118]
train() client id: f_00009-10-1 loss: 0.922678  [   64/  118]
train() client id: f_00009-10-2 loss: 1.033513  [   96/  118]
At round 52 accuracy: 0.6472148541114059
At round 52 training accuracy: 0.5969148222669349
At round 52 training loss: 0.8273484754437119
update_location
xs = [  -3.9056584     4.20031788  280.00902392   18.81129433    0.97929623
    3.95640986 -242.44319194 -221.32485185  264.66397685 -207.06087855]
ys = [ 272.5879595   255.55583871    1.32061395 -242.45517586  234.35018685
  217.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [290.37811527 274.45660744 297.33280595 262.94177511 254.79593618
 239.70535547 262.27007427 242.86903117 283.47079323 229.97873659]
dists_bs = [196.96873753 196.4739094  487.0295073  460.25171554 185.71795115
 184.00665242 189.99794523 180.23275607 466.99992549 173.97813991]
uav_gains = [3.23107212e-12 4.49127941e-12 2.81429478e-12 5.71301961e-12
 6.74655076e-12 9.01929979e-12 5.79291830e-12 8.50621989e-12
 3.72132531e-12 1.07099613e-11]
bs_gains = [4.15893030e-11 4.18832527e-11 3.29715465e-12 3.86284178e-12
 4.90346648e-11 5.03222692e-11 4.60041651e-11 5.33285356e-11
 3.70857455e-12 5.88720247e-11]
Round 53
-------------------------------
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.00899084 8.17097489 3.95724982 1.44293614 9.42099912 4.53288609
 1.77835404 5.58181324 4.12270253 3.67526442]
obj_prev = 46.692171130454035
eta_min = 9.633908147429921e-24	eta_max = 0.9382560279743308
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 10.770764989587082	eta = 0.9090909090909091
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 22.73132098521351	eta = 0.4307538722609915
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 16.477782321424517	eta = 0.5942307250446651
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.352697592199675	eta = 0.6377774640049659
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.286662235887265	eta = 0.6405325364618376
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.286409541549396	eta = 0.6405431248831895
eta = 0.6405431248831895
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [0.03675654 0.07730543 0.03617311 0.0125439  0.08926589 0.04259092
 0.0157528  0.05221763 0.03792342 0.0344228 ]
ene_total = [1.46579906 2.36500809 1.47528965 0.70621205 2.69190422 1.38875393
 0.79404133 1.76977105 1.47499189 1.15463827]
ti_comp = [0.82252146 0.90567545 0.81328504 0.85268029 0.90809929 0.90848315
 0.85328562 0.86816903 0.8308949  0.91072419]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [4.58764259e-06 3.52017978e-05 4.47251598e-06 1.69670107e-07
 5.39100853e-05 5.85055712e-06 3.35555868e-07 1.18065477e-05
 4.93753617e-06 3.07358121e-06]
ene_total = [0.44053948 0.20942103 0.46630285 0.35628294 0.2031812  0.20076965
 0.35459888 0.31339904 0.41719008 0.19444042]
optimize_network_iter = 0 obj = 3.1561255646288915
eta = 0.6405431248831895
freqs = [22343821.0822694  42678329.73940749 22238886.46292163  7355570.59611931
 49149853.28596357 23440674.99681275  9230675.16871088 30073423.9891545
 22820828.02431027 18898584.29975834]
eta_min = 0.6405431248831939	eta_max = 0.6991595698398908
af = 0.002799252834533405	bf = 1.1332441370306174	zeta = 0.0030791781179867455	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [9.81614110e-07 7.53209970e-06 9.56980564e-07 3.63041731e-08
 1.15350966e-05 1.25183889e-06 7.17986128e-08 2.52623730e-06
 1.05648055e-06 6.57651642e-07]
ene_total = [1.74150662 0.82500401 1.84338565 1.40873715 0.79870952 0.79334107
 1.40206398 1.23816416 1.64915218 0.76855593]
ti_comp = [0.6626492  0.74580319 0.65341278 0.69280803 0.74822703 0.7486109
 0.69341336 0.70829677 0.67102264 0.75085193]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.66565189e-06 2.69213035e-05 3.59332702e-06 1.33286597e-07
 4.11817149e-05 4.46841209e-06 2.63514067e-07 9.19889168e-06
 3.92611172e-06 2.34500816e-06]
ene_total = [0.52634448 0.2499491  0.55712907 0.42570072 0.24234523 0.239842
 0.42368735 0.37437551 0.49844265 0.23230136]
optimize_network_iter = 1 obj = 3.770117456851034
eta = 0.6991595698398908
freqs = [22282599.06518497 41639030.99946182 22238886.46292161  7273350.56555898
 47925539.38367743 22854708.69293805  9126001.3904321  29615341.56378339
 22703099.87437893 18416482.47146711]
eta_min = 0.6991595698399091	eta_max = 0.6991595698398846
af = 0.002681430697799304	bf = 1.1332441370306174	zeta = 0.0029495737675792346	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [9.76242239e-07 7.16972436e-06 9.56980564e-07 3.54970983e-08
 1.09675798e-05 1.19003461e-06 7.01794852e-08 2.44986346e-06
 1.04560832e-06 6.24526300e-07]
ene_total = [1.74150603 0.82496404 1.84338565 1.40873707 0.79864692 0.79333425
 1.4020638  1.23815573 1.64915098 0.76855228]
ti_comp = [0.6626492  0.74580319 0.65341278 0.69280803 0.74822703 0.7486109
 0.69341336 0.70829677 0.67102264 0.75085193]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.66565189e-06 2.69213035e-05 3.59332702e-06 1.33286597e-07
 4.11817149e-05 4.46841209e-06 2.63514067e-07 9.19889168e-06
 3.92611172e-06 2.34500816e-06]
ene_total = [0.52634448 0.2499491  0.55712907 0.42570072 0.24234523 0.239842
 0.42368735 0.37437551 0.49844265 0.23230136]
optimize_network_iter = 2 obj = 3.770117456850956
eta = 0.6991595698398846
freqs = [22282599.06518495 41639030.99946193 22238886.46292159  7273350.56555898
 47925539.38367755 22854708.6929381   9126001.39043211 29615341.56378343
 22703099.87437893 18416482.47146716]
Done!
At round 53 eta: 0.6991595698398846
At round 53 local rounds: 11.718687629159392
At round 53 global rounds: 33.33219860622975
At round 53 a_n: 9.685127119911243
gradient difference: 0.46595779061317444
train() client id: f_00000-0-0 loss: 0.997630  [   32/  126]
train() client id: f_00000-0-1 loss: 1.041551  [   64/  126]
train() client id: f_00000-0-2 loss: 1.196490  [   96/  126]
train() client id: f_00000-1-0 loss: 1.039496  [   32/  126]
train() client id: f_00000-1-1 loss: 0.993230  [   64/  126]
train() client id: f_00000-1-2 loss: 0.859344  [   96/  126]
train() client id: f_00000-2-0 loss: 0.973496  [   32/  126]
train() client id: f_00000-2-1 loss: 0.946850  [   64/  126]
train() client id: f_00000-2-2 loss: 0.913419  [   96/  126]
train() client id: f_00000-3-0 loss: 0.774843  [   32/  126]
train() client id: f_00000-3-1 loss: 1.012950  [   64/  126]
train() client id: f_00000-3-2 loss: 0.878124  [   96/  126]
train() client id: f_00000-4-0 loss: 0.899696  [   32/  126]
train() client id: f_00000-4-1 loss: 0.793883  [   64/  126]
train() client id: f_00000-4-2 loss: 0.910069  [   96/  126]
train() client id: f_00000-5-0 loss: 0.819250  [   32/  126]
train() client id: f_00000-5-1 loss: 0.868309  [   64/  126]
train() client id: f_00000-5-2 loss: 0.895273  [   96/  126]
train() client id: f_00000-6-0 loss: 0.799113  [   32/  126]
train() client id: f_00000-6-1 loss: 1.051497  [   64/  126]
train() client id: f_00000-6-2 loss: 0.821193  [   96/  126]
train() client id: f_00000-7-0 loss: 0.834015  [   32/  126]
train() client id: f_00000-7-1 loss: 0.913196  [   64/  126]
train() client id: f_00000-7-2 loss: 0.879370  [   96/  126]
train() client id: f_00000-8-0 loss: 0.905112  [   32/  126]
train() client id: f_00000-8-1 loss: 0.801525  [   64/  126]
train() client id: f_00000-8-2 loss: 0.870435  [   96/  126]
train() client id: f_00000-9-0 loss: 0.925035  [   32/  126]
train() client id: f_00000-9-1 loss: 0.842429  [   64/  126]
train() client id: f_00000-9-2 loss: 0.800838  [   96/  126]
train() client id: f_00000-10-0 loss: 0.878613  [   32/  126]
train() client id: f_00000-10-1 loss: 0.882323  [   64/  126]
train() client id: f_00000-10-2 loss: 0.802440  [   96/  126]
train() client id: f_00001-0-0 loss: 0.416039  [   32/  265]
train() client id: f_00001-0-1 loss: 0.440870  [   64/  265]
train() client id: f_00001-0-2 loss: 0.386362  [   96/  265]
train() client id: f_00001-0-3 loss: 0.390567  [  128/  265]
train() client id: f_00001-0-4 loss: 0.404520  [  160/  265]
train() client id: f_00001-0-5 loss: 0.489795  [  192/  265]
train() client id: f_00001-0-6 loss: 0.510405  [  224/  265]
train() client id: f_00001-0-7 loss: 0.451410  [  256/  265]
train() client id: f_00001-1-0 loss: 0.473115  [   32/  265]
train() client id: f_00001-1-1 loss: 0.410542  [   64/  265]
train() client id: f_00001-1-2 loss: 0.332574  [   96/  265]
train() client id: f_00001-1-3 loss: 0.408468  [  128/  265]
train() client id: f_00001-1-4 loss: 0.529681  [  160/  265]
train() client id: f_00001-1-5 loss: 0.397851  [  192/  265]
train() client id: f_00001-1-6 loss: 0.452181  [  224/  265]
train() client id: f_00001-1-7 loss: 0.481560  [  256/  265]
train() client id: f_00001-2-0 loss: 0.350299  [   32/  265]
train() client id: f_00001-2-1 loss: 0.412966  [   64/  265]
train() client id: f_00001-2-2 loss: 0.597407  [   96/  265]
train() client id: f_00001-2-3 loss: 0.401665  [  128/  265]
train() client id: f_00001-2-4 loss: 0.395717  [  160/  265]
train() client id: f_00001-2-5 loss: 0.348489  [  192/  265]
train() client id: f_00001-2-6 loss: 0.444613  [  224/  265]
train() client id: f_00001-2-7 loss: 0.395652  [  256/  265]
train() client id: f_00001-3-0 loss: 0.352572  [   32/  265]
train() client id: f_00001-3-1 loss: 0.435126  [   64/  265]
train() client id: f_00001-3-2 loss: 0.471065  [   96/  265]
train() client id: f_00001-3-3 loss: 0.476260  [  128/  265]
train() client id: f_00001-3-4 loss: 0.334788  [  160/  265]
train() client id: f_00001-3-5 loss: 0.459240  [  192/  265]
train() client id: f_00001-3-6 loss: 0.377776  [  224/  265]
train() client id: f_00001-3-7 loss: 0.351847  [  256/  265]
train() client id: f_00001-4-0 loss: 0.375756  [   32/  265]
train() client id: f_00001-4-1 loss: 0.420842  [   64/  265]
train() client id: f_00001-4-2 loss: 0.335389  [   96/  265]
train() client id: f_00001-4-3 loss: 0.366353  [  128/  265]
train() client id: f_00001-4-4 loss: 0.378686  [  160/  265]
train() client id: f_00001-4-5 loss: 0.550938  [  192/  265]
train() client id: f_00001-4-6 loss: 0.356232  [  224/  265]
train() client id: f_00001-4-7 loss: 0.621585  [  256/  265]
train() client id: f_00001-5-0 loss: 0.465429  [   32/  265]
train() client id: f_00001-5-1 loss: 0.393619  [   64/  265]
train() client id: f_00001-5-2 loss: 0.535868  [   96/  265]
train() client id: f_00001-5-3 loss: 0.381097  [  128/  265]
train() client id: f_00001-5-4 loss: 0.354913  [  160/  265]
train() client id: f_00001-5-5 loss: 0.422282  [  192/  265]
train() client id: f_00001-5-6 loss: 0.519710  [  224/  265]
train() client id: f_00001-5-7 loss: 0.324438  [  256/  265]
train() client id: f_00001-6-0 loss: 0.459345  [   32/  265]
train() client id: f_00001-6-1 loss: 0.554700  [   64/  265]
train() client id: f_00001-6-2 loss: 0.427523  [   96/  265]
train() client id: f_00001-6-3 loss: 0.328435  [  128/  265]
train() client id: f_00001-6-4 loss: 0.357967  [  160/  265]
train() client id: f_00001-6-5 loss: 0.463508  [  192/  265]
train() client id: f_00001-6-6 loss: 0.426546  [  224/  265]
train() client id: f_00001-6-7 loss: 0.379887  [  256/  265]
train() client id: f_00001-7-0 loss: 0.341729  [   32/  265]
train() client id: f_00001-7-1 loss: 0.400541  [   64/  265]
train() client id: f_00001-7-2 loss: 0.343056  [   96/  265]
train() client id: f_00001-7-3 loss: 0.440600  [  128/  265]
train() client id: f_00001-7-4 loss: 0.353810  [  160/  265]
train() client id: f_00001-7-5 loss: 0.458388  [  192/  265]
train() client id: f_00001-7-6 loss: 0.472066  [  224/  265]
train() client id: f_00001-7-7 loss: 0.555064  [  256/  265]
train() client id: f_00001-8-0 loss: 0.333289  [   32/  265]
train() client id: f_00001-8-1 loss: 0.405397  [   64/  265]
train() client id: f_00001-8-2 loss: 0.322321  [   96/  265]
train() client id: f_00001-8-3 loss: 0.545489  [  128/  265]
train() client id: f_00001-8-4 loss: 0.575066  [  160/  265]
train() client id: f_00001-8-5 loss: 0.325060  [  192/  265]
train() client id: f_00001-8-6 loss: 0.394215  [  224/  265]
train() client id: f_00001-8-7 loss: 0.482820  [  256/  265]
train() client id: f_00001-9-0 loss: 0.447280  [   32/  265]
train() client id: f_00001-9-1 loss: 0.575638  [   64/  265]
train() client id: f_00001-9-2 loss: 0.398066  [   96/  265]
train() client id: f_00001-9-3 loss: 0.337846  [  128/  265]
train() client id: f_00001-9-4 loss: 0.390704  [  160/  265]
train() client id: f_00001-9-5 loss: 0.328913  [  192/  265]
train() client id: f_00001-9-6 loss: 0.330146  [  224/  265]
train() client id: f_00001-9-7 loss: 0.519664  [  256/  265]
train() client id: f_00001-10-0 loss: 0.323863  [   32/  265]
train() client id: f_00001-10-1 loss: 0.481918  [   64/  265]
train() client id: f_00001-10-2 loss: 0.338167  [   96/  265]
train() client id: f_00001-10-3 loss: 0.520907  [  128/  265]
train() client id: f_00001-10-4 loss: 0.505199  [  160/  265]
train() client id: f_00001-10-5 loss: 0.439050  [  192/  265]
train() client id: f_00001-10-6 loss: 0.451893  [  224/  265]
train() client id: f_00001-10-7 loss: 0.352391  [  256/  265]
train() client id: f_00002-0-0 loss: 1.033716  [   32/  124]
train() client id: f_00002-0-1 loss: 0.787953  [   64/  124]
train() client id: f_00002-0-2 loss: 1.004741  [   96/  124]
train() client id: f_00002-1-0 loss: 0.870118  [   32/  124]
train() client id: f_00002-1-1 loss: 0.934649  [   64/  124]
train() client id: f_00002-1-2 loss: 0.841809  [   96/  124]
train() client id: f_00002-2-0 loss: 0.938825  [   32/  124]
train() client id: f_00002-2-1 loss: 0.785294  [   64/  124]
train() client id: f_00002-2-2 loss: 0.738766  [   96/  124]
train() client id: f_00002-3-0 loss: 0.882285  [   32/  124]
train() client id: f_00002-3-1 loss: 0.881252  [   64/  124]
train() client id: f_00002-3-2 loss: 0.751155  [   96/  124]
train() client id: f_00002-4-0 loss: 0.799631  [   32/  124]
train() client id: f_00002-4-1 loss: 0.810160  [   64/  124]
train() client id: f_00002-4-2 loss: 0.807658  [   96/  124]
train() client id: f_00002-5-0 loss: 0.978501  [   32/  124]
train() client id: f_00002-5-1 loss: 0.712408  [   64/  124]
train() client id: f_00002-5-2 loss: 0.735911  [   96/  124]
train() client id: f_00002-6-0 loss: 0.728711  [   32/  124]
train() client id: f_00002-6-1 loss: 0.770329  [   64/  124]
train() client id: f_00002-6-2 loss: 0.914950  [   96/  124]
train() client id: f_00002-7-0 loss: 0.653485  [   32/  124]
train() client id: f_00002-7-1 loss: 0.690453  [   64/  124]
train() client id: f_00002-7-2 loss: 0.863552  [   96/  124]
train() client id: f_00002-8-0 loss: 0.604590  [   32/  124]
train() client id: f_00002-8-1 loss: 0.873951  [   64/  124]
train() client id: f_00002-8-2 loss: 0.723787  [   96/  124]
train() client id: f_00002-9-0 loss: 0.920194  [   32/  124]
train() client id: f_00002-9-1 loss: 0.605997  [   64/  124]
train() client id: f_00002-9-2 loss: 0.751558  [   96/  124]
train() client id: f_00002-10-0 loss: 0.781521  [   32/  124]
train() client id: f_00002-10-1 loss: 0.711052  [   64/  124]
train() client id: f_00002-10-2 loss: 0.789072  [   96/  124]
train() client id: f_00003-0-0 loss: 0.767851  [   32/   43]
train() client id: f_00003-1-0 loss: 0.655025  [   32/   43]
train() client id: f_00003-2-0 loss: 0.884411  [   32/   43]
train() client id: f_00003-3-0 loss: 0.696366  [   32/   43]
train() client id: f_00003-4-0 loss: 0.638915  [   32/   43]
train() client id: f_00003-5-0 loss: 0.852816  [   32/   43]
train() client id: f_00003-6-0 loss: 1.001179  [   32/   43]
train() client id: f_00003-7-0 loss: 0.656822  [   32/   43]
train() client id: f_00003-8-0 loss: 0.856879  [   32/   43]
train() client id: f_00003-9-0 loss: 0.904828  [   32/   43]
train() client id: f_00003-10-0 loss: 0.701270  [   32/   43]
train() client id: f_00004-0-0 loss: 0.840963  [   32/  306]
train() client id: f_00004-0-1 loss: 0.870956  [   64/  306]
train() client id: f_00004-0-2 loss: 0.870216  [   96/  306]
train() client id: f_00004-0-3 loss: 0.778079  [  128/  306]
train() client id: f_00004-0-4 loss: 0.619861  [  160/  306]
train() client id: f_00004-0-5 loss: 0.749777  [  192/  306]
train() client id: f_00004-0-6 loss: 0.800878  [  224/  306]
train() client id: f_00004-0-7 loss: 0.813121  [  256/  306]
train() client id: f_00004-0-8 loss: 0.742052  [  288/  306]
train() client id: f_00004-1-0 loss: 0.653868  [   32/  306]
train() client id: f_00004-1-1 loss: 0.831555  [   64/  306]
train() client id: f_00004-1-2 loss: 0.835003  [   96/  306]
train() client id: f_00004-1-3 loss: 0.832458  [  128/  306]
train() client id: f_00004-1-4 loss: 0.797256  [  160/  306]
train() client id: f_00004-1-5 loss: 0.760660  [  192/  306]
train() client id: f_00004-1-6 loss: 0.814070  [  224/  306]
train() client id: f_00004-1-7 loss: 0.716858  [  256/  306]
train() client id: f_00004-1-8 loss: 0.839587  [  288/  306]
train() client id: f_00004-2-0 loss: 0.774455  [   32/  306]
train() client id: f_00004-2-1 loss: 0.760871  [   64/  306]
train() client id: f_00004-2-2 loss: 0.886373  [   96/  306]
train() client id: f_00004-2-3 loss: 0.812379  [  128/  306]
train() client id: f_00004-2-4 loss: 0.798087  [  160/  306]
train() client id: f_00004-2-5 loss: 0.826650  [  192/  306]
train() client id: f_00004-2-6 loss: 0.828516  [  224/  306]
train() client id: f_00004-2-7 loss: 0.785857  [  256/  306]
train() client id: f_00004-2-8 loss: 0.769764  [  288/  306]
train() client id: f_00004-3-0 loss: 0.731667  [   32/  306]
train() client id: f_00004-3-1 loss: 0.723478  [   64/  306]
train() client id: f_00004-3-2 loss: 0.814587  [   96/  306]
train() client id: f_00004-3-3 loss: 0.888236  [  128/  306]
train() client id: f_00004-3-4 loss: 0.702257  [  160/  306]
train() client id: f_00004-3-5 loss: 0.840135  [  192/  306]
train() client id: f_00004-3-6 loss: 0.848042  [  224/  306]
train() client id: f_00004-3-7 loss: 0.737607  [  256/  306]
train() client id: f_00004-3-8 loss: 0.799316  [  288/  306]
train() client id: f_00004-4-0 loss: 0.851125  [   32/  306]
train() client id: f_00004-4-1 loss: 0.816589  [   64/  306]
train() client id: f_00004-4-2 loss: 0.773421  [   96/  306]
train() client id: f_00004-4-3 loss: 0.903512  [  128/  306]
train() client id: f_00004-4-4 loss: 0.844833  [  160/  306]
train() client id: f_00004-4-5 loss: 0.804606  [  192/  306]
train() client id: f_00004-4-6 loss: 0.830658  [  224/  306]
train() client id: f_00004-4-7 loss: 0.705077  [  256/  306]
train() client id: f_00004-4-8 loss: 0.699804  [  288/  306]
train() client id: f_00004-5-0 loss: 0.770796  [   32/  306]
train() client id: f_00004-5-1 loss: 0.968264  [   64/  306]
train() client id: f_00004-5-2 loss: 0.849101  [   96/  306]
train() client id: f_00004-5-3 loss: 0.760732  [  128/  306]
train() client id: f_00004-5-4 loss: 0.781833  [  160/  306]
train() client id: f_00004-5-5 loss: 0.832376  [  192/  306]
train() client id: f_00004-5-6 loss: 0.626249  [  224/  306]
train() client id: f_00004-5-7 loss: 0.680129  [  256/  306]
train() client id: f_00004-5-8 loss: 0.921875  [  288/  306]
train() client id: f_00004-6-0 loss: 0.765447  [   32/  306]
train() client id: f_00004-6-1 loss: 0.870712  [   64/  306]
train() client id: f_00004-6-2 loss: 0.805067  [   96/  306]
train() client id: f_00004-6-3 loss: 0.778574  [  128/  306]
train() client id: f_00004-6-4 loss: 0.812287  [  160/  306]
train() client id: f_00004-6-5 loss: 0.792269  [  192/  306]
train() client id: f_00004-6-6 loss: 0.707798  [  224/  306]
train() client id: f_00004-6-7 loss: 0.822298  [  256/  306]
train() client id: f_00004-6-8 loss: 0.714819  [  288/  306]
train() client id: f_00004-7-0 loss: 0.932939  [   32/  306]
train() client id: f_00004-7-1 loss: 0.686966  [   64/  306]
train() client id: f_00004-7-2 loss: 0.880922  [   96/  306]
train() client id: f_00004-7-3 loss: 0.789755  [  128/  306]
train() client id: f_00004-7-4 loss: 0.717359  [  160/  306]
train() client id: f_00004-7-5 loss: 0.770736  [  192/  306]
train() client id: f_00004-7-6 loss: 0.758601  [  224/  306]
train() client id: f_00004-7-7 loss: 0.916789  [  256/  306]
train() client id: f_00004-7-8 loss: 0.688074  [  288/  306]
train() client id: f_00004-8-0 loss: 0.841731  [   32/  306]
train() client id: f_00004-8-1 loss: 0.754933  [   64/  306]
train() client id: f_00004-8-2 loss: 0.982506  [   96/  306]
train() client id: f_00004-8-3 loss: 0.730491  [  128/  306]
train() client id: f_00004-8-4 loss: 0.713862  [  160/  306]
train() client id: f_00004-8-5 loss: 0.717688  [  192/  306]
train() client id: f_00004-8-6 loss: 0.844979  [  224/  306]
train() client id: f_00004-8-7 loss: 0.842322  [  256/  306]
train() client id: f_00004-8-8 loss: 0.761255  [  288/  306]
train() client id: f_00004-9-0 loss: 0.762584  [   32/  306]
train() client id: f_00004-9-1 loss: 0.857912  [   64/  306]
train() client id: f_00004-9-2 loss: 0.794003  [   96/  306]
train() client id: f_00004-9-3 loss: 0.869711  [  128/  306]
train() client id: f_00004-9-4 loss: 0.743843  [  160/  306]
train() client id: f_00004-9-5 loss: 0.876777  [  192/  306]
train() client id: f_00004-9-6 loss: 0.733335  [  224/  306]
train() client id: f_00004-9-7 loss: 0.763627  [  256/  306]
train() client id: f_00004-9-8 loss: 0.802456  [  288/  306]
train() client id: f_00004-10-0 loss: 0.833600  [   32/  306]
train() client id: f_00004-10-1 loss: 0.721485  [   64/  306]
train() client id: f_00004-10-2 loss: 0.772313  [   96/  306]
train() client id: f_00004-10-3 loss: 0.774758  [  128/  306]
train() client id: f_00004-10-4 loss: 0.861218  [  160/  306]
train() client id: f_00004-10-5 loss: 0.743510  [  192/  306]
train() client id: f_00004-10-6 loss: 0.820188  [  224/  306]
train() client id: f_00004-10-7 loss: 0.850870  [  256/  306]
train() client id: f_00004-10-8 loss: 0.856344  [  288/  306]
train() client id: f_00005-0-0 loss: 0.727978  [   32/  146]
train() client id: f_00005-0-1 loss: 0.330673  [   64/  146]
train() client id: f_00005-0-2 loss: 0.665457  [   96/  146]
train() client id: f_00005-0-3 loss: 0.901516  [  128/  146]
train() client id: f_00005-1-0 loss: 0.527071  [   32/  146]
train() client id: f_00005-1-1 loss: 0.615706  [   64/  146]
train() client id: f_00005-1-2 loss: 0.739872  [   96/  146]
train() client id: f_00005-1-3 loss: 0.593643  [  128/  146]
train() client id: f_00005-2-0 loss: 0.347956  [   32/  146]
train() client id: f_00005-2-1 loss: 0.726946  [   64/  146]
train() client id: f_00005-2-2 loss: 0.607698  [   96/  146]
train() client id: f_00005-2-3 loss: 0.570617  [  128/  146]
train() client id: f_00005-3-0 loss: 0.765183  [   32/  146]
train() client id: f_00005-3-1 loss: 0.509627  [   64/  146]
train() client id: f_00005-3-2 loss: 0.746264  [   96/  146]
train() client id: f_00005-3-3 loss: 0.480494  [  128/  146]
train() client id: f_00005-4-0 loss: 0.549959  [   32/  146]
train() client id: f_00005-4-1 loss: 0.877188  [   64/  146]
train() client id: f_00005-4-2 loss: 0.811682  [   96/  146]
train() client id: f_00005-4-3 loss: 0.333739  [  128/  146]
train() client id: f_00005-5-0 loss: 0.905497  [   32/  146]
train() client id: f_00005-5-1 loss: 0.638800  [   64/  146]
train() client id: f_00005-5-2 loss: 0.609703  [   96/  146]
train() client id: f_00005-5-3 loss: 0.370775  [  128/  146]
train() client id: f_00005-6-0 loss: 0.662810  [   32/  146]
train() client id: f_00005-6-1 loss: 0.620803  [   64/  146]
train() client id: f_00005-6-2 loss: 0.692307  [   96/  146]
train() client id: f_00005-6-3 loss: 0.702314  [  128/  146]
train() client id: f_00005-7-0 loss: 0.671580  [   32/  146]
train() client id: f_00005-7-1 loss: 0.623376  [   64/  146]
train() client id: f_00005-7-2 loss: 0.521878  [   96/  146]
train() client id: f_00005-7-3 loss: 0.723028  [  128/  146]
train() client id: f_00005-8-0 loss: 0.624960  [   32/  146]
train() client id: f_00005-8-1 loss: 0.325932  [   64/  146]
train() client id: f_00005-8-2 loss: 0.728356  [   96/  146]
train() client id: f_00005-8-3 loss: 0.828212  [  128/  146]
train() client id: f_00005-9-0 loss: 0.535154  [   32/  146]
train() client id: f_00005-9-1 loss: 0.901066  [   64/  146]
train() client id: f_00005-9-2 loss: 0.611951  [   96/  146]
train() client id: f_00005-9-3 loss: 0.510671  [  128/  146]
train() client id: f_00005-10-0 loss: 0.440884  [   32/  146]
train() client id: f_00005-10-1 loss: 0.574773  [   64/  146]
train() client id: f_00005-10-2 loss: 0.563864  [   96/  146]
train() client id: f_00005-10-3 loss: 0.858021  [  128/  146]
train() client id: f_00006-0-0 loss: 0.417075  [   32/   54]
train() client id: f_00006-1-0 loss: 0.502714  [   32/   54]
train() client id: f_00006-2-0 loss: 0.470371  [   32/   54]
train() client id: f_00006-3-0 loss: 0.405532  [   32/   54]
train() client id: f_00006-4-0 loss: 0.469744  [   32/   54]
train() client id: f_00006-5-0 loss: 0.516028  [   32/   54]
train() client id: f_00006-6-0 loss: 0.511987  [   32/   54]
train() client id: f_00006-7-0 loss: 0.515878  [   32/   54]
train() client id: f_00006-8-0 loss: 0.492752  [   32/   54]
train() client id: f_00006-9-0 loss: 0.509026  [   32/   54]
train() client id: f_00006-10-0 loss: 0.445842  [   32/   54]
train() client id: f_00007-0-0 loss: 0.701452  [   32/  179]
train() client id: f_00007-0-1 loss: 0.500465  [   64/  179]
train() client id: f_00007-0-2 loss: 0.512783  [   96/  179]
train() client id: f_00007-0-3 loss: 0.622867  [  128/  179]
train() client id: f_00007-0-4 loss: 0.527965  [  160/  179]
train() client id: f_00007-1-0 loss: 0.504868  [   32/  179]
train() client id: f_00007-1-1 loss: 0.690463  [   64/  179]
train() client id: f_00007-1-2 loss: 0.551896  [   96/  179]
train() client id: f_00007-1-3 loss: 0.357504  [  128/  179]
train() client id: f_00007-1-4 loss: 0.632718  [  160/  179]
train() client id: f_00007-2-0 loss: 0.351938  [   32/  179]
train() client id: f_00007-2-1 loss: 0.534351  [   64/  179]
train() client id: f_00007-2-2 loss: 0.570832  [   96/  179]
train() client id: f_00007-2-3 loss: 0.810700  [  128/  179]
train() client id: f_00007-2-4 loss: 0.404236  [  160/  179]
train() client id: f_00007-3-0 loss: 0.473520  [   32/  179]
train() client id: f_00007-3-1 loss: 0.584474  [   64/  179]
train() client id: f_00007-3-2 loss: 0.536750  [   96/  179]
train() client id: f_00007-3-3 loss: 0.538883  [  128/  179]
train() client id: f_00007-3-4 loss: 0.458658  [  160/  179]
train() client id: f_00007-4-0 loss: 0.535522  [   32/  179]
train() client id: f_00007-4-1 loss: 0.649834  [   64/  179]
train() client id: f_00007-4-2 loss: 0.601548  [   96/  179]
train() client id: f_00007-4-3 loss: 0.578063  [  128/  179]
train() client id: f_00007-4-4 loss: 0.325267  [  160/  179]
train() client id: f_00007-5-0 loss: 0.566030  [   32/  179]
train() client id: f_00007-5-1 loss: 0.416656  [   64/  179]
train() client id: f_00007-5-2 loss: 0.553087  [   96/  179]
train() client id: f_00007-5-3 loss: 0.632951  [  128/  179]
train() client id: f_00007-5-4 loss: 0.458486  [  160/  179]
train() client id: f_00007-6-0 loss: 0.375085  [   32/  179]
train() client id: f_00007-6-1 loss: 0.543509  [   64/  179]
train() client id: f_00007-6-2 loss: 0.485132  [   96/  179]
train() client id: f_00007-6-3 loss: 0.512454  [  128/  179]
train() client id: f_00007-6-4 loss: 0.584741  [  160/  179]
train() client id: f_00007-7-0 loss: 0.649369  [   32/  179]
train() client id: f_00007-7-1 loss: 0.567115  [   64/  179]
train() client id: f_00007-7-2 loss: 0.560997  [   96/  179]
train() client id: f_00007-7-3 loss: 0.490340  [  128/  179]
train() client id: f_00007-7-4 loss: 0.341700  [  160/  179]
train() client id: f_00007-8-0 loss: 0.634012  [   32/  179]
train() client id: f_00007-8-1 loss: 0.357782  [   64/  179]
train() client id: f_00007-8-2 loss: 0.585706  [   96/  179]
train() client id: f_00007-8-3 loss: 0.544928  [  128/  179]
train() client id: f_00007-8-4 loss: 0.430881  [  160/  179]
train() client id: f_00007-9-0 loss: 0.630781  [   32/  179]
train() client id: f_00007-9-1 loss: 0.415381  [   64/  179]
train() client id: f_00007-9-2 loss: 0.344161  [   96/  179]
train() client id: f_00007-9-3 loss: 0.627337  [  128/  179]
train() client id: f_00007-9-4 loss: 0.468472  [  160/  179]
train() client id: f_00007-10-0 loss: 0.635857  [   32/  179]
train() client id: f_00007-10-1 loss: 0.431698  [   64/  179]
train() client id: f_00007-10-2 loss: 0.543050  [   96/  179]
train() client id: f_00007-10-3 loss: 0.559536  [  128/  179]
train() client id: f_00007-10-4 loss: 0.407160  [  160/  179]
train() client id: f_00008-0-0 loss: 0.697394  [   32/  130]
train() client id: f_00008-0-1 loss: 0.770072  [   64/  130]
train() client id: f_00008-0-2 loss: 0.526843  [   96/  130]
train() client id: f_00008-0-3 loss: 0.676684  [  128/  130]
train() client id: f_00008-1-0 loss: 0.751448  [   32/  130]
train() client id: f_00008-1-1 loss: 0.532856  [   64/  130]
train() client id: f_00008-1-2 loss: 0.644058  [   96/  130]
train() client id: f_00008-1-3 loss: 0.730309  [  128/  130]
train() client id: f_00008-2-0 loss: 0.726122  [   32/  130]
train() client id: f_00008-2-1 loss: 0.648467  [   64/  130]
train() client id: f_00008-2-2 loss: 0.670770  [   96/  130]
train() client id: f_00008-2-3 loss: 0.619162  [  128/  130]
train() client id: f_00008-3-0 loss: 0.651810  [   32/  130]
train() client id: f_00008-3-1 loss: 0.685114  [   64/  130]
train() client id: f_00008-3-2 loss: 0.638573  [   96/  130]
train() client id: f_00008-3-3 loss: 0.691272  [  128/  130]
train() client id: f_00008-4-0 loss: 0.608204  [   32/  130]
train() client id: f_00008-4-1 loss: 0.757226  [   64/  130]
train() client id: f_00008-4-2 loss: 0.616688  [   96/  130]
train() client id: f_00008-4-3 loss: 0.681097  [  128/  130]
train() client id: f_00008-5-0 loss: 0.593919  [   32/  130]
train() client id: f_00008-5-1 loss: 0.601544  [   64/  130]
train() client id: f_00008-5-2 loss: 0.718172  [   96/  130]
train() client id: f_00008-5-3 loss: 0.693098  [  128/  130]
train() client id: f_00008-6-0 loss: 0.653120  [   32/  130]
train() client id: f_00008-6-1 loss: 0.730663  [   64/  130]
train() client id: f_00008-6-2 loss: 0.742538  [   96/  130]
train() client id: f_00008-6-3 loss: 0.542804  [  128/  130]
train() client id: f_00008-7-0 loss: 0.705886  [   32/  130]
train() client id: f_00008-7-1 loss: 0.659771  [   64/  130]
train() client id: f_00008-7-2 loss: 0.615755  [   96/  130]
train() client id: f_00008-7-3 loss: 0.646272  [  128/  130]
train() client id: f_00008-8-0 loss: 0.686489  [   32/  130]
train() client id: f_00008-8-1 loss: 0.745397  [   64/  130]
train() client id: f_00008-8-2 loss: 0.650988  [   96/  130]
train() client id: f_00008-8-3 loss: 0.575939  [  128/  130]
train() client id: f_00008-9-0 loss: 0.716705  [   32/  130]
train() client id: f_00008-9-1 loss: 0.651998  [   64/  130]
train() client id: f_00008-9-2 loss: 0.647174  [   96/  130]
train() client id: f_00008-9-3 loss: 0.623075  [  128/  130]
train() client id: f_00008-10-0 loss: 0.610201  [   32/  130]
train() client id: f_00008-10-1 loss: 0.554991  [   64/  130]
train() client id: f_00008-10-2 loss: 0.689814  [   96/  130]
train() client id: f_00008-10-3 loss: 0.742300  [  128/  130]
train() client id: f_00009-0-0 loss: 1.050546  [   32/  118]
train() client id: f_00009-0-1 loss: 0.852180  [   64/  118]
train() client id: f_00009-0-2 loss: 0.819371  [   96/  118]
train() client id: f_00009-1-0 loss: 0.870904  [   32/  118]
train() client id: f_00009-1-1 loss: 0.901662  [   64/  118]
train() client id: f_00009-1-2 loss: 0.879075  [   96/  118]
train() client id: f_00009-2-0 loss: 0.930123  [   32/  118]
train() client id: f_00009-2-1 loss: 0.798486  [   64/  118]
train() client id: f_00009-2-2 loss: 0.713069  [   96/  118]
train() client id: f_00009-3-0 loss: 0.860086  [   32/  118]
train() client id: f_00009-3-1 loss: 0.670632  [   64/  118]
train() client id: f_00009-3-2 loss: 0.858728  [   96/  118]
train() client id: f_00009-4-0 loss: 0.824881  [   32/  118]
train() client id: f_00009-4-1 loss: 0.757894  [   64/  118]
train() client id: f_00009-4-2 loss: 0.679349  [   96/  118]
train() client id: f_00009-5-0 loss: 0.781755  [   32/  118]
train() client id: f_00009-5-1 loss: 0.772988  [   64/  118]
train() client id: f_00009-5-2 loss: 0.595240  [   96/  118]
train() client id: f_00009-6-0 loss: 0.670144  [   32/  118]
train() client id: f_00009-6-1 loss: 0.757174  [   64/  118]
train() client id: f_00009-6-2 loss: 0.726706  [   96/  118]
train() client id: f_00009-7-0 loss: 0.806082  [   32/  118]
train() client id: f_00009-7-1 loss: 0.737737  [   64/  118]
train() client id: f_00009-7-2 loss: 0.685260  [   96/  118]
train() client id: f_00009-8-0 loss: 0.564223  [   32/  118]
train() client id: f_00009-8-1 loss: 0.652967  [   64/  118]
train() client id: f_00009-8-2 loss: 0.826000  [   96/  118]
train() client id: f_00009-9-0 loss: 0.665130  [   32/  118]
train() client id: f_00009-9-1 loss: 0.698329  [   64/  118]
train() client id: f_00009-9-2 loss: 0.731756  [   96/  118]
train() client id: f_00009-10-0 loss: 0.618803  [   32/  118]
train() client id: f_00009-10-1 loss: 0.796689  [   64/  118]
train() client id: f_00009-10-2 loss: 0.640716  [   96/  118]
At round 53 accuracy: 0.6472148541114059
At round 53 training accuracy: 0.5902079141515761
At round 53 training loss: 0.8196480347478641
update_location
xs = [  -3.9056584     4.20031788  285.00902392   18.81129433    0.97929623
    3.95640986 -247.44319194 -226.32485185  269.66397685 -212.06087855]
ys = [ 277.5879595   260.55583871    1.32061395 -247.45517586  239.35018685
  222.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [295.07681953 279.11823257 302.04616822 267.55920626 259.4021414
 244.2576487  266.89890179 247.43406156 288.14463449 234.49057138]
dists_bs = [199.49326595 198.57632151 491.70400773 464.79155911 187.3757168
 185.23387822 191.82974512 181.58219868 471.71026083 174.96857419]
uav_gains = [2.94167056e-12 4.07370156e-12 2.57043737e-12 5.18972845e-12
 6.14485954e-12 8.28695143e-12 5.26179225e-12 7.79933417e-12
 3.38079010e-12 9.90488682e-12]
bs_gains = [4.01323889e-11 4.06534300e-11 3.21013717e-12 3.75812338e-12
 4.78296084e-11 4.93943086e-11 4.47846749e-11 5.22262602e-11
 3.60581283e-12 5.79436618e-11]
Round 54
-------------------------------
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.87808648 7.89231983 3.82850312 1.39788506 9.0995415  4.37831172
 1.72181282 5.39414421 3.98347864 3.54993313]
obj_prev = 45.124016510283404
eta_min = 1.5688388831448793e-24	eta_max = 0.9379204733020944
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 10.402835079222914	eta = 0.9090909090909091
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 22.247241049856704	eta = 0.4250919373822522
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 16.022057938092566	eta = 0.5902564349620266
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.904186500929747	eta = 0.6345279427832983
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.838111741409746	eta = 0.6373535234204303
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.83785471002873	eta = 0.6373645640903601
eta = 0.6373645640903601
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [0.03716704 0.07816878 0.03657709 0.01268399 0.09026281 0.04306657
 0.01592873 0.0528008  0.03834695 0.03480723]
ene_total = [1.43077897 2.28852743 1.44075621 0.69142509 2.60477925 1.34299583
 0.77637967 1.71811452 1.42782004 1.11627769]
ti_comp = [0.85812284 0.94697766 0.84848758 0.89012496 0.94950542 0.94998635
 0.89076325 0.90687475 0.8710298  0.95228188]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [4.35767693e-06 3.32889784e-05 4.24831188e-06 1.60970289e-07
 5.09813926e-05 5.53179598e-06 3.18345717e-07 1.11868234e-05
 4.64522066e-06 2.90641279e-06]
ene_total = [0.4382524  0.20171506 0.46398296 0.35267029 0.19543653 0.19293825
 0.35096977 0.30823006 0.40378868 0.18673731]
optimize_network_iter = 0 obj = 3.094721297443309
eta = 0.6373645640903601
freqs = [21656013.90690634 41272766.08711902 21554285.58755618  7124836.67893045
 47531488.32308537 22666941.75621849  8941057.85172046 29111404.29436724
 22012419.35744325 18275695.9412735 ]
eta_min = 0.6373645640903616	eta_max = 0.700248974656232
af = 0.0025292806273427074	bf = 1.1211163846456331	zeta = 0.002782208690076978	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [9.22110454e-07 7.04414656e-06 8.98968156e-07 3.40622741e-08
 1.07879670e-05 1.17056105e-06 6.73638541e-08 2.36719862e-06
 9.82956424e-07 6.15014298e-07]
ene_total = [1.74781835 0.80184327 1.85046646 1.40678466 0.77531226 0.76916407
 1.39998808 1.22858694 1.61031875 0.74464912]
ti_comp = [0.68086857 0.76972339 0.67123331 0.71287069 0.77225115 0.77273208
 0.71350898 0.72962048 0.69377552 0.77502761]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.42697873e-06 2.49456369e-05 3.36081341e-06 1.24253947e-07
 3.81569359e-05 4.13930121e-06 2.45645457e-07 8.55640634e-06
 3.62508715e-06 2.17239112e-06]
ene_total = [0.53016278 0.24376304 0.56129271 0.42665538 0.23602256 0.23336954
 0.42459695 0.37280828 0.48846606 0.22588899]
optimize_network_iter = 1 obj = 3.7430263059334705
eta = 0.700248974656232
freqs = [21591991.81873228 40169519.43771622 21554285.58755618  7037900.85535273
 46232598.93038776 22044962.3818897   8830387.52189105 28624734.01565526
 21863003.51871486 17764389.24875431]
eta_min = 0.7002489746562424	eta_max = 0.7002489746562317
af = 0.0024124103758265034	bf = 1.1211163846456331	zeta = 0.002653651413409154	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [9.16666407e-07 6.67259097e-06 8.98968156e-07 3.32361034e-08
 1.02064191e-05 1.10720219e-06 6.57065467e-08 2.28871285e-06
 9.69657495e-07 5.81082673e-07]
ene_total = [1.74781777 0.80180369 1.85046646 1.40678457 0.7752503  0.76915732
 1.3999879  1.22857858 1.61031734 0.7446455 ]
ti_comp = [0.68086857 0.76972339 0.67123331 0.71287069 0.77225115 0.77273208
 0.71350898 0.72962048 0.69377552 0.77502761]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.42697873e-06 2.49456369e-05 3.36081341e-06 1.24253947e-07
 3.81569359e-05 4.13930121e-06 2.45645457e-07 8.55640634e-06
 3.62508715e-06 2.17239112e-06]
ene_total = [0.53016278 0.24376304 0.56129271 0.42665538 0.23602256 0.23336954
 0.42459695 0.37280828 0.48846606 0.22588899]
optimize_network_iter = 2 obj = 3.743026305933466
eta = 0.7002489746562317
freqs = [21591991.81873227 40169519.4377162  21554285.58755618  7037900.85535273
 46232598.93038776 22044962.38188971  8830387.52189105 28624734.01565526
 21863003.51871486 17764389.24875431]
Done!
At round 54 eta: 0.7002489746562317
At round 54 local rounds: 11.66770515392171
At round 54 global rounds: 32.310572111651304
At round 54 a_n: 9.342581272941924
gradient difference: 0.5029524564743042
train() client id: f_00000-0-0 loss: 1.203822  [   32/  126]
train() client id: f_00000-0-1 loss: 1.266114  [   64/  126]
train() client id: f_00000-0-2 loss: 0.860864  [   96/  126]
train() client id: f_00000-1-0 loss: 1.025414  [   32/  126]
train() client id: f_00000-1-1 loss: 1.190343  [   64/  126]
train() client id: f_00000-1-2 loss: 1.059440  [   96/  126]
train() client id: f_00000-2-0 loss: 1.093299  [   32/  126]
train() client id: f_00000-2-1 loss: 0.842664  [   64/  126]
train() client id: f_00000-2-2 loss: 1.036559  [   96/  126]
train() client id: f_00000-3-0 loss: 1.025901  [   32/  126]
train() client id: f_00000-3-1 loss: 0.860547  [   64/  126]
train() client id: f_00000-3-2 loss: 1.026169  [   96/  126]
train() client id: f_00000-4-0 loss: 0.988062  [   32/  126]
train() client id: f_00000-4-1 loss: 0.866991  [   64/  126]
train() client id: f_00000-4-2 loss: 0.919128  [   96/  126]
train() client id: f_00000-5-0 loss: 0.862193  [   32/  126]
train() client id: f_00000-5-1 loss: 0.968139  [   64/  126]
train() client id: f_00000-5-2 loss: 0.931165  [   96/  126]
train() client id: f_00000-6-0 loss: 0.855901  [   32/  126]
train() client id: f_00000-6-1 loss: 0.911920  [   64/  126]
train() client id: f_00000-6-2 loss: 0.981448  [   96/  126]
train() client id: f_00000-7-0 loss: 0.919518  [   32/  126]
train() client id: f_00000-7-1 loss: 0.953016  [   64/  126]
train() client id: f_00000-7-2 loss: 0.834611  [   96/  126]
train() client id: f_00000-8-0 loss: 0.955851  [   32/  126]
train() client id: f_00000-8-1 loss: 0.924198  [   64/  126]
train() client id: f_00000-8-2 loss: 0.794918  [   96/  126]
train() client id: f_00000-9-0 loss: 0.822203  [   32/  126]
train() client id: f_00000-9-1 loss: 0.957098  [   64/  126]
train() client id: f_00000-9-2 loss: 0.917764  [   96/  126]
train() client id: f_00000-10-0 loss: 0.998062  [   32/  126]
train() client id: f_00000-10-1 loss: 0.863845  [   64/  126]
train() client id: f_00000-10-2 loss: 0.958689  [   96/  126]
train() client id: f_00001-0-0 loss: 0.567565  [   32/  265]
train() client id: f_00001-0-1 loss: 0.605058  [   64/  265]
train() client id: f_00001-0-2 loss: 0.487778  [   96/  265]
train() client id: f_00001-0-3 loss: 0.546546  [  128/  265]
train() client id: f_00001-0-4 loss: 0.479201  [  160/  265]
train() client id: f_00001-0-5 loss: 0.571370  [  192/  265]
train() client id: f_00001-0-6 loss: 0.521342  [  224/  265]
train() client id: f_00001-0-7 loss: 0.507096  [  256/  265]
train() client id: f_00001-1-0 loss: 0.446872  [   32/  265]
train() client id: f_00001-1-1 loss: 0.552490  [   64/  265]
train() client id: f_00001-1-2 loss: 0.614522  [   96/  265]
train() client id: f_00001-1-3 loss: 0.507547  [  128/  265]
train() client id: f_00001-1-4 loss: 0.507435  [  160/  265]
train() client id: f_00001-1-5 loss: 0.552939  [  192/  265]
train() client id: f_00001-1-6 loss: 0.593735  [  224/  265]
train() client id: f_00001-1-7 loss: 0.441037  [  256/  265]
train() client id: f_00001-2-0 loss: 0.405315  [   32/  265]
train() client id: f_00001-2-1 loss: 0.545872  [   64/  265]
train() client id: f_00001-2-2 loss: 0.583942  [   96/  265]
train() client id: f_00001-2-3 loss: 0.440450  [  128/  265]
train() client id: f_00001-2-4 loss: 0.501433  [  160/  265]
train() client id: f_00001-2-5 loss: 0.418343  [  192/  265]
train() client id: f_00001-2-6 loss: 0.662191  [  224/  265]
train() client id: f_00001-2-7 loss: 0.654183  [  256/  265]
train() client id: f_00001-3-0 loss: 0.530870  [   32/  265]
train() client id: f_00001-3-1 loss: 0.541815  [   64/  265]
train() client id: f_00001-3-2 loss: 0.425076  [   96/  265]
train() client id: f_00001-3-3 loss: 0.502094  [  128/  265]
train() client id: f_00001-3-4 loss: 0.540473  [  160/  265]
train() client id: f_00001-3-5 loss: 0.607229  [  192/  265]
train() client id: f_00001-3-6 loss: 0.465787  [  224/  265]
train() client id: f_00001-3-7 loss: 0.561414  [  256/  265]
train() client id: f_00001-4-0 loss: 0.550257  [   32/  265]
train() client id: f_00001-4-1 loss: 0.615476  [   64/  265]
train() client id: f_00001-4-2 loss: 0.497228  [   96/  265]
train() client id: f_00001-4-3 loss: 0.533933  [  128/  265]
train() client id: f_00001-4-4 loss: 0.453412  [  160/  265]
train() client id: f_00001-4-5 loss: 0.429609  [  192/  265]
train() client id: f_00001-4-6 loss: 0.471148  [  224/  265]
train() client id: f_00001-4-7 loss: 0.607733  [  256/  265]
train() client id: f_00001-5-0 loss: 0.567366  [   32/  265]
train() client id: f_00001-5-1 loss: 0.516025  [   64/  265]
train() client id: f_00001-5-2 loss: 0.429350  [   96/  265]
train() client id: f_00001-5-3 loss: 0.523329  [  128/  265]
train() client id: f_00001-5-4 loss: 0.401765  [  160/  265]
train() client id: f_00001-5-5 loss: 0.421306  [  192/  265]
train() client id: f_00001-5-6 loss: 0.750353  [  224/  265]
train() client id: f_00001-5-7 loss: 0.490578  [  256/  265]
train() client id: f_00001-6-0 loss: 0.474136  [   32/  265]
train() client id: f_00001-6-1 loss: 0.413183  [   64/  265]
train() client id: f_00001-6-2 loss: 0.473506  [   96/  265]
train() client id: f_00001-6-3 loss: 0.583514  [  128/  265]
train() client id: f_00001-6-4 loss: 0.642402  [  160/  265]
train() client id: f_00001-6-5 loss: 0.416348  [  192/  265]
train() client id: f_00001-6-6 loss: 0.550347  [  224/  265]
train() client id: f_00001-6-7 loss: 0.508126  [  256/  265]
train() client id: f_00001-7-0 loss: 0.645620  [   32/  265]
train() client id: f_00001-7-1 loss: 0.589899  [   64/  265]
train() client id: f_00001-7-2 loss: 0.581465  [   96/  265]
train() client id: f_00001-7-3 loss: 0.425420  [  128/  265]
train() client id: f_00001-7-4 loss: 0.422679  [  160/  265]
train() client id: f_00001-7-5 loss: 0.439810  [  192/  265]
train() client id: f_00001-7-6 loss: 0.525308  [  224/  265]
train() client id: f_00001-7-7 loss: 0.440356  [  256/  265]
train() client id: f_00001-8-0 loss: 0.511321  [   32/  265]
train() client id: f_00001-8-1 loss: 0.489238  [   64/  265]
train() client id: f_00001-8-2 loss: 0.427853  [   96/  265]
train() client id: f_00001-8-3 loss: 0.688059  [  128/  265]
train() client id: f_00001-8-4 loss: 0.594175  [  160/  265]
train() client id: f_00001-8-5 loss: 0.462661  [  192/  265]
train() client id: f_00001-8-6 loss: 0.478063  [  224/  265]
train() client id: f_00001-8-7 loss: 0.503222  [  256/  265]
train() client id: f_00001-9-0 loss: 0.589234  [   32/  265]
train() client id: f_00001-9-1 loss: 0.519530  [   64/  265]
train() client id: f_00001-9-2 loss: 0.500704  [   96/  265]
train() client id: f_00001-9-3 loss: 0.516729  [  128/  265]
train() client id: f_00001-9-4 loss: 0.532825  [  160/  265]
train() client id: f_00001-9-5 loss: 0.472626  [  192/  265]
train() client id: f_00001-9-6 loss: 0.424270  [  224/  265]
train() client id: f_00001-9-7 loss: 0.550246  [  256/  265]
train() client id: f_00001-10-0 loss: 0.463955  [   32/  265]
train() client id: f_00001-10-1 loss: 0.474166  [   64/  265]
train() client id: f_00001-10-2 loss: 0.420578  [   96/  265]
train() client id: f_00001-10-3 loss: 0.663419  [  128/  265]
train() client id: f_00001-10-4 loss: 0.422467  [  160/  265]
train() client id: f_00001-10-5 loss: 0.620507  [  192/  265]
train() client id: f_00001-10-6 loss: 0.468657  [  224/  265]
train() client id: f_00001-10-7 loss: 0.599724  [  256/  265]
train() client id: f_00002-0-0 loss: 1.208061  [   32/  124]
train() client id: f_00002-0-1 loss: 1.008715  [   64/  124]
train() client id: f_00002-0-2 loss: 1.132046  [   96/  124]
train() client id: f_00002-1-0 loss: 1.022933  [   32/  124]
train() client id: f_00002-1-1 loss: 1.091427  [   64/  124]
train() client id: f_00002-1-2 loss: 1.150674  [   96/  124]
train() client id: f_00002-2-0 loss: 1.023899  [   32/  124]
train() client id: f_00002-2-1 loss: 1.162128  [   64/  124]
train() client id: f_00002-2-2 loss: 0.948969  [   96/  124]
train() client id: f_00002-3-0 loss: 0.927052  [   32/  124]
train() client id: f_00002-3-1 loss: 0.918752  [   64/  124]
train() client id: f_00002-3-2 loss: 1.094903  [   96/  124]
train() client id: f_00002-4-0 loss: 0.956402  [   32/  124]
train() client id: f_00002-4-1 loss: 1.112158  [   64/  124]
train() client id: f_00002-4-2 loss: 1.056080  [   96/  124]
train() client id: f_00002-5-0 loss: 1.043190  [   32/  124]
train() client id: f_00002-5-1 loss: 1.012910  [   64/  124]
train() client id: f_00002-5-2 loss: 0.906106  [   96/  124]
train() client id: f_00002-6-0 loss: 0.905046  [   32/  124]
train() client id: f_00002-6-1 loss: 1.045791  [   64/  124]
train() client id: f_00002-6-2 loss: 1.079957  [   96/  124]
train() client id: f_00002-7-0 loss: 1.089393  [   32/  124]
train() client id: f_00002-7-1 loss: 0.668964  [   64/  124]
train() client id: f_00002-7-2 loss: 1.086739  [   96/  124]
train() client id: f_00002-8-0 loss: 0.842153  [   32/  124]
train() client id: f_00002-8-1 loss: 0.870926  [   64/  124]
train() client id: f_00002-8-2 loss: 1.034959  [   96/  124]
train() client id: f_00002-9-0 loss: 0.848983  [   32/  124]
train() client id: f_00002-9-1 loss: 0.949698  [   64/  124]
train() client id: f_00002-9-2 loss: 0.798876  [   96/  124]
train() client id: f_00002-10-0 loss: 1.004550  [   32/  124]
train() client id: f_00002-10-1 loss: 0.921708  [   64/  124]
train() client id: f_00002-10-2 loss: 1.022130  [   96/  124]
train() client id: f_00003-0-0 loss: 0.766043  [   32/   43]
train() client id: f_00003-1-0 loss: 0.750357  [   32/   43]
train() client id: f_00003-2-0 loss: 0.689178  [   32/   43]
train() client id: f_00003-3-0 loss: 0.731036  [   32/   43]
train() client id: f_00003-4-0 loss: 0.721281  [   32/   43]
train() client id: f_00003-5-0 loss: 0.794180  [   32/   43]
train() client id: f_00003-6-0 loss: 0.896277  [   32/   43]
train() client id: f_00003-7-0 loss: 0.624044  [   32/   43]
train() client id: f_00003-8-0 loss: 0.838596  [   32/   43]
train() client id: f_00003-9-0 loss: 0.881427  [   32/   43]
train() client id: f_00003-10-0 loss: 0.946175  [   32/   43]
train() client id: f_00004-0-0 loss: 0.747366  [   32/  306]
train() client id: f_00004-0-1 loss: 0.731839  [   64/  306]
train() client id: f_00004-0-2 loss: 0.912220  [   96/  306]
train() client id: f_00004-0-3 loss: 0.733360  [  128/  306]
train() client id: f_00004-0-4 loss: 0.958619  [  160/  306]
train() client id: f_00004-0-5 loss: 0.686179  [  192/  306]
train() client id: f_00004-0-6 loss: 0.816444  [  224/  306]
train() client id: f_00004-0-7 loss: 0.740199  [  256/  306]
train() client id: f_00004-0-8 loss: 0.782309  [  288/  306]
train() client id: f_00004-1-0 loss: 0.841337  [   32/  306]
train() client id: f_00004-1-1 loss: 0.828063  [   64/  306]
train() client id: f_00004-1-2 loss: 0.828806  [   96/  306]
train() client id: f_00004-1-3 loss: 0.640808  [  128/  306]
train() client id: f_00004-1-4 loss: 0.692951  [  160/  306]
train() client id: f_00004-1-5 loss: 0.747131  [  192/  306]
train() client id: f_00004-1-6 loss: 0.813339  [  224/  306]
train() client id: f_00004-1-7 loss: 0.844731  [  256/  306]
train() client id: f_00004-1-8 loss: 0.879142  [  288/  306]
train() client id: f_00004-2-0 loss: 0.840108  [   32/  306]
train() client id: f_00004-2-1 loss: 0.813919  [   64/  306]
train() client id: f_00004-2-2 loss: 0.713421  [   96/  306]
train() client id: f_00004-2-3 loss: 0.665510  [  128/  306]
train() client id: f_00004-2-4 loss: 0.823680  [  160/  306]
train() client id: f_00004-2-5 loss: 0.705869  [  192/  306]
train() client id: f_00004-2-6 loss: 0.948245  [  224/  306]
train() client id: f_00004-2-7 loss: 0.787977  [  256/  306]
train() client id: f_00004-2-8 loss: 0.822705  [  288/  306]
train() client id: f_00004-3-0 loss: 0.803555  [   32/  306]
train() client id: f_00004-3-1 loss: 0.731217  [   64/  306]
train() client id: f_00004-3-2 loss: 0.778945  [   96/  306]
train() client id: f_00004-3-3 loss: 0.739823  [  128/  306]
train() client id: f_00004-3-4 loss: 0.749723  [  160/  306]
train() client id: f_00004-3-5 loss: 1.007611  [  192/  306]
train() client id: f_00004-3-6 loss: 0.749630  [  224/  306]
train() client id: f_00004-3-7 loss: 0.646955  [  256/  306]
train() client id: f_00004-3-8 loss: 0.860452  [  288/  306]
train() client id: f_00004-4-0 loss: 0.927075  [   32/  306]
train() client id: f_00004-4-1 loss: 0.812413  [   64/  306]
train() client id: f_00004-4-2 loss: 0.800091  [   96/  306]
train() client id: f_00004-4-3 loss: 0.718851  [  128/  306]
train() client id: f_00004-4-4 loss: 0.847374  [  160/  306]
train() client id: f_00004-4-5 loss: 0.956988  [  192/  306]
train() client id: f_00004-4-6 loss: 0.748069  [  224/  306]
train() client id: f_00004-4-7 loss: 0.630464  [  256/  306]
train() client id: f_00004-4-8 loss: 0.739122  [  288/  306]
train() client id: f_00004-5-0 loss: 0.762332  [   32/  306]
train() client id: f_00004-5-1 loss: 0.811708  [   64/  306]
train() client id: f_00004-5-2 loss: 0.772685  [   96/  306]
train() client id: f_00004-5-3 loss: 0.733038  [  128/  306]
train() client id: f_00004-5-4 loss: 0.825935  [  160/  306]
train() client id: f_00004-5-5 loss: 0.844662  [  192/  306]
train() client id: f_00004-5-6 loss: 0.905755  [  224/  306]
train() client id: f_00004-5-7 loss: 0.686763  [  256/  306]
train() client id: f_00004-5-8 loss: 0.740704  [  288/  306]
train() client id: f_00004-6-0 loss: 0.722579  [   32/  306]
train() client id: f_00004-6-1 loss: 0.924913  [   64/  306]
train() client id: f_00004-6-2 loss: 0.731492  [   96/  306]
train() client id: f_00004-6-3 loss: 0.786493  [  128/  306]
train() client id: f_00004-6-4 loss: 0.924043  [  160/  306]
train() client id: f_00004-6-5 loss: 0.813087  [  192/  306]
train() client id: f_00004-6-6 loss: 0.682480  [  224/  306]
train() client id: f_00004-6-7 loss: 0.855136  [  256/  306]
train() client id: f_00004-6-8 loss: 0.783189  [  288/  306]
train() client id: f_00004-7-0 loss: 0.859282  [   32/  306]
train() client id: f_00004-7-1 loss: 0.740317  [   64/  306]
train() client id: f_00004-7-2 loss: 0.702511  [   96/  306]
train() client id: f_00004-7-3 loss: 0.832922  [  128/  306]
train() client id: f_00004-7-4 loss: 0.854204  [  160/  306]
train() client id: f_00004-7-5 loss: 0.823669  [  192/  306]
train() client id: f_00004-7-6 loss: 0.784304  [  224/  306]
train() client id: f_00004-7-7 loss: 0.863888  [  256/  306]
train() client id: f_00004-7-8 loss: 0.727209  [  288/  306]
train() client id: f_00004-8-0 loss: 0.780170  [   32/  306]
train() client id: f_00004-8-1 loss: 0.733293  [   64/  306]
train() client id: f_00004-8-2 loss: 1.022122  [   96/  306]
train() client id: f_00004-8-3 loss: 0.825501  [  128/  306]
train() client id: f_00004-8-4 loss: 0.894082  [  160/  306]
train() client id: f_00004-8-5 loss: 0.685865  [  192/  306]
train() client id: f_00004-8-6 loss: 0.679209  [  224/  306]
train() client id: f_00004-8-7 loss: 0.835251  [  256/  306]
train() client id: f_00004-8-8 loss: 0.712464  [  288/  306]
train() client id: f_00004-9-0 loss: 0.918234  [   32/  306]
train() client id: f_00004-9-1 loss: 0.878490  [   64/  306]
train() client id: f_00004-9-2 loss: 0.751500  [   96/  306]
train() client id: f_00004-9-3 loss: 0.737333  [  128/  306]
train() client id: f_00004-9-4 loss: 0.856342  [  160/  306]
train() client id: f_00004-9-5 loss: 0.656193  [  192/  306]
train() client id: f_00004-9-6 loss: 0.859772  [  224/  306]
train() client id: f_00004-9-7 loss: 0.687118  [  256/  306]
train() client id: f_00004-9-8 loss: 0.894894  [  288/  306]
train() client id: f_00004-10-0 loss: 0.861454  [   32/  306]
train() client id: f_00004-10-1 loss: 0.812597  [   64/  306]
train() client id: f_00004-10-2 loss: 0.833800  [   96/  306]
train() client id: f_00004-10-3 loss: 0.742763  [  128/  306]
train() client id: f_00004-10-4 loss: 0.794065  [  160/  306]
train() client id: f_00004-10-5 loss: 0.803070  [  192/  306]
train() client id: f_00004-10-6 loss: 0.757961  [  224/  306]
train() client id: f_00004-10-7 loss: 0.850228  [  256/  306]
train() client id: f_00004-10-8 loss: 0.763502  [  288/  306]
train() client id: f_00005-0-0 loss: 0.369468  [   32/  146]
train() client id: f_00005-0-1 loss: 0.590973  [   64/  146]
train() client id: f_00005-0-2 loss: 0.204768  [   96/  146]
train() client id: f_00005-0-3 loss: 0.558971  [  128/  146]
train() client id: f_00005-1-0 loss: 0.350554  [   32/  146]
train() client id: f_00005-1-1 loss: 0.254207  [   64/  146]
train() client id: f_00005-1-2 loss: 0.484488  [   96/  146]
train() client id: f_00005-1-3 loss: 0.665872  [  128/  146]
train() client id: f_00005-2-0 loss: 0.516624  [   32/  146]
train() client id: f_00005-2-1 loss: 0.209800  [   64/  146]
train() client id: f_00005-2-2 loss: 0.301527  [   96/  146]
train() client id: f_00005-2-3 loss: 0.578483  [  128/  146]
train() client id: f_00005-3-0 loss: 0.582382  [   32/  146]
train() client id: f_00005-3-1 loss: 0.444758  [   64/  146]
train() client id: f_00005-3-2 loss: 0.243633  [   96/  146]
train() client id: f_00005-3-3 loss: 0.366906  [  128/  146]
train() client id: f_00005-4-0 loss: 0.233136  [   32/  146]
train() client id: f_00005-4-1 loss: 0.461172  [   64/  146]
train() client id: f_00005-4-2 loss: 0.519684  [   96/  146]
train() client id: f_00005-4-3 loss: 0.567769  [  128/  146]
train() client id: f_00005-5-0 loss: 0.538608  [   32/  146]
train() client id: f_00005-5-1 loss: 0.205258  [   64/  146]
train() client id: f_00005-5-2 loss: 0.355860  [   96/  146]
train() client id: f_00005-5-3 loss: 0.626354  [  128/  146]
train() client id: f_00005-6-0 loss: 0.404636  [   32/  146]
train() client id: f_00005-6-1 loss: 0.352113  [   64/  146]
train() client id: f_00005-6-2 loss: 0.380935  [   96/  146]
train() client id: f_00005-6-3 loss: 0.626646  [  128/  146]
train() client id: f_00005-7-0 loss: 0.463679  [   32/  146]
train() client id: f_00005-7-1 loss: 0.399424  [   64/  146]
train() client id: f_00005-7-2 loss: 0.285581  [   96/  146]
train() client id: f_00005-7-3 loss: 0.614642  [  128/  146]
train() client id: f_00005-8-0 loss: 0.512214  [   32/  146]
train() client id: f_00005-8-1 loss: 0.379555  [   64/  146]
train() client id: f_00005-8-2 loss: 0.432112  [   96/  146]
train() client id: f_00005-8-3 loss: 0.364354  [  128/  146]
train() client id: f_00005-9-0 loss: 0.154153  [   32/  146]
train() client id: f_00005-9-1 loss: 0.317373  [   64/  146]
train() client id: f_00005-9-2 loss: 0.415208  [   96/  146]
train() client id: f_00005-9-3 loss: 0.643896  [  128/  146]
train() client id: f_00005-10-0 loss: 0.563311  [   32/  146]
train() client id: f_00005-10-1 loss: 0.486070  [   64/  146]
train() client id: f_00005-10-2 loss: 0.531266  [   96/  146]
train() client id: f_00005-10-3 loss: 0.241617  [  128/  146]
train() client id: f_00006-0-0 loss: 0.520296  [   32/   54]
train() client id: f_00006-1-0 loss: 0.575984  [   32/   54]
train() client id: f_00006-2-0 loss: 0.505797  [   32/   54]
train() client id: f_00006-3-0 loss: 0.560421  [   32/   54]
train() client id: f_00006-4-0 loss: 0.558292  [   32/   54]
train() client id: f_00006-5-0 loss: 0.501876  [   32/   54]
train() client id: f_00006-6-0 loss: 0.560237  [   32/   54]
train() client id: f_00006-7-0 loss: 0.489469  [   32/   54]
train() client id: f_00006-8-0 loss: 0.550295  [   32/   54]
train() client id: f_00006-9-0 loss: 0.546703  [   32/   54]
train() client id: f_00006-10-0 loss: 0.491232  [   32/   54]
train() client id: f_00007-0-0 loss: 0.470119  [   32/  179]
train() client id: f_00007-0-1 loss: 0.793112  [   64/  179]
train() client id: f_00007-0-2 loss: 0.690234  [   96/  179]
train() client id: f_00007-0-3 loss: 0.684112  [  128/  179]
train() client id: f_00007-0-4 loss: 0.575357  [  160/  179]
train() client id: f_00007-1-0 loss: 0.527525  [   32/  179]
train() client id: f_00007-1-1 loss: 0.817211  [   64/  179]
train() client id: f_00007-1-2 loss: 0.469613  [   96/  179]
train() client id: f_00007-1-3 loss: 0.600060  [  128/  179]
train() client id: f_00007-1-4 loss: 0.798630  [  160/  179]
train() client id: f_00007-2-0 loss: 0.546136  [   32/  179]
train() client id: f_00007-2-1 loss: 0.621189  [   64/  179]
train() client id: f_00007-2-2 loss: 0.734963  [   96/  179]
train() client id: f_00007-2-3 loss: 0.488571  [  128/  179]
train() client id: f_00007-2-4 loss: 0.713607  [  160/  179]
train() client id: f_00007-3-0 loss: 0.614289  [   32/  179]
train() client id: f_00007-3-1 loss: 0.550471  [   64/  179]
train() client id: f_00007-3-2 loss: 0.622865  [   96/  179]
train() client id: f_00007-3-3 loss: 0.428441  [  128/  179]
train() client id: f_00007-3-4 loss: 0.786038  [  160/  179]
train() client id: f_00007-4-0 loss: 0.455532  [   32/  179]
train() client id: f_00007-4-1 loss: 0.777780  [   64/  179]
train() client id: f_00007-4-2 loss: 0.437722  [   96/  179]
train() client id: f_00007-4-3 loss: 0.706465  [  128/  179]
train() client id: f_00007-4-4 loss: 0.498049  [  160/  179]
train() client id: f_00007-5-0 loss: 0.419107  [   32/  179]
train() client id: f_00007-5-1 loss: 0.641077  [   64/  179]
train() client id: f_00007-5-2 loss: 0.536147  [   96/  179]
train() client id: f_00007-5-3 loss: 0.612596  [  128/  179]
train() client id: f_00007-5-4 loss: 0.753278  [  160/  179]
train() client id: f_00007-6-0 loss: 0.571578  [   32/  179]
train() client id: f_00007-6-1 loss: 0.573448  [   64/  179]
train() client id: f_00007-6-2 loss: 0.646669  [   96/  179]
train() client id: f_00007-6-3 loss: 0.592285  [  128/  179]
train() client id: f_00007-6-4 loss: 0.545039  [  160/  179]
train() client id: f_00007-7-0 loss: 0.526908  [   32/  179]
train() client id: f_00007-7-1 loss: 0.658745  [   64/  179]
train() client id: f_00007-7-2 loss: 0.572190  [   96/  179]
train() client id: f_00007-7-3 loss: 0.777458  [  128/  179]
train() client id: f_00007-7-4 loss: 0.486970  [  160/  179]
train() client id: f_00007-8-0 loss: 0.650217  [   32/  179]
train() client id: f_00007-8-1 loss: 0.612008  [   64/  179]
train() client id: f_00007-8-2 loss: 0.509892  [   96/  179]
train() client id: f_00007-8-3 loss: 0.538385  [  128/  179]
train() client id: f_00007-8-4 loss: 0.618707  [  160/  179]
train() client id: f_00007-9-0 loss: 0.744910  [   32/  179]
train() client id: f_00007-9-1 loss: 0.535421  [   64/  179]
train() client id: f_00007-9-2 loss: 0.426530  [   96/  179]
train() client id: f_00007-9-3 loss: 0.613908  [  128/  179]
train() client id: f_00007-9-4 loss: 0.417138  [  160/  179]
train() client id: f_00007-10-0 loss: 0.522560  [   32/  179]
train() client id: f_00007-10-1 loss: 0.672870  [   64/  179]
train() client id: f_00007-10-2 loss: 0.698130  [   96/  179]
train() client id: f_00007-10-3 loss: 0.407092  [  128/  179]
train() client id: f_00007-10-4 loss: 0.561961  [  160/  179]
train() client id: f_00008-0-0 loss: 0.719772  [   32/  130]
train() client id: f_00008-0-1 loss: 0.654968  [   64/  130]
train() client id: f_00008-0-2 loss: 0.666838  [   96/  130]
train() client id: f_00008-0-3 loss: 0.615070  [  128/  130]
train() client id: f_00008-1-0 loss: 0.685681  [   32/  130]
train() client id: f_00008-1-1 loss: 0.633509  [   64/  130]
train() client id: f_00008-1-2 loss: 0.664313  [   96/  130]
train() client id: f_00008-1-3 loss: 0.674706  [  128/  130]
train() client id: f_00008-2-0 loss: 0.616410  [   32/  130]
train() client id: f_00008-2-1 loss: 0.648984  [   64/  130]
train() client id: f_00008-2-2 loss: 0.768151  [   96/  130]
train() client id: f_00008-2-3 loss: 0.618800  [  128/  130]
train() client id: f_00008-3-0 loss: 0.656637  [   32/  130]
train() client id: f_00008-3-1 loss: 0.717234  [   64/  130]
train() client id: f_00008-3-2 loss: 0.651972  [   96/  130]
train() client id: f_00008-3-3 loss: 0.629090  [  128/  130]
train() client id: f_00008-4-0 loss: 0.560274  [   32/  130]
train() client id: f_00008-4-1 loss: 0.614812  [   64/  130]
train() client id: f_00008-4-2 loss: 0.750143  [   96/  130]
train() client id: f_00008-4-3 loss: 0.687744  [  128/  130]
train() client id: f_00008-5-0 loss: 0.677002  [   32/  130]
train() client id: f_00008-5-1 loss: 0.651009  [   64/  130]
train() client id: f_00008-5-2 loss: 0.609683  [   96/  130]
train() client id: f_00008-5-3 loss: 0.716433  [  128/  130]
train() client id: f_00008-6-0 loss: 0.653472  [   32/  130]
train() client id: f_00008-6-1 loss: 0.696731  [   64/  130]
train() client id: f_00008-6-2 loss: 0.650601  [   96/  130]
train() client id: f_00008-6-3 loss: 0.639814  [  128/  130]
train() client id: f_00008-7-0 loss: 0.668580  [   32/  130]
train() client id: f_00008-7-1 loss: 0.617691  [   64/  130]
train() client id: f_00008-7-2 loss: 0.834560  [   96/  130]
train() client id: f_00008-7-3 loss: 0.531839  [  128/  130]
train() client id: f_00008-8-0 loss: 0.539303  [   32/  130]
train() client id: f_00008-8-1 loss: 0.676446  [   64/  130]
train() client id: f_00008-8-2 loss: 0.655324  [   96/  130]
train() client id: f_00008-8-3 loss: 0.722016  [  128/  130]
train() client id: f_00008-9-0 loss: 0.730851  [   32/  130]
train() client id: f_00008-9-1 loss: 0.635787  [   64/  130]
train() client id: f_00008-9-2 loss: 0.639553  [   96/  130]
train() client id: f_00008-9-3 loss: 0.640788  [  128/  130]
train() client id: f_00008-10-0 loss: 0.651758  [   32/  130]
train() client id: f_00008-10-1 loss: 0.766849  [   64/  130]
train() client id: f_00008-10-2 loss: 0.696043  [   96/  130]
train() client id: f_00008-10-3 loss: 0.531317  [  128/  130]
train() client id: f_00009-0-0 loss: 1.124465  [   32/  118]
train() client id: f_00009-0-1 loss: 1.042839  [   64/  118]
train() client id: f_00009-0-2 loss: 0.905067  [   96/  118]
train() client id: f_00009-1-0 loss: 1.126297  [   32/  118]
train() client id: f_00009-1-1 loss: 0.932447  [   64/  118]
train() client id: f_00009-1-2 loss: 1.002644  [   96/  118]
train() client id: f_00009-2-0 loss: 1.093455  [   32/  118]
train() client id: f_00009-2-1 loss: 0.996645  [   64/  118]
train() client id: f_00009-2-2 loss: 0.868758  [   96/  118]
train() client id: f_00009-3-0 loss: 1.126368  [   32/  118]
train() client id: f_00009-3-1 loss: 0.958608  [   64/  118]
train() client id: f_00009-3-2 loss: 0.795317  [   96/  118]
train() client id: f_00009-4-0 loss: 0.885490  [   32/  118]
train() client id: f_00009-4-1 loss: 0.984228  [   64/  118]
train() client id: f_00009-4-2 loss: 0.928919  [   96/  118]
train() client id: f_00009-5-0 loss: 0.974646  [   32/  118]
train() client id: f_00009-5-1 loss: 0.928496  [   64/  118]
train() client id: f_00009-5-2 loss: 0.880979  [   96/  118]
train() client id: f_00009-6-0 loss: 0.967702  [   32/  118]
train() client id: f_00009-6-1 loss: 0.909374  [   64/  118]
train() client id: f_00009-6-2 loss: 0.976924  [   96/  118]
train() client id: f_00009-7-0 loss: 0.850211  [   32/  118]
train() client id: f_00009-7-1 loss: 0.791865  [   64/  118]
train() client id: f_00009-7-2 loss: 0.973143  [   96/  118]
train() client id: f_00009-8-0 loss: 0.945830  [   32/  118]
train() client id: f_00009-8-1 loss: 0.939618  [   64/  118]
train() client id: f_00009-8-2 loss: 0.799133  [   96/  118]
train() client id: f_00009-9-0 loss: 0.890642  [   32/  118]
train() client id: f_00009-9-1 loss: 0.907320  [   64/  118]
train() client id: f_00009-9-2 loss: 0.942492  [   96/  118]
train() client id: f_00009-10-0 loss: 1.072977  [   32/  118]
train() client id: f_00009-10-1 loss: 0.807126  [   64/  118]
train() client id: f_00009-10-2 loss: 0.848154  [   96/  118]
At round 54 accuracy: 0.6472148541114059
At round 54 training accuracy: 0.5928906773977196
At round 54 training loss: 0.8187210218918028
update_location
xs = [  -3.9056584     4.20031788  290.00902392   18.81129433    0.97929623
    3.95640986 -252.44319194 -231.32485185  274.66397685 -217.06087855]
ys = [ 282.5879595   265.55583871    1.32061395 -252.45517586  244.35018685
  227.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [299.78527152 283.79137785 306.76860657 272.19015525 264.02267484
 248.82712967 271.54089139 252.01560138 292.82925085 239.02183343]
dists_bs = [202.10997688 200.78125871 496.38485217 469.34075593 189.15115944
 186.58706051 193.77327739 183.05830601 476.42650004 176.09545916]
uav_gains = [2.68378500e-12 3.69674554e-12 2.35370123e-12 4.70996853e-12
 5.58646308e-12 7.59172095e-12 4.77454866e-12 7.13118064e-12
 3.07591018e-12 9.13258179e-12]
bs_gains = [3.86944233e-11 3.94156962e-11 3.12609541e-12 3.65701662e-12
 4.65831512e-11 4.83978224e-11 4.35382740e-11 5.10556319e-11
 3.50675591e-12 5.69113968e-11]
Round 55
-------------------------------
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.74701293 7.6136546  3.69953246 1.35283391 8.77808353 4.22374522
 1.66527442 5.20651362 3.84415712 3.42461541]
obj_prev = 43.55542321522954
eta_min = 2.2364048538474024e-25	eta_max = 0.9376658335407816
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 10.034905168858742	eta = 0.909090909090909
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 21.758383945021716	eta = 0.4192701574551497
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 15.56372063459429	eta = 0.5861478290943805
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.453751471416155	eta = 0.6311607806900414
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.387691546337155	eta = 0.6340587044987988
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.387430378338086	eta = 0.6340702142568856
eta = 0.6340702142568856
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [0.03759466 0.07906812 0.03699791 0.01282992 0.09130131 0.04356206
 0.016112   0.05340828 0.03878814 0.03520769]
ene_total = [1.39513647 2.21195089 1.40544566 0.67656761 2.51756531 1.29725943
 0.75865034 1.66646122 1.38042471 1.07796874]
ti_comp = [0.89681784 0.99162304 0.88681254 0.93061922 0.99425194 0.99482833
 0.93128983 0.94868174 0.91451495 0.99717632]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [4.12904738e-06 3.14189100e-05 4.02483533e-06 1.52408097e-07
 4.81191659e-05 5.22046259e-06 3.01410426e-07 1.05794898e-05
 4.36109102e-06 2.74314532e-06]
ene_total = [0.43540987 0.19405873 0.46095183 0.34900978 0.18777323 0.1852064
 0.34730146 0.30316045 0.39023318 0.17914848]
optimize_network_iter = 0 obj = 3.032253401302107
eta = 0.6340702142568856
freqs = [20960028.75390409 39868035.25301737 20860054.05324307  6893217.87674232
 45914572.57827595 21894260.42256957  8650365.72831919 28148681.6418485
 21206945.13364052 17653694.93957705]
eta_min = 0.6340702142568873	eta_max = 0.7017829950267004
af = 0.0022772242984259367	bf = 1.1085543441262717	zeta = 0.0025049467282685306	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [8.63792940e-07 6.57280727e-06 8.41991876e-07 3.18836346e-08
 1.00664856e-05 1.09211601e-06 6.30547859e-08 2.21321959e-06
 9.12336258e-07 5.73863496e-07]
ene_total = [1.75228827 0.77857718 1.85510894 1.40483142 0.75191939 0.74507368
 1.39794292 1.21943002 1.57042295 0.72089048]
ti_comp = [0.69931807 0.79412327 0.68931277 0.73311945 0.79675218 0.79732856
 0.73379006 0.75118197 0.71701518 0.79967655]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.18905741e-06 2.30071054e-05 3.12847447e-06 1.15333663e-07
 3.51898948e-05 3.81666577e-06 2.28001571e-07 7.92444745e-06
 3.33175889e-06 2.00316859e-06]
ene_total = [0.53424404 0.23785794 0.56558689 0.42825434 0.23000374 0.22721516
 0.42615699 0.3719125  0.47880679 0.21980253]
optimize_network_iter = 1 obj = 3.71984092060831
eta = 0.7017829950267004
freqs = [20893243.25277815 38696173.21796749 20860054.05324307  6801485.54855279
 44535695.14880739 21233696.28865842  8533594.53110815 27632357.80406622
 21024470.67520079 17111091.4088647 ]
eta_min = 0.7017829950267082	eta_max = 0.7017829950267004
af = 0.0021615562993590274	bf = 1.1085543441262717	zeta = 0.0023777119292949303	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [8.58297057e-07 6.19209011e-06 8.41991876e-07 3.10406904e-08
 9.47094369e-06 1.02721042e-06 6.13639241e-08 2.13277124e-06
 8.96703470e-07 5.39129116e-07]
ene_total = [1.75228771 0.77853806 1.85510894 1.40483134 0.75185818 0.74506701
 1.39794275 1.21942176 1.57042134 0.72088691]
ti_comp = [0.69931807 0.79412327 0.68931277 0.73311945 0.79675218 0.79732856
 0.73379006 0.75118197 0.71701518 0.79967655]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.18905741e-06 2.30071054e-05 3.12847447e-06 1.15333663e-07
 3.51898948e-05 3.81666577e-06 2.28001571e-07 7.92444745e-06
 3.33175889e-06 2.00316859e-06]
ene_total = [0.53424404 0.23785794 0.56558689 0.42825434 0.23000374 0.22721516
 0.42615699 0.3719125  0.47880679 0.21980253]
optimize_network_iter = 2 obj = 3.71984092060831
eta = 0.7017829950267004
freqs = [20893243.25277815 38696173.21796749 20860054.05324307  6801485.54855279
 44535695.14880739 21233696.28865842  8533594.53110815 27632357.80406622
 21024470.67520079 17111091.4088647 ]
Done!
At round 55 eta: 0.7017829950267004
At round 55 local rounds: 11.596049668428781
At round 55 global rounds: 31.328130579872187
At round 55 a_n: 9.000035425972609
gradient difference: 0.5875476002693176
train() client id: f_00000-0-0 loss: 1.179350  [   32/  126]
train() client id: f_00000-0-1 loss: 1.136504  [   64/  126]
train() client id: f_00000-0-2 loss: 1.091388  [   96/  126]
train() client id: f_00000-1-0 loss: 0.817061  [   32/  126]
train() client id: f_00000-1-1 loss: 0.842093  [   64/  126]
train() client id: f_00000-1-2 loss: 1.207191  [   96/  126]
train() client id: f_00000-2-0 loss: 1.114158  [   32/  126]
train() client id: f_00000-2-1 loss: 0.933664  [   64/  126]
train() client id: f_00000-2-2 loss: 0.770128  [   96/  126]
train() client id: f_00000-3-0 loss: 0.870805  [   32/  126]
train() client id: f_00000-3-1 loss: 0.815572  [   64/  126]
train() client id: f_00000-3-2 loss: 0.821743  [   96/  126]
train() client id: f_00000-4-0 loss: 1.069729  [   32/  126]
train() client id: f_00000-4-1 loss: 0.750481  [   64/  126]
train() client id: f_00000-4-2 loss: 0.791923  [   96/  126]
train() client id: f_00000-5-0 loss: 0.744297  [   32/  126]
train() client id: f_00000-5-1 loss: 0.990764  [   64/  126]
train() client id: f_00000-5-2 loss: 0.851705  [   96/  126]
train() client id: f_00000-6-0 loss: 0.811282  [   32/  126]
train() client id: f_00000-6-1 loss: 0.781737  [   64/  126]
train() client id: f_00000-6-2 loss: 0.793956  [   96/  126]
train() client id: f_00000-7-0 loss: 0.800038  [   32/  126]
train() client id: f_00000-7-1 loss: 0.724387  [   64/  126]
train() client id: f_00000-7-2 loss: 0.907215  [   96/  126]
train() client id: f_00000-8-0 loss: 0.757952  [   32/  126]
train() client id: f_00000-8-1 loss: 0.709836  [   64/  126]
train() client id: f_00000-8-2 loss: 0.865593  [   96/  126]
train() client id: f_00000-9-0 loss: 0.970941  [   32/  126]
train() client id: f_00000-9-1 loss: 0.795783  [   64/  126]
train() client id: f_00000-9-2 loss: 0.671726  [   96/  126]
train() client id: f_00000-10-0 loss: 0.705960  [   32/  126]
train() client id: f_00000-10-1 loss: 0.876375  [   64/  126]
train() client id: f_00000-10-2 loss: 0.808845  [   96/  126]
train() client id: f_00001-0-0 loss: 0.481055  [   32/  265]
train() client id: f_00001-0-1 loss: 0.365026  [   64/  265]
train() client id: f_00001-0-2 loss: 0.455511  [   96/  265]
train() client id: f_00001-0-3 loss: 0.525195  [  128/  265]
train() client id: f_00001-0-4 loss: 0.316524  [  160/  265]
train() client id: f_00001-0-5 loss: 0.426929  [  192/  265]
train() client id: f_00001-0-6 loss: 0.352263  [  224/  265]
train() client id: f_00001-0-7 loss: 0.404233  [  256/  265]
train() client id: f_00001-1-0 loss: 0.311912  [   32/  265]
train() client id: f_00001-1-1 loss: 0.518133  [   64/  265]
train() client id: f_00001-1-2 loss: 0.342831  [   96/  265]
train() client id: f_00001-1-3 loss: 0.357988  [  128/  265]
train() client id: f_00001-1-4 loss: 0.432556  [  160/  265]
train() client id: f_00001-1-5 loss: 0.443847  [  192/  265]
train() client id: f_00001-1-6 loss: 0.334789  [  224/  265]
train() client id: f_00001-1-7 loss: 0.531873  [  256/  265]
train() client id: f_00001-2-0 loss: 0.374813  [   32/  265]
train() client id: f_00001-2-1 loss: 0.299551  [   64/  265]
train() client id: f_00001-2-2 loss: 0.462996  [   96/  265]
train() client id: f_00001-2-3 loss: 0.313385  [  128/  265]
train() client id: f_00001-2-4 loss: 0.313630  [  160/  265]
train() client id: f_00001-2-5 loss: 0.458178  [  192/  265]
train() client id: f_00001-2-6 loss: 0.594567  [  224/  265]
train() client id: f_00001-2-7 loss: 0.406239  [  256/  265]
train() client id: f_00001-3-0 loss: 0.443565  [   32/  265]
train() client id: f_00001-3-1 loss: 0.389288  [   64/  265]
train() client id: f_00001-3-2 loss: 0.498347  [   96/  265]
train() client id: f_00001-3-3 loss: 0.490802  [  128/  265]
train() client id: f_00001-3-4 loss: 0.353841  [  160/  265]
train() client id: f_00001-3-5 loss: 0.384700  [  192/  265]
train() client id: f_00001-3-6 loss: 0.310163  [  224/  265]
train() client id: f_00001-3-7 loss: 0.326184  [  256/  265]
train() client id: f_00001-4-0 loss: 0.494812  [   32/  265]
train() client id: f_00001-4-1 loss: 0.452693  [   64/  265]
train() client id: f_00001-4-2 loss: 0.312728  [   96/  265]
train() client id: f_00001-4-3 loss: 0.366566  [  128/  265]
train() client id: f_00001-4-4 loss: 0.384306  [  160/  265]
train() client id: f_00001-4-5 loss: 0.392875  [  192/  265]
train() client id: f_00001-4-6 loss: 0.344330  [  224/  265]
train() client id: f_00001-4-7 loss: 0.429682  [  256/  265]
train() client id: f_00001-5-0 loss: 0.382271  [   32/  265]
train() client id: f_00001-5-1 loss: 0.526415  [   64/  265]
train() client id: f_00001-5-2 loss: 0.264128  [   96/  265]
train() client id: f_00001-5-3 loss: 0.355140  [  128/  265]
train() client id: f_00001-5-4 loss: 0.559756  [  160/  265]
train() client id: f_00001-5-5 loss: 0.371876  [  192/  265]
train() client id: f_00001-5-6 loss: 0.309256  [  224/  265]
train() client id: f_00001-5-7 loss: 0.376213  [  256/  265]
train() client id: f_00001-6-0 loss: 0.339547  [   32/  265]
train() client id: f_00001-6-1 loss: 0.395007  [   64/  265]
train() client id: f_00001-6-2 loss: 0.345018  [   96/  265]
train() client id: f_00001-6-3 loss: 0.287010  [  128/  265]
train() client id: f_00001-6-4 loss: 0.358517  [  160/  265]
train() client id: f_00001-6-5 loss: 0.583186  [  192/  265]
train() client id: f_00001-6-6 loss: 0.407123  [  224/  265]
train() client id: f_00001-6-7 loss: 0.293319  [  256/  265]
train() client id: f_00001-7-0 loss: 0.294109  [   32/  265]
train() client id: f_00001-7-1 loss: 0.377208  [   64/  265]
train() client id: f_00001-7-2 loss: 0.607177  [   96/  265]
train() client id: f_00001-7-3 loss: 0.337619  [  128/  265]
train() client id: f_00001-7-4 loss: 0.421374  [  160/  265]
train() client id: f_00001-7-5 loss: 0.426680  [  192/  265]
train() client id: f_00001-7-6 loss: 0.375113  [  224/  265]
train() client id: f_00001-7-7 loss: 0.294532  [  256/  265]
train() client id: f_00001-8-0 loss: 0.332817  [   32/  265]
train() client id: f_00001-8-1 loss: 0.337730  [   64/  265]
train() client id: f_00001-8-2 loss: 0.340663  [   96/  265]
train() client id: f_00001-8-3 loss: 0.311505  [  128/  265]
train() client id: f_00001-8-4 loss: 0.329868  [  160/  265]
train() client id: f_00001-8-5 loss: 0.542133  [  192/  265]
train() client id: f_00001-8-6 loss: 0.463911  [  224/  265]
train() client id: f_00001-8-7 loss: 0.480250  [  256/  265]
train() client id: f_00001-9-0 loss: 0.290454  [   32/  265]
train() client id: f_00001-9-1 loss: 0.470831  [   64/  265]
train() client id: f_00001-9-2 loss: 0.373376  [   96/  265]
train() client id: f_00001-9-3 loss: 0.303302  [  128/  265]
train() client id: f_00001-9-4 loss: 0.348245  [  160/  265]
train() client id: f_00001-9-5 loss: 0.332657  [  192/  265]
train() client id: f_00001-9-6 loss: 0.523983  [  224/  265]
train() client id: f_00001-9-7 loss: 0.467496  [  256/  265]
train() client id: f_00001-10-0 loss: 0.361786  [   32/  265]
train() client id: f_00001-10-1 loss: 0.294402  [   64/  265]
train() client id: f_00001-10-2 loss: 0.517597  [   96/  265]
train() client id: f_00001-10-3 loss: 0.295847  [  128/  265]
train() client id: f_00001-10-4 loss: 0.333324  [  160/  265]
train() client id: f_00001-10-5 loss: 0.548138  [  192/  265]
train() client id: f_00001-10-6 loss: 0.411053  [  224/  265]
train() client id: f_00001-10-7 loss: 0.378542  [  256/  265]
train() client id: f_00002-0-0 loss: 1.102513  [   32/  124]
train() client id: f_00002-0-1 loss: 0.757365  [   64/  124]
train() client id: f_00002-0-2 loss: 0.697031  [   96/  124]
train() client id: f_00002-1-0 loss: 0.892024  [   32/  124]
train() client id: f_00002-1-1 loss: 0.698904  [   64/  124]
train() client id: f_00002-1-2 loss: 0.902040  [   96/  124]
train() client id: f_00002-2-0 loss: 0.741836  [   32/  124]
train() client id: f_00002-2-1 loss: 0.792756  [   64/  124]
train() client id: f_00002-2-2 loss: 0.945229  [   96/  124]
train() client id: f_00002-3-0 loss: 0.759727  [   32/  124]
train() client id: f_00002-3-1 loss: 0.789885  [   64/  124]
train() client id: f_00002-3-2 loss: 0.933587  [   96/  124]
train() client id: f_00002-4-0 loss: 0.792466  [   32/  124]
train() client id: f_00002-4-1 loss: 0.845000  [   64/  124]
train() client id: f_00002-4-2 loss: 0.814130  [   96/  124]
train() client id: f_00002-5-0 loss: 0.872005  [   32/  124]
train() client id: f_00002-5-1 loss: 0.734385  [   64/  124]
train() client id: f_00002-5-2 loss: 0.856583  [   96/  124]
train() client id: f_00002-6-0 loss: 0.635745  [   32/  124]
train() client id: f_00002-6-1 loss: 0.849634  [   64/  124]
train() client id: f_00002-6-2 loss: 0.798992  [   96/  124]
train() client id: f_00002-7-0 loss: 0.898463  [   32/  124]
train() client id: f_00002-7-1 loss: 0.893963  [   64/  124]
train() client id: f_00002-7-2 loss: 0.649624  [   96/  124]
train() client id: f_00002-8-0 loss: 0.888774  [   32/  124]
train() client id: f_00002-8-1 loss: 0.894199  [   64/  124]
train() client id: f_00002-8-2 loss: 0.853356  [   96/  124]
train() client id: f_00002-9-0 loss: 0.845721  [   32/  124]
train() client id: f_00002-9-1 loss: 0.987513  [   64/  124]
train() client id: f_00002-9-2 loss: 0.810842  [   96/  124]
train() client id: f_00002-10-0 loss: 0.867266  [   32/  124]
train() client id: f_00002-10-1 loss: 0.944581  [   64/  124]
train() client id: f_00002-10-2 loss: 0.816738  [   96/  124]
train() client id: f_00003-0-0 loss: 0.483187  [   32/   43]
train() client id: f_00003-1-0 loss: 0.308304  [   32/   43]
train() client id: f_00003-2-0 loss: 0.518438  [   32/   43]
train() client id: f_00003-3-0 loss: 0.399334  [   32/   43]
train() client id: f_00003-4-0 loss: 0.528593  [   32/   43]
train() client id: f_00003-5-0 loss: 0.458936  [   32/   43]
train() client id: f_00003-6-0 loss: 0.482567  [   32/   43]
train() client id: f_00003-7-0 loss: 0.376648  [   32/   43]
train() client id: f_00003-8-0 loss: 0.573026  [   32/   43]
train() client id: f_00003-9-0 loss: 0.278290  [   32/   43]
train() client id: f_00003-10-0 loss: 0.187129  [   32/   43]
train() client id: f_00004-0-0 loss: 1.054626  [   32/  306]
train() client id: f_00004-0-1 loss: 0.861551  [   64/  306]
train() client id: f_00004-0-2 loss: 0.969406  [   96/  306]
train() client id: f_00004-0-3 loss: 0.886608  [  128/  306]
train() client id: f_00004-0-4 loss: 1.031216  [  160/  306]
train() client id: f_00004-0-5 loss: 0.856679  [  192/  306]
train() client id: f_00004-0-6 loss: 0.875508  [  224/  306]
train() client id: f_00004-0-7 loss: 0.866237  [  256/  306]
train() client id: f_00004-0-8 loss: 0.848614  [  288/  306]
train() client id: f_00004-1-0 loss: 0.991821  [   32/  306]
train() client id: f_00004-1-1 loss: 0.908114  [   64/  306]
train() client id: f_00004-1-2 loss: 0.897471  [   96/  306]
train() client id: f_00004-1-3 loss: 1.036144  [  128/  306]
train() client id: f_00004-1-4 loss: 0.931859  [  160/  306]
train() client id: f_00004-1-5 loss: 0.880706  [  192/  306]
train() client id: f_00004-1-6 loss: 0.750830  [  224/  306]
train() client id: f_00004-1-7 loss: 0.885372  [  256/  306]
train() client id: f_00004-1-8 loss: 0.914414  [  288/  306]
train() client id: f_00004-2-0 loss: 0.738023  [   32/  306]
train() client id: f_00004-2-1 loss: 0.981197  [   64/  306]
train() client id: f_00004-2-2 loss: 0.981836  [   96/  306]
train() client id: f_00004-2-3 loss: 0.940967  [  128/  306]
train() client id: f_00004-2-4 loss: 0.728259  [  160/  306]
train() client id: f_00004-2-5 loss: 0.869510  [  192/  306]
train() client id: f_00004-2-6 loss: 1.029671  [  224/  306]
train() client id: f_00004-2-7 loss: 1.088077  [  256/  306]
train() client id: f_00004-2-8 loss: 0.906583  [  288/  306]
train() client id: f_00004-3-0 loss: 0.960719  [   32/  306]
train() client id: f_00004-3-1 loss: 0.970355  [   64/  306]
train() client id: f_00004-3-2 loss: 0.883929  [   96/  306]
train() client id: f_00004-3-3 loss: 0.889616  [  128/  306]
train() client id: f_00004-3-4 loss: 0.871917  [  160/  306]
train() client id: f_00004-3-5 loss: 0.942348  [  192/  306]
train() client id: f_00004-3-6 loss: 0.871937  [  224/  306]
train() client id: f_00004-3-7 loss: 0.828283  [  256/  306]
train() client id: f_00004-3-8 loss: 0.888799  [  288/  306]
train() client id: f_00004-4-0 loss: 0.976295  [   32/  306]
train() client id: f_00004-4-1 loss: 0.887870  [   64/  306]
train() client id: f_00004-4-2 loss: 0.900920  [   96/  306]
train() client id: f_00004-4-3 loss: 0.822143  [  128/  306]
train() client id: f_00004-4-4 loss: 0.874600  [  160/  306]
train() client id: f_00004-4-5 loss: 0.870228  [  192/  306]
train() client id: f_00004-4-6 loss: 1.101867  [  224/  306]
train() client id: f_00004-4-7 loss: 0.788687  [  256/  306]
train() client id: f_00004-4-8 loss: 0.926854  [  288/  306]
train() client id: f_00004-5-0 loss: 0.833538  [   32/  306]
train() client id: f_00004-5-1 loss: 0.900162  [   64/  306]
train() client id: f_00004-5-2 loss: 0.869220  [   96/  306]
train() client id: f_00004-5-3 loss: 0.925096  [  128/  306]
train() client id: f_00004-5-4 loss: 0.928141  [  160/  306]
train() client id: f_00004-5-5 loss: 0.845583  [  192/  306]
train() client id: f_00004-5-6 loss: 1.002000  [  224/  306]
train() client id: f_00004-5-7 loss: 0.992893  [  256/  306]
train() client id: f_00004-5-8 loss: 0.892672  [  288/  306]
train() client id: f_00004-6-0 loss: 0.934753  [   32/  306]
train() client id: f_00004-6-1 loss: 1.009231  [   64/  306]
train() client id: f_00004-6-2 loss: 0.918102  [   96/  306]
train() client id: f_00004-6-3 loss: 0.969245  [  128/  306]
train() client id: f_00004-6-4 loss: 0.852052  [  160/  306]
train() client id: f_00004-6-5 loss: 0.935970  [  192/  306]
train() client id: f_00004-6-6 loss: 0.892732  [  224/  306]
train() client id: f_00004-6-7 loss: 0.786532  [  256/  306]
train() client id: f_00004-6-8 loss: 0.833527  [  288/  306]
train() client id: f_00004-7-0 loss: 1.029549  [   32/  306]
train() client id: f_00004-7-1 loss: 0.981728  [   64/  306]
train() client id: f_00004-7-2 loss: 0.975472  [   96/  306]
train() client id: f_00004-7-3 loss: 0.819147  [  128/  306]
train() client id: f_00004-7-4 loss: 0.891034  [  160/  306]
train() client id: f_00004-7-5 loss: 0.796683  [  192/  306]
train() client id: f_00004-7-6 loss: 0.767463  [  224/  306]
train() client id: f_00004-7-7 loss: 0.899470  [  256/  306]
train() client id: f_00004-7-8 loss: 0.842702  [  288/  306]
train() client id: f_00004-8-0 loss: 0.831374  [   32/  306]
train() client id: f_00004-8-1 loss: 0.955396  [   64/  306]
train() client id: f_00004-8-2 loss: 0.953211  [   96/  306]
train() client id: f_00004-8-3 loss: 0.933176  [  128/  306]
train() client id: f_00004-8-4 loss: 0.755696  [  160/  306]
train() client id: f_00004-8-5 loss: 0.900517  [  192/  306]
train() client id: f_00004-8-6 loss: 0.879015  [  224/  306]
train() client id: f_00004-8-7 loss: 0.921339  [  256/  306]
train() client id: f_00004-8-8 loss: 0.886187  [  288/  306]
train() client id: f_00004-9-0 loss: 0.870902  [   32/  306]
train() client id: f_00004-9-1 loss: 0.909860  [   64/  306]
train() client id: f_00004-9-2 loss: 0.787739  [   96/  306]
train() client id: f_00004-9-3 loss: 0.947553  [  128/  306]
train() client id: f_00004-9-4 loss: 0.888126  [  160/  306]
train() client id: f_00004-9-5 loss: 0.943499  [  192/  306]
train() client id: f_00004-9-6 loss: 0.953578  [  224/  306]
train() client id: f_00004-9-7 loss: 0.975112  [  256/  306]
train() client id: f_00004-9-8 loss: 0.738301  [  288/  306]
train() client id: f_00004-10-0 loss: 0.907467  [   32/  306]
train() client id: f_00004-10-1 loss: 0.865302  [   64/  306]
train() client id: f_00004-10-2 loss: 0.872753  [   96/  306]
train() client id: f_00004-10-3 loss: 0.886251  [  128/  306]
train() client id: f_00004-10-4 loss: 0.995164  [  160/  306]
train() client id: f_00004-10-5 loss: 0.823628  [  192/  306]
train() client id: f_00004-10-6 loss: 0.904285  [  224/  306]
train() client id: f_00004-10-7 loss: 0.942464  [  256/  306]
train() client id: f_00004-10-8 loss: 0.762070  [  288/  306]
train() client id: f_00005-0-0 loss: 0.847350  [   32/  146]
train() client id: f_00005-0-1 loss: 0.892661  [   64/  146]
train() client id: f_00005-0-2 loss: 0.791602  [   96/  146]
train() client id: f_00005-0-3 loss: 0.867406  [  128/  146]
train() client id: f_00005-1-0 loss: 0.533980  [   32/  146]
train() client id: f_00005-1-1 loss: 0.983549  [   64/  146]
train() client id: f_00005-1-2 loss: 0.722362  [   96/  146]
train() client id: f_00005-1-3 loss: 1.121685  [  128/  146]
train() client id: f_00005-2-0 loss: 0.868592  [   32/  146]
train() client id: f_00005-2-1 loss: 0.874010  [   64/  146]
train() client id: f_00005-2-2 loss: 0.762494  [   96/  146]
train() client id: f_00005-2-3 loss: 0.586135  [  128/  146]
train() client id: f_00005-3-0 loss: 0.971008  [   32/  146]
train() client id: f_00005-3-1 loss: 0.588969  [   64/  146]
train() client id: f_00005-3-2 loss: 0.769919  [   96/  146]
train() client id: f_00005-3-3 loss: 1.028923  [  128/  146]
train() client id: f_00005-4-0 loss: 0.603352  [   32/  146]
train() client id: f_00005-4-1 loss: 0.921803  [   64/  146]
train() client id: f_00005-4-2 loss: 0.863803  [   96/  146]
train() client id: f_00005-4-3 loss: 0.745156  [  128/  146]
train() client id: f_00005-5-0 loss: 1.023625  [   32/  146]
train() client id: f_00005-5-1 loss: 0.763372  [   64/  146]
train() client id: f_00005-5-2 loss: 0.809621  [   96/  146]
train() client id: f_00005-5-3 loss: 0.674709  [  128/  146]
train() client id: f_00005-6-0 loss: 1.008756  [   32/  146]
train() client id: f_00005-6-1 loss: 0.721752  [   64/  146]
train() client id: f_00005-6-2 loss: 0.644789  [   96/  146]
train() client id: f_00005-6-3 loss: 0.966176  [  128/  146]
train() client id: f_00005-7-0 loss: 0.788067  [   32/  146]
train() client id: f_00005-7-1 loss: 0.888684  [   64/  146]
train() client id: f_00005-7-2 loss: 0.762490  [   96/  146]
train() client id: f_00005-7-3 loss: 0.930411  [  128/  146]
train() client id: f_00005-8-0 loss: 0.619331  [   32/  146]
train() client id: f_00005-8-1 loss: 0.692163  [   64/  146]
train() client id: f_00005-8-2 loss: 0.936985  [   96/  146]
train() client id: f_00005-8-3 loss: 1.068810  [  128/  146]
train() client id: f_00005-9-0 loss: 0.784340  [   32/  146]
train() client id: f_00005-9-1 loss: 0.798225  [   64/  146]
train() client id: f_00005-9-2 loss: 0.743775  [   96/  146]
train() client id: f_00005-9-3 loss: 0.793939  [  128/  146]
train() client id: f_00005-10-0 loss: 0.608996  [   32/  146]
train() client id: f_00005-10-1 loss: 1.124338  [   64/  146]
train() client id: f_00005-10-2 loss: 0.802253  [   96/  146]
train() client id: f_00005-10-3 loss: 0.933147  [  128/  146]
train() client id: f_00006-0-0 loss: 0.552774  [   32/   54]
train() client id: f_00006-1-0 loss: 0.493340  [   32/   54]
train() client id: f_00006-2-0 loss: 0.500990  [   32/   54]
train() client id: f_00006-3-0 loss: 0.511127  [   32/   54]
train() client id: f_00006-4-0 loss: 0.552593  [   32/   54]
train() client id: f_00006-5-0 loss: 0.464811  [   32/   54]
train() client id: f_00006-6-0 loss: 0.572185  [   32/   54]
train() client id: f_00006-7-0 loss: 0.456422  [   32/   54]
train() client id: f_00006-8-0 loss: 0.513946  [   32/   54]
train() client id: f_00006-9-0 loss: 0.506372  [   32/   54]
train() client id: f_00006-10-0 loss: 0.452852  [   32/   54]
train() client id: f_00007-0-0 loss: 0.679400  [   32/  179]
train() client id: f_00007-0-1 loss: 0.604740  [   64/  179]
train() client id: f_00007-0-2 loss: 0.531251  [   96/  179]
train() client id: f_00007-0-3 loss: 0.639840  [  128/  179]
train() client id: f_00007-0-4 loss: 0.839844  [  160/  179]
train() client id: f_00007-1-0 loss: 0.413220  [   32/  179]
train() client id: f_00007-1-1 loss: 0.586865  [   64/  179]
train() client id: f_00007-1-2 loss: 0.651893  [   96/  179]
train() client id: f_00007-1-3 loss: 0.552915  [  128/  179]
train() client id: f_00007-1-4 loss: 0.847715  [  160/  179]
train() client id: f_00007-2-0 loss: 0.642195  [   32/  179]
train() client id: f_00007-2-1 loss: 0.639388  [   64/  179]
train() client id: f_00007-2-2 loss: 0.542368  [   96/  179]
train() client id: f_00007-2-3 loss: 0.727101  [  128/  179]
train() client id: f_00007-2-4 loss: 0.637450  [  160/  179]
train() client id: f_00007-3-0 loss: 0.714510  [   32/  179]
train() client id: f_00007-3-1 loss: 0.685634  [   64/  179]
train() client id: f_00007-3-2 loss: 0.588184  [   96/  179]
train() client id: f_00007-3-3 loss: 0.610011  [  128/  179]
train() client id: f_00007-3-4 loss: 0.550533  [  160/  179]
train() client id: f_00007-4-0 loss: 0.653035  [   32/  179]
train() client id: f_00007-4-1 loss: 0.555790  [   64/  179]
train() client id: f_00007-4-2 loss: 0.729832  [   96/  179]
train() client id: f_00007-4-3 loss: 0.531097  [  128/  179]
train() client id: f_00007-4-4 loss: 0.477148  [  160/  179]
train() client id: f_00007-5-0 loss: 0.702381  [   32/  179]
train() client id: f_00007-5-1 loss: 0.745371  [   64/  179]
train() client id: f_00007-5-2 loss: 0.732677  [   96/  179]
train() client id: f_00007-5-3 loss: 0.440429  [  128/  179]
train() client id: f_00007-5-4 loss: 0.432801  [  160/  179]
train() client id: f_00007-6-0 loss: 0.786344  [   32/  179]
train() client id: f_00007-6-1 loss: 0.563070  [   64/  179]
train() client id: f_00007-6-2 loss: 0.566467  [   96/  179]
train() client id: f_00007-6-3 loss: 0.719513  [  128/  179]
train() client id: f_00007-6-4 loss: 0.527314  [  160/  179]
train() client id: f_00007-7-0 loss: 0.712980  [   32/  179]
train() client id: f_00007-7-1 loss: 0.602287  [   64/  179]
train() client id: f_00007-7-2 loss: 0.527050  [   96/  179]
train() client id: f_00007-7-3 loss: 0.586789  [  128/  179]
train() client id: f_00007-7-4 loss: 0.550789  [  160/  179]
train() client id: f_00007-8-0 loss: 0.835047  [   32/  179]
train() client id: f_00007-8-1 loss: 0.745897  [   64/  179]
train() client id: f_00007-8-2 loss: 0.444615  [   96/  179]
train() client id: f_00007-8-3 loss: 0.461102  [  128/  179]
train() client id: f_00007-8-4 loss: 0.569058  [  160/  179]
train() client id: f_00007-9-0 loss: 0.649848  [   32/  179]
train() client id: f_00007-9-1 loss: 0.539108  [   64/  179]
train() client id: f_00007-9-2 loss: 0.648470  [   96/  179]
train() client id: f_00007-9-3 loss: 0.751687  [  128/  179]
train() client id: f_00007-9-4 loss: 0.567159  [  160/  179]
train() client id: f_00007-10-0 loss: 0.771335  [   32/  179]
train() client id: f_00007-10-1 loss: 0.641376  [   64/  179]
train() client id: f_00007-10-2 loss: 0.645512  [   96/  179]
train() client id: f_00007-10-3 loss: 0.427316  [  128/  179]
train() client id: f_00007-10-4 loss: 0.657580  [  160/  179]
train() client id: f_00008-0-0 loss: 0.806670  [   32/  130]
train() client id: f_00008-0-1 loss: 0.883596  [   64/  130]
train() client id: f_00008-0-2 loss: 0.767455  [   96/  130]
train() client id: f_00008-0-3 loss: 0.884682  [  128/  130]
train() client id: f_00008-1-0 loss: 0.777152  [   32/  130]
train() client id: f_00008-1-1 loss: 0.856615  [   64/  130]
train() client id: f_00008-1-2 loss: 0.878994  [   96/  130]
train() client id: f_00008-1-3 loss: 0.785385  [  128/  130]
train() client id: f_00008-2-0 loss: 0.758169  [   32/  130]
train() client id: f_00008-2-1 loss: 0.775090  [   64/  130]
train() client id: f_00008-2-2 loss: 0.918609  [   96/  130]
train() client id: f_00008-2-3 loss: 0.891075  [  128/  130]
train() client id: f_00008-3-0 loss: 0.802707  [   32/  130]
train() client id: f_00008-3-1 loss: 0.852696  [   64/  130]
train() client id: f_00008-3-2 loss: 0.876889  [   96/  130]
train() client id: f_00008-3-3 loss: 0.797555  [  128/  130]
train() client id: f_00008-4-0 loss: 0.817197  [   32/  130]
train() client id: f_00008-4-1 loss: 0.855740  [   64/  130]
train() client id: f_00008-4-2 loss: 0.915895  [   96/  130]
train() client id: f_00008-4-3 loss: 0.752889  [  128/  130]
train() client id: f_00008-5-0 loss: 0.773818  [   32/  130]
train() client id: f_00008-5-1 loss: 0.916131  [   64/  130]
train() client id: f_00008-5-2 loss: 0.816097  [   96/  130]
train() client id: f_00008-5-3 loss: 0.798118  [  128/  130]
train() client id: f_00008-6-0 loss: 0.798550  [   32/  130]
train() client id: f_00008-6-1 loss: 0.779203  [   64/  130]
train() client id: f_00008-6-2 loss: 0.944541  [   96/  130]
train() client id: f_00008-6-3 loss: 0.795770  [  128/  130]
train() client id: f_00008-7-0 loss: 0.783682  [   32/  130]
train() client id: f_00008-7-1 loss: 0.814198  [   64/  130]
train() client id: f_00008-7-2 loss: 0.850858  [   96/  130]
train() client id: f_00008-7-3 loss: 0.862712  [  128/  130]
train() client id: f_00008-8-0 loss: 0.845462  [   32/  130]
train() client id: f_00008-8-1 loss: 0.830361  [   64/  130]
train() client id: f_00008-8-2 loss: 0.826150  [   96/  130]
train() client id: f_00008-8-3 loss: 0.825645  [  128/  130]
train() client id: f_00008-9-0 loss: 0.911385  [   32/  130]
train() client id: f_00008-9-1 loss: 0.891201  [   64/  130]
train() client id: f_00008-9-2 loss: 0.782742  [   96/  130]
train() client id: f_00008-9-3 loss: 0.736414  [  128/  130]
train() client id: f_00008-10-0 loss: 0.753681  [   32/  130]
train() client id: f_00008-10-1 loss: 0.847670  [   64/  130]
train() client id: f_00008-10-2 loss: 0.862425  [   96/  130]
train() client id: f_00008-10-3 loss: 0.817918  [  128/  130]
train() client id: f_00009-0-0 loss: 1.031272  [   32/  118]
train() client id: f_00009-0-1 loss: 1.123891  [   64/  118]
train() client id: f_00009-0-2 loss: 1.101083  [   96/  118]
train() client id: f_00009-1-0 loss: 1.093940  [   32/  118]
train() client id: f_00009-1-1 loss: 0.892823  [   64/  118]
train() client id: f_00009-1-2 loss: 1.075454  [   96/  118]
train() client id: f_00009-2-0 loss: 1.000352  [   32/  118]
train() client id: f_00009-2-1 loss: 0.954210  [   64/  118]
train() client id: f_00009-2-2 loss: 1.101538  [   96/  118]
train() client id: f_00009-3-0 loss: 0.986468  [   32/  118]
train() client id: f_00009-3-1 loss: 0.996078  [   64/  118]
train() client id: f_00009-3-2 loss: 0.957080  [   96/  118]
train() client id: f_00009-4-0 loss: 0.856039  [   32/  118]
train() client id: f_00009-4-1 loss: 0.973230  [   64/  118]
train() client id: f_00009-4-2 loss: 0.994950  [   96/  118]
train() client id: f_00009-5-0 loss: 0.859114  [   32/  118]
train() client id: f_00009-5-1 loss: 0.993506  [   64/  118]
train() client id: f_00009-5-2 loss: 0.923485  [   96/  118]
train() client id: f_00009-6-0 loss: 0.947026  [   32/  118]
train() client id: f_00009-6-1 loss: 0.821493  [   64/  118]
train() client id: f_00009-6-2 loss: 1.059831  [   96/  118]
train() client id: f_00009-7-0 loss: 1.056167  [   32/  118]
train() client id: f_00009-7-1 loss: 0.946364  [   64/  118]
train() client id: f_00009-7-2 loss: 0.744576  [   96/  118]
train() client id: f_00009-8-0 loss: 1.020332  [   32/  118]
train() client id: f_00009-8-1 loss: 0.813233  [   64/  118]
train() client id: f_00009-8-2 loss: 0.796739  [   96/  118]
train() client id: f_00009-9-0 loss: 0.889417  [   32/  118]
train() client id: f_00009-9-1 loss: 0.837629  [   64/  118]
train() client id: f_00009-9-2 loss: 0.839518  [   96/  118]
train() client id: f_00009-10-0 loss: 0.747933  [   32/  118]
train() client id: f_00009-10-1 loss: 1.021838  [   64/  118]
train() client id: f_00009-10-2 loss: 0.895973  [   96/  118]
At round 55 accuracy: 0.6472148541114059
At round 55 training accuracy: 0.5868544600938967
At round 55 training loss: 0.8310546174268149
update_location
xs = [  -3.9056584     4.20031788  295.00902392   18.81129433    0.97929623
    3.95640986 -257.44319194 -236.32485185  279.66397685 -222.06087855]
ys = [ 287.5879595   270.55583871    1.32061395 -257.45517586  249.35018685
  232.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [304.50301906 288.47548341 311.49970821 276.83394368 268.65679724
 253.41286859 276.19537943 256.61276636 297.52413334 243.57143847]
dists_bs = [204.81533719 203.08538164 501.07186281 473.89903665 191.04099818
 188.06348039 195.82521531 184.65804049 481.14846951 177.35619392]
uav_gains = [2.45442945e-12 3.35809681e-12 2.16122255e-12 4.27299009e-12
 5.07197201e-12 6.93588738e-12 4.33052174e-12 6.50389323e-12
 2.80382037e-12 8.39484877e-12]
bs_gains = [3.72802792e-11 3.81762994e-11 3.04490705e-12 3.55937522e-12
 4.53043251e-11 4.73414518e-11 4.22728929e-11 4.98268075e-11
 3.41124240e-12 5.57858785e-11]
Round 56
-------------------------------
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.61572451 7.33497615 3.57029297 1.30775167 8.45662183 4.06918315
 1.60870824 5.01890964 3.70473634 3.29930761]
obj_prev = 41.986212112103765
eta_min = 2.748442212366637e-26	eta_max = 0.9375089458427088
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 9.666975258494574	eta = 0.909090909090909
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 21.262798910283387	eta = 0.41331150066296846
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 15.102037364580543	eta = 0.5819187910708925
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 14.000878826985186	eta = 0.6276862641626415
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 13.93490538719279	eta = 0.6306579830804683
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 13.934640394529897	eta = 0.6306699761950073
eta = 0.6306699761950073
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [0.03803835 0.0800013  0.03743457 0.01298134 0.09237885 0.04407619
 0.01630215 0.05403861 0.03924592 0.03562322]
ene_total = [1.3587627  2.13524871 1.36925661 0.66156516 2.4302258  1.25152696
 0.7407784  1.61476761 1.33281256 1.03969587]
ti_comp = [0.93897812 1.03996482 0.92863396 0.97451355 1.04269211 1.04336226
 0.97521529 0.99392524 0.96170282 1.04576067]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [3.90152269e-06 2.95892377e-05 3.80197612e-06 1.43967031e-07
 4.53195465e-05 4.91611198e-06 2.84717213e-07 9.98355239e-06
 4.08490276e-06 2.58353803e-06]
ene_total = [0.43191848 0.18645457 0.45712329 0.34523224 0.18019189 0.17757424
 0.34352564 0.29816856 0.37654616 0.17167282]
optimize_network_iter = 0 obj = 2.96840789621886
eta = 0.6306699761950073
freqs = [20255185.25779217 38463462.35602392 20155718.00296714  6660421.60236042
 44298241.69414774 21122186.81412296  8358231.69852659 27184444.26259059
 20404389.67271167 17032203.97092095]
eta_min = 0.6306699761950091	eta_max = 0.7038160577019408
af = 0.0020423891099591697	bf = 1.0953806736156944	zeta = 0.0022466280209550867	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [8.06674524e-07 6.11783813e-06 7.86092385e-07 2.97664645e-08
 9.37021941e-06 1.01644989e-06 5.88678167e-08 2.06418827e-06
 8.44589985e-07 5.34169471e-07]
ene_total = [1.75441871 0.75517231 1.85682433 1.40253966 0.72849401 0.72103243
 1.3955953  1.21056458 1.52944703 0.69724031]
ti_comp = [0.71791719 0.81890388 0.70757302 0.75345262 0.82163117 0.82230133
 0.75415436 0.77286431 0.74064189 0.82469974]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.95240886e-06 2.11098588e-05 2.89691726e-06 1.06538614e-07
 3.22867276e-05 3.50114543e-06 2.10607755e-07 7.30409071e-06
 3.04668167e-06 1.83767148e-06]
ene_total = [0.53855695 0.23224405 0.56998772 0.43049024 0.22429635 0.22138528
 0.42836106 0.37172334 0.46950708 0.21404676]
optimize_network_iter = 1 obj = 3.70059882926673
eta = 0.7038160577019408
freqs = [20185710.85684811 37218665.59464449 20155718.00296712  6563875.93252434
 42834368.86978544 20420657.36097779  8235336.89300485 26637755.85065275
 20187518.35748624 16456368.51125106]
eta_min = 0.7038160577019519	eta_max = 0.703816057701941
af = 0.0019282538903956608	bf = 1.0953806736156944	zeta = 0.0021210792794352272	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [8.01150298e-07 5.72826139e-06 7.86092385e-07 2.89097636e-08
 8.76115832e-06 9.50052597e-07 5.71494239e-08 1.98200003e-06
 8.26731677e-07 4.98660964e-07]
ene_total = [1.75441816 0.75513375 1.85682433 1.40253957 0.72843371 0.72102585
 1.39559513 1.21055645 1.52944526 0.6972368 ]
ti_comp = [0.71791719 0.81890388 0.70757302 0.75345262 0.82163117 0.82230133
 0.75415436 0.77286431 0.74064189 0.82469974]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.95240886e-06 2.11098588e-05 2.89691726e-06 1.06538614e-07
 3.22867276e-05 3.50114543e-06 2.10607755e-07 7.30409071e-06
 3.04668167e-06 1.83767148e-06]
ene_total = [0.53855695 0.23224405 0.56998772 0.43049024 0.22429635 0.22138528
 0.42836106 0.37172334 0.46950708 0.21404676]
optimize_network_iter = 2 obj = 3.7005988292667333
eta = 0.703816057701941
freqs = [20185710.85684812 37218665.5946445  20155718.00296713  6563875.93252435
 42834368.86978545 20420657.36097779  8235336.89300485 26637755.85065275
 20187518.35748625 16456368.51125106]
Done!
At round 56 eta: 0.703816057701941
At round 56 local rounds: 11.50132443724633
At round 56 global rounds: 30.386642017599986
At round 56 a_n: 8.65748957900329
gradient difference: 0.43493834137916565
train() client id: f_00000-0-0 loss: 1.255926  [   32/  126]
train() client id: f_00000-0-1 loss: 1.208809  [   64/  126]
train() client id: f_00000-0-2 loss: 1.001780  [   96/  126]
train() client id: f_00000-1-0 loss: 0.980220  [   32/  126]
train() client id: f_00000-1-1 loss: 1.180508  [   64/  126]
train() client id: f_00000-1-2 loss: 1.052055  [   96/  126]
train() client id: f_00000-2-0 loss: 0.960361  [   32/  126]
train() client id: f_00000-2-1 loss: 0.950451  [   64/  126]
train() client id: f_00000-2-2 loss: 0.818886  [   96/  126]
train() client id: f_00000-3-0 loss: 0.854888  [   32/  126]
train() client id: f_00000-3-1 loss: 0.988726  [   64/  126]
train() client id: f_00000-3-2 loss: 0.834433  [   96/  126]
train() client id: f_00000-4-0 loss: 0.836327  [   32/  126]
train() client id: f_00000-4-1 loss: 0.939875  [   64/  126]
train() client id: f_00000-4-2 loss: 0.865436  [   96/  126]
train() client id: f_00000-5-0 loss: 0.863739  [   32/  126]
train() client id: f_00000-5-1 loss: 0.877661  [   64/  126]
train() client id: f_00000-5-2 loss: 0.864413  [   96/  126]
train() client id: f_00000-6-0 loss: 0.855887  [   32/  126]
train() client id: f_00000-6-1 loss: 0.922200  [   64/  126]
train() client id: f_00000-6-2 loss: 0.795889  [   96/  126]
train() client id: f_00000-7-0 loss: 0.876218  [   32/  126]
train() client id: f_00000-7-1 loss: 0.910661  [   64/  126]
train() client id: f_00000-7-2 loss: 0.772773  [   96/  126]
train() client id: f_00000-8-0 loss: 0.817149  [   32/  126]
train() client id: f_00000-8-1 loss: 0.909663  [   64/  126]
train() client id: f_00000-8-2 loss: 0.806368  [   96/  126]
train() client id: f_00000-9-0 loss: 0.832407  [   32/  126]
train() client id: f_00000-9-1 loss: 0.840863  [   64/  126]
train() client id: f_00000-9-2 loss: 0.774843  [   96/  126]
train() client id: f_00000-10-0 loss: 0.895149  [   32/  126]
train() client id: f_00000-10-1 loss: 0.781906  [   64/  126]
train() client id: f_00000-10-2 loss: 0.887444  [   96/  126]
train() client id: f_00001-0-0 loss: 0.363715  [   32/  265]
train() client id: f_00001-0-1 loss: 0.508270  [   64/  265]
train() client id: f_00001-0-2 loss: 0.493594  [   96/  265]
train() client id: f_00001-0-3 loss: 0.402816  [  128/  265]
train() client id: f_00001-0-4 loss: 0.442069  [  160/  265]
train() client id: f_00001-0-5 loss: 0.507518  [  192/  265]
train() client id: f_00001-0-6 loss: 0.331569  [  224/  265]
train() client id: f_00001-0-7 loss: 0.379243  [  256/  265]
train() client id: f_00001-1-0 loss: 0.432081  [   32/  265]
train() client id: f_00001-1-1 loss: 0.585386  [   64/  265]
train() client id: f_00001-1-2 loss: 0.502882  [   96/  265]
train() client id: f_00001-1-3 loss: 0.460099  [  128/  265]
train() client id: f_00001-1-4 loss: 0.336541  [  160/  265]
train() client id: f_00001-1-5 loss: 0.357460  [  192/  265]
train() client id: f_00001-1-6 loss: 0.299927  [  224/  265]
train() client id: f_00001-1-7 loss: 0.458397  [  256/  265]
train() client id: f_00001-2-0 loss: 0.415967  [   32/  265]
train() client id: f_00001-2-1 loss: 0.360095  [   64/  265]
train() client id: f_00001-2-2 loss: 0.379805  [   96/  265]
train() client id: f_00001-2-3 loss: 0.481410  [  128/  265]
train() client id: f_00001-2-4 loss: 0.458992  [  160/  265]
train() client id: f_00001-2-5 loss: 0.426233  [  192/  265]
train() client id: f_00001-2-6 loss: 0.388501  [  224/  265]
train() client id: f_00001-2-7 loss: 0.411830  [  256/  265]
train() client id: f_00001-3-0 loss: 0.394957  [   32/  265]
train() client id: f_00001-3-1 loss: 0.649193  [   64/  265]
train() client id: f_00001-3-2 loss: 0.370632  [   96/  265]
train() client id: f_00001-3-3 loss: 0.324561  [  128/  265]
train() client id: f_00001-3-4 loss: 0.366361  [  160/  265]
train() client id: f_00001-3-5 loss: 0.496984  [  192/  265]
train() client id: f_00001-3-6 loss: 0.342630  [  224/  265]
train() client id: f_00001-3-7 loss: 0.392872  [  256/  265]
train() client id: f_00001-4-0 loss: 0.415516  [   32/  265]
train() client id: f_00001-4-1 loss: 0.392195  [   64/  265]
train() client id: f_00001-4-2 loss: 0.369112  [   96/  265]
train() client id: f_00001-4-3 loss: 0.395744  [  128/  265]
train() client id: f_00001-4-4 loss: 0.492707  [  160/  265]
train() client id: f_00001-4-5 loss: 0.416032  [  192/  265]
train() client id: f_00001-4-6 loss: 0.402490  [  224/  265]
train() client id: f_00001-4-7 loss: 0.437733  [  256/  265]
train() client id: f_00001-5-0 loss: 0.307789  [   32/  265]
train() client id: f_00001-5-1 loss: 0.371647  [   64/  265]
train() client id: f_00001-5-2 loss: 0.392372  [   96/  265]
train() client id: f_00001-5-3 loss: 0.542896  [  128/  265]
train() client id: f_00001-5-4 loss: 0.347739  [  160/  265]
train() client id: f_00001-5-5 loss: 0.426417  [  192/  265]
train() client id: f_00001-5-6 loss: 0.462129  [  224/  265]
train() client id: f_00001-5-7 loss: 0.416216  [  256/  265]
train() client id: f_00001-6-0 loss: 0.391337  [   32/  265]
train() client id: f_00001-6-1 loss: 0.419227  [   64/  265]
train() client id: f_00001-6-2 loss: 0.373398  [   96/  265]
train() client id: f_00001-6-3 loss: 0.375708  [  128/  265]
train() client id: f_00001-6-4 loss: 0.399516  [  160/  265]
train() client id: f_00001-6-5 loss: 0.336071  [  192/  265]
train() client id: f_00001-6-6 loss: 0.352401  [  224/  265]
train() client id: f_00001-6-7 loss: 0.645547  [  256/  265]
train() client id: f_00001-7-0 loss: 0.316881  [   32/  265]
train() client id: f_00001-7-1 loss: 0.516244  [   64/  265]
train() client id: f_00001-7-2 loss: 0.374465  [   96/  265]
train() client id: f_00001-7-3 loss: 0.342646  [  128/  265]
train() client id: f_00001-7-4 loss: 0.469935  [  160/  265]
train() client id: f_00001-7-5 loss: 0.464122  [  192/  265]
train() client id: f_00001-7-6 loss: 0.443042  [  224/  265]
train() client id: f_00001-7-7 loss: 0.338898  [  256/  265]
train() client id: f_00001-8-0 loss: 0.372667  [   32/  265]
train() client id: f_00001-8-1 loss: 0.590934  [   64/  265]
train() client id: f_00001-8-2 loss: 0.363485  [   96/  265]
train() client id: f_00001-8-3 loss: 0.383447  [  128/  265]
train() client id: f_00001-8-4 loss: 0.396184  [  160/  265]
train() client id: f_00001-8-5 loss: 0.461060  [  192/  265]
train() client id: f_00001-8-6 loss: 0.314413  [  224/  265]
train() client id: f_00001-8-7 loss: 0.383895  [  256/  265]
train() client id: f_00001-9-0 loss: 0.324254  [   32/  265]
train() client id: f_00001-9-1 loss: 0.342777  [   64/  265]
train() client id: f_00001-9-2 loss: 0.505292  [   96/  265]
train() client id: f_00001-9-3 loss: 0.560342  [  128/  265]
train() client id: f_00001-9-4 loss: 0.394588  [  160/  265]
train() client id: f_00001-9-5 loss: 0.454459  [  192/  265]
train() client id: f_00001-9-6 loss: 0.313731  [  224/  265]
train() client id: f_00001-9-7 loss: 0.316314  [  256/  265]
train() client id: f_00001-10-0 loss: 0.344413  [   32/  265]
train() client id: f_00001-10-1 loss: 0.364791  [   64/  265]
train() client id: f_00001-10-2 loss: 0.560651  [   96/  265]
train() client id: f_00001-10-3 loss: 0.512089  [  128/  265]
train() client id: f_00001-10-4 loss: 0.299574  [  160/  265]
train() client id: f_00001-10-5 loss: 0.314393  [  192/  265]
train() client id: f_00001-10-6 loss: 0.329655  [  224/  265]
train() client id: f_00001-10-7 loss: 0.496343  [  256/  265]
train() client id: f_00002-0-0 loss: 1.413690  [   32/  124]
train() client id: f_00002-0-1 loss: 1.214018  [   64/  124]
train() client id: f_00002-0-2 loss: 0.916550  [   96/  124]
train() client id: f_00002-1-0 loss: 1.183000  [   32/  124]
train() client id: f_00002-1-1 loss: 0.992274  [   64/  124]
train() client id: f_00002-1-2 loss: 1.280694  [   96/  124]
train() client id: f_00002-2-0 loss: 0.818948  [   32/  124]
train() client id: f_00002-2-1 loss: 1.235487  [   64/  124]
train() client id: f_00002-2-2 loss: 1.099831  [   96/  124]
train() client id: f_00002-3-0 loss: 1.051374  [   32/  124]
train() client id: f_00002-3-1 loss: 1.226686  [   64/  124]
train() client id: f_00002-3-2 loss: 1.059244  [   96/  124]
train() client id: f_00002-4-0 loss: 1.072137  [   32/  124]
train() client id: f_00002-4-1 loss: 1.157614  [   64/  124]
train() client id: f_00002-4-2 loss: 0.943963  [   96/  124]
train() client id: f_00002-5-0 loss: 0.927182  [   32/  124]
train() client id: f_00002-5-1 loss: 1.183892  [   64/  124]
train() client id: f_00002-5-2 loss: 1.134373  [   96/  124]
train() client id: f_00002-6-0 loss: 0.924224  [   32/  124]
train() client id: f_00002-6-1 loss: 1.307755  [   64/  124]
train() client id: f_00002-6-2 loss: 0.854687  [   96/  124]
train() client id: f_00002-7-0 loss: 1.247232  [   32/  124]
train() client id: f_00002-7-1 loss: 0.920177  [   64/  124]
train() client id: f_00002-7-2 loss: 0.970334  [   96/  124]
train() client id: f_00002-8-0 loss: 1.087476  [   32/  124]
train() client id: f_00002-8-1 loss: 1.043805  [   64/  124]
train() client id: f_00002-8-2 loss: 1.046325  [   96/  124]
train() client id: f_00002-9-0 loss: 1.019446  [   32/  124]
train() client id: f_00002-9-1 loss: 0.953673  [   64/  124]
train() client id: f_00002-9-2 loss: 0.940536  [   96/  124]
train() client id: f_00002-10-0 loss: 1.132884  [   32/  124]
train() client id: f_00002-10-1 loss: 0.960320  [   64/  124]
train() client id: f_00002-10-2 loss: 1.054332  [   96/  124]
train() client id: f_00003-0-0 loss: 1.076887  [   32/   43]
train() client id: f_00003-1-0 loss: 0.952123  [   32/   43]
train() client id: f_00003-2-0 loss: 0.924567  [   32/   43]
train() client id: f_00003-3-0 loss: 1.149800  [   32/   43]
train() client id: f_00003-4-0 loss: 1.013057  [   32/   43]
train() client id: f_00003-5-0 loss: 0.819232  [   32/   43]
train() client id: f_00003-6-0 loss: 1.113997  [   32/   43]
train() client id: f_00003-7-0 loss: 0.927842  [   32/   43]
train() client id: f_00003-8-0 loss: 0.866075  [   32/   43]
train() client id: f_00003-9-0 loss: 1.042498  [   32/   43]
train() client id: f_00003-10-0 loss: 0.914006  [   32/   43]
train() client id: f_00004-0-0 loss: 0.924360  [   32/  306]
train() client id: f_00004-0-1 loss: 0.935603  [   64/  306]
train() client id: f_00004-0-2 loss: 0.995716  [   96/  306]
train() client id: f_00004-0-3 loss: 1.037182  [  128/  306]
train() client id: f_00004-0-4 loss: 1.019180  [  160/  306]
train() client id: f_00004-0-5 loss: 0.936416  [  192/  306]
train() client id: f_00004-0-6 loss: 1.103247  [  224/  306]
train() client id: f_00004-0-7 loss: 0.900211  [  256/  306]
train() client id: f_00004-0-8 loss: 0.960125  [  288/  306]
train() client id: f_00004-1-0 loss: 0.776142  [   32/  306]
train() client id: f_00004-1-1 loss: 0.991625  [   64/  306]
train() client id: f_00004-1-2 loss: 0.917336  [   96/  306]
train() client id: f_00004-1-3 loss: 1.029015  [  128/  306]
train() client id: f_00004-1-4 loss: 0.973269  [  160/  306]
train() client id: f_00004-1-5 loss: 0.999162  [  192/  306]
train() client id: f_00004-1-6 loss: 1.013269  [  224/  306]
train() client id: f_00004-1-7 loss: 1.003556  [  256/  306]
train() client id: f_00004-1-8 loss: 1.042046  [  288/  306]
train() client id: f_00004-2-0 loss: 0.896714  [   32/  306]
train() client id: f_00004-2-1 loss: 1.107924  [   64/  306]
train() client id: f_00004-2-2 loss: 0.908024  [   96/  306]
train() client id: f_00004-2-3 loss: 0.963153  [  128/  306]
train() client id: f_00004-2-4 loss: 0.921769  [  160/  306]
train() client id: f_00004-2-5 loss: 0.985182  [  192/  306]
train() client id: f_00004-2-6 loss: 1.076358  [  224/  306]
train() client id: f_00004-2-7 loss: 0.987561  [  256/  306]
train() client id: f_00004-2-8 loss: 0.809681  [  288/  306]
train() client id: f_00004-3-0 loss: 1.052548  [   32/  306]
train() client id: f_00004-3-1 loss: 0.965841  [   64/  306]
train() client id: f_00004-3-2 loss: 1.051013  [   96/  306]
train() client id: f_00004-3-3 loss: 1.042485  [  128/  306]
train() client id: f_00004-3-4 loss: 0.907703  [  160/  306]
train() client id: f_00004-3-5 loss: 0.962174  [  192/  306]
train() client id: f_00004-3-6 loss: 1.008705  [  224/  306]
train() client id: f_00004-3-7 loss: 0.935923  [  256/  306]
train() client id: f_00004-3-8 loss: 0.829716  [  288/  306]
train() client id: f_00004-4-0 loss: 0.898739  [   32/  306]
train() client id: f_00004-4-1 loss: 1.045978  [   64/  306]
train() client id: f_00004-4-2 loss: 0.966978  [   96/  306]
train() client id: f_00004-4-3 loss: 1.109499  [  128/  306]
train() client id: f_00004-4-4 loss: 0.817868  [  160/  306]
train() client id: f_00004-4-5 loss: 0.924184  [  192/  306]
train() client id: f_00004-4-6 loss: 0.813748  [  224/  306]
train() client id: f_00004-4-7 loss: 0.977437  [  256/  306]
train() client id: f_00004-4-8 loss: 1.042023  [  288/  306]
train() client id: f_00004-5-0 loss: 0.898301  [   32/  306]
train() client id: f_00004-5-1 loss: 0.993292  [   64/  306]
train() client id: f_00004-5-2 loss: 1.107981  [   96/  306]
train() client id: f_00004-5-3 loss: 0.877685  [  128/  306]
train() client id: f_00004-5-4 loss: 0.977352  [  160/  306]
train() client id: f_00004-5-5 loss: 0.823199  [  192/  306]
train() client id: f_00004-5-6 loss: 1.033961  [  224/  306]
train() client id: f_00004-5-7 loss: 0.927878  [  256/  306]
train() client id: f_00004-5-8 loss: 1.019275  [  288/  306]
train() client id: f_00004-6-0 loss: 0.923715  [   32/  306]
train() client id: f_00004-6-1 loss: 0.909857  [   64/  306]
train() client id: f_00004-6-2 loss: 0.959701  [   96/  306]
train() client id: f_00004-6-3 loss: 0.987418  [  128/  306]
train() client id: f_00004-6-4 loss: 0.945106  [  160/  306]
train() client id: f_00004-6-5 loss: 1.014526  [  192/  306]
train() client id: f_00004-6-6 loss: 0.947551  [  224/  306]
train() client id: f_00004-6-7 loss: 1.000688  [  256/  306]
train() client id: f_00004-6-8 loss: 0.973539  [  288/  306]
train() client id: f_00004-7-0 loss: 0.887345  [   32/  306]
train() client id: f_00004-7-1 loss: 1.052419  [   64/  306]
train() client id: f_00004-7-2 loss: 0.959920  [   96/  306]
train() client id: f_00004-7-3 loss: 0.879335  [  128/  306]
train() client id: f_00004-7-4 loss: 1.118945  [  160/  306]
train() client id: f_00004-7-5 loss: 0.983821  [  192/  306]
train() client id: f_00004-7-6 loss: 0.850130  [  224/  306]
train() client id: f_00004-7-7 loss: 0.941705  [  256/  306]
train() client id: f_00004-7-8 loss: 0.802846  [  288/  306]
train() client id: f_00004-8-0 loss: 0.965165  [   32/  306]
train() client id: f_00004-8-1 loss: 0.920988  [   64/  306]
train() client id: f_00004-8-2 loss: 0.858024  [   96/  306]
train() client id: f_00004-8-3 loss: 0.995978  [  128/  306]
train() client id: f_00004-8-4 loss: 1.051992  [  160/  306]
train() client id: f_00004-8-5 loss: 0.873565  [  192/  306]
train() client id: f_00004-8-6 loss: 1.031470  [  224/  306]
train() client id: f_00004-8-7 loss: 0.853332  [  256/  306]
train() client id: f_00004-8-8 loss: 1.051789  [  288/  306]
train() client id: f_00004-9-0 loss: 0.956578  [   32/  306]
train() client id: f_00004-9-1 loss: 1.027214  [   64/  306]
train() client id: f_00004-9-2 loss: 0.971582  [   96/  306]
train() client id: f_00004-9-3 loss: 0.920378  [  128/  306]
train() client id: f_00004-9-4 loss: 0.877442  [  160/  306]
train() client id: f_00004-9-5 loss: 1.030441  [  192/  306]
train() client id: f_00004-9-6 loss: 0.894048  [  224/  306]
train() client id: f_00004-9-7 loss: 0.888168  [  256/  306]
train() client id: f_00004-9-8 loss: 1.067363  [  288/  306]
train() client id: f_00004-10-0 loss: 0.949811  [   32/  306]
train() client id: f_00004-10-1 loss: 1.013692  [   64/  306]
train() client id: f_00004-10-2 loss: 0.953209  [   96/  306]
train() client id: f_00004-10-3 loss: 0.900244  [  128/  306]
train() client id: f_00004-10-4 loss: 0.996951  [  160/  306]
train() client id: f_00004-10-5 loss: 0.967311  [  192/  306]
train() client id: f_00004-10-6 loss: 0.990442  [  224/  306]
train() client id: f_00004-10-7 loss: 1.005514  [  256/  306]
train() client id: f_00004-10-8 loss: 0.840982  [  288/  306]
train() client id: f_00005-0-0 loss: 0.549814  [   32/  146]
train() client id: f_00005-0-1 loss: 0.360913  [   64/  146]
train() client id: f_00005-0-2 loss: 0.695559  [   96/  146]
train() client id: f_00005-0-3 loss: 0.693389  [  128/  146]
train() client id: f_00005-1-0 loss: 0.419320  [   32/  146]
train() client id: f_00005-1-1 loss: 0.704040  [   64/  146]
train() client id: f_00005-1-2 loss: 0.337533  [   96/  146]
train() client id: f_00005-1-3 loss: 0.561618  [  128/  146]
train() client id: f_00005-2-0 loss: 0.228472  [   32/  146]
train() client id: f_00005-2-1 loss: 0.561900  [   64/  146]
train() client id: f_00005-2-2 loss: 0.571674  [   96/  146]
train() client id: f_00005-2-3 loss: 0.697616  [  128/  146]
train() client id: f_00005-3-0 loss: 0.406865  [   32/  146]
train() client id: f_00005-3-1 loss: 0.732296  [   64/  146]
train() client id: f_00005-3-2 loss: 0.384038  [   96/  146]
train() client id: f_00005-3-3 loss: 0.574747  [  128/  146]
train() client id: f_00005-4-0 loss: 0.613670  [   32/  146]
train() client id: f_00005-4-1 loss: 0.424786  [   64/  146]
train() client id: f_00005-4-2 loss: 0.626409  [   96/  146]
train() client id: f_00005-4-3 loss: 0.397453  [  128/  146]
train() client id: f_00005-5-0 loss: 0.342362  [   32/  146]
train() client id: f_00005-5-1 loss: 0.483725  [   64/  146]
train() client id: f_00005-5-2 loss: 0.706042  [   96/  146]
train() client id: f_00005-5-3 loss: 0.622817  [  128/  146]
train() client id: f_00005-6-0 loss: 0.532470  [   32/  146]
train() client id: f_00005-6-1 loss: 0.541777  [   64/  146]
train() client id: f_00005-6-2 loss: 0.863498  [   96/  146]
train() client id: f_00005-6-3 loss: 0.267925  [  128/  146]
train() client id: f_00005-7-0 loss: 0.642954  [   32/  146]
train() client id: f_00005-7-1 loss: 0.518649  [   64/  146]
train() client id: f_00005-7-2 loss: 0.645577  [   96/  146]
train() client id: f_00005-7-3 loss: 0.323209  [  128/  146]
train() client id: f_00005-8-0 loss: 0.614341  [   32/  146]
train() client id: f_00005-8-1 loss: 0.609296  [   64/  146]
train() client id: f_00005-8-2 loss: 0.767209  [   96/  146]
train() client id: f_00005-8-3 loss: 0.309229  [  128/  146]
train() client id: f_00005-9-0 loss: 0.467470  [   32/  146]
train() client id: f_00005-9-1 loss: 0.511809  [   64/  146]
train() client id: f_00005-9-2 loss: 0.621611  [   96/  146]
train() client id: f_00005-9-3 loss: 0.536296  [  128/  146]
train() client id: f_00005-10-0 loss: 0.420784  [   32/  146]
train() client id: f_00005-10-1 loss: 0.516760  [   64/  146]
train() client id: f_00005-10-2 loss: 0.821184  [   96/  146]
train() client id: f_00005-10-3 loss: 0.439276  [  128/  146]
train() client id: f_00006-0-0 loss: 0.476762  [   32/   54]
train() client id: f_00006-1-0 loss: 0.538514  [   32/   54]
train() client id: f_00006-2-0 loss: 0.596834  [   32/   54]
train() client id: f_00006-3-0 loss: 0.550117  [   32/   54]
train() client id: f_00006-4-0 loss: 0.478972  [   32/   54]
train() client id: f_00006-5-0 loss: 0.567202  [   32/   54]
train() client id: f_00006-6-0 loss: 0.522581  [   32/   54]
train() client id: f_00006-7-0 loss: 0.545562  [   32/   54]
train() client id: f_00006-8-0 loss: 0.553253  [   32/   54]
train() client id: f_00006-9-0 loss: 0.526504  [   32/   54]
train() client id: f_00006-10-0 loss: 0.529455  [   32/   54]
train() client id: f_00007-0-0 loss: 0.458142  [   32/  179]
train() client id: f_00007-0-1 loss: 0.492633  [   64/  179]
train() client id: f_00007-0-2 loss: 0.676965  [   96/  179]
train() client id: f_00007-0-3 loss: 0.616367  [  128/  179]
train() client id: f_00007-0-4 loss: 0.646214  [  160/  179]
train() client id: f_00007-1-0 loss: 0.643063  [   32/  179]
train() client id: f_00007-1-1 loss: 0.584880  [   64/  179]
train() client id: f_00007-1-2 loss: 0.524000  [   96/  179]
train() client id: f_00007-1-3 loss: 0.547566  [  128/  179]
train() client id: f_00007-1-4 loss: 0.627676  [  160/  179]
train() client id: f_00007-2-0 loss: 0.532777  [   32/  179]
train() client id: f_00007-2-1 loss: 0.490424  [   64/  179]
train() client id: f_00007-2-2 loss: 0.822377  [   96/  179]
train() client id: f_00007-2-3 loss: 0.490226  [  128/  179]
train() client id: f_00007-2-4 loss: 0.517732  [  160/  179]
train() client id: f_00007-3-0 loss: 0.503516  [   32/  179]
train() client id: f_00007-3-1 loss: 0.510961  [   64/  179]
train() client id: f_00007-3-2 loss: 0.514315  [   96/  179]
train() client id: f_00007-3-3 loss: 0.534034  [  128/  179]
train() client id: f_00007-3-4 loss: 0.638883  [  160/  179]
train() client id: f_00007-4-0 loss: 0.370661  [   32/  179]
train() client id: f_00007-4-1 loss: 0.871817  [   64/  179]
train() client id: f_00007-4-2 loss: 0.410709  [   96/  179]
train() client id: f_00007-4-3 loss: 0.536196  [  128/  179]
train() client id: f_00007-4-4 loss: 0.610531  [  160/  179]
train() client id: f_00007-5-0 loss: 0.454542  [   32/  179]
train() client id: f_00007-5-1 loss: 0.539610  [   64/  179]
train() client id: f_00007-5-2 loss: 0.517095  [   96/  179]
train() client id: f_00007-5-3 loss: 0.544788  [  128/  179]
train() client id: f_00007-5-4 loss: 0.579606  [  160/  179]
train() client id: f_00007-6-0 loss: 0.480596  [   32/  179]
train() client id: f_00007-6-1 loss: 0.500636  [   64/  179]
train() client id: f_00007-6-2 loss: 0.686514  [   96/  179]
train() client id: f_00007-6-3 loss: 0.494625  [  128/  179]
train() client id: f_00007-6-4 loss: 0.446843  [  160/  179]
train() client id: f_00007-7-0 loss: 0.448548  [   32/  179]
train() client id: f_00007-7-1 loss: 0.435613  [   64/  179]
train() client id: f_00007-7-2 loss: 0.704333  [   96/  179]
train() client id: f_00007-7-3 loss: 0.692471  [  128/  179]
train() client id: f_00007-7-4 loss: 0.409490  [  160/  179]
train() client id: f_00007-8-0 loss: 0.646267  [   32/  179]
train() client id: f_00007-8-1 loss: 0.467645  [   64/  179]
train() client id: f_00007-8-2 loss: 0.473445  [   96/  179]
train() client id: f_00007-8-3 loss: 0.358798  [  128/  179]
train() client id: f_00007-8-4 loss: 0.709291  [  160/  179]
train() client id: f_00007-9-0 loss: 0.524218  [   32/  179]
train() client id: f_00007-9-1 loss: 0.589996  [   64/  179]
train() client id: f_00007-9-2 loss: 0.518673  [   96/  179]
train() client id: f_00007-9-3 loss: 0.501415  [  128/  179]
train() client id: f_00007-9-4 loss: 0.439465  [  160/  179]
train() client id: f_00007-10-0 loss: 0.378493  [   32/  179]
train() client id: f_00007-10-1 loss: 0.613487  [   64/  179]
train() client id: f_00007-10-2 loss: 0.543008  [   96/  179]
train() client id: f_00007-10-3 loss: 0.537817  [  128/  179]
train() client id: f_00007-10-4 loss: 0.441910  [  160/  179]
train() client id: f_00008-0-0 loss: 0.710203  [   32/  130]
train() client id: f_00008-0-1 loss: 0.717248  [   64/  130]
train() client id: f_00008-0-2 loss: 0.732464  [   96/  130]
train() client id: f_00008-0-3 loss: 0.699177  [  128/  130]
train() client id: f_00008-1-0 loss: 0.593231  [   32/  130]
train() client id: f_00008-1-1 loss: 0.625565  [   64/  130]
train() client id: f_00008-1-2 loss: 0.687899  [   96/  130]
train() client id: f_00008-1-3 loss: 0.930404  [  128/  130]
train() client id: f_00008-2-0 loss: 0.720753  [   32/  130]
train() client id: f_00008-2-1 loss: 0.789065  [   64/  130]
train() client id: f_00008-2-2 loss: 0.726692  [   96/  130]
train() client id: f_00008-2-3 loss: 0.635173  [  128/  130]
train() client id: f_00008-3-0 loss: 0.663464  [   32/  130]
train() client id: f_00008-3-1 loss: 0.631812  [   64/  130]
train() client id: f_00008-3-2 loss: 0.821279  [   96/  130]
train() client id: f_00008-3-3 loss: 0.726062  [  128/  130]
train() client id: f_00008-4-0 loss: 0.651221  [   32/  130]
train() client id: f_00008-4-1 loss: 0.668664  [   64/  130]
train() client id: f_00008-4-2 loss: 0.730411  [   96/  130]
train() client id: f_00008-4-3 loss: 0.806562  [  128/  130]
train() client id: f_00008-5-0 loss: 0.750203  [   32/  130]
train() client id: f_00008-5-1 loss: 0.762638  [   64/  130]
train() client id: f_00008-5-2 loss: 0.766168  [   96/  130]
train() client id: f_00008-5-3 loss: 0.586985  [  128/  130]
train() client id: f_00008-6-0 loss: 0.690689  [   32/  130]
train() client id: f_00008-6-1 loss: 0.691509  [   64/  130]
train() client id: f_00008-6-2 loss: 0.722791  [   96/  130]
train() client id: f_00008-6-3 loss: 0.735557  [  128/  130]
train() client id: f_00008-7-0 loss: 0.744212  [   32/  130]
train() client id: f_00008-7-1 loss: 0.584755  [   64/  130]
train() client id: f_00008-7-2 loss: 0.717255  [   96/  130]
train() client id: f_00008-7-3 loss: 0.810731  [  128/  130]
train() client id: f_00008-8-0 loss: 0.751835  [   32/  130]
train() client id: f_00008-8-1 loss: 0.777824  [   64/  130]
train() client id: f_00008-8-2 loss: 0.610600  [   96/  130]
train() client id: f_00008-8-3 loss: 0.666879  [  128/  130]
train() client id: f_00008-9-0 loss: 0.646410  [   32/  130]
train() client id: f_00008-9-1 loss: 0.638727  [   64/  130]
train() client id: f_00008-9-2 loss: 0.884179  [   96/  130]
train() client id: f_00008-9-3 loss: 0.662911  [  128/  130]
train() client id: f_00008-10-0 loss: 0.738942  [   32/  130]
train() client id: f_00008-10-1 loss: 0.707862  [   64/  130]
train() client id: f_00008-10-2 loss: 0.690316  [   96/  130]
train() client id: f_00008-10-3 loss: 0.741151  [  128/  130]
train() client id: f_00009-0-0 loss: 0.902598  [   32/  118]
train() client id: f_00009-0-1 loss: 0.984947  [   64/  118]
train() client id: f_00009-0-2 loss: 1.079484  [   96/  118]
train() client id: f_00009-1-0 loss: 0.928682  [   32/  118]
train() client id: f_00009-1-1 loss: 1.129211  [   64/  118]
train() client id: f_00009-1-2 loss: 0.925757  [   96/  118]
train() client id: f_00009-2-0 loss: 0.883476  [   32/  118]
train() client id: f_00009-2-1 loss: 0.938117  [   64/  118]
train() client id: f_00009-2-2 loss: 0.952067  [   96/  118]
train() client id: f_00009-3-0 loss: 0.967562  [   32/  118]
train() client id: f_00009-3-1 loss: 0.717181  [   64/  118]
train() client id: f_00009-3-2 loss: 0.941029  [   96/  118]
train() client id: f_00009-4-0 loss: 0.808715  [   32/  118]
train() client id: f_00009-4-1 loss: 0.844447  [   64/  118]
train() client id: f_00009-4-2 loss: 1.047842  [   96/  118]
train() client id: f_00009-5-0 loss: 0.802109  [   32/  118]
train() client id: f_00009-5-1 loss: 0.812046  [   64/  118]
train() client id: f_00009-5-2 loss: 0.987260  [   96/  118]
train() client id: f_00009-6-0 loss: 0.872882  [   32/  118]
train() client id: f_00009-6-1 loss: 0.942067  [   64/  118]
train() client id: f_00009-6-2 loss: 0.728457  [   96/  118]
train() client id: f_00009-7-0 loss: 0.742091  [   32/  118]
train() client id: f_00009-7-1 loss: 0.942687  [   64/  118]
train() client id: f_00009-7-2 loss: 0.765959  [   96/  118]
train() client id: f_00009-8-0 loss: 0.887190  [   32/  118]
train() client id: f_00009-8-1 loss: 0.800936  [   64/  118]
train() client id: f_00009-8-2 loss: 0.801620  [   96/  118]
train() client id: f_00009-9-0 loss: 0.743612  [   32/  118]
train() client id: f_00009-9-1 loss: 0.901338  [   64/  118]
train() client id: f_00009-9-2 loss: 0.800841  [   96/  118]
train() client id: f_00009-10-0 loss: 0.774924  [   32/  118]
train() client id: f_00009-10-1 loss: 0.904755  [   64/  118]
train() client id: f_00009-10-2 loss: 0.724996  [   96/  118]
At round 56 accuracy: 0.6472148541114059
At round 56 training accuracy: 0.5915492957746479
At round 56 training loss: 0.8233965578686597
update_location
xs = [  -3.9056584     4.20031788  300.00902392   18.81129433    0.97929623
    3.95640986 -262.44319194 -241.32485185  284.66397685 -227.06087855]
ys = [ 292.5879595   275.55583871    1.32061395 -262.45517586  254.35018685
  237.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [309.22963669 293.17002391 316.23908432 281.48993611 273.30381733
 258.0139986  280.86174452 261.22473155 302.22880354 248.13837757]
dists_bs = [207.60588129 205.4853538  505.76486823 478.46614164 193.04187331
 189.66025984 197.98218826 186.37821878 485.87600216 178.74794631]
uav_gains = [2.25068246e-12 3.05507187e-12 1.99030200e-12 3.87733494e-12
 4.60120712e-12 6.32147895e-12 3.92831823e-12 5.91917228e-12
 2.56156197e-12 7.69388670e-12]
bs_gains = [3.58940992e-11 3.69409145e-11 2.96645552e-12 3.46505949e-12
 4.40017369e-11 4.62338788e-11 4.09959494e-11 4.85498225e-11
 3.31911903e-12 5.45781883e-11]
Round 57
-------------------------------
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.48417658 7.05628152 3.44074209 1.26260335 8.13515315 3.91462212
 1.55207962 4.8313157  3.56521466 3.17400612]
obj_prev = 40.416194911724524
eta_min = 2.86092118758865e-27	eta_max = 0.9374658932840896
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 9.299045348130402	eta = 0.909090909090909
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 20.758435476455084	eta = 0.4072405937720066
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 14.636258285641127	eta = 0.5775846137877274
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.545044928479468	eta = 0.6241158766062833
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.479247501397975	eta = 0.6271624278976032
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.478979113930322	eta = 0.6271749156783474
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.478979109435532	eta = 0.6271749158874893
eta = 0.6271749158874893
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [0.03849692 0.08096575 0.03788586 0.01313784 0.09349252 0.04460754
 0.01649868 0.05469007 0.03971905 0.03605267]
ene_total = [1.32155705 2.05839072 1.33209932 0.6463361  2.34272368 1.20578025
 0.72268136 1.56297838 1.28498919 1.00144307]
ti_comp = [0.9850295  1.0924086  0.97437897 1.022215   1.09523156 1.09599371
 1.02294619 1.0429956  1.01299828 1.09844051]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [3.67501858e-06 2.77980148e-05 3.57977419e-06 1.35633586e-07
 4.25793239e-05 4.61837109e-06 2.68239096e-07 9.39810980e-06
 3.81644508e-06 2.42738867e-06]
ene_total = [0.42769316 0.17890492 0.45242289 0.34126134 0.17269288 0.17004157
 0.3395665  0.29322113 0.36274925 0.16430889]
optimize_network_iter = 0 obj = 2.90286252803159
eta = 0.6271749158874893
freqs = [19540999.15308214 37058362.31216387 19441028.73626838  6426161.89579366
 42681624.07611285 20350273.65967661  8064295.3890079  26217785.78934446
 19604695.49055661 16410844.32139915]
eta_min = 0.6271749158874926	eta_max = 0.7063960382409014
af = 0.0018241050152146168	bf = 1.0814089668242106	zeta = 0.002006515516736079	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [7.50791650e-07 5.67902365e-06 7.31333600e-07 2.77094010e-08
 8.69878619e-06 9.43514808e-07 5.48001783e-08 1.91999638e-06
 7.79684519e-07 4.95905832e-07]
ene_total = [1.75372365 0.73159623 1.85514936 1.39952837 0.7050001  0.6970034
 1.39256766 1.20180996 1.48737301 0.67365931]
ti_comp = [0.73659322 0.84397231 0.72594269 0.77377872 0.84679527 0.84755742
 0.77450991 0.79455932 0.764562   0.85000422]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.71785248e-06 1.92597612e-05 2.66704329e-06 9.78907100e-08
 2.94562262e-05 3.19366957e-06 1.93507141e-07 6.69693547e-06
 2.77060309e-06 1.67638409e-06]
ene_total = [0.54306642 0.22692583 0.57447011 0.4333404  0.21890245 0.21588069
 0.43118717 0.37225931 0.46059653 0.20862108]
optimize_network_iter = 1 obj = 3.685249982672195
eta = 0.7063960382409014
freqs = [19468958.52365561 35736956.5886699  19441028.73626838  6324869.42089178
 41128502.02279741 19605756.94700696  7935360.67409761 25640506.16051596
 19352208.45479425 15800135.59527822]
eta_min = 0.7063960382409034	eta_max = 0.7063960382409006
af = 0.0017119208968572406	bf = 1.0814089668242106	zeta = 0.0018831129865429649	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [7.45266057e-07 5.28124555e-06 7.31333600e-07 2.68427459e-08
 8.07723219e-06 8.75740514e-07 5.30618587e-08 1.83637587e-06
 7.59730875e-07 4.59683582e-07]
ene_total = [1.75372312 0.73155835 1.85514936 1.39952829 0.7049409  0.69699695
 1.39256749 1.20180199 1.48737111 0.67365586]
ti_comp = [0.73659322 0.84397231 0.72594269 0.77377872 0.84679527 0.84755742
 0.77450991 0.79455932 0.764562   0.85000422]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.71785248e-06 1.92597612e-05 2.66704329e-06 9.78907100e-08
 2.94562262e-05 3.19366957e-06 1.93507141e-07 6.69693547e-06
 2.77060309e-06 1.67638409e-06]
ene_total = [0.54306642 0.22692583 0.57447011 0.4333404  0.21890245 0.21588069
 0.43118717 0.37225931 0.46059653 0.20862108]
optimize_network_iter = 2 obj = 3.6852499826721847
eta = 0.7063960382409006
freqs = [19468958.52365561 35736956.5886699  19441028.73626836  6324869.42089178
 41128502.02279742 19605756.94700696  7935360.67409761 25640506.16051596
 19352208.45479425 15800135.59527823]
Done!
At round 57 eta: 0.7063960382409006
At round 57 local rounds: 11.38151016928333
At round 57 global rounds: 29.486964437171718
At round 57 a_n: 8.314943732033974
gradient difference: 0.45807409286499023
train() client id: f_00000-0-0 loss: 1.369357  [   32/  126]
train() client id: f_00000-0-1 loss: 0.968554  [   64/  126]
train() client id: f_00000-0-2 loss: 1.063993  [   96/  126]
train() client id: f_00000-1-0 loss: 1.031920  [   32/  126]
train() client id: f_00000-1-1 loss: 0.917159  [   64/  126]
train() client id: f_00000-1-2 loss: 1.137051  [   96/  126]
train() client id: f_00000-2-0 loss: 1.119194  [   32/  126]
train() client id: f_00000-2-1 loss: 0.769261  [   64/  126]
train() client id: f_00000-2-2 loss: 1.008732  [   96/  126]
train() client id: f_00000-3-0 loss: 0.988684  [   32/  126]
train() client id: f_00000-3-1 loss: 0.893994  [   64/  126]
train() client id: f_00000-3-2 loss: 0.840364  [   96/  126]
train() client id: f_00000-4-0 loss: 1.014460  [   32/  126]
train() client id: f_00000-4-1 loss: 0.727191  [   64/  126]
train() client id: f_00000-4-2 loss: 0.841041  [   96/  126]
train() client id: f_00000-5-0 loss: 0.745889  [   32/  126]
train() client id: f_00000-5-1 loss: 0.772636  [   64/  126]
train() client id: f_00000-5-2 loss: 0.875710  [   96/  126]
train() client id: f_00000-6-0 loss: 0.707910  [   32/  126]
train() client id: f_00000-6-1 loss: 0.814164  [   64/  126]
train() client id: f_00000-6-2 loss: 0.878794  [   96/  126]
train() client id: f_00000-7-0 loss: 0.860900  [   32/  126]
train() client id: f_00000-7-1 loss: 0.884334  [   64/  126]
train() client id: f_00000-7-2 loss: 0.717512  [   96/  126]
train() client id: f_00000-8-0 loss: 0.919691  [   32/  126]
train() client id: f_00000-8-1 loss: 0.786862  [   64/  126]
train() client id: f_00000-8-2 loss: 0.685532  [   96/  126]
train() client id: f_00000-9-0 loss: 0.849480  [   32/  126]
train() client id: f_00000-9-1 loss: 0.720197  [   64/  126]
train() client id: f_00000-9-2 loss: 0.722790  [   96/  126]
train() client id: f_00000-10-0 loss: 0.814496  [   32/  126]
train() client id: f_00000-10-1 loss: 0.926330  [   64/  126]
train() client id: f_00000-10-2 loss: 0.634556  [   96/  126]
train() client id: f_00001-0-0 loss: 0.387098  [   32/  265]
train() client id: f_00001-0-1 loss: 0.474522  [   64/  265]
train() client id: f_00001-0-2 loss: 0.527378  [   96/  265]
train() client id: f_00001-0-3 loss: 0.348548  [  128/  265]
train() client id: f_00001-0-4 loss: 0.324160  [  160/  265]
train() client id: f_00001-0-5 loss: 0.312712  [  192/  265]
train() client id: f_00001-0-6 loss: 0.408874  [  224/  265]
train() client id: f_00001-0-7 loss: 0.435775  [  256/  265]
train() client id: f_00001-1-0 loss: 0.323794  [   32/  265]
train() client id: f_00001-1-1 loss: 0.465021  [   64/  265]
train() client id: f_00001-1-2 loss: 0.414102  [   96/  265]
train() client id: f_00001-1-3 loss: 0.262988  [  128/  265]
train() client id: f_00001-1-4 loss: 0.298740  [  160/  265]
train() client id: f_00001-1-5 loss: 0.521919  [  192/  265]
train() client id: f_00001-1-6 loss: 0.375612  [  224/  265]
train() client id: f_00001-1-7 loss: 0.382106  [  256/  265]
train() client id: f_00001-2-0 loss: 0.394160  [   32/  265]
train() client id: f_00001-2-1 loss: 0.456129  [   64/  265]
train() client id: f_00001-2-2 loss: 0.448995  [   96/  265]
train() client id: f_00001-2-3 loss: 0.357975  [  128/  265]
train() client id: f_00001-2-4 loss: 0.306693  [  160/  265]
train() client id: f_00001-2-5 loss: 0.306253  [  192/  265]
train() client id: f_00001-2-6 loss: 0.495473  [  224/  265]
train() client id: f_00001-2-7 loss: 0.286242  [  256/  265]
train() client id: f_00001-3-0 loss: 0.373845  [   32/  265]
train() client id: f_00001-3-1 loss: 0.317724  [   64/  265]
train() client id: f_00001-3-2 loss: 0.286672  [   96/  265]
train() client id: f_00001-3-3 loss: 0.479102  [  128/  265]
train() client id: f_00001-3-4 loss: 0.477724  [  160/  265]
train() client id: f_00001-3-5 loss: 0.268646  [  192/  265]
train() client id: f_00001-3-6 loss: 0.376558  [  224/  265]
train() client id: f_00001-3-7 loss: 0.374642  [  256/  265]
train() client id: f_00001-4-0 loss: 0.334819  [   32/  265]
train() client id: f_00001-4-1 loss: 0.238166  [   64/  265]
train() client id: f_00001-4-2 loss: 0.277756  [   96/  265]
train() client id: f_00001-4-3 loss: 0.369367  [  128/  265]
train() client id: f_00001-4-4 loss: 0.362039  [  160/  265]
train() client id: f_00001-4-5 loss: 0.464292  [  192/  265]
train() client id: f_00001-4-6 loss: 0.440636  [  224/  265]
train() client id: f_00001-4-7 loss: 0.405554  [  256/  265]
train() client id: f_00001-5-0 loss: 0.352188  [   32/  265]
train() client id: f_00001-5-1 loss: 0.316962  [   64/  265]
train() client id: f_00001-5-2 loss: 0.326563  [   96/  265]
train() client id: f_00001-5-3 loss: 0.536315  [  128/  265]
train() client id: f_00001-5-4 loss: 0.313844  [  160/  265]
train() client id: f_00001-5-5 loss: 0.323789  [  192/  265]
train() client id: f_00001-5-6 loss: 0.406741  [  224/  265]
train() client id: f_00001-5-7 loss: 0.355460  [  256/  265]
train() client id: f_00001-6-0 loss: 0.376040  [   32/  265]
train() client id: f_00001-6-1 loss: 0.360728  [   64/  265]
train() client id: f_00001-6-2 loss: 0.407565  [   96/  265]
train() client id: f_00001-6-3 loss: 0.419143  [  128/  265]
train() client id: f_00001-6-4 loss: 0.351594  [  160/  265]
train() client id: f_00001-6-5 loss: 0.264705  [  192/  265]
train() client id: f_00001-6-6 loss: 0.452482  [  224/  265]
train() client id: f_00001-6-7 loss: 0.289561  [  256/  265]
train() client id: f_00001-7-0 loss: 0.361464  [   32/  265]
train() client id: f_00001-7-1 loss: 0.364105  [   64/  265]
train() client id: f_00001-7-2 loss: 0.425270  [   96/  265]
train() client id: f_00001-7-3 loss: 0.284111  [  128/  265]
train() client id: f_00001-7-4 loss: 0.476884  [  160/  265]
train() client id: f_00001-7-5 loss: 0.314941  [  192/  265]
train() client id: f_00001-7-6 loss: 0.350062  [  224/  265]
train() client id: f_00001-7-7 loss: 0.335952  [  256/  265]
train() client id: f_00001-8-0 loss: 0.430080  [   32/  265]
train() client id: f_00001-8-1 loss: 0.325486  [   64/  265]
train() client id: f_00001-8-2 loss: 0.266649  [   96/  265]
train() client id: f_00001-8-3 loss: 0.276371  [  128/  265]
train() client id: f_00001-8-4 loss: 0.279298  [  160/  265]
train() client id: f_00001-8-5 loss: 0.427243  [  192/  265]
train() client id: f_00001-8-6 loss: 0.319880  [  224/  265]
train() client id: f_00001-8-7 loss: 0.475099  [  256/  265]
train() client id: f_00001-9-0 loss: 0.339692  [   32/  265]
train() client id: f_00001-9-1 loss: 0.346233  [   64/  265]
train() client id: f_00001-9-2 loss: 0.306351  [   96/  265]
train() client id: f_00001-9-3 loss: 0.432585  [  128/  265]
train() client id: f_00001-9-4 loss: 0.263075  [  160/  265]
train() client id: f_00001-9-5 loss: 0.477581  [  192/  265]
train() client id: f_00001-9-6 loss: 0.342411  [  224/  265]
train() client id: f_00001-9-7 loss: 0.372422  [  256/  265]
train() client id: f_00001-10-0 loss: 0.334104  [   32/  265]
train() client id: f_00001-10-1 loss: 0.264285  [   64/  265]
train() client id: f_00001-10-2 loss: 0.416920  [   96/  265]
train() client id: f_00001-10-3 loss: 0.258121  [  128/  265]
train() client id: f_00001-10-4 loss: 0.309158  [  160/  265]
train() client id: f_00001-10-5 loss: 0.505792  [  192/  265]
train() client id: f_00001-10-6 loss: 0.415586  [  224/  265]
train() client id: f_00001-10-7 loss: 0.378702  [  256/  265]
train() client id: f_00002-0-0 loss: 1.095577  [   32/  124]
train() client id: f_00002-0-1 loss: 1.156785  [   64/  124]
train() client id: f_00002-0-2 loss: 0.950662  [   96/  124]
train() client id: f_00002-1-0 loss: 1.005846  [   32/  124]
train() client id: f_00002-1-1 loss: 1.040101  [   64/  124]
train() client id: f_00002-1-2 loss: 1.065362  [   96/  124]
train() client id: f_00002-2-0 loss: 1.102473  [   32/  124]
train() client id: f_00002-2-1 loss: 1.064874  [   64/  124]
train() client id: f_00002-2-2 loss: 0.870919  [   96/  124]
train() client id: f_00002-3-0 loss: 0.893263  [   32/  124]
train() client id: f_00002-3-1 loss: 1.166921  [   64/  124]
train() client id: f_00002-3-2 loss: 0.846456  [   96/  124]
train() client id: f_00002-4-0 loss: 0.934338  [   32/  124]
train() client id: f_00002-4-1 loss: 1.027433  [   64/  124]
train() client id: f_00002-4-2 loss: 0.915534  [   96/  124]
train() client id: f_00002-5-0 loss: 0.811078  [   32/  124]
train() client id: f_00002-5-1 loss: 0.880049  [   64/  124]
train() client id: f_00002-5-2 loss: 0.986262  [   96/  124]
train() client id: f_00002-6-0 loss: 0.828067  [   32/  124]
train() client id: f_00002-6-1 loss: 0.862660  [   64/  124]
train() client id: f_00002-6-2 loss: 1.064365  [   96/  124]
train() client id: f_00002-7-0 loss: 0.910324  [   32/  124]
train() client id: f_00002-7-1 loss: 0.740851  [   64/  124]
train() client id: f_00002-7-2 loss: 0.900972  [   96/  124]
train() client id: f_00002-8-0 loss: 0.758983  [   32/  124]
train() client id: f_00002-8-1 loss: 0.864798  [   64/  124]
train() client id: f_00002-8-2 loss: 1.028388  [   96/  124]
train() client id: f_00002-9-0 loss: 0.784541  [   32/  124]
train() client id: f_00002-9-1 loss: 0.920703  [   64/  124]
train() client id: f_00002-9-2 loss: 0.849687  [   96/  124]
train() client id: f_00002-10-0 loss: 0.929630  [   32/  124]
train() client id: f_00002-10-1 loss: 0.952679  [   64/  124]
train() client id: f_00002-10-2 loss: 0.848637  [   96/  124]
train() client id: f_00003-0-0 loss: 0.842581  [   32/   43]
train() client id: f_00003-1-0 loss: 0.685329  [   32/   43]
train() client id: f_00003-2-0 loss: 0.771932  [   32/   43]
train() client id: f_00003-3-0 loss: 0.802254  [   32/   43]
train() client id: f_00003-4-0 loss: 0.813500  [   32/   43]
train() client id: f_00003-5-0 loss: 0.681760  [   32/   43]
train() client id: f_00003-6-0 loss: 0.769693  [   32/   43]
train() client id: f_00003-7-0 loss: 0.883866  [   32/   43]
train() client id: f_00003-8-0 loss: 0.643998  [   32/   43]
train() client id: f_00003-9-0 loss: 0.833297  [   32/   43]
train() client id: f_00003-10-0 loss: 0.823814  [   32/   43]
train() client id: f_00004-0-0 loss: 0.865257  [   32/  306]
train() client id: f_00004-0-1 loss: 0.986978  [   64/  306]
train() client id: f_00004-0-2 loss: 1.018587  [   96/  306]
train() client id: f_00004-0-3 loss: 0.791504  [  128/  306]
train() client id: f_00004-0-4 loss: 0.842083  [  160/  306]
train() client id: f_00004-0-5 loss: 0.863654  [  192/  306]
train() client id: f_00004-0-6 loss: 0.869947  [  224/  306]
train() client id: f_00004-0-7 loss: 0.902077  [  256/  306]
train() client id: f_00004-0-8 loss: 1.007284  [  288/  306]
train() client id: f_00004-1-0 loss: 1.169511  [   32/  306]
train() client id: f_00004-1-1 loss: 0.799773  [   64/  306]
train() client id: f_00004-1-2 loss: 0.838204  [   96/  306]
train() client id: f_00004-1-3 loss: 0.693937  [  128/  306]
train() client id: f_00004-1-4 loss: 1.093246  [  160/  306]
train() client id: f_00004-1-5 loss: 0.915308  [  192/  306]
train() client id: f_00004-1-6 loss: 0.803476  [  224/  306]
train() client id: f_00004-1-7 loss: 0.853153  [  256/  306]
train() client id: f_00004-1-8 loss: 0.985448  [  288/  306]
train() client id: f_00004-2-0 loss: 0.970923  [   32/  306]
train() client id: f_00004-2-1 loss: 0.978526  [   64/  306]
train() client id: f_00004-2-2 loss: 0.883714  [   96/  306]
train() client id: f_00004-2-3 loss: 0.920511  [  128/  306]
train() client id: f_00004-2-4 loss: 0.888762  [  160/  306]
train() client id: f_00004-2-5 loss: 0.919331  [  192/  306]
train() client id: f_00004-2-6 loss: 0.907484  [  224/  306]
train() client id: f_00004-2-7 loss: 0.742150  [  256/  306]
train() client id: f_00004-2-8 loss: 1.014191  [  288/  306]
train() client id: f_00004-3-0 loss: 0.787678  [   32/  306]
train() client id: f_00004-3-1 loss: 0.898225  [   64/  306]
train() client id: f_00004-3-2 loss: 0.975679  [   96/  306]
train() client id: f_00004-3-3 loss: 0.983400  [  128/  306]
train() client id: f_00004-3-4 loss: 0.900740  [  160/  306]
train() client id: f_00004-3-5 loss: 0.885162  [  192/  306]
train() client id: f_00004-3-6 loss: 0.924327  [  224/  306]
train() client id: f_00004-3-7 loss: 0.927973  [  256/  306]
train() client id: f_00004-3-8 loss: 0.875717  [  288/  306]
train() client id: f_00004-4-0 loss: 0.981115  [   32/  306]
train() client id: f_00004-4-1 loss: 0.769969  [   64/  306]
train() client id: f_00004-4-2 loss: 1.045135  [   96/  306]
train() client id: f_00004-4-3 loss: 0.902637  [  128/  306]
train() client id: f_00004-4-4 loss: 0.940111  [  160/  306]
train() client id: f_00004-4-5 loss: 0.933004  [  192/  306]
train() client id: f_00004-4-6 loss: 0.894847  [  224/  306]
train() client id: f_00004-4-7 loss: 0.906431  [  256/  306]
train() client id: f_00004-4-8 loss: 0.807307  [  288/  306]
train() client id: f_00004-5-0 loss: 0.795676  [   32/  306]
train() client id: f_00004-5-1 loss: 0.938365  [   64/  306]
train() client id: f_00004-5-2 loss: 0.950284  [   96/  306]
train() client id: f_00004-5-3 loss: 0.842508  [  128/  306]
train() client id: f_00004-5-4 loss: 0.849160  [  160/  306]
train() client id: f_00004-5-5 loss: 0.873389  [  192/  306]
train() client id: f_00004-5-6 loss: 0.920743  [  224/  306]
train() client id: f_00004-5-7 loss: 0.823898  [  256/  306]
train() client id: f_00004-5-8 loss: 1.071537  [  288/  306]
train() client id: f_00004-6-0 loss: 0.966313  [   32/  306]
train() client id: f_00004-6-1 loss: 0.860789  [   64/  306]
train() client id: f_00004-6-2 loss: 0.870435  [   96/  306]
train() client id: f_00004-6-3 loss: 0.928083  [  128/  306]
train() client id: f_00004-6-4 loss: 1.039149  [  160/  306]
train() client id: f_00004-6-5 loss: 0.899138  [  192/  306]
train() client id: f_00004-6-6 loss: 0.723353  [  224/  306]
train() client id: f_00004-6-7 loss: 0.828813  [  256/  306]
train() client id: f_00004-6-8 loss: 0.950696  [  288/  306]
train() client id: f_00004-7-0 loss: 0.821820  [   32/  306]
train() client id: f_00004-7-1 loss: 0.908362  [   64/  306]
train() client id: f_00004-7-2 loss: 0.923247  [   96/  306]
train() client id: f_00004-7-3 loss: 0.881628  [  128/  306]
train() client id: f_00004-7-4 loss: 0.855167  [  160/  306]
train() client id: f_00004-7-5 loss: 0.988819  [  192/  306]
train() client id: f_00004-7-6 loss: 0.796299  [  224/  306]
train() client id: f_00004-7-7 loss: 0.940160  [  256/  306]
train() client id: f_00004-7-8 loss: 0.930901  [  288/  306]
train() client id: f_00004-8-0 loss: 0.906369  [   32/  306]
train() client id: f_00004-8-1 loss: 0.809910  [   64/  306]
train() client id: f_00004-8-2 loss: 0.894834  [   96/  306]
train() client id: f_00004-8-3 loss: 0.916480  [  128/  306]
train() client id: f_00004-8-4 loss: 0.856093  [  160/  306]
train() client id: f_00004-8-5 loss: 0.935726  [  192/  306]
train() client id: f_00004-8-6 loss: 0.925875  [  224/  306]
train() client id: f_00004-8-7 loss: 0.906424  [  256/  306]
train() client id: f_00004-8-8 loss: 0.888543  [  288/  306]
train() client id: f_00004-9-0 loss: 1.029387  [   32/  306]
train() client id: f_00004-9-1 loss: 0.857773  [   64/  306]
train() client id: f_00004-9-2 loss: 0.801281  [   96/  306]
train() client id: f_00004-9-3 loss: 0.943360  [  128/  306]
train() client id: f_00004-9-4 loss: 0.835595  [  160/  306]
train() client id: f_00004-9-5 loss: 0.834515  [  192/  306]
train() client id: f_00004-9-6 loss: 0.853927  [  224/  306]
train() client id: f_00004-9-7 loss: 1.096904  [  256/  306]
train() client id: f_00004-9-8 loss: 0.881438  [  288/  306]
train() client id: f_00004-10-0 loss: 0.843924  [   32/  306]
train() client id: f_00004-10-1 loss: 0.834979  [   64/  306]
train() client id: f_00004-10-2 loss: 0.820042  [   96/  306]
train() client id: f_00004-10-3 loss: 0.914441  [  128/  306]
train() client id: f_00004-10-4 loss: 0.820376  [  160/  306]
train() client id: f_00004-10-5 loss: 0.985851  [  192/  306]
train() client id: f_00004-10-6 loss: 0.923494  [  224/  306]
train() client id: f_00004-10-7 loss: 0.874228  [  256/  306]
train() client id: f_00004-10-8 loss: 0.927830  [  288/  306]
train() client id: f_00005-0-0 loss: 0.657093  [   32/  146]
train() client id: f_00005-0-1 loss: 0.652407  [   64/  146]
train() client id: f_00005-0-2 loss: 0.557193  [   96/  146]
train() client id: f_00005-0-3 loss: 0.955488  [  128/  146]
train() client id: f_00005-1-0 loss: 0.656101  [   32/  146]
train() client id: f_00005-1-1 loss: 0.772519  [   64/  146]
train() client id: f_00005-1-2 loss: 0.900047  [   96/  146]
train() client id: f_00005-1-3 loss: 0.653786  [  128/  146]
train() client id: f_00005-2-0 loss: 0.947586  [   32/  146]
train() client id: f_00005-2-1 loss: 0.697780  [   64/  146]
train() client id: f_00005-2-2 loss: 0.773272  [   96/  146]
train() client id: f_00005-2-3 loss: 0.418159  [  128/  146]
train() client id: f_00005-3-0 loss: 0.564656  [   32/  146]
train() client id: f_00005-3-1 loss: 0.874209  [   64/  146]
train() client id: f_00005-3-2 loss: 0.840000  [   96/  146]
train() client id: f_00005-3-3 loss: 0.586859  [  128/  146]
train() client id: f_00005-4-0 loss: 0.864339  [   32/  146]
train() client id: f_00005-4-1 loss: 0.786327  [   64/  146]
train() client id: f_00005-4-2 loss: 0.639931  [   96/  146]
train() client id: f_00005-4-3 loss: 0.620514  [  128/  146]
train() client id: f_00005-5-0 loss: 0.558043  [   32/  146]
train() client id: f_00005-5-1 loss: 0.804249  [   64/  146]
train() client id: f_00005-5-2 loss: 0.945595  [   96/  146]
train() client id: f_00005-5-3 loss: 0.590193  [  128/  146]
train() client id: f_00005-6-0 loss: 0.713176  [   32/  146]
train() client id: f_00005-6-1 loss: 0.672847  [   64/  146]
train() client id: f_00005-6-2 loss: 0.883227  [   96/  146]
train() client id: f_00005-6-3 loss: 0.712855  [  128/  146]
train() client id: f_00005-7-0 loss: 1.037093  [   32/  146]
train() client id: f_00005-7-1 loss: 0.873901  [   64/  146]
train() client id: f_00005-7-2 loss: 0.453199  [   96/  146]
train() client id: f_00005-7-3 loss: 0.626265  [  128/  146]
train() client id: f_00005-8-0 loss: 0.786312  [   32/  146]
train() client id: f_00005-8-1 loss: 0.815459  [   64/  146]
train() client id: f_00005-8-2 loss: 0.662152  [   96/  146]
train() client id: f_00005-8-3 loss: 0.682008  [  128/  146]
train() client id: f_00005-9-0 loss: 0.879432  [   32/  146]
train() client id: f_00005-9-1 loss: 0.610403  [   64/  146]
train() client id: f_00005-9-2 loss: 0.846818  [   96/  146]
train() client id: f_00005-9-3 loss: 0.576581  [  128/  146]
train() client id: f_00005-10-0 loss: 0.446710  [   32/  146]
train() client id: f_00005-10-1 loss: 0.775891  [   64/  146]
train() client id: f_00005-10-2 loss: 0.839131  [   96/  146]
train() client id: f_00005-10-3 loss: 0.800434  [  128/  146]
train() client id: f_00006-0-0 loss: 0.576461  [   32/   54]
train() client id: f_00006-1-0 loss: 0.466839  [   32/   54]
train() client id: f_00006-2-0 loss: 0.540827  [   32/   54]
train() client id: f_00006-3-0 loss: 0.506020  [   32/   54]
train() client id: f_00006-4-0 loss: 0.522184  [   32/   54]
train() client id: f_00006-5-0 loss: 0.557446  [   32/   54]
train() client id: f_00006-6-0 loss: 0.501217  [   32/   54]
train() client id: f_00006-7-0 loss: 0.572489  [   32/   54]
train() client id: f_00006-8-0 loss: 0.485168  [   32/   54]
train() client id: f_00006-9-0 loss: 0.554007  [   32/   54]
train() client id: f_00006-10-0 loss: 0.539262  [   32/   54]
train() client id: f_00007-0-0 loss: 0.554752  [   32/  179]
train() client id: f_00007-0-1 loss: 0.476787  [   64/  179]
train() client id: f_00007-0-2 loss: 0.820666  [   96/  179]
train() client id: f_00007-0-3 loss: 0.678600  [  128/  179]
train() client id: f_00007-0-4 loss: 0.785498  [  160/  179]
train() client id: f_00007-1-0 loss: 0.548542  [   32/  179]
train() client id: f_00007-1-1 loss: 0.643884  [   64/  179]
train() client id: f_00007-1-2 loss: 0.769894  [   96/  179]
train() client id: f_00007-1-3 loss: 0.577289  [  128/  179]
train() client id: f_00007-1-4 loss: 0.713034  [  160/  179]
train() client id: f_00007-2-0 loss: 0.554858  [   32/  179]
train() client id: f_00007-2-1 loss: 0.612913  [   64/  179]
train() client id: f_00007-2-2 loss: 0.612431  [   96/  179]
train() client id: f_00007-2-3 loss: 0.598448  [  128/  179]
train() client id: f_00007-2-4 loss: 0.679294  [  160/  179]
train() client id: f_00007-3-0 loss: 0.546896  [   32/  179]
train() client id: f_00007-3-1 loss: 0.710479  [   64/  179]
train() client id: f_00007-3-2 loss: 0.644142  [   96/  179]
train() client id: f_00007-3-3 loss: 0.703573  [  128/  179]
train() client id: f_00007-3-4 loss: 0.548330  [  160/  179]
train() client id: f_00007-4-0 loss: 0.784003  [   32/  179]
train() client id: f_00007-4-1 loss: 0.639475  [   64/  179]
train() client id: f_00007-4-2 loss: 0.510170  [   96/  179]
train() client id: f_00007-4-3 loss: 0.501290  [  128/  179]
train() client id: f_00007-4-4 loss: 0.636776  [  160/  179]
train() client id: f_00007-5-0 loss: 0.579900  [   32/  179]
train() client id: f_00007-5-1 loss: 0.575081  [   64/  179]
train() client id: f_00007-5-2 loss: 0.566561  [   96/  179]
train() client id: f_00007-5-3 loss: 0.655256  [  128/  179]
train() client id: f_00007-5-4 loss: 0.670298  [  160/  179]
train() client id: f_00007-6-0 loss: 0.505109  [   32/  179]
train() client id: f_00007-6-1 loss: 0.544862  [   64/  179]
train() client id: f_00007-6-2 loss: 0.548015  [   96/  179]
train() client id: f_00007-6-3 loss: 0.545670  [  128/  179]
train() client id: f_00007-6-4 loss: 0.648546  [  160/  179]
train() client id: f_00007-7-0 loss: 0.911573  [   32/  179]
train() client id: f_00007-7-1 loss: 0.454178  [   64/  179]
train() client id: f_00007-7-2 loss: 0.505244  [   96/  179]
train() client id: f_00007-7-3 loss: 0.553880  [  128/  179]
train() client id: f_00007-7-4 loss: 0.505390  [  160/  179]
train() client id: f_00007-8-0 loss: 0.575005  [   32/  179]
train() client id: f_00007-8-1 loss: 0.582847  [   64/  179]
train() client id: f_00007-8-2 loss: 0.753762  [   96/  179]
train() client id: f_00007-8-3 loss: 0.518245  [  128/  179]
train() client id: f_00007-8-4 loss: 0.589172  [  160/  179]
train() client id: f_00007-9-0 loss: 0.544898  [   32/  179]
train() client id: f_00007-9-1 loss: 0.639580  [   64/  179]
train() client id: f_00007-9-2 loss: 0.419896  [   96/  179]
train() client id: f_00007-9-3 loss: 0.633153  [  128/  179]
train() client id: f_00007-9-4 loss: 0.816790  [  160/  179]
train() client id: f_00007-10-0 loss: 0.552744  [   32/  179]
train() client id: f_00007-10-1 loss: 0.525607  [   64/  179]
train() client id: f_00007-10-2 loss: 0.705876  [   96/  179]
train() client id: f_00007-10-3 loss: 0.609154  [  128/  179]
train() client id: f_00007-10-4 loss: 0.619865  [  160/  179]
train() client id: f_00008-0-0 loss: 0.657379  [   32/  130]
train() client id: f_00008-0-1 loss: 0.712643  [   64/  130]
train() client id: f_00008-0-2 loss: 0.801808  [   96/  130]
train() client id: f_00008-0-3 loss: 0.632177  [  128/  130]
train() client id: f_00008-1-0 loss: 0.731165  [   32/  130]
train() client id: f_00008-1-1 loss: 0.641893  [   64/  130]
train() client id: f_00008-1-2 loss: 0.705598  [   96/  130]
train() client id: f_00008-1-3 loss: 0.732784  [  128/  130]
train() client id: f_00008-2-0 loss: 0.740568  [   32/  130]
train() client id: f_00008-2-1 loss: 0.667555  [   64/  130]
train() client id: f_00008-2-2 loss: 0.682012  [   96/  130]
train() client id: f_00008-2-3 loss: 0.706467  [  128/  130]
train() client id: f_00008-3-0 loss: 0.758171  [   32/  130]
train() client id: f_00008-3-1 loss: 0.683416  [   64/  130]
train() client id: f_00008-3-2 loss: 0.700139  [   96/  130]
train() client id: f_00008-3-3 loss: 0.637318  [  128/  130]
train() client id: f_00008-4-0 loss: 0.668749  [   32/  130]
train() client id: f_00008-4-1 loss: 0.699916  [   64/  130]
train() client id: f_00008-4-2 loss: 0.736525  [   96/  130]
train() client id: f_00008-4-3 loss: 0.652995  [  128/  130]
train() client id: f_00008-5-0 loss: 0.639570  [   32/  130]
train() client id: f_00008-5-1 loss: 0.671418  [   64/  130]
train() client id: f_00008-5-2 loss: 0.764221  [   96/  130]
train() client id: f_00008-5-3 loss: 0.726348  [  128/  130]
train() client id: f_00008-6-0 loss: 0.599542  [   32/  130]
train() client id: f_00008-6-1 loss: 0.749208  [   64/  130]
train() client id: f_00008-6-2 loss: 0.691361  [   96/  130]
train() client id: f_00008-6-3 loss: 0.765889  [  128/  130]
train() client id: f_00008-7-0 loss: 0.753849  [   32/  130]
train() client id: f_00008-7-1 loss: 0.713563  [   64/  130]
train() client id: f_00008-7-2 loss: 0.691326  [   96/  130]
train() client id: f_00008-7-3 loss: 0.615093  [  128/  130]
train() client id: f_00008-8-0 loss: 0.692328  [   32/  130]
train() client id: f_00008-8-1 loss: 0.719506  [   64/  130]
train() client id: f_00008-8-2 loss: 0.622869  [   96/  130]
train() client id: f_00008-8-3 loss: 0.776892  [  128/  130]
train() client id: f_00008-9-0 loss: 0.693856  [   32/  130]
train() client id: f_00008-9-1 loss: 0.736145  [   64/  130]
train() client id: f_00008-9-2 loss: 0.650032  [   96/  130]
train() client id: f_00008-9-3 loss: 0.721665  [  128/  130]
train() client id: f_00008-10-0 loss: 0.702777  [   32/  130]
train() client id: f_00008-10-1 loss: 0.742993  [   64/  130]
train() client id: f_00008-10-2 loss: 0.704563  [   96/  130]
train() client id: f_00008-10-3 loss: 0.661957  [  128/  130]
train() client id: f_00009-0-0 loss: 1.055161  [   32/  118]
train() client id: f_00009-0-1 loss: 0.924881  [   64/  118]
train() client id: f_00009-0-2 loss: 1.029170  [   96/  118]
train() client id: f_00009-1-0 loss: 0.856988  [   32/  118]
train() client id: f_00009-1-1 loss: 0.977192  [   64/  118]
train() client id: f_00009-1-2 loss: 1.042344  [   96/  118]
train() client id: f_00009-2-0 loss: 0.807074  [   32/  118]
train() client id: f_00009-2-1 loss: 1.031090  [   64/  118]
train() client id: f_00009-2-2 loss: 0.977052  [   96/  118]
train() client id: f_00009-3-0 loss: 0.839065  [   32/  118]
train() client id: f_00009-3-1 loss: 0.977295  [   64/  118]
train() client id: f_00009-3-2 loss: 0.843562  [   96/  118]
train() client id: f_00009-4-0 loss: 1.032281  [   32/  118]
train() client id: f_00009-4-1 loss: 0.976879  [   64/  118]
train() client id: f_00009-4-2 loss: 0.746551  [   96/  118]
train() client id: f_00009-5-0 loss: 0.919346  [   32/  118]
train() client id: f_00009-5-1 loss: 1.069325  [   64/  118]
train() client id: f_00009-5-2 loss: 0.778637  [   96/  118]
train() client id: f_00009-6-0 loss: 0.928458  [   32/  118]
train() client id: f_00009-6-1 loss: 0.986048  [   64/  118]
train() client id: f_00009-6-2 loss: 0.837170  [   96/  118]
train() client id: f_00009-7-0 loss: 0.696659  [   32/  118]
train() client id: f_00009-7-1 loss: 1.021415  [   64/  118]
train() client id: f_00009-7-2 loss: 0.847639  [   96/  118]
train() client id: f_00009-8-0 loss: 0.837865  [   32/  118]
train() client id: f_00009-8-1 loss: 0.991068  [   64/  118]
train() client id: f_00009-8-2 loss: 0.868649  [   96/  118]
train() client id: f_00009-9-0 loss: 1.075862  [   32/  118]
train() client id: f_00009-9-1 loss: 0.722391  [   64/  118]
train() client id: f_00009-9-2 loss: 0.859652  [   96/  118]
train() client id: f_00009-10-0 loss: 0.868058  [   32/  118]
train() client id: f_00009-10-1 loss: 0.897574  [   64/  118]
train() client id: f_00009-10-2 loss: 0.930217  [   96/  118]
At round 57 accuracy: 0.6472148541114059
At round 57 training accuracy: 0.5881958417169685
At round 57 training loss: 0.8279531636868959
update_location
xs = [  -3.9056584     4.20031788  305.00902392   18.81129433    0.97929623
    3.95640986 -267.44319194 -246.32485185  289.66397685 -232.06087855]
ys = [ 297.5879595   280.55583871    1.32061395 -267.45517586  259.35018685
  242.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [313.96472382 297.87450596 320.98636839 286.15753684 277.96308826
 262.62971077 285.53940438 265.85072671 306.94281138 252.721711  ]
dists_bs = [210.47822106 207.97785702 510.4637031  483.04182061 195.15036951
 191.37438614 200.24080201 188.21553855 490.60893718 180.26768178]
uav_gains = [2.06976062e-12 2.78476917e-12 1.83844324e-12 3.52095540e-12
 4.17321580e-12 5.75002013e-12 3.56593276e-12 5.37807949e-12
 2.34619438e-12 7.03198726e-12]
bs_gains = [3.45393392e-11 3.57146344e-11 2.89062969e-12 3.37393595e-12
 4.26834807e-11 4.50836859e-11 3.97142964e-11 4.72344384e-11
 3.23024015e-12 5.32996109e-11]
Round 58
-------------------------------
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.3523265  6.77756785 3.31084036 1.21735079 7.81367428 3.76005882
 1.49535056 4.64371042 3.4255904  3.04870737]
obj_prev = 38.84517734952248
eta_min = 2.470941254922374e-28	eta_max = 0.9375517380484754
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 8.931115437766236	eta = 0.909090909090909
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 20.2431781023763	eta = 0.40108306173335817
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 14.165631330058979	eta = 0.5731615953668162
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.085726257080514	eta = 0.6204619975235665
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.020212302687447	eta = 0.623583983407007
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.019941071601606	eta = 0.6235969738929089
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.01994106692125	eta = 0.623596974117077
eta = 0.623596974117077
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [0.03896902 0.08195865 0.03835046 0.01329895 0.09463905 0.04515458
 0.01670101 0.05536075 0.04020613 0.0364948 ]
ene_total = [1.28342906 1.9813468  1.29389683 0.63079383 2.25502199 1.16000083
 0.7042715  1.51102723 1.23695895 0.96319404]
ti_comp = [1.03546301 1.14942413 1.02453888 1.07419893 1.1523401  1.15319241
 1.07495744 1.09635059 1.06887034 1.15568559]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [3.44960441e-06 2.60437692e-05 3.35842002e-06 1.27398049e-07
 3.98960382e-05 4.32694980e-06 2.51956506e-07 8.82240566e-06
 3.55555046e-06 2.27453939e-06]
ene_total = [0.42265881 0.17141173 0.4467888  0.33701571 0.16527621 0.16260767
 0.33534287 0.28827359 0.34886259 0.15705475]
optimize_network_iter = 0 obj = 2.83529273119976
eta = 0.623596974117077
freqs = [18817195.6470524  35652050.01875965 18715963.43024579  6190171.56284024
 41063853.09945352 19578076.71969618  7768218.52091367 25247740.71552675
 18807768.58899782 15789240.96805261]
eta_min = 0.6235969741170779	eta_max = 0.7095637697055857
af = 0.0016217254572042322	bf = 1.0664469018281375	zeta = 0.0017838980029246557	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [6.96202700e-07 5.25618019e-06 6.77799772e-07 2.57116048e-08
 8.05185933e-06 8.73269445e-07 5.08501203e-08 1.78054696e-06
 7.17584839e-07 4.59049871e-07]
ene_total = [1.74973877 0.70781749 1.84965402 1.39538178 0.68140253 0.67295033
 1.38844638 1.19293349 1.4441827  0.65010868]
ti_comp = [0.75528321 0.86924434 0.74435908 0.79401913 0.8721603  0.87301261
 0.79477765 0.81617079 0.78869054 0.8755058 ]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.48645699e-06 1.74639655e-05 2.43999568e-06 8.94195368e-08
 2.67091848e-05 2.89538630e-06 1.76757958e-07 6.10500434e-06
 2.50441200e-06 1.51990678e-06]
ene_total = [0.54773486 0.22190259 0.5790084  0.43676858 0.21381912 0.21069725
 0.43459951 0.37352246 0.45209302 0.2035201 ]
optimize_network_iter = 1 obj = 3.6736658995998672
eta = 0.7095637697055857
freqs = [18742766.932176   34251295.00082695 18715963.43024578  6084298.06393133
 39418319.48246514 18789072.00779956  7633454.28294466 24640242.31966294
 18518666.61499257 15142443.926058  ]
eta_min = 0.7095637697056919	eta_max = 0.7095637697055788
af = 0.0015119964848909963	bf = 1.0664469018281375	zeta = 0.001663196133380096	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [6.90706132e-07 4.85126751e-06 6.77799772e-07 2.48396102e-08
 7.41947189e-06 8.04301492e-07 4.91011128e-08 1.69589257e-06
 6.95693805e-07 4.22210775e-07]
ene_total = [1.74973827 0.70778046 1.84965402 1.3953817  0.68134469 0.67294402
 1.38844622 1.19292575 1.44418069 0.65010531]
ti_comp = [0.75528321 0.86924434 0.74435908 0.79401913 0.8721603  0.87301261
 0.79477765 0.81617079 0.78869054 0.8755058 ]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.48645699e-06 1.74639655e-05 2.43999568e-06 8.94195368e-08
 2.67091848e-05 2.89538630e-06 1.76757958e-07 6.10500434e-06
 2.50441200e-06 1.51990678e-06]
ene_total = [0.54773486 0.22190259 0.5790084  0.43676858 0.21381912 0.21069725
 0.43459951 0.37352246 0.45209302 0.2035201 ]
optimize_network_iter = 2 obj = 3.67366589959978
eta = 0.7095637697055788
freqs = [18742766.93217598 34251295.00082704 18715963.43024575  6084298.06393133
 39418319.48246525 18789072.00779961  7633454.28294467 24640242.31966297
 18518666.61499257 15142443.92605804]
Done!
At round 58 eta: 0.7095637697055788
At round 58 local rounds: 11.234997825322132
At round 58 global rounds: 28.629154577598474
At round 58 a_n: 7.972397885064655
gradient difference: 0.5394331812858582
train() client id: f_00000-0-0 loss: 1.113405  [   32/  126]
train() client id: f_00000-0-1 loss: 1.233663  [   64/  126]
train() client id: f_00000-0-2 loss: 1.011209  [   96/  126]
train() client id: f_00000-1-0 loss: 1.233036  [   32/  126]
train() client id: f_00000-1-1 loss: 1.004398  [   64/  126]
train() client id: f_00000-1-2 loss: 0.925237  [   96/  126]
train() client id: f_00000-2-0 loss: 1.031734  [   32/  126]
train() client id: f_00000-2-1 loss: 1.070000  [   64/  126]
train() client id: f_00000-2-2 loss: 0.971927  [   96/  126]
train() client id: f_00000-3-0 loss: 0.954527  [   32/  126]
train() client id: f_00000-3-1 loss: 0.901079  [   64/  126]
train() client id: f_00000-3-2 loss: 1.057151  [   96/  126]
train() client id: f_00000-4-0 loss: 0.940116  [   32/  126]
train() client id: f_00000-4-1 loss: 0.981198  [   64/  126]
train() client id: f_00000-4-2 loss: 0.952670  [   96/  126]
train() client id: f_00000-5-0 loss: 1.088714  [   32/  126]
train() client id: f_00000-5-1 loss: 0.830388  [   64/  126]
train() client id: f_00000-5-2 loss: 0.913917  [   96/  126]
train() client id: f_00000-6-0 loss: 0.917447  [   32/  126]
train() client id: f_00000-6-1 loss: 0.924303  [   64/  126]
train() client id: f_00000-6-2 loss: 0.926319  [   96/  126]
train() client id: f_00000-7-0 loss: 0.980193  [   32/  126]
train() client id: f_00000-7-1 loss: 0.933307  [   64/  126]
train() client id: f_00000-7-2 loss: 0.898124  [   96/  126]
train() client id: f_00000-8-0 loss: 0.926812  [   32/  126]
train() client id: f_00000-8-1 loss: 1.061397  [   64/  126]
train() client id: f_00000-8-2 loss: 0.823022  [   96/  126]
train() client id: f_00000-9-0 loss: 0.878371  [   32/  126]
train() client id: f_00000-9-1 loss: 0.991823  [   64/  126]
train() client id: f_00000-9-2 loss: 1.043640  [   96/  126]
train() client id: f_00000-10-0 loss: 0.816054  [   32/  126]
train() client id: f_00000-10-1 loss: 0.947945  [   64/  126]
train() client id: f_00000-10-2 loss: 1.083071  [   96/  126]
train() client id: f_00001-0-0 loss: 0.413860  [   32/  265]
train() client id: f_00001-0-1 loss: 0.335892  [   64/  265]
train() client id: f_00001-0-2 loss: 0.490632  [   96/  265]
train() client id: f_00001-0-3 loss: 0.532765  [  128/  265]
train() client id: f_00001-0-4 loss: 0.458855  [  160/  265]
train() client id: f_00001-0-5 loss: 0.407472  [  192/  265]
train() client id: f_00001-0-6 loss: 0.456801  [  224/  265]
train() client id: f_00001-0-7 loss: 0.386345  [  256/  265]
train() client id: f_00001-1-0 loss: 0.515576  [   32/  265]
train() client id: f_00001-1-1 loss: 0.388575  [   64/  265]
train() client id: f_00001-1-2 loss: 0.486009  [   96/  265]
train() client id: f_00001-1-3 loss: 0.372407  [  128/  265]
train() client id: f_00001-1-4 loss: 0.344082  [  160/  265]
train() client id: f_00001-1-5 loss: 0.514673  [  192/  265]
train() client id: f_00001-1-6 loss: 0.348702  [  224/  265]
train() client id: f_00001-1-7 loss: 0.466391  [  256/  265]
train() client id: f_00001-2-0 loss: 0.391978  [   32/  265]
train() client id: f_00001-2-1 loss: 0.351444  [   64/  265]
train() client id: f_00001-2-2 loss: 0.512905  [   96/  265]
train() client id: f_00001-2-3 loss: 0.356536  [  128/  265]
train() client id: f_00001-2-4 loss: 0.315244  [  160/  265]
train() client id: f_00001-2-5 loss: 0.432359  [  192/  265]
train() client id: f_00001-2-6 loss: 0.588644  [  224/  265]
train() client id: f_00001-2-7 loss: 0.416794  [  256/  265]
train() client id: f_00001-3-0 loss: 0.426871  [   32/  265]
train() client id: f_00001-3-1 loss: 0.373103  [   64/  265]
train() client id: f_00001-3-2 loss: 0.508414  [   96/  265]
train() client id: f_00001-3-3 loss: 0.490990  [  128/  265]
train() client id: f_00001-3-4 loss: 0.458860  [  160/  265]
train() client id: f_00001-3-5 loss: 0.377910  [  192/  265]
train() client id: f_00001-3-6 loss: 0.339921  [  224/  265]
train() client id: f_00001-3-7 loss: 0.367706  [  256/  265]
train() client id: f_00001-4-0 loss: 0.401114  [   32/  265]
train() client id: f_00001-4-1 loss: 0.462625  [   64/  265]
train() client id: f_00001-4-2 loss: 0.539509  [   96/  265]
train() client id: f_00001-4-3 loss: 0.355525  [  128/  265]
train() client id: f_00001-4-4 loss: 0.324133  [  160/  265]
train() client id: f_00001-4-5 loss: 0.372319  [  192/  265]
train() client id: f_00001-4-6 loss: 0.421872  [  224/  265]
train() client id: f_00001-4-7 loss: 0.401300  [  256/  265]
train() client id: f_00001-5-0 loss: 0.519319  [   32/  265]
train() client id: f_00001-5-1 loss: 0.419735  [   64/  265]
train() client id: f_00001-5-2 loss: 0.335231  [   96/  265]
train() client id: f_00001-5-3 loss: 0.370416  [  128/  265]
train() client id: f_00001-5-4 loss: 0.416830  [  160/  265]
train() client id: f_00001-5-5 loss: 0.421066  [  192/  265]
train() client id: f_00001-5-6 loss: 0.483842  [  224/  265]
train() client id: f_00001-5-7 loss: 0.321569  [  256/  265]
train() client id: f_00001-6-0 loss: 0.405699  [   32/  265]
train() client id: f_00001-6-1 loss: 0.358107  [   64/  265]
train() client id: f_00001-6-2 loss: 0.440493  [   96/  265]
train() client id: f_00001-6-3 loss: 0.491968  [  128/  265]
train() client id: f_00001-6-4 loss: 0.442122  [  160/  265]
train() client id: f_00001-6-5 loss: 0.321186  [  192/  265]
train() client id: f_00001-6-6 loss: 0.497264  [  224/  265]
train() client id: f_00001-6-7 loss: 0.311516  [  256/  265]
train() client id: f_00001-7-0 loss: 0.468543  [   32/  265]
train() client id: f_00001-7-1 loss: 0.417114  [   64/  265]
train() client id: f_00001-7-2 loss: 0.454667  [   96/  265]
train() client id: f_00001-7-3 loss: 0.400669  [  128/  265]
train() client id: f_00001-7-4 loss: 0.364381  [  160/  265]
train() client id: f_00001-7-5 loss: 0.388125  [  192/  265]
train() client id: f_00001-7-6 loss: 0.426966  [  224/  265]
train() client id: f_00001-7-7 loss: 0.330415  [  256/  265]
train() client id: f_00001-8-0 loss: 0.305477  [   32/  265]
train() client id: f_00001-8-1 loss: 0.480801  [   64/  265]
train() client id: f_00001-8-2 loss: 0.423468  [   96/  265]
train() client id: f_00001-8-3 loss: 0.340994  [  128/  265]
train() client id: f_00001-8-4 loss: 0.389049  [  160/  265]
train() client id: f_00001-8-5 loss: 0.399121  [  192/  265]
train() client id: f_00001-8-6 loss: 0.402303  [  224/  265]
train() client id: f_00001-8-7 loss: 0.420420  [  256/  265]
train() client id: f_00001-9-0 loss: 0.300543  [   32/  265]
train() client id: f_00001-9-1 loss: 0.446778  [   64/  265]
train() client id: f_00001-9-2 loss: 0.319911  [   96/  265]
train() client id: f_00001-9-3 loss: 0.422018  [  128/  265]
train() client id: f_00001-9-4 loss: 0.346703  [  160/  265]
train() client id: f_00001-9-5 loss: 0.392130  [  192/  265]
train() client id: f_00001-9-6 loss: 0.394584  [  224/  265]
train() client id: f_00001-9-7 loss: 0.579951  [  256/  265]
train() client id: f_00001-10-0 loss: 0.468961  [   32/  265]
train() client id: f_00001-10-1 loss: 0.566948  [   64/  265]
train() client id: f_00001-10-2 loss: 0.377443  [   96/  265]
train() client id: f_00001-10-3 loss: 0.299780  [  128/  265]
train() client id: f_00001-10-4 loss: 0.405668  [  160/  265]
train() client id: f_00001-10-5 loss: 0.365211  [  192/  265]
train() client id: f_00001-10-6 loss: 0.320270  [  224/  265]
train() client id: f_00001-10-7 loss: 0.378783  [  256/  265]
train() client id: f_00002-0-0 loss: 1.167596  [   32/  124]
train() client id: f_00002-0-1 loss: 1.102442  [   64/  124]
train() client id: f_00002-0-2 loss: 1.053173  [   96/  124]
train() client id: f_00002-1-0 loss: 1.140350  [   32/  124]
train() client id: f_00002-1-1 loss: 1.222266  [   64/  124]
train() client id: f_00002-1-2 loss: 1.039503  [   96/  124]
train() client id: f_00002-2-0 loss: 1.174441  [   32/  124]
train() client id: f_00002-2-1 loss: 1.211455  [   64/  124]
train() client id: f_00002-2-2 loss: 0.948995  [   96/  124]
train() client id: f_00002-3-0 loss: 0.897682  [   32/  124]
train() client id: f_00002-3-1 loss: 1.002281  [   64/  124]
train() client id: f_00002-3-2 loss: 1.105748  [   96/  124]
train() client id: f_00002-4-0 loss: 0.994563  [   32/  124]
train() client id: f_00002-4-1 loss: 1.110240  [   64/  124]
train() client id: f_00002-4-2 loss: 0.945796  [   96/  124]
train() client id: f_00002-5-0 loss: 1.027042  [   32/  124]
train() client id: f_00002-5-1 loss: 1.074223  [   64/  124]
train() client id: f_00002-5-2 loss: 0.990785  [   96/  124]
train() client id: f_00002-6-0 loss: 0.973454  [   32/  124]
train() client id: f_00002-6-1 loss: 1.097158  [   64/  124]
train() client id: f_00002-6-2 loss: 0.933499  [   96/  124]
train() client id: f_00002-7-0 loss: 1.183370  [   32/  124]
train() client id: f_00002-7-1 loss: 0.892198  [   64/  124]
train() client id: f_00002-7-2 loss: 1.056492  [   96/  124]
train() client id: f_00002-8-0 loss: 0.852132  [   32/  124]
train() client id: f_00002-8-1 loss: 1.052390  [   64/  124]
train() client id: f_00002-8-2 loss: 1.098905  [   96/  124]
train() client id: f_00002-9-0 loss: 1.109073  [   32/  124]
train() client id: f_00002-9-1 loss: 1.084121  [   64/  124]
train() client id: f_00002-9-2 loss: 0.954253  [   96/  124]
train() client id: f_00002-10-0 loss: 1.127035  [   32/  124]
train() client id: f_00002-10-1 loss: 0.940555  [   64/  124]
train() client id: f_00002-10-2 loss: 1.083069  [   96/  124]
train() client id: f_00003-0-0 loss: 0.523563  [   32/   43]
train() client id: f_00003-1-0 loss: 0.779823  [   32/   43]
train() client id: f_00003-2-0 loss: 0.720410  [   32/   43]
train() client id: f_00003-3-0 loss: 0.681477  [   32/   43]
train() client id: f_00003-4-0 loss: 0.833554  [   32/   43]
train() client id: f_00003-5-0 loss: 0.524003  [   32/   43]
train() client id: f_00003-6-0 loss: 0.761840  [   32/   43]
train() client id: f_00003-7-0 loss: 0.445262  [   32/   43]
train() client id: f_00003-8-0 loss: 0.388918  [   32/   43]
train() client id: f_00003-9-0 loss: 0.570902  [   32/   43]
train() client id: f_00003-10-0 loss: 0.576375  [   32/   43]
train() client id: f_00004-0-0 loss: 0.931648  [   32/  306]
train() client id: f_00004-0-1 loss: 0.788744  [   64/  306]
train() client id: f_00004-0-2 loss: 0.952774  [   96/  306]
train() client id: f_00004-0-3 loss: 0.717620  [  128/  306]
train() client id: f_00004-0-4 loss: 0.746583  [  160/  306]
train() client id: f_00004-0-5 loss: 0.677280  [  192/  306]
train() client id: f_00004-0-6 loss: 0.750179  [  224/  306]
train() client id: f_00004-0-7 loss: 0.733172  [  256/  306]
train() client id: f_00004-0-8 loss: 0.628233  [  288/  306]
train() client id: f_00004-1-0 loss: 0.619623  [   32/  306]
train() client id: f_00004-1-1 loss: 0.848708  [   64/  306]
train() client id: f_00004-1-2 loss: 0.765465  [   96/  306]
train() client id: f_00004-1-3 loss: 0.761684  [  128/  306]
train() client id: f_00004-1-4 loss: 0.873523  [  160/  306]
train() client id: f_00004-1-5 loss: 0.774325  [  192/  306]
train() client id: f_00004-1-6 loss: 0.822965  [  224/  306]
train() client id: f_00004-1-7 loss: 0.817448  [  256/  306]
train() client id: f_00004-1-8 loss: 0.703553  [  288/  306]
train() client id: f_00004-2-0 loss: 0.811581  [   32/  306]
train() client id: f_00004-2-1 loss: 0.854384  [   64/  306]
train() client id: f_00004-2-2 loss: 0.654187  [   96/  306]
train() client id: f_00004-2-3 loss: 0.807224  [  128/  306]
train() client id: f_00004-2-4 loss: 0.702118  [  160/  306]
train() client id: f_00004-2-5 loss: 0.705597  [  192/  306]
train() client id: f_00004-2-6 loss: 0.769151  [  224/  306]
train() client id: f_00004-2-7 loss: 0.876300  [  256/  306]
train() client id: f_00004-2-8 loss: 0.800262  [  288/  306]
train() client id: f_00004-3-0 loss: 0.718313  [   32/  306]
train() client id: f_00004-3-1 loss: 0.826496  [   64/  306]
train() client id: f_00004-3-2 loss: 0.650854  [   96/  306]
train() client id: f_00004-3-3 loss: 0.692505  [  128/  306]
train() client id: f_00004-3-4 loss: 0.825606  [  160/  306]
train() client id: f_00004-3-5 loss: 0.777395  [  192/  306]
train() client id: f_00004-3-6 loss: 0.778082  [  224/  306]
train() client id: f_00004-3-7 loss: 0.851958  [  256/  306]
train() client id: f_00004-3-8 loss: 0.844605  [  288/  306]
train() client id: f_00004-4-0 loss: 0.970941  [   32/  306]
train() client id: f_00004-4-1 loss: 0.633674  [   64/  306]
train() client id: f_00004-4-2 loss: 0.814484  [   96/  306]
train() client id: f_00004-4-3 loss: 0.786673  [  128/  306]
train() client id: f_00004-4-4 loss: 0.914596  [  160/  306]
train() client id: f_00004-4-5 loss: 0.783284  [  192/  306]
train() client id: f_00004-4-6 loss: 0.609308  [  224/  306]
train() client id: f_00004-4-7 loss: 0.791148  [  256/  306]
train() client id: f_00004-4-8 loss: 0.712485  [  288/  306]
train() client id: f_00004-5-0 loss: 0.715533  [   32/  306]
train() client id: f_00004-5-1 loss: 0.846071  [   64/  306]
train() client id: f_00004-5-2 loss: 0.834442  [   96/  306]
train() client id: f_00004-5-3 loss: 0.829318  [  128/  306]
train() client id: f_00004-5-4 loss: 0.810216  [  160/  306]
train() client id: f_00004-5-5 loss: 0.810141  [  192/  306]
train() client id: f_00004-5-6 loss: 0.685667  [  224/  306]
train() client id: f_00004-5-7 loss: 0.787435  [  256/  306]
train() client id: f_00004-5-8 loss: 0.749478  [  288/  306]
train() client id: f_00004-6-0 loss: 0.780467  [   32/  306]
train() client id: f_00004-6-1 loss: 0.814119  [   64/  306]
train() client id: f_00004-6-2 loss: 0.664443  [   96/  306]
train() client id: f_00004-6-3 loss: 0.773335  [  128/  306]
train() client id: f_00004-6-4 loss: 0.760340  [  160/  306]
train() client id: f_00004-6-5 loss: 0.826713  [  192/  306]
train() client id: f_00004-6-6 loss: 0.788539  [  224/  306]
train() client id: f_00004-6-7 loss: 0.764200  [  256/  306]
train() client id: f_00004-6-8 loss: 0.820450  [  288/  306]
train() client id: f_00004-7-0 loss: 0.881498  [   32/  306]
train() client id: f_00004-7-1 loss: 0.755494  [   64/  306]
train() client id: f_00004-7-2 loss: 0.763204  [   96/  306]
train() client id: f_00004-7-3 loss: 0.716945  [  128/  306]
train() client id: f_00004-7-4 loss: 0.776841  [  160/  306]
train() client id: f_00004-7-5 loss: 0.663336  [  192/  306]
train() client id: f_00004-7-6 loss: 0.814110  [  224/  306]
train() client id: f_00004-7-7 loss: 0.776450  [  256/  306]
train() client id: f_00004-7-8 loss: 0.869099  [  288/  306]
train() client id: f_00004-8-0 loss: 0.700667  [   32/  306]
train() client id: f_00004-8-1 loss: 0.829729  [   64/  306]
train() client id: f_00004-8-2 loss: 0.839446  [   96/  306]
train() client id: f_00004-8-3 loss: 0.685289  [  128/  306]
train() client id: f_00004-8-4 loss: 0.770482  [  160/  306]
train() client id: f_00004-8-5 loss: 0.753747  [  192/  306]
train() client id: f_00004-8-6 loss: 0.873857  [  224/  306]
train() client id: f_00004-8-7 loss: 0.733960  [  256/  306]
train() client id: f_00004-8-8 loss: 0.806841  [  288/  306]
train() client id: f_00004-9-0 loss: 0.727544  [   32/  306]
train() client id: f_00004-9-1 loss: 0.825944  [   64/  306]
train() client id: f_00004-9-2 loss: 0.831864  [   96/  306]
train() client id: f_00004-9-3 loss: 0.723006  [  128/  306]
train() client id: f_00004-9-4 loss: 0.820438  [  160/  306]
train() client id: f_00004-9-5 loss: 0.750157  [  192/  306]
train() client id: f_00004-9-6 loss: 0.650739  [  224/  306]
train() client id: f_00004-9-7 loss: 0.822842  [  256/  306]
train() client id: f_00004-9-8 loss: 0.743894  [  288/  306]
train() client id: f_00004-10-0 loss: 0.748887  [   32/  306]
train() client id: f_00004-10-1 loss: 0.755880  [   64/  306]
train() client id: f_00004-10-2 loss: 0.764118  [   96/  306]
train() client id: f_00004-10-3 loss: 0.617007  [  128/  306]
train() client id: f_00004-10-4 loss: 0.754012  [  160/  306]
train() client id: f_00004-10-5 loss: 0.748139  [  192/  306]
train() client id: f_00004-10-6 loss: 0.788200  [  224/  306]
train() client id: f_00004-10-7 loss: 0.734702  [  256/  306]
train() client id: f_00004-10-8 loss: 0.917781  [  288/  306]
train() client id: f_00005-0-0 loss: 0.691013  [   32/  146]
train() client id: f_00005-0-1 loss: 0.450127  [   64/  146]
train() client id: f_00005-0-2 loss: 0.495004  [   96/  146]
train() client id: f_00005-0-3 loss: 0.700732  [  128/  146]
train() client id: f_00005-1-0 loss: 0.339133  [   32/  146]
train() client id: f_00005-1-1 loss: 0.813506  [   64/  146]
train() client id: f_00005-1-2 loss: 0.298402  [   96/  146]
train() client id: f_00005-1-3 loss: 0.593949  [  128/  146]
train() client id: f_00005-2-0 loss: 0.384031  [   32/  146]
train() client id: f_00005-2-1 loss: 0.605342  [   64/  146]
train() client id: f_00005-2-2 loss: 0.620442  [   96/  146]
train() client id: f_00005-2-3 loss: 0.615775  [  128/  146]
train() client id: f_00005-3-0 loss: 0.376527  [   32/  146]
train() client id: f_00005-3-1 loss: 0.364343  [   64/  146]
train() client id: f_00005-3-2 loss: 0.802035  [   96/  146]
train() client id: f_00005-3-3 loss: 0.616494  [  128/  146]
train() client id: f_00005-4-0 loss: 0.651410  [   32/  146]
train() client id: f_00005-4-1 loss: 0.493585  [   64/  146]
train() client id: f_00005-4-2 loss: 0.526266  [   96/  146]
train() client id: f_00005-4-3 loss: 0.443339  [  128/  146]
train() client id: f_00005-5-0 loss: 0.809336  [   32/  146]
train() client id: f_00005-5-1 loss: 0.579896  [   64/  146]
train() client id: f_00005-5-2 loss: 0.350505  [   96/  146]
train() client id: f_00005-5-3 loss: 0.431845  [  128/  146]
train() client id: f_00005-6-0 loss: 0.604885  [   32/  146]
train() client id: f_00005-6-1 loss: 0.305616  [   64/  146]
train() client id: f_00005-6-2 loss: 0.676523  [   96/  146]
train() client id: f_00005-6-3 loss: 0.495372  [  128/  146]
train() client id: f_00005-7-0 loss: 0.493037  [   32/  146]
train() client id: f_00005-7-1 loss: 0.314863  [   64/  146]
train() client id: f_00005-7-2 loss: 0.738561  [   96/  146]
train() client id: f_00005-7-3 loss: 0.459898  [  128/  146]
train() client id: f_00005-8-0 loss: 0.617220  [   32/  146]
train() client id: f_00005-8-1 loss: 0.346360  [   64/  146]
train() client id: f_00005-8-2 loss: 0.464366  [   96/  146]
train() client id: f_00005-8-3 loss: 0.659544  [  128/  146]
train() client id: f_00005-9-0 loss: 0.466283  [   32/  146]
train() client id: f_00005-9-1 loss: 0.564594  [   64/  146]
train() client id: f_00005-9-2 loss: 0.529373  [   96/  146]
train() client id: f_00005-9-3 loss: 0.582017  [  128/  146]
train() client id: f_00005-10-0 loss: 0.569381  [   32/  146]
train() client id: f_00005-10-1 loss: 0.338531  [   64/  146]
train() client id: f_00005-10-2 loss: 0.542861  [   96/  146]
train() client id: f_00005-10-3 loss: 0.595123  [  128/  146]
train() client id: f_00006-0-0 loss: 0.445685  [   32/   54]
train() client id: f_00006-1-0 loss: 0.454547  [   32/   54]
train() client id: f_00006-2-0 loss: 0.433758  [   32/   54]
train() client id: f_00006-3-0 loss: 0.472011  [   32/   54]
train() client id: f_00006-4-0 loss: 0.529625  [   32/   54]
train() client id: f_00006-5-0 loss: 0.557815  [   32/   54]
train() client id: f_00006-6-0 loss: 0.550802  [   32/   54]
train() client id: f_00006-7-0 loss: 0.515753  [   32/   54]
train() client id: f_00006-8-0 loss: 0.496371  [   32/   54]
train() client id: f_00006-9-0 loss: 0.549847  [   32/   54]
train() client id: f_00006-10-0 loss: 0.557098  [   32/   54]
train() client id: f_00007-0-0 loss: 0.430290  [   32/  179]
train() client id: f_00007-0-1 loss: 0.613061  [   64/  179]
train() client id: f_00007-0-2 loss: 0.721076  [   96/  179]
train() client id: f_00007-0-3 loss: 0.709973  [  128/  179]
train() client id: f_00007-0-4 loss: 0.429425  [  160/  179]
train() client id: f_00007-1-0 loss: 0.395535  [   32/  179]
train() client id: f_00007-1-1 loss: 1.021576  [   64/  179]
train() client id: f_00007-1-2 loss: 0.522600  [   96/  179]
train() client id: f_00007-1-3 loss: 0.419973  [  128/  179]
train() client id: f_00007-1-4 loss: 0.453472  [  160/  179]
train() client id: f_00007-2-0 loss: 0.612254  [   32/  179]
train() client id: f_00007-2-1 loss: 0.736914  [   64/  179]
train() client id: f_00007-2-2 loss: 0.474014  [   96/  179]
train() client id: f_00007-2-3 loss: 0.581472  [  128/  179]
train() client id: f_00007-2-4 loss: 0.385325  [  160/  179]
train() client id: f_00007-3-0 loss: 0.629999  [   32/  179]
train() client id: f_00007-3-1 loss: 0.493871  [   64/  179]
train() client id: f_00007-3-2 loss: 0.668255  [   96/  179]
train() client id: f_00007-3-3 loss: 0.501159  [  128/  179]
train() client id: f_00007-3-4 loss: 0.444301  [  160/  179]
train() client id: f_00007-4-0 loss: 0.641435  [   32/  179]
train() client id: f_00007-4-1 loss: 0.394068  [   64/  179]
train() client id: f_00007-4-2 loss: 0.518508  [   96/  179]
train() client id: f_00007-4-3 loss: 0.617291  [  128/  179]
train() client id: f_00007-4-4 loss: 0.542202  [  160/  179]
train() client id: f_00007-5-0 loss: 0.565802  [   32/  179]
train() client id: f_00007-5-1 loss: 0.648220  [   64/  179]
train() client id: f_00007-5-2 loss: 0.355991  [   96/  179]
train() client id: f_00007-5-3 loss: 0.687676  [  128/  179]
train() client id: f_00007-5-4 loss: 0.375657  [  160/  179]
train() client id: f_00007-6-0 loss: 0.375850  [   32/  179]
train() client id: f_00007-6-1 loss: 0.665756  [   64/  179]
train() client id: f_00007-6-2 loss: 0.594222  [   96/  179]
train() client id: f_00007-6-3 loss: 0.352832  [  128/  179]
train() client id: f_00007-6-4 loss: 0.650855  [  160/  179]
train() client id: f_00007-7-0 loss: 0.676641  [   32/  179]
train() client id: f_00007-7-1 loss: 0.730645  [   64/  179]
train() client id: f_00007-7-2 loss: 0.391145  [   96/  179]
train() client id: f_00007-7-3 loss: 0.456760  [  128/  179]
train() client id: f_00007-7-4 loss: 0.439239  [  160/  179]
train() client id: f_00007-8-0 loss: 0.364329  [   32/  179]
train() client id: f_00007-8-1 loss: 0.807231  [   64/  179]
train() client id: f_00007-8-2 loss: 0.406734  [   96/  179]
train() client id: f_00007-8-3 loss: 0.477190  [  128/  179]
train() client id: f_00007-8-4 loss: 0.485741  [  160/  179]
train() client id: f_00007-9-0 loss: 0.464252  [   32/  179]
train() client id: f_00007-9-1 loss: 0.432430  [   64/  179]
train() client id: f_00007-9-2 loss: 0.357551  [   96/  179]
train() client id: f_00007-9-3 loss: 0.591905  [  128/  179]
train() client id: f_00007-9-4 loss: 0.500607  [  160/  179]
train() client id: f_00007-10-0 loss: 0.329734  [   32/  179]
train() client id: f_00007-10-1 loss: 0.850700  [   64/  179]
train() client id: f_00007-10-2 loss: 0.364620  [   96/  179]
train() client id: f_00007-10-3 loss: 0.443190  [  128/  179]
train() client id: f_00007-10-4 loss: 0.515996  [  160/  179]
train() client id: f_00008-0-0 loss: 0.719229  [   32/  130]
train() client id: f_00008-0-1 loss: 0.688766  [   64/  130]
train() client id: f_00008-0-2 loss: 0.898469  [   96/  130]
train() client id: f_00008-0-3 loss: 0.703654  [  128/  130]
train() client id: f_00008-1-0 loss: 0.783429  [   32/  130]
train() client id: f_00008-1-1 loss: 0.710882  [   64/  130]
train() client id: f_00008-1-2 loss: 0.666936  [   96/  130]
train() client id: f_00008-1-3 loss: 0.830206  [  128/  130]
train() client id: f_00008-2-0 loss: 0.684817  [   32/  130]
train() client id: f_00008-2-1 loss: 0.735143  [   64/  130]
train() client id: f_00008-2-2 loss: 0.820965  [   96/  130]
train() client id: f_00008-2-3 loss: 0.732813  [  128/  130]
train() client id: f_00008-3-0 loss: 0.666752  [   32/  130]
train() client id: f_00008-3-1 loss: 0.725512  [   64/  130]
train() client id: f_00008-3-2 loss: 0.855540  [   96/  130]
train() client id: f_00008-3-3 loss: 0.730337  [  128/  130]
train() client id: f_00008-4-0 loss: 0.657207  [   32/  130]
train() client id: f_00008-4-1 loss: 0.756911  [   64/  130]
train() client id: f_00008-4-2 loss: 0.813488  [   96/  130]
train() client id: f_00008-4-3 loss: 0.770190  [  128/  130]
train() client id: f_00008-5-0 loss: 0.810969  [   32/  130]
train() client id: f_00008-5-1 loss: 0.672246  [   64/  130]
train() client id: f_00008-5-2 loss: 0.773445  [   96/  130]
train() client id: f_00008-5-3 loss: 0.749744  [  128/  130]
train() client id: f_00008-6-0 loss: 0.681517  [   32/  130]
train() client id: f_00008-6-1 loss: 0.806457  [   64/  130]
train() client id: f_00008-6-2 loss: 0.664110  [   96/  130]
train() client id: f_00008-6-3 loss: 0.803773  [  128/  130]
train() client id: f_00008-7-0 loss: 0.724709  [   32/  130]
train() client id: f_00008-7-1 loss: 0.709991  [   64/  130]
train() client id: f_00008-7-2 loss: 0.781063  [   96/  130]
train() client id: f_00008-7-3 loss: 0.792151  [  128/  130]
train() client id: f_00008-8-0 loss: 0.794801  [   32/  130]
train() client id: f_00008-8-1 loss: 0.741413  [   64/  130]
train() client id: f_00008-8-2 loss: 0.671172  [   96/  130]
train() client id: f_00008-8-3 loss: 0.794852  [  128/  130]
train() client id: f_00008-9-0 loss: 0.802687  [   32/  130]
train() client id: f_00008-9-1 loss: 0.786206  [   64/  130]
train() client id: f_00008-9-2 loss: 0.657777  [   96/  130]
train() client id: f_00008-9-3 loss: 0.747562  [  128/  130]
train() client id: f_00008-10-0 loss: 0.689524  [   32/  130]
train() client id: f_00008-10-1 loss: 0.792203  [   64/  130]
train() client id: f_00008-10-2 loss: 0.726154  [   96/  130]
train() client id: f_00008-10-3 loss: 0.808178  [  128/  130]
train() client id: f_00009-0-0 loss: 1.017024  [   32/  118]
train() client id: f_00009-0-1 loss: 0.998846  [   64/  118]
train() client id: f_00009-0-2 loss: 1.041472  [   96/  118]
train() client id: f_00009-1-0 loss: 0.914864  [   32/  118]
train() client id: f_00009-1-1 loss: 1.069350  [   64/  118]
train() client id: f_00009-1-2 loss: 0.985288  [   96/  118]
train() client id: f_00009-2-0 loss: 0.879382  [   32/  118]
train() client id: f_00009-2-1 loss: 0.837428  [   64/  118]
train() client id: f_00009-2-2 loss: 1.071684  [   96/  118]
train() client id: f_00009-3-0 loss: 1.023407  [   32/  118]
train() client id: f_00009-3-1 loss: 0.976667  [   64/  118]
train() client id: f_00009-3-2 loss: 0.920949  [   96/  118]
train() client id: f_00009-4-0 loss: 0.871034  [   32/  118]
train() client id: f_00009-4-1 loss: 0.867015  [   64/  118]
train() client id: f_00009-4-2 loss: 0.904351  [   96/  118]
train() client id: f_00009-5-0 loss: 0.925955  [   32/  118]
train() client id: f_00009-5-1 loss: 0.952100  [   64/  118]
train() client id: f_00009-5-2 loss: 0.709067  [   96/  118]
train() client id: f_00009-6-0 loss: 0.791986  [   32/  118]
train() client id: f_00009-6-1 loss: 0.821342  [   64/  118]
train() client id: f_00009-6-2 loss: 1.006664  [   96/  118]
train() client id: f_00009-7-0 loss: 0.750728  [   32/  118]
train() client id: f_00009-7-1 loss: 0.821680  [   64/  118]
train() client id: f_00009-7-2 loss: 0.835968  [   96/  118]
train() client id: f_00009-8-0 loss: 0.825350  [   32/  118]
train() client id: f_00009-8-1 loss: 0.999038  [   64/  118]
train() client id: f_00009-8-2 loss: 0.619986  [   96/  118]
train() client id: f_00009-9-0 loss: 0.803663  [   32/  118]
train() client id: f_00009-9-1 loss: 0.790525  [   64/  118]
train() client id: f_00009-9-2 loss: 0.734405  [   96/  118]
train() client id: f_00009-10-0 loss: 0.708521  [   32/  118]
train() client id: f_00009-10-1 loss: 0.772105  [   64/  118]
train() client id: f_00009-10-2 loss: 0.800171  [   96/  118]
At round 58 accuracy: 0.6472148541114059
At round 58 training accuracy: 0.590878604963112
At round 58 training loss: 0.813900855012203
update_location
xs = [  -3.9056584     4.20031788  310.00902392   18.81129433    0.97929623
    3.95640986 -272.44319194 -251.32485185  294.66397685 -237.06087855]
ys = [ 302.5879595   285.55583871    1.32061395 -272.45517586  264.35018685
  247.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [318.70790295 302.58846589 325.74121466 290.83618696 282.63400416
 267.25924958 290.22781289 270.490032   311.66573316 257.32056272]
dists_bs = [213.4290541  210.55960534 515.16820789 487.62583218 197.36303755
 193.20273595 202.59765721 190.16660451 495.34711972 181.91219277]
uav_gains = [1.90906750e-12 2.54420374e-12 1.70337274e-12 3.20136059e-12
 3.78635653e-12 5.22235086e-12 3.24089386e-12 4.88091679e-12
 2.15488058e-12 6.41123405e-12]
bs_gains = [3.32188179e-11 3.45019724e-11 2.81732361e-12 3.28587708e-12
 4.13570723e-11 4.38992321e-11 3.84341882e-11 4.58900129e-11
 3.14446733e-12 5.19614211e-11]
Round 59
-------------------------------
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.22013439 6.49883235 3.18055184 1.17195349 7.49218211 3.60548997
 1.43848068 4.45606788 3.28586187 2.92340788]
obj_prev = 37.272962461679306
eta_min = 1.7285175423879787e-29	eta_max = 0.9377803249311158
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 8.56318552740206	eta = 0.909090909090909
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 19.714881852065698	eta = 0.3948648627079844
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 13.689416383079331	eta = 0.5686666179167633
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.622409124377576	eta = 0.6167375846489944
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557303683729339	eta = 0.6199351637809642
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557030281762911	eta = 0.6199486615180114
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557030276907934	eta = 0.6199486617577047
eta = 0.6199486617577047
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [0.0394532  0.08297697 0.03882696 0.01346419 0.09581492 0.04571561
 0.01690851 0.0560486  0.04070568 0.03694824]
ene_total = [1.24429988 1.90408729 1.25458552 0.61484934 2.16708445 1.11417022
 0.68545833 1.4588381  1.18872479 0.92493235]
ti_comp = [1.09084903 1.21156017 1.07968336 1.13102359 1.21456653 1.21550713
 1.13180693 1.15453068 1.12986699 1.21804473]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [3.22550318e-06 2.43255565e-05 3.13824884e-06 1.19255159e-07
 3.72680608e-05 4.04164983e-06 2.35858602e-07 8.25587810e-06
 3.30210249e-06 2.12488134e-06]
ene_total = [0.4167514  0.16397633 0.44017198 0.33241142 0.15794132 0.15527123
 0.33077065 0.2832709  0.3349044  0.14990784]
optimize_network_iter = 0 obj = 2.7653774631169483
eta = 0.6199486617577047
freqs = [18083713.04857494 34243850.94632931 17980716.29606591  5952213.52469169
 39444079.62545728 18805161.01539245  7469699.35866398 24273324.57197437
 18013484.95120145 15167027.70060071]
eta_min = 0.6199486617577048	eta_max = 0.7133529451356779
af = 0.0014346255173217418	bf = 1.0502994844473539	zeta = 0.001578088069053916	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [6.42985407e-07 4.84915903e-06 6.25591760e-07 2.37728263e-08
 7.42917241e-06 8.05679522e-07 4.70170484e-08 1.64576156e-06
 6.58255038e-07 4.23582807e-07]
ene_total = [1.74202974 0.68380564 1.83994709 1.38965906 0.65766716 0.64883763
 1.38279153 1.18365265 1.39985765 0.62655025]
ti_comp = [0.77393525 0.89464639 0.76276958 0.81410981 0.89765275 0.89859335
 0.81489315 0.8376169  0.81295321 0.90113095]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.25947941e-06 1.57304375e-05 2.21709516e-06 8.11606270e-08
 2.40576692e-05 2.60758319e-06 1.60430136e-07 5.53062528e-06
 2.24908028e-06 1.36891415e-06]
ene_total = [0.55252348 0.21716916 0.58357692 0.44072701 0.20903928 0.20582665
 0.43855056 0.37549926 0.44400411 0.19873447]
optimize_network_iter = 1 obj = 3.6656508882711805
eta = 0.7133529451356779
freqs = [18007133.55773868 32762205.35417381 17980716.2960659   5842034.71544074
 37704373.23913116 17970837.74832523  7329456.31607533 23636668.595279
 17687093.31024747 14483474.75860257]
eta_min = 0.7133529451356788	eta_max = 0.7133529451356644
af = 0.0013279300383138625	bf = 1.0502994844473539	zeta = 0.0014607230421452488	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [6.37551209e-07 4.43861511e-06 6.25591760e-07 2.29008752e-08
 6.78828762e-06 7.35774715e-07 4.52681388e-08 1.56056161e-06
 6.34616915e-07 3.86262812e-07]
ene_total = [1.74202926 0.68376964 1.83994709 1.38965899 0.65761096 0.6488315
 1.38279138 1.18364518 1.39985558 0.62654698]
ti_comp = [0.77393525 0.89464639 0.76276958 0.81410981 0.89765275 0.89859335
 0.81489315 0.8376169  0.81295321 0.90113095]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.25947941e-06 1.57304375e-05 2.21709516e-06 8.11606270e-08
 2.40576692e-05 2.60758319e-06 1.60430136e-07 5.53062528e-06
 2.24908028e-06 1.36891415e-06]
ene_total = [0.55252348 0.21716916 0.58357692 0.44072701 0.20903928 0.20582665
 0.43855056 0.37549926 0.44400411 0.19873447]
optimize_network_iter = 2 obj = 3.6656508882710073
eta = 0.7133529451356644
freqs = [18007133.55773862 32762205.35417396 17980716.29606583  5842034.71544073
 37704373.23913135 17970837.74832532  7329456.31607533 23636668.59527903
 17687093.31024747 14483474.75860265]
Done!
At round 59 eta: 0.7133529451356644
At round 59 local rounds: 11.060599497015481
At round 59 global rounds: 27.812593047006303
At round 59 a_n: 7.62985203809534
gradient difference: 0.4699738919734955
train() client id: f_00000-0-0 loss: 1.363903  [   32/  126]
train() client id: f_00000-0-1 loss: 1.193836  [   64/  126]
train() client id: f_00000-0-2 loss: 1.285837  [   96/  126]
train() client id: f_00000-1-0 loss: 1.129111  [   32/  126]
train() client id: f_00000-1-1 loss: 1.142314  [   64/  126]
train() client id: f_00000-1-2 loss: 1.108313  [   96/  126]
train() client id: f_00000-2-0 loss: 0.970546  [   32/  126]
train() client id: f_00000-2-1 loss: 1.037900  [   64/  126]
train() client id: f_00000-2-2 loss: 1.002384  [   96/  126]
train() client id: f_00000-3-0 loss: 0.931831  [   32/  126]
train() client id: f_00000-3-1 loss: 0.938778  [   64/  126]
train() client id: f_00000-3-2 loss: 0.967608  [   96/  126]
train() client id: f_00000-4-0 loss: 1.023263  [   32/  126]
train() client id: f_00000-4-1 loss: 0.858539  [   64/  126]
train() client id: f_00000-4-2 loss: 0.808052  [   96/  126]
train() client id: f_00000-5-0 loss: 0.769688  [   32/  126]
train() client id: f_00000-5-1 loss: 0.890473  [   64/  126]
train() client id: f_00000-5-2 loss: 0.765126  [   96/  126]
train() client id: f_00000-6-0 loss: 0.777490  [   32/  126]
train() client id: f_00000-6-1 loss: 0.814689  [   64/  126]
train() client id: f_00000-6-2 loss: 0.702455  [   96/  126]
train() client id: f_00000-7-0 loss: 0.825870  [   32/  126]
train() client id: f_00000-7-1 loss: 0.803241  [   64/  126]
train() client id: f_00000-7-2 loss: 0.581715  [   96/  126]
train() client id: f_00000-8-0 loss: 0.923393  [   32/  126]
train() client id: f_00000-8-1 loss: 0.599009  [   64/  126]
train() client id: f_00000-8-2 loss: 0.669206  [   96/  126]
train() client id: f_00000-9-0 loss: 0.642882  [   32/  126]
train() client id: f_00000-9-1 loss: 0.731291  [   64/  126]
train() client id: f_00000-9-2 loss: 0.776783  [   96/  126]
train() client id: f_00000-10-0 loss: 0.705043  [   32/  126]
train() client id: f_00000-10-1 loss: 0.630251  [   64/  126]
train() client id: f_00000-10-2 loss: 0.718539  [   96/  126]
train() client id: f_00001-0-0 loss: 0.422967  [   32/  265]
train() client id: f_00001-0-1 loss: 0.443306  [   64/  265]
train() client id: f_00001-0-2 loss: 0.420069  [   96/  265]
train() client id: f_00001-0-3 loss: 0.395836  [  128/  265]
train() client id: f_00001-0-4 loss: 0.395628  [  160/  265]
train() client id: f_00001-0-5 loss: 0.534970  [  192/  265]
train() client id: f_00001-0-6 loss: 0.448957  [  224/  265]
train() client id: f_00001-0-7 loss: 0.477251  [  256/  265]
train() client id: f_00001-1-0 loss: 0.485717  [   32/  265]
train() client id: f_00001-1-1 loss: 0.543937  [   64/  265]
train() client id: f_00001-1-2 loss: 0.386567  [   96/  265]
train() client id: f_00001-1-3 loss: 0.323657  [  128/  265]
train() client id: f_00001-1-4 loss: 0.430676  [  160/  265]
train() client id: f_00001-1-5 loss: 0.454576  [  192/  265]
train() client id: f_00001-1-6 loss: 0.403552  [  224/  265]
train() client id: f_00001-1-7 loss: 0.463903  [  256/  265]
train() client id: f_00001-2-0 loss: 0.410419  [   32/  265]
train() client id: f_00001-2-1 loss: 0.504194  [   64/  265]
train() client id: f_00001-2-2 loss: 0.506853  [   96/  265]
train() client id: f_00001-2-3 loss: 0.428290  [  128/  265]
train() client id: f_00001-2-4 loss: 0.417467  [  160/  265]
train() client id: f_00001-2-5 loss: 0.398515  [  192/  265]
train() client id: f_00001-2-6 loss: 0.410978  [  224/  265]
train() client id: f_00001-2-7 loss: 0.351034  [  256/  265]
train() client id: f_00001-3-0 loss: 0.419675  [   32/  265]
train() client id: f_00001-3-1 loss: 0.518347  [   64/  265]
train() client id: f_00001-3-2 loss: 0.438917  [   96/  265]
train() client id: f_00001-3-3 loss: 0.352863  [  128/  265]
train() client id: f_00001-3-4 loss: 0.409413  [  160/  265]
train() client id: f_00001-3-5 loss: 0.424333  [  192/  265]
train() client id: f_00001-3-6 loss: 0.456458  [  224/  265]
train() client id: f_00001-3-7 loss: 0.355125  [  256/  265]
train() client id: f_00001-4-0 loss: 0.313721  [   32/  265]
train() client id: f_00001-4-1 loss: 0.342026  [   64/  265]
train() client id: f_00001-4-2 loss: 0.593891  [   96/  265]
train() client id: f_00001-4-3 loss: 0.412340  [  128/  265]
train() client id: f_00001-4-4 loss: 0.455444  [  160/  265]
train() client id: f_00001-4-5 loss: 0.489021  [  192/  265]
train() client id: f_00001-4-6 loss: 0.392306  [  224/  265]
train() client id: f_00001-4-7 loss: 0.304595  [  256/  265]
train() client id: f_00001-5-0 loss: 0.418961  [   32/  265]
train() client id: f_00001-5-1 loss: 0.371599  [   64/  265]
train() client id: f_00001-5-2 loss: 0.435708  [   96/  265]
train() client id: f_00001-5-3 loss: 0.307561  [  128/  265]
train() client id: f_00001-5-4 loss: 0.362472  [  160/  265]
train() client id: f_00001-5-5 loss: 0.430991  [  192/  265]
train() client id: f_00001-5-6 loss: 0.443264  [  224/  265]
train() client id: f_00001-5-7 loss: 0.466425  [  256/  265]
train() client id: f_00001-6-0 loss: 0.462702  [   32/  265]
train() client id: f_00001-6-1 loss: 0.397832  [   64/  265]
train() client id: f_00001-6-2 loss: 0.341524  [   96/  265]
train() client id: f_00001-6-3 loss: 0.526816  [  128/  265]
train() client id: f_00001-6-4 loss: 0.410887  [  160/  265]
train() client id: f_00001-6-5 loss: 0.432072  [  192/  265]
train() client id: f_00001-6-6 loss: 0.359490  [  224/  265]
train() client id: f_00001-6-7 loss: 0.340672  [  256/  265]
train() client id: f_00001-7-0 loss: 0.353927  [   32/  265]
train() client id: f_00001-7-1 loss: 0.433457  [   64/  265]
train() client id: f_00001-7-2 loss: 0.470211  [   96/  265]
train() client id: f_00001-7-3 loss: 0.522905  [  128/  265]
train() client id: f_00001-7-4 loss: 0.393676  [  160/  265]
train() client id: f_00001-7-5 loss: 0.369469  [  192/  265]
train() client id: f_00001-7-6 loss: 0.332427  [  224/  265]
train() client id: f_00001-7-7 loss: 0.371742  [  256/  265]
train() client id: f_00001-8-0 loss: 0.415180  [   32/  265]
train() client id: f_00001-8-1 loss: 0.362791  [   64/  265]
train() client id: f_00001-8-2 loss: 0.350950  [   96/  265]
train() client id: f_00001-8-3 loss: 0.494036  [  128/  265]
train() client id: f_00001-8-4 loss: 0.461056  [  160/  265]
train() client id: f_00001-8-5 loss: 0.473193  [  192/  265]
train() client id: f_00001-8-6 loss: 0.303742  [  224/  265]
train() client id: f_00001-8-7 loss: 0.381700  [  256/  265]
train() client id: f_00001-9-0 loss: 0.352484  [   32/  265]
train() client id: f_00001-9-1 loss: 0.374944  [   64/  265]
train() client id: f_00001-9-2 loss: 0.443661  [   96/  265]
train() client id: f_00001-9-3 loss: 0.454165  [  128/  265]
train() client id: f_00001-9-4 loss: 0.316440  [  160/  265]
train() client id: f_00001-9-5 loss: 0.388748  [  192/  265]
train() client id: f_00001-9-6 loss: 0.495056  [  224/  265]
train() client id: f_00001-9-7 loss: 0.388033  [  256/  265]
train() client id: f_00001-10-0 loss: 0.381143  [   32/  265]
train() client id: f_00001-10-1 loss: 0.487679  [   64/  265]
train() client id: f_00001-10-2 loss: 0.353570  [   96/  265]
train() client id: f_00001-10-3 loss: 0.397434  [  128/  265]
train() client id: f_00001-10-4 loss: 0.498327  [  160/  265]
train() client id: f_00001-10-5 loss: 0.302013  [  192/  265]
train() client id: f_00001-10-6 loss: 0.439878  [  224/  265]
train() client id: f_00001-10-7 loss: 0.349496  [  256/  265]
train() client id: f_00002-0-0 loss: 1.345205  [   32/  124]
train() client id: f_00002-0-1 loss: 0.764921  [   64/  124]
train() client id: f_00002-0-2 loss: 0.801709  [   96/  124]
train() client id: f_00002-1-0 loss: 1.137152  [   32/  124]
train() client id: f_00002-1-1 loss: 1.001264  [   64/  124]
train() client id: f_00002-1-2 loss: 1.005071  [   96/  124]
train() client id: f_00002-2-0 loss: 0.870319  [   32/  124]
train() client id: f_00002-2-1 loss: 0.956268  [   64/  124]
train() client id: f_00002-2-2 loss: 0.867543  [   96/  124]
train() client id: f_00002-3-0 loss: 0.968532  [   32/  124]
train() client id: f_00002-3-1 loss: 0.853176  [   64/  124]
train() client id: f_00002-3-2 loss: 0.936260  [   96/  124]
train() client id: f_00002-4-0 loss: 0.866041  [   32/  124]
train() client id: f_00002-4-1 loss: 1.099069  [   64/  124]
train() client id: f_00002-4-2 loss: 0.764830  [   96/  124]
train() client id: f_00002-5-0 loss: 0.788483  [   32/  124]
train() client id: f_00002-5-1 loss: 0.893767  [   64/  124]
train() client id: f_00002-5-2 loss: 1.069004  [   96/  124]
train() client id: f_00002-6-0 loss: 0.936488  [   32/  124]
train() client id: f_00002-6-1 loss: 0.805595  [   64/  124]
train() client id: f_00002-6-2 loss: 0.895163  [   96/  124]
train() client id: f_00002-7-0 loss: 0.927605  [   32/  124]
train() client id: f_00002-7-1 loss: 0.699794  [   64/  124]
train() client id: f_00002-7-2 loss: 0.989199  [   96/  124]
train() client id: f_00002-8-0 loss: 0.946158  [   32/  124]
train() client id: f_00002-8-1 loss: 0.759193  [   64/  124]
train() client id: f_00002-8-2 loss: 0.835602  [   96/  124]
train() client id: f_00002-9-0 loss: 0.798635  [   32/  124]
train() client id: f_00002-9-1 loss: 0.970564  [   64/  124]
train() client id: f_00002-9-2 loss: 0.869140  [   96/  124]
train() client id: f_00002-10-0 loss: 1.106871  [   32/  124]
train() client id: f_00002-10-1 loss: 0.766473  [   64/  124]
train() client id: f_00002-10-2 loss: 0.854053  [   96/  124]
train() client id: f_00003-0-0 loss: 0.613335  [   32/   43]
train() client id: f_00003-1-0 loss: 0.523602  [   32/   43]
train() client id: f_00003-2-0 loss: 0.661662  [   32/   43]
train() client id: f_00003-3-0 loss: 0.628550  [   32/   43]
train() client id: f_00003-4-0 loss: 0.694963  [   32/   43]
train() client id: f_00003-5-0 loss: 0.811140  [   32/   43]
train() client id: f_00003-6-0 loss: 0.687388  [   32/   43]
train() client id: f_00003-7-0 loss: 0.535526  [   32/   43]
train() client id: f_00003-8-0 loss: 0.675784  [   32/   43]
train() client id: f_00003-9-0 loss: 0.489869  [   32/   43]
train() client id: f_00003-10-0 loss: 0.689203  [   32/   43]
train() client id: f_00004-0-0 loss: 0.670096  [   32/  306]
train() client id: f_00004-0-1 loss: 0.729962  [   64/  306]
train() client id: f_00004-0-2 loss: 0.663399  [   96/  306]
train() client id: f_00004-0-3 loss: 0.576602  [  128/  306]
train() client id: f_00004-0-4 loss: 0.653178  [  160/  306]
train() client id: f_00004-0-5 loss: 0.391552  [  192/  306]
train() client id: f_00004-0-6 loss: 0.588492  [  224/  306]
train() client id: f_00004-0-7 loss: 0.831538  [  256/  306]
train() client id: f_00004-0-8 loss: 0.779466  [  288/  306]
train() client id: f_00004-1-0 loss: 0.574064  [   32/  306]
train() client id: f_00004-1-1 loss: 0.820148  [   64/  306]
train() client id: f_00004-1-2 loss: 0.744743  [   96/  306]
train() client id: f_00004-1-3 loss: 0.747502  [  128/  306]
train() client id: f_00004-1-4 loss: 0.566849  [  160/  306]
train() client id: f_00004-1-5 loss: 0.578739  [  192/  306]
train() client id: f_00004-1-6 loss: 0.763200  [  224/  306]
train() client id: f_00004-1-7 loss: 0.571621  [  256/  306]
train() client id: f_00004-1-8 loss: 0.576913  [  288/  306]
train() client id: f_00004-2-0 loss: 0.674223  [   32/  306]
train() client id: f_00004-2-1 loss: 0.764820  [   64/  306]
train() client id: f_00004-2-2 loss: 0.581286  [   96/  306]
train() client id: f_00004-2-3 loss: 0.603347  [  128/  306]
train() client id: f_00004-2-4 loss: 0.511911  [  160/  306]
train() client id: f_00004-2-5 loss: 0.693543  [  192/  306]
train() client id: f_00004-2-6 loss: 0.740526  [  224/  306]
train() client id: f_00004-2-7 loss: 0.634603  [  256/  306]
train() client id: f_00004-2-8 loss: 0.692256  [  288/  306]
train() client id: f_00004-3-0 loss: 0.630135  [   32/  306]
train() client id: f_00004-3-1 loss: 0.736757  [   64/  306]
train() client id: f_00004-3-2 loss: 0.713266  [   96/  306]
train() client id: f_00004-3-3 loss: 0.651387  [  128/  306]
train() client id: f_00004-3-4 loss: 0.558813  [  160/  306]
train() client id: f_00004-3-5 loss: 0.460017  [  192/  306]
train() client id: f_00004-3-6 loss: 0.790294  [  224/  306]
train() client id: f_00004-3-7 loss: 0.682976  [  256/  306]
train() client id: f_00004-3-8 loss: 0.679277  [  288/  306]
train() client id: f_00004-4-0 loss: 0.591330  [   32/  306]
train() client id: f_00004-4-1 loss: 0.686457  [   64/  306]
train() client id: f_00004-4-2 loss: 0.641195  [   96/  306]
train() client id: f_00004-4-3 loss: 0.658544  [  128/  306]
train() client id: f_00004-4-4 loss: 0.732290  [  160/  306]
train() client id: f_00004-4-5 loss: 0.642667  [  192/  306]
train() client id: f_00004-4-6 loss: 0.556444  [  224/  306]
train() client id: f_00004-4-7 loss: 0.717090  [  256/  306]
train() client id: f_00004-4-8 loss: 0.743981  [  288/  306]
train() client id: f_00004-5-0 loss: 0.584607  [   32/  306]
train() client id: f_00004-5-1 loss: 0.679632  [   64/  306]
train() client id: f_00004-5-2 loss: 0.761459  [   96/  306]
train() client id: f_00004-5-3 loss: 0.621152  [  128/  306]
train() client id: f_00004-5-4 loss: 0.560483  [  160/  306]
train() client id: f_00004-5-5 loss: 0.809128  [  192/  306]
train() client id: f_00004-5-6 loss: 0.719519  [  224/  306]
train() client id: f_00004-5-7 loss: 0.563767  [  256/  306]
train() client id: f_00004-5-8 loss: 0.594052  [  288/  306]
train() client id: f_00004-6-0 loss: 0.653241  [   32/  306]
train() client id: f_00004-6-1 loss: 0.562869  [   64/  306]
train() client id: f_00004-6-2 loss: 0.677269  [   96/  306]
train() client id: f_00004-6-3 loss: 0.605557  [  128/  306]
train() client id: f_00004-6-4 loss: 0.810157  [  160/  306]
train() client id: f_00004-6-5 loss: 0.650673  [  192/  306]
train() client id: f_00004-6-6 loss: 0.633517  [  224/  306]
train() client id: f_00004-6-7 loss: 0.654674  [  256/  306]
train() client id: f_00004-6-8 loss: 0.698830  [  288/  306]
train() client id: f_00004-7-0 loss: 0.732862  [   32/  306]
train() client id: f_00004-7-1 loss: 0.685488  [   64/  306]
train() client id: f_00004-7-2 loss: 0.614743  [   96/  306]
train() client id: f_00004-7-3 loss: 0.636840  [  128/  306]
train() client id: f_00004-7-4 loss: 0.639590  [  160/  306]
train() client id: f_00004-7-5 loss: 0.708765  [  192/  306]
train() client id: f_00004-7-6 loss: 0.726929  [  224/  306]
train() client id: f_00004-7-7 loss: 0.634325  [  256/  306]
train() client id: f_00004-7-8 loss: 0.601699  [  288/  306]
train() client id: f_00004-8-0 loss: 0.644753  [   32/  306]
train() client id: f_00004-8-1 loss: 0.690440  [   64/  306]
train() client id: f_00004-8-2 loss: 0.779107  [   96/  306]
train() client id: f_00004-8-3 loss: 0.669141  [  128/  306]
train() client id: f_00004-8-4 loss: 0.636624  [  160/  306]
train() client id: f_00004-8-5 loss: 0.639440  [  192/  306]
train() client id: f_00004-8-6 loss: 0.568599  [  224/  306]
train() client id: f_00004-8-7 loss: 0.739677  [  256/  306]
train() client id: f_00004-8-8 loss: 0.649976  [  288/  306]
train() client id: f_00004-9-0 loss: 0.620787  [   32/  306]
train() client id: f_00004-9-1 loss: 0.683328  [   64/  306]
train() client id: f_00004-9-2 loss: 0.722371  [   96/  306]
train() client id: f_00004-9-3 loss: 0.624642  [  128/  306]
train() client id: f_00004-9-4 loss: 0.757792  [  160/  306]
train() client id: f_00004-9-5 loss: 0.621267  [  192/  306]
train() client id: f_00004-9-6 loss: 0.643103  [  224/  306]
train() client id: f_00004-9-7 loss: 0.583629  [  256/  306]
train() client id: f_00004-9-8 loss: 0.737155  [  288/  306]
train() client id: f_00004-10-0 loss: 0.617435  [   32/  306]
train() client id: f_00004-10-1 loss: 0.754433  [   64/  306]
train() client id: f_00004-10-2 loss: 0.697488  [   96/  306]
train() client id: f_00004-10-3 loss: 0.619200  [  128/  306]
train() client id: f_00004-10-4 loss: 0.669527  [  160/  306]
train() client id: f_00004-10-5 loss: 0.606997  [  192/  306]
train() client id: f_00004-10-6 loss: 0.639975  [  224/  306]
train() client id: f_00004-10-7 loss: 0.685326  [  256/  306]
train() client id: f_00004-10-8 loss: 0.812182  [  288/  306]
train() client id: f_00005-0-0 loss: 0.514838  [   32/  146]
train() client id: f_00005-0-1 loss: 0.615230  [   64/  146]
train() client id: f_00005-0-2 loss: 0.723116  [   96/  146]
train() client id: f_00005-0-3 loss: 0.596998  [  128/  146]
train() client id: f_00005-1-0 loss: 0.675909  [   32/  146]
train() client id: f_00005-1-1 loss: 0.592556  [   64/  146]
train() client id: f_00005-1-2 loss: 0.632822  [   96/  146]
train() client id: f_00005-1-3 loss: 0.767714  [  128/  146]
train() client id: f_00005-2-0 loss: 0.551251  [   32/  146]
train() client id: f_00005-2-1 loss: 0.657034  [   64/  146]
train() client id: f_00005-2-2 loss: 0.471243  [   96/  146]
train() client id: f_00005-2-3 loss: 0.810008  [  128/  146]
train() client id: f_00005-3-0 loss: 0.726745  [   32/  146]
train() client id: f_00005-3-1 loss: 0.439309  [   64/  146]
train() client id: f_00005-3-2 loss: 0.729969  [   96/  146]
train() client id: f_00005-3-3 loss: 0.597946  [  128/  146]
train() client id: f_00005-4-0 loss: 0.410301  [   32/  146]
train() client id: f_00005-4-1 loss: 0.710849  [   64/  146]
train() client id: f_00005-4-2 loss: 0.467971  [   96/  146]
train() client id: f_00005-4-3 loss: 0.613165  [  128/  146]
train() client id: f_00005-5-0 loss: 0.723236  [   32/  146]
train() client id: f_00005-5-1 loss: 0.621736  [   64/  146]
train() client id: f_00005-5-2 loss: 0.437059  [   96/  146]
train() client id: f_00005-5-3 loss: 0.620778  [  128/  146]
train() client id: f_00005-6-0 loss: 0.596593  [   32/  146]
train() client id: f_00005-6-1 loss: 0.742432  [   64/  146]
train() client id: f_00005-6-2 loss: 0.561881  [   96/  146]
train() client id: f_00005-6-3 loss: 0.653259  [  128/  146]
train() client id: f_00005-7-0 loss: 0.329387  [   32/  146]
train() client id: f_00005-7-1 loss: 0.746534  [   64/  146]
train() client id: f_00005-7-2 loss: 0.681599  [   96/  146]
train() client id: f_00005-7-3 loss: 0.676162  [  128/  146]
train() client id: f_00005-8-0 loss: 0.499611  [   32/  146]
train() client id: f_00005-8-1 loss: 0.539193  [   64/  146]
train() client id: f_00005-8-2 loss: 0.806812  [   96/  146]
train() client id: f_00005-8-3 loss: 0.563903  [  128/  146]
train() client id: f_00005-9-0 loss: 0.948486  [   32/  146]
train() client id: f_00005-9-1 loss: 0.337435  [   64/  146]
train() client id: f_00005-9-2 loss: 0.627812  [   96/  146]
train() client id: f_00005-9-3 loss: 0.723312  [  128/  146]
train() client id: f_00005-10-0 loss: 0.600880  [   32/  146]
train() client id: f_00005-10-1 loss: 0.460003  [   64/  146]
train() client id: f_00005-10-2 loss: 0.625205  [   96/  146]
train() client id: f_00005-10-3 loss: 0.760627  [  128/  146]
train() client id: f_00006-0-0 loss: 0.492217  [   32/   54]
train() client id: f_00006-1-0 loss: 0.459676  [   32/   54]
train() client id: f_00006-2-0 loss: 0.406692  [   32/   54]
train() client id: f_00006-3-0 loss: 0.405694  [   32/   54]
train() client id: f_00006-4-0 loss: 0.467182  [   32/   54]
train() client id: f_00006-5-0 loss: 0.450232  [   32/   54]
train() client id: f_00006-6-0 loss: 0.499019  [   32/   54]
train() client id: f_00006-7-0 loss: 0.427606  [   32/   54]
train() client id: f_00006-8-0 loss: 0.449714  [   32/   54]
train() client id: f_00006-9-0 loss: 0.507012  [   32/   54]
train() client id: f_00006-10-0 loss: 0.506699  [   32/   54]
train() client id: f_00007-0-0 loss: 0.530051  [   32/  179]
train() client id: f_00007-0-1 loss: 0.577389  [   64/  179]
train() client id: f_00007-0-2 loss: 0.624990  [   96/  179]
train() client id: f_00007-0-3 loss: 0.538973  [  128/  179]
train() client id: f_00007-0-4 loss: 0.562534  [  160/  179]
train() client id: f_00007-1-0 loss: 0.456722  [   32/  179]
train() client id: f_00007-1-1 loss: 0.568184  [   64/  179]
train() client id: f_00007-1-2 loss: 0.595230  [   96/  179]
train() client id: f_00007-1-3 loss: 0.527336  [  128/  179]
train() client id: f_00007-1-4 loss: 0.516187  [  160/  179]
train() client id: f_00007-2-0 loss: 0.605632  [   32/  179]
train() client id: f_00007-2-1 loss: 0.664985  [   64/  179]
train() client id: f_00007-2-2 loss: 0.578116  [   96/  179]
train() client id: f_00007-2-3 loss: 0.463130  [  128/  179]
train() client id: f_00007-2-4 loss: 0.496327  [  160/  179]
train() client id: f_00007-3-0 loss: 0.572459  [   32/  179]
train() client id: f_00007-3-1 loss: 0.571759  [   64/  179]
train() client id: f_00007-3-2 loss: 0.487616  [   96/  179]
train() client id: f_00007-3-3 loss: 0.437672  [  128/  179]
train() client id: f_00007-3-4 loss: 0.629470  [  160/  179]
train() client id: f_00007-4-0 loss: 0.384893  [   32/  179]
train() client id: f_00007-4-1 loss: 0.391011  [   64/  179]
train() client id: f_00007-4-2 loss: 0.546053  [   96/  179]
train() client id: f_00007-4-3 loss: 0.831888  [  128/  179]
train() client id: f_00007-4-4 loss: 0.472661  [  160/  179]
train() client id: f_00007-5-0 loss: 0.453941  [   32/  179]
train() client id: f_00007-5-1 loss: 0.595407  [   64/  179]
train() client id: f_00007-5-2 loss: 0.528047  [   96/  179]
train() client id: f_00007-5-3 loss: 0.519574  [  128/  179]
train() client id: f_00007-5-4 loss: 0.627210  [  160/  179]
train() client id: f_00007-6-0 loss: 0.415270  [   32/  179]
train() client id: f_00007-6-1 loss: 0.404317  [   64/  179]
train() client id: f_00007-6-2 loss: 0.443072  [   96/  179]
train() client id: f_00007-6-3 loss: 0.631225  [  128/  179]
train() client id: f_00007-6-4 loss: 0.770349  [  160/  179]
train() client id: f_00007-7-0 loss: 0.505358  [   32/  179]
train() client id: f_00007-7-1 loss: 0.722465  [   64/  179]
train() client id: f_00007-7-2 loss: 0.361946  [   96/  179]
train() client id: f_00007-7-3 loss: 0.478317  [  128/  179]
train() client id: f_00007-7-4 loss: 0.487328  [  160/  179]
train() client id: f_00007-8-0 loss: 0.558476  [   32/  179]
train() client id: f_00007-8-1 loss: 0.548845  [   64/  179]
train() client id: f_00007-8-2 loss: 0.601671  [   96/  179]
train() client id: f_00007-8-3 loss: 0.392525  [  128/  179]
train() client id: f_00007-8-4 loss: 0.501012  [  160/  179]
train() client id: f_00007-9-0 loss: 0.649382  [   32/  179]
train() client id: f_00007-9-1 loss: 0.342412  [   64/  179]
train() client id: f_00007-9-2 loss: 0.457515  [   96/  179]
train() client id: f_00007-9-3 loss: 0.634188  [  128/  179]
train() client id: f_00007-9-4 loss: 0.455147  [  160/  179]
train() client id: f_00007-10-0 loss: 0.426218  [   32/  179]
train() client id: f_00007-10-1 loss: 0.554050  [   64/  179]
train() client id: f_00007-10-2 loss: 0.523884  [   96/  179]
train() client id: f_00007-10-3 loss: 0.533972  [  128/  179]
train() client id: f_00007-10-4 loss: 0.458413  [  160/  179]
train() client id: f_00008-0-0 loss: 0.650729  [   32/  130]
train() client id: f_00008-0-1 loss: 0.647514  [   64/  130]
train() client id: f_00008-0-2 loss: 0.604285  [   96/  130]
train() client id: f_00008-0-3 loss: 0.669860  [  128/  130]
train() client id: f_00008-1-0 loss: 0.633500  [   32/  130]
train() client id: f_00008-1-1 loss: 0.691656  [   64/  130]
train() client id: f_00008-1-2 loss: 0.705170  [   96/  130]
train() client id: f_00008-1-3 loss: 0.555006  [  128/  130]
train() client id: f_00008-2-0 loss: 0.577896  [   32/  130]
train() client id: f_00008-2-1 loss: 0.534278  [   64/  130]
train() client id: f_00008-2-2 loss: 0.682221  [   96/  130]
train() client id: f_00008-2-3 loss: 0.736202  [  128/  130]
train() client id: f_00008-3-0 loss: 0.617624  [   32/  130]
train() client id: f_00008-3-1 loss: 0.640002  [   64/  130]
train() client id: f_00008-3-2 loss: 0.597731  [   96/  130]
train() client id: f_00008-3-3 loss: 0.676860  [  128/  130]
train() client id: f_00008-4-0 loss: 0.599736  [   32/  130]
train() client id: f_00008-4-1 loss: 0.708619  [   64/  130]
train() client id: f_00008-4-2 loss: 0.677126  [   96/  130]
train() client id: f_00008-4-3 loss: 0.595773  [  128/  130]
train() client id: f_00008-5-0 loss: 0.605099  [   32/  130]
train() client id: f_00008-5-1 loss: 0.512036  [   64/  130]
train() client id: f_00008-5-2 loss: 0.759743  [   96/  130]
train() client id: f_00008-5-3 loss: 0.695172  [  128/  130]
train() client id: f_00008-6-0 loss: 0.570857  [   32/  130]
train() client id: f_00008-6-1 loss: 0.681250  [   64/  130]
train() client id: f_00008-6-2 loss: 0.663022  [   96/  130]
train() client id: f_00008-6-3 loss: 0.664842  [  128/  130]
train() client id: f_00008-7-0 loss: 0.703201  [   32/  130]
train() client id: f_00008-7-1 loss: 0.632806  [   64/  130]
train() client id: f_00008-7-2 loss: 0.597789  [   96/  130]
train() client id: f_00008-7-3 loss: 0.648720  [  128/  130]
train() client id: f_00008-8-0 loss: 0.635853  [   32/  130]
train() client id: f_00008-8-1 loss: 0.725607  [   64/  130]
train() client id: f_00008-8-2 loss: 0.555358  [   96/  130]
train() client id: f_00008-8-3 loss: 0.661165  [  128/  130]
train() client id: f_00008-9-0 loss: 0.583580  [   32/  130]
train() client id: f_00008-9-1 loss: 0.664756  [   64/  130]
train() client id: f_00008-9-2 loss: 0.668943  [   96/  130]
train() client id: f_00008-9-3 loss: 0.630257  [  128/  130]
train() client id: f_00008-10-0 loss: 0.615124  [   32/  130]
train() client id: f_00008-10-1 loss: 0.645605  [   64/  130]
train() client id: f_00008-10-2 loss: 0.691623  [   96/  130]
train() client id: f_00008-10-3 loss: 0.629435  [  128/  130]
train() client id: f_00009-0-0 loss: 1.097634  [   32/  118]
train() client id: f_00009-0-1 loss: 1.012989  [   64/  118]
train() client id: f_00009-0-2 loss: 0.896555  [   96/  118]
train() client id: f_00009-1-0 loss: 0.853181  [   32/  118]
train() client id: f_00009-1-1 loss: 0.897689  [   64/  118]
train() client id: f_00009-1-2 loss: 0.999974  [   96/  118]
train() client id: f_00009-2-0 loss: 0.815267  [   32/  118]
train() client id: f_00009-2-1 loss: 0.966698  [   64/  118]
train() client id: f_00009-2-2 loss: 0.999603  [   96/  118]
train() client id: f_00009-3-0 loss: 0.937823  [   32/  118]
train() client id: f_00009-3-1 loss: 0.810452  [   64/  118]
train() client id: f_00009-3-2 loss: 0.966859  [   96/  118]
train() client id: f_00009-4-0 loss: 1.052726  [   32/  118]
train() client id: f_00009-4-1 loss: 0.875706  [   64/  118]
train() client id: f_00009-4-2 loss: 0.708089  [   96/  118]
train() client id: f_00009-5-0 loss: 0.812312  [   32/  118]
train() client id: f_00009-5-1 loss: 0.908997  [   64/  118]
train() client id: f_00009-5-2 loss: 0.877704  [   96/  118]
train() client id: f_00009-6-0 loss: 0.892415  [   32/  118]
train() client id: f_00009-6-1 loss: 0.848858  [   64/  118]
train() client id: f_00009-6-2 loss: 0.837204  [   96/  118]
train() client id: f_00009-7-0 loss: 0.829233  [   32/  118]
train() client id: f_00009-7-1 loss: 0.953132  [   64/  118]
train() client id: f_00009-7-2 loss: 0.733763  [   96/  118]
train() client id: f_00009-8-0 loss: 0.846720  [   32/  118]
train() client id: f_00009-8-1 loss: 0.810745  [   64/  118]
train() client id: f_00009-8-2 loss: 0.912573  [   96/  118]
train() client id: f_00009-9-0 loss: 0.938426  [   32/  118]
train() client id: f_00009-9-1 loss: 0.773096  [   64/  118]
train() client id: f_00009-9-2 loss: 0.724323  [   96/  118]
train() client id: f_00009-10-0 loss: 0.697789  [   32/  118]
train() client id: f_00009-10-1 loss: 0.733549  [   64/  118]
train() client id: f_00009-10-2 loss: 0.942928  [   96/  118]
At round 59 accuracy: 0.6472148541114059
At round 59 training accuracy: 0.5875251509054326
At round 59 training loss: 0.8314195327197588
update_location
xs = [  -3.9056584     4.20031788  315.00902392   18.81129433    0.97929623
    3.95640986 -277.44319194 -256.32485185  299.66397685 -242.06087855]
ys = [ 307.5879595   290.55583871    1.32061395 -277.45517586  269.35018685
  252.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [323.45881808 307.31146753 330.50329676 295.5253617  287.31599707
 271.90190877 294.92645743 275.14197413 316.3971697  261.93411534]
dists_bs = [216.45517025 213.22735703 519.87822868 492.21794357 199.67641438
 195.1420987  205.04936632 192.2279532  500.09040061 183.67812789]
uav_gains = [1.76622191e-12 2.33041781e-12 1.58304528e-12 2.91576877e-12
 3.43842860e-12 4.73853249e-12 2.95041862e-12 4.42719338e-12
 1.98494725e-12 5.83324094e-12]
bs_gains = [3.19347696e-11 3.33068759e-11 2.74643623e-12 3.20076101e-12
 4.00294047e-11 4.26885463e-11 3.71612640e-11 4.45253937e-11
 3.06166898e-12 5.05746914e-11]
Round 60
-------------------------------
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.0875637  6.22007234 3.04984448 1.12636964 7.17067363 3.45091238
 1.38142813 4.26835796 3.14602737 2.7981042 ]
obj_prev = 35.69935382138897
eta_min = 9.518668131305417e-31	eta_max = 0.9381641580347129
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 8.195255617037894	eta = 0.909090909090909
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 19.17140763760524	eta = 0.38861165126506014
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 13.206898476870052	eta = 0.5641167297661407
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.154598615650826	eta = 0.6129558543819038
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.090043572188366	eta = 0.6162287451356823
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.089768790589424	eta = 0.6162427510544748
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.089768785575135	eta = 0.6162427513100645
eta = 0.6162427513100645
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [0.03994795 0.08401752 0.03931386 0.01363303 0.09701646 0.0462889
 0.01712055 0.05675146 0.04121614 0.03741158]
ene_total = [1.20410292 1.8265834  1.21411492 0.59841362 2.07887597 1.06827008
 0.66615118 1.4063269  1.14028819 0.8866416 ]
ti_comp = [1.15185544 1.27946326 1.14047878 1.19334864 1.28255751 1.28358448
 1.19415401 1.21817826 1.19663411 1.28616459]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [3.00308521e-06 2.26429994e-05 2.91972948e-06 1.11204574e-07
 3.46946537e-05 3.76237107e-06 2.19944230e-07 7.69820333e-06
 3.05604171e-06 1.97835807e-06]
ene_total = [0.40991842 0.15659935 0.43253582 0.32736424 0.15068698 0.14803017
 0.32576518 0.27814891 0.32089064 0.14286493]
optimize_network_iter = 0 obj = 2.6928046428038024
eta = 0.6162427513100645
freqs = [17340697.3279414  32833111.65400368 17235681.79847541  5712090.78528441
 37821484.27558679 18031106.89702021  7168485.47995044 23293576.56497024
 17221697.8440477  14543852.06878811]
eta_min = 0.6162427513100649	eta_max = 0.7177903732762834
af = 0.0012621994794922648	bf = 1.0327722521438556	zeta = 0.0013884194274414914	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [5.91233479e-07 4.45784864e-06 5.74822789e-07 2.18934405e-08
 6.83052241e-06 7.40718154e-07 4.33015993e-08 1.51558655e-06
 6.01659308e-07 3.89489955e-07]
ene_total = [1.73019841 0.65953123 1.82567951 1.3819049  0.63376085 0.62463054
 1.37514743 1.17363935 1.35437921 0.60294663]
ti_comp = [0.79250899 0.92011681 0.78113233 0.8340022  0.92321106 0.92423803
 0.83480756 0.85883181 0.83728766 0.92681814]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [2.03829682e-06 1.40674660e-05 1.99976986e-06 7.31535092e-08
 2.15142627e-05 2.33160556e-06 1.44601475e-07 4.97630551e-06
 2.00560251e-06 1.22411200e-06]
ene_total = [0.55739348 0.21271672 0.58815051 0.44515884 0.2045524  0.20125725
 0.44298339 0.37816177 0.43632847 0.19425169]
optimize_network_iter = 1 obj = 3.6609545155387906
eta = 0.7177903732762834
freqs = [17262263.50383285 31270462.24446457 17235681.79847541  5597996.92086191
 35987511.86898271 17151433.11384352  7023260.57451295 22629570.05842017
 16857767.37652736 13823527.31845897]
eta_min = 0.7177903732762844	eta_max = 0.7177903732762764
af = 0.001159170611255296	bf = 1.0327722521438556	zeta = 0.0012750876723808255	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [5.85897150e-07 4.04361532e-06 5.74822789e-07 2.10275717e-08
 6.18415585e-06 6.70207126e-07 4.15648943e-08 1.43041150e-06
 5.76499351e-07 3.51864226e-07]
ene_total = [1.73019796 0.65949647 1.82567951 1.38190483 0.6337066  0.62462463
 1.37514729 1.1736322  1.3543771  0.60294347]
ti_comp = [0.79250899 0.92011681 0.78113233 0.8340022  0.92321106 0.92423803
 0.83480756 0.85883181 0.83728766 0.92681814]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [2.03829682e-06 1.40674660e-05 1.99976986e-06 7.31535092e-08
 2.15142627e-05 2.33160556e-06 1.44601475e-07 4.97630551e-06
 2.00560251e-06 1.22411200e-06]
ene_total = [0.55739348 0.21271672 0.58815051 0.44515884 0.2045524  0.20125725
 0.44298339 0.37816177 0.43632847 0.19425169]
optimize_network_iter = 2 obj = 3.6609545155387
eta = 0.7177903732762764
freqs = [17262263.50383282 31270462.24446465 17235681.79847537  5597996.92086191
 35987511.8689828  17151433.11384356  7023260.57451294 22629570.05842018
 16857767.37652736 13823527.31845901]
Done!
At round 60 eta: 0.7177903732762764
At round 60 local rounds: 10.857538900724403
At round 60 global rounds: 27.03611541063686
At round 60 a_n: 7.287306191126021
gradient difference: 0.44420164823532104
train() client id: f_00000-0-0 loss: 1.141624  [   32/  126]
train() client id: f_00000-0-1 loss: 1.156100  [   64/  126]
train() client id: f_00000-0-2 loss: 1.161053  [   96/  126]
train() client id: f_00000-1-0 loss: 0.926625  [   32/  126]
train() client id: f_00000-1-1 loss: 1.062793  [   64/  126]
train() client id: f_00000-1-2 loss: 1.016176  [   96/  126]
train() client id: f_00000-2-0 loss: 0.965838  [   32/  126]
train() client id: f_00000-2-1 loss: 0.969417  [   64/  126]
train() client id: f_00000-2-2 loss: 0.948208  [   96/  126]
train() client id: f_00000-3-0 loss: 1.039205  [   32/  126]
train() client id: f_00000-3-1 loss: 0.918280  [   64/  126]
train() client id: f_00000-3-2 loss: 0.955135  [   96/  126]
train() client id: f_00000-4-0 loss: 0.902761  [   32/  126]
train() client id: f_00000-4-1 loss: 0.944640  [   64/  126]
train() client id: f_00000-4-2 loss: 0.824787  [   96/  126]
train() client id: f_00000-5-0 loss: 0.881371  [   32/  126]
train() client id: f_00000-5-1 loss: 0.934676  [   64/  126]
train() client id: f_00000-5-2 loss: 0.820830  [   96/  126]
train() client id: f_00000-6-0 loss: 0.926670  [   32/  126]
train() client id: f_00000-6-1 loss: 0.823526  [   64/  126]
train() client id: f_00000-6-2 loss: 0.876474  [   96/  126]
train() client id: f_00000-7-0 loss: 0.863303  [   32/  126]
train() client id: f_00000-7-1 loss: 0.855762  [   64/  126]
train() client id: f_00000-7-2 loss: 0.806690  [   96/  126]
train() client id: f_00000-8-0 loss: 0.863504  [   32/  126]
train() client id: f_00000-8-1 loss: 0.803159  [   64/  126]
train() client id: f_00000-8-2 loss: 0.903358  [   96/  126]
train() client id: f_00000-9-0 loss: 0.811910  [   32/  126]
train() client id: f_00000-9-1 loss: 0.871479  [   64/  126]
train() client id: f_00000-9-2 loss: 0.855475  [   96/  126]
train() client id: f_00001-0-0 loss: 0.473308  [   32/  265]
train() client id: f_00001-0-1 loss: 0.409754  [   64/  265]
train() client id: f_00001-0-2 loss: 0.619462  [   96/  265]
train() client id: f_00001-0-3 loss: 0.551772  [  128/  265]
train() client id: f_00001-0-4 loss: 0.630496  [  160/  265]
train() client id: f_00001-0-5 loss: 0.391196  [  192/  265]
train() client id: f_00001-0-6 loss: 0.492254  [  224/  265]
train() client id: f_00001-0-7 loss: 0.443889  [  256/  265]
train() client id: f_00001-1-0 loss: 0.546763  [   32/  265]
train() client id: f_00001-1-1 loss: 0.495842  [   64/  265]
train() client id: f_00001-1-2 loss: 0.432197  [   96/  265]
train() client id: f_00001-1-3 loss: 0.405060  [  128/  265]
train() client id: f_00001-1-4 loss: 0.561875  [  160/  265]
train() client id: f_00001-1-5 loss: 0.544732  [  192/  265]
train() client id: f_00001-1-6 loss: 0.607012  [  224/  265]
train() client id: f_00001-1-7 loss: 0.441340  [  256/  265]
train() client id: f_00001-2-0 loss: 0.421610  [   32/  265]
train() client id: f_00001-2-1 loss: 0.549119  [   64/  265]
train() client id: f_00001-2-2 loss: 0.376110  [   96/  265]
train() client id: f_00001-2-3 loss: 0.560031  [  128/  265]
train() client id: f_00001-2-4 loss: 0.466217  [  160/  265]
train() client id: f_00001-2-5 loss: 0.581287  [  192/  265]
train() client id: f_00001-2-6 loss: 0.595946  [  224/  265]
train() client id: f_00001-2-7 loss: 0.425353  [  256/  265]
train() client id: f_00001-3-0 loss: 0.594364  [   32/  265]
train() client id: f_00001-3-1 loss: 0.486666  [   64/  265]
train() client id: f_00001-3-2 loss: 0.436088  [   96/  265]
train() client id: f_00001-3-3 loss: 0.524065  [  128/  265]
train() client id: f_00001-3-4 loss: 0.497017  [  160/  265]
train() client id: f_00001-3-5 loss: 0.423262  [  192/  265]
train() client id: f_00001-3-6 loss: 0.480598  [  224/  265]
train() client id: f_00001-3-7 loss: 0.497193  [  256/  265]
train() client id: f_00001-4-0 loss: 0.562049  [   32/  265]
train() client id: f_00001-4-1 loss: 0.569533  [   64/  265]
train() client id: f_00001-4-2 loss: 0.556614  [   96/  265]
train() client id: f_00001-4-3 loss: 0.451235  [  128/  265]
train() client id: f_00001-4-4 loss: 0.546413  [  160/  265]
train() client id: f_00001-4-5 loss: 0.438680  [  192/  265]
train() client id: f_00001-4-6 loss: 0.416753  [  224/  265]
train() client id: f_00001-4-7 loss: 0.388536  [  256/  265]
train() client id: f_00001-5-0 loss: 0.519090  [   32/  265]
train() client id: f_00001-5-1 loss: 0.505755  [   64/  265]
train() client id: f_00001-5-2 loss: 0.462530  [   96/  265]
train() client id: f_00001-5-3 loss: 0.436705  [  128/  265]
train() client id: f_00001-5-4 loss: 0.402843  [  160/  265]
train() client id: f_00001-5-5 loss: 0.662149  [  192/  265]
train() client id: f_00001-5-6 loss: 0.398793  [  224/  265]
train() client id: f_00001-5-7 loss: 0.515189  [  256/  265]
train() client id: f_00001-6-0 loss: 0.622164  [   32/  265]
train() client id: f_00001-6-1 loss: 0.565794  [   64/  265]
train() client id: f_00001-6-2 loss: 0.493851  [   96/  265]
train() client id: f_00001-6-3 loss: 0.401119  [  128/  265]
train() client id: f_00001-6-4 loss: 0.500467  [  160/  265]
train() client id: f_00001-6-5 loss: 0.397951  [  192/  265]
train() client id: f_00001-6-6 loss: 0.472919  [  224/  265]
train() client id: f_00001-6-7 loss: 0.444266  [  256/  265]
train() client id: f_00001-7-0 loss: 0.380314  [   32/  265]
train() client id: f_00001-7-1 loss: 0.489059  [   64/  265]
train() client id: f_00001-7-2 loss: 0.663385  [   96/  265]
train() client id: f_00001-7-3 loss: 0.475989  [  128/  265]
train() client id: f_00001-7-4 loss: 0.406102  [  160/  265]
train() client id: f_00001-7-5 loss: 0.511027  [  192/  265]
train() client id: f_00001-7-6 loss: 0.541267  [  224/  265]
train() client id: f_00001-7-7 loss: 0.385848  [  256/  265]
train() client id: f_00001-8-0 loss: 0.529450  [   32/  265]
train() client id: f_00001-8-1 loss: 0.476319  [   64/  265]
train() client id: f_00001-8-2 loss: 0.387467  [   96/  265]
train() client id: f_00001-8-3 loss: 0.400000  [  128/  265]
train() client id: f_00001-8-4 loss: 0.428908  [  160/  265]
train() client id: f_00001-8-5 loss: 0.389176  [  192/  265]
train() client id: f_00001-8-6 loss: 0.422884  [  224/  265]
train() client id: f_00001-8-7 loss: 0.732621  [  256/  265]
train() client id: f_00001-9-0 loss: 0.462330  [   32/  265]
train() client id: f_00001-9-1 loss: 0.483952  [   64/  265]
train() client id: f_00001-9-2 loss: 0.473812  [   96/  265]
train() client id: f_00001-9-3 loss: 0.414720  [  128/  265]
train() client id: f_00001-9-4 loss: 0.531578  [  160/  265]
train() client id: f_00001-9-5 loss: 0.385268  [  192/  265]
train() client id: f_00001-9-6 loss: 0.551827  [  224/  265]
train() client id: f_00001-9-7 loss: 0.515707  [  256/  265]
train() client id: f_00002-0-0 loss: 1.096639  [   32/  124]
train() client id: f_00002-0-1 loss: 1.074111  [   64/  124]
train() client id: f_00002-0-2 loss: 1.211890  [   96/  124]
train() client id: f_00002-1-0 loss: 1.279818  [   32/  124]
train() client id: f_00002-1-1 loss: 0.943312  [   64/  124]
train() client id: f_00002-1-2 loss: 1.186234  [   96/  124]
train() client id: f_00002-2-0 loss: 1.094652  [   32/  124]
train() client id: f_00002-2-1 loss: 1.063903  [   64/  124]
train() client id: f_00002-2-2 loss: 1.232718  [   96/  124]
train() client id: f_00002-3-0 loss: 1.098156  [   32/  124]
train() client id: f_00002-3-1 loss: 1.125871  [   64/  124]
train() client id: f_00002-3-2 loss: 0.936154  [   96/  124]
train() client id: f_00002-4-0 loss: 0.898017  [   32/  124]
train() client id: f_00002-4-1 loss: 1.119026  [   64/  124]
train() client id: f_00002-4-2 loss: 1.144620  [   96/  124]
train() client id: f_00002-5-0 loss: 1.132787  [   32/  124]
train() client id: f_00002-5-1 loss: 1.108850  [   64/  124]
train() client id: f_00002-5-2 loss: 0.814605  [   96/  124]
train() client id: f_00002-6-0 loss: 1.133914  [   32/  124]
train() client id: f_00002-6-1 loss: 0.844611  [   64/  124]
train() client id: f_00002-6-2 loss: 1.075604  [   96/  124]
train() client id: f_00002-7-0 loss: 1.028049  [   32/  124]
train() client id: f_00002-7-1 loss: 0.978204  [   64/  124]
train() client id: f_00002-7-2 loss: 0.874019  [   96/  124]
train() client id: f_00002-8-0 loss: 1.068870  [   32/  124]
train() client id: f_00002-8-1 loss: 1.006092  [   64/  124]
train() client id: f_00002-8-2 loss: 0.755347  [   96/  124]
train() client id: f_00002-9-0 loss: 0.857563  [   32/  124]
train() client id: f_00002-9-1 loss: 0.910877  [   64/  124]
train() client id: f_00002-9-2 loss: 1.004035  [   96/  124]
train() client id: f_00003-0-0 loss: 0.511405  [   32/   43]
train() client id: f_00003-1-0 loss: 0.527981  [   32/   43]
train() client id: f_00003-2-0 loss: 0.484414  [   32/   43]
train() client id: f_00003-3-0 loss: 0.524520  [   32/   43]
train() client id: f_00003-4-0 loss: 0.477714  [   32/   43]
train() client id: f_00003-5-0 loss: 0.672468  [   32/   43]
train() client id: f_00003-6-0 loss: 0.602919  [   32/   43]
train() client id: f_00003-7-0 loss: 0.475412  [   32/   43]
train() client id: f_00003-8-0 loss: 0.595329  [   32/   43]
train() client id: f_00003-9-0 loss: 0.541783  [   32/   43]
train() client id: f_00004-0-0 loss: 0.823920  [   32/  306]
train() client id: f_00004-0-1 loss: 0.744123  [   64/  306]
train() client id: f_00004-0-2 loss: 0.752324  [   96/  306]
train() client id: f_00004-0-3 loss: 0.636959  [  128/  306]
train() client id: f_00004-0-4 loss: 0.770728  [  160/  306]
train() client id: f_00004-0-5 loss: 0.725899  [  192/  306]
train() client id: f_00004-0-6 loss: 0.622625  [  224/  306]
train() client id: f_00004-0-7 loss: 0.626141  [  256/  306]
train() client id: f_00004-0-8 loss: 0.435755  [  288/  306]
train() client id: f_00004-1-0 loss: 0.600565  [   32/  306]
train() client id: f_00004-1-1 loss: 0.798315  [   64/  306]
train() client id: f_00004-1-2 loss: 0.738866  [   96/  306]
train() client id: f_00004-1-3 loss: 0.716472  [  128/  306]
train() client id: f_00004-1-4 loss: 0.630131  [  160/  306]
train() client id: f_00004-1-5 loss: 0.702175  [  192/  306]
train() client id: f_00004-1-6 loss: 0.739124  [  224/  306]
train() client id: f_00004-1-7 loss: 0.610452  [  256/  306]
train() client id: f_00004-1-8 loss: 0.627041  [  288/  306]
train() client id: f_00004-2-0 loss: 0.747367  [   32/  306]
train() client id: f_00004-2-1 loss: 0.509827  [   64/  306]
train() client id: f_00004-2-2 loss: 0.770208  [   96/  306]
train() client id: f_00004-2-3 loss: 0.538412  [  128/  306]
train() client id: f_00004-2-4 loss: 0.724751  [  160/  306]
train() client id: f_00004-2-5 loss: 0.622908  [  192/  306]
train() client id: f_00004-2-6 loss: 0.748167  [  224/  306]
train() client id: f_00004-2-7 loss: 0.634418  [  256/  306]
train() client id: f_00004-2-8 loss: 0.698941  [  288/  306]
train() client id: f_00004-3-0 loss: 0.693193  [   32/  306]
train() client id: f_00004-3-1 loss: 0.590261  [   64/  306]
train() client id: f_00004-3-2 loss: 0.730225  [   96/  306]
train() client id: f_00004-3-3 loss: 0.697396  [  128/  306]
train() client id: f_00004-3-4 loss: 0.752762  [  160/  306]
train() client id: f_00004-3-5 loss: 0.571720  [  192/  306]
train() client id: f_00004-3-6 loss: 0.722486  [  224/  306]
train() client id: f_00004-3-7 loss: 0.727657  [  256/  306]
train() client id: f_00004-3-8 loss: 0.609221  [  288/  306]
train() client id: f_00004-4-0 loss: 0.647466  [   32/  306]
train() client id: f_00004-4-1 loss: 0.846001  [   64/  306]
train() client id: f_00004-4-2 loss: 0.645629  [   96/  306]
train() client id: f_00004-4-3 loss: 0.544956  [  128/  306]
train() client id: f_00004-4-4 loss: 0.617636  [  160/  306]
train() client id: f_00004-4-5 loss: 0.651137  [  192/  306]
train() client id: f_00004-4-6 loss: 0.666902  [  224/  306]
train() client id: f_00004-4-7 loss: 0.769768  [  256/  306]
train() client id: f_00004-4-8 loss: 0.699043  [  288/  306]
train() client id: f_00004-5-0 loss: 0.640480  [   32/  306]
train() client id: f_00004-5-1 loss: 0.644030  [   64/  306]
train() client id: f_00004-5-2 loss: 0.716892  [   96/  306]
train() client id: f_00004-5-3 loss: 0.712342  [  128/  306]
train() client id: f_00004-5-4 loss: 0.659190  [  160/  306]
train() client id: f_00004-5-5 loss: 0.723041  [  192/  306]
train() client id: f_00004-5-6 loss: 0.708629  [  224/  306]
train() client id: f_00004-5-7 loss: 0.595841  [  256/  306]
train() client id: f_00004-5-8 loss: 0.653706  [  288/  306]
train() client id: f_00004-6-0 loss: 0.819562  [   32/  306]
train() client id: f_00004-6-1 loss: 0.761374  [   64/  306]
train() client id: f_00004-6-2 loss: 0.538514  [   96/  306]
train() client id: f_00004-6-3 loss: 0.685489  [  128/  306]
train() client id: f_00004-6-4 loss: 0.714391  [  160/  306]
train() client id: f_00004-6-5 loss: 0.796401  [  192/  306]
train() client id: f_00004-6-6 loss: 0.570447  [  224/  306]
train() client id: f_00004-6-7 loss: 0.561541  [  256/  306]
train() client id: f_00004-6-8 loss: 0.692062  [  288/  306]
train() client id: f_00004-7-0 loss: 0.609534  [   32/  306]
train() client id: f_00004-7-1 loss: 0.530616  [   64/  306]
train() client id: f_00004-7-2 loss: 0.645533  [   96/  306]
train() client id: f_00004-7-3 loss: 0.750957  [  128/  306]
train() client id: f_00004-7-4 loss: 0.570589  [  160/  306]
train() client id: f_00004-7-5 loss: 0.656954  [  192/  306]
train() client id: f_00004-7-6 loss: 0.816847  [  224/  306]
train() client id: f_00004-7-7 loss: 0.711714  [  256/  306]
train() client id: f_00004-7-8 loss: 0.786472  [  288/  306]
train() client id: f_00004-8-0 loss: 0.711897  [   32/  306]
train() client id: f_00004-8-1 loss: 0.689579  [   64/  306]
train() client id: f_00004-8-2 loss: 0.521390  [   96/  306]
train() client id: f_00004-8-3 loss: 0.778242  [  128/  306]
train() client id: f_00004-8-4 loss: 0.680928  [  160/  306]
train() client id: f_00004-8-5 loss: 0.734084  [  192/  306]
train() client id: f_00004-8-6 loss: 0.626211  [  224/  306]
train() client id: f_00004-8-7 loss: 0.697340  [  256/  306]
train() client id: f_00004-8-8 loss: 0.675741  [  288/  306]
train() client id: f_00004-9-0 loss: 0.632459  [   32/  306]
train() client id: f_00004-9-1 loss: 0.732533  [   64/  306]
train() client id: f_00004-9-2 loss: 0.561863  [   96/  306]
train() client id: f_00004-9-3 loss: 0.737760  [  128/  306]
train() client id: f_00004-9-4 loss: 0.659110  [  160/  306]
train() client id: f_00004-9-5 loss: 0.604684  [  192/  306]
train() client id: f_00004-9-6 loss: 0.769524  [  224/  306]
train() client id: f_00004-9-7 loss: 0.722299  [  256/  306]
train() client id: f_00004-9-8 loss: 0.687667  [  288/  306]
train() client id: f_00005-0-0 loss: 0.406775  [   32/  146]
train() client id: f_00005-0-1 loss: 0.460177  [   64/  146]
train() client id: f_00005-0-2 loss: 0.443969  [   96/  146]
train() client id: f_00005-0-3 loss: 0.713882  [  128/  146]
train() client id: f_00005-1-0 loss: 0.506267  [   32/  146]
train() client id: f_00005-1-1 loss: 0.376054  [   64/  146]
train() client id: f_00005-1-2 loss: 0.900491  [   96/  146]
train() client id: f_00005-1-3 loss: 0.220582  [  128/  146]
train() client id: f_00005-2-0 loss: 0.329272  [   32/  146]
train() client id: f_00005-2-1 loss: 0.489142  [   64/  146]
train() client id: f_00005-2-2 loss: 0.392827  [   96/  146]
train() client id: f_00005-2-3 loss: 0.695655  [  128/  146]
train() client id: f_00005-3-0 loss: 0.496970  [   32/  146]
train() client id: f_00005-3-1 loss: 0.453183  [   64/  146]
train() client id: f_00005-3-2 loss: 0.759075  [   96/  146]
train() client id: f_00005-3-3 loss: 0.294687  [  128/  146]
train() client id: f_00005-4-0 loss: 0.605388  [   32/  146]
train() client id: f_00005-4-1 loss: 0.293447  [   64/  146]
train() client id: f_00005-4-2 loss: 0.685948  [   96/  146]
train() client id: f_00005-4-3 loss: 0.358245  [  128/  146]
train() client id: f_00005-5-0 loss: 0.384755  [   32/  146]
train() client id: f_00005-5-1 loss: 0.478243  [   64/  146]
train() client id: f_00005-5-2 loss: 0.605473  [   96/  146]
train() client id: f_00005-5-3 loss: 0.441125  [  128/  146]
train() client id: f_00005-6-0 loss: 0.451744  [   32/  146]
train() client id: f_00005-6-1 loss: 0.441857  [   64/  146]
train() client id: f_00005-6-2 loss: 0.526345  [   96/  146]
train() client id: f_00005-6-3 loss: 0.447200  [  128/  146]
train() client id: f_00005-7-0 loss: 0.481564  [   32/  146]
train() client id: f_00005-7-1 loss: 0.387258  [   64/  146]
train() client id: f_00005-7-2 loss: 0.505477  [   96/  146]
train() client id: f_00005-7-3 loss: 0.567737  [  128/  146]
train() client id: f_00005-8-0 loss: 0.691093  [   32/  146]
train() client id: f_00005-8-1 loss: 0.437009  [   64/  146]
train() client id: f_00005-8-2 loss: 0.281223  [   96/  146]
train() client id: f_00005-8-3 loss: 0.590189  [  128/  146]
train() client id: f_00005-9-0 loss: 0.520164  [   32/  146]
train() client id: f_00005-9-1 loss: 0.438658  [   64/  146]
train() client id: f_00005-9-2 loss: 0.626208  [   96/  146]
train() client id: f_00005-9-3 loss: 0.378635  [  128/  146]
train() client id: f_00006-0-0 loss: 0.515595  [   32/   54]
train() client id: f_00006-1-0 loss: 0.504521  [   32/   54]
train() client id: f_00006-2-0 loss: 0.426920  [   32/   54]
train() client id: f_00006-3-0 loss: 0.426322  [   32/   54]
train() client id: f_00006-4-0 loss: 0.464308  [   32/   54]
train() client id: f_00006-5-0 loss: 0.455823  [   32/   54]
train() client id: f_00006-6-0 loss: 0.439145  [   32/   54]
train() client id: f_00006-7-0 loss: 0.416497  [   32/   54]
train() client id: f_00006-8-0 loss: 0.432757  [   32/   54]
train() client id: f_00006-9-0 loss: 0.436566  [   32/   54]
train() client id: f_00007-0-0 loss: 0.843450  [   32/  179]
train() client id: f_00007-0-1 loss: 0.588669  [   64/  179]
train() client id: f_00007-0-2 loss: 0.672132  [   96/  179]
train() client id: f_00007-0-3 loss: 0.512939  [  128/  179]
train() client id: f_00007-0-4 loss: 0.680604  [  160/  179]
train() client id: f_00007-1-0 loss: 0.514766  [   32/  179]
train() client id: f_00007-1-1 loss: 0.615849  [   64/  179]
train() client id: f_00007-1-2 loss: 0.746802  [   96/  179]
train() client id: f_00007-1-3 loss: 0.618078  [  128/  179]
train() client id: f_00007-1-4 loss: 0.685746  [  160/  179]
train() client id: f_00007-2-0 loss: 0.507889  [   32/  179]
train() client id: f_00007-2-1 loss: 0.645367  [   64/  179]
train() client id: f_00007-2-2 loss: 0.457367  [   96/  179]
train() client id: f_00007-2-3 loss: 0.560731  [  128/  179]
train() client id: f_00007-2-4 loss: 0.836353  [  160/  179]
train() client id: f_00007-3-0 loss: 0.575949  [   32/  179]
train() client id: f_00007-3-1 loss: 0.535682  [   64/  179]
train() client id: f_00007-3-2 loss: 0.738963  [   96/  179]
train() client id: f_00007-3-3 loss: 0.675843  [  128/  179]
train() client id: f_00007-3-4 loss: 0.730929  [  160/  179]
train() client id: f_00007-4-0 loss: 0.474500  [   32/  179]
train() client id: f_00007-4-1 loss: 0.697277  [   64/  179]
train() client id: f_00007-4-2 loss: 0.866191  [   96/  179]
train() client id: f_00007-4-3 loss: 0.607665  [  128/  179]
train() client id: f_00007-4-4 loss: 0.593224  [  160/  179]
train() client id: f_00007-5-0 loss: 0.654416  [   32/  179]
train() client id: f_00007-5-1 loss: 0.786523  [   64/  179]
train() client id: f_00007-5-2 loss: 0.650795  [   96/  179]
train() client id: f_00007-5-3 loss: 0.463673  [  128/  179]
train() client id: f_00007-5-4 loss: 0.635178  [  160/  179]
train() client id: f_00007-6-0 loss: 0.589450  [   32/  179]
train() client id: f_00007-6-1 loss: 0.529022  [   64/  179]
train() client id: f_00007-6-2 loss: 0.647804  [   96/  179]
train() client id: f_00007-6-3 loss: 0.614688  [  128/  179]
train() client id: f_00007-6-4 loss: 0.515069  [  160/  179]
train() client id: f_00007-7-0 loss: 0.773776  [   32/  179]
train() client id: f_00007-7-1 loss: 0.524270  [   64/  179]
train() client id: f_00007-7-2 loss: 0.489552  [   96/  179]
train() client id: f_00007-7-3 loss: 0.493573  [  128/  179]
train() client id: f_00007-7-4 loss: 0.857018  [  160/  179]
train() client id: f_00007-8-0 loss: 0.574174  [   32/  179]
train() client id: f_00007-8-1 loss: 0.449066  [   64/  179]
train() client id: f_00007-8-2 loss: 0.623052  [   96/  179]
train() client id: f_00007-8-3 loss: 0.663351  [  128/  179]
train() client id: f_00007-8-4 loss: 0.748910  [  160/  179]
train() client id: f_00007-9-0 loss: 0.740160  [   32/  179]
train() client id: f_00007-9-1 loss: 0.650739  [   64/  179]
train() client id: f_00007-9-2 loss: 0.580099  [   96/  179]
train() client id: f_00007-9-3 loss: 0.634566  [  128/  179]
train() client id: f_00007-9-4 loss: 0.460850  [  160/  179]
train() client id: f_00008-0-0 loss: 0.652826  [   32/  130]
train() client id: f_00008-0-1 loss: 0.795135  [   64/  130]
train() client id: f_00008-0-2 loss: 0.787526  [   96/  130]
train() client id: f_00008-0-3 loss: 0.791373  [  128/  130]
train() client id: f_00008-1-0 loss: 0.699328  [   32/  130]
train() client id: f_00008-1-1 loss: 0.845738  [   64/  130]
train() client id: f_00008-1-2 loss: 0.715805  [   96/  130]
train() client id: f_00008-1-3 loss: 0.763790  [  128/  130]
train() client id: f_00008-2-0 loss: 0.851311  [   32/  130]
train() client id: f_00008-2-1 loss: 0.655316  [   64/  130]
train() client id: f_00008-2-2 loss: 0.726501  [   96/  130]
train() client id: f_00008-2-3 loss: 0.789922  [  128/  130]
train() client id: f_00008-3-0 loss: 0.759508  [   32/  130]
train() client id: f_00008-3-1 loss: 0.848151  [   64/  130]
train() client id: f_00008-3-2 loss: 0.670600  [   96/  130]
train() client id: f_00008-3-3 loss: 0.745217  [  128/  130]
train() client id: f_00008-4-0 loss: 0.735215  [   32/  130]
train() client id: f_00008-4-1 loss: 0.663783  [   64/  130]
train() client id: f_00008-4-2 loss: 0.771304  [   96/  130]
train() client id: f_00008-4-3 loss: 0.842218  [  128/  130]
train() client id: f_00008-5-0 loss: 0.757887  [   32/  130]
train() client id: f_00008-5-1 loss: 0.750339  [   64/  130]
train() client id: f_00008-5-2 loss: 0.722180  [   96/  130]
train() client id: f_00008-5-3 loss: 0.781796  [  128/  130]
train() client id: f_00008-6-0 loss: 0.746145  [   32/  130]
train() client id: f_00008-6-1 loss: 0.686471  [   64/  130]
train() client id: f_00008-6-2 loss: 0.799100  [   96/  130]
train() client id: f_00008-6-3 loss: 0.785458  [  128/  130]
train() client id: f_00008-7-0 loss: 0.729070  [   32/  130]
train() client id: f_00008-7-1 loss: 0.728022  [   64/  130]
train() client id: f_00008-7-2 loss: 0.731673  [   96/  130]
train() client id: f_00008-7-3 loss: 0.827824  [  128/  130]
train() client id: f_00008-8-0 loss: 0.752510  [   32/  130]
train() client id: f_00008-8-1 loss: 0.771788  [   64/  130]
train() client id: f_00008-8-2 loss: 0.742329  [   96/  130]
train() client id: f_00008-8-3 loss: 0.749609  [  128/  130]
train() client id: f_00008-9-0 loss: 0.804345  [   32/  130]
train() client id: f_00008-9-1 loss: 0.639438  [   64/  130]
train() client id: f_00008-9-2 loss: 0.745424  [   96/  130]
train() client id: f_00008-9-3 loss: 0.805843  [  128/  130]
train() client id: f_00009-0-0 loss: 0.803894  [   32/  118]
train() client id: f_00009-0-1 loss: 1.110511  [   64/  118]
train() client id: f_00009-0-2 loss: 0.954397  [   96/  118]
train() client id: f_00009-1-0 loss: 1.027494  [   32/  118]
train() client id: f_00009-1-1 loss: 0.902753  [   64/  118]
train() client id: f_00009-1-2 loss: 0.779119  [   96/  118]
train() client id: f_00009-2-0 loss: 0.742426  [   32/  118]
train() client id: f_00009-2-1 loss: 0.844266  [   64/  118]
train() client id: f_00009-2-2 loss: 1.043333  [   96/  118]
train() client id: f_00009-3-0 loss: 0.842124  [   32/  118]
train() client id: f_00009-3-1 loss: 0.931034  [   64/  118]
train() client id: f_00009-3-2 loss: 0.762313  [   96/  118]
train() client id: f_00009-4-0 loss: 0.844606  [   32/  118]
train() client id: f_00009-4-1 loss: 0.815483  [   64/  118]
train() client id: f_00009-4-2 loss: 0.778618  [   96/  118]
train() client id: f_00009-5-0 loss: 0.682195  [   32/  118]
train() client id: f_00009-5-1 loss: 0.736501  [   64/  118]
train() client id: f_00009-5-2 loss: 0.794500  [   96/  118]
train() client id: f_00009-6-0 loss: 0.619052  [   32/  118]
train() client id: f_00009-6-1 loss: 0.954936  [   64/  118]
train() client id: f_00009-6-2 loss: 0.773817  [   96/  118]
train() client id: f_00009-7-0 loss: 0.654394  [   32/  118]
train() client id: f_00009-7-1 loss: 0.823716  [   64/  118]
train() client id: f_00009-7-2 loss: 0.801515  [   96/  118]
train() client id: f_00009-8-0 loss: 0.688876  [   32/  118]
train() client id: f_00009-8-1 loss: 0.778652  [   64/  118]
train() client id: f_00009-8-2 loss: 0.643781  [   96/  118]
train() client id: f_00009-9-0 loss: 0.641703  [   32/  118]
train() client id: f_00009-9-1 loss: 0.793393  [   64/  118]
train() client id: f_00009-9-2 loss: 0.838580  [   96/  118]
At round 60 accuracy: 0.6472148541114059
At round 60 training accuracy: 0.5861837692823608
At round 60 training loss: 0.8318167030897925
update_location
xs = [  -3.9056584     4.20031788  320.00902392   18.81129433    0.97929623
    3.95640986 -282.44319194 -261.32485185  304.66397685 -247.06087855]
ys = [ 312.5879595   295.55583871    1.32061395 -282.45517586  274.35018685
  257.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [328.2171333  312.04310033 335.27230636 300.22456789 292.0085342
 276.55702758 299.63485647 279.80592283 321.13674465 266.56160558]
dists_bs = [219.55345664 215.9779252  524.5936169  496.81793016 202.08704146
 197.18919898 207.59256862 194.39607637 504.83863615 185.5620205 ]
uav_gains = [1.63906976e-12 2.14056491e-12 1.47563907e-12 2.66124979e-12
 3.12682385e-12 4.29784027e-12 2.69155883e-12 4.01567151e-12
 1.83392238e-12 5.29895911e-12]
bs_gains = [3.06888987e-11 3.21327477e-11 2.67787117e-12 3.11847132e-12
 3.87067209e-11 4.14592405e-11 3.59005447e-11 4.31488375e-11
 2.98171993e-12 4.91501249e-11]
Round 61
-------------------------------
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.95458156 5.94128521 2.91869021 1.0805571  6.84914591 3.29632295
 1.32415061 4.08054691 3.00608516 2.67279298]
obj_prev = 34.12415860899734
eta_min = 3.98916673942854e-32	eta_max = 0.9387143479457988
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 7.8273257066737205	eta = 0.9090909090909091
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 18.61065565296208	eta = 0.3823481974584871
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 12.717399503431304	eta = 0.5595287496088128
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.681826444107466	eta = 0.60912997436458
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617979436161402	eta = 0.6124774692131586
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617704178505617	eta = 0.6124919806097139
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617704173351733	eta = 0.6124919808814296
eta = 0.6124919808814296
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [0.04045173 0.08507706 0.03980964 0.01380496 0.09823992 0.04687264
 0.01733646 0.05746714 0.04173591 0.03788337]
ene_total = [1.16278399 1.74880761 1.17244705 0.58140006 1.9903631  1.02228243
 0.6462616  1.35340359 1.09164915 0.84830559]
ti_comp = [1.21927116 1.35390191 1.20771185 1.26195871 1.35708161 1.35819303
 1.2627831  1.28806187 1.26993947 1.36081379]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [2.78285633e-06 2.09963099e-05 2.70344885e-06 1.03251135e-07
 3.21760040e-05 3.48911535e-06 2.04222466e-07 7.14933098e-06
 2.81736924e-06 1.83496748e-06]
ene_total = [0.40211875 0.14928058 0.4238552  0.32179204 0.14351122 0.14088165
 0.32024363 0.27283609 0.3068348  0.13592205]
optimize_network_iter = 0 obj = 2.617276020137961
eta = 0.6124919808814296
freqs = [16588488.88317586 31419209.64704492 16481431.96089219  5469654.50966602
 36195288.80600585 17255515.63488949  6864384.21294392 22307602.23984509
 16432245.4685148  13919379.90342778]
eta_min = 0.6124919808814304	eta_max = 0.7228965206792303
af = 0.0011038579011319113	bf = 1.0136743134601105	zeta = 0.0012142436912451024	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [5.41052682e-07 4.08217617e-06 5.25614002e-07 2.00744476e-08
 6.25577148e-06 6.78366036e-07 3.97056477e-08 1.38999799e-06
 5.47762803e-07 3.56760810e-07]
ene_total = [1.71388696 0.63496578 1.80654562 1.37166028 0.60965148 0.60029516
 1.36505352 1.16252622 1.30772847 0.57926126]
ti_comp = [0.8109757  0.94560644 0.79941638 0.85366324 0.94878614 0.94989756
 0.85448763 0.8797664  0.861644   0.95251833]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.82433614e-06 1.24831928e-05 1.78948422e-06 6.54396715e-08
 1.90913448e-05 2.06877816e-06 1.29353631e-07 4.44460588e-06
 1.77493813e-06 1.08619612e-06]
ene_total = [0.56230711 0.20853359 0.59270505 0.45000056 0.20034537 0.19697486
 0.44783425 0.38146927 0.42905758 0.19005691]
optimize_network_iter = 1 obj = 3.659284534384905
eta = 0.7228965206792303
freqs = [16508552.89543737 29777055.50547944 16481431.96089219  5352148.75441332
 34268839.33358462 16331361.10328626  6714818.53144301 21618818.75231183
 16031043.09450184 13163002.59895364]
eta_min = 0.7228965206792295	eta_max = 0.7228965206792275
af = 0.0010051578008740583	bf = 1.0136743134601105	zeta = 0.0011056735809614642	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [5.35850837e-07 3.66661011e-06 5.25614002e-07 1.92211852e-08
 5.60758124e-06 6.07649264e-07 3.79942326e-08 1.30548627e-06
 5.21341470e-07 3.19041588e-07]
ene_total = [1.71388654 0.63493247 1.80654562 1.37166021 0.60959952 0.60028949
 1.36505338 1.16251944 1.30772635 0.57925823]
ti_comp = [0.8109757  0.94560644 0.79941638 0.85366324 0.94878614 0.94989756
 0.85448763 0.8797664  0.861644   0.95251833]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.82433614e-06 1.24831928e-05 1.78948422e-06 6.54396715e-08
 1.90913448e-05 2.06877816e-06 1.29353631e-07 4.44460588e-06
 1.77493813e-06 1.08619612e-06]
ene_total = [0.56230711 0.20853359 0.59270505 0.45000056 0.20034537 0.19697486
 0.44783425 0.38146927 0.42905758 0.19005691]
optimize_network_iter = 2 obj = 3.6592845343848683
eta = 0.7228965206792275
freqs = [16508552.89543735 29777055.50547947 16481431.96089218  5352148.75441332
 34268839.33358465 16331361.10328627  6714818.531443   21618818.75231183
 16031043.09450183 13163002.59895365]
Done!
At round 61 eta: 0.7228965206792275
At round 61 local rounds: 10.625424748609518
At round 61 global rounds: 26.298140351714242
At round 61 a_n: 6.944760344156705
gradient difference: 0.4622512459754944
train() client id: f_00000-0-0 loss: 1.132481  [   32/  126]
train() client id: f_00000-0-1 loss: 1.404215  [   64/  126]
train() client id: f_00000-0-2 loss: 1.084700  [   96/  126]
train() client id: f_00000-1-0 loss: 1.001690  [   32/  126]
train() client id: f_00000-1-1 loss: 1.187442  [   64/  126]
train() client id: f_00000-1-2 loss: 1.324970  [   96/  126]
train() client id: f_00000-2-0 loss: 1.131163  [   32/  126]
train() client id: f_00000-2-1 loss: 0.885104  [   64/  126]
train() client id: f_00000-2-2 loss: 1.028275  [   96/  126]
train() client id: f_00000-3-0 loss: 0.923372  [   32/  126]
train() client id: f_00000-3-1 loss: 0.998269  [   64/  126]
train() client id: f_00000-3-2 loss: 0.972056  [   96/  126]
train() client id: f_00000-4-0 loss: 0.937753  [   32/  126]
train() client id: f_00000-4-1 loss: 0.781514  [   64/  126]
train() client id: f_00000-4-2 loss: 1.091895  [   96/  126]
train() client id: f_00000-5-0 loss: 1.077379  [   32/  126]
train() client id: f_00000-5-1 loss: 0.774297  [   64/  126]
train() client id: f_00000-5-2 loss: 0.839742  [   96/  126]
train() client id: f_00000-6-0 loss: 0.999073  [   32/  126]
train() client id: f_00000-6-1 loss: 0.712761  [   64/  126]
train() client id: f_00000-6-2 loss: 0.828616  [   96/  126]
train() client id: f_00000-7-0 loss: 0.877018  [   32/  126]
train() client id: f_00000-7-1 loss: 0.828842  [   64/  126]
train() client id: f_00000-7-2 loss: 0.771202  [   96/  126]
train() client id: f_00000-8-0 loss: 0.795172  [   32/  126]
train() client id: f_00000-8-1 loss: 0.762822  [   64/  126]
train() client id: f_00000-8-2 loss: 0.805788  [   96/  126]
train() client id: f_00000-9-0 loss: 0.876852  [   32/  126]
train() client id: f_00000-9-1 loss: 0.630926  [   64/  126]
train() client id: f_00000-9-2 loss: 0.699130  [   96/  126]
train() client id: f_00001-0-0 loss: 0.328884  [   32/  265]
train() client id: f_00001-0-1 loss: 0.399883  [   64/  265]
train() client id: f_00001-0-2 loss: 0.314035  [   96/  265]
train() client id: f_00001-0-3 loss: 0.431408  [  128/  265]
train() client id: f_00001-0-4 loss: 0.401903  [  160/  265]
train() client id: f_00001-0-5 loss: 0.549570  [  192/  265]
train() client id: f_00001-0-6 loss: 0.483253  [  224/  265]
train() client id: f_00001-0-7 loss: 0.368020  [  256/  265]
train() client id: f_00001-1-0 loss: 0.410952  [   32/  265]
train() client id: f_00001-1-1 loss: 0.310734  [   64/  265]
train() client id: f_00001-1-2 loss: 0.459251  [   96/  265]
train() client id: f_00001-1-3 loss: 0.416572  [  128/  265]
train() client id: f_00001-1-4 loss: 0.378749  [  160/  265]
train() client id: f_00001-1-5 loss: 0.329846  [  192/  265]
train() client id: f_00001-1-6 loss: 0.453907  [  224/  265]
train() client id: f_00001-1-7 loss: 0.411251  [  256/  265]
train() client id: f_00001-2-0 loss: 0.299507  [   32/  265]
train() client id: f_00001-2-1 loss: 0.431219  [   64/  265]
train() client id: f_00001-2-2 loss: 0.474064  [   96/  265]
train() client id: f_00001-2-3 loss: 0.391889  [  128/  265]
train() client id: f_00001-2-4 loss: 0.490726  [  160/  265]
train() client id: f_00001-2-5 loss: 0.302235  [  192/  265]
train() client id: f_00001-2-6 loss: 0.381811  [  224/  265]
train() client id: f_00001-2-7 loss: 0.374585  [  256/  265]
train() client id: f_00001-3-0 loss: 0.408082  [   32/  265]
train() client id: f_00001-3-1 loss: 0.466606  [   64/  265]
train() client id: f_00001-3-2 loss: 0.390582  [   96/  265]
train() client id: f_00001-3-3 loss: 0.330145  [  128/  265]
train() client id: f_00001-3-4 loss: 0.306825  [  160/  265]
train() client id: f_00001-3-5 loss: 0.469535  [  192/  265]
train() client id: f_00001-3-6 loss: 0.385392  [  224/  265]
train() client id: f_00001-3-7 loss: 0.392505  [  256/  265]
train() client id: f_00001-4-0 loss: 0.360787  [   32/  265]
train() client id: f_00001-4-1 loss: 0.404243  [   64/  265]
train() client id: f_00001-4-2 loss: 0.455871  [   96/  265]
train() client id: f_00001-4-3 loss: 0.348341  [  128/  265]
train() client id: f_00001-4-4 loss: 0.270794  [  160/  265]
train() client id: f_00001-4-5 loss: 0.288157  [  192/  265]
train() client id: f_00001-4-6 loss: 0.522570  [  224/  265]
train() client id: f_00001-4-7 loss: 0.470513  [  256/  265]
train() client id: f_00001-5-0 loss: 0.302567  [   32/  265]
train() client id: f_00001-5-1 loss: 0.449938  [   64/  265]
train() client id: f_00001-5-2 loss: 0.461750  [   96/  265]
train() client id: f_00001-5-3 loss: 0.271301  [  128/  265]
train() client id: f_00001-5-4 loss: 0.381959  [  160/  265]
train() client id: f_00001-5-5 loss: 0.359512  [  192/  265]
train() client id: f_00001-5-6 loss: 0.360726  [  224/  265]
train() client id: f_00001-5-7 loss: 0.359445  [  256/  265]
train() client id: f_00001-6-0 loss: 0.551838  [   32/  265]
train() client id: f_00001-6-1 loss: 0.341075  [   64/  265]
train() client id: f_00001-6-2 loss: 0.351229  [   96/  265]
train() client id: f_00001-6-3 loss: 0.416903  [  128/  265]
train() client id: f_00001-6-4 loss: 0.308828  [  160/  265]
train() client id: f_00001-6-5 loss: 0.367277  [  192/  265]
train() client id: f_00001-6-6 loss: 0.312540  [  224/  265]
train() client id: f_00001-6-7 loss: 0.346498  [  256/  265]
train() client id: f_00001-7-0 loss: 0.330689  [   32/  265]
train() client id: f_00001-7-1 loss: 0.468105  [   64/  265]
train() client id: f_00001-7-2 loss: 0.377983  [   96/  265]
train() client id: f_00001-7-3 loss: 0.467939  [  128/  265]
train() client id: f_00001-7-4 loss: 0.329942  [  160/  265]
train() client id: f_00001-7-5 loss: 0.434964  [  192/  265]
train() client id: f_00001-7-6 loss: 0.284719  [  224/  265]
train() client id: f_00001-7-7 loss: 0.354823  [  256/  265]
train() client id: f_00001-8-0 loss: 0.505447  [   32/  265]
train() client id: f_00001-8-1 loss: 0.332428  [   64/  265]
train() client id: f_00001-8-2 loss: 0.374913  [   96/  265]
train() client id: f_00001-8-3 loss: 0.464316  [  128/  265]
train() client id: f_00001-8-4 loss: 0.283012  [  160/  265]
train() client id: f_00001-8-5 loss: 0.425244  [  192/  265]
train() client id: f_00001-8-6 loss: 0.276528  [  224/  265]
train() client id: f_00001-8-7 loss: 0.390549  [  256/  265]
train() client id: f_00001-9-0 loss: 0.374458  [   32/  265]
train() client id: f_00001-9-1 loss: 0.459867  [   64/  265]
train() client id: f_00001-9-2 loss: 0.279360  [   96/  265]
train() client id: f_00001-9-3 loss: 0.348777  [  128/  265]
train() client id: f_00001-9-4 loss: 0.423283  [  160/  265]
train() client id: f_00001-9-5 loss: 0.336985  [  192/  265]
train() client id: f_00001-9-6 loss: 0.394871  [  224/  265]
train() client id: f_00001-9-7 loss: 0.442852  [  256/  265]
train() client id: f_00002-0-0 loss: 1.181454  [   32/  124]
train() client id: f_00002-0-1 loss: 1.199062  [   64/  124]
train() client id: f_00002-0-2 loss: 0.925128  [   96/  124]
train() client id: f_00002-1-0 loss: 1.225772  [   32/  124]
train() client id: f_00002-1-1 loss: 1.094831  [   64/  124]
train() client id: f_00002-1-2 loss: 0.856034  [   96/  124]
train() client id: f_00002-2-0 loss: 0.885330  [   32/  124]
train() client id: f_00002-2-1 loss: 1.121843  [   64/  124]
train() client id: f_00002-2-2 loss: 0.920439  [   96/  124]
train() client id: f_00002-3-0 loss: 1.040106  [   32/  124]
train() client id: f_00002-3-1 loss: 0.872281  [   64/  124]
train() client id: f_00002-3-2 loss: 1.086953  [   96/  124]
train() client id: f_00002-4-0 loss: 0.938138  [   32/  124]
train() client id: f_00002-4-1 loss: 1.026177  [   64/  124]
train() client id: f_00002-4-2 loss: 1.077899  [   96/  124]
train() client id: f_00002-5-0 loss: 0.988582  [   32/  124]
train() client id: f_00002-5-1 loss: 0.939819  [   64/  124]
train() client id: f_00002-5-2 loss: 0.891520  [   96/  124]
train() client id: f_00002-6-0 loss: 0.914860  [   32/  124]
train() client id: f_00002-6-1 loss: 1.093312  [   64/  124]
train() client id: f_00002-6-2 loss: 0.797289  [   96/  124]
train() client id: f_00002-7-0 loss: 0.963686  [   32/  124]
train() client id: f_00002-7-1 loss: 0.862819  [   64/  124]
train() client id: f_00002-7-2 loss: 0.931466  [   96/  124]
train() client id: f_00002-8-0 loss: 0.830366  [   32/  124]
train() client id: f_00002-8-1 loss: 0.755436  [   64/  124]
train() client id: f_00002-8-2 loss: 0.942424  [   96/  124]
train() client id: f_00002-9-0 loss: 0.953322  [   32/  124]
train() client id: f_00002-9-1 loss: 1.015690  [   64/  124]
train() client id: f_00002-9-2 loss: 0.729754  [   96/  124]
train() client id: f_00003-0-0 loss: 0.967979  [   32/   43]
train() client id: f_00003-1-0 loss: 0.554342  [   32/   43]
train() client id: f_00003-2-0 loss: 0.639149  [   32/   43]
train() client id: f_00003-3-0 loss: 0.540121  [   32/   43]
train() client id: f_00003-4-0 loss: 0.864645  [   32/   43]
train() client id: f_00003-5-0 loss: 0.671221  [   32/   43]
train() client id: f_00003-6-0 loss: 0.669568  [   32/   43]
train() client id: f_00003-7-0 loss: 0.678762  [   32/   43]
train() client id: f_00003-8-0 loss: 0.499116  [   32/   43]
train() client id: f_00003-9-0 loss: 0.784743  [   32/   43]
train() client id: f_00004-0-0 loss: 0.765033  [   32/  306]
train() client id: f_00004-0-1 loss: 0.811617  [   64/  306]
train() client id: f_00004-0-2 loss: 0.862251  [   96/  306]
train() client id: f_00004-0-3 loss: 0.772711  [  128/  306]
train() client id: f_00004-0-4 loss: 0.897486  [  160/  306]
train() client id: f_00004-0-5 loss: 0.729673  [  192/  306]
train() client id: f_00004-0-6 loss: 0.979047  [  224/  306]
train() client id: f_00004-0-7 loss: 0.890240  [  256/  306]
train() client id: f_00004-0-8 loss: 0.728516  [  288/  306]
train() client id: f_00004-1-0 loss: 0.905338  [   32/  306]
train() client id: f_00004-1-1 loss: 0.829021  [   64/  306]
train() client id: f_00004-1-2 loss: 0.777835  [   96/  306]
train() client id: f_00004-1-3 loss: 0.816997  [  128/  306]
train() client id: f_00004-1-4 loss: 0.959500  [  160/  306]
train() client id: f_00004-1-5 loss: 0.826939  [  192/  306]
train() client id: f_00004-1-6 loss: 0.901811  [  224/  306]
train() client id: f_00004-1-7 loss: 0.777198  [  256/  306]
train() client id: f_00004-1-8 loss: 0.818065  [  288/  306]
train() client id: f_00004-2-0 loss: 0.857829  [   32/  306]
train() client id: f_00004-2-1 loss: 0.916894  [   64/  306]
train() client id: f_00004-2-2 loss: 0.727338  [   96/  306]
train() client id: f_00004-2-3 loss: 0.681809  [  128/  306]
train() client id: f_00004-2-4 loss: 0.835642  [  160/  306]
train() client id: f_00004-2-5 loss: 0.923547  [  192/  306]
train() client id: f_00004-2-6 loss: 0.733616  [  224/  306]
train() client id: f_00004-2-7 loss: 0.870652  [  256/  306]
train() client id: f_00004-2-8 loss: 0.877153  [  288/  306]
train() client id: f_00004-3-0 loss: 0.976762  [   32/  306]
train() client id: f_00004-3-1 loss: 0.904464  [   64/  306]
train() client id: f_00004-3-2 loss: 0.935324  [   96/  306]
train() client id: f_00004-3-3 loss: 0.804510  [  128/  306]
train() client id: f_00004-3-4 loss: 0.739720  [  160/  306]
train() client id: f_00004-3-5 loss: 0.721936  [  192/  306]
train() client id: f_00004-3-6 loss: 0.817385  [  224/  306]
train() client id: f_00004-3-7 loss: 0.758503  [  256/  306]
train() client id: f_00004-3-8 loss: 0.816043  [  288/  306]
train() client id: f_00004-4-0 loss: 0.777405  [   32/  306]
train() client id: f_00004-4-1 loss: 0.788847  [   64/  306]
train() client id: f_00004-4-2 loss: 0.789918  [   96/  306]
train() client id: f_00004-4-3 loss: 0.789763  [  128/  306]
train() client id: f_00004-4-4 loss: 0.822145  [  160/  306]
train() client id: f_00004-4-5 loss: 0.756769  [  192/  306]
train() client id: f_00004-4-6 loss: 0.842163  [  224/  306]
train() client id: f_00004-4-7 loss: 0.961225  [  256/  306]
train() client id: f_00004-4-8 loss: 0.882545  [  288/  306]
train() client id: f_00004-5-0 loss: 0.821773  [   32/  306]
train() client id: f_00004-5-1 loss: 0.758839  [   64/  306]
train() client id: f_00004-5-2 loss: 0.815511  [   96/  306]
train() client id: f_00004-5-3 loss: 0.798510  [  128/  306]
train() client id: f_00004-5-4 loss: 0.872697  [  160/  306]
train() client id: f_00004-5-5 loss: 0.822842  [  192/  306]
train() client id: f_00004-5-6 loss: 0.817622  [  224/  306]
train() client id: f_00004-5-7 loss: 0.935226  [  256/  306]
train() client id: f_00004-5-8 loss: 0.877204  [  288/  306]
train() client id: f_00004-6-0 loss: 0.793591  [   32/  306]
train() client id: f_00004-6-1 loss: 0.720273  [   64/  306]
train() client id: f_00004-6-2 loss: 0.848861  [   96/  306]
train() client id: f_00004-6-3 loss: 0.709511  [  128/  306]
train() client id: f_00004-6-4 loss: 0.887402  [  160/  306]
train() client id: f_00004-6-5 loss: 0.835820  [  192/  306]
train() client id: f_00004-6-6 loss: 0.912388  [  224/  306]
train() client id: f_00004-6-7 loss: 0.874878  [  256/  306]
train() client id: f_00004-6-8 loss: 0.866277  [  288/  306]
train() client id: f_00004-7-0 loss: 0.804687  [   32/  306]
train() client id: f_00004-7-1 loss: 0.659023  [   64/  306]
train() client id: f_00004-7-2 loss: 0.782770  [   96/  306]
train() client id: f_00004-7-3 loss: 0.777292  [  128/  306]
train() client id: f_00004-7-4 loss: 0.908222  [  160/  306]
train() client id: f_00004-7-5 loss: 1.001694  [  192/  306]
train() client id: f_00004-7-6 loss: 0.960137  [  224/  306]
train() client id: f_00004-7-7 loss: 0.760966  [  256/  306]
train() client id: f_00004-7-8 loss: 0.828445  [  288/  306]
train() client id: f_00004-8-0 loss: 0.753881  [   32/  306]
train() client id: f_00004-8-1 loss: 0.813185  [   64/  306]
train() client id: f_00004-8-2 loss: 0.931910  [   96/  306]
train() client id: f_00004-8-3 loss: 0.873663  [  128/  306]
train() client id: f_00004-8-4 loss: 0.808207  [  160/  306]
train() client id: f_00004-8-5 loss: 0.757927  [  192/  306]
train() client id: f_00004-8-6 loss: 0.877370  [  224/  306]
train() client id: f_00004-8-7 loss: 0.858253  [  256/  306]
train() client id: f_00004-8-8 loss: 0.766050  [  288/  306]
train() client id: f_00004-9-0 loss: 0.990974  [   32/  306]
train() client id: f_00004-9-1 loss: 0.802931  [   64/  306]
train() client id: f_00004-9-2 loss: 0.720276  [   96/  306]
train() client id: f_00004-9-3 loss: 0.843364  [  128/  306]
train() client id: f_00004-9-4 loss: 0.739190  [  160/  306]
train() client id: f_00004-9-5 loss: 0.821877  [  192/  306]
train() client id: f_00004-9-6 loss: 0.900675  [  224/  306]
train() client id: f_00004-9-7 loss: 0.932293  [  256/  306]
train() client id: f_00004-9-8 loss: 0.823761  [  288/  306]
train() client id: f_00005-0-0 loss: 0.651459  [   32/  146]
train() client id: f_00005-0-1 loss: 0.583584  [   64/  146]
train() client id: f_00005-0-2 loss: 0.704022  [   96/  146]
train() client id: f_00005-0-3 loss: 0.860213  [  128/  146]
train() client id: f_00005-1-0 loss: 0.874747  [   32/  146]
train() client id: f_00005-1-1 loss: 0.738523  [   64/  146]
train() client id: f_00005-1-2 loss: 0.606426  [   96/  146]
train() client id: f_00005-1-3 loss: 0.662342  [  128/  146]
train() client id: f_00005-2-0 loss: 0.639944  [   32/  146]
train() client id: f_00005-2-1 loss: 0.823908  [   64/  146]
train() client id: f_00005-2-2 loss: 0.540779  [   96/  146]
train() client id: f_00005-2-3 loss: 0.798681  [  128/  146]
train() client id: f_00005-3-0 loss: 0.732295  [   32/  146]
train() client id: f_00005-3-1 loss: 0.748193  [   64/  146]
train() client id: f_00005-3-2 loss: 0.519167  [   96/  146]
train() client id: f_00005-3-3 loss: 0.727246  [  128/  146]
train() client id: f_00005-4-0 loss: 0.576797  [   32/  146]
train() client id: f_00005-4-1 loss: 0.859921  [   64/  146]
train() client id: f_00005-4-2 loss: 0.813156  [   96/  146]
train() client id: f_00005-4-3 loss: 0.463997  [  128/  146]
train() client id: f_00005-5-0 loss: 0.754451  [   32/  146]
train() client id: f_00005-5-1 loss: 0.666341  [   64/  146]
train() client id: f_00005-5-2 loss: 0.386344  [   96/  146]
train() client id: f_00005-5-3 loss: 1.048631  [  128/  146]
train() client id: f_00005-6-0 loss: 0.523532  [   32/  146]
train() client id: f_00005-6-1 loss: 0.767550  [   64/  146]
train() client id: f_00005-6-2 loss: 0.766224  [   96/  146]
train() client id: f_00005-6-3 loss: 0.734939  [  128/  146]
train() client id: f_00005-7-0 loss: 0.901452  [   32/  146]
train() client id: f_00005-7-1 loss: 0.693224  [   64/  146]
train() client id: f_00005-7-2 loss: 0.466510  [   96/  146]
train() client id: f_00005-7-3 loss: 0.549437  [  128/  146]
train() client id: f_00005-8-0 loss: 0.615256  [   32/  146]
train() client id: f_00005-8-1 loss: 0.651703  [   64/  146]
train() client id: f_00005-8-2 loss: 0.763596  [   96/  146]
train() client id: f_00005-8-3 loss: 0.514888  [  128/  146]
train() client id: f_00005-9-0 loss: 0.636183  [   32/  146]
train() client id: f_00005-9-1 loss: 0.589883  [   64/  146]
train() client id: f_00005-9-2 loss: 0.599322  [   96/  146]
train() client id: f_00005-9-3 loss: 0.960194  [  128/  146]
train() client id: f_00006-0-0 loss: 0.536740  [   32/   54]
train() client id: f_00006-1-0 loss: 0.472080  [   32/   54]
train() client id: f_00006-2-0 loss: 0.495204  [   32/   54]
train() client id: f_00006-3-0 loss: 0.478582  [   32/   54]
train() client id: f_00006-4-0 loss: 0.526898  [   32/   54]
train() client id: f_00006-5-0 loss: 0.499178  [   32/   54]
train() client id: f_00006-6-0 loss: 0.529591  [   32/   54]
train() client id: f_00006-7-0 loss: 0.455957  [   32/   54]
train() client id: f_00006-8-0 loss: 0.518360  [   32/   54]
train() client id: f_00006-9-0 loss: 0.520108  [   32/   54]
train() client id: f_00007-0-0 loss: 0.709979  [   32/  179]
train() client id: f_00007-0-1 loss: 0.784657  [   64/  179]
train() client id: f_00007-0-2 loss: 0.632923  [   96/  179]
train() client id: f_00007-0-3 loss: 0.621273  [  128/  179]
train() client id: f_00007-0-4 loss: 0.787774  [  160/  179]
train() client id: f_00007-1-0 loss: 0.598568  [   32/  179]
train() client id: f_00007-1-1 loss: 0.704366  [   64/  179]
train() client id: f_00007-1-2 loss: 0.917916  [   96/  179]
train() client id: f_00007-1-3 loss: 0.625047  [  128/  179]
train() client id: f_00007-1-4 loss: 0.694932  [  160/  179]
train() client id: f_00007-2-0 loss: 0.790566  [   32/  179]
train() client id: f_00007-2-1 loss: 0.794910  [   64/  179]
train() client id: f_00007-2-2 loss: 0.547564  [   96/  179]
train() client id: f_00007-2-3 loss: 0.729988  [  128/  179]
train() client id: f_00007-2-4 loss: 0.720903  [  160/  179]
train() client id: f_00007-3-0 loss: 0.849584  [   32/  179]
train() client id: f_00007-3-1 loss: 0.543290  [   64/  179]
train() client id: f_00007-3-2 loss: 0.646681  [   96/  179]
train() client id: f_00007-3-3 loss: 0.869462  [  128/  179]
train() client id: f_00007-3-4 loss: 0.747470  [  160/  179]
train() client id: f_00007-4-0 loss: 0.814326  [   32/  179]
train() client id: f_00007-4-1 loss: 0.509733  [   64/  179]
train() client id: f_00007-4-2 loss: 0.851289  [   96/  179]
train() client id: f_00007-4-3 loss: 0.791909  [  128/  179]
train() client id: f_00007-4-4 loss: 0.540890  [  160/  179]
train() client id: f_00007-5-0 loss: 0.654618  [   32/  179]
train() client id: f_00007-5-1 loss: 0.782485  [   64/  179]
train() client id: f_00007-5-2 loss: 0.789393  [   96/  179]
train() client id: f_00007-5-3 loss: 0.505554  [  128/  179]
train() client id: f_00007-5-4 loss: 0.781575  [  160/  179]
train() client id: f_00007-6-0 loss: 0.596006  [   32/  179]
train() client id: f_00007-6-1 loss: 0.991446  [   64/  179]
train() client id: f_00007-6-2 loss: 0.801666  [   96/  179]
train() client id: f_00007-6-3 loss: 0.654100  [  128/  179]
train() client id: f_00007-6-4 loss: 0.594216  [  160/  179]
train() client id: f_00007-7-0 loss: 0.681596  [   32/  179]
train() client id: f_00007-7-1 loss: 0.559312  [   64/  179]
train() client id: f_00007-7-2 loss: 0.828053  [   96/  179]
train() client id: f_00007-7-3 loss: 0.703814  [  128/  179]
train() client id: f_00007-7-4 loss: 0.636904  [  160/  179]
train() client id: f_00007-8-0 loss: 0.722588  [   32/  179]
train() client id: f_00007-8-1 loss: 0.769993  [   64/  179]
train() client id: f_00007-8-2 loss: 0.661575  [   96/  179]
train() client id: f_00007-8-3 loss: 0.696939  [  128/  179]
train() client id: f_00007-8-4 loss: 0.811977  [  160/  179]
train() client id: f_00007-9-0 loss: 0.885558  [   32/  179]
train() client id: f_00007-9-1 loss: 0.948543  [   64/  179]
train() client id: f_00007-9-2 loss: 0.572732  [   96/  179]
train() client id: f_00007-9-3 loss: 0.639954  [  128/  179]
train() client id: f_00007-9-4 loss: 0.590177  [  160/  179]
train() client id: f_00008-0-0 loss: 0.784434  [   32/  130]
train() client id: f_00008-0-1 loss: 0.636154  [   64/  130]
train() client id: f_00008-0-2 loss: 0.624970  [   96/  130]
train() client id: f_00008-0-3 loss: 0.752697  [  128/  130]
train() client id: f_00008-1-0 loss: 0.674128  [   32/  130]
train() client id: f_00008-1-1 loss: 0.664449  [   64/  130]
train() client id: f_00008-1-2 loss: 0.679926  [   96/  130]
train() client id: f_00008-1-3 loss: 0.789774  [  128/  130]
train() client id: f_00008-2-0 loss: 0.664939  [   32/  130]
train() client id: f_00008-2-1 loss: 0.827755  [   64/  130]
train() client id: f_00008-2-2 loss: 0.709246  [   96/  130]
train() client id: f_00008-2-3 loss: 0.592443  [  128/  130]
train() client id: f_00008-3-0 loss: 0.766182  [   32/  130]
train() client id: f_00008-3-1 loss: 0.696957  [   64/  130]
train() client id: f_00008-3-2 loss: 0.629066  [   96/  130]
train() client id: f_00008-3-3 loss: 0.724191  [  128/  130]
train() client id: f_00008-4-0 loss: 0.783322  [   32/  130]
train() client id: f_00008-4-1 loss: 0.757909  [   64/  130]
train() client id: f_00008-4-2 loss: 0.607970  [   96/  130]
train() client id: f_00008-4-3 loss: 0.660459  [  128/  130]
train() client id: f_00008-5-0 loss: 0.651312  [   32/  130]
train() client id: f_00008-5-1 loss: 0.665923  [   64/  130]
train() client id: f_00008-5-2 loss: 0.749447  [   96/  130]
train() client id: f_00008-5-3 loss: 0.701955  [  128/  130]
train() client id: f_00008-6-0 loss: 0.682492  [   32/  130]
train() client id: f_00008-6-1 loss: 0.674807  [   64/  130]
train() client id: f_00008-6-2 loss: 0.616147  [   96/  130]
train() client id: f_00008-6-3 loss: 0.819427  [  128/  130]
train() client id: f_00008-7-0 loss: 0.588189  [   32/  130]
train() client id: f_00008-7-1 loss: 0.627953  [   64/  130]
train() client id: f_00008-7-2 loss: 0.743917  [   96/  130]
train() client id: f_00008-7-3 loss: 0.799836  [  128/  130]
train() client id: f_00008-8-0 loss: 0.711911  [   32/  130]
train() client id: f_00008-8-1 loss: 0.687133  [   64/  130]
train() client id: f_00008-8-2 loss: 0.758419  [   96/  130]
train() client id: f_00008-8-3 loss: 0.631124  [  128/  130]
train() client id: f_00008-9-0 loss: 0.652438  [   32/  130]
train() client id: f_00008-9-1 loss: 0.768442  [   64/  130]
train() client id: f_00008-9-2 loss: 0.657964  [   96/  130]
train() client id: f_00008-9-3 loss: 0.727410  [  128/  130]
train() client id: f_00009-0-0 loss: 0.872900  [   32/  118]
train() client id: f_00009-0-1 loss: 0.877895  [   64/  118]
train() client id: f_00009-0-2 loss: 0.875821  [   96/  118]
train() client id: f_00009-1-0 loss: 0.846747  [   32/  118]
train() client id: f_00009-1-1 loss: 0.904117  [   64/  118]
train() client id: f_00009-1-2 loss: 0.808220  [   96/  118]
train() client id: f_00009-2-0 loss: 0.884618  [   32/  118]
train() client id: f_00009-2-1 loss: 0.791675  [   64/  118]
train() client id: f_00009-2-2 loss: 0.719727  [   96/  118]
train() client id: f_00009-3-0 loss: 0.734512  [   32/  118]
train() client id: f_00009-3-1 loss: 0.853207  [   64/  118]
train() client id: f_00009-3-2 loss: 0.795004  [   96/  118]
train() client id: f_00009-4-0 loss: 0.730776  [   32/  118]
train() client id: f_00009-4-1 loss: 0.843652  [   64/  118]
train() client id: f_00009-4-2 loss: 0.904671  [   96/  118]
train() client id: f_00009-5-0 loss: 0.851457  [   32/  118]
train() client id: f_00009-5-1 loss: 0.554174  [   64/  118]
train() client id: f_00009-5-2 loss: 0.871578  [   96/  118]
train() client id: f_00009-6-0 loss: 0.797447  [   32/  118]
train() client id: f_00009-6-1 loss: 0.716300  [   64/  118]
train() client id: f_00009-6-2 loss: 0.645741  [   96/  118]
train() client id: f_00009-7-0 loss: 0.723386  [   32/  118]
train() client id: f_00009-7-1 loss: 0.671561  [   64/  118]
train() client id: f_00009-7-2 loss: 0.883597  [   96/  118]
train() client id: f_00009-8-0 loss: 0.745958  [   32/  118]
train() client id: f_00009-8-1 loss: 0.641860  [   64/  118]
train() client id: f_00009-8-2 loss: 0.827681  [   96/  118]
train() client id: f_00009-9-0 loss: 0.730201  [   32/  118]
train() client id: f_00009-9-1 loss: 0.596258  [   64/  118]
train() client id: f_00009-9-2 loss: 0.977427  [   96/  118]
At round 61 accuracy: 0.6472148541114059
At round 61 training accuracy: 0.5855130784708249
At round 61 training loss: 0.8240866351976789
update_location
xs = [  -3.9056584     4.20031788  325.00902392   18.81129433    0.97929623
    3.95640986 -287.44319194 -266.32485185  309.66397685 -252.06087855]
ys = [ 317.5879595   300.55583871    1.32061395 -287.45517586  279.35018685
  262.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [332.98253135 316.78297753 340.04795198 304.93334177 296.71111525
 281.22398726 304.3525573  284.48128755 325.88410291 271.20231996]
dists_bs = [222.7209014  218.80818669 529.31422911 501.42557522 204.59148124
 199.34071762 210.22394361 196.66744272 509.59168784 187.56031626]
uav_gains = [1.52568368e-12 1.97196846e-12 1.37954414e-12 2.43484695e-12
 2.84867986e-12 3.89882946e-12 2.46132727e-12 3.64447161e-12
 1.69955459e-12 4.80856961e-12]
bs_gains = [2.94824353e-11 3.09824743e-11 2.61153645e-12 3.03889670e-12
 3.73946054e-11 4.02184414e-11 3.46564429e-11 4.17679507e-11
 2.90450111e-12 4.76979163e-11]
Round 62
-------------------------------
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.82115899 5.66246847 2.78706489 1.03447434 6.52759612 3.14171865
 1.26660636 3.89259813 2.86603349 2.54747095]
obj_prev = 32.54719040436046
eta_min = 1.2218214947720352e-33	eta_max = 0.9394406220974982
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 7.459395796309552	eta = 0.9090909090909091
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 18.0305957827678	eta = 0.37609788314465736
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 12.220288063290287	eta = 0.5549189078534793
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.203657475325056	eta = 0.605272779953424
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.140690513111194	eta = 0.6086937697223753
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.140415786273087	eta = 0.6087087803393881
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.14041578100363	eta = 0.6087087806273096
eta = 0.6087087806273096
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [0.040963   0.08615235 0.0403128  0.01397944 0.09948158 0.04746507
 0.01755557 0.05819347 0.04226342 0.03836218]
ene_total = [1.12030085 1.67073395 1.1295553  0.56372656 1.90151444 0.97618988
 0.62570552 1.29997444 1.04280632 0.80990852]
ti_comp = [1.2940368  1.43579767 1.28232047 1.33779388 1.43906047 1.44025443
 1.33863419 1.36510734 1.35070396 1.44291406]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [2.56544140e-06 1.93862966e-05 2.49009332e-06 9.54048993e-08
 2.97132334e-05 3.22198749e-06 1.88712718e-07 6.60950921e-06
 2.58614832e-06 1.69476237e-06]
ene_total = [0.39332206 0.14201894 0.41411524 0.31561678 0.13641131 0.13382205
 0.31412702 0.26725561 0.29274776 0.12907455]
optimize_network_iter = 0 obj = 2.5385113123879783
eta = 0.6087087806273096
freqs = [15827603.24540961 30001562.15823117 15718689.94891933  5224809.85098799
 34564766.11963843 16478014.31267369  6557270.26566169 21314613.85120587
 15644958.54527248 13293299.23888385]
eta_min = 0.6087087806273109	eta_max = 0.7286862650795968
af = 0.0009590243179496368	bf = 0.9928211124535382	zeta = 0.0010549267497446007	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [4.92556679e-07 3.72210798e-06 4.78090085e-07 1.83174406e-08
 5.70484736e-06 6.18611464e-07 3.62322484e-08 1.26900498e-06
 4.96532343e-07 3.25389044e-07]
ene_total = [1.6927798  0.61008177 1.78228252 1.35847294 0.58530795 0.57579846
 1.35205494 1.14991453 1.25988629 0.5554585 ]
ti_comp = [0.82931765 0.97107852 0.81760132 0.87307473 0.97434132 0.97553528
 0.87391504 0.90038819 0.88598481 0.97819491]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.61900639e-06 1.09851926e-05 1.58767144e-06 5.80605679e-08
 1.68004452e-05 1.82033517e-06 1.14768207e-07 3.93802316e-06
 1.55795981e-06 9.55815292e-07]
ene_total = [0.56722855 0.204606   0.59721779 0.45518453 0.19640314 0.19296354
 0.45303505 0.38537017 0.42217726 0.18613361]
optimize_network_iter = 1 obj = 3.660319648720293
eta = 0.7286862650795968
freqs = [15746566.89412167 28283149.87566576 15718689.94891934  5104500.91249742
 32549667.43739824 15511225.99197442  6404139.63058966 20604377.17176979
 15207342.34336579 12502384.68143362]
eta_min = 0.7286862650796062	eta_max = 0.7286862650795745
af = 0.0008653145558439159	bf = 0.9928211124535382	zeta = 0.0009518460114283075	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [4.87525871e-07 3.30793358e-06 4.78090085e-07 1.74835808e-08
 5.05906076e-06 5.48151319e-07 3.45597587e-08 1.18584348e-06
 4.69143123e-07 2.87821399e-07]
ene_total = [1.69277942 0.61005013 1.78228252 1.35847287 0.58525862 0.57579307
 1.35205481 1.14990818 1.25988419 0.55545563]
ti_comp = [0.82931765 0.97107852 0.81760132 0.87307473 0.97434132 0.97553528
 0.87391504 0.90038819 0.88598481 0.97819491]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.61900639e-06 1.09851926e-05 1.58767144e-06 5.80605679e-08
 1.68004452e-05 1.82033517e-06 1.14768207e-07 3.93802316e-06
 1.55795981e-06 9.55815292e-07]
ene_total = [0.56722855 0.204606   0.59721779 0.45518453 0.19640314 0.19296354
 0.45303505 0.38537017 0.42217726 0.18613361]
optimize_network_iter = 2 obj = 3.6603196487199923
eta = 0.7286862650795745
freqs = [15746566.89412155 28283149.87566597 15718689.9489192   5104500.91249741
 32549667.4373985  15511225.99197455  6404139.63058965 20604377.17176981
 15207342.34336578 12502384.68143372]
Done!
At round 62 eta: 0.7286862650795745
At round 62 local rounds: 10.364211052331338
At round 62 global rounds: 25.59678869996595
At round 62 a_n: 6.602214497187386
gradient difference: 0.4965956211090088
train() client id: f_00000-0-0 loss: 1.244729  [   32/  126]
train() client id: f_00000-0-1 loss: 1.114140  [   64/  126]
train() client id: f_00000-0-2 loss: 1.247475  [   96/  126]
train() client id: f_00000-1-0 loss: 1.150108  [   32/  126]
train() client id: f_00000-1-1 loss: 1.157627  [   64/  126]
train() client id: f_00000-1-2 loss: 1.226321  [   96/  126]
train() client id: f_00000-2-0 loss: 1.103944  [   32/  126]
train() client id: f_00000-2-1 loss: 1.115513  [   64/  126]
train() client id: f_00000-2-2 loss: 0.997337  [   96/  126]
train() client id: f_00000-3-0 loss: 1.152074  [   32/  126]
train() client id: f_00000-3-1 loss: 0.953213  [   64/  126]
train() client id: f_00000-3-2 loss: 0.936717  [   96/  126]
train() client id: f_00000-4-0 loss: 0.990270  [   32/  126]
train() client id: f_00000-4-1 loss: 0.987598  [   64/  126]
train() client id: f_00000-4-2 loss: 1.029492  [   96/  126]
train() client id: f_00000-5-0 loss: 0.949552  [   32/  126]
train() client id: f_00000-5-1 loss: 1.015971  [   64/  126]
train() client id: f_00000-5-2 loss: 1.059162  [   96/  126]
train() client id: f_00000-6-0 loss: 0.866042  [   32/  126]
train() client id: f_00000-6-1 loss: 0.976533  [   64/  126]
train() client id: f_00000-6-2 loss: 1.006848  [   96/  126]
train() client id: f_00000-7-0 loss: 0.958829  [   32/  126]
train() client id: f_00000-7-1 loss: 1.038484  [   64/  126]
train() client id: f_00000-7-2 loss: 0.865338  [   96/  126]
train() client id: f_00000-8-0 loss: 0.995191  [   32/  126]
train() client id: f_00000-8-1 loss: 0.990731  [   64/  126]
train() client id: f_00000-8-2 loss: 0.893390  [   96/  126]
train() client id: f_00000-9-0 loss: 0.879679  [   32/  126]
train() client id: f_00000-9-1 loss: 0.845296  [   64/  126]
train() client id: f_00000-9-2 loss: 1.014862  [   96/  126]
train() client id: f_00001-0-0 loss: 0.388493  [   32/  265]
train() client id: f_00001-0-1 loss: 0.351379  [   64/  265]
train() client id: f_00001-0-2 loss: 0.475739  [   96/  265]
train() client id: f_00001-0-3 loss: 0.352709  [  128/  265]
train() client id: f_00001-0-4 loss: 0.355955  [  160/  265]
train() client id: f_00001-0-5 loss: 0.365397  [  192/  265]
train() client id: f_00001-0-6 loss: 0.354734  [  224/  265]
train() client id: f_00001-0-7 loss: 0.403764  [  256/  265]
train() client id: f_00001-1-0 loss: 0.531084  [   32/  265]
train() client id: f_00001-1-1 loss: 0.289694  [   64/  265]
train() client id: f_00001-1-2 loss: 0.398357  [   96/  265]
train() client id: f_00001-1-3 loss: 0.334276  [  128/  265]
train() client id: f_00001-1-4 loss: 0.512004  [  160/  265]
train() client id: f_00001-1-5 loss: 0.282661  [  192/  265]
train() client id: f_00001-1-6 loss: 0.401245  [  224/  265]
train() client id: f_00001-1-7 loss: 0.360768  [  256/  265]
train() client id: f_00001-2-0 loss: 0.411183  [   32/  265]
train() client id: f_00001-2-1 loss: 0.381002  [   64/  265]
train() client id: f_00001-2-2 loss: 0.443573  [   96/  265]
train() client id: f_00001-2-3 loss: 0.379612  [  128/  265]
train() client id: f_00001-2-4 loss: 0.366317  [  160/  265]
train() client id: f_00001-2-5 loss: 0.322622  [  192/  265]
train() client id: f_00001-2-6 loss: 0.403901  [  224/  265]
train() client id: f_00001-2-7 loss: 0.343224  [  256/  265]
train() client id: f_00001-3-0 loss: 0.322790  [   32/  265]
train() client id: f_00001-3-1 loss: 0.290183  [   64/  265]
train() client id: f_00001-3-2 loss: 0.389165  [   96/  265]
train() client id: f_00001-3-3 loss: 0.307541  [  128/  265]
train() client id: f_00001-3-4 loss: 0.485510  [  160/  265]
train() client id: f_00001-3-5 loss: 0.460952  [  192/  265]
train() client id: f_00001-3-6 loss: 0.410797  [  224/  265]
train() client id: f_00001-3-7 loss: 0.331898  [  256/  265]
train() client id: f_00001-4-0 loss: 0.407890  [   32/  265]
train() client id: f_00001-4-1 loss: 0.393984  [   64/  265]
train() client id: f_00001-4-2 loss: 0.322519  [   96/  265]
train() client id: f_00001-4-3 loss: 0.419854  [  128/  265]
train() client id: f_00001-4-4 loss: 0.383683  [  160/  265]
train() client id: f_00001-4-5 loss: 0.360689  [  192/  265]
train() client id: f_00001-4-6 loss: 0.292273  [  224/  265]
train() client id: f_00001-4-7 loss: 0.364819  [  256/  265]
train() client id: f_00001-5-0 loss: 0.426775  [   32/  265]
train() client id: f_00001-5-1 loss: 0.259373  [   64/  265]
train() client id: f_00001-5-2 loss: 0.339451  [   96/  265]
train() client id: f_00001-5-3 loss: 0.284702  [  128/  265]
train() client id: f_00001-5-4 loss: 0.476809  [  160/  265]
train() client id: f_00001-5-5 loss: 0.354418  [  192/  265]
train() client id: f_00001-5-6 loss: 0.358856  [  224/  265]
train() client id: f_00001-5-7 loss: 0.389954  [  256/  265]
train() client id: f_00001-6-0 loss: 0.274864  [   32/  265]
train() client id: f_00001-6-1 loss: 0.438341  [   64/  265]
train() client id: f_00001-6-2 loss: 0.381966  [   96/  265]
train() client id: f_00001-6-3 loss: 0.294073  [  128/  265]
train() client id: f_00001-6-4 loss: 0.391308  [  160/  265]
train() client id: f_00001-6-5 loss: 0.409555  [  192/  265]
train() client id: f_00001-6-6 loss: 0.353620  [  224/  265]
train() client id: f_00001-6-7 loss: 0.317889  [  256/  265]
train() client id: f_00001-7-0 loss: 0.497496  [   32/  265]
train() client id: f_00001-7-1 loss: 0.261647  [   64/  265]
train() client id: f_00001-7-2 loss: 0.395939  [   96/  265]
train() client id: f_00001-7-3 loss: 0.343424  [  128/  265]
train() client id: f_00001-7-4 loss: 0.308992  [  160/  265]
train() client id: f_00001-7-5 loss: 0.457757  [  192/  265]
train() client id: f_00001-7-6 loss: 0.275961  [  224/  265]
train() client id: f_00001-7-7 loss: 0.357118  [  256/  265]
train() client id: f_00001-8-0 loss: 0.404674  [   32/  265]
train() client id: f_00001-8-1 loss: 0.405014  [   64/  265]
train() client id: f_00001-8-2 loss: 0.513706  [   96/  265]
train() client id: f_00001-8-3 loss: 0.337944  [  128/  265]
train() client id: f_00001-8-4 loss: 0.273757  [  160/  265]
train() client id: f_00001-8-5 loss: 0.340283  [  192/  265]
train() client id: f_00001-8-6 loss: 0.309150  [  224/  265]
train() client id: f_00001-8-7 loss: 0.299295  [  256/  265]
train() client id: f_00001-9-0 loss: 0.347199  [   32/  265]
train() client id: f_00001-9-1 loss: 0.437182  [   64/  265]
train() client id: f_00001-9-2 loss: 0.260387  [   96/  265]
train() client id: f_00001-9-3 loss: 0.469694  [  128/  265]
train() client id: f_00001-9-4 loss: 0.351844  [  160/  265]
train() client id: f_00001-9-5 loss: 0.260512  [  192/  265]
train() client id: f_00001-9-6 loss: 0.388429  [  224/  265]
train() client id: f_00001-9-7 loss: 0.361839  [  256/  265]
train() client id: f_00002-0-0 loss: 1.220371  [   32/  124]
train() client id: f_00002-0-1 loss: 1.032124  [   64/  124]
train() client id: f_00002-0-2 loss: 1.271025  [   96/  124]
train() client id: f_00002-1-0 loss: 1.157305  [   32/  124]
train() client id: f_00002-1-1 loss: 1.299191  [   64/  124]
train() client id: f_00002-1-2 loss: 1.227625  [   96/  124]
train() client id: f_00002-2-0 loss: 1.280691  [   32/  124]
train() client id: f_00002-2-1 loss: 1.056911  [   64/  124]
train() client id: f_00002-2-2 loss: 1.192225  [   96/  124]
train() client id: f_00002-3-0 loss: 1.257916  [   32/  124]
train() client id: f_00002-3-1 loss: 1.194815  [   64/  124]
train() client id: f_00002-3-2 loss: 1.013217  [   96/  124]
train() client id: f_00002-4-0 loss: 1.036616  [   32/  124]
train() client id: f_00002-4-1 loss: 1.116954  [   64/  124]
train() client id: f_00002-4-2 loss: 1.152537  [   96/  124]
train() client id: f_00002-5-0 loss: 1.167035  [   32/  124]
train() client id: f_00002-5-1 loss: 1.070727  [   64/  124]
train() client id: f_00002-5-2 loss: 1.074506  [   96/  124]
train() client id: f_00002-6-0 loss: 1.121162  [   32/  124]
train() client id: f_00002-6-1 loss: 1.055618  [   64/  124]
train() client id: f_00002-6-2 loss: 1.272798  [   96/  124]
train() client id: f_00002-7-0 loss: 1.389102  [   32/  124]
train() client id: f_00002-7-1 loss: 0.943768  [   64/  124]
train() client id: f_00002-7-2 loss: 1.050766  [   96/  124]
train() client id: f_00002-8-0 loss: 1.035892  [   32/  124]
train() client id: f_00002-8-1 loss: 1.096985  [   64/  124]
train() client id: f_00002-8-2 loss: 1.288399  [   96/  124]
train() client id: f_00002-9-0 loss: 1.112469  [   32/  124]
train() client id: f_00002-9-1 loss: 1.227359  [   64/  124]
train() client id: f_00002-9-2 loss: 1.114601  [   96/  124]
train() client id: f_00003-0-0 loss: 0.469727  [   32/   43]
train() client id: f_00003-1-0 loss: 0.513257  [   32/   43]
train() client id: f_00003-2-0 loss: 0.605550  [   32/   43]
train() client id: f_00003-3-0 loss: 0.527022  [   32/   43]
train() client id: f_00003-4-0 loss: 0.501298  [   32/   43]
train() client id: f_00003-5-0 loss: 0.386446  [   32/   43]
train() client id: f_00003-6-0 loss: 0.705447  [   32/   43]
train() client id: f_00003-7-0 loss: 0.508677  [   32/   43]
train() client id: f_00003-8-0 loss: 0.629676  [   32/   43]
train() client id: f_00003-9-0 loss: 0.399721  [   32/   43]
train() client id: f_00004-0-0 loss: 0.936204  [   32/  306]
train() client id: f_00004-0-1 loss: 0.741767  [   64/  306]
train() client id: f_00004-0-2 loss: 0.730846  [   96/  306]
train() client id: f_00004-0-3 loss: 0.932625  [  128/  306]
train() client id: f_00004-0-4 loss: 0.814073  [  160/  306]
train() client id: f_00004-0-5 loss: 0.927276  [  192/  306]
train() client id: f_00004-0-6 loss: 0.893585  [  224/  306]
train() client id: f_00004-0-7 loss: 0.748548  [  256/  306]
train() client id: f_00004-0-8 loss: 0.749203  [  288/  306]
train() client id: f_00004-1-0 loss: 0.699881  [   32/  306]
train() client id: f_00004-1-1 loss: 0.990236  [   64/  306]
train() client id: f_00004-1-2 loss: 0.824978  [   96/  306]
train() client id: f_00004-1-3 loss: 0.850848  [  128/  306]
train() client id: f_00004-1-4 loss: 0.944776  [  160/  306]
train() client id: f_00004-1-5 loss: 0.781672  [  192/  306]
train() client id: f_00004-1-6 loss: 0.799151  [  224/  306]
train() client id: f_00004-1-7 loss: 0.784371  [  256/  306]
train() client id: f_00004-1-8 loss: 0.836260  [  288/  306]
train() client id: f_00004-2-0 loss: 0.933744  [   32/  306]
train() client id: f_00004-2-1 loss: 0.816955  [   64/  306]
train() client id: f_00004-2-2 loss: 0.781818  [   96/  306]
train() client id: f_00004-2-3 loss: 0.920566  [  128/  306]
train() client id: f_00004-2-4 loss: 0.769697  [  160/  306]
train() client id: f_00004-2-5 loss: 0.949989  [  192/  306]
train() client id: f_00004-2-6 loss: 0.750559  [  224/  306]
train() client id: f_00004-2-7 loss: 0.739595  [  256/  306]
train() client id: f_00004-2-8 loss: 0.825167  [  288/  306]
train() client id: f_00004-3-0 loss: 0.946652  [   32/  306]
train() client id: f_00004-3-1 loss: 0.811443  [   64/  306]
train() client id: f_00004-3-2 loss: 0.844162  [   96/  306]
train() client id: f_00004-3-3 loss: 0.919238  [  128/  306]
train() client id: f_00004-3-4 loss: 0.823563  [  160/  306]
train() client id: f_00004-3-5 loss: 0.742882  [  192/  306]
train() client id: f_00004-3-6 loss: 0.778177  [  224/  306]
train() client id: f_00004-3-7 loss: 0.802256  [  256/  306]
train() client id: f_00004-3-8 loss: 0.800124  [  288/  306]
train() client id: f_00004-4-0 loss: 0.829608  [   32/  306]
train() client id: f_00004-4-1 loss: 0.858205  [   64/  306]
train() client id: f_00004-4-2 loss: 0.731096  [   96/  306]
train() client id: f_00004-4-3 loss: 0.822950  [  128/  306]
train() client id: f_00004-4-4 loss: 0.821477  [  160/  306]
train() client id: f_00004-4-5 loss: 0.786894  [  192/  306]
train() client id: f_00004-4-6 loss: 0.879623  [  224/  306]
train() client id: f_00004-4-7 loss: 0.896100  [  256/  306]
train() client id: f_00004-4-8 loss: 0.765258  [  288/  306]
train() client id: f_00004-5-0 loss: 0.834801  [   32/  306]
train() client id: f_00004-5-1 loss: 0.854031  [   64/  306]
train() client id: f_00004-5-2 loss: 0.876822  [   96/  306]
train() client id: f_00004-5-3 loss: 0.627375  [  128/  306]
train() client id: f_00004-5-4 loss: 0.906378  [  160/  306]
train() client id: f_00004-5-5 loss: 0.795197  [  192/  306]
train() client id: f_00004-5-6 loss: 0.786480  [  224/  306]
train() client id: f_00004-5-7 loss: 0.869943  [  256/  306]
train() client id: f_00004-5-8 loss: 0.841964  [  288/  306]
train() client id: f_00004-6-0 loss: 0.769121  [   32/  306]
train() client id: f_00004-6-1 loss: 0.915845  [   64/  306]
train() client id: f_00004-6-2 loss: 0.776091  [   96/  306]
train() client id: f_00004-6-3 loss: 0.745425  [  128/  306]
train() client id: f_00004-6-4 loss: 0.769757  [  160/  306]
train() client id: f_00004-6-5 loss: 0.847641  [  192/  306]
train() client id: f_00004-6-6 loss: 0.893760  [  224/  306]
train() client id: f_00004-6-7 loss: 0.820767  [  256/  306]
train() client id: f_00004-6-8 loss: 0.919179  [  288/  306]
train() client id: f_00004-7-0 loss: 0.826777  [   32/  306]
train() client id: f_00004-7-1 loss: 0.880565  [   64/  306]
train() client id: f_00004-7-2 loss: 0.833181  [   96/  306]
train() client id: f_00004-7-3 loss: 0.712143  [  128/  306]
train() client id: f_00004-7-4 loss: 0.654748  [  160/  306]
train() client id: f_00004-7-5 loss: 0.907374  [  192/  306]
train() client id: f_00004-7-6 loss: 0.915649  [  224/  306]
train() client id: f_00004-7-7 loss: 0.902398  [  256/  306]
train() client id: f_00004-7-8 loss: 0.781883  [  288/  306]
train() client id: f_00004-8-0 loss: 0.730157  [   32/  306]
train() client id: f_00004-8-1 loss: 0.816038  [   64/  306]
train() client id: f_00004-8-2 loss: 0.839558  [   96/  306]
train() client id: f_00004-8-3 loss: 0.767627  [  128/  306]
train() client id: f_00004-8-4 loss: 0.905839  [  160/  306]
train() client id: f_00004-8-5 loss: 0.885630  [  192/  306]
train() client id: f_00004-8-6 loss: 0.792991  [  224/  306]
train() client id: f_00004-8-7 loss: 0.819649  [  256/  306]
train() client id: f_00004-8-8 loss: 0.778850  [  288/  306]
train() client id: f_00004-9-0 loss: 0.789803  [   32/  306]
train() client id: f_00004-9-1 loss: 0.840829  [   64/  306]
train() client id: f_00004-9-2 loss: 0.779581  [   96/  306]
train() client id: f_00004-9-3 loss: 0.867782  [  128/  306]
train() client id: f_00004-9-4 loss: 0.815289  [  160/  306]
train() client id: f_00004-9-5 loss: 0.831107  [  192/  306]
train() client id: f_00004-9-6 loss: 0.809349  [  224/  306]
train() client id: f_00004-9-7 loss: 0.841750  [  256/  306]
train() client id: f_00004-9-8 loss: 0.787467  [  288/  306]
train() client id: f_00005-0-0 loss: 0.255591  [   32/  146]
train() client id: f_00005-0-1 loss: 0.437864  [   64/  146]
train() client id: f_00005-0-2 loss: 0.481002  [   96/  146]
train() client id: f_00005-0-3 loss: 0.502523  [  128/  146]
train() client id: f_00005-1-0 loss: 0.400653  [   32/  146]
train() client id: f_00005-1-1 loss: 0.249430  [   64/  146]
train() client id: f_00005-1-2 loss: 0.485844  [   96/  146]
train() client id: f_00005-1-3 loss: 0.418961  [  128/  146]
train() client id: f_00005-2-0 loss: 0.437607  [   32/  146]
train() client id: f_00005-2-1 loss: 0.593571  [   64/  146]
train() client id: f_00005-2-2 loss: 0.163659  [   96/  146]
train() client id: f_00005-2-3 loss: 0.415094  [  128/  146]
train() client id: f_00005-3-0 loss: 0.592698  [   32/  146]
train() client id: f_00005-3-1 loss: 0.114413  [   64/  146]
train() client id: f_00005-3-2 loss: 0.599549  [   96/  146]
train() client id: f_00005-3-3 loss: 0.253003  [  128/  146]
train() client id: f_00005-4-0 loss: 0.529518  [   32/  146]
train() client id: f_00005-4-1 loss: 0.399173  [   64/  146]
train() client id: f_00005-4-2 loss: 0.094581  [   96/  146]
train() client id: f_00005-4-3 loss: 0.712234  [  128/  146]
train() client id: f_00005-5-0 loss: 0.084349  [   32/  146]
train() client id: f_00005-5-1 loss: 0.391496  [   64/  146]
train() client id: f_00005-5-2 loss: 0.521027  [   96/  146]
train() client id: f_00005-5-3 loss: 0.543257  [  128/  146]
train() client id: f_00005-6-0 loss: 0.218736  [   32/  146]
train() client id: f_00005-6-1 loss: 0.547015  [   64/  146]
train() client id: f_00005-6-2 loss: 0.475335  [   96/  146]
train() client id: f_00005-6-3 loss: 0.172535  [  128/  146]
train() client id: f_00005-7-0 loss: 0.330831  [   32/  146]
train() client id: f_00005-7-1 loss: 0.501451  [   64/  146]
train() client id: f_00005-7-2 loss: 0.552660  [   96/  146]
train() client id: f_00005-7-3 loss: 0.221088  [  128/  146]
train() client id: f_00005-8-0 loss: 0.183822  [   32/  146]
train() client id: f_00005-8-1 loss: 0.626138  [   64/  146]
train() client id: f_00005-8-2 loss: 0.168336  [   96/  146]
train() client id: f_00005-8-3 loss: 0.528443  [  128/  146]
train() client id: f_00005-9-0 loss: 0.493331  [   32/  146]
train() client id: f_00005-9-1 loss: 0.257686  [   64/  146]
train() client id: f_00005-9-2 loss: 0.227822  [   96/  146]
train() client id: f_00005-9-3 loss: 0.400902  [  128/  146]
train() client id: f_00006-0-0 loss: 0.435607  [   32/   54]
train() client id: f_00006-1-0 loss: 0.426879  [   32/   54]
train() client id: f_00006-2-0 loss: 0.407620  [   32/   54]
train() client id: f_00006-3-0 loss: 0.378211  [   32/   54]
train() client id: f_00006-4-0 loss: 0.382912  [   32/   54]
train() client id: f_00006-5-0 loss: 0.442639  [   32/   54]
train() client id: f_00006-6-0 loss: 0.438149  [   32/   54]
train() client id: f_00006-7-0 loss: 0.489722  [   32/   54]
train() client id: f_00006-8-0 loss: 0.444773  [   32/   54]
train() client id: f_00006-9-0 loss: 0.490547  [   32/   54]
train() client id: f_00007-0-0 loss: 0.837488  [   32/  179]
train() client id: f_00007-0-1 loss: 0.697978  [   64/  179]
train() client id: f_00007-0-2 loss: 0.629304  [   96/  179]
train() client id: f_00007-0-3 loss: 0.769253  [  128/  179]
train() client id: f_00007-0-4 loss: 0.629376  [  160/  179]
train() client id: f_00007-1-0 loss: 0.878536  [   32/  179]
train() client id: f_00007-1-1 loss: 0.722197  [   64/  179]
train() client id: f_00007-1-2 loss: 0.576549  [   96/  179]
train() client id: f_00007-1-3 loss: 0.722693  [  128/  179]
train() client id: f_00007-1-4 loss: 0.794300  [  160/  179]
train() client id: f_00007-2-0 loss: 0.668755  [   32/  179]
train() client id: f_00007-2-1 loss: 0.787300  [   64/  179]
train() client id: f_00007-2-2 loss: 0.721835  [   96/  179]
train() client id: f_00007-2-3 loss: 0.739215  [  128/  179]
train() client id: f_00007-2-4 loss: 0.779276  [  160/  179]
train() client id: f_00007-3-0 loss: 0.889051  [   32/  179]
train() client id: f_00007-3-1 loss: 0.734654  [   64/  179]
train() client id: f_00007-3-2 loss: 0.662859  [   96/  179]
train() client id: f_00007-3-3 loss: 0.694192  [  128/  179]
train() client id: f_00007-3-4 loss: 0.557293  [  160/  179]
train() client id: f_00007-4-0 loss: 0.651888  [   32/  179]
train() client id: f_00007-4-1 loss: 0.834189  [   64/  179]
train() client id: f_00007-4-2 loss: 0.768843  [   96/  179]
train() client id: f_00007-4-3 loss: 0.660979  [  128/  179]
train() client id: f_00007-4-4 loss: 0.632249  [  160/  179]
train() client id: f_00007-5-0 loss: 0.778458  [   32/  179]
train() client id: f_00007-5-1 loss: 0.697538  [   64/  179]
train() client id: f_00007-5-2 loss: 0.707290  [   96/  179]
train() client id: f_00007-5-3 loss: 0.645615  [  128/  179]
train() client id: f_00007-5-4 loss: 0.679518  [  160/  179]
train() client id: f_00007-6-0 loss: 0.636670  [   32/  179]
train() client id: f_00007-6-1 loss: 0.749866  [   64/  179]
train() client id: f_00007-6-2 loss: 0.718511  [   96/  179]
train() client id: f_00007-6-3 loss: 0.647245  [  128/  179]
train() client id: f_00007-6-4 loss: 0.897897  [  160/  179]
train() client id: f_00007-7-0 loss: 0.702519  [   32/  179]
train() client id: f_00007-7-1 loss: 0.872279  [   64/  179]
train() client id: f_00007-7-2 loss: 0.800391  [   96/  179]
train() client id: f_00007-7-3 loss: 0.553014  [  128/  179]
train() client id: f_00007-7-4 loss: 0.646626  [  160/  179]
train() client id: f_00007-8-0 loss: 0.633097  [   32/  179]
train() client id: f_00007-8-1 loss: 0.772526  [   64/  179]
train() client id: f_00007-8-2 loss: 0.626248  [   96/  179]
train() client id: f_00007-8-3 loss: 0.641500  [  128/  179]
train() client id: f_00007-8-4 loss: 0.903366  [  160/  179]
train() client id: f_00007-9-0 loss: 0.584588  [   32/  179]
train() client id: f_00007-9-1 loss: 0.802583  [   64/  179]
train() client id: f_00007-9-2 loss: 0.806155  [   96/  179]
train() client id: f_00007-9-3 loss: 0.594774  [  128/  179]
train() client id: f_00007-9-4 loss: 0.785210  [  160/  179]
train() client id: f_00008-0-0 loss: 0.701354  [   32/  130]
train() client id: f_00008-0-1 loss: 0.604075  [   64/  130]
train() client id: f_00008-0-2 loss: 0.773385  [   96/  130]
train() client id: f_00008-0-3 loss: 0.722928  [  128/  130]
train() client id: f_00008-1-0 loss: 0.680957  [   32/  130]
train() client id: f_00008-1-1 loss: 0.632853  [   64/  130]
train() client id: f_00008-1-2 loss: 0.733880  [   96/  130]
train() client id: f_00008-1-3 loss: 0.801748  [  128/  130]
train() client id: f_00008-2-0 loss: 0.827212  [   32/  130]
train() client id: f_00008-2-1 loss: 0.622347  [   64/  130]
train() client id: f_00008-2-2 loss: 0.714151  [   96/  130]
train() client id: f_00008-2-3 loss: 0.683407  [  128/  130]
train() client id: f_00008-3-0 loss: 0.716007  [   32/  130]
train() client id: f_00008-3-1 loss: 0.745943  [   64/  130]
train() client id: f_00008-3-2 loss: 0.753467  [   96/  130]
train() client id: f_00008-3-3 loss: 0.642203  [  128/  130]
train() client id: f_00008-4-0 loss: 0.635274  [   32/  130]
train() client id: f_00008-4-1 loss: 0.671061  [   64/  130]
train() client id: f_00008-4-2 loss: 0.885080  [   96/  130]
train() client id: f_00008-4-3 loss: 0.658286  [  128/  130]
train() client id: f_00008-5-0 loss: 0.684152  [   32/  130]
train() client id: f_00008-5-1 loss: 0.696938  [   64/  130]
train() client id: f_00008-5-2 loss: 0.697999  [   96/  130]
train() client id: f_00008-5-3 loss: 0.775749  [  128/  130]
train() client id: f_00008-6-0 loss: 0.692854  [   32/  130]
train() client id: f_00008-6-1 loss: 0.657829  [   64/  130]
train() client id: f_00008-6-2 loss: 0.717901  [   96/  130]
train() client id: f_00008-6-3 loss: 0.781451  [  128/  130]
train() client id: f_00008-7-0 loss: 0.727186  [   32/  130]
train() client id: f_00008-7-1 loss: 0.745303  [   64/  130]
train() client id: f_00008-7-2 loss: 0.731562  [   96/  130]
train() client id: f_00008-7-3 loss: 0.645586  [  128/  130]
train() client id: f_00008-8-0 loss: 0.703345  [   32/  130]
train() client id: f_00008-8-1 loss: 0.636821  [   64/  130]
train() client id: f_00008-8-2 loss: 0.774741  [   96/  130]
train() client id: f_00008-8-3 loss: 0.714560  [  128/  130]
train() client id: f_00008-9-0 loss: 0.797681  [   32/  130]
train() client id: f_00008-9-1 loss: 0.652755  [   64/  130]
train() client id: f_00008-9-2 loss: 0.720847  [   96/  130]
train() client id: f_00008-9-3 loss: 0.644850  [  128/  130]
train() client id: f_00009-0-0 loss: 0.952814  [   32/  118]
train() client id: f_00009-0-1 loss: 0.923514  [   64/  118]
train() client id: f_00009-0-2 loss: 0.770172  [   96/  118]
train() client id: f_00009-1-0 loss: 0.856061  [   32/  118]
train() client id: f_00009-1-1 loss: 0.796052  [   64/  118]
train() client id: f_00009-1-2 loss: 0.731558  [   96/  118]
train() client id: f_00009-2-0 loss: 0.686393  [   32/  118]
train() client id: f_00009-2-1 loss: 0.809291  [   64/  118]
train() client id: f_00009-2-2 loss: 0.796865  [   96/  118]
train() client id: f_00009-3-0 loss: 0.651819  [   32/  118]
train() client id: f_00009-3-1 loss: 0.729840  [   64/  118]
train() client id: f_00009-3-2 loss: 0.676071  [   96/  118]
train() client id: f_00009-4-0 loss: 0.608272  [   32/  118]
train() client id: f_00009-4-1 loss: 0.731570  [   64/  118]
train() client id: f_00009-4-2 loss: 0.737077  [   96/  118]
train() client id: f_00009-5-0 loss: 0.451552  [   32/  118]
train() client id: f_00009-5-1 loss: 0.713607  [   64/  118]
train() client id: f_00009-5-2 loss: 0.744273  [   96/  118]
train() client id: f_00009-6-0 loss: 0.593006  [   32/  118]
train() client id: f_00009-6-1 loss: 0.603563  [   64/  118]
train() client id: f_00009-6-2 loss: 0.706010  [   96/  118]
train() client id: f_00009-7-0 loss: 0.590879  [   32/  118]
train() client id: f_00009-7-1 loss: 0.733063  [   64/  118]
train() client id: f_00009-7-2 loss: 0.641885  [   96/  118]
train() client id: f_00009-8-0 loss: 0.495554  [   32/  118]
train() client id: f_00009-8-1 loss: 0.666482  [   64/  118]
train() client id: f_00009-8-2 loss: 0.618176  [   96/  118]
train() client id: f_00009-9-0 loss: 0.497759  [   32/  118]
train() client id: f_00009-9-1 loss: 0.687497  [   64/  118]
train() client id: f_00009-9-2 loss: 0.621255  [   96/  118]
At round 62 accuracy: 0.6472148541114059
At round 62 training accuracy: 0.5881958417169685
At round 62 training loss: 0.8211811051946137
update_location
xs = [  -3.9056584     4.20031788  330.00902392   18.81129433    0.97929623
    3.95640986 -292.44319194 -271.32485185  314.66397685 -257.06087855]
ys = [ 322.5879595   305.55583871    1.32061395 -292.45517586  284.35018685
  267.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [337.75471245 321.53073452 344.82995793 309.65124686 301.42327014
 285.90220796 309.07913397 289.16751458 330.63890923 275.85559109]
dists_bs = [225.95459613 221.71508958 534.03992676 506.04066956 207.18633175
 201.59331142 212.94022257 199.03851774 514.34942217 189.66939928]
uav_gains = [1.42435425e-12 1.82215782e-12 1.29334638e-12 2.23367367e-12
 2.60101956e-12 3.53945382e-12 2.25679855e-12 3.31121347e-12
 1.57981843e-12 4.36146341e-12]
bs_gains = [2.83161877e-11 2.98584584e-11 2.54734430e-12 2.96193078e-12
 3.60979885e-11 3.89727411e-11 3.34327819e-11 4.03896532e-11
 2.82989921e-12 4.62276411e-11]
Round 63
-------------------------------
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.68727091 5.38361967 2.65494812 0.98808132 6.20602153 2.98709652
 1.20875503 3.70447299 2.72587058 2.42213493]
obj_prev = 30.968271608412685
eta_min = 2.604478585360512e-35	eta_max = 0.9403513878285256
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 7.091465885945381	eta = 0.9090909090909091
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 17.42929397319813	eta = 0.36988229006606876
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 11.714987188776163	eta = 0.5503025368408222
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.719694765497792	eta = 0.601396523881516
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657792616593055	eta = 0.6048895302207599
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657519519314839	eta = 0.6049050304207853
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657519513957864	eta = 0.6049050307248393
eta = 0.6049050307248393
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [0.04148026 0.08724024 0.04082185 0.01415596 0.10073778 0.04806443
 0.01777726 0.05892831 0.0427971  0.0388466 ]
ene_total = [1.07662235 1.59233833 1.08542302 0.54531731 1.81230098 0.92997575
 0.60440513 1.2459445  0.99375704 0.77143509]
ti_comp = [1.37728495 1.52626582 1.36543421 1.42198973 1.52960948 1.53088406
 1.42284286 1.45043831 1.44004221 1.53358086]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [2.35156460e-06 1.78143569e-05 2.28042807e-06 8.76809665e-08
 2.73083858e-05 2.96119390e-06 1.73444402e-07 6.07929826e-06
 2.36250370e-06 1.55784967e-06]
ene_total = [0.38350781 0.13481247 0.40330974 0.30876628 0.12938372 0.12684698
 0.3073421  0.26132755 0.2786378  0.12231706]
optimize_network_iter = 0 obj = 2.4562515021459133
eta = 0.6049050307248393
freqs = [15058707.65852771 28579633.50514324 14948302.02718542  4977519.32111269
 32929248.53672925 15698259.84497274  6247090.26876788 20313966.31628257
 14859667.46433555 12665323.49976712]
eta_min = 0.6049050307248401	eta_max = 0.735169781083726
af = 0.0008271317221307269	bf = 0.9700368260492027	zeta = 0.0009098448943437997	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [4.45862869e-07 3.37764920e-06 4.32375195e-07 1.66245432e-08
 5.17774221e-06 5.61450199e-07 3.28855174e-08 1.15265103e-06
 4.47936952e-07 2.95372418e-07]
ene_total = [1.66660366 0.58485258 1.75266791 1.34190688 0.5607002  0.55110835
 1.3357123  1.1353834  1.21083327 0.53150371]
ti_comp = [0.84752723 0.9965081  0.83567649 0.89223201 0.99985176 1.00112634
 0.89308514 0.92068059 0.91028449 1.00382314]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.42363747e-06 9.58012233e-06 1.39567334e-06 5.10557738e-08
 1.46517044e-05 1.58736166e-06 1.00923117e-07 3.45888573e-06
 1.35541058e-06 8.33540515e-07]
ene_total = [0.57212459 0.20091879 0.60166777 0.4606414  0.19270948 0.18920625
 0.4585158  0.38980408 0.41566912 0.18246434]
optimize_network_iter = 1 obj = 3.6637216068534983
eta = 0.735169781083726
freqs = [14977014.44528698 26790042.71364152 14948302.02718542  4855109.38680831
 30831466.09028835 14691709.46586062  6091289.81349057 19586300.25463695
 14387143.5153923  11842221.20609098]
eta_min = 0.7351697810837274	eta_max = 0.735169781083714
af = 0.0007390421746511915	bf = 0.9700368260492027	zeta = 0.0008129463921163107	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [4.41038396e-07 2.96789166e-06 4.32375195e-07 1.58169176e-08
 4.53905178e-06 4.91759633e-07 3.12656630e-08 1.07155188e-06
 4.19901921e-07 2.58228221e-07]
ene_total = [1.66660331 0.58482282 1.75266791 1.34190682 0.56065381 0.55110329
 1.33571218 1.13537751 1.21083123 0.53150101]
ti_comp = [0.84752723 0.9965081  0.83567649 0.89223201 0.99985176 1.00112634
 0.89308514 0.92068059 0.91028449 1.00382314]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.42363747e-06 9.58012233e-06 1.39567334e-06 5.10557738e-08
 1.46517044e-05 1.58736166e-06 1.00923117e-07 3.45888573e-06
 1.35541058e-06 8.33540515e-07]
ene_total = [0.57212459 0.20091879 0.60166777 0.4606414  0.19270948 0.18920625
 0.4585158  0.38980408 0.41566912 0.18246434]
optimize_network_iter = 2 obj = 3.6637216068533323
eta = 0.735169781083714
freqs = [14977014.44528692 26790042.71364162 14948302.02718534  4855109.38680831
 30831466.09028849 14691709.46586069  6091289.81349056 19586300.25463695
 14387143.51539229 11842221.20609103]
Done!
At round 63 eta: 0.735169781083714
At round 63 local rounds: 10.074148866251232
At round 63 global rounds: 24.929989199134315
At round 63 a_n: 6.259668650218071
gradient difference: 0.4991563856601715
train() client id: f_00000-0-0 loss: 1.113604  [   32/  126]
train() client id: f_00000-0-1 loss: 0.959272  [   64/  126]
train() client id: f_00000-0-2 loss: 1.057683  [   96/  126]
train() client id: f_00000-1-0 loss: 0.862251  [   32/  126]
train() client id: f_00000-1-1 loss: 0.934690  [   64/  126]
train() client id: f_00000-1-2 loss: 0.854435  [   96/  126]
train() client id: f_00000-2-0 loss: 0.809422  [   32/  126]
train() client id: f_00000-2-1 loss: 0.918496  [   64/  126]
train() client id: f_00000-2-2 loss: 0.906285  [   96/  126]
train() client id: f_00000-3-0 loss: 0.837985  [   32/  126]
train() client id: f_00000-3-1 loss: 0.890484  [   64/  126]
train() client id: f_00000-3-2 loss: 0.805304  [   96/  126]
train() client id: f_00000-4-0 loss: 0.683417  [   32/  126]
train() client id: f_00000-4-1 loss: 0.880124  [   64/  126]
train() client id: f_00000-4-2 loss: 0.958199  [   96/  126]
train() client id: f_00000-5-0 loss: 0.781426  [   32/  126]
train() client id: f_00000-5-1 loss: 0.767803  [   64/  126]
train() client id: f_00000-5-2 loss: 0.856999  [   96/  126]
train() client id: f_00000-6-0 loss: 0.742101  [   32/  126]
train() client id: f_00000-6-1 loss: 0.904242  [   64/  126]
train() client id: f_00000-6-2 loss: 0.759155  [   96/  126]
train() client id: f_00000-7-0 loss: 0.764982  [   32/  126]
train() client id: f_00000-7-1 loss: 0.801901  [   64/  126]
train() client id: f_00000-7-2 loss: 0.867678  [   96/  126]
train() client id: f_00000-8-0 loss: 0.784701  [   32/  126]
train() client id: f_00000-8-1 loss: 0.743657  [   64/  126]
train() client id: f_00000-8-2 loss: 0.837343  [   96/  126]
train() client id: f_00000-9-0 loss: 0.865203  [   32/  126]
train() client id: f_00000-9-1 loss: 0.840507  [   64/  126]
train() client id: f_00000-9-2 loss: 0.728963  [   96/  126]
train() client id: f_00001-0-0 loss: 0.365168  [   32/  265]
train() client id: f_00001-0-1 loss: 0.375115  [   64/  265]
train() client id: f_00001-0-2 loss: 0.354668  [   96/  265]
train() client id: f_00001-0-3 loss: 0.429802  [  128/  265]
train() client id: f_00001-0-4 loss: 0.434389  [  160/  265]
train() client id: f_00001-0-5 loss: 0.367700  [  192/  265]
train() client id: f_00001-0-6 loss: 0.471035  [  224/  265]
train() client id: f_00001-0-7 loss: 0.709788  [  256/  265]
train() client id: f_00001-1-0 loss: 0.364242  [   32/  265]
train() client id: f_00001-1-1 loss: 0.449522  [   64/  265]
train() client id: f_00001-1-2 loss: 0.425779  [   96/  265]
train() client id: f_00001-1-3 loss: 0.443866  [  128/  265]
train() client id: f_00001-1-4 loss: 0.379336  [  160/  265]
train() client id: f_00001-1-5 loss: 0.384702  [  192/  265]
train() client id: f_00001-1-6 loss: 0.542811  [  224/  265]
train() client id: f_00001-1-7 loss: 0.491393  [  256/  265]
train() client id: f_00001-2-0 loss: 0.478593  [   32/  265]
train() client id: f_00001-2-1 loss: 0.410704  [   64/  265]
train() client id: f_00001-2-2 loss: 0.488002  [   96/  265]
train() client id: f_00001-2-3 loss: 0.577638  [  128/  265]
train() client id: f_00001-2-4 loss: 0.338708  [  160/  265]
train() client id: f_00001-2-5 loss: 0.358434  [  192/  265]
train() client id: f_00001-2-6 loss: 0.459793  [  224/  265]
train() client id: f_00001-2-7 loss: 0.400237  [  256/  265]
train() client id: f_00001-3-0 loss: 0.633481  [   32/  265]
train() client id: f_00001-3-1 loss: 0.482418  [   64/  265]
train() client id: f_00001-3-2 loss: 0.335190  [   96/  265]
train() client id: f_00001-3-3 loss: 0.347639  [  128/  265]
train() client id: f_00001-3-4 loss: 0.402533  [  160/  265]
train() client id: f_00001-3-5 loss: 0.476771  [  192/  265]
train() client id: f_00001-3-6 loss: 0.330009  [  224/  265]
train() client id: f_00001-3-7 loss: 0.456823  [  256/  265]
train() client id: f_00001-4-0 loss: 0.491656  [   32/  265]
train() client id: f_00001-4-1 loss: 0.453479  [   64/  265]
train() client id: f_00001-4-2 loss: 0.450393  [   96/  265]
train() client id: f_00001-4-3 loss: 0.338212  [  128/  265]
train() client id: f_00001-4-4 loss: 0.430298  [  160/  265]
train() client id: f_00001-4-5 loss: 0.379867  [  192/  265]
train() client id: f_00001-4-6 loss: 0.442571  [  224/  265]
train() client id: f_00001-4-7 loss: 0.451281  [  256/  265]
train() client id: f_00001-5-0 loss: 0.341211  [   32/  265]
train() client id: f_00001-5-1 loss: 0.506244  [   64/  265]
train() client id: f_00001-5-2 loss: 0.405271  [   96/  265]
train() client id: f_00001-5-3 loss: 0.453274  [  128/  265]
train() client id: f_00001-5-4 loss: 0.410284  [  160/  265]
train() client id: f_00001-5-5 loss: 0.461614  [  192/  265]
train() client id: f_00001-5-6 loss: 0.339488  [  224/  265]
train() client id: f_00001-5-7 loss: 0.424243  [  256/  265]
train() client id: f_00001-6-0 loss: 0.455257  [   32/  265]
train() client id: f_00001-6-1 loss: 0.338903  [   64/  265]
train() client id: f_00001-6-2 loss: 0.407129  [   96/  265]
train() client id: f_00001-6-3 loss: 0.420790  [  128/  265]
train() client id: f_00001-6-4 loss: 0.383478  [  160/  265]
train() client id: f_00001-6-5 loss: 0.478509  [  192/  265]
train() client id: f_00001-6-6 loss: 0.477464  [  224/  265]
train() client id: f_00001-6-7 loss: 0.420051  [  256/  265]
train() client id: f_00001-7-0 loss: 0.426709  [   32/  265]
train() client id: f_00001-7-1 loss: 0.508321  [   64/  265]
train() client id: f_00001-7-2 loss: 0.437073  [   96/  265]
train() client id: f_00001-7-3 loss: 0.460230  [  128/  265]
train() client id: f_00001-7-4 loss: 0.331154  [  160/  265]
train() client id: f_00001-7-5 loss: 0.475829  [  192/  265]
train() client id: f_00001-7-6 loss: 0.432492  [  224/  265]
train() client id: f_00001-7-7 loss: 0.329488  [  256/  265]
train() client id: f_00001-8-0 loss: 0.436876  [   32/  265]
train() client id: f_00001-8-1 loss: 0.641033  [   64/  265]
train() client id: f_00001-8-2 loss: 0.369887  [   96/  265]
train() client id: f_00001-8-3 loss: 0.328771  [  128/  265]
train() client id: f_00001-8-4 loss: 0.342001  [  160/  265]
train() client id: f_00001-8-5 loss: 0.314444  [  192/  265]
train() client id: f_00001-8-6 loss: 0.325505  [  224/  265]
train() client id: f_00001-8-7 loss: 0.631882  [  256/  265]
train() client id: f_00001-9-0 loss: 0.375762  [   32/  265]
train() client id: f_00001-9-1 loss: 0.593644  [   64/  265]
train() client id: f_00001-9-2 loss: 0.453536  [   96/  265]
train() client id: f_00001-9-3 loss: 0.528094  [  128/  265]
train() client id: f_00001-9-4 loss: 0.390310  [  160/  265]
train() client id: f_00001-9-5 loss: 0.362138  [  192/  265]
train() client id: f_00001-9-6 loss: 0.320826  [  224/  265]
train() client id: f_00001-9-7 loss: 0.312328  [  256/  265]
train() client id: f_00002-0-0 loss: 1.268600  [   32/  124]
train() client id: f_00002-0-1 loss: 1.189977  [   64/  124]
train() client id: f_00002-0-2 loss: 0.852311  [   96/  124]
train() client id: f_00002-1-0 loss: 1.010678  [   32/  124]
train() client id: f_00002-1-1 loss: 1.002169  [   64/  124]
train() client id: f_00002-1-2 loss: 1.124063  [   96/  124]
train() client id: f_00002-2-0 loss: 0.935786  [   32/  124]
train() client id: f_00002-2-1 loss: 0.997539  [   64/  124]
train() client id: f_00002-2-2 loss: 0.922378  [   96/  124]
train() client id: f_00002-3-0 loss: 1.013441  [   32/  124]
train() client id: f_00002-3-1 loss: 0.920369  [   64/  124]
train() client id: f_00002-3-2 loss: 0.933925  [   96/  124]
train() client id: f_00002-4-0 loss: 0.898687  [   32/  124]
train() client id: f_00002-4-1 loss: 0.916523  [   64/  124]
train() client id: f_00002-4-2 loss: 0.976927  [   96/  124]
train() client id: f_00002-5-0 loss: 1.012134  [   32/  124]
train() client id: f_00002-5-1 loss: 1.093791  [   64/  124]
train() client id: f_00002-5-2 loss: 0.884587  [   96/  124]
train() client id: f_00002-6-0 loss: 0.907885  [   32/  124]
train() client id: f_00002-6-1 loss: 0.983829  [   64/  124]
train() client id: f_00002-6-2 loss: 1.070369  [   96/  124]
train() client id: f_00002-7-0 loss: 0.854414  [   32/  124]
train() client id: f_00002-7-1 loss: 1.057953  [   64/  124]
train() client id: f_00002-7-2 loss: 0.900060  [   96/  124]
train() client id: f_00002-8-0 loss: 0.910169  [   32/  124]
train() client id: f_00002-8-1 loss: 1.000735  [   64/  124]
train() client id: f_00002-8-2 loss: 0.930279  [   96/  124]
train() client id: f_00002-9-0 loss: 0.695100  [   32/  124]
train() client id: f_00002-9-1 loss: 0.964189  [   64/  124]
train() client id: f_00002-9-2 loss: 1.242091  [   96/  124]
train() client id: f_00003-0-0 loss: 0.766163  [   32/   43]
train() client id: f_00003-1-0 loss: 0.739213  [   32/   43]
train() client id: f_00003-2-0 loss: 0.836588  [   32/   43]
train() client id: f_00003-3-0 loss: 0.606294  [   32/   43]
train() client id: f_00003-4-0 loss: 0.748169  [   32/   43]
train() client id: f_00003-5-0 loss: 0.784652  [   32/   43]
train() client id: f_00003-6-0 loss: 0.981104  [   32/   43]
train() client id: f_00003-7-0 loss: 0.920723  [   32/   43]
train() client id: f_00003-8-0 loss: 0.768752  [   32/   43]
train() client id: f_00003-9-0 loss: 0.735890  [   32/   43]
train() client id: f_00004-0-0 loss: 0.909018  [   32/  306]
train() client id: f_00004-0-1 loss: 0.897900  [   64/  306]
train() client id: f_00004-0-2 loss: 0.833080  [   96/  306]
train() client id: f_00004-0-3 loss: 1.054483  [  128/  306]
train() client id: f_00004-0-4 loss: 0.760483  [  160/  306]
train() client id: f_00004-0-5 loss: 0.737496  [  192/  306]
train() client id: f_00004-0-6 loss: 0.857574  [  224/  306]
train() client id: f_00004-0-7 loss: 0.900910  [  256/  306]
train() client id: f_00004-0-8 loss: 0.972265  [  288/  306]
train() client id: f_00004-1-0 loss: 0.917171  [   32/  306]
train() client id: f_00004-1-1 loss: 1.005204  [   64/  306]
train() client id: f_00004-1-2 loss: 0.739838  [   96/  306]
train() client id: f_00004-1-3 loss: 0.788131  [  128/  306]
train() client id: f_00004-1-4 loss: 0.866358  [  160/  306]
train() client id: f_00004-1-5 loss: 0.794303  [  192/  306]
train() client id: f_00004-1-6 loss: 1.023577  [  224/  306]
train() client id: f_00004-1-7 loss: 0.725205  [  256/  306]
train() client id: f_00004-1-8 loss: 0.957123  [  288/  306]
train() client id: f_00004-2-0 loss: 0.918450  [   32/  306]
train() client id: f_00004-2-1 loss: 0.893360  [   64/  306]
train() client id: f_00004-2-2 loss: 0.821695  [   96/  306]
train() client id: f_00004-2-3 loss: 0.838458  [  128/  306]
train() client id: f_00004-2-4 loss: 0.904314  [  160/  306]
train() client id: f_00004-2-5 loss: 0.845467  [  192/  306]
train() client id: f_00004-2-6 loss: 0.805926  [  224/  306]
train() client id: f_00004-2-7 loss: 0.941832  [  256/  306]
train() client id: f_00004-2-8 loss: 0.886725  [  288/  306]
train() client id: f_00004-3-0 loss: 0.817577  [   32/  306]
train() client id: f_00004-3-1 loss: 0.938239  [   64/  306]
train() client id: f_00004-3-2 loss: 0.851128  [   96/  306]
train() client id: f_00004-3-3 loss: 0.973909  [  128/  306]
train() client id: f_00004-3-4 loss: 0.899260  [  160/  306]
train() client id: f_00004-3-5 loss: 0.765942  [  192/  306]
train() client id: f_00004-3-6 loss: 0.938586  [  224/  306]
train() client id: f_00004-3-7 loss: 0.899239  [  256/  306]
train() client id: f_00004-3-8 loss: 0.850191  [  288/  306]
train() client id: f_00004-4-0 loss: 0.901649  [   32/  306]
train() client id: f_00004-4-1 loss: 0.880580  [   64/  306]
train() client id: f_00004-4-2 loss: 0.898611  [   96/  306]
train() client id: f_00004-4-3 loss: 0.849102  [  128/  306]
train() client id: f_00004-4-4 loss: 0.791408  [  160/  306]
train() client id: f_00004-4-5 loss: 0.981521  [  192/  306]
train() client id: f_00004-4-6 loss: 0.923575  [  224/  306]
train() client id: f_00004-4-7 loss: 0.926173  [  256/  306]
train() client id: f_00004-4-8 loss: 0.795395  [  288/  306]
train() client id: f_00004-5-0 loss: 0.897378  [   32/  306]
train() client id: f_00004-5-1 loss: 0.840338  [   64/  306]
train() client id: f_00004-5-2 loss: 0.919653  [   96/  306]
train() client id: f_00004-5-3 loss: 0.984307  [  128/  306]
train() client id: f_00004-5-4 loss: 0.730434  [  160/  306]
train() client id: f_00004-5-5 loss: 0.900193  [  192/  306]
train() client id: f_00004-5-6 loss: 0.880072  [  224/  306]
train() client id: f_00004-5-7 loss: 0.865226  [  256/  306]
train() client id: f_00004-5-8 loss: 0.914110  [  288/  306]
train() client id: f_00004-6-0 loss: 0.811350  [   32/  306]
train() client id: f_00004-6-1 loss: 0.890875  [   64/  306]
train() client id: f_00004-6-2 loss: 0.986601  [   96/  306]
train() client id: f_00004-6-3 loss: 0.863990  [  128/  306]
train() client id: f_00004-6-4 loss: 0.936496  [  160/  306]
train() client id: f_00004-6-5 loss: 0.851969  [  192/  306]
train() client id: f_00004-6-6 loss: 0.955623  [  224/  306]
train() client id: f_00004-6-7 loss: 0.793070  [  256/  306]
train() client id: f_00004-6-8 loss: 0.788331  [  288/  306]
train() client id: f_00004-7-0 loss: 0.850493  [   32/  306]
train() client id: f_00004-7-1 loss: 0.921190  [   64/  306]
train() client id: f_00004-7-2 loss: 0.860498  [   96/  306]
train() client id: f_00004-7-3 loss: 0.935689  [  128/  306]
train() client id: f_00004-7-4 loss: 0.819278  [  160/  306]
train() client id: f_00004-7-5 loss: 0.783373  [  192/  306]
train() client id: f_00004-7-6 loss: 0.993711  [  224/  306]
train() client id: f_00004-7-7 loss: 0.919741  [  256/  306]
train() client id: f_00004-7-8 loss: 0.790464  [  288/  306]
train() client id: f_00004-8-0 loss: 0.845712  [   32/  306]
train() client id: f_00004-8-1 loss: 0.940679  [   64/  306]
train() client id: f_00004-8-2 loss: 0.814793  [   96/  306]
train() client id: f_00004-8-3 loss: 0.954037  [  128/  306]
train() client id: f_00004-8-4 loss: 0.811296  [  160/  306]
train() client id: f_00004-8-5 loss: 0.874052  [  192/  306]
train() client id: f_00004-8-6 loss: 0.855960  [  224/  306]
train() client id: f_00004-8-7 loss: 0.816300  [  256/  306]
train() client id: f_00004-8-8 loss: 0.829278  [  288/  306]
train() client id: f_00004-9-0 loss: 0.794919  [   32/  306]
train() client id: f_00004-9-1 loss: 1.010597  [   64/  306]
train() client id: f_00004-9-2 loss: 0.739078  [   96/  306]
train() client id: f_00004-9-3 loss: 0.871047  [  128/  306]
train() client id: f_00004-9-4 loss: 0.885501  [  160/  306]
train() client id: f_00004-9-5 loss: 0.993103  [  192/  306]
train() client id: f_00004-9-6 loss: 0.887146  [  224/  306]
train() client id: f_00004-9-7 loss: 0.837204  [  256/  306]
train() client id: f_00004-9-8 loss: 0.934929  [  288/  306]
train() client id: f_00005-0-0 loss: 0.724319  [   32/  146]
train() client id: f_00005-0-1 loss: 0.515699  [   64/  146]
train() client id: f_00005-0-2 loss: 0.624423  [   96/  146]
train() client id: f_00005-0-3 loss: 0.540625  [  128/  146]
train() client id: f_00005-1-0 loss: 0.897814  [   32/  146]
train() client id: f_00005-1-1 loss: 0.330247  [   64/  146]
train() client id: f_00005-1-2 loss: 0.563263  [   96/  146]
train() client id: f_00005-1-3 loss: 0.710027  [  128/  146]
train() client id: f_00005-2-0 loss: 0.679673  [   32/  146]
train() client id: f_00005-2-1 loss: 0.424866  [   64/  146]
train() client id: f_00005-2-2 loss: 0.732554  [   96/  146]
train() client id: f_00005-2-3 loss: 0.696843  [  128/  146]
train() client id: f_00005-3-0 loss: 0.532926  [   32/  146]
train() client id: f_00005-3-1 loss: 0.559449  [   64/  146]
train() client id: f_00005-3-2 loss: 0.669595  [   96/  146]
train() client id: f_00005-3-3 loss: 0.641310  [  128/  146]
train() client id: f_00005-4-0 loss: 0.576444  [   32/  146]
train() client id: f_00005-4-1 loss: 0.774790  [   64/  146]
train() client id: f_00005-4-2 loss: 0.586694  [   96/  146]
train() client id: f_00005-4-3 loss: 0.631680  [  128/  146]
train() client id: f_00005-5-0 loss: 0.471209  [   32/  146]
train() client id: f_00005-5-1 loss: 0.665598  [   64/  146]
train() client id: f_00005-5-2 loss: 0.415118  [   96/  146]
train() client id: f_00005-5-3 loss: 0.862559  [  128/  146]
train() client id: f_00005-6-0 loss: 0.718573  [   32/  146]
train() client id: f_00005-6-1 loss: 0.517676  [   64/  146]
train() client id: f_00005-6-2 loss: 0.557541  [   96/  146]
train() client id: f_00005-6-3 loss: 0.636048  [  128/  146]
train() client id: f_00005-7-0 loss: 0.556914  [   32/  146]
train() client id: f_00005-7-1 loss: 0.770091  [   64/  146]
train() client id: f_00005-7-2 loss: 0.611514  [   96/  146]
train() client id: f_00005-7-3 loss: 0.532121  [  128/  146]
train() client id: f_00005-8-0 loss: 0.644528  [   32/  146]
train() client id: f_00005-8-1 loss: 0.580804  [   64/  146]
train() client id: f_00005-8-2 loss: 0.679842  [   96/  146]
train() client id: f_00005-8-3 loss: 0.426609  [  128/  146]
train() client id: f_00005-9-0 loss: 0.502096  [   32/  146]
train() client id: f_00005-9-1 loss: 0.445237  [   64/  146]
train() client id: f_00005-9-2 loss: 0.610541  [   96/  146]
train() client id: f_00005-9-3 loss: 0.865839  [  128/  146]
train() client id: f_00006-0-0 loss: 0.475875  [   32/   54]
train() client id: f_00006-1-0 loss: 0.382582  [   32/   54]
train() client id: f_00006-2-0 loss: 0.483538  [   32/   54]
train() client id: f_00006-3-0 loss: 0.380004  [   32/   54]
train() client id: f_00006-4-0 loss: 0.481150  [   32/   54]
train() client id: f_00006-5-0 loss: 0.485165  [   32/   54]
train() client id: f_00006-6-0 loss: 0.412834  [   32/   54]
train() client id: f_00006-7-0 loss: 0.379922  [   32/   54]
train() client id: f_00006-8-0 loss: 0.472133  [   32/   54]
train() client id: f_00006-9-0 loss: 0.426220  [   32/   54]
train() client id: f_00007-0-0 loss: 0.670316  [   32/  179]
train() client id: f_00007-0-1 loss: 0.490757  [   64/  179]
train() client id: f_00007-0-2 loss: 0.572734  [   96/  179]
train() client id: f_00007-0-3 loss: 0.605674  [  128/  179]
train() client id: f_00007-0-4 loss: 0.356193  [  160/  179]
train() client id: f_00007-1-0 loss: 0.653051  [   32/  179]
train() client id: f_00007-1-1 loss: 0.411589  [   64/  179]
train() client id: f_00007-1-2 loss: 0.424616  [   96/  179]
train() client id: f_00007-1-3 loss: 0.283912  [  128/  179]
train() client id: f_00007-1-4 loss: 0.674681  [  160/  179]
train() client id: f_00007-2-0 loss: 0.617997  [   32/  179]
train() client id: f_00007-2-1 loss: 0.446037  [   64/  179]
train() client id: f_00007-2-2 loss: 0.512767  [   96/  179]
train() client id: f_00007-2-3 loss: 0.505350  [  128/  179]
train() client id: f_00007-2-4 loss: 0.529451  [  160/  179]
train() client id: f_00007-3-0 loss: 0.419220  [   32/  179]
train() client id: f_00007-3-1 loss: 0.481543  [   64/  179]
train() client id: f_00007-3-2 loss: 0.555180  [   96/  179]
train() client id: f_00007-3-3 loss: 0.501338  [  128/  179]
train() client id: f_00007-3-4 loss: 0.571650  [  160/  179]
train() client id: f_00007-4-0 loss: 0.596419  [   32/  179]
train() client id: f_00007-4-1 loss: 0.428253  [   64/  179]
train() client id: f_00007-4-2 loss: 0.454072  [   96/  179]
train() client id: f_00007-4-3 loss: 0.347982  [  128/  179]
train() client id: f_00007-4-4 loss: 0.606766  [  160/  179]
train() client id: f_00007-5-0 loss: 0.417175  [   32/  179]
train() client id: f_00007-5-1 loss: 0.543106  [   64/  179]
train() client id: f_00007-5-2 loss: 0.322042  [   96/  179]
train() client id: f_00007-5-3 loss: 0.492594  [  128/  179]
train() client id: f_00007-5-4 loss: 0.656015  [  160/  179]
train() client id: f_00007-6-0 loss: 0.705789  [   32/  179]
train() client id: f_00007-6-1 loss: 0.432705  [   64/  179]
train() client id: f_00007-6-2 loss: 0.344979  [   96/  179]
train() client id: f_00007-6-3 loss: 0.565182  [  128/  179]
train() client id: f_00007-6-4 loss: 0.429293  [  160/  179]
train() client id: f_00007-7-0 loss: 0.331722  [   32/  179]
train() client id: f_00007-7-1 loss: 0.625929  [   64/  179]
train() client id: f_00007-7-2 loss: 0.444995  [   96/  179]
train() client id: f_00007-7-3 loss: 0.311111  [  128/  179]
train() client id: f_00007-7-4 loss: 0.587028  [  160/  179]
train() client id: f_00007-8-0 loss: 0.559137  [   32/  179]
train() client id: f_00007-8-1 loss: 0.320264  [   64/  179]
train() client id: f_00007-8-2 loss: 0.377111  [   96/  179]
train() client id: f_00007-8-3 loss: 0.331319  [  128/  179]
train() client id: f_00007-8-4 loss: 0.675297  [  160/  179]
train() client id: f_00007-9-0 loss: 0.326107  [   32/  179]
train() client id: f_00007-9-1 loss: 0.508538  [   64/  179]
train() client id: f_00007-9-2 loss: 0.361857  [   96/  179]
train() client id: f_00007-9-3 loss: 0.785835  [  128/  179]
train() client id: f_00007-9-4 loss: 0.336966  [  160/  179]
train() client id: f_00008-0-0 loss: 0.663440  [   32/  130]
train() client id: f_00008-0-1 loss: 0.738118  [   64/  130]
train() client id: f_00008-0-2 loss: 0.679049  [   96/  130]
train() client id: f_00008-0-3 loss: 0.778147  [  128/  130]
train() client id: f_00008-1-0 loss: 0.688150  [   32/  130]
train() client id: f_00008-1-1 loss: 0.730805  [   64/  130]
train() client id: f_00008-1-2 loss: 0.617497  [   96/  130]
train() client id: f_00008-1-3 loss: 0.828294  [  128/  130]
train() client id: f_00008-2-0 loss: 0.772241  [   32/  130]
train() client id: f_00008-2-1 loss: 0.705761  [   64/  130]
train() client id: f_00008-2-2 loss: 0.687319  [   96/  130]
train() client id: f_00008-2-3 loss: 0.654924  [  128/  130]
train() client id: f_00008-3-0 loss: 0.761776  [   32/  130]
train() client id: f_00008-3-1 loss: 0.646110  [   64/  130]
train() client id: f_00008-3-2 loss: 0.845005  [   96/  130]
train() client id: f_00008-3-3 loss: 0.641013  [  128/  130]
train() client id: f_00008-4-0 loss: 0.879408  [   32/  130]
train() client id: f_00008-4-1 loss: 0.604745  [   64/  130]
train() client id: f_00008-4-2 loss: 0.582224  [   96/  130]
train() client id: f_00008-4-3 loss: 0.832770  [  128/  130]
train() client id: f_00008-5-0 loss: 0.768072  [   32/  130]
train() client id: f_00008-5-1 loss: 0.638254  [   64/  130]
train() client id: f_00008-5-2 loss: 0.654585  [   96/  130]
train() client id: f_00008-5-3 loss: 0.802678  [  128/  130]
train() client id: f_00008-6-0 loss: 0.777750  [   32/  130]
train() client id: f_00008-6-1 loss: 0.671697  [   64/  130]
train() client id: f_00008-6-2 loss: 0.723560  [   96/  130]
train() client id: f_00008-6-3 loss: 0.713512  [  128/  130]
train() client id: f_00008-7-0 loss: 0.732124  [   32/  130]
train() client id: f_00008-7-1 loss: 0.796285  [   64/  130]
train() client id: f_00008-7-2 loss: 0.656485  [   96/  130]
train() client id: f_00008-7-3 loss: 0.672090  [  128/  130]
train() client id: f_00008-8-0 loss: 0.800937  [   32/  130]
train() client id: f_00008-8-1 loss: 0.669809  [   64/  130]
train() client id: f_00008-8-2 loss: 0.754149  [   96/  130]
train() client id: f_00008-8-3 loss: 0.654487  [  128/  130]
train() client id: f_00008-9-0 loss: 0.797992  [   32/  130]
train() client id: f_00008-9-1 loss: 0.614937  [   64/  130]
train() client id: f_00008-9-2 loss: 0.754050  [   96/  130]
train() client id: f_00008-9-3 loss: 0.671384  [  128/  130]
train() client id: f_00009-0-0 loss: 0.991975  [   32/  118]
train() client id: f_00009-0-1 loss: 1.084534  [   64/  118]
train() client id: f_00009-0-2 loss: 1.077484  [   96/  118]
train() client id: f_00009-1-0 loss: 0.937078  [   32/  118]
train() client id: f_00009-1-1 loss: 1.042102  [   64/  118]
train() client id: f_00009-1-2 loss: 0.854539  [   96/  118]
train() client id: f_00009-2-0 loss: 0.842199  [   32/  118]
train() client id: f_00009-2-1 loss: 1.024311  [   64/  118]
train() client id: f_00009-2-2 loss: 0.917164  [   96/  118]
train() client id: f_00009-3-0 loss: 0.753945  [   32/  118]
train() client id: f_00009-3-1 loss: 1.009965  [   64/  118]
train() client id: f_00009-3-2 loss: 0.949491  [   96/  118]
train() client id: f_00009-4-0 loss: 0.917774  [   32/  118]
train() client id: f_00009-4-1 loss: 0.863329  [   64/  118]
train() client id: f_00009-4-2 loss: 0.887278  [   96/  118]
train() client id: f_00009-5-0 loss: 0.740429  [   32/  118]
train() client id: f_00009-5-1 loss: 0.821552  [   64/  118]
train() client id: f_00009-5-2 loss: 0.925798  [   96/  118]
train() client id: f_00009-6-0 loss: 0.716566  [   32/  118]
train() client id: f_00009-6-1 loss: 0.963088  [   64/  118]
train() client id: f_00009-6-2 loss: 0.827414  [   96/  118]
train() client id: f_00009-7-0 loss: 0.813363  [   32/  118]
train() client id: f_00009-7-1 loss: 0.891196  [   64/  118]
train() client id: f_00009-7-2 loss: 0.671067  [   96/  118]
train() client id: f_00009-8-0 loss: 0.841262  [   32/  118]
train() client id: f_00009-8-1 loss: 0.628756  [   64/  118]
train() client id: f_00009-8-2 loss: 0.899143  [   96/  118]
train() client id: f_00009-9-0 loss: 0.810291  [   32/  118]
train() client id: f_00009-9-1 loss: 0.882389  [   64/  118]
train() client id: f_00009-9-2 loss: 0.774131  [   96/  118]
At round 63 accuracy: 0.6445623342175066
At round 63 training accuracy: 0.5828303152246814
At round 63 training loss: 0.8338885023746044
update_location
xs = [  -3.9056584     4.20031788  335.00902392   18.81129433    0.97929623
    3.95640986 -297.44319194 -276.32485185  319.66397685 -262.06087855]
ys = [ 327.5879595   310.55583871    1.32061395 -297.45517586  289.35018685
  272.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [342.53339308 326.28602732 349.61806322 314.37787206 306.14455678
 290.59114581 313.81418542 293.86408424 335.40084685 280.5207941 ]
dists_bs = [229.25173742 224.69565936 538.77057605 510.6630112  209.86823946
 203.94363122 215.73819853 201.50578171 519.11171038 191.88561647]
uav_gains = [1.33357588e-12 1.68888635e-12 1.21580973e-12 2.05498399e-12
 2.38086830e-12 3.21721303e-12 2.07518341e-12 3.01317070e-12
 1.47290941e-12 3.95629760e-12]
bs_gains = [2.71905946e-11 2.87626551e-11 2.48521091e-12 2.88747186e-12
 3.48211629e-11 3.77281641e-11 3.22328236e-11 3.90201606e-11
 2.75780638e-12 4.47481726e-11]
Round 64
-------------------------------
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.55289596 5.10473648 2.52232289 0.94134025 5.8844195  2.83245373
 1.15055847 3.5161317  2.58559463 2.29678183]
obj_prev = 29.387235422694697
eta_min = 3.6403482068472025e-37	eta_max = 0.941453835955468
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 6.723535975581212	eta = 0.909090909090909
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 16.80493378903063	eta = 0.36372088751318643
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 11.200979803524296	eta = 0.5456938178232738
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.229583039555175	eta = 0.5975126658351408
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168941452928603	eta = 0.601075879986136
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168671162537121	eta = 0.6010918570034193
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168671157124352	eta = 0.6010918573233796
eta = 0.6010918573233796
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [0.04200208 0.08833771 0.04133538 0.01433404 0.10200505 0.04866908
 0.01800089 0.05966962 0.04333548 0.03933528]
ene_total = [1.03172726 1.51359867 1.04004204 0.52610415 1.72269629 0.88362426
 0.58229031 1.19121994 0.94449752 0.7328707 ]
ti_comp = [1.47039382 1.62666926 1.45842809 1.51593046 1.63009164 1.63144497
 1.51679335 1.54542978 1.53931646 1.63417731]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [2.14202768e-06 1.62824548e-05 2.07527568e-06 8.00990907e-08
 2.49643934e-05 2.70703886e-06 1.58456196e-07 5.55957146e-06
 2.14661920e-06 1.42438847e-06]
ene_total = [0.37266401 0.1276584  0.3914396  0.30117553 0.12242423 0.11995133
 0.29982271 0.25497116 0.26451064 0.11564362]
optimize_network_iter = 0 obj = 2.3702612355707346
eta = 0.6010918573233796
freqs = [14282595.45433634 27152940.78877741 14171209.70092077  4727803.67422734
 31288134.07679844 14915942.00582812  5933864.17259814 19305186.95139418
 14076208.69920771 12035193.86722137]
eta_min = 0.6010918573233806	eta_max = 0.7423534842131878
af = 0.0007076189579367038	bf = 0.9451563237723394	zeta = 0.0007783808537303743	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [4.01088465e-07 3.04884240e-06 3.88589348e-07 1.49983222e-08
 4.67451020e-06 5.06885170e-07 2.96704627e-08 1.04101362e-06
 4.01948215e-07 2.66712607e-07]
ene_total = [1.63512592 0.5592525  1.71751674 1.3215506  0.53579916 0.52619371
 1.31561006 1.11849968 1.16054979 0.50736327]
ti_comp = [0.86560563 1.02188107 0.8536399  0.91114227 1.02530345 1.02665678
 0.91200517 0.9406416  0.93452827 1.02938912]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.23942807e-06 8.27345194e-06 1.21469007e-06 4.44613668e-08
 1.26534594e-05 1.37074871e-06 8.78894039e-08 3.00926595e-06
 1.16787135e-06 7.19841221e-07]
ene_total = [0.57696514 0.19745596 0.60603597 0.46630224 0.18924751 0.18568541
 0.46420685 0.394704   0.40951193 0.17903122]
optimize_network_iter = 1 obj = 3.6691462360745932
eta = 0.7423534842131878
freqs = [14200721.70636506 25299122.79358197 14171209.70092077  4604073.01677737
 29115814.90637482 13873547.37129865  5776388.64214124 18564736.76689159
 13570968.78990086 11183104.37494726]
eta_min = 0.7423534842131893	eta_max = 0.7423534842131851
af = 0.0006257175094953287	bf = 0.9451563237723394	zeta = 0.0006882892604448616	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [3.96503235e-07 2.64674534e-06 3.88589348e-07 1.42235570e-08
 4.04794575e-06 4.38513790e-07 2.81165434e-08 9.62688933e-07
 3.73611652e-07 2.30283129e-07]
ene_total = [1.63512561 0.55922482 1.71751674 1.32155055 0.53575602 0.526189
 1.31560995 1.11849429 1.16054784 0.50736076]
ti_comp = [0.86560563 1.02188107 0.8536399  0.91114227 1.02530345 1.02665678
 0.91200517 0.9406416  0.93452827 1.02938912]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.23942807e-06 8.27345194e-06 1.21469007e-06 4.44613668e-08
 1.26534594e-05 1.37074871e-06 8.78894039e-08 3.00926595e-06
 1.16787135e-06 7.19841221e-07]
ene_total = [0.57696514 0.19745596 0.60603597 0.46630224 0.18924751 0.18568541
 0.46420685 0.394704   0.40951193 0.17903122]
optimize_network_iter = 2 obj = 3.669146236074556
eta = 0.7423534842131851
freqs = [14200721.70636504 25299122.79358199 14171209.70092075  4604073.01677736
 29115814.90637484 13873547.37129866  5776388.64214123 18564736.76689158
 13570968.78990086 11183104.37494727]
Done!
At round 64 eta: 0.7423534842131851
At round 64 local rounds: 9.755733857653984
At round 64 global rounds: 24.295568799375985
At round 64 a_n: 5.917122803248752
gradient difference: 0.5047537684440613
train() client id: f_00000-0-0 loss: 1.145816  [   32/  126]
train() client id: f_00000-0-1 loss: 1.166444  [   64/  126]
train() client id: f_00000-0-2 loss: 1.195845  [   96/  126]
train() client id: f_00000-1-0 loss: 1.034714  [   32/  126]
train() client id: f_00000-1-1 loss: 1.302152  [   64/  126]
train() client id: f_00000-1-2 loss: 1.089103  [   96/  126]
train() client id: f_00000-2-0 loss: 1.000263  [   32/  126]
train() client id: f_00000-2-1 loss: 1.061559  [   64/  126]
train() client id: f_00000-2-2 loss: 1.091744  [   96/  126]
train() client id: f_00000-3-0 loss: 0.989239  [   32/  126]
train() client id: f_00000-3-1 loss: 1.002058  [   64/  126]
train() client id: f_00000-3-2 loss: 1.023013  [   96/  126]
train() client id: f_00000-4-0 loss: 1.029978  [   32/  126]
train() client id: f_00000-4-1 loss: 1.001526  [   64/  126]
train() client id: f_00000-4-2 loss: 0.972292  [   96/  126]
train() client id: f_00000-5-0 loss: 0.919118  [   32/  126]
train() client id: f_00000-5-1 loss: 0.917867  [   64/  126]
train() client id: f_00000-5-2 loss: 1.082574  [   96/  126]
train() client id: f_00000-6-0 loss: 0.976560  [   32/  126]
train() client id: f_00000-6-1 loss: 0.984098  [   64/  126]
train() client id: f_00000-6-2 loss: 0.936784  [   96/  126]
train() client id: f_00000-7-0 loss: 0.941577  [   32/  126]
train() client id: f_00000-7-1 loss: 0.864283  [   64/  126]
train() client id: f_00000-7-2 loss: 0.887511  [   96/  126]
train() client id: f_00000-8-0 loss: 0.918640  [   32/  126]
train() client id: f_00000-8-1 loss: 1.038311  [   64/  126]
train() client id: f_00000-8-2 loss: 0.884990  [   96/  126]
train() client id: f_00001-0-0 loss: 0.538549  [   32/  265]
train() client id: f_00001-0-1 loss: 0.572877  [   64/  265]
train() client id: f_00001-0-2 loss: 0.402930  [   96/  265]
train() client id: f_00001-0-3 loss: 0.529994  [  128/  265]
train() client id: f_00001-0-4 loss: 0.439017  [  160/  265]
train() client id: f_00001-0-5 loss: 0.644992  [  192/  265]
train() client id: f_00001-0-6 loss: 0.501059  [  224/  265]
train() client id: f_00001-0-7 loss: 0.408522  [  256/  265]
train() client id: f_00001-1-0 loss: 0.690687  [   32/  265]
train() client id: f_00001-1-1 loss: 0.496273  [   64/  265]
train() client id: f_00001-1-2 loss: 0.606486  [   96/  265]
train() client id: f_00001-1-3 loss: 0.413514  [  128/  265]
train() client id: f_00001-1-4 loss: 0.449992  [  160/  265]
train() client id: f_00001-1-5 loss: 0.505443  [  192/  265]
train() client id: f_00001-1-6 loss: 0.449914  [  224/  265]
train() client id: f_00001-1-7 loss: 0.366865  [  256/  265]
train() client id: f_00001-2-0 loss: 0.444978  [   32/  265]
train() client id: f_00001-2-1 loss: 0.468866  [   64/  265]
train() client id: f_00001-2-2 loss: 0.554880  [   96/  265]
train() client id: f_00001-2-3 loss: 0.525656  [  128/  265]
train() client id: f_00001-2-4 loss: 0.480412  [  160/  265]
train() client id: f_00001-2-5 loss: 0.553078  [  192/  265]
train() client id: f_00001-2-6 loss: 0.497796  [  224/  265]
train() client id: f_00001-2-7 loss: 0.438199  [  256/  265]
train() client id: f_00001-3-0 loss: 0.605101  [   32/  265]
train() client id: f_00001-3-1 loss: 0.405286  [   64/  265]
train() client id: f_00001-3-2 loss: 0.592089  [   96/  265]
train() client id: f_00001-3-3 loss: 0.394062  [  128/  265]
train() client id: f_00001-3-4 loss: 0.397137  [  160/  265]
train() client id: f_00001-3-5 loss: 0.542050  [  192/  265]
train() client id: f_00001-3-6 loss: 0.446105  [  224/  265]
train() client id: f_00001-3-7 loss: 0.557394  [  256/  265]
train() client id: f_00001-4-0 loss: 0.464700  [   32/  265]
train() client id: f_00001-4-1 loss: 0.472246  [   64/  265]
train() client id: f_00001-4-2 loss: 0.528345  [   96/  265]
train() client id: f_00001-4-3 loss: 0.471678  [  128/  265]
train() client id: f_00001-4-4 loss: 0.435694  [  160/  265]
train() client id: f_00001-4-5 loss: 0.488219  [  192/  265]
train() client id: f_00001-4-6 loss: 0.537406  [  224/  265]
train() client id: f_00001-4-7 loss: 0.517351  [  256/  265]
train() client id: f_00001-5-0 loss: 0.435044  [   32/  265]
train() client id: f_00001-5-1 loss: 0.411340  [   64/  265]
train() client id: f_00001-5-2 loss: 0.542425  [   96/  265]
train() client id: f_00001-5-3 loss: 0.432056  [  128/  265]
train() client id: f_00001-5-4 loss: 0.462399  [  160/  265]
train() client id: f_00001-5-5 loss: 0.464921  [  192/  265]
train() client id: f_00001-5-6 loss: 0.677990  [  224/  265]
train() client id: f_00001-5-7 loss: 0.453681  [  256/  265]
train() client id: f_00001-6-0 loss: 0.391281  [   32/  265]
train() client id: f_00001-6-1 loss: 0.378550  [   64/  265]
train() client id: f_00001-6-2 loss: 0.456564  [   96/  265]
train() client id: f_00001-6-3 loss: 0.508131  [  128/  265]
train() client id: f_00001-6-4 loss: 0.596757  [  160/  265]
train() client id: f_00001-6-5 loss: 0.503076  [  192/  265]
train() client id: f_00001-6-6 loss: 0.676906  [  224/  265]
train() client id: f_00001-6-7 loss: 0.399977  [  256/  265]
train() client id: f_00001-7-0 loss: 0.395830  [   32/  265]
train() client id: f_00001-7-1 loss: 0.479229  [   64/  265]
train() client id: f_00001-7-2 loss: 0.444227  [   96/  265]
train() client id: f_00001-7-3 loss: 0.621008  [  128/  265]
train() client id: f_00001-7-4 loss: 0.512689  [  160/  265]
train() client id: f_00001-7-5 loss: 0.626281  [  192/  265]
train() client id: f_00001-7-6 loss: 0.458266  [  224/  265]
train() client id: f_00001-7-7 loss: 0.394198  [  256/  265]
train() client id: f_00001-8-0 loss: 0.433464  [   32/  265]
train() client id: f_00001-8-1 loss: 0.480618  [   64/  265]
train() client id: f_00001-8-2 loss: 0.480701  [   96/  265]
train() client id: f_00001-8-3 loss: 0.396599  [  128/  265]
train() client id: f_00001-8-4 loss: 0.589729  [  160/  265]
train() client id: f_00001-8-5 loss: 0.376315  [  192/  265]
train() client id: f_00001-8-6 loss: 0.581197  [  224/  265]
train() client id: f_00001-8-7 loss: 0.533023  [  256/  265]
train() client id: f_00002-0-0 loss: 0.856346  [   32/  124]
train() client id: f_00002-0-1 loss: 0.740561  [   64/  124]
train() client id: f_00002-0-2 loss: 0.876701  [   96/  124]
train() client id: f_00002-1-0 loss: 0.920289  [   32/  124]
train() client id: f_00002-1-1 loss: 0.672562  [   64/  124]
train() client id: f_00002-1-2 loss: 0.630171  [   96/  124]
train() client id: f_00002-2-0 loss: 0.623069  [   32/  124]
train() client id: f_00002-2-1 loss: 0.742769  [   64/  124]
train() client id: f_00002-2-2 loss: 0.912832  [   96/  124]
train() client id: f_00002-3-0 loss: 0.775803  [   32/  124]
train() client id: f_00002-3-1 loss: 0.675474  [   64/  124]
train() client id: f_00002-3-2 loss: 0.721777  [   96/  124]
train() client id: f_00002-4-0 loss: 0.509333  [   32/  124]
train() client id: f_00002-4-1 loss: 0.576076  [   64/  124]
train() client id: f_00002-4-2 loss: 0.572214  [   96/  124]
train() client id: f_00002-5-0 loss: 0.636577  [   32/  124]
train() client id: f_00002-5-1 loss: 0.753153  [   64/  124]
train() client id: f_00002-5-2 loss: 0.592514  [   96/  124]
train() client id: f_00002-6-0 loss: 0.612048  [   32/  124]
train() client id: f_00002-6-1 loss: 0.690868  [   64/  124]
train() client id: f_00002-6-2 loss: 0.600837  [   96/  124]
train() client id: f_00002-7-0 loss: 0.606748  [   32/  124]
train() client id: f_00002-7-1 loss: 0.748398  [   64/  124]
train() client id: f_00002-7-2 loss: 0.436882  [   96/  124]
train() client id: f_00002-8-0 loss: 0.660482  [   32/  124]
train() client id: f_00002-8-1 loss: 0.714743  [   64/  124]
train() client id: f_00002-8-2 loss: 0.533498  [   96/  124]
train() client id: f_00003-0-0 loss: 0.674801  [   32/   43]
train() client id: f_00003-1-0 loss: 0.583388  [   32/   43]
train() client id: f_00003-2-0 loss: 0.537204  [   32/   43]
train() client id: f_00003-3-0 loss: 0.758256  [   32/   43]
train() client id: f_00003-4-0 loss: 0.691153  [   32/   43]
train() client id: f_00003-5-0 loss: 0.562144  [   32/   43]
train() client id: f_00003-6-0 loss: 0.896098  [   32/   43]
train() client id: f_00003-7-0 loss: 0.966161  [   32/   43]
train() client id: f_00003-8-0 loss: 0.866044  [   32/   43]
train() client id: f_00004-0-0 loss: 0.832105  [   32/  306]
train() client id: f_00004-0-1 loss: 0.658760  [   64/  306]
train() client id: f_00004-0-2 loss: 0.705662  [   96/  306]
train() client id: f_00004-0-3 loss: 0.676703  [  128/  306]
train() client id: f_00004-0-4 loss: 0.849151  [  160/  306]
train() client id: f_00004-0-5 loss: 0.750061  [  192/  306]
train() client id: f_00004-0-6 loss: 0.880188  [  224/  306]
train() client id: f_00004-0-7 loss: 0.700864  [  256/  306]
train() client id: f_00004-0-8 loss: 0.798666  [  288/  306]
train() client id: f_00004-1-0 loss: 0.738969  [   32/  306]
train() client id: f_00004-1-1 loss: 0.749060  [   64/  306]
train() client id: f_00004-1-2 loss: 0.889212  [   96/  306]
train() client id: f_00004-1-3 loss: 0.650419  [  128/  306]
train() client id: f_00004-1-4 loss: 0.837891  [  160/  306]
train() client id: f_00004-1-5 loss: 0.874027  [  192/  306]
train() client id: f_00004-1-6 loss: 0.663919  [  224/  306]
train() client id: f_00004-1-7 loss: 0.759387  [  256/  306]
train() client id: f_00004-1-8 loss: 0.604723  [  288/  306]
train() client id: f_00004-2-0 loss: 0.858704  [   32/  306]
train() client id: f_00004-2-1 loss: 0.790702  [   64/  306]
train() client id: f_00004-2-2 loss: 0.716147  [   96/  306]
train() client id: f_00004-2-3 loss: 0.685303  [  128/  306]
train() client id: f_00004-2-4 loss: 0.779723  [  160/  306]
train() client id: f_00004-2-5 loss: 0.757662  [  192/  306]
train() client id: f_00004-2-6 loss: 0.800248  [  224/  306]
train() client id: f_00004-2-7 loss: 0.779137  [  256/  306]
train() client id: f_00004-2-8 loss: 0.544392  [  288/  306]
train() client id: f_00004-3-0 loss: 0.669006  [   32/  306]
train() client id: f_00004-3-1 loss: 0.662002  [   64/  306]
train() client id: f_00004-3-2 loss: 0.716364  [   96/  306]
train() client id: f_00004-3-3 loss: 0.742571  [  128/  306]
train() client id: f_00004-3-4 loss: 0.850067  [  160/  306]
train() client id: f_00004-3-5 loss: 0.739907  [  192/  306]
train() client id: f_00004-3-6 loss: 0.764087  [  224/  306]
train() client id: f_00004-3-7 loss: 0.823391  [  256/  306]
train() client id: f_00004-3-8 loss: 0.790936  [  288/  306]
train() client id: f_00004-4-0 loss: 0.675326  [   32/  306]
train() client id: f_00004-4-1 loss: 0.902741  [   64/  306]
train() client id: f_00004-4-2 loss: 0.783477  [   96/  306]
train() client id: f_00004-4-3 loss: 0.660885  [  128/  306]
train() client id: f_00004-4-4 loss: 0.810404  [  160/  306]
train() client id: f_00004-4-5 loss: 0.693753  [  192/  306]
train() client id: f_00004-4-6 loss: 0.769579  [  224/  306]
train() client id: f_00004-4-7 loss: 0.710633  [  256/  306]
train() client id: f_00004-4-8 loss: 0.833184  [  288/  306]
train() client id: f_00004-5-0 loss: 0.784552  [   32/  306]
train() client id: f_00004-5-1 loss: 0.807535  [   64/  306]
train() client id: f_00004-5-2 loss: 0.689319  [   96/  306]
train() client id: f_00004-5-3 loss: 0.896524  [  128/  306]
train() client id: f_00004-5-4 loss: 0.765250  [  160/  306]
train() client id: f_00004-5-5 loss: 0.841564  [  192/  306]
train() client id: f_00004-5-6 loss: 0.785877  [  224/  306]
train() client id: f_00004-5-7 loss: 0.655864  [  256/  306]
train() client id: f_00004-5-8 loss: 0.701738  [  288/  306]
train() client id: f_00004-6-0 loss: 0.773628  [   32/  306]
train() client id: f_00004-6-1 loss: 0.777809  [   64/  306]
train() client id: f_00004-6-2 loss: 0.829833  [   96/  306]
train() client id: f_00004-6-3 loss: 0.855692  [  128/  306]
train() client id: f_00004-6-4 loss: 0.713166  [  160/  306]
train() client id: f_00004-6-5 loss: 0.654087  [  192/  306]
train() client id: f_00004-6-6 loss: 0.781919  [  224/  306]
train() client id: f_00004-6-7 loss: 0.760178  [  256/  306]
train() client id: f_00004-6-8 loss: 0.739933  [  288/  306]
train() client id: f_00004-7-0 loss: 0.812449  [   32/  306]
train() client id: f_00004-7-1 loss: 0.840559  [   64/  306]
train() client id: f_00004-7-2 loss: 0.707693  [   96/  306]
train() client id: f_00004-7-3 loss: 0.825667  [  128/  306]
train() client id: f_00004-7-4 loss: 0.717817  [  160/  306]
train() client id: f_00004-7-5 loss: 0.765897  [  192/  306]
train() client id: f_00004-7-6 loss: 0.814044  [  224/  306]
train() client id: f_00004-7-7 loss: 0.718249  [  256/  306]
train() client id: f_00004-7-8 loss: 0.664665  [  288/  306]
train() client id: f_00004-8-0 loss: 0.789003  [   32/  306]
train() client id: f_00004-8-1 loss: 0.639562  [   64/  306]
train() client id: f_00004-8-2 loss: 0.664223  [   96/  306]
train() client id: f_00004-8-3 loss: 0.717151  [  128/  306]
train() client id: f_00004-8-4 loss: 0.819569  [  160/  306]
train() client id: f_00004-8-5 loss: 0.825645  [  192/  306]
train() client id: f_00004-8-6 loss: 0.757239  [  224/  306]
train() client id: f_00004-8-7 loss: 0.881423  [  256/  306]
train() client id: f_00004-8-8 loss: 0.762438  [  288/  306]
train() client id: f_00005-0-0 loss: 0.348760  [   32/  146]
train() client id: f_00005-0-1 loss: 0.602386  [   64/  146]
train() client id: f_00005-0-2 loss: 0.140060  [   96/  146]
train() client id: f_00005-0-3 loss: 0.404103  [  128/  146]
train() client id: f_00005-1-0 loss: 0.471500  [   32/  146]
train() client id: f_00005-1-1 loss: 0.393923  [   64/  146]
train() client id: f_00005-1-2 loss: 0.258851  [   96/  146]
train() client id: f_00005-1-3 loss: 0.490492  [  128/  146]
train() client id: f_00005-2-0 loss: 0.023896  [   32/  146]
train() client id: f_00005-2-1 loss: 0.290972  [   64/  146]
train() client id: f_00005-2-2 loss: 0.258856  [   96/  146]
train() client id: f_00005-2-3 loss: 1.035365  [  128/  146]
train() client id: f_00005-3-0 loss: 0.678768  [   32/  146]
train() client id: f_00005-3-1 loss: 0.295902  [   64/  146]
train() client id: f_00005-3-2 loss: 0.326627  [   96/  146]
train() client id: f_00005-3-3 loss: 0.275695  [  128/  146]
train() client id: f_00005-4-0 loss: 0.277685  [   32/  146]
train() client id: f_00005-4-1 loss: 0.156663  [   64/  146]
train() client id: f_00005-4-2 loss: 0.508283  [   96/  146]
train() client id: f_00005-4-3 loss: 0.789558  [  128/  146]
train() client id: f_00005-5-0 loss: 0.393373  [   32/  146]
train() client id: f_00005-5-1 loss: 0.355426  [   64/  146]
train() client id: f_00005-5-2 loss: 0.209098  [   96/  146]
train() client id: f_00005-5-3 loss: 0.574443  [  128/  146]
train() client id: f_00005-6-0 loss: 0.013434  [   32/  146]
train() client id: f_00005-6-1 loss: 0.456215  [   64/  146]
train() client id: f_00005-6-2 loss: 0.465370  [   96/  146]
train() client id: f_00005-6-3 loss: 0.606700  [  128/  146]
train() client id: f_00005-7-0 loss: 0.355037  [   32/  146]
train() client id: f_00005-7-1 loss: 0.314820  [   64/  146]
train() client id: f_00005-7-2 loss: 0.587478  [   96/  146]
train() client id: f_00005-7-3 loss: 0.343368  [  128/  146]
train() client id: f_00005-8-0 loss: 0.289376  [   32/  146]
train() client id: f_00005-8-1 loss: 0.765242  [   64/  146]
train() client id: f_00005-8-2 loss: 0.194383  [   96/  146]
train() client id: f_00005-8-3 loss: 0.510037  [  128/  146]
train() client id: f_00006-0-0 loss: 0.515069  [   32/   54]
train() client id: f_00006-1-0 loss: 0.491898  [   32/   54]
train() client id: f_00006-2-0 loss: 0.526870  [   32/   54]
train() client id: f_00006-3-0 loss: 0.482344  [   32/   54]
train() client id: f_00006-4-0 loss: 0.524221  [   32/   54]
train() client id: f_00006-5-0 loss: 0.476967  [   32/   54]
train() client id: f_00006-6-0 loss: 0.490588  [   32/   54]
train() client id: f_00006-7-0 loss: 0.541237  [   32/   54]
train() client id: f_00006-8-0 loss: 0.530723  [   32/   54]
train() client id: f_00007-0-0 loss: 0.802815  [   32/  179]
train() client id: f_00007-0-1 loss: 0.693954  [   64/  179]
train() client id: f_00007-0-2 loss: 0.789362  [   96/  179]
train() client id: f_00007-0-3 loss: 0.707059  [  128/  179]
train() client id: f_00007-0-4 loss: 0.732740  [  160/  179]
train() client id: f_00007-1-0 loss: 0.862147  [   32/  179]
train() client id: f_00007-1-1 loss: 0.947662  [   64/  179]
train() client id: f_00007-1-2 loss: 0.616548  [   96/  179]
train() client id: f_00007-1-3 loss: 0.619859  [  128/  179]
train() client id: f_00007-1-4 loss: 0.719650  [  160/  179]
train() client id: f_00007-2-0 loss: 0.500754  [   32/  179]
train() client id: f_00007-2-1 loss: 0.745642  [   64/  179]
train() client id: f_00007-2-2 loss: 0.855164  [   96/  179]
train() client id: f_00007-2-3 loss: 0.632294  [  128/  179]
train() client id: f_00007-2-4 loss: 0.832274  [  160/  179]
train() client id: f_00007-3-0 loss: 0.789019  [   32/  179]
train() client id: f_00007-3-1 loss: 0.695042  [   64/  179]
train() client id: f_00007-3-2 loss: 0.701573  [   96/  179]
train() client id: f_00007-3-3 loss: 0.719499  [  128/  179]
train() client id: f_00007-3-4 loss: 0.762086  [  160/  179]
train() client id: f_00007-4-0 loss: 0.868907  [   32/  179]
train() client id: f_00007-4-1 loss: 0.585791  [   64/  179]
train() client id: f_00007-4-2 loss: 0.706127  [   96/  179]
train() client id: f_00007-4-3 loss: 0.559438  [  128/  179]
train() client id: f_00007-4-4 loss: 0.802143  [  160/  179]
train() client id: f_00007-5-0 loss: 0.714111  [   32/  179]
train() client id: f_00007-5-1 loss: 0.882038  [   64/  179]
train() client id: f_00007-5-2 loss: 0.613134  [   96/  179]
train() client id: f_00007-5-3 loss: 0.696416  [  128/  179]
train() client id: f_00007-5-4 loss: 0.765448  [  160/  179]
train() client id: f_00007-6-0 loss: 0.799709  [   32/  179]
train() client id: f_00007-6-1 loss: 0.720624  [   64/  179]
train() client id: f_00007-6-2 loss: 0.703074  [   96/  179]
train() client id: f_00007-6-3 loss: 0.705847  [  128/  179]
train() client id: f_00007-6-4 loss: 0.642711  [  160/  179]
train() client id: f_00007-7-0 loss: 0.688719  [   32/  179]
train() client id: f_00007-7-1 loss: 0.815584  [   64/  179]
train() client id: f_00007-7-2 loss: 0.586215  [   96/  179]
train() client id: f_00007-7-3 loss: 0.755770  [  128/  179]
train() client id: f_00007-7-4 loss: 0.690169  [  160/  179]
train() client id: f_00007-8-0 loss: 0.665113  [   32/  179]
train() client id: f_00007-8-1 loss: 0.959370  [   64/  179]
train() client id: f_00007-8-2 loss: 0.721801  [   96/  179]
train() client id: f_00007-8-3 loss: 0.747397  [  128/  179]
train() client id: f_00007-8-4 loss: 0.640984  [  160/  179]
train() client id: f_00008-0-0 loss: 0.783896  [   32/  130]
train() client id: f_00008-0-1 loss: 0.651586  [   64/  130]
train() client id: f_00008-0-2 loss: 0.799931  [   96/  130]
train() client id: f_00008-0-3 loss: 0.731889  [  128/  130]
train() client id: f_00008-1-0 loss: 0.793603  [   32/  130]
train() client id: f_00008-1-1 loss: 0.778044  [   64/  130]
train() client id: f_00008-1-2 loss: 0.698122  [   96/  130]
train() client id: f_00008-1-3 loss: 0.688899  [  128/  130]
train() client id: f_00008-2-0 loss: 0.731929  [   32/  130]
train() client id: f_00008-2-1 loss: 0.770684  [   64/  130]
train() client id: f_00008-2-2 loss: 0.710301  [   96/  130]
train() client id: f_00008-2-3 loss: 0.742083  [  128/  130]
train() client id: f_00008-3-0 loss: 0.655118  [   32/  130]
train() client id: f_00008-3-1 loss: 0.702038  [   64/  130]
train() client id: f_00008-3-2 loss: 0.872465  [   96/  130]
train() client id: f_00008-3-3 loss: 0.723427  [  128/  130]
train() client id: f_00008-4-0 loss: 0.674747  [   32/  130]
train() client id: f_00008-4-1 loss: 0.659446  [   64/  130]
train() client id: f_00008-4-2 loss: 0.865077  [   96/  130]
train() client id: f_00008-4-3 loss: 0.756439  [  128/  130]
train() client id: f_00008-5-0 loss: 0.686179  [   32/  130]
train() client id: f_00008-5-1 loss: 0.757587  [   64/  130]
train() client id: f_00008-5-2 loss: 0.708731  [   96/  130]
train() client id: f_00008-5-3 loss: 0.805878  [  128/  130]
train() client id: f_00008-6-0 loss: 0.636056  [   32/  130]
train() client id: f_00008-6-1 loss: 0.767222  [   64/  130]
train() client id: f_00008-6-2 loss: 0.810699  [   96/  130]
train() client id: f_00008-6-3 loss: 0.750405  [  128/  130]
train() client id: f_00008-7-0 loss: 0.708483  [   32/  130]
train() client id: f_00008-7-1 loss: 0.711189  [   64/  130]
train() client id: f_00008-7-2 loss: 0.825146  [   96/  130]
train() client id: f_00008-7-3 loss: 0.702526  [  128/  130]
train() client id: f_00008-8-0 loss: 0.556797  [   32/  130]
train() client id: f_00008-8-1 loss: 0.908504  [   64/  130]
train() client id: f_00008-8-2 loss: 0.814298  [   96/  130]
train() client id: f_00008-8-3 loss: 0.678233  [  128/  130]
train() client id: f_00009-0-0 loss: 1.143129  [   32/  118]
train() client id: f_00009-0-1 loss: 1.080433  [   64/  118]
train() client id: f_00009-0-2 loss: 0.962020  [   96/  118]
train() client id: f_00009-1-0 loss: 1.093750  [   32/  118]
train() client id: f_00009-1-1 loss: 1.000070  [   64/  118]
train() client id: f_00009-1-2 loss: 0.901588  [   96/  118]
train() client id: f_00009-2-0 loss: 0.879473  [   32/  118]
train() client id: f_00009-2-1 loss: 0.993744  [   64/  118]
train() client id: f_00009-2-2 loss: 1.010097  [   96/  118]
train() client id: f_00009-3-0 loss: 0.742874  [   32/  118]
train() client id: f_00009-3-1 loss: 1.074367  [   64/  118]
train() client id: f_00009-3-2 loss: 0.920474  [   96/  118]
train() client id: f_00009-4-0 loss: 0.934625  [   32/  118]
train() client id: f_00009-4-1 loss: 0.865775  [   64/  118]
train() client id: f_00009-4-2 loss: 0.995783  [   96/  118]
train() client id: f_00009-5-0 loss: 0.867269  [   32/  118]
train() client id: f_00009-5-1 loss: 1.061472  [   64/  118]
train() client id: f_00009-5-2 loss: 0.819871  [   96/  118]
train() client id: f_00009-6-0 loss: 0.853391  [   32/  118]
train() client id: f_00009-6-1 loss: 0.769640  [   64/  118]
train() client id: f_00009-6-2 loss: 0.972207  [   96/  118]
train() client id: f_00009-7-0 loss: 0.967066  [   32/  118]
train() client id: f_00009-7-1 loss: 0.765113  [   64/  118]
train() client id: f_00009-7-2 loss: 0.830488  [   96/  118]
train() client id: f_00009-8-0 loss: 0.898822  [   32/  118]
train() client id: f_00009-8-1 loss: 0.932641  [   64/  118]
train() client id: f_00009-8-2 loss: 0.916567  [   96/  118]
At round 64 accuracy: 0.6419098143236074
At round 64 training accuracy: 0.590878604963112
At round 64 training loss: 0.8192384866695696
update_location
xs = [  -3.9056584     4.20031788  340.00902392   18.81129433    0.97929623
    3.95640986 -302.44319194 -281.32485185  324.66397685 -267.06087855]
ys = [ 332.5879595   315.55583871    1.32061395 -302.45517586  294.35018685
  277.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [347.31830497 331.04853121 354.41202063 319.11282989 310.87455913
 295.29029028 318.55733376 298.57050846 340.16961628 285.19734345]
dists_bs = [232.60962728 227.74700376 543.50604767 515.29240511 212.63391028
 206.38833839 218.61473469 204.06574573 523.87842829 194.20530012]
uav_gains = [1.25202968e-12 1.57013546e-12 1.14585755e-12 1.89621938e-12
 2.18534457e-12 2.92930714e-12 1.91387845e-12 2.74742002e-12
 1.37723251e-12 3.59110754e-12]
bs_gains = [2.61057735e-11 2.76966092e-11 2.42505626e-12 2.81542271e-12
 3.35678104e-11 3.64901503e-11 3.10593012e-11 3.76649830e-11
 2.68811993e-12 4.32676259e-11]
Round 65
-------------------------------
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.41801623 4.82581662 2.38917526 0.89421619 5.56278746 2.67778749
 1.09198131 3.3275343  2.44520381 2.17140866]
obj_prev = 27.80392733965525
eta_min = 3.100306852884529e-39	eta_max = 0.942754072368237
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 6.3556060652170405	eta = 0.9090909090909091
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 16.15583264303265	eta = 0.3576308212219315
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 10.677811898063974	eta = 0.5411055889361986
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.733010612728588	eta = 0.5936317061131898
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673834452832171	eta = 0.597263031926322
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673568210715082	eta = 0.5972794701806058
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673568205281397	eta = 0.5972794705161003
eta = 0.5972794705161003
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [0.04252711 0.08944194 0.04185208 0.01451322 0.10328012 0.04927744
 0.0182259  0.0604155  0.04387718 0.03982698]
ene_total = [0.98560292 1.43449508 0.99341101 0.50602748 1.63267669 0.83712064
 0.55929955 1.13571028 0.89502304 0.69420151]
ti_comp = [1.57505947 1.73869087 1.56299493 1.62132055 1.74218994 1.74362015
 1.62219032 1.65177999 1.65020898 1.7463865 ]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.93768734e-06 1.47930885e-05 1.87549493e-06 7.26831216e-08
 2.26850246e-05 2.45991889e-06 1.43794955e-07 5.05150367e-06
 1.93873362e-06 1.29458708e-06]
ene_total = [0.36078585 0.12055314 0.37851121 0.29278761 0.11552795 0.1131294
 0.29151072 0.24810709 0.25036961 0.10904772]
optimize_network_iter = 0 obj = 2.280330304694935
eta = 0.5972794705161003
freqs = [13500159.94622198 25721057.81080511 13388423.45249819  4475740.44373556
 29640890.63331935 14130785.41729783  5617683.65648641 18287997.60708139
 13294430.26761132 11402680.79036755]
eta_min = 0.5972794705161009	eta_max = 0.7502409697515174
af = 0.0005999271736932453	bf = 0.9180266431472464	zeta = 0.0006599198910625699	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [3.58346965e-07 2.73576559e-06 3.46845386e-07 1.34416814e-08
 4.19526387e-06 4.54926060e-07 2.65927760e-08 9.34201802e-07
 3.58540459e-07 2.39414968e-07]
ene_total = [1.59815174 0.53325669 1.67667707 1.29702355 0.51057679 0.50102436
 1.29136322 1.09882803 1.10901597 0.48300462]
ti_comp = [0.8835615  1.0471929  0.87149697 0.92982258 1.05069198 1.05212218
 0.93069235 0.96028202 0.95871102 1.05488853]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.06740436e-06 7.06927946e-06 1.04574117e-06 3.83085836e-08
 1.08119582e-05 1.17116240e-06 7.57285947e-08 2.59091048e-06
 9.95739029e-07 6.15068927e-07]
ene_total = [0.58172358 0.19420128 0.61030556 0.47210056 0.18600018 0.18238342
 0.47004085 0.39999859 0.40368272 0.1758164 ]
optimize_network_iter = 1 obj = 3.676253139763781
eta = 0.7502409697515174
freqs = [13418605.79030925 23811832.48313907 13388423.4524982   4351530.1809025
 27404358.81401067 13057508.37108007  5459605.33203652 17539930.50999551
 12759371.1105045  10525653.53083322]
eta_min = 0.7502409697515208	eta_max = 0.7502409697515149
af = 0.00052469220506583	bf = 0.9180266431472464	zeta = 0.000577161425572413	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [3.54030510e-07 2.34469776e-06 3.46845386e-07 1.27059696e-08
 3.58604780e-06 3.88444377e-07 2.51172226e-08 8.59338214e-07
 3.30260967e-07 2.04002507e-07]
ene_total = [1.59815146 0.53323124 1.67667707 1.2970235  0.51053714 0.50102003
 1.29136313 1.09882316 1.10901413 0.48300231]
ti_comp = [0.8835615  1.0471929  0.87149697 0.92982258 1.05069198 1.05212218
 0.93069235 0.96028202 0.95871102 1.05488853]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.06740436e-06 7.06927946e-06 1.04574117e-06 3.83085836e-08
 1.08119582e-05 1.17116240e-06 7.57285947e-08 2.59091048e-06
 9.95739029e-07 6.15068927e-07]
ene_total = [0.58172358 0.19420128 0.61030556 0.47210056 0.18600018 0.18238342
 0.47004085 0.39999859 0.40368272 0.1758164 ]
optimize_network_iter = 2 obj = 3.6762531397637437
eta = 0.7502409697515149
freqs = [13418605.79030923 23811832.48313908 13388423.45249818  4351530.1809025
 27404358.81401069 13057508.37108008  5459605.33203651 17539930.5099955
 12759371.11050449 10525653.53083322]
Done!
At round 65 eta: 0.7502409697515149
At round 65 local rounds: 9.409653576790653
At round 65 global rounds: 23.691326785509258
At round 65 a_n: 5.574576956279433
gradient difference: 0.593521237373352
train() client id: f_00000-0-0 loss: 1.256971  [   32/  126]
train() client id: f_00000-0-1 loss: 1.160198  [   64/  126]
train() client id: f_00000-0-2 loss: 1.012979  [   96/  126]
train() client id: f_00000-1-0 loss: 1.079557  [   32/  126]
train() client id: f_00000-1-1 loss: 1.066011  [   64/  126]
train() client id: f_00000-1-2 loss: 0.976407  [   96/  126]
train() client id: f_00000-2-0 loss: 1.022427  [   32/  126]
train() client id: f_00000-2-1 loss: 1.043425  [   64/  126]
train() client id: f_00000-2-2 loss: 0.866344  [   96/  126]
train() client id: f_00000-3-0 loss: 1.081416  [   32/  126]
train() client id: f_00000-3-1 loss: 0.873645  [   64/  126]
train() client id: f_00000-3-2 loss: 0.970678  [   96/  126]
train() client id: f_00000-4-0 loss: 0.885140  [   32/  126]
train() client id: f_00000-4-1 loss: 0.909904  [   64/  126]
train() client id: f_00000-4-2 loss: 0.988121  [   96/  126]
train() client id: f_00000-5-0 loss: 0.863841  [   32/  126]
train() client id: f_00000-5-1 loss: 0.982807  [   64/  126]
train() client id: f_00000-5-2 loss: 0.884714  [   96/  126]
train() client id: f_00000-6-0 loss: 0.877970  [   32/  126]
train() client id: f_00000-6-1 loss: 0.897187  [   64/  126]
train() client id: f_00000-6-2 loss: 0.810919  [   96/  126]
train() client id: f_00000-7-0 loss: 0.824490  [   32/  126]
train() client id: f_00000-7-1 loss: 0.887834  [   64/  126]
train() client id: f_00000-7-2 loss: 0.899880  [   96/  126]
train() client id: f_00000-8-0 loss: 0.932221  [   32/  126]
train() client id: f_00000-8-1 loss: 0.823031  [   64/  126]
train() client id: f_00000-8-2 loss: 0.860768  [   96/  126]
train() client id: f_00001-0-0 loss: 0.338735  [   32/  265]
train() client id: f_00001-0-1 loss: 0.621066  [   64/  265]
train() client id: f_00001-0-2 loss: 0.353968  [   96/  265]
train() client id: f_00001-0-3 loss: 0.367789  [  128/  265]
train() client id: f_00001-0-4 loss: 0.311529  [  160/  265]
train() client id: f_00001-0-5 loss: 0.476257  [  192/  265]
train() client id: f_00001-0-6 loss: 0.483652  [  224/  265]
train() client id: f_00001-0-7 loss: 0.337214  [  256/  265]
train() client id: f_00001-1-0 loss: 0.337403  [   32/  265]
train() client id: f_00001-1-1 loss: 0.347444  [   64/  265]
train() client id: f_00001-1-2 loss: 0.319735  [   96/  265]
train() client id: f_00001-1-3 loss: 0.554967  [  128/  265]
train() client id: f_00001-1-4 loss: 0.526690  [  160/  265]
train() client id: f_00001-1-5 loss: 0.448324  [  192/  265]
train() client id: f_00001-1-6 loss: 0.408971  [  224/  265]
train() client id: f_00001-1-7 loss: 0.319970  [  256/  265]
train() client id: f_00001-2-0 loss: 0.388294  [   32/  265]
train() client id: f_00001-2-1 loss: 0.479666  [   64/  265]
train() client id: f_00001-2-2 loss: 0.354095  [   96/  265]
train() client id: f_00001-2-3 loss: 0.358788  [  128/  265]
train() client id: f_00001-2-4 loss: 0.381591  [  160/  265]
train() client id: f_00001-2-5 loss: 0.374044  [  192/  265]
train() client id: f_00001-2-6 loss: 0.418135  [  224/  265]
train() client id: f_00001-2-7 loss: 0.368671  [  256/  265]
train() client id: f_00001-3-0 loss: 0.321059  [   32/  265]
train() client id: f_00001-3-1 loss: 0.572623  [   64/  265]
train() client id: f_00001-3-2 loss: 0.467900  [   96/  265]
train() client id: f_00001-3-3 loss: 0.341228  [  128/  265]
train() client id: f_00001-3-4 loss: 0.322008  [  160/  265]
train() client id: f_00001-3-5 loss: 0.496485  [  192/  265]
train() client id: f_00001-3-6 loss: 0.374238  [  224/  265]
train() client id: f_00001-3-7 loss: 0.290046  [  256/  265]
train() client id: f_00001-4-0 loss: 0.436326  [   32/  265]
train() client id: f_00001-4-1 loss: 0.307063  [   64/  265]
train() client id: f_00001-4-2 loss: 0.423090  [   96/  265]
train() client id: f_00001-4-3 loss: 0.304836  [  128/  265]
train() client id: f_00001-4-4 loss: 0.353339  [  160/  265]
train() client id: f_00001-4-5 loss: 0.457154  [  192/  265]
train() client id: f_00001-4-6 loss: 0.354357  [  224/  265]
train() client id: f_00001-4-7 loss: 0.367622  [  256/  265]
train() client id: f_00001-5-0 loss: 0.406295  [   32/  265]
train() client id: f_00001-5-1 loss: 0.353218  [   64/  265]
train() client id: f_00001-5-2 loss: 0.519338  [   96/  265]
train() client id: f_00001-5-3 loss: 0.346164  [  128/  265]
train() client id: f_00001-5-4 loss: 0.366276  [  160/  265]
train() client id: f_00001-5-5 loss: 0.313564  [  192/  265]
train() client id: f_00001-5-6 loss: 0.300347  [  224/  265]
train() client id: f_00001-5-7 loss: 0.409795  [  256/  265]
train() client id: f_00001-6-0 loss: 0.396824  [   32/  265]
train() client id: f_00001-6-1 loss: 0.388196  [   64/  265]
train() client id: f_00001-6-2 loss: 0.371680  [   96/  265]
train() client id: f_00001-6-3 loss: 0.445343  [  128/  265]
train() client id: f_00001-6-4 loss: 0.404643  [  160/  265]
train() client id: f_00001-6-5 loss: 0.388641  [  192/  265]
train() client id: f_00001-6-6 loss: 0.296883  [  224/  265]
train() client id: f_00001-6-7 loss: 0.395492  [  256/  265]
train() client id: f_00001-7-0 loss: 0.365926  [   32/  265]
train() client id: f_00001-7-1 loss: 0.388456  [   64/  265]
train() client id: f_00001-7-2 loss: 0.377302  [   96/  265]
train() client id: f_00001-7-3 loss: 0.269642  [  128/  265]
train() client id: f_00001-7-4 loss: 0.544328  [  160/  265]
train() client id: f_00001-7-5 loss: 0.279460  [  192/  265]
train() client id: f_00001-7-6 loss: 0.439039  [  224/  265]
train() client id: f_00001-7-7 loss: 0.408843  [  256/  265]
train() client id: f_00001-8-0 loss: 0.501373  [   32/  265]
train() client id: f_00001-8-1 loss: 0.363360  [   64/  265]
train() client id: f_00001-8-2 loss: 0.270737  [   96/  265]
train() client id: f_00001-8-3 loss: 0.275154  [  128/  265]
train() client id: f_00001-8-4 loss: 0.369780  [  160/  265]
train() client id: f_00001-8-5 loss: 0.405468  [  192/  265]
train() client id: f_00001-8-6 loss: 0.415540  [  224/  265]
train() client id: f_00001-8-7 loss: 0.392842  [  256/  265]
train() client id: f_00002-0-0 loss: 1.149491  [   32/  124]
train() client id: f_00002-0-1 loss: 1.092470  [   64/  124]
train() client id: f_00002-0-2 loss: 1.239308  [   96/  124]
train() client id: f_00002-1-0 loss: 1.266735  [   32/  124]
train() client id: f_00002-1-1 loss: 1.048620  [   64/  124]
train() client id: f_00002-1-2 loss: 1.344988  [   96/  124]
train() client id: f_00002-2-0 loss: 1.070070  [   32/  124]
train() client id: f_00002-2-1 loss: 1.118668  [   64/  124]
train() client id: f_00002-2-2 loss: 1.172223  [   96/  124]
train() client id: f_00002-3-0 loss: 1.024718  [   32/  124]
train() client id: f_00002-3-1 loss: 1.140598  [   64/  124]
train() client id: f_00002-3-2 loss: 1.157081  [   96/  124]
train() client id: f_00002-4-0 loss: 1.165994  [   32/  124]
train() client id: f_00002-4-1 loss: 1.028591  [   64/  124]
train() client id: f_00002-4-2 loss: 1.185035  [   96/  124]
train() client id: f_00002-5-0 loss: 1.035688  [   32/  124]
train() client id: f_00002-5-1 loss: 1.213777  [   64/  124]
train() client id: f_00002-5-2 loss: 1.054961  [   96/  124]
train() client id: f_00002-6-0 loss: 0.984149  [   32/  124]
train() client id: f_00002-6-1 loss: 1.280777  [   64/  124]
train() client id: f_00002-6-2 loss: 1.121324  [   96/  124]
train() client id: f_00002-7-0 loss: 1.057528  [   32/  124]
train() client id: f_00002-7-1 loss: 1.258791  [   64/  124]
train() client id: f_00002-7-2 loss: 1.079617  [   96/  124]
train() client id: f_00002-8-0 loss: 1.085487  [   32/  124]
train() client id: f_00002-8-1 loss: 1.089530  [   64/  124]
train() client id: f_00002-8-2 loss: 1.027386  [   96/  124]
train() client id: f_00003-0-0 loss: 0.568889  [   32/   43]
train() client id: f_00003-1-0 loss: 0.478033  [   32/   43]
train() client id: f_00003-2-0 loss: 0.335775  [   32/   43]
train() client id: f_00003-3-0 loss: 0.400473  [   32/   43]
train() client id: f_00003-4-0 loss: 0.508999  [   32/   43]
train() client id: f_00003-5-0 loss: 0.556086  [   32/   43]
train() client id: f_00003-6-0 loss: 0.577521  [   32/   43]
train() client id: f_00003-7-0 loss: 0.415888  [   32/   43]
train() client id: f_00003-8-0 loss: 0.439641  [   32/   43]
train() client id: f_00004-0-0 loss: 0.805781  [   32/  306]
train() client id: f_00004-0-1 loss: 0.819326  [   64/  306]
train() client id: f_00004-0-2 loss: 0.815966  [   96/  306]
train() client id: f_00004-0-3 loss: 0.805826  [  128/  306]
train() client id: f_00004-0-4 loss: 0.857292  [  160/  306]
train() client id: f_00004-0-5 loss: 0.910469  [  192/  306]
train() client id: f_00004-0-6 loss: 0.745101  [  224/  306]
train() client id: f_00004-0-7 loss: 0.821891  [  256/  306]
train() client id: f_00004-0-8 loss: 0.672856  [  288/  306]
train() client id: f_00004-1-0 loss: 0.829334  [   32/  306]
train() client id: f_00004-1-1 loss: 0.858042  [   64/  306]
train() client id: f_00004-1-2 loss: 0.868534  [   96/  306]
train() client id: f_00004-1-3 loss: 0.729146  [  128/  306]
train() client id: f_00004-1-4 loss: 0.867816  [  160/  306]
train() client id: f_00004-1-5 loss: 0.843249  [  192/  306]
train() client id: f_00004-1-6 loss: 0.892845  [  224/  306]
train() client id: f_00004-1-7 loss: 0.749095  [  256/  306]
train() client id: f_00004-1-8 loss: 0.715036  [  288/  306]
train() client id: f_00004-2-0 loss: 0.723165  [   32/  306]
train() client id: f_00004-2-1 loss: 0.882115  [   64/  306]
train() client id: f_00004-2-2 loss: 0.849102  [   96/  306]
train() client id: f_00004-2-3 loss: 0.854058  [  128/  306]
train() client id: f_00004-2-4 loss: 0.781949  [  160/  306]
train() client id: f_00004-2-5 loss: 0.788815  [  192/  306]
train() client id: f_00004-2-6 loss: 0.930519  [  224/  306]
train() client id: f_00004-2-7 loss: 0.661687  [  256/  306]
train() client id: f_00004-2-8 loss: 0.836541  [  288/  306]
train() client id: f_00004-3-0 loss: 0.606074  [   32/  306]
train() client id: f_00004-3-1 loss: 0.859357  [   64/  306]
train() client id: f_00004-3-2 loss: 0.728003  [   96/  306]
train() client id: f_00004-3-3 loss: 0.818657  [  128/  306]
train() client id: f_00004-3-4 loss: 0.922314  [  160/  306]
train() client id: f_00004-3-5 loss: 0.703780  [  192/  306]
train() client id: f_00004-3-6 loss: 0.744588  [  224/  306]
train() client id: f_00004-3-7 loss: 1.004291  [  256/  306]
train() client id: f_00004-3-8 loss: 0.810399  [  288/  306]
train() client id: f_00004-4-0 loss: 0.754820  [   32/  306]
train() client id: f_00004-4-1 loss: 0.864843  [   64/  306]
train() client id: f_00004-4-2 loss: 0.909074  [   96/  306]
train() client id: f_00004-4-3 loss: 0.846076  [  128/  306]
train() client id: f_00004-4-4 loss: 0.854702  [  160/  306]
train() client id: f_00004-4-5 loss: 0.693695  [  192/  306]
train() client id: f_00004-4-6 loss: 0.874601  [  224/  306]
train() client id: f_00004-4-7 loss: 0.782236  [  256/  306]
train() client id: f_00004-4-8 loss: 0.762373  [  288/  306]
train() client id: f_00004-5-0 loss: 0.942054  [   32/  306]
train() client id: f_00004-5-1 loss: 0.772733  [   64/  306]
train() client id: f_00004-5-2 loss: 0.742790  [   96/  306]
train() client id: f_00004-5-3 loss: 0.860055  [  128/  306]
train() client id: f_00004-5-4 loss: 0.794441  [  160/  306]
train() client id: f_00004-5-5 loss: 0.924292  [  192/  306]
train() client id: f_00004-5-6 loss: 0.894895  [  224/  306]
train() client id: f_00004-5-7 loss: 0.693305  [  256/  306]
train() client id: f_00004-5-8 loss: 0.687649  [  288/  306]
train() client id: f_00004-6-0 loss: 0.778390  [   32/  306]
train() client id: f_00004-6-1 loss: 0.909310  [   64/  306]
train() client id: f_00004-6-2 loss: 0.700023  [   96/  306]
train() client id: f_00004-6-3 loss: 0.911091  [  128/  306]
train() client id: f_00004-6-4 loss: 0.690298  [  160/  306]
train() client id: f_00004-6-5 loss: 0.790806  [  192/  306]
train() client id: f_00004-6-6 loss: 0.836588  [  224/  306]
train() client id: f_00004-6-7 loss: 0.795068  [  256/  306]
train() client id: f_00004-6-8 loss: 0.756965  [  288/  306]
train() client id: f_00004-7-0 loss: 0.796440  [   32/  306]
train() client id: f_00004-7-1 loss: 0.716319  [   64/  306]
train() client id: f_00004-7-2 loss: 0.860464  [   96/  306]
train() client id: f_00004-7-3 loss: 0.822009  [  128/  306]
train() client id: f_00004-7-4 loss: 0.838497  [  160/  306]
train() client id: f_00004-7-5 loss: 0.731303  [  192/  306]
train() client id: f_00004-7-6 loss: 0.920857  [  224/  306]
train() client id: f_00004-7-7 loss: 0.846560  [  256/  306]
train() client id: f_00004-7-8 loss: 0.769881  [  288/  306]
train() client id: f_00004-8-0 loss: 0.711872  [   32/  306]
train() client id: f_00004-8-1 loss: 0.834828  [   64/  306]
train() client id: f_00004-8-2 loss: 0.851385  [   96/  306]
train() client id: f_00004-8-3 loss: 0.906383  [  128/  306]
train() client id: f_00004-8-4 loss: 0.846876  [  160/  306]
train() client id: f_00004-8-5 loss: 0.743223  [  192/  306]
train() client id: f_00004-8-6 loss: 0.896158  [  224/  306]
train() client id: f_00004-8-7 loss: 0.842251  [  256/  306]
train() client id: f_00004-8-8 loss: 0.762579  [  288/  306]
train() client id: f_00005-0-0 loss: 0.380732  [   32/  146]
train() client id: f_00005-0-1 loss: 0.664826  [   64/  146]
train() client id: f_00005-0-2 loss: 0.471904  [   96/  146]
train() client id: f_00005-0-3 loss: 0.550227  [  128/  146]
train() client id: f_00005-1-0 loss: 0.598838  [   32/  146]
train() client id: f_00005-1-1 loss: 0.674076  [   64/  146]
train() client id: f_00005-1-2 loss: 0.417339  [   96/  146]
train() client id: f_00005-1-3 loss: 0.285695  [  128/  146]
train() client id: f_00005-2-0 loss: 0.603898  [   32/  146]
train() client id: f_00005-2-1 loss: 0.493952  [   64/  146]
train() client id: f_00005-2-2 loss: 0.680931  [   96/  146]
train() client id: f_00005-2-3 loss: 0.339171  [  128/  146]
train() client id: f_00005-3-0 loss: 0.424288  [   32/  146]
train() client id: f_00005-3-1 loss: 0.537112  [   64/  146]
train() client id: f_00005-3-2 loss: 0.832215  [   96/  146]
train() client id: f_00005-3-3 loss: 0.319835  [  128/  146]
train() client id: f_00005-4-0 loss: 0.673167  [   32/  146]
train() client id: f_00005-4-1 loss: 0.348641  [   64/  146]
train() client id: f_00005-4-2 loss: 0.377955  [   96/  146]
train() client id: f_00005-4-3 loss: 0.688826  [  128/  146]
train() client id: f_00005-5-0 loss: 0.424455  [   32/  146]
train() client id: f_00005-5-1 loss: 0.677260  [   64/  146]
train() client id: f_00005-5-2 loss: 0.716662  [   96/  146]
train() client id: f_00005-5-3 loss: 0.375347  [  128/  146]
train() client id: f_00005-6-0 loss: 0.401181  [   32/  146]
train() client id: f_00005-6-1 loss: 0.513201  [   64/  146]
train() client id: f_00005-6-2 loss: 0.603965  [   96/  146]
train() client id: f_00005-6-3 loss: 0.601934  [  128/  146]
train() client id: f_00005-7-0 loss: 0.406681  [   32/  146]
train() client id: f_00005-7-1 loss: 0.563478  [   64/  146]
train() client id: f_00005-7-2 loss: 0.537980  [   96/  146]
train() client id: f_00005-7-3 loss: 0.571832  [  128/  146]
train() client id: f_00005-8-0 loss: 0.321104  [   32/  146]
train() client id: f_00005-8-1 loss: 0.482356  [   64/  146]
train() client id: f_00005-8-2 loss: 0.499532  [   96/  146]
train() client id: f_00005-8-3 loss: 0.630880  [  128/  146]
train() client id: f_00006-0-0 loss: 0.500720  [   32/   54]
train() client id: f_00006-1-0 loss: 0.422388  [   32/   54]
train() client id: f_00006-2-0 loss: 0.449854  [   32/   54]
train() client id: f_00006-3-0 loss: 0.467133  [   32/   54]
train() client id: f_00006-4-0 loss: 0.505173  [   32/   54]
train() client id: f_00006-5-0 loss: 0.432522  [   32/   54]
train() client id: f_00006-6-0 loss: 0.423929  [   32/   54]
train() client id: f_00006-7-0 loss: 0.465262  [   32/   54]
train() client id: f_00006-8-0 loss: 0.442946  [   32/   54]
train() client id: f_00007-0-0 loss: 0.609115  [   32/  179]
train() client id: f_00007-0-1 loss: 0.579714  [   64/  179]
train() client id: f_00007-0-2 loss: 0.539222  [   96/  179]
train() client id: f_00007-0-3 loss: 0.970887  [  128/  179]
train() client id: f_00007-0-4 loss: 0.551745  [  160/  179]
train() client id: f_00007-1-0 loss: 0.423350  [   32/  179]
train() client id: f_00007-1-1 loss: 0.597420  [   64/  179]
train() client id: f_00007-1-2 loss: 0.499433  [   96/  179]
train() client id: f_00007-1-3 loss: 0.788671  [  128/  179]
train() client id: f_00007-1-4 loss: 0.850959  [  160/  179]
train() client id: f_00007-2-0 loss: 0.632526  [   32/  179]
train() client id: f_00007-2-1 loss: 0.596617  [   64/  179]
train() client id: f_00007-2-2 loss: 0.750155  [   96/  179]
train() client id: f_00007-2-3 loss: 0.613706  [  128/  179]
train() client id: f_00007-2-4 loss: 0.644173  [  160/  179]
train() client id: f_00007-3-0 loss: 0.637090  [   32/  179]
train() client id: f_00007-3-1 loss: 0.809430  [   64/  179]
train() client id: f_00007-3-2 loss: 0.611964  [   96/  179]
train() client id: f_00007-3-3 loss: 0.641717  [  128/  179]
train() client id: f_00007-3-4 loss: 0.537326  [  160/  179]
train() client id: f_00007-4-0 loss: 0.758447  [   32/  179]
train() client id: f_00007-4-1 loss: 0.630181  [   64/  179]
train() client id: f_00007-4-2 loss: 0.592890  [   96/  179]
train() client id: f_00007-4-3 loss: 0.468138  [  128/  179]
train() client id: f_00007-4-4 loss: 0.604016  [  160/  179]
train() client id: f_00007-5-0 loss: 0.620711  [   32/  179]
train() client id: f_00007-5-1 loss: 0.627763  [   64/  179]
train() client id: f_00007-5-2 loss: 0.660154  [   96/  179]
train() client id: f_00007-5-3 loss: 0.741460  [  128/  179]
train() client id: f_00007-5-4 loss: 0.448265  [  160/  179]
train() client id: f_00007-6-0 loss: 0.416324  [   32/  179]
train() client id: f_00007-6-1 loss: 0.589151  [   64/  179]
train() client id: f_00007-6-2 loss: 0.447432  [   96/  179]
train() client id: f_00007-6-3 loss: 0.659577  [  128/  179]
train() client id: f_00007-6-4 loss: 0.840326  [  160/  179]
train() client id: f_00007-7-0 loss: 0.712586  [   32/  179]
train() client id: f_00007-7-1 loss: 0.459885  [   64/  179]
train() client id: f_00007-7-2 loss: 0.478289  [   96/  179]
train() client id: f_00007-7-3 loss: 0.709665  [  128/  179]
train() client id: f_00007-7-4 loss: 0.545307  [  160/  179]
train() client id: f_00007-8-0 loss: 0.533650  [   32/  179]
train() client id: f_00007-8-1 loss: 0.963399  [   64/  179]
train() client id: f_00007-8-2 loss: 0.497181  [   96/  179]
train() client id: f_00007-8-3 loss: 0.527702  [  128/  179]
train() client id: f_00007-8-4 loss: 0.623818  [  160/  179]
train() client id: f_00008-0-0 loss: 0.706324  [   32/  130]
train() client id: f_00008-0-1 loss: 0.683378  [   64/  130]
train() client id: f_00008-0-2 loss: 0.906385  [   96/  130]
train() client id: f_00008-0-3 loss: 0.724887  [  128/  130]
train() client id: f_00008-1-0 loss: 0.717888  [   32/  130]
train() client id: f_00008-1-1 loss: 0.804171  [   64/  130]
train() client id: f_00008-1-2 loss: 0.665654  [   96/  130]
train() client id: f_00008-1-3 loss: 0.824064  [  128/  130]
train() client id: f_00008-2-0 loss: 0.687576  [   32/  130]
train() client id: f_00008-2-1 loss: 0.780506  [   64/  130]
train() client id: f_00008-2-2 loss: 0.730289  [   96/  130]
train() client id: f_00008-2-3 loss: 0.795508  [  128/  130]
train() client id: f_00008-3-0 loss: 0.694544  [   32/  130]
train() client id: f_00008-3-1 loss: 0.708585  [   64/  130]
train() client id: f_00008-3-2 loss: 0.884191  [   96/  130]
train() client id: f_00008-3-3 loss: 0.728662  [  128/  130]
train() client id: f_00008-4-0 loss: 0.668208  [   32/  130]
train() client id: f_00008-4-1 loss: 0.731540  [   64/  130]
train() client id: f_00008-4-2 loss: 0.714260  [   96/  130]
train() client id: f_00008-4-3 loss: 0.898684  [  128/  130]
train() client id: f_00008-5-0 loss: 0.718553  [   32/  130]
train() client id: f_00008-5-1 loss: 0.767056  [   64/  130]
train() client id: f_00008-5-2 loss: 0.736894  [   96/  130]
train() client id: f_00008-5-3 loss: 0.794058  [  128/  130]
train() client id: f_00008-6-0 loss: 0.880108  [   32/  130]
train() client id: f_00008-6-1 loss: 0.728397  [   64/  130]
train() client id: f_00008-6-2 loss: 0.748227  [   96/  130]
train() client id: f_00008-6-3 loss: 0.642100  [  128/  130]
train() client id: f_00008-7-0 loss: 0.647591  [   32/  130]
train() client id: f_00008-7-1 loss: 0.770102  [   64/  130]
train() client id: f_00008-7-2 loss: 0.852010  [   96/  130]
train() client id: f_00008-7-3 loss: 0.737355  [  128/  130]
train() client id: f_00008-8-0 loss: 0.779543  [   32/  130]
train() client id: f_00008-8-1 loss: 0.674838  [   64/  130]
train() client id: f_00008-8-2 loss: 0.801964  [   96/  130]
train() client id: f_00008-8-3 loss: 0.733274  [  128/  130]
train() client id: f_00009-0-0 loss: 0.911348  [   32/  118]
train() client id: f_00009-0-1 loss: 0.673281  [   64/  118]
train() client id: f_00009-0-2 loss: 0.913668  [   96/  118]
train() client id: f_00009-1-0 loss: 0.725189  [   32/  118]
train() client id: f_00009-1-1 loss: 0.860612  [   64/  118]
train() client id: f_00009-1-2 loss: 0.820886  [   96/  118]
train() client id: f_00009-2-0 loss: 0.883347  [   32/  118]
train() client id: f_00009-2-1 loss: 0.887658  [   64/  118]
train() client id: f_00009-2-2 loss: 0.815015  [   96/  118]
train() client id: f_00009-3-0 loss: 0.754084  [   32/  118]
train() client id: f_00009-3-1 loss: 0.709313  [   64/  118]
train() client id: f_00009-3-2 loss: 0.897850  [   96/  118]
train() client id: f_00009-4-0 loss: 0.648202  [   32/  118]
train() client id: f_00009-4-1 loss: 0.878848  [   64/  118]
train() client id: f_00009-4-2 loss: 0.742158  [   96/  118]
train() client id: f_00009-5-0 loss: 0.944784  [   32/  118]
train() client id: f_00009-5-1 loss: 0.707965  [   64/  118]
train() client id: f_00009-5-2 loss: 0.745267  [   96/  118]
train() client id: f_00009-6-0 loss: 0.769188  [   32/  118]
train() client id: f_00009-6-1 loss: 0.674715  [   64/  118]
train() client id: f_00009-6-2 loss: 0.848723  [   96/  118]
train() client id: f_00009-7-0 loss: 0.655039  [   32/  118]
train() client id: f_00009-7-1 loss: 0.861197  [   64/  118]
train() client id: f_00009-7-2 loss: 0.871695  [   96/  118]
train() client id: f_00009-8-0 loss: 0.720466  [   32/  118]
train() client id: f_00009-8-1 loss: 0.797330  [   64/  118]
train() client id: f_00009-8-2 loss: 0.814944  [   96/  118]
At round 65 accuracy: 0.6419098143236074
At round 65 training accuracy: 0.5895372233400402
At round 65 training loss: 0.8244373370741515
update_location
xs = [  -3.9056584     4.20031788  345.00902392   18.81129433    0.97929623
    3.95640986 -307.44319194 -286.32485185  329.66397685 -272.06087855]
ys = [ 337.5879595   320.55583871    1.32061395 -307.45517586  299.35018685
  282.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [352.10919409 335.81793937 359.21159587 323.85575486 315.61288533
 299.99916173 323.30822262 303.28632848 344.94493416 289.88469   ]
dists_bs = [236.02567297 230.86631653 548.24621667 519.92866292 215.48011896
 208.92411955 221.5667713  206.71496584 528.64945606 196.62478831]
uav_gains = [1.17856502e-12 1.46410888e-12 1.08255453e-12 1.75503525e-12
 2.01172458e-12 2.67278145e-12 1.77049497e-12 2.51097274e-12
 1.29138642e-12 3.26345136e-12]
bs_gains = [2.50615657e-11 2.66614938e-11 2.36680394e-12 2.74569035e-12
 3.23410344e-11 3.52635523e-11 2.99144570e-11 3.63289386e-11
 2.62074203e-12 4.17933260e-11]
Round 66
-------------------------------
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.28261697 4.54685792 2.2554939  0.84667744 5.24112296 2.52309512
 1.03299146 3.13864151 2.30469629 2.04601254]
obj_prev = 26.218206122007363
eta_min = 1.468265776943342e-41	eta_max = 0.944257265961695
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 5.987676154852869	eta = 0.9090909090909091
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 15.480452461148737	eta = 0.3516267998379439
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 10.14509351030873	eta = 0.5365492149900848
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.229709829328627	eta = 0.5897630651031099
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.172211188857027	eta = 0.5934601642807859
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.17195028532138	eta = 0.5934770457345999
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.17195027990423	eta = 0.5934770460851201
eta = 0.5934770460851201
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [0.04305412 0.09055032 0.04237072 0.01469307 0.10456    0.0498881
 0.01845176 0.06116418 0.04442091 0.04032052]
ene_total = [0.9382438  1.35500986 0.94553393 0.48503672 1.54222126 0.79045116
 0.53538055 1.0793304  0.84532808 0.65541452]
ti_comp = [1.69339463 1.86443235 1.68124436 1.74028295 1.86800619 1.86951147
 1.74115692 1.77160865 1.77482088 1.87231038]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [1.73943290e-06 1.33492469e-05 1.68196045e-06 6.54603068e-08
 2.04748185e-05 2.22031557e-06 1.29514340e-07 4.55654761e-06
 1.73913543e-06 1.16869917e-06]
ene_total = [0.34787422 0.11349245 0.36453489 0.28355414 0.10868941 0.10637493
 0.28235657 0.24065936 0.23621585 0.10252241]
optimize_network_iter = 0 obj = 2.186274239551174
eta = 0.5934770460851201
freqs = [12712369.24332196 24283617.19415748 12600999.04046834  4221460.42224169
 27987058.04437446 13342550.5059045   5298707.90102926 17262328.31372014
 12514196.1057277  10767584.65528894]
eta_min = 0.5934770460851205	eta_max = 0.7588338965075888
af = 0.0005034964568992424	bf = 0.8885079592597985	zeta = 0.0005538461025891666	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [3.17745128e-07 2.43852935e-06 3.07246539e-07 1.19577441e-08
 3.74016947e-06 4.05588772e-07 2.36586018e-08 8.32352201e-07
 3.17690788e-07 2.13488239e-07]
ene_total = [1.55552023 0.5068411  1.6300255  1.26798066 0.48500598 0.47557109
 1.26262219 1.07594073 1.05621167 0.45839627]
ti_comp = [0.90140949 1.07244721 0.88925922 0.94829781 1.07602105 1.07752633
 0.94917178 0.97962351 0.98283574 1.08032524]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.08390094e-07 5.97022622e-06 8.89637979e-07 3.26227732e-08
 9.13119788e-06 9.89026201e-07 6.44906348e-08 2.20518843e-06
 8.39214901e-07 5.19447888e-07]
ene_total = [0.58637686 0.1911386  0.6144619  0.47797382 0.18295069 0.17928302
 0.47595438 0.40561441 0.39815774 0.17280245]
optimize_network_iter = 1 obj = 3.6847138773635306
eta = 0.7588338965075888
freqs = [12631650.06371988 22329634.8077465  12600999.04046835  4097654.84679903
 25698769.41757561 12244375.3475877   5141153.96101874 16512221.31849963
 11952921.87158274  9870499.99002256]
eta_min = 0.7588338965075906	eta_max = 0.7588338965075868
af = 0.00043529367837054833	bf = 0.8885079592597985	zeta = 0.0004788230462076032	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [3.13722794e-07 2.06188515e-06 3.07246539e-07 1.12666437e-08
 3.15356247e-06 3.41571386e-07 2.22725702e-08 7.61586768e-07
 2.89832359e-07 1.79397204e-07]
ene_total = [1.55551998 0.506818   1.6300255  1.26798062 0.48497001 0.47556716
 1.2626221  1.0759364  1.05620996 0.45839418]
ti_comp = [0.90140949 1.07244721 0.88925922 0.94829781 1.07602105 1.07752633
 0.94917178 0.97962351 0.98283574 1.08032524]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.08390094e-07 5.97022622e-06 8.89637979e-07 3.26227732e-08
 9.13119788e-06 9.89026201e-07 6.44906348e-08 2.20518843e-06
 8.39214901e-07 5.19447888e-07]
ene_total = [0.58637686 0.1911386  0.6144619  0.47797382 0.18295069 0.17928302
 0.47595438 0.40561441 0.39815774 0.17280245]
optimize_network_iter = 2 obj = 3.6847138773635004
eta = 0.7588338965075868
freqs = [12631650.06371987 22329634.80774652 12600999.04046834  4097654.84679903
 25698769.41757564 12244375.34758771  5141153.96101873 16512221.31849963
 11952921.87158274  9870499.99002257]
Done!
At round 66 eta: 0.7588338965075868
At round 66 local rounds: 9.036737525084137
At round 66 global rounds: 23.115093189100694
At round 66 a_n: 5.2320311093101175
gradient difference: 0.5156475901603699
train() client id: f_00000-0-0 loss: 1.355680  [   32/  126]
train() client id: f_00000-0-1 loss: 0.958063  [   64/  126]
train() client id: f_00000-0-2 loss: 0.968386  [   96/  126]
train() client id: f_00000-1-0 loss: 1.219229  [   32/  126]
train() client id: f_00000-1-1 loss: 0.923774  [   64/  126]
train() client id: f_00000-1-2 loss: 0.969628  [   96/  126]
train() client id: f_00000-2-0 loss: 1.119850  [   32/  126]
train() client id: f_00000-2-1 loss: 0.926227  [   64/  126]
train() client id: f_00000-2-2 loss: 0.902851  [   96/  126]
train() client id: f_00000-3-0 loss: 1.030430  [   32/  126]
train() client id: f_00000-3-1 loss: 0.813779  [   64/  126]
train() client id: f_00000-3-2 loss: 0.987552  [   96/  126]
train() client id: f_00000-4-0 loss: 0.869825  [   32/  126]
train() client id: f_00000-4-1 loss: 0.929194  [   64/  126]
train() client id: f_00000-4-2 loss: 0.831417  [   96/  126]
train() client id: f_00000-5-0 loss: 0.979104  [   32/  126]
train() client id: f_00000-5-1 loss: 0.866759  [   64/  126]
train() client id: f_00000-5-2 loss: 0.911084  [   96/  126]
train() client id: f_00000-6-0 loss: 0.791201  [   32/  126]
train() client id: f_00000-6-1 loss: 0.898727  [   64/  126]
train() client id: f_00000-6-2 loss: 0.961514  [   96/  126]
train() client id: f_00000-7-0 loss: 0.731497  [   32/  126]
train() client id: f_00000-7-1 loss: 1.090367  [   64/  126]
train() client id: f_00000-7-2 loss: 0.821302  [   96/  126]
train() client id: f_00000-8-0 loss: 0.932637  [   32/  126]
train() client id: f_00000-8-1 loss: 0.781204  [   64/  126]
train() client id: f_00000-8-2 loss: 0.937453  [   96/  126]
train() client id: f_00001-0-0 loss: 0.515812  [   32/  265]
train() client id: f_00001-0-1 loss: 0.384247  [   64/  265]
train() client id: f_00001-0-2 loss: 0.535271  [   96/  265]
train() client id: f_00001-0-3 loss: 0.440771  [  128/  265]
train() client id: f_00001-0-4 loss: 0.406751  [  160/  265]
train() client id: f_00001-0-5 loss: 0.452387  [  192/  265]
train() client id: f_00001-0-6 loss: 0.413923  [  224/  265]
train() client id: f_00001-0-7 loss: 0.525908  [  256/  265]
train() client id: f_00001-1-0 loss: 0.496507  [   32/  265]
train() client id: f_00001-1-1 loss: 0.493822  [   64/  265]
train() client id: f_00001-1-2 loss: 0.385645  [   96/  265]
train() client id: f_00001-1-3 loss: 0.545414  [  128/  265]
train() client id: f_00001-1-4 loss: 0.463608  [  160/  265]
train() client id: f_00001-1-5 loss: 0.408189  [  192/  265]
train() client id: f_00001-1-6 loss: 0.359269  [  224/  265]
train() client id: f_00001-1-7 loss: 0.468235  [  256/  265]
train() client id: f_00001-2-0 loss: 0.444929  [   32/  265]
train() client id: f_00001-2-1 loss: 0.499297  [   64/  265]
train() client id: f_00001-2-2 loss: 0.532301  [   96/  265]
train() client id: f_00001-2-3 loss: 0.346483  [  128/  265]
train() client id: f_00001-2-4 loss: 0.366612  [  160/  265]
train() client id: f_00001-2-5 loss: 0.370328  [  192/  265]
train() client id: f_00001-2-6 loss: 0.499649  [  224/  265]
train() client id: f_00001-2-7 loss: 0.528973  [  256/  265]
train() client id: f_00001-3-0 loss: 0.424733  [   32/  265]
train() client id: f_00001-3-1 loss: 0.360018  [   64/  265]
train() client id: f_00001-3-2 loss: 0.460064  [   96/  265]
train() client id: f_00001-3-3 loss: 0.347853  [  128/  265]
train() client id: f_00001-3-4 loss: 0.484074  [  160/  265]
train() client id: f_00001-3-5 loss: 0.428104  [  192/  265]
train() client id: f_00001-3-6 loss: 0.462907  [  224/  265]
train() client id: f_00001-3-7 loss: 0.494952  [  256/  265]
train() client id: f_00001-4-0 loss: 0.351871  [   32/  265]
train() client id: f_00001-4-1 loss: 0.340067  [   64/  265]
train() client id: f_00001-4-2 loss: 0.468664  [   96/  265]
train() client id: f_00001-4-3 loss: 0.391392  [  128/  265]
train() client id: f_00001-4-4 loss: 0.586896  [  160/  265]
train() client id: f_00001-4-5 loss: 0.435255  [  192/  265]
train() client id: f_00001-4-6 loss: 0.426473  [  224/  265]
train() client id: f_00001-4-7 loss: 0.530379  [  256/  265]
train() client id: f_00001-5-0 loss: 0.461288  [   32/  265]
train() client id: f_00001-5-1 loss: 0.420579  [   64/  265]
train() client id: f_00001-5-2 loss: 0.499440  [   96/  265]
train() client id: f_00001-5-3 loss: 0.348460  [  128/  265]
train() client id: f_00001-5-4 loss: 0.428676  [  160/  265]
train() client id: f_00001-5-5 loss: 0.439046  [  192/  265]
train() client id: f_00001-5-6 loss: 0.357378  [  224/  265]
train() client id: f_00001-5-7 loss: 0.537814  [  256/  265]
train() client id: f_00001-6-0 loss: 0.372814  [   32/  265]
train() client id: f_00001-6-1 loss: 0.395001  [   64/  265]
train() client id: f_00001-6-2 loss: 0.426451  [   96/  265]
train() client id: f_00001-6-3 loss: 0.446471  [  128/  265]
train() client id: f_00001-6-4 loss: 0.579536  [  160/  265]
train() client id: f_00001-6-5 loss: 0.396964  [  192/  265]
train() client id: f_00001-6-6 loss: 0.394162  [  224/  265]
train() client id: f_00001-6-7 loss: 0.457309  [  256/  265]
train() client id: f_00001-7-0 loss: 0.340022  [   32/  265]
train() client id: f_00001-7-1 loss: 0.442625  [   64/  265]
train() client id: f_00001-7-2 loss: 0.497029  [   96/  265]
train() client id: f_00001-7-3 loss: 0.429076  [  128/  265]
train() client id: f_00001-7-4 loss: 0.349091  [  160/  265]
train() client id: f_00001-7-5 loss: 0.517334  [  192/  265]
train() client id: f_00001-7-6 loss: 0.409430  [  224/  265]
train() client id: f_00001-7-7 loss: 0.447814  [  256/  265]
train() client id: f_00001-8-0 loss: 0.501128  [   32/  265]
train() client id: f_00001-8-1 loss: 0.473259  [   64/  265]
train() client id: f_00001-8-2 loss: 0.352561  [   96/  265]
train() client id: f_00001-8-3 loss: 0.433802  [  128/  265]
train() client id: f_00001-8-4 loss: 0.421288  [  160/  265]
train() client id: f_00001-8-5 loss: 0.467229  [  192/  265]
train() client id: f_00001-8-6 loss: 0.459253  [  224/  265]
train() client id: f_00001-8-7 loss: 0.316457  [  256/  265]
train() client id: f_00002-0-0 loss: 1.295728  [   32/  124]
train() client id: f_00002-0-1 loss: 1.075881  [   64/  124]
train() client id: f_00002-0-2 loss: 1.078378  [   96/  124]
train() client id: f_00002-1-0 loss: 1.029077  [   32/  124]
train() client id: f_00002-1-1 loss: 1.268794  [   64/  124]
train() client id: f_00002-1-2 loss: 0.954869  [   96/  124]
train() client id: f_00002-2-0 loss: 1.061950  [   32/  124]
train() client id: f_00002-2-1 loss: 1.026304  [   64/  124]
train() client id: f_00002-2-2 loss: 1.258498  [   96/  124]
train() client id: f_00002-3-0 loss: 1.200910  [   32/  124]
train() client id: f_00002-3-1 loss: 1.025277  [   64/  124]
train() client id: f_00002-3-2 loss: 1.143201  [   96/  124]
train() client id: f_00002-4-0 loss: 1.086599  [   32/  124]
train() client id: f_00002-4-1 loss: 1.111331  [   64/  124]
train() client id: f_00002-4-2 loss: 0.947643  [   96/  124]
train() client id: f_00002-5-0 loss: 0.994897  [   32/  124]
train() client id: f_00002-5-1 loss: 0.997647  [   64/  124]
train() client id: f_00002-5-2 loss: 1.070140  [   96/  124]
train() client id: f_00002-6-0 loss: 0.836600  [   32/  124]
train() client id: f_00002-6-1 loss: 1.030190  [   64/  124]
train() client id: f_00002-6-2 loss: 1.007677  [   96/  124]
train() client id: f_00002-7-0 loss: 0.979416  [   32/  124]
train() client id: f_00002-7-1 loss: 1.053159  [   64/  124]
train() client id: f_00002-7-2 loss: 0.930098  [   96/  124]
train() client id: f_00002-8-0 loss: 0.991611  [   32/  124]
train() client id: f_00002-8-1 loss: 1.175513  [   64/  124]
train() client id: f_00002-8-2 loss: 0.921002  [   96/  124]
train() client id: f_00003-0-0 loss: 0.550441  [   32/   43]
train() client id: f_00003-1-0 loss: 0.764915  [   32/   43]
train() client id: f_00003-2-0 loss: 0.639366  [   32/   43]
train() client id: f_00003-3-0 loss: 0.913338  [   32/   43]
train() client id: f_00003-4-0 loss: 0.465364  [   32/   43]
train() client id: f_00003-5-0 loss: 0.872961  [   32/   43]
train() client id: f_00003-6-0 loss: 0.750086  [   32/   43]
train() client id: f_00003-7-0 loss: 0.689930  [   32/   43]
train() client id: f_00003-8-0 loss: 0.641140  [   32/   43]
train() client id: f_00004-0-0 loss: 0.698213  [   32/  306]
train() client id: f_00004-0-1 loss: 0.797365  [   64/  306]
train() client id: f_00004-0-2 loss: 0.824612  [   96/  306]
train() client id: f_00004-0-3 loss: 0.692091  [  128/  306]
train() client id: f_00004-0-4 loss: 0.742211  [  160/  306]
train() client id: f_00004-0-5 loss: 0.922502  [  192/  306]
train() client id: f_00004-0-6 loss: 0.858464  [  224/  306]
train() client id: f_00004-0-7 loss: 0.953800  [  256/  306]
train() client id: f_00004-0-8 loss: 0.938752  [  288/  306]
train() client id: f_00004-1-0 loss: 0.820727  [   32/  306]
train() client id: f_00004-1-1 loss: 0.809943  [   64/  306]
train() client id: f_00004-1-2 loss: 0.578615  [   96/  306]
train() client id: f_00004-1-3 loss: 1.033990  [  128/  306]
train() client id: f_00004-1-4 loss: 0.842387  [  160/  306]
train() client id: f_00004-1-5 loss: 0.850317  [  192/  306]
train() client id: f_00004-1-6 loss: 0.889271  [  224/  306]
train() client id: f_00004-1-7 loss: 0.798203  [  256/  306]
train() client id: f_00004-1-8 loss: 0.880046  [  288/  306]
train() client id: f_00004-2-0 loss: 0.899899  [   32/  306]
train() client id: f_00004-2-1 loss: 0.791794  [   64/  306]
train() client id: f_00004-2-2 loss: 0.842609  [   96/  306]
train() client id: f_00004-2-3 loss: 0.936533  [  128/  306]
train() client id: f_00004-2-4 loss: 0.670471  [  160/  306]
train() client id: f_00004-2-5 loss: 0.831743  [  192/  306]
train() client id: f_00004-2-6 loss: 0.766844  [  224/  306]
train() client id: f_00004-2-7 loss: 0.845521  [  256/  306]
train() client id: f_00004-2-8 loss: 0.997684  [  288/  306]
train() client id: f_00004-3-0 loss: 0.923035  [   32/  306]
train() client id: f_00004-3-1 loss: 0.872296  [   64/  306]
train() client id: f_00004-3-2 loss: 0.833142  [   96/  306]
train() client id: f_00004-3-3 loss: 0.804060  [  128/  306]
train() client id: f_00004-3-4 loss: 0.858810  [  160/  306]
train() client id: f_00004-3-5 loss: 0.711692  [  192/  306]
train() client id: f_00004-3-6 loss: 0.843208  [  224/  306]
train() client id: f_00004-3-7 loss: 0.814960  [  256/  306]
train() client id: f_00004-3-8 loss: 0.861753  [  288/  306]
train() client id: f_00004-4-0 loss: 0.793627  [   32/  306]
train() client id: f_00004-4-1 loss: 0.927682  [   64/  306]
train() client id: f_00004-4-2 loss: 0.611882  [   96/  306]
train() client id: f_00004-4-3 loss: 0.849101  [  128/  306]
train() client id: f_00004-4-4 loss: 0.822317  [  160/  306]
train() client id: f_00004-4-5 loss: 0.826796  [  192/  306]
train() client id: f_00004-4-6 loss: 0.793020  [  224/  306]
train() client id: f_00004-4-7 loss: 0.839262  [  256/  306]
train() client id: f_00004-4-8 loss: 0.989649  [  288/  306]
train() client id: f_00004-5-0 loss: 0.839810  [   32/  306]
train() client id: f_00004-5-1 loss: 0.815995  [   64/  306]
train() client id: f_00004-5-2 loss: 0.735120  [   96/  306]
train() client id: f_00004-5-3 loss: 0.901370  [  128/  306]
train() client id: f_00004-5-4 loss: 0.778836  [  160/  306]
train() client id: f_00004-5-5 loss: 0.934345  [  192/  306]
train() client id: f_00004-5-6 loss: 0.862685  [  224/  306]
train() client id: f_00004-5-7 loss: 0.811274  [  256/  306]
train() client id: f_00004-5-8 loss: 0.794707  [  288/  306]
train() client id: f_00004-6-0 loss: 0.756115  [   32/  306]
train() client id: f_00004-6-1 loss: 0.883139  [   64/  306]
train() client id: f_00004-6-2 loss: 0.892674  [   96/  306]
train() client id: f_00004-6-3 loss: 0.760530  [  128/  306]
train() client id: f_00004-6-4 loss: 0.880444  [  160/  306]
train() client id: f_00004-6-5 loss: 0.713443  [  192/  306]
train() client id: f_00004-6-6 loss: 0.929521  [  224/  306]
train() client id: f_00004-6-7 loss: 0.784110  [  256/  306]
train() client id: f_00004-6-8 loss: 0.834886  [  288/  306]
train() client id: f_00004-7-0 loss: 0.812792  [   32/  306]
train() client id: f_00004-7-1 loss: 0.790652  [   64/  306]
train() client id: f_00004-7-2 loss: 0.820722  [   96/  306]
train() client id: f_00004-7-3 loss: 0.874218  [  128/  306]
train() client id: f_00004-7-4 loss: 0.792187  [  160/  306]
train() client id: f_00004-7-5 loss: 0.968921  [  192/  306]
train() client id: f_00004-7-6 loss: 0.923052  [  224/  306]
train() client id: f_00004-7-7 loss: 0.805361  [  256/  306]
train() client id: f_00004-7-8 loss: 0.796070  [  288/  306]
train() client id: f_00004-8-0 loss: 0.798726  [   32/  306]
train() client id: f_00004-8-1 loss: 1.016836  [   64/  306]
train() client id: f_00004-8-2 loss: 0.899168  [   96/  306]
train() client id: f_00004-8-3 loss: 0.637753  [  128/  306]
train() client id: f_00004-8-4 loss: 0.765381  [  160/  306]
train() client id: f_00004-8-5 loss: 0.855627  [  192/  306]
train() client id: f_00004-8-6 loss: 0.826077  [  224/  306]
train() client id: f_00004-8-7 loss: 0.832628  [  256/  306]
train() client id: f_00004-8-8 loss: 0.861723  [  288/  306]
train() client id: f_00005-0-0 loss: 0.580801  [   32/  146]
train() client id: f_00005-0-1 loss: 0.516197  [   64/  146]
train() client id: f_00005-0-2 loss: 0.489842  [   96/  146]
train() client id: f_00005-0-3 loss: 0.597713  [  128/  146]
train() client id: f_00005-1-0 loss: 0.633441  [   32/  146]
train() client id: f_00005-1-1 loss: 0.630819  [   64/  146]
train() client id: f_00005-1-2 loss: 0.517677  [   96/  146]
train() client id: f_00005-1-3 loss: 0.566895  [  128/  146]
train() client id: f_00005-2-0 loss: 0.863295  [   32/  146]
train() client id: f_00005-2-1 loss: 0.547462  [   64/  146]
train() client id: f_00005-2-2 loss: 0.408310  [   96/  146]
train() client id: f_00005-2-3 loss: 0.551444  [  128/  146]
train() client id: f_00005-3-0 loss: 0.644297  [   32/  146]
train() client id: f_00005-3-1 loss: 0.927059  [   64/  146]
train() client id: f_00005-3-2 loss: 0.284298  [   96/  146]
train() client id: f_00005-3-3 loss: 0.503485  [  128/  146]
train() client id: f_00005-4-0 loss: 0.424263  [   32/  146]
train() client id: f_00005-4-1 loss: 0.720193  [   64/  146]
train() client id: f_00005-4-2 loss: 0.413473  [   96/  146]
train() client id: f_00005-4-3 loss: 0.348109  [  128/  146]
train() client id: f_00005-5-0 loss: 0.354469  [   32/  146]
train() client id: f_00005-5-1 loss: 0.585799  [   64/  146]
train() client id: f_00005-5-2 loss: 0.336677  [   96/  146]
train() client id: f_00005-5-3 loss: 0.870422  [  128/  146]
train() client id: f_00005-6-0 loss: 0.388383  [   32/  146]
train() client id: f_00005-6-1 loss: 0.610528  [   64/  146]
train() client id: f_00005-6-2 loss: 0.739710  [   96/  146]
train() client id: f_00005-6-3 loss: 0.489901  [  128/  146]
train() client id: f_00005-7-0 loss: 0.714665  [   32/  146]
train() client id: f_00005-7-1 loss: 0.379294  [   64/  146]
train() client id: f_00005-7-2 loss: 0.621149  [   96/  146]
train() client id: f_00005-7-3 loss: 0.534561  [  128/  146]
train() client id: f_00005-8-0 loss: 0.476917  [   32/  146]
train() client id: f_00005-8-1 loss: 0.345044  [   64/  146]
train() client id: f_00005-8-2 loss: 0.355077  [   96/  146]
train() client id: f_00005-8-3 loss: 0.655959  [  128/  146]
train() client id: f_00006-0-0 loss: 0.470197  [   32/   54]
train() client id: f_00006-1-0 loss: 0.455918  [   32/   54]
train() client id: f_00006-2-0 loss: 0.413108  [   32/   54]
train() client id: f_00006-3-0 loss: 0.475225  [   32/   54]
train() client id: f_00006-4-0 loss: 0.434105  [   32/   54]
train() client id: f_00006-5-0 loss: 0.422888  [   32/   54]
train() client id: f_00006-6-0 loss: 0.467055  [   32/   54]
train() client id: f_00006-7-0 loss: 0.534891  [   32/   54]
train() client id: f_00006-8-0 loss: 0.449996  [   32/   54]
train() client id: f_00007-0-0 loss: 0.625290  [   32/  179]
train() client id: f_00007-0-1 loss: 0.650552  [   64/  179]
train() client id: f_00007-0-2 loss: 0.481746  [   96/  179]
train() client id: f_00007-0-3 loss: 0.628620  [  128/  179]
train() client id: f_00007-0-4 loss: 0.608081  [  160/  179]
train() client id: f_00007-1-0 loss: 0.400395  [   32/  179]
train() client id: f_00007-1-1 loss: 0.590498  [   64/  179]
train() client id: f_00007-1-2 loss: 0.608959  [   96/  179]
train() client id: f_00007-1-3 loss: 0.466193  [  128/  179]
train() client id: f_00007-1-4 loss: 0.567078  [  160/  179]
train() client id: f_00007-2-0 loss: 0.639174  [   32/  179]
train() client id: f_00007-2-1 loss: 0.489460  [   64/  179]
train() client id: f_00007-2-2 loss: 0.612418  [   96/  179]
train() client id: f_00007-2-3 loss: 0.385006  [  128/  179]
train() client id: f_00007-2-4 loss: 0.701048  [  160/  179]
train() client id: f_00007-3-0 loss: 0.410274  [   32/  179]
train() client id: f_00007-3-1 loss: 0.687495  [   64/  179]
train() client id: f_00007-3-2 loss: 0.635702  [   96/  179]
train() client id: f_00007-3-3 loss: 0.413541  [  128/  179]
train() client id: f_00007-3-4 loss: 0.571267  [  160/  179]
train() client id: f_00007-4-0 loss: 0.486756  [   32/  179]
train() client id: f_00007-4-1 loss: 0.470463  [   64/  179]
train() client id: f_00007-4-2 loss: 0.601637  [   96/  179]
train() client id: f_00007-4-3 loss: 0.411556  [  128/  179]
train() client id: f_00007-4-4 loss: 0.800831  [  160/  179]
train() client id: f_00007-5-0 loss: 0.737894  [   32/  179]
train() client id: f_00007-5-1 loss: 0.403783  [   64/  179]
train() client id: f_00007-5-2 loss: 0.504285  [   96/  179]
train() client id: f_00007-5-3 loss: 0.511441  [  128/  179]
train() client id: f_00007-5-4 loss: 0.587979  [  160/  179]
train() client id: f_00007-6-0 loss: 0.474561  [   32/  179]
train() client id: f_00007-6-1 loss: 0.522301  [   64/  179]
train() client id: f_00007-6-2 loss: 0.623839  [   96/  179]
train() client id: f_00007-6-3 loss: 0.464638  [  128/  179]
train() client id: f_00007-6-4 loss: 0.647018  [  160/  179]
train() client id: f_00007-7-0 loss: 0.491586  [   32/  179]
train() client id: f_00007-7-1 loss: 0.597670  [   64/  179]
train() client id: f_00007-7-2 loss: 0.468197  [   96/  179]
train() client id: f_00007-7-3 loss: 0.368752  [  128/  179]
train() client id: f_00007-7-4 loss: 0.699578  [  160/  179]
train() client id: f_00007-8-0 loss: 0.608714  [   32/  179]
train() client id: f_00007-8-1 loss: 0.336068  [   64/  179]
train() client id: f_00007-8-2 loss: 0.489161  [   96/  179]
train() client id: f_00007-8-3 loss: 0.354808  [  128/  179]
train() client id: f_00007-8-4 loss: 0.778078  [  160/  179]
train() client id: f_00008-0-0 loss: 0.787410  [   32/  130]
train() client id: f_00008-0-1 loss: 0.746353  [   64/  130]
train() client id: f_00008-0-2 loss: 0.805746  [   96/  130]
train() client id: f_00008-0-3 loss: 0.630047  [  128/  130]
train() client id: f_00008-1-0 loss: 0.753089  [   32/  130]
train() client id: f_00008-1-1 loss: 0.752098  [   64/  130]
train() client id: f_00008-1-2 loss: 0.691062  [   96/  130]
train() client id: f_00008-1-3 loss: 0.741793  [  128/  130]
train() client id: f_00008-2-0 loss: 0.600712  [   32/  130]
train() client id: f_00008-2-1 loss: 0.796751  [   64/  130]
train() client id: f_00008-2-2 loss: 0.792283  [   96/  130]
train() client id: f_00008-2-3 loss: 0.759564  [  128/  130]
train() client id: f_00008-3-0 loss: 0.788032  [   32/  130]
train() client id: f_00008-3-1 loss: 0.796514  [   64/  130]
train() client id: f_00008-3-2 loss: 0.618046  [   96/  130]
train() client id: f_00008-3-3 loss: 0.744482  [  128/  130]
train() client id: f_00008-4-0 loss: 0.907858  [   32/  130]
train() client id: f_00008-4-1 loss: 0.644334  [   64/  130]
train() client id: f_00008-4-2 loss: 0.619917  [   96/  130]
train() client id: f_00008-4-3 loss: 0.789143  [  128/  130]
train() client id: f_00008-5-0 loss: 0.691957  [   32/  130]
train() client id: f_00008-5-1 loss: 0.879869  [   64/  130]
train() client id: f_00008-5-2 loss: 0.726245  [   96/  130]
train() client id: f_00008-5-3 loss: 0.653991  [  128/  130]
train() client id: f_00008-6-0 loss: 0.806474  [   32/  130]
train() client id: f_00008-6-1 loss: 0.765883  [   64/  130]
train() client id: f_00008-6-2 loss: 0.668575  [   96/  130]
train() client id: f_00008-6-3 loss: 0.715858  [  128/  130]
train() client id: f_00008-7-0 loss: 0.719341  [   32/  130]
train() client id: f_00008-7-1 loss: 0.719325  [   64/  130]
train() client id: f_00008-7-2 loss: 0.788488  [   96/  130]
train() client id: f_00008-7-3 loss: 0.717318  [  128/  130]
train() client id: f_00008-8-0 loss: 0.789298  [   32/  130]
train() client id: f_00008-8-1 loss: 0.806101  [   64/  130]
train() client id: f_00008-8-2 loss: 0.706130  [   96/  130]
train() client id: f_00008-8-3 loss: 0.651614  [  128/  130]
train() client id: f_00009-0-0 loss: 0.852313  [   32/  118]
train() client id: f_00009-0-1 loss: 0.770293  [   64/  118]
train() client id: f_00009-0-2 loss: 1.006909  [   96/  118]
train() client id: f_00009-1-0 loss: 1.018610  [   32/  118]
train() client id: f_00009-1-1 loss: 0.842651  [   64/  118]
train() client id: f_00009-1-2 loss: 0.834497  [   96/  118]
train() client id: f_00009-2-0 loss: 0.871934  [   32/  118]
train() client id: f_00009-2-1 loss: 0.886681  [   64/  118]
train() client id: f_00009-2-2 loss: 0.840275  [   96/  118]
train() client id: f_00009-3-0 loss: 0.840972  [   32/  118]
train() client id: f_00009-3-1 loss: 0.773748  [   64/  118]
train() client id: f_00009-3-2 loss: 0.759229  [   96/  118]
train() client id: f_00009-4-0 loss: 0.922567  [   32/  118]
train() client id: f_00009-4-1 loss: 0.807457  [   64/  118]
train() client id: f_00009-4-2 loss: 0.734425  [   96/  118]
train() client id: f_00009-5-0 loss: 0.874723  [   32/  118]
train() client id: f_00009-5-1 loss: 0.880213  [   64/  118]
train() client id: f_00009-5-2 loss: 0.715962  [   96/  118]
train() client id: f_00009-6-0 loss: 0.728236  [   32/  118]
train() client id: f_00009-6-1 loss: 0.759390  [   64/  118]
train() client id: f_00009-6-2 loss: 0.730123  [   96/  118]
train() client id: f_00009-7-0 loss: 0.804217  [   32/  118]
train() client id: f_00009-7-1 loss: 0.861167  [   64/  118]
train() client id: f_00009-7-2 loss: 0.751418  [   96/  118]
train() client id: f_00009-8-0 loss: 0.646445  [   32/  118]
train() client id: f_00009-8-1 loss: 0.879838  [   64/  118]
train() client id: f_00009-8-2 loss: 0.730536  [   96/  118]
At round 66 accuracy: 0.6419098143236074
At round 66 training accuracy: 0.5875251509054326
At round 66 training loss: 0.8076447420094555
update_location
xs = [  -3.9056584     4.20031788  350.00902392   18.81129433    0.97929623
    3.95640986 -312.44319194 -291.32485185  334.66397685 -277.06087855]
ys = [ 342.5879595   325.55583871    1.32061395 -312.45517586  304.35018685
  287.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [356.90581973 340.59396176 364.01656672 328.606302   320.35916602
 304.71730923 328.06651571 308.01111272 349.72653227 294.58231834]
dists_bs = [239.49738599 234.05088014 552.99096225 524.57160263 218.40371686
 211.54769968 224.59133123 209.45005519 533.42467806 199.14044332]
uav_gains = [1.11218118e-12 1.36922033e-12 1.02508951e-12 1.62931179e-12
 1.85748367e-12 2.44465100e-12 1.64287124e-12 2.30088160e-12
 1.21414591e-12 2.97056458e-12]
bs_gains = [2.40575783e-11 2.56581480e-11 2.31038093e-12 2.67818585e-12
 3.11433994e-11 3.40526430e-11 2.88000814e-11 3.50161782e-11
 2.55557949e-12 4.03317968e-11]
Round 67
-------------------------------
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.14668611 4.26785824 2.12126966 0.79869584 4.91942362 2.36837405
 0.97356032 2.94941557 2.16407019 1.92059068]
obj_prev = 24.629944275071566
eta_min = 3.445448803028318e-44	eta_max = 0.9459678027745403
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 5.6197462444887005	eta = 0.9090909090909091
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 14.777404826319295	eta = 0.34572107093955495
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 9.602497695143875	eta = 0.5320345168992939
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.719456153128892	eta = 0.585915008062652
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.663852506476184	eta = 0.5896753457475884
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.663598265580141	eta = 0.5896926502882287
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.66359826021894	eta = 0.5896926506531418
eta = 0.5896926506531418
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [0.04358199 0.09166053 0.04289021 0.01487322 0.10584197 0.05049976
 0.01867799 0.06191409 0.04496554 0.04081488]
ene_total = [0.88965002 1.27512756 0.89641868 0.46309031 1.45131187 0.74360323
 0.51049018 1.02200221 0.79540655 0.61649765]
ti_comp = [1.82806619 2.00655166 1.81584028 1.87549598 2.01019846 2.01177704
 1.87637173 1.90759375 1.91580954 2.01460717]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [1.54816501e-06 1.19543613e-05 1.49554405e-06 5.84605015e-08
 1.83390085e-05 1.98878720e-06 1.15673257e-07 4.07639963e-06
 1.54815642e-06 1.04701937e-06]
ene_total = [0.33393439 0.10647147 0.34952357 0.27343533 0.10190267 0.09968125
 0.27231935 0.23255711 0.22204852 0.09606041]
optimize_network_iter = 0 obj = 2.0879340736516
eta = 0.5896926506531418
freqs = [11920243.00940963 22840310.78845544 11810016.9072595   3965142.48672103
 26326248.17150466 12551033.48678927  4977157.22105609 16228322.10506473
 11735389.30408466 10129735.66511689]
eta_min = 0.589692650653142	eta_max = 0.7681327803953306
af = 0.00041776276143721885	bf = 0.8564740523773019	zeta = 0.00045953903758094077	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [2.79380519e-07 2.15727370e-06 2.69884586e-07 1.05497316e-08
 3.30944163e-06 3.58894819e-07 2.08742959e-08 7.35623554e-07
 2.79378970e-07 1.88944211e-07]
ene_total = [1.50710005 0.47998245 1.57746246 1.23411502 0.45906057 0.44980564
 1.22907549 1.04942654 1.00211652 0.43350782]
ti_comp = [0.91916882 1.09765429 0.90694292 0.96659861 1.10130109 1.10287968
 0.96747436 0.99869639 1.00691217 1.1057098 ]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.62987513e-07 4.97740325e-06 7.46967171e-07 2.74226484e-08
 7.61287159e-06 8.24515199e-07 5.42124102e-08 1.85305581e-06
 6.98301926e-07 4.33071962e-07]
ene_total = [0.59090555 0.18825224 0.61849267 0.4838647  0.18008278 0.17636757
 0.4818892  0.411478   0.39291321 0.16997262]
optimize_network_iter = 1 obj = 3.694218537284266
eta = 0.7681327803953306
freqs = [11840881.82931954 20853986.15252445 11810016.9072595   3842652.168879
 24000712.97449805 11434929.97175277  4821288.08222465 15482045.45566182
 11152199.96866208  9218274.46340382]
eta_min = 0.7681327803953335	eta_max = 0.7681327803953281
af = 0.0003568274848523184	bf = 0.8564740523773019	zeta = 0.00039251023333755023	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [2.75672850e-07 1.79837142e-06 2.69884586e-07 9.90799918e-09
 2.75058500e-06 2.97903243e-07 1.95873319e-08 6.69522327e-07
 2.52301484e-07 1.56472000e-07]
ene_total = [1.50709984 0.47996179 1.57746246 1.23411499 0.4590284  0.44980213
 1.22907542 1.04942273 1.00211496 0.43350595]
ti_comp = [0.91916882 1.09765429 0.90694292 0.96659861 1.10130109 1.10287968
 0.96747436 0.99869639 1.00691217 1.1057098 ]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.62987513e-07 4.97740325e-06 7.46967171e-07 2.74226484e-08
 7.61287159e-06 8.24515199e-07 5.42124102e-08 1.85305581e-06
 6.98301926e-07 4.33071962e-07]
ene_total = [0.59090555 0.18825224 0.61849267 0.4838647  0.18008278 0.17636757
 0.4818892  0.411478   0.39291321 0.16997262]
optimize_network_iter = 2 obj = 3.6942185372842253
eta = 0.7681327803953281
freqs = [11840881.82931953 20853986.15252446 11810016.90725948  3842652.168879
 24000712.97449807 11434929.97175278  4821288.08222465 15482045.45566182
 11152199.96866208  9218274.46340384]
Done!
At round 67 eta: 0.7681327803953281
At round 67 local rounds: 8.637912231455728
At round 67 global rounds: 22.564772710133866
At round 67 a_n: 4.8894852623407985
gradient difference: 0.6677201986312866
train() client id: f_00000-0-0 loss: 1.098839  [   32/  126]
train() client id: f_00000-0-1 loss: 1.116991  [   64/  126]
train() client id: f_00000-0-2 loss: 0.825425  [   96/  126]
train() client id: f_00000-1-0 loss: 0.939197  [   32/  126]
train() client id: f_00000-1-1 loss: 0.909260  [   64/  126]
train() client id: f_00000-1-2 loss: 0.960652  [   96/  126]
train() client id: f_00000-2-0 loss: 0.994451  [   32/  126]
train() client id: f_00000-2-1 loss: 1.047995  [   64/  126]
train() client id: f_00000-2-2 loss: 0.675906  [   96/  126]
train() client id: f_00000-3-0 loss: 0.870304  [   32/  126]
train() client id: f_00000-3-1 loss: 0.744773  [   64/  126]
train() client id: f_00000-3-2 loss: 0.819202  [   96/  126]
train() client id: f_00000-4-0 loss: 0.865557  [   32/  126]
train() client id: f_00000-4-1 loss: 0.663885  [   64/  126]
train() client id: f_00000-4-2 loss: 0.905509  [   96/  126]
train() client id: f_00000-5-0 loss: 0.803861  [   32/  126]
train() client id: f_00000-5-1 loss: 0.825836  [   64/  126]
train() client id: f_00000-5-2 loss: 0.739947  [   96/  126]
train() client id: f_00000-6-0 loss: 0.765637  [   32/  126]
train() client id: f_00000-6-1 loss: 0.706871  [   64/  126]
train() client id: f_00000-6-2 loss: 0.842985  [   96/  126]
train() client id: f_00000-7-0 loss: 0.669759  [   32/  126]
train() client id: f_00000-7-1 loss: 0.811683  [   64/  126]
train() client id: f_00000-7-2 loss: 0.823958  [   96/  126]
train() client id: f_00001-0-0 loss: 0.518493  [   32/  265]
train() client id: f_00001-0-1 loss: 0.589634  [   64/  265]
train() client id: f_00001-0-2 loss: 0.447167  [   96/  265]
train() client id: f_00001-0-3 loss: 0.508741  [  128/  265]
train() client id: f_00001-0-4 loss: 0.583218  [  160/  265]
train() client id: f_00001-0-5 loss: 0.597807  [  192/  265]
train() client id: f_00001-0-6 loss: 0.480146  [  224/  265]
train() client id: f_00001-0-7 loss: 0.482000  [  256/  265]
train() client id: f_00001-1-0 loss: 0.467745  [   32/  265]
train() client id: f_00001-1-1 loss: 0.526975  [   64/  265]
train() client id: f_00001-1-2 loss: 0.607940  [   96/  265]
train() client id: f_00001-1-3 loss: 0.560431  [  128/  265]
train() client id: f_00001-1-4 loss: 0.589587  [  160/  265]
train() client id: f_00001-1-5 loss: 0.416823  [  192/  265]
train() client id: f_00001-1-6 loss: 0.466686  [  224/  265]
train() client id: f_00001-1-7 loss: 0.438373  [  256/  265]
train() client id: f_00001-2-0 loss: 0.542111  [   32/  265]
train() client id: f_00001-2-1 loss: 0.514192  [   64/  265]
train() client id: f_00001-2-2 loss: 0.514156  [   96/  265]
train() client id: f_00001-2-3 loss: 0.423322  [  128/  265]
train() client id: f_00001-2-4 loss: 0.507764  [  160/  265]
train() client id: f_00001-2-5 loss: 0.504416  [  192/  265]
train() client id: f_00001-2-6 loss: 0.484091  [  224/  265]
train() client id: f_00001-2-7 loss: 0.653781  [  256/  265]
train() client id: f_00001-3-0 loss: 0.561698  [   32/  265]
train() client id: f_00001-3-1 loss: 0.431687  [   64/  265]
train() client id: f_00001-3-2 loss: 0.413647  [   96/  265]
train() client id: f_00001-3-3 loss: 0.515164  [  128/  265]
train() client id: f_00001-3-4 loss: 0.509780  [  160/  265]
train() client id: f_00001-3-5 loss: 0.582332  [  192/  265]
train() client id: f_00001-3-6 loss: 0.502193  [  224/  265]
train() client id: f_00001-3-7 loss: 0.577758  [  256/  265]
train() client id: f_00001-4-0 loss: 0.475910  [   32/  265]
train() client id: f_00001-4-1 loss: 0.448044  [   64/  265]
train() client id: f_00001-4-2 loss: 0.587412  [   96/  265]
train() client id: f_00001-4-3 loss: 0.585661  [  128/  265]
train() client id: f_00001-4-4 loss: 0.420746  [  160/  265]
train() client id: f_00001-4-5 loss: 0.596799  [  192/  265]
train() client id: f_00001-4-6 loss: 0.485299  [  224/  265]
train() client id: f_00001-4-7 loss: 0.408622  [  256/  265]
train() client id: f_00001-5-0 loss: 0.634806  [   32/  265]
train() client id: f_00001-5-1 loss: 0.430110  [   64/  265]
train() client id: f_00001-5-2 loss: 0.599678  [   96/  265]
train() client id: f_00001-5-3 loss: 0.504875  [  128/  265]
train() client id: f_00001-5-4 loss: 0.431625  [  160/  265]
train() client id: f_00001-5-5 loss: 0.414749  [  192/  265]
train() client id: f_00001-5-6 loss: 0.481480  [  224/  265]
train() client id: f_00001-5-7 loss: 0.512853  [  256/  265]
train() client id: f_00001-6-0 loss: 0.577471  [   32/  265]
train() client id: f_00001-6-1 loss: 0.470479  [   64/  265]
train() client id: f_00001-6-2 loss: 0.492478  [   96/  265]
train() client id: f_00001-6-3 loss: 0.405846  [  128/  265]
train() client id: f_00001-6-4 loss: 0.488481  [  160/  265]
train() client id: f_00001-6-5 loss: 0.456063  [  192/  265]
train() client id: f_00001-6-6 loss: 0.450650  [  224/  265]
train() client id: f_00001-6-7 loss: 0.720588  [  256/  265]
train() client id: f_00001-7-0 loss: 0.461395  [   32/  265]
train() client id: f_00001-7-1 loss: 0.587159  [   64/  265]
train() client id: f_00001-7-2 loss: 0.566095  [   96/  265]
train() client id: f_00001-7-3 loss: 0.559142  [  128/  265]
train() client id: f_00001-7-4 loss: 0.479923  [  160/  265]
train() client id: f_00001-7-5 loss: 0.433255  [  192/  265]
train() client id: f_00001-7-6 loss: 0.537563  [  224/  265]
train() client id: f_00001-7-7 loss: 0.426189  [  256/  265]
train() client id: f_00002-0-0 loss: 1.187886  [   32/  124]
train() client id: f_00002-0-1 loss: 1.072857  [   64/  124]
train() client id: f_00002-0-2 loss: 1.259739  [   96/  124]
train() client id: f_00002-1-0 loss: 1.190174  [   32/  124]
train() client id: f_00002-1-1 loss: 1.041780  [   64/  124]
train() client id: f_00002-1-2 loss: 1.117628  [   96/  124]
train() client id: f_00002-2-0 loss: 1.100913  [   32/  124]
train() client id: f_00002-2-1 loss: 1.239625  [   64/  124]
train() client id: f_00002-2-2 loss: 0.980353  [   96/  124]
train() client id: f_00002-3-0 loss: 0.989003  [   32/  124]
train() client id: f_00002-3-1 loss: 1.114873  [   64/  124]
train() client id: f_00002-3-2 loss: 1.170799  [   96/  124]
train() client id: f_00002-4-0 loss: 1.095727  [   32/  124]
train() client id: f_00002-4-1 loss: 0.970249  [   64/  124]
train() client id: f_00002-4-2 loss: 1.160019  [   96/  124]
train() client id: f_00002-5-0 loss: 1.225533  [   32/  124]
train() client id: f_00002-5-1 loss: 0.946165  [   64/  124]
train() client id: f_00002-5-2 loss: 1.213722  [   96/  124]
train() client id: f_00002-6-0 loss: 0.917873  [   32/  124]
train() client id: f_00002-6-1 loss: 1.092383  [   64/  124]
train() client id: f_00002-6-2 loss: 1.034209  [   96/  124]
train() client id: f_00002-7-0 loss: 1.158832  [   32/  124]
train() client id: f_00002-7-1 loss: 1.150780  [   64/  124]
train() client id: f_00002-7-2 loss: 0.938520  [   96/  124]
train() client id: f_00003-0-0 loss: 0.667105  [   32/   43]
train() client id: f_00003-1-0 loss: 0.433002  [   32/   43]
train() client id: f_00003-2-0 loss: 0.658015  [   32/   43]
train() client id: f_00003-3-0 loss: 0.639167  [   32/   43]
train() client id: f_00003-4-0 loss: 0.800121  [   32/   43]
train() client id: f_00003-5-0 loss: 0.481007  [   32/   43]
train() client id: f_00003-6-0 loss: 0.486858  [   32/   43]
train() client id: f_00003-7-0 loss: 0.560520  [   32/   43]
train() client id: f_00004-0-0 loss: 0.798602  [   32/  306]
train() client id: f_00004-0-1 loss: 0.770775  [   64/  306]
train() client id: f_00004-0-2 loss: 0.630525  [   96/  306]
train() client id: f_00004-0-3 loss: 0.766515  [  128/  306]
train() client id: f_00004-0-4 loss: 0.806444  [  160/  306]
train() client id: f_00004-0-5 loss: 0.801438  [  192/  306]
train() client id: f_00004-0-6 loss: 0.725177  [  224/  306]
train() client id: f_00004-0-7 loss: 0.735765  [  256/  306]
train() client id: f_00004-0-8 loss: 0.980121  [  288/  306]
train() client id: f_00004-1-0 loss: 0.722708  [   32/  306]
train() client id: f_00004-1-1 loss: 0.722697  [   64/  306]
train() client id: f_00004-1-2 loss: 0.781453  [   96/  306]
train() client id: f_00004-1-3 loss: 0.952716  [  128/  306]
train() client id: f_00004-1-4 loss: 0.733963  [  160/  306]
train() client id: f_00004-1-5 loss: 0.826909  [  192/  306]
train() client id: f_00004-1-6 loss: 0.762949  [  224/  306]
train() client id: f_00004-1-7 loss: 0.742364  [  256/  306]
train() client id: f_00004-1-8 loss: 0.933107  [  288/  306]
train() client id: f_00004-2-0 loss: 0.705084  [   32/  306]
train() client id: f_00004-2-1 loss: 0.818712  [   64/  306]
train() client id: f_00004-2-2 loss: 0.769793  [   96/  306]
train() client id: f_00004-2-3 loss: 0.771217  [  128/  306]
train() client id: f_00004-2-4 loss: 0.888992  [  160/  306]
train() client id: f_00004-2-5 loss: 0.855575  [  192/  306]
train() client id: f_00004-2-6 loss: 0.808503  [  224/  306]
train() client id: f_00004-2-7 loss: 0.710989  [  256/  306]
train() client id: f_00004-2-8 loss: 0.791556  [  288/  306]
train() client id: f_00004-3-0 loss: 0.831507  [   32/  306]
train() client id: f_00004-3-1 loss: 0.675919  [   64/  306]
train() client id: f_00004-3-2 loss: 1.000258  [   96/  306]
train() client id: f_00004-3-3 loss: 0.847080  [  128/  306]
train() client id: f_00004-3-4 loss: 0.729167  [  160/  306]
train() client id: f_00004-3-5 loss: 0.668571  [  192/  306]
train() client id: f_00004-3-6 loss: 0.707548  [  224/  306]
train() client id: f_00004-3-7 loss: 0.910158  [  256/  306]
train() client id: f_00004-3-8 loss: 0.760757  [  288/  306]
train() client id: f_00004-4-0 loss: 0.669574  [   32/  306]
train() client id: f_00004-4-1 loss: 0.727034  [   64/  306]
train() client id: f_00004-4-2 loss: 0.712336  [   96/  306]
train() client id: f_00004-4-3 loss: 0.822109  [  128/  306]
train() client id: f_00004-4-4 loss: 0.712982  [  160/  306]
train() client id: f_00004-4-5 loss: 0.868329  [  192/  306]
train() client id: f_00004-4-6 loss: 0.930284  [  224/  306]
train() client id: f_00004-4-7 loss: 0.767902  [  256/  306]
train() client id: f_00004-4-8 loss: 1.070647  [  288/  306]
train() client id: f_00004-5-0 loss: 0.824447  [   32/  306]
train() client id: f_00004-5-1 loss: 0.845075  [   64/  306]
train() client id: f_00004-5-2 loss: 0.742774  [   96/  306]
train() client id: f_00004-5-3 loss: 0.786447  [  128/  306]
train() client id: f_00004-5-4 loss: 0.861001  [  160/  306]
train() client id: f_00004-5-5 loss: 0.888342  [  192/  306]
train() client id: f_00004-5-6 loss: 0.790911  [  224/  306]
train() client id: f_00004-5-7 loss: 0.673565  [  256/  306]
train() client id: f_00004-5-8 loss: 0.914942  [  288/  306]
train() client id: f_00004-6-0 loss: 0.795757  [   32/  306]
train() client id: f_00004-6-1 loss: 0.831612  [   64/  306]
train() client id: f_00004-6-2 loss: 0.697835  [   96/  306]
train() client id: f_00004-6-3 loss: 0.735086  [  128/  306]
train() client id: f_00004-6-4 loss: 0.835528  [  160/  306]
train() client id: f_00004-6-5 loss: 0.824712  [  192/  306]
train() client id: f_00004-6-6 loss: 0.848148  [  224/  306]
train() client id: f_00004-6-7 loss: 0.917341  [  256/  306]
train() client id: f_00004-6-8 loss: 0.731382  [  288/  306]
train() client id: f_00004-7-0 loss: 0.893548  [   32/  306]
train() client id: f_00004-7-1 loss: 0.663455  [   64/  306]
train() client id: f_00004-7-2 loss: 1.002762  [   96/  306]
train() client id: f_00004-7-3 loss: 0.822892  [  128/  306]
train() client id: f_00004-7-4 loss: 0.957114  [  160/  306]
train() client id: f_00004-7-5 loss: 0.684700  [  192/  306]
train() client id: f_00004-7-6 loss: 0.737994  [  224/  306]
train() client id: f_00004-7-7 loss: 0.804096  [  256/  306]
train() client id: f_00004-7-8 loss: 0.829113  [  288/  306]
train() client id: f_00005-0-0 loss: 0.395973  [   32/  146]
train() client id: f_00005-0-1 loss: 0.483677  [   64/  146]
train() client id: f_00005-0-2 loss: 0.372387  [   96/  146]
train() client id: f_00005-0-3 loss: 0.433772  [  128/  146]
train() client id: f_00005-1-0 loss: 0.824033  [   32/  146]
train() client id: f_00005-1-1 loss: 0.394002  [   64/  146]
train() client id: f_00005-1-2 loss: 0.380324  [   96/  146]
train() client id: f_00005-1-3 loss: 0.418620  [  128/  146]
train() client id: f_00005-2-0 loss: 0.366755  [   32/  146]
train() client id: f_00005-2-1 loss: 0.773682  [   64/  146]
train() client id: f_00005-2-2 loss: 0.472337  [   96/  146]
train() client id: f_00005-2-3 loss: 0.439826  [  128/  146]
train() client id: f_00005-3-0 loss: 0.687027  [   32/  146]
train() client id: f_00005-3-1 loss: 0.448426  [   64/  146]
train() client id: f_00005-3-2 loss: 0.529846  [   96/  146]
train() client id: f_00005-3-3 loss: 0.448032  [  128/  146]
train() client id: f_00005-4-0 loss: 0.627246  [   32/  146]
train() client id: f_00005-4-1 loss: 0.581137  [   64/  146]
train() client id: f_00005-4-2 loss: 0.366913  [   96/  146]
train() client id: f_00005-4-3 loss: 0.428201  [  128/  146]
train() client id: f_00005-5-0 loss: 0.262128  [   32/  146]
train() client id: f_00005-5-1 loss: 0.282234  [   64/  146]
train() client id: f_00005-5-2 loss: 0.652862  [   96/  146]
train() client id: f_00005-5-3 loss: 0.391123  [  128/  146]
train() client id: f_00005-6-0 loss: 0.578354  [   32/  146]
train() client id: f_00005-6-1 loss: 0.401100  [   64/  146]
train() client id: f_00005-6-2 loss: 0.546004  [   96/  146]
train() client id: f_00005-6-3 loss: 0.454843  [  128/  146]
train() client id: f_00005-7-0 loss: 0.798652  [   32/  146]
train() client id: f_00005-7-1 loss: 0.450667  [   64/  146]
train() client id: f_00005-7-2 loss: 0.449169  [   96/  146]
train() client id: f_00005-7-3 loss: 0.246385  [  128/  146]
train() client id: f_00006-0-0 loss: 0.505693  [   32/   54]
train() client id: f_00006-1-0 loss: 0.441592  [   32/   54]
train() client id: f_00006-2-0 loss: 0.482934  [   32/   54]
train() client id: f_00006-3-0 loss: 0.513125  [   32/   54]
train() client id: f_00006-4-0 loss: 0.487896  [   32/   54]
train() client id: f_00006-5-0 loss: 0.481981  [   32/   54]
train() client id: f_00006-6-0 loss: 0.516489  [   32/   54]
train() client id: f_00006-7-0 loss: 0.455060  [   32/   54]
train() client id: f_00007-0-0 loss: 0.364248  [   32/  179]
train() client id: f_00007-0-1 loss: 0.324525  [   64/  179]
train() client id: f_00007-0-2 loss: 0.285250  [   96/  179]
train() client id: f_00007-0-3 loss: 0.488213  [  128/  179]
train() client id: f_00007-0-4 loss: 0.608062  [  160/  179]
train() client id: f_00007-1-0 loss: 0.254609  [   32/  179]
train() client id: f_00007-1-1 loss: 0.418215  [   64/  179]
train() client id: f_00007-1-2 loss: 0.358966  [   96/  179]
train() client id: f_00007-1-3 loss: 0.458696  [  128/  179]
train() client id: f_00007-1-4 loss: 0.409296  [  160/  179]
train() client id: f_00007-2-0 loss: 0.345026  [   32/  179]
train() client id: f_00007-2-1 loss: 0.413357  [   64/  179]
train() client id: f_00007-2-2 loss: 0.463496  [   96/  179]
train() client id: f_00007-2-3 loss: 0.194213  [  128/  179]
train() client id: f_00007-2-4 loss: 0.302607  [  160/  179]
train() client id: f_00007-3-0 loss: 0.384418  [   32/  179]
train() client id: f_00007-3-1 loss: 0.294878  [   64/  179]
train() client id: f_00007-3-2 loss: 0.256738  [   96/  179]
train() client id: f_00007-3-3 loss: 0.557914  [  128/  179]
train() client id: f_00007-3-4 loss: 0.393475  [  160/  179]
train() client id: f_00007-4-0 loss: 0.250245  [   32/  179]
train() client id: f_00007-4-1 loss: 0.361230  [   64/  179]
train() client id: f_00007-4-2 loss: 0.278514  [   96/  179]
train() client id: f_00007-4-3 loss: 0.400892  [  128/  179]
train() client id: f_00007-4-4 loss: 0.534054  [  160/  179]
train() client id: f_00007-5-0 loss: 0.396871  [   32/  179]
train() client id: f_00007-5-1 loss: 0.284492  [   64/  179]
train() client id: f_00007-5-2 loss: 0.292317  [   96/  179]
train() client id: f_00007-5-3 loss: 0.332500  [  128/  179]
train() client id: f_00007-5-4 loss: 0.456567  [  160/  179]
train() client id: f_00007-6-0 loss: 0.247735  [   32/  179]
train() client id: f_00007-6-1 loss: 0.462482  [   64/  179]
train() client id: f_00007-6-2 loss: 0.451951  [   96/  179]
train() client id: f_00007-6-3 loss: 0.338555  [  128/  179]
train() client id: f_00007-6-4 loss: 0.190795  [  160/  179]
train() client id: f_00007-7-0 loss: 0.319291  [   32/  179]
train() client id: f_00007-7-1 loss: 0.279688  [   64/  179]
train() client id: f_00007-7-2 loss: 0.375489  [   96/  179]
train() client id: f_00007-7-3 loss: 0.186357  [  128/  179]
train() client id: f_00007-7-4 loss: 0.504184  [  160/  179]
train() client id: f_00008-0-0 loss: 0.816425  [   32/  130]
train() client id: f_00008-0-1 loss: 0.727342  [   64/  130]
train() client id: f_00008-0-2 loss: 0.736698  [   96/  130]
train() client id: f_00008-0-3 loss: 0.733211  [  128/  130]
train() client id: f_00008-1-0 loss: 0.697582  [   32/  130]
train() client id: f_00008-1-1 loss: 0.805485  [   64/  130]
train() client id: f_00008-1-2 loss: 0.800500  [   96/  130]
train() client id: f_00008-1-3 loss: 0.676535  [  128/  130]
train() client id: f_00008-2-0 loss: 0.678874  [   32/  130]
train() client id: f_00008-2-1 loss: 0.735175  [   64/  130]
train() client id: f_00008-2-2 loss: 0.754185  [   96/  130]
train() client id: f_00008-2-3 loss: 0.838538  [  128/  130]
train() client id: f_00008-3-0 loss: 0.750222  [   32/  130]
train() client id: f_00008-3-1 loss: 0.639896  [   64/  130]
train() client id: f_00008-3-2 loss: 0.754993  [   96/  130]
train() client id: f_00008-3-3 loss: 0.815390  [  128/  130]
train() client id: f_00008-4-0 loss: 0.738784  [   32/  130]
train() client id: f_00008-4-1 loss: 0.781375  [   64/  130]
train() client id: f_00008-4-2 loss: 0.716645  [   96/  130]
train() client id: f_00008-4-3 loss: 0.754946  [  128/  130]
train() client id: f_00008-5-0 loss: 0.792396  [   32/  130]
train() client id: f_00008-5-1 loss: 0.722632  [   64/  130]
train() client id: f_00008-5-2 loss: 0.668579  [   96/  130]
train() client id: f_00008-5-3 loss: 0.806294  [  128/  130]
train() client id: f_00008-6-0 loss: 0.743758  [   32/  130]
train() client id: f_00008-6-1 loss: 0.718789  [   64/  130]
train() client id: f_00008-6-2 loss: 0.791440  [   96/  130]
train() client id: f_00008-6-3 loss: 0.711689  [  128/  130]
train() client id: f_00008-7-0 loss: 0.818184  [   32/  130]
train() client id: f_00008-7-1 loss: 0.714490  [   64/  130]
train() client id: f_00008-7-2 loss: 0.720866  [   96/  130]
train() client id: f_00008-7-3 loss: 0.739343  [  128/  130]
train() client id: f_00009-0-0 loss: 1.025497  [   32/  118]
train() client id: f_00009-0-1 loss: 0.915832  [   64/  118]
train() client id: f_00009-0-2 loss: 0.911620  [   96/  118]
train() client id: f_00009-1-0 loss: 0.974717  [   32/  118]
train() client id: f_00009-1-1 loss: 0.677431  [   64/  118]
train() client id: f_00009-1-2 loss: 0.910836  [   96/  118]
train() client id: f_00009-2-0 loss: 0.850727  [   32/  118]
train() client id: f_00009-2-1 loss: 0.879884  [   64/  118]
train() client id: f_00009-2-2 loss: 1.047303  [   96/  118]
train() client id: f_00009-3-0 loss: 0.831360  [   32/  118]
train() client id: f_00009-3-1 loss: 0.876445  [   64/  118]
train() client id: f_00009-3-2 loss: 1.072111  [   96/  118]
train() client id: f_00009-4-0 loss: 0.881283  [   32/  118]
train() client id: f_00009-4-1 loss: 0.944886  [   64/  118]
train() client id: f_00009-4-2 loss: 0.887006  [   96/  118]
train() client id: f_00009-5-0 loss: 0.782171  [   32/  118]
train() client id: f_00009-5-1 loss: 0.980899  [   64/  118]
train() client id: f_00009-5-2 loss: 0.874892  [   96/  118]
train() client id: f_00009-6-0 loss: 0.871130  [   32/  118]
train() client id: f_00009-6-1 loss: 0.854489  [   64/  118]
train() client id: f_00009-6-2 loss: 1.023605  [   96/  118]
train() client id: f_00009-7-0 loss: 0.805096  [   32/  118]
train() client id: f_00009-7-1 loss: 0.849669  [   64/  118]
train() client id: f_00009-7-2 loss: 1.038332  [   96/  118]
At round 67 accuracy: 0.6419098143236074
At round 67 training accuracy: 0.5942320590207915
At round 67 training loss: 0.8145105014128952
update_location
xs = [  -3.9056584     4.20031788  355.00902392   18.81129433    0.97929623
    3.95640986 -317.44319194 -296.32485185  339.66397685 -282.06087855]
ys = [ 347.5879595   330.55583871    1.32061395 -317.45517586  309.35018685
  292.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [361.70795368 345.37632399 368.8267223  333.36414546 325.11305283
 309.44430849 332.83189548 312.74445491 354.51415648 299.28974434]
dists_bs = [243.02238063 237.29806759 557.74016761 529.22104837 221.40163822
 214.25585347 227.68552432 212.26769452 538.20398264 201.74866778]
uav_gains = [1.05200963e-12 1.28407742e-12 9.72759726e-13 1.51715294e-12
 1.72031827e-12 2.24199922e-12 1.52907249e-12 2.11432101e-12
 1.14444349e-12 2.70950730e-12]
bs_gains = [2.30932218e-11 2.46871139e-11 2.25571746e-12 2.61282415e-12
 2.99769719e-11 3.28611333e-11 2.77175544e-11 3.37302187e-11
 2.49254349e-12 3.88887688e-11]
Round 68
-------------------------------
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.01021389 3.98881553 1.98649518 0.75024676 4.59768714 2.21362175
 0.91366287 2.75982094 2.02332361 1.79514037]
obj_prev = 23.039028040351884
eta_min = nan	eta_max = nan
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 5.251816334124527	eta = 0.9090909090909091
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 14.045450908523552	eta = 0.3399234753417846
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 9.049757744866326	eta = 0.5275697560275713
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.202066092537866	eta = 0.5820946127112314
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148578542585753	eta = 0.5859155017793716
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148332307464996	eta = 0.5859332076078638
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148332302200442	eta = 0.5859332079864292
eta = 0.5859332079864292
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [0.04410974 0.09277049 0.04340959 0.01505332 0.10712366 0.05111129
 0.01890417 0.06266384 0.04551005 0.04130912]
ene_total = [0.83982608 1.19483484 0.84607581 0.44015542 1.35993302 0.69656538
 0.48459427 0.96365584 0.74525191 0.57743974]
ti_comp = [1.98249026 2.16845803 1.97019612 2.03038779 2.17217608 2.17382626
 2.03126319 2.06316582 2.07658364 2.17668634]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [1.36477616e-06 1.06122506e-05 1.31709789e-06 5.17153298e-08
 1.62834394e-05 1.76595985e-06 1.02334201e-07 3.61295660e-06
 1.36616489e-06 9.29878567e-07]
ene_total = [0.31897471 0.09948487 0.33349161 0.26239966 0.0951614  0.09304137
 0.26136655 0.22373596 0.20786507 0.08965418]
optimize_network_iter = 0 obj = 1.9851753783046104
eta = 0.5859332079864292
freqs = [11124831.81396149 21390888.52316821 11016564.88463251  3707007.23591558
 24658143.18962516 11756065.47830876  4653305.14511856 15186331.26526243
 10957914.22550441  9488993.01846851]
eta_min = 0.5859332079864303	eta_max = 0.7781376768963073
af = 0.0003421552139497174	bf = 0.8218123015887785	zeta = 0.00037637073534468917	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [2.43339611e-07 1.89216446e-06 2.34838575e-07 9.22084412e-09
 2.90333750e-06 3.14870668e-07 1.82461896e-08 6.44190219e-07
 2.43587222e-07 1.65797363e-07]
ene_total = [1.45278485 0.4526582  1.5189076  1.19515856 0.43271528 0.42370069
 1.19045077 1.01889828 0.94670988 0.40830993]
ti_comp = [0.93686199 1.12282976 0.92456785 0.98475952 1.12654781 1.12819799
 0.98563492 1.01753754 1.03095536 1.13105807]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.31568265e-07 4.09043688e-06 6.18083830e-07 2.27198204e-08
 6.25640601e-06 6.77560105e-07 4.49168218e-08 1.53503439e-06
 5.72809530e-07 3.55906665e-07]
ene_total = [0.59529372 0.18552719 0.62238771 0.48972193 0.17738094 0.17362127
 0.48779318 0.41751781 0.38792591 0.16731103]
optimize_network_iter = 1 obj = 3.7044806936267416
eta = 0.7781376768963073
freqs = [11047352.84864209 19386314.72442644 11016564.8846325   3586753.80166813
 22311825.119887   10629940.50483284  4500294.94608312 14449934.82166095
 10357782.55225607  8569597.11605592]
eta_min = 0.7781376768963095	eta_max = 0.7781376768963063
af = 0.0002885807068502459	bf = 0.8218123015887785	zeta = 0.0003174387775352705	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [2.39961934e-07 1.55414576e-06 2.34838575e-07 8.63230839e-09
 2.37709741e-06 2.57436357e-07 1.70659737e-08 5.83230415e-07
 2.17636778e-07 1.35225369e-07]
ene_total = [1.45278467 0.45264002 1.5189076  1.19515853 0.43268697 0.4236976
 1.19045071 1.018895   0.94670848 0.40830829]
ti_comp = [0.93686199 1.12282976 0.92456785 0.98475952 1.12654781 1.12819799
 0.98563492 1.01753754 1.03095536 1.13105807]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.31568265e-07 4.09043688e-06 6.18083830e-07 2.27198204e-08
 6.25640601e-06 6.77560105e-07 4.49168218e-08 1.53503439e-06
 5.72809530e-07 3.55906665e-07]
ene_total = [0.59529372 0.18552719 0.62238771 0.48972193 0.17738094 0.17362127
 0.48779318 0.41751781 0.38792591 0.16731103]
optimize_network_iter = 2 obj = 3.7044806936267247
eta = 0.7781376768963063
freqs = [11047352.84864208 19386314.72442644 11016564.88463249  3586753.80166813
 22311825.119887   10629940.50483284  4500294.94608312 14449934.82166094
 10357782.55225607  8569597.11605592]
Done!
At round 68 eta: 0.7781376768963063
At round 68 local rounds: 8.214162669625793
At round 68 global rounds: 22.038375844715002
At round 68 a_n: 4.546939415371483
gradient difference: 0.6170907020568848
train() client id: f_00000-0-0 loss: 0.925393  [   32/  126]
train() client id: f_00000-0-1 loss: 1.198910  [   64/  126]
train() client id: f_00000-0-2 loss: 0.934575  [   96/  126]
train() client id: f_00000-1-0 loss: 0.925058  [   32/  126]
train() client id: f_00000-1-1 loss: 1.127969  [   64/  126]
train() client id: f_00000-1-2 loss: 0.723674  [   96/  126]
train() client id: f_00000-2-0 loss: 0.965652  [   32/  126]
train() client id: f_00000-2-1 loss: 0.917975  [   64/  126]
train() client id: f_00000-2-2 loss: 0.753899  [   96/  126]
train() client id: f_00000-3-0 loss: 0.677348  [   32/  126]
train() client id: f_00000-3-1 loss: 0.756465  [   64/  126]
train() client id: f_00000-3-2 loss: 1.031770  [   96/  126]
train() client id: f_00000-4-0 loss: 0.834267  [   32/  126]
train() client id: f_00000-4-1 loss: 0.779000  [   64/  126]
train() client id: f_00000-4-2 loss: 0.768719  [   96/  126]
train() client id: f_00000-5-0 loss: 0.839697  [   32/  126]
train() client id: f_00000-5-1 loss: 0.800345  [   64/  126]
train() client id: f_00000-5-2 loss: 0.742882  [   96/  126]
train() client id: f_00000-6-0 loss: 0.706481  [   32/  126]
train() client id: f_00000-6-1 loss: 0.818410  [   64/  126]
train() client id: f_00000-6-2 loss: 0.826916  [   96/  126]
train() client id: f_00000-7-0 loss: 0.766963  [   32/  126]
train() client id: f_00000-7-1 loss: 0.773751  [   64/  126]
train() client id: f_00000-7-2 loss: 0.826103  [   96/  126]
train() client id: f_00001-0-0 loss: 0.505330  [   32/  265]
train() client id: f_00001-0-1 loss: 0.629071  [   64/  265]
train() client id: f_00001-0-2 loss: 0.543216  [   96/  265]
train() client id: f_00001-0-3 loss: 0.537899  [  128/  265]
train() client id: f_00001-0-4 loss: 0.457315  [  160/  265]
train() client id: f_00001-0-5 loss: 0.543835  [  192/  265]
train() client id: f_00001-0-6 loss: 0.579512  [  224/  265]
train() client id: f_00001-0-7 loss: 0.641251  [  256/  265]
train() client id: f_00001-1-0 loss: 0.511504  [   32/  265]
train() client id: f_00001-1-1 loss: 0.516376  [   64/  265]
train() client id: f_00001-1-2 loss: 0.613619  [   96/  265]
train() client id: f_00001-1-3 loss: 0.587894  [  128/  265]
train() client id: f_00001-1-4 loss: 0.519720  [  160/  265]
train() client id: f_00001-1-5 loss: 0.477626  [  192/  265]
train() client id: f_00001-1-6 loss: 0.579577  [  224/  265]
train() client id: f_00001-1-7 loss: 0.474084  [  256/  265]
train() client id: f_00001-2-0 loss: 0.487177  [   32/  265]
train() client id: f_00001-2-1 loss: 0.586864  [   64/  265]
train() client id: f_00001-2-2 loss: 0.480772  [   96/  265]
train() client id: f_00001-2-3 loss: 0.546208  [  128/  265]
train() client id: f_00001-2-4 loss: 0.534921  [  160/  265]
train() client id: f_00001-2-5 loss: 0.659155  [  192/  265]
train() client id: f_00001-2-6 loss: 0.465609  [  224/  265]
train() client id: f_00001-2-7 loss: 0.575984  [  256/  265]
train() client id: f_00001-3-0 loss: 0.511805  [   32/  265]
train() client id: f_00001-3-1 loss: 0.521583  [   64/  265]
train() client id: f_00001-3-2 loss: 0.567610  [   96/  265]
train() client id: f_00001-3-3 loss: 0.726773  [  128/  265]
train() client id: f_00001-3-4 loss: 0.470436  [  160/  265]
train() client id: f_00001-3-5 loss: 0.594342  [  192/  265]
train() client id: f_00001-3-6 loss: 0.504242  [  224/  265]
train() client id: f_00001-3-7 loss: 0.490440  [  256/  265]
train() client id: f_00001-4-0 loss: 0.480719  [   32/  265]
train() client id: f_00001-4-1 loss: 0.451810  [   64/  265]
train() client id: f_00001-4-2 loss: 0.573633  [   96/  265]
train() client id: f_00001-4-3 loss: 0.444440  [  128/  265]
train() client id: f_00001-4-4 loss: 0.567999  [  160/  265]
train() client id: f_00001-4-5 loss: 0.602792  [  192/  265]
train() client id: f_00001-4-6 loss: 0.488744  [  224/  265]
train() client id: f_00001-4-7 loss: 0.709230  [  256/  265]
train() client id: f_00001-5-0 loss: 0.692189  [   32/  265]
train() client id: f_00001-5-1 loss: 0.426202  [   64/  265]
train() client id: f_00001-5-2 loss: 0.550959  [   96/  265]
train() client id: f_00001-5-3 loss: 0.471740  [  128/  265]
train() client id: f_00001-5-4 loss: 0.457153  [  160/  265]
train() client id: f_00001-5-5 loss: 0.648463  [  192/  265]
train() client id: f_00001-5-6 loss: 0.548362  [  224/  265]
train() client id: f_00001-5-7 loss: 0.598348  [  256/  265]
train() client id: f_00001-6-0 loss: 0.593816  [   32/  265]
train() client id: f_00001-6-1 loss: 0.607665  [   64/  265]
train() client id: f_00001-6-2 loss: 0.625392  [   96/  265]
train() client id: f_00001-6-3 loss: 0.491735  [  128/  265]
train() client id: f_00001-6-4 loss: 0.507346  [  160/  265]
train() client id: f_00001-6-5 loss: 0.533704  [  192/  265]
train() client id: f_00001-6-6 loss: 0.520299  [  224/  265]
train() client id: f_00001-6-7 loss: 0.524472  [  256/  265]
train() client id: f_00001-7-0 loss: 0.551794  [   32/  265]
train() client id: f_00001-7-1 loss: 0.515743  [   64/  265]
train() client id: f_00001-7-2 loss: 0.456051  [   96/  265]
train() client id: f_00001-7-3 loss: 0.541969  [  128/  265]
train() client id: f_00001-7-4 loss: 0.562329  [  160/  265]
train() client id: f_00001-7-5 loss: 0.594794  [  192/  265]
train() client id: f_00001-7-6 loss: 0.627216  [  224/  265]
train() client id: f_00001-7-7 loss: 0.512890  [  256/  265]
train() client id: f_00002-0-0 loss: 1.231887  [   32/  124]
train() client id: f_00002-0-1 loss: 1.229276  [   64/  124]
train() client id: f_00002-0-2 loss: 1.185860  [   96/  124]
train() client id: f_00002-1-0 loss: 1.251938  [   32/  124]
train() client id: f_00002-1-1 loss: 1.130038  [   64/  124]
train() client id: f_00002-1-2 loss: 1.128133  [   96/  124]
train() client id: f_00002-2-0 loss: 1.379595  [   32/  124]
train() client id: f_00002-2-1 loss: 1.171600  [   64/  124]
train() client id: f_00002-2-2 loss: 1.120188  [   96/  124]
train() client id: f_00002-3-0 loss: 1.222645  [   32/  124]
train() client id: f_00002-3-1 loss: 1.098427  [   64/  124]
train() client id: f_00002-3-2 loss: 1.108676  [   96/  124]
train() client id: f_00002-4-0 loss: 1.124256  [   32/  124]
train() client id: f_00002-4-1 loss: 1.264522  [   64/  124]
train() client id: f_00002-4-2 loss: 1.192818  [   96/  124]
train() client id: f_00002-5-0 loss: 1.179906  [   32/  124]
train() client id: f_00002-5-1 loss: 1.304510  [   64/  124]
train() client id: f_00002-5-2 loss: 0.911201  [   96/  124]
train() client id: f_00002-6-0 loss: 1.248243  [   32/  124]
train() client id: f_00002-6-1 loss: 1.139230  [   64/  124]
train() client id: f_00002-6-2 loss: 1.010986  [   96/  124]
train() client id: f_00002-7-0 loss: 1.148569  [   32/  124]
train() client id: f_00002-7-1 loss: 1.103627  [   64/  124]
train() client id: f_00002-7-2 loss: 1.069470  [   96/  124]
train() client id: f_00003-0-0 loss: 0.598845  [   32/   43]
train() client id: f_00003-1-0 loss: 0.606660  [   32/   43]
train() client id: f_00003-2-0 loss: 0.891659  [   32/   43]
train() client id: f_00003-3-0 loss: 0.847701  [   32/   43]
train() client id: f_00003-4-0 loss: 0.683962  [   32/   43]
train() client id: f_00003-5-0 loss: 0.920070  [   32/   43]
train() client id: f_00003-6-0 loss: 0.577434  [   32/   43]
train() client id: f_00003-7-0 loss: 0.849408  [   32/   43]
train() client id: f_00004-0-0 loss: 0.670524  [   32/  306]
train() client id: f_00004-0-1 loss: 0.809937  [   64/  306]
train() client id: f_00004-0-2 loss: 0.817737  [   96/  306]
train() client id: f_00004-0-3 loss: 0.806058  [  128/  306]
train() client id: f_00004-0-4 loss: 0.891092  [  160/  306]
train() client id: f_00004-0-5 loss: 0.767601  [  192/  306]
train() client id: f_00004-0-6 loss: 0.746758  [  224/  306]
train() client id: f_00004-0-7 loss: 0.966613  [  256/  306]
train() client id: f_00004-0-8 loss: 0.921247  [  288/  306]
train() client id: f_00004-1-0 loss: 0.929604  [   32/  306]
train() client id: f_00004-1-1 loss: 0.747564  [   64/  306]
train() client id: f_00004-1-2 loss: 0.815018  [   96/  306]
train() client id: f_00004-1-3 loss: 0.720195  [  128/  306]
train() client id: f_00004-1-4 loss: 0.968777  [  160/  306]
train() client id: f_00004-1-5 loss: 0.779841  [  192/  306]
train() client id: f_00004-1-6 loss: 0.777325  [  224/  306]
train() client id: f_00004-1-7 loss: 0.832117  [  256/  306]
train() client id: f_00004-1-8 loss: 0.853437  [  288/  306]
train() client id: f_00004-2-0 loss: 0.742615  [   32/  306]
train() client id: f_00004-2-1 loss: 0.792905  [   64/  306]
train() client id: f_00004-2-2 loss: 0.812002  [   96/  306]
train() client id: f_00004-2-3 loss: 0.928464  [  128/  306]
train() client id: f_00004-2-4 loss: 0.813625  [  160/  306]
train() client id: f_00004-2-5 loss: 0.724319  [  192/  306]
train() client id: f_00004-2-6 loss: 0.943859  [  224/  306]
train() client id: f_00004-2-7 loss: 0.883934  [  256/  306]
train() client id: f_00004-2-8 loss: 0.854779  [  288/  306]
train() client id: f_00004-3-0 loss: 0.831030  [   32/  306]
train() client id: f_00004-3-1 loss: 0.925902  [   64/  306]
train() client id: f_00004-3-2 loss: 0.778743  [   96/  306]
train() client id: f_00004-3-3 loss: 0.715484  [  128/  306]
train() client id: f_00004-3-4 loss: 0.883229  [  160/  306]
train() client id: f_00004-3-5 loss: 0.813136  [  192/  306]
train() client id: f_00004-3-6 loss: 0.862420  [  224/  306]
train() client id: f_00004-3-7 loss: 0.769987  [  256/  306]
train() client id: f_00004-3-8 loss: 0.786097  [  288/  306]
train() client id: f_00004-4-0 loss: 0.932601  [   32/  306]
train() client id: f_00004-4-1 loss: 0.990154  [   64/  306]
train() client id: f_00004-4-2 loss: 0.756589  [   96/  306]
train() client id: f_00004-4-3 loss: 0.958181  [  128/  306]
train() client id: f_00004-4-4 loss: 0.762993  [  160/  306]
train() client id: f_00004-4-5 loss: 0.802588  [  192/  306]
train() client id: f_00004-4-6 loss: 0.753917  [  224/  306]
train() client id: f_00004-4-7 loss: 0.753420  [  256/  306]
train() client id: f_00004-4-8 loss: 0.823065  [  288/  306]
train() client id: f_00004-5-0 loss: 0.806183  [   32/  306]
train() client id: f_00004-5-1 loss: 0.777338  [   64/  306]
train() client id: f_00004-5-2 loss: 0.768977  [   96/  306]
train() client id: f_00004-5-3 loss: 0.759640  [  128/  306]
train() client id: f_00004-5-4 loss: 0.802678  [  160/  306]
train() client id: f_00004-5-5 loss: 0.859264  [  192/  306]
train() client id: f_00004-5-6 loss: 0.983014  [  224/  306]
train() client id: f_00004-5-7 loss: 0.778661  [  256/  306]
train() client id: f_00004-5-8 loss: 0.939958  [  288/  306]
train() client id: f_00004-6-0 loss: 0.655004  [   32/  306]
train() client id: f_00004-6-1 loss: 0.880836  [   64/  306]
train() client id: f_00004-6-2 loss: 0.838029  [   96/  306]
train() client id: f_00004-6-3 loss: 0.885659  [  128/  306]
train() client id: f_00004-6-4 loss: 0.835258  [  160/  306]
train() client id: f_00004-6-5 loss: 0.813652  [  192/  306]
train() client id: f_00004-6-6 loss: 0.809486  [  224/  306]
train() client id: f_00004-6-7 loss: 0.908846  [  256/  306]
train() client id: f_00004-6-8 loss: 0.806142  [  288/  306]
train() client id: f_00004-7-0 loss: 0.763601  [   32/  306]
train() client id: f_00004-7-1 loss: 0.825325  [   64/  306]
train() client id: f_00004-7-2 loss: 0.826568  [   96/  306]
train() client id: f_00004-7-3 loss: 0.883961  [  128/  306]
train() client id: f_00004-7-4 loss: 0.868695  [  160/  306]
train() client id: f_00004-7-5 loss: 0.899379  [  192/  306]
train() client id: f_00004-7-6 loss: 0.821735  [  224/  306]
train() client id: f_00004-7-7 loss: 0.813889  [  256/  306]
train() client id: f_00004-7-8 loss: 0.755277  [  288/  306]
train() client id: f_00005-0-0 loss: 0.836817  [   32/  146]
train() client id: f_00005-0-1 loss: 0.798330  [   64/  146]
train() client id: f_00005-0-2 loss: 0.682152  [   96/  146]
train() client id: f_00005-0-3 loss: 1.027185  [  128/  146]
train() client id: f_00005-1-0 loss: 0.879837  [   32/  146]
train() client id: f_00005-1-1 loss: 1.084430  [   64/  146]
train() client id: f_00005-1-2 loss: 0.554917  [   96/  146]
train() client id: f_00005-1-3 loss: 0.714523  [  128/  146]
train() client id: f_00005-2-0 loss: 0.968872  [   32/  146]
train() client id: f_00005-2-1 loss: 0.911096  [   64/  146]
train() client id: f_00005-2-2 loss: 0.574797  [   96/  146]
train() client id: f_00005-2-3 loss: 0.835293  [  128/  146]
train() client id: f_00005-3-0 loss: 0.758597  [   32/  146]
train() client id: f_00005-3-1 loss: 0.677240  [   64/  146]
train() client id: f_00005-3-2 loss: 0.804274  [   96/  146]
train() client id: f_00005-3-3 loss: 0.965183  [  128/  146]
train() client id: f_00005-4-0 loss: 0.951194  [   32/  146]
train() client id: f_00005-4-1 loss: 0.465312  [   64/  146]
train() client id: f_00005-4-2 loss: 0.716506  [   96/  146]
train() client id: f_00005-4-3 loss: 1.083553  [  128/  146]
train() client id: f_00005-5-0 loss: 0.860634  [   32/  146]
train() client id: f_00005-5-1 loss: 0.820702  [   64/  146]
train() client id: f_00005-5-2 loss: 0.764694  [   96/  146]
train() client id: f_00005-5-3 loss: 0.750661  [  128/  146]
train() client id: f_00005-6-0 loss: 1.022281  [   32/  146]
train() client id: f_00005-6-1 loss: 0.621138  [   64/  146]
train() client id: f_00005-6-2 loss: 0.701562  [   96/  146]
train() client id: f_00005-6-3 loss: 0.765615  [  128/  146]
train() client id: f_00005-7-0 loss: 0.837988  [   32/  146]
train() client id: f_00005-7-1 loss: 0.791682  [   64/  146]
train() client id: f_00005-7-2 loss: 0.701508  [   96/  146]
train() client id: f_00005-7-3 loss: 0.674787  [  128/  146]
train() client id: f_00006-0-0 loss: 0.564881  [   32/   54]
train() client id: f_00006-1-0 loss: 0.572057  [   32/   54]
train() client id: f_00006-2-0 loss: 0.568226  [   32/   54]
train() client id: f_00006-3-0 loss: 0.544675  [   32/   54]
train() client id: f_00006-4-0 loss: 0.585657  [   32/   54]
train() client id: f_00006-5-0 loss: 0.568175  [   32/   54]
train() client id: f_00006-6-0 loss: 0.523481  [   32/   54]
train() client id: f_00006-7-0 loss: 0.605438  [   32/   54]
train() client id: f_00007-0-0 loss: 0.391791  [   32/  179]
train() client id: f_00007-0-1 loss: 0.621895  [   64/  179]
train() client id: f_00007-0-2 loss: 0.379981  [   96/  179]
train() client id: f_00007-0-3 loss: 0.481539  [  128/  179]
train() client id: f_00007-0-4 loss: 0.570979  [  160/  179]
train() client id: f_00007-1-0 loss: 0.372327  [   32/  179]
train() client id: f_00007-1-1 loss: 0.413942  [   64/  179]
train() client id: f_00007-1-2 loss: 0.614444  [   96/  179]
train() client id: f_00007-1-3 loss: 0.437509  [  128/  179]
train() client id: f_00007-1-4 loss: 0.467326  [  160/  179]
train() client id: f_00007-2-0 loss: 0.523022  [   32/  179]
train() client id: f_00007-2-1 loss: 0.341831  [   64/  179]
train() client id: f_00007-2-2 loss: 0.459036  [   96/  179]
train() client id: f_00007-2-3 loss: 0.487007  [  128/  179]
train() client id: f_00007-2-4 loss: 0.489904  [  160/  179]
train() client id: f_00007-3-0 loss: 0.385749  [   32/  179]
train() client id: f_00007-3-1 loss: 0.483774  [   64/  179]
train() client id: f_00007-3-2 loss: 0.496883  [   96/  179]
train() client id: f_00007-3-3 loss: 0.360686  [  128/  179]
train() client id: f_00007-3-4 loss: 0.429827  [  160/  179]
train() client id: f_00007-4-0 loss: 0.825663  [   32/  179]
train() client id: f_00007-4-1 loss: 0.270775  [   64/  179]
train() client id: f_00007-4-2 loss: 0.356368  [   96/  179]
train() client id: f_00007-4-3 loss: 0.385833  [  128/  179]
train() client id: f_00007-4-4 loss: 0.335122  [  160/  179]
train() client id: f_00007-5-0 loss: 0.389442  [   32/  179]
train() client id: f_00007-5-1 loss: 0.307855  [   64/  179]
train() client id: f_00007-5-2 loss: 0.453518  [   96/  179]
train() client id: f_00007-5-3 loss: 0.542576  [  128/  179]
train() client id: f_00007-5-4 loss: 0.354501  [  160/  179]
train() client id: f_00007-6-0 loss: 0.302779  [   32/  179]
train() client id: f_00007-6-1 loss: 0.589161  [   64/  179]
train() client id: f_00007-6-2 loss: 0.288561  [   96/  179]
train() client id: f_00007-6-3 loss: 0.264896  [  128/  179]
train() client id: f_00007-6-4 loss: 0.543615  [  160/  179]
train() client id: f_00007-7-0 loss: 0.279022  [   32/  179]
train() client id: f_00007-7-1 loss: 0.676319  [   64/  179]
train() client id: f_00007-7-2 loss: 0.448967  [   96/  179]
train() client id: f_00007-7-3 loss: 0.335050  [  128/  179]
train() client id: f_00007-7-4 loss: 0.371873  [  160/  179]
train() client id: f_00008-0-0 loss: 0.748067  [   32/  130]
train() client id: f_00008-0-1 loss: 0.640062  [   64/  130]
train() client id: f_00008-0-2 loss: 0.750169  [   96/  130]
train() client id: f_00008-0-3 loss: 0.664381  [  128/  130]
train() client id: f_00008-1-0 loss: 0.807289  [   32/  130]
train() client id: f_00008-1-1 loss: 0.649302  [   64/  130]
train() client id: f_00008-1-2 loss: 0.779563  [   96/  130]
train() client id: f_00008-1-3 loss: 0.593274  [  128/  130]
train() client id: f_00008-2-0 loss: 0.684585  [   32/  130]
train() client id: f_00008-2-1 loss: 0.749614  [   64/  130]
train() client id: f_00008-2-2 loss: 0.599342  [   96/  130]
train() client id: f_00008-2-3 loss: 0.726595  [  128/  130]
train() client id: f_00008-3-0 loss: 0.654606  [   32/  130]
train() client id: f_00008-3-1 loss: 0.783864  [   64/  130]
train() client id: f_00008-3-2 loss: 0.614380  [   96/  130]
train() client id: f_00008-3-3 loss: 0.778555  [  128/  130]
train() client id: f_00008-4-0 loss: 0.676514  [   32/  130]
train() client id: f_00008-4-1 loss: 0.635728  [   64/  130]
train() client id: f_00008-4-2 loss: 0.681917  [   96/  130]
train() client id: f_00008-4-3 loss: 0.785186  [  128/  130]
train() client id: f_00008-5-0 loss: 0.763262  [   32/  130]
train() client id: f_00008-5-1 loss: 0.602109  [   64/  130]
train() client id: f_00008-5-2 loss: 0.733746  [   96/  130]
train() client id: f_00008-5-3 loss: 0.719745  [  128/  130]
train() client id: f_00008-6-0 loss: 0.690936  [   32/  130]
train() client id: f_00008-6-1 loss: 0.837896  [   64/  130]
train() client id: f_00008-6-2 loss: 0.649470  [   96/  130]
train() client id: f_00008-6-3 loss: 0.645386  [  128/  130]
train() client id: f_00008-7-0 loss: 0.769020  [   32/  130]
train() client id: f_00008-7-1 loss: 0.717060  [   64/  130]
train() client id: f_00008-7-2 loss: 0.613147  [   96/  130]
train() client id: f_00008-7-3 loss: 0.737733  [  128/  130]
train() client id: f_00009-0-0 loss: 0.599224  [   32/  118]
train() client id: f_00009-0-1 loss: 0.782054  [   64/  118]
train() client id: f_00009-0-2 loss: 0.668038  [   96/  118]
train() client id: f_00009-1-0 loss: 0.682279  [   32/  118]
train() client id: f_00009-1-1 loss: 0.671292  [   64/  118]
train() client id: f_00009-1-2 loss: 0.691701  [   96/  118]
train() client id: f_00009-2-0 loss: 0.695494  [   32/  118]
train() client id: f_00009-2-1 loss: 0.769022  [   64/  118]
train() client id: f_00009-2-2 loss: 0.658235  [   96/  118]
train() client id: f_00009-3-0 loss: 0.697060  [   32/  118]
train() client id: f_00009-3-1 loss: 0.524823  [   64/  118]
train() client id: f_00009-3-2 loss: 0.769903  [   96/  118]
train() client id: f_00009-4-0 loss: 0.544920  [   32/  118]
train() client id: f_00009-4-1 loss: 0.721810  [   64/  118]
train() client id: f_00009-4-2 loss: 0.679121  [   96/  118]
train() client id: f_00009-5-0 loss: 0.520769  [   32/  118]
train() client id: f_00009-5-1 loss: 0.709745  [   64/  118]
train() client id: f_00009-5-2 loss: 0.688920  [   96/  118]
train() client id: f_00009-6-0 loss: 0.789581  [   32/  118]
train() client id: f_00009-6-1 loss: 0.568426  [   64/  118]
train() client id: f_00009-6-2 loss: 0.499585  [   96/  118]
train() client id: f_00009-7-0 loss: 0.501634  [   32/  118]
train() client id: f_00009-7-1 loss: 0.630723  [   64/  118]
train() client id: f_00009-7-2 loss: 0.580536  [   96/  118]
At round 68 accuracy: 0.6445623342175066
At round 68 training accuracy: 0.5888665325285044
At round 68 training loss: 0.8240750427693693
update_location
xs = [  -3.9056584     4.20031788  360.00902392   18.81129433    0.97929623
    3.95640986 -322.44319194 -301.32485185  344.66397685 -287.06087855]
ys = [ 352.5879595   335.55583871    1.32061395 -322.45517586  314.35018685
  297.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [366.51537942 350.16476631 373.64186238 338.12897722 329.87421692
 314.17975995 337.60406184 317.48597228 359.3075659  304.00651284]
dists_bs = [246.59837202 240.60534339 562.4937198  533.87683018 224.47090518
 217.04541519 230.84655055 215.16464081 542.987262   204.44591885]
uav_gains = [9.97297579e-13 1.20746354e-12 9.24956572e-13 1.41687733e-12
 1.59815306e-12 2.06205032e-12 1.42738234e-12 1.94864202e-12
 1.08135149e-12 2.47729216e-12]
bs_gains = [2.21677447e-11 2.37486723e-11 2.20274684e-12 2.54952386e-12
 2.88433648e-11 3.16921979e-11 2.66678863e-11 3.24739829e-11
 2.43154935e-12 3.74692015e-11]
Round 69
-------------------------------
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.8731924  3.7097278  1.85116443 0.70130907 4.2759113  2.0588358
 0.85327763 2.56982483 1.88245466 1.66965903]
obj_prev = 21.445356959883735
eta_min = 1.2228941532952692e-50	eta_max = 0.9500254409851863
af = 4.439896748873053	bf = 0.784423253509249	zeta = 4.883886423760359	eta = 0.9090909090909091
af = 4.439896748873053	bf = 0.784423253509249	zeta = 13.283496720526747	eta = 0.3342415662294824
af = 4.439896748873053	bf = 0.784423253509249	zeta = 8.486662978303045	eta = 0.5231616667498248
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.677394178566005	eta = 0.5783077754778437
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.626245838238497	eta = 0.5821864181995182
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.626008957353865	eta = 0.5822045022110287
eta = 0.5822045022110287
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [0.04463654 0.09387843 0.04392802 0.0152331  0.10840302 0.0517217
 0.01912994 0.06341222 0.04605357 0.04180247]
ene_total = [0.78877956 1.11412037 0.79451743 0.41620728 1.26807169 0.64932725
 0.45766691 0.90423052 0.69485736 0.53823058]
ti_comp = [2.16111529 2.35459485 2.14875789 2.20941898 2.35838257 2.36010268
 2.21029221 2.24279005 2.26158603 2.36299155]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [1.19013336e-06 9.32706721e-06 1.14743992e-06 4.52573450e-08
 1.43144819e-05 1.55251797e-06 8.95615827e-08 3.16826659e-06
 1.19355847e-06 8.17638912e-07]
ene_total = [0.30300549 0.09252695 0.3164538  0.25042331 0.088459   0.08644808
 0.24947344 0.214139   0.19366152 0.08329608]
optimize_network_iter = 0 obj = 1.8778866839980972
eta = 0.5822045022110287
freqs = [10327199.39136077 19935155.93888518 10221724.11669465  3447309.92299876
 22982492.36545908 10957510.88478662  4327469.55320673 14136905.79686903
 10181697.58740329  8845243.50076053]
eta_min = 0.5822045022110292	eta_max = 0.7888487440198617
af = 0.000276093861788328	bf = 0.784423253509249	zeta = 0.00030370324796716086	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [2.09696425e-07 1.64338949e-06 2.02174023e-07 7.97415130e-09
 2.52215070e-06 2.73547049e-07 1.57803692e-08 5.58235069e-07
 2.10299915e-07 1.44064491e-07]
ene_total = [1.3924886  0.42484648 1.45429544 1.1508812  0.40594569 0.39722982
 1.14651401 0.98399884 0.88997085 0.38277432]
ti_comp = [0.9545136  1.14799316 0.9421562  1.00281729 1.15178087 1.15350099
 1.00369052 1.03618836 1.05498434 1.15638985]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [5.14272772e-07 3.30754040e-06 5.03112813e-07 1.85185886e-08
 5.05907005e-06 5.47858951e-07 3.66123645e-08 1.25120311e-06
 4.62364171e-07 2.87795246e-07]
ene_total = [0.59952879 0.1829493  0.62613904 0.49550078 0.17483054 0.17102929
 0.49362075 0.42366592 0.3831736  0.1648028 ]
optimize_network_iter = 1 obj = 3.715240812383447
eta = 0.7888487440198617
freqs = [10252122.85547994 17928004.42999087 10221724.11669466  3330213.08308708
 20633691.92866442  9830152.63900685  4178489.52262742 13416514.38117568
  9570237.57402436  7925070.10389261]
eta_min = 0.7888487440198628	eta_max = 0.7888487440198605
af = 0.00022982602808494754	bf = 0.784423253509249	zeta = 0.0002528086308934423	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [2.06658611e-07 1.32912288e-06 2.02174023e-07 7.44162632e-09
 2.03296858e-06 2.20155092e-07 1.47125432e-08 5.02791340e-07
 1.85799332e-07 1.15649455e-07]
ene_total = [1.39248845 0.42483077 1.45429544 1.15088117 0.40592122 0.39722715
 1.14651395 0.98399607 0.88996963 0.38277289]
ti_comp = [0.9545136  1.14799316 0.9421562  1.00281729 1.15178087 1.15350099
 1.00369052 1.03618836 1.05498434 1.15638985]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [5.14272772e-07 3.30754040e-06 5.03112813e-07 1.85185886e-08
 5.05907005e-06 5.47858951e-07 3.66123645e-08 1.25120311e-06
 4.62364171e-07 2.87795246e-07]
ene_total = [0.59952879 0.1829493  0.62613904 0.49550078 0.17483054 0.17102929
 0.49362075 0.42366592 0.3831736  0.1648028 ]
optimize_network_iter = 2 obj = 3.715240812383425
eta = 0.7888487440198605
freqs = [10252122.85547993 17928004.42999088 10221724.11669465  3330213.08308708
 20633691.92866442  9830152.63900685  4178489.52262742 13416514.38117567
  9570237.57402436  7925070.10389262]
Done!
At round 69 eta: 0.7888487440198605
At round 69 local rounds: 7.766500565656417
At round 69 global rounds: 21.53403916195109
At round 69 a_n: 4.204393568402164
gradient difference: 0.6385729908943176
train() client id: f_00000-0-0 loss: 1.001354  [   32/  126]
train() client id: f_00000-0-1 loss: 1.005449  [   64/  126]
train() client id: f_00000-0-2 loss: 0.780810  [   96/  126]
train() client id: f_00000-1-0 loss: 0.796219  [   32/  126]
train() client id: f_00000-1-1 loss: 1.051940  [   64/  126]
train() client id: f_00000-1-2 loss: 0.819423  [   96/  126]
train() client id: f_00000-2-0 loss: 0.820179  [   32/  126]
train() client id: f_00000-2-1 loss: 0.929712  [   64/  126]
train() client id: f_00000-2-2 loss: 0.784409  [   96/  126]
train() client id: f_00000-3-0 loss: 0.866960  [   32/  126]
train() client id: f_00000-3-1 loss: 0.786398  [   64/  126]
train() client id: f_00000-3-2 loss: 0.801709  [   96/  126]
train() client id: f_00000-4-0 loss: 0.982734  [   32/  126]
train() client id: f_00000-4-1 loss: 0.617384  [   64/  126]
train() client id: f_00000-4-2 loss: 0.822925  [   96/  126]
train() client id: f_00000-5-0 loss: 0.659906  [   32/  126]
train() client id: f_00000-5-1 loss: 0.750011  [   64/  126]
train() client id: f_00000-5-2 loss: 0.861617  [   96/  126]
train() client id: f_00000-6-0 loss: 0.904804  [   32/  126]
train() client id: f_00000-6-1 loss: 0.757567  [   64/  126]
train() client id: f_00000-6-2 loss: 0.763241  [   96/  126]
train() client id: f_00001-0-0 loss: 0.549289  [   32/  265]
train() client id: f_00001-0-1 loss: 0.418697  [   64/  265]
train() client id: f_00001-0-2 loss: 0.463310  [   96/  265]
train() client id: f_00001-0-3 loss: 0.456390  [  128/  265]
train() client id: f_00001-0-4 loss: 0.400881  [  160/  265]
train() client id: f_00001-0-5 loss: 0.650611  [  192/  265]
train() client id: f_00001-0-6 loss: 0.414307  [  224/  265]
train() client id: f_00001-0-7 loss: 0.597176  [  256/  265]
train() client id: f_00001-1-0 loss: 0.417147  [   32/  265]
train() client id: f_00001-1-1 loss: 0.525392  [   64/  265]
train() client id: f_00001-1-2 loss: 0.398671  [   96/  265]
train() client id: f_00001-1-3 loss: 0.618006  [  128/  265]
train() client id: f_00001-1-4 loss: 0.494531  [  160/  265]
train() client id: f_00001-1-5 loss: 0.473574  [  192/  265]
train() client id: f_00001-1-6 loss: 0.501476  [  224/  265]
train() client id: f_00001-1-7 loss: 0.473347  [  256/  265]
train() client id: f_00001-2-0 loss: 0.599849  [   32/  265]
train() client id: f_00001-2-1 loss: 0.407758  [   64/  265]
train() client id: f_00001-2-2 loss: 0.370465  [   96/  265]
train() client id: f_00001-2-3 loss: 0.554404  [  128/  265]
train() client id: f_00001-2-4 loss: 0.515155  [  160/  265]
train() client id: f_00001-2-5 loss: 0.454964  [  192/  265]
train() client id: f_00001-2-6 loss: 0.534561  [  224/  265]
train() client id: f_00001-2-7 loss: 0.441887  [  256/  265]
train() client id: f_00001-3-0 loss: 0.377604  [   32/  265]
train() client id: f_00001-3-1 loss: 0.441543  [   64/  265]
train() client id: f_00001-3-2 loss: 0.481908  [   96/  265]
train() client id: f_00001-3-3 loss: 0.482645  [  128/  265]
train() client id: f_00001-3-4 loss: 0.552814  [  160/  265]
train() client id: f_00001-3-5 loss: 0.559336  [  192/  265]
train() client id: f_00001-3-6 loss: 0.405417  [  224/  265]
train() client id: f_00001-3-7 loss: 0.546131  [  256/  265]
train() client id: f_00001-4-0 loss: 0.475665  [   32/  265]
train() client id: f_00001-4-1 loss: 0.485927  [   64/  265]
train() client id: f_00001-4-2 loss: 0.429477  [   96/  265]
train() client id: f_00001-4-3 loss: 0.449560  [  128/  265]
train() client id: f_00001-4-4 loss: 0.523408  [  160/  265]
train() client id: f_00001-4-5 loss: 0.475887  [  192/  265]
train() client id: f_00001-4-6 loss: 0.545018  [  224/  265]
train() client id: f_00001-4-7 loss: 0.452601  [  256/  265]
train() client id: f_00001-5-0 loss: 0.524847  [   32/  265]
train() client id: f_00001-5-1 loss: 0.531397  [   64/  265]
train() client id: f_00001-5-2 loss: 0.487061  [   96/  265]
train() client id: f_00001-5-3 loss: 0.473555  [  128/  265]
train() client id: f_00001-5-4 loss: 0.415580  [  160/  265]
train() client id: f_00001-5-5 loss: 0.386244  [  192/  265]
train() client id: f_00001-5-6 loss: 0.389736  [  224/  265]
train() client id: f_00001-5-7 loss: 0.608597  [  256/  265]
train() client id: f_00001-6-0 loss: 0.537239  [   32/  265]
train() client id: f_00001-6-1 loss: 0.347785  [   64/  265]
train() client id: f_00001-6-2 loss: 0.616931  [   96/  265]
train() client id: f_00001-6-3 loss: 0.428927  [  128/  265]
train() client id: f_00001-6-4 loss: 0.449689  [  160/  265]
train() client id: f_00001-6-5 loss: 0.560490  [  192/  265]
train() client id: f_00001-6-6 loss: 0.385400  [  224/  265]
train() client id: f_00001-6-7 loss: 0.491138  [  256/  265]
train() client id: f_00002-0-0 loss: 1.202484  [   32/  124]
train() client id: f_00002-0-1 loss: 0.925383  [   64/  124]
train() client id: f_00002-0-2 loss: 1.181364  [   96/  124]
train() client id: f_00002-1-0 loss: 0.925397  [   32/  124]
train() client id: f_00002-1-1 loss: 1.186339  [   64/  124]
train() client id: f_00002-1-2 loss: 1.206957  [   96/  124]
train() client id: f_00002-2-0 loss: 1.037892  [   32/  124]
train() client id: f_00002-2-1 loss: 1.046159  [   64/  124]
train() client id: f_00002-2-2 loss: 1.076016  [   96/  124]
train() client id: f_00002-3-0 loss: 0.997933  [   32/  124]
train() client id: f_00002-3-1 loss: 1.031954  [   64/  124]
train() client id: f_00002-3-2 loss: 0.949733  [   96/  124]
train() client id: f_00002-4-0 loss: 0.978175  [   32/  124]
train() client id: f_00002-4-1 loss: 1.220170  [   64/  124]
train() client id: f_00002-4-2 loss: 1.006705  [   96/  124]
train() client id: f_00002-5-0 loss: 0.994148  [   32/  124]
train() client id: f_00002-5-1 loss: 1.172321  [   64/  124]
train() client id: f_00002-5-2 loss: 0.893764  [   96/  124]
train() client id: f_00002-6-0 loss: 0.904397  [   32/  124]
train() client id: f_00002-6-1 loss: 1.043927  [   64/  124]
train() client id: f_00002-6-2 loss: 1.149198  [   96/  124]
train() client id: f_00003-0-0 loss: 0.512187  [   32/   43]
train() client id: f_00003-1-0 loss: 0.477509  [   32/   43]
train() client id: f_00003-2-0 loss: 0.382012  [   32/   43]
train() client id: f_00003-3-0 loss: 0.624473  [   32/   43]
train() client id: f_00003-4-0 loss: 0.445286  [   32/   43]
train() client id: f_00003-5-0 loss: 0.377407  [   32/   43]
train() client id: f_00003-6-0 loss: 0.415131  [   32/   43]
train() client id: f_00004-0-0 loss: 0.951383  [   32/  306]
train() client id: f_00004-0-1 loss: 0.768001  [   64/  306]
train() client id: f_00004-0-2 loss: 1.015308  [   96/  306]
train() client id: f_00004-0-3 loss: 0.806020  [  128/  306]
train() client id: f_00004-0-4 loss: 0.855040  [  160/  306]
train() client id: f_00004-0-5 loss: 0.908833  [  192/  306]
train() client id: f_00004-0-6 loss: 0.877117  [  224/  306]
train() client id: f_00004-0-7 loss: 0.885546  [  256/  306]
train() client id: f_00004-0-8 loss: 0.856370  [  288/  306]
train() client id: f_00004-1-0 loss: 0.905283  [   32/  306]
train() client id: f_00004-1-1 loss: 0.928906  [   64/  306]
train() client id: f_00004-1-2 loss: 0.799108  [   96/  306]
train() client id: f_00004-1-3 loss: 0.946805  [  128/  306]
train() client id: f_00004-1-4 loss: 0.650648  [  160/  306]
train() client id: f_00004-1-5 loss: 0.859625  [  192/  306]
train() client id: f_00004-1-6 loss: 0.695126  [  224/  306]
train() client id: f_00004-1-7 loss: 0.917195  [  256/  306]
train() client id: f_00004-1-8 loss: 0.983500  [  288/  306]
train() client id: f_00004-2-0 loss: 0.899094  [   32/  306]
train() client id: f_00004-2-1 loss: 0.748505  [   64/  306]
train() client id: f_00004-2-2 loss: 0.906841  [   96/  306]
train() client id: f_00004-2-3 loss: 0.878768  [  128/  306]
train() client id: f_00004-2-4 loss: 1.009097  [  160/  306]
train() client id: f_00004-2-5 loss: 0.861269  [  192/  306]
train() client id: f_00004-2-6 loss: 0.830823  [  224/  306]
train() client id: f_00004-2-7 loss: 0.957132  [  256/  306]
train() client id: f_00004-2-8 loss: 0.856620  [  288/  306]
train() client id: f_00004-3-0 loss: 0.768262  [   32/  306]
train() client id: f_00004-3-1 loss: 0.992626  [   64/  306]
train() client id: f_00004-3-2 loss: 0.824390  [   96/  306]
train() client id: f_00004-3-3 loss: 0.851123  [  128/  306]
train() client id: f_00004-3-4 loss: 0.848906  [  160/  306]
train() client id: f_00004-3-5 loss: 0.854024  [  192/  306]
train() client id: f_00004-3-6 loss: 0.936407  [  224/  306]
train() client id: f_00004-3-7 loss: 0.942994  [  256/  306]
train() client id: f_00004-3-8 loss: 0.859323  [  288/  306]
train() client id: f_00004-4-0 loss: 0.873415  [   32/  306]
train() client id: f_00004-4-1 loss: 0.917830  [   64/  306]
train() client id: f_00004-4-2 loss: 0.864267  [   96/  306]
train() client id: f_00004-4-3 loss: 0.990918  [  128/  306]
train() client id: f_00004-4-4 loss: 0.910503  [  160/  306]
train() client id: f_00004-4-5 loss: 0.773409  [  192/  306]
train() client id: f_00004-4-6 loss: 0.856511  [  224/  306]
train() client id: f_00004-4-7 loss: 0.868119  [  256/  306]
train() client id: f_00004-4-8 loss: 0.767709  [  288/  306]
train() client id: f_00004-5-0 loss: 0.823057  [   32/  306]
train() client id: f_00004-5-1 loss: 0.807728  [   64/  306]
train() client id: f_00004-5-2 loss: 0.806356  [   96/  306]
train() client id: f_00004-5-3 loss: 0.975377  [  128/  306]
train() client id: f_00004-5-4 loss: 0.955813  [  160/  306]
train() client id: f_00004-5-5 loss: 0.907870  [  192/  306]
train() client id: f_00004-5-6 loss: 0.796465  [  224/  306]
train() client id: f_00004-5-7 loss: 0.861888  [  256/  306]
train() client id: f_00004-5-8 loss: 0.857016  [  288/  306]
train() client id: f_00004-6-0 loss: 0.689409  [   32/  306]
train() client id: f_00004-6-1 loss: 0.778481  [   64/  306]
train() client id: f_00004-6-2 loss: 0.843573  [   96/  306]
train() client id: f_00004-6-3 loss: 0.752522  [  128/  306]
train() client id: f_00004-6-4 loss: 0.996127  [  160/  306]
train() client id: f_00004-6-5 loss: 0.756076  [  192/  306]
train() client id: f_00004-6-6 loss: 1.027708  [  224/  306]
train() client id: f_00004-6-7 loss: 1.071428  [  256/  306]
train() client id: f_00004-6-8 loss: 0.941157  [  288/  306]
train() client id: f_00005-0-0 loss: 0.831743  [   32/  146]
train() client id: f_00005-0-1 loss: 1.068686  [   64/  146]
train() client id: f_00005-0-2 loss: 0.620203  [   96/  146]
train() client id: f_00005-0-3 loss: 0.702551  [  128/  146]
train() client id: f_00005-1-0 loss: 0.745292  [   32/  146]
train() client id: f_00005-1-1 loss: 0.781995  [   64/  146]
train() client id: f_00005-1-2 loss: 0.946369  [   96/  146]
train() client id: f_00005-1-3 loss: 0.687179  [  128/  146]
train() client id: f_00005-2-0 loss: 1.082221  [   32/  146]
train() client id: f_00005-2-1 loss: 0.676515  [   64/  146]
train() client id: f_00005-2-2 loss: 0.668927  [   96/  146]
train() client id: f_00005-2-3 loss: 0.892384  [  128/  146]
train() client id: f_00005-3-0 loss: 0.887372  [   32/  146]
train() client id: f_00005-3-1 loss: 1.096514  [   64/  146]
train() client id: f_00005-3-2 loss: 0.642064  [   96/  146]
train() client id: f_00005-3-3 loss: 0.773301  [  128/  146]
train() client id: f_00005-4-0 loss: 0.775160  [   32/  146]
train() client id: f_00005-4-1 loss: 0.554909  [   64/  146]
train() client id: f_00005-4-2 loss: 1.116998  [   96/  146]
train() client id: f_00005-4-3 loss: 0.978461  [  128/  146]
train() client id: f_00005-5-0 loss: 1.012443  [   32/  146]
train() client id: f_00005-5-1 loss: 0.841833  [   64/  146]
train() client id: f_00005-5-2 loss: 0.669779  [   96/  146]
train() client id: f_00005-5-3 loss: 0.820958  [  128/  146]
train() client id: f_00005-6-0 loss: 0.756576  [   32/  146]
train() client id: f_00005-6-1 loss: 0.994270  [   64/  146]
train() client id: f_00005-6-2 loss: 0.842971  [   96/  146]
train() client id: f_00005-6-3 loss: 0.921187  [  128/  146]
train() client id: f_00006-0-0 loss: 0.496367  [   32/   54]
train() client id: f_00006-1-0 loss: 0.425913  [   32/   54]
train() client id: f_00006-2-0 loss: 0.497358  [   32/   54]
train() client id: f_00006-3-0 loss: 0.483355  [   32/   54]
train() client id: f_00006-4-0 loss: 0.387342  [   32/   54]
train() client id: f_00006-5-0 loss: 0.437964  [   32/   54]
train() client id: f_00006-6-0 loss: 0.463401  [   32/   54]
train() client id: f_00007-0-0 loss: 0.681092  [   32/  179]
train() client id: f_00007-0-1 loss: 0.644134  [   64/  179]
train() client id: f_00007-0-2 loss: 0.717745  [   96/  179]
train() client id: f_00007-0-3 loss: 0.778870  [  128/  179]
train() client id: f_00007-0-4 loss: 0.640635  [  160/  179]
train() client id: f_00007-1-0 loss: 0.548890  [   32/  179]
train() client id: f_00007-1-1 loss: 0.781607  [   64/  179]
train() client id: f_00007-1-2 loss: 0.738278  [   96/  179]
train() client id: f_00007-1-3 loss: 0.819142  [  128/  179]
train() client id: f_00007-1-4 loss: 0.553611  [  160/  179]
train() client id: f_00007-2-0 loss: 0.562942  [   32/  179]
train() client id: f_00007-2-1 loss: 0.666980  [   64/  179]
train() client id: f_00007-2-2 loss: 0.872114  [   96/  179]
train() client id: f_00007-2-3 loss: 0.570054  [  128/  179]
train() client id: f_00007-2-4 loss: 0.662428  [  160/  179]
train() client id: f_00007-3-0 loss: 0.797233  [   32/  179]
train() client id: f_00007-3-1 loss: 0.629608  [   64/  179]
train() client id: f_00007-3-2 loss: 0.522955  [   96/  179]
train() client id: f_00007-3-3 loss: 0.704867  [  128/  179]
train() client id: f_00007-3-4 loss: 0.632344  [  160/  179]
train() client id: f_00007-4-0 loss: 0.691233  [   32/  179]
train() client id: f_00007-4-1 loss: 0.742404  [   64/  179]
train() client id: f_00007-4-2 loss: 0.710348  [   96/  179]
train() client id: f_00007-4-3 loss: 0.606221  [  128/  179]
train() client id: f_00007-4-4 loss: 0.677652  [  160/  179]
train() client id: f_00007-5-0 loss: 0.661830  [   32/  179]
train() client id: f_00007-5-1 loss: 0.711776  [   64/  179]
train() client id: f_00007-5-2 loss: 0.531273  [   96/  179]
train() client id: f_00007-5-3 loss: 0.851675  [  128/  179]
train() client id: f_00007-5-4 loss: 0.639639  [  160/  179]
train() client id: f_00007-6-0 loss: 0.698195  [   32/  179]
train() client id: f_00007-6-1 loss: 0.632044  [   64/  179]
train() client id: f_00007-6-2 loss: 0.523718  [   96/  179]
train() client id: f_00007-6-3 loss: 0.880934  [  128/  179]
train() client id: f_00007-6-4 loss: 0.615483  [  160/  179]
train() client id: f_00008-0-0 loss: 0.702314  [   32/  130]
train() client id: f_00008-0-1 loss: 0.632392  [   64/  130]
train() client id: f_00008-0-2 loss: 0.652593  [   96/  130]
train() client id: f_00008-0-3 loss: 0.806433  [  128/  130]
train() client id: f_00008-1-0 loss: 0.725067  [   32/  130]
train() client id: f_00008-1-1 loss: 0.785209  [   64/  130]
train() client id: f_00008-1-2 loss: 0.594348  [   96/  130]
train() client id: f_00008-1-3 loss: 0.699199  [  128/  130]
train() client id: f_00008-2-0 loss: 0.771655  [   32/  130]
train() client id: f_00008-2-1 loss: 0.666607  [   64/  130]
train() client id: f_00008-2-2 loss: 0.664513  [   96/  130]
train() client id: f_00008-2-3 loss: 0.695090  [  128/  130]
train() client id: f_00008-3-0 loss: 0.671950  [   32/  130]
train() client id: f_00008-3-1 loss: 0.748383  [   64/  130]
train() client id: f_00008-3-2 loss: 0.671515  [   96/  130]
train() client id: f_00008-3-3 loss: 0.688343  [  128/  130]
train() client id: f_00008-4-0 loss: 0.715021  [   32/  130]
train() client id: f_00008-4-1 loss: 0.597582  [   64/  130]
train() client id: f_00008-4-2 loss: 0.711677  [   96/  130]
train() client id: f_00008-4-3 loss: 0.758981  [  128/  130]
train() client id: f_00008-5-0 loss: 0.763589  [   32/  130]
train() client id: f_00008-5-1 loss: 0.609813  [   64/  130]
train() client id: f_00008-5-2 loss: 0.741959  [   96/  130]
train() client id: f_00008-5-3 loss: 0.668228  [  128/  130]
train() client id: f_00008-6-0 loss: 0.783011  [   32/  130]
train() client id: f_00008-6-1 loss: 0.712379  [   64/  130]
train() client id: f_00008-6-2 loss: 0.683182  [   96/  130]
train() client id: f_00008-6-3 loss: 0.578927  [  128/  130]
train() client id: f_00009-0-0 loss: 1.061753  [   32/  118]
train() client id: f_00009-0-1 loss: 1.077939  [   64/  118]
train() client id: f_00009-0-2 loss: 0.834516  [   96/  118]
train() client id: f_00009-1-0 loss: 0.979373  [   32/  118]
train() client id: f_00009-1-1 loss: 0.959343  [   64/  118]
train() client id: f_00009-1-2 loss: 0.956154  [   96/  118]
train() client id: f_00009-2-0 loss: 1.060960  [   32/  118]
train() client id: f_00009-2-1 loss: 0.902430  [   64/  118]
train() client id: f_00009-2-2 loss: 0.819330  [   96/  118]
train() client id: f_00009-3-0 loss: 0.957980  [   32/  118]
train() client id: f_00009-3-1 loss: 0.945869  [   64/  118]
train() client id: f_00009-3-2 loss: 0.906821  [   96/  118]
train() client id: f_00009-4-0 loss: 0.978090  [   32/  118]
train() client id: f_00009-4-1 loss: 0.858475  [   64/  118]
train() client id: f_00009-4-2 loss: 0.829739  [   96/  118]
train() client id: f_00009-5-0 loss: 0.898498  [   32/  118]
train() client id: f_00009-5-1 loss: 0.969304  [   64/  118]
train() client id: f_00009-5-2 loss: 0.945583  [   96/  118]
train() client id: f_00009-6-0 loss: 1.091256  [   32/  118]
train() client id: f_00009-6-1 loss: 0.757325  [   64/  118]
train() client id: f_00009-6-2 loss: 0.866632  [   96/  118]
At round 69 accuracy: 0.6445623342175066
At round 69 training accuracy: 0.5881958417169685
At round 69 training loss: 0.815293293288646
update_location
xs = [  -3.9056584     4.20031788  365.00902392   18.81129433    0.97929623
    3.95640986 -327.44319194 -306.32485185  349.66397685 -292.06087855]
ys = [ 357.5879595   340.55583871    1.32061395 -327.45517586  319.35018685
  302.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [371.32789142 354.95904264 378.4617967  342.90050597 334.64234768
 318.92328712 342.382731   322.23530396 364.10653205 308.73219566]
dists_bs = [250.22317375 243.97026388 567.25150952 538.53878371 227.60863152
 219.91328691 234.07170231 218.13773441 547.77441202 207.22872031]
uav_gains = [9.47393004e-13 1.13831941e-12 8.81152900e-13 1.32700416e-12
 1.48913730e-12 1.90221719e-12 1.33628878e-12 1.80140553e-12
 1.02406515e-12 2.27098709e-12]
bs_gains = [2.12802637e-11 2.28428757e-11 2.15140531e-12 2.48820711e-12
 2.77437810e-11 3.05485056e-11 2.56517576e-11 3.12498438e-11
 2.37251633e-12 3.60773175e-11]
Round 70
-------------------------------
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.73561523 3.43059311 1.71527241 0.65186493 3.95409396 1.90401388
 0.79238643 2.37939759 1.74146138 1.54414415]
obj_prev = 19.848843077614966
eta_min = 1.1829730818327539e-54	eta_max = 0.9523787254678187
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 4.515956513396188	eta = 0.9090909090909091
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 12.490584420921488	eta = 0.32868077856324013
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.913053448586266	eta = 0.518815529157309
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.145329232998529	eta = 0.5745592509885677
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.096743771047384	eta = 0.5784927770574487
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.096517585671171	eta = 0.5785112152005008
eta = 0.5785112152005008
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [0.04516167 0.09498287 0.04444481 0.01541231 0.10967833 0.05233018
 0.019355   0.06415824 0.04659537 0.04229426]
ene_total = [0.7365201  1.0329747  0.74175636 0.39122836 1.17571715 0.60187953
 0.42968967 0.84367489 0.64421599 0.49886084]
ti_comp = [2.36984321 2.57086067 2.35742536 2.41850329 2.57471655 2.57650502
 2.41937284 2.45238669 2.47671475 2.57942159]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.02506317e-06 8.10324115e-06 9.87341466e-07 3.91192234e-08
 1.24389480e-05 1.34919525e-06 7.74201308e-08 2.74447586e-06
 1.03075701e-06 7.10688982e-07]
ene_total = [0.28603811 0.08559175 0.29842467 0.23748935 0.08178872 0.07989408
 0.23662235 0.2037173  0.17943272 0.0769784 ]
optimize_network_iter = 0 obj = 1.7659774351917215
eta = 0.5785112152005008
freqs = [ 9528407.84474552 18472970.62942295  9426557.92578857  3186333.13230265
 21299107.60121082 10155265.18397391  4000003.44569053 13080775.34726803
  9406688.61857887  8198399.6009876 ]
eta_min = 0.578511215200502	eta_max = 0.8002666857305597
af = 0.00021898789766085596	bf = 0.7442198316471329	zeta = 0.00024088668742694157	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.78511656e-07 1.41115497e-06 1.71942534e-07 6.81249461e-09
 2.16620522e-06 2.34958278e-07 1.34824820e-08 4.77942182e-07
 1.79503221e-07 1.23764340e-07]
ene_total = [1.32614126 0.39652609 1.38357144 1.10108858 0.3787282  0.37036751
 1.09706736 0.9444053  0.83187829 0.35687372]
ti_comp = [0.9721494  1.17316687 0.95973155 1.02080948 1.17702275 1.17881122
 1.02167903 1.05469288 1.07902094 1.18172779]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [4.11016540e-07 2.62561918e-06 4.01956633e-07 1.48159555e-08
 4.01613458e-06 4.34894323e-07 2.92931455e-08 1.00119980e-06
 3.66424074e-07 2.28467704e-07]
ene_total = [0.60360123 0.18050535 0.62974066 0.50116324 0.17241798 0.16857786
 0.49933314 0.42985932 0.37863531 0.16243412]
optimize_network_iter = 1 obj = 3.726268206640733
eta = 0.8002666857305597
freqs = [ 9456245.97160665 16480383.53188894  9426557.92578857  3073300.22792621
 18967836.56712048  9036283.02009842  3856208.50116259 12382497.36052094
  8790118.03381017  7285272.29487025]
eta_min = 0.800266685730563	eta_max = 0.8002666857305585
af = 0.00017982620690873804	bf = 0.7442198316471329	zeta = 0.00019780882759961186	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.75818035e-07 1.12314508e-06 1.71942534e-07 6.33773081e-09
 1.71795736e-06 1.86032089e-07 1.25305500e-08 4.28277123e-07
 1.56742989e-07 9.77302347e-08]
ene_total = [1.32614114 0.39651277 1.38357144 1.10108855 0.37870747 0.37036524
 1.09706732 0.944403   0.83187723 0.35687251]
ti_comp = [0.9721494  1.17316687 0.95973155 1.02080948 1.17702275 1.17881122
 1.02167903 1.05469288 1.07902094 1.18172779]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [4.11016540e-07 2.62561918e-06 4.01956633e-07 1.48159555e-08
 4.01613458e-06 4.34894323e-07 2.92931455e-08 1.00119980e-06
 3.66424074e-07 2.28467704e-07]
ene_total = [0.60360123 0.18050535 0.62974066 0.50116324 0.17241798 0.16857786
 0.49933314 0.42985932 0.37863531 0.16243412]
optimize_network_iter = 2 obj = 3.7262682066407105
eta = 0.8002666857305585
freqs = [ 9456245.97160664 16480383.53188894  9426557.92578855  3073300.22792621
 18967836.56712048  9036283.02009842  3856208.50116259 12382497.36052093
  8790118.03381016  7285272.29487025]
Done!
At round 70 eta: 0.8002666857305585
At round 70 local rounds: 7.295939587071601
At round 70 global rounds: 21.05003656390746
At round 70 a_n: 3.8618477214328486
gradient difference: 0.6512014865875244
train() client id: f_00000-0-0 loss: 1.146725  [   32/  126]
train() client id: f_00000-0-1 loss: 1.172877  [   64/  126]
train() client id: f_00000-0-2 loss: 1.069522  [   96/  126]
train() client id: f_00000-1-0 loss: 1.242842  [   32/  126]
train() client id: f_00000-1-1 loss: 0.963800  [   64/  126]
train() client id: f_00000-1-2 loss: 0.905281  [   96/  126]
train() client id: f_00000-2-0 loss: 0.996200  [   32/  126]
train() client id: f_00000-2-1 loss: 0.886920  [   64/  126]
train() client id: f_00000-2-2 loss: 1.053851  [   96/  126]
train() client id: f_00000-3-0 loss: 1.006052  [   32/  126]
train() client id: f_00000-3-1 loss: 0.893056  [   64/  126]
train() client id: f_00000-3-2 loss: 0.996255  [   96/  126]
train() client id: f_00000-4-0 loss: 0.931030  [   32/  126]
train() client id: f_00000-4-1 loss: 0.807382  [   64/  126]
train() client id: f_00000-4-2 loss: 0.928291  [   96/  126]
train() client id: f_00000-5-0 loss: 0.948767  [   32/  126]
train() client id: f_00000-5-1 loss: 0.784625  [   64/  126]
train() client id: f_00000-5-2 loss: 0.948946  [   96/  126]
train() client id: f_00000-6-0 loss: 0.942616  [   32/  126]
train() client id: f_00000-6-1 loss: 0.843241  [   64/  126]
train() client id: f_00000-6-2 loss: 0.960986  [   96/  126]
train() client id: f_00001-0-0 loss: 0.561428  [   32/  265]
train() client id: f_00001-0-1 loss: 0.397800  [   64/  265]
train() client id: f_00001-0-2 loss: 0.432001  [   96/  265]
train() client id: f_00001-0-3 loss: 0.513833  [  128/  265]
train() client id: f_00001-0-4 loss: 0.504583  [  160/  265]
train() client id: f_00001-0-5 loss: 0.462531  [  192/  265]
train() client id: f_00001-0-6 loss: 0.531269  [  224/  265]
train() client id: f_00001-0-7 loss: 0.580364  [  256/  265]
train() client id: f_00001-1-0 loss: 0.506982  [   32/  265]
train() client id: f_00001-1-1 loss: 0.404096  [   64/  265]
train() client id: f_00001-1-2 loss: 0.560420  [   96/  265]
train() client id: f_00001-1-3 loss: 0.648655  [  128/  265]
train() client id: f_00001-1-4 loss: 0.534931  [  160/  265]
train() client id: f_00001-1-5 loss: 0.415939  [  192/  265]
train() client id: f_00001-1-6 loss: 0.467006  [  224/  265]
train() client id: f_00001-1-7 loss: 0.393366  [  256/  265]
train() client id: f_00001-2-0 loss: 0.420767  [   32/  265]
train() client id: f_00001-2-1 loss: 0.518089  [   64/  265]
train() client id: f_00001-2-2 loss: 0.461415  [   96/  265]
train() client id: f_00001-2-3 loss: 0.482765  [  128/  265]
train() client id: f_00001-2-4 loss: 0.590325  [  160/  265]
train() client id: f_00001-2-5 loss: 0.487072  [  192/  265]
train() client id: f_00001-2-6 loss: 0.469908  [  224/  265]
train() client id: f_00001-2-7 loss: 0.458878  [  256/  265]
train() client id: f_00001-3-0 loss: 0.435608  [   32/  265]
train() client id: f_00001-3-1 loss: 0.404630  [   64/  265]
train() client id: f_00001-3-2 loss: 0.374911  [   96/  265]
train() client id: f_00001-3-3 loss: 0.492004  [  128/  265]
train() client id: f_00001-3-4 loss: 0.574869  [  160/  265]
train() client id: f_00001-3-5 loss: 0.516204  [  192/  265]
train() client id: f_00001-3-6 loss: 0.432737  [  224/  265]
train() client id: f_00001-3-7 loss: 0.534415  [  256/  265]
train() client id: f_00001-4-0 loss: 0.493640  [   32/  265]
train() client id: f_00001-4-1 loss: 0.424623  [   64/  265]
train() client id: f_00001-4-2 loss: 0.482596  [   96/  265]
train() client id: f_00001-4-3 loss: 0.482472  [  128/  265]
train() client id: f_00001-4-4 loss: 0.577602  [  160/  265]
train() client id: f_00001-4-5 loss: 0.468329  [  192/  265]
train() client id: f_00001-4-6 loss: 0.433763  [  224/  265]
train() client id: f_00001-4-7 loss: 0.385837  [  256/  265]
train() client id: f_00001-5-0 loss: 0.474142  [   32/  265]
train() client id: f_00001-5-1 loss: 0.389671  [   64/  265]
train() client id: f_00001-5-2 loss: 0.520215  [   96/  265]
train() client id: f_00001-5-3 loss: 0.370757  [  128/  265]
train() client id: f_00001-5-4 loss: 0.385142  [  160/  265]
train() client id: f_00001-5-5 loss: 0.506716  [  192/  265]
train() client id: f_00001-5-6 loss: 0.476676  [  224/  265]
train() client id: f_00001-5-7 loss: 0.557539  [  256/  265]
train() client id: f_00001-6-0 loss: 0.433345  [   32/  265]
train() client id: f_00001-6-1 loss: 0.542077  [   64/  265]
train() client id: f_00001-6-2 loss: 0.447003  [   96/  265]
train() client id: f_00001-6-3 loss: 0.547074  [  128/  265]
train() client id: f_00001-6-4 loss: 0.386958  [  160/  265]
train() client id: f_00001-6-5 loss: 0.473009  [  192/  265]
train() client id: f_00001-6-6 loss: 0.522819  [  224/  265]
train() client id: f_00001-6-7 loss: 0.429806  [  256/  265]
train() client id: f_00002-0-0 loss: 0.780313  [   32/  124]
train() client id: f_00002-0-1 loss: 0.867661  [   64/  124]
train() client id: f_00002-0-2 loss: 0.865203  [   96/  124]
train() client id: f_00002-1-0 loss: 0.736566  [   32/  124]
train() client id: f_00002-1-1 loss: 0.712686  [   64/  124]
train() client id: f_00002-1-2 loss: 0.937790  [   96/  124]
train() client id: f_00002-2-0 loss: 0.682641  [   32/  124]
train() client id: f_00002-2-1 loss: 0.914561  [   64/  124]
train() client id: f_00002-2-2 loss: 0.875508  [   96/  124]
train() client id: f_00002-3-0 loss: 0.917367  [   32/  124]
train() client id: f_00002-3-1 loss: 0.883591  [   64/  124]
train() client id: f_00002-3-2 loss: 0.523438  [   96/  124]
train() client id: f_00002-4-0 loss: 0.806850  [   32/  124]
train() client id: f_00002-4-1 loss: 0.571267  [   64/  124]
train() client id: f_00002-4-2 loss: 0.834778  [   96/  124]
train() client id: f_00002-5-0 loss: 0.665166  [   32/  124]
train() client id: f_00002-5-1 loss: 0.804474  [   64/  124]
train() client id: f_00002-5-2 loss: 0.965886  [   96/  124]
train() client id: f_00002-6-0 loss: 0.992972  [   32/  124]
train() client id: f_00002-6-1 loss: 0.640551  [   64/  124]
train() client id: f_00002-6-2 loss: 0.636187  [   96/  124]
train() client id: f_00003-0-0 loss: 0.407569  [   32/   43]
train() client id: f_00003-1-0 loss: 0.878784  [   32/   43]
train() client id: f_00003-2-0 loss: 0.545685  [   32/   43]
train() client id: f_00003-3-0 loss: 0.616046  [   32/   43]
train() client id: f_00003-4-0 loss: 0.657852  [   32/   43]
train() client id: f_00003-5-0 loss: 0.707481  [   32/   43]
train() client id: f_00003-6-0 loss: 0.623985  [   32/   43]
train() client id: f_00004-0-0 loss: 0.783454  [   32/  306]
train() client id: f_00004-0-1 loss: 0.735602  [   64/  306]
train() client id: f_00004-0-2 loss: 0.928506  [   96/  306]
train() client id: f_00004-0-3 loss: 0.846383  [  128/  306]
train() client id: f_00004-0-4 loss: 0.640854  [  160/  306]
train() client id: f_00004-0-5 loss: 0.838807  [  192/  306]
train() client id: f_00004-0-6 loss: 0.674733  [  224/  306]
train() client id: f_00004-0-7 loss: 0.859394  [  256/  306]
train() client id: f_00004-0-8 loss: 0.830334  [  288/  306]
train() client id: f_00004-1-0 loss: 0.718013  [   32/  306]
train() client id: f_00004-1-1 loss: 0.764747  [   64/  306]
train() client id: f_00004-1-2 loss: 0.832509  [   96/  306]
train() client id: f_00004-1-3 loss: 0.660480  [  128/  306]
train() client id: f_00004-1-4 loss: 0.792321  [  160/  306]
train() client id: f_00004-1-5 loss: 0.912763  [  192/  306]
train() client id: f_00004-1-6 loss: 0.964579  [  224/  306]
train() client id: f_00004-1-7 loss: 0.674482  [  256/  306]
train() client id: f_00004-1-8 loss: 0.745045  [  288/  306]
train() client id: f_00004-2-0 loss: 0.880498  [   32/  306]
train() client id: f_00004-2-1 loss: 0.836979  [   64/  306]
train() client id: f_00004-2-2 loss: 0.722950  [   96/  306]
train() client id: f_00004-2-3 loss: 0.773642  [  128/  306]
train() client id: f_00004-2-4 loss: 0.643188  [  160/  306]
train() client id: f_00004-2-5 loss: 0.800761  [  192/  306]
train() client id: f_00004-2-6 loss: 0.739195  [  224/  306]
train() client id: f_00004-2-7 loss: 0.780985  [  256/  306]
train() client id: f_00004-2-8 loss: 0.891883  [  288/  306]
train() client id: f_00004-3-0 loss: 0.916217  [   32/  306]
train() client id: f_00004-3-1 loss: 0.689571  [   64/  306]
train() client id: f_00004-3-2 loss: 0.765307  [   96/  306]
train() client id: f_00004-3-3 loss: 0.732754  [  128/  306]
train() client id: f_00004-3-4 loss: 0.703546  [  160/  306]
train() client id: f_00004-3-5 loss: 0.725775  [  192/  306]
train() client id: f_00004-3-6 loss: 0.965369  [  224/  306]
train() client id: f_00004-3-7 loss: 0.791027  [  256/  306]
train() client id: f_00004-3-8 loss: 0.842531  [  288/  306]
train() client id: f_00004-4-0 loss: 0.645136  [   32/  306]
train() client id: f_00004-4-1 loss: 0.879209  [   64/  306]
train() client id: f_00004-4-2 loss: 0.752273  [   96/  306]
train() client id: f_00004-4-3 loss: 0.839828  [  128/  306]
train() client id: f_00004-4-4 loss: 0.917067  [  160/  306]
train() client id: f_00004-4-5 loss: 0.658402  [  192/  306]
train() client id: f_00004-4-6 loss: 0.824064  [  224/  306]
train() client id: f_00004-4-7 loss: 0.799485  [  256/  306]
train() client id: f_00004-4-8 loss: 0.761875  [  288/  306]
train() client id: f_00004-5-0 loss: 0.774486  [   32/  306]
train() client id: f_00004-5-1 loss: 0.623801  [   64/  306]
train() client id: f_00004-5-2 loss: 0.813838  [   96/  306]
train() client id: f_00004-5-3 loss: 0.723876  [  128/  306]
train() client id: f_00004-5-4 loss: 0.969369  [  160/  306]
train() client id: f_00004-5-5 loss: 0.630362  [  192/  306]
train() client id: f_00004-5-6 loss: 0.808566  [  224/  306]
train() client id: f_00004-5-7 loss: 0.833663  [  256/  306]
train() client id: f_00004-5-8 loss: 0.883137  [  288/  306]
train() client id: f_00004-6-0 loss: 0.977367  [   32/  306]
train() client id: f_00004-6-1 loss: 0.714259  [   64/  306]
train() client id: f_00004-6-2 loss: 0.596554  [   96/  306]
train() client id: f_00004-6-3 loss: 0.718180  [  128/  306]
train() client id: f_00004-6-4 loss: 0.871947  [  160/  306]
train() client id: f_00004-6-5 loss: 0.812778  [  192/  306]
train() client id: f_00004-6-6 loss: 0.758515  [  224/  306]
train() client id: f_00004-6-7 loss: 0.818730  [  256/  306]
train() client id: f_00004-6-8 loss: 0.788721  [  288/  306]
train() client id: f_00005-0-0 loss: 0.663778  [   32/  146]
train() client id: f_00005-0-1 loss: 0.633383  [   64/  146]
train() client id: f_00005-0-2 loss: 0.669891  [   96/  146]
train() client id: f_00005-0-3 loss: 0.416533  [  128/  146]
train() client id: f_00005-1-0 loss: 0.490225  [   32/  146]
train() client id: f_00005-1-1 loss: 0.765213  [   64/  146]
train() client id: f_00005-1-2 loss: 0.582061  [   96/  146]
train() client id: f_00005-1-3 loss: 0.640182  [  128/  146]
train() client id: f_00005-2-0 loss: 0.603783  [   32/  146]
train() client id: f_00005-2-1 loss: 0.796935  [   64/  146]
train() client id: f_00005-2-2 loss: 0.362263  [   96/  146]
train() client id: f_00005-2-3 loss: 0.652041  [  128/  146]
train() client id: f_00005-3-0 loss: 0.377574  [   32/  146]
train() client id: f_00005-3-1 loss: 0.254497  [   64/  146]
train() client id: f_00005-3-2 loss: 0.528361  [   96/  146]
train() client id: f_00005-3-3 loss: 0.910759  [  128/  146]
train() client id: f_00005-4-0 loss: 0.538630  [   32/  146]
train() client id: f_00005-4-1 loss: 0.446771  [   64/  146]
train() client id: f_00005-4-2 loss: 0.709828  [   96/  146]
train() client id: f_00005-4-3 loss: 0.696600  [  128/  146]
train() client id: f_00005-5-0 loss: 0.651131  [   32/  146]
train() client id: f_00005-5-1 loss: 0.767112  [   64/  146]
train() client id: f_00005-5-2 loss: 0.491255  [   96/  146]
train() client id: f_00005-5-3 loss: 0.584744  [  128/  146]
train() client id: f_00005-6-0 loss: 0.330250  [   32/  146]
train() client id: f_00005-6-1 loss: 0.713383  [   64/  146]
train() client id: f_00005-6-2 loss: 0.728293  [   96/  146]
train() client id: f_00005-6-3 loss: 0.538191  [  128/  146]
train() client id: f_00006-0-0 loss: 0.465303  [   32/   54]
train() client id: f_00006-1-0 loss: 0.474137  [   32/   54]
train() client id: f_00006-2-0 loss: 0.524269  [   32/   54]
train() client id: f_00006-3-0 loss: 0.448423  [   32/   54]
train() client id: f_00006-4-0 loss: 0.517107  [   32/   54]
train() client id: f_00006-5-0 loss: 0.519014  [   32/   54]
train() client id: f_00006-6-0 loss: 0.465005  [   32/   54]
train() client id: f_00007-0-0 loss: 0.497613  [   32/  179]
train() client id: f_00007-0-1 loss: 0.794943  [   64/  179]
train() client id: f_00007-0-2 loss: 0.690120  [   96/  179]
train() client id: f_00007-0-3 loss: 0.615148  [  128/  179]
train() client id: f_00007-0-4 loss: 0.729421  [  160/  179]
train() client id: f_00007-1-0 loss: 0.734130  [   32/  179]
train() client id: f_00007-1-1 loss: 0.896414  [   64/  179]
train() client id: f_00007-1-2 loss: 0.571689  [   96/  179]
train() client id: f_00007-1-3 loss: 0.528611  [  128/  179]
train() client id: f_00007-1-4 loss: 0.514383  [  160/  179]
train() client id: f_00007-2-0 loss: 0.680143  [   32/  179]
train() client id: f_00007-2-1 loss: 0.523168  [   64/  179]
train() client id: f_00007-2-2 loss: 0.686190  [   96/  179]
train() client id: f_00007-2-3 loss: 0.600167  [  128/  179]
train() client id: f_00007-2-4 loss: 0.533687  [  160/  179]
train() client id: f_00007-3-0 loss: 0.594317  [   32/  179]
train() client id: f_00007-3-1 loss: 0.538302  [   64/  179]
train() client id: f_00007-3-2 loss: 0.628242  [   96/  179]
train() client id: f_00007-3-3 loss: 0.696845  [  128/  179]
train() client id: f_00007-3-4 loss: 0.545718  [  160/  179]
train() client id: f_00007-4-0 loss: 0.479042  [   32/  179]
train() client id: f_00007-4-1 loss: 0.645154  [   64/  179]
train() client id: f_00007-4-2 loss: 0.824686  [   96/  179]
train() client id: f_00007-4-3 loss: 0.495086  [  128/  179]
train() client id: f_00007-4-4 loss: 0.741583  [  160/  179]
train() client id: f_00007-5-0 loss: 0.624809  [   32/  179]
train() client id: f_00007-5-1 loss: 0.493026  [   64/  179]
train() client id: f_00007-5-2 loss: 0.793560  [   96/  179]
train() client id: f_00007-5-3 loss: 0.797903  [  128/  179]
train() client id: f_00007-5-4 loss: 0.460606  [  160/  179]
train() client id: f_00007-6-0 loss: 0.572013  [   32/  179]
train() client id: f_00007-6-1 loss: 0.694009  [   64/  179]
train() client id: f_00007-6-2 loss: 0.536817  [   96/  179]
train() client id: f_00007-6-3 loss: 0.596003  [  128/  179]
train() client id: f_00007-6-4 loss: 0.648584  [  160/  179]
train() client id: f_00008-0-0 loss: 0.908825  [   32/  130]
train() client id: f_00008-0-1 loss: 0.619939  [   64/  130]
train() client id: f_00008-0-2 loss: 0.893467  [   96/  130]
train() client id: f_00008-0-3 loss: 0.703673  [  128/  130]
train() client id: f_00008-1-0 loss: 0.913369  [   32/  130]
train() client id: f_00008-1-1 loss: 0.898994  [   64/  130]
train() client id: f_00008-1-2 loss: 0.686201  [   96/  130]
train() client id: f_00008-1-3 loss: 0.640266  [  128/  130]
train() client id: f_00008-2-0 loss: 0.817378  [   32/  130]
train() client id: f_00008-2-1 loss: 0.635270  [   64/  130]
train() client id: f_00008-2-2 loss: 0.885778  [   96/  130]
train() client id: f_00008-2-3 loss: 0.768495  [  128/  130]
train() client id: f_00008-3-0 loss: 0.711241  [   32/  130]
train() client id: f_00008-3-1 loss: 0.858604  [   64/  130]
train() client id: f_00008-3-2 loss: 0.754485  [   96/  130]
train() client id: f_00008-3-3 loss: 0.792945  [  128/  130]
train() client id: f_00008-4-0 loss: 0.748130  [   32/  130]
train() client id: f_00008-4-1 loss: 0.840234  [   64/  130]
train() client id: f_00008-4-2 loss: 0.781321  [   96/  130]
train() client id: f_00008-4-3 loss: 0.708217  [  128/  130]
train() client id: f_00008-5-0 loss: 0.619323  [   32/  130]
train() client id: f_00008-5-1 loss: 0.896898  [   64/  130]
train() client id: f_00008-5-2 loss: 0.712764  [   96/  130]
train() client id: f_00008-5-3 loss: 0.895959  [  128/  130]
train() client id: f_00008-6-0 loss: 0.745414  [   32/  130]
train() client id: f_00008-6-1 loss: 0.742233  [   64/  130]
train() client id: f_00008-6-2 loss: 0.798073  [   96/  130]
train() client id: f_00008-6-3 loss: 0.840086  [  128/  130]
train() client id: f_00009-0-0 loss: 0.888703  [   32/  118]
train() client id: f_00009-0-1 loss: 0.803986  [   64/  118]
train() client id: f_00009-0-2 loss: 0.881143  [   96/  118]
train() client id: f_00009-1-0 loss: 0.971242  [   32/  118]
train() client id: f_00009-1-1 loss: 0.782783  [   64/  118]
train() client id: f_00009-1-2 loss: 0.848031  [   96/  118]
train() client id: f_00009-2-0 loss: 0.845531  [   32/  118]
train() client id: f_00009-2-1 loss: 0.730390  [   64/  118]
train() client id: f_00009-2-2 loss: 0.894480  [   96/  118]
train() client id: f_00009-3-0 loss: 0.849972  [   32/  118]
train() client id: f_00009-3-1 loss: 0.882507  [   64/  118]
train() client id: f_00009-3-2 loss: 0.916624  [   96/  118]
train() client id: f_00009-4-0 loss: 0.975663  [   32/  118]
train() client id: f_00009-4-1 loss: 0.766192  [   64/  118]
train() client id: f_00009-4-2 loss: 0.859967  [   96/  118]
train() client id: f_00009-5-0 loss: 0.787388  [   32/  118]
train() client id: f_00009-5-1 loss: 0.767294  [   64/  118]
train() client id: f_00009-5-2 loss: 0.768375  [   96/  118]
train() client id: f_00009-6-0 loss: 0.950198  [   32/  118]
train() client id: f_00009-6-1 loss: 0.724210  [   64/  118]
train() client id: f_00009-6-2 loss: 0.709149  [   96/  118]
At round 70 accuracy: 0.6472148541114059
At round 70 training accuracy: 0.5928906773977196
At round 70 training loss: 0.8090615612998349
update_location
xs = [  -3.9056584     4.20031788  370.00902392   18.81129433    0.97929623
    3.95640986 -332.44319194 -311.32485185  354.66397685 -297.06087855]
ys = [ 362.5879595   345.55583871    1.32061395 -332.45517586  324.35018685
  307.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [376.14529445 359.75891974 383.2863444  347.67845598 339.41715149
 323.67453495 347.16763445 326.99210944 368.91083807 313.46638962]
dists_bs = [253.89469525 247.39047687 572.01343104 543.20675007 230.81202527
 222.85644543 237.35836564 221.18390469 552.5653321  210.09367269]
uav_gains = [9.01731222e-13 1.07572493e-12 8.40891865e-13 1.24623615e-12
 1.39163386e-12 1.76012882e-12 1.25446710e-12 1.67039793e-12
 9.71887012e-13 2.08779148e-12]
bs_gains = [2.04297913e-11 2.19695797e-11 2.10163190e-12 2.42879938e-12
 2.66790571e-11 2.94322543e-11 2.46695578e-11 3.00596713e-11
 2.31536738e-12 3.47166460e-11]
Round 71
-------------------------------
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.59747704 3.1514096  1.57881481 0.60189948 3.63223305 1.7491537
 0.73097414 2.18851291 1.6003418  1.41859333]
obj_prev = 18.24940985510058
eta_min = 2.2153155362897003e-59	eta_max = 0.954951968660827
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 4.14802660303202	eta = 0.909090909090909
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 11.665880506041221	eta = 0.323244634087488
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 7.328813927254386	eta = 0.5145352730897301
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.605790167817666	eta = 0.5708527185521312
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.559990536518958	eta = 0.5748382188192438
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.55977636966569	eta = 0.5748569864243457
eta = 0.5748569864243457
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [0.04568455 0.09608258 0.0449594  0.01559076 0.11094819 0.05293607
 0.01957909 0.06490107 0.04713485 0.04278394]
ene_total = [0.68305847 0.95139002 0.68780539 0.3652074  1.08286069 0.55421391
 0.40065058 0.78194702 0.59332082 0.45932206]
ti_comp = [2.6166745  2.82525406 2.60419719 2.66565241 2.82917673 2.83103203
 2.66651706 2.69997525 2.72996789 2.83397532]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [8.70339434e-07 6.94542833e-06 8.37517236e-07 3.33330274e-08
 1.06640102e-05 1.15676577e-06 6.59734165e-08 2.34377520e-06
 8.78195905e-07 6.09439114e-07]
ene_total = [0.26808419 0.07867313 0.27941784 0.2235868  0.07514369 0.07337205
 0.22280168 0.19243014 0.16517258 0.0706935 ]
optimize_network_iter = 0 obj = 1.6493755938248962
eta = 0.5748569864243457
freqs = [ 8729505.68014144 17004237.96072038  8632103.28335727  2924379.60099508
 19607858.17168588  9349252.32733673  3671285.85880158 12018826.22316111
  8632858.47902665  7548397.32371458]
eta_min = 0.5748569864243458	eta_max = 0.8123930873707683
af = 0.00017023437323749214	bf = 0.7011262629418353	zeta = 0.00018725781056124136	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [1.49832217e-07 1.19568169e-06 1.44181752e-07 5.73840643e-09
 1.83584959e-06 1.99141592e-07 1.13575726e-08 4.03489744e-07
 1.51184738e-07 1.04917243e-07]
ene_total = [1.25368474 0.36767638 1.30668851 1.04561886 0.35103995 0.34308906
 1.04194602 0.8998312  0.77241077 0.33058187]
ti_comp = [0.98979546 1.19837503 0.97731815 1.03877337 1.2022977  1.20415299
 1.03963802 1.07309621 1.10308885 1.20709628]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [3.21501828e-07 2.04039780e-06 3.14308302e-07 1.16018294e-08
 3.12106616e-06 3.37954282e-07 2.29392754e-08 7.84231878e-07
 2.84296680e-07 1.77551736e-07]
ene_total = [0.60750428 0.17818314 0.63318841 0.50667793 0.17013065 0.16625427
 0.5048983  0.436041   0.37429151 0.16019228]
optimize_network_iter = 1 obj = 3.7373617634764136
eta = 0.8123930873707683
freqs = [ 8660759.82156696 15044717.2754607   8632103.28335727  2816297.66515272
 17315710.58589685  8249014.9990354   3533804.43880649 11348678.03144383
  8017957.73562188  6650755.80408876]
eta_min = 0.8123930873707704	eta_max = 0.8123930873707671
af = 0.0001378387179910577	bf = 0.7011262629418353	zeta = 0.0001516225897901635	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [1.47481618e-07 9.35985870e-07 1.44181752e-07 5.32207417e-09
 1.43171779e-06 1.55028805e-07 1.05228684e-08 3.59748455e-07
 1.30414606e-07 8.14478020e-08]
ene_total = [1.25368464 0.36766535 1.30668851 1.04561884 0.35102278 0.34308719
 1.04194599 0.89982934 0.77240989 0.33058088]
ti_comp = [0.98979546 1.19837503 0.97731815 1.03877337 1.2022977  1.20415299
 1.03963802 1.07309621 1.10308885 1.20709628]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [3.21501828e-07 2.04039780e-06 3.14308302e-07 1.16018294e-08
 3.12106616e-06 3.37954282e-07 2.29392754e-08 7.84231878e-07
 2.84296680e-07 1.77551736e-07]
ene_total = [0.60750428 0.17818314 0.63318841 0.50667793 0.17013065 0.16625427
 0.5048983  0.436041   0.37429151 0.16019228]
optimize_network_iter = 2 obj = 3.7373617634763896
eta = 0.8123930873707671
freqs = [ 8660759.82156695 15044717.2754607   8632103.28335726  2816297.66515272
 17315710.58589685  8249014.9990354   3533804.43880649 11348678.03144382
  8017957.73562187  6650755.80408876]
Done!
At round 71 eta: 0.8123930873707671
At round 71 local rounds: 6.803476774877765
At round 71 global rounds: 20.584783723108377
At round 71 a_n: 3.5193018744635296
gradient difference: 0.8473189473152161
train() client id: f_00000-0-0 loss: 0.696528  [   32/  126]
train() client id: f_00000-0-1 loss: 0.760898  [   64/  126]
train() client id: f_00000-0-2 loss: 0.930396  [   96/  126]
train() client id: f_00000-1-0 loss: 0.821798  [   32/  126]
train() client id: f_00000-1-1 loss: 1.034378  [   64/  126]
train() client id: f_00000-1-2 loss: 0.641927  [   96/  126]
train() client id: f_00000-2-0 loss: 0.862035  [   32/  126]
train() client id: f_00000-2-1 loss: 0.754804  [   64/  126]
train() client id: f_00000-2-2 loss: 0.874243  [   96/  126]
train() client id: f_00000-3-0 loss: 0.770775  [   32/  126]
train() client id: f_00000-3-1 loss: 0.870819  [   64/  126]
train() client id: f_00000-3-2 loss: 0.782830  [   96/  126]
train() client id: f_00000-4-0 loss: 0.899070  [   32/  126]
train() client id: f_00000-4-1 loss: 0.804125  [   64/  126]
train() client id: f_00000-4-2 loss: 0.687757  [   96/  126]
train() client id: f_00000-5-0 loss: 0.875390  [   32/  126]
train() client id: f_00000-5-1 loss: 0.637824  [   64/  126]
train() client id: f_00000-5-2 loss: 0.932180  [   96/  126]
train() client id: f_00001-0-0 loss: 0.565771  [   32/  265]
train() client id: f_00001-0-1 loss: 0.606148  [   64/  265]
train() client id: f_00001-0-2 loss: 0.512943  [   96/  265]
train() client id: f_00001-0-3 loss: 0.489240  [  128/  265]
train() client id: f_00001-0-4 loss: 0.643369  [  160/  265]
train() client id: f_00001-0-5 loss: 0.486449  [  192/  265]
train() client id: f_00001-0-6 loss: 0.452713  [  224/  265]
train() client id: f_00001-0-7 loss: 0.539759  [  256/  265]
train() client id: f_00001-1-0 loss: 0.437098  [   32/  265]
train() client id: f_00001-1-1 loss: 0.570805  [   64/  265]
train() client id: f_00001-1-2 loss: 0.714210  [   96/  265]
train() client id: f_00001-1-3 loss: 0.515332  [  128/  265]
train() client id: f_00001-1-4 loss: 0.604350  [  160/  265]
train() client id: f_00001-1-5 loss: 0.440236  [  192/  265]
train() client id: f_00001-1-6 loss: 0.473782  [  224/  265]
train() client id: f_00001-1-7 loss: 0.590022  [  256/  265]
train() client id: f_00001-2-0 loss: 0.570575  [   32/  265]
train() client id: f_00001-2-1 loss: 0.641443  [   64/  265]
train() client id: f_00001-2-2 loss: 0.517923  [   96/  265]
train() client id: f_00001-2-3 loss: 0.487607  [  128/  265]
train() client id: f_00001-2-4 loss: 0.475307  [  160/  265]
train() client id: f_00001-2-5 loss: 0.463368  [  192/  265]
train() client id: f_00001-2-6 loss: 0.507138  [  224/  265]
train() client id: f_00001-2-7 loss: 0.544320  [  256/  265]
train() client id: f_00001-3-0 loss: 0.520420  [   32/  265]
train() client id: f_00001-3-1 loss: 0.502450  [   64/  265]
train() client id: f_00001-3-2 loss: 0.521909  [   96/  265]
train() client id: f_00001-3-3 loss: 0.602430  [  128/  265]
train() client id: f_00001-3-4 loss: 0.434224  [  160/  265]
train() client id: f_00001-3-5 loss: 0.634110  [  192/  265]
train() client id: f_00001-3-6 loss: 0.506152  [  224/  265]
train() client id: f_00001-3-7 loss: 0.606082  [  256/  265]
train() client id: f_00001-4-0 loss: 0.633840  [   32/  265]
train() client id: f_00001-4-1 loss: 0.460164  [   64/  265]
train() client id: f_00001-4-2 loss: 0.441199  [   96/  265]
train() client id: f_00001-4-3 loss: 0.579370  [  128/  265]
train() client id: f_00001-4-4 loss: 0.445580  [  160/  265]
train() client id: f_00001-4-5 loss: 0.631420  [  192/  265]
train() client id: f_00001-4-6 loss: 0.527574  [  224/  265]
train() client id: f_00001-4-7 loss: 0.529858  [  256/  265]
train() client id: f_00001-5-0 loss: 0.708140  [   32/  265]
train() client id: f_00001-5-1 loss: 0.632587  [   64/  265]
train() client id: f_00001-5-2 loss: 0.455090  [   96/  265]
train() client id: f_00001-5-3 loss: 0.457591  [  128/  265]
train() client id: f_00001-5-4 loss: 0.441548  [  160/  265]
train() client id: f_00001-5-5 loss: 0.497189  [  192/  265]
train() client id: f_00001-5-6 loss: 0.658853  [  224/  265]
train() client id: f_00001-5-7 loss: 0.480420  [  256/  265]
train() client id: f_00002-0-0 loss: 1.139529  [   32/  124]
train() client id: f_00002-0-1 loss: 1.192318  [   64/  124]
train() client id: f_00002-0-2 loss: 1.352585  [   96/  124]
train() client id: f_00002-1-0 loss: 1.051845  [   32/  124]
train() client id: f_00002-1-1 loss: 1.393197  [   64/  124]
train() client id: f_00002-1-2 loss: 0.967964  [   96/  124]
train() client id: f_00002-2-0 loss: 0.961608  [   32/  124]
train() client id: f_00002-2-1 loss: 1.329078  [   64/  124]
train() client id: f_00002-2-2 loss: 1.091608  [   96/  124]
train() client id: f_00002-3-0 loss: 1.211702  [   32/  124]
train() client id: f_00002-3-1 loss: 1.289479  [   64/  124]
train() client id: f_00002-3-2 loss: 0.967643  [   96/  124]
train() client id: f_00002-4-0 loss: 1.107488  [   32/  124]
train() client id: f_00002-4-1 loss: 1.300798  [   64/  124]
train() client id: f_00002-4-2 loss: 1.126705  [   96/  124]
train() client id: f_00002-5-0 loss: 1.038402  [   32/  124]
train() client id: f_00002-5-1 loss: 1.175190  [   64/  124]
train() client id: f_00002-5-2 loss: 1.046797  [   96/  124]
train() client id: f_00003-0-0 loss: 0.907946  [   32/   43]
train() client id: f_00003-1-0 loss: 0.796611  [   32/   43]
train() client id: f_00003-2-0 loss: 0.727679  [   32/   43]
train() client id: f_00003-3-0 loss: 0.485160  [   32/   43]
train() client id: f_00003-4-0 loss: 0.703769  [   32/   43]
train() client id: f_00003-5-0 loss: 0.499641  [   32/   43]
train() client id: f_00004-0-0 loss: 0.742513  [   32/  306]
train() client id: f_00004-0-1 loss: 0.908972  [   64/  306]
train() client id: f_00004-0-2 loss: 0.698544  [   96/  306]
train() client id: f_00004-0-3 loss: 0.944588  [  128/  306]
train() client id: f_00004-0-4 loss: 1.009158  [  160/  306]
train() client id: f_00004-0-5 loss: 0.652867  [  192/  306]
train() client id: f_00004-0-6 loss: 0.775650  [  224/  306]
train() client id: f_00004-0-7 loss: 0.813496  [  256/  306]
train() client id: f_00004-0-8 loss: 0.688110  [  288/  306]
train() client id: f_00004-1-0 loss: 0.773680  [   32/  306]
train() client id: f_00004-1-1 loss: 0.772170  [   64/  306]
train() client id: f_00004-1-2 loss: 0.793115  [   96/  306]
train() client id: f_00004-1-3 loss: 0.737535  [  128/  306]
train() client id: f_00004-1-4 loss: 0.972159  [  160/  306]
train() client id: f_00004-1-5 loss: 0.827076  [  192/  306]
train() client id: f_00004-1-6 loss: 0.819948  [  224/  306]
train() client id: f_00004-1-7 loss: 0.757052  [  256/  306]
train() client id: f_00004-1-8 loss: 0.896190  [  288/  306]
train() client id: f_00004-2-0 loss: 0.718007  [   32/  306]
train() client id: f_00004-2-1 loss: 0.775651  [   64/  306]
train() client id: f_00004-2-2 loss: 0.694994  [   96/  306]
train() client id: f_00004-2-3 loss: 0.871258  [  128/  306]
train() client id: f_00004-2-4 loss: 0.795451  [  160/  306]
train() client id: f_00004-2-5 loss: 0.859642  [  192/  306]
train() client id: f_00004-2-6 loss: 0.695120  [  224/  306]
train() client id: f_00004-2-7 loss: 1.153770  [  256/  306]
train() client id: f_00004-2-8 loss: 0.715840  [  288/  306]
train() client id: f_00004-3-0 loss: 0.890201  [   32/  306]
train() client id: f_00004-3-1 loss: 0.889264  [   64/  306]
train() client id: f_00004-3-2 loss: 0.922352  [   96/  306]
train() client id: f_00004-3-3 loss: 0.724074  [  128/  306]
train() client id: f_00004-3-4 loss: 0.777034  [  160/  306]
train() client id: f_00004-3-5 loss: 0.737025  [  192/  306]
train() client id: f_00004-3-6 loss: 0.855079  [  224/  306]
train() client id: f_00004-3-7 loss: 0.828792  [  256/  306]
train() client id: f_00004-3-8 loss: 0.631040  [  288/  306]
train() client id: f_00004-4-0 loss: 0.751679  [   32/  306]
train() client id: f_00004-4-1 loss: 0.689757  [   64/  306]
train() client id: f_00004-4-2 loss: 0.800225  [   96/  306]
train() client id: f_00004-4-3 loss: 0.794880  [  128/  306]
train() client id: f_00004-4-4 loss: 0.912398  [  160/  306]
train() client id: f_00004-4-5 loss: 0.702302  [  192/  306]
train() client id: f_00004-4-6 loss: 0.902567  [  224/  306]
train() client id: f_00004-4-7 loss: 0.796444  [  256/  306]
train() client id: f_00004-4-8 loss: 0.852708  [  288/  306]
train() client id: f_00004-5-0 loss: 0.828071  [   32/  306]
train() client id: f_00004-5-1 loss: 0.866406  [   64/  306]
train() client id: f_00004-5-2 loss: 0.848638  [   96/  306]
train() client id: f_00004-5-3 loss: 0.847197  [  128/  306]
train() client id: f_00004-5-4 loss: 0.848669  [  160/  306]
train() client id: f_00004-5-5 loss: 0.727881  [  192/  306]
train() client id: f_00004-5-6 loss: 0.689257  [  224/  306]
train() client id: f_00004-5-7 loss: 0.879345  [  256/  306]
train() client id: f_00004-5-8 loss: 0.742711  [  288/  306]
train() client id: f_00005-0-0 loss: 0.571687  [   32/  146]
train() client id: f_00005-0-1 loss: 0.850681  [   64/  146]
train() client id: f_00005-0-2 loss: 0.944623  [   96/  146]
train() client id: f_00005-0-3 loss: 0.546821  [  128/  146]
train() client id: f_00005-1-0 loss: 0.488119  [   32/  146]
train() client id: f_00005-1-1 loss: 0.797028  [   64/  146]
train() client id: f_00005-1-2 loss: 0.689410  [   96/  146]
train() client id: f_00005-1-3 loss: 0.800469  [  128/  146]
train() client id: f_00005-2-0 loss: 0.675363  [   32/  146]
train() client id: f_00005-2-1 loss: 0.796671  [   64/  146]
train() client id: f_00005-2-2 loss: 0.621316  [   96/  146]
train() client id: f_00005-2-3 loss: 0.833035  [  128/  146]
train() client id: f_00005-3-0 loss: 0.600615  [   32/  146]
train() client id: f_00005-3-1 loss: 0.616446  [   64/  146]
train() client id: f_00005-3-2 loss: 0.892610  [   96/  146]
train() client id: f_00005-3-3 loss: 0.663661  [  128/  146]
train() client id: f_00005-4-0 loss: 0.681909  [   32/  146]
train() client id: f_00005-4-1 loss: 0.587624  [   64/  146]
train() client id: f_00005-4-2 loss: 0.876999  [   96/  146]
train() client id: f_00005-4-3 loss: 0.882779  [  128/  146]
train() client id: f_00005-5-0 loss: 0.783784  [   32/  146]
train() client id: f_00005-5-1 loss: 0.975391  [   64/  146]
train() client id: f_00005-5-2 loss: 0.608482  [   96/  146]
train() client id: f_00005-5-3 loss: 0.560387  [  128/  146]
train() client id: f_00006-0-0 loss: 0.579184  [   32/   54]
train() client id: f_00006-1-0 loss: 0.522159  [   32/   54]
train() client id: f_00006-2-0 loss: 0.577480  [   32/   54]
train() client id: f_00006-3-0 loss: 0.566711  [   32/   54]
train() client id: f_00006-4-0 loss: 0.566305  [   32/   54]
train() client id: f_00006-5-0 loss: 0.454674  [   32/   54]
train() client id: f_00007-0-0 loss: 0.540013  [   32/  179]
train() client id: f_00007-0-1 loss: 0.440495  [   64/  179]
train() client id: f_00007-0-2 loss: 0.570114  [   96/  179]
train() client id: f_00007-0-3 loss: 0.614865  [  128/  179]
train() client id: f_00007-0-4 loss: 0.791640  [  160/  179]
train() client id: f_00007-1-0 loss: 0.534353  [   32/  179]
train() client id: f_00007-1-1 loss: 0.465271  [   64/  179]
train() client id: f_00007-1-2 loss: 0.590311  [   96/  179]
train() client id: f_00007-1-3 loss: 0.882655  [  128/  179]
train() client id: f_00007-1-4 loss: 0.558930  [  160/  179]
train() client id: f_00007-2-0 loss: 0.501631  [   32/  179]
train() client id: f_00007-2-1 loss: 0.897755  [   64/  179]
train() client id: f_00007-2-2 loss: 0.442694  [   96/  179]
train() client id: f_00007-2-3 loss: 0.636226  [  128/  179]
train() client id: f_00007-2-4 loss: 0.379887  [  160/  179]
train() client id: f_00007-3-0 loss: 0.600324  [   32/  179]
train() client id: f_00007-3-1 loss: 0.697951  [   64/  179]
train() client id: f_00007-3-2 loss: 0.682226  [   96/  179]
train() client id: f_00007-3-3 loss: 0.497925  [  128/  179]
train() client id: f_00007-3-4 loss: 0.454567  [  160/  179]
train() client id: f_00007-4-0 loss: 0.841138  [   32/  179]
train() client id: f_00007-4-1 loss: 0.637092  [   64/  179]
train() client id: f_00007-4-2 loss: 0.591480  [   96/  179]
train() client id: f_00007-4-3 loss: 0.453843  [  128/  179]
train() client id: f_00007-4-4 loss: 0.426717  [  160/  179]
train() client id: f_00007-5-0 loss: 0.538007  [   32/  179]
train() client id: f_00007-5-1 loss: 0.590987  [   64/  179]
train() client id: f_00007-5-2 loss: 0.579371  [   96/  179]
train() client id: f_00007-5-3 loss: 0.699664  [  128/  179]
train() client id: f_00007-5-4 loss: 0.534339  [  160/  179]
train() client id: f_00008-0-0 loss: 0.733248  [   32/  130]
train() client id: f_00008-0-1 loss: 0.602234  [   64/  130]
train() client id: f_00008-0-2 loss: 0.617650  [   96/  130]
train() client id: f_00008-0-3 loss: 0.659331  [  128/  130]
train() client id: f_00008-1-0 loss: 0.609824  [   32/  130]
train() client id: f_00008-1-1 loss: 0.729008  [   64/  130]
train() client id: f_00008-1-2 loss: 0.637688  [   96/  130]
train() client id: f_00008-1-3 loss: 0.624295  [  128/  130]
train() client id: f_00008-2-0 loss: 0.677451  [   32/  130]
train() client id: f_00008-2-1 loss: 0.580534  [   64/  130]
train() client id: f_00008-2-2 loss: 0.760764  [   96/  130]
train() client id: f_00008-2-3 loss: 0.586753  [  128/  130]
train() client id: f_00008-3-0 loss: 0.570808  [   32/  130]
train() client id: f_00008-3-1 loss: 0.569801  [   64/  130]
train() client id: f_00008-3-2 loss: 0.781514  [   96/  130]
train() client id: f_00008-3-3 loss: 0.663611  [  128/  130]
train() client id: f_00008-4-0 loss: 0.574772  [   32/  130]
train() client id: f_00008-4-1 loss: 0.613279  [   64/  130]
train() client id: f_00008-4-2 loss: 0.663644  [   96/  130]
train() client id: f_00008-4-3 loss: 0.723440  [  128/  130]
train() client id: f_00008-5-0 loss: 0.693936  [   32/  130]
train() client id: f_00008-5-1 loss: 0.593794  [   64/  130]
train() client id: f_00008-5-2 loss: 0.690129  [   96/  130]
train() client id: f_00008-5-3 loss: 0.625393  [  128/  130]
train() client id: f_00009-0-0 loss: 0.957841  [   32/  118]
train() client id: f_00009-0-1 loss: 0.958471  [   64/  118]
train() client id: f_00009-0-2 loss: 0.790499  [   96/  118]
train() client id: f_00009-1-0 loss: 0.809282  [   32/  118]
train() client id: f_00009-1-1 loss: 1.057614  [   64/  118]
train() client id: f_00009-1-2 loss: 0.950549  [   96/  118]
train() client id: f_00009-2-0 loss: 0.933690  [   32/  118]
train() client id: f_00009-2-1 loss: 0.998601  [   64/  118]
train() client id: f_00009-2-2 loss: 0.837940  [   96/  118]
train() client id: f_00009-3-0 loss: 0.966476  [   32/  118]
train() client id: f_00009-3-1 loss: 1.017428  [   64/  118]
train() client id: f_00009-3-2 loss: 0.845180  [   96/  118]
train() client id: f_00009-4-0 loss: 0.812964  [   32/  118]
train() client id: f_00009-4-1 loss: 1.116871  [   64/  118]
train() client id: f_00009-4-2 loss: 0.876224  [   96/  118]
train() client id: f_00009-5-0 loss: 0.969079  [   32/  118]
train() client id: f_00009-5-1 loss: 0.863871  [   64/  118]
train() client id: f_00009-5-2 loss: 0.963020  [   96/  118]
At round 71 accuracy: 0.6445623342175066
At round 71 training accuracy: 0.5895372233400402
At round 71 training loss: 0.8229867574133157
update_location
xs = [  -3.9056584     4.20031788  375.00902392   18.81129433    0.97929623
    3.95640986 -337.44319194 -316.32485185  359.66397685 -302.06087855]
ys = [ 367.5879595   350.55583871    1.32061395 -337.45517586  329.35018685
  312.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [380.96740298 364.56417641 388.11533343 352.46256611 344.19835066
 328.43316837 351.95851791 331.75606724 373.72027804 318.20871485]
dists_bs = [257.61093895 250.86372084 576.77938202 547.88057556 234.07839046
 225.87194774 240.70402087 224.30017435 557.35992501 213.03746171]
uav_gains = [8.59823011e-13 1.01888229e-12 8.03777175e-13 1.17344140e-12
 1.30420376e-12 1.63364151e-12 1.18076150e-12 1.55363346e-12
 9.24212893e-13 1.92508765e-12]
bs_gains = [1.96152589e-11 2.11284714e-11 2.05336828e-12 2.37122932e-12
 2.56497054e-11 2.83452084e-11 2.37214220e-11 2.89048802e-11
 2.26002902e-12 3.33900711e-11]
Round 72
-------------------------------
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.45877326 2.87217542 1.44178771 0.55140047 3.31032656 1.59425308
 0.66902829 1.99714791 1.45909395 1.29300424]
obj_prev = 16.646990883599326
eta_min = 4.971520837874105e-65	eta_max = 0.9577477121135591
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 3.7800966926678474	eta = 0.9090909090909091
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 10.808661784694127	eta = 0.3179349680138224
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.733867507222867	eta = 0.5103236045412168
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.058721545799979	eta = 0.5671908690326205
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.015928897442357	eta = 0.5712254245973457
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.015728044345185	eta = 0.5712444966689002
eta = 0.5712444966689002
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [0.04620474 0.09717663 0.04547133 0.01576828 0.11221151 0.05353882
 0.01980203 0.06564007 0.04767156 0.04327111]
ene_total = [0.62840586 0.86936    0.63267678 0.33813836 0.98949538 0.50632297
 0.37054313 0.71901402 0.54216497 0.41960659]
ti_comp = [2.91272976 3.128895   2.90019242 2.96199725 3.13288317 3.13480384
 2.96285607 2.99669538 3.03246495 3.13777295]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [7.26673296e-07 5.85846172e-06 6.98617374e-07 2.79295552e-08
 8.99712661e-06 9.76035873e-07 5.52825571e-08 1.96834785e-06
 7.36319715e-07 5.14317079e-07]
ene_total = [0.24915505 0.07176489 0.2594457  0.20870969 0.06851708 0.06687473
 0.20800497 0.18024477 0.15087435 0.06443384]
optimize_network_iter = 0 obj = 1.52802506642978
eta = 0.5712444966689002
freqs = [ 7931518.36924844 15528906.23802998  7839364.44612375  2661765.47781446
 17908664.85509679  8539421.8514933   3341713.30591816 10952075.17012927
  7860199.06137994  6895193.77630764]
eta_min = 0.5712444966689012	eta_max = 0.8252306516812821
af = 0.0001292173845627228	bf = 0.6550768027395515	zeta = 0.00014213912301899509	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [1.23691134e-07 9.97201600e-07 1.18915578e-07 4.75404610e-09
 1.53145134e-06 1.66136535e-07 9.40995383e-09 3.35043518e-07
 1.25333105e-07 8.75447921e-08]
ene_total = [1.17506927 0.33827729 1.22360405 0.98433881 0.32285882 0.31537062
 0.98101428 0.85002707 0.71154662 0.30387348]
ti_comp = [1.00747753 1.22364276 0.99494019 1.05674501 1.22763094 1.22955161
 1.05760384 1.09144314 1.12721272 1.23252071]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.45233236e-07 1.54655995e-06 2.39667719e-07 8.85938112e-09
 2.36574025e-06 2.56155376e-07 1.75175625e-08 5.99094442e-07
 2.15157618e-07 1.34584796e-07]
ene_total = [0.61123365 0.17597144 0.63647978 0.51201976 0.16795702 0.16404692
 0.51029054 0.44216059 0.37012424 0.15806563]
optimize_network_iter = 1 obj = 3.7483495686155184
eta = 0.8252306516812821
freqs = [ 7866677.00292003 13622203.68026247  7839364.44612375  2559495.62396928
 15678688.91426082  7468996.16234366  3211640.19547591 10315922.15655571
  7254268.29694454  6022043.97636097]
eta_min = 0.8252306516812807	eta_max = 0.8252306516812821
af = 0.00010312038831994186	bf = 0.6550768027395515	zeta = 0.00011343242715193606	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [1.21677013e-07 7.67354369e-07 1.18915578e-07 4.39574606e-09
 1.17380585e-06 1.27096235e-07 8.69166314e-09 2.97251805e-07
 1.06754438e-07 6.67767395e-08]
ene_total = [1.17506919 0.33826839 1.22360405 0.98433879 0.32284498 0.31536911
 0.98101425 0.85002561 0.7115459  0.30387267]
ti_comp = [1.00747753 1.22364276 0.99494019 1.05674501 1.22763094 1.22955161
 1.05760384 1.09144314 1.12721272 1.23252071]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.45233236e-07 1.54655995e-06 2.39667719e-07 8.85938112e-09
 2.36574025e-06 2.56155376e-07 1.75175625e-08 5.99094442e-07
 2.15157618e-07 1.34584796e-07]
ene_total = [0.61123365 0.17597144 0.63647978 0.51201976 0.16795702 0.16404692
 0.51029054 0.44216059 0.37012424 0.15806563]
optimize_network_iter = 2 obj = 3.7483495686155184
eta = 0.8252306516812821
freqs = [ 7866677.00292003 13622203.68026247  7839364.44612375  2559495.62396928
 15678688.91426082  7468996.16234366  3211640.19547591 10315922.15655571
  7254268.29694454  6022043.97636097]
Done!
At round 72 eta: 0.8252306516812821
At round 72 local rounds: 6.290079613751617
At round 72 global rounds: 20.136836970093626
At round 72 a_n: 3.176756027494214
gradient difference: 0.7553352117538452
train() client id: f_00000-0-0 loss: 0.841902  [   32/  126]
train() client id: f_00000-0-1 loss: 0.932735  [   64/  126]
train() client id: f_00000-0-2 loss: 0.889819  [   96/  126]
train() client id: f_00000-1-0 loss: 0.978428  [   32/  126]
train() client id: f_00000-1-1 loss: 0.786523  [   64/  126]
train() client id: f_00000-1-2 loss: 0.789976  [   96/  126]
train() client id: f_00000-2-0 loss: 0.939707  [   32/  126]
train() client id: f_00000-2-1 loss: 0.824947  [   64/  126]
train() client id: f_00000-2-2 loss: 0.698311  [   96/  126]
train() client id: f_00000-3-0 loss: 0.833771  [   32/  126]
train() client id: f_00000-3-1 loss: 0.708630  [   64/  126]
train() client id: f_00000-3-2 loss: 0.687459  [   96/  126]
train() client id: f_00000-4-0 loss: 0.736768  [   32/  126]
train() client id: f_00000-4-1 loss: 0.873654  [   64/  126]
train() client id: f_00000-4-2 loss: 0.790384  [   96/  126]
train() client id: f_00000-5-0 loss: 0.810951  [   32/  126]
train() client id: f_00000-5-1 loss: 0.745777  [   64/  126]
train() client id: f_00000-5-2 loss: 0.855447  [   96/  126]
train() client id: f_00001-0-0 loss: 0.557436  [   32/  265]
train() client id: f_00001-0-1 loss: 0.440652  [   64/  265]
train() client id: f_00001-0-2 loss: 0.579022  [   96/  265]
train() client id: f_00001-0-3 loss: 0.455547  [  128/  265]
train() client id: f_00001-0-4 loss: 0.589473  [  160/  265]
train() client id: f_00001-0-5 loss: 0.423023  [  192/  265]
train() client id: f_00001-0-6 loss: 0.519320  [  224/  265]
train() client id: f_00001-0-7 loss: 0.614717  [  256/  265]
train() client id: f_00001-1-0 loss: 0.617730  [   32/  265]
train() client id: f_00001-1-1 loss: 0.566697  [   64/  265]
train() client id: f_00001-1-2 loss: 0.490411  [   96/  265]
train() client id: f_00001-1-3 loss: 0.652278  [  128/  265]
train() client id: f_00001-1-4 loss: 0.426280  [  160/  265]
train() client id: f_00001-1-5 loss: 0.567050  [  192/  265]
train() client id: f_00001-1-6 loss: 0.507867  [  224/  265]
train() client id: f_00001-1-7 loss: 0.443886  [  256/  265]
train() client id: f_00001-2-0 loss: 0.586678  [   32/  265]
train() client id: f_00001-2-1 loss: 0.521869  [   64/  265]
train() client id: f_00001-2-2 loss: 0.578374  [   96/  265]
train() client id: f_00001-2-3 loss: 0.539851  [  128/  265]
train() client id: f_00001-2-4 loss: 0.612079  [  160/  265]
train() client id: f_00001-2-5 loss: 0.503099  [  192/  265]
train() client id: f_00001-2-6 loss: 0.469942  [  224/  265]
train() client id: f_00001-2-7 loss: 0.458702  [  256/  265]
train() client id: f_00001-3-0 loss: 0.568530  [   32/  265]
train() client id: f_00001-3-1 loss: 0.501570  [   64/  265]
train() client id: f_00001-3-2 loss: 0.580714  [   96/  265]
train() client id: f_00001-3-3 loss: 0.464119  [  128/  265]
train() client id: f_00001-3-4 loss: 0.425610  [  160/  265]
train() client id: f_00001-3-5 loss: 0.608268  [  192/  265]
train() client id: f_00001-3-6 loss: 0.450603  [  224/  265]
train() client id: f_00001-3-7 loss: 0.650843  [  256/  265]
train() client id: f_00001-4-0 loss: 0.646416  [   32/  265]
train() client id: f_00001-4-1 loss: 0.590161  [   64/  265]
train() client id: f_00001-4-2 loss: 0.566319  [   96/  265]
train() client id: f_00001-4-3 loss: 0.437141  [  128/  265]
train() client id: f_00001-4-4 loss: 0.416639  [  160/  265]
train() client id: f_00001-4-5 loss: 0.499647  [  192/  265]
train() client id: f_00001-4-6 loss: 0.470320  [  224/  265]
train() client id: f_00001-4-7 loss: 0.614321  [  256/  265]
train() client id: f_00001-5-0 loss: 0.581916  [   32/  265]
train() client id: f_00001-5-1 loss: 0.454464  [   64/  265]
train() client id: f_00001-5-2 loss: 0.600402  [   96/  265]
train() client id: f_00001-5-3 loss: 0.637594  [  128/  265]
train() client id: f_00001-5-4 loss: 0.585617  [  160/  265]
train() client id: f_00001-5-5 loss: 0.479731  [  192/  265]
train() client id: f_00001-5-6 loss: 0.432844  [  224/  265]
train() client id: f_00001-5-7 loss: 0.421635  [  256/  265]
train() client id: f_00002-0-0 loss: 1.124705  [   32/  124]
train() client id: f_00002-0-1 loss: 1.123211  [   64/  124]
train() client id: f_00002-0-2 loss: 1.039503  [   96/  124]
train() client id: f_00002-1-0 loss: 1.087832  [   32/  124]
train() client id: f_00002-1-1 loss: 1.123757  [   64/  124]
train() client id: f_00002-1-2 loss: 1.094818  [   96/  124]
train() client id: f_00002-2-0 loss: 1.106265  [   32/  124]
train() client id: f_00002-2-1 loss: 1.177699  [   64/  124]
train() client id: f_00002-2-2 loss: 0.870187  [   96/  124]
train() client id: f_00002-3-0 loss: 0.945433  [   32/  124]
train() client id: f_00002-3-1 loss: 1.242035  [   64/  124]
train() client id: f_00002-3-2 loss: 1.025050  [   96/  124]
train() client id: f_00002-4-0 loss: 1.180979  [   32/  124]
train() client id: f_00002-4-1 loss: 0.937649  [   64/  124]
train() client id: f_00002-4-2 loss: 0.893826  [   96/  124]
train() client id: f_00002-5-0 loss: 1.062104  [   32/  124]
train() client id: f_00002-5-1 loss: 1.286770  [   64/  124]
train() client id: f_00002-5-2 loss: 0.942343  [   96/  124]
train() client id: f_00003-0-0 loss: 0.778430  [   32/   43]
train() client id: f_00003-1-0 loss: 0.955587  [   32/   43]
train() client id: f_00003-2-0 loss: 0.810196  [   32/   43]
train() client id: f_00003-3-0 loss: 0.822171  [   32/   43]
train() client id: f_00003-4-0 loss: 0.874847  [   32/   43]
train() client id: f_00003-5-0 loss: 0.808507  [   32/   43]
train() client id: f_00004-0-0 loss: 0.950477  [   32/  306]
train() client id: f_00004-0-1 loss: 0.928425  [   64/  306]
train() client id: f_00004-0-2 loss: 1.076201  [   96/  306]
train() client id: f_00004-0-3 loss: 0.975743  [  128/  306]
train() client id: f_00004-0-4 loss: 0.842519  [  160/  306]
train() client id: f_00004-0-5 loss: 0.879135  [  192/  306]
train() client id: f_00004-0-6 loss: 0.680562  [  224/  306]
train() client id: f_00004-0-7 loss: 0.787783  [  256/  306]
train() client id: f_00004-0-8 loss: 0.781246  [  288/  306]
train() client id: f_00004-1-0 loss: 1.016526  [   32/  306]
train() client id: f_00004-1-1 loss: 0.958397  [   64/  306]
train() client id: f_00004-1-2 loss: 0.925347  [   96/  306]
train() client id: f_00004-1-3 loss: 0.782474  [  128/  306]
train() client id: f_00004-1-4 loss: 0.866589  [  160/  306]
train() client id: f_00004-1-5 loss: 0.774307  [  192/  306]
train() client id: f_00004-1-6 loss: 0.876966  [  224/  306]
train() client id: f_00004-1-7 loss: 0.926830  [  256/  306]
train() client id: f_00004-1-8 loss: 0.760853  [  288/  306]
train() client id: f_00004-2-0 loss: 0.838703  [   32/  306]
train() client id: f_00004-2-1 loss: 0.971541  [   64/  306]
train() client id: f_00004-2-2 loss: 0.742457  [   96/  306]
train() client id: f_00004-2-3 loss: 0.921622  [  128/  306]
train() client id: f_00004-2-4 loss: 0.998535  [  160/  306]
train() client id: f_00004-2-5 loss: 0.895319  [  192/  306]
train() client id: f_00004-2-6 loss: 0.891456  [  224/  306]
train() client id: f_00004-2-7 loss: 0.822475  [  256/  306]
train() client id: f_00004-2-8 loss: 0.876220  [  288/  306]
train() client id: f_00004-3-0 loss: 0.901792  [   32/  306]
train() client id: f_00004-3-1 loss: 0.851947  [   64/  306]
train() client id: f_00004-3-2 loss: 0.828076  [   96/  306]
train() client id: f_00004-3-3 loss: 0.765343  [  128/  306]
train() client id: f_00004-3-4 loss: 0.936225  [  160/  306]
train() client id: f_00004-3-5 loss: 0.902540  [  192/  306]
train() client id: f_00004-3-6 loss: 0.983093  [  224/  306]
train() client id: f_00004-3-7 loss: 0.809040  [  256/  306]
train() client id: f_00004-3-8 loss: 0.805039  [  288/  306]
train() client id: f_00004-4-0 loss: 0.883093  [   32/  306]
train() client id: f_00004-4-1 loss: 0.903448  [   64/  306]
train() client id: f_00004-4-2 loss: 0.883906  [   96/  306]
train() client id: f_00004-4-3 loss: 0.918783  [  128/  306]
train() client id: f_00004-4-4 loss: 0.802926  [  160/  306]
train() client id: f_00004-4-5 loss: 0.936762  [  192/  306]
train() client id: f_00004-4-6 loss: 0.899567  [  224/  306]
train() client id: f_00004-4-7 loss: 0.871785  [  256/  306]
train() client id: f_00004-4-8 loss: 0.760604  [  288/  306]
train() client id: f_00004-5-0 loss: 0.932879  [   32/  306]
train() client id: f_00004-5-1 loss: 0.815991  [   64/  306]
train() client id: f_00004-5-2 loss: 0.833113  [   96/  306]
train() client id: f_00004-5-3 loss: 0.847079  [  128/  306]
train() client id: f_00004-5-4 loss: 0.832884  [  160/  306]
train() client id: f_00004-5-5 loss: 0.803009  [  192/  306]
train() client id: f_00004-5-6 loss: 0.896450  [  224/  306]
train() client id: f_00004-5-7 loss: 0.910818  [  256/  306]
train() client id: f_00004-5-8 loss: 0.951567  [  288/  306]
train() client id: f_00005-0-0 loss: 0.876910  [   32/  146]
train() client id: f_00005-0-1 loss: 0.611174  [   64/  146]
train() client id: f_00005-0-2 loss: 0.847937  [   96/  146]
train() client id: f_00005-0-3 loss: 0.806583  [  128/  146]
train() client id: f_00005-1-0 loss: 0.744960  [   32/  146]
train() client id: f_00005-1-1 loss: 0.573473  [   64/  146]
train() client id: f_00005-1-2 loss: 0.577133  [   96/  146]
train() client id: f_00005-1-3 loss: 0.963518  [  128/  146]
train() client id: f_00005-2-0 loss: 1.169467  [   32/  146]
train() client id: f_00005-2-1 loss: 0.620496  [   64/  146]
train() client id: f_00005-2-2 loss: 0.709548  [   96/  146]
train() client id: f_00005-2-3 loss: 0.542085  [  128/  146]
train() client id: f_00005-3-0 loss: 0.846010  [   32/  146]
train() client id: f_00005-3-1 loss: 0.786348  [   64/  146]
train() client id: f_00005-3-2 loss: 0.587230  [   96/  146]
train() client id: f_00005-3-3 loss: 0.817238  [  128/  146]
train() client id: f_00005-4-0 loss: 0.666905  [   32/  146]
train() client id: f_00005-4-1 loss: 0.558230  [   64/  146]
train() client id: f_00005-4-2 loss: 0.730062  [   96/  146]
train() client id: f_00005-4-3 loss: 1.081093  [  128/  146]
train() client id: f_00005-5-0 loss: 0.751917  [   32/  146]
train() client id: f_00005-5-1 loss: 0.810568  [   64/  146]
train() client id: f_00005-5-2 loss: 0.621488  [   96/  146]
train() client id: f_00005-5-3 loss: 0.762377  [  128/  146]
train() client id: f_00006-0-0 loss: 0.528285  [   32/   54]
train() client id: f_00006-1-0 loss: 0.587949  [   32/   54]
train() client id: f_00006-2-0 loss: 0.494112  [   32/   54]
train() client id: f_00006-3-0 loss: 0.506397  [   32/   54]
train() client id: f_00006-4-0 loss: 0.537940  [   32/   54]
train() client id: f_00006-5-0 loss: 0.604531  [   32/   54]
train() client id: f_00007-0-0 loss: 0.693313  [   32/  179]
train() client id: f_00007-0-1 loss: 0.628813  [   64/  179]
train() client id: f_00007-0-2 loss: 0.637273  [   96/  179]
train() client id: f_00007-0-3 loss: 0.528870  [  128/  179]
train() client id: f_00007-0-4 loss: 0.645614  [  160/  179]
train() client id: f_00007-1-0 loss: 0.718674  [   32/  179]
train() client id: f_00007-1-1 loss: 0.560150  [   64/  179]
train() client id: f_00007-1-2 loss: 0.610173  [   96/  179]
train() client id: f_00007-1-3 loss: 0.688202  [  128/  179]
train() client id: f_00007-1-4 loss: 0.700613  [  160/  179]
train() client id: f_00007-2-0 loss: 0.558096  [   32/  179]
train() client id: f_00007-2-1 loss: 0.666580  [   64/  179]
train() client id: f_00007-2-2 loss: 0.793122  [   96/  179]
train() client id: f_00007-2-3 loss: 0.784943  [  128/  179]
train() client id: f_00007-2-4 loss: 0.440558  [  160/  179]
train() client id: f_00007-3-0 loss: 0.695611  [   32/  179]
train() client id: f_00007-3-1 loss: 0.471888  [   64/  179]
train() client id: f_00007-3-2 loss: 0.523735  [   96/  179]
train() client id: f_00007-3-3 loss: 0.785387  [  128/  179]
train() client id: f_00007-3-4 loss: 0.657557  [  160/  179]
train() client id: f_00007-4-0 loss: 0.792716  [   32/  179]
train() client id: f_00007-4-1 loss: 0.848687  [   64/  179]
train() client id: f_00007-4-2 loss: 0.512469  [   96/  179]
train() client id: f_00007-4-3 loss: 0.570089  [  128/  179]
train() client id: f_00007-4-4 loss: 0.583598  [  160/  179]
train() client id: f_00007-5-0 loss: 0.764475  [   32/  179]
train() client id: f_00007-5-1 loss: 0.711200  [   64/  179]
train() client id: f_00007-5-2 loss: 0.651223  [   96/  179]
train() client id: f_00007-5-3 loss: 0.648265  [  128/  179]
train() client id: f_00007-5-4 loss: 0.481977  [  160/  179]
train() client id: f_00008-0-0 loss: 0.710364  [   32/  130]
train() client id: f_00008-0-1 loss: 0.778001  [   64/  130]
train() client id: f_00008-0-2 loss: 0.769241  [   96/  130]
train() client id: f_00008-0-3 loss: 0.803326  [  128/  130]
train() client id: f_00008-1-0 loss: 0.851325  [   32/  130]
train() client id: f_00008-1-1 loss: 0.685594  [   64/  130]
train() client id: f_00008-1-2 loss: 0.830418  [   96/  130]
train() client id: f_00008-1-3 loss: 0.693196  [  128/  130]
train() client id: f_00008-2-0 loss: 0.815031  [   32/  130]
train() client id: f_00008-2-1 loss: 0.722033  [   64/  130]
train() client id: f_00008-2-2 loss: 0.765052  [   96/  130]
train() client id: f_00008-2-3 loss: 0.754824  [  128/  130]
train() client id: f_00008-3-0 loss: 0.696707  [   32/  130]
train() client id: f_00008-3-1 loss: 0.692183  [   64/  130]
train() client id: f_00008-3-2 loss: 0.818449  [   96/  130]
train() client id: f_00008-3-3 loss: 0.844471  [  128/  130]
train() client id: f_00008-4-0 loss: 0.738341  [   32/  130]
train() client id: f_00008-4-1 loss: 0.766342  [   64/  130]
train() client id: f_00008-4-2 loss: 0.746906  [   96/  130]
train() client id: f_00008-4-3 loss: 0.808211  [  128/  130]
train() client id: f_00008-5-0 loss: 0.929777  [   32/  130]
train() client id: f_00008-5-1 loss: 0.657141  [   64/  130]
train() client id: f_00008-5-2 loss: 0.696690  [   96/  130]
train() client id: f_00008-5-3 loss: 0.767183  [  128/  130]
train() client id: f_00009-0-0 loss: 1.066684  [   32/  118]
train() client id: f_00009-0-1 loss: 0.834191  [   64/  118]
train() client id: f_00009-0-2 loss: 0.769784  [   96/  118]
train() client id: f_00009-1-0 loss: 0.822508  [   32/  118]
train() client id: f_00009-1-1 loss: 0.774210  [   64/  118]
train() client id: f_00009-1-2 loss: 0.913783  [   96/  118]
train() client id: f_00009-2-0 loss: 0.598483  [   32/  118]
train() client id: f_00009-2-1 loss: 0.872802  [   64/  118]
train() client id: f_00009-2-2 loss: 0.866028  [   96/  118]
train() client id: f_00009-3-0 loss: 0.863009  [   32/  118]
train() client id: f_00009-3-1 loss: 0.753521  [   64/  118]
train() client id: f_00009-3-2 loss: 0.836007  [   96/  118]
train() client id: f_00009-4-0 loss: 0.909898  [   32/  118]
train() client id: f_00009-4-1 loss: 0.635317  [   64/  118]
train() client id: f_00009-4-2 loss: 0.713704  [   96/  118]
train() client id: f_00009-5-0 loss: 0.696584  [   32/  118]
train() client id: f_00009-5-1 loss: 0.743981  [   64/  118]
train() client id: f_00009-5-2 loss: 0.822767  [   96/  118]
At round 72 accuracy: 0.6445623342175066
At round 72 training accuracy: 0.5928906773977196
At round 72 training loss: 0.8126266657641517
update_location
xs = [  -3.9056584     4.20031788  380.00902392   18.81129433    0.97929623
    3.95640986 -342.44319194 -321.32485185  364.66397685 -307.06087855]
ys = [ 372.5879595   355.55583871    1.32061395 -342.45517586  334.35018685
  317.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [385.79404056 369.37460269 392.94860005 357.25258889 348.98568232
 333.19887093 356.75514046 336.52687362 378.53465625 322.95881315]
dists_bs = [261.36999725 254.38782364 581.5492634  552.56011151 237.40512789
 228.95693543 244.1062424  227.48366256 562.15809678 216.05686492]
uav_gains = [8.21244196e-13 9.67100320e-13 7.69464620e-13 1.10763527e-12
 1.22558846e-12 1.52083801e-12 1.11416668e-12 1.44934733e-12
 8.80519312e-13 1.78047100e-12]
bs_gains = [1.88355381e-11 2.03190964e-11 2.00655865e-12 2.31542863e-12
 2.46559544e-11 2.72887370e-11 2.28072648e-11 2.77864773e-11
 2.20643109e-12 3.20998859e-11]
Round 73
-------------------------------
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.3194998  2.59288881 1.30418738 0.50035792 2.98837254 1.43930991
 0.6065387  1.80528297 1.31771582 1.16737463]
obj_prev = 15.041528473465261
eta_min = 6.730086322824428e-72	eta_max = 0.960768447983313
af = 3.101969802094253	bf = 0.606014338290774	zeta = 3.4121667823036788	eta = 0.9090909090909091
af = 3.101969802094253	bf = 0.606014338290774	zeta = 9.918300016635595	eta = 0.3127521648761819
af = 3.101969802094253	bf = 0.606014338290774	zeta = 6.128169132936568	eta = 0.5061821458912337
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.504089108696306	eta = 0.5635755055624424
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.464521898505789	eta = 0.5676562121459247
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.464335618649047	eta = 0.5676755636142928
eta = 0.5676755636142928
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [0.0467219  0.09826431 0.04598028 0.01594477 0.11346746 0.05413807
 0.02002367 0.06637476 0.04820513 0.04375543]
ene_total = [0.57257327 0.78687954 0.57638183 0.31001941 0.89561579 0.45820012
 0.33936518 0.65485131 0.49074166 0.3797075 ]
ti_comp = [3.27393193 3.49770683 3.26133269 3.32347001 3.50175933 3.50374398
 3.32432233 3.35848843 3.40012884 3.50673809]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [5.94705746e-07 4.84730784e-06 5.71221622e-07 2.29377929e-08
 7.44597339e-06 8.07836791e-07 4.54051187e-08 1.62032140e-06
 6.05576618e-07 4.25764209e-07]
ene_total = [0.22926124 0.06486083 0.2385191  0.19285604 0.06190211 0.060395
 0.19222992 0.16713595 0.13653078 0.05819209]
optimize_network_iter = 0 obj = 1.401883055865988
eta = 0.5676755636142928
freqs = [ 7135441.14428652 14046961.66554712  7049308.38877608  2398814.24536511
 16201493.85610097  7725745.89318307  3011692.04068683  9881641.69424418
  7088721.37557428  6238764.68717198]
eta_min = 0.5676755636142934	eta_max = 0.8387833537635655
af = 9.530769728321942e-05	bf = 0.606014338290774	zeta = 0.00010483846701154136	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.00107717e-07 8.15954657e-07 9.61545994e-08 3.86115336e-09
 1.25339196e-06 1.35984388e-07 7.64311227e-09 2.72751152e-07
 1.01937628e-07 7.16695331e-08]
ene_total = [1.09025033 0.30830925 1.13427738 0.91713951 0.29416335 0.28718908
 0.91416128 0.79477943 0.64926391 0.27672413]
ti_comp = [1.02522061 1.24899552 1.01262137 1.0747587  1.25304801 1.25503266
 1.07561102 1.10977712 1.15141753 1.25802677]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.81535984e-07 1.13789349e-06 1.77360386e-07 6.56552073e-09
 1.74066261e-06 1.88466540e-07 1.29824466e-08 4.44194635e-07
 1.58070248e-07 9.90266306e-08]
ene_total = [0.61478717 0.17386006 0.63961375 0.51716947 0.16588653 0.16194522
 0.51549012 0.44817469 0.36611709 0.1560436 ]
optimize_network_iter = 1 obj = 3.7590876976936305
eta = 0.8387833537635655
freqs = [ 7074978.59491438 12213971.70314103  7049308.38877607  2303188.06336389
 14058067.6277585   6696837.19701407  2890083.78088009  9285155.45638792
  6499537.15984087  5399630.45377395]
eta_min = 0.8387833537635758	eta_max = 0.8387833537635625
af = 7.4931901688053e-05	bf = 0.606014338290774	zeta = 8.242509185685831e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [9.84183688e-08 6.16900397e-07 9.61545994e-08 3.55944769e-09
 9.43687147e-07 1.02175717e-07 7.03833579e-09 2.40816781e-07
 8.56965960e-08 5.36865431e-08]
ene_total = [1.09025027 0.30830229 1.13427738 0.9171395  0.29415253 0.2871879
 0.91416126 0.79477832 0.64926334 0.2767235 ]
ti_comp = [1.02522061 1.24899552 1.01262137 1.0747587  1.25304801 1.25503266
 1.07561102 1.10977712 1.15141753 1.25802677]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.81535984e-07 1.13789349e-06 1.77360386e-07 6.56552073e-09
 1.74066261e-06 1.88466540e-07 1.29824466e-08 4.44194635e-07
 1.58070248e-07 9.90266306e-08]
ene_total = [0.61478717 0.17386006 0.63961375 0.51716947 0.16588653 0.16194522
 0.51549012 0.44817469 0.36611709 0.1560436 ]
optimize_network_iter = 2 obj = 3.759087697693561
eta = 0.8387833537635625
freqs = [ 7074978.59491436 12213971.70314105  7049308.38877605  2303188.06336388
 14058067.62775852  6696837.19701408  2890083.78088009  9285155.45638791
  6499537.15984087  5399630.45377396]
Done!
At round 73 eta: 0.8387833537635625
At round 73 local rounds: 5.756677690695127
At round 73 global rounds: 19.70488843214887
At round 73 a_n: 2.834210180524895
gradient difference: 0.6721806526184082
train() client id: f_00000-0-0 loss: 1.169379  [   32/  126]
train() client id: f_00000-0-1 loss: 0.977647  [   64/  126]
train() client id: f_00000-0-2 loss: 0.801657  [   96/  126]
train() client id: f_00000-1-0 loss: 0.886855  [   32/  126]
train() client id: f_00000-1-1 loss: 0.878536  [   64/  126]
train() client id: f_00000-1-2 loss: 1.048395  [   96/  126]
train() client id: f_00000-2-0 loss: 0.977405  [   32/  126]
train() client id: f_00000-2-1 loss: 0.919661  [   64/  126]
train() client id: f_00000-2-2 loss: 0.778535  [   96/  126]
train() client id: f_00000-3-0 loss: 0.844487  [   32/  126]
train() client id: f_00000-3-1 loss: 0.779548  [   64/  126]
train() client id: f_00000-3-2 loss: 0.871787  [   96/  126]
train() client id: f_00000-4-0 loss: 0.881023  [   32/  126]
train() client id: f_00000-4-1 loss: 0.844555  [   64/  126]
train() client id: f_00000-4-2 loss: 0.805485  [   96/  126]
train() client id: f_00001-0-0 loss: 0.549603  [   32/  265]
train() client id: f_00001-0-1 loss: 0.565664  [   64/  265]
train() client id: f_00001-0-2 loss: 0.563150  [   96/  265]
train() client id: f_00001-0-3 loss: 0.643040  [  128/  265]
train() client id: f_00001-0-4 loss: 0.435604  [  160/  265]
train() client id: f_00001-0-5 loss: 0.520552  [  192/  265]
train() client id: f_00001-0-6 loss: 0.621267  [  224/  265]
train() client id: f_00001-0-7 loss: 0.578438  [  256/  265]
train() client id: f_00001-1-0 loss: 0.486072  [   32/  265]
train() client id: f_00001-1-1 loss: 0.548537  [   64/  265]
train() client id: f_00001-1-2 loss: 0.645288  [   96/  265]
train() client id: f_00001-1-3 loss: 0.517359  [  128/  265]
train() client id: f_00001-1-4 loss: 0.455963  [  160/  265]
train() client id: f_00001-1-5 loss: 0.436135  [  192/  265]
train() client id: f_00001-1-6 loss: 0.498679  [  224/  265]
train() client id: f_00001-1-7 loss: 0.830191  [  256/  265]
train() client id: f_00001-2-0 loss: 0.609153  [   32/  265]
train() client id: f_00001-2-1 loss: 0.698191  [   64/  265]
train() client id: f_00001-2-2 loss: 0.520732  [   96/  265]
train() client id: f_00001-2-3 loss: 0.612423  [  128/  265]
train() client id: f_00001-2-4 loss: 0.473333  [  160/  265]
train() client id: f_00001-2-5 loss: 0.524291  [  192/  265]
train() client id: f_00001-2-6 loss: 0.458141  [  224/  265]
train() client id: f_00001-2-7 loss: 0.552703  [  256/  265]
train() client id: f_00001-3-0 loss: 0.675802  [   32/  265]
train() client id: f_00001-3-1 loss: 0.466993  [   64/  265]
train() client id: f_00001-3-2 loss: 0.582130  [   96/  265]
train() client id: f_00001-3-3 loss: 0.474806  [  128/  265]
train() client id: f_00001-3-4 loss: 0.612037  [  160/  265]
train() client id: f_00001-3-5 loss: 0.541608  [  192/  265]
train() client id: f_00001-3-6 loss: 0.486151  [  224/  265]
train() client id: f_00001-3-7 loss: 0.589665  [  256/  265]
train() client id: f_00001-4-0 loss: 0.565267  [   32/  265]
train() client id: f_00001-4-1 loss: 0.539481  [   64/  265]
train() client id: f_00001-4-2 loss: 0.451158  [   96/  265]
train() client id: f_00001-4-3 loss: 0.501006  [  128/  265]
train() client id: f_00001-4-4 loss: 0.578924  [  160/  265]
train() client id: f_00001-4-5 loss: 0.566782  [  192/  265]
train() client id: f_00001-4-6 loss: 0.661377  [  224/  265]
train() client id: f_00001-4-7 loss: 0.567847  [  256/  265]
train() client id: f_00002-0-0 loss: 1.204257  [   32/  124]
train() client id: f_00002-0-1 loss: 1.127010  [   64/  124]
train() client id: f_00002-0-2 loss: 1.144188  [   96/  124]
train() client id: f_00002-1-0 loss: 1.112505  [   32/  124]
train() client id: f_00002-1-1 loss: 1.070749  [   64/  124]
train() client id: f_00002-1-2 loss: 1.173738  [   96/  124]
train() client id: f_00002-2-0 loss: 0.975970  [   32/  124]
train() client id: f_00002-2-1 loss: 1.150657  [   64/  124]
train() client id: f_00002-2-2 loss: 1.128208  [   96/  124]
train() client id: f_00002-3-0 loss: 1.075180  [   32/  124]
train() client id: f_00002-3-1 loss: 1.175637  [   64/  124]
train() client id: f_00002-3-2 loss: 1.041484  [   96/  124]
train() client id: f_00002-4-0 loss: 0.950737  [   32/  124]
train() client id: f_00002-4-1 loss: 1.096293  [   64/  124]
train() client id: f_00002-4-2 loss: 1.045317  [   96/  124]
train() client id: f_00003-0-0 loss: 0.812506  [   32/   43]
train() client id: f_00003-1-0 loss: 0.845289  [   32/   43]
train() client id: f_00003-2-0 loss: 0.668089  [   32/   43]
train() client id: f_00003-3-0 loss: 0.728046  [   32/   43]
train() client id: f_00003-4-0 loss: 0.447329  [   32/   43]
train() client id: f_00004-0-0 loss: 0.794855  [   32/  306]
train() client id: f_00004-0-1 loss: 0.717824  [   64/  306]
train() client id: f_00004-0-2 loss: 0.663992  [   96/  306]
train() client id: f_00004-0-3 loss: 0.829872  [  128/  306]
train() client id: f_00004-0-4 loss: 0.699641  [  160/  306]
train() client id: f_00004-0-5 loss: 0.796121  [  192/  306]
train() client id: f_00004-0-6 loss: 0.644662  [  224/  306]
train() client id: f_00004-0-7 loss: 0.787587  [  256/  306]
train() client id: f_00004-0-8 loss: 0.691197  [  288/  306]
train() client id: f_00004-1-0 loss: 0.787843  [   32/  306]
train() client id: f_00004-1-1 loss: 0.696735  [   64/  306]
train() client id: f_00004-1-2 loss: 0.532483  [   96/  306]
train() client id: f_00004-1-3 loss: 0.825466  [  128/  306]
train() client id: f_00004-1-4 loss: 0.877319  [  160/  306]
train() client id: f_00004-1-5 loss: 0.839184  [  192/  306]
train() client id: f_00004-1-6 loss: 0.762193  [  224/  306]
train() client id: f_00004-1-7 loss: 0.897924  [  256/  306]
train() client id: f_00004-1-8 loss: 0.674419  [  288/  306]
train() client id: f_00004-2-0 loss: 0.670316  [   32/  306]
train() client id: f_00004-2-1 loss: 0.844153  [   64/  306]
train() client id: f_00004-2-2 loss: 0.699007  [   96/  306]
train() client id: f_00004-2-3 loss: 0.826182  [  128/  306]
train() client id: f_00004-2-4 loss: 0.753082  [  160/  306]
train() client id: f_00004-2-5 loss: 0.794813  [  192/  306]
train() client id: f_00004-2-6 loss: 0.661256  [  224/  306]
train() client id: f_00004-2-7 loss: 0.781718  [  256/  306]
train() client id: f_00004-2-8 loss: 0.733758  [  288/  306]
train() client id: f_00004-3-0 loss: 0.866793  [   32/  306]
train() client id: f_00004-3-1 loss: 0.765770  [   64/  306]
train() client id: f_00004-3-2 loss: 0.615091  [   96/  306]
train() client id: f_00004-3-3 loss: 0.813564  [  128/  306]
train() client id: f_00004-3-4 loss: 0.818263  [  160/  306]
train() client id: f_00004-3-5 loss: 0.803620  [  192/  306]
train() client id: f_00004-3-6 loss: 0.654449  [  224/  306]
train() client id: f_00004-3-7 loss: 0.806331  [  256/  306]
train() client id: f_00004-3-8 loss: 0.677823  [  288/  306]
train() client id: f_00004-4-0 loss: 0.602368  [   32/  306]
train() client id: f_00004-4-1 loss: 0.851810  [   64/  306]
train() client id: f_00004-4-2 loss: 0.764012  [   96/  306]
train() client id: f_00004-4-3 loss: 0.791713  [  128/  306]
train() client id: f_00004-4-4 loss: 0.810402  [  160/  306]
train() client id: f_00004-4-5 loss: 0.814610  [  192/  306]
train() client id: f_00004-4-6 loss: 0.751022  [  224/  306]
train() client id: f_00004-4-7 loss: 0.727642  [  256/  306]
train() client id: f_00004-4-8 loss: 0.811373  [  288/  306]
train() client id: f_00005-0-0 loss: 0.543198  [   32/  146]
train() client id: f_00005-0-1 loss: 0.545840  [   64/  146]
train() client id: f_00005-0-2 loss: 0.939710  [   96/  146]
train() client id: f_00005-0-3 loss: 0.475999  [  128/  146]
train() client id: f_00005-1-0 loss: 0.808121  [   32/  146]
train() client id: f_00005-1-1 loss: 0.557619  [   64/  146]
train() client id: f_00005-1-2 loss: 0.586067  [   96/  146]
train() client id: f_00005-1-3 loss: 0.533867  [  128/  146]
train() client id: f_00005-2-0 loss: 0.779127  [   32/  146]
train() client id: f_00005-2-1 loss: 0.606085  [   64/  146]
train() client id: f_00005-2-2 loss: 0.373781  [   96/  146]
train() client id: f_00005-2-3 loss: 0.870821  [  128/  146]
train() client id: f_00005-3-0 loss: 0.948886  [   32/  146]
train() client id: f_00005-3-1 loss: 0.396008  [   64/  146]
train() client id: f_00005-3-2 loss: 0.525293  [   96/  146]
train() client id: f_00005-3-3 loss: 0.535236  [  128/  146]
train() client id: f_00005-4-0 loss: 0.476512  [   32/  146]
train() client id: f_00005-4-1 loss: 0.778643  [   64/  146]
train() client id: f_00005-4-2 loss: 0.726151  [   96/  146]
train() client id: f_00005-4-3 loss: 0.470056  [  128/  146]
train() client id: f_00006-0-0 loss: 0.447888  [   32/   54]
train() client id: f_00006-1-0 loss: 0.382132  [   32/   54]
train() client id: f_00006-2-0 loss: 0.508456  [   32/   54]
train() client id: f_00006-3-0 loss: 0.405102  [   32/   54]
train() client id: f_00006-4-0 loss: 0.366689  [   32/   54]
train() client id: f_00007-0-0 loss: 0.650184  [   32/  179]
train() client id: f_00007-0-1 loss: 0.760665  [   64/  179]
train() client id: f_00007-0-2 loss: 0.782362  [   96/  179]
train() client id: f_00007-0-3 loss: 0.695864  [  128/  179]
train() client id: f_00007-0-4 loss: 0.972017  [  160/  179]
train() client id: f_00007-1-0 loss: 0.669227  [   32/  179]
train() client id: f_00007-1-1 loss: 0.764683  [   64/  179]
train() client id: f_00007-1-2 loss: 0.784507  [   96/  179]
train() client id: f_00007-1-3 loss: 0.822616  [  128/  179]
train() client id: f_00007-1-4 loss: 0.609776  [  160/  179]
train() client id: f_00007-2-0 loss: 0.799040  [   32/  179]
train() client id: f_00007-2-1 loss: 0.638936  [   64/  179]
train() client id: f_00007-2-2 loss: 0.864718  [   96/  179]
train() client id: f_00007-2-3 loss: 0.873312  [  128/  179]
train() client id: f_00007-2-4 loss: 0.703174  [  160/  179]
train() client id: f_00007-3-0 loss: 0.818396  [   32/  179]
train() client id: f_00007-3-1 loss: 0.623923  [   64/  179]
train() client id: f_00007-3-2 loss: 0.866397  [   96/  179]
train() client id: f_00007-3-3 loss: 0.782274  [  128/  179]
train() client id: f_00007-3-4 loss: 0.733922  [  160/  179]
train() client id: f_00007-4-0 loss: 0.739661  [   32/  179]
train() client id: f_00007-4-1 loss: 0.579248  [   64/  179]
train() client id: f_00007-4-2 loss: 1.107087  [   96/  179]
train() client id: f_00007-4-3 loss: 0.689398  [  128/  179]
train() client id: f_00007-4-4 loss: 0.612024  [  160/  179]
train() client id: f_00008-0-0 loss: 0.702370  [   32/  130]
train() client id: f_00008-0-1 loss: 0.783871  [   64/  130]
train() client id: f_00008-0-2 loss: 0.753242  [   96/  130]
train() client id: f_00008-0-3 loss: 0.688077  [  128/  130]
train() client id: f_00008-1-0 loss: 0.810889  [   32/  130]
train() client id: f_00008-1-1 loss: 0.756399  [   64/  130]
train() client id: f_00008-1-2 loss: 0.696153  [   96/  130]
train() client id: f_00008-1-3 loss: 0.638383  [  128/  130]
train() client id: f_00008-2-0 loss: 0.641155  [   32/  130]
train() client id: f_00008-2-1 loss: 0.702072  [   64/  130]
train() client id: f_00008-2-2 loss: 0.820439  [   96/  130]
train() client id: f_00008-2-3 loss: 0.728350  [  128/  130]
train() client id: f_00008-3-0 loss: 0.738301  [   32/  130]
train() client id: f_00008-3-1 loss: 0.661199  [   64/  130]
train() client id: f_00008-3-2 loss: 0.718263  [   96/  130]
train() client id: f_00008-3-3 loss: 0.777132  [  128/  130]
train() client id: f_00008-4-0 loss: 0.772841  [   32/  130]
train() client id: f_00008-4-1 loss: 0.763752  [   64/  130]
train() client id: f_00008-4-2 loss: 0.723176  [   96/  130]
train() client id: f_00008-4-3 loss: 0.644418  [  128/  130]
train() client id: f_00009-0-0 loss: 0.916093  [   32/  118]
train() client id: f_00009-0-1 loss: 0.705061  [   64/  118]
train() client id: f_00009-0-2 loss: 0.842223  [   96/  118]
train() client id: f_00009-1-0 loss: 0.830516  [   32/  118]
train() client id: f_00009-1-1 loss: 0.706714  [   64/  118]
train() client id: f_00009-1-2 loss: 0.853162  [   96/  118]
train() client id: f_00009-2-0 loss: 0.740614  [   32/  118]
train() client id: f_00009-2-1 loss: 0.850882  [   64/  118]
train() client id: f_00009-2-2 loss: 0.811746  [   96/  118]
train() client id: f_00009-3-0 loss: 0.752357  [   32/  118]
train() client id: f_00009-3-1 loss: 0.708280  [   64/  118]
train() client id: f_00009-3-2 loss: 0.888363  [   96/  118]
train() client id: f_00009-4-0 loss: 0.916743  [   32/  118]
train() client id: f_00009-4-1 loss: 0.715238  [   64/  118]
train() client id: f_00009-4-2 loss: 0.684400  [   96/  118]
At round 73 accuracy: 0.6445623342175066
At round 73 training accuracy: 0.5895372233400402
At round 73 training loss: 0.8276120825551773
update_location
xs = [  -3.9056584     4.20031788  385.00902392   18.81129433    0.97929623
    3.95640986 -347.44319194 -326.32485185  369.66397685 -312.06087855]
ys = [ 377.5879595   360.55583871    1.32061395 -347.45517586  339.35018685
  322.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [390.6250393  374.18999919 397.78598834 362.04828963 353.77889752
 337.97134361 361.5572737  341.30424139 383.35378667 327.71634652]
dists_bs = [265.17004933 257.96070089 586.32297926 557.24521406 240.78973528
 232.10863791 247.56269812 230.73158702 566.95975654 219.14875693]
uav_gains = [7.85626588e-13 9.19780429e-13 7.37654771e-13 1.04796295e-12
 1.15469134e-12 1.42001853e-12 1.05381008e-12 1.35598266e-12
 8.40352469e-13 1.65176343e-12]
bs_gains = [1.80894587e-11 1.95408815e-11 1.96114959e-12 2.26133192e-12
 2.36977859e-11 2.62638529e-11 2.19268126e-11 2.67051078e-11
 2.15450662e-12 3.08478466e-11]
Round 74
-------------------------------
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.17965282 2.31354803 1.16601007 0.44876366 2.6663691  1.28432213
 0.54349708 1.6129016  1.17620535 1.04170236]
obj_prev = 13.43297219536657
eta_min = 1.985153234470665e-80	eta_max = 0.9640166902686983
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 3.044236871939507	eta = 0.9090909090909091
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 8.994246026442418	eta = 0.3076953929504865
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 5.5116993178884695	eta = 0.5021115822515108
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.941875446838901	eta = 0.5600076519876258
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.905748712654075	eta = 0.5641316397354368
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.905578223300182	eta = 0.5641512456685993
eta = 0.5641512456685993
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [0.04723579 0.09934512 0.04648602 0.01612015 0.11471549 0.05473354
 0.02024391 0.06710481 0.04873534 0.04423669]
ene_total = [0.51557111 0.70394459 0.51893062 0.28085198 0.80121777 0.40983951
 0.30711799 0.58944182 0.4390443  0.33961853]
ti_comp = [3.7239081  3.95531799 3.71124405 3.77370601 3.95943373 3.96148105
 3.77455137 3.80899893 3.85658756 3.96449942]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [4.75002260e-07 3.91702877e-06 4.55835322e-07 1.83844731e-08
 6.01838646e-06 6.53018300e-07 3.63942310e-08 1.30172529e-06
 4.86413493e-07 3.44232011e-07]
ene_total = [0.20841223 0.05795483 0.2166472  0.17602698 0.05529214 0.05392593
 0.17547739 0.15308527 0.12213433 0.05196115]
optimize_network_iter = 0 obj = 1.2709174591621142
eta = 0.5641512456685993
freqs = [ 6342233.67091708 12558423.2934157   6262861.66135149  2135851.43012001
 14486350.74522172  6908216.2161498   2681631.30754287  8808720.47508642
  6318453.65727492  5579101.94228267]
eta_min = 0.5641512456686001	eta_max = 0.8530565293483942
af = 6.786276083478417e-05	bf = 0.5538889445570291	zeta = 7.464903691826259e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [7.90879612e-08 6.52185990e-07 7.58966627e-08 3.06101806e-09
 1.00206242e-06 1.08727664e-07 6.05964597e-09 2.16737493e-07
 8.09879336e-08 5.73146914e-08]
ene_total = [0.99918599 0.27775315 1.03866771 0.84393208 0.2649327  0.25852208
 0.84129665 0.73390845 0.58554042 0.2491103 ]
ti_comp = [1.04304861 1.27445851 1.03038457 1.09284653 1.27857424 1.28062156
 1.09369189 1.12813944 1.17572807 1.28363993]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.29575823e-07 8.07435167e-07 1.26557476e-07 4.69146470e-09
 1.23519014e-06 1.33732939e-07 9.27711067e-09 3.17581333e-07
 1.12005080e-07 7.02717911e-08]
ene_total = [0.61816447 0.17183973 0.64259052 0.52211305 0.16390964 0.15993959
 0.52048262 0.45404696 0.36225519 0.1541166 ]
optimize_network_iter = 1 obj = 3.769458367807797
eta = 0.8530565293483942
freqs = [ 6286609.35895016 10821081.10112869  6262861.66135149  2047669.00897199
 12455063.70509777  5933111.71394756  2569503.70041527  8257350.63294458
  5754226.36383451  4783979.0224429 ]
eta_min = 0.8530565293483974	eta_max = 0.8530565293483936
af = 5.2542085376783516e-05	bf = 0.5538889445570291	zeta = 5.7796293914461874e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [7.77067690e-08 4.84219790e-07 7.58966627e-08 2.81347673e-09
 7.40744934e-07 8.01997961e-08 5.56349384e-09 1.90453887e-07
 6.71695741e-08 4.21420732e-08]
ene_total = [0.99918595 0.27774791 1.03866771 0.84393207 0.26492456 0.25852119
 0.84129664 0.73390763 0.58553999 0.24910983]
ti_comp = [1.04304861 1.27445851 1.03038457 1.09284653 1.27857424 1.28062156
 1.09369189 1.12813944 1.17572807 1.28363993]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.29575823e-07 8.07435167e-07 1.26557476e-07 4.69146470e-09
 1.23519014e-06 1.33732939e-07 9.27711067e-09 3.17581333e-07
 1.12005080e-07 7.02717911e-08]
ene_total = [0.61816447 0.17183973 0.64259052 0.52211305 0.16390964 0.15993959
 0.52048262 0.45404696 0.36225519 0.1541166 ]
optimize_network_iter = 2 obj = 3.7694583678077827
eta = 0.8530565293483936
freqs = [ 6286609.35895015 10821081.10112868  6262861.66135148  2047669.00897199
 12455063.70509776  5933111.71394756  2569503.70041527  8257350.63294457
  5754226.3638345   4783979.0224429 ]
Done!
At round 74 eta: 0.8530565293483936
At round 74 local rounds: 5.2041580597357475
At round 74 global rounds: 19.287758537054202
At round 74 a_n: 2.4916643335555797
gradient difference: 0.9602071642875671
train() client id: f_00000-0-0 loss: 1.140197  [   32/  126]
train() client id: f_00000-0-1 loss: 0.774699  [   64/  126]
train() client id: f_00000-0-2 loss: 0.817786  [   96/  126]
train() client id: f_00000-1-0 loss: 0.828928  [   32/  126]
train() client id: f_00000-1-1 loss: 0.764231  [   64/  126]
train() client id: f_00000-1-2 loss: 1.112890  [   96/  126]
train() client id: f_00000-2-0 loss: 1.050555  [   32/  126]
train() client id: f_00000-2-1 loss: 0.973135  [   64/  126]
train() client id: f_00000-2-2 loss: 1.042396  [   96/  126]
train() client id: f_00000-3-0 loss: 1.038984  [   32/  126]
train() client id: f_00000-3-1 loss: 0.871579  [   64/  126]
train() client id: f_00000-3-2 loss: 0.983195  [   96/  126]
train() client id: f_00000-4-0 loss: 0.999411  [   32/  126]
train() client id: f_00000-4-1 loss: 0.778564  [   64/  126]
train() client id: f_00000-4-2 loss: 1.131098  [   96/  126]
train() client id: f_00001-0-0 loss: 0.492424  [   32/  265]
train() client id: f_00001-0-1 loss: 0.472461  [   64/  265]
train() client id: f_00001-0-2 loss: 0.575469  [   96/  265]
train() client id: f_00001-0-3 loss: 0.442470  [  128/  265]
train() client id: f_00001-0-4 loss: 0.475998  [  160/  265]
train() client id: f_00001-0-5 loss: 0.574175  [  192/  265]
train() client id: f_00001-0-6 loss: 0.490082  [  224/  265]
train() client id: f_00001-0-7 loss: 0.444618  [  256/  265]
train() client id: f_00001-1-0 loss: 0.599172  [   32/  265]
train() client id: f_00001-1-1 loss: 0.438736  [   64/  265]
train() client id: f_00001-1-2 loss: 0.396416  [   96/  265]
train() client id: f_00001-1-3 loss: 0.433075  [  128/  265]
train() client id: f_00001-1-4 loss: 0.595799  [  160/  265]
train() client id: f_00001-1-5 loss: 0.560429  [  192/  265]
train() client id: f_00001-1-6 loss: 0.564511  [  224/  265]
train() client id: f_00001-1-7 loss: 0.443279  [  256/  265]
train() client id: f_00001-2-0 loss: 0.535226  [   32/  265]
train() client id: f_00001-2-1 loss: 0.587683  [   64/  265]
train() client id: f_00001-2-2 loss: 0.437340  [   96/  265]
train() client id: f_00001-2-3 loss: 0.492601  [  128/  265]
train() client id: f_00001-2-4 loss: 0.484791  [  160/  265]
train() client id: f_00001-2-5 loss: 0.529724  [  192/  265]
train() client id: f_00001-2-6 loss: 0.552412  [  224/  265]
train() client id: f_00001-2-7 loss: 0.450048  [  256/  265]
train() client id: f_00001-3-0 loss: 0.593979  [   32/  265]
train() client id: f_00001-3-1 loss: 0.435846  [   64/  265]
train() client id: f_00001-3-2 loss: 0.435296  [   96/  265]
train() client id: f_00001-3-3 loss: 0.453550  [  128/  265]
train() client id: f_00001-3-4 loss: 0.510195  [  160/  265]
train() client id: f_00001-3-5 loss: 0.395918  [  192/  265]
train() client id: f_00001-3-6 loss: 0.452668  [  224/  265]
train() client id: f_00001-3-7 loss: 0.712027  [  256/  265]
train() client id: f_00001-4-0 loss: 0.499319  [   32/  265]
train() client id: f_00001-4-1 loss: 0.491458  [   64/  265]
train() client id: f_00001-4-2 loss: 0.471153  [   96/  265]
train() client id: f_00001-4-3 loss: 0.466576  [  128/  265]
train() client id: f_00001-4-4 loss: 0.506804  [  160/  265]
train() client id: f_00001-4-5 loss: 0.564674  [  192/  265]
train() client id: f_00001-4-6 loss: 0.567886  [  224/  265]
train() client id: f_00001-4-7 loss: 0.432487  [  256/  265]
train() client id: f_00002-0-0 loss: 1.029311  [   32/  124]
train() client id: f_00002-0-1 loss: 1.095173  [   64/  124]
train() client id: f_00002-0-2 loss: 0.700923  [   96/  124]
train() client id: f_00002-1-0 loss: 0.806326  [   32/  124]
train() client id: f_00002-1-1 loss: 1.022798  [   64/  124]
train() client id: f_00002-1-2 loss: 0.982445  [   96/  124]
train() client id: f_00002-2-0 loss: 1.123916  [   32/  124]
train() client id: f_00002-2-1 loss: 1.062914  [   64/  124]
train() client id: f_00002-2-2 loss: 0.837294  [   96/  124]
train() client id: f_00002-3-0 loss: 1.015288  [   32/  124]
train() client id: f_00002-3-1 loss: 1.075201  [   64/  124]
train() client id: f_00002-3-2 loss: 0.715990  [   96/  124]
train() client id: f_00002-4-0 loss: 0.942512  [   32/  124]
train() client id: f_00002-4-1 loss: 0.764133  [   64/  124]
train() client id: f_00002-4-2 loss: 1.017401  [   96/  124]
train() client id: f_00003-0-0 loss: 0.830809  [   32/   43]
train() client id: f_00003-1-0 loss: 0.924816  [   32/   43]
train() client id: f_00003-2-0 loss: 0.725538  [   32/   43]
train() client id: f_00003-3-0 loss: 0.913170  [   32/   43]
train() client id: f_00003-4-0 loss: 0.926988  [   32/   43]
train() client id: f_00004-0-0 loss: 0.790266  [   32/  306]
train() client id: f_00004-0-1 loss: 0.802777  [   64/  306]
train() client id: f_00004-0-2 loss: 0.960284  [   96/  306]
train() client id: f_00004-0-3 loss: 0.851937  [  128/  306]
train() client id: f_00004-0-4 loss: 0.736275  [  160/  306]
train() client id: f_00004-0-5 loss: 0.757028  [  192/  306]
train() client id: f_00004-0-6 loss: 0.765282  [  224/  306]
train() client id: f_00004-0-7 loss: 0.647269  [  256/  306]
train() client id: f_00004-0-8 loss: 0.791166  [  288/  306]
train() client id: f_00004-1-0 loss: 0.725603  [   32/  306]
train() client id: f_00004-1-1 loss: 0.778442  [   64/  306]
train() client id: f_00004-1-2 loss: 0.885186  [   96/  306]
train() client id: f_00004-1-3 loss: 0.768749  [  128/  306]
train() client id: f_00004-1-4 loss: 0.837097  [  160/  306]
train() client id: f_00004-1-5 loss: 0.817763  [  192/  306]
train() client id: f_00004-1-6 loss: 0.737725  [  224/  306]
train() client id: f_00004-1-7 loss: 0.741791  [  256/  306]
train() client id: f_00004-1-8 loss: 0.816744  [  288/  306]
train() client id: f_00004-2-0 loss: 0.742598  [   32/  306]
train() client id: f_00004-2-1 loss: 0.684388  [   64/  306]
train() client id: f_00004-2-2 loss: 0.800335  [   96/  306]
train() client id: f_00004-2-3 loss: 0.714223  [  128/  306]
train() client id: f_00004-2-4 loss: 0.852341  [  160/  306]
train() client id: f_00004-2-5 loss: 0.733821  [  192/  306]
train() client id: f_00004-2-6 loss: 0.926963  [  224/  306]
train() client id: f_00004-2-7 loss: 0.802996  [  256/  306]
train() client id: f_00004-2-8 loss: 0.787094  [  288/  306]
train() client id: f_00004-3-0 loss: 0.743662  [   32/  306]
train() client id: f_00004-3-1 loss: 0.802426  [   64/  306]
train() client id: f_00004-3-2 loss: 0.703266  [   96/  306]
train() client id: f_00004-3-3 loss: 0.754320  [  128/  306]
train() client id: f_00004-3-4 loss: 0.704408  [  160/  306]
train() client id: f_00004-3-5 loss: 0.694065  [  192/  306]
train() client id: f_00004-3-6 loss: 0.871518  [  224/  306]
train() client id: f_00004-3-7 loss: 0.814149  [  256/  306]
train() client id: f_00004-3-8 loss: 0.933278  [  288/  306]
train() client id: f_00004-4-0 loss: 0.692483  [   32/  306]
train() client id: f_00004-4-1 loss: 0.639342  [   64/  306]
train() client id: f_00004-4-2 loss: 0.898742  [   96/  306]
train() client id: f_00004-4-3 loss: 0.779258  [  128/  306]
train() client id: f_00004-4-4 loss: 0.756471  [  160/  306]
train() client id: f_00004-4-5 loss: 0.710005  [  192/  306]
train() client id: f_00004-4-6 loss: 0.928450  [  224/  306]
train() client id: f_00004-4-7 loss: 0.789031  [  256/  306]
train() client id: f_00004-4-8 loss: 0.880343  [  288/  306]
train() client id: f_00005-0-0 loss: 0.843303  [   32/  146]
train() client id: f_00005-0-1 loss: 0.524349  [   64/  146]
train() client id: f_00005-0-2 loss: 0.853066  [   96/  146]
train() client id: f_00005-0-3 loss: 0.784683  [  128/  146]
train() client id: f_00005-1-0 loss: 0.716495  [   32/  146]
train() client id: f_00005-1-1 loss: 0.663257  [   64/  146]
train() client id: f_00005-1-2 loss: 0.853669  [   96/  146]
train() client id: f_00005-1-3 loss: 0.889384  [  128/  146]
train() client id: f_00005-2-0 loss: 0.790471  [   32/  146]
train() client id: f_00005-2-1 loss: 0.960254  [   64/  146]
train() client id: f_00005-2-2 loss: 0.875785  [   96/  146]
train() client id: f_00005-2-3 loss: 0.599408  [  128/  146]
train() client id: f_00005-3-0 loss: 0.827681  [   32/  146]
train() client id: f_00005-3-1 loss: 0.777378  [   64/  146]
train() client id: f_00005-3-2 loss: 0.848193  [   96/  146]
train() client id: f_00005-3-3 loss: 0.686604  [  128/  146]
train() client id: f_00005-4-0 loss: 0.732949  [   32/  146]
train() client id: f_00005-4-1 loss: 0.882462  [   64/  146]
train() client id: f_00005-4-2 loss: 0.909248  [   96/  146]
train() client id: f_00005-4-3 loss: 0.583379  [  128/  146]
train() client id: f_00006-0-0 loss: 0.581820  [   32/   54]
train() client id: f_00006-1-0 loss: 0.576027  [   32/   54]
train() client id: f_00006-2-0 loss: 0.489118  [   32/   54]
train() client id: f_00006-3-0 loss: 0.537284  [   32/   54]
train() client id: f_00006-4-0 loss: 0.475588  [   32/   54]
train() client id: f_00007-0-0 loss: 0.843738  [   32/  179]
train() client id: f_00007-0-1 loss: 0.753570  [   64/  179]
train() client id: f_00007-0-2 loss: 0.743740  [   96/  179]
train() client id: f_00007-0-3 loss: 0.689332  [  128/  179]
train() client id: f_00007-0-4 loss: 0.860490  [  160/  179]
train() client id: f_00007-1-0 loss: 0.632301  [   32/  179]
train() client id: f_00007-1-1 loss: 0.861119  [   64/  179]
train() client id: f_00007-1-2 loss: 0.773488  [   96/  179]
train() client id: f_00007-1-3 loss: 0.845071  [  128/  179]
train() client id: f_00007-1-4 loss: 0.842305  [  160/  179]
train() client id: f_00007-2-0 loss: 0.762848  [   32/  179]
train() client id: f_00007-2-1 loss: 0.565872  [   64/  179]
train() client id: f_00007-2-2 loss: 0.924567  [   96/  179]
train() client id: f_00007-2-3 loss: 0.745940  [  128/  179]
train() client id: f_00007-2-4 loss: 0.906751  [  160/  179]
train() client id: f_00007-3-0 loss: 0.641713  [   32/  179]
train() client id: f_00007-3-1 loss: 0.811658  [   64/  179]
train() client id: f_00007-3-2 loss: 0.734689  [   96/  179]
train() client id: f_00007-3-3 loss: 0.803433  [  128/  179]
train() client id: f_00007-3-4 loss: 0.888123  [  160/  179]
train() client id: f_00007-4-0 loss: 0.766862  [   32/  179]
train() client id: f_00007-4-1 loss: 0.723728  [   64/  179]
train() client id: f_00007-4-2 loss: 0.751243  [   96/  179]
train() client id: f_00007-4-3 loss: 0.896636  [  128/  179]
train() client id: f_00007-4-4 loss: 0.801067  [  160/  179]
train() client id: f_00008-0-0 loss: 0.857909  [   32/  130]
train() client id: f_00008-0-1 loss: 0.732612  [   64/  130]
train() client id: f_00008-0-2 loss: 0.788283  [   96/  130]
train() client id: f_00008-0-3 loss: 0.889661  [  128/  130]
train() client id: f_00008-1-0 loss: 0.746175  [   32/  130]
train() client id: f_00008-1-1 loss: 0.832878  [   64/  130]
train() client id: f_00008-1-2 loss: 0.805314  [   96/  130]
train() client id: f_00008-1-3 loss: 0.885357  [  128/  130]
train() client id: f_00008-2-0 loss: 0.843612  [   32/  130]
train() client id: f_00008-2-1 loss: 0.872718  [   64/  130]
train() client id: f_00008-2-2 loss: 0.818331  [   96/  130]
train() client id: f_00008-2-3 loss: 0.746444  [  128/  130]
train() client id: f_00008-3-0 loss: 0.766222  [   32/  130]
train() client id: f_00008-3-1 loss: 0.855739  [   64/  130]
train() client id: f_00008-3-2 loss: 0.831775  [   96/  130]
train() client id: f_00008-3-3 loss: 0.826723  [  128/  130]
train() client id: f_00008-4-0 loss: 0.861591  [   32/  130]
train() client id: f_00008-4-1 loss: 0.836848  [   64/  130]
train() client id: f_00008-4-2 loss: 0.755486  [   96/  130]
train() client id: f_00008-4-3 loss: 0.828742  [  128/  130]
train() client id: f_00009-0-0 loss: 1.011914  [   32/  118]
train() client id: f_00009-0-1 loss: 0.743856  [   64/  118]
train() client id: f_00009-0-2 loss: 0.925964  [   96/  118]
train() client id: f_00009-1-0 loss: 0.934636  [   32/  118]
train() client id: f_00009-1-1 loss: 0.892352  [   64/  118]
train() client id: f_00009-1-2 loss: 0.893268  [   96/  118]
train() client id: f_00009-2-0 loss: 0.758096  [   32/  118]
train() client id: f_00009-2-1 loss: 0.908189  [   64/  118]
train() client id: f_00009-2-2 loss: 0.970608  [   96/  118]
train() client id: f_00009-3-0 loss: 0.814736  [   32/  118]
train() client id: f_00009-3-1 loss: 0.754379  [   64/  118]
train() client id: f_00009-3-2 loss: 0.958265  [   96/  118]
train() client id: f_00009-4-0 loss: 0.763616  [   32/  118]
train() client id: f_00009-4-1 loss: 0.869950  [   64/  118]
train() client id: f_00009-4-2 loss: 0.828304  [   96/  118]
At round 74 accuracy: 0.6445623342175066
At round 74 training accuracy: 0.5895372233400402
At round 74 training loss: 0.8225366015478605
update_location
xs = [  -3.9056584     4.20031788  390.00902392   18.81129433    0.97929623
    3.95640986 -352.44319194 -331.32485185  374.66397685 -317.06087855]
ys = [ 382.5879595   365.55583871    1.32061395 -352.45517586  344.35018685
  327.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [395.46023937 379.01017649 402.62734974 366.84944567 358.57776033
 342.75030358 366.36470093 346.08789882 388.17749229 332.48099579]
dists_bs = [269.00935793 261.58035399 591.10043668 561.93574397 244.22980671
 235.32437464 251.07114812 234.04126509 571.76481643 222.31011324]
uav_gains = [7.52650140e-13 8.76404134e-13 7.08086688e-13 9.93683329e-13
 1.09055942e-12 1.32968634e-12 9.98935416e-13 1.27217390e-12
 8.03318596e-13 1.53701405e-12]
bs_gains = [1.73758236e-11 1.87931569e-11 1.91709000e-12 2.20887654e-12
 2.27749704e-11 2.52712502e-11 2.10796318e-11 2.56610996e-11
 2.10419163e-12 2.96352278e-11]
Round 75
-------------------------------
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.03922851 2.03415137 1.02725189 0.39661097 2.34431442 1.12928775
 0.47989662 1.41999007 1.0345605  0.91598534]
obj_prev = 11.821277437641127
eta_min = 2.632614097881737e-91	eta_max = 0.9674950324086027
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 2.6763069615753383	eta = 0.9090909090909091
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 8.03601399311375	eta = 0.3027628287842397
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.8844582536009025	eta = 0.4981118073659859
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.372075946270912	eta = 0.5564876636646818
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.3396007499684774	eta = 0.5606521126909028
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.3394472210647415	eta = 0.5606719484671788
eta = 0.5606719484671788
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [0.04774628 0.10041876 0.0469884  0.01629436 0.11595525 0.05532505
 0.02046269 0.06783003 0.04926203 0.04471477]
ene_total = [0.45740881 0.62055196 0.46033181 0.25063986 0.70629813 0.3612359
 0.2738053  0.5227749  0.38706651 0.29933403]
ti_comp = [4.2992849  4.53835717 4.28655232 4.34933896 4.54253515 4.54464388
 4.35017711 4.38486951 4.43846938 4.54768586]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [3.68049423e-07 3.07275006e-06 3.52887078e-07 1.42937379e-08
 4.72231164e-06 5.12443289e-07 2.82979099e-08 1.01445480e-06
 3.79271804e-07 2.70179295e-07]
ene_total = [0.18661622 0.05104089 0.19383747 0.15822588 0.04868069 0.04746084
 0.1577506  0.13808028 0.10767732 0.0457342 ]
optimize_network_iter = 0 obj = 1.1351043989055445
eta = 0.5606719484671788
freqs = [ 5552816.27091243 11063338.14405196  5480908.36118572  1873200.15342277
 12763274.63448044  6086841.3557449   2351937.65383736  7734555.17253756
  5549439.33843933  4916211.22498621]
eta_min = 0.5606719484671795	eta_max = 0.8680569114774389
af = 4.622705191075305e-05	bf = 0.4986564559927847	zeta = 5.084975710182836e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [6.06251059e-08 5.06143430e-07 5.81275641e-08 2.35446469e-09
 7.77859235e-07 8.44096653e-08 4.66123208e-09 1.67101009e-07
 6.24736566e-08 4.45039370e-08]
ene_total = [0.90183473 0.24659032 0.93673252 0.76464342 0.23514662 0.22934794
 0.76234626 0.66726472 0.52035367 0.22100929]
ti_comp = [1.06098413 1.30005639 1.04825154 1.11103819 1.30423438 1.30634311
 1.11187634 1.14656874 1.20016861 1.30938509]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [8.83797899e-08 5.47611225e-07 8.62964911e-08 3.20336552e-09
 8.37745486e-07 9.06991369e-08 6.33471974e-09 2.16979151e-07
 7.58585669e-08 4.76617893e-08]
ene_total = [0.62136672 0.16990208 0.64541139 0.52684109 0.16201769 0.15802138
 0.52525835 0.45974786 0.35852513 0.15227597]
optimize_network_iter = 1 obj = 3.7793676563367
eta = 0.8680569114774389
freqs = [ 5502474.32221998  9444523.43544286  5480908.36118572  1793229.33817215
 10870816.12174629  5178356.71698614  2250264.85743831  7233513.5913764
  5018771.87691086  4175523.98380345]
eta_min = 0.8680569114774714	eta_max = 0.8680569114774374
af = 3.523192231617816e-05	bf = 0.4986564559927847	zeta = 3.8755114547795976e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [5.95308319e-08 3.68859801e-07 5.81275641e-08 2.15772197e-09
 5.64288349e-07 6.10930969e-08 4.26693859e-09 1.46152750e-07
 5.10967904e-08 3.21040135e-08]
ene_total = [0.9018347  0.24658655 0.93673252 0.76464342 0.23514077 0.2293473
 0.76234625 0.66726414 0.52035336 0.22100895]
ti_comp = [1.06098413 1.30005639 1.04825154 1.11103819 1.30423438 1.30634311
 1.11187634 1.14656874 1.20016861 1.30938509]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [8.83797899e-08 5.47611225e-07 8.62964911e-08 3.20336552e-09
 8.37745486e-07 9.06991369e-08 6.33471974e-09 2.16979151e-07
 7.58585669e-08 4.76617893e-08]
ene_total = [0.62136672 0.16990208 0.64541139 0.52684109 0.16201769 0.15802138
 0.52525835 0.45974786 0.35852513 0.15227597]
optimize_network_iter = 2 obj = 3.779367656336656
eta = 0.8680569114774374
freqs = [ 5502474.32221997  9444523.43544286  5480908.3611857   1793229.33817215
 10870816.12174629  5178356.71698613  2250264.85743831  7233513.59137638
  5018771.87691085  4175523.98380344]
Done!
At round 75 eta: 0.8680569114774374
At round 75 local rounds: 4.633363424333758
At round 75 global rounds: 18.884386908447258
At round 75 a_n: 2.1491184865862607
gradient difference: 0.9847601056098938
train() client id: f_00000-0-0 loss: 0.942286  [   32/  126]
train() client id: f_00000-0-1 loss: 0.509869  [   64/  126]
train() client id: f_00000-0-2 loss: 0.564213  [   96/  126]
train() client id: f_00000-1-0 loss: 0.632122  [   32/  126]
train() client id: f_00000-1-1 loss: 0.739397  [   64/  126]
train() client id: f_00000-1-2 loss: 0.755051  [   96/  126]
train() client id: f_00000-2-0 loss: 0.934198  [   32/  126]
train() client id: f_00000-2-1 loss: 0.731736  [   64/  126]
train() client id: f_00000-2-2 loss: 0.676216  [   96/  126]
train() client id: f_00000-3-0 loss: 0.726918  [   32/  126]
train() client id: f_00000-3-1 loss: 0.933600  [   64/  126]
train() client id: f_00000-3-2 loss: 0.636535  [   96/  126]
train() client id: f_00001-0-0 loss: 0.578377  [   32/  265]
train() client id: f_00001-0-1 loss: 0.613877  [   64/  265]
train() client id: f_00001-0-2 loss: 0.660182  [   96/  265]
train() client id: f_00001-0-3 loss: 0.479342  [  128/  265]
train() client id: f_00001-0-4 loss: 0.474713  [  160/  265]
train() client id: f_00001-0-5 loss: 0.661206  [  192/  265]
train() client id: f_00001-0-6 loss: 0.515947  [  224/  265]
train() client id: f_00001-0-7 loss: 0.726489  [  256/  265]
train() client id: f_00001-1-0 loss: 0.669533  [   32/  265]
train() client id: f_00001-1-1 loss: 0.627029  [   64/  265]
train() client id: f_00001-1-2 loss: 0.515844  [   96/  265]
train() client id: f_00001-1-3 loss: 0.550305  [  128/  265]
train() client id: f_00001-1-4 loss: 0.476716  [  160/  265]
train() client id: f_00001-1-5 loss: 0.502001  [  192/  265]
train() client id: f_00001-1-6 loss: 0.655149  [  224/  265]
train() client id: f_00001-1-7 loss: 0.591684  [  256/  265]
train() client id: f_00001-2-0 loss: 0.613459  [   32/  265]
train() client id: f_00001-2-1 loss: 0.593726  [   64/  265]
train() client id: f_00001-2-2 loss: 0.574241  [   96/  265]
train() client id: f_00001-2-3 loss: 0.514890  [  128/  265]
train() client id: f_00001-2-4 loss: 0.569705  [  160/  265]
train() client id: f_00001-2-5 loss: 0.544976  [  192/  265]
train() client id: f_00001-2-6 loss: 0.534319  [  224/  265]
train() client id: f_00001-2-7 loss: 0.708202  [  256/  265]
train() client id: f_00001-3-0 loss: 0.801743  [   32/  265]
train() client id: f_00001-3-1 loss: 0.561260  [   64/  265]
train() client id: f_00001-3-2 loss: 0.533581  [   96/  265]
train() client id: f_00001-3-3 loss: 0.522295  [  128/  265]
train() client id: f_00001-3-4 loss: 0.631871  [  160/  265]
train() client id: f_00001-3-5 loss: 0.515238  [  192/  265]
train() client id: f_00001-3-6 loss: 0.556740  [  224/  265]
train() client id: f_00001-3-7 loss: 0.596663  [  256/  265]
train() client id: f_00002-0-0 loss: 1.026587  [   32/  124]
train() client id: f_00002-0-1 loss: 0.705929  [   64/  124]
train() client id: f_00002-0-2 loss: 0.814423  [   96/  124]
train() client id: f_00002-1-0 loss: 0.663888  [   32/  124]
train() client id: f_00002-1-1 loss: 1.073399  [   64/  124]
train() client id: f_00002-1-2 loss: 0.731820  [   96/  124]
train() client id: f_00002-2-0 loss: 0.901128  [   32/  124]
train() client id: f_00002-2-1 loss: 0.709085  [   64/  124]
train() client id: f_00002-2-2 loss: 0.868364  [   96/  124]
train() client id: f_00002-3-0 loss: 0.819137  [   32/  124]
train() client id: f_00002-3-1 loss: 0.874191  [   64/  124]
train() client id: f_00002-3-2 loss: 0.938524  [   96/  124]
train() client id: f_00003-0-0 loss: 0.550156  [   32/   43]
train() client id: f_00003-1-0 loss: 0.361394  [   32/   43]
train() client id: f_00003-2-0 loss: 0.419867  [   32/   43]
train() client id: f_00003-3-0 loss: 0.666573  [   32/   43]
train() client id: f_00004-0-0 loss: 0.663735  [   32/  306]
train() client id: f_00004-0-1 loss: 0.806509  [   64/  306]
train() client id: f_00004-0-2 loss: 0.857090  [   96/  306]
train() client id: f_00004-0-3 loss: 0.853401  [  128/  306]
train() client id: f_00004-0-4 loss: 0.922330  [  160/  306]
train() client id: f_00004-0-5 loss: 0.723239  [  192/  306]
train() client id: f_00004-0-6 loss: 0.704341  [  224/  306]
train() client id: f_00004-0-7 loss: 0.827037  [  256/  306]
train() client id: f_00004-0-8 loss: 0.812945  [  288/  306]
train() client id: f_00004-1-0 loss: 0.919740  [   32/  306]
train() client id: f_00004-1-1 loss: 0.906470  [   64/  306]
train() client id: f_00004-1-2 loss: 0.691960  [   96/  306]
train() client id: f_00004-1-3 loss: 0.640923  [  128/  306]
train() client id: f_00004-1-4 loss: 0.703303  [  160/  306]
train() client id: f_00004-1-5 loss: 0.826007  [  192/  306]
train() client id: f_00004-1-6 loss: 0.672023  [  224/  306]
train() client id: f_00004-1-7 loss: 0.894360  [  256/  306]
train() client id: f_00004-1-8 loss: 0.885186  [  288/  306]
train() client id: f_00004-2-0 loss: 0.784908  [   32/  306]
train() client id: f_00004-2-1 loss: 0.656299  [   64/  306]
train() client id: f_00004-2-2 loss: 0.778240  [   96/  306]
train() client id: f_00004-2-3 loss: 0.619899  [  128/  306]
train() client id: f_00004-2-4 loss: 0.773103  [  160/  306]
train() client id: f_00004-2-5 loss: 0.854994  [  192/  306]
train() client id: f_00004-2-6 loss: 0.813151  [  224/  306]
train() client id: f_00004-2-7 loss: 0.794756  [  256/  306]
train() client id: f_00004-2-8 loss: 1.018713  [  288/  306]
train() client id: f_00004-3-0 loss: 0.719648  [   32/  306]
train() client id: f_00004-3-1 loss: 0.756811  [   64/  306]
train() client id: f_00004-3-2 loss: 0.854172  [   96/  306]
train() client id: f_00004-3-3 loss: 0.746384  [  128/  306]
train() client id: f_00004-3-4 loss: 0.847901  [  160/  306]
train() client id: f_00004-3-5 loss: 0.757836  [  192/  306]
train() client id: f_00004-3-6 loss: 0.942581  [  224/  306]
train() client id: f_00004-3-7 loss: 0.811971  [  256/  306]
train() client id: f_00004-3-8 loss: 0.793909  [  288/  306]
train() client id: f_00005-0-0 loss: 0.999591  [   32/  146]
train() client id: f_00005-0-1 loss: 0.709702  [   64/  146]
train() client id: f_00005-0-2 loss: 0.614423  [   96/  146]
train() client id: f_00005-0-3 loss: 0.914967  [  128/  146]
train() client id: f_00005-1-0 loss: 0.948410  [   32/  146]
train() client id: f_00005-1-1 loss: 0.732956  [   64/  146]
train() client id: f_00005-1-2 loss: 0.889442  [   96/  146]
train() client id: f_00005-1-3 loss: 0.795498  [  128/  146]
train() client id: f_00005-2-0 loss: 0.792616  [   32/  146]
train() client id: f_00005-2-1 loss: 0.935723  [   64/  146]
train() client id: f_00005-2-2 loss: 0.598737  [   96/  146]
train() client id: f_00005-2-3 loss: 1.066510  [  128/  146]
train() client id: f_00005-3-0 loss: 0.879908  [   32/  146]
train() client id: f_00005-3-1 loss: 0.703009  [   64/  146]
train() client id: f_00005-3-2 loss: 0.793454  [   96/  146]
train() client id: f_00005-3-3 loss: 0.965532  [  128/  146]
train() client id: f_00006-0-0 loss: 0.551551  [   32/   54]
train() client id: f_00006-1-0 loss: 0.551170  [   32/   54]
train() client id: f_00006-2-0 loss: 0.502769  [   32/   54]
train() client id: f_00006-3-0 loss: 0.541904  [   32/   54]
train() client id: f_00007-0-0 loss: 0.693814  [   32/  179]
train() client id: f_00007-0-1 loss: 0.538478  [   64/  179]
train() client id: f_00007-0-2 loss: 0.649777  [   96/  179]
train() client id: f_00007-0-3 loss: 0.544313  [  128/  179]
train() client id: f_00007-0-4 loss: 0.578895  [  160/  179]
train() client id: f_00007-1-0 loss: 0.934777  [   32/  179]
train() client id: f_00007-1-1 loss: 0.640247  [   64/  179]
train() client id: f_00007-1-2 loss: 0.442215  [   96/  179]
train() client id: f_00007-1-3 loss: 0.574195  [  128/  179]
train() client id: f_00007-1-4 loss: 0.614146  [  160/  179]
train() client id: f_00007-2-0 loss: 0.615636  [   32/  179]
train() client id: f_00007-2-1 loss: 0.511550  [   64/  179]
train() client id: f_00007-2-2 loss: 0.582961  [   96/  179]
train() client id: f_00007-2-3 loss: 0.594461  [  128/  179]
train() client id: f_00007-2-4 loss: 0.865168  [  160/  179]
train() client id: f_00007-3-0 loss: 0.624109  [   32/  179]
train() client id: f_00007-3-1 loss: 0.547628  [   64/  179]
train() client id: f_00007-3-2 loss: 0.837498  [   96/  179]
train() client id: f_00007-3-3 loss: 0.477957  [  128/  179]
train() client id: f_00007-3-4 loss: 0.709001  [  160/  179]
train() client id: f_00008-0-0 loss: 0.752489  [   32/  130]
train() client id: f_00008-0-1 loss: 0.624318  [   64/  130]
train() client id: f_00008-0-2 loss: 0.599133  [   96/  130]
train() client id: f_00008-0-3 loss: 0.862390  [  128/  130]
train() client id: f_00008-1-0 loss: 0.704932  [   32/  130]
train() client id: f_00008-1-1 loss: 0.784436  [   64/  130]
train() client id: f_00008-1-2 loss: 0.673306  [   96/  130]
train() client id: f_00008-1-3 loss: 0.674109  [  128/  130]
train() client id: f_00008-2-0 loss: 0.763863  [   32/  130]
train() client id: f_00008-2-1 loss: 0.714998  [   64/  130]
train() client id: f_00008-2-2 loss: 0.627521  [   96/  130]
train() client id: f_00008-2-3 loss: 0.710490  [  128/  130]
train() client id: f_00008-3-0 loss: 0.689113  [   32/  130]
train() client id: f_00008-3-1 loss: 0.659417  [   64/  130]
train() client id: f_00008-3-2 loss: 0.683904  [   96/  130]
train() client id: f_00008-3-3 loss: 0.801202  [  128/  130]
train() client id: f_00009-0-0 loss: 0.843139  [   32/  118]
train() client id: f_00009-0-1 loss: 0.918386  [   64/  118]
train() client id: f_00009-0-2 loss: 0.846354  [   96/  118]
train() client id: f_00009-1-0 loss: 0.806324  [   32/  118]
train() client id: f_00009-1-1 loss: 0.951556  [   64/  118]
train() client id: f_00009-1-2 loss: 0.776824  [   96/  118]
train() client id: f_00009-2-0 loss: 1.068707  [   32/  118]
train() client id: f_00009-2-1 loss: 0.615492  [   64/  118]
train() client id: f_00009-2-2 loss: 0.735691  [   96/  118]
train() client id: f_00009-3-0 loss: 0.829724  [   32/  118]
train() client id: f_00009-3-1 loss: 0.832786  [   64/  118]
train() client id: f_00009-3-2 loss: 0.829140  [   96/  118]
At round 75 accuracy: 0.6445623342175066
At round 75 training accuracy: 0.5902079141515761
At round 75 training loss: 0.8199843697448106
update_location
xs = [  -3.9056584     4.20031788  395.00902392   18.81129433    0.97929623
    3.95640986 -357.44319194 -336.32485185  379.66397685 -322.06087855]
ys = [ 387.5879595   370.55583871    1.32061395 -357.45517586  349.35018685
  332.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [400.29948853 383.83495447 407.47254263 371.65584557 363.38204698
 347.53548324 371.17721644 350.87758866 393.00560465 337.25245937]
dists_bs = [272.88626614 265.24486796 595.88154568 566.63156646 247.72303154
 238.60155659 254.62944319 237.41011412 576.57319143 225.53801284]
uav_gains = [7.22036186e-13 8.36522048e-13 6.80532523e-13 9.44154307e-13
 1.03236608e-12 1.24853056e-12 9.48887642e-13 1.19672866e-12
 7.69075581e-13 1.43449114e-12]
bs_gains = [1.66934225e-11 1.80751751e-11 1.87433090e-12 2.15800250e-12
 2.18870991e-11 2.43113414e-11 2.02651559e-11 2.46545045e-11
 2.05542502e-12 2.84628764e-11]
Round 76
-------------------------------
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.89822293 1.75469717 0.88790864 0.34389424 2.02220671 0.97420485
 0.41573163 1.22653712 0.89277918 0.79022156]
obj_prev = 10.206404031521155
eta_min = 1.1840901563596855e-105	eta_max = 0.9712061927430241
af = 2.098524592010151	bf = 0.440277105517973	zeta = 2.308377051211166	eta = 0.9090909090909091
af = 2.098524592010151	bf = 0.440277105517973	zeta = 7.043166478768837	eta = 0.29795186559000353
af = 2.098524592010151	bf = 0.440277105517973	zeta = 4.246460455912228	eta = 0.49418206381468455
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.794695110359663	eta = 0.5530153361415251
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.766078122653466	eta = 0.557217488237231
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.765942672970741	eta = 0.5572375296819754
eta = 0.5572375296819754
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [0.0482533  0.10148511 0.04748737 0.0164674  0.11718658 0.05591255
 0.02067998 0.06855032 0.04978515 0.0451896 ]
ene_total = [0.39809468 0.53669908 0.4005926  0.21938841 0.6108545  0.31238457
 0.23943255 0.45484532 0.3348021  0.25884885]
ti_comp = [5.06004778 5.30681242 5.04724229 5.11036031 5.31105174 5.31322071
 5.11119117 5.14610005 5.20576198 5.31628572]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [2.74253247e-07 2.31963433e-06 2.62727860e-07 1.06868990e-08
 3.56576398e-06 3.86983308e-07 2.11585747e-08 7.60242285e-07
 2.84584225e-07 2.04069817e-07]
ene_total = [0.16388001 0.0441132  0.1700956  0.13945761 0.04206153 0.04099331
 0.13905437 0.12211358 0.09315205 0.0395047 ]
optimize_network_iter = 0 obj = 0.9944259543834252
eta = 0.5572375296819754
freqs = [ 4768067.39444     9561776.65897993  4704288.96039803  1611177.51564226
 11032332.74977597  5261643.95832219  2023010.29856296  6660414.59689484
  4781734.99264716  4250109.82064523]
eta_min = 0.5572375296819757	eta_max = 0.8837926289249317
af = 2.9732682841315033e-05	bf = 0.440277105517973	zeta = 3.2705951125446536e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [4.47003242e-08 3.78075401e-07 4.28218103e-08 1.74184938e-09
 5.81181107e-07 6.30741094e-08 3.44861970e-09 1.23911301e-07
 4.63841622e-08 3.32611812e-08]
ene_total = [0.79815367 0.21480247 0.82842621 0.67921232 0.20478538 0.19964562
 0.67724819 0.59472538 0.45368094 0.19239915]
ti_comp = [1.07904836 1.325813   1.06624286 1.12936088 1.33005232 1.33222129
 1.13019174 1.16510062 1.22476255 1.33528629]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.68571582e-08 3.50371208e-07 5.55019335e-08 2.06298094e-09
 5.36021441e-07 5.80311479e-08 4.07973874e-09 1.39825878e-07
 4.84709564e-08 3.04966807e-08]
ene_total = [0.62439628 0.16803958 0.64807852 0.53134818 0.16020288 0.15618279
 0.52981165 0.46525429 0.35491489 0.15051391]
optimize_network_iter = 1 obj = 3.7887429688072136
eta = 0.8837926289249317
freqs = [4723436.47190019 8085223.78253195 4704288.96039802 1540154.03017325
 9306387.77331266 4433073.47420022 1932725.03801711 6214669.51457857
 4293583.32001691 3574670.85257318]
eta_min = 0.8837926289249345	eta_max = 0.8837926289249313
af = 2.229825334963228e-05	bf = 0.440277105517973	zeta = 2.452807868459551e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [4.38674166e-08 2.70324445e-07 4.28218103e-08 1.59166668e-09
 4.13560520e-07 4.47731935e-08 3.14767049e-09 1.07880876e-07
 3.73971494e-08 2.35293258e-08]
ene_total = [0.79815365 0.21479992 0.82842621 0.67921232 0.20478142 0.19964519
 0.67724818 0.594725   0.45368072 0.19239892]
ti_comp = [1.07904836 1.325813   1.06624286 1.12936088 1.33005232 1.33222129
 1.13019174 1.16510062 1.22476255 1.33528629]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.68571582e-08 3.50371208e-07 5.55019335e-08 2.06298094e-09
 5.36021441e-07 5.80311479e-08 4.07973874e-09 1.39825878e-07
 4.84709564e-08 3.04966807e-08]
ene_total = [0.62439628 0.16803958 0.64807852 0.53134818 0.16020288 0.15618279
 0.52981165 0.46525429 0.35491489 0.15051391]
optimize_network_iter = 2 obj = 3.788742968807199
eta = 0.8837926289249313
freqs = [4723436.47190018 8085223.78253194 4704288.96039801 1540154.03017325
 9306387.77331265 4433073.47420021 1932725.03801711 6214669.51457855
 4293583.3200169  3574670.85257317]
Done!
At round 76 eta: 0.8837926289249313
At round 76 local rounds: 4.045092362532633
At round 76 global rounds: 18.49382243745926
At round 76 a_n: 1.8065726396169453
gradient difference: 0.8213909864425659
train() client id: f_00000-0-0 loss: 1.052393  [   32/  126]
train() client id: f_00000-0-1 loss: 1.303630  [   64/  126]
train() client id: f_00000-0-2 loss: 1.006287  [   96/  126]
train() client id: f_00000-1-0 loss: 1.187092  [   32/  126]
train() client id: f_00000-1-1 loss: 1.014477  [   64/  126]
train() client id: f_00000-1-2 loss: 1.098740  [   96/  126]
train() client id: f_00000-2-0 loss: 1.123773  [   32/  126]
train() client id: f_00000-2-1 loss: 1.030404  [   64/  126]
train() client id: f_00000-2-2 loss: 1.012382  [   96/  126]
train() client id: f_00000-3-0 loss: 1.073283  [   32/  126]
train() client id: f_00000-3-1 loss: 1.028502  [   64/  126]
train() client id: f_00000-3-2 loss: 0.911881  [   96/  126]
train() client id: f_00001-0-0 loss: 0.678996  [   32/  265]
train() client id: f_00001-0-1 loss: 0.432138  [   64/  265]
train() client id: f_00001-0-2 loss: 0.487888  [   96/  265]
train() client id: f_00001-0-3 loss: 0.534649  [  128/  265]
train() client id: f_00001-0-4 loss: 0.427163  [  160/  265]
train() client id: f_00001-0-5 loss: 0.596375  [  192/  265]
train() client id: f_00001-0-6 loss: 0.509332  [  224/  265]
train() client id: f_00001-0-7 loss: 0.556881  [  256/  265]
train() client id: f_00001-1-0 loss: 0.567291  [   32/  265]
train() client id: f_00001-1-1 loss: 0.520974  [   64/  265]
train() client id: f_00001-1-2 loss: 0.449704  [   96/  265]
train() client id: f_00001-1-3 loss: 0.559844  [  128/  265]
train() client id: f_00001-1-4 loss: 0.437825  [  160/  265]
train() client id: f_00001-1-5 loss: 0.502929  [  192/  265]
train() client id: f_00001-1-6 loss: 0.565412  [  224/  265]
train() client id: f_00001-1-7 loss: 0.596802  [  256/  265]
train() client id: f_00001-2-0 loss: 0.466963  [   32/  265]
train() client id: f_00001-2-1 loss: 0.485780  [   64/  265]
train() client id: f_00001-2-2 loss: 0.499150  [   96/  265]
train() client id: f_00001-2-3 loss: 0.610838  [  128/  265]
train() client id: f_00001-2-4 loss: 0.473127  [  160/  265]
train() client id: f_00001-2-5 loss: 0.529805  [  192/  265]
train() client id: f_00001-2-6 loss: 0.475708  [  224/  265]
train() client id: f_00001-2-7 loss: 0.739488  [  256/  265]
train() client id: f_00001-3-0 loss: 0.585934  [   32/  265]
train() client id: f_00001-3-1 loss: 0.590421  [   64/  265]
train() client id: f_00001-3-2 loss: 0.430235  [   96/  265]
train() client id: f_00001-3-3 loss: 0.499781  [  128/  265]
train() client id: f_00001-3-4 loss: 0.575093  [  160/  265]
train() client id: f_00001-3-5 loss: 0.513448  [  192/  265]
train() client id: f_00001-3-6 loss: 0.480698  [  224/  265]
train() client id: f_00001-3-7 loss: 0.546271  [  256/  265]
train() client id: f_00002-0-0 loss: 0.823896  [   32/  124]
train() client id: f_00002-0-1 loss: 0.890490  [   64/  124]
train() client id: f_00002-0-2 loss: 0.873392  [   96/  124]
train() client id: f_00002-1-0 loss: 0.847651  [   32/  124]
train() client id: f_00002-1-1 loss: 0.841042  [   64/  124]
train() client id: f_00002-1-2 loss: 0.732324  [   96/  124]
train() client id: f_00002-2-0 loss: 0.758631  [   32/  124]
train() client id: f_00002-2-1 loss: 0.821415  [   64/  124]
train() client id: f_00002-2-2 loss: 0.855951  [   96/  124]
train() client id: f_00002-3-0 loss: 0.780477  [   32/  124]
train() client id: f_00002-3-1 loss: 0.800218  [   64/  124]
train() client id: f_00002-3-2 loss: 0.904692  [   96/  124]
train() client id: f_00003-0-0 loss: 0.610587  [   32/   43]
train() client id: f_00003-1-0 loss: 0.573371  [   32/   43]
train() client id: f_00003-2-0 loss: 0.409259  [   32/   43]
train() client id: f_00003-3-0 loss: 0.527248  [   32/   43]
train() client id: f_00004-0-0 loss: 0.798298  [   32/  306]
train() client id: f_00004-0-1 loss: 0.858197  [   64/  306]
train() client id: f_00004-0-2 loss: 0.990076  [   96/  306]
train() client id: f_00004-0-3 loss: 1.092921  [  128/  306]
train() client id: f_00004-0-4 loss: 0.940136  [  160/  306]
train() client id: f_00004-0-5 loss: 0.787584  [  192/  306]
train() client id: f_00004-0-6 loss: 1.001265  [  224/  306]
train() client id: f_00004-0-7 loss: 0.852267  [  256/  306]
train() client id: f_00004-0-8 loss: 0.871199  [  288/  306]
train() client id: f_00004-1-0 loss: 0.842317  [   32/  306]
train() client id: f_00004-1-1 loss: 0.819413  [   64/  306]
train() client id: f_00004-1-2 loss: 0.917906  [   96/  306]
train() client id: f_00004-1-3 loss: 0.821524  [  128/  306]
train() client id: f_00004-1-4 loss: 0.993782  [  160/  306]
train() client id: f_00004-1-5 loss: 0.946493  [  192/  306]
train() client id: f_00004-1-6 loss: 0.817291  [  224/  306]
train() client id: f_00004-1-7 loss: 1.010409  [  256/  306]
train() client id: f_00004-1-8 loss: 0.975634  [  288/  306]
train() client id: f_00004-2-0 loss: 1.031046  [   32/  306]
train() client id: f_00004-2-1 loss: 0.904066  [   64/  306]
train() client id: f_00004-2-2 loss: 1.116064  [   96/  306]
train() client id: f_00004-2-3 loss: 0.906597  [  128/  306]
train() client id: f_00004-2-4 loss: 0.826553  [  160/  306]
train() client id: f_00004-2-5 loss: 0.780369  [  192/  306]
train() client id: f_00004-2-6 loss: 0.878691  [  224/  306]
train() client id: f_00004-2-7 loss: 0.899701  [  256/  306]
train() client id: f_00004-2-8 loss: 0.848380  [  288/  306]
train() client id: f_00004-3-0 loss: 0.948172  [   32/  306]
train() client id: f_00004-3-1 loss: 1.131888  [   64/  306]
train() client id: f_00004-3-2 loss: 0.960590  [   96/  306]
train() client id: f_00004-3-3 loss: 0.995512  [  128/  306]
train() client id: f_00004-3-4 loss: 0.856244  [  160/  306]
train() client id: f_00004-3-5 loss: 0.862577  [  192/  306]
train() client id: f_00004-3-6 loss: 0.789142  [  224/  306]
train() client id: f_00004-3-7 loss: 0.829629  [  256/  306]
train() client id: f_00004-3-8 loss: 0.808396  [  288/  306]
train() client id: f_00005-0-0 loss: 0.405627  [   32/  146]
train() client id: f_00005-0-1 loss: 0.461003  [   64/  146]
train() client id: f_00005-0-2 loss: 0.493952  [   96/  146]
train() client id: f_00005-0-3 loss: 0.760404  [  128/  146]
train() client id: f_00005-1-0 loss: 0.611394  [   32/  146]
train() client id: f_00005-1-1 loss: 0.645533  [   64/  146]
train() client id: f_00005-1-2 loss: 0.397883  [   96/  146]
train() client id: f_00005-1-3 loss: 0.450159  [  128/  146]
train() client id: f_00005-2-0 loss: 0.343738  [   32/  146]
train() client id: f_00005-2-1 loss: 0.490155  [   64/  146]
train() client id: f_00005-2-2 loss: 0.730779  [   96/  146]
train() client id: f_00005-2-3 loss: 0.632568  [  128/  146]
train() client id: f_00005-3-0 loss: 0.451100  [   32/  146]
train() client id: f_00005-3-1 loss: 0.807336  [   64/  146]
train() client id: f_00005-3-2 loss: 0.452649  [   96/  146]
train() client id: f_00005-3-3 loss: 0.505374  [  128/  146]
train() client id: f_00006-0-0 loss: 0.520491  [   32/   54]
train() client id: f_00006-1-0 loss: 0.462452  [   32/   54]
train() client id: f_00006-2-0 loss: 0.528769  [   32/   54]
train() client id: f_00006-3-0 loss: 0.412879  [   32/   54]
train() client id: f_00007-0-0 loss: 0.474029  [   32/  179]
train() client id: f_00007-0-1 loss: 0.456213  [   64/  179]
train() client id: f_00007-0-2 loss: 0.775091  [   96/  179]
train() client id: f_00007-0-3 loss: 0.723439  [  128/  179]
train() client id: f_00007-0-4 loss: 0.533835  [  160/  179]
train() client id: f_00007-1-0 loss: 0.688111  [   32/  179]
train() client id: f_00007-1-1 loss: 0.834364  [   64/  179]
train() client id: f_00007-1-2 loss: 0.421661  [   96/  179]
train() client id: f_00007-1-3 loss: 0.568059  [  128/  179]
train() client id: f_00007-1-4 loss: 0.468702  [  160/  179]
train() client id: f_00007-2-0 loss: 0.495628  [   32/  179]
train() client id: f_00007-2-1 loss: 0.639845  [   64/  179]
train() client id: f_00007-2-2 loss: 0.567079  [   96/  179]
train() client id: f_00007-2-3 loss: 0.619879  [  128/  179]
train() client id: f_00007-2-4 loss: 0.488189  [  160/  179]
train() client id: f_00007-3-0 loss: 0.513222  [   32/  179]
train() client id: f_00007-3-1 loss: 0.467949  [   64/  179]
train() client id: f_00007-3-2 loss: 0.787911  [   96/  179]
train() client id: f_00007-3-3 loss: 0.708431  [  128/  179]
train() client id: f_00007-3-4 loss: 0.498913  [  160/  179]
train() client id: f_00008-0-0 loss: 0.573156  [   32/  130]
train() client id: f_00008-0-1 loss: 0.822037  [   64/  130]
train() client id: f_00008-0-2 loss: 0.620564  [   96/  130]
train() client id: f_00008-0-3 loss: 0.658630  [  128/  130]
train() client id: f_00008-1-0 loss: 0.590922  [   32/  130]
train() client id: f_00008-1-1 loss: 0.609071  [   64/  130]
train() client id: f_00008-1-2 loss: 0.719839  [   96/  130]
train() client id: f_00008-1-3 loss: 0.703361  [  128/  130]
train() client id: f_00008-2-0 loss: 0.659254  [   32/  130]
train() client id: f_00008-2-1 loss: 0.621328  [   64/  130]
train() client id: f_00008-2-2 loss: 0.570131  [   96/  130]
train() client id: f_00008-2-3 loss: 0.819740  [  128/  130]
train() client id: f_00008-3-0 loss: 0.744462  [   32/  130]
train() client id: f_00008-3-1 loss: 0.646357  [   64/  130]
train() client id: f_00008-3-2 loss: 0.541540  [   96/  130]
train() client id: f_00008-3-3 loss: 0.749391  [  128/  130]
train() client id: f_00009-0-0 loss: 0.825491  [   32/  118]
train() client id: f_00009-0-1 loss: 0.864598  [   64/  118]
train() client id: f_00009-0-2 loss: 0.875656  [   96/  118]
train() client id: f_00009-1-0 loss: 0.890724  [   32/  118]
train() client id: f_00009-1-1 loss: 0.619220  [   64/  118]
train() client id: f_00009-1-2 loss: 0.879296  [   96/  118]
train() client id: f_00009-2-0 loss: 0.879308  [   32/  118]
train() client id: f_00009-2-1 loss: 0.711560  [   64/  118]
train() client id: f_00009-2-2 loss: 0.784163  [   96/  118]
train() client id: f_00009-3-0 loss: 0.726740  [   32/  118]
train() client id: f_00009-3-1 loss: 0.824353  [   64/  118]
train() client id: f_00009-3-2 loss: 0.751264  [   96/  118]
At round 76 accuracy: 0.6445623342175066
At round 76 training accuracy: 0.5935613682092555
At round 76 training loss: 0.8206803130110439
update_location
xs = [  -3.9056584     4.20031788  400.00902392   18.81129433    0.97929623
    3.95640986 -362.44319194 -341.32485185  384.66397685 -327.06087855]
ys = [ 392.5879595   375.55583871    1.32061395 -362.45517586  354.35018685
  337.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [405.14264168 388.66416179 412.32143194 376.46728849 368.19154517
 352.32662917 375.99462486 355.67306722 397.83796332 342.03045206]
dists_bs = [276.79919408 268.95240911 600.66621906 571.33255103 251.26719289
 241.93768684 258.23552284 240.83565102 581.38479929 228.82963973]
uav_gains = [6.93541635e-13 7.99744288e-13 6.54792901e-13 8.98819590e-13
 9.79395095e-13 1.17540781e-12 9.03099454e-13 1.12860915e-12
 7.37325739e-13 1.34266847e-12]
bs_gains = [1.60410424e-11 1.73861274e-11 1.83282543e-12 2.10865230e-12
 2.10336130e-11 2.33842916e-11 1.94827084e-11 2.36851376e-11
 2.00814839e-12 2.73312636e-11]
Round 77
-------------------------------
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.7566319  1.47518382 0.74797579 0.2906086  1.70004424 0.81907155
 0.35099723 1.03253353 0.75085927 0.66440907]
obj_prev = 8.588314982517188
eta_min = nan	eta_max = nan
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 1.9404471408469977	eta = 0.9090909090909091
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 6.015300613417023	eta = 0.2932593013524188
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.5977300389560636	eta = 0.49032107362544536
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.2097433156161213	eta = 0.5495900082517465
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.1851865241926465	eta = 0.5538271752429276
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.1850702185308974	eta = 0.5538473987330524
eta = 0.5538473987330524
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [0.04875685 0.10254417 0.04798293 0.01663924 0.1184095  0.05649604
 0.02089579 0.06926569 0.05030469 0.04566118]
ene_total = [0.33763567 0.45238391 0.33971862 0.18710392 0.51488509 0.26328129
 0.20400615 0.38565213 0.28224513 0.2181583 ]
ti_comp = [6.11168561 6.36617567 6.09880235 6.16226391 6.37047552 6.37270362
 6.16308753 6.19819213 6.2639569  6.37579113]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [1.93938935e-07 1.66286030e-06 1.85631348e-07 7.58228597e-09
 2.55679530e-06 2.77515029e-07 1.50127409e-08 5.40635308e-07
 2.02771968e-07 1.46370394e-07]
ene_total = [0.14020895 0.03716612 0.14542564 0.1197279  0.03542864 0.0345172
 0.11939442 0.10518191 0.0785509  0.03326647]
optimize_network_iter = 0 obj = 0.848868136074443
eta = 0.5538473987330524
freqs = [3988822.08036756 8053828.5629115  3933799.78027688 1350091.76188237
 9293615.50887199 4432658.36750978 1695237.49554636 5587571.86903341
 4015408.34280281 3580824.62742287]
eta_min = 0.5538473987330529	eta_max = 0.9002731777032497
af = 1.770021082250513e-05	bf = 0.3787142685879881	zeta = 1.9470231904755644e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.12834926e-08 2.68229162e-07 2.99434299e-08 1.22306739e-09
 4.12426142e-07 4.47648089e-08 2.42164354e-09 8.72076598e-08
 3.27083129e-08 2.36104067e-08]
ene_total = [0.68809723 0.18237166 0.71369919 0.58758591 0.17382975 0.1693947
 0.58594921 0.51618999 0.38549919 0.16325868]
ti_comp = [1.09726106 1.35175113 1.08437781 1.14783937 1.35605097 1.35827907
 1.14866299 1.18376759 1.24953236 1.36136658]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.38201293e-08 2.07313558e-07 3.30055808e-08 1.22836293e-09
 3.17172712e-07 3.43371396e-08 2.42929128e-09 8.33124237e-08
 2.86430070e-08 1.80459360e-08]
ene_total = [0.6272565  0.16624547 0.65059478 0.53563224 0.1584582  0.15441683
 0.53414025 0.47054899 0.35141372 0.14882344]
optimize_network_iter = 1 obj = 3.7975304096863765
eta = 0.9002731777032497
freqs = [3950315.32823697 6744042.83153091 3933799.78027688 1288719.87693548
 7762767.85413468 3697728.61147668 1617231.97677077 5201849.38696053
 3579043.95899813 2981797.2351148 ]
eta_min = 0.8902239180004755	eta_max = 0.9002731777032479
af = 1.3057149041253039e-05	bf = 0.3787142685879881	zeta = 1.4362863945378344e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.06824073e-08 1.88079677e-07 2.99434299e-08 1.11439939e-09
 2.87746454e-07 3.11514510e-08 2.20390950e-09 7.55829668e-08
 2.59856015e-08 1.63716925e-08]
ene_total = [0.68809721 0.18237007 0.71369919 0.58758591 0.17382727 0.16939443
 0.58594921 0.51618975 0.38549905 0.16325854]
ti_comp = [1.09726106 1.35175113 1.08437781 1.14783937 1.35605097 1.35827907
 1.14866299 1.18376759 1.24953236 1.36136658]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.38201293e-08 2.07313558e-07 3.30055808e-08 1.22836293e-09
 3.17172712e-07 3.43371396e-08 2.42929128e-09 8.33124237e-08
 2.86430070e-08 1.80459360e-08]
ene_total = [0.6272565  0.16624547 0.65059478 0.53563224 0.1584582  0.15441683
 0.53414025 0.47054899 0.35141372 0.14882344]
optimize_network_iter = 2 obj = 3.7975304096863085
eta = 0.9002731777032479
freqs = [3950315.32823696 6744042.83153091 3933799.78027686 1288719.87693548
 7762767.85413468 3697728.61147668 1617231.97677076 5201849.38696052
 3579043.95899813 2981797.2351148 ]
Done!
At round 77 eta: 0.9002731777032479
At round 77 local rounds: 3.4401009464594003
At round 77 global rounds: 18.115213119307235
At round 77 a_n: 1.4640267926476263
gradient difference: 0.9135053753852844
train() client id: f_00000-0-0 loss: 1.233915  [   32/  126]
train() client id: f_00000-0-1 loss: 1.045865  [   64/  126]
train() client id: f_00000-0-2 loss: 1.065887  [   96/  126]
train() client id: f_00000-1-0 loss: 0.988864  [   32/  126]
train() client id: f_00000-1-1 loss: 0.878588  [   64/  126]
train() client id: f_00000-1-2 loss: 0.997379  [   96/  126]
train() client id: f_00000-2-0 loss: 0.882409  [   32/  126]
train() client id: f_00000-2-1 loss: 1.017025  [   64/  126]
train() client id: f_00000-2-2 loss: 1.074076  [   96/  126]
train() client id: f_00001-0-0 loss: 0.547401  [   32/  265]
train() client id: f_00001-0-1 loss: 0.483407  [   64/  265]
train() client id: f_00001-0-2 loss: 0.545063  [   96/  265]
train() client id: f_00001-0-3 loss: 0.552258  [  128/  265]
train() client id: f_00001-0-4 loss: 0.442843  [  160/  265]
train() client id: f_00001-0-5 loss: 0.581233  [  192/  265]
train() client id: f_00001-0-6 loss: 0.495026  [  224/  265]
train() client id: f_00001-0-7 loss: 0.396036  [  256/  265]
train() client id: f_00001-1-0 loss: 0.578939  [   32/  265]
train() client id: f_00001-1-1 loss: 0.517493  [   64/  265]
train() client id: f_00001-1-2 loss: 0.466718  [   96/  265]
train() client id: f_00001-1-3 loss: 0.474676  [  128/  265]
train() client id: f_00001-1-4 loss: 0.657249  [  160/  265]
train() client id: f_00001-1-5 loss: 0.429379  [  192/  265]
train() client id: f_00001-1-6 loss: 0.481559  [  224/  265]
train() client id: f_00001-1-7 loss: 0.410740  [  256/  265]
train() client id: f_00001-2-0 loss: 0.399656  [   32/  265]
train() client id: f_00001-2-1 loss: 0.582152  [   64/  265]
train() client id: f_00001-2-2 loss: 0.574008  [   96/  265]
train() client id: f_00001-2-3 loss: 0.539454  [  128/  265]
train() client id: f_00001-2-4 loss: 0.452435  [  160/  265]
train() client id: f_00001-2-5 loss: 0.500184  [  192/  265]
train() client id: f_00001-2-6 loss: 0.521324  [  224/  265]
train() client id: f_00001-2-7 loss: 0.412920  [  256/  265]
train() client id: f_00002-0-0 loss: 0.985608  [   32/  124]
train() client id: f_00002-0-1 loss: 1.003637  [   64/  124]
train() client id: f_00002-0-2 loss: 0.790771  [   96/  124]
train() client id: f_00002-1-0 loss: 0.866972  [   32/  124]
train() client id: f_00002-1-1 loss: 0.946742  [   64/  124]
train() client id: f_00002-1-2 loss: 0.741927  [   96/  124]
train() client id: f_00002-2-0 loss: 0.952568  [   32/  124]
train() client id: f_00002-2-1 loss: 0.744486  [   64/  124]
train() client id: f_00002-2-2 loss: 0.891065  [   96/  124]
train() client id: f_00003-0-0 loss: 0.509110  [   32/   43]
train() client id: f_00003-1-0 loss: 0.519428  [   32/   43]
train() client id: f_00003-2-0 loss: 0.755224  [   32/   43]
train() client id: f_00004-0-0 loss: 0.749447  [   32/  306]
train() client id: f_00004-0-1 loss: 0.920395  [   64/  306]
train() client id: f_00004-0-2 loss: 0.917217  [   96/  306]
train() client id: f_00004-0-3 loss: 0.889460  [  128/  306]
train() client id: f_00004-0-4 loss: 0.942521  [  160/  306]
train() client id: f_00004-0-5 loss: 0.807207  [  192/  306]
train() client id: f_00004-0-6 loss: 1.005784  [  224/  306]
train() client id: f_00004-0-7 loss: 0.838069  [  256/  306]
train() client id: f_00004-0-8 loss: 0.928412  [  288/  306]
train() client id: f_00004-1-0 loss: 0.952978  [   32/  306]
train() client id: f_00004-1-1 loss: 1.026229  [   64/  306]
train() client id: f_00004-1-2 loss: 0.852846  [   96/  306]
train() client id: f_00004-1-3 loss: 0.949946  [  128/  306]
train() client id: f_00004-1-4 loss: 0.804430  [  160/  306]
train() client id: f_00004-1-5 loss: 0.921041  [  192/  306]
train() client id: f_00004-1-6 loss: 0.900413  [  224/  306]
train() client id: f_00004-1-7 loss: 0.770920  [  256/  306]
train() client id: f_00004-1-8 loss: 0.872296  [  288/  306]
train() client id: f_00004-2-0 loss: 0.811369  [   32/  306]
train() client id: f_00004-2-1 loss: 0.866586  [   64/  306]
train() client id: f_00004-2-2 loss: 0.896689  [   96/  306]
train() client id: f_00004-2-3 loss: 0.981088  [  128/  306]
train() client id: f_00004-2-4 loss: 0.870608  [  160/  306]
train() client id: f_00004-2-5 loss: 0.866778  [  192/  306]
train() client id: f_00004-2-6 loss: 0.961505  [  224/  306]
train() client id: f_00004-2-7 loss: 0.816708  [  256/  306]
train() client id: f_00004-2-8 loss: 0.937162  [  288/  306]
train() client id: f_00005-0-0 loss: 0.922179  [   32/  146]
train() client id: f_00005-0-1 loss: 0.620040  [   64/  146]
train() client id: f_00005-0-2 loss: 0.593304  [   96/  146]
train() client id: f_00005-0-3 loss: 0.831667  [  128/  146]
train() client id: f_00005-1-0 loss: 0.633428  [   32/  146]
train() client id: f_00005-1-1 loss: 0.731154  [   64/  146]
train() client id: f_00005-1-2 loss: 0.866620  [   96/  146]
train() client id: f_00005-1-3 loss: 0.671365  [  128/  146]
train() client id: f_00005-2-0 loss: 0.885463  [   32/  146]
train() client id: f_00005-2-1 loss: 0.769704  [   64/  146]
train() client id: f_00005-2-2 loss: 0.515728  [   96/  146]
train() client id: f_00005-2-3 loss: 0.750924  [  128/  146]
train() client id: f_00006-0-0 loss: 0.556393  [   32/   54]
train() client id: f_00006-1-0 loss: 0.516174  [   32/   54]
train() client id: f_00006-2-0 loss: 0.459020  [   32/   54]
train() client id: f_00007-0-0 loss: 0.707257  [   32/  179]
train() client id: f_00007-0-1 loss: 0.890066  [   64/  179]
train() client id: f_00007-0-2 loss: 0.915507  [   96/  179]
train() client id: f_00007-0-3 loss: 0.741339  [  128/  179]
train() client id: f_00007-0-4 loss: 0.691237  [  160/  179]
train() client id: f_00007-1-0 loss: 0.823725  [   32/  179]
train() client id: f_00007-1-1 loss: 0.857654  [   64/  179]
train() client id: f_00007-1-2 loss: 0.689302  [   96/  179]
train() client id: f_00007-1-3 loss: 0.737530  [  128/  179]
train() client id: f_00007-1-4 loss: 0.813405  [  160/  179]
train() client id: f_00007-2-0 loss: 0.859060  [   32/  179]
train() client id: f_00007-2-1 loss: 0.682577  [   64/  179]
train() client id: f_00007-2-2 loss: 0.763846  [   96/  179]
train() client id: f_00007-2-3 loss: 0.990934  [  128/  179]
train() client id: f_00007-2-4 loss: 0.739589  [  160/  179]
train() client id: f_00008-0-0 loss: 0.483571  [   32/  130]
train() client id: f_00008-0-1 loss: 0.710720  [   64/  130]
train() client id: f_00008-0-2 loss: 0.751807  [   96/  130]
train() client id: f_00008-0-3 loss: 0.576895  [  128/  130]
train() client id: f_00008-1-0 loss: 0.593601  [   32/  130]
train() client id: f_00008-1-1 loss: 0.569599  [   64/  130]
train() client id: f_00008-1-2 loss: 0.625061  [   96/  130]
train() client id: f_00008-1-3 loss: 0.744969  [  128/  130]
train() client id: f_00008-2-0 loss: 0.608695  [   32/  130]
train() client id: f_00008-2-1 loss: 0.608567  [   64/  130]
train() client id: f_00008-2-2 loss: 0.694213  [   96/  130]
train() client id: f_00008-2-3 loss: 0.622445  [  128/  130]
train() client id: f_00009-0-0 loss: 0.844411  [   32/  118]
train() client id: f_00009-0-1 loss: 0.982436  [   64/  118]
train() client id: f_00009-0-2 loss: 0.770391  [   96/  118]
train() client id: f_00009-1-0 loss: 0.697256  [   32/  118]
train() client id: f_00009-1-1 loss: 0.795515  [   64/  118]
train() client id: f_00009-1-2 loss: 0.959459  [   96/  118]
train() client id: f_00009-2-0 loss: 0.914257  [   32/  118]
train() client id: f_00009-2-1 loss: 0.693105  [   64/  118]
train() client id: f_00009-2-2 loss: 0.870266  [   96/  118]
At round 77 accuracy: 0.6445623342175066
At round 77 training accuracy: 0.5888665325285044
At round 77 training loss: 0.8312596055937412
update_location
xs = [  -3.9056584     4.20031788  405.00902392   18.81129433    0.97929623
    3.95640986 -367.44319194 -346.32485185  389.66397685 -332.06087855]
ys = [ 397.5879595   380.55583871    1.32061395 -367.45517586  359.35018685
  342.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [409.98956048 393.49763537 417.17388877 381.28358352 373.00605331
 357.12350123 380.8167405  360.47410346 402.67441541 346.81470401]
dists_bs = [280.74663567 272.7012225  605.45437232 576.0385713  254.86016576
 245.33036058 261.88741317 244.31549137 586.1995604  232.18228357]
uav_gains = [6.66953995e-13 7.65732155e-13 6.30692965e-13 8.57197052e-13
 9.31026172e-13 1.10932372e-12 8.61079374e-13 1.06691429e-12
 7.07809580e-13 1.26020837e-12]
bs_gains = [1.54174772e-11 1.67251587e-11 1.79252867e-12 2.06077086e-12
 2.02138293e-11 2.24900517e-11 1.87315249e-11 2.27526125e-11
 1.96230594e-12 2.62405338e-11]
Round 78
-------------------------------
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.61445089 1.1956097  0.60744834 0.2367497  1.37782529 0.66388603
 0.28568902 0.83797174 0.60879864 0.53854598]
obj_prev = 6.96697533321086
eta_min = nan	eta_max = nan
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 1.572517230482826	eta = 0.909090909090909
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 4.952035710772682	eta = 0.2886815043580717
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.9382966582345253	eta = 0.48652715668257346
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.6172340285407683	eta = 0.5462106571408896
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.596934549245399	eta = 0.5504802264023727
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.5968383965379944	eta = 0.5505006089430082
eta = 0.5505006089430082
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [0.049257   0.10359608 0.04847515 0.01680993 0.11962415 0.05707558
 0.02111014 0.06997622 0.05082072 0.04612957]
ene_total = [0.27603738 0.36760475 0.27771398 0.15379303 0.4183885  0.21392216
 0.16753295 0.31519772 0.22938986 0.17725807]
ti_comp = [7.65842292 7.92067487 7.64545672 7.70927839 7.9250345  7.92732068
 7.71009494 7.74538131 7.81728173 7.93043026]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.27351857e-07 1.10760673e-06 1.21795299e-07 4.99516971e-09
 1.70346924e-06 1.84917516e-07 9.89086254e-09 3.56981594e-07
 1.34242758e-07 9.75494579e-08]
ene_total = [0.11560694 0.03019425 0.11983004 0.09904284 0.02877625 0.02802669
 0.0987769  0.0872852  0.06386639 0.02701362]
optimize_network_iter = 0 obj = 0.6984191265921221
eta = 0.5505006089430082
freqs = [3215871.18363491 6539599.20133967 3170192.94839821 1090240.14955988
 7547232.16704463 3599928.48687781 1368993.7919406  4517286.87955715
 3250536.39445521 2908390.39625094]
eta_min = 0.5505006089430086	eta_max = 0.917509374892285
af = 9.439588067529733e-06	bf = 0.3139333373586834	zeta = 1.0383546874282708e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [2.03340174e-08 1.76849361e-07 1.94468128e-08 7.97568798e-10
 2.71989541e-07 2.95254116e-08 1.57925432e-09 5.69985401e-08
 2.14342739e-08 1.55755277e-08]
ene_total = [0.57161603 0.14928029 0.59249717 0.48971657 0.14226095 0.1385753
 0.48840159 0.43157626 0.31578514 0.13356732]
ti_comp = [1.26571924 1.52797119 1.25275304 1.31657471 1.53233081 1.534617
 1.31739126 1.35267763 1.42457805 1.53772658]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
optimize_network_iter = 1 obj = 3.4532738111991463
eta = 0.909090909090909
freqs = [3106837.34109527 5412726.23484139 3089168.32072196 1019314.72951421
 6232384.57566306 2969191.46464435 1279276.24484534 4129943.16102444
 2848016.20722417 2394904.72079761]
eta_min = 0.9009430532547689	eta_max = 0.9090909090909057
af = 6.772388892494084e-06	bf = 0.3139333373586834	zeta = 7.449627781743493e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
ti_comp = [1.26571924 1.52797119 1.25275304 1.31657471 1.53233081 1.534617
 1.31739126 1.35267763 1.42457805 1.53772658]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
optimize_network_iter = 2 obj = 3.4532738111990233
eta = 0.9090909090909057
freqs = [3106837.34109525 5412726.23484139 3089168.32072194 1019314.7295142
 6232384.57566307 2969191.46464435 1279276.24484533 4129943.16102442
 2848016.20722416 2394904.72079761]
Done!
At round 78 eta: 0.9090909090909057
At round 78 local rounds: 3.120939520577876
At round 78 global rounds: 16.104294719123292
At round 78 a_n: 1.1214809456783108
gradient difference: 0.9536986351013184
train() client id: f_00000-0-0 loss: 0.915736  [   32/  126]
train() client id: f_00000-0-1 loss: 1.023972  [   64/  126]
train() client id: f_00000-0-2 loss: 0.758308  [   96/  126]
train() client id: f_00000-1-0 loss: 0.933938  [   32/  126]
train() client id: f_00000-1-1 loss: 0.878299  [   64/  126]
train() client id: f_00000-1-2 loss: 0.813918  [   96/  126]
train() client id: f_00000-2-0 loss: 0.914172  [   32/  126]
train() client id: f_00000-2-1 loss: 0.745101  [   64/  126]
train() client id: f_00000-2-2 loss: 0.808431  [   96/  126]
train() client id: f_00001-0-0 loss: 0.508398  [   32/  265]
train() client id: f_00001-0-1 loss: 0.623801  [   64/  265]
train() client id: f_00001-0-2 loss: 0.701384  [   96/  265]
train() client id: f_00001-0-3 loss: 0.601318  [  128/  265]
train() client id: f_00001-0-4 loss: 0.563049  [  160/  265]
train() client id: f_00001-0-5 loss: 0.634443  [  192/  265]
train() client id: f_00001-0-6 loss: 0.558864  [  224/  265]
train() client id: f_00001-0-7 loss: 0.705531  [  256/  265]
train() client id: f_00001-1-0 loss: 0.534529  [   32/  265]
train() client id: f_00001-1-1 loss: 0.619734  [   64/  265]
train() client id: f_00001-1-2 loss: 0.676740  [   96/  265]
train() client id: f_00001-1-3 loss: 0.667942  [  128/  265]
train() client id: f_00001-1-4 loss: 0.675742  [  160/  265]
train() client id: f_00001-1-5 loss: 0.562476  [  192/  265]
train() client id: f_00001-1-6 loss: 0.562641  [  224/  265]
train() client id: f_00001-1-7 loss: 0.546417  [  256/  265]
train() client id: f_00001-2-0 loss: 0.673198  [   32/  265]
train() client id: f_00001-2-1 loss: 0.607112  [   64/  265]
train() client id: f_00001-2-2 loss: 0.547605  [   96/  265]
train() client id: f_00001-2-3 loss: 0.644008  [  128/  265]
train() client id: f_00001-2-4 loss: 0.647920  [  160/  265]
train() client id: f_00001-2-5 loss: 0.600621  [  192/  265]
train() client id: f_00001-2-6 loss: 0.540145  [  224/  265]
train() client id: f_00001-2-7 loss: 0.658676  [  256/  265]
train() client id: f_00002-0-0 loss: 0.971527  [   32/  124]
train() client id: f_00002-0-1 loss: 0.868847  [   64/  124]
train() client id: f_00002-0-2 loss: 1.134432  [   96/  124]
train() client id: f_00002-1-0 loss: 0.954268  [   32/  124]
train() client id: f_00002-1-1 loss: 0.997770  [   64/  124]
train() client id: f_00002-1-2 loss: 0.834036  [   96/  124]
train() client id: f_00002-2-0 loss: 0.746637  [   32/  124]
train() client id: f_00002-2-1 loss: 0.901473  [   64/  124]
train() client id: f_00002-2-2 loss: 1.281322  [   96/  124]
train() client id: f_00003-0-0 loss: 0.131933  [   32/   43]
train() client id: f_00003-1-0 loss: 0.371415  [   32/   43]
train() client id: f_00003-2-0 loss: 0.248071  [   32/   43]
train() client id: f_00004-0-0 loss: 0.851162  [   32/  306]
train() client id: f_00004-0-1 loss: 0.767751  [   64/  306]
train() client id: f_00004-0-2 loss: 0.659636  [   96/  306]
train() client id: f_00004-0-3 loss: 0.694177  [  128/  306]
train() client id: f_00004-0-4 loss: 0.662424  [  160/  306]
train() client id: f_00004-0-5 loss: 0.751731  [  192/  306]
train() client id: f_00004-0-6 loss: 0.853082  [  224/  306]
train() client id: f_00004-0-7 loss: 0.799207  [  256/  306]
train() client id: f_00004-0-8 loss: 0.800419  [  288/  306]
train() client id: f_00004-1-0 loss: 0.677308  [   32/  306]
train() client id: f_00004-1-1 loss: 0.881128  [   64/  306]
train() client id: f_00004-1-2 loss: 0.755785  [   96/  306]
train() client id: f_00004-1-3 loss: 0.851633  [  128/  306]
train() client id: f_00004-1-4 loss: 0.781458  [  160/  306]
train() client id: f_00004-1-5 loss: 0.688939  [  192/  306]
train() client id: f_00004-1-6 loss: 0.756580  [  224/  306]
train() client id: f_00004-1-7 loss: 0.842099  [  256/  306]
train() client id: f_00004-1-8 loss: 0.717697  [  288/  306]
train() client id: f_00004-2-0 loss: 0.863365  [   32/  306]
train() client id: f_00004-2-1 loss: 0.668984  [   64/  306]
train() client id: f_00004-2-2 loss: 0.761011  [   96/  306]
train() client id: f_00004-2-3 loss: 0.728897  [  128/  306]
train() client id: f_00004-2-4 loss: 0.815395  [  160/  306]
train() client id: f_00004-2-5 loss: 0.767265  [  192/  306]
train() client id: f_00004-2-6 loss: 0.752866  [  224/  306]
train() client id: f_00004-2-7 loss: 0.818294  [  256/  306]
train() client id: f_00004-2-8 loss: 0.802623  [  288/  306]
train() client id: f_00005-0-0 loss: 0.842735  [   32/  146]
train() client id: f_00005-0-1 loss: 0.615716  [   64/  146]
train() client id: f_00005-0-2 loss: 0.671145  [   96/  146]
train() client id: f_00005-0-3 loss: 0.787619  [  128/  146]
train() client id: f_00005-1-0 loss: 0.637231  [   32/  146]
train() client id: f_00005-1-1 loss: 0.531449  [   64/  146]
train() client id: f_00005-1-2 loss: 0.911459  [   96/  146]
train() client id: f_00005-1-3 loss: 0.749190  [  128/  146]
train() client id: f_00005-2-0 loss: 0.778982  [   32/  146]
train() client id: f_00005-2-1 loss: 0.804716  [   64/  146]
train() client id: f_00005-2-2 loss: 0.537730  [   96/  146]
train() client id: f_00005-2-3 loss: 0.734639  [  128/  146]
train() client id: f_00006-0-0 loss: 0.603909  [   32/   54]
train() client id: f_00006-1-0 loss: 0.583848  [   32/   54]
train() client id: f_00006-2-0 loss: 0.601308  [   32/   54]
train() client id: f_00007-0-0 loss: 0.637164  [   32/  179]
train() client id: f_00007-0-1 loss: 0.819184  [   64/  179]
train() client id: f_00007-0-2 loss: 0.529041  [   96/  179]
train() client id: f_00007-0-3 loss: 0.583757  [  128/  179]
train() client id: f_00007-0-4 loss: 0.805414  [  160/  179]
train() client id: f_00007-1-0 loss: 0.838772  [   32/  179]
train() client id: f_00007-1-1 loss: 0.818635  [   64/  179]
train() client id: f_00007-1-2 loss: 0.626789  [   96/  179]
train() client id: f_00007-1-3 loss: 0.500943  [  128/  179]
train() client id: f_00007-1-4 loss: 0.753297  [  160/  179]
train() client id: f_00007-2-0 loss: 0.888286  [   32/  179]
train() client id: f_00007-2-1 loss: 0.901563  [   64/  179]
train() client id: f_00007-2-2 loss: 0.681351  [   96/  179]
train() client id: f_00007-2-3 loss: 0.546544  [  128/  179]
train() client id: f_00007-2-4 loss: 0.667701  [  160/  179]
train() client id: f_00008-0-0 loss: 0.751117  [   32/  130]
train() client id: f_00008-0-1 loss: 0.713078  [   64/  130]
train() client id: f_00008-0-2 loss: 0.700112  [   96/  130]
train() client id: f_00008-0-3 loss: 0.730614  [  128/  130]
train() client id: f_00008-1-0 loss: 0.700530  [   32/  130]
train() client id: f_00008-1-1 loss: 0.665753  [   64/  130]
train() client id: f_00008-1-2 loss: 0.814987  [   96/  130]
train() client id: f_00008-1-3 loss: 0.715399  [  128/  130]
train() client id: f_00008-2-0 loss: 0.575717  [   32/  130]
train() client id: f_00008-2-1 loss: 0.764509  [   64/  130]
train() client id: f_00008-2-2 loss: 0.840178  [   96/  130]
train() client id: f_00008-2-3 loss: 0.683526  [  128/  130]
train() client id: f_00009-0-0 loss: 0.881283  [   32/  118]
train() client id: f_00009-0-1 loss: 0.794002  [   64/  118]
train() client id: f_00009-0-2 loss: 0.605899  [   96/  118]
train() client id: f_00009-1-0 loss: 0.814194  [   32/  118]
train() client id: f_00009-1-1 loss: 0.810412  [   64/  118]
train() client id: f_00009-1-2 loss: 0.685259  [   96/  118]
train() client id: f_00009-2-0 loss: 0.673553  [   32/  118]
train() client id: f_00009-2-1 loss: 0.705988  [   64/  118]
train() client id: f_00009-2-2 loss: 0.823245  [   96/  118]
At round 78 accuracy: 0.6445623342175066
At round 78 training accuracy: 0.5888665325285044
At round 78 training loss: 0.8187033468966094
update_location
xs = [  -3.9056584     4.20031788  410.00902392   18.81129433    0.97929623
    3.95640986 -372.44319194 -351.32485185  394.66397685 -337.06087855]
ys = [ 402.5879595   385.55583871    1.32061395 -372.45517586  364.35018685
  347.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [414.84011293 398.33521992 422.02979008 386.10454908 377.82537987
 361.92587174 385.64338677 365.28047824 407.51481519 351.60495973]
dists_bs = [284.72715542 276.48962935 610.24592354 580.74950485 258.49991481
 248.77726449 265.58322443 247.84734786 591.0173977  235.59333944]
uav_gains = [6.42087105e-13 7.34190954e-13 6.08078996e-13 8.18868515e-13
 8.86722082e-13 1.04941536e-12 8.22401288e-13 1.01086288e-12
 6.80300468e-13 1.18594342e-12]
bs_gains = [1.48215354e-11 1.60913809e-11 1.75339758e-12 2.01430537e-12
 1.94269648e-11 2.16283883e-11 1.80107706e-11 2.18563739e-11
 1.91784428e-12 2.51905507e-11]
Round 79
-------------------------------
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.47167495 0.91597325 0.46632083 0.18231339 1.05554823 0.50864652
 0.2198029  0.64284551 0.46659513 0.41263047]
obj_prev = 5.342351171013073
eta_min = 2.6352711032412936e-201	eta_max = 0.9959613407486563
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 1.2045873201186579	eta = 0.909090909090909
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 3.853002462740199	eta = 0.28421455540603213
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.268192123899988	eta = 0.48279833546161194
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0171814832442236	eta = 0.5428759836545998
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0013314548574064	eta = 0.5471754212767701
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0012564070127876	eta = 0.5471959405544855
eta = 0.5471959405544855
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [0.04975385 0.10464104 0.04896411 0.01697949 0.12083078 0.05765129
 0.02132308 0.07068206 0.05133334 0.04659488]
ene_total = [0.21330398 0.28236012 0.21458127 0.11946231 0.32136361 0.16430363
 0.13001973 0.24348682 0.17623076 0.13614417]
ti_comp = [10.15400802 10.42406192 10.14095348 10.20515528 10.42848067 10.43082397
 10.20596501 10.24142523 10.31948805 10.43395522]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [7.46595073e-08 6.59040563e-07 7.13437549e-08 2.93774855e-09
 1.01384295e-06 1.10070217e-07 5.81730210e-09 2.10419888e-07
 7.93893490e-08 5.80759835e-08]
ene_total = [0.09007648 0.02319242 0.09330975 0.07740842 0.02209888 0.02151627
 0.07720788 0.06842579 0.04909128 0.02074061]
optimize_network_iter = 0 obj = 0.5430677938747402
eta = 0.5471959405544855
freqs = [2449961.18756652 5019206.37355413 2414176.71155869  831907.42340227
 5793307.05286581 2763505.9294842  1044638.0630291  3450792.08881117
 2487203.73595765 2232848.20767479]
eta_min = 0.5471959405544858	eta_max = 0.9355133020148048
af = 4.251199729612188e-06	bf = 0.24590073836681936	zeta = 4.6763197025734075e-06	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [1.18016929e-08 1.04176878e-07 1.12775602e-08 4.64380326e-10
 1.60261749e-07 1.73991893e-08 9.19561561e-10 3.32618174e-08
 1.25493558e-08 9.18027656e-09]
ene_total = [0.44865618 0.11551099 0.46476064 0.3855593  0.11006058 0.10716806
 0.3845604  0.34081604 0.24451523 0.10330516]
ti_comp = [1.74794103 2.01799493 1.73488648 1.79908829 2.02241368 2.02475698
 1.79989802 1.83535824 1.91342105 2.02788823]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
optimize_network_iter = 1 obj = 2.7049111966856447
eta = 0.909090909090909
freqs = [2249723.37868762 4098369.93259797 2230673.32764145  745935.59638761
 4722117.45608519 2250428.94068191  936334.90519755 3043809.99590222
 2120401.76543593 1816031.36876833]
eta_min = 0.9090909090910358	eta_max = 0.909090909090905
af = 2.9326740829202082e-06	bf = 0.24590073836681936	zeta = 3.2259414912122293e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
ti_comp = [1.74794103 2.01799493 1.73488648 1.79908829 2.02241368 2.02475698
 1.79989802 1.83535824 1.91342105 2.02788823]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
optimize_network_iter = 2 obj = 2.7049111966855257
eta = 0.909090909090905
freqs = [2249723.37868761 4098369.93259797 2230673.32764143  745935.59638761
 4722117.4560852  2250428.94068191  936334.90519755 3043809.99590221
 2120401.76543593 1816031.36876833]
Done!
At round 79 eta: 0.909090909090905
At round 79 local rounds: 3.120939520577902
At round 79 global rounds: 12.336290402460858
At round 79 a_n: 0.7789350987089918
gradient difference: 0.9557412266731262
train() client id: f_00000-0-0 loss: 0.957283  [   32/  126]
train() client id: f_00000-0-1 loss: 0.846113  [   64/  126]
train() client id: f_00000-0-2 loss: 1.009328  [   96/  126]
train() client id: f_00000-1-0 loss: 0.897684  [   32/  126]
train() client id: f_00000-1-1 loss: 0.940062  [   64/  126]
train() client id: f_00000-1-2 loss: 1.125959  [   96/  126]
train() client id: f_00000-2-0 loss: 1.018716  [   32/  126]
train() client id: f_00000-2-1 loss: 0.937649  [   64/  126]
train() client id: f_00000-2-2 loss: 1.095641  [   96/  126]
train() client id: f_00001-0-0 loss: 0.412389  [   32/  265]
train() client id: f_00001-0-1 loss: 0.538897  [   64/  265]
train() client id: f_00001-0-2 loss: 0.513923  [   96/  265]
train() client id: f_00001-0-3 loss: 0.466467  [  128/  265]
train() client id: f_00001-0-4 loss: 0.595699  [  160/  265]
train() client id: f_00001-0-5 loss: 0.491848  [  192/  265]
train() client id: f_00001-0-6 loss: 0.577843  [  224/  265]
train() client id: f_00001-0-7 loss: 0.391862  [  256/  265]
train() client id: f_00001-1-0 loss: 0.484748  [   32/  265]
train() client id: f_00001-1-1 loss: 0.505911  [   64/  265]
train() client id: f_00001-1-2 loss: 0.489958  [   96/  265]
train() client id: f_00001-1-3 loss: 0.428379  [  128/  265]
train() client id: f_00001-1-4 loss: 0.534203  [  160/  265]
train() client id: f_00001-1-5 loss: 0.525764  [  192/  265]
train() client id: f_00001-1-6 loss: 0.508596  [  224/  265]
train() client id: f_00001-1-7 loss: 0.455869  [  256/  265]
train() client id: f_00001-2-0 loss: 0.484525  [   32/  265]
train() client id: f_00001-2-1 loss: 0.584610  [   64/  265]
train() client id: f_00001-2-2 loss: 0.527996  [   96/  265]
train() client id: f_00001-2-3 loss: 0.503767  [  128/  265]
train() client id: f_00001-2-4 loss: 0.459038  [  160/  265]
train() client id: f_00001-2-5 loss: 0.394938  [  192/  265]
train() client id: f_00001-2-6 loss: 0.516082  [  224/  265]
train() client id: f_00001-2-7 loss: 0.412054  [  256/  265]
train() client id: f_00002-0-0 loss: 0.643175  [   32/  124]
train() client id: f_00002-0-1 loss: 0.340269  [   64/  124]
train() client id: f_00002-0-2 loss: 0.565624  [   96/  124]
train() client id: f_00002-1-0 loss: 0.758644  [   32/  124]
train() client id: f_00002-1-1 loss: 0.400467  [   64/  124]
train() client id: f_00002-1-2 loss: 0.490031  [   96/  124]
train() client id: f_00002-2-0 loss: 0.522419  [   32/  124]
train() client id: f_00002-2-1 loss: 0.339511  [   64/  124]
train() client id: f_00002-2-2 loss: 0.469823  [   96/  124]
train() client id: f_00003-0-0 loss: 0.542832  [   32/   43]
train() client id: f_00003-1-0 loss: 0.837134  [   32/   43]
train() client id: f_00003-2-0 loss: 0.761240  [   32/   43]
train() client id: f_00004-0-0 loss: 0.791025  [   32/  306]
train() client id: f_00004-0-1 loss: 0.725790  [   64/  306]
train() client id: f_00004-0-2 loss: 0.865448  [   96/  306]
train() client id: f_00004-0-3 loss: 0.821406  [  128/  306]
train() client id: f_00004-0-4 loss: 0.895167  [  160/  306]
train() client id: f_00004-0-5 loss: 0.753855  [  192/  306]
train() client id: f_00004-0-6 loss: 0.871457  [  224/  306]
train() client id: f_00004-0-7 loss: 0.854921  [  256/  306]
train() client id: f_00004-0-8 loss: 0.710096  [  288/  306]
train() client id: f_00004-1-0 loss: 0.792809  [   32/  306]
train() client id: f_00004-1-1 loss: 0.933788  [   64/  306]
train() client id: f_00004-1-2 loss: 0.798263  [   96/  306]
train() client id: f_00004-1-3 loss: 0.793145  [  128/  306]
train() client id: f_00004-1-4 loss: 0.877770  [  160/  306]
train() client id: f_00004-1-5 loss: 0.724087  [  192/  306]
train() client id: f_00004-1-6 loss: 0.998870  [  224/  306]
train() client id: f_00004-1-7 loss: 0.700627  [  256/  306]
train() client id: f_00004-1-8 loss: 0.707634  [  288/  306]
train() client id: f_00004-2-0 loss: 0.800873  [   32/  306]
train() client id: f_00004-2-1 loss: 0.786453  [   64/  306]
train() client id: f_00004-2-2 loss: 0.736100  [   96/  306]
train() client id: f_00004-2-3 loss: 0.804365  [  128/  306]
train() client id: f_00004-2-4 loss: 0.607506  [  160/  306]
train() client id: f_00004-2-5 loss: 0.826874  [  192/  306]
train() client id: f_00004-2-6 loss: 0.926552  [  224/  306]
train() client id: f_00004-2-7 loss: 0.884058  [  256/  306]
train() client id: f_00004-2-8 loss: 0.927395  [  288/  306]
train() client id: f_00005-0-0 loss: 0.553694  [   32/  146]
train() client id: f_00005-0-1 loss: 0.748412  [   64/  146]
train() client id: f_00005-0-2 loss: 0.443161  [   96/  146]
train() client id: f_00005-0-3 loss: 0.679271  [  128/  146]
train() client id: f_00005-1-0 loss: 0.703943  [   32/  146]
train() client id: f_00005-1-1 loss: 0.615923  [   64/  146]
train() client id: f_00005-1-2 loss: 0.773640  [   96/  146]
train() client id: f_00005-1-3 loss: 0.636303  [  128/  146]
train() client id: f_00005-2-0 loss: 0.307810  [   32/  146]
train() client id: f_00005-2-1 loss: 0.874724  [   64/  146]
train() client id: f_00005-2-2 loss: 0.831607  [   96/  146]
train() client id: f_00005-2-3 loss: 0.760714  [  128/  146]
train() client id: f_00006-0-0 loss: 0.479804  [   32/   54]
train() client id: f_00006-1-0 loss: 0.415006  [   32/   54]
train() client id: f_00006-2-0 loss: 0.504092  [   32/   54]
train() client id: f_00007-0-0 loss: 0.741543  [   32/  179]
train() client id: f_00007-0-1 loss: 1.009167  [   64/  179]
train() client id: f_00007-0-2 loss: 0.718683  [   96/  179]
train() client id: f_00007-0-3 loss: 0.579092  [  128/  179]
train() client id: f_00007-0-4 loss: 0.584864  [  160/  179]
train() client id: f_00007-1-0 loss: 0.983192  [   32/  179]
train() client id: f_00007-1-1 loss: 0.680810  [   64/  179]
train() client id: f_00007-1-2 loss: 0.652943  [   96/  179]
train() client id: f_00007-1-3 loss: 0.522050  [  128/  179]
train() client id: f_00007-1-4 loss: 0.727988  [  160/  179]
train() client id: f_00007-2-0 loss: 0.539323  [   32/  179]
train() client id: f_00007-2-1 loss: 0.720721  [   64/  179]
train() client id: f_00007-2-2 loss: 0.754576  [   96/  179]
train() client id: f_00007-2-3 loss: 0.901421  [  128/  179]
train() client id: f_00007-2-4 loss: 0.683340  [  160/  179]
train() client id: f_00008-0-0 loss: 0.835027  [   32/  130]
train() client id: f_00008-0-1 loss: 0.786585  [   64/  130]
train() client id: f_00008-0-2 loss: 0.816018  [   96/  130]
train() client id: f_00008-0-3 loss: 0.694081  [  128/  130]
train() client id: f_00008-1-0 loss: 0.763035  [   32/  130]
train() client id: f_00008-1-1 loss: 0.784326  [   64/  130]
train() client id: f_00008-1-2 loss: 0.875575  [   96/  130]
train() client id: f_00008-1-3 loss: 0.768314  [  128/  130]
train() client id: f_00008-2-0 loss: 0.881420  [   32/  130]
train() client id: f_00008-2-1 loss: 0.752690  [   64/  130]
train() client id: f_00008-2-2 loss: 0.846546  [   96/  130]
train() client id: f_00008-2-3 loss: 0.717710  [  128/  130]
train() client id: f_00009-0-0 loss: 0.679914  [   32/  118]
train() client id: f_00009-0-1 loss: 0.746178  [   64/  118]
train() client id: f_00009-0-2 loss: 0.574262  [   96/  118]
train() client id: f_00009-1-0 loss: 0.668268  [   32/  118]
train() client id: f_00009-1-1 loss: 0.834223  [   64/  118]
train() client id: f_00009-1-2 loss: 0.640845  [   96/  118]
train() client id: f_00009-2-0 loss: 0.715806  [   32/  118]
train() client id: f_00009-2-1 loss: 0.458241  [   64/  118]
train() client id: f_00009-2-2 loss: 0.802058  [   96/  118]
At round 79 accuracy: 0.6445623342175066
At round 79 training accuracy: 0.5915492957746479
At round 79 training loss: 0.8201504257715625
update_location
xs = [  -3.9056584     4.20031788  415.00902392   18.81129433    0.97929623
    3.95640986 -377.44319194 -356.32485185  399.66397685 -342.06087855]
ys = [ 407.5879595   390.55583871    1.32061395 -377.45517586  369.35018685
  352.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [419.69417305 403.17676746 426.88901831 390.93001238 382.6493428
 366.73352471 390.47439568 370.09198357 412.35902362 356.40097712]
dists_bs = [288.73938531 280.31602439 615.04079331 585.46523308 262.18449197
 252.27617572 269.32114847 251.42902847 595.83823656 239.06030698]
uav_gains = [6.18777499e-13 7.04863808e-13 5.86815526e-13 7.83470858e-13
 8.46017284e-13 9.94934833e-13 7.86695341e-13 9.59778197e-13
 6.54600040e-13 1.11885798e-12]
bs_gains = [1.42520458e-11 1.54838835e-11 1.71539091e-12 1.96920521e-12
 1.86721568e-11 2.07989120e-11 1.73195580e-11 2.09957270e-11
 1.87471239e-12 2.41809390e-11]
Round 80
-------------------------------
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.32829867 0.63627291 0.3245873  0.12729558 0.73321141 0.35335129
 0.15333479 0.44714953 0.32424655 0.28666076]
obj_prev = 3.7144087846743608
eta_min = 2.74115185868049e-289	eta_max = 0.9972962516128097
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 0.8366574097544858	eta = 0.9090909090909091
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 2.7178337538132036	eta = 0.2798543671643672
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.5874476546409413	eta = 0.4791324255686323
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.4095987989406396	eta = 0.539584487304448
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.398385343651186	eta = 0.5439113393776205
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.39833229498431	eta = 0.5439319737944585
eta = 0.5439319737944585
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [0.05024753 0.10567934 0.04944995 0.01714797 0.12202973 0.05822333
 0.02153466 0.0713834  0.05184269 0.04705721]
ene_total = [0.14943821 0.1966487  0.15032166 0.08411788 0.22380943 0.11442236
 0.09147288 0.17052575 0.12276251 0.0948129 ]
ti_comp = [14.85036606 15.12826574 14.83721764 14.90182223 15.13274303 15.13514254
 14.90262547 14.93825677 15.02250443 15.13829514]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [3.59542268e-08 3.22308656e-07 3.43299174e-08 1.41918577e-09
 4.95954233e-07 5.38515457e-08 2.81040133e-09 1.01876001e-07
 3.85885102e-08 2.84187337e-08]
ene_total = [0.06361869 0.01615569 0.06586436 0.05483025 0.01539129 0.01498072
 0.05469306 0.04860763 0.03421856 0.01444223]
optimize_network_iter = 0 obj = 0.38280246883118135
eta = 0.5439319737944585
freqs = [1691794.45393543 3492777.65574166 1666416.00768811  575364.79907189
 4031976.3853505  1923448.44978197  722512.19806255 2389281.50597988
 1725501.02801081 1554244.18120876]
eta_min = 0.5439319737944591	eta_max = 0.9542982433752891
af = 1.4269447035111872e-06	bf = 0.17458309747545425	zeta = 1.569639173862306e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [5.62757246e-09 5.04479022e-08 5.37333478e-09 2.22131623e-10
 7.76269894e-08 8.42886922e-09 4.39885336e-10 1.59456795e-08
 6.03989174e-09 4.44811354e-09]
ene_total = [0.3191587  0.08104669 0.33042464 0.27506954 0.07721065 0.07515409
 0.2743813  0.24385151 0.17166562 0.07245282]
ti_comp = [2.66191833 2.93981801 2.64876991 2.7133745  2.9442953  2.9466948
 2.71417774 2.74980904 2.8340567  2.9498474 ]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
optimize_network_iter = 1 obj = 1.9204150651233343
eta = 0.9090909090909091
freqs = [1477274.39950027 2813265.88968522 1461042.34829177  494588.56272364
 3243585.97051685 1546333.09614372  620927.40517717 2031588.97688467
 1431594.99214584 1248440.38657298]
eta_min = 0.9090909090909186	eta_max = 0.9090909090909067
af = 9.470450164325744e-07	bf = 0.17458309747545425	zeta = 1.0417495180758319e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
ti_comp = [2.66191833 2.93981801 2.64876991 2.7133745  2.9442953  2.9466948
 2.71417774 2.74980904 2.8340567  2.9498474 ]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
optimize_network_iter = 2 obj = 1.9204150651232854
eta = 0.9090909090909067
freqs = [1477274.39950027 2813265.88968522 1461042.34829176  494588.56272364
 3243585.97051684 1546333.09614371  620927.40517717 2031588.97688466
 1431594.99214584 1248440.38657297]
Done!
At round 80 eta: 0.9090909090909067
At round 80 local rounds: 3.120939520577836
At round 80 global rounds: 8.568286085798688
At round 80 a_n: 0.43638925173967635
gradient difference: 1.0103222131729126
train() client id: f_00000-0-0 loss: 0.999422  [   32/  126]
train() client id: f_00000-0-1 loss: 0.721216  [   64/  126]
train() client id: f_00000-0-2 loss: 0.603621  [   96/  126]
train() client id: f_00000-1-0 loss: 0.698803  [   32/  126]
train() client id: f_00000-1-1 loss: 0.816141  [   64/  126]
train() client id: f_00000-1-2 loss: 0.749044  [   96/  126]
train() client id: f_00000-2-0 loss: 0.790594  [   32/  126]
train() client id: f_00000-2-1 loss: 0.723225  [   64/  126]
train() client id: f_00000-2-2 loss: 0.837334  [   96/  126]
train() client id: f_00001-0-0 loss: 0.628249  [   32/  265]
train() client id: f_00001-0-1 loss: 0.611499  [   64/  265]
train() client id: f_00001-0-2 loss: 0.653009  [   96/  265]
train() client id: f_00001-0-3 loss: 0.529484  [  128/  265]
train() client id: f_00001-0-4 loss: 0.593937  [  160/  265]
train() client id: f_00001-0-5 loss: 0.696648  [  192/  265]
train() client id: f_00001-0-6 loss: 0.703851  [  224/  265]
train() client id: f_00001-0-7 loss: 0.611001  [  256/  265]
train() client id: f_00001-1-0 loss: 0.627586  [   32/  265]
train() client id: f_00001-1-1 loss: 0.708407  [   64/  265]
train() client id: f_00001-1-2 loss: 0.564901  [   96/  265]
train() client id: f_00001-1-3 loss: 0.730632  [  128/  265]
train() client id: f_00001-1-4 loss: 0.605918  [  160/  265]
train() client id: f_00001-1-5 loss: 0.507913  [  192/  265]
train() client id: f_00001-1-6 loss: 0.650010  [  224/  265]
train() client id: f_00001-1-7 loss: 0.627133  [  256/  265]
train() client id: f_00001-2-0 loss: 0.530296  [   32/  265]
train() client id: f_00001-2-1 loss: 0.629326  [   64/  265]
train() client id: f_00001-2-2 loss: 0.664662  [   96/  265]
train() client id: f_00001-2-3 loss: 0.600098  [  128/  265]
train() client id: f_00001-2-4 loss: 0.625449  [  160/  265]
train() client id: f_00001-2-5 loss: 0.599834  [  192/  265]
train() client id: f_00001-2-6 loss: 0.628331  [  224/  265]
train() client id: f_00001-2-7 loss: 0.661675  [  256/  265]
train() client id: f_00002-0-0 loss: 0.980504  [   32/  124]
train() client id: f_00002-0-1 loss: 1.021003  [   64/  124]
train() client id: f_00002-0-2 loss: 0.856035  [   96/  124]
train() client id: f_00002-1-0 loss: 0.917312  [   32/  124]
train() client id: f_00002-1-1 loss: 1.088605  [   64/  124]
train() client id: f_00002-1-2 loss: 1.004544  [   96/  124]
train() client id: f_00002-2-0 loss: 0.941743  [   32/  124]
train() client id: f_00002-2-1 loss: 0.853098  [   64/  124]
train() client id: f_00002-2-2 loss: 0.973865  [   96/  124]
train() client id: f_00003-0-0 loss: 0.673958  [   32/   43]
train() client id: f_00003-1-0 loss: 0.662849  [   32/   43]
train() client id: f_00003-2-0 loss: 0.248031  [   32/   43]
train() client id: f_00004-0-0 loss: 0.773508  [   32/  306]
train() client id: f_00004-0-1 loss: 0.579128  [   64/  306]
train() client id: f_00004-0-2 loss: 0.696134  [   96/  306]
train() client id: f_00004-0-3 loss: 0.592474  [  128/  306]
train() client id: f_00004-0-4 loss: 0.685384  [  160/  306]
train() client id: f_00004-0-5 loss: 0.725481  [  192/  306]
train() client id: f_00004-0-6 loss: 0.683923  [  224/  306]
train() client id: f_00004-0-7 loss: 0.573172  [  256/  306]
train() client id: f_00004-0-8 loss: 0.608299  [  288/  306]
train() client id: f_00004-1-0 loss: 0.552481  [   32/  306]
train() client id: f_00004-1-1 loss: 0.616070  [   64/  306]
train() client id: f_00004-1-2 loss: 0.639000  [   96/  306]
train() client id: f_00004-1-3 loss: 0.677177  [  128/  306]
train() client id: f_00004-1-4 loss: 0.732230  [  160/  306]
train() client id: f_00004-1-5 loss: 0.685321  [  192/  306]
train() client id: f_00004-1-6 loss: 0.728804  [  224/  306]
train() client id: f_00004-1-7 loss: 0.561185  [  256/  306]
train() client id: f_00004-1-8 loss: 0.622228  [  288/  306]
train() client id: f_00004-2-0 loss: 0.692622  [   32/  306]
train() client id: f_00004-2-1 loss: 0.724201  [   64/  306]
train() client id: f_00004-2-2 loss: 0.729139  [   96/  306]
train() client id: f_00004-2-3 loss: 0.727244  [  128/  306]
train() client id: f_00004-2-4 loss: 0.493503  [  160/  306]
train() client id: f_00004-2-5 loss: 0.753444  [  192/  306]
train() client id: f_00004-2-6 loss: 0.644458  [  224/  306]
train() client id: f_00004-2-7 loss: 0.599663  [  256/  306]
train() client id: f_00004-2-8 loss: 0.602005  [  288/  306]
train() client id: f_00005-0-0 loss: 0.565433  [   32/  146]
train() client id: f_00005-0-1 loss: 0.686372  [   64/  146]
train() client id: f_00005-0-2 loss: 0.613410  [   96/  146]
train() client id: f_00005-0-3 loss: 0.485488  [  128/  146]
train() client id: f_00005-1-0 loss: 0.583267  [   32/  146]
train() client id: f_00005-1-1 loss: 0.640034  [   64/  146]
train() client id: f_00005-1-2 loss: 0.565632  [   96/  146]
train() client id: f_00005-1-3 loss: 0.560754  [  128/  146]
train() client id: f_00005-2-0 loss: 0.752843  [   32/  146]
train() client id: f_00005-2-1 loss: 0.655602  [   64/  146]
train() client id: f_00005-2-2 loss: 0.651749  [   96/  146]
train() client id: f_00005-2-3 loss: 0.502789  [  128/  146]
train() client id: f_00006-0-0 loss: 0.501261  [   32/   54]
train() client id: f_00006-1-0 loss: 0.456597  [   32/   54]
train() client id: f_00006-2-0 loss: 0.456496  [   32/   54]
train() client id: f_00007-0-0 loss: 0.774090  [   32/  179]
train() client id: f_00007-0-1 loss: 0.621605  [   64/  179]
train() client id: f_00007-0-2 loss: 0.858454  [   96/  179]
train() client id: f_00007-0-3 loss: 0.544785  [  128/  179]
train() client id: f_00007-0-4 loss: 0.896086  [  160/  179]
train() client id: f_00007-1-0 loss: 0.613400  [   32/  179]
train() client id: f_00007-1-1 loss: 0.956259  [   64/  179]
train() client id: f_00007-1-2 loss: 0.724685  [   96/  179]
train() client id: f_00007-1-3 loss: 0.664796  [  128/  179]
train() client id: f_00007-1-4 loss: 0.503547  [  160/  179]
train() client id: f_00007-2-0 loss: 0.659733  [   32/  179]
train() client id: f_00007-2-1 loss: 0.535301  [   64/  179]
train() client id: f_00007-2-2 loss: 0.844806  [   96/  179]
train() client id: f_00007-2-3 loss: 0.801403  [  128/  179]
train() client id: f_00007-2-4 loss: 0.529388  [  160/  179]
train() client id: f_00008-0-0 loss: 0.856415  [   32/  130]
train() client id: f_00008-0-1 loss: 0.731444  [   64/  130]
train() client id: f_00008-0-2 loss: 0.851251  [   96/  130]
train() client id: f_00008-0-3 loss: 0.799703  [  128/  130]
train() client id: f_00008-1-0 loss: 0.731510  [   32/  130]
train() client id: f_00008-1-1 loss: 0.763573  [   64/  130]
train() client id: f_00008-1-2 loss: 0.854993  [   96/  130]
train() client id: f_00008-1-3 loss: 0.862469  [  128/  130]
train() client id: f_00008-2-0 loss: 0.716746  [   32/  130]
train() client id: f_00008-2-1 loss: 0.848432  [   64/  130]
train() client id: f_00008-2-2 loss: 0.703181  [   96/  130]
train() client id: f_00008-2-3 loss: 0.924955  [  128/  130]
train() client id: f_00009-0-0 loss: 0.691766  [   32/  118]
train() client id: f_00009-0-1 loss: 0.583409  [   64/  118]
train() client id: f_00009-0-2 loss: 0.666777  [   96/  118]
train() client id: f_00009-1-0 loss: 0.605703  [   32/  118]
train() client id: f_00009-1-1 loss: 0.676385  [   64/  118]
train() client id: f_00009-1-2 loss: 0.606247  [   96/  118]
train() client id: f_00009-2-0 loss: 0.845942  [   32/  118]
train() client id: f_00009-2-1 loss: 0.469691  [   64/  118]
train() client id: f_00009-2-2 loss: 0.702559  [   96/  118]
At round 80 accuracy: 0.6445623342175066
At round 80 training accuracy: 0.5895372233400402
At round 80 training loss: 0.8279585305432291
update_location
xs = [  -3.9056584     4.20031788  420.00902392   18.81129433    0.97929623
    3.95640986 -382.44319194 -361.32485185  404.66397685 -347.06087855]
ys = [ 412.5879595   395.55583871    1.32061395 -382.45517586  374.35018685
  357.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [424.55162052 408.02213691 431.75146113 395.7598089  387.47776893
 371.54625506 395.30960727 374.90842191 417.20690806 361.20252668]
dists_bs = [292.78202169 284.1788731  619.83890462 590.18564105 265.91203375
 255.82496036 273.09945612 255.05843424 600.66200473 242.58078894]
uav_gains = [5.96881287e-13 6.77526367e-13 5.66782867e-13 7.50688319e-13
 8.08508001e-13 9.45234363e-13 7.53640053e-13 9.13074203e-13
 6.30534284e-13 1.05807042e-12]
bs_gains = [1.37078628e-11 1.49017434e-11 1.67846913e-12 1.92542185e-12
 1.79484814e-11 2.00011019e-11 1.66569601e-11 2.01698633e-11
 1.83286144e-12 2.32111230e-11]
Round 81
-------------------------------
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.18431614 0.35650715 0.18224127 0.07169206 0.41081325 0.19799863
 0.08628049 0.25087914 0.1817507  0.16063512]
obj_prev = 2.0831139657376503
eta_min = 0.0	eta_max = inf
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.4687274993903175	eta = 0.909090909090909
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 1.5461570538938256	eta = 0.2755967820109389
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.8960917211906734	eta = 0.47552711230325256
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7944965020417399	eta = 0.5363345306638816
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7881017357421507	eta = 0.5406864230991465
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7880715228657817	eta = 0.54070715179138
eta = 0.54070715179138
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [0.05073821 0.10671132 0.04993285 0.01731542 0.12322138 0.0587919
 0.02174495 0.07208048 0.05234895 0.04751674]
ene_total = [0.08444145 0.1104692  0.08493497 0.04776518 0.12572503 0.06427522
 0.05189808 0.09632167 0.06897994 0.05326078]
ti_comp = [26.93025964 27.2160528  26.91701177 26.98204373 27.22058811 27.22304297
 26.98284085 27.01864488 27.10909704 27.22621665]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.12565281e-08 1.02532430e-07 1.07395299e-08 4.45686613e-10
 1.57813146e-07 1.71379604e-08 8.82632352e-10 3.20631268e-08
 1.22003941e-08 9.04576573e-09]
ene_total = [0.03623342 0.00907936 0.03749214 0.03131323 0.00864849 0.00841512
 0.03123749 0.02783567 0.01924149 0.00811357]
optimize_network_iter = 0 obj = 0.21760996983692812
eta = 0.54070715179138
freqs = [ 942029.79379861 1960448.1887988   927533.22358696  320869.35975109
 2263385.64084244 1079818.64242409  402940.3153214  1333902.54699206
  965523.69021721  872628.40406785]
eta_min = 0.5407071517913803	eta_max = 0.9738786232505544
af = 2.5132315156122307e-07	bf = 0.0999465483120546	zeta = 2.764554667173454e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.74483827e-09 1.58932227e-08 1.66470005e-09 6.90844506e-11
 2.44621090e-08 2.65650021e-09 1.36814006e-10 4.97000232e-09
 1.89114391e-09 1.40215510e-09]
ene_total = [0.1830592  0.0458705  0.18941855 0.15820137 0.04369346 0.04251496
 0.15781873 0.14063181 0.09721221 0.04099149]
ti_comp = [5.02451092 5.31030408 5.01126305 5.07629501 5.3148394  5.31729426
 5.07709213 5.11289616 5.20334832 5.32046794]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
optimize_network_iter = 1 obj = 1.0994121997748156
eta = 0.909090909090909
freqs = [ 782640.11306705 1557441.83511423  772253.41470056  264366.82466173
 1796869.89992426  856934.27544998  331943.42290144 1092625.69941991
  779732.80435494  692177.58267536]
eta_min = 0.9090909090909166	eta_max = 0.9090909090908771
af = 1.605687647826223e-07	bf = 0.0999465483120546	zeta = 1.7662564126088454e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
ti_comp = [5.02451092 5.31030408 5.01126305 5.07629501 5.3148394  5.31729426
 5.07709213 5.11289616 5.20334832 5.32046794]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
optimize_network_iter = 2 obj = 1.09941219977443
eta = 0.9090909090908771
freqs = [ 782640.11306704 1557441.83511425  772253.41470055  264366.82466173
 1796869.89992429  856934.27544999  331943.42290144 1092625.69941991
  779732.80435495  692177.58267537]
Done!
At round 81 eta: 0.9090909090908771
At round 81 local rounds: 3.120939520578907
At round 81 global rounds: 4.80028176913475
At round 81 a_n: 0.09384340477035735
gradient difference: 0.9524320363998413
train() client id: f_00000-0-0 loss: 0.828887  [   32/  126]
train() client id: f_00000-0-1 loss: 1.161105  [   64/  126]
train() client id: f_00000-0-2 loss: 0.702332  [   96/  126]
train() client id: f_00000-1-0 loss: 1.082686  [   32/  126]
train() client id: f_00000-1-1 loss: 0.982602  [   64/  126]
train() client id: f_00000-1-2 loss: 0.717060  [   96/  126]
train() client id: f_00000-2-0 loss: 0.936359  [   32/  126]
train() client id: f_00000-2-1 loss: 0.701126  [   64/  126]
train() client id: f_00000-2-2 loss: 0.953182  [   96/  126]
train() client id: f_00001-0-0 loss: 0.607041  [   32/  265]
train() client id: f_00001-0-1 loss: 0.605130  [   64/  265]
train() client id: f_00001-0-2 loss: 0.466643  [   96/  265]
train() client id: f_00001-0-3 loss: 0.427595  [  128/  265]
train() client id: f_00001-0-4 loss: 0.525149  [  160/  265]
train() client id: f_00001-0-5 loss: 0.610497  [  192/  265]
train() client id: f_00001-0-6 loss: 0.552167  [  224/  265]
train() client id: f_00001-0-7 loss: 0.590843  [  256/  265]
train() client id: f_00001-1-0 loss: 0.457987  [   32/  265]
train() client id: f_00001-1-1 loss: 0.645685  [   64/  265]
train() client id: f_00001-1-2 loss: 0.506261  [   96/  265]
train() client id: f_00001-1-3 loss: 0.507756  [  128/  265]
train() client id: f_00001-1-4 loss: 0.615365  [  160/  265]
train() client id: f_00001-1-5 loss: 0.622489  [  192/  265]
train() client id: f_00001-1-6 loss: 0.466790  [  224/  265]
train() client id: f_00001-1-7 loss: 0.467287  [  256/  265]
train() client id: f_00001-2-0 loss: 0.533222  [   32/  265]
train() client id: f_00001-2-1 loss: 0.463958  [   64/  265]
train() client id: f_00001-2-2 loss: 0.535667  [   96/  265]
train() client id: f_00001-2-3 loss: 0.629850  [  128/  265]
train() client id: f_00001-2-4 loss: 0.505962  [  160/  265]
train() client id: f_00001-2-5 loss: 0.488107  [  192/  265]
train() client id: f_00001-2-6 loss: 0.644490  [  224/  265]
train() client id: f_00001-2-7 loss: 0.577483  [  256/  265]
train() client id: f_00002-0-0 loss: 0.946602  [   32/  124]
train() client id: f_00002-0-1 loss: 0.820673  [   64/  124]
train() client id: f_00002-0-2 loss: 0.747798  [   96/  124]
train() client id: f_00002-1-0 loss: 0.687417  [   32/  124]
train() client id: f_00002-1-1 loss: 0.962931  [   64/  124]
train() client id: f_00002-1-2 loss: 0.990609  [   96/  124]
train() client id: f_00002-2-0 loss: 0.767709  [   32/  124]
train() client id: f_00002-2-1 loss: 0.968265  [   64/  124]
train() client id: f_00002-2-2 loss: 1.073028  [   96/  124]
train() client id: f_00003-0-0 loss: 0.765591  [   32/   43]
train() client id: f_00003-1-0 loss: 1.021257  [   32/   43]
train() client id: f_00003-2-0 loss: 1.051251  [   32/   43]
train() client id: f_00004-0-0 loss: 0.878212  [   32/  306]
train() client id: f_00004-0-1 loss: 0.909670  [   64/  306]
train() client id: f_00004-0-2 loss: 0.930672  [   96/  306]
train() client id: f_00004-0-3 loss: 0.714056  [  128/  306]
train() client id: f_00004-0-4 loss: 0.994758  [  160/  306]
train() client id: f_00004-0-5 loss: 0.827797  [  192/  306]
train() client id: f_00004-0-6 loss: 0.874140  [  224/  306]
train() client id: f_00004-0-7 loss: 0.920580  [  256/  306]
train() client id: f_00004-0-8 loss: 0.990101  [  288/  306]
train() client id: f_00004-1-0 loss: 0.829113  [   32/  306]
train() client id: f_00004-1-1 loss: 0.789593  [   64/  306]
train() client id: f_00004-1-2 loss: 0.817693  [   96/  306]
train() client id: f_00004-1-3 loss: 0.809011  [  128/  306]
train() client id: f_00004-1-4 loss: 0.955619  [  160/  306]
train() client id: f_00004-1-5 loss: 1.131350  [  192/  306]
train() client id: f_00004-1-6 loss: 1.118079  [  224/  306]
train() client id: f_00004-1-7 loss: 0.803619  [  256/  306]
train() client id: f_00004-1-8 loss: 0.801738  [  288/  306]
train() client id: f_00004-2-0 loss: 1.038841  [   32/  306]
train() client id: f_00004-2-1 loss: 1.136014  [   64/  306]
train() client id: f_00004-2-2 loss: 0.925986  [   96/  306]
train() client id: f_00004-2-3 loss: 0.909976  [  128/  306]
train() client id: f_00004-2-4 loss: 0.683906  [  160/  306]
train() client id: f_00004-2-5 loss: 0.842106  [  192/  306]
train() client id: f_00004-2-6 loss: 0.844359  [  224/  306]
train() client id: f_00004-2-7 loss: 0.841421  [  256/  306]
train() client id: f_00004-2-8 loss: 0.915577  [  288/  306]
train() client id: f_00005-0-0 loss: 1.069664  [   32/  146]
train() client id: f_00005-0-1 loss: 0.862851  [   64/  146]
train() client id: f_00005-0-2 loss: 0.722897  [   96/  146]
train() client id: f_00005-0-3 loss: 0.503148  [  128/  146]
train() client id: f_00005-1-0 loss: 0.868061  [   32/  146]
train() client id: f_00005-1-1 loss: 0.874678  [   64/  146]
train() client id: f_00005-1-2 loss: 0.632256  [   96/  146]
train() client id: f_00005-1-3 loss: 0.605694  [  128/  146]
train() client id: f_00005-2-0 loss: 1.093813  [   32/  146]
train() client id: f_00005-2-1 loss: 0.666976  [   64/  146]
train() client id: f_00005-2-2 loss: 0.528378  [   96/  146]
train() client id: f_00005-2-3 loss: 0.840100  [  128/  146]
train() client id: f_00006-0-0 loss: 0.501198  [   32/   54]
train() client id: f_00006-1-0 loss: 0.446204  [   32/   54]
train() client id: f_00006-2-0 loss: 0.537754  [   32/   54]
train() client id: f_00007-0-0 loss: 0.763483  [   32/  179]
train() client id: f_00007-0-1 loss: 0.845769  [   64/  179]
train() client id: f_00007-0-2 loss: 0.745968  [   96/  179]
train() client id: f_00007-0-3 loss: 0.757537  [  128/  179]
train() client id: f_00007-0-4 loss: 0.887574  [  160/  179]
train() client id: f_00007-1-0 loss: 0.893281  [   32/  179]
train() client id: f_00007-1-1 loss: 0.700599  [   64/  179]
train() client id: f_00007-1-2 loss: 0.903962  [   96/  179]
train() client id: f_00007-1-3 loss: 0.790854  [  128/  179]
train() client id: f_00007-1-4 loss: 0.755060  [  160/  179]
train() client id: f_00007-2-0 loss: 0.825545  [   32/  179]
train() client id: f_00007-2-1 loss: 0.665450  [   64/  179]
train() client id: f_00007-2-2 loss: 0.821986  [   96/  179]
train() client id: f_00007-2-3 loss: 0.742311  [  128/  179]
train() client id: f_00007-2-4 loss: 0.808699  [  160/  179]
train() client id: f_00008-0-0 loss: 0.619009  [   32/  130]
train() client id: f_00008-0-1 loss: 0.553040  [   64/  130]
train() client id: f_00008-0-2 loss: 0.713534  [   96/  130]
train() client id: f_00008-0-3 loss: 0.678908  [  128/  130]
train() client id: f_00008-1-0 loss: 0.695186  [   32/  130]
train() client id: f_00008-1-1 loss: 0.525183  [   64/  130]
train() client id: f_00008-1-2 loss: 0.630983  [   96/  130]
train() client id: f_00008-1-3 loss: 0.745169  [  128/  130]
train() client id: f_00008-2-0 loss: 0.709503  [   32/  130]
train() client id: f_00008-2-1 loss: 0.565198  [   64/  130]
train() client id: f_00008-2-2 loss: 0.663936  [   96/  130]
train() client id: f_00008-2-3 loss: 0.653255  [  128/  130]
train() client id: f_00009-0-0 loss: 0.641522  [   32/  118]
train() client id: f_00009-0-1 loss: 0.722593  [   64/  118]
train() client id: f_00009-0-2 loss: 0.676669  [   96/  118]
train() client id: f_00009-1-0 loss: 0.783738  [   32/  118]
train() client id: f_00009-1-1 loss: 0.714003  [   64/  118]
train() client id: f_00009-1-2 loss: 0.723996  [   96/  118]
train() client id: f_00009-2-0 loss: 0.687335  [   32/  118]
train() client id: f_00009-2-1 loss: 0.560628  [   64/  118]
train() client id: f_00009-2-2 loss: 0.701545  [   96/  118]
At round 81 accuracy: 0.6472148541114059
At round 81 training accuracy: 0.5902079141515761
At round 81 training loss: 0.8122121649796987
update_location
xs = [  -3.9056584     4.20031788  425.00902392   18.81129433    0.97929623
    3.95640986 -387.44319194 -366.32485185  409.66397685 -352.06087855]
ys = [ 417.5879595   400.55583871    1.32061395 -387.45517586  379.35018685
  362.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [429.4123404  412.87119371 436.61701116 400.5937819  392.31049346
 376.36386803 400.1488692  379.72960556 422.05834182 366.00939068]
dists_bs = [296.85382231 288.07670906 624.64018276 594.91061737 269.68075861
 259.42157167 276.91649437 258.73355676 605.48863217 246.15248922]
uav_gains = [5.76271498e-13 6.51982263e-13 5.47875000e-13 7.20245856e-13
 7.73843588e-13 8.99752934e-13 7.22955533e-13 8.70243248e-13
 6.07950194e-13 1.00281652e-12]
bs_gains = [1.31878706e-11 1.43440329e-11 1.64259431e-12 1.88290873e-12
 1.72549694e-11 1.92343292e-11 1.60220239e-11 1.93778844e-11
 1.79224469e-12 2.22803616e-11]
Round 82
-------------------------------
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.03972093 0.07667447 0.03927574 0.01549837 0.08835219 0.04258691
 0.01863559 0.05403005 0.03910533 0.03455187]
obj_prev = 0.44843144567355053
eta_min = 0.0	eta_max = inf
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.10079758902614538	eta = 0.9090909090909091
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.3375882886973583	eta = 0.2714376502678347
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.19414841544868844	eta = 0.47198001400206313
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17188140883090305	eta = 0.5331243935293785
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17048248758463935	eta = 0.5374990307814271
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17047588964991173	eta = 0.5375198336265012
eta = 0.5375198336265012
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [0.05122607 0.10773737 0.05041296 0.01748191 0.12440617 0.05935719
 0.02195403 0.07277355 0.0528523  0.04797362]
ene_total = [0.01831375 0.02382031 0.0184197  0.01040877 0.02710943 0.01385921
 0.01130017 0.020882   0.01487805 0.0114845 ]
ti_comp = [127.24870803 127.54244623 127.23535515 127.30084038 127.54703912
 127.54954855 127.30163179 127.33761381 127.43428871 127.5527431 ]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [5.18855480e-10 4.80473758e-09 4.94641987e-10 2.06055571e-11
 7.39716068e-09 8.03419283e-10 4.08088986e-11 1.48554980e-09
 5.68197076e-10 4.24138750e-10]
ene_total = [0.00791929 0.00195895 0.00819024 0.00686146 0.00186576 0.00181484
 0.0068454  0.00611528 0.00415361 0.00175002]
optimize_network_iter = 0 obj = 0.04747485012945179
eta = 0.5375198336265012
freqs = [201283.26753271 422358.89209453 198109.08366522  68663.77799187
 487687.42285783 232682.88526082  86228.39443656 285750.39151138
 207370.78109251 188054.06037245]
eta_min = 0.537519833626502	eta_max = 0.9942699445819028
af = 2.5030539272411267e-09	bf = 0.021956175032555886	zeta = 2.7533593199652398e-09	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [7.96601739e-11 7.37674065e-10 7.59426628e-11 3.16358278e-12
 1.13569024e-09 1.23349415e-10 6.26541318e-12 2.28077296e-10
 8.72356169e-11 6.51182610e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
ti_comp = [24.69954088 24.99327907 24.686188   24.75167323 24.99787197 25.0003814
 24.75246463 24.78844665 24.88512156 25.00357594]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
optimize_network_iter = 1 obj = 0.24151792697303298
eta = 0.909090909090909
freqs = [159208.78105798 330908.549687   156766.40742107  54218.71812527
 382035.51668375 182260.08745128  68086.44578208 225366.34992481
 163038.03730914 147287.27775164]
eta_min = 0.909090909090918	eta_max = 0.9684934797724077
af = 1.5403473503468153e-09	bf = 0.021956175032555886	zeta = 1.694382085381497e-09	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
ti_comp = [24.69954088 24.99327907 24.686188   24.75167323 24.99787197 25.0003814
 24.75246463 24.78844665 24.88512156 25.00357594]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
optimize_network_iter = 2 obj = 0.24151792697305713
eta = 0.909090909090918
freqs = [159208.78105798 330908.549687   156766.40742107  54218.71812527
 382035.51668375 182260.08745128  68086.44578208 225366.34992481
 163038.03730914 147287.27775163]
Done!
At round 82 eta: 0.909090909090918
At round 82 local rounds: 3.1209395205774326
At round 82 global rounds: 1.0322774524740326
At round 82 a_n: -0.2487024421989581
gradient difference: 0.9032615423202515
train() client id: f_00000-0-0 loss: 0.941748  [   32/  126]
train() client id: f_00000-0-1 loss: 0.875675  [   64/  126]
train() client id: f_00000-0-2 loss: 0.773678  [   96/  126]
train() client id: f_00000-1-0 loss: 0.943423  [   32/  126]
train() client id: f_00000-1-1 loss: 0.776347  [   64/  126]
train() client id: f_00000-1-2 loss: 0.888511  [   96/  126]
train() client id: f_00000-2-0 loss: 0.915644  [   32/  126]
train() client id: f_00000-2-1 loss: 0.836213  [   64/  126]
train() client id: f_00000-2-2 loss: 0.748539  [   96/  126]
train() client id: f_00001-0-0 loss: 0.465477  [   32/  265]
train() client id: f_00001-0-1 loss: 0.447714  [   64/  265]
train() client id: f_00001-0-2 loss: 0.564028  [   96/  265]
train() client id: f_00001-0-3 loss: 0.528581  [  128/  265]
train() client id: f_00001-0-4 loss: 0.517128  [  160/  265]
train() client id: f_00001-0-5 loss: 0.700495  [  192/  265]
train() client id: f_00001-0-6 loss: 0.499556  [  224/  265]
train() client id: f_00001-0-7 loss: 0.541361  [  256/  265]
train() client id: f_00001-1-0 loss: 0.418126  [   32/  265]
train() client id: f_00001-1-1 loss: 0.660994  [   64/  265]
train() client id: f_00001-1-2 loss: 0.512391  [   96/  265]
train() client id: f_00001-1-3 loss: 0.513330  [  128/  265]
train() client id: f_00001-1-4 loss: 0.504683  [  160/  265]
train() client id: f_00001-1-5 loss: 0.515127  [  192/  265]
train() client id: f_00001-1-6 loss: 0.533125  [  224/  265]
train() client id: f_00001-1-7 loss: 0.580258  [  256/  265]
train() client id: f_00001-2-0 loss: 0.509340  [   32/  265]
train() client id: f_00001-2-1 loss: 0.535549  [   64/  265]
train() client id: f_00001-2-2 loss: 0.564905  [   96/  265]
train() client id: f_00001-2-3 loss: 0.512127  [  128/  265]
train() client id: f_00001-2-4 loss: 0.595363  [  160/  265]
train() client id: f_00001-2-5 loss: 0.442043  [  192/  265]
train() client id: f_00001-2-6 loss: 0.470586  [  224/  265]
train() client id: f_00001-2-7 loss: 0.609563  [  256/  265]
train() client id: f_00002-0-0 loss: 0.849729  [   32/  124]
train() client id: f_00002-0-1 loss: 0.810214  [   64/  124]
train() client id: f_00002-0-2 loss: 0.697516  [   96/  124]
train() client id: f_00002-1-0 loss: 0.503314  [   32/  124]
train() client id: f_00002-1-1 loss: 0.659470  [   64/  124]
train() client id: f_00002-1-2 loss: 1.059108  [   96/  124]
train() client id: f_00002-2-0 loss: 0.830828  [   32/  124]
train() client id: f_00002-2-1 loss: 0.809335  [   64/  124]
train() client id: f_00002-2-2 loss: 0.522856  [   96/  124]
train() client id: f_00003-0-0 loss: 0.567141  [   32/   43]
train() client id: f_00003-1-0 loss: 0.355088  [   32/   43]
train() client id: f_00003-2-0 loss: 0.608983  [   32/   43]
train() client id: f_00004-0-0 loss: 0.720193  [   32/  306]
train() client id: f_00004-0-1 loss: 0.894151  [   64/  306]
train() client id: f_00004-0-2 loss: 0.910420  [   96/  306]
train() client id: f_00004-0-3 loss: 0.828898  [  128/  306]
train() client id: f_00004-0-4 loss: 0.776004  [  160/  306]
train() client id: f_00004-0-5 loss: 0.791941  [  192/  306]
train() client id: f_00004-0-6 loss: 0.779794  [  224/  306]
train() client id: f_00004-0-7 loss: 0.835768  [  256/  306]
train() client id: f_00004-0-8 loss: 0.704696  [  288/  306]
train() client id: f_00004-1-0 loss: 0.738033  [   32/  306]
train() client id: f_00004-1-1 loss: 0.723982  [   64/  306]
train() client id: f_00004-1-2 loss: 0.787981  [   96/  306]
train() client id: f_00004-1-3 loss: 0.811547  [  128/  306]
train() client id: f_00004-1-4 loss: 0.744507  [  160/  306]
train() client id: f_00004-1-5 loss: 0.920398  [  192/  306]
train() client id: f_00004-1-6 loss: 0.833515  [  224/  306]
train() client id: f_00004-1-7 loss: 0.713314  [  256/  306]
train() client id: f_00004-1-8 loss: 0.897125  [  288/  306]
train() client id: f_00004-2-0 loss: 0.729971  [   32/  306]
train() client id: f_00004-2-1 loss: 0.850325  [   64/  306]
train() client id: f_00004-2-2 loss: 0.788646  [   96/  306]
train() client id: f_00004-2-3 loss: 0.755303  [  128/  306]
train() client id: f_00004-2-4 loss: 0.728326  [  160/  306]
train() client id: f_00004-2-5 loss: 0.813916  [  192/  306]
train() client id: f_00004-2-6 loss: 0.806313  [  224/  306]
train() client id: f_00004-2-7 loss: 0.950520  [  256/  306]
train() client id: f_00004-2-8 loss: 0.787365  [  288/  306]
train() client id: f_00005-0-0 loss: 0.710005  [   32/  146]
train() client id: f_00005-0-1 loss: 0.819088  [   64/  146]
train() client id: f_00005-0-2 loss: 0.535209  [   96/  146]
train() client id: f_00005-0-3 loss: 0.460737  [  128/  146]
train() client id: f_00005-1-0 loss: 0.980122  [   32/  146]
train() client id: f_00005-1-1 loss: 0.381211  [   64/  146]
train() client id: f_00005-1-2 loss: 0.357891  [   96/  146]
train() client id: f_00005-1-3 loss: 0.673169  [  128/  146]
train() client id: f_00005-2-0 loss: 0.459525  [   32/  146]
train() client id: f_00005-2-1 loss: 0.621449  [   64/  146]
train() client id: f_00005-2-2 loss: 0.848444  [   96/  146]
train() client id: f_00005-2-3 loss: 0.612325  [  128/  146]
train() client id: f_00006-0-0 loss: 0.507054  [   32/   54]
train() client id: f_00006-1-0 loss: 0.497827  [   32/   54]
train() client id: f_00006-2-0 loss: 0.508584  [   32/   54]
train() client id: f_00007-0-0 loss: 0.518249  [   32/  179]
train() client id: f_00007-0-1 loss: 0.548153  [   64/  179]
train() client id: f_00007-0-2 loss: 0.515679  [   96/  179]
train() client id: f_00007-0-3 loss: 0.916879  [  128/  179]
train() client id: f_00007-0-4 loss: 0.551795  [  160/  179]
train() client id: f_00007-1-0 loss: 0.539655  [   32/  179]
train() client id: f_00007-1-1 loss: 0.880193  [   64/  179]
train() client id: f_00007-1-2 loss: 0.598662  [   96/  179]
train() client id: f_00007-1-3 loss: 0.542628  [  128/  179]
train() client id: f_00007-1-4 loss: 0.551580  [  160/  179]
train() client id: f_00007-2-0 loss: 0.716553  [   32/  179]
train() client id: f_00007-2-1 loss: 0.563349  [   64/  179]
train() client id: f_00007-2-2 loss: 0.528924  [   96/  179]
train() client id: f_00007-2-3 loss: 0.518900  [  128/  179]
train() client id: f_00007-2-4 loss: 0.687789  [  160/  179]
train() client id: f_00008-0-0 loss: 0.758978  [   32/  130]
train() client id: f_00008-0-1 loss: 0.793225  [   64/  130]
train() client id: f_00008-0-2 loss: 0.710068  [   96/  130]
train() client id: f_00008-0-3 loss: 0.719255  [  128/  130]
train() client id: f_00008-1-0 loss: 0.759384  [   32/  130]
train() client id: f_00008-1-1 loss: 0.669744  [   64/  130]
train() client id: f_00008-1-2 loss: 0.867628  [   96/  130]
train() client id: f_00008-1-3 loss: 0.661124  [  128/  130]
train() client id: f_00008-2-0 loss: 0.746116  [   32/  130]
train() client id: f_00008-2-1 loss: 0.813156  [   64/  130]
train() client id: f_00008-2-2 loss: 0.763355  [   96/  130]
train() client id: f_00008-2-3 loss: 0.629475  [  128/  130]
train() client id: f_00009-0-0 loss: 0.711978  [   32/  118]
train() client id: f_00009-0-1 loss: 0.758437  [   64/  118]
train() client id: f_00009-0-2 loss: 0.534211  [   96/  118]
train() client id: f_00009-1-0 loss: 0.763081  [   32/  118]
train() client id: f_00009-1-1 loss: 0.733310  [   64/  118]
train() client id: f_00009-1-2 loss: 0.661123  [   96/  118]
train() client id: f_00009-2-0 loss: 0.735123  [   32/  118]
train() client id: f_00009-2-1 loss: 0.619673  [   64/  118]
train() client id: f_00009-2-2 loss: 0.847557  [   96/  118]
At round 82 accuracy: 0.6472148541114059
At round 82 training accuracy: 0.5861837692823608
At round 82 training loss: 0.8276024441509813
Done!
